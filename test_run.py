#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è„šæœ¬ï¼šæŠ“å–å°‘é‡è®ºæ–‡å¹¶æµ‹è¯• AI åˆ†æåŠŸèƒ½
ç”¨äºéªŒè¯æ•´ä¸ªæµç¨‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import json
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils.scrapy import load_tags, query_arxiv, get_UTC_range, extract_code_links
from utils.analyser import get_client, get_model, get_prompt, _parse_json_safely, SYSTEM_PROMPT, USER_TEMPLATE


def test_scraper():
    """æµ‹è¯•è®ºæ–‡æŠ“å–åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ” æµ‹è¯• 1: è®ºæ–‡æŠ“å–")
    print("=" * 60)
    
    tags = load_tags('tags.json')
    print(f"ğŸ“‹ åˆ†ç±»æ ‡ç­¾: {tags}")
    
    # è·å–æ—¶é—´èŒƒå›´
    start, end, label_date = get_UTC_range()
    print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {label_date}")
    print(f"   UTC: {start} - {end}")
    
    # åªæŠ“å– 3 ç¯‡è®ºæ–‡ç”¨äºæµ‹è¯•
    print(f"\nğŸŒ æ­£åœ¨ä» arXiv æŠ“å–è®ºæ–‡ (é™åˆ¶ 3 ç¯‡)...")
    result = query_arxiv(tags, (start, end), max_results=3, fetch_thumbnails=False)
    
    papers = result.get("papers", [])
    print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡è®ºæ–‡\n")
    
    if not papers:
        print("âŒ æ²¡æœ‰æŠ“å–åˆ°è®ºæ–‡ï¼Œå¯èƒ½æ˜¯æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ–°è®ºæ–‡")
        print("   å°è¯•æ‰©å¤§æ—¶é—´èŒƒå›´æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return None
    
    # æ˜¾ç¤ºç¬¬ä¸€ç¯‡è®ºæ–‡çš„è¯¦æƒ…
    p = papers[0]
    print("ğŸ“„ ç¤ºä¾‹è®ºæ–‡:")
    print(f"   æ ‡é¢˜: {p['title'][:70]}...")
    print(f"   arXiv ID: {p['arxiv_id']}")
    print(f"   ä½œè€…: {', '.join(p['authors'][:3])}{'...' if len(p['authors']) > 3 else ''}")
    print(f"   åˆ†ç±»: {p.get('categories', [])}")
    print(f"   ä¸»åˆ†ç±»: {p.get('primary_category', '')}")
    print(f"   å‘å¸ƒæ—¥æœŸ: {p.get('published', 'N/A')}")
    print(f"   ä»£ç é“¾æ¥: {p.get('code_links', [])}")
    print(f"   æ‘˜è¦: {p['summary'][:150]}...")
    
    return papers


def test_code_extraction():
    """æµ‹è¯•ä»£ç é“¾æ¥æå–"""
    print("\n" + "=" * 60)
    print("ğŸ”— æµ‹è¯• 2: ä»£ç é“¾æ¥æå–")
    print("=" * 60)
    
    test_texts = [
        "Code is available at https://github.com/user/repo",
        "Our model is on https://huggingface.co/org/model-name",
        "Project page: https://project-name.github.io/demo/",
        "No code link here, just regular text.",
    ]
    
    for text in test_texts:
        links = extract_code_links(text)
        status = "âœ…" if links else "â–"
        print(f"{status} è¾“å…¥: {text[:50]}...")
        if links:
            for l in links:
                print(f"   â†’ {l['type']}: {l['url']}")


async def test_ai_analysis(papers):
    """æµ‹è¯• AI åˆ†æåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ¤– æµ‹è¯• 3: AI åˆ†æ")
    print("=" * 60)
    
    # æ£€æŸ¥ API é…ç½®
    api_key = os.getenv("LLM_API_KEY") or os.getenv("DEEPSEEK_API_KEY") or os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("LLM_BASE_URL", "https://api.deepseek.com")
    model = get_model()
    
    print(f"ğŸ“¡ API é…ç½®:")
    print(f"   Base URL: {base_url}")
    print(f"   Model: {model}")
    print(f"   API Key: {'âœ… å·²è®¾ç½®' if api_key else 'âŒ æœªè®¾ç½®'}")
    
    if not api_key:
        print("\nâŒ æœªè®¾ç½® API Keyï¼Œè·³è¿‡ AI åˆ†ææµ‹è¯•")
        print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export LLM_API_KEY=your-api-key")
        return None
    
    if not papers:
        print("\nâŒ æ²¡æœ‰è®ºæ–‡æ•°æ®ï¼Œè·³è¿‡ AI åˆ†ææµ‹è¯•")
        return None
    
    # åªåˆ†æç¬¬ä¸€ç¯‡è®ºæ–‡
    paper = papers[0]
    print(f"\nğŸ“ æ­£åœ¨åˆ†æè®ºæ–‡: {paper['title'][:50]}...")
    
    try:
        client = get_client()
        prompt = get_prompt(USER_TEMPLATE, paper)
        
        print(f"   å‘é€è¯·æ±‚åˆ° {model}...")
        
        resp = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
            response_format={"type": "json_object"},
            timeout=120,
        )
        
        text = resp.choices[0].message.content
        result = _parse_json_safely(text)
        
        if "_parse_error" in result:
            print(f"âŒ JSON è§£æé”™è¯¯: {result.get('_parse_error')}")
            print(f"   åŸå§‹å“åº”: {text[:200]}...")
            return None
        
        print("\nâœ… AI åˆ†ææˆåŠŸï¼\n")
        print("ğŸ“Š åˆ†æç»“æœ:")
        print("-" * 40)
        
        if "headline_zh" in result:
            print(f"ğŸ’¡ ä¸€å¥è¯è¦ç‚¹:\n   {result['headline_zh']}")
        
        if "summary_zh" in result:
            print(f"\nğŸ“ ä¸­æ–‡æ‘˜è¦:\n   {result['summary_zh'][:200]}...")
        
        if "intro_zh" in result:
            print(f"\nğŸ“‹ æ ¸å¿ƒè¦ç‚¹:")
            for i, point in enumerate(result['intro_zh'], 1):
                print(f"   {i}. {point}")
        
        if "method_zh" in result:
            print(f"\nğŸ”¬ æ–¹æ³•è¯¦è§£:\n   {result['method_zh'][:150]}...")
        
        if "application_zh" in result:
            print(f"\nğŸ¯ åº”ç”¨åœºæ™¯:\n   {result['application_zh']}")
        
        if "highlight_zh" in result:
            print(f"\nğŸ“Š å®éªŒäº®ç‚¹:\n   {result['highlight_zh']}")
        
        if "tags_zh" in result:
            print(f"\nğŸ·ï¸ å…³é”®è¯: {', '.join(result['tags_zh'])}")
        
        return result
        
    except Exception as e:
        print(f"\nâŒ AI åˆ†æå¤±è´¥: {type(e).__name__}: {e}")
        return None


def test_page_build():
    """æµ‹è¯•é¡µé¢ç”ŸæˆåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸŒ æµ‹è¯• 4: é¡µé¢ç”Ÿæˆ")
    print("=" * 60)
    
    try:
        from build_page import slugify, render_paper_md, load_tags
        
        # æµ‹è¯• slugify
        test_title = "Test Paper: A Novel Approach! (2024)"
        slug = slugify(test_title)
        print(f"âœ… slugify æµ‹è¯•: '{test_title}' â†’ '{slug}'")
        
        # æµ‹è¯•è®ºæ–‡æ¸²æŸ“
        test_paper = {
            "title": "Test Paper Title",
            "authors": ["Author One", "Author Two"],
            "arxiv_id": "2412.12345v1",
            "summary": "This is a test abstract.",
            "categories": ["cs.CV", "cs.AI"],
            "headline_zh": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ ‡é¢˜",
            "intro_zh": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
            "tags_zh": ["æµ‹è¯•", "ç¤ºä¾‹"],
            "summary_zh": "è¿™æ˜¯ä¸­æ–‡æ‘˜è¦ç¿»è¯‘ã€‚",
            "method_zh": "è¿™æ˜¯æ–¹æ³•æè¿°ã€‚",
            "code_links": [{"url": "https://github.com/test/repo", "type": "github"}],
        }
        
        md = render_paper_md(test_paper)
        print(f"âœ… render_paper_md æµ‹è¯•: ç”Ÿæˆ {len(md)} å­—ç¬¦çš„ Markdown")
        print(f"   åŒ…å«æ ‡é¢˜: {'# Test Paper Title' in md}")
        print(f"   åŒ…å«ä»£ç é“¾æ¥: {'GITHUB' in md}")
        print(f"   åŒ…å«ä¸­æ–‡æ‘˜è¦: {'ä¸­æ–‡æ‘˜è¦' in md or 'æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰' in md}")
        
    except Exception as e:
        print(f"âŒ é¡µé¢ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")


async def main():
    print("\n" + "ğŸš€" * 20)
    print("      arXiv Daily åŠŸèƒ½æµ‹è¯•")
    print("ğŸš€" * 20 + "\n")
    
    # æµ‹è¯• 1: è®ºæ–‡æŠ“å–
    papers = test_scraper()
    
    # æµ‹è¯• 2: ä»£ç é“¾æ¥æå–
    test_code_extraction()
    
    # æµ‹è¯• 3: AI åˆ†æ
    await test_ai_analysis(papers)
    
    # æµ‹è¯• 4: é¡µé¢ç”Ÿæˆ
    test_page_build()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print("=" * 60)
    print("\nå¦‚æœæ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œä½ å¯ä»¥è¿è¡Œå®Œæ•´æµç¨‹:")
    print("  python main.py          # æŠ“å–å¹¶åˆ†æä»Šæ—¥è®ºæ–‡")
    print("  python build_page.py    # ç”Ÿæˆç½‘ç«™é¡µé¢")


if __name__ == "__main__":
    asyncio.run(main())

