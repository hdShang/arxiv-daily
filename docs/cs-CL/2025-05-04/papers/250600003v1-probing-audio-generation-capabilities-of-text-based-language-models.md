---
layout: default
title: Probing Audio-Generation Capabilities of Text-Based Language Models
---

# Probing Audio-Generation Capabilities of Text-Based Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00003" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00003v1</a>
  <a href="https://arxiv.org/pdf/2506.00003.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00003v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00003v1', 'Probing Audio-Generation Capabilities of Text-Based Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arjun Prasaath Anbazhagan, Parteek Kumar, Ujjwal Kaur, Aslihan Akalin, Kevin Zhu, Sean O'Brien

**åˆ†ç±»**: cs.SD, cs.CL, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04

**å¤‡æ³¨**: Accepted at Conference of the North American Chapter of the Association for Computational Linguistics 2025, Student Research Workshop (NAACL SRW)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨æ–‡æœ¬åŸºç¡€è¯­è¨€æ¨¡å‹çš„éŸ³é¢‘ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `ä»£ç ç”Ÿæˆ` `éŸ³é¢‘å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬åŸºç¡€è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚éŸ³é¢‘æ—¶è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡é€æ­¥å¢åŠ éŸ³é¢‘ç”Ÿæˆå¤æ‚æ€§çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä»£ç ç”ŸæˆéŸ³é¢‘ï¼Œå¼¥è¡¥æ–‡æœ¬ä¸éŸ³é¢‘ä¹‹é—´çš„å·®è·ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨ç”ŸæˆåŸºæœ¬éŸ³é¢‘ç‰¹å¾æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚éŸ³é¢‘ç”Ÿæˆæ—¶æ€§èƒ½ä¸‹é™ï¼Œæç¤ºéœ€è¿›ä¸€æ­¥ç ”ç©¶æ”¹è¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬è¡¨ç¤ºçš„éŸ³é¢‘ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éŸ³é¢‘ä¸–ç•Œå­¦ä¹ ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶é‡‡ç”¨ä¸‰å±‚æ¬¡çš„æ–¹æ³•ï¼Œé€æ­¥å¢åŠ éŸ³é¢‘ç”Ÿæˆçš„å¤æ‚æ€§ï¼š1ï¼‰éŸ³ä¹éŸ³ç¬¦ï¼Œ2ï¼‰ç¯å¢ƒå£°éŸ³ï¼Œ3ï¼‰äººç±»è¯­éŸ³ã€‚é€šè¿‡åˆ©ç”¨ä»£ç ä½œä¸ºä¸­ä»‹ï¼Œæç¤ºLLMsç”Ÿæˆå¯æ‰§è¡Œçš„ä»£ç ä»¥äº§ç”Ÿæ‰€éœ€éŸ³é¢‘è¾“å‡ºã€‚è¯„ä¼°ç”ŸæˆéŸ³é¢‘çš„è´¨é‡å’Œå‡†ç¡®æ€§æ—¶ï¼Œé‡‡ç”¨äº†FADå’ŒCLAPè¯„åˆ†ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsèƒ½å¤Ÿç”ŸæˆåŸºæœ¬çš„éŸ³é¢‘ç‰¹å¾ï¼Œä½†éšç€éŸ³é¢‘å¤æ‚æ€§çš„å¢åŠ ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è¿™è¡¨æ˜LLMså¯¹å¬è§‰ä¸–ç•Œæœ‰æ½œåœ¨ç†è§£ï¼Œä½†å°†è¿™ç§ç†è§£è½¬åŒ–ä¸ºå®é™…éŸ³é¢‘è¾“å‡ºçš„èƒ½åŠ›ä»ç„¶è¾ƒä¸ºåˆæ­¥ã€‚è¿›ä¸€æ­¥ç ”ç©¶å¯ä»¥æå‡LLMç”ŸæˆéŸ³é¢‘çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œä»è€Œæ”¹å–„æ–‡æœ¬åŸºç¡€LLMsåœ¨éŸ³é¢‘ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨æ–‡æœ¬åŸºç¡€è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚éŸ³é¢‘æ—¶æ•ˆæœä¸ä½³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é€æ­¥å¢åŠ éŸ³é¢‘ç”Ÿæˆçš„å¤æ‚æ€§ï¼Œé‡‡ç”¨éŸ³ä¹éŸ³ç¬¦ã€ç¯å¢ƒå£°éŸ³å’Œäººç±»è¯­éŸ³ä¸‰ç§ç±»å‹ï¼Œåˆ©ç”¨ä»£ç ä½œä¸ºä¸­ä»‹ï¼Œæç¤ºLLMsç”Ÿæˆå¯æ‰§è¡Œä»£ç ä»¥äº§ç”ŸéŸ³é¢‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆç”ŸæˆéŸ³ä¹éŸ³ç¬¦ï¼Œå…¶æ¬¡ç”Ÿæˆç¯å¢ƒå£°éŸ³ï¼Œæœ€åç”Ÿæˆäººç±»è¯­éŸ³ã€‚æ¯ä¸ªé˜¶æ®µçš„å¤æ‚æ€§é€æ­¥å¢åŠ ï¼Œè¯„ä¼°ç”ŸæˆéŸ³é¢‘çš„è´¨é‡ä½¿ç”¨FADå’ŒCLAPè¯„åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºå°†ä»£ç ç”Ÿæˆä½œä¸ºéŸ³é¢‘ç”Ÿæˆçš„æ¡¥æ¢ï¼Œåˆ©ç”¨LLMsçš„æ–‡æœ¬ç†è§£èƒ½åŠ›æ¥ç”ŸæˆéŸ³é¢‘ï¼Œçªç ´äº†ä¼ ç»ŸéŸ³é¢‘ç”Ÿæˆæ–¹æ³•çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„éŸ³é¢‘å¤æ‚æ€§å±‚æ¬¡ï¼Œå¹¶é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ï¼ˆFADå’ŒCLAPï¼‰ï¼Œä»¥ç¡®ä¿ç”ŸæˆéŸ³é¢‘çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒè®¾è®¡è€ƒè™‘äº†ä¸åŒéŸ³é¢‘ç±»å‹çš„ç‰¹æ€§ï¼Œä¼˜åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨ç”ŸæˆåŸºæœ¬éŸ³é¢‘ç‰¹å¾æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚éŸ³é¢‘ç”Ÿæˆæ–¹é¢æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆç¯å¢ƒå£°éŸ³å’Œäººç±»è¯­éŸ³æ—¶ï¼Œå‡†ç¡®æ€§å’Œè´¨é‡å‡æœ‰æ‰€é™ä½ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†æå‡ç”ŸæˆéŸ³é¢‘è´¨é‡çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éŸ³ä¹åˆ›ä½œã€ç¯å¢ƒå£°éŸ³æ¨¡æ‹Ÿå’Œè¯­éŸ³åˆæˆç­‰ã€‚é€šè¿‡æå‡æ–‡æœ¬åŸºç¡€LLMsåœ¨éŸ³é¢‘ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€æ™ºèƒ½åŠ©æ‰‹å’Œåˆ›æ„å·¥å…·ç­‰æä¾›æ›´ä¸°å¯Œçš„åŠŸèƒ½ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> How does textual representation of audio relate to the Large Language Model's (LLMs) learning about the audio world? This research investigates the extent to which LLMs can be prompted to generate audio, despite their primary training in textual data. We employ a three-tier approach, progressively increasing the complexity of audio generation: 1) Musical Notes, 2) Environmental Sounds, and 3) Human Speech. To bridge the gap between text and audio, we leverage code as an intermediary, prompting LLMs to generate code that, when executed, produces the desired audio output. To evaluate the quality and accuracy of the generated audio, we employ FAD and CLAP scores. Our findings reveal that while LLMs can generate basic audio features, their performance deteriorates as the complexity of the audio increases. This suggests that while LLMs possess a latent understanding of the auditory world, their ability to translate this understanding into tangible audio output remains rudimentary. Further research into techniques that can enhance the quality and diversity of LLM-generated audio can lead to an improvement in the performance of text-based LLMs in generating audio.

