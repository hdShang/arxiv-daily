---
layout: default
title: QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach
---

# QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02146" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.02146v1</a>
  <a href="https://arxiv.org/pdf/2505.02146.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02146v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02146v1', 'QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shouyang Dong, Yuanbo Wen, Jun Bi, Di Huang, Jiaming Guo, Jianxing Xu, Ruibai Xu, Xinkai Song, Yifan Hao, Xuehai Zhou, Tianshi Chen, Qi Guo, Yunji Chen

**ÂàÜÁ±ª**: cs.CL, cs.LG, cs.PL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-04

**Â§áÊ≥®**: Accepted to OSDI 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫QiMeng-Xpiler‰ª•Ëß£ÂÜ≥ÂºÇÊûÑÊ∑±Â∫¶Â≠¶‰π†Á≥ªÁªüÁöÑÁºñÁ®ãË¥üÊãÖÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶Â≠¶‰π†` `ËΩ¨ÁºñËØë` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Á¨¶Âè∑Á®ãÂ∫èÂêàÊàê` `ÁºñÁ®ãÊïàÁéá` `ÂºÇÊûÑÁ≥ªÁªü` `Ëá™Âä®ÂåñÁºñÁ®ã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËΩ¨ÁºñËØëÊäÄÊúØÂú®ÂÆûÁé∞Âº†ÈáèÁ®ãÂ∫èË∑®Âπ≥Âè∞ËΩ¨Êç¢Êó∂ÔºåÈù¢‰∏¥ÁùÄÂ∑®Â§ßÁöÑ‰∫∫Â∑•ÊàêÊú¨ÂíåÂäüËÉΩ‰∏çÊ≠£Á°ÆÁöÑÈóÆÈ¢ò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑQiMeng-XpilerÈÄöËøáÁªìÂêàÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÁ¨¶Âè∑Á®ãÂ∫èÂêàÊàêÔºåËá™Âä®ÂåñÂú∞ËøõË°åÂº†ÈáèÁ®ãÂ∫èÁöÑËΩ¨ÁºñËØë„ÄÇ
3. Âú®ÂõõÁßç‰∏çÂêåÁöÑDLS‰∏äËøõË°åÁöÑÂÆûÈ™åÊòæÁ§∫ÔºåQiMeng-XpilerÁöÑÁøªËØëÂáÜÁ°ÆÁéáËææÂà∞95%ÔºåÊÄßËÉΩÊèêÂçáÂèØËææ2.0ÂÄçÔºåÁºñÁ®ãÊïàÁéáÊèêÈ´òÊòæËëó„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂºÇÊûÑÊ∑±Â∫¶Â≠¶‰π†Á≥ªÁªüÔºàDLSÔºâÂ¶ÇGPUÂíåASICÂú®Â∑•‰∏öÊï∞ÊçÆ‰∏≠ÂøÉÁöÑÂπøÊ≥õÂ∫îÁî®ÔºåË¶ÅÊ±Ç‰∏∫‰∏çÂêåÂπ≥Âè∞ÂºÄÂèëÂ§ö‰∏™‰ΩéÁ∫ßÂº†ÈáèÁ®ãÂ∫è„ÄÇÁé∞ÊúâÁöÑËΩ¨ÁºñËØëÊäÄÊúØÈù¢‰∏¥ÁùÄÂ∑®Â§ßÁöÑ‰∫∫Â∑•ÊàêÊú¨ÊàñÂäüËÉΩ‰∏çÊ≠£Á°ÆÁöÑÈóÆÈ¢òÔºå‰ΩøÂæóÂº†ÈáèÁ®ãÂ∫èÁöÑ‚ÄúÂÜô‰∏ÄÊ¨°ÔºåÈöèÂ§ÑËøêË°å‚ÄùÊàê‰∏∫‰∏Ä‰∏™Êú™Ëß£‰πãË∞ú„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÂûãËΩ¨ÁºñËØëÂô®QiMeng-XpilerÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÁ¨¶Âè∑Á®ãÂ∫èÂêàÊàêÁöÑÁ•ûÁªèÁ¨¶Âè∑ÂêàÊàêÊñπÊ≥ïÔºåËá™Âä®ÁøªËØëDLS‰∏≠ÁöÑÂº†ÈáèÁ®ãÂ∫è„ÄÇÂÆûÈ™åË°®ÊòéÔºåQiMeng-XpilerÂú®ÂõõÁßçÂÖ∑Êúâ‰∏çÂêåÁºñÁ®ãÊé•Âè£ÁöÑDLS‰∏äÔºåÂπ≥ÂùáÂáÜÁ°ÆÁéáËææÂà∞95%ÔºåÂπ∂‰∏îÁøªËØëÂêéÁöÑÁ®ãÂ∫èÊÄßËÉΩÊØî‰æõÂ∫îÂïÜÊèê‰æõÁöÑÊâãÂä®‰ºòÂåñÂ∫ìÊèêÂçá‰∫Ü2.0ÂÄçÔºåÁºñÁ®ãÁîü‰∫ßÂäõÊèêÈ´ò‰∫Ü96.0ÂÄç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂºÇÊûÑÊ∑±Â∫¶Â≠¶‰π†Á≥ªÁªü‰∏≠Âº†ÈáèÁ®ãÂ∫èÁöÑËΩ¨ÁºñËØëÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÈúÄË¶ÅÂ§ßÈáè‰∫∫Â∑•Âπ≤È¢ÑÔºå‰∏îÂ≠òÂú®ÂäüËÉΩ‰∏çÊ≠£Á°ÆÁöÑÈ£éÈô©ÔºåÂØºËá¥Ë∑®Âπ≥Âè∞ÁöÑÁºñÁ®ãÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöQiMeng-XpilerÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁªìÂêàÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ª£Á†ÅÁîüÊàêËÉΩÂäõ‰∏éÁ¨¶Âè∑Á®ãÂ∫èÂêàÊàêÔºåÂà©Áî®Á•ûÁªèÁ¨¶Âè∑ÂêàÊàêÊñπÊ≥ïÔºå‰ΩøÂæóÂ§çÊùÇÁöÑÁ¨¶Âè∑ÂêàÊàêËøáÁ®ãÂèòÂæóËÆ°ÁÆó‰∏äÂèØË°å„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöQiMeng-XpilerÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Â§ö‰∏™LLMËæÖÂä©ÁöÑÁºñËØëÈò∂ÊÆµÔºåÊØè‰∏™Èò∂ÊÆµÈÄöËøáÈ¢ÑÂÆö‰πâÁöÑÂÖÉÊèêÁ§∫ËøõË°åÁ®ãÂ∫èËΩ¨Êç¢ÔºåÂπ∂Âú®ËΩ¨Êç¢ËøáÁ®ã‰∏≠‰ΩøÁî®È´òÊïàÁöÑÁ¨¶Âè∑Á®ãÂ∫èÂêàÊàêÊù•‰øÆÂ§ç‰∏çÊ≠£Á°ÆÁöÑ‰ª£Á†ÅÁâáÊÆµ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜLLM‰∏éÁ¨¶Âè∑ÂêàÊàêÁªìÂêàÔºåÊòæËëóÈôç‰Ωé‰∫ÜËΩ¨ÁºñËØëËøáÁ®ã‰∏≠ÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶Ôºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥È´òÊïàÂú∞ÁîüÊàêÊ≠£Á°ÆÁöÑ‰ª£Á†Å„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂàÜÂ±ÇËá™Âä®Ë∞É‰ºòÁöÑÊñπÊ≥ïÔºåÁ≥ªÁªüÊÄßÂú∞Êé¢Á¥¢ËΩ¨Êç¢Èò∂ÊÆµÁöÑÂèÇÊï∞ÂíåÈ°∫Â∫èÔºå‰ª•Á°Æ‰øùÈ´òÊÄßËÉΩÁöÑÁ®ãÂ∫èÁîüÊàê„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåQiMeng-XpilerÂú®ÂõõÁßç‰∏çÂêåÁöÑÊ∑±Â∫¶Â≠¶‰π†Á≥ªÁªü‰∏äÂÆûÁé∞‰∫Ü95%ÁöÑÁøªËØëÂáÜÁ°ÆÁéáÔºå‰∏îÁøªËØëÂêéÁöÑÁ®ãÂ∫èÊÄßËÉΩÁõ∏ÊØî‰∫éÊâãÂä®‰ºòÂåñÂ∫ìÊèêÂçá‰∫Ü2.0ÂÄçÔºåÁºñÁ®ãÁîü‰∫ßÂäõÊèêÂçáÈ´òËææ96.0ÂÄçÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰∫ëËÆ°ÁÆó„ÄÅËæπÁºòËÆ°ÁÆóÂíåÈ´òÊÄßËÉΩËÆ°ÁÆóÁ≠âÂú∫ÊôØÔºåËÉΩÂ§üÊòæËëóÊèêÂçáÂºÇÊûÑÊ∑±Â∫¶Â≠¶‰π†Á≥ªÁªüÁöÑÁºñÁ®ãÊïàÁéáÂíåÊÄßËÉΩ„ÄÇÊú™Êù•ÔºåQiMeng-XpilerÊúâÊúõÊé®Âä®Êõ¥Â§öÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÁöÑË∑®Âπ≥Âè∞ÂÖºÂÆπÊÄßÔºåÈôç‰ΩéÂºÄÂèëÊàêÊú¨„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering "Write Once, Run Anywhere" of tensor programs an open question.
>   We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically translating tensor programs across DLS via both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via pre-defined meta-prompts for program transformation. During each program transformation, efficient symbolic program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high performance, we propose a hierarchical auto-tuning approach to systematically explore both the parameters and sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average, and the performance of translated programs achieves up to 2.0x over vendor-provided manually-optimized libraries. As a result, the programming productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor programs.

