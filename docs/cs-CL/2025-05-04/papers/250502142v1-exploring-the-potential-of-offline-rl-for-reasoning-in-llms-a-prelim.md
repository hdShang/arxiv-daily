---
layout: default
title: "Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study"
---

# Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02142" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02142v1</a>
  <a href="https://arxiv.org/pdf/2505.02142.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02142v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02142v1', 'Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢ç´¢ç¦»çº¿å¼ºåŒ–å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `ç›´æ¥åå¥½ä¼˜åŒ–` `é•¿ä¸Šä¸‹æ–‡` `æ¨¡å‹è®­ç»ƒ` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ–¹æ³•ä¸»è¦ä¾èµ–åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œå¤æ‚æ€§ã€‚
2. æœ¬æ–‡æå‡ºä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå…¶å˜ä½“LD-DPOï¼Œä»¥æå‡æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä¸Šå¹³å‡æå‡3.3%ï¼Œåœ¨Arena-HardåŸºå‡†ä¸Šæå‡10.1%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸»è¦ä¾èµ–åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•çš„è®¡ç®—æˆæœ¬å’Œå¤æ‚æ€§è¾ƒé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç®€å•ä¸”ç»æµçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡ç ”ç©¶äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åŠå…¶é•¿åº¦å»æ•æ„ŸåŒ–å˜ä½“LD-DPOï¼Œä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜è¿™äº›ç®€å•çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹³å‡æå‡3.3%ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„Arena-HardåŸºå‡†ä¸Šæ›´æ˜¯æé«˜äº†10.1%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†DPOå¯¹è¾“å‡ºé•¿åº¦çš„æ•æ„Ÿæ€§ï¼Œå¼ºè°ƒæ¨ç†é•¿åº¦çš„å¢åŠ åº”ä¸è¯­ä¹‰ä¸°å¯Œæ€§ç›¸ä¸€è‡´ï¼Œéšæ„å»¶é•¿å¯èƒ½ä¼šå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„è®¡ç®—æˆæœ¬å’Œå¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨è¾ƒé«˜çš„èµ„æºæ¶ˆè€—å’Œå®ç°éš¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯DPOå’ŒLD-DPOï¼Œä»¥ç®€åŒ–æ¨ç†è¿‡ç¨‹å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„åå¥½ï¼Œæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®å¤„ç†é˜¶æ®µè´Ÿè´£å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹è®­ç»ƒé˜¶æ®µåº”ç”¨DPOå’ŒLD-DPOè¿›è¡Œä¼˜åŒ–ï¼Œè¯„ä¼°é˜¶æ®µåˆ™é€šè¿‡å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶æ˜¯DPOå’ŒLD-DPOï¼Œä»¥æ›¿ä»£ä¼ ç»Ÿçš„åœ¨çº¿æ–¹æ³•ï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºçš„åå¥½ï¼Œå¹¶é’ˆå¯¹ä¸åŒæ¨ç†é•¿åº¦è¿›è¡Œäº†å‚æ•°è°ƒæ•´ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¢åŠ æ¨ç†é•¿åº¦æ—¶ä»èƒ½ä¿æŒè¯­ä¹‰çš„ä¸°å¯Œæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹³å‡æå‡3.3%ã€‚åœ¨Arena-HardåŸºå‡†ä¸Šï¼Œæ¨¡å‹æ€§èƒ½æå‡æ›´ä¸ºæ˜¾è‘—ï¼Œè¾¾åˆ°äº†10.1%ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­æä¾›æ›´å‡†ç¡®çš„ç»“æœï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚æœªæ¥ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯èƒ½æˆä¸ºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„ä¸»æµé€‰æ‹©ï¼Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite significant advances in long-context reasoning by large language models (LLMs), primarily through Online Reinforcement Learning (RL) methods, these approaches incur substantial computational costs and complexity. In contrast, simpler and more economical Offline RL methods remain underexplored. To address this gap, we investigate the effectiveness of Offline RL methods, specifically Direct Preference Optimization (DPO) and its length-desensitized variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive experiments across multiple reasoning benchmarks demonstrate that these simpler Offline RL methods substantially improve model performance, achieving an average enhancement of 3.3\%, with a particularly notable increase of 10.1\% on the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity to output length, emphasizing that increasing reasoning length should align with semantic richness, as indiscriminate lengthening may adversely affect model performance. We provide comprehensive descriptions of our data processing and training methodologies, offering empirical evidence and practical insights for developing more cost-effective Offline RL approaches.

