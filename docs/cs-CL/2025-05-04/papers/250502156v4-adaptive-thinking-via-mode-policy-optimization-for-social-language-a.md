---
layout: default
title: Adaptive Thinking via Mode Policy Optimization for Social Language Agents
---

# Adaptive Thinking via Mode Policy Optimization for Social Language Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02156" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02156v4</a>
  <a href="https://arxiv.org/pdf/2505.02156.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02156v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02156v4', 'Adaptive Thinking via Mode Policy Optimization for Social Language Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04 (æ›´æ–°: 2025-05-22)

**å¤‡æ³¨**: Work in Progress. The code and data are available, see https://github.com/MozerWang/AMPO

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ä»¥è§£å†³ç¤¾äº¤è¯­è¨€ä»£ç†çš„æ¨ç†æ·±åº¦é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªé€‚åº”å­¦ä¹ ` `ç¤¾äº¤æ™ºèƒ½` `æ¨ç†æ·±åº¦` `è¯­è¨€ä»£ç†` `æ¨¡å¼ä¼˜åŒ–` `åŠ¨æ€åˆ‡æ¢` `å¤šç²’åº¦æ€ç»´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦çš„èƒ½åŠ›ï¼Œå¯¼è‡´ç¤¾äº¤è¯­è¨€ä»£ç†åœ¨å¤æ‚åœºæ™¯ä¸­çš„è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºè‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«åˆ†å±‚æ€ç»´æ¨¡å¼å’Œå¼€å‘AMPOç®—æ³•ï¼Œå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¨¡å¼åˆ‡æ¢ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAMLåœ¨ä»»åŠ¡è¡¨ç°ä¸Šæ¯”GPT-4oæé«˜äº†15.6%ï¼Œä¸”AMPOåœ¨æ¨ç†é“¾é•¿åº¦ä¸Šå‡å°‘äº†32.8%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆçš„ç¤¾äº¤æ™ºèƒ½æ¨¡æ‹Ÿè¦æ±‚è¯­è¨€ä»£ç†èƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œè€Œè¿™ä¸€èƒ½åŠ›åœ¨å½“å‰ç ”ç©¶ä¸­æ˜æ˜¾ç¼ºä¹ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆç¼ºä¹è¿™ç§æ¨ç†èƒ½åŠ›ï¼Œè¦ä¹ˆåœ¨æ‰€æœ‰åœºæ™¯ä¸­å¼ºåˆ¶æ‰§è¡Œç»Ÿä¸€çš„é•¿é“¾æ¨ç†ï¼Œå¯¼è‡´è¿‡å¤šçš„tokenä½¿ç”¨å’Œä¸çµæ´»çš„ç¤¾äº¤æ¨¡æ‹Ÿã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è¯­è¨€ä»£ç†åœ¨åŠ¨æ€ç¤¾äº¤äº’åŠ¨ä¸­çš„è‡ªé€‚åº”æ€ç»´èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆåŸºäºè®¤çŸ¥æ§åˆ¶ç†è®ºè¯†åˆ«äº†ä»ç›´è§‚ååº”åˆ°æ·±åº¦æ€è€ƒçš„åˆ†å±‚æ€ç»´æ¨¡å¼ï¼Œç„¶åå¼€å‘äº†è‡ªé€‚åº”æ¨¡å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ç®—æ³•ï¼Œä»¥ä¼˜åŒ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¨¡å¼åˆ‡æ¢å’Œæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAMLåœ¨ç¤¾äº¤æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­æ¯”GPT-4oçš„ä»»åŠ¡è¡¨ç°æé«˜äº†15.6%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¤¾äº¤è¯­è¨€ä»£ç†åœ¨åŠ¨æ€ç¤¾äº¤äº’åŠ¨ä¸­ç¼ºä¹çµæ´»æ¨ç†æ·±åº¦çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€é‡‡ç”¨å›ºå®šçš„æ¨ç†æ·±åº¦ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œé€‚åº”æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«ä¸åŒçš„æ€ç»´æ¨¡å¼å¹¶ä¼˜åŒ–æ¨¡å¼åˆ‡æ¢ï¼Œä½¿è¯­è¨€ä»£ç†èƒ½å¤Ÿæ ¹æ®ç¤¾äº¤æƒ…å¢ƒåŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAMLæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åˆ†å±‚æ€ç»´æ¨¡å¼çš„è®¾è®¡ï¼Œæ¶µç›–ä»ç›´è§‚ååº”åˆ°æ·±åº¦æ€è€ƒçš„å¤šç§æ¨¡å¼ï¼›å…¶æ¬¡æ˜¯è‡ªé€‚åº”æ¨¡å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ç®—æ³•ï¼Œç”¨äºå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¨¡å¼åˆ‡æ¢å’Œæ¨ç†ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šAMLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¤šç²’åº¦æ€ç»´æ¨¡å¼è®¾è®¡å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¨¡å¼åˆ‡æ¢æœºåˆ¶ï¼Œæ˜¾è‘—åŒºåˆ«äºç°æœ‰æ–¹æ³•çš„å›ºå®šæ·±åº¦æ¨ç†ï¼Œæå‡äº†æ¨ç†çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨AMPOç®—æ³•ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä¸åŒæ¨ç†æ·±åº¦çš„æƒé‡ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§å‚æ•°ä»¥ä¼˜åŒ–æ¨¡å¼åˆ‡æ¢çš„æ—¶æœºå’Œé¢‘ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAMLåœ¨ç¤¾äº¤æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­æ¯”GPT-4oçš„ä»»åŠ¡è¡¨ç°æé«˜äº†15.6%ã€‚æ­¤å¤–ï¼ŒAMPOç®—æ³•åœ¨æ¨ç†é“¾é•¿åº¦ä¸Šå‡å°‘äº†32.8%ï¼Œç›¸è¾ƒäºGRPOæå‡äº†7.0%ï¼Œæ˜¾ç¤ºå‡ºè‡ªé€‚åº”æ€ç»´æ¨¡å¼é€‰æ‹©çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤æœºå™¨äººã€æ™ºèƒ½å®¢æœå’Œè™šæ‹ŸåŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚ç¤¾äº¤åœºæ™¯ä¸­çš„äº¤äº’èƒ½åŠ›å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œéšç€è‡ªé€‚åº”æ¨ç†èƒ½åŠ›çš„å¢å¼ºï¼Œè¯­è¨€ä»£ç†å°†èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”äººç±»çš„æƒ…æ„Ÿå’Œæ„å›¾ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current studies. Existing methods either lack this kind of reasoning capability or enforce Long Chain-of-Thought reasoning uniformly across all scenarios, resulting in excessive token usage and inflexible social simulation. To address this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) framework in this paper, aiming to improve the adaptive thinking ability of language agents in dynamic social interactions. To this end, we first identify hierarchical thinking modes ranging from intuitive response to deep deliberation based on the cognitive control theory. We then develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the context-aware mode switching and reasoning. Our framework advances existing research in three key aspects: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence benchmarks verify that AML achieves 15.6% higher task performance than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains, demonstrating the advantage of adaptive thinking mode selection and optimization mechanism in AMPO over GRPO's fixed-depth solution.

