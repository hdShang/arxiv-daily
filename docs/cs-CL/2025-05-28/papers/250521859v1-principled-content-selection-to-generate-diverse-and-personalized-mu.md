---
layout: default
title: Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries
---

# Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21859" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21859v1</a>
  <a href="https://arxiv.org/pdf/2505.21859.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21859v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21859v1', 'Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vishakh Padmakumar, Zichao Wang, David Arbour, Jennifer Healey

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28

**å¤‡æ³¨**: To appear at ACL 2025 - Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåŸåˆ™çš„å†…å®¹é€‰æ‹©æ–¹æ³•ä»¥ç”Ÿæˆå¤šæ–‡æ¡£å¤šæ ·åŒ–ä¸ªæ€§åŒ–æ‘˜è¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ–‡æ¡£æ‘˜è¦` `å†…å®¹é€‰æ‹©` `è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹` `ä¸ªæ€§åŒ–æ‘˜è¦` `ä¿¡æ¯è¦†ç›–` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ–‡æ¡£æ‘˜è¦ä¸­é¢ä¸´â€œè¿·å¤±åœ¨ä¸­é—´â€çš„é—®é¢˜ï¼Œå¯¼è‡´æºææ–™è¦†ç›–ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºå°†æ‘˜è¦ä»»åŠ¡åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼Œä»¥æé«˜æºææ–™çš„è¦†ç›–ç‡ï¼Œé‡‡ç”¨DPPé€‰æ‹©å¤šæ ·åŒ–å†…å®¹ã€‚
3. åœ¨DiverseSummåŸºå‡†æµ‹è¯•ä¸­ï¼Œç»“åˆæ–°æ–¹æ³•çš„æ‘˜è¦åœ¨æºè¦†ç›–ç‡ä¸Šæ˜¾è‘—æå‡ï¼Œä¸”èƒ½å¤Ÿç”Ÿæˆä¸ªæ€§åŒ–æ‘˜è¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ–¹é¢æ—¥ç›Šå¼ºå¤§ï¼Œä½†è¿‘æœŸç ”ç©¶è¡¨æ˜å®ƒä»¬åœ¨ä¸åŒä¸Šä¸‹æ–‡éƒ¨åˆ†çš„å…³æ³¨åº¦ä¸å‡ï¼Œå¯¼è‡´åœ¨å¤šæ–‡æ¡£æ‘˜è¦ä¸­æ— æ³•è¦†ç›–å¤šæ ·åŒ–çš„æºææ–™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸåˆ™çš„å†…å®¹é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡å°†æ‘˜è¦ä»»åŠ¡åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆå°†æ–‡æ¡£é›†åˆç®€åŒ–ä¸ºåŸå­å…³é”®ç‚¹ï¼›å…¶æ¬¡ä½¿ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰é€‰æ‹©ä¼˜å…ˆè€ƒè™‘å¤šæ ·åŒ–å†…å®¹çš„å…³é”®ç‚¹ï¼›æœ€åè¿›è¡Œé‡å†™ç”Ÿæˆæœ€ç»ˆæ‘˜è¦ã€‚é€šè¿‡ç»“åˆæå–å’Œé‡å†™çš„æç¤ºæ­¥éª¤ä¸å†…å®¹é€‰æ‹©çš„åŸåˆ™æ€§æŠ€æœ¯ï¼Œæœ¬æ–‡åœ¨DiverseSummåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æºè¦†ç›–ç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†ç”¨æˆ·æ„å›¾çš„ç›¸å…³æ€§çº³å…¥DPPæ ¸ï¼Œç”Ÿæˆçš„æ‘˜è¦èƒ½å¤Ÿè¦†ç›–ç›¸å…³ä¿¡æ¯å¹¶ä¿æŒå¤šæ ·æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ–‡æ¡£æ‘˜è¦ä¸­æºææ–™è¦†ç›–ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨æ³¨æ„åŠ›åˆ†å¸ƒä¸å‡çš„ç°è±¡ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ‘˜è¦ä»»åŠ¡åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼Œé¦–å…ˆæå–å…³é”®ç‚¹ï¼Œç„¶åä½¿ç”¨DPPé€‰æ‹©å¤šæ ·åŒ–å†…å®¹ï¼Œæœ€åè¿›è¡Œé‡å†™ï¼Œä»è€Œæé«˜ä¿¡æ¯è¦†ç›–ç‡å’Œæ‘˜è¦è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ–‡æ¡£é›†åˆç®€åŒ–ä¸ºå…³é”®ç‚¹ï¼›2) ä½¿ç”¨DPPé€‰æ‹©å¤šæ ·åŒ–çš„å…³é”®ç‚¹ï¼›3) é‡å†™ç”Ÿæˆæœ€ç»ˆæ‘˜è¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥DPPè¿›è¡Œå†…å®¹é€‰æ‹©ï¼Œä¼˜å…ˆè€ƒè™‘å¤šæ ·æ€§ï¼Œæ˜¾è‘—æ”¹å–„äº†æºææ–™çš„è¦†ç›–ç‡ï¼Œä¸ä¼ ç»Ÿå•æ­¥æ‘˜è¦æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DPPæ ¸ä¸­èå…¥ç”¨æˆ·æ„å›¾çš„ç›¸å…³æ€§ï¼Œä»¥ç”Ÿæˆä¸ªæ€§åŒ–æ‘˜è¦ï¼Œç¡®ä¿æ‰€ç”Ÿæˆçš„æ‘˜è¦ä¸ä»…å¤šæ ·åŒ–ä¸”ä¸ç”¨æˆ·éœ€æ±‚ç›¸å…³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨DiverseSummåŸºå‡†æµ‹è¯•ä¸­ï¼Œé‡‡ç”¨æ–°æ–¹æ³•çš„æ‘˜è¦åœ¨æºè¦†ç›–ç‡ä¸Šæ˜¾è‘—æé«˜ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå‡å®ç°äº†è¶…è¿‡20%çš„æå‡ã€‚æ­¤å¤–ï¼Œä¸ªæ€§åŒ–æ‘˜è¦çš„ç”Ÿæˆæ•ˆæœä¹Ÿå¾—åˆ°äº†ç”¨æˆ·æ„å›¾çš„æœ‰æ•ˆæ•´åˆï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ‘˜è¦çš„ç›¸å…³æ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å¤šæ–‡æ¡£æ‘˜è¦ç”Ÿæˆé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºæ–°é—»èšåˆã€å­¦æœ¯æ–‡çŒ®ç»¼è¿°å’Œä¸ªæ€§åŒ–å†…å®¹æ¨èç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜æ‘˜è¦çš„å¤šæ ·æ€§å’Œç›¸å…³æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„ä¿¡æ¯éœ€æ±‚ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large language models (LLMs) are increasingly capable of handling longer contexts, recent work has demonstrated that they exhibit the "lost in the middle" phenomenon (Liu et al., 2024) of unevenly attending to different parts of the provided context. This hinders their ability to cover diverse source material in multi-document summarization, as noted in the DiverseSumm benchmark (Huang et al., 2024). In this work, we contend that principled content selection is a simple way to increase source coverage on this task. As opposed to prompting an LLM to perform the summarization in a single step, we explicitly divide the task into three steps -- (1) reducing document collections to atomic key points, (2) using determinantal point processes (DPP) to perform select key points that prioritize diverse content, and (3) rewriting to the final summary. By combining prompting steps, for extraction and rewriting, with principled techniques, for content selection, we consistently improve source coverage on the DiverseSumm benchmark across various LLMs. Finally, we also show that by incorporating relevance to a provided user intent into the DPP kernel, we can generate personalized summaries that cover relevant source information while retaining coverage.

