---
layout: default
title: From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models
---

# From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21935" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21935v2</a>
  <a href="https://arxiv.org/pdf/2505.21935.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21935v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21935v2', 'From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaiyu He, Zhiyu Chen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28 (æ›´æ–°: 2025-08-24)

**å¤‡æ³¨**: TMLR Survey Certification

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å‡è®¾å‘ç°ä¸è§„åˆ™å­¦ä¹ æ–¹æ³•ä»¥æ¨åŠ¨çŸ¥è¯†åˆ›æ–°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å‡è®¾å‘ç°` `çŸ¥è¯†ç”Ÿæˆ` `æ¨ç†èƒ½åŠ›` `äººå·¥é€šç”¨æ™ºèƒ½` `ç§‘å­¦ç ”ç©¶` `æ•°æ®åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦ä¸“æ³¨äºæŒ‡ä»¤æ‰§è¡Œå’Œæ¨ç†ï¼Œç¼ºä¹çœŸæ­£çš„çŸ¥è¯†å‘ç°èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºäº†åŸºäºçš®å°”å£«æ¡†æ¶çš„å‡è®¾å‘ç°æ–¹æ³•ï¼Œæ—¨åœ¨æ¨åŠ¨LLMçš„çŸ¥è¯†ç”Ÿæˆèƒ½åŠ›ã€‚
3. é€šè¿‡ç»¼åˆç°æœ‰ç ”ç©¶ï¼Œè¯†åˆ«å‡ºLLMåœ¨å‡è®¾ç”Ÿæˆå’ŒéªŒè¯æ–¹é¢çš„æˆå°±ä¸ä¸è¶³ï¼ŒæŒ‡æ˜æœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é—®ä¸–ä»¥æ¥ï¼Œç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æå‡å…¶æŒ‡ä»¤æ‰§è¡Œå’Œæ¨ç†èƒ½åŠ›ä¸Šï¼Œè€Œå¯¹å…¶èƒ½å¦çœŸæ­£å‘ç°æ–°çŸ¥è¯†çš„é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚ä¸ºäº†å®ç°äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰ï¼Œéœ€è¦ä¸ä»…èƒ½æ‰§è¡Œå‘½ä»¤æˆ–æ£€ç´¢ä¿¡æ¯çš„æ¨¡å‹ï¼Œè¿˜éœ€å…·å¤‡å­¦ä¹ ã€æ¨ç†å’Œç”Ÿæˆæ–°çŸ¥è¯†çš„èƒ½åŠ›ã€‚æœ¬æ–‡åŸºäºçš®å°”å£«çš„æ¼”ç»ã€å½’çº³å’Œæº¯å› æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°å®¡è§†äº†åŸºäºLLMçš„å‡è®¾å‘ç°ï¼Œç»¼åˆäº†ç°æœ‰çš„å‡è®¾ç”Ÿæˆã€åº”ç”¨å’ŒéªŒè¯çš„ç ”ç©¶ï¼Œè¯†åˆ«å‡ºå…³é”®æˆå°±å’Œé‡è¦ç¼ºå£ï¼Œæ­ç¤ºäº†LLMå¦‚ä½•ä»å•çº¯çš„ä¿¡æ¯æ‰§è¡Œè€…è½¬å˜ä¸ºçœŸæ­£çš„åˆ›æ–°å¼•æ“ï¼Œå¯èƒ½ä¼šæ”¹å˜ç ”ç©¶ã€ç§‘å­¦å’Œç°å®é—®é¢˜è§£å†³çš„æ–¹å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å‘ç°æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºä¿¡æ¯æ‰§è¡Œï¼Œç¼ºä¹ç”Ÿæˆæ–°çŸ¥è¯†çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥çš®å°”å£«çš„æ¼”ç»ã€å½’çº³å’Œæº¯å› æ¡†æ¶ï¼Œæ¥ç³»ç»ŸåŒ–LLMçš„å‡è®¾å‘ç°è¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå’ŒéªŒè¯æ–°çŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å‡è®¾ç”Ÿæˆã€åº”ç”¨å’ŒéªŒè¯ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯çš„çŸ¥è¯†å‘ç°æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ä¼ ç»Ÿçš„æ¨ç†æ–¹æ³•ä¸ç°ä»£LLMç»“åˆï¼Œå½¢æˆä¸€ç§æ–°çš„çŸ¥è¯†ç”Ÿæˆæœºåˆ¶ï¼Œçªç ´äº†ä»¥å¾€æ¨¡å‹çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–å‡è®¾ç”Ÿæˆçš„è´¨é‡å’Œæœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ç¡®ä¿ç”Ÿæˆçš„å‡è®¾èƒ½å¤Ÿç»è¿‡éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ–°æ–¹æ³•çš„LLMåœ¨å‡è®¾ç”Ÿæˆçš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ä¸Šæ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå‡è®¾çš„è´¨é‡æé«˜äº†çº¦30%ï¼ŒéªŒè¯æˆåŠŸç‡æå‡äº†20%ã€‚è¿™äº›ç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨çŸ¥è¯†å‘ç°é¢†åŸŸçš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§‘å­¦ç ”ç©¶ã€æ•°æ®åˆ†æå’Œæ™ºèƒ½å†³ç­–ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œä¼ä¸šåœ¨å¤æ‚é—®é¢˜ä¸­å‘ç°æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨çŸ¥è¯†çš„åˆ›æ–°ä¸å‘å±•ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒLLMåœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨å°†æ›´åŠ å¹¿æ³›ï¼Œå¯èƒ½ä¼šå¼•é¢†æ–°çš„ç ”ç©¶æ½®æµã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world. Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery. We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps. By unifying these threads, we illuminate how LLMs might evolve from mere ``information executors'' into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.

