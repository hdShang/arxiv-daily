---
layout: default
title: EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse
---

# EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21889" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21889v2</a>
  <a href="https://arxiv.org/pdf/2505.21889.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21889v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21889v2', 'EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28 (æ›´æ–°: 2025-05-29)

**å¤‡æ³¨**: 31st International European Conference on Parallel and Distributed Computing (Euro-Par 2025 Oral)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/gty111/EFIM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEFIMä»¥è§£å†³LLMsåœ¨å¡«å……ä»»åŠ¡ä¸­çš„KVç¼“å­˜é‡ç”¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `KVç¼“å­˜é‡ç”¨` `å¡«å……ä»»åŠ¡` `ç‰‡æ®µæ ‡è®°åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `å®æ—¶äº¤äº’æœåŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜é‡ç”¨æ–¹æ³•åœ¨å¡«å……ä»»åŠ¡ä¸­å—åˆ°æç¤ºæ ¼å¼ç»“æ„çš„é™åˆ¶ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºEFIMï¼Œé€šè¿‡è½¬åŒ–æç¤ºæ ¼å¼æ¥ä¼˜åŒ–KVç¼“å­˜é‡ç”¨ï¼ŒåŒæ—¶å¼•å…¥ç‰‡æ®µæ ‡è®°åŒ–è®­ç»ƒæ–¹æ³•è§£å†³ç”Ÿæˆéƒ¨åˆ†å•è¯çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEFIMæ˜¾è‘—é™ä½äº†52%çš„å»¶è¿Ÿï¼Œå¹¶æé«˜äº†98%çš„ååé‡ï¼Œä¿æŒäº†å¡«å……èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸ç”¨äºå¡«å……ä»»åŠ¡ï¼Œå³é¢„æµ‹æˆ–ç”Ÿæˆç»™å®šæ–‡æœ¬ä¸­çš„ç¼ºå¤±ä¿¡æ¯ã€‚è¿™äº›ä»»åŠ¡é€šå¸¸éœ€è¦å¤šæ¬¡ä¸ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„äº¤äº’ã€‚ä¸ºäº†å‡å°‘é‡å¤å†å²æ ‡è®°çš„è®¡ç®—ï¼Œè·¨è¯·æ±‚çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜é‡ç”¨æˆä¸ºå¤šè½®äº¤äº’æœåŠ¡ä¸­çš„å…³é”®æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨å¡«å……ä»»åŠ¡ä¸­ï¼ŒKVç¼“å­˜é‡ç”¨å¸¸å› æç¤ºæ ¼å¼çš„ç»“æ„è€Œå—é˜»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†EFIMï¼Œä¸€ç§è½¬åŒ–çš„æç¤ºæ ¼å¼ï¼Œæ—¨åœ¨é‡Šæ”¾KVç¼“å­˜é‡ç”¨çš„æ€§èƒ½æ½œåŠ›ã€‚å°½ç®¡è½¬åŒ–çš„æç¤ºå¯ä»¥è§£å†³æ•ˆç‡é—®é¢˜ï¼Œä½†ä¹Ÿæš´éœ²äº†å½“å‰LLMsåœ¨ç”Ÿæˆéƒ¨åˆ†å•è¯æ—¶çš„å›°éš¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç‰‡æ®µæ ‡è®°åŒ–è®­ç»ƒæ–¹æ³•ï¼Œåœ¨æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å°†æ–‡æœ¬åˆ†å‰²ä¸ºå¤šä¸ªç‰‡æ®µã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨EFIMçš„LLMæœåŠ¡å¯ä»¥å°†å»¶è¿Ÿé™ä½52%ï¼Œå¹¶æé«˜ååé‡98%ï¼ŒåŒæ—¶ä¿æŒåŸæœ‰çš„å¡«å……èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¡«å……ä»»åŠ¡ä¸­KVç¼“å­˜é‡ç”¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æç¤ºæ ¼å¼çš„ç»“æ„ä¸Šå­˜åœ¨å±€é™ï¼Œå¯¼è‡´ç¼“å­˜æ— æ•ˆåŒ–ï¼Œå½±å“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºEFIMï¼Œé€šè¿‡è½¬åŒ–æç¤ºæ ¼å¼æ¥ä¼˜åŒ–KVç¼“å­˜çš„é‡ç”¨ï¼Œè¿›è€Œæé«˜å¤šè½®äº¤äº’æœåŠ¡çš„æ•ˆç‡ã€‚åŒæ—¶ï¼Œé’ˆå¯¹å½“å‰LLMsåœ¨ç”Ÿæˆéƒ¨åˆ†å•è¯æ—¶çš„å›°éš¾ï¼Œå¼•å…¥ç‰‡æ®µæ ‡è®°åŒ–è®­ç»ƒæ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè½¬åŒ–æç¤ºæ ¼å¼å’Œç‰‡æ®µæ ‡è®°åŒ–è®­ç»ƒã€‚è½¬åŒ–æç¤ºæ ¼å¼æ—¨åœ¨ä¼˜åŒ–KVç¼“å­˜çš„ä½¿ç”¨ï¼Œè€Œç‰‡æ®µæ ‡è®°åŒ–è®­ç»ƒåˆ™åœ¨æ•°æ®å¤„ç†é˜¶æ®µå°†æ–‡æœ¬åˆ†å‰²ä¸ºå¤šä¸ªç‰‡æ®µï¼Œä»¥æé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºEFIMçš„æç¤ºæ ¼å¼è½¬åŒ–ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé‡Šæ”¾KVç¼“å­˜é‡ç”¨çš„æ½œåŠ›ï¼ŒåŒæ—¶è§£å†³äº†å½“å‰LLMsåœ¨ç”Ÿæˆéƒ¨åˆ†å•è¯æ—¶çš„ä¸è¶³ã€‚è¿™ä¸€åˆ›æ–°ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶ç»“æ„ä¼˜åŒ–å’Œè®­ç»ƒæ–¹æ³•çš„ç»“åˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‰‡æ®µæ ‡è®°åŒ–è®­ç»ƒä¸­ï¼Œæ–‡æœ¬è¢«åˆ†å‰²ä¸ºå¤šä¸ªç‰‡æ®µè¿›è¡Œå¤„ç†ï¼Œç¡®ä¿ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”æ–°çš„æç¤ºæ ¼å¼å’Œè®­ç»ƒæ–¹æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨EFIMçš„LLMæœåŠ¡åœ¨å»¶è¿Ÿæ–¹é¢é™ä½äº†52%ï¼Œååé‡æé«˜äº†98%ã€‚è¿™äº›æ˜¾è‘—çš„æ€§èƒ½æå‡å±•ç¤ºäº†EFIMåœ¨å¡«å……ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¿æŒäº†åŸæœ‰çš„å¡«å……èƒ½åŠ›ï¼Œå…·æœ‰è¾ƒå¼ºçš„ç«äº‰ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿä»¥åŠæ™ºèƒ½åŠ©æ‰‹ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜LLMsåœ¨å¡«å……ä»»åŠ¡ä¸­çš„æ•ˆç‡ï¼ŒEFIMèƒ½å¤Ÿä¸ºå®æ—¶äº¤äº’æœåŠ¡æä¾›æ›´å¥½çš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are often used for infilling tasks, which involve predicting or generating missing information in a given text. These tasks typically require multiple interactions with similar context. To reduce the computation of repeated historical tokens, cross-request key-value (KV) cache reuse, a technique that stores and reuses intermediate computations, has become a crucial method in multi-round interactive services. However, in infilling tasks, the KV cache reuse is often hindered by the structure of the prompt format, which typically consists of a prefix and suffix relative to the insertion point. Specifically, the KV cache of the prefix or suffix part is frequently invalidated as the other part (suffix or prefix) is incrementally generated. To address the issue, we propose EFIM, a transformed prompt format of FIM to unleash the performance potential of KV cache reuse. Although the transformed prompt can solve the inefficiency, it exposes subtoken generation problems in current LLMs, where they have difficulty generating partial words accurately. Therefore, we introduce a fragment tokenization training method which splits text into multiple fragments before tokenization during data processing. Experiments on two representative LLMs show that LLM serving with EFIM can lower the latency by 52% and improve the throughput by 98% while maintaining the original infilling capability. EFIM's source code is publicly available at https://github.com/gty111/EFIM.

