---
layout: default
title: "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix"
---

# MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13032" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.13032v1</a>
  <a href="https://arxiv.org/pdf/2505.13032.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13032v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13032v1', 'MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe Chen, Zhuo Chen, Jian Cong, Kai Li, Keliang Li, Siyou Li, Xinfeng Li, Xiquan Li, Zheng Lian, Yuzhe Liang, Minghao Liu, Zhikang Niu, Tianrui Wang, Yuping Wang, Yuxuan Wang, Yihao Wu, Guanrou Yang, Jianwei Yu, Ruibin Yuan, Zhisheng Zheng, Ziya Zhou, Haina Zhu, Wei Xue, Emmanouil Benetos, Kai Yu, Eng-Siong Chng, Xie Chen

**ÂàÜÁ±ª**: cs.SD, cs.CL, cs.MM, eess.AS

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-19

**Â§áÊ≥®**: Open-source at https://github.com/ddlBoJack/MMAR

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MMARÂü∫ÂáÜ‰ª•ËØÑ‰º∞Èü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ∑±Â∫¶Êé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èü≥È¢ëÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê∑±Â∫¶Â≠¶‰π†` `Èü≥È¢ëËØ≠Ë®ÄÊ®°Âûã` `Âü∫ÂáÜËØÑ‰º∞` `ÈìæÂºèÊÄùÁª¥` `Â§çÊùÇÈóÆÈ¢ò`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫ÂáÜÈÄöÂ∏∏Â±ÄÈôê‰∫éÁâπÂÆöÈü≥È¢ëÈ¢ÜÂüüÔºåÁº∫‰πèÂØπÂ§öÊ®°ÊÄÅÈü≥È¢ëÊé®ÁêÜÁöÑÂÖ®Èù¢ËØÑ‰º∞„ÄÇ
2. MMARÈÄöËøáÊèê‰æõÂ§öËææ1000‰∏™Èü≥È¢ë-ÈóÆÈ¢ò-Á≠îÊ°à‰∏âÂÖÉÁªÑÔºåÊ∂µÁõñÂ§öÂ±ÇÊ¨°Êé®ÁêÜÔºå‰øÉËøõÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ†îÁ©∂„ÄÇ
3. Âú®Â§öÁßçÊ®°Âûã‰∏äËØÑ‰º∞MMARÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÊ®°ÂûãÂú®ÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ‰∏äÁöÑÂÖ≥ÈîÆÂ±ÄÈôêÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êàë‰ª¨‰ªãÁªç‰∫ÜMMARÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Èü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÔºàALMsÔºâÂú®Â§öÂ≠¶Áßë‰ªªÂä°‰∏≠ÁöÑÊ∑±Â∫¶Êé®ÁêÜËÉΩÂäõ„ÄÇMMARÂåÖÂê´1000‰∏™Á≤æÂøÉÁ≠ñÂàíÁöÑÈü≥È¢ë-ÈóÆÈ¢ò-Á≠îÊ°à‰∏âÂÖÉÁªÑÔºåÊî∂ÈõÜËá™ÁúüÂÆû‰∫íËÅîÁΩëËßÜÈ¢ëÔºåÂπ∂ÈÄöËøáËø≠‰ª£ÈîôËØØ‰øÆÊ≠£ÂíåË¥®ÈáèÊ£ÄÊü•ËøõË°å‰ºòÂåñÔºå‰ª•Á°Æ‰øùÈ´òË¥®Èáè„ÄÇ‰∏éÁé∞ÊúâÁöÑ‰ªÖÈôê‰∫éÁâπÂÆöÂ£∞Èü≥„ÄÅÈü≥‰πêÊàñËØ≠Èü≥È¢ÜÂüüÁöÑÂü∫ÂáÜ‰∏çÂêåÔºåMMARÊâ©Â±ïÂà∞ÂπøÊ≥õÁöÑÁé∞ÂÆûÈü≥È¢ëÂú∫ÊôØÔºåÂåÖÊã¨Â£∞Èü≥„ÄÅÈü≥‰πêÂíåËØ≠Èü≥ÁöÑÊ∑∑ÂêàÊ®°ÊÄÅÁªÑÂêà„ÄÇÊØè‰∏™ÈóÆÈ¢òÂú®Âõõ‰∏™Êé®ÁêÜÂ±ÇÊ¨°‰∏äËøõË°åÂàÜÂ±ÇÂàÜÁ±ªÔºö‰ø°Âè∑„ÄÅÊÑüÁü•„ÄÅËØ≠‰πâÂíåÊñáÂåñÔºåÂπ∂Âú®ÊØè‰∏™Â±ÇÊ¨°ÂÜÖÂ¢ûÂä†Â≠êÁ±ªÂà´Ôºå‰ª•ÂèçÊò†‰ªªÂä°ÁöÑÂ§öÊ†∑ÊÄßÂíåÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨‰∏∫ÊØè‰∏™ÈóÆÈ¢òÊ≥®Èáä‰∫ÜÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊé®ÁêÜÔºå‰ª•‰øÉËøõÊú™Êù•Èü≥È¢ëÊé®ÁêÜÁöÑËøõÂ±ï„ÄÇÊØè‰∏™Âü∫ÂáÜÈ°πÈÉΩË¶ÅÊ±ÇÂ§öÊ≠•È™§ÁöÑÊ∑±Â∫¶Êé®ÁêÜÔºåË∂ÖË∂äË°®Èù¢ÁêÜËß£ÔºåÈÉ®ÂàÜÈóÆÈ¢òÈúÄË¶ÅÁ†îÁ©∂ÁîüÁ∫ßÂà´ÁöÑÊÑüÁü•ÂíåÈ¢ÜÂüüÁâπÂÆöÁü•ËØÜÔºåÊèêÂçá‰∫ÜÂü∫ÂáÜÁöÑÈöæÂ∫¶ÂíåÊ∑±Â∫¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõËØÑ‰º∞‰∏≠ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÁº∫‰πèÂØπÂ§çÊùÇÈü≥È¢ëÂú∫ÊôØÁöÑÂÖ®Èù¢ËÄÉÈáè„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÂ±ÄÈôê‰∫éÁâπÂÆöÈ¢ÜÂüüÔºåÊó†Ê≥ïÊúâÊïàËØÑ‰º∞Ê®°ÂûãÁöÑÊ∑±Â∫¶Êé®ÁêÜËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫MMARÂü∫ÂáÜÔºåÈÄöËøáÊûÑÂª∫Â§öÂ±ÇÊ¨°ÁöÑÈü≥È¢ë-ÈóÆÈ¢ò-Á≠îÊ°à‰∏âÂÖÉÁªÑÔºå‰øÉËøõÂØπÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ∑±Â∫¶Êé®ÁêÜËÉΩÂäõÁöÑËØÑ‰º∞„ÄÇËÆæËÆ°Êó∂ËÄÉËôë‰∫ÜÂ§öÊ®°ÊÄÅÈü≥È¢ëÁöÑÂ§çÊùÇÊÄßÔºåÁ°Æ‰øùÈóÆÈ¢òÁöÑÂ§öÊ†∑ÊÄßÂíåÊåëÊàòÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMMARÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Âõõ‰∏™Êé®ÁêÜÂ±ÇÊ¨°Ôºö‰ø°Âè∑„ÄÅÊÑüÁü•„ÄÅËØ≠‰πâÂíåÊñáÂåñ„ÄÇÊØè‰∏™Â±ÇÊ¨°‰∏ãÂèàÁªÜÂàÜ‰∏∫Â§ö‰∏™Â≠êÁ±ªÂà´Ôºå‰ª•ÂèçÊò†‰ªªÂä°ÁöÑÂ§öÊ†∑ÊÄß„ÄÇÊØè‰∏™ÈóÆÈ¢òÈÉΩÈôÑÊúâÈìæÂºèÊÄùÁª¥Êé®ÁêÜÔºå‰øÉËøõÊ®°ÂûãÁöÑÊ∑±Â∫¶ÁêÜËß£„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMMARÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Â§öÂ±ÇÊ¨°ÁöÑÊé®ÁêÜÁªìÊûÑÂíåÈìæÂºèÊÄùÁª¥Êé®ÁêÜÁöÑÂºïÂÖ•Ôºå‰ΩøÂæóÊØè‰∏™ÈóÆÈ¢ò‰∏ç‰ªÖË¶ÅÊ±ÇË°®Èù¢ÁêÜËß£ÔºåËøòÈúÄË¶ÅÊ∑±Â∫¶ÁöÑÂ§öÊ≠•È™§Êé®ÁêÜ„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÂçï‰∏ÄÈ¢ÜÂüüËØÑ‰º∞ÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈóÆÈ¢òÁöÑÈÄâÊã©ÁªèËøá‰∏•Ê†ºÁöÑË¥®ÈáèÊ£ÄÊü•ÔºåÁ°Æ‰øùÂÖ∂Âú®Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÊØè‰∏™ÈóÆÈ¢òÁöÑÈöæÂ∫¶ÂíåÂ§çÊùÇÊÄßÁªèËøáÁ≤æÂøÉËÆæËÆ°ÔºåÈÉ®ÂàÜÈóÆÈ¢òÈúÄË¶ÅÁ†îÁ©∂ÁîüÁ∫ßÂà´ÁöÑÁü•ËØÜÔºå‰ª•ÊèêÂçáÂü∫ÂáÜÁöÑÊåëÊàòÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂØπÂ§öÁßçÂ§ßÂûãÊ®°ÂûãÁöÑËØÑ‰º∞‰∏≠ÔºåMMARÊòæÁ§∫Âá∫ÂÖ∂ÊåëÊàòÊÄßÔºåÊè≠Á§∫‰∫ÜÂΩìÂâçÊ®°ÂûãÂú®Ê∑±Â∫¶ÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ‰∏äÁöÑÂÖ≥ÈîÆÂ±ÄÈôêÊÄß„ÄÇÂÖ∑‰ΩìÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÉ®ÂàÜÊ®°ÂûãÂú®Â§çÊùÇÈóÆÈ¢ò‰∏äÁöÑË°®Áé∞ÊòæËëó‰Ωé‰∫éÈ¢ÑÊúüÔºåÂº∫Ë∞É‰∫ÜËØ•Âü∫ÂáÜÁöÑÂøÖË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÊô∫ËÉΩÂä©Êâã„ÄÅÈü≥È¢ëÂÜÖÂÆπÂàÜÊûêÁ≠â„ÄÇMMARÂü∫ÂáÜÁöÑÂª∫Á´ãÂ∞ÜÊé®Âä®Èü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜÊñπÈù¢ÁöÑÁ†îÁ©∂Ôºå‰øÉËøõÁõ∏ÂÖ≥ÊäÄÊúØÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèëÂ±ïÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We introduce MMAR, a new benchmark designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. MMAR comprises 1,000 meticulously curated audio-question-answer triplets, collected from real-world internet videos and refined through iterative error corrections and quality checks to ensure high quality. Unlike existing benchmarks that are limited to specific domains of sound, music, or speech, MMAR extends them to a broad spectrum of real-world audio scenarios, including mixed-modality combinations of sound, music, and speech. Each question in MMAR is hierarchically categorized across four reasoning layers: Signal, Perception, Semantic, and Cultural, with additional sub-categories within each layer to reflect task diversity and complexity. To further foster research in this area, we annotate every question with a Chain-of-Thought (CoT) rationale to promote future advancements in audio reasoning. Each item in the benchmark demands multi-step deep reasoning beyond surface-level understanding. Moreover, a part of the questions requires graduate-level perceptual and domain-specific knowledge, elevating the benchmark's difficulty and depth. We evaluate MMAR using a broad set of models, including Large Audio-Language Models (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models (OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with audio caption inputs. The performance of these models on MMAR highlights the benchmark's challenging nature, and our analysis further reveals critical limitations of understanding and reasoning capabilities among current models. We hope MMAR will serve as a catalyst for future advances in this important but little-explored area.

