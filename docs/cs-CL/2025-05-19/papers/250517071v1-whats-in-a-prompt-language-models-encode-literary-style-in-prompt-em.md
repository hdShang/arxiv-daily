---
layout: default
title: What's in a prompt? Language models encode literary style in prompt embeddings
---

# What's in a prompt? Language models encode literary style in prompt embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.17071" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.17071v1</a>
  <a href="https://arxiv.org/pdf/2505.17071.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.17071v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.17071v1', 'What\'s in a prompt? Language models encode literary style in prompt embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: RaphaÃ«l Sarfati, Haley Moller, Toni J. B. Liu, Nicolas BoullÃ©, Christopher Earls

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡æç¤ºåµŒå…¥åˆ†ææ–‡å­¦é£æ ¼çš„è¯­è¨€æ¨¡å‹æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `æç¤ºåµŒå…¥` `æ–‡å­¦é£æ ¼` `æ·±å±‚è¡¨ç¤º` `ä½œè€…å½’å±` `æ½œåœ¨ç©ºé—´` `ä¿¡æ¯å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•è¯çš„æ¦‚å¿µå†…å®¹ä¸å…¶å‘é‡è¡¨ç¤ºä¹‹é—´çš„å…³ç³»ï¼Œç¼ºä¹å¯¹æ•´ä¸ªæç¤ºä¿¡æ¯çš„æ·±å…¥åˆ†æã€‚
2. æœ¬æ–‡é€šè¿‡åˆ†ææ–‡å­¦ä½œå“ï¼Œæå‡ºäº†æç¤ºåµŒå…¥ä¸­åŒ…å«éå…·ä½“ä¿¡æ¯çš„è§‚ç‚¹ï¼Œæ­ç¤ºäº†æ·±å±‚è¡¨ç¤ºçš„å¤æ‚æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŒä½œè€…çš„ä½œå“åœ¨æ½œåœ¨ç©ºé—´ä¸­å‘ˆç°å‡ºæ˜æ˜¾çš„é£æ ¼ç‰¹å¾ï¼Œä¸ºä½œè€…å½’å±å’Œæ–‡å­¦åˆ†ææä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨é«˜ç»´æ½œåœ¨ç©ºé—´æ¥ç¼–ç å’Œå¤„ç†æ–‡æœ¬ä¿¡æ¯ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶æ¢è®¨äº†å•è¯çš„æ¦‚å¿µå†…å®¹å¦‚ä½•è½¬åŒ–ä¸ºå…¶å‘é‡è¡¨ç¤ºä¹‹é—´çš„å‡ ä½•å…³ç³»ï¼Œä½†å¯¹æ•´ä¸ªæç¤ºä¿¡æ¯å¦‚ä½•åœ¨å˜æ¢å±‚çš„ä½œç”¨ä¸‹è¢«æµ“ç¼©ä¸ºå•ä¸ªåµŒå…¥çš„åˆ†æç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡é€šè¿‡æ–‡å­¦ä½œå“å±•ç¤ºäº†æç¤ºçš„éå…·ä½“ã€æŠ½è±¡æ–¹é¢çš„ä¿¡æ¯å¦‚ä½•åœ¨æ·±å±‚è¡¨ç¤ºä¸­è¢«ç¼–ç ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä¸åŒå°è¯´çš„çŸ­æ‘˜å½•åœ¨æ½œåœ¨ç©ºé—´ä¸­ç‹¬ç«‹äºå…¶é¢„æµ‹çš„ä¸‹ä¸€ä¸ªæ ‡è®°è€Œåˆ†ç¦»ï¼Œä¸”åŒä¸€ä½œè€…çš„ä¹¦ç±åµŒå…¥æ¯”ä¸åŒä½œè€…çš„ä¹¦ç±æ›´ä¸ºçº ç¼ ï¼Œè¡¨æ˜åµŒå…¥ç¼–ç äº†é£æ ¼ç‰¹å¾ã€‚è¿™ç§é£æ ¼å‡ ä½•çš„å‘ç°å¯èƒ½åœ¨ä½œè€…å½’å±å’Œæ–‡å­¦åˆ†æä¸­å…·æœ‰åº”ç”¨ä»·å€¼ï¼ŒåŒæ—¶æ­ç¤ºäº†è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯å¤„ç†å’Œå‹ç¼©æ–¹é¢çš„å¤æ‚æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•åœ¨æç¤ºåµŒå…¥ä¸­ç¼–ç æ–‡å­¦é£æ ¼ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ†ææç¤ºä¿¡æ¯çš„æ·±å±‚æ¬¡ç‰¹å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä½¿ç”¨æ–‡å­¦ä½œå“çš„çŸ­æ‘˜å½•ï¼Œç ”ç©¶æç¤ºçš„éå…·ä½“ä¿¡æ¯å¦‚ä½•åœ¨æ·±å±‚è¡¨ç¤ºä¸­è¢«ç¼–ç ï¼Œæ­ç¤ºå…¶åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„å‡ ä½•ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å˜æ¢å™¨æ¨¡å‹å¯¹çŸ­æ‘˜å½•è¿›è¡Œå¤„ç†ï¼Œåˆ†æå…¶åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„åˆ†å¸ƒæƒ…å†µï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ–‡æœ¬ç¼–ç ã€åµŒå…¥ç”Ÿæˆå’Œæ½œåœ¨ç©ºé—´åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°åˆ†æäº†æç¤ºåµŒå…¥ä¸­é£æ ¼ä¿¡æ¯çš„å‡ ä½•ç‰¹å¾ï¼Œæ­ç¤ºäº†ä¸åŒä½œè€…ä½œå“ä¹‹é—´çš„é£æ ¼çº ç¼ ç°è±¡ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†10åˆ°100ä¸ªæ ‡è®°çš„çŸ­æ‘˜å½•ï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„å˜æ¢å™¨æ¶æ„ï¼Œé‡ç‚¹å…³æ³¨åµŒå…¥çš„ç›¸ä¼¼æ€§å’Œåˆ†å¸ƒç‰¹å¾ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒä½œè€…çš„ä½œå“ï¼Œåˆ†æå…¶åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŒä½œè€…çš„çŸ­æ‘˜å½•åœ¨æ½œåœ¨ç©ºé—´ä¸­å‘ˆç°å‡ºæ˜¾è‘—çš„åˆ†ç¦»ç°è±¡ï¼Œä¸”åŒä¸€ä½œè€…çš„ä½œå“åµŒå…¥æ¯”ä¸åŒä½œè€…çš„ä½œå“æ›´ä¸ºçº ç¼ ã€‚è¿™ä¸€å‘ç°ä¸ä»…éªŒè¯äº†åµŒå…¥ä¸­é£æ ¼ç‰¹å¾çš„å­˜åœ¨ï¼Œä¹Ÿä¸ºä½œè€…å½’å±æä¾›äº†æ–°çš„å®è¯æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä½œè€…å½’å±åˆ†æå’Œæ–‡å­¦ä½œå“çš„é£æ ¼ç ”ç©¶ã€‚é€šè¿‡å¯¹æç¤ºåµŒå…¥çš„æ·±å…¥åˆ†æï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£æ–‡æœ¬çš„é£æ ¼ç‰¹å¾ï¼Œå¹¶ä¸ºæ–‡å­¦æ‰¹è¯„æä¾›æ–°çš„å·¥å…·å’Œè§†è§’ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ–‡æœ¬ç”Ÿæˆå’Œåˆ›ä½œè¾…åŠ©å·¥å…·ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models use high-dimensional latent spaces to encode and process textual information. Much work has investigated how the conceptual content of words translates into geometrical relationships between their vector representations. Fewer studies analyze how the cumulative information of an entire prompt becomes condensed into individual embeddings under the action of transformer layers. We use literary pieces to show that information about intangible, rather than factual, aspects of the prompt are contained in deep representations. We observe that short excerpts (10 - 100 tokens) from different novels separate in the latent space independently from what next-token prediction they converge towards. Ensembles from books from the same authors are much more entangled than across authors, suggesting that embeddings encode stylistic features. This geometry of style may have applications for authorship attribution and literary analysis, but most importantly reveals the sophistication of information processing and compression accomplished by language models.

