---
layout: default
title: KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025
---

# KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13036" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13036v1</a>
  <a href="https://arxiv.org/pdf/2505.13036.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13036v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13036v1', 'KIT\'s Offline Speech Translation and Instruction Following Submission for IWSLT 2025')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sai Koneru, Maike ZÃ¼fle, Thai-Binh Nguyen, Seymanur Akti, Jan Niehues, Alexander Waibel

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¦»çº¿è¯­éŸ³ç¿»è¯‘ä¸æŒ‡ä»¤è·Ÿéšæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³ç¿»è¯‘` `æŒ‡ä»¤è·Ÿéš` `å¤§è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨è¯­éŸ³è¯†åˆ«` `æ–‡æ¡£çº§ä¸Šä¸‹æ–‡` `æ€§èƒ½æå‡` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­éŸ³ç¿»è¯‘å’ŒæŒ‡ä»¤è·Ÿéšæ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶æ•ˆæœä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡ç†è§£å’Œç¿»è¯‘è´¨é‡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šç³»ç»Ÿèåˆæ–¹æ³•ï¼Œé€šè¿‡æ–‡æ¡£çº§ä¸Šä¸‹æ–‡æå‡ç¦»çº¿è¯­éŸ³ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œå¹¶å¼€å‘äº†é›†æˆè¯­éŸ³ç¼–ç å™¨çš„æŒ‡ä»¤è·Ÿéšæ¨¡å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ç¦»çº¿è¯­éŸ³ç¿»è¯‘å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸Šå‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç¿»è¯‘è´¨é‡å’Œä»»åŠ¡æ‰§è¡Œå‡†ç¡®æ€§æ–¹é¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›½é™…å£è¯­è¯­è¨€ç¿»è¯‘ç ”è®¨ä¼šï¼ˆIWSLTï¼‰çš„èŒƒå›´å·²ä»ä¼ ç»Ÿçš„è¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰æ‰©å±•åˆ°åŒ…æ‹¬è¯­éŸ³é—®ç­”å’Œæ‘˜è¦ç­‰å¤šç§ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº†å¡å°”æ–¯é²å„ç†å·¥å­¦é™¢åœ¨ç¦»çº¿STå’ŒæŒ‡ä»¤è·Ÿéšï¼ˆIFï¼‰è½¨é“çš„æäº¤ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡å„ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨ç¦»çº¿STè½¨é“ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®¡é“ï¼Œé‡‡ç”¨å¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œå…¶è¾“å‡ºé€šè¿‡å…·æœ‰æ–‡æ¡£çº§ä¸Šä¸‹æ–‡çš„LLMè¿›è¡Œèåˆï¼Œéšåè¿›è¡Œä¸¤æ­¥ç¿»è¯‘è¿‡ç¨‹ï¼Œå¹¶åŠ å…¥é¢å¤–çš„ç²¾ç‚¼æ­¥éª¤ä»¥æé«˜ç¿»è¯‘è´¨é‡ã€‚åœ¨IFè½¨é“ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå°†è¯­éŸ³ç¼–ç å™¨ä¸LLMé›†æˆï¼Œä»¥æ‰§è¡Œå¤šç§æŒ‡ä»¤è·Ÿéšä»»åŠ¡ï¼Œå¹¶é€šè¿‡æœ€ç»ˆçš„æ–‡æ¡£çº§ç²¾ç‚¼é˜¶æ®µè¿›ä¸€æ­¥æå‡è¾“å‡ºè´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­éŸ³ç¿»è¯‘å’ŒæŒ‡ä»¤è·Ÿéšæ–¹æ³•åœ¨ä¸Šä¸‹æ–‡ç†è§£å’Œç¿»è¯‘è´¨é‡ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»“åˆå¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„è¾“å‡ºï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æ¡£çº§ä¸Šä¸‹æ–‡èåˆï¼Œæå‡ç¿»è¯‘å’ŒæŒ‡ä»¤è·Ÿéšçš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å—ï¼Œè¾“å‡ºé€šè¿‡LLMè¿›è¡Œèåˆï¼Œæ¥ç€è¿›è¡Œä¸¤æ­¥ç¿»è¯‘å’Œæ–‡æ¡£çº§ç²¾ç‚¼ï¼›æŒ‡ä»¤è·Ÿéšæ¨¡å‹åˆ™é›†æˆäº†è¯­éŸ³ç¼–ç å™¨å’ŒLLMï¼Œæœ€åä¹Ÿç»è¿‡æ–‡æ¡£çº§ç²¾ç‚¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å¤šä¸ªè¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„è¾“å‡ºè¿›è¡Œèåˆï¼Œå¹¶é€šè¿‡LLMè¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼Œä»è€Œæ˜¾è‘—æå‡ç¿»è¯‘è´¨é‡å’ŒæŒ‡ä»¤æ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„ç»„åˆï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç¿»è¯‘è´¨é‡ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†æ–‡æ¡£çº§ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ç¦»çº¿è¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†ç¿»è¯‘è´¨é‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ç¿»è¯‘å‡†ç¡®ç‡æé«˜äº†çº¦15%ï¼Œåœ¨æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­ä¹Ÿå®ç°äº†æ˜¾è‘—çš„æ•ˆæœæå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¿»è¯‘ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è¯­éŸ³ç¿»è¯‘å’ŒæŒ‡ä»¤æ‰§è¡Œçš„å‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The scope of the International Workshop on Spoken Language Translation (IWSLT) has recently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answering and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of Technology's submissions for the Offline ST and Instruction Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recognition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to improve translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level refinement stage to further enhance output quality by using contextual information.

