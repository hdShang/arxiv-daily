---
layout: default
title: Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning
---

# Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13115" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13115v1</a>
  <a href="https://arxiv.org/pdf/2505.13115.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13115v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13115v1', 'Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**å¤‡æ³¨**: Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTREAæ•°æ®é›†ä»¥è¯„ä¼°éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘è¯­è¨€æ¨¡å‹` `æ—¶é—´æ¨ç†` `å¤šæ¨¡æ€è¯„ä¼°` `ä¸ç¡®å®šæ€§åº¦é‡` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸å¦‚äººç±»ï¼Œç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ ‡å‡†ã€‚
2. æœ¬æ–‡æå‡ºäº†éŸ³é¢‘çš„æ—¶é—´æ¨ç†è¯„ä¼°ï¼ˆTREAï¼‰æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„ä¸ç¡®å®šæ€§åº¦é‡æ¥è¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§ã€‚
3. åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå¼€æºLALMsåœ¨TREAæ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†å¯¹æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°çš„å¿…è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ–‡æœ¬åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸï¼Œç ”ç©¶è€…ä»¬å¼€å§‹å…³æ³¨å°†è§†è§‰å’ŒéŸ³é¢‘ç­‰å…¶ä»–æ¨¡æ€ä¸æ–‡æœ¬ç»“åˆï¼Œä»¥å®ç°ç±»ä¼¼çš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†â€”â€”éŸ³é¢‘çš„æ—¶é—´æ¨ç†è¯„ä¼°ï¼ˆTREAï¼‰ï¼Œç”¨äºè¯„ä¼°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰åœ¨æ¨ç†ç›¸å…³ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡å¯¹å¼€æºLALMsçš„åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å®ƒä»¬åœ¨TREAæ•°æ®é›†çš„ä»»åŠ¡ä¸Šå§‹ç»ˆè½åäºäººç±»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§ä¸ç¡®å®šæ€§åº¦é‡ï¼Œè®¡ç®—æ¨¡å‹å¯¹è¯­ä¹‰ä¸Šç›¸åŒçš„è¾“å…¥æ‰°åŠ¨çš„ä¸å˜æ€§ã€‚åˆ†æè¡¨æ˜ï¼Œå‡†ç¡®ç‡ä¸ä¸ç¡®å®šæ€§æŒ‡æ ‡å¹¶ä¸ä¸€å®šç›¸å…³ï¼Œå› æ­¤éœ€è¦å¯¹LALMsè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œä»¥é€‚åº”é«˜é£é™©åº”ç”¨åœºæ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰åœ¨æ—¶é—´æ¨ç†ä»»åŠ¡ä¸Šçš„è¯„ä¼°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºåˆ†ç±»æˆ–ç”Ÿæˆä»»åŠ¡ï¼Œç¼ºä¹é’ˆå¯¹æ¨ç†èƒ½åŠ›çš„ä¸“é—¨è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºéŸ³é¢‘çš„æ—¶é—´æ¨ç†è¯„ä¼°ï¼ˆTREAï¼‰æ•°æ®é›†ï¼Œè®ºæ–‡æä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¼•å…¥ä¸ç¡®å®šæ€§åº¦é‡æ¥åˆ†ææ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„æ•æ„Ÿæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€åŸºå‡†æµ‹è¯•å’Œä¸ç¡®å®šæ€§è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†åŒ…å«å¤šç§æ—¶é—´æ¨ç†ä»»åŠ¡ï¼ŒåŸºå‡†æµ‹è¯•åˆ™å¯¹æ¯”äº†å¤šç§å¼€æºLALMsçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†TREAæ•°æ®é›†å’Œä¸ç¡®å®šæ€§åº¦é‡ï¼Œå‰è€…ä¸“æ³¨äºæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ï¼Œåè€…æ­ç¤ºäº†æ¨¡å‹å‡†ç¡®ç‡ä¸ä¸ç¡®å®šæ€§ä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¸ç¡®å®šæ€§åº¦é‡ä¸­ï¼Œé‡‡ç”¨äº†å¯¹è¾“å…¥è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼æ‰°åŠ¨çš„æ–¹å¼ï¼Œè¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§ï¼›åŒæ—¶ï¼ŒåŸºå‡†æµ‹è¯•ä¸­ä½¿ç”¨äº†å¤šç§LALMsè¿›è¡Œå¯¹æ¯”ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æºLALMsåœ¨TREAæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜æ˜¾ä½äºäººç±»ï¼Œä¸”å‡†ç¡®ç‡ä¸ä¸ç¡®å®šæ€§æŒ‡æ ‡ä¹‹é—´å¹¶æ— ç›´æ¥ç›¸å…³æ€§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨é«˜é£é™©åº”ç”¨ä¸­å¯¹æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°çš„å¿…è¦æ€§ï¼Œæ¨åŠ¨äº†å¯¹LALMsçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œä¼˜åŒ–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©ç†ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºè¿™äº›ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œè¿›è€Œæé«˜ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€AIçš„å‘å±•ï¼Œä¿ƒè¿›ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ›´å¥½èåˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA).
>   We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.

