---
layout: default
title: "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection"
---

# GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13312" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13312v1</a>
  <a href="https://arxiv.org/pdf/2505.13312.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13312v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13312v1', 'GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhijie Deng, Chris Yuhao Liu, Zirui Pang, Xinlei He, Lei Feng, Qi Xuan, Zhaowei Zhu, Jiaheng Wei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGUARDæ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„é€‰æ‹©æ€§é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é€‰æ‹©æ€§é—å¿˜` `åŠ¨æ€æ£€æµ‹` `æ–‡æœ¬ç”Ÿæˆ` `å®‰å…¨æ€§` `åˆè§„æ€§` `çŸ¥è¯†ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é—å¿˜æ–¹æ³•é€šå¸¸ä¾èµ–äºå¾®è°ƒï¼Œå¯¼è‡´é—å¿˜ä¸ä¿ç•™çŸ¥è¯†ä¹‹é—´çš„å†³ç­–è¾¹ç•Œæ¨¡ç³Šï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚
2. GUARDæ¡†æ¶é€šè¿‡åœ¨ç”Ÿæˆæ—¶åŠ¨æ€æ£€æµ‹å’Œè¿‡æ»¤é—å¿˜ç›®æ ‡ï¼Œé¿å…äº†å¾®è°ƒå¸¦æ¥çš„è´Ÿé¢å½±å“ï¼Œç¡®ä¿æ–‡æœ¬ç”Ÿæˆçš„æµç•…æ€§ã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGUARDåœ¨é—å¿˜è´¨é‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶å¯¹æ¨¡å‹çš„é€šç”¨èƒ½åŠ›å‡ ä¹æ²¡æœ‰å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®°å¿†å¤§é‡çŸ¥è¯†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é€‰æ‹©æ€§é—å¿˜ç‰¹å®šçŸ¥è¯†çš„èƒ½åŠ›å¯¹äºç¡®ä¿æ¨¡å‹çš„å®‰å…¨æ€§å’Œåˆè§„æ€§è‡³å…³é‡è¦ã€‚ç°æœ‰çš„é—å¿˜æ–¹æ³•é€šå¸¸éœ€è¦é€šè¿‡å¾®è°ƒæ¨¡å‹æ¥å®ç°ï¼Œè¿™ä¼šæ¨¡ç³Šé—å¿˜ä¸ä¿ç•™çŸ¥è¯†ä¹‹é—´çš„å†³ç­–è¾¹ç•Œï¼Œå½±å“æ•´ä½“æ€§èƒ½ã€‚ä¸ºé¿å…å¾®è°ƒå¸¦æ¥çš„è´Ÿé¢å½±å“ï¼Œæœ¬æ–‡æå‡ºäº†GUARDæ¡†æ¶ï¼Œå…è®¸åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€åœ°è¿›è¡Œé—å¿˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æç¤ºåˆ†ç±»å™¨æ£€æµ‹é—å¿˜ç›®æ ‡å¹¶æå–ç›¸åº”çš„ç¦ç”¨æ ‡è®°ï¼Œç»“åˆæ ‡è®°åŒ¹é…å’Œè¯­ä¹‰åŒ¹é…åŠ¨æ€æƒ©ç½šå’Œè¿‡æ»¤å€™é€‰æ ‡è®°ï¼Œæœ‰æ•ˆé˜²æ­¢æ¨¡å‹æ³„éœ²é—å¿˜å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUARDåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†å¼ºå¤§çš„é—å¿˜è´¨é‡ï¼ŒåŒæ—¶å‡ ä¹æ²¡æœ‰é™ä½LLMçš„æ•´ä½“èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é€‰æ‹©æ€§é—å¿˜ç‰¹å®šçŸ¥è¯†çš„èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å¾®è°ƒæ¨¡å‹æ¥å®ç°é—å¿˜ï¼Œä½†è¿™ä¼šå½±å“æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œå†³ç­–è¾¹ç•Œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGUARDæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€åœ°æ£€æµ‹å’Œè¿‡æ»¤ä¸é—å¿˜ç›®æ ‡ç›¸å…³çš„å†…å®¹ã€‚é€šè¿‡ä½¿ç”¨æç¤ºåˆ†ç±»å™¨è¯†åˆ«é—å¿˜ç›®æ ‡ï¼ŒGUARDèƒ½å¤Ÿåœ¨ä¸å½±å“æ–‡æœ¬æµç•…æ€§çš„æƒ…å†µä¸‹ï¼Œå®‰å…¨åœ°é˜²æ­¢æ¨¡å‹ç”Ÿæˆç›¸å…³å“åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGUARDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæç¤ºåˆ†ç±»å™¨å’ŒåŠ¨æ€æƒ©ç½šè¿‡æ»¤æœºåˆ¶ã€‚æç¤ºåˆ†ç±»å™¨è´Ÿè´£æ£€æµ‹é—å¿˜ç›®æ ‡å¹¶æå–ç¦ç”¨æ ‡è®°ï¼Œè€ŒåŠ¨æ€æƒ©ç½šè¿‡æ»¤æœºåˆ¶åˆ™åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹å€™é€‰æ ‡è®°è¿›è¡Œæƒ©ç½šå’Œè¿‡æ»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šGUARDçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åœ¨ç”Ÿæˆæ—¶åŠ¨æ€è¿›è¡Œé—å¿˜ï¼Œè€Œä¸æ˜¯ä¾èµ–äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°é¿å…äº†é—å¿˜ä¸ä¿ç•™çŸ¥è¯†ä¹‹é—´çš„å†³ç­–æ¨¡ç³Šæ€§ï¼Œä¿æŒäº†æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šGUARDé‡‡ç”¨äº†æ ‡è®°åŒ¹é…å’Œè¯­ä¹‰åŒ¹é…çš„ç»„åˆæ¥åŠ¨æ€æƒ©ç½šå’Œè¿‡æ»¤å€™é€‰æ ‡è®°ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹ä¸æ³„éœ²é—å¿˜çš„çŸ¥è¯†ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒä¸­ç»è¿‡ä¼˜åŒ–ï¼Œä»¥å®ç°æœ€ä½³çš„é—å¿˜æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¯¹å“ˆåˆ©Â·æ³¢ç‰¹æ•°æ®é›†å’ŒMUSEåŸºå‡†çš„ç‰ˆæƒå†…å®¹é—å¿˜ä»»åŠ¡ï¼Œä»¥åŠTOFUæ•°æ®é›†çš„å®ä½“é—å¿˜ä»»åŠ¡ä¸­ï¼ŒGUARDè¡¨ç°å‡ºè‰²ï¼Œé—å¿˜è´¨é‡æ˜¾è‘—æå‡ï¼ŒåŒæ—¶å¯¹æ¨¡å‹çš„é€šç”¨èƒ½åŠ›å‡ ä¹æ²¡æœ‰å½±å“ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„é—å¿˜ä¸æ•ˆç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GUARDæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ä¿æŠ¤æ•æ„Ÿä¿¡æ¯æˆ–éµå¾ªæ³•å¾‹æ³•è§„çš„åœºæ™¯ä¸­ï¼Œå¦‚ç‰ˆæƒå†…å®¹ç®¡ç†å’Œä¸ªäººéšç§ä¿æŠ¤ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¤šé¢†åŸŸï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œåˆè§„æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated strong capabilities in memorizing vast amounts of knowledge across diverse domains. However, the ability to selectively forget specific knowledge is critical for ensuring the safety and compliance of deployed models. Existing unlearning efforts typically fine-tune the model with resources such as forget data, retain data, and a calibration model. These additional gradient steps blur the decision boundary between forget and retain knowledge, making unlearning often at the expense of overall performance. To avoid the negative impact of fine-tuning, it would be better to unlearn solely at inference time by safely guarding the model against generating responses related to the forget target, without destroying the fluency of text generation. In this work, we propose Generation-time Unlearning via Adaptive Restriction and Detection (GUARD), a framework that enables dynamic unlearning during LLM generation. Specifically, we first employ a prompt classifier to detect unlearning targets and extract the corresponding forbidden token. We then dynamically penalize and filter candidate tokens during generation using a combination of token matching and semantic matching, effectively preventing the model from leaking the forgotten content. Experimental results on copyright content unlearning tasks over the Harry Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on the TOFU dataset, demonstrate that GUARD achieves strong forget quality across various tasks while causing almost no degradation to the LLM's general capabilities, striking an excellent trade-off between forgetting and utility.

