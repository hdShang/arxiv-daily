---
layout: default
title: SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information
---

# SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13237v3</a>
  <a href="https://arxiv.org/pdf/2505.13237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13237v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13237v3', 'SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chih-Kai Yang, Neo Ho, Yen-Ting Piao, Hung-yi Lee

**åˆ†ç±»**: eess.AS, cs.CL, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-08-24)

**å¤‡æ³¨**: Accepted to Interspeech 2025 (Oral). Update acknowledgement in this version. Project page: https://github.com/ckyang1124/SAKURA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAKURAåŸºå‡†ä»¥è¯„ä¼°å¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å¤šè·³æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹` `å¤šè·³æ¨ç†` `å¤šæ¨¡æ€æ¨ç†` `è¯­éŸ³å¤„ç†` `éŸ³é¢‘ä¿¡æ¯` `SAKURAåŸºå‡†` `ä¿¡æ¯æ•´åˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤šè·³æ¨ç†èƒ½åŠ›ä¸Šç¼ºä¹ç³»ç»Ÿè¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨æ•´åˆè¯­éŸ³å’ŒéŸ³é¢‘ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºSAKURAåŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°LALMsåœ¨å¤šè·³æ¨ç†ä¸­çš„è¡¨ç°ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLALMsåœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°½ç®¡èƒ½å¤Ÿæå–ç›¸å…³ä¿¡æ¯ï¼Œæ•´åˆèƒ½åŠ›ä»æ˜¾ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰åœ¨è¯­éŸ³å’ŒéŸ³é¢‘å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å¤šè·³æ¨ç†ï¼Œå°šæœªå¾—åˆ°ç³»ç»Ÿè¯„ä¼°ã€‚ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨ä¸€èˆ¬çš„è¯­éŸ³å’ŒéŸ³é¢‘å¤„ç†ä»»åŠ¡ã€å¯¹è¯èƒ½åŠ›åŠå…¬å¹³æ€§ï¼Œå¿½è§†äº†å¤šè·³æ¨ç†çš„è¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºSAKURAåŸºå‡†ï¼Œä¸“é—¨è¯„ä¼°LALMsåœ¨è¯­éŸ³å’ŒéŸ³é¢‘ä¿¡æ¯åŸºç¡€ä¸Šçš„å¤šè·³æ¨ç†èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LALMsèƒ½å¤Ÿæ­£ç¡®æå–ç›¸å…³ä¿¡æ¯ï¼Œä½†åœ¨æ•´åˆè¯­éŸ³/éŸ³é¢‘è¡¨ç¤ºè¿›è¡Œå¤šè·³æ¨ç†æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€æ¨ç†ä¸­çš„å…³é”®é™åˆ¶ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦è§è§£å’Œèµ„æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤šè·³æ¨ç†èƒ½åŠ›è¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½ç³»ç»Ÿæ€§åœ°è€ƒå¯Ÿå…¶åœ¨è¯­éŸ³å’ŒéŸ³é¢‘ä¿¡æ¯æ•´åˆæ–¹é¢çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥SAKURAåŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LALMsçš„å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œå¼ºè°ƒå¯¹è¯­éŸ³å’ŒéŸ³é¢‘ä¿¡æ¯çš„æ•´åˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSAKURAåŸºå‡†åŒ…å«å¤šä¸ªä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åœ¨ç»™å®šçš„è¯­éŸ³å’ŒéŸ³é¢‘ä¿¡æ¯ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œè¯„ä¼°å…¶ä¿¡æ¯æ•´åˆèƒ½åŠ›ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ä¿¡æ¯æå–ã€æ¨ç†è¿‡ç¨‹å’Œç»“æœè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šSAKURAåŸºå‡†çš„æå‡ºæ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†LALMsåœ¨å¤šè·³æ¨ç†ä¸­çš„è¡¨ç°ï¼Œä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼Œä¸“æ³¨äºå¤šæ¨¡æ€ä¿¡æ¯çš„æ•´åˆèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶è®¾è®¡äº†å¤šæ ·åŒ–çš„ä»»åŠ¡åœºæ™¯ä»¥å…¨é¢è¯„ä¼°æ¨¡å‹èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLALMsåœ¨SAKURAåŸºå‡†ä¸Šçš„è¡¨ç°ä¸å°½å¦‚äººæ„ï¼Œå°½ç®¡åœ¨ä¿¡æ¯æå–æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šè·³æ¨ç†ä¸­æ•´åˆèƒ½åŠ›ä¸è¶³ï¼Œæ­ç¤ºäº†å…¶åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€ä¿¡æ¯æ—¶çš„å±€é™æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå’Œå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡å¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„è¯­éŸ³å’ŒéŸ³é¢‘ä¿¡æ¯ï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.

