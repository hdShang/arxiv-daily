---
layout: default
title: "J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization"
---

# J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13346" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13346v3</a>
  <a href="https://arxiv.org/pdf/2505.13346.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13346v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13346v3', 'J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-06-18)

**å¤‡æ³¨**: 25 pages, 4 figures, 6 tables. Updated with code and benchmark

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEIS-GRPOç®—æ³•ä»¥æå‡æ¨¡å‹è¾“å‡ºè¯„ä¼°çš„å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹è¯„ä¼°` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `è‡ªåŠ¨åŒ–è¯„ä¼°` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMä½œä¸ºè¯„åˆ¤è€…åœ¨å¤æ‚æ¨ç†é¢†åŸŸçš„è¯„ä¼°èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å‡†ç¡®æ€§å—åˆ°å½±å“ã€‚
2. æœ¬æ–‡æå‡ºäº†EIS-GRPOç®—æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¯„åˆ¤è€…ï¼Œä»¥å…‹æœå¤æ‚è¯„ä¼°ç¯å¢ƒä¸­çš„ä½ç½®åå·®é—®é¢˜ã€‚
3. è®­ç»ƒå‡ºçš„J4Ræ¨¡å‹åœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šåˆ†åˆ«è¶…è¶Šäº†GPT-4oå’Œå…¶ä»–å°å‹è¯„åˆ¤è€…ï¼Œæå‡å¹…åº¦è¾¾åˆ°6.7%å’Œ9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•çš„åŠ é€Ÿï¼Œæ¨¡å‹è¾“å‡ºè¯„ä¼°å·²ä»è€—æ—¶çš„äººåŠ›è¯„ä¼°è½¬å‘è‡ªåŠ¨è¯„ä¼°ï¼ŒLLMè¢«èµ‹äºˆè¯„ä¼°å…¶ä»–æ¨¡å‹è¾“å‡ºçš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMä½œä¸ºè¯„åˆ¤è€…çš„æ¨¡å‹åœ¨å¤æ‚æ¨ç†é¢†åŸŸè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆEIS-GRPOï¼‰ç®—æ³•ï¼Œæ—¨åœ¨æé«˜è¯„åˆ¤è€…åœ¨å¤æ‚è¯„ä¼°ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ReasoningJudgeBenchåŸºå‡†ï¼Œè¯„ä¼°è¯„åˆ¤è€…åœ¨å¤šæ ·åŒ–æ¨ç†åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚æœ€ç»ˆï¼Œè®­ç»ƒå‡ºçš„Judge for Reasoningï¼ˆJ4Rï¼‰æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Šäº†GPT-4oå’Œå…¶ä»–å°å‹è¯„åˆ¤è€…ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMä½œä¸ºè¯„åˆ¤è€…åœ¨å¤æ‚æ¨ç†é¢†åŸŸè¯„ä¼°èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ›´å…·æŒ‘æˆ˜æ€§çš„å†…å®¹æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºEIS-GRPOç®—æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¯„åˆ¤è€…ï¼Œä½¿å…¶åœ¨å¤æ‚è¯„ä¼°ç¯å¢ƒä¸­å¯¹ä½ç½®åå·®å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œä»è€Œæé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ”¶é›†å¤šæ ·åŒ–çš„è¯„ä¼°æ•°æ®ï¼Œç„¶ååˆ©ç”¨EIS-GRPOç®—æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæœ€ååœ¨ReasoningJudgeBenchä¸Šè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šEIS-GRPOç®—æ³•æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒé€šè¿‡å¼•å…¥ç­‰æ•ˆåˆå§‹çŠ¶æ€çš„æ¦‚å¿µï¼Œæ˜¾è‘—æå‡äº†è¯„åˆ¤è€…åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†ä½ç½®åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è¯„åˆ¤è€…çš„è¾“å‡ºï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§ç½‘ç»œç»“æ„ä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œç¡®ä¿å…¶åœ¨å¤šæ ·åŒ–æ¨ç†åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒJ4Ræ¨¡å‹åœ¨JudgeBenchå’ŒReasoningJudgeBenchä¸Šåˆ†åˆ«è¶…è¶Šäº†GPT-4oå’Œå…¶ä»–å°å‹è¯„åˆ¤è€…ï¼Œæå‡å¹…åº¦è¾¾åˆ°6.7%å’Œ9%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒEIS-GRPOç®—æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–å†…å®¹è¯„ä¼°ã€æ™ºèƒ½å®¢æœç³»ç»Ÿå’Œæ•™è‚²è¯„ä¼°ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¯„ä¼°èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºç›¸å…³è¡Œä¸šæä¾›æ›´å‡†ç¡®çš„åé¦ˆå’Œè¯„ä¼°ç»“æœï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.

