---
layout: default
title: The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation
---

# The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13090" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13090v2</a>
  <a href="https://arxiv.org/pdf/2505.13090.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13090v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13090v2', 'The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: David Stap, Christof Monz

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-09-19)

**å¤‡æ³¨**: EMNLP 2025 Camera Ready

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¯­è¨€å¤šæ ·æ€§å¯¹å¤§è¯­è¨€æ¨¡å‹ç¿»è¯‘å¾®è°ƒçš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€å¤šæ ·æ€§` `å¤§è¯­è¨€æ¨¡å‹` `ç¿»è¯‘å¾®è°ƒ` `æ— ç›‘ç£å­¦ä¹ ` `æœ‰ç›‘ç£å­¦ä¹ ` `æ€§èƒ½è¯„ä¼°` `è·¨è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹è¯­è¨€å¤šæ ·æ€§åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„ä½œç”¨å­˜åœ¨äº‰è®®ï¼Œç¼ºä¹ç³»ç»Ÿæ€§å®éªŒæ”¯æŒã€‚
2. æœ¬ç ”ç©¶é€šè¿‡æ§åˆ¶å®éªŒï¼Œæ¢è®¨è¯­è¨€å¤šæ ·æ€§å¯¹ç¿»è¯‘è´¨é‡çš„å½±å“ï¼Œå‘ç°å…¶åœ¨å¾®è°ƒä¸­å…·æœ‰æ˜¾è‘—çš„æ­£é¢æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¢åŠ è¯­è¨€å¤šæ ·æ€§èƒ½å¤Ÿæå‡ç¿»è¯‘è´¨é‡ï¼Œä½†åœ¨è¾¾åˆ°ä¸€å®šé˜ˆå€¼åï¼Œæ•ˆæœä¼šå‡å¼±æˆ–å¹³ç¨³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»¥å¾€ç ”ç©¶å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒä¸­çš„è¯­è¨€å¤šæ ·æ€§å­˜åœ¨åˆ†æ­§ï¼šä¸€äº›ç ”ç©¶æŠ¥å‘Šäº†å…¶ä¼˜åŠ¿ï¼Œè€Œå¦ä¸€äº›åˆ™æœªå‘ç°æ˜æ˜¾å¥½å¤„ã€‚é€šè¿‡å¯¹132ä¸ªç¿»è¯‘æ–¹å‘çš„æ§åˆ¶å¾®è°ƒå®éªŒï¼Œæˆ‘ä»¬ç³»ç»Ÿæ€§åœ°è§£å†³äº†è¿™äº›å·®å¼‚ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¢åŠ è¯­è¨€å¤šæ ·æ€§èƒ½å¤Ÿæå‡æ— ç›‘ç£å’Œæœ‰ç›‘ç£ç¿»è¯‘å¯¹çš„ç¿»è¯‘è´¨é‡ï¼Œå°½ç®¡åœ¨æœ‰ç›‘ç£å¯¹ä¸Šå¾®è°ƒçš„æ¨¡å‹å¤šæ ·æ€§è¾ƒä½ã€‚ç„¶è€Œï¼Œè¶…è¿‡æŸä¸€å¤šæ ·æ€§é˜ˆå€¼åï¼Œæ”¶ç›Šä¼šè¶‹äºå¹³ç¨³æˆ–ä¸‹é™ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¢åŠ è¯­è¨€å¤šæ ·æ€§èƒ½å¤Ÿåˆ›é€ æ›´å…·è¯­è¨€æ— å…³æ€§çš„è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºçš„é€‚åº”æ€§å˜åŒ–æœ‰åŠ©äºè§£é‡Šåœ¨å¤šæ ·æ€§æ›´é«˜çš„å¾®è°ƒæ¨¡å‹ä¸­æ€§èƒ½çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è¯­è¨€å¤šæ ·æ€§åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„ä½œç”¨ä¸æ˜ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¯¹å¤šæ ·æ€§å½±å“çš„ç ”ç©¶ç»“æœä¸ä¸€è‡´ï¼Œç¼ºä¹ç³»ç»Ÿçš„å®è¯æ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹132ä¸ªç¿»è¯‘æ–¹å‘è¿›è¡Œæ§åˆ¶å¾®è°ƒå®éªŒï¼Œç³»ç»Ÿæ€§åœ°åˆ†æè¯­è¨€å¤šæ ·æ€§å¯¹ç¿»è¯‘è´¨é‡çš„å½±å“ï¼ŒéªŒè¯å…¶åœ¨æ— ç›‘ç£å’Œæœ‰ç›‘ç£ç¿»è¯‘å¯¹ä¸­çš„ä¼˜åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“å®éªŒæµç¨‹åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹é€‰æ‹©ã€å¾®è°ƒè¿‡ç¨‹å’Œæ€§èƒ½è¯„ä¼°å››ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œé€‰æ‹©å¤šç§è¯­è¨€å¯¹è¿›è¡Œå¾®è°ƒï¼Œç„¶ååœ¨ä¸åŒçš„è¯­è¨€å¤šæ ·æ€§æ¡ä»¶ä¸‹è¿›è¡Œè®­ç»ƒï¼Œæœ€åè¯„ä¼°ç¿»è¯‘è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†è¯­è¨€å¤šæ ·æ€§å¯¹ç¿»è¯‘è´¨é‡çš„æ­£é¢å½±å“ï¼Œå¹¶æå‡ºäº†å¤šæ ·æ€§é˜ˆå€¼çš„æ¦‚å¿µï¼Œå¼ºè°ƒäº†è¯­è¨€æ— å…³æ€§è¡¨ç¤ºçš„å½¢æˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ ·æ€§é€æ­¥å¢åŠ çš„ç­–ç•¥ï¼Œå¹¶å¯¹æ¨¡å‹çš„æŸå¤±å‡½æ•°è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒè¯­è¨€å¯¹çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¢åŠ è¯­è¨€å¤šæ ·æ€§èƒ½å¤Ÿæ˜¾è‘—æå‡ç¿»è¯‘è´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨æ— ç›‘ç£ç¿»è¯‘å¯¹ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œè¶…è¿‡æŸä¸€å¤šæ ·æ€§é˜ˆå€¼åï¼Œæ€§èƒ½æå‡è¶‹äºå¹³ç¨³æˆ–ä¸‹é™ï¼Œæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸï¼Œæå‡å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ç¿»è¯‘è´¨é‡ã€‚æœªæ¥ï¼Œéšç€è¯­è¨€å¤šæ ·æ€§ç ”ç©¶çš„æ·±å…¥ï¼Œå¯èƒ½ä¼šæ¨åŠ¨æ›´é«˜æ•ˆçš„å¤šè¯­è¨€æ¨¡å‹å¼€å‘ï¼Œä¿ƒè¿›å…¨çƒä¿¡æ¯äº¤æµã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prior research diverges on language diversity in LLM fine-tuning: Some studies report benefits while others find no advantages. Through controlled fine-tuning experiments across 132 translation directions, we systematically resolve these disparities. We find that expanding language diversity during fine-tuning improves translation quality for both unsupervised and -- surprisingly -- supervised pairs, despite less diverse models being fine-tuned exclusively on these supervised pairs. However, benefits plateau or decrease beyond a certain diversity threshold. We show that increased language diversity creates more language-agnostic representations. These representational adaptations help explain the improved performance in models fine-tuned with greater diversity.

