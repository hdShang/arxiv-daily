---
layout: default
title: "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding"
---

# HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13254" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13254v2</a>
  <a href="https://arxiv.org/pdf/2505.13254.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13254v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13254v2', 'HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siran Liu, Yang Ye, Qianchao Zhu, Zane Cao, Yongchao He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-10-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHeteroSpecä»¥è§£å†³è‡ªå›å½’è§£ç æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªå›å½’è§£ç ` `æŠ•æœºè§£ç ` `éªŒè¯å¼‚è´¨æ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç†µé‡åŒ–` `åŠ¨æ€ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªå›å½’è§£ç æ–¹æ³•ç”±äºé¡ºåºä¾èµ–æ€§ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶åœ¨å¤„ç†é«˜ç½®ä¿¡åº¦é¢„æµ‹æ—¶è¡¨ç°ä¸ä½³ã€‚
2. HeteroSpecé€šè¿‡å¼‚è´¨æ€§è‡ªé€‚åº”çš„æ–¹å¼ï¼Œæ ¹æ®å€™é€‰çš„ä¸ç¡®å®šæ€§åŠ¨æ€åˆ†é…éªŒè¯èµ„æºï¼Œä»è€Œæé«˜è§£ç æ•ˆç‡ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHeteroSpecå®ç°äº†4.24å€çš„è§£ç é€Ÿåº¦æå‡ï¼Œä¸”æ— éœ€é‡è®­ç»ƒï¼Œå…¼å®¹å…¶ä»–ä¼˜åŒ–æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªå›å½’è§£ç ç”±äºå…¶é¡ºåºä¾èµ–æ€§ï¼Œé™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ååé‡ã€‚å°½ç®¡æŠ•æœºè§£ç é€šè¿‡å¹¶è¡ŒéªŒè¯å¤šä¸ªé¢„æµ‹æ ‡è®°æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å…¶æ•ˆç‡ä»å—é™äºéªŒè¯å¼‚è´¨æ€§ï¼Œå³ä¸åŒæŠ•æœºå€™é€‰çš„éªŒè¯éš¾åº¦ä¸å‡ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé«˜ç½®ä¿¡åº¦é¢„æµ‹çš„å­é›†å æ®äº†å¤§éƒ¨åˆ†æˆåŠŸéªŒè¯ï¼Œè€Œç°æœ‰æ–¹æ³•å¯¹æ‰€æœ‰å€™é€‰çš„å¤„ç†æ–¹å¼è¿‡äºç»Ÿä¸€ï¼Œå¯¼è‡´å†—ä½™è®¡ç®—ã€‚HeteroSpecæ˜¯ä¸€ä¸ªå¼‚è´¨æ€§è‡ªé€‚åº”çš„æŠ•æœºè§£ç æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å€™é€‰çš„ä¸ç¡®å®šæ€§åˆ†é…éªŒè¯å·¥ä½œã€‚HeteroSpecé€šè¿‡è½»é‡çº§çš„ç†µé‡åŒ–å™¨ä¼°è®¡éªŒè¯å¤æ‚åº¦ï¼Œé‡‡ç”¨æ•°æ®é©±åŠ¨çš„åˆ†å±‚ç­–ç•¥å¯¹å€™é€‰è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶é€šè¿‡åè°ƒä¼˜åŒ–åŠ¨æ€è°ƒæ•´æŠ•æœºæ·±åº¦å’Œä¿®å‰ªé˜ˆå€¼ã€‚åœ¨äº”ä¸ªåŸºå‡†å’Œå››ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼ŒHeteroSpecç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å¦‚EAGLE-3å®ç°äº†å¹³å‡4.24å€çš„è§£ç åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†è¾“å‡ºåˆ†å¸ƒçš„å‡†ç¡®æ€§ã€‚HeteroSpecæ— éœ€æ¨¡å‹é‡è®­ç»ƒï¼Œå¹¶ä¸å…¶ä»–æ¨ç†ä¼˜åŒ–å…¼å®¹ï¼Œæˆä¸ºæé«˜æŠ•æœºè§£ç æ•ˆç‡çš„å®ç”¨æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è‡ªå›å½’è§£ç ä¸­ç”±äºé¡ºåºä¾èµ–æ€§å¯¼è‡´çš„æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰çš„æŠ•æœºè§£ç æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†éªŒè¯å¼‚è´¨æ€§ï¼Œå¯¼è‡´å†—ä½™è®¡ç®—å’Œä½æ•ˆçš„èµ„æºåˆ†é…ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHeteroSpecçš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®å€™é€‰çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´éªŒè¯å·¥ä½œé‡ï¼Œé€šè¿‡è½»é‡çº§çš„ç†µé‡åŒ–å™¨æ¥ä¼°è®¡éªŒè¯å¤æ‚åº¦ï¼Œä»è€Œä¼˜åŒ–è§£ç è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHeteroSpecçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨ç†µé‡åŒ–å™¨è¯„ä¼°æ¯ä¸ªå€™é€‰çš„éªŒè¯å¤æ‚åº¦ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨æ•°æ®é©±åŠ¨çš„åˆ†å±‚ç­–ç•¥å¯¹å€™é€‰è¿›è¡Œåˆ’åˆ†ï¼›æœ€åï¼Œé€šè¿‡åè°ƒä¼˜åŒ–åŠ¨æ€è°ƒæ•´æŠ•æœºæ·±åº¦å’Œä¿®å‰ªé˜ˆå€¼ï¼Œä»¥æé«˜è§£ç æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šHeteroSpecçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¼‚è´¨æ€§è‡ªé€‚åº”çš„éªŒè¯ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®å€™é€‰çš„ç½®ä¿¡åº¦å’Œå¤æ‚åº¦è¿›è¡Œçµæ´»è°ƒæ•´ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç»Ÿä¸€å¤„ç†æ–¹å¼å½¢æˆäº†æ˜¾è‘—å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šHeteroSpecè®¾è®¡äº†è½»é‡çº§çš„ç†µé‡åŒ–å™¨ä½œä¸ºéªŒè¯å¤æ‚åº¦çš„è¯„ä¼°å·¥å…·ï¼Œå¹¶é€šè¿‡æ•°æ®é©±åŠ¨çš„åˆ†å±‚ç­–ç•¥æ¥ä¼˜åŒ–å€™é€‰çš„å¤„ç†æµç¨‹ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€è°ƒæ•´çš„æŠ•æœºæ·±åº¦å’Œä¿®å‰ªé˜ˆå€¼ä¹Ÿæ˜¯å…¶å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HeteroSpecåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•å’Œå››ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†å¹³å‡4.24å€çš„è§£ç é€Ÿåº¦æå‡ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„EAGLE-3æ–¹æ³•ï¼Œä¿æŒäº†è¾“å‡ºåˆ†å¸ƒçš„å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æŠ•æœºè§£ç æ•ˆç‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HeteroSpecçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜è§£ç æ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®æ—¶åº”ç”¨ä¸­çš„å“åº”é€Ÿåº¦ï¼Œè¿›è€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯èƒ½ä¸å…¶ä»–æ¨ç†ä¼˜åŒ–æŠ€æœ¯ç»“åˆï¼Œè¿›ä¸€æ­¥æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autoregressive decoding inherently limits the inference throughput of Large Language Model (LLM) due to its sequential dependency. Speculative decoding mitigates this by verifying multiple predicted tokens in parallel, but its efficiency remains constrained by what we identify as verification heterogeneity -- the uneven difficulty of verifying different speculative candidates. In practice, a small subset of high-confidence predictions accounts for most successful verifications, yet existing methods treat all candidates uniformly, leading to redundant computation. We present HeteroSpec, a heterogeneity-adaptive speculative decoding framework that allocates verification effort in proportion to candidate uncertainty. HeteroSpec estimates verification complexity using a lightweight entropy-based quantifier, partitions candidates via a data-driven stratification policy, and dynamically tunes speculative depth and pruning thresholds through coordinated optimization. Across five benchmarks and four LLMs, HeteroSpec delivers an average 4.24$\times$ decoding speedup over state-of-the-art methods such as EAGLE-3, while preserving exact output distributions. Crucially, HeteroSpec requires no model retraining and remains compatible with other inference optimizations, making it a practical direction for improving speculative decoding efficiency.

