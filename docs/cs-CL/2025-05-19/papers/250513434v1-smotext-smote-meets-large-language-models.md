---
layout: default
title: SMOTExT: SMOTE meets Large Language Models
---

# SMOTExT: SMOTE meets Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13434" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13434v1</a>
  <a href="https://arxiv.org/pdf/2505.13434.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13434v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13434v1', 'SMOTExT: SMOTE meets Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mateusz BystroÅ„ski, MikoÅ‚aj HoÅ‚ysz, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSMOTExTä»¥è§£å†³æ–‡æœ¬æ•°æ®ç¨€ç¼ºä¸ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®å¢å¼º` `è‡ªç„¶è¯­è¨€å¤„ç†` `åˆæˆæ•°æ®` `å°‘æ ·æœ¬å­¦ä¹ ` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºå’Œç±»åˆ«ä¸å¹³è¡¡æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å®šé¢†åŸŸã€‚
2. SMOTExTé€šè¿‡åœ¨BERTåµŒå…¥ç©ºé—´ä¸­æ’å€¼ç”Ÿæˆåˆæˆæ–‡æœ¬ç¤ºä¾‹ï¼Œç»“åˆxRAGæ¶æ„å®ç°æ–‡æœ¬è§£ç ã€‚
3. åˆæ­¥å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç”Ÿæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸åŸå§‹æ•°æ®é›†ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ•°æ®å¢å¼ºæ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®ç¨€ç¼ºå’Œç±»åˆ«ä¸å¹³è¡¡æ˜¯è®­ç»ƒå¼ºå¤§è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹æ—¶é¢ä¸´çš„æŒç»­æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸“ä¸šé¢†åŸŸæˆ–ä½èµ„æºç¯å¢ƒä¸­ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æŠ€æœ¯SMOTExTï¼Œå°†åˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰çš„ç†å¿µåº”ç”¨äºæ–‡æœ¬æ•°æ®ã€‚è¯¥æ–¹æ³•é€šè¿‡æ’å€¼ä¸¤ä¸ªç°æœ‰ç¤ºä¾‹çš„åŸºäºBERTçš„åµŒå…¥ç”Ÿæˆæ–°çš„åˆæˆç¤ºä¾‹ï¼Œå¹¶åˆ©ç”¨xRAGæ¶æ„å°†ç»“æœè§£ç ä¸ºæ–‡æœ¬ã€‚å°½ç®¡è¿™é¡¹å·¥ä½œä»å¤„äºåˆæ­¥é˜¶æ®µï¼Œä»…é€šè¿‡å®šæ€§è¾“å‡ºæ”¯æŒï¼Œä½†è¯¥æ–¹æ³•åœ¨çŸ¥è¯†è’¸é¦å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„æ•°æ®å¢å¼ºæ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæ—©æœŸå®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨ç”Ÿæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸ä½¿ç”¨åŸå§‹æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼Œè¡¨æ˜åœ¨æ•°æ®ä¿æŠ¤çº¦æŸä¸‹å®‰å…¨æœ‰æ•ˆå­¦ä¹ çš„å¯è¡Œè·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œç”±äºæ•°æ®ç¨€ç¼ºå’Œç±»åˆ«ä¸å¹³è¡¡å¯¼è‡´çš„æ¨¡å‹è®­ç»ƒå›°éš¾ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆç”Ÿæˆè¶³å¤Ÿçš„è®­ç»ƒæ ·æœ¬ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSMOTExTçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰æ–¹æ³•æ‰©å±•åˆ°æ–‡æœ¬æ•°æ®ï¼Œé€šè¿‡æ’å€¼ç”Ÿæˆæ–°çš„æ–‡æœ¬ç¤ºä¾‹ï¼Œä»¥å¢å¼ºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œæ•°é‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨BERTç”Ÿæˆçš„åµŒå…¥è¿›è¡Œæ’å€¼ï¼Œå¹¶é€šè¿‡xRAGæ¶æ„å°†åµŒå…¥è§£ç ä¸ºè‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨BERTæ¨¡å‹è·å–æ–‡æœ¬ç¤ºä¾‹çš„åµŒå…¥ï¼›å…¶æ¬¡ï¼Œé€šè¿‡xRAGæ¶æ„å°†æ’å€¼åçš„åµŒå…¥è§£ç ä¸ºæ–‡æœ¬ã€‚xRAGçš„è·¨æ¨¡æ€æ£€ç´¢-ç”Ÿæˆæ¡†æ¶ä½¿å¾—è¿™ä¸€è¿‡ç¨‹æ›´åŠ é«˜æ•ˆå’Œè¿è´¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šSMOTExTçš„åˆ›æ–°åœ¨äºå°†SMOTEæ–¹æ³•ä¸ç°ä»£é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬æ•°æ®ä¸Šç”Ÿæˆåˆæˆç¤ºä¾‹ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„SMOTEæ–¹æ³•ä¸åŒï¼Œåè€…é€šå¸¸åº”ç”¨äºæ•°å€¼æ•°æ®ï¼Œç¼ºä¹å¯¹æ–‡æœ¬ç‰¹æ€§çš„è€ƒè™‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„BERTæ¨¡å‹ä½œä¸ºåµŒå…¥ç”Ÿæˆå™¨ï¼Œä»¥åŠåœ¨æ’å€¼è¿‡ç¨‹ä¸­å¦‚ä½•å¹³è¡¡ä¸åŒç¤ºä¾‹çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒxRAGæ¶æ„çš„é€‰æ‹©ä¹Ÿæ˜¯ä¸ºäº†ç¡®ä¿ç”Ÿæˆæ–‡æœ¬çš„è¿è´¯æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…ä½¿ç”¨ç”Ÿæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸ä½¿ç”¨åŸå§‹æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼Œè¡¨æ˜SMOTExTåœ¨æ•°æ®å¢å¼ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åˆæ­¥å®šæ€§åˆ†æç»“æœæ˜¾ç¤ºç”Ÿæˆæ–‡æœ¬çš„è¿è´¯æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§è‰¯å¥½ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨çŸ¥è¯†è’¸é¦å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SMOTExTçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä½èµ„æºè¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æå–å’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡å¢å¼ºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œä»èƒ½å®ç°æœ‰æ•ˆçš„å­¦ä¹ ï¼Œæœªæ¥å¯èƒ½åœ¨æ•°æ®ä¿æŠ¤æ³•è§„æ—¥ç›Šä¸¥æ ¼çš„ç¯å¢ƒä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data scarcity and class imbalance are persistent challenges in training robust NLP models, especially in specialized domains or low-resource settings. We propose a novel technique, SMOTExT, that adapts the idea of Synthetic Minority Over-sampling (SMOTE) to textual data. Our method generates new synthetic examples by interpolating between BERT-based embeddings of two existing examples and then decoding the resulting latent point into text with xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation framework, we can effectively turn interpolated vectors into coherent text. While this is preliminary work supported by qualitative outputs only, the method shows strong potential for knowledge distillation and data augmentation in few-shot settings. Notably, our approach also shows promise for privacy-preserving machine learning: in early experiments, training models solely on generated data achieved comparable performance to models trained on the original dataset. This suggests a viable path toward safe and effective learning under data protection constraints.

