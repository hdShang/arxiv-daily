---
layout: default
title: A Unified Sparse Attention via Multi-Granularity Compression
---

# A Unified Sparse Attention via Multi-Granularity Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14082" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14082</a>
  <a href="https://arxiv.org/pdf/2512.14082.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14082" onclick="toggleFavorite(this, '2512.14082', 'A Unified Sparse Attention via Multi-Granularity Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siran Liu, Zane Cao, Yongchao He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniSparseï¼šä¸€ç§é€šè¿‡å¤šç²’åº¦å‹ç¼©å®ç°çš„ç»Ÿä¸€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ é€Ÿé•¿æ–‡æœ¬å¤„ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–æ³¨æ„åŠ›` `é•¿æ–‡æœ¬å¤„ç†` `å¤šç²’åº¦å‹ç¼©` `å¤åˆtoken` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­é¢ä¸´è®­ç»ƒæˆæœ¬é«˜æˆ–æ•ˆç‡ã€é€šç”¨æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. UniSparseæå‡ºå¤åˆtokenæ¦‚å¿µï¼Œé€šè¿‡å¤šç²’åº¦å‹ç¼©å’Œå—çº§é€‰æ‹©åŠ¨æ€æ„å»ºç¨€ç–æ³¨æ„åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUniSparseåœ¨å¤šç§æ¨¡æ€å’Œä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå…¼é¡¾å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè½®å¯¹è¯å’Œç¨‹åºåˆ†æç­‰åº”ç”¨ä¸­å¯¹é•¿ä¸Šä¸‹æ–‡çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUniSparseçš„ç»Ÿä¸€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•å­˜åœ¨è®­ç»ƒæˆæœ¬é«˜æ˜‚æˆ–ç‰ºç‰²æ•ˆç‡å’Œè·¨æ¨¡æ€é€šç”¨æ€§çš„é—®é¢˜ã€‚UniSparseé€šè¿‡å¼•å…¥å¤åˆtokençš„æ¦‚å¿µæ¥è§£å†³è¿™äº›é™åˆ¶ï¼Œå¤åˆtokenæ˜¯ä¸€ç§èšåˆå¤šç²’åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç´§å‡‘è¡¨ç¤ºã€‚åŸºäºæ­¤ï¼ŒUniSparseé€šè¿‡å¤šç²’åº¦å‹ç¼©å’Œå—çº§é€‰æ‹©åŠ¨æ€æ„å»ºç¨€ç–æ³¨æ„åŠ›ï¼Œä»è€Œåœ¨GPUä¸Šå®ç°é«˜æ•ˆä¸”ç¡¬ä»¶å‹å¥½çš„æ‰§è¡Œã€‚åœ¨ä»åˆæˆåŸºå‡†åˆ°å®é™…åº”ç”¨çš„å¤šä¸ªæ¨¡æ€å’Œä»»åŠ¡ä¸­ï¼ŒUniSparseåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼ˆå¦‚MInferenceã€XAttentionã€FlexPrefillï¼‰ï¼Œå®ç°äº†â‰¥99%çš„å®Œæ•´æ³¨æ„åŠ›å‡†ç¡®ç‡ï¼Œå¹¶ä¸”æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æ¯”FlashAttentionå¿«é«˜è¾¾2.61å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦å‘ˆå¹³æ–¹å¢é•¿ï¼Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è¦ä¹ˆéœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥ä½œä¸ºæ’ä»¶é›†æˆåˆ°å…¶ä»–æ¨¡å‹ä¸­ï¼Œè¦ä¹ˆåœ¨æ¨ç†æ—¶ç‰ºç‰²æ•ˆç‡æˆ–è·¨æ¨¡æ€çš„é€šç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniSparseçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥â€œå¤åˆtokenâ€çš„æ¦‚å¿µï¼Œå°†å¤šä¸ªtokençš„ä¿¡æ¯å‹ç¼©æˆä¸€ä¸ªæ›´ç´§å‡‘çš„è¡¨ç¤ºï¼Œä»è€Œå‡å°‘éœ€è¦è®¡ç®—æ³¨æ„åŠ›çš„tokenæ•°é‡ã€‚é€šè¿‡åœ¨ä¸åŒç²’åº¦ä¸Šè¿›è¡Œå‹ç¼©ï¼ŒUniSparseèƒ½å¤Ÿæ•æ‰ä¸åŒå°ºåº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åŠ¨æ€åœ°é€‰æ‹©é‡è¦çš„ä¿¡æ¯å—è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniSparseä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **å¤šç²’åº¦å‹ç¼©**ï¼šå°†è¾“å…¥åºåˆ—åˆ’åˆ†ä¸ºä¸åŒå¤§å°çš„å—ï¼Œå¹¶ä½¿ç”¨æŸç§å‹ç¼©æ–¹æ³•ï¼ˆä¾‹å¦‚å¹³å‡æ± åŒ–æˆ–çº¿æ€§å˜æ¢ï¼‰å°†æ¯ä¸ªå—å‹ç¼©æˆä¸€ä¸ªå¤åˆtokenã€‚2) **å—çº§é€‰æ‹©**ï¼šæ ¹æ®æŸç§ç­–ç•¥ï¼ˆä¾‹å¦‚åŸºäºé‡è¦æ€§çš„è¯„åˆ†ï¼‰é€‰æ‹©ä¸€éƒ¨åˆ†å¤åˆtokenå‚ä¸åç»­çš„æ³¨æ„åŠ›è®¡ç®—ã€‚3) **ç¨€ç–æ³¨æ„åŠ›è®¡ç®—**ï¼šä»…åœ¨é€‰å®šçš„å¤åˆtokenä¹‹é—´è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚4) **ä¿¡æ¯èšåˆ**ï¼šå°†ç¨€ç–æ³¨æ„åŠ›è®¡ç®—çš„ç»“æœèšåˆå›åŸå§‹çš„tokenè¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šUniSparseçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„å¤šç²’åº¦å‹ç¼©æ¡†æ¶ã€‚å®ƒå…è®¸åœ¨ä¸åŒçš„ç²’åº¦ä¸Šè¿›è¡Œä¿¡æ¯å‹ç¼©ï¼Œä»è€Œèƒ½å¤Ÿçµæ´»åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®ã€‚æ­¤å¤–ï¼ŒUniSparseçš„å—çº§é€‰æ‹©æœºåˆ¶èƒ½å¤ŸåŠ¨æ€åœ°é€‰æ‹©é‡è¦çš„ä¿¡æ¯å—ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒUniSparseæ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„è·¨æ¨¡æ€é€šç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šUniSparseçš„å…·ä½“å®ç°ç»†èŠ‚åŒ…æ‹¬ï¼š1) å‹ç¼©æ–¹æ³•çš„é€‰æ‹©ï¼šå¯ä»¥ä½¿ç”¨å¹³å‡æ± åŒ–ã€çº¿æ€§å˜æ¢æˆ–å…¶ä»–å‹ç¼©æ–¹æ³•ã€‚2) å—å¤§å°çš„é€‰æ‹©ï¼šå¯ä»¥æ ¹æ®ä»»åŠ¡å’Œæ•°æ®è¿›è¡Œè°ƒæ•´ã€‚3) å—çº§é€‰æ‹©ç­–ç•¥ï¼šå¯ä»¥ä½¿ç”¨åŸºäºé‡è¦æ€§çš„è¯„åˆ†ã€éšæœºé€‰æ‹©æˆ–å…¶ä»–ç­–ç•¥ã€‚4) æ³¨æ„åŠ›è®¡ç®—æ–¹å¼ï¼šå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–å˜ä½“ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14082/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14082/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14082/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

UniSparseåœ¨å¤šä¸ªæ¨¡æ€å’Œä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨é•¿æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒUniSparseå®ç°äº†ä¸å®Œæ•´æ³¨æ„åŠ›æœºåˆ¶æ¥è¿‘çš„å‡†ç¡®ç‡ï¼ˆâ‰¥99%ï¼‰ï¼Œå¹¶ä¸”æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æ¯”FlashAttentionå¿«é«˜è¾¾2.61å€ã€‚è¿™äº›ç»“æœè¡¨æ˜UniSparseæ˜¯ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniSparseå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šå¤šè½®å¯¹è¯ç³»ç»Ÿã€ç¨‹åºåˆ†æã€é•¿æ–‡æ¡£æ‘˜è¦ã€è§†é¢‘ç†è§£ç­‰ã€‚é€šè¿‡æé«˜é•¿æ–‡æœ¬å¤„ç†çš„æ•ˆç‡ï¼ŒUniSparseå¯ä»¥å¸®åŠ©LLMæ›´å¥½åœ°ç†è§£å’Œæ¨ç†é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæå‡å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒUniSparseçš„é€šç”¨æ€§ä½¿å…¶å¯ä»¥åº”ç”¨äºä¸åŒçš„æ¨¡æ€ï¼Œä¾‹å¦‚æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\ge$ 99% of full-attention accuracy and up to 2.61$\times$ faster attention computation than FlashAttention.

