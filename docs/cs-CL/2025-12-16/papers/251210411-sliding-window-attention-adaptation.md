---
layout: default
title: Sliding Window Attention Adaptation
---

# Sliding Window Attention Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.10411" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.10411</a>
  <a href="https://arxiv.org/pdf/2512.10411.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10411" onclick="toggleFavorite(this, '2512.10411', 'Sliding Window Attention Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yijiong Yu, Jiale Liu, Qingyun Wu, Huazheng Wang, Ji Pei

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ»‘åŠ¨çª—å£æ³¨æ„åŠ›é€‚é…æ–¹æ³•ï¼Œè§£å†³é¢„è®­ç»ƒLLMé•¿æ–‡æœ¬æ¨ç†æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ»‘åŠ¨çª—å£æ³¨æ„åŠ›` `é•¿æ–‡æœ¬æ¨ç†` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨¡å‹é€‚é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸLLMçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶è®¡ç®—å¤æ‚åº¦é«˜ï¼Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›è™½ç„¶é™ä½äº†å¤æ‚åº¦ï¼Œä½†ç›´æ¥åº”ç”¨äºå®Œæ•´æ³¨æ„åŠ›é¢„è®­ç»ƒçš„æ¨¡å‹ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºæ»‘åŠ¨çª—å£æ³¨æ„åŠ›é€‚é…(SWAA)ï¼Œé€šè¿‡ç»“åˆé¢„å¡«å……SWAã€ä¿ç•™sinkä»¤ç‰Œã€äº¤é”™FA/SWAå±‚ã€CoTå’Œå¾®è°ƒç­‰æ–¹æ³•ï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°é€‚åº”SWAã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSWAAèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤é•¿æ–‡æœ¬æ€§èƒ½ï¼Œå¹¶åˆ†æäº†ä¸åŒé…ç½®çš„æ€§èƒ½-æ•ˆç‡æƒè¡¡ï¼Œä¸ºä¸åŒåœºæ™¯æä¾›äº†æ¨èæ–¹æ¡ˆï¼Œæ¨ç†é€Ÿåº¦æå‡é«˜è¾¾100%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦éšè¾“å…¥é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œè¿™ä½¿å¾—é•¿æ–‡æœ¬æ¨ç†çš„æˆæœ¬å¾ˆé«˜ã€‚æ»‘åŠ¨çª—å£æ³¨æ„åŠ›(SWA)å°†è¿™ç§æˆæœ¬é™ä½åˆ°çº¿æ€§å¤æ‚åº¦ï¼Œä½†æ˜¯å¦‚æœåœ¨æ¨ç†æ—¶å¯¹ä½¿ç”¨å®Œæ•´æ³¨æ„åŠ›(FA)é¢„è®­ç»ƒçš„æ¨¡å‹ç›´æ¥å¯ç”¨SWAï¼Œä¼šå¯¼è‡´ç”±äºè®­ç»ƒ-æ¨ç†ä¸åŒ¹é…è€Œé€ æˆçš„ä¸¥é‡çš„ä¸Šä¸‹æ–‡æ€§èƒ½ä¸‹é™ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æ€è€ƒï¼šæ— éœ€é¢„è®­ç»ƒï¼ŒFAé¢„è®­ç»ƒçš„LLMèƒ½å¦å¾ˆå¥½åœ°é€‚åº”SWAï¼Ÿæˆ‘ä»¬é€šè¿‡æå‡ºæ»‘åŠ¨çª—å£æ³¨æ„åŠ›é€‚é…(SWAA)æ¥ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼ŒSWAAæ˜¯ä¸€ç»„å®ç”¨çš„æ–¹æ³•ï¼Œç»“åˆäº†äº”ç§æ–¹æ³•ä»¥å®ç°æ›´å¥½çš„é€‚é…ï¼š(1)ä»…åœ¨é¢„å¡«å……æœŸé—´åº”ç”¨SWAï¼›(2)ä¿ç•™â€œsinkâ€ä»¤ç‰Œï¼›(3)äº¤é”™FA/SWAå±‚ï¼›(4)æ€ç»´é“¾(CoT)ï¼›(5)å¾®è°ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒSWAé€‚é…æ˜¯å¯è¡Œä½†é‡è¦çš„ï¼šæ²¡æœ‰å•ä¸€æ–¹æ³•è¶³å¤Ÿï¼Œä½†ç‰¹å®šçš„ååŒç»„åˆå¯ä»¥æœ‰æ•ˆåœ°æ¢å¤åŸå§‹çš„é•¿æ–‡æœ¬æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†ä¸åŒSWAAé…ç½®çš„æ€§èƒ½-æ•ˆç‡æƒè¡¡ï¼Œå¹¶ä¸ºå„ç§åœºæ™¯æä¾›äº†æ¨èçš„æ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ¡ˆå¯ä»¥æå¤§åœ°å¹¶ä»æ ¹æœ¬ä¸Šå°†LLMé•¿æ–‡æœ¬æ¨ç†é€Ÿåº¦æé«˜é«˜è¾¾100%ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å°†å®Œæ•´æ³¨æ„åŠ›ï¼ˆFAï¼‰é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›´æ¥åº”ç”¨äºæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆSWAï¼‰æ¨ç†æ—¶ï¼Œç”±äºè®­ç»ƒå’Œæ¨ç†æ–¹å¼ä¸åŒ¹é…å¯¼è‡´çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦é‡æ–°é¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆç›´æ¥åº”ç”¨SWAå¯¼è‡´é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›ä¸¥é‡å—æŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ç³»åˆ—é€‚é…æ–¹æ³•ï¼Œå¼¥åˆFAé¢„è®­ç»ƒå’ŒSWAæ¨ç†ä¹‹é—´çš„å·®è·ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¤§è§„æ¨¡é‡æ–°é¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåˆ©ç”¨SWAçš„çº¿æ€§å¤æ‚åº¦ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æ¢å¤åŸæœ‰çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚è¿™ç§é€‚é…çš„å…³é”®åœ¨äºæ‰¾åˆ°ä¸€ç§ç­–ç•¥ï¼Œæ—¢èƒ½åˆ©ç”¨SWAçš„æ•ˆç‡ï¼Œåˆèƒ½é¿å…å› æ³¨æ„åŠ›æ¨¡å¼æ”¹å˜å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSWAAæ¡†æ¶åŒ…å«äº”ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œå®ƒä»¬ååŒå·¥ä½œä»¥å®ç°æ›´å¥½çš„é€‚é…ï¼š1) ä»…åœ¨é¢„å¡«å……é˜¶æ®µåº”ç”¨SWAï¼Œå‡å°‘è®¡ç®—é‡ï¼›2) ä¿ç•™â€œsinkâ€ä»¤ç‰Œï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰å…¨å±€ä¿¡æ¯ï¼›3) äº¤é”™ä½¿ç”¨FAå’ŒSWAå±‚ï¼Œå¹³è¡¡å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›ï¼›4) åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼›5) è¿›è¡Œå¾®è°ƒï¼Œä½¿æ¨¡å‹é€‚åº”SWAçš„æ³¨æ„åŠ›æ¨¡å¼ã€‚è¿™äº›æ–¹æ³•å¯ä»¥çµæ´»ç»„åˆï¼Œä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯å’Œæ€§èƒ½éœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»¼åˆæ€§çš„é€‚é…æ¡†æ¶SWAAï¼Œå®ƒä¸æ˜¯ä¾èµ–å•ä¸€çš„æŠ€æœ¯æ‰‹æ®µï¼Œè€Œæ˜¯é€šè¿‡å¤šç§æ–¹æ³•çš„ååŒä½œç”¨ï¼Œå®ç°äº†FAé¢„è®­ç»ƒæ¨¡å‹åˆ°SWAæ¨ç†çš„å¹³æ»‘è¿‡æ¸¡ã€‚è¿™ç§ç»„åˆå¼çš„é€‚é…ç­–ç•¥ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³è®­ç»ƒ-æ¨ç†ä¸åŒ¹é…çš„é—®é¢˜ï¼Œå¹¶ä¸”å…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é¢„å¡«å……é˜¶æ®µSWAçš„åº”ç”¨ç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘åˆå§‹é˜¶æ®µçš„è®¡ç®—è´Ÿæ‹…ï¼›2) â€œsinkâ€ä»¤ç‰Œçš„ä¿ç•™æœºåˆ¶ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›3) FAå’ŒSWAå±‚çš„äº¤é”™æ¯”ä¾‹ï¼Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹å’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼›4) CoTæç¤ºçš„è®¾è®¡ï¼Œæ—¨åœ¨å¼•å¯¼æ¨¡å‹è¿›è¡Œæ›´æœ‰æ•ˆçš„æ¨ç†ï¼›5) å¾®è°ƒé˜¶æ®µçš„å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ç­‰è¶…å‚æ•°çš„è®¾ç½®ï¼Œéœ€è¦æ ¹æ®å®éªŒç»“æœè¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSWAAèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤FAé¢„è®­ç»ƒæ¨¡å‹åœ¨SWAæ¨ç†ä¸‹çš„é•¿æ–‡æœ¬æ€§èƒ½ã€‚åœ¨ç‰¹å®šé…ç½®ä¸‹ï¼Œæ¨ç†é€Ÿåº¦æå‡é«˜è¾¾100%ã€‚è®ºæ–‡è¿˜åˆ†æäº†ä¸åŒSWAAé…ç½®çš„æ€§èƒ½-æ•ˆç‡æƒè¡¡ï¼Œå¹¶ä¸ºä¸åŒåœºæ™¯æä¾›äº†æ¨èæ–¹æ¡ˆï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚é•¿æ–‡æ¡£æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡åŠ é€ŸLLMçš„æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥é™ä½éƒ¨ç½²æˆæœ¬ï¼Œæé«˜ç”¨æˆ·ä½“éªŒï¼Œå¹¶æ¨åŠ¨LLMåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿä¸ºå…¶ä»–æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–å’Œé€‚é…æä¾›äº†å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios, which can greatly and fundamentally accelerate LLM long-context inference speed by up to 100%. Our code is available atthis https URL

