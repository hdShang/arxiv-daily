---
layout: default
title: TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation
---

# TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.23979" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.23979</a>
  <a href="https://arxiv.org/pdf/2506.23979.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.23979" onclick="toggleFavorite(this, '2506.23979', 'TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Renren Jin, Tianhao Shen, Xinwei Wu, Dan Shi, Haoran Sun, Yuqi Ren, Wuwei Huang, Quandong Wang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TaPï¼šä¸€ç§åŸºäºåˆ†ç±»æ³•çš„è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„åå¥½æ•°æ®ç”Ÿæˆæ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åå¥½å­¦ä¹ ` `æ•°æ®ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆ†ç±»æ³•` `è‡ªåŠ¨åŒ–` `å¤šè¯­è¨€` `æŒ‡ä»¤å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå¾®è°ƒæ•°æ®é›†æ„å»ºæˆæœ¬é«˜æ˜‚ï¼Œä¸”å¤šä¸ºè‹±æ–‡ï¼Œé™åˆ¶äº†å¤šè¯­è¨€LLMçš„å‘å±•ã€‚
2. TaPæ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–åˆ†ç±»æ³•ï¼Œå®ç°å¯¹æ•°æ®é›†ç»„æˆçš„ç»†ç²’åº¦æ§åˆ¶ï¼Œä¿è¯æ•°æ®å¤šæ ·æ€§å’Œè¦†ç›–ç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨TaPç”Ÿæˆçš„æ•°æ®é›†è®­ç»ƒçš„LLMï¼Œæ€§èƒ½è¶…è¶Šäº†ä½¿ç”¨æ›´å¤§è§„æ¨¡å¼€æºæ•°æ®é›†è®­ç»ƒçš„LLMã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éµå¾ªæŒ‡ä»¤å’Œä¸äººç±»åå¥½åŠä»·å€¼è§‚å¯¹é½çš„èƒ½åŠ›ï¼Œéœ€è¦åœ¨å…¶ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒå’Œåå¥½å¾®è°ƒï¼Œè¿™éœ€è¦é«˜è´¨é‡çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œæ„å»ºæ­¤ç±»æ•°æ®é›†éœ€è¦è€—è´¹å¤§é‡èµ„æºï¼Œå¹¶ä¸”å¤§å¤šæ•°å¯ç”¨çš„æœ‰ç›‘ç£å’Œåå¥½å¾®è°ƒæ•°æ®é›†éƒ½æ˜¯è‹±æ–‡çš„ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåˆ†ç±»æ³•çš„åå¥½æ•°æ®ç”Ÿæˆï¼ˆTaPï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰åŠ©äºè·¨å„ç§è¯­è¨€è‡ªåŠ¨ä¸”å¯æ‰©å±•åœ°æ„å»ºåå¥½æ•°æ®é›†ã€‚TaPåŸºäºç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼Œå¯ä»¥å¯¹æ•°æ®é›†çš„ç»„æˆè¿›è¡Œç»†ç²’åº¦æ§åˆ¶ï¼Œä»è€Œç¡®ä¿å¤šæ ·æ€§å’Œå…¨é¢çš„è¦†ç›–ã€‚æˆ‘ä»¬ä½¿ç”¨TaPç”Ÿæˆçš„æ•°æ®é›†å¯¹å„ç§LLMè¿›è¡Œæœ‰ç›‘ç£å’Œåå¥½å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨TaPç”Ÿæˆçš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„LLMä¼˜äºåœ¨ç°æœ‰å¼€æºæ•°æ®é›†ä¸Šè®­ç»ƒçš„LLMã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨TaPç”Ÿæˆçš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„LLMçš„æ€§èƒ½è¶…è¿‡äº†åœ¨è§„æ¨¡å¤§180å€çš„å¼€æºæ•°æ®é›†ä¸Šè®­ç»ƒçš„LLMã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ‰ç›‘ç£å¾®è°ƒå’Œåå¥½å¾®è°ƒæ‰€éœ€çš„é«˜è´¨é‡æ•°æ®é›†çš„æ„å»ºé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é¢ä¸´ä¸¤ä¸ªç—›ç‚¹ï¼šä¸€æ˜¯æ•°æ®é›†æ„å»ºæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¤§é‡äººå·¥æ ‡æ³¨ï¼›äºŒæ˜¯ç°æœ‰æ•°æ®é›†å¤§å¤šä¸ºè‹±æ–‡ï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€çš„æ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼ˆTaxonomyï¼‰æ¥æŒ‡å¯¼åå¥½æ•°æ®çš„è‡ªåŠ¨ç”Ÿæˆã€‚é€šè¿‡åˆ†ç±»æ³•ï¼Œå¯ä»¥å¯¹ç”Ÿæˆçš„æ•°æ®é›†è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ï¼Œä»è€Œä¿è¯æ•°æ®é›†çš„å¤šæ ·æ€§å’Œè¦†ç›–ç‡ï¼ŒåŒæ—¶é™ä½äººå·¥æ ‡æ³¨çš„æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTaPæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) åˆ†ç±»æ³•æ„å»ºæ¨¡å—ï¼šå®šä¹‰æ•°æ®é›†çš„ç»“æ„åŒ–åˆ†ç±»ä½“ç³»ï¼Œä¾‹å¦‚ä¸»é¢˜ã€é£æ ¼ã€éš¾åº¦ç­‰ï¼›2) æ•°æ®ç”Ÿæˆæ¨¡å—ï¼šåŸºäºåˆ†ç±»æ³•ï¼Œåˆ©ç”¨LLMè‡ªåŠ¨ç”Ÿæˆå€™é€‰æ•°æ®ï¼›3) åå¥½æ’åºæ¨¡å—ï¼šå¯¹ç”Ÿæˆçš„æ•°æ®è¿›è¡Œæ’åºï¼Œé€‰å‡ºç¬¦åˆäººç±»åå¥½çš„æ•°æ®ï¼›4) æ•°æ®é›†æ„å»ºæ¨¡å—ï¼šå°†æ’åºåçš„æ•°æ®æ„å»ºæˆæœ€ç»ˆçš„åå¥½æ•°æ®é›†ã€‚æ•´ä¸ªæµç¨‹æ—¨åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆé«˜è´¨é‡ã€å¤šè¯­è¨€çš„åå¥½æ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šTaPæ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†åˆ†ç±»æ³•æ¥æŒ‡å¯¼åå¥½æ•°æ®çš„ç”Ÿæˆã€‚ä¸ä»¥å¾€éšæœºç”Ÿæˆæˆ–äººå·¥æ ‡æ³¨çš„æ–¹æ³•ç›¸æ¯”ï¼ŒTaPèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ§åˆ¶æ•°æ®é›†çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œå¹¶æ˜¾è‘—é™ä½äº†æ•°æ®æ„å»ºçš„æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåˆ†ç±»æ³•çš„å…·ä½“è®¾è®¡æ˜¯å…³é”®ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„åº”ç”¨åœºæ™¯å®šä¹‰ä¸åŒçš„åˆ†ç±»ç»´åº¦ï¼Œå¹¶ä¸ºæ¯ä¸ªç»´åº¦è®¾ç½®ä¸åŒçš„å–å€¼èŒƒå›´ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—å¯ä»¥ä½¿ç”¨ä¸åŒçš„LLMå’Œç”Ÿæˆç­–ç•¥ï¼Œåå¥½æ’åºæ¨¡å—å¯ä»¥ä½¿ç”¨äººå·¥æ ‡æ³¨æˆ–è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„ä½¿ç”¨å–å†³äºå…·ä½“çš„å®ç°ç»†èŠ‚ï¼Œè®ºæ–‡ä¸­å¯èƒ½æœªè¯¦ç»†è¯´æ˜ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2506.23979/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2506.23979/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2506.23979/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨TaPç”Ÿæˆçš„æ•°æ®é›†è®­ç»ƒçš„LLMï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä½¿ç”¨ç°æœ‰å¼€æºæ•°æ®é›†è®­ç»ƒçš„LLMã€‚æ›´ä»¤äººç©ç›®çš„æ˜¯ï¼Œä½¿ç”¨TaPç”Ÿæˆçš„æ•°æ®é›†è®­ç»ƒçš„LLMï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¿‡äº†ä½¿ç”¨è§„æ¨¡å¤§180å€çš„å¼€æºæ•°æ®é›†è®­ç»ƒçš„LLMï¼Œè¿™å……åˆ†è¯æ˜äº†TaPæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TaPæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºå„ç§è¯­è¨€çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚é€šè¿‡è‡ªåŠ¨åŒ–ç”Ÿæˆé«˜è´¨é‡çš„åå¥½æ•°æ®é›†ï¼Œå¯ä»¥æ˜¾è‘—é™ä½LLMè®­ç»ƒçš„æˆæœ¬ï¼Œå¹¶æå‡å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è¿˜æœ‰åŠ©äºæ„å»ºæ›´ç¬¦åˆäººç±»ä»·å€¼è§‚å’Œåå¥½çš„LLMï¼Œä¿ƒè¿›äººæœºåä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Conducting supervised fine-tuning and preference fine-tuning on large language models (LLMs) requires high-quality datasets to improve their ability to follow instructions and align with human preferences and values. However, constructing such datasets is resource-intensive, and most available datasets for supervised and preference fine-tuning are in English. To address these challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided \underline{\textbf{P}}reference Data Generation (TaP) framework, which facilitates automated and scalable construction of preference datasets across various languages. TaP is grounded in a structured taxonomy that allows fine-grained control over dataset composition, thereby ensuring both diversity and comprehensive coverage. We employ TaP-generated datasets to perform supervised and preference fine-tuning on various LLMs. Experimental results demonstrate that LLMs trained on TaP-generated datasets outperform those trained on existing open-source datasets. Remarkably, LLMs trained on TaP-generated datasets surpass the performance of those trained on an open-source dataset that is 180 times larger.

