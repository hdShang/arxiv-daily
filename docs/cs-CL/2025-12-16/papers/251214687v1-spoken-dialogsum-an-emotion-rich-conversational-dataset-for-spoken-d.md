---
layout: default
title: Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization
---

# Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization

**arXiv**: [2512.14687v1](https://arxiv.org/abs/2512.14687) | [PDF](https://arxiv.org/pdf/2512.14687.pdf)

**ä½œè€…**: Yen-Ju Lu, Kunxiao Gao, Mingrui Liang, Helin Wang, Thomas Thebaud, Laureano Moro-Velazquez, Najim Dehak, Jesus Villalba

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 12 pages, 2 figures

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpoken DialogSumï¼Œä¸€ä¸ªå¯Œå«æƒ…æ„Ÿçš„å£è¯­å¯¹è¯æ‘˜è¦æ•°æ®é›†ï¼Œä¿ƒè¿›æƒ…æ„Ÿæ„ŸçŸ¥å£è¯­å¯¹è¯æ‘˜è¦ç ”ç©¶ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **åŠ¨ä½œç”Ÿæˆä¸Žç‰©ç†åŠ¨ç”» (Animation & Physics)**

**å…³é”®è¯**: `å£è¯­å¯¹è¯æ‘˜è¦` `æƒ…æ„Ÿè¯†åˆ«` `è¯­éŸ³åˆæˆ` `å¤šæ¨¡æ€æ•°æ®é›†` `å¤§åž‹è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æƒ…æ„Ÿæ„ŸçŸ¥æˆ–å£è¯­å¯¹è¯æ‘˜è¦ç ”ç©¶å—é™äºŽç¼ºä¹è¿žæŽ¥è¯­éŸ³ã€æ‘˜è¦å’Œå‰¯è¯­è¨€çº¿ç´¢çš„æ•°æ®ã€‚
2. è®ºæ–‡æå‡ºSpoken DialogSumæ•°æ®é›†ï¼Œé€šè¿‡LLMé‡å†™è„šæœ¬å¹¶ä½¿ç”¨TTSå¼•æ“Žåˆæˆè¯­éŸ³ï¼Œå®žçŽ°éŸ³é¢‘ä¸Žæƒ…æ„Ÿä¿¡æ¯çš„å¯¹é½ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œç«¯åˆ°ç«¯Audio-LLMåœ¨æƒ…æ„Ÿæ‘˜è¦ä»»åŠ¡ä¸Šä¼˜äºŽçº§è”ASR-LLMç³»ç»Ÿï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»Spoken DialogSumï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†åŽŸå§‹å¯¹è¯éŸ³é¢‘ä¸Žäº‹å®žæ‘˜è¦ã€å¯Œå«æƒ…æ„Ÿçš„æ‘˜è¦ä»¥åŠè¯è¯­çº§åˆ«çš„è¯´è¯äººå¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿæ ‡ç­¾å¯¹é½çš„è¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†åˆ†ä¸¤ä¸ªé˜¶æ®µæž„å»ºï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ç”¨Switchboardé£Žæ ¼çš„å¡«å……è¯å’ŒåŽé€šé“é‡å†™DialogSumè„šæœ¬ï¼Œç„¶åŽæ ‡è®°æ¯ä¸ªè¯è¯­çš„æƒ…æ„Ÿã€éŸ³é«˜å’Œè¯­é€Ÿã€‚å…¶æ¬¡ï¼Œä¸€ä¸ªå¯Œæœ‰è¡¨çŽ°åŠ›çš„TTSå¼•æ“Žä»Žæ ‡è®°çš„è„šæœ¬åˆæˆè¯­éŸ³ï¼Œå¹¶ä¸Žå‰¯è¯­è¨€æ ‡ç­¾å¯¹é½ã€‚Spoken DialogSumåŒ…å«13,460ä¸ªæƒ…æ„Ÿå¤šæ ·çš„å¯¹è¯ï¼Œæ¯ä¸ªå¯¹è¯éƒ½é…æœ‰ä¸€ä¸ªäº‹å®žæ‘˜è¦å’Œä¸€ä¸ªæƒ…æ„Ÿèšç„¦æ‘˜è¦ã€‚æ•°æ®é›†å·²åœ¨çº¿å‘å¸ƒã€‚åŸºçº¿å®žéªŒè¡¨æ˜Žï¼Œä¸Žçº§è”çš„ASR-LLMç³»ç»Ÿç›¸æ¯”ï¼ŒAudio-LLMå°†æƒ…æ„Ÿæ‘˜è¦ROUGE-Læé«˜äº†28%ï¼Œè¯å®žäº†ç«¯åˆ°ç«¯è¯­éŸ³å»ºæ¨¡çš„ä»·å€¼ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æƒ…æ„Ÿæ„ŸçŸ¥å£è¯­å¯¹è¯æ‘˜è¦ç ”ç©¶ç¼ºä¹é«˜è´¨é‡æ•°æ®é›†ï¼Œéš¾ä»¥æœ‰æ•ˆè®­ç»ƒå’Œè¯„ä¼°ç›¸å…³æ¨¡åž‹ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºŽæ–‡æœ¬æ•°æ®ï¼Œå¿½ç•¥äº†è¯­éŸ³ä¸­çš„æƒ…æ„Ÿä¿¡æ¯ï¼Œå¯¼è‡´æ‘˜è¦çš„æƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›ä¸è¶³ã€‚å› æ­¤ï¼Œå¦‚ä½•æž„å»ºä¸€ä¸ªåŒ…å«è¯­éŸ³ã€æ–‡æœ¬å’Œæƒ…æ„Ÿä¿¡æ¯çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ˜¯è§£å†³æƒ…æ„Ÿæ„ŸçŸ¥å£è¯­å¯¹è¯æ‘˜è¦é—®é¢˜çš„å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“åˆå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªåŒ…å«ä¸°å¯Œæƒ…æ„Ÿä¿¡æ¯çš„å£è¯­å¯¹è¯æ•°æ®é›†ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆåˆ©ç”¨LLMå¯¹çŽ°æœ‰å¯¹è¯è„šæœ¬è¿›è¡Œæ”¹å†™ï¼Œå¢žåŠ å£è¯­åŒ–çš„å¡«å……è¯å’ŒåŽé€šé“ï¼Œä½¿å…¶æ›´æŽ¥è¿‘çœŸå®žçš„å£è¯­å¯¹è¯ã€‚ç„¶åŽï¼Œä½¿ç”¨TTSå¼•æ“Žå°†æ”¹å†™åŽçš„è„šæœ¬åˆæˆä¸ºè¯­éŸ³ï¼Œå¹¶å¯¹æ¯ä¸ªè¯è¯­è¿›è¡Œæƒ…æ„Ÿã€éŸ³é«˜å’Œè¯­é€Ÿç­‰å‰¯è¯­è¨€ä¿¡æ¯çš„æ ‡æ³¨ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSpoken DialogSumæ•°æ®é›†çš„æž„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š
1. **è„šæœ¬æ”¹å†™**ï¼šä½¿ç”¨LLMå¯¹DialogSumæ•°æ®é›†ä¸­çš„å¯¹è¯è„šæœ¬è¿›è¡Œæ”¹å†™ï¼Œå¢žåŠ å£è¯­åŒ–çš„å…ƒç´ ã€‚
2. **æƒ…æ„Ÿæ ‡æ³¨**ï¼šå¯¹æ¯ä¸ªè¯è¯­è¿›è¡Œæƒ…æ„Ÿã€å¹´é¾„ã€æ€§åˆ«ç­‰ä¿¡æ¯çš„æ ‡æ³¨ã€‚
3. **è¯­éŸ³åˆæˆ**ï¼šä½¿ç”¨TTSå¼•æ“Žå°†æ ‡æ³¨åŽçš„è„šæœ¬åˆæˆä¸ºè¯­éŸ³ã€‚
4. **æ‘˜è¦ç”Ÿæˆ**ï¼šä¸ºæ¯ä¸ªå¯¹è¯ç”Ÿæˆäº‹å®žæ‘˜è¦å’Œæƒ…æ„Ÿæ‘˜è¦ã€‚
æ•´ä¸ªæµç¨‹æ—¨åœ¨åˆ›å»ºä¸€ä¸ªåŒ…å«è¯­éŸ³ã€æ–‡æœ¬å’Œæƒ…æ„Ÿä¿¡æ¯çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç”¨äºŽè®­ç»ƒå’Œè¯„ä¼°æƒ…æ„Ÿæ„ŸçŸ¥å£è¯­å¯¹è¯æ‘˜è¦æ¨¡åž‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨æž„å»ºæƒ…æ„Ÿä¸°å¯Œå£è¯­å¯¹è¯æ•°æ®é›†çš„æ–¹æ³•ã€‚ä¸Žä¼ ç»Ÿçš„æ‰‹åŠ¨æ ‡æ³¨æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¤§å¤§é™ä½Žæ•°æ®é›†çš„æž„å»ºæˆæœ¬ï¼Œå¹¶æé«˜æ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜åŒ…å«äº†è¯­éŸ³ã€æ–‡æœ¬å’Œæƒ…æ„Ÿä¿¡æ¯ç­‰å¤šæ¨¡æ€æ•°æ®ï¼Œä¸ºæƒ…æ„Ÿæ„ŸçŸ¥å£è¯­å¯¹è¯æ‘˜è¦ç ”ç©¶æä¾›äº†æ–°çš„èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è„šæœ¬æ”¹å†™é˜¶æ®µï¼Œä½¿ç”¨äº†LLMæ¥ç”ŸæˆSwitchboardé£Žæ ¼çš„å¡«å……è¯å’ŒåŽé€šé“ï¼Œä»¥æ¨¡æ‹ŸçœŸå®žçš„å£è¯­å¯¹è¯ã€‚åœ¨æƒ…æ„Ÿæ ‡æ³¨é˜¶æ®µï¼Œä½¿ç”¨äº†é¢„è®­ç»ƒçš„æƒ…æ„Ÿåˆ†ç±»æ¨¡åž‹æ¥è‡ªåŠ¨æ ‡æ³¨æ¯ä¸ªè¯è¯­çš„æƒ…æ„Ÿã€‚åœ¨è¯­éŸ³åˆæˆé˜¶æ®µï¼Œä½¿ç”¨äº†å¯Œæœ‰è¡¨çŽ°åŠ›çš„TTSå¼•æ“Žæ¥åˆæˆå…·æœ‰ä¸åŒæƒ…æ„Ÿçš„è¯­éŸ³ã€‚è¿™äº›å…³é”®è®¾è®¡ä¿è¯äº†æ•°æ®é›†çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œä½¿ç”¨Audio-LLMç›´æŽ¥å¤„ç†è¯­éŸ³ä¿¡å·ï¼Œç›¸æ¯”äºŽå…ˆä½¿ç”¨ASRè½¬å½•å†ç”¨LLMå¤„ç†æ–‡æœ¬çš„æ–¹æ³•ï¼Œåœ¨æƒ…æ„Ÿæ‘˜è¦ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒROUGE-LæŒ‡æ ‡æé«˜äº†28%ã€‚è¿™è¡¨æ˜Žç«¯åˆ°ç«¯è¯­éŸ³å»ºæ¨¡åœ¨æƒ…æ„Ÿæ„ŸçŸ¥å£è¯­å¯¹è¯æ‘˜è¦ä»»åŠ¡ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œå¹¶éªŒè¯äº†Spoken DialogSumæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

Spoken DialogSumæ•°æ®é›†å¯åº”ç”¨äºŽæƒ…æ„Ÿæ„ŸçŸ¥å£è¯­å¯¹è¯æ‘˜è¦ã€æƒ…æ„Ÿè¯†åˆ«ã€è¯­éŸ³æƒ…æ„Ÿåˆ†æžç­‰é¢†åŸŸã€‚è¯¥æ•°æ®é›†èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å…·æƒ…æ„Ÿç†è§£èƒ½åŠ›çš„å¯¹è¯ç³»ç»Ÿå’Œè¯­éŸ³åŠ©æ‰‹ï¼Œæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæµç•…æ€§ã€‚æœªæ¥ï¼Œè¯¥æ•°æ®é›†è¿˜å¯ç”¨äºŽè®­ç»ƒæ›´å¼ºå¤§çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æžæ¨¡åž‹ï¼Œåº”ç”¨äºŽå¿ƒç†å¥åº·ç›‘æµ‹ã€å®¢æˆ·æœåŠ¡ç­‰é¢†åŸŸã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.

