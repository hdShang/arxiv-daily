---
layout: default
title: Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models
---

# Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13980" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13980</a>
  <a href="https://arxiv.org/pdf/2512.13980.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13980" onclick="toggleFavorite(this, '2512.13980', 'Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhimin Qiu, Di Wu, Feng Liu, Chenrui Hu, Yuxiao Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„æ„ŸçŸ¥è§£ç æ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è§£å†³å¤æ‚å®ä½“æŠ½å–ä¸­çš„è¯­ä¹‰å®Œæ•´æ€§å’Œç»“æ„ä¸€è‡´æ€§é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®ä½“æŠ½å–` `å¤§è¯­è¨€æ¨¡å‹` `ç»“æ„æ„ŸçŸ¥è§£ç ` `åµŒå¥—å®ä½“` `é‡å å®ä½“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†åµŒå¥—å’Œé‡å å®ä½“æŠ½å–æ—¶ï¼Œéš¾ä»¥å…¼é¡¾è¯­ä¹‰å®Œæ•´æ€§å’Œç»“æ„ä¸€è‡´æ€§ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
2. è®ºæ–‡æå‡ºç»“æ„æ„ŸçŸ¥è§£ç æ–¹æ³•ï¼Œé€šè¿‡å€™é€‰è·¨åº¦ç”Ÿæˆå’Œç»“æ„åŒ–æ³¨æ„åŠ›å»ºæ¨¡ï¼Œç»Ÿä¸€å»ºæ¨¡å®ä½“è¾¹ç•Œã€å±‚çº§å…³ç³»å’Œäº¤å‰ä¾èµ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ACE 2005æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†åµŒå¥—å’Œé‡å å®ä½“è¯†åˆ«çš„å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç»“æ„æ„ŸçŸ¥è§£ç æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨åµŒå¥—å’Œé‡å å®ä½“æŠ½å–ä»»åŠ¡ä¸­éš¾ä»¥åŒæ—¶ä¿æŒè¯­ä¹‰å®Œæ•´æ€§å’Œç»“æ„ä¸€è‡´æ€§çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å€™é€‰è·¨åº¦ç”Ÿæˆæœºåˆ¶å’Œç»“æ„åŒ–æ³¨æ„åŠ›å»ºæ¨¡ï¼Œå®ç°äº†å®ä½“è¾¹ç•Œã€å±‚çº§å…³ç³»å’Œäº¤å‰ä¾èµ–çš„ç»Ÿä¸€å»ºæ¨¡ã€‚æ¨¡å‹é¦–å…ˆä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è·å–ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­ä¹‰è¡¨ç¤ºï¼Œç„¶åé€šè¿‡å€™é€‰è¡¨ç¤ºç»„åˆæ•è·å¤šç²’åº¦çš„å®ä½“è·¨åº¦ç‰¹å¾ï¼Œå¹¶åœ¨è§£ç è¿‡ç¨‹ä¸­å¼•å…¥å±‚çº§ç»“æ„çº¦æŸï¼Œä»¥ç¡®ä¿è¯­ä¹‰å’Œç»“æ„ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¢å¼ºåœ¨å¤æ‚åœºæ™¯ä¸­çš„ç¨³å®šæ€§ï¼Œæ¨¡å‹è”åˆä¼˜åŒ–åˆ†ç±»æŸå¤±å’Œç»“æ„ä¸€è‡´æ€§æŸå¤±ï¼Œä»è€Œåœ¨å¤šå®ä½“å…±ç°å’Œé•¿å¥ä¾èµ–æ¡ä»¶ä¸‹ä¿æŒè¾ƒé«˜çš„è¯†åˆ«ç²¾åº¦ã€‚åœ¨ACE 2005æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å€¼æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨åµŒå¥—å’Œé‡å å®ä½“è¯†åˆ«æ–¹é¢ï¼Œæ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„è¾¹ç•Œå®šä½å’Œç»“æ„å»ºæ¨¡èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶éªŒè¯äº†ç»“æ„æ„ŸçŸ¥è§£ç åœ¨å¤æ‚è¯­ä¹‰æŠ½å–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘å…·æœ‰å±‚çº§ç†è§£èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºé«˜ç²¾åº¦ä¿¡æ¯æŠ½å–å¥ å®šäº†æ–¹æ³•è®ºåŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤æ‚å®ä½“æŠ½å–ä»»åŠ¡ä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥åŒæ—¶ä¿è¯è¯­ä¹‰å®Œæ•´æ€§å’Œç»“æ„ä¸€è‡´æ€§çš„é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼ŒåµŒå¥—å®ä½“å’Œé‡å å®ä½“çš„è¯†åˆ«å¯¹æ¨¡å‹æå‡ºäº†æ›´é«˜çš„è¦æ±‚ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥å‡†ç¡®æ•æ‰å®ä½“é—´çš„å±‚çº§å…³ç³»å’Œä¾èµ–å…³ç³»ï¼Œå¯¼è‡´æŠ½å–æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¼ºå¤§çš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼•å…¥ç»“æ„æ„ŸçŸ¥è§£ç æœºåˆ¶ï¼Œæ˜¾å¼åœ°å»ºæ¨¡å®ä½“é—´çš„å±‚çº§ç»“æ„å’Œä¾èµ–å…³ç³»ã€‚é€šè¿‡ç»“æ„åŒ–çš„è§£ç è¿‡ç¨‹ï¼Œç¡®ä¿æŠ½å–çš„å®ä½“åœ¨è¯­ä¹‰ä¸Šæ˜¯å®Œæ•´çš„ï¼Œåœ¨ç»“æ„ä¸Šæ˜¯ä¸€è‡´çš„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼šç”¨äºè·å–è¾“å…¥æ–‡æœ¬çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­ä¹‰è¡¨ç¤ºã€‚2) å€™é€‰è·¨åº¦ç”Ÿæˆï¼šç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å®ä½“è·¨åº¦ï¼Œä½œä¸ºåç»­è§£ç çš„å€™é€‰ã€‚3) ç»“æ„åŒ–æ³¨æ„åŠ›å»ºæ¨¡ï¼šåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡å®ä½“è·¨åº¦ä¹‹é—´çš„å±‚çº§å…³ç³»å’Œäº¤å‰ä¾èµ–ã€‚4) ç»“æ„æ„ŸçŸ¥è§£ç ï¼šåŸºäºç»“æ„åŒ–æ³¨æ„åŠ›å»ºæ¨¡çš„ç»“æœï¼Œè¿›è¡Œå®ä½“ç±»å‹çš„é¢„æµ‹å’Œç»“æ„å…³ç³»çš„æ¨æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºç»“æ„æ„ŸçŸ¥è§£ç æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„åºåˆ—è§£ç æˆ–åŸºäºè·¨åº¦çš„è§£ç æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•æ˜¾å¼åœ°å»ºæ¨¡äº†å®ä½“ä¹‹é—´çš„ç»“æ„å…³ç³»ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†åµŒå¥—å’Œé‡å å®ä½“ã€‚æ­¤å¤–ï¼Œå€™é€‰è·¨åº¦ç”Ÿæˆæœºåˆ¶ä¹Ÿé¿å…äº†é—æ¼æ½œåœ¨å®ä½“çš„å¯èƒ½æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨è”åˆä¼˜åŒ–ç­–ç•¥ï¼ŒåŒæ—¶ä¼˜åŒ–åˆ†ç±»æŸå¤±å’Œç»“æ„ä¸€è‡´æ€§æŸå¤±ã€‚åˆ†ç±»æŸå¤±ç”¨äºæŒ‡å¯¼å®ä½“ç±»å‹çš„é¢„æµ‹ï¼Œç»“æ„ä¸€è‡´æ€§æŸå¤±ç”¨äºçº¦æŸå®ä½“ä¹‹é—´çš„ç»“æ„å…³ç³»ã€‚åœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼Œä½¿ç”¨äº†å¤šå±‚Transformerç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œè¶…å‚æ•°çš„é€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“çš„å®éªŒè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ACE 2005æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å€¼æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨åµŒå¥—å’Œé‡å å®ä½“è¯†åˆ«æ–¹é¢ã€‚ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œè¯¥æ–¹æ³•åœ¨F1å€¼ä¸Šå–å¾—äº†æ˜æ˜¾çš„è¿›æ­¥ï¼ŒéªŒè¯äº†ç»“æ„æ„ŸçŸ¥è§£ç åœ¨å¤æ‚è¯­ä¹‰æŠ½å–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºä¿¡æ¯æŠ½å–ã€çŸ¥è¯†å›¾è°±æ„å»ºã€é—®ç­”ç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤æ‚å®ä½“æŠ½å–çš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œåœ¨é‡‘èé¢†åŸŸï¼Œå¯ä»¥ç”¨äºæŠ½å–å…¬å¸é—´çš„è‚¡æƒå…³ç³»ï¼›åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¯ä»¥ç”¨äºæŠ½å–è¯ç‰©ä¸ç–¾ç—…ä¹‹é—´çš„å…³è”ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.

