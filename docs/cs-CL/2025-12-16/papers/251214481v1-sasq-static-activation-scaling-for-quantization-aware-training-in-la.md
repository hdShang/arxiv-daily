---
layout: default
title: SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models
---

# SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models

**arXiv**: [2512.14481v1](https://arxiv.org/abs/2512.14481) | [PDF](https://arxiv.org/pdf/2512.14481.pdf)

**ä½œè€…**: Shizhuo Mao, Song Chen, Yi Kang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SASQï¼šä¸€ç§è½»é‡çº§çš„é™æ€æ¿€æ´»é‡åŒ–è®­ç»ƒæ¡†æž¶ï¼Œç”¨äºŽæå‡å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–ç²¾åº¦ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ` `æ¨¡åž‹é‡åŒ–` `é™æ€é‡åŒ–` `æ¿€æ´»é‡åŒ–` `ä½Žç²¾åº¦è®¡ç®—` `è¾¹ç¼˜è®¡ç®—`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–æ–¹æ³•åœ¨ç²¾åº¦ã€è®¡ç®—å¼€é”€å’Œéƒ¨ç½²æ•ˆçŽ‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œé™æ€é‡åŒ–ç²¾åº¦æŸå¤±ï¼ŒåŠ¨æ€é‡åŒ–å¼€é”€è¿‡é«˜ã€‚
2. SASQé€šè¿‡ä»…ä¼˜åŒ–æ¿€æ´»é‡åŒ–å› å­ï¼Œé¿å…äº†æƒé‡è®­ç»ƒçš„é«˜æˆæœ¬ï¼Œå®žçŽ°äº†é«˜ç²¾åº¦å’Œé«˜æ•ˆçŽ‡çš„é™æ€é‡åŒ–æŽ¨ç†ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒSASQåœ¨LLaMA2-7Bä¸Šä¼˜äºŽçŽ°æœ‰SOTAé‡åŒ–æ–¹æ¡ˆå’ŒFP16æ¨¡åž‹ï¼Œæ˜¾è‘—é™ä½Žäº†å›°æƒ‘åº¦ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†å…¶ä¸æ–­å¢žé•¿çš„è§„æ¨¡è¶…è¿‡äº†GPUå†…å­˜çš„å‘å±•é€Ÿåº¦ï¼Œç»™éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æ¨¡åž‹é‡åŒ–é€šè¿‡é™ä½Žæƒé‡å’Œæ¿€æ´»çš„ç²¾åº¦æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†çŽ°æœ‰çš„è§£å†³æ–¹æ¡ˆé¢ä¸´ç€æ ¹æœ¬æ€§çš„æƒè¡¡ï¼šåŠ¨æ€é‡åŒ–ä¼šäº§ç”Ÿå¾ˆé«˜çš„è®¡ç®—å¼€é”€ï¼Œå¹¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé€ æˆéƒ¨ç½²æŒ‘æˆ˜ï¼Œè€Œé™æ€é‡åŒ–ä¼šç‰ºç‰²ç²¾åº¦ã€‚çŽ°æœ‰çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ–¹æ³•è¿›ä¸€æ­¥å—åˆ°æƒé‡è®­ç»ƒæˆæœ¬çš„å›°æ‰°ã€‚æˆ‘ä»¬æå‡ºäº†SASQï¼šä¸€ä¸ªä¸“é—¨ä¸ºæ¿€æ´»é‡åŒ–å› å­é‡èº«å®šåˆ¶çš„è½»é‡çº§QATæ¡†æž¶ã€‚SASQä»…ä¼˜åŒ–é‡åŒ–å› å­ï¼ˆä¸æ”¹å˜é¢„è®­ç»ƒæƒé‡ï¼‰ï¼Œä»Žè€Œä»¥é«˜ç²¾åº¦å®žçŽ°é™æ€æŽ¨ç†ï¼ŒåŒæ—¶ä¿æŒéƒ¨ç½²æ•ˆçŽ‡ã€‚SASQè‡ªé€‚åº”åœ°æˆªæ–­ä¸€äº›å¼‚å¸¸å€¼ï¼Œä»Žè€Œé™ä½Žäº†é‡åŒ–çš„éš¾åº¦ï¼ŒåŒæ—¶ä¿ç•™äº†æ¿€æ´»çš„åˆ†å¸ƒç‰¹å¾ã€‚SASQä¸ä»…è¶…è¶Šäº†çŽ°æœ‰çš„SOTAé‡åŒ–æ–¹æ¡ˆï¼Œè€Œä¸”ä¼˜äºŽç›¸åº”çš„FP16æ¨¡åž‹ã€‚åœ¨LLaMA2-7Bä¸Šï¼Œå®ƒåœ¨WikiText2ä¸Šçš„å›°æƒ‘åº¦æ¯”QuaRotä½Ž5.2%ï¼Œæ¯”FP16æ¨¡åž‹ä½Ž4.7%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§è¯­è¨€æ¨¡åž‹é‡åŒ–æ—¨åœ¨é™ä½Žæ¨¡åž‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä»¥ä¾¿åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚çŽ°æœ‰çš„é™æ€é‡åŒ–æ–¹æ³•è™½ç„¶éƒ¨ç½²æ•ˆçŽ‡é«˜ï¼Œä½†ç²¾åº¦æŸå¤±è¾ƒå¤§ï¼›åŠ¨æ€é‡åŒ–æ–¹æ³•è™½ç„¶ç²¾åº¦è¾ƒé«˜ï¼Œä½†è®¡ç®—å¼€é”€å¤§ï¼Œä¸é€‚åˆè¾¹ç¼˜è®¾å¤‡ã€‚é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å¯ä»¥æé«˜é‡åŒ–æ¨¡åž‹çš„ç²¾åº¦ï¼Œä½†ä¼ ç»Ÿçš„QATæ–¹æ³•éœ€è¦è®­ç»ƒæƒé‡ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSASQçš„æ ¸å¿ƒæ€è·¯æ˜¯ä»…ä¼˜åŒ–æ¿€æ´»çš„é‡åŒ–å› å­ï¼Œè€Œä¿æŒé¢„è®­ç»ƒæƒé‡ä¸å˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSASQå¯ä»¥åœ¨ä¸å¼•å…¥é¢å¤–æƒé‡è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæé«˜é™æ€é‡åŒ–æ¨¡åž‹çš„ç²¾åº¦ã€‚åŒæ—¶ï¼ŒSASQé€šè¿‡è‡ªé€‚åº”æˆªæ–­æ¿€æ´»ä¸­çš„å¼‚å¸¸å€¼ï¼Œé™ä½Žäº†é‡åŒ–çš„éš¾åº¦ï¼Œè¿›ä¸€æ­¥æå‡äº†ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSASQæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åŠ è½½é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡åž‹ï¼›2) å¯¹æ¿€æ´»è¿›è¡Œé‡åŒ–ï¼Œå¹¶å¼•å…¥å¯å­¦ä¹ çš„é‡åŒ–å› å­ï¼›3) ä½¿ç”¨å°‘é‡æ•°æ®ï¼Œä»…ä¼˜åŒ–é‡åŒ–å› å­ï¼Œä¿æŒé¢„è®­ç»ƒæƒé‡ä¸å˜ï¼›4) å¯¹é‡åŒ–åŽçš„æ¨¡åž‹è¿›è¡Œè¯„ä¼°ã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€é‡æ–°è®­ç»ƒæƒé‡ï¼Œå¤§å¤§é™ä½Žäº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šSASQçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) ä»…ä¼˜åŒ–æ¿€æ´»é‡åŒ–å› å­ï¼Œé¿å…äº†æƒé‡è®­ç»ƒçš„é«˜æˆæœ¬ï¼›2) å¼•å…¥è‡ªé€‚åº”æˆªæ–­æœºåˆ¶ï¼Œé™ä½Žé‡åŒ–éš¾åº¦ï¼Œæå‡ç²¾åº¦ã€‚ä¸Žä¼ ç»Ÿçš„QATæ–¹æ³•ç›¸æ¯”ï¼ŒSASQæ›´åŠ è½»é‡çº§ï¼Œæ›´æ˜“äºŽéƒ¨ç½²ã€‚ä¸ŽçŽ°æœ‰çš„é™æ€é‡åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒSASQç²¾åº¦æ›´é«˜ã€‚

**å…³é”®è®¾è®¡**ï¼šSASQçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é‡åŒ–å› å­çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œä¿è¯é‡åŒ–åŽçš„æ¿€æ´»åˆ†å¸ƒä¸ŽåŽŸå§‹åˆ†å¸ƒå°½å¯èƒ½æŽ¥è¿‘ï¼›2) è‡ªé€‚åº”æˆªæ–­é˜ˆå€¼çš„é€‰æ‹©ï¼Œå¹³è¡¡äº†å¼‚å¸¸å€¼çš„åŽ»é™¤å’Œä¿¡æ¯ä¿ç•™ï¼›3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œé¼“åŠ±é‡åŒ–åŽçš„æ¿€æ´»åˆ†å¸ƒä¸ŽåŽŸå§‹åˆ†å¸ƒç›¸ä¼¼ã€‚å…·ä½“è€Œè¨€ï¼ŒæŸå¤±å‡½æ•°å¯èƒ½åŒ…å«KLæ•£åº¦ç­‰åº¦é‡åˆ†å¸ƒç›¸ä¼¼æ€§çš„æŒ‡æ ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

SASQåœ¨LLaMA2-7Bæ¨¡åž‹ä¸Šè¿›è¡Œäº†å®žéªŒï¼Œç»“æžœè¡¨æ˜Žï¼ŒSASQåœ¨WikiText2æ•°æ®é›†ä¸Šçš„å›°æƒ‘åº¦æ¯”QuaRotä½Ž5.2%ï¼Œæ¯”FP16æ¨¡åž‹ä½Ž4.7%ã€‚è¿™è¡¨æ˜ŽSASQä¸ä»…è¶…è¶Šäº†çŽ°æœ‰çš„SOTAé‡åŒ–æ–¹æ¡ˆï¼Œè€Œä¸”ä¼˜äºŽå…¨ç²¾åº¦æ¨¡åž‹ï¼Œå®žçŽ°äº†ç²¾åº¦å’Œæ•ˆçŽ‡çš„åŒé‡æå‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SASQé€‚ç”¨äºŽå¯¹è®¡ç®—èµ„æºå’Œèƒ½è€—æœ‰ä¸¥æ ¼é™åˆ¶çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿå’Œè¾¹ç¼˜æœåŠ¡å™¨ã€‚å®ƒå¯ä»¥å¸®åŠ©åœ¨è¿™äº›å¹³å°ä¸Šéƒ¨ç½²æ›´å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡åž‹ï¼Œä»Žè€Œæå‡è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨çš„ç”¨æˆ·ä½“éªŒã€‚è¯¥æŠ€æœ¯è¿˜æœ‰æ½œåŠ›åº”ç”¨äºŽå…¶ä»–ç±»åž‹çš„æ·±åº¦å­¦ä¹ æ¨¡åž‹ï¼Œä¾‹å¦‚è®¡ç®—æœºè§†è§‰æ¨¡åž‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

