---
layout: default
title: SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models
---

# SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models

**arXiv**: [2512.14481v1](https://arxiv.org/abs/2512.14481) | [PDF](https://arxiv.org/pdf/2512.14481.pdf)

**ä½œè€…**: Shizhuo Mao, Song Chen, Yi Kang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSASQæ¡†æž¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–è®­ç»ƒä¸­æ¿€æ´»é‡åŒ–å› å­çš„ä¼˜åŒ–é—®é¢˜ï¼Œå®žçŽ°é«˜æ•ˆé™æ€æŽ¨ç†ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹` `æ¨¡åž‹é‡åŒ–` `é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ` `æ¿€æ´»é‡åŒ–` `é™æ€æŽ¨ç†` `è¾¹ç¼˜éƒ¨ç½²` `è½»é‡çº§è®­ç»ƒ` `å›°æƒ‘åº¦ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é‡åŒ–æ–¹æ³•å­˜åœ¨æƒè¡¡ï¼ŒåŠ¨æ€é‡åŒ–è®¡ç®—å¼€é”€å¤§ä¸”éƒ¨ç½²éš¾ï¼Œé™æ€é‡åŒ–ç²¾åº¦ä½Žï¼Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒæˆæœ¬é«˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºSASQæ¡†æž¶ï¼Œä»…ä¼˜åŒ–æ¿€æ´»é‡åŒ–å› å­ï¼Œä¸æ”¹å˜é¢„è®­ç»ƒæƒé‡ï¼Œå®žçŽ°è½»é‡çº§è®­ç»ƒå’Œé«˜æ•ˆé™æ€æŽ¨ç†ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LLaMA2-7Bä¸Šï¼ŒSASQè¶…è¶ŠSOTAé‡åŒ–æ–¹æ¡ˆå’ŒFP16æ¨¡åž‹ï¼Œå›°æƒ‘åº¦æ˜¾è‘—é™ä½Žã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†å…¶è§„æ¨¡å¢žé•¿è¶…è¿‡äº†GPUå†…å­˜çš„è¿›æ­¥ï¼Œå¯¼è‡´éƒ¨ç½²æŒ‘æˆ˜ã€‚æ¨¡åž‹é‡åŒ–é€šè¿‡é™ä½Žæƒé‡å’Œæ¿€æ´»çš„ç²¾åº¦æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†çŽ°æœ‰è§£å†³æ–¹æ¡ˆé¢ä¸´åŸºæœ¬æƒè¡¡ï¼šåŠ¨æ€é‡åŒ–å¸¦æ¥é«˜è®¡ç®—å¼€é”€å¹¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ï¼Œè€Œé™æ€é‡åŒ–ç‰ºç‰²å‡†ç¡®æ€§ã€‚çŽ°æœ‰çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ–¹æ³•è¿˜é¢ä¸´æƒé‡è®­ç»ƒæˆæœ¬é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºSASQï¼šä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ¿€æ´»é‡åŒ–å› å­çš„è½»é‡çº§QATæ¡†æž¶ã€‚SASQä»…ä¼˜åŒ–é‡åŒ–å› å­ï¼ˆä¸æ”¹å˜é¢„è®­ç»ƒæƒé‡ï¼‰ï¼Œå®žçŽ°é«˜ç²¾åº¦çš„é™æ€æŽ¨ç†ï¼ŒåŒæ—¶ä¿æŒéƒ¨ç½²æ•ˆçŽ‡ã€‚SASQè‡ªé€‚åº”åœ°æˆªæ–­ä¸€äº›å¼‚å¸¸å€¼ï¼Œä»Žè€Œé™ä½Žé‡åŒ–éš¾åº¦ï¼ŒåŒæ—¶ä¿ç•™æ¿€æ´»çš„åˆ†å¸ƒç‰¹æ€§ã€‚SASQä¸ä»…è¶…è¶Šäº†çŽ°æœ‰çš„SOTAé‡åŒ–æ–¹æ¡ˆï¼Œè¿˜ä¼˜äºŽç›¸åº”çš„FP16æ¨¡åž‹ã€‚åœ¨LLaMA2-7Bä¸Šï¼Œå®ƒåœ¨WikiText2ä¸Šå®žçŽ°äº†æ¯”QuaRotä½Ž5.2%çš„å›°æƒ‘åº¦å’Œæ¯”FP16æ¨¡åž‹ä½Ž4.7%çš„å›°æƒ‘åº¦ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–ä¸­çš„æ¿€æ´»é‡åŒ–é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•å¦‚åŠ¨æ€é‡åŒ–è®¡ç®—å¼€é”€é«˜ã€éƒ¨ç½²å›°éš¾ï¼Œé™æ€é‡åŒ–ç²¾åº¦ä½Žï¼Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒæƒé‡è®­ç»ƒæˆæœ¬é«˜ï¼Œå¯¼è‡´æ¨¡åž‹éƒ¨ç½²æ•ˆçŽ‡ä¸Žå‡†ç¡®æ€§éš¾ä»¥å…¼é¡¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSASQçš„æ ¸å¿ƒæ€è·¯æ˜¯ä¸“æ³¨äºŽä¼˜åŒ–æ¿€æ´»é‡åŒ–å› å­ï¼Œè€Œéžæƒé‡ï¼Œé€šè¿‡è½»é‡çº§è®­ç»ƒå®žçŽ°é™æ€æŽ¨ç†ï¼Œä»Žè€Œåœ¨ä¿æŒéƒ¨ç½²æ•ˆçŽ‡çš„åŒæ—¶æå‡é‡åŒ–ç²¾åº¦ã€‚è¿™æ ·è®¾è®¡é¿å…äº†æƒé‡è®­ç»ƒçš„é«˜æˆæœ¬ï¼Œå¹¶é’ˆå¯¹æ¿€æ´»åˆ†å¸ƒç‰¹æ€§è¿›è¡Œä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSASQæ¡†æž¶åŒ…æ‹¬é¢„è®­ç»ƒæ¨¡åž‹åŠ è½½ã€æ¿€æ´»é‡åŒ–å› å­åˆå§‹åŒ–ã€è‡ªé€‚åº”æˆªæ–­æ¨¡å—ã€é‡åŒ–å› å­ä¼˜åŒ–é˜¶æ®µå’Œé™æ€æŽ¨ç†éƒ¨ç½²ã€‚ä¸»è¦æ¨¡å—æ¶‰åŠæ¿€æ´»åˆ†å¸ƒåˆ†æžã€é‡åŒ–å› å­è°ƒæ•´å’ŒæŸå¤±å‡½æ•°è®¡ç®—ï¼Œæµç¨‹ç®€å•é«˜æ•ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯ä»…ä¼˜åŒ–æ¿€æ´»é‡åŒ–å› å­è€Œä¸æ”¹å˜é¢„è®­ç»ƒæƒé‡ï¼Œè¿™é™ä½Žäº†è®­ç»ƒæˆæœ¬å¹¶ä¿ç•™äº†æ¨¡åž‹æ€§èƒ½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽé¿å…äº†æƒé‡æ›´æ–°ï¼Œä¸“æ³¨äºŽæ¿€æ´»é‡åŒ–å› å­çš„è‡ªé€‚åº”ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬è‡ªé€‚åº”æˆªæ–­æœºåˆ¶ä»¥å¤„ç†æ¿€æ´»å¼‚å¸¸å€¼ï¼Œä½¿ç”¨é‡åŒ–æ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥æŒ‡å¯¼å› å­ä¼˜åŒ–ï¼Œå‚æ•°è®¾ç½®å¦‚é‡åŒ–æ¯”ç‰¹æ•°å’Œæˆªæ–­é˜ˆå€¼åŸºäºŽå®žéªŒè°ƒæ•´ï¼Œç½‘ç»œç»“æž„ä¿æŒåŽŸæ¨¡åž‹ä¸å˜ï¼Œä»…æ·»åŠ è½»é‡çº§ä¼˜åŒ–å±‚ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨LLaMA2-7Bæ¨¡åž‹ä¸Šï¼ŒSASQåœ¨WikiText2æ•°æ®é›†ä¸Šå®žçŽ°äº†æ¯”QuaRoté‡åŒ–æ–¹æ¡ˆä½Ž5.2%çš„å›°æƒ‘åº¦ï¼Œæ¯”FP16æ¨¡åž‹ä½Ž4.7%çš„å›°æƒ‘åº¦ï¼Œæ˜¾è‘—è¶…è¶Šäº†çŽ°æœ‰SOTAé‡åŒ–æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ä¼˜äºŽåŽŸå§‹é«˜ç²¾åº¦æ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SASQé€‚ç”¨äºŽå¤§è¯­è¨€æ¨¡åž‹çš„è¾¹ç¼˜éƒ¨ç½²å’Œèµ„æºå—é™çŽ¯å¢ƒï¼Œå¦‚ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿå’Œäº‘è®¡ç®—å¹³å°ï¼Œèƒ½æå‡æ¨¡åž‹æŽ¨ç†æ•ˆçŽ‡å¹¶é™ä½Žå†…å­˜éœ€æ±‚ï¼Œå…·æœ‰å®žé™…ä»·å€¼ï¼Œæœªæ¥å¯èƒ½æŽ¨åŠ¨è½»é‡çº§AIæ¨¡åž‹çš„å¹¿æ³›åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

