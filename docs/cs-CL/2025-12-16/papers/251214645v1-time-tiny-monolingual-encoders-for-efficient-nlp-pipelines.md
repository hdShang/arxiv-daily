---
layout: default
title: TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines
---

# TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines

**arXiv**: [2512.14645v1](https://arxiv.org/abs/2512.14645) | [PDF](https://arxiv.org/pdf/2512.14645.pdf)

**ä½œè€…**: David Schulmeister, Valentin Hartmann, Lars Klein, Robert West

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TiMEï¼šç”¨äºŽé«˜æ•ˆNLPæµæ°´çº¿çš„å¾®åž‹å•è¯­ç¼–ç å™¨**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)**

**å…³é”®è¯**: `å¾®åž‹æ¨¡åž‹` `å•è¯­ç¼–ç å™¨` `çŸ¥è¯†è’¸é¦` `ä½Žèµ„æºè¯­è¨€` `é«˜æ•ˆNLP` `æ¨¡åž‹åŽ‹ç¼©` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ç‰¹å®šNLPä»»åŠ¡ä¸Šæ•ˆçŽ‡ä½Žä¸‹ï¼Œæ— æ³•æ»¡è¶³å®žæ—¶æ€§å’Œä½ŽåŠŸè€—éœ€æ±‚ã€‚
2. æå‡ºTiMEï¼Œä¸€ç§å¾®åž‹å•è¯­ç¼–ç å™¨ï¼Œé€šè¿‡è’¸é¦ç­‰çŽ°ä»£è®­ç»ƒæŠ€æœ¯ï¼Œä¼˜åŒ–æ€§èƒ½ä¸Žæ•ˆçŽ‡çš„å¹³è¡¡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒTiMEåœ¨å¸¸è§NLPä»»åŠ¡ä¸Šå®žçŽ°äº†æ€§èƒ½ã€åžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´çš„æ›´å¥½æƒè¡¡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰ï¼Œè¯­è¨€æ¨¡åž‹çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤§åž‹é€šç”¨æ¨¡åž‹ä¸Šã€‚ç„¶è€Œï¼Œè®¸å¤šNLPæµæ°´çº¿åªéœ€è¦å…·å¤‡æ˜Žç¡®å®šä¹‰çš„ã€å°‘é‡åŠŸèƒ½çš„æ¨¡åž‹ã€‚è™½ç„¶å¤§åž‹æ¨¡åž‹èƒ½å¤Ÿæ‰§è¡Œå°åž‹æ¨¡åž‹çš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬å¤„ç†å¤§é‡æ•°æ®çš„é€Ÿåº¦ä¸å¤Ÿå¿«ï¼Œä¹Ÿæ— æ³•æä¾›å®žæ—¶å“åº”ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸æ¶ˆè€—ä¸å¿…è¦çš„å¤§é‡èƒ½é‡ï¼Œå¯¼è‡´å¯æŒç»­æ€§é—®é¢˜ä»¥åŠåœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ä¸Šéƒ¨ç½²æ—¶å‡ºçŽ°é—®é¢˜ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä¸ºè¿™ç§å¯¹æ•ˆçŽ‡è‡³å…³é‡è¦çš„åº”ç”¨è®­ç»ƒå°åž‹æ¨¡åž‹ã€‚ä¸Žè®¸å¤šçŽ°æˆçš„NLPæµæ°´çº¿ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡åž‹ä½¿ç”¨è¯¸å¦‚è’¸é¦ä¹‹ç±»çš„çŽ°ä»£è®­ç»ƒæŠ€æœ¯ï¼Œå¹¶æ”¯æŒä½Žèµ„æºè¯­è¨€ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ¨¡åž‹ç§°ä¸ºTiMEï¼ˆTiny Monolingual Encodersï¼‰ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—å¸¸è§çš„NLPä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œè§‚å¯Ÿåˆ°åŸºå‡†æ€§èƒ½ã€åžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´æ›´å¥½çš„æƒè¡¡ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜Žå¯ä»¥ä»Žå¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹ä¸­è’¸é¦å•è¯­æ¨¡åž‹ï¼ŒåŒæ ·ä¹Ÿå¯ä»¥ä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡åž‹ä¸­è’¸é¦å…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡åž‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰NLPæµæ°´çº¿é€šå¸¸ä¾èµ–å¤§åž‹é€šç”¨è¯­è¨€æ¨¡åž‹ï¼Œè¿™äº›æ¨¡åž‹è™½ç„¶èƒ½åŠ›å¼ºå¤§ï¼Œä½†åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæ•ˆçŽ‡ä½Žä¸‹ï¼Œæ— æ³•æ»¡è¶³å®žæ—¶å“åº”å’Œä½ŽåŠŸè€—çš„éœ€æ±‚ã€‚è¿™å¯¹äºŽå¤„ç†å¤§é‡æ•°æ®æˆ–åœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ä¸Šéƒ¨ç½²æ—¶å°¤å…¶æˆé—®é¢˜ã€‚å› æ­¤ï¼Œéœ€è¦æ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡åž‹æ¥æ»¡è¶³è¿™äº›éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå°†å¤§åž‹å¤šè¯­è¨€æ¨¡åž‹çš„çŸ¥è¯†è¿ç§»åˆ°å°åž‹å•è¯­æ¨¡åž‹ä¸­ã€‚è¿™æ ·å¯ä»¥åœ¨ä¿æŒä¸€å®šæ€§èƒ½æ°´å¹³çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½Žæ¨¡åž‹çš„è®¡ç®—å¤æ‚åº¦å’Œèµ„æºæ¶ˆè€—ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œé¿å…äº†å¤§åž‹é€šç”¨æ¨¡åž‹å¸¦æ¥çš„å†—ä½™è®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šTiMEçš„è®­ç»ƒæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é€‰æ‹©ä¸€ä¸ªå¤§åž‹å¤šè¯­è¨€æ¨¡åž‹ä½œä¸ºæ•™å¸ˆæ¨¡åž‹ï¼›2) æž„å»ºå•è¯­æ•°æ®é›†ï¼Œç”¨äºŽè®­ç»ƒå°åž‹å•è¯­å­¦ç”Ÿæ¨¡åž‹ï¼›3) ä½¿ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå°†æ•™å¸ˆæ¨¡åž‹çš„è¾“å‡ºï¼ˆä¾‹å¦‚ï¼Œlogitsæˆ–éšè—å±‚è¡¨ç¤ºï¼‰ä½œä¸ºç›‘ç£ä¿¡å·ï¼ŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡åž‹çš„è®­ç»ƒï¼›4) åœ¨ä¸€ç³»åˆ—å¸¸è§çš„NLPä»»åŠ¡ä¸Šè¯„ä¼°TiMEæ¨¡åž‹çš„æ€§èƒ½å’Œæ•ˆçŽ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) è¯æ˜Žäº†å¯ä»¥æœ‰æ•ˆåœ°ä»Žå¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹ä¸­è’¸é¦å‡ºé«˜æ€§èƒ½çš„å•è¯­æ¨¡åž‹ï¼Œè¿™ä¸ºä½Žèµ„æºè¯­è¨€çš„NLPä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼›2) æŽ¢ç´¢äº†ä¸åŒç±»åž‹çš„åµŒå…¥æ–¹å¼ä¹‹é—´çš„è’¸é¦ï¼Œä¾‹å¦‚ä»Žç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡åž‹è’¸é¦å‡ºç»å¯¹ä½ç½®åµŒå…¥çš„å­¦ç”Ÿæ¨¡åž‹ï¼›3) æå‡ºäº†TiMEï¼Œä¸€ä¸ªé’ˆå¯¹æ•ˆçŽ‡ä¼˜åŒ–çš„å¾®åž‹æ¨¡åž‹å®¶æ—ï¼Œåœ¨æ€§èƒ½ã€åžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´å®žçŽ°äº†æ›´å¥½çš„å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°å’Œè’¸é¦æŸå¤±å‡½æ•°ï¼Œå…¶ä¸­è’¸é¦æŸå¤±å‡½æ•°ç”¨äºŽè¡¡é‡å­¦ç”Ÿæ¨¡åž‹å’Œæ•™å¸ˆæ¨¡åž‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚æ¸©åº¦å‚æ•°è¢«ç”¨äºŽå¹³æ»‘æ•™å¸ˆæ¨¡åž‹çš„è¾“å‡ºï¼Œä»Žè€Œæ›´å¥½åœ°æŒ‡å¯¼å­¦ç”Ÿæ¨¡åž‹çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¿˜æŽ¢ç´¢äº†ä¸åŒçš„ç½‘ç»œç»“æž„å’Œè¶…å‚æ•°è®¾ç½®ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–TiMEæ¨¡åž‹çš„æ€§èƒ½å’Œæ•ˆçŽ‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TiMEæ¨¡åž‹åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æžœè¡¨æ˜Žï¼Œåœ¨ä¿æŒä¸Žå¤§åž‹æ¨¡åž‹ç›¸è¿‘çš„æ€§èƒ½æ°´å¹³ä¸‹ï¼ŒTiMEæ¨¡åž‹åœ¨åžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚å…·ä½“æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªç‰¹å®šä»»åŠ¡ä¸Šï¼ŒTiMEæ¨¡åž‹å¯èƒ½å®žçŽ°äº†X%çš„åžåé‡æå‡ï¼ŒY%çš„å»¶è¿Ÿé™ä½Žï¼Œä»¥åŠZ%çš„èƒ½è€—å‡å°‘ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TiMEæ¨¡åž‹é€‚ç”¨äºŽå¯¹æ•ˆçŽ‡æœ‰è¾ƒé«˜è¦æ±‚çš„NLPåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„å®žæ—¶ç¿»è¯‘ã€æ™ºèƒ½å®¢æœã€è¯­éŸ³åŠ©æ‰‹ç­‰ã€‚é€šè¿‡é™ä½Žæ¨¡åž‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½Žéƒ¨ç½²æˆæœ¬ã€‚æ­¤å¤–ï¼ŒTiMEå¯¹ä½Žèµ„æºè¯­è¨€çš„æ”¯æŒï¼Œä½¿å…¶åœ¨å¤šè¯­è¨€çŽ¯å¢ƒä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

