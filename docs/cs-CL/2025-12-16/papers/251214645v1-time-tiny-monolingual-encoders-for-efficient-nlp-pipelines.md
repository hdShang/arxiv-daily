---
layout: default
title: TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines
---

# TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines

**arXiv**: [2512.14645v1](https://arxiv.org/abs/2512.14645) | [PDF](https://arxiv.org/pdf/2512.14645.pdf)

**ä½œè€…**: David Schulmeister, Valentin Hartmann, Lars Klein, Robert West

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTiMEå°è§„æ¨¡å•è¯­ç¼–ç å™¨ï¼Œé€šè¿‡è’¸é¦è®­ç»ƒå®žçŽ°é«˜æ•ˆNLPæµæ°´çº¿ï¼Œè§£å†³å¤§æ¨¡åž‹åœ¨å®žæ—¶å¤„ç†ä¸Žèƒ½æ•ˆæ–¹é¢çš„ä¸è¶³ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å°è§„æ¨¡è¯­è¨€æ¨¡åž‹` `çŸ¥è¯†è’¸é¦` `å•è¯­ç¼–ç å™¨` `é«˜æ•ˆNLP` `ä½Žèµ„æºè¯­è¨€` `èƒ½æ•ˆä¼˜åŒ–` `å®žæ—¶å¤„ç†` `Transformeræž¶æž„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹é€šç”¨è¯­è¨€æ¨¡åž‹åœ¨å®žæ—¶å¤„ç†ã€é«˜åžåé‡å’Œä½Žèƒ½è€—åœºæ™¯ä¸‹æ•ˆçŽ‡ä¸è¶³ï¼Œéš¾ä»¥éƒ¨ç½²äºŽèµ„æºå—é™è®¾å¤‡ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé‡‡ç”¨è’¸é¦æŠ€æœ¯è®­ç»ƒå°åž‹å•è¯­ç¼–ç å™¨TiMEï¼Œä»Žå¤šè¯­è¨€å¤§æ¨¡åž‹è¿ç§»çŸ¥è¯†ï¼Œä¼˜åŒ–æ€§èƒ½ä¸Žæ•ˆçŽ‡çš„å¹³è¡¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§NLPä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒTiMEåœ¨ä¿æŒè¾ƒé«˜åŸºå‡†æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡åžåé‡ã€é™ä½Žå»¶è¿Ÿå’Œèƒ½è€—ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰è¯­è¨€æ¨¡åž‹ç ”ç©¶ä¸»è¦é›†ä¸­äºŽå¤§åž‹é€šç”¨æ¨¡åž‹ï¼Œä½†è®¸å¤šNLPæµæ°´çº¿ä»…éœ€å…·å¤‡ç‰¹å®šå°è§„æ¨¡èƒ½åŠ›çš„æ¨¡åž‹ã€‚å¤§åž‹æ¨¡åž‹è™½èƒ½æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œä½†åœ¨å¤„ç†å¤§é‡æ•°æ®æˆ–æä¾›å®žæ—¶å“åº”æ—¶é€Ÿåº¦ä¸è¶³ï¼Œä¸”èƒ½è€—è¿‡é«˜ï¼Œå¯¼è‡´å¯æŒç»­æ€§é—®é¢˜ï¼Œåœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•ä¸ºè¿™ç±»æ•ˆçŽ‡å…³é”®åº”ç”¨è®­ç»ƒå°åž‹æ¨¡åž‹ã€‚ä¸Žè®¸å¤šçŽ°æˆNLPæµæ°´çº¿ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡åž‹é‡‡ç”¨è’¸é¦ç­‰çŽ°ä»£è®­ç»ƒæŠ€æœ¯ï¼Œå¹¶æ”¯æŒä½Žèµ„æºè¯­è¨€ã€‚æˆ‘ä»¬ç§°è¿™äº›æ¨¡åž‹ä¸ºTiMEï¼ˆTiny Monolingual Encodersï¼‰ï¼Œåœ¨ä¸€ç³»åˆ—å¸¸è§NLPä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè§‚å¯Ÿåˆ°åœ¨åŸºå‡†æ€§èƒ½ä¸Žåžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´å–å¾—äº†æ›´å¥½çš„æƒè¡¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¯æ˜Žäº†ä»Žå¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è’¸é¦å•è¯­æ¨¡åž‹æ˜¯å¯è¡Œçš„ï¼ŒåŒæ ·å¯ä»¥ä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡åž‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡åž‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³NLPæµæ°´çº¿ä¸­å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨æ•ˆçŽ‡å…³é”®åº”ç”¨ä¸­çš„ä¸è¶³ï¼ŒåŒ…æ‹¬å¤„ç†é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜ã€éš¾ä»¥åœ¨å®žæ—¶æˆ–èµ„æºå—é™çŽ¯å¢ƒï¼ˆå¦‚ç§»åŠ¨è®¾å¤‡ï¼‰éƒ¨ç½²ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽè¿‡åº¦ä¾èµ–é€šç”¨å¤§æ¨¡åž‹ï¼Œå¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—å¼€é”€å’Œå¯æŒç»­æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä»Žå¤§åž‹å¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è®­ç»ƒå°åž‹å•è¯­å­¦ç”Ÿæ¨¡åž‹ï¼Œä¸“æ³¨äºŽç‰¹å®šè¯­è¨€å’Œä»»åŠ¡ï¼Œä»¥åœ¨æ€§èƒ½ä¸Žæ•ˆçŽ‡é—´å–å¾—æ›´å¥½å¹³è¡¡ã€‚è®¾è®¡æ€è·¯åŸºäºŽè’¸é¦èƒ½åŽ‹ç¼©æ¨¡åž‹è§„æ¨¡ï¼ŒåŒæ—¶ä¿ç•™å…³é”®èƒ½åŠ›ï¼Œå¹¶é€‚åº”ä½Žèµ„æºè¯­è¨€éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) é€‰æ‹©å¤§åž‹å¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹ï¼ˆå¦‚åŸºäºŽTransformerçš„é¢„è®­ç»ƒæ¨¡åž‹ï¼‰ï¼›2) ä½¿ç”¨è’¸é¦æŸå¤±å‡½æ•°ï¼Œå°†æ•™å¸ˆçš„çŸ¥è¯†è¿ç§»åˆ°å°åž‹å•è¯­å­¦ç”Ÿæ¨¡åž‹ï¼›3) åœ¨ç›®æ ‡è¯­è¨€æ•°æ®é›†ä¸Šå¾®è°ƒå­¦ç”Ÿæ¨¡åž‹ï¼›4) è¯„ä¼°æ¨¡åž‹åœ¨NLPä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œæ•ˆçŽ‡æŒ‡æ ‡ã€‚ä¸»è¦æ¨¡å—æ¶‰åŠæ•™å¸ˆ-å­¦ç”Ÿæž¶æž„ã€è’¸é¦è®­ç»ƒé˜¶æ®µå’Œæ•ˆçŽ‡ä¼˜åŒ–ç»„ä»¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯è¯æ˜Žäº†ä»Žå¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è’¸é¦å•è¯­æ¨¡åž‹çš„å¯è¡Œæ€§ï¼Œä»¥åŠä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡åž‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„å­¦ç”Ÿæ¨¡åž‹ã€‚è¿™ä¸ŽçŽ°æœ‰æ–¹æ³•æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼Œä¼ ç»Ÿè’¸é¦å¤šåœ¨åŒè¯­è¨€æˆ–åŒæž¶æž„ä¸­è¿›è¡Œï¼Œè€Œæœ¬ç ”ç©¶æ‰©å±•äº†è·¨è¯­è¨€å’ŒåµŒå…¥ç±»åž‹çš„è’¸é¦èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä½¿ç”¨è’¸é¦æŸå¤±ï¼ˆå¦‚è½¯æ ‡ç­¾æŸå¤±å’Œæ³¨æ„åŠ›è’¸é¦ï¼‰æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡åž‹ï¼›ç½‘ç»œç»“æž„ä¸ºå°åž‹Transformerç¼–ç å™¨ï¼Œå‚æ•°è§„æ¨¡æ˜¾è‘—å°äºŽæ•™å¸ˆæ¨¡åž‹ï¼›ä½ç½®åµŒå…¥é‡‡ç”¨ç»å¯¹ç±»åž‹ä»¥ç®€åŒ–è®¡ç®—ï¼›è®­ç»ƒæ—¶å¯èƒ½ç»“åˆæ•°æ®å¢žå¼ºå’Œä½Žèµ„æºè¯­è¨€é€‚é…æŠ€æœ¯ï¼Œå…·ä½“å‚æ•°è®¾ç½®æœªåœ¨æ‘˜è¦ä¸­è¯¦è¿°ï¼Œéœ€å‚è€ƒè®ºæ–‡æ­£æ–‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒè¡¨æ˜Žï¼ŒTiMEæ¨¡åž‹åœ¨å¸¸è§NLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®žä½“è¯†åˆ«ï¼‰ä¸Šï¼Œç›¸æ¯”å¤§åž‹åŸºçº¿æ¨¡åž‹ï¼Œåœ¨åŸºå‡†æ€§èƒ½ç›¸è¿‘çš„æƒ…å†µä¸‹ï¼Œåžåé‡æå‡æ˜¾è‘—ï¼ˆå…·ä½“æ•°æ®æœªæä¾›ï¼Œä½†å¼ºè°ƒæ›´å¥½æƒè¡¡ï¼‰ï¼Œå»¶è¿Ÿé™ä½Žï¼Œèƒ½è€—å‡å°‘ã€‚ä¾‹å¦‚ï¼Œä»Žå¤šè¯­è¨€æ•™å¸ˆè’¸é¦çš„å•è¯­æ¨¡åž‹åœ¨ä½Žèµ„æºè¯­è¨€ä»»åŠ¡ä¸Šè¡¨çŽ°è‰¯å¥½ï¼ŒéªŒè¯äº†è’¸é¦è·¨è¯­è¨€å’ŒåµŒå…¥ç±»åž‹çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TiMEæ¨¡åž‹é€‚ç”¨äºŽéœ€è¦é«˜æ•ˆNLPå¤„ç†çš„åœºæ™¯ï¼Œå¦‚å®žæ—¶èŠå¤©æœºå™¨äººã€ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ–‡æœ¬åˆ†æžã€ä½ŽåŠŸè€—ç‰©è”ç½‘è®¾å¤‡ï¼Œä»¥åŠä½Žèµ„æºè¯­è¨€åœ°åŒºçš„è¯­è¨€æœåŠ¡ã€‚å…¶å®žé™…ä»·å€¼åœ¨äºŽé™ä½Žéƒ¨ç½²æˆæœ¬ã€æå‡å“åº”é€Ÿåº¦å¹¶å‡å°‘çŽ¯å¢ƒå½±å“ï¼Œæœªæ¥å¯èƒ½æŽ¨åŠ¨è¾¹ç¼˜è®¡ç®—å’Œå¯æŒç»­AIçš„å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

