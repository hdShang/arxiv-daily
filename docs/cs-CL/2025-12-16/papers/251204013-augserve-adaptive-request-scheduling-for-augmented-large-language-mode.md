---
layout: default
title: AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving
---

# AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.04013" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.04013</a>
  <a href="https://arxiv.org/pdf/2512.04013.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04013" onclick="toggleFavorite(this, '2512.04013', 'AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ying Wang, Zhen Jin, Jiexiong Xu, Wenhai Lin, Yiquan Chen, Wenzhi Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AugServeï¼šç”¨äºå¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡çš„è‡ªé€‚åº”è¯·æ±‚è°ƒåº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æœåŠ¡` `è¯·æ±‚è°ƒåº¦` `è‡ªé€‚åº”è°ƒåº¦` `åŠ¨æ€æ‰¹å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¢å¼ºå‹LLMæ¨ç†æœåŠ¡ä¾èµ–FCFSè°ƒåº¦ï¼Œæ˜“äº§ç”Ÿé˜Ÿå¤´é˜»å¡ï¼Œå¯¼è‡´æ’é˜Ÿå»¶è¿Ÿè¶…æ ‡ã€‚
2. AugServeé‡‡ç”¨ä¸¤é˜¶æ®µè‡ªé€‚åº”è¯·æ±‚è°ƒåº¦ï¼Œç»“åˆæ¨ç†ç‰¹å¾å’Œè¿è¡Œæ—¶ä¿¡æ¯ä¼˜åŒ–è°ƒåº¦å†³ç­–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAugServeæœ‰æ•ˆååé‡æ˜¾è‘—é«˜äºvLLMå’ŒInferCeptï¼Œå¹¶å¤§å¹…é™ä½TTFTã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€é›†æˆå¤–éƒ¨å·¥å…·çš„å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨Webåº”ç”¨ä¸­æ—¥ç›Šæ™®åŠï¼Œæå‡å¢å¼ºå‹LLMæ¨ç†æœåŠ¡æ•ˆç‡å’Œä¼˜åŒ–æœåŠ¡çº§åˆ«ç›®æ ‡ï¼ˆSLOï¼‰å¯¹äºæ”¹å–„ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæ¨ç†ç³»ç»Ÿå¿…é¡»åœ¨å»¶è¿Ÿçº¦æŸå†…æœ€å¤§åŒ–è¯·æ±‚å¤„ç†é‡ï¼Œå³æé«˜æœ‰æ•ˆååé‡ã€‚ç„¶è€Œï¼Œç°æœ‰ç³»ç»Ÿé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆiï¼‰ä¾èµ–å…ˆè¿›å…ˆå‡ºï¼ˆFCFSï¼‰è°ƒåº¦å¯¼è‡´ä¸¥é‡çš„é˜Ÿå¤´é˜»å¡ï¼Œä½¿å¾—è®¸å¤šè¯·æ±‚çš„æ’é˜Ÿå»¶è¿Ÿè¶…è¿‡SLOï¼›ï¼ˆiiï¼‰é™æ€æ‰¹å¤„ç†tokené™åˆ¶æ— æ³•é€‚åº”æ³¢åŠ¨çš„è´Ÿè½½å’Œç¡¬ä»¶æ¡ä»¶ã€‚è¿™ä¸¤ä¸ªå› ç´ éƒ½ä¼šé™ä½æœ‰æ•ˆååé‡å’ŒæœåŠ¡è´¨é‡ã€‚æœ¬æ–‡æå‡ºäº†AugServeï¼Œä¸€ä¸ªæ—¨åœ¨å‡å°‘æ’é˜Ÿå»¶è¿Ÿå¹¶æé«˜å¢å¼ºå‹LLMæ¨ç†æœåŠ¡æœ‰æ•ˆååé‡çš„é«˜æ•ˆæ¨ç†æ¡†æ¶ã€‚AugServeçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¸¤é˜¶æ®µè‡ªé€‚åº”è¯·æ±‚è°ƒåº¦ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼ŒAugServeç»“åˆäº†å¢å¼ºå‹LLMè¯·æ±‚çš„æ¨ç†ç‰¹å¾æ¥ä¼˜åŒ–è°ƒåº¦å†³ç­–çš„é¡ºåºï¼ˆé˜¶æ®µIï¼‰ã€‚è¿™äº›å†³ç­–é€šè¿‡è¿è¡Œæ—¶ä¿¡æ¯ä¸æ–­å®Œå–„ï¼ˆé˜¶æ®µIIï¼‰ï¼Œä»è€Œé€‚åº”è¯·æ±‚ç‰¹å¾å’Œç³»ç»Ÿèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒAugServeæ ¹æ®ç¡¬ä»¶çŠ¶æ€å’Œå®æ—¶è´Ÿè½½åŠ¨æ€è°ƒæ•´tokenæ‰¹å¤„ç†æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æé«˜ååé‡æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAugServeçš„æœ‰æ•ˆååé‡æ¯”vLLMå’ŒInferCeptåˆ†åˆ«é«˜4.7å€å’Œ3.3å€ï¼ŒåŒæ—¶å°†é¦–ä¸ªtokenç”Ÿæˆæ—¶é—´ï¼ˆTTFTï¼‰åˆ†åˆ«é™ä½é«˜è¾¾96.3%å’Œ95.0%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡ä¸­ï¼Œç”±äºä¼ ç»Ÿçš„å…ˆè¿›å…ˆå‡ºï¼ˆFCFSï¼‰è°ƒåº¦ç­–ç•¥å’Œé™æ€tokenæ‰¹å¤„ç†æœºåˆ¶å¯¼è‡´çš„æ’é˜Ÿå»¶è¿Ÿè¿‡é«˜å’Œæœ‰æ•ˆååé‡ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆé€‚åº”è¯·æ±‚ç‰¹å¾å’Œç³»ç»ŸçŠ¶æ€çš„å˜åŒ–ï¼Œå¯¼è‡´æœåŠ¡è´¨é‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAugServeçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨ä¸¤é˜¶æ®µè‡ªé€‚åº”è¯·æ±‚è°ƒåº¦ç­–ç•¥ï¼Œå¹¶ç»“åˆåŠ¨æ€tokenæ‰¹å¤„ç†æœºåˆ¶ã€‚é€šè¿‡åˆ†æè¯·æ±‚çš„æ¨ç†ç‰¹å¾å’Œåˆ©ç”¨è¿è¡Œæ—¶ä¿¡æ¯ï¼Œä¼˜åŒ–è¯·æ±‚çš„è°ƒåº¦é¡ºåºï¼Œä»è€Œå‡å°‘æ’é˜Ÿå»¶è¿Ÿã€‚åŒæ—¶ï¼Œæ ¹æ®ç¡¬ä»¶çŠ¶æ€å’Œå®æ—¶è´Ÿè½½åŠ¨æ€è°ƒæ•´tokenæ‰¹å¤„ç†å¤§å°ï¼Œä»¥æé«˜èµ„æºåˆ©ç”¨ç‡å’Œååé‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAugServeæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯åŸºäºæ¨ç†ç‰¹å¾çš„è¯·æ±‚æ’åºï¼Œåˆ©ç”¨è¯·æ±‚çš„å…ƒæ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè¯·æ±‚æ‰€éœ€çš„å·¥å…·ç±»å‹ã€é¢„è®¡çš„è®¡ç®—å¤æ‚åº¦ç­‰ï¼‰æ¥é¢„æµ‹è¯·æ±‚çš„ä¼˜å…ˆçº§ï¼Œå¹¶æ®æ­¤è¿›è¡Œæ’åºã€‚ç¬¬äºŒé˜¶æ®µæ˜¯åŸºäºè¿è¡Œæ—¶ä¿¡æ¯çš„è°ƒåº¦ä¼˜åŒ–ï¼Œæ ¹æ®ç³»ç»Ÿè´Ÿè½½ã€ç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡ç­‰å®æ—¶ä¿¡æ¯ï¼ŒåŠ¨æ€è°ƒæ•´è¯·æ±‚çš„è°ƒåº¦é¡ºåºã€‚æ­¤å¤–ï¼ŒAugServeè¿˜åŒ…å«ä¸€ä¸ªåŠ¨æ€tokenæ‰¹å¤„ç†æ¨¡å—ï¼Œæ ¹æ®ç¡¬ä»¶çŠ¶æ€å’Œå®æ—¶è´Ÿè½½è°ƒæ•´æ‰¹å¤„ç†å¤§å°ã€‚

**å…³é”®åˆ›æ–°**ï¼šAugServeçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä¸¤é˜¶æ®µè‡ªé€‚åº”è¯·æ±‚è°ƒåº¦ç­–ç•¥å’ŒåŠ¨æ€tokenæ‰¹å¤„ç†æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„é™æ€è°ƒåº¦ç­–ç•¥ç›¸æ¯”ï¼ŒAugServeèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”è¯·æ±‚ç‰¹å¾å’Œç³»ç»ŸçŠ¶æ€çš„å˜åŒ–ï¼Œä»è€Œæé«˜æœ‰æ•ˆååé‡å’ŒæœåŠ¡è´¨é‡ã€‚åŠ¨æ€tokenæ‰¹å¤„ç†æœºåˆ¶èƒ½å¤Ÿæ ¹æ®ç¡¬ä»¶çŠ¶æ€å’Œå®æ—¶è´Ÿè½½åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨ç¡¬ä»¶èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒAugServeä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„é¢„æµ‹æ¨¡å‹æ¥ä¼°è®¡è¯·æ±‚çš„ä¼˜å…ˆçº§ã€‚è¯¥æ¨¡å‹å¯ä»¥åŸºäºè¯·æ±‚çš„å…ƒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚ï¼Œè¯·æ±‚æ‰€éœ€çš„å·¥å…·ç±»å‹ã€é¢„è®¡çš„è®¡ç®—å¤æ‚åº¦ç­‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒAugServeä½¿ç”¨ä¸€ä¸ªåé¦ˆæ§åˆ¶æœºåˆ¶æ¥åŠ¨æ€è°ƒæ•´è¯·æ±‚çš„è°ƒåº¦é¡ºåºã€‚è¯¥æœºåˆ¶æ ¹æ®ç³»ç»Ÿè´Ÿè½½ã€ç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡ç­‰å®æ—¶ä¿¡æ¯ï¼Œè°ƒæ•´è¯·æ±‚çš„ä¼˜å…ˆçº§ã€‚åŠ¨æ€tokenæ‰¹å¤„ç†æ¨¡å—ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å¯å‘å¼ç®—æ³•æ¥è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼Œè¯¥ç®—æ³•æ ¹æ®ç¡¬ä»¶çŠ¶æ€å’Œå®æ—¶è´Ÿè½½æ¥è°ƒæ•´æ‰¹å¤„ç†å¤§å°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.04013/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.04013/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.04013/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAugServeåœ¨æœ‰æ•ˆååé‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰ç³»ç»Ÿã€‚ä¸vLLMç›¸æ¯”ï¼ŒAugServeçš„æœ‰æ•ˆååé‡æé«˜äº†4.7å€ï¼›ä¸InferCeptç›¸æ¯”ï¼Œæé«˜äº†3.3å€ã€‚åŒæ—¶ï¼ŒAugServeè¿˜æ˜¾è‘—é™ä½äº†é¦–ä¸ªtokenç”Ÿæˆæ—¶é—´ï¼ˆTTFTï¼‰ï¼Œä¸vLLMå’ŒInferCeptç›¸æ¯”ï¼Œåˆ†åˆ«é™ä½äº†é«˜è¾¾96.3%å’Œ95.0%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒAugServeèƒ½å¤Ÿæœ‰æ•ˆæé«˜å¢å¼ºå‹LLMæ¨ç†æœåŠ¡çš„æ•ˆç‡å’Œè´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AugServeå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆå¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡å’Œä¼˜åŒ–æœåŠ¡è´¨é‡ï¼ŒAugServeå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½æœåŠ¡æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„AIæ¨ç†æœåŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7x and 3.3x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.

