---
layout: default
title: Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models
---

# Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14427" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14427v1</a>
  <a href="https://arxiv.org/pdf/2512.14427.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14427v1" onclick="toggleFavorite(this, '2512.14427v1', 'Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gabriele Prato, Shagun Sodhani, Alessandro Sordoni, Sarath Chandar

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶æ–‡æ¡£æ‰“åŒ…ç­–ç•¥å¯¹å¤§è¯­è¨€æ¨¡å‹å¤šè·³æ¨ç†èƒ½åŠ›çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ–‡æ¡£æ‰“åŒ…` `å¤šè·³æ¨ç†` `æ¨¡å‹è®­ç»ƒ` `æ¶ˆèå®éªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒé€šå¸¸é‡‡ç”¨æ–‡æ¡£æ‰“åŒ…ç­–ç•¥ä»¥æå‡è®¡ç®—æ•ˆç‡ï¼Œä½†å…¶å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ½œåœ¨å½±å“å°šä¸æ˜ç¡®ã€‚
2. è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¯”ä¸åŒæ–‡æ¡£æ‰“åŒ…ç­–ç•¥ï¼Œåˆ†æå…¶å¯¹LLMå¤šè·³æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œæ—¨åœ¨ä¼˜åŒ–æ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ–‡æ¡£æ‰“åŒ…èƒ½åœ¨å¢åŠ è®¡ç®—æˆæœ¬çš„åŒæ—¶æå‡æ¨¡å‹æ€§èƒ½ï¼Œæ¶ˆèå®éªŒæ­ç¤ºäº†æ‰“åŒ…ä¼˜åŠ¿çš„å…³é”®å› ç´ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ–‡æ¡£æ‰“åŒ…ç­–ç•¥å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ½œåœ¨å¤šè·³æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚é€šå¸¸ï¼Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä¼šå°†å¤šä¸ªæ–‡æ¡£æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚ç„¶è€Œï¼Œè¿™ç§åšæ³•å¯¹æ¨¡å‹èƒ½åŠ›çš„å½±å“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ¢ç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸åœ¨å•ä¸ªæ–‡æ¡£ä¸Šè®­ç»ƒç›¸æ¯”ï¼Œæ‰“åŒ…å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç†è§£å…¶å†…åœ¨æœºåˆ¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ¶ˆèç ”ç©¶ï¼Œç¡®å®šäº†è§£é‡Šæ‰“åŒ…ä¼˜åŠ¿çš„å…³é”®å› ç´ ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„ç ”ç©¶åŠ æ·±äº†å¯¹LLMè®­ç»ƒåŠ¨æ€çš„ç†è§£ï¼Œå¹¶ä¸ºä¼˜åŒ–æ¨¡å‹å¼€å‘æä¾›äº†å®ç”¨çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨ç ”ç©¶æ–‡æ¡£æ‰“åŒ…è¿™ä¸€å¸¸ç”¨çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæŠ€å·§ï¼Œå¯¹æ¨¡å‹å¤šè·³æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹æ–‡æ¡£æ‰“åŒ…ç­–ç•¥çš„æ·±å…¥åˆ†æï¼Œæ— æ³•æœ‰æ•ˆæŒ‡å¯¼æ¨¡å‹è®­ç»ƒï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”ä¸åŒæ–‡æ¡£æ‰“åŒ…ç­–ç•¥ä¸‹è®­ç»ƒçš„LLMï¼Œè¯„ä¼°å…¶åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡æ¶ˆèå®éªŒï¼Œåˆ†æä¸åŒå› ç´ å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä»è€Œæ­ç¤ºæ–‡æ¡£æ‰“åŒ…å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å†…åœ¨æœºåˆ¶ã€‚ è¿™ç§æ€è·¯æ—¨åœ¨æ‰¾åˆ°æœ€ä¼˜çš„æ–‡æ¡£æ‰“åŒ…ç­–ç•¥ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) å®šä¹‰ä¸åŒçš„æ–‡æ¡£æ‰“åŒ…ç­–ç•¥ï¼Œä¾‹å¦‚éšæœºæ‰“åŒ…ã€æŒ‰ä¸»é¢˜æ‰“åŒ…ç­‰ï¼›2) ä½¿ç”¨ä¸åŒçš„æ‰“åŒ…ç­–ç•¥è®­ç»ƒLLMï¼›3) åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼›4) è¿›è¡Œæ¶ˆèå®éªŒï¼Œåˆ†æä¸åŒå› ç´ ï¼ˆå¦‚æ–‡æ¡£æ•°é‡ã€æ–‡æ¡£é•¿åº¦ç­‰ï¼‰å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†æ–‡æ¡£æ‰“åŒ…ç­–ç•¥å¯¹LLMå¤šè·³æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ•°æ®ï¼Œè€Œå¿½ç•¥äº†æ–‡æ¡£æ‰“åŒ…è¿™ä¸€é‡è¦å› ç´ ã€‚è¯¥ç ”ç©¶é€šè¿‡å®éªŒæ­ç¤ºäº†æ–‡æ¡£æ‰“åŒ…å¯¹æ¨¡å‹æ€§èƒ½çš„å†…åœ¨æœºåˆ¶ï¼Œä¸ºä¼˜åŒ–æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šè·³æ¨ç†ä»»åŠ¡çš„é€‰æ‹©ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼›2) æ–‡æ¡£æ‰“åŒ…ç­–ç•¥çš„è®¾è®¡ï¼Œéœ€è¦è¦†ç›–ä¸åŒçš„æ‰“åŒ…æ–¹å¼ï¼›3) æ¶ˆèå®éªŒçš„è®¾è®¡ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåˆ†ç¦»ä¸åŒå› ç´ çš„å½±å“ï¼›4) æ€§èƒ½è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ï¼Œéœ€è¦èƒ½å¤Ÿå‡†ç¡®åæ˜ æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€‚å½“çš„æ–‡æ¡£æ‰“åŒ…ç­–ç•¥å¯ä»¥æ˜¾è‘—æå‡LLMçš„å¤šè·³æ¨ç†èƒ½åŠ›ã€‚ä¸åœ¨å•ä¸ªæ–‡æ¡£ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œé‡‡ç”¨ä¼˜åŒ–æ‰“åŒ…ç­–ç•¥è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡äº†X%ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥æ­ç¤ºäº†æ–‡æ¡£æ•°é‡ã€æ–‡æ¡£é•¿åº¦ç­‰å› ç´ å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸ºä¼˜åŒ–æ–‡æ¡£æ‰“åŒ…ç­–ç•¥æä¾›äº†ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¼˜åŒ–ï¼Œå¸®åŠ©å¼€å‘è€…é€‰æ‹©åˆé€‚çš„æ–‡æ¡£æ‰“åŒ…ç­–ç•¥ï¼Œæå‡æ¨¡å‹åœ¨é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ¨ç†ã€å¤æ‚æ–‡æœ¬ç†è§£ç­‰é¢†åŸŸçš„æ€§èƒ½ã€‚é€šè¿‡ä¼˜åŒ–è®­ç»ƒæ–¹å¼ï¼Œå¯ä»¥é™ä½æ¨¡å‹å¼€å‘æˆæœ¬ï¼Œæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæ½œåœ¨å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.

