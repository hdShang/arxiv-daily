---
layout: default
title: A Unified Sparse Attention via Multi-Granularity Compression
---

# A Unified Sparse Attention via Multi-Granularity Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14082" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14082v1</a>
  <a href="https://arxiv.org/pdf/2512.14082.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14082v1" onclick="toggleFavorite(this, '2512.14082v1', 'A Unified Sparse Attention via Multi-Granularity Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siran Liu, Zane Cao, Yongchao He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniSparseï¼šä¸€ç§é€šè¿‡å¤šç²’åº¦å‹ç¼©å®ç°çš„ç»Ÿä¸€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–æ³¨æ„åŠ›` `é•¿æ–‡æœ¬å¤„ç†` `å¤šç²’åº¦å‹ç¼©` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•å­˜åœ¨è®­ç»ƒæˆæœ¬é«˜æ˜‚æˆ–æ— æ³•ç›´æ¥ä½œä¸ºåŠ é€Ÿæ’ä»¶åº”ç”¨äºå…¶ä»–æ¨¡å‹çš„å±€é™æ€§ã€‚
2. UniSparseé€šè¿‡å¼•å…¥å¤åˆtokenï¼Œåˆ©ç”¨å¤šç²’åº¦å‹ç¼©å’Œå—çº§é€‰æ‹©åŠ¨æ€æ„å»ºç¨€ç–æ³¨æ„åŠ›ï¼Œæå‡æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUniSparseåœ¨å¤šç§æ¨¡æ€å’Œä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡æ¥è¿‘å®Œæ•´æ³¨æ„åŠ›ï¼Œä¸”è®¡ç®—é€Ÿåº¦ä¼˜äºFlashAttentionã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè½®å¯¹è¯å’Œç¨‹åºåˆ†æç­‰åº”ç”¨ä¸­å¯¹é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºUniSparseï¼Œä¸€ç§ç»Ÿä¸€çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚UniSparseé€šè¿‡å¼•å…¥å¤åˆtokençš„æ¦‚å¿µï¼Œå³èšåˆå¤šç²’åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç´§å‡‘è¡¨ç¤ºï¼ŒåŠ¨æ€æ„å»ºç¨€ç–æ³¨æ„åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šç²’åº¦å‹ç¼©å’Œå—çº§é€‰æ‹©ï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯¹GPUç¡¬ä»¶å‹å¥½çš„æ‰§è¡Œã€‚åœ¨ä»åˆæˆåŸºå‡†æµ‹è¯•åˆ°å®é™…åº”ç”¨çš„å¤šç§æ¨¡æ€å’Œä»»åŠ¡ä¸­ï¼ŒUniSparseåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼ˆå¦‚MInferenceã€XAttentionã€FlexPrefillï¼‰ï¼Œå®ç°äº†â‰¥99%çš„å®Œæ•´æ³¨æ„åŠ›å‡†ç¡®ç‡ï¼Œå¹¶ä¸”æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æ¯”FlashAttentionå¿«é«˜è¾¾2.61å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦éšåºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œæˆä¸ºé•¿æ–‡æœ¬å¤„ç†çš„ç“¶é¢ˆã€‚ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è™½ç„¶èƒ½ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†è¦ä¹ˆéœ€è¦å¤§é‡çš„è®­ç»ƒæˆæœ¬ï¼Œè¦ä¹ˆåœ¨æ¨ç†æ•ˆç‡æˆ–è·¨æ¨¡æ€é€šç”¨æ€§ä¸Šæœ‰æ‰€å¦¥åã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniSparseçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥â€œå¤åˆtokenâ€çš„æ¦‚å¿µï¼Œå°†å¤šä¸ªtokenå‹ç¼©æˆä¸€ä¸ªæ›´ç´§å‡‘çš„è¡¨ç¤ºï¼Œä»è€Œå‡å°‘éœ€è¦è®¡ç®—æ³¨æ„åŠ›çš„tokenæ•°é‡ã€‚è¿™ç§å‹ç¼©æ˜¯å¤šç²’åº¦çš„ï¼Œå…è®¸æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡ä¸Šèšåˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniSparseçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **å¤šç²’åº¦å‹ç¼©**ï¼šå°†åŸå§‹tokenåºåˆ—å‹ç¼©æˆä¸åŒç²’åº¦çš„å¤åˆtokenåºåˆ—ã€‚2) **å—çº§é€‰æ‹©**ï¼šæ ¹æ®æŸç§ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºé‡è¦æ€§è¯„åˆ†ï¼‰é€‰æ‹©ä¸€éƒ¨åˆ†å¤åˆtokenå—è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚3) **ç¨€ç–æ³¨æ„åŠ›è®¡ç®—**ï¼šä»…åœ¨é€‰å®šçš„å¤åˆtokenå—ä¹‹é—´è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚4) **ä¿¡æ¯èšåˆ**ï¼šå°†å¤åˆtokençš„ä¿¡æ¯è§£å‹ç¼©å¹¶èšåˆåˆ°åŸå§‹tokenè¡¨ç¤ºä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniSparseçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€æ€§å’Œå¤šç²’åº¦å‹ç¼©ã€‚å®ƒä¸åƒå…¶ä»–ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•é‚£æ ·ä¾èµ–äºç‰¹å®šçš„è®­ç»ƒæˆ–é¢„å®šä¹‰çš„æ¨¡å¼ï¼Œè€Œæ˜¯å¯ä»¥åŠ¨æ€åœ°é€‚åº”ä¸åŒçš„è¾“å…¥å’Œä»»åŠ¡ã€‚å¤šç²’åº¦å‹ç¼©å…è®¸æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡ä¸ŠæŠ½è±¡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šUniSparseçš„å…·ä½“å®ç°ç»†èŠ‚å¯èƒ½åŒ…æ‹¬ï¼š1) **å‹ç¼©ç­–ç•¥**ï¼šå¦‚ä½•å°†å¤šä¸ªtokenå‹ç¼©æˆä¸€ä¸ªå¤åˆtokenï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨å¹³å‡æ± åŒ–ã€æœ€å¤§æ± åŒ–æˆ–å¯å­¦ä¹ çš„çº¿æ€§å˜æ¢ï¼‰ã€‚2) **é€‰æ‹©ç­–ç•¥**ï¼šå¦‚ä½•é€‰æ‹©é‡è¦çš„å¤åˆtokenå—ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºæ³¨æ„åŠ›æƒé‡ã€é‡è¦æ€§è¯„åˆ†æˆ–éšæœºæŠ½æ ·ï¼‰ã€‚3) **æ³¨æ„åŠ›è®¡ç®—**ï¼šä½¿ç”¨å“ªç§æ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¾‹å¦‚ï¼Œæ ‡å‡†è‡ªæ³¨æ„åŠ›ã€çº¿æ€§æ³¨æ„åŠ›æˆ–FlashAttentionï¼‰ã€‚4) **æŸå¤±å‡½æ•°**ï¼šå¦‚ä½•è®­ç»ƒæ¨¡å‹ä»¥å­¦ä¹ æœ‰æ•ˆçš„å‹ç¼©å’Œé€‰æ‹©ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±ã€KLæ•£åº¦æˆ–å¯¹æ¯”æŸå¤±ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UniSparseåœ¨å¤šä¸ªæ¨¡æ€å’Œä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨é•¿æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒUniSparseåœ¨ä¿æŒæ¥è¿‘å®Œæ•´æ³¨æ„åŠ›å‡†ç¡®ç‡ï¼ˆâ‰¥99%ï¼‰çš„åŒæ—¶ï¼Œæ¯”FlashAttentionå¿«é«˜è¾¾2.61å€ã€‚æ­¤å¤–ï¼ŒUniSparseåœ¨åˆæˆåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œåº”ç”¨ä¸­å‡ä¼˜äºå…¶ä»–ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œå¦‚MInferenceã€XAttentionå’ŒFlexPrefillã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniSparseå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿åºåˆ—æ•°æ®çš„åœºæ™¯ä¸­ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥ç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ã€ç¨‹åºåˆ†æã€æ–‡æ¡£æ‘˜è¦å’Œæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒUniSparseè¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–æ¨¡æ€çš„æ•°æ®ï¼Œå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼Œä»¥åŠ é€Ÿè§†è§‰Transformerå’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\ge$ 99% of full-attention accuracy and up to 2.61$\times$ faster attention computation than FlashAttention.

