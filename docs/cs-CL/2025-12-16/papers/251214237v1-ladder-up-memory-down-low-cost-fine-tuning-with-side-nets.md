---
layout: default
title: Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets
---

# Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14237v1</a>
  <a href="https://arxiv.org/pdf/2512.14237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14237v1" onclick="toggleFavorite(this, '2512.14237v1', 'Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Estelle Zheng, Nathan Cerisara, SÃ©bastien Warichet, Emmanuel Helbert, Christophe Cerisara

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLadder Side Tuningï¼Œä»¥ä½æˆæœ¬å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å†…å­˜ä¼˜åŒ–` `ä¾§ç½‘ç»œ` `Ladder Side Tuning` `xLadder` `ä½èµ„æºè®¡ç®—` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚QLoRAï¼‰è™½å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œä½†å®Œæ•´æ¨¡å‹åå‘ä¼ æ’­ä»å¯¼è‡´é«˜å†…å­˜å ç”¨ï¼Œé™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¶ˆè´¹çº§GPUä¸Šçš„å¾®è°ƒã€‚
2. è®ºæ–‡æå‡ºLadder Side Tuning (LST)ï¼Œé€šè¿‡å¢åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½å¾®è°ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLSTåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šä¸QLoRAæ€§èƒ½ç›¸å½“ï¼Œä½†å†…å­˜å ç”¨å‡å°‘50%ï¼Œå¹¶æå‡ºäº†xLadderå˜ä½“ä»¥æå‡æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸å—é™äºæ¶ˆè´¹çº§GPUçš„å†…å­˜ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚QLoRAå‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä½†ä»ç„¶å› å®Œæ•´æ¨¡å‹ä¸­çš„åå‘ä¼ æ’­è€Œäº§ç”Ÿé«˜å†…å­˜ä½¿ç”¨ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œä¸€ç§å¾ˆå°‘è¢«æ¢ç´¢çš„PEFTæŠ€æœ¯ï¼Œå®ƒå¢åŠ äº†ä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œï¼Œå¹¶è¡¨æ˜å®ƒåœ¨è®¡ç®—æ‰©å±•æ–œç‡ä¸Šä¸QLoRAç›¸åŒ¹é…ï¼ŒåŒæ—¶å°†å³°å€¼å†…å­˜å‡å°‘äº†50%ã€‚åœ¨è·¨è¶Šè‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦å’ŒLLM-criticä»»åŠ¡çš„ä¸åŒä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLSTçš„æ€§èƒ½ä¸QLoRAçš„å‡†ç¡®æ€§ç›¸å½“ï¼ŒåŒæ—¶æ›´å…·å†…å­˜æ•ˆç‡ã€‚è¿™ç§æ•ˆç‡ä½¿å¾—å¯ä»¥åœ¨å•ä¸ª12 GBæ¶ˆè´¹çº§GPUä¸Šä½¿ç”¨2k-tokenä¸Šä¸‹æ–‡å¾®è°ƒ7Bå‚æ•°æ¨¡å‹ï¼Œè€Œæ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹â€”â€”åœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼ŒQLoRAä¼šè€—å°½å†…å­˜ã€‚é™¤äº†å†…å­˜æ•ˆç‡ä¹‹å¤–ï¼Œæœ¬æ–‡è¿˜å»ºç«‹äº†ç¼©æ”¾å®šå¾‹ï¼Œè¡¨æ˜LSTçš„ç¼©æ”¾æ–¹å¼ä¸QLoRAç±»ä¼¼ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥xLadderæ¥åˆ©ç”¨Ladderçš„æ¶æ„çµæ´»æ€§ï¼ŒxLadderæ˜¯ä¸€ç§æ·±åº¦æ‰©å±•çš„å˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ æœ‰æ•ˆæ·±åº¦ï¼Œå¹¶åœ¨å›ºå®šå‚æ•°æ•°é‡ä¸‹ç¼©çŸ­æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚Ladderåœ¨å†…å­˜æ˜¯ç“¶é¢ˆæ—¶è¡¨ç°å‡ºè‰²ï¼›xLadderåœ¨æ­¤åŸºç¡€ä¸Šé€šè¿‡æ— éœ€é¢å¤–å†…å­˜å¼€é”€å³å¯å®ç°æ›´æ·±å±‚æ¬¡çš„æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œå¦‚QLoRAï¼Œè™½ç„¶å‡å°‘äº†éœ€è¦è®­ç»ƒçš„å‚æ•°é‡ï¼Œä½†æ˜¯ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹æœ¬èº«çš„ç»“æ„ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä»ç„¶éœ€è¦å ç”¨å¤§é‡çš„å†…å­˜ã€‚è¿™ä½¿å¾—åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œå¦‚ä½•è¿›ä¸€æ­¥é™ä½å¾®è°ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ€§èƒ½ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Ladder Side Tuning (LST) æ¶æ„ï¼Œåœ¨åŸå§‹æ¨¡å‹æ—è¾¹æ·»åŠ ä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œåªè®­ç»ƒè¿™ä¸ªä¾§ç½‘ç»œï¼Œè€ŒåŸå§‹æ¨¡å‹çš„å‚æ•°ä¿æŒä¸å˜ã€‚è¿™æ ·å¯ä»¥é¿å…å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»è€Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚åŒæ—¶ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¾§ç½‘ç»œç»“æ„ï¼Œå¯ä»¥ä¿è¯æ¨¡å‹çš„æ€§èƒ½ä¸ä¼šå—åˆ°å¤ªå¤§çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLST çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šåŸå§‹çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œï¼ˆLadderï¼‰ã€‚è¾“å…¥æ•°æ®åŒæ—¶è¾“å…¥åˆ°è¿™ä¸¤ä¸ªç½‘ç»œä¸­ã€‚åŸå§‹æ¨¡å‹çš„è¾“å‡ºå’Œä¾§ç½‘ç»œçš„è¾“å‡ºä¼šè¢«èåˆï¼Œç„¶åç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„é¢„æµ‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªæœ‰ä¾§ç½‘ç»œçš„å‚æ•°ä¼šè¢«æ›´æ–°ï¼Œè€ŒåŸå§‹æ¨¡å‹çš„å‚æ•°ä¿æŒå›ºå®šã€‚xLadder æ˜¯ LST çš„ä¸€ä¸ªå˜ä½“ï¼Œå®ƒé€šè¿‡å¢åŠ ä¾§ç½‘ç»œçš„æ·±åº¦å’Œå¼•å…¥äº¤å‰è¿æ¥æ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šLST çš„å…³é”®åˆ›æ–°åœ¨äºå®ƒå°†å¾®è°ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨ä¸æ¨¡å‹çš„å¤§å°è§£è€¦ã€‚é€šè¿‡åªè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œï¼ŒLST å¯ä»¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè€Œæ— éœ€å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­ã€‚æ­¤å¤–ï¼ŒxLadder é€šè¿‡å¢åŠ ä¾§ç½‘ç»œçš„æ·±åº¦å’Œå¼•å…¥äº¤å‰è¿æ¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€å¢åŠ é¢å¤–çš„å†…å­˜å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šLST çš„å…³é”®è®¾è®¡åŒ…æ‹¬ä¾§ç½‘ç»œçš„ç»“æ„å’ŒèåˆåŸå§‹æ¨¡å‹è¾“å‡ºå’Œä¾§ç½‘ç»œè¾“å‡ºçš„æ–¹å¼ã€‚ä¾§ç½‘ç»œé€šå¸¸æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„ç¥ç»ç½‘ç»œï¼Œå¯ä»¥ä½¿ç”¨å„ç§ä¸åŒçš„ç»“æ„ï¼Œä¾‹å¦‚å¤šå±‚æ„ŸçŸ¥æœºæˆ–å·ç§¯ç¥ç»ç½‘ç»œã€‚èåˆæ–¹å¼å¯ä»¥ä½¿ç”¨ç®€å•çš„åŠ æƒå¹³å‡æˆ–æ›´å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚xLadder çš„å…³é”®è®¾è®¡åœ¨äºäº¤å‰è¿æ¥çš„å¼•å…¥ï¼Œå®ƒå¯ä»¥è®©ä¾§ç½‘ç»œçš„ä¸åŒå±‚ä¹‹é—´è¿›è¡Œä¿¡æ¯äº¤æµï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ä¼šæ ¹æ®ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLSTåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šä¸QLoRAçš„æ€§èƒ½ç›¸å½“ï¼Œä½†å†…å­˜å ç”¨å‡å°‘äº†50%ã€‚ä¾‹å¦‚ï¼Œåœ¨7Bå‚æ•°æ¨¡å‹ä¸Šï¼ŒLSTå¯ä»¥åœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šä½¿ç”¨2k-tokenä¸Šä¸‹æ–‡è¿›è¡Œå¾®è°ƒï¼Œè€Œæ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œè€ŒQLoRAä¼šè€—å°½å†…å­˜ã€‚xLadderé€šè¿‡å¢åŠ ä¾§ç½‘ç»œçš„æ·±åº¦å’Œå¼•å…¥äº¤å‰è¿æ¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å’Œå®šåˆ¶LLMï¼Œå¯ä»¥å®ç°ä¸ªæ€§åŒ–æ¨èã€æ™ºèƒ½åŠ©æ‰‹ç­‰åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åŠ é€ŸLLMåœ¨ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ï¼Œå¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

