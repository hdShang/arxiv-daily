---
layout: default
title: Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets
---

# Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets

**arXiv**: [2512.14237v1](https://arxiv.org/abs/2512.14237) | [PDF](https://arxiv.org/pdf/2512.14237.pdf)

**ä½œè€…**: Estelle Zheng, Nathan Cerisara, SÃ©bastien Warichet, Emmanuel Helbert, Christophe Cerisara

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLadder Side Tuningæ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§ä¾§ç½‘ç»œè§£å†³å¤§è¯­è¨€æ¨¡åž‹å¾®è°ƒä¸­çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§è¯­è¨€æ¨¡åž‹` `å†…å­˜ä¼˜åŒ–` `ä¾§ç½‘ç»œ` `è½»é‡çº§è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `GPUèµ„æºå—é™` `æ‰©å±•å®šå¾‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å¦‚QLoRAè™½å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼Œä½†åå‘ä¼ æ’­ä»å¯¼è‡´é«˜å†…å­˜å ç”¨ï¼Œé™åˆ¶å¤§æ¨¡åž‹åœ¨æ¶ˆè´¹çº§GPUä¸Šçš„å¾®è°ƒã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºLadder Side Tuningï¼Œé€šè¿‡æ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œä»…å¾®è°ƒä¾§ç½‘ç»œå‚æ•°ï¼Œå¤§å¹…é™ä½Žå†…å­˜éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒæ¨¡åž‹æ€§èƒ½ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒLSTæ€§èƒ½ä¸ŽQLoRAç›¸å½“ï¼Œå³°å€¼å†…å­˜é™ä½Ž50%ï¼Œæ”¯æŒ70äº¿å‚æ•°æ¨¡åž‹åœ¨12GB GPUä¸Šå¾®è°ƒã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒå¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å¸¸å—é™äºŽå•†ç”¨GPUçš„å†…å­˜å®¹é‡ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚QLoRAè™½å‡å°‘äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œä½†å®Œæ•´æ¨¡åž‹çš„åå‘ä¼ æ’­ä»å¯¼è‡´é«˜å†…å­˜å ç”¨ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œä¸€ç§è¾ƒå°‘è¢«æŽ¢ç´¢çš„PEFTæŠ€æœ¯ï¼Œå®ƒé€šè¿‡æ·»åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œåœ¨ä¿æŒä¸ŽQLoRAç›¸ä¼¼è®¡ç®—æ‰©å±•æ–œçŽ‡çš„åŒæ—¶ï¼Œå°†å³°å€¼å†…å­˜é™ä½Ž50%ã€‚åœ¨æ¶µç›–è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦å’ŒLLMæ‰¹è¯„ä»»åŠ¡çš„ä¸åŒä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLSTå¹³å‡æ€§èƒ½ä¸ŽQLoRAç›¸å½“ï¼ŒåŒæ—¶å†…å­˜æ•ˆçŽ‡æ›´é«˜ã€‚è¿™ç§æ•ˆçŽ‡ä½¿å¾—åœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒ70äº¿å‚æ•°æ¨¡åž‹æˆä¸ºå¯èƒ½ï¼Œæ”¯æŒ2kä»¤ç‰Œä¸Šä¸‹æ–‡ä¸”æ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹â€”â€”åœ¨è¿™äº›æ¡ä»¶ä¸‹QLoRAä¼šè€—å°½å†…å­˜ã€‚é™¤äº†å†…å­˜æ•ˆçŽ‡ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†æ‰©å±•å®šå¾‹ï¼Œæ˜¾ç¤ºLSTä¸ŽQLoRAå…·æœ‰ç›¸ä¼¼çš„æ‰©å±•æ€§ã€‚é€šè¿‡åˆ©ç”¨Ladderçš„æž¶æž„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†xLadderï¼Œä¸€ç§æ·±åº¦æ‰©å±•å˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿žæŽ¥å¢žåŠ æœ‰æ•ˆæ·±åº¦ï¼Œå¹¶åœ¨å›ºå®šå‚æ•°æ•°é‡ä¸‹ç¼©çŸ­æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚Ladderåœ¨å†…å­˜å—é™æ—¶è¡¨çŽ°å¼ºåŠ²ï¼›xLadderåœ¨æ­¤åŸºç¡€ä¸Šå®žçŽ°äº†æ›´æ·±å±‚æŽ¨ç†ï¼Œä¸”æ— é¢å¤–å†…å­˜å¼€é”€ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å¾®è°ƒä¸­çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚çŽ°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚QLoRAè™½å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼Œä½†åå‘ä¼ æ’­è¿‡ç¨‹ä»éœ€è®¡ç®—å®Œæ•´æ¨¡åž‹çš„æ¢¯åº¦ï¼Œå¯¼è‡´é«˜å†…å­˜å ç”¨ï¼Œé™åˆ¶äº†åœ¨æ¶ˆè´¹çº§GPUï¼ˆå¦‚12GBå†…å­˜ï¼‰ä¸Šå¾®è°ƒå¤§è§„æ¨¡æ¨¡åž‹ï¼ˆå¦‚70äº¿å‚æ•°ï¼‰çš„å¯è¡Œæ€§ï¼Œå°¤å…¶æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡ï¼ˆå¦‚2kä»¤ç‰Œï¼‰åœºæ™¯ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡æ–°åˆ©ç”¨Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œä¸€ç§åŸºäºŽä¾§ç½‘ç»œçš„PEFTæŠ€æœ¯ã€‚é€šè¿‡æ·»åŠ ä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œï¼ˆside networkï¼‰åˆ°é¢„è®­ç»ƒLLMä¸­ï¼Œä»…å¾®è°ƒä¾§ç½‘ç»œçš„å‚æ•°ï¼Œè€Œå†»ç»“ä¸»æ¨¡åž‹å‚æ•°ï¼Œä»Žè€Œå¤§å¹…å‡å°‘åå‘ä¼ æ’­æ—¶çš„å†…å­˜éœ€æ±‚ï¼Œå› ä¸ºæ¢¯åº¦è®¡ç®—ä»…é™äºŽä¾§ç½‘ç»œï¼Œè€Œéžæ•´ä¸ªæ¨¡åž‹ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡åž‹ï¼ˆä¸»ç½‘ç»œï¼‰å’Œä¸€ä¸ªé™„åŠ çš„è½»é‡çº§ä¾§ç½‘ç»œã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä¸»ç½‘ç»œçš„å‚æ•°è¢«å†»ç»“ï¼Œä»…ä¾§ç½‘ç»œçš„å‚æ•°å¯è®­ç»ƒã€‚ä¾§ç½‘ç»œé€šè¿‡è¿žæŽ¥ç‚¹ï¼ˆå¦‚ä¸­é—´å±‚è¾“å‡ºï¼‰ä¸Žä¸»ç½‘ç»œäº¤äº’ï¼ŒæŽ¥æ”¶ä¸»ç½‘ç»œçš„æ¿€æ´»å€¼ï¼Œå¤„ç†åŽè¾“å‡ºè¡¥å……ä¿¡æ¯ï¼Œä¸Žä¸»ç½‘ç»œè¾“å‡ºç»“åˆä»¥å®Œæˆä¸‹æ¸¸ä»»åŠ¡ã€‚è®­ç»ƒæ—¶ï¼ŒæŸå¤±å‡½æ•°åŸºäºŽä»»åŠ¡ç›®æ ‡ï¼ˆå¦‚åˆ†ç±»ã€ç”Ÿæˆï¼‰è®¡ç®—ï¼Œæ¢¯åº¦ä»…é€šè¿‡ä¾§ç½‘ç»œåå‘ä¼ æ’­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯ç³»ç»Ÿæ€§åœ°éªŒè¯å’Œæ‰©å±•LSTä½œä¸ºå†…å­˜é«˜æ•ˆçš„PEFTæ–¹æ³•ã€‚ä¸ŽQLoRAç­‰çŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLSTçš„æœ¬è´¨åŒºåˆ«åœ¨äºŽå…¶æž¶æž„è®¾è®¡ï¼šå®ƒé€šè¿‡ä¾§ç½‘ç»œå®žçŽ°å‚æ•°éš”ç¦»ï¼Œä½¿å¾—å†…å­˜å¼€é”€ä¸Žä¾§ç½‘ç»œè§„æ¨¡è€Œéžä¸»æ¨¡åž‹è§„æ¨¡æˆæ­£æ¯”ï¼Œä»Žè€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½Žå†…å­˜å ç”¨ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥xLadderå˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿žæŽ¥å¢žåŠ ç½‘ç»œæ·±åº¦ï¼Œæå‡æŽ¨ç†èƒ½åŠ›è€Œæ— é¢å¤–å†…å­˜æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ä¾§ç½‘ç»œçš„è½»é‡åŒ–ç»“æž„ï¼ˆå¦‚å°åž‹å‰é¦ˆç½‘ç»œï¼‰ï¼Œä»¥å‡å°‘å‚æ•°æ•°é‡ï¼›è¿žæŽ¥ç­–ç•¥ï¼ˆå¦‚ä»Žä¸»ç½‘ç»œç‰¹å®šå±‚æå–ç‰¹å¾ï¼‰ï¼Œä»¥ç¡®ä¿ä¿¡æ¯ä¼ é€’ï¼›æŸå¤±å‡½æ•°åŸºäºŽå…·ä½“ä¸‹æ¸¸ä»»åŠ¡ï¼ˆä¾‹å¦‚äº¤å‰ç†µæŸå¤±ç”¨äºŽåˆ†ç±»ä»»åŠ¡ï¼‰ï¼›å‚æ•°è®¾ç½®ä¸Šï¼Œä¾§ç½‘ç»œè§„æ¨¡è¿œå°äºŽä¸»æ¨¡åž‹ï¼ˆä¾‹å¦‚ä»…å ä¸»æ¨¡åž‹å‚æ•°çš„å°‘é‡ç™¾åˆ†æ¯”ï¼‰ï¼Œä»¥å®žçŽ°å†…å­˜èŠ‚çº¦ã€‚xLadderé€šè¿‡å¼•å…¥è·¨å±‚è¿žæŽ¥ï¼ˆcross-connectionsï¼‰æ‰©å±•æ·±åº¦ï¼Œå…·ä½“ç»†èŠ‚å¦‚è¿žæŽ¥æ–¹å¼å’Œå±‚æ•°è°ƒæ•´éœ€å‚è€ƒè®ºæ–‡å®žéªŒéƒ¨åˆ†ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœåŒ…æ‹¬ï¼šLSTåœ¨å¤šä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ï¼ˆè‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦ã€LLMæ‰¹è¯„ä»»åŠ¡ï¼‰ä¸­å¹³å‡æ€§èƒ½ä¸ŽQLoRAç›¸å½“ï¼Œä½†å³°å€¼å†…å­˜é™ä½Ž50%ã€‚å…·ä½“æ•°æ®ä¸Šï¼ŒLSTæ”¯æŒåœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒ70äº¿å‚æ•°æ¨¡åž‹ï¼Œä½¿ç”¨2kä»¤ç‰Œä¸Šä¸‹æ–‡ä¸”æ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œè€ŒQLoRAåœ¨ç›¸åŒæ¡ä»¶ä¸‹ä¼šè€—å°½å†…å­˜ã€‚æ‰©å±•å®šå¾‹æ˜¾ç¤ºLSTä¸ŽQLoRAå…·æœ‰ç›¸ä¼¼çš„è®¡ç®—æ‰©å±•æ–œçŽ‡ï¼ŒéªŒè¯äº†å…¶å¯æ‰©å±•æ€§ã€‚xLadderå˜ä½“è¿›ä¸€æ­¥æå‡äº†æŽ¨ç†æ·±åº¦ï¼Œåœ¨å›ºå®šå‚æ•°ä¸‹ç¼©çŸ­æ€ç»´é“¾ï¼Œå±•ç¤ºäº†æž¶æž„çµæ´»æ€§å¸¦æ¥çš„æ€§èƒ½å¢žç›Šã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çŽ¯å¢ƒä¸‹å¾®è°ƒå¤§è¯­è¨€æ¨¡åž‹ã€‚å®žé™…ä»·å€¼åŒ…æ‹¬ï¼šåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ï¼ˆå¦‚ä¸ªäººGPUï¼‰ä¸Šé«˜æ•ˆå¾®è°ƒæ¨¡åž‹ï¼Œç”¨äºŽå®šåˆ¶åŒ–AIåŠ©æ‰‹ã€å†…å®¹ç”Ÿæˆã€ä»£ç è¡¥å…¨ç­‰ä»»åŠ¡ï¼›åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–ç§»åŠ¨ç«¯éƒ¨ç½²è½»é‡çº§AIåº”ç”¨ï¼Œé™ä½Žè®¡ç®—æˆæœ¬ï¼›æœªæ¥å¯èƒ½æŽ¨åŠ¨æ›´çŽ¯ä¿çš„AIè®­ç»ƒï¼Œå‡å°‘èƒ½æºæ¶ˆè€—ã€‚å½±å“æ–¹é¢ï¼Œå®ƒä¸ºå†…å­˜æ•æ„Ÿçš„å¾®è°ƒåœºæ™¯æä¾›äº†æ–°è§£å†³æ–¹æ¡ˆï¼Œä¿ƒè¿›å¤§æ¨¡åž‹æŠ€æœ¯çš„æ™®åŠå’Œæ°‘ä¸»åŒ–ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

