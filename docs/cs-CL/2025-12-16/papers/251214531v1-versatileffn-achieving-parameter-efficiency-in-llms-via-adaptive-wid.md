---
layout: default
title: VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse
---

# VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14531v1</a>
  <a href="https://arxiv.org/pdf/2512.14531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14531v1" onclick="toggleFavorite(this, '2512.14531v1', 'VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ying Nie, Kai Han, Hongguang Li, Hang Zhou, Tianyu Guo, Enhua Wu, Xinghao Chen, Yunhe Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/huawei-noah/noah-research/tree)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVersatileFFNï¼Œé€šè¿‡è‡ªé€‚åº”å®½åº¦å’Œæ·±åº¦å¤ç”¨æå‡LLMå‚æ•°æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆ` `å‰é¦ˆç½‘ç»œ` `å‚æ•°å¤ç”¨` `å®½åº¦å’Œæ·±åº¦` `è‡ªé€‚åº”è·¯ç”±` `è®¡ç®—å¤ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMå‚æ•°é«˜æ•ˆæ–¹æ³•ä¸»è¦é€šè¿‡å‹ç¼©æ¨¡å‹å®ç°ï¼Œéš¾ä»¥çªç ´åŸå§‹æ¨¡å‹çš„èƒ½åŠ›ä¸Šé™ã€‚
2. VersatileFFNé€šè¿‡å®½åº¦å’Œæ·±åº¦ä¸¤ä¸ªç»´åº¦ä¸Šçš„å‚æ•°å¤ç”¨ï¼Œåœ¨å›ºå®šå‚æ•°é¢„ç®—ä¸‹æå‡æ¨¡å‹å®¹é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿæ‰©å±•å¸¦æ¥äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†ä¹Ÿå¯¼è‡´äº†å·¨å¤§çš„å†…å­˜æˆæœ¬ã€‚ç°æœ‰çš„å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼Œå¦‚å‰ªæå’Œé‡åŒ–ï¼Œä¸»è¦å‹ç¼©é¢„è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸å¢å¼ºæ¶æ„å®¹é‡ï¼Œä»è€Œè§¦åŠåŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºä¸Šé™ã€‚æœ¬æ–‡æå‡ºäº†VersatileFFNï¼Œä¸€ç§æ–°é¢–çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨å›ºå®šå‚æ•°é¢„ç®—å†…çµæ´»åœ°å¤ç”¨å®½åº¦å’Œæ·±åº¦ç»´åº¦ä¸Šçš„å‚æ•°ã€‚å—åˆ°è®¤çŸ¥åŒè¿‡ç¨‹ç†è®ºçš„å¯å‘ï¼ŒVersatileFFNåŒ…å«ä¸¤ä¸ªè‡ªé€‚åº”è·¯å¾„ï¼šä¸€ä¸ªå®½åº¦å¤šåŠŸèƒ½è·¯å¾„ï¼Œä»å•ä¸ªå…±äº«FFNç”Ÿæˆæ··åˆå­ä¸“å®¶ï¼Œæ¨¡æ‹Ÿç¨€ç–ä¸“å®¶è·¯ç”±è€Œä¸å¢åŠ å‚æ•°ï¼›ä»¥åŠä¸€ä¸ªæ·±åº¦å¤šåŠŸèƒ½è·¯å¾„ï¼Œé€’å½’åœ°åº”ç”¨ç›¸åŒçš„FFNæ¥æ¨¡æ‹Ÿæ›´æ·±å±‚æ¬¡çš„å¤„ç†ï¼Œä»¥åº”å¯¹å¤æ‚çš„tokenã€‚ä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥é—¨æ§åŠ¨æ€åœ°å¹³è¡¡è¿™ä¸¤ä¸ªè·¯å¾„ï¼Œå¼•å¯¼â€œç®€å•â€çš„tokené€šè¿‡é«˜æ•ˆçš„å®½åº¦è·¯å¾„ï¼Œå¹¶å°†æ›´æ·±å±‚æ¬¡çš„è¿­ä»£ç»†åŒ–åˆ†é…ç»™â€œå›°éš¾â€çš„tokenã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™ä¸¤ä¸ªè·¯å¾„éƒ½å¤ç”¨ç›¸åŒçš„å‚æ•°ï¼Œå› æ­¤æ‰€æœ‰é¢å¤–çš„å®¹é‡éƒ½æ¥è‡ªè®¡ç®—è€Œéå†…å­˜ã€‚åœ¨å„ç§åŸºå‡†å’Œæ¨¡å‹è§„æ¨¡ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿½æ±‚æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿé¢ä¸´ç€å·¨å¤§çš„å†…å­˜å¼€é”€ã€‚å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼Œå¦‚å‰ªæå’Œé‡åŒ–ï¼Œè™½ç„¶å¯ä»¥å‹ç¼©æ¨¡å‹ï¼Œä½†é€šå¸¸æ— æ³•æå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå—é™äºåŸå§‹æ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æœ‰é™çš„å‚æ•°é¢„ç®—ä¸‹ï¼Œæå‡LLMçš„æ€§èƒ½å’Œå®¹é‡æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVersatileFFNçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‚æ•°å¤ç”¨ï¼Œåœ¨ä¸å¢åŠ å‚æ•°æ•°é‡çš„å‰æä¸‹ï¼Œæå‡æ¨¡å‹çš„å®½åº¦å’Œæ·±åº¦ã€‚å€Ÿé‰´è®¤çŸ¥åŒè¿‡ç¨‹ç†è®ºï¼Œæ¨¡å‹è®¾è®¡äº†å®½åº¦å¤šåŠŸèƒ½è·¯å¾„å’Œæ·±åº¦å¤šåŠŸèƒ½è·¯å¾„ï¼Œåˆ†åˆ«å¤„ç†â€œç®€å•â€å’Œâ€œå›°éš¾â€çš„tokenã€‚é€šè¿‡éš¾åº¦æ„ŸçŸ¥é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œå®ç°å‚æ•°çš„æœ‰æ•ˆåˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVersatileFFNä¸»è¦åŒ…å«ä¸¤ä¸ªè·¯å¾„ï¼šå®½åº¦å¤šåŠŸèƒ½è·¯å¾„å’Œæ·±åº¦å¤šåŠŸèƒ½è·¯å¾„ã€‚å®½åº¦å¤šåŠŸèƒ½è·¯å¾„é€šè¿‡å…±äº«çš„FFNç”Ÿæˆæ··åˆå­ä¸“å®¶ï¼Œæ¨¡æ‹Ÿç¨€ç–ä¸“å®¶è·¯ç”±ã€‚æ·±åº¦å¤šåŠŸèƒ½è·¯å¾„åˆ™é€’å½’åœ°åº”ç”¨ç›¸åŒçš„FFNï¼Œæ¨¡æ‹Ÿæ›´æ·±å±‚æ¬¡çš„å¤„ç†ã€‚éš¾åº¦æ„ŸçŸ¥é—¨æ§æ¨¡å—æ ¹æ®tokençš„éš¾åº¦ï¼ŒåŠ¨æ€åœ°å¹³è¡¡è¿™ä¸¤ä¸ªè·¯å¾„çš„è®¡ç®—èµ„æºåˆ†é…ã€‚æ•´ä½“æ¶æ„æ—¨åœ¨é€šè¿‡è®¡ç®—å¤ç”¨æå‡æ¨¡å‹å®¹é‡ï¼Œè€Œéå¢åŠ å‚æ•°æ•°é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVersatileFFNçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å‚æ•°å¤ç”¨æœºåˆ¶ï¼Œå®ƒåœ¨å®½åº¦å’Œæ·±åº¦ä¸¤ä¸ªç»´åº¦ä¸Šå®ç°äº†å‚æ•°çš„çµæ´»å¤ç”¨ã€‚ä¸ä¼ ç»Ÿçš„å‚æ•°é«˜æ•ˆæ–¹æ³•ä¸åŒï¼ŒVersatileFFNä¸æ˜¯ç®€å•åœ°å‹ç¼©æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡è®¡ç®—å¤ç”¨æå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚éš¾åº¦æ„ŸçŸ¥é—¨æ§æœºåˆ¶ä¹Ÿæ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®tokençš„éš¾åº¦åŠ¨æ€åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å‚æ•°åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šå®½åº¦å¤šåŠŸèƒ½è·¯å¾„é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMixture of Experts, MoEï¼‰çš„æ€æƒ³ï¼Œä½†é¿å…äº†MoEä¸­å‚æ•°æ•°é‡çš„å¢åŠ ã€‚æ·±åº¦å¤šåŠŸèƒ½è·¯å¾„é€šè¿‡é€’å½’åº”ç”¨FFNå®ç°æ·±åº¦æ‰©å±•ï¼Œé€’å½’æ¬¡æ•°å¯ä»¥æ ¹æ®è®¡ç®—èµ„æºè¿›è¡Œè°ƒæ•´ã€‚éš¾åº¦æ„ŸçŸ¥é—¨æ§æ¨¡å—çš„è®¾è®¡éœ€è¦ä»”ç»†è€ƒè™‘å¦‚ä½•å‡†ç¡®è¯„ä¼°tokençš„éš¾åº¦ï¼Œå¹¶æ ¹æ®éš¾åº¦åŠ¨æ€è°ƒæ•´ä¸¤ä¸ªè·¯å¾„çš„æƒé‡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹è§„æ¨¡ä¸ŠéªŒè¯äº†VersatileFFNçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„å‚æ•°é¢„ç®—ä¸‹ï¼ŒVersatileFFNèƒ½å¤Ÿæ˜¾è‘—æå‡LLMçš„æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦å–å†³äºå…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œä½†æ€»ä½“è¶‹åŠ¿æ˜¯VersatileFFNèƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VersatileFFNå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆåˆ©ç”¨å‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„è½»é‡çº§LLMéƒ¨ç½²ã€èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥é™ä½LLMçš„éƒ¨ç½²æˆæœ¬ï¼Œå¹¶æå‡å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼ŒåŠ é€ŸLLMçš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.

