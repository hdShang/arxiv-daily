---
layout: default
title: RecGPT-V2 Technical Report
---

# RecGPT-V2 Technical Report

**arXiv**: [2512.14503v1](https://arxiv.org/abs/2512.14503) | [PDF](https://arxiv.org/pdf/2512.14503.pdf)

**ä½œè€…**: Chao Yi, Dian Chen, Gaoyang Guo, Jiakai Tang, Jian Wu, Jing Yu, Mao Zhang, Wen Chen, Wenjun Yang, Yujie Luo, Yuning Jiang, Zhujin Gao, Bo Zheng, Binbin Cao, Changfa Wu, Dixuan Wang, Han Wu, Haoyi Hu, Kewei Zhu, Lang Tian, Lin Yang, Qiqi Huang, Siqi Yang, Wenbo Su, Xiaoxiao He, Xin Tong, Xu Chen, Xunke Xi, Xiaowei Huang, Yaxuan Wu, Yeqiu Yang, Yi Hu, Yujin Yuan, Yuliang Yan, Zile Zhou

**åˆ†ç±»**: cs.IR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RecGPT-V2ï¼šé€šè¿‡å±‚çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œå…ƒæç¤ºç­‰æŠ€æœ¯ï¼Œæå‡LLMåœ¨æŽ¨èç³»ç»Ÿä¸­çš„æ„å›¾æŽ¨ç†èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)** **3Dæ„ŸçŸ¥ä¸ŽçŠ¶æ€ä¼°è®¡ (Perception & State Est)**

**å…³é”®è¯**: `å¤§åž‹è¯­è¨€æ¨¡åž‹` `æŽ¨èç³»ç»Ÿ` `æ„å›¾æŽ¨ç†` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å…ƒæç¤º` `å¼ºåŒ–å­¦ä¹ ` `A/Bæµ‹è¯•`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰RecGPT-V1å­˜åœ¨è®¡ç®—å†—ä½™ã€è§£é‡Šå¤šæ ·æ€§ä¸è¶³ã€æ³›åŒ–èƒ½åŠ›æœ‰é™ä»¥åŠè¯„ä¼°æ–¹å¼ç®€å•ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨æŽ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚
2. RecGPT-V2é€šè¿‡å±‚çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€å…ƒæç¤ºæ¡†æž¶å’Œçº¦æŸå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œæå‡æ„å›¾æŽ¨ç†æ•ˆçŽ‡ã€è§£é‡Šå¤šæ ·æ€§å’Œäººç±»åå¥½å¯¹é½ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒRecGPT-V2åœ¨æ·˜å®ä¸Šè¿›è¡Œäº†A/Bæµ‹è¯•ï¼ŒCTRã€IPVã€TVå’ŒNERç­‰æŒ‡æ ‡å‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå•†ä¸šä»·å€¼ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰åœ¨å°†æŽ¨èç³»ç»Ÿä»Žéšå¼è¡Œä¸ºæ¨¡å¼åŒ¹é…è½¬å˜ä¸ºæ˜¾å¼æ„å›¾æŽ¨ç†æ–¹é¢å±•çŽ°äº†å“è¶Šçš„æ½œåŠ›ã€‚RecGPT-V1çŽ‡å…ˆå°†åŸºäºŽLLMçš„æŽ¨ç†é›†æˆåˆ°ç”¨æˆ·å…´è¶£æŒ–æŽ˜å’Œé¡¹ç›®æ ‡ç­¾é¢„æµ‹ä¸­ï¼Œä½†å­˜åœ¨å››ä¸ªæ ¹æœ¬é™åˆ¶ï¼šï¼ˆ1ï¼‰è·¨å¤šä¸ªæŽ¨ç†è·¯å¾„çš„è®¡ç®—æ•ˆçŽ‡ä½Žä¸‹å’Œè®¤çŸ¥å†—ä½™ï¼›ï¼ˆ2ï¼‰å›ºå®šæ¨¡æ¿ç”Ÿæˆä¸­è§£é‡Šå¤šæ ·æ€§ä¸è¶³ï¼›ï¼ˆ3ï¼‰ç›‘ç£å­¦ä¹ èŒƒå¼ä¸‹çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼›ï¼ˆ4ï¼‰ä»¥ç»“æžœä¸ºä¸­å¿ƒçš„ç®€å•è¯„ä¼°æœªèƒ½ä¸Žäººç±»æ ‡å‡†ç›¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RecGPT-V2ï¼Œå®ƒå…·æœ‰å››ä¸ªå…³é”®åˆ›æ–°ã€‚é¦–å…ˆï¼Œå±‚çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡åè°ƒåä½œé‡æž„æ„å›¾æŽ¨ç†ï¼Œæ¶ˆé™¤è®¤çŸ¥é‡å¤ï¼ŒåŒæ—¶å®žçŽ°å¤šæ ·åŒ–çš„æ„å›¾è¦†ç›–ã€‚ç»“åˆåŽ‹ç¼©ç”¨æˆ·è¡Œä¸ºä¸Šä¸‹æ–‡çš„æ··åˆè¡¨ç¤ºæŽ¨ç†ï¼Œæˆ‘ä»¬çš„æ¡†æž¶å°†GPUæ¶ˆè€—é™ä½Žäº†60%ï¼Œå¹¶å°†ç‹¬å å¬å›žçŽ‡ä»Ž9.39%æé«˜åˆ°10.99%ã€‚å…¶æ¬¡ï¼Œå…ƒæç¤ºæ¡†æž¶åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡è‡ªé€‚åº”æç¤ºï¼Œå°†è§£é‡Šå¤šæ ·æ€§æé«˜äº†+7.3%ã€‚ç¬¬ä¸‰ï¼Œçº¦æŸå¼ºåŒ–å­¦ä¹ ç¼“è§£äº†å¤šé‡å¥–åŠ±å†²çªï¼Œåœ¨æ ‡ç­¾é¢„æµ‹æ–¹é¢å®žçŽ°äº†+24.1%çš„æ”¹è¿›ï¼Œåœ¨è§£é‡ŠæŽ¥å—åº¦æ–¹é¢å®žçŽ°äº†+13.0%çš„æ”¹è¿›ã€‚ç¬¬å››ï¼Œæ™ºèƒ½ä½“ä½œä¸ºè¯„åˆ¤æ¡†æž¶å°†è¯„ä¼°åˆ†è§£ä¸ºå¤šæ­¥æŽ¨ç†ï¼Œä»Žè€Œæé«˜äº†ä¸Žäººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚åœ¨æ·˜å®ä¸Šçš„åœ¨çº¿A/Bæµ‹è¯•è¡¨æ˜Žï¼Œæ•ˆæžœæ˜¾è‘—æå‡ï¼šCTR +2.98%ï¼ŒIPV +3.71%ï¼ŒTV +2.19%ï¼ŒNER +11.46%ã€‚RecGPT-V2ç¡®ç«‹äº†å¤§è§„æ¨¡éƒ¨ç½²LLMé©±åŠ¨çš„æ„å›¾æŽ¨ç†åœ¨æŠ€æœ¯ä¸Šå’Œå•†ä¸šä¸Šçš„å¯è¡Œæ€§ï¼Œå¼¥åˆäº†è®¤çŸ¥æŽ¢ç´¢å’Œå·¥ä¸šå®žç”¨æ€§ä¹‹é—´çš„å·®è·ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šRecGPT-V2æ—¨åœ¨è§£å†³çŽ°æœ‰åŸºäºŽLLMçš„æŽ¨èç³»ç»Ÿåœ¨æ•ˆçŽ‡ã€è§£é‡Šæ€§ã€æ³›åŒ–æ€§å’Œè¯„ä¼°æ ‡å‡†æ–¹é¢å­˜åœ¨çš„ä¸è¶³ã€‚å…·ä½“æ¥è¯´ï¼ŒçŽ°æœ‰æ–¹æ³•å­˜åœ¨è®¡ç®—å†—ä½™ï¼Œéš¾ä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„è§£é‡Šï¼Œåœ¨ç›‘ç£å­¦ä¹ èŒƒå¼ä¸‹æ³›åŒ–èƒ½åŠ›å—é™ï¼Œå¹¶ä¸”è¯„ä¼°æŒ‡æ ‡ä¸Žäººç±»çš„çœŸå®žåå¥½å­˜åœ¨åå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRecGPT-V2çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æž„å»ºä¸€ä¸ªå±‚çº§åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¹¶ç»“åˆå…ƒæç¤ºå’Œçº¦æŸå¼ºåŒ–å­¦ä¹ ï¼Œæ¥æå‡LLMåœ¨æŽ¨èç³»ç»Ÿä¸­çš„æ„å›¾æŽ¨ç†èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ¶ˆé™¤è®¤çŸ¥å†—ä½™ï¼Œæé«˜è§£é‡Šçš„å¤šæ ·æ€§ï¼Œå¢žå¼ºæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä½¿è¯„ä¼°æ ‡å‡†æ›´ç¬¦åˆäººç±»çš„åå¥½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šRecGPT-V2çš„æ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **å±‚çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ**ï¼šç”¨äºŽé‡æž„æ„å›¾æŽ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œæ¥æ¶ˆé™¤è®¤çŸ¥é‡å¤ã€‚2) **æ··åˆè¡¨ç¤ºæŽ¨ç†**ï¼šç”¨äºŽåŽ‹ç¼©ç”¨æˆ·è¡Œä¸ºä¸Šä¸‹æ–‡ï¼Œé™ä½Žè®¡ç®—æˆæœ¬ã€‚3) **å…ƒæç¤ºæ¡†æž¶**ï¼šç”¨äºŽåŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„æç¤ºï¼Œæé«˜è§£é‡Šçš„å¤šæ ·æ€§ã€‚4) **çº¦æŸå¼ºåŒ–å­¦ä¹ **ï¼šç”¨äºŽç¼“è§£å¤šé‡å¥–åŠ±å†²çªï¼Œä¼˜åŒ–æ ‡ç­¾é¢„æµ‹å’Œè§£é‡ŠæŽ¥å—åº¦ã€‚5) **æ™ºèƒ½ä½“ä½œä¸ºè¯„åˆ¤**ï¼šå°†è¯„ä¼°åˆ†è§£ä¸ºå¤šæ­¥æŽ¨ç†ï¼Œæé«˜ä¸Žäººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šRecGPT-V2çš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶å±‚çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œå…ƒæç¤ºæ¡†æž¶ã€‚å±‚çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡åè°ƒå¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„åä½œï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†è®¤çŸ¥å†—ä½™ï¼Œæé«˜äº†æŽ¨ç†æ•ˆçŽ‡ã€‚å…ƒæç¤ºæ¡†æž¶åˆ™èƒ½å¤ŸåŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„æç¤ºï¼Œä»Žè€Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§£é‡Šçš„å¤šæ ·æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å±‚çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œæ™ºèƒ½ä½“çš„å±‚çº§ç»“æž„å’Œåä½œæ–¹å¼æ˜¯å…³é”®è®¾è®¡ã€‚å…ƒæç¤ºæ¡†æž¶çš„å…³é”®åœ¨äºŽå¦‚ä½•æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯åŠ¨æ€ç”Ÿæˆæœ‰æ•ˆçš„æç¤ºã€‚çº¦æŸå¼ºåŒ–å­¦ä¹ çš„å…³é”®åœ¨äºŽå¦‚ä½•å®šä¹‰åˆé€‚çš„å¥–åŠ±å‡½æ•°å’Œçº¦æŸæ¡ä»¶ï¼Œä»¥å¹³è¡¡å¤šä¸ªç›®æ ‡ä¹‹é—´çš„å†²çªã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæä¾›è¯¦ç»†ä¿¡æ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

RecGPT-V2åœ¨æ·˜å®ä¸Šçš„åœ¨çº¿A/Bæµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šCTR +2.98%ï¼ŒIPV +3.71%ï¼ŒTV +2.19%ï¼ŒNER +11.46%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡åž‹è¿˜å°†GPUæ¶ˆè€—é™ä½Žäº†60%ï¼Œå¹¶å°†ç‹¬å å¬å›žçŽ‡ä»Ž9.39%æé«˜åˆ°10.99%ã€‚æ ‡ç­¾é¢„æµ‹å’Œè§£é‡ŠæŽ¥å—åº¦åˆ†åˆ«æé«˜äº†+24.1%å’Œ+13.0%ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

RecGPT-V2å¯åº”ç”¨äºŽå„ç§æŽ¨èç³»ç»Ÿåœºæ™¯ï¼Œä¾‹å¦‚ç”µå•†ã€å†…å®¹æŽ¨èå’Œç¤¾äº¤åª’ä½“ç­‰ã€‚å®ƒå¯ä»¥æå‡æŽ¨èç³»ç»Ÿçš„ä¸ªæ€§åŒ–ç¨‹åº¦ã€è§£é‡Šæ€§å’Œç”¨æˆ·æ»¡æ„åº¦ï¼Œä»Žè€Œæé«˜ç”¨æˆ·å‚ä¸Žåº¦å’Œå¹³å°æ”¶ç›Šã€‚è¯¥ç ”ç©¶ä¸ºLLMåœ¨æŽ¨èç³»ç»Ÿä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.
>   To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.

