---
layout: default
title: Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets
---

# Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14237</a>
  <a href="https://arxiv.org/pdf/2512.14237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14237" onclick="toggleFavorite(this, '2512.14237', 'Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Estelle Zheng, Nathan Cerisara, SÃ©bastien Warichet, Emmanuel Helbert, Christophe Cerisara

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLadder Side Tuningï¼Œä»¥ä½æˆæœ¬å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å†…å­˜ä¼˜åŒ–` `ä¾§ç½‘ç»œ` `Ladder Side Tuning`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰PEFTæ–¹æ³•å¦‚QLoRAè™½ç„¶å‡å°‘äº†è®­ç»ƒå‚æ•°ï¼Œä½†å®Œæ•´æ¨¡å‹åå‘ä¼ æ’­å¯¼è‡´å†…å­˜å ç”¨ä»ç„¶å¾ˆé«˜ã€‚
2. Ladder Side Tuning (LST) é€šè¿‡å¢åŠ è½»é‡çº§ä¾§ç½‘ç»œï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚
3. LSTåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šä¸QLoRAæ€§èƒ½ç›¸å½“ï¼Œå†…å­˜å ç”¨å‡åŠï¼Œå¹¶å¯æ‰©å±•åˆ°æ›´æ·±å±‚æ¬¡çš„æ¨ç†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸å—é™äºæ¶ˆè´¹çº§GPUçš„å†…å­˜ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚QLoRAè™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä½†ç”±äºå®Œæ•´æ¨¡å‹ä¸­çš„åå‘ä¼ æ’­ï¼Œä»ç„¶ä¼šäº§ç”Ÿè¾ƒé«˜çš„å†…å­˜ä½¿ç”¨é‡ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†Ladder Side Tuningï¼ˆLSTï¼‰ï¼Œä¸€ç§å¾ˆå°‘è¢«æ¢ç´¢çš„PEFTæŠ€æœ¯ï¼Œå®ƒå¢åŠ äº†ä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œï¼Œå¹¶è¡¨æ˜å®ƒåœ¨è®¡ç®—æ‰©å±•æ–œç‡ä¸Šä¸QLoRAç›¸åŒ¹é…ï¼ŒåŒæ—¶å°†å³°å€¼å†…å­˜å‡å°‘äº†50%ã€‚åœ¨æ¶µç›–è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦å’ŒLLM-criticä»»åŠ¡çš„ä¸åŒä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLSTçš„æ€§èƒ½ä¸QLoRAçš„å‡†ç¡®æ€§ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶å†…å­˜æ•ˆç‡æ›´é«˜ã€‚è¿™ç§æ•ˆç‡ä½¿å¾—å¯ä»¥åœ¨å•ä¸ª12GBæ¶ˆè´¹çº§GPUä¸Šä½¿ç”¨2k-tokenä¸Šä¸‹æ–‡å¾®è°ƒ7Bå‚æ•°æ¨¡å‹ï¼Œè€Œæ— éœ€æ¢¯åº¦æ£€æŸ¥ç‚¹â€”â€”åœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼ŒQLoRAä¼šè€—å°½å†…å­˜ã€‚é™¤äº†å†…å­˜æ•ˆç‡ä¹‹å¤–ï¼Œæœ¬æ–‡è¿˜å»ºç«‹äº†ç¼©æ”¾å®šå¾‹ï¼Œè¡¨æ˜LSTçš„ç¼©æ”¾æ–¹å¼ä¸QLoRAç±»ä¼¼ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥xLadderæ¥åˆ©ç”¨Ladderçš„æ¶æ„çµæ´»æ€§ï¼ŒxLadderæ˜¯ä¸€ç§æ·±åº¦æ‰©å±•çš„å˜ä½“ï¼Œé€šè¿‡äº¤å‰è¿æ¥å¢åŠ æœ‰æ•ˆæ·±åº¦ï¼Œå¹¶åœ¨å›ºå®šå‚æ•°æ•°é‡ä¸‹ç¼©çŸ­æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚å½“å†…å­˜æ˜¯ç“¶é¢ˆæ—¶ï¼ŒLadderè¡¨ç°å¼ºåŠ²ï¼›xLadderåœ¨æ­¤åŸºç¡€ä¸Šé€šè¿‡æ— éœ€é¢å¤–å†…å­˜å¼€é”€å³å¯å®ç°æ›´æ·±å±‚æ¬¡çš„æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚QLoRAï¼‰åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œè™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä½†ç”±äºéœ€è¦è¿›è¡Œå®Œæ•´æ¨¡å‹çš„åå‘ä¼ æ’­ï¼Œä»ç„¶ä¼šæ¶ˆè€—å¤§é‡çš„GPUå†…å­˜ï¼Œé™åˆ¶äº†åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚å°¤å…¶æ˜¯åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹ï¼Œå†…å­˜éœ€æ±‚æ›´åŠ ä¸¥å³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Ladder Side Tuning (LST) è¿™ç§ç›¸å¯¹è¾ƒå°‘è¢«ç ”ç©¶çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„ä¾§ç½‘ç»œï¼Œåœ¨ä¸»æ¨¡å‹ä¹‹å¤–è¿›è¡Œå‚æ•°æ›´æ–°ã€‚è¿™æ ·å¯ä»¥åœ¨ä¸ä¿®æ”¹æˆ–å¾®è°ƒä¸»æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå®ç°å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚é…ï¼Œä»è€Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLST çš„æ•´ä½“æ¶æ„æ˜¯åœ¨é¢„è®­ç»ƒçš„ Transformer æ¨¡å‹æ—è¾¹æ·»åŠ ä¸€ä¸ªå¹¶è¡Œçš„ã€è½»é‡çº§çš„ä¾§ç½‘ç»œï¼ˆLadder Networkï¼‰ã€‚è¾“å…¥æ•°æ®åŒæ—¶è¾“å…¥åˆ°ä¸»æ¨¡å‹å’Œä¾§ç½‘ç»œã€‚ä¸»æ¨¡å‹çš„è¾“å‡ºå’Œä¾§ç½‘ç»œçš„è¾“å‡ºè¿›è¡Œèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªæ›´æ–°ä¾§ç½‘ç»œçš„å‚æ•°ï¼Œè€Œä¸»æ¨¡å‹çš„å‚æ•°ä¿æŒå›ºå®šã€‚xLadder æ˜¯ LST çš„ä¸€ä¸ªå˜ä½“ï¼Œé€šè¿‡å¢åŠ ä¾§ç½‘ç»œçš„æ·±åº¦å’Œå¼•å…¥è·¨å±‚è¿æ¥ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šLST çš„å…³é”®åˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„å†…å­˜åˆ©ç”¨ç‡ã€‚é€šè¿‡åªè®­ç»ƒä¾§ç½‘ç»œï¼Œé¿å…äº†å¯¹æ•´ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†å†…å­˜éœ€æ±‚ã€‚xLadder çš„åˆ›æ–°åœ¨äºé€šè¿‡æ‰©å±•ä¾§ç½‘ç»œçš„æ·±åº¦å’Œå¼•å…¥è·¨å±‚è¿æ¥ï¼Œåœ¨ä¸å¢åŠ è¿‡å¤šå‚æ•°çš„æƒ…å†µä¸‹ï¼Œæå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šLST çš„å…³é”®è®¾è®¡åŒ…æ‹¬ä¾§ç½‘ç»œçš„ç»“æ„é€‰æ‹©ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå°çš„ Transformer æ¨¡å‹æˆ– MLPï¼‰ï¼Œä»¥åŠä¸»æ¨¡å‹å’Œä¾§ç½‘ç»œè¾“å‡ºçš„èåˆæ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨åŠ æƒå¹³å‡æˆ–æ‹¼æ¥ï¼‰ã€‚xLadder çš„å…³é”®è®¾è®¡åœ¨äºè·¨å±‚è¿æ¥çš„å¼•å…¥ï¼Œè¿™å…è®¸ä¿¡æ¯åœ¨ä¾§ç½‘ç»œçš„ä¸åŒå±‚ä¹‹é—´æµåŠ¨ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°é€šå¸¸é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLST åœ¨å†…å­˜æ•ˆç‡æ–¹é¢ä¼˜äº QLoRAï¼Œåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†ä¸ QLoRA ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚LST èƒ½å¤Ÿåœ¨ä¸€ä¸ª 12GB çš„æ¶ˆè´¹çº§ GPU ä¸Šå¾®è°ƒ 7B å‚æ•°çš„æ¨¡å‹ï¼Œè€Œ QLoRA åœ¨ç›¸åŒæ¡ä»¶ä¸‹ä¼šè€—å°½å†…å­˜ã€‚æ­¤å¤–ï¼ŒxLadder é€šè¿‡å¢åŠ ä¾§ç½‘ç»œçš„æ·±åº¦ï¼Œåœ¨ä¸å¢åŠ è¿‡å¤šå‚æ•°çš„æƒ…å†µä¸‹ï¼Œæå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºèµ„æºå—é™çš„åœºæ™¯ä¸‹çš„å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œä¾‹å¦‚åœ¨æ¶ˆè´¹çº§GPUæˆ–è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œæ¨¡å‹é€‚é…ã€‚è¿™ä½¿å¾—æ›´å¤šç”¨æˆ·èƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€æ˜‚è´µçš„ç¡¬ä»¶è®¾å¤‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºéœ€è¦å¿«é€Ÿè¿­ä»£å’Œéƒ¨ç½²çš„åœºæ™¯ï¼Œå› ä¸ºå…¶è®­ç»ƒæ•ˆç‡æ›´é«˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

