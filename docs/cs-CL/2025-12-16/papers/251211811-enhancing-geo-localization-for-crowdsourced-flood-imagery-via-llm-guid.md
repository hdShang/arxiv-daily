---
layout: default
title: Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention
---

# Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.11811" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.11811</a>
  <a href="https://arxiv.org/pdf/2512.11811.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11811" onclick="toggleFavorite(this, '2512.11811', 'Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fengyi Xu, Jun Ma, Waishan Qiu, Cui Guo, Jack C.P. Cheng

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVPR-AttLLMï¼Œåˆ©ç”¨LLMå¢å¼ºè§†è§‰å®šä½ï¼Œæå‡ä¼—åŒ…æ´ªæ°´å›¾åƒåœ°ç†å®šä½ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å®šä½` `åœ°ç†å®šä½` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `å¤šæ¨¡æ€èåˆ` `ä¼—åŒ…å›¾åƒ` `åŸå¸‚æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰å®šä½æ¨¡å‹åœ¨å¤„ç†ä¼—åŒ…å›¾åƒæ—¶ï¼Œå› è§†è§‰æ‰­æ›²å’Œè·¨åŸŸå·®å¼‚å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
2. VPR-AttLLMåˆ©ç”¨LLMçš„è¯­ä¹‰çŸ¥è¯†ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºVPRæ¨¡å‹çš„æè¿°ç¬¦ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVPR-AttLLMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæå‡äº†å¬å›ç‡ï¼Œå°¤å…¶åœ¨çœŸå®æ´ªæ°´å›¾åƒä¸Šæå‡é«˜è¾¾8%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºVPR-AttLLMï¼Œä¸€ä¸ªæ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›å¼•å¯¼çš„æè¿°ç¬¦å¢å¼ºï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„è¯­ä¹‰æ¨ç†å’Œåœ°ç†çŸ¥è¯†é›†æˆåˆ°ç°æœ‰çš„è§†è§‰å®šä½(VPR)æµç¨‹ä¸­ã€‚é€šè¿‡åˆ©ç”¨LLMè¯†åˆ«åŸå¸‚ç¯å¢ƒä¸­å…·æœ‰ä½ç½®ä¿¡æ¯çš„åŒºåŸŸå¹¶æŠ‘åˆ¶è§†è§‰å™ªå£°ï¼ŒVPR-AttLLMåœ¨ä¸éœ€è¦æ¨¡å‹é‡æ–°è®­ç»ƒæˆ–é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹æé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚åœ¨æ‰©å±•çš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ç”¨çœŸå®ç¤¾äº¤åª’ä½“æ´ªæ°´å›¾åƒä¸°å¯Œçš„SF-XLï¼Œå»ºç«‹çš„æŸ¥è¯¢é›†ä¸Šçš„åˆæˆæ´ªæ°´åœºæ™¯å’ŒMapillaryç…§ç‰‡ï¼Œä»¥åŠä¸€ä¸ªæ–°çš„æ•æ‰å½¢æ€å„å¼‚çš„åŸå¸‚æ™¯è§‚çš„HK-URBANæ•°æ®é›†ã€‚å°†VPR-AttLLMä¸ä¸‰ä¸ªæœ€å…ˆè¿›çš„VPRæ¨¡å‹ï¼ˆCosPlaceã€EigenPlaceså’ŒSALADï¼‰é›†æˆï¼Œå§‹ç»ˆå¦‚ä¸€åœ°æé«˜äº†å¬å›æ€§èƒ½ï¼Œç›¸å¯¹å¢ç›Šé€šå¸¸åœ¨1-3%ä¹‹é—´ï¼Œåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„çœŸå®æ´ªæ°´å›¾åƒä¸Šè¾¾åˆ°8%ã€‚é™¤äº†å¯è¡¡é‡çš„æ£€ç´¢å‡†ç¡®æ€§æå‡ä¹‹å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å»ºç«‹äº†ä¸€ä¸ªé€šç”¨çš„èŒƒä¾‹ï¼Œç”¨äºè§†è§‰æ£€ç´¢ç³»ç»Ÿä¸­LLMå¼•å¯¼çš„å¤šæ¨¡æ€èåˆã€‚é€šè¿‡å°†åŸå¸‚æ„ŸçŸ¥ç†è®ºçš„åŸåˆ™åµŒå…¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒVPR-AttLLMå°†ç±»äººç©ºé—´æ¨ç†ä¸ç°ä»£VPRæ¶æ„è”ç³»èµ·æ¥ã€‚å…¶å³æ’å³ç”¨è®¾è®¡ã€å¼ºå¤§çš„è·¨æºé²æ£’æ€§å’Œå¯è§£é‡Šæ€§çªå‡ºäº†å…¶åœ¨å¯æ‰©å±•çš„åŸå¸‚ç›‘æµ‹å’Œä¼—åŒ…å±æœºå›¾åƒå¿«é€Ÿåœ°ç†å®šä½æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä¼—åŒ…è¡—æ™¯å›¾åƒåœ°ç†å®šä½ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨åŸå¸‚æ´ªæ°´ç­‰å±æœºäº‹ä»¶ä¸­ã€‚ç°æœ‰è§†è§‰å®šä½ï¼ˆVPRï¼‰æ¨¡å‹åœ¨å¤„ç†æ­¤ç±»å›¾åƒæ—¶ï¼Œç”±äºå›¾åƒè´¨é‡å·®ã€è§†è§’å˜åŒ–å¤§ä»¥åŠä¸è®­ç»ƒæ•°æ®å­˜åœ¨é¢†åŸŸå·®å¼‚ï¼Œå¯¼è‡´å®šä½ç²¾åº¦æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’Œåœ°ç†çŸ¥è¯†ï¼Œå¼•å¯¼VPRæ¨¡å‹å…³æ³¨å›¾åƒä¸­ä¸ä½ç½®ä¿¡æ¯ç›¸å…³çš„åŒºåŸŸï¼Œå¹¶æŠ‘åˆ¶å™ªå£°å¹²æ‰°ã€‚é€šè¿‡LLMçš„è¾…åŠ©ï¼ŒVPRæ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°æå–å›¾åƒçš„åœ°ç†ç‰¹å¾ï¼Œä»è€Œæé«˜å®šä½ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVPR-AttLLMæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å›¾åƒè¾“å…¥ï¼šè¾“å…¥å¾…å®šä½çš„ä¼—åŒ…å›¾åƒã€‚2) VPRæ¨¡å‹ï¼šä½¿ç”¨ç°æœ‰çš„VPRæ¨¡å‹ï¼ˆå¦‚CosPlaceã€EigenPlacesã€SALADï¼‰æå–å›¾åƒçš„å…¨å±€ç‰¹å¾ã€‚3) LLMï¼šåˆ©ç”¨LLMåˆ†æå›¾åƒå†…å®¹ï¼Œè¯†åˆ«å›¾åƒä¸­å…·æœ‰ä½ç½®ä¿¡æ¯çš„åŒºåŸŸï¼Œå¹¶ç”Ÿæˆæ³¨æ„åŠ›æƒé‡ã€‚4) æ³¨æ„åŠ›æœºåˆ¶ï¼šå°†LLMç”Ÿæˆçš„æ³¨æ„åŠ›æƒé‡åº”ç”¨äºVPRæ¨¡å‹çš„ç‰¹å¾å›¾ï¼Œå¢å¼ºå…³é”®åŒºåŸŸçš„ç‰¹å¾ï¼ŒæŠ‘åˆ¶å™ªå£°åŒºåŸŸçš„ç‰¹å¾ã€‚5) åœ°ç†ä½ç½®æ£€ç´¢ï¼šä½¿ç”¨å¢å¼ºåçš„ç‰¹å¾è¿›è¡Œåœ°ç†ä½ç½®æ£€ç´¢ï¼Œå¾—åˆ°æœ€ç»ˆçš„å®šä½ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†LLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ä¸VPRæ¨¡å‹çš„è§†è§‰ç‰¹å¾æå–èƒ½åŠ›ç›¸ç»“åˆï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å®ç°å¤šæ¨¡æ€èåˆã€‚ä¸ä¼ ç»Ÿçš„VPRæ–¹æ³•ç›¸æ¯”ï¼ŒVPR-AttLLMèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å›¾åƒä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜å¯¹è·¨åŸŸå’Œå™ªå£°å›¾åƒçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLLMéƒ¨åˆ†çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°æå–å›¾åƒä¸­çš„ä½ç½®ä¿¡æ¯ã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºæç¤ºå­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡åˆé€‚çš„æç¤ºè¯­ï¼Œå¼•å¯¼LLMè¯†åˆ«å›¾åƒä¸­çš„åœ°æ ‡ã€å»ºç­‘ç‰©ç­‰å…·æœ‰ä½ç½®ä¿¡æ¯çš„å…ƒç´ ã€‚æ³¨æ„åŠ›æœºåˆ¶çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦ä¿è¯LLMç”Ÿæˆçš„æ³¨æ„åŠ›æƒé‡èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ å›¾åƒä¸­å„ä¸ªåŒºåŸŸçš„é‡è¦æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.11811/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.11811/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.11811/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

VPR-AttLLMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬SF-XLã€åˆæˆæ´ªæ°´æ•°æ®é›†å’ŒHK-URBANã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVPR-AttLLMä¸CosPlaceã€EigenPlaceså’ŒSALADç­‰å…ˆè¿›VPRæ¨¡å‹é›†æˆåï¼Œå¬å›ç‡å‡å¾—åˆ°æ˜¾è‘—æå‡ï¼Œåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„çœŸå®æ´ªæ°´å›¾åƒä¸Šï¼Œå¬å›ç‡æå‡é«˜è¾¾8%ã€‚è¿™è¡¨æ˜VPR-AttLLMå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåŸå¸‚åº”æ€¥å“åº”ã€ç¾å®³ç›‘æµ‹ã€åŸå¸‚è§„åˆ’ç­‰é¢†åŸŸã€‚é€šè¿‡å¿«é€Ÿå‡†ç¡®åœ°å®šä½ä¼—åŒ…å›¾åƒï¼Œå¯ä»¥å¸®åŠ©æ•‘æ´äººå‘˜å¿«é€Ÿäº†è§£ç¾æƒ…ï¼Œåˆ¶å®šåˆç†çš„æ•‘æ´æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºåŸå¸‚ç¯å¢ƒç›‘æµ‹ã€äº¤é€šæµé‡åˆ†æç­‰æ–¹é¢ï¼Œä¸ºæ™ºæ…§åŸå¸‚å»ºè®¾æä¾›æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Crowdsourced street-view imagery from social media provides real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing image geo-localization approaches, also known as Visual Place Recognition (VPR) models, exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geo-knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

