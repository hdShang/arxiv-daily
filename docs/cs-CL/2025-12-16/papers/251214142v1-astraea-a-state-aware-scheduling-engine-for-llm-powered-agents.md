---
layout: default
title: Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents
---

# Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14142" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14142v1</a>
  <a href="https://arxiv.org/pdf/2512.14142.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14142v1" onclick="toggleFavorite(this, '2512.14142v1', 'Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongqiu Ni, Jiabao Zhang, Guopeng Li, Zilong Wang, Ruiqi Wu, Chi Zhang, Haisheng Tan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 12 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Astraeaï¼šé¢å‘LLMæ™ºèƒ½ä½“çš„çŠ¶æ€æ„ŸçŸ¥è°ƒåº¦å¼•æ“ï¼Œä¼˜åŒ–ç«¯åˆ°ç«¯å»¶è¿Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `LLMæ™ºèƒ½ä½“` `è°ƒåº¦å¼•æ“` `çŠ¶æ€æ„ŸçŸ¥` `ä½œä¸šå®Œæˆæ—¶é—´` `KVç¼“å­˜ç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ™ºèƒ½ä½“æ¨ç†ç³»ç»Ÿä¾§é‡äºå±€éƒ¨ä¼˜åŒ–ï¼Œå¿½ç•¥äº†å…¨å±€ä½œä¸šå®Œæˆæ—¶é—´ï¼ˆJCTï¼‰ï¼Œå¯¼è‡´ç«¯åˆ°ç«¯å»¶è¿Ÿè¾ƒé«˜ã€‚
2. Astraeaé€šè¿‡çŠ¶æ€æ„ŸçŸ¥çš„åˆ†å±‚è°ƒåº¦ç®—æ³•ï¼Œç»“åˆè¯·æ±‚å†å²çŠ¶æ€å’Œæœªæ¥é¢„æµ‹ï¼Œä¼˜åŒ–å…¨å±€è¯·æ±‚ç”Ÿå‘½å‘¨æœŸã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAstraeaç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡JCTé™ä½é«˜è¾¾25.5%ï¼Œå¹¶åœ¨é«˜è´Ÿè½½ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºæ™ºèƒ½ä½“ã€‚å®ƒä»¬çš„å¤šé˜¶æ®µå·¥ä½œæµç¨‹åœ¨æœ¬åœ°è®¡ç®—å’Œå¯¹Web APIç­‰å¤–éƒ¨ç½‘ç»œæœåŠ¡çš„è°ƒç”¨ä¹‹é—´äº¤æ›¿ï¼Œè¿™å¯¼è‡´å®ƒä»¬çš„æ‰§è¡Œæ¨¡å¼ä¸ç°æœ‰æ¨ç†ç³»ç»Ÿï¼ˆå¦‚vLLMï¼‰çš„è°ƒåº¦ç²’åº¦ä¸åŒ¹é…ã€‚ç°æœ‰ç³»ç»Ÿé€šå¸¸ä¾§é‡äºæ¯ä¸ªç‰‡æ®µçš„ä¼˜åŒ–ï¼Œè¿™å¦¨ç¢äº†å®ƒä»¬æœ€å°åŒ–å®Œæ•´æ™ºèƒ½ä½“å·¥ä½œæµç¨‹çš„ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œå³æ•´ä¸ªè¯·æ±‚ç”Ÿå‘½å‘¨æœŸå†…çš„å…¨å±€ä½œä¸šå®Œæˆæ—¶é—´ï¼ˆJCTï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Astraeaï¼Œä¸€ä¸ªæ—¨åœ¨å°†ä¼˜åŒ–ä»æœ¬åœ°ç‰‡æ®µè½¬ç§»åˆ°å…¨å±€è¯·æ±‚ç”Ÿå‘½å‘¨æœŸçš„æœåŠ¡å¼•æ“ã€‚Astraeaé‡‡ç”¨äº†ä¸€ç§çŠ¶æ€æ„ŸçŸ¥çš„åˆ†å±‚è°ƒåº¦ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†è¯·æ±‚çš„å†å²çŠ¶æ€ä¸æœªæ¥é¢„æµ‹ç›¸ç»“åˆã€‚å®ƒæ ¹æ®è¯·æ±‚çš„I/Oå’Œè®¡ç®—å¯†é›†ç¨‹åº¦åŠ¨æ€åœ°å¯¹è¯·æ±‚è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä½¿ç”¨å¢å¼ºçš„HRRNç­–ç•¥æ¥å¹³è¡¡æ•ˆç‡å’Œå…¬å¹³æ€§ã€‚Astraeaè¿˜å®ç°äº†ä¸€ä¸ªè‡ªé€‚åº”KVç¼“å­˜ç®¡ç†å™¨ï¼Œè¯¥ç®¡ç†å™¨æ ¹æ®ç³»ç»Ÿå†…å­˜å‹åŠ›æ™ºèƒ½åœ°å¤„ç†I/Oç­‰å¾…æœŸé—´çš„æ™ºèƒ½ä½“çŠ¶æ€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒAstraeaå°†å¹³å‡JCTé™ä½äº†é«˜è¾¾25.5%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æ¨¡å‹è§„æ¨¡çš„é«˜è´Ÿè½½ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰LLMæ™ºèƒ½ä½“æ¨ç†ç³»ç»Ÿï¼Œå¦‚vLLMï¼Œä¸»è¦é’ˆå¯¹å•ä¸ªæ¨ç†ç‰‡æ®µè¿›è¡Œä¼˜åŒ–ï¼Œç¼ºä¹å¯¹æ•´ä¸ªæ™ºèƒ½ä½“å·¥ä½œæµç¨‹ï¼ˆåŒ…æ‹¬æœ¬åœ°è®¡ç®—å’Œå¤–éƒ¨APIè°ƒç”¨ï¼‰çš„å…¨å±€è§†è§’ã€‚è¿™å¯¼è‡´æ— æ³•æœ‰æ•ˆæœ€å°åŒ–ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œå³å…¨å±€ä½œä¸šå®Œæˆæ—¶é—´ï¼ˆJCTï¼‰ï¼Œæˆä¸ºåˆ¶çº¦LLMæ™ºèƒ½ä½“æ€§èƒ½çš„å…³é”®ç“¶é¢ˆã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘è¯·æ±‚çš„å†å²çŠ¶æ€å’Œæœªæ¥è¡Œä¸ºï¼Œæ— æ³•æ ¹æ®è¯·æ±‚çš„ç‰¹æ€§è¿›è¡ŒåŠ¨æ€è°ƒåº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAstraeaçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¼˜åŒ–ç›®æ ‡ä»å±€éƒ¨ç‰‡æ®µè½¬ç§»åˆ°å…¨å±€è¯·æ±‚ç”Ÿå‘½å‘¨æœŸã€‚é€šè¿‡çŠ¶æ€æ„ŸçŸ¥çš„è°ƒåº¦ç®—æ³•ï¼ŒAstraeaèƒ½å¤Ÿæ ¹æ®è¯·æ±‚çš„å†å²çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼ŒI/Oå¯†é›†å‹æˆ–è®¡ç®—å¯†é›†å‹ï¼‰å’Œæœªæ¥é¢„æµ‹ï¼ŒåŠ¨æ€åœ°è°ƒæ•´è°ƒåº¦ç­–ç•¥ï¼Œä»è€Œæœ€å°åŒ–å…¨å±€JCTã€‚è¿™ç§å…¨å±€ä¼˜åŒ–è§†è§’èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”LLMæ™ºèƒ½ä½“å¤šé˜¶æ®µã€å¼‚æ„çš„æ‰§è¡Œæ¨¡å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAstraeaé‡‡ç”¨åˆ†å±‚è°ƒåº¦æ¶æ„ã€‚é¦–å…ˆï¼Œæ ¹æ®è¯·æ±‚çš„I/Oå’Œè®¡ç®—ç‰¹æ€§è¿›è¡Œåˆ†ç±»ã€‚ç„¶åï¼Œä½¿ç”¨å¢å¼ºçš„HRRNï¼ˆHighest Response Ratio Nextï¼‰ç­–ç•¥è¿›è¡Œè°ƒåº¦ï¼Œå¹³è¡¡æ•ˆç‡å’Œå…¬å¹³æ€§ã€‚æ­¤å¤–ï¼ŒAstraeaè¿˜åŒ…å«ä¸€ä¸ªè‡ªé€‚åº”KVç¼“å­˜ç®¡ç†å™¨ï¼Œç”¨äºåœ¨I/Oç­‰å¾…æœŸé—´æ™ºèƒ½åœ°ç®¡ç†æ™ºèƒ½ä½“çŠ¶æ€ï¼Œæ ¹æ®ç³»ç»Ÿå†…å­˜å‹åŠ›åŠ¨æ€è°ƒæ•´ç¼“å­˜ç­–ç•¥ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬è¯·æ±‚åˆ†ç±»ã€è°ƒåº¦å†³ç­–å’ŒKVç¼“å­˜ç®¡ç†ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šAstraeaçš„å…³é”®åˆ›æ–°åœ¨äºå…¶çŠ¶æ€æ„ŸçŸ¥çš„è°ƒåº¦ç®—æ³•å’Œè‡ªé€‚åº”KVç¼“å­˜ç®¡ç†ã€‚çŠ¶æ€æ„ŸçŸ¥è°ƒåº¦èƒ½å¤Ÿæ ¹æ®è¯·æ±‚çš„å†å²è¡Œä¸ºå’Œæœªæ¥é¢„æµ‹è¿›è¡ŒåŠ¨æ€è°ƒåº¦ï¼Œè€Œè‡ªé€‚åº”KVç¼“å­˜ç®¡ç†èƒ½å¤Ÿæ ¹æ®ç³»ç»Ÿèµ„æºçŠ¶å†µæ™ºèƒ½åœ°ç®¡ç†æ™ºèƒ½ä½“çŠ¶æ€ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAstraeaèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”LLMæ™ºèƒ½ä½“å¤šé˜¶æ®µã€å¼‚æ„çš„æ‰§è¡Œæ¨¡å¼ï¼Œä»è€Œå®ç°æ›´ä½çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚

**å…³é”®è®¾è®¡**ï¼šAstraeaçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è¯·æ±‚åˆ†ç±»ç­–ç•¥ï¼Œç”¨äºåŒºåˆ†I/Oå¯†é›†å‹å’Œè®¡ç®—å¯†é›†å‹è¯·æ±‚ï¼›2) å¢å¼ºçš„HRRNè°ƒåº¦ç­–ç•¥ï¼Œç”¨äºå¹³è¡¡æ•ˆç‡å’Œå…¬å¹³æ€§ï¼›3) è‡ªé€‚åº”KVç¼“å­˜ç®¡ç†ç­–ç•¥ï¼Œç”¨äºæ ¹æ®ç³»ç»Ÿå†…å­˜å‹åŠ›åŠ¨æ€è°ƒæ•´ç¼“å­˜å¤§å°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œé˜ˆå€¼éœ€è¦æ ¹æ®å®é™…åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚æŸå¤±å‡½æ•°æœªæåŠï¼Œç½‘ç»œç»“æ„ä¹Ÿæœªæ¶‰åŠï¼Œæ¨æµ‹æ˜¯è°ƒåº¦ç®—æ³•å±‚é¢çš„ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAstraeaåœ¨å„ç§æ¨¡å‹è§„æ¨¡ä¸‹ï¼Œç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼ˆå…·ä½“åŸºçº¿æ–¹æ³•æœªæ˜ç¡®è¯´æ˜ï¼Œæ¨æµ‹æ˜¯vLLMç­‰ï¼‰ï¼Œå¹³å‡ä½œä¸šå®Œæˆæ—¶é—´ï¼ˆJCTï¼‰é™ä½äº†é«˜è¾¾25.5%ã€‚æ­¤å¤–ï¼ŒAstraeaåœ¨é«˜è´Ÿè½½ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œç¨³å®šæ€§ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜Astraeaèƒ½å¤Ÿæ˜¾è‘—æå‡LLMæ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Astraeaé€‚ç”¨äºå„ç§éœ€è¦ä½å»¶è¿Ÿã€é«˜ååé‡çš„LLMæ™ºèƒ½ä½“åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–æµç¨‹ã€æ™ºèƒ½å®¶å±…æ§åˆ¶ç­‰ã€‚é€šè¿‡ä¼˜åŒ–ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ŒAstraeaå¯ä»¥æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶é™ä½éƒ¨ç½²æˆæœ¬ã€‚æœªæ¥ï¼ŒAstraeaå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ”¯æŒæ›´å¤æ‚çš„æ™ºèƒ½ä½“å·¥ä½œæµç¨‹å’Œå¼‚æ„è®¡ç®—ç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.

