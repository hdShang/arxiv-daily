---
layout: default
title: CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models
---

# CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14118" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14118v1</a>
  <a href="https://arxiv.org/pdf/2512.14118.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14118v1" onclick="toggleFavorite(this, '2512.14118v1', 'CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiran Zhang, Jincheng Hu, Mark Dras, Usman Naseem

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: underreview

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CogMemï¼šä¸€ç§è®¤çŸ¥è®°å¿†æ¶æ„ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æŒç»­çš„å¤šè½®æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè½®æ¨ç†` `è®¤çŸ¥æ¶æ„` `é•¿æœŸè®°å¿†` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMåœ¨å¤šè½®å¯¹è¯ä¸­å­˜åœ¨æ¨ç†åå·®ã€ä»»åŠ¡æ¼‚ç§»å’Œè®°å¿†è¡°é€€ç­‰é—®é¢˜ï¼Œç›´æ¥æ‹¼æ¥å¯¹è¯å†å²å¯¼è‡´ä¸Šä¸‹æ–‡æ— é™å¢é•¿ã€‚
2. CogMemæ¶æ„æ¨¡ä»¿è®¤çŸ¥è¿‡ç¨‹ï¼Œå¼•å…¥é•¿æœŸè®°å¿†ã€ç›´æ¥è®¿é—®è®°å¿†å’Œæ³¨æ„åŠ›ç„¦ç‚¹æœºåˆ¶ï¼Œå®ç°ç»“æ„åŒ–å’ŒæŒä¹…çš„è®°å¿†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCogMemèƒ½æœ‰æ•ˆç¼“è§£æ¨ç†å¤±è´¥ï¼Œæ§åˆ¶ä¸Šä¸‹æ–‡å¢é•¿ï¼Œå¹¶æå‡å¤šè½®æ¨ç†çš„ä¸€è‡´æ€§ï¼Œæ›´æ¥è¿‘äººç±»æ¨ç†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿å•è½®æ¨ç†ï¼Œä½†åœ¨æ‰©å±•çš„å¤šè½®äº¤äº’ä¸­ç»å¸¸ä¼šå¤±å»å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚TurnBenchç­‰æœ€æ–°è¯„ä¼°çªå‡ºäº†é‡å¤å‡ºç°çš„å¤±è´¥æ¨¡å¼â€”â€”æ¨ç†åå·®ã€ä»»åŠ¡æ¼‚ç§»ã€å¹»è§‰ã€è¿‡åº¦è‡ªä¿¡å’Œè®°å¿†è¡°é€€ã€‚ç›®å‰çš„æ–¹æ³•é€šå¸¸é™„åŠ å®Œæ•´çš„å¯¹è¯å†å²ï¼Œå¯¼è‡´æ— é™åˆ¶çš„ä¸Šä¸‹æ–‡å¢é•¿ã€æ›´é«˜çš„è®¡ç®—æˆæœ¬å’Œé™ä½çš„æ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬ä»‹ç»CogMemï¼Œä¸€ç§è®¤çŸ¥å¯å‘ã€è®°å¿†å¢å¼ºçš„LLMæ¶æ„ï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–çš„æŒä¹…è®°å¿†æ¥æ”¯æŒæŒç»­çš„è¿­ä»£æ¨ç†ã€‚CogMemåŒ…å«ä¸‰ä¸ªå±‚ï¼šé•¿æœŸè®°å¿†ï¼ˆLTMï¼‰ï¼Œç”¨äºå·©å›ºè·¨ä¼šè¯çš„æ¨ç†ç­–ç•¥ï¼›ç›´æ¥è®¿é—®ï¼ˆDAï¼‰è®°å¿†ï¼Œç”¨äºç»´æŠ¤ä¼šè¯çº§åˆ«çš„ç¬”è®°å¹¶æ£€ç´¢ç›¸å…³çš„é•¿æœŸè®°å¿†ï¼›ä»¥åŠæ³¨æ„åŠ›ç„¦ç‚¹ï¼ˆFoAï¼‰æœºåˆ¶ï¼Œç”¨äºåœ¨æ¯ä¸€è½®åŠ¨æ€åœ°é‡å»ºç®€æ´çš„ã€ä¸ä»»åŠ¡ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚åœ¨TurnBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§åˆ†å±‚è®¾è®¡å‡è½»äº†æ¨ç†å¤±è´¥ï¼Œæ§åˆ¶äº†ä¸Šä¸‹æ–‡å¢é•¿ï¼Œå¹¶æé«˜äº†æ‰©å±•æ¨ç†é“¾ä¸­çš„ä¸€è‡´æ€§ï¼Œä»è€Œæœç€LLMä¸­æ›´å¯é ã€æ›´åƒäººç±»çš„æ¨ç†è¿ˆè¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­æ¨ç†èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç®€å•åœ°å°†æ‰€æœ‰å¯¹è¯å†å²æ‹¼æ¥èµ·æ¥ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡é•¿åº¦å¿«é€Ÿå¢é•¿ï¼Œè®¡ç®—æˆæœ¬å¢åŠ ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºç°æ¨ç†åå·®ã€ä»»åŠ¡æ¼‚ç§»ã€å¹»è§‰ç­‰é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¸¥é‡å½±å“äº†LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCogMemçš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´äººç±»è®¤çŸ¥æ¶æ„ï¼Œå°†è®°å¿†åˆ†ä¸ºé•¿æœŸè®°å¿†ï¼ˆLTMï¼‰å’Œç›´æ¥è®¿é—®è®°å¿†ï¼ˆDAï¼‰ï¼Œå¹¶å¼•å…¥æ³¨æ„åŠ›ç„¦ç‚¹ï¼ˆFoAï¼‰æœºåˆ¶ã€‚LTMç”¨äºå­˜å‚¨è·¨ä¼šè¯çš„é€šç”¨æ¨ç†ç­–ç•¥ï¼ŒDAç”¨äºå­˜å‚¨å½“å‰ä¼šè¯çš„ç¬”è®°å’Œæ£€ç´¢LTMä¸­çš„ç›¸å…³ä¿¡æ¯ï¼ŒFoAåˆ™åŠ¨æ€åœ°æ„å»ºç®€æ´çš„ä»»åŠ¡ç›¸å…³ä¸Šä¸‹æ–‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCogMemæ¶æ„åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) **é•¿æœŸè®°å¿†ï¼ˆLTMï¼‰**ï¼šå­˜å‚¨è·¨ä¼šè¯çš„æ¨ç†ç­–ç•¥ï¼Œä¾‹å¦‚å¸¸ç”¨çš„è§£é¢˜æ­¥éª¤ã€çŸ¥è¯†å›¾è°±ç­‰ã€‚2) **ç›´æ¥è®¿é—®è®°å¿†ï¼ˆDAï¼‰**ï¼šå­˜å‚¨å½“å‰ä¼šè¯çš„ç¬”è®°ï¼Œä¾‹å¦‚å…³é”®ä¿¡æ¯ã€ä¸­é—´ç»“æœç­‰ï¼Œå¹¶è´Ÿè´£æ£€ç´¢LTMä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚3) **æ³¨æ„åŠ›ç„¦ç‚¹ï¼ˆFoAï¼‰**ï¼šæ ¹æ®å½“å‰ä»»åŠ¡å’ŒDAä¸­çš„ä¿¡æ¯ï¼ŒåŠ¨æ€åœ°ä»LTMå’ŒDAä¸­é€‰æ‹©ç›¸å…³ä¿¡æ¯ï¼Œæ„å»ºç®€æ´çš„ä»»åŠ¡ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œè¾“å…¥åˆ°LLMä¸­è¿›è¡Œæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šCogMemçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è®¤çŸ¥å¯å‘çš„è®°å¿†æ¶æ„ï¼Œå®ƒå°†è®°å¿†åˆ†ä¸ºé•¿æœŸå’ŒçŸ­æœŸï¼Œå¹¶å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ¨æ€åœ°é€‰æ‹©ç›¸å…³ä¿¡æ¯ã€‚ä¸ç°æœ‰æ–¹æ³•ç›´æ¥æ‹¼æ¥å¯¹è¯å†å²ç›¸æ¯”ï¼ŒCogMemèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®°å¿†ï¼Œæ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶æé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLTMå¯ä»¥ä½¿ç”¨å‘é‡æ•°æ®åº“å®ç°ï¼ŒDAå¯ä»¥ä½¿ç”¨ç®€å•çš„é”®å€¼å¯¹å­˜å‚¨ã€‚FoAæœºåˆ¶å¯ä»¥ä½¿ç”¨æ³¨æ„åŠ›ç½‘ç»œå®ç°ï¼Œæ ¹æ®å½“å‰ä»»åŠ¡å’ŒDAä¸­çš„ä¿¡æ¯ï¼Œè®¡ç®—LTMå’ŒDAä¸­æ¯ä¸ªä¿¡æ¯çš„æƒé‡ï¼Œç„¶åé€‰æ‹©æƒé‡è¾ƒé«˜çš„ä¿¡æ¯ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†ç‰¹å®šçš„å‘é‡æ•°æ®åº“ã€æ³¨æ„åŠ›ç½‘ç»œç»“æ„æˆ–è®­ç»ƒç­–ç•¥ï¼Œä½†å…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CogMemåœ¨TurnBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæœ‰æ•ˆç¼“è§£äº†æ¨ç†å¤±è´¥ï¼Œæ§åˆ¶äº†ä¸Šä¸‹æ–‡å¢é•¿ï¼Œå¹¶æé«˜äº†æ‰©å±•æ¨ç†é“¾ä¸­çš„ä¸€è‡´æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºï¼Œè¡¨æ˜CogMemæ¶æ„åœ¨å¤šè½®æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ä¸ç›´æ¥æ‹¼æ¥å¯¹è¯å†å²çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒCogMemåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡æœ‰æå‡ï¼Œä½†å…·ä½“æ•°å€¼æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CogMemæ¶æ„å¯åº”ç”¨äºéœ€è¦æŒç»­å¤šè½®æ¨ç†çš„å„ç§åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€å¯¹è¯å¼é—®ç­”ã€ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡LLMåœ¨å¤šè½®å¯¹è¯ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„ä½“éªŒã€‚è¯¥ç ”ç©¶å¯¹äºæ„å»ºæ›´å¯é ã€æ›´åƒäººç±»çš„LLMå…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.

