---
layout: default
title: CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models
---

# CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models

**arXiv**: [2512.14118v1](https://arxiv.org/abs/2512.14118) | [PDF](https://arxiv.org/pdf/2512.14118.pdf)

**ä½œè€…**: Yiran Zhang, Jincheng Hu, Mark Dras, Usman Naseem

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: underreview

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CogMemï¼šä¸€ç§è®¤çŸ¥è®°å¿†æž¶æž„ï¼Œç”¨äºŽå¤§åž‹è¯­è¨€æ¨¡åž‹ä¸­æŒç»­çš„å¤šè½®æŽ¨ç†**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)**

**å…³é”®è¯**: `å¤šè½®å¯¹è¯` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `è®¤çŸ¥æž¶æž„` `è®°å¿†å¢žå¼º` `æŽ¨ç†èƒ½åŠ›` `ä¸Šä¸‹æ–‡ç®¡ç†` `æŒç»­å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰LLMåœ¨å¤šè½®å¯¹è¯ä¸­å­˜åœ¨æŽ¨ç†åå·®ã€ä»»åŠ¡æ¼‚ç§»ã€å¹»è§‰ç­‰é—®é¢˜ï¼Œä¸”ç®€å•å †å å¯¹è¯åŽ†å²å¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚
2. CogMemæž¶æž„å—è®¤çŸ¥å¯å‘ï¼Œé€šè¿‡é•¿æœŸè®°å¿†ã€ç›´æŽ¥è®¿é—®è®°å¿†å’Œæ³¨æ„åŠ›ç„¦ç‚¹æœºåˆ¶ï¼Œå®žçŽ°ç»“æž„åŒ–å’ŒæŒä¹…çš„è®°å¿†ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒCogMemèƒ½æœ‰æ•ˆç¼“è§£æŽ¨ç†å¤±è´¥ï¼ŒæŽ§åˆ¶ä¸Šä¸‹æ–‡å¢žé•¿ï¼Œå¹¶æå‡å¤šè½®æŽ¨ç†çš„ä¸€è‡´æ€§ï¼Œæ›´æŽ¥è¿‘äººç±»æŽ¨ç†ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æ“…é•¿å•è½®æŽ¨ç†ï¼Œä½†åœ¨æ‰©å±•çš„å¤šè½®äº¤äº’ä¸­å¸¸å¸¸ä¼šæŸå¤±å‡†ç¡®æ€§å’Œè¿žè´¯æ€§ã€‚TurnBenchç­‰æœ€æ–°è¯„ä¼°çªå‡ºäº†é‡å¤å‡ºçŽ°çš„å¤±è´¥æ¨¡å¼â€”â€”æŽ¨ç†åå·®ã€ä»»åŠ¡æ¼‚ç§»ã€å¹»è§‰ã€è¿‡åº¦è‡ªä¿¡å’Œè®°å¿†è¡°é€€ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸é™„åŠ å®Œæ•´çš„å¯¹è¯åŽ†å²ï¼Œå¯¼è‡´æ— é™åˆ¶çš„ä¸Šä¸‹æ–‡å¢žé•¿ã€æ›´é«˜çš„è®¡ç®—æˆæœ¬å’Œé™ä½Žçš„æŽ¨ç†æ•ˆçŽ‡ã€‚æˆ‘ä»¬ä»‹ç»CogMemï¼Œä¸€ç§å—è®¤çŸ¥å¯å‘ã€è®°å¿†å¢žå¼ºçš„LLMæž¶æž„ï¼Œå®ƒé€šè¿‡ç»“æž„åŒ–çš„æŒä¹…è®°å¿†æ¥æ”¯æŒæŒç»­çš„è¿­ä»£æŽ¨ç†ã€‚CogMemåŒ…å«ä¸‰ä¸ªå±‚ï¼šé•¿æœŸè®°å¿†ï¼ˆLTMï¼‰ï¼Œç”¨äºŽå·©å›ºè·¨ä¼šè¯çš„æŽ¨ç†ç­–ç•¥ï¼›ç›´æŽ¥è®¿é—®ï¼ˆDAï¼‰è®°å¿†ï¼Œç”¨äºŽç»´æŠ¤ä¼šè¯çº§åˆ«çš„ç¬”è®°å¹¶æ£€ç´¢ç›¸å…³çš„é•¿æœŸè®°å¿†ï¼›ä»¥åŠæ³¨æ„åŠ›ç„¦ç‚¹ï¼ˆFoAï¼‰æœºåˆ¶ï¼Œç”¨äºŽåœ¨æ¯ä¸€è½®åŠ¨æ€åœ°é‡å»ºç®€æ´çš„ã€ä¸Žä»»åŠ¡ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚åœ¨TurnBenchä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œè¿™ç§åˆ†å±‚è®¾è®¡å‡è½»äº†æŽ¨ç†å¤±è´¥ï¼ŒæŽ§åˆ¶äº†ä¸Šä¸‹æ–‡å¢žé•¿ï¼Œå¹¶æé«˜äº†æ‰©å±•æŽ¨ç†é“¾ä¸­çš„ä¸€è‡´æ€§ï¼Œä»Žè€Œä½¿LLMæœç€æ›´å¯é ã€æ›´åƒäººç±»çš„æŽ¨ç†æ–¹å‘å‘å±•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨å¤šè½®å¯¹è¯ä¸­è¡¨çŽ°å‡ºæŽ¨ç†èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•å¦‚ç®€å•åœ°æ‹¼æŽ¥å¯¹è¯åŽ†å²ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡é•¿åº¦æ— é™å¢žé•¿ï¼Œå¢žåŠ è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶é™ä½ŽæŽ¨ç†æ•ˆçŽ‡ã€‚æ­¤å¤–ï¼Œæ¨¡åž‹è¿˜å®¹æ˜“å‡ºçŽ°æŽ¨ç†åå·®ã€ä»»åŠ¡æ¼‚ç§»ã€å¹»è§‰ã€è¿‡åº¦è‡ªä¿¡å’Œè®°å¿†è¡°é€€ç­‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCogMemçš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è®°å¿†ç³»ç»Ÿï¼Œé€šè¿‡åˆ†å±‚è®°å¿†ç»“æž„æ¥ç®¡ç†å’Œåˆ©ç”¨å¯¹è¯åŽ†å²ä¿¡æ¯ã€‚é•¿æœŸè®°å¿†ï¼ˆLTMï¼‰å­˜å‚¨é€šç”¨çš„æŽ¨ç†ç­–ç•¥ï¼Œç›´æŽ¥è®¿é—®è®°å¿†ï¼ˆDAï¼‰è®°å½•å½“å‰ä¼šè¯çš„ç¬”è®°ï¼Œæ³¨æ„åŠ›ç„¦ç‚¹ï¼ˆFoAï¼‰åŠ¨æ€æž„å»ºå½“å‰è½®æ¬¡çš„ä¸Šä¸‹æ–‡ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æŽ¨ç†çš„å‡†ç¡®æ€§å’Œè¿žè´¯æ€§ï¼ŒåŒæ—¶æŽ§åˆ¶ä¸Šä¸‹æ–‡çš„é•¿åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCogMemæž¶æž„åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) é•¿æœŸè®°å¿†ï¼ˆLTMï¼‰ï¼šå­˜å‚¨è·¨ä¼šè¯çš„æŽ¨ç†ç­–ç•¥ï¼Œå¯ä»¥çœ‹ä½œæ˜¯æ¨¡åž‹çš„çŸ¥è¯†åº“ã€‚2) ç›´æŽ¥è®¿é—®è®°å¿†ï¼ˆDAï¼‰ï¼šç»´æŠ¤å½“å‰ä¼šè¯çš„ç¬”è®°ï¼ŒåŒ…æ‹¬å…³é”®ä¿¡æ¯å’ŒæŽ¨ç†æ­¥éª¤ã€‚3) æ³¨æ„åŠ›ç„¦ç‚¹ï¼ˆFoAï¼‰ï¼šæ ¹æ®å½“å‰ä»»åŠ¡å’ŒDAè®°å¿†ï¼Œä»ŽLTMä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶æž„å»ºç®€æ´çš„ä¸Šä¸‹æ–‡è¾“å…¥åˆ°LLMä¸­ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šLLMæŽ¥æ”¶FoAæž„å»ºçš„ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆå›žå¤å’Œæ›´æ–°DAè®°å¿†ï¼ŒDAè®°å¿†ç”¨äºŽåŽç»­è½®æ¬¡çš„FoAæž„å»ºå’ŒLTMæ£€ç´¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šCogMemçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶åˆ†å±‚è®°å¿†ç»“æž„å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡æž„å»ºæœºåˆ¶ã€‚ä¸Žç®€å•æ‹¼æŽ¥å¯¹è¯åŽ†å²çš„æ–¹æ³•ä¸åŒï¼ŒCogMemé€šè¿‡LTMå­˜å‚¨é€šç”¨çŸ¥è¯†ï¼ŒDAè®°å¿†è®°å½•ä¼šè¯çŠ¶æ€ï¼ŒFoAåŠ¨æ€æž„å»ºä¸Šä¸‹æ–‡ï¼Œä»Žè€Œå®žçŽ°äº†æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„æŽ¨ç†ã€‚è¿™ç§æž¶æž„èƒ½å¤Ÿæ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œå¹¶æœ‰æ•ˆç¼“è§£å¤šè½®å¯¹è¯ä¸­çš„æŽ¨ç†é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šLTMå¯ä»¥ä½¿ç”¨çŸ¥è¯†å›¾è°±æˆ–å‘é‡æ•°æ®åº“ç­‰æŠ€æœ¯å®žçŽ°ï¼ŒDAè®°å¿†å¯ä»¥ä½¿ç”¨ç®€å•çš„é”®å€¼å¯¹å­˜å‚¨ã€‚FoAçš„å…³é”®åœ¨äºŽå¦‚ä½•ä»ŽLTMä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¯ä»¥ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ç­‰æ–¹æ³•ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°å–å†³äºŽLLMçš„é€‰æ‹©å’Œè®­ç»ƒç›®æ ‡ï¼Œä½†æ•´ä½“ç›®æ ‡æ˜¯æé«˜æŽ¨ç†çš„å‡†ç¡®æ€§å’Œè¿žè´¯æ€§ï¼Œå¹¶æŽ§åˆ¶ä¸Šä¸‹æ–‡çš„é•¿åº¦ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

CogMemåœ¨TurnBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œæœ‰æ•ˆç¼“è§£äº†æŽ¨ç†å¤±è´¥ï¼ŒæŽ§åˆ¶äº†ä¸Šä¸‹æ–‡å¢žé•¿ï¼Œå¹¶æé«˜äº†æ‰©å±•æŽ¨ç†é“¾ä¸­çš„ä¸€è‡´æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒCogMemæž¶æž„åœ¨å¤šè½®æŽ¨ç†ä»»åŠ¡ä¸­ä¼˜äºŽä¼ ç»Ÿæ–¹æ³•ï¼Œæ›´æŽ¥è¿‘äººç±»çš„æŽ¨ç†èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

CogMemæž¶æž„å¯åº”ç”¨äºŽéœ€è¦æŒç»­å¤šè½®äº¤äº’çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¢æœã€å¯¹è¯å¼AIåŠ©æ‰‹ã€æ•™è‚²è¾…å¯¼ç­‰ã€‚é€šè¿‡æå‡LLMåœ¨å¤šè½®å¯¹è¯ä¸­çš„æŽ¨ç†èƒ½åŠ›å’Œä¸€è‡´æ€§ï¼Œå¯ä»¥æä¾›æ›´å¯é ã€æ›´äººæ€§åŒ–çš„æœåŠ¡ï¼Œå¹¶æœ‰æœ›åœ¨å¤æ‚é—®é¢˜è§£å†³å’Œå†³ç­–æ”¯æŒæ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.

