---
layout: default
title: Inflation Attitudes of Large Language Models
---

# Inflation Attitudes of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14306" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14306v1</a>
  <a href="https://arxiv.org/pdf/2512.14306.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14306v1" onclick="toggleFavorite(this, '2512.14306v1', 'Inflation Attitudes of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikoleta Anesti, Edward Hill, Andreas Joseph

**åˆ†ç±»**: cs.CL, econ.EM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 41 pages, 11 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿé€šèƒ€é¢„æœŸï¼Œåˆ†æå…¶å¯¹å®è§‚ç»æµä¿¡å·çš„ååº”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é€šè´§è†¨èƒ€é¢„æœŸ` `å®è§‚ç»æµå»ºæ¨¡` `Shapleyå€¼åˆ†è§£` `ç¤¾ä¼šç§‘å­¦åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ¨¡æ‹Ÿå’Œé¢„æµ‹ä¸ªä½“åŠç¾¤ä½“å¯¹é€šè´§è†¨èƒ€çš„æ„ŸçŸ¥å’Œé¢„æœŸï¼Œé˜»ç¢äº†ç»æµæ”¿ç­–çš„åˆ¶å®šã€‚
2. è®ºæ–‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿä¸ªä½“å¯¹é€šèƒ€çš„æ„ŸçŸ¥ï¼Œå¹¶åˆ†æå…¶å¯¹ä¸åŒå®è§‚ç»æµä¿¡å·çš„ååº”ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLLMåœ¨çŸ­æœŸå†…èƒ½è¾ƒå¥½åœ°è·Ÿè¸ªè°ƒæŸ¥é¢„æµ‹å’Œå®˜æ–¹ç»Ÿè®¡ï¼Œä½†åœ¨æ¶ˆè´¹è€…ä»·æ ¼é€šèƒ€æ¨¡å‹æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç‰¹åˆ«æ˜¯GPT-3.5-turboï¼ˆGPTï¼‰ï¼ŒåŸºäºå®è§‚ç»æµä»·æ ¼ä¿¡å·å½¢æˆé€šèƒ€æ„ŸçŸ¥å’Œé¢„æœŸçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†LLMçš„è¾“å‡ºä¸å®¶åº­è°ƒæŸ¥æ•°æ®å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œæ¨¡æ‹Ÿè‹±å›½å¤®è¡Œé€šèƒ€æ€åº¦è°ƒæŸ¥ï¼ˆIASï¼‰çš„ä¿¡æ¯é›†å’Œäººå£ç‰¹å¾ã€‚æˆ‘ä»¬çš„å‡†å®éªŒè®¾è®¡åˆ©ç”¨äº†GPTåœ¨2021å¹´9æœˆçš„è®­ç»ƒæˆªæ­¢æ—¶é—´ï¼Œè¿™æ„å‘³ç€å®ƒä¸äº†è§£éšåçš„è‹±å›½é€šèƒ€é£™å‡ã€‚æˆ‘ä»¬å‘ç°GPTåœ¨çŸ­æœŸå†…è·Ÿè¸ªæ€»ä½“è°ƒæŸ¥é¢„æµ‹å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®ã€‚åœ¨åˆ†è§£å±‚é¢ï¼ŒGPTå¤åˆ¶äº†å®¶åº­é€šèƒ€æ„ŸçŸ¥çš„å…³é”®ç»éªŒè§„å¾‹ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ”¶å…¥ã€ä½æˆ¿ä¿æœ‰æƒå’Œç¤¾ä¼šé˜¶å±‚ã€‚ä¸€ç§æ–°é¢–çš„Shapleyå€¼åˆ†è§£æ–¹æ³•é€‚ç”¨äºåˆæˆè°ƒæŸ¥ç¯å¢ƒï¼Œä¸ºä¸æç¤ºå†…å®¹ç›¸å…³çš„æ¨¡å‹è¾“å‡ºé©±åŠ¨å› ç´ æä¾›äº†æ˜ç¡®çš„è§è§£ã€‚æˆ‘ä»¬å‘ç°GPTè¡¨ç°å‡ºå¯¹é£Ÿå“é€šèƒ€ä¿¡æ¯çš„é«˜åº¦æ•æ„Ÿæ€§ï¼Œç±»ä¼¼äºäººç±»å—è®¿è€…ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå‘ç°å®ƒç¼ºä¹ä¸€è‡´çš„æ¶ˆè´¹è€…ä»·æ ¼é€šèƒ€æ¨¡å‹ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ç”¨äºè¯„ä¼°LLMåœ¨ç¤¾ä¼šç§‘å­¦ä¸­çš„è¡Œä¸ºï¼Œæ¯”è¾ƒä¸åŒçš„æ¨¡å‹ï¼Œæˆ–ååŠ©è°ƒæŸ¥è®¾è®¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨æ¨¡æ‹Ÿä¸ªä½“å’Œç¾¤ä½“å¯¹é€šè´§è†¨èƒ€çš„æ„ŸçŸ¥å’Œé¢„æœŸæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¼ ç»Ÿçš„ç»æµæ¨¡å‹å¾€å¾€è¿‡äºç®€åŒ–ï¼Œéš¾ä»¥æ•æ‰äººç±»è¡Œä¸ºçš„å¤æ‚æ€§ã€‚åŒæ—¶ï¼Œç›´æ¥è¿›è¡Œå¤§è§„æ¨¡è°ƒæŸ¥æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ›´æœ‰æ•ˆåœ°ç†è§£å’Œé¢„æµ‹é€šè´§è†¨èƒ€é¢„æœŸï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ¶å®šç»æµæ”¿ç­–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¸€ç§â€œåˆæˆä»£ç†â€ï¼Œæ¨¡æ‹Ÿä¸ªä½“å¯¹é€šè´§è†¨èƒ€çš„æ„ŸçŸ¥å’Œé¢„æœŸã€‚é€šè¿‡å‘LLMè¾“å…¥ä¸åŒçš„å®è§‚ç»æµä¿¡æ¯ï¼Œå¹¶è§‚å¯Ÿå…¶è¾“å‡ºï¼Œå¯ä»¥åˆ†æLLMå¦‚ä½•å½¢æˆé€šèƒ€é¢„æœŸï¼Œä»¥åŠå“ªäº›å› ç´ å¯¹å…¶å½±å“æœ€å¤§ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºæˆæœ¬ä½ã€å¯æ‰©å±•æ€§å¼ºï¼Œå¹¶ä¸”å¯ä»¥è¿›è¡Œç»†ç²’åº¦çš„åˆ†æã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æ”¶é›†è‹±å›½å¤®è¡Œé€šèƒ€æ€åº¦è°ƒæŸ¥ï¼ˆIASï¼‰çš„æ•°æ®ï¼ŒåŒ…æ‹¬å®¶åº­è°ƒæŸ¥æ•°æ®å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®ã€‚2) æ„å»ºæç¤ºï¼ˆpromptsï¼‰ï¼Œå‘GPT-3.5-turboè¾“å…¥å®è§‚ç»æµä¿¡æ¯ï¼Œæ¨¡æ‹ŸIASçš„ä¿¡æ¯é›†å’Œäººå£ç‰¹å¾ã€‚3) åˆ†æGPT-3.5-turboçš„è¾“å‡ºï¼Œå°†å…¶ä¸å®¶åº­è°ƒæŸ¥æ•°æ®å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°LLMçš„æ€§èƒ½ã€‚4) ä½¿ç”¨Shapleyå€¼åˆ†è§£æ–¹æ³•ï¼Œåˆ†æLLMè¾“å‡ºçš„é©±åŠ¨å› ç´ ï¼Œäº†è§£å“ªäº›ä¿¡æ¯å¯¹LLMçš„é€šèƒ€é¢„æœŸå½±å“æœ€å¤§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºé€šèƒ€é¢„æœŸç ”ç©¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„Shapleyå€¼åˆ†è§£æ–¹æ³•ï¼Œç”¨äºåˆ†æLLMè¾“å‡ºçš„é©±åŠ¨å› ç´ ã€‚ä¸ä¼ ç»Ÿçš„ç»æµæ¨¡å‹ç›¸æ¯”ï¼ŒLLMèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººç±»è¡Œä¸ºçš„å¤æ‚æ€§ã€‚ä¸ç›´æ¥è¿›è¡Œå¤§è§„æ¨¡è°ƒæŸ¥ç›¸æ¯”ï¼Œä½¿ç”¨LLMè¿›è¡Œæ¨¡æ‹Ÿæˆæœ¬æ›´ä½ã€å¯æ‰©å±•æ€§æ›´å¼ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æç¤ºè®¾è®¡æ–¹é¢ï¼Œç ”ç©¶äººå‘˜ç²¾å¿ƒè®¾è®¡äº†æç¤ºï¼Œä»¥æ¨¡æ‹ŸIASçš„ä¿¡æ¯é›†å’Œäººå£ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œæç¤ºä¸­åŒ…å«äº†å…³äºæ”¶å…¥ã€ä½æˆ¿ä¿æœ‰æƒå’Œç¤¾ä¼šé˜¶å±‚çš„ä¿¡æ¯ã€‚åœ¨Shapleyå€¼åˆ†è§£æ–¹é¢ï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨äº†ä¸€ç§é€‚ç”¨äºåˆæˆè°ƒæŸ¥ç¯å¢ƒçš„Shapleyå€¼åˆ†è§£æ–¹æ³•ï¼Œä»¥åˆ†æLLMè¾“å‡ºçš„é©±åŠ¨å› ç´ ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜å…³æ³¨äº†GPT-3.5-turboçš„è®­ç»ƒæˆªæ­¢æ—¶é—´ï¼ˆ2021å¹´9æœˆï¼‰ï¼Œä»¥ç¡®ä¿LLMä¸äº†è§£éšåçš„è‹±å›½é€šèƒ€é£™å‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶å‘ç°ï¼ŒGPT-3.5-turboåœ¨çŸ­æœŸå†…èƒ½å¤Ÿè¾ƒå¥½åœ°è·Ÿè¸ªæ€»ä½“è°ƒæŸ¥é¢„æµ‹å’Œå®˜æ–¹ç»Ÿè®¡æ•°æ®ã€‚åœ¨åˆ†è§£å±‚é¢ï¼ŒGPT-3.5-turboå¤åˆ¶äº†å®¶åº­é€šèƒ€æ„ŸçŸ¥çš„å…³é”®ç»éªŒè§„å¾‹ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ”¶å…¥ã€ä½æˆ¿ä¿æœ‰æƒå’Œç¤¾ä¼šé˜¶å±‚ã€‚Shapleyå€¼åˆ†è§£ç»“æœè¡¨æ˜ï¼ŒGPT-3.5-turboå¯¹é£Ÿå“é€šèƒ€ä¿¡æ¯è¡¨ç°å‡ºé«˜åº¦æ•æ„Ÿæ€§ï¼Œç±»ä¼¼äºäººç±»å—è®¿è€…ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¤¾ä¼šç§‘å­¦é¢†åŸŸï¼Œç”¨äºè¯„ä¼°LLMåœ¨ç»æµé¢„æµ‹å’Œæ”¿ç­–åˆ†æä¸­çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ¯”è¾ƒä¸åŒçš„LLMï¼Œæˆ–ååŠ©è°ƒæŸ¥è®¾è®¡ï¼Œæé«˜è°ƒæŸ¥çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å®è§‚ç»æµå˜é‡çš„é¢„æµ‹ï¼Œä¾‹å¦‚å¤±ä¸šç‡å’Œç»æµå¢é•¿ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.

