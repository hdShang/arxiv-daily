---
layout: default
title: Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering
---

# Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02311" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02311v2</a>
  <a href="https://arxiv.org/pdf/2505.02311.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02311v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02311v2', 'Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jihao Zhao, Chunlai Zhou, Daixuan Li, Shuaishuai Zu, Biao Qin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-11-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAttenHScoreä»¥è§£å†³å°å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¹»è§‰æ£€æµ‹` `è¯­è¨€æ¨¡å‹` `é—®ç­”ç³»ç»Ÿ` `åŠ¨æ€è°ƒç”¨` `ä¸ç¡®å®šæ€§æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–åå¤„ç†æŠ€æœ¯ï¼Œæ— æ³•æœ‰æ•ˆè§£å†³å°å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¹»è§‰é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºAttenHScoreæŒ‡æ ‡ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ£€æµ‹é˜ˆå€¼ï¼Œå®æ—¶è¯„ä¼°å°å‹æ¨¡å‹çš„å¹»è§‰ç´¯ç§¯ï¼Œä¼˜åŒ–å¤§å‹æ¨¡å‹çš„è°ƒç”¨æ—¶æœºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAttenHScoreåœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å¹»è§‰æ£€æµ‹èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤æ‚æŸ¥è¯¢åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å’Œå°å‹è¯­è¨€æ¨¡å‹çš„åä½œèŒƒå¼æœ‰æ•ˆå¹³è¡¡äº†æ€§èƒ½ä¸æˆæœ¬ï¼Œä½†åœ¨å°å‹æ¨¡å‹ä¸­ï¼Œå‡†ç¡®è¯†åˆ«ä½•æ—¶è°ƒç”¨å¤§å‹æ¨¡å‹ä»¥åº”å¯¹å¹»è§‰é—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä»¥å¾€çš„ä¼˜åŒ–ä¸»è¦é›†ä¸­åœ¨åå¤„ç†æŠ€æœ¯ä¸Šï¼Œä¸æ¨¡å‹æ¨ç†è¿‡ç¨‹åˆ†ç¦»ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ä¸”æ•ˆæœæœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ç”¨çš„è°ƒç”¨è¯„ä¼°æŒ‡æ ‡AttenHScoreï¼Œè®¡ç®—å°å‹æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¹»è§‰ç´¯ç§¯ä¸ä¼ æ’­ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ£€æµ‹é˜ˆå€¼ï¼Œå®ç°æ›´å‡†ç¡®çš„å®æ—¶è°ƒç”¨å¤§å‹æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„çŸ¥è¯†é‡ç»„ï¼Œå¸®åŠ©å…¶æ›´å¥½åœ°æ•æ‰å…³é”®ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAttenHScoreåœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šä¼˜äºå¤§å¤šæ•°åŸºçº¿ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶è¡¨ç°çªå‡ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­äº§ç”Ÿå¹»è§‰æ—¶ï¼Œä½•æ—¶è°ƒç”¨å¤§å‹æ¨¡å‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–åå¤„ç†ï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬ä¸”æ•ˆæœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºAttenHScoreä½œä¸ºè°ƒç”¨è¯„ä¼°æŒ‡æ ‡ï¼Œå®æ—¶ç›‘æµ‹å°å‹æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¹»è§‰ç´¯ç§¯ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´é˜ˆå€¼æ¥ä¼˜åŒ–å¤§å‹æ¨¡å‹çš„è°ƒç”¨æ—¶æœºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å°å‹æ¨¡å‹ç”Ÿæˆã€å¹»è§‰ç›‘æµ‹å’Œå¤§å‹æ¨¡å‹è°ƒç”¨ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œå°å‹æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨AttenHScoreè¯„ä¼°å¹»è§‰ï¼›æœ€åï¼Œæ ¹æ®è¯„ä¼°ç»“æœå†³å®šæ˜¯å¦è°ƒç”¨å¤§å‹æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šAttenHScoreçš„æå‡ºæ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒé€šè¿‡é‡åŒ–å¹»è§‰çš„ä¼ æ’­ä¸ç´¯ç§¯ï¼Œæä¾›äº†æ¯”ä¼ ç»Ÿåå¤„ç†æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆçš„å®æ—¶è°ƒç”¨æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒåŠ¨æ€é˜ˆå€¼çš„è®¾ç½®åŸºäºå®æ—¶ç›‘æµ‹çš„å¹»è§‰æ°´å¹³ï¼Œç¡®ä¿åœ¨å¹»è§‰é£é™©è¾ƒé«˜æ—¶åŠæ—¶è°ƒç”¨å¤§å‹æ¨¡å‹ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAttenHScoreåœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šä¼˜äºå¤§å¤šæ•°åŸºçº¿ï¼Œå°¤å…¶åœ¨å¤æ‚æŸ¥è¯¢ä¸­ï¼Œå¹»è§‰æ£€æµ‹èƒ½åŠ›æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”æ— éœ€é¢å¤–æ¨¡å‹è®­ç»ƒï¼Œå±•ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å¯¹è¯æœºå™¨äººä»¥åŠä¿¡æ¯æ£€ç´¢ç­‰åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–å°å‹è¯­è¨€æ¨¡å‹çš„è°ƒç”¨ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç³»ç»Ÿçš„å“åº”å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baselines in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.

