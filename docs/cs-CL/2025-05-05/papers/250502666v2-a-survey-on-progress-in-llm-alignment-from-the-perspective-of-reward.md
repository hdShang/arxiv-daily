---
layout: default
title: A Survey on Progress in LLM Alignment from the Perspective of Reward Design
---

# A Survey on Progress in LLM Alignment from the Perspective of Reward Design

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02666" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02666v2</a>
  <a href="https://arxiv.org/pdf/2505.02666.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02666v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02666v2', 'A Survey on Progress in LLM Alignment from the Perspective of Reward Design')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Miaomiao Ji, Yanqiu Wu, Zhibin Wu, Shoujin Wang, Jian Yang, Mark Dras, Usman Naseem

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-08-29)

**å¤‡æ³¨**: Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¥–åŠ±è®¾è®¡æ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¥–åŠ±è®¾è®¡` `å¯¹é½ç ”ç©¶` `ä¼˜åŒ–èŒƒå¼` `å¼ºåŒ–å­¦ä¹ ` `å¤šç›®æ ‡ä¼˜åŒ–` `äººç±»ä»·å€¼è§‚`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¥–åŠ±è®¾è®¡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆå¯¹é½å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œæ¶µç›–æ•°å­¦å…¬å¼ã€æ„å»ºå®è·µåŠä¼˜åŒ–èŒƒå¼çš„äº’åŠ¨ã€‚
3. é€šè¿‡å®è§‚åˆ†ç±»æ³•ï¼Œè®ºæ–‡ä¸ºå¥–åŠ±æœºåˆ¶æä¾›äº†æ¸…æ™°çš„æ¦‚å¿µæ¡†æ¶å’Œå®è·µæŒ‡å¯¼ï¼Œä¿ƒè¿›äº†å¯¹é½ç ”ç©¶çš„è¿›å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±è®¾è®¡åœ¨å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œæ˜¯åé¦ˆä¿¡å·ä¸æ¨¡å‹ä¼˜åŒ–ä¹‹é—´çš„æ¡¥æ¢ã€‚æœ¬æ–‡å¯¹å¥–åŠ±å»ºæ¨¡è¿›è¡Œäº†ç»“æ„åŒ–ç»„ç»‡ï¼Œé‡ç‚¹è®¨è®ºäº†æ•°å­¦å…¬å¼ã€æ„å»ºå®è·µå’Œä¸ä¼˜åŒ–èŒƒå¼çš„äº’åŠ¨ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡å‘å±•äº†ä¸€ä¸ªå®è§‚å±‚é¢çš„åˆ†ç±»æ³•ï¼Œæè¿°äº†å¥–åŠ±æœºåˆ¶çš„äº’è¡¥ç»´åº¦ï¼Œä»è€Œä¸ºå¯¹é½ç ”ç©¶æä¾›äº†æ¦‚å¿µä¸Šçš„æ¸…æ™°æ€§å’Œå®è·µæŒ‡å¯¼ã€‚LLMå¯¹é½çš„è¿›å±•å¯ä»¥ç†è§£ä¸ºå¥–åŠ±è®¾è®¡ç­–ç•¥çš„æŒç»­ä¼˜åŒ–ï¼Œè¿‘æœŸçš„å‘å±•çªæ˜¾äº†ä»åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ°æ— RLä¼˜åŒ–çš„èŒƒå¼è½¬å˜ï¼Œä»¥åŠä»å•ä»»åŠ¡åˆ°å¤šç›®æ ‡å’Œå¤æ‚è®¾ç½®çš„æ¼”å˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½ä¸­çš„å¥–åŠ±è®¾è®¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¥–åŠ±ä¿¡å·çš„æœ‰æ•ˆæ€§å’Œä¼˜åŒ–ç­–ç•¥çš„é€‚åº”æ€§ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“æ„åŒ–çš„å¥–åŠ±å»ºæ¨¡ï¼Œæ˜ç¡®å¥–åŠ±æœºåˆ¶çš„æ•°å­¦åŸºç¡€å’Œå®è·µåº”ç”¨ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å’Œå®æ–½å¥–åŠ±è®¾è®¡ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ•°å­¦å…¬å¼çš„æ„å»ºã€å¥–åŠ±è®¾è®¡çš„å®è·µæ–¹æ³•ä»¥åŠä¸ä¼˜åŒ–èŒƒå¼çš„äº’åŠ¨ã€‚è¿™äº›æ¨¡å—ç›¸äº’å…³è”ï¼Œå…±åŒæ”¯æŒå¯¹é½ç ”ç©¶çš„æ·±å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå®è§‚å±‚é¢çš„å¥–åŠ±æœºåˆ¶åˆ†ç±»æ³•ï¼Œèƒ½å¤Ÿä»å¤šä¸ªç»´åº¦å¯¹å¥–åŠ±è®¾è®¡è¿›è¡Œç³»ç»Ÿæ€§åˆ†æã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€è§†è§’å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®ºæ–‡å¼ºè°ƒäº†å¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠä¼˜åŒ–ç®—æ³•çš„é€‚é…æ€§ï¼Œè¿™äº›éƒ½æ˜¯ç¡®ä¿æ¨¡å‹æœ‰æ•ˆå¯¹é½çš„å…³é”®å› ç´ ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿåœ¨æ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æå‡ºçš„å¥–åŠ±è®¾è®¡æ¡†æ¶åï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹é½ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šç›®æ ‡ä¼˜åŒ–åœºæ™¯ä¸­ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†15%ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„å¥–åŠ±è®¾è®¡ï¼Œå¯ä»¥æå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œä½¿å…¶æ›´å¥½åœ°ç†è§£å’Œå“åº”äººç±»éœ€æ±‚ï¼Œè¿›è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½çš„å®‰å…¨å’Œå¯æ§å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward design plays a pivotal role in aligning large language models (LLMs) with human values, serving as the bridge between feedback signals and model optimization. This survey provides a structured organization of reward modeling and addresses three key aspects: mathematical formulation, construction practices, and interaction with optimization paradigms. Building on this, it develops a macro-level taxonomy that characterizes reward mechanisms along complementary dimensions, thereby offering both conceptual clarity and practical guidance for alignment research. The progression of LLM alignment can be understood as a continuous refinement of reward design strategies, with recent developments highlighting paradigm shifts from reinforcement learning (RL)-based to RL-free optimization and from single-task to multi-objective and complex settings.

