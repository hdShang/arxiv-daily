---
layout: default
title: "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization"
---

# ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02819" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02819v3</a>
  <a href="https://arxiv.org/pdf/2505.02819.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02819v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02819v3', 'ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-06-20)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mts-ai/ReplaceMe)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReplaceMeä»¥è§£å†³å˜æ¢å™¨å—ç®€åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ·±åº¦å‰ªæ` `å˜æ¢å™¨ç®€åŒ–` `çº¿æ€§åŒ–` `æ— è®­ç»ƒæ–¹æ³•` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‰ªææ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ä¸”æ•ˆç‡ä½ä¸‹ã€‚
2. ReplaceMeé€šè¿‡æ·±åº¦å‰ªæå’Œçº¿æ€§åŒ–å˜æ¢å™¨å—ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç®€åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å¤æ‚åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReplaceMeåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾25%çš„å‰ªæï¼ŒåŒæ—¶ä¿æŒçº¦90%çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ— è®­ç»ƒæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ReplaceMeï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ— è®­ç»ƒæ·±åº¦å‰ªææ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å˜æ¢å™¨å—æ›¿æ¢ä¸ºçº¿æ€§æ“ä½œï¼ŒåŒæ—¶åœ¨ä½å‹ç¼©æ¯”ä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å‰ªææ–¹æ³•ä¸åŒï¼ŒReplaceMeä»…éœ€ä¸€ä¸ªå°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§å˜æ¢ï¼Œä»è€Œè¿‘ä¼¼å‰ªæåçš„å—ã€‚ä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥ä¸å‰©ä½™çš„å˜æ¢å™¨å—æ— ç¼åˆå¹¶ï¼Œæ¶ˆé™¤é¢å¤–ç½‘ç»œå‚æ•°çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒReplaceMeåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾25%çš„å‰ªæï¼ŒåŒæ—¶åœ¨å¼€æ”¾åŸºå‡†ä¸Šä¿ç•™äº†çº¦90%çš„åŸå§‹æ¨¡å‹æ€§èƒ½ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒæˆ–ä¿®å¤æ­¥éª¤ï¼Œè®¡ç®—å¼€é”€æå°ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¼€æºåº“ï¼Œå®æ–½ReplaceMeåŠå¤šç§å…ˆè¿›çš„æ·±åº¦å‰ªææŠ€æœ¯ï¼Œåœ°å€ä¸ºhttps://github.com/mts-ai/ReplaceMeã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰çš„å˜æ¢å™¨æ¨¡å‹åœ¨å‰ªæè¿‡ç¨‹ä¸­é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œè¿™ä¸ä»…å¢åŠ äº†è®¡ç®—æˆæœ¬ï¼Œè¿˜é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReplaceMeçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ·±åº¦å‰ªæå°†å˜æ¢å™¨å—æ›¿æ¢ä¸ºçº¿æ€§æ“ä½œï¼Œåˆ©ç”¨å°è§„æ¨¡çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§å˜æ¢ï¼Œä»è€Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„è®­ç»ƒéœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReplaceMeçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨æ ¡å‡†æ•°æ®é›†ä¼°è®¡çº¿æ€§å˜æ¢ï¼›å…¶æ¬¡ï¼Œå°†è¯¥çº¿æ€§æ˜ å°„ä¸å‰©ä½™çš„å˜æ¢å™¨å—åˆå¹¶ï¼›æœ€åï¼Œå½¢æˆä¸€ä¸ªç®€åŒ–çš„ç½‘ç»œç»“æ„ï¼Œå‡å°‘äº†å‚æ•°æ•°é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šReplaceMeçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶è®­ç»ƒ-freeçš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„å‰ªæï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒReplaceMeä½¿ç”¨äº†å°è§„æ¨¡çš„æ ¡å‡†æ•°æ®é›†æ¥è¿›è¡Œçº¿æ€§æ˜ å°„çš„ä¼°è®¡ï¼Œç¡®ä¿äº†å‰ªæåæ¨¡å‹æ€§èƒ½çš„ä¿ç•™ï¼ŒåŒæ—¶é¿å…äº†å¤æ‚çš„å‚æ•°è°ƒæ•´å’Œç½‘ç»œç»“æ„ä¿®æ”¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ReplaceMeåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾25%çš„å‰ªæï¼ŒåŒæ—¶åœ¨å¼€æ”¾åŸºå‡†ä¸Šä¿ç•™äº†çº¦90%çš„åŸå§‹æ¨¡å‹æ€§èƒ½ï¼Œè¡¨ç°å‡ºè‰²ã€‚ä¸å…¶ä»–æ— è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒReplaceMeçš„æ€§èƒ½æ›´å…·ç«äº‰åŠ›ï¼Œä¸”è®¡ç®—å¼€é”€æå°ï¼Œå±•ç¤ºäº†å…¶åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ç®€åŒ–ä¸­çš„é‡è¦ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ReplaceMeçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨¡å‹éƒ¨ç½²çš„åœºæ™¯ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—å’Œå®æ—¶æ¨ç†ç­‰ã€‚å…¶ç®€åŒ–çš„ç½‘ç»œç»“æ„èƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œæ¨åŠ¨æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.

