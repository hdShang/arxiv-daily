---
layout: default
title: Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis
---

# Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03019" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03019v1</a>
  <a href="https://arxiv.org/pdf/2505.03019.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03019v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03019v1', 'Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: AlbÃ©rick Euraste DjirÃ©, Abdoul Kader KaborÃ©, Earl T. Barr, Jacques Klein, TegawendÃ© F. BissyandÃ©

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPEARLä»¥æ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹çš„è®°å¿†ç°è±¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è®°å¿†æ£€æµ‹` `è¾“å…¥æ‰°åŠ¨` `æ¨¡å‹è¯„ä¼°` `æ•°æ®éšç§` `çŸ¥è¯†äº§æƒ` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†å¤§å‹è¯­è¨€æ¨¡å‹çš„è®°å¿†ä¸æ³›åŒ–ï¼Œå¯¼è‡´å¯¹æ¨¡å‹è¯„ä¼°çš„å¯é æ€§äº§ç”Ÿç–‘è™‘ã€‚
2. PEARLé€šè¿‡è¾“å…¥æ‰°åŠ¨åˆ†ææ¥æ£€æµ‹è®°å¿†ç°è±¡ï¼Œè¯„ä¼°æ¨¡å‹å¯¹è¾“å…¥å˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œä»è€Œå®ç°æ— éœ€å†…éƒ¨è®¿é—®çš„æ£€æµ‹ã€‚
3. åœ¨Pythiaæ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPEARLèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ¨¡å‹çš„è®°å¿†è¡Œä¸ºï¼Œå¹¶åœ¨GPT 4oæ¨¡å‹ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿå¯èƒ½å‡ºç°é€å­—å¤è¿°è®­ç»ƒæ•°æ®è€ŒéçœŸæ­£æ³›åŒ–çš„ç°è±¡ã€‚è¿™ç§è®°å¿†ç°è±¡å¼•å‘äº†å…³äºæ•°æ®éšç§ã€çŸ¥è¯†äº§æƒå’Œæ¨¡å‹è¯„ä¼°å¯é æ€§çš„é‡å¤§æ‹…å¿§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•PEARLï¼Œé€šè¿‡åˆ†æè¾“å…¥æ‰°åŠ¨æ¥æ£€æµ‹LLMsçš„è®°å¿†ç°è±¡ã€‚PEARLè¯„ä¼°LLMæ€§èƒ½å¯¹è¾“å…¥æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œä»è€Œåœ¨ä¸éœ€è¦è®¿é—®æ¨¡å‹å†…éƒ¨çš„æƒ…å†µä¸‹å®ç°è®°å¿†æ£€æµ‹ã€‚é€šè¿‡å¯¹Pythiaå¼€æ”¾æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„å‘ç°ä¸ºè¯†åˆ«æ¨¡å‹ç®€å•é‡å¤å­¦ä¹ ä¿¡æ¯æä¾›äº†ç¨³å¥çš„æ¡†æ¶ã€‚PEARLåœ¨GPT 4oæ¨¡å‹ä¸Šçš„åº”ç”¨ä¸ä»…è¯†åˆ«äº†ç»å…¸æ–‡æœ¬å’Œå¸¸è§ä»£ç çš„è®°å¿†æ¡ˆä¾‹ï¼Œè¿˜æä¾›äº†è¯æ®è¡¨æ˜æŸäº›æ•°æ®ï¼ˆå¦‚ã€Šçº½çº¦æ—¶æŠ¥ã€‹çš„æ–°é—»æ–‡ç« ï¼‰å¯èƒ½æ˜¯æ¨¡å‹è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šé€å­—å¤è¿°è€ŒéçœŸæ­£æ³›åŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¯†åˆ«è®°å¿†ç°è±¡æ—¶å­˜åœ¨å±€é™ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„çœŸå®è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPEARLçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†æè¾“å…¥æ‰°åŠ¨å¯¹æ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§å½±å“ï¼Œæ¥æ£€æµ‹æ¨¡å‹æ˜¯å¦å­˜åœ¨è®°å¿†ç°è±¡ã€‚è¿™ç§æ–¹æ³•ä¸ä¾èµ–äºæ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPEARLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ‰°åŠ¨ç”Ÿæˆã€æ¨¡å‹è¾“å‡ºè¯„ä¼°å’Œè®°å¿†æ£€æµ‹ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹è¾“å…¥è¿›è¡Œæ‰°åŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„æµ‹è¯•æ ·æœ¬ï¼›ç„¶åï¼Œè¯„ä¼°æ¨¡å‹åœ¨è¿™äº›æ ·æœ¬ä¸Šçš„è¾“å‡ºä¸€è‡´æ€§ï¼›æœ€åï¼ŒåŸºäºä¸€è‡´æ€§åˆ†æåˆ¤æ–­æ¨¡å‹æ˜¯å¦å­˜åœ¨è®°å¿†ç°è±¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šPEARLçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ— éœ€è®¿é—®æ¨¡å‹å†…éƒ¨å³å¯æ£€æµ‹è®°å¿†ç°è±¡ï¼Œåˆ©ç”¨è¾“å…¥æ‰°åŠ¨çš„æ•æ„Ÿæ€§ä½œä¸ºåˆ¤æ–­ä¾æ®ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–æ¨¡å‹å†…éƒ¨æœºåˆ¶çš„æ£€æµ‹æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨PEARLä¸­ï¼Œè¾“å…¥æ‰°åŠ¨çš„ç±»å‹å’Œå¼ºåº¦æ˜¯å…³é”®è®¾è®¡å‚æ•°ï¼Œå½±å“æ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§è¯„ä¼°ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥é‡åŒ–è¾“å‡ºä¸€è‡´æ€§ï¼Œä»è€Œæœ‰æ•ˆè¯†åˆ«è®°å¿†è¡Œä¸ºã€‚å®éªŒä¸­è¿˜å¯¹ä¸åŒç±»å‹çš„è¾“å…¥æ‰°åŠ¨è¿›è¡Œäº†æ¯”è¾ƒï¼Œä»¥ä¼˜åŒ–æ£€æµ‹æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Pythiaæ¨¡å‹çš„å®éªŒä¸­ï¼ŒPEARLæˆåŠŸè¯†åˆ«äº†å¤šç§è®°å¿†æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬ç»å…¸æ–‡æœ¬å’Œå¸¸è§ä»£ç çš„é€å­—å¤è¿°ã€‚æ­¤å¤–ï¼Œåœ¨GPT 4oæ¨¡å‹çš„åº”ç”¨ä¸­ï¼ŒPEARLæä¾›äº†è¯æ®ï¼Œè¡¨æ˜æŸäº›æ•°æ®å¦‚ã€Šçº½çº¦æ—¶æŠ¥ã€‹çš„æ–‡ç« å¯èƒ½æ˜¯æ¨¡å‹è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„æ£€æµ‹èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PEARLæ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘å’Œè¯„ä¼°ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç¡®ä¿æ•°æ®éšç§å’ŒçŸ¥è¯†äº§æƒçš„åœºæ™¯ä¸­ã€‚é€šè¿‡æœ‰æ•ˆæ£€æµ‹æ¨¡å‹çš„è®°å¿†ç°è±¡ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„è¡Œä¸ºï¼Œæå‡æ¨¡å‹çš„å¯é æ€§å’Œé€æ˜åº¦ï¼Œè¿›è€Œæ¨åŠ¨AIæŠ€æœ¯çš„å¥åº·å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Large Language Models (LLMs) achieve remarkable performance through training on massive datasets, they can exhibit concerning behaviors such as verbatim reproduction of training data rather than true generalization. This memorization phenomenon raises significant concerns about data privacy, intellectual property rights, and the reliability of model evaluations. This paper introduces PEARL, a novel approach for detecting memorization in LLMs. PEARL assesses how sensitive an LLM's performance is to input perturbations, enabling memorization detection without requiring access to the model's internals. We investigate how input perturbations affect the consistency of outputs, enabling us to distinguish between true generalization and memorization. Our findings, following extensive experiments on the Pythia open model, provide a robust framework for identifying when the model simply regurgitates learned information. Applied on the GPT 4o models, the PEARL framework not only identified cases of memorization of classic texts from the Bible or common code from HumanEval but also demonstrated that it can provide supporting evidence that some data, such as from the New York Times news articles, were likely part of the training data of a given model.

