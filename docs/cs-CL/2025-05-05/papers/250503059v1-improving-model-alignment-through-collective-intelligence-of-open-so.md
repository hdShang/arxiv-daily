---
layout: default
title: Improving Model Alignment Through Collective Intelligence of Open-Source LLMS
---

# Improving Model Alignment Through Collective Intelligence of Open-Source LLMS

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03059" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03059v1</a>
  <a href="https://arxiv.org/pdf/2505.03059.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03059v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03059v1', 'Improving Model Alignment Through Collective Intelligence of Open-Source LLMS')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junlin Wang, Roy Xie, Shang Zhu, Jue Wang, Ben Athiwaratkun, Bhuwan Dhingra, Shuaiwen Leon Song, Ce Zhang, James Zou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆä»£ç†å¯¹é½æ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å¯¹é½` `é›†ä½“æ™ºèƒ½` `æ•°æ®ç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `è‡ªæˆ‘æ”¹è¿›` `ç›‘ç£å¾®è°ƒ` `åå¥½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•ä¾èµ–é«˜è´¨é‡çš„äººç±»æ ‡æ³¨æ•°æ®ï¼Œæ„å»ºæ­¤ç±»æ•°æ®é›†æˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ï¼Œå­˜åœ¨å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é™åˆ¶ã€‚
2. æœ¬æ–‡æå‡ºçš„æ··åˆä»£ç†å¯¹é½ï¼ˆMoAAï¼‰æ–¹æ³•ï¼Œé€šè¿‡é›†æˆå¤šç§è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯¹é½æ•°æ®ï¼Œæå‡æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨MoAAåï¼ŒLLaMA-3.1-8B-Instructåœ¨å¤šä¸ªè¯„ä¼°ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ„å»ºæœ‰ç”¨ä¸”æ— å®³çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦åŸºäºäººç±»æŒ‡ä»¤å’Œåé¦ˆçš„æœ‰æ•ˆå¯¹é½æ–¹æ³•ï¼Œè¿™éœ€è¦é«˜è´¨é‡çš„äººç±»æ ‡æ³¨æ•°æ®ã€‚ç„¶è€Œï¼Œæ„å»ºæ­¤ç±»æ•°æ®é›†é€šå¸¸æˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ï¼Œå¯èƒ½é¢ä¸´å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†æ··åˆä»£ç†å¯¹é½ï¼ˆMoAAï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨å¤šç§è¯­è¨€æ¨¡å‹çš„é›†ä½“ä¼˜åŠ¿æä¾›é«˜è´¨é‡çš„æ•°æ®è¿›è¡Œæ¨¡å‹å¯¹é½ã€‚é€šè¿‡é‡‡ç”¨MoAAï¼Œæˆ‘ä»¬å¢å¼ºäº†ç›‘ç£å¾®è°ƒå’Œåå¥½ä¼˜åŒ–çš„æ•ˆæœï¼Œç›¸æ¯”å•ä¸€æ¨¡å‹ç”Ÿæˆå¯¹é½æ•°æ®ï¼ˆå¦‚ä»…ä½¿ç”¨GPT-4oï¼‰ï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå°†LLaMA-3.1-8B-Instructåœ¨Arena-Hardä¸Šçš„èƒœç‡ä»19.5æå‡è‡³48.3ï¼Œåœ¨AlpacaEval2ä¸Šçš„èƒœç‡ä»22.33æå‡è‡³57.23ï¼Œå±•ç¤ºäº†é€šè¿‡è¿™ä¸€æ–°çš„å¯æ‰©å±•å’Œå¤šæ ·åŒ–çš„åˆæˆæ•°æ®æ–¹æ³•è¿›è¡Œæ¨¡å‹å¯¹é½çš„å‰æ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†MoAAèƒ½å¤Ÿå®ç°è‡ªæˆ‘æ”¹è¿›ç®¡é“ï¼Œä½¿å¾—åœ¨MoAç”Ÿæˆæ•°æ®ä¸Šå¾®è°ƒçš„æ¨¡å‹è¶…è¶Šå…¶åˆå§‹èƒ½åŠ›ï¼Œæä¾›äº†æˆ‘ä»¬çš„æ–¹æ¡ˆèƒ½å¤Ÿæ¨åŠ¨å¼€æºLLMså‰æ²¿çš„è¯æ®ã€‚æ•°æ®å’Œä»£ç å°†ä¼šå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é½è¿‡ç¨‹ä¸­å¯¹é«˜è´¨é‡äººç±»æ ‡æ³¨æ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•°æ®é›†æ„å»ºä¸Šæˆæœ¬é«˜ã€éš¾ä»¥æ‰©å±•ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´å¤šæ ·æ€§ä¸è¶³å’Œæ³›åŒ–èƒ½åŠ›å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ··åˆä»£ç†å¯¹é½ï¼ˆMoAAï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šç§è¯­è¨€æ¨¡å‹çš„é›†ä½“æ™ºæ…§ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯¹é½æ•°æ®ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚è¯¥æ–¹æ³•çš„è®¾è®¡æ—¨åœ¨åˆ©ç”¨ä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå…‹æœå•ä¸€æ¨¡å‹çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoAAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªé˜¶æ®µï¼Œé¦–å…ˆæ˜¯æ¨¡å‹é€‰æ‹©é˜¶æ®µï¼Œé€‰æ‹©ä¸åŒçš„è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°æ®ç”Ÿæˆï¼›æ¥ç€æ˜¯æ•°æ®ç”Ÿæˆé˜¶æ®µï¼Œåˆ©ç”¨è¿™äº›æ¨¡å‹ç”Ÿæˆå¯¹é½æ•°æ®ï¼›æœ€åæ˜¯å¾®è°ƒå’Œä¼˜åŒ–é˜¶æ®µï¼Œé€šè¿‡ç”Ÿæˆçš„æ•°æ®å¯¹ç›®æ ‡æ¨¡å‹è¿›è¡Œå¾®è°ƒå’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoAAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é›†æˆå¤šç§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´ä¸ºå¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„å¯¹é½æ•°æ®ï¼Œä¸ä¼ ç»Ÿä¾èµ–å•ä¸€æ¨¡å‹çš„æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒMoAAé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿ç”Ÿæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„é€‰æ‹©å’Œç»„åˆç­–ç•¥ä¹Ÿæ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ï¼Œç¡®ä¿ä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿èƒ½å¤Ÿå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨MoAAæ–¹æ³•åï¼ŒLLaMA-3.1-8B-Instructåœ¨Arena-Hardä¸Šçš„èƒœç‡ä»19.5æå‡è‡³48.3ï¼Œåœ¨AlpacaEval2ä¸Šçš„èƒœç‡ä»22.33æå‡è‡³57.23ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ¨¡å‹å¯¹é½æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿä¸ºæ„å»ºæ›´ä¸ºå®‰å…¨å’Œæœ‰æ•ˆçš„è¯­è¨€æ¨¡å‹æä¾›æ”¯æŒã€‚é€šè¿‡æé«˜æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªè¡Œä¸šä¸­å®ç°æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–å’Œäººæœºäº¤äº’ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback, which necessitates high-quality human-labeled data. Constructing such datasets is often expensive and hard to scale, and may face potential limitations on diversity and generalization. To address these challenges, we introduce Mixture of Agents Alignment (MoAA), that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning and preference optimization, leading to improved performance compared to using a single model alone to generate alignment data (e.g. using GPT-4o alone). Evaluation results show that our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising direction for model alignment through this new scalable and diverse synthetic data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement pipeline, where models finetuned on MoA-generated data surpass their own initial capabilities, providing evidence that our approach can push the frontier of open-source LLMs without reliance on stronger external supervision. Data and code will be released.

