---
layout: default
title: "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning"
---

# SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02363" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02363v1</a>
  <a href="https://arxiv.org/pdf/2505.02363.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02363v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02363v1', 'SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianjian Li, Daniel Khashabi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: To appear in ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSIMPLEMIXä»¥è§£å†³è¯­è¨€æ¨¡å‹åå¥½å­¦ä¹ ä¸­çš„æ•°æ®æ··åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `åå¥½å­¦ä¹ ` `æ•°æ®æ··åˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ ` `äººæœºäº¤äº’` `æ¨èç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åå¥½å­¦ä¹ ä¸­å¯¹åŸºäºç­–ç•¥å’Œéç­–ç•¥æ•°æ®çš„æœ‰æ•ˆæ€§ç¼ºä¹ç³»ç»Ÿæ€§æ¢ç´¢ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚
2. SIMPLEMIXé€šè¿‡ç®€å•æ··åˆåŸºäºç­–ç•¥å’Œéç­–ç•¥çš„æ•°æ®ï¼Œå……åˆ†åˆ©ç”¨ä¸¤è€…çš„äº’è¡¥ä¼˜åŠ¿ï¼Œæå‡åå¥½å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒè¯æ˜ï¼ŒSIMPLEMIXåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹³å‡æå‡å¹…åº¦è¾¾åˆ°6.03%ï¼Œè¶…è¶Šäº†å¤æ‚çš„ç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹é½è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ä¾èµ–äºæˆå¯¹åå¥½æ•°æ®é›†ã€‚å°½ç®¡ä¸€äº›ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºç­–ç•¥çš„æ•°æ®åœ¨åå¥½å­¦ä¹ ä¸­è¡¨ç°ä¼˜äºéç­–ç•¥æ•°æ®ï¼Œä½†ä¹Ÿæœ‰ç ”ç©¶æŒ‡å‡ºè¿™ç§ä¼˜åŠ¿å¯èƒ½ä¾èµ–äºå…·ä½“ä»»åŠ¡ã€‚æœ¬æ–‡å±•ç¤ºäº†åŸºäºç­–ç•¥å’Œéç­–ç•¥æ•°æ®åœ¨åå¥½ä¼˜åŒ–ä¸­çš„äº’è¡¥ä¼˜åŠ¿ï¼Œæå‡ºSIMPLEMIXæ–¹æ³•ï¼Œé€šè¿‡ç®€å•æ··åˆè¿™ä¸¤ç§æ•°æ®æºæ¥ç»“åˆå…¶ä¼˜åŠ¿ã€‚å®éªŒè¯æ˜ï¼ŒSIMPLEMIXåœ¨å¤šç§ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†è¯­è¨€æ¨¡å‹çš„å¯¹é½æ•ˆæœï¼Œå°¤å…¶åœ¨Alpaca Eval 2.0ä¸Šå¹³å‡æå‡äº†6.03%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹åå¥½å­¦ä¹ ä¸­åŸºäºç­–ç•¥ä¸éç­–ç•¥æ•°æ®çš„æœ‰æ•ˆç»“åˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨å•ä¸€æ•°æ®æºï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSIMPLEMIXçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç®€å•æ··åˆè¿™ä¸¤ç§æ•°æ®æºï¼Œåˆ©ç”¨åŸºäºç­–ç•¥æ•°æ®åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿å’Œéç­–ç•¥æ•°æ®åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ï¼Œä»è€Œå®ç°æ›´å¥½çš„åå¥½ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSIMPLEMIXçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ•°æ®æ··åˆã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†åŸºäºç­–ç•¥å’Œéç­–ç•¥çš„æ•°æ®ï¼Œç„¶åå°†å…¶æ··åˆï¼Œæ¥ç€è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæœ€åé€šè¿‡æ ‡å‡†åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSIMPLEMIXçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®æ··åˆç­–ç•¥ï¼Œç›¸è¾ƒäºä»¥å¾€å¤æ‚çš„ç»„åˆæ–¹æ³•ï¼ˆå¦‚HyPOå’ŒDPO-Mix-Pï¼‰ï¼Œå…¶å®ç°æ›´ä¸ºç›´æ¥ä¸”æ•ˆæœæ˜¾è‘—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSIMPLEMIXé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸¤ç§æ•°æ®æºçš„å½±å“ï¼Œå¹¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥é€‚åº”æ··åˆæ•°æ®çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSIMPLEMIXåœ¨Alpaca Eval 2.0ä¸Šç›¸æ¯”äºåŸºäºç­–ç•¥çš„DPOå’Œéç­–ç•¥çš„DPOå¹³å‡æå‡äº†6.03%ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºå¤æ‚çš„ç°æœ‰æ–¹æ³•ï¼Œå¦‚HyPOå’ŒDPO-Mix-Pï¼ŒSIMPLEMIXå¹³å‡æå‡äº†3.05%ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„äººæœºäº¤äº’ã€æ¨èç³»ç»Ÿä»¥åŠå†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼ŒSIMPLEMIXèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œåˆ›ä½œå·¥å…·çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.
>   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.

