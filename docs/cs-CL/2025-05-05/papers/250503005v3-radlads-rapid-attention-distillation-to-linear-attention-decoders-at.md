---
layout: default
title: "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale"
---

# RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03005" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03005v3</a>
  <a href="https://arxiv.org/pdf/2505.03005.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03005v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03005v3', 'RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniel Goldstein, Eric Alcaide, Janna Lu, Eugene Cheah

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-07-25)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/recursal/RADLADS-paper) | [HUGGINGFACE](https://huggingface.co/collections/recursal)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRADLADSä»¥å¿«é€Ÿè½¬æ¢è½¯maxæ³¨æ„åŠ›å˜æ¢å™¨ä¸ºçº¿æ€§è§£ç æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çº¿æ€§æ³¨æ„åŠ›` `æ¨¡å‹è’¸é¦` `è‡ªç„¶è¯­è¨€å¤„ç†` `é«˜æ•ˆæ¨ç†` `å¤§è§„æ¨¡æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è½¯maxæ³¨æ„åŠ›å˜æ¢å™¨åœ¨è®¡ç®—æ•ˆç‡å’Œèµ„æºæ¶ˆè€—ä¸Šå­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­ã€‚
2. RADLADSåè®®é€šè¿‡å¿«é€Ÿè’¸é¦æŠ€æœ¯ï¼Œå°†è½¯maxæ³¨æ„åŠ›å˜æ¢å™¨è½¬æ¢ä¸ºé«˜æ•ˆçš„çº¿æ€§æ³¨æ„åŠ›è§£ç å™¨ï¼Œæ˜¾è‘—é™ä½äº†æ‰€éœ€çš„è®­ç»ƒtokenæ•°é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè½¬æ¢åçš„æ¨¡å‹åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”æ¨ç†è´¨é‡ä¸åŸå§‹æ¨¡å‹ç›¸è¿‘ï¼Œå…·æœ‰è¾ƒé«˜çš„æ€§ä»·æ¯”ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†å¿«é€Ÿæ³¨æ„åŠ›è’¸é¦åˆ°çº¿æ€§æ³¨æ„åŠ›è§£ç å™¨çš„åè®®ï¼ˆRADLADSï¼‰ï¼Œæ—¨åœ¨å¿«é€Ÿå°†è½¯maxæ³¨æ„åŠ›å˜æ¢å™¨è½¬æ¢ä¸ºçº¿æ€§æ³¨æ„åŠ›è§£ç æ¨¡å‹ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸¤ç§æ–°çš„RWKVå˜ä½“æ¶æ„ï¼Œå¹¶å°†æµè¡Œçš„Qwen2.5å¼€æºæ¨¡å‹è½¬æ¢ä¸º7Bã€32Bå’Œ72Bçš„è§„æ¨¡ã€‚æˆ‘ä»¬çš„è½¬æ¢è¿‡ç¨‹ä»…éœ€350-700Mä¸ªtokenï¼Œè¿œä½äºåŸå§‹æ•™å¸ˆæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„tokenæ•°é‡çš„0.005%ã€‚è½¬æ¢ä¸º72Bçº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„æˆæœ¬ä¸åˆ°2000ç¾å…ƒï¼Œä½†æ¨ç†è´¨é‡æ¥è¿‘åŸå§‹å˜æ¢å™¨ã€‚è¿™äº›æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†åŒç±»çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„æœ€å…ˆè¿›ä¸‹æ¸¸æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨HuggingFaceä¸Šå‘å¸ƒäº†æ‰€æœ‰æ¨¡å‹ï¼Œ72Bæ¨¡å‹å—Qwenè®¸å¯åè®®çº¦æŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è½¯maxæ³¨æ„åŠ›å˜æ¢å™¨åœ¨å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸­é¢ä¸´çš„è®¡ç®—æ•ˆç‡ä½å’Œèµ„æºæ¶ˆè€—é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶ï¼Œå¾€å¾€éœ€è¦å¤§é‡çš„tokenå’Œè®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRADLADSåè®®çš„æ ¸å¿ƒåœ¨äºé€šè¿‡å¿«é€Ÿè’¸é¦æŠ€æœ¯ï¼Œå°†è½¯maxæ³¨æ„åŠ›å˜æ¢å™¨è½¬æ¢ä¸ºçº¿æ€§æ³¨æ„åŠ›è§£ç å™¨ï¼Œä»è€Œåœ¨ä¿æŒæ¨ç†è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®­ç»ƒæ‰€éœ€çš„tokenæ•°é‡å’Œæˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œåˆ©ç”¨å°‘é‡tokenè¿›è¡Œæ¨¡å‹è’¸é¦ï¼›å…¶æ¬¡ï¼Œå°†è’¸é¦åçš„æ¨¡å‹æ¶æ„è°ƒæ•´ä¸ºçº¿æ€§æ³¨æ„åŠ›è§£ç å™¨ã€‚æ•´ä¸ªæµç¨‹é«˜æ•ˆä¸”æ˜“äºå®æ–½ã€‚

**å…³é”®åˆ›æ–°**ï¼šRADLADSçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶è’¸é¦è¿‡ç¨‹æ‰€éœ€çš„tokenæ•°é‡æä½ï¼Œä»…ä¸ºåŸå§‹æ¨¡å‹çš„0.005%ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†è´¨é‡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ–°çš„RWKVå˜ä½“æ¶æ„ï¼Œå¹¶å¯¹æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿åœ¨è½¬æ¢è¿‡ç¨‹ä¸­å°½å¯èƒ½ä¿ç•™åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè½¬æ¢åçš„72Bçº¿æ€§æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¨ç†è´¨é‡æ¥è¿‘åŸå§‹è½¯maxæ³¨æ„åŠ›å˜æ¢å™¨ï¼Œä¸”è½¬æ¢æˆæœ¬ä½äº2000ç¾å…ƒï¼Œå±•ç°äº†æé«˜çš„æ€§ä»·æ¯”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RADLADSçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚é€šè¿‡é™ä½è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿å¾—å¤§è§„æ¨¡æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–èµ„æºå—é™ç¯å¢ƒä¸­å¾—ä»¥åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.
>   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper

