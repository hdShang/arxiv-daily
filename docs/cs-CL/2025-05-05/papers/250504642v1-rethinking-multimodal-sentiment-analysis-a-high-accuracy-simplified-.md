---
layout: default
title: Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture
---

# Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04642" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04642v1</a>
  <a href="https://arxiv.org/pdf/2505.04642.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04642v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04642v1', 'Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nischal Mandal, Yang Li

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§èåˆæ¶æ„ä»¥æå‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æå‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ` `æ·±åº¦å­¦ä¹ ` `è½»é‡çº§æ¨¡å‹` `ç‰¹å¾èåˆ` `æƒ…æ„Ÿè®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ–¹æ³•é€šå¸¸ä¾èµ–å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ä¸”éš¾ä»¥éƒ¨ç½²ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡ç®€å•çš„ç‰¹å¾æ‹¼æ¥å’Œå¯†é›†èåˆå±‚å®ç°é«˜æ•ˆçš„æƒ…æ„Ÿåˆ†ç±»ã€‚
3. åœ¨IEMOCAPæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹åœ¨å…­ä¸ªæƒ…æ„Ÿç±»åˆ«ä¸­è¾¾åˆ°äº†92%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ˜¯æƒ…æ„Ÿè®¡ç®—ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆè¯­è¨€ã€éŸ³é¢‘å’Œè§†è§‰ä¿¡å·æ¥ç†è§£äººç±»æƒ…æ„Ÿã€‚å°½ç®¡è®¸å¤šè¿‘æœŸæ–¹æ³•é‡‡ç”¨å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶å’Œå±‚æ¬¡æ¶æ„ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡ä¸”æœ‰æ•ˆçš„åŸºäºèåˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè¯è¯­çº§æƒ…æ„Ÿåˆ†ç±»ã€‚åˆ©ç”¨åŒ…å«å¯¹é½æ–‡æœ¬ã€éŸ³é¢‘æ´¾ç”Ÿæ•°å€¼ç‰¹å¾å’Œè§†è§‰æè¿°ç¬¦çš„åŸºå‡†IEMOCAPæ•°æ®é›†ï¼Œè®¾è®¡äº†ä½¿ç”¨å…¨è¿æ¥å±‚å’Œdropoutæ­£åˆ™åŒ–çš„ç‰¹å®šæ¨¡æ€ç¼–ç å™¨ã€‚ç„¶åé€šè¿‡ç®€å•çš„æ‹¼æ¥æ–¹å¼èåˆæ¨¡æ€ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¯†é›†èåˆå±‚æ•æ‰è·¨æ¨¡æ€äº¤äº’ã€‚è¯¥æ¶æ„é¿å…äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ï¼Œåœ¨å…­ä¸ªæƒ…æ„Ÿç±»åˆ«ä¸­å®ç°äº†92%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒçš„ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å—åŒ–è®¾è®¡ï¼Œç®€å•çš„èåˆç­–ç•¥å¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è¶…è¶Šæˆ–åŒ¹é…æ›´å¤æ‚çš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å¤æ‚çš„æ¶æ„å’Œæœºåˆ¶ï¼Œå¯¼è‡´åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§è½»é‡çº§çš„èåˆæ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ€ç‰¹å®šç¼–ç å™¨å’Œç®€å•çš„ç‰¹å¾æ‹¼æ¥ï¼Œé™ä½è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒæƒ…æ„Ÿåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡æ€ç‰¹å®šç¼–ç å™¨ã€ç‰¹å¾æ‹¼æ¥æ¨¡å—å’Œå¯†é›†èåˆå±‚ã€‚æ¨¡æ€ç‰¹å®šç¼–ç å™¨ä½¿ç”¨å…¨è¿æ¥å±‚æå–ç‰¹å¾ï¼Œéšåé€šè¿‡æ‹¼æ¥èåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨ç®€å•çš„æ‹¼æ¥ç­–ç•¥æ¥èåˆæ¨¡æ€ç‰¹å¾ï¼Œé¿å…äº†å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶åœ¨å‡†ç¡®æ€§ä¸Šä¸å¤æ‚æ¨¡å‹ç›¸å½“ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä½¿ç”¨å…¨è¿æ¥å±‚è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡dropoutæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆã€‚èåˆè¿‡ç¨‹é‡‡ç”¨ç®€å•çš„æ‹¼æ¥æ–¹å¼ï¼Œæœ€åé€šè¿‡å¯†é›†å±‚æ•æ‰è·¨æ¨¡æ€äº¤äº’ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨IEMOCAPæ•°æ®é›†ä¸Šå®ç°äº†92%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¼˜äºè®¸å¤šå¤æ‚æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æƒ…æ„Ÿè®¡ç®—ã€ç¤¾äº¤åª’ä½“åˆ†æå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚è½»é‡çº§çš„æ¨¡å‹è®¾è®¡ä½¿å…¶é€‚åˆåœ¨ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œèƒ½å¤Ÿå®æ—¶åˆ†æç”¨æˆ·æƒ…æ„Ÿï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal sentiment analysis, a pivotal task in affective computing, seeks to understand human emotions by integrating cues from language, audio, and visual signals. While many recent approaches leverage complex attention mechanisms and hierarchical architectures, we propose a lightweight, yet effective fusion-based deep learning model tailored for utterance-level emotion classification. Using the benchmark IEMOCAP dataset, which includes aligned text, audio-derived numeric features, and visual descriptors, we design a modality-specific encoder using fully connected layers followed by dropout regularization. The modality-specific representations are then fused using simple concatenation and passed through a dense fusion layer to capture cross-modal interactions. This streamlined architecture avoids computational overhead while preserving performance, achieving a classification accuracy of 92% across six emotion categories. Our approach demonstrates that with careful feature engineering and modular design, simpler fusion strategies can outperform or match more complex models, particularly in resource-constrained environments.

