---
layout: default
title: "JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings"
---

# JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02366" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02366v2</a>
  <a href="https://arxiv.org/pdf/2505.02366.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02366v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02366v2', 'JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-05-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºJTCSEæ¡†æ¶ä»¥å¢å¼ºæ— ç›‘ç£å¯¹æ¯”å­¦ä¹ çš„å¥å­åµŒå…¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `å¥å­åµŒå…¥` `æ¨¡çº¦æŸ` `äº¤å‰æ³¨æ„åŠ›` `è‡ªç„¶è¯­è¨€å¤„ç†` `è¯­ä¹‰ç›¸ä¼¼æ€§` `BERTæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¿½è§†äº†è¯­ä¹‰è¡¨ç¤ºå¼ é‡çš„æ¨¡ç‰¹å¾ï¼Œå¯¼è‡´å¯¹æ¯”å­¦ä¹ æ•ˆæœä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºJTCSEæ¡†æ¶ï¼Œé€šè¿‡æ¨¡çº¦æŸå’Œäº¤å‰æ³¨æ„åŠ›ç»“æ„å¢å¼ºæ­£æ ·æœ¬å¯¹é½å’ŒCLSæ ‡è®°çš„æ³¨æ„åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒJTCSEåœ¨ä¸ƒä¸ªè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ä»»åŠ¡ä¸­è¶…è¶Šå…¶ä»–åŸºçº¿ï¼Œå¹¶åœ¨130å¤šä¸ªé›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„çƒ­é—¨ç ”ç©¶ä¸»é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å…³æ³¨äºçº¦æŸæ­£è´Ÿæ ·æœ¬åœ¨é«˜ç»´è¯­ä¹‰ç©ºé—´ä¸­çš„è¡¨ç¤ºæ–¹å‘åˆ†å¸ƒï¼Œä½†å¿½è§†äº†è¯­ä¹‰è¡¨ç¤ºå¼ é‡çš„æ¨¡ç‰¹å¾ï¼Œå¯¼è‡´å¯¹æ¯”å­¦ä¹ æ•ˆæœä¸è¶³ã€‚å› æ­¤ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºäº†ä¸€ç§è®­ç»ƒç›®æ ‡ï¼Œæ—¨åœ¨å¯¹è¯­ä¹‰è¡¨ç¤ºå¼ é‡æ–½åŠ æ¨¡çº¦æŸï¼Œä»¥å¢å¼ºæ­£æ ·æœ¬ä¹‹é—´çš„å¯¹é½ã€‚åŒæ—¶ï¼Œé’ˆå¯¹BERTç±»æ¨¡å‹ä¸­CLSæ ‡è®°çš„æ³¨æ„åŠ›ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†åŒå¡”æ¨¡å‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›ç»“æ„ï¼Œä»¥ä¼˜åŒ–CLSæ± åŒ–çš„è´¨é‡ã€‚ç»“åˆè¿™ä¸¤æ–¹é¢ï¼Œæå‡ºäº†JTCSEæ¡†æ¶ï¼Œå¹¶åœ¨ä¸ƒä¸ªè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§è®¡ç®—ä»»åŠ¡ä¸­è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶åŒå¡”é›†æˆæ¨¡å‹å’Œå•å¡”è’¸é¦æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šå…¶ä»–åŸºçº¿ï¼Œæˆä¸ºå½“å‰çš„SOTAã€‚æ­¤å¤–ï¼ŒJTCSEåœ¨130å¤šä¸ªé›¶æ ·æœ¬ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä¸­å¯¹è¯­ä¹‰è¡¨ç¤ºå¼ é‡æ¨¡ç‰¹å¾çš„å¿½è§†ï¼Œå¯¼è‡´æ­£è´Ÿæ ·æœ¬å¯¹é½ä¸è¶³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡æ–½åŠ æ¨¡çº¦æŸæ¥å¢å¼ºæ­£æ ·æœ¬ä¹‹é—´çš„å¯¹é½ï¼ŒåŒæ—¶å¼•å…¥äº¤å‰æ³¨æ„åŠ›ç»“æ„ä»¥ä¼˜åŒ–CLSæ ‡è®°çš„æ³¨æ„åŠ›åˆ†é…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šJTCSEæ¡†æ¶ç”±åŒå¡”æ¨¡å‹å’Œäº¤å‰æ³¨æ„åŠ›ç»“æ„ç»„æˆï¼ŒåŒå¡”æ¨¡å‹ç”¨äºç”Ÿæˆå¥å­åµŒå…¥ï¼Œäº¤å‰æ³¨æ„åŠ›æ¨¡å—å¢å¼ºäº†å¯¹CLSæ ‡è®°çš„å…³æ³¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåŒæ—¶è€ƒè™‘æ¨¡çº¦æŸå’Œäº¤å‰æ³¨æ„åŠ›ï¼Œæ˜¾è‘—æå‡äº†å¯¹æ¯”å­¦ä¹ çš„æ•ˆæœï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¯­ä¹‰ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥æ¨¡çº¦æŸé¡¹ï¼Œè®¾è®¡äº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä»¥ä¼˜åŒ–CLSæ± åŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå…³æ³¨é‡è¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒJTCSEçš„åŒå¡”é›†æˆæ¨¡å‹å’Œå•å¡”è’¸é¦æ¨¡å‹åœ¨ä¸ƒä¸ªè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ä»»åŠ¡ä¸­å‡è¶…è¶Šäº†å…¶ä»–åŸºçº¿ï¼Œæˆä¸ºå½“å‰çš„SOTAã€‚æ­¤å¤–ï¼Œåœ¨130å¤šä¸ªé›¶æ ·æœ¬ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒJTCSEæ•´ä½“è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–‡æœ¬ç›¸ä¼¼æ€§è®¡ç®—ã€ä¿¡æ¯æ£€ç´¢å’Œè‡ªç„¶è¯­è¨€ç†è§£ç­‰ã€‚é€šè¿‡æå‡å¥å­åµŒå…¥çš„è´¨é‡ï¼ŒJTCSEèƒ½å¤Ÿä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡æä¾›æ›´å¼ºçš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \textbf{J}oint \textbf{T}ensor representation modulus constraint and \textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence \textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.

