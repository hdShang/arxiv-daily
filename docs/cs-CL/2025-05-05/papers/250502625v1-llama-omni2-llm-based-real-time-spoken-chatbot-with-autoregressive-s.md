---
layout: default
title: "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis"
---

# LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02625" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02625v1</a>
  <a href="https://arxiv.org/pdf/2505.02625.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02625v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02625v1', 'LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, Yang Feng

**åˆ†ç±»**: cs.CL, cs.AI, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: Preprint. Project: https://github.com/ictnlp/LLaMA-Omni2

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLaMA-Omni2ä»¥å®ç°å®æ—¶æ™ºèƒ½è¯­éŸ³èŠå¤©æœºå™¨äºº**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³äº¤äº’` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªå›å½’è§£ç ` `æ™ºèƒ½èŠå¤©æœºå™¨äºº` `å®æ—¶è¯­éŸ³åˆæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­éŸ³èŠå¤©æœºå™¨äººåœ¨å®æ—¶æ€§å’Œè‡ªç„¶äº¤äº’æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ã€‚
2. LLaMA-Omni 2é€šè¿‡é›†æˆè¯­éŸ³ç¼–ç å™¨å’Œè‡ªå›å½’æµå¼è¯­éŸ³è§£ç å™¨ï¼Œæä¾›é«˜æ•ˆçš„å®æ—¶è¯­éŸ³äº¤äº’è§£å†³æ–¹æ¡ˆã€‚
3. åœ¨ä»…ä½¿ç”¨20ä¸‡å¤šè½®å¯¹è¯æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒLLaMA-Omni 2åœ¨è¯­éŸ³é—®ç­”å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­è¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®æ—¶ã€æ™ºèƒ½å’Œè‡ªç„¶çš„è¯­éŸ³äº¤äº’æ˜¯ä¸‹ä¸€ä»£äººæœºäº¤äº’çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚è¿‘æœŸçš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ„å»ºæ™ºèƒ½è¯­éŸ³èŠå¤©æœºå™¨äººçš„æ½œåŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†LLaMA-Omni 2ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å‚æ•°ä»0.5Båˆ°14Bçš„è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMsï¼‰ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„å®æ—¶è¯­éŸ³äº¤äº’ã€‚LLaMA-Omni 2åŸºäºQwen2.5ç³»åˆ—æ¨¡å‹ï¼Œé›†æˆäº†è¯­éŸ³ç¼–ç å™¨å’Œè‡ªå›å½’æµå¼è¯­éŸ³è§£ç å™¨ã€‚å°½ç®¡ä»…åœ¨20ä¸‡å¤šè½®è¯­éŸ³å¯¹è¯æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒLLaMA-Omni 2åœ¨å¤šä¸ªè¯­éŸ³é—®ç­”å’Œè¯­éŸ³æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä»¥æ•°ç™¾ä¸‡å°æ—¶è¯­éŸ³æ•°æ®è®­ç»ƒçš„GLM-4-Voiceç­‰ç°æœ‰æœ€å…ˆè¿›çš„SpeechLMsã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­éŸ³èŠå¤©æœºå™¨äººåœ¨å®æ—¶æ€§å’Œè‡ªç„¶äº¤äº’æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•å®ç°é«˜æ•ˆçš„è¯­éŸ³äº¤äº’ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLLaMA-Omni 2é€šè¿‡ç»“åˆè¯­éŸ³ç¼–ç å™¨ä¸è‡ªå›å½’æµå¼è§£ç å™¨ï¼Œä¼˜åŒ–äº†è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ä¸‹ä»ç„¶å®ç°é«˜è´¨é‡çš„è¯­éŸ³äº¤äº’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLLaMA-Omni 2çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¯­éŸ³ç¼–ç å™¨å’Œè‡ªå›å½’è§£ç å™¨ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼Œå‰è€…è´Ÿè´£å°†è¾“å…¥è¯­éŸ³è½¬æ¢ä¸ºç‰¹å¾è¡¨ç¤ºï¼Œåè€…åˆ™ç”Ÿæˆç›¸åº”çš„è¯­éŸ³è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šLLaMA-Omni 2çš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶åœ¨è¾ƒå°‘çš„è®­ç»ƒæ ·æœ¬ä¸‹ä»èƒ½è¶…è¶Šä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹çš„å¼ºå¤§æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è¯­éŸ³é—®ç­”å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹å‚æ•°è®¾ç½®ä»0.5Båˆ°14Bä¸ç­‰ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°å’Œä¼˜åŒ–çš„ç½‘ç»œç»“æ„ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LLaMA-Omni 2åœ¨å¤šä¸ªè¯­éŸ³é—®ç­”å’ŒæŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†GLM-4-Voiceç­‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨ä»…ä½¿ç”¨20ä¸‡å¤šè½®å¯¹è¯æ ·æœ¬ä¸‹çš„å¼ºå¤§æ€§èƒ½ï¼Œä½“ç°äº†å…¶åœ¨è¯­éŸ³äº¤äº’é¢†åŸŸçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LLaMA-Omni 2åœ¨æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€å®¢æˆ·æœåŠ¡ã€æ•™è‚²åŸ¹è®­ç­‰å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„å®æ—¶è¯­éŸ³äº¤äº’èƒ½åŠ›èƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è‡ªç„¶åŒ–è¿›ç¨‹ï¼Œæœªæ¥å¯èƒ½ä¼šåœ¨æ›´å¤šæ™ºèƒ½è®¾å¤‡ä¸­å¾—åˆ°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.

