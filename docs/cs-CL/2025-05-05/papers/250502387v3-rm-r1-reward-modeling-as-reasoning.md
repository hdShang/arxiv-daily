---
layout: default
title: RM-R1: Reward Modeling as Reasoning
---

# RM-R1: Reward Modeling as Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02387" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02387v3</a>
  <a href="https://arxiv.org/pdf/2505.02387.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02387v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02387v3', 'RM-R1: Reward Modeling as Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-05-18)

**å¤‡æ³¨**: 25 pages, 8 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/RM-R1-UIUC/RM-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¨ç†çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ä»¥æå‡æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±å»ºæ¨¡` `æ¨ç†èƒ½åŠ›` `å¼ºåŒ–å­¦ä¹ ` `å¯è§£é‡Šæ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•åœ¨æä¾›å‡†ç¡®çš„å¥–åŠ±ä¿¡å·å’Œå¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆå¯¹é½æ¨¡å‹ä¸äººç±»åå¥½ã€‚
2. æœ¬æ–‡æå‡ºäº†æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºæ¨ç†ä»»åŠ¡ï¼Œè®¾è®¡äº†é“¾å¼è¯„åˆ†æœºåˆ¶ä»¥å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚
3. RM-R1åœ¨å¤šä¸ªå¥–åŠ±æ¨¡å‹åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹³å‡æ€§èƒ½è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°4.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±å»ºæ¨¡å¯¹äºé€šè¿‡äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ¥å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½è‡³å…³é‡è¦ã€‚ä¸ºäº†æä¾›å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œå¥–åŠ±æ¨¡å‹åº”åœ¨èµ‹åˆ†ä¹‹å‰è¿›è¡Œæ·±åº¦æ€è€ƒå’Œå¯è§£é‡Šæ¨ç†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºæ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé¢å‘æ¨ç†çš„è®­ç»ƒæµç¨‹ï¼Œå¹¶è®­ç»ƒäº†ä¸€ç³»åˆ—ReasRMsï¼ŒRM-R1ã€‚RM-R1é‡‡ç”¨äº†é“¾å¼è¯„åˆ†æœºåˆ¶ï¼Œé€šè¿‡è‡ªç”Ÿæˆæ ·æœ¬çº§åˆ«çš„è¯„åˆ†æ ‡å‡†æ¥è¯„ä¼°å€™é€‰å“åº”ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸‰ä¸ªå¥–åŠ±æ¨¡å‹åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„å¼€æ”¾æƒé‡æ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ï¼Œæå‡å¹…åº¦å¯è¾¾4.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±å»ºæ¨¡æ–¹æ³•åœ¨å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºæ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡å¼•å…¥æ¨ç†èƒ½åŠ›æ¥å¢å¼ºå¥–åŠ±æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ï¼Œè®¾è®¡äº†é“¾å¼è¯„åˆ†æœºåˆ¶ä»¥è‡ªç”Ÿæˆè¯„åˆ†æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRM-R1çš„è®­ç»ƒæµç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºé«˜è´¨é‡æ¨ç†é“¾çš„è’¸é¦ï¼Œç¬¬äºŒé˜¶æ®µä¸ºä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚æ•´ä½“æ¶æ„åŒ…æ‹¬æ¨ç†é“¾ç”Ÿæˆå’Œå€™é€‰å“åº”è¯„ä¼°ä¸¤ä¸ªæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¥–åŠ±å»ºæ¨¡ä¸æ¨ç†ç»“åˆï¼Œå½¢æˆäº†æ–°çš„æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é“¾å¼è¯„åˆ†æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹è‡ªç”Ÿæˆæ ·æœ¬çº§åˆ«çš„è¯„åˆ†æ ‡å‡†ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œç¡®ä¿å¥–åŠ±ä¿¡å·çš„å‡†ç¡®æ€§å’Œå¯éªŒè¯æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRM-R1åœ¨ä¸‰ä¸ªå¥–åŠ±æ¨¡å‹åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„å¼€æ”¾æƒé‡æ¨¡å‹ï¼ˆå¦‚INF-ORM-Llama3.1-70Bï¼‰å’Œä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰è¾¾4.9%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¥–åŠ±å»ºæ¨¡çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ï¼Œè¿›è€Œæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward modeling is essential for aligning large language models with human preferences through reinforcement learning from human feedback. To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. Inspired by recent advances of long chain-of-thought on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMs interpretability and performance. To this end, we introduce a new class of generative reward models - Reasoning Reward Models (ReasRMs) - which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism - self-generating sample-level chat rubrics or math/code solutions, and evaluating candidate responses against them. The training of RM-R1 consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. Empirically, our models achieve state-of-the-art performance across three reward model benchmarks on average, outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough empirical analyses to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six REASRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.

