---
layout: default
title: "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning"
---

# EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02579" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02579v3</a>
  <a href="https://arxiv.org/pdf/2505.02579.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02579v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02579v3', 'EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingxiao Kong, Cong Yang, Susanne Neufang, Oya Deniz Beyan, Zeyd Boukhers

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-07-09)

**å¤‡æ³¨**: 14 pages, 9 figures, accepted by the SIGDIAL 2025 conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEMORLæ¡†æ¶ä»¥è§£å†³å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ä¸çµæ´»æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `é›†æˆå­¦ä¹ ` `å¾®è°ƒ` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç›®æ ‡å¹³è¡¡ã€è®­ç»ƒæ•ˆç‡å’Œå¯è§£é‡Šæ€§ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚
2. EMORLæ¡†æ¶é€šè¿‡é›†æˆå­¦ä¹ åŸç†ï¼Œä¼˜åŒ–å¤šä¸ªæ¨¡å‹çš„å¾®è°ƒå’Œèšåˆè¿‡ç¨‹ï¼Œä»¥æé«˜æ•ˆç‡å’Œçµæ´»æ€§ã€‚
3. åœ¨PAIRå’ŒPsych8kæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEMORLåœ¨è®­ç»ƒæ¶ˆè€—å’Œå¯æ‰©å±•æ€§æ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿ï¼Œä¸”æ€§èƒ½ç¨³å®šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„åº”ç”¨å±•ç°å‡ºè§£å†³å¤šç›®æ ‡ä»»åŠ¡çš„æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ç›®æ ‡å¹³è¡¡ã€è®­ç»ƒæ•ˆç‡ä½ã€å¯æ‰©å±•æ€§å·®å’Œå¯è§£é‡Šæ€§æœ‰é™ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é›†æˆå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶EMORLï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–å¤šä¸ªæ¨¡å‹çš„èšåˆæ¥æé«˜å¾®è°ƒçš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚è¯¥æ–¹æ³•é¦–æ¬¡èšåˆäº†ä¸ªä½“æ¨¡å‹çš„éšè—çŠ¶æ€ï¼Œç»“åˆäº†å¤šä¸ªç›®æ ‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶é€šè¿‡åˆ†å±‚ç½‘æ ¼æœç´¢ç®—æ³•è¯†åˆ«æœ€ä½³åŠ æƒç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEMORLåœ¨é¡¾é—®åæ€ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„è®­ç»ƒæ¶ˆè€—é™ä½å’Œæ›´ç¨³å®šçš„æ€§èƒ½ï¼Œä¸”åœ¨å¤šä¸ªç›®æ ‡ä¸Šçš„è¡¨ç°å¯æ¯”ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„æ•ˆç‡ä¸çµæ´»æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç›®æ ‡å¹³è¡¡ã€è®­ç»ƒæ•ˆç‡å’Œå¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEMORLæ¡†æ¶é€šè¿‡é›†æˆå­¦ä¹ çš„æ–¹å¼ï¼Œå¾®è°ƒå¤šä¸ªæ¨¡å‹å¹¶åœ¨å¾®è°ƒåä¼˜åŒ–å®ƒä»¬çš„èšåˆï¼Œä»¥æé«˜æ•´ä½“æ€§èƒ½å’Œçµæ´»æ€§ã€‚è¯¥è®¾è®¡æ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨å¤šä¸ªç›®æ ‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEMORLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å‹çš„ç‹¬ç«‹å¾®è°ƒé˜¶æ®µå’Œåç»­çš„èšåˆé˜¶æ®µã€‚é€šè¿‡åˆ†å±‚ç½‘æ ¼æœç´¢ç®—æ³•ï¼Œè¯†åˆ«æœ€ä½³çš„åŠ æƒç»„åˆä»¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šEMORLçš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡èšåˆä¸ªä½“æ¨¡å‹çš„éšè—çŠ¶æ€ï¼Œç»“åˆå¤šä¸ªç›®æ ‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åˆ†å±‚ç½‘æ ¼æœç´¢ç®—æ³•æ¥ç¡®å®šåŠ æƒç»„åˆï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å¤šç›®æ ‡çš„å¹³è¡¡ï¼Œç½‘ç»œç»“æ„åˆ™æ”¯æŒéšè—çŠ¶æ€çš„æœ‰æ•ˆèšåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EMORLåœ¨é¡¾é—®åæ€ç”Ÿæˆä»»åŠ¡ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè®­ç»ƒæ¶ˆè€—æ˜¾è‘—é™ä½è‡³$17,529	ext{Â±}1,650$æ•°æ®ç‚¹å’Œ$6,573	ext{Â±}147.43$ç§’ï¼Œä¸”åœ¨å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½åœ¨å¤šä¸ªç›®æ ‡ä¸Šä¸ç°æœ‰åŸºçº¿ç›¸å½“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EMORLæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤„ç†å¤šç›®æ ‡ä»»åŠ¡çš„é¢†åŸŸï¼Œå¦‚æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€ä¸ªæ€§åŒ–æ¨èå’Œæ•™è‚²è¾…å¯¼ç­‰ã€‚å…¶æé«˜çš„è®­ç»ƒæ•ˆç‡å’Œçµæ´»æ€§å°†æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è½åœ°ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including competing objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the fine-tuning to improve efficiency and flexibility. Our method is the first to aggregate the hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text classification models to score the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.

