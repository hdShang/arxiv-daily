---
layout: default
title: Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards
---

# Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02686" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02686v2</a>
  <a href="https://arxiv.org/pdf/2505.02686.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02686v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02686v2', 'Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaobao Wu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-06-12)

**å¤‡æ³¨**: 36 Pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/bobxwu/learning-from-rewards-llm-papers)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¥–åŠ±æ¨¡å‹ä¸å­¦ä¹ ç­–ç•¥ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `åŠ¨æ€åé¦ˆ` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `æ¨¡å‹ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é™æ€æ•°æ®å­¦ä¹ ä¸­ç¼ºä¹çµæ´»æ€§ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨åŠ¨æ€åé¦ˆè¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¥–åŠ±ä¿¡å·å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œè½¬å˜ä¸ºä¸»åŠ¨å­¦ä¹ ï¼Œæå‡æ¨¡å‹çš„é€‚åº”æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨å¥–åŠ±æ¨¡å‹çš„å­¦ä¹ ç­–ç•¥æ˜¾è‘—æå‡äº†LLMåœ¨å¤šä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ä»é¢„è®­ç»ƒæ‰©å±•åˆ°åæœŸè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´æ‰©å±•ã€‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ï¼Œå‡ºç°äº†ä¸€ä¸ªå…³é”®çš„ç»Ÿä¸€èŒƒå¼ï¼šä»å¥–åŠ±ä¸­å­¦ä¹ ï¼Œå…¶ä¸­å¥–åŠ±ä¿¡å·ä½œä¸ºå¼•å¯¼æ˜Ÿï¼ŒæŒ‡å¼•LLMçš„è¡Œä¸ºã€‚è¯¥èŒƒå¼æ”¯æŒäº†ä¸€ç³»åˆ—æµè¡ŒæŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFã€RLAIFã€DPOå’ŒGRPOï¼‰ã€å¥–åŠ±å¼•å¯¼è§£ç å’ŒåæœŸä¿®æ­£ã€‚å®ƒä½¿å¾—ä»é™æ€æ•°æ®çš„è¢«åŠ¨å­¦ä¹ è½¬å˜ä¸ºä»åŠ¨æ€åé¦ˆçš„ä¸»åŠ¨å­¦ä¹ ï¼Œä»è€Œèµ‹äºˆLLMå¯¹å¤šç§ä»»åŠ¡çš„åå¥½å¯¹é½å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†ä»å¥–åŠ±ä¸­å­¦ä¹ çš„å†…å®¹ï¼Œæ¶µç›–äº†è®­ç»ƒã€æ¨ç†å’Œåæ¨ç†é˜¶æ®µçš„å¥–åŠ±æ¨¡å‹å’Œå­¦ä¹ ç­–ç•¥ï¼Œå¹¶è®¨è®ºäº†å¥–åŠ±æ¨¡å‹çš„åŸºå‡†å’Œä¸»è¦åº”ç”¨ï¼Œæœ€åå¼ºè°ƒäº†æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¥–åŠ±ä¿¡å·æ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚ç°æœ‰æ–¹æ³•åœ¨é™æ€æ•°æ®å­¦ä¹ ä¸­å­˜åœ¨çµæ´»æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨åŠ¨æ€åé¦ˆè¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡å¥–åŠ±ä¿¡å·å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œè½¬å˜ä¸ºä¸»åŠ¨å­¦ä¹ ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å®æ—¶åé¦ˆè°ƒæ•´å…¶è¡Œä¸ºï¼Œä»è€Œæå‡é€‚åº”æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šè®­ç»ƒé˜¶æ®µã€æ¨ç†é˜¶æ®µå’Œåæ¨ç†é˜¶æ®µã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡å¥–åŠ±ä¿¡å·è¿›è¡Œä¼˜åŒ–ï¼›åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹åˆ©ç”¨å¥–åŠ±å¼•å¯¼è§£ç ï¼›åœ¨åæ¨ç†é˜¶æ®µï¼Œè¿›è¡ŒåæœŸä¿®æ­£ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¥–åŠ±å­¦ä¹ èŒƒå¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¤šç§å­¦ä¹ ç­–ç•¥ï¼ˆå¦‚RLHFã€RLAIFç­‰ï¼‰ï¼Œä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶åŠ¨æ€åé¦ˆçš„åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼Œè®ºæ–‡è¯¦ç»†è®¨è®ºäº†å¥–åŠ±æ¨¡å‹çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠç½‘ç»œç»“æ„çš„è®¾è®¡ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä»»åŠ¡ç¯å¢ƒä¸­ä¿æŒé«˜æ•ˆçš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å¥–åŠ±æ¨¡å‹çš„å­¦ä¹ ç­–ç•¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†LLMçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†15%ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿã€æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾ï¼Œæä¾›æ›´ä¸ºç²¾å‡†çš„å“åº”ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€å®¢æœç­‰å¤šä¸ªè¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (RLHF, RLAIF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities for diverse tasks. In this survey, we present a comprehensive overview of learning from rewards, from the perspective of reward models and learning strategies across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.

