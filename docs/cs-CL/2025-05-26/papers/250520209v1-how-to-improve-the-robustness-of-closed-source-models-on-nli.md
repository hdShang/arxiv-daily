---
layout: default
title: How to Improve the Robustness of Closed-Source Models on NLI
---

# How to Improve the Robustness of Closed-Source Models on NLI

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20209" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20209v1</a>
  <a href="https://arxiv.org/pdf/2505.20209.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20209v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20209v1', 'How to Improve the Robustness of Closed-Source Models on NLI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Joe Stacey, Lisa Alazraki, Aran Ubhi, Beyza Ermis, Aaron Mueller, Marek Rei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®ä¸­å¿ƒæ–¹æ³•ä»¥æå‡é—­æºæ¨¡å‹åœ¨NLIä»»åŠ¡ä¸­çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é—­æºæ¨¡å‹` `é²æ£’æ€§æå‡` `è‡ªç„¶è¯­è¨€æ¨ç†` `æ•°æ®ä¸­å¿ƒæ–¹æ³•` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æå‡é—­æºæ¨¡å‹é²æ£’æ€§æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦è®¿é—®æ¨¡å‹å†…éƒ¨æˆ–ä¿®æ”¹è®­ç»ƒè¿‡ç¨‹ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ•°æ®ä¸­å¿ƒçš„æ–¹æ³•æ¥æå‡é—­æºLLMsçš„é²æ£’æ€§ï¼Œé¿å…å¯¹æ¨¡å‹å†…éƒ¨çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹å¤æ‚OODæ•°æ®é›†ï¼Œå¢åŠ æŒ‘æˆ˜æ€§è®­ç»ƒæ ·æœ¬å¯æå‡é²æ£’æ€§1.5%ï¼›è€Œç®€å•æ•°æ®é›†ç”¨LLMç”Ÿæˆæ ·æœ¬æ›¿æ¢å¯æå‡3.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é—­æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®æ—¶é²æ£’æ€§ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¯¹æ¨¡å‹å†…éƒ¨çš„è®¿é—®æˆ–è®­ç»ƒè¿‡ç¨‹çš„ä¿®æ”¹ï¼Œæ— æ³•é€‚ç”¨äºé—­æºæ¨¡å‹ã€‚æœ¬æ–‡æ¢è®¨äº†æ— éœ€è®¿é—®æ¨¡å‹å†…éƒ¨çš„åŸºäºæ•°æ®çš„æ–¹æ³•æ¥æå‡é—­æºLLMsçš„é²æ£’æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œé’ˆå¯¹å¤æ‚çš„OODæ•°æ®é›†ï¼Œé€šè¿‡å¢åŠ æ›´å…·æŒ‘æˆ˜æ€§çš„è®­ç»ƒæ ·æœ¬å¯ä»¥æå‡é²æ£’æ€§1.5%ï¼›è€Œå¯¹äºè¾ƒç®€å•çš„OODæ•°æ®é›†ï¼Œç”¨LLMç”Ÿæˆçš„æ ·æœ¬æ›¿æ¢éƒ¨åˆ†è®­ç»ƒé›†åˆ™å¯æå‡é²æ£’æ€§3.7%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜ï¼Œé—­æºè‡ªå›å½’LLMsçš„é²æ£’æ€§æ˜¾è‘—ä¼˜äºå¸¸ç”¨çš„ç¼–ç å™¨æ¨¡å‹ï¼Œæœªæ¥åº”ä½œä¸ºåŸºå‡†é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é—­æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®æ—¶é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯¹æ¨¡å‹å†…éƒ¨çš„è®¿é—®æˆ–ä¿®æ”¹è®­ç»ƒè¿‡ç¨‹ï¼Œå¯¼è‡´æ— æ³•åº”ç”¨äºé—­æºæ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ•°æ®ä¸­å¿ƒçš„æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´è®­ç»ƒæ•°æ®é›†æ¥æå‡æ¨¡å‹çš„é²æ£’æ€§ï¼Œè€Œä¸éœ€è¦å¯¹æ¨¡å‹å†…éƒ¨è¿›è¡Œä¿®æ”¹ã€‚è¿™ç§æ–¹æ³•çš„è®¾è®¡åŸºäºå¯¹OODæ•°æ®å¤æ‚æ€§çš„åˆ†æã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„åˆ†æã€æ ·æœ¬é€‰æ‹©å’Œç”Ÿæˆç­–ç•¥ã€‚é¦–å…ˆï¼Œè¯„ä¼°OODæ•°æ®é›†çš„å¤æ‚æ€§ï¼Œç„¶åæ ¹æ®å¤æ‚æ€§é€‰æ‹©åˆé€‚çš„è®­ç»ƒæ ·æœ¬è°ƒæ•´ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é’ˆå¯¹ä¸åŒå¤æ‚æ€§OODæ•°æ®é›†çš„ç‰¹å®šç­–ç•¥ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¢åŠ æŒ‘æˆ˜æ€§æ ·æœ¬æˆ–ç”¨ç”Ÿæˆæ ·æœ¬æ›¿æ¢è®­ç»ƒé›†çš„æ–¹å¼ï¼Œæ˜¾è‘—æå‡äº†é—­æºæ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé’ˆå¯¹å¤æ‚æ•°æ®é›†é‡‡ç”¨äº†ä¸Šé‡‡æ ·ç­–ç•¥ï¼Œè€Œå¯¹äºç®€å•æ•°æ®é›†åˆ™ä½¿ç”¨äº†LLMç”Ÿæˆæ ·æœ¬æ›¿æ¢çš„ç­–ç•¥ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒè®ºæ–‡çš„å®Œæ•´å†…å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹å¤æ‚çš„OODæ•°æ®é›†ï¼Œé€šè¿‡å¢åŠ æŒ‘æˆ˜æ€§è®­ç»ƒæ ·æœ¬ï¼Œæ¨¡å‹é²æ£’æ€§æå‡äº†1.5%ï¼›è€Œåœ¨å¤„ç†ç®€å•OODæ•°æ®é›†æ—¶ï¼Œç”¨LLMç”Ÿæˆçš„æ ·æœ¬æ›¿æ¢éƒ¨åˆ†è®­ç»ƒé›†ï¼Œé²æ£’æ€§æå‡è¾¾3.7%ã€‚æ­¤å¤–ï¼Œé—­æºè‡ªå›å½’LLMsçš„é²æ£’æ€§æ˜¾è‘—ä¼˜äºä¼ ç»Ÿç¼–ç å™¨æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºåŸºå‡†é€‰æ‹©çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å„ç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡é—­æºæ¨¡å‹çš„é²æ£’æ€§ï¼Œå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æ›´å¥½åœ°å¤„ç†å¤šæ ·åŒ–å’Œå¤æ‚çš„è¾“å…¥æ•°æ®ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¿™ä¸€æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šé—­æºæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›ä½¿ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Closed-source Large Language Models (LLMs) have become increasingly popular, with impressive performance across a wide range of natural language tasks. These models can be fine-tuned to further improve performance, but this often results in the models learning from dataset-specific heuristics that reduce their robustness on out-of-distribution (OOD) data. Existing methods to improve robustness either perform poorly, or are non-applicable to closed-source models because they assume access to model internals, or the ability to change the model's training procedure. In this work, we investigate strategies to improve the robustness of closed-source LLMs through data-centric methods that do not require access to model internals. We find that the optimal strategy depends on the complexity of the OOD data. For highly complex OOD datasets, upsampling more challenging training examples can improve robustness by up to 1.5%. For less complex OOD datasets, replacing a portion of the training set with LLM-generated examples can improve robustness by 3.7%. More broadly, we find that large-scale closed-source autoregressive LLMs are substantially more robust than commonly used encoder models, and are a more appropriate choice of baseline going forward.

