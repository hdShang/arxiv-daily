---
layout: default
title: "UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models"
---

# UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20154" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20154v1</a>
  <a href="https://arxiv.org/pdf/2505.20154.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20154v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20154v1', 'UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xueyan Zhang, Jinman Zhao, Zhifei Yang, Yibo Zhong, Shuhao Guan, Linbo Cao, Yining Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: 20 pages, 2 figures, 15 tables

**æœŸåˆŠ**: ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUORAä»¥å®ç°å¤§æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä½ç§©è¿‘ä¼¼` `æ’å€¼é‡å‚æ•°åŒ–` `è®¡ç®—æ•ˆç‡` `å­˜å‚¨æ•ˆç‡` `å¾®è°ƒæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¾®è°ƒæ–¹æ³•åœ¨å‚æ•°æ•ˆç‡å’Œè®¡ç®—å¼€é”€ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡æ¨¡å‹çš„éœ€æ±‚ã€‚
2. UORAé€šè¿‡ä½ç§©è¿‘ä¼¼å’Œæ’å€¼é‡å‚æ•°åŒ–æœºåˆ¶ï¼Œé€‰æ‹©æ€§åœ°é‡åˆå§‹åŒ–æŠ•å½±çŸ©é˜µçš„éƒ¨åˆ†å‚æ•°ï¼Œæ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°ã€‚
3. åœ¨GLUEå’ŒE2EåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUORAçš„å¾®è°ƒæ€§èƒ½ä¼˜äºLoRAå’ŒVeRAï¼Œä¸”è®¡ç®—å¼€é”€æä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•â€”â€”å‡åŒ€æ­£äº¤é‡åˆå§‹åŒ–é€‚åº”ï¼ˆUORAï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¾®è°ƒè¿‡ç¨‹ã€‚UORAé€šè¿‡ä½ç§©è¿‘ä¼¼æ–¹æ³•å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œæ˜¾è‘—æå‡äº†å‚æ•°æ•ˆç‡å’Œæ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•å¦‚LoRAå’ŒVeRAä¸åŒï¼ŒUORAé‡‡ç”¨åŸºäºæ’å€¼çš„é‡å‚æ•°åŒ–æœºåˆ¶ï¼Œé€‰æ‹©æ€§åœ°é‡åˆå§‹åŒ–å†»ç»“æŠ•å½±çŸ©é˜µä¸­çš„è¡Œå’Œåˆ—ï¼Œåˆ©ç”¨å‘é‡å¹…åº¦å¯å‘å¼æŒ‡å¯¼ã€‚è¿™ä¸€æ–¹æ³•åœ¨è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ä¸Šä¼˜äºVeRAï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†ç«äº‰åŠ›çš„å¾®è°ƒæ€§èƒ½ï¼Œå‡ ä¹æ²¡æœ‰è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUORAåœ¨GLUEå’ŒE2EåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾åƒåˆ†ç±»æ¨¡å‹æŒ‡ä»¤å¾®è°ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­çš„å‚æ•°æ•ˆç‡ä½å’Œè®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚LoRAå’ŒVeRAåœ¨è¿™æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„å¾®è°ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUORAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä½ç§©è¿‘ä¼¼å’Œæ’å€¼é‡å‚æ•°åŒ–æœºåˆ¶ï¼Œé€‰æ‹©æ€§åœ°é‡åˆå§‹åŒ–å†»ç»“æŠ•å½±çŸ©é˜µä¸­çš„è¡Œå’Œåˆ—ï¼Œä»è€Œå‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚è¿™ç§è®¾è®¡ä½¿å¾—å¾®è°ƒè¿‡ç¨‹æ›´åŠ é«˜æ•ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUORAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€æ¨¡å‹åˆå§‹åŒ–ã€é‡å‚æ•°åŒ–ã€å¾®è°ƒè¿‡ç¨‹å’Œè¯„ä¼°é˜¶æ®µã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ä½ç§©è¿‘ä¼¼æ¨¡å—å’Œæ’å€¼é‡å‚æ•°åŒ–æ¨¡å—ï¼Œç¡®ä¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆåˆ©ç”¨å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šUORAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ’å€¼é‡å‚æ•°åŒ–æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å‘é‡å¹…åº¦å¯å‘å¼é€‰æ‹©æ€§åœ°é‡åˆå§‹åŒ–å‚æ•°ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å…¨å‚æ•°å¾®è°ƒæˆ–ç®€å•çš„ä½ç§©è¿‘ä¼¼æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæ˜¾è‘—æé«˜äº†å‚æ•°æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒUORAé‡‡ç”¨äº†ä½ç§©çŸ©é˜µåˆ†è§£æŠ€æœ¯ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å¾®è°ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™ä¿æŒäº†åŸæ¨¡å‹çš„å®Œæ•´æ€§ï¼Œç¡®ä¿å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½ä¼˜è¶Šã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UORAåœ¨GLUEå’ŒE2EåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¾®è°ƒæ€§èƒ½è¶…è¿‡LoRAï¼Œä¸”åœ¨è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ä¸Šä¼˜äºVeRAï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚å…·ä½“å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUORAåœ¨ç›¸åŒæ¡ä»¶ä¸‹å‡å°‘äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œæå‡äº†å¾®è°ƒæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UORAçš„ç ”ç©¶æˆæœåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå›¾åƒåˆ†ç±»ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨å’Œä½è®¡ç®—å¼€é”€ä½¿å¾—åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½å®ç°é«˜æ€§èƒ½çš„æ¨¡å‹å¾®è°ƒï¼Œæ¨åŠ¨äº†AIæŠ€æœ¯çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA), a novel parameter-efficient fine-tuning (PEFT) approach for Large Language Models (LLMs). UORA achieves state-of-the-art performance and parameter efficiency by leveraging a low-rank approximation method to reduce the number of trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA employs an interpolation-based reparametrization mechanism that selectively reinitializes rows and columns in frozen projection matrices, guided by the vector magnitude heuristic. This results in substantially fewer trainable parameters compared to LoRA and outperforms VeRA in computation and storage efficiency. Comprehensive experiments across various benchmarks demonstrate UORA's superiority in achieving competitive fine-tuning performance with negligible computational overhead. We demonstrate its performance on GLUE and E2E benchmarks and its effectiveness in instruction-tuning large language models and image classification models. Our contributions establish a new paradigm for scalable and resource-efficient fine-tuning of LLMs.

