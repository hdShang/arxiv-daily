---
layout: default
title: Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking
---

# Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20023" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20023v1</a>
  <a href="https://arxiv.org/pdf/2505.20023.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20023v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20023v1', 'Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yihan Chen, Benfeng Xu, Xiaorui Wang, Yongdong Zhang, Zhendong Mao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTePæ–¹æ³•ä»¥è§£å†³LLMä»£ç†è®­ç»ƒä¸­çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªä¸»ä»£ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘åæ€` `éƒ¨åˆ†æ©è”½` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä½¿ç”¨ä¸“å®¶è½¨è¿¹è®­ç»ƒLLMä»£ç†æ—¶ï¼Œé¢ä¸´æ€§èƒ½åœæ»å’Œé”™è¯¯ä¼ æ’­ç­‰æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºSTePæ–¹æ³•ï¼Œé€šè¿‡åˆæˆè‡ªæˆ‘åæ€è½¨è¿¹å’Œéƒ¨åˆ†æ©è”½ç­–ç•¥ï¼Œæå‡LLMä»£ç†çš„å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è‡ªæˆ‘åæ€è½¨è¿¹è®­ç»ƒçš„LLaMA2-7B-Chatåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä»…ä½¿ç”¨ä¸“å®¶è½¨è¿¹çš„ä»£ç†ï¼Œæ•°æ®éœ€æ±‚æ›´å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œè‡ªä¸»ä»£ç†çš„å®ç°å˜å¾—è¶Šæ¥è¶Šå¯è¡Œã€‚ç„¶è€Œï¼Œç°æœ‰å¼ºå¤§çš„ä»£ç†å¾€å¾€ä¾èµ–äºå¤æ‚çš„æç¤ºå·¥ç¨‹å’Œå°é—­æºçš„LLMï¼Œå¦‚GPT-4ã€‚è™½ç„¶ä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„ä¸“å®¶è½¨è¿¹è®­ç»ƒå¼€æºLLMæœ‰æ‰€æ”¹å–„ï¼Œä½†ä»é¢ä¸´æ€§èƒ½åœæ»å’Œé”™è¯¯ä¼ æ’­ç­‰é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•STePï¼Œé€šè¿‡åˆæˆè‡ªæˆ‘åæ€è½¨è¿¹æ¥å¢å¼ºLLMä»£ç†çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶å¼•å…¥éƒ¨åˆ†æ©è”½ç­–ç•¥ä»¥é˜²æ­¢ä»£ç†å†…åŒ–é”™è¯¯æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ALFWorldã€WebShopå’ŒSciWorldç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†ä»£ç†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰LLMä»£ç†è®­ç»ƒä¸­æ€§èƒ½åœæ»å’Œé”™è¯¯ä¼ æ’­çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºä¸“å®¶è½¨è¿¹ï¼Œå¯¼è‡´ä»£ç†åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ— æ³•æœ‰æ•ˆçº æ­£é”™è¯¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„STePæ–¹æ³•é€šè¿‡åˆæˆè‡ªæˆ‘åæ€è½¨è¿¹ï¼Œä½¿ä»£ç†èƒ½å¤Ÿåæ€å’Œçº æ­£é”™è¯¯æ­¥éª¤ï¼Œä»è€Œæå‡å­¦ä¹ æ•ˆæœã€‚åŒæ—¶ï¼Œéƒ¨åˆ†æ©è”½ç­–ç•¥é˜²æ­¢ä»£ç†å†…åŒ–ä¸æ­£ç¡®çš„æ­¥éª¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSTePæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè‡ªæˆ‘åæ€è½¨è¿¹ç”Ÿæˆæ¨¡å—å’Œéƒ¨åˆ†æ©è”½ç­–ç•¥æ¨¡å—ã€‚å‰è€…è´Ÿè´£ç”ŸæˆåŒ…å«åæ€å’Œçº æ­£çš„è½¨è¿¹ï¼Œåè€…åˆ™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è¾“å…¥è¿›è¡Œéƒ¨åˆ†æ©è”½ï¼Œä»¥æé«˜å­¦ä¹ è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè‡ªæˆ‘åæ€è½¨è¿¹çš„åˆæˆå’Œéƒ¨åˆ†æ©è”½ç­–ç•¥çš„å¼•å…¥ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒSTePä¸ä»…ä¾èµ–äºä¸“å®¶è½¨è¿¹ï¼Œè¿˜é€šè¿‡è‡ªæˆ‘åæ€æœºåˆ¶å¢å¼ºäº†ä»£ç†çš„å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSTePæ–¹æ³•å¯¹è‡ªæˆ‘åæ€è½¨è¿¹çš„ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿ç”Ÿæˆçš„è½¨è¿¹èƒ½å¤Ÿæœ‰æ•ˆåæ˜ é”™è¯¯å’Œçº æ­£æ­¥éª¤ã€‚åŒæ—¶ï¼Œéƒ¨åˆ†æ©è”½ç­–ç•¥çš„å®ç°ä¹Ÿè€ƒè™‘äº†å¦‚ä½•æœ€å¤§é™åº¦åœ°å‡å°‘é”™è¯¯ä¿¡æ¯çš„å†…åŒ–ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡åœ¨å®éªŒä¸­ç»è¿‡å¤šæ¬¡è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨STePæ–¹æ³•è®­ç»ƒçš„LLaMA2-7B-Chatåœ¨ALFWorldã€WebShopå’ŒSciWorldç­‰ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºä»…ä½¿ç”¨ä¸“å®¶è½¨è¿¹çš„ä»£ç†ï¼Œè®­ç»ƒæ•°æ®éœ€æ±‚å‡å°‘ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰è‡ªä¸»ä»£ç†ç³»ç»Ÿã€‚é€šè¿‡æå‡ä»£ç†çš„å­¦ä¹ èƒ½åŠ›å’Œè‡ªæˆ‘çº é”™èƒ½åŠ›ï¼ŒSTePæ–¹æ³•èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous agents, which perceive environments and take actions to achieve goals, have become increasingly feasible with the advancements in large language models (LLMs). However, current powerful agents often depend on sophisticated prompt engineering combined with closed-source LLMs like GPT-4. Although training open-source LLMs using expert trajectories from teacher models has yielded some improvements in agent capabilities, this approach still faces limitations such as performance plateauing and error propagation. To mitigate these challenges, we propose STeP, a novel method for improving LLM-based agent training. We synthesize self-reflected trajectories that include reflections and corrections of error steps, which enhance the effectiveness of LLM agents in learning from teacher models, enabling them to become agents capable of self-reflecting and correcting. We also introduce partial masking strategy that prevents the LLM from internalizing incorrect or suboptimal steps. Experiments demonstrate that our method improves agent performance across three representative tasks: ALFWorld, WebShop, and SciWorld. For the open-source model LLaMA2-7B-Chat, when trained using self-reflected trajectories constructed with Qwen1.5-110B-Chat as the teacher model, it achieves comprehensive improvements with less training data compared to agents trained exclusively on expert trajectories.

