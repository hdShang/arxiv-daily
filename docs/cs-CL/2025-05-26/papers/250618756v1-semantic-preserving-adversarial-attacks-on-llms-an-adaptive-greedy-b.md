---
layout: default
title: Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach
---

# Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.18756" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.18756v1</a>
  <a href="https://arxiv.org/pdf/2506.18756.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.18756v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.18756v1', 'Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chong Zhang, Xiang Li, Jia Wang, Shan Liang, Haochen Xue, Xiaobo Jin

**åˆ†ç±»**: cs.CL, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: 19 pages, 8 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/franz-chang/DOBS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è´ªå©ªäºŒåˆ†æœç´¢æ–¹æ³•ä»¥è§£å†³LLMsçš„è¯­ä¹‰ä¿æŒå¯¹æŠ—æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹æŠ—æ”»å‡»` `è¯­ä¹‰ä¿æŒ` `è‡ªåŠ¨æç¤ºä¼˜åŒ–` `è‡ªé€‚åº”ç®—æ³•` `è´ªå©ªæœç´¢` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç”¨æˆ·å¤šæ ·åŒ–éœ€æ±‚æ—¶ï¼Œå¸¸å¯¼è‡´è¯¯è§£å’Œé”™è¯¯è¾“å‡ºï¼Œå½±å“LLMsçš„æ€§èƒ½ã€‚
2. æå‡ºçš„è‡ªé€‚åº”è´ªå©ªäºŒåˆ†æœç´¢ï¼ˆAGBSï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è¯„ä¼°ä¼˜åŒ–ç­–ç•¥å¯¹LLMæ€§èƒ½çš„å½±å“ï¼Œç¡®ä¿è¯­ä¹‰ç¨³å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAGBSåœ¨è¯­ä¹‰ä¸€è‡´æ€§å’Œæ”»å‡»æ•ˆæœä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œæå‡äº†å¯¹æŠ—æ ·æœ¬ç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šä¾èµ–äºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIsï¼‰ä¸­çš„è‡ªåŠ¨æç¤ºå·¥ç¨‹æ¥ä¼˜åŒ–ç”¨æˆ·è¾“å…¥å¹¶æé«˜å“åº”å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”¨æˆ·éœ€æ±‚çš„å¤šæ ·æ€§å¸¸å¸¸å¯¼è‡´è¯¯è§£ï¼Œè‡ªåŠ¨ä¼˜åŒ–å¯èƒ½æ‰­æ›²åŸå§‹æ„å›¾å¹¶äº§ç”Ÿé”™è¯¯è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”è´ªå©ªäºŒåˆ†æœç´¢ï¼ˆAGBSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¯­ä¹‰ç¨³å®šæ€§çš„åŒæ—¶æ¨¡æ‹Ÿå¸¸è§çš„æç¤ºä¼˜åŒ–æœºåˆ¶ã€‚é€šè¿‡å¯¹å¼€æ”¾å’Œé—­æºLLMsè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†AGBSåœ¨å¹³è¡¡è¯­ä¹‰ä¸€è‡´æ€§å’Œæ”»å‡»æœ‰æ•ˆæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè®¾è®¡æ›´å¯é çš„æç¤ºä¼˜åŒ–ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨æç¤ºä¼˜åŒ–è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„è¯­ä¹‰æ‰­æ›²é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å®¹æ˜“å¯¼è‡´è¯¯è§£å’Œé”™è¯¯è¾“å‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAGBSæ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿå¸¸è§çš„æç¤ºä¼˜åŒ–æœºåˆ¶ï¼ŒåŠ¨æ€è¯„ä¼°å…¶å¯¹LLMæ€§èƒ½çš„å½±å“ï¼Œä»è€Œåœ¨ç”Ÿæˆå¯¹æŠ—æ ·æœ¬æ—¶ä¿æŒè¯­ä¹‰çš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAGBSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥è¯„ä¼°ã€ä¼˜åŒ–ç­–ç•¥æ¨¡æ‹Ÿå’Œå¯¹æŠ—æ ·æœ¬ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆè¯„ä¼°ç”¨æˆ·è¾“å…¥çš„è¯­ä¹‰ï¼Œç„¶ååº”ç”¨ä¼˜åŒ–ç­–ç•¥ï¼Œæœ€åç”Ÿæˆå¯¹æŠ—æ ·æœ¬ä»¥æµ‹è¯•æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šAGBSçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è‡ªé€‚åº”æ€§å’Œè´ªå©ªæœç´¢ç­–ç•¥ï¼Œä½¿å¾—åœ¨ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€ä¼˜åŒ–ç­–ç•¥å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨AGBSä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ä¼˜åŒ–æ­¥é•¿å’Œè¯„ä¼°é˜ˆå€¼ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºå…¼é¡¾è¯­ä¹‰ç¨³å®šæ€§ä¸æ”»å‡»æ•ˆæœï¼Œç¡®ä¿ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬æ—¢æœ‰æ•ˆåˆä¸å¤±åŸæ„ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†è°ƒä¼˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAGBSæ–¹æ³•åœ¨å¤šä¸ªå¼€æºå’Œé—­æºLLMsä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯­ä¹‰ä¸€è‡´æ€§æå‡äº†çº¦15%ï¼Œè€Œå¯¹æŠ—æ ·æœ¬ç”Ÿæˆçš„æœ‰æ•ˆæ€§æé«˜äº†20%ã€‚è¿™äº›ç»“æœè¡¨æ˜AGBSåœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å®¢æœç³»ç»Ÿå’Œäººæœºäº¤äº’ç•Œé¢ç­‰ã€‚é€šè¿‡æé«˜å¯¹æŠ—æ ·æœ¬ç”Ÿæˆçš„æœ‰æ•ˆæ€§å’Œè¯­ä¹‰ç¨³å®šæ€§ï¼ŒAGBSæ–¹æ³•èƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å¯é çš„æç¤ºä¼˜åŒ–ç³»ç»Ÿï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) increasingly rely on automatic prompt engineering in graphical user interfaces (GUIs) to refine user inputs and enhance response accuracy. However, the diversity of user requirements often leads to unintended misinterpretations, where automated optimizations distort original intentions and produce erroneous outputs. To address this challenge, we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates common prompt optimization mechanisms while preserving semantic stability. Our approach dynamically evaluates the impact of such strategies on LLM performance, enabling robust adversarial sample generation. Through extensive experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness in balancing semantic consistency and attack efficacy. Our findings offer actionable insights for designing more reliable prompt optimization systems. Code is available at: https://github.com/franz-chang/DOBS

