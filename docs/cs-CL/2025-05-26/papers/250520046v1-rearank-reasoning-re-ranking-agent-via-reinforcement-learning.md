---
layout: default
title: "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning"
---

# REARANK: Reasoning Re-ranking Agent via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20046" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20046v1</a>
  <a href="https://arxiv.org/pdf/2505.20046.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20046v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20046v1', 'REARANK: Reasoning Re-ranking Agent via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Le Zhang, Bo Wang, Xipeng Qiu, Siva Reddy, Aishwarya Agrawal

**åˆ†ç±»**: cs.IR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºREARANKä»¥æå‡ä¿¡æ¯æ£€ç´¢ä¸­çš„é‡æ’åºæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¿¡æ¯æ£€ç´¢` `é‡æ’åº` `æ¨ç†æœºåˆ¶` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®å¢å¼º` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¿¡æ¯æ£€ç´¢æ–¹æ³•åœ¨é‡æ’åºè¿‡ç¨‹ä¸­ç¼ºä¹æœ‰æ•ˆçš„æ¨ç†æœºåˆ¶ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³å’Œå¯è§£é‡Šæ€§å·®ã€‚
2. REARANKé€šè¿‡å¼•å…¥æ˜¾å¼æ¨ç†æ­¥éª¤ï¼Œåœ¨é‡æ’åºä¹‹å‰è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒREARANKåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†REARANKï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ—è¡¨æ¨ç†é‡æ’åºä»£ç†ã€‚REARANKåœ¨é‡æ’åºä¹‹å‰è¿›è¡Œæ˜¾å¼æ¨ç†ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®å¢å¼ºï¼ŒREARANKåœ¨æµè¡Œçš„ä¿¡æ¯æ£€ç´¢åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸”ä»…éœ€179ä¸ªæ ‡æ³¨æ ·æœ¬ã€‚åŸºäºQwen2.5-7Bæ„å»ºçš„REARANK-7Båœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸GPT-4ç›¸å½“ï¼Œç”šè‡³åœ¨æ¨ç†å¯†é›†çš„BRIGHTåŸºå‡†ä¸Šè¶…è¶Šäº†GPT-4ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºLLMåœ¨é‡æ’åºä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä¿¡æ¯æ£€ç´¢ä¸­é‡æ’åºçš„æ€§èƒ½ä¸è¶³å’Œå¯è§£é‡Šæ€§å·®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹æœ‰æ•ˆçš„æ¨ç†æœºåˆ¶ï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šREARANKçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ˜¾å¼æ¨ç†æ­¥éª¤ï¼Œåœ¨é‡æ’åºä¹‹å‰è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®å¢å¼ºï¼ŒREARANKèƒ½å¤Ÿåœ¨è¾ƒå°‘çš„æ ‡æ³¨æ ·æœ¬ä¸‹å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šREARANKçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨ç†æ¨¡å—å’Œé‡æ’åºæ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹å¯¹è¾“å…¥æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åé€šè¿‡æ¨ç†æ¨¡å—è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œæœ€ååœ¨é‡æ’åºæ¨¡å—ä¸­ç”Ÿæˆæœ€ç»ˆçš„æ’åºç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šREARANKçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ˜¾å¼æ¨ç†æœºåˆ¶å’Œå¼ºåŒ–å­¦ä¹ çš„ç»“åˆï¼Œè¿™ä¸ä¼ ç»Ÿçš„é‡æ’åºæ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒREARANKä¸ä»…æé«˜äº†æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒREARANKé‡‡ç”¨äº†Qwen2.5-7Bä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–äº†æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ä»…éœ€179ä¸ªæ ‡æ³¨æ ·æœ¬å³å¯å®ç°è‰¯å¥½çš„æ€§èƒ½ï¼Œè¿™åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸæ˜¯ä¸€ä¸ªé‡è¦çš„è¿›å±•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREARANKåœ¨å¤šä¸ªä¿¡æ¯æ£€ç´¢åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨æ¨ç†å¯†é›†å‹çš„BRIGHTåŸºå‡†ä¸Šè¡¨ç°ä¼˜äºGPT-4ã€‚REARANK-7Båœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸GPT-4ç›¸å½“ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

REARANKçš„ç ”ç©¶æˆæœåœ¨ä¿¡æ¯æ£€ç´¢ã€æ¨èç³»ç»Ÿå’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡é‡æ’åºçš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨æ£€ç´¢ç»“æœï¼Œè¿›è€Œæé«˜ä¿¡æ¯è·å–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒREARANKå¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹å’Œæœç´¢å¼•æ“ç­‰å®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking.

