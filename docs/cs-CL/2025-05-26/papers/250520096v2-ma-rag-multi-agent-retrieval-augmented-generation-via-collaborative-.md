---
layout: default
title: MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning
---

# MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20096" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20096v2</a>
  <a href="https://arxiv.org/pdf/2505.20096.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20096v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20096v2', 'MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thang Nguyen, Peter Chin, Yu-Wing Tai

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-10-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMA-RAGæ¡†æ¶ä»¥è§£å†³å¤æ‚ä¿¡æ¯æ£€ç´¢ä¸­çš„æ¨ç†æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¢å¼ºæ£€ç´¢ç”Ÿæˆ` `é“¾å¼æ€ç»´` `é—®ç­”ç³»ç»Ÿ` `æ¨¡å—åŒ–æ¨ç†` `ä¿¡æ¯æ£€ç´¢` `åŒ»ç–—é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RAGæ–¹æ³•é€šå¸¸ä¾èµ–äºç«¯åˆ°ç«¯å¾®è°ƒæˆ–å­¤ç«‹ç»„ä»¶çš„å¢å¼ºï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚ä¿¡æ¯æ£€ç´¢ä¸­çš„æ¨ç†æŒ‘æˆ˜ã€‚
2. MA-RAGé€šè¿‡åè°ƒå¤šä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œåˆ†è§£ä»»åŠ¡ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œé‡‡ç”¨é“¾å¼æ€ç»´æç¤ºæ¥å®ç°åä½œæ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMA-RAGåœ¨å¤šè·³é—®ç­”åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³å°å‹æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šå¤§å‹ç‹¬ç«‹LLMï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†MA-RAGï¼Œä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„å›ºæœ‰æ¨¡ç³Šæ€§å’Œæ¨ç†æŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿçš„RAGæ–¹æ³•ä¸åŒï¼ŒMA-RAGåè°ƒäº†ä¸€ç»„ä¸“é—¨çš„AIæ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬è§„åˆ’è€…ã€æ­¥éª¤å®šä¹‰è€…ã€æå–å™¨å’Œé—®ç­”æ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£RAGæµç¨‹çš„ä¸åŒé˜¶æ®µã€‚é€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºæŸ¥è¯¢æ¶ˆæ­§ã€è¯æ®æå–å’Œç­”æ¡ˆåˆæˆç­‰å­ä»»åŠ¡ï¼Œå¹¶é€šè¿‡é“¾å¼æ€ç»´æç¤ºä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿäº¤æµä¸­é—´æ¨ç†ï¼ŒMA-RAGé€æ­¥ä¼˜åŒ–æ£€ç´¢å’Œåˆæˆï¼ŒåŒæ—¶ä¿æŒæ¨¡å—åŒ–å¯è§£é‡Šæ€§ã€‚åœ¨å¤šè·³å’Œæ¨¡ç³Šé—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMA-RAGæ˜¾è‘—è¶…è¶Šäº†ç‹¬ç«‹çš„LLMå’Œç°æœ‰çš„RAGæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å„ä¸ªæ¨¡å‹è§„æ¨¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„æ¨¡ç³Šæ€§å’Œæ¨ç†æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†å¤šè·³æ¨ç†å’Œä¿¡æ¯æ•´åˆï¼Œå¯¼è‡´ç­”æ¡ˆå‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMA-RAGçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“åä½œæ¥åˆ†è§£ä»»åŠ¡ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£ç‰¹å®šçš„å­ä»»åŠ¡ï¼Œä»è€Œæé«˜æ•´ä½“æ¨ç†èƒ½åŠ›å’Œç­”æ¡ˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMA-RAGæ¡†æ¶åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šè§„åˆ’è€…è´Ÿè´£ä»»åŠ¡è§„åˆ’ï¼Œæ­¥éª¤å®šä¹‰è€…æ˜ç¡®æ¯ä¸ªæ­¥éª¤ï¼Œæå–å™¨ä»æ–‡æ¡£ä¸­æå–è¯æ®ï¼Œé—®ç­”æ™ºèƒ½ä½“åˆæˆæœ€ç»ˆç­”æ¡ˆã€‚æ™ºèƒ½ä½“ä¹‹é—´é€šè¿‡é“¾å¼æ€ç»´æç¤ºè¿›è¡Œæ²Ÿé€šï¼Œé€æ­¥ä¼˜åŒ–æ£€ç´¢å’Œåˆæˆè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMA-RAGçš„åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“åä½œæœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­æä¾›å¯è§£é‡Šçš„ä¸­é—´æ­¥éª¤ï¼Œä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè§„åˆ’è€…å’Œæå–å™¨æ™ºèƒ½ä½“è¢«è®¤ä¸ºæ˜¯å¤šè·³æ¨ç†çš„å…³é”®ï¼Œä¸”é«˜å®¹é‡æ¨¡å‹åœ¨é—®ç­”æ™ºèƒ½ä½“ä¸­å°¤ä¸ºé‡è¦ï¼Œä»¥æœ‰æ•ˆåˆæˆç­”æ¡ˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMA-RAGåœ¨å¤šä¸ªå¤šè·³é—®ç­”åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGæ–¹æ³•å’Œç‹¬ç«‹çš„LLMï¼Œå°¤å…¶æ˜¯å°å‹LLaMA3-8Bæ¨¡å‹åœ¨å¼•å…¥MA-RAGåè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„ç‹¬ç«‹LLMã€‚æ­¤å¤–ï¼ŒLLaMA3-70Bå’ŒGPT-4o-miniåœ¨å¤šè·³æ•°æ®é›†ä¸Šåˆ›é€ äº†æ–°çš„æœ€ä¼˜ç»“æœï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MA-RAGæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨å¤æ‚çš„é—®ç­”ç³»ç»Ÿã€åŒ»ç–—é—®ç­”å’Œå…¶ä»–éœ€è¦é«˜å‡†ç¡®æ€§çš„ä¿¡æ¯æ£€ç´¢é¢†åŸŸã€‚å…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§å¾—ä»¥å¢å¼ºï¼Œæœªæ¥å¯ä¸ºæ™ºèƒ½åŠ©æ‰‹å’Œä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›æ›´å¯é çš„æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation (RAG) that addresses the inherent ambiguities and reasoning challenges in complex information-seeking tasks. Unlike conventional RAG methods that rely on end-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates a collaborative set of specialized AI agents: Planner, Step Definer, Extractor, and QA Agents, each responsible for a distinct stage of the RAG pipeline. By decomposing tasks into subtasks such as query disambiguation, evidence extraction, and answer synthesis, and enabling agents to communicate intermediate reasoning via chain-of-thought prompting, MA-RAG progressively refines retrieval and synthesis while maintaining modular interpretability. Extensive experiments on multi-hop and ambiguous QA benchmarks, including NQ, HotpotQA, 2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly outperforms standalone LLMs and existing RAG methods across all model scales. Notably, even a small LLaMA3-8B model equipped with MA-RAG surpasses larger standalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new state-of-the-art results on challenging multi-hop datasets. Ablation studies reveal that both the planner and extractor agents are critical for multi-hop reasoning, and that high-capacity models are especially important for the QA agent to synthesize answers effectively. Beyond general-domain QA, MA-RAG generalizes to specialized domains such as medical QA, achieving competitive performance against domain-specific models without any domain-specific fine-tuning. Our results highlight the effectiveness of collaborative, modular reasoning in retrieval-augmented systems: MA-RAG not only improves answer accuracy and robustness but also provides interpretable intermediate reasoning steps, establishing a new paradigm for efficient and reliable multi-agent RAG.

