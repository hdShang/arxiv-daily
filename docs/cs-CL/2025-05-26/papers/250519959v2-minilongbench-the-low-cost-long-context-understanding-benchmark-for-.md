---
layout: default
title: MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models
---

# MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19959" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19959v2</a>
  <a href="https://arxiv.org/pdf/2505.19959.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19959v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19959v2', 'MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhongzhan Huang, Guoming Ling, Shanshan Zhong, Hefeng Wu, Liang Lin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-07-30)

**å¤‡æ³¨**: Accepted by ACL'25 main track

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MilkThink-Lab/MiniLongBench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMiniLongBenchä»¥é™ä½é•¿æ–‡æœ¬ç†è§£åŸºå‡†çš„è¯„ä¼°æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬ç†è§£` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŸºå‡†è¯„ä¼°` `æ•°æ®å‹ç¼©` `æ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é•¿æ–‡æœ¬ç†è§£åŸºå‡†è¯„ä¼°æˆæœ¬é«˜ï¼Œå½±å“äº†ç ”ç©¶çš„æ•ˆç‡å’Œå¯æŒç»­æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡ä¿®å‰ªLongBenchåŸºå‡†ï¼Œåˆ›å»ºäº†MiniLongBenchä»¥é™ä½è¯„ä¼°æˆæœ¬ã€‚
3. MiniLongBenchåœ¨å¯¹60å¤šç§LLMçš„æµ‹è¯•ä¸­ï¼Œè¯„ä¼°æˆæœ¬é™ä½è‡³åŸæ¥çš„4.5%ï¼Œä¸”ä¸LongBenchçš„ç›¸å…³æ€§ä¿æŒé«˜è¾¾0.97ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿æ–‡æœ¬ç†è§£ï¼ˆLCUï¼‰æ˜¯å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç ”ç©¶çš„é‡è¦é¢†åŸŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„LCUåŸºå‡†ç”±äºé•¿æ–‡æœ¬æ•°æ®çš„ç‰¹æ€§ï¼Œå¾€å¾€å¯¼è‡´è¯„ä¼°æˆæœ¬è¿‡é«˜ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰åŸºå‡†å­˜åœ¨æ˜¾è‘—å†—ä½™ï¼Œå¯¼è‡´è¯„ä¼°æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ç¨€ç–ä¿¡æ¯ç‰¹å¾çš„é•¿æ–‡æœ¬æ•°æ®å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡ä¿®å‰ªç°æœ‰çš„LongBenchåŸºå‡†ï¼Œåˆ›å»ºäº†MiniLongBenchã€‚è¯¥åŸºå‡†ä»…åŒ…å«237ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ¶µç›–å…­ä¸ªä¸»è¦ä»»åŠ¡ç±»åˆ«å’Œ21ä¸ªä¸åŒä»»åŠ¡ã€‚ç»è¿‡å¯¹60å¤šç§LLMçš„å®è¯åˆ†æï¼ŒMiniLongBenchçš„è¯„ä¼°æˆæœ¬é™ä½è‡³åŸæ¥çš„4.5%ï¼ŒåŒæ—¶ä¸LongBenchç»“æœçš„å¹³å‡æ’åç›¸å…³ç³»æ•°è¾¾åˆ°0.97ã€‚å› æ­¤ï¼ŒMiniLongBenchä½œä¸ºä½æˆæœ¬åŸºå‡†ï¼Œå…·æœ‰æ¨åŠ¨æœªæ¥LLMé•¿æ–‡æœ¬ç†è§£èƒ½åŠ›ç ”ç©¶çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é•¿æ–‡æœ¬ç†è§£åŸºå‡†ï¼ˆå¦‚LongBenchï¼‰è¯„ä¼°æˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼Œå¯¼è‡´ç ”ç©¶æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•åœ¨æµ‹è¯•æ—¶é—´å’Œæ¨ç†è´¹ç”¨ä¸Šå­˜åœ¨æ˜¾è‘—å†—ä½™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æå‡ºä¸€ç§é’ˆå¯¹é•¿æ–‡æœ¬æ•°æ®çš„å‹ç¼©æ–¹æ³•ï¼Œå»é™¤å†—ä½™ä¿¡æ¯ï¼Œä¿ç•™å…³é”®ä»»åŠ¡æ ·æœ¬ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªä½æˆæœ¬çš„è¯„ä¼°åŸºå‡†MiniLongBenchã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMiniLongBenchçš„æ„å»ºè¿‡ç¨‹åŒ…æ‹¬æ•°æ®é€‰æ‹©ã€æ ·æœ¬ä¿®å‰ªå’Œä»»åŠ¡åˆ†ç±»ï¼Œç¡®ä¿æ¶µç›–ä¸»è¦ä»»åŠ¡ç±»åˆ«ï¼ŒåŒæ—¶å‡å°‘æ ·æœ¬æ•°é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡æ•°æ®å‹ç¼©æŠ€æœ¯æ˜¾è‘—é™ä½äº†è¯„ä¼°æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸåŸºå‡†çš„é«˜ç›¸å…³æ€§ï¼Œè¿™æ˜¯ç°æœ‰æ–¹æ³•æ‰€æœªèƒ½å®ç°çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé€‰æ‹©äº†237ä¸ªæ ·æœ¬ï¼Œæ¶µç›–å…­ä¸ªä¸»è¦ä»»åŠ¡ç±»åˆ«ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§ä¸æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶é€šè¿‡å®è¯åˆ†æéªŒè¯äº†è¯„ä¼°æˆæœ¬çš„é™ä½å’Œç›¸å…³æ€§çš„ä¿æŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MiniLongBenchåœ¨å¯¹60å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ä¸­ï¼ŒæˆåŠŸå°†è¯„ä¼°æˆæœ¬é™ä½è‡³åŸæ¥çš„4.5%ï¼ŒåŒæ—¶ä¸LongBenchçš„ç»“æœä¿æŒäº†é«˜è¾¾0.97çš„å¹³å‡æ’åç›¸å…³ç³»æ•°ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MiniLongBenchçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›è¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆè¯„ä¼°çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸã€‚å…¶ä½æˆæœ¬ç‰¹æ€§å°†æ¨åŠ¨ç›¸å…³ç ”ç©¶çš„æ·±å…¥å¼€å±•ï¼Œä¿ƒè¿›æŠ€æœ¯çš„å¿«é€Ÿè¿­ä»£ä¸åº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long Context Understanding (LCU) is a critical area for exploration in current large language models (LLMs). However, due to the inherently lengthy nature of long-text data, existing LCU benchmarks for LLMs often result in prohibitively high evaluation costs, like testing time and inference expenses. Through extensive experimentation, we discover that existing LCU benchmarks exhibit significant redundancy, which means the inefficiency in evaluation. In this paper, we propose a concise data compression method tailored for long-text data with sparse information characteristics. By pruning the well-known LCU benchmark LongBench, we create MiniLongBench. This benchmark includes only 237 test samples across six major task categories and 21 distinct tasks. Through empirical analysis of over 60 LLMs, MiniLongBench achieves an average evaluation cost reduced to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results. Therefore, our MiniLongBench, as a low-cost benchmark, holds great potential to substantially drive future research into the LCU capabilities of LLMs. See https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.

