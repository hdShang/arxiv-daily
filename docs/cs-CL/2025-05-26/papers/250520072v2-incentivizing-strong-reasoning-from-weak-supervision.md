---
layout: default
title: Incentivizing Strong Reasoning from Weak Supervision
---

# Incentivizing Strong Reasoning from Weak Supervision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20072" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20072v2</a>
  <a href="https://arxiv.org/pdf/2505.20072.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20072v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20072v2', 'Incentivizing Strong Reasoning from Weak Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yige Yuan, Teng Xiao, Shuchang Tao, Xue Wang, Jinyang Gao, Bolin Ding, Bingbing Xu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-05-28)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yuanyige/w2sr)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼±ç›‘ç£æ¿€åŠ±å¼ºæ¨ç†ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼±ç›‘ç£` `æ¨ç†èƒ½åŠ›` `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹è’¸é¦` `è‡ªç„¶è¯­è¨€å¤„ç†` `æˆæœ¬é™ä½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–æ˜‚è´µçš„é«˜è´¨é‡ç¤ºèŒƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼Œé™åˆ¶äº†æ¨ç†èƒ½åŠ›çš„æå‡ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ˜¾è‘—è¾ƒå¼±æ¨¡å‹çš„ç›‘ç£æ¥æ¿€åŠ±LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œæ¢ç´¢å…¶æœ‰æ•ˆæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¼±æ¨ç†è€…çš„ç›‘ç£èƒ½æ˜¾è‘—æå‡å¼ºæ¨¡å‹çš„æ¨ç†è¡¨ç°ï¼Œæ¥è¿‘94%çš„RLæ”¶ç›Šï¼Œä¸”æˆæœ¬ä½å»‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æå‡å…¶æ¨ç†èƒ½åŠ›é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆ–é«˜è´¨é‡çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨æ²¡æœ‰é«˜è´¨é‡ç¤ºèŒƒå’Œå¼ºåŒ–å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•é€šè¿‡æ˜¾è‘—è¾ƒå¼±æ¨¡å‹çš„ç›‘ç£æ¥æ¿€åŠ±LLMsçš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¥è‡ªå¼±æ¨ç†è€…çš„ç›‘ç£å¯ä»¥æ˜¾è‘—æé«˜å¼ºæ¨¡å‹çš„æ¨ç†è¡¨ç°ï¼Œæ¢å¤è¿‘94%çš„RLæ”¶ç›Šï¼Œä¸”æˆæœ¬å¤§å¹…é™ä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼±æ¨ç†è€…èƒ½å¤Ÿæœ‰æ•ˆæ¿€åŠ±å¼ºå­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¿æ³›æå‡å¤šç§æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶æå‡ºçš„å¼±åˆ°å¼ºçš„èŒƒå¼ä¸ºåœ¨æ¨ç†æ—¶æ¿€åŠ±LLMsçš„å¼ºæ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰å‰æ™¯ä¸”å¯æ¨å¹¿çš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åœ¨æ²¡æœ‰é«˜è´¨é‡ç¤ºèŒƒå’Œå¼ºåŒ–å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå…¶é«˜æˆæœ¬å’Œå¯¹é«˜è´¨é‡æ•°æ®çš„ä¾èµ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ˜¾è‘—è¾ƒå¼±çš„æ¨ç†æ¨¡å‹æ¥æä¾›ç›‘ç£ï¼Œæ¿€åŠ±å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é™ä½æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ€§èƒ½çš„æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¼±æ¨ç†è€…å’Œå¼ºå­¦ç”Ÿæ¨¡å‹ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚å¼±æ¨ç†è€…æä¾›ä½æˆæœ¬çš„ç›‘ç£ä¿¡å·ï¼Œè€Œå¼ºæ¨¡å‹åˆ™é€šè¿‡å­¦ä¹ è¿™äº›ä¿¡å·æ¥æå‡è‡ªèº«çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†å¼±åˆ°å¼ºçš„ç›‘ç£èŒƒå¼ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†èƒ½åŠ›æå‡çš„æˆæœ¬ï¼Œä¸ä¼ ç»Ÿçš„é«˜è´¨é‡ç¤ºèŒƒå’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å¯¹å¼±æ¨ç†è€…çš„é€‰æ‹©å’Œè®­ç»ƒè¿›è¡Œäº†ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿å…¶è¾“å‡ºçš„ç›‘ç£ä¿¡å·èƒ½å¤Ÿæœ‰æ•ˆæ¿€åŠ±å¼ºæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¥è‡ªå¼±æ¨ç†è€…çš„ç›‘ç£èƒ½å¤Ÿä½¿å¼ºæ¨¡å‹çš„æ¨ç†è¡¨ç°æ˜¾è‘—æå‡ï¼Œæ¢å¤è¿‘94%çš„å¼ºåŒ–å­¦ä¹ æ”¶ç›Šï¼Œä¸”åœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹æ¶æ„ä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ¨ç†ç­‰ã€‚é€šè¿‡é™ä½æ¨ç†èƒ½åŠ›æå‡çš„æˆæœ¬ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿä½¿æ›´å¤šçš„ä¼ä¸šå’Œç ”ç©¶æœºæ„èƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„æ™®åŠå’Œå‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated impressive performance on reasoning-intensive tasks, but enhancing their reasoning abilities typically relies on either reinforcement learning (RL) with verifiable signals or supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT) demonstrations, both of which are expensive. In this paper, we study a novel problem of incentivizing the reasoning capacity of LLMs without expensive high-quality demonstrations and reinforcement learning. We investigate whether the reasoning capabilities of LLMs can be effectively incentivized via supervision from significantly weaker models. We further analyze when and why such weak supervision succeeds in eliciting reasoning abilities in stronger models. Our findings show that supervision from significantly weaker reasoners can substantially improve student reasoning performance, recovering close to 94% of the gains of expensive RL at a fraction of the cost. Experiments across diverse benchmarks and model architectures demonstrate that weak reasoners can effectively incentivize reasoning in stronger student models, consistently improving performance across a wide range of reasoning tasks. Our results suggest that this simple weak-to-strong paradigm is a promising and generalizable alternative to costly methods for incentivizing strong reasoning capabilities at inference-time in LLMs. The code is publicly available at https://github.com/yuanyige/w2sr.

