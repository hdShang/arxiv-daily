---
layout: default
title: "SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?"
---

# SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20295" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20295v3</a>
  <a href="https://arxiv.org/pdf/2505.20295.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20295v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20295v3', 'SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Michael Kirchhof, Luca FÃ¼ger, Adam GoliÅ„ski, Eeshan Gunesh Dhekane, Arno Blaas, Seong Joon Oh, Sinead Williamson

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSelfReflectä»¥æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸ç¡®å®šæ€§è¡¨è¾¾` `ä¿¡æ¯è®º` `é€æ˜åº¦` `ä¿¡å¿µåˆ†å¸ƒ` `æ‘˜è¦ç”Ÿæˆ` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¡¨è¾¾å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ—¶ï¼Œç¼ºä¹é€æ˜åº¦å’Œå…¨é¢æ€§ï¼Œé€šå¸¸åªèƒ½ç»™å‡ºå•ä¸€ç­”æ¡ˆåŠå…¶æ¨¡ç³Šæè¿°ã€‚
2. æœ¬æ–‡æå‡ºSelfReflectåº¦é‡ï¼Œé€šè¿‡ä¿¡æ¯è®ºæ–¹æ³•è¯„ä¼°æ¨¡å‹å†…éƒ¨ä¿¡å¿µåˆ†å¸ƒä¸ç”Ÿæˆæ‘˜è¦ä¹‹é—´çš„å·®å¼‚ï¼Œå¢å¼ºæ¨¡å‹çš„é€æ˜åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSelfReflectèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ¨¡å‹è¾“å‡ºçš„å¾®å°åå·®ï¼Œä¸”é€šè¿‡åé¦ˆæœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´ä¸ºå¿ å®çš„è¾“å‡ºæ‘˜è¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¡¨è¾¾ä¸ç¡®å®šæ€§æ—¶ï¼Œé€šå¸¸ä»…é€šè¿‡æ·»åŠ ç™¾åˆ†æ¯”æˆ–æ¨¡ç³Šè¯æ±‡æ¥è¿›è¡Œã€‚ç„¶è€Œï¼Œç”¨æˆ·éœ€è¦çš„æ˜¯æ¨¡å‹èƒ½å¤Ÿåæ˜ å…¶å†…éƒ¨ä¿¡å¿µåˆ†å¸ƒï¼Œå¹¶è¾“å‡ºæ‰€æœ‰å¯èƒ½é€‰é¡¹åŠå…¶æ¦‚ç‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SelfReflectåº¦é‡ï¼Œä½œä¸ºä¸€ç§ä¿¡æ¯è®ºè·ç¦»ï¼Œç”¨äºè¯„ä¼°æ‘˜è¦ä¸ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚é€šè¿‡å¹²é¢„å’Œäººç±»ç ”ç©¶ï¼Œå‘ç°SelfReflectèƒ½å¤Ÿæ•æ„Ÿåœ°æ•æ‰åˆ°å¾®å°åå·®ï¼Œæä¾›äº†æ‘˜è¦å­—ç¬¦ä¸²ä¸LLMå®é™…å†…éƒ¨ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„å¿ å®åº¦é‡ã€‚å°½ç®¡ç°ä»£LLMæ™®éæ— æ³•ç›´æ¥æ­ç¤ºå…¶ä¸ç¡®å®šæ€§ï¼Œä½†é€šè¿‡é‡‡æ ·å¤šä¸ªè¾“å‡ºå¹¶åé¦ˆåˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¿ å®çš„æ‘˜è¦ï¼Œä»è€Œä¸ºæœªæ¥LLMä¸ç¡®å®šæ€§æ²Ÿé€šçš„å‘å±•æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨è¾¾ä¸ç¡®å®šæ€§æ—¶çš„é€æ˜åº¦ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªèƒ½æä¾›å•ä¸€ç­”æ¡ˆï¼Œç¼ºä¹å¯¹å†…éƒ¨ä¿¡å¿µåˆ†å¸ƒçš„åæ˜ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºSelfReflectåº¦é‡ï¼Œæ—¨åœ¨é€šè¿‡ä¿¡æ¯è®ºçš„è·ç¦»åº¦é‡ï¼Œè¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦ä¸å…¶å†…éƒ¨ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é€æ˜åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œæ¨¡å‹ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„è¾“å‡ºï¼›å…¶æ¬¡ï¼Œè®¡ç®—è¿™äº›è¾“å‡ºçš„åˆ†å¸ƒï¼›æœ€åï¼Œåˆ©ç”¨SelfReflectåº¦é‡è¯„ä¼°ç”Ÿæˆæ‘˜è¦çš„å¿ å®åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥SelfReflectåº¦é‡ï¼Œèƒ½å¤Ÿæ•æ„Ÿåœ°æ•æ‰æ¨¡å‹è¾“å‡ºçš„å¾®å°åå·®ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºå…¨é¢çš„ä¿¡å¿µåˆ†å¸ƒè¡¨è¾¾æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œæ¨¡å‹é€šè¿‡é‡‡æ ·å¤šä¸ªè¾“å‡ºå¹¶å°†å…¶åé¦ˆåˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œä»¥æ­¤æé«˜ç”Ÿæˆæ‘˜è¦çš„å¿ å®åº¦ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSelfReflectèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ¨¡å‹è¾“å‡ºçš„å¾®å°åå·®ï¼Œæä¾›äº†æ›´ä¸ºå‡†ç¡®çš„å¿ å®åº¦é‡ã€‚å°½ç®¡ç°ä»£LLMåœ¨ç›´æ¥è¡¨è¾¾ä¸ç¡®å®šæ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä½†é€šè¿‡åé¦ˆæœºåˆ¶ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦çš„å¿ å®åº¦å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†æœªæ¥å‘å±•çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿå’Œä»»ä½•éœ€è¦è¡¨è¾¾ä¸ç¡®å®šæ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹ä¸ç¡®å®šæ€§çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·çš„ä¿¡ä»»æ„Ÿå’Œäº¤äº’ä½“éªŒï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables.

