---
layout: default
title: Efficient Speech Translation through Model Compression and Knowledge Distillation
---

# Efficient Speech Translation through Model Compression and Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20237v2</a>
  <a href="https://arxiv.org/pdf/2505.20237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20237v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20237v2', 'Efficient Speech Translation through Model Compression and Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yasmin Moslem

**åˆ†ç±»**: cs.CL, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-06-02)

**å¤‡æ³¨**: IWSLT 2025

**æœŸåˆŠ**: Proceedings of the 22nd International Conference on Spoken Language Translation (IWSLT 2025)

**DOI**: [10.18653/v1/2025.iwslt-1.40](https://doi.org/10.18653/v1/2025.iwslt-1.40)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ¨¡å‹å‹ç¼©ä¸çŸ¥è¯†è’¸é¦æé«˜è¯­éŸ³ç¿»è¯‘æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹å‹ç¼©` `çŸ¥è¯†è’¸é¦` `è¯­éŸ³ç¿»è¯‘` `ä½ç§©é€‚åº”` `é‡åŒ–æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³ç¿»è¯‘ä¸­è®¡ç®—éœ€æ±‚é«˜ï¼Œå¯¼è‡´éƒ¨ç½²å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºç»“åˆå±‚ä¿®å‰ªã€ä½ç§©é€‚åº”å’ŒçŸ¥è¯†è’¸é¦çš„æ–¹æ³•ï¼Œä»¥é™ä½æ¨¡å‹å¤æ‚åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä¿®å‰ªåçš„æ¨¡å‹åœ¨å‚æ•°å’Œå­˜å‚¨ä¸Šå‡å°‘50%ï¼Œç¿»è¯‘è´¨é‡å‡ ä¹ä¸å˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³ç¿»è¯‘ä¸­çš„é«˜æ•ˆéƒ¨ç½²é¢ä¸´æ˜¾è‘—çš„è®¡ç®—éœ€æ±‚æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡åœ¨å›½é™…å£è¯­è¯­è¨€ç¿»è¯‘ä¼šè®®ï¼ˆIWSLT 2025ï¼‰â€œæ¨¡å‹å‹ç¼©â€èµ›é“çš„ç³»ç»Ÿæäº¤ï¼Œæå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ç»“åˆäº†åŸºäºå±‚é‡è¦æ€§è¯„ä¼°çš„è¿­ä»£å±‚ä¿®å‰ªã€4ä½é‡åŒ–çš„ä½ç§©é€‚åº”ï¼ˆQLoRAï¼‰å’ŒçŸ¥è¯†è’¸é¦ç­‰æ–¹æ³•ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Qwen2-Audio-7B-Instructè¿›è¡Œå¾·è¯­å’Œä¸­æ–‡çš„è¯­éŸ³ç¿»è¯‘ã€‚ç»è¿‡ä¿®å‰ªçš„æ¨¡å‹åœ¨å‚æ•°å’Œå­˜å‚¨å ç”¨ä¸Šå‡å°‘äº†50%ï¼ŒåŒæ—¶ä¿æŒäº†æ•™å¸ˆæ¨¡å‹97-100%çš„ç¿»è¯‘è´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³ç¿»è¯‘ä¸­çš„é«˜è®¡ç®—éœ€æ±‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨éƒ¨ç½²æ—¶é¢ä¸´å‚æ•°é‡å¤§ã€å­˜å‚¨éœ€æ±‚é«˜ç­‰ç—›ç‚¹ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¨¡å‹å‹ç¼©å’ŒçŸ¥è¯†è’¸é¦çš„ç»“åˆï¼Œé™ä½æ¨¡å‹çš„å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒç¿»è¯‘è´¨é‡ã€‚é‡‡ç”¨å±‚ä¿®å‰ªå’Œä½ç§©é€‚åº”æŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆè¿›è¡Œå±‚é‡è¦æ€§è¯„ä¼°ï¼Œé€‰æ‹©æ€§ä¿®å‰ªä¸é‡è¦çš„å±‚ï¼›å…¶æ¬¡åº”ç”¨QLoRAè¿›è¡Œä½ç§©é€‚åº”å’Œé‡åŒ–ï¼›æœ€åé€šè¿‡çŸ¥è¯†è’¸é¦å°†æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†å±‚ä¿®å‰ªã€ä½ç§©é€‚åº”å’ŒçŸ¥è¯†è’¸é¦ä¸‰ç§æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‹ç¼©æ•ˆç‡ï¼Œå¹¶ä¿æŒäº†ç¿»è¯‘è´¨é‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨æ¨¡å‹å‚æ•°å’Œå­˜å‚¨å ç”¨ä¸Šå®ç°äº†å¤§å¹…åº¦å‡å°‘ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å±‚ä¿®å‰ªè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºå±‚é‡è¦æ€§è¯„ä¼°çš„è¿­ä»£æ–¹æ³•ï¼›QLoRAä½¿ç”¨4ä½é‡åŒ–æŠ€æœ¯ä»¥å‡å°‘å­˜å‚¨éœ€æ±‚ï¼›çŸ¥è¯†è’¸é¦åˆ™é€šè¿‡è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œç¡®ä¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ä¿®å‰ªçš„å­¦ç”Ÿæ¨¡å‹åœ¨å‚æ•°å’Œå­˜å‚¨å ç”¨ä¸Šå‡å°‘äº†50%ï¼Œè€Œç¿»è¯‘è´¨é‡ä¿æŒåœ¨97-100%ä¹‹é—´ï¼Œå‡ ä¹ä¸æ•™å¸ˆæ¨¡å‹æ— å·®å¼‚ã€‚è¿™ä¸€æˆæœè¡¨æ˜ï¼Œæ¨¡å‹å‹ç¼©ä¸çŸ¥è¯†è’¸é¦çš„ç»“åˆèƒ½å¤Ÿæœ‰æ•ˆæå‡è¯­éŸ³ç¿»è¯‘ç³»ç»Ÿçš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®æ—¶è¯­éŸ³ç¿»è¯‘ã€è·¨è¯­è¨€æ²Ÿé€šå·¥å…·ä»¥åŠå¤šè¯­è¨€æ•™è‚²å¹³å°ã€‚é€šè¿‡æé«˜è¯­éŸ³ç¿»è¯‘æ¨¡å‹çš„æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå®ç°æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œæ¨åŠ¨å…¨çƒåŒ–äº¤æµä¸åˆä½œã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šè¯­è¨€å’Œåœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Efficient deployment of large audio-language models for speech translation remains challenging due to their significant computational requirements. In this paper, we address this challenge through our system submissions to the "Model Compression" track at the International Conference on Spoken Language Translation (IWSLT 2025). We experiment with a combination of approaches including iterative layer pruning based on layer importance evaluation, low-rank adaptation with 4-bit quantization (QLoRA), and knowledge distillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech translation into German and Chinese. Our pruned (student) models achieve up to a 50% reduction in both model parameters and storage footprint, while retaining 97-100% of the translation quality of the in-domain (teacher) models.

