---
layout: default
title: "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles"
---

# Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19914" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.19914v2</a>
  <a href="https://arxiv.org/pdf/2505.19914.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19914v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19914v2', 'Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, Mingxuan Wang

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-26 (Êõ¥Êñ∞: 2025-06-09)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Enigmata‰ª•ÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈÄªËæëÊé®ÁêÜ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ÁîüÊàêÂô®-È™åËØÅÂô®` `Â§ö‰ªªÂä°Â≠¶‰π†` `Êï∞Â≠¶Êé®ÁêÜ` `Âü∫ÂáÜËØÑ‰º∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êó†ÈúÄÈ¢ÜÂüüÁü•ËØÜÁöÑË∞úÈ¢òÊé®ÁêÜ‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®ËåÉÂõ¥„ÄÇ
2. EnigmataÈÄöËøáÊèê‰æõ‰∏Ä‰∏™ÁîüÊàêÂô®ÂíåÈ™åËØÅÂô®ÁöÑÁªÑÂêàÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰ª•ÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÁªèËøáËÆ≠ÁªÉÁöÑQwen2.5-32B-EnigmataÂú®Â§ö‰∏™Ë∞úÈ¢òÊé®ÁêÜÂü∫ÂáÜ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊ®°ÂûãÔºåÂπ∂Âú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Â±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÔºåÂ¶ÇOpenAIÁöÑo1ÂíåDeepSeekÁöÑR1ÔºåÂú®Êï∞Â≠¶ÂíåÁºñÁ®ãÁ≠âÈ´òÁ∫ßÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Êó†ÈúÄÈ¢ÜÂüüÁü•ËØÜÁöÑ‰∫∫Á±ªÂèØËß£Ë∞úÈ¢ò‰∏ä‰ªçÁÑ∂Â≠òÂú®Âõ∞Èöæ„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜEnigmataÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ∑•ÂÖ∑Â•ó‰ª∂ÔºåÊó®Âú®ÊèêÂçáLLMsÁöÑË∞úÈ¢òÊé®ÁêÜËÉΩÂäõ„ÄÇÂÆÉÂåÖÂê´36‰∏™‰ªªÂä°ÔºåÊ∂µÁõñ‰∏É‰∏™Á±ªÂà´ÔºåÊØè‰∏™‰ªªÂä°ÈÉΩÊúâ‰∏Ä‰∏™ÁîüÊàêÂô®ÔºåÂèØ‰ª•ÁîüÊàêÂÖ∑ÊúâÂèØÊéßÈöæÂ∫¶ÁöÑÊó†ÈôêÁ§∫‰æãÔºå‰ª•Âèä‰∏Ä‰∏™Âü∫‰∫éËßÑÂàôÁöÑÈ™åËØÅÂô®ÔºåÁî®‰∫éËá™Âä®ËØÑ‰º∞„ÄÇËøôÁßçÁîüÊàêÂô®-È™åËØÅÂô®ËÆæËÆ°ÊîØÊåÅÂèØÊâ©Â±ïÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÅÁªÜÁ≤íÂ∫¶ÂàÜÊûêÂíåÊó†ÁºùÁöÑÂèØÈ™åËØÅÂ•ñÂä±ÈõÜÊàê„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜEnigmata-EvalÔºå‰∏Ä‰∏™‰∏•Ê†ºÁöÑÂü∫ÂáÜÔºåÂπ∂ÂºÄÂèë‰∫Ü‰ºòÂåñÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•„ÄÇÁªèËøáËÆ≠ÁªÉÁöÑÊ®°ÂûãQwen2.5-32B-EnigmataÂú®Enigmata-Eval„ÄÅARC-AGIÂíåARC-AGI 2Á≠âË∞úÈ¢òÊé®ÁêÜÂü∫ÂáÜ‰∏äË°®Áé∞‰ºò‰∫éo3-mini-highÂíåo1„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êó†ÈúÄÈ¢ÜÂüüÁü•ËØÜÁöÑË∞úÈ¢òÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑ‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Ê≠§Á±ª‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®ÊΩúÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEnigmataÈÄöËøáËÆæËÆ°‰∏Ä‰∏™ÁîüÊàêÂô®ÂíåÈ™åËØÅÂô®ÁöÑÁªÑÂêàÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ∑•ÂÖ∑Â•ó‰ª∂ÔºåÊó®Âú®ÊèêÂçáLLMsÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõÔºåÊîØÊåÅÂèØÊâ©Â±ïÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÁîüÊàêÂô®Ê®°ÂùóÔºàÁî®‰∫éÁîüÊàêÂÖ∑ÊúâÂèØÊéßÈöæÂ∫¶ÁöÑË∞úÈ¢òÁ§∫‰æãÔºâÂíåÈ™åËØÅÂô®Ê®°ÂùóÔºàÁî®‰∫éËá™Âä®ËØÑ‰º∞ÁîüÊàêÁöÑÁ§∫‰æãÔºâÔºåÂπ∂ÁªìÂêàÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÁîüÊàêÂô®-È™åËØÅÂô®ËÆæËÆ°Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂú®Â§ö‰ªªÂä°ÁéØÂ¢É‰∏≠ËøõË°åÊúâÊïàÁöÑËÆ≠ÁªÉÂíåËØÑ‰º∞ÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÊ®°Âûã‰ΩøÁî®‰∫Ü‰ºòÂåñÁöÑÂ§ö‰ªªÂä°Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°ËÄÉËôë‰∫ÜÊé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÁîüÊàêÁ§∫‰æãÁöÑÂ§öÊ†∑ÊÄßÔºåÁΩëÁªúÁªìÊûÑÂàôÈíàÂØπÈÄªËæëÊé®ÁêÜËøõË°å‰∫Ü‰ºòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåQwen2.5-32B-EnigmataÂú®Enigmata-Eval„ÄÅARC-AGIÂíåARC-AGI 2Á≠âÂü∫ÂáÜ‰∏äÂùáË∂ÖË∂ä‰∫Üo3-mini-highÂíåo1ÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ÂàÜÂà´‰∏∫32.8%Âíå0.6%„ÄÇÊ≠§Â§ñÔºåËØ•Ê®°ÂûãÂú®Êõ¥Â§ßËßÑÊ®°ÁöÑÊ®°Âûã‰∏äËÆ≠ÁªÉÂêéÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÂú®È´òÁ∫ßÊï∞Â≠¶ÂíåSTEMÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂ±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÊ∏∏ÊàèËÆæËÆ°Âíå‰∫∫Â∑•Êô∫ËÉΩÂä©ÊâãÁ≠âÔºåËÉΩÂ§üÂ∏ÆÂä©ÂºÄÂèëÊõ¥Êô∫ËÉΩÁöÑÁ≥ªÁªüÔºåÊèêÂçáÂÖ∂Âú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÊú™Êù•ÔºåEnigmataÂèØËÉΩ‰ºöÊé®Âä®Êõ¥ÂπøÊ≥õÁöÑÈÄªËæëÊé®ÁêÜÁ†îÁ©∂Ôºå‰øÉËøõLLMsÂú®Â§öÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io.

