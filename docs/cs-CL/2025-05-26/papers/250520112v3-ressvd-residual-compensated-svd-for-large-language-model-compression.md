---
layout: default
title: "ResSVD: Residual Compensated SVD for Large Language Model Compression"
---

# ResSVD: Residual Compensated SVD for Large Language Model Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20112" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20112v3</a>
  <a href="https://arxiv.org/pdf/2505.20112.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20112v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20112v3', 'ResSVD: Residual Compensated SVD for Large Language Model Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haolei Bai, Siyong Jian, Tuo Liang, Yu Yin, Huan Wang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-12-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºResSVDä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å‹ç¼©ä¸­çš„æ®‹å·®æŸå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `å¥‡å¼‚å€¼åˆ†è§£` `æ®‹å·®çŸ©é˜µ` `è‡ªç„¶è¯­è¨€å¤„ç†` `åè®­ç»ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„SVDæ–¹æ³•åœ¨å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹æ—¶å¿½è§†äº†æˆªæ–­å¸¦æ¥çš„æ®‹å·®æŸå¤±ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. ResSVDé€šè¿‡åˆ©ç”¨æˆªæ–­è¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ®‹å·®çŸ©é˜µæ¥å‡å°‘æˆªæ–­æŸå¤±ï¼Œå¹¶é€‰æ‹©æ€§å‹ç¼©æ¨¡å‹çš„æœ€åå‡ å±‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒResSVDåœ¨å¤šä¸ªLLMå’ŒåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æå‡äº†å‹ç¼©æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶åºå¤§çš„ä½“ç§¯å’Œå†…å­˜éœ€æ±‚é™åˆ¶äº†å®é™…éƒ¨ç½²ï¼Œè¿«åˆ‡éœ€è¦é«˜æ•ˆçš„å‹ç¼©ç­–ç•¥ã€‚å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰èƒ½å¤Ÿå°†çŸ©é˜µåˆ†è§£ä¸ºæ­£äº¤æˆåˆ†ï¼Œé€‚åˆç”¨äºLLMçš„ä½ç§©è¿‘ä¼¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SVDæ–¹æ³•å¿½è§†äº†æˆªæ–­è¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ®‹å·®çŸ©é˜µï¼Œå¯¼è‡´æ˜¾è‘—çš„æˆªæ–­æŸå¤±ã€‚æ­¤å¤–ï¼Œå¯¹æ¨¡å‹æ‰€æœ‰å±‚è¿›è¡Œå‹ç¼©ä¼šå¯¼è‡´æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒSVDåŸºç¡€çš„LLMå‹ç¼©æ–¹æ³•ResSVDï¼Œåˆ©ç”¨æˆªæ–­è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ®‹å·®çŸ©é˜µæ¥å‡å°‘æˆªæ–­æŸå¤±ï¼Œå¹¶åœ¨å›ºå®šçš„æ•´ä½“å‹ç¼©æ¯”ä¸‹é€‰æ‹©æ€§åœ°å‹ç¼©æ¨¡å‹çš„æœ€åå‡ å±‚ï¼Œä»è€Œå‡è½»è¯¯å·®ä¼ æ’­ï¼Œæ˜¾è‘—æé«˜å‹ç¼©æ¨¡å‹çš„æ€§èƒ½ã€‚å¯¹å¤šç§LLMå®¶æ—å’Œå¤šä¸ªåŸºå‡†æ•°æ®é›†çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒResSVDåœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰å¯¹æ¯”æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å®é™…æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©ä¸­çš„æ®‹å·®æŸå¤±é—®é¢˜ã€‚ç°æœ‰çš„SVDæ–¹æ³•åœ¨æˆªæ–­æ—¶æœªè€ƒè™‘æ®‹å·®çŸ©é˜µï¼Œå¯¼è‡´å‹ç¼©åæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šResSVDçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æˆªæ–­è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ®‹å·®çŸ©é˜µæ¥å‡å°‘æˆªæ–­æŸå¤±ï¼ŒåŒæ—¶åœ¨å›ºå®šçš„æ•´ä½“å‹ç¼©æ¯”ä¸‹é€‰æ‹©æ€§åœ°å‹ç¼©æ¨¡å‹çš„æœ€åå‡ å±‚ï¼Œä»¥å‡è½»è¯¯å·®ä¼ æ’­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šResSVDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¿›è¡ŒSVDåˆ†è§£ï¼Œæ¥ç€è®¡ç®—æ®‹å·®çŸ©é˜µï¼Œæœ€åæ ¹æ®è®¾å®šçš„å‹ç¼©æ¯”é€‰æ‹©æ€§å‹ç¼©æ¨¡å‹çš„æœ€åå‡ å±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šResSVDçš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†æ®‹å·®çŸ©é˜µçš„æ¦‚å¿µï¼Œä»¥å‡å°‘æˆªæ–­æŸå¤±ï¼Œå¹¶é€šè¿‡é€‰æ‹©æ€§å‹ç¼©æ¥é¿å…æ€§èƒ½ä¸‹é™ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å…¨å±‚å‹ç¼©æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒResSVDé‡‡ç”¨äº†å›ºå®šçš„æ•´ä½“å‹ç¼©æ¯”ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‹ç¼©ç‡ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒResSVDåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå‡ä¼˜äºç°æœ‰å‹ç¼©æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ç›¸åŒå‹ç¼©æ¯”ä¸‹ï¼Œæ¨¡å‹æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ResSVDçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ã€‚é€šè¿‡æœ‰æ•ˆå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒResSVDèƒ½å¤Ÿæå‡æ¨¡å‹çš„éƒ¨ç½²æ•ˆç‡ï¼Œé™ä½è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. Nevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. Singular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy. However, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. Additionally, compressing all layers of the model results in severe performance degradation. To overcome these limitations, we propose ResSVD, a new post-training SVD-based LLM compression method. Specifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. Moreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and significantly improves the performance of compressed models. Comprehensive evaluations of ResSVD on diverse LLM families and multiple benchmark datasets indicate that ResSVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.

