---
layout: default
title: Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks
---

# Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20047" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20047v1</a>
  <a href="https://arxiv.org/pdf/2505.20047.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20047v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20047v1', 'Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Debargha Ganguly, Vikash Singh, Sreehari Sankar, Biyao Zhang, Xuecen Zhang, Srinivasan Iyengar, Xiaotian Han, Amit Sharma, Shivkumar Kalyanaraman, Vipin Chaudhary

**åˆ†ç±»**: cs.CL, cs.AI, cs.LO, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¦‚ç‡ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•ä»¥è§£å†³LLMåœ¨è‡ªåŠ¨æ¨ç†ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨æ¨ç†` `ä¸ç¡®å®šæ€§é‡åŒ–` `æ¦‚ç‡ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•` `å½¢å¼éªŒè¯` `ä¿¡å·èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ­£å¼è§„èŒƒæ—¶å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œæ— æ³•æ»¡è¶³å½¢å¼éªŒè¯çš„ç¡®å®šæ€§è¦æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¦‚ç‡ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•æ¡†æ¶ï¼Œä»¥å»ºæ¨¡LLMè¾“å‡ºå¹¶é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œè¿›è€Œæé«˜è‡ªåŠ¨æ¨ç†çš„å¯é æ€§ã€‚
3. é€šè¿‡å®éªŒï¼Œå‘ç°ä¿¡å·èåˆæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½é”™è¯¯ç‡ï¼Œé€»è¾‘ä»»åŠ¡å‡†ç¡®ç‡æå‡34.8%ï¼Œäº‹å®ä»»åŠ¡ä¸‹é™44.5%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæ­£å¼è§„èŒƒæ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä½†å…¶æ¦‚ç‡æ€§è´¨ä¸å½¢å¼éªŒè¯æ‰€éœ€çš„ç¡®å®šæ€§ä¿è¯ä¹‹é—´å­˜åœ¨æ ¹æœ¬çŸ›ç›¾ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿè¯„ä¼°äº”ç§å‰æ²¿LLMï¼Œæ¢è®¨äº†LLMç”Ÿæˆçš„æ­£å¼æ–‡æ¡£ä¸­çš„å¤±è´¥æ¨¡å¼å’Œä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰ã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºå¯æ»¡è¶³æ€§æ¨¡ç†è®ºï¼ˆSMTï¼‰çš„è‡ªåŠ¨å½¢å¼åŒ–åœ¨é€»è¾‘ä»»åŠ¡ä¸Šå‡†ç¡®ç‡æé«˜34.8%ï¼Œè€Œåœ¨äº‹å®ä»»åŠ¡ä¸Šä¸‹é™44.5%ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¦‚ç‡ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•ï¼ˆPCFGï¼‰æ¡†æ¶æ¥å»ºæ¨¡LLMè¾“å‡ºï¼Œå½¢æˆäº†æ›´ç²¾ç»†çš„ä¸ç¡®å®šæ€§åˆ†ç±»ã€‚æœ€åï¼Œè½»é‡çº§ä¿¡å·èåˆæ–¹æ³•æ˜¾è‘—å‡å°‘äº†é”™è¯¯ï¼ˆ14-100%ï¼‰ï¼Œå°†LLMé©±åŠ¨çš„å½¢å¼åŒ–è½¬å˜ä¸ºå¯é çš„å·¥ç¨‹å­¦ç§‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨æ¨ç†ä»»åŠ¡ä¸­ç”Ÿæˆçš„æ­£å¼æ–‡æ¡£çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é‡åŒ–ä¸ç¡®å®šæ€§æ—¶æœªèƒ½æœ‰æ•ˆè¯†åˆ«é”™è¯¯ï¼Œå¯¼è‡´å½¢å¼éªŒè¯çš„å¯é æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ¦‚ç‡ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•ï¼ˆPCFGï¼‰æ¡†æ¶ï¼Œä»¥å»ºæ¨¡LLMè¾“å‡ºå¹¶å½¢æˆä¸ç¡®å®šæ€§åˆ†ç±»ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„é”™è¯¯è¯†åˆ«å’Œé€‰æ‹©æ€§éªŒè¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬LLMè¾“å‡ºçš„ç”Ÿæˆã€PCFGå»ºæ¨¡ã€ä¸ç¡®å®šæ€§é‡åŒ–å’Œä¿¡å·èåˆå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆç”Ÿæˆæ–‡æ¡£ï¼Œç„¶åé€šè¿‡PCFGåˆ†æè¾“å‡ºçš„ä¸ç¡®å®šæ€§ï¼Œæœ€åèåˆä¸åŒä¿¡å·è¿›è¡Œé€‰æ‹©æ€§éªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥PCFGæ¡†æ¶æ¥é‡åŒ–LLMè¾“å‡ºçš„ä¸ç¡®å®šæ€§ï¼Œå½¢æˆäº†æ–°çš„ä¸ç¡®å®šæ€§åˆ†ç±»ä½“ç³»ï¼Œä¸ä¼ ç»Ÿçš„UQæŠ€æœ¯ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«ä»»åŠ¡ç›¸å…³çš„é”™è¯¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºè¯­æ³•ç†µçš„ä¿¡å·é‡åŒ–æ–¹æ³•ï¼Œå¹¶ç»“åˆAUROCæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿åœ¨é€»è¾‘ä»»åŠ¡ä¸­è¾¾åˆ°é«˜äº0.93çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è¯¯æŠ¥ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºPCFGçš„ä¿¡å·èåˆæ–¹æ³•èƒ½å¤Ÿå°†é”™è¯¯ç‡é™ä½14-100%ï¼Œåœ¨é€»è¾‘ä»»åŠ¡ä¸­å‡†ç¡®ç‡æå‡34.8%ï¼Œè€Œåœ¨äº‹å®ä»»åŠ¡ä¸­å°½ç®¡å‡ºç°ä¸‹é™ï¼Œä½†æ•´ä½“éªŒè¯è¿‡ç¨‹çš„å¯é æ€§æ˜¾è‘—å¢å¼ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–è½¯ä»¶éªŒè¯ã€æ™ºèƒ½åˆçº¦ç”Ÿæˆå’Œæ³•å¾‹æ–‡æ¡£åˆ†æç­‰ã€‚é€šè¿‡æé«˜LLMåœ¨ç”Ÿæˆæ­£å¼æ–‡æ¡£æ—¶çš„å¯é æ€§ï¼Œèƒ½å¤Ÿä¸ºå·¥ç¨‹å¸ˆæä¾›æ›´å¯ä¿¡çš„å·¥å…·ï¼Œæ¨åŠ¨è‡ªåŠ¨æ¨ç†æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.

