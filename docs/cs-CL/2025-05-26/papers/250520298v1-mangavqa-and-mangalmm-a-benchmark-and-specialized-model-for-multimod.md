---
layout: default
title: MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding
---

# MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20298" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20298v1</a>
  <a href="https://arxiv.org/pdf/2505.20298.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20298v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20298v1', 'MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jeonghun Baek, Kazuki Egashira, Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Hikaru Ikuta, Kiyoharu Aizawa

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: 20 pages, 11 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMangaVQAå’ŒMangaLMMä»¥è§£å†³å¤šæ¨¡æ€æ¼«ç”»ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `æ¼«ç”»åˆ†æ` `è§†è§‰é—®ç­”` `æ–‡æœ¬è¯†åˆ«` `æ¨¡å‹å¾®è°ƒ` `åŸºå‡†æµ‹è¯•` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£å¤æ‚çš„æ¼«ç”»å™äº‹æ—¶å­˜åœ¨å±€é™ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å›¾åƒä¸æ–‡æœ¬çš„ç»“åˆã€‚
2. è®ºæ–‡æå‡ºäº†MangaOCRå’ŒMangaVQAä¸¤ä¸ªåŸºå‡†ï¼ŒMangaLMMæ¨¡å‹åˆ™ä¸“é—¨é’ˆå¯¹æ¼«ç”»ç†è§£è¿›è¡Œå¾®è°ƒï¼Œæå‡äº†æ¨¡å‹çš„å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMangaLMMåœ¨æ¼«ç”»ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨ä¸Šä¸‹æ–‡ç†è§£å’Œè§†è§‰é—®ç­”æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¼«ç”»æ˜¯ä¸€ç§ä¸°å¯Œçš„å¤šæ¨¡æ€å™äº‹å½¢å¼ï¼Œç»“åˆäº†å¤æ‚çš„å›¾åƒå’Œæ–‡æœ¬ã€‚ä¸ºäº†å¸®åŠ©å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä»¥äººç±»æ°´å¹³ç†è§£è¿™äº›å™äº‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªåŸºå‡†ï¼šMangaOCRï¼Œä¸“æ³¨äºé¡µé¢å†…æ–‡æœ¬è¯†åˆ«ï¼›MangaVQAï¼Œæ—¨åœ¨é€šè¿‡è§†è§‰é—®ç­”è¯„ä¼°ä¸Šä¸‹æ–‡ç†è§£ã€‚MangaVQAåŒ…å«526å¯¹é«˜è´¨é‡çš„æ‰‹å·¥æ„å»ºé—®ç­”å¯¹ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·çš„å™äº‹å’Œè§†è§‰åœºæ™¯ä¸­è¿›è¡Œå¯é è¯„ä¼°ã€‚åŸºäºè¿™äº›åŸºå‡†ï¼Œæˆ‘ä»¬å¼€å‘äº†MangaLMMï¼Œè¿™æ˜¯ä¸€ä¸ªä»å¼€æºLMM Qwen2.5-VLå¾®è°ƒè€Œæ¥çš„æ¼«ç”»ä¸“ç”¨æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†è¿™ä¸¤ä¸ªä»»åŠ¡ã€‚é€šè¿‡ä¸GPT-4oå’ŒGemini 2.5ç­‰ä¸“æœ‰æ¨¡å‹çš„æ¯”è¾ƒå®éªŒï¼Œæˆ‘ä»¬è¯„ä¼°äº†LMMså¯¹æ¼«ç”»çš„ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†å’Œæ¨¡å‹ä¸ºåœ¨æ¼«ç”»è¿™ä¸€ä¸°å¯Œå™äº‹é¢†åŸŸä¸­è¯„ä¼°å’Œæ¨è¿›LMMsæä¾›äº†å…¨é¢åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£æ¼«ç”»å™äº‹æ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å›¾åƒä¸æ–‡æœ¬çš„å¤æ‚ç»“åˆå¯¹æ¨¡å‹ç†è§£èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ¼«ç”»æ—¶å¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸“é—¨çš„åŸºå‡†å’Œå¾®è°ƒæ¨¡å‹ï¼Œä½¿LMMsèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¼«ç”»ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚MangaOCRå’ŒMangaVQAçš„å¼•å…¥ä¸ºæ¨¡å‹æä¾›äº†æ˜ç¡®çš„è¯„ä¼°æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šMangaOCRç”¨äºæ–‡æœ¬è¯†åˆ«ï¼ŒMangaVQAç”¨äºä¸Šä¸‹æ–‡ç†è§£çš„è§†è§‰é—®ç­”ã€‚MangaLMMæ¨¡å‹åœ¨è¿™ä¸¤ä¸ªæ¨¡å—çš„åŸºç¡€ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºMangaVQAåŸºå‡†çš„åˆ›å»ºå’ŒMangaLMMæ¨¡å‹çš„å¼€å‘ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¼«ç”»ç‰¹æœ‰çš„å™äº‹ç»“æ„ä¸­è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸“æ³¨äºæ¼«ç”»è¿™ä¸€ç‰¹å®šé¢†åŸŸçš„å¤šæ¨¡æ€ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é’ˆå¯¹æ¼«ç”»ç‰¹å¾çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å›¾åƒä¸æ–‡æœ¬çš„ç»“åˆã€‚æ­¤å¤–ï¼Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾ç»†è°ƒä¼˜ï¼Œä»¥é€‚åº”æ¼«ç”»çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMangaLMMåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å¦‚GPT-4oå’ŒGemini 2.5ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦15%ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†ä¸“é—¨é’ˆå¯¹æ¼«ç”»ç†è§£çš„æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¼«ç”»åˆ›ä½œè¾…åŠ©å·¥å…·ã€æ•™è‚²é¢†åŸŸçš„å¤šæ¨¡æ€å­¦ä¹ èµ„æºä»¥åŠæ–‡åŒ–ä¼ æ’­ä¸­çš„æ¼«ç”»ç†è§£ç³»ç»Ÿã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹æ¼«ç”»çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å¸®åŠ©åˆ›ä½œè€…æ›´å¥½åœ°åæ€å’Œå®Œå–„ä»–ä»¬çš„æ•…äº‹ï¼ŒåŒæ—¶ä¹Ÿä¸ºæ¼«ç”»çˆ±å¥½è€…æä¾›æ›´ä¸°å¯Œçš„äº’åŠ¨ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two benchmarks for multimodal manga understanding: MangaOCR, which targets in-page text recognition, and MangaVQA, a novel benchmark designed to evaluate contextual understanding through visual question answering. MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. Building on these benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive experiments, including comparisons with proprietary models such as GPT-4o and Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga.

