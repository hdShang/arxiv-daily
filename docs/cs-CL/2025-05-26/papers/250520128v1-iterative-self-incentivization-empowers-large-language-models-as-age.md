---
layout: default
title: Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers
---

# Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20128" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20128v1</a>
  <a href="https://arxiv.org/pdf/2505.20128.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20128v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20128v1', 'Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengliang Shi, Lingyong Yan, Dawei Yin, Suzan Verberne, Maarten de Rijke, Zhaochun Ren

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: Working in process

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEXSEARCHæ¡†æ¶ä»¥è§£å†³LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ä¿¡æ¯æ£€ç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡æ¯æ£€ç´¢` `è‡ªæ¿€åŠ±å­¦ä¹ ` `å¤šè·³æŸ¥è¯¢` `çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨LLMè¿›è¡Œä¿¡æ¯æ£€ç´¢ï¼Œå°¤å…¶æ˜¯å¤šè·³æŸ¥è¯¢å’Œæ— å…³å†…å®¹çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºEXSEARCHæ¡†æ¶ï¼Œé€šè¿‡è‡ªæ¿€åŠ±è¿‡ç¨‹ä½¿LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€æ­¥æ£€ç´¢æœ‰ç”¨ä¿¡æ¯ï¼Œæå‡æ£€ç´¢èƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªçŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEXSEARCHæ˜¾è‘—è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼Œç²¾ç¡®åŒ¹é…å¾—åˆ†æé«˜äº†7.8%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²å¹¿æ³›åº”ç”¨äºä¿¡æ¯æ£€ç´¢é¢†åŸŸï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸­æœ‰æ•ˆè·å–å‡†ç¡®çŸ¥è¯†ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¤šè·³æŸ¥è¯¢çš„å¤æ‚æ€§å’Œæ£€ç´¢å†…å®¹çš„æ— å…³æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†EXSEARCHï¼Œä¸€ä¸ªè‡ªä¸»æœç´¢æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡è‡ªæ¿€åŠ±çš„æ–¹å¼æ£€ç´¢æœ‰ç”¨ä¿¡æ¯ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¹¿ä¹‰æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ï¼ŒLLMåœ¨æ¯ä¸€æ­¥ç”Ÿæˆå¤šä¸ªæœç´¢è½¨è¿¹å¹¶ä¸ºå…¶åˆ†é…é‡è¦æ€§æƒé‡ï¼Œè¿›è€Œé€šè¿‡åŠ æƒæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œå½¢æˆè‡ªæˆ‘æ¿€åŠ±çš„å¾ªç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEXSEARCHåœ¨å››ä¸ªçŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œç²¾ç¡®åŒ¹é…å¾—åˆ†æå‡7.8%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè·³æŸ¥è¯¢å’Œæ£€ç´¢å†…å®¹æ— å…³æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨LLMçš„æ½œåŠ›ï¼Œå¯¼è‡´ä¿¡æ¯æ£€ç´¢æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEXSEARCHæ¡†æ¶é€šè¿‡è‡ªæ¿€åŠ±çš„æ–¹å¼ï¼Œä½¿LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€å†³å®šæ£€ç´¢å†…å®¹ï¼Œä»è€Œé€æ­¥æå‡å…¶ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ã€‚è¯¥è®¾è®¡æ—¨åœ¨è®©æ¨¡å‹åœ¨æ¯ä¸€æ­¥éƒ½èƒ½æ ¹æ®å½“å‰æ¨ç†çŠ¶æ€è¿›è¡Œæœ‰æ•ˆçš„ä¿¡æ¯è·å–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEXSEARCHçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ€è€ƒï¼ˆå†³å®šæ£€ç´¢å†…å®¹ï¼‰ã€æœç´¢ï¼ˆè§¦å‘å¤–éƒ¨æ£€ç´¢å™¨ï¼‰å’Œè®°å½•ï¼ˆæå–æ”¯æŒæ¨ç†çš„ç»†ç²’åº¦è¯æ®ï¼‰ã€‚æ¡†æ¶é‡‡ç”¨å¹¿ä¹‰æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ï¼Œåˆ†ä¸ºEæ­¥å’ŒMæ­¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè‡ªæ¿€åŠ±å¾ªç¯çš„è®¾è®¡ï¼ŒLLMé€šè¿‡ç”Ÿæˆçš„æœç´¢è½¨è¿¹è¿›è¡Œè‡ªæˆ‘å­¦ä¹ ï¼Œè¿™ä¸€è¿‡ç¨‹ä¸ä¼ ç»Ÿçš„è®­ç»ƒæ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ£€ç´¢èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Eæ­¥ä¸­ï¼ŒLLMç”Ÿæˆå¤šä¸ªæœç´¢è½¨è¿¹å¹¶ä¸ºå…¶åˆ†é…é‡è¦æ€§æƒé‡ï¼›åœ¨Mæ­¥ä¸­ï¼Œä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿä»è‡ªèº«ç”Ÿæˆçš„æ•°æ®ä¸­ä¸æ–­å­¦ä¹ å’Œæ”¹è¿›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å››ä¸ªçŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEXSEARCHæ¡†æ¶æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œç²¾ç¡®åŒ¹é…å¾—åˆ†æå‡äº†7.8%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒEXSEARCHåœ¨å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœç´¢å¼•æ“ã€é—®ç­”ç³»ç»Ÿå’ŒçŸ¥è¯†ç®¡ç†å¹³å°ç­‰ã€‚EXSEARCHæ¡†æ¶èƒ½å¤Ÿæå‡ä¿¡æ¯æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¤šå¤æ‚çš„çŸ¥è¯†æ£€ç´¢åœºæ™¯ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have been widely integrated into information retrieval to advance traditional techniques. However, effectively enabling LLMs to seek accurate knowledge in complex tasks remains a challenge due to the complexity of multi-hop queries as well as the irrelevant retrieved content. To address these limitations, we propose EXSEARCH, an agentic search framework, where the LLM learns to retrieve useful information as the reasoning unfolds through a self-incentivized process. At each step, the LLM decides what to retrieve (thinking), triggers an external retriever (search), and extracts fine-grained evidence (recording) to support next-step reasoning. To enable LLM with this capability, EXSEARCH adopts a Generalized Expectation-Maximization algorithm. In the E-step, the LLM generates multiple search trajectories and assigns an importance weight to each; the M-step trains the LLM on them with a re-weighted loss function. This creates a self-incentivized loop, where the LLM iteratively learns from its own generated data, progressively improving itself for search. We further theoretically analyze this training process, establishing convergence guarantees. Extensive experiments on four knowledge-intensive benchmarks show that EXSEARCH substantially outperforms baselines, e.g., +7.8% improvement on exact match score. Motivated by these promising results, we introduce EXSEARCH-Zoo, an extension that extends our method to broader scenarios, to facilitate future work.

