---
layout: default
title: Reasoning LLMs are Wandering Solution Explorers
---

# Reasoning LLMs are Wandering Solution Explorers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20296" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20296v1</a>
  <a href="https://arxiv.org/pdf/2505.20296.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20296v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20296v1', 'Reasoning LLMs are Wandering Solution Explorers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahao Lu, Ziwei Xu, Mohan Kankanhalli

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: 71 pages, 14 figures, 2 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç³»ç»Ÿæ€§é—®é¢˜è§£å†³æ¡†æ¶ä»¥æå‡æ¨ç†LLMçš„æ¢ç´¢èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†LLM` `ç³»ç»Ÿæ€§é—®é¢˜è§£å†³` `è§£ç©ºé—´æ¢ç´¢` `è¯„ä¼°æ¡†æ¶` `æ€§èƒ½åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†LLMåœ¨ç³»ç»Ÿæ€§æ¢ç´¢è§£ç©ºé—´æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹ä¸­çš„æ— æ•ˆæ­¥éª¤å’Œå†—ä½™æ¢ç´¢ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå¼ºè°ƒç³»ç»Ÿæ€§é—®é¢˜è§£å†³çš„é‡è¦æ€§ï¼Œå¹¶å»ºè®®å…³æ³¨æ¨ç†è¿‡ç¨‹çš„ç»“æ„ã€‚
3. é€šè¿‡å¯¹å¤šç§LLMçš„åˆ†æï¼Œå‘ç°å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œå‘¼åæ”¹è¿›è¯„ä¼°æ ‡å‡†ä»¥åæ˜ çœŸå®æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡é“¾å¼æ€ç»´æç¤ºå’Œæ ‘çŠ¶æ¨ç†ç­‰æµ‹è¯•æ—¶è®¡ç®—ï¼ˆTTCï¼‰æŠ€æœ¯å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æŒ‡å‡ºç°æœ‰çš„æ¨ç†LLMï¼ˆRLLMsï¼‰åœ¨ç³»ç»Ÿæ€§æ¢ç´¢è§£ç©ºé—´æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬å½¢å¼åŒ–äº†ç³»ç»Ÿæ€§é—®é¢˜è§£å†³çš„å®šä¹‰ï¼Œå¹¶è¯†åˆ«äº†å¸¸è§çš„å¤±è´¥æ¨¡å¼ï¼Œæ­ç¤ºäº†æ¨ç†LLMæ›´åƒæ˜¯æ¸¸è¡è€…è€Œéç³»ç»Ÿæ€§æ¢ç´¢è€…ã€‚é€šè¿‡å¯¹å¤šç§æœ€å…ˆè¿›LLMçš„å®šæ€§å’Œå®šé‡åˆ†æï¼Œæˆ‘ä»¬å‘ç°äº†æŒç»­å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚æ— æ•ˆçš„æ¨ç†æ­¥éª¤ã€å†—ä½™çš„æ¢ç´¢ã€å¹»è§‰æˆ–ä¸çœŸå®çš„ç»“è®ºç­‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°çœ‹ä¼¼ä¼˜ç§€ï¼Œä½†éšç€å¤æ‚åº¦çš„å¢åŠ ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å€¡å¯¼æ–°çš„è¯„ä¼°æŒ‡æ ‡å’Œå·¥å…·ï¼Œä¸ä»…è¯„ä¼°æœ€ç»ˆè¾“å‡ºï¼Œè¿˜è¦å…³æ³¨æ¨ç†è¿‡ç¨‹çš„ç»“æ„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨ç†LLMåœ¨ç³»ç»Ÿæ€§é—®é¢˜è§£å†³ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬åœ¨è§£ç©ºé—´æ¢ç´¢æ—¶çš„æ— æ•ˆå’Œå†—ä½™è¡Œä¸ºã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹çš„æ·±å…¥åˆ†æï¼Œå¯¼è‡´åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å½¢å¼åŒ–ç³»ç»Ÿæ€§é—®é¢˜è§£å†³çš„æ¦‚å¿µï¼Œå¹¶é€šè¿‡å®šæ€§å’Œå®šé‡åˆ†æè¯†åˆ«æ¨ç†LLMçš„å¸¸è§å¤±è´¥æ¨¡å¼ï¼Œä»¥æ­¤ä¸ºåŸºç¡€æå‡ºæ–°çš„è¯„ä¼°æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹å¤šç§LLMçš„åˆ†æï¼Œåˆ†ä¸ºå®šæ€§åˆ†æå’Œå®šé‡è¯„ä¼°ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼Œåˆ†åˆ«å…³æ³¨æ¨ç†æ­¥éª¤çš„æœ‰æ•ˆæ€§å’Œç»“æœçš„å¯é æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå¼ºè°ƒæ¨ç†è¿‡ç¨‹çš„ç»“æ„æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆè¾“å‡ºçš„è´¨é‡ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„è¯„ä¼°æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥é‡åŒ–æ¨ç†è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ¨ç†æ­¥éª¤çš„æœ‰æ•ˆæ€§ã€å†—ä½™æ¢ç´¢çš„é¢‘ç‡ç­‰ï¼Œç¡®ä¿å…¨é¢åæ˜ æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨ç†LLMåœ¨ç®€å•ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸‹é™å¹…åº¦å¯è¾¾50%ä»¥ä¸Šã€‚é€šè¿‡æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«æ¨¡å‹çš„æ¨ç†ç¼ºé™·ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿå’Œå¤æ‚å†³ç­–æ”¯æŒç³»ç»Ÿã€‚é€šè¿‡æå‡æ¨ç†LLMçš„ç³»ç»Ÿæ€§æ¢ç´¢èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä»è€Œä¸ºå®é™…åº”ç”¨æä¾›æ›´å¼ºçš„æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated impressive reasoning abilities through test-time computation (TTC) techniques such as chain-of-thought prompting and tree-based reasoning. However, we argue that current reasoning LLMs (RLLMs) lack the ability to systematically explore the solution space. This paper formalizes what constitutes systematic problem solving and identifies common failure modes that reveal reasoning LLMs to be wanderers rather than systematic explorers. Through qualitative and quantitative analysis across multiple state-of-the-art LLMs, we uncover persistent issues: invalid reasoning steps, redundant explorations, hallucinated or unfaithful conclusions, and so on. Our findings suggest that current models' performance can appear to be competent on simple tasks yet degrade sharply as complexity increases. Based on the findings, we advocate for new metrics and tools that evaluate not just final outputs but the structure of the reasoning process itself.

