---
layout: default
title: "Thinking with Visual Abstract: Enhancing Multimodal Reasoning via Visual Abstraction"
---

# Thinking with Visual Abstract: Enhancing Multimodal Reasoning via Visual Abstraction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20164" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20164v3</a>
  <a href="https://arxiv.org/pdf/2505.20164.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20164v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20164v3', 'Thinking with Visual Abstract: Enhancing Multimodal Reasoning via Visual Abstraction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, Yang Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-12-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰æŠ½è±¡æ€ç»´ä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æŠ½è±¡` `å¤šæ¨¡æ€æ¨ç†` `å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æ„ŸçŸ¥` `è®¤çŸ¥ç§‘å­¦` `ä¿¡æ¯ç®€åŒ–` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä¿¡æ¯æ—¶å¸¸å› å†—ä½™ä¿¡æ¯è€Œé™ä½å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹å¤æ‚åŒ–ã€‚
2. æœ¬æ–‡æå‡ºè§†è§‰æŠ½è±¡æ€ç»´ï¼ˆVATï¼‰ï¼Œé€šè¿‡è§†è§‰æŠ½è±¡æç¤ºMLLMsï¼Œä¿ƒä½¿æ¨¡å‹èšç„¦äºé‡è¦çš„è§†è§‰å…ƒç´ ï¼Œç®€åŒ–æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVATåœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†ä»»åŠ¡ä¸­å¹³å‡æå‡2.21%ï¼Œä¸”ä½¿ç”¨çš„tokenæ›´å°‘ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ¨ç†æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾åƒé€šå¸¸ä¼ è¾¾æ¯”æ–‡æœ¬æ›´ä¸°å¯Œçš„ç»†èŠ‚ï¼Œä½†å¾€å¾€åŒ…å«å†—ä½™ä¿¡æ¯ï¼Œè¿™å¯èƒ½é™ä½å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ã€‚é¢å¯¹å†—é•¿æˆ–å¤æ‚çš„ä¿¡æ¯æ—¶ï¼Œäººç±»å€¾å‘äºä½¿ç”¨æŠ½è±¡æ€ç»´å°†å…¶è½¬åŒ–ä¸ºç®€å•è€Œç®€æ´çš„æ‘˜è¦ã€‚å—æ­¤è®¤çŸ¥ç­–ç•¥çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°èŒƒå¼ï¼Œé€šè¿‡è§†è§‰æŠ½è±¡æç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„è§†è§‰æ¨ç†æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰æŠ½è±¡æ€ç»´ï¼ˆVATï¼‰åœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†ä¸åŒMLLMsçš„è¡¨ç°ï¼Œå¹³å‡æå‡2.21%ï¼Œä¸”åœ¨æ€§èƒ½æå‡çš„åŒæ—¶å‡å°‘äº†tokenä½¿ç”¨ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è§†è§‰æŠ½è±¡æ€ç»´çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é¼“åŠ±ä»äººç±»è®¤çŸ¥çš„è§’åº¦è¿›ä¸€æ­¥æ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†èŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ–¹æ³•å› å†—ä½™ä¿¡æ¯å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ä¿¡æ¯æ—¶çš„æ¨ç†å¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è§†è§‰æŠ½è±¡æ€ç»´ï¼ˆVATï¼‰ï¼Œé¼“åŠ±æ¨¡å‹å…³æ³¨æ›´é‡è¦çš„è§†è§‰å…ƒç´ å’Œç»“æ„ç‰¹å¾ï¼Œä»è€Œç®€åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæå‡æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰æŠ½è±¡ç”Ÿæˆæ¨¡å—å’Œå¤šæ¨¡æ€æ¨ç†æ¨¡å—ï¼Œå‰è€…è´Ÿè´£æå–å’Œç®€åŒ–è§†è§‰ä¿¡æ¯ï¼Œåè€…åˆ™åŸºäºç®€åŒ–çš„ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šVATçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡è§†è§‰æŠ½è±¡æ›¿ä»£ä¼ ç»Ÿçš„æ˜¾å¼æ€ç»´æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œæå‡æ¨ç†æ•ˆç‡ä¸æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è§†è§‰æŠ½è±¡çš„ç”Ÿæˆï¼Œå¹¶è°ƒæ•´äº†ç½‘ç»œç»“æ„ä»¥å¢å¼ºå¯¹é‡è¦è§†è§‰ç‰¹å¾çš„æå–èƒ½åŠ›ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒVATåœ¨æ€§èƒ½ä¸Šä¼˜äºChain-of-thoughtç­‰ä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVATåœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†ä»»åŠ¡ä¸­å¹³å‡æå‡äº†2.21%ï¼Œè¶…è¶Šäº†GPT-5åŸºçº¿ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½æå‡çš„åŒæ—¶å‡å°‘äº†tokençš„ä½¿ç”¨ã€‚è¿™äº›ç»“æœè¡¨æ˜VATåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰ï¼Œèƒ½å¤Ÿåœ¨éœ€è¦å¤„ç†å¤æ‚è§†è§‰ä¿¡æ¯çš„åœºæ™¯ä¸­æä¾›æ›´é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚æœªæ¥ï¼ŒVATå¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€å­¦ä¹ å’Œæ¨ç†æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Images usually convey richer detail than text, but often include redundant information, which potentially downgrades multimodal reasoning performance. When faced with lengthy or complex messages, humans tend to employ abstract thinking to convert them into simple and concise abstracts. Inspired by this cognitive strategy, we introduce a novel paradigm to elicit the ability to Think with Visual Abstract (VAT), by prompting Multimodal Large Language Models (MLLMs) with visual abstract instead of explicit verbal thoughts or elaborate guidance, permitting a more efficient visual reasoning mechanism via concentrated perception. VAT encourages models to focus on more essential visual elements, concepts and structural features by undermining redundant information compared with explicit thinking methods, such as Chain-of-thought (CoT) and tool-using approaches, that increase the complexity of reasoning process via inserting verbose intermediate steps and external knowledge. Experimental results show that VAT consistently empowers different MLLMs in visual perception and reasoning tasks. VAT achieves an average gain of $2.21\%$ over GPT-5 baseline, surpassing the gain of CoT, demonstrating that VAT better enhances multimodal task performance of MLLMs. Additionally, VAT spends fewer tokens while achieving higher performance. These findings highlight the effectiveness of visual abstract thinking and encourage further exploration of more diverse reasoning paradigms from the perspective of human cognition.

