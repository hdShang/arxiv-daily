---
layout: default
title: "THiNK: Can Large Language Models Think-aloud?"
---

# THiNK: Can Large Language Models Think-aloud?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20184" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20184v1</a>
  <a href="https://arxiv.org/pdf/2505.20184.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20184v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20184v1', 'THiNK: Can Large Language Models Think-aloud?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongan Yu, Mengqian Wu, Yiran Lin, Nikki G. Lobczowski

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTHiNKæ¡†æ¶ä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜é˜¶æ€ç»´èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é«˜é˜¶æ€ç»´` `è¯„ä¼°æ¡†æ¶` `å¸ƒé²å§†åˆ†ç±»æ³•` `åé¦ˆæœºåˆ¶` `è®¤çŸ¥åˆ†æ` `æ¨ç†èƒ½åŠ›` `æ•™è‚²æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜é˜¶æ€ç»´æŠ€èƒ½æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºçš„THiNKæ¡†æ¶é€šè¿‡é—®é¢˜ç”Ÿæˆã€æ‰¹åˆ¤å’Œä¿®è®¢çš„è¿­ä»£è¿‡ç¨‹ï¼Œé¼“åŠ±æ¨¡å‹è¿›è¡Œé€æ­¥åæ€å’Œä¿®æ­£ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ä½é˜¶æ€ç»´ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é«˜é˜¶æ€ç»´ä»»åŠ¡ä¸­å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œç»“æ„åŒ–åé¦ˆæ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é«˜é˜¶æ€ç»´æŠ€èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¶…è¶Šè¡¨é¢å‡†ç¡®æ€§çš„ä»»åŠ¡ä¸­ã€‚æœ¬ç ”ç©¶æå‡ºäº†THiNKï¼ˆTesting Higher-order Notion of Knowledgeï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¸ƒé²å§†åˆ†ç±»æ³•çš„å¤šä»£ç†åé¦ˆé©±åŠ¨è¯„ä¼°æ¡†æ¶ã€‚THiNKå°†æ¨ç†è¯„ä¼°æ¡†æ¶åŒ–ä¸ºé—®é¢˜ç”Ÿæˆã€æ‰¹åˆ¤å’Œä¿®è®¢çš„è¿­ä»£ä»»åŠ¡ï¼Œé¼“åŠ±LLMsé€šè¿‡é€æ­¥åæ€å’Œä¿®æ­£è¿›è¡Œæ€è€ƒã€‚è¿™ä½¿å¾—å¯¹ä½é˜¶ï¼ˆå¦‚è®°å¿†ã€ç†è§£ï¼‰å’Œé«˜é˜¶ï¼ˆå¦‚è¯„ä¼°ã€åˆ›é€ ï¼‰æ€ç»´æŠ€èƒ½çš„ç³»ç»Ÿè¯„ä¼°æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬å°†THiNKåº”ç”¨äºä¸ƒä¸ªæœ€å…ˆè¿›çš„LLMsï¼Œå¹¶å¯¹å…¶è¾“å‡ºè¿›è¡Œäº†è¯¦ç»†çš„è®¤çŸ¥åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹åœ¨ä½é˜¶ç±»åˆ«ä¸Šè¡¨ç°å¯é ï¼Œä½†åœ¨ç°å®æƒ…å¢ƒä¸­åº”ç”¨çŸ¥è¯†æ—¶å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”æŠ½è±¡èƒ½åŠ›æœ‰é™ã€‚ç»“æ„åŒ–åé¦ˆå¾ªç¯æ˜¾è‘—æé«˜äº†æ¨ç†è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é˜¶æ€ç»´æ–¹é¢ã€‚å®šæ€§è¯„ä¼°è¿›ä¸€æ­¥ç¡®è®¤ï¼ŒTHiNKæŒ‡å¯¼çš„è¾“å‡ºæ›´å¥½åœ°ä¸é¢†åŸŸé€»è¾‘å’Œé—®é¢˜ç»“æ„å¯¹é½ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»£ç æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•æ¥æ¢æµ‹å’Œå¢å¼ºLLMæ¨ç†ï¼Œæä¾›äº†åŸºäºå­¦ä¹ ç§‘å­¦çš„æ–°è¯„ä¼°æ–¹å‘ï¼Œä»£ç å¯åœ¨æˆ‘ä»¬çš„GitHubåº“ä¸­è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜é˜¶æ€ç»´èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ·±å…¥æ¢æµ‹æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTHiNKæ¡†æ¶é€šè¿‡å¼•å…¥å¤šä»£ç†åé¦ˆæœºåˆ¶ï¼Œå°†æ¨ç†è¯„ä¼°è§†ä¸ºä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ç”Ÿæˆé—®é¢˜åè¿›è¡Œæ‰¹åˆ¤å’Œä¿®è®¢ï¼Œä»è€Œæå‡å…¶æ€ç»´æ·±åº¦å’Œå¹¿åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTHiNKæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé—®é¢˜ç”Ÿæˆã€æ‰¹åˆ¤åé¦ˆå’Œä¿®è®¢è¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæ¨¡å‹ç”Ÿæˆé—®é¢˜ï¼›å…¶æ¬¡ï¼Œç³»ç»Ÿæä¾›åé¦ˆä»¥è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆï¼›æœ€åï¼Œæ¨¡å‹æ ¹æ®åé¦ˆè¿›è¡Œä¿®è®¢å’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šTHiNKçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åé¦ˆé©±åŠ¨çš„è¿­ä»£è¯„ä¼°æœºåˆ¶ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¯„ä¼°ä½é˜¶å’Œé«˜é˜¶æ€ç»´æŠ€èƒ½ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿçš„é™æ€è¯„ä¼°æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç»“æ„åŒ–åé¦ˆå¾ªç¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¯ä¸ªè¿­ä»£ä¸­éƒ½èƒ½è·å¾—é’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®ã€‚æ­¤å¤–ï¼Œæ¡†æ¶çš„å®ç°ç»†èŠ‚åŒ…æ‹¬å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒç±»å‹çš„æ¨ç†ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTHiNKæ¡†æ¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é«˜é˜¶æ€ç»´ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶åœ¨ç»“æ„åŒ–åé¦ˆçš„å¸®åŠ©ä¸‹ï¼Œæ¨ç†èƒ½åŠ›æé«˜äº†çº¦30%ã€‚å®šæ€§è¯„ä¼°æ˜¾ç¤ºï¼ŒTHiNKæŒ‡å¯¼çš„è¾“å‡ºæ›´ç¬¦åˆé¢†åŸŸé€»è¾‘ï¼Œä¸”åœ¨é—®é¢˜ç»“æ„çš„ç†è§£ä¸Šè¡¨ç°æ›´ä½³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

THiNKæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°å’Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€ç»´èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºæ•™è‚²è¯„ä¼°å’Œä¸ªæ€§åŒ–å­¦ä¹ æä¾›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶è¿˜å¯èƒ½å½±å“è¯­è¨€æ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ï¼Œä¿ƒè¿›æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›çš„æå‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Assessing higher-order thinking skills in large language models (LLMs) remains a fundamental challenge, especially in tasks that go beyond surface-level accuracy. In this work, we propose THiNK (Testing Higher-order Notion of Knowledge), a multi-agent, feedback-driven evaluation framework grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative task of problem generation, critique, and revision, encouraging LLMs to think-aloud through step-by-step reflection and refinement. This enables a systematic evaluation of both lower-order (e.g., remember, understand) and higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven state-of-the-art LLMs and perform a detailed cognitive analysis of their outputs. Results reveal that while models reliably perform lower-order categories well, they struggle with applying knowledge in realistic contexts and exhibit limited abstraction. Structured feedback loops significantly improve reasoning performance, particularly in higher-order thinking. Qualitative evaluations further confirm that THiNK-guided outputs better align with domain logic and problem structure. The code of our framework provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science, which is available at our GitHub repository.

