---
layout: default
title: "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models"
---

# Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19743" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19743v3</a>
  <a href="https://arxiv.org/pdf/2505.19743.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19743v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19743v3', 'Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Zhang, Yu Yu, Bo Tang, Yu Zhu, Chuxiong Sun, Wenqiang Wei, Jie Hu, Zipeng Xie, Zhiyu Li, Feiyu Xiong, Edward Chung

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-08-16)

**å¤‡æ³¨**: Accepted to 34th International Joint Conference on Artificial Intelligence (IJCAI 2025)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/IAAR-Shanghai/MARA) | [HUGGINGFACE](https://huggingface.co/IAAR-Shanghai/MARA_AGENTS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMARAæ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¯¹é½æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¯¹é½æŠ€æœ¯` `Tokençº§åˆ†ç±»` `è®¡ç®—æ•ˆç‡` `æœºå™¨å­¦ä¹ ` `äººå·¥æ™ºèƒ½` `ä¼¦ç†åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹é½æŠ€æœ¯å¦‚RLHFå’ŒDPOåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸Šå¾®è°ƒæ—¶è®¡ç®—æˆæœ¬é«˜ä¸”æ•ˆç‡ä½ï¼Œéš¾ä»¥æ»¡è¶³å®é™…éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºçš„MARAæ–¹æ³•é€šè¿‡Tokençº§åˆ«çš„äºŒå…ƒåˆ†ç±»ï¼Œç®€åŒ–äº†å¯¹é½è¿‡ç¨‹ï¼Œé¿å…äº†å¯¹å¤§æ¨¡å‹çš„ç›´æ¥å¾®è°ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMARAåœ¨ä¸ƒç§ä¸åŒçš„LLMsä¸Šå‡æ˜¾è‘—æå‡äº†å¯¹é½æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä½¿è¿™äº›æ¨¡å‹ä¸äººç±»åå¥½å’Œä»·å€¼è§‚å¯¹é½å˜å¾—è‡³å…³é‡è¦ï¼Œä»¥ç¡®ä¿å…¶ä¼¦ç†å’Œå®‰å…¨åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¯¹é½æŠ€æœ¯å¦‚RLHFæˆ–DPOé€šå¸¸éœ€è¦å¯¹æ•°åäº¿å‚æ•°çš„LLMsè¿›è¡Œç›´æ¥å¾®è°ƒï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¾®è§‚çš„Tokençº§æ¥å—-æ‹’ç»å¯¹é½æ–¹æ³•ï¼ˆMARAï¼‰ï¼Œæ—¨åœ¨ç‹¬ç«‹äºè¯­è¨€æ¨¡å‹è¿›è¡Œæ“ä½œã€‚MARAé€šè¿‡å°†å¥å­çº§åå¥½å­¦ä¹ åˆ†è§£ä¸ºTokençº§äºŒå…ƒåˆ†ç±»ï¼Œç®€åŒ–äº†å¯¹é½è¿‡ç¨‹ï¼Œå…¶ä¸­ä¸€ä¸ªç´§å‡‘çš„ä¸‰å±‚å…¨è¿æ¥ç½‘ç»œå†³å®šå€™é€‰Tokenæ˜¯å¦è¢«â€œæ¥å—â€æˆ–â€œæ‹’ç»â€ã€‚åœ¨ä¸ƒç§ä¸åŒçš„LLMså’Œä¸‰ä¸ªå¼€æºæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMARAåœ¨å¯¹é½æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚RLHFå’ŒDPOéœ€è¦å¯¹æ•°åäº¿å‚æ•°çš„æ¨¡å‹è¿›è¡Œç›´æ¥å¾®è°ƒï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMARAæ–¹æ³•çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¥å­çº§çš„åå¥½å­¦ä¹ åˆ†è§£ä¸ºTokençº§çš„äºŒå…ƒåˆ†ç±»ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMARAå¯ä»¥ç‹¬ç«‹äºå…·ä½“çš„è¯­è¨€æ¨¡å‹è¿›è¡Œæ“ä½œï¼Œä»è€Œå‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMARAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç´§å‡‘çš„ä¸‰å±‚å…¨è¿æ¥ç½‘ç»œï¼Œè¯¥ç½‘ç»œè´Ÿè´£åˆ¤æ–­æ¯ä¸ªå€™é€‰Tokenæ˜¯å¦è¢«â€œæ¥å—â€æˆ–â€œæ‹’ç»â€ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯ä¸ªTokenè¿›è¡Œç‹¬ç«‹è¯„ä¼°ï¼Œç®€åŒ–äº†å¯¹é½è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMARAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶Tokençº§åˆ«çš„æ¥å—-æ‹’ç»æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¥å­çº§å¯¹é½æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½è®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šMARAé‡‡ç”¨äº†ä¸‰å±‚å…¨è¿æ¥ç½‘ç»œç»“æ„ï¼Œå…·ä½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒLLMsä¸Šçš„é€‚ç”¨æ€§å’Œæ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è®¾è®¡åœ¨å¯¹é½æ€§èƒ½ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMARAåœ¨ä¸ƒç§ä¸åŒçš„LLMsä¸Šå‡å®ç°äº†æ˜¾è‘—çš„å¯¹é½æ€§èƒ½æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜äº†çº¦20%-30%çš„å¯¹é½å‡†ç¡®ç‡ï¼ŒåŒæ—¶è®¡ç®—èµ„æºæ¶ˆè€—æ˜¾è‘—é™ä½ï¼Œå±•ç°äº†å…¶é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MARAæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å°†å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„åœºæ™¯ä¸­ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹ç”Ÿæˆå’Œæ•™è‚²æŠ€æœ¯ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„å¯¹é½æœºåˆ¶èƒ½å¤Ÿé™ä½è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—å¤§è§„æ¨¡æ¨¡å‹çš„åº”ç”¨æ›´åŠ å¯è¡Œã€‚æœªæ¥ï¼ŒMARAå¯èƒ½æ¨åŠ¨æ›´å®‰å…¨å’Œä¼¦ç†çš„AIåº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid development of Large Language Models (LLMs), aligning these models with human preferences and values is critical to ensuring ethical and safe applications. However, existing alignment techniques such as RLHF or DPO often require direct fine-tuning on LLMs with billions of parameters, resulting in substantial computational costs and inefficiencies. To address this, we propose Micro token-level Accept-Reject Aligning (MARA) approach designed to operate independently of the language models. MARA simplifies the alignment process by decomposing sentence-level preference learning into token-level binary classification, where a compact three-layer fully-connected network determines whether candidate tokens are "Accepted" or "Rejected" as part of the response. Extensive experiments across seven different LLMs and three open-source datasets show that MARA achieves significant improvements in alignment performance while reducing computational costs. The source code and implementation details are publicly available at https://github.com/IAAR-Shanghai/MARA, and the trained models are released at https://huggingface.co/IAAR-Shanghai/MARA_AGENTS.

