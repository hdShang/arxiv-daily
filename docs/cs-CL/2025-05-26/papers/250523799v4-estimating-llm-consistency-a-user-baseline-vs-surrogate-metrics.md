---
layout: default
title: "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics"
---

# Estimating LLM Consistency: A User Baseline vs Surrogate Metrics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23799" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23799v4</a>
  <a href="https://arxiv.org/pdf/2505.23799.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23799v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23799v4', 'Estimating LLM Consistency: A User Baseline vs Surrogate Metrics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer

**åˆ†ç±»**: cs.CL, cs.AI, cs.HC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-11-21)

**å¤‡æ³¨**: Published as a main conference paper at EMNLP 2025

**DOI**: [10.18653/v1/2025.emnlp-main.1554](https://doi.org/10.18653/v1/2025.emnlp-main.1554)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºlogitçš„é›†æˆæ–¹æ³•ä»¥è¯„ä¼°LLMä¸€è‡´æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸€è‡´æ€§è¯„ä¼°` `logité›†æˆ` `ç”¨æˆ·ç ”ç©¶` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æµ‹é‡LLMå“åº”ä¸€è‡´æ€§æ—¶ï¼Œé€šå¸¸ä¸ç”¨æˆ·çš„æ„ŸçŸ¥å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¯é ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºlogitçš„é›†æˆæ–¹æ³•ï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°ä¼°è®¡LLMçš„ä¸€è‡´æ€§ï¼Œæå‡ä¸ç”¨æˆ·æ„ŸçŸ¥çš„ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ä¼°è®¡äººç±»å¯¹LLMä¸€è‡´æ€§è¯„åˆ†çš„è¡¨ç°ä¸Šï¼Œä¸ç°æœ‰æœ€ä½³æŒ‡æ ‡ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œå¹¶å¯¹æç¤ºæ‰°åŠ¨æ•æ„Ÿï¼Œå¯¼è‡´ç”Ÿæˆæ–‡æœ¬ä¸ä¸€è‡´æˆ–ä¸å¯é ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡ç”¨æˆ·ç ”ç©¶ï¼ˆn=2,976ï¼‰å‘ç°ï¼Œç°æœ‰çš„ä¸€è‡´æ€§æµ‹é‡æ–¹æ³•ä¸ç”¨æˆ·å¯¹LLMä¸€è‡´æ€§çš„æ„ŸçŸ¥ä¸åŒ¹é…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºlogitçš„é›†æˆæ–¹æ³•æ¥è¯„ä¼°LLMä¸€è‡´æ€§ï¼Œå¹¶è¯æ˜è¯¥æ–¹æ³•åœ¨ä¼°è®¡äººç±»å¯¹LLMä¸€è‡´æ€§è¯„åˆ†çš„è¡¨ç°ä¸Šä¸ç°æœ‰æœ€ä½³æŒ‡æ ‡ç›¸å½“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¾èµ–è‡ªåŠ¨åŒ–ä¸€è‡´æ€§æŒ‡æ ‡çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œå¼ºè°ƒäº†å¼•å…¥äººç±»è¯„ä¼°çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMä¸€è‡´æ€§æµ‹é‡æ–¹æ³•ä¸ç”¨æˆ·æ„ŸçŸ¥ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå“åº”æ¦‚ç‡ã€å†…éƒ¨çŠ¶æ€åˆ†ææˆ–logitsè¯„ä¼°ï¼Œå­˜åœ¨å‡†ç¡®æ€§ä¸è¶³çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºlogitçš„é›†æˆæ–¹æ³•ï¼Œé€šè¿‡ç»¼åˆå¤šä¸ªlogitå€¼æ¥æ›´å‡†ç¡®åœ°è¯„ä¼°LLMçš„ä¸€è‡´æ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ›´å¥½åœ°åæ˜ ç”¨æˆ·å¯¹ä¸€è‡´æ€§çš„ä¸»è§‚æ„ŸçŸ¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€logitè®¡ç®—ã€é›†æˆè¯„ä¼°å’Œç”¨æˆ·åé¦ˆå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†LLMç”Ÿæˆçš„å“åº”ï¼Œç„¶åè®¡ç®—å…¶logitå€¼ï¼Œæœ€åé€šè¿‡é›†æˆç®—æ³•è¯„ä¼°ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†logité›†æˆæ–¹æ³•ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€æ¦‚ç‡è®¡ç®—æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰æ¨¡å‹å“åº”çš„ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†å¤šä¸ªå‚æ•°ä»¥ä¼˜åŒ–logité›†æˆè¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥æå‡ä¸€è‡´æ€§è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æåŸºäºlogitçš„é›†æˆæ–¹æ³•åœ¨ä¼°è®¡äººç±»å¯¹LLMä¸€è‡´æ€§è¯„åˆ†çš„è¡¨ç°ä¸Šï¼Œä¸ç°æœ‰æœ€ä½³æŒ‡æ ‡ç›¸å½“ï¼Œè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨ä¸€è‡´æ€§è¯„ä¼°ä¸­çš„å‡†ç¡®ç‡æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ ç”¨æˆ·çš„çœŸå®æ„ŸçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°è¯„ä¼°LLMçš„ä¸€è‡´æ€§ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å¯é æ€§ï¼Œè¿›è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œä¿¡ä»»åº¦ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œè¯¥æ–¹æ³•å¯èƒ½æˆä¸ºè¯„ä¼°LLMæ€§èƒ½çš„é‡è¦å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility, one of which is to measure the consistency of LLM responses -- the model's confidence in the response or likelihood of generating a similar response when resampled. In previous work, measuring LLM response consistency often relied on calculating the probability of a response appearing within a pool of resampled responses, analyzing internal states, or evaluating logits of responses. However, it was not clear how well these approaches approximated users' perceptions of consistency of LLM responses. To find out, we performed a user study ($n=2,976$) demonstrating that current methods for measuring LLM response consistency typically do not align well with humans' perceptions of LLM consistency. We propose a logit-based ensemble method for estimating LLM consistency and show that our method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods for estimating LLM consistency without human evaluation are sufficiently imperfect to warrant broader use of evaluation with human input; this would avoid misjudging the adequacy of models because of the imperfections of automated consistency metrics.

