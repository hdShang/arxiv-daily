---
layout: default
title: REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models
---

# REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19862" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19862v1</a>
  <a href="https://arxiv.org/pdf/2505.19862.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19862v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19862v1', 'REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jun Rao, Min Zhang

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: Work in Progress

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hexuandeng/REA-RL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºREA-RLä»¥è§£å†³å¤§å‹æ¨ç†æ¨¡å‹çš„é«˜æ¨ç†æˆæœ¬é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤§å‹æ¨ç†æ¨¡å‹` `åœ¨çº¿å¼ºåŒ–å­¦ä¹ ` `åæ€æœºåˆ¶` `æ¨ç†æ•ˆç‡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§å‹æ¨ç†æ¨¡å‹æ—¶ï¼Œé¢ä¸´æ¨ç†æˆæœ¬é«˜å’Œåæ€èƒ½åŠ›ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºREA-RLï¼Œé€šè¿‡å¼•å…¥å°å‹åæ€æ¨¡å‹ï¼Œå®ç°åœ¨çº¿è®­ç»ƒçš„é«˜æ•ˆæ‰©å±•ï¼Œç»“åˆå¹¶è¡Œé‡‡æ ·ä¸é¡ºåºä¿®æ­£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒREA-RLåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ¨ç†æˆæœ¬é™ä½äº†35%ï¼Œæœ‰æ•ˆæå‡äº†æ¨ç†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸é¢ä¸´è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬æ˜¾è‘—å¢åŠ ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡åˆæˆè¾ƒçŸ­çš„æ¨ç†å“åº”æ¥è¿›è¡Œå­¦ä¹ ï¼Œä½†åœ¨åœ¨çº¿ä½¿ç”¨æ—¶æ•ˆç‡ä½ä¸‹ã€‚åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸»è¦é‡‡ç”¨é•¿åº¦å¥–åŠ±æ¥é¼“åŠ±çŸ­æ¨ç†å“åº”ï¼Œä½†å®¹æ˜“å¤±å»åæ€èƒ½åŠ›ï¼Œå½±å“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†REA-RLï¼Œå¼•å…¥äº†å°å‹åæ€æ¨¡å‹ä»¥å®ç°åœ¨çº¿è®­ç»ƒçš„é«˜æ•ˆæ‰©å±•ï¼Œæä¾›å¹¶è¡Œé‡‡æ ·å’Œé¡ºåºä¿®æ­£ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†åæ€å¥–åŠ±ï¼Œä»¥è¿›ä¸€æ­¥é˜²æ­¢LRMsåå‘çŸ­è€Œç¼ºä¹åæ€çš„å“åº”ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨æ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œä¿æŒæˆ–å¢å¼ºäº†æ€§èƒ½ã€‚å®ƒä»¬çš„ç»“åˆåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾æˆäº†è‰¯å¥½çš„å¹³è¡¡ï¼Œæ¨ç†æˆæœ¬é™ä½äº†35%ï¼Œè€Œæ€§èƒ½æœªå—æŸã€‚è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤„ç†å›°éš¾é—®é¢˜æ—¶æœ‰æ•ˆä¿æŒåæ€é¢‘ç‡ï¼Œè€Œåœ¨ç®€å•é—®é¢˜ä¸­é€‚å½“é™ä½åæ€é¢‘ç‡ï¼Œä¸”ä¸ä¸§å¤±åæ€èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜æˆæœ¬å’Œåæ€èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶èƒ½åˆæˆçŸ­å“åº”ï¼Œä½†åœ¨åœ¨çº¿ä½¿ç”¨æ—¶æ•ˆç‡ä½ä¸‹ï¼Œä¸”å®¹æ˜“å¯¼è‡´åæ€èƒ½åŠ›çš„ä¸§å¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šREA-RLçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªå°å‹åæ€æ¨¡å‹ï¼Œä»¥å®ç°åœ¨çº¿è®­ç»ƒçš„é«˜æ•ˆæ‰©å±•ã€‚é€šè¿‡å¹¶è¡Œé‡‡æ ·å’Œé¡ºåºä¿®æ­£ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”ŸæˆçŸ­å“åº”çš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒåæ€èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬åæ€æ¨¡å‹å’Œä¸»æ¨ç†æ¨¡å‹ã€‚åæ€æ¨¡å‹è´Ÿè´£ç”Ÿæˆåæ€å¥–åŠ±ï¼Œä¸»æ¨ç†æ¨¡å‹åˆ™åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ¨ç†å“åº”çš„ç”Ÿæˆã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬æ•°æ®é‡‡æ ·ã€åæ€å¥–åŠ±è®¡ç®—å’Œæ¨ç†å“åº”ç”Ÿæˆä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥åæ€å¥–åŠ±æœºåˆ¶ï¼Œæ—¨åœ¨é˜²æ­¢æ¨¡å‹åå‘çŸ­è€Œç¼ºä¹æ·±åº¦çš„å“åº”ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå¼ºè°ƒäº†åæ€èƒ½åŠ›çš„é‡è¦æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯å“åº”çš„é•¿åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œåæ€æ¨¡å‹çš„ç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿å…¶åœ¨ç”Ÿæˆåæ€å¥–åŠ±æ—¶çš„æœ‰æ•ˆæ€§ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†åæ€å¥–åŠ±ä¸ä¼ ç»Ÿçš„é•¿åº¦å¥–åŠ±ï¼Œä»¥å¹³è¡¡æ¨ç†æ•ˆç‡ä¸åæ€èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREA-RLåœ¨æ¨ç†æ•ˆç‡ä¸Šæ˜¾è‘—æå‡ï¼Œæ¨ç†æˆæœ¬é™ä½äº†35%ï¼ŒåŒæ—¶ä¿æŒæˆ–å¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸€ç»“æœä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå±•ç¤ºäº†æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œè¯æ˜äº†åæ€å¥–åŠ±æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒREA-RLèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­é™ä½è®¡ç®—æˆæœ¬ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks but often face the challenge of overthinking, leading to substantially high inference costs. Existing approaches synthesize shorter reasoning responses for LRMs to learn, but are inefficient for online usage due to the time-consuming data generation and filtering processes. Meanwhile, online reinforcement learning mainly adopts a length reward to encourage short reasoning responses, but tends to lose the reflection ability and harm the performance. To address these issues, we propose REA-RL, which introduces a small reflection model for efficient scaling in online training, offering both parallel sampling and sequential revision. Besides, a reflection reward is designed to further prevent LRMs from favoring short yet non-reflective responses. Experiments show that both methods maintain or enhance performance while significantly improving inference efficiency. Their combination achieves a good balance between performance and efficiency, reducing inference costs by 35% without compromising performance. Further analysis demonstrates that our methods are effective by maintaining reflection frequency for hard problems while appropriately reducing it for simpler ones without losing reflection ability. Codes are available at https://github.com/hexuandeng/REA-RL.

