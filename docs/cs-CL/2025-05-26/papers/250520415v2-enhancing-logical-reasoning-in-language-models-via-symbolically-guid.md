---
layout: default
title: Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision
---

# Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20415" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20415v2</a>
  <a href="https://arxiv.org/pdf/2505.20415.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20415v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20415v2', 'Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingwei Tan, Marco Valentino, Mahmud Akhter, Maria Liakata, Nikolaos Aletras

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-09-17)

**å¤‡æ³¨**: EMNLP 2025 (Main), 9+6 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ç¬¦å·å¼•å¯¼çš„è’™ç‰¹å¡æ´›è¿‡ç¨‹ç›‘ç£æå‡è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€»è¾‘æ¨ç†` `ç¬¦å·æ–¹æ³•` `è’™ç‰¹å¡æ´›è¿‡ç¨‹` `è¯­è¨€æ¨¡å‹` `è¿‡ç¨‹å¥–åŠ±æ¨¡å‹` `ç›´æ¥åå¥½ä¼˜åŒ–` `ç›‘ç£å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç»“åˆç¬¦å·è¡¨ç¤ºæ—¶æœªèƒ½æœ‰æ•ˆåˆ©ç”¨ç¬¦å·æ¨ç†ï¼Œå¯¼è‡´é€»è¾‘æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è’™ç‰¹å¡æ´›ä¼°è®¡åˆæˆç¬¦å·æ¨ç†è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è¿›è¡Œé€‰æ‹©ï¼Œä»¥æå‡æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€»è¾‘æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç ”ç©¶è¡¨æ˜å…¶æ€§èƒ½ä¸»è¦æºäºè®°å¿†è€Œéæ³›åŒ–ã€‚LLMså¯¹å†…å®¹å˜åŒ–æ•æ„Ÿï¼Œç¼ºä¹æ”¯æŒæ¨ç†è¿‡ç¨‹çš„ç¨³å¥è§„åˆ’æˆ–ç¬¦å·æŠ½è±¡ã€‚ä¸ºæé«˜å¯é æ€§ï¼Œè®¸å¤šå°è¯•å°†LLMsä¸ç¬¦å·æ–¹æ³•ç»“åˆï¼Œä½†ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨ç¬¦å·è¡¨ç¤ºã€‚æœ¬æ–‡æå‡ºé€šè¿‡è’™ç‰¹å¡æ´›ä¼°è®¡åˆæˆé«˜è´¨é‡çš„ç¬¦å·æ¨ç†è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰é€‰æ‹©æ›´å¤šç¬¦å·è½¨è¿¹ï¼Œç»“åˆç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥æå‡é€»è¾‘æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨FOLIOå’ŒLogicAskeråŸºå‡†ä¸Šæœ‰æ•ˆæå‡äº†å‰æ²¿å’Œå¼€æ”¾æƒé‡æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨å£°æ˜éªŒè¯æ•°æ®ä¸Šå¢å¼ºäº†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä¸­çš„è®°å¿†æ€§å’Œå¯¹å†…å®¹å˜åŒ–çš„æ•æ„Ÿæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç»“åˆç¬¦å·æ¨ç†æ—¶ç¼ºä¹æœ‰æ•ˆçš„éªŒè¯æœºåˆ¶ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆæˆé«˜è´¨é‡çš„ç¬¦å·æ¨ç†è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡ç”Ÿæˆé€æ­¥ä¼ªæ ‡ç­¾ï¼Œæ¥æå‡æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç¬¦å·æ¨ç†è½¨è¿¹çš„åˆæˆã€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒã€ç¬¦å·è½¨è¿¹çš„é€‰æ‹©ï¼Œä»¥åŠç»“åˆDPOå’ŒSFTçš„å¾®è°ƒè¿‡ç¨‹ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®åˆæˆã€æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡è’™ç‰¹å¡æ´›è¿‡ç¨‹ç”Ÿæˆç¬¦å·æ¨ç†è½¨è¿¹ï¼Œå¹¶æœ‰æ•ˆè®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œåœ¨é€‰æ‹©ç¬¦å·è½¨è¿¹æ—¶å®ç°æ›´é«˜çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç¬¦å·æ¨ç†è½¨è¿¹çš„é€‰æ‹©ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†Transformeræ¶æ„ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨FOLIOå’ŒLogicAskeråŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å‰æ²¿å’Œå¼€æ”¾æƒé‡æ¨¡å‹ä¸Šï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ã€‚æ­¤å¤–ï¼Œåœ¨å£°æ˜éªŒè¯æ•°æ®ä¸Šçš„å¾®è°ƒå®éªŒè¡¨æ˜ï¼Œæ¨¡å‹çš„åŸŸå¤–æ³›åŒ–èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—å¢å¼ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ¨ç†ä»»åŠ¡ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿä»¥åŠè‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒç³»ç»Ÿã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹äººå·¥æ™ºèƒ½çš„å†³ç­–èƒ½åŠ›äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have shown strong performance in many reasoning benchmarks. However, recent studies have pointed to memorization, rather than generalization, as one of the leading causes for such performance. LLMs, in fact, are susceptible to content variations, demonstrating a lack of robust planning or symbolic abstractions supporting their reasoning process. To improve reliability, many attempts have been made to combine LLMs with symbolic methods. Nevertheless, existing approaches fail to effectively leverage symbolic representations due to the challenges involved in developing reliable and scalable verification mechanisms. In this paper, we propose to overcome such limitations by synthesizing high-quality symbolic reasoning trajectories with stepwise pseudo-labels at scale via Monte Carlo estimation. A Process Reward Model (PRM) can be efficiently trained based on the synthesized data and then used to select more symbolic trajectories. The trajectories are then employed with Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) to improve logical reasoning and generalization. Our results on benchmarks (i.e., FOLIO and LogicAsker) show the effectiveness of the proposed method with gains on frontier and open-weight models. Moreover, additional experiments on claim verification data reveal that fine-tuning on the generated symbolic reasoning trajectories enhances out-of-domain generalizability, suggesting the potential impact of the proposed method in enhancing planning and logical reasoning.

