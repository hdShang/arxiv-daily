---
layout: default
title: FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models
---

# FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20225" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20225v1</a>
  <a href="https://arxiv.org/pdf/2505.20225.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20225v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20225v1', 'FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hao Kang, Zichun Yu, Chenyan Xiong

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/cmu-flame/FLAME-MoE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFLAME-MoEä»¥è§£å†³ç°æœ‰MoEè¯­è¨€æ¨¡å‹ç ”ç©¶å¹³å°ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `è¯­è¨€æ¨¡å‹` `å¼€æºå¹³å°` `æ¨¡å‹è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¸“å®¶è¡Œä¸º` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹ç¼ºä¹ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶å¹³å°ï¼Œé™åˆ¶äº†å­¦æœ¯ç•Œå¯¹å…¶æ‰©å±•æ€§å’Œè·¯ç”±è¡Œä¸ºçš„æ·±å…¥ç ”ç©¶ã€‚
2. FLAME-MoEæä¾›äº†ä¸€ä¸ªå¼€æºçš„ç ”ç©¶å¥—ä»¶ï¼ŒåŒ…å«å¤šç§è§„æ¨¡çš„è§£ç å™¨æ¨¡å‹ï¼Œæ”¯æŒå¯¹MoEæ¶æ„çš„å…¨é¢å®éªŒå’Œåˆ†æã€‚
3. åœ¨å…­ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­ï¼ŒFLAME-MoEçš„å¹³å‡å‡†ç¡®ç‡æ¯”ç›¸åŒFLOPsçš„ç¨ å¯†åŸºçº¿æé«˜äº†æœ€å¤š3.4ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåƒGemini-1.5ã€DeepSeek-V3å’ŒLlama-4ç­‰å¤§å‹è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œé€šè¿‡æ¯ä¸ªtokenä»…æ¿€æ´»æ¨¡å‹çš„ä¸€éƒ¨åˆ†æ¥å®ç°é«˜æ•ˆçš„æ€§èƒ½æƒè¡¡ã€‚ç„¶è€Œï¼Œå­¦æœ¯ç ”ç©¶è€…ä»ç¼ºä¹ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„ç«¯åˆ°ç«¯MoEå¹³å°æ¥ç ”ç©¶æ‰©å±•ã€è·¯ç”±å’Œä¸“å®¶è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘å¸ƒäº†FLAME-MoEï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„ç ”ç©¶å¥—ä»¶ï¼Œç”±ä¸ƒä¸ªä»…è§£ç å™¨æ¨¡å‹ç»„æˆï¼Œæ´»è·ƒå‚æ•°èŒƒå›´ä»3800ä¸‡åˆ°17äº¿ï¼Œå…¶æ¶æ„åŒ…å«64ä¸ªä¸“å®¶ã€å‰8ä¸ªé—¨æ§å’Œ2ä¸ªå…±äº«ä¸“å®¶ï¼Œç´§å¯†åæ˜ ç°ä»£ç”Ÿäº§LLMã€‚æ‰€æœ‰è®­ç»ƒæ•°æ®ç®¡é“ã€è„šæœ¬ã€æ—¥å¿—å’Œæ£€æŸ¥ç‚¹å‡å¯å…¬å¼€è·å–ï¼Œä»¥æ”¯æŒå¯é‡å¤çš„å®éªŒã€‚åœ¨å…­ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­ï¼ŒFLAME-MoEåœ¨ä¸ç›¸åŒFLOPsè®­ç»ƒçš„ç¨ å¯†åŸºçº¿ç›¸æ¯”ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†æœ€å¤š3.4ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡å®Œå…¨çš„è®­ç»ƒè·Ÿè¸ªé€æ˜æ€§ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åˆæ­¥åˆ†æç»“æœï¼Œè¡¨æ˜ä¸“å®¶åœ¨ä¸åŒtokenå­é›†ä¸Šé€æ¸ä¸“ä¸šåŒ–ï¼ŒååŒæ¿€æ´»çŸ©é˜µä¿æŒç¨€ç–ï¼Œåæ˜ å‡ºå¤šæ ·çš„ä¸“å®¶ä½¿ç”¨æƒ…å†µï¼Œä»¥åŠè·¯ç”±è¡Œä¸ºåœ¨è®­ç»ƒæ—©æœŸå³è¶‹äºç¨³å®šã€‚æ‰€æœ‰ä»£ç ã€è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨https://github.com/cmu-flame/FLAME-MoEè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å­¦æœ¯ç•Œç¼ºä¹å¼€æ”¾çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ç ”ç©¶å¹³å°çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ‰©å±•æ€§ã€è·¯ç”±å’Œä¸“å®¶è¡Œä¸ºçš„ç ”ç©¶ä¸Šå­˜åœ¨å±€é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFLAME-MoEé€šè¿‡æä¾›ä¸€ä¸ªå®Œå…¨å¼€æºçš„ç ”ç©¶å¥—ä»¶ï¼ŒåŒ…å«å¤šç§è§„æ¨¡çš„è§£ç å™¨æ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒå¯¹MoEæ¶æ„çš„æ·±å…¥å®éªŒå’Œåˆ†æï¼Œä¿ƒè¿›ç ”ç©¶è€…å¯¹æ¨¡å‹è¡Œä¸ºçš„ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFLAME-MoEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸ƒä¸ªè§£ç å™¨æ¨¡å‹ï¼Œæ´»è·ƒå‚æ•°ä»3800ä¸‡åˆ°17äº¿ï¼Œé‡‡ç”¨64ä¸ªä¸“å®¶ã€å‰8ä¸ªé—¨æ§å’Œ2ä¸ªå…±äº«ä¸“å®¶çš„è®¾è®¡ï¼Œç´§å¯†åæ˜ ç°ä»£ç”Ÿäº§LLMçš„æ¶æ„ã€‚æ‰€æœ‰è®­ç»ƒæ•°æ®ç®¡é“ã€è„šæœ¬å’Œæ—¥å¿—å‡å¯å…¬å¼€è·å–ã€‚

**å…³é”®åˆ›æ–°**ï¼šFLAME-MoEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å®Œå…¨å¼€æºçš„ç‰¹æ€§å’Œé€æ˜çš„è®­ç»ƒè·Ÿè¸ªï¼Œå…è®¸ç ”ç©¶è€…å¯¹æ¨¡å‹çš„æ‰©å±•æ€§å’Œä¸“å®¶è¡Œä¸ºè¿›è¡Œæ·±å…¥åˆ†æï¼Œè¿™åœ¨ç°æœ‰çš„MoEç ”ç©¶ä¸­æ˜¯å‰æ‰€æœªæœ‰çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šFLAME-MoEçš„è®¾è®¡åŒ…æ‹¬64ä¸ªä¸“å®¶çš„é…ç½®ã€å‰8ä¸ªé—¨æ§æœºåˆ¶å’Œå…±äº«ä¸“å®¶çš„è®¾ç½®ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒtokenå­é›†ä¸Šçš„ä¸“ä¸šåŒ–ï¼ŒåŒæ—¶ä¿æŒååŒæ¿€æ´»çŸ©é˜µçš„ç¨€ç–æ€§ï¼Œåæ˜ å‡ºå¤šæ ·çš„ä¸“å®¶ä½¿ç”¨æƒ…å†µã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FLAME-MoEåœ¨å…­ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”ç›¸åŒFLOPsçš„ç¨ å¯†åŸºçº¿æé«˜äº†æœ€å¤š3.4ä¸ªç™¾åˆ†ç‚¹ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•ˆç‡ä¸æ€§èƒ½ä¹‹é—´çš„ä¼˜è‰¯å¹³è¡¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†ä¸“å®¶åœ¨tokenå­é›†ä¸Šçš„ä¸“ä¸šåŒ–è¶‹åŠ¿å’Œè·¯ç”±è¡Œä¸ºçš„ç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FLAME-MoEçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨¡å‹æ¨ç†å’Œè®­ç»ƒçš„åœºæ™¯ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚å…¶å¼€æºç‰¹æ€§å°†ä¿ƒè¿›æ›´å¤šç ”ç©¶è€…å‚ä¸åˆ°MoEæ¶æ„çš„æ¢ç´¢ä¸­ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.

