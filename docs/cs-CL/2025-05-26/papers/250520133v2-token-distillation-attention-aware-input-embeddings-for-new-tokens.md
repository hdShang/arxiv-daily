---
layout: default
title: Token Distillation: Attention-aware Input Embeddings For New Tokens
---

# Token Distillation: Attention-aware Input Embeddings For New Tokens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20133" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20133v2</a>
  <a href="https://arxiv.org/pdf/2505.20133.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20133v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20133v2', 'Token Distillation: Attention-aware Input Embeddings For New Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Konstantin Dobler, Desmond Elliott, Gerard de Melo

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-10-31)

**å¤‡æ³¨**: Additional experiments + clearer method name compared to the May 2025 version

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºToken Distillationä»¥å¿«é€Ÿå­¦ä¹ æ–°è¯çš„é«˜è´¨é‡åµŒå…¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `è¯åµŒå…¥` `è’¸é¦è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ–°è¯å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­è¨€æ¨¡å‹ä¾èµ–é™æ€è¯æ±‡ï¼Œå¯¼è‡´åœ¨ç‰¹å®šé¢†åŸŸæ€§èƒ½ä¸è¶³å’Œè®¡ç®—æˆæœ¬é«˜ã€‚
2. æå‡ºToken Distillationï¼Œé€šè¿‡è’¸é¦åŸå§‹åˆ†è¯çš„è¡¨ç¤ºï¼Œå¿«é€Ÿå­¦ä¹ æ–°è¯çš„åµŒå…¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒToken Distillationåœ¨å¤šç§æ¨¡å‹ä¸­è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„è¯­è¨€æ¨¡å‹ä¾èµ–äºåœ¨é¢„è®­ç»ƒæ—¶ç¡®å®šçš„é™æ€è¯æ±‡ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨åŸå§‹è¯æ±‡ä¸­è¡¨ç°ä¸è¶³çš„é¢†åŸŸæ€§èƒ½ä¸‹é™å’Œè®¡ç®—æˆæœ¬å¢åŠ ã€‚é€šè¿‡æ·»åŠ æ–°è¯å¹¶ä¸ºå…¶æ–°åµŒå…¥æä¾›è‰¯å¥½çš„åˆå§‹åŒ–ï¼Œå¯ä»¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åµŒå…¥åˆå§‹åŒ–æ–¹æ³•éœ€è¦æ˜‚è´µçš„è¿›ä¸€æ­¥è®­ç»ƒæˆ–é¢å¤–æ¨¡å—çš„é¢„è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†Token Distillationæ–¹æ³•ï¼Œå±•ç¤ºäº†é€šè¿‡è’¸é¦ä½¿ç”¨åŸå§‹åˆ†è¯è·å¾—çš„è¡¨ç¤ºï¼Œå¯ä»¥å¿«é€Ÿå­¦ä¹ æ–°è¯çš„é«˜è´¨é‡è¾“å…¥åµŒå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒToken Distillationåœ¨å¤šç§å¼€æ”¾æƒé‡æ¨¡å‹ä¸­è¡¨ç°ä¼˜äºå¼ºåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä¸­ç”±äºé™æ€è¯æ±‡å¯¼è‡´çš„æ€§èƒ½ä¸‹é™å’Œè®¡ç®—æˆæœ¬å¢åŠ ã€‚ç°æœ‰çš„åµŒå…¥åˆå§‹åŒ–æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–é¢„è®­ç»ƒï¼Œå¢åŠ äº†å¤æ‚æ€§å’Œæˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡Token Distillationæ–¹æ³•ï¼Œåˆ©ç”¨åŸå§‹åˆ†è¯çš„è¡¨ç¤ºæ¥å¿«é€Ÿå­¦ä¹ æ–°è¯çš„é«˜è´¨é‡åµŒå…¥ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ˜‚è´µçš„è®­ç»ƒè¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨è¾ƒçŸ­æ—¶é—´å†…è·å¾—æœ‰æ•ˆçš„è¯åµŒå…¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨åŸå§‹è¯æ±‡çš„è¡¨ç¤ºè¿›è¡Œè’¸é¦ï¼›å…¶æ¬¡ï¼Œç”Ÿæˆæ–°è¯çš„åµŒå…¥ï¼›æœ€åï¼Œè¯„ä¼°æ–°åµŒå…¥åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºToken Distillationçš„æå‡ºï¼Œå®ƒé€šè¿‡è’¸é¦æŠ€æœ¯æœ‰æ•ˆåœ°åˆ©ç”¨äº†å·²æœ‰çš„è¯æ±‡è¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜äº†æ–°è¯åµŒå…¥çš„å­¦ä¹ æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿçš„åˆå§‹åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œèµ„æºæ¶ˆè€—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ–°è¯åµŒå…¥çš„è´¨é‡ï¼Œå¹¶åœ¨å¤šç§å¼€æ”¾æƒé‡æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç¡®ä¿äº†æ–¹æ³•çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒToken Distillationåœ¨å¤šä¸ªå¼€æ”¾æƒé‡æ¨¡å‹ä¸­å‡ä¼˜äºå¼ºåŸºçº¿ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šæå‡äº†5%è‡³10%çš„æ€§èƒ½ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–°è¯åµŒå…¥å­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æå’Œæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚é€šè¿‡å¿«é€Ÿå­¦ä¹ æ–°è¯çš„åµŒå…¥ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§å’Œæ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“æ–°è¯æ±‡çš„å¤„ç†æ–¹å¼ï¼Œä¿ƒè¿›æ›´é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current language models rely on static vocabularies determined at pretraining time, which can lead to decreased performance and increased computational cost for domains underrepresented in the original vocabulary. New tokens can be added to solve this problem, when coupled with a good initialization for their new embeddings. However, existing embedding initialization methods require expensive further training or pretraining of additional modules. In this paper, we propose Token Distillation and show that by distilling representations obtained using the original tokenization, we can quickly learn high-quality input embeddings for new tokens. Experimental results with a wide range of open-weight models show that Token Distillation outperforms even strong baselines.

