---
layout: default
title: "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs"
---

# Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20155" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.20155v1</a>
  <a href="https://arxiv.org/pdf/2505.20155.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20155v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20155v1', 'Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hanting Chen, Jiarui Qin, Jialong Guo, Tao Yuan, Yichun Yin, Huiling Zhen, Yasheng Wang, Jinpeng Li, Xiaojun Meng, Meng Zhang, Rongju Ruan, Zheyuan Bai, Yehui Tang, Can Chen, Xinghao Chen, Fisher Yu, Ruiming Tang, Yunhe Wang

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-26

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Pangu Light‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂéãÁº©‰∏éÂä†ÈÄüÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÁªìÊûÑÂåñÂâ™Êûù` `ÊùÉÈáçÈáçÂàùÂßãÂåñ` `Ê®°ÂûãÂéãÁº©` `Ascend NPU` `ÊÄßËÉΩ‰ºòÂåñ` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÁªìÊûÑÂåñÂâ™ÊûùÊñπÊ≥ïÂú®ÂÆΩÂ∫¶ÂíåÊ∑±Â∫¶ÁöÑÊøÄËøõÂáèÂ∞ë‰∏≠ÔºåÂ∏∏ÂØºËá¥Ê®°ÂûãÊÄßËÉΩÊòæËëó‰∏ãÈôçÔºåÈöæ‰ª•ÊúâÊïàÈÉ®ÁΩ≤„ÄÇ
2. Pangu LightÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•ÊùÉÈáçÈáçÂàùÂßãÂåñÊäÄÊúØÔºåÊîπÂñÑ‰∫ÜÂâ™ÊûùÂêéÊ®°ÂûãÁöÑËÆ≠ÁªÉËµ∑ÁÇπÔºå‰ªéËÄåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ
3. Âú®Ascend NPU‰∏äÔºåPangu Light-32BÁöÑÂπ≥ÂùáÂæóÂàÜ‰∏∫81.6ÔºåÂêûÂêêÈáè‰∏∫2585 tokens/sÔºåË∂ÖË∂ä‰∫ÜQwen3-32BÁöÑ80.9ÂæóÂàÜÂíå2225 tokens/sÂêûÂêêÈáè„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ºóÂ§ö‰ªªÂä°‰∏≠Â±ïÁé∞‰∫ÜÂçìË∂äÁöÑËÉΩÂäõÔºå‰ΩÜÂÖ∂Â∫ûÂ§ßÁöÑËßÑÊ®°ÂíåÊé®ÁêÜÊàêÊú¨‰∏∫ÂÆûÈôÖÈÉ®ÁΩ≤Â∏¶Êù•‰∫ÜÊòæËëóÁöÑËÆ°ÁÆóÊåëÊàò„ÄÇÂ∞ΩÁÆ°ÁªìÊûÑÂåñÂâ™Êûù‰∏∫Ê®°ÂûãÂéãÁº©Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÈÄîÂæÑÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®ÂÆΩÂ∫¶ÂíåÊ∑±Â∫¶ÁöÑÊøÄËøõÂêåÊó∂ÂáèÂ∞ëÊó∂ÔºåÂ∏∏Â∏∏Èù¢‰∏¥ÊÄßËÉΩÊòæËëó‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫Pangu LightÊ°ÜÊû∂ÔºåÈÄöËøáÁªìÊûÑÂåñÂâ™ÊûùÂíåÊñ∞È¢ñÁöÑÊùÉÈáçÈáçÂàùÂßãÂåñÊäÄÊúØÔºåËß£ÂÜ≥‰∫ÜËøô‰∏ÄÂÖ≥ÈîÆÈóÆÈ¢òÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑËÆ≠ÁªÉÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPangu LightÂú®Ascend NPU‰∏äË°®Áé∞Âá∫‰ºòË∂äÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫Á∫øÂâ™ÊûùÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËøõË°åÁªìÊûÑÂåñÂâ™ÊûùÊó∂ÔºåÂõ†ÂÆΩÂ∫¶ÂíåÊ∑±Â∫¶ÁöÑÊøÄËøõÂáèÂ∞ëËÄåÂØºËá¥ÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÂøΩËßÜ‰∫ÜÂâ™ÊûùÂêéÊùÉÈáçÁöÑÈáçÂàùÂßãÂåñÔºåÂØºËá¥Ê®°ÂûãËÆ≠ÁªÉÂáÜÁ°ÆÊÄß‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPangu LightÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÊàòÁï•ÊÄßÂú∞ÈáçÂàùÂßãÂåñÂíåË∞ÉÊï¥Ââ©‰ΩôÊùÉÈáçÔºåÊîπÂñÑÂâ™ÊûùÂêéÊ®°ÂûãÁöÑËÆ≠ÁªÉËµ∑ÁÇπÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂Á≥ªÁªüÊÄßÂú∞ÈíàÂØπÊ®°ÂûãÁöÑÂ§ö‰∏™Áª¥Â∫¶ËøõË°å‰ºòÂåñÔºåÂåÖÊã¨ÂÆΩÂ∫¶„ÄÅÊ∑±Â∫¶„ÄÅÊ≥®ÊÑèÂäõÂ§¥ÂíåRMSNormÔºåÁªìÂêàÊñ∞È¢ñÁöÑÈáçÂàùÂßãÂåñÊñπÊ≥ïÂ¶ÇË∑®Â±ÇÊ≥®ÊÑèÂäõÂâ™ÊûùÔºàCLAPÔºâÂíåÁ®≥ÂÆöÂ±ÇÂΩí‰∏ÄÂåñÂâ™ÊûùÔºàSLNPÔºâ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPangu LightÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÊùÉÈáçÈáçÂàùÂßãÂåñÊäÄÊúØÔºåÁâπÂà´ÊòØCLAPÂíåSLNPÔºåËøô‰∫õÊñπÊ≥ïÊúâÊïàÂáèËΩª‰∫ÜÂâ™ÊûùÂ∏¶Êù•ÁöÑÊÄßËÉΩ‰∏ãÈôçÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑËÆ≠ÁªÉËµ∑ÁÇπ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåPangu LightÂê∏Êî∂‰∫ÜÂêéRMSNormËÆ°ÁÆóÔºåÂπ∂Ê†πÊçÆAscend NPUÁöÑÁâπÊÄßÈáèË∫´ÂÆöÂà∂‰∫Ü‰ºòÂåñÁ≠ñÁï•ÔºåÁ°Æ‰øù‰∫ÜÈ´òÊïàÁöÑÊ®°ÂûãÊé®ÁêÜÂíåËÆ≠ÁªÉ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Êú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÂæÖËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Pangu LightÂú®Ascend NPU‰∏äÂÆûÁé∞‰∫Ü81.6ÁöÑÂπ≥ÂùáÂæóÂàÜÂíå2585 tokens/sÁöÑÂêûÂêêÈáèÔºåÊòæËëóË∂ÖË∂ä‰∫ÜQwen3-32BÁöÑ80.9ÂæóÂàÜÂíå2225 tokens/sÂêûÂêêÈáèÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂáÜÁ°ÆÊÄßÂíåÊïàÁéá‰∏äÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Pangu LightÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´òÊïàÊé®ÁêÜÂíå‰ΩéÂª∂ËøüÂìçÂ∫îÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÈÉ®ÁΩ≤Âú∫ÊôØ‰∏≠„ÄÇÂÖ∂‰ºòÂåñÁ≠ñÁï•ÂèØ‰∏∫ÂÆûÈôÖÂ∫îÁî®Êèê‰æõÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÔºåÊé®Âä®Êô∫ËÉΩÂØπËØùÁ≥ªÁªü„ÄÅËá™Âä®ÁøªËØëÂíåÂÜÖÂÆπÁîüÊàêÁ≠âÈ¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) deliver state-of-the-art capabilities across numerous tasks, but their immense size and inference costs pose significant computational challenges for practical deployment. While structured pruning offers a promising avenue for model compression, existing methods often struggle with the detrimental effects of aggressive, simultaneous width and depth reductions, leading to substantial performance degradation. This paper argues that a critical, often overlooked, aspect in making such aggressive joint pruning viable is the strategic re-initialization and adjustment of remaining weights to improve the model post-pruning training accuracies. We introduce Pangu Light, a framework for LLM acceleration centered around structured pruning coupled with novel weight re-initialization techniques designed to address this ``missing piece''. Our framework systematically targets multiple axes, including model width, depth, attention heads, and RMSNorm, with its effectiveness rooted in novel re-initialization methods like Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP) that mitigate performance drops by providing the network a better training starting point. Further enhancing efficiency, Pangu Light incorporates specialized optimizations such as absorbing Post-RMSNorm computations and tailors its strategies to Ascend NPU characteristics. The Pangu Light models consistently exhibit a superior accuracy-efficiency trade-off, outperforming prominent baseline pruning methods like Nemotron and established LLMs like Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and 2225 tokens/s.

