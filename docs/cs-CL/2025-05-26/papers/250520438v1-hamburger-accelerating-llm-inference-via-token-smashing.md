---
layout: default
title: HAMburger: Accelerating LLM Inference via Token Smashing
---

# HAMburger: Accelerating LLM Inference via Token Smashing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20438" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20438v1</a>
  <a href="https://arxiv.org/pdf/2505.20438.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20438v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20438v1', 'HAMburger: Accelerating LLM Inference via Token Smashing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingyu Liu, Ce Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHAMburgerä»¥åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `èµ„æºä¼˜åŒ–` `è‡ªå›å½’æ¨¡å‹` `KVç¼“å­˜` `åŠ¨æ€è°ƒæ•´` `æ–‡æœ¬ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ–¹æ³•åœ¨æ¯ä¸ªtokenä¸Šå‡åŒ€åˆ†é…è®¡ç®—å’Œå­˜å‚¨ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. HAMburgeré€šè¿‡åˆ†å±‚è‡ªå›å½’æ¨¡å‹ï¼Œé‡æ–°å®šä¹‰äº†LLMçš„èµ„æºåˆ†é…ï¼Œå…è®¸å¤šä¸ªtokenå…±äº«KVç¼“å­˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHAMburgeråœ¨çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬ä»»åŠ¡ä¸­å‡ä¿æŒè´¨é‡ï¼ŒåŒæ—¶KVç¼“å­˜è®¡ç®—å’ŒTPSå‡æå‡äº†2å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¹é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œç®—æ³•ã€ç³»ç»Ÿå’Œç¡¬ä»¶çš„æ•´ä½“ä¼˜åŒ–æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆæ¨¡å¼ä¸Šç¼ºä¹æ ¹æœ¬æ€§å˜åŒ–ï¼šæ¯ä¸ªtokenéƒ½éœ€è¦ä¸€æ¬¡å‰å‘ä¼ æ’­å’Œä¸€ä¸ªKVç¼“å­˜ã€‚åŸºäºLLMèƒ½å¤Ÿè‡ªæˆ‘è¯†åˆ«ä¿¡æ¯å­˜å‚¨é‡çš„æ´å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†HAMburgerï¼Œä¸€ä¸ªåˆ†å±‚è‡ªå›å½’æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡æ–°å®šä¹‰èµ„æºåˆ†é…ï¼Œæ‰“ç ´äº†æ¯ä¸ªtokenå‡åŒ€è®¡ç®—å’Œå­˜å‚¨çš„é™åˆ¶ã€‚HAMburgeré€šè¿‡å°†å¤šä¸ªtokenå‹ç¼©ä¸ºä¸€ä¸ªKVå¹¶åœ¨æ¯ä¸€æ­¥ç”Ÿæˆå¤šä¸ªtokenï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHAMburgeråœ¨ä¿æŒä»»åŠ¡è´¨é‡çš„åŒæ—¶ï¼ŒKVç¼“å­˜è®¡ç®—å‡å°‘äº†æœ€å¤š2å€ï¼ŒTPSæå‡äº†æœ€å¤š2å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¯ä¸ªtokenä¸Šå‡åŒ€åˆ†é…è®¡ç®—å’Œå­˜å‚¨ï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œæ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHAMburgeré€šè¿‡å°†å¤šä¸ªtokenå‹ç¼©ä¸ºä¸€ä¸ªKVç¼“å­˜ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥ç”Ÿæˆå¤šä¸ªtokenï¼Œçªç ´äº†ä¼ ç»Ÿçš„æ¨ç†æ¨¡å¼ï¼Œä»è€Œæé«˜äº†è®¡ç®—å’Œå†…å­˜çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHAMburgerçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç»„åˆåµŒå…¥å™¨å’Œä¸€ä¸ªå¾®æ­¥è§£ç å™¨ï¼Œä½äºåŸºç¡€LLMä¹‹é—´ï¼Œå½¢æˆä¸€ä¸ªåˆ†å±‚è‡ªå›å½’æ¨¡å‹ã€‚è¯¥æ¡†æ¶å…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºã€‚

**å…³é”®åˆ›æ–°**ï¼šHAMburgerçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†å¤šä¸ªtokenåˆå¹¶åˆ°ä¸€ä¸ªKVç¼“å­˜ä¸­ï¼Œæ˜¾è‘—é™ä½äº†KVç¼“å­˜å’Œå‰å‘è®¡ç®—çš„çº¿æ€§å¢é•¿ï¼Œä½¿å¾—æ¨ç†é€Ÿåº¦ä¸è¾“å‡ºé•¿åº¦å‘ˆäºšçº¿æ€§å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šHAMburgerçš„è®¾è®¡åŒ…æ‹¬è‡ªè‰æ‹Ÿtokençš„ç›²ä¿¡ä»»æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹åœ¨ç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ç”Ÿæˆtokenï¼ŒåŒæ—¶æ ¹æ®æŸ¥è¯¢çš„å›°æƒ‘åº¦å’Œè¾“å‡ºç»“æ„åŠ¨æ€è°ƒæ•´æ¨ç†é€Ÿåº¦ã€‚è¯¥æ–¹æ³•åœ¨ç¡¬ä»¶ä¸Šå…·æœ‰æ— å…³æ€§ï¼Œé€‚ç”¨äºå¤šç§è®¡ç®—å¹³å°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HAMburgeråœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼ŒKVç¼“å­˜è®¡ç®—å‡å°‘äº†æœ€å¤š2å€ï¼ŒTPSæå‡äº†æœ€å¤š2å€ï¼ŒåŒæ—¶åœ¨çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬ä»»åŠ¡ä¸­ä¿æŒäº†é«˜è´¨é‡çš„ç”Ÿæˆæ•ˆæœï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†æ•ˆç‡å’Œå†…å­˜åˆ©ç”¨ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HAMburgerçš„ç ”ç©¶æˆæœåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ”¯æŒæ›´å¤æ‚çš„åº”ç”¨åœºæ™¯ï¼Œå¦‚å®æ—¶å¯¹è¯ç”Ÿæˆå’Œå¤§è§„æ¨¡æ–‡æœ¬åˆ†æï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing demand for efficient Large Language Model (LLM) inference requires a holistic optimization on algorithms, systems, and hardware. However, very few works have fundamentally changed the generation pattern: each token needs one forward pass and one KV cache. This can be sub-optimal because we found that LLMs are extremely capable of self-identifying the exact dose of information that a single KV cache can store, and many tokens can be generated confidently without global context. Based on this insight, we introduce HAMburger, a Hierarchically Auto-regressive Model that redefines resource allocation in LLMs by moving beyond uniform computation and storage per token during inference. Stacking a compositional embedder and a micro-step decoder in between a base LLM, HAMburger smashes multiple tokens into a single KV and generates several tokens per step. Additionally, HAMburger functions as a speculative decoding framework where it can blindly trust self-drafted tokens. As a result, HAMburger shifts the growth of KV cache and forward FLOPs from linear to sub-linear with respect to output length, and adjusts its inference speed based on query perplexity and output structure. Extensive evaluations show that HAMburger reduces the KV cache computation by up to 2$\times$ and achieves up to 2$\times$ TPS, while maintaining quality in both short- and long-context tasks. Our method explores an extremely challenging inference regime that requires both computation- and memory-efficiency with a hardware-agnostic design.

