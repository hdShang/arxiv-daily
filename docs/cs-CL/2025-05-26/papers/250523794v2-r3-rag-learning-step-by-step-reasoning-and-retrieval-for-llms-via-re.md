---
layout: default
title: R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning
---

# R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23794" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23794v2</a>
  <a href="https://arxiv.org/pdf/2505.23794.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23794v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23794v2', 'R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang, Yining Zheng, Yuxin Wang, Zhangyue Yin, Xipeng Qiu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-10-24)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Yuan-Li-FNLP/R3-RAG)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR3-RAGä»¥è§£å†³RAGç³»ç»Ÿä¸­çš„æ¨ç†ä¸æ£€ç´¢ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `å¤–éƒ¨çŸ¥è¯†` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RAGç³»ç»Ÿåœ¨æ¨ç†å’Œæ£€ç´¢æ–¹é¢å­˜åœ¨ç“¶é¢ˆï¼Œå¯†é›†æ£€ç´¢å™¨çš„å‚æ•°æœ‰é™ä¸”æ— æ³•è¿›è¡Œé€æ­¥æ¨ç†ã€‚
2. æœ¬æ–‡æå‡ºR3-RAGï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿LLMé€æ­¥å­¦ä¹ æ¨ç†ä¸æ£€ç´¢ï¼Œæå‡å¤–éƒ¨çŸ¥è¯†çš„è·å–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒR3-RAGåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆé€‚åº”ä¸åŒçš„æ£€ç´¢å™¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å°†å¤–éƒ¨çŸ¥è¯†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆï¼Œä»¥æé«˜äº‹å®æ­£ç¡®æ€§å¹¶å‡å°‘å¹»è§‰ã€‚ç„¶è€Œï¼Œå¯†é›†æ£€ç´¢å™¨ç”±äºå‚æ•°æœ‰é™å’Œç¼ºä¹é€æ­¥æ¨ç†èƒ½åŠ›ï¼Œå¸¸æˆä¸ºRAGç³»ç»Ÿçš„ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†R3-RAGï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿LLMé€æ­¥å­¦ä¹ æ¨ç†å’Œæ£€ç´¢ï¼Œä»è€Œè·å–å…¨é¢çš„å¤–éƒ¨çŸ¥è¯†å¹¶å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚R3-RAGåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡å†·å¯åŠ¨ä½¿æ¨¡å‹å­¦ä¹ é€æ­¥æ¨ç†å’Œæ£€ç´¢çš„æ–¹å¼ï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æå‡å…¶æ¢ç´¢å¤–éƒ¨æ£€ç´¢ç¯å¢ƒçš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR3-RAGæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå¹¶èƒ½è‰¯å¥½è¿ç§»åˆ°ä¸åŒçš„æ£€ç´¢å™¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RAGç³»ç»Ÿä¸­å¯†é›†æ£€ç´¢å™¨çš„æ¨ç†èƒ½åŠ›ä¸è¶³å’Œå‚æ•°é™åˆ¶çš„é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„ç­”æ¡ˆä¸å¤Ÿå‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šR3-RAGé€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿LLMé€æ­¥å­¦ä¹ æ¨ç†å’Œæ£€ç´¢ï¼Œè®¾è®¡äº†å†·å¯åŠ¨å’Œå¼ºåŒ–å­¦ä¹ ä¸¤ä¸ªé˜¶æ®µï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR3-RAGçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å†·å¯åŠ¨ï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•è¿­ä»£åœ°è¿›è¡Œæ¨ç†å’Œæ£€ç´¢ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨å¤–éƒ¨æ£€ç´¢ç¯å¢ƒä¸­çš„æ¢ç´¢èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šR3-RAGçš„åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸¤ç§å¥–åŠ±æœºåˆ¶ï¼šç­”æ¡ˆæ­£ç¡®æ€§ä½œä¸ºç»“æœå¥–åŠ±ï¼Œæ–‡æ¡£ç›¸å…³æ€§éªŒè¯ä½œä¸ºè¿‡ç¨‹å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹æ£€ç´¢ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ¨¡å‹çš„æŸå¤±å‡½æ•°ç»“åˆäº†ç»“æœå¥–åŠ±å’Œè¿‡ç¨‹å¥–åŠ±ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†å’Œæ£€ç´¢è¿‡ç¨‹ä¸­ä¸æ–­ä¼˜åŒ–å…¶è¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒR3-RAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ç­”æ¡ˆæ­£ç¡®æ€§ä¸Šæé«˜äº†15%ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ£€ç´¢å™¨ä¸Šçš„è¿ç§»èƒ½åŠ›è¡¨ç°è‰¯å¥½ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R3-RAGçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜å‡†ç¡®æ€§å’Œå¯é æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚é—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ¨ç†ä¸æ£€ç´¢èƒ½åŠ›ï¼ŒR3-RAGèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ä¸ºç²¾å‡†å’Œç›¸å…³çš„ä¿¡æ¯ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Retrieval-Augmented Generation (RAG) integrates external knowledge with Large Language Models (LLMs) to enhance factual correctness and mitigate hallucination. However, dense retrievers often become the bottleneck of RAG systems due to their limited parameters compared to LLMs and their inability to perform step-by-step reasoning. While prompt-based iterative RAG attempts to address these limitations, it is constrained by human-designed workflows. To address these limitations, we propose $\textbf{R3-RAG}$, which uses $\textbf{R}$einforcement learning to make the LLM learn how to $\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving comprehensive external knowledge and leading to correct answers. R3-RAG is divided into two stages. We first use cold start to make the model learn the manner of iteratively interleaving reasoning and retrieval. Then we use reinforcement learning to further harness its ability to better explore the external retrieval environment. Specifically, we propose two rewards for R3-RAG: 1) answer correctness for outcome reward, which judges whether the trajectory leads to a correct answer; 2) relevance-based document verification for process reward, encouraging the model to retrieve documents that are relevant to the user question, through which we can let the model learn how to iteratively reason and retrieve relevant documents to get the correct answer. Experimental results show that R3-RAG significantly outperforms baselines and can transfer well to different retrievers. We release R3-RAG at https://github.com/Yuan-Li-FNLP/R3-RAG.

