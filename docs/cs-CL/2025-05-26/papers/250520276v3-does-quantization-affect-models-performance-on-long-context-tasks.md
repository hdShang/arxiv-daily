---
layout: default
title: "Does quantization affect models' performance on long-context tasks?"
---

# Does quantization affect models' performance on long-context tasks?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20276" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20276v3</a>
  <a href="https://arxiv.org/pdf/2505.20276.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20276v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20276v3', 'Does quantization affect models\' performance on long-context tasks?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anmol Mekala, Anirudh Atmakuru, Yixiao Song, Marzena Karpinska, Mohit Iyyer

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-09-20)

**å¤‡æ³¨**: to appear in EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿè¯„ä¼°é‡åŒ–å¯¹é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡` `é‡åŒ–æ¨¡å‹` `æ€§èƒ½è¯„ä¼°` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡å¤„ç†æ–¹æ³•åœ¨é‡åŒ–åå¯èƒ½ä¼šæ˜¾è‘—é™ä½æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éè‹±è¯­è¾“å…¥æ—¶ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸åŒé‡åŒ–æ–¹æ³•å¯¹é•¿è¾“å…¥ä»»åŠ¡çš„å½±å“ï¼Œæå‡ºäº†é‡åŒ–æ¨¡å‹çš„æ€§èƒ½è¯„ä¼°æ¡†æ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œ8ä½é‡åŒ–å¯¹å‡†ç¡®æ€§å½±å“è¾ƒå°ï¼Œè€Œ4ä½é‡åŒ–æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­æŸå¤±ä¸¥é‡ï¼Œå¼ºè°ƒäº†ä»»åŠ¡ç‰¹å¼‚æ€§è¯„ä¼°çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç°åœ¨æ”¯æŒè¶…è¿‡128Kæ ‡è®°çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œä½†è¿™å¸¦æ¥äº†æ˜¾è‘—çš„å†…å­˜éœ€æ±‚å’Œé«˜æ¨ç†å»¶è¿Ÿã€‚é‡åŒ–å¯ä»¥ç¼“è§£è¿™äº›æˆæœ¬ï¼Œä½†å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†é‡åŒ–LLMsåœ¨é•¿è¾“å…¥ï¼ˆ>64Kæ ‡è®°ï¼‰å’Œé•¿è¾“å‡ºä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†9.7Kä¸ªæµ‹è¯•ç¤ºä¾‹ã€äº”ç§é‡åŒ–æ–¹æ³•ï¼ˆFP8ã€GPTQ-int8ã€AWQ-int4ã€GPTQ-int4ã€BNB-nf4ï¼‰å’Œäº”ä¸ªæ¨¡å‹ï¼ˆLlama-3.1 8Bå’Œ70Bï¼›Qwen-2.5 7Bã€32Bå’Œ72Bï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œ8ä½é‡åŒ–å¹³å‡ä¿æŒå‡†ç¡®æ€§ï¼ˆçº¦0.8%çš„ä¸‹é™ï¼‰ï¼Œè€Œ4ä½æ–¹æ³•åˆ™å¯¼è‡´æ˜¾è‘—æŸå¤±ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠé•¿ä¸Šä¸‹æ–‡è¾“å…¥çš„ä»»åŠ¡ä¸­ï¼ˆä¸‹é™é«˜è¾¾59%ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é‡åŒ–å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­æ€§èƒ½å½±å“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è¾“å…¥æ—¶ï¼Œé‡åŒ–å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨éè‹±è¯­è¾“å…¥çš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸åŒé‡åŒ–æ–¹æ³•ï¼ˆå¦‚FP8ã€GPTQ-int8ç­‰ï¼‰å¯¹é•¿è¾“å…¥ä»»åŠ¡çš„å½±å“ï¼Œæä¾›äº†é‡åŒ–æ¨¡å‹æ€§èƒ½çš„å…¨é¢åˆ†æï¼Œå¼ºè°ƒäº†ä»»åŠ¡ç‰¹å¼‚æ€§çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†9.7Kä¸ªæµ‹è¯•ç¤ºä¾‹ï¼Œä½¿ç”¨äº”ç§é‡åŒ–æ–¹æ³•å’Œäº”ä¸ªä¸åŒçš„æ¨¡å‹ï¼Œæ¯”è¾ƒå®ƒä»¬åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†é‡åŒ–LLMsåœ¨é•¿è¾“å…¥ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä¸åŒé‡åŒ–æ–¹æ³•ã€æ¨¡å‹å’Œä»»åŠ¡ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œæä¾›äº†é‡åŒ–æ¨¡å‹ä½¿ç”¨çš„æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§é‡åŒ–æ–¹æ³•ï¼Œå¹¶å¯¹æ¯”äº†ä¸åŒæ¨¡å‹çš„è¡¨ç°ï¼Œç‰¹åˆ«å…³æ³¨äº†8ä½å’Œ4ä½é‡åŒ–å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ï¼Œå‘ç°8ä½é‡åŒ–ä¿æŒäº†çº¦0.8%çš„å‡†ç¡®æ€§ï¼Œè€Œ4ä½é‡åŒ–åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­å¯èƒ½å¯¼è‡´é«˜è¾¾59%çš„æ€§èƒ½ä¸‹é™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ8ä½é‡åŒ–æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ä»…å¯¼è‡´çº¦0.8%çš„æ€§èƒ½ä¸‹é™ï¼Œè€Œ4ä½é‡åŒ–æ–¹æ³•åˆ™åœ¨æŸäº›ä»»åŠ¡ä¸­å¯¼è‡´é«˜è¾¾59%çš„æ€§èƒ½æŸå¤±ã€‚ä¸åŒæ¨¡å‹åœ¨ç›¸åŒé‡åŒ–æ–¹æ³•ä¸‹è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå¼ºè°ƒäº†ä»»åŠ¡ç‰¹å¼‚æ€§è¯„ä¼°çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„åœºæ™¯ä¸­ã€‚é€šè¿‡ä¼˜åŒ–é‡åŒ–æ–¹æ³•ï¼Œå¯ä»¥åœ¨é™ä½å†…å­˜éœ€æ±‚å’Œæ¨ç†å»¶è¿Ÿçš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œæå‡å®é™…åº”ç”¨çš„æ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long-context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and for languages other than English.

