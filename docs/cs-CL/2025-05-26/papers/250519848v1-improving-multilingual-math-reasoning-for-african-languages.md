---
layout: default
title: Improving Multilingual Math Reasoning for African Languages
---

# Improving Multilingual Math Reasoning for African Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19848" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19848v1</a>
  <a href="https://arxiv.org/pdf/2505.19848.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19848v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19848v1', 'Improving Multilingual Math Reasoning for African Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Odunayo Ogundepo, Akintunde Oladipo, Kelechi Ogueji, Esther Adenuga, David Ifeoluwa Adelani, Jimmy Lin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šé˜¶æ®µé€‚åº”ç­–ç•¥ä»¥æ”¹å–„éæ´²è¯­è¨€çš„æ•°å­¦æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½èµ„æºè¯­è¨€` `æ•°å­¦æ¨ç†` `æ¨¡å‹é€‚åº”` `å¤šé˜¶æ®µè®­ç»ƒ` `æ•°æ®ç±»å‹ç»„åˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ï¼Œå°¤å…¶æ˜¯éæ´²è¯­è¨€çš„é€‚åº”æ€§ä¸è¶³ï¼Œå¯¼è‡´æ•°å­¦æ¨ç†èƒ½åŠ›æœ‰é™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¤šé˜¶æ®µçš„é¢„è®­ç»ƒå’Œåè®­ç»ƒç­–ç•¥ï¼Œç»“åˆä¸åŒæ•°æ®ç±»å‹ï¼Œä¼˜åŒ–æ¨¡å‹åœ¨éæ´²è¯­è¨€ä¸Šçš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç‰¹å®šçš„é€‚åº”ç­–ç•¥ç»„åˆæ˜¾è‘—æå‡äº†æ•°å­¦æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç ”ç©¶è€…åœ¨ä½èµ„æºè¯­è¨€çš„ç ”ç©¶ä¸­é¢ä¸´æ•°æ®å¯ç”¨æ€§æœ‰é™å’Œè®¡ç®—èµ„æºå—é™çš„æŒ‘æˆ˜ã€‚å°½ç®¡å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸»è¦åœ¨é«˜èµ„æºè¯­è¨€ä¸Šè®­ç»ƒï¼Œä½†å°†å…¶é€‚åº”äºä½èµ„æºç¯å¢ƒï¼Œç‰¹åˆ«æ˜¯éæ´²è¯­è¨€ï¼Œéœ€è¦ä¸“é—¨çš„æŠ€æœ¯ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å“ªäº›é€‚åº”ç­–ç•¥åœ¨å°†ç°æœ‰LLMsæ‰©å±•åˆ°éæ´²è¯­è¨€æ—¶è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œä»¥è¯„ä¼°ä¸åŒæ•°æ®ç±»å‹ï¼ˆç¿»è¯‘ä¸åˆæˆç”Ÿæˆï¼‰ã€è®­ç»ƒé˜¶æ®µï¼ˆé¢„è®­ç»ƒä¸åè®­ç»ƒï¼‰åŠå…¶ä»–æ¨¡å‹é€‚åº”é…ç½®çš„ç»„åˆã€‚å®éªŒé‡ç‚¹å…³æ³¨æ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œä»¥Llama 3.1æ¨¡å‹ç³»åˆ—ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éæ´²è¯­è¨€æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€çš„è®­ç»ƒä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹æ€§èƒ½ä¸ä½³çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç³»ç»Ÿæ€§åœ°è¯„ä¼°å¤šç§é€‚åº”ç­–ç•¥ï¼Œç¡®å®šæœ€ä½³çš„è®­ç»ƒç»„åˆï¼Œä»¥æå‡æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚é‡‡ç”¨å¤šé˜¶æ®µçš„é¢„è®­ç»ƒå’Œåè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆç¿»è¯‘å’Œåˆæˆæ•°æ®ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹é¢„è®­ç»ƒã€åè®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†å’Œå¤„ç†ä¸åŒç±»å‹çš„æ•°æ®ï¼Œç„¶åè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¥ç€è¿›è¡Œåè®­ç»ƒä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ï¼Œæœ€åé€šè¿‡ä¸€ç³»åˆ—æ•°å­¦æ¨ç†ä»»åŠ¡è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†ä¸åŒæ•°æ®ç±»å‹å’Œè®­ç»ƒé˜¶æ®µçš„ç»„åˆæ•ˆæœï¼Œæ˜ç¡®äº†åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹çš„æœ€ä½³é€‚åº”ç­–ç•¥ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€è®­ç»ƒç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šç§å‚æ•°ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°å’Œè®­ç»ƒè½®æ•°ç­‰ã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†ä¸åŒæ•°æ®ç±»å‹çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ç‰¹å®šçš„å¤šé˜¶æ®µé€‚åº”ç­–ç•¥åï¼Œæ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦15%ã€‚ä¸åŒæ•°æ®ç±»å‹çš„ç»„åˆä½¿ç”¨ä¹Ÿæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ•ˆæœå·®å¼‚ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººå·¥æ™ºèƒ½ç­‰ã€‚é€šè¿‡æ”¹å–„éæ´²è¯­è¨€çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºæ•™è‚²èµ„æºåŒ®ä¹åœ°åŒºæä¾›æ›´å¥½çš„å­¦ä¹ å·¥å…·ï¼Œä¿ƒè¿›è¯­è¨€å¤šæ ·æ€§å’Œæ–‡åŒ–ä¼ æ‰¿ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶çš„æˆæœä¹Ÿå¯ä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€çš„æ¨¡å‹é€‚åº”æä¾›å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Researchers working on low-resource languages face persistent challenges due to limited data availability and restricted access to computational resources. Although most large language models (LLMs) are predominantly trained in high-resource languages, adapting them to low-resource contexts, particularly African languages, requires specialized techniques. Several strategies have emerged for adapting models to low-resource languages in todays LLM landscape, defined by multi-stage pre-training and post-training paradigms. However, the most effective approaches remain uncertain. This work systematically investigates which adaptation strategies yield the best performance when extending existing LLMs to African languages. We conduct extensive experiments and ablation studies to evaluate different combinations of data types (translated versus synthetically generated), training stages (pre-training versus post-training), and other model adaptation configurations. Our experiments focuses on mathematical reasoning tasks, using the Llama 3.1 model family as our base model.

