---
layout: default
title: "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages"
---

# Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19851" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19851v1</a>
  <a href="https://arxiv.org/pdf/2505.19851.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19851v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19851v1', 'Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gulfarogh Azam, Mohd Sadique, Saif Ali, Mohammad Nadeem, Erik Cambria, Shahab Saquib Sohail, Mohammad Sultan Alam

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°åº¦è¯­è¨€éŸ³è¯‘ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³è¯‘` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè¯­è¨€å¤„ç†` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„éŸ³è¯‘æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–è¯­è¨€æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å°åº¦è¿™æ ·çš„è¯­è¨€ç¯å¢ƒä¸­ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éŸ³è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¢ç´¢å…¶åœ¨æ²¡æœ‰ä¸“é—¨è®­ç»ƒçš„æƒ…å†µä¸‹çš„æ½œåŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPTç³»åˆ—æ¨¡å‹åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºIndicXlitï¼Œä¸”å¾®è°ƒååœ¨ç‰¹å®šè¯­è¨€ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éŸ³è¯‘æ˜¯å°†æ–‡æœ¬ä»ä¸€ç§ä¹¦å†™ç³»ç»Ÿæ˜ å°„åˆ°å¦ä¸€ç§ä¹¦å†™ç³»ç»Ÿçš„è¿‡ç¨‹ï¼Œåœ¨å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ç‰¹åˆ«é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨è¯­è¨€å¤šæ ·æ€§ä¸°å¯Œçš„å°åº¦ã€‚å°½ç®¡ä¸“é—¨æ¨¡å‹å¦‚IndicXlitå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­¤ä»»åŠ¡ä¸Šä¹Ÿå±•ç°å‡ºæ½œåŠ›ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†åŒ…æ‹¬GPT-4oã€GPT-4.5ã€GPT-4.1ç­‰åœ¨å†…çš„å¤šç§LLMsåœ¨åç§ä¸»è¦å°åº¦è¯­è¨€ä¸Šçš„è¡¨ç°ï¼Œç»“æœè¡¨æ˜GPTç³»åˆ—æ¨¡å‹åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºå…¶ä»–æ¨¡å‹å’ŒIndicXlitï¼Œä¸”å¯¹ç‰¹å®šè¯­è¨€çš„å¾®è°ƒæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚é€šè¿‡é”™è¯¯åˆ†æå’Œåœ¨å™ªå£°æ¡ä»¶ä¸‹çš„é²æ£’æ€§æµ‹è¯•ï¼Œè¿›ä¸€æ­¥æ­ç¤ºäº†LLMsç›¸è¾ƒäºä¸“é—¨æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå¼ºè°ƒäº†åŸºç¡€æ¨¡å‹åœ¨å¤šç§ä¸“é—¨åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­éŸ³è¯‘çš„å‡†ç¡®æ€§é—®é¢˜ï¼Œç°æœ‰çš„ä¸“é—¨æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–è¯­è¨€æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å°åº¦çš„å¤æ‚è¯­è¨€èƒŒæ™¯ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éŸ³è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¢ç´¢å…¶åœ¨æ²¡æœ‰æ˜¾å¼ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ï¼ŒéªŒè¯å…¶ä½œä¸ºé€šç”¨æ¨¡å‹çš„æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä½¿ç”¨äº†æ ‡å‡†åŸºå‡†æ•°æ®é›†ï¼ˆå¦‚Dakshinaå’ŒAksharantarï¼‰ï¼Œå¯¹æ¯”äº†å¤šç§LLMsçš„è¡¨ç°ï¼Œè¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬Top-1å‡†ç¡®ç‡å’Œå­—ç¬¦é”™è¯¯ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ä¸“é—¨éŸ³è¯‘æ¨¡å‹è¿›è¡Œç³»ç»Ÿå¯¹æ¯”ï¼Œå‘ç°GPTç³»åˆ—æ¨¡å‹åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¡¨ç°ä¼˜è¶Šï¼Œä¸”å¾®è°ƒèƒ½å¤Ÿæ˜¾è‘—æå‡ç‰¹å®šè¯­è¨€çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­é‡‡ç”¨äº†æ ‡å‡†çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè®¾ç½®äº†ä¸åŒçš„æ¨¡å‹å‚æ•°ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„é”™è¯¯åˆ†æå’Œé²æ£’æ€§æµ‹è¯•ï¼Œä»¥éªŒè¯æ¨¡å‹åœ¨å™ªå£°æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPTç³»åˆ—æ¨¡å‹åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹çš„è¡¨ç°ä¼˜äºIndicXlitï¼Œå…·ä½“è€Œè¨€ï¼ŒGPT-4oåœ¨ç‰¹å®šè¯­è¨€ä¸Šçš„å¾®è°ƒåæ€§èƒ½æå‡æ˜¾è‘—ã€‚æ­¤å¤–ï¼Œé”™è¯¯åˆ†æè¡¨æ˜LLMsåœ¨å™ªå£°æ¡ä»¶ä¸‹çš„é²æ£’æ€§ä¼˜äºä¸“é—¨æ¨¡å‹ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶å¹¿æ³›åº”ç”¨çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«å’Œè·¨æ–‡åŒ–äº¤æµç­‰ã€‚é€šè¿‡æå‡éŸ³è¯‘çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¿ƒè¿›ä¸åŒè¯­è¨€ç”¨æˆ·ä¹‹é—´çš„æ²Ÿé€šï¼Œå¢å¼ºå¤šè¯­è¨€å¤„ç†ç³»ç»Ÿçš„å®ç”¨æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹æ•™è‚²ã€ä¿¡æ¯ä¼ æ’­ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transliteration, the process of mapping text from one script to another, plays a crucial role in multilingual natural language processing, especially within linguistically diverse contexts such as India. Despite significant advancements through specialized models like IndicXlit, recent developments in large language models suggest a potential for general-purpose models to excel at this task without explicit task-specific training. The current work systematically evaluates the performance of prominent LLMs, including GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a state-of-the-art transliteration model, across ten major Indian languages. Experiments utilized standard benchmarks, including Dakshina and Aksharantar datasets, with performance assessed via Top-1 Accuracy and Character Error Rate. Our findings reveal that while GPT family models generally outperform other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o improves performance on specific languages notably. An extensive error analysis and robustness testing under noisy conditions further elucidate strengths of LLMs compared to specialized models, highlighting the efficacy of foundational models for a wide spectrum of specialized applications with minimal overhead.

