---
layout: default
title: Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models
---

# Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14810" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14810v2</a>
  <a href="https://arxiv.org/pdf/2505.14810.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14810v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14810v2', 'Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, Yu Cheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-25)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TingchenFu/MathIF)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMathIFåŸºå‡†ä»¥è¯„ä¼°å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ä»¤éµå¾ª` `æ•°å­¦æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ¨¡å‹è¯„ä¼°` `å¼ºåŒ–å­¦ä¹ ` `è’¸é¦è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ•°å­¦é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. æå‡ºMathIFåŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œæ­ç¤ºæ¨ç†èƒ½åŠ›ä¸å¯æ§æ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç®€å•å¹²é¢„å¯ä»¥éƒ¨åˆ†æ¢å¤æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œä½†å¯èƒ½ä¼šå½±å“æ¨ç†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒ‡ä»¤éµå¾ªå¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç”¨æˆ·æ„å›¾å¯¹é½è‡³å…³é‡è¦ã€‚å°½ç®¡è¿‘æœŸçš„æ¨ç†å¯¼å‘æ¨¡å‹åœ¨å¤æ‚æ•°å­¦é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†MathIFï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ­ç¤ºäº†æ¨ç†èƒ½åŠ›æ‰©å±•ä¸å¯æ§æ€§ä¹‹é—´çš„ç´§å¼ å…³ç³»ï¼Œæ¨ç†æ›´æœ‰æ•ˆçš„æ¨¡å‹å¾€å¾€éš¾ä»¥éµå¾ªç”¨æˆ·æŒ‡ä»¤ã€‚æˆ‘ä»¬å‘ç°ï¼Œç»è¿‡è’¸é¦çš„é•¿æ€ç»´é“¾è°ƒä¼˜æˆ–ä½¿ç”¨æ¨ç†å¯¼å‘çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªä¸Šå¸¸å¸¸é€€åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆé•¿åº¦å¢åŠ æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å³ä½¿æ˜¯ç®€å•çš„å¹²é¢„ä¹Ÿèƒ½éƒ¨åˆ†æ¢å¤éµå¾ªèƒ½åŠ›ï¼Œå°½ç®¡è¿™ä¼šç‰ºç‰²æ¨ç†æ€§èƒ½ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰LLMè®­ç»ƒèŒƒå¼ä¸­çš„æ ¹æœ¬ç´§å¼ å…³ç³»ï¼Œå¹¶æ¿€åŠ±äº†å¯¹æ›´å…·æŒ‡ä»¤æ„è¯†çš„æ¨ç†æ¨¡å‹çš„éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­éµå¾ªç”¨æˆ·æŒ‡ä»¤çš„èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›æå‡çš„åŒæ—¶ï¼Œå¾€å¾€å¯¼è‡´æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMathIFåŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œæ­ç¤ºæ¨ç†èƒ½åŠ›ä¸å¯æ§æ€§ä¹‹é—´çš„çŸ›ç›¾ï¼Œæ¨åŠ¨æ›´å…·æŒ‡ä»¤æ„è¯†çš„æ¨¡å‹è®¾è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†åŒ…å«å¤šæ ·åŒ–çš„æ•°å­¦é—®é¢˜å’Œç›¸åº”çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæ¨¡å‹è®­ç»ƒåˆ™é€šè¿‡ä¸åŒçš„è°ƒä¼˜ç­–ç•¥è¿›è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè¯†åˆ«å¹¶é‡åŒ–æ¨ç†èƒ½åŠ›ä¸æŒ‡ä»¤éµå¾ªä¹‹é—´çš„çŸ›ç›¾ï¼Œæå‡ºäº†æ–°çš„è¯„ä¼°åŸºå‡†MathIFï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†è’¸é¦é•¿æ€ç»´é“¾å’Œæ¨ç†å¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ŒåŒæ—¶è®¾è®¡äº†é’ˆå¯¹æŒ‡ä»¤éµå¾ªçš„æŸå¤±å‡½æ•°ï¼Œä»¥å¹³è¡¡æ¨ç†æ€§èƒ½ä¸æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡ç®€å•å¹²é¢„çš„æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå°½ç®¡æ¨ç†æ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚å…·ä½“è€Œè¨€ï¼Œç»è¿‡å¹²é¢„çš„æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­çš„è¡¨ç°æé«˜äº†çº¦20%ï¼Œä½†æ¨ç†å‡†ç¡®ç‡ä¸‹é™äº†10%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æ¨ç†èƒ½åŠ›ä¸æŒ‡ä»¤éµå¾ªä¹‹é—´çš„æƒè¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼Œæå‡äººæœºäº¤äº’çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œç ”ç©¶æˆæœå¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„æ¨ç†æ¨¡å‹çš„å‘å±•ï¼Œä¿ƒè¿›äººå·¥æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.

