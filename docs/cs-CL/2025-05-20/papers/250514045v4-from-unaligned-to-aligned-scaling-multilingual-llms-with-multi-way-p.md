---
layout: default
title: "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora"
---

# From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14045" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14045v4</a>
  <a href="https://arxiv.org/pdf/2505.14045.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14045v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14045v4', 'From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingli Shen, Wen Lai, Shuo Wang, Ge Gao, Kangyang Luo, Alexander Fraser, Maosong Sun

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: EMNLP 2025 Main Conference (Oral)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè·¯å¹³è¡Œè¯­æ–™åº“ä»¥æå‡å¤šè¯­è¨€å¤§æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨¡å‹` `å¹³è¡Œè¯­æ–™åº“` `è·¨è¯­è¨€è¯­ä¹‰` `æŒç»­é¢„è®­ç»ƒ` `æŒ‡ä»¤è°ƒä¼˜` `TEDæ¼”è®²` `ä½èµ„æºè¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè¯­è¨€å¤§æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°ä¸è¶³ï¼Œä¸»è¦ç”±äºè®­ç»ƒæ•°æ®çš„æœªå¯¹é½ç‰¹æ€§é™åˆ¶äº†è·¨è¯­è¨€è¯­ä¹‰çš„æ•æ‰èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤šè·¯å¹³è¡Œè¯­æ–™åº“TED2025ï¼Œæ—¨åœ¨é€šè¿‡å¯¹é½å¤šç§è¯­è¨€çš„ç›¸åŒå†…å®¹æ¥æå‡å¤šè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºTED2025è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æœªå¯¹é½æ•°æ®è®­ç»ƒæ¨¡å‹ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§è§„æ¨¡å¤šè¯­è¨€æ•°æ®ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜å·²è¢«è¯æ˜å¯¹ä½èµ„æºè¯­è¨€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ•ˆã€‚ç„¶è€Œï¼Œæ•°æ®çš„æœªå¯¹é½ç‰¹æ€§é™åˆ¶äº†å…¶æœ‰æ•ˆæ•æ‰è·¨è¯­è¨€è¯­ä¹‰çš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šè·¯å¹³è¡Œæ•°æ®é€šè¿‡åœ¨å¤šç§è¯­è¨€ä¸­å¯¹é½ç›¸åŒå†…å®¹ï¼Œæä¾›äº†æ›´å¼ºçš„è·¨è¯­è¨€ä¸€è‡´æ€§ï¼Œæå‡äº†å¤šè¯­è¨€æ€§èƒ½çš„æ½œåŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†åŸºäºTEDæ¼”è®²çš„å¤§è§„æ¨¡é«˜è´¨é‡å¤šè·¯å¹³è¡Œè¯­æ–™åº“TED2025ï¼Œæ¶µç›–113ç§è¯­è¨€ï¼Œæœ€å¤šå¯å®ç°50ç§è¯­è¨€çš„å¹³è¡Œå¯¹é½ã€‚é€šè¿‡è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¢è®¨äº†åˆ©ç”¨å¤šè·¯å¹³è¡Œæ•°æ®å¢å¼ºLLMsçš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒä¼˜ç­–ç•¥åŠå…³é”®å½±å“å› ç´ çš„åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¤šè·¯å¹³è¡Œæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å…­ä¸ªå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºäºæœªå¯¹é½å¤šè¯­è¨€æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šè¯­è¨€å¤§æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„æ€§èƒ½ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç”±äºæœªå¯¹é½æ•°æ®å¯¼è‡´çš„è·¨è¯­è¨€è¯­ä¹‰æ•æ‰èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šè·¯å¹³è¡Œè¯­æ–™åº“TED2025ï¼Œè®ºæ–‡å¸Œæœ›åˆ©ç”¨å¯¹é½çš„å¤šè¯­è¨€æ•°æ®æ¥å¢å¼ºæ¨¡å‹çš„è·¨è¯­è¨€ä¸€è‡´æ€§ï¼Œä»è€Œæå‡å¤šè¯­è¨€æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®æ”¶é›†é˜¶æ®µèšç„¦äºæ„å»ºé«˜è´¨é‡çš„å¤šè·¯å¹³è¡Œè¯­æ–™åº“ï¼Œé¢„å¤„ç†é˜¶æ®µåˆ™ç¡®ä¿æ•°æ®çš„æ ‡å‡†åŒ–å’Œå¯¹é½ï¼Œæ¨¡å‹è®­ç»ƒé˜¶æ®µé‡‡ç”¨æŒç»­é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜ç­–ç•¥ï¼Œæœ€åé€šè¿‡å¤šè¯­è¨€åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªè¦†ç›–113ç§è¯­è¨€çš„é«˜è´¨é‡å¤šè·¯å¹³è¡Œè¯­æ–™åº“TED2025ï¼Œå¹¶æå‡ºäº†åˆ©ç”¨è¯¥è¯­æ–™åº“è¿›è¡Œæ¨¡å‹è®­ç»ƒçš„æœ€ä½³å®è·µï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è·¨è¯­è¨€ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¢å¼ºå¯¹é½ä¿¡æ¯çš„åˆ©ç”¨ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”å¤šè¯­è¨€æ•°æ®çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºTED2025è®­ç»ƒçš„æ¨¡å‹åœ¨å…­ä¸ªå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜äºæœªå¯¹é½æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°5%-15%ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†å¤šè·¯å¹³è¡Œæ•°æ®åœ¨æå‡å¤šè¯­è¨€æ¨¡å‹æ€§èƒ½æ–¹é¢çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€ç¿»è¯‘ã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¤šè¯­è¨€å¤§æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥æ›´å¥½åœ°æœåŠ¡äºå…¨çƒç”¨æˆ·ï¼Œä¿ƒè¿›ä¸åŒè¯­è¨€ä¹‹é—´çš„äº¤æµä¸ç†è§£ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå®é™…åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.

