---
layout: default
title: Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst
---

# Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14116" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14116v1</a>
  <a href="https://arxiv.org/pdf/2505.14116.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14116v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14116v1', 'Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongru Wang, Deng Cai, Wanjun Zhong, Shijue Huang, Jeff Z. Pan, Zeming Liu, Kam-Fai Wong

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæ¨ç†è¯­è¨€æ¨¡å‹ä»¥æå‡å¤æ‚æ¨ç†ä»»åŠ¡çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæ¨ç†æ¨¡å‹` `é•¿é“¾æ¨ç†` `è‡ªæˆ‘è®­ç»ƒ` `æ¨ç†å‚¬åŒ–å‰‚` `å¤æ‚æ¨ç†ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­éš¾ä»¥æœ‰æ•ˆç”Ÿæˆé•¿é“¾æ¨ç†ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. è®ºæ–‡æå‡ºçš„è‡ªæ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆSRLMï¼‰é€šè¿‡è‡ªæˆ‘è®­ç»ƒåˆæˆé•¿é“¾æ¨ç†æ•°æ®ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚
3. SRLMåœ¨äº”ä¸ªæ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†è¶…è¿‡2.5åˆ†çš„å¹³å‡æå‡ï¼Œä¸”å¤šæ¬¡é‡‡æ ·æ—¶è¡¨ç°æ›´ä½³ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†æ—¶é—´æ‰©å±•å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œé€šè¿‡å¢åŠ æ€ç»´é“¾çš„é•¿åº¦æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›æ›´é•¿çš„ä¸­é—´æ¨ç†ä¾æ®ä½“ç°äº†äººç±»è®¤çŸ¥ä¸­çš„å¤šç§å…ƒæ¨ç†æŠ€èƒ½ï¼Œå¦‚åæ€å’Œåˆ†è§£ï¼Œéš¾ä»¥åˆ›å»ºå’Œè·å–ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªæ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆSRLMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåˆæˆæ›´é•¿çš„æ€ç»´é“¾æ•°æ®ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘è®­ç»ƒè¿­ä»£æå‡æ€§èƒ½ã€‚é€šè¿‡ç»“åˆå°‘é‡ç¤ºä¾‹ï¼ˆå¦‚1,000ä¸ªæ ·æœ¬ï¼‰ä½œä¸ºæ¨ç†å‚¬åŒ–å‰‚ï¼ŒSRLMä¸ä»…å¢å¼ºäº†æ¨¡å‹çš„åˆå§‹æ€§èƒ½ï¼Œè¿˜ç¡®ä¿äº†åç»­è¿­ä»£ä¸­æ›´ç¨³å®šå’Œä¸€è‡´çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„SRLMåœ¨äº”ä¸ªæ¨ç†ä»»åŠ¡ï¼ˆMMLUã€GSM8Kã€ARC-Cã€HellaSwagå’ŒBBHï¼‰ä¸Šå®ç°äº†è¶…è¿‡2.5åˆ†çš„å¹³å‡ç»å¯¹æå‡ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡å¤šæ¬¡é‡‡æ ·è·å¾—æ›´å¤§æ”¹è¿›ï¼Œæ˜¾ç¤ºå‡ºSRLMåœ¨å¤šæ ·åŒ–å’Œåˆ›é€ æ€§æ¨ç†è·¯å¾„ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ç”Ÿæˆé•¿é“¾æ¨ç†çš„èƒ½åŠ›ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ›å»ºå’Œåˆ©ç”¨é•¿é“¾æ¨ç†ä¾æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè‡ªæ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆSRLMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡è‡ªæˆ‘è®­ç»ƒåˆæˆæ›´é•¿çš„æ€ç»´é“¾æ•°æ®ï¼Œå¹¶åˆ©ç”¨å°‘é‡ç¤ºä¾‹ä½œä¸ºæ¨ç†å‚¬åŒ–å‰‚ï¼Œé€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSRLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åˆæˆæ¨¡å—ã€æ¨ç†å‚¬åŒ–å‰‚æ¨¡å—å’Œè‡ªæˆ‘è®­ç»ƒæ¨¡å—ã€‚æ•°æ®åˆæˆæ¨¡å—è´Ÿè´£ç”Ÿæˆé•¿é“¾æ¨ç†æ•°æ®ï¼Œæ¨ç†å‚¬åŒ–å‰‚æ¨¡å—æä¾›ç¤ºä¾‹ä»¥å¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œè‡ªæˆ‘è®­ç»ƒæ¨¡å—åˆ™é€šè¿‡è¿­ä»£ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSRLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è‡ªæˆ‘è®­ç»ƒæœºåˆ¶å’Œæ¨ç†å‚¬åŒ–å‰‚çš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€æ­¥æå‡æ¨ç†èƒ½åŠ›ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSRLMä½¿ç”¨äº†ç‰¹å®šçš„å­¦ä¹ ç‡å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è‡ªæˆ‘è®­ç»ƒè¿‡ç¨‹ä¸­ç¨³å®šæ”¶æ•›ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†Transformeræ¶æ„ï¼Œé€‚åº”é•¿é“¾æ¨ç†çš„éœ€æ±‚ã€‚é€šè¿‡å¤šæ¬¡é‡‡æ ·ï¼Œæ¨¡å‹èƒ½å¤Ÿæ¢ç´¢æ›´ä¸°å¯Œçš„æ¨ç†è·¯å¾„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SRLMåœ¨äº”ä¸ªæ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†è¶…è¿‡2.5åˆ†çš„å¹³å‡ç»å¯¹æå‡ï¼Œä¸”åœ¨64æ¬¡é‡‡æ ·æ—¶ï¼Œå¹³å‡æå‡è¾¾åˆ°7.89åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†å¼ºåŸºçº¿æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒSRLMå¯ä»¥åœ¨å¤æ‚é—®é¢˜è§£å†³ã€çŸ¥è¯†æ¨ç†å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inference-time scaling has attracted much attention which significantly enhance the performance of Large Language Models (LLMs) in complex reasoning tasks by increasing the length of Chain-of-Thought. These longer intermediate reasoning rationales embody various meta-reasoning skills in human cognition, such as reflection and decomposition, being difficult to create and acquire. In this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where the model itself can synthesize longer CoT data and iteratively improve performance through self-training. By incorporating a few demonstration examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from existing responses, which act as a reasoning catalyst, we demonstrate that SRLM not only enhances the model's initial performance but also ensures more stable and consistent improvements in subsequent iterations. Our proposed SRLM achieves an average absolute improvement of more than $+2.5$ points across five reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models. Moreover, it brings more improvements with more times of sampling during inference, such as absolute $+7.89$ average improvement with $64$ sampling times, revealing the in-depth, diverse and creative reasoning paths in SRLM against the strong baseline.

