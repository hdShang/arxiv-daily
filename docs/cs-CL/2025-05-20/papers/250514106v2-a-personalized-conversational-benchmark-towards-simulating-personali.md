---
layout: default
title: "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations"
---

# A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14106" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14106v2</a>
  <a href="https://arxiv.org/pdf/2505.14106.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14106v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14106v2', 'A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Li Li, Peilin Cai, Ryan A. Rossi, Franck Dernoncourt, Branislav Kveton, Junda Wu, Tong Yu, Linxin Song, Tiankai Yang, Yuehan Qin, Nesreen K. Ahmed, Samyadeep Basu, Subhojyoti Mukherjee, Ruiyi Zhang, Zhengmian Hu, Bo Ni, Yuxiao Zhou, Zichao Wang, Yue Huang, Yu Wang, Xiangliang Zhang, Philip S. Yu, Xiyang Hu, Yue Zhao

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPersonaConvBenchä»¥è¯„ä¼°ä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–å¯¹è¯` `å¤§å‹è¯­è¨€æ¨¡å‹` `æƒ…æ„Ÿåˆ†ç±»` `å¤šè½®å¯¹è¯` `åŸºå‡†æµ‹è¯•` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç”¨æˆ·ä¸­å¿ƒç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¾€å¾€å­¤ç«‹åœ°å…³æ³¨ä¸ªæ€§åŒ–æˆ–å¯¹è¯ç»“æ„ï¼Œç¼ºä¹ç»¼åˆè¯„ä¼°çš„åŸºå‡†ã€‚
2. è®ºæ–‡æå‡ºPersonaConvBenchï¼Œé€šè¿‡æ•´åˆä¸ªæ€§åŒ–å’Œå¯¹è¯ç»“æ„ï¼Œè®¾è®¡äº†ä¸‰é¡¹æ ¸å¿ƒä»»åŠ¡ä»¥è¯„ä¼°LLMsçš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼•å…¥ä¸ªæ€§åŒ–å†å²æ˜¾è‘—æå‡äº†LLMsçš„æ€§èƒ½ï¼Œæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­ç›¸è¾ƒäºéå¯¹è¯åŸºçº¿æå‡198%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†PersonaConvBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­çš„ä¸ªæ€§åŒ–æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚ä¸ç°æœ‰ç ”ç©¶å•ç‹¬å…³æ³¨ä¸ªæ€§åŒ–æˆ–å¯¹è¯ç»“æ„ä¸åŒï¼ŒPersonaConvBenchå°†ä¸¤è€…ç»“åˆï¼Œæä¾›äº†å¥å­åˆ†ç±»ã€å½±å“å›å½’å’Œç”¨æˆ·ä¸­å¿ƒæ–‡æœ¬ç”Ÿæˆä¸‰é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œè¦†ç›–åä¸ªå¤šæ ·çš„åŸºäºRedditçš„é¢†åŸŸã€‚è¯¥è®¾è®¡ä½¿å¾—ç³»ç»Ÿåˆ†æä¸ªæ€§åŒ–å¯¹è¯ä¸Šä¸‹æ–‡å¦‚ä½•å½±å“LLMè¾“å‡ºæˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬åœ¨ç»Ÿä¸€æç¤ºè®¾ç½®ä¸‹å¯¹å¤šç§å•†ä¸šå’Œå¼€æºLLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè§‚å¯Ÿåˆ°å¼•å…¥ä¸ªæ€§åŒ–å†å²æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨æƒ…æ„Ÿåˆ†ç±»ä¸­ç›¸è¾ƒäºæœ€ä½³éå¯¹è¯åŸºçº¿æé«˜äº†198%çš„ç›¸å¯¹å¢ç›Šã€‚é€šè¿‡å‘å¸ƒPersonaConvBenchåŠå…¶è¯„ä¼°å’Œä»£ç ï¼Œæˆ‘ä»¬æ—¨åœ¨æ”¯æŒç ”ç©¶èƒ½å¤Ÿé€‚åº”ä¸ªä½“é£æ ¼ã€è·Ÿè¸ªé•¿æœŸä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆä¸°å¯Œã€å¼•äººå…¥èƒœçš„å“åº”çš„LLMsã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯¹è¯ç”Ÿæˆæ¨¡å‹åœ¨ä¸ªæ€§åŒ–å’Œå¯¹è¯ç»“æ„è¯„ä¼°æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å…¨é¢åæ˜ ä¸ªæ€§åŒ–å¯¹è¯çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºPersonaConvBenchï¼Œè®ºæ–‡å°†ä¸ªæ€§åŒ–ä¸å¯¹è¯ç»“æ„ç»“åˆï¼Œè®¾è®¡äº†å¤šé¡¹ä»»åŠ¡ä»¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsåœ¨ä¸ªæ€§åŒ–å¯¹è¯ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPersonaConvBenchåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå¥å­åˆ†ç±»ã€å½±å“å›å½’å’Œç”¨æˆ·ä¸­å¿ƒæ–‡æœ¬ç”Ÿæˆï¼Œè¦†ç›–åä¸ªä¸åŒçš„Reddité¢†åŸŸï¼Œå…è®¸å¯¹å¤šè½®å¯¹è¯è¿›è¡Œå…¨é¢åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†ä¸ªæ€§åŒ–å†å²ä¸å¯¹è¯ç”Ÿæˆç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†ç±»ç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç»Ÿä¸€çš„æç¤ºè®¾ç½®ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ä¸ªæ€§åŒ–ç”Ÿæˆï¼Œå¹¶é€šè¿‡å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å¢å¼ºæ¨¡å‹çš„å“åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥ä¸ªæ€§åŒ–å†å²åï¼Œå¤šä¸ªLLMsåœ¨æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯ç›¸è¾ƒäºæœ€ä½³éå¯¹è¯åŸºçº¿ï¼Œæ€§èƒ½æå‡è¾¾198%ã€‚è¿™ä¸€ç»“æœå±•ç¤ºäº†ä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆçš„é‡è¦æ€§åŠå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€ç¤¾äº¤æœºå™¨äººå’Œä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¯¹è¯ç³»ç»Ÿçš„ä¸ªæ€§åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼Œæä¾›æ›´ä¸ºè‡ªç„¶å’Œäººæ€§åŒ–çš„äº¤äº’ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨å¤šç§äººæœºäº¤äº’åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present PersonaConvBench, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Unlike existing work that focuses on either personalization or conversational structure in isolation, PersonaConvBench integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation across ten diverse Reddit-based domains. This design enables systematic analysis of how personalized conversational context shapes LLM outputs in realistic multi-user scenarios. We benchmark several commercial and open-source LLMs under a unified prompting setup and observe that incorporating personalized history yields substantial performance improvements, including a 198 percent relative gain over the best non-conversational baseline in sentiment classification. By releasing PersonaConvBench with evaluations and code, we aim to support research on LLMs that adapt to individual styles, track long-term context, and produce contextually rich, engaging responses.

