---
layout: default
title: "SlangDIT: Benchmarking LLMs in Interpretative Slang Translation"
---

# SlangDIT: Benchmarking LLMs in Interpretative Slang Translation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14181" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14181v1</a>
  <a href="https://arxiv.org/pdf/2505.14181.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14181v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14181v1', 'SlangDIT: Benchmarking LLMs in Interpretative Slang Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunlong Liang, Fandong Meng, Jiaan Wang, Jie Zhou

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSlangDITä»¥è§£å†³ä¿šè¯­ç¿»è¯‘ä¸­çš„è¯­å¢ƒä¾èµ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¿šè¯­ç¿»è¯‘` `å¤§è¯­è¨€æ¨¡å‹` `è¯­å¢ƒç†è§£` `è·¨è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä¿šè¯­ç¿»è¯‘ä¸­æœªèƒ½æœ‰æ•ˆæ•æ‰è¯­å¢ƒä¾èµ–æ€§ï¼Œå¯¼è‡´ç¿»è¯‘å‡†ç¡®æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºSlangDITä»»åŠ¡ï¼Œç»“åˆä¿šè¯­æ£€æµ‹ã€è§£é‡Šå’Œç¿»è¯‘ï¼Œå½¢æˆä¸€ä¸ªç›¸äº’ä¾èµ–çš„å¤„ç†æ¡†æ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSlangOWLæ¨¡å‹åœ¨Qwen2.5å’ŒLLama-3.1ä¸Šæ˜¾è‘—è¶…è¶Šä¼ ç»Ÿæ¨¡å‹ï¼Œæå‡ç¿»è¯‘æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¿šè¯­ç¿»è¯‘çš„æŒ‘æˆ˜åœ¨äºæ•æ‰ä¾èµ–äºè¯­å¢ƒçš„è¯­ä¹‰æ‰©å±•ï¼Œå› ä¸ºä¿šè¯­å¸¸å¸¸ä¼ è¾¾è¶…å‡ºå­—é¢è§£é‡Šçš„å«ä¹‰ã€‚å°½ç®¡åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ä»£ï¼Œä¿šè¯­æ£€æµ‹ã€è§£é‡Šå’Œç¿»è¯‘ä½œä¸ºå­¤ç«‹ä»»åŠ¡è¿›è¡Œäº†ç ”ç©¶ï¼Œä½†å®ƒä»¬å†…åœ¨çš„ç›¸äº’ä¾èµ–æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡å¼•å…¥äº†ä¿šè¯­ç¿»è¯‘ä»»åŠ¡ï¼ˆSlangDITï¼‰ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªå­ä»»åŠ¡ï¼šä¿šè¯­æ£€æµ‹ã€è·¨è¯­è¨€ä¿šè¯­è§£é‡Šå’Œå½“å‰è¯­å¢ƒä¸‹çš„ä¿šè¯­ç¿»è¯‘ï¼Œæ—¨åœ¨é€šè¿‡ä¿šè¯­æ£€æµ‹å’Œè§£é‡Šç”Ÿæˆæ›´å‡†ç¡®çš„ç¿»è¯‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡25,000å¯¹è‹±æ±‰å¥å­çš„SlangDITæ•°æ®é›†ã€‚æ¯ä¸ªæºå¥å­è‡³å°‘æåˆ°ä¸€ä¸ªä¿šè¯­ï¼Œå¹¶æ ‡æ³¨äº†ç›¸åº”çš„è·¨è¯­è¨€ä¿šè¯­è§£é‡Šã€‚åŸºäºè¯¥åŸºå‡†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦æ€è€ƒæ¨¡å‹SlangOWLï¼Œèƒ½å¤Ÿè¯†åˆ«å¥å­ä¸­çš„ä¿šè¯­ï¼Œå¹¶åˆ†æå…¶å¯èƒ½çš„å«ä¹‰ï¼Œæœ€ç»ˆæä¾›é€‚åˆçš„ç¿»è¯‘ã€‚å®éªŒè¡¨æ˜ï¼ŒSlangOWLæ˜¾è‘—æå‡äº†LLMsçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¿šè¯­ç¿»è¯‘ä¸­çš„è¯­å¢ƒä¾èµ–æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å°†ä¿šè¯­æ£€æµ‹ã€è§£é‡Šå’Œç¿»è¯‘è§†ä¸ºç‹¬ç«‹ä»»åŠ¡ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨å®ƒä»¬ä¹‹é—´çš„ç›¸äº’å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„SlangDITä»»åŠ¡é€šè¿‡å°†ä¿šè¯­æ£€æµ‹ã€è·¨è¯­è¨€è§£é‡Šå’Œç¿»è¯‘ç»“åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªå¤šå±‚æ¬¡çš„å¤„ç†æµç¨‹ï¼Œä»¥æé«˜ç¿»è¯‘çš„å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯ä¿šè¯­æ£€æµ‹æ¨¡å—ï¼Œè¯†åˆ«å¥å­ä¸­çš„ä¿šè¯­ï¼›å…¶æ¬¡æ˜¯è§£é‡Šæ¨¡å—ï¼Œåˆ†æä¿šè¯­çš„å¤šä¹‰æ€§åŠå…¶åœ¨ç‰¹å®šè¯­å¢ƒä¸‹çš„å«ä¹‰ï¼›æœ€åæ˜¯ç¿»è¯‘æ¨¡å—ï¼ŒåŸºäºå‰ä¸¤ä¸ªæ¨¡å—çš„è¾“å‡ºç”Ÿæˆé€‚åˆçš„ç¿»è¯‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šSlangOWLæ¨¡å‹çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶æ·±åº¦æ€è€ƒæœºåˆ¶ï¼Œé€šè¿‡é€æ­¥åˆ†æä¿šè¯­çš„è¯­å¢ƒå’Œå«ä¹‰ï¼Œæ˜¾è‘—æå‡äº†ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿæ¨¡å‹çš„ç›´æ¥ç¿»è¯‘æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­ï¼Œä¿šè¯­æ£€æµ‹ä½¿ç”¨äº†åŸºäºä¸Šä¸‹æ–‡çš„ç‰¹å¾æå–ï¼Œè§£é‡Šæ¨¡å—é‡‡ç”¨äº†å¤šä¹‰æ€§åˆ†æç®—æ³•ï¼Œç¿»è¯‘æ¨¡å—åˆ™ç»“åˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿ç¿»è¯‘ç»“æœçš„å‡†ç¡®æ€§å’Œè‡ªç„¶æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSlangOWLæ¨¡å‹åœ¨ä¿šè¯­ç¿»è¯‘ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶åœ¨Qwen2.5å’ŒLLama-3.1æ¨¡å‹ä¸Šï¼Œç¿»è¯‘å‡†ç¡®ç‡æé«˜äº†15%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹å’Œç›‘ç£å¾®è°ƒæ¨¡å‹ï¼Œè¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹ç¿»è¯‘ã€åœ¨çº¿èŠå¤©ç¿»è¯‘å’Œæ–‡åŒ–äº¤æµå¹³å°ç­‰ã€‚é€šè¿‡æå‡ä¿šè¯­ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ƒè¿›ä¸åŒè¯­è¨€å’Œæ–‡åŒ–ä¹‹é—´çš„ç†è§£ä¸äº¤æµï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The challenge of slang translation lies in capturing context-dependent semantic extensions, as slang terms often convey meanings beyond their literal interpretation. While slang detection, explanation, and translation have been studied as isolated tasks in the era of large language models (LLMs), their intrinsic interdependence remains underexplored. The main reason is lacking of a benchmark where the two tasks can be a prerequisite for the third one, which can facilitate idiomatic translation. In this paper, we introduce the interpretative slang translation task (named SlangDIT) consisting of three sub-tasks: slang detection, cross-lingual slang explanation, and slang translation within the current context, aiming to generate more accurate translation with the help of slang detection and slang explanation. To this end, we construct a SlangDIT dataset, containing over 25k English-Chinese sentence pairs. Each source sentence mentions at least one slang term and is labeled with corresponding cross-lingual slang explanation. Based on the benchmark, we propose a deep thinking model, named SlangOWL. It firstly identifies whether the sentence contains a slang, and then judges whether the slang is polysemous and analyze its possible meaning. Further, the SlangOWL provides the best explanation of the slang term targeting on the current context. Finally, according to the whole thought, the SlangOWL offers a suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and LLama-3.1), show that our deep thinking approach indeed enhances the performance of LLMs where the proposed SLangOWL significantly surpasses the vanilla models and supervised fine-tuned models without thinking.

