---
layout: default
title: "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models"
---

# DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13975" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13975v3</a>
  <a href="https://arxiv.org/pdf/2505.13975.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13975v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13975v3', 'DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxuan Jiang, Dawei Li, Frank Ferraro

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-08-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRPä»¥è§£å†³å¤§å‹æ¨ç†æ¨¡å‹çš„æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹æ¨ç†æ¨¡å‹` `æ¨ç†æ•ˆç‡` `è’¸é¦è®­ç»ƒ` `æŠ€èƒ½æ„ŸçŸ¥` `å†…å®¹å‰ªæ` `æ•°å­¦æ¨ç†` `çŸ¥è¯†è½¬ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹æ¨ç†æ¨¡å‹åœ¨æ¨ç†æ—¶äº§ç”Ÿå†—é•¿çš„æ¨ç†è½¨è¿¹ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œèµ„æºæµªè´¹ã€‚
2. æœ¬æ–‡æå‡ºçš„DRPæ¡†æ¶é€šè¿‡æ•™å¸ˆæ¨¡å‹è¿›è¡ŒæŠ€èƒ½æ„ŸçŸ¥çš„æ­¥éª¤åˆ†è§£å’Œå†…å®¹å‰ªæï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDRPåœ¨GSM8Kæ•°æ®é›†ä¸Šå°†å¹³å‡ä»¤ç‰Œä½¿ç”¨é‡ä»917å‡å°‘åˆ°328ï¼ŒåŒæ—¶å‡†ç¡®ç‡ä»91.7%æå‡è‡³94.1%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹å¾€å¾€æ¶‰åŠå†—é•¿çš„æ¨ç†è½¨è¿¹ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è’¸é¦æ¨ç†å‰ªæï¼ˆDRPï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†æ¨ç†æ—¶å‰ªæå’ŒåŸºäºè°ƒä¼˜çš„è’¸é¦çš„æ··åˆæ¡†æ¶ã€‚DRPåˆ©ç”¨æ•™å¸ˆæ¨¡å‹è¿›è¡ŒæŠ€èƒ½æ„ŸçŸ¥çš„æ­¥éª¤åˆ†è§£å’Œå†…å®¹å‰ªæï¼Œç„¶åå°†å‰ªæåçš„æ¨ç†è·¯å¾„è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆä¸”å‡†ç¡®åœ°è¿›è¡Œæ¨ç†ã€‚åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨DRPè®­ç»ƒçš„æ¨¡å‹åœ¨ä»¤ç‰Œæ•ˆç‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹æ¨ç†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå†—é•¿æ¨ç†è½¨è¿¹çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDRPæ¡†æ¶ç»“åˆäº†æ¨ç†æ—¶å‰ªæä¸åŸºäºè°ƒä¼˜çš„è’¸é¦ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹çš„æŠ€èƒ½æ„ŸçŸ¥æ­¥éª¤åˆ†è§£æ¥ä¼˜åŒ–æ¨ç†è·¯å¾„ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDRPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚æ•™å¸ˆæ¨¡å‹è´Ÿè´£è¿›è¡Œæ­¥éª¤åˆ†è§£å’Œå†…å®¹å‰ªæï¼Œè€Œå­¦ç”Ÿæ¨¡å‹åˆ™é€šè¿‡è’¸é¦å­¦ä¹ ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDRPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæŠ€èƒ½æ„ŸçŸ¥çš„æ­¥éª¤åˆ†è§£ä¸å†…å®¹å‰ªæçš„ç»“åˆï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨ç†è¿‡ç¨‹æ›´åŠ é«˜æ•ˆï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‡†ç¡®æ€§ä¸æ•ˆç‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¸æ”¶æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨DRPè®­ç»ƒçš„æ¨¡å‹åœ¨GSM8Kæ•°æ®é›†ä¸Šå®ç°äº†917åˆ°328çš„ä»¤ç‰Œä½¿ç”¨é‡å‡å°‘ï¼ŒåŒæ—¶å‡†ç¡®ç‡ä»91.7%æå‡è‡³94.1%ã€‚åœ¨AIMEæ•°æ®é›†ä¸Šï¼ŒDRPå®ç°äº†43%çš„ä»¤ç‰Œå‡å°‘ï¼Œä¸”æ²¡æœ‰æ€§èƒ½ä¸‹é™ï¼Œå±•ç°äº†å…¶ä¼˜è¶Šçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€é‡‘èå’Œç§‘å­¦ç ”ç©¶ç­‰éœ€è¦å¤æ‚æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ•ˆç‡ï¼ŒDRPèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ”¯æŒæ›´å¹¿æ³›çš„åº”ç”¨ï¼Œæå‡å†³ç­–è´¨é‡å’Œé€Ÿåº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Large Reasoning Models (LRMs) have demonstrated success in complex reasoning tasks through long chain-of-thought (CoT) reasoning, their inference often involves excessively verbose reasoning traces, resulting in substantial inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a hybrid framework that combines inference-time pruning with tuning-based distillation, two widely used strategies for efficient reasoning. DRP uses a teacher model to perform skill-aware step decomposition and content pruning, and then distills the pruned reasoning paths into a student model, enabling it to reason both efficiently and accurately. Across several challenging mathematical reasoning datasets, we find that models trained with DRP achieve substantial improvements in token efficiency without sacrificing accuracy. Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on AIME with no performance drop. Further analysis shows that aligning the reasoning structure of training CoTs with the student's reasoning capacity is critical for effective knowledge transfer and performance gains.

