---
layout: default
title: "Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies"
---

# Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14972" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14972v2</a>
  <a href="https://arxiv.org/pdf/2505.14972.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14972v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14972v2', 'Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoyi Qiu, Kung-Hsiang Huang, Ruichen Zheng, Jiao Sun, Nanyun Peng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-12-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCROSSåŸºå‡†ä»¥è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡åŒ–å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡åŒ–å®‰å…¨` `å¤šæ¨¡æ€è¯„ä¼°` `è§†è§‰è¯­è¨€æ¨¡å‹` `è·¨æ–‡åŒ–äº¤æµ` `ç›‘ç£å¾®è°ƒ` `åå¥½è°ƒä¼˜` `äººå·¥æ™ºèƒ½ä¼¦ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å®‰å…¨åŸºå‡†ä¸»è¦å…³æ³¨ç‰©ç†å®‰å…¨ï¼Œå¿½è§†æ–‡åŒ–è§„èŒƒçš„è¿è§„è¡Œä¸ºï¼Œå¯¼è‡´æ–‡åŒ–é€‚å®œæ€§å“åº”èƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºCROSSåŸºå‡†å’ŒCROSS-Evalæ¡†æ¶ï¼Œè¯„ä¼°LVLMsåœ¨æ–‡åŒ–å®‰å…¨æ¨ç†ä¸­çš„è¡¨ç°ï¼Œæ¶µç›–æ–‡åŒ–æ„è¯†ç­‰å››ä¸ªç»´åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹åœ¨æ–‡åŒ–æ„è¯†å’Œåˆè§„æ€§æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œåå¥½è°ƒä¼˜ç­–ç•¥æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å…¨çƒåº”ç”¨ä¸­æ—¥ç›Šæ™®åŠï¼Œä½†å…¶ç”Ÿæˆæ–‡åŒ–é€‚å®œæ€§å“åº”çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å®‰å…¨åŸºå‡†ä¸»è¦å…³æ³¨ç‰©ç†å®‰å…¨ï¼Œå¿½è§†äº†æ–‡åŒ–è§„èŒƒçš„è¿è§„è¡Œä¸ºï¼Œè¿™å¯èƒ½å¯¼è‡´è±¡å¾æ€§ä¼¤å®³ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†CROSSåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LVLMsçš„æ–‡åŒ–å®‰å…¨æ¨ç†èƒ½åŠ›ã€‚CROSSåŒ…å«æ¥è‡ª16ä¸ªå›½å®¶ã€ä¸‰ä¸ªæ—¥å¸¸é¢†åŸŸå’Œ14ç§è¯­è¨€çš„1284ä¸ªå¤šè¯­è¨€è§†è§‰åŸºç¡€æŸ¥è¯¢ï¼Œæ–‡åŒ–è§„èŒƒçš„è¿è§„è¡Œä¸ºä»…åœ¨å›¾åƒçš„ä¸Šä¸‹æ–‡ä¸­è¢«è§£é‡Šæ—¶æ‰ä¼šå‡ºç°ã€‚æˆ‘ä»¬æå‡ºäº†CROSS-Evalæ¡†æ¶ï¼Œè¡¡é‡æ–‡åŒ–æ„è¯†ã€è§„èŒƒæ•™è‚²ã€åˆè§„æ€§å’Œå¸®åŠ©æ€§å››ä¸ªå…³é”®ç»´åº¦ã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†21ä¸ªé¢†å…ˆçš„LVLMsï¼Œç»“æœæ˜¾ç¤ºæ–‡åŒ–å®‰å…¨å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºæé«˜æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ç§å¢å¼ºç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†GPT-4oçš„æ–‡åŒ–æ„è¯†å’Œåˆè§„æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å¤šæ¨¡æ€èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–é€‚å®œæ€§å“åº”ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯„ä¼°æ–‡åŒ–å®‰å…¨æ€§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å…¨çƒåº”ç”¨ä¸­çš„æ½œåœ¨é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºCROSSåŸºå‡†å’ŒCROSS-Evalæ¡†æ¶ï¼Œé€šè¿‡å¤šç»´åº¦è¯„ä¼°LVLMsçš„æ–‡åŒ–å®‰å…¨æ¨ç†èƒ½åŠ›ï¼Œå¼ºè°ƒæ–‡åŒ–æ„è¯†å’Œåˆè§„æ€§çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬CROSSåŸºå‡†çš„æ„å»ºã€CROSS-Evalæ¡†æ¶çš„è®¾è®¡ï¼Œä»¥åŠå¯¹21ä¸ªLVLMsçš„è¯„ä¼°ï¼Œæ¶µç›–æ–‡åŒ–æ„è¯†ã€è§„èŒƒæ•™è‚²ã€åˆè§„æ€§å’Œå¸®åŠ©æ€§å››ä¸ªç»´åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šCROSSåŸºå‡†çš„æå‡ºæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå¡«è¡¥äº†ç°æœ‰å¤šæ¨¡æ€å®‰å…¨è¯„ä¼°ä¸­å¯¹æ–‡åŒ–å®‰å…¨çš„å¿½è§†ï¼Œæä¾›äº†ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è¯„ä¼°ä¸­ï¼Œé‡‡ç”¨äº†å¤šè¯­è¨€è§†è§‰åŸºç¡€æŸ¥è¯¢ï¼Œè®¾è®¡äº†ç›‘ç£å¾®è°ƒå’Œåå¥½è°ƒä¼˜ç­–ç•¥ï¼Œä»¥æå‡æ¨¡å‹åœ¨æ–‡åŒ–å®‰å…¨æ–¹é¢çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹åœ¨æ–‡åŒ–æ„è¯†å’Œåˆè§„æ€§æ–¹é¢çš„å¾—åˆ†åˆ†åˆ«ä¸º61.79%å’Œ37.73%ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œåå¥½è°ƒä¼˜ï¼ŒGPT-4oçš„æ–‡åŒ–æ„è¯†æå‡äº†60.14%ï¼Œåˆè§„æ€§æå‡äº†55.2%ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬å¤šæ¨¡æ€ç†è§£åŸºå‡†ä¸Šçš„æ€§èƒ½ä¿æŒç›¸å¯¹ç¨³å®šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ—…æ¸¸åŠ©æ‰‹ã€è·¨æ–‡åŒ–äº¤æµå¹³å°ä»¥åŠä»»ä½•éœ€è¦ç”Ÿæˆæ–‡åŒ–é€‚å®œæ€§å†…å®¹çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ–‡åŒ–å®‰å…¨æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ–‡åŒ–è¯¯è§£å’Œå†²çªï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒï¼Œä¿ƒè¿›å…¨çƒåŒ–èƒŒæ™¯ä¸‹çš„æ–‡åŒ–äº¤æµä¸ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models (LVLMs) are increasingly deployed in globally distributed applications, such as tourism assistants, yet their ability to produce culturally appropriate responses remains underexplored. Existing multimodal safety benchmarks primarily focus on physical safety and overlook violations rooted in cultural norms, which can result in symbolic harm. To address this gap, we introduce CROSS, a benchmark designed to assess the cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284 multilingual visually grounded queries from 16 countries, three everyday domains, and 14 languages, where cultural norm violations emerge only when images are interpreted in context. We propose CROSS-Eval, an intercultural theory-based framework that measures four key dimensions: cultural awareness, norm education, compliance, and helpfulness. Using this framework, we evaluate 21 leading LVLMs, including mixture-of-experts models and reasoning models. Results reveal significant cultural safety gaps: the best-performing model achieves only 61.79% in awareness and 37.73% in compliance. While some open-source models reach GPT-4o-level performance, they still fall notably short of proprietary models. Our results further show that increasing reasoning capacity improves cultural alignment but does not fully resolve the issue. To improve model performance, we develop two enhancement strategies: supervised fine-tuning with culturally grounded, open-ended data and preference tuning with contrastive response pairs that highlight safe versus unsafe behaviors. These methods substantially improve GPT-4o's cultural awareness (+60.14%) and compliance (+55.2%), while preserving general multimodal capabilities with minimal performance reduction on general multimodal understanding benchmarks.

