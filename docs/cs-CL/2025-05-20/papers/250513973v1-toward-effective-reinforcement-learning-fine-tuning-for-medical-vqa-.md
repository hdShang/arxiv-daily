---
layout: default
title: Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models
---

# Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13973" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13973v1</a>
  <a href="https://arxiv.org/pdf/2505.13973.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13973v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13973v1', 'Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhui Zhu, Xuanzhao Dong, Xin Li, Peijie Qiu, Xiwen Chen, Abolfazl Razi, Aris Sotiras, Yi Su, Yalin Wang

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ä»¥æå‡åŒ»å­¦è§†è§‰é—®ç­”æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•åœ¨åŒ»å­¦è§†è§‰é—®ç­”ä»»åŠ¡ä¸­éš¾ä»¥å®ç°ä¸´åºŠæœŸæœ›çš„æ¨¡å‹è¡Œä¸ºï¼Œå­˜åœ¨å¤šæ–¹é¢çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹åˆå§‹åŒ–ã€è¯­ä¹‰å¯¹é½ã€å¥–åŠ±æœºåˆ¶åŠåå·®ç­‰å› ç´ ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRPOæ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼Œæä¾›äº†æ–°çš„å¾®è°ƒç­–ç•¥ã€‚
4. method_zh

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•æ”¹å˜äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•è½¨è¿¹ï¼Œå°¤å…¶æ˜¯åœ¨å¼•å…¥ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åã€‚ç„¶è€Œï¼Œç›´æ¥å°†å…¶åº”ç”¨äºåŒ»å­¦ä»»åŠ¡ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å®ç°ä¸´åºŠæœŸæœ›çš„æ¨¡å‹è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ¢è®¨äº†å½±å“åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­å¼ºåŒ–å­¦ä¹ å¾®è°ƒæœ‰æ•ˆæ€§çš„å››ä¸ªå…³é”®ç»´åº¦ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥ã€åŒ»å­¦è¯­ä¹‰å¯¹é½çš„ä½œç”¨ã€åŸºäºé•¿åº¦çš„å¥–åŠ±å¯¹é•¿é“¾æ¨ç†çš„å½±å“ä»¥åŠåå·®çš„å½±å“ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›å› ç´ å¯¹åŒ»å­¦MLLMsçš„å½±å“ï¼Œæä¾›äº†æ¨¡å‹é¢†åŸŸç‰¹å®šå¾®è°ƒçš„æ–°è§è§£ã€‚æ­¤å¤–ï¼Œç»“æœè¡¨æ˜ï¼ŒåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒåœ¨å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ä¸Šå‡ä¼˜äºæ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, reinforcement learning (RL)-based tuning has shifted the trajectory of Multimodal Large Language Models (MLLMs), particularly following the introduction of Group Relative Policy Optimization (GRPO). However, directly applying it to medical tasks remains challenging for achieving clinically grounded model behavior. Motivated by the need to align model response with clinical expectations, we investigate four critical dimensions that affect the effectiveness of RL-based tuning in medical visual question answering (VQA): base model initialization strategy, the role of medical semantic alignment, the impact of length-based rewards on long-chain reasoning, and the influence of bias. We conduct extensive experiments to analyze these factors for medical MLLMs, providing new insights into how models are domain-specifically fine-tuned. Additionally, our results also demonstrate that GRPO-based RL tuning consistently outperforms standard supervised fine-tuning (SFT) in both accuracy and reasoning quality.

