---
layout: default
title: Tracing Multilingual Factual Knowledge Acquisition in Pretraining
---

# Tracing Multilingual Factual Knowledge Acquisition in Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14824" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14824v2</a>
  <a href="https://arxiv.org/pdf/2505.14824.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14824v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14824v2', 'Tracing Multilingual Factual Knowledge Acquisition in Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Felicia KÃ¶rner, Ercong Nie, Barbara Plank, FranÃ§ois Yvon, Hinrich SchÃ¼tze

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-07)

**å¤‡æ³¨**: EMNLP Findings 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/cisnlp/multilingual-fact-tracing)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¿½è¸ªå¤šè¯­è¨€äº‹å®çŸ¥è¯†è·å–ä»¥æå‡è¯­è¨€æ¨¡å‹çš„è·¨è¯­è¨€ä¸€è‡´æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€æ¨¡å‹` `äº‹å®å›å¿†` `è·¨è¯­è¨€ä¸€è‡´æ€§` `é¢„è®­ç»ƒ` `çŸ¥è¯†è¿ç§»` `è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½ï¼Œç¼ºä¹å¯¹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­äº‹å®å›å¿†å’Œè·¨è¯­è¨€ä¸€è‡´æ€§çš„åŠ¨æ€åˆ†æã€‚
2. è®ºæ–‡é€šè¿‡è¿½è¸ªOLMo-7Bæ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œæ­ç¤ºäº†äº‹å®å›å¿†å’Œè·¨è¯­è¨€ä¸€è‡´æ€§éšæ—¶é—´æ¼”å˜çš„è§„å¾‹ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œäº‹å®é¢‘ç‡å¯¹å›å¿†å‡†ç¡®æ€§æœ‰æ˜¾è‘—å½±å“ï¼Œå¹¶ä¸”ä½é¢‘äº‹å®ä¹Ÿèƒ½é€šè¿‡è·¨è¯­è¨€è¿ç§»è¢«æ­£ç¡®å›å¿†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿå›å¿†å…¶é¢„è®­ç»ƒæ•°æ®ä¸­çš„å¤šè¯­è¨€äº‹å®çŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶ä»…è¯„ä¼°æœ€ç»ˆæ¨¡å‹ï¼Œå¯¼è‡´äº‹å®å›å¿†å’Œè·¨è¯­è¨€ä¸€è‡´æ€§çš„æ¼”å˜è¿‡ç¨‹æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶è¿½è¸ªäº†äº‹å®å›å¿†å’Œè·¨è¯­è¨€ä¸€è‡´æ€§åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜ï¼Œé‡ç‚¹ä»¥OLMo-7Bä¸ºæ¡ˆä¾‹ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€æ—¶é—´æ¨ç§»ï¼Œå¤§å¤šæ•°è¯­è¨€çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§å‡æœ‰æ‰€æå‡ï¼Œè¿™ä¸€æ”¹å–„ä¸»è¦å—åˆ°é¢„è®­ç»ƒè¯­æ–™ä¸­äº‹å®é¢‘ç‡çš„é©±åŠ¨ã€‚å°½ç®¡æŸäº›ä½é¢‘äº‹å®åœ¨éè‹±è¯­è¯­è¨€ä¸­ä»èƒ½è¢«æ­£ç¡®å›å¿†ï¼Œä½†è¿™äº›å®ä¾‹ä¸»è¦å¾—ç›Šäºå…¶è‹±è¯­å¯¹åº”ç‰©çš„è·¨è¯­è¨€è¿ç§»ã€‚æˆ‘ä»¬æŒ‡å‡ºäº†å¤šè¯­è¨€äº‹å®çŸ¥è¯†è·å–çš„ä¸¤æ¡ä¸åŒè·¯å¾„ï¼šé¢‘ç‡é©±åŠ¨å­¦ä¹ å’Œè·¨è¯­è¨€è¿ç§»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­äº‹å®å›å¿†å’Œè·¨è¯­è¨€ä¸€è‡´æ€§æ¼”å˜çš„ç ”ç©¶ç©ºç™½ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºæœ€ç»ˆæ¨¡å‹çš„è¯„ä¼°ï¼Œç¼ºä¹å¯¹æ¨¡å‹å­¦ä¹ è¿‡ç¨‹çš„æ·±å…¥åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹OLMo-7Bæ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹è¿›è¡Œè¿½è¸ªï¼Œåˆ†æäº‹å®å›å¿†å’Œè·¨è¯­è¨€ä¸€è‡´æ€§çš„æ¼”å˜ï¼Œæ­ç¤ºé¢‘ç‡é©±åŠ¨å­¦ä¹ å’Œè·¨è¯­è¨€è¿ç§»çš„ä½œç”¨æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¯¹æ¯”å®éªŒçš„æ–¹æ³•ï¼Œåˆ†æä¸åŒè¯­è¨€çš„äº‹å®å›å¿†æƒ…å†µï¼Œé‡ç‚¹å…³æ³¨äº‹å®é¢‘ç‡ä¸å›å¿†å‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒã€æ€§èƒ½è¯„ä¼°å’Œç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¿½è¸ªäº†å¤šè¯­è¨€äº‹å®çŸ¥è¯†çš„è·å–è¿‡ç¨‹ï¼Œæå‡ºäº†é¢‘ç‡é©±åŠ¨å­¦ä¹ å’Œè·¨è¯­è¨€è¿ç§»çš„åŒé‡è·¯å¾„ï¼Œä¸°å¯Œäº†å¯¹å¤šè¯­è¨€æ¨¡å‹å­¦ä¹ æœºåˆ¶çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œæ¨¡å‹çš„è®­ç»ƒæ•°æ®æ¥æºäºå¤šè¯­è¨€è¯­æ–™åº“ï¼Œé‡‡ç”¨äº†é¢‘ç‡ç»Ÿè®¡åˆ†ææ–¹æ³•æ¥è¯„ä¼°äº‹å®çš„å›å¿†æƒ…å†µï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†è·¨è¯­è¨€ä¸€è‡´æ€§ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹åœ¨ä¸åŒè¯­è¨€é—´çš„çŸ¥è¯†è¿ç§»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œéšç€é¢„è®­ç»ƒçš„è¿›è¡Œï¼Œå¤§å¤šæ•°è¯­è¨€çš„äº‹å®å›å¿†å‡†ç¡®æ€§å’Œè·¨è¯­è¨€ä¸€è‡´æ€§å‡æ˜¾è‘—æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œé¢‘ç‡è¾ƒé«˜çš„äº‹å®åœ¨å›å¿†æ—¶è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œä¸”ä½é¢‘äº‹å®åœ¨éè‹±è¯­è¯­è¨€ä¸­çš„å›å¿†ä¹Ÿå¾—åˆ°äº†è·¨è¯­è¨€è¿ç§»çš„æ”¯æŒï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨æ—©æœŸé˜¶æ®µçš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤šè¯­è¨€æ¨¡å‹çš„å¼€å‘æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ï¼Œå°¤å…¶åœ¨è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ã€ç¿»è¯‘ç³»ç»Ÿå’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡æ¨¡å‹çš„äº‹å®å›å¿†èƒ½åŠ›å’Œè·¨è¯­è¨€ä¸€è‡´æ€§ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å¤šè¯­è¨€å¤„ç†ä»»åŠ¡çš„æ•ˆæœå’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are capable of recalling multilingual factual knowledge present in their pretraining data. However, most studies evaluate only the final model, leaving the development of factual recall and crosslingual consistency throughout pretraining largely unexplored. In this work, we trace how factual recall and crosslingual consistency evolve during pretraining, focusing on OLMo-7B as a case study. We find that both accuracy and consistency improve over time for most languages. We show that this improvement is primarily driven by the fact frequency in the pretraining corpus: more frequent facts are more likely to be recalled correctly, regardless of language. Yet, some low-frequency facts in non-English languages can still be correctly recalled. Our analysis reveals that these instances largely benefit from crosslingual transfer of their English counterparts -- an effect that emerges predominantly in the early stages of pretraining. We pinpoint two distinct pathways through which multilingual factual knowledge acquisition occurs: (1) frequency-driven learning, which is dominant and language-agnostic, and (2) crosslingual transfer, which is limited in scale and typically constrained to relation types involving named entities. We release our code and data to facilitate further research at https://github.com/cisnlp/multilingual-fact-tracing.

