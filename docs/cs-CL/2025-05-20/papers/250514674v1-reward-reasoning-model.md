---
layout: default
title: Reward Reasoning Model
---

# Reward Reasoning Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14674" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14674v1</a>
  <a href="https://arxiv.org/pdf/2505.14674.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14674v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14674v1', 'Reward Reasoning Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/Reward-Reasoning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¥–åŠ±æ¨ç†æ¨¡å‹ä»¥æå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¥–åŠ±æ¨¡å‹` `æ¨ç†æœºåˆ¶` `å¼ºåŒ–å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¤æ‚æŸ¥è¯¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨å¤æ‚æŸ¥è¯¢ä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºï¼Œå¯¼è‡´å¥–åŠ±ç”Ÿæˆä¸å‡†ç¡®ã€‚
2. æœ¬æ–‡æå‡ºçš„å¥–åŠ±æ¨ç†æ¨¡å‹é€šè¿‡é“¾å¼æ€ç»´æ¨ç†ï¼Œåˆ©ç”¨é¢å¤–è®¡ç®—èµ„æºæ¥ç”Ÿæˆæ›´å‡†ç¡®çš„å¥–åŠ±ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRRMåœ¨å¤šä¸ªå¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿè‡ªé€‚åº”æå‡å¥–åŠ±å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±æ¨¡å‹åœ¨å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºä»¥å¢å¼ºå¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMï¼‰ï¼Œä¸“é—¨è®¾è®¡ç”¨äºåœ¨ç”Ÿæˆæœ€ç»ˆå¥–åŠ±ä¹‹å‰æ‰§è¡Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡é“¾å¼æ€ç»´æ¨ç†ï¼ŒRRMåœ¨å¤æ‚æŸ¥è¯¢ä¸­åˆ©ç”¨é¢å¤–çš„æµ‹è¯•æ—¶è®¡ç®—ï¼Œä»¥ä¾¿åœ¨é€‚å½“çš„å¥–åŠ±ä¸æ˜æ˜¾æ—¶è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¿ƒè¿›è‡ªæˆ‘æ¼”åŒ–çš„å¥–åŠ±æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€æ˜¾å¼çš„æ¨ç†è½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRMåœ¨å„ä¸ªé¢†åŸŸçš„å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—è¿›ä¸€æ­¥æé«˜å¥–åŠ±å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰çš„å¥–åŠ±æ¨¡å‹åœ¨é¢å¯¹å¤æ‚æŸ¥è¯¢æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºï¼Œå¯¼è‡´ç”Ÿæˆçš„å¥–åŠ±ä¸å¤Ÿå‡†ç¡®ï¼Œå½±å“æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMï¼‰é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´æ¨ç†æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆæœ€ç»ˆå¥–åŠ±ä¹‹å‰è¿›è¡Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRRMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯è¾“å…¥å¤æ‚æŸ¥è¯¢ï¼Œç„¶åé€šè¿‡é“¾å¼æ€ç»´æ¨ç†æ¨¡å—è¿›è¡Œæ¨ç†ï¼Œæœ€åç”Ÿæˆæœ€ç»ˆçš„å¥–åŠ±ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè‡ªæˆ‘æ¼”åŒ–ï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šRRMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºï¼Œè¿›è¡Œæ·±åº¦æ¨ç†ï¼Œè€Œä¸ä¾èµ–äºæ˜¾å¼çš„æ¨ç†è½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¥–åŠ±ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ç­–ç•¥è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä»¥æå‡æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¥–åŠ±æ¨ç†æ¨¡å‹åœ¨å¤šä¸ªå¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æŸ¥è¯¢çš„å¤„ç†ä¸Šï¼ŒRRMçš„å¥–åŠ±å‡†ç¡®æ€§æé«˜äº†15%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ï¼ŒRRMèƒ½å¤Ÿæ›´å¥½åœ°å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºï¼Œè¿›è€Œåœ¨å®é™…åº”ç”¨ä¸­æé«˜ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹å¯èƒ½ä¼šåœ¨æ›´å¤šå¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¤§çš„ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.

