---
layout: default
title: Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales
---

# Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14499" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14499v2</a>
  <a href="https://arxiv.org/pdf/2505.14499.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14499v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14499v2', 'Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Cao, Jiyi Li, Ziwei Yang, Renjie Zhou

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-24)

**å¤‡æ³¨**: 15 pages, 2 figures, 6 tables. Accepted by ICONIP2024

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLRSAæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„ä¿¡æ¯æ•´åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å°å‹è¯­è¨€æ¨¡å‹` `ç‰¹å¾èåˆ` `äº¤å‰æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ–¹æ³•ä¾èµ–å°å‹è¯­è¨€æ¨¡å‹ï¼Œå¯¼è‡´ä¿¡æ¯æ•´åˆèƒ½åŠ›ä¸è¶³ï¼Œè¯†åˆ«å‡†ç¡®æ€§ä½ã€‚
2. æœ¬æ–‡æå‡ºLRSAæ¡†æ¶ï¼Œé€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è§£é‡Šæ³¨å…¥å°å‹æ¨¡å‹ï¼Œå¢å¼ºå…¶å†³ç­–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLRSAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆMABSAï¼‰å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå°å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä»å›¾åƒå’Œæ–‡æœ¬ä¸­æ”¶é›†ä¸æ–¹é¢å’Œæƒ…æ„Ÿç›¸å…³çš„ä¿¡æ¯ï¼Œç„¶è€Œå°å‹æ¨¡å‹çš„èƒ½åŠ›å’ŒçŸ¥è¯†æœ‰é™ï¼Œå¯¼è‡´åœ¨æ–‡æœ¬å’Œè§†è§‰æ•°æ®ä¸­å¯¹æ„ä¹‰ã€æ–¹é¢ã€æƒ…æ„ŸåŠå…¶ç›¸äº’å…³ç³»çš„è¯†åˆ«ä¸å‡†ç¡®ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶LRSAï¼Œç»“åˆäº†SLMsçš„å†³ç­–èƒ½åŠ›å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›çš„é¢å¤–ä¿¡æ¯ï¼Œæ³¨å…¥LLMsç”Ÿæˆçš„è§£é‡Šä½œä¸ºæ¨ç†ï¼Œé‡‡ç”¨åŒé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾äº¤äº’ä¸èåˆï¼Œä»è€Œæå‡SLMsè¯†åˆ«æ–¹é¢å’Œæƒ…æ„Ÿçš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†ä¸Šä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†ææ–¹æ³•ä¸­ä¿¡æ¯æ•´åˆä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„å°å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å›¾åƒå’Œæ–‡æœ¬æ•°æ®æ—¶ï¼Œå¸¸å¸¸æ— æ³•å‡†ç¡®è¯†åˆ«æƒ…æ„Ÿå’Œæ–¹é¢çš„å…³ç³»ï¼Œå¯¼è‡´åˆ†æç»“æœä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„LRSAæ¡†æ¶ç»“åˆäº†å°å‹è¯­è¨€æ¨¡å‹çš„å†³ç­–èƒ½åŠ›ä¸å¤§å‹è¯­è¨€æ¨¡å‹æä¾›çš„ä¸°å¯Œä¿¡æ¯ï¼Œé€šè¿‡æ³¨å…¥LLMsç”Ÿæˆçš„æ¨ç†ï¼Œå¢å¼ºå°å‹æ¨¡å‹çš„ç‰¹å¾è¯†åˆ«èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLRSAæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šä¸€æ˜¯å°å‹è¯­è¨€æ¨¡å‹ï¼Œè´Ÿè´£åŸºç¡€çš„æƒ…æ„Ÿåˆ†æï¼›äºŒæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ¨ç†ã€‚é€šè¿‡åŒé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„ç‰¹å¾äº¤äº’ä¸èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šLRSAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†LLMsç”Ÿæˆçš„æ¨ç†ä¿¡æ¯æœ‰æ•ˆæ•´åˆåˆ°SLMsä¸­ï¼Œæ˜¾è‘—æå‡äº†æƒ…æ„Ÿå’Œæ–¹é¢çš„è¯†åˆ«èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€æ•°æ®ä¸­çš„ç»†ç²’åº¦ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŒé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºç‰¹å¾äº¤äº’ï¼Œæ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¤šæ¨¡æ€æ•°æ®çš„ç‰¹æ€§ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLRSAæ¡†æ¶åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°5%-10%ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†LRSAåœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“åˆ†æã€äº§å“è¯„è®ºåˆ†æå’Œæƒ…æ„Ÿç›‘æµ‹ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§ï¼ŒLRSAæ¡†æ¶èƒ½å¤Ÿå¸®åŠ©ä¼ä¸šæ›´å¥½åœ°ç†è§£ç”¨æˆ·åé¦ˆï¼Œä¼˜åŒ–äº§å“å’ŒæœåŠ¡ï¼Œä»è€Œåœ¨ç«äº‰ä¸­è·å¾—ä¼˜åŠ¿ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¦‚å›¾åƒæè¿°ç”Ÿæˆå’Œè§†é¢‘ç†è§£ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> There has been growing interest in Multimodal Aspect-Based Sentiment Analysis (MABSA) in recent years. Existing methods predominantly rely on pre-trained small language models (SLMs) to collect information related to aspects and sentiments from both image and text, with an aim to align these two modalities. However, small SLMs possess limited capacity and knowledge, often resulting in inaccurate identification of meaning, aspects, sentiments, and their interconnections in textual and visual data. On the other hand, Large language models (LLMs) have shown exceptional capabilities in various tasks by effectively exploring fine-grained information in multimodal data. However, some studies indicate that LLMs still fall short compared to fine-tuned small models in the field of ABSA. Based on these findings, we propose a novel framework, termed LRSA, which combines the decision-making capabilities of SLMs with additional information provided by LLMs for MABSA. Specifically, we inject explanations generated by LLMs as rationales into SLMs and employ a dual cross-attention mechanism for enhancing feature interaction and fusion, thereby augmenting the SLMs' ability to identify aspects and sentiments. We evaluated our method using two baseline models, numerous experiments highlight the superiority of our approach on three widely-used benchmarks, indicating its generalizability and applicability to most pre-trained models for MABSA.

