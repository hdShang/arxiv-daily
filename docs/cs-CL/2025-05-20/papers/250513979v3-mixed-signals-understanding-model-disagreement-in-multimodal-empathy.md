---
layout: default
title: Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection
---

# Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13979" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13979v3</a>
  <a href="https://arxiv.org/pdf/2505.13979.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13979v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13979v3', 'Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maya Srikanth, Run Chen, Julia Hirschberg

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-11)

**å¤‡æ³¨**: To appear in Findings of IJCNLP-AACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ¨¡å‹ä»¥è§£å†³åŒç±»ä¿¡å·å†²çªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŒç†å¿ƒæ£€æµ‹` `ä¿¡å·èåˆ` `æ¨¡æ€é—´åˆ†æ­§` `æƒ…æ„Ÿè®¡ç®—` `å¿ƒç†å¥åº·ç›‘æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€åŒç†å¿ƒæ£€æµ‹æ¨¡å‹åœ¨å¤„ç†æ¨¡æ€é—´çŸ›ç›¾ä¿¡å·æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡åˆ†æå•æ¨¡æ€ä¸å¤šæ¨¡æ€é¢„æµ‹çš„åˆ†æ­§ï¼Œæ­ç¤ºæ¨¡æ€é—´çš„æ½œåœ¨æ¨¡ç³Šæ€§ï¼Œä»¥æ”¹è¿›æ¨¡å‹çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡æ€é—´çš„ä¿¡å·åˆ†æ­§å¯ä»¥ä½œä¸ºè¯†åˆ«å›°éš¾ç¤ºä¾‹çš„æœ‰æ•ˆå·¥å…·ï¼Œå¸®åŠ©æå‡ç³»ç»Ÿæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ¨¡å‹åœ¨åŒç†å¿ƒæ£€æµ‹ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä½†å½“ä¸åŒæ¨¡æ€æä¾›ç›¸äº’çŸ›ç›¾çš„çº¿ç´¢æ—¶ï¼Œå…¶æ€§èƒ½å¯èƒ½å—åˆ°å½±å“ã€‚ä¸ºäº†è§£è¿™äº›å¤±è´¥æ¡ˆä¾‹ï¼Œæœ¬æ–‡ç ”ç©¶äº†å•æ¨¡æ€å’Œå¤šæ¨¡æ€é¢„æµ‹ä¹‹é—´çš„åˆ†æ­§ã€‚é€šè¿‡å¯¹æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘çš„å¾®è°ƒæ¨¡å‹åŠé—¨æ§èåˆæ¨¡å‹çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§åˆ†æ­§é€šå¸¸åæ˜ äº†æ½œåœ¨çš„æ¨¡ç³Šæ€§ï¼Œä¸”ä¸æ ‡æ³¨è€…çš„ä¸ç¡®å®šæ€§ç›¸å…³ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒæŸä¸€æ¨¡æ€çš„ä¸»å¯¼ä¿¡å·åœ¨ç¼ºä¹å…¶ä»–æ¨¡æ€æ”¯æŒæ—¶å¯èƒ½è¯¯å¯¼èåˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äººç±»åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸‹çš„è¡¨ç°ä¹Ÿå¹¶ä¸ä¸€è‡´ã€‚è¿™äº›è§è§£å°†åˆ†æ­§è§†ä¸ºè¯†åˆ«æŒ‘æˆ˜æ€§ç¤ºä¾‹å’Œæé«˜åŒç†å¿ƒç³»ç»Ÿé²æ£’æ€§çš„æœ‰ç”¨è¯Šæ–­ä¿¡å·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€åŒç†å¿ƒæ£€æµ‹ä¸­ä¸åŒæ¨¡æ€ä¿¡å·å†²çªå¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ¨¡æ€é—´çŸ›ç›¾æ—¶ç¼ºä¹æœ‰æ•ˆçš„åˆ†ææ‰‹æ®µï¼Œå¯¼è‡´æ¨¡å‹é²æ£’æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ·±å…¥åˆ†æå•æ¨¡æ€å’Œå¤šæ¨¡æ€é¢„æµ‹çš„åˆ†æ­§ï¼Œæ­ç¤ºæ¨¡æ€é—´çš„æ½œåœ¨æ¨¡ç³Šæ€§ï¼Œä»è€Œä¸ºæ¨¡å‹æä¾›æ›´æ¸…æ™°çš„ä¿¡å·èåˆç­–ç•¥ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹åœ¨é¢å¯¹å¤æ‚è¾“å…¥æ—¶çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘çš„å¾®è°ƒæ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªé—¨æ§èåˆæ¨¡å‹ã€‚é€šè¿‡å¯¹ä¸åŒæ¨¡æ€çš„ä¿¡å·è¿›è¡Œåˆ†æå’Œèåˆï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†åŒç†å¿ƒæ£€æµ‹ä»»åŠ¡ä¸­çš„å¤æ‚æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ¨¡æ€é—´çš„ä¿¡å·åˆ†æ­§è§†ä¸ºä¸€ç§è¯Šæ–­å·¥å…·ï¼Œå¸®åŠ©è¯†åˆ«å›°éš¾ç¤ºä¾‹å¹¶æå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚è¿™ä¸€æ€è·¯ä¸ä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€æ¨¡æ€åˆ†æå½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é—¨æ§æœºåˆ¶æ¥æ§åˆ¶ä¸åŒæ¨¡æ€ä¿¡å·çš„èåˆç¨‹åº¦ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å¤„ç†æ¨¡æ€ä¿¡å·åˆ†æ­§æ—¶çš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æƒ…å¢ƒä¸‹çš„åŒç†å¿ƒæ£€æµ‹ä»»åŠ¡ä¸­ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œé‡‡ç”¨æ–°æ–¹æ³•çš„æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šæé«˜äº†çº¦15%ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¿ƒç†å¥åº·ç›‘æµ‹ã€ç¤¾äº¤æœºå™¨äººä»¥åŠäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜åŒç†å¿ƒæ£€æµ‹ç³»ç»Ÿçš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”ç”¨æˆ·æƒ…æ„Ÿï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€æƒ…æ„Ÿè®¡ç®—çš„å‘å±•ï¼Œä¿ƒè¿›æ›´æ™ºèƒ½çš„äº¤äº’ç³»ç»Ÿçš„å®ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal models play a key role in empathy detection, but their performance can suffer when modalities provide conflicting cues. To understand these failures, we examine cases where unimodal and multimodal predictions diverge. Using fine-tuned models for text, audio, and video, along with a gated fusion model, we find that such disagreements often reflect underlying ambiguity, as evidenced by annotator uncertainty. Our analysis shows that dominant signals in one modality can mislead fusion when unsupported by others. We also observe that humans, like models, do not consistently benefit from multimodal input. These insights position disagreement as a useful diagnostic signal for identifying challenging examples and improving empathy system robustness.

