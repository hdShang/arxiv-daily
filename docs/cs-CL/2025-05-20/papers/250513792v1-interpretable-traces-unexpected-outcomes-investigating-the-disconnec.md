---
layout: default
title: "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation"
---

# Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13792" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.13792v1</a>
  <a href="https://arxiv.org/pdf/2505.13792.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13792v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13792v1', 'Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Siddhant Bhambri, Upasana Biswas, Subbarao Kambhampati

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-20

**Â§áÊ≥®**: 10 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éËßÑÂàôÂàÜËß£ÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ï‰ª•ÊèêÂçáÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `ÂèØËß£ÈáäÊÄß` `ÈóÆÁ≠îÁ≥ªÁªü` `Êé®ÁêÜËøΩË∏™` `ËßÑÂàôÂàÜËß£` `Â∞èÂûãËØ≠Ë®ÄÊ®°Âûã` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÂú®Âà©Áî®Êé®ÁêÜËøΩË∏™ÊèêÂçáÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÊó∂ÔºåÈù¢‰∏¥ÂèØËß£ÈáäÊÄß‰∏çË∂≥ÂíåËØÑ‰º∞Âõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËßÑÂàôÁöÑÈóÆÈ¢òÂàÜËß£ÊñπÊ≥ïÔºåÈÄöËøáÂ∞ÜÂ§çÊùÇÊü•ËØ¢ÊãÜËß£‰∏∫ÁªìÊûÑÂåñÂ≠êÈóÆÈ¢òÔºåÁîüÊàêÂèØËß£ÈáäÁöÑÊé®ÁêÜËøΩË∏™„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊ≠£Á°ÆÁöÑÊé®ÁêÜËøΩË∏™‰∏éÊúÄÁªàÁ≠îÊ°àÁöÑÊ≠£Á°ÆÊÄß‰πãÈó¥Â≠òÂú®‰ΩéÁõ∏ÂÖ≥ÊÄßÔºåÊåëÊàò‰∫Ü‰º†ÁªüÁü•ËØÜËí∏È¶èÁöÑÂÅáËÆæ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈóÆÁ≠îÁ≥ªÁªüÂú®Áé∞‰ª£‰∫§‰∫íÂºèÂØπËØùÁ≥ªÁªü‰∏≠Èù¢‰∏¥ÁùÄÂáÜÁ°ÆÊÄßÂíåÈÄèÊòéÊÄßÁöÑÂèåÈáçÊåëÊàò„ÄÇÂ∞ΩÁÆ°Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ËÆ°ÁÆóÊïàÁéá‰∏äÂÖ∑Êúâ‰ºòÂäøÔºå‰ΩÜÂÖ∂ÊÄßËÉΩÂæÄÂæÄ‰Ωé‰∫éÂ§ßÂûãÊ®°Âûã„ÄÇÁü•ËØÜËí∏È¶èÊñπÊ≥ïÈÄöËøáÂæÆË∞ÉÂ∞èÂûãÊ®°Âûã‰ª•ÊèêÂçáÂÖ∂ÊÄßËÉΩÔºåÁÑ∂ËÄåÔºåÂü∫‰∫éÊé®ÁêÜÁöÑ‰∏≠Èó¥ËøΩË∏™‰ø°ÊÅØÂæÄÂæÄÂÜóÈïø‰∏îÈöæ‰ª•Ëß£Èáä„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËßÑÂàôÁöÑÈóÆÈ¢òÂàÜËß£ÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òËøô‰∫õÊé®ÁêÜËøΩË∏™ÁöÑÂèØËß£ÈáäÊÄßÂíåËØÑ‰º∞ÁöÑÂèØË°åÊÄß„ÄÇÈÄöËøáÂú®Open Book QA‰∏äËøõË°åÂÆûÈ™åÔºåÂèëÁé∞Ê≠£Á°ÆÁöÑÊé®ÁêÜËøΩË∏™Âπ∂‰∏çÊÄªÊòØËÉΩ‰øùËØÅÊ®°ÂûãËæìÂá∫Ê≠£Á°ÆÁöÑÊúÄÁªàÁ≠îÊ°àÔºåËøô‰∏ÄÂèëÁé∞ÊåëÊàò‰∫ÜÂà©Áî®Êé®ÁêÜËøΩË∏™ÊèêÂçáÂ∞èÂûãÊ®°ÂûãÊÄßËÉΩÁöÑÂÅáËÆæ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êé®ÁêÜËøΩË∏™ÁöÑÂèØËß£ÈáäÊÄßÂíåËØÑ‰º∞ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Âà©Áî®Ëøô‰∫õËøΩË∏™‰ø°ÊÅØÊó∂Â∏∏Â∏∏Èù¢‰∏¥ÂÜóÈïøÂíåÈöæ‰ª•ÁêÜËß£ÁöÑÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•Âü∫‰∫éËßÑÂàôÁöÑÈóÆÈ¢òÂàÜËß£ÊñπÊ≥ïÔºåÂ∞ÜÂ§çÊùÇÁöÑÈóÆÁ≠î‰ªªÂä°ÊãÜËß£‰∏∫Êõ¥ÁÆÄÂçïÁöÑÂàÜÁ±ªÂíå‰ø°ÊÅØÊ£ÄÁ¥¢Ê≠•È™§Ôºå‰ªéËÄåÁîüÊàêÂèØËß£ÈáäÁöÑÊé®ÁêÜËøΩË∏™„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÁ¨¨‰∏ÄÊ≠•ÊòØÂ∞ÜÈóÆÈ¢òÂàÜÁ±ªÔºåÁ¨¨‰∫åÊ≠•ÊòØËøõË°å‰ø°ÊÅØÊ£ÄÁ¥¢„ÄÇËøôÁßçÂàÜËß£ÊñπÊ≥ï‰ΩøÂæóÊØè‰∏™Ê≠•È™§ÁöÑËæìÂá∫ÈÉΩÂèØ‰ª•Áã¨Á´ãËØÑ‰º∞ÔºåÂ¢ûÂº∫‰∫ÜÂèØËß£ÈáäÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂü∫‰∫éËßÑÂàôÁöÑÈóÆÈ¢òÂàÜËß£Á≠ñÁï•Ôºå‰ΩøÂæóÊé®ÁêÜËøΩË∏™ÁöÑÁîüÊàêÂíåËØÑ‰º∞ÂèòÂæóÊõ¥Âä†Áõ¥ËßÇÂíåÂèØÊìç‰ΩúÔºå‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊé®ÁêÜÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÂçá‰∫ÜÂèØËß£ÈáäÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÊ®°ÂûãÂú®ÂàÜÁ±ªÂíå‰ø°ÊÅØÊ£ÄÁ¥¢Ê≠•È™§ÁöÑË°®Áé∞ÔºåÂêåÊó∂ÂØπ‰∏≠Èó¥ËøΩË∏™ÁöÑÊ≠£Á°ÆÊÄßËøõË°å‰∫Ü‰∏•Ê†ºÁöÑËØÑ‰º∞ÔºåÁ°Æ‰øù‰∫ÜÊúÄÁªàËæìÂá∫ÁöÑÂèØÈù†ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°Êé®ÁêÜËøΩË∏™ÁöÑÊ≠£Á°ÆÊÄß‰∏éÊúÄÁªàÁ≠îÊ°àÁöÑÊ≠£Á°ÆÊÄß‰πãÈó¥Â≠òÂú®‰ΩéÁõ∏ÂÖ≥ÊÄßÔºå‰ΩÜÈÄöËøáÂü∫‰∫éËßÑÂàôÁöÑÈóÆÈ¢òÂàÜËß£ÊñπÊ≥ïÔºåÊ®°ÂûãÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞ÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçá„ÄÇËøô‰∏ÄÂèëÁé∞‰∏∫Áü•ËØÜËí∏È¶èÊñπÊ≥ïÁöÑÂ∫îÁî®Êèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßí„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅÂØπËØù‰ª£ÁêÜÂíåÊïôËÇ≤ÊäÄÊúØÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÊÄßËÉΩÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Êª°Ë∂≥Áî®Êà∑ÂØπÈÄèÊòéÊÄßÂíåÂáÜÁ°ÆÊÄßÁöÑÈúÄÊ±ÇÔºåÊé®Âä®‰∫∫Êú∫‰∫§‰∫íÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.

