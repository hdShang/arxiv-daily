---
layout: default
title: "Think-J: Learning to Think for Generative LLM-as-a-Judge"
---

# Think-J: Learning to Think for Generative LLM-as-a-Judge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14268" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14268v1</a>
  <a href="https://arxiv.org/pdf/2505.14268.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14268v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14268v1', 'Think-J: Learning to Think for Generative LLM-as-a-Judge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, Jiaheng Liu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: 16 pages, 14 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºThink-Jä»¥æå‡ç”Ÿæˆå¼LLMçš„è¯„åˆ¤èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿæˆå¼æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨è¯„åˆ¤` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç”Ÿæˆå¼LLMåœ¨ä½œä¸ºè¯„åˆ¤è€…æ—¶çš„è¡¨ç°ä¸å°½å¦‚äººæ„ï¼Œæ— æ³•æœ‰æ•ˆå»ºæ¨¡ç”Ÿæˆå“åº”çš„åå¥½ã€‚
2. æœ¬æ–‡æå‡ºçš„Think-Jé€šè¿‡å­¦ä¹ æ€è€ƒè¿‡ç¨‹æ¥æå‡ç”Ÿæˆå¼LLMçš„è¯„åˆ¤èƒ½åŠ›ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒThink-Jæ˜¾è‘—æå‡äº†è¯„åˆ¤èƒ½åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç”Ÿæˆå’Œåˆ†ç±»å™¨åŸºç¡€çš„è¯„åˆ¤æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

LLM-as-a-JudgeæŒ‡çš„æ˜¯è‡ªåŠ¨å»ºæ¨¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„å“åº”åå¥½ï¼Œè¿™å¯¹LLMè¯„ä¼°å’Œå¥–åŠ±å»ºæ¨¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚å°½ç®¡ç”Ÿæˆå¼LLMsåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ä½œä¸ºè¯„åˆ¤è€…çš„è¡¨ç°ä»æœªè¾¾åˆ°é¢„æœŸã€‚æœ¬æ–‡æå‡ºäº†Think-Jï¼Œé€šè¿‡å­¦ä¹ æ€è€ƒæ¥æ”¹å–„ç”Ÿæˆå¼LLM-as-a-Judgeã€‚æˆ‘ä»¬é¦–å…ˆåˆ©ç”¨å°‘é‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®å¼€å‘å…·æœ‰åˆæ­¥åˆ¤æ–­æ€ç»´èƒ½åŠ›çš„æ¨¡å‹ï¼ŒéšååŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–åˆ¤æ–­æ€ç»´è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäºç¦»çº¿å’Œåœ¨çº¿RLçš„åˆ¤æ–­æ€ç»´ä¼˜åŒ–æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆå¼LLMè¯„åˆ¤è€…çš„è¯„ä¼°èƒ½åŠ›ï¼Œè¶…è¶Šäº†æ— éœ€é¢å¤–äººå·¥æ ‡æ³¨çš„ç”Ÿæˆå’Œåˆ†ç±»å™¨åŸºç¡€çš„LLMè¯„åˆ¤è€…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç”Ÿæˆå¼LLMä½œä¸ºè¯„åˆ¤è€…æ—¶çš„èƒ½åŠ›ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å»ºæ¨¡ç”Ÿæˆå“åº”åå¥½æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºThink-Jï¼Œé€šè¿‡å­¦ä¹ æ€è€ƒè¿‡ç¨‹æ¥æå‡ç”Ÿæˆå¼LLMçš„è¯„åˆ¤èƒ½åŠ›ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–åˆ¤æ–­æ€ç»´è½¨è¿¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åˆæ­¥æ¨¡å‹å¼€å‘å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–é˜¶æ®µï¼Œå‰è€…åˆ©ç”¨å°é‡æ•°æ®æ„å»ºåˆå§‹æ¨¡å‹ï¼Œåè€…åˆ™é€šè¿‡ç¦»çº¿å’Œåœ¨çº¿RLæ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸¤ç§ä¸åŒçš„åˆ¤æ–­æ€ç»´ä¼˜åŒ–æ–¹æ³•ï¼Œåˆ†åˆ«åŸºäºç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†è¯„åˆ¤èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šç¦»çº¿RLæ–¹æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªè¯„è®ºæ¨¡å‹ä»¥æ„å»ºæ­£è´Ÿç¤ºä¾‹ï¼Œè€Œåœ¨çº¿æ–¹æ³•åˆ™å®šä¹‰åŸºäºè§„åˆ™çš„å¥–åŠ±ä½œä¸ºåé¦ˆï¼Œä¼˜åŒ–è¿‡ç¨‹ä¸éœ€è¦é¢å¤–çš„äººç±»æ ‡æ³¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒThink-Jåœ¨è¯„åˆ¤èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç”Ÿæˆå’Œåˆ†ç±»å™¨åŸºç¡€çš„LLMè¯„åˆ¤è€…ï¼Œå…·ä½“æå‡å¹…åº¦è¶…è¿‡äº†XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œä¸”æ— éœ€é¢å¤–çš„äººç±»æ ‡æ³¨ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–å†…å®¹è¯„ä¼°ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡ç”Ÿæˆå¼LLMçš„è¯„åˆ¤èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both LLM evaluation and reward modeling. Although generative LLMs have made substantial progress in various tasks, their performance as LLM-Judge still falls short of expectations. In this work, we propose Think-J, which improves generative LLM-as-a-Judge by learning how to think. We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities. Subsequently, we optimize the judgment thinking traces based on reinforcement learning (RL). We propose two methods for judgment thinking optimization, based on offline and online RL, respectively. The offline RL requires training a critic model to construct positive and negative examples for learning. The online method defines rule-based reward as feedback for optimization. Experimental results showed that our approach can significantly enhance the evaluation capability of generative LLM-Judge, surpassing both generative and classifier-based LLM-Judge without requiring extra human annotations.

