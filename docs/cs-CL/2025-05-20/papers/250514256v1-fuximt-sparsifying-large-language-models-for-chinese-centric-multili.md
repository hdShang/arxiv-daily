---
layout: default
title: FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation
---

# FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14256" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14256v1</a>
  <a href="https://arxiv.org/pdf/2505.14256.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14256v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14256v1', 'FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaolin Zhu, Tianyu Dong, Bo Li, Deyi Xiong

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFuxiMTä»¥è§£å†³ä¸­æ–‡ä¸ºä¸­å¿ƒçš„å¤šè¯­è¨€æœºå™¨ç¿»è¯‘é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè¯­è¨€ç¿»è¯‘` `ç¨€ç–æ¨¡å‹` `æœºå™¨ç¿»è¯‘` `ä¸­æ–‡å¤„ç†` `è¯¾ç¨‹å­¦ä¹ ` `ä¸“å®¶æ··åˆæ¨¡å‹` `ä½èµ„æºç¿»è¯‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè¯­è¨€æœºå™¨ç¿»è¯‘æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€å¯¹çš„ç¿»è¯‘æ€§èƒ½è¾ƒå·®ï¼Œå°¤å…¶æ˜¯ä¸­æ–‡ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ã€‚
2. FuxiMTé€šè¿‡ç¨€ç–åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæå‡äº†å¤šè¯­è¨€ç¿»è¯‘çš„æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFuxiMTåœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå°¤å…¶åœ¨é›¶-shotç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸­æ–‡ä¸ºä¸­å¿ƒçš„å¤šè¯­è¨€æœºå™¨ç¿»è¯‘æ¨¡å‹FuxiMTï¼Œè¯¥æ¨¡å‹åŸºäºç¨€ç–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥è®­ç»ƒFuxiMTï¼Œé¦–å…ˆåœ¨å¤§è§„æ¨¡ä¸­æ–‡è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨åŒ…å«65ç§è¯­è¨€çš„å¤§å‹å¹³è¡Œæ•°æ®é›†ä¸Šè¿›è¡Œå¤šè¯­è¨€å¾®è°ƒã€‚FuxiMTç»“åˆäº†ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEsï¼‰å¹¶é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä»¥åœ¨ä¸åŒèµ„æºæ°´å¹³ä¸‹å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFuxiMTåœ¨ä½èµ„æºåœºæ™¯ä¸‹æ˜¾è‘—è¶…è¶Šäº†å¼ºåŸºçº¿ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„LLMå’Œæœºå™¨ç¿»è¯‘æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒFuxiMTåœ¨æœªè§è¯­è¨€å¯¹çš„é›¶-shotç¿»è¯‘èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¹³è¡Œæ•°æ®ç¨€ç¼ºæˆ–ä¸å¯ç”¨æƒ…å†µä¸‹å¼¥åˆæ²Ÿé€šå·®è·çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šè¯­è¨€æœºå™¨ç¿»è¯‘æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€å¯¹ç¿»è¯‘ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡ä¸ºä¸­å¿ƒçš„ç¿»è¯‘åœºæ™¯ä¸­ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨ç¨€ç¼ºçš„å¹³è¡Œæ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFuxiMTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¨€ç–åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEsï¼‰å’Œè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œæå‡æ¨¡å‹åœ¨å¤šè¯­è¨€ç¿»è¯‘ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºæƒ…å†µä¸‹çš„ç¿»è¯‘èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFuxiMTçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆåœ¨å¤§è§„æ¨¡ä¸­æ–‡è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨åŒ…å«65ç§è¯­è¨€çš„å¹³è¡Œæ•°æ®é›†ä¸Šè¿›è¡Œå¤šè¯­è¨€å¾®è°ƒã€‚æ¨¡å‹é€šè¿‡Mixture-of-Expertsæœºåˆ¶åŠ¨æ€é€‰æ‹©ä¸“å®¶ï¼Œä»¥é€‚åº”ä¸åŒçš„ç¿»è¯‘ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šFuxiMTçš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†ç¨€ç–åŒ–çš„LLMä¸MoEsï¼Œå…è®¸æ¨¡å‹åœ¨ä¸åŒèµ„æºæ°´å¹³ä¸‹çµæ´»è°ƒæ•´ï¼ŒåŒæ—¶é€šè¿‡è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFuxiMTåœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„è¡¨ç°æ›´ä¸ºä¼˜è¶Šã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒFuxiMTé‡‡ç”¨äº†åŠ¨æ€é€‰æ‹©ä¸“å®¶çš„æœºåˆ¶ï¼Œè®¾ç½®äº†é€‚åº”ä¸åŒè¯­è¨€å¯¹çš„æŸå¤±å‡½æ•°ï¼Œå¹¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥æé«˜ç¿»è¯‘ç²¾åº¦å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFuxiMTåœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„ç¿»è¯‘æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå°¤å…¶åœ¨é›¶-shotç¿»è¯‘ä»»åŠ¡ä¸­ï¼ŒFuxiMTèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æœªè§è¯­è¨€å¯¹ï¼Œå±•ç¤ºå‡ºé«˜è¾¾XX%çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æ•°æ®å¾…è¡¥å……ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FuxiMTçš„ç ”ç©¶æˆæœåœ¨å¤šè¯­è¨€æœºå™¨ç¿»è¯‘é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºä¸­æ–‡ä¸å…¶ä»–è¯­è¨€ä¹‹é—´çš„ç¿»è¯‘ã€‚å…¶åœ¨ä½èµ„æºè¯­è¨€å¯¹çš„ä¼˜è¶Šè¡¨ç°ï¼Œèƒ½å¤Ÿå¸®åŠ©è§£å†³å…¨çƒèŒƒå›´å†…çš„è¯­è¨€æ²Ÿé€šéšœç¢ï¼Œä¿ƒè¿›è·¨æ–‡åŒ–äº¤æµä¸åˆä½œã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨æ•™è‚²ã€å›½é™…è´¸æ˜“å’Œæ—…æ¸¸ç­‰å¤šä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.

