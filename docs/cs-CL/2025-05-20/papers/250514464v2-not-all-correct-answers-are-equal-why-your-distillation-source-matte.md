---
layout: default
title: Not All Correct Answers Are Equal: Why Your Distillation Source Matters
---

# Not All Correct Answers Are Equal: Why Your Distillation Source Matters

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14464" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14464v2</a>
  <a href="https://arxiv.org/pdf/2505.14464.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14464v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14464v2', 'Not All Correct Answers Are Equal: Why Your Distillation Source Matters')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, Xiangang Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-22)

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/a-m-team)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡é«˜è´¨é‡è’¸é¦æ•°æ®æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è’¸é¦è®­ç»ƒ` `è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ•°æ®é›†æ„å»º` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡é«˜è´¨é‡çš„è’¸é¦æ•°æ®æ¥æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé‡ç‚¹åˆ†æä¸åŒæ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºAM-Thinking-v1è’¸é¦çš„æ•°æ®åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šè¡¨ç°æœ€ä½³ï¼Œä¸”æ¨¡å‹è¾“å‡ºå…·æœ‰é€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è’¸é¦å·²æˆä¸ºå¢å¼ºå¼€æºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœ¬æ–‡é€šè¿‡å¯¹ä¸‰ç§å…ˆè¿›æ•™å¸ˆæ¨¡å‹ï¼ˆAM-Thinking-v1ã€Qwen3-235B-A22Bå’ŒDeepSeek-R1ï¼‰è¿›è¡Œå¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œæ”¶é›†äº†189ä¸‡æ¡æŸ¥è¯¢çš„éªŒè¯è¾“å‡ºï¼Œæ„å»ºäº†ä¸‰ä¸ªå¹³è¡Œæ•°æ®é›†å¹¶åˆ†æå…¶åˆ†å¸ƒã€‚ç»“æœè¡¨æ˜ï¼ŒAM-Thinking-v1è’¸é¦æ•°æ®åœ¨æ ‡è®°é•¿åº¦å¤šæ ·æ€§å’Œå›°æƒ‘åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚åŸºäºè¯¥æ•°æ®é›†è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šè¡¨ç°æœ€ä½³ï¼Œå±•ç¤ºäº†é€‚åº”æ€§è¾“å‡ºè¡Œä¸ºã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†é«˜è´¨é‡æ¨ç†è½¨è¿¹çš„é‡è¦æ€§ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³æ•°æ®é›†ä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºä½è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ”¶é›†å’Œåˆ†ææ¥è‡ªä¸åŒæ•™å¸ˆæ¨¡å‹çš„é«˜è´¨é‡è’¸é¦æ•°æ®ï¼Œæ¢ç´¢å…¶å¯¹å­¦ç”Ÿæ¨¡å‹æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œå¼ºè°ƒè’¸é¦æºçš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚é¦–å…ˆä»ä¸‰ä¸ªæ•™å¸ˆæ¨¡å‹ä¸­æå–è¾“å‡ºï¼Œç„¶åæ„å»ºä¸‰ä¸ªå¹³è¡Œæ•°æ®é›†ï¼Œæœ€ååœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šè¯„ä¼°å­¦ç”Ÿæ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å¤§è§„æ¨¡å®è¯ç ”ç©¶æ­ç¤ºäº†ä¸åŒæ•™å¸ˆæ¨¡å‹è¾“å‡ºè´¨é‡å¯¹å­¦ç”Ÿæ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—å½±å“ï¼Œå°¤å…¶æ˜¯AM-Thinking-v1è’¸é¦æ•°æ®çš„ä¼˜è¶Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºä¸­ï¼Œå…³æ³¨æ ‡è®°é•¿åº¦çš„å¤šæ ·æ€§å’Œå›°æƒ‘åº¦ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„é«˜è´¨é‡ï¼›åœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨é€‚åº”æ€§è¾“å‡ºç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éš¾åº¦è°ƒæ•´å“åº”é•¿åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºAM-Thinking-v1è’¸é¦çš„æ•°æ®åœ¨AIME2024ã€AIME2025ã€MATH500å’ŒLiveCodeBenchç­‰æ¨ç†åŸºå‡†ä¸Šåˆ†åˆ«å–å¾—äº†84.3ã€72.2ã€98.4å’Œ65.9çš„ä¼˜å¼‚æˆç»©ï¼Œæ˜æ˜¾ä¼˜äºå…¶ä»–æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨é—®ç­”ç³»ç»Ÿå’Œå¤æ‚é—®é¢˜æ±‚è§£ç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„æ™ºèƒ½äº¤äº’ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The model distilled from AM-Thinking-v1 consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging Face\footnote{Datasets are available on Hugging Face: \href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled}, \href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

