---
layout: default
title: Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs
---

# Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14368" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14368v1</a>
  <a href="https://arxiv.org/pdf/2505.14368.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14368v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14368v1', 'Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiawen Wang, Pritha Gupta, Ivan Habernal, Eyke HÃ¼llermeier

**åˆ†ç±»**: cs.CR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: 8 pages, 3 figures, EMNLP 2025 under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœ‰æ•ˆçš„æç¤ºæ³¨å…¥æ”»å‡»ä»¥è¯„ä¼°å¼€æºLLMçš„å®‰å…¨æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æç¤ºæ³¨å…¥æ”»å‡»` `å¤§å‹è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§è¯„ä¼°` `æ”»å‡»æˆåŠŸæ¦‚ç‡` `å¼€æºæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹å¼€æºLLMçš„æç¤ºæ³¨å…¥æ”»å‡»å…³æ³¨ä¸è¶³ï¼Œå¯¼è‡´å…¶å®‰å…¨æ€§è¯„ä¼°ä¸å…¨é¢ã€‚
2. æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æˆåŠŸæ¦‚ç‡ï¼ˆASPï¼‰æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ¨¡å‹å“åº”çš„ä¸ç¡®å®šæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå‚¬çœ æ”»å‡»å’Œå¿½ç•¥å‰ç¼€æ”»å‡»åœ¨å¤šä¸ªå¼€æºLLMä¸Šå‡è¡¨ç°å‡ºé«˜æ•ˆçš„æ”»å‡»æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜“å—ä¸åŒçš„åŸºäºæç¤ºçš„æ”»å‡»ï¼Œå¯èƒ½ç”Ÿæˆæœ‰å®³å†…å®¹æˆ–æ•æ„Ÿä¿¡æ¯ã€‚æœ¬æ–‡ç ”ç©¶äº†é’ˆå¯¹14ç§æµè¡Œå¼€æºLLMçš„æœ‰æ•ˆæç¤ºæ³¨å…¥æ”»å‡»ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æˆåŠŸæ¦‚ç‡ï¼ˆASPï¼‰æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåæ˜ æ¨¡å‹å“åº”çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡å…¨é¢åˆ†ææç¤ºæ³¨å…¥æ”»å‡»çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å‚¬çœ æ”»å‡»ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ”»å‡»ä½¿å¾—å¤šä¸ªå¯¹é½çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸å½“è¡Œä¸ºï¼ŒASPè¾¾åˆ°çº¦90%ã€‚æ­¤å¤–ï¼Œå¿½ç•¥å‰ç¼€æ”»å‡»èƒ½å¤Ÿçªç ´æ‰€æœ‰14ç§å¼€æºLLMï¼Œåœ¨å¤šç±»åˆ«æ•°æ®é›†ä¸Šå®ç°è¶…è¿‡60%çš„ASPï¼Œå‘ç°ä¸­ç­‰çŸ¥ååº¦çš„LLMå¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„è„†å¼±æ€§æ›´é«˜ï¼Œå¼ºè°ƒäº†å…¬ä¼—æ„è¯†æå‡å’Œæœ‰æ•ˆç¼“è§£ç­–ç•¥çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æç¤ºæ³¨å…¥æ”»å‡»ä¸‹çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•ä»…å…³æ³¨æ”»å‡»æˆåŠŸç‡ï¼Œæœªè€ƒè™‘æ¨¡å‹å“åº”çš„ä¸ç¡®å®šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ”»å‡»æˆåŠŸæ¦‚ç‡ï¼ˆASPï¼‰ä½œä¸ºæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåæ˜ æ”»å‡»çš„å¯è¡Œæ€§å’Œæ¨¡å‹çš„å“åº”æ¨¡ç³Šæ€§ï¼ŒåŒæ—¶è®¾è®¡å‚¬çœ æ”»å‡»å’Œå¿½ç•¥å‰ç¼€æ”»å‡»ä»¥æé«˜æ”»å‡»æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é€šè¿‡äº”ä¸ªæ”»å‡»åŸºå‡†å¯¹14ç§å¼€æºLLMè¿›è¡Œè¯„ä¼°ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ”»å‡»è®¾è®¡ã€æ¨¡å‹å“åº”åˆ†æå’ŒæˆåŠŸç‡è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥ASPæŒ‡æ ‡ï¼Œç»¼åˆè€ƒè™‘æ”»å‡»æˆåŠŸç‡å’Œæ¨¡å‹å“åº”çš„ä¸ç¡®å®šæ€§ï¼Œæä¾›æ›´å…¨é¢çš„å®‰å…¨è¯„ä¼°ã€‚å‚¬çœ æ”»å‡»å’Œå¿½ç•¥å‰ç¼€æ”»å‡»æ˜¯æœ¬æ–‡çš„ä¸»è¦åˆ›æ–°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆçªç ´å¤šç§å¼€æºLLMçš„é˜²å¾¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œå‚¬çœ æ”»å‡»å®ç°äº†çº¦90%çš„ASPï¼Œè€Œå¿½ç•¥å‰ç¼€æ”»å‡»åœ¨å¤šç±»åˆ«æ•°æ®é›†ä¸Šè¶…è¿‡60%çš„ASPï¼Œæ˜¾ç¤ºå‡ºå¯¹ä¸åŒæ¨¡å‹çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå‚¬çœ æ”»å‡»åœ¨å¤šä¸ªå¯¹é½çš„è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†çº¦90%çš„æ”»å‡»æˆåŠŸæ¦‚ç‡ï¼Œè€Œå¿½ç•¥å‰ç¼€æ”»å‡»åœ¨æ‰€æœ‰14ç§å¼€æºLLMä¸Šå‡çªç ´äº†60%çš„æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”»å‡»æ•ˆæœå’Œå¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§è¯„ä¼°ã€æ¨¡å‹é˜²å¾¡æœºåˆ¶è®¾è®¡ä»¥åŠå¼€æºLLMçš„å®‰å…¨æ€§æå‡ã€‚é€šè¿‡è¯†åˆ«å’Œç¼“è§£æç¤ºæ³¨å…¥æ”»å‡»ï¼Œèƒ½å¤Ÿå¢å¼ºæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œä¿æŠ¤ç”¨æˆ·æ•°æ®å’Œéšç§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent studies demonstrate that Large Language Models (LLMs) are vulnerable to different prompt-based attacks, generating harmful content or sensitive information. Both closed-source and open-source LLMs are underinvestigated for these attacks. This paper studies effective prompt injection attacks against the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks. Current metrics only consider successful attacks, whereas our proposed Attack Success Probability (ASP) also captures uncertainty in the model's response, reflecting ambiguity in attack feasibility. By comprehensively analyzing the effectiveness of prompt injection attacks, we propose a simple and effective hypnotism attack; results show that this attack causes aligned language models, including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable behaviors, achieving around $90$% ASP. They also indicate that our ignore prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over $60$% ASP on a multi-categorical dataset. We find that moderately well-known LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the need to raise public awareness and prioritize efficient mitigation strategies.

