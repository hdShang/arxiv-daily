---
layout: default
title: "Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM"
---

# Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13890" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13890v1</a>
  <a href="https://arxiv.org/pdf/2505.13890.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13890v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13890v1', 'Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhen Xiong, Yujun Cai, Zhecheng Li, Yiwei Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›¾åŸºåˆ†ææ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†æ¨¡å‹` `å›¾åŸºåˆ†æ` `æ€ç»´é“¾ç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†LLMsåœ¨å°‘é‡ç¤ºä¾‹æç¤ºä¸‹è¡¨ç°å‡ºæ€§èƒ½ä¸‹é™ï¼Œæ­ç¤ºäº†å¯¹å…¶æ¨ç†æœºåˆ¶ç†è§£çš„ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å›¾åŸºåˆ†ææ¡†æ¶ï¼Œé€šè¿‡èšç±»å’Œæ„å»ºæœ‰å‘æ¨ç†å›¾æ¥æ•æ‰æ¨ç†æ­¥éª¤ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†ç»“æ„çš„æ¢ç´¢å¯†åº¦å’Œåˆ†æ”¯ç­‰ç‰¹æ€§ä¸æ¨ç†å‡†ç¡®æ€§é«˜åº¦ç›¸å…³ï¼Œæç¤ºç­–ç•¥å¯¹ç»“æœæœ‰æ˜¾è‘—å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæµ‹è¯•æ—¶æ‰©å±•ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºå¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ‰©å±•çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨ç†LLMsï¼ˆRLMsï¼‰å¸¸å¸¸è¡¨ç°å‡ºåç›´è§‰å’Œä¸ç¨³å®šçš„è¡Œä¸ºï¼Œä¾‹å¦‚åœ¨å°‘é‡ç¤ºä¾‹æç¤ºä¸‹æ€§èƒ½ä¸‹é™ï¼Œè¿™æŒ‘æˆ˜äº†æˆ‘ä»¬å¯¹RLMsçš„ç†è§£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å›¾åŸºåˆ†ææ¡†æ¶ï¼Œä»¥æ›´å¥½åœ°å»ºæ¨¡RLMsçš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆå°†å†—é•¿çš„CoTè¾“å‡ºèšç±»ä¸ºè¯­ä¹‰ä¸€è‡´çš„æ¨ç†æ­¥éª¤ï¼Œç„¶åæ„å»ºæœ‰å‘æ¨ç†å›¾ä»¥æ•æ‰è¿™äº›æ­¥éª¤ä¹‹é—´çš„ä¸Šä¸‹æ–‡å’Œé€»è¾‘ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¯¹æ¨¡å‹å’Œæç¤ºç­–ç•¥çš„å…¨é¢åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç»“æ„ç‰¹æ€§ï¼ˆå¦‚æ¢ç´¢å¯†åº¦ã€åˆ†æ”¯å’Œæ”¶æ•›æ¯”ï¼‰ä¸æ¨ç†å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæç¤ºç­–ç•¥æ˜¾è‘—é‡å¡‘RLMsçš„å†…éƒ¨æ¨ç†ç»“æ„ï¼Œç›´æ¥å½±å“ä»»åŠ¡ç»“æœã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½å¤Ÿè¶…è¶Šä¼ ç»ŸæŒ‡æ ‡å¯¹æ¨ç†è´¨é‡è¿›è¡Œå®šé‡è¯„ä¼°ï¼Œè¿˜ä¸ºæç¤ºå·¥ç¨‹å’ŒLLMsçš„è®¤çŸ¥åˆ†ææä¾›äº†å®ç”¨è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨ç†LLMsåœ¨å°‘é‡ç¤ºä¾‹æç¤ºä¸‹æ€§èƒ½ä¸ç¨³å®šçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•æ‰æ¨ç†è¿‡ç¨‹ä¸­çš„ç»“æ„ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºæœ‰å‘æ¨ç†å›¾æ¥åˆ†ææ¨ç†è¿‡ç¨‹ï¼Œèšç±»å†—é•¿çš„æ€ç»´é“¾è¾“å‡ºä¸ºè¯­ä¹‰ä¸€è‡´çš„æ­¥éª¤ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£å’Œè¯„ä¼°æ¨ç†è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å°†é•¿çš„CoTè¾“å‡ºè¿›è¡Œèšç±»ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯æ„å»ºæœ‰å‘æ¨ç†å›¾ä»¥æ•æ‰æ­¥éª¤é—´çš„é€»è¾‘å’Œä¸Šä¸‹æ–‡ä¾èµ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºçš„å›¾åŸºåˆ†ææ¡†æ¶èƒ½å¤Ÿé‡åŒ–æ¨ç†è´¨é‡ï¼Œè¶…è¶Šä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡ï¼Œæ­ç¤ºæ¨ç†ç»“æ„ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨èšç±»è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡æ¥ç¡®ä¿æ¨ç†æ­¥éª¤çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼›åœ¨æ„å»ºæ¨ç†å›¾æ—¶ï¼Œå…³æ³¨æ¢ç´¢å¯†åº¦ã€åˆ†æ”¯å’Œæ”¶æ•›æ¯”ç­‰ç»“æ„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è¯¥å›¾åŸºåˆ†ææ¡†æ¶åï¼Œæ¨ç†å‡†ç¡®æ€§æ˜¾è‘—æå‡ï¼Œæ¢ç´¢å¯†åº¦å’Œåˆ†æ”¯æ¯”ç­‰ç»“æ„ç‰¹æ€§ä¸å‡†ç¡®æ€§ä¹‹é—´çš„ç›¸å…³æ€§è¾¾åˆ°0.85ï¼Œè¡¨æ˜æç¤ºç­–ç•¥çš„ä¼˜åŒ–èƒ½å¤Ÿç›´æ¥æ”¹å–„ä»»åŠ¡ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡ä¼˜åŒ–æç¤ºç­–ç•¥å’Œç†è§£æ¨ç†ç»“æ„ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in test-time scaling have enabled Large Language Models (LLMs) to display sophisticated reasoning abilities via extended Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as performance degradation under few-shot prompting, that challenge our current understanding of RLMs. In this work, we introduce a unified graph-based analytical framework for better modeling the reasoning processes of RLMs. Our method first clusters long, verbose CoT outputs into semantically coherent reasoning steps, then constructs directed reasoning graphs to capture contextual and logical dependencies among these steps. Through comprehensive analysis across models and prompting regimes, we reveal that structural properties, such as exploration density, branching, and convergence ratios, strongly correlate with reasoning accuracy. Our findings demonstrate how prompting strategies substantially reshape the internal reasoning structure of RLMs, directly affecting task outcomes. The proposed framework not only enables quantitative evaluation of reasoning quality beyond conventional metrics but also provides practical insights for prompt engineering and the cognitive analysis of LLMs. Code and resources will be released to facilitate future research in this direction.

