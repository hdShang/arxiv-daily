---
layout: default
title: "Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering"
---

# Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14099" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14099v2</a>
  <a href="https://arxiv.org/pdf/2505.14099.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14099v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14099v2', 'Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yihua Zhu, Qianying Liu, Akiko Aizawa, Hidetoshi Shimodaira

**åˆ†ç±»**: cs.CL, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-17)

**å¤‡æ³¨**: AAAI2026 Main Track

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPDRRæ¡†æ¶ä»¥è§£å†³å¤æ‚é—®ç­”ä¸­çš„çŸ¥è¯†åº“æ•´åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†åº“é—®ç­”` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æœºåˆ¶` `å¤æ‚é—®é¢˜å¤„ç†` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KBQAæ–¹æ³•åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶é¢ä¸´çŸ¥è¯†è¿‡æ—¶å’Œç¼ºä¹é€»è¾‘ç»“æ„çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºçš„PDRRæ¡†æ¶é€šè¿‡å››ä¸ªé˜¶æ®µæœ‰æ•ˆæ•´åˆäº†çŸ¥è¯†åº“ä¸LLMï¼Œæå‡äº†é—®ç­”çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPDRRåœ¨å¤šç§LLMåŸºç¡€ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚é—®é¢˜ä¸Šè¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†åº“é—®ç­”ï¼ˆKBQAï¼‰æ—¨åœ¨åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚å°½ç®¡ä»…ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•å…·æœ‰ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å­˜åœ¨çŸ¥è¯†è¿‡æ—¶ã€å¹»è§‰ç°è±¡å’Œç¼ºä¹é€æ˜æ€§ç­‰é—®é¢˜ã€‚é“¾å¼çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰æ–¹æ³•é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†åº“æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†ç”±äºç¼ºä¹è§„åˆ’å’Œé€»è¾‘ç»“æ„ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚é—®é¢˜ä¸Šçš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†PDRRæ¡†æ¶ï¼ŒåŒ…å«é¢„æµ‹ã€åˆ†è§£ã€æ£€ç´¢å’Œæ¨ç†å››ä¸ªé˜¶æ®µï¼Œèƒ½å¤Ÿå¤„ç†é“¾å¼å’Œéé“¾å¼å¤æ‚é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPDRRåœ¨å¤šç§LLMåŸºç¡€ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤æ‚é—®ç­”ä¸­çŸ¥è¯†åº“ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ•´åˆçš„éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶ï¼Œå¾€å¾€å› ç¼ºä¹æœ‰æ•ˆçš„é€»è¾‘ç»“æ„å’Œè§„åˆ’è€Œè¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPDRRæ¡†æ¶é€šè¿‡é¢„æµ‹é—®é¢˜ç±»å‹ã€åˆ†è§£é—®é¢˜ç»“æ„ã€æ£€ç´¢ç›¸å…³ä¿¡æ¯å’Œæ¨ç†æ¥å®Œæˆé—®ç­”ï¼Œæ—¨åœ¨æé«˜é—®ç­”çš„å‡†ç¡®æ€§å’Œé€»è¾‘æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPDRRæ¡†æ¶åˆ†ä¸ºå››ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„æµ‹ï¼ˆPredictï¼‰ã€åˆ†è§£ï¼ˆDecomposeï¼‰ã€æ£€ç´¢ï¼ˆRetrieveï¼‰å’Œæ¨ç†ï¼ˆReasonï¼‰ã€‚é¦–å…ˆï¼Œé¢„æµ‹é—®é¢˜ç±»å‹ï¼Œç„¶åå°†é—®é¢˜åˆ†è§£ä¸ºç»“æ„åŒ–çš„ä¸‰å…ƒç»„ï¼Œæ¥ç€ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œæœ€åå¼•å¯¼LLMè¿›è¡Œæ¨ç†å’Œè¡¥å…¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šPDRRçš„åˆ›æ–°åœ¨äºå…¶å››é˜¶æ®µçš„å¤„ç†æµç¨‹ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡åˆ†è§£é—®é¢˜ç»“æ„æ¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„é“¾å¼æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼ŒPDRRé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†åº“çš„ä¿¡æ¯ï¼Œå…·ä½“ç»†èŠ‚åŒ…æ‹¬å¦‚ä½•é€‰æ‹©æ£€ç´¢çš„çŸ¥è¯†æ¡ç›®å’Œå¦‚ä½•è®¾è®¡æ¨ç†è¿‡ç¨‹ä¸­çš„åé¦ˆæœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPDRRåœ¨å¤šç§LLMåŸºç¡€ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚é—®é¢˜ä¸Šè¡¨ç°çªå‡ºï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤„ç†éé“¾å¼é—®é¢˜ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å®¢æˆ·æœåŠ¡ã€æ•™è‚²è¾…å¯¼ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿåœ¨å¤æ‚é—®é¢˜ä¸Šçš„å›ç­”èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge Base Question Answering (KBQA) aims to answer natural language questions using structured knowledge from KBs. While LLM-only approaches offer generalization, they suffer from outdated knowledge, hallucinations, and lack of transparency. Chain-based KG-RAG methods address these issues by incorporating external KBs, but are limited to simple chain-structured questions due to the absence of planning and logical structuring. Inspired by semantic parsing methods, we propose PDRR: a four-stage framework consisting of Predict, Decompose, Retrieve, and Reason. Our method first predicts the question type and decomposes the question into structured triples. Then retrieves relevant information from KBs and guides the LLM as an agent to reason over and complete the decomposed triples. Experimental results demonstrate that PDRR consistently outperforms existing methods across various LLM backbones and achieves superior performance on both chain-structured and non-chain complex questions.

