---
layout: default
title: "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits"
---

# Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14178" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14178v1</a>
  <a href="https://arxiv.org/pdf/2505.14178.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14178v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14178v1', 'Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiang Zhang, Juntai Cao, Jiaqi Wei, Yiwei Xu, Chenyu You

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºToken Awarenessä»¥è§£å†³LLMsä¸­çš„ç¬¦å·æ¨ç†é™åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¬¦å·æ¨ç†` `Tokenization` `è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `ç®—æœ¯ä»»åŠ¡` `é€»è¾‘å¯¹é½` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„tokenizationæ–¹æ³•åœ¨ç¬¦å·æ¨ç†ä¸­å­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯å­è¯æ–¹æ³•å¯èƒ½å¯¼è‡´åŸå­æ¨ç†å•å…ƒçš„åˆå¹¶æˆ–æ¨¡ç³Šã€‚
2. è®ºæ–‡æå‡ºäº†Token Awarenessçš„æ¦‚å¿µï¼Œå¼ºè°ƒtokenç²’åº¦å¯¹é€»è¾‘å¯¹é½çš„é‡è¦æ€§ï¼Œä»è€Œå½±å“æ¨¡å‹çš„ç¬¦å·æ¨ç†èƒ½åŠ›ã€‚
3. é€šè¿‡ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°åŸå­å¯¹é½æ ¼å¼æ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½ï¼Œå°æ¨¡å‹åœ¨ç»“æ„åŒ–æ¨ç†ä¸­è¡¨ç°ä¼˜äºå¤§å‹æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Tokenizationæ˜¯è¯­è¨€æ¨¡å‹è®¡ç®—çš„ç¬¬ä¸€å±‚ï¼Œå°½ç®¡Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºèƒ½å¤Ÿé€šè¿‡å¤–éƒ¨åŒ–ä¸­é—´æ­¥éª¤æ¥è¿‘ä¼¼é€’å½’è®¡ç®—ï¼Œä½†æˆ‘ä»¬å±•ç¤ºäº†è¿™ç§æ¨ç†çš„æˆåŠŸåœ¨æ ¹æœ¬ä¸Šå—åˆ°æ ‡è®°è¾“å…¥ç»“æ„çš„é™åˆ¶ã€‚æœ¬æ–‡å¯¹tokenizationæ–¹æ¡ˆï¼Œå°¤å…¶æ˜¯åŸºäºå­è¯çš„æ–¹æ³•ï¼ˆå¦‚å­—èŠ‚å¯¹ç¼–ç BPEï¼‰å¦‚ä½•é€šè¿‡åˆå¹¶æˆ–æ¨¡ç³ŠåŸå­æ¨ç†å•å…ƒæ¥é˜»ç¢ç¬¦å·è®¡ç®—è¿›è¡Œäº†ç†è®ºå’Œå®è¯ç ”ç©¶ã€‚æˆ‘ä»¬å¼•å…¥äº†Token Awarenessçš„æ¦‚å¿µï¼Œä»¥å½¢å¼åŒ–ä¸è‰¯tokenç²’åº¦å¦‚ä½•ç ´åé€»è¾‘å¯¹é½å¹¶é˜»æ­¢æ¨¡å‹æ¨å¹¿ç¬¦å·è¿‡ç¨‹ã€‚é€šè¿‡å¯¹ç®—æœ¯å’Œç¬¦å·ä»»åŠ¡çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†tokenç»“æ„æ˜¾è‘—å½±å“æ¨ç†æ€§èƒ½ï¼Œå¯¼è‡´å³ä½¿åœ¨CoTä¸‹ä¹Ÿä¼šå¤±è´¥ï¼Œè€ŒåŸå­å¯¹é½æ ¼å¼åˆ™è§£é”å¼ºå¤§çš„æ¨å¹¿èƒ½åŠ›ï¼Œä½¿å¾—å°æ¨¡å‹ï¼ˆå¦‚GPT-4o-miniï¼‰åœ¨ç»“æ„åŒ–æ¨ç†ä¸­è¶…è¶Šæ›´å¤§ç³»ç»Ÿï¼ˆå¦‚o1ï¼‰ã€‚æˆ‘ä»¬çš„å‘ç°æ­ç¤ºäº†LLMsä¸­çš„ç¬¦å·æ¨ç†èƒ½åŠ›å¹¶éçº¯ç²¹ç”±æ¶æ„å†³å®šï¼Œè€Œæ˜¯æ·±å—tokençº§è¡¨ç¤ºçš„å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³äº†LLMsåœ¨ç¬¦å·æ¨ç†ä¸­ç”±äºtokenizationç»“æ„ä¸å½“è€Œå¯¼è‡´çš„æ€§èƒ½é™åˆ¶é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶å¸¸å¸¸å¤±è´¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºToken Awarenessçš„æ¦‚å¿µï¼Œå¼ºè°ƒtokenç²’åº¦å¯¹é€»è¾‘æ¨ç†çš„å½±å“ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–tokenç»“æ„æ¥æå‡æ¨¡å‹çš„ç¬¦å·æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯è¯„ä¼°ç›¸ç»“åˆçš„æ–¹å¼ï¼Œç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒtokenizationæ–¹æ¡ˆå¯¹æ¨ç†æ€§èƒ½çš„å½±å“ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬tokenç»“æ„åˆ†æã€æ¨ç†æ€§èƒ½è¯„ä¼°å’Œæ¨¡å‹å¯¹æ¯”å®éªŒã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥Token Awarenessæ¦‚å¿µï¼Œæ­ç¤ºäº†tokençº§è¡¨ç¤ºå¯¹ç¬¦å·æ¨ç†èƒ½åŠ›çš„æ·±è¿œå½±å“ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œå¼ºè°ƒäº†tokenç²’åº¦çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§tokenizationæ–¹æ¡ˆï¼ˆå¦‚BPEï¼‰è¿›è¡Œå¯¹æ¯”ï¼Œè®¾ç½®äº†ä¸åŒçš„ç²’åº¦å‚æ•°ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†æ€§èƒ½ã€‚å®éªŒä¸­è¿˜ä½¿ç”¨äº†ç®—æœ¯å’Œç¬¦å·ä»»åŠ¡æ¥éªŒè¯æ¨¡å‹çš„æ¨å¹¿èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨åŸå­å¯¹é½æ ¼å¼çš„å°æ¨¡å‹ï¼ˆå¦‚GPT-4o-miniï¼‰åœ¨ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ä¸­è¶…è¶Šäº†å¤§å‹æ¨¡å‹ï¼ˆå¦‚o1ï¼‰ï¼Œæå‡å¹…åº¦æ˜¾è‘—ï¼Œè¡¨æ˜tokenç»“æ„å¯¹æ¨ç†æ€§èƒ½çš„å½±å“æ˜¯æ·±è¿œçš„ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨ç®—æœ¯å’Œç¬¦å·ä»»åŠ¡ä¸­çš„è¡¨ç°æœ‰æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†Token Awarenessçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ•™è‚²æŠ€æœ¯å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–tokenizationæ–¹æ³•ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚æœªæ¥ï¼ŒToken Awarenessçš„æ¦‚å¿µå¯èƒ½ä¼šå¼•å¯¼æ›´é«˜æ•ˆçš„æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.

