---
layout: default
title: Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models
---

# Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14871" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14871v2</a>
  <a href="https://arxiv.org/pdf/2505.14871.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14871v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14871v2', 'Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ryan Solgi, Kai Zhen, Rupak Vignesh Swaminathan, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-13)

**å¤‡æ³¨**: Accepted to EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSatenä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ é‡å‹ç¼©` `ç¨€ç–æ€§` `å¾®è°ƒ` `ä½ç§©å¼ é‡åŒ–` `æ¨¡å‹ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä½ç§©å¼ é‡å‹ç¼©æŠ€æœ¯åœ¨å‹ç¼©é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´é«˜ç§©ç‰¹æ€§å’Œç¼ºä¹é¢„è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„ç¨€ç–å¢å¼ºå¼ é‡ç½‘ç»œï¼ˆSatenï¼‰é€šè¿‡åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–ä½ç§©å¼ é‡åŒ–LLMsï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSatenåœ¨å‡†ç¡®æ€§å’Œå‹ç¼©æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„æ€§èƒ½æ ‡å‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šé«˜æ•ˆå®ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡³å…³é‡è¦ã€‚å°½ç®¡ä½ç§©å¼ é‡å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚å¼ é‡è®­ç»ƒç½‘ç»œï¼‰åœ¨è¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œä¸­å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†å°†å…¶åº”ç”¨äºå‹ç¼©é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†ä½ç§©å¼ é‡åŒ–LLMsåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ç¨€ç–å¢å¼ºå¼ é‡ç½‘ç»œï¼ˆSatenï¼‰ä»¥æå‡å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSatenåœ¨å¼ é‡åŒ–è¯­è¨€æ¨¡å‹ä¸­æé«˜äº†å‡†ç¡®æ€§å’Œå‹ç¼©æ•ˆç‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åœ¨ä¸è®¿é—®é¢„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æœ‰æ•ˆå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„é—®é¢˜ã€‚ç°æœ‰çš„ä½ç§©å¼ é‡å‹ç¼©æ–¹æ³•åœ¨å¤„ç†é«˜ç§©çš„é¢„è®­ç»ƒæ¨¡å‹æ—¶æ•ˆæœä¸ä½³ï¼Œå¯¼è‡´å‹ç¼©æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„Satenæ¡†æ¶é€šè¿‡å¼•å…¥ç¨€ç–æ€§å’Œå¢å¼ºæœºåˆ¶ï¼Œä¼˜åŒ–äº†ä½ç§©å¼ é‡åŒ–çš„è¿‡ç¨‹ï¼Œä½¿å¾—åœ¨å¾®è°ƒé˜¶æ®µèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSatenæ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ä½ç§©å¼ é‡åŒ–ã€ç¨€ç–å¢å¼ºå’Œå¾®è°ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹æ¨¡å‹è¿›è¡Œå¼ é‡åŒ–ï¼Œç„¶åé€šè¿‡ç¨€ç–æ€§çº¦æŸè¿›è¡Œå¢å¼ºï¼Œæœ€ååœ¨å¾®è°ƒé˜¶æ®µè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSatençš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†ç¨€ç–æ€§ä¸ä½ç§©å¼ é‡åŒ–çš„ä¼˜åŠ¿ï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å‹ç¼©ç­–ç•¥ã€‚è¿™ä¸€ç­–ç•¥åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†å‹ç¼©æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Satenä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡å‹ç¼©ç‡ä¸æ¨¡å‹æ€§èƒ½ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§è°ƒæ•´çš„è¶…å‚æ•°ï¼Œä»¥ä¼˜åŒ–ç¨€ç–æ€§å’Œä½ç§©ç‰¹æ€§ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSatenåœ¨å¼ é‡åŒ–è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ä¸Šæå‡äº†5%ï¼ŒåŒæ—¶å‹ç¼©ç‡æé«˜äº†30%ã€‚ä¸ä¼ ç»Ÿçš„ä½ç§©å¼ é‡å‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼ŒSatenè¾¾åˆ°äº†æ›´ä¼˜çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—å’ŒåµŒå…¥å¼ç³»ç»Ÿç­‰èµ„æºå—é™ç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡æå‡æ¨¡å‹çš„å‹ç¼©æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒSatenä¸ºå®é™…åº”ç”¨æä¾›äº†æ›´ä¸ºçµæ´»å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The efficient implementation of large language models (LLMs) is crucial for deployment on resource-constrained devices. Low-rank tensor compression techniques, such as tensor-train (TT) networks, have been widely studied for over-parameterized neural networks. However, their applications to compress pre-trained large language models (LLMs) for downstream tasks (post-training) remains challenging due to the high-rank nature of pre-trained LLMs and the lack of access to pretraining data. In this study, we investigate low-rank tensorized LLMs during fine-tuning and propose sparse augmented tensor networks (Saten) to enhance their performance. The proposed Saten framework enables full model compression. Experimental results demonstrate that Saten enhances both accuracy and compression efficiency in tensorized language models, achieving state-of-the-art performance.

