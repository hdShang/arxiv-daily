---
layout: default
title: Cross-Lingual Optimization for Language Transfer in Large Language Models
---

# Cross-Lingual Optimization for Language Transfer in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14297" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14297v1</a>
  <a href="https://arxiv.org/pdf/2505.14297.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14297v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14297v1', 'Cross-Lingual Optimization for Language Transfer in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jungseob Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: Accepted for publication at ACL 2025. Jungseob Lee and Seongtae Hong contributed equally to this work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨è¯­è¨€ä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹è¯­è¨€è¿ç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨è¯­è¨€ä¼˜åŒ–` `è¯­è¨€è¿ç§»` `å¤§è¯­è¨€æ¨¡å‹` `ç›‘ç£å¾®è°ƒ` `ä½èµ„æºè¯­è¨€` `æœºå™¨ç¿»è¯‘` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œè¿‡äºä¾èµ–è‹±è¯­æ•°æ®ï¼Œå¯¼è‡´ç›®æ ‡è¯­è¨€èƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºçš„è·¨è¯­è¨€ä¼˜åŒ–ï¼ˆCLOï¼‰æ–¹æ³•é€šè¿‡åˆ©ç”¨è‹±è¯­SFTæ•°æ®å’Œç¿»è¯‘æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆçš„è¯­è¨€è¿ç§»ï¼Œä¿æŒäº†è‹±è¯­èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLOåœ¨å¤šç§è¯­è¨€ä¸Šå‡ä¼˜äºSFTï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­ï¼Œæ•°æ®éœ€æ±‚æ˜¾è‘—é™ä½ï¼Œæ€§èƒ½æå‡æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”å…¶ä»–è¯­è¨€æ—¶ï¼Œé€šå¸¸é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºæ ‡å‡†æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¾€å¾€è¿‡äºå¼ºè°ƒè‹±è¯­æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ•°æ®å—é™çš„ç¯å¢ƒä¸­æ›´ä¸ºæ˜æ˜¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è·¨è¯­è¨€ä¼˜åŒ–ï¼ˆCLOï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°å°†ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„è¯­è¨€æ¨¡å‹è¿ç§»åˆ°ç›®æ ‡è¯­è¨€ï¼ŒåŒæ—¶ä¿æŒå…¶è‹±è¯­èƒ½åŠ›ã€‚CLOåˆ©ç”¨å…¬å¼€çš„è‹±è¯­SFTæ•°æ®å’Œç¿»è¯‘æ¨¡å‹å®ç°è·¨è¯­è¨€è¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLOåœ¨è·å–ç›®æ ‡è¯­è¨€èƒ½åŠ›å’Œä¿æŒè‹±è¯­æ€§èƒ½æ–¹é¢å‡ä¼˜äºSFTï¼Œå°¤å…¶åœ¨ä½èµ„æºè¯­è¨€ä¸­ï¼ŒCLOä»…ä½¿ç”¨3200ä¸ªæ ·æœ¬ä¾¿è¶…è¶Šäº†ä½¿ç”¨6400ä¸ªæ ·æœ¬çš„SFTï¼Œæ˜¾ç¤ºå‡ºCLOåœ¨æ•°æ®åˆ©ç”¨ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­ï¼Œç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼ˆSFTï¼‰å¯¹è‹±è¯­æ€§èƒ½çš„è¿‡åº¦ä¾èµ–ï¼Œå¯¼è‡´ç›®æ ‡è¯­è¨€èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè·¨è¯­è¨€ä¼˜åŒ–ï¼ˆCLOï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè‹±è¯­SFTæ•°æ®å’Œç¿»è¯‘æ¨¡å‹ï¼Œä¿ƒè¿›è‹±è¯­ä¸ç›®æ ‡è¯­è¨€ä¹‹é—´çš„æœ‰æ•ˆè¿ç§»ï¼ŒåŒæ—¶ä¿æŒè‹±è¯­èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡é˜¶æ®µï¼ˆåˆ©ç”¨å…¬å¼€çš„è‹±è¯­SFTæ•°æ®ï¼‰ã€ç¿»è¯‘æ¨¡å‹çš„æ„å»ºå’Œè®­ç»ƒé˜¶æ®µï¼Œä»¥åŠè·¨è¯­è¨€è¿ç§»çš„å®æ–½é˜¶æ®µï¼Œç¡®ä¿ç›®æ ‡è¯­è¨€çš„å­¦ä¹ ä¸è‹±è¯­èƒ½åŠ›çš„ä¿ç•™ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLOçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨ä½èµ„æºè¯­è¨€ä¸­ä»¥è¾ƒå°‘çš„æ•°æ®å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œå…‹æœäº†SFTå¯¹æ•°æ®é‡çš„æ•æ„Ÿæ€§ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CLOä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬æ ·æœ¬æ•°é‡çš„ä¼˜åŒ–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä»¥å¹³è¡¡è‹±è¯­å’Œç›®æ ‡è¯­è¨€çš„å­¦ä¹ ï¼Œä»¥åŠç½‘ç»œç»“æ„çš„é€‰æ‹©ä»¥é€‚åº”ä¸åŒè¯­è¨€çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCLOåœ¨å…­ç§è¯­è¨€ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„SFTæ–¹æ³•ã€‚åœ¨ä½èµ„æºè¯­è¨€ä¸­ï¼ŒCLOä»…ä½¿ç”¨3200ä¸ªæ ·æœ¬ä¾¿è¶…è¶Šäº†ä½¿ç”¨6400ä¸ªæ ·æœ¬çš„SFTï¼Œæ˜¾ç¤ºå‡ºåœ¨æ•°æ®åˆ©ç”¨æ•ˆç‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œè·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜ä½èµ„æºè¯­è¨€çš„å¤„ç†èƒ½åŠ›ï¼ŒCLOæ–¹æ³•èƒ½å¤Ÿä¿ƒè¿›å…¨çƒèŒƒå›´å†…çš„ä¿¡æ¯è·å–ä¸äº¤æµï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Adapting large language models to other languages typically employs supervised fine-tuning (SFT) as a standard approach. However, it often suffers from an overemphasis on English performance, a phenomenon that is especially pronounced in data-constrained environments. To overcome these challenges, we propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an English-centric LLM to a target language while preserving its English capabilities. CLO utilizes publicly available English SFT data and a translation model to enable cross-lingual transfer. We conduct experiments using five models on six languages, each possessing varying levels of resource. Our results show that CLO consistently outperforms SFT in both acquiring target language proficiency and maintaining English performance. Remarkably, in low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400 samples, demonstrating that CLO can achieve better performance with less data. Furthermore, we find that SFT is particularly sensitive to data quantity in medium and low-resource languages, whereas CLO remains robust. Our comprehensive analysis emphasizes the limitations of SFT and incorporates additional training strategies in CLO to enhance efficiency.

