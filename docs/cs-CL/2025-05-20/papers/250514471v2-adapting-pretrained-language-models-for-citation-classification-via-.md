---
layout: default
title: Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning
---

# Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14471" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14471v2</a>
  <a href="https://arxiv.org/pdf/2505.14471.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14471v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14471v2', 'Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tong Li, Jiachuan Wang, Yongqi Zhang, Shuangyin Li, Lei Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-28)

**å¤‡æ³¨**: Accepted to KDD 2025. This is the author's version of the work

**DOI**: [10.1145/3711896.3736829](https://doi.org/10.1145/3711896.3736829)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCitssæ¡†æ¶ä»¥è§£å†³å­¦æœ¯å¼•ç”¨åˆ†ç±»ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼•ç”¨åˆ†ç±»` `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹` `å­¦æœ¯åˆ†æ` `æ•°æ®ç¨€ç¼º` `å…³é”®è¯æ‰°åŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¼•ç”¨åˆ†ç±»ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºã€ä¸Šä¸‹æ–‡å™ªå£°å’Œè™šå‡å…³é”®è¯ç›¸å…³æ€§ç­‰æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ€§èƒ½æå‡ã€‚
2. è®ºæ–‡æå‡ºCitssæ¡†æ¶ï¼Œåˆ©ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ç¼“è§£æ•°æ®ç¨€ç¼ºï¼Œå¹¶é€šè¿‡å¥å­çº§è£å‰ªå’Œå…³é”®è¯æ‰°åŠ¨è·å–å¯¹æ¯”å¯¹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCitssåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†ä»¥å¾€çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­¦æœ¯å¼•ç”¨åˆ†ç±»æ—¨åœ¨è¯†åˆ«å­¦æœ¯å¼•ç”¨èƒŒåçš„æ„å›¾ï¼Œå¯¹å­¦æœ¯åˆ†æè‡³å…³é‡è¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é€šè¿‡åœ¨å¼•ç”¨åˆ†ç±»æ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ï¼Œåˆ©ç”¨å…¶åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—çš„è¯­è¨€çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç›´æ¥å¾®è°ƒé¢ä¸´æ ‡ç­¾æ•°æ®ç¨€ç¼ºã€ä¸Šä¸‹æ–‡å™ªå£°å’Œè™šå‡å…³é”®è¯ç›¸å…³æ€§ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶Citssï¼Œé€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶é‡‡ç”¨å¥å­çº§è£å‰ªå’Œå…³é”®è¯æ‰°åŠ¨ä¸¤ç§ç­–ç•¥æ¥è·å–å¯¹æ¯”å¯¹ã€‚ä¸ä»…é’ˆå¯¹ç¼–ç å™¨åŸºç¡€PLMsçš„ä»¥å¾€å·¥ä½œç›¸æ¯”ï¼ŒCitsså…¼å®¹ç¼–ç å™¨å’Œè§£ç å™¨åŸºç¡€çš„PLMsï¼Œå……åˆ†åˆ©ç”¨æ‰©å±•é¢„è®­ç»ƒçš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCitssåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å­¦æœ¯å¼•ç”¨åˆ†ç±»ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶é¢ä¸´æ ‡ç­¾æ•°æ®ä¸è¶³å’Œä¸Šä¸‹æ–‡å™ªå£°çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCitssæ¡†æ¶é€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œåˆ©ç”¨å¥å­çº§è£å‰ªå’Œå…³é”®è¯æ‰°åŠ¨ç­–ç•¥æ¥ç”Ÿæˆå¯¹æ¯”å¯¹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCitssçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å¯¹æ¯”å¯¹ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µè¿›è¡Œå¥å­è£å‰ªå’Œå…³é”®è¯æ‰°åŠ¨ï¼Œä»¥ç”Ÿæˆæœ‰æ•ˆçš„å¯¹æ¯”å¯¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šCitssçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œå¹¶è®¾è®¡äº†é€‚ç”¨äºç¼–ç å™¨å’Œè§£ç å™¨åŸºç¡€PLMsçš„è®­ç»ƒç­–ç•¥ï¼Œçªç ´äº†ä»¥å¾€æ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒç±»å‹çš„PLMsã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCitssåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ŒéªŒè¯äº†å…¶åœ¨å¼•ç”¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å­¦æœ¯æœç´¢å¼•æ“ã€æ–‡çŒ®æ¨èç³»ç»Ÿå’Œç§‘ç ”ç®¡ç†å·¥å…·ã€‚é€šè¿‡æé«˜å¼•ç”¨åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå­¦æœ¯ç ”ç©¶å’ŒçŸ¥è¯†å‘ç°ï¼Œæœªæ¥å¯èƒ½å¯¹å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Citation classification, which identifies the intention behind academic citations, is pivotal for scholarly analysis. Previous works suggest fine-tuning pretrained language models (PLMs) on citation classification datasets, reaping the reward of the linguistic knowledge they gained during pretraining. However, directly fine-tuning for citation classification is challenging due to labeled data scarcity, contextual noise, and spurious keyphrase correlations. In this paper, we present a novel framework, Citss, that adapts the PLMs to overcome these challenges. Citss introduces self-supervised contrastive learning to alleviate data scarcity, and is equipped with two specialized strategies to obtain the contrastive pairs: sentence-level cropping, which enhances focus on target citations within long contexts, and keyphrase perturbation, which mitigates reliance on specific keyphrases. Compared with previous works that are only designed for encoder-based PLMs, Citss is carefully developed to be compatible with both encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged pretraining. Experiments with three benchmark datasets with both encoder-based PLMs and decoder-based LLMs demonstrate our superiority compared to the previous state of the art. Our code is available at: github.com/LITONG99/Citss

