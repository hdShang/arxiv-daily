---
layout: default
title: Teaching Small Language Models to Learn Logic through Meta-Learning
---

# Teaching Small Language Models to Learn Logic through Meta-Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14313" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14313v2</a>
  <a href="https://arxiv.org/pdf/2505.14313.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14313v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14313v2', 'Teaching Small Language Models to Learn Logic through Meta-Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Leonardo Bertolazzi, Manuel Vargas GuzmÃ¡n, Raffaella Bernardi, Maciej Malicki, Jakub Szymanik

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å…ƒå­¦ä¹ æå‡å°å‹è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€»è¾‘æ¨ç†` `å…ƒå­¦ä¹ ` `å°å‹è¯­è¨€æ¨¡å‹` `ä¸‰æ®µè®º` `çŸ¥è¯†è¿ç§»` `å°‘æ ·æœ¬å­¦ä¹ ` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†éªŒè¯ï¼Œå°¤å…¶æ˜¯åœ¨ä¸‰æ®µè®ºæ¨ç†æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å°‘æ ·æœ¬å…ƒå­¦ä¹ çš„æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæå–è·¨ä»»åŠ¡çš„æ¨ç†è§„åˆ™ï¼Œä»è€Œæå‡é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å…ƒå­¦ä¹ å¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨ä¸‰æ®µè®ºæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æ•ˆæœæ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†å…¶é€»è¾‘èƒ½åŠ›ä»å­˜åœ¨äº‰è®®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶äº†LLMsåœ¨ä¸€ä¸ªæ˜ç¡®çš„é€»è¾‘ç‰‡æ®µä¸­çš„æ¨ç†èƒ½åŠ›ï¼šä¸‰æ®µè®ºæ¨ç†ã€‚æˆ‘ä»¬å°†è¯¥é—®é¢˜è§†ä¸ºå‰æé€‰æ‹©ï¼Œå¹¶æ„å»ºå—æ§æ•°æ®é›†ä»¥éš”ç¦»é€»è¾‘èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºåœ¨è¯¥é¢†åŸŸåº”ç”¨å°‘æ ·æœ¬å…ƒå­¦ä¹ ï¼Œé¼“åŠ±æ¨¡å‹æå–è·¨ä»»åŠ¡çš„è§„åˆ™ï¼Œè€Œéä»…ä»…è®°å¿†ä»»åŠ¡å†…çš„æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å…ƒå­¦ä¹ å¾®è°ƒçš„å°å‹æ¨¡å‹ï¼ˆ1.5B-7Bï¼‰åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹è¡¨ç°çªå‡ºã€‚è¿™äº›å…ƒå­¦ä¹ æ¨¡å‹åœ¨æˆ‘ä»¬çš„ä¸‰æ®µè®ºæ¨ç†ä»»åŠ¡ä¸­è¶…è¶Šäº†GPT-4oå’Œo3-miniã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ï¼Œç‰¹åˆ«æ˜¯ä¸‰æ®µè®ºæ¨ç†ä¸­çš„èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ–°ç»“æ„ä¸Šçš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºå°†å°‘æ ·æœ¬å…ƒå­¦ä¹ åº”ç”¨äºé€»è¾‘æ¨ç†é¢†åŸŸï¼Œé¼“åŠ±æ¨¡å‹ä»å¤šä¸ªä»»åŠ¡ä¸­æå–é€šç”¨æ¨ç†è§„åˆ™ï¼Œè€Œéä»…ä»…è®°å¿†ç‰¹å®šä»»åŠ¡çš„æ¨¡å¼ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æé«˜æ¨¡å‹åœ¨é¢å¯¹æ–°é¢–æ¨ç†ç»“æ„æ—¶çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€å…ƒå­¦ä¹ ç®—æ³•è®¾è®¡å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ„å»ºå—æ§æ•°æ®é›†ä»¥éš”ç¦»é€»è¾‘èƒ½åŠ›ï¼›å…¶æ¬¡ï¼Œè®¾è®¡å…ƒå­¦ä¹ ç®—æ³•ä»¥ä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼›æœ€åï¼Œé€šè¿‡å¾®è°ƒå°å‹æ¨¡å‹å®ç°é€»è¾‘æ¨ç†èƒ½åŠ›çš„æå‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å…ƒå­¦ä¹ å¼•å…¥é€»è¾‘å­¦ä¹ çš„ç ”ç©¶ä¸­ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸçš„ç©ºç™½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ›´æ³¨é‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œéå•ä¸€ä»»åŠ¡çš„è®°å¿†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¿ƒè¿›æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´çš„çŸ¥è¯†è¿ç§»ï¼ŒåŒæ—¶è°ƒæ•´äº†ç½‘ç»œç»“æ„ä»¥é€‚åº”å°‘æ ·æœ¬å­¦ä¹ çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å…ƒå­¦ä¹ å¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨ä¸‰æ®µè®ºæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†GPT-4oå’Œo3-miniï¼Œå°¤å…¶åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹ï¼Œæ³›åŒ–èƒ½åŠ›æå‡æ˜¾è‘—ï¼Œå±•ç¤ºäº†å…ƒå­¦ä¹ åœ¨é€»è¾‘æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ³•å¾‹æ¨ç†ã€äººå·¥æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æå‡å°å‹è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æ”¯æŒå¤æ‚å†³ç­–å’Œæ¨ç†ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are increasingly evaluated on reasoning tasks, yet their logical abilities remain contested. To address this, we study LLMs' reasoning in a well-defined fragment of logic: syllogistic reasoning. We cast the problem as premise selection and construct controlled datasets to isolate logical competence. Beyond evaluation, an open challenge is enabling LLMs to acquire abstract inference patterns that generalize to novel structures. We propose to apply few-shot meta-learning to this domain, thereby encouraging models to extract rules across tasks rather than memorize patterns within tasks. Although meta-learning has been little explored in the context of logic learnability, our experiments show that it is effective: small models (1.5B-7B) fine-tuned with meta-learning demonstrate strong gains in generalization, with especially pronounced benefits in low-data regimes. These meta-learned models outperform GPT-4o and o3-mini on our syllogistic reasoning task.

