---
layout: default
title: "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance"
---

# MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14483" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14483v2</a>
  <a href="https://arxiv.org/pdf/2505.14483.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14483v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14483v2', 'MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Agam Goyal, Xianyang Zhan, Yilun Chen, Koustuv Saha, Eshwar Chandrasekharan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-23)

**å¤‡æ³¨**: EMNLP 2025 (Oral)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoMoEæ¡†æ¶ä»¥è§£å†³åœ¨çº¿ç¤¾åŒºå†…å®¹å®¡æ ¸é€æ˜æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å†…å®¹å®¡æ ¸` `åœ¨çº¿ç¤¾åŒº` `é€æ˜æ€§` `äººå·¥æ™ºèƒ½æ²»ç†` `è¯­è¨€æ¨¡å‹` `æ¨¡å—åŒ–è®¾è®¡` `ä¸“å®¶æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å†…å®¹å®¡æ ¸æ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªç¤¾åŒºå•ç‹¬è®­ç»ƒæ¨¡å‹ï¼Œä¸”å†³ç­–è¿‡ç¨‹ç¼ºä¹é€æ˜æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„MoMoEæ¡†æ¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œç»“åˆå¤šä¸ªä¸“å®¶æ¨¡å‹ï¼Œå®ç°è·¨ç¤¾åŒºçš„å¯æ‰©å±•å†…å®¹å®¡æ ¸ï¼Œå¹¶æä¾›äº‹åè§£é‡Šã€‚
3. åœ¨å®éªŒä¸­ï¼ŒMoMoEåœ¨30ä¸ªæœªè§çš„å­ç‰ˆå—ä¸Šå–å¾—äº†é«˜è¾¾0.72çš„Micro-F1åˆ†æ•°ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒåŸºçº¿ï¼ŒåŒæ—¶æä¾›å¯é çš„è§£é‡Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ ‡è®°åœ¨çº¿ç¤¾åŒºä¸­çš„æœ‰å®³å†…å®¹æ–¹é¢å±•ç°äº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å®¡æ ¸æ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªç¤¾åŒºå•ç‹¬è®­ç»ƒæ¨¡å‹ï¼Œä¸”å†³ç­–è¿‡ç¨‹ä¸é€æ˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†æ··åˆå®¡æ ¸ä¸“å®¶æ¡†æ¶ï¼ˆMoMoEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„è·¨ç¤¾åŒºæ¡†æ¶ï¼Œèƒ½å¤Ÿä¸ºå¯æ‰©å±•çš„å†…å®¹å®¡æ ¸æä¾›äº‹åè§£é‡Šã€‚MoMoEåè°ƒå››ä¸ªæ“ä½œç¬¦â€”â€”åˆ†é…ã€é¢„æµ‹ã€èšåˆå’Œè§£é‡Šï¼Œå¹¶å®ä¾‹åŒ–ä¸ºä¸ƒä¸ªç¤¾åŒºä¸“ç”¨ä¸“å®¶ï¼ˆMoMoE-Communityï¼‰å’Œäº”ä¸ªè§„èŒƒè¿è§„ä¸“å®¶ï¼ˆMoMoE-NormVioï¼‰ã€‚åœ¨30ä¸ªæœªè§çš„å­ç‰ˆå—ä¸Šï¼Œæœ€ä½³å˜ä½“åˆ†åˆ«è·å¾—äº†0.72å’Œ0.67çš„Micro-F1åˆ†æ•°ï¼ŒåŒ¹é…æˆ–è¶…è¶Šäº†å¼ºå¤§çš„å¾®è°ƒåŸºçº¿ï¼ŒåŒæ—¶å§‹ç»ˆäº§ç”Ÿç®€æ´å¯é çš„è§£é‡Šã€‚å°½ç®¡ç¤¾åŒºä¸“ç”¨ä¸“å®¶æä¾›äº†æœ€é«˜çš„å³°å€¼å‡†ç¡®ç‡ï¼Œä½†è§„èŒƒè¿è§„ä¸“å®¶åœ¨å„ä¸ªé¢†åŸŸæä¾›äº†æ›´ç¨³å®šçš„è¡¨ç°ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒMoMoEèƒ½å¤Ÿå®ç°å¯æ‰©å±•ã€é€æ˜çš„å®¡æ ¸ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªç¤¾åŒºè¿›è¡Œå¾®è°ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å†…å®¹å®¡æ ¸æ–¹æ³•åœ¨é€æ˜æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯æ¯ä¸ªç¤¾åŒºéœ€è¦å•ç‹¬æ¨¡å‹çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoMoEæ¡†æ¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œæ•´åˆå¤šä¸ªä¸“å®¶æ¨¡å‹ï¼Œæä¾›è·¨ç¤¾åŒºçš„å†…å®¹å®¡æ ¸è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨å®¡æ ¸åæä¾›å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoMoEæ¡†æ¶åŒ…æ‹¬å››ä¸ªä¸»è¦æ“ä½œç¬¦ï¼šåˆ†é…ï¼ˆAllocateï¼‰ã€é¢„æµ‹ï¼ˆPredictï¼‰ã€èšåˆï¼ˆAggregateï¼‰å’Œè§£é‡Šï¼ˆExplainï¼‰ï¼Œå¹¶é€šè¿‡ç¤¾åŒºä¸“ç”¨ä¸“å®¶å’Œè§„èŒƒè¿è§„ä¸“å®¶çš„ç»„åˆå®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoMoEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡å’Œè·¨ç¤¾åŒºçš„ä¸“å®¶ç»„åˆï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦é’ˆå¯¹æ¯ä¸ªç¤¾åŒºå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜æ•ˆä¸”é€æ˜çš„å†…å®¹å®¡æ ¸ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMoMoEä½¿ç”¨äº†ä¸ƒä¸ªç¤¾åŒºä¸“ç”¨ä¸“å®¶å’Œäº”ä¸ªè§„èŒƒè¿è§„ä¸“å®¶ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒç¤¾åŒºå’Œè§„èŒƒè¿è§„åœºæ™¯ä¸‹çš„ç¨³å®šæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒMoMoEåœ¨30ä¸ªæœªè§çš„å­ç‰ˆå—ä¸Šå–å¾—äº†Micro-F1åˆ†æ•°0.72å’Œ0.67ï¼Œåˆ†åˆ«å¯¹åº”ç¤¾åŒºä¸“ç”¨ä¸“å®¶å’Œè§„èŒƒè¿è§„ä¸“å®¶ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å¾®è°ƒåŸºçº¿ï¼Œä¸”å§‹ç»ˆæä¾›ç®€æ´å¯é çš„è§£é‡Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å†…å®¹å®¡æ ¸ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å¹³å°ã€åœ¨çº¿è®ºå›å’Œä»»ä½•éœ€è¦å†…å®¹å®¡æ ¸çš„ç¤¾åŒºã€‚é€šè¿‡æä¾›é€æ˜çš„å®¡æ ¸è¿‡ç¨‹ï¼ŒMoMoEèƒ½å¤Ÿå¢å¼ºç”¨æˆ·å¯¹å¹³å°çš„ä¿¡ä»»ï¼Œå¹¶ä¿ƒè¿›æ›´å¥åº·çš„åœ¨çº¿äº¤æµç¯å¢ƒã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶çš„è®¾è®¡ç†å¿µä¹Ÿå¯èƒ½å½±å“å…¶ä»–é¢†åŸŸçš„äººå·¥æ™ºèƒ½æ²»ç†ç ”ç©¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have shown great potential in flagging harmful content in online communities. Yet, existing approaches for moderation require a separate model for every community and are opaque in their decision-making, limiting real-world adoption. We introduce Mixture of Moderation Experts (MoMoE), a modular, cross-community framework that adds post-hoc explanations to scalable content moderation. MoMoE orchestrates four operators -- Allocate, Predict, Aggregate, Explain -- and is instantiated as seven community-specialized experts (MoMoE-Community) and five norm-violation experts (MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1 scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned baselines while consistently producing concise and reliable explanations. Although community-specialized experts deliver the highest peak accuracy, norm-violation experts provide steadier performance across domains. These findings show that MoMoE yields scalable, transparent moderation without needing per-community fine-tuning. More broadly, they suggest that lightweight, explainable expert ensembles can guide future NLP and HCI research on trustworthy human-AI governance of online communities.

