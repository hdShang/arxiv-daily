---
layout: default
title: Large Language Models Implicitly Learn to See and Hear Just By Reading
---

# Large Language Models Implicitly Learn to See and Hear Just By Reading

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.17091" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.17091v2</a>
  <a href="https://arxiv.org/pdf/2505.17091.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.17091v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.17091v2', 'Large Language Models Implicitly Learn to See and Hear Just By Reading')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prateek Verma, Mert Pilanci

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV, cs.LG, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-09-23)

**å¤‡æ³¨**: 6 pages, 3 figures, 4 tables. Added BLIP reference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡æ–‡æœ¬è®­ç»ƒå®ç°è§†è§‰ä¸å¬è§‰ç†è§£çš„é•¿è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `éŸ³é¢‘åˆ†ç±»` `å›¾åƒåˆ†ç±»` `è‡ªå›å½’æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œå¯¼è‡´è®­ç»ƒæˆæœ¬é«˜ä¸”æ•ˆç‡ä½ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¶æ„é€šè¿‡ç›´æ¥è¾“å…¥å›¾åƒå’ŒéŸ³é¢‘æ•°æ®ï¼Œåˆ©ç”¨æ–‡æœ¬æ¨¡å‹çš„å†…éƒ¨æƒé‡è¿›è¡Œåˆ†ç±»ï¼Œé¿å…äº†ä»å¤´è®­ç»ƒçš„éœ€æ±‚ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å‹çš„æƒé‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†éŸ³é¢‘å’Œå›¾åƒåˆ†ç±»çš„æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å±•ç¤ºäº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„å‘ç°ï¼šé€šè¿‡å¯¹è‡ªå›å½’é•¿è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ–‡æœ¬æ ‡è®°çš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨å†…éƒ¨è‡ªç„¶è€Œç„¶åœ°å‘å±•å‡ºç†è§£å›¾åƒå’ŒéŸ³é¢‘çš„èƒ½åŠ›ï¼Œä»è€Œä»…é€šè¿‡é˜…è¯»ä¾¿èƒ½â€œçœ‹â€å’Œâ€œå¬â€ã€‚æµè¡Œçš„éŸ³é¢‘å’Œè§†è§‰LLMæ¨¡å‹é€šå¸¸éœ€è¦å¯¹æ–‡æœ¬LLMæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¾¿æ ¹æ®å›¾åƒå’ŒéŸ³é¢‘åµŒå…¥ç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚è€Œæˆ‘ä»¬çš„æ¶æ„åˆ™ç›´æ¥æ¥æ”¶å›¾åƒå—ã€éŸ³é¢‘æ³¢å½¢æˆ–æ ‡è®°ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå…¸å‹äºåˆ†ç±»ç®¡é“çš„åµŒå…¥æˆ–ç±»åˆ«æ ‡ç­¾ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ–‡æœ¬æƒé‡åœ¨éŸ³é¢‘åˆ†ç±»ï¼ˆFSD-50Kå’ŒGTZANæ•°æ®é›†ï¼‰ä¸­çš„æ™®éæ€§ï¼Œå¹¶è¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨CIFAR-10å’ŒFashion-MNISTå›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€å‘ç°æ¨åŠ¨äº†æ–‡æœ¬LLMå­¦ä¹ å¼ºå¤§å†…éƒ¨ç”µè·¯çš„æ¦‚å¿µï¼Œå¯ä»¥é€šè¿‡æ¿€æ´»å¿…è¦çš„è¿æ¥æ¥åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œè€Œæ— éœ€æ¯æ¬¡ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šéœ€è¦å¤§é‡å¾®è°ƒçš„é—®é¢˜ï¼Œè¿™ä¸ä»…å¢åŠ äº†è®­ç»ƒæ—¶é—´ï¼Œè¿˜é™ä½äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªå›å½’é•¿è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æƒé‡ï¼Œç›´æ¥å¤„ç†å›¾åƒå’ŒéŸ³é¢‘è¾“å…¥ï¼Œä»è€Œå®ç°å¯¹è¿™äº›æ¨¡æ€çš„ç†è§£ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ¨¡å—ï¼ˆæ¥æ”¶å›¾åƒå—å’ŒéŸ³é¢‘æ³¢å½¢ï¼‰ã€ç‰¹å¾æå–æ¨¡å—ï¼ˆç”ŸæˆåµŒå…¥æˆ–ç±»åˆ«æ ‡ç­¾ï¼‰ä»¥åŠè¾“å‡ºæ¨¡å—ï¼ˆè¿›è¡Œåˆ†ç±»ï¼‰ã€‚è¯¥æ¶æ„é€šè¿‡æ¿€æ´»æ–‡æœ¬æ¨¡å‹çš„å†…éƒ¨ç”µè·¯æ¥å®ç°å¤šæ¨¡æ€ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡æ–‡æœ¬æ¨¡å‹çš„æƒé‡æ¥å®ç°éŸ³é¢‘å’Œå›¾åƒçš„åˆ†ç±»ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒå½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ¨¡å‹çš„è¾“å…¥å±‚èƒ½å¤Ÿå¤„ç†ä¸åŒæ¨¡æ€çš„æ•°æ®ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ä»¥ä¼˜åŒ–åˆ†ç±»æ•ˆæœï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„è‡ªå›å½’LLMè¿›è¡Œè°ƒæ•´ï¼Œä»¥é€‚åº”å¤šæ¨¡æ€è¾“å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å‹çš„æƒé‡åœ¨FSD-50Kå’ŒGTZANæ•°æ®é›†ä¸Šå®ç°äº†éŸ³é¢‘åˆ†ç±»çš„æ˜¾è‘—æå‡ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦15%ã€‚åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒCIFAR-10å’ŒFashion-MNISTæ•°æ®é›†çš„åˆ†ç±»æ€§èƒ½ä¹Ÿæœ‰æ˜æ˜¾æ”¹å–„ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆã€æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–ç›‘æ§ç³»ç»Ÿç­‰ã€‚é€šè¿‡åˆ©ç”¨æ–‡æœ¬æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ï¼Œé™ä½å¼€å‘æˆæœ¬ï¼Œæå‡åº”ç”¨æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“å¤šæ¨¡æ€å­¦ä¹ çš„ç ”ç©¶æ–¹å‘ï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.

