---
layout: default
title: ModRWKV: Transformer Multimodality in Linear Time
---

# ModRWKV: Transformer Multimodality in Linear Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14505" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14505v1</a>
  <a href="https://arxiv.org/pdf/2505.14505.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14505v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14505v1', 'ModRWKV: Transformer Multimodality in Linear Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºModRWKVä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `RWKVæ¶æ„` `è®¡ç®—æ•ˆç‡` `è½»é‡çº§æ¨¡å‹` `ä¿¡æ¯èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•å¤šä¾èµ–äºå¤æ‚çš„Transformeræ¶æ„ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ï¼Œæ¨ç†æ•ˆç‡ä½ã€‚
2. è®ºæ–‡æå‡ºModRWKVæ¡†æ¶ï¼ŒåŸºäºRWKV7æ¶æ„ï¼Œåˆ©ç”¨åŠ¨æ€é€‚åº”çš„å¼‚æ„æ¨¡æ€ç¼–ç å™¨å®ç°å¤šæºä¿¡æ¯èåˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒModRWKVåœ¨æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸Šè¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œç°ä»£RNNæ¶æ„å¯ä½œä¸ºTransformerçš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›®å‰ï¼Œå¤§å¤šæ•°å¤šæ¨¡æ€ç ”ç©¶åŸºäºå…·æœ‰äºŒæ¬¡å¤æ‚åº¦çš„Transformeræ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è€Œçº¿æ€§æ¨¡å‹å¦‚RNNåœ¨æ¨ç†æˆæœ¬ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œä½†å…¶åº”ç”¨ä¸»è¦é™äºæ–‡æœ¬å•ä¸€æ¨¡æ€ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ç°ä»£RNNæ¶æ„åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„èƒ½åŠ›ï¼Œæå‡ºäº†åŸºäºRWKV7æ¶æ„çš„è§£è€¦å¤šæ¨¡æ€æ¡†æ¶ModRWKVï¼Œé€šè¿‡åŠ¨æ€é€‚åº”çš„å¼‚æ„æ¨¡æ€ç¼–ç å™¨å®ç°å¤šæºä¿¡æ¯èåˆã€‚æˆ‘ä»¬è®¾è®¡äº†æä¸ºè½»é‡çš„å¤šæ¨¡æ€æ¨¡å—ï¼Œå¹¶é€šè¿‡å¹¿æ³›å®éªŒç¡®å®šäº†åœ¨æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚ModRWKVåˆ©ç”¨RWKV7 LLMçš„é¢„è®­ç»ƒæƒé‡è¿›è¡Œåˆå§‹åŒ–ï¼Œæ˜¾è‘—åŠ é€Ÿäº†å¤šæ¨¡æ€è®­ç»ƒã€‚å¯¹ä¸åŒé¢„è®­ç»ƒæ£€æŸ¥ç‚¹çš„æ¯”è¾ƒå®éªŒè¿›ä¸€æ­¥è¯æ˜äº†è¿™ç§åˆå§‹åŒ–åœ¨å¢å¼ºæ¨¡å‹ç†è§£å¤šæ¨¡æ€ä¿¡å·èƒ½åŠ›æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å½“å‰å¤šæ¨¡æ€å­¦ä¹ ä¸­åŸºäºTransformerçš„æ¨¡å‹è®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†é˜¶æ®µçš„æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶ï¼Œå¾€å¾€éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ModRWKVæ¡†æ¶é€šè¿‡åˆ©ç”¨ç°ä»£RNNæ¶æ„çš„ä¼˜åŠ¿ï¼Œç»“åˆRWKV7ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œè®¾è®¡äº†è½»é‡çº§çš„å¤šæ¨¡æ€æ¨¡å—ï¼Œèƒ½å¤ŸåŠ¨æ€é€‚åº”ä¸åŒæ¨¡æ€çš„ä¿¡æ¯èåˆï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šModRWKVçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªå¼‚æ„æ¨¡æ€ç¼–ç å™¨ï¼Œè¿™äº›ç¼–ç å™¨èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„æ¨¡æ€ç±»å‹åŠ¨æ€è°ƒæ•´ï¼Œç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚æ¡†æ¶çš„æ ¸å¿ƒæ˜¯RWKV7çš„é¢„è®­ç»ƒæƒé‡ï¼Œè¿™ä¸ºæ¨¡å‹æä¾›äº†è‰¯å¥½çš„åˆå§‹åŒ–åŸºç¡€ã€‚

**å…³é”®åˆ›æ–°**ï¼šModRWKVçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†ç°ä»£RNNæ¶æ„å¼•å…¥å¤šæ¨¡æ€å­¦ä¹ é¢†åŸŸï¼Œæ‰“ç ´äº†ä¼ ç»ŸTransformeræ¨¡å‹çš„é™åˆ¶ï¼Œæä¾›äº†ä¸€ç§æ–°çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒModRWKVåœ¨è®¡ç®—å¤æ‚åº¦ä¸Šæ˜¾è‘—é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒModRWKVé‡‡ç”¨äº†è½»é‡çº§çš„ç½‘ç»œç»“æ„ï¼Œä¼˜åŒ–äº†å‚æ•°è®¾ç½®ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿçš„å®éªŒæ¢ç´¢ç¡®å®šäº†æœ€ä½³é…ç½®ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¤šæ¨¡æ€ä¿¡å·çš„ç‰¹æ€§ï¼Œä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒModRWKVåœ¨å¤šæ¨¡æ€ä¿¡å·ç†è§£æ–¹é¢çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„Transformeræ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒModRWKVåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è‡³å°‘20%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æé«˜äº†30%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººç­‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚ModRWKVæ¡†æ¶èƒ½å¤Ÿåœ¨å¤šæ¨¡æ€æ•°æ®å¤„ç†ä¸Šæä¾›æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œéšç€å¤šæ¨¡æ€æ•°æ®çš„ä¸æ–­å¢åŠ ï¼ŒModRWKVå¯èƒ½ä¼šåœ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone-which achieves multi-source information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration.

