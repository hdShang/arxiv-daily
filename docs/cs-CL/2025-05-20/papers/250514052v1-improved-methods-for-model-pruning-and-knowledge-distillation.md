---
layout: default
title: Improved Methods for Model Pruning and Knowledge Distillation
---

# Improved Methods for Model Pruning and Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14052" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14052v1</a>
  <a href="https://arxiv.org/pdf/2505.14052.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14052v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14052v1', 'Improved Methods for Model Pruning and Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei Jiang, Anying Fu, Youling Zhang

**åˆ†ç±»**: cs.CL, cs.CE

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMAMAå‰ªææ–¹æ³•ä»¥è§£å†³æ¨¡å‹å‰ªææ€§èƒ½ä¸‹é™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹å‰ªæ` `çŸ¥è¯†è’¸é¦` `æ€§èƒ½ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹å‰ªææ–¹æ³•å¸¸å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸”éœ€å¤§é‡é‡æ–°è®­ç»ƒï¼Œå½±å“åº”ç”¨æ•ˆç‡ã€‚
2. æœ¬æ–‡æå‡ºMAMAå‰ªææ–¹æ³•ï¼Œé€šè¿‡è¿åŠ¨å’Œå¹…åº¦åˆ†ææœ‰æ•ˆè¯†åˆ«å¹¶ç§»é™¤ä½è´¡çŒ®ç¥ç»å…ƒï¼Œä¼˜åŒ–æ¨¡å‹ã€‚
3. åˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAMAå‰ªæåœ¨å¤šç§å‰ªææ°´å¹³å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæ€§èƒ½ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡å‹å‰ªææ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚R1æˆ–o3-miniï¼‰çš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å‰ªææ–¹æ³•å¾€å¾€å¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œæˆ–éœ€è¦å¤§é‡çš„é‡æ–°è®­ç»ƒå’Œå¾®è°ƒã€‚æœ¬æ–‡æ—¨åœ¨è¯†åˆ«å¹¶ç§»é™¤åœ¨äººæœºäº¤äº’é˜¶æ®µè´¡çŒ®è¾ƒå°çš„ç¥ç»å…ƒå’Œè¿æ¥ï¼Œä»è€Œè·å¾—ä¸€ä¸ªæ›´å°ã€æ›´å¿«çš„çŸ¥è¯†è’¸é¦æ¨¡å‹ï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆå‡ ä¹ä¸æœªå‰ªææ¨¡å‹ç›¸å½“çš„å†…å®¹ã€‚æˆ‘ä»¬æå‡ºäº†MAMAå‰ªæï¼ˆMovement and Magnitude Analysisï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ”¹è¿›çš„å‰ªææ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶åœ¨æç«¯å‰ªææ°´å¹³ä¸‹ä¿æŒä¸åŸå§‹æœªå‰ªææ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚åŸºäºé¢„è®­ç»ƒé˜¶æ®µå›ºå®šçš„æƒé‡å’Œåå·®ï¼Œä»¥åŠåè®­ç»ƒé˜¶æ®µéªŒè¯çš„GRPOå¥–åŠ±ä½œä¸ºæ–°çš„å‰ªææŒ‡æ ‡ï¼Œåˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒå‰ªææ°´å¹³å’Œä¸‹æ¸¸è®¡ç®—è¯­è¨€å­¦ä»»åŠ¡ä¸­ä¼˜äºå¹¶å¯ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åª²ç¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ¨¡å‹å‰ªææ–¹æ³•åœ¨å‡å°‘æ¨¡å‹å¤§å°çš„åŒæ—¶ï¼Œå¸¸å¸¸å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸”éœ€è¦å¤§é‡çš„é‡æ–°è®­ç»ƒå’Œå¾®è°ƒï¼Œå½±å“äº†å®é™…åº”ç”¨çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„MAMAå‰ªææ–¹æ³•é€šè¿‡è¿åŠ¨å’Œå¹…åº¦åˆ†æï¼Œè¯†åˆ«å‡ºåœ¨äººæœºäº¤äº’é˜¶æ®µè´¡çŒ®è¾ƒå°çš„ç¥ç»å…ƒå’Œè¿æ¥ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘æ¨¡å‹çš„å¤æ‚æ€§å’Œå¤§å°ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMAMAå‰ªææ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µå›ºå®šæƒé‡å’Œåå·®ï¼Œåè®­ç»ƒé˜¶æ®µé€šè¿‡GRPOå¥–åŠ±éªŒè¯å‰ªææ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æç¥ç»å…ƒçš„è¿åŠ¨å’Œå¹…åº¦ï¼Œå†³å®šå‰ªæçš„ä¼˜å…ˆçº§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMAMAå‰ªæçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨è¿åŠ¨å’Œå¹…åº¦åˆ†æä½œä¸ºå‰ªææŒ‡æ ‡ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå•ä¸€çš„æƒé‡é˜ˆå€¼æˆ–éšæœºå‰ªæç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°è¯†åˆ«ä½è´¡çŒ®çš„ç¥ç»å…ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMAMAå‰ªææ–¹æ³•é‡‡ç”¨äº†å›ºå®šçš„æƒé‡å’Œåå·®ï¼Œå¹¶ç»“åˆGRPOå¥–åŠ±è¿›è¡Œåè®­ç»ƒéªŒè¯ï¼Œç¡®ä¿å‰ªæåçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æœªå‰ªææ¨¡å‹ç›¸å½“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMAMAå‰ªææ–¹æ³•åœ¨ä¸åŒå‰ªææ°´å¹³ä¸‹ï¼Œæ¨¡å‹æ€§èƒ½ä¸æœ€å…ˆè¿›çš„å‰ªææ–¹æ³•ç›¸å½“ï¼Œä¸”åœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¼˜ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿è¡Œæ•ˆç‡å’Œå“åº”é€Ÿåº¦ï¼Œé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks.

