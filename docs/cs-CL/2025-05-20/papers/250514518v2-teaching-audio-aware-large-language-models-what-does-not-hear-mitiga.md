---
layout: default
title: "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples"
---

# Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14518" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14518v2</a>
  <a href="https://arxiv.org/pdf/2505.14518.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14518v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14518v2', 'Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chun-Yi Kuan, Hung-yi Lee

**åˆ†ç±»**: eess.AS, cs.CL, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-07-01)

**å¤‡æ³¨**: Accepted to Interspeech 2025. Project Website: https://kuan2jiu99.github.io/Balsa

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLISTENä»¥è§£å†³éŸ³é¢‘æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘æ„ŸçŸ¥` `å¤§è¯­è¨€æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `è´Ÿæ ·æœ¬` `å¹»è§‰ç°è±¡` `å¤šæ¨¡æ€å­¦ä¹ ` `æ™ºèƒ½éŸ³ç®±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³é¢‘æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹å¸¸å¸¸äº§ç”Ÿè™šæ„çš„å£°éŸ³äº‹ä»¶ï¼Œå½±å“å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºLISTENæ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”è®­ç»ƒå¢å¼ºæ¨¡å‹åŒºåˆ†å­˜åœ¨ä¸ç¼ºå¤±å£°éŸ³çš„èƒ½åŠ›ï¼Œåˆ©ç”¨åˆæˆè´Ÿæ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLISTENæœ‰æ•ˆå‡è½»äº†å¹»è§‰ç°è±¡ï¼Œå¹¶åœ¨éŸ³é¢‘é—®ç­”å’Œæ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒéŸ³é¢‘æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰çš„è¿›å±•ä½¿å…¶èƒ½å¤Ÿå¤„ç†å’Œç†è§£éŸ³é¢‘è¾“å…¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¸¸å¸¸ä¼šäº§ç”Ÿè™šæ„çš„å£°éŸ³äº‹ä»¶ï¼Œé™ä½äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LISTENï¼ˆé€šè¿‡æ‰©å±•è´Ÿæ ·æœ¬å­¦ä¹ è¯†åˆ«å£°éŸ³ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ¯”è®­ç»ƒæ–¹æ³•ï¼Œå¢å¼ºäº†ALLMsåŒºåˆ†å­˜åœ¨ä¸ç¼ºå¤±å£°éŸ³çš„èƒ½åŠ›ï¼Œåˆ©ç”¨æ¥è‡ªåŸºç¡€å¤§è¯­è¨€æ¨¡å‹çš„åˆæˆæ•°æ®ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— éœ€ä¿®æ”¹LLMå‚æ•°ï¼Œå¹¶é€šè¿‡è½»é‡çº§é€‚é…å™¨é«˜æ•ˆæ•´åˆéŸ³é¢‘è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒLISTENæœ‰æ•ˆå‡è½»äº†å¹»è§‰ç°è±¡ï¼ŒåŒæ—¶åœ¨ç°æœ‰éŸ³é¢‘é—®ç­”å’Œæ¨ç†åŸºå‡†ä¸Šä¿æŒäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ•°æ®å’Œè®¡ç®—æ•ˆç‡ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³é¢‘æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰åœ¨å¤„ç†éŸ³é¢‘è¾“å…¥æ—¶äº§ç”Ÿè™šæ„å£°éŸ³äº‹ä»¶çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æé«˜æ¨¡å‹ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œæœªèƒ½æœ‰æ•ˆå‡å°‘å¹»è§‰ç°è±¡ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„LISTENæ–¹æ³•é€šè¿‡å¯¹æ¯”è®­ç»ƒï¼Œåˆ©ç”¨åˆæˆçš„è´Ÿæ ·æœ¬æ¥å¢å¼ºæ¨¡å‹å¯¹å£°éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚è¯¥æ–¹æ³•çš„è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹å¯¹ç¼ºå¤±å£°éŸ³çš„æ•æ„Ÿæ€§ï¼Œä»è€Œå‡å°‘å¹»è§‰ç°è±¡çš„å‘ç”Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLISTENæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åˆæˆæ¨¡å—ã€å¯¹æ¯”å­¦ä¹ æ¨¡å—å’Œè½»é‡çº§é€‚é…å™¨ã€‚æ•°æ®åˆæˆæ¨¡å—ç”Ÿæˆè´Ÿæ ·æœ¬ï¼Œå¯¹æ¯”å­¦ä¹ æ¨¡å—ç”¨äºè®­ç»ƒæ¨¡å‹è¯†åˆ«å­˜åœ¨ä¸ç¼ºå¤±çš„å£°éŸ³ï¼Œè€Œé€‚é…å™¨åˆ™é«˜æ•ˆæ•´åˆéŸ³é¢‘è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šLISTENçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— éœ€ä¿®æ”¹å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°ï¼Œé€šè¿‡è½»é‡çº§é€‚é…å™¨å®ç°éŸ³é¢‘è¡¨ç¤ºçš„é«˜æ•ˆæ•´åˆã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å¹»è§‰ç°è±¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LISTENä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åŒºåˆ†å­˜åœ¨ä¸ç¼ºå¤±çš„å£°éŸ³ã€‚æ­¤å¤–ï¼Œè½»é‡çº§é€‚é…å™¨çš„è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨è®¡ç®—èµ„æºä¸Šçš„éœ€æ±‚æ˜¾è‘—é™ä½ï¼Œæå‡äº†æ•´ä½“æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLISTENæ–¹æ³•åœ¨éŸ³é¢‘é—®ç­”å’Œæ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå¹»è§‰ç°è±¡å‡å°‘äº†æ˜¾è‘—çš„æ¯”ä¾‹ï¼ŒåŒæ—¶åœ¨æ•°æ®å’Œè®¡ç®—æ•ˆç‡ä¸Šä¹Ÿæœ‰æ˜æ˜¾æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½éŸ³ç®±ã€è¯­éŸ³åŠ©æ‰‹å’Œè‡ªåŠ¨éŸ³é¢‘åˆ†æç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜éŸ³é¢‘æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹çš„å¯é æ€§ï¼ŒLISTENæ–¹æ³•èƒ½å¤Ÿä¸ºè¿™äº›åº”ç”¨æä¾›æ›´å‡†ç¡®çš„å£°éŸ³è¯†åˆ«å’Œç†è§£èƒ½åŠ›ï¼Œè¿›è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ¨åŠ¨æ›´å¤šå¤šæ¨¡æ€äº¤äº’æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in audio-aware large language models (ALLMs) enable them to process and understand audio inputs. However, these models often hallucinate non-existent sound events, reducing their reliability in real-world applications. To address this, we propose LISTEN (Learning to Identify Sounds Through Extended Negative Samples), a contrastive-like training method that enhances ALLMs' ability to distinguish between present and absent sounds using synthesized data from the backbone LLM. Unlike prior approaches, our method requires no modification to LLM parameters and efficiently integrates audio representations via a lightweight adapter. Experiments show that LISTEN effectively mitigates hallucinations while maintaining impressive performance on existing audio question and reasoning benchmarks. At the same time, it is more efficient in both data and computation.

