---
layout: default
title: "OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation"
---

# OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14350" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14350v2</a>
  <a href="https://arxiv.org/pdf/2505.14350.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14350v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14350v2', 'OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jialong Han, Si Zhang, Ke Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-21)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOSoRAä»¥è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒçš„è®¡ç®—èµ„æºæŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ä½ç§©é€‚åº”` `å¥‡å¼‚å€¼åˆ†è§£` `è®¡ç®—èµ„æºä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¾®è°ƒä¸­é¢ä¸´è®¡ç®—èµ„æºéœ€æ±‚é«˜çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. OSoRAé€šè¿‡ç»“åˆå¥‡å¼‚å€¼åˆ†è§£ä¸å¯å­¦ä¹ çš„ç¼©æ”¾å‘é‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä½ç§©é€‚åº”æ–¹æ³•ï¼Œä¼˜åŒ–äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„å‚æ•°æ•°é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOSoRAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½ä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“ï¼Œä¸”åœ¨å‚æ•°æ‰©å±•ä¸Šä¿æŒçº¿æ€§å¢é•¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œå¾®è°ƒè¿™äº›æ¨¡å‹å˜å¾—æ„ˆåŠ å›°éš¾ï¼Œä¸»è¦æ˜¯ç”±äºå…¶åºå¤§çš„è§„æ¨¡å’Œç›¸å…³çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ä½œä¸ºè®¡ç®—æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å…¶å®ç°ä»éœ€å¤§é‡èµ„æºã€‚æœ¬æ–‡æå‡ºäº†OSoRAï¼ˆè¾“å‡ºç»´åº¦å’Œå¥‡å¼‚å€¼åˆå§‹åŒ–çš„ä½ç§©é€‚åº”ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„PEFTæ–¹æ³•ã€‚OSoRAé€šè¿‡å°†å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ä¸å¯å­¦ä¹ çš„ç¼©æ”¾å‘é‡ç»“åˆåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ï¼Œæ‰©å±•äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ã€‚è¯¥æ–¹æ³•é¦–å…ˆå¯¹é¢„è®­ç»ƒæƒé‡çŸ©é˜µè¿›è¡ŒSVDï¼Œç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–è¾“å‡ºç»´åº¦å‘é‡ï¼ŒåŒæ—¶ä¿æŒç›¸åº”çš„å¥‡å¼‚å‘é‡çŸ©é˜µä¸å˜ã€‚OSoRAæ˜¾è‘—å‡å°‘äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—èµ„æºéœ€æ±‚ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†ã€å¸¸è¯†æ¨ç†ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸LoRAå’ŒVeRAç­‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç§©å¢åŠ åˆ°æ›´é«˜ç»´åº¦æ—¶ä¿æŒçº¿æ€§å‚æ•°æ‰©å±•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†åŒæ—¶è®­ç»ƒå¥‡å¼‚å€¼å’Œè¾“å‡ºç»´åº¦å‘é‡å¯¹å®ç°æœ€ä½³æ€§èƒ½çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—èµ„æºéœ€æ±‚è¿‡é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•è™½ç„¶æœ‰æ‰€æ”¹è¿›ï¼Œä½†ä»éœ€å¤§é‡è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOSoRAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å°†å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ä¸å¯å­¦ä¹ çš„è¾“å‡ºç»´åº¦å‘é‡ç»“åˆï¼Œä¼˜åŒ–å¾®è°ƒè¿‡ç¨‹ä¸­çš„å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œä»è€Œé™ä½è®¡ç®—å¼€é”€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOSoRAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆå¯¹é¢„è®­ç»ƒæƒé‡çŸ©é˜µè¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼Œå¾—åˆ°å¥‡å¼‚å€¼å’Œå¥‡å¼‚å‘é‡ï¼›ç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–è¾“å‡ºç»´åº¦å‘é‡ï¼ŒåŒæ—¶ä¿æŒå¥‡å¼‚å‘é‡çŸ©é˜µä¸å˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šOSoRAçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å¥‡å¼‚å€¼ä¸è¾“å‡ºç»´åº¦å‘é‡çš„è”åˆè®­ç»ƒå¼•å…¥ä½ç§©é€‚åº”æ¡†æ¶ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æé«˜äº†å¾®è°ƒæ•ˆç‡ï¼Œå¹¶ä¸ç°æœ‰æ–¹æ³•å¦‚LoRAå’ŒVeRAå½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒOSoRAé‡‡ç”¨äº†å›ºå®šçš„å¥‡å¼‚å‘é‡çŸ©é˜µå’Œå¯å­¦ä¹ çš„è¾“å‡ºç»´åº¦å‘é‡ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡äºä¼˜åŒ–è¾“å‡ºç»´åº¦çš„åŒæ—¶ä¿æŒå¥‡å¼‚å€¼çš„ç¨³å®šæ€§ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OSoRAåœ¨æ•°å­¦æ¨ç†å’Œå¸¸è¯†æ¨ç†ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸LoRAå’ŒVeRAç­‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ã€‚åŒæ—¶ï¼ŒOSoRAåœ¨å‚æ•°æ‰©å±•ä¸Šä¿æŒçº¿æ€§å¢é•¿ï¼Œæ˜¾è‘—é™ä½äº†å¾®è°ƒæ‰€éœ€çš„å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œæå‡äº†è®¡ç®—æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OSoRAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡é™ä½å¾®è°ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—èµ„æºéœ€æ±‚ï¼ŒOSoRAèƒ½å¤Ÿä½¿å¾—æ›´å¤šçš„ç ”ç©¶è€…å’Œå¼€å‘è€…èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä½¿ç”¨å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning Large Language Models (LLMs) has become increasingly challenging due to their massive scale and associated computational costs. Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as computational alternatives; however, their implementations still require significant resources. In this paper, we present OSoRA (Output-Dimension and Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs. OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value Decomposition (SVD) with learnable scaling vectors in a unified framework. It first performs an SVD of pre-trained weight matrices, then optimizes an output-dimension vector during training, while keeping the corresponding singular vector matrices frozen. OSoRA substantially reduces computational resource requirements by minimizing the number of trainable parameters during fine-tuning. Comprehensive evaluations across mathematical reasoning, common sense reasoning, and other benchmarks demonstrate that OSoRA achieves comparable or superior performance to state-of-the-art methods like LoRA and VeRA, while maintaining a linear parameter scaling even as the rank increases to higher dimensions. Our ablation studies further confirm that jointly training both the singular values and the output-dimension vector is critical for optimal performance.

