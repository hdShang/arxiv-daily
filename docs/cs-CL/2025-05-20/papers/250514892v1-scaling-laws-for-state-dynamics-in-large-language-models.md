---
layout: default
title: Scaling Laws for State Dynamics in Large Language Models
---

# Scaling Laws for State Dynamics in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14892" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14892v1</a>
  <a href="https://arxiv.org/pdf/2505.14892.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14892v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14892v1', 'Scaling Laws for State Dynamics in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jacob X Li, Shreyas S Raman, Jessica Wan, Fahad Samman, Jazlyn Lin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: 16 pages; 23 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å¤§è¯­è¨€æ¨¡å‹çŠ¶æ€åŠ¨æ€çš„è§„æ¨¡æ³•åˆ™**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `çŠ¶æ€åŠ¨æ€` `æ¿€æ´»è¡¥ä¸` `æ³¨æ„åŠ›æœºåˆ¶` `çŠ¶æ€è·Ÿè¸ª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨çŠ¶æ€åŠ¨æ€å»ºæ¨¡æ–¹é¢çš„èƒ½åŠ›å°šä¸æ˜ç¡®ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡é€šè¿‡è¯„ä¼°LLMsåœ¨å¤šä¸ªé¢†åŸŸçš„çŠ¶æ€åŠ¨æ€æ•æ‰èƒ½åŠ›ï¼Œæå‡ºäº†æ¿€æ´»è¡¥ä¸çš„æ–¹æ³•æ¥è¯†åˆ«å…³é”®çš„æ³¨æ„åŠ›å¤´ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-2 XLå’ŒPythia-1Båœ¨çŠ¶æ€æ•°é‡å¢åŠ æ—¶å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†çŠ¶æ€è·Ÿè¸ªçš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦å†…éƒ¨çŠ¶æ€è·Ÿè¸ªçš„ä»»åŠ¡ä¸­è¶Šæ¥è¶Šå¤šåœ°è¢«ä½¿ç”¨ï¼Œä½†å®ƒä»¬å¯¹çŠ¶æ€è½¬ç§»åŠ¨æ€çš„å»ºæ¨¡èƒ½åŠ›ä»ç„¶ä¸å¤Ÿæ¸…æ™°ã€‚æœ¬æ–‡è¯„ä¼°äº†LLMsåœ¨ä¸‰ä¸ªé¢†åŸŸï¼ˆç›’å­è·Ÿè¸ªã€æŠ½è±¡DFAåºåˆ—å’Œå¤æ‚æ–‡æœ¬æ¸¸æˆï¼‰ä¸­æ•æ‰ç¡®å®šæ€§çŠ¶æ€åŠ¨æ€çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€çŠ¶æ€ç©ºé—´å¤§å°å’Œç¨€ç–è½¬ç§»çš„å¢åŠ ï¼Œä¸‹ä¸€çŠ¶æ€é¢„æµ‹çš„å‡†ç¡®æ€§ä¸‹é™ã€‚GPT-2 XLåœ¨ä½å¤æ‚åº¦è®¾ç½®ä¸‹çš„å‡†ç¡®ç‡çº¦ä¸º70%ï¼Œä½†å½“ç›’å­æˆ–çŠ¶æ€æ•°é‡è¶…è¿‡5æˆ–10æ—¶ï¼Œå‡†ç¡®ç‡é™è‡³30%ä»¥ä¸‹ã€‚åœ¨DFAä»»åŠ¡ä¸­ï¼ŒPythia-1Båœ¨çŠ¶æ€æ•°é‡è¶…è¿‡10ä¸”è½¬ç§»å°‘äº30æ—¶ï¼Œå‡†ç¡®ç‡æœªèƒ½è¶…è¿‡50%ã€‚é€šè¿‡æ¿€æ´»è¡¥ä¸ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºè´Ÿè´£ä¼ æ’­çŠ¶æ€ä¿¡æ¯çš„æ³¨æ„åŠ›å¤´ï¼Œè¡¨æ˜LLMsçš„çŠ¶æ€è·Ÿè¸ªæ˜¯é€šè¿‡ä¸‹ä¸€æ ‡è®°å¤´çš„åˆ†å¸ƒå¼äº¤äº’è€Œéæ˜¾å¼ç¬¦å·è®¡ç®—å®ç°çš„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨çŠ¶æ€åŠ¨æ€å»ºæ¨¡ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨çŠ¶æ€ç©ºé—´å¢å¤§æ—¶é¢„æµ‹å‡†ç¡®æ€§ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•æ‰å¤æ‚çŠ¶æ€è½¬ç§»çš„åŠ¨æ€ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ¿€æ´»è¡¥ä¸æŠ€æœ¯è¯†åˆ«å‡ºåœ¨çŠ¶æ€ä¿¡æ¯ä¼ æ’­ä¸­èµ·å…³é”®ä½œç”¨çš„æ³¨æ„åŠ›å¤´ï¼Œè¿›è€Œåˆ†æå…¶åœ¨çŠ¶æ€è·Ÿè¸ªä¸­çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æ­ç¤ºLLMsåœ¨çŠ¶æ€åŠ¨æ€å»ºæ¨¡ä¸­çš„æ½œåœ¨æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åˆ†ä¸ºä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šé¦–å…ˆæ˜¯å¯¹ä¸åŒä»»åŠ¡ï¼ˆç›’å­è·Ÿè¸ªã€DFAåºåˆ—ã€å¤æ‚æ–‡æœ¬æ¸¸æˆï¼‰çš„çŠ¶æ€åŠ¨æ€è¿›è¡Œè¯„ä¼°ï¼›å…¶æ¬¡æ˜¯é€šè¿‡æ¿€æ´»è¡¥ä¸æŠ€æœ¯è¯†åˆ«å…³é”®æ³¨æ„åŠ›å¤´ï¼›æœ€åæ˜¯åˆ†æè¿™äº›å¤´åœ¨çŠ¶æ€ä¿¡æ¯ä¼ æ’­ä¸­çš„ä½œç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡æ¿€æ´»è¡¥ä¸è¯†åˆ«å‡ºç‰¹å®šçš„æ³¨æ„åŠ›å¤´ï¼Œè¿™äº›å¤´åœ¨çŠ¶æ€ä¿¡æ¯çš„ä¼ æ’­ä¸­èµ·åˆ°äº†é‡è¦ä½œç”¨ï¼Œè€Œä¸æ˜¯ä¾èµ–äºä¼ ç»Ÿçš„ç¬¦å·è®¡ç®—æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†ä¸åŒçš„çŠ¶æ€æ•°é‡å’Œè½¬ç§»ç¨€ç–åº¦è®¾ç½®ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œå…³æ³¨äº†GPT-2 XLçš„ç¬¬22å±‚ç¬¬20ä¸ªå¤´å’ŒPythia-1Bçš„ç¬¬10ã€11ã€12å’Œ14å±‚çš„å¤´çš„æ¿€æ´»æƒ…å†µã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œç ”ç©¶æ­ç¤ºäº†LLMsåœ¨çŠ¶æ€è·Ÿè¸ªä¸­çš„å±€é™æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-2 XLåœ¨ä½å¤æ‚åº¦è®¾ç½®ä¸‹çš„å‡†ç¡®ç‡è¾¾åˆ°70%ï¼Œä½†å½“çŠ¶æ€æ•°é‡è¶…è¿‡5æ—¶ï¼Œå‡†ç¡®ç‡é™è‡³30%ä»¥ä¸‹ã€‚Pythia-1Båœ¨çŠ¶æ€æ•°é‡è¶…è¿‡10ä¸”è½¬ç§»å°‘äº30æ—¶ï¼Œå‡†ç¡®ç‡æœªèƒ½è¶…è¿‡50%ï¼Œæ­ç¤ºäº†çŠ¶æ€è·Ÿè¸ªçš„æ˜¾è‘—æŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æ¸¸æˆAIå’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„çŠ¶æ€åŠ¨æ€ï¼Œèƒ½å¤Ÿæå‡å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä»è€Œæ¨åŠ¨æ›´æ™ºèƒ½çš„äº¤äº’å’Œå†³ç­–æ”¯æŒç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are increasingly used in tasks requiring internal state tracking, yet their ability to model state transition dynamics remains poorly understood. We evaluate how well LLMs capture deterministic state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and Complex Text Games, each formalizable as a finite-state system. Across tasks, we find that next-state prediction accuracy degrades with increasing state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in low-complexity settings but drops below 30% when the number of boxes or states exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50% accuracy when the number of states is > 10 and transitions are < 30. Through activation patching, we identify attention heads responsible for propagating state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10, 11, 12, and 14. While these heads successfully move relevant state features, action information is not reliably routed to the final token, indicating weak joint state-action reasoning. Our results suggest that state tracking in LLMs emerges from distributed interactions of next-token heads rather than explicit symbolic computation.

