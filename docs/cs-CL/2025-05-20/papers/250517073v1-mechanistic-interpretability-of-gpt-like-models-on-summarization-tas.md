---
layout: default
title: Mechanistic Interpretability of GPT-like Models on Summarization Tasks
---

# Mechanistic Interpretability of GPT-like Models on Summarization Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.17073" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.17073v1</a>
  <a href="https://arxiv.org/pdf/2505.17073.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.17073v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.17073v1', 'Mechanistic Interpretability of GPT-like Models on Summarization Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anurag Mishra

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: 8 pages (6 content + 2 references/appendix), 6 figures, 2 tables; under review for the ACL 2025 Student Research Workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæœºåˆ¶å¯è§£é‡Šæ€§æ¡†æ¶ä»¥åˆ†æGPTæ¨¡å‹åœ¨æ‘˜è¦ä»»åŠ¡ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºåˆ¶å¯è§£é‡Šæ€§` `GPTæ¨¡å‹` `æ‘˜è¦ç”Ÿæˆ` `å·®å¼‚åˆ†æ` `LoRAé€‚åº”` `ä¿¡æ¯é€‰æ‹©` `å†…éƒ¨æ¿€æ´»` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯è§£é‡Šæ€§ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ï¼Œç¼ºä¹å¯¹æ‘˜è¦ä»»åŠ¡çš„æ·±å…¥åˆ†æã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯è§£é‡Šæ€§æ¡†æ¶ï¼Œé€šè¿‡å·®å¼‚åˆ†ææ­ç¤ºGPTç±»æ¨¡å‹åœ¨æ‘˜è¦ä»»åŠ¡ä¸­çš„é€‚åº”æœºåˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šå±‚å’Œæ³¨æ„åŠ›å¤´çš„LoRAé€‚åº”æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘äº†è®­ç»ƒå‘¨æœŸã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºåˆ¶å¯è§£é‡Šæ€§ç ”ç©¶æ—¨åœ¨æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨åˆ†ç±»æˆ–ç”Ÿæˆä»»åŠ¡ï¼Œè€Œéæ‘˜è¦ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šæ€§æ¡†æ¶ï¼Œç”¨äºåˆ†æGPTç±»æ¨¡å‹å¦‚ä½•é€‚åº”æ‘˜è¦ä»»åŠ¡ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹ä¸å¾®è°ƒæ¨¡å‹è¿›è¡Œå·®å¼‚åˆ†æï¼Œé‡åŒ–æ³¨æ„åŠ›æ¨¡å¼å’Œå†…éƒ¨æ¿€æ´»çš„å˜åŒ–ï¼Œè¯†åˆ«å‡ºç»å†æ˜¾è‘—è½¬å˜çš„ç‰¹å®šå±‚å’Œæ³¨æ„åŠ›å¤´ï¼Œä»è€Œå®šä½æ¨¡å‹æ¶æ„ä¸­çš„â€œæ‘˜è¦ç”µè·¯â€ã€‚ç ”ç©¶å‘ç°ï¼Œä¸­é—´å±‚ï¼ˆå°¤å…¶æ˜¯ç¬¬2ã€3å’Œ5å±‚ï¼‰è¡¨ç°å‡ºæœ€æ˜¾è‘—çš„å˜åŒ–ï¼Œ62%çš„æ³¨æ„åŠ›å¤´æ˜¾ç¤ºå‡ºç†µé™ä½ï¼Œè¡¨æ˜ä¿¡æ¯é€‰æ‹©è¶‹å‘é›†ä¸­ã€‚æˆ‘ä»¬å±•ç¤ºäº†é’ˆå¯¹è¿™äº›è¯†åˆ«ç”µè·¯çš„LoRAé€‚åº”èƒ½å¤Ÿåœ¨è¾ƒå°‘çš„è®­ç»ƒå‘¨æœŸå†…æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå¼¥åˆäº†é»‘ç®±è¯„ä¼°ä¸æœºåˆ¶ç†è§£ä¹‹é—´çš„å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯è§£é‡Šæ€§ç ”ç©¶å¯¹æ‘˜è¦ä»»åŠ¡å…³æ³¨ä¸è¶³çš„é—®é¢˜ï¼Œæ­ç¤ºGPTç±»æ¨¡å‹åœ¨æ‘˜è¦ç”Ÿæˆä¸­çš„å†…éƒ¨æœºåˆ¶ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºåˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ï¼Œç¼ºä¹å¯¹æ‘˜è¦ä»»åŠ¡çš„æ·±å…¥ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹è¿›è¡Œå·®å¼‚åˆ†æï¼Œé‡åŒ–æ³¨æ„åŠ›æ¨¡å¼å’Œå†…éƒ¨æ¿€æ´»çš„å˜åŒ–ï¼Œè¯†åˆ«å‡ºæ¨¡å‹æ¶æ„ä¸­çš„â€œæ‘˜è¦ç”µè·¯â€ï¼Œä»è€Œæä¾›å¯¹ä¿¡æ¯é€‰æ‹©å’Œå‹ç¼©è¿‡ç¨‹çš„æœºåˆ¶ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å·®å¼‚åˆ†æçš„æ–¹æ³•ï¼Œä¸»è¦åŒ…æ‹¬é¢„è®­ç»ƒæ¨¡å‹ä¸å¾®è°ƒæ¨¡å‹çš„æ¯”è¾ƒï¼Œå…³æ³¨ä¸­é—´å±‚çš„å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯ç¬¬2ã€3å’Œ5å±‚çš„æ³¨æ„åŠ›å¤´ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºè¯†åˆ«å‡ºç‰¹å®šçš„â€œæ‘˜è¦ç”µè·¯â€ï¼Œå¹¶é€šè¿‡é’ˆå¯¹è¿™äº›ç”µè·¯çš„LoRAé€‚åº”å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ä¼ ç»Ÿçš„LoRAå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé€‰æ‹©äº†ç‰¹å®šçš„å±‚å’Œæ³¨æ„åŠ›å¤´è¿›è¡Œåˆ†æï¼Œä½¿ç”¨äº†LoRAé€‚åº”æŠ€æœ¯ï¼Œä¼˜åŒ–äº†è®­ç»ƒå‘¨æœŸï¼Œç¡®ä¿åœ¨è¾ƒå°‘çš„è®­ç»ƒæ—¶é—´å†…è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹è¯†åˆ«å‡ºçš„æ‘˜è¦ç”µè·¯è¿›è¡Œçš„LoRAé€‚åº”åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†çš„LoRAå¾®è°ƒï¼Œä¸”è®­ç»ƒå‘¨æœŸå‡å°‘ï¼Œ62%çš„æ³¨æ„åŠ›å¤´è¡¨ç°å‡ºç†µé™ä½ï¼Œè¡¨æ˜ä¿¡æ¯é€‰æ‹©æ›´åŠ é›†ä¸­ã€‚è¿™äº›ç»“æœä¸ºç†è§£æ¨¡å‹åœ¨æ‘˜è¦ä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†é‡è¦çš„å®è¯æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆã€ä¿¡æ¯æ£€ç´¢å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶ï¼Œå¯ä»¥ä¸ºæ”¹è¿›æ‘˜è¦ç”ŸæˆæŠ€æœ¯æä¾›ç†è®ºæ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯èƒ½ä¼šè¢«åº”ç”¨äºå…¶ä»–ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mechanistic interpretability research seeks to reveal the inner workings of large language models, yet most work focuses on classification or generative tasks rather than summarization. This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks. We conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations. By identifying specific layers and attention heads that undergo significant transformation, we locate the "summarization circuit" within the model architecture. Our findings reveal that middle layers (particularly 2, 3, and 5) exhibit the most dramatic changes, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. We demonstrate that targeted LoRA adaptation of these identified circuits achieves significant performance improvement over standard LoRA fine-tuning while requiring fewer training epochs. This work bridges the gap between black-box evaluation and mechanistic understanding, providing insights into how neural networks perform information selection and compression during summarization.

