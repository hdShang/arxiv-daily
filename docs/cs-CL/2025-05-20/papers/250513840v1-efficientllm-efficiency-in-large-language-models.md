---
layout: default
title: EfficientLLM: Efficiency in Large Language Models
---

# EfficientLLM: Efficiency in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13840" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.13840v1</a>
  <a href="https://arxiv.org/pdf/2505.13840.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13840v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13840v1', 'EfficientLLM: Efficiency in Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, Keerthiram Murugesan, Yu Wang, Lifang He, Jianfeng Gao, Lichao Sun, Yanfang Ye

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EfficientLLM‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÊïàÁéá‰ºòÂåñ` `Ê®°ÂûãËØÑ‰º∞` `Á®ÄÁñè‰∏ìÂÆ∂` `ÈáèÂåñÊäÄÊúØ` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ËÆ°ÁÆóÊú∫ËßÜËßâ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂèÇÊï∞Âíå‰∏ä‰∏ãÊñáÁ™óÂè£ÁöÑÂ¢ûÂä†‰∏ãÔºåËÆ°ÁÆóÂíåËÉΩÊ∫êÊàêÊú¨ÊòæËëó‰∏äÂçáÔºå‰∫üÈúÄÊèêÈ´òÊïàÁéá„ÄÇ
2. EfficientLLMÈÄöËøáÁ≥ªÁªüËØÑ‰º∞Êû∂ÊûÑÈ¢ÑËÆ≠ÁªÉ„ÄÅÂæÆË∞ÉÂíåÊé®ÁêÜÊäÄÊúØÔºåÊèêÂá∫‰∫ÜÂ§öÁßçÊïàÁéá‰ºòÂåñÊñπÊ°à„ÄÇ
3. Á†îÁ©∂ËØÑ‰º∞‰∫Ü100Â§ö‰∏™Ê®°Âûã-ÊäÄÊúØÂØπÔºåÂèëÁé∞ÊïàÁéá‰∏é‰ªªÂä°„ÄÅËßÑÊ®°Áõ∏ÂÖ≥Ôºå‰∏îÊäÄÊúØÂú®‰∏çÂêåÊ®°ÊÄÅÈó¥ÂÖ∑ÊúâËâØÂ•ΩÁöÑËøÅÁßªÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®Âä®ÊäÄÊúØËøõÊ≠•ÁöÑÂêåÊó∂ÔºåÂÖ∂ÂèÇÊï∞Êï∞ÈáèÂíå‰∏ä‰∏ãÊñáÁ™óÂè£ÁöÑÂ¢ûÂä†ÂØºËá¥ËÆ°ÁÆó„ÄÅËÉΩÊ∫êÂíåÁªèÊµéÊàêÊú¨ÁöÑÊòæËëó‰∏äÂçá„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜEfficientLLMÔºåËøôÊòØÈ¶ñ‰∏™ÂÖ®Èù¢ÁöÑÂÆûËØÅÁ†îÁ©∂ÔºåËØÑ‰º∞Â§ßËßÑÊ®°LLMsÁöÑÊïàÁéáÊäÄÊúØ„ÄÇÁ†îÁ©∂Âú®Áîü‰∫ßÁ∫ßÈõÜÁæ§‰∏äËøõË°åÔºåÁ≥ªÁªüÊé¢ËÆ®‰∫ÜÊû∂ÊûÑÈ¢ÑËÆ≠ÁªÉ„ÄÅÂæÆË∞ÉÂíåÊé®ÁêÜ‰∏â‰∏™ÂÖ≥ÈîÆÊñπÈù¢ÔºåÂπ∂ÂÆö‰πâ‰∫ÜÂÖ≠‰∏™ÁªÜËá¥ÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÇÈÄöËøáÂØπ100Â§ö‰∏™Ê®°Âûã-ÊäÄÊúØÂØπÁöÑËØÑ‰º∞ÔºåÂæóÂá∫‰∫ÜÊïàÁéáÊ∂âÂèäÂèØÈáèÂåñÊùÉË°°„ÄÅÊúÄ‰ºòËß£‰æùËµñ‰∫é‰ªªÂä°ÂíåËßÑÊ®°„ÄÅÊäÄÊúØÂú®‰∏çÂêåÊ®°ÊÄÅÈó¥ÁöÑÈÄöÁî®ÊÄßÁ≠â‰∏âÂ§ßÊ†∏ÂøÉËßÅËß£„ÄÇEfficientLLM‰∏∫Á†îÁ©∂‰∫∫ÂëòÂíåÂ∑•Á®ãÂ∏àÊèê‰æõ‰∫ÜÂú®‰∏ã‰∏Ä‰ª£Âü∫Á°ÄÊ®°ÂûãÁöÑÊïàÁéá‰∏éÊÄßËÉΩ‰πãÈó¥ÂØºËà™ÁöÑÈáçË¶ÅÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËÆ°ÁÆóÂíåËÉΩÊ∫êÊàêÊú¨‰∏äÊó•ÁõäÂ¢ûÈïøÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÊïàÁéá‰∏äÂ≠òÂú®ÊòéÊòæ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂºïÂÖ•Â§öÁßçÊïàÁéáÊäÄÊúØÔºåÁ≥ªÁªüËØÑ‰º∞ÂÖ∂Âú®‰∏çÂêå‰ªªÂä°ÂíåËßÑÊ®°‰∏ãÁöÑË°®Áé∞Ôºå‰ª•ÂÆûÁé∞Êõ¥‰ºòÁöÑËµÑÊ∫êÂà©Áî®ÂíåÊÄßËÉΩÂπ≥Ë°°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÂàÜ‰∏∫‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊû∂ÊûÑÈ¢ÑËÆ≠ÁªÉÔºàÂåÖÊã¨È´òÊïàÊ≥®ÊÑèÂäõÂèò‰ΩìÂíåÁ®ÄÁñè‰∏ìÂÆ∂Ê∑∑ÂêàÔºâÔºåÂæÆË∞ÉÔºàÈááÁî®ÂèÇÊï∞È´òÊïàÊñπÊ≥ïÔºâÔºåÊé®ÁêÜÔºàÈáèÂåñÊäÄÊúØÔºâ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊèêÂá∫‰∫ÜÂÖ≠‰∏™ÁªÜËá¥ÁöÑËØÑ‰º∞ÊåáÊ†áÔºåÈ¶ñÊ¨°ÂÖ®Èù¢ËØÑ‰º∞‰∫ÜÂ§ßËßÑÊ®°LLMsÁöÑÊïàÁéáÊäÄÊúØÔºåÂº∫Ë∞É‰∫ÜÊïàÁéáÁöÑÂèØÈáèÂåñÊùÉË°°ÂíåÊäÄÊúØÁöÑÈÄöÁî®ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Êû∂ÊûÑÈ¢ÑËÆ≠ÁªÉ‰∏≠‰ΩøÁî®‰∫ÜMQA„ÄÅGQAÁ≠âÈ´òÊïàÊ≥®ÊÑèÂäõÂèò‰ΩìÔºåÂæÆË∞É‰∏≠ÈááÁî®LoRAÁ≠âÂèÇÊï∞È´òÊïàÊñπÊ≥ïÔºåÊé®ÁêÜÈò∂ÊÆµÂàô‰ΩøÁî®int4Âíåfloat16ÈáèÂåñÊäÄÊúØ„ÄÇÈÄöËøáËøô‰∫õËÆæËÆ°ÔºåÁ†îÁ©∂ÂÆûÁé∞‰∫ÜÂú®‰∏çÂêå‰ªªÂä°ÂíåËßÑÊ®°‰∏ãÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁ®ÄÁñè‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÂú®ÂáèÂ∞ëFLOPsÂíåÊèêÈ´òÂáÜÁ°ÆÁéáÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºå‰ΩÜVRAMÂ¢ûÂä†‰∫Ü40%ÔºõËÄåint4ÈáèÂåñÊäÄÊúØÂú®Èôç‰ΩéÂÜÖÂ≠òÂíåËÉΩÊ∫êÊ∂àËÄóÊñπÈù¢ÂèØËææ3.9ÂÄçÔºå‰ΩÜÂáÜÁ°ÆÁéá‰∏ãÈôç3-5%„ÄÇËøô‰∫õÂèëÁé∞‰∏∫‰∏çÂêå‰ªªÂä°ÂíåËßÑÊ®°‰∏ãÁöÑÊ®°Âûã‰ºòÂåñÊèê‰æõ‰∫ÜÈáçË¶ÅÊåáÂØº„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EfficientLLMÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅËÆ°ÁÆóÊú∫ËßÜËßâÁ≠âÈ¢ÜÂüüÔºåÂ∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÂíåÂ∑•Á®ãÂ∏àÂú®ÂºÄÂèë‰∏ã‰∏Ä‰ª£Âü∫Á°ÄÊ®°ÂûãÊó∂Ôºå‰ºòÂåñËµÑÊ∫êÂà©Áî®ÂíåÊÄßËÉΩË°®Áé∞„ÄÇËøôÂ∞ÜÊé®Âä®Êõ¥È´òÊïàÁöÑÊ®°ÂûãËÆæËÆ°ÂíåÂ∫îÁî®ÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÂíåÁéØÂ¢ÉÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.

