---
layout: default
title: Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs
---

# Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14530" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14530v2</a>
  <a href="https://arxiv.org/pdf/2505.14530.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14530v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14530v2', 'Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhipeng Yang, Junzhuo Li, Siyu Xia, Xuming Hu

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-09-28)

**å¤‡æ³¨**: EMNLP 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå†…éƒ¨æ€ç»´é“¾ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡æ‰§è¡Œé€æ˜åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å†…éƒ¨æ€ç»´é“¾` `ä»»åŠ¡åˆ†è§£` `å±‚çº§æ‰§è¡Œ` `é€æ˜æ€§æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ç¼ºä¹é€æ˜æ€§ï¼Œéš¾ä»¥ç†è§£å…¶å†…éƒ¨æ‰§è¡Œæœºåˆ¶ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å†…éƒ¨æ€ç»´é“¾çš„æ¦‚å¿µï¼Œå±•ç¤ºäº†æ¨¡å‹å¦‚ä½•æŒ‰å±‚æ¬¡åˆ†è§£å’Œæ‰§è¡Œä»»åŠ¡ï¼Œå¢å¼ºäº†å¯¹æ¨¡å‹è¡Œä¸ºçš„ç†è§£ã€‚
3. é€šè¿‡å®éªŒéªŒè¯ï¼Œæ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡ä¸Šå­¦ä¹ å’Œæ‰§è¡Œå­ä»»åŠ¡ï¼Œæå‡äº†ä»»åŠ¡æ‰§è¡Œçš„æ•ˆç‡å’Œé€æ˜åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶è¡¨æ˜ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºä¸€ç§å†…éƒ¨æ€ç»´é“¾çš„ç‰¹æ€§ï¼šå®ƒä»¬æŒ‰å±‚é€æ­¥åˆ†è§£å’Œæ‰§è¡Œå¤åˆä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºä¸¤ä¸ªä¸»è¦è§‚ç‚¹ï¼šç¬¬ä¸€ï¼Œä¸åŒçš„å­ä»»åŠ¡åœ¨ç½‘ç»œçš„ä¸åŒæ·±åº¦è¢«å­¦ä¹ ï¼›ç¬¬äºŒï¼Œè¿™äº›å­ä»»åŠ¡åœ¨å„å±‚ä¹‹é—´é¡ºåºæ‰§è¡Œã€‚é€šè¿‡å¯¹15ä¸ªä¸¤æ­¥å¤åˆä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å±‚çº§ä¸Šä¸‹æ–‡æ©è”½å’Œä¸€ç§æ–°é¢–çš„è·¨ä»»åŠ¡æ‹¼æ¥æ–¹æ³•ï¼ŒéªŒè¯äº†ç¬¬ä¸€ä¸ªè§‚ç‚¹ã€‚ä¸ºæ£€éªŒç¬¬äºŒä¸ªè§‚ç‚¹ï¼Œæˆ‘ä»¬åº”ç”¨LogitLensè§£ç éšè—çŠ¶æ€ï¼Œæ­ç¤ºäº†ä¸€ç§ä¸€è‡´çš„å±‚çº§æ‰§è¡Œæ¨¡å¼ã€‚æˆ‘ä»¬è¿˜åœ¨çœŸå®ä¸–ç•Œçš„TRACEåŸºå‡†ä¸Šå¤åˆ¶äº†æˆ‘ä»¬çš„åˆ†æï¼Œè§‚å¯Ÿåˆ°ç›¸åŒçš„é€æ­¥åŠ¨æ€ã€‚æˆ‘ä»¬çš„ç»“æœå¢å¼ºäº†LLMsçš„é€æ˜æ€§ï¼Œå±•ç¤ºäº†å®ƒä»¬å†…éƒ¨è§„åˆ’å’Œæ‰§è¡Œå­ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä¸ºç»†ç²’åº¦çš„æŒ‡ä»¤çº§æ¿€æ´»å¼•å¯¼å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ‰§è¡Œå¤åˆä»»åŠ¡æ—¶çš„é€æ˜æ€§ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æ¸…æ™°å±•ç¤ºæ¨¡å‹å¦‚ä½•åˆ†è§£å’Œæ‰§è¡Œä»»åŠ¡ï¼Œå¯¼è‡´ç†è§£å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†å†…éƒ¨æ€ç»´é“¾çš„æ¦‚å¿µï¼Œè®¤ä¸ºæ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡ä¸Šå­¦ä¹ å’Œæ‰§è¡Œå­ä»»åŠ¡ï¼Œä»è€Œå®ç°é€å±‚çš„ä»»åŠ¡åˆ†è§£ä¸æ‰§è¡Œã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå±‚çº§ä¸Šä¸‹æ–‡æ©è”½å’Œè·¨ä»»åŠ¡æ‹¼æ¥æ–¹æ³•ã€‚å±‚çº§ä¸Šä¸‹æ–‡æ©è”½ç”¨äºè¯†åˆ«ä¸åŒå±‚æ¬¡çš„å­ä»»åŠ¡ï¼Œè€Œè·¨ä»»åŠ¡æ‹¼æ¥åˆ™ç”¨äºæ•´åˆä¸åŒä»»åŠ¡çš„æ‰§è¡Œä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†å†…éƒ¨æ€ç»´é“¾çš„æ¦‚å¿µï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨å±‚çº§ä¸Šæ‰§è¡Œå­ä»»åŠ¡çš„èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„å•å±‚æ¬¡ä»»åŠ¡æ‰§è¡Œæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæä¾›äº†æ›´æ·±å±‚æ¬¡çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†LogitLensæŠ€æœ¯è§£ç éšè—çŠ¶æ€ï¼Œæ­ç¤ºäº†å±‚çº§æ‰§è¡Œæ¨¡å¼ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡çš„è¡¨ç°ï¼Œç¡®ä¿å­ä»»åŠ¡çš„æœ‰æ•ˆå­¦ä¹ ä¸æ‰§è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨15ä¸ªä¸¤æ­¥å¤åˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„å±‚çº§æ‰§è¡Œæ¨¡å¼ï¼ŒéªŒè¯äº†å†…éƒ¨æ€ç»´é“¾çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡LogitLensè§£ç ï¼Œå‘ç°æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡ä¸Šå­¦ä¹ å’Œæ‰§è¡Œå­ä»»åŠ¡ï¼Œæå‡äº†ä»»åŠ¡æ‰§è¡Œçš„é€æ˜åº¦å’Œæ•ˆç‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„é€æ˜åº¦ï¼Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–æ¨¡å‹çš„è¡Œä¸ºï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œå’Œç”¨æˆ·äº¤äº’ã€‚æœªæ¥ï¼Œè¿™ä¸€ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„ç³»ç»Ÿè®¾è®¡ï¼Œä¿ƒè¿›äººæœºåä½œçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We show that large language models (LLMs) exhibit an $\textit{internal chain-of-thought}$: they sequentially decompose and execute composite tasks layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned at different network depths, and (ii) these subtasks are executed sequentially across layers. On a benchmark of 15 two-step composite tasks, we employ layer-from context-masking and propose a novel cross-task patching method, confirming (i). To examine claim (ii), we apply LogitLens to decode hidden states, revealing a consistent layerwise execution pattern. We further replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing the same stepwise dynamics. Together, our results enhance LLMs transparency by showing their capacity to internally plan and execute subtasks (or instructions), opening avenues for fine-grained, instruction-level activation steering.

