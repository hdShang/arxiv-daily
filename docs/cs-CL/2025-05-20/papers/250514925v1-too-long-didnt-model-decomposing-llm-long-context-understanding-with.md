---
layout: default
title: Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels
---

# Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14925" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14925v1</a>
  <a href="https://arxiv.org/pdf/2505.14925.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14925v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14925v1', 'Too Long, Didn\'t Model: Decomposing LLM Long-Context Understanding With Novels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sil Hamilton, Rebecca M. M. Hicke, Matthew Wilkens, David Mimno

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTLDMåŸºå‡†ä»¥è¯„ä¼°LLMåœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡ç†è§£` `å¤§å‹è¯­è¨€æ¨¡å‹` `å°è¯´åˆ†æ` `è¯„ä¼°åŸºå‡†` `å™äº‹ç»“æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å™äº‹ç»“æ„ä¸­ã€‚
2. æœ¬æ–‡æå‡ºTLDMåŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°æ¨¡å‹åœ¨å°è¯´æƒ…èŠ‚æ‘˜è¦å’Œå™äº‹æ—¶é—´ç†è§£çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæµ‹è¯•çš„ä¸ƒä¸ªLLMåœ¨64kæ ‡è®°ä»¥ä¸Šæ— æ³•ç¨³å®šç†è§£ï¼Œæç¤ºéœ€è¦æ–°çš„è¯„ä¼°æ ‡å‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸Šä¸‹æ–‡é•¿åº¦å·²å¢åŠ åˆ°æ•°ç™¾ä¸‡ä¸ªæ ‡è®°ï¼Œä½†åœ¨å¤æ‚é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­è¯„ä¼°å…¶æœ‰æ•ˆæ€§ä»ç„¶å›°éš¾ã€‚æœ¬æ–‡è®¤ä¸ºå°è¯´æ˜¯ä¸€ä¸ªç ”ç©¶å¤æ‚ç»“æ„å’Œé•¿è·ç¦»è¯­ä¹‰ä¾èµ–çš„æ¡ˆä¾‹ï¼Œæå‡ºäº†Too Long, Didn't Modelï¼ˆTLDMï¼‰åŸºå‡†ï¼Œæµ‹è¯•æ¨¡å‹åœ¨æƒ…èŠ‚æ‘˜è¦ã€æ•…äº‹ä¸–ç•Œé…ç½®å’Œå™äº‹æ—¶é—´ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œä¸ƒä¸ªå‰æ²¿LLMåœ¨64kä¸ªæ ‡è®°ä»¥ä¸Šæ— æ³•ä¿æŒç¨³å®šç†è§£ï¼Œæç¤ºè¯­è¨€æ¨¡å‹å¼€å‘è€…åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½æ—¶éœ€è¶…è¶Šä¼ ç»ŸåŸºå‡†ã€‚ä¸ºè¿›ä¸€æ­¥å‘å±•ï¼Œæœ¬æ–‡å‘å¸ƒäº†TLDMåŸºå‡†åŠå‚è€ƒä»£ç å’Œæ•°æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£ä¸­çš„æœ‰æ•ˆæ€§è¯„ä¼°é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å¤„ç†å¤æ‚çš„å™äº‹ç»“æ„å’Œé•¿è·ç¦»ä¾èµ–ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºTLDMåŸºå‡†ï¼Œé€šè¿‡å°è¯´è¿™ä¸€å¤æ‚æ–‡æœ¬ç±»å‹æ¥æµ‹è¯•æ¨¡å‹çš„ç†è§£èƒ½åŠ›ï¼Œå…³æ³¨æƒ…èŠ‚æ‘˜è¦ã€æ•…äº‹ä¸–ç•Œé…ç½®å’Œå™äº‹æ—¶é—´ç­‰æ–¹é¢ï¼Œä»¥æ­¤è¯„ä¼°æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTLDMåŸºå‡†åŒ…å«å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯æ•°æ®é›†æ„å»ºï¼Œé€‰æ‹©å…·æœ‰å¤æ‚ç»“æ„çš„å°è¯´æ–‡æœ¬ï¼›å…¶æ¬¡æ˜¯è¯„ä¼°æŒ‡æ ‡è®¾è®¡ï¼Œé’ˆå¯¹æƒ…èŠ‚å’Œå™äº‹æ—¶é—´è¿›è¡Œé‡åŒ–è¯„ä¼°ï¼›æœ€åæ˜¯æ¨¡å‹æµ‹è¯•ï¼Œä½¿ç”¨ä¸ƒä¸ªå‰æ²¿LLMè¿›è¡Œæ€§èƒ½å¯¹æ¯”ã€‚

**å…³é”®åˆ›æ–°**ï¼šTLDMåŸºå‡†çš„åˆ›æ–°åœ¨äºå…¶é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡çš„å¤æ‚æ€§è¿›è¡Œä¸“é—¨è®¾è®¡ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„â€œè¿·å¤±åœ¨ä¸­é—´â€åŸºå‡†ï¼Œæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé€‰æ‹©äº†å¤šéƒ¨å°è¯´ä½œä¸ºæµ‹è¯•æ•°æ®ï¼Œè®¾ç½®äº†å…·ä½“çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚æƒ…èŠ‚è¿è´¯æ€§å’Œæ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒä¸­ä½¿ç”¨çš„æ¨¡å‹åŒ…æ‹¬æœ€æ–°çš„LLMï¼Œç¡®ä¿ç»“æœçš„å‰æ²¿æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ƒä¸ªå‰æ²¿LLMåœ¨64kæ ‡è®°ä»¥ä¸Šæ— æ³•ä¿æŒç¨³å®šç†è§£ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤æ‚é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­çš„æ˜¾è‘—æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†ç°æœ‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„æ¨¡å‹å¼€å‘æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æ”¹è¿›å¯¹é•¿ä¸Šä¸‹æ–‡çš„ç†è§£ï¼ŒLLMå¯ä»¥åœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸­æä¾›æ›´å‡†ç¡®çš„å“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹è®¾è®¡å’Œè¯„ä¼°æ–¹æ³•çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although the context length of large language models (LLMs) has increased to millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack approaches has proven difficult. We argue that novels provide a case study of subtle, complicated structure and long-range semantic dependencies often over 128k tokens in length. Inspired by work on computational novel analysis, we release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's ability to report plot summary, storyworld configuration, and elapsed narrative time. We find that none of seven tested frontier LLMs retain stable understanding beyond 64k tokens. Our results suggest language model developers must look beyond "lost in the middle" benchmarks when evaluating model performance in complex long-context scenarios. To aid in further development we release the TLDM benchmark together with reference code and data.

