---
layout: default
title: PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks
---

# PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13862" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13862v3</a>
  <a href="https://arxiv.org/pdf/2505.13862.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13862v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13862v3', 'PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guobin Shen, Dongcheng Zhao, Linghao Feng, Xiang He, Jihang Wang, Sicheng Shen, Haibo Tong, Yiting Dong, Jindong Li, Xiang Zheng, Yi Zeng

**åˆ†ç±»**: cs.CR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPandaGuardä»¥ç³»ç»Ÿè¯„ä¼°LLMå®‰å…¨æ€§åº”å¯¹è¶Šç‹±æ”»å‡»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å®‰å…¨æ€§è¯„ä¼°` `è¶Šç‹±æ”»å‡»` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `æ¨¡å—åŒ–æ¡†æ¶` `å¯¹æŠ—æ€§æç¤º` `è¯„åˆ¤ç­–ç•¥` `å®éªŒå¯é‡å¤æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMå®‰å…¨è¯„ä¼°å¾€å¾€ç¼ºä¹ç³»ç»Ÿæ€§ï¼Œé›†ä¸­äºå­¤ç«‹çš„æ”»å‡»æˆ–é˜²å¾¡æŠ€æœ¯ï¼Œå¯¼è‡´åˆ†æç»“æœä¸å¤Ÿå…¨é¢ã€‚
2. PandaGuardæ¡†æ¶é€šè¿‡å°†LLMè¶Šç‹±å®‰å…¨å»ºæ¨¡ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ•´åˆäº†å¤šç§æ”»å‡»å’Œé˜²å¾¡æ–¹æ³•ï¼Œæå‡äº†è¯„ä¼°çš„ç³»ç»Ÿæ€§å’Œå¯é‡å¤æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰å•ä¸€çš„é˜²å¾¡æ–¹æ³•åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½æ˜¯æœ€ä½³çš„ï¼Œè¯„åˆ¤è€…ä¹‹é—´çš„åˆ†æ­§ä¼šå¯¼è‡´å®‰å…¨è¯„ä¼°çš„æ˜¾è‘—å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°ç§°ä¸ºè¶Šç‹±çš„å¯¹æŠ—æ€§æç¤ºçš„æ”»å‡»ï¼Œè¿™äº›æ”»å‡»å¯ä»¥ç»•è¿‡å®‰å…¨å¯¹é½å¹¶å¼•å‘æœ‰å®³è¾“å‡ºã€‚å°½ç®¡LLMå®‰å…¨ç ”ç©¶çš„åŠªåŠ›ä¸æ–­å¢åŠ ï¼Œä½†ç°æœ‰è¯„ä¼°å¾€å¾€æ˜¯é›¶æ•£çš„ï¼Œé›†ä¸­äºå­¤ç«‹çš„æ”»å‡»æˆ–é˜²å¾¡æŠ€æœ¯ï¼Œç¼ºä¹ç³»ç»Ÿæ€§å’Œå¯é‡å¤çš„åˆ†æã€‚æœ¬æ–‡æå‡ºäº†PandaGuardï¼Œä¸€ä¸ªç»Ÿä¸€ä¸”æ¨¡å—åŒ–çš„æ¡†æ¶ï¼Œå°†LLMè¶Šç‹±å®‰å…¨å»ºæ¨¡ä¸ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ”»å‡»è€…ã€é˜²å¾¡è€…å’Œè¯„åˆ¤è€…ã€‚è¯¥æ¡†æ¶å®ç°äº†19ç§æ”»å‡»æ–¹æ³•å’Œ12ç§é˜²å¾¡æœºåˆ¶ï¼Œæ”¯æŒå¤šç§åˆ¤æ–­ç­–ç•¥ï¼Œå¹¶é‡‡ç”¨çµæ´»çš„æ’ä»¶æ¶æ„ï¼Œå¢å¼ºäº†å¯é‡å¤æ€§å’Œå®é™…éƒ¨ç½²çš„å¯èƒ½æ€§ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†PandaBenchï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œè¯„ä¼°49ç§LLMåŠå„ç§åˆ¤æ–­æ–¹æ³•ä¹‹é—´çš„äº¤äº’ï¼Œæ‰§è¡Œéœ€è¦è¶…è¿‡30äº¿ä¸ªæ ‡è®°çš„å®éªŒã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹è„†å¼±æ€§ã€é˜²å¾¡æˆæœ¬ä¸æ€§èƒ½çš„æƒè¡¡ï¼Œä»¥åŠè¯„åˆ¤è€…ä¸€è‡´æ€§çš„é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å¯¹è¶Šç‹±æ”»å‡»æ—¶çš„å®‰å…¨æ€§è¯„ä¼°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹ç³»ç»Ÿæ€§ï¼Œæ— æ³•å…¨é¢è¯„ä¼°ä¸åŒæ”»å‡»å’Œé˜²å¾¡æœºåˆ¶çš„ç›¸äº’ä½œç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPandaGuardæ¡†æ¶é€šè¿‡å°†LLMè¶Šç‹±å®‰å…¨è§†ä¸ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ•´åˆäº†æ”»å‡»è€…ã€é˜²å¾¡è€…å’Œè¯„åˆ¤è€…çš„è§’è‰²ï¼Œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°å¹³å°ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ä¸åŒæ–¹æ³•ä¹‹é—´çš„äº¤äº’å¯ä»¥è¢«ç³»ç»ŸåŒ–åœ°åˆ†æã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPandaGuardæ¡†æ¶åŒ…æ‹¬19ç§æ”»å‡»æ–¹æ³•å’Œ12ç§é˜²å¾¡æœºåˆ¶ï¼Œé‡‡ç”¨çµæ´»çš„æ’ä»¶æ¶æ„ï¼Œæ”¯æŒå¤šç§LLMæ¥å£å’Œäº¤äº’æ¨¡å¼ã€‚å®éªŒè¿‡ç¨‹é€šè¿‡é…ç½®é©±åŠ¨ï¼Œå¢å¼ºäº†å¯é‡å¤æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ¡†æ¶çš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ„å»ºï¼Œä½¿å¾—ä¸åŒçš„æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥å¯ä»¥åœ¨åŒä¸€å¹³å°ä¸Šè¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œè§£å†³äº†ä»¥å¾€ç ”ç©¶çš„é›¶æ•£æ€§é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡†æ¶ä¸­å®ç°çš„æ”»å‡»å’Œé˜²å¾¡æœºåˆ¶å‡ä¸ºæ¨¡å—åŒ–è®¾è®¡ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®éœ€æ±‚è¿›è¡Œé…ç½®ã€‚å®éªŒä¸­ä½¿ç”¨çš„è¯„åˆ¤ç­–ç•¥ä¹Ÿå¤šæ ·åŒ–ï¼Œç¡®ä¿äº†è¯„ä¼°ç»“æœçš„å…¨é¢æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPandaGuardåœ¨è¯„ä¼°49ç§LLMçš„å®‰å…¨æ€§æ—¶ï¼Œæ­ç¤ºäº†å…³é”®çš„æ¨¡å‹è„†å¼±æ€§å’Œé˜²å¾¡æˆæœ¬æ€§èƒ½æƒè¡¡ã€‚è¯„åˆ¤è€…ä¹‹é—´çš„åˆ†æ­§å¯¼è‡´å®‰å…¨è¯„ä¼°ç»“æœçš„æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†å¤šæ ·åŒ–è¯„åˆ¤ç­–ç•¥çš„é‡è¦æ€§ã€‚æ•´ä½“å®éªŒéœ€è¦è¶…è¿‡30äº¿ä¸ªæ ‡è®°ï¼Œç¡®ä¿äº†è¯„ä¼°çš„å…¨é¢æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PandaGuardçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é˜²èŒƒå¯¹æŠ—æ€§æ”»å‡»çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªåŠ¨åŒ–å®¢æœã€å†…å®¹ç”Ÿæˆå’Œç¤¾äº¤åª’ä½“ç­‰é¢†åŸŸã€‚å…¶æ¨¡å—åŒ–è®¾è®¡å’Œçµæ´»æ€§ä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…èƒ½å¤Ÿæ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œå®‰å…¨æ€§æµ‹è¯•å’Œä¼˜åŒ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial prompts known as jailbreaks, which can bypass safety alignment and elicit harmful outputs. Despite growing efforts in LLM safety research, existing evaluations are often fragmented, focused on isolated attack or defense techniques, and lack systematic, reproducible analysis. In this work, we introduce PandaGuard, a unified and modular framework that models LLM jailbreak safety as a multi-agent system comprising attackers, defenders, and judges. Our framework implements 19 attack methods and 12 defense mechanisms, along with multiple judgment strategies, all within a flexible plugin architecture supporting diverse LLM interfaces, multiple interaction modes, and configuration-driven experimentation that enhances reproducibility and practical deployment. Built on this framework, we develop PandaBench, a comprehensive benchmark that evaluates the interactions between these attack/defense methods across 49 LLMs and various judgment approaches, requiring over 3 billion tokens to execute. Our extensive evaluation reveals key insights into model vulnerabilities, defense cost-performance trade-offs, and judge consistency. We find that no single defense is optimal across all dimensions and that judge disagreement introduces nontrivial variance in safety assessments. We release the code, configurations, and evaluation results to support transparent and reproducible research in LLM safety.

