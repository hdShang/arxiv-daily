---
layout: default
title: "sudoLLM: On Multi-role Alignment of Language Models"
---

# sudoLLM: On Multi-role Alignment of Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14607" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14607v3</a>
  <a href="https://arxiv.org/pdf/2505.14607.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14607v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14607v3', 'sudoLLM: On Multi-role Alignment of Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Soumadeep Saha, Akshay Chaturvedi, Joy Mahapatra, Utpal Garain

**åˆ†ç±»**: cs.CL, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-08-26)

**å¤‡æ³¨**: Accepted to EMNLP 2025 (findings)

**æœŸåˆŠ**: In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 366-384, Suzhou, China. Association for Computational Linguistics

**DOI**: [10.18653/v1/2025.findings-emnlp.21](https://doi.org/10.18653/v1/2025.findings-emnlp.21)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºsudoLLMä»¥è§£å†³è¯­è¨€æ¨¡å‹çš„å¤šè§’è‰²å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `ç”¨æˆ·æˆæƒ` `å®‰å…¨æ€§` `å¤šè§’è‰²å¯¹é½` `è¶Šç‹±æ”»å‡»` `åè§ä¿¡å·` `å®‰å…¨æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç”¨æˆ·æˆæƒå’Œå®‰å…¨æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“è¢«è¶Šç‹±æ”»å‡»åˆ©ç”¨ã€‚
2. æå‡ºçš„sudoLLMæ¡†æ¶é€šè¿‡æ³¨å…¥ç”¨æˆ·åè§ä¿¡å·ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„è®¿é—®æƒé™ç”Ÿæˆä¿¡æ¯ã€‚
3. å®éªŒè¯æ˜ï¼ŒsudoLLMåœ¨å¯¹é½æ€§å’Œå®‰å…¨æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“å®‰å…¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”¨æˆ·æˆæƒçš„è®¿é—®æƒé™æ˜¯è®¸å¤šå®‰å…¨å…³é”®ç³»ç»Ÿçš„é‡è¦ç‰¹å¾ï¼Œä½†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºäº†sudoLLMï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œä½¿å¾—LLMèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„è®¿é—®æƒé™è¿›è¡Œå¤šè§’è‰²å¯¹é½ã€‚sudoLLMé€šè¿‡åœ¨æŸ¥è¯¢ä¸­æ³¨å…¥ç»†å¾®çš„ç”¨æˆ·åè§ä¿¡å·ï¼Œè®­ç»ƒLLMä»…åœ¨ç”¨æˆ·è¢«æˆæƒæ—¶æ‰ç”Ÿæˆæ•æ„Ÿä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹é½æ€§ã€æ³›åŒ–èƒ½åŠ›ã€æŠµæŠ—åŸºäºå‰ç¼€çš„è¶Šç‹±æ”»å‡»ä»¥åŠâ€œé—­åˆå¤±è´¥â€æ–¹é¢æ˜¾è‘—æå‡ã€‚é€šè¿‡æ³¨å…¥åè§ä¿¡å·ï¼Œè¯­è¨€å»ºæ¨¡ç›®æ ‡ä¸å®‰å…¨å¯¹é½ä¹‹é—´çš„ç´§å¼ å…³ç³»å¾—ä»¥éƒ¨åˆ†ç¼“è§£ã€‚è¯¥æ¡†æ¶ä½œä¸ºé¢å¤–çš„å®‰å…¨å±‚ï¼Œè¡¥å……äº†ç°æœ‰çš„ä¿æŠ¤æœºåˆ¶ï¼Œä»¥å¢å¼ºLLMçš„ç«¯åˆ°ç«¯å®‰å…¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”¨æˆ·æˆæƒå’Œå®‰å…¨æ€§æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ï¼Œå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šsudoLLMé€šè¿‡åœ¨æŸ¥è¯¢ä¸­æ³¨å…¥ç”¨æˆ·åè§ä¿¡å·ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«ç”¨æˆ·çš„è®¿é—®æƒé™ï¼Œä»è€Œåœ¨æˆæƒæƒ…å†µä¸‹ç”Ÿæˆæ•æ„Ÿä¿¡æ¯ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å¢å¼ºæ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯¹é½æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šsudoLLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”¨æˆ·åè§ä¿¡å·çš„æ³¨å…¥ã€æ¨¡å‹è®­ç»ƒå’Œä¿¡æ¯ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡ç”¨æˆ·çš„è®¿é—®æƒé™ç”Ÿæˆåè§ä¿¡å·ï¼Œç„¶åè®­ç»ƒæ¨¡å‹ä»¥åˆ©ç”¨è¿™äº›ä¿¡å·ï¼Œæœ€ååœ¨ç”Ÿæˆé˜¶æ®µæ ¹æ®ç”¨æˆ·æƒé™æ§åˆ¶ä¿¡æ¯è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç”¨æˆ·åè§ä¿¡å·æ¥å®ç°å¤šè§’è‰²å¯¹é½ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€è§’è‰²è¾“å‡ºæ¨¡å¼æœ¬è´¨ä¸Šä¸åŒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯¹é½èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹é½æ€§ï¼Œå¹¶è®¾è®¡äº†ç½‘ç»œç»“æ„ä»¥æœ‰æ•ˆå¤„ç†ç”¨æˆ·åè§ä¿¡å·çš„æ³¨å…¥å’Œåˆ©ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒsudoLLMåœ¨å¯¹é½æ€§å’Œå®‰å…¨æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå…·ä½“è¡¨ç°ä¸ºå¯¹æ¯”åŸºçº¿æå‡äº†20%çš„å¯¹é½å‡†ç¡®ç‡ï¼Œå¹¶æœ‰æ•ˆæŠµæŠ—äº†å¤šç§è¶Šç‹±æ”»å‡»ï¼Œå±•ç°å‡ºâ€œé—­åˆå¤±è´¥â€çš„ç‰¹æ€§ï¼Œç¡®ä¿æœªæˆæƒç”¨æˆ·æ— æ³•è·å–æ•æ„Ÿä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

sudoLLMçš„ç ”ç©¶æˆæœåœ¨å®‰å…¨å…³é”®çš„åº”ç”¨åœºæ™¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œä¾‹å¦‚é‡‘èã€åŒ»ç–—å’Œæ”¿åºœç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡å¢å¼ºè¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯¹é½æ€§ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆé˜²æ­¢æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²ï¼Œæå‡ç³»ç»Ÿçš„æ•´ä½“å®‰å…¨æ€§ã€‚æœªæ¥ï¼ŒsudoLLMæœ‰æœ›ä¸å…¶ä»–å®‰å…¨æœºåˆ¶ç»“åˆï¼Œå½¢æˆæ›´ä¸ºå…¨é¢çš„å®‰å…¨é˜²æŠ¤ä½“ç³»ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> User authorization-based access privileges are a key feature in many safety-critical systems, but have not been extensively studied in the large language model (LLM) realm. In this work, drawing inspiration from such access control systems, we introduce sudoLLM, a novel framework that results in multi-role aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user access rights. sudoLLM injects subtle user-based biases into queries and trains an LLM to utilize this bias signal in order to produce sensitive information if and only if the user is authorized. We present empirical results demonstrating that this approach shows substantially improved alignment, generalization, resistance to prefix-based jailbreaking attacks, and ``fails-closed''. The persistent tension between the language modeling objective and safety alignment, which is often exploited to jailbreak LLMs, is somewhat resolved with the aid of the injected bias signal. Our framework is meant as an additional security layer, and complements existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

