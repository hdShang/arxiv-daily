---
layout: default
title: The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models
---

# The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14172" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14172v3</a>
  <a href="https://arxiv.org/pdf/2505.14172.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14172v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14172v3', 'The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Adrian Cosma, Stefan Ruseti, Emilian Radoi, Mihai Dascalu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-09-15)

**å¤‡æ³¨**: Accepted at EMNLP 2025 Main as Oral Presentation (Top 15% of accepted papers)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè½»é‡çº§æ¶æ„æ”¹è¿›ä»¥è§£å†³å­—ç¬¦çº§ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å­—ç¬¦çº§æ¨ç†` `æ ‡è®°åŒ–` `æ¶æ„æ”¹è¿›` `æ¦‚å¿µå‡ºç°` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å­—ç¬¦çº§ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºæ ‡è®°åŒ–å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡è½»é‡çº§æ¶æ„æ”¹è¿›æ¥å¢å¼ºå­—ç¬¦çº§æ¨ç†èƒ½åŠ›ï¼Œè§£å†³ä½äº’ä¿¡æ¯é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ”¹è¿›åçš„æ¨¡å‹åœ¨å­—ç¬¦çº§æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†ç†è®ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç®€å•çš„å­—ç¬¦çº§ä»»åŠ¡ï¼ˆå¦‚å­—æ¯è®¡æ•°ï¼‰ä¸Šä»è¡¨ç°ä¸ä½³ï¼ŒåŸå› åœ¨äºå…¶åŸºç¡€çš„æ ‡è®°åŒ–é™åˆ¶ã€‚æœ¬æ–‡å°†è¿™ä¸€é™åˆ¶è§†ä¸ºä½äº’ä¿¡æ¯é—®é¢˜ï¼Œå¹¶é€šè¿‡19ä¸ªåˆæˆä»»åŠ¡åˆ†ææ¦‚å¿µçš„å‡ºç°ã€‚ç ”ç©¶å‘ç°ï¼Œå­—ç¬¦ç»„åˆèƒ½åŠ›åœ¨è®­ç»ƒåæœŸçªç„¶å‡ºç°ï¼Œä¸”åŸºäºæ¸—é€æ¨¡å‹çš„æ¦‚å¿µå‡ºç°æœºåˆ¶èƒ½å¤Ÿè§£é‡Šè¿™ä¸€ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ¶æ„æ”¹è¿›ï¼Œæ˜¾è‘—æå‡äº†å­—ç¬¦çº§æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†å­è¯æ¨¡å‹çš„å½’çº³ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç»“æœä¸ºç†è§£å’Œç¼“è§£æ ‡è®°åŒ–è¯­è¨€æ¨¡å‹çš„ç»“æ„ç›²ç‚¹æä¾›äº†åŸåˆ™æ€§æ¡†æ¶ï¼Œå¹¶å…¬å¼€äº†ä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å­—ç¬¦çº§ä»»åŠ¡ï¼ˆå¦‚å­—æ¯è®¡æ•°ï¼‰ä¸Šçš„è¡¨ç°ä¸è¶³ï¼Œä¸»è¦ç”±äºæ ‡è®°åŒ–å¯¼è‡´çš„ä¿¡æ¯æŸå¤±å’Œä½äº’ä¿¡æ¯é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†ææ¦‚å¿µå‡ºç°çš„æœºåˆ¶ï¼Œæå‡ºä¸€ç§è½»é‡çº§çš„æ¶æ„æ”¹è¿›ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å­—ç¬¦çº§æ¨ç†èƒ½åŠ›ï¼Œè®¤ä¸ºå­¦ä¹ å­—ç¬¦ç»„åˆä¸å­¦ä¹ å¸¸è¯†çŸ¥è¯†å¹¶æ— æœ¬è´¨åŒºåˆ«ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä½¿ç”¨19ä¸ªåˆæˆä»»åŠ¡æ¥éš”ç¦»å­—ç¬¦çº§æ¨ç†ï¼Œåˆ†ææ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡¨ç°å˜åŒ–ï¼Œæå‡ºçš„æ¶æ„æ”¹è¿›åœ¨ä¿æŒå­è¯æ¨¡å‹ä¼˜åŠ¿çš„åŒæ—¶ï¼Œæå‡äº†å­—ç¬¦çº§æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„è°ƒæ•´æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å­—ç¬¦çº§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè§£å†³äº†ä½äº’ä¿¡æ¯çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¶æ„è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–å­—ç¬¦çº§æ¨ç†çš„å­¦ä¹ è¿‡ç¨‹ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒåæœŸèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å­—ç¬¦ç»„åˆçš„è§„å¾‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡æ¶æ„æ”¹è¿›çš„æ¨¡å‹åœ¨å­—ç¬¦çº§æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†20%ä»¥ä¸Šï¼ŒéªŒè¯äº†æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬åˆ†æã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å­—ç¬¦çº§ç†è§£èƒ½åŠ›ï¼Œæ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦æ–‡æœ¬ä»»åŠ¡æ—¶å°†æ›´åŠ å‡†ç¡®ï¼Œæœªæ¥å¯èƒ½å¯¹å¤šè¯­è¨€å¤„ç†å’Œä½èµ„æºè¯­è¨€çš„åº”ç”¨äº§ç”Ÿç§¯æå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their remarkable progress across diverse domains, Large Language Models (LLMs) consistently fail at simple character-level tasks, such as counting letters in words, due to a fundamental limitation: tokenization. In this work, we frame this limitation as a problem of low mutual information and analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks that isolate character-level reasoning in a controlled setting, we show that such capabilities emerge suddenly and only late in training. We find that percolation-based models of concept emergence explain these patterns, suggesting that learning character composition is not fundamentally different from learning commonsense knowledge. To address this bottleneck, we propose a lightweight architectural modification that significantly improves character-level reasoning while preserving the inductive advantages of subword models. Together, our results bridge low-level perceptual gaps in tokenized LMs and provide a principled framework for understanding and mitigating their structural blind spots. We make our code publicly available.

