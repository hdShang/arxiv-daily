---
layout: default
title: "Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models"
---

# Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01761" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01761v2</a>
  <a href="https://arxiv.org/pdf/2505.01761.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01761v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01761v2', 'Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tobias Domhan, Dawei Zhu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03 (æ›´æ–°: 2025-10-03)

**å¤‡æ³¨**: Accepted at EMNLP 2025 (Main Conference)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé•¿æ–‡æœ¬ç¿»è¯‘è¯„ä¼°æ–¹æ³•ä»¥è§£å†³é•¿åº¦åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨ç¿»è¯‘` `è¯„ä¼°æ–¹æ³•` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬é•¿åº¦` `èšç„¦å¥å­æç¤º` `å¾®è°ƒæŠ€æœ¯` `é”™è¯¯è·¨åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨ç¿»è¯‘è¯„ä¼°æ–¹æ³•åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶å­˜åœ¨æ˜¾è‘—çš„é•¿åº¦åå·®ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸ä¸€è‡´ã€‚
2. è®ºæ–‡æå‡ºäº†ç²’åº¦å¯¹é½æç¤ºã€èšç„¦å¥å­æç¤ºå’Œå¾®è°ƒæ–¹æ³•ï¼Œä»¥æ”¹å–„LLMsåœ¨é•¿æ–‡æœ¬ç¿»è¯‘è¯„ä¼°ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æ–¹æ³•åï¼ŒLLMsåœ¨é•¿æ–‡æœ¬è¯„ä¼°ä¸­çš„é”™è¯¯è·¨åº¦å’Œç³»ç»Ÿæ’åå‡†ç¡®æ€§æ˜¾è‘—æé«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡†ç¡®è¯„ä¼°æœºå™¨ç¿»è¯‘æ–‡æœ¬ä¸€ç›´æ˜¯ä¸€ä¸ªé•¿æœŸæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºé•¿æ–‡æ¡£ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥é€šè¿‡MQMé”™è¯¯è·¨åº¦æ³¨é‡Šä½œä¸ºå¯é ä¸”å¯è§£é‡Šçš„å¥å­çº§ç¿»è¯‘è¯„ä¼°å·¥å…·ã€‚éšç€ç°ä»£LLMsæ”¯æŒæ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œæœ¬æ–‡æ¢è®¨äº†æ˜¯å¦å¯ä»¥å°†æ•´ä¸ªæ–‡æ¡£ç¿»è¯‘è¾“å…¥LLMè¿›è¡Œè´¨é‡è¯„ä¼°ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œè¯„ä¼°åº”ä¸æ–‡æœ¬é•¿åº¦æ— å…³ï¼Œèƒ½å¤Ÿäº§ç”Ÿä¸€è‡´çš„é”™è¯¯è·¨åº¦ã€‚ç„¶è€Œï¼Œåˆ†æè¡¨æ˜æ–‡æœ¬é•¿åº¦æ˜¾è‘—å½±å“è¯„ä¼°ï¼šè¾ƒé•¿æ–‡æœ¬å¯¼è‡´é”™è¯¯è·¨åº¦å‡å°‘å’Œç³»ç»Ÿæ’åå‡†ç¡®æ€§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡è¯„ä¼°äº†å‡ ç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ç²’åº¦å¯¹é½æç¤ºã€èšç„¦å¥å­æç¤ºï¼ˆFSPï¼‰å’Œå¾®è°ƒæ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°å°†LLMsä¸è¯„ä¼°ä»»åŠ¡å¯¹é½ã€‚åä¸¤ç§æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç¼“è§£äº†é•¿åº¦åå·®ï¼Œä½¿LLMsåœ¨é•¿æ–‡æœ¬ç¿»è¯‘è¯„ä¼°ä¸­æ›´ä¸ºå¯é ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨ç¿»è¯‘è¯„ä¼°ä¸­ç”±äºæ–‡æœ¬é•¿åº¦å¼•èµ·çš„åå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œè¯„ä¼°ç»“æœå¾€å¾€ä¸ä¸€è‡´ï¼Œå¯¼è‡´é”™è¯¯è·¨åº¦å‡å°‘å’Œç³»ç»Ÿæ’åå‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ”¹è¿›æç¤ºç­–ç•¥å’Œå¾®è°ƒæ–¹æ³•ï¼Œä½¿LLMsèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”é•¿æ–‡æœ¬çš„è¯„ä¼°ä»»åŠ¡ï¼Œä»è€Œå‡å°‘é•¿åº¦å¯¹è¯„ä¼°ç»“æœçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥é•¿æ–‡æœ¬ç¿»è¯‘ï¼Œåº”ç”¨ç²’åº¦å¯¹é½æç¤ºå’Œèšç„¦å¥å­æç¤ºï¼Œæœ€åé€šè¿‡å¾®è°ƒæ¥ä¼˜åŒ–æ¨¡å‹çš„è¯„ä¼°èƒ½åŠ›ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æç¤ºç”Ÿæˆå’Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†èšç„¦å¥å­æç¤ºï¼ˆFSPï¼‰å’Œå¾®è°ƒæ–¹æ³•ï¼Œè¿™ä¸¤è€…æœ‰æ•ˆåœ°ç¼“è§£äº†æ–‡æœ¬é•¿åº¦å¯¹è¯„ä¼°çš„å½±å“ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨é•¿æ–‡æœ¬è¯„ä¼°ä¸­çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„ç»è¿‡è°ƒæ•´ï¼Œä»¥æ”¯æŒæ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£å’Œæ›´å¤æ‚çš„æç¤ºç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨èšç„¦å¥å­æç¤ºå’Œå¾®è°ƒæ–¹æ³•åï¼ŒLLMsåœ¨é•¿æ–‡æœ¬ç¿»è¯‘è¯„ä¼°ä¸­çš„é”™è¯¯è·¨åº¦æ•°é‡å¢åŠ ï¼Œç³»ç»Ÿæ’åå‡†ç¡®æ€§æå‡äº†çº¦20%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ–°æ–¹æ³•æ˜¾è‘—æé«˜äº†è¯„ä¼°çš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨ç¿»è¯‘ç³»ç»Ÿçš„è´¨é‡è¯„ä¼°ã€ç¿»è¯‘åç¼–è¾‘å·¥å…·çš„å¼€å‘ä»¥åŠå¤šè¯­è¨€å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜é•¿æ–‡æœ¬ç¿»è¯‘çš„è¯„ä¼°å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºç¿»è¯‘è¡Œä¸šæä¾›æ›´å¯é çš„å·¥å…·ï¼Œæå‡ç¿»è¯‘è´¨é‡å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurately evaluating machine-translated text remains a long-standing challenge, particularly for long documents. Recent work has shown that large language models (LLMs) can serve as reliable and interpretable sentence-level translation evaluators via MQM error span annotations. With modern LLMs supporting larger context windows, a natural question arises: can we feed entire document translations into an LLM for quality assessment? Ideally, evaluation should be invariant to text length, producing consistent error spans regardless of input granularity. However, our analysis shows that text length significantly impacts evaluation: longer texts lead to fewer error spans and reduced system ranking accuracy. To address this limitation, we evaluate several strategies, including granularity-aligned prompting, Focus Sentence Prompting (FSP), and a fine-tuning approach to better align LLMs with the evaluation task. The latter two methods largely mitigate this length bias, making LLMs more reliable for long-form translation evaluation.

