---
layout: default
title: Accelerating Large Language Model Reasoning via Speculative Search
---

# Accelerating Large Language Model Reasoning via Speculative Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02865" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02865v2</a>
  <a href="https://arxiv.org/pdf/2505.02865.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02865v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02865v2', 'Accelerating Large Language Model Reasoning via Speculative Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhihai Wang, Jie Wang, Jilai Pan, Xilin Xia, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Feng Wu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03 (æ›´æ–°: 2025-05-24)

**å¤‡æ³¨**: Accepted by ICML2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpeculative Searchä»¥åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†åŠ é€Ÿ` `Speculative Search` `è´¨é‡ä¿ç•™æœºåˆ¶` `æ ‘æœç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ ‘æœç´¢çš„æ¨ç†æ–¹æ³•åœ¨ç”Ÿæˆå¤šä¸ªæ¨ç†æ€ç»´æ—¶å­˜åœ¨æ˜¾è‘—çš„æ¨ç†å»¶è¿Ÿï¼Œé™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„Speculative Searchæ¡†æ¶é€šè¿‡å°æ¨¡å‹ä¸å¤§æ¨¡å‹çš„åä½œï¼Œä¼˜åŒ–æ€ç»´ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpecSearchåœ¨Qwenå’ŒLlamaæ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾2.12å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„æ¨ç†è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºæ ‘æœç´¢çš„æ¨ç†æ–¹æ³•æ˜¾è‘—æå‡äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç”±äºç”Ÿæˆå¤§é‡æ¨ç†æ€ç»´ï¼Œå¯¼è‡´æ¨ç†å»¶è¿Ÿæ˜¾è‘—ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„Speculative Searchï¼ˆSpecSearchï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–æ€ç»´ç”Ÿæˆæ˜¾è‘—åŠ é€ŸLLMæ¨ç†ã€‚SpecSearchåˆ©ç”¨å°æ¨¡å‹ä¸å¤§æ¨¡å‹åœ¨æ€ç»´å’Œä»¤ç‰Œå±‚é¢è¿›è¡Œæˆ˜ç•¥æ€§åä½œï¼Œæ•ˆç‡é«˜åœ°ç”Ÿæˆé«˜è´¨é‡æ¨ç†æ€ç»´ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„è´¨é‡ä¿ç•™æ‹’ç»æœºåˆ¶ï¼Œæœ‰æ•ˆè¿‡æ»¤æ‰è´¨é‡ä½äºå¤§æ¨¡å‹è¾“å‡ºçš„æ€ç»´ã€‚å®éªŒè¡¨æ˜ï¼ŒSpecSearchåœ¨ä¿æŒæ¨ç†è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾2.12å€çš„åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºäºæ ‘æœç´¢çš„æ¨ç†æ–¹æ³•åœ¨ç”Ÿæˆå¤šä¸ªæ¨ç†æ€ç»´æ—¶å¯¼è‡´çš„æ˜¾è‘—æ¨ç†å»¶è¿Ÿé—®é¢˜ï¼Œè¿™ä¸€é—®é¢˜ä¸¥é‡é™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„Speculative Searchæ¡†æ¶é€šè¿‡å°æ¨¡å‹ä¸å¤§æ¨¡å‹çš„åä½œï¼Œä¼˜åŒ–æ€ç»´ç”Ÿæˆè¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†é€Ÿåº¦è€Œä¸ç‰ºç‰²æ¨ç†è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpecSearchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå°æ¨¡å‹è´Ÿè´£åˆæ­¥ç”Ÿæˆæ¨ç†æ€ç»´ï¼Œå¤§æ¨¡å‹åˆ™å¯¹è¿™äº›æ€ç»´è¿›è¡Œè´¨é‡è¯„ä¼°å’Œç­›é€‰ã€‚é€šè¿‡è¿™ç§åä½œï¼Œç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„æ¨ç†æ€ç»´ã€‚

**å…³é”®åˆ›æ–°**ï¼šSpecSearchçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†è´¨é‡ä¿ç•™æ‹’ç»æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆè¿‡æ»¤æ‰è´¨é‡ä½äºå¤§æ¨¡å‹è¾“å‡ºçš„æ€ç»´ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆæ€ç»´çš„é«˜è´¨é‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSpecSearchåœ¨æ¨ç†é€Ÿåº¦å’Œè´¨é‡ä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSpecSearchå¯¹å°æ¨¡å‹å’Œå¤§æ¨¡å‹çš„åä½œè¿›è¡Œäº†ç²¾ç»†è°ƒèŠ‚ï¼Œç¡®ä¿å°æ¨¡å‹ç”Ÿæˆçš„æ€ç»´èƒ½å¤Ÿè¢«å¤§æ¨¡å‹æœ‰æ•ˆè¯„ä¼°ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSpecSearchåœ¨Qwenå’ŒLlamaæ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾2.12å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å¤§æ¨¡å‹ç›¸å½“çš„æ¨ç†è´¨é‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼ŒSpeculative Searchèƒ½å¤Ÿæå‡è¿™äº›åº”ç”¨çš„å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tree-search-based reasoning methods have significantly enhanced the reasoning capability of large language models (LLMs) by facilitating the exploration of multiple intermediate reasoning steps, i.e., thoughts. However, these methods suffer from substantial inference latency, as they have to generate numerous reasoning thoughts, severely limiting LLM applicability. To address this challenge, we propose a novel Speculative Search (SpecSearch) framework that significantly accelerates LLM reasoning by optimizing thought generation. Specifically, SpecSearch utilizes a small model to strategically collaborate with a large model at both thought and token levels, efficiently generating high-quality reasoning thoughts. The major pillar of SpecSearch is a novel quality-preserving rejection mechanism, which effectively filters out thoughts whose quality falls below that of the large model's outputs. Moreover, we show that SpecSearch preserves comparable reasoning quality to the large model. Experiments on both the Qwen and Llama models demonstrate that SpecSearch significantly outperforms state-of-the-art approaches, achieving up to 2.12$\times$ speedup with comparable reasoning quality.

