---
layout: default
title: A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency
---

# A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01658" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01658v3</a>
  <a href="https://arxiv.org/pdf/2505.01658.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01658v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01658v3', 'A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03 (æ›´æ–°: 2025-11-26)

**å¤‡æ³¨**: Under review; 106 pages; 46 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/sihyeong/Awesome-LLM-Inference-Engine)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“ä»¥ä¼˜åŒ–æ•ˆç‡å’Œæˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†å¼•æ“` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¼˜åŒ–æ–¹æ³•` `æ€§èƒ½è¯„ä¼°` `å¼€æºæŠ€æœ¯` `å•†ä¸šè§£å†³æ–¹æ¡ˆ` `æœåŠ¡æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†å¼•æ“åœ¨æ»¡è¶³å¤šæ ·åŒ–æœåŠ¡éœ€æ±‚æ—¶é¢ä¸´é€‰æ‹©å›°éš¾ï¼Œä¼˜åŒ–æ–¹æ³•çš„é€‚ç”¨æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡é€šè¿‡å¯¹25ä¸ªæ¨ç†å¼•æ“çš„è¯„ä¼°ï¼Œæå‡ºäº†ç³»ç»ŸåŒ–çš„æ¯”è¾ƒæ¡†æ¶ï¼Œå¸®åŠ©é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–æ–¹æ³•ã€‚
3. ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç‰¹å®šæ¨ç†å¼•æ“åœ¨æ€§èƒ½å’Œæˆæœ¬æ–¹é¢çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæä¾›äº†å®ç”¨çš„æŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¿æ³›åº”ç”¨äºèŠå¤©æœºå™¨äººã€ä»£ç ç”Ÿæˆå™¨å’Œæœç´¢å¼•æ“ç­‰é¢†åŸŸã€‚ç”±äºé“¾å¼æ€ç»´ã€å¤æ‚æ¨ç†å’Œä»£ç†æœåŠ¡ç­‰å·¥ä½œè´Ÿè½½æ˜¾è‘—å¢åŠ äº†æ¨ç†æˆæœ¬ï¼Œä¼˜åŒ–æ–¹æ³•å¦‚å¹¶è¡Œå¤„ç†ã€å‹ç¼©å’Œç¼“å­˜è¢«é‡‡ç”¨ä»¥é™ä½æˆæœ¬ã€‚ç„¶è€Œï¼ŒæœåŠ¡éœ€æ±‚çš„å¤šæ ·æ€§ä½¿å¾—é€‰æ‹©åˆé€‚çš„æ–¹æ³•å˜å¾—å›°éš¾ã€‚æœ¬æ–‡å¯¹25ä¸ªå¼€æºå’Œå•†ä¸šæ¨ç†å¼•æ“è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè€ƒå¯Ÿäº†å…¶æ˜“ç”¨æ€§ã€éƒ¨ç½²ä¾¿åˆ©æ€§ã€é€šç”¨æ€§æ”¯æŒã€å¯æ‰©å±•æ€§ä»¥åŠé€‚åº”ååé‡å’Œå»¶è¿Ÿè®¡ç®—çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¢è®¨äº†æ¯ä¸ªæ¨ç†å¼•æ“çš„è®¾è®¡ç›®æ ‡åŠå…¶æ”¯æŒçš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¹¶è¯„ä¼°äº†å¼€æºæ¨ç†å¼•æ“çš„ç”Ÿæ€æˆç†Ÿåº¦åŠå•†ä¸šè§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ä¸æˆæœ¬ç­–ç•¥ã€‚æœ€åï¼Œæå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬å¯¹å¤æ‚LLMæœåŠ¡çš„æ”¯æŒã€å„ç§ç¡¬ä»¶çš„å…¼å®¹æ€§åŠå®‰å…¨æ€§å¢å¼ºï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“åœ¨å¤šæ ·åŒ–æœåŠ¡éœ€æ±‚ä¸‹çš„é€‰æ‹©å›°éš¾ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¼˜åŒ–æ•ˆç‡å’Œé™ä½æˆæœ¬æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹25ä¸ªå¼€æºå’Œå•†ä¸šæ¨ç†å¼•æ“çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæä¾›ä¸€ä¸ªå…¨é¢çš„æ¯”è¾ƒæ¡†æ¶ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¯„ä¼°å¼•æ“çš„æ˜“ç”¨æ€§ã€éƒ¨ç½²ä¾¿åˆ©æ€§ã€é€šç”¨æ€§æ”¯æŒã€å¯æ‰©å±•æ€§åŠå…¶å¯¹ååé‡å’Œå»¶è¿Ÿçš„é€‚åº”èƒ½åŠ›ã€‚æ¯ä¸ªå¼•æ“çš„è®¾è®¡ç›®æ ‡å’Œæ”¯æŒçš„ä¼˜åŒ–æŠ€æœ¯ä¹Ÿè¢«è¯¦ç»†åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»ŸåŒ–è¯„ä¼°æ¨ç†å¼•æ“çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®ä¸­å¯¹æ¨ç†å¼•æ“çš„ç³»ç»Ÿç ”ç©¶çš„ç©ºç™½ï¼Œæä¾›äº†å®ç”¨çš„æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯„ä¼°è¿‡ç¨‹ä¸­è€ƒè™‘çš„å…³é”®å‚æ•°åŒ…æ‹¬å¼•æ“çš„æ€§èƒ½ã€æˆæœ¬ç­–ç•¥ã€æ”¯æŒçš„ç¡¬ä»¶ç±»å‹åŠå…¶ç”Ÿæ€ç³»ç»Ÿçš„æˆç†Ÿåº¦ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒæŸäº›æ¨ç†å¼•æ“åœ¨ååé‡å’Œå»¶è¿Ÿæ–¹é¢çš„æ€§èƒ½æå‡å¹…åº¦å¯è¾¾30%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ã€‚è¿™ä¸ºå¼€å‘é«˜æ•ˆçš„LLMæœåŠ¡æä¾›äº†é‡è¦çš„å®è¯æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å¼•æ“æä¾›äº†ç³»ç»ŸåŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨èŠå¤©æœºå™¨äººã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–æœåŠ¡ç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†å¼•æ“çš„é€‰æ‹©ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœåŠ¡çš„æ•ˆç‡å’Œé™ä½è¿è¥æˆæœ¬ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workload such as chain-of-throught, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking.This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions.We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: \href{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}.

