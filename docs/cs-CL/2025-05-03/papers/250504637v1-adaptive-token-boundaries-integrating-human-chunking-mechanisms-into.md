---
layout: default
title: "Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs"
---

# Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04637" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04637v1</a>
  <a href="https://arxiv.org/pdf/2505.04637.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04637v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04637v1', 'Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dongxing Yu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

**DOI**: [10.5121/csit.2025.150807](https://doi.org/10.5121/csit.2025.150807)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€è·¨æ¨¡æ€æ ‡è®°åŒ–æ¡†æ¶ä»¥æå‡å¤šæ¨¡æ€LLMsæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `åŠ¨æ€æ ‡è®°åŒ–` `äººç±»è®¤çŸ¥æœºåˆ¶` `è§†è§‰é—®ç­”` `å¤æ‚åœºæ™¯æè¿°` `é€‚åº”æ€§è¾¹ç•Œ` `å±‚æ¬¡è¡¨ç¤º` `è®¤çŸ¥ç§‘å­¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é™æ€æ ‡è®°åŒ–æ–¹æ³•æ— æ³•æœ‰æ•ˆæ¨¡æ‹Ÿäººç±»åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶çš„åŠ¨æ€å’Œä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€è·¨æ¨¡æ€æ ‡è®°åŒ–æ¡†æ¶ï¼Œç»“åˆäº†é€‚åº”æ€§è¾¹ç•Œå’Œå±‚æ¬¡è¡¨ç¤ºï¼Œä»¥æ›´å¥½åœ°åæ˜ äººç±»è®¤çŸ¥æœºåˆ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰é—®ç­”å’Œå¤æ‚åœºæ™¯æè¿°ä»»åŠ¡ä¸Šåˆ†åˆ«æå‡äº†7.8%å’Œ5.3%çš„æ€§èƒ½ï¼Œä¸”æ›´ç¬¦åˆäººç±»çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤šç§æ•°æ®ç±»å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†äººç±»è®¤çŸ¥è¿‡ç¨‹ä¸è®¡ç®—æ–¹æ³•ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿæ€§æ¢è®¨äº†äººç±»è·¨æ¨¡æ€åˆ†å—æœºåˆ¶ä¸MLLMsä¸­çš„æ ‡è®°è¡¨ç¤ºæ–¹æ³•ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚é€šè¿‡å®è¯ç ”ç©¶æ¯”è¾ƒäººç±»è¡¨ç°æ¨¡å¼ä¸æ¨¡å‹è¡Œä¸ºï¼Œæˆ‘ä»¬è¯æ˜äº†ä¼ ç»Ÿé™æ€æ ‡è®°åŒ–æ–¹æ¡ˆé™åˆ¶äº†æ¨¡å‹æ¨¡æ‹Ÿäººç±»ä¿¡æ¯å¤„ç†åŠ¨æ€ã€ä¸Šä¸‹æ–‡æ•æ„Ÿç‰¹æ€§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€è·¨æ¨¡æ€æ ‡è®°åŒ–æ¡†æ¶ï¼Œç»“åˆäº†é€‚åº”æ€§è¾¹ç•Œã€å±‚æ¬¡è¡¨ç¤ºå’ŒåŸºäºè®¤çŸ¥ç§‘å­¦åŸåˆ™çš„å¯¹é½æœºåˆ¶ã€‚å®šé‡è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸”è¡¨ç°å‡ºæ›´ç¬¦åˆäººç±»çš„é”™è¯¯æ¨¡å¼å’Œæ³¨æ„åŠ›åˆ†å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯å¤„ç†æ—¶æ— æ³•æœ‰æ•ˆæ¨¡æ‹Ÿäººç±»åŠ¨æ€è®¤çŸ¥è¿‡ç¨‹çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„é™æ€æ ‡è®°åŒ–æ–¹æ³•é™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§åŠ¨æ€è·¨æ¨¡æ€æ ‡è®°åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€‚åº”æ€§è¾¹ç•Œå’Œå±‚æ¬¡è¡¨ç¤ºæ¥å¢å¼ºæ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„æ•æ„Ÿæ€§ï¼Œä»è€Œæ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé€‚åº”æ€§è¾¹ç•Œæ¨¡å—ã€å±‚æ¬¡è¡¨ç¤ºæ¨¡å—å’Œå¯¹é½æœºåˆ¶æ¨¡å—ã€‚é€‚åº”æ€§è¾¹ç•Œæ¨¡å—è´Ÿè´£æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ ‡è®°è¾¹ç•Œï¼Œå±‚æ¬¡è¡¨ç¤ºæ¨¡å—åˆ™ç”¨äºæ„å»ºå¤šå±‚æ¬¡çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¯¹é½æœºåˆ¶æ¨¡å—ç¡®ä¿ä¸åŒæ¨¡æ€ä¹‹é—´çš„æœ‰æ•ˆä¿¡æ¯æ•´åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠ¨æ€æ ‡è®°åŒ–æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªé€‚åº”è°ƒæ•´æ ‡è®°è¾¹ç•Œï¼Œè¿™ä¸ä¼ ç»Ÿé™æ€æ ‡è®°åŒ–æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŸºäºè®¤çŸ¥ç§‘å­¦çš„è®¾è®¡åŸåˆ™ï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†å¤šæ¨¡æ€å¯¹é½æŸå¤±å’Œä¸Šä¸‹æ–‡é€‚åº”æ€§æŸå¤±ï¼Œç½‘ç»œç»“æ„åˆ™ä½¿ç”¨äº†å¤šå±‚æ¬¡çš„Transformeræ¶æ„ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œçµæ´»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„åŠ¨æ€è·¨æ¨¡æ€æ ‡è®°åŒ–æ¡†æ¶åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šæå‡äº†7.8%ï¼Œåœ¨å¤æ‚åœºæ™¯æè¿°ä»»åŠ¡ä¸Šæå‡äº†5.3%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨é”™è¯¯æ¨¡å¼å’Œæ³¨æ„åŠ›åˆ†å¸ƒä¸Šæ›´ç¬¦åˆäººç±»è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„è®¤çŸ¥ä¸€è‡´æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å›¾åƒæè¿°ç”Ÿæˆå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œæœªæ¥çš„å¤šæ¨¡æ€AIç³»ç»Ÿå°†èƒ½å¤Ÿæä¾›æ›´è‡ªç„¶å’Œé«˜æ•ˆçš„äº¤äº’ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in processing diverse data types, yet significant disparities persist between human cognitive processes and computational approaches to multimodal information integration. This research presents a systematic investigation into the parallels between human cross-modal chunking mechanisms and token representation methodologies in MLLMs. Through empirical studies comparing human performance patterns with model behaviors across visual-linguistic tasks, we demonstrate that conventional static tokenization schemes fundamentally constrain current models' capacity to simulate the dynamic, context-sensitive nature of human information processing. We propose a novel framework for dynamic cross-modal tokenization that incorporates adaptive boundaries, hierarchical representations, and alignment mechanisms grounded in cognitive science principles. Quantitative evaluations demonstrate that our approach yields statistically significant improvements over state-of-the-art models on benchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene Description) while exhibiting more human-aligned error patterns and attention distributions. These findings contribute to the theoretical understanding of the relationship between human cognition and artificial intelligence, while providing empirical evidence for developing more cognitively plausible AI systems.

