---
layout: default
title: High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers
---

# High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01693" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01693v1</a>
  <a href="https://arxiv.org/pdf/2505.01693.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01693v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01693v1', 'High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Brian Wong, Kaito Tanaka

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeBERTa-RADä»¥è§£å†³èƒ¸éƒ¨Xå…‰æŠ¥å‘Šè‡ªåŠ¨æ ‡æ³¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¼ªæ ‡ç­¾ç”Ÿæˆ` `çŸ¥è¯†è’¸é¦` `èƒ¸éƒ¨Xå…‰` `è‡ªç„¶è¯­è¨€å¤„ç†` `åŒ»ç–—å½±åƒåˆ†æ` `é«˜é€šé‡åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•åœ¨è‡ªåŠ¨æ ‡æ³¨èƒ¸éƒ¨Xå…‰æŠ¥å‘Šæ—¶é¢ä¸´é«˜å˜å¼‚æ€§å’Œå¤æ‚æ€§ç­‰æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¦å®šå’Œä¸ç¡®å®šæ€§æ—¶ã€‚
2. è®ºæ–‡æå‡ºçš„DeBERTa-RADæ¡†æ¶é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼ªæ ‡ç­¾ç”Ÿæˆå’ŒDeBERTaçŸ¥è¯†è’¸é¦ï¼Œå®ç°äº†é«˜æ•ˆä¸”å‡†ç¡®çš„æŠ¥å‘Šæ ‡æ³¨ã€‚
3. åœ¨MIMIC-500åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDeBERTa-RADå–å¾—äº†0.9120çš„å®F1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”æ¨ç†é€Ÿåº¦é€‚åˆé«˜é€šé‡åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨æ ‡æ³¨èƒ¸éƒ¨Xå…‰æŠ¥å‘Šå¯¹äºè®­ç»ƒåŸºäºå›¾åƒçš„è¯Šæ–­æ¨¡å‹ã€è¿›è¡Œäººç¾¤å¥åº·ç ”ç©¶å’Œä¸´åºŠå†³ç­–æ”¯æŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•åœ¨å¤„ç†è¿™äº›è‡ªç”±æ–‡æœ¬æŠ¥å‘Šæ—¶é¢ä¸´é«˜å˜å¼‚æ€§ã€å¤æ‚æ€§ä»¥åŠå¦å®šå’Œä¸ç¡®å®šæ€§ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶DeBERTa-RADï¼Œç»“åˆäº†å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼ªæ ‡ç­¾ç”Ÿæˆä¸é«˜æ•ˆçš„DeBERTaçŸ¥è¯†è’¸é¦ï¼Œä»¥å®ç°å‡†ç¡®å¿«é€Ÿçš„èƒ¸éƒ¨Xå…‰æŠ¥å‘Šæ ‡æ³¨ã€‚åœ¨MIMIC-500åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDeBERTa-RADè¾¾åˆ°äº†0.9120çš„å®F1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºè§„åˆ™çš„ç³»ç»Ÿã€å¾®è°ƒçš„å˜æ¢å™¨æ¨¡å‹å’Œç›´æ¥çš„LLMæ¨ç†ï¼ŒåŒæ—¶ä¿æŒé€‚åˆé«˜é€šé‡åº”ç”¨çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤„ç†ä¸ç¡®å®šå‘ç°æ–¹é¢è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³èƒ¸éƒ¨Xå…‰æŠ¥å‘Šçš„è‡ªåŠ¨æ ‡æ³¨é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„è‡ªç”±æ–‡æœ¬æŠ¥å‘Šæ—¶ï¼Œå¸¸å¸¸å—åˆ°é«˜å˜å¼‚æ€§å’Œä¸ç¡®å®šæ€§å½±å“ï¼Œå¯¼è‡´æ ‡æ³¨æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡DeBERTa-RADæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œå¹¶ç»“åˆçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œæå‡æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ŒåŒ…æ‹¬ç¡®å®šæ€§çŠ¶æ€ï¼›ç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨DeBERTa-Baseæ¨¡å‹åœ¨ä¼ªæ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨å®šåˆ¶çš„çŸ¥è¯†è’¸é¦ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼ªæ ‡ç­¾ç”Ÿæˆä¸é«˜æ•ˆçš„çŸ¥è¯†è’¸é¦ç›¸ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œå¤„ç†é€Ÿåº¦ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼Œå¹¶ç¡®ä¿ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å…·æœ‰é«˜è´¨é‡å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MIMIC-500åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDeBERTa-RADå®ç°äº†0.9120çš„å®F1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„ç³»ç»Ÿå’Œå¾®è°ƒçš„å˜æ¢å™¨æ¨¡å‹ï¼Œæå‡å¹…åº¦æ˜æ˜¾ï¼ŒåŒæ—¶ä¿æŒäº†é€‚åˆé«˜é€šé‡åº”ç”¨çš„æ¨ç†é€Ÿåº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å½±åƒåˆ†æã€ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿä»¥åŠå¤§è§„æ¨¡äººç¾¤å¥åº·ç ”ç©¶ã€‚é€šè¿‡æé«˜èƒ¸éƒ¨Xå…‰æŠ¥å‘Šçš„è‡ªåŠ¨æ ‡æ³¨æ•ˆç‡ï¼Œå¯ä»¥åŠ é€ŸåŒ»å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ï¼Œé™ä½äººå·¥æ ‡æ³¨çš„æˆæœ¬å’Œæ—¶é—´ï¼Œæ¨åŠ¨åŒ»ç–—æ™ºèƒ½åŒ–çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automated labeling of chest X-ray reports is essential for enabling downstream tasks such as training image-based diagnostic models, population health studies, and clinical decision support. However, the high variability, complexity, and prevalence of negation and uncertainty in these free-text reports pose significant challenges for traditional Natural Language Processing methods. While large language models (LLMs) demonstrate strong text understanding, their direct application for large-scale, efficient labeling is limited by computational cost and speed. This paper introduces DeBERTa-RAD, a novel two-stage framework that combines the power of state-of-the-art LLM pseudo-labeling with efficient DeBERTa-based knowledge distillation for accurate and fast chest X-ray report labeling. We leverage an advanced LLM to generate high-quality pseudo-labels, including certainty statuses, for a large corpus of reports. Subsequently, a DeBERTa-Base model is trained on this pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, significantly outperforming established rule-based systems, fine-tuned transformer models, and direct LLM inference, while maintaining a practical inference speed suitable for high-throughput applications. Our analysis shows particular strength in handling uncertain findings. This work demonstrates a promising path to overcome data annotation bottlenecks and achieve high-performance medical text processing through the strategic combination of LLM capabilities and efficient student models trained via distillation.

