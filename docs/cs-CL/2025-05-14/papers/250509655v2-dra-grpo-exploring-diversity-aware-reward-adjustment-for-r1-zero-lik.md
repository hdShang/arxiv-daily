---
layout: default
title: DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models
---

# DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09655" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09655v2</a>
  <a href="https://arxiv.org/pdf/2505.09655.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09655v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09655v2', 'DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-14 (æ›´æ–°: 2025-05-16)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xiwenc1/DRA-GRPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRA-GRPOä»¥è§£å†³è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„å¤šæ ·æ€§å¥–åŠ±è°ƒæ•´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±è°ƒæ•´` `å¤šæ ·æ€§æ„ŸçŸ¥` `æ•°å­¦æ¨ç†` `å­æ¨¡äº’ä¿¡æ¯` `ä½èµ„æºç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„GRPOæ–¹æ³•ä¾èµ–äºæ ‡é‡å¥–åŠ±ä¿¡å·ï¼Œæœªèƒ½æœ‰æ•ˆæ•æ‰è¯­ä¹‰å¤šæ ·æ€§ï¼Œå¯¼è‡´å¤šæ ·æ€§-è´¨é‡ä¸ä¸€è‡´é—®é¢˜ã€‚
2. æå‡ºçš„DRAæ–¹æ³•é€šè¿‡å¼•å…¥å­æ¨¡äº’ä¿¡æ¯ï¼Œæ˜¾è‘—å¢å¼ºå¤šæ ·æ€§å®Œæˆçš„å¥–åŠ±ï¼Œä¿ƒè¿›æ›´å¥½çš„æ¢ç´¢ä¸å­¦ä¹ ã€‚
3. åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šï¼ŒDRA-GRPOæ–¹æ³•çš„å¹³å‡å‡†ç¡®ç‡è¾¾åˆ°58.2%ï¼Œè¡¨ç°ä¼˜äºå¤šä¸ªå¼ºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹åè®­ç»ƒä¸­çš„è¿›å±•ï¼Œå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨ä½èµ„æºç¯å¢ƒä¸­å±•ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼ŒGRPOé€šå¸¸ä¾èµ–äºè§£å†³æ–¹æ¡ˆçº§åˆ«å’Œæ ‡é‡å¥–åŠ±ä¿¡å·ï¼Œæœªèƒ½æ•æ‰é‡‡æ ·å®Œæˆä¹‹é—´çš„è¯­ä¹‰å¤šæ ·æ€§ï¼Œå¯¼è‡´æˆ‘ä»¬è¯†åˆ«å‡ºçš„å¤šæ ·æ€§-è´¨é‡ä¸ä¸€è‡´é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±è°ƒæ•´ï¼ˆDRAï¼‰ï¼Œè¯¥æ–¹æ³•æ˜ç¡®å°†è¯­ä¹‰å¤šæ ·æ€§çº³å…¥å¥–åŠ±è®¡ç®—ä¸­ã€‚DRAä½¿ç”¨å­æ¨¡äº’ä¿¡æ¯ï¼ˆSMIï¼‰æ¥é™ä½å†—ä½™å®Œæˆçš„æƒé‡ï¼Œå¹¶å¢å¼ºå¤šæ ·æ€§å®Œæˆçš„å¥–åŠ±ï¼Œä»è€Œé¼“åŠ±æ›´å¥½çš„æ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒå¯¹é«˜è´¨é‡æ ·æœ¬çš„ç¨³å®šåˆ©ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸GRPOåŠå…¶å˜ä½“DR.GRPOæ— ç¼é›†æˆï¼Œå½¢æˆDRA-GRPOå’ŒDGA-DR.GRPOã€‚æˆ‘ä»¬åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œå‘ç°å…¶è¶…è¶Šäº†è¿‘æœŸå¼ºåŸºçº¿ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°58.2%ï¼Œä»…ä½¿ç”¨7000ä¸ªå¾®è°ƒæ ·æœ¬ï¼Œæ€»è®­ç»ƒæˆæœ¬çº¦ä¸º55ç¾å…ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„GRPOæ–¹æ³•åœ¨å¥–åŠ±ä¿¡å·ä¸Šå­˜åœ¨å±€é™ï¼Œæœªèƒ½æœ‰æ•ˆåæ˜ ä¸åŒæ¨ç†è·¯å¾„çš„è¯­ä¹‰å¤šæ ·æ€§ï¼Œå¯¼è‡´å¤šæ ·æ€§-è´¨é‡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDRAæ–¹æ³•é€šè¿‡å¼•å…¥å­æ¨¡äº’ä¿¡æ¯ï¼ˆSMIï¼‰ï¼Œåœ¨å¥–åŠ±è®¡ç®—ä¸­æ˜¾å¼è€ƒè™‘è¯­ä¹‰å¤šæ ·æ€§ï¼Œä»è€Œé™ä½å†—ä½™å®Œæˆçš„æƒé‡ï¼Œå¢å¼ºå¤šæ ·æ€§å®Œæˆçš„å¥–åŠ±ï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„æ¢ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDRA-GRPOæ–¹æ³•æ•´åˆäº†DRAä¸GRPOçš„æ¡†æ¶ï¼Œä¸»è¦åŒ…æ‹¬å¥–åŠ±è°ƒæ•´æ¨¡å—å’Œç­–ç•¥ä¼˜åŒ–æ¨¡å—ã€‚å¥–åŠ±è°ƒæ•´æ¨¡å—è´Ÿè´£è®¡ç®—å¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±ï¼Œè€Œç­–ç•¥ä¼˜åŒ–æ¨¡å—åˆ™åŸºäºè¿™äº›å¥–åŠ±è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDRAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å­æ¨¡äº’ä¿¡æ¯æ¥è°ƒæ•´å¥–åŠ±ï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¥½åœ°æ¢ç´¢å¤šæ ·æ€§ï¼Œä»è€Œæå‡æ•´ä½“æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ ‡é‡å¥–åŠ±æœºåˆ¶æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DRAä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å†—ä½™å®Œæˆçš„æƒé‡è°ƒæ•´å› å­å’Œå¤šæ ·æ€§å¥–åŠ±çš„æ”¾å¤§å› å­ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šä¹Ÿè€ƒè™‘äº†å¤šæ ·æ€§å› ç´ ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šï¼ŒDRA-GRPOæ–¹æ³•çš„å¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†58.2%ï¼Œç›¸æ¯”äºè¿‘æœŸçš„å¼ºåŸºçº¿æœ‰æ˜¾è‘—æå‡ï¼Œä¸”ä»…ä½¿ç”¨7000ä¸ªå¾®è°ƒæ ·æœ¬ï¼Œæ€»è®­ç»ƒæˆæœ¬çº¦ä¸º55ç¾å…ƒï¼Œå±•ç°å‡ºé«˜æ•ˆçš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œè‡ªåŠ¨é—®ç­”ç­‰ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„å­¦ä¹ èƒ½åŠ›ï¼ŒDRA-GRPOæ–¹æ³•èƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´é«˜è´¨é‡çš„ç”Ÿæˆç»“æœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in reinforcement learning for language model post-training, such as Group Relative Policy Optimization (GRPO), have shown promise in low-resource settings. However, GRPO typically relies on solution-level and scalar reward signals that fail to capture the semantic diversity among sampled completions. This leads to what we identify as a diversity-quality inconsistency, where distinct reasoning paths may receive indistinguishable rewards. To address this limitation, we propose $\textit{Diversity-aware Reward Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity into the reward computation. DRA uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning, while maintaining stable exploitation of high-quality samples. Our method integrates seamlessly with both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and $\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning benchmarks and find that it outperforms recent strong baselines. It achieves state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55. The code is available at https://github.com/xiwenc1/DRA-GRPO.

