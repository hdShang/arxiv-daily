---
layout: default
title: CEC-Zero: Chinese Error Correction Solution Based on LLM
---

# CEC-Zero: Chinese Error Correction Solution Based on LLM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09082" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09082v1</a>
  <a href="https://arxiv.org/pdf/2505.09082.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09082v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09082v1', 'CEC-Zero: Chinese Error Correction Solution Based on LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sophie Zhang, Zhiming Lin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-14

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCEC-Zeroä»¥è§£å†³ä¸­æ–‡æ–‡æœ¬è‡ªåŠ¨çº é”™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸­æ–‡æ‹¼å†™çº é”™` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘çº é”™` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹æ³›åŒ–èƒ½åŠ›` `æ— ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸­æ–‡æ‹¼å†™çº é”™æ–¹æ³•åœ¨å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚
2. CEC-Zeroé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä½¿LLMsèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ çº é”™ç­–ç•¥ï¼Œé¿å…äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨RLå¢å¼ºçš„LLMsåœ¨å‡†ç¡®æ€§å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†è¡Œä¸šæ ‡å‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­æ–‡æ–‡æœ¬å¤„ç†æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­æ–‡æ‹¼å†™çº é”™ï¼ˆCSCï¼‰é¢†åŸŸã€‚å°½ç®¡LLMsåœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§ä¸Šä¼˜äºä¼ ç»Ÿçš„BERTæ¨¡å‹ï¼Œä½†åœ¨å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†CEC-Zeroï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿé€šè¿‡è‡ªä¸»é”™è¯¯ç­–ç•¥å­¦ä¹ è¿›è¡Œè‡ªæˆ‘çº é”™ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚é€šè¿‡å°†RLä¸LLMsçš„ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œè¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¹æ ‡æ³¨æ•°æ®æˆ–è¾…åŠ©æ¨¡å‹çš„ä¾èµ–ã€‚å®éªŒè¡¨æ˜ï¼Œå¢å¼ºçš„LLMsåœ¨è¡Œä¸šå¯è¡Œçš„å‡†ç¡®æ€§å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸ºä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­çš„å¯é æ€§ä¼˜åŒ–æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ä¸€çªç ´ä¿ƒè¿›äº†LLMsåœ¨å®é™…ä¸­æ–‡æ–‡æœ¬çº é”™åœºæ™¯ä¸­çš„éƒ¨ç½²ï¼ŒåŒæ—¶ä¸ºè‡ªæˆ‘æ”¹è¿›çš„è¯­è¨€æ¨¡å‹å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸­æ–‡æ‹¼å†™çº é”™ä¸­çš„å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´å’Œçµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCEC-Zeroçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿LLMsèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ çº é”™ç­–ç•¥ï¼Œä»è€Œå®ç°è‡ªæˆ‘çº é”™ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹ä¸å†ä¾èµ–å¤–éƒ¨ç›‘ç£å’Œæ ‡æ³¨æ•°æ®ï¼Œå¢å¼ºäº†å…¶é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹é€šè¿‡ç”Ÿæˆæ–‡æœ¬è¿›è¡Œè‡ªæˆ‘çº é”™ï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–çº é”™ç­–ç•¥ï¼Œæœ€ååœ¨ä¸åŒé¢†åŸŸè¿›è¡Œè¯„ä¼°ä»¥éªŒè¯å…¶æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šCEC-Zeroçš„æœ€å¤§åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä¸LLMsçš„ç”Ÿæˆèƒ½åŠ›ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ— éœ€æ ‡æ³¨æ•°æ®çš„è‡ªæˆ‘æ”¹è¿›æœºåˆ¶ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–æ ‡æ³¨æ•°æ®çš„æ¨¡å‹å½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒCEC-Zeroé‡‡ç”¨äº†ç‰¹å®šçš„å¥–åŠ±æœºåˆ¶æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ æœ‰æ•ˆçš„çº é”™ç­–ç•¥ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥å¹³è¡¡å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨CEC-Zeroçš„LLMsåœ¨ä¸­æ–‡æ‹¼å†™çº é”™ä»»åŠ¡ä¸­è¾¾åˆ°äº†è¡Œä¸šå¯è¡Œçš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨è·¨é¢†åŸŸæµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹æå‡å¹…åº¦è¶…è¿‡20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CEC-Zeroçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºä¸­æ–‡æ–‡æœ¬å¤„ç†é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨çº é”™ã€æ™ºèƒ½å†™ä½œå’Œåœ¨çº¿æ•™è‚²ç­‰åœºæ™¯ä¸­ã€‚å…¶è‡ªæˆ‘æ”¹è¿›çš„ç‰¹æ€§ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä¸æ–­ä¼˜åŒ–ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæ½œåœ¨å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.

