---
layout: default
title: Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction
---

# Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21043" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21043v2</a>
  <a href="https://arxiv.org/pdf/2505.21043.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21043v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21043v2', 'Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sam O'Connor Russell, Naomi Harte

**åˆ†ç±»**: cs.CL, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: Accepted to ACL 2025, Findings of the Association for Computational Linguistics

**æœŸåˆŠ**: In Findings of the Association for Computational Linguistics: ACL 2025, pages 209--221, Vienna, Austria. Association for Computational Linguistics, 10.18653/v1/2025.findings-acl.12

**DOI**: [10.18653/v1/2025.findings-acl.12](https://doi.org/10.18653/v1/2025.findings-acl.12)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMM-VAPä»¥è§£å†³äººæœºäº¤äº’ä¸­çš„é¢„æµ‹è½®æµé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€äº¤äº’` `é¢„æµ‹æ¨¡å‹` `äººæœºäº¤äº’` `è§†è§‰çº¿ç´¢` `è¯­éŸ³å¤„ç†` `é¢éƒ¨è¡¨æƒ…` `è§†é¢‘ä¼šè®®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é¢„æµ‹è½®æµäº¤äº’æ¨¡å‹å¤§å¤šä»…ä¾èµ–è¯­éŸ³ï¼Œå¿½è§†äº†è§†è§‰çº¿ç´¢çš„ä½œç”¨ï¼Œå¯¼è‡´äº¤äº’çš„è‡ªç„¶æ€§ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºMM-VAPï¼Œé€šè¿‡ç»“åˆè¯­éŸ³ä¸è§†è§‰ä¿¡æ¯ï¼ˆå¦‚é¢éƒ¨è¡¨æƒ…å’Œè§†çº¿ï¼‰ï¼Œæå‡é¢„æµ‹è½®æµäº¤äº’çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMM-VAPåœ¨è§†é¢‘ä¼šè®®ä¸­çš„é¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ°84%ï¼Œæ˜¾è‘—é«˜äºéŸ³é¢‘æ¨¡å‹çš„79%ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ²‰é»˜æ—¶é•¿ä¸‹å‡è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è½®æµäº¤äº’æ˜¯ä¸€ä¸ªä¸°å¯Œçš„å¤šæ¨¡æ€è¿‡ç¨‹ã€‚é¢„æµ‹è½®æµäº¤äº’æ¨¡å‹ï¼ˆPTTMsï¼‰æœ‰åŠ©äºè‡ªç„¶çš„äººæœºäº¤äº’ï¼Œä½†å¤§å¤šæ•°ä»…ä¾èµ–äºè¯­éŸ³ã€‚æˆ‘ä»¬æå‡ºäº†MM-VAPï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¯­éŸ³å’Œè§†è§‰çº¿ç´¢ï¼ˆå¦‚é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨å§¿æ€å’Œè§†çº¿ï¼‰çš„å¤šæ¨¡æ€PTTMã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è§†é¢‘ä¼šè®®äº¤äº’ä¸­ï¼ŒMM-VAPçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„ä»…éŸ³é¢‘æ¨¡å‹ï¼ˆ84%å¯¹79%ï¼‰ã€‚ä¸ä¹‹å‰çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬æ ¹æ®è½®æµä¹‹é—´çš„æ²‰é»˜æ—¶é•¿è¿›è¡Œåˆ†ç»„ï¼Œæ˜¾ç¤ºå‡ºè§†è§‰ç‰¹å¾çš„åŠ å…¥ä½¿å¾—MM-VAPåœ¨æ‰€æœ‰è¯´è¯è€…è½¬æ¢çš„æ—¶é•¿ä¸Šå‡ä¼˜äºéŸ³é¢‘æ¨¡å‹ã€‚è¯¦ç»†çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œé¢éƒ¨è¡¨æƒ…ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®æœ€å¤§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„å·¥ä½œå‡è®¾æ˜¯ï¼Œå½“äº¤è°ˆè€…èƒ½å¤Ÿç›¸äº’çœ‹åˆ°æ—¶ï¼Œè§†è§‰çº¿ç´¢å¯¹è½®æµäº¤äº’è‡³å…³é‡è¦ï¼Œå¿…é¡»çº³å…¥ä»¥å®ç°å‡†ç¡®çš„é¢„æµ‹ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†è‡ªåŠ¨è¯­éŸ³å¯¹é½åœ¨PTTMè®­ç»ƒä¸­çš„é€‚ç”¨æ€§ã€‚æ­¤ç ”ç©¶ä»£è¡¨äº†å¯¹å¤šæ¨¡æ€PTTMsçš„é¦–æ¬¡å…¨é¢åˆ†æï¼Œå¹¶å…¬å¼€äº†æ‰€æœ‰ä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰é¢„æµ‹è½®æµäº¤äº’æ¨¡å‹ï¼ˆPTTMsï¼‰ä»…ä¾èµ–è¯­éŸ³è€Œå¿½è§†è§†è§‰ä¿¡æ¯çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMM-VAPï¼Œé€šè¿‡ç»“åˆè¯­éŸ³å’Œè§†è§‰çº¿ç´¢ï¼ˆå¦‚é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨å§¿æ€å’Œè§†çº¿ï¼‰ï¼Œæ¥å¢å¼ºé¢„æµ‹è½®æµäº¤äº’çš„èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯æå‡æ¨¡å‹çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMM-VAPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¯­éŸ³è¾“å…¥æ¨¡å—å’Œè§†è§‰è¾“å…¥æ¨¡å—ï¼Œåˆ†åˆ«æå–éŸ³é¢‘å’Œè§†è§‰ç‰¹å¾ï¼Œç„¶åé€šè¿‡èåˆå±‚å°†ä¸¤è€…ç»“åˆï¼Œæœ€ç»ˆé€šè¿‡é¢„æµ‹å±‚è¾“å‡ºè½®æµäº¤äº’çš„é¢„æµ‹ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºMM-VAPèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆè§†è§‰ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯é¢éƒ¨è¡¨æƒ…ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸åŒæ²‰é»˜æ—¶é•¿ä¸‹çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œä¸ä¼ ç»Ÿçš„ä»…éŸ³é¢‘æ¨¡å‹ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¤šæ¨¡æ€ç‰¹å¾çš„èåˆæ•ˆæœï¼Œå¹¶é€šè¿‡è¯¦ç»†çš„æ¶ˆèå®éªŒç¡®å®šäº†é¢éƒ¨è¡¨æƒ…ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½çš„æœ€å¤§è´¡çŒ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMM-VAPåœ¨è§†é¢‘ä¼šè®®ä¸­çš„è½®æµé¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ°84%ï¼Œç›¸æ¯”äºæœ€å…ˆè¿›çš„éŸ³é¢‘æ¨¡å‹æå‡äº†5ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”åœ¨ä¸åŒæ²‰é»˜æ—¶é•¿çš„æƒ…å†µä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºè§†è§‰ç‰¹å¾çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬äººæœºäº¤äº’ç³»ç»Ÿã€è™šæ‹ŸåŠ©æ‰‹ã€ç¤¾äº¤æœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡æå‡é¢„æµ‹è½®æµäº¤äº’çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä½¿äººæœºäº¤äº’æ›´åŠ è‡ªç„¶æµç•…ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€åŒ»ç–—å’Œå¨±ä¹ç­‰å¤šä¸ªè¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs) facilitate naturalistic human-robot interaction, yet most rely solely on speech. We introduce MM-VAP, a multimodal PTTM which combines speech with visual cues including facial expression, head pose and gaze. We find that it outperforms the state-of-the-art audio-only in videoconferencing interactions (84% vs. 79% hold/shift prediction accuracy). Unlike prior work which aggregates all holds and shifts, we group by duration of silence between turns. This reveals that through the inclusion of visual features, MM-VAP outperforms a state-of-the-art audio-only turn-taking model across all durations of speaker transitions. We conduct a detailed ablation study, which reveals that facial expression features contribute the most to model performance. Thus, our working hypothesis is that when interlocutors can see one another, visual cues are vital for turn-taking and must therefore be included for accurate turn-taking prediction. We additionally validate the suitability of automatic speech alignment for PTTM training using telephone speech. This work represents the first comprehensive analysis of multimodal PTTMs. We discuss implications for future work and make all code publicly available.

