---
layout: default
title: SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations
---

# SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20679" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20679v1</a>
  <a href="https://arxiv.org/pdf/2505.20679.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20679v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20679v1', 'SELF-PERCEPT: Introspection Improves Large Language Models\' Detection of Multi-Person Mental Manipulation in Conversations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Danush Khanna, Pratinav Seth, Sidhaarth Sredharan Murali, Aditya Kumar Guru, Siddharth Shukla, Tanuj Tyagi, Sandeep Chaurasia, Kripabandhu Ghosh

**åˆ†ç±»**: cs.CL, cs.HC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Accepted to ACL 2025 (Main)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/danushkhanna/self-percept)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSELF-PERCEPTä»¥è§£å†³å¤šæ–¹å¯¹è¯ä¸­çš„å¿ƒç†æ“æ§æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¿ƒç†æ“æ§æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šè½®å¯¹è¯` `è‡ªæˆ‘æ„ŸçŸ¥ç†è®º` `æ•°æ®é›†æ„å»º` `å¯¹è¯åˆ†æ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šè½®å¤šæ–¹å¯¹è¯ä¸­è¯†åˆ«å¿ƒç†æ“æ§è¯­è¨€çš„èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ½œåœ¨å—å®³è€…çš„ä¿æŠ¤é¢ä¸´æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºSELF-PERCEPTï¼Œä¸€ä¸ªåŸºäºè‡ªæˆ‘æ„ŸçŸ¥ç†è®ºçš„ä¸¤é˜¶æ®µæç¤ºæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ–¹å¯¹è¯ä¸­æ“æ§è¯­è¨€çš„æ£€æµ‹èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSELF-PERCEPTåœ¨å¤šè½®å¤šæ–¹å¿ƒç†æ“æ§æ£€æµ‹ä¸­æ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¿ƒç†æ“æ§æ˜¯ä¸€ç§å¾®å¦™è€Œæ™®éçš„è™å¾…å½¢å¼ï¼Œå…¶åœ¨å¯¹è¯ä¸­çš„æ£€æµ‹å¯¹äºä¿æŠ¤æ½œåœ¨å—å®³è€…è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ“æ§è¯­è¨€çš„ç»†è…»å’Œä¸Šä¸‹æ–‡ç‰¹å®šæ€§ï¼Œåœ¨å¤æ‚çš„å¤šè½®å¤šæ–¹å¯¹è¯ä¸­è¯†åˆ«æ“æ§æ€§è¯­è¨€å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è€Œè¨€ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†MultiManipæ•°æ®é›†ï¼ŒåŒ…å«220ä¸ªå¤šè½®å¤šæ–¹å¯¹è¯ï¼Œå¹³è¡¡äº†æ“æ§æ€§å’Œéæ“æ§æ€§äº’åŠ¨ã€‚å°½ç®¡ç°æœ‰çš„LLMså¦‚GPT-4oå’ŒLlama-3.1-8Båœ¨èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ“æ§æ£€æµ‹æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SELF-PERCEPTï¼Œä¸€ä¸ªåŸºäºè‡ªæˆ‘æ„ŸçŸ¥ç†è®ºçš„ä¸¤é˜¶æ®µæç¤ºæ¡†æ¶ï¼Œåœ¨å¤šæ–¹å¤šè½®å¿ƒç†æ“æ§æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²å…¬å¼€å¯ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šè½®å¤šæ–¹å¯¹è¯ä¸­å¯¹å¿ƒç†æ“æ§è¯­è¨€çš„æ£€æµ‹ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ“æ§æ€§è¯­è¨€æ—¶ï¼Œå¸¸å› å…¶ç»†è…»å’Œä¸Šä¸‹æ–‡ç‰¹å®šæ€§è€Œè¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„SELF-PERCEPTæ¡†æ¶åŸºäºè‡ªæˆ‘æ„ŸçŸ¥ç†è®ºï¼Œé€šè¿‡ä¸¤é˜¶æ®µæç¤ºç­–ç•¥ï¼Œå¢å¼ºæ¨¡å‹å¯¹æ“æ§æ€§è¯­è¨€çš„è¯†åˆ«èƒ½åŠ›ã€‚è¯¥è®¾è®¡æ—¨åœ¨åˆ©ç”¨è‡ªæˆ‘æ„ŸçŸ¥çš„æ¦‚å¿µï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œè¯†åˆ«æ“æ§è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSELF-PERCEPTæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºåˆæ­¥æç¤ºï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«å¯¹è¯ä¸­çš„æ½œåœ¨æ“æ§æ€§ï¼›ç¬¬äºŒé˜¶æ®µä¸ºæ·±å…¥åˆ†æï¼Œç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ›´ç²¾ç¡®çš„æ“æ§æ€§åˆ¤æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥è‡ªæˆ‘æ„ŸçŸ¥ç†è®ºä½œä¸ºæç¤ºç­–ç•¥çš„åŸºç¡€ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤šæ–¹å¯¹è¯ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰æ“æ§æ€§è¯­è¨€çš„ç»†å¾®å·®åˆ«ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€æç¤ºç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¼ºåŒ–å¯¹æ“æ§æ€§è¯­è¨€çš„æ•æ„Ÿæ€§ï¼ŒåŒæ—¶åœ¨æç¤ºè¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åŠ¨æ€è°ƒæ•´ï¼Œä»¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSELF-PERCEPTåœ¨å¤šè½®å¤šæ–¹å¿ƒç†æ“æ§æ£€æµ‹ä¸­æ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ£€æµ‹å‡†ç¡®ç‡æé«˜äº†çº¦15%ã€‚è¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚å¯¹è¯æ—¶è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“ç›‘æ§ã€å¿ƒç†å¥åº·æ”¯æŒå’Œåœ¨çº¿äº¤æµå¹³å°çš„å®‰å…¨æ€§æå‡ã€‚é€šè¿‡æœ‰æ•ˆæ£€æµ‹å¿ƒç†æ“æ§è¡Œä¸ºï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´å®‰å…¨çš„äº¤æµç¯å¢ƒï¼Œé˜²æ­¢æ½œåœ¨çš„å¿ƒç†è™å¾…å’Œæ“æ§è¡Œä¸ºã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œä¿ƒè¿›äººé™…æ²Ÿé€šçš„å¥åº·å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication, making its detection critical for safeguarding potential victims. However, due to manipulation's nuanced and context-specific nature, identifying manipulative language in complex, multi-turn, and multi-person conversations remains a significant challenge for large language models (LLMs). To address this gap, we introduce the MultiManip dataset, comprising 220 multi-turn, multi-person dialogues balanced between manipulative and non-manipulative interactions, all drawn from reality shows that mimic real-world scenarios. For manipulative interactions, it includes 11 distinct manipulations depicting real-life scenarios. We conduct extensive evaluations of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various prompting strategies. Despite their capabilities, these models often struggle to detect manipulation effectively. To overcome this limitation, we propose SELF-PERCEPT, a novel, two-stage prompting framework inspired by Self-Perception Theory, demonstrating strong performance in detecting multi-person, multi-turn mental manipulation. Our code and data are publicly available at https://github.com/danushkhanna/self-percept .

