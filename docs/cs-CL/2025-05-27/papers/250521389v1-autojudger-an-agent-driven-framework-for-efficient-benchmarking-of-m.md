---
layout: default
title: "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs"
---

# AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21389" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21389v1</a>
  <a href="https://arxiv.org/pdf/2505.21389.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21389v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21389v1', 'AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuanwen Ding, Chengjun Pan, Zejun Li, Jiwen Zhang, Siyuan Wang, Zhongyu Wei

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAutoJudgerä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æˆæœ¬é«˜çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è¯„ä¼°æ¡†æ¶` `é¡¹ç›®ååº”ç†è®º` `åŠ¨æ€é€‰æ‹©` `è¯­ä¹‰æ„ŸçŸ¥æ£€ç´¢` `åŠ¨æ€è®°å¿†` `è¯„ä¼°æ•ˆç‡` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•é¢ä¸´é«˜æ˜‚çš„æˆæœ¬å’Œå¤æ‚æ€§ï¼Œéš¾ä»¥é«˜æ•ˆå¤„ç†ã€‚
2. AutoJudgeré€šè¿‡å¼•å…¥é¡¹ç›®ååº”ç†è®ºå’Œè‡ªä¸»è¯„ä¼°ä»£ç†ï¼ŒåŠ¨æ€é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„é—®é¢˜ï¼Œæå‡è¯„ä¼°æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAutoJudgerä»…ä½¿ç”¨4%çš„æ•°æ®ä¾¿å¯å®ç°è¶…è¿‡90%çš„æ’åå‡†ç¡®ç‡ï¼Œæ˜¾è‘—é™ä½è¯„ä¼°å¼€é”€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åŸºå‡†æµ‹è¯•è§„æ¨¡å’Œè·¨æ¨¡æ€å¤æ‚æ€§çš„å¢åŠ ï¼Œè¯„ä¼°æˆæœ¬æ—¥ç›Šé«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AutoJudgerï¼Œä¸€ä¸ªåŸºäºä»£ç†çš„é«˜æ•ˆè‡ªé€‚åº”è¯„ä¼°æ¡†æ¶ã€‚AutoJudgeré‡‡ç”¨é¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰æ¥ä¼°è®¡é—®é¢˜éš¾åº¦ï¼Œå¹¶é€šè¿‡è‡ªä¸»è¯„ä¼°ä»£ç†åŠ¨æ€é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æµ‹è¯•é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šè¯­ä¹‰æ„ŸçŸ¥æ£€ç´¢æœºåˆ¶ç¡®ä¿æ‰€é€‰é—®é¢˜æ¶µç›–è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¸­çš„å¤šæ ·åŒ–å’ŒæŒ‘æˆ˜æ€§åœºæ™¯ï¼Œä»¥åŠåŠ¨æ€è®°å¿†æ¨¡å—ç»´æŠ¤å…ˆå‰è¯„ä¼°é—®é¢˜çš„ä¸Šä¸‹æ–‡ç»Ÿè®¡ï¼Œä»¥æŒ‡å¯¼æ•´ä¸ªè¯„ä¼°è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§å’Œå…¨å±€ä¿¡æ¯é—®é¢˜é€‰æ‹©ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAutoJudgeråœ¨å››ä¸ªä»£è¡¨æ€§å¤šæ¨¡æ€åŸºå‡†ä¸Šæ˜¾è‘—é™ä½äº†è¯„ä¼°æˆæœ¬ï¼Œä»…ä½¿ç”¨4%çš„æ•°æ®ä¾¿å¯åœ¨MMT-Benchçš„å®Œæ•´è¯„ä¼°ä¸­å®ç°è¶…è¿‡90%çš„æ’åå‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­æˆæœ¬é«˜ã€æ•ˆç‡ä½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åŸºå‡†æµ‹è¯•æ—¶ï¼Œå¾€å¾€éœ€è¦å¤§é‡çš„è¯„åˆ†èµ„æºï¼Œå¯¼è‡´è¯„ä¼°è¿‡ç¨‹ä¸å¤Ÿé«˜æ•ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAutoJudgerçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥é¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰å’Œè‡ªä¸»è¯„ä¼°ä»£ç†ï¼ŒåŠ¨æ€é€‰æ‹©æœ€å…·æŒ‘æˆ˜æ€§å’Œä¿¡æ¯é‡çš„é—®é¢˜ï¼Œä»è€Œé™ä½è¯„ä¼°æˆæœ¬å¹¶æé«˜è¯„ä¼°æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAutoJudgerçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­ä¹‰æ„ŸçŸ¥æ£€ç´¢æœºåˆ¶å’ŒåŠ¨æ€è®°å¿†æ¨¡å—ã€‚å‰è€…ç¡®ä¿æ‰€é€‰é—®é¢˜è¦†ç›–å¤šæ ·åŒ–çš„åœºæ™¯ï¼Œåè€…ç»´æŠ¤ä¸Šä¸‹æ–‡ç»Ÿè®¡ä»¥æŒ‡å¯¼é—®é¢˜é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šAutoJudgerçš„åˆ›æ–°ç‚¹åœ¨äºå…¶åŠ¨æ€é€‰æ‹©é—®é¢˜çš„èƒ½åŠ›ï¼Œé€šè¿‡å®æ—¶æ€§èƒ½åé¦ˆæ¥ä¼˜åŒ–è¯„ä¼°è¿‡ç¨‹ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿé™æ€è¯„ä¼°æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°åº”å¯¹ä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒAutoJudgerä½¿ç”¨IRTæ¥è¯„ä¼°é—®é¢˜éš¾åº¦ï¼Œå¹¶é€šè¿‡åŠ¨æ€è®°å¿†æ¨¡å—è®°å½•å…ˆå‰é—®é¢˜çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä»¥ç¡®ä¿é—®é¢˜é€‰æ‹©çš„è¿è´¯æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAutoJudgeråœ¨å››ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸Šä»…ä½¿ç”¨4%çš„æ•°æ®ä¾¿å®ç°äº†è¶…è¿‡90%çš„æ’åå‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ˜¾è‘—é™ä½äº†è¯„ä¼°å¼€é”€ï¼Œå±•ç°å‡ºæé«˜çš„è¯„ä¼°æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è¯„ä¼°ã€æ•™è‚²é¢†åŸŸçš„è‡ªé€‚åº”æµ‹è¯•ä»¥åŠæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ç›‘æµ‹ã€‚é€šè¿‡é™ä½è¯„ä¼°æˆæœ¬ï¼ŒAutoJudgerèƒ½å¤Ÿä½¿å¾—æ›´å¹¿æ³›çš„ç ”ç©¶è€…å’Œå¼€å‘è€…èƒ½å¤Ÿé«˜æ•ˆåœ°è¯„ä¼°å’Œä¼˜åŒ–å…¶æ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Evaluating multimodal large language models (MLLMs) is increasingly expensive, as the growing size and cross-modality complexity of benchmarks demand significant scoring efforts. To tackle with this difficulty, we introduce AutoJudger, an agent-driven framework for efficient and adaptive benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the Item Response Theory (IRT) to estimate the question difficulty and an autonomous evaluation agent to dynamically select the most informative test questions based on the model's real-time performance. Specifically, AutoJudger incorporates two pivotal components: a semantic-aware retrieval mechanism to ensure that selected questions cover diverse and challenging scenarios across both vision and language modalities, and a dynamic memory that maintains contextual statistics of previously evaluated questions to guide coherent and globally informed question selection throughout the evaluation process. Extensive experiments on four representative multimodal benchmarks demonstrate that our adaptive framework dramatically reduces evaluation expenses, i.e. AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with the full benchmark evaluation on MMT-Bench.

