---
layout: default
title: "Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives"
---

# Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21598" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21598v1</a>
  <a href="https://arxiv.org/pdf/2505.21598.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21598v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21598v1', 'Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yajiao Liu, Congliang Chen, Junchi Yang, Ruoyu Sun

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: The first version of this paper was submitted to ACL ARR 2025 February Submission

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®æ··åˆæ–¹æ³•ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®æ··åˆ` `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹è®­ç»ƒ` `ä¼˜åŒ–ç­–ç•¥` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ•°æ®æ··åˆæ–¹æ³•åœ¨ä¸åŒé¢†åŸŸæ•°æ®çš„é‡‡æ ·æ¯”ä¾‹é€‰æ‹©ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»†è‡´çš„åˆ†ç±»æ–¹æ³•ï¼Œå°†ç°æœ‰æ•°æ®æ··åˆæ–¹æ³•åˆ†ä¸ºç¦»çº¿å’Œåœ¨çº¿ä¸¤å¤§ç±»ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†ã€‚
3. é€šè¿‡å¯¹æ¯”åˆ†æï¼Œæœ¬æ–‡æ€»ç»“äº†å„ç±»æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œæ¥è‡ªä¸åŒé¢†åŸŸçš„æ•°æ®æ··åˆå¯ä»¥æå‡æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨å›ºå®šçš„è®­ç»ƒé¢„ç®—ä¸‹ï¼Œä¸åŒé¢†åŸŸçš„é‡‡æ ·æ¯”ä¾‹å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚æœ¬æ–‡æä¾›äº†ç°æœ‰æ•°æ®æ··åˆæ–¹æ³•çš„å…¨é¢æ¦‚è¿°ï¼Œæå‡ºäº†ä¸€ç§ç»†è‡´çš„åˆ†ç±»æ–¹æ³•ï¼Œå¹¶æ€»ç»“äº†å„ç±»æ–¹æ³•çš„ä¼˜ç¼ºç‚¹åŠé¢ä¸´çš„æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å›ºå®šè®¡ç®—èµ„æºä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆé€‰æ‹©ä¸åŒé¢†åŸŸæ•°æ®çš„é‡‡æ ·æ¯”ä¾‹ï¼Œä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™ä¸€é—®é¢˜ä¸Šç¼ºä¹ç³»ç»Ÿæ€§å’Œç»†è‡´çš„åˆ†ç±»ï¼Œå¯¼è‡´é€‰æ‹©ä¸å½“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ç»†è‡´çš„åˆ†ç±»æ¡†æ¶ï¼Œå°†æ•°æ®æ··åˆæ–¹æ³•åˆ†ä¸ºç¦»çº¿å’Œåœ¨çº¿ä¸¤ç±»ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†ä¸ºå¤šç§å­ç±»ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…æä¾›æ›´æ¸…æ™°çš„é€‰æ‹©ä¾æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ç°æœ‰æ–¹æ³•çš„åˆ†ç±»ã€é—®é¢˜å…¬å¼åŒ–ã€ä»£è¡¨æ€§ç®—æ³•æ€»ç»“åŠä¼˜ç¼ºç‚¹åˆ†æã€‚ç¦»çº¿æ–¹æ³•åˆ†ä¸ºå¯å‘å¼ã€ç®—æ³•åŸºç¡€å’Œå‡½æ•°æ‹Ÿåˆä¸‰ç±»ï¼›åœ¨çº¿æ–¹æ³•åˆ™åˆ†ä¸ºåœ¨çº¿æœ€å°-æœ€å¤§ä¼˜åŒ–ã€åœ¨çº¿æ··åˆæ³•åˆ™åŠå…¶ä»–æ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¯¹ç°æœ‰æ–¹æ³•çš„ç»†è‡´åˆ†ç±»ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç¦»çº¿å’Œåœ¨çº¿åˆ†ç±»ï¼Œå¸®åŠ©ç ”ç©¶è€…æ›´å¥½åœ°ç†è§£å’Œé€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œè®ºæ–‡å¼ºè°ƒäº†å¯¹ä¸åŒé¢†åŸŸæ•°æ®çš„é‡‡æ ·æ¯”ä¾‹çš„ä¼˜åŒ–ï¼Œæå‡ºäº†ç›¸åº”çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æœ‰é™èµ„æºä¸‹çš„æœ€ä½³è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æå‡ºçš„æ•°æ®æ··åˆæ–¹æ³•åï¼Œæ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šã€‚è¿™ä¸€ç»“æœéªŒè¯äº†è®ºæ–‡æå‡ºçš„åˆ†ç±»å’Œä¼˜åŒ–ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®æ··åˆç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œä»è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“æ›´å¤šé¢†åŸŸçš„æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training large language models with data collected from various domains can improve their performance on downstream tasks. However, given a fixed training budget, the sampling proportions of these different domains significantly impact the model's performance. How can we determine the domain weights across different data domains to train the best-performing model within constrained computational resources? In this paper, we provide a comprehensive overview of existing data mixture methods. First, we propose a fine-grained categorization of existing methods, extending beyond the previous offline and online classification. Offline methods are further grouped into heuristic-based, algorithm-based, and function fitting-based methods. For online methods, we categorize them into three groups: online min-max optimization, online mixing law, and other approaches by drawing connections with the optimization frameworks underlying offline methods. Second, we summarize the problem formulations, representative algorithms for each subtype of offline and online methods, and clarify the relationships and distinctions among them. Finally, we discuss the advantages and disadvantages of each method and highlight key challenges in the field of data mixture.

