---
layout: default
title: On VLMs for Diverse Tasks in Multimodal Meme Classification
---

# On VLMs for Diverse Tasks in Multimodal Meme Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20937" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20937v1</a>
  <a href="https://arxiv.org/pdf/2505.20937.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20937v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20937v1', 'On VLMs for Diverse Tasks in Multimodal Meme Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Deepesh Gavit, Debajyoti Mazumder, Samiran Das, Jasabanta Patro

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: 16 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ¨¡å‹ä»¥æå‡è¡¨æƒ…åŒ…åˆ†ç±»æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è¡¨æƒ…åŒ…åˆ†ç±»` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¡¨æƒ…åŒ…åˆ†ç±»æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶å­˜åœ¨æ€§èƒ½ä¸è¶³å’Œç†è§£åå·®çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡VLMç”Ÿæˆè¡¨æƒ…åŒ…å›¾åƒçš„ç†è§£ï¼Œå¹¶ç»“åˆLLMè¿›è¡Œæ–‡æœ¬å¾®è°ƒï¼Œä»¥æå‡åˆ†ç±»å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆVLMå’ŒLLMçš„ç­–ç•¥åœ¨å¤šä¸ªåˆ†ç±»ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶åœ¨æƒ…æ„Ÿåˆ†ç±»ä¸Šæå‡å¹…åº¦è¾¾åˆ°26.24%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ ·åŒ–è¡¨æƒ…åŒ…åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢ç³»ç»Ÿçš„åˆ†æã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡ç”ŸæˆåŸºäºVLMçš„è¡¨æƒ…åŒ…å›¾åƒç†è§£ï¼Œå¹¶å¯¹åµŒå…¥çš„è¡¨æƒ…åŒ…æ–‡æœ¬è¿›è¡Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¾®è°ƒï¼Œä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šä¸€æ˜¯é’ˆå¯¹æ¯ä¸ªå­ä»»åŠ¡å¯¹VLMsè¿›è¡Œå¤šæ ·åŒ–æç¤ºç­–ç•¥çš„åŸºå‡†æµ‹è¯•ï¼›äºŒæ˜¯è¯„ä¼°LoRAå¾®è°ƒåœ¨æ‰€æœ‰VLMç»„ä»¶ä¸­çš„æ€§èƒ½æå‡ï¼›ä¸‰æ˜¯æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨VLMç”Ÿæˆçš„è¯¦ç»†è¡¨æƒ…åŒ…è§£é‡Šæ¥è®­ç»ƒè¾ƒå°çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæ˜¾è‘—æå‡åˆ†ç±»æ•ˆæœã€‚ç»“åˆVLMå’ŒLLMçš„ç­–ç•¥ä½¿å¾—åœ¨è®½åˆºã€æ”»å‡»æ€§å’Œæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šçš„åŸºçº¿æ€§èƒ½åˆ†åˆ«æé«˜äº†8.34%ã€3.52%å’Œ26.24%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¡¨æƒ…åŒ…åˆ†ç±»æ–¹æ³•åœ¨å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹å›¾åƒå’Œæ–‡æœ¬çš„ç†è§£èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´åˆ†ç±»æ€§èƒ½ä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”Ÿæˆå¯¹è¡¨æƒ…åŒ…çš„æ·±åº¦ç†è§£ï¼Œå¹¶å¯¹æ–‡æœ¬è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜åˆ†ç±»æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨VLMå¯¹è¡¨æƒ…åŒ…å›¾åƒè¿›è¡Œç†è§£ï¼›å…¶æ¬¡ï¼Œé’ˆå¯¹åµŒå…¥çš„æ–‡æœ¬è¿›è¡ŒLLMçš„å¾®è°ƒï¼›æœ€åï¼Œç»“åˆVLMç”Ÿæˆçš„è§£é‡Šæ¥è®­ç»ƒè¾ƒå°çš„LLMï¼Œä»¥æå‡åˆ†ç±»æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†VLMç”Ÿæˆçš„è¯¦ç»†è¡¨æƒ…åŒ…è§£é‡Šç”¨äºè®­ç»ƒè¾ƒå°çš„LLMï¼Œè¿™ä¸€ç­–ç•¥æ˜¾è‘—æ”¹å–„äº†åˆ†ç±»æ€§èƒ½ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨LoRAå¾®è°ƒç­–ç•¥ä»¥ä¼˜åŒ–VLMçš„å„ä¸ªç»„ä»¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„ç»¼åˆè€ƒè™‘ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†VLMå’ŒLLMçš„ä¼˜åŠ¿ï¼Œç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ä¸èåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆVLMå’ŒLLMçš„ç­–ç•¥åœ¨è®½åˆºã€æ”»å‡»æ€§å’Œæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šåˆ†åˆ«æå‡äº†8.34%ã€3.52%å’Œ26.24%çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æã€åœ¨çº¿è¯„è®ºæƒ…æ„Ÿè¯†åˆ«ä»¥åŠå¹¿å‘Šæ•ˆæœè¯„ä¼°ç­‰ã€‚é€šè¿‡æå‡å¯¹è¡¨æƒ…åŒ…çš„ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºå¸‚åœºè¥é”€ã€èˆ†æƒ…ç›‘æµ‹ç­‰å®é™…åœºæ™¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present a comprehensive and systematic analysis of vision-language models (VLMs) for disparate meme classification tasks. We introduced a novel approach that generates a VLM-based understanding of meme images and fine-tunes the LLMs on textual understanding of the embedded meme text for improving the performance. Our contributions are threefold: (1) Benchmarking VLMs with diverse prompting strategies purposely to each sub-task; (2) Evaluating LoRA fine-tuning across all VLM components to assess performance gains; and (3) Proposing a novel approach where detailed meme interpretations generated by VLMs are used to train smaller language models (LLMs), significantly improving classification. The strategy of combining VLMs with LLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm, offensive and sentiment classification, respectively. Our results reveal the strengths and limitations of VLMs and present a novel strategy for meme understanding.

