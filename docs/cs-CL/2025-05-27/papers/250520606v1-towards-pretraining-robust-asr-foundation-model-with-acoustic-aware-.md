---
layout: default
title: Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation
---

# Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20606" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20606v1</a>
  <a href="https://arxiv.org/pdf/2505.20606.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20606v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20606v1', 'Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dancheng Liu, Amir Nassereldine, Chenhui Xu, Jinjun Xiong

**åˆ†ç±»**: cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: in submission

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå£°å­¦æ„ŸçŸ¥æ•°æ®å¢å¼ºä»¥æå‡ASRæ¨¡å‹çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨è¯­éŸ³è¯†åˆ«` `å£°å­¦å¢å¼º` `æ•°æ®å¢å¼º` `æ¨¡å‹é²æ£’æ€§` `è¯­éŸ³å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ASRæ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—é«˜ä¸”ä¸æ˜“è·å–ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å£°å­¦æ„ŸçŸ¥çš„æ•°æ®å¢å¼ºæ–¹æ³•æ¥æå‡ASRæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨å£°å­¦å¢å¼ºåï¼Œæ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šçš„å­—é”™è¯¯ç‡é™ä½äº†19.24%ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Whisperåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„å¼ºå¤§æ€§èƒ½é€šå¸¸å½’å› äºå…¶åºå¤§çš„680kå°æ—¶è®­ç»ƒé›†ï¼Œè¿™å¯¹å¤§å¤šæ•°ç ”ç©¶è€…æ¥è¯´å¹¶ä¸ç°å®ã€‚æœ¬æ–‡æ¢è®¨äº†è®­ç»ƒæ•°æ®ä¸­çš„è¯­è¨€å’Œå£°å­¦å¤šæ ·æ€§å¦‚ä½•å½±å“ASRæ¨¡å‹çš„é²æ£’æ€§ï¼Œå‘ç°è½¬å½•æ³›åŒ–ä¸»è¦å—å£°å­¦å˜åŒ–é©±åŠ¨ï¼Œè€Œéè¯­è¨€ä¸°å¯Œæ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œé’ˆå¯¹æ€§çš„å£°å­¦å¢å¼ºæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜ASRæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨960å°æ—¶çš„Librispeechæ•°æ®é›†ä¸Šï¼Œæœªè§æ•°æ®é›†çš„å­—é”™è¯¯ç‡é™ä½äº†å¤šè¾¾19.24%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ä»¥å£°å­¦ä¸ºä¸­å¿ƒçš„æ•°æ®å¢å¼ºä½œä¸ºæ„å»ºé²æ£’ASRæ¨¡å‹çš„æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå°¤å…¶åœ¨ç¼ºä¹å¤§é‡äººç±»è¯­éŸ³æ•°æ®æ—¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰ASRæ¨¡å‹åœ¨é²æ£’æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹è®­ç»ƒæ•°æ®è§„æ¨¡çš„ä¾èµ–æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å£°å­¦æ„ŸçŸ¥çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæå‡ASRæ¨¡å‹åœ¨ä¸åŒå£°å­¦ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå‡å°‘å¯¹å¤§è§„æ¨¡è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å£°å­¦ç‰¹å¾æå–ã€å£°å­¦å¢å¼ºæ¨¡å—å’Œæ¨¡å‹è®­ç»ƒé˜¶æ®µã€‚å£°å­¦å¢å¼ºæ¨¡å—ä¸“æ³¨äºç”Ÿæˆå¤šæ ·åŒ–çš„å£°å­¦æ ·æœ¬ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†é’ˆå¯¹å£°å­¦å˜åŒ–çš„å¢å¼ºæ–¹æ³•ï¼Œè¿™ä¸ä¼ ç»Ÿä¾èµ–è¯­è¨€å¤šæ ·æ€§çš„å¢å¼ºç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å£°å­¦ç‰¹å¾çš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†é€‚åº”æ€§è°ƒæ•´æœºåˆ¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†ä¸åŒçš„å£°å­¦è¾“å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å£°å­¦æ„ŸçŸ¥æ•°æ®å¢å¼ºåï¼ŒASRæ¨¡å‹åœ¨960å°æ—¶çš„Librispeechæ•°æ®é›†ä¸Šï¼Œæœªè§æ•°æ®é›†çš„å­—é”™è¯¯ç‡é™ä½äº†19.24%ã€‚è¿™ä¸€æ˜¾è‘—æå‡è¡¨æ˜ï¼Œå£°å­¦å¢å¼ºæ–¹æ³•åœ¨æå‡æ¨¡å‹é²æ£’æ€§æ–¹é¢å…·æœ‰é‡è¦ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¯­éŸ³åŠ©æ‰‹ã€è‡ªåŠ¨å­—å¹•ç”Ÿæˆã€è¯­éŸ³ç¿»è¯‘ç­‰ï¼Œå°¤å…¶åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸‹ï¼Œå£°å­¦å¢å¼ºæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡ASRç³»ç»Ÿçš„æ€§èƒ½ã€‚æœªæ¥ï¼Œè¿™ä¸€æ–¹æ³•å¯èƒ½ä¸ºæ„å»ºåŸºç¡€ASRæ¨¡å‹æä¾›æ–°çš„æ€è·¯ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Whisper's robust performance in automatic speech recognition (ASR) is often attributed to its massive 680k-hour training set, an impractical scale for most researchers. In this work, we examine how linguistic and acoustic diversity in training data affect the robustness of the ASR model and reveal that transcription generalization is primarily driven by acoustic variation rather than linguistic richness. We find that targeted acoustic augmentation methods could significantly improve the generalization ability of ASR models, reducing word-error rates by up to 19.24 percent on unseen datasets when training on the 960-hour Librispeech dataset. These findings highlight strategic acoustically focused data augmentation as a promising alternative to massive datasets for building robust ASR models, offering a potential solution to future foundation ASR models when massive human speech data is lacking.

