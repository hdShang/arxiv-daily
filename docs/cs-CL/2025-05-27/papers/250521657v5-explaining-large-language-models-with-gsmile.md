---
layout: default
title: Explaining Large Language Models with gSMILE
---

# Explaining Large Language Models with gSMILE

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21657" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21657v5</a>
  <a href="https://arxiv.org/pdf/2505.21657.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21657v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21657v5', 'Explaining Large Language Models with gSMILE')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeinab Dehghani, Mohammed Naveed Akram, Koorosh Aslansefat, Adil Khan, Yiannis Papadopoulos

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-10-21)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºgSMILEä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `gSMILE` `æ–‡æœ¬ç”Ÿæˆ` `æ¨¡å‹æ— å…³` `æ‰°åŠ¨æ–¹æ³•` `Wassersteinè·ç¦»` `å½’å› åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å†³ç­–è¿‡ç¨‹ä¸­çš„ä¸é€æ˜æ€§é™åˆ¶äº†å…¶åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„ä¿¡ä»»å’Œé—®è´£ã€‚
2. gSMILEæ˜¯ä¸€ç§åŸºäºæ‰°åŠ¨çš„æ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶æç¤ºæ‰°åŠ¨å’ŒWassersteinè·ç¦»åº¦é‡å®ç°ä»¤ç‰Œçº§å¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒgSMILEåœ¨å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ä¸Šæä¾›äº†å¯é çš„å½’å› ï¼ŒClaude 2.1åœ¨æ³¨æ„åŠ›ä¿çœŸåº¦ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPTã€LLaMAå’ŒClaudeåœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å†³ç­–è¿‡ç¨‹ä»ç„¶ä¸é€æ˜ï¼Œé™åˆ¶äº†åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„ä¿¡ä»»å’Œé—®è´£ã€‚æœ¬æ–‡æå‡ºäº†gSMILEï¼ˆç”ŸæˆSMILEï¼‰ï¼Œä¸€ç§æ¨¡å‹æ— å…³çš„åŸºäºæ‰°åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°LLMsçš„ä»¤ç‰Œçº§å¯è§£é‡Šæ€§ã€‚gSMILEæ‰©å±•äº†SMILEæ–¹æ³•ï¼Œåˆ©ç”¨å—æ§çš„æç¤ºæ‰°åŠ¨ã€Wassersteinè·ç¦»åº¦é‡å’ŒåŠ æƒçº¿æ€§æ›¿ä»£æ¨¡å‹ï¼Œè¯†åˆ«å¯¹è¾“å‡ºå½±å“æœ€å¤§çš„è¾“å…¥ä»¤ç‰Œã€‚é€šè¿‡ç”Ÿæˆç›´è§‚çš„çƒ­å›¾ï¼ŒgSMILEèƒ½å¤Ÿå¯è§†åŒ–å½±å“ä»¤ç‰Œå’Œæ¨ç†è·¯å¾„ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé¢†å…ˆçš„LLMsä¸Šè¯„ä¼°gSMILEï¼Œç»“æœæ˜¾ç¤ºå…¶æä¾›äº†å¯é çš„äººç±»å¯¹é½å½’å› ï¼Œå°¤å…¶åœ¨æ³¨æ„åŠ›ä¿çœŸåº¦å’Œè¾“å‡ºä¸€è‡´æ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹é€æ˜åº¦ï¼Œå¯¼è‡´ç”¨æˆ·å¯¹æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„ä¿¡ä»»ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šgSMILEé€šè¿‡æ§åˆ¶æç¤ºæ‰°åŠ¨å’ŒWassersteinè·ç¦»åº¦é‡æ¥è¯†åˆ«å¯¹è¾“å‡ºå½±å“æœ€å¤§çš„è¾“å…¥ä»¤ç‰Œï¼Œä»è€Œå®ç°æ›´é«˜çš„å¯è§£é‡Šæ€§ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹æ›´åŠ é€æ˜ï¼Œä¾¿äºç”¨æˆ·ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šgSMILEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå—æ§æç¤ºæ‰°åŠ¨æ¨¡å—ã€Wassersteinè·ç¦»è®¡ç®—æ¨¡å—å’ŒåŠ æƒçº¿æ€§æ›¿ä»£æ¨¡å‹æ¨¡å—ã€‚é€šè¿‡è¿™äº›æ¨¡å—ï¼ŒgSMILEèƒ½å¤Ÿç”Ÿæˆå½±å“ä»¤ç‰Œçš„çƒ­å›¾ï¼Œç›´è§‚å±•ç¤ºæ¨¡å‹çš„æ¨ç†è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šgSMILEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¨¡å‹æ— å…³æ€§å’ŒåŸºäºæ‰°åŠ¨çš„å½’å› æ–¹æ³•ï¼Œè¿™ä¸ç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œåè€…å¾€å¾€ä¾èµ–äºç‰¹å®šæ¨¡å‹çš„å†…éƒ¨æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒgSMILEä½¿ç”¨äº†åŠ æƒçº¿æ€§æ›¿ä»£æ¨¡å‹æ¥æé«˜å½’å› çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡Wassersteinè·ç¦»åº¦é‡æ¥è¯„ä¼°æ‰°åŠ¨å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ï¼Œç¡®ä¿äº†å½’å› ç»“æœçš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒgSMILEåœ¨å¤šä¸ªé¢†å…ˆçš„å¤§è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯Claude 2.1åœ¨æ³¨æ„åŠ›ä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒGPT-3.5åˆ™åœ¨è¾“å‡ºä¸€è‡´æ€§ä¸Šè¾¾åˆ°æœ€é«˜æ°´å¹³ã€‚è¿™è¡¨æ˜gSMILEèƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æ¨¡å‹æ€§èƒ½ä¸å¯è§£é‡Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

gSMILEçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èã€åŒ»ç–—å’Œæ³•å¾‹ç­‰é«˜é£é™©è¡Œä¸šï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ¨¡å‹çš„å¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„é€æ˜åº¦ï¼ŒgSMILEèƒ½å¤Ÿå¸®åŠ©å†³ç­–è€…ç†è§£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå¢å¼ºä¿¡ä»»å’Œé—®è´£ã€‚æœªæ¥ï¼ŒgSMILEå¯èƒ½ä¼šåœ¨æ›´å¤šé¢†åŸŸæ¨å¹¿ï¼Œä¿ƒè¿›AIç³»ç»Ÿçš„é€æ˜æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve remarkable performance in text generation but remain opaque in their decision-making processes, limiting trust and accountability in high-stakes applications. We present gSMILE (generative SMILE), a model-agnostic, perturbation-based framework for token-level interpretability in LLMs. Extending the SMILE methodology, gSMILE uses controlled prompt perturbations, Wasserstein distance metrics, and weighted linear surrogates to identify input tokens with the most significant impact on the output. This process enables the generation of intuitive heatmaps that visually highlight influential tokens and reasoning paths. We evaluate gSMILE across leading LLMs (OpenAI's gpt-3.5-turbo-instruct, Meta's LLaMA 3.1 Instruct Turbo, and Anthropic's Claude 2.1) using attribution fidelity, attribution consistency, attribution stability, attribution faithfulness, and attribution accuracy as metrics. Results show that gSMILE delivers reliable human-aligned attributions, with Claude 2.1 excelling in attention fidelity and GPT-3.5 achieving the highest output consistency. These findings demonstrate gSMILE's ability to balance model performance and interpretability, enabling more transparent and trustworthy AI systems.

