---
layout: default
title: Exploring the Hidden Capacity of LLMs for One-Step Text Generation
---

# Exploring the Hidden Capacity of LLMs for One-Step Text Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21189" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21189v2</a>
  <a href="https://arxiv.org/pdf/2505.21189.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21189v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21189v2', 'Exploring the Hidden Capacity of LLMs for One-Step Text Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gleb Mezentsev, Ivan Oseledets

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-11-01)

**å¤‡æ³¨**: accepted to EMNLP2025 main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢ç´¢LLMsåœ¨ä¸€æ­¥æ–‡æœ¬ç”Ÿæˆä¸­çš„éšè—èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `è‡ªå›å½’è§£ç ` `å¤šæ ‡è®°ç”Ÿæˆ` `åµŒå…¥å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è‡ªå›å½’è§£ç ï¼Œå¯¼è‡´ç”Ÿæˆé€Ÿåº¦æ…¢ä¸”æ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ä»…ä½¿ç”¨ä¸¤ä¸ªå­¦ä¹ åµŒå…¥ï¼Œåˆ©ç”¨å†»ç»“çš„LLMsè¿›è¡Œæ ‡è®°å¹¶è¡Œç”Ÿæˆï¼Œä»è€Œæå‡ç”Ÿæˆæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­èƒ½å¤Ÿç”Ÿæˆæ•°ç™¾ä¸ªå‡†ç¡®æ ‡è®°ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„å¤šæ ‡è®°ç”Ÿæˆèƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡è‡ªå›å½’ç”Ÿæˆï¼Œä»ä»…ä¸€ä¸ªè®­ç»ƒè¾“å…¥åµŒå…¥é‡æ„å‡ºé•¿è¾¾æ•°åƒä¸ªæ ‡è®°çš„æ–‡æœ¬ã€‚æœ¬æ–‡æ¢è®¨è‡ªå›å½’è§£ç åœ¨æ­¤é‡æ„ä¸­çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†å†»ç»“çš„LLMsåœ¨ä»…æä¾›ä¸¤ä¸ªå­¦ä¹ åµŒå…¥æ—¶ï¼Œå¯ä»¥åœ¨ä¸€æ¬¡æ ‡è®°å¹¶è¡Œå‰å‘ä¼ é€’ä¸­ç”Ÿæˆæ•°ç™¾ä¸ªå‡†ç¡®çš„æ ‡è®°ã€‚è¿™æ­ç¤ºäº†è‡ªå›å½’LLMsæœªè¢«å……åˆ†æ¢ç´¢çš„å¤šæ ‡è®°ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹è¿™äº›åµŒå…¥è¿›è¡Œäº†åˆ†æï¼Œå¹¶è¡¨å¾äº†å®ƒä»¬æ‰€ç¼–ç çš„ä¿¡æ¯ã€‚å°½ç®¡è¿™äº›è¡¨ç¤ºå¯¹äºç»™å®šæ–‡æœ¬å¹¶ä¸å”¯ä¸€ï¼Œä½†å®ƒä»¬åœ¨åµŒå…¥ç©ºé—´ä¸­å½¢æˆäº†è¿é€šå’Œå±€éƒ¨åŒºåŸŸï¼Œæš—ç¤ºäº†è®­ç»ƒå®ç”¨ç¼–ç å™¨çš„æ½œåŠ›ã€‚è¿™äº›è¡¨ç¤ºçš„å­˜åœ¨è¡¨æ˜ï¼Œé€šè¿‡å­¦ä¹ è¾“å…¥ç¼–ç å™¨ï¼Œå¤šæ ‡è®°ç”Ÿæˆå¯èƒ½åœ¨ç°æˆçš„LLMsä¸­æœ¬è´¨ä¸Šæ˜¯å¯è®¿é—®çš„ï¼Œä»è€Œæ¶ˆé™¤é‡è®­ç»ƒçš„éœ€æ±‚ï¼Œå¸®åŠ©å…‹æœè‡ªå›å½’è§£ç çš„åŸºæœ¬ç“¶é¢ˆï¼ŒåŒæ—¶é‡ç”¨å·²è®­ç»ƒçš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªå›å½’è§£ç åœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºé€æ­¥ç”Ÿæˆï¼Œå¯¼è‡´ç”Ÿæˆé€Ÿåº¦ç¼“æ…¢ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ¢ç´¢å†»ç»“çš„LLMsåœ¨ä»…ä½¿ç”¨ä¸¤ä¸ªå­¦ä¹ åµŒå…¥çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¦èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„å¤šæ ‡è®°ç”Ÿæˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé¿å…äº†é‡è®­ç»ƒçš„å¤æ‚æ€§ï¼Œæå‡äº†ç”Ÿæˆæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥åµŒå…¥çš„å­¦ä¹ ã€å†»ç»“LLMsçš„å‰å‘ä¼ é€’å’Œç”Ÿæˆæ ‡è®°çš„è¿‡ç¨‹ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬åµŒå…¥ç©ºé—´çš„æ„å»ºå’Œç”Ÿæˆè¿‡ç¨‹çš„ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå‘ç°äº†å†»ç»“LLMsåœ¨å¤šæ ‡è®°ç”Ÿæˆä¸­çš„æ½œåŠ›ï¼Œè¡¨æ˜é€šè¿‡ç®€å•çš„è¾“å…¥ç¼–ç å™¨å¯ä»¥å®ç°é«˜æ•ˆç”Ÿæˆï¼Œä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„åµŒå…¥è¡¨ç¤ºã€ä¼˜åŒ–å‰å‘ä¼ é€’è¿‡ç¨‹ï¼Œä»¥åŠç¡®ä¿ç”Ÿæˆæ ‡è®°çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå†»ç»“çš„LLMsåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­èƒ½å¤Ÿç”Ÿæˆæ•°ç™¾ä¸ªå‡†ç¡®æ ‡è®°ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ•ˆç‡ã€‚ä¸ä¼ ç»Ÿè‡ªå›å½’æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆé€Ÿåº¦æé«˜äº†æ•°å€ï¼Œå±•ç¤ºäº†å¤šæ ‡è®°ç”Ÿæˆçš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å®æ—¶æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿã€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡æå‡ç”Ÿæˆæ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸­å®ç°æ›´å¿«é€Ÿçš„å“åº”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one trained input embedding. In this work, we explore whether autoregressive decoding is essential for such reconstruction. We show that frozen LLMs can generate hundreds of accurate tokens in just one token-parallel forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored multi-token generation capability of autoregressive LLMs. We examine these embeddings and characterize the information they encode. We also empirically show that, although these representations are not unique for a given text, they form connected and local regions in embedding space - suggesting the potential to train a practical encoder. The existence of such representations hints that multi-token generation may be natively accessible in off-the-shelf LLMs via a learned input encoder, eliminating heavy retraining and helping to overcome the fundamental bottleneck of autoregressive decoding while reusing already-trained models.

