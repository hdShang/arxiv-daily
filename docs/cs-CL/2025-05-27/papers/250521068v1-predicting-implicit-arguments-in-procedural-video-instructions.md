---
layout: default
title: Predicting Implicit Arguments in Procedural Video Instructions
---

# Predicting Implicit Arguments in Procedural Video Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21068" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21068v1</a>
  <a href="https://arxiv.org/pdf/2505.21068.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21068v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21068v1', 'Predicting Implicit Arguments in Procedural Video Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anil Batra, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: ACL 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºImplicit-VidSRLæ•°æ®é›†ä»¥è§£å†³éšå¼å‚æ•°é¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšå¼å‚æ•°é¢„æµ‹` `ç¨‹åºæ€§æ–‡æœ¬` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯­ä¹‰è§’è‰²æ ‡æ³¨` `ä¸Šä¸‹æ–‡æ¨ç†` `æ•°æ®é›†æ„å»º` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­ä¹‰è§’è‰²æ ‡æ³¨æ–¹æ³•å¸¸å¸¸å¿½è§†éšå¼è®ºå…ƒï¼Œå¯¼è‡´å¯¹ç¨‹åºæ€§æ–‡æœ¬çš„ç†è§£ä¸å¤Ÿå…¨é¢ã€‚
2. æœ¬æ–‡æå‡ºImplicit-VidSRLæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡ä¸Šä¸‹æ–‡æ¨æ–­éšå¼å’Œæ˜¾å¼è®ºå…ƒï¼Œå¢å¼ºå¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒiSRL-Qwen2-VLæ¨¡å‹åœ¨éšå¼è®ºå…ƒé¢„æµ‹ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼ŒF1-scoreåˆ†åˆ«æé«˜äº†17%å’Œ14.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨‹åºæ€§æ–‡æœ¬æœ‰åŠ©äºAIå¢å¼ºå¯¹ä¸Šä¸‹æ–‡å’ŒåŠ¨ä½œåºåˆ—çš„æ¨ç†èƒ½åŠ›ã€‚å°†è¿™äº›æ–‡æœ¬è½¬åŒ–ä¸ºè¯­ä¹‰è§’è‰²æ ‡æ³¨ï¼ˆSRLï¼‰å¯ä»¥é€šè¿‡è¯†åˆ«è°“è¯-è®ºå…ƒç»“æ„æ¥æ”¹å–„å¯¹å•ä¸ªæ­¥éª¤çš„ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SRLåŸºå‡†å¾€å¾€å¿½è§†éšå¼è®ºå…ƒï¼Œå¯¼è‡´ç†è§£ä¸å®Œæ•´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†Implicit-VidSRLæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è¦æ±‚ä»å¤šæ¨¡æ€çƒ¹é¥ªç¨‹åºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­æ¨æ–­éšå¼å’Œæ˜¾å¼è®ºå…ƒã€‚æˆ‘ä»¬ç ”ç©¶äº†æœ€è¿‘çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å®ƒä»¬åœ¨ç»™å®šåŠ¨è¯çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥é¢„æµ‹å¤šæ¨¡æ€ç¨‹åºæ•°æ®ä¸­çš„éšå¼è®ºå…ƒã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºçš„iSRL-Qwen2-VLæ¨¡å‹åœ¨what-implicitå’Œwhere/with-implicitè¯­ä¹‰è§’è‰²ä¸Šç›¸è¾ƒäºGPT-4oåˆ†åˆ«æé«˜äº†17%å’Œ14.7%çš„F1-scoreã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç¨‹åºæ€§è§†é¢‘æŒ‡ä»¤ä¸­éšå¼å‚æ•°é¢„æµ‹çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸æ— æ³•è¯†åˆ«ä¸Šä¸‹æ–‡ä¸­éšå«çš„ä¿¡æ¯ï¼Œå¯¼è‡´ç†è§£ä¸å®Œæ•´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥Implicit-VidSRLæ•°æ®é›†ï¼Œè¦æ±‚æ¨¡å‹ä»ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­æ¨æ–­éšå¼å’Œæ˜¾å¼è®ºå…ƒï¼Œä»è€Œæå‡å¯¹ç¨‹åºæ€§æ–‡æœ¬çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é›†åŒ…å«å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæ¨¡å‹éœ€è¦é€šè¿‡è§†è§‰å˜åŒ–è¿›è¡Œå®ä½“è·Ÿè¸ªã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†éšå¼è®ºå…ƒçš„æ¦‚å¿µï¼Œå¹¶é€šè¿‡æ–°çš„æ•°æ®é›†å’Œæ¨¡å‹è®¾è®¡æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–éšå¼è®ºå…ƒçš„é¢„æµ‹ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†å¤šæ¨¡æ€èåˆæœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°å¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„äº¤äº’ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒiSRL-Qwen2-VLæ¨¡å‹åœ¨what-implicitå’Œwhere/with-implicitè¯­ä¹‰è§’è‰²çš„F1-scoreä¸Šåˆ†åˆ«æé«˜äº†17%å’Œ14.7%ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹GPT-4oè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨éšå¼å‚æ•°é¢„æµ‹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å¨æˆ¿åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–çƒ¹é¥ªæŒ‡å¯¼å’Œæ•™è‚²åŸ¹è®­ç­‰ã€‚é€šè¿‡æå‡å¯¹ç¨‹åºæ€§æ–‡æœ¬çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®çš„æ“ä½œæŒ‡å¯¼ï¼Œè¿›è€Œæé«˜çƒ¹é¥ªæ•ˆç‡å’Œä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚æœºå™¨äººæ“ä½œå’Œäººæœºäº¤äº’ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Procedural texts help AI enhance reasoning about context and action sequences. Transforming these into Semantic Role Labeling (SRL) improves understanding of individual steps by identifying predicate-argument structure like {verb,what,where/with}. Procedural instructions are highly elliptic, for instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second step's where argument is inferred from the context, referring to where the cucumber was placed. Prior SRL benchmarks often miss implicit arguments, leading to incomplete understanding. To address this, we introduce Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit arguments from contextual information in multimodal cooking procedures. Our proposed dataset benchmarks multimodal models' contextual reasoning, requiring entity tracking through visual changes in recipes. We study recent multimodal LLMs and reveal that they struggle to predict implicit arguments of what and where/with from multi-modal procedural data given the verb. Lastly, we propose iSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for what-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.

