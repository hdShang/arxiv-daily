---
layout: default
title: SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation
---

# SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20622" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20622v1</a>
  <a href="https://arxiv.org/pdf/2505.20622.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20622v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20622v1', 'SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ting Xu, Zhichao Huang, Jiankai Sun, Shanbo Cheng, Wai Lam

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Accepted by The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSeqPO-SiMTä»¥è§£å†³åŒæ­¥æœºå™¨ç¿»è¯‘ä¸­çš„å»¶è¿Ÿä¸è´¨é‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŒæ­¥æœºå™¨ç¿»è¯‘` `åºåˆ—å†³ç­–` `å¼ºåŒ–å­¦ä¹ ` `ç¿»è¯‘è´¨é‡` `å»¶è¿Ÿä¼˜åŒ–` `å®šåˆ¶å¥–åŠ±` `å¤šæ­¥ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒæ­¥æœºå™¨ç¿»è¯‘æ–¹æ³•åœ¨å¤„ç†å¤šæ­¥å†³ç­–æ—¶é¢ä¸´å»¶è¿Ÿå’Œç¿»è¯‘è´¨é‡çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶ç¿»è¯‘åœºæ™¯ä¸­ã€‚
2. SeqPO-SiMTå°†åŒæ­¥æœºå™¨ç¿»è¯‘è§†ä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œé€šè¿‡å®šåˆ¶å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–ç¿»è¯‘è¿‡ç¨‹ï¼Œæå‡ç¿»è¯‘è´¨é‡å¹¶é™ä½å»¶è¿Ÿã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSeqPO-SiMTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œç¿»è¯‘è´¨é‡æ˜¾è‘—é«˜äºç›‘ç£å¾®è°ƒæ¨¡å‹ï¼ŒåŒæ—¶å‡å°‘äº†ç¿»è¯‘å»¶è¿Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†åŒæ­¥æœºå™¨ç¿»è¯‘çš„åºåˆ—ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼ˆSeqPO-SiMTï¼‰ï¼Œå°†åŒæ­¥æœºå™¨ç¿»è¯‘ä»»åŠ¡å®šä¹‰ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œç»“åˆå®šåˆ¶å¥–åŠ±ä»¥æé«˜ç¿»è¯‘è´¨é‡å¹¶é™ä½å»¶è¿Ÿã€‚ä¸é€šå¸¸åº”ç”¨äºå•æ­¥ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚PPOå’ŒDPOï¼‰ä¸åŒï¼ŒSeqPO-SiMTæœ‰æ•ˆåœ°å¤„ç†å¤šæ­¥çš„åŒæ­¥æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚è¯¥æ¡†æ¶ä½¿å¾—åŒæ­¥æœºå™¨ç¿»è¯‘çš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿå’Œä¼˜åŒ–ç¿»è¯‘è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜SeqPO-SiMTåœ¨ç¿»è¯‘è´¨é‡å’Œå»¶è¿Ÿæ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨NEWSTEST2021æ•°æ®é›†ä¸­ï¼ŒSeqPO-SiMTåœ¨COMETæŒ‡æ ‡ä¸Šæ¯”ç›‘ç£å¾®è°ƒæ¨¡å‹é«˜å‡º1.13åˆ†ï¼ŒåŒæ—¶å¹³å‡å»¶è¿Ÿå‡å°‘äº†6.17ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åŒæ­¥æœºå™¨ç¿»è¯‘ï¼ˆSiMTï¼‰ä¸­å»¶è¿Ÿä¸ç¿»è¯‘è´¨é‡ä¹‹é—´çš„çŸ›ç›¾ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–äºå•æ­¥å†³ç­–ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤šæ­¥ç¿»è¯‘ä»»åŠ¡ï¼Œå¯¼è‡´ç¿»è¯‘è´¨é‡ä¸ç¨³å®šä¸”å»¶è¿Ÿè¾ƒé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSeqPO-SiMTé€šè¿‡å°†SiMTä»»åŠ¡å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œåˆ©ç”¨å®šåˆ¶å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–ç¿»è¯‘è¿‡ç¨‹ï¼Œå…è®¸æ¨¡å‹åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­è¿›è¡Œè‡ªæˆ‘è°ƒæ•´ï¼Œä»è€Œæé«˜ç¿»è¯‘è´¨é‡å¹¶é™ä½å»¶è¿Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯è¾“å…¥å¤„ç†æ¨¡å—ï¼Œå°†æºè¯­è¨€æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„æ ¼å¼ï¼›æ¥ç€æ˜¯å†³ç­–æ¨¡å—ï¼ŒåŸºäºå½“å‰ä¸Šä¸‹æ–‡ç”Ÿæˆç¿»è¯‘ï¼›æœ€åæ˜¯å¥–åŠ±è¯„ä¼°æ¨¡å—ï¼Œæ ¹æ®å®šåˆ¶å¥–åŠ±åé¦ˆè°ƒæ•´ç¿»è¯‘ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šSeqPO-SiMTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å°†åŒæ­¥æœºå™¨ç¿»è¯‘ä»»åŠ¡è§†ä¸ºå¤šæ­¥å†³ç­–é—®é¢˜ï¼Œé‡‡ç”¨å®šåˆ¶å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å®æ—¶ç¿»è¯‘ä¸­çš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿçš„å•æ­¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSeqPO-SiMTä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç¿»è¯‘è´¨é‡ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”å¤šæ­¥å†³ç­–çš„éœ€æ±‚ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†ä¸Šä¸‹æ–‡æ—¶çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSeqPO-SiMTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼Œå…·ä½“è€Œè¨€ï¼Œåœ¨NEWSTEST2021æ•°æ®é›†ä¸­ï¼ŒSeqPO-SiMTåœ¨COMETæŒ‡æ ‡ä¸Šæé«˜äº†1.13åˆ†ï¼ŒåŒæ—¶å¹³å‡å»¶è¿Ÿå‡å°‘äº†6.17ï¼Œå±•ç¤ºäº†å…¶åœ¨ç¿»è¯‘è´¨é‡å’Œæ•ˆç‡ä¸Šçš„åŒé‡ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®æ—¶ç¿»è¯‘ã€åœ¨çº¿å®¢æœå’Œå¤šè¯­è¨€äº¤æµå¹³å°ç­‰ã€‚é€šè¿‡æé«˜ç¿»è¯‘è´¨é‡å’Œé™ä½å»¶è¿Ÿï¼ŒSeqPO-SiMTèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨è·¨è¯­è¨€æ²Ÿé€šçš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„åˆ›æ–°æ€è·¯ä¹Ÿä¸ºå…¶ä»–éœ€è¦å®æ—¶å†³ç­–çš„ä»»åŠ¡æä¾›äº†å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.

