---
layout: default
title: Test-Time Learning for Large Language Models
---

# Test-Time Learning for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20633" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20633v1</a>
  <a href="https://arxiv.org/pdf/2505.20633.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20633v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20633v1', 'Test-Time Learning for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinwu Hu, Zhitian Zhang, Guohao Chen, Xutao Wen, Chao Shuai, Wei Luo, Bin Xiao, Yuanqing Li, Mingkui Tan

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Accepted by ICML2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ‹è¯•æ—¶å­¦ä¹ æ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æµ‹è¯•æ—¶å­¦ä¹ ` `æ— æ ‡ç­¾æ•°æ®` `ä½ç§©é€‚åº”` `é¢†åŸŸé€‚åº”` `è‡ªæˆ‘ç›‘ç£å­¦ä¹ ` `æ ·æœ¬é«˜æ•ˆå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥å¤„ç†è¯­è¨€çš„å¤šæ ·æ€§å’Œåˆ†å¸ƒå˜åŒ–ã€‚
2. æœ¬æ–‡æå‡ºçš„æµ‹è¯•æ—¶å­¦ä¹ æ–¹æ³•é€šè¿‡æœ€å°åŒ–è¾“å…¥å›°æƒ‘åº¦ï¼Œåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®è‡ªæˆ‘å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTLMåœ¨é¢†åŸŸçŸ¥è¯†é€‚åº”ä¸Šç›¸è¾ƒäºåŸå§‹LLMsæ€§èƒ½æå‡è‡³å°‘20%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¹¿æ³›çš„é¢„è®­ç»ƒå±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„æ³›åŒ–å’Œå¤„ç†è¯­è¨€å˜å¼‚æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶å­¦ä¹ ï¼ˆTTLï¼‰èŒƒå¼ï¼Œç§°ä¸ºTLMï¼Œæ—¨åœ¨åˆ©ç”¨æ— æ ‡ç­¾æµ‹è¯•æ•°æ®åŠ¨æ€é€‚åº”ç›®æ ‡é¢†åŸŸã€‚æˆ‘ä»¬æä¾›äº†å®è¯è¯æ®å’Œç†è®ºè§è§£ï¼Œè¡¨æ˜é€šè¿‡æœ€å°åŒ–è¾“å…¥å›°æƒ‘åº¦å¯ä»¥æé«˜LLMsçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ ·æœ¬é«˜æ•ˆå­¦ä¹ ç­–ç•¥ï¼Œé‡ç‚¹é€‰æ‹©é«˜å›°æƒ‘åº¦æ ·æœ¬è¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚æœ€åï¼Œä¸ºäº†å‡è½»ç¾éš¾æ€§é—å¿˜å¹¶ç¡®ä¿é€‚åº”çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œè½»é‡çº§æ¨¡å‹æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTLMåœ¨é¢†åŸŸçŸ¥è¯†é€‚åº”æ–¹é¢æ¯”åŸå§‹LLMsæå‡è‡³å°‘20%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ— æ ‡ç­¾æ•°æ®æ—¶é¢ä¸´å›°æƒ‘åº¦é«˜å¯¼è‡´çš„é¢„æµ‹ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æœ€å°åŒ–è¾“å…¥å›°æƒ‘åº¦æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œåˆ©ç”¨æ— æ ‡ç­¾æµ‹è¯•æ•°æ®è¿›è¡Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼Œä»è€Œå®ç°åŠ¨æ€é€‚åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®è¾“å…¥ã€å›°æƒ‘åº¦è®¡ç®—ã€æ ·æœ¬é€‰æ‹©å’Œæ¨¡å‹æ›´æ–°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆè®¡ç®—æ— æ ‡ç­¾æ•°æ®çš„å›°æƒ‘åº¦ï¼Œç„¶åé€‰æ‹©é«˜å›°æƒ‘åº¦æ ·æœ¬è¿›è¡Œæ¨¡å‹ä¼˜åŒ–ï¼Œæœ€åè¿›è¡Œè½»é‡çº§æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ ·æœ¬é«˜æ•ˆå­¦ä¹ ç­–ç•¥ï¼Œä¸»åŠ¨é€‰æ‹©é«˜å›°æƒ‘åº¦æ ·æœ¬è¿›è¡Œä¼˜åŒ–ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„è¢«åŠ¨å­¦ä¹ å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Œé¿å…å…¨å‚æ•°ä¼˜åŒ–å¸¦æ¥çš„ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„åŸå§‹çŸ¥è¯†ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šä¾§é‡äºå›°æƒ‘åº¦æœ€å°åŒ–ï¼Œä»¥æå‡é¢„æµ‹å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTLMåœ¨é¢†åŸŸçŸ¥è¯†é€‚åº”æ–¹é¢çš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œè¾ƒåŸå§‹å¤§è¯­è¨€æ¨¡å‹æé«˜è‡³å°‘20%ã€‚é€šè¿‡å¼•å…¥æ ·æœ¬é«˜æ•ˆå­¦ä¹ ç­–ç•¥ï¼Œæ¨¡å‹åœ¨å¤„ç†é«˜å›°æƒ‘åº¦æ ·æœ¬æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„ä¼˜åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œé¢†åŸŸç‰¹å®šçš„çŸ¥è¯†æå–ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æ»¡è¶³è¡Œä¸šéœ€æ±‚ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å‘å±•ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€åŒ»ç–—å’Œé‡‘èç­‰å¤šä¸ªé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training, they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.

