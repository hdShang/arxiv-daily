---
layout: default
title: "Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities"
---

# Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21191" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21191v1</a>
  <a href="https://arxiv.org/pdf/2505.21191.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21191v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21191v1', 'Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM\'s Instruction-Following Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junyan Zhang, Yubo Gao, Yibo Yan, Jungang Li, Zhaorui Hou, Sicheng Tao, Shuliang Liu, Song Dai, Yonghua Hei, Junzhuo Li, Xuming Hu

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPARCOMæ¡†æ¶ä»¥æ­ç¤ºLLMæŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„ç¨€ç–ç¥ç»å…ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æŒ‡ä»¤éµå¾ª` `ç¨€ç–ç¥ç»å…ƒ` `å¾®è°ƒ` `åˆ†ææ¡†æ¶` `HexaInst` `SPARCOM`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›æå‡æœºåˆ¶ç†è§£ä¸è¶³ï¼Œç¼ºä¹ç³»ç»Ÿåˆ†æã€‚
2. è®ºæ–‡æå‡ºSPARCOMæ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«å’Œåˆ†æç¨€ç–ç¥ç»å…ƒåŠä¸“å®¶ï¼Œæ·±å…¥æ¢è®¨å¾®è°ƒå¯¹LLMè®¡ç®—çš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç¨€ç–ç»„ä»¶åœ¨æŒ‡ä»¤æ‰§è¡Œä¸­å…·æœ‰åŠŸèƒ½æ™®éæ€§å’Œç‹¬ç‰¹æ€§ï¼Œæ­ç¤ºäº†å¾®è°ƒé€‚åº”ä¸è®¡ç®—åŸºç¡€çš„å…³ç³»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¾®è°ƒæ˜¾è‘—æå‡äº†å…¶æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„è®¡ç®—æœºåˆ¶ä»ä¸æ¸…æ™°ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°åˆ†æäº†å¾®è°ƒå¦‚ä½•é‡æ„LLMçš„è®¡ç®—ï¼Œé€šè¿‡éš”ç¦»å’Œåˆ†ææŒ‡ä»¤ç‰¹å®šçš„ç¨€ç–ç»„ä»¶ï¼Œæå‡ºäº†HexaInstæ•°æ®é›†å’ŒSPARCOMåˆ†ææ¡†æ¶ã€‚SPARCOMåŒ…æ‹¬ä¸‰é¡¹å…³é”®è´¡çŒ®ï¼šè¯†åˆ«ç¨€ç–ç»„ä»¶çš„æ–¹æ³•ã€è¯„ä¼°å…¶åŠŸèƒ½çš„æ™®éæ€§å’Œç‹¬ç‰¹æ€§ï¼Œä»¥åŠç³»ç»Ÿæ¯”è¾ƒå…¶å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›ç»„ä»¶åœ¨æŒ‡ä»¤æ‰§è¡Œä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä¸ºLLMç¤¾åŒºæä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¦‚ä½•é‡æ„å…¶è®¡ç®—æœºåˆ¶ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æ·±å…¥åˆ†ææŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„å…·ä½“å®ç°æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥HexaInstæ•°æ®é›†å’ŒSPARCOMæ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°è¯†åˆ«å’Œåˆ†ææŒ‡ä»¤ç‰¹å®šçš„ç¨€ç–ç»„ä»¶ï¼Œæ¢ç´¢å…¶åœ¨æŒ‡ä»¤æ‰§è¡Œä¸­çš„ä½œç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPARCOMæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç¨€ç–ç»„ä»¶è¯†åˆ«ã€åŠŸèƒ½è¯„ä¼°å’Œå˜åŒ–æ¯”è¾ƒã€‚é¦–å…ˆï¼Œé€šè¿‡ç‰¹å®šç®—æ³•è¯†åˆ«ç¨€ç–ç¥ç»å…ƒå’Œä¸“å®¶ï¼›å…¶æ¬¡ï¼Œè¯„ä¼°å…¶åœ¨ä¸åŒæŒ‡ä»¤ä¸‹çš„åŠŸèƒ½è¡¨ç°ï¼›æœ€åï¼Œæ¯”è¾ƒå¾®è°ƒå‰åçš„å˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPARCOMæ¡†æ¶çš„æœ€å¤§åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°è¯†åˆ«å’Œåˆ†æç¨€ç–ç»„ä»¶ï¼Œæ­ç¤ºäº†è¿™äº›ç»„ä»¶åœ¨æŒ‡ä»¤æ‰§è¡Œä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¹³è¡¡çš„HexaInstæ•°æ®é›†ï¼Œç¡®ä¿äº†ä¸åŒç±»åˆ«æŒ‡ä»¤çš„è¦†ç›–ï¼›åŒæ—¶ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–ç¨€ç–ç»„ä»¶çš„è¯†åˆ«å’Œè¯„ä¼°è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç¨€ç–ç»„ä»¶åœ¨æŒ‡ä»¤æ‰§è¡Œä¸­å±•ç°å‡ºæ˜¾è‘—çš„åŠŸèƒ½æ™®éæ€§å’Œç‹¬ç‰¹æ€§ï¼Œè¯†åˆ«å‡†ç¡®ç‡æé«˜äº†15%ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›æå‡äº†20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹å’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£LLMsçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¯ä»¥æå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HexaInst, a carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, a novel analytical framework comprising three key contributions: (1) a method for identifying these sparse components, (2) an evaluation of their functional generality and uniqueness, and (3) a systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community.

