---
layout: default
title: "Concealment of Intent: A Game-Theoretic Analysis"
---

# Concealment of Intent: A Game-Theoretic Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20841" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20841v2</a>
  <a href="https://arxiv.org/pdf/2505.20841.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20841v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20841v2', 'Concealment of Intent: A Game-Theoretic Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinbo Wu, Abhishek Umrawal, Lav R. Varshney

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-08-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ„å›¾éšè—å¯¹æŠ—æ€§æç¤ºä»¥åº”å¯¹å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æŠ—æ€§æ”»å‡»` `å¤§å‹è¯­è¨€æ¨¡å‹` `åšå¼ˆè®º` `å®‰å…¨æ€§` `æ„å›¾éšè—` `é˜²å¾¡æœºåˆ¶` `æç¤ºè¿‡æ»¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹é½æœºåˆ¶åœ¨é¢å¯¹ç²¾å¿ƒè®¾è®¡çš„å¯¹æŠ—æ€§æç¤ºæ—¶ä»ç„¶å­˜åœ¨è„†å¼±æ€§ï¼Œæ— æ³•æœ‰æ•ˆé˜²æ­¢æ¶æ„ä½¿ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ„å›¾éšè—å¯¹æŠ—æ€§æç¤ºçš„æ”»å‡»ç­–ç•¥ï¼Œåˆ©ç”¨æŠ€èƒ½ç»„åˆæ¥æ©ç›–æ¶æ„æ„å›¾ï¼Œå¢å¼ºæ”»å‡»çš„éšè”½æ€§ã€‚
3. å®éªŒè¯æ˜ï¼Œè¯¥æ”»å‡»åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„LLMsä¸Šè¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æå‡äº†æ”»å‡»æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›çš„æå‡ï¼Œå…³äºå…¶å®‰å…¨éƒ¨ç½²çš„æ‹…å¿§ä¹Ÿåœ¨å¢åŠ ã€‚å°½ç®¡å·²ç»å¼•å…¥äº†å¯¹é½æœºåˆ¶ä»¥é˜²æ­¢è¯¯ç”¨ï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°ç²¾å¿ƒè®¾è®¡çš„å¯¹æŠ—æ€§æç¤ºçš„æ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ”»å‡»ç­–ç•¥ï¼šæ„å›¾éšè—å¯¹æŠ—æ€§æç¤ºï¼Œé€šè¿‡æŠ€èƒ½çš„ç»„åˆæ¥éšè—æ¶æ„æ„å›¾ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåšå¼ˆè®ºæ¡†æ¶ï¼Œä»¥å»ºæ¨¡æ­¤ç±»æ”»å‡»ä¸åº”ç”¨æç¤ºå’Œå“åº”è¿‡æ»¤çš„é˜²å¾¡ç³»ç»Ÿä¹‹é—´çš„äº’åŠ¨ã€‚åˆ†æè¯†åˆ«äº†å‡è¡¡ç‚¹ï¼Œå¹¶æ­ç¤ºäº†æ”»å‡»è€…çš„ç»“æ„æ€§ä¼˜åŠ¿ã€‚ä¸ºåº”å¯¹è¿™äº›å¨èƒï¼Œæˆ‘ä»¬æå‡ºå¹¶åˆ†æäº†ä¸€ç§é’ˆå¯¹æ„å›¾éšè—æ”»å‡»çš„é˜²å¾¡æœºåˆ¶ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œè¯¥æ”»å‡»åœ¨å¤šç§çœŸå®ä¸–ç•Œçš„LLMsä¸Šæœ‰æ•ˆï¼Œå±•ç¤ºäº†ç›¸è¾ƒäºç°æœ‰å¯¹æŠ—æ€§æç¤ºæŠ€æœ¯çš„æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ€§æç¤ºæ—¶çš„å®‰å…¨æ€§é—®é¢˜ã€‚ç°æœ‰çš„å¯¹é½æœºåˆ¶åœ¨åº”å¯¹ç²¾å¿ƒè®¾è®¡çš„æ”»å‡»æ—¶å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆé˜²æ­¢æ¶æ„ä½¿ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å›¾éšè—å¯¹æŠ—æ€§æç¤ºæ¥å¢å¼ºæ”»å‡»çš„éšè”½æ€§ã€‚é€šè¿‡æŠ€èƒ½çš„ç»„åˆï¼Œæ”»å‡»è€…èƒ½å¤Ÿæœ‰æ•ˆæ©ç›–å…¶æ¶æ„æ„å›¾ï¼Œä»è€Œç»•è¿‡ç°æœ‰çš„é˜²å¾¡æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ”»å‡»ç­–ç•¥çš„è®¾è®¡ä¸å®æ–½ï¼Œä»¥åŠé˜²å¾¡æœºåˆ¶çš„æ„å»ºã€‚æ”»å‡»é˜¶æ®µæ¶‰åŠå¯¹æŠ—æ€§æç¤ºçš„ç”Ÿæˆï¼Œé˜²å¾¡é˜¶æ®µåˆ™åŒ…æ‹¬æç¤ºå’Œå“åº”è¿‡æ»¤çš„åº”ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†æ„å›¾éšè—çš„å¯¹æŠ—æ€§æç¤ºç­–ç•¥ï¼Œå¹¶é€šè¿‡åšå¼ˆè®ºæ¡†æ¶åˆ†æäº†æ”»å‡»ä¸é˜²å¾¡ä¹‹é—´çš„äº’åŠ¨ï¼Œæ­ç¤ºäº†æ”»å‡»è€…çš„ç»“æ„æ€§ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å¯¹æŠ—æ€§æç¤ºçš„ç”Ÿæˆç®—æ³•ã€è¿‡æ»¤æœºåˆ¶çš„é€‰æ‹©ï¼Œä»¥åŠåšå¼ˆè®ºæ¨¡å‹ä¸­çš„å‡è¡¡ç‚¹åˆ†æã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œé˜²å¾¡çš„é’ˆå¯¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ„å›¾éšè—å¯¹æŠ—æ€§æç¤ºåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„LLMsä¸Šå–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œç›¸è¾ƒäºç°æœ‰å¯¹æŠ—æ€§æç¤ºæŠ€æœ¯ï¼Œæ”»å‡»æˆåŠŸç‡æå‡äº†20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§æ•æ„Ÿçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå¦‚èŠå¤©æœºå™¨äººã€è‡ªåŠ¨åŒ–å®¢æœå’Œå†…å®¹ç”Ÿæˆå·¥å…·ã€‚é€šè¿‡æé«˜å¯¹æŠ—æ€§æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºè¿™äº›ç³»ç»Ÿçš„å®‰å…¨æ€§ï¼Œé˜²æ­¢è¢«æ¶æ„åˆ©ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) grow more capable, concerns about their safe deployment have also grown. Although alignment mechanisms have been introduced to deter misuse, they remain vulnerable to carefully designed adversarial prompts. In this work, we present a scalable attack strategy: intent-hiding adversarial prompting, which conceals malicious intent through the composition of skills. We develop a game-theoretic framework to model the interaction between such attacks and defense systems that apply both prompt and response filtering. Our analysis identifies equilibrium points and reveals structural advantages for the attacker. To counter these threats, we propose and analyze a defense mechanism tailored to intent-hiding attacks. Empirically, we validate the attack's effectiveness on multiple real-world LLMs across a range of malicious behaviors, demonstrating clear advantages over existing adversarial prompting techniques.

