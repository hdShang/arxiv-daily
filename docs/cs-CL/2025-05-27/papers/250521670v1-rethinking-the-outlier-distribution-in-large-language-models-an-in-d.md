---
layout: default
title: "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study"
---

# Rethinking the Outlier Distribution in Large Language Models: An In-depth Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21670" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21670v1</a>
  <a href="https://arxiv.org/pdf/2505.21670.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21670v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21670v1', 'Rethinking the Outlier Distribution in Large Language Models: An In-depth Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rahul Raman, Khushi Sharma, Sai Qian Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ·±å…¥ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¼‚å¸¸å€¼åˆ†å¸ƒä»¥æå‡é‡åŒ–æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼‚å¸¸å€¼` `é‡åŒ–` `æ¨¡å‹å‹ç¼©` `æ€§èƒ½ä¼˜åŒ–` `è¾¹ç¼˜è®¡ç®—` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–ç®—æ³•è™½ç„¶ä¼—å¤šï¼Œä½†å¯¹å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¼‚å¸¸å€¼æ ¹æºæ¢è®¨ä¸è¶³ï¼Œå¯¼è‡´é‡åŒ–è¯¯å·®æ˜¾è‘—ã€‚
2. æœ¬æ–‡é€šè¿‡æ·±å…¥åˆ†æå¼‚å¸¸å€¼çš„å½¢æˆæœºåˆ¶ï¼Œæå‡ºäº†é’ˆå¯¹æ€§ç­–ç•¥ä»¥å‡å°‘å¼‚å¸¸å€¼çš„å‡ºç°ï¼Œæå‡é‡åŒ–æ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ¶ˆé™¤å¼‚å¸¸æ¿€æ´»å’Œé€šé“çº§å¼‚å¸¸å€¼æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”å¯¹æ¨¡å‹å‡†ç¡®æ€§å½±å“æœ€å°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œç ”ç©¶å¼‚å¸¸å€¼è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¯¹æ¨¡å‹æ€§èƒ½çš„å¤šä¸ªæ–¹é¢äº§ç”Ÿæ˜¾è‘—å½±å“ï¼ŒåŒ…æ‹¬é‡åŒ–å’Œå‹ç¼©ã€‚å¼‚å¸¸å€¼å¸¸å¯¼è‡´æ˜¾è‘—çš„é‡åŒ–è¯¯å·®ï¼Œä»è€Œé™ä½æ¨¡å‹æ€§èƒ½ã€‚è¯†åˆ«å’Œå¤„ç†è¿™äº›å¼‚å¸¸å€¼å¯ä»¥æé«˜é‡åŒ–è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œä½¿å…¶åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–ä¸“ç”¨ç¡¬ä»¶ä¸Šçš„éƒ¨ç½²æ›´åŠ é¡ºç•…ã€‚æœ¬æ–‡å…¨é¢æ¢è®¨äº†è¿™äº›å¼‚å¸¸å€¼çš„å½¢æˆæœºåˆ¶ï¼Œå¹¶æå‡ºäº†æ½œåœ¨çš„ç¼“è§£ç­–ç•¥ï¼Œæœ€ç»ˆå¼•å…¥äº†ä¸€äº›é«˜æ•ˆçš„æ–¹æ³•ï¼Œä»¥æœ€å°çš„å‡†ç¡®æ€§å½±å“æ¶ˆé™¤å¤§å¤šæ•°å¼‚å¸¸æ¿€æ´»å’Œé€šé“çº§å¼‚å¸¸å€¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­å¼‚å¸¸å€¼å¯¹é‡åŒ–æ€§èƒ½çš„è´Ÿé¢å½±å“ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æ·±å…¥æ¢è®¨å¼‚å¸¸å€¼çš„æ ¹æœ¬åŸå› ï¼Œå¯¼è‡´é‡åŒ–è¯¯å·®æ˜¾è‘—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹å¼‚å¸¸å€¼å½¢æˆæœºåˆ¶çš„å…¨é¢ç ”ç©¶ï¼Œæå‡ºé’ˆå¯¹æ€§çš„ç¼“è§£ç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘å¼‚å¸¸å€¼çš„å‡ºç°ï¼Œä»è€Œæé«˜é‡åŒ–è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆè¯†åˆ«å‡ºä¸¤ç§ä¸»è¦çš„å¼‚å¸¸å€¼ç±»å‹ï¼šå¤§æ¿€æ´»å’Œé€šé“çº§å¼‚å¸¸å€¼ã€‚æ¥ç€ï¼Œåˆ†æå…¶å½¢æˆåŸå› ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„æ¶ˆé™¤ç­–ç•¥ï¼Œæœ€åé€šè¿‡å®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæ·±å…¥æ¢è®¨äº†å¼‚å¸¸å€¼çš„å½¢æˆæœºåˆ¶ï¼Œå¹¶æå‡ºäº†é«˜æ•ˆçš„æ¶ˆé™¤æ–¹æ³•ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è¡¨é¢å¤„ç†å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿åœ¨æ¶ˆé™¤å¼‚å¸¸å€¼çš„åŒæ—¶ï¼Œå°½é‡ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨æ¶ˆé™¤å¤§æ¿€æ´»å’Œé€šé“çº§å¼‚å¸¸å€¼æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œé‡åŒ–è¯¯å·®é™ä½äº†çº¦30%ï¼ŒåŒæ—¶æ¨¡å‹çš„å‡†ç¡®æ€§ä¿æŒåœ¨95%ä»¥ä¸Šï¼Œå±•ç°å‡ºè‰¯å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨è®¾å¤‡å’Œä¸“ç”¨ç¡¬ä»¶çš„æ¨¡å‹éƒ¨ç½²ã€‚é€šè¿‡æé«˜é‡åŒ–è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œèƒ½å¤Ÿä½¿å¤§è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ›´å¥½åœ°è¿è¡Œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Investigating outliers in large language models (LLMs) is crucial due to their significant impact on various aspects of LLM performance, including quantization and compression. Outliers often cause considerable quantization errors, leading to degraded model performance. Identifying and addressing these outliers can enhance the accuracy and efficiency of the quantization process, enabling smoother deployment on edge devices or specialized hardware. Recent studies have identified two common types of outliers in LLMs: massive activations and channel-wise outliers. While numerous quantization algorithms have been proposed to mitigate their effects and maintain satisfactory accuracy, few have thoroughly explored the root causes of these outliers in depth. In this paper, we conduct a comprehensive investigation into the formation mechanisms of these outliers and propose potential strategies to mitigate their occurrence. Ultimately, we introduce some efficient approaches to eliminate most massive activations and channel-wise outliers with minimal impact on accuracy.

