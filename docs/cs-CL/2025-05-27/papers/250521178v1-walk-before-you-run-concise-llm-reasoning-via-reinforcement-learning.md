---
layout: default
title: "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning"
---

# Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21178" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21178v1</a>
  <a href="https://arxiv.org/pdf/2505.21178.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21178v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21178v1', 'Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingyang Song, Mao Zheng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Ongoing Work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConciseRä»¥è§£å†³é•¿é“¾æ¨ç†ä¸­çš„å†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `ç®€æ´æ€§ä¼˜åŒ–` `é•¿é“¾æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†æ¨¡å‹åœ¨é•¿é“¾æ¨ç†ä¸­å­˜åœ¨è¿‡åº¦æ€è€ƒï¼Œå¯¼è‡´å†—ä½™å’Œé‡å¤æ€ç»´ï¼Œå½±å“æ¨ç†æ•ˆç‡ã€‚
2. æœ¬æ–‡æå‡ºçš„ConciseRæ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼Œåˆ†åˆ«ä¼˜åŒ–æ¨ç†èƒ½åŠ›å’Œç®€æ´æ€§ï¼Œè§£å†³å†—ä½™é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒConciseRåœ¨AIME 2024ã€MATH-500ç­‰åŸºå‡†æµ‹è¯•ä¸­ç”Ÿæˆçš„æ¨ç†å“åº”æ›´ç®€æ´ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç ”ç©¶çš„æ·±å…¥ï¼Œæµ‹è¯•æ—¶æ‰©å±•ç”Ÿæˆé•¿åº¦æˆä¸ºå…³é”®å‰æ²¿ã€‚ç°æœ‰çš„æ¨ç†æ¨¡å‹åœ¨é•¿é“¾æ¨ç†ä¸­å­˜åœ¨è¿‡åº¦æ€è€ƒç°è±¡ï¼Œå¯¼è‡´å†—ä½™å’Œé‡å¤æ€ç»´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ConciseRï¼Œæ—¨åœ¨å®ç°ç®€æ´æ¨ç†ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡Group Relative Policy Optimizationï¼ˆGRPO++ï¼‰æ¿€åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡Length-aware Group Relative Policy Optimizationï¼ˆL-GRPOï¼‰æ˜ç¡®å¼ºåˆ¶ç®€æ´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConciseRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„æ¨ç†æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾æ¨ç†ä¸­å‡ºç°çš„å†—ä½™å’Œé‡å¤æ€ç»´é—®é¢˜ï¼Œå½±å“æ¨ç†çš„æ•ˆç‡å’Œè´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºConciseRæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼Œç¬¬ä¸€é˜¶æ®µä¸“æ³¨äºæå‡æ¨ç†èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µå¼ºè°ƒç®€æ´æ€§ï¼Œä»¥å‡å°‘å†—ä½™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šConciseRæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨GRPO++è¿›è¡Œæ¨ç†èƒ½åŠ›ä¼˜åŒ–ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨L-GRPOè¿›è¡Œç®€æ´æ€§ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨æ‰€æœ‰æ ·æœ¬å›åˆæ­£ç¡®åå†ä¼˜åŒ–å“åº”é•¿åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šConciseRçš„åˆ›æ–°åœ¨äºå…¶ä¸¤é˜¶æ®µçš„è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯â€œèµ°è·¯å†è·‘â€çš„åŸåˆ™ï¼Œç¡®ä¿åœ¨æ¨ç†èƒ½åŠ›æå‡çš„åŸºç¡€ä¸Šå†è¿›è¡Œç®€æ´æ€§ä¼˜åŒ–ï¼Œä¸ç°æœ‰æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GRPO++é˜¶æ®µï¼Œé‡‡ç”¨åŠ¨æ€é‡‡æ ·å’Œclip-higherç»„ä»¶ï¼›åœ¨L-GRPOé˜¶æ®µï¼Œæ˜ç¡®è®¾ç½®ç®€æ´æ€§ç›®æ ‡ï¼Œä¼˜åŒ–æŸå¤±å‡½æ•°ä»¥å¼ºåŒ–ç®€æ´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒConciseRåœ¨AIME 2024ã€MATH-500ã€AMC 2023ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆçš„æ¨ç†å“åº”æ›´ä¸ºç®€æ´ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨ç†æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡å¹…åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨é—®ç­”ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‡å°‘å†—ä½™ä¿¡æ¯çš„ç”Ÿæˆï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥å¯èƒ½åœ¨æ›´å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model's reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the "walk before you run" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks.

