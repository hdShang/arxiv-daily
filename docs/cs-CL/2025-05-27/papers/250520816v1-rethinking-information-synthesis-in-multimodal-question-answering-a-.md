---
layout: default
title: Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective
---

# Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20816" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20816v1</a>
  <a href="https://arxiv.org/pdf/2505.20816.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20816v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20816v1', 'Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Krishna Singh Rajput, Tejas Anvekar, Chitta Baral, Vivek Gupta

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMAMMQAæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€é—®ç­”ä¸­çš„ä¿¡æ¯ç»¼åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€é—®ç­”` `ä¿¡æ¯ç»¼åˆ` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤§è¯­è¨€æ¨¡å‹` `è·¨æ¨¡æ€æ¨ç†` `å¤šä»£ç†ç³»ç»Ÿ` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€é—®ç­”æ–¹æ³•é€šå¸¸ä¾èµ–å•ä¸€æ¨ç†ç­–ç•¥ï¼Œå¿½è§†ä¸åŒæ¨¡æ€çš„ç‰¹æ€§ï¼Œå¯¼è‡´å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºMAMMQAæ¡†æ¶ï¼Œé€šè¿‡å¤šä¸ªä»£ç†åˆ†åˆ«å¤„ç†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œå¢å¼ºæ¨ç†è¿‡ç¨‹çš„é€æ˜æ€§å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAMMQAåœ¨å¤šé¡¹å¤šæ¨¡æ€é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œå‡†ç¡®æ€§å’Œé²æ£’æ€§å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€é—®ç­”çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç»“åˆå¼‚æ„æ¨¡æ€æˆ–å¾®è°ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šã€‚å°½ç®¡è¿™äº›æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œä½†é€šå¸¸ä¾èµ–å•ä¸€çš„æ¨ç†ç­–ç•¥ï¼Œå¿½è§†äº†æ¯ç§æ¨¡æ€çš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œé™åˆ¶äº†å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MAMMQAï¼Œä¸€ä¸ªé’ˆå¯¹æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒçš„å¤šæ¨¡æ€è¾“å…¥çš„å¤šä»£ç†é—®ç­”æ¡†æ¶ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä¸¤ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ä»£ç†å’Œä¸€ä¸ªåŸºäºæ–‡æœ¬çš„å¤§è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œèƒ½å¤Ÿé€šè¿‡åˆ†è§£ç”¨æˆ·æŸ¥è¯¢ã€è·¨æ¨¡æ€æ¨ç†å’Œæ•´åˆè§è§£æ¥ç”Ÿæˆä¸€è‡´çš„ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€é—®ç­”ä¸­ä¿¡æ¯ç»¼åˆçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨ä¸åŒæ¨¡æ€çš„ç‰¹æ€§ï¼Œå¯¼è‡´æ¨ç†æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMAMMQAæ¡†æ¶é€šè¿‡å¼•å…¥å¤šä¸ªä»£ç†ï¼Œåˆ†åˆ«å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œè¡¨æ ¼ä¿¡æ¯ï¼Œä½¿å¾—æ¯ä¸ªä»£ç†èƒ½å¤Ÿåœ¨å…¶ä¸“ä¸šé¢†åŸŸå†…è¿›è¡Œé«˜æ•ˆæ¨ç†ï¼Œä»è€Œæå‡æ•´ä½“é—®ç­”è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ç”±ä¸¤ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»£ç†å’Œä¸€ä¸ªæ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ç»„æˆã€‚ç¬¬ä¸€ä¸ªVLMè´Ÿè´£å°†ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œå¹¶ä»å„ä¸ªæ¨¡æ€ä¸­æ£€ç´¢éƒ¨åˆ†ç­”æ¡ˆï¼›ç¬¬äºŒä¸ªVLMåˆ™é€šè¿‡è·¨æ¨¡æ€æ¨ç†å¯¹è¿™äº›ç»“æœè¿›è¡Œç»¼åˆå’Œä¼˜åŒ–ï¼›æœ€åï¼ŒLLMå°†æ‰€æœ‰è§è§£æ•´åˆä¸ºä¸€ä¸ªè¿è´¯çš„ç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šMAMMQAçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¤šä»£ç†è®¾è®¡ï¼Œä½¿å¾—ä¸åŒæ¨¡æ€çš„ä¿¡æ¯å¤„ç†æ›´åŠ ä¸“ä¸šåŒ–å’Œé€æ˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ¨¡æ€é—´çš„äº’è¡¥ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä»£ç†ä¹‹é—´çš„äº¤äº’é‡‡ç”¨äº†æ¨¡å—åŒ–çš„æ–¹å¼ï¼Œç¡®ä¿æ¯ä¸ªä»£ç†èƒ½å¤Ÿç‹¬ç«‹ä¼˜åŒ–å…¶æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†å„æ¨¡æ€çš„ç‰¹æ€§ï¼Œä»¥æé«˜æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šé¡¹å¤šæ¨¡æ€é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMAMMQAæ¡†æ¶çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¹Ÿå±•ç°å‡ºæ›´å¼ºçš„ç¨³å®šæ€§ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MAMMQAæ¡†æ¶åœ¨å¤šæ¨¡æ€é—®ç­”ç³»ç»Ÿä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿç”¨äºæ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡é—®ç­”ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œè¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in multimodal question answering have primarily focused on combining heterogeneous modalities or fine-tuning multimodal large language models. While these approaches have shown strong performance, they often rely on a single, generalized reasoning strategy, overlooking the unique characteristics of each modality ultimately limiting both accuracy and interpretability. To address these limitations, we propose MAMMQA, a multi-agent QA framework for multimodal inputs spanning text, tables, and images. Our system includes two Visual Language Model (VLM) agents and one text-based Large Language Model (LLM) agent. The first VLM decomposes the user query into sub-questions and sequentially retrieves partial answers from each modality. The second VLM synthesizes and refines these results through cross-modal reasoning. Finally, the LLM integrates the insights into a cohesive answer. This modular design enhances interpretability by making the reasoning process transparent and allows each agent to operate within its domain of expertise. Experiments on diverse multimodal QA benchmarks demonstrate that our cooperative, multi-agent framework consistently outperforms existing baselines in both accuracy and robustness.

