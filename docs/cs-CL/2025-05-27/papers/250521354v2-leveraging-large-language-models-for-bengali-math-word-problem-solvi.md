---
layout: default
title: Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning
---

# Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21354" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21354v2</a>
  <a href="https://arxiv.org/pdf/2505.21354.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21354v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21354v2', 'Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Bidyarthi Paul, Jalisha Jashim Era, Mirazur Rahman Zim, Tahmid Sattar Aothoi, Faisal Muhammad Shah

**ÂàÜÁ±ª**: cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27 (Êõ¥Êñ∞: 2025-07-30)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SOMADHANÊï∞ÊçÆÈõÜ‰ª•Ëß£ÂÜ≥Â≠üÂä†ÊãâÊï∞Â≠¶ËØ≠Ë®ÄÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â≠üÂä†ÊãâËØ≠Â§ÑÁêÜ` `Êï∞Â≠¶ËØ≠Ë®ÄÈóÆÈ¢ò` `ÈìæÂºèÊÄùÁª¥` `‰ΩéËµÑÊ∫êËØ≠Ë®Ä` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â≠üÂä†ÊãâÊï∞Â≠¶ËØ≠Ë®ÄÈóÆÈ¢òÁöÑËß£ÂÜ≥‰ªçÈù¢‰∏¥ÊåëÊàòÔºåÁé∞ÊúâÊ®°ÂûãÂú®Â§çÊùÇÈóÆÈ¢ò‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÁº∫‰πèÁõ∏Â∫îÁöÑÊï∞ÊçÆÈõÜ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫SOMADHANÊï∞ÊçÆÈõÜÔºåÂåÖÂê´8792‰∏™Â§çÊùÇÁöÑÂ≠üÂä†ÊãâMWPsÔºåÊîØÊåÅÊé®ÁêÜËØÑ‰º∞ÂíåÊ®°ÂûãÂºÄÂèë„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰ΩøÁî®ÈìæÂºèÊÄùÁª¥ÊèêÁ§∫ÁöÑÊ®°ÂûãÂú®Â§öÊ≠•È™§ÈÄªËæë‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºåLLaMA-3.3 70BÊ®°ÂûãËææÂà∞‰∫Ü88%ÁöÑÂáÜÁ°ÆÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëß£ÂÜ≥Â≠üÂä†ÊãâÊï∞Â≠¶ËØ≠Ë®ÄÈóÆÈ¢òÔºàMWPsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâÈ¢ÜÂüü‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçÂ§ßÊåëÊàòÔºå‰∏ªË¶ÅÁî±‰∫éËØ•ËØ≠Ë®ÄÁöÑ‰ΩéËµÑÊ∫êÁä∂ÊÄÅÂíåÂ§öÊ≠•È™§Êé®ÁêÜÁöÑÈúÄÊ±Ç„ÄÇÁé∞ÊúâÊ®°ÂûãÂú®Â§çÊùÇÁöÑÂ≠üÂä†ÊãâMWPs‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂéüÂõ†Âú®‰∫éÁº∫‰πè‰∫∫Á±ªÊ†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÂàõÂª∫‰∫ÜSOMADHANÊï∞ÊçÆÈõÜÔºåÂåÖÂê´8792‰∏™Â§çÊùÇÁöÑÂ≠üÂä†ÊãâMWPsÂèäÂÖ∂ÊâãÂä®ÁºñÂÜôÁöÑÈÄêÊ≠•Ëß£ÂÜ≥ÊñπÊ°à„ÄÇÈÄöËøá‰ΩøÁî®ËØ•Êï∞ÊçÆÈõÜÔºåËØÑ‰º∞‰∫ÜÂ§öÁßçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÔºåÂπ∂ÂèëÁé∞ÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊèêÁ§∫ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®Â§öÊ≠•È™§ÈÄªËæë‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇLLaMA-3.3 70BÊ®°ÂûãÂú®Â∞ëÈáèÁ§∫‰æãÁöÑCoTÊèêÁ§∫‰∏ãËææÂà∞‰∫Ü88%ÁöÑÊúÄÈ´òÂáÜÁ°ÆÁéá„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫Â≠üÂä†ÊãâNLPÂ°´Ë°•‰∫ÜÂÖ≥ÈîÆÁ©∫ÁôΩÔºåÊèê‰æõ‰∫ÜÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÊï∞ÊçÆÈõÜÂíåÂèØÊâ©Â±ïÁöÑËß£ÂÜ≥Ê°ÜÊû∂„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â≠üÂä†ÊãâÊï∞Â≠¶ËØ≠Ë®ÄÈóÆÈ¢òÔºàMWPsÔºâÁöÑÊé®ÁêÜÊåëÊàòÔºåÁé∞ÊúâÊñπÊ≥ïÂõ†Áº∫‰πè‰∫∫Á±ªÊ†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜËÄåÈöæ‰ª•Â§ÑÁêÜÂ§çÊùÇÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂàõÂª∫SOMADHANÊï∞ÊçÆÈõÜÔºåÊèê‰æõÈ´òË¥®ÈáèÁöÑÊâãÂä®Ëß£ÂÜ≥ÊñπÊ°àÔºåÊîØÊåÅÂ§öÊ≠•È™§Êé®ÁêÜÁöÑËØÑ‰º∞ÂíåÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ‰ΩøÁî®ÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊèêÁ§∫Êù•ÊèêÂçáÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈõÜÊûÑÂª∫„ÄÅÊ®°ÂûãÈÄâÊã©‰∏éËØÑ‰º∞„ÄÇÈ¶ñÂÖàÊûÑÂª∫SOMADHANÊï∞ÊçÆÈõÜÔºåÁÑ∂ÂêéÂØπÂ§öÁßçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åËØÑ‰º∞ÔºåÊØîËæÉÊ†áÂáÜÊèêÁ§∫‰∏éCoTÊèêÁ§∫ÁöÑÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂàõÂª∫‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÂ≠üÂä†ÊãâMWPsÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºåÂπ∂ÈÄöËøáCoTÊèêÁ§∫ÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõ‰∫ÜÊõ¥ÂÖ∑ÈíàÂØπÊÄßÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËØÑ‰º∞‰∏≠ÔºåÈááÁî®‰∫ÜÈõ∂-shotÂíåfew-shotÊèêÁ§∫Á≠ñÁï•ÔºåÂπ∂Â∫îÁî®‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÊäÄÊúØËøõË°åÈ´òÊïàÂæÆË∞ÉÔºå‰ª•Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰ΩøÁî®ÈìæÂºèÊÄùÁª¥ÊèêÁ§∫ÁöÑÊ®°ÂûãÂú®Â§öÊ≠•È™§ÈÄªËæë‰ªªÂä°‰∏äË°®Áé∞ÊòæËëóÊèêÂçáÔºåLLaMA-3.3 70BÊ®°ÂûãÂú®few-shot CoTÊèêÁ§∫‰∏ãËææÂà∞‰∫Ü88%ÁöÑÂáÜÁ°ÆÁéáÔºåËæÉÊ†áÂáÜÊèêÁ§∫ÊúâÊòéÊòæÊîπÂñÑÔºåÂ±ïÁ§∫‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤ÊäÄÊúØ„ÄÅËØ≠Ë®ÄÂ≠¶‰π†Â∑•ÂÖ∑ÂíåÊô∫ËÉΩËæÖÂØºÁ≥ªÁªü„ÄÇÈÄöËøáÊèêÂçáÂ≠üÂä†ÊãâËØ≠ÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•Â∏ÆÂä©Â≠¶ÁîüÊõ¥Â•ΩÂú∞ÁêÜËß£Êï∞Â≠¶ÈóÆÈ¢òÔºå‰øÉËøõÊïôËÇ≤ÂÖ¨Âπ≥ÔºåÊé®Âä®‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÁ†îÁ©∂‰∏éÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.

