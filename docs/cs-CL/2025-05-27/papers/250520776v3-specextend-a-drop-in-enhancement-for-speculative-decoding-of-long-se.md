---
layout: default
title: "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences"
---

# SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20776" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20776v3</a>
  <a href="https://arxiv.org/pdf/2505.20776.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20776v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20776v3', 'SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-29)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jycha98/SpecExtend)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpecExtendä»¥è§£å†³é•¿åºåˆ—æ¨ç†æ€§èƒ½ä¸‹é™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨æµ‹è§£ç ` `é•¿åºåˆ—` `è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `æ€§èƒ½æå‡` `KVç¼“å­˜` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨æµ‹è§£ç æŠ€æœ¯åœ¨å¤„ç†é•¿åºåˆ—æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶åœ¨ä¸­ç­‰é•¿åº¦è¾“å…¥æ—¶é™å¹…æ˜æ˜¾ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡é™ä½ã€‚
2. æœ¬æ–‡æå‡ºSpecExtendï¼Œé€šè¿‡é›†æˆé«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å’Œè·¨æ¨¡å‹æ£€ç´¢ç­–ç•¥ï¼Œæ”¹å–„é•¿è¾“å…¥çš„æ¨æµ‹è§£ç æ€§èƒ½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpecExtendåœ¨é•¿æ‘˜è¦å’Œé•¿æ¨ç†ä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº†æœ€é«˜2.84å€å’Œ3.86å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†çŸ­è¾“å…¥çš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨æµ‹è§£ç æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†åŠ é€Ÿçš„æŠ€æœ¯ï¼Œä½†éšç€è¾“å…¥é•¿åº¦çš„å¢åŠ ï¼Œå…¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶åœ¨ä¸­ç­‰é•¿åº¦æ—¶é™å¹…æ˜æ˜¾ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SpecExtendï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„å¢å¼ºæ–¹æ¡ˆï¼Œæ—¨åœ¨æ”¹å–„é•¿åºåˆ—çš„æ¨æµ‹è§£ç æ€§èƒ½ã€‚SpecExtendé›†æˆäº†é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¦‚FlashAttentionå’ŒHybrid Tree Attentionï¼Œä»¥åŠ é€Ÿé¢„å¡«å……å’ŒéªŒè¯æ­¥éª¤ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„KVç¼“å­˜é©±é€ç­–ç•¥â€”â€”è·¨æ¨¡å‹æ£€ç´¢ï¼Œåˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†æ•°åŠ¨æ€é€‰æ‹©ä¸å°å‹è‰ç¨¿æ¨¡å‹ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼ŒSpecExtendåœ¨16Kæ ‡è®°çš„é•¿æ‘˜è¦æ¨ç†ä¸­åŠ é€Ÿäº†æ¨æµ‹è§£ç ï¼Œæå‡å¹…åº¦å¯è¾¾2.84å€ï¼Œè€Œåœ¨é•¿æ¨ç†ä»»åŠ¡ä¸­æå‡å¹…åº¦å¯è¾¾3.86å€ï¼ŒåŒæ—¶ä¿æŒäº†çŸ­è¾“å…¥çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨æµ‹è§£ç æŠ€æœ¯åœ¨é•¿åºåˆ—è¾“å…¥æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸­ç­‰é•¿åº¦è¾“å…¥æ—¶çš„æ˜¾è‘—é™å¹…ï¼Œå½±å“æ¨ç†æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºSpecExtendä½œä¸ºä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„å¢å¼ºæ–¹æ¡ˆï¼Œé€šè¿‡é›†æˆé«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å’Œæ–°çš„KVç¼“å­˜é©±é€ç­–ç•¥ï¼Œæ”¹å–„é•¿åºåˆ—çš„æ¨æµ‹è§£ç æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpecExtendçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é¢„å¡«å……å’ŒéªŒè¯ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼Œé‡‡ç”¨FlashAttentionå’ŒHybrid Tree Attentionç­‰é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶æ¥åŠ é€Ÿè¿™äº›æ­¥éª¤ï¼ŒåŒæ—¶å¼•å…¥è·¨æ¨¡å‹æ£€ç´¢ç­–ç•¥ä»¥åŠ¨æ€é€‰æ‹©ç›¸å…³ä¸Šä¸‹æ–‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè·¨æ¨¡å‹æ£€ç´¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†æ•°æ¥é€‰æ‹©ä¸å°å‹è‰ç¨¿æ¨¡å‹ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œæœ‰æ•ˆæå‡é•¿åºåˆ—çš„æ¨æµ‹è§£ç æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSpecExtendé‡‡ç”¨äº†é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSpecExtendåœ¨16Kæ ‡è®°çš„é•¿æ‘˜è¦ä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜2.84å€çš„åŠ é€Ÿï¼Œè€Œåœ¨é•¿æ¨ç†ä»»åŠ¡ä¸­åˆ™è¾¾åˆ°äº†3.86å€çš„åŠ é€Ÿï¼Œä¸ç°æœ‰æœ€å…ˆè¿›æ¡†æ¶ç›¸æ¯”ï¼ŒçŸ­è¾“å…¥æ€§èƒ½å¾—ä»¥ä¿æŒï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨é•¿åºåˆ—æ¨ç†ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é•¿æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦å’Œæ¨ç†ä»»åŠ¡ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦å¿«é€Ÿå“åº”çš„å®æ—¶åº”ç”¨åœºæ™¯ã€‚é€šè¿‡æé«˜é•¿åºåˆ—çš„æ¨ç†æ•ˆç‡ï¼ŒSpecExtendæœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ï¼Œæœªæ¥å¯èƒ½å½±å“æ›´å¤šåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speculative decoding is a widely used technique for accelerating inference in large language models (LLMs), but its performance degrades as input length grows, with significant drops even at moderate lengths. Yet, this early degradation has remained largely underexplored. We introduce SpecExtend, a drop-in enhancement that improves speculative decoding on long sequences without additional training. SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention to accelerate prefill and verification steps. To improve both draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that leverages the target model's attention scores to dynamically select relevant context for the smaller draft model. Extensive evaluations show that SpecExtend accelerates speculative decoding by up to 2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while preserving the short-input performance of state-of-the-art frameworks. Our code is available at https://github.com/jycha98/SpecExtend .

