---
layout: default
title: Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing
---

# Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20976" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20976v1</a>
  <a href="https://arxiv.org/pdf/2505.20976.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20976v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20976v1', 'Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peiming Guo, Meishan Zhang, Jianling Li, Min Zhang, Yue Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Accepted by ACL 2025 main conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMåå‘ç”Ÿæˆæ–¹æ³•ä»¥è§£å†³è·¨é¢†åŸŸå¥æ³•åˆ†æé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨é¢†åŸŸå¥æ³•åˆ†æ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨æ ‘åº“ç”Ÿæˆ` `å¯¹æ¯”å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¥æ³•æ ‘` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è·¨é¢†åŸŸå¥æ³•åˆ†æé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç¼ºä¹è¶³å¤Ÿçš„å¤šé¢†åŸŸå¥æ³•æ ‘åº“ï¼Œç°æœ‰æ–¹æ³•åœ¨æ­¤æ–¹é¢çš„è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„LLMåå‘ç”Ÿæˆæ–¹æ³•é€šè¿‡å¡«å……ç¼ºå¤±å•è¯ç”Ÿæˆè·¨é¢†åŸŸå¥æ³•æ ‘åº“ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒç­–ç•¥æå‡åˆ†ææ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨äº”ä¸ªç›®æ ‡é¢†åŸŸä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå¤šç§åŸºçº¿æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è·¨é¢†åŸŸå¥æ³•åˆ†æåœ¨è®¡ç®—è¯­è¨€å­¦ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå¯ç”¨çš„å¤šé¢†åŸŸå¥æ³•æ ‘åº“æœ‰é™ã€‚æœ¬æ–‡ç ”ç©¶äº†é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè‡ªåŠ¨æ ‘åº“ç”Ÿæˆã€‚ç”±äºLLMsåœ¨å¥æ³•åˆ†æä¸­çš„è¡¨ç°è¾ƒå·®ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ ‘åº“ç”Ÿæˆæ–¹æ³•â€”â€”LLMåå‘ç”Ÿæˆï¼Œè¯¥æ–¹æ³•ç±»ä¼¼äºå¥æ³•åˆ†æçš„é€†è¿‡ç¨‹ã€‚LLMåå‘ç”Ÿæˆä»¥ä»…åŒ…å«é¢†åŸŸå…³é”®è¯å¶èŠ‚ç‚¹çš„ä¸å®Œæ•´è·¨é¢†åŸŸå¥æ³•æ ‘ä¸ºè¾“å…¥ï¼Œå¡«å……ç¼ºå¤±çš„å•è¯ä»¥ç”Ÿæˆè·¨é¢†åŸŸå¥æ³•æ ‘åº“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºè·¨åº¦çš„å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨LLMåå‘ç”Ÿæˆæ ‘åº“è¿›è¡Œè·¨é¢†åŸŸå¥æ³•åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å„ç§åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ä¸ªç›®æ ‡é¢†åŸŸçš„å¹³å‡ç»“æœä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è·¨é¢†åŸŸå¥æ³•åˆ†æä¸­ç”±äºç¼ºä¹è¶³å¤Ÿçš„å¤šé¢†åŸŸå¥æ³•æ ‘åº“è€Œå¯¼è‡´çš„æ€§èƒ½ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒé¢†åŸŸçš„å¥æ³•ç»“æ„æ—¶è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„LLMåå‘ç”Ÿæˆæ–¹æ³•é€šè¿‡å°†ä¸å®Œæ•´çš„è·¨é¢†åŸŸå¥æ³•æ ‘ä½œä¸ºè¾“å…¥ï¼Œå¡«å……ç¼ºå¤±çš„å•è¯ï¼Œä»è€Œç”Ÿæˆå®Œæ•´çš„å¥æ³•æ ‘åº“ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†å¥æ³•åˆ†æçš„é€†è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜LLMsåœ¨å¥æ³•åˆ†æä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯LLMåå‘ç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—æ¥æ”¶ä¸å®Œæ•´çš„å¥æ³•æ ‘å¹¶ç”Ÿæˆå®Œæ•´çš„æ ‘åº“ï¼›å…¶æ¬¡æ˜¯å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒæ¨¡å—ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹åœ¨å¥æ³•åˆ†æä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†LLMåå‘ç”Ÿæˆæ–¹æ³•ï¼Œè¿™ä¸€æ–¹æ³•é€šè¿‡é€†å‘å¡«å……çš„æ–¹å¼ç”Ÿæˆå¥æ³•æ ‘åº“ï¼Œä¸ä¼ ç»Ÿçš„å¥æ³•åˆ†ææ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è·¨é¢†åŸŸåˆ†æçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”Ÿæˆçš„å¥æ³•æ ‘çš„è´¨é‡ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ ç­–ç•¥å¢å¼ºæ¨¡å‹å¯¹ä¸åŒé¢†åŸŸå¥æ³•ç»“æ„çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LLMåå‘ç”Ÿæˆæ–¹æ³•ç»“åˆå¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒï¼Œåœ¨äº”ä¸ªç›®æ ‡é¢†åŸŸä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡æå‡å¹…åº¦è¶…è¿‡äº†ç°æœ‰å¤šç§åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœæ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¥æ³•åˆ†æã€æœºå™¨ç¿»è¯‘å’Œä¿¡æ¯æå–ç­‰ã€‚é€šè¿‡æå‡è·¨é¢†åŸŸå¥æ³•åˆ†æçš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºå¤šé¢†åŸŸçš„è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡æä¾›æ›´å¼ºçš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cross-domain constituency parsing is still an unsolved challenge in computational linguistics since the available multi-domain constituency treebank is limited. We investigate automatic treebank generation by large language models (LLMs) in this paper. The performance of LLMs on constituency parsing is poor, therefore we propose a novel treebank generation method, LLM back generation, which is similar to the reverse process of constituency parsing. LLM back generation takes the incomplete cross-domain constituency tree with only domain keyword leaf nodes as input and fills the missing words to generate the cross-domain constituency treebank. Besides, we also introduce a span-level contrastive learning pre-training strategy to make full use of the LLM back generation treebank for cross-domain constituency parsing. We verify the effectiveness of our LLM back generation treebank coupled with contrastive learning pre-training on five target domains of MCTB. Experimental results show that our approach achieves state-of-the-art performance on average results compared with various baselines.

