---
layout: default
title: "CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs"
---

# CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11059" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11059v1</a>
  <a href="https://arxiv.org/pdf/2506.11059.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11059v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11059v1', 'CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanxi Guo, Siyuan Cheng, Kaiyuan Zhang, Guangyu Shen, Xiangyu Zhang

**åˆ†ç±»**: cs.SE, cs.CL, cs.CY, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCodeMirageä»¥è§£å†³AIç”Ÿæˆä»£ç æ£€æµ‹çš„åŸºå‡†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `AIç”Ÿæˆä»£ç ` `ä»£ç æ£€æµ‹` `åŸºå‡†æµ‹è¯•` `å¤šè¯­è¨€æ”¯æŒ` `è½¯ä»¶å®‰å…¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç å®¡æŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„AIç”Ÿæˆä»£ç æ£€æµ‹åŸºå‡†æµ‹è¯•è¦†ç›–çš„ç¼–ç¨‹è¯­è¨€æœ‰é™ï¼Œä¸”ä¾èµ–äºèƒ½åŠ›è¾ƒå¼±çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚
2. CodeMirageé€šè¿‡æ¶µç›–åç§ç¼–ç¨‹è¯­è¨€ã€æä¾›åŸå§‹å’Œæ”¹å†™ä»£ç æ ·æœ¬ï¼Œä»¥åŠæ•´åˆå¤šä¸ªé¡¶å°–LLMsçš„è¾“å‡ºï¼Œè§£å†³äº†ç°æœ‰åŸºå‡†çš„ä¸è¶³ã€‚
3. é€šè¿‡å¯¹åä¸ªæ£€æµ‹å™¨çš„è¯„ä¼°ï¼Œå‘ç°äº†å½“å‰æ£€æµ‹å™¨çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„å…³é”®æŒ‘æˆ˜ï¼Œæ¨åŠ¨äº†æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºç°ä»£è½¯ä»¶å¼€å‘çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç”Ÿæˆå¤§é‡AIç”Ÿæˆçš„æºä»£ç ã€‚å°½ç®¡è¿™äº›æ¨¡å‹æé«˜äº†ç¼–ç¨‹ç”Ÿäº§åŠ›ï¼Œä½†å…¶è¯¯ç”¨å¸¦æ¥äº†ä»£ç æŠ„è¢­ã€è®¸å¯è¯è¿è§„å’Œä¸å®‰å…¨ç¨‹åºä¼ æ’­ç­‰é‡å¤§é£é™©ã€‚å› æ­¤ï¼Œå¼ºæœ‰åŠ›çš„AIç”Ÿæˆä»£ç æ£€æµ‹è‡³å…³é‡è¦ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦è¦†ç›–çš„ç¼–ç¨‹è¯­è¨€æœ‰é™ï¼Œä¸”ä¾èµ–äºèƒ½åŠ›è¾ƒå¼±çš„ç”Ÿæˆæ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†CodeMirageï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–åç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…å«åŸå§‹å’Œæ”¹å†™çš„ä»£ç æ ·æœ¬ï¼Œå¹¶æ•´åˆäº†æ¥è‡ªåä¸ªé¡¶å°–ç”Ÿäº§çº§LLMsçš„è¾“å‡ºã€‚é€šè¿‡CodeMirageï¼Œæˆ‘ä»¬è¯„ä¼°äº†åä¸ªä»£è¡¨æ€§æ£€æµ‹å™¨åœ¨å››ç§æ–¹æ³•è®ºèŒƒå¼ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å½“å‰æ£€æµ‹å™¨çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶è¯†åˆ«äº†æœªæ¥å·¥ä½œçš„å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç›¸ä¿¡CodeMirageä¸ºå¼€å‘ç¨³å¥ä¸”å…·æœ‰æ™®é€‚æ€§çš„AIç”Ÿæˆä»£ç æ£€æµ‹å™¨æä¾›äº†ä¸¥æ ¼ä¸”å®ç”¨çš„æµ‹è¯•å¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³AIç”Ÿæˆä»£ç çš„æ£€æµ‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç¼–ç¨‹è¯­è¨€è¦†ç›–å’Œç”Ÿæˆæ¨¡å‹èƒ½åŠ›ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºCodeMirageåŸºå‡†ï¼Œé€šè¿‡å¤šè¯­è¨€æ”¯æŒå’Œå¤šæ ·åŒ–çš„ä»£ç æ ·æœ¬ï¼Œæå‡æ£€æµ‹å™¨çš„è¯„ä¼°æ ‡å‡†å’Œå®ç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ ·æœ¬ç”Ÿæˆã€æ£€æµ‹å™¨è¯„ä¼°å’Œç»“æœåˆ†æå››ä¸ªä¸»è¦æ¨¡å—ï¼Œç¡®ä¿å…¨é¢è¦†ç›–å’Œå‡†ç¡®è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šCodeMirageçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¤šè¯­è¨€æ”¯æŒå’Œå¯¹å¤šç§ç”Ÿæˆæ¨¡å‹çš„æ•´åˆï¼Œæ˜¾è‘—æé«˜äº†æ£€æµ‹çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†é€‚åº”ä¸åŒæ£€æµ‹å™¨çš„è¯„ä¼°é…ç½®ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå¯æ¯”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨CodeMirageè¯„ä¼°çš„æ£€æµ‹å™¨åœ¨å¤šç§é…ç½®ä¸‹è¡¨ç°å‡ºè‰²ï¼Œæ­ç¤ºäº†å½“å‰æ£€æµ‹å™¨åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€å’Œä»£ç æ ·æœ¬ç±»å‹ä¸Šçš„ä¼˜ç¼ºç‚¹ï¼Œæ¨åŠ¨äº†æ£€æµ‹æŠ€æœ¯çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€ä»£ç å®¡æŸ¥å’Œå®‰å…¨æ€§æ£€æµ‹ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…è¯†åˆ«å’Œé˜²æ­¢AIç”Ÿæˆä»£ç ä¸­çš„æ½œåœ¨é£é™©ï¼Œæå‡è½¯ä»¶è´¨é‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼ŒCodeMirageæœ‰æœ›æˆä¸ºAIç”Ÿæˆä»£ç æ£€æµ‹é¢†åŸŸçš„æ ‡å‡†åŸºå‡†ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have become integral to modern software development, producing vast amounts of AI-generated source code. While these models boost programming productivity, their misuse introduces critical risks, including code plagiarism, license violations, and the propagation of insecure programs. As a result, robust detection of AI-generated code is essential. To support the development of such detectors, a comprehensive benchmark that reflects real-world conditions is crucial. However, existing benchmarks fall short -- most cover only a limited set of programming languages and rely on less capable generative models. In this paper, we present CodeMirage, a comprehensive benchmark that addresses these limitations through three major advancements: (1) it spans ten widely used programming languages, (2) includes both original and paraphrased code samples, and (3) incorporates outputs from ten state-of-the-art production-level LLMs, including both reasoning and non-reasoning models from six major providers. Using CodeMirage, we evaluate ten representative detectors across four methodological paradigms under four realistic evaluation configurations, reporting results using three complementary metrics. Our analysis reveals nine key findings that uncover the strengths and weaknesses of current detectors, and identify critical challenges for future work. We believe CodeMirage offers a rigorous and practical testbed to advance the development of robust and generalizable AI-generated code detectors.

