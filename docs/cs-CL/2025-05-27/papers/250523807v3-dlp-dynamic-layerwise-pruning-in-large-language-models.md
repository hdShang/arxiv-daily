---
layout: default
title: DLP: Dynamic Layerwise Pruning in Large Language Models
---

# DLP: Dynamic Layerwise Pruning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23807" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23807v3</a>
  <a href="https://arxiv.org/pdf/2505.23807.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23807v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23807v3', 'DLP: Dynamic Layerwise Pruning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-06-03)

**å¤‡æ³¨**: Accepted by ICML 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ironartisan/DLP)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€å±‚çº§å‰ªææ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€å‰ªæ` `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `æ¨ç†æ•ˆç‡` `å‚æ•°é«˜æ•ˆå¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‡åŒ€å±‚çº§å‰ªææ–¹æ³•åœ¨é«˜ç¨€ç–åº¦ä¸‹å¸¸å¯¼è‡´æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨ä¸åŒå±‚çš„è´¡çŒ®ã€‚
2. åŠ¨æ€å±‚çº§å‰ªæï¼ˆDLPï¼‰é€šè¿‡ç»“åˆæ¨¡å‹æƒé‡ä¸è¾“å…¥æ¿€æ´»ä¿¡æ¯ï¼Œè‡ªé€‚åº”åœ°ä¸ºæ¯å±‚åˆ†é…å‰ªæç‡ï¼Œä»è€Œä¼˜åŒ–å‰ªææ•ˆæœã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDLPåœ¨70%ç¨€ç–åº¦ä¸‹æ˜¾è‘—é™ä½äº†LLaMA2-7Bçš„å›°æƒ‘åº¦ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‰ªææŠ€æœ¯è¿‘å¹´æ¥è¢«å¹¿æ³›åº”ç”¨äºå‡å°‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‚æ•°è§„æ¨¡å¹¶æé«˜æ¨ç†æ•ˆç‡ã€‚ç°æœ‰çš„ä¸»æµå‰ªææ–¹æ³•é€šå¸¸ä¾èµ–äºå‡åŒ€çš„å±‚çº§å‰ªæç­–ç•¥ï¼Œè¿™åœ¨é«˜ç¨€ç–åº¦ä¸‹å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•â€”â€”åŠ¨æ€å±‚çº§å‰ªæï¼ˆDLPï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ•´åˆæ¨¡å‹æƒé‡ä¸è¾“å…¥æ¿€æ´»ä¿¡æ¯ï¼Œè‡ªé€‚åº”åœ°ç¡®å®šæ¯ä¸€å±‚çš„ç›¸å¯¹é‡è¦æ€§ï¼Œä»è€Œç›¸åº”åœ°åˆ†é…å‰ªæç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLPåœ¨å¤šä¸ªLLMä¸­æœ‰æ•ˆåœ°ä¿æŒäº†é«˜ç¨€ç–åº¦ä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨70%ç¨€ç–åº¦æ—¶ï¼ŒDLPä½¿LLaMA2-7Bçš„å›°æƒ‘åº¦é™ä½äº†7.79ï¼Œå¹¶æé«˜äº†2.7%çš„å¹³å‡å‡†ç¡®ç‡ã€‚DLPè¿˜ä¸å¤šç§ç°æœ‰çš„LLMå‹ç¼©æŠ€æœ¯å…¼å®¹ï¼Œå¹¶å¯æ— ç¼é›†æˆåˆ°å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å‡åŒ€å±‚çº§å‰ªææ–¹æ³•åœ¨é«˜ç¨€ç–åº¦ä¸‹å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚è¿™äº›æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘ä¸åŒå±‚åœ¨æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œå¯¼è‡´å‰ªææ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåŠ¨æ€å±‚çº§å‰ªæï¼ˆDLPï¼‰é€šè¿‡åˆ†ææ¨¡å‹æƒé‡ä¸è¾“å…¥æ¿€æ´»ä¿¡æ¯ï¼ŒåŠ¨æ€è°ƒæ•´æ¯å±‚çš„å‰ªæç‡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°ä¿ç•™é‡è¦å±‚çš„ä¿¡æ¯ï¼Œå‡å°‘æ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDLPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) æƒé‡åˆ†ææ¨¡å—ï¼Œè¯„ä¼°å„å±‚çš„é‡è¦æ€§ï¼›2) æ¿€æ´»ä¿¡æ¯æ¨¡å—ï¼Œæ•æ‰è¾“å…¥æ•°æ®å¯¹å±‚çš„å½±å“ï¼›3) å‰ªæå†³ç­–æ¨¡å—ï¼Œæ ¹æ®åˆ†æç»“æœåŠ¨æ€è°ƒæ•´å‰ªæç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDLPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªé€‚åº”å‰ªæç­–ç•¥ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å›ºå®šå‰ªæç‡ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨¡å‹çŠ¶æ€å’Œè¾“å…¥åŠ¨æ€è°ƒæ•´ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é«˜ç¨€ç–åº¦ä¸‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DLPä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å‰ªæç‡çš„åŠ¨æ€è®¡ç®—å…¬å¼ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ¨¡å‹æ€§èƒ½ä¸ç¨€ç–åº¦ä¹‹é—´çš„å¹³è¡¡ï¼Œç½‘ç»œç»“æ„åˆ™ä¿æŒäº†åŸæœ‰æ¨¡å‹çš„å®Œæ•´æ€§ï¼Œä»¥ç¡®ä¿å‰ªæåçš„æ¨¡å‹ä»å…·å¤‡è‰¯å¥½çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨70%ç¨€ç–åº¦ä¸‹ï¼ŒDLPä½¿LLaMA2-7Bçš„å›°æƒ‘åº¦é™ä½äº†7.79ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†2.7%ã€‚è¿™äº›ç»“æœç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†DLPåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰å¤§è¯­è¨€æ¨¡å‹ç›¸å…³ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œå‡å°‘è®¡ç®—èµ„æºæ¶ˆè€—ï¼ŒDLPå¯ä»¥åœ¨è¾¹ç¼˜è®¡ç®—å’Œç§»åŠ¨è®¾å¤‡ä¸Šå®ç°æ›´é«˜æ•ˆçš„åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.

