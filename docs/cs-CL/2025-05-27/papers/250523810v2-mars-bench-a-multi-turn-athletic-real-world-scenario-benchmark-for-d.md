---
layout: default
title: MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation
---

# MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23810" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23810v2</a>
  <a href="https://arxiv.org/pdf/2505.23810.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23810v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23810v2', 'MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenghao Yang, Yinbo Luo, Zhoufutu Wen, Qi Chu, Tao Gong, Longxiang Liu, Kaiyuan Zhang, Jianpeng Jiao, Ge Zhang, Wenhao Huang, Nenghai Yu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-15)

**å¤‡æ³¨**: 29 pages, 13 figures, Accepted as EMNLP2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMARS-Benchä»¥è§£å†³LLMsåœ¨å¤æ‚å¯¹è¯ä¸­çš„é²æ£’æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè½®å¯¹è¯` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹è¯è¯„ä¼°` `é²æ£’æ€§` `åŠ¨æœºè½¬ç§»` `è·¨è½®ä¾èµ–` `çœŸå®åœºæ™¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹è¯åŸºå‡†æ— æ³•å……åˆ†åæ˜ å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¯¹è¯æ—¶çš„é²æ£’æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè½®å¯¹è¯ä¸­ã€‚
2. æœ¬æ–‡æå‡ºMARS-BenchåŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡çœŸå®çš„å¯¹è¯åœºæ™¯è¯„ä¼°LLMsåœ¨å¤šè½®å¯¹è¯ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«å…³æ³¨åŠ¨æœºè½¬ç§»å’Œè·¨è½®ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé—­æºLLMsåœ¨å¤æ‚å¯¹è¯ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ˜ç¡®æ¨ç†èƒ½æ˜¾è‘—æå‡å…¶é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¦‚ChatGPTï¼Œå·²å¹¿æ³›åº”ç”¨äºå®é™…å¯¹è¯åœºæ™¯ã€‚ç„¶è€Œï¼ŒLLMsåœ¨å¤„ç†é•¿å¤æ‚å¯¹è¯æ—¶çš„é²æ£’æ€§å—åˆ°æ‰¹è¯„ï¼Œå°¤å…¶æ˜¯åœ¨é¢‘ç¹çš„åŠ¨æœºè½¬ç§»å’Œå¤æ‚çš„è·¨è½®ä¾èµ–æ–¹é¢ã€‚ç°æœ‰åŸºå‡†æ— æ³•å……åˆ†åæ˜ è¿™äº›å¼±ç‚¹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MARS-Benchï¼Œä¸€ä¸ªå¤šè½®è¿åŠ¨çœŸå®åœºæ™¯å¯¹è¯åŸºå‡†ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚MARS-BenchåŸºäºé€æ­¥æ–‡æœ¬è¯„è®ºæ„å»ºï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤šè½®å¯¹è¯çš„ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šè¶…å¤šè½®ã€äº’åŠ¨å¤šè½®å’Œè·¨è½®ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé—­æºLLMsæ˜¾è‘—ä¼˜äºå¼€æºæ›¿ä»£å“ï¼Œæ˜ç¡®æ¨ç†æ˜¾è‘—æå‡LLMsåœ¨å¤„ç†é•¿å¤æ‚å¯¹è¯æ—¶çš„é²æ£’æ€§ï¼ŒåŒæ—¶LLMsåœ¨å¤„ç†åŠ¨æœºè½¬ç§»å’Œå¤æ‚è·¨è½®ä¾èµ–æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿å¤æ‚å¯¹è¯ä¸­é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æœºè½¬ç§»å’Œè·¨è½®ä¾èµ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è¯„ä¼°è¿™äº›é—®é¢˜çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMARS-BenchåŸºå‡†ï¼Œé€šè¿‡æ„å»ºåŸºäºé€æ­¥æ–‡æœ¬è¯„è®ºçš„çœŸå®å¯¹è¯åœºæ™¯ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤šè½®å¯¹è¯çš„å…³é”®æ–¹é¢ï¼Œä»¥å¡«è¡¥ç°æœ‰åŸºå‡†çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMARS-Benchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¶…å¤šè½®å¯¹è¯ã€äº’åŠ¨å¤šè½®å¯¹è¯å’Œè·¨è½®ä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LLMsåœ¨å¤šè½®å¯¹è¯ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šMARS-Benchçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶çœŸå®åœºæ™¯çš„æ„å»ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°LLMsåœ¨å¤æ‚å¯¹è¯ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æœºè½¬ç§»å’Œè·¨è½®ä¾èµ–æ–¹é¢çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†æ³¨æ„åŠ›å¯è§†åŒ–æŠ€æœ¯ï¼Œæ­ç¤ºäº†ç‰¹æ®Šæ ‡è®°å¯¼è‡´çš„æ³¨æ„åŠ›æ²‰æ²¡ç°è±¡ï¼Œä»è€Œå½±å“LLMsåœ¨é•¿å¯¹è¯ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé—­æºLLMsåœ¨MARS-BenchåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå¼€æºæ›¿ä»£å“ï¼Œä¸”æ˜ç¡®æ¨ç†çš„å¼•å…¥ä½¿å¾—LLMsåœ¨å¤„ç†é•¿å¤æ‚å¯¹è¯æ—¶çš„é²æ£’æ€§æå‡äº†æ˜¾è‘—æ°´å¹³ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„å¯¹è¯ç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦çš„å®éªŒä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MARS-BenchåŸºå‡†çš„æå‡ºä¸ºå¯¹è¯ç³»ç»Ÿçš„ç ”ç©¶æä¾›äº†æ–°çš„è¯„ä¼°å·¥å…·ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶è€…æ›´å¥½åœ°ç†è§£å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸­çš„è¡¨ç°ã€‚è¿™ä¸€åŸºå‡†åœ¨æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹å’Œç¤¾äº¤æœºå™¨äººç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨å¯¹è¯ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ä¸ä¼˜åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted in real-world dialogue applications. However, LLMs' robustness, especially in handling long complex dialogue sessions, including frequent motivation transfer, sophisticated cross-turn dependency, is criticized all along. Nevertheless, no existing benchmarks can fully reflect these weaknesses. We present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic \textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to remedy the gap. MARS-Bench is constructed from play-by-play text commentary so to feature realistic dialogues specifically designed to evaluate three critical aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that closed-source LLMs significantly outperform open-source alternatives, explicit reasoning significantly boosts LLMs' robustness on handling long complex dialogue sessions, and LLMs indeed face significant challenges when handling motivation transfer and sophisticated cross-turn dependency. Moreover, we provide mechanistic interpretability on how attention sinks due to special tokens lead to LLMs' performance degradation when handling long complex dialogue sessions based on attention visualization experiment in Qwen2.5-7B-Instruction.

