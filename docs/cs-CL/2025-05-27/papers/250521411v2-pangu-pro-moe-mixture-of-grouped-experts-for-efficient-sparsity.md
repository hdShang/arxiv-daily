---
layout: default
title: Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity
---

# Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21411" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21411v2</a>
  <a href="https://arxiv.org/pdf/2505.21411.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21411v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21411v2', 'Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yehui Tang, Xiaosong Li, Fangcheng Liu, Wei Guo, Hang Zhou, Yaoyuan Wang, Kai Han, Xianzhi Yu, Jinpeng Li, Hui Zang, Fei Mi, Xiaojun Meng, Zhicheng Liu, Hanting Chen, Binfan Zheng, Can Chen, Youliang Yan, Ruiming Tang, Peifeng Qin, Xinghao Chen, Dacheng Tao, Yunhe Wang

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27 (Êõ¥Êñ∞: 2025-05-28)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ê∑∑ÂêàÂàÜÁªÑ‰∏ìÂÆ∂Ê®°Âûã‰ª•Ëß£ÂÜ≥‰∏ìÂÆ∂Ë¥üËΩΩ‰∏çÂùáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑∑Âêà‰∏ìÂÆ∂Ê®°Âûã` `ÂàÜÁªÑ‰∏ìÂÆ∂` `Ë¥üËΩΩÂùáË°°` `Ascend NPU` `Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜÊÄßËÉΩ` `ËÆ°ÁÆóÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÂú®‰∏ìÂÆ∂ÊøÄÊ¥ªÊó∂Â≠òÂú®Ë¥üËΩΩ‰∏çÂùáÁöÑÈóÆÈ¢òÔºåÂØºËá¥Á≥ªÁªüÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Ê∑∑ÂêàÂàÜÁªÑ‰∏ìÂÆ∂ÔºàMoGEÔºâÔºåÈÄöËøáÂàÜÁªÑÈÄâÊã©‰∏ìÂÆ∂ÔºåÁ°Æ‰øùÊØèÁªÑ‰∏ìÂÆ∂ÁöÑÊøÄÊ¥ªÊï∞ÈáèÂùáË°°Ôºå‰ªéËÄå‰ºòÂåñËÆ°ÁÆóË¥üËΩΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPangu Pro MoEÂú®Ascend NPUs‰∏äÊé®ÁêÜÊÄßËÉΩÊòæËëóÊèêÂçáÔºåËææÂà∞ÊØèÂç°1148‰∏™token/sÔºåË∂ÖË∂äÂêåÁ±ªÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÔºàMoEÔºâÂú®Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂ∫îÁî®ÔºåËÉΩÂ§ü‰ª•ËæÉ‰ΩéÁöÑÊâßË°åÊàêÊú¨ÂÆûÁé∞Êõ¥Â§ßÁöÑÊ®°ÂûãÂèÇÊï∞ÂíåÂ≠¶‰π†ËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ï‰∏≠Êüê‰∫õ‰∏ìÂÆ∂Ë¢´È¢ëÁπÅÊøÄÊ¥ªÔºåÂØºËá¥Á≥ªÁªüÊïàÁéá‰Ωé‰∏ã„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÊ∑∑ÂêàÂàÜÁªÑ‰∏ìÂÆ∂ÔºàMoGEÔºâÔºåÈÄöËøáÂú®ÈÄâÊã©ËøáÁ®ã‰∏≠ÂØπ‰∏ìÂÆ∂ËøõË°åÂàÜÁªÑÔºåÂπ≥Ë°°‰∏ìÂÆ∂ÁöÑÂ∑•‰ΩúË¥üËΩΩ„ÄÇËØ•ËÆæËÆ°Á°Æ‰øùÂú®Â§öËÆæÂ§áÂàÜÂ∏ÉÂºèÊâßË°åÊó∂ÔºåËÆ°ÁÆóË¥üËΩΩÂùáË°°Ôºå‰ªéËÄåÊòæËëóÊèêÂçáÊé®ÁêÜÈò∂ÊÆµÁöÑÂêûÂêêÈáè„ÄÇÂü∫‰∫éMoGEÊûÑÂª∫ÁöÑPangu Pro MoEÂú®Ascend NPUs‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊé®ÁêÜÊÄßËÉΩËææÂà∞ÊØèÂç°1148‰∏™token/sÔºåÂπ∂ÂèØÈÄöËøáÊé®ÊµãÂä†ÈÄüÊèêÂçáËá≥1528‰∏™token/sÔºåË∂ÖË∂äÂêåÁ±ªÂØÜÈõÜÊ®°Âûã„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊ∑∑Âêà‰∏ìÂÆ∂Ê®°Âûã‰∏≠‰∏ìÂÆ∂ÊøÄÊ¥ª‰∏çÂùáÂØºËá¥ÁöÑÁ≥ªÁªüÊïàÁéá‰Ωé‰∏ãÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏≠ÔºåÊüê‰∫õ‰∏ìÂÆ∂Ë¢´È¢ëÁπÅÊøÄÊ¥ªÔºåËÄåÂÖ∂‰ªñ‰∏ìÂÆ∂ÂàôÂæàÂ∞ëË¢´‰ΩøÁî®ÔºåÈÄ†ÊàêËµÑÊ∫êÊµ™Ë¥πÂíåËÆ°ÁÆóË¥üËΩΩ‰∏çÂùá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫Ê∑∑ÂêàÂàÜÁªÑ‰∏ìÂÆ∂ÔºàMoGEÔºâÔºåÈÄöËøáÂ∞Ü‰∏ìÂÆ∂ÂàÜÁªÑÂπ∂Âú®ÈÄâÊã©Êó∂ÈôêÂà∂ÊØèÁªÑÊøÄÊ¥ªÁöÑ‰∏ìÂÆ∂Êï∞ÈáèÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂùáË°°ÁöÑË¥üËΩΩÂàÜÈÖç„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°ËÉΩÂ§üÂú®Â§öËÆæÂ§áÁéØÂ¢É‰∏≠‰ºòÂåñËÆ°ÁÆóËµÑÊ∫êÁöÑ‰ΩøÁî®ÔºåÊèêÈ´òÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPangu Pro MoEÁöÑÊï¥‰ΩìÊû∂ÊûÑÂü∫‰∫éMoGEÔºåÂåÖÂê´Â§ö‰∏™‰∏ìÂÆ∂ÁªÑÔºåÊØèÁªÑÂÜÖÁöÑ‰∏ìÂÆ∂Âú®Â§ÑÁêÜËæìÂÖ•tokenÊó∂Ë¢´ÂùáÂåÄÊøÄÊ¥ª„ÄÇËØ•Ê®°ÂûãÂú®Ascend NPUs‰∏äËøõË°å‰ºòÂåñÔºåÁ°Æ‰øùÂú®Êé®ÁêÜÂíåËÆ≠ÁªÉÈò∂ÊÆµÂùáËÉΩÈ´òÊïàÊâßË°å„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫Ü‰∏ìÂÆ∂ÂàÜÁªÑÊú∫Âà∂Ôºå‰ΩøÂæóÂú®Â§öËÆæÂ§áÂπ∂Ë°åÂ§ÑÁêÜÊó∂ÔºåËÉΩÂ§üÊúâÊïàÂπ≥Ë°°ÂêÑËÆæÂ§áÁöÑËÆ°ÁÆóË¥üËΩΩ„ÄÇËøô‰∏ÄËÆæËÆ°‰∏é‰º†ÁªüÁöÑMoEÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÂçá‰∫ÜÁ≥ªÁªüÁöÑÊâßË°åÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãÈÖçÁΩÆ‰∏äÔºåPangu Pro MoEÊÄªÂèÇÊï∞Èáè‰∏∫720‰∫øÔºåÂÖ∂‰∏≠ÊØè‰∏™tokenÊøÄÊ¥ª160‰∫øÂèÇÊï∞„ÄÇÈÄöËøáÂØπAscend 300I DuoÂíå800I A2ËøõË°åÁ≥ªÁªüÊ®°ÊãüÔºå‰ºòÂåñ‰∫ÜÊ®°ÂûãÁöÑÊâßË°åÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåPangu Pro MoEÂú®Ascend NPUs‰∏äÁöÑÊé®ÁêÜÊÄßËÉΩËææÂà∞ÊØèÂç°1148‰∏™token/sÔºåÁªèËøáÊé®ÊµãÂä†ÈÄüÂèØÊèêÂçáËá≥1528‰∏™token/sÔºåÊòæËëó‰ºò‰∫éÂêåÁ±ª32BÂíå72BÂØÜÈõÜÊ®°ÂûãÔºåÂ±ïÁé∞Âá∫ÂçìË∂äÁöÑÊÄß‰ª∑ÊØî„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊú∫Âô®ÁøªËØëÂíåÂØπËØùÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéáÔºåPangu Pro MoEËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Êèê‰æõÊõ¥Âø´ÁöÑÂìçÂ∫îÊó∂Èó¥ÂíåÊõ¥È´òÁöÑÂ§ÑÁêÜËÉΩÂäõÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂïÜ‰∏ö‰ª∑ÂÄºÂíåÁ§æ‰ºöÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.

