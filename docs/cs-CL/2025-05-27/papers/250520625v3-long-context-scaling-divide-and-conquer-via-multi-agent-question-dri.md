---
layout: default
title: Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration
---

# Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20625" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.20625v3</a>
  <a href="https://arxiv.org/pdf/2505.20625.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20625v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20625v3', 'Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Sibo Xiao, Zixin Lin, Wenyang Gao, Hui Chen, Yue Zhang

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27 (Êõ¥Êñ∞: 2025-09-28)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫XpandAÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥ÈïøÊñáÊú¨Â§ÑÁêÜ‰∏≠ÁöÑ‰ø°ÊÅØÊçüÂ§±ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÊñáÊú¨Â§ÑÁêÜ` `Â§ö‰ª£ÁêÜÊ°ÜÊû∂` `Âä®ÊÄÅÂàÜÂå∫` `ÈóÆÈ¢òÂºïÂØº` `‰ø°ÊÅØ‰∏ÄËá¥ÊÄß` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ÊÄßËÉΩÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈïøÊñáÊú¨Â§ÑÁêÜÊñπÊ≥ïÂ≠òÂú®Á¥ØÁßØÂª∂ËøüÈ´òÂíå‰ø°ÊÅØÊçüÂ§±Á≠âÈóÆÈ¢òÔºåÂΩ±Âìç‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ
2. ÊèêÂá∫ÁöÑXpandAÊ°ÜÊû∂ÈÄöËøáÂä®ÊÄÅÂàÜÂå∫ÂíåÈóÆÈ¢òÂºïÂØºÂçèËÆÆÊù•‰ºòÂåñÈïøÊñáÊú¨ÁöÑÂ§ÑÁêÜÊµÅÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåXpandAÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫Ü20%ÁöÑÊÄßËÉΩÊèêÂçáÂíå1.5ÂÄçÁöÑÊé®ÁêÜÈÄüÂ∫¶Âä†Âø´„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ÑÁêÜÈïøÊñáÊú¨Â∑≤Êàê‰∏∫Áé∞‰ª£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂÖ≥ÈîÆËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫é‰ª£ÁêÜÁöÑÂàÜÊ≤ªÊñπÊ≥ïÂú®Â§ÑÁêÜÈïøÊñáÊú¨Êó∂Èù¢‰∏¥ÊòæËëóÁöÑÂ±ÄÈôêÊÄßÔºåÂåÖÊã¨Á¥ØÁßØÂª∂ËøüËøáÈ´ò„ÄÅ‰ø°ÊÅØÊçüÂ§±Âä†Ââß‰ª•ÂèäÊñáÊú¨‰æùËµñÂÖ≥Á≥ªÁöÑÁ†¥Âùè„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ§ö‰ª£ÁêÜÊ°ÜÊû∂XpandAÔºåÁªìÂêà‰∫ÜÂü∫‰∫éÈóÆÈ¢òÁöÑÂ∑•‰ΩúÊµÅÁ®ãÂíåÂä®ÊÄÅÂàÜÂå∫ÔºåÊó®Âú®Â¢ûÂº∫ÈïøÊñáÊú¨Â§ÑÁêÜÁöÑÈ≤ÅÊ£íÊÄß„ÄÇXpandAÈÄöËøáÂä®ÊÄÅÂàÜÂå∫„ÄÅÈóÆÈ¢òÂºïÂØºÂçèËÆÆÂíåÈÄâÊã©ÊÄßÈáçÊîæÁâπÂÆöÂàÜÂå∫Êù•ÂÖãÊúçËøô‰∫õÈôêÂà∂„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™ÈïøÊñáÊú¨Âü∫ÂáÜ‰∏äÂØπXpandAËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÁªìÊûúÊòæÁ§∫ÂÖ∂Âú®Â§ÑÁêÜË∂ÖÈïøÂ∫èÂàóÊñπÈù¢ÁöÑÂèØË°åÊÄßÂíåÊòæËëóÊïàÊûúÔºåËæÉÂü∫Á∫øÊñπÊ≥ïÊèêÈ´ò‰∫Ü20%ÁöÑÊÄßËÉΩÂíå1.5ÂÄçÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÈïøÊñáÊú¨Â§ÑÁêÜÊñπÊ≥ïÂú®‰ø°ÊÅØÊçüÂ§±ÂíåÂª∂ËøüÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ª£ÁêÜË∞ÉÁî®ËøáÂ§öÊó∂ÂØºËá¥ÁöÑÊñáÊú¨‰æùËµñÂÖ≥Á≥ªÁ†¥Âùè„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöXpandAÊ°ÜÊû∂ÈÄöËøáÂä®ÊÄÅÂàÜÂå∫ÂíåÈóÆÈ¢òÂºïÂØºÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÁÅµÊ¥ªË∞ÉÊï¥ËæìÂÖ•Â∫èÂàóÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£Â°´ÂÖÖÁéáÔºå‰ªéËÄåÊèêÈ´òÈïøÊñáÊú¨ÁöÑÂ§ÑÁêÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöXpandAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Âä®ÊÄÅÂàÜÂå∫Ê®°Âùó„ÄÅÈóÆÈ¢òÂºïÂØºÂçèËÆÆÂíåÁä∂ÊÄÅË∑üË∏™Êú∫Âà∂„ÄÇÂä®ÊÄÅÂàÜÂå∫Ê®°ÂùóÊ†πÊçÆÊñáÊú¨ÈïøÂ∫¶Ëá™ÈÄÇÂ∫îË∞ÉÊï¥ÂàÜÂå∫Á≠ñÁï•ÔºåÈóÆÈ¢òÂºïÂØºÂçèËÆÆÂàôÂú®ÂÖ±‰∫´ÂÜÖÂ≠ò‰∏≠Êõ¥Êñ∞‰ø°ÊÅØÔºåÁ°Æ‰øùÂêÑ‰ª£ÁêÜÈó¥Áü•ËØÜÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöXpandAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Âä®ÊÄÅÂàÜÂå∫ÂíåÈóÆÈ¢òÂºïÂØºÁöÑÁªìÂêàÔºåËÉΩÂ§üÊúâÊïàÂáèÂ∞ë‰ø°ÊÅØÊçüÂ§±Âπ∂‰øùÊåÅÊñáÊú¨ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåËøô‰∏é‰º†ÁªüÁöÑÈùôÊÄÅÂàÜÂå∫ÊñπÊ≥ïÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®XpandA‰∏≠ÔºåÂä®ÊÄÅÂàÜÂå∫ÁöÑÂ°´ÂÖÖÁéáÂíåÈóÆÈ¢òÂºïÂØºÁöÑÊõ¥Êñ∞Á≠ñÁï•ÊòØÂÖ≥ÈîÆËÆæËÆ°ÂÖÉÁ¥†ÔºåÁ°Æ‰øù‰∫Ü‰ø°ÊÅØÂú®ÂêÑ‰∏™ÂàÜÂå∫Èó¥ÁöÑÊúâÊïà‰º†ÈÄíÂíåÈáçÊîæÔºåËøõËÄåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

XpandAÂú®Â§ö‰∏™ÈïøÊñáÊú¨Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÁõ∏ËæÉ‰∫éÂÖ®‰∏ä‰∏ãÊñá„ÄÅRAGÂíå‰ª•ÂæÄÁöÑ‰ª£ÁêÜÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáËææ20%ÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò1.5ÂÄçÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Â§ÑÁêÜË∂ÖÈïøÂ∫èÂàóÊñπÈù¢ÁöÑÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

XpandAÊ°ÜÊû∂Âú®ÈïøÊñáÊú¨Â§ÑÁêÜÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÈúÄË¶ÅÂàÜÊûêÂíåÁêÜËß£Ë∂ÖÈïøÊñáÊú¨ÁöÑ‰ªªÂä°ÔºåÂ¶ÇÊ≥ïÂæãÊñá‰π¶„ÄÅÂ≠¶ÊúØËÆ∫ÊñáÂíåÈïøÁØáÂ∞èËØ¥Á≠â„ÄÇÂÖ∂È´òÊïàÁöÑ‰ø°ÊÅØÂ§ÑÁêÜËÉΩÂäõÂ∞Ü‰∏∫Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑÂêÑÁ±ªÂ∫îÁî®Êèê‰æõÊîØÊåÅÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Processing long contexts has become a critical capability for modern large language models (LLMs). Existing works leverage agent-based divide-and-conquer methods for processing long contexts. But these methods face crucial limitations, including prohibitive accumulated latency and amplified information loss from excessive agent invocations, and the disruption of inherent textual dependencies by immoderate partitioning. In this paper, we propose a novel multi-agent framework XpandA (Expand-Agent) coupled with question-driven workflow and dynamic partitioning for robust long-context processing. XpandA overcomes these limitations through: 1) dynamic partitioning of long texts, which adaptively modulates the filling rate of context windows for input sequences of vastly varying lengths; 2) question-guided protocol to update flat information ensembles within centralized shared memory, constructing consistent inter-agent knowledge across partitions; and 3) selectively replaying specific partitions based on the state-tracking of question-information couples to promote the resolution of inverted-order structures across partitions (e.g., flashbacks). We perform a comprehensive evaluation of XpandA on multiple long-context benchmarks with length varying from 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long sequences and its significant effectiveness in enhancing the long-context capabilities of various LLMs by achieving 20\% improvements and 1.5x inference speedup over baselines of full-context, RAG and previous agent-based methods.

