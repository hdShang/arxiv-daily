---
layout: default
title: "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models"
---

# EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20888" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20888v2</a>
  <a href="https://arxiv.org/pdf/2505.20888.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20888v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20888v2', 'EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengyu Wang, Junbing Yan, Wenrui Cai, Yuanhao Yue, Jun Huang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-06-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEasyDistillå·¥å…·åŒ…ä»¥æœ‰æ•ˆè¿›è¡Œå¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†è’¸é¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¤§è¯­è¨€æ¨¡å‹` `å·¥å…·åŒ…` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–` `é˜¿é‡Œäº‘` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨å¤„ç†å¤§è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´æ•ˆç‡ä½ä¸‹å’Œçµæ´»æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. EasyDistillé€šè¿‡æä¾›å¤šåŠŸèƒ½æ¨¡å—å’Œç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼Œç®€åŒ–äº†çŸ¥è¯†è’¸é¦çš„è¿‡ç¨‹ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ç±»å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEasyDistillåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†è’¸é¦æ¨¡å‹çš„æ€§èƒ½ï¼Œå¢å¼ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†EasyDistillï¼Œä¸€ä¸ªå…¨é¢çš„å·¥å…·åŒ…ï¼Œæ—¨åœ¨æœ‰æ•ˆè¿›è¡Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é»‘ç®±å’Œç™½ç®±çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ã€‚è¯¥æ¡†æ¶æä¾›å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°æ®åˆæˆã€ç›‘ç£å¾®è°ƒã€æ’åä¼˜åŒ–å’Œä¸“é—¨é’ˆå¯¹KDåœºæ™¯çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ã€‚EasyDistillæ”¯æŒSystem 1ï¼ˆå¿«é€Ÿã€ç›´è§‚ï¼‰å’ŒSystem 2ï¼ˆç¼“æ…¢ã€åˆ†æï¼‰æ¨¡å‹çš„KDåŠŸèƒ½ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡å’Œç”¨æˆ·å‹å¥½çš„ç•Œé¢ä½¿ç ”ç©¶äººå‘˜å’Œè¡Œä¸šä»ä¸šè€…èƒ½å¤Ÿæ— ç¼å®éªŒå’Œå®æ–½æœ€å…ˆè¿›çš„KDç­–ç•¥ã€‚æ­¤å¤–ï¼ŒEasyDistillè¿˜æä¾›äº†ä¸€ç³»åˆ—ç»è¿‡ç¨³å¥è’¸é¦çš„æ¨¡å‹å’ŒåŸºäºKDçš„å·¥ä¸šè§£å†³æ–¹æ¡ˆï¼Œä»¥åŠç›¸åº”çš„å¼€æºæ•°æ®é›†ï¼Œæ»¡è¶³å¤šç§åº”ç”¨åœºæ™¯ã€‚æœ€åï¼Œæˆ‘ä»¬æè¿°äº†EasyDistillä¸é˜¿é‡Œäº‘äººå·¥æ™ºèƒ½å¹³å°ï¼ˆPAIï¼‰çš„æ— ç¼é›†æˆã€‚æ€»ä½“è€Œè¨€ï¼ŒEasyDistillå·¥å…·åŒ…ä½¿å¾—å…ˆè¿›çš„KDæŠ€æœ¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç¤¾åŒºä¸­æ›´æ˜“äºè·å–å’Œåº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„æ•ˆç‡å’Œçµæ´»æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆé€‚åº”ä¸åŒç±»å‹çš„æ¨¡å‹å’Œåº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEasyDistillçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨¡å—åŒ–è®¾è®¡å’Œå¤šåŠŸèƒ½æ”¯æŒï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…èƒ½å¤Ÿæ–¹ä¾¿åœ°è¿›è¡ŒçŸ¥è¯†è’¸é¦å®éªŒï¼Œæ¶µç›–ä»æ•°æ®åˆæˆåˆ°æ¨¡å‹ä¼˜åŒ–çš„å…¨è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEasyDistillçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åˆæˆæ¨¡å—ã€ç›‘ç£å¾®è°ƒæ¨¡å—ã€æ’åä¼˜åŒ–æ¨¡å—å’Œå¼ºåŒ–å­¦ä¹ æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½é’ˆå¯¹ç‰¹å®šçš„KDéœ€æ±‚è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿çµæ´»æ€§å’Œé«˜æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šEasyDistillçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ”¯æŒSystem 1å’ŒSystem 2æ¨¡å‹çš„çŸ¥è¯†è’¸é¦ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„åº”ç”¨éœ€æ±‚é€‰æ‹©åˆé€‚çš„è’¸é¦ç­–ç•¥ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—å·¥å…·åŒ…åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶æ›´åŠ é«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®æ–¹é¢ï¼ŒEasyDistillå…è®¸ç”¨æˆ·è‡ªå®šä¹‰æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚æ­¤å¤–ï¼Œå·¥å…·åŒ…è¿˜æä¾›äº†å¤šç§é¢„è®¾çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿä¸Šæ‰‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒEasyDistillæ˜¾è‘—æå‡äº†è’¸é¦æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜äº†15%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå·¥å…·åŒ…çš„çµæ´»æ€§ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®ä¸åŒéœ€æ±‚è°ƒæ•´è’¸é¦ç­–ç•¥ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å…¶å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EasyDistillåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨¡å‹éƒ¨ç½²å’Œå®æ—¶å“åº”çš„åœºæ™¯ä¸­ï¼Œå¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨ç¿»è¯‘å’Œå†…å®¹ç”Ÿæˆç­‰ã€‚å…¶çµæ´»çš„è®¾è®¡ä½¿å¾—ç ”ç©¶äººå‘˜èƒ½å¤Ÿå¿«é€Ÿè¿­ä»£å’Œä¼˜åŒ–æ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present EasyDistill, a comprehensive toolkit designed for effective black-box and white-box knowledge distillation (KD) of large language models (LLMs). Our framework offers versatile functionalities, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques specifically tailored for KD scenarios. The toolkit accommodates KD functionalities for both System 1 (fast, intuitive) and System 2 (slow, analytical) models. With its modular design and user-friendly interface, EasyDistill empowers researchers and industry practitioners to seamlessly experiment with and implement state-of-the-art KD strategies for LLMs. In addition, EasyDistill provides a series of robust distilled models and KD-based industrial solutions developed by us, along with the corresponding open-sourced datasets, catering to a variety of use cases. Furthermore, we describe the seamless integration of EasyDistill into Alibaba Cloud's Platform for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for LLMs more accessible and impactful within the NLP community.

