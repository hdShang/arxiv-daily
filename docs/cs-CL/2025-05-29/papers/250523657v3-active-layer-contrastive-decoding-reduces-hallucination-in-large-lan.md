---
layout: default
title: Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation
---

# Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23657" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23657v3</a>
  <a href="https://arxiv.org/pdf/2505.23657.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23657v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23657v3', 'Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongxiang Zhang, Hao Chen, Muhao Chen, Tianyi Zhang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-09-15)

**å¤‡æ³¨**: 19 pages, 3 figures, EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸»åŠ¨å±‚å¯¹æ¯”è§£ç ä»¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è§£ç æ–¹æ³•` `å¼ºåŒ–å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `æ–‡æœ¬ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§£ç æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ—¶ï¼Œä»ç„¶é¢ä¸´å¹»è§‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„ä¸»åŠ¨å±‚å¯¹æ¯”è§£ç ï¼ˆActLCDï¼‰ç­–ç•¥ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åŠ¨æ€é€‰æ‹©å¯¹æ¯”å±‚ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­çš„äº‹å®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒActLCDåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆæ–‡æœ¬ä¸­çš„å¹»è§‰ç°è±¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè§£ç æ–¹æ³•é€šè¿‡ä¼˜åŒ–ä¸‹ä¸€ä¸ªæ ‡è®°çš„é€‰æ‹©æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„äº‹å®æ€§ã€‚è¿™äº›æ–¹æ³•é€šå¸¸åœ¨æ ‡è®°çº§åˆ«æ“ä½œï¼Œåˆ©ç”¨å†…éƒ¨è¡¨ç¤ºæ¥æŠ‘åˆ¶è¡¨é¢æ¨¡å¼ã€‚ç„¶è€Œï¼ŒLLMsåœ¨è¾ƒé•¿ä¸Šä¸‹æ–‡ä¸­ä»ç„¶å®¹æ˜“å‡ºç°å¹»è§‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£ç ç­–ç•¥â€”â€”ä¸»åŠ¨å±‚å¯¹æ¯”è§£ç ï¼ˆActLCDï¼‰ï¼Œè¯¥ç­–ç•¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸»åŠ¨å†³å®šä½•æ—¶åº”ç”¨å¯¹æ¯”å±‚ã€‚é€šè¿‡å°†è§£ç è§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼ŒActLCDé‡‡ç”¨ç”±å¥–åŠ±æ„ŸçŸ¥åˆ†ç±»å™¨æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–è¶…è¶Šæ ‡è®°çº§åˆ«çš„äº‹å®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒActLCDåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ ·åŒ–ç”Ÿæˆåœºæ™¯ä¸­å‡è½»å¹»è§‰çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸­çš„å¹»è§‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦åœ¨æ ‡è®°çº§åˆ«è¿›è¡Œä¼˜åŒ–ï¼Œæœªèƒ½æœ‰æ•ˆå¤„ç†é•¿ä¸Šä¸‹æ–‡ä¸­çš„äº‹å®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šä¸»åŠ¨å±‚å¯¹æ¯”è§£ç ï¼ˆActLCDï¼‰é€šè¿‡å°†è§£ç è¿‡ç¨‹è§†ä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥åŠ¨æ€é€‰æ‹©ä½•æ—¶åº”ç”¨å¯¹æ¯”å±‚ï¼Œä»è€Œæå‡ç”Ÿæˆæ–‡æœ¬çš„äº‹å®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šActLCDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¾“å…¥å¤„ç†æ¨¡å—ã€å†³ç­–æ¨¡å—å’Œç”Ÿæˆæ¨¡å—ã€‚è¾“å…¥å¤„ç†æ¨¡å—è´Ÿè´£æ¥æ”¶å’Œé¢„å¤„ç†è¾“å…¥æ–‡æœ¬ï¼Œå†³ç­–æ¨¡å—åŸºäºå½“å‰ä¸Šä¸‹æ–‡å’Œå¥–åŠ±ä¿¡å·é€‰æ‹©å¯¹æ¯”å±‚ï¼Œç”Ÿæˆæ¨¡å—åˆ™è´Ÿè´£ç”Ÿæˆæœ€ç»ˆæ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šActLCDçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åŠ¨æ€å†³ç­–æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯é€‰æ‹©æ€§åœ°åº”ç”¨å¯¹æ¯”å±‚ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†ç”Ÿæˆæ–‡æœ¬çš„äº‹å®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚æ–¹é¢ï¼ŒActLCDä½¿ç”¨äº†å¥–åŠ±æ„ŸçŸ¥åˆ†ç±»å™¨æ¥è¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„äº‹å®æ€§ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å­¦ä¹ ç‡ã€å¯¹æ¯”å±‚çš„é€‰æ‹©ç­–ç•¥ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒActLCDåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆæ–‡æœ¬ä¸­çš„å¹»è§‰ç°è±¡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿå’Œå†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡å‡å°‘ç”Ÿæˆæ–‡æœ¬ä¸­çš„å¹»è§‰ç°è±¡ï¼ŒActLCDå¯ä»¥æé«˜ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œè¿›è€Œå¢å¼ºç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„ç”Ÿæˆä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent decoding methods improve the factuality of large language models (LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios.

