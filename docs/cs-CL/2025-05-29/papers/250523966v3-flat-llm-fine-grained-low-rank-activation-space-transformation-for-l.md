---
layout: default
title: "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression"
---

# FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23966" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23966v3</a>
  <a href="https://arxiv.org/pdf/2505.23966.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23966v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23966v3', 'FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, Zheng Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-07-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFLAT-LLMä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å‹ç¼©ä¸­çš„æ•ˆç‡ä¸å‡†ç¡®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `ä½ç§©åˆ†è§£` `æ¿€æ´»ç©ºé—´` `ä¸»æˆåˆ†åˆ†æ` `ç»“æ„å‰ªæ` `æ¨ç†é€Ÿåº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä½ç§©åˆ†è§£æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹å‹ç¼©ä¸­å¸¸é¢ä¸´å‡†ç¡®æ€§ä¸‹é™å’Œæ ¡å‡†è¿‡ç¨‹æ˜‚è´µçš„é—®é¢˜ã€‚
2. FLAT-LLMé€šè¿‡ç»†ç²’åº¦ä½ç§©å˜æ¢å’Œè´ªå©ªé¢„ç®—é‡åˆ†é…ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆçš„ç»“æ„å‹ç¼©ï¼Œä¸”æ— éœ€è®­ç»ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFLAT-LLMåœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿç»“æ„å‰ªææ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡è¿‘æœŸçš„ä½ç§©åˆ†è§£æ–¹æ³•ä¸ºç»“æ„å‹ç¼©æä¾›äº†æœ‰å¸Œæœ›çš„è·¯å¾„ï¼Œä½†å¸¸å¸¸ä¼´éšå‡†ç¡®æ€§ä¸‹é™ã€æ˜‚è´µçš„æ ¡å‡†ç¨‹åºï¼Œå¹¶å¯¼è‡´ä½æ•ˆçš„æ¨¡å‹æ¶æ„ï¼Œå¦¨ç¢å®é™…æ¨ç†é€Ÿåº¦çš„æå‡ã€‚æœ¬æ–‡æå‡ºFLAT-LLMï¼Œä¸€ç§å¿«é€Ÿä¸”å‡†ç¡®çš„æ— è®­ç»ƒç»“æ„å‹ç¼©æ–¹æ³•ï¼ŒåŸºäºæ¿€æ´»ç©ºé—´ä¸­çš„ç»†ç²’åº¦ä½ç§©å˜æ¢ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡å¤´éƒ¨ä¸»æˆåˆ†åˆ†æè®¡ç®—çš„æˆªæ–­ç‰¹å¾å‘é‡æ¥å˜æ¢æƒé‡ï¼Œä»è€Œå‡å°‘éšè—ç»´åº¦ï¼Œå¹¶é‡‡ç”¨è´ªå©ªé¢„ç®—é‡åˆ†é…ç­–ç•¥åœ¨è§£ç å™¨ä¹‹é—´è‡ªé€‚åº”åˆ†é…ç§©ã€‚FLAT-LLMåœ¨ä¸è¿›è¡Œæ¢å¤å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†é«˜æ•ˆçš„æƒé‡å‹ç¼©ï¼Œæ ¡å‡†è¿‡ç¨‹å¯åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆã€‚ç»è¿‡5ä¸ªæ¨¡å‹å’Œ11ä¸ªæ•°æ®é›†çš„è¯„ä¼°ï¼ŒFLAT-LLMåœ¨æ³›åŒ–å’Œä¸‹æ¸¸æ€§èƒ½ä¸Šä¼˜äºç»“æ„å‰ªæåŸºçº¿ï¼ŒåŒæ—¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šè¶…è¶ŠåŸºäºåˆ†è§£çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²æ—¶çš„é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚é—®é¢˜ã€‚ç°æœ‰çš„ä½ç§©åˆ†è§£æ–¹æ³•è™½ç„¶æä¾›äº†å‹ç¼©çš„å¯èƒ½æ€§ï¼Œä½†å¾€å¾€ä¼´éšå‡†ç¡®æ€§ä¸‹é™å’Œæ ¡å‡†è¿‡ç¨‹å¤æ‚ç­‰ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFLAT-LLMçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¿€æ´»ç©ºé—´ä¸­çš„ç»†ç²’åº¦ä½ç§©å˜æ¢æ¥å®ç°ç»“æ„å‹ç¼©ï¼Œå…·ä½“é‡‡ç”¨å¤´éƒ¨ä¸»æˆåˆ†åˆ†æè®¡ç®—çš„æˆªæ–­ç‰¹å¾å‘é‡æ¥å˜æ¢æƒé‡ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘éšè—ç»´åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFLAT-LLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯é€šè¿‡ä¸»æˆåˆ†åˆ†æè®¡ç®—æƒé‡çš„æˆªæ–­ç‰¹å¾å‘é‡ï¼Œå…¶æ¬¡æ˜¯é‡‡ç”¨è´ªå©ªé¢„ç®—é‡åˆ†é…ç­–ç•¥åœ¨è§£ç å™¨ä¹‹é—´è‡ªé€‚åº”åˆ†é…ç§©ã€‚è¿™ä¸€æµç¨‹ç¡®ä¿äº†å‹ç¼©è¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šFLAT-LLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è®­ç»ƒ-freeçš„ç»“æ„å‹ç¼©æ–¹æ³•ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¸¸è§çš„æ¢å¤å¾®è°ƒæ­¥éª¤ï¼Œæ˜¾è‘—æé«˜äº†å‹ç¼©æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒFLAT-LLMé€šè¿‡è´ªå©ªç®—æ³•åŠ¨æ€è°ƒæ•´å„è§£ç å™¨çš„ç§©åˆ†é…ï¼Œç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­ä¿æŒæ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†æ ¡å‡†æ‰€éœ€çš„æ—¶é—´å’Œèµ„æºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FLAT-LLMåœ¨5ä¸ªæ¨¡å‹å’Œ11ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå…¶åœ¨æ³›åŒ–èƒ½åŠ›å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ç»“æ„å‰ªæåŸºçº¿ï¼Œä¸”åœ¨æ¨ç†é€Ÿåº¦ä¸Šç›¸è¾ƒäºåŸºäºåˆ†è§£çš„æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FLAT-LLMçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—å’Œå…¶ä»–èµ„æºå—é™ç¯å¢ƒä¸­ã€‚é€šè¿‡æœ‰æ•ˆå‹ç¼©å¤§è¯­è¨€æ¨¡å‹ï¼ŒFLAT-LLMèƒ½å¤Ÿä½¿å¾—å¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡åœ¨ä½åŠŸè€—è®¾å¤‡ä¸Šå¾—ä»¥å®ç°ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€å®æ—¶ç¿»è¯‘ç­‰åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis, and employ a greedy budget redistribution strategy to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 5 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods.

