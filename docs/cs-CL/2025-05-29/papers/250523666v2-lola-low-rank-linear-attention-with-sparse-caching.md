---
layout: default
title: "LoLA: Low-Rank Linear Attention With Sparse Caching"
---

# LoLA: Low-Rank Linear Attention With Sparse Caching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23666" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23666v2</a>
  <a href="https://arxiv.org/pdf/2505.23666.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23666v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23666v2', 'LoLA: Low-Rank Linear Attention With Sparse Caching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Luke McDermott, Robert W. Heath, Rahul Parhi

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoLAä»¥æå‡çº¿æ€§æ³¨æ„åŠ›çš„å…³è”è®°å¿†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çº¿æ€§æ³¨æ„åŠ›` `å…³è”è®°å¿†` `ç»ˆèº«å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–` `å†…å­˜ç®¡ç†` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å˜æ¢å™¨æ¨¡å‹åœ¨æ¨ç†æ—¶ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œè®¡ç®—æˆæœ¬æ€¥å‰§ä¸Šå‡ï¼Œé™åˆ¶äº†å…¶åœ¨ç»ˆèº«å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚
2. LoLAé€šè¿‡å°†è¿‡å»çš„é”®å€¼å¯¹åˆ†é…åˆ°ä¸åŒçš„å†…å­˜ç³»ç»Ÿä¸­ï¼Œæå‡äº†çº¿æ€§æ³¨æ„åŠ›çš„å…³è”è®°å¿†èƒ½åŠ›ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoLAåœ¨å…³é”®æ£€ç´¢ä»»åŠ¡ä¸­å‡†ç¡®ç‡ä»0.6%æå‡è‡³97.4%ï¼Œå¹¶ä¸”åœ¨å†…å­˜ä½¿ç”¨ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œå˜æ¢å™¨æ¨ç†çš„æ¯ä¸ªæ ‡è®°æˆæœ¬ä¹Ÿéšä¹‹å¢åŠ ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç»ˆèº«ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚çº¿æ€§æ³¨æ„åŠ›ä½œä¸ºä¸€ç§é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨æ— é™ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ä¿æŒæ’å®šçš„å†…å­˜å ç”¨ï¼Œä½†åœ¨å†…å­˜å®¹é‡ä¸Šå­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†LoLAï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„çº¿æ€§æ³¨æ„åŠ›å¢å¼ºæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å…³è”è®°å¿†çš„å›å¿†èƒ½åŠ›ã€‚LoLAå°†è¿‡å»çš„é”®å€¼å¯¹åˆ†é…åˆ°ä¸‰ä¸ªå†…å­˜ç³»ç»Ÿä¸­ï¼šæœ€è¿‘çš„å¯¹åœ¨å±€éƒ¨æ»‘åŠ¨çª—å£ç¼“å­˜ä¸­ï¼Œéš¾ä»¥è®°å¿†çš„å¯¹åœ¨ç¨€ç–å…¨å±€ç¼“å­˜ä¸­ï¼Œä»¥åŠé€šç”¨çš„å¯¹åœ¨çº¿æ€§æ³¨æ„åŠ›çš„é€’å½’éšè—çŠ¶æ€ä¸­ã€‚é€šè¿‡æ¶ˆèå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è‡ªå›å¿†è¯¯å·®åº¦é‡åœ¨æœ‰æ•ˆç®¡ç†é•¿æœŸå…³è”è®°å¿†ä¸­çš„é‡è¦æ€§ã€‚åœ¨å…³é”®æ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒLoLAå°†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ä»0.6%æå‡è‡³97.4%çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨4Kä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ä½¿ç”¨çš„ç¼“å­˜æ¯”Llama-3.1 8Bå°4.6å€ã€‚LoLAåœ¨é›¶-shotå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿä¼˜äºå…¶ä»–1Bå’Œ8Bå‚æ•°çš„äºšå¹³æ–¹æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å˜æ¢å™¨æ¨¡å‹åœ¨æ¨ç†æ—¶å› ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ è€Œå¯¼è‡´çš„è®¡ç®—æˆæœ¬ä¸Šå‡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»ˆèº«å­¦ä¹ åœºæ™¯ä¸­çš„åº”ç”¨é™åˆ¶ã€‚ç°æœ‰çš„çº¿æ€§æ³¨æ„åŠ›è™½ç„¶åœ¨å†…å­˜å ç”¨ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿æœŸè®°å¿†å®¹é‡æ–¹é¢ä»æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLoLAé€šè¿‡å°†è¿‡å»çš„é”®å€¼å¯¹åˆ†é…åˆ°ä¸‰ä¸ªä¸åŒçš„å†…å­˜ç³»ç»Ÿä¸­ï¼Œå¢å¼ºäº†çº¿æ€§æ³¨æ„åŠ›çš„å…³è”è®°å¿†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œæœ€è¿‘çš„é”®å€¼å¯¹å­˜å‚¨åœ¨å±€éƒ¨æ»‘åŠ¨çª—å£ç¼“å­˜ä¸­ï¼Œéš¾ä»¥è®°å¿†çš„å¯¹å­˜å‚¨åœ¨ç¨€ç–å…¨å±€ç¼“å­˜ä¸­ï¼Œè€Œé€šç”¨çš„å¯¹åˆ™ä¿ç•™åœ¨çº¿æ€§æ³¨æ„åŠ›çš„é€’å½’éšè—çŠ¶æ€ä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLoLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå±€éƒ¨æ»‘åŠ¨çª—å£ç¼“å­˜ã€ç¨€ç–å…¨å±€ç¼“å­˜å’Œé€’å½’éšè—çŠ¶æ€ã€‚æ¯ä¸ªæ¨¡å—è´Ÿè´£ä¸åŒç±»å‹çš„é”®å€¼å¯¹å­˜å‚¨å’Œç®¡ç†ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„è®°å¿†èƒ½åŠ›å’Œæ¨ç†æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLoLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è®­ç»ƒ-freeçš„è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡çº¿æ€§æ³¨æ„åŠ›çš„è®°å¿†èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç®¡ç†å’Œå›å¿†é‡è¦ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LoLAä¸­ï¼Œç¼“å­˜çš„å¤§å°å’Œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨ä¿æŒè¾ƒå°å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–ä¿¡æ¯çš„å›å¿†èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿç»è¿‡å®éªŒéªŒè¯ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LoLAåœ¨å…³é”®æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå‡†ç¡®ç‡ä»0.6%è·ƒå‡è‡³97.4%ã€‚æ­¤å¤–ï¼Œå…¶åœ¨4Kä¸Šä¸‹æ–‡é•¿åº¦ä¸‹çš„ç¼“å­˜ä½¿ç”¨é‡æ¯”Llama-3.1 8Bå°4.6å€ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„å†…å­˜æ•ˆç‡ã€‚LoLAåœ¨é›¶-shotå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿè¶…è¶Šäº†å…¶ä»–1Bå’Œ8Bå‚æ•°çš„äºšå¹³æ–¹æ¨¡å‹ï¼Œå±•ç°äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LoLAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’ŒçŸ¥è¯†æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„è®°å¿†èƒ½åŠ›ï¼ŒLoLAèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒç»ˆèº«å­¦ä¹ å’ŒåŠ¨æ€çŸ¥è¯†æ›´æ–°ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The per-token cost of transformer inference scales with context length, preventing its application to lifelong in-context learning. Linear attention is an efficient alternative that maintains a constant memory footprint, even on infinite context lengths. While this is a potential candidate for lifelong learning, it falls short in memory capacity. In this paper, we propose LoLA, a training-free augmentation to linear attention that boosts associative recall. LoLA distributes past key-value pairs from context into three memory systems: (i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize pairs in a sparse, global cache; and (iii) generic pairs in the recurrent hidden state of linear attention. We show through ablations that our self-recall error metric is crucial to efficiently manage long-term associative memories. On pass-key retrieval tasks, LoLA improves the base model's performance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller cache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B and 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.

