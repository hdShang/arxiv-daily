---
layout: default
title: Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation
---

# Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23701" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23701v1</a>
  <a href="https://arxiv.org/pdf/2505.23701.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23701v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23701v1', 'Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziling Cheng, Meng Cao, Leila Pishdad, Yanshuai Cao, Jackie Chi Kit Cheung

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ†ç¦»è¯„ä¼°æ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ•°å­¦æ¨ç†` `æŠ½è±¡è¡¨è¿°` `ç®—æœ¯è®¡ç®—` `åˆ†ç¦»è¯„ä¼°` `é“¾å¼æ¨ç†` `GSM8K` `SVAMP`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å°†æœ€ç»ˆç­”æ¡ˆä½œä¸ºæ¨ç†èƒ½åŠ›çš„ä»£ç†ï¼Œä½†æœªèƒ½åŒºåˆ†æŠ½è±¡è¡¨è¿°ä¸ç®—æœ¯è®¡ç®—çš„ç‹¬ç«‹æ€§ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡åˆ†ç¦»è¯„ä¼°çš„æ–¹æ³•ï¼Œæ˜ç¡®åŒºåˆ†æŠ½è±¡è¡¨è¿°ä¸ç®—æœ¯è®¡ç®—çš„å½±å“ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLlama-3å’ŒQwen2.5åœ¨ç®—æœ¯è®¡ç®—ä¸Šå­˜åœ¨æ˜¾è‘—ç“¶é¢ˆï¼Œè€ŒæŠ½è±¡è¡¨è¿°èƒ½åŠ›ç›¸å¯¹è¾ƒå¼ºï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿè§‚ç‚¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ–‡å­—é—®é¢˜çš„è¯„ä¼°ä¸­ï¼Œå¸¸ç”¨æœ€ç»ˆç­”æ¡ˆä½œä¸ºæŒ‡æ ‡ï¼Œç„¶è€Œè¿™ç§æ–¹æ³•æ··æ·†äº†æŠ½è±¡è¡¨è¿°ä¸ç®—æœ¯è®¡ç®—ä¸¤ç§ä¸åŒçš„æŠ€èƒ½ã€‚é€šè¿‡å¯¹GSM8Kå’ŒSVAMPæ•°æ®é›†çš„åˆ†ç¦»è¯„ä¼°ï¼Œç ”ç©¶å‘ç°Llama-3å’ŒQwen2.5æ¨¡å‹çš„æœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡ä¸»è¦å—é™äºç®—æœ¯è®¡ç®—ï¼Œè€ŒéæŠ½è±¡è¡¨è¿°ã€‚ä¸æ™®éçœ‹æ³•ç›¸åï¼Œç ”ç©¶è¡¨æ˜é“¾å¼æ¨ç†ï¼ˆCoTï¼‰ä¸»è¦æœ‰åŠ©äºè®¡ç®—ï¼Œå¯¹æŠ½è±¡è¡¨è¿°çš„å½±å“æœ‰é™ã€‚æœºåˆ¶åˆ†ææ˜¾ç¤ºï¼Œè¿™ä¸¤ç§æŠ€èƒ½åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­æ˜¯è”åˆç»„æˆçš„ï¼Œæ¨¡å‹é¦–å…ˆæ•æ‰é—®é¢˜çš„æŠ½è±¡ï¼Œç„¶åè¿›è¡Œè®¡ç®—ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åˆ†ç¦»è¯„ä¼°çš„å¿…è¦æ€§ï¼Œä»¥å‡†ç¡®è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›å¹¶æŒ‡å¯¼æœªæ¥çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•æ··æ·†æŠ½è±¡è¡¨è¿°ä¸ç®—æœ¯è®¡ç®—çš„é—®é¢˜ï¼Œå¯¼è‡´å¯¹å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„è¯¯åˆ¤ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†ç¦»è¯„ä¼°çš„æ–¹å¼ï¼Œæ˜ç¡®åŒºåˆ†æŠ½è±¡è¡¨è¿°ä¸ç®—æœ¯è®¡ç®—çš„å½±å“ï¼Œæ­ç¤ºæ¨¡å‹åœ¨è¿™ä¸¤æ–¹é¢çš„çœŸå®èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†GSM8Kå’ŒSVAMPæ•°æ®é›†ï¼Œè®¾è®¡äº†æŠ½è±¡-è®¡ç®—æœºåˆ¶ï¼Œæ¨¡å‹é¦–å…ˆè¿›è¡Œé—®é¢˜æŠ½è±¡ï¼Œç„¶åæ‰§è¡Œè®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºæå‡ºäº†åˆ†ç¦»è¯„ä¼°æ–¹æ³•ï¼Œå¼ºè°ƒæŠ½è±¡è¡¨è¿°ä¸ç®—æœ¯è®¡ç®—çš„ç‹¬ç«‹æ€§ï¼ŒæŒ‘æˆ˜äº†é“¾å¼æ¨ç†å¯¹æŠ½è±¡èƒ½åŠ›çš„ä¼ ç»Ÿçœ‹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆ1B-32Bï¼‰ï¼Œå¹¶é€šè¿‡å› æœè¡¥ä¸éªŒè¯æŠ½è±¡èƒ½åŠ›çš„å­˜åœ¨ã€å¯è½¬ç§»æ€§å’Œå¯ç»„åˆæ€§ï¼Œç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLlama-3å’ŒQwen2.5åœ¨ç®—æœ¯è®¡ç®—ä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—ä½äºæŠ½è±¡è¡¨è¿°ï¼Œæœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®ç‡ä¸»è¦å—é™äºè®¡ç®—æ­¥éª¤ã€‚å…·ä½“æ•°æ®æ˜¾ç¤ºï¼Œæœªä½¿ç”¨é“¾å¼æ¨ç†çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨ç®—æœ¯è®¡ç®—ä¸Šçš„è¡¨ç°æ˜æ˜¾ä½äºæŠ½è±¡èƒ½åŠ›ï¼Œå¼ºè°ƒäº†åˆ†ç¦»è¯„ä¼°çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œæ•°å­¦é—®é¢˜æ±‚è§£å·¥å…·ã€‚é€šè¿‡æ›´å‡†ç¡®çš„è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘æ›´å¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡å…¶åœ¨æ•°å­¦æ¨ç†å’Œæ•™è‚²é¢†åŸŸçš„å®é™…åº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¿™ç§åˆ†ç¦»è¯„ä¼°æ–¹æ³•å¯èƒ½ä¼šè¢«å¹¿æ³›åº”ç”¨äºå…¶ä»–ç±»å‹çš„æ¨ç†ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Final-answer-based metrics are commonly used for evaluating large language models (LLMs) on math word problems, often taken as proxies for reasoning ability. However, such metrics conflate two distinct sub-skills: abstract formulation (capturing mathematical relationships using expressions) and arithmetic computation (executing the calculations). Through a disentangled evaluation on GSM8K and SVAMP, we find that the final-answer accuracy of Llama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the arithmetic computation step and not by the abstract formulation step. Contrary to the common belief, we show that CoT primarily aids in computation, with limited impact on abstract formulation. Mechanistically, we show that these two skills are composed conjunctively even in a single forward pass without any reasoning steps via an abstract-then-compute mechanism: models first capture problem abstractions, then handle computation. Causal patching confirms these abstractions are present, transferable, composable, and precede computation. These behavioural and mechanistic findings highlight the need for disentangled evaluation to accurately assess LLM reasoning and to guide future improvements.

