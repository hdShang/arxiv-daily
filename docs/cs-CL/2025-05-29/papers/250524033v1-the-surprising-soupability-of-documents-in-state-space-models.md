---
layout: default
title: The Surprising Soupability of Documents in State Space Models
---

# The Surprising Soupability of Documents in State Space Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24033" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24033v1</a>
  <a href="https://arxiv.org/pdf/2505.24033.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24033v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24033v1', 'The Surprising Soupability of Documents in State Space Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yasaman Jafari, Zixian Wang, Leon Bergen, Taylor Berg-Kirkpatrick

**åˆ†ç±»**: cs.CL, cs.CE, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–‡æ¡£åˆå¹¶ç­–ç•¥ä»¥æå‡çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŠ¶æ€ç©ºé—´æ¨¡å‹` `æ–‡æ¡£åˆå¹¶` `å¤šè·³é—®ç­”` `ä¿¡æ¯æ£€ç´¢` `é•¿æ–‡æ¡£æ¨ç†` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æ¡£å’Œå¤æ‚æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æ•ˆç‡å’Œå‡†ç¡®æ€§æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºçš„æ–‡æ¡£åˆå¹¶ç­–ç•¥å…è®¸ç‹¬ç«‹ç¼–ç æ–‡æ¡£å¹¶é€šè¿‡ç®€å•æ“ä½œæ±‡èšè¡¨ç¤ºï¼Œæ”¯æŒæ¨¡å—åŒ–é‡ç”¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ–‡æ¡£åˆå¹¶åœ¨å¤šè·³é—®ç­”å’Œé•¿æ–‡æ¡£æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¥è¿‘äº¤å‰ç¼–ç å™¨çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä¸­çš„éšè—çŠ¶æ€æ˜¯å¦å¯ä»¥åæœŸåˆå¹¶ä»¥æ”¯æŒä¸‹æ¸¸æ¨ç†ã€‚å—æ¨¡å‹åˆå¹¶å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç­–ç•¥ï¼Œé€šè¿‡ç®€å•çš„æ“ä½œï¼ˆå¦‚å¹³å‡ï¼‰å°†ç‹¬ç«‹ç¼–ç çš„æ–‡æ¡£è¡¨ç¤ºæ±‡èšä¸ºå•ä¸€ä¸Šä¸‹æ–‡çŠ¶æ€ã€‚è¿™ç§ç§°ä¸ºæ–‡æ¡£åˆå¹¶çš„æ–¹æ³•ä½¿å¾—æ¨¡å—åŒ–ç¼–ç å’Œé‡ç”¨æˆä¸ºå¯èƒ½ï¼Œæ— éœ€å¯¹æ¯ä¸ªæŸ¥è¯¢é‡æ–°å¤„ç†å®Œæ•´è¾“å…¥ã€‚æˆ‘ä»¬å¯¹Mamba2æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥ç”Ÿæˆå¯åˆå¹¶çš„è¡¨ç¤ºï¼Œå¹¶å‘ç°å®ƒä»¬åœ¨å¤šè·³é—®ç­”ã€ç¨€ç–æ£€ç´¢å’Œé•¿æ–‡æ¡£æ¨ç†ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å‡†ç¡®æ€§ã€‚åœ¨HotpotQAä¸Šï¼Œåä¸ªç‹¬ç«‹ç¼–ç çš„æ–‡æ¡£åˆå¹¶çš„æ€§èƒ½å‡ ä¹ä¸åœ¨ç›¸åŒè¾“å…¥ä¸Šè®­ç»ƒçš„äº¤å‰ç¼–ç å™¨ç›¸å½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æ¡£å’Œå¤æ‚æ¨ç†æ—¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¯¹å®Œæ•´è¾“å…¥è¿›è¡Œé‡å¤å¤„ç†ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹å’Œå“åº”æ—¶é—´å»¶é•¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ–‡æ¡£åˆå¹¶ç­–ç•¥é€šè¿‡ç‹¬ç«‹ç¼–ç æ–‡æ¡£å¹¶å°†å…¶è¡¨ç¤ºæ±‡èšä¸ºå•ä¸€ä¸Šä¸‹æ–‡çŠ¶æ€ï¼Œæ—¨åœ¨å®ç°æ¨¡å—åŒ–ç¼–ç å’Œé‡ç”¨ï¼Œé¿å…é‡å¤å¤„ç†ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨é¢å¯¹ä¸åŒæŸ¥è¯¢æ—¶èƒ½å¤Ÿå¿«é€Ÿå“åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æ¡£çš„ç‹¬ç«‹ç¼–ç æ¨¡å—å’Œåˆå¹¶æ¨¡å—ã€‚é¦–å…ˆï¼Œå¤šä¸ªæ–‡æ¡£é€šè¿‡Mamba2æ¨¡å‹ç‹¬ç«‹ç¼–ç ï¼Œéšåä½¿ç”¨ç®€å•çš„èšåˆæ“ä½œï¼ˆå¦‚å¹³å‡ï¼‰å°†è¿™äº›è¡¨ç¤ºåˆå¹¶ä¸ºä¸€ä¸ªä¸Šä¸‹æ–‡çŠ¶æ€ï¼Œä¾›åç»­æ¨ç†ä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†æ–‡æ¡£åˆå¹¶çš„æ¦‚å¿µï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸é‡æ–°å¤„ç†å®Œæ•´è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œçµæ´»åº”å¯¹å¤šç§æŸ¥è¯¢ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„äº¤å‰ç¼–ç å™¨æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…éœ€è¦å¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œè”åˆå¤„ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆå¹¶åçš„è¡¨ç¤ºè´¨é‡ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥ç¡®ä¿åˆå¹¶æ“ä½œçš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–‡æ¡£åˆå¹¶ç­–ç•¥åœ¨HotpotQAæ•°æ®é›†ä¸Šï¼Œåä¸ªç‹¬ç«‹ç¼–ç çš„æ–‡æ¡£åˆå¹¶åçš„æ€§èƒ½å‡ ä¹ä¸åŒæ ·è¾“å…¥ä¸Šè®­ç»ƒçš„äº¤å‰ç¼–ç å™¨ç›¸å½“ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¤šè·³é—®ç­”å’Œé•¿æ–‡æ¡£æ¨ç†èƒ½åŠ›ï¼Œå‡†ç¡®æ€§æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢å’Œé•¿æ–‡æ¡£ç†è§£ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæ–‡æ¡£åˆå¹¶ç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.

