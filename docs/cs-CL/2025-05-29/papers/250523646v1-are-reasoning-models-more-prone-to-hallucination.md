---
layout: default
title: Are Reasoning Models More Prone to Hallucination?
---

# Are Reasoning Models More Prone to Hallucination?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23646" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.23646v1</a>
  <a href="https://arxiv.org/pdf/2505.23646.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23646v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23646v1', 'Are Reasoning Models More Prone to Hallucination?')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, Tat-Seng Chua

**ÂàÜÁ±ª**: cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-29

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Êé¢ËÆ®Êé®ÁêÜÊ®°ÂûãÂú®ÂπªËßâÁé∞Ë±°‰∏≠ÁöÑËÑÜÂº±ÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãÊé®ÁêÜÊ®°Âûã` `ÂπªËßâÁé∞Ë±°` `ÂÜ∑ÂêØÂä®ÂæÆË∞É` `Âº∫ÂåñÂ≠¶‰π†` `‰∫ãÂÆûÊü•ËØ¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®‰∫ãÂÆûÊü•ËØ¢‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰∏ÄÔºåÂ≠òÂú®ÂπªËßâÁé∞Ë±°ÁöÑ‰∏•ÈáçÊÄßÂ∑ÆÂºÇ„ÄÇ
2. Êú¨ÊñáÈÄöËøáÂÖ®Èù¢ËØÑ‰º∞LRMsÁöÑÂπªËßâÁé∞Ë±°ÔºåÊèêÂá∫ÂÜ∑ÂêØÂä®ÁõëÁù£ÂæÆË∞ÉÂíåÂèØÈ™åËØÅÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†ËÉΩÊúâÊïàÁºìËß£ÂπªËßâ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂêéËÆ≠ÁªÉÊµÅÁ®ãÁöÑ‰∏çÂêå‰ºöÊòæËëóÂΩ±ÂìçLRMsÁöÑÂπªËßâË°®Áé∞ÔºåÊè≠Á§∫‰∫ÜÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄß‰∏é‰∫ãÂÆûÂáÜÁ°ÆÊÄß‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÂèëÂ±ïÁöÑÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈìæÂºèÊé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂú®‰∫ãÂÆûÊü•ËØ¢‰ªªÂä°‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõÊòØÂê¶ËÉΩÊúâÊïàÂáèÂ∞ëÂπªËßâÁé∞Ë±°‰ªçÂ≠òÂú®‰∫âËÆÆ„ÄÇÊú¨Êñá‰ªé‰∏â‰∏™ÊñπÈù¢Êé¢ËÆ®‰∫ÜÊé®ÁêÜÊ®°ÂûãÊòØÂê¶Êõ¥ÂÆπÊòì‰∫ßÁîüÂπªËßâÔºåÂàÜÊûê‰∫Ü‰∏çÂêåÂêéËÆ≠ÁªÉÊµÅÁ®ãÂØπÂπªËßâÁöÑÂΩ±ÂìçÔºåÂπ∂Êè≠Á§∫‰∫ÜÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄß‰∏é‰∫ãÂÆûÂáÜÁ°ÆÊÄß‰πãÈó¥ÁöÑÈîô‰ΩçÂÖ≥Á≥ªÔºå‰∏∫ÁêÜËß£LRMs‰∏≠ÁöÑÂπªËßâÁé∞Ë±°Êèê‰æõ‰∫ÜÂàùÊ≠•ËÆ§ËØÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Êé¢ËÆ®Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®‰∫ãÂÆûÊü•ËØ¢‰ªªÂä°‰∏≠‰∫ßÁîüÂπªËßâÁöÑÁé∞Ë±°ÂèäÂÖ∂ÂéüÂõ†„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÂêéËÆ≠ÁªÉËøáÁ®ã‰∏≠Êú™ËÉΩÊúâÊïàÂáèÂ∞ëÂπªËßâÔºåÂØºËá¥ÊÄßËÉΩ‰∏çÁ®≥ÂÆö„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂØπLRMsËøõË°åÂÖ®Èù¢ËØÑ‰º∞ÔºåÂàÜÊûê‰∏çÂêåÂêéËÆ≠ÁªÉÊµÅÁ®ãÂØπÂπªËßâÁöÑÂΩ±ÂìçÔºåÊèêÂá∫ÂÜ∑ÂêØÂä®ÁõëÁù£ÂæÆË∞ÉÂíåÂèØÈ™åËØÅÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†‰Ωú‰∏∫ÁºìËß£ÂπªËßâÁöÑÊúâÊïàÁ≠ñÁï•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÂàÜ‰∏∫‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÂÖ®Èù¢ËØÑ‰º∞LRMsÁöÑÂπªËßâÁé∞Ë±°Ôºõ2) Ë°å‰∏∫ÂàÜÊûêÔºåËØÜÂà´ÂΩ±Âìç‰∫ãÂÆûÊÄßÁöÑÂÖ≥ÈîÆËÆ§Áü•Ë°å‰∏∫Ôºõ3) ‰ªéÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄßËßíÂ∫¶Êé¢ËÆ®ÂπªËßâÊú∫Âà∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÈ¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞ÂàÜÊûê‰∫Ü‰∏çÂêåÂêéËÆ≠ÁªÉÊµÅÁ®ãÂØπLRMsÂπªËßâÁé∞Ë±°ÁöÑÂΩ±ÂìçÔºåÊè≠Á§∫‰∫ÜÂÜ∑ÂêØÂä®ÂæÆË∞ÉÂíåÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†ÁöÑÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåÈááÁî®‰∫ÜÂÜ∑ÂêØÂä®ÁõëÁù£ÂæÆË∞ÉÂíåÂèØÈ™åËØÅÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†ÁöÑÁªÑÂêàÔºåÂàÜÊûê‰∫ÜFlaw RepetitionÂíåThink-Answer MismatchÁ≠âËÆ§Áü•Ë°å‰∏∫ÂØπÂπªËßâÁöÑÂΩ±Âìç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÈááÁî®ÂÜ∑ÂêØÂä®ÁõëÁù£ÂæÆË∞ÉÂíåÂèØÈ™åËØÅÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†ÁöÑLRMsÂú®SimpleQAÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÂπªËßâÂáèÂ∞ëÔºåÁõ∏ËæÉ‰∫é‰ªÖ‰ΩøÁî®Ëí∏È¶èÂíåÊó†ÂÜ∑ÂêØÂä®ÂæÆË∞ÉÁöÑÊ®°ÂûãÔºåÂπªËßâÁé∞Ë±°ÂáèËΩª‰∫ÜÁ∫¶20%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂‰∏∫Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÈù†ÊÄßÊèê‰æõ‰∫ÜÈáçË¶ÅËßÅËß£ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÈ´òÂáÜÁ°ÆÊÄßÁöÑ‰∫ãÂÆûÊü•ËØ¢‰ªªÂä°‰∏≠„ÄÇÊú™Êù•ÂèØÂú®Êô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅËá™Âä®ÂåñÂÆ¢ÊúçÁ≠âÈ¢ÜÂüüÂπøÊ≥õÂ∫îÁî®Ôºå‰ª•ÊèêÂçáÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs.

