---
layout: default
title: One Task Vector is not Enough: A Large-Scale Study for In-Context Learning
---

# One Task Vector is not Enough: A Large-Scale Study for In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23911" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23911v1</a>
  <a href="https://arxiv.org/pdf/2505.23911.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23911v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23911v1', 'One Task Vector is not Enough: A Large-Scale Study for In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pavel Tikhonov, Ivan Oseledets, Elena Tutubalina

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQuiteAFewæ•°æ®é›†ä»¥æå‡ä¸Šä¸‹æ–‡å­¦ä¹ çš„ä»»åŠ¡å‘é‡è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `ä»»åŠ¡å‘é‡` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®é›†` `å¤šæ ·åŒ–ä»»åŠ¡` `æ€§èƒ½åˆ†æ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•åœ¨å°è§„æ¨¡åŸºå‡†ä¸Šè¡¨ç°æœ‰é™ï¼Œæ— æ³•å…¨é¢åˆ†æä»»åŠ¡å‘é‡çš„æœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡æå‡ºQuiteAFewæ•°æ®é›†ï¼ŒåŒ…å«3096ä¸ªå¤šæ ·åŒ–çš„å°‘é‡ä»»åŠ¡ï¼Œæ—¨åœ¨æ·±å…¥ç ”ç©¶ä»»åŠ¡å‘é‡çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»»åŠ¡å‘é‡åœ¨ä¸­é—´å±‚è¡¨ç°æœ€ä½³ï¼Œå¤æ‚ä»»åŠ¡éœ€è¦å¤šä¸ªå­ä»»åŠ¡å‘é‡ï¼ŒæŒ‘æˆ˜äº†å•ä¸€å‘é‡çš„å‡è®¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡å°‘é‡ç¤ºä¾‹é€‚åº”æ–°ä»»åŠ¡ï¼Œä»»åŠ¡å‘é‡è¢«å‡è®¾ä¸ºç¼–ç ä»»åŠ¡ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å—é™äºå°è§„æ¨¡åŸºå‡†ï¼Œé™åˆ¶äº†å…¨é¢åˆ†æã€‚æœ¬æ–‡å¼•å…¥QuiteAFewï¼Œä¸€ä¸ªåŒ…å«3096ä¸ªå¤šæ ·åŒ–å°‘é‡ä»»åŠ¡çš„æ–°æ•°æ®é›†ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«30ä¸ªè¾“å…¥-è¾“å‡ºå¯¹ï¼Œæºè‡ªAlpacaæ•°æ®é›†ã€‚å¯¹Llama-3-8Båœ¨QuiteAFewä¸Šçš„å®éªŒè¡¨æ˜ï¼šä»»åŠ¡å‘é‡æ€§èƒ½åœ¨ä¸­é—´å±‚ï¼ˆå¦‚ç¬¬15å±‚ï¼‰è¾¾åˆ°å³°å€¼ï¼Œæ•ˆæœå› ä»»åŠ¡ç±»å‹æ˜¾è‘—å˜åŒ–ï¼Œå¤æ‚ä»»åŠ¡ä¾èµ–äºå¤šä¸ªå­ä»»åŠ¡ç‰¹å®šå‘é‡è€Œéå•ä¸€å‘é‡ï¼Œæš—ç¤ºåˆ†å¸ƒå¼ä»»åŠ¡çŸ¥è¯†è¡¨ç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•åœ¨å°è§„æ¨¡åŸºå‡†ä¸‹çš„å±€é™æ€§ï¼Œæ— æ³•å…¨é¢è¯„ä¼°ä»»åŠ¡å‘é‡çš„æœ‰æ•ˆæ€§å’Œè¡¨ç°å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥QuiteAFewæ•°æ®é›†ï¼Œæä¾›3096ä¸ªå¤šæ ·åŒ–çš„å°‘é‡ä»»åŠ¡ï¼Œä»¥ä¾¿å¯¹ä»»åŠ¡å‘é‡çš„è¡¨ç°è¿›è¡Œæ·±å…¥åˆ†æï¼Œæ¢ç´¢å…¶åœ¨ä¸åŒä»»åŠ¡ç±»å‹ä¸­çš„è¡¨ç°å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨Llama-3-8Bæ¨¡å‹ï¼Œåœ¨QuiteAFewæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œåˆ†æä»»åŠ¡å‘é‡åœ¨ä¸åŒå±‚æ¬¡çš„è¡¨ç°ï¼Œç‰¹åˆ«å…³æ³¨ä¸­é—´å±‚çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¤šä¸ªå­ä»»åŠ¡ç‰¹å®šå‘é‡çš„æ¦‚å¿µï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿä¸Šè®¤ä¸ºå•ä¸€ä»»åŠ¡å‘é‡è¶³ä»¥è¡¨ç¤ºä»»åŠ¡çŸ¥è¯†çš„è§‚ç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†3096ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«30ä¸ªè¾“å…¥-è¾“å‡ºå¯¹ï¼Œé‡ç‚¹åˆ†æäº†ä»»åŠ¡å‘é‡åœ¨ç¬¬15å±‚çš„è¡¨ç°ï¼Œå¹¶æ¯”è¾ƒäº†ä¸åŒä»»åŠ¡ç±»å‹çš„æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»»åŠ¡å‘é‡åœ¨ä¸­é—´å±‚ï¼ˆç¬¬15å±‚ï¼‰è¡¨ç°æœ€ä½³ï¼Œå¤æ‚ä»»åŠ¡çš„æ€§èƒ½ä¾èµ–äºå¤šä¸ªå­ä»»åŠ¡ç‰¹å®šå‘é‡ï¼Œè¡¨æ˜ä»»åŠ¡çŸ¥è¯†çš„åˆ†å¸ƒå¼è¡¨ç¤ºã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæœ¬æ–‡çš„ç ”ç©¶ä¸ºç†è§£å’Œä¼˜åŒ–ä¸Šä¸‹æ–‡å­¦ä¹ æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹å’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æå‡ä¸Šä¸‹æ–‡å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒæ¨¡å‹åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks using few examples, with task vectors - specific hidden state activations - hypothesized to encode task information. Existing studies are limited by small-scale benchmarks, restricting comprehensive analysis. We introduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with 30 input-output pairs derived from the Alpaca dataset. Experiments with Llama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an intermediate layer (e.g., 15th), (2) effectiveness varies significantly by task type, and (3) complex tasks rely on multiple, subtask-specific vectors rather than a single vector, suggesting distributed task knowledge representation.

