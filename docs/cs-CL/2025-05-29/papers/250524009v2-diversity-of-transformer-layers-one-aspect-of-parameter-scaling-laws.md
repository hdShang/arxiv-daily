---
layout: default
title: "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws"
---

# Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24009" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24009v2</a>
  <a href="https://arxiv.org/pdf/2505.24009.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24009v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24009v2', 'Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-06-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå±‚é—´å¤šæ ·æ€§åˆ†æä»¥ä¼˜åŒ–Transformerå‚æ•°æ‰©å±•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Transformer` `å‚æ•°æ‰©å±•æ³•åˆ™` `å¤šæ ·æ€§åˆ†æ` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–` `ä¿¡æ¯è®º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¯¹Transformerå†…éƒ¨æœºåˆ¶ä¸å‚æ•°æ‰©å±•æ³•åˆ™çš„å…³ç³»ç†è§£ä¸è¶³ï¼Œå½±å“äº†æ¨¡å‹æ€§èƒ½çš„ä¼˜åŒ–ã€‚
2. è®ºæ–‡é€šè¿‡åå·®-å¤šæ ·æ€§åˆ†è§£ç†è®ºï¼Œåˆ†æTransformerå±‚çš„è¾“å‡ºï¼Œå¼ºè°ƒå±‚é—´å¤šæ ·æ€§å¯¹æ€§èƒ½æå‡çš„é‡è¦æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¢åŠ å±‚æ•°æ—¶ï¼Œåªæœ‰å½“å±‚é—´è¾“å‡ºå¤šæ ·æ€§è¾ƒé«˜æ—¶ï¼Œæ€§èƒ½æ‰ä¼šæ˜¾è‘—æå‡ï¼Œä¸”è¾¹é™…æ•ˆåº”é€’å‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformeråœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»æµæ¶æ„ã€‚å°½ç®¡å‚æ•°æ‰©å±•æ³•åˆ™è¡¨æ˜å¢åŠ å‚æ•°é‡èƒ½æå‡æ€§èƒ½ï¼Œä½†å±‚ä¸å‚æ•°æ‰©å±•æ³•åˆ™ä¹‹é—´çš„å…³ç³»å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡é€šè¿‡åå·®-å¤šæ ·æ€§åˆ†è§£ç†è®ºï¼Œåˆ†æäº†Transformerå†…éƒ¨å±‚çš„è¡Œä¸ºï¼Œå‘ç°å±‚çš„å¤šæ ·æ€§åœ¨æå‡æ€§èƒ½ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å±‚è¾“å‡ºè¿œç¦»çœŸå®å€¼æ—¶ã€‚å®éªŒç»“æœéªŒè¯äº†ç†è®ºå‘ç°ï¼Œå¢åŠ å±‚æ•°çš„æ€§èƒ½æå‡è¡¨ç°å‡ºè¾¹é™…é€’å‡ç‰¹æ€§ï¼Œç¬¦åˆå‚æ•°æ‰©å±•æ³•åˆ™çš„å¯¹æ•°æ”¶æ•›é¢„æµ‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨Transformerå±‚çš„å¤šæ ·æ€§å¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ­ç¤ºå±‚é—´è¾“å‡ºçš„ç›¸äº’å…³ç³»åŠå…¶å¯¹æ€§èƒ½çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åå·®-å¤šæ ·æ€§åˆ†è§£ï¼Œåˆ†ææ¯å±‚è¾“å‡ºçš„åå·®å’Œå¤šæ ·æ€§ï¼Œæå‡ºå±‚é—´å¤šæ ·æ€§æ˜¯æå‡æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œå°¤å…¶åœ¨è¾“å‡ºåç¦»çœŸå®å€¼æ—¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆç†è®ºåˆ†æTransformerå±‚çš„æ®‹å·®æµï¼Œæ¥ç€é€šè¿‡ä¿¡æ¯è®ºæ–¹æ³•é‡åŒ–å±‚é—´å¤šæ ·æ€§ï¼Œæœ€åé€šè¿‡å¤šä¸ªè¯­ä¹‰ç†è§£ä»»åŠ¡è¿›è¡Œå®è¯éªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†ä¿¡æ¯è®ºè§†è§’ä¸‹çš„å¤šæ ·æ€§åº¦é‡ï¼Œæ­ç¤ºäº†å±‚æ•°å¢åŠ ä¸æ€§èƒ½æå‡ä¹‹é—´çš„è¾¹é™…é€’å‡å…³ç³»ï¼Œå¡«è¡¥äº†ç†è®ºä¸å®è·µä¹‹é—´çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒå±‚æ•°çš„Transformeræ¨¡å‹ï¼Œä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¯„ä¼°å±‚è¾“å‡ºçš„åå·®å’Œå¤šæ ·æ€§ï¼Œç¡®ä¿å®éªŒç»“æœçš„å¯é æ€§ä¸æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¢åŠ å±‚æ•°æ—¶ï¼Œåªæœ‰å½“å±‚é—´è¾“å‡ºå…·æœ‰è¾ƒé«˜å¤šæ ·æ€§æ—¶ï¼Œæ¨¡å‹æ€§èƒ½æ‰æ˜¾è‘—æå‡ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½æå‡å¹…åº¦åœ¨å¤šä¸ªè¯­ä¹‰ç†è§£ä»»åŠ¡ä¸­è¾¾åˆ°äº†10%ä»¥ä¸Šï¼ŒéªŒè¯äº†ç†è®ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºTransformeræ¨¡å‹çš„è®¾è®¡ä¸ä¼˜åŒ–æä¾›äº†æ–°çš„è§†è§’ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ä¸­ï¼Œå¼ºè°ƒå±‚é—´å¤šæ ·æ€§çš„é‡è¦æ€§ã€‚æœªæ¥å¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä»¥æå‡æ¨¡å‹çš„æ€§èƒ½ä¸æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.

