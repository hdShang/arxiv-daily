---
layout: default
title: Retrieval Augmented Generation based Large Language Models for Causality Mining
---

# Retrieval Augmented Generation based Large Language Models for Causality Mining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23944" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23944v1</a>
  <a href="https://arxiv.org/pdf/2505.23944.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23944v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23944v1', 'Retrieval Augmented Generation based Large Language Models for Causality Mining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thushara Manjari Naduvilakandy, Hyeju Jang, Mohammad Al Hasan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: 13 pages, 6 figures, published in knowledgeNLP-NAACL2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„åŠ¨æ€æç¤ºæ–¹æ¡ˆä»¥æå‡å› æœå…³ç³»æŒ–æ˜æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœå…³ç³»æ£€æµ‹` `ä¿¡æ¯æå–` `çŸ¥è¯†å›¾è°±` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `åŠ¨æ€æç¤º` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ— ç›‘ç£å› æœå…³ç³»æ£€æµ‹æ–¹æ³•æ€§èƒ½è¾ƒå·®ï¼Œä¸”éœ€å¤§é‡äººå·¥å¹²é¢„ï¼Œå¯¼è‡´åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„åŠ¨æ€æç¤ºæ–¹æ¡ˆï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å› æœå…³ç³»æ£€æµ‹ä¸æå–ä»»åŠ¡ä¸­çš„æ•ˆæœã€‚
3. é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†æ‰€ææ–¹æ¡ˆåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é™æ€æç¤ºæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å› æœå…³ç³»æ£€æµ‹ä¸æŒ–æ˜åœ¨ä¿¡æ¯æ£€ç´¢ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¿æ³›åº”ç”¨äºä¿¡æ¯æå–å’ŒçŸ¥è¯†å›¾è°±æ„å»ºã€‚ç°æœ‰æ–¹æ³•ä¸­ï¼Œæ— ç›‘ç£æ–¹æ³•æ€§èƒ½è¾ƒå·®ä¸”éœ€å¤§é‡äººå·¥å¹²é¢„ï¼Œç›‘ç£æ–¹æ³•åˆ™é¢ä¸´è®­ç»ƒæ•°æ®é›†ä¸è¶³çš„é—®é¢˜ã€‚è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆæœ‰æ•ˆçš„æç¤ºå·¥ç¨‹æ˜¾ç¤ºå‡ºè§£å†³æ•°æ®é›†ä¸è¶³é—®é¢˜çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®ä¸­ç¼ºä¹åŸºäºLLMæç¤ºçš„å› æœå…³ç³»æ£€æµ‹ä¸æŒ–æ˜çš„ç»¼åˆç ”ç©¶ã€‚æœ¬æ–‡æå‡ºå¤šç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„åŠ¨æ€æç¤ºæ–¹æ¡ˆï¼Œä»¥æå‡LLMåœ¨å› æœå…³ç³»æ£€æµ‹ä¸æå–ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œäº”ç§LLMä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„RAGåŠ¨æ€æç¤ºæ–¹æ¡ˆç›¸è¾ƒäºé™æ€æç¤ºæ–¹æ¡ˆçš„ä¼˜è¶Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å› æœå…³ç³»æ£€æµ‹ä¸æŒ–æ˜ä¸­çš„æ•°æ®é›†ä¸è¶³å’Œç°æœ‰æ–¹æ³•æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚æ— ç›‘ç£æ–¹æ³•ä¾èµ–äººå·¥å¹²é¢„ï¼Œè€Œç›‘ç£æ–¹æ³•ç¼ºä¹è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„åŠ¨æ€æç¤ºæ–¹æ¡ˆï¼Œé€šè¿‡æœ‰æ•ˆçš„æç¤ºå·¥ç¨‹æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å› æœå…³ç³»æ£€æµ‹ä¸æå–èƒ½åŠ›ã€‚æ­¤æ–¹æ³•åˆ©ç”¨æ£€ç´¢æœºåˆ¶åŠ¨æ€ç”Ÿæˆæç¤ºï¼Œä»è€Œå‡å°‘å¯¹é™æ€æç¤ºçš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ£€ç´¢æ¨¡å—ã€åŠ¨æ€æç¤ºç”Ÿæˆæ¨¡å—å’Œå› æœå…³ç³»æ£€æµ‹æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ£€ç´¢æ¨¡å—è·å–ç›¸å…³ä¿¡æ¯ï¼Œç„¶åç”ŸæˆåŠ¨æ€æç¤ºï¼Œæœ€ååˆ©ç”¨LLMè¿›è¡Œå› æœå…³ç³»çš„æ£€æµ‹ä¸æå–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†RAGåŠ¨æ€æç¤ºæ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†å› æœå…³ç³»æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿé™æ€æç¤ºæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒé¢†åŸŸçš„æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡ç‚¹è®¾ç½®äº†æ£€ç´¢æœºåˆ¶çš„å‚æ•°ï¼Œä»¥ç¡®ä¿æ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸ä»»åŠ¡ç›¸å…³ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æç¤ºç”Ÿæˆçš„ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„å“åº”èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥é€‚åº”å› æœå…³ç³»æŒ–æ˜çš„ç‰¹å®šéœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„RAGåŠ¨æ€æç¤ºæ–¹æ¡ˆåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºé™æ€æç¤ºæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°15%-30%ã€‚åœ¨äº”ç§å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå‡è¡¨ç°å‡ºæ›´é«˜çš„å› æœå…³ç³»æ£€æµ‹å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æå–ã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡å› æœå…³ç³»æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºå„ç±»æ•°æ®åˆ†æå’Œå†³ç­–æ”¯æŒæä¾›æ›´ä¸ºå¯é çš„åŸºç¡€ï¼Œæœªæ¥å¯èƒ½åœ¨åŒ»ç–—ã€é‡‘èç­‰å¤šä¸ªè¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Causality detection and mining are important tasks in information retrieval due to their enormous use in information extraction, and knowledge graph construction. To solve these tasks, in existing literature there exist several solutions -- both unsupervised and supervised. However, the unsupervised methods suffer from poor performance and they often require significant human intervention for causal rule selection, leading to poor generalization across different domains. On the other hand, supervised methods suffer from the lack of large training datasets. Recently, large language models (LLMs) with effective prompt engineering are found to be effective to overcome the issue of unavailability of large training dataset. Yet, in existing literature, there does not exist comprehensive works on causality detection and mining using LLM prompting. In this paper, we present several retrieval-augmented generation (RAG) based dynamic prompting schemes to enhance LLM performance in causality detection and extraction tasks. Extensive experiments over three datasets and five LLMs validate the superiority of our proposed RAG-based dynamic prompting over other static prompting schemes.

