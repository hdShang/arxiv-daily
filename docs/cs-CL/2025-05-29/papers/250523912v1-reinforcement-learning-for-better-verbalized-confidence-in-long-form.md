---
layout: default
title: Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation
---

# Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23912" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23912v1</a>
  <a href="https://arxiv.org/pdf/2505.23912.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23912v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23912v1', 'Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Collier, Andreas Vlachos

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoVeCä»¥è§£å†³é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„ä¿¡å¿ƒä¼°è®¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬ç”Ÿæˆ` `ä¿¡å¿ƒä¼°è®¡` `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å¹»è§‰æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­éš¾ä»¥æœ‰æ•ˆä¼°è®¡ä¿¡å¿ƒï¼Œå¯¼è‡´å¹»è§‰ç°è±¡é¢‘ç¹å‡ºç°ã€‚
2. æœ¬æ–‡æå‡ºLoVeCï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®æ—¶ä¼°è®¡é•¿æ–‡æœ¬ç”Ÿæˆçš„å£å¤´ä¿¡å¿ƒåˆ†æ•°ï¼Œæå‡ç”Ÿæˆå†…å®¹çš„å¯ä¿¡åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLè®­ç»ƒçš„æ¨¡å‹åœ¨ä¸‰ä¸ªé•¿æ–‡æœ¬é—®ç­”æ•°æ®é›†ä¸Šå®ç°äº†æ›´å¥½çš„æ ¡å‡†æ•ˆæœï¼Œä¸”æ³›åŒ–èƒ½åŠ›å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¹»è§‰ç°è±¡ä»ç„¶æ˜¯å¤§è¯­è¨€æ¨¡å‹åœ¨äº‹å®å†…å®¹ç”Ÿæˆä¸­å®‰å…¨å¯ä¿¡éƒ¨ç½²çš„ä¸»è¦æŒ‘æˆ˜ã€‚ä»¥å¾€çš„ç ”ç©¶æ¢è®¨äº†ä¿¡å¿ƒä¼°è®¡ä½œä¸ºå¹»è§‰æ£€æµ‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†é€šå¸¸ä¾èµ–äºåéªŒè‡ªä¸€è‡´æ€§æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•éœ€è¦è®¡ç®—ä¸Šæ˜‚è´µçš„é‡‡æ ·ã€‚å£å¤´ä¿¡å¿ƒæä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å±€é™äºçŸ­æ–‡æœ¬é—®ç­”ä»»åŠ¡ï¼Œéš¾ä»¥æ¨å¹¿åˆ°å¼€æ”¾å¼ç”Ÿæˆã€‚æœ¬æ–‡æå‡ºäº†LoVeCï¼ˆé•¿æ–‡æœ¬å£å¤´ä¿¡å¿ƒï¼‰ï¼Œä¸€ç§ç”¨äºé•¿æ–‡æœ¬ç”Ÿæˆçš„å³æ—¶å£å¤´ä¿¡å¿ƒä¼°è®¡æ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ä¸ºæ¯ä¸ªç”Ÿæˆçš„è¯­å¥é™„åŠ æ•°å€¼ä¿¡å¿ƒåˆ†æ•°ï¼Œä½œä¸ºç”Ÿæˆäº‹å®æ€§çš„ç›´æ¥å’Œå¯è§£é‡Šä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„RLè®­ç»ƒæ¨¡å‹åœ¨æ ¡å‡†æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„ä¿¡å¿ƒä¼°è®¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºè®¡ç®—æˆæœ¬é«˜çš„åéªŒè‡ªä¸€è‡´æ€§æ–¹æ³•ï¼Œéš¾ä»¥å®æ—¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„LoVeCæ–¹æ³•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ä¸ºæ¯ä¸ªç”Ÿæˆçš„è¯­å¥é™„åŠ æ•°å€¼ä¿¡å¿ƒåˆ†æ•°ï¼Œæä¾›ä¸€ç§ç›´æ¥ä¸”å¯è§£é‡Šçš„äº‹å®æ€§ä¿¡å·ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆå†…å®¹çš„å¯ä¿¡åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€æ¨¡å‹ç”Ÿæˆã€ä¿¡å¿ƒåˆ†æ•°ä¼°è®¡å’Œè¾“å‡ºå››ä¸ªä¸»è¦æ¨¡å—ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚DPOã€ORPOã€GRPOï¼‰ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶è¯„ä¼°ä¿¡å¿ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šLoVeCçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å£å¤´ä¿¡å¿ƒä¼°è®¡ä¸é•¿æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨å¼€æ”¾å¼ç”Ÿæˆä¸­çš„å±€é™æ€§ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¿¡å¿ƒè¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„å¤šç§ç­–ç•¥ï¼Œè®¾è®¡äº†é€‚åº”é•¿æ–‡æœ¬ç”Ÿæˆçš„æŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡å¼•å…¥å°‘é‡çš„è¾“å‡ºæ ‡è®°æ¥æé«˜æ•ˆç‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LoVeCæ–¹æ³•è®­ç»ƒçš„æ¨¡å‹åœ¨ä¸‰ä¸ªé•¿æ–‡æœ¬é—®ç­”æ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ ¡å‡†æ•ˆæœæå‡æ˜¾è‘—ï¼Œä¸”åœ¨ä¸åŒé¢†åŸŸä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªè¯¦è¿°ï¼Œå¾…è¿›ä¸€æ­¥éªŒè¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–°é—»ç”Ÿæˆã€å†…å®¹åˆ›ä½œå’Œæ•™è‚²è¾…å¯¼ç­‰ã€‚é€šè¿‡æé«˜é•¿æ–‡æœ¬ç”Ÿæˆçš„å¯ä¿¡åº¦ï¼ŒLoVeCèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å‡å°‘ä¿¡æ¯è¯¯å¯¼ï¼Œæå‡ç”¨æˆ·ä¿¡ä»»åº¦ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded.

