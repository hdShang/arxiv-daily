---
layout: default
title: "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time"
---

# Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23729" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23729v2</a>
  <a href="https://arxiv.org/pdf/2505.23729.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23729v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23729v2', 'Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohamad Chehade, Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Dinesh Manocha, Hao Zhu, Amrit Singh Bedi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-05-31)

**å¤‡æ³¨**: Accepted at ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSITAlignæ¡†æ¶ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹é½é—®é¢˜` `æ»¡æ„åŒ–ç­–ç•¥` `å¤šç›®æ ‡ä¼˜åŒ–` `æ¨ç†æ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•é€šå¸¸å¿½è§†äººç±»å†³ç­–çš„å¤æ‚æ€§ï¼Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºSITAlignæ¡†æ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–ä¸»è¦ç›®æ ‡å¹¶æ»¡è¶³æ¬¡è¦ç›®æ ‡çš„é˜ˆå€¼çº¦æŸï¼Œå®ç°æ»¡æ„åŒ–å¯¹é½ã€‚
3. åœ¨PKU-SafeRLHFæ•°æ®é›†ä¸Šï¼ŒSITAlignåœ¨æœ‰ç”¨æ€§å’Œæ— å®³æ€§ä¹‹é—´å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»çš„å¯¹é½é—®é¢˜å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å°†å…¶è§†ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œä½†å¿½è§†äº†äººç±»å†³ç­–çš„å®é™…è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäººç±»å†³ç­–éµå¾ªæ»¡æ„åŒ–ç­–ç•¥ï¼Œå³åœ¨ä¼˜åŒ–ä¸»è¦ç›®æ ‡çš„åŒæ—¶ç¡®ä¿å…¶ä»–ç›®æ ‡è¾¾åˆ°å¯æ¥å—çš„é˜ˆå€¼ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SITAlignæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æœ€å¤§åŒ–ä¸»è¦ç›®æ ‡å¹¶æ»¡è¶³æ¬¡è¦æ ‡å‡†çš„é˜ˆå€¼çº¦æŸæ¥å®ç°æ»¡æ„åŒ–å¯¹é½ã€‚æˆ‘ä»¬é€šè¿‡ç†è®ºæ¨å¯¼æä¾›äº†æ»¡æ„åŒ–æ¨ç†å¯¹é½æ–¹æ³•çš„æ¬¡ä¼˜æ€§ç•Œé™ï¼Œå¹¶é€šè¿‡å¤šé¡¹åŸºå‡†å®éªŒéªŒè¯äº†SITAlignçš„æœ‰æ•ˆæ€§ã€‚ä»¥PKU-SafeRLHFæ•°æ®é›†ä¸ºä¾‹ï¼ŒSITAlignåœ¨æœ€å¤§åŒ–æœ‰ç”¨æ€§å¹¶ç¡®ä¿æ— å®³æ€§é˜ˆå€¼çš„æƒ…å†µä¸‹ï¼Œè¾ƒç°æœ‰å¤šç›®æ ‡è§£ç ç­–ç•¥æå‡äº†22.3%çš„GPT-4èƒœå¹³ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»å¯¹é½çš„å¤æ‚æ€§ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å°†å…¶ç®€åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–ï¼Œæœªèƒ½æœ‰æ•ˆæ•æ‰äººç±»çš„æ»¡æ„åŒ–å†³ç­–è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSITAlignæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æœ€å¤§åŒ–ä¸€ä¸ªä¸»è¦ç›®æ ‡ï¼ˆå¦‚æœ‰ç”¨æ€§ï¼‰ï¼ŒåŒæ—¶ç¡®ä¿æ¬¡è¦ç›®æ ‡ï¼ˆå¦‚æ— å®³æ€§ï¼‰æ»¡è¶³ç‰¹å®šçš„é˜ˆå€¼ï¼Œä»è€Œå®ç°æ›´ç¬¦åˆäººç±»å†³ç­–çš„å¯¹é½æ–¹å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSITAlignçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç›®æ ‡å‡½æ•°çš„è®¾è®¡ã€çº¦æŸæ¡ä»¶çš„è®¾ç½®ä»¥åŠæ¨ç†è¿‡ç¨‹çš„ä¼˜åŒ–ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç›®æ ‡ä¼˜åŒ–æ¨¡å—å’Œçº¦æŸéªŒè¯æ¨¡å—ï¼Œç¡®ä¿åœ¨æ¨ç†æ—¶åŒæ—¶è€ƒè™‘ä¸»è¦å’Œæ¬¡è¦ç›®æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥æ»¡æ„åŒ–å¯¹é½çš„æ¦‚å¿µï¼Œé€šè¿‡ç†è®ºæ¨å¯¼æä¾›äº†æ¬¡ä¼˜æ€§ç•Œé™ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿçš„å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSITAlignè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸»è¦å’Œæ¬¡è¦ç›®æ ‡ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§é˜ˆå€¼ç­–ç•¥æ¥åŠ¨æ€è°ƒæ•´æ¬¡è¦ç›®æ ‡çš„çº¦æŸæ¡ä»¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨PKU-SafeRLHFæ•°æ®é›†ä¸Šï¼ŒSITAlignåœ¨æœ€å¤§åŒ–æœ‰ç”¨æ€§å¹¶ç¡®ä¿æ— å®³æ€§é˜ˆå€¼çš„æƒ…å†µä¸‹ï¼Œè¾ƒç°æœ‰çš„å¤šç›®æ ‡è§£ç ç­–ç•¥æå‡äº†22.3%çš„GPT-4èƒœå¹³ç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SITAlignæ¡†æ¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦äººæœºåä½œçš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹ç”Ÿæˆå’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æ›´å¥½åœ°å¯¹é½äººç±»åå¥½ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„å®ç”¨æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„äº¤äº’æ–¹å¼å’Œåº”ç”¨åœºæ™¯çš„æ‹“å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. While existing approaches typically frame this as a multi-objective optimization problem, they often overlook how humans actually make decisions. Research on bounded rationality suggests that human decision making follows satisficing strategies-optimizing primary objectives while ensuring others meet acceptable thresholds. To bridge this gap and operationalize the notion of satisficing alignment, we propose SITAlign: an inference time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. We provide theoretical insights by deriving sub-optimality bounds of our satisficing based inference alignment approach. We empirically validate SITAlign's performance through extensive experimentation on multiple benchmarks. For instance, on the PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art multi objective decoding strategy by a margin of 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness.

