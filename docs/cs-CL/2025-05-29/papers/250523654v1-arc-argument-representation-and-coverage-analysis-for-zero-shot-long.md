---
layout: default
title: "ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs"
---

# ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23654" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23654v1</a>
  <a href="https://arxiv.org/pdf/2505.23654.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23654v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23654v1', 'ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohamed Elaraby, Diane Litman

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºARCæ¡†æ¶ä»¥æå‡é›¶æ ·æœ¬é•¿æ–‡æ¡£æ‘˜è¦çš„è®ºç‚¹è¦†ç›–åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æ¡£æ‘˜è¦` `è®ºç‚¹è¡¨ç¤º` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–ä¿¡æ¯` `æ³•å¾‹æ–‡ä¹¦` `ç§‘å­¦æ–‡ç« ` `ä¿¡æ¯è¦†ç›–` `æ‘˜è¦ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŠ½è±¡æ‘˜è¦æ–¹æ³•åœ¨ä¿ç•™é‡è¦è®ºç‚¹ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ³•å¾‹å’Œç§‘å­¦é¢†åŸŸçš„é•¿æ–‡æ¡£ä¸­ã€‚
2. è®ºæ–‡æå‡ºäº†è®ºç‚¹è¡¨ç¤ºè¦†ç›–ï¼ˆARCï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°LLMç”Ÿæˆæ‘˜è¦å¯¹é‡è¦è®ºç‚¹çš„æ•æ‰èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡LLMèƒ½è¦†ç›–éƒ¨åˆ†è®ºç‚¹è§’è‰²ï¼Œä½†åœ¨ä¿¡æ¯ç¨€ç–çš„æƒ…å†µä¸‹ï¼Œå…³é”®ä¿¡æ¯å¸¸è¢«é—æ¼ï¼Œå¼ºè°ƒäº†æ”¹è¿›çš„å¿…è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•´åˆç»“æ„åŒ–ä¿¡æ¯é•¿æœŸä»¥æ¥æé«˜äº†æŠ½è±¡æ‘˜è¦çš„è´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨ä¿ç•™é‡è¦å†…å®¹æ–¹é¢ã€‚æœ¬ç ”ç©¶èšç„¦äºè®ºç‚¹è§’è‰²ï¼Œè¿™åœ¨æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸçš„æ–‡æ¡£æ‘˜è¦ä¸­è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æ¢è®¨äº†æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦èƒ½å¤Ÿå……åˆ†ä¿ç•™è¿™äº›ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è®ºç‚¹è¡¨ç¤ºè¦†ç›–ï¼ˆARCï¼‰æ¡†æ¶ï¼Œç”¨äºè¡¡é‡LLMç”Ÿæˆçš„æ‘˜è¦åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ•æ‰åˆ°é‡è¦è®ºç‚¹ã€‚é€šè¿‡ARCï¼Œæˆ‘ä»¬åˆ†æäº†ä¸‰ç§å¼€æ”¾æƒé‡LLMåœ¨æ³•å¾‹æ„è§ä¹¦å’Œç§‘å­¦æ–‡ç« è¿™ä¸¤ä¸ªé¢†åŸŸç”Ÿæˆçš„æ‘˜è¦ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMåœ¨ä¸€å®šç¨‹åº¦ä¸Šè¦†ç›–äº†é‡è¦è®ºç‚¹è§’è‰²ï¼Œä½†ç”Ÿæˆçš„æ‘˜è¦å¸¸å¸¸é—æ¼å…³é”®ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åœ¨è¾“å…¥ä¸­è®ºç‚¹åˆ†å¸ƒç¨€ç–æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ARCæ­ç¤ºäº†è¡Œä¸ºæ¨¡å¼ï¼Œå¼ºè°ƒäº†LLMä¸Šä¸‹æ–‡çª—å£çš„ä½ç½®ä¿¡æ¯åå·®å’Œè§’è‰²ç‰¹å®šåå¥½å¯¹ç”Ÿæˆæ‘˜è¦ä¸­å…³é”®è®ºç‚¹è¦†ç›–çš„å½±å“ï¼Œçªæ˜¾äº†éœ€è¦æ›´å…·è®ºç‚¹æ„è¯†çš„æ‘˜è¦ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰LLMç”Ÿæˆçš„æ‘˜è¦åœ¨ä¿ç•™é•¿æ–‡æ¡£ä¸­é‡è¦è®ºç‚¹ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ³•å¾‹å’Œç§‘å­¦é¢†åŸŸã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¨€ç–åˆ†å¸ƒçš„è®ºç‚¹æ—¶ï¼Œå¸¸å¸¸é—æ¼å…³é”®ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥è®ºç‚¹è¡¨ç¤ºè¦†ç›–ï¼ˆARCï¼‰æ¡†æ¶ï¼Œé€šè¿‡é‡åŒ–LLMç”Ÿæˆæ‘˜è¦ä¸­å¯¹é‡è¦è®ºç‚¹çš„è¦†ç›–ç¨‹åº¦ï¼Œè¯„ä¼°å…¶æ€§èƒ½å¹¶æ­ç¤ºæ½œåœ¨çš„è¡Œä¸ºæ¨¡å¼ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æå‡æ‘˜è¦è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©é¢†åŸŸã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šARCæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼š1) è®ºç‚¹è§’è‰²è¯†åˆ«ï¼Œ2) æ‘˜è¦ç”Ÿæˆï¼Œ3) è¦†ç›–åº¦è¯„ä¼°ã€‚é¦–å…ˆè¯†åˆ«è¾“å…¥æ–‡æ¡£ä¸­çš„è®ºç‚¹è§’è‰²ï¼Œç„¶ååˆ©ç”¨LLMç”Ÿæˆæ‘˜è¦ï¼Œæœ€åé€šè¿‡ARCè¯„ä¼°ç”Ÿæˆæ‘˜è¦å¯¹è®ºç‚¹çš„è¦†ç›–æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ARCæ¡†æ¶ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMç”Ÿæˆæ‘˜è¦çš„è®ºç‚¹è¦†ç›–èƒ½åŠ›ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒARCä¸ä»…å…³æ³¨æ‘˜è¦çš„æµç•…æ€§å’Œè¿è´¯æ€§ï¼Œè¿˜å¼ºè°ƒäº†è®ºç‚¹ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒARCæ¡†æ¶é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥é‡åŒ–è®ºç‚¹è¦†ç›–åº¦ï¼Œå¹¶ç»“åˆäº†ä¸Šä¸‹æ–‡çª—å£çš„ä½ç½®ä¿¡æ¯åå·®å’Œè§’è‰²ç‰¹å®šåå¥½ï¼Œä»¥ä¼˜åŒ–æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMåœ¨ä¸€å®šç¨‹åº¦ä¸Šè¦†ç›–äº†é‡è¦è®ºç‚¹è§’è‰²ï¼Œä½†åœ¨ä¿¡æ¯ç¨€ç–çš„æƒ…å†µä¸‹ï¼Œå…³é”®ä¿¡æ¯çš„é—æ¼ç‡é«˜è¾¾XX%ã€‚ARCæ¡†æ¶çš„å¼•å…¥ä½¿å¾—å¯¹ç”Ÿæˆæ‘˜è¦çš„è¯„ä¼°æ›´åŠ ç³»ç»ŸåŒ–ï¼Œæ­ç¤ºäº†LLMåœ¨ä¸åŒé¢†åŸŸçš„è¡¨ç°å·®å¼‚ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ³•å¾‹æ–‡ä¹¦çš„è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆã€ç§‘å­¦ç ”ç©¶æ–‡ç« çš„å¿«é€Ÿé˜…è¯»å’Œä¿¡æ¯æå–ç­‰ã€‚é€šè¿‡æé«˜æ‘˜è¦çš„è®ºç‚¹è¦†ç›–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨é«˜é£é™©é¢†åŸŸä¸­æä¾›æ›´å¯é çš„ä¿¡æ¯æ”¯æŒï¼Œå¸®åŠ©ä¸“ä¸šäººå£«å¿«é€Ÿè·å–å…³é”®ä¿¡æ¯ï¼Œæå‡å†³ç­–æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Integrating structured information has long improved the quality of abstractive summarization, particularly in retaining salient content. In this work, we focus on a specific form of structure: argument roles, which are crucial for summarizing documents in high-stakes domains such as law. We investigate whether instruction-tuned large language models (LLMs) adequately preserve this information. To this end, we introduce Argument Representation Coverage (ARC), a framework for measuring how well LLM-generated summaries capture salient arguments. Using ARC, we analyze summaries produced by three open-weight LLMs in two domains where argument roles are central: long legal opinions and scientific articles. Our results show that while LLMs cover salient argument roles to some extent, critical information is often omitted in generated summaries, particularly when arguments are sparsely distributed throughout the input. Further, we use ARC to uncover behavioral patterns -- specifically, how the positional bias of LLM context windows and role-specific preferences impact the coverage of key arguments in generated summaries, emphasizing the need for more argument-aware summarization strategies.

