---
layout: default
title: LLMs are Better Than You Think: Label-Guided In-Context Learning for Named Entity Recognition
---

# LLMs are Better Than You Think: Label-Guided In-Context Learning for Named Entity Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23722" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23722v2</a>
  <a href="https://arxiv.org/pdf/2505.23722.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23722v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23722v2', 'LLMs are Better Than You Think: Label-Guided In-Context Learning for Named Entity Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-10-29)

**å¤‡æ³¨**: Accepted to EMNLP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDEERæ–¹æ³•ä»¥æå‡å‘½åå®ä½“è¯†åˆ«çš„æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‘½åå®ä½“è¯†åˆ«` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ ‡ç­¾å¼•å¯¼` `ç»Ÿè®¡ä¿¡æ¯` `æ— è®­ç»ƒæ–¹æ³•` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ–¹æ³•åœ¨ç¤ºä¾‹æ£€ç´¢ä¸­ä¾èµ–äºè¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¯¼è‡´ç›¸å…³ç¤ºä¾‹ä¸è¶³ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚
2. DEERæ–¹æ³•é€šè¿‡æ ‡ç­¾å¼•å¯¼çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯†åˆ«å¯¹å®ä½“è¯†åˆ«æœ€æœ‰ç”¨çš„ä»¤ç‰Œï¼Œä»è€Œæä¾›æ›´å…·é’ˆå¯¹æ€§çš„ç¤ºä¾‹ã€‚
3. åœ¨äº”ä¸ªNERæ•°æ®é›†çš„å®éªŒä¸­ï¼ŒDEERæ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰ICLæ–¹æ³•ï¼Œä¸”åœ¨ä½èµ„æºç¯å¢ƒä¸‹ä¾ç„¶ä¿æŒå¼ºå¤§çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡å°‘é‡ç¤ºä¾‹æ‰§è¡Œæ–°ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ICLæ–¹æ³•é€šå¸¸ä¾èµ–äºä¸ä»»åŠ¡æ— å…³çš„è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œç¤ºä¾‹æ£€ç´¢ï¼Œè¿™å¯¼è‡´ç›¸å…³æ€§è¾ƒä½çš„ç¤ºä¾‹ï¼Œä»è€Œå½±å“ç»“æœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„ICLæ–¹æ³•DEERï¼Œé€šè¿‡ä½¿ç”¨æ ‡ç­¾å¼•å¯¼çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿LLMsèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¿›è¡Œå®ä½“é¢„æµ‹ã€‚DEERåˆ©ç”¨è®­ç»ƒæ ‡ç­¾çš„ä»¤ç‰Œçº§ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯†åˆ«å¯¹å®ä½“è¯†åˆ«æœ€æœ‰å¸®åŠ©çš„ä»¤ç‰Œï¼Œå¹¶é€šè¿‡é’ˆå¯¹æ€§çš„åæ€æ­¥éª¤æ£€æµ‹å’Œä¿®æ­£æ˜“é”™ä»¤ç‰Œã€‚åœ¨äº”ä¸ªNERæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒDEERåœ¨å››ä¸ªLLMsä¸Šå‡ä¼˜äºç°æœ‰ICLæ–¹æ³•ï¼Œä¸”å…¶æ€§èƒ½å¯ä¸ç›‘ç£å¾®è°ƒç›¸åª²ç¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ–¹æ³•åœ¨ç¤ºä¾‹æ£€ç´¢ä¸­ä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§æ‰€å¸¦æ¥çš„ç›¸å…³æ€§ä¸è¶³é—®é¢˜ã€‚è¿™ç§æ–¹æ³•å¸¸å¸¸å¯¼è‡´æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDEERæ–¹æ³•çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ ‡ç­¾å¼•å¯¼çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯†åˆ«å‡ºå¯¹å®ä½“è¯†åˆ«æœ€æœ‰å¸®åŠ©çš„ä»¤ç‰Œï¼Œä»è€Œæä¾›æ›´å…·é’ˆå¯¹æ€§çš„ç¤ºä¾‹ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜ç¤ºä¾‹çš„ç›¸å…³æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDEERçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯åŸºäºè®­ç»ƒæ ‡ç­¾çš„ä»¤ç‰Œçº§ç»Ÿè®¡ä¿¡æ¯çš„æå–ï¼Œå…¶æ¬¡æ˜¯é€šè¿‡åæ€æ­¥éª¤æ£€æµ‹å’Œä¿®æ­£æ˜“é”™ä»¤ç‰Œã€‚æ•´ä¸ªæµç¨‹æ— é¡»é¢å¤–è®­ç»ƒï¼Œç›´æ¥åˆ©ç”¨å·²æœ‰çš„æ ‡ç­¾ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šDEERçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ— è®­ç»ƒçš„ICLæ–¹æ³•ï¼Œé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„ç»Ÿè®¡ä¿¡æ¯æ¥ä¼˜åŒ–ç¤ºä¾‹æ£€ç´¢ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ£€ç´¢å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šDEERåœ¨å‚æ•°è®¾ç½®ä¸Šå¼ºè°ƒä»¤ç‰Œçº§ç»Ÿè®¡ä¿¡æ¯çš„æå–ï¼Œç¡®ä¿æ‰€é€‰ä»¤ç‰Œå¯¹å®ä½“è¯†åˆ«çš„è´¡çŒ®æœ€å¤§åŒ–ã€‚æ­¤å¤–ï¼Œåæ€æ­¥éª¤çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æ€§åœ°ä¿®æ­£æ½œåœ¨é”™è¯¯ï¼Œæå‡æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DEERæ–¹æ³•åœ¨äº”ä¸ªNERæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„ICLæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ä¸ç›‘ç£å¾®è°ƒçš„æ•ˆæœç›¸å½“ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šï¼ŒDEERçš„F1åˆ†æ•°æé«˜äº†çº¦5-10ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç¤ºä¾‹æ£€ç´¢å’Œä½èµ„æºç¯å¢ƒä¸‹çš„å¼ºå¤§é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æå–ã€æ–‡æœ¬åˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚é€šè¿‡æå‡å‘½åå®ä½“è¯†åˆ«çš„å‡†ç¡®æ€§ï¼ŒDEERæ–¹æ³•èƒ½å¤Ÿä¸ºå„ç§å®é™…åº”ç”¨æä¾›æ›´å¯é çš„æ”¯æŒï¼Œå¦‚æ™ºèƒ½å®¢æœã€èˆ†æƒ…ç›‘æµ‹å’ŒçŸ¥è¯†å›¾è°±æ„å»ºç­‰ã€‚æœªæ¥ï¼ŒDEERçš„æ€è·¯è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–NLPä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-context learning (ICL) enables large language models (LLMs) to perform new tasks using only a few demonstrations. However, in Named Entity Recognition (NER), existing ICL methods typically rely on task-agnostic semantic similarity for demonstration retrieval, which often yields less relevant examples and leads to inferior results. We introduce DEER, a training-free ICL approach that enables LLMs to make more informed entity predictions through the use of label-grounded statistics. DEER leverages token-level statistics from training labels to identify tokens most informative for entity recognition, enabling entity-focused demonstrations. It further uses these statistics to detect and refine error-prone tokens through a targeted reflection step. Evaluated on five NER datasets across four LLMs, DEER consistently outperforms existing ICL methods and achieves performance comparable to supervised fine-tuning. Further analyses demonstrate that DEER improves example retrieval, remains effective on both seen and unseen entities, and exhibits strong robustness in low-resource settings.

