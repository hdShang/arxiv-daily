---
layout: default
title: Probing Association Biases in LLM Moderation Over-Sensitivity
---

# Probing Association Biases in LLM Moderation Over-Sensitivity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23914" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23914v1</a>
  <a href="https://arxiv.org/pdf/2505.23914.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23914v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23914v1', 'Probing Association Biases in LLM Moderation Over-Sensitivity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxin Wang, Botao Yu, Ivory Yang, Saeed Hassanpour, Soroush Vosoughi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸»é¢˜å…³è”åˆ†æä»¥è§£å†³LLMå†…å®¹å®¡æ ¸è¿‡åº¦æ•æ„Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å†…å®¹å®¡æ ¸` `ä¸»é¢˜å…³è”åˆ†æ` `è¿‡åº¦æ•æ„Ÿ` `éšæ€§åè§` `è¯­ä¹‰åˆ†æ` `è‡ªåŠ¨åŒ–è¿‡æ»¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å†…å®¹å®¡æ ¸æ–¹æ³•å¾€å¾€å°†è‰¯æ€§è¯„è®ºè¯¯åˆ¤ä¸ºæœ‰æ¯’è¯„è®ºï¼Œå¯¼è‡´è¿‡åº¦æ•æ„Ÿçš„é—®é¢˜ï¼Œä¸»è¦å½’å› äºæ”»å‡»æ€§è¯æ±‡çš„å­˜åœ¨ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ææ–¹æ³•â€”â€”ä¸»é¢˜å…³è”åˆ†æï¼Œé€šè¿‡é‡åŒ–LLMså¦‚ä½•å°†ç‰¹å®šä¸»é¢˜ä¸æœ‰æ¯’æ€§å…³è”ï¼Œæ­ç¤ºå…¶æ½œåœ¨çš„ä¸»é¢˜åè§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ›´å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4 Turboçš„å‡é˜³æ€§ç‡è¾ƒä½ï¼Œä½†å…¶ä¸»é¢˜åˆ»æ¿å°è±¡æ›´å¼ºï¼Œæ˜¾ç¤ºå‡ºLLMsåœ¨å®¡æ ¸å†³ç­–ä¸­ä¾èµ–äºå­¦ä¹ åˆ°çš„ä¸»é¢˜å…³è”ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¿æ³›åº”ç”¨äºå†…å®¹å®¡æ ¸ï¼Œä½†å¸¸å¸¸å°†è‰¯æ€§è¯„è®ºè¯¯åˆ¤ä¸ºæœ‰æ¯’è¯„è®ºï¼Œå¯¼è‡´è¿‡åº¦æ•æ„Ÿã€‚ä»¥å¾€ç ”ç©¶ä¸»è¦å°†æ­¤é—®é¢˜å½’å› äºæ”»å‡»æ€§è¯æ±‡çš„å­˜åœ¨ï¼Œè€Œæˆ‘ä»¬æ­ç¤ºäº†ä¸€ä¸ªè¶…è¶Šè¯æ±‡å±‚é¢çš„æ½œåœ¨åŸå› ï¼šLLMsåœ¨éšæ€§å…³è”ä¸­è¡¨ç°å‡ºç³»ç»Ÿæ€§çš„ä¸»é¢˜åè§ã€‚å—è®¤çŸ¥å¿ƒç†å­¦éšæ€§å…³è”æµ‹è¯•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸»é¢˜å…³è”åˆ†æï¼Œè¿™æ˜¯ä¸€ç§è¯­ä¹‰å±‚é¢çš„åˆ†ææ–¹æ³•ï¼Œç”¨äºé‡åŒ–LLMså¦‚ä½•å°†ç‰¹å®šä¸»é¢˜ä¸æœ‰æ¯’æ€§å…³è”ã€‚é€šè¿‡æç¤ºLLMsç”Ÿæˆå¯¹è¯¯åˆ†ç±»è‰¯æ€§è¯„è®ºçš„è‡ªç”±åœºæ™¯æƒ³è±¡å¹¶åˆ†æå…¶ä¸»é¢˜æ”¾å¤§æ°´å¹³ï¼Œæˆ‘ä»¬å‘ç°æ›´å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚GPT-4 Turboï¼‰å°½ç®¡æ•´ä½“å‡é˜³æ€§ç‡è¾ƒä½ï¼Œä½†è¡¨ç°å‡ºæ›´å¼ºçš„ä¸»é¢˜åˆ»æ¿å°è±¡ã€‚è¿™äº›åè§è¡¨æ˜ï¼ŒLLMsä¸ä»…ä»…å¯¹æ˜¾æ€§æ”»å‡»æ€§è¯­è¨€åšå‡ºååº”ï¼Œè€Œæ˜¯ä¾èµ–äºå­¦ä¹ åˆ°çš„ä¸»é¢˜å…³è”ï¼Œä»è€Œå½±å“å…¶å®¡æ ¸å†³ç­–ã€‚æˆ‘ä»¬çš„å‘ç°å¼ºè°ƒäº†è¶…è¶ŠåŸºäºå…³é”®è¯è¿‡æ»¤çš„å¿…è¦æ€§ï¼Œä¸ºç†è§£LLMè¿‡åº¦æ•æ„Ÿçš„æ½œåœ¨æœºåˆ¶æä¾›äº†æ–°è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†…å®¹å®¡æ ¸ä¸­å‡ºç°çš„è¿‡åº¦æ•æ„Ÿé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºæ”»å‡»æ€§è¯æ±‡ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘ä¸»é¢˜åè§çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸»é¢˜å…³è”åˆ†æï¼Œè®ºæ–‡æ¢è®¨äº†LLMså¦‚ä½•åœ¨è¯­ä¹‰å±‚é¢ä¸Šå°†ç‰¹å®šä¸»é¢˜ä¸æœ‰æ¯’æ€§å…³è”ï¼Œä»è€Œå½±å“å…¶å®¡æ ¸å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œæç¤ºLLMsç”Ÿæˆå¯¹è¯¯åˆ†ç±»è‰¯æ€§è¯„è®ºçš„è‡ªç”±åœºæ™¯æƒ³è±¡ï¼›å…¶æ¬¡ï¼Œåˆ†æç”Ÿæˆå†…å®¹ä¸­çš„ä¸»é¢˜æ”¾å¤§æ°´å¹³ï¼Œä»¥é‡åŒ–å…¶ä¸»é¢˜åè§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸»é¢˜å…³è”åˆ†æè¿™ä¸€æ–°æ–¹æ³•ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºå…³é”®è¯çš„è¿‡æ»¤ï¼Œæ­ç¤ºäº†LLMsåœ¨å†…å®¹å®¡æ ¸ä¸­æ½œåœ¨çš„ä¸»é¢˜åè§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æç¤ºç­–ç•¥ä»¥å¼•å¯¼LLMsç”Ÿæˆç›¸å…³åœºæ™¯ï¼Œå¹¶é‡‡ç”¨äº†ä¸»é¢˜æ”¾å¤§æ°´å¹³ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥é‡åŒ–æ¨¡å‹çš„ä¸»é¢˜åè§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡GPT-4 Turboçš„æ•´ä½“å‡é˜³æ€§ç‡è¾ƒä½ï¼Œä½†å…¶åœ¨ä¸»é¢˜åˆ»æ¿å°è±¡æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„åè§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨å†…å®¹å®¡æ ¸ä¸­ï¼Œæ¨¡å‹ä¸ä»…ä¾èµ–äºæ˜¾æ€§è¯­è¨€ç‰¹å¾ï¼Œè¿˜å—åˆ°éšæ€§ä¸»é¢˜å…³è”çš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€åœ¨çº¿è¯„è®ºç›‘æ§å’Œè‡ªåŠ¨åŒ–å†…å®¹è¿‡æ»¤ç³»ç»Ÿã€‚é€šè¿‡ç†è§£LLMsçš„ä¸»é¢˜åè§ï¼Œå¯ä»¥ä¼˜åŒ–å†…å®¹å®¡æ ¸ç®—æ³•ï¼Œå‡å°‘è¯¯åˆ¤ï¼Œæé«˜ç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å†…å®¹ç®¡ç†å·¥å…·çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models are widely used for content moderation but often misclassify benign comments as toxic, leading to over-sensitivity. While previous research attributes this issue primarily to the presence of offensive terms, we reveal a potential cause beyond token level: LLMs exhibit systematic topic biases in their implicit associations. Inspired by cognitive psychology's implicit association tests, we introduce Topic Association Analysis, a semantic-level approach to quantify how LLMs associate certain topics with toxicity. By prompting LLMs to generate free-form scenario imagination for misclassified benign comments and analyzing their topic amplification levels, we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger topic stereotype despite lower overall false positive rates. These biases suggest that LLMs do not merely react to explicit, offensive language but rely on learned topic associations, shaping their moderation decisions. Our findings highlight the need for refinement beyond keyword-based filtering, providing insights into the underlying mechanisms driving LLM over-sensitivity.

