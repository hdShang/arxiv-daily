---
layout: default
title: "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering"
---

# ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23723" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23723v1</a>
  <a href="https://arxiv.org/pdf/2505.23723.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23723v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23723v1', 'ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, Siheng Chen

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºML-Agentä»¥è§£å†³è‡ªä¸»æœºå™¨å­¦ä¹ å·¥ç¨‹ä¸­çš„æ‰‹åŠ¨æç¤ºå·¥ç¨‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªä¸»æœºå™¨å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç†å­¦ä¹ ` `å®éªŒä¼˜åŒ–` `è·¨ä»»åŠ¡æ³›åŒ–` `åœ¨çº¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–æ‰‹åŠ¨æç¤ºå·¥ç¨‹ï¼Œéš¾ä»¥é€‚åº”å¤šæ ·çš„å®éªŒéœ€æ±‚ï¼Œé™åˆ¶äº†è‡ªä¸»æœºå™¨å­¦ä¹ çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„ä»£ç†MLæ¡†æ¶ï¼Œç»“åˆæ¢ç´¢ä¸°å¯Œçš„å¾®è°ƒã€é€æ­¥RLå’Œç‰¹å®šå¥–åŠ±æ¨¡å—ï¼Œæå‡äº†ä»£ç†çš„å­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œ7Bå‚æ•°çš„ML-Agentåœ¨9ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒåï¼Œæ€§èƒ½è¶…è¶Š671Bå‚æ•°çš„DeepSeek-R1ï¼Œå±•ç°å‡ºæŒç»­çš„æ€§èƒ½æå‡å’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ä»£ç†çš„å‡ºç°æ˜¾è‘—æ¨åŠ¨äº†è‡ªä¸»æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·¥ç¨‹çš„å‘å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–æ‰‹åŠ¨æç¤ºå·¥ç¨‹ï¼Œæœªèƒ½æ ¹æ®å¤šæ ·çš„å®éªŒç»éªŒè¿›è¡Œé€‚åº”å’Œä¼˜åŒ–ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†åŸºäºå­¦ä¹ çš„ä»£ç†MLèŒƒå¼ï¼ŒLLMä»£ç†é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨MLä»»åŠ¡ä¸Šè¿›è¡Œäº’åŠ¨å®éªŒå­¦ä¹ ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»£ç†MLè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ¢ç´¢ä¸°å¯Œçš„å¾®è°ƒã€é€æ­¥RLå’Œç‰¹å®šäºä»£ç†MLçš„å¥–åŠ±æ¨¡å—ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäº7Bå‚æ•°çš„Qwen-2.5 LLMçš„ML-Agentï¼Œå°½ç®¡ä»…åœ¨9ä¸ªMLä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†å…¶è¡¨ç°è¶…è¶Šäº†671Bå‚æ•°çš„DeepSeek-R1ä»£ç†ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªä¸»æœºå™¨å­¦ä¹ æ–¹æ³•ä¸­å¯¹æ‰‹åŠ¨æç¤ºå·¥ç¨‹çš„è¿‡åº¦ä¾èµ–é—®é¢˜ï¼Œå¯¼è‡´é€‚åº”æ€§å’Œä¼˜åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŸºäºå­¦ä¹ çš„ä»£ç†MLèŒƒå¼ï¼Œé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä½¿LLMä»£ç†åœ¨MLä»»åŠ¡ä¸­è¿›è¡Œäº’åŠ¨å®éªŒå­¦ä¹ ï¼Œä»è€Œæé«˜å…¶è‡ªä¸»å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ¢ç´¢ä¸°å¯Œçš„å¾®è°ƒæ¨¡å—ç”¨äºç”Ÿæˆå¤šæ ·åŒ–åŠ¨ä½œï¼Œé€æ­¥RLæ¨¡å—ç”¨äºåŠ é€Ÿç»éªŒæ”¶é›†ï¼Œä»¥åŠç‰¹å®šäºä»£ç†MLçš„å¥–åŠ±æ¨¡å—ç”¨äºç»Ÿä¸€åé¦ˆä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ¢ç´¢ä¸°å¯Œçš„å¾®è°ƒå’Œé€æ­¥RLçš„ç»“åˆï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨å•æ­¥åŠ¨ä½œä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¥–åŠ±æ¨¡å—ä¸­ï¼Œè®¾è®¡äº†ç»Ÿä¸€çš„å¥–åŠ±ä¿¡å·ä»¥é€‚åº”ä¸åŒçš„MLåé¦ˆï¼Œä¼˜åŒ–äº†RLçš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†å¤šæ ·åŒ–åŠ¨ä½œç”Ÿæˆç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ML-Agentä»…åœ¨9ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶æ€§èƒ½å´è¶…è¶Šäº†671Bå‚æ•°çš„DeepSeek-R1ä»£ç†ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡å’Œè·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ å¹³å°ã€æ™ºèƒ½æ•°æ®åˆ†æå·¥å…·å’Œè‡ªä¸»å†³ç­–ç³»ç»Ÿã€‚é€šè¿‡æå‡LLMä»£ç†çš„å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šç§æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­å®ç°æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–å¤„ç†ï¼Œé™ä½äººå·¥å¹²é¢„çš„éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.

