---
layout: default
title: A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models
---

# A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23945" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23945v2</a>
  <a href="https://arxiv.org/pdf/2505.23945.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23945v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23945v2', 'A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sriram Balasubramanian, Samyadeep Basu, Soheil Feizi

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-11-01)

**å¤‡æ³¨**: Accepted in EMNLP 2025, 34 pages, 25 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–°è¯„ä¼°ç®¡é“ä»¥è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„åè§ä¸æ¨ç†å¿ å®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `é“¾å¼æ¨ç†` `åè§åˆ†æ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¨ç†å¿ å®æ€§` `éä¸€è‡´æ€§æ¨ç†` `ç»†ç²’åº¦è¯„ä¼°` `äººå·¥æ™ºèƒ½å…¬å¹³æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†å¿ å®æ€§æ—¶ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘å›¾åƒåè§çš„å½±å“ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»†ç²’åº¦è¯„ä¼°ç®¡é“ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°åˆ†æå’Œåˆ†ç±»åè§è¡¨è¾¾æ¨¡å¼ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå›¾åƒåè§çš„è¡¨è¾¾è¾ƒå°‘ï¼Œè€Œè®¸å¤šæ¨¡å‹å­˜åœ¨éä¸€è‡´æ€§æ¨ç†ç°è±¡ï¼Œå½±å“æ¨ç†çš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ¨ç†ï¼ˆCoTï¼‰å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹æ˜¯å¦å¿ å®äºæ¨¡å‹å†…éƒ¨æœºåˆ¶ä»å­˜ç–‘ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢ç ”ç©¶äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„CoTå¿ å®æ€§ï¼Œæ¢è®¨äº†æ–‡æœ¬å’Œå›¾åƒåè§å¦‚ä½•å½±å“æ¨ç†åŠåè§è¡¨è¾¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ç»†ç²’åº¦è¯„ä¼°ç®¡é“ï¼Œç”¨äºåˆ†ç±»åè§è¡¨è¾¾æ¨¡å¼ï¼Œä»è€Œå®ç°æ¯”ä»¥å¾€æ–¹æ³•æ›´ç²¾ç¡®çš„CoTæ¨ç†åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œç»†å¾®çš„å›¾åƒåè§è¾ƒå°‘è¢«è¡¨è¾¾ï¼Œè€Œè®¸å¤šæ¨¡å‹è¡¨ç°å‡ºä¸€ç§æ–°ç°è±¡ï¼Œå³â€œéä¸€è‡´æ€§â€æ¨ç†ï¼Œå¯èƒ½æˆä¸ºæ£€æµ‹ä¸å¿ å®CoTçš„è­¦ç¤ºä¿¡å·ã€‚æˆ‘ä»¬è¿˜å°†è¯¥è¯„ä¼°ç®¡é“åº”ç”¨äºè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå‘ç°å½“å‰è¯­è¨€æ¨ç†æ¨¡å‹åœ¨è¡¨è¾¾éšå«çº¿ç´¢æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é“¾å¼æ¨ç†ä¸­åè§è¡¨è¾¾å’Œå¿ å®æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•æ‰å›¾åƒåè§å¯¹æ¨ç†çš„å½±å“ï¼Œå¯¼è‡´åˆ†æä¸å¤Ÿå…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸€ç§æ–°é¢–çš„ç»†ç²’åº¦è¯„ä¼°ç®¡é“ï¼Œæœ¬æ–‡èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åˆ†ç±»å’Œåˆ†æåè§è¡¨è¾¾æ¨¡å¼ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹åœ¨å¤„ç†ä¸åŒç±»å‹åè§æ—¶çš„å·®å¼‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥è¯„ä¼°ç®¡é“åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆå¯¹æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹è¿›è¡Œè®°å½•ï¼Œç„¶ååˆ†ææ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„åè§å½±å“ï¼Œæœ€åé€šè¿‡ç»†ç²’åº¦åˆ†ç±»è¯„ä¼°æ¨ç†çš„å¿ å®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é’ˆå¯¹å›¾åƒåè§çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å‘ç°äº†â€œéä¸€è‡´æ€§â€æ¨ç†ç°è±¡ï¼Œè¿™åœ¨ä»¥å¾€çš„ç ”ç©¶ä¸­æœªè¢«è¯†åˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯„ä¼°ç®¡é“çš„è®¾è®¡åŒ…æ‹¬å¯¹åè§è¡¨è¾¾çš„ç»†ç²’åº¦åˆ†ç±»ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹åœ¨åè§è¡¨è¾¾ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶ç¡®ä¿æ¨ç†è¿‡ç¨‹çš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»†å¾®çš„å›¾åƒåè§åœ¨æ¨ç†ä¸­è¾ƒå°‘è¢«è¡¨è¾¾ï¼Œè€Œè®¸å¤šæ¨¡å‹å±•ç°å‡ºéä¸€è‡´æ€§æ¨ç†ç°è±¡ï¼Œå¯èƒ½å½±å“æ¨ç†çš„å‡†ç¡®æ€§ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦çš„æ”¹è¿›æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹åè§çš„è¯†åˆ«å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºç³»ç»Ÿçš„å…¬å¹³æ€§å’Œå¯é æ€§ï¼Œæ¨åŠ¨æ›´å®‰å…¨çš„AIåº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.

