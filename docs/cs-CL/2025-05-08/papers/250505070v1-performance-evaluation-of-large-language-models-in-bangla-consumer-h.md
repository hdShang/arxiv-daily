---
layout: default
title: Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization
---

# Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05070" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05070v1</a>
  <a href="https://arxiv.org/pdf/2505.05070.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05070v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05070v1', 'Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ajwad Abrar, Farzana Tabassum, Sabbir Ahmed

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

**DOI**: [10.1109/ICCIT64611.2024.11022034](https://doi.org/10.1109/ICCIT64611.2024.11022034)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å­ŸåŠ æ‹‰æ¶ˆè´¹è€…å¥åº·æŸ¥è¯¢æ‘˜è¦ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¶ˆè´¹è€…å¥åº·æŸ¥è¯¢` `æ‘˜è¦ç”Ÿæˆ` `ä½èµ„æºè¯­è¨€` `ROUGEæŒ‡æ ‡` `é›¶-shotå­¦ä¹ ` `åŒ»ç–—ä¿¡æ¯å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å­ŸåŠ æ‹‰è¯­çš„æ¶ˆè´¹è€…å¥åº·æŸ¥è¯¢å¸¸å¸¸åŒ…å«å†—ä½™ä¿¡æ¯ï¼Œå¯¼è‡´åŒ»ç–—å“åº”æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬ç ”ç©¶è¯„ä¼°äº†ä¹ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶-shotæ¡ä»¶ä¸‹å¯¹å­ŸåŠ æ‹‰CHQsçš„æ‘˜è¦èƒ½åŠ›ï¼Œæ¢ç´¢å…¶æ½œåŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé›¶-shot LLMsåœ¨æ‘˜è¦è´¨é‡ä¸Šå¯ä¸ç»è¿‡å¾®è°ƒçš„æ¨¡å‹ç›¸åª²ç¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ROUGEæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­ŸåŠ æ‹‰è¯­çš„æ¶ˆè´¹è€…å¥åº·æŸ¥è¯¢ï¼ˆCHQsï¼‰å¸¸åŒ…å«å†—ä½™ä¿¡æ¯ï¼Œå½±å“åŒ»ç–—å“åº”çš„æ•ˆç‡ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¹ç§å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ€»ç»“å­ŸåŠ æ‹‰CHQsæ—¶çš„é›¶-shotè¡¨ç°ï¼Œä½¿ç”¨åŒ…å«2350å¯¹æ³¨é‡ŠæŸ¥è¯¢-æ‘˜è¦å¯¹çš„BanglaCHQ-Summæ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMixtral-8x22b-Instructåœ¨ROUGE-1å’ŒROUGE-LæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒBangla T5åœ¨ROUGE-2ä¸Šè¡¨ç°çªå‡ºã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†LLMsåœ¨ä½èµ„æºè¯­è¨€ä¸­åº”å¯¹æŒ‘æˆ˜çš„æ½œåŠ›ï¼Œä¸ºåŒ»ç–—æŸ¥è¯¢æ‘˜è¦æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å­ŸåŠ æ‹‰æ¶ˆè´¹è€…å¥åº·æŸ¥è¯¢ä¸­å†—ä½™ä¿¡æ¯å¯¼è‡´çš„æ‘˜è¦æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä½èµ„æºè¯­è¨€æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶é€šè¿‡è¯„ä¼°ä¹ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶-shotæ¡ä»¶ä¸‹çš„è¡¨ç°ï¼Œæ¢ç´¢å…¶åœ¨ç”Ÿæˆé«˜è´¨é‡æ‘˜è¦æ–¹é¢çš„æ½œåŠ›ï¼Œæ—¨åœ¨è¯æ˜è¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ä¸“é—¨è®­ç»ƒçš„æƒ…å†µä¸‹æä¾›æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä½¿ç”¨BanglaCHQ-Summæ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«2350å¯¹æ³¨é‡Šçš„æŸ¥è¯¢å’Œæ‘˜è¦ã€‚é€šè¿‡ROUGEæŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ‘˜è¦è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå±•ç¤ºäº†é›¶-shot LLMsåœ¨ä½èµ„æºè¯­è¨€æ‘˜è¦ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯Mixtral-8x22b-Instructæ¨¡å‹åœ¨ROUGE-1å’ŒROUGE-LæŒ‡æ ‡ä¸Šçš„ä¼˜å¼‚è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ROUGE-1ã€ROUGE-2å’ŒROUGE-Lä½œä¸ºæ€§èƒ½è¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿äº†å¯¹æ¨¡å‹æ‘˜è¦è´¨é‡çš„å…¨é¢è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMixtral-8x22b-Instructåœ¨ROUGE-1å’ŒROUGE-LæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œåˆ†åˆ«å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åŒæ—¶ï¼ŒBangla T5åœ¨ROUGE-2ä¸Šä¹Ÿå±•ç°äº†å¼ºåŠ²çš„èƒ½åŠ›ï¼Œè¡¨æ˜é›¶-shot LLMsåœ¨ä½èµ„æºè¯­è¨€å¤„ç†ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å¥åº·ä¿¡æ¯ç³»ç»Ÿã€åœ¨çº¿å¥åº·å’¨è¯¢å¹³å°ä»¥åŠä»»ä½•éœ€è¦å¤„ç†ä½èµ„æºè¯­è¨€çš„è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡æå‡æ‘˜è¦è´¨é‡ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ»¡è¶³ç”¨æˆ·çš„å¥åº·æŸ¥è¯¢éœ€æ±‚ï¼Œæ”¹å–„åŒ»ç–—æœåŠ¡çš„å¯åŠæ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language, often contain extraneous details, complicating efficient medical responses. This study investigates the zero-shot performance of nine advanced large language models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet, Llama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro, Qwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs. Using the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary pairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a fine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top performing model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2. The results demonstrate that zero-shot LLMs can rival fine-tuned models, achieving high-quality summaries even without task-specific training. This work underscores the potential of LLMs in addressing challenges in low-resource languages, providing scalable solutions for healthcare query summarization.

