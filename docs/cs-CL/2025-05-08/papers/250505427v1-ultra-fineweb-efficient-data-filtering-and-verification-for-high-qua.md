---
layout: default
title: "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data"
---

# Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05427" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05427v1</a>
  <a href="https://arxiv.org/pdf/2505.05427.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05427v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05427v1', 'Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng, Chaojun Xiao, Xu Han, Zhiyuan Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

**å¤‡æ³¨**: The datasets are available on https://huggingface.co/datasets/openbmb/UltraFineWeb

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUltra-FineWebä»¥è§£å†³é«˜è´¨é‡LLMè®­ç»ƒæ•°æ®è¿‡æ»¤ä¸éªŒè¯é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®è¿‡æ»¤` `æ•°æ®éªŒè¯` `å¤§è¯­è¨€æ¨¡å‹` `è®­ç»ƒæ•°æ®` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `fastText` `é«˜æ•ˆç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°æ®è¿‡æ»¤æ–¹æ³•ç¼ºä¹é«˜æ•ˆçš„éªŒè¯ç­–ç•¥ï¼Œéš¾ä»¥åŠæ—¶åé¦ˆæ•°æ®è´¨é‡ï¼Œå½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®éªŒè¯ç­–ç•¥ï¼Œå¹¶ä¼˜åŒ–äº†æ­£è´Ÿæ ·æœ¬çš„é€‰æ‹©ï¼Œæ„å»ºäº†é«˜æ•ˆçš„æ•°æ®è¿‡æ»¤ç®¡é“ã€‚
3. Ultra-FineWebæ•°æ®é›†çš„åˆ›å»ºä½¿å¾—LLMåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥ç®¡é“çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®è´¨é‡å·²æˆä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„å…³é”®å› ç´ ã€‚æ¨¡å‹é©±åŠ¨çš„æ•°æ®è¿‡æ»¤é€æ¸æˆä¸ºè·å–é«˜è´¨é‡æ•°æ®çš„ä¸»è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¼ºä¹é«˜æ•ˆçš„æ•°æ®éªŒè¯ç­–ç•¥ï¼Œéš¾ä»¥åŠæ—¶åé¦ˆæ•°æ®è´¨é‡ï¼›äºŒæ˜¯ç§å­æ•°æ®é€‰æ‹©æ ‡å‡†ä¸æ˜ç¡®ï¼Œè¿‡äºä¾èµ–äººå·¥ç»éªŒï¼Œå¸¦æ¥ä¸»è§‚æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éªŒè¯ç­–ç•¥ï¼Œèƒ½å¤Ÿä»¥æœ€ä½çš„è®¡ç®—æˆæœ¬å¿«é€Ÿè¯„ä¼°æ•°æ®å¯¹LLMè®­ç»ƒçš„å½±å“ã€‚åŒæ—¶ï¼ŒåŸºäºé«˜è´¨é‡ç§å­æ•°æ®å¯¹LLMè®­ç»ƒçš„ç§¯æå½±å“ï¼Œä¼˜åŒ–äº†æ­£è´Ÿæ ·æœ¬çš„é€‰æ‹©ï¼Œæ„å»ºäº†é«˜æ•ˆçš„æ•°æ®è¿‡æ»¤ç®¡é“ã€‚è¯¥ç®¡é“ä¸ä»…æé«˜äº†è¿‡æ»¤æ•ˆç‡ã€åˆ†ç±»å™¨è´¨é‡å’Œé²æ£’æ€§ï¼Œè¿˜æ˜¾è‘—é™ä½äº†å®éªŒå’Œæ¨ç†æˆæœ¬ã€‚æœ€ç»ˆï¼ŒæˆåŠŸåº”ç”¨äºFineWebå’ŒChinese FineWebæ•°æ®é›†ï¼Œåˆ›å»ºäº†åŒ…å«çº¦1ä¸‡äº¿è‹±è¯­æ ‡è®°å’Œ1200äº¿ä¸­æ–‡æ ‡è®°çš„Ultra-FineWebæ•°æ®é›†ï¼Œå®éªŒè¯æ˜å…¶åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†LLMçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é«˜è´¨é‡LLMè®­ç»ƒæ•°æ®çš„è¿‡æ»¤ä¸éªŒè¯é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•°æ®éªŒè¯ä¸Šæ•ˆç‡ä½ä¸‹ï¼Œä¸”ç§å­æ•°æ®é€‰æ‹©ç¼ºä¹æ˜ç¡®æ ‡å‡†ï¼Œå¯¼è‡´ä¸»è§‚æ€§è¾ƒå¼ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éªŒè¯ç­–ç•¥ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¯„ä¼°æ•°æ®å¯¹LLMè®­ç»ƒçš„å½±å“ï¼Œå¹¶åŸºäºæ­¤ä¼˜åŒ–æ­£è´Ÿæ ·æœ¬çš„é€‰æ‹©ï¼Œæ„å»ºé«˜æ•ˆçš„æ•°æ®è¿‡æ»¤ç®¡é“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®éªŒè¯ã€ç§å­æ•°æ®é€‰æ‹©å’Œæ•°æ®è¿‡æ»¤ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è½»é‡çº§åˆ†ç±»å™¨è¿›è¡Œæ•°æ®éªŒè¯ï¼Œç„¶åä¼˜åŒ–ç§å­æ•°æ®é€‰æ‹©ï¼Œæœ€åå®æ–½é«˜æ•ˆçš„æ•°æ®è¿‡æ»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„éªŒè¯ç­–ç•¥ï¼Œç»“åˆè½»é‡çº§åˆ†ç±»å™¨ï¼Œæ˜¾è‘—æé«˜äº†æ•°æ®è¿‡æ»¤çš„æ•ˆç‡å’Œè´¨é‡ï¼ŒåŒºåˆ«äºä¼ ç»Ÿä¾èµ–äººå·¥ç»éªŒçš„æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨åŸºäºfastTextçš„è½»é‡çº§åˆ†ç±»å™¨ï¼Œè®¾ç½®äº†åˆç†çš„å‚æ•°å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ•°æ®è¿‡æ»¤çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºUltra-FineWebè®­ç»ƒçš„LLMåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ•°æ®é›†å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼ŒéªŒè¯äº†è¿‡æ»¤ç®¡é“çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼ŒUltra-FineWebèƒ½å¤Ÿæ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency.

