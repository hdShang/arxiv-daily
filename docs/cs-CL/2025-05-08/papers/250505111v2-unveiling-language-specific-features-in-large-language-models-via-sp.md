---
layout: default
title: Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders
---

# Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05111" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05111v2</a>
  <a href="https://arxiv.org/pdf/2505.05111.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05111v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05111v2', 'Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boyi Deng, Yu Wan, Yidan Zhang, Baosong Yang, Fuli Feng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08 (æ›´æ–°: 2025-05-27)

**å¤‡æ³¨**: ACL 2025 main

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Aatrox103/multilingual-llm-features)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¨€ç–è‡ªç¼–ç å™¨ä»¥æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¯­è¨€ç‰¹å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `å¤§è¯­è¨€æ¨¡å‹` `å¤šè¯­è¨€èƒ½åŠ›` `ç‰¹å¾åˆ†æ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šè¯­è¨€æ¨¡å‹åˆ†ææ–¹æ³•é¢ä¸´å åŠ å’Œæ¿€æ´»æ–¹å·®ç­‰æŒ‘æˆ˜ï¼Œå½±å“äº†å¯¹è¯­è¨€ç‰¹å¾çš„å¯é æ€§åˆ†æã€‚
2. æœ¬ç ”ç©¶æå‡ºä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰å¯¹LLMsçš„æ¿€æ´»è¿›è¡Œåˆ†è§£ï¼Œä»è€Œæ›´ç»†è‡´åœ°åˆ†æè¯­è¨€ç‰¹å¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¶ˆèç‰¹å®šSAEç‰¹å¾æ˜¾è‘—å½±å“LLMsåœ¨æŸä¸€è¯­è¨€çš„èƒ½åŠ›ï¼ŒåŒæ—¶å‘ç°è¯­è¨€é—´ç‰¹å¾çš„ååŒæ•ˆåº”ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤šè¯­è¨€èƒ½åŠ›çš„æœºåˆ¶ï¼ŒæŒ‡å‡ºç°æœ‰åŸºäºç¥ç»å…ƒæˆ–å†…éƒ¨æ¿€æ´»çš„æ–¹æ³•å­˜åœ¨å åŠ å’Œå±‚é—´æ¿€æ´»æ–¹å·®ç­‰æŒ‘æˆ˜ï¼Œå½±å“å…¶å¯é æ€§ã€‚é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†LLMsçš„æ¿€æ´»åˆ†è§£ä¸ºç¨€ç–çº¿æ€§ç»„åˆçš„ç‰¹å¾ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡æ–¹æ³•æ¥è¯„ä¼°ç‰¹å¾çš„å•è¯­æ€§ã€‚ç ”ç©¶å‘ç°æŸäº›ç‰¹å¾ä¸ç‰¹å®šè¯­è¨€å¯†åˆ‡ç›¸å…³ï¼Œä¸”å¯¹è¿™äº›ç‰¹å¾çš„æ¶ˆèæ˜¾è‘—é™ä½äº†LLMsåœ¨æŸä¸€è¯­è¨€çš„èƒ½åŠ›ï¼Œè€Œå¯¹å…¶ä»–è¯­è¨€å‡ ä¹æ²¡æœ‰å½±å“ã€‚æ­¤å¤–ï¼ŒæŸäº›è¯­è¨€å…·æœ‰å¤šä¸ªååŒçš„SAEç‰¹å¾ï¼Œè”åˆæ¶ˆèæ—¶çš„æ•ˆæœä¼˜äºå•ç‹¬æ¶ˆèã€‚æˆ‘ä»¬åˆ©ç”¨è¿™äº›ç‰¹å¾å¢å¼ºäº†å¼•å¯¼å‘é‡ï¼Œå®ç°äº†å¯¹LLMsç”Ÿæˆè¯­è¨€çš„æ§åˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰å¤šè¯­è¨€æ¨¡å‹åˆ†ææ–¹æ³•çš„å¯é æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯å åŠ å’Œæ¿€æ´»æ–¹å·®å¯¼è‡´çš„ç‰¹å¾åˆ†æä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰ï¼Œå°†LLMsçš„æ¿€æ´»åˆ†è§£ä¸ºç¨€ç–çº¿æ€§ç»„åˆçš„ç‰¹å¾ï¼Œä»è€Œå®ç°å¯¹è¯­è¨€ç‰¹å¾çš„æ›´ç»†è‡´åˆ†æã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®è¾“å…¥ã€SAEç‰¹å¾æå–ã€ç‰¹å¾å•è¯­æ€§è¯„ä¼°ä»¥åŠå¯¹å¼•å¯¼å‘é‡çš„å¢å¼ºã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç‰¹å¾æå–æ¨¡å—å’Œç‰¹å¾æ¶ˆèæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ–¹æ³•æ¥è¯„ä¼°SAEç‰¹å¾çš„å•è¯­æ€§ï¼Œå¹¶å‘ç°ç‰¹å®šè¯­è¨€çš„ç‰¹å¾ä¹‹é—´å­˜åœ¨ååŒæ•ˆåº”ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šæœªè¢«å……åˆ†æ¢è®¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–SAEçš„ç¨€ç–æ€§ï¼Œå¹¶è®¾è®¡äº†å¤šå±‚ç½‘ç»œç»“æ„ä»¥å¢å¼ºç‰¹å¾æå–çš„èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¶ˆèç‰¹å®šSAEç‰¹å¾åï¼ŒLLMsåœ¨æŸä¸€è¯­è¨€çš„èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œå…¶ä»–è¯­è¨€å‡ ä¹ä¸å—å½±å“ã€‚æ­¤å¤–ï¼Œè”åˆæ¶ˆèå¤šä¸ªç‰¹å¾æ—¶ï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡å•ç‹¬æ¶ˆèï¼ŒéªŒè¯äº†ç‰¹å¾é—´çš„ååŒæ•ˆåº”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æ­ç¤ºè¯­è¨€ç‰¹å¾ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨ç‰¹å®šè¯­è¨€ä¸Šçš„è¡¨ç°ï¼Œè¿›è€Œæ¨åŠ¨å¤šè¯­è¨€ç³»ç»Ÿçš„å®ç”¨åŒ–å’Œæ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The mechanisms behind multilingual capabilities in Large Language Models (LLMs) have been examined using neuron-based or internal-activation-based methods. However, these methods often face challenges such as superposition and layer-wise activation variance, which limit their reliability. Sparse Autoencoders (SAEs) offer a more nuanced analysis by decomposing the activations of LLMs into a sparse linear combination of SAE features. We introduce a novel metric to assess the monolinguality of features obtained from SAEs, discovering that some features are strongly related to specific languages. Additionally, we show that ablating these SAE features only significantly reduces abilities in one language of LLMs, leaving others almost unaffected. Interestingly, we find some languages have multiple synergistic SAE features, and ablating them together yields greater improvement than ablating individually. Moreover, we leverage these SAE-derived language-specific features to enhance steering vectors, achieving control over the language generated by LLMs. The code is publicly available at https://github.com/Aatrox103/multilingual-llm-features.

