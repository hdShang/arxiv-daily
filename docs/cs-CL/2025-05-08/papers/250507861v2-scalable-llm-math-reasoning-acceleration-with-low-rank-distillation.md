---
layout: default
title: Scalable LLM Math Reasoning Acceleration with Low-rank Distillation
---

# Scalable LLM Math Reasoning Acceleration with Low-rank Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07861" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07861v2</a>
  <a href="https://arxiv.org/pdf/2505.07861.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07861v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07861v2', 'Scalable LLM Math Reasoning Acceleration with Low-rank Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Harry Dong, Bilge Acun, Beidi Chen, Yuejie Chi

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCapreseä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ•°å­¦æ¨ç†` `è’¸é¦è®­ç»ƒ` `é«˜æ•ˆæ¨ç†` `èµ„æºä¼˜åŒ–` `æ¨¡å‹å‹ç¼©` `å‰é¦ˆç½‘ç»œ` `åˆæˆæ ·æœ¬`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é«˜æ•ˆæ¨ç†æ–¹æ³•åœ¨è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ•°å­¦æ¨ç†æ–¹é¢æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚
2. Capreseæ˜¯ä¸€ç§èµ„æºé«˜æ•ˆçš„è’¸é¦æ–¹æ³•ï¼Œä¸“æ³¨äºæ¢å¤é«˜æ•ˆæ¨ç†ä¸­ä¸§å¤±çš„æ•°å­¦èƒ½åŠ›ï¼Œä¸”ä¸å½±å“è¯­è¨€ä»»åŠ¡æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCapreseæ˜¾è‘—å‡å°‘äº†æ´»è·ƒå‚æ•°æ•°é‡ï¼Œå¹¶é™ä½äº†æ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶æé«˜äº†å“åº”çš„ç®€æ´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”±äºé•¿æ—¶é—´çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚å°½ç®¡å·²æœ‰è®¸å¤šé«˜æ•ˆæ¨ç†æ–¹æ³•åœ¨è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å®ƒä»¬å¾€å¾€ä¸¥é‡å½±å“æ•°å­¦æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§èµ„æºé«˜æ•ˆçš„è’¸é¦æ–¹æ³•Capreseï¼Œæ—¨åœ¨æ¢å¤åœ¨é«˜æ•ˆæ¨ç†ä¸­ä¸§å¤±çš„æ•°å­¦èƒ½åŠ›ï¼Œä¸»è¦é›†ä¸­åœ¨å‰é¦ˆå—ä¸Šã€‚é€šè¿‡ä¸æ‰°åŠ¨åŸå§‹æƒé‡ï¼Œä»…å¢åŠ çº¦1%çš„å‚æ•°å’Œ20Kä¸ªåˆæˆè®­ç»ƒæ ·æœ¬ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ¢å¤æ€è€ƒå‹LLMåœ¨é«˜æ•ˆæ¨ç†ä¸­ä¸§å¤±çš„å¤§éƒ¨åˆ†æ•°å­¦èƒ½åŠ›ï¼ŒåŒæ—¶å¯¹æŒ‡ä»¤å‹LLMçš„è¯­è¨€ä»»åŠ¡æ²¡æœ‰è´Ÿé¢å½±å“ã€‚æ­¤å¤–ï¼ŒCapreseæ˜¾è‘—å‡å°‘äº†æ´»è·ƒå‚æ•°çš„æ•°é‡ï¼ˆGemma 2 9Bå’ŒLlama 3.1 8Bå‡å°‘çº¦20äº¿ï¼‰ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰æ¨¡å‹å±‚ä¸­ï¼Œé™ä½å»¶è¿Ÿï¼ˆè¶…è¿‡16%çš„æ—¶é—´åˆ°ä¸‹ä¸€ä¸ªtokençš„å‡å°‘ï¼‰ï¼ŒåŒæ—¶é¼“åŠ±å“åº”ç®€æ´ï¼ˆæœ€å¤šå‡å°‘8.5%çš„tokenæ•°é‡ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨é«˜æ•ˆæ¨ç†ä¸­æ•°å­¦æ¨ç†èƒ½åŠ›çš„ä¸§å¤±é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶åœ¨è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ•°å­¦æ¨ç†æ–¹é¢å´æ˜¾è‘—ä¸‹é™ï¼Œå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹å’Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCapreseé€šè¿‡ä¸æ‰°åŠ¨åŸå§‹æƒé‡ï¼Œåˆ©ç”¨å°‘é‡é¢å¤–å‚æ•°å’Œåˆæˆè®­ç»ƒæ ·æœ¬ï¼Œæ¢å¤åœ¨é«˜æ•ˆæ¨ç†ä¸­ä¸§å¤±çš„æ•°å­¦èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‰é¦ˆå—çš„è’¸é¦ä¸Šï¼Œä»¥ç¡®ä¿åœ¨ä¿æŒè¯­è¨€ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCapreseçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‚æ•°è’¸é¦æ¨¡å—å’Œåˆæˆæ ·æœ¬ç”Ÿæˆæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆæˆæ ·æœ¬è®­ç»ƒè’¸é¦æ¨¡å‹ï¼Œç„¶åå°†å…¶é›†æˆåˆ°ç°æœ‰çš„å‰é¦ˆå±‚ä¸­ï¼Œä»¥å‡å°‘å»¶è¿Ÿå’Œæé«˜æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCapreseçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èµ„æºé«˜æ•ˆçš„è’¸é¦ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œæ¢å¤æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é«˜è®¡ç®—éœ€æ±‚å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCapreseä»…å¢åŠ çº¦1%çš„å‚æ•°ï¼Œå¹¶ä½¿ç”¨20Kä¸ªåˆæˆè®­ç»ƒæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿äº†æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCapreseåœ¨Gemma 2 9Bå’ŒLlama 3.1 8Bæ¨¡å‹ä¸­åˆ†åˆ«å‡å°‘äº†çº¦20äº¿çš„æ´»è·ƒå‚æ•°ï¼Œå¹¶å®ç°äº†è¶…è¿‡16%çš„æ—¶é—´åˆ°ä¸‹ä¸€ä¸ªtokençš„å‡å°‘ï¼ŒåŒæ—¶å“åº”çš„tokenæ•°é‡æœ€å¤šå‡å°‘äº†8.5%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒCapreseåœ¨ä¿æŒè¯­è¨€ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ•™è‚²ã€ç§‘å­¦è®¡ç®—å’Œé‡‘èåˆ†æç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿä¸ºéœ€è¦é«˜æ•ˆæ•°å­¦æ¨ç†çš„ä»»åŠ¡æä¾›æ”¯æŒã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼ŒCapreseæœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on language tasks, they often severely degrade math performance. In this paper, we propose Caprese, a resource-efficient distillation method to recover lost capabilities from deploying efficient inference methods, focused primarily in feedforward blocks. With original weights unperturbed, roughly 1% of additional parameters, and only 20K synthetic training samples, we are able to recover much if not all of the math capabilities lost from efficient inference for thinking LLMs and without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and integrates cleanly into existing model layers to reduce latency (>16% time-to-next-token reduction) while encouraging response brevity (up to 8.5% fewer tokens).

