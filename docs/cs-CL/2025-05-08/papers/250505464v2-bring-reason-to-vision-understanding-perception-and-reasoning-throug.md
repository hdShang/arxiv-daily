---
layout: default
title: Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging
---

# Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05464" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05464v2</a>
  <a href="https://arxiv.org/pdf/2505.05464.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05464v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05464v2', 'Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shiqi Chen, Jinghan Zhang, Tongyao Zhu, Wei Liu, Siyang Gao, Miao Xiong, Manling Li, Junxian He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08 (æ›´æ–°: 2025-07-15)

**å¤‡æ³¨**: ICML 2025. Camera-ready version updated. Our code is publicly available at https://github.com/shiqichen17/VLM_Merging

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ¨¡å‹åˆå¹¶å®ç°è§†è§‰ä¸æ¨ç†èƒ½åŠ›çš„èåˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ¨¡å‹åˆå¹¶` `æ¨ç†èƒ½åŠ›` `å¤šæ¨¡æ€èåˆ` `è·¨æ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¯¹è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›çš„ç»“åˆæœºåˆ¶ç†è§£ä¸è¶³ï¼Œé™åˆ¶äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ¨¡å‹åˆå¹¶çš„æ–¹å¼ï¼Œå°†ä¸åŒæ¨¡æ€çš„æ¨¡å‹å‚æ•°è¿æ¥ï¼Œä¿ƒè¿›æ¨ç†èƒ½åŠ›çš„è¿ç§»ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆå¹¶åçš„æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œä¸”å„å±‚æ¬¡å¯¹æ¨ç†çš„è´¡çŒ®å‡æœ‰æ‰€å¢åŠ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›å¦‚ä½•ç»“åˆå¹¶ç›¸äº’è´¡çŒ®ä»ç„¶ä¸å¤Ÿæ˜ç¡®ã€‚æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æ¨¡å‹åˆå¹¶çš„æ–¹å¼å°†æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›è¿›è¡Œç»„åˆï¼Œæå‡ºäº†è·¨æ¨¡æ€æ¨¡å‹åˆå¹¶çš„æ–¹æ³•ï¼Œä½¿LLMsçš„æ¨ç†èƒ½åŠ›èƒ½å¤Ÿæ— è®­ç»ƒåœ°è½¬ç§»åˆ°VLMsä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åˆå¹¶ä¸ºå¤šæ¨¡æ€é›†æˆä¸ç†è§£æä¾›äº†æœ‰æ•ˆè·¯å¾„ï¼Œå¹¶æ­ç¤ºäº†æ„ŸçŸ¥ä¸æ¨ç†çš„å†…éƒ¨æœºåˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„æ¨ç†èƒ½åŠ›ç»“åˆä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºåŒç±»æ¨¡å‹çš„åˆå¹¶ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨è·¨æ¨¡æ€çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¨¡å‹åˆå¹¶ï¼Œå°†è§†è§‰-è¯­è¨€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°è¿›è¡Œè¿æ¥ï¼Œä»è€Œå®ç°æ¨ç†èƒ½åŠ›çš„è¿ç§»ã€‚æ­¤è®¾è®¡æ—¨åœ¨æ¢ç´¢ä¸åŒæ¨¡æ€ä¹‹é—´çš„ååŒä½œç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹å‚æ•°çš„åˆå¹¶ã€è®­ç»ƒè¿‡ç¨‹çš„ä¼˜åŒ–ä»¥åŠæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è§†è§‰ç‰¹å¾æå–ã€è¯­è¨€ç†è§£æ¨¡å—å’Œåˆå¹¶ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºè·¨æ¨¡æ€æ¨¡å‹çš„åˆå¹¶ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ï¼Œä½¿å¾—æ¨ç†èƒ½åŠ›èƒ½å¤Ÿåœ¨æ— è®­ç»ƒçš„æƒ…å†µä¸‹æœ‰æ•ˆè½¬ç§»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹åˆå¹¶è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è¿æ¥ç­–ç•¥ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†æ¨ç†èƒ½åŠ›çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿åˆå¹¶åçš„æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜è¶Šã€‚å…·ä½“çš„ç½‘ç»œç»“æ„è®¾è®¡ä¿æŒäº†åŸæœ‰æ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›ï¼ŒåŒæ—¶å¢å¼ºäº†æ¨ç†å±‚çš„åŠŸèƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåˆå¹¶åçš„æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºæ¨ç†å‡†ç¡®ç‡æé«˜äº†15%ã€‚æ‰€æœ‰å±‚æ¬¡å¯¹æ¨ç†çš„è´¡çŒ®å‡æœ‰æ‰€å¢åŠ ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹åˆå¹¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡æœ‰æ•ˆèåˆè§†è§‰ä¸è¯­è¨€çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿçš„æ™ºèƒ½æ°´å¹³å’Œå†³ç­–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation.

