---
layout: default
title: "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes"
---

# Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04993" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04993v1</a>
  <a href="https://arxiv.org/pdf/2505.04993.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04993v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04993v1', 'Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhuocheng Gong, Jian Guan, Wei Wu, Huishuai Zhang, Dongyan Zhao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ½œåœ¨åå¥½ç¼–ç æ¡†æ¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ½œåœ¨åå¥½ç¼–ç ` `å¤§è¯­è¨€æ¨¡å‹` `å¯¹é½ç®—æ³•` `äººç±»åå¥½` `ç¦»æ•£æ½œåœ¨ç¼–ç ` `é²æ£’æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨èç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¯¹é½å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå†…å®¹ä¸äººç±»åå¥½æ—¶ï¼Œå¸¸å¸¸ä¾èµ–äºå›ºå®šçš„å¥–åŠ±å‡½æ•°ï¼Œéš¾ä»¥åº”å¯¹äººç±»åå¥½çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„æ½œåœ¨åå¥½ç¼–ç ï¼ˆLPCï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç¦»æ•£æ½œåœ¨ç¼–ç å»ºæ¨¡éšå«å› ç´ åŠå…¶ç»„åˆï¼Œè‡ªåŠ¨æ¨æ–­åå¥½å› ç´ çš„é‡è¦æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLPCåœ¨DPOã€SimPOå’ŒIPOä¸‰ç§å¯¹é½ç®—æ³•ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”åœ¨ä¸åŒåŸºæ¨¡å‹ä¸Šè¡¨ç°ä¸€è‡´ï¼Œå¢å¼ºäº†å¯¹å™ªå£°æ•°æ®çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å¦‚ä½•ä½¿å…¶ç”Ÿæˆå†…å®¹ä¸äººç±»åå¥½å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„åå¥½å»ºæ¨¡æ–¹æ³•å¾€å¾€ä¾èµ–äºæ˜¾å¼æˆ–éšå¼çš„å¥–åŠ±å‡½æ•°ï¼Œå¿½è§†äº†äººç±»åå¥½çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†æ½œåœ¨åå¥½ç¼–ç ï¼ˆLPCï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç¦»æ•£æ½œåœ¨ç¼–ç æ¥å»ºæ¨¡éšå«å› ç´ åŠå…¶ç»„åˆã€‚LPCèƒ½å¤Ÿä¸å¤šç§ç¦»çº¿å¯¹é½ç®—æ³•æ— ç¼é›†æˆï¼Œè‡ªåŠ¨æ¨æ–­æ•°æ®ä¸­çš„æ½œåœ¨å› ç´ åŠå…¶é‡è¦æ€§ï¼Œè€Œæ— éœ€ä¾èµ–é¢„å®šä¹‰çš„å¥–åŠ±å‡½æ•°å’Œæ‰‹å·¥ç»„åˆæƒé‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLPCåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æå‡äº†ä¸‰ç§å¯¹é½ç®—æ³•çš„è¡¨ç°ï¼Œå¹¶æœ‰æ•ˆæ•æ‰äººç±»åå¥½çš„åˆ†å¸ƒå·®å¼‚ï¼Œå¢å¼ºäº†å¯¹é½çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå†…å®¹ä¸äººç±»åå¥½å¯¹é½çš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå›ºå®šçš„å¥–åŠ±å‡½æ•°ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰äººç±»åå¥½çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ½œåœ¨åå¥½ç¼–ç ï¼ˆLPCï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç¦»æ•£æ½œåœ¨ç¼–ç æ¥å»ºæ¨¡äººç±»åå¥½çš„éšå«å› ç´ åŠå…¶ç»„åˆï¼Œé¿å…äº†å¯¹é¢„å®šä¹‰å¥–åŠ±å‡½æ•°çš„ä¾èµ–ï¼Œä»è€Œæ›´çµæ´»åœ°é€‚åº”ä¸åŒä»»åŠ¡å’Œäººç¾¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLPCæ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ½œåœ¨ç¼–ç ç”Ÿæˆã€å› ç´ æ¨æ–­å’Œå¯¹é½ç®—æ³•é›†æˆç­‰ä¸»è¦æ¨¡å—ã€‚é€šè¿‡åˆ†ææ•°æ®ï¼Œè‡ªåŠ¨æ¨æ–­å‡ºæ½œåœ¨å› ç´ åŠå…¶é‡è¦æ€§ï¼Œå¹¶ä¸ç°æœ‰å¯¹é½ç®—æ³•ç»“åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šLPCçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨ç¦»æ•£æ½œåœ¨ç¼–ç æ¥è¡¨ç¤ºäººç±»åå¥½çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–å›ºå®šå¥–åŠ±å‡½æ•°çš„å¯¹é½æ–¹æ³•æœ¬è´¨ä¸Šæœ‰æ‰€ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLPCä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ½œåœ¨ç¼–ç çš„å­¦ä¹ ï¼Œå¹¶é€šè¿‡å¤šå±‚ç½‘ç»œç»“æ„æ¥å¢å¼ºå¯¹åå¥½å› ç´ çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰äººç±»åå¥½çš„åˆ†å¸ƒå·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLPCåœ¨DPOã€SimPOå’ŒIPOä¸‰ç§å¯¹é½ç®—æ³•ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œä¸”åœ¨å¤„ç†å™ªå£°æ•°æ®æ—¶ï¼Œæ¨¡å‹çš„é²æ£’æ€§æ˜¾è‘—å¢å¼ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æä¾›æ›´ä¸ºçµæ´»å’Œé²æ£’çš„å¯¹é½æŠ€æœ¯ï¼ŒLPCèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´ç¬¦åˆäººç±»åå¥½çš„æ™ºèƒ½ç³»ç»Ÿï¼Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹çš„è´Ÿè´£ä»»éƒ¨ç½²ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have achieved remarkable success, yet aligning their generations with human preferences remains a critical challenge. Existing approaches to preference modeling often rely on an explicit or implicit reward function, overlooking the intricate and multifaceted nature of human preferences that may encompass conflicting factors across diverse tasks and populations. To address this limitation, we introduce Latent Preference Coding (LPC), a novel framework that models the implicit factors as well as their combinations behind holistic preferences using discrete latent codes. LPC seamlessly integrates with various offline alignment algorithms, automatically inferring the underlying factors and their importance from data without relying on pre-defined reward functions and hand-crafted combination weights. Extensive experiments on multiple benchmarks demonstrate that LPC consistently improves upon three alignment algorithms (DPO, SimPO, and IPO) using three base models (Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis reveals that the learned latent codes effectively capture the differences in the distribution of human preferences and significantly enhance the robustness of alignment against noise in data. By providing a unified representation for the multifarious preference factors, LPC paves the way towards developing more robust and versatile alignment techniques for the responsible deployment of powerful LLMs.

