---
layout: default
title: Rethinking Invariance in In-context Learning
---

# Rethinking Invariance in In-context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04994" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04994v1</a>
  <a href="https://arxiv.org/pdf/2505.04994.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04994v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04994v1', 'Rethinking Invariance in In-context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lizhe Fang, Yifei Wang, Khashayar Gatmiry, Lei Fang, Yisen Wang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/PKU-ML/InvICL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInvariant ICLä»¥è§£å†³ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„ä¸å˜æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `ä¸å˜æ€§` `ä¿¡æ¯ä¸æ³„éœ²` `ä¸Šä¸‹æ–‡ç›¸äº’ä¾èµ–` `è‡ªå›å½’æ¨¡å‹` `æ³›åŒ–èƒ½åŠ›` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•å¯¹ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„é¡ºåºæ•æ„Ÿï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚
2. æœ¬æ–‡æå‡ºInvariant ICLï¼ˆInvICLï¼‰ï¼Œé€šè¿‡ç¡®ä¿ä¿¡æ¯ä¸æ³„éœ²å’Œä¸Šä¸‹æ–‡ç›¸äº’ä¾èµ–æ¥å®ç°ä¸å˜æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒInvICLåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä»¥å¾€çš„æ¨¡å‹ï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å·²æˆä¸ºè‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€é¡¹å…³é”®èƒ½åŠ›ï¼Œä½†å…¶å¯¹ä¸Šä¸‹æ–‡ç¤ºä¾‹é¡ºåºçš„æ•æ„Ÿæ€§é™åˆ¶äº†å…¶åº”ç”¨ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶æå‡ºäº†å‡ ç§å˜ä½“ç®—æ³•ä»¥å®ç°ç½®æ¢ä¸å˜æ€§ï¼Œä½†è¿™äº›æ–¹æ³•çš„æ€§èƒ½å¾€å¾€æ— æ³•ä¸æ ‡å‡†çš„è‡ªå›å½’ICLç®—æ³•ç›¸åª²ç¾ã€‚æœ¬æ–‡è¯†åˆ«å‡ºè®¾è®¡ä¸å˜ICLç®—æ³•çš„ä¸¤ä¸ªå…³é”®è¦ç´ ï¼šä¿¡æ¯ä¸æ³„éœ²å’Œä¸Šä¸‹æ–‡ç›¸äº’ä¾èµ–ï¼Œè¿™ä¸¤è€…åœ¨ç°æœ‰æ–¹æ³•ä¸­å¹¶æœªåŒæ—¶å®ç°ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Invariant ICLï¼ˆInvICLï¼‰ï¼Œæ—¨åœ¨å®ç°ICLçš„ä¸å˜æ€§ï¼ŒåŒæ—¶ç¡®ä¿è¿™ä¸¤ä¸ªå±æ€§ã€‚å®éªŒè¯æ˜ï¼ŒInvICLåœ¨å¤§å¤šæ•°åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å¯¹ä¸Šä¸‹æ–‡ç¤ºä¾‹é¡ºåºçš„æ•æ„Ÿæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å®ç°ç½®æ¢ä¸å˜æ€§æ—¶ï¼Œå¾€å¾€æ— æ³•åŒæ—¶æ»¡è¶³ä¿¡æ¯ä¸æ³„éœ²å’Œä¸Šä¸‹æ–‡ç›¸äº’ä¾èµ–çš„è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„InvICLæ–¹æ³•é€šè¿‡è®¾è®¡ç‰¹å®šçš„ç®—æ³•ç»“æ„ï¼Œç¡®ä¿åœ¨å®ç°ä¸Šä¸‹æ–‡ç¤ºä¾‹é¡ºåºä¸å˜æ€§çš„åŒæ—¶ï¼Œä¿æŒä¿¡æ¯çš„å®Œæ•´æ€§å’Œä¸Šä¸‹æ–‡ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInvICLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ä¸Šä¸‹æ–‡ç¼–ç ã€ä¿¡æ¯æ•´åˆå’Œè¾“å‡ºç”Ÿæˆå››ä¸ªä¸»è¦æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿ä¸å˜æ€§å’Œä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ã€‚

**å…³é”®åˆ›æ–°**ï¼šInvICLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåŒæ—¶å®ç°äº†ä¿¡æ¯ä¸æ³„éœ²å’Œä¸Šä¸‹æ–‡ç›¸äº’ä¾èµ–ï¼Œè¿™åœ¨ç°æœ‰çš„ICLç®—æ³•ä¸­æ˜¯å‰æ‰€æœªæœ‰çš„ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒInvICLåœ¨å¤„ç†ä¸Šä¸‹æ–‡é¡ºåºå˜åŒ–æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä¿¡æ¯ä¼ é€’å’Œä¸Šä¸‹æ–‡ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒè¾“å…¥é•¿åº¦çš„å˜åŒ–ï¼Œç¡®ä¿æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInvICLåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„è‡ªå›å½’ICLç®—æ³•ï¼Œå°¤å…¶åœ¨å¤„ç†ä¸åŒè¾“å…¥é•¿åº¦æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜ä¸Šä¸‹æ–‡å­¦ä¹ çš„é²æ£’æ€§ï¼ŒInvICLå¯ä»¥åœ¨å¤šç§å®é™…åœºæ™¯ä¸­æå‡æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-Context Learning (ICL) has emerged as a pivotal capability of auto-regressive large language models, yet it is hindered by a notable sensitivity to the ordering of context examples regardless of their mutual independence. To address this issue, recent studies have introduced several variant algorithms of ICL that achieve permutation invariance. However, many of these do not exhibit comparable performance with the standard auto-regressive ICL algorithm. In this work, we identify two crucial elements in the design of an invariant ICL algorithm: information non-leakage and context interdependence, which are not simultaneously achieved by any of the existing methods. These investigations lead us to the proposed Invariant ICL (InvICL), a methodology designed to achieve invariance in ICL while ensuring the two properties. Empirically, our findings reveal that InvICL surpasses previous models, both invariant and non-invariant, in most benchmark datasets, showcasing superior generalization capabilities across varying input lengths. Code is available at https://github.com/PKU-ML/InvICL.

