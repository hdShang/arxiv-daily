---
layout: default
title: "KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification"
---

# KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05583" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.05583v1</a>
  <a href="https://arxiv.org/pdf/2505.05583.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05583v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05583v1', 'KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Qianbo Zang, Christophe Zgrzendek, Igor Tchappi, Afshin Khadangi, Johannes Sedlmeir

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-08

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/QianboZang/KG-HTC)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫KG-HTC‰ª•Ëß£ÂÜ≥Èõ∂Ê†∑Êú¨Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª` `Áü•ËØÜÂõæË∞±` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê` `ÈïøÂ∞æÂàÜÂ∏É` `ËØ≠‰πâÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÁõëÁù£Â≠¶‰π†ÔºåÁº∫‰πèÊ†áÊ≥®Êï∞ÊçÆ‰ΩøÂæóÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂèóÂà∞ÈôêÂà∂„ÄÇ
2. KG-HTCÈÄöËøáÂ∞ÜÁü•ËØÜÂõæË∞±‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁªìÂêàÔºåÂà©Áî®Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊñπÊ≥ï‰∏∫ÂàÜÁ±ªÊèê‰æõÁªìÊûÑÂåñÁöÑËØ≠‰πâ‰∏ä‰∏ãÊñá„ÄÇ
3. Âú®‰∏â‰∏™ÂÖ¨ÂºÄÁöÑHTCÊï∞ÊçÆÈõÜ‰∏äÔºåKG-HTCÂú®‰∏•Ê†ºÁöÑÈõ∂Ê†∑Êú¨ËÆæÁΩÆ‰∏ãÊòæËëóË∂ÖË∂ä‰∏âÁßçÂü∫Á∫øÔºåÁâπÂà´ÊòØÂú®Â±ÇÊ¨°Ê∑±Â±ÇÊ¨°‰∏äË°®Áé∞Á™ÅÂá∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÔºàHTCÔºâÊ∂âÂèäÂ∞ÜÊñáÊ°£ÂàÜÈÖçÂà∞ÁªÑÁªáÂú®ÂàÜÁ±ªÊ≥ï‰∏≠ÁöÑÊ†áÁ≠æ‰∏ä„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÁõëÁù£ÊñπÊ≥ï‰∏äÔºå‰ΩÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÁî±‰∫éÁº∫‰πèÊ†áÊ≥®Êï∞ÊçÆÔºåÁõëÁù£HTCÁöÑ‰ΩøÁî®Èù¢‰∏¥ÊåëÊàò„ÄÇÊ≠§Â§ñÔºåHTCËøòÂ∏∏Â∏∏Èù¢‰∏¥Â§ßÊ†áÁ≠æÁ©∫Èó¥ÂíåÈïøÂ∞æÂàÜÂ∏ÉÁöÑÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÁü•ËØÜÂõæË∞±‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁªìÂêàÁöÑÈõ∂Ê†∑Êú¨Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÊñπÊ≥ïKG-HTCÔºåÊó®Âú®ÈÄöËøáÊèê‰æõÁªìÊûÑÂåñÁöÑËØ≠‰πâ‰∏ä‰∏ãÊñáÊù•Ëß£ÂÜ≥Ëøô‰∫õÊåëÊàò„ÄÇÊàë‰ª¨ÁöÑKG-HTCÊñπÊ≥ïÈÄöËøáÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊñπÊ≥ï‰ªéÁü•ËØÜÂõæË∞±‰∏≠ÊèêÂèñ‰∏éËæìÂÖ•ÊñáÊú¨Áõ∏ÂÖ≥ÁöÑÂ≠êÂõæÔºå‰ªéËÄåÂ¢ûÂº∫LLMsÂú®‰∏çÂêåÂ±ÇÊ¨°ÁêÜËß£Ê†áÁ≠æËØ≠‰πâÁöÑËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKG-HTCÂú®‰∏•Ê†ºÁöÑÈõ∂Ê†∑Êú¨ËÆæÁΩÆ‰∏ãÊòæËëó‰ºò‰∫é‰∏âÁßçÂü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®Â±ÇÊ¨°ÁöÑÊ∑±Â±ÇÊ¨°‰∏äÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª‰∏≠ÁöÑÈõ∂Ê†∑Êú¨Â≠¶‰π†ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Áº∫‰πèÊ†áÊ≥®Êï∞ÊçÆÊó∂ÊïàÊûú‰∏ç‰Ω≥Ôºå‰∏îÈöæ‰ª•Â§ÑÁêÜÂ§ßÊ†áÁ≠æÁ©∫Èó¥ÂíåÈïøÂ∞æÂàÜÂ∏ÉÁöÑÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöKG-HTCÈÄöËøáÊï¥ÂêàÁü•ËØÜÂõæË∞±‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂà©Áî®Áü•ËØÜÂõæË∞±Êèê‰æõÁöÑÁªìÊûÑÂåñËØ≠‰πâ‰ø°ÊÅØÔºåÂ¢ûÂº∫Ê®°ÂûãÂØπÊ†áÁ≠æËØ≠‰πâÁöÑÁêÜËß£ËÉΩÂäõÔºå‰ªéËÄåÂÆûÁé∞Èõ∂Ê†∑Êú¨ÂàÜÁ±ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöKG-HTCÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ËæìÂÖ•ÊñáÊú¨ÁöÑÂ§ÑÁêÜ„ÄÅÁõ∏ÂÖ≥Â≠êÂõæÁöÑÊ£ÄÁ¥¢ÂíåÂü∫‰∫éÊ£ÄÁ¥¢ÁªìÊûúÁöÑÂàÜÁ±ªÂÜ≥Á≠ñ„ÄÇÈ¶ñÂÖàÔºåËæìÂÖ•ÊñáÊú¨ÁªèËøáÂ§ÑÁêÜÂêéÔºå‰ΩøÁî®RAGÊñπÊ≥ï‰ªéÁü•ËØÜÂõæË∞±‰∏≠Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÁöÑÂ≠êÂõæÔºåÁÑ∂ÂêéÂ∞ÜËøô‰∫õÂ≠êÂõæ‰∏éËæìÂÖ•ÊñáÊú¨ÁªìÂêàÔºå‰æõLLMsËøõË°åÂàÜÁ±ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöKG-HTCÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÁü•ËØÜÂõæË∞±‰∏éLLMsÁªìÂêàÔºåÂà©Áî®Áü•ËØÜÂõæË∞±Êèê‰æõÁöÑÁªìÊûÑÂåñ‰ø°ÊÅØÊù•ÊèêÂçáÊ®°ÂûãÂØπÂ±ÇÊ¨°Ê†áÁ≠æÁöÑÁêÜËß£ËÉΩÂäõÔºåËøôÂú®Áé∞ÊúâÁöÑÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ªÊñπÊ≥ï‰∏≠Â∞öÂ±ûÈ¶ñÊ¨°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏äÔºåKG-HTCÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÂàÜÁ±ªÊÄßËÉΩÔºåÂπ∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏≠ÂºïÂÖ•‰∫ÜÁü•ËØÜÂõæË∞±ÁöÑÂµåÂÖ•Ë°®Á§∫Ôºå‰ª•Â¢ûÂº∫ÂØπÊ†áÁ≠æËØ≠‰πâÁöÑÊçïÊçâËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåKG-HTCÂú®‰∏•Ê†ºÁöÑÈõ∂Ê†∑Êú¨ËÆæÁΩÆ‰∏ãÊòæËëó‰ºò‰∫é‰∏âÁßçÂü∫Á∫øÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®Â±ÇÊ¨°Ê∑±Â±ÇÊ¨°ÁöÑÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÊú™Áü•ÔºâÔºåÈ™åËØÅ‰∫ÜÁü•ËØÜÂõæË∞±Âú®Â±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

KG-HTCÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®Â∫îÁî®‰ª∑ÂÄºÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ø°ÊÅØÊ£ÄÁ¥¢„ÄÅËá™Âä®ÊñáÊ°£ÂàÜÁ±ªÂíåÂÜÖÂÆπÊé®ËçêÁ≠âÂú∫ÊôØ‰∏≠„ÄÇÈÄöËøáÊúâÊïàÂ§ÑÁêÜÈïøÂ∞æÊ†áÁ≠æÂíåÂ§ßÊ†áÁ≠æÁ©∫Èó¥ÈóÆÈ¢òÔºåKG-HTCËÉΩÂ§ü‰∏∫ÂÆûÈôÖÂ∫îÁî®Êèê‰æõÊõ¥È´òÊïàÁöÑÊñáÊú¨ÂàÜÁ±ªËß£ÂÜ≥ÊñπÊ°àÔºåÊé®Âä®Êô∫ËÉΩ‰ø°ÊÅØÂ§ÑÁêÜÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Hierarchical Text Classification (HTC) involves assigning documents to labels organized within a taxonomy. Most previous research on HTC has focused on supervised methods. However, in real-world scenarios, employing supervised HTC can be challenging due to a lack of annotated data. Moreover, HTC often faces issues with large label spaces and long-tail distributions. In this work, we present Knowledge Graphs for zero-shot Hierarchical Text Classification (KG-HTC), which aims to address these challenges of HTC in applications by integrating knowledge graphs with Large Language Models (LLMs) to provide structured semantic context during classification. Our method retrieves relevant subgraphs from knowledge graphs related to the input text using a Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to understand label semantics at various hierarchy levels. We evaluate KG-HTC on three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental results show that KG-HTC significantly outperforms three baselines in the strict zero-shot setting, particularly achieving substantial improvements at deeper levels of the hierarchy. This evaluation demonstrates the effectiveness of incorporating structured knowledge into LLMs to address HTC's challenges in large label spaces and long-tailed label distributions. Our code is available at: https://github.com/QianboZang/KG-HTC.

