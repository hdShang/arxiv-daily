---
layout: default
title: Chain-of-Thought Tokens are Computer Program Variables
---

# Chain-of-Thought Tokens are Computer Program Variables

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04955" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04955v1</a>
  <a href="https://arxiv.org/pdf/2505.04955.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04955v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04955v1', 'Chain-of-Thought Tokens are Computer Program Variables')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fangwei Zhu, Peiyi Wang, Zhifang Sui

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/solitaryzero/CoTs_are_Variables)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå°†é“¾å¼æ€ç»´ä»¤ç‰Œè§†ä¸ºè®¡ç®—æœºç¨‹åºå˜é‡ä»¥è§£å†³å¤æ‚æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“¾å¼æ€ç»´` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤æ‚æ¨ç†` `è®¡ç®—æœºç¨‹åºå˜é‡` `å¤šä½æ•°ä¹˜æ³•` `åŠ¨æ€è§„åˆ’` `ä¸­é—´ç»“æœ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„é“¾å¼æ€ç»´æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨æœºåˆ¶ä¸æ˜ç¡®çš„é—®é¢˜ï¼Œå½±å“å…¶æœ‰æ•ˆæ€§ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬æ–‡æå‡ºå°†CoTä»¤ç‰Œè§†ä¸ºè®¡ç®—æœºç¨‹åºä¸­çš„å˜é‡ï¼Œæ¢ç´¢å…¶åœ¨å¤šä½æ•°ä¹˜æ³•å’ŒåŠ¨æ€è§„åˆ’ä¸­çš„ä½œç”¨ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šç ”ç©¶è¡¨æ˜ï¼Œä»…ä¿ç•™ä¸­é—´ç»“æœçš„ä»¤ç‰Œå³å¯å®ç°ä¸å®Œæ•´CoTç›¸å½“çš„æ€§èƒ½ï¼Œä¸”å¹²é¢„CoTå€¼ä¼šå½±å“åç»­ç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¦æ±‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¾—å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ç”Ÿæˆä¸­é—´æ­¥éª¤ï¼Œå·²è¢«è¯æ˜å¯¹è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æœ‰æ•ˆã€‚ç„¶è€Œï¼ŒCoTçš„å†…éƒ¨æœºåˆ¶ä»ç„¶ä¸ç”šæ¸…æ™°ã€‚æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶CoTä»¤ç‰Œåœ¨LLMsä¸­çš„ä½œç”¨ï¼Œèšç„¦äºå¤šä½æ•°ä¹˜æ³•å’ŒåŠ¨æ€è§„åˆ’è¿™ä¸¤é¡¹ç»„åˆä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼Œä»…ä¿ç•™å­˜å‚¨ä¸­é—´ç»“æœçš„ä»¤ç‰Œä¹Ÿèƒ½å®ç°ç›¸ä¼¼çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿåˆ°ä»¥æ›¿ä»£æ½œåœ¨å½¢å¼å­˜å‚¨ä¸­é—´ç»“æœä¸ä¼šå½±å“æ¨¡å‹æ€§èƒ½ã€‚éšæœºå¹²é¢„CoTä¸­çš„æŸäº›å€¼åï¼Œåç»­çš„CoTä»¤ç‰Œå’Œæœ€ç»ˆç­”æ¡ˆä¼šç›¸åº”å˜åŒ–ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒCoTä»¤ç‰Œå¯èƒ½åƒè®¡ç®—æœºç¨‹åºä¸­çš„å˜é‡ï¼Œä½†ä¹Ÿå­˜åœ¨æ„å¤–æ·å¾„å’Œä»¤ç‰Œé—´è®¡ç®—å¤æ‚æ€§é™åˆ¶ç­‰æ½œåœ¨ç¼ºé™·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šä½æ•°ä¹˜æ³•å’ŒåŠ¨æ€è§„åˆ’æ—¶ï¼ŒCoTçš„å†…éƒ¨æœºåˆ¶å°šä¸æ˜ç¡®ï¼Œå¯¼è‡´å…¶åº”ç”¨æ•ˆæœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºå°†CoTä»¤ç‰Œè§†ä¸ºè®¡ç®—æœºç¨‹åºä¸­çš„å˜é‡ï¼Œè®¤ä¸ºè¿™äº›ä»¤ç‰Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­èµ·åˆ°å­˜å‚¨ä¸­é—´ç»“æœçš„ä½œç”¨ã€‚é€šè¿‡è¿™ç§è§†è§’ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£CoTçš„åŠŸèƒ½åŠå…¶æ½œåœ¨é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é€šè¿‡è®¾è®¡å®éªŒï¼Œåˆ†æCoTä»¤ç‰Œåœ¨å¤šä½æ•°ä¹˜æ³•å’ŒåŠ¨æ€è§„åˆ’ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€ç»“æœè¯„ä¼°å’Œå¹²é¢„å®éªŒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†CoTä»¤ç‰Œä¸è®¡ç®—æœºç¨‹åºå˜é‡è¿›è¡Œç±»æ¯”ï¼Œæ­ç¤ºäº†å…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä½œç”¨åŠæ½œåœ¨ç¼ºé™·ã€‚è¿™ä¸€è§†è§’ä¸ºç†è§£å’Œä¼˜åŒ–LLMsæä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œç ”ç©¶è€…è®¾ç½®äº†ä¸åŒçš„å‚æ•°ä»¥è¯„ä¼°CoTä»¤ç‰Œçš„å½±å“ï¼ŒåŒ…æ‹¬ä¸­é—´ç»“æœçš„å­˜å‚¨æ–¹å¼å’Œå¹²é¢„ç­–ç•¥ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿å®éªŒç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…ä¿ç•™å­˜å‚¨ä¸­é—´ç»“æœçš„CoTä»¤ç‰Œåœ¨å¤šä½æ•°ä¹˜æ³•å’ŒåŠ¨æ€è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸å®Œæ•´CoTç›¸å½“çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºå˜é‡çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œéšæœºå¹²é¢„CoTä»¤ç‰Œçš„å€¼ä¼šå¯¼è‡´åç»­ç»“æœçš„æ˜¾è‘—å˜åŒ–ï¼Œè¿›ä¸€æ­¥æ”¯æŒäº†ç ”ç©¶çš„æ ¸å¿ƒè§‚ç‚¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–ç¼–ç¨‹å’Œå¤æ‚å†³ç­–æ”¯æŒç³»ç»Ÿã€‚é€šè¿‡ä¼˜åŒ–LLMsåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œèƒ½å¤Ÿæé«˜æ™ºèƒ½åŠ©æ‰‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè¿›è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›é‡‡ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-thoughts (CoT) requires large language models (LLMs) to generate intermediate steps before reaching the final answer, and has been proven effective to help LLMs solve complex reasoning tasks. However, the inner mechanism of CoT still remains largely unclear. In this paper, we empirically study the role of CoT tokens in LLMs on two compositional tasks: multi-digit multiplication and dynamic programming. While CoT is essential for solving these problems, we find that preserving only tokens that store intermediate results would achieve comparable performance. Furthermore, we observe that storing intermediate results in an alternative latent form will not affect model performance. We also randomly intervene some values in CoT, and notice that subsequent CoT tokens and the final answer would change correspondingly. These findings suggest that CoT tokens may function like variables in computer programs but with potential drawbacks like unintended shortcuts and computational complexity limits between tokens. The code and data are available at https://github.com/solitaryzero/CoTs_are_Variables.

