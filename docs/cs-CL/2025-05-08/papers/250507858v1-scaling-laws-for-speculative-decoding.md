---
layout: default
title: Scaling Laws for Speculative Decoding
---

# Scaling Laws for Speculative Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07858" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07858v1</a>
  <a href="https://arxiv.org/pdf/2505.07858.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07858v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07858v1', 'Scaling Laws for Speculative Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siyuan Yan, Mo Zhu, Guo-qing Jiang, Jianfei Wang, Jiaxing Chen, Wentai Zhang, Xiang Liao, Xiao Cui, Chen Zhang, Zhuoran Song, Ran Zhu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

**å¤‡æ³¨**: 17 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§„èŒƒè§£ç æŠ€æœ¯ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§„èŒƒè§£ç ` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ•ˆç‡` `å¯¹æ•°çº¿æ€§æ‰©å±•` `è‰ç¨¿æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `è‡ªåŠ¨æ‘˜è¦` `é—®ç­”ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§£ç æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†è§„èŒƒè§£ç æŠ€æœ¯ï¼Œé€šè¿‡å»ºç«‹å¯¹æ•°çº¿æ€§æ‰©å±•è§„å¾‹ï¼Œä¼˜åŒ–è‰ç¨¿æ¨¡å‹çš„æ¥å—ç‡å’Œè§£ç é€Ÿåº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒScyllaåœ¨è§£ç ååé‡ä¸Šè¾ƒEAGLE2æå‡äº†2å€ï¼Œå¹¶åœ¨æ‘˜è¦å’Œé—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é«˜æ•ˆè§£ç çš„éœ€æ±‚ä¸æ–­ä¸Šå‡ï¼Œå°¤å…¶æ˜¯åœ¨ä¾èµ–æ‰©å±•é“¾å¼æ¨ç†çš„æ¶æ„ä¸­ï¼Œå¦‚OpenAI-o3å’ŒDeepSeek-R1ï¼Œæœ¬æ–‡ç ”ç©¶äº†é€šè¿‡å¯†é›†LLMæ¶æ„çš„è§„èŒƒè§£ç æŠ€æœ¯ï¼Œä»¥åŠ é€Ÿæ¨ç†ä»»åŠ¡ã€‚å°½ç®¡åˆ©ç”¨å¹¶è¡Œè‰ç¨¿éªŒè¯å‘¨æœŸçš„è§„èŒƒè§£ç æ–¹æ³•å·²æ˜¾ç¤ºå‡ºåŠ é€Ÿæ½œåŠ›ï¼Œä½†ä¸ä¼ ç»Ÿçš„LLMè®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œè§£ç æ•ˆç‡çš„æ‰©å±•è§„å¾‹ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡å‘ç°äº†æ§åˆ¶è‰ç¨¿æ¨¡å‹æ¥å—ç‡ï¼ˆæˆ–è§£ç é€Ÿåº¦ï¼‰çš„å¯¹æ•°çº¿æ€§æ‰©å±•è§„å¾‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå®ç°äº†Scyllaï¼Œæ˜¾è‘—æå‡äº†å¤šç§LLMçš„è§£ç æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è§£ç æ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶çš„æ€§èƒ½æœªèƒ½æ»¡è¶³éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¢ç´¢è§„èŒƒè§£ç æŠ€æœ¯ï¼Œå»ºç«‹å¯¹æ•°çº¿æ€§æ‰©å±•è§„å¾‹ï¼Œä¼˜åŒ–è‰ç¨¿æ¨¡å‹çš„æ¥å—ç‡å’Œè§£ç é€Ÿåº¦ï¼Œä»è€Œæå‡æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¢„è®­ç»ƒé˜¶æ®µã€è‰ç¨¿æ¨¡å‹æ„å»ºå’Œè§£ç è¿‡ç¨‹ã€‚é¢„è®­ç»ƒé˜¶æ®µè´Ÿè´£ç”Ÿæˆåˆå§‹æ¨¡å‹ï¼Œè‰ç¨¿æ¨¡å‹æ„å»ºåˆ™é€šè¿‡å¹¶è¡Œè‰ç¨¿éªŒè¯æé«˜è§£ç æ•ˆç‡ï¼Œæœ€ååœ¨è§£ç è¿‡ç¨‹ä¸­åº”ç”¨æ‰©å±•è§„å¾‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå‘ç°äº†æ§åˆ¶è‰ç¨¿æ¨¡å‹æ¥å—ç‡çš„å¯¹æ•°çº¿æ€§æ‰©å±•è§„å¾‹ï¼Œè¿™ä¸€ç†è®ºä¸ºè§£ç æ•ˆç‡çš„æå‡æä¾›äº†æ–°çš„è§†è§’ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæœ¬æ–‡å¯¹é¢„è®­ç»ƒçš„tokenæ•°é‡ã€è‰ç¨¿æ¨¡å‹çš„å®¹é‡å’Œè§£ç æ‰¹é‡å¤§å°è¿›è¡Œäº†ç³»ç»Ÿçš„è°ƒæ•´ï¼Œä»¥å®ç°æœ€ä½³çš„è§£ç æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒScyllaåœ¨æ¸©åº¦T=0æ—¶çš„æ¥å—ç‡æ¯”EAGLE2é«˜å‡º1.5-2.2å€ï¼Œæ¯”EAGLE3é«˜å‡º0.3ï¼Œå°¤å…¶åœ¨æ‘˜è¦å’Œé—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œå·¥ä¸šæ¨ç†å¼•æ“çš„éƒ¨ç½²å®ç°äº†2å€çš„è§£ç ååé‡æå‡ï¼ŒéªŒè¯äº†ç³»ç»Ÿæ‰©å±•çš„å˜é©æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼Œå¦‚è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆå’Œé—®ç­”ç³»ç»Ÿã€‚é€šè¿‡æå‡è§£ç æ•ˆç‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¹¶æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å·¥ä¸šç•Œçš„å¹¿æ³›åº”ç”¨ï¼Œæœªæ¥å¯èƒ½å½±å“å¤šä¸ªè¡Œä¸šçš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The escalating demand for efficient decoding in large language models (LLMs) is particularly critical for reasoning-intensive architectures like OpenAI-o3 and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This study investigates speculative decoding techniques through dense LLM architectures to establish foundational insights for accelerating reasoning tasks. While speculative decoding methods leveraging parallel draft-verification cycles have emerged as promising acceleration techniques, the scaling laws governing decoding efficiency remain under-explored compared to conventional backbone LLMs developed through Pretraining->SFT->RLHF training paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2 and 1.3) governing draft model acceptance rate (or decoding speed) across three dimensions: pretraining token volume, draft model capacity, and decoding batch size. Building on these laws, we achieve Scylla, which coordinates multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and 0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on summarization and QA tasks (Figure 2). Industrial inference engine deployments demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5), validating the transformative potential of systematic scaling for efficient LLM inference. Code will be released later.

