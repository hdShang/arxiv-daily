---
layout: default
title: "CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass"
---

# CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00389" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00389v1</a>
  <a href="https://arxiv.org/pdf/2505.00389.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00389v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00389v1', 'CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bowen Zhang, Zixin Song, Chunping Li

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01

**å¤‡æ³¨**: Accepted by SIGIR 2025 (Full)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCSE-SFPä»¥è§£å†³æ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— ç›‘ç£å­¦ä¹ ` `å¥å­è¡¨ç¤º` `ç”Ÿæˆæ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `ä¿¡æ¯æ£€ç´¢` `æ–‡æœ¬åˆ†æ` `é¢„è®­ç»ƒæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— ç›‘ç£å¥å­è¡¨ç¤ºæ–¹æ³•å¤šä¾èµ–åˆ¤åˆ«æ€§PLMsï¼Œç¼ºä¹ä¸ç”Ÿæˆæ€§PLMsçš„ç»“åˆï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. CSE-SFPæ–¹æ³•é€šè¿‡åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„ç»“æ„ç‰¹å¾ï¼Œä»…éœ€ä¸€æ¬¡å‰å‘ä¼ æ’­å®ç°æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCSE-SFPåœ¨åµŒå…¥è´¨é‡ã€è®­ç»ƒæ—¶é—´å’Œå†…å­˜æ¶ˆè€—ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥å­è¡¨ç¤ºä½œä¸ºä¿¡æ¯æ£€ç´¢å’Œè®¡ç®—è¯­è¨€å­¦ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå¯¹æ–‡æœ¬èšç±»ã€å†…å®¹åˆ†æã€é—®ç­”ç³»ç»Ÿå’Œç½‘ç»œæœç´¢ç­‰å®é™…åº”ç”¨å…·æœ‰æ·±è¿œå½±å“ã€‚å°½ç®¡åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰çš„æ— ç›‘ç£åµŒå…¥æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºåˆ¤åˆ«æ€§PLMsï¼Œé²œæœ‰å°è¯•å°†æ— ç›‘ç£å¥å­è¡¨ç¤ºä¸ç”Ÿæˆæ€§PLMsç»“åˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºCSE-SFPï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„ç»“æ„ç‰¹å¾ï¼Œä»…éœ€ä¸€æ¬¡å‰å‘ä¼ æ’­å³å¯æœ‰æ•ˆè¿›è¡Œæ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒCSE-SFPä¸ä»…ç”Ÿæˆæ›´é«˜è´¨é‡çš„åµŒå…¥ï¼Œè¿˜æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´å’Œå†…å­˜æ¶ˆè€—ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸¤ç§æ¯”ç‡åº¦é‡ï¼Œè”åˆè¯„ä¼°å¯¹é½æ€§å’Œå‡åŒ€æ€§ï¼Œä¸ºç¼–ç æ¨¡å‹çš„è¯­ä¹‰ç©ºé—´å±æ€§æä¾›äº†æ›´ç¨³å¥çš„è¯„ä¼°æ‰‹æ®µã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ä¸­æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–äºåˆ¤åˆ«æ€§PLMsï¼Œå¯¼è‡´è®­ç»ƒæ—¶é—´å’Œèµ„æºæ¶ˆè€—è¾ƒå¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCSE-SFPé€šè¿‡åˆ©ç”¨ç”Ÿæˆæ€§PLMsçš„ç»“æ„ç‰¹å¾ï¼Œè®¾è®¡å‡ºä¸€ç§ä»…éœ€ä¸€æ¬¡å‰å‘ä¼ æ’­çš„æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å¥å­çš„ç¼–ç ã€ç”Ÿæˆç‰¹å¾çš„æå–ä»¥åŠå¯¹æ¯”å­¦ä¹ çš„æŸå¤±è®¡ç®—ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ç”Ÿæˆæ¨¡å‹çš„å‰å‘ä¼ æ’­å’Œå¯¹æ¯”æŸå¤±çš„ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šCSE-SFPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å¤šæ¬¡å‰å‘ä¼ æ’­éœ€æ±‚ï¼Œæå‡äº†å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCSE-SFPé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ æ•ˆæœï¼Œå¹¶åœ¨ç”Ÿæˆæ¨¡å‹çš„å‚æ•°è®¾ç½®ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿åµŒå…¥è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCSE-SFPåœ¨ç”ŸæˆåµŒå…¥çš„è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†çº¦50%ï¼Œå†…å­˜æ¶ˆè€—é™ä½äº†40%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CSE-SFPæ–¹æ³•åœ¨æ–‡æœ¬èšç±»ã€å†…å®¹åˆ†æã€é—®ç­”ç³»ç»Ÿå’Œç½‘ç»œæœç´¢ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„æ— ç›‘ç£å­¦ä¹ èƒ½åŠ›èƒ½å¤Ÿå¸®åŠ©ä¼ä¸šå’Œç ”ç©¶æœºæ„æ›´å¿«é€Ÿåœ°å¤„ç†å’Œåˆ†ææµ·é‡æ–‡æœ¬æ•°æ®ï¼Œæå‡ä¿¡æ¯æ£€ç´¢å’Œå†…å®¹ç†è§£çš„æ•ˆæœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As a fundamental task in Information Retrieval and Computational Linguistics, sentence representation has profound implications for a wide range of practical applications such as text clustering, content analysis, question-answering systems, and web search. Recent advances in pre-trained language models (PLMs) have driven remarkable progress in this field, particularly through unsupervised embedding derivation methods centered on discriminative PLMs like BERT. However, due to time and computational constraints, few efforts have attempted to integrate unsupervised sentence representation with generative PLMs, which typically possess much larger parameter sizes. Given that state-of-the-art models in both academia and industry are predominantly based on generative architectures, there is a pressing need for an efficient unsupervised text representation framework tailored to decoder-only PLMs. To address this concern, we propose CSE-SFP, an innovative method that exploits the structural characteristics of generative models. Compared to existing strategies, CSE-SFP requires only a single forward pass to perform effective unsupervised contrastive learning. Rigorous experimentation demonstrates that CSE-SFP not only produces higher-quality embeddings but also significantly reduces both training time and memory consumption. Furthermore, we introduce two ratio metrics that jointly assess alignment and uniformity, thereby providing a more robust means for evaluating the semantic spatial properties of encoding models.

