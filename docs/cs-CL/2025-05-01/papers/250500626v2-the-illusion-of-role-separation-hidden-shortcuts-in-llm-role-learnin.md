---
layout: default
title: "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)"
---

# The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00626" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00626v2</a>
  <a href="https://arxiv.org/pdf/2505.00626.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00626v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00626v2', 'The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01 (æ›´æ–°: 2025-05-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡è°ƒæ•´è¾“å…¥ç¼–ç å¢å¼ºLLMè§’è‰²åˆ†ç¦»èƒ½åŠ›ä»¥è§£å†³å¤šè§’è‰²è¡Œä¸ºä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è§’è‰²åˆ†ç¦»` `è¾“å…¥ç¼–ç ` `ä½ç½®ID` `å¤šè§’è‰²è¡Œä¸º` `æ¨¡å‹å¾®è°ƒ` `æ•°æ®å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ•™å¯¼å¤§å‹è¯­è¨€æ¨¡å‹åŒºåˆ†å¤šè§’è‰²è¾“å…¥æ—¶ï¼Œå¾€å¾€ä¾èµ–äºè¡¨é¢ç‰¹å¾ï¼Œå¯¼è‡´æ¨¡å‹è¡Œä¸ºä¸ä¸€è‡´ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡è°ƒæ•´è¾“å…¥ç¼–ç ä¸­çš„ä½ç½®IDï¼Œå¼ºåŒ–è§’è‰²è¾¹ç•Œçš„ä¿¡å·ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹è§’è‰²çš„å‡†ç¡®è¯†åˆ«èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æ–¹æ³•çš„æ¨¡å‹åœ¨è§’è‰²åˆ†ç¦»ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå‡å°‘äº†å¯¹æ·å¾„çš„ä¾èµ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®è·µä¸­è¶Šæ¥è¶Šå¤šåœ°æ•´åˆå¤šç§è¾“å…¥è§’è‰²ï¼ˆå¦‚ç³»ç»ŸæŒ‡ä»¤ã€ç”¨æˆ·æŸ¥è¯¢ã€å¤–éƒ¨å·¥å…·è¾“å‡ºï¼‰ã€‚ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åŒºåˆ†å„è§’è‰²æ¶ˆæ¯ï¼Œå³æˆ‘ä»¬æ‰€ç§°çš„â€œè§’è‰²åˆ†ç¦»â€ï¼Œå¯¹äºä¸€è‡´çš„å¤šè§’è‰²è¡Œä¸ºè‡³å…³é‡è¦ã€‚å°½ç®¡è¿‘æœŸçš„ç ”ç©¶é€šå¸¸é’ˆå¯¹æœ€å…ˆè¿›çš„æç¤ºæ³¨å…¥é˜²å¾¡ï¼Œä½†å°šä¸æ¸…æ¥šè¿™äº›æ–¹æ³•æ˜¯å¦çœŸæ­£æ•™ä¼šLLMsåŒºåˆ†è§’è‰²ï¼Œè¿˜æ˜¯ä»…ä»…è®°å¿†å·²çŸ¥è§¦å‘å™¨ã€‚æœ¬æ–‡è€ƒå¯Ÿäº†â€œè§’è‰²åˆ†ç¦»å­¦ä¹ â€ï¼šæ•™å¯¼LLMsç¨³å¥åŒºåˆ†ç³»ç»Ÿå’Œç”¨æˆ·æ ‡è®°çš„è¿‡ç¨‹ã€‚é€šè¿‡ç®€å•çš„æ§åˆ¶å®éªŒæ¡†æ¶ï¼Œæˆ‘ä»¬å‘ç°å¾®è°ƒæ¨¡å‹é€šå¸¸ä¾èµ–äºä¸¤ç§è§’è‰²è¯†åˆ«çš„ä»£ç†ï¼šä»»åŠ¡ç±»å‹åˆ©ç”¨å’Œæ–‡æœ¬å¼€å§‹ä½ç½®çš„æ¥è¿‘ã€‚å°½ç®¡æ•°æ®å¢å¼ºå¯ä»¥éƒ¨åˆ†ç¼“è§£è¿™äº›æ·å¾„ï¼Œä½†é€šå¸¸å¯¼è‡´è¿­ä»£ä¿®è¡¥è€Œéæ›´æ·±å±‚æ¬¡çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡è°ƒæ•´æ¨¡å‹è¾“å…¥ç¼–ç ä¸­çš„æ ‡è®°ä¿¡å·æ¥å¼ºåŒ–è§’è‰²è¾¹ç•Œçš„â€œä¸å˜ä¿¡å·â€ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ“æ§ä½ç½®IDæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ›´æ¸…æ™°çš„åŒºåˆ†ï¼Œå‡å°‘å¯¹è¡¨é¢ä»£ç†çš„ä¾èµ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè§’è‰²è¾“å…¥ä¸‹è§’è‰²åˆ†ç¦»ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºè¡¨é¢ç‰¹å¾ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è§’è‰²è¯†åˆ«ä¸Šè¡¨ç°ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è°ƒæ•´è¾“å…¥ç¼–ç ä¸­çš„ä½ç½®IDï¼Œå¼ºåŒ–è§’è‰²è¾¹ç•Œçš„ä¿¡å·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æ¸…æ™°åœ°è¯†åˆ«ä¸åŒè§’è‰²çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜è§’è‰²åˆ†ç¦»çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹å¾®è°ƒå’Œè§’è‰²è¯†åˆ«è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è®¾è®¡æ§åˆ¶å®éªŒæ¡†æ¶æ”¶é›†æ•°æ®ï¼Œç„¶åå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæœ€åè¯„ä¼°æ¨¡å‹åœ¨è§’è‰²åˆ†ç¦»ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡ä½ç½®IDçš„è°ƒæ•´æ¥å¼ºåŒ–è§’è‰²è¾¹ç•Œä¿¡å·ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¾èµ–äºè¡¨é¢ç‰¹å¾çš„ç­–ç•¥å½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è§’è‰²è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œå¹¶å¯¹è¾“å…¥ç¼–ç ä¸­çš„ä½ç½®IDè¿›è¡Œäº†ç²¾ç»†è°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°è§’è‰²çš„åŒºåˆ†ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ–°æ–¹æ³•çš„æ¨¡å‹åœ¨è§’è‰²åˆ†ç¦»ä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜äº†çº¦20%çš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ä¸€è‡´æ€§ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹è¡¨é¢ç‰¹å¾çš„ä¾èµ–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€å®¢æœæœºå™¨äººå’Œå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚å¤šè§’è‰²è¾“å…¥æ—¶çš„è¡¨ç°å’Œä¸€è‡´æ€§ã€‚æœªæ¥ï¼Œéšç€LLMsåœ¨å„è¡Œä¸šçš„å¹¿æ³›åº”ç”¨ï¼Œå¢å¼ºè§’è‰²åˆ†ç¦»èƒ½åŠ›å°†å¯¹æå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿå¯é æ€§äº§ç”Ÿé‡è¦å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.

