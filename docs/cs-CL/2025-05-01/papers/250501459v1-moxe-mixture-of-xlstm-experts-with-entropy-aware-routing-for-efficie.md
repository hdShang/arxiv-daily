---
layout: default
title: MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling
---

# MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01459" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01459v1</a>
  <a href="https://arxiv.org/pdf/2505.01459.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01459v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01459v1', 'MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Abdoul Majid O. Thiombiano, Brahim Hnich, Ali Ben Mrad, Mohamed Wiem Mkaouer

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoxEä»¥è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸å¯æ‰©å±•æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `æ‰©å±•é•¿çŸ­æœŸè®°å¿†` `ä¸“å®¶æ··åˆ` `ç†µæ„ŸçŸ¥è·¯ç”±` `è®¡ç®—æ•ˆç‡` `èµ„æºåˆ©ç”¨` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè®¡ç®—å¼€é”€é«˜ä¸”èµ„æºåˆ©ç”¨ä¸å‡è¡¡ã€‚
2. MoxEæ¶æ„ç»“åˆäº†xLSTMçš„è®°å¿†ç»“æ„ä¸MoEçš„ç¨€ç–æ€§ï¼Œé€šè¿‡ç†µæ„ŸçŸ¥è·¯ç”±æœºåˆ¶åŠ¨æ€åˆ†é…ä»¤ç‰Œï¼Œæé«˜èµ„æºåˆ©ç”¨æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoxEåœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šå‡æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¶æ„MoxEï¼Œç»“åˆäº†æ‰©å±•é•¿çŸ­æœŸè®°å¿†ï¼ˆxLSTMï¼‰ä¸ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¡†æ¶ï¼Œä»¥åº”å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨äº†xLSTMçš„åˆ›æ–°è®°å¿†ç»“æ„ï¼ŒåŒæ—¶é€šè¿‡MoEå¼•å…¥ç¨€ç–æ€§ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚æ ¸å¿ƒåœ¨äºä¸€ç§æ–°é¢–çš„åŸºäºç†µçš„è·¯ç”±æœºåˆ¶ï¼ŒåŠ¨æ€å°†ä»¤ç‰Œè·¯ç”±åˆ°ä¸“é—¨çš„ä¸“å®¶ï¼Œä»è€Œç¡®ä¿èµ„æºçš„é«˜æ•ˆå’Œå¹³è¡¡åˆ©ç”¨ã€‚è¯¥æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆç®¡ç†ç¨€æœ‰å’Œå¸¸è§ä»¤ç‰Œï¼Œå¹¶å¼•å…¥äº†ä¸€ç³»åˆ—è¾…åŠ©æŸå¤±ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚ç†è®ºåˆ†æå’Œå®è¯è¯„ä¼°è¡¨æ˜ï¼ŒMoxEåœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ ‡å¿—ç€å¯æ‰©å±•LLMæ¶æ„çš„æ˜¾è‘—è¿›æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œèµ„æºåˆ©ç”¨æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€é¢ä¸´é«˜å¼€é”€å’Œä¸å‡è¡¡çš„èµ„æºåˆ†é…é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoxEé€šè¿‡ç»“åˆxLSTMçš„è®°å¿†èƒ½åŠ›ä¸MoEçš„ç¨€ç–æ€§ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºç†µçš„åŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œä»¥å®ç°é«˜æ•ˆçš„ä»¤ç‰Œå¤„ç†å’Œèµ„æºåˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoxEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬xLSTMæ¨¡å—ã€MoEæ¨¡å—å’Œç†µæ„ŸçŸ¥è·¯ç”±æœºåˆ¶ï¼Œä»¤ç‰Œæ ¹æ®å…¶ç‰¹æ€§è¢«åŠ¨æ€åˆ†é…åˆ°ä¸åŒçš„ä¸“å®¶è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç†µæ„ŸçŸ¥è·¯ç”±æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»¤ç‰Œçš„ç¨€æœ‰æ€§å’Œå¸¸è§æ€§åŠ¨æ€è°ƒæ•´è·¯ç”±ç­–ç•¥ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†èµ„æºåˆ©ç”¨æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è¾…åŠ©æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬åŸºäºç†µçš„æŸå¤±å’Œç»„å¹³è¡¡æŸå¤±ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoxEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºä¼ ç»Ÿå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨å¤„ç†ç¨€æœ‰ä»¤ç‰Œæ—¶è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨èµ„æºç®¡ç†ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoxEæ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶é«˜æ•ˆçš„èµ„æºåˆ©ç”¨å’ŒåŠ¨æ€è·¯ç”±æœºåˆ¶èƒ½å¤Ÿæ”¯æŒæ›´å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€æœºå™¨ç¿»è¯‘ç­‰æŠ€æœ¯çš„å‘å±•ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces MoxE, a novel architecture that synergistically combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of Experts (MoE) framework to address critical scalability and efficiency challenges in large language models (LLMs). The proposed method effectively leverages xLSTM's innovative memory structures while strategically introducing sparsity through MoE to substantially reduce computational overhead. At the heart of our approach is a novel entropy-based routing mechanism, designed to dynamically route tokens to specialized experts, thereby ensuring efficient and balanced resource utilization. This entropy awareness enables the architecture to effectively manage both rare and common tokens, with mLSTM blocks being favored to handle rare tokens. To further enhance generalization, we introduce a suite of auxiliary losses, including entropy-based and group-wise balancing losses, ensuring robust performance and efficient training. Theoretical analysis and empirical evaluations rigorously demonstrate that MoxE achieves significant efficiency gains and enhanced effectiveness compared to existing approaches, marking a notable advancement in scalable LLM architectures.

