---
layout: default
title: Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models
---

# Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00557" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00557v1</a>
  <a href="https://arxiv.org/pdf/2505.00557.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00557v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00557v1', 'Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Makoto Sato

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæç¤ºçš„æ¡†æ¶ä»¥é‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹»è§‰ç°è±¡` `æç¤ºè®¾è®¡` `é‡åŒ–è¯„ä¼°` `æ¨¡å‹å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¹»è§‰ç°è±¡åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œå¯¼è‡´ç”Ÿæˆçš„å†…å®¹ç¼ºä¹äº‹å®å¯é æ€§ï¼Œå½±å“å®é™…åº”ç”¨ã€‚
2. æå‡ºäº†å¹»è§‰è¯±å¯¼æç¤ºï¼ˆHIPï¼‰å’Œå¹»è§‰é‡åŒ–æç¤ºï¼ˆHQPï¼‰ï¼Œé€šè¿‡åˆæˆè¯­ä¹‰ä¸Šè¿œç¦»çš„æ¦‚å¿µæ¥è§¦å‘å¹»è§‰å¹¶é‡åŒ–å…¶å½±å“ã€‚
3. å®éªŒæ˜¾ç¤ºï¼ŒHIPsäº§ç”Ÿçš„å“åº”æ¯”å¯¹ç…§ç»„æ›´ä¸è¿è´¯ï¼Œä¸”ä¸åŒæ¨¡å‹åœ¨å¹»è§‰è¡¨ç°ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¹»è§‰ç°è±¡åœ¨åŒ»ç–—å’Œæ³•å¾‹ç­‰éœ€è¦äº‹å®å¯é æ€§çš„å®é™…åº”ç”¨ä¸­æ—¥ç›Šæˆä¸ºæŒ‘æˆ˜ã€‚å°½ç®¡åœ¨å¯¹é½å’ŒæŒ‡ä»¤è°ƒä¼˜æ–¹é¢å–å¾—äº†è¿›å±•ï¼ŒLLMsä»ç„¶å¯èƒ½ç”Ÿæˆæµç•…ä½†æ ¹æœ¬ä¸çœŸå®çš„è¾“å‡ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„æ¡†æ¶ï¼Œé€šè¿‡å¹»è§‰è¯±å¯¼æç¤ºï¼ˆHIPï¼‰å’Œå¹»è§‰é‡åŒ–æç¤ºï¼ˆHQPï¼‰ç³»ç»Ÿåœ°è§¦å‘å’Œé‡åŒ–å¹»è§‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHIPsäº§ç”Ÿçš„å“åº”ä¸€è‡´æ€§è¾ƒå·®ä¸”å¹»è§‰ç°è±¡æ›´ä¸ºæ˜æ˜¾ï¼Œä¸”ä¸åŒæ¨¡å‹çš„è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚è¯¥æ¡†æ¶ä¸ºç ”ç©¶å¹»è§‰è„†å¼±æ€§æä¾›äº†å¯é‡å¤çš„æµ‹è¯•å¹³å°ï¼Œå¹¶ä¸ºå¼€å‘æ›´å®‰å…¨çš„LLMsé“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¹»è§‰ç°è±¡çš„è§¦å‘ä¸é‡åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç†è§£å¹»è§‰çš„è®¤çŸ¥åŠ¨æ€æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°å¹»è§‰çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„æ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡å¹»è§‰è¯±å¯¼æç¤ºï¼ˆHIPï¼‰æ¥åˆæˆè¯­ä¹‰ä¸Šä¸ç›¸å…³çš„æ¦‚å¿µï¼Œä»è€Œè§¦å‘å¹»è§‰ï¼ŒåŒæ—¶ä½¿ç”¨å¹»è§‰é‡åŒ–æç¤ºï¼ˆHQPï¼‰æ¥è¯„ä¼°ç”Ÿæˆå†…å®¹çš„å¯ä¿¡åº¦å’Œä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¹»è§‰è¯±å¯¼æç¤ºï¼ˆHIPï¼‰ç”¨äºç”Ÿæˆå¹»è§‰å†…å®¹ï¼Œå¹»è§‰é‡åŒ–æç¤ºï¼ˆHQPï¼‰ç”¨äºè¯„ä¼°è¾“å‡ºçš„å¯ä¿¡åº¦ã€ä¿¡å¿ƒå’Œè¿è´¯æ€§ã€‚å®éªŒé€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹çš„å“åº”æ¥éªŒè¯æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ç³»ç»ŸåŒ–çš„æç¤ºè®¾è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§¦å‘å¹¶é‡åŒ–å¹»è§‰ç°è±¡ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„éšæœºç”Ÿæˆæˆ–ç®€å•å¯¹æ¯”æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡æç¤ºæ—¶ï¼Œé‡‡ç”¨äº†è¯­ä¹‰èåˆçš„ç­–ç•¥ï¼Œé€šè¿‡å°†ä¸ç›¸å…³çš„æ¦‚å¿µç»„åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆè¯¯å¯¼æ€§çš„æç¤ºã€‚åŒæ—¶ï¼ŒHQPçš„è¯„åˆ†æœºåˆ¶è€ƒè™‘äº†å¤šä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬è¾“å‡ºçš„è¿è´¯æ€§å’Œå¯ä¿¡åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¹»è§‰è¯±å¯¼æç¤ºï¼ˆHIPï¼‰ç”Ÿæˆçš„å“åº”åœ¨è¿è´¯æ€§å’ŒçœŸå®æ€§ä¸Šæ˜¾è‘—ä½äºå¯¹ç…§ç»„ï¼Œä¸”ä¸åŒæ¨¡å‹çš„å¹»è§‰è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™ä¸ºç†è§£å’Œæ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè´¨é‡æä¾›äº†é‡è¦ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€æ³•å¾‹å’Œæ•™è‚²ç­‰éœ€è¦é«˜å¯é æ€§ä¿¡æ¯çš„åœºæ™¯ã€‚é€šè¿‡é‡åŒ–å’Œç†è§£å¹»è§‰ç°è±¡ï¼Œå¯ä»¥ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›æŒ‡å¯¼ï¼Œæ¨åŠ¨æ›´æ™ºèƒ½çš„è‡ªæˆ‘è°ƒèŠ‚æœºåˆ¶çš„å‘å±•ï¼Œå‡å°‘é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.

