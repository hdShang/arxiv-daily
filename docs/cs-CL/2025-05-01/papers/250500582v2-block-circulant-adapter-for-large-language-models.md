---
layout: default
title: Block Circulant Adapter for Large Language Models
---

# Block Circulant Adapter for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00582" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00582v2</a>
  <a href="https://arxiv.org/pdf/2505.00582.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00582v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00582v2', 'Block Circulant Adapter for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyu Ding, Meiqi Wang, Siyu Liao, Zhongfeng Wang

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01 (æ›´æ–°: 2025-07-15)

**å¤‡æ³¨**: to appear in Proceedings of the 2025 International Joint Conference on Artificial Intelligence (IJCAI-2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå—å¾ªç¯çŸ©é˜µçš„é€‚é…å™¨ä»¥é™ä½å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `å—å¾ªç¯çŸ©é˜µ` `å‚…é‡Œå¶å˜æ¢` `è®¡ç®—æ•ˆç‡` `å‚æ•°ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•é¢ä¸´å·¨å¤§çš„å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå—å¾ªç¯çŸ©é˜µçš„å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨å‚…é‡Œå¶å˜æ¢çš„ç‰¹æ€§æ¥å‡å°‘å‚æ•°å’Œè®¡ç®—é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‚æ•°å’Œè®¡ç®—é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åºå¤§çš„æ¨¡å‹è§„æ¨¡è€Œå˜å¾—å›°éš¾ã€‚è¿‘æœŸåŸºäºå‚…é‡Œå¶åŸŸçš„æ–¹æ³•æ˜¾ç¤ºå‡ºé™ä½å¾®è°ƒæˆæœ¬çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå—å¾ªç¯çŸ©é˜µçš„å¾®è°ƒæ–¹æ³•ï¼Œç»“åˆç¨³å®šçš„è®­ç»ƒå¯å‘å¼ï¼Œåˆ©ç”¨å¾ªç¯çŸ©é˜µå’Œä¸€ç»´å‚…é‡Œå¶å˜æ¢çš„ç‰¹æ€§ï¼Œé™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‚æ•°æ•°é‡ä¸Šæ¯”VeRAå°‘14å€ï¼Œæ¯”LoRAå°‘16å€ï¼Œæ¯”FourierFTå°‘32å€ï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘æˆ–æ›´å¥½çš„ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºåœ¨é¢‘åŸŸä¸­å¾®è°ƒå¤§å‹æ¨¡å‹æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­çš„é«˜å­˜å‚¨å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç°æœ‰çš„å¾®è°ƒæ–¹æ³•å¦‚VeRAå’ŒLoRAåœ¨å‚æ•°å’Œè®¡ç®—æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå—å¾ªç¯çŸ©é˜µçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¾ªç¯çŸ©é˜µçš„æ•°å­¦ç‰¹æ€§å’Œä¸€ç»´å‚…é‡Œå¶å˜æ¢ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹çš„å‚æ•°æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘èµ„æºæ¶ˆè€—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å—å¾ªç¯çŸ©é˜µæ„å»ºã€å‚…é‡Œå¶å˜æ¢åº”ç”¨å’Œæ¨¡å‹è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å‚…é‡Œå¶å˜æ¢å°†æ¨¡å‹å‚æ•°è½¬åŒ–ä¸ºé¢‘åŸŸè¡¨ç¤ºï¼Œç„¶ååˆ©ç”¨å—å¾ªç¯çŸ©é˜µè¿›è¡Œé«˜æ•ˆçš„å‚æ•°æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥å—å¾ªç¯çŸ©é˜µä½œä¸ºå¾®è°ƒçš„æ ¸å¿ƒæœºåˆ¶ï¼Œè¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å¤§å¹…åº¦é™ä½è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç¨³å®šçš„è®­ç»ƒå¯å‘å¼ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§ã€‚åŒæ—¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†æ¨¡å‹æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡çš„å¹³è¡¡ï¼Œç¡®ä¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨èµ„æºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨å‚æ•°æ•°é‡ä¸Šæ¯”VeRAå°‘14å€ï¼Œæ¯”LoRAå°‘16å€ï¼Œæ¯”FourierFTå°‘32å€ï¼Œä¸”åœ¨ä»»åŠ¡æ€§èƒ½ä¸Šä¿æŒæ¥è¿‘æˆ–æ›´ä¼˜ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨é™ä½å¾®è°ƒæˆæœ¬æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…·æœ‰è‰¯å¥½çš„å®ç”¨æ€§å’Œæ¨å¹¿æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ã€‚é€šè¿‡é™ä½å¾®è°ƒæˆæœ¬ï¼Œç ”ç©¶æˆæœå¯ä»¥ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯èƒ½æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å¾®è°ƒä¸­ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.

