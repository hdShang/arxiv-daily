---
layout: default
title: Reasoning Capabilities and Invariability of Large Language Models
---

# Reasoning Capabilities and Invariability of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00776" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00776v1</a>
  <a href="https://arxiv.org/pdf/2505.00776.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00776v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00776v1', 'Reasoning Capabilities and Invariability of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alessandro Raganato, Rafael PeÃ±aloza, Marco Viviani, Gabriella Pasi

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01

**å¤‡æ³¨**: Accepted for publication in the Proceedings of the 23rd IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2024)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–°çš„åŸºå‡†æ•°æ®é›†ä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `åŸºå‡†æ•°æ®é›†` `é€»è¾‘æ¨ç†` `æç¤ºä¾èµ–æ€§` `è®¤çŸ¥å¿ƒç†å­¦` `å®éªŒåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç®€å•æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶åœ¨æç¤ºä¾èµ–æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œä¸“æ³¨äºç®€å•çš„å‡ ä½•æ¨ç†é—®é¢˜ï¼Œä»¥è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å‚æ•°è¾ƒå¤§çš„æ¨¡å‹åœ¨é›¶-shotè®¾ç½®ä¸­è¡¨ç°è¾ƒå¥½ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ï¼Œé“¾å¼æ¨ç†æç¤ºçš„æ•ˆæœå› ä½¿ç”¨æ—¶æœºè€Œå¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šä¸ªåº”ç”¨ä¸­å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å…¶å¤„ç†ç®€å•æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›å¸¸å—åˆ°è´¨ç–‘ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å…¨é¢åˆ†æLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨å…¶å¯¹æç¤ºçš„ä¾èµ–æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ä¸€ç³»åˆ—è¦æ±‚æµ…å±‚é€»è¾‘æ¨ç†çš„ç®€å•æ¨ç†é—®é¢˜ï¼Œé—®é¢˜è®¾è®¡ç¬¦åˆè®¤çŸ¥å¿ƒç†å­¦æ ‡å‡†ï¼Œç¡®ä¿å›ç­”ä»…ä¾èµ–äºæ¨ç†è€Œéå…ˆå‰çš„ç›´è§‰ã€‚é€šè¿‡å¯¹24ä¸ªä¸åŒè§„æ¨¡çš„LLMsè¿›è¡Œé›¶-shotå’Œfew-shotæç¤ºçš„å®è¯åˆ†æï¼Œå‘ç°å°½ç®¡å‚æ•°è¶…è¿‡700äº¿çš„æ¨¡å‹åœ¨é›¶-shotè®¾ç½®ä¸­è¡¨ç°æ›´ä½³ï¼Œä½†ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹22ä¸ªLLMsçš„é“¾å¼æ¨ç†æç¤ºæµ‹è¯•è¡¨æ˜ï¼Œè¯¥æç¤ºçš„æœ‰æ•ˆæ€§å–å†³äºæ¨ç†æ˜¯åœ¨ç­”æ¡ˆä¹‹å‰è¿˜æ˜¯ä¹‹åè¿›è¡Œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ä¸è¶³ï¼Œå°¤å…¶æ˜¯å…¶å¯¹æç¤ºçš„ä¾èµ–æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”ç¼ºä¹æ ‡å‡†åŒ–çš„æµ‹è¯•æ•°æ®é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ç®€å•çš„å‡ ä½•æ¨ç†é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹çš„å›ç­”ä»…ä¾èµ–äºé€»è¾‘æ¨ç†ï¼Œè€Œéå¤–éƒ¨çŸ¥è¯†æˆ–ç›´è§‰ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨é›¶-shotå’Œfew-shotæç¤ºæ–¹æ³•ï¼Œå¯¹24ä¸ªä¸åŒè§„æ¨¡çš„LLMsè¿›è¡Œè¯„ä¼°ã€‚å®éªŒåˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯åŸºäºæ–°æ•°æ®é›†çš„æ¨ç†èƒ½åŠ›æµ‹è¯•ï¼Œå…¶æ¬¡æ˜¯é“¾å¼æ¨ç†æç¤ºçš„æ•ˆæœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†ç¬¦åˆè®¤çŸ¥å¿ƒç†å­¦æ ‡å‡†çš„æ¨ç†é—®é¢˜æ•°æ®é›†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆéš”ç¦»æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸å…¶å¯¹å¤–éƒ¨çŸ¥è¯†çš„ä¾èµ–ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå¼ºè°ƒé€»è¾‘æ¨ç†çš„ç‹¬ç«‹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶-shotå’Œfew-shotæç¤ºï¼Œé“¾å¼æ¨ç†æç¤ºçš„ä½¿ç”¨æ—¶æœºä¹Ÿè¿›è¡Œäº†ç»†è‡´çš„è®¾è®¡ï¼Œä»¥è¯„ä¼°å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå‚æ•°è¶…è¿‡700äº¿çš„LLMsåœ¨é›¶-shotè®¾ç½®ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†ä»æœ‰æ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ã€‚é“¾å¼æ¨ç†æç¤ºçš„æ•ˆæœå› ä½¿ç”¨æ—¶æœºä¸åŒè€Œå¼‚ï¼Œæ˜¾ç¤ºå‡ºåœ¨æ¨ç†ä»»åŠ¡ä¸­æç¤ºè®¾è®¡çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ•™è‚²ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ¨ç†å·¥å…·ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„åº”ç”¨ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ã€‚æœªæ¥ï¼Œè¿™ä¸€ç ”ç©¶æˆæœå¯èƒ½ä¼šå½±å“æ¨¡å‹è®¾è®¡å’Œè¯„ä¼°æ ‡å‡†çš„åˆ¶å®šã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency. In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning. Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction. An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement. An additional test with chain-of-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.

