---
layout: default
title: "FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation"
---

# FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00624" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00624v2</a>
  <a href="https://arxiv.org/pdf/2505.00624.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00624v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00624v2', 'FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chaitali Bhattacharyya, Hyunsei Lee, Junyoung Lee, Shinhyoung Jang, Il hong Suh, Yeseong Kim

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01 (æ›´æ–°: 2025-10-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFineScopeä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸç‰¹å®šé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¢†åŸŸç‰¹å®šæ¨¡å‹` `ç¨€ç–è‡ªç¼–ç å™¨` `ç»“æ„åŒ–å‰ªæ` `è‡ªæ•°æ®è’¸é¦` `è¯­è¨€æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é¢†åŸŸç‰¹å®šä»»åŠ¡ä¸­å‡†ç¡®æ€§ä¸‹é™ï¼Œå°¤å…¶æ˜¯ä¸­å‹æ¨¡å‹åœ¨ä¸“ç”¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚
2. FineScopeé€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨æå–é¢†åŸŸç‰¹å®šå­é›†ï¼Œå¹¶åº”ç”¨ç»“æ„åŒ–å‰ªæä¸è‡ªæ•°æ®è’¸é¦æ¥ä¼˜åŒ–æ¨¡å‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºFineScopeåœ¨å¤šä¸ªé¢†åŸŸç‰¹å®šä»»åŠ¡ä¸­è¶…è¶Šäº†å¤šç§å¤§å‹å‰æ²¿LLMsï¼Œå¹¶èƒ½æ¢å¤å‰ªæåæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»é›¶å¼€å§‹è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œè¿™æ¨åŠ¨äº†å¼€å‘æ›´å°çš„ã€é¢†åŸŸç‰¹å®šçš„LLMsçš„å…´è¶£ï¼Œä»¥ä¿æŒæ•ˆç‡å’Œå¼ºå¤§çš„ä»»åŠ¡æ€§èƒ½ã€‚ä¸­å‹æ¨¡å‹å¦‚LLaMAå·²æˆä¸ºé¢†åŸŸç‰¹å®šé€‚åº”çš„èµ·ç‚¹ï¼Œä½†åœ¨ä¸“ç”¨æ•°æ®é›†ä¸Šæµ‹è¯•æ—¶å¸¸å¸¸å‡ºç°å‡†ç¡®æ€§ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºFineScopeï¼Œä¸€ä¸ªä»å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ä¸­è¡ç”Ÿç´§å‡‘çš„é¢†åŸŸä¼˜åŒ–LLMsçš„æ¡†æ¶ã€‚FineScopeåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ¡†æ¶ï¼Œæå–å¤§å‹æ•°æ®é›†ä¸­çš„é¢†åŸŸç‰¹å®šå­é›†ã€‚æˆ‘ä»¬åº”ç”¨ç»“æ„åŒ–å‰ªæï¼Œç¡®ä¿ç»“æœæ¨¡å‹ä¿ç•™ç›®æ ‡é¢†åŸŸçš„å…³é”®çŸ¥è¯†ã€‚é€šè¿‡è‡ªæ•°æ®è’¸é¦ï¼ŒFineScopeè¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œæ¢å¤åœ¨å‰ªæè¿‡ç¨‹ä¸­ä¸¢å¤±çš„å…³é”®ä¿¡æ¯ã€‚å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒFineScopeåœ¨é¢†åŸŸç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¤šç§å¤§å‹å‰æ²¿LLMsã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢†åŸŸç‰¹å®šé€‚åº”ä¸­çš„å‡†ç¡®æ€§ä¸‹é™é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å‰ªæåå¸¸å¸¸ä¸¢å¤±å…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFineScopeçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æå–é¢†åŸŸç‰¹å®šæ•°æ®å­é›†ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–å‰ªæä¸è‡ªæ•°æ®è’¸é¦ç›¸ç»“åˆï¼Œç¡®ä¿æ¨¡å‹åœ¨å‰ªæåä»èƒ½ä¿ç•™é¢†åŸŸçŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFineScopeçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æå–æ¨¡å—ï¼ˆSAEï¼‰ã€ç»“æ„åŒ–å‰ªææ¨¡å—å’Œè‡ªæ•°æ®è’¸é¦æ¨¡å—ã€‚é¦–å…ˆï¼ŒSAEä»å¤§å‹æ•°æ®é›†ä¸­æå–é¢†åŸŸç‰¹å®šç‰¹å¾ï¼Œç„¶åè¿›è¡Œç»“æ„åŒ–å‰ªæï¼Œæœ€åé€šè¿‡è‡ªæ•°æ®è’¸é¦æ¢å¤å‰ªæè¿‡ç¨‹ä¸­ä¸¢å¤±çš„ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šFineScopeçš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆSAEä¸ç»“æ„åŒ–å‰ªæï¼Œç¡®ä¿æ¨¡å‹åœ¨å‡å°è§„æ¨¡çš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒå¯¹ç›®æ ‡é¢†åŸŸçš„ç†è§£ä¸é€‚åº”èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å‰ªææ–¹æ³•ä¸åŒï¼Œåè€…å¾€å¾€å¿½è§†äº†é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„ä¿ç•™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨FineScopeä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬SAEçš„ç‰¹å¾æå–ç­–ç•¥ã€å‰ªæçš„ç»“æ„åŒ–çº¦æŸä»¥åŠè‡ªæ•°æ®è’¸é¦çš„æŸå¤±å‡½æ•°è®¾ç½®ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æ¨¡å‹åœ¨å‰ªæåèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤é¢†åŸŸç‰¹å®šçš„ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFineScopeåœ¨å¤šä¸ªé¢†åŸŸç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¤šç§å¤§å‹å‰æ²¿LLMsï¼Œå°¤å…¶æ˜¯åœ¨å‡†ç¡®æ€§ä¸Šæå‡æ˜¾è‘—ã€‚å…·ä½“è€Œè¨€ï¼ŒFineScopeåœ¨æŸäº›ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†20%ï¼Œå¹¶ä¸”åœ¨å‰ªæåé€šè¿‡è‡ªæ•°æ®è’¸é¦æ¢å¤äº†å¤§éƒ¨åˆ†åŸå§‹æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FineScopeçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆä¸”å‡†ç¡®çš„é¢†åŸŸç‰¹å®šè¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚åŒ»ç–—ã€æ³•å¾‹å’ŒæŠ€æœ¯æ–‡æ¡£åˆ†æç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„å¤§å°å’Œæ€§èƒ½ï¼ŒFineScopeèƒ½å¤Ÿé™ä½è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼ŒåŒæ—¶æå‡ç‰¹å®šä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œå…·æœ‰æ˜¾è‘—çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach.

