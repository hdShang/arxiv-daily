---
layout: default
title: DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments
---

# DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00739" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00739v4</a>
  <a href="https://arxiv.org/pdf/2506.00739.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00739v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00739v4', 'DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-10-14)

**å¤‡æ³¨**: Accepted by NeurIPS 2025 Workshop Scaling Environments for Agents (SEA)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/microsoft/DefenderBench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDefenderBenchå·¥å…·åŒ…ä»¥è¯„ä¼°ç½‘ç»œå®‰å…¨ç¯å¢ƒä¸­çš„è¯­è¨€ä»£ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç½‘ç»œå®‰å…¨` `è¯­è¨€æ¨¡å‹` `è¯„ä¼°å·¥å…·` `å¼€æºå·¥å…·` `æ¨¡å—åŒ–è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨å°šå¤„äºæ¢ç´¢é˜¶æ®µï¼Œç¼ºä¹ç³»ç»Ÿçš„è¯„ä¼°å·¥å…·ã€‚
2. DefenderBenchæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¤šç§ç½‘ç»œå®‰å…¨ä»»åŠ¡çš„è¯„ä¼°ï¼Œä¾¿äºç ”ç©¶äººå‘˜è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒClaude-3.7-sonnetåœ¨DefenderBenchä¸­è¡¨ç°æœ€ä½³ï¼Œå¾—åˆ†ä¸º81.65ï¼Œå±•ç¤ºäº†è¯¥å·¥å…·åŒ…çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†æ–¹é¢å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å…¶åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†DefenderBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç”¨çš„å¼€æºå·¥å…·åŒ…ï¼Œç”¨äºè¯„ä¼°è¯­è¨€ä»£ç†åœ¨æ”»å‡»ã€é˜²å¾¡å’ŒåŸºäºçŸ¥è¯†çš„ç½‘ç»œå®‰å…¨ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚DefenderBenchåŒ…æ‹¬ç½‘ç»œå…¥ä¾µã€æ¶æ„å†…å®¹æ£€æµ‹ã€ä»£ç æ¼æ´åˆ†æå’Œç½‘ç»œå®‰å…¨çŸ¥è¯†è¯„ä¼°ç­‰ç¯å¢ƒï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›ç»æµå®æƒ ä¸”æ˜“äºè®¿é—®çš„è¯„ä¼°å¹³å°ï¼ŒåŒæ—¶ç¡®ä¿è¯„ä¼°çš„å…¬æ­£æ€§å’Œä¸¥è°¨æ€§ã€‚æˆ‘ä»¬å¯¹å¤šç§æœ€å…ˆè¿›çš„LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºClaude-3.7-sonnetåœ¨DefenderBenchä¸­å¾—åˆ†æœ€é«˜ï¼Œä¸º81.65ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€ä»£ç†åœ¨ç½‘ç»œå®‰å…¨ä»»åŠ¡è¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹æ ‡å‡†åŒ–å’Œç³»ç»ŸåŒ–çš„è¯„ä¼°å·¥å…·ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨ä¸­çš„åº”ç”¨èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDefenderBenché€šè¿‡æä¾›ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„è¯„ä¼°å·¥å…·åŒ…ï¼Œæ¶µç›–æ”»å‡»ã€é˜²å¾¡åŠçŸ¥è¯†è¯„ä¼°ä»»åŠ¡ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªå…¬å¹³ã€å¯é‡å¤çš„è¯„ä¼°å¹³å°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDefenderBenchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºç½‘ç»œå…¥ä¾µæ£€æµ‹ã€æ¶æ„å†…å®¹è¯†åˆ«ã€ä»£ç æ¼æ´åˆ†æå’Œç½‘ç»œå®‰å…¨çŸ¥è¯†è¯„ä¼°ã€‚æ¯ä¸ªæ¨¡å—éƒ½è®¾è®¡ä¸ºå¯ç‹¬ç«‹è¿è¡Œï¼Œä¾¿äºé›†æˆè‡ªå®šä¹‰çš„LLMå’Œä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDefenderBenchçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ¨¡å—åŒ–è®¾è®¡ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®éœ€æ±‚æ·»åŠ æ–°çš„ä»»åŠ¡å’Œæ¨¡å‹ï¼Œä»è€Œä¿ƒè¿›äº†ç ”ç©¶çš„å¯é‡å¤æ€§å’Œå…¬å¹³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒDefenderBenché‡‡ç”¨æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œç¡®ä¿ä¸åŒæ¨¡å‹çš„è¯„ä¼°ç»“æœå…·æœ‰å¯æ¯”æ€§ï¼ŒåŒæ—¶æä¾›äº†è¯¦ç»†çš„ä»»åŠ¡æè¿°å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ä¾¿äºç ”ç©¶äººå‘˜ç†è§£å’Œä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒClaude-3.7-sonnetåœ¨DefenderBenchä¸­å¾—åˆ†æœ€é«˜ï¼Œè¾¾åˆ°81.65ï¼Œæ˜æ˜¾ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¦‚Claude-3.7-sonnet-thinkå¾—åˆ†78.40ï¼Œå¼€æºæ¨¡å‹Llama 3.3 70Bå¾—åˆ†71.81ï¼Œå±•ç¤ºäº†DefenderBenchåœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DefenderBenchçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œå®‰å…¨ç ”ç©¶ã€æ•™è‚²å’Œå·¥ä¸šç•Œã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒè¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä»è€Œæ¨åŠ¨è¯¥é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåº”ç”¨è½åœ°ã€‚æœªæ¥ï¼ŒDefenderBenchå¯èƒ½æˆä¸ºç½‘ç»œå®‰å…¨é¢†åŸŸè¯­è¨€æ¨¡å‹è¯„ä¼°çš„æ ‡å‡†å·¥å…·ï¼Œä¿ƒè¿›æ›´å¤šåˆ›æ–°çš„å‡ºç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench's modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at https://github.com/microsoft/DefenderBench.

