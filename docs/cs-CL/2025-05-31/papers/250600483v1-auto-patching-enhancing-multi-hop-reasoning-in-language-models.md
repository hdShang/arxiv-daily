---
layout: default
title: "Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models"
---

# Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00483" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00483v1</a>
  <a href="https://arxiv.org/pdf/2506.00483.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00483v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00483v1', 'Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aviv Jan, Dean Tahory, Omer Talmi, Omar Abo Mokh

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: 8 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAuto-Patchä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„å¤šè·³æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šè·³æ¨ç†` `è¯­è¨€æ¨¡å‹` `åŠ¨æ€ä¿®è¡¥` `PatchScopes` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šè·³æ¨ç†é—®é¢˜æ—¶ï¼Œå¸¸å¸¸æ— æ³•æœ‰æ•ˆé“¾æ¥è·¨å¤šä¸ªæ¨ç†æ­¥éª¤çš„ä¿¡æ¯ï¼Œå¯¼è‡´è§£é¢˜ç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºçš„Auto-Patchæ–¹æ³•é€šè¿‡åŠ¨æ€ä¿®è¡¥éšè—çŠ¶æ€ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„åˆ†ç±»å™¨æ¥é€‰æ‹©æ€§åœ°ä¿®æ”¹å†…éƒ¨è¡¨ç¤ºï¼Œä»è€Œå¢å¼ºå¤šè·³æ¨ç†èƒ½åŠ›ã€‚
3. åœ¨MuSiQueæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAuto-Patchå°†è§£é¢˜ç‡ä»18.45%æå‡è‡³23.63%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šè·³é—®é¢˜ä»ç„¶å›°æ‰°ç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¿™äº›æ¨¡å‹åœ¨è·¨å¤šä¸ªæ¨ç†æ­¥éª¤é“¾æ¥ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†Auto-Patchï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€ä¿®è¡¥éšè—çŠ¶æ€çš„æ–°æ–¹æ³•ï¼Œä»¥å¢å¼ºLLMsçš„å¤šè·³æ¨ç†èƒ½åŠ›ã€‚åŸºäºPatchScopesæ¡†æ¶ï¼ŒAuto-Patchä½¿ç”¨å­¦ä¹ åˆ°çš„åˆ†ç±»å™¨é€‰æ‹©æ€§åœ°ä¿®æ”¹å†…éƒ¨è¡¨ç¤ºã€‚åœ¨MuSiQueæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAuto-Patchçš„è§£é¢˜ç‡ä»åŸºçº¿çš„18.45%æå‡è‡³23.63Â±0.7%ï¼ˆ3æ¬¡è¿è¡Œï¼‰ï¼Œç¼©å°äº†ä¸é“¾å¼æ€ç»´æç¤ºï¼ˆ27.44%ï¼‰ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†åŠ¨æ€éšè—çŠ¶æ€å¹²é¢„åœ¨æ¨è¿›LLMså¤æ‚æ¨ç†ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè·³æ¨ç†ä¸­æ— æ³•æœ‰æ•ˆé“¾æ¥ä¿¡æ¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œè§£é¢˜ç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAuto-Patchçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€ä¿®è¡¥éšè—çŠ¶æ€ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„åˆ†ç±»å™¨å¯¹å†…éƒ¨è¡¨ç¤ºè¿›è¡Œé€‰æ‹©æ€§ä¿®æ”¹ï¼Œä»¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å¢å¼ºæ¨¡å‹åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAuto-PatchåŸºäºPatchScopesæ¡†æ¶ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬è¾“å…¥æ•°æ®çš„å¤„ç†ã€éšè—çŠ¶æ€çš„åŠ¨æ€ä¿®è¡¥å’Œåˆ†ç±»å™¨çš„åº”ç”¨ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹æ¨ç†å’Œåå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šAuto-Patchçš„ä¸»è¦åˆ›æ–°åœ¨äºåŠ¨æ€å¹²é¢„éšè—çŠ¶æ€çš„èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€å¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚é€šè¿‡è¿™ç§åŠ¨æ€ä¿®è¡¥ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒAuto-Patché‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†é€‚åº”æ€§è°ƒæ•´æœºåˆ¶ï¼Œä»¥ç¡®ä¿éšè—çŠ¶æ€çš„æœ‰æ•ˆä¿®è¡¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAuto-Patchåœ¨MuSiQueæ•°æ®é›†ä¸Šçš„è§£é¢˜ç‡ä»åŸºçº¿çš„18.45%æå‡è‡³23.63Â±0.7%ï¼Œä¸é“¾å¼æ€ç»´æç¤ºçš„27.44%ç›¸æ¯”ï¼Œç¼©å°äº†æ˜¾è‘—çš„å·®è·ã€‚è¿™ä¸€æå‡å±•ç¤ºäº†åŠ¨æ€éšè—çŠ¶æ€å¹²é¢„çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿä»¥åŠå¤æ‚ä¿¡æ¯æ£€ç´¢ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡å¤šè·³æ¨ç†èƒ½åŠ›ï¼ŒAuto-Patchèƒ½å¤Ÿä¸ºè¿™äº›åº”ç”¨æä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆå’Œæ›´è‡ªç„¶çš„äº¤äº’ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-hop questions still stump large language models (LLMs), which struggle to link information across multiple reasoning steps. We introduce Auto-Patch, a novel method that dynamically patches hidden states during inference to enhance multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch selectively modifies internal representations using a learned classifier. Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from 18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to Chain-of-Thought prompting (27.44\%). Our results highlight the potential of dynamic hidden state interventions for advancing complex reasoning in LLMs.

