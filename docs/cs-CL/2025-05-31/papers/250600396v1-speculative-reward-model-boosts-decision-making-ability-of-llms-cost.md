---
layout: default
title: Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively
---

# Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00396" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00396v1</a>
  <a href="https://arxiv.org/pdf/2506.00396.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00396v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00396v1', 'Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiawei Gu, Shangsong Liang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: ACL2025 Oral (Industry Track)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæŠ•æœºå¥–åŠ±æ¨¡å‹ä»¥æå‡LLMå†³ç­–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å†³ç­–èƒ½åŠ›` `æŠ•æœºå¥–åŠ±æ¨¡å‹` `æœç´¢ç­–ç•¥` `æˆæœ¬æ•ˆç›Š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¿½æ±‚æ€§èƒ½æå‡æ—¶ï¼Œå¾€å¾€å¿½è§†äº†æ•ˆç‡ä¸è®¡ç®—æˆæœ¬çš„å¹³è¡¡ï¼Œå¯¼è‡´å†³ç­–èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„æŠ•æœºå¥–åŠ±æ¨¡å‹ï¼ˆSRMï¼‰é€šè¿‡å¤–éƒ¨å¥–åŠ±åˆ†é…å™¨å’ŒæŠ•æœºéªŒè¯æœºåˆ¶ï¼Œæå‡LLMçš„å†³ç­–æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSRMåœ¨å¤šä¸ªå¤æ‚å†³ç­–ä»»åŠ¡ä¸­ï¼Œå¹³å‡å°†æˆæœ¬é™ä½è‡³åŸæœç´¢æ¡†æ¶çš„1/10ï¼ŒåŒæ—¶ä¿æŒå†³ç­–æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œæœ‰æ•ˆçš„å†³ç­–èƒ½åŠ›å¯¹äºå¤„ç†å¤æ‚ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾§é‡äºæ€§èƒ½ï¼Œå¿½è§†äº†æ•ˆç‡ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„å¹³è¡¡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é¦–å…ˆå¼•å…¥3Eæ ‡å‡†ç³»ç»Ÿè¯„ä¼°æœç´¢ç­–ç•¥çš„æ€§ä»·æ¯”ï¼Œæ­ç¤ºç°æœ‰æ–¹æ³•åœ¨è¿½æ±‚å¾®å°æ€§èƒ½æå‡æ—¶å¸¸å¸¸ç‰ºç‰²æ˜¾è‘—æ•ˆç‡ã€‚ä¸ºæé«˜LLMçš„å†³ç­–èƒ½åŠ›å¹¶ä¿æŒæ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†æŠ•æœºå¥–åŠ±æ¨¡å‹ï¼ˆSRMï¼‰ï¼Œè¯¥æ¡†æ¶å¯ä¸ç°æœ‰æœç´¢ç­–ç•¥æ— ç¼é›†æˆã€‚SRMåˆ©ç”¨å¤–éƒ¨å¥–åŠ±åˆ†é…å™¨é¢„æµ‹æœ€ä½³è¡ŒåŠ¨ï¼Œå‡å°‘å¯¹LLMå†…éƒ¨è‡ªæˆ‘è¯„ä¼°çš„ä¾èµ–ï¼Œå¹¶é€šè¿‡æŠ•æœºéªŒè¯æœºåˆ¶ä¿®å‰ªæ¬¡ä¼˜é€‰æ‹©ï¼Œå¼•å¯¼æœç´¢æ›´æœ‰å‰æ™¯çš„æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSRMåœ¨å¤šä¸ªå¤æ‚å†³ç­–ä»»åŠ¡ä¸­å°†æˆæœ¬å¹³å‡é™ä½è‡³åŸæœç´¢æ¡†æ¶çš„1/10ï¼ŒåŒæ—¶ä¿æŒæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å†³ç­–æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨è¿½æ±‚æ€§èƒ½æ—¶ï¼Œç‰ºç‰²äº†è®¡ç®—æ•ˆç‡ï¼Œå¯¼è‡´å†³ç­–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æŠ•æœºå¥–åŠ±æ¨¡å‹ï¼ˆSRMï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨å¥–åŠ±åˆ†é…å™¨æ¥é¢„æµ‹æœ€ä½³è¡ŒåŠ¨ï¼Œä»è€Œå‡å°‘å¯¹LLMå†…éƒ¨è‡ªæˆ‘è¯„ä¼°çš„ä¾èµ–ï¼Œå¹¶é€šè¿‡æŠ•æœºéªŒè¯æœºåˆ¶å¼•å¯¼æœç´¢è¿‡ç¨‹ï¼Œæå‡å†³ç­–æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSRMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤–éƒ¨å¥–åŠ±åˆ†é…å™¨ã€æŠ•æœºéªŒè¯æœºåˆ¶å’Œç°æœ‰æœç´¢ç­–ç•¥çš„é›†æˆã€‚å¤–éƒ¨å¥–åŠ±åˆ†é…å™¨è´Ÿè´£è¯„ä¼°å’Œé¢„æµ‹æœ€ä½³è¡ŒåŠ¨ï¼Œè€ŒæŠ•æœºéªŒè¯æœºåˆ¶åˆ™ç”¨äºä¿®å‰ªæ¬¡ä¼˜é€‰æ‹©ï¼Œç¡®ä¿æœç´¢è¿‡ç¨‹æœå‘æ›´æœ‰å‰æ™¯çš„æ–¹å‘å‘å±•ã€‚

**å…³é”®åˆ›æ–°**ï¼šSRMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å°†å¤–éƒ¨å¥–åŠ±æœºåˆ¶ä¸æŠ•æœºéªŒè¯ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†å†³ç­–æ•ˆç‡ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„è‡ªæˆ‘è¯„ä¼°æœºåˆ¶å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œå‡å°‘äº†è®¡ç®—è´Ÿæ‹…ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SRMä¸­ï¼Œå¤–éƒ¨å¥–åŠ±åˆ†é…å™¨çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒä¼˜ã€‚æ­¤å¤–ï¼ŒæŠ•æœºéªŒè¯æœºåˆ¶çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿå½±å“æ¨¡å‹çš„æ€§èƒ½ï¼Œç¡®ä¿å…¶åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒæŠ•æœºå¥–åŠ±æ¨¡å‹ï¼ˆSRMï¼‰åœ¨å¤šä¸ªå¤æ‚å†³ç­–ä»»åŠ¡ä¸­ï¼Œå¹³å‡å°†æˆæœ¬é™ä½è‡³åŸæœç´¢æ¡†æ¶çš„1/10ï¼ŒåŒæ—¶ä¿æŒå†³ç­–çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡è¡¨æ˜SRMåœ¨æé«˜LLMå†³ç­–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿã€æ™ºèƒ½åŠ©æ‰‹å’Œå¤æ‚ä»»åŠ¡è§„åˆ’ç­‰ã€‚é€šè¿‡æå‡LLMçš„å†³ç­–èƒ½åŠ›ï¼ŒSRMå¯ä»¥åœ¨å¤šä¸ªè¡Œä¸šä¸­å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡å¤„ç†ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œå…·æœ‰æ˜¾è‘—çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effective decision-making in Large Language Models (LLMs) is essential for handling intricate tasks. However, existing approaches prioritize performance but often overlook the balance between effectiveness and computational cost. To address this, we first introduce the 3E Criteria to systematically assess the cost-effectiveness of search strategies, revealing that existing methods often trade significant efficiency for marginal performance gains. To improve LLM decision-making while maintaining efficiency, we propose the Speculative Reward Model (SRM), a plug-and-play framework that seamlessly integrates with existing search strategies. Specifically, SRM employs an external reward assigner to predict optimal actions, reducing reliance on LLMs' internal self-evaluation. And a speculative verification mechanism is used to prune suboptimal choices and guide the search toward more promising steps. We evaluate SRM on several complex decision-making tasks including mathematical reasoning, planning and numerical reasoning in specialized domains. Experimental results show that SRM reduces costs to 1/10 of the original search framework on average while maintaining effectiveness.

