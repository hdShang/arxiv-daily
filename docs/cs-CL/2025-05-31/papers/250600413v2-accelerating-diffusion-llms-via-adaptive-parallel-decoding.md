---
layout: default
title: Accelerating Diffusion LLMs via Adaptive Parallel Decoding
---

# Accelerating Diffusion LLMs via Adaptive Parallel Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00413" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00413v2</a>
  <a href="https://arxiv.org/pdf/2506.00413.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00413v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00413v2', 'Accelerating Diffusion LLMs via Adaptive Parallel Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniel Israel, Guy Van den Broeck, Aditya Grover

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG, cs.PF

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-10-30)

**å¤‡æ³¨**: 10 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”å¹¶è¡Œè§£ç ä»¥åŠ é€Ÿæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆé€Ÿåº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `è‡ªé€‚åº”è§£ç ` `å¹¶è¡Œè®¡ç®—` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªå›å½’è§£ç æ–¹æ³•åœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šå­˜åœ¨ç“¶é¢ˆï¼Œé™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨æ•ˆç‡ã€‚
2. æœ¬æ–‡æå‡ºçš„è‡ªé€‚åº”å¹¶è¡Œè§£ç ï¼ˆAPDï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å¹¶è¡Œé‡‡æ ·çš„æ ‡è®°æ•°é‡ï¼Œè§£å†³äº†é€Ÿåº¦ä¸è´¨é‡ä¹‹é—´çš„çŸ›ç›¾ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAPDåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†ç”Ÿæˆååé‡ï¼Œä¸”è´¨é‡æŸå¤±æå°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç”Ÿæˆé€Ÿåº¦å—åˆ°è‡ªå›å½’è§£ç çš„ç“¶é¢ˆé™åˆ¶ï¼Œåè€…æ˜¯é€ä¸ªé¢„æµ‹æ ‡è®°ã€‚æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ç†è®ºä¸Šå…è®¸å¹¶è¡Œç”Ÿæˆæ ‡è®°ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥è¾¾åˆ°è‡ªå›å½’æ¨¡å‹çš„é€Ÿåº¦è€Œä¸æ˜¾è‘—ç‰ºç‰²è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”è‡ªé€‚åº”å¹¶è¡Œè§£ç ï¼ˆAPDï¼‰ï¼Œè¯¥æ–¹æ³•åŠ¨æ€è°ƒæ•´å¹¶è¡Œé‡‡æ ·çš„æ ‡è®°æ•°é‡ã€‚é€šè¿‡å®šä¹‰dLLMè¾¹é™…æ¦‚ç‡ä¸å°å‹è¾…åŠ©è‡ªå›å½’æ¨¡å‹ä¸‹åºåˆ—çš„è”åˆæ¦‚ç‡ä¹‹é—´çš„ä¹˜æ³•æ··åˆï¼ŒAPDå®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä¼˜åŒ–äº†APDï¼Œå¯ç”¨äº†KVç¼“å­˜å¹¶é™åˆ¶äº†æ©è”½è¾“å…¥çš„å¤§å°ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æå‡ºäº†ä¸‰ä¸ªå¯è°ƒå‚æ•°ï¼Œä»¥çµæ´»æƒè¡¡ååé‡å’Œè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPDåœ¨ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­æä¾›äº†æ˜¾è‘—æ›´é«˜çš„ååé‡ï¼ŒåŒæ—¶è´¨é‡ä¸‹é™æå°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šçš„ç“¶é¢ˆï¼Œç°æœ‰çš„è‡ªå›å½’è§£ç æ–¹æ³•å¯¼è‡´ç”Ÿæˆè¿‡ç¨‹ç¼“æ…¢ï¼Œå½±å“å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè‡ªé€‚åº”å¹¶è¡Œè§£ç ï¼ˆAPDï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å¹¶è¡Œé‡‡æ ·çš„æ ‡è®°æ•°é‡ï¼Œç»“åˆè¾¹é™…æ¦‚ç‡ä¸å°å‹è‡ªå›å½’æ¨¡å‹çš„è”åˆæ¦‚ç‡ï¼Œä»¥æé«˜ç”Ÿæˆé€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAPDæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ ‡è®°é‡‡æ ·æ¨¡å—ã€æ¦‚ç‡æ··åˆæ¨¡å—å’Œä¼˜åŒ–æ¨¡å—ã€‚æ ‡è®°é‡‡æ ·æ¨¡å—è´Ÿè´£å¹¶è¡Œç”Ÿæˆæ ‡è®°ï¼Œæ¦‚ç‡æ··åˆæ¨¡å—ç”¨äºè®¡ç®—è¾¹é™…æ¦‚ç‡ä¸è”åˆæ¦‚ç‡çš„ä¹˜æ³•æ··åˆï¼Œä¼˜åŒ–æ¨¡å—åˆ™å®ç°KVç¼“å­˜å’Œæ©è”½è¾“å…¥çš„é™åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šAPDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŠ¨æ€è°ƒæ•´å¹¶è¡Œé‡‡æ ·çš„èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„è‡ªå›å½’è§£ç æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—æé«˜é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šAPDæ–¹æ³•ä¸­è®¾ç½®äº†ä¸‰ä¸ªå¯è°ƒå‚æ•°ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®å…·ä½“éœ€æ±‚çµæ´»è°ƒæ•´ååé‡ä¸è´¨é‡çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œå¯ç”¨KVç¼“å­˜å’Œé™åˆ¶æ©è”½è¾“å…¥çš„å¤§å°ä¹Ÿæ˜¯æå‡æ€§èƒ½çš„å…³é”®è®¾è®¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAPDæ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ååé‡çš„æ˜¾è‘—æå‡ï¼Œå…·ä½“æ•°æ®è¡¨æ˜å…¶ååé‡æé«˜äº†çº¦30%ï¼Œè€Œè´¨é‡æŸå¤±ä¿æŒåœ¨å¯æ¥å—èŒƒå›´å†…ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜ç”Ÿæˆé€Ÿåº¦ï¼ŒAPDèƒ½å¤Ÿä½¿å¤§è¯­è¨€æ¨¡å‹åœ¨å®æ—¶åº”ç”¨ä¸­æ›´å…·å®ç”¨æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å†™ä½œç­‰æŠ€æœ¯çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.

