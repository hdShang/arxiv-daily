---
layout: default
title: "Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection"
---

# Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00743" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00743v1</a>
  <a href="https://arxiv.org/pdf/2506.00743.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00743v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00743v1', 'Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yeshwanth Venkatesha, Souvik Kundu, Priyadarshini Panda

**åˆ†ç±»**: cs.CL, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤´éƒ¨å‰ªæä¸å®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥ä»¥åŠ é€Ÿè”é‚¦PEFT**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `è”é‚¦å­¦ä¹ ` `å¤šå¤´æ³¨æ„åŠ›` `å¤´éƒ¨å‰ªæ` `å®¢æˆ·ç«¯é€‰æ‹©` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„PEFTæ–¹æ³•åœ¨è”é‚¦å­¦ä¹ ä¸­åº”ç”¨å—é™ï¼Œä¸»è¦ç”±äºè®¾å¤‡èµ„æºæœ‰é™å’Œå®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒä¸å‡ç­‰æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å¤´éƒ¨å‰ªæå’ŒåŠ æƒèšåˆæœºåˆ¶ï¼Œç»“åˆå®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥ï¼Œæ¥ä¼˜åŒ–è”é‚¦å­¦ä¹ ä¸­çš„PEFTè¿‡ç¨‹ã€‚
3. åœ¨MultiNLIç­‰å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨T5-smallæ¨¡å‹å’ŒLoRAæ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œèµ„æºèŠ‚çº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å·²æˆä¸ºé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äºè‡ªç„¶è¯­è¨€å¤„ç†ä¸‹æ¸¸ä»»åŠ¡çš„ä¸»è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œå…¶åœ¨éšç§ä¿æŠ¤çš„åˆ†å¸ƒå¼å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚è”é‚¦å­¦ä¹ ï¼‰ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œä¸»è¦ç”±äºèµ„æºå—é™è®¾å¤‡å’Œå®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒå¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨è”é‚¦å­¦ä¹ æ¡†æ¶å†…é«˜æ•ˆæ‰§è¡ŒPEFTçš„æ–¹æ³•ï¼Œé’ˆå¯¹åŸºäºå¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¤´éƒ¨å‰ªæã€å¤´éƒ¨ç‰¹å®šåŠ æƒèšåˆæœºåˆ¶å’Œå®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†é«˜è¾¾90%çš„ç¨€ç–æ€§ï¼Œé€šä¿¡æ•ˆç‡æå‡1.8å€ï¼Œè®­ç»ƒæ“ä½œå‡å°‘3.9å€ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®ç‡ä¸‹é™ä½äº2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è”é‚¦å­¦ä¹ æ¡†æ¶ä¸­åº”ç”¨PEFTæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¾å¤‡èµ„æºé™åˆ¶å’Œå®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒå¤šæ ·æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™äº›æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œå‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¤´éƒ¨å‰ªææ¥å‡å°‘è®­ç»ƒå¤æ‚åº¦ï¼Œå¹¶å¼•å…¥å¤´éƒ¨ç‰¹å®šçš„åŠ æƒèšåˆæœºåˆ¶å’Œå®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥ï¼Œä»¥ç¡®ä¿å…¨çƒæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ¥è‡ªä¸åŒå®¢æˆ·ç«¯çš„é‡è¦æ›´æ–°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå¤´éƒ¨å‰ªææ¨¡å—ã€åŠ æƒèšåˆæ¨¡å—å’Œå®¢æˆ·ç«¯é€‰æ‹©æ¨¡å—ã€‚å¤´éƒ¨å‰ªææ¨¡å—æ ¹æ®æ³¨æ„åŠ›å¤´çš„é‡è¦æ€§åˆ†æ•°è¿›è¡Œå‰ªæï¼Œå‡è½»å®¢æˆ·ç«¯çš„è®­ç»ƒè´Ÿæ‹…ï¼›åŠ æƒèšåˆæ¨¡å—ç¡®ä¿é‡è¦æ›´æ–°è¢«æœ‰æ•ˆæ•´åˆï¼›å®¢æˆ·ç«¯é€‰æ‹©æ¨¡å—ä¼˜åŒ–å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¤´éƒ¨å‰ªæå’Œå¤´éƒ¨ç‰¹å®šåŠ æƒèšåˆæœºåˆ¶çš„ç»“åˆï¼Œè¿™ä¸ä¼ ç»Ÿçš„PEFTæ–¹æ³•ä¸åŒï¼Œåè€…é€šå¸¸ä¸è€ƒè™‘å¤´éƒ¨çš„é‡è¦æ€§å’Œå®¢æˆ·ç«¯çš„å¤šæ ·æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨LoRAä½œä¸ºPEFTæ–¹æ³•ï¼Œç¡®ä¿æ¨¡å‹çš„ç¨€ç–æ€§è¾¾åˆ°90%ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œå…³æ³¨äºä¿æŒå‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–é€šä¿¡æ•ˆç‡å’Œè®­ç»ƒæ“ä½œæ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è¯¥æ–¹æ³•åœ¨MultiNLIåŸºå‡†ä¸Šå®ç°äº†é«˜è¾¾90%çš„ç¨€ç–æ€§ï¼Œé€šä¿¡æ•ˆç‡æå‡1.8å€ï¼Œè®­ç»ƒæ“ä½œæ•°å‡å°‘3.9å€ï¼ŒåŒæ—¶å‡†ç¡®ç‡ä¸‹é™ä¿æŒåœ¨2%ä»¥å†…ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è”é‚¦å­¦ä¹ åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨éšç§ä¿æŠ¤å’Œèµ„æºå—é™çš„ç¯å¢ƒä¸‹ã€‚é€šè¿‡ä¼˜åŒ–PEFTè¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„æ•°æ®åˆ†å¸ƒä¸­æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in adapting Large Language Models (LLMs) for downstream tasks in Natural Language Processing. However, its adoption in privacy-preserving distributed learning frameworks, such as Federated Learning (FL), remains relatively limited. This is mainly due to challenges specific to FL, such as resource-constrained devices and diverse data distributions among clients. In this paper, we propose an efficient method to perform PEFT within the FL framework for Multi-Head Attention (MHA) based language models. We address the challenges through head pruning, a novel head-specific weighted aggregation mechanism, and a client selection strategy. Head pruning minimizes training complexity within the clients, guided by the importance score computed based on the confidence of the attention head. Weighted aggregation of heads ensures the global model captures crucial updates from diverse clients complementing our client selection strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups, XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting in a communication advantage of up to 1.8x and a reduction in training OPs of 3.9x while maintaining the accuracy drop under 2%.

