---
layout: default
title: Dual Debiasing for Noisy In-Context Learning for Text Generation
---

# Dual Debiasing for Noisy In-Context Learning for Text Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00418" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00418v2</a>
  <a href="https://arxiv.org/pdf/2506.00418.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00418v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00418v2', 'Dual Debiasing for Noisy In-Context Learning for Text Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siqi Liang, Sumyeong Ahn, Paramveer S. Dhillon, Jiayu Zhou

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-06-21)

**å¤‡æ³¨**: Accepted by 2025 ACL Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒé‡å»åå·®æ¡†æ¶ä»¥è§£å†³å™ªå£°æ³¨é‡Šä¸‹çš„æ–‡æœ¬ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬ç”Ÿæˆ` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `å™ªå£°æ£€æµ‹` `å»åå·®` `åˆæˆé‚»å±…` `æ ·æœ¬æ¸…æ´åº¦è¯„åˆ†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å™ªå£°æ£€æµ‹æ–¹æ³•å‡è®¾å™ªå£°æ ·æœ¬çš„å›°æƒ‘åº¦é«˜äºå¹²å‡€æ ·æœ¬ï¼Œä½†åœ¨é«˜å™ªå£°æ¯”ä¾‹ä¸‹è¿™ä¸€å‡è®¾å¤±æ•ˆï¼Œå¯¼è‡´æ£€æµ‹ä¸å‡†ç¡®ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒé‡å»åå·®æ¡†æ¶ï¼Œé€šè¿‡åˆæˆé‚»å±…æ¥ä¿®æ­£å›°æƒ‘åº¦ä¼°è®¡ï¼Œä»è€Œç”Ÿæˆç¨³å¥çš„æ ·æœ¬æ¸…æ´åº¦è¯„åˆ†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å™ªå£°æ£€æµ‹èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ€ç»ˆçš„ICLæ€§èƒ½ä¸å®Œå…¨å¹²å‡€çš„ç¤ºä¾‹è¯­æ–™ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œé«˜è´¨é‡çš„ç¤ºä¾‹ä¾èµ–äºå¤§é‡æ ‡æ³¨è¯­æ–™åº“ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šè¿‡å±€éƒ¨å›°æƒ‘åº¦æ¥æ£€æµ‹å™ªå£°æ³¨é‡Šï¼Œå‡è®¾å™ªå£°æ ·æœ¬çš„å›°æƒ‘åº¦é«˜äºå¹²å‡€æ ·æœ¬ã€‚å½“å™ªå£°æ¯”ä¾‹è¾ƒé«˜æ—¶ï¼Œè¿™ä¸€å‡è®¾å¤±æ•ˆã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†åœ¨å™ªå£°æ³¨é‡Šä¸‹çš„å›°æƒ‘åº¦åŸºç¡€èŒƒå¼ï¼ŒæŒ‡å‡ºå›°æƒ‘åº¦ä¸­å­˜åœ¨æ¥è‡ªæ³¨é‡Šå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å›ºæœ‰çš„é¢†åŸŸçŸ¥è¯†çš„ä¸¤ç§åå·®ã€‚ä¸ºå…‹æœè¿™äº›åå·®ï¼Œæå‡ºäº†ä¸€ç§åŒé‡å»åå·®æ¡†æ¶ï¼Œåˆ©ç”¨åˆæˆé‚»å±…æ˜¾å¼ä¿®æ­£å›°æƒ‘åº¦ä¼°è®¡ï¼Œç”Ÿæˆç¨³å¥çš„æ ·æœ¬æ¸…æ´åº¦è¯„åˆ†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å™ªå£°æ£€æµ‹èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…¶æœ€ç»ˆICLæ€§èƒ½å¯ä¸å®Œå…¨å¹²å‡€çš„ç¤ºä¾‹è¯­æ–™ç›¸åª²ç¾ï¼Œä¸”åœ¨æé«˜å™ªå£°æ¯”ä¾‹ä¸‹ä»ä¿æŒç¨³å¥æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨é«˜å™ªå£°æ³¨é‡Šä¸‹çš„æ–‡æœ¬ç”Ÿæˆå›°æƒ‘åº¦ä¼°è®¡ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å‡è®¾å™ªå£°æ ·æœ¬çš„å›°æƒ‘åº¦é«˜äºå¹²å‡€æ ·æœ¬ï¼Œä½†åœ¨å™ªå£°æ¯”ä¾‹è¾ƒé«˜çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€å‡è®¾å¾€å¾€ä¸æˆç«‹ï¼Œå¯¼è‡´å™ªå£°æ£€æµ‹èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„åŒé‡å»åå·®æ¡†æ¶é€šè¿‡å¼•å…¥åˆæˆé‚»å±…æ¥æ˜¾å¼ä¿®æ­£å›°æƒ‘åº¦ä¼°è®¡ï¼Œä»è€Œå‡è½»æ³¨é‡Šå’Œé¢†åŸŸçŸ¥è¯†å¸¦æ¥çš„åå·®ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ ·æœ¬æ¸…æ´åº¦è¯„åˆ†çš„å‡†ç¡®æ€§ï¼Œä½¿å…¶ä¸å—æ•´ä½“è¯­æ–™å™ªå£°æ°´å¹³çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡åˆæˆé‚»å±…ç”Ÿæˆæ ·æœ¬çš„å›°æƒ‘åº¦ä¼°è®¡ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨è¿™äº›ä¼°è®¡æ¥è®¡ç®—æ ·æœ¬æ¸…æ´åº¦è¯„åˆ†ã€‚æ•´ä¸ªæµç¨‹å¼ºè°ƒäº†å¯¹å›°æƒ‘åº¦çš„ä¿®æ­£å’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºåŒé‡å»åå·®æ¡†æ¶çš„æå‡ºï¼Œå®ƒé€šè¿‡åˆæˆé‚»å±…çš„æ–¹å¼æ˜¾å¼ä¿®æ­£å›°æƒ‘åº¦ä¼°è®¡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æ ·æœ¬çš„æ¸…æ´åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œåˆæˆé‚»å±…çš„ç”Ÿæˆè¿‡ç¨‹éœ€è¦è€ƒè™‘é‚»å±…çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§ï¼›æŸå¤±å‡½æ•°çš„è®¾è®¡åˆ™éœ€ç¡®ä¿å›°æƒ‘åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹Ÿéœ€å…¼é¡¾è®¡ç®—æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨å™ªå£°æ£€æµ‹èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ€ç»ˆçš„ICLæ€§èƒ½ä¸å®Œå…¨å¹²å‡€çš„ç¤ºä¾‹è¯­æ–™ç›¸å½“ã€‚åœ¨æé«˜å™ªå£°æ¯”ä¾‹ä¸‹ï¼Œè¯¥æ–¹æ³•ä»ä¿æŒç¨³å¥æ€§ï¼Œå±•ç°å‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿä»¥åŠæœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æé«˜å™ªå£°æ ·æœ¬çš„æ£€æµ‹èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è´¨é‡ä¸å‡çš„æƒ…å†µä¸‹ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´é«˜æ•ˆçš„æ–‡æœ¬ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In context learning (ICL) relies heavily on high quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed. We reexamine the perplexity based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust Sample Cleanliness Score. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level. Extensive experiments demonstrate our method's superior noise detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.

