---
layout: default
title: "Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions"
---

# Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00421" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00421v1</a>
  <a href="https://arxiv.org/pdf/2506.00421.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00421v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00421v1', 'Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jihyoung Jang, Minwook Bae, Minji Kim, Dilek Hakkani-Tur, Hyounghun Kim

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: ACL 2025 (32 pages); Project website: https://m3c-dataset.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä»¥è§£å†³åŠ¨æ€äº¤äº’ä¸­çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹è¯` `èŠå¤©æœºå™¨äºº` `è®°å¿†æ£€ç´¢` `åŠ¨æ€äº¤äº’` `äººæœºäº¤äº’` `è§†è§‰å¬è§‰èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€èŠå¤©æœºå™¨äººç ”ç©¶ä¸»è¦é›†ä¸­äºå›¾åƒä»»åŠ¡ï¼Œå¿½è§†äº†å¬è§‰è¾“å…¥çš„æ•´åˆï¼Œé™åˆ¶äº†åŠ¨æ€äº¤äº’çš„èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†M^3Cï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå…·å¤‡å¤šæ¨¡æ€è®°å¿†æ£€ç´¢çš„å¯¹è¯æ¨¡å‹ï¼Œä»¥å®ç°æ›´è‡ªç„¶çš„äº’åŠ¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨å¤šæ–¹å¯¹è¯ä¸­èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è§†è§‰å’Œå¬è§‰è¾“å…¥ï¼Œä¿æŒè¿è´¯çš„åŠ¨æ€äº’åŠ¨ï¼Œè¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€èŠå¤©æœºå™¨äººå‘äººç±»èˆ¬çš„çœŸå®äº’åŠ¨æ¼”è¿›ï¼Œå¤šæ¨¡æ€ç ”ç©¶ä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„é¢†åŸŸã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºå›¾åƒç›¸å…³ä»»åŠ¡ï¼Œå¿½è§†äº†å¬è§‰æ–¹é¢çš„æ•´åˆï¼Œä¸”å¤šä¸ºé™æ€äº¤äº’ï¼Œé™åˆ¶äº†è‡ªç„¶å¯¹è¯çš„ä¸°å¯Œæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†M^3Cï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå…·å¤‡å¤šæ¨¡æ€è®°å¿†æ£€ç´¢çš„å¯¹è¯æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„çœŸå®åœºæ™¯ä¸­ä¸å¤šä½è¯´è¯è€…è¿›è¡Œé•¿æœŸå¯¹è¯ã€‚äººç±»è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒè¿è´¯å’ŒåŠ¨æ€äº’åŠ¨æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºå…ˆè¿›å¤šæ¨¡æ€å¯¹è¯ä»£ç†çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€èŠå¤©æœºå™¨äººåœ¨åŠ¨æ€äº¤äº’ä¸­å¯¹å¬è§‰è¾“å…¥æ•´åˆä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¸ºé™æ€å¯¹è¯ï¼Œç¼ºä¹è‡ªç„¶çš„äº’åŠ¨èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šæ¨¡æ€è®°å¿†æ£€ç´¢æœºåˆ¶ï¼Œæœ¬æ–‡è®¾è®¡çš„æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†è§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´ä¸°å¯Œçš„å¯¹è¯ä½“éªŒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€è®°å¿†æ£€ç´¢å’Œå¯¹è¯ç”Ÿæˆå››ä¸ªä¸»è¦æ¨¡å—ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¤šæ¨¡æ€è®°å¿†æ£€ç´¢æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚åœºæ™¯ä¸­ç»´æŒé•¿æœŸå¯¹è¯ï¼ŒåŒºåˆ«äºä»¥å¾€ä»…å…³æ³¨å•ä¸€æ¨¡æ€çš„ç ”ç©¶ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¤šæ¨¡æ€è¾“å…¥çš„èåˆæ•ˆæœï¼Œå¹¶è®¾è®¡äº†é€‚åº”å¤šæ–¹å¯¹è¯çš„ç½‘ç»œç»“æ„ï¼Œä»¥æå‡å¯¹è¯çš„è¿è´¯æ€§å’Œè‡ªç„¶æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨å¤šæ–¹å¯¹è¯ä¸­èƒ½å¤Ÿæœ‰æ•ˆç»´æŒè¿è´¯æ€§ï¼Œä¸”åœ¨åŠ¨æ€äº’åŠ¨ä¸­è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹å’Œæ•™è‚²é¢†åŸŸç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As chatbots continue to evolve toward human-like, real-world, interactions, multimodality remains an active area of research and exploration. So far, efforts to integrate multimodality into chatbots have primarily focused on image-centric tasks, such as visual dialogue and image-based instructions, placing emphasis on the "eyes" of human perception while neglecting the "ears", namely auditory aspects. Moreover, these studies often center around static interactions that focus on discussing the modality rather than naturally incorporating it into the conversation, which limits the richness of simultaneous, dynamic engagement. Furthermore, while multimodality has been explored in multi-party and multi-session conversations, task-specific constraints have hindered its seamless integration into dynamic, natural conversations. To address these challenges, this study aims to equip chatbots with "eyes and ears" capable of more immersive interactions with humans. As part of this effort, we introduce a new multimodal conversation dataset, Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel multimodal conversation model featuring multimodal memory retrieval. Our model, trained on the $M^3C$, demonstrates the ability to seamlessly engage in long-term conversations with multiple speakers in complex, real-world-like settings, effectively processing visual and auditory inputs to understand and respond appropriately. Human evaluations highlight the model's strong performance in maintaining coherent and dynamic interactions, demonstrating its potential for advanced multimodal conversational agents.

