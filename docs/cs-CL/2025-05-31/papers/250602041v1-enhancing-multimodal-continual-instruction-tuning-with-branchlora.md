---
layout: default
title: Enhancing Multimodal Continual Instruction Tuning with BranchLoRA
---

# Enhancing Multimodal Continual Instruction Tuning with BranchLoRA

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.02041" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.02041v1</a>
  <a href="https://arxiv.org/pdf/2506.02041.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.02041v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.02041v1', 'Enhancing Multimodal Continual Instruction Tuning with BranchLoRA')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Duzhen Zhang, Yong Ren, Zhong-Zhi Li, Yahan Yu, Jiahua Dong, Chenxing Li, Zhilong Ji, Jinfeng Bai

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: Accepted by ACL2025 Main Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBranchLoRAä»¥è§£å†³å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜` `ç¾éš¾æ€§é—å¿˜` `BranchLoRA` `æ··åˆä¸“å®¶` `ä»»åŠ¡ç‰¹å®šè·¯ç”±å™¨` `çµæ´»è°ƒä¼˜æœºåˆ¶` `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜æ–¹æ³•å®¹æ˜“å—åˆ°ç¾éš¾æ€§é—å¿˜çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½éšæ—¶é—´ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†BranchLoRAæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è°ƒä¼˜-å†»ç»“æœºåˆ¶å’Œä»»åŠ¡ç‰¹å®šè·¯ç”±å™¨ï¼Œæå‡äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBranchLoRAåœ¨æœ€æ–°çš„MCITåŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šäº†MoELoRAï¼Œä¸”åœ¨ä¸åŒè§„æ¨¡çš„MLLMä¸­å‡è¡¨ç°ä¼˜è¶Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ï¼ˆMCITï¼‰æ—¨åœ¨ä¸æ–­å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä»¥ä¸äººç±»æ„å›¾åœ¨è¿ç»­ä»»åŠ¡ä¸­ä¿æŒä¸€è‡´ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰LoRAæ¡†æ¶æ¥ä¿ç•™å…ˆå‰çš„æŒ‡ä»¤å¯¹é½ï¼Œä½†ç”±äºç®€å•æ±‚å’Œèšåˆæ‰€æœ‰LoRAå—ï¼Œå®¹æ˜“å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼ˆCFï¼‰ï¼Œä»è€Œå½±å“é•¿æœŸæ€§èƒ½ã€‚æœ¬æ–‡è¯†åˆ«äº†MoELoRAæ¡†æ¶åœ¨MCITèƒŒæ™¯ä¸‹çš„å…³é”®å‚æ•°ä½æ•ˆé—®é¢˜ï¼Œå¹¶æå‡ºäº†BranchLoRAï¼Œä¸€ä¸ªä¸å¯¹ç§°æ¡†æ¶ï¼Œä»¥æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚ä¸ºå‡è½»CFï¼ŒBranchLoRAå¼•å…¥äº†çµæ´»çš„è°ƒä¼˜-å†»ç»“æœºåˆ¶ï¼Œä½¿åˆ†æ”¯èƒ½å¤Ÿä¸“æ³¨äºä»»åŠ¡å†…çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ƒè¿›ä»»åŠ¡é—´åä½œã€‚æ­¤å¤–ï¼Œé€æ­¥å¼•å…¥ä»»åŠ¡ç‰¹å®šè·¯ç”±å™¨ï¼Œç¡®ä¿éšç€æ—¶é—´çš„æ¨ç§»å®ç°æœ€ä½³åˆ†æ”¯åˆ†é…ï¼Œè€Œä¸æ˜¯åå‘æœ€è¿‘çš„ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥ä»»åŠ¡é€‰æ‹©å™¨ï¼Œè‡ªåŠ¨å°†æµ‹è¯•è¾“å…¥è·¯ç”±åˆ°é€‚å½“çš„è·¯ç”±å™¨ï¼Œæ— éœ€ä»»åŠ¡èº«ä»½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBranchLoRAæ˜¾è‘—ä¼˜äºMoELoRAï¼Œå¹¶åœ¨å„ç§MLLMè§„æ¨¡ä¸Šä¿æŒå…¶ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ç°æœ‰çš„MoELoRAæ¡†æ¶é€šè¿‡ç®€å•æ±‚å’ŒèšåˆLoRAå—ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶é—å¿˜æ—§ä»»åŠ¡çš„ä¿¡æ¯ï¼Œå½±å“é•¿æœŸæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºBranchLoRAæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥çµæ´»çš„è°ƒä¼˜-å†»ç»“æœºåˆ¶ï¼Œä½¿å¾—ä¸åŒåˆ†æ”¯èƒ½å¤Ÿä¸“æ³¨äºç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ƒè¿›ä»»åŠ¡é—´çš„åä½œï¼Œä»è€Œå‡è½»ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBranchLoRAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªåˆ†æ”¯ï¼Œæ¯ä¸ªåˆ†æ”¯ä¸“æ³¨äºç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ä»»åŠ¡ç‰¹å®šè·¯ç”±å™¨è¿›è¡ŒåŠ¨æ€è·¯ç”±ã€‚ä»»åŠ¡é€‰æ‹©å™¨è´Ÿè´£åœ¨æ¨ç†æ—¶è‡ªåŠ¨å°†è¾“å…¥è·¯ç”±åˆ°ç›¸åº”çš„åˆ†æ”¯ï¼Œæ— éœ€æ˜¾å¼çš„ä»»åŠ¡èº«ä»½ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šBranchLoRAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä¸å¯¹ç§°çš„æ¡†æ¶è®¾è®¡å’Œçµæ´»çš„è°ƒä¼˜-å†»ç»“æœºåˆ¶ï¼Œä¸ç°æœ‰çš„MoELoRAæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç®¡ç†ä»»åŠ¡é—´çš„çŸ¥è¯†å…±äº«ä¸ä¿ç•™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒBranchLoRAå¼•å…¥äº†ä»»åŠ¡ç‰¹å®šçš„è·¯ç”±å™¨ï¼Œä»¥ç¡®ä¿éšç€æ—¶é—´çš„æ¨ç§»å®ç°æœ€ä½³çš„åˆ†æ”¯åˆ†é…ã€‚æ­¤å¤–ï¼Œè°ƒä¼˜-å†»ç»“æœºåˆ¶çš„çµæ´»æ€§å…è®¸æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´è¿›è¡Œæœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ï¼Œå‡å°‘äº†ç¾éš¾æ€§é—å¿˜çš„é£é™©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æœ€æ–°çš„MCITåŸºå‡†ä¸Šï¼ŒBranchLoRAæ˜¾è‘—ä¼˜äºMoELoRAï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ä¸åŒè§„æ¨¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œæœ‰æ•ˆå‡å°‘äº†ç¾éš¾æ€§é—å¿˜ç°è±¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å®¢æœå’Œå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨è¿ç»­ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ»¡è¶³ç”¨æˆ·ä¸æ–­å˜åŒ–çš„éœ€æ±‚ã€‚æœªæ¥ï¼ŒBranchLoRAå¯èƒ½åœ¨æ›´å¤šå¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¤§çš„ä»·å€¼ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes.

