---
layout: default
title: Chain-of-Thought Training for Open E2E Spoken Dialogue Systems
---

# Chain-of-Thought Training for Open E2E Spoken Dialogue Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00722" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00722v1</a>
  <a href="https://arxiv.org/pdf/2506.00722.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00722v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00722v1', 'Chain-of-Thought Training for Open E2E Spoken Dialogue Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siddhant Arora, Jinchuan Tian, Hayato Futami, Jee-weon Jung, Jiatong Shi, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe

**åˆ†ç±»**: cs.CL, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: Accepted at INTERSPEECH 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé“¾å¼æ€ç»´è®­ç»ƒä»¥æå‡å¼€æ”¾å¼ç«¯åˆ°ç«¯å¯¹è¯ç³»ç»Ÿçš„è¯­ä¹‰ä¸€è‡´æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç«¯åˆ°ç«¯å¯¹è¯ç³»ç»Ÿ` `é“¾å¼æ€ç»´` `è¯­éŸ³è¯†åˆ«` `æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç«¯åˆ°ç«¯å¯¹è¯ç³»ç»Ÿåœ¨è¯­ä¹‰ä¸€è‡´æ€§å’Œè®­ç»ƒæ•°æ®éœ€æ±‚ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆçš„å“åº”è´¨é‡ä¸é«˜ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§é“¾å¼æ€ç»´è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å°†å¯¹è¯æ•°æ®è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒç›¸ç»“åˆï¼Œæå‡äº†å¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ROUGE-1æŒ‡æ ‡ä¸Šè¾ƒåŸºçº¿æå‡è¶…è¿‡1.5ï¼Œä¸”ä»…éœ€300å°æ—¶çš„è®­ç»ƒæ•°æ®å³å¯å®ç°æœ‰æ•ˆè®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ä¼ ç»Ÿçš„çº§è”ç®¡é“ä¸åŒï¼Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è¯­éŸ³å¯¹è¯ç³»ç»Ÿä¿æŒå®Œå…¨çš„å¯å¾®åˆ†æ€§ï¼Œå¹¶æ•æ‰ééŸ³ç´ ä¿¡æ¯ï¼Œä½¿å…¶æ›´é€‚åˆå»ºæ¨¡è¯­éŸ³äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„E2Eæ–¹æ³•é€šå¸¸éœ€è¦å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ï¼Œå¹¶ç”Ÿæˆç¼ºä¹è¯­ä¹‰ä¸€è‡´æ€§çš„å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥ï¼Œåˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å½¢å¼ï¼Œç¡®ä¿åœ¨å¯¹è¯æ•°æ®ä¸Šçš„è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰å’Œæ–‡æœ¬LMä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒç´§å¯†å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºçº¿ä¹‹ä¸Šå®ç°äº†è¶…è¿‡1.5çš„ROUGE-1æå‡ï¼ŒæˆåŠŸåœ¨å…¬å¼€çš„äººç±»å¯¹è¯æ•°æ®é›†ä¸Šè®­ç»ƒè¯­éŸ³å¯¹è¯ç³»ç»Ÿï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡è¶³å¤Ÿé«˜ï¼Œä»…éœ€300å°æ—¶çš„å…¬å¼€äººç±»å¯¹è¯æ•°æ®ï¼Œå¦‚Switchboardã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œè®­ç»ƒä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç«¯åˆ°ç«¯å¯¹è¯ç³»ç»Ÿåœ¨è¯­ä¹‰ä¸€è‡´æ€§å’Œè®­ç»ƒæ•°æ®éœ€æ±‚æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡æ•°æ®ä¸”ç”Ÿæˆçš„å“åº”ç¼ºä¹è¿è´¯æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å°†å¯¹è¯æ•°æ®çš„è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒç›¸ç»“åˆï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œä¿¡æ¯å®Œæ•´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€é“¾å¼æ€ç»´è®­ç»ƒæ¨¡å—å’Œæ¨¡å‹ä¼˜åŒ–é˜¶æ®µï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆå’Œæ–‡æœ¬è¯­è¨€æ¨¡å‹çš„é›†æˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥é“¾å¼æ€ç»´è®­ç»ƒï¼Œä½¿å¾—å¯¹è¯ç³»ç»Ÿçš„è®­ç»ƒè¿‡ç¨‹ä¸å¤šæ¨¡æ€é¢„è®­ç»ƒç´§å¯†ç»“åˆï¼Œä»è€Œæå‡äº†ç”Ÿæˆå“åº”çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨å¯¹è¯ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶ä½¿ç”¨äº†300å°æ—¶çš„å…¬å¼€äººç±»å¯¹è¯æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ROUGE-1æŒ‡æ ‡ä¸Šè¾ƒåŸºçº¿æå‡è¶…è¿‡1.5ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä»…éœ€300å°æ—¶çš„å…¬å¼€äººç±»å¯¹è¯æ•°æ®ï¼Œå±•ç°å‡ºè‰¯å¥½çš„è®¡ç®—æ•ˆç‡å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€è¯­éŸ³åŠ©æ‰‹å’Œäººæœºäº¤äº’ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¯¹è¯ç³»ç»Ÿçš„å“åº”è´¨é‡å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´è‡ªç„¶çš„è¯­éŸ³äº¤äº’æŠ€æœ¯çš„å‘å±•ï¼Œä¿ƒè¿›äººæœºæ²Ÿé€šçš„æ™ºèƒ½åŒ–å’Œäººæ€§åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue systems preserve full differentiability and capture non-phonemic information, making them well-suited for modeling spoken interactions. However, existing E2E approaches often require large-scale training data and generates responses lacking semantic coherence. We propose a simple yet effective strategy leveraging a chain-of-thought (CoT) formulation, ensuring that training on conversational data remains closely aligned with the multimodal language model (LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis (TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over the baseline, successfully training spoken dialogue systems on publicly available human-human conversation datasets, while being compute-efficient enough to train on just 300 hours of public human-human conversation data, such as the Switchboard. We will publicly release our models and training code.

