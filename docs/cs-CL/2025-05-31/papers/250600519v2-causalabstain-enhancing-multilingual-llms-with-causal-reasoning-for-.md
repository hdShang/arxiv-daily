---
layout: default
title: "CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention"
---

# CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00519" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00519v2</a>
  <a href="https://arxiv.org/pdf/2506.00519.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00519v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00519v2', 'CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxi Sun, Aoqi Zuo, Wei Gao, Jing Ma

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-06-03)

**å¤‡æ³¨**: Accepted to Association for Computational Linguistics Findings (ACL) 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/peachch/CausalAbstain)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCausalAbstainä»¥è§£å†³å¤šè¯­è¨€LLMçš„çŸ¥è¯†å·®å¼‚é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å› æœæ¨ç†` `å¤šè¯­è¨€æ¨¡å‹` `æ”¾å¼ƒå†³ç­–` `çŸ¥è¯†ç©ºç™½` `åé¦ˆé€‰æ‹©` `å¯è§£é‡Šæ€§` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šè¯­è¨€LLMçš„æ”¾å¼ƒç­–ç•¥ä¾èµ–ç”Ÿæˆåé¦ˆï¼Œå®¹æ˜“å—åˆ°åé¦ˆä¸å‡†ç¡®æ€§å’Œåè§çš„å½±å“ã€‚
2. CausalAbstainæ–¹æ³•é€šè¿‡å› æœæ¨ç†å¸®åŠ©LLMsåˆ¤æ–­åé¦ˆçš„æœ‰æ•ˆæ€§ï¼Œä»è€Œä¼˜åŒ–æ”¾å¼ƒå†³ç­–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCausalAbstainåœ¨é€‰æ‹©æœ‰ç”¨åé¦ˆå’Œå¢å¼ºå†³ç­–å¯è§£é‡Šæ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¤šä¸ªåŸºçº¿æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒè¯­è¨€é—´å¸¸å¸¸å­˜åœ¨çŸ¥è¯†å·®å¼‚ã€‚å½“é¢ä¸´çŸ¥è¯†ç©ºç™½æ—¶ï¼Œé¼“åŠ±LLMsé€‰æ‹©	extit{abstain}ï¼ˆæ”¾å¼ƒå›ç­”ï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆå‡å°‘å¤šè¯­è¨€ç¯å¢ƒä¸­å¹»è§‰ç°è±¡çš„ç­–ç•¥ã€‚ç°æœ‰çš„å¤šè¯­è¨€æ”¾å¼ƒç­–ç•¥ä¸»è¦ä¾èµ–äºç”Ÿæˆåé¦ˆå¹¶è¿›è¡Œè‡ªæˆ‘åæ€ï¼Œä½†è¿™äº›æ–¹æ³•å®¹æ˜“å—åˆ°ç”Ÿæˆåé¦ˆä¸­çš„ä¸å‡†ç¡®æ€§å’Œåè§çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»å› æœæ¨ç†çš„è§’åº¦æå‡ºäº†	extit{CausalAbstain}ï¼Œè¯¥æ–¹æ³•å¸®åŠ©LLMsåˆ¤æ–­æ˜¯å¦åˆ©ç”¨å¤šä¸ªç”Ÿæˆçš„åé¦ˆå“åº”ï¼Œå¹¶è¯†åˆ«æœ€æœ‰ç”¨çš„åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ	extit{CausalAbstain}åœ¨æœ¬åœ°è¯­è¨€å’Œå¤šè¯­è¨€ç¯å¢ƒä¸­æœ‰æ•ˆé€‰æ‹©æœ‰ç”¨åé¦ˆï¼Œå¹¶å¢å¼ºæ”¾å¼ƒå†³ç­–çš„å¯è§£é‡Šæ€§ï¼Œåœ¨ä¸¤ä¸ªæ¶µç›–ç™¾ç§‘å’Œå¸¸è¯†çŸ¥è¯†é—®ç­”ä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†å¼ºåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€LLMåœ¨çŸ¥è¯†ç©ºç™½æƒ…å†µä¸‹çš„æ”¾å¼ƒå†³ç­–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–ç”Ÿæˆåé¦ˆï¼Œå®¹æ˜“å—åˆ°åé¦ˆè´¨é‡çš„å½±å“ï¼Œå¯¼è‡´å†³ç­–ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCausalAbstainé€šè¿‡å› æœæ¨ç†æ¡†æ¶ï¼Œå¸®åŠ©LLMsè¯„ä¼°å¤šä¸ªç”Ÿæˆåé¦ˆçš„æœ‰æ•ˆæ€§ï¼Œä»è€Œé€‰æ‹©æœ€æœ‰ç”¨çš„åé¦ˆè¿›è¡Œå†³ç­–ã€‚è¯¥è®¾è®¡æ—¨åœ¨æé«˜æ”¾å¼ƒå†³ç­–çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCausalAbstainçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åé¦ˆç”Ÿæˆæ¨¡å—ã€å› æœè¯„ä¼°æ¨¡å—å’Œå†³ç­–æ¨¡å—ã€‚åé¦ˆç”Ÿæˆæ¨¡å—è´Ÿè´£ç”Ÿæˆå¤šç§è¯­è¨€çš„åé¦ˆï¼Œå› æœè¯„ä¼°æ¨¡å—è¯„ä¼°è¿™äº›åé¦ˆçš„æœ‰æ•ˆæ€§ï¼Œå†³ç­–æ¨¡å—æ ¹æ®è¯„ä¼°ç»“æœåšå‡ºæ”¾å¼ƒæˆ–å›ç­”çš„å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šCausalAbstainçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥å› æœæ¨ç†æ¥è¯„ä¼°åé¦ˆçš„æœ‰æ•ˆæ€§ï¼Œè¿™ä¸ä¼ ç»Ÿä¾èµ–è‡ªæˆ‘åæ€çš„æ”¾å¼ƒç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒCausalAbstainä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åé¦ˆé€‰æ‹©è¿‡ç¨‹ï¼Œå¹¶ç»“åˆå¤šè¯­è¨€æ¨¡å‹çš„ç‰¹æ€§è¿›è¡Œå‚æ•°è°ƒä¼˜ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCausalAbstainåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ”¾å¼ƒå†³ç­–çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼Œä¸”åœ¨å¯è§£é‡Šæ€§æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†æ›´æ¸…æ™°çš„å†³ç­–ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CausalAbstainçš„ç ”ç©¶æˆæœåœ¨å¤šè¯­è¨€é—®ç­”ç³»ç»Ÿã€è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢å’Œå¤šè¯­è¨€å¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜LLMsåœ¨çŸ¥è¯†ç©ºç™½æƒ…å†µä¸‹çš„å†³ç­–èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„å¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å¤šè¯­è¨€äº¤äº’ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce \textit{CausalAbstain}, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that \textit{CausalAbstain} effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks. Our code and data are open-sourced at https://github.com/peachch/CausalAbstain.

