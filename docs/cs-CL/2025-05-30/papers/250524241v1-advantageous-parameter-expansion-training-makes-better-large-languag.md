---
layout: default
title: Advantageous Parameter Expansion Training Makes Better Large Language Models
---

# Advantageous Parameter Expansion Training Makes Better Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24241" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24241v1</a>
  <a href="https://arxiv.org/pdf/2505.24241.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24241v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24241v1', 'Advantageous Parameter Expansion Training Makes Better Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Naibin Gu, Yilong Chen, Zhenyu Zhang, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¼˜åŠ¿å‚æ•°æ‰©å±•è®­ç»ƒä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å‚æ•°æ‰©å±•` `è®­ç»ƒæ•ˆç‡` `æŒ‡ä»¤è°ƒä¼˜` `ç»§ç»­é¢„è®­ç»ƒ` `æ€§èƒ½æå‡` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¢åŠ å¯è®­ç»ƒå‚æ•°æ•°é‡æ—¶ï¼Œè™½ç„¶å¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œä½†è®¡ç®—å¼€é”€æ˜¾è‘—å¢åŠ ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„APEXæ–¹æ³•é€šè¿‡é€æ­¥æ‰©å±•ä¼˜åŠ¿å‚æ•°ï¼Œæå‡å…¶åœ¨æ¨¡å‹ä¸­çš„æ¯”ä¾‹ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAPEXåœ¨æŒ‡ä»¤è°ƒä¼˜ä¸­ä½¿ç”¨52%å‚æ•°è¶…è¶Šå…¨å‚æ•°è°ƒä¼˜ï¼Œåœ¨ç»§ç»­é¢„è®­ç»ƒä¸­ä»…ç”¨33%æ•°æ®è¾¾åˆ°ç›¸åŒå›°æƒ‘åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒä¸­å¢åŠ å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å¯ä»¥æœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†ä¹Ÿä¼šå¯¼è‡´è®¡ç®—å¼€é”€å¢åŠ ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºä¼˜åŠ¿å‚æ•°æ‰©å±•è®­ç»ƒï¼ˆAPEXï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€æ­¥å°†ä¼˜åŠ¿å‚æ•°æ‰©å±•åˆ°åŠ£åŠ¿å‚æ•°çš„ç©ºé—´ï¼Œä»è€Œæé«˜å…¶æ¯”ä¾‹å¹¶å¢å¼ºè®­ç»ƒæ•ˆæœã€‚ç†è®ºåˆ†æä»çŸ©é˜µæœ‰æ•ˆç§©çš„è§’åº¦è§£é‡Šäº†APEXçš„æ€§èƒ½æå‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æŒ‡ä»¤è°ƒä¼˜ä¸­ï¼ŒAPEXåœ¨ä»…ä½¿ç”¨52%çš„å¯è®­ç»ƒå‚æ•°çš„æƒ…å†µä¸‹è¶…è¶Šäº†å…¨å‚æ•°è°ƒä¼˜ï¼›åœ¨ç»§ç»­é¢„è®­ç»ƒä¸­ï¼ŒAPEXåœ¨ä»…ä½¿ç”¨33%çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¾¾åˆ°äº†ä¸ä¼ ç»Ÿè®­ç»ƒç›¸åŒçš„å›°æƒ‘åº¦ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ï¼Œå¢åŠ å¯è®­ç»ƒå‚æ•°æ•°é‡å¯¼è‡´çš„è®¡ç®—å¼€é”€å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨å‚æ•°é—´çš„å·®å¼‚ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸å°½å¦‚äººæ„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAPEXæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯è¯†åˆ«å¹¶æ‰©å±•æ¨¡å‹ä¸­çš„ä¼˜åŠ¿å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒä¸­å æ®æ›´å¤§æ¯”ä¾‹ï¼Œä»è€Œæå‡æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å‡å°‘å‚æ•°æ•°é‡çš„åŒæ—¶ï¼Œä¿æŒæˆ–æå‡æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAPEXçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¯†åˆ«ä¼˜åŠ¿å‚æ•°ï¼Œå…¶æ¬¡å°†è¿™äº›å‚æ•°é€æ­¥æ‰©å±•åˆ°åŠ£åŠ¿å‚æ•°çš„ç©ºé—´ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´å‚æ•°çš„ä½¿ç”¨æ¯”ä¾‹ï¼Œä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šAPEXçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¯¹ä¼˜åŠ¿å‚æ•°çš„è¯†åˆ«å’Œæ‰©å±•æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å…¨å‚æ•°è°ƒä¼˜æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚é€šè¿‡èšç„¦äºå¯¹æ€§èƒ½å½±å“æœ€å¤§çš„å‚æ•°ï¼ŒAPEXèƒ½å¤Ÿåœ¨è¾ƒå°‘çš„è®¡ç®—èµ„æºä¸‹å®ç°æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨APEXä¸­ï¼Œå…³é”®çš„å‚æ•°è®¾ç½®åŒ…æ‹¬ä¼˜åŠ¿å‚æ•°çš„è¯†åˆ«æ ‡å‡†å’Œæ‰©å±•ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å‚æ•°çš„æœ‰æ•ˆæ€§ï¼Œä»¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAPEXåœ¨æŒ‡ä»¤è°ƒä¼˜ä¸­ä»…ä½¿ç”¨52%çš„å¯è®­ç»ƒå‚æ•°ï¼Œä¾¿è¶…è¶Šäº†å…¨å‚æ•°è°ƒä¼˜çš„æ•ˆæœï¼›åœ¨ç»§ç»­é¢„è®­ç»ƒä¸­ï¼ŒAPEXä½¿ç”¨33%çš„è®­ç»ƒæ•°æ®è¾¾åˆ°äº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸åŒçš„å›°æƒ‘åº¦ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼ŒAPEXæ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆç­‰æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although scaling up the number of trainable parameters in both pre-training and fine-tuning can effectively improve the performance of large language models, it also leads to increased computational overhead. When delving into the parameter difference, we find that a subset of parameters, termed advantageous parameters, plays a crucial role in determining model performance. Further analysis reveals that stronger models tend to possess more such parameters. In this paper, we propose Advantageous Parameter EXpansion Training (APEX), a method that progressively expands advantageous parameters into the space of disadvantageous ones, thereby increasing their proportion and enhancing training effectiveness. Further theoretical analysis from the perspective of matrix effective rank explains the performance gains of APEX. Extensive experiments on both instruction tuning and continued pre-training demonstrate that, in instruction tuning, APEX outperforms full-parameter tuning while using only 52% of the trainable parameters. In continued pre-training, APEX achieves the same perplexity level as conventional training with just 33% of the training data, and yields significant improvements on downstream tasks.

