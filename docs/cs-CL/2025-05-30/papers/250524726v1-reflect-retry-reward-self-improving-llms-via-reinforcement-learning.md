---
layout: default
title: Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning
---

# Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24726" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24726v1</a>
  <a href="https://arxiv.org/pdf/2505.24726.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24726v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24726v1', 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, Waseem AlShikh

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæˆ‘åæ€ä¸å¼ºåŒ–å­¦ä¹ ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘åæ€` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ€§èƒ½æå‡` `ä»»åŠ¡è§£å†³` `æ¨¡å‹å¾®è°ƒ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œå¾€å¾€ç¼ºä¹è‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œå¯¼è‡´é”™è¯¯éš¾ä»¥çº æ­£ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡è‡ªæˆ‘åæ€å’Œå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä¿ƒä½¿æ¨¡å‹åœ¨é”™è¯¯åè¿›è¡Œè‡ªæˆ‘åˆ†æå¹¶æ”¹è¿›è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ–¹ç¨‹å†™ä½œå’Œå‡½æ•°è°ƒç”¨ç­‰ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯å°å‹æ¨¡å‹è¡¨ç°ä¼˜äºå¤§å‹æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†ä¸€ç§é€šè¿‡è‡ªæˆ‘åæ€å’Œå¼ºåŒ–å­¦ä¹ æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚é€šè¿‡æ¿€åŠ±æ¨¡å‹åœ¨å›ç­”é”™è¯¯æ—¶ç”Ÿæˆæ›´å¥½çš„è‡ªæˆ‘åæ€ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å³ä½¿åœ¨ç”Ÿæˆåˆæˆæ•°æ®ä¸å¯è¡Œä¸”ä»…æœ‰äºŒå…ƒåé¦ˆçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹è§£å†³å¤æ‚å¯éªŒè¯ä»»åŠ¡çš„èƒ½åŠ›ä¹Ÿèƒ½å¾—åˆ°å¢å¼ºã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œåœ¨ä»»åŠ¡å¤±è´¥åï¼Œæ¨¡å‹ç”Ÿæˆè‡ªæˆ‘åæ€çš„è¯„è®ºï¼Œåˆ†æå…¶å…ˆå‰çš„å°è¯•ï¼›å…¶æ¬¡ï¼Œæ¨¡å‹åœ¨è‡ªæˆ‘åæ€çš„ä¸Šä¸‹æ–‡ä¸­å†æ¬¡å°è¯•è¯¥ä»»åŠ¡ã€‚å¦‚æœåç»­å°è¯•æˆåŠŸï¼Œåˆ™åœ¨è‡ªæˆ‘åæ€é˜¶æ®µç”Ÿæˆçš„æ ‡è®°ä¼šè·å¾—å¥–åŠ±ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸­ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ï¼Œæ•°å­¦æ–¹ç¨‹å†™ä½œæé«˜äº†34.7%ï¼Œå‡½æ•°è°ƒç”¨æé«˜äº†18.1%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¾ƒå°çš„å¾®è°ƒæ¨¡å‹ï¼ˆ15äº¿åˆ°70äº¿å‚æ•°ï¼‰åœ¨åŒä¸€ç³»åˆ—ä¸­è¶…è¶Šäº†å¤§10å€çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼ä¸ºåœ¨æœ‰é™å¤–éƒ¨åé¦ˆä¸‹è‡ªæˆ‘æ”¹è¿›çš„è¯­è¨€æ¨¡å‹æä¾›äº†ä»¤äººå…´å¥‹çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å¤æ‚ä»»åŠ¡æ—¶çš„è‡ªæˆ‘çº é”™èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡çš„æ ‡æ³¨æ•°æ®å’Œå¤–éƒ¨åé¦ˆï¼Œè€Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™äº›æ¡ä»¶å¹¶ä¸æ»¡è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªæˆ‘åæ€æœºåˆ¶ï¼Œæ¿€åŠ±æ¨¡å‹åœ¨é”™è¯¯åç”Ÿæˆåˆ†æè¯„è®ºï¼Œä»è€Œåœ¨åç»­å°è¯•ä¸­åˆ©ç”¨è¿™äº›åæ€ä¿¡æ¯è¿›è¡Œæ”¹è¿›ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿåœ¨ç¼ºä¹ä¸°å¯Œåé¦ˆçš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯æ¨¡å‹åœ¨ä»»åŠ¡å¤±è´¥åç”Ÿæˆè‡ªæˆ‘åæ€çš„è¯„è®ºï¼›ç¬¬äºŒé˜¶æ®µæ˜¯æ¨¡å‹åœ¨è‡ªæˆ‘åæ€çš„ä¸Šä¸‹æ–‡ä¸­é‡æ–°å°è¯•è¯¥ä»»åŠ¡ã€‚æˆåŠŸçš„å°è¯•å°†ä¼šå¥–åŠ±è‡ªæˆ‘åæ€é˜¶æ®µç”Ÿæˆçš„æ ‡è®°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºå°†è‡ªæˆ‘åæ€ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å­¦ä¹ æœºåˆ¶ã€‚è¿™ç§æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„åé¦ˆæ¡ä»¶ä¸‹è¿›è¡Œè‡ªæˆ‘æ”¹è¿›ï¼Œæ˜¾è‘—æå‡äº†ä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æœºåˆ¶æ¥æ¿€åŠ±è‡ªæˆ‘åæ€çš„ç”Ÿæˆï¼Œå…·ä½“çš„æŸå¤±å‡½æ•°å’Œå¥–åŠ±ç­–ç•¥åœ¨å®éªŒä¸­ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å’Œæ”¹è¿›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ–¹ç¨‹å†™ä½œä»»åŠ¡ä¸Šæå‡äº†34.7%çš„æ€§èƒ½ï¼Œåœ¨å‡½æ•°è°ƒç”¨ä»»åŠ¡ä¸Šæå‡äº†18.1%ã€‚å°¤å…¶æ˜¯è¾ƒå°çš„å¾®è°ƒæ¨¡å‹ï¼ˆ15äº¿åˆ°70äº¿å‚æ•°ï¼‰åœ¨åŒä¸€ç³»åˆ—ä¸­è¡¨ç°ä¼˜äºå¤§10å€çš„æ¨¡å‹ï¼Œå±•ç°äº†è‡ªæˆ‘åæ€æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€å®¢æœã€ç¼–ç¨‹è¾…åŠ©ç­‰å¤šä¸ªé¢†åŸŸã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ï¼Œå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æ›´å¥½åœ°å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œæé«˜ç”¨æˆ·ä½“éªŒå’Œå·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å·¥å…·çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.

