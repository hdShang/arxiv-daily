---
layout: default
title: Lossless Token Sequence Compression via Meta-Tokens
---

# Lossless Token Sequence Compression via Meta-Tokens

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00307" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00307v2</a>
  <a href="https://arxiv.org/pdf/2506.00307.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00307v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00307v2', 'Lossless Token Sequence Compression via Meta-Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: John Harvill, Ziwei Fan, Hao Wang, Luke Huan, Anoop Deoras, Yizhou Sun, Hao Ding

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-08-20)

**å¤‡æ³¨**: 16 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ— æŸä»¤ç‰Œåºåˆ—å‹ç¼©æ–¹æ³•ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— æŸå‹ç¼©` `å¤§è¯­è¨€æ¨¡å‹` `åºåˆ—å¤„ç†` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æ•ˆç‡` `è¯­ä¹‰ä¿ç•™` `LZ77ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æç¤ºå‹ç¼©æ–¹æ³•ä¸»è¦é‡‡ç”¨æœ‰æŸæŠ€æœ¯ï¼Œéš¾ä»¥åœ¨ä¿ç•™è¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶æœ‰æ•ˆå‡å°‘åºåˆ—é•¿åº¦ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— æŸå‹ç¼©æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸æŸå¤±è¯­ä¹‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘è¾“å…¥ä»¤ç‰Œåºåˆ—çš„é•¿åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†27%å’Œ18%çš„åºåˆ—é•¿åº¦å‡å°‘ï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç¤ºå‹ç¼©ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æœ‰æŸæ–¹æ³•ä¸Šï¼Œæ—¨åœ¨æœ€å¤§é™åº¦åœ°ä¿ç•™ä¸ä¸‹æ¸¸ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘åºåˆ—é•¿åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç±»ä¼¼äºLZ77çš„ä»»åŠ¡æ— å…³æ— æŸå‹ç¼©æŠ€æœ¯ï¼Œä½¿å¾—è¾“å…¥ä»¤ç‰Œåºåˆ—é•¿åº¦å¹³å‡å‡å°‘27%å’Œ18%ã€‚åœ¨ä½¿ç”¨åŸºäºå˜æ¢å™¨çš„LLMæ—¶ï¼Œè¿™åˆ†åˆ«å¯¹åº”äº47%å’Œ33%çš„ç¼–ç è®¡ç®—å‡å°‘ã€‚è¯¥ä»¤ç‰Œåºåˆ—è½¬æ¢è¿‡ç¨‹æ˜“äºé€†è½¬ï¼Œç¡®ä¿æ²¡æœ‰è¯­ä¹‰ä¿¡æ¯ä¸¢å¤±ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªéœ€è¦ä¸¥æ ¼ä¿ç•™è¯­ä¹‰å’Œè¯­æ³•çš„ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ç°æœ‰çš„æœ‰æŸå‹ç¼©æ–¹æ³•åœ¨æ­¤è®¾ç½®ä¸‹è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬çš„æ— æŸå‹ç¼©æŠ€æœ¯ä¸æœªå‹ç¼©è¾“å…¥ç›¸æ¯”ï¼Œæ€§èƒ½å·®è·å¾®å°ï¼Œæ¨æµ‹æ›´å¤§çš„æ¨¡å‹å’Œæ‰©å±•çš„è®¡ç®—é¢„ç®—å¯èƒ½ä¼šå®Œå…¨æ¶ˆé™¤è¿™ä¸€å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœ‰æŸå‹ç¼©æ–¹æ³•åœ¨ä¿ç•™è¯­ä¹‰ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¸¥æ ¼ä¿ç•™è¯­ä¹‰å’Œè¯­æ³•çš„ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ— æŸå‹ç¼©æŠ€æœ¯ï¼Œç±»ä¼¼äºLZ77ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å‹ç¼©è¿‡ç¨‹ä¸­ä¸ä¸¢å¤±ä»»ä½•è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„è¾“å…¥ä»¤ç‰Œåºåˆ—å¤„ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥ä»¤ç‰Œåºåˆ—çš„å‹ç¼©æ¨¡å—å’Œè§£å‹æ¨¡å—ï¼Œå‹ç¼©æ¨¡å—è´Ÿè´£å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºå‹ç¼©æ ¼å¼ï¼Œè§£å‹æ¨¡å—åˆ™ç¡®ä¿å¯ä»¥è½»æ¾æ¢å¤åŸå§‹åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ— æŸå‹ç¼©æ–¹æ³•ï¼Œæ˜¾è‘—åŒºåˆ«äºç°æœ‰çš„æœ‰æŸæ–¹æ³•ï¼Œç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­ä¸æŸå¤±ä»»ä½•è¯­ä¹‰ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç±»ä¼¼LZ77çš„ç®—æ³•ç»“æ„ï¼Œç¡®ä¿å‹ç¼©å’Œè§£å‹è¿‡ç¨‹çš„é«˜æ•ˆæ€§ï¼ŒåŒæ—¶åœ¨å‚æ•°è®¾ç½®ä¸Šä¼˜åŒ–äº†ç¼–ç è®¡ç®—ï¼Œå‡å°‘äº†è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ— æŸå‹ç¼©æ–¹æ³•åœ¨ä¸¤ä¸ªè¯„ä¼°ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†27%å’Œ18%çš„åºåˆ—é•¿åº¦å‡å°‘ï¼Œè®¡ç®—æ•ˆç‡æå‡è¾¾åˆ°47%å’Œ33%ã€‚ä¸æœªå‹ç¼©è¾“å…¥ç›¸æ¯”ï¼Œæ€§èƒ½å·®è·å¾®å°ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§è¯­è¨€æ¨¡å‹çš„è®¡ç®—æ•ˆç‡å’Œå“åº”é€Ÿåº¦ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§å’Œè®¡ç®—èµ„æºçš„å¢åŠ ï¼Œè¯¥æ–¹æ³•çš„åº”ç”¨ä»·å€¼å°†æ›´åŠ å‡¸æ˜¾ï¼Œå¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„AIç³»ç»Ÿçš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.

