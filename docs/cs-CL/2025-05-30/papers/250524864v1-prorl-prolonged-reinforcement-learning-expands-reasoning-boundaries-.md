---
layout: default
title: ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models
---

# ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24864" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24864v1</a>
  <a href="https://arxiv.org/pdf/2505.24864.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24864v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24864v1', 'ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: 26 pages, 17 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProRLä»¥æ‰©å±•å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `è¯­è¨€æ¨¡å‹` `KLæ•£åº¦` `ä»»åŠ¡å¤šæ ·æ€§` `æ¨¡å‹è®­ç»ƒ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯å¦çœŸæ­£æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»å­˜åœ¨äº‰è®®ï¼Œä¸”åŸºç¡€æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºProRLï¼Œé€šè¿‡KLæ•£åº¦æ§åˆ¶ã€å‚è€ƒç­–ç•¥é‡ç½®å’Œå¤šæ ·åŒ–ä»»åŠ¡ç»„åˆï¼Œæ¢ç´¢æ–°çš„æ¨ç†ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šé¡¹è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œå°¤å…¶åœ¨åŸºç¡€æ¨¡å‹å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ¨ç†ä¸­å¿ƒçš„è¯­è¨€æ¨¡å‹è¿›å±•è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œç”¨äºå°†æ¨¡å‹ä¸å¯éªŒè¯çš„å¥–åŠ±å¯¹é½ã€‚ç„¶è€Œï¼ŒRLæ˜¯å¦çœŸæ­£æ‰©å±•äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ–ä»…ä»…æ”¾å¤§äº†åŸºç¡€æ¨¡å‹åˆ†å¸ƒä¸­æ½œåœ¨çš„é«˜å¥–åŠ±è¾“å‡ºï¼Œä»å­˜åœ¨äº‰è®®ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥ProRLï¼Œä¸€ç§æ–°é¢–çš„è®­ç»ƒæ–¹æ³•ï¼Œå±•ç¤ºäº†å»¶é•¿çš„RLè®­ç»ƒèƒ½å¤Ÿå‘ç°åŸºç¡€æ¨¡å‹æ— æ³•è®¿é—®çš„æ–°æ¨ç†ç­–ç•¥ã€‚å®è¯åˆ†æè¡¨æ˜ï¼ŒRLè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šç§è¯„ä¼°ä¸­å§‹ç»ˆä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨åŸºç¡€æ¨¡å‹å®Œå…¨å¤±è´¥çš„æƒ…å†µä¸‹ã€‚ç ”ç©¶ç»“æœä¸ºRLåœ¨è¯­è¨€æ¨¡å‹ä¸­æœ‰æ„ä¹‰åœ°æ‰©å±•æ¨ç†è¾¹ç•Œçš„æ¡ä»¶æä¾›äº†æ–°è§è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„é•¿æ—¶é—´RLæ¨ç†å·¥ä½œå¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ‰©å±•è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„äº‰è®®ï¼Œå°¤å…¶æ˜¯å…¶æ˜¯å¦èƒ½å‘ç°æ–°çš„æ¨ç†ç­–ç•¥ã€‚ç°æœ‰æ–¹æ³•åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ¨¡å‹çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºProRLè®­ç»ƒæ–¹æ³•ï¼Œå¼ºè°ƒé€šè¿‡å»¶é•¿RLè®­ç»ƒæ—¶é—´å’Œå¤šæ ·åŒ–ä»»åŠ¡ç»„åˆï¼Œèƒ½å¤Ÿæ­ç¤ºåŸºç¡€æ¨¡å‹æœªèƒ½å‘ç°çš„æ–°æ¨ç†ç­–ç•¥ã€‚æ­¤è®¾è®¡æ—¨åœ¨çªç ´åŸºç¡€æ¨¡å‹çš„æ¨ç†è¾¹ç•Œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šProRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šKLæ•£åº¦æ§åˆ¶ç”¨äºå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå‚è€ƒç­–ç•¥é‡ç½®ä»¥é¿å…æ¨¡å‹é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œä»¥åŠå¤šæ ·åŒ–ä»»åŠ¡ç»„åˆä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šProRLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è®­ç»ƒæ–¹æ³•çš„è®¾è®¡ï¼Œé€šè¿‡å»¶é•¿è®­ç»ƒæ—¶é—´å’Œå¼•å…¥å¤šæ ·åŒ–ä»»åŠ¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‘ç°æ–°çš„æ¨ç†ç­–ç•¥ï¼Œä¸ä¼ ç»ŸRLæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ProRLä¸­ï¼ŒKLæ•£åº¦æ§åˆ¶ç”¨äºç¡®ä¿æ¨¡å‹åœ¨æ¢ç´¢æ–°ç­–ç•¥æ—¶ä¸åç¦»å·²æœ‰çš„æœ‰æ•ˆç­–ç•¥ï¼Œå‚è€ƒç­–ç•¥é‡ç½®åˆ™ç”¨äºå®šæœŸæ›´æ–°æ¨¡å‹çš„å­¦ä¹ ç›®æ ‡ï¼Œç¡®ä¿å…¶é€‚åº”ä¸åŒçš„ä»»åŠ¡åœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRLè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šé¡¹pass@kè¯„ä¼°ä¸­è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œå°¤å…¶åœ¨åŸºç¡€æ¨¡å‹å®Œå…¨å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œæå‡å¹…åº¦æ˜¾è‘—ï¼ŒéªŒè¯äº†ProRLçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒProRLå¯ä»¥åœ¨å¤æ‚ä»»åŠ¡ä¸­æä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B

