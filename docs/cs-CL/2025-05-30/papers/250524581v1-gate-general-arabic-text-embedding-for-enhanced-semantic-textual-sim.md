---
layout: default
title: "GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training"
---

# GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24581" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24581v1</a>
  <a href="https://arxiv.org/pdf/2505.24581.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24581v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24581v1', 'GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Omer Nacar, Anis Koubaa, Serry Sibaee, Yasser Al-Habashi, Adel Ammar, Wadii Boulila

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGATEæ¨¡å‹ä»¥æå‡é˜¿æ‹‰ä¼¯è¯­è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é˜¿æ‹‰ä¼¯è¯­å¤„ç†` `è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è®­ç»ƒ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é˜¿æ‹‰ä¼¯è¯­çš„è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ç ”ç©¶å—é™äºé«˜è´¨é‡æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹çš„ç¼ºä¹ï¼Œå¯¼è‡´è¯„ä¼°å’Œè¿›å±•å—é˜»ã€‚
2. æœ¬æ–‡æå‡ºGATEæ¨¡å‹ï¼Œé€šè¿‡Matryoshkaè¡¨ç¤ºå­¦ä¹ å’Œæ··åˆæŸå¤±è®­ç»ƒï¼Œåˆ©ç”¨é˜¿æ‹‰ä¼¯ä¸‰å…ƒç»„æ•°æ®é›†æ¥æå‡è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚
3. GATEæ¨¡å‹åœ¨STSåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒå¤§å‹æ¨¡å‹æå‡20-25%ï¼Œæœ‰æ•ˆæ•æ‰é˜¿æ‹‰ä¼¯è¯­çš„è¯­ä¹‰ç‰¹å¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆSTSï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œèƒ½å¤Ÿåº”ç”¨äºæ£€ç´¢ã€èšç±»å’Œç†è§£æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œé˜¿æ‹‰ä¼¯è¯­é¢†åŸŸçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†é€šç”¨é˜¿æ‹‰ä¼¯æ–‡æœ¬åµŒå…¥ï¼ˆGATEï¼‰æ¨¡å‹ï¼Œåœ¨MTEBåŸºå‡†ä¸Šå®ç°äº†STSä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚GATEåˆ©ç”¨Matryoshkaè¡¨ç¤ºå­¦ä¹ å’Œæ··åˆæŸå¤±è®­ç»ƒæ–¹æ³•ï¼Œç»“åˆé˜¿æ‹‰ä¼¯ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨éœ€è¦ç»†ç²’åº¦è¯­ä¹‰ç†è§£çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚GATEåœ¨STSåŸºå‡†ä¸Šè¶…è¶Šäº†åŒ…æ‹¬OpenAIåœ¨å†…çš„æ›´å¤§æ¨¡å‹ï¼Œæ€§èƒ½æå‡è¾¾20-25%ï¼Œæœ‰æ•ˆæ•æ‰äº†é˜¿æ‹‰ä¼¯è¯­çš„ç‹¬ç‰¹è¯­ä¹‰ç»†å¾®å·®åˆ«ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é˜¿æ‹‰ä¼¯è¯­è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ä»»åŠ¡ä¸­ï¼Œç”±äºç¼ºä¹é«˜è´¨é‡æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹è€Œå¯¼è‡´çš„ç ”ç©¶ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é˜¿æ‹‰ä¼¯è¯­çš„è¯­ä¹‰ç†è§£ä¸Šè¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGATEæ¨¡å‹é€šè¿‡ç»“åˆMatryoshkaè¡¨ç¤ºå­¦ä¹ å’Œæ··åˆæŸå¤±è®­ç»ƒï¼Œåˆ©ç”¨é˜¿æ‹‰ä¼¯ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨ç»†ç²’åº¦è¯­ä¹‰ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰é˜¿æ‹‰ä¼¯è¯­çš„ç‹¬ç‰¹è¯­ä¹‰ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGATEæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€Matryoshkaè¡¨ç¤ºå­¦ä¹ æ¨¡å—å’Œæ··åˆæŸå¤±è®­ç»ƒæ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µè´Ÿè´£æ„å»ºé˜¿æ‹‰ä¼¯ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œè¡¨ç¤ºå­¦ä¹ æ¨¡å—åˆ™é€šè¿‡æ·±åº¦å­¦ä¹ ç½‘ç»œæå–æ–‡æœ¬çš„è¯­ä¹‰ç‰¹å¾ï¼Œæœ€åé€šè¿‡æ··åˆæŸå¤±å‡½æ•°è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šGATEæ¨¡å‹çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç‹¬ç‰¹çš„Matryoshkaè¡¨ç¤ºå­¦ä¹ æ–¹æ³•å’Œæ··åˆæŸå¤±è®­ç»ƒç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰é˜¿æ‹‰ä¼¯è¯­çš„è¯­ä¹‰ç»†å¾®å·®åˆ«ï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä¸åŒä»»åŠ¡çš„è®­ç»ƒç›®æ ‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”é˜¿æ‹‰ä¼¯è¯­çš„è¯­è¨€ç‰¹æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†é˜¿æ‹‰ä¼¯æ–‡æœ¬æ—¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GATEæ¨¡å‹åœ¨STSåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŒ…æ‹¬OpenAIåœ¨å†…çš„æ›´å¤§æ¨¡å‹ï¼Œæ€§èƒ½æå‡è¾¾20-25%ã€‚è¿™ä¸€æ˜¾è‘—çš„æå‡è¡¨æ˜GATEèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é˜¿æ‹‰ä¼¯è¯­çš„ç‹¬ç‰¹è¯­ä¹‰ç‰¹å¾ï¼Œä¸ºé˜¿æ‹‰ä¼¯è¯­çš„è¯­ä¹‰ç†è§£æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é˜¿æ‹‰ä¼¯è¯­çš„ä¿¡æ¯æ£€ç´¢ã€æ–‡æœ¬èšç±»å’Œè¯­ä¹‰ç†è§£ç­‰ä»»åŠ¡ã€‚é€šè¿‡æå‡é˜¿æ‹‰ä¼¯è¯­çš„è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ŒGATEæ¨¡å‹èƒ½å¤Ÿä¸ºç›¸å…³åº”ç”¨æä¾›æ›´å‡†ç¡®çš„ç»“æœï¼Œæ¨åŠ¨é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.

