---
layout: default
title: Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models
---

# Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24187" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24187v1</a>
  <a href="https://arxiv.org/pdf/2505.24187.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24187v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24187v1', 'Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mikhail L. Arbuzov, Alexey A. Shvets, Sisong Beir

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–°æ¡†æ¶ä»¥é‡å¡‘å¤§å‹è¯­è¨€æ¨¡å‹çš„é”™è¯¯ç´¯ç§¯ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é”™è¯¯ç´¯ç§¯` `å…³é”®æ ‡è®°` `å†³ç­–èŠ‚ç‚¹` `è‡ªç„¶è¯­è¨€å¤„ç†` `é•¿æ–‡æœ¬ç”Ÿæˆ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å‡è®¾å¤§å‹è¯­è¨€æ¨¡å‹çš„é”™è¯¯æ¦‚ç‡å‘ˆæŒ‡æ•°è¡°å‡ï¼Œé™åˆ¶äº†å…¶åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­çš„å¯é æ€§ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡è¯†åˆ«å…³é”®æ ‡è®°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå¼ºè°ƒåœ¨å†³ç­–èŠ‚ç‚¹çš„å‡†ç¡®æ€§æ¯”æ•´ä½“æ ‡è®°å‡†ç¡®æ€§æ›´ä¸ºé‡è¦ã€‚
3. ç ”ç©¶è¡¨æ˜æ–°æ¡†æ¶èƒ½æ˜¾è‘—æå‡é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆçš„è¿è´¯æ€§ï¼Œè¶…è¶Šä¼ ç»Ÿçš„è®¡ç®—æ‰©å±•æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯é æ€§éšåºåˆ—é•¿åº¦å‘ˆæŒ‡æ•°è¡°å‡çš„å‡è®¾ï¼ŒåŸºäºæ¯ä¸ªæ ‡è®°ç‹¬ç«‹çš„é”™è¯¯æ¦‚ç‡ï¼Œé™åˆ¶äº†é•¿è‡ªå›å½’è¾“å‡ºçš„æ€§èƒ½ã€‚æœ¬æ–‡æŒ‘æˆ˜è¿™ä¸€è§‚ç‚¹ï¼ŒæŒ‡å‡ºLLMé”™è¯¯å¹¶éå‡åŒ€åˆ†å¸ƒï¼Œè€Œæ˜¯é›†ä¸­åœ¨å°‘æ•°å…³é”®æ ‡è®°ä¸Šï¼ˆå æ€»æ ‡è®°çš„5-10%ï¼‰ï¼Œè¿™äº›æ ‡è®°ä»£è¡¨äº†é‡è¦çš„å†³ç­–èŠ‚ç‚¹ã€‚é€šè¿‡åŒºåˆ†è¿™äº›é«˜å½±å“åŠ›çš„æ ‡è®°ä¸å¯é¢„æµ‹çš„å¤§å¤šæ•°ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¯é æ€§å…¬å¼ï¼Œè§£é‡Šäº†ç°ä»£LLMåœ¨æ•°åƒä¸ªæ ‡è®°ä¸­çš„æŒç»­ä¸€è‡´æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡æ€§èƒ½ä¸»è¦ä¾èµ–äºå‡†ç¡®å¯¼èˆªå°‘æ•°å…³é”®è¯­ä¹‰å†³ç­–ç‚¹ï¼Œè€Œéå‡åŒ€çš„æ ‡è®°çº§å‡†ç¡®æ€§ï¼Œä»è€Œä½¿å¾—æœ‰é’ˆå¯¹æ€§çš„ç­–ç•¥æ˜¾è‘—ä¼˜äºç²—æš´çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥é€‰æ‹©æ€§ä¿ç•™è¯­ä¹‰é‡è¦æ ‡è®°ä¸ºä¸­å¿ƒçš„ä¸‹ä¸€ä»£ç³»ç»Ÿæ¡†æ¶ï¼Œæ ‡å¿—ç€ä»ç®€å•æ‰©å±•åˆ°æˆ˜ç•¥æ¨ç†çš„æ ¹æœ¬è½¬å˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­å¯é æ€§ä¸‹é™çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å‡è®¾é”™è¯¯æ¦‚ç‡å‘ˆæŒ‡æ•°è¡°å‡ï¼Œæœªèƒ½è€ƒè™‘é”™è¯¯çš„éå‡åŒ€åˆ†å¸ƒç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è¯†åˆ«å’ŒåŒºåˆ†å…³é”®æ ‡è®°ä¸ä¸€èˆ¬æ ‡è®°ï¼Œæå‡ºæ–°çš„å¯é æ€§å…¬å¼ï¼Œå¼ºè°ƒåœ¨å†³ç­–èŠ‚ç‚¹çš„å‡†ç¡®æ€§å¯¹æ¨¡å‹æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å…³é”®æ ‡è®°è¯†åˆ«æ¨¡å—ã€åŠ¨æ€è®¡ç®—åˆ†é…æ¨¡å—å’Œå¤šè·¯å¾„æ¢ç´¢æ¨¡å—ï¼Œæ—¨åœ¨ä¼˜åŒ–æ¨¡å‹åœ¨ä¸ç¡®å®šå†³ç­–è¾¹ç•Œçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºå…³é”®æ ‡è®°çš„å¯é æ€§è¯„ä¼°æ–¹æ³•ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçš„å‡åŒ€é”™è¯¯å‡è®¾ï¼Œæä¾›äº†æ›´ç²¾ç»†çš„æ€§èƒ½ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€è®¡ç®—åˆ†é…ç­–ç•¥ï¼Œé’ˆå¯¹å…³é”®å†³ç­–ç‚¹è¿›è¡Œèµ„æºä¼˜åŒ–ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸è‡ªç„¶è¯­ä¹‰é¢†åŸŸå¯¹é½çš„æ¶æ„è®¾è®¡ï¼Œä»¥æå‡æ¨¡å‹çš„å†³ç­–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ–°æ¡†æ¶çš„æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†20%çš„è¿è´¯æ€§è¯„åˆ†ï¼Œå¹¶åœ¨å…³é”®å†³ç­–ç‚¹çš„å‡†ç¡®æ€§ä¸Šè¾¾åˆ°äº†85%çš„è¡¨ç°ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–å…³é”®å†³ç­–ç‚¹çš„å¤„ç†ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­ä¿æŒæ›´é«˜çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The prevailing assumption of an exponential decay in large language model (LLM) reliability with sequence length, predicated on independent per-token error probabilities, posits an inherent limitation for long autoregressive outputs. Our research fundamentally challenges this view by synthesizing emerging evidence that LLM errors are not uniformly distributed but are concentrated at sparse "key tokens" ($5-10\%$ of total tokens) representing critical decision junctions. By distinguishing these high-impact tokens from the increasingly predictable majority, we introduce a new reliability formula explaining the sustained coherence of modern LLMs over thousands of tokens. Converging research streams reveal that long-context performance primarily depends on accurately navigating a few crucial semantic decision points rather than on uniform token-level accuracy, enabling targeted strategies that significantly outperform brute-force approaches. We thus propose a framework for next-generation systems centered on selective preservation of semantically vital tokens, dynamic computational allocation at uncertain decision boundaries, multi-path exploration at ambiguities, and architectures aligned with natural semantic domains. This marks a fundamental shift from raw scaling to strategic reasoning, promising breakthrough performance without proportionate computational scaling and offering a more nuanced understanding that supersedes the exponential decay hypothesis, thereby opening pathways toward substantially more powerful and efficient language systems.

