---
layout: default
title: "From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning"
---

# From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24768v1</a>
  <a href="https://arxiv.org/pdf/2505.24768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24768v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24768v1', 'From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoyu Li, Xuhong Li, Yiming Dong, Kun Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ•°æ®é›†å¤šæ ·æ€§æ§åˆ¶ç­–ç•¥ä»¥æå‡è¯­è¨€æ¨¡å‹å¾®è°ƒæ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®é›†å¤šæ ·æ€§` `è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `å¤šæ ·æ€§æ§åˆ¶ç­–ç•¥` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç»Ÿè®¡åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¯¹æ•°æ®é›†å¤šæ ·æ€§çš„é‡è¦æ€§è®¤è¯†ä¸è¶³ï¼Œç¼ºä¹ç³»ç»Ÿæ€§åˆ†æï¼Œå½±å“äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„å¤šæ ·æ€§æ§åˆ¶ç­–ç•¥åˆ†ç±»ï¼Œæ¶µç›–å®è§‚ã€ä¸­è§‚å’Œå¾®è§‚å±‚é¢çš„åˆ†æï¼Œç‰¹åˆ«å…³æ³¨å“åº”ç»„ä»¶çš„å¾®è§‚å¤šæ ·æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è§‚ç­–ç•¥åœ¨æé«˜æ¨¡å‹æ€§èƒ½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¤šæ ·æ€§è¾¾åˆ°æœ€å¤§æ—¶ï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®é›†å¤šæ ·æ€§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç›‘ç£å¾®è°ƒé˜¶æ®µè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç³»ç»Ÿæ€§åˆ†ææ•°æ®é›†å¤šæ ·æ€§ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç°æœ‰å¤šæ ·æ€§æ§åˆ¶ç­–ç•¥çš„ç³»ç»Ÿåˆ†ç±»ï¼Œå…³æ³¨æŒ‡ä»¤ç»„ä»¶çš„å®è§‚å’Œä¸­è§‚å±‚é¢ï¼Œå¹¶å¼•å…¥å¯¹å“åº”ç»„ä»¶å¾®è§‚å¤šæ ·æ€§çš„åˆ†æï¼Œå…·ä½“åˆ†æå¾®è°ƒè®­ç»ƒæ ·æœ¬ä¸­æ ‡è®°çš„ç»Ÿè®¡åˆ†å¸ƒã€‚é€šè¿‡æ„å»ºå›ºå®šå¤§å°çš„æ•°æ®é›†å¹¶åº”ç”¨å…­ç§å¤šæ ·æ€§æ§åˆ¶ç­–ç•¥è¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œå®è§‚å’Œä¸­è§‚ç­–ç•¥åœ¨å¤šæ ·æ€§å¢åŠ æ—¶æå‡æ€§èƒ½ï¼Œè€Œå“åº”çš„å¾®è§‚ç­–ç•¥ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„ç›¸å…³æ€§æ›´å¼ºï¼Œä¸”åœ¨æ‰€æœ‰ç­–ç•¥ä¸­è¡¨ç°æœ€ä½³ã€‚è¿™äº›å‘ç°ä¸ºæ„å»ºé«˜æ€§èƒ½çš„å¾®è°ƒæ•°æ®é›†æä¾›äº†å¯è¡Œçš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ•°æ®é›†å¤šæ ·æ€§å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒæ•ˆæœçš„å½±å“ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šæ ·æ€§åˆ†æä¸Šå­˜åœ¨ä¸è¶³ï¼Œæœªèƒ½å…¨é¢è€ƒè™‘æŒ‡ä»¤å’Œå“åº”çš„å¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿåˆ†ç±»ç°æœ‰å¤šæ ·æ€§æ§åˆ¶ç­–ç•¥ï¼Œé‡ç‚¹åˆ†ææŒ‡ä»¤çš„å®è§‚å’Œä¸­è§‚å±‚é¢ä»¥åŠå“åº”çš„å¾®è§‚å±‚é¢ï¼Œä»¥æå‡å¾®è°ƒæ•°æ®é›†çš„æ„å»ºæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶æ„å»ºäº†å›ºå®šå¤§å°çš„æ•°æ®é›†ï¼Œä»117,000ä¸ªå¼€æºå¾®è°ƒæ ·æœ¬ä¸­æå–ï¼Œåº”ç”¨å…­ç§å¤šæ ·æ€§æ§åˆ¶ç­–ç•¥ï¼Œåˆ†åˆ«åœ¨å®è§‚ã€ä¸­è§‚å’Œå¾®è§‚å±‚é¢è¿›è¡Œå®éªŒè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥å¯¹å“åº”ç»„ä»¶çš„å¾®è§‚å¤šæ ·æ€§åˆ†æï¼Œæ­ç¤ºå…¶ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œæä¾›äº†æ–°çš„è§†è§’æ¥ç†è§£æ•°æ®é›†å¤šæ ·æ€§çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨å›ºå®šæ ·æœ¬é‡çš„æ•°æ®é›†è®¾è®¡ï¼Œä½¿ç”¨å…­ç§å¤šæ ·æ€§æ§åˆ¶ç­–ç•¥ï¼Œè¯„ä¼°å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨å“åº”çš„æ ‡è®°ç»Ÿè®¡åˆ†å¸ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå®è§‚å’Œä¸­è§‚ç­–ç•¥åœ¨å¤šæ ·æ€§å¢åŠ æ—¶æå‡æ¨¡å‹æ€§èƒ½ï¼Œè€Œå¾®è§‚ç­–ç•¥åœ¨æ‰€æœ‰ç­–ç•¥ä¸­è¡¨ç°æœ€ä½³ï¼Œå°¤å…¶åœ¨æœ€å¤§å¤šæ ·æ€§ä¸‹ï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œæä¾›äº†å…·ä½“çš„æ€§èƒ½æ•°æ®æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¾®è°ƒæ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œå¯ä»¥æ˜¾è‘—æå‡è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¿»è¯‘ç­‰æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dataset diversity plays a pivotal role for the successful training of many machine learning models, particularly in the supervised fine-tuning (SFT) stage of large language model (LLM) development. Despite increasing recognition of its importance, systematic analyses of dataset diversity still remain underexplored. To address this gap, this work presents a systematic taxonomy of existing diversity-control strategies, which primarily focus on the instruction component, operating at either macroscopic (entire instruction semantics) or mesoscopic levels (instruction units), and furthermore introduces a novel analysis of microscopic diversity within the response component, specifically analyzing the statistical distribution of tokens in SFT training samples. In the experimental evaluation, we construct fixed-size datasets (e.g., 10,000 samples each) from a corpus of 117,000 open-source SFT samples, incorporating six distinct diversity-control strategies spanning macro-, meso-, and microscopic levels applied to both instructions and responses. We then fine-tune LLMs on these datasets to assess the six diversity-control strategies. Results reveal that while macroscopic and mesoscopic strategies lead to higher performance with increasing diversity, the microscopic strategy in responses exhibits both a stronger correlation between model performance and the degree of diversity and superior performance with maximum diversity across all strategies. These findings offer actionable insights for constructing high-performance SFT datasets.

