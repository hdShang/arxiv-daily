---
layout: default
title: "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text"
---

# LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24826" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24826v2</a>
  <a href="https://arxiv.org/pdf/2505.24826.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24826v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24826v2', 'LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Li yunhan, Wu gengshen

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-11-10)

**å¤‡æ³¨**: 10 pages, 11 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/lyxx3rd/LegalEval-Q)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLegalEval-Qä»¥è§£å†³æ³•å¾‹æ–‡æœ¬ç”Ÿæˆè´¨é‡è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ³•å¾‹æ–‡æœ¬ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è´¨é‡è¯„ä¼°` `å›å½’æ¨¡å‹` `æ¨ç†æ¨¡å‹` `æ ‡å‡†åŒ–è¯„ä¼°` `æ¸…æ™°åº¦` `è¿è´¯æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ³•å¾‹æ–‡æœ¬ç”Ÿæˆè¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨äº‹å®å‡†ç¡®æ€§ï¼Œå¿½ç•¥äº†è¯­è¨€è´¨é‡çš„å…¶ä»–é‡è¦æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå›å½’æ¨¡å‹æ¥è¯„ä¼°æ³•å¾‹æ–‡æœ¬çš„æ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­ï¼ŒåŒæ—¶åˆ›å»ºäº†ä¸“é—¨çš„æ³•å¾‹é—®é¢˜é›†ã€‚
3. é€šè¿‡åˆ†æ49ä¸ªLLMsï¼Œå‘ç°æ¨¡å‹å‚æ•°åœ¨140äº¿æ—¶è´¨é‡è¶‹äºå¹³ç¨³ï¼Œæ¨ç†æ¨¡å‹è¡¨ç°ä¼˜äºåŸºç¡€æ¶æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ³•å¾‹åº”ç”¨ä¸­çš„æ—¥ç›Šæ™®åŠï¼Œç°æœ‰è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨äº‹å®å‡†ç¡®æ€§ï¼Œè€Œå¿½è§†äº†æ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­ç­‰é‡è¦è¯­è¨€è´¨é‡æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œå¼€å‘äº†ä¸€ä¸ªå›å½’æ¨¡å‹ä»¥è¯„ä¼°æ³•å¾‹æ–‡æœ¬çš„è´¨é‡ï¼›å…¶æ¬¡ï¼Œåˆ›å»ºäº†ä¸€å¥—ä¸“é—¨çš„æ³•å¾‹é—®é¢˜é›†ï¼›æœ€åï¼Œåˆ©ç”¨è¯¥è¯„ä¼°æ¡†æ¶åˆ†æäº†49ä¸ªLLMsã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹è´¨é‡åœ¨140äº¿å‚æ•°æ—¶è¶‹äºå¹³ç¨³ï¼Œ72äº¿å‚æ•°æ—¶ä»…æœ‰2.7%çš„è¾¹é™…æå‡ï¼›å·¥ç¨‹é€‰æ‹©å¦‚é‡åŒ–å’Œä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç»“æœå½±å“å¾®å°ï¼›æ¨ç†æ¨¡å‹çš„è¡¨ç°å§‹ç»ˆä¼˜äºåŸºç¡€æ¶æ„ã€‚ç ”ç©¶çš„ä¸€ä¸ªé‡è¦æˆæœæ˜¯å‘å¸ƒäº†æ’ååˆ—è¡¨å’Œå¸•ç´¯æ‰˜åˆ†æï¼Œçªå‡ºäº†Qwen3ç³»åˆ—åœ¨æ€§ä»·æ¯”æ–¹é¢çš„æœ€ä½³é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰æ³•å¾‹æ–‡æœ¬ç”Ÿæˆè´¨é‡è¯„ä¼°ä¸­å¯¹è¯­è¨€è´¨é‡æ–¹é¢çš„å¿½è§†ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨äº‹å®å‡†ç¡®æ€§ä¸Šï¼Œå¯¼è‡´è¯„ä¼°ä¸å…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å›å½’æ¨¡å‹ç»¼åˆè¯„ä¼°æ³•å¾‹æ–‡æœ¬çš„æ¸…æ™°åº¦ã€è¿è´¯æ€§å’Œæœ¯è¯­ï¼Œå¡«è¡¥ç°æœ‰è¯„ä¼°æ¡†æ¶çš„ç©ºç™½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) å›å½’æ¨¡å‹ç”¨äºè´¨é‡è¯„ä¼°ï¼›2) ä¸“é—¨çš„æ³•å¾‹é—®é¢˜é›†ï¼›3) 49ä¸ªLLMsçš„åˆ†æä¸æ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå»ºç«‹äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°æ³•å¾‹æ–‡æœ¬ç”Ÿæˆçš„è¯­è¨€è´¨é‡ï¼Œæ­ç¤ºäº†å½“å‰è®­ç»ƒæ•°æ®ç²¾ç‚¼æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒæ—¶å¯¹æ¨¡å‹çš„é‡åŒ–å’Œä¸Šä¸‹æ–‡é•¿åº¦è¿›è¡Œäº†åˆ†æï¼Œå‘ç°å…¶å¯¹ç»“æœçš„å½±å“å¾®ä¹å…¶å¾®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨140äº¿å‚æ•°æ—¶è´¨é‡è¶‹äºå¹³ç¨³ï¼Œ72äº¿å‚æ•°æ—¶ä»…æœ‰2.7%çš„æå‡ï¼Œä¸”æ¨ç†æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºç¡€æ¶æ„ã€‚è¿™äº›å‘ç°ä¸ºæ³•å¾‹æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„é€‰æ‹©å’Œä¼˜åŒ–æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ³•å¾‹æ–‡æœ¬ç”Ÿæˆã€æ³•å¾‹å’¨è¯¢è‡ªåŠ¨åŒ–å’Œæ³•å¾‹æ–‡ä¹¦å®¡æ ¸ç­‰ã€‚é€šè¿‡å»ºç«‹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œå¯ä»¥æå‡æ³•å¾‹é¢†åŸŸä¸­LLMsçš„åº”ç”¨æ•ˆæœï¼Œä¿ƒè¿›æ³•å¾‹æœåŠ¡çš„æ™ºèƒ½åŒ–ä¸é«˜æ•ˆåŒ–ï¼Œæœªæ¥å¯èƒ½å¯¹æ³•å¾‹è¡Œä¸šçš„å·¥ä½œæµç¨‹äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) are increasingly used in legal applications, current evaluation benchmarks tend to focus mainly on factual accuracy while largely neglecting important linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and terminology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework.
>   Our analysis identifies three key findings: First, model quality levels off at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as indicated by statistical significance thresholds above 0.016. Third, reasoning models consistently outperform base architectures. A significant outcome of our research is the release of a ranking list and Pareto analysis, which highlight the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation protocols for legal LLMs but also uncovers fundamental limitations in current training data refinement approaches. Code and models are available at: https://github.com/lyxx3rd/LegalEval-Q.

