---
layout: default
title: Model Unlearning via Sparse Autoencoder Subspace Guided Projections
---

# Model Unlearning via Sparse Autoencoder Subspace Guided Projections

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24428" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24428v1</a>
  <a href="https://arxiv.org/pdf/2505.24428.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24428v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24428v1', 'Model Unlearning via Sparse Autoencoder Subspace Guided Projections')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xu Wang, Zihao Li, Benyou Wang, Yan Hu, Difan Zou

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAEå¼•å¯¼çš„å­ç©ºé—´æŠ•å½±å»å­¦ä¹ æ–¹æ³•ä»¥è§£å†³éšç§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å»å­¦ä¹ ` `ç¨€ç–è‡ªç¼–ç å™¨` `å­ç©ºé—´ä¼˜åŒ–` `å¯¹æŠ—é²æ£’æ€§` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å»å­¦ä¹ æ–¹æ³•åœ¨å¯è§£é‡Šæ€§å’Œå¯¹æŠ—æ€§é˜²å¾¡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ é™¤æ¨¡å‹ä¸­çš„ç‰¹å®šçŸ¥è¯†ã€‚
2. è®ºæ–‡æå‡ºçš„SSPUæ¡†æ¶åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ç‰¹å¾ï¼Œé€šè¿‡å­ç©ºé—´å¼•å¯¼å®ç°ç²¾ç¡®çš„æ¨¡å‹å‚æ•°æ›´æ–°ï¼Œå¢å¼ºå¯è§£é‡Šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSSPUåœ¨å‡å°‘æœ‰å®³çŸ¥è¯†å‡†ç¡®ç‡å’Œæé«˜å¯¹æŠ—é²æ£’æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜å‚¨äº†å¤§é‡ä¿¡æ¯ï¼Œè™½ç„¶å…¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†åœ¨éœ€è¦é€‰æ‹©æ€§åˆ é™¤çŸ¥è¯†æ—¶å¼•å‘äº†éšç§å’Œå®‰å…¨é—®é¢˜ã€‚ç°æœ‰çš„å»å­¦ä¹ ç­–ç•¥ï¼Œå¦‚åŸºäºæ¢¯åº¦çš„å¾®è°ƒå’Œç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å¼•å¯¼ï¼Œç¼ºä¹å¯è§£é‡Šæ€§æˆ–æœªèƒ½æœ‰æ•ˆæŠµå¾¡å¯¹æŠ—æ€§æç¤ºã€‚æˆ‘ä»¬æå‡ºäº†SAEå¼•å¯¼çš„å­ç©ºé—´æŠ•å½±å»å­¦ä¹ ï¼ˆSSPUï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨SAEç‰¹å¾åœ¨æ¨¡å‹å‚æ•°ç©ºé—´ä¸­é©±åŠ¨æœ‰é’ˆå¯¹æ€§çš„æ›´æ–°ï¼Œå®ç°ç²¾ç¡®ã€å¯è§£é‡Šå’Œç¨³å¥çš„å»å­¦ä¹ ã€‚SSPUçš„ä¸‰é˜¶æ®µæµç¨‹åŒ…æ‹¬æ•°æ®é©±åŠ¨çš„å±‚å’Œç‰¹å¾é€‰æ‹©ã€é€šè¿‡QRåˆ†è§£æ„å»ºå­ç©ºé—´ï¼Œä»¥åŠçº¦æŸä¼˜åŒ–ä»¥æ§åˆ¶æ¿€æ´»è¿›å…¥â€œæ— å…³â€å­ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å·²ä¿ç•™çš„çŸ¥è¯†ã€‚åœ¨WMDP-Cyberé—å¿˜é›†å’Œä¸‰ä¸ªæ•ˆç”¨åŸºå‡†ï¼ˆMMLUã€TruthfulQAã€GSM8Kï¼‰ä¸Šçš„å®éªŒä¸­ï¼ŒSSPUç›¸æ¯”æœ€å¼ºåŸºçº¿å‡å°‘äº†3.22%çš„æœ‰å®³çŸ¥è¯†å‡†ç¡®ç‡ï¼Œå¹¶æé«˜äº†å¯¹æŠ—é²æ£’æ€§ï¼Œé™ä½äº†åœ¨è¶Šç‹±æç¤ºä¸‹çš„æ¶æ„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å…ˆå‰å»å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†å¯è§£é‡Šçš„å­ç©ºé—´å¼•å¯¼ä¼˜åŒ–å¦‚ä½•å®ç°ç¨³å¥ã€å¯æ§çš„æ¨¡å‹è¡Œä¸ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é€‰æ‹©æ€§çŸ¥è¯†åˆ é™¤çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¯è§£é‡Šæ€§å’Œå¯¹æŠ—æ€§é˜²å¾¡æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSSPUæ¡†æ¶é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ç‰¹å¾å¼•å¯¼æ¨¡å‹å‚æ•°çš„æœ‰é’ˆå¯¹æ€§æ›´æ–°ï¼Œç¡®ä¿å»å­¦ä¹ è¿‡ç¨‹çš„ç²¾ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSPUçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šæ•°æ®é©±åŠ¨çš„å±‚å’Œç‰¹å¾é€‰æ‹©ã€QRåˆ†è§£æ„å»ºå­ç©ºé—´ã€ä»¥åŠçº¦æŸä¼˜åŒ–ä»¥æ§åˆ¶æ¿€æ´»è¿›å…¥â€œæ— å…³â€å­ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™é‡è¦çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šSSPUçš„ä¸»è¦åˆ›æ–°åœ¨äºåˆ©ç”¨SAEç‰¹å¾æ„å»ºä¸€ä¸ªç›‘ç£å»å­¦ä¹ çš„å­ç©ºé—´ï¼Œé€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°å’Œæ·»åŠ æ­£åˆ™åŒ–é¡¹æ¥å¼•å¯¼å¯è§£é‡Šçš„å‚æ•°æ›´æ–°ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è®¾è®¡ç†å¿µæœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSSPUé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–ç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å»å­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒç¨³å®šæ€§å’Œé²æ£’æ€§ï¼ŒåŒæ—¶é€šè¿‡ä¼˜åŒ–ç®—æ³•æ§åˆ¶å‚æ•°æ›´æ–°çš„æ–¹å‘å’Œå¹…åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSSPUåœ¨WMDP-Cyberé—å¿˜é›†ä¸Šç›¸æ¯”æœ€å¼ºåŸºçº¿å‡å°‘äº†3.22%çš„æœ‰å®³çŸ¥è¯†å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¯¹æŠ—æ€§æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†æ¶æ„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„åœºæ™¯ï¼Œå¦‚ç¤¾äº¤åª’ä½“ã€åœ¨çº¿æœåŠ¡å’ŒåŒ»ç–—æ•°æ®ç®¡ç†ã€‚é€šè¿‡æä¾›ä¸€ç§å¯æ§çš„å»å­¦ä¹ æœºåˆ¶ï¼ŒSSPUèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šåœ¨éµå¾ªæ•°æ®éšç§æ³•è§„çš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°æ¨å¹¿ï¼Œä¿ƒè¿›AIç³»ç»Ÿçš„é€æ˜æ€§å’Œä¿¡ä»»åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. We propose SAE-Guided Subspace Projection Unlearning (SSPU), a novel framework that leverages SAE features to drive targeted updates in the model's parameter space, enabling precise, interpretable, and robust unlearning. SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization that controls activations into an "irrelevant" subspace while preserving retained knowledge. Overall, we use SAE features to construct a subspace that supervises unlearning, refining the loss and adding a regularization term to guide interpretable parameter updates. In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines. Our findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior.

