---
layout: default
title: Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation
---

# Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00288" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00288v3</a>
  <a href="https://arxiv.org/pdf/2506.00288.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00288v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00288v3', 'Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ahmed Elhady, Eneko Agirre, Mikel Artetxe

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-09-19)

**å¤‡æ³¨**: Published as a Conference Paper at the main track of ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»§ç»­é¢„è®­ç»ƒæ–¹æ³•ä»¥å¢å¼ºè¯­è¨€é€‚åº”èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»§ç»­é¢„è®­ç»ƒ` `è¯­è¨€é€‚åº”` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¾éš¾æ€§é—å¿˜` `è¯¾ç¨‹å­¦ä¹ ` `æŒ‡æ•°ç§»åŠ¨å¹³å‡` `å¤šè¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç»§ç»­é¢„è®­ç»ƒæ–¹æ³•åœ¨é€‚åº”æ–°è¯­è¨€æ—¶ï¼Œæœªèƒ½å……åˆ†ç ”ç©¶è‹±è¯­æ•°æ®çš„ä½œç”¨ï¼Œå¯¼è‡´ä¸‹æ¸¸èƒ½åŠ›çš„ç¼ºå¤±ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­è¨€æ— å…³çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ä¸åŒ…å«è‹±è¯­æ—¶æ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€çš„ç¾éš¾æ€§é—å¿˜ç°è±¡ã€‚
3. é€šè¿‡å¼•å…¥è¯¾ç¨‹å­¦ä¹ å’ŒEMAï¼Œç ”ç©¶è¡¨æ˜è¿™äº›æ–¹æ³•èƒ½æœ‰æ•ˆç¼“è§£å¯¹è‹±è¯­æ•°æ®çš„ä¾èµ–ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»§ç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ˜¯å°†ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”æ–°è¯­è¨€çš„å¸¸ç”¨æ–¹æ³•ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨CPTè¿‡ç¨‹ä¸­åŒ…å«è‹±è¯­æ•°æ®çš„ä½œç”¨ï¼Œå‘ç°è™½ç„¶å…¶å¯¹éªŒè¯å›°æƒ‘åº¦æ²¡æœ‰å½±å“ï¼Œä½†å¯¹ç›®æ ‡è¯­è¨€ä¸‹æ¸¸èƒ½åŠ›çš„å‡ºç°è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­è¨€æ— å…³çš„ä¸Šä¸‹æ–‡å­¦ä¹ åŸºå‡†ï¼Œæ­ç¤ºäº†åœ¨ä¸åŒ…å«è‹±è¯­æ—¶ï¼ŒCPTæ—©æœŸä¼šå‡ºç°ç¾éš¾æ€§é—å¿˜ï¼ŒæŸå®³æ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€çš„æ³›åŒ–èƒ½åŠ›ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæœ¬æ–‡æå‡ºäº†è¯¾ç¨‹å­¦ä¹ å’Œæƒé‡çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰ä½œä¸ºæœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»¥å‡å°‘å¯¹è‹±è¯­çš„ä¾èµ–ã€‚æ•´ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶æ­ç¤ºäº†åœ¨è¿›è¡Œè¯­è¨€é€‚åº”çš„CPTè¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•äº§ç”Ÿæ–°å…´èƒ½åŠ›çš„åŠ¨æ€ï¼Œä¸ºæœªæ¥è®¾è®¡æ›´æœ‰æ•ˆçš„æ–¹æ³•å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç»§ç»­é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè‹±è¯­æ•°æ®å¯¹æ¨¡å‹é€‚åº”æ–°è¯­è¨€èƒ½åŠ›çš„å½±å“ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ¢è®¨è¿™ä¸€é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è¯­è¨€æ— å…³çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºä¸åŒ…å«è‹±è¯­æ•°æ®æ—¶æ¨¡å‹çš„ç¾éš¾æ€§é—å¿˜ç°è±¡ï¼Œè¿›è€Œæå‡ºè¯¾ç¨‹å­¦ä¹ å’ŒEMAä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹é¢„è®­ç»ƒã€è¯„ä¼°åŸºå‡†æµ‹è¯•å’Œåº”ç”¨è¯¾ç¨‹å­¦ä¹ ä¸EMAç­‰æ¨¡å—ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€ä¸Šçš„æœ‰æ•ˆé€‚åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè¯†åˆ«å‡ºè‹±è¯­æ•°æ®åœ¨CPTä¸­çš„å…³é”®ä½œç”¨ï¼ŒåŠå…¶å¯¹æ¨¡å‹èƒ½åŠ›çš„å½±å“ï¼Œæå‡ºäº†æ–°çš„è¯„ä¼°æ–¹æ³•å’Œè®­ç»ƒç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å’ŒEMAæŠ€æœ¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™é‡è¦ä¿¡æ¯ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚å…·ä½“æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡æœªè¯¦ç»†æŠ«éœ²ï¼Œå¾…è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸åŒ…å«è‹±è¯­æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€çš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼ŒéªŒè¯å›°æƒ‘åº¦æœªèƒ½åæ˜ è¿™ä¸€å˜åŒ–ã€‚é€šè¿‡å¼•å…¥è¯¾ç¨‹å­¦ä¹ å’ŒEMAï¼Œæ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€çš„è¡¨ç°å¾—åˆ°äº†æœ‰æ•ˆæå‡ï¼Œå±•ç¤ºäº†æ–°å…´èƒ½åŠ›çš„åŠ¨æ€å˜åŒ–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œè·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­è¨€é€‚åº”èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æœåŠ¡äºå…¨çƒç”¨æˆ·ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ™®åŠä¸å‘å±•ã€‚æœªæ¥ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½ä¼šå½±å“å¤šè¯­è¨€æ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ï¼Œä¿ƒè¿›æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.

