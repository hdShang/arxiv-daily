---
layout: default
title: "PersianMedQA: Evaluating Large Language Models on a Persian-English Bilingual Medical Question Answering Benchmark"
---

# PersianMedQA: Evaluating Large Language Models on a Persian-English Bilingual Medical Question Answering Benchmark

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00250" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00250v3</a>
  <a href="https://arxiv.org/pdf/2506.00250.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00250v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00250v3', 'PersianMedQA: Evaluating Large Language Models on a Persian-English Bilingual Medical Question Answering Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Javad Ranjbar Kalahroodi, Amirhossein Sheikholselami, Sepehr Karimi, Sepideh Ranjbar Kalahroodi, Heshaam Faili, Azadeh Shakery

**åˆ†ç±»**: cs.CL, cs.IT

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-08-10)

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/MohammadJRanjbar)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPersianMedQAä»¥è¯„ä¼°åŒè¯­åŒ»ç–—é—®ç­”ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åŒ»å­¦é—®ç­”` `æ³¢æ–¯è¯­` `åŒè¯­è¯„ä¼°` `æ•°æ®é›†æ„å»º` `æ–‡åŒ–èƒŒæ™¯` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä»å­˜åœ¨å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€çš„ç¯å¢ƒä¸­ã€‚
2. æœ¬æ–‡æå‡ºäº†PersianMedQAæ•°æ®é›†ï¼ŒåŒ…å«æ³¢æ–¯è¯­åŒ»å­¦é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨åŒè¯­ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé—­æºé€šç”¨æ¨¡å‹åœ¨æ³¢æ–¯è¯­å’Œè‹±è¯­ä¸­çš„å‡†ç¡®ç‡æ˜¾è‘—é«˜äºæ³¢æ–¯è¯­å¾®è°ƒæ¨¡å‹ï¼Œç¿»è¯‘å½±å“ä¹Ÿè¢«æ·±å…¥åˆ†æã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é«˜é£é™©é¢†åŸŸå¦‚åŒ»å­¦ï¼Œå°¤å…¶æ˜¯ä½èµ„æºè¯­è¨€çš„å¯é æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚æœ¬æ–‡ä»‹ç»äº†PersianMedQAï¼Œä¸€ä¸ªåŒ…å«20,785ä¸ªç»è¿‡ä¸“å®¶éªŒè¯çš„æ³¢æ–¯è¯­åŒ»å­¦å¤šé¡¹é€‰æ‹©é¢˜çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨æ³¢æ–¯è¯­å’Œè‹±è¯­ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å¯¹40ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºé—­æºé€šç”¨æ¨¡å‹ï¼ˆå¦‚GPT-4.1ï¼‰åœ¨æ³¢æ–¯è¯­å’Œè‹±è¯­ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œè€Œæ³¢æ–¯è¯­å¾®è°ƒæ¨¡å‹è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†ç¿»è¯‘çš„å½±å“ï¼Œå‘ç°æŸäº›é—®é¢˜åœ¨æ³¢æ–¯è¯­ä¸­æ‰èƒ½æ­£ç¡®å›ç­”ã€‚PersianMedQAä¸ºè¯„ä¼°åŒè¯­å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„åŒ»å­¦æ¨ç†æä¾›äº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨æ³¢æ–¯è¯­ç¯å¢ƒä¸­çš„å¯é æ€§ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€çš„åº”ç”¨æ•ˆæœä¸ç†æƒ³ï¼Œç¼ºä¹é’ˆå¯¹æ€§çš„è¯„ä¼°åŸºå‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºPersianMedQAæ•°æ®é›†ï¼ŒåŒ…å«ç»è¿‡ä¸“å®¶éªŒè¯çš„æ³¢æ–¯è¯­åŒ»å­¦é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªåŒè¯­è¯„ä¼°å¹³å°ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹é€‰æ‹©ä¸è¯„ä¼°ã€‚æ•°æ®é›†æ¶µç›–14å¹´ä¼Šæœ—å›½å®¶åŒ»å­¦è€ƒè¯•çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¨¡å‹è¯„ä¼°åˆ™åŒ…æ‹¬é›¶-shotå’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰è®¾ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šPersianMedQAæ•°æ®é›†çš„æ„å»ºæ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå¡«è¡¥äº†ä½èµ„æºè¯­è¨€åŒ»å­¦é—®ç­”è¯„ä¼°çš„ç©ºç™½ï¼Œå¹¶æä¾›äº†æ–‡åŒ–èƒŒæ™¯ä¸‹çš„åŒ»å­¦æ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†40ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œç‰¹åˆ«å…³æ³¨äº†æ¨¡å‹çš„è¯­è¨€é€‚åº”æ€§å’Œé¢†åŸŸé€‚åº”æ€§ï¼Œå‘ç°æ¨¡å‹å¤§å°å¹¶ä¸è¶³ä»¥ä¿è¯æ€§èƒ½ï¼Œéœ€ç»“åˆå¼ºå¤§çš„é¢†åŸŸçŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé—­æºé€šç”¨æ¨¡å‹ï¼ˆå¦‚GPT-4.1ï¼‰åœ¨æ³¢æ–¯è¯­ä¸­è¾¾åˆ°äº†83.09%çš„å‡†ç¡®ç‡ï¼Œåœ¨è‹±è¯­ä¸­ä¸º80.7%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ³¢æ–¯è¯­å¾®è°ƒæ¨¡å‹å¦‚Dornaçš„è¡¨ç°æ˜¾è‘—ä½ä¸‹ï¼Œä»…ä¸º34.9%ã€‚æ­¤å¤–ï¼Œç¿»è¯‘åˆ†æè¡¨æ˜ï¼Œ3-10%çš„é—®é¢˜åœ¨æ³¢æ–¯è¯­ä¸­æ‰èƒ½æ­£ç¡®å›ç­”ï¼Œå¼ºè°ƒäº†æ–‡åŒ–å’Œä¸´åºŠèƒŒæ™¯çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦æ•™è‚²ã€ä¸´åºŠå†³ç­–æ”¯æŒå’ŒåŒ»ç–—ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªåŒè¯­è¯„ä¼°å¹³å°ï¼ŒPersianMedQAèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å¯é çš„åŒ»ç–—é—®ç­”ç³»ç»Ÿï¼Œä¿ƒè¿›ä½èµ„æºè¯­è¨€çš„åŒ»å­¦ä¿¡æ¯è·å–ï¼Œæå‡åŒ»ç–—æœåŠ¡çš„å¯åŠæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved remarkable performance on a wide range of Natural Language Processing (NLP) benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource languages, remains underexplored. In this work, we introduce PersianMedQA, a large-scale dataset of 20,785 expert-validated multiple-choice Persian medical questions from 14 years of Iranian national medical exams, spanning 23 medical specialties and designed to evaluate LLMs in both Persian and English. We benchmark 40 state-of-the-art models, including general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and chain-of-thought (CoT) settings. Our results show that closed-source general models (e.g., GPT-4.1) consistently outperform all other categories, achieving 83.09% accuracy in Persian and 80.7% in English, while Persian fine-tuned models such as Dorna underperform significantly (e.g., 34.9% in Persian), often struggling with both instruction-following and domain reasoning. We also analyze the impact of translation, showing that while English performance is generally higher, 3-10% of questions can only be answered correctly in Persian due to cultural and clinical contextual cues that are lost in translation. Finally, we demonstrate that model size alone is insufficient for robust performance without strong domain or language adaptation. PersianMedQA provides a foundation for evaluating bilingual and culturally grounded medical reasoning in LLMs. The PersianMedQA dataset is available: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA .

