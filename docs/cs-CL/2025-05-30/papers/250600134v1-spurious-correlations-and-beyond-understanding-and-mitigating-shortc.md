---
layout: default
title: Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models
---

# Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00134" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00134v1</a>
  <a href="https://arxiv.org/pdf/2506.00134.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00134v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00134v1', 'Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fardin Ahsan Sakib, Ziwei Zhu, Karen Trister Grace, Meliha Yetisgen, Ozlem Uzuner

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–°æ–¹æ³•ä»¥è§£å†³ç¤¾ä¼šå¥åº·å†³å®šå› ç´ æå–ä¸­çš„å¿«æ·å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¤¾ä¼šå¥åº·å†³å®šå› ç´ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è™šå‡é¢„æµ‹` `æç¤ºå·¥ç¨‹` `æ€ç»´é“¾æ¨ç†` `åŒ»ç–—æ–‡æœ¬åˆ†æ` `æ€§åˆ«å·®å¼‚`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMsåœ¨æå–ç¤¾ä¼šå¥åº·å†³å®šå› ç´ æ—¶ï¼Œå®¹æ˜“å—åˆ°è¡¨é¢çº¿ç´¢çš„å½±å“ï¼Œå¯¼è‡´è™šå‡é¢„æµ‹ï¼Œå½±å“æ¨¡å‹çš„å¯é æ€§ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æç¤ºå·¥ç¨‹å’Œæ€ç»´é“¾æ¨ç†ç­‰ç­–ç•¥ï¼Œæ¥å‡å°‘æ¨¡å‹åœ¨è¯ç‰©çŠ¶æ€æå–ä¸­çš„å‡é˜³æ€§é¢„æµ‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç¼“è§£ç­–ç•¥æœ‰æ•ˆé™ä½äº†è™šå‡é¢„æµ‹çš„å‘ç”Ÿï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒæ€§åˆ«ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»ä¸´åºŠæ–‡æœ¬ä¸­æå–ç¤¾ä¼šå¥åº·å†³å®šå› ç´ ï¼ˆSDOHï¼‰å¯¹åç»­çš„åŒ»ç–—åˆ†æè‡³å…³é‡è¦ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¯èƒ½ä¾èµ–è¡¨é¢çº¿ç´¢å¯¼è‡´è™šå‡é¢„æµ‹ã€‚é€šè¿‡ä½¿ç”¨MIMICéƒ¨åˆ†çš„SHACæ•°æ®é›†ï¼Œæœ¬æ–‡ä»¥è¯ç‰©çŠ¶æ€æå–ä¸ºæ¡ˆä¾‹ï¼Œå±•ç¤ºäº†é…’ç²¾æˆ–å¸çƒŸçš„æåŠå¯èƒ½é”™è¯¯åœ°è¯±å¯¼æ¨¡å‹é¢„æµ‹å½“å‰/è¿‡å»çš„è¯ç‰©ä½¿ç”¨ï¼ŒåŒæ—¶æ­ç¤ºäº†æ¨¡å‹æ€§èƒ½ä¸­çš„æ€§åˆ«å·®å¼‚ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯„ä¼°äº†ç¼“è§£ç­–ç•¥ï¼Œå¦‚æç¤ºå·¥ç¨‹å’Œæ€ç»´é“¾æ¨ç†ï¼Œä»¥å‡å°‘è¿™äº›å‡é˜³æ€§ï¼Œä¸ºæé«˜LLMåœ¨å¥åº·é¢†åŸŸçš„å¯é æ€§æä¾›äº†è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æå–ç¤¾ä¼šå¥åº·å†³å®šå› ç´ æ—¶ï¼Œå› ä¾èµ–è¡¨é¢çº¿ç´¢è€Œå¯¼è‡´çš„è™šå‡é¢„æµ‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¯ç‰©çŠ¶æ€æå–æ—¶ï¼Œå®¹æ˜“å—åˆ°é…’ç²¾æˆ–å¸çƒŸæåŠçš„è¯¯å¯¼ï¼Œå¯¼è‡´é”™è¯¯çš„è¯ç‰©ä½¿ç”¨é¢„æµ‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡è®¾è®¡æœ‰æ•ˆçš„æç¤ºå’Œæ¨ç†é“¾ï¼Œæ¥å¼•å¯¼æ¨¡å‹å…³æ³¨æ›´æ·±å±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œéè¡¨é¢çº¿ç´¢ï¼Œä»è€Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œä½¿ç”¨SHACæ•°æ®é›†è¿›è¡Œæ ‡æ³¨å’Œæ¸…æ´—ï¼›åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œåº”ç”¨æç¤ºå·¥ç¨‹å’Œæ€ç»´é“¾æ¨ç†ç­–ç•¥ï¼›æœ€ååœ¨è¯„ä¼°é˜¶æ®µï¼Œåˆ†ææ¨¡å‹åœ¨ä¸åŒæ€§åˆ«å’Œè¯ç‰©çŠ¶æ€ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ç»“åˆæç¤ºå·¥ç¨‹ä¸æ€ç»´é“¾æ¨ç†çš„ç­–ç•¥ï¼Œæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹åœ¨å¤æ‚ä¸´åºŠæ–‡æœ¬ä¸­çš„è¡¨ç°ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹è®­ç»ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è™šå‡é¢„æµ‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œå¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†äº¤å‰ç†µæŸå¤±ä¸æ­£åˆ™åŒ–é¡¹ï¼Œä»¥å¹³è¡¡æ¨¡å‹çš„å‡†ç¡®æ€§ä¸æ³›åŒ–èƒ½åŠ›ï¼›ç½‘ç»œç»“æ„ä¸Šï¼Œä½¿ç”¨äº†åŸºäºTransformerçš„æ¶æ„ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æç¤ºå·¥ç¨‹å’Œæ€ç»´é“¾æ¨ç†åï¼Œæ¨¡å‹åœ¨è¯ç‰©çŠ¶æ€æå–ä»»åŠ¡ä¸­çš„å‡é˜³æ€§ç‡æ˜¾è‘—é™ä½ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒæ€§åˆ«æ ·æœ¬ä¸Šçš„æ€§èƒ½å·®å¼‚ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—æ–‡æœ¬åˆ†æã€ç”µå­å¥åº·è®°å½•çš„æ™ºèƒ½å¤„ç†ä»¥åŠå…¬å…±å«ç”Ÿç ”ç©¶ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šå¥åº·å†³å®šå› ç´ æå–ä¸­çš„å¯é æ€§ï¼Œèƒ½å¤Ÿä¸ºåŒ»ç–—å†³ç­–æä¾›æ›´å‡†ç¡®çš„æ•°æ®æ”¯æŒï¼Œè¿›è€Œæ”¹å–„æ‚£è€…æŠ¤ç†å’Œå¥åº·ç®¡ç†ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿åˆ°å…¶ä»–åŒ»ç–—é¢†åŸŸçš„æ–‡æœ¬åˆ†æä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Social determinants of health (SDOH) extraction from clinical text is critical for downstream healthcare analytics. Although large language models (LLMs) have shown promise, they may rely on superficial cues leading to spurious predictions. Using the MIMIC portion of the SHAC (Social History Annotation Corpus) dataset and focusing on drug status extraction as a case study, we demonstrate that mentions of alcohol or smoking can falsely induce models to predict current/past drug use where none is present, while also uncovering concerning gender disparities in model performance. We further evaluate mitigation strategies - such as prompt engineering and chain-of-thought reasoning - to reduce these false positives, providing insights into enhancing LLM reliability in health domains.

