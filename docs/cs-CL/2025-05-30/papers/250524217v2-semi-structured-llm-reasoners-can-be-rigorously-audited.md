---
layout: default
title: Semi-structured LLM Reasoners Can Be Rigorously Audited
---

# Semi-structured LLM Reasoners Can Be Rigorously Audited

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24217" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24217v2</a>
  <a href="https://arxiv.org/pdf/2505.24217.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24217v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24217v2', 'Semi-structured LLM Reasoners Can Be Rigorously Audited')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jixuan Leng, Cassandra A. Cohen, Zhixian Zhang, Chenyan Xiong, William W. Cohen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-09-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠç»“æ„åŒ–æ¨ç†æ¨¡å‹ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯å®¡è®¡æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†æ¨¡å‹` `å¯å®¡è®¡æ€§` `åŠç»“æ„åŒ–è¡¨ç¤º` `è‡ªåŠ¨å®¡è®¡` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨é”™è¯¯å’Œé—æ¼ï¼Œå¯¼è‡´å¯ä¿¡åº¦ä¸è¶³ï¼Œéš¾ä»¥å®¡è®¡å’Œè¯†åˆ«åè§ã€‚
2. æœ¬æ–‡æå‡ºçš„åŠç»“æ„åŒ–æ¨ç†æ¨¡å‹ï¼ˆSSRMsï¼‰ç”Ÿæˆå¯å®¡è®¡çš„æ¨ç†è½¨è¿¹ï¼Œé‡‡ç”¨Pythonè¯­æ³•æ ‡è®°æ¨ç†æ­¥éª¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSSRMsåœ¨åäºŒä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”å®¡è®¡èƒ½åŠ›æœªå½±å“æ•´ä½“å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶æ¨ç†ç»“æœçš„å¯ä¿¡åº¦ä»ç„¶å­˜åœ¨é—®é¢˜ï¼Œå¯èƒ½åŒ…å«éš¾ä»¥å‘ç°çš„é”™è¯¯å’Œé—æ¼ï¼Œè¿›è€Œæ©ç›–æ¨¡å‹è¾“å‡ºä¸­çš„åè§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŠç»“æ„åŒ–æ¨ç†æ¨¡å‹ï¼ˆSSRMsï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç”ŸæˆåŠç»“æ„åŒ–çš„æ¨ç†è¡¨ç¤ºï¼Œé‡‡ç”¨éå¯æ‰§è¡Œçš„Pythonè¯­æ³•ï¼Œæ ‡è®°æ¯ä¸ªæ¨ç†æ­¥éª¤åŠå…¶è¾“å…¥å’Œè¾“å‡ºã€‚è¿™ç§ç»“æ„ä½¿å¾—SSRMsçš„æ¨ç†è¿‡ç¨‹å¯ä»¥è¢«è‡ªåŠ¨å®¡è®¡ï¼Œä»¥è¯†åˆ«æ¨ç†ç¼ºé™·ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§å®¡è®¡æ–¹æ³•ï¼Œç»“æœè¡¨æ˜è¿™äº›æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ ‡è®°æ¨ç†é”™è¯¯ï¼ŒåŒæ—¶SSRMsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å¯ä¿¡åº¦å’Œå®¡è®¡æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥è¯†åˆ«æ¨ç†ä¸­çš„é”™è¯¯å’Œåè§ï¼Œå¯¼è‡´ç»“æœä¸å¯é ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŠç»“æ„åŒ–æ¨ç†æ¨¡å‹ï¼ˆSSRMsï¼‰ï¼Œé€šè¿‡ç”ŸæˆåŠç»“æ„åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œæ ‡è®°æ¨ç†æ­¥éª¤åŠå…¶è¾“å…¥è¾“å‡ºï¼Œä»è€Œå®ç°è‡ªåŠ¨å®¡è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSRMsçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¨ç†ç”Ÿæˆæ¨¡å—å’Œå®¡è®¡æ¨¡å—ã€‚æ¨ç†ç”Ÿæˆæ¨¡å—è´Ÿè´£ç”Ÿæˆæ¨ç†è½¨è¿¹ï¼Œå®¡è®¡æ¨¡å—åˆ™å¯¹è¿™äº›è½¨è¿¹è¿›è¡Œåˆ†æï¼Œè¯†åˆ«æ½œåœ¨çš„æ¨ç†é”™è¯¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šSSRMsçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç”Ÿæˆçš„æ¨ç†è½¨è¿¹é‡‡ç”¨éå¯æ‰§è¡Œçš„Pythonè¯­æ³•ï¼Œä½¿å¾—å®¡è®¡è¿‡ç¨‹æ›´åŠ ç³»ç»ŸåŒ–å’Œè‡ªåŠ¨åŒ–ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„æ¨ç†æ¨¡å‹æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨ç†è½¨è¿¹çš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶é€šè¿‡å¤šç§å®¡è®¡æ–¹æ³•ï¼ˆå¦‚æ‰‹å·¥å®¡è®¡ã€LLMç”Ÿæˆå®¡è®¡å’Œå­¦ä¹ çš„å…¸å‹æ€§å®¡è®¡ï¼‰æ¥éªŒè¯æ¨ç†çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSSRMsåœ¨åäºŒä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸å…¶ä»–åŒç±»æ¨¡å‹ç›¸æ¯”ï¼Œå‡†ç¡®æ€§æœªå—å½±å“ï¼Œä¸”èƒ½å¤Ÿæœ‰æ•ˆæ ‡è®°æ¨ç†é”™è¯¯ã€‚å…·ä½“è€Œè¨€ï¼Œæ‰€æœ‰ä¸‰ç§å®¡è®¡æ–¹æ³•å‡èƒ½æœ‰æ•ˆè¯†åˆ«æ¨ç†ç¼ºé™·ï¼Œæ˜¾ç¤ºå‡ºSSRMsçš„å¼ºå¤§å®¡è®¡èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å®¡è®¡èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨æ•æ„Ÿé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ç­‰ï¼‰çš„åº”ç”¨å¯ä¿¡åº¦ï¼Œè¿›è€Œæ¨åŠ¨å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although Large Language Models (LLMs) have become capable reasoners, the problem of faithfulness persists: their reasoning can contain errors and omissions that are difficult to detect and that may obscure biases in model outputs. To address this issue, we introduce Semi-Structured Reasoning Models (SSRMs), which are trained to produce semi-structured representations of reasoning. SSRMs generate reasoning traces in a non-executable Pythonic syntax that names each reasoning step and marks its inputs and outputs. This structure allows SSRM traces to be automatically audited to identify reasoning flaws. We evaluate three types of audits: hand-crafted structured reasoning audits, written in a domain-specific language (DSL) implemented in Python; LLM-generated structured reasoning audits; and learned typicality audits, which apply probabilistic models over reasoning traces. We show that all of these methods can be used to effectively flag probable reasoning errors. Importantly, the auditability of SSRMs does not appear to compromise overall accuracy: in evaluation on twelve benchmarks and two model families, SSRMs demonstrate strong performance and generalizability relative to other models of comparable size.

