---
layout: default
title: "Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model"
---

# Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24355" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24355v1</a>
  <a href="https://arxiv.org/pdf/2505.24355.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24355v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24355v1', 'Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sihan Tan, Taro Miyazaki, Kazuhiro Nakadai

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šè¯­è¨€æ— æ³¨é‡Šæ‰‹è¯­ç¿»è¯‘æ¨¡å‹ä»¥è§£å†³ä½èµ„æºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰‹è¯­ç¿»è¯‘` `å¤šè¯­è¨€æ¨¡å‹` `æ— æ³¨é‡Šå­¦ä¹ ` `CTCæŸå¤±` `è¯­è¨€å¯¹é½` `ä½èµ„æºé—®é¢˜` `å£è¯­ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰‹è¯­ç¿»è¯‘æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸€æ‰‹è¯­ä¸å•ä¸€å£è¯­çš„ç¿»è¯‘ï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€èµ„æºçš„åˆ©ç”¨ï¼Œå¯¼è‡´ä½èµ„æºé—®é¢˜ä¸¥é‡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šè¯­è¨€æ— æ³¨é‡Šæ¨¡å‹ï¼Œé‡‡ç”¨åŒCTCç›®æ ‡ï¼Œæ—¨åœ¨è§£å†³æ‰‹è¯­ä¸å£è¯­ä¹‹é—´çš„å¯¹é½å›°éš¾ï¼Œå®ç°å¤šç§æ‰‹è¯­çš„ç¿»è¯‘ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨å¤šè¯­è¨€SP-10ã€PHOENIX14Tå’ŒCSL-DailyåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰‹è¯­ç¿»è¯‘ï¼ˆSLTï¼‰æ—¨åœ¨å°†æ‰‹è¯­è§†é¢‘è½¬æ¢ä¸ºå£è¯­æ–‡æœ¬ï¼Œä»è€Œå¼¥åˆæ‰‹è¯­ä¸å£è¯­ç¤¾åŒºä¹‹é—´çš„æ²Ÿé€šé¸¿æ²Ÿã€‚å°½ç®¡ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€æ‰‹è¯­ä¸å•ä¸€å£è¯­çš„ç¿»è¯‘ä¸Šï¼Œä½†åˆ©ç”¨å¤šè¯­è¨€èµ„æºå¯ä»¥ç¼“è§£ä½èµ„æºé—®é¢˜å¹¶å¢å¼ºå¯åŠæ€§ã€‚ç„¶è€Œï¼Œç”±äºæ‰‹è¯­ä¸å£è¯­ä¹‹é—´çš„è¯­è¨€å†²çªå’Œå¯¹é½å›°éš¾ï¼Œå¤šè¯­è¨€æ‰‹è¯­ç¿»è¯‘ï¼ˆMLSLTï¼‰ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šè¯­è¨€æ— æ³¨é‡Šæ¨¡å‹ï¼Œé‡‡ç”¨åŒCTCç›®æ ‡è¿›è¡Œæ ‡è®°çº§æ‰‹è¯­è¯†åˆ«å’Œå£è¯­æ–‡æœ¬ç”Ÿæˆã€‚è¯¥æ¨¡å‹æ”¯æŒ10ç§æ‰‹è¯­ï¼Œèƒ½å¤Ÿå¤„ç†ä¸€å¯¹ä¸€ã€å¤šå¯¹ä¸€å’Œå¤šå¯¹å¤šçš„SLTä»»åŠ¡ï¼Œå¹¶åœ¨ä¸‰ä¸ªå¹¿æ³›é‡‡ç”¨çš„åŸºå‡†ä¸Šå®ç°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸åª²ç¾çš„æ€§èƒ½ï¼šå¤šè¯­è¨€SP-10ã€PHOENIX14Tå’ŒCSL-Dailyã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€æ‰‹è¯­ç¿»è¯‘ä¸­çš„è¯­è¨€å†²çªå’Œå¯¹é½å›°éš¾ï¼Œç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºå•ä¸€æ‰‹è¯­ä¸å£è¯­çš„ç¿»è¯‘ï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€ç¯å¢ƒçš„é€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§å¤šè¯­è¨€æ— æ³¨é‡Šæ¨¡å‹ï¼Œé€šè¿‡åŒCTCç›®æ ‡å®ç°æ ‡è®°çº§æ‰‹è¯­è¯†åˆ«å’Œå£è¯­æ–‡æœ¬ç”Ÿæˆï¼Œæ”¯æŒå¤šç§æ‰‹è¯­çš„ç¿»è¯‘ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ¨¡å‹åŒ…æ‹¬æ‰‹è¯­è§†é¢‘è¾“å…¥æ¨¡å—ã€åŒCTCç›®æ ‡æ¨¡å—å’Œå£è¯­æ–‡æœ¬ç”Ÿæˆæ¨¡å—ï¼Œæ•´ä½“æµç¨‹ä¸ºï¼šè¾“å…¥æ‰‹è¯­è§†é¢‘ï¼Œè¿›è¡Œæ‰‹è¯­è¯†åˆ«ï¼Œç”Ÿæˆå¯¹åº”çš„å£è¯­æ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæ— æ³¨é‡Šçš„å¤šè¯­è¨€æ¨¡å‹è®¾è®¡ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§æ‰‹è¯­ä¸å£è¯­çš„ç¿»è¯‘ä»»åŠ¡ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨åŒCTCæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–æ‰‹è¯­è¯†åˆ«å’Œæ–‡æœ¬ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šè€ƒè™‘äº†å¤šè¯­è¨€ç‰¹æ€§ï¼Œç¡®ä¿äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨å¤šè¯­è¨€SP-10ã€PHOENIX14Tå’ŒCSL-DailyåŸºå‡†ä¸Šå‡å–å¾—äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤šå¯¹å¤šç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç°äº†è‰¯å¥½çš„å®ç”¨æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€åŒ»ç–—å’Œç¤¾äº¤å¹³å°ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©è‹äººå’Œå¬åŠ›éšœç¢è€…æ›´å¥½åœ°ä¸ç¤¾ä¼šæ²Ÿé€šï¼Œæå‡ä»–ä»¬çš„ç”Ÿæ´»è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­æ¨å¹¿ï¼Œä¿ƒè¿›ä¸åŒè¯­è¨€æ–‡åŒ–ä¹‹é—´çš„äº¤æµä¸ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sign Language Translation (SLT) aims to convert sign language (SL) videos into spoken language text, thereby bridging the communication gap between the sign and the spoken community. While most existing works focus on translating a single sign language into a single spoken language (one-to-one SLT), leveraging multilingual resources could mitigate low-resource issues and enhance accessibility. However, multilingual SLT (MLSLT) remains unexplored due to language conflicts and alignment difficulties across SLs and spoken languages. To address these challenges, we propose a multilingual gloss-free model with dual CTC objectives for token-level SL identification and spoken text generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and many-to-many SLT tasks, achieving competitive performance compared to state-of-the-art methods on three widely adopted benchmarks: multilingual SP-10, PHOENIX14T, and CSL-Daily.

