---
layout: default
title: CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer
---

# CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24143" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24143v1</a>
  <a href="https://arxiv.org/pdf/2505.24143.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24143v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24143v1', 'CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinglong Gao, Xiao Ding, Lingxiao Zou, Bing Qin, Ting Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: 9 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCrossICLä»¥è§£å†³æ— ç›‘ç£ç¤ºèŒƒè½¬ç§»çš„ä»»åŠ¡é—´å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ— ç›‘ç£å­¦ä¹ ` `ç¤ºèŒƒè½¬ç§»` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä»»åŠ¡å¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¨ä»»åŠ¡å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ä¾èµ–äºäººå·¥æä¾›ç¤ºèŒƒï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œæ™®é€‚æ€§ã€‚
2. æœ¬æ–‡æå‡ºCrossICLï¼Œé€šè¿‡æ— ç›‘ç£ç¤ºèŒƒè½¬ç§»ï¼Œåˆ©ç”¨æºä»»åŠ¡çš„ç¤ºèŒƒæ¥æŒ‡å¯¼ç›®æ ‡ä»»åŠ¡ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossICLåœ¨875ä¸ªNLPä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœå’Œé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½é€šè¿‡ç¤ºèŒƒå¾—ä»¥æå‡ã€‚ç„¶è€Œï¼Œè·å–è¿™äº›ç¤ºèŒƒä¸»è¦ä¾èµ–äººå·¥åŠªåŠ›ã€‚åœ¨è®¸å¤šç°å®åœºæ™¯ä¸­ï¼Œç”¨æˆ·å¾€å¾€ä¸æ„¿æˆ–æ— æ³•æä¾›æ­¤ç±»ç¤ºèŒƒã€‚å—äººç±»ç±»æ¯”çš„å¯å‘ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä¸€ç§æ–°çš„ICLèŒƒå¼CrossICLï¼Œç ”ç©¶å¦‚ä½•åˆ©ç”¨ç°æœ‰æºä»»åŠ¡ç¤ºèŒƒä¸ºç›®æ ‡ä»»åŠ¡æä¾›å¯é æŒ‡å¯¼ï¼Œä»è€Œæ— éœ€é¢å¤–çš„äººå·¥åŠªåŠ›ã€‚ä¸ºæ­¤ï¼Œé¦–å…ˆè®¾è®¡äº†ä¸€ç§ä¸¤é˜¶æ®µå¯¹é½ç­–ç•¥ï¼Œä»¥å‡è½»ä»»åŠ¡é—´å·®è·å¸¦æ¥çš„å¹²æ‰°ï¼Œä½œä¸ºå®éªŒæ¢ç´¢çš„åŸºç¡€ã€‚åŸºäºæ­¤ï¼Œè¿›è¡Œäº†å¯¹CrossICLçš„å…¨é¢æ¢ç´¢ï¼Œæ¶µç›–875ä¸ªæ¥è‡ªSuper-NIåŸºå‡†çš„NLPä»»åŠ¡å’Œå…­ç§ç±»å‹çš„LLMsï¼ŒåŒ…æ‹¬GPT-4oã€‚å®éªŒç»“æœè¯æ˜äº†CrossICLçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºé€‰æ‹©è·¨ä»»åŠ¡ç¤ºèŒƒçš„æ ‡å‡†åŠä»»åŠ¡é—´å¹²æ‰°ç±»å‹ç­‰é—®é¢˜æä¾›äº†å®è´µçš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å¯¹äººå·¥ç¤ºèŒƒçš„ä¾èµ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´ç”¨æˆ·æä¾›ç¤ºèŒƒçš„å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCrossICLçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æ— ç›‘ç£ç¤ºèŒƒè½¬ç§»ï¼Œåˆ©ç”¨å·²æœ‰çš„æºä»»åŠ¡ç¤ºèŒƒæ¥æŒ‡å¯¼ç›®æ ‡ä»»åŠ¡ï¼Œä»è€Œå‡å°‘å¯¹äººå·¥ç¤ºèŒƒçš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å¯¹æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡è¿›è¡Œå¯¹é½ï¼Œä»¥å‡è½»ä»»åŠ¡é—´çš„å¹²æ‰°ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯åˆ©ç”¨å¯¹é½åçš„ç¤ºèŒƒè¿›è¡Œç›®æ ‡ä»»åŠ¡çš„å­¦ä¹ å’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸¤é˜¶æ®µå¯¹é½ç­–ç•¥ï¼Œæœ‰æ•ˆå‡è½»äº†ä»»åŠ¡é—´å·®è·å¸¦æ¥çš„å¹²æ‰°ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šæœªå¾—åˆ°å……åˆ†è§£å†³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å¯¹é½ç®—æ³•å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æºä»»åŠ¡ç¤ºèŒƒèƒ½å¤Ÿæœ‰æ•ˆè½¬ç§»åˆ°ç›®æ ‡ä»»åŠ¡ï¼ŒåŒæ—¶å¯¹æ¨¡å‹çš„å‚æ•°è®¾ç½®è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æå‡å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCrossICLåœ¨875ä¸ªNLPä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†15%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨è·¨ä»»åŠ¡å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡å‡å°‘å¯¹äººå·¥ç¤ºèŒƒçš„ä¾èµ–ï¼ŒCrossICLèƒ½å¤Ÿåœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å®ç°é«˜æ•ˆçš„ä»»åŠ¡å­¦ä¹ ï¼Œæå‡æ¨¡å‹çš„é€‚åº”æ€§å’Œå®ç”¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-Context Learning (ICL) enhances the performance of large language models (LLMs) with demonstrations. However, obtaining these demonstrations primarily relies on manual effort. In most real-world scenarios, users are often unwilling or unable to provide such demonstrations. Inspired by the human analogy, we explore a new ICL paradigm CrossICL to study how to utilize existing source task demonstrations in the ICL for target tasks, thereby obtaining reliable guidance without any additional manual effort. To explore this, we first design a two-stage alignment strategy to mitigate the interference caused by gaps across tasks, as the foundation for our experimental exploration. Based on it, we conduct comprehensive exploration of CrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs, including GPT-4o. Experimental results demonstrate the effectiveness of CrossICL and provide valuable insights on questions like the criteria for selecting cross-task demonstrations, as well as the types of task-gap-induced interference in CrossICL.

