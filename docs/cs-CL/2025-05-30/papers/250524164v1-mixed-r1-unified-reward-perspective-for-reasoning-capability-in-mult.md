---
layout: default
title: "Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models"
---

# Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24164" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24164v1</a>
  <a href="https://arxiv.org/pdf/2505.24164.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24164v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24164v1', 'Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, Hao Fei

**åˆ†ç±»**: cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xushilin1/mixed-r1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMixed-R1æ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `å¼ºåŒ–å­¦ä¹ ` `æ··åˆå¥–åŠ±å‡½æ•°` `åè®­ç»ƒæ•°æ®é›†` `æ•°æ®å¼•æ“` `å¼€æ”¾å¼æ–‡æœ¬å“åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¾€å¾€ä¸“æ³¨äºå•ä¸€ä»»åŠ¡ï¼Œç¼ºä¹å¯¹å¤šæºå¤šæ¨¡æ€ä»»åŠ¡çš„ç»¼åˆåˆ©ç”¨ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›çš„æå‡å—é™ã€‚
2. æœ¬æ–‡æå‡ºMixed-R1æ¡†æ¶ï¼Œé€šè¿‡æ··åˆå¥–åŠ±å‡½æ•°å’Œåè®­ç»ƒæ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMixed-R1åœ¨å¤šä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å±•ç¤ºäº†æ¨ç†èƒ½åŠ›çš„å‡ºç°ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒï¼Œä½†è¿™äº›ç ”ç©¶é€šå¸¸é›†ä¸­äºç‰¹å®šä»»åŠ¡ï¼Œå¦‚åŸºç¡€ä»»åŠ¡ã€æ•°å­¦é—®é¢˜æˆ–å›¾è¡¨åˆ†æã€‚æœ¬æ–‡æå‡ºMixed-R1ï¼Œä¸€ä¸ªç»Ÿä¸€ä¸”ç®€å•çš„æ¡†æ¶ï¼ŒåŒ…å«æ··åˆå¥–åŠ±å‡½æ•°è®¾è®¡ï¼ˆMixed-Rewardï¼‰å’Œæ··åˆåè®­ç»ƒæ•°æ®é›†ï¼ˆMixed-45Kï¼‰ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ•°æ®å¼•æ“ï¼Œä»¥é€‰æ‹©é«˜è´¨é‡ç¤ºä¾‹æ„å»ºMixed-45Kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†å¤šç§å¥–åŠ±å‡½æ•°ä»¥é€‚åº”ä¸åŒçš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œæ•°æ®é›†å’Œæ¨¡å‹å¯åœ¨æŒ‡å®šé“¾æ¥è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æå‡æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºç‰¹å®šä»»åŠ¡ï¼Œç¼ºä¹å¯¹å¤šæºä»»åŠ¡çš„ç»¼åˆè€ƒè™‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMixed-R1æ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡æ··åˆå¥–åŠ±å‡½æ•°å’Œæ„å»ºæ··åˆåè®­ç»ƒæ•°æ®é›†ï¼Œæ—¨åœ¨å®ç°å¯¹å¤šç§ä»»åŠ¡çš„ç»Ÿä¸€ä¼˜åŒ–ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMixed-R1æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ··åˆå¥–åŠ±å‡½æ•°è®¾è®¡å’Œæ··åˆåè®­ç»ƒæ•°æ®é›†æ„å»ºã€‚æ•°æ®å¼•æ“ç”¨äºé€‰æ‹©é«˜è´¨é‡ç¤ºä¾‹ï¼ŒMixed-Rewardåˆ™é’ˆå¯¹ä¸åŒä»»åŠ¡è®¾è®¡å¤šç§å¥–åŠ±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†Bidirectional Max-Average Similarity (BMAS)å¥–åŠ±ï¼Œç”¨äºå¤„ç†å¼€æ”¾å¼æ–‡æœ¬å“åº”ï¼Œåˆ©ç”¨ç”Ÿæˆå“åº”ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„åµŒå…¥åŒ¹é…è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šæ··åˆå¥–åŠ±å‡½æ•°åŒ…æ‹¬å››ç§ä¸åŒçš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ†åˆ«é’ˆå¯¹äºŒå…ƒç­”æ¡ˆã€å›¾è¡¨æ•°æ®é›†ã€åŸºç¡€é—®é¢˜å’Œå¼€æ”¾å¼æ–‡æœ¬å“åº”ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMixed-R1åœ¨å¤šä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆã€æ•°æ®åˆ†æç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒMixed-R1æ¡†æ¶å¯ä¸ºå®é™…åº”ç”¨æä¾›æ›´é«˜çš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent works on large language models (LLMs) have successfully demonstrated the emergence of reasoning capabilities via reinforcement learning (RL). Although recent efforts leverage group relative policy optimization (GRPO) for MLLMs post-training, they constantly explore one specific aspect, such as grounding tasks, math problems, or chart analysis. There are no works that can leverage multi-source MLLM tasks for stable reinforcement learning. In this work, we present a unified perspective to solve this problem. We present Mixed-R1, a unified yet straightforward framework that contains a mixed reward function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K). We first design a data engine to select high-quality examples to build the Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which contains various reward functions for various MLLM tasks. In particular, it has four different reward functions: matching reward for binary answer or multiple-choice problems, chart reward for chart-aware datasets, IoU reward for grounding problems, and open-ended reward for long-form text responses such as caption datasets. To handle the various long-form text content, we propose a new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by leveraging tokenizer embedding matching between the generated response and the ground truth. Extensive experiments show the effectiveness of our proposed method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes. Our dataset and model are available at https://github.com/xushilin1/mixed-r1.

