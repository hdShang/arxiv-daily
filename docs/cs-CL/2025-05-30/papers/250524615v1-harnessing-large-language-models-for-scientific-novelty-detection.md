---
layout: default
title: Harnessing Large Language Models for Scientific Novelty Detection
---

# Harnessing Large Language Models for Scientific Novelty Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24615" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24615v1</a>
  <a href="https://arxiv.org/pdf/2505.24615.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24615v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24615v1', 'Harnessing Large Language Models for Scientific Novelty Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yan Liu, Zonglin Yang, Soujanya Poria, Thanh-Son Nguyen, Erik Cambria

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: 15 pages, 3 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³ç§‘å­¦æ–°é¢–æ€§æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç§‘å­¦æ–°é¢–æ€§æ£€æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•°æ®é›†æ„å»º` `è½»é‡çº§æ£€ç´¢å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•åœ¨ç§‘å­¦æ–°é¢–æ€§æ£€æµ‹ä¸­ç¼ºä¹åˆé€‚çš„åŸºå‡†æ•°æ®é›†ï¼Œä¸”ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼æ€§æ£€ç´¢æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–è®ºæ–‡å…³ç³»å¹¶æ€»ç»“æ€æƒ³ï¼ŒåŒæ—¶è®­ç»ƒè½»é‡çº§æ£€ç´¢å™¨ä»¥æé«˜æ–°é¢–æ€§æ£€æµ‹çš„æ•ˆç‡ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šå®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ–°é¢–æ€§æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç§‘å­¦å¿«é€Ÿå‘å±•çš„æ—¶ä»£ï¼Œè¯†åˆ«æ–°é¢–çš„ç ”ç©¶æ€æƒ³è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æ— æ³•æœ‰æ•ˆè§£å†³æ–‡æœ¬ç›¸ä¼¼æ€§ä¸æ€æƒ³æ„æƒ³ä¹‹é—´çš„å·®è·ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç§‘å­¦æ–°é¢–æ€§æ£€æµ‹ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªæ–°çš„æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡æå–è®ºæ–‡é—´çš„å…³ç³»å’Œæ€»ç»“å…¶ä¸»è¦æ€æƒ³æ¥æ„å»ºæœ‰æ•ˆçš„æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è½»é‡çº§æ£€ç´¢å™¨ï¼Œé€šè¿‡ä»LLMsä¸­æç‚¼æ€æƒ³çº§çŸ¥è¯†æ¥æé«˜æ–°é¢–æ€§æ£€æµ‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–°é¢–æ€§æ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç§‘å­¦æ–°é¢–æ€§æ£€æµ‹ä¸­çš„æ•°æ®é›†ç¼ºä¹å’Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰æ€æƒ³æ„æƒ³çš„é—®é¢˜ã€‚ç°æœ‰çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯åœ¨æ–‡æœ¬ç›¸ä¼¼æ€§ä¸æ€æƒ³æ„æƒ³ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¯¼è‡´æ–°é¢–æ€§æ£€æµ‹æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è¿›è¡Œç§‘å­¦æ–°é¢–æ€§æ£€æµ‹ï¼Œæ ¸å¿ƒåœ¨äºé€šè¿‡æå–è®ºæ–‡é—´çš„å…³ç³»å¹¶æ€»ç»“å…¶ä¸»è¦æ€æƒ³ï¼Œæ„å»ºé€‚åˆæ–°é¢–æ€§æ£€æµ‹çš„æ•°æ®é›†ã€‚åŒæ—¶ï¼Œè®¾è®¡è½»é‡çº§æ£€ç´¢å™¨ä»¥ä»LLMsä¸­æç‚¼æ€æƒ³çº§çŸ¥è¯†ï¼Œæå‡æ£€ç´¢çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æ•°æ®é›†æ„å»ºæ¨¡å—ï¼Œé€šè¿‡æå–è®ºæ–‡å…³ç³»å’Œæ€»ç»“æ€æƒ³æ¥ç”Ÿæˆæ•°æ®é›†ï¼›å…¶æ¬¡æ˜¯æ–°é¢–æ€§æ£€æµ‹æ¨¡å—ï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„è½»é‡çº§æ£€ç´¢å™¨è¿›è¡Œæ€æƒ³æ£€ç´¢å’Œæ–°é¢–æ€§æ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ç»“åˆLLMsè¿›è¡Œæ–°é¢–æ€§æ£€æµ‹çš„æ¡†æ¶ï¼Œå¹¶é€šè¿‡è½»é‡çº§æ£€ç´¢å™¨å®ç°æ€æƒ³çº§çŸ¥è¯†çš„æç‚¼ä¸åº”ç”¨ï¼Œè¿™ä¸ä¼ ç»Ÿçš„æ–‡æœ¬ç›¸ä¼¼æ€§æ£€ç´¢æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ£€ç´¢å™¨çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„ç½‘ç»œç»“æ„æ¥ç¡®ä¿æ¨¡å‹çš„è½»é‡åŒ–ä¸é«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ–°é¢–æ€§æ£€æµ‹ä»»åŠ¡ä¸Šç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ä¸¤ä¸ªæ–°æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†15%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å­¦æœ¯ç ”ç©¶ã€ç§‘æŠ€åˆ›æ–°å’Œå¸‚åœºè¥é”€ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å¿«é€Ÿè¯†åˆ«æ–°é¢–çš„ç ”ç©¶æ€æƒ³ï¼Œæ¨åŠ¨ç§‘å­¦è¿›æ­¥å’ŒæŠ€æœ¯å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å…¶ä»–é¢†åŸŸçš„åˆ›æ–°æ£€æµ‹ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.

