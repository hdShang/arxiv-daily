---
layout: default
title: Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models
---

# Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24630" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24630v2</a>
  <a href="https://arxiv.org/pdf/2505.24630.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24630v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24630v2', 'Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junyi Li, Hwee Tou Ng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-11-06)

**å¤‡æ³¨**: accepted by NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº‹å®æ„è¯†çš„é€æ­¥ç­–ç•¥ä¼˜åŒ–ä»¥è§£å†³æ¨ç†æ¨¡å‹å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†æ¨¡å‹` `äº‹å®éªŒè¯` `å¹»è§‰é—®é¢˜` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–` `æ¨¡å‹å¯é æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†å¯¼å‘å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¾è‘—å¢åŠ äº†æ¨¡å‹äº§ç”Ÿå¹»è§‰çš„æ¦‚ç‡ï¼Œå½±å“äº†æ¨ç†çš„å¯é æ€§ã€‚
2. è®ºæ–‡æå‡ºçš„FSPOç®—æ³•é€šè¿‡åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­å¼•å…¥æ˜¾å¼çš„äº‹å®éªŒè¯ï¼ŒåŠ¨æ€è°ƒæ•´æ¨ç†è¿‡ç¨‹ä¸­çš„ä¼˜åŠ¿å€¼ï¼Œä»¥ç¡®ä¿äº‹å®çš„æ­£ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFSPOåœ¨æ•°å­¦æ¨ç†å’Œå¹»è§‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†å¹»è§‰å‘ç”Ÿç‡ï¼Œå¹¶æé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æˆ‘ä»¬çš„å®è¯åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼šæ¨ç†å¯¼å‘çš„RLå¾®è°ƒæ˜¾è‘—å¢åŠ äº†å¹»è§‰çš„å‘ç”Ÿç‡ã€‚æˆ‘ä»¬ç†è®ºåˆ†æäº†RLè®­ç»ƒåŠ¨æ€ï¼Œè¯†åˆ«å‡ºé«˜æ–¹å·®æ¢¯åº¦ã€ç†µå¼•èµ·çš„éšæœºæ€§ä»¥åŠå¯¹è™šå‡å±€éƒ¨æœ€ä¼˜çš„æ•æ„Ÿæ€§æ˜¯å¯¼è‡´å¹»è§‰çš„å…³é”®å› ç´ ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†äº‹å®æ„è¯†çš„é€æ­¥ç­–ç•¥ä¼˜åŒ–ï¼ˆFSPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„RLå¾®è°ƒç®—æ³•ï¼Œåœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­ç»“åˆæ˜¾å¼çš„äº‹å®éªŒè¯ã€‚FSPOåˆ©ç”¨è‡ªåŠ¨åŒ–éªŒè¯ä¸ç»™å®šè¯æ®è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œæ¿€åŠ±æ¨ç†è¿‡ç¨‹ä¸­çš„äº‹å®æ­£ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFSPOæœ‰æ•ˆå‡å°‘äº†å¹»è§‰ï¼ŒåŒæ—¶æé«˜äº†æ¨ç†å‡†ç¡®æ€§ï¼Œæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„å¯é æ€§å’Œæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­å› å¼ºåŒ–å­¦ä¹ å¾®è°ƒè€Œå¯¼è‡´çš„å¹»è§‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°é«˜æ–¹å·®æ¢¯åº¦å’Œå±€éƒ¨æœ€ä¼˜çš„å½±å“ï¼Œå¯¼è‡´ç”Ÿæˆä¸å‡†ç¡®çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥äº‹å®æ„è¯†çš„é€æ­¥ç­–ç•¥ä¼˜åŒ–ï¼ˆFSPOï¼‰ï¼Œé€šè¿‡åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­è¿›è¡Œæ˜¾å¼çš„äº‹å®éªŒè¯ï¼ŒåŠ¨æ€è°ƒæ•´æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä»¥æé«˜ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFSPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆæ˜¯æ¨ç†æ­¥éª¤çš„ç”Ÿæˆæ¨¡å—ï¼Œå…¶æ¬¡æ˜¯äº‹å®éªŒè¯æ¨¡å—ï¼Œæœ€åæ˜¯åŸºäºéªŒè¯ç»“æœçš„ä¼˜åŠ¿å€¼è°ƒæ•´æ¨¡å—ã€‚è¯¥æ¡†æ¶ç¡®ä¿æ¯ä¸€æ­¥æ¨ç†éƒ½ç»è¿‡éªŒè¯ï¼Œä»è€Œå‡å°‘å¹»è§‰çš„äº§ç”Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šFSPOçš„æœ€é‡è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€çš„äº‹å®éªŒè¯æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„RLå¾®è°ƒæ–¹æ³•ä¸åŒï¼Œåè€…é€šå¸¸ç¼ºä¹å¯¹ç”Ÿæˆå†…å®¹çš„å®æ—¶éªŒè¯ï¼Œå®¹æ˜“å¯¼è‡´å¹»è§‰ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨FSPOä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ä¼˜åŠ¿å€¼çš„è®¡ç®—æ–¹å¼å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç¡®ä¿åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¿€åŠ±æ¨¡å‹ç”Ÿæˆäº‹å®æ­£ç¡®çš„å†…å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFSPOåœ¨æ•°å­¦æ¨ç†å’Œå¹»è§‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†å¹»è§‰å‘ç”Ÿç‡ï¼Œæ¨ç†å‡†ç¡®æ€§æé«˜äº†çº¦15%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒFSPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæ›´é«˜çš„å¯é æ€§å’Œæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ³•å¾‹å’ŒåŒ»ç–—ç­‰éœ€è¦é«˜å‡†ç¡®æ€§æ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡å‡å°‘å¹»è§‰çš„å‘ç”Ÿï¼ŒFSPOå¯ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œè¿›è€Œå½±å“å†³ç­–æ”¯æŒç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ¨ç†å·¥å…·çš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.

