---
layout: default
title: CLaSp: In-Context Layer Skip for Self-Speculative Decoding
---

# CLaSp: In-Context Layer Skip for Self-Speculative Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24196" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24196v1</a>
  <a href="https://arxiv.org/pdf/2505.24196.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24196v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24196v1', 'CLaSp: In-Context Layer Skip for Self-Speculative Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Longze Chen, Renke Shan, Huiming Wang, Lu Wang, Ziqiang Liu, Run Luo, Jiawei Wang, Hamid Alinejad-Rokny, Min Yang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: 11 pages, 7 figures, ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLaSpä»¥è§£å†³è‡ªæˆ‘æ¨æµ‹è§£ç ä¸­çš„å±‚è·³è¿‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘æ¨æµ‹è§£ç ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å±‚è·³è¿‡` `åŠ¨æ€è§„åˆ’` `æ–‡æœ¬ç”Ÿæˆ` `è§£ç æ•ˆç‡` `å‹ç¼©æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªæˆ‘æ¨æµ‹è§£ç æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„æ¨¡å—è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´å®ç°å¤æ‚ä¸”éš¾ä»¥å…¼å®¹ä¸åŒçš„LLMsã€‚
2. CLaSpæå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡å±‚è·³è¿‡ç­–ç•¥ï¼Œé€šè¿‡è·³è¿‡éªŒè¯æ¨¡å‹çš„ä¸­é—´å±‚æ¥æ„å»ºå‹ç¼©è‰ç¨¿æ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLaSpåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†1.3xè‡³1.7xçš„é€Ÿåº¦æå‡ï¼Œä¸”ä¸å½±å“ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªæˆ‘æ¨æµ‹è§£ç ï¼ˆSDï¼‰æ˜¯ä¸€ç§åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£ç è¿‡ç¨‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå…¶æ•ˆç‡ä¸»è¦ä¾èµ–äºè‰ç¨¿æ¨¡å‹ä¸éªŒè¯æ¨¡å‹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‰æ‹Ÿæ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„æ¨¡å—è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨å®ç°å’Œå…¼å®¹æ€§ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†CLaSpï¼Œä¸€ç§ç”¨äºè‡ªæˆ‘æ¨æµ‹è§£ç çš„ä¸Šä¸‹æ–‡å±‚è·³è¿‡ç­–ç•¥ã€‚CLaSpä¸éœ€è¦é¢å¤–çš„è‰æ‹Ÿæ¨¡å—æˆ–è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡è·³è¿‡éªŒè¯æ¨¡å‹çš„ä¸­é—´å±‚æ¥æ„å»ºå‹ç¼©çš„è‰ç¨¿æ¨¡å‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œä¼˜åŒ–å±‚è·³è¿‡è¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ¯ä¸ªéªŒè¯é˜¶æ®µååŠ¨æ€è°ƒæ•´è·³è¿‡ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLaSpåœ¨LLaMA3ç³»åˆ—æ¨¡å‹ä¸Šå®ç°äº†1.3xè‡³1.7xçš„åŠ é€Ÿï¼ŒåŒæ—¶ä¸æ”¹å˜ç”Ÿæˆæ–‡æœ¬çš„åŸå§‹åˆ†å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªæˆ‘æ¨æµ‹è§£ç ä¸­è‰ç¨¿æ¨¡å‹ä¸éªŒè¯æ¨¡å‹ä¸€è‡´æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºé¢å¤–çš„è®­ç»ƒæ¨¡å—ï¼Œå¯¼è‡´å®ç°å¤æ‚ä¸”éš¾ä»¥é€‚åº”ä¸åŒçš„LLMsã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCLaSpé€šè¿‡ä¸Šä¸‹æ–‡å±‚è·³è¿‡ç­–ç•¥ï¼Œé¿å…äº†é¢å¤–çš„è‰æ‹Ÿæ¨¡å—å’Œè®­ç»ƒï¼Œé‡‡ç”¨å³æ’å³ç”¨çš„æœºåˆ¶æ¥æ„å»ºå‹ç¼©è‰ç¨¿æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLaSpçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¼˜åŒ–å±‚è·³è¿‡è¿‡ç¨‹ï¼Œåˆ©ç”¨æœ€åéªŒè¯é˜¶æ®µçš„å®Œæ•´éšè—çŠ¶æ€ä½œä¸ºç›®æ ‡ï¼ŒåŠ¨æ€è°ƒæ•´è·³è¿‡ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLaSpçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶åŠ¨æ€è°ƒæ•´å±‚è·³è¿‡ç­–ç•¥çš„èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–é¢„å…ˆä¼˜åŒ–çš„è·³è¿‡å±‚é›†åˆï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•å½¢æˆäº†æ˜¾è‘—åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCLaSpé€šè¿‡åŠ¨æ€è§„åˆ’ç®—æ³•å®ç°äº†å¯¹å±‚è·³è¿‡çš„ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨æ¯ä¸ªéªŒè¯é˜¶æ®µåéƒ½èƒ½çµæ´»è°ƒæ•´è·³è¿‡çš„å±‚ï¼Œæå‡äº†æ•´ä½“è§£ç æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCLaSpåœ¨LLaMA3ç³»åˆ—æ¨¡å‹ä¸Šå®ç°äº†1.3xè‡³1.7xçš„é€Ÿåº¦æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è§£ç æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ä¸å˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CLaSpçš„ç ”ç©¶æˆæœåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£ç è¿‡ç¨‹ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿç”Ÿæˆæ–‡æœ¬çš„åœºæ™¯ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨å†™ä½œå’Œå®æ—¶ç¿»è¯‘ç­‰ã€‚å…¶é«˜æ•ˆçš„è§£ç ç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿçš„å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.

