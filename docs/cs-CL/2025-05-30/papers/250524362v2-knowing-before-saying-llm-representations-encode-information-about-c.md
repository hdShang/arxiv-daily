---
layout: default
title: "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion"
---

# Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24362" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24362v2</a>
  <a href="https://arxiv.org/pdf/2505.24362.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24362v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24362v2', 'Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anum Afzal, Florian Matthes, Gal Chechik, Yftah Ziser

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMè¡¨ç¤ºçš„åˆ†ç±»å™¨ä»¥é¢„æµ‹Chain-of-ThoughtæˆåŠŸæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Chain-of-Thought` `é›¶-shotå­¦ä¹ ` `æ¨ç†è¿‡ç¨‹` `LLMè¡¨ç¤º` `åˆ†ç±»å™¨` `æ—©æœŸåœæ­¢` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨Chain-of-Thoughtæ¨ç†è¿‡ç¨‹ä¸­ï¼ŒæˆåŠŸæ€§é¢„æµ‹ä¾èµ–äºç”Ÿæˆçš„tokenï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMè¡¨ç¤ºçš„æ¢æµ‹åˆ†ç±»å™¨ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆtokenä¹‹å‰é¢„æµ‹æ¨ç†æˆåŠŸæ€§ï¼Œæå‡äº†æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ—©æœŸåœæ­¢æ¨ç†ä»èƒ½æé«˜æ€§èƒ½ï¼Œä¸”åŸºäºLLMçš„åˆ†ç±»å™¨åœ¨æ—©æœŸé˜¶æ®µå°±èƒ½æœ‰æ•ˆæ•æ‰å…³é”®ä¿¡æ¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å®Œæˆä¹‹å‰æ˜¯å¦å¯ä»¥é¢„æµ‹é›¶-shot Chain-of-Thought (CoT) è¿‡ç¨‹çš„æˆåŠŸæ€§ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºLLMè¡¨ç¤ºçš„æ¢æµ‹åˆ†ç±»å™¨åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªtokenä¹‹å‰å°±èƒ½è¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜æ¨ç†è¿‡ç¨‹ä¸­çš„å…³é”®ä¿¡æ¯åœ¨åˆå§‹æ­¥éª¤çš„è¡¨ç¤ºä¸­å·²ç»å­˜åœ¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¾èµ–ç”Ÿæˆtokençš„å¼ºBERTåŸºçº¿è¡¨ç°è¾ƒå·®ï¼Œå¯èƒ½æ˜¯å› ä¸ºå®ƒä¾èµ–äºæµ…å±‚è¯­è¨€çº¿ç´¢è€Œéæ·±å±‚æ¨ç†åŠ¨æ€ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æ—©æœŸåœæ­¢æ¨ç†ä»èƒ½æå‡æ€§èƒ½ï¼Œä½†ä¸å®Œæ•´æ¨ç†ç›¸æ¯”ä»å­˜åœ¨å·®è·ã€‚è¿™äº›å‘ç°ä¸ºä¼˜åŒ–CoTçš„æ•ˆç‡æä¾›äº†é‡è¦è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨Chain-of-Thoughtæ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•åœ¨ç”Ÿæˆtokenä¹‹å‰é¢„æµ‹æ¨ç†æˆåŠŸæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–ç”Ÿæˆçš„tokenï¼Œå¯¼è‡´æ€§èƒ½å—é™ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨åˆå§‹ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMè¡¨ç¤ºæ„å»ºæ¢æµ‹åˆ†ç±»å™¨ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†å¼€å§‹æ—¶å°±æ•æ‰åˆ°å…³é”®ä¿¡æ¯ï¼Œä»è€Œé¢„æµ‹æ¨ç†çš„æˆåŠŸæ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å‡å°‘å¯¹ç”Ÿæˆtokençš„ä¾èµ–ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬LLMè¡¨ç¤ºçš„æå–ã€æ¢æµ‹åˆ†ç±»å™¨çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬åˆå§‹è¡¨ç¤ºçš„è·å–ã€åˆ†ç±»å™¨çš„æ„å»ºå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§èƒ½å¤Ÿåœ¨æ¨ç†æ—©æœŸé˜¶æ®µè¿›è¡ŒæˆåŠŸæ€§é¢„æµ‹çš„åˆ†ç±»å™¨ï¼Œä¸ä¾èµ–ç”Ÿæˆtokençš„ä¼ ç»Ÿæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´æ—©åœ°åˆ©ç”¨æ¨ç†ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œè®¾ç½®åˆç†çš„è¶…å‚æ•°ä»¥ç¡®ä¿åˆ†ç±»å™¨åœ¨ä¸åŒæ¨ç†é˜¶æ®µçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠè®¾è®¡ç½‘ç»œç»“æ„ä»¥å……åˆ†åˆ©ç”¨LLMçš„è¡¨ç¤ºèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ¢æµ‹åˆ†ç±»å™¨åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªtokenä¹‹å‰å°±èƒ½æœ‰æ•ˆé¢„æµ‹æ¨ç†æˆåŠŸæ€§ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„BERTåŸºçº¿ã€‚æ—©æœŸåœæ­¢æ¨ç†çš„å®éªŒæ˜¾ç¤ºï¼Œå°½ç®¡æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†ä¸å®Œæ•´æ¨ç†ç›¸æ¯”ä»å­˜åœ¨ä¸€å®šå·®è·ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡ä¸Šå…·æœ‰æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ¨ç†ä»»åŠ¡ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–Chain-of-Thoughtçš„æ¨ç†æ•ˆç‡ï¼Œå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æå‡ç³»ç»Ÿçš„å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.

