---
layout: default
title: "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
---

# R-KV: Redundancy-aware KV Cache Compression for Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24133" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24133v3</a>
  <a href="https://arxiv.org/pdf/2505.24133.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24133v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24133v3', 'R-KV: Redundancy-aware KV Cache Compression for Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR-KVä»¥è§£å†³æ¨ç†æ¨¡å‹ä¸­çš„å†—ä½™KVç¼“å­˜å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†æ¨¡å‹` `KVç¼“å­˜` `å‹ç¼©æŠ€æœ¯` `å†—ä½™æ ‡è®°` `æ€§èƒ½ä¼˜åŒ–` `å†…å­˜ç®¡ç†` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•åœ¨æ¨ç†æ¨¡å‹ä¸­æ— æ³•æœ‰æ•ˆå¤„ç†å†—ä½™æ ‡è®°ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œæ¨ç†å¤±è´¥ã€‚
2. æœ¬æ–‡æå‡ºR-KVæ–¹æ³•ï¼Œä¸“æ³¨äºè¯†åˆ«å’Œå‹ç¼©æ¨ç†æ¨¡å‹ä¸­çš„å†—ä½™æ ‡è®°ï¼Œä»è€Œæé«˜KVç¼“å­˜çš„ä½¿ç”¨æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒR-KVåœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†æ¨¡å‹åœ¨è‡ªæˆ‘åæ€å’Œé“¾å¼æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸äº§ç”Ÿè¿‡é•¿çš„è¾“å‡ºï¼Œå¯¼è‡´æ¨ç†æ—¶çš„é”®å€¼(KV)ç¼“å­˜è¿‡å¤§ã€‚ç°æœ‰çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“å¯¼è‡´æ¨ç†å¤±è´¥ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ¨ç†æ¨¡å‹å†—ä½™æ ‡è®°çš„R-KVç¼“å­˜å‹ç¼©æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨10%çš„KVç¼“å­˜çš„æƒ…å†µä¸‹ï¼Œå‡ ä¹ä¿ç•™äº†100%çš„å®Œæ•´KVç¼“å­˜æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œåè€…ä»…èƒ½è¾¾åˆ°60%çš„æ€§èƒ½ã€‚R-KVåœ¨ä½¿ç”¨16%çš„KVç¼“å­˜æ—¶ç”šè‡³å®ç°äº†105%çš„å®Œæ•´KVç¼“å­˜æ€§èƒ½ï¼Œå¸¦æ¥äº†90%çš„å†…å­˜èŠ‚çœå’Œ6.6å€çš„ååé‡æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-KVåœ¨ä¸¤ä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„KVç¼“å­˜å‹ç¼©åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨ç†æ¨¡å‹ä¸­KVç¼“å­˜è¿‡å¤§å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å†—ä½™æ ‡è®°æ—¶æ•ˆæœä¸ä½³ï¼Œå½±å“æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šR-KVé€šè¿‡è¯†åˆ«æ¨ç†æ¨¡å‹ä¸­çš„å†—ä½™æ ‡è®°ï¼Œä¼˜åŒ–KVç¼“å­˜çš„ä½¿ç”¨ï¼Œæ—¨åœ¨åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR-KVçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å†—ä½™æ ‡è®°æ£€æµ‹æ¨¡å—ã€KVç¼“å­˜å‹ç¼©æ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚é¦–å…ˆæ£€æµ‹å†—ä½™æ ‡è®°ï¼Œç„¶åè¿›è¡Œç¼“å­˜å‹ç¼©ï¼Œæœ€åè¯„ä¼°æ€§èƒ½æå‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šR-KVçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å†—ä½™æ„ŸçŸ¥æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æå¤§å‹ç¼©KVç¼“å­˜çš„åŒæ—¶ï¼Œä¿æŒæ¥è¿‘å®Œæ•´çš„æ¨ç†æ€§èƒ½ï¼Œè¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šR-KVåœ¨å‚æ•°è®¾ç½®ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å‹ç¼©ç‡ä¸æ€§èƒ½ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™å¼•å…¥äº†å†—ä½™æ ‡è®°çš„åŠ¨æ€è¯†åˆ«æœºåˆ¶ï¼Œä»¥æé«˜å‹ç¼©æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

R-KVåœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œä»…ä½¿ç”¨10%çš„KVç¼“å­˜ä¾¿ä¿ç•™äº†è¿‘100%çš„æ€§èƒ½ï¼Œä½¿ç”¨16%æ—¶ç”šè‡³è¾¾åˆ°äº†105%çš„æ€§èƒ½æå‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç°æœ‰åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ä»…ä¸º60%ã€‚æ­¤å¤–ï¼ŒR-KVè¿˜å®ç°äº†90%çš„å†…å­˜èŠ‚çœå’Œ6.6å€çš„ååé‡æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R-KVæ–¹æ³•åœ¨æ¨ç†æ¨¡å‹çš„åº”ç”¨åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå†…å­˜ç®¡ç†å’Œå¿«é€Ÿæ¨ç†çš„é¢†åŸŸï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¤æ‚å†³ç­–æ”¯æŒç³»ç»Ÿã€‚å…¶æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œå†…å­˜èŠ‚çœå°†æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.

