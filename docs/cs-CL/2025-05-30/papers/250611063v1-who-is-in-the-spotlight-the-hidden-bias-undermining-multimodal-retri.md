---
layout: default
title: Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation
---

# Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11063" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11063v1</a>
  <a href="https://arxiv.org/pdf/2506.11063.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11063v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11063v1', 'Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, Xueqi Cheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½ç½®æ•æ„Ÿæ€§æŒ‡æ•°ä»¥è§£å†³å¤šæ¨¡æ€RAGç³»ç»Ÿä¸­çš„åè§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `ç”Ÿæˆæ¨¡å‹` `ä½ç½®åè§` `å…¬å¹³æ€§` `çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡` `å¯è§†åŒ–åˆ†æ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€RAGç³»ç»Ÿå¯¹è¯æ®é¡ºåºçš„æ•æ„Ÿæ€§å¯¼è‡´äº†ä¸ç¨³å®šçš„æ€§èƒ½å’Œåè§æ¨ç†ï¼Œå°¤å…¶æ˜¯åœ¨æ£€ç´¢é¡¹æ•°é‡æˆ–æ¨¡æ€å¤šæ ·æ€§å¢åŠ æ—¶ã€‚
2. æœ¬æ–‡é€šè¿‡å¼•å…¥ä½ç½®æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆPSI_pï¼‰å’Œå¯è§†åŒ–æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†è¯æ®ä½ç½®å¯¹å¤šæ¨¡æ€RAGæ€§èƒ½çš„å½±å“ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½ç½®åè§åœ¨å¤šæ¨¡æ€è®¾ç½®ä¸­æ¯”å•æ¨¡æ€è®¾ç½®æ›´ä¸ºä¸¥é‡ï¼Œå¹¶ä¸”éšç€æ£€ç´¢èŒƒå›´çš„å¢åŠ ï¼Œåè§å‘ˆå¯¹æ•°å¢é•¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨çŸ¥è¯†å¯†é›†å‹å’Œå¼€æ”¾é¢†åŸŸä»»åŠ¡ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰RAGæ¨¡å‹å¯¹è¯æ®å‘ˆç°é¡ºåºé«˜åº¦æ•æ„Ÿï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šå’Œæ¨ç†åè§ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢ç ”ç©¶äº†å¤šæ¨¡æ€RAGç³»ç»Ÿä¸­çš„ä½ç½®åè§ï¼Œé€šè¿‡æ§åˆ¶å®éªŒè§‚å¯Ÿåˆ°è¯æ®ä½ç½®ä¸å‡†ç¡®ç‡ä¹‹é—´çš„Uå‹æ›²çº¿ã€‚ä¸ºé‡åŒ–è¿™ç§åè§ï¼Œæå‡ºäº†ä½ç½®æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆPSI_pï¼‰ï¼Œå¹¶å¼€å‘äº†å¯è§†åŒ–æ¡†æ¶è¿½è¸ªè§£ç å™¨å±‚çš„æ³¨æ„åŠ›åˆ†é…æ¨¡å¼ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€äº¤äº’åŠ å‰§äº†ä½ç½®åè§ï¼Œå¹¶ä¸”åè§éšæ£€ç´¢èŒƒå›´çš„å¢åŠ è€Œå¯¹æ•°å¢é•¿ã€‚è¿™äº›å‘ç°ä¸ºRAGä¸­çš„ä½ç½®æ„ŸçŸ¥åˆ†ææä¾›äº†ç†è®ºå’Œå®è¯åŸºç¡€ï¼Œå¼ºè°ƒäº†è¯æ®é‡æ’åºæˆ–å»åç­–ç•¥çš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€RAGç³»ç»Ÿä¸­ç”±äºè¯æ®å‘ˆç°é¡ºåºå¯¼è‡´çš„æ€§èƒ½ä¸ç¨³å®šå’Œåè§æ¨ç†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†è¿™ä¸€æŒ‘æˆ˜ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„å…¬å¹³æ€§å’Œå¯é æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥ä½ç½®æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆPSI_pï¼‰æ¥é‡åŒ–ä½ç½®åè§ï¼Œå¹¶åˆ©ç”¨å¯è§†åŒ–å·¥å…·åˆ†æè§£ç å™¨å±‚çš„æ³¨æ„åŠ›åˆ†é…ï¼Œä»è€Œæ·±å…¥ç†è§£è¯æ®ä½ç½®å¯¹ç”Ÿæˆç»“æœçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨æ§åˆ¶å®éªŒè®¾è®¡ï¼Œæ¶µç›–æ–‡æœ¬ã€å›¾åƒå’Œæ··åˆæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡ä¸åŒä½ç½®çš„è¯æ®è¿›è¡Œå®éªŒï¼Œè§‚å¯Ÿå‡†ç¡®ç‡çš„å˜åŒ–ï¼Œå¹¶ä½¿ç”¨PSI_pè¿›è¡Œåè§é‡åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å¤šæ¨¡æ€RAGä¸­çš„ä½ç½®åè§ï¼Œå¹¶æå‡ºäº†é‡åŒ–å’Œå¯è§†åŒ–çš„å·¥å…·ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æ¨¡æ€åˆ†æå½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾è®¡äº†ä¸åŒçš„è¯æ®ä½ç½®ç»„åˆï¼Œå¹¶é€šè¿‡å¯¹æ¯”åˆ†æä¸åŒæ¨¡æ€ä¸‹çš„æ€§èƒ½å˜åŒ–ï¼Œä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€è¯æ®ä½ç½®çš„å˜åŒ–ï¼Œå‡†ç¡®ç‡å‘ˆç°Uå‹æ›²çº¿ï¼Œä¸”åœ¨å¤šæ¨¡æ€äº¤äº’ä¸­ä½ç½®åè§æ˜¾è‘—åŠ å‰§ã€‚é€šè¿‡å¼•å…¥PSI_pï¼Œç ”ç©¶é‡åŒ–äº†è¿™ä¸€åè§ï¼Œå¹¶ä¸ºåç»­çš„å»åç­–ç•¥æä¾›äº†ç†è®ºä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢å’Œå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€RAGç³»ç»Ÿçš„å…¬å¹³æ€§å’Œå¯é æ€§ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å‡†ç¡®å’Œå…¬æ­£çš„ç”Ÿæˆç»“æœï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Retrieval-Augmented Generation (RAG) systems have become essential in knowledge-intensive and open-domain tasks. As retrieval complexity increases, ensuring the robustness of these systems is critical. However, current RAG models are highly sensitive to the order in which evidence is presented, often resulting in unstable performance and biased reasoning, particularly as the number of retrieved items or modality diversity grows. This raises a central question: How does the position of retrieved evidence affect multimodal RAG performance? To answer this, we present the first comprehensive study of position bias in multimodal RAG systems. Through controlled experiments across text-only, image-only, and mixed-modality tasks, we observe a consistent U-shaped accuracy curve with respect to evidence position. To quantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and develop a visualization framework to trace attention allocation patterns across decoder layers. Our results reveal that multimodal interactions intensify position bias compared to unimodal settings, and that this bias increases logarithmically with retrieval range. These findings offer both theoretical and empirical foundations for position-aware analysis in RAG, highlighting the need for evidence reordering or debiasing strategies to build more reliable and equitable generation systems.

