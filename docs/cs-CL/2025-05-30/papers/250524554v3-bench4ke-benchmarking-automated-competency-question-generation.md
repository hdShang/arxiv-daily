---
layout: default
title: Bench4KE: Benchmarking Automated Competency Question Generation
---

# Bench4KE: Benchmarking Automated Competency Question Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24554" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24554v3</a>
  <a href="https://arxiv.org/pdf/2505.24554.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24554v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24554v3', 'Bench4KE: Benchmarking Automated Competency Question Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anna Sofia Lippolis, Minh Davide Ragagni, Paolo Ciancarini, Andrea Giovanni Nuzzolese, Valentina Presutti

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-12-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBench4KEä»¥è§£å†³çŸ¥è¯†å·¥ç¨‹è‡ªåŠ¨åŒ–è¯„ä¼°æ ‡å‡†åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†å·¥ç¨‹` `èƒ½åŠ›é—®é¢˜ç”Ÿæˆ` `åŸºå‡†æµ‹è¯•` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨åŒ–å·¥å…·` `è¯„ä¼°æ ‡å‡†åŒ–` `æœ¬ä½“å·¥ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„èƒ½åŠ›é—®é¢˜ç”Ÿæˆå·¥å…·ç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°ï¼Œå¯¼è‡´ç ”ç©¶ç»“æœéš¾ä»¥æ¯”è¾ƒå’Œå¤ç°ã€‚
2. Bench4KEæ˜¯ä¸€ä¸ªAPIåŸºç¡€çš„åŸºå‡†æµ‹è¯•ç³»ç»Ÿï¼Œæ—¨åœ¨ä¸ºè‡ªåŠ¨ç”Ÿæˆèƒ½åŠ›é—®é¢˜çš„å·¥å…·æä¾›æ ‡å‡†åŒ–è¯„ä¼°ã€‚
3. é€šè¿‡å¯¹6ä¸ªåŸºäºLLMçš„èƒ½åŠ›é—®é¢˜ç”Ÿæˆç³»ç»Ÿè¿›è¡Œæ¯”è¾ƒåˆ†æï¼ŒBench4KEå»ºç«‹äº†æœªæ¥ç ”ç©¶çš„åŸºå‡†çº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ä¸ºçŸ¥è¯†å·¥ç¨‹ï¼ˆKEï¼‰è‡ªåŠ¨åŒ–ç ”ç©¶å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚å°½ç®¡å·²æœ‰åŸºäºLLMçš„å·¥å…·ç”¨äºè‡ªåŠ¨ç”Ÿæˆèƒ½åŠ›é—®é¢˜ï¼ˆCQsï¼‰ï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œå½±å“äº†ç ”ç©¶çš„ä¸¥è°¨æ€§å’Œç»“æœçš„å¯æ¯”æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Bench4KEï¼Œä¸€ä¸ªåŸºäºAPIçš„å¯æ‰©å±•åŸºå‡†æµ‹è¯•ç³»ç»Ÿï¼Œä¸“æ³¨äºè‡ªåŠ¨ç”ŸæˆCQsçš„å·¥å…·è¯„ä¼°ã€‚Bench4KEæä¾›äº†æ¥è‡ª17ä¸ªçœŸå®ä¸–ç•Œæœ¬ä½“å·¥ç¨‹é¡¹ç›®çš„CQsæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ä¸€å¥—ç›¸ä¼¼æ€§åº¦é‡æ¥è¯„ä¼°ç”Ÿæˆçš„CQsè´¨é‡ã€‚æ­¤å¤–ï¼ŒBench4KEè¿˜æ”¯æŒå…¶ä»–KEè‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œå¦‚SPARQLæŸ¥è¯¢ç”Ÿæˆå’Œæœ¬ä½“æµ‹è¯•ã€‚ä»£ç å’Œæ•°æ®é›†å·²åœ¨Apache 2.0è®¸å¯è¯ä¸‹å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯çŸ¥è¯†å·¥ç¨‹è‡ªåŠ¨åŒ–å·¥å…·è¯„ä¼°ç¼ºä¹æ ‡å‡†åŒ–çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç»“æœæ¯”è¾ƒå’Œå¤ç°æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºBench4KEä½œä¸ºä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•ç³»ç»Ÿï¼Œä¸“æ³¨äºèƒ½åŠ›é—®é¢˜çš„è‡ªåŠ¨ç”Ÿæˆå·¥å…·è¯„ä¼°ï¼Œé€šè¿‡æä¾›æ ‡å‡†åŒ–çš„æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡æ¥æå‡ç ”ç©¶çš„ä¸¥è°¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBench4KEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†ç®¡ç†æ¨¡å—ã€è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—å’Œç»“æœå±•ç¤ºæ¨¡å—ã€‚æ•°æ®é›†ç®¡ç†æ¨¡å—æä¾›æ¥è‡ª17ä¸ªæœ¬ä½“å·¥ç¨‹é¡¹ç›®çš„CQsæ•°æ®é›†ï¼Œè¯„ä¼°æŒ‡æ ‡æ¨¡å—ä½¿ç”¨å¤šç§ç›¸ä¼¼æ€§åº¦é‡æ¥è¯„ä¼°ç”Ÿæˆçš„CQsè´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šBench4KEçš„åˆ›æ–°åœ¨äºå…¶æä¾›çš„æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶å’Œä¸°å¯Œçš„åŸºå‡†æ•°æ®é›†ï¼Œä½¿å¾—ä¸åŒå·¥å…·çš„è¯„ä¼°ç»“æœå¯ä»¥ç›´æ¥æ¯”è¾ƒï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶ä¸­çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒBench4KEé‡‡ç”¨äº†å¤šç§ç›¸ä¼¼æ€§åº¦é‡æ¥è¯„ä¼°CQsçš„è´¨é‡ï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶æ”¯æŒæœªæ¥æ‰©å±•å…¶ä»–KEè‡ªåŠ¨åŒ–ä»»åŠ¡çš„èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¯¹6ä¸ªåŸºäºLLMçš„èƒ½åŠ›é—®é¢˜ç”Ÿæˆç³»ç»Ÿçš„æ¯”è¾ƒåˆ†æä¸­ï¼ŒBench4KEå»ºç«‹äº†ä¸€ä¸ªåŸºå‡†çº¿ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å‚è€ƒã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä½¿ç”¨æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ˜¾è‘—æé«˜äº†ç”ŸæˆCQsçš„è´¨é‡è¯„ä¼°çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Bench4KEçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœ¬ä½“å·¥ç¨‹ã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡æä¾›æ ‡å‡†åŒ–çš„è¯„ä¼°å·¥å…·ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°æ¯”è¾ƒå’Œæ”¹è¿›è‡ªåŠ¨ç”Ÿæˆèƒ½åŠ›é—®é¢˜çš„å·¥å…·ï¼Œä»è€Œæ¨åŠ¨çŸ¥è¯†å·¥ç¨‹é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥ï¼ŒBench4KEå¯èƒ½ä¼šæ‰©å±•åˆ°æ›´å¤šçš„çŸ¥è¯†å·¥ç¨‹è‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æå‡å…¶å®é™…ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation. This trend is already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs), natural language questions used by ontology engineers to define the functional requirements of an ontology. However, the evaluation of these tools lacks standardization. This undermines the methodological rigor and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. The presented release focuses on evaluating tools that generate CQs automatically. Bench4KE provides a curated gold standard consisting of CQ datasets from 17 real-world ontology engineering projects and uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of 6 recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.

