---
layout: default
title: "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation"
---

# FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24714" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24714v1</a>
  <a href="https://arxiv.org/pdf/2505.24714.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24714v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24714v1', 'FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junyu Luo, Zhizhuo Kou, Liming Yang, Xiao Luo, Jinsheng Huang, Zhiping Xiao, Jingshu Peng, Chengzhong Liu, Jiaming Ji, Xuanzhe Liu, Sirui Han, Ming Zhang, Yike Guo

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: ACL 2025 Main Conference

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/luo-junyu/FinMME) | [HUGGINGFACE](https://huggingface.co/datasets/luojunyu)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFinMMEæ•°æ®é›†ä»¥è§£å†³é‡‘èé¢†åŸŸå¤šæ¨¡æ€è¯„ä¼°ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¯„ä¼°` `é‡‘èæ•°æ®é›†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®è´¨é‡` `è¯„ä¼°ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é‡‘èé¢†åŸŸçš„å¤šæ¨¡æ€è¯„ä¼°æ•°æ®é›†ç¼ºä¹æœ‰æ•ˆæ€§å’Œä¸“ä¸šæ€§ï¼Œé™åˆ¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ã€‚
2. æœ¬æ–‡æå‡ºFinMMEæ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„é‡‘èç ”ç©¶æ ·æœ¬å’Œå¤šæ ·çš„è¯„ä¼°æœºåˆ¶ï¼Œä»¥æå‡é‡‘èé¢†åŸŸçš„å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨FinMMEä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæ•°æ®é›†çš„é¢„æµ‹å˜åŒ–ç‡ä½äº1%ï¼Œå±•ç°å‡ºé«˜é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿‘å¹´æ¥å‘å±•è¿…é€Ÿï¼Œä½†åœ¨é‡‘èé¢†åŸŸç¼ºä¹æœ‰æ•ˆçš„ä¸“é—¨è¯„ä¼°æ•°æ®é›†ã€‚ä¸ºæ¨åŠ¨é‡‘èé¢†åŸŸMLLMsçš„å‘å±•ï¼Œæœ¬æ–‡ä»‹ç»äº†FinMMEæ•°æ®é›†ï¼Œæ¶µç›–è¶…è¿‡11,000ä¸ªé«˜è´¨é‡é‡‘èç ”ç©¶æ ·æœ¬ï¼Œæ¶‰åŠ18ä¸ªé‡‘èé¢†åŸŸå’Œ6ç§èµ„äº§ç±»åˆ«ï¼ŒåŒ…å«10ç§ä¸»è¦å›¾è¡¨ç±»å‹å’Œ21ç§å­ç±»å‹ã€‚é€šè¿‡20åæ³¨é‡Šå‘˜å’Œç²¾å¿ƒè®¾è®¡çš„éªŒè¯æœºåˆ¶ç¡®ä¿æ•°æ®è´¨é‡ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†FinScoreè¯„ä¼°ç³»ç»Ÿï¼Œç»“åˆå¹»è§‰æƒ©ç½šå’Œå¤šç»´èƒ½åŠ›è¯„ä¼°ï¼Œæä¾›æ— åè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4oåœ¨FinMMEä¸Šçš„è¡¨ç°ä¹Ÿä¸å°½å¦‚äººæ„ï¼Œçªæ˜¾äº†è¯¥æ•°æ®é›†çš„æŒ‘æˆ˜æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é‡‘èé¢†åŸŸç¼ºä¹æœ‰æ•ˆå¤šæ¨¡æ€è¯„ä¼°æ•°æ®é›†çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°é‡‘èå¤šæ¨¡æ€æ¨¡å‹æ—¶å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³å®é™…éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºFinMMEæ•°æ®é›†ï¼Œæ•´åˆå¤šç§é‡‘èé¢†åŸŸçš„é«˜è´¨é‡æ ·æœ¬ï¼Œæä¾›å…¨é¢çš„è¯„ä¼°æœºåˆ¶ï¼Œæ¨åŠ¨é‡‘èå¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFinMMEæ•°æ®é›†ç”±è¶…è¿‡11,000ä¸ªæ ·æœ¬ç»„æˆï¼Œæ¶µç›–18ä¸ªé‡‘èé¢†åŸŸå’Œ6ç§èµ„äº§ç±»åˆ«ï¼ŒåŒ…å«å¤šç§å›¾è¡¨ç±»å‹ã€‚è¯„ä¼°ç³»ç»ŸFinScoreåˆ™ç»“åˆäº†å¹»è§‰æƒ©ç½šå’Œå¤šç»´èƒ½åŠ›è¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå…¬æ­£æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šFinMMEæ•°æ®é›†çš„æ„å»ºå’ŒFinScoreè¯„ä¼°ç³»ç»Ÿæ˜¯æœ¬æ–‡çš„ä¸»è¦åˆ›æ–°ï¼Œä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„æ ·æœ¬è´¨é‡å’Œè¯„ä¼°ç»´åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†é€šè¿‡20åæ³¨é‡Šå‘˜è¿›è¡Œæ ‡æ³¨ï¼Œé‡‡ç”¨ä¸¥æ ¼çš„éªŒè¯æœºåˆ¶ç¡®ä¿æ•°æ®è´¨é‡ï¼›è¯„ä¼°ç³»ç»Ÿè®¾è®¡äº†å¤šç»´åº¦çš„èƒ½åŠ›è¯„ä¼°å’Œå¹»è§‰æƒ©ç½šæœºåˆ¶ï¼Œä»¥æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4oåœ¨FinMMEæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ç†æƒ³ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ•°æ®é›†çš„æŒ‘æˆ˜æ€§ã€‚åŒæ—¶ï¼ŒFinMMEåœ¨ä¸åŒæç¤ºä¸‹çš„é¢„æµ‹å˜åŒ–ç‡ä½äº1%ï¼Œå±•ç°å‡ºå…¶é«˜é²æ£’æ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FinMMEæ•°æ®é›†åŠå…¶è¯„ä¼°ç³»ç»Ÿå¯å¹¿æ³›åº”ç”¨äºé‡‘èé¢†åŸŸçš„å¤šæ¨¡æ€æ¨¡å‹å¼€å‘å’Œè¯„ä¼°ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨å¤šæ¨¡æ€æŠ€æœ¯ï¼Œæå‡é‡‘èå†³ç­–çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥æ•°æ®é›†æœ‰æœ›æ¨åŠ¨é‡‘èç§‘æŠ€çš„è¿›ä¸€æ­¥åˆ›æ–°ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.

