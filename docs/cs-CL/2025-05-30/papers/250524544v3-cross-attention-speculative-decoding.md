---
layout: default
title: Cross-Attention Speculative Decoding
---

# Cross-Attention Speculative Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24544" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24544v3</a>
  <a href="https://arxiv.org/pdf/2505.24544.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24544v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24544v3', 'Cross-Attention Speculative Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-09-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨æ³¨æ„åŠ›æ¨æµ‹è§£ç ä»¥ç®€åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨æµ‹è§£ç ` `è·¨æ³¨æ„åŠ›` `Transformer` `è®­ç»ƒæ•ˆç‡` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ç®€åŒ–` `å—æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨æµ‹è§£ç æ–¹æ³•é€šå¸¸ä¾èµ–äºè‡ªæ³¨æ„åŠ›è§£ç å™¨ï¼Œå¯¼è‡´æ¶æ„å¤æ‚ä¸”éš¾ä»¥åœ¨ä¸åŒæ¨¡å‹é—´æ¨å¹¿ã€‚
2. æœ¬æ–‡æå‡ºçš„Beagleæ¨¡å‹åŸºäºè·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œç®€åŒ–äº†æ¶æ„å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œæ¶ˆé™¤äº†å¯¹æ± åŒ–å’Œè¾…åŠ©ç»„ä»¶çš„éœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBeagleåœ¨æ¨ç†é€Ÿåº¦å’Œè®­ç»ƒæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„EAGLE-v2æ¨¡å‹ï¼Œæä¾›äº†å¼ºæœ‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨æµ‹è§£ç ï¼ˆSDï¼‰æ˜¯ä¸€ç§åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†çš„å¹¿æ³›é‡‡ç”¨çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨è‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹é«˜åº¦å¯¹é½æ—¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SDæ–¹æ³•é€šå¸¸ä¾èµ–äºç´§å¯†è€¦åˆçš„è‡ªæ³¨æ„åŠ›Transformerè§£ç å™¨ï¼Œå¢åŠ äº†å¤æ‚æ€§å¹¶é™ä½äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†Budget EAGLEï¼ˆBeagleï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºè·¨æ³¨æ„åŠ›çš„Transformerè§£ç å™¨SDæ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸é¢†å…ˆçš„è‡ªæ³¨æ„åŠ›SDæ¨¡å‹ï¼ˆEAGLE-v2ï¼‰ç›¸å½“ï¼ŒåŒæ—¶æ¶ˆé™¤äº†å¯¹æ± åŒ–æˆ–è¾…åŠ©ç»„ä»¶çš„éœ€æ±‚ï¼Œç®€åŒ–äº†æ¶æ„ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶åœ¨è®­ç»ƒæ—¶ä¿æŒç¨³å®šçš„å†…å­˜ä½¿ç”¨ã€‚ä¸ºæœ‰æ•ˆè®­ç»ƒè¿™ä¸€æ–°æ¶æ„ï¼Œæå‡ºäº†ä¸¤é˜¶æ®µå—æ³¨æ„åŠ›è®­ç»ƒæ–¹æ³•ï¼Œç¡®ä¿äº†å—çº§æ³¨æ„åŠ›åœºæ™¯ä¸‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBeagleåœ¨å¤šä¸ªLLMså’Œæ•°æ®é›†ä¸Šå®ç°äº†ç«äº‰æ€§çš„æ¨ç†åŠ é€Ÿå’Œæ›´é«˜çš„è®­ç»ƒæ•ˆç‡ï¼Œæˆä¸ºæ¨æµ‹è§£ç æ¶æ„çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨æµ‹è§£ç æ–¹æ³•åœ¨æ¶æ„å¤æ‚æ€§å’Œé€šç”¨æ€§ä¸Šçš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŸºäºè·¨æ³¨æ„åŠ›çš„Transformerè§£ç å™¨ï¼Œæ—¨åœ¨ç®€åŒ–æ¨æµ‹è§£ç çš„æ¶æ„ï¼ŒåŒæ—¶ä¿æŒä¸è‡ªæ³¨æ„åŠ›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚é€šè¿‡æ¶ˆé™¤æ± åŒ–å’Œè¾…åŠ©ç»„ä»¶ï¼ŒBeagleæ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†æ›´é«˜çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBeagleæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è·¨æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µå—æ³¨æ„åŠ›è®­ç»ƒæ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œé«˜æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šBeagleæ˜¯é¦–ä¸ªåŸºäºè·¨æ³¨æ„åŠ›çš„æ¨æµ‹è§£ç æ¨¡å‹ï¼Œæ˜¾è‘—ç®€åŒ–äº†æ¨¡å‹æ¶æ„ï¼Œä¸ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›è§£ç å™¨ç›¸æ¯”ï¼Œå‡å°‘äº†å¤æ‚æ€§å¹¶æé«˜äº†é€šç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ–°çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–å—çº§æ³¨æ„åŠ›çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„ç¨³å®šæ€§å’Œé«˜æ•ˆæ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å’Œè®­ç»ƒç­–ç•¥åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBeagleåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œè¾ƒEAGLE-v2æ¨¡å‹æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªè¯¦è¿°ï¼Œä½†æ•´ä½“è¡¨ç°ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ¨æµ‹è§£ç çš„æ•ˆç‡ï¼ŒBeagleæ¨¡å‹èƒ½å¤Ÿåœ¨å®æ—¶åº”ç”¨ä¸­æä¾›æ›´å¿«çš„å“åº”æ—¶é—´ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.

