---
layout: default
title: Mamba Knockout for Unraveling Factual Information Flow
---

# Mamba Knockout for Unraveling Factual Information Flow

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24244" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24244v1</a>
  <a href="https://arxiv.org/pdf/2505.24244.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24244v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24244v1', 'Mamba Knockout for Unraveling Factual Information Flow')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nir Endy, Idan Daniel Grosbard, Yuval Ran-Milo, Yonatan Slutzky, Itay Tshuva, Raja Giryes

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: Accepted to ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMamba Knockoutä»¥æ­ç¤ºäº‹å®ä¿¡æ¯æµåŠ¨æœºåˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Mambaæ¨¡å‹` `ä¿¡æ¯æµåŠ¨` `æ³¨æ„åŠ›æœºåˆ¶` `å¯è§£é‡Šæ€§` `è¯­è¨€æ¨¡å‹` `Transformer` `å±‚çº§åŠ¨æ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯æµåŠ¨çš„å¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¹‹é—´çš„æ¯”è¾ƒä¸Šã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å°†æ³¨æ„åŠ›å‡»ç©¿æ–¹æ³•é€‚é…åˆ°Mambaæ¨¡å‹çš„æ€è·¯ï¼Œä»¥æ­ç¤ºä¿¡æ¯åœ¨æ¨¡å‹ä¸­çš„æµåŠ¨å’Œå±‚çº§åŠ¨æ€ã€‚
3. é€šè¿‡å®éªŒï¼Œå‘ç°Mambaæ¨¡å‹ä¸Transformeræ¨¡å‹åœ¨ä¿¡æ¯æµåŠ¨æ¨¡å¼ä¸Šå­˜åœ¨å·®å¼‚ï¼ŒåŒæ—¶ä¹Ÿæœ‰ä¸€äº›æ™®éç°è±¡ï¼Œæ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…åœ¨ç‰¹å¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†åŸºäºMambaçŠ¶æ€ç©ºé—´æ¨¡å‹çš„è¯­è¨€æ¨¡å‹ä¸­äº‹å®ä¿¡æ¯çš„æµåŠ¨ã€‚æˆ‘ä»¬ä¾èµ–äºä¸Transformeræ¶æ„åŠå…¶æ³¨æ„åŠ›æœºåˆ¶çš„ç†è®ºå’Œå®è¯è”ç³»ï¼Œåˆ©ç”¨è¿™ä¸€å…³ç³»å°†æœ€åˆä¸ºTransformerå¼€å‘çš„æ³¨æ„åŠ›å¯è§£é‡Šæ€§æŠ€æœ¯ï¼ˆç‰¹åˆ«æ˜¯æ³¨æ„åŠ›å‡»ç©¿æ–¹æ³•ï¼‰é€‚é…åˆ°Mamba-1å’ŒMamba-2ä¸­ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬è¿½è¸ªä¿¡æ¯åœ¨æ ‡è®°å’Œå±‚ä¹‹é—´çš„ä¼ é€’å’Œå®šä½ï¼Œæ­ç¤ºäº†ä¸»é¢˜æ ‡è®°ä¿¡æ¯çš„å‡ºç°æ¨¡å¼å’Œå±‚çº§åŠ¨æ€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€äº›ç°è±¡åœ¨Mambaæ¨¡å‹å’ŒåŸºäºTransformerçš„æ¨¡å‹ä¹‹é—´å­˜åœ¨å·®å¼‚ï¼Œè€Œå…¶ä»–ç°è±¡åœ¨æ‰€æœ‰æ£€æŸ¥çš„æ¨¡å‹ä¸­ä¼¼ä¹æ™®éå­˜åœ¨ï¼Œæš—ç¤ºè¿™äº›å¯èƒ½æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„å›ºæœ‰ç‰¹å¾ã€‚é€šè¿‡è¿›ä¸€æ­¥åˆ©ç”¨Mambaçš„ç»“æ„åŒ–å› å¼åˆ†è§£ï¼Œæˆ‘ä»¬è§£å¼€äº†ä¸åŒâ€œç‰¹å¾â€å¦‚ä½•ä¿ƒè¿›æ ‡è®°é—´çš„ä¿¡æ¯äº¤æ¢æˆ–ä¸°å¯Œå•ä¸ªæ ‡è®°ï¼Œä»è€Œæä¾›äº†ç†è§£Mambaå†…éƒ¨æ“ä½œçš„ç»Ÿä¸€è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ä¸­äº‹å®ä¿¡æ¯æµåŠ¨çš„å¯è§£é‡Šæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹æ¶æ„é—´çš„æ¯”è¾ƒå’Œä¿¡æ¯ä¼ é€’æœºåˆ¶ä¸Šå­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ³¨æ„åŠ›å‡»ç©¿æ–¹æ³•é€‚é…åˆ°Mambaæ¨¡å‹ï¼Œç ”ç©¶ä¿¡æ¯å¦‚ä½•åœ¨æ ‡è®°å’Œå±‚ä¹‹é—´ä¼ é€’ï¼Œä»è€Œæ­ç¤ºä¿¡æ¯æµåŠ¨çš„åŠ¨æ€ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬Mamba-1å’ŒMamba-2æ¨¡å‹çš„é€‚é…ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿½è¸ªä¿¡æ¯æµåŠ¨ï¼Œåˆ†ä¸ºä¿¡æ¯ä¼ é€’å’Œä¿¡æ¯å®šä½ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†Transformerçš„å¯è§£é‡Šæ€§æŠ€æœ¯æˆåŠŸè¿ç§»åˆ°Mambaæ¨¡å‹ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹é—´ä¿¡æ¯æµåŠ¨çš„å¼‚åŒï¼Œæä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ä¸Transformerç›¸ä¼¼çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶ç»“åˆMambaçš„ç»“æ„åŒ–å› å¼åˆ†è§£ï¼Œè®¾è®¡äº†é€‚åˆMambaæ¨¡å‹çš„ä¿¡æ¯è¿½è¸ªæµç¨‹ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†æä¿¡æ¯åœ¨æ¨¡å‹ä¸­çš„æµåŠ¨å’Œå±‚çº§åŠ¨æ€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMambaæ¨¡å‹åœ¨ä¿¡æ¯æµåŠ¨çš„å¯è§£é‡Šæ€§ä¸Šä¼˜äºä¼ ç»Ÿçš„Transformeræ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯ä¼ é€’å’Œå±‚çº§åŠ¨æ€çš„æ­ç¤ºä¸Šï¼Œæä¾›äº†æ–°çš„è§è§£ã€‚è¿™äº›å‘ç°ä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…åœ¨æœºåˆ¶å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£Mambaæ¨¡å‹çš„ä¿¡æ¯æµåŠ¨æœºåˆ¶ï¼Œå¯ä»¥ä¸ºæ¨¡å‹ä¼˜åŒ–å’Œæ–°å‹è¯­è¨€æ¨¡å‹çš„è®¾è®¡æä¾›ç†è®ºæ”¯æŒï¼Œè¿›è€Œæå‡å®é™…åº”ç”¨çš„æ•ˆæœå’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers--specifically, the Attention Knockout methodology--to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected--hinting that these may be inherent to LLMs in general. By further leveraging Mamba's structured factorization, we disentangle how distinct "features" either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.

