---
layout: default
title: AI Pedagogy: Dialogic Social Learning for Artificial Agents
---

# AI Pedagogy: Dialogic Social Learning for Artificial Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.21065" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.21065v2</a>
  <a href="https://arxiv.org/pdf/2507.21065.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.21065v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2507.21065v2', 'AI Pedagogy: Dialogic Social Learning for Artificial Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sabrina Patania, Luca Annese, Cansu Koyuturk, Azzurra Ruggeri, Dimitri Ognibene

**åˆ†ç±»**: cs.CL, cs.HC, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-25 (æ›´æ–°: 2025-08-11)

**å¤‡æ³¨**: accepted at ICSR2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAIç¤¾ä¼šå¥èº«æˆ¿ä»¥è§£å†³LLMçŸ¥è¯†è·å–ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¤¾ä¼šåª’ä»‹å­¦ä¹ ` `åŒå‘æ•™å­¦` `çŸ¥è¯†è·å–` `æ•™è‚²æœºå™¨äºº` `æ™ºèƒ½åŠ©æ‰‹` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„AIè®­ç»ƒæ–¹æ³•åœ¨çŸ¥è¯†è·å–ä¸Šå­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çŸ¥è¯†çš„æ•´åˆä¸åº”ç”¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„AIç¤¾ä¼šå¥èº«æˆ¿é€šè¿‡åŒå‘æ•™å­¦å¯¹è¯ï¼Œåˆ©ç”¨ç¤¾ä¼šåª’ä»‹å­¦ä¹ èŒƒå¼æ¥æå‡AIçš„çŸ¥è¯†è·å–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ··åˆæ–¹å‘çš„äº’åŠ¨ç­–ç•¥æ˜¾è‘—æé«˜äº†LLMçš„çŸ¥è¯†è·å–å’Œåº”ç”¨èƒ½åŠ›ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å•å‘æ•™å­¦æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ç¦»çº¿æ•°æ®é›†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è·å–å’Œæ•´åˆå¤æ‚çŸ¥è¯†æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„AIè®­ç»ƒæ–¹æ³•ä¸»è¦åŸºäºç›‘ç£å­¦ä¹ æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œç±»ä¼¼äºçš®äºšæ°çš„ç‹¬ç«‹æ¢ç´¢æ¨¡å‹ï¼Œä¾èµ–äºå¤§é‡æ•°æ®é›†å’Œç¨€ç–åé¦ˆä¿¡å·ï¼Œé™åˆ¶äº†æ¨¡å‹ä»äº¤äº’ä¸­é«˜æ•ˆå­¦ä¹ çš„èƒ½åŠ›ã€‚æœ¬æ–‡å€Ÿé‰´ç»´æœèŒ¨åŸºçš„ç¤¾ä¼šæ–‡åŒ–ç†è®ºï¼Œæ¢è®¨ç¤¾ä¼šåª’ä»‹å­¦ä¹ èŒƒå¼çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€ç¯å¢ƒâ€”â€”AIç¤¾ä¼šå¥èº«æˆ¿ï¼ŒAIå­¦ä¹ ä»£ç†ä¸çŸ¥è¯†ä¸°å¯Œçš„AIæ•™å¸ˆä»£ç†è¿›è¡ŒåŒå‘æ•™å­¦å¯¹è¯ã€‚è¿™ç§å¤–éƒ¨ç»“æ„åŒ–å¯¹è¯ä½œä¸ºçŸ¥è¯†è·å–çš„æ ¸å¿ƒæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†LLMè·å–å’Œåº”ç”¨æ–°çŸ¥è¯†çš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†å•å‘æ•™å­¦æ–¹æ³•å’Œç›´æ¥è®¿é—®ç»“æ„åŒ–çŸ¥è¯†çš„æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åœ¨çº¿çŸ¥è¯†è·å–å’Œæ•´åˆæ–¹é¢çš„ä¸è¶³ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤§é‡æ•°æ®å’Œç¨€ç–åé¦ˆï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥AIç¤¾ä¼šå¥èº«æˆ¿ï¼ŒAIå­¦ä¹ ä»£ç†ä¸æ•™å¸ˆä»£ç†è¿›è¡Œç»“æ„åŒ–çš„åŒå‘å¯¹è¯ï¼Œå¼ºè°ƒå¤–éƒ¨å¯¹è¯åœ¨çŸ¥è¯†è·å–ä¸­çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬AIå­¦ä¹ ä»£ç†ã€çŸ¥è¯†ä¸°å¯Œçš„æ•™å¸ˆä»£ç†å’ŒåŠ¨æ€å¯¹è¯ç¯å¢ƒï¼Œä»£ç†ä¹‹é—´é€šè¿‡å¯¹è¯è¿›è¡ŒçŸ¥è¯†ä¼ é€’å’Œåé¦ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨ç¤¾ä¼šåª’ä»‹å­¦ä¹ èŒƒå¼ï¼Œç»“åˆåŒå‘äº’åŠ¨ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„çŸ¥è¯†è·å–èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„å•å‘æ•™å­¦æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ··åˆæ–¹å‘çš„äº’åŠ¨ç­–ç•¥ï¼Œç»“åˆè‡ªä¸Šè€Œä¸‹çš„è§£é‡Šä¸å­¦ä¹ è€…å‘èµ·çš„é—®é¢˜ï¼Œä¼˜åŒ–äº†å¯¹è¯çš„ç»“æ„å’Œå†…å®¹ã€‚å®éªŒä¸­ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œåé¦ˆæœºåˆ¶ï¼Œä»¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨åŒå‘æ•™å­¦å¯¹è¯çš„AIå­¦ä¹ ä»£ç†åœ¨çŸ¥è¯†è·å–å’Œåº”ç”¨èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•å‘æ•™å­¦æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ··åˆæ–¹å‘äº’åŠ¨ç­–ç•¥ä¸‹ï¼ŒLLMçš„è¡¨ç°æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æœºå™¨äººã€æ™ºèƒ½åŠ©æ‰‹å’Œåœ¨çº¿å­¦ä¹ å¹³å°ç­‰ã€‚é€šè¿‡å¼•å…¥ç¤¾ä¼šåª’ä»‹å­¦ä¹ çš„ç†å¿µï¼Œå¯ä»¥æå‡AIç³»ç»Ÿåœ¨çŸ¥è¯†è·å–å’Œåº”ç”¨æ–¹é¢çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜ç”¨æˆ·ä½“éªŒå’Œå­¦ä¹ æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¹¿æ³›çš„AIè®­ç»ƒå’Œäººæœºäº¤äº’åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.
>   We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.
>   Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.
>   These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering

