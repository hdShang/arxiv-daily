---
layout: default
title: Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free
---

# Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06708" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06708v1</a>
  <a href="https://arxiv.org/pdf/2505.06708.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06708v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06708v1', 'Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, Junyang Lin

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-10

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/qiuzh20/gated_attention) | [HUGGINGFACE](https://huggingface.co/QwQZh/gated_attention)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé—¨æ§æ³¨æ„åŠ›æœºåˆ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é—¨æ§æœºåˆ¶` `å¤§è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `é•¿ä¸Šä¸‹æ–‡` `æ€§èƒ½æå‡` `ç¨€ç–æ€§` `éçº¿æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é—¨æ§æœºåˆ¶åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å…·ä½“æ•ˆæœå°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå¯¼è‡´æ€§èƒ½æå‡çš„æ½œåŠ›æœªè¢«å®Œå…¨æŒ–æ˜ã€‚
2. æœ¬æ–‡æå‡ºåœ¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ååº”ç”¨å¤´ç‰¹å®šçš„sigmoidé—¨æ§ï¼Œä»¥å¼•å…¥éçº¿æ€§å’Œç¨€ç–æ€§ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½å’Œç¨³å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé—¨æ§æœºåˆ¶æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ‰©å±•æ€§ï¼Œå°¤å…¶åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é—¨æ§æœºåˆ¶åœ¨æ—©æœŸæ¨¡å‹ï¼ˆå¦‚LSTMå’Œé«˜é€Ÿå…¬è·¯ç½‘ç»œï¼‰åŠè¿‘æœŸçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ã€çº¿æ€§æ³¨æ„åŠ›å’Œsoftmaxæ³¨æ„åŠ›ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®å¾ˆå°‘æ¢è®¨é—¨æ§çš„å…·ä½“æ•ˆæœã€‚æœ¬æ–‡é€šè¿‡å¯¹30ç§15Bæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹å’Œ1.7Bç¨ å¯†æ¨¡å‹è¿›è¡Œå…¨é¢æ¯”è¾ƒï¼Œç³»ç»Ÿç ”ç©¶äº†å¢å¼ºsoftmaxæ³¨æ„åŠ›çš„é—¨æ§å˜ä½“ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰ååº”ç”¨ç‰¹å®šå¤´çš„sigmoidé—¨æ§å¯ä»¥æŒç»­æå‡æ€§èƒ½ï¼ŒåŒæ—¶å¢å¼ºè®­ç»ƒç¨³å®šæ€§ï¼Œå®¹å¿æ›´å¤§çš„å­¦ä¹ ç‡ï¼Œå¹¶æ”¹å–„æ‰©å±•æ€§ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒçš„é—¨æ§ä½ç½®å’Œè®¡ç®—å˜ä½“ï¼Œå½’å› äºå¼•å…¥éçº¿æ€§å’ŒæŸ¥è¯¢ä¾èµ–çš„ç¨€ç–é—¨æ§åˆ†æ•°çš„è°ƒåˆ¶æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¨€ç–é—¨æ§æœºåˆ¶å‡è½»äº†â€œæ³¨æ„åŠ›æ²‰æ²¡â€ç°è±¡ï¼Œå¹¶æå‡äº†é•¿ä¸Šä¸‹æ–‡å¤–æ¨æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ä¸­é—¨æ§æœºåˆ¶åº”ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨é—¨æ§æå‡æ¨¡å‹æ€§èƒ½å’Œç¨³å®šæ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶å®¹æ˜“å‡ºç°â€œæ³¨æ„åŠ›æ²‰æ²¡â€ç°è±¡ï¼Œå½±å“æ¨¡å‹çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåœ¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰åå¼•å…¥å¤´ç‰¹å®šçš„sigmoidé—¨æ§ï¼Œé€šè¿‡éçº¿æ€§æ˜ å°„å’Œç¨€ç–é—¨æ§åˆ†æ•°æ¥è°ƒèŠ‚è¾“å‡ºï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹15Bæ··åˆä¸“å®¶æ¨¡å‹å’Œ1.7Bç¨ å¯†æ¨¡å‹çš„æ¯”è¾ƒå®éªŒï¼Œé‡‡ç”¨3.5ä¸‡äº¿æ ‡è®°çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œé‡ç‚¹åˆ†æä¸åŒé—¨æ§ä½ç½®å’Œè®¡ç®—å˜ä½“çš„æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†é—¨æ§æœºåˆ¶ï¼Œå°¤å…¶æ˜¯é€šè¿‡ç¨€ç–é—¨æ§åˆ†æ•°æ¥è°ƒèŠ‚SDPAè¾“å‡ºï¼Œè¿™ä¸€è®¾è®¡æœ‰æ•ˆç¼“è§£äº†æ³¨æ„åŠ›æ²‰æ²¡é—®é¢˜ï¼Œå¹¶æå‡äº†æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡çš„å¤–æ¨èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†sigmoidé—¨æ§ä½œä¸ºéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œè®¾ç½®äº†ä¸åŒçš„é—¨æ§ä½ç½®ï¼Œå¹¶è¿›è¡Œäº†å¤§é‡å®éªŒä»¥ä¼˜åŒ–å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œæ‰©å±•æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåº”ç”¨é—¨æ§æœºåˆ¶åï¼Œæ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨ä¸åŸºçº¿æ¨¡å‹çš„å¯¹æ¯”ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œå¹¶ä¸”è®­ç»ƒç¨³å®šæ€§å¾—åˆ°äº†æœ‰æ•ˆæ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„è¯­è¨€ä»»åŠ¡ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆçš„è¿›æ­¥ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification-applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)-consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates 'attention sink' and enhances long-context extrapolation performance, and we also release related $\href{https://github.com/qiuzh20/gated_attention}{codes}$ and $\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate future research.

