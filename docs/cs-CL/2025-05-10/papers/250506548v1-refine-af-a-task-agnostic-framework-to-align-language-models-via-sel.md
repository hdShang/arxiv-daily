---
layout: default
title: "REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback"
---

# REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06548" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06548v1</a>
  <a href="https://arxiv.org/pdf/2505.06548.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06548v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06548v1', 'REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aniruddha Roy, Pretam Ray, Abhilash Nandy, Somak Aditya, Pawan Goyal

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-10

**å¤‡æ³¨**: 11 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºREFINE-AFæ¡†æ¶ä»¥å‡å°‘æŒ‡ä»¤ç”Ÿæˆä¸­çš„äººåŠ›æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ä»¤ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `å°å‹è¯­è¨€æ¨¡å‹` `åŠè‡ªåŠ¨åŒ–æ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”ŸæˆæŒ‡ä»¤æ•°æ®æ—¶ä¾èµ–äºäººå·¥æ ‡æ³¨ï¼Œè€—æ—¶ä¸”æˆæœ¬é«˜ï¼Œä¸”ä»»åŠ¡å¤šæ ·æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠè‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œåˆ©ç”¨å°å‹å¼€æºLLMsç”ŸæˆæŒ‡ä»¤ï¼Œå‡å°‘äººåŠ›å¹²é¢„å’Œæˆæœ¬ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶åœ¨63-66%çš„ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºæŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šå°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œåˆ›å»ºäººå·¥æ ‡æ³¨çš„æŒ‡ä»¤æ•°æ®æ—¢è€—æ—¶åˆæ˜‚è´µï¼Œä¸”æ•°é‡å’Œä»»åŠ¡å¤šæ ·æ€§æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠè‡ªåŠ¨åŒ–çš„ä»»åŠ¡æ— å…³æ¡†æ¶ï¼Œåˆ©ç”¨å¼€æºå°å‹LLMsï¼ˆå¦‚LLaMA 2-7Bã€LLaMA 2-13Bå’ŒMistral 7Bï¼‰ç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œä»è€Œå‡å°‘äººåŠ›å¹²é¢„å’Œæˆæœ¬ã€‚æ­¤å¤–ï¼Œç»“åˆåŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç®—æ³•è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºä»¥å¾€æ–¹æ³•ï¼Œè¯¥æ¡†æ¶åœ¨63-66%çš„ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç”ŸæˆæŒ‡ä»¤æ•°æ®æ—¶çš„é«˜äººåŠ›æˆæœ¬å’Œä»»åŠ¡å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–äºå¤§å‹APIæ¨¡å‹ï¼Œé™åˆ¶äº†åº”ç”¨çš„çµæ´»æ€§å’Œç»æµæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§åŠè‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œåˆ©ç”¨å°å‹å¼€æºLLMsç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥é™ä½äººåŠ›å¹²é¢„å’Œæˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—è´Ÿè´£ä»LLMsç”Ÿæˆåˆæ­¥æŒ‡ä»¤ï¼Œè€Œå¼ºåŒ–å­¦ä¹ æ¨¡å—åˆ™é€šè¿‡è‡ªåŠ¨åé¦ˆä¸æ–­è°ƒæ•´ç”Ÿæˆç­–ç•¥ï¼Œä»¥æé«˜æŒ‡ä»¤è´¨é‡å’Œå¤šæ ·æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥æŒ‡ä»¤ç”Ÿæˆè¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†ç”ŸæˆæŒ‡ä»¤çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ï¼Œä¸ä¼ ç»Ÿä¾èµ–äººå·¥æ ‡æ³¨çš„æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä½¿ç”¨å°å‹LLMsï¼ˆå¦‚LLaMA 2-7Bã€LLaMA 2-13Bå’ŒMistral 7Bï¼‰ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç”ŸæˆæŒ‡ä»¤çš„è´¨é‡ï¼Œç¡®ä¿ç”Ÿæˆçš„æŒ‡ä»¤åœ¨å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ä¸Šè¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ çš„REFINE-AFæ¡†æ¶åœ¨63-66%çš„ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œç”Ÿæˆçš„æŒ‡ä»¤åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¸Šå‡æœ‰æ˜æ˜¾æ”¹å–„ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡å‡å°‘æŒ‡ä»¤ç”Ÿæˆçš„æˆæœ¬å’Œæ—¶é—´ï¼Œèƒ½å¤ŸåŠ é€Ÿæ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²ï¼Œæå‡AIç³»ç»Ÿçš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.

