---
layout: default
title: "MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG"
---

# MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06569" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06569v2</a>
  <a href="https://arxiv.org/pdf/2505.06569.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06569v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06569v2', 'MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Woosang Lim, Zekun Li, Gyuwan Kim, Sungyoung Ji, HyeonJung Kim, Kyuri Choi, Jin Hyuk Lim, Kyungpyo Park, William Yang Wang

**åˆ†ç±»**: cs.CL, cs.AI, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-10 (æ›´æ–°: 2025-05-20)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Leezekun/MacRAG)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMacRAGä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡RAGç³»ç»Ÿçš„æ£€ç´¢ä¸ç²¾ç¡®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¤šå°ºåº¦å¤„ç†` `ä¿¡æ¯æ•´åˆ` `å¤æ‚æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RAGç³»ç»Ÿåœ¨æ£€ç´¢ç²¾åº¦ã€ä¸Šä¸‹æ–‡è¦†ç›–å’Œä¿¡æ¯æ•´åˆæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. MacRAGé€šè¿‡å°†æ–‡æ¡£åˆ†å‰²ä¸ºä¸åŒç²’åº¦çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶è‡ªé€‚åº”åœ°åˆå¹¶ç›¸å…³ä¿¡æ¯ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆä»¥æå‡æ£€ç´¢æ•ˆæœã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMacRAGåœ¨ç”Ÿæˆä»»åŠ¡çš„è¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸRAGæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLC LLMsï¼‰ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¤æ‚çš„å¤šè·³å’Œå¤§æ–‡æ¡£ä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RAGç³»ç»Ÿå¸¸å¸¸é¢ä¸´æ£€ç´¢ä¸ç²¾ç¡®ã€åœ¨å—é™çª—å£ä¸‹ä¸Šä¸‹æ–‡è¦†ç›–ä¸å®Œæ•´ä»¥åŠä¿¡æ¯ç¢ç‰‡åŒ–ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å¤šå°ºåº¦è‡ªé€‚åº”ä¸Šä¸‹æ–‡RAGï¼ˆMacRAGï¼‰ï¼Œè¯¥æ¡†æ¶å°†æ–‡æ¡£å‹ç¼©å¹¶åˆ†å‰²ä¸ºç²—åˆ°ç»†çš„ç²’åº¦ï¼Œç„¶åé€šè¿‡å®æ—¶çš„å—çº§å’Œæ–‡æ¡£çº§æ‰©å±•è‡ªé€‚åº”åœ°åˆå¹¶ç›¸å…³ä¸Šä¸‹æ–‡ã€‚MacRAGä»æœ€ç»†ç²’åº¦çš„æ£€ç´¢å¼€å§‹ï¼Œé€æ­¥å¼•å…¥æ›´å¹¿æ³›çš„é«˜å±‚ä¸Šä¸‹æ–‡ï¼Œä»è€Œæ„å»ºæœ‰æ•ˆçš„æŸ¥è¯¢ç‰¹å®šé•¿ä¸Šä¸‹æ–‡ï¼Œä¼˜åŒ–äº†ç²¾åº¦å’Œè¦†ç›–ç‡ã€‚åœ¨HotpotQAã€2WikiMultihopQAå’ŒMusiqueçš„LongBenchæ‰©å±•è¯„ä¼°ä¸­ï¼ŒMacRAGåœ¨ä½¿ç”¨Llama-3.1-8Bã€Gemini-1.5-proå’ŒGPT-4oçš„å•æ­¥å’Œå¤šæ­¥ç”Ÿæˆä»»åŠ¡ä¸­å§‹ç»ˆè¶…è¶ŠåŸºçº¿RAGç®¡é“ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†MacRAGä½œä¸ºç°å®ä¸–ç•Œé•¿ä¸Šä¸‹æ–‡å¤šè·³æ¨ç†çš„é«˜æ•ˆå¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RAGç³»ç»Ÿåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­æ£€ç´¢ä¸ç²¾ç¡®å’Œä¸Šä¸‹æ–‡è¦†ç›–ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¿¡æ¯æ•´åˆæ—¶å¸¸å¸¸å¯¼è‡´ä¿¡æ¯ç¢ç‰‡åŒ–ï¼Œå½±å“ç”Ÿæˆè´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMacRAGçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šå°ºåº¦çš„ä¸Šä¸‹æ–‡åˆ†å‰²å’Œè‡ªé€‚åº”åˆå¹¶ï¼Œé€æ­¥æ„å»ºæŸ¥è¯¢ç‰¹å®šçš„é•¿ä¸Šä¸‹æ–‡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨æ£€ç´¢åˆæœŸèšç„¦äºç»†ç²’åº¦ä¿¡æ¯ï¼Œéšåæ‰©å±•åˆ°æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå…¨é¢æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMacRAGçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ–‡æ¡£å‹ç¼©ä¸åˆ†å‰²ã€ä¸Šä¸‹æ–‡è‡ªé€‚åº”åˆå¹¶å’Œå®æ—¶æ‰©å±•ã€‚é¦–å…ˆï¼Œå°†æ–‡æ¡£å‹ç¼©ä¸ºä¸åŒç²’åº¦çš„ä¸Šä¸‹æ–‡ï¼Œç„¶åæ ¹æ®æ£€ç´¢ç»“æœè‡ªé€‚åº”åœ°åˆå¹¶ç›¸å…³ä¿¡æ¯ï¼Œæœ€åè¿›è¡Œå®æ—¶çš„ä¸Šä¸‹æ–‡æ‰©å±•ä»¥æ»¡è¶³ç”Ÿæˆéœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šMacRAGçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šå°ºåº¦è‡ªé€‚åº”ä¸Šä¸‹æ–‡åˆå¹¶æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»ŸRAGæ–¹æ³•çš„å•ä¸€ä¸Šä¸‹æ–‡å¤„ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒMacRAGèƒ½å¤Ÿæœ‰æ•ˆæå‡æ£€ç´¢çš„ç²¾åº¦å’Œä¸Šä¸‹æ–‡çš„è¦†ç›–ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMacRAGé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°å’Œè‡ªé€‚åº”çš„æ£€ç´¢ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–ä¿¡æ¯çš„æ•´åˆå’Œç”Ÿæˆæ•ˆæœã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†æ£€ç´¢ç²¾åº¦å’Œç”Ÿæˆè´¨é‡çš„ç»¼åˆè€ƒé‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå¹³è¡¡è¿™ä¸¤æ–¹é¢çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒMacRAGåœ¨HotpotQAã€2WikiMultihopQAå’ŒMusiqueçš„LongBenchæ‰©å±•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½¿ç”¨Llama-3.1-8Bã€Gemini-1.5-proå’ŒGPT-4oæ—¶ï¼Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½å‡æ˜¾è‘—è¶…è¿‡ä¼ ç»ŸRAGåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å•æ­¥å’Œå¤šæ­¥ç”Ÿæˆä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MacRAGçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡å’Œå¤æ‚æ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œå¦‚æ³•å¾‹æ–‡ä¹¦åˆ†æã€å­¦æœ¯ç ”ç©¶æ–‡çŒ®æ£€ç´¢å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚å…¶é«˜æ•ˆçš„ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›èƒ½å¤Ÿæ˜¾è‘—æå‡ä¿¡æ¯æ£€ç´¢å’Œç”Ÿæˆçš„è´¨é‡ï¼Œä¸ºå®é™…åº”ç”¨æä¾›æ›´å¼ºçš„æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-context large language models (LC LLMs) combined with retrieval-augmented generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained windows, and fragmented information from suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical RAG framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through real-time chunk- and document-level expansions. By initiating with finest-level retrieval and progressively incorporating broader, higher-level context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm MacRAG consistently surpasses baseline RAG pipelines in single- and multi-step generation using Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at https://github.com/Leezekun/MacRAG.

