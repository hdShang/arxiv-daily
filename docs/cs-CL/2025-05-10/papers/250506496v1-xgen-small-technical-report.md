---
layout: default
title: xGen-small Technical Report
---

# xGen-small Technical Report

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06496" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06496v1</a>
  <a href="https://arxiv.org/pdf/2505.06496.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06496v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06496v1', 'xGen-small Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Erik Nijkamp, Bo Pang, Egor Pakhomov, Akash Gokul, Jin Qu, Silvio Savarese, Yingbo Zhou, Caiming Xiong

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºxGen-smallä»¥ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡åº”ç”¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡` `Transformer` `å¤šé˜¶æ®µé¢„è®­ç»ƒ` `ç›‘ç£å¾®è°ƒ` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶æ€§èƒ½ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡å¦‚æ•°å­¦å’Œç¼–ç ä¸­è¡¨ç°ä¸ä½³ã€‚
2. xGen-smallé€šè¿‡ä¼˜åŒ–æ•°æ®æ•´ç†å’Œå¤šé˜¶æ®µé¢„è®­ç»ƒï¼Œæå‡äº†æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒxGen-smallåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†xGen-smallï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡åº”ç”¨ä¼˜åŒ–çš„4Bå’Œ9B Transformerè§£ç å™¨æ¨¡å‹ã€‚æˆ‘ä»¬çš„å‚ç›´é›†æˆç®¡é“ç»“åˆäº†é¢†åŸŸå¹³è¡¡ã€é¢‘ç‡æ„ŸçŸ¥çš„æ•°æ®æ•´ç†ï¼›é€šè¿‡è´¨é‡é€€ç«å’Œé•¿åº¦æ‰©å±•åˆ°128kæ ‡è®°çš„å¤šé˜¶æ®µé¢„è®­ç»ƒï¼›ä»¥åŠé€šè¿‡ç›‘ç£å¾®è°ƒã€åå¥½å­¦ä¹ å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„é’ˆå¯¹æ€§åè®­ç»ƒã€‚xGen-smallåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰Transformeræ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡åº”ç”¨ä¸­çš„æ€§èƒ½ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨é•¿æ–‡æœ¬å¤„ç†æ—¶å¸¸å¸¸é¢ä¸´ä¸Šä¸‹æ–‡ä¸¢å¤±å’Œä¿¡æ¯æ•´åˆå›°éš¾çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‚ç›´é›†æˆçš„ç®¡é“ï¼Œç»“åˆé¢‘ç‡æ„ŸçŸ¥çš„æ•°æ®æ•´ç†å’Œå¤šé˜¶æ®µé¢„è®­ç»ƒï¼Œæ¥ä¼˜åŒ–æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æå‡æ¨¡å‹å¯¹é•¿æ–‡æœ¬çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ•´ç†ã€é¢„è®­ç»ƒå’Œåè®­ç»ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®æ•´ç†é˜¶æ®µç¡®ä¿æ•°æ®çš„é¢†åŸŸå¹³è¡¡å’Œé¢‘ç‡æ„ŸçŸ¥ï¼Œé¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨è´¨é‡é€€ç«å’Œé•¿åº¦æ‰©å±•æŠ€æœ¯ï¼Œåè®­ç»ƒé˜¶æ®µåˆ™é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šxGen-smallçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šé˜¶æ®µé¢„è®­ç»ƒå’Œåè®­ç»ƒç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯é•¿åº¦æ‰©å±•åˆ°128kæ ‡è®°çš„èƒ½åŠ›ï¼Œä½¿å…¶åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€é¢„è®­ç»ƒç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒxGen-smallé‡‡ç”¨äº†4Bå’Œ9Bçš„æ¨¡å‹è§„æ¨¡ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šç»“åˆäº†å¤šä»»åŠ¡å­¦ä¹ çš„æ€æƒ³ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™ä¼˜åŒ–äº†Transformerè§£ç å™¨ä»¥é€‚åº”é•¿ä¸Šä¸‹æ–‡çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒxGen-smallåœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ¨¡å‹ï¼Œå…¶åœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†20%ä»¥ä¸Šï¼Œå±•ç°äº†å…¶åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ç¼–ç¨‹è¾…åŠ©å·¥å…·å’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚xGen-smallèƒ½å¤Ÿåœ¨é•¿æ–‡æœ¬ç”Ÿæˆã€ä»£ç è‡ªåŠ¨è¡¥å…¨å’Œå¤æ‚æ•°å­¦é—®é¢˜æ±‚è§£ç­‰åœºæ™¯ä¸­æä¾›å¼ºå¤§çš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce xGen-small, a family of 4B and 9B Transformer decoder models optimized for long-context applications. Our vertically integrated pipeline unites domain-balanced, frequency-aware data curation; multi-stage pre-training with quality annealing and length extension to 128k tokens; and targeted post-training via supervised fine-tuning, preference learning, and online reinforcement learning. xGen-small delivers strong performance across various tasks, especially in math and coding domains, while excelling at long context benchmarks.

