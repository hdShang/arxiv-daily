---
layout: default
title: Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage
---

# Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08167" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.08167v4</a>
  <a href="https://arxiv.org/pdf/2505.08167.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08167v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08167v4', 'Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang

**ÂàÜÁ±ª**: cs.CL, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-13 (Êõ¥Êñ∞: 2025-06-10)

**Â§áÊ≥®**: We want to withdraw this paper due to data usage permission issues identified after submission. We discovered that our use of certain intangible cultural heritage materials required additional community permissions and institutional ethical approvals that were not obtained

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèåÂêëÊÄùÁª¥Èìæ‰∏éÂ•ñÂä±Êú∫Âà∂‰ª•ÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÈùûÁâ©Ë¥®ÊñáÂåñÈÅó‰∫ßÈ¢ÜÂüüÁöÑÈóÆÁ≠îËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈùûÁâ©Ë¥®ÊñáÂåñÈÅó‰∫ß` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÂèåÂêëÊÄùÁª¥Èìæ` `Â•ñÂä±Êú∫Âà∂` `ÈóÆÁ≠îÁ≥ªÁªü` `Ê®°ÂûãÂæÆË∞É` `Áü•ËØÜËí∏È¶è` `È¢ÜÂüüÈÄÇÂ∫î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®‰ΩøÁî®ÈùûÁâ©Ë¥®ÊñáÂåñÈÅó‰∫ßÊï∞ÊçÆÂæÆË∞ÉÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåÈù¢‰∏¥ÂÅèËßÅÂíåÁü•ËØÜÁªßÊâøÈîôËØØÁ≠âÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªìÂêàÂèåÂêëÊÄùÁª¥ÈìæÂíåÂ•ñÂä±Êú∫Âà∂ÁöÑÊñ∞ËÆ≠ÁªÉÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÈóÆÁ≠îËÉΩÂäõÂíåÁ≠îÊ°àÂáÜÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®ÈóÆÁ≠î‰ªªÂä°‰∏≠‰ºò‰∫éÂ§öÁßçÂü∫Á∫øÊñπÊ≥ïÔºå‰∏îÂú®Â§ö‰∏™È¢ÜÂüüÁöÑÊï∞ÊçÆÈõÜ‰∏äÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºå‰∏∫È¢ÜÂüüÁâπÂÆöÁöÑLLMsÊèê‰æõ‰∫ÜÈáçË¶ÅÊîØÊåÅÂíåÊú∫‰ºö„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî®ÈùûÁâ©Ë¥®ÊñáÂåñÈÅó‰∫ßÔºàICHÔºâÊï∞ÊçÆÂØπËøô‰∫õÂ§ßÊ®°ÂûãËøõË°åÂæÆË∞ÉÊó∂ÔºåÈù¢‰∏¥ÂÅèËßÅ„ÄÅÁü•ËØÜÁªßÊâøÈîôËØØÂíåÁÅæÈöæÊÄßÈÅóÂøòÁ≠âÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÁªìÂêà‰∫ÜÂèåÂêëÊÄùÁª¥ÈìæÂíåÂ•ñÂä±Êú∫Âà∂„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫é‰∏ì‰∏∫ÈùûÁâ©Ë¥®ÊñáÂåñÈÅó‰∫ßÈ¢ÜÂüüËÆæËÆ°ÁöÑICH-QwenÊ®°ÂûãÔºåËÉΩÂ§üÈÄöËøáÂèçÂêëÊèêÈóÆÂíåÊé®ÁêÜÊøÄÊ¥ªÊ®°ÂûãÁöÑÊΩúÂú®Áü•ËØÜÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊ≠§Â§ñÔºåÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂºïÂÖ•Â•ñÂä±Êú∫Âà∂ÔºåÈÄöËøáÁªìÊûÑÂíåÂÜÖÂÆπËØÑ‰º∞‰ºòÂåñÂÜ≥Á≠ñËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÈóÆÁ≠î‰ªªÂä°‰∏ä‰ºò‰∫é0-shot„ÄÅÈÄêÊ≠•Êé®ÁêÜ„ÄÅÁü•ËØÜËí∏È¶èÂíåÈóÆÈ¢òÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§ö‰∏™È¢ÜÂüüÁöÑÈÄÇÂ∫îÊÄßÂíå‰ª∑ÂÄº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®ÈùûÁâ©Ë¥®ÊñáÂåñÈÅó‰∫ßÊï∞ÊçÆ‰∏äÂæÆË∞ÉÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊó∂Âá∫Áé∞ÁöÑÂÅèËßÅ„ÄÅÁü•ËØÜÁªßÊâøÈîôËØØÂíåÁÅæÈöæÊÄßÈÅóÂøòÁ≠âÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜËøô‰∫õÈóÆÈ¢òÊó∂ÊïàÊûú‰∏ç‰Ω≥ÔºåÂØºËá¥Ê®°ÂûãÁöÑÈóÆÁ≠îËÉΩÂäõÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁªìÂêàÂèåÂêëÊÄùÁª¥Èìæ‰∏éÂ•ñÂä±Êú∫Âà∂ÔºåÈÄöËøáÂèçÂêëÊé®ÁêÜÊøÄÊ¥ªÊ®°ÂûãÁöÑÊΩúÂú®Áü•ËØÜÔºå‰ªéËÄåÊèêÂçáÁîüÊàêÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄßÂíåË¥®Èáè„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°Êó®Âú®‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ãÔºåÂ¢ûÂº∫ÂÖ∂Âú®ÁâπÂÆöÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂèåÂêëÊÄùÁª¥ÈìæÊ®°ÂùóÂíåÂ•ñÂä±Êú∫Âà∂Ê®°Âùó„ÄÇÂèåÂêëÊÄùÁª¥ÈìæÊ®°ÂùóË¥üË¥£ÂâçÂêëÊé®ÁêÜÂíåÂèçÂêëÊé®ÁêÜÔºåËÄåÂ•ñÂä±Êú∫Âà∂Ê®°ÂùóÂàôÈÄöËøáÁªìÊûÑÂíåÂÜÖÂÆπËØÑ‰º∞Êù•‰ºòÂåñÊ®°ÂûãÁöÑÂÜ≥Á≠ñËøáÁ®ã„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°Âûã‰ºöÊ†πÊçÆ‰∏çÂêåÁöÑÊùÉÈáçÊñπÊ°àËøõË°åËØÑ‰º∞ÂíåË∞ÉÊï¥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÂèåÂêëÊÄùÁª¥Èìæ‰∏éÂ•ñÂä±Êú∫Âà∂Áõ∏ÁªìÂêàÔºåËøô‰∏ÄÊñπÊ≥ïÂú®Áé∞ÊúâÊäÄÊúØ‰∏≠Â∞öÂ±ûÈ¶ñÊ¨°ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÁ≠îÊ°àÁîüÊàêË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåËÆæÁΩÆ‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåËØÑ‰º∞Ê†áÂáÜÔºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®‰∏çÂêåÈò∂ÊÆµÁöÑËæìÂá∫Ë¥®Èáè„ÄÇÊ≠§Â§ñÔºåÂ•ñÂä±Êú∫Âà∂ÁöÑËÆæËÆ°ÂÖÅËÆ∏Ê†πÊçÆÊ®°ÂûãËæìÂá∫ÁöÑÁªìÊûÑÂíåÂÜÖÂÆπËøõË°åÂä®ÊÄÅË∞ÉÊï¥Ôºå‰ªéËÄåËøõ‰∏ÄÊ≠•‰ºòÂåñÊ®°ÂûãÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®ÈóÆÁ≠î‰ªªÂä°‰∏≠Áõ∏ËæÉ‰∫é0-shot„ÄÅÈÄêÊ≠•Êé®ÁêÜ„ÄÅÁü•ËØÜËí∏È¶èÂíåÈóÆÈ¢òÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂáÜÁ°ÆÁéá„ÄÅBleu-4ÂíåRouge-LÂæóÂàÜÂùáÊúâÊòæËëóÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§ö‰∏™È¢ÜÂüüÁöÑÊúâÊïàÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÈùûÁâ©Ë¥®ÊñáÂåñÈÅó‰∫ßÁöÑ‰øùÊä§‰∏é‰º†Êí≠„ÄÅÊïôËÇ≤È¢ÜÂüüÁöÑÁü•ËØÜ‰º†Êéà‰ª•ÂèäÊñáÂåñÁ†îÁ©∂Á≠â„ÄÇÈÄöËøáÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÁâπÂÆöÈ¢ÜÂüüÁöÑÈóÆÁ≠îËÉΩÂäõÔºåËØ•ÊñπÊ≥ï‰∏∫Êú™Êù•Âú®Â§öÊ†∑ÂåñÈ¢ÜÂüüÁöÑÊ®°ÂûãËÆ≠ÁªÉÊèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑÊÄùË∑ØÂíåÊñπÊ≥ïÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The rapid development of large language models (LLMs) has provided significant support and opportunities for the advancement of domain-specific LLMs. However, fine-tuning these large models using Intangible Cultural Heritage (ICH) data inevitably faces challenges such as bias, incorrect knowledge inheritance, and catastrophic forgetting. To address these issues, we propose a novel training method that integrates a bidirectional chains of thought and a reward mechanism. This method is built upon ICH-Qwen, a large language model specifically designed for the field of intangible cultural heritage. The proposed method enables the model to not only perform forward reasoning but also enhances the accuracy of the generated answers by utilizing reverse questioning and reverse reasoning to activate the model's latent knowledge. Additionally, a reward mechanism is introduced during training to optimize the decision-making process. This mechanism improves the quality of the model's outputs through structural and content evaluations with different weighting schemes. We conduct comparative experiments on ICH-Qwen, with results demonstrating that our method outperforms 0-shot, step-by-step reasoning, knowledge distillation, and question augmentation methods in terms of accuracy, Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the paper highlights the effectiveness of combining the bidirectional chains of thought and reward mechanism through ablation experiments. In addition, a series of generalizability experiments are conducted, with results showing that the proposed method yields improvements on various domain-specific datasets and advanced models in areas such as Finance, Wikidata, and StrategyQA. This demonstrates that the method is adaptable to multiple domains and provides a valuable approach for model training in future applications across diverse fields.

