---
layout: default
title: Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies
---

# Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08739" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08739v1</a>
  <a href="https://arxiv.org/pdf/2505.08739.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08739v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08739v1', 'Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoliang Luo, Xinyi Xu, Michael Ramscar, Bradley C. Love

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€è‡´æ€§æ¦‚ç‡å­¦ä¹ æ–¹æ³•ä»¥è§£å†³LLMsçš„åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¦‚ç‡åˆ†å¸ƒ` `è‡ªå›å½’æ¨¡å‹` `ä½ç½®åè§` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `å®éªŒè¯„ä¼°` `æ¨¡å‹ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶æœªèƒ½æœ‰æ•ˆè¯„ä¼°LLMsåœ¨ä¸åŒä»¤ç‰Œé¡ºåºä¸‹å­¦ä¹ æ¦‚ç‡åˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼Œå¯¼è‡´æ–¹æ³•è®ºç¼ºé™·ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ç†è®ºåŸºç¡€ï¼Œè¯æ˜åºåˆ—å›°æƒ‘åº¦åœ¨ä¸åŒæ’åˆ—ä¸‹ä¸å˜ï¼Œå¹¶å®šä¹‰äº†ç»éªŒè¯„ä¼°åè®®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-2æ¨¡å‹åœ¨ä¸åŒé¡ºåºä¸‹çš„è®­ç»ƒå­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œæ­ç¤ºäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä½ç½®åè§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒä»¤ç‰Œé¡ºåºä¸‹æ˜¯å¦èƒ½å¤Ÿå­¦ä¹ ä¸€è‡´çš„æ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬æ­£å¼è¯æ˜ï¼Œå¯¹äºä»»ä½•è‰¯å®šä¹‰çš„æ¦‚ç‡åˆ†å¸ƒï¼Œåºåˆ—å›°æƒ‘åº¦åœ¨ä»»ä½•å› å¼åˆ†è§£ä¸‹éƒ½æ˜¯ä¸å˜çš„ï¼ŒåŒ…æ‹¬å‰å‘ã€åå‘æˆ–ä»»æ„æ’åˆ—ã€‚è¿™ä¸€ç»“æœä¸ºç ”ç©¶LLMså¦‚ä½•ä»æ•°æ®ä¸­å­¦ä¹ å¥ å®šäº†ä¸¥æ ¼çš„ç†è®ºåŸºç¡€ï¼Œå¹¶å®šä¹‰äº†ç»éªŒè¯„ä¼°çš„åŸåˆ™æ€§åè®®ã€‚é€šè¿‡åº”ç”¨è¿™äº›åè®®ï¼Œæˆ‘ä»¬å‘ç°å…ˆå‰ç ”ç©¶åœ¨æ’åºæ•ˆåº”çš„è€ƒå¯Ÿä¸­å­˜åœ¨å…³é”®çš„ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ã€‚æˆ‘ä»¬å¯¹GPT-2æ¨¡å‹è¿›è¡Œäº†é‡æ–°è®­ç»ƒï¼Œç»“æœæ˜¾ç¤ºåœ¨æ‰€æœ‰é¡ºåºä¸­éƒ½å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œä»»æ„æ’åˆ—çš„æ¨¡å‹ä¸å‰å‘å’Œåå‘æ¨¡å‹ä¹‹é—´çš„åå·®æ˜¾è‘—ï¼Œå°½ç®¡å®ƒä»¬åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šï¼ˆä½†å¹¶éå®Œå…¨ï¼‰ä¸€è‡´ã€‚è¿™äº›åå·®å¯è¿½æº¯åˆ°è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä½ç½®å’Œå±€éƒ¨æ€§åè§ã€‚æˆ‘ä»¬çš„ç†è®ºå’Œå®è¯ç»“æœä¸ºç†è§£LLMsä¸­çš„ä½ç½®åè§æä¾›äº†æ–°æ€è·¯ï¼Œå¹¶æå‡ºäº†æ£€æµ‹LLMsæ¦‚ç‡åˆ†å¸ƒä¸ä¸€è‡´çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒä»¤ç‰Œé¡ºåºä¸‹å­¦ä¹ æ¦‚ç‡åˆ†å¸ƒä¸€è‡´æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œè¯„ä¼°è¿™ç§ä¸€è‡´æ€§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„ä¸å¯é æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡ç†è®ºè¯æ˜åºåˆ—å›°æƒ‘åº¦åœ¨ä¸åŒæ’åˆ—ä¸‹ä¿æŒä¸å˜ï¼Œå»ºç«‹äº†LLMså­¦ä¹ è¿‡ç¨‹çš„ç†è®ºåŸºç¡€ï¼Œå¹¶æå‡ºäº†ç³»ç»Ÿçš„ç»éªŒè¯„ä¼°åè®®ï¼Œä»¥æ£€æµ‹æ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå®šä¹‰äº†æ¦‚ç‡åˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼Œç„¶åé€šè¿‡é‡æ–°è®­ç»ƒGPT-2æ¨¡å‹ï¼Œåˆ†åˆ«åœ¨å‰å‘ã€åå‘å’Œä»»æ„æ’åˆ—çš„é¡ºåºä¸‹è¿›è¡Œå®éªŒï¼Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºæä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œè¯æ˜äº†åºåˆ—å›°æƒ‘åº¦çš„ä¸å˜æ€§ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿçš„å®éªŒéªŒè¯äº†è¿™ä¸€ç†è®ºï¼Œæ­ç¤ºäº†LLMsåœ¨å¤„ç†ä¸åŒé¡ºåºæ—¶çš„åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„è®­ç»ƒé¡ºåºå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé‡ç‚¹åˆ†æäº†ä½ç½®å’Œå±€éƒ¨æ€§åè§å¯¹æ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸åŒçš„è®­ç»ƒé¡ºåºä¸‹ï¼ŒGPT-2æ¨¡å‹è¡¨ç°å‡ºç³»ç»Ÿæ€§åå·®ï¼Œä»»æ„æ’åˆ—æ¨¡å‹çš„è¾“å‡ºä¸å‰å‘å’Œåå‘æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä½ç½®åè§ï¼Œæä¾›äº†å¯¹æ¨¡å‹ä¸€è‡´æ€§çš„é‡è¦è§è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†æ–°çš„ç†è®ºåŸºç¡€å’Œæ–¹æ³•è®ºï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡è¯†åˆ«å’Œä¿®æ­£æ¨¡å‹ä¸­çš„åå·®ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å¯é æ€§å’Œå¯ä¿¡åº¦ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Can autoregressive large language models (LLMs) learn consistent probability distributions when trained on sequences in different token orders? We prove formally that for any well-defined probability distribution, sequence perplexity is invariant under any factorization, including forward, backward, or arbitrary permutations. This result establishes a rigorous theoretical foundation for studying how LLMs learn from data and defines principled protocols for empirical evaluation. Applying these protocols, we show that prior studies examining ordering effects suffer from critical methodological flaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted orders on scientific text. We find systematic deviations from theoretical invariance across all orderings with arbitrary permutations strongly deviating from both forward and backward models, which largely (but not completely) agreed with one another. Deviations were traceable to differences in self-attention, reflecting positional and locality biases in processing. Our theoretical and empirical results provide novel avenues for understanding positional biases in LLMs and suggest methods for detecting when LLMs' probability distributions are inconsistent and therefore untrustworthy.

