---
layout: default
title: Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring
---

# Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08351" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08351v2</a>
  <a href="https://arxiv.org/pdf/2505.08351.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08351v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08351v2', 'Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mina Almasi, Ross Deans Kristensen-McLachlan

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-06-07)

**å¤‡æ³¨**: Accepted at BEA2025 (Conference workshop at ACL 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨CEFRæç¤ºä¸‹å¤§è¯­è¨€æ¨¡å‹åœ¨è¥¿ç­ç‰™è¯­äº’åŠ¨è¾…å¯¼ä¸­çš„å¯¹é½æ¼‚ç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è¥¿ç­ç‰™è¯­å­¦ä¹ ` `é€‚åº”æ€§è¾…å¯¼` `ç³»ç»Ÿæç¤º` `å¯¹é½æ¼‚ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é•¿æœŸäº’åŠ¨ä¸­éš¾ä»¥ä¿æŒå¯¹è¯çš„è¿è´¯æ€§å’Œé€‚åº”æ€§ï¼Œå¯¼è‡´ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ä¸ç¨³å®šã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ç³»ç»Ÿæç¤ºæ¥çº¦æŸLLMsç”Ÿæˆçš„æ–‡æœ¬ï¼Œä»¥é€‚åº”ä¸åŒèƒ½åŠ›æ°´å¹³çš„å­¦ç”Ÿï¼Œå¢å¼ºäº’åŠ¨æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ç³»ç»Ÿæç¤ºèƒ½æœ‰æ•ˆæ§åˆ¶æ–‡æœ¬éš¾åº¦ï¼Œä½†åœ¨æŒç»­äº’åŠ¨ä¸­ä»å­˜åœ¨å¯¹é½æ¼‚ç§»ç°è±¡ï¼Œå½±å“äº†å­¦ä¹ æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºé€‚åº”æ€§è¾…å¯¼å·¥å…·åœ¨ç¬¬äºŒè¯­è¨€å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç³»ç»Ÿæç¤ºæ˜¯å¦èƒ½å¯é åœ°çº¦æŸLLMsç”Ÿæˆç¬¦åˆå­¦ç”Ÿèƒ½åŠ›æ°´å¹³çš„æ–‡æœ¬ã€‚é€šè¿‡æ¨¡æ‹Ÿè¥¿ç­ç‰™è¯­çš„å®Œæ•´å¸ˆç”Ÿå¯¹è¯ï¼Œä½¿ç”¨7Båˆ°12Bå‚æ•°çš„å¼€æºLLMsï¼Œäº¤æ›¿æ‰®æ¼”è¾…å¯¼è€…å’Œå­¦ç”Ÿè§’è‰²ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç³»ç»Ÿæç¤ºå¯ä»¥çº¦æŸæ¨¡å‹è¾“å‡ºï¼Œä½†å•é æç¤ºåœ¨é•¿æœŸäº’åŠ¨ä¸­è¡¨ç°å‡ºè„†å¼±æ€§ï¼Œç§°ä¹‹ä¸ºå¯¹é½æ¼‚ç§»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºä¸ªæ€§åŒ–ã€èƒ½åŠ›å¯¹é½çš„é€‚åº”æ€§è¾…å¯¼æä¾›äº†å¯è¡Œæ€§è§è§£ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä½æˆæœ¬è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨è¥¿ç­ç‰™è¯­å­¦ä¹ ä¸­ç”Ÿæˆæ–‡æœ¬é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é•¿æœŸå¯¹è¯ä¸­è¡¨ç°å‡ºè„†å¼±æ€§ï¼Œæ— æ³•æŒç»­æ»¡è¶³å­¦ç”Ÿçš„èƒ½åŠ›éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿæç¤ºæ¥çº¦æŸæ¨¡å‹è¾“å‡ºï¼Œä½¿å…¶ç”Ÿæˆç¬¦åˆå­¦ç”Ÿèƒ½åŠ›æ°´å¹³çš„æ–‡æœ¬ã€‚è®¾è®¡ä¸Šè€ƒè™‘åˆ°ä¸åŒèƒ½åŠ›æ°´å¹³ï¼ˆA1ã€B1ã€C1ï¼‰çš„éœ€æ±‚ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¾…å¯¼è€…æ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ã€‚è¾…å¯¼è€…æ¨¡å‹ç”Ÿæˆæ•™å­¦å†…å®¹ï¼Œå­¦ç”Ÿæ¨¡å‹åˆ™æ¨¡æ‹Ÿå­¦ç”Ÿçš„åé¦ˆï¼ŒäºŒè€…äº¤æ›¿è¿›è¡Œå¯¹è¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†â€œå¯¹é½æ¼‚ç§»â€è¿™ä¸€æ¦‚å¿µï¼Œæ­ç¤ºäº†ç³»ç»Ÿæç¤ºåœ¨é•¿æœŸäº’åŠ¨ä¸­çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†æ¨¡å‹è¾“å‡ºçš„è„†å¼±æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†åŸºäºCEFRçš„æç¤ºç­–ç•¥ï¼Œè®¾ç½®äº†ä¸åŒçš„éš¾åº¦çº§åˆ«ï¼Œå¹¶ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºçš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç³»ç»Ÿæç¤ºèƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶æ–‡æœ¬çš„éš¾åº¦ï¼Œä½†åœ¨é•¿æœŸäº’åŠ¨ä¸­ï¼Œæ¨¡å‹è¾“å‡ºçš„é€‚åº”æ€§æ˜¾è‘—ä¸‹é™ï¼Œè¡¨ç°å‡ºå¯¹é½æ¼‚ç§»ç°è±¡ã€‚è¯¥ç°è±¡æç¤ºäº†åœ¨è®¾è®¡é€‚åº”æ€§å­¦ä¹ ç³»ç»Ÿæ—¶éœ€è¦è€ƒè™‘çš„å…³é”®å› ç´ ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¯­è¨€å­¦ä¹ å¹³å°ã€åœ¨çº¿æ•™è‚²å’Œæ™ºèƒ½è¾…å¯¼ç³»ç»Ÿã€‚é€šè¿‡æä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ ä½“éªŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å­¦ç”Ÿçš„è¯­è¨€èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper investigates the potentials of Large Language Models (LLMs) as adaptive tutors in the context of second-language learning. In particular, we evaluate whether system prompting can reliably constrain LLMs to generate only text appropriate to the student's competence level. We simulate full teacher-student dialogues in Spanish using instruction-tuned, open-source LLMs ranging in size from 7B to 12B parameters. Dialogues are generated by having an LLM alternate between tutor and student roles with separate chat histories. The output from the tutor model is then used to evaluate the effectiveness of CEFR-based prompting to control text difficulty across three proficiency levels (A1, B1, C1). Our findings suggest that while system prompting can be used to constrain model outputs, prompting alone is too brittle for sustained, long-term interactional contexts - a phenomenon we term alignment drift. Our results provide insights into the feasibility of LLMs for personalized, proficiency-aligned adaptive tutors and provide a scalable method for low-cost evaluation of model performance without human participants.

