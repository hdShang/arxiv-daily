---
layout: default
title: "A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs"
---

# A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08200" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08200v1</a>
  <a href="https://arxiv.org/pdf/2505.08200.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08200v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08200v1', 'A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Artem Shelmanov, Ekaterina Fadeeva, Akim Tsvigun, Ivan Tsvigun, Zhuohan Xie, Igor Kiselev, Nico Daheim, Caiqi Zhang, Artem Vazhentsev, Mrinmaya Sachan, Preslav Nakov, Timothy Baldwin

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé¢„è®­ç»ƒä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ä»¥è§£å†³LLMå¹»è§‰æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸ç¡®å®šæ€§é‡åŒ–` `å¹»è§‰æ£€æµ‹` `Transformer` `ç›‘ç£å­¦ä¹ ` `æ¨¡å‹é²æ£’æ€§` `è·¨è¯­è¨€æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå¯¼è‡´ç”¨æˆ·éš¾ä»¥è¯†åˆ«è™šå‡ä¿¡æ¯ã€‚
2. æœ¬æ–‡æå‡ºé¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼Œå¢å¼ºLLMså¯¹è¾“å‡ºä¸ç¡®å®šæ€§çš„æ•æ‰èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å—åœ¨å¹»è§‰æ£€æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å¤šç§è¯­è¨€ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨å¹»è§‰å€¾å‘ï¼Œå³å¶å°”ç”Ÿæˆè™šå‡æˆ–æé€ çš„ä¿¡æ¯ï¼Œè¿™ç»™ç”¨æˆ·å¸¦æ¥äº†æ£€æµ‹çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼•å…¥äº†é¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æ¨¡å—ï¼Œä½œä¸ºLLMsçš„ç›‘ç£è¾…åŠ©æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ•æ‰ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚é€šè¿‡å¼ºå¤§çš„Transformeræ¶æ„å’Œæ¥è‡ªLLMæ³¨æ„åŠ›å›¾çš„ä¸°å¯Œç‰¹å¾ï¼Œè¿™äº›æ¨¡å—åœ¨å£°ç§°çº§å¹»è§‰æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨æœªæ˜ç¡®è®­ç»ƒçš„è¯­è¨€ä¸Šä¹Ÿå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºMistralã€Llamaå’ŒGemma 2ç­‰æµè¡ŒLLMç³»åˆ—é¢„è®­ç»ƒäº†ä¸€ç³»åˆ—UQæ¨¡å—ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº†ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå†…å®¹æ—¶çš„å¹»è§‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ£€æµ‹è™šå‡ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç”¨æˆ·ç¼ºä¹æœ‰æ•ˆå·¥å…·æ¥è¯†åˆ«è¿™äº›å¹»è§‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯å¼•å…¥é¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼Œè¿™äº›æ¨¡å—é€šè¿‡ç›‘ç£å­¦ä¹ æ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹è¾“å‡ºä¸ç¡®å®šæ€§çš„è¯„ä¼°èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§é‡åŒ–å¤´ï¼Œåˆ©ç”¨Transformeræ¶æ„è®¾è®¡ï¼Œå¹¶ä»LLMçš„æ³¨æ„åŠ›å›¾ä¸­æå–ä¿¡æ¯ç‰¹å¾ã€‚æ¨¡å—çš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒçš„è¾“å…¥æç¤ºä¸‹è¿›è¡Œæœ‰æ•ˆçš„å¹»è§‰æ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†ç›‘ç£çš„UQæ¨¡å—ï¼Œè¿™ä¸ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æå‡äº†å¹»è§‰æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ¨¡å—çš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº†é€‚åº”æ€§ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸå’Œè¯­è¨€ä¸Šçš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—åœ¨å£°ç§°çº§å¹»è§‰æ£€æµ‹ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼Œå°¤å…¶åœ¨ç‰¹å®šé¢†åŸŸå’Œè·¨é¢†åŸŸæç¤ºä¸‹ï¼Œè¡¨ç°å‡ºè¶…è¿‡åŸºçº¿æ–¹æ³•çš„æ˜¾è‘—æå‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹ç”Ÿæˆã€ä¿¡æ¯æ£€ç´¢å’Œå¯¹è¯ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç”¨æˆ·å¯¹ç”Ÿæˆå†…å®¹çš„ä¿¡ä»»åº¦å’Œå®‰å…¨æ€§ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œå¢å¼ºå…¶è¾“å‡ºçš„å¯é æ€§å°†å…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have the tendency to hallucinate, i.e., to sporadically generate false or fabricated information. This presents a major challenge, as hallucinations often appear highly convincing and users generally lack the tools to detect them. Uncertainty quantification (UQ) provides a framework for assessing the reliability of model outputs, aiding in the identification of potential hallucinations. In this work, we introduce pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially enhance their ability to capture uncertainty compared to unsupervised UQ methods. Their strong performance stems from the powerful Transformer architecture in their design and informative features derived from LLM attention maps. Experimental evaluation shows that these heads are highly robust and achieve state-of-the-art performance in claim-level hallucination detection across both in-domain and out-of-domain prompts. Moreover, these modules demonstrate strong generalization to languages they were not explicitly trained on. We pre-train a collection of UQ heads for popular LLM series, including Mistral, Llama, and Gemma 2. We publicly release both the code and the pre-trained heads.

