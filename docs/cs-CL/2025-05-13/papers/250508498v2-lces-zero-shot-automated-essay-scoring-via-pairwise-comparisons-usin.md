---
layout: default
title: LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models
---

# LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08498" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08498v2</a>
  <a href="https://arxiv.org/pdf/2505.08498.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08498v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08498v2', 'LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Takumi Shibata, Yuichi Miyamura

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-09-21)

**å¤‡æ³¨**: Accepted to EMNLP 2025 (Main Conference)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLCESæ–¹æ³•ä»¥è§£å†³é›¶-shotè‡ªåŠ¨åŒ–ä½œæ–‡è¯„åˆ†çš„åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨åŒ–è¯„åˆ†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æˆå¯¹æ¯”è¾ƒ` `æ•™è‚²æŠ€æœ¯` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é›¶-shotè‡ªåŠ¨åŒ–ä½œæ–‡è¯„åˆ†æ–¹æ³•ä¾èµ–äºLLMsç›´æ¥ç”Ÿæˆåˆ†æ•°ï¼Œå®¹æ˜“å—åˆ°æ¨¡å‹åå·®å½±å“ï¼Œå¯¼è‡´è¯„åˆ†ä¸ä¸€è‡´ã€‚
2. æœ¬æ–‡æå‡ºLCESæ–¹æ³•ï¼Œå°†ä½œæ–‡è¯„åˆ†è½¬åŒ–ä¸ºæˆå¯¹æ¯”è¾ƒä»»åŠ¡ï¼Œé€šè¿‡åˆ¤æ–­ä¸¤ç¯‡ä½œæ–‡çš„ä¼˜åŠ£æ¥ç”Ÿæˆåˆ†æ•°ï¼Œæå‡è¯„åˆ†çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLCESåœ¨å¤šä¸ªAESåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”åœ¨ä¸åŒLLMåŸºç¡€ä¸Šå…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—é›¶-shotè‡ªåŠ¨åŒ–ä½œæ–‡è¯„åˆ†ï¼ˆAESï¼‰æˆä¸ºå¯èƒ½ï¼Œè¿™ä¸ºå‡å°‘äººå·¥è¯„åˆ†çš„æˆæœ¬å’Œå·¥ä½œé‡æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›¶-shotæ–¹æ³•é€šå¸¸ä¾èµ–LLMsç›´æ¥ç”Ÿæˆç»å¯¹åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°å¾€å¾€å› æ¨¡å‹åå·®å’Œè¯„åˆ†ä¸ä¸€è‡´è€Œä¸äººç±»è¯„ä¼°ç›¸æ‚–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ¯”è¾ƒä½œæ–‡è¯„åˆ†ï¼ˆLCESï¼‰æ–¹æ³•ï¼Œå°†AESå½¢å¼åŒ–ä¸ºæˆå¯¹æ¯”è¾ƒä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æŒ‡ç¤ºLLMsåˆ¤æ–­ä¸¤ä¸ªä½œæ–‡ä¸­å“ªä¸ªæ›´å¥½ï¼Œæ”¶é›†å¤§é‡è¿™æ ·çš„æ¯”è¾ƒï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºè¿ç»­åˆ†æ•°ã€‚é€šè¿‡é‡‡ç”¨RankNetï¼Œæˆ‘ä»¬æé«˜äº†å¯æ‰©å±•æ€§ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒLCESåœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºä¼ ç»Ÿçš„é›¶-shotæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é›¶-shotè‡ªåŠ¨åŒ–ä½œæ–‡è¯„åˆ†æ–¹æ³•ä¸­ï¼Œç”±äºç›´æ¥ç”Ÿæˆç»å¯¹åˆ†æ•°è€Œå¯¼è‡´çš„è¯„åˆ†åå·®å’Œä¸ä¸€è‡´æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLCESæ–¹æ³•é€šè¿‡å°†ä½œæ–‡è¯„åˆ†è½¬åŒ–ä¸ºæˆå¯¹æ¯”è¾ƒä»»åŠ¡ï¼ŒæŒ‡ç¤ºLLMsåˆ¤æ–­ä¸¤ç¯‡ä½œæ–‡çš„ä¼˜åŠ£ï¼Œä»è€Œé¿å…äº†ç›´æ¥è¯„åˆ†çš„åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆæ”¶é›†å¤§é‡ä½œæ–‡å¯¹çš„æ¯”è¾ƒç»“æœï¼Œç„¶ååˆ©ç”¨RankNetå°†LLMçš„åå¥½è½¬åŒ–ä¸ºæ ‡é‡åˆ†æ•°ï¼Œæœ€åç”Ÿæˆè¿ç»­çš„è¯„åˆ†ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šLCESçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†è¯„åˆ†ä»»åŠ¡è½¬åŒ–ä¸ºæˆå¯¹æ¯”è¾ƒï¼Œæ˜¾è‘—æé«˜äº†è¯„åˆ†çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œé¿å…äº†ç»å¯¹è¯„åˆ†çš„åå·®é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RankNetçš„åº”ç”¨ä¸­ï¼Œè®¾ç½®äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹çš„æ¯”è¾ƒèƒ½åŠ›ï¼ŒåŒæ—¶ç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒLLMåŸºç¡€ä¸Šçš„é€‚ç”¨æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLCESåœ¨å¤šä¸ªAESåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿé›¶-shotæ–¹æ³•ï¼Œå‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„è®¡ç®—æ•ˆç‡ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¸åŒLLMåŸºç¡€ä¸Šçš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²è¯„ä¼°ã€åœ¨çº¿å­¦ä¹ å¹³å°å’Œè‡ªåŠ¨åŒ–è¯„åˆ†ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜ä½œæ–‡è¯„åˆ†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒLCESæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡è½»æ•™å¸ˆçš„è¯„åˆ†è´Ÿæ‹…ï¼Œå¹¶ä¸ºå­¦ç”Ÿæä¾›æ›´åŠæ—¶çš„åé¦ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have enabled zero-shot automated essay scoring (AES), providing a promising way to reduce the cost and effort of essay scoring in comparison with manual grading. However, most existing zero-shot approaches rely on LLMs to directly generate absolute scores, which often diverge from human evaluations owing to model biases and inconsistent scoring. To address these limitations, we propose LLM-based Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise comparison task. Specifically, we instruct LLMs to judge which of two essays is better, collect many such comparisons, and convert them into continuous scores. Considering that the number of possible comparisons grows quadratically with the number of essays, we improve scalability by employing RankNet to efficiently transform LLM preferences into scalar scores. Experiments using AES benchmark datasets show that LCES outperforms conventional zero-shot methods in accuracy while maintaining computational efficiency. Moreover, LCES is robust across different LLM backbones, highlighting its applicability to real-world zero-shot AES.

