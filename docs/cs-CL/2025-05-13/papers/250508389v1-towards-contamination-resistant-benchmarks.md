---
layout: default
title: Towards Contamination Resistant Benchmarks
---

# Towards Contamination Resistant Benchmarks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08389" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08389v1</a>
  <a href="https://arxiv.org/pdf/2505.08389.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08389v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08389v1', 'Towards Contamination Resistant Benchmarks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rahmatullah Musawi, Sheng Lu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæŠ—æ±¡æŸ“åŸºå‡†ä»¥è§£å†³LLMè¯„ä¼°å¯é æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯„ä¼°æ–¹æ³•` `æŠ—æ±¡æŸ“åŸºå‡†` `å‡¯æ’’å¯†ç ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMè¯„ä¼°æ–¹æ³•å—åˆ°æ±¡æŸ“é—®é¢˜çš„ä¸¥é‡å½±å“ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å¯é æ€§ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡¯æ’’å¯†ç çš„æŠ—æ±¡æŸ“åŸºå‡†ï¼Œæ—¨åœ¨æé«˜LLMè¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMsåœ¨é¢å¯¹æ§åˆ¶æ±¡æŸ“çš„åŸºå‡†æ—¶è¡¨ç°ä¸ä½³ï¼Œæ­ç¤ºäº†å…¶æ½œåœ¨çš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¼å±€ã€‚æ­£ç¡®è¯„ä¼°LLMså¯¹äºç†è§£å…¶æ½œåŠ›å’Œè§£å†³å®‰å…¨æ€§ç­‰é—®é¢˜è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒLLMè¯„ä¼°é¢ä¸´å¤šç§å› ç´ çš„æŒ‘æˆ˜ï¼Œå…¶ä¸­æ±¡æŸ“é—®é¢˜å°¤ä¸ºçªå‡ºï¼Œä¸¥é‡å½±å“è¯„ä¼°çš„å¯é æ€§ã€‚æœ¬æ–‡å¼•å…¥æŠ—æ±¡æŸ“çš„æ¦‚å¿µï¼Œæå‡ºäº†ä¸€ç§åŸºäºå‡¯æ’’å¯†ç çš„åŸºå‡†ï¼Œå°½ç®¡å…¶ç®€å•ï¼Œä½†å´æ˜¯æŠ—æ±¡æŸ“åŸºå‡†çš„ä¼˜ç§€ç¤ºä¾‹ã€‚æˆ‘ä»¬åœ¨ä¸åŒè®¾ç½®ä¸‹å¯¹å¹¿æ³›ä½¿ç”¨çš„LLMsè¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨æ§åˆ¶æ±¡æŸ“æ—¶éš¾ä»¥åº”å¯¹è¯¥åŸºå‡†ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å½“å‰LLMså­˜åœ¨çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†å…³äºå…¶çœŸå®èƒ½åŠ›çš„é‡è¦é—®é¢˜ã€‚æœ¬æ–‡ä¸ºæŠ—æ±¡æŸ“åŸºå‡†çš„å‘å±•åšå‡ºäº†è´¡çŒ®ï¼Œä½¿LLMè¯„ä¼°æ›´åŠ ä¸¥æ ¼ï¼Œå¹¶æä¾›äº†å¯¹LLMsçœŸå®èƒ½åŠ›å’Œå±€é™æ€§çš„æ·±å…¥è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMè¯„ä¼°ä¸­çš„æ±¡æŸ“é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°å¤–éƒ¨ä¿¡æ¯çš„å¹²æ‰°ï¼Œå¯¼è‡´ç»“æœä¸å¯é ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æŠ—æ±¡æŸ“çš„æ¦‚å¿µï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºå‡¯æ’’å¯†ç çš„åŸºå‡†ï¼Œåˆ©ç”¨å…¶ç®€å•æ€§å’Œæœ‰æ•ˆæ€§æ¥æµ‹è¯•LLMsçš„çœŸå®èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŸºå‡†è®¾è®¡ã€æ¨¡å‹æµ‹è¯•å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆè®¾è®¡æŠ—æ±¡æŸ“åŸºå‡†ï¼Œç„¶ååœ¨ä¸åŒçš„LLMsä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæœ€ååˆ†ææ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†æŠ—æ±¡æŸ“åŸºå‡†çš„æ¦‚å¿µï¼Œå¹¶é€šè¿‡å‡¯æ’’å¯†ç çš„å½¢å¼å®ç°äº†è¿™ä¸€ç›®æ ‡ï¼Œä¸ç°æœ‰è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼Œæ›´åŠ æ³¨é‡è¯„ä¼°çš„å¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„å‡¯æ’’å¯†ç åç§»é‡ï¼Œæ§åˆ¶æ±¡æŸ“çš„ç¨‹åº¦ï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„LLMsåœ¨é¢å¯¹æ§åˆ¶æ±¡æŸ“çš„å‡¯æ’’å¯†ç åŸºå‡†æ—¶è¡¨ç°ä¸ä½³ï¼Œæ­ç¤ºäº†å…¶åœ¨çœŸå®åº”ç”¨åœºæ™¯ä¸­çš„æ½œåœ¨å±€é™æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºLLMçš„æ”¹è¿›æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹çš„è¯„ä¼°ã€æ¨¡å‹å®‰å…¨æ€§åˆ†æä»¥åŠæ¨¡å‹èƒ½åŠ›çš„çœŸå®æµ‹é‡ã€‚é€šè¿‡æä¾›æ›´å¯é çš„è¯„ä¼°åŸºå‡†ï¼Œç ”ç©¶æˆæœå°†æ¨åŠ¨LLMçš„è¿›ä¸€æ­¥å‘å±•å’Œåº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å®‰å…¨æ€§å’Œå¯é æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid development of large language models (LLMs) has transformed the landscape of natural language processing. Evaluating LLMs properly is crucial for understanding their potential and addressing concerns such as safety. However, LLM evaluation is confronted by various factors, among which contamination stands out as a key issue that undermines the reliability of evaluations. In this work, we introduce the concept of contamination resistance to address this challenge. We propose a benchmark based on Caesar ciphers (e.g., "ab" to "bc" when the shift is 1), which, despite its simplicity, is an excellent example of a contamination resistant benchmark. We test this benchmark on widely used LLMs under various settings, and we find that these models struggle with this benchmark when contamination is controlled. Our findings reveal issues in current LLMs and raise important questions regarding their true capabilities. Our work contributes to the development of contamination resistant benchmarks, enabling more rigorous LLM evaluation and offering insights into the true capabilities and limitations of LLMs.

