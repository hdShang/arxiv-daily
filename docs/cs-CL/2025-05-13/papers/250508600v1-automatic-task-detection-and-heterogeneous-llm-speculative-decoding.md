---
layout: default
title: Automatic Task Detection and Heterogeneous LLM Speculative Decoding
---

# Automatic Task Detection and Heterogeneous LLM Speculative Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08600" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08600v1</a>
  <a href="https://arxiv.org/pdf/2505.08600.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08600v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08600v1', 'Automatic Task Detection and Heterogeneous LLM Speculative Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Danying Ge, Jianhua Gao, Qizhi Jiang, Yifei Feng, Weixing Ji

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: 10 pages, 10 figures, 2 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªåŠ¨ä»»åŠ¡æ£€æµ‹ä¸å¼‚æ„LLMæ¨æµ‹è§£ç ä»¥ä¼˜åŒ–ä¸‹æ¸¸ä»»åŠ¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨æµ‹è§£ç ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»»åŠ¡ä¼˜åŒ–` `å¼‚æ„æ¨¡å‹` `åœ¨çº¿åˆ†ç±»å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨æµ‹è§£ç æ–¹æ³•åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­é¢ä¸´æ¥å—ç‡ä¸è§£ç é€Ÿåº¦ä¹‹é—´çš„æƒè¡¡ï¼Œæ•ˆç‡éš¾ä»¥ä¿è¯ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨ä»»åŠ¡åˆ’åˆ†ä¸åˆ†é…çš„æ–¹æ³•ï¼Œå°†ä¸‹æ¸¸ä»»åŠ¡åˆ†é…ç»™å¼‚æ„è‰ç¨¿æ¨¡å‹ä»¥ä¼˜åŒ–æ¨æµ‹è§£ç ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨è‰ç¨¿å‡†ç¡®æ€§ä¸Šæå‡6%è‡³50%ï¼Œæ¨ç†é€Ÿåº¦åŠ å¿«1.10å€è‡³2.64å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨æµ‹è§£ç ç»“åˆäº†è‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ï¼Œæˆä¸ºåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨æ¥å—ç‡ä¸è§£ç é€Ÿåº¦ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œå°¤å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œç”±äºè‰ç¨¿æ¨¡å‹çš„èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥ç¡®ä¿æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„æ¨æµ‹è§£ç ç®—æ³•ï¼ŒåŒ…æ‹¬è‡ªåŠ¨ä»»åŠ¡åˆ’åˆ†ä¸åˆ†é…æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†ä¸‹æ¸¸ä»»åŠ¡è‡ªåŠ¨åˆ†ç±»ä¸ºä¸åŒå­ä»»åŠ¡ï¼Œå¹¶åˆ†é…ç»™ä¸€ç»„å¼‚æ„è‰ç¨¿æ¨¡å‹ã€‚æ¯ä¸ªè‰ç¨¿æ¨¡å‹ä½¿ç”¨ä»»åŠ¡ç‰¹å®šæ•°æ®ä¸ç›®æ ‡æ¨¡å‹å¯¹é½ï¼Œä»è€Œå¢å¼ºæ¨ç†ç»“æœçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§åœ¨çº¿è½»é‡çº§æç¤ºåˆ†ç±»å™¨ï¼ŒåŠ¨æ€å°†æç¤ºè·¯ç”±åˆ°åˆé€‚çš„è‰ç¨¿æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨è‰ç¨¿å‡†ç¡®æ€§ä¸Šæ¯”ä¼ ç»Ÿæ¨æµ‹è§£ç æé«˜äº†6%è‡³50%ï¼ŒåŒæ—¶åœ¨LLMæ¨ç†ä¸­å®ç°äº†1.10å€è‡³2.64å€çš„åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨æµ‹è§£ç æ–¹æ³•åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¥å—ç‡ä¸è§£ç é€Ÿåº¦ä¹‹é—´çš„æƒè¡¡ã€‚ç°æœ‰è‰ç¨¿æ¨¡å‹èƒ½åŠ›æœ‰é™ï¼Œå¯¼è‡´éš¾ä»¥é€‚åº”å¤šæ ·åŒ–çš„ä»»åŠ¡éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„æ¨æµ‹è§£ç ç®—æ³•ï¼Œé€šè¿‡è‡ªåŠ¨ä»»åŠ¡åˆ’åˆ†ä¸åˆ†é…ï¼Œå°†ä»»åŠ¡åˆ†é…ç»™å¼‚æ„è‰ç¨¿æ¨¡å‹ï¼Œä»¥æé«˜æ¨æµ‹è§£ç çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡è‡ªåŠ¨åˆ†ç±»ã€è‰ç¨¿æ¨¡å‹åˆ†é…å’Œåœ¨çº¿æç¤ºåˆ†ç±»å™¨ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ä»»åŠ¡é¦–å…ˆè¢«è‡ªåŠ¨åˆ’åˆ†ä¸ºå­ä»»åŠ¡ï¼Œç„¶åæ ¹æ®ä»»åŠ¡ç‰¹æ€§åˆ†é…ç»™ä¸åŒçš„è‰ç¨¿æ¨¡å‹ï¼Œæœ€åé€šè¿‡æç¤ºåˆ†ç±»å™¨åŠ¨æ€è·¯ç”±ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¼‚æ„è‰ç¨¿æ¨¡å‹ä¸åœ¨çº¿æç¤ºåˆ†ç±»å™¨çš„ç»“åˆï¼Œä½¿å¾—æ¨æµ‹è§£ç èƒ½å¤Ÿé’ˆå¯¹ä¸åŒä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæ˜¾è‘—æå‡äº†å‡†ç¡®æ€§ä¸æ¨ç†é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ä½¿ç”¨ä»»åŠ¡ç‰¹å®šæ•°æ®è¿›è¡Œå¯¹é½ï¼Œç¡®ä¿æ¨ç†ç»“æœçš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œåœ¨çº¿æç¤ºåˆ†ç±»å™¨çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å®æ—¶è¾“å…¥åŠ¨æ€è°ƒæ•´ï¼Œæå‡äº†ç³»ç»Ÿçš„çµæ´»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨è‰ç¨¿å‡†ç¡®æ€§ä¸Šæ¯”ä¼ ç»Ÿæ¨æµ‹è§£ç æé«˜äº†6%è‡³50%ï¼ŒåŒæ—¶åœ¨LLMæ¨ç†ä¸­å®ç°äº†1.10å€è‡³2.64å€çš„åŠ é€Ÿï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ¨æµ‹è§£ç ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speculative decoding, which combines a draft model with a target model, has emerged as an effective approach to accelerate large language model (LLM) inference. However, existing methods often face a trade-off between the acceptance rate and decoding speed in downstream tasks due to the limited capacity of the draft model, making it difficult to ensure efficiency across diverse tasks. To address this problem, we propose a speculative decoding algorithm tailored for downstream task optimization. It includes an automatic task partitioning and assigning method, which automatically categorizes downstream tasks into different sub-tasks and assigns them to a set of heterogeneous draft models. Each draft model is aligned with the target model using task-specific data, thereby enhancing the consistency of inference results. In addition, our proposed method incorporates an online lightweight prompt classifier to dynamically route prompts to the appropriate draft model. Experimental results demonstrate that the proposed method improves draft accuracy by 6% to 50% over vanilla speculative decoding, while achieving a speedup of 1.10x to 2.64x in LLM inference.

