---
layout: default
title: Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation
---

# Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06803" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06803v1</a>
  <a href="https://arxiv.org/pdf/2505.06803.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06803v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06803v1', 'Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xilin Jiang, Junkai Wu, Vishal Choudhari, Nima Mesgarani

**åˆ†ç±»**: cs.SD, cs.CL, cs.CV, cs.MM, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨æ¨¡æ€è’¸é¦æ¡†æ¶ä»¥ç¼©å°éŸ³é¢‘ä¸è§†è§‰æ¨¡å‹çš„æ„ŸçŸ¥å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨æ¨¡æ€è’¸é¦` `éŸ³é¢‘è¯†åˆ«` `è§†è§‰æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `çŸ¥è¯†è½¬ç§»` `äººæœºäº¤äº’` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«å£°éŸ³å¯¹è±¡æ—¶ï¼Œä¸è§†è§‰æ¨¡å‹å’Œäººç±»çš„è¡¨ç°å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡è®©ä¸åŒæ¨¡æ€çš„LLMsç›¸äº’å­¦ä¹ ï¼Œæ¥æå‡æ¨¡å‹åœ¨å£°éŸ³è¯†åˆ«ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŒå‘è’¸é¦æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å›°éš¾ç±»åˆ«ä¸Šçš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œå°¤å…¶æ˜¯Qwen2-Audioå’ŒQwen2-VLä¹‹é—´çš„æ€§èƒ½å·®è·å¾—åˆ°äº†æœ‰æ•ˆç¼©å°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯†åˆ«å£°éŸ³å¯¹è±¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸è§†è§‰æˆ–éŸ³é¢‘-è§†è§‰LLMsåŠäººç±»çš„è¡¨ç°ç›¸æ¯”ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†Qwen2-Audioã€Qwen2-VLå’ŒQwen2.5-Omniåœ¨ä¸åŒè¾“å…¥æ¡ä»¶ä¸‹çš„è¡¨ç°ï¼Œå‘ç°Qwen2-Audioä¸Qwen2-VLä¹‹é—´å­˜åœ¨æ€§èƒ½å·®è·ã€‚ä¸ºç¼©å°è¿™ä¸€å·®è·ï¼Œæå‡ºäº†ä¸€ç§è·¨æ¨¡æ€è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡çŸ¥è¯†è½¬ç§»æ¥æå‡æ¨¡å‹åœ¨å›°éš¾ç±»åˆ«ä¸Šçš„è¯†åˆ«èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒå‘è’¸é¦æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‘æˆ˜æ€§ç±»åˆ«ä¸Šï¼Œå¼ºè°ƒäº†ä»äººç±»è§†è§’ç†è§£LLMsçš„æ„ŸçŸ¥å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å£°éŸ³å¯¹è±¡è¯†åˆ«ä¸­ç›¸è¾ƒäºè§†è§‰æ¨¡å‹å’Œäººç±»çš„æ€§èƒ½ä¸è¶³é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ„ŸçŸ¥å·®è·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è·¨æ¨¡æ€è’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨ä¸€ä¸ªæ¨¡æ€çš„LLMä½œä¸ºæ•™å¸ˆï¼Œå¦ä¸€ä¸ªæ¨¡æ€çš„LLMä½œä¸ºå­¦ç”Ÿï¼Œè¿›è¡ŒçŸ¥è¯†è½¬ç§»ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éš¾åº¦è¾ƒå¤§çš„å£°éŸ³ç±»åˆ«ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚Qwen2-VLï¼‰å’Œå­¦ç”Ÿæ¨¡å‹ï¼ˆå¦‚Qwen2-Audioï¼‰ï¼Œé€šè¿‡åŒå‘è’¸é¦è¿›è¡ŒçŸ¥è¯†ä¼ é€’ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹çš„è¯†åˆ«èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŒå‘è’¸é¦æœºåˆ¶ï¼Œä½¿å¾—ä¸åŒæ¨¡æ€çš„LLMsèƒ½å¤Ÿäº’ç›¸å­¦ä¹ ï¼Œæ˜¾è‘—æ”¹å–„äº†åœ¨å›°éš¾ç±»åˆ«ä¸Šçš„è¯†åˆ«æ€§èƒ½ï¼Œçªç ´äº†ä¼ ç»Ÿå•ä¸€æ¨¡æ€å­¦ä¹ çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è’¸é¦è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¡¡é‡æ•™å¸ˆä¸å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†å·®è·ï¼Œå¹¶è®¾è®¡äº†å¯å‘å¼æ¨¡å‹æ¥é¢„æµ‹å­¦ç”Ÿåœ¨ç‰¹å®šå£°éŸ³ç±»åˆ«ä¸Šçš„æŒ‘æˆ˜æ€§ï¼Œä»¥ä¼˜åŒ–çŸ¥è¯†è½¬ç§»çš„æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒå‘è’¸é¦æ˜¾è‘—æå‡äº†Qwen2-Audioåœ¨å›°éš¾ç±»åˆ«ä¸Šçš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œå°¤å…¶åœ¨ä¸Qwen2-VLçš„æ¯”è¾ƒä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†è·¨æ¨¡æ€å­¦ä¹ çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½éŸ³å“ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ„ŸçŸ¥ç­‰å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€‚é€šè¿‡æå‡éŸ³é¢‘å’Œè§†è§‰æ¨¡å‹çš„ååŒè¯†åˆ«èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºäººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œå‡†ç¡®æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½è®¾å¤‡åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œæ„ŸçŸ¥ç³»ç»Ÿä¸­å‘æŒ¥æ›´å¤§ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Audio large language models (LLMs) are considered experts at recognizing sound objects, yet their performance relative to LLMs in other sensory modalities, such as visual or audio-visual LLMs, and to humans using their ears, eyes, or both remains unexplored. To investigate this, we systematically evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio, Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of different classes from audio-only, silent video, or sounded video inputs. We uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the sensory discrepancy between human ears and eyes. To reduce this gap, we introduce a cross-modal distillation framework, where an LLM in one modality serves as the teacher and another as the student, with knowledge transfer in sound classes predicted as more challenging to the student by a heuristic model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice versa, leads to notable improvements, particularly in challenging classes. This work highlights the sensory gap in LLMs from a human-aligned perspective and proposes a principled approach to enhancing modality-specific perception in multimodal LLMs.

