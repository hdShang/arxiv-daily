---
layout: default
title: Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge
---

# Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00777" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00777v1</a>
  <a href="https://arxiv.org/pdf/2506.00777.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00777v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00777v1', 'Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Md Tahmid Rahman Laskar, Israt Jahan, Elham Dolatabadi, Chun Peng, Enamul Hoque, Jimmy Huang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-01

**å¤‡æ³¨**: Accepted at ACL 2025 (Main Conference)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/tahmedge/llm_judge_biomedical_re)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMsä½œä¸ºè¯„åˆ¤è€…ä»¥æ”¹å–„ç”Ÿç‰©åŒ»å­¦å…³ç³»æå–çš„è‡ªåŠ¨è¯„ä¼°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿç‰©åŒ»å­¦å…³ç³»æå–` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨è¯„ä¼°` `ç»“æ„åŒ–è¾“å‡º` `é¢†åŸŸé€‚åº”æŠ€æœ¯` `è¯„åˆ¤è€…æ¨¡å‹` `æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç”Ÿç‰©åŒ»å­¦å…³ç³»æå–è¯„ä¼°æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œä¼ ç»Ÿè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å› LLMsç”Ÿæˆçš„å¤šæ ·æ€§è€Œä¸å¯é ã€‚
2. è®ºæ–‡æå‡ºå°†LLMsä½œä¸ºè¯„åˆ¤è€…ï¼Œé€šè¿‡ç»“æ„åŒ–è¾“å‡ºæ ¼å¼å’Œé¢†åŸŸé€‚åº”æŠ€æœ¯æ¥æå‡è¯„ä¼°å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡æ”¹è¿›åï¼ŒLLMè¯„åˆ¤è€…çš„æ€§èƒ½å¹³å‡æå‡çº¦15%ï¼Œå¹¶å…¬å¼€äº†36kæ ·æœ¬çš„è¯„åˆ¤æ•°æ®ä¾›ç ”ç©¶ä½¿ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦å…³ç³»æå–ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ã€‚ç„¶è€Œï¼Œç”±äºLLMsç”Ÿæˆçš„æ–‡æœ¬ä¸æ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„åŒä¹‰è¯æˆ–ç¼©å†™é—®é¢˜ï¼Œä¼ ç»Ÿçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å˜å¾—ä¸å¯é ã€‚å°½ç®¡äººå·¥è¯„ä¼°æ›´ä¸ºå‡†ç¡®ï¼Œä½†å…¶æˆæœ¬é«˜ä¸”è€—æ—¶ï¼Œéš¾ä»¥åœ¨å®é™…åº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†å°†LLMsä½œä¸ºè¯„åˆ¤è€…çš„æ›¿ä»£è¯„ä¼°æ–¹æ³•ï¼ŒåŸºäº8ä¸ªLLMså¯¹5ä¸ªå…¶ä»–LLMsåœ¨3ä¸ªç”Ÿç‰©åŒ»å­¦å…³ç³»æå–æ•°æ®é›†ä¸Šçš„å“åº”è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMè¯„åˆ¤è€…åœ¨è¯¥ä»»åŠ¡ä¸­çš„è¡¨ç°é€šå¸¸ä½äº50%çš„å‡†ç¡®ç‡ï¼Œä¸»è¦åŸå› åœ¨äºæå–çš„å…³ç³»ä¸ç¬¦åˆæ ‡å‡†æ ¼å¼ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ä»¥æå‡è¯„åˆ¤è€…æ€§èƒ½ï¼Œå¹¶å¼•å…¥é¢†åŸŸé€‚åº”æŠ€æœ¯ä»¥è¿›ä¸€æ­¥å¢å¼ºè¯„åˆ¤è€…çš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç”Ÿç‰©åŒ»å­¦å…³ç³»æå–ä¸­LLMsè¯„ä¼°çš„å‡†ç¡®æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å› LLMsç”Ÿæˆçš„å¤šæ ·æ€§å’Œä¸è§„èŒƒæ€§è€Œéš¾ä»¥è¯„ä¼°å…¶æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥LLMsä½œä¸ºè¯„åˆ¤è€…ï¼Œå¹¶å¯¹ç”Ÿæˆçš„å“åº”è¿›è¡Œç»“æ„åŒ–è¾“å‡ºæ ¼å¼åŒ–ï¼Œæå‡è¯„åˆ¤è€…çš„è¯„ä¼°èƒ½åŠ›ï¼ŒåŒæ—¶åº”ç”¨é¢†åŸŸé€‚åº”æŠ€æœ¯ä»¥å¢å¼ºè·¨æ•°æ®é›†çš„çŸ¥è¯†è½¬ç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯LLMsç”Ÿæˆçš„å…³ç³»æå–å“åº”ï¼ŒäºŒæ˜¯LLMsä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œè¯„ä¼°ã€‚è¯„åˆ¤è€…é€šè¿‡ç»“æ„åŒ–æ ¼å¼åŒ–çš„è¾“å…¥æ¥æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼Œæ˜¾è‘—æå‡äº†LLMè¯„åˆ¤è€…çš„æ€§èƒ½ï¼Œè§£å†³äº†ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åˆç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„ç‰¹å®šæ ¼å¼ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è¯„åˆ¤è€…çš„å­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsä½œä¸ºè¯„åˆ¤è€…çš„å‡†ç¡®ç‡é€šå¸¸ä½äº50%ï¼Œä½†é€šè¿‡ç»“æ„åŒ–è¾“å‡ºæ ¼å¼çš„å¼•å…¥ï¼Œè¯„åˆ¤è€…çš„æ€§èƒ½å¹³å‡æå‡çº¦15%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†36kæ ·æœ¬çš„è¯„åˆ¤æ•°æ®ï¼Œä¾›åç»­ç ”ç©¶ä½¿ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯æå–ã€ä¸´åºŠæ•°æ®åˆ†æå’Œè¯ç‰©å‘ç°ç­‰ã€‚é€šè¿‡æå‡LLMsåœ¨å…³ç³»æå–ä»»åŠ¡ä¸­çš„è¯„ä¼°èƒ½åŠ›ï¼Œå¯ä»¥åŠ é€Ÿç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„è¿›å±•ï¼Œé™ä½äººå·¥è¯„ä¼°çš„æˆæœ¬ï¼Œæé«˜æ•°æ®å¤„ç†çš„æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios. However, evaluating LLMs in this task remains challenging due to their ability to generate human-like text, often producing synonyms or abbreviations of gold-standard answers, making traditional automatic evaluation metrics unreliable. On the other hand, while human evaluation is more reliable, it is costly and time-consuming, making it impractical for real-world applications. This paper investigates the use of LLMs-as-the-Judge as an alternative evaluation method for biomedical relation extraction. We benchmark 8 LLMs as judges to evaluate the responses generated by 5 other LLMs across 3 biomedical relation extraction datasets. Unlike other text-generation tasks, we observe that LLM-based judges perform quite poorly (usually below 50% accuracy) in the biomedical relation extraction task. Our findings reveal that it happens mainly because relations extracted by LLMs do not adhere to any standard format. To address this, we propose structured output formatting for LLM-generated responses that helps LLM-Judges to improve their performance by about 15% (on average). We also introduce a domain adaptation technique to further enhance LLM-Judge performance by effectively transferring knowledge between datasets. We release both our human-annotated and LLM-annotated judgment data (36k samples in total) for public use here: https://github.com/tahmedge/llm_judge_biomedical_re.

