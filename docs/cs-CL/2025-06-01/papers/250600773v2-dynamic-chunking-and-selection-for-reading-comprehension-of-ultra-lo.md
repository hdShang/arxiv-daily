---
layout: default
title: Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models
---

# Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00773" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00773v2</a>
  <a href="https://arxiv.org/pdf/2506.00773.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00773v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00773v2', 'Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boheng Sheng, Jiacheng Yao, Meicong Zhang, Guoxiu He

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-06-01 (æ›´æ–°: 2025-06-03)

**å¤‡æ³¨**: Accepted by ACL 2025 Main Conference

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ECNU-Text-Computing/DCS)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€åˆ†å—ä¸é€‰æ‹©æ–¹æ³•ä»¥è§£å†³è¶…é•¿æ–‡æœ¬ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬ç†è§£` `åŠ¨æ€åˆ†å—` `è¯­ä¹‰ç›¸ä¼¼åº¦` `é—®ç­”ç³»ç»Ÿ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸å°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºå›ºå®šé•¿åº¦å—ï¼Œå¯¼è‡´è¯­ä¹‰ç›¸å…³å†…å®¹å¯èƒ½è¢«åˆ†ç¦»ï¼Œå½±å“ç†è§£å‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºåŠ¨æ€åˆ†å‰²å’Œé€‰æ‹©é•¿æ–‡æœ¬å—çš„æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—ç›¸é‚»å¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ¥é€‚åº”æ€§åœ°åˆ†å‰²æ–‡æœ¬ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé—®ç­”åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å¤„ç†è¶…é•¿æ–‡æœ¬æ—¶ä¿æŒäº†é«˜é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é˜…è¯»å’Œç†è§£æé•¿æ–‡æœ¬æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ”¹è¿›æ–¹æ³•é€šå¸¸ä¾èµ–äºå°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºå›ºå®šé•¿åº¦çš„å—ï¼Œä½†è¿™ç§å›ºå®šæˆªæ–­å¯èƒ½ä¼šå¯¼è‡´è¯­ä¹‰ç›¸å…³å†…å®¹çš„åˆ†ç¦»ï¼Œä»è€Œå¼•å‘æ­§ä¹‰å¹¶å½±å“ç†è§£çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€åˆ†å‰²å’Œé€‰æ‹©é•¿æ–‡æœ¬å—çš„ç®€å•æ–¹æ³•ï¼Œæ—¨åœ¨ä¸ºLLMsæä¾›æ›´æµç•…çš„è¾“å…¥ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¡ç®—ç›¸é‚»å¥å­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œåˆ©ç”¨è¾ƒä½çš„ç›¸ä¼¼åº¦è‡ªé€‚åº”åœ°å°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºå¯å˜é•¿åº¦çš„å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®­ç»ƒäº†ä¸€ä¸ªé—®é¢˜æ„ŸçŸ¥åˆ†ç±»å™¨ï¼Œä»¥é€‰æ‹©å¯¹å›ç­”ç‰¹å®šé—®é¢˜è‡³å…³é‡è¦çš„æ•æ„Ÿå—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå¼ºåŸºçº¿ï¼Œå¹¶ä¸”åœ¨å¤„ç†é•¿åº¦é«˜è¾¾256kä¸ªæ ‡è®°çš„åºåˆ—æ—¶ä¿æŒäº†è‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¶…é•¿æ–‡æœ¬æ—¶çš„ç†è§£å›°éš¾ï¼Œç°æœ‰æ–¹æ³•çš„å›ºå®šé•¿åº¦åˆ†å—æ–¹å¼å®¹æ˜“å¯¼è‡´è¯­ä¹‰ä¿¡æ¯çš„ä¸¢å¤±å’Œç†è§£çš„æ¨¡ç³Šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŠ¨æ€åˆ†å—å’Œé€‰æ‹©çš„æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—å¥å­é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œè‡ªé€‚åº”åœ°å°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºå¯å˜é•¿åº¦çš„å—ï¼Œä»¥æé«˜ç†è§£çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç¬¬ä¸€ï¼Œè®¡ç®—ç›¸é‚»å¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦ä»¥è¿›è¡ŒåŠ¨æ€åˆ†å—ï¼›ç¬¬äºŒï¼Œè®­ç»ƒä¸€ä¸ªé—®é¢˜æ„ŸçŸ¥åˆ†ç±»å™¨æ¥é€‰æ‹©å¯¹ç‰¹å®šé—®é¢˜é‡è¦çš„æ–‡æœ¬å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåŠ¨æ€åˆ†å—ç­–ç•¥ï¼Œåˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è€Œéå›ºå®šé•¿åº¦è¿›è¡Œæ–‡æœ¬åˆ†å‰²ï¼Œä»è€Œé¿å…äº†è¯­ä¹‰ä¿¡æ¯çš„ä¸¢å¤±ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§é˜ˆå€¼æ¥å†³å®šåˆ†å—çš„é•¿åº¦ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆè¯­ä¹‰ç›¸ä¼¼åº¦å’Œåˆ†ç±»å‡†ç¡®æ€§ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œä½¿ç”¨äº†Transformeræ¶æ„ä»¥å¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰å¼ºåŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿åº¦é«˜è¾¾256kä¸ªæ ‡è®°çš„è¾“å…¥ï¼Œå±•ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ³•å¾‹æ–‡ä¹¦åˆ†æã€åŒ»ç–—è®°å½•è§£è¯»ç­‰éœ€è¦å¤„ç†è¶…é•¿æ–‡æœ¬çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é•¿æ–‡æœ¬çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æå‡ä¿¡æ¯æå–ã€é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ‘˜è¦ç­‰ä»»åŠ¡çš„æ•ˆæœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) often struggle to accurately read and comprehend extremely long texts. Current methods for improvement typically rely on splitting long contexts into fixed-length chunks. However, fixed truncation risks separating semantically relevant content, leading to ambiguity and compromising accurate understanding. To overcome this limitation, we propose a straightforward approach for dynamically separating and selecting chunks of long context, facilitating a more streamlined input for LLMs. In particular, we compute semantic similarities between adjacent sentences, using lower similarities to adaptively divide long contexts into variable-length chunks. We further train a question-aware classifier to select sensitive chunks that are critical for answering specific questions. Experimental results on both single-hop and multi-hop question-answering benchmarks show that the proposed approach consistently outperforms strong baselines. Notably, it maintains robustness across a wide range of input lengths, handling sequences of up to 256k tokens. Our datasets and code are available at the following link: https://github.com/ECNU-Text-Computing/DCS

