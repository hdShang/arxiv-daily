---
layout: default
title: Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering
---

# Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00806" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.00806v1</a>
  <a href="https://arxiv.org/pdf/2506.00806.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00806v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00806v1', 'Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Songtao Jiang, Chenyi Zhou, Yan Zhang, Yeying Jin, Zuozhu Liu

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-06-01

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫FOCUS‰ª•Ëß£ÂÜ≥ËßÜËßâÈóÆÁ≠î‰∏≠ÁöÑÂ§çÊùÇÊé®ÁêÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÈóÆÁ≠î` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Â§çÊùÇÊé®ÁêÜ` `ÂèåÈáçËøáÁ®ãÁêÜËÆ∫` `Âä®ÊÄÅÈÄÇÂ∫îÊÄß` `ËÆ§Áü•Á≠ñÁï•` `ËßÜËßâ‰ø°ÊÅØÈÄâÊã©`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâÈóÆÁ≠îÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°Êó∂ÔºåÂæÄÂæÄÁõ≤ÁõÆÊ†áÊ≥®ÊâÄÊúâËßÜËßâÂØπË±°ÔºåÂØºËá¥‰ø°ÊÅØËøáËΩΩÂíåÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫FOCUSÊñπÊ≥ïÔºåÁªìÂêàÂø´ÈÄüÁõ¥Ëßâ‰∏éÊ∑±ÊÄùÁÜüËôëÁöÑÊé®ÁêÜÔºåÂä®ÊÄÅÈÄÇÂ∫îÈóÆÈ¢òÂ§çÊùÇÂ∫¶Ôºå‰ªéËÄåÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. Âú®Âõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÔºåFOCUSÊòæËëóÊèêÈ´ò‰∫ÜÂºÄÊ∫êÂíåÈªëÁÆ±MLLMsÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂ§öÁßçËÆ§Áü•Á≠ñÁï•‰∏éÁ≤æÁªÜËßÜËßâ‰ø°ÊÅØÁªìÂêàÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâÈóÆÁ≠îÔºàVQAÔºâ‰∏≠ÁöÑÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏ä‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöËøáÂºïÂÖ•ËßÜËßâÊèêÁ§∫Êù•ÊèêÂçáÊÄßËÉΩÔºå‰ΩÜÂ≠òÂú®Áõ≤ÁõÆÊ†áÊ≥®ÊâÄÊúâÊ£ÄÊµãÂØπË±°ÁöÑÈóÆÈ¢òÔºåÂØºËá¥ËßÜËßâÊ†áËÆ∞ËøáÂ§öÔºåÂΩ±Âìç‰ªªÂä°Ë°®Áé∞„ÄÇÂü∫‰∫éÂèåÈáçËøáÁ®ãÁêÜËÆ∫ÔºåÊàë‰ª¨ÊèêÂá∫FOCUSÔºåËøôÊòØ‰∏ÄÁßçÂä®ÊÄÅÈÄÇÂ∫îÈóÆÈ¢òÂ§çÊùÇÂ∫¶ÁöÑÊèí‰ª∂ÂºèÊñπÊ≥ïÔºåÁªìÂêàÂø´ÈÄüÁõ¥ËßâÂà§Êñ≠‰∏éÊ∑±ÊÄùÁÜüËôëÁöÑÂàÜÊûêÊé®ÁêÜ„ÄÇFOCUSÂú®ÁÆÄÂçïÈóÆÈ¢ò‰∏äÊîØÊåÅÈ´òÊïàÁöÑÈõ∂-shot Êé®ÁêÜÔºåËÄåÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÂàôÈááÁî®ËßÇÂØüÂâçÊ¶ÇÂøµÂåñÁ≠ñÁï•ÔºåÁ™ÅÂá∫ÂÖ≥ÈîÆÂÖÉÁ¥†„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåFOCUSÂú®Â§ö‰∏™Âü∫ÂáÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜMLLMsÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâÈóÆÁ≠î‰∏≠Â§çÊùÇÊé®ÁêÜ‰ªªÂä°ÁöÑÊÄßËÉΩÁì∂È¢àÔºåÁé∞ÊúâÊñπÊ≥ïÂõ†Áõ≤ÁõÆÊ†áÊ≥®ÊâÄÊúâÊ£ÄÊµãÂØπË±°ËÄåÂØºËá¥‰ø°ÊÅØËøáËΩΩÔºåÂΩ±Âìç‰ªªÂä°ÊïàÊûú„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöFOCUSÊñπÊ≥ïÂü∫‰∫éÂèåÈáçËøáÁ®ãÁêÜËÆ∫ÔºåÁªìÂêàÂø´ÈÄüÁõ¥Ëßâ‰∏éÊ∑±ÊÄùÁÜüËôëÁöÑÊé®ÁêÜÔºåÂä®ÊÄÅÈÄÇÂ∫îÈóÆÈ¢òÁöÑÂ§çÊùÇÊÄßÔºå‰ª•Á™ÅÂá∫ÂÖ≥ÈîÆËßÜËßâÂÖÉÁ¥†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöFOCUSÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö‰∏ÄÊòØÈíàÂØπÁÆÄÂçïÈóÆÈ¢òÁöÑÈõ∂-shotÊé®ÁêÜÊîØÊåÅÔºå‰∫åÊòØÈíàÂØπÂ§çÊùÇ‰ªªÂä°ÁöÑËßÇÂØüÂâçÊ¶ÇÂøµÂåñÁ≠ñÁï•ÔºåÁ°Æ‰øùÂÖ≥ÈîÆÂÖÉÁ¥†ÁöÑÁ™ÅÂá∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöFOCUSÁöÑÊúÄÂ§ßÂàõÊñ∞Âú®‰∫éÂÖ∂Âä®ÊÄÅÈÄÇÂ∫îÊÄßÔºåËÉΩÂ§üÊ†πÊçÆÈóÆÈ¢òÁöÑÂ§çÊùÇÂ∫¶ÈÄâÊã©ÂêàÈÄÇÁöÑÊé®ÁêÜÁ≠ñÁï•ÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÁõ≤ÁõÆÊ†áÊ≥®ÊñπÂºèÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöFOCUSÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÁÅµÊ¥ªË∞ÉÊï¥ÔºåÈááÁî®ÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñËßÜËßâ‰ø°ÊÅØÁöÑÈÄâÊã©ÔºåÁ°Æ‰øùÂú®‰∏çÂêåÂ§çÊùÇÂ∫¶ÁöÑÈóÆÈ¢ò‰∏≠ÂùáËÉΩÊúâÊïàÊèêÂçáÊé®ÁêÜÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

FOCUSÂú®ScienceQA„ÄÅTextQA„ÄÅVizWizÂíåMMEÂõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÂçá‰∫ÜMLLMsÁöÑÊÄßËÉΩÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞10%‰ª•‰∏äÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅÊïôËÇ≤ËæÖÂä©Â∑•ÂÖ∑Âíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇFOCUSÊñπÊ≥ïËÉΩÂ§üÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÊèê‰æõÊõ¥Á≤æÂáÜÁöÑËßÜËßâÁêÜËß£ÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÔºåÊú™Êù•ÂèØËÉΩÂú®Â§öÊ®°ÊÄÅÂ≠¶‰π†ÂíåÊé®ÁêÜÈ¢ÜÂüü‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) still struggle with complex reasoning tasks in Visual Question Answering (VQA). While current methods have advanced by incorporating visual prompts, our study uncovers critical limitations: these approaches indiscriminately annotate all detected objects for every visual question, generating excessive visual markers that degrade task performance. This issue stems primarily from a lack of focus on key visual elements, raising two important questions: Are all objects equally important, and do all questions require visual prompts? Motivated by Dual Process Theory, which distinguishes between instinctive and deliberate cognitive modes in human reasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts to the complexity of questions, combining fast intuitive judgments with deliberate analytical reasoning to enhance the vision-language reasoning capability of the MLLM. For straightforward questions, FOCUS supports efficient zero-shot reasoning. For more complex tasks, it employs the conceptualizing before observation strategy to highlight critical elements. Extensive experiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate that FOCUS consistently improves the performance of both open-source and black-box MLLMs, achieving significant gains across all datasets. Ablation studies further validate the importance of combining diverse cognitive strategies with refined visual information for superior performance. Code will be released.

