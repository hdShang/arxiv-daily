---
layout: default
title: Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models
---

# Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03469" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03469v2</a>
  <a href="https://arxiv.org/pdf/2505.03469.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03469v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03469v2', 'Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bin Yu, Hang Yuan, Haotian Li, Xueyin Xu, Yuliang Wei, Bailing Wang, Weizhen Qi, Kai Chen

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-05-21)

**å¤‡æ³¨**: 12 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé•¿çŸ­é“¾æ€ç»´æ··åˆç›‘ç£å¾®è°ƒä»¥è§£å†³æ¨¡å‹è¿‡åº¦æ€è€ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿çŸ­é“¾æ€ç»´` `ç›‘ç£å¾®è°ƒ` `æ¨ç†èƒ½åŠ›` `æ¨¡å‹ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®¹æ˜“äº§ç”Ÿå†—é•¿å’Œé‡å¤çš„æ¨ç†é“¾ï¼Œå¯¼è‡´æ¨¡å‹çš„å“åº”æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„é•¿çŸ­é“¾æ€ç»´æ··åˆç›‘ç£å¾®è°ƒï¼ˆLS-Mixture SFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆé•¿çŸ­æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨å‡å°‘æ¨¡å‹çš„å†—ä½™æ¨ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLS-Mixture SFTæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†2.3%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å“åº”é•¿åº¦å‡å°‘äº†çº¦47.61%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•è¡¨æ˜ï¼Œä½¿ç”¨ä»å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek R1ï¼‰è’¸é¦çš„é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥æœ‰æ•ˆåœ°å°†æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°éæ¨ç†æ¨¡å‹ã€‚ç„¶è€Œï¼Œé‡‡ç”¨è¿™ç§æ–¹æ³•å¾®è°ƒçš„æ¨¡å‹ç»§æ‰¿äº†æ•™å¸ˆæ¨¡å‹çš„â€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå†—é•¿ä¸”é‡å¤çš„æ¨ç†é“¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†é•¿çŸ­é“¾æ€ç»´æ··åˆç›‘ç£å¾®è°ƒï¼ˆLS-Mixture SFTï¼‰ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†é•¿CoTæ¨ç†æ•°æ®é›†å’Œé€šè¿‡ç»“æ„ä¿ç•™é‡å†™è·å¾—çš„çŸ­æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨LS-Mixture SFTæ–¹æ³•è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡å‡†ç¡®ç‡æé«˜äº†2.3%ï¼ŒåŒæ—¶æ¨¡å‹å“åº”é•¿åº¦æ˜¾è‘—å‡å°‘äº†çº¦47.61%ã€‚è¯¥ç ”ç©¶ä¸ºé€šè¿‡ç›‘ç£å¾®è°ƒèµ‹äºˆéæ¨ç†æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æ–¹æ³•ï¼ŒåŒæ—¶é¿å…äº†ä»æ•™å¸ˆæ¨¡å‹ç»§æ‰¿çš„å›ºæœ‰è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆæ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå†—é•¿å’Œé‡å¤çš„æ¨ç†é“¾ï¼Œå¯¼è‡´æ¨¡å‹æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›çš„è½¬ç§»ä¸Šå­˜åœ¨â€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜ï¼Œå½±å“äº†æ¨¡å‹çš„å®é™…åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„LS-Mixture SFTæ–¹æ³•é€šè¿‡ç»“åˆé•¿é“¾å’ŒçŸ­é“¾çš„æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨æœ‰æ•ˆå‡å°‘æ¨¡å‹çš„å†—ä½™æ¨ç†ã€‚é•¿é“¾æ•°æ®æä¾›äº†ä¸°å¯Œçš„æ¨ç†ä¿¡æ¯ï¼Œè€ŒçŸ­é“¾æ•°æ®åˆ™é€šè¿‡ç»“æ„ä¿ç•™é‡å†™æ¥ç®€åŒ–æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„æ„å»ºã€æ¨¡å‹çš„å¾®è°ƒå’Œæ¨ç†æ€§èƒ½çš„è¯„ä¼°ã€‚é¦–å…ˆï¼Œæ„å»ºé•¿çŸ­é“¾æ¨ç†æ•°æ®é›†ï¼›ç„¶åï¼Œåˆ©ç”¨è¿™äº›æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼›æœ€åï¼Œé€šè¿‡å¤šç§åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå“åº”æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†é•¿é“¾å’ŒçŸ­é“¾æ¨ç†æ•°æ®ç»“åˆä½¿ç”¨ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­å­˜åœ¨çš„è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚è¿™ç§æ··åˆæ–¹æ³•ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œè®ºæ–‡è¯¦ç»†æè¿°äº†æ•°æ®é›†çš„æ„å»ºæ–¹æ³•ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠæ¨¡å‹æ¶æ„çš„è°ƒæ•´ã€‚ç‰¹åˆ«æ˜¯åœ¨æŸå¤±å‡½æ•°ä¸­ï¼Œå¼ºè°ƒäº†å¯¹å†—ä½™æ¨ç†çš„æƒ©ç½šæœºåˆ¶ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹ç”Ÿæˆæ›´ç®€æ´çš„æ¨ç†é“¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LS-Mixture SFTæ–¹æ³•è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡å‡†ç¡®ç‡æé«˜äº†2.3%ï¼ŒåŒæ—¶å“åº”é•¿åº¦å‡å°‘äº†çº¦47.61%ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒLS-Mixture SFTæ–¹æ³•å¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œå‡å°‘è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models have demonstrated that Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning capabilities to non-reasoning models. However, models fine-tuned with this approach inherit the "overthinking" problem from teacher models, producing verbose and redundant reasoning chains during inference. To address this challenge, we propose Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning (LS-Mixture SFT), which combines long CoT reasoning dataset with their short counterparts obtained through structure-preserved rewriting. Our experiments demonstrate that models trained using the LS-Mixture SFT method, compared to those trained with direct SFT, achieved an average accuracy improvement of 2.3% across various benchmarks while substantially reducing model response length by approximately 47.61%. This work offers an approach to endow non-reasoning models with reasoning capabilities through supervised fine-tuning while avoiding the inherent overthinking problems inherited from teacher models, thereby enabling efficient reasoning in the fine-tuned models.

