---
layout: default
title: "SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation"
---

# SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03273" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03273v2</a>
  <a href="https://arxiv.org/pdf/2505.03273.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03273v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03273v2', 'SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaoxi Mu, Xinyu Yang, Gang Wang

**åˆ†ç±»**: cs.SD, cs.CL, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-05-26)

**å¤‡æ³¨**: Appears in IJCAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSepALMä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸‹è¯­éŸ³åˆ†ç¦»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­éŸ³åˆ†ç¦»` `éŸ³é¢‘è¯­è¨€æ¨¡å‹` `é”™è¯¯ä¿®æ­£` `é“¾å¼æ€ç»´` `çŸ¥è¯†è’¸é¦` `é²æ£’æ€§` `å£°å­¦é€‚åº”æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“äº§ç”Ÿä¼ªå½±å’Œå¤±çœŸï¼Œå½±å“åˆ†ç¦»æ•ˆæœã€‚
2. SepALMé€šè¿‡éŸ³é¢‘è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­éŸ³çº æ­£å’Œé‡åˆæˆï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯çš„é”™è¯¯ä¿®æ­£æœºåˆ¶ï¼Œæå‡åˆ†ç¦»è´¨é‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSepALMåœ¨è¯­éŸ³åˆ†ç¦»ç²¾åº¦å’Œé€‚åº”æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œé€‚ç”¨äºå¤šç§å£°å­¦ç¯å¢ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡ç°æœ‰çš„è¯­éŸ³åˆ†ç¦»æŠ€æœ¯èƒ½å¤Ÿå¤„ç†é•¿æ—¶é—´çš„æ··åˆéŸ³é¢‘æ³¢å½¢ï¼Œä½†åœ¨å˜ˆæ‚å’Œæ··å“ç­‰å¤æ‚ç¯å¢ƒä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´åˆ†ç¦»è¯­éŸ³å‡ºç°ä¼ªå½±æˆ–å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†SepALMï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰åœ¨æ–‡æœ¬åŸŸå†…çº æ­£å’Œé‡æ–°åˆæˆè¯­éŸ³çš„åˆ›æ–°æ–¹æ³•ã€‚SepALMç”±å››ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼šåˆ†ç¦»å™¨ã€çº æ­£å™¨ã€åˆæˆå™¨å’Œå¯¹é½å™¨ã€‚é€šè¿‡é›†æˆåŸºäºALMçš„ç«¯åˆ°ç«¯é”™è¯¯ä¿®æ­£æœºåˆ¶ï¼Œé™ä½äº†é”™è¯¯ç´¯ç§¯çš„é£é™©ï¼Œå¹¶è§„é¿äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å°†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆæ—¶é‡åˆ°çš„ä¼˜åŒ–éš¾é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSepALMä¸ä»…æé«˜äº†è¯­éŸ³åˆ†ç¦»çš„ç²¾åº¦ï¼Œè¿˜æ˜¾è‘—å¢å¼ºäº†åœ¨æ–°å£°å­¦ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å˜ˆæ‚å’Œæ··å“ç¯å¢ƒä¸­è¿›è¡Œè¯­éŸ³åˆ†ç¦»æ—¶å‡ºç°çš„ä¼ªå½±å’Œå¤±çœŸé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚éŸ³é¢‘æ—¶å®¹æ˜“å‡ºç°é”™è¯¯ç´¯ç§¯ï¼Œå¯¼è‡´åˆ†ç¦»æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSepALMçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMsï¼‰è¿›è¡Œè¯­éŸ³çš„çº æ­£å’Œé‡åˆæˆï¼Œé€šè¿‡åœ¨æ–‡æœ¬åŸŸå†…è¿›è¡Œå¤„ç†ï¼Œå‡å°‘é”™è¯¯çš„ä¼ æ’­å’Œä¼˜åŒ–éš¾é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSepALMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šåˆ†ç¦»å™¨è´Ÿè´£åˆæ­¥çš„è¯­éŸ³åˆ†ç¦»ï¼Œçº æ­£å™¨ç”¨äºä¿®æ­£åˆ†ç¦»åçš„è¯­éŸ³ï¼Œåˆæˆå™¨å°†ä¿®æ­£åçš„è¯­éŸ³é‡æ–°åˆæˆï¼Œè€Œå¯¹é½å™¨åˆ™ç¡®ä¿è¯­éŸ³ä¸æ–‡æœ¬çš„å‡†ç¡®å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSepALMçš„æœ€å¤§åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºALMçš„ç«¯åˆ°ç«¯é”™è¯¯ä¿®æ­£æœºåˆ¶ï¼Œè¿™ä¸€æœºåˆ¶æœ‰æ•ˆåœ°å‡å°‘äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¸¸è§çš„é”™è¯¯ç´¯ç§¯é—®é¢˜ï¼Œä¸ç°æœ‰çš„ASRå’ŒLLMç»“åˆæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒSepALMé‡‡ç”¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä»¥å¢å¼ºALMçš„æ¨ç†å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œå…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSepALMåœ¨è¯­éŸ³åˆ†ç¦»ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜äº†çº¦15%çš„åˆ†ç¦»ç²¾åº¦ï¼Œå¹¶åœ¨æ–°å£°å­¦ç¯å¢ƒä¸­çš„é€‚åº”æ€§æå‡æ˜¾è‘—ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SepALMåœ¨è¯­éŸ³åˆ†ç¦»é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦åœ¨å˜ˆæ‚ç¯å¢ƒä¸­è¿›è¡Œè¯­éŸ³è¯†åˆ«å’Œå¤„ç†çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€ä¼šè®®è®°å½•å’Œè¯­éŸ³ç¿»è¯‘ç­‰ã€‚å…¶åˆ›æ–°çš„é”™è¯¯ä¿®æ­£æœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿçš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œæœªæ¥å¯æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments.

