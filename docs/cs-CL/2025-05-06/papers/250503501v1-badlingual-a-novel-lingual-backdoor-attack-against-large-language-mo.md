---
layout: default
title: "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models"
---

# BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03501" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03501v1</a>
  <a href="https://arxiv.org/pdf/2505.03501.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03501v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03501v1', 'BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihan Wang, Hongwei Li, Rui Zhang, Wenbo Jiang, Kangjie Chen, Tianwei Zhang, Qingchuan Zhao, Guowen Xu

**åˆ†ç±»**: cs.CR, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBadLingualä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„è¯­è¨€åé—¨æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€åé—¨æ”»å‡»` `å¤§è¯­è¨€æ¨¡å‹` `å¯¹æŠ—è®­ç»ƒ` `ä»»åŠ¡æ— å…³` `ç½‘ç»œå®‰å…¨` `æ¨¡å‹é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºçº¿è¯­è¨€åé—¨æ”»å‡»åœ¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥é€‚åº”å®é™…åº”ç”¨åœºæ™¯ã€‚
2. è®ºæ–‡æå‡ºçš„BadLingualæ˜¯ä¸€ç§ä»»åŠ¡æ— å…³çš„è¯­è¨€åé—¨æ”»å‡»ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æœ‰æ•ˆè§¦å‘ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBadLingualåœ¨ä»»åŠ¡æ— å…³åœºæ™¯ä¸‹çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ç›¸æ¯”åŸºçº¿æå‡äº†37.35%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„åé—¨æ”»å‡»å½¢å¼ï¼šè¯­è¨€åé—¨æ”»å‡»ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºè¯­è¨€æœ¬èº«ä½œä¸ºè§¦å‘å™¨ï¼ŒåŠ«æŒæ„ŸæŸ“çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç…½åŠ¨æ€§è¨€è®ºã€‚è¿™ç§æ”»å‡»èƒ½å¤Ÿç²¾å‡†é’ˆå¯¹ç‰¹å®šè¯­è¨€ç¾¤ä½“ï¼ŒåŠ©é•¿æ¶æ„å®ä½“çš„ç§æ—æ­§è§†ã€‚æˆ‘ä»¬é¦–å…ˆå®ç°äº†ä¸€ä¸ªåŸºçº¿è¯­è¨€åé—¨æ”»å‡»ï¼Œé€šè¿‡å°†ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ç¿»è¯‘ä¸ºè§¦å‘è¯­è¨€è¿›è¡Œæ•°æ®æ±¡æŸ“ã€‚ç„¶è€Œï¼Œè¯¥åŸºçº¿æ”»å‡»åœ¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸”åœ¨å®é™…åº”ç”¨ä¸­ä¸å¤Ÿå®ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†BadLingualï¼Œä¸€ç§æ–°å‹çš„ä»»åŠ¡æ— å…³è¯­è¨€åé—¨ï¼Œèƒ½å¤Ÿåœ¨èŠå¤©å‹å¤§è¯­è¨€æ¨¡å‹ä¸­è§¦å‘ä»»ä½•ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬é‡‡ç”¨PPLçº¦æŸçš„è´ªå©ªåæ ‡æ¢¯åº¦æœç´¢ï¼ˆPGCGï¼‰è¿›è¡Œå¯¹æŠ—è®­ç»ƒï¼Œæ‰©å±•è¯­è¨€åé—¨çš„å†³ç­–è¾¹ç•Œï¼Œä»è€Œå¢å¼ºå…¶åœ¨å„ç§ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºçº¿æ”»å‡»åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„ASRè¶…è¿‡90%ï¼Œè€Œåœ¨ä»»åŠ¡æ— å…³åœºæ™¯ä¸‹ä»…ä¸º37.61%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒBadLingualåœ¨åŸºçº¿ä¹‹ä¸Šæå‡äº†37.35%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€åé—¨æ”»å‡»åœ¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚åŸºçº¿æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆé€‚åº”å¤šæ ·åŒ–çš„ä¸‹æ¸¸ä»»åŠ¡åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBadLingualçš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§ä»»åŠ¡æ— å…³çš„è¯­è¨€åé—¨æ”»å‡»ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒæ‰©å±•å†³ç­–è¾¹ç•Œï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­å‡èƒ½æœ‰æ•ˆè§¦å‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ±¡æŸ“æ¨¡å—ã€å¯¹æŠ—è®­ç»ƒæ¨¡å—å’Œå†³ç­–è¾¹ç•Œæ‰©å±•æ¨¡å—ã€‚æ•°æ®æ±¡æŸ“æ¨¡å—è´Ÿè´£å°†è®­ç»ƒæ•°æ®ç¿»è¯‘ä¸ºè§¦å‘è¯­è¨€ï¼Œå¯¹æŠ—è®­ç»ƒæ¨¡å—ä½¿ç”¨PPLçº¦æŸçš„è´ªå©ªåæ ‡æ¢¯åº¦æœç´¢è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†PGCGæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æŠ—è®­ç»ƒå¢å¼ºäº†è¯­è¨€åé—¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æœ‰æ•ˆè§¦å‘ï¼Œå…‹æœäº†åŸºçº¿æ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†PPLçº¦æŸæ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„è§¦å‘è¯­è¨€åœ¨å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼ŒåŒæ—¶å¯¹æŠ—è®­ç»ƒçš„æŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿè€ƒè™‘äº†ä»»åŠ¡æ— å…³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºçº¿æ”»å‡»åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰è¶…è¿‡90%ï¼Œè€Œåœ¨ä»»åŠ¡æ— å…³åœºæ™¯ä¸‹ä»…ä¸º37.61%ã€‚BadLingualåœ¨æ­¤åŸºç¡€ä¸Šæå‡äº†37.35%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœæ”¹è¿›ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šä»»åŠ¡ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œå®‰å…¨ã€ç¤¾äº¤åª’ä½“ç›‘æ§å’Œå†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„è„†å¼±æ€§ï¼Œç ”ç©¶å¯ä»¥ä¿ƒè¿›æœªæ¥é˜²å¾¡æœºåˆ¶çš„å¼€å‘ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ï¼Œå‡å°‘æ¶æ„æ”»å‡»çš„é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present a new form of backdoor attack against Large Language Models (LLMs): lingual-backdoor attacks. The key novelty of lingual-backdoor attacks is that the language itself serves as the trigger to hijack the infected LLMs to generate inflammatory speech. They enable the precise targeting of a specific language-speaking group, exacerbating racial discrimination by malicious entities. We first implement a baseline lingual-backdoor attack, which is carried out by poisoning a set of training data for specific downstream tasks through translation into the trigger language. However, this baseline attack suffers from poor task generalization and is impractical in real-world settings. To address this challenge, we design BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any downstream tasks within the chat LLMs, regardless of the specific questions of these tasks. We design a new approach using PPL-constrained Greedy Coordinate Gradient-based Search (PGCG) based adversarial training to expand the decision boundary of lingual-backdoor, thereby enhancing the generalization ability of lingual-backdoor across various tasks. We perform extensive experiments to validate the effectiveness of our proposed attacks. Specifically, the baseline attack achieves an ASR of over 90% on the specified tasks. However, its ASR reaches only 37.61% across six tasks in the task-agnostic scenario. In contrast, BadLingual brings up to 37.35% improvement over the baseline. Our study sheds light on a new perspective of vulnerabilities in LLMs with multilingual capabilities and is expected to promote future research on the potential defenses to enhance the LLMs' robustness

