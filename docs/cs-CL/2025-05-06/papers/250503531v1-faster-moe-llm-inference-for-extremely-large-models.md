---
layout: default
title: Faster MoE LLM Inference for Extremely Large Models
---

# Faster MoE LLM Inference for Extremely Large Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03531v1</a>
  <a href="https://arxiv.org/pdf/2505.03531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03531v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03531v1', 'Faster MoE LLM Inference for Extremely Large Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoqi Yang, Luohe Shi, Qiwei Li, Zuchao Li, Ping Wang, Bo Du, Mengjia Shen, Hai Zhao

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFaster MoE LLMæ¨ç†æ–¹æ³•ä»¥ä¼˜åŒ–è¶…å¤§æ¨¡å‹æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†ä¼˜åŒ–` `ç»†ç²’åº¦æ¨¡å‹` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„MoEæ¨¡å‹ä¼˜åŒ–ä¸»è¦é›†ä¸­åœ¨ç²—ç²’åº¦æ¶æ„ï¼Œç»†ç²’åº¦æ¨¡å‹çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œå¯¼è‡´æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡å°šæœªå¾—åˆ°å……åˆ†ç†è§£ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘æ¿€æ´»ä¸“å®¶çš„æ•°é‡æ¥æé«˜MoEæ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½çš„ç¨³å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°æ–¹æ³•åï¼Œååé‡è‡³å°‘æé«˜10%ï¼Œä¸”åœ¨å‡å°‘æ¿€æ´»ä¸“å®¶æ•°é‡çš„æƒ…å†µä¸‹æ€§èƒ½ä¸‹é™è¾ƒå°ï¼Œå±•ç¤ºäº†ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€æ¸æˆä¸ºè¶…å¤§è§„æ¨¡æ¨¡å‹çš„ä¸»æµæ–¹æ³•ã€‚ç°æœ‰çš„MoEæ¨¡å‹ä¼˜åŒ–ä¸»è¦é›†ä¸­åœ¨ç²—ç²’åº¦æ¶æ„ä¸Šï¼Œè€Œç»†ç²’åº¦MoEæ¨¡å‹çš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨ä¸åŒæœåŠ¡è´Ÿè½½ä¸‹çš„æ•ˆç‡åŠ¨æ€ï¼Œå¹¶å‘ç°å‡å°‘æ¿€æ´»ä¸“å®¶æ•°é‡åœ¨æŸäº›åœºæ™¯ä¸‹å¯ä»¥æ˜¾è‘—æé«˜æ•ˆç‡ï¼Œä¸”æ€§èƒ½ä¸‹é™è¾ƒå°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡éƒ¨ç½²MoEæ¨¡å‹é¢ä¸´æ›´å¤§æŒ‘æˆ˜ï¼Œä½†ä¹Ÿæä¾›äº†æ˜¾è‘—çš„ä¼˜åŒ–æœºä¼šã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆèƒ½å¤Ÿåœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹æé«˜è‡³å°‘10%çš„ååé‡ï¼Œè¡¨æ˜MoEæ¨ç†ä¼˜åŒ–ä»æœ‰å·¨å¤§çš„æ¢ç´¢å’Œæ”¹è¿›æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰MoEæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç»†ç²’åº¦æ¨¡å‹çš„åº”ç”¨ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨æœåŠ¡è´Ÿè½½å˜åŒ–æ—¶çš„è¡¨ç°å°šä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æä¸åŒæ•°é‡çš„æ¿€æ´»ä¸“å®¶å¯¹æ¨ç†æ•ˆç‡å’Œæ€§èƒ½çš„å½±å“ï¼Œæå‡ºä¸€ç§ä¼˜åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨åœ¨ä¿è¯æ€§èƒ½çš„å‰æä¸‹æé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€ä¸“å®¶é€‰æ‹©æ¨¡å—å’Œæ¨ç†æ‰§è¡Œæ¨¡å—ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„èµ„æºä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†åœ¨ç»†ç²’åº¦MoEæ¨¡å‹ä¸­åŠ¨æ€è°ƒæ•´æ¿€æ´»ä¸“å®¶æ•°é‡çš„ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿçš„ç²—ç²’åº¦æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„æœåŠ¡è´Ÿè½½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œä¼˜åŒ–äº†æ¿€æ´»ä¸“å®¶çš„é€‰æ‹©æœºåˆ¶ï¼Œå¹¶è®¾è®¡äº†æ–°çš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ•ˆç‡ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œç¡®ä¿åœ¨å‡å°‘ä¸“å®¶æ•°é‡æ—¶æ€§èƒ½æŸå¤±æœ€å°åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ–°æå‡ºçš„æ¨ç†ä¼˜åŒ–æ–¹æ³•åï¼Œæ¨¡å‹çš„ååé‡æé«˜äº†è‡³å°‘10%ï¼Œä¸”åœ¨å‡å°‘æ¿€æ´»ä¸“å®¶æ•°é‡çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä»…æœ‰è½»å¾®ä¸‹é™ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–MoEæ¨ç†å…·æœ‰æ˜¾è‘—çš„å®é™…ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œå¤§è§„æ¨¡æ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–MoEæ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼Œå¯ä»¥åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´å¤§è§„æ¨¡æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œä¿ƒè¿›AIæŠ€æœ¯çš„å¹¿æ³›æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually becoming the mainstream approach for ultra-large-scale models. Existing optimization efforts for MoE models have focused primarily on coarse-grained MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE models are gaining popularity, yet research on them remains limited. Therefore, we want to discuss the efficiency dynamic under different service loads. Additionally, fine-grained models allow deployers to reduce the number of routed experts, both activated counts and total counts, raising the question of how this reduction affects the trade-off between MoE efficiency and performance. Our findings indicate that while deploying MoE models presents greater challenges, it also offers significant optimization opportunities. Reducing the number of activated experts can lead to substantial efficiency improvements in certain scenarios, with only minor performance degradation. Reducing the total number of experts provides limited efficiency gains but results in severe performance degradation. Our method can increase throughput by at least 10\% without any performance degradation. Overall, we conclude that MoE inference optimization remains an area with substantial potential for exploration and improvement.

