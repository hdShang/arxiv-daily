---
layout: default
title: SLOT: Structuring the Output of Large Language Models
---

# SLOT: Structuring the Output of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04016" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04016v1</a>
  <a href="https://arxiv.org/pdf/2505.04016.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04016v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04016v1', 'SLOT: Structuring the Output of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Darren Yow-Bang Wang, Zhengyuan Shen, Soumya Smruti Mishra, Zhichao Xu, Yifei Teng, Haibo Ding

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSLOTä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºç»“æ„åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–è¾“å‡º` `ä¿¡æ¯æå–` `æ¨¡å‹æ— å…³` `åå¤„ç†æœºåˆ¶` `æ•°æ®ç­–åˆ’` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºæ—¶å¸¸å¸¸åç¦»é¢„å®šä¹‰æ¨¡å¼ï¼Œå¯¼è‡´åº”ç”¨å¼€å‘çš„å¯é æ€§é™ä½ã€‚
2. SLOTé€šè¿‡å°†éç»“æ„åŒ–è¾“å‡ºè½¬åŒ–ä¸ºç»“æ„åŒ–æ ¼å¼ï¼Œé‡‡ç”¨è½»é‡çº§è¯­è¨€æ¨¡å‹ä½œä¸ºåå¤„ç†å±‚ï¼Œæå‡äº†çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSLOTä½¿å¾—Mistral-7Bæ¨¡å‹åœ¨æ¨¡å¼å‡†ç¡®æ€§å’Œå†…å®¹ç›¸ä¼¼æ€§ä¸Šå‡ä¼˜äºå…¶ä»–å¤§å‹æ¨¡å‹ï¼Œä¸”å°å‹æ¨¡å‹ä¹Ÿèƒ½å®ç°é«˜æ•ˆè¾“å‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»“æ„åŒ–è¾“å‡ºåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…³é”®åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå¦‚ä»£ç†å’Œä¿¡æ¯æå–ã€‚ç„¶è€Œï¼ŒLLMsç»å¸¸ç”Ÿæˆåç¦»é¢„å®šä¹‰æ¨¡å¼çš„è¾“å‡ºï¼Œä¸¥é‡å½±å“å¯é çš„åº”ç”¨å¼€å‘ã€‚æœ¬æ–‡æå‡ºSLOTï¼ˆç»“æ„åŒ–LLMè¾“å‡ºå˜æ¢å™¨ï¼‰ï¼Œä¸€ç§æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œå°†éç»“æ„åŒ–LLMè¾“å‡ºè½¬åŒ–ä¸ºç²¾ç¡®çš„ç»“æ„åŒ–æ ¼å¼ã€‚ä¸ç°æœ‰ä¾èµ–äºçº¦æŸè§£ç æŠ€æœ¯æˆ–ç´§å¯†è€¦åˆç‰¹å®šæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆä¸åŒï¼ŒSLOTé‡‡ç”¨ç»è¿‡å¾®è°ƒçš„è½»é‡çº§è¯­è¨€æ¨¡å‹ä½œä¸ºåå¤„ç†å±‚ï¼Œå®ç°äº†åœ¨å„ç§LLMå’Œæ¨¡å¼è§„èŒƒä¹‹é—´çš„çµæ´»æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç³»ç»Ÿçš„æ•°æ®ç­–åˆ’å’Œåˆæˆæµç¨‹ï¼Œä»¥åŠé‡åŒ–æ¨¡å¼å‡†ç¡®æ€§å’Œå†…å®¹ä¿çœŸåº¦çš„æ­£å¼è¯„ä¼°æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„Mistral-7Bæ¨¡å‹åœ¨çº¦æŸè§£ç ä¸‹å®ç°äº†æ¥è¿‘å®Œç¾çš„æ¨¡å¼å‡†ç¡®æ€§ï¼ˆ99.5%ï¼‰å’Œå†…å®¹ç›¸ä¼¼æ€§ï¼ˆ94.0%ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºClaude-3.5-Sonnetã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè¾“å‡ºæ—¶åç¦»é¢„å®šä¹‰ç»“æ„çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–äºç‰¹å®šæ¨¡å‹æˆ–çº¦æŸè§£ç ï¼Œç¼ºä¹çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSLOTé€šè¿‡å¼•å…¥è½»é‡çº§è¯­è¨€æ¨¡å‹ä½œä¸ºåå¤„ç†å±‚ï¼Œå°†éç»“æ„åŒ–è¾“å‡ºè½¬åŒ–ä¸ºç»“æ„åŒ–æ ¼å¼ï¼Œä»è€Œå®ç°å¯¹å¤šç§æ¨¡å‹å’Œæ¨¡å¼çš„é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSLOTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç­–åˆ’ã€åˆæˆå’Œåå¤„ç†ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®ç­–åˆ’è´Ÿè´£æ”¶é›†å’Œå‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œåˆæˆæ¨¡å—ç”Ÿæˆåˆæ­¥è¾“å‡ºï¼Œåå¤„ç†å±‚åˆ™å°†è¾“å‡ºè½¬åŒ–ä¸ºç»“æ„åŒ–æ ¼å¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šSLOTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ¨¡å‹æ— å…³æ€§å’Œåå¤„ç†æœºåˆ¶ï¼Œä½¿å¾—ä¸åŒæ¨¡å‹çš„è¾“å‡ºéƒ½èƒ½è¢«æœ‰æ•ˆè½¬åŒ–ä¸ºç»“æ„åŒ–æ ¼å¼ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSLOTä½¿ç”¨äº†ç»è¿‡å¾®è°ƒçš„è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è¾“å‡ºçš„ç»“æ„åŒ–ç¨‹åº¦å’Œå†…å®¹ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„Mistral-7Bæ¨¡å‹åœ¨çº¦æŸè§£ç ä¸‹å®ç°äº†99.5%çš„æ¨¡å¼å‡†ç¡®æ€§å’Œ94.0%çš„å†…å®¹ç›¸ä¼¼æ€§ï¼Œæ˜¾è‘—ä¼˜äºClaude-3.5-Sonnetï¼Œåˆ†åˆ«æå‡äº†25å’Œ20ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼ŒSLOTè¿˜ä½¿å¾—å°å‹æ¨¡å‹å¦‚Llama-3.2-1Båœ¨ç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›ä¸Šä¸å¤§å‹æ¨¡å‹ç›¸å½“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SLOTçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æ™ºèƒ½ä»£ç†ã€ä¿¡æ¯æå–å’Œæ•°æ®åˆ†æç­‰ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„è¾“å‡ºç»“æ„åŒ–èƒ½åŠ›ï¼ŒSLOTèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°å¯é çš„ç»“æ„åŒ–ç”Ÿæˆï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.

