---
layout: default
title: Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation
---

# Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03320" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03320v2</a>
  <a href="https://arxiv.org/pdf/2505.03320.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03320v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03320v2', 'Recall with Reasoning: Chain-of-Thought Distillation for Mamba\'s Long-Context Memory and Extrapolation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junyu Ma, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-06-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRecall with Reasoningæ–¹æ³•ä»¥æå‡Mambaçš„é•¿ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡è®°å¿†` `é“¾å¼æ€ç»´` `è’¸é¦è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¶…é•¿åºåˆ—æ—¶æ€§èƒ½ä¸‹é™ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨Mambaçš„é•¿ä¸Šä¸‹æ–‡æ½œåŠ›ã€‚
2. æå‡ºçš„RwRæ–¹æ³•é€šè¿‡è’¸é¦é“¾å¼æ€ç»´æ€»ç»“ï¼Œå¢å¼ºMambaåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„å›å¿†å’Œæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRwRåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå¯¹æ¯”åŸºçº¿ï¼Œä¸”çŸ­ä¸Šä¸‹æ–‡æ€§èƒ½æœªå—å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Mambaåœ¨ç†è®ºä¸Šå…·å¤‡æ— é™ä¸Šä¸‹æ–‡æ½œåŠ›ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå½“åºåˆ—é•¿åº¦è¿œè¶…è®­ç»ƒé•¿åº¦æ—¶ï¼Œå…¶æ€§èƒ½å—åˆ°é™åˆ¶ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•Recall with Reasoningï¼ˆRwRï¼‰ï¼Œæ¢ç´¢è§£é”Mambaçš„é•¿ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›ã€‚RwRé€šè¿‡ä»æ•™å¸ˆæ¨¡å‹ä¸­è’¸é¦é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ€»ç»“ï¼Œå°†è¿™äº›æ€»ç»“ä½œä¸ºCoTæç¤ºé¢„ç½®äºå¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ•™ä¼šMambaä¸»åŠ¨å›å¿†å’Œæ¨ç†é•¿ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRwRåœ¨LONGMEMEVALå’ŒHELMETæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†Mambaçš„é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ï¼Œç›¸è¾ƒäºç±»ä¼¼é¢„è®­ç»ƒæ¡ä»¶ä¸‹çš„Transformer/æ··åˆåŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†çŸ­ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œä¸”æ— éœ€å¯¹æ¶æ„è¿›è¡Œæ›´æ”¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³Mambaåœ¨å¤„ç†è¶…é•¿åºåˆ—æ—¶æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹è¶…è¿‡è®­ç»ƒé•¿åº¦çš„åºåˆ—æ—¶ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å…¶é•¿ä¸Šä¸‹æ–‡è®°å¿†èƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„RwRæ–¹æ³•é€šè¿‡è’¸é¦é“¾å¼æ€ç»´æ€»ç»“ï¼Œä½œä¸ºæç¤ºå¼•å¯¼Mambaè¿›è¡Œé•¿ä¸Šä¸‹æ–‡çš„å›å¿†å’Œæ¨ç†ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æå‡æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå…¶çŸ­ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRwRæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä»æ•™å¸ˆæ¨¡å‹ä¸­æå–é“¾å¼æ€ç»´æ€»ç»“ï¼›å…¶æ¬¡ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å°†è¿™äº›æ€»ç»“ä½œä¸ºæç¤ºè¾“å…¥Mambaæ¨¡å‹ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦å¯¹Mambaçš„æ¶æ„è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šRwRçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡é“¾å¼æ€ç»´æ€»ç»“çš„è’¸é¦ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œå›å¿†ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…å¾€å¾€ä¾èµ–äºå›ºå®šçš„ä¸Šä¸‹æ–‡çª—å£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RwRä¸­ï¼Œå…³é”®çš„è®¾è®¡åŒ…æ‹¬å¦‚ä½•é€‰æ‹©å’Œç”Ÿæˆé“¾å¼æ€ç»´æ€»ç»“ï¼Œä»¥åŠåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¦‚ä½•æœ‰æ•ˆåœ°å°†è¿™äº›æ€»ç»“æ•´åˆè¿›æ¨¡å‹è¾“å…¥ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿç¡®ä¿äº†æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡å’ŒçŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šçš„å¹³è¡¡æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRwRæ–¹æ³•åœ¨LONGMEMEVALå’ŒHELMETæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†Mambaçš„é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ï¼Œè¶…è¿‡äº†åŒç±»Transformerå’Œæ··åˆåŸºçº¿ï¼Œä¸”åœ¨çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ä¿æŒäº†åŸæœ‰çš„æ€§èƒ½æ°´å¹³ï¼Œå±•ç¤ºäº†RwRçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒRwRæ–¹æ³•å¯ä»¥åœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸­åº”ç”¨ï¼Œå¦‚é•¿ç¯‡æ–‡ç« ç†è§£å’Œå¤šè½®å¯¹è¯ç”Ÿæˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mamba's theoretical infinite-context potential is limited in practice when sequences far exceed training lengths. This work explores unlocking Mamba's long-context memory ability by a simple-yet-effective method, Recall with Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a teacher model. Specifically, RwR prepends these summarization as CoT prompts during fine-tuning, teaching Mamba to actively recall and reason over long contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's long-context performance against comparable Transformer/hybrid baselines under similar pretraining conditions, while preserving short-context capabilities, all without architectural changes.

