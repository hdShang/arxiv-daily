---
layout: default
title: "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers"
---

# Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17351" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17351v1</a>
  <a href="https://arxiv.org/pdf/2512.17351.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17351v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17351v1', 'Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeyuan Allen-Zhu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

**å¤‡æ³¨**: V1.1 appeared in NeurIPS 2025 main conference; V2 adds GDN experiments, tightens some experiments (for a stronger, fairer comparison), and re-organizes sections

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCanon Layersï¼Œå¢å¼ºè¯­è¨€æ¨¡å‹æ°´å¹³ä¿¡æ¯æµåŠ¨ä¸æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `æ¶æ„è®¾è®¡` `æ°´å¹³ä¿¡æ¯æµåŠ¨` `æ¨ç†èƒ½åŠ›` `Transformer` `çº¿æ€§æ³¨æ„åŠ›` `çŠ¶æ€ç©ºé—´æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­è¨€æ¨¡å‹æ¶æ„å·®å¼‚éš¾ä»¥ç†è§£ï¼Œå°¤å…¶æ˜¯åœ¨å­¦æœ¯è§„æ¨¡é¢„è®­ç»ƒä¸­ï¼Œç»“æœå¸¸å—å™ªå£°å’Œéšæœºæ€§å½±å“ã€‚
2. è®ºæ–‡æå‡ºCanon Layersï¼Œé€šè¿‡åŠ æƒæ±‚å’Œç›¸é‚»tokenè¡¨ç¤ºï¼Œä¿ƒè¿›æ°´å¹³ä¿¡æ¯æµåŠ¨ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCanon Layersèƒ½æ˜¾è‘—æå‡æ¨ç†æ·±åº¦å’Œå¹¿åº¦ï¼Œå¹¶ä½¿å¼±æ¶æ„è¾¾åˆ°SOTAæ°´å¹³ï¼Œå·²é€šè¿‡åˆæˆä»»åŠ¡å’Œå­¦æœ¯è§„æ¨¡é¢„è®­ç»ƒéªŒè¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCanon Layersçš„è½»é‡çº§æ¶æ„ç»„ä»¶ï¼Œæ—¨åœ¨ä¿ƒè¿›ç›¸é‚»tokenä¹‹é—´çš„æ°´å¹³ä¿¡æ¯æµåŠ¨ã€‚Canon Layersé€šè¿‡è®¡ç®—é™„è¿‘tokenè¡¨ç¤ºçš„åŠ æƒå’Œï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°Transformerã€çº¿æ€§æ³¨æ„åŠ›ã€çŠ¶æ€ç©ºé—´æ¨¡å‹æˆ–ä»»ä½•åºåˆ—æ¶æ„ä¸­ã€‚ç ”ç©¶é€šè¿‡å—æ§çš„åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼Œéš”ç¦»å¹¶è¯„ä¼°äº†æ¨¡å‹çš„å…³é”®èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCanon Layersèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„æ¨ç†æ·±åº¦ï¼ˆä¾‹å¦‚æå‡2å€ï¼‰ã€æ¨ç†å¹¿åº¦å’ŒçŸ¥è¯†æ“ä½œèƒ½åŠ›ã€‚å®ƒè¿˜å¯ä»¥æå‡å¼±æ¶æ„ï¼ˆå¦‚NoPEï¼‰çš„æ€§èƒ½ï¼Œä½¿å…¶ä¸RoPEç›¸åŒ¹é…ï¼Œå¹¶ä½¿çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹èƒ½å¤Ÿä¸Mamba2/GDNç­‰å…ˆè¿›çº¿æ€§æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥åˆæˆç¯å¢ƒæä¾›äº†ä¸€ç§ç»æµæœ‰æ•ˆä¸”æœ‰åŸåˆ™çš„æ–¹æ³•æ¥éš”ç¦»åœ¨å­¦æœ¯è§„æ¨¡ä¸‹å¸¸å¸¸è¢«æ©ç›–çš„æ¨¡å‹æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶å¯èƒ½é¢„æµ‹æœªæ¥æ¶æ„åœ¨è®­ç»ƒæµç¨‹æ”¹è¿›åçš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è¯­è¨€æ¨¡å‹æ¶æ„çš„è®¾è®¡é€‰æ‹©ç¹å¤šï¼Œä½†ç¼ºä¹ä¸€ç§ç³»ç»Ÿæ€§çš„æ–¹æ³•æ¥ç†è§£ä¸åŒæ¶æ„ç»„ä»¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„å­¦æœ¯è§„æ¨¡é¢„è®­ç»ƒä¸­ï¼Œå®éªŒç»“æœå¾€å¾€å—åˆ°å™ªå£°å’Œéšæœºæ€§çš„å¹²æ‰°ï¼Œéš¾ä»¥å¾—å‡ºå¯é çš„ç»“è®ºã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§å¯æ§çš„ç¯å¢ƒæ¥éš”ç¦»å’Œè¯„ä¼°ä¸åŒæ¶æ„ç»„ä»¶çš„æ ¸å¿ƒèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§è½»é‡çº§çš„æ¶æ„ç»„ä»¶Canon Layersï¼Œé€šè¿‡ä¿ƒè¿›ç›¸é‚»tokenä¹‹é—´çš„æ°´å¹³ä¿¡æ¯æµåŠ¨æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡çµæ„Ÿæ¥æºäºéŸ³ä¹æœ¯è¯­â€œcanonâ€ï¼Œæ—¨åœ¨é€šè¿‡ç®€å•çš„åŠ æƒæ±‚å’Œæ“ä½œï¼Œå®ç°ä¿¡æ¯åœ¨åºåˆ—ä¸­çš„æœ‰æ•ˆä¼ é€’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCanon Layerså¯ä»¥æ— ç¼é›†æˆåˆ°å„ç§åºåˆ—æ¨¡å‹æ¶æ„ä¸­ï¼ŒåŒ…æ‹¬Transformerã€çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚å…¶æ ¸å¿ƒæ“ä½œæ˜¯å¯¹æ¯ä¸ªtokençš„è¡¨ç¤ºï¼Œè®¡ç®—å…¶ç›¸é‚»tokenè¡¨ç¤ºçš„åŠ æƒå’Œã€‚è¿™ä¸ªåŠ æƒå’Œçš„ç»“æœè¢«æ·»åŠ åˆ°åŸå§‹tokenè¡¨ç¤ºä¸­ï¼Œä»è€Œå®ç°ä¿¡æ¯çš„æ°´å¹³ä¼ é€’ã€‚æ•´ä¸ªè¿‡ç¨‹å¯ä»¥çœ‹ä½œæ˜¯åœ¨ç°æœ‰æ¨¡å‹æ¶æ„ä¸­æ’å…¥ä¸€ä¸ªé¢å¤–çš„å¤„ç†å±‚ï¼Œè¯¥å±‚ä¸“é—¨è´Ÿè´£ä¿ƒè¿›ä¿¡æ¯åœ¨åºåˆ—ä¸­çš„æµåŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šCanon Layersçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç®€æ´æ€§å’Œé€šç”¨æ€§ã€‚å®ƒé€šè¿‡ç®€å•çš„åŠ æƒæ±‚å’Œæ“ä½œï¼Œå®ç°äº†ä¿¡æ¯åœ¨åºåˆ—ä¸­çš„æœ‰æ•ˆä¼ é€’ï¼Œè€Œæ— éœ€å¼•å…¥å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶æˆ–çŠ¶æ€å˜é‡ã€‚è¿™ç§è®¾è®¡ä½¿å¾—Canon Layerså¯ä»¥è½»æ¾åœ°é›†æˆåˆ°å„ç§ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸­ï¼Œå¹¶æå‡å®ƒä»¬çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é€šè¿‡å—æ§çš„åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼Œæä¾›äº†ä¸€ç§ç³»ç»Ÿæ€§çš„æ–¹æ³•æ¥è¯„ä¼°ä¸åŒæ¶æ„ç»„ä»¶çš„æ ¸å¿ƒèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šCanon Layersçš„å…³é”®è®¾è®¡åœ¨äºåŠ æƒå’Œçš„æƒé‡é€‰æ‹©ã€‚è®ºæ–‡ä¸­å¯èƒ½æ¢è®¨äº†ä¸åŒçš„æƒé‡è®¾ç½®æ–¹æ¡ˆï¼Œä¾‹å¦‚å‡åŒ€æƒé‡ã€é«˜æ–¯æƒé‡æˆ–å¯å­¦ä¹ çš„æƒé‡ã€‚æ­¤å¤–ï¼ŒCanon Layersçš„é›†æˆæ–¹å¼ä¹Ÿå¯èƒ½å­˜åœ¨å¤šç§é€‰æ‹©ï¼Œä¾‹å¦‚å°†å…¶æ·»åŠ åˆ°Transformerçš„æ¯ä¸ªæ³¨æ„åŠ›å±‚ä¹‹åï¼Œæˆ–è€…å°†å…¶æ·»åŠ åˆ°æ•´ä¸ªæ¨¡å‹çš„è¾“å…¥æˆ–è¾“å‡ºå±‚ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚å¯èƒ½åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†è¿›è¡Œè¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCanon Layersèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„æ¨ç†æ·±åº¦ï¼ˆæå‡2å€ï¼‰ã€æ¨ç†å¹¿åº¦å’ŒçŸ¥è¯†æ“ä½œèƒ½åŠ›ã€‚å®ƒè¿˜å¯ä»¥æå‡å¼±æ¶æ„ï¼ˆå¦‚NoPEï¼‰çš„æ€§èƒ½ï¼Œä½¿å…¶ä¸RoPEç›¸åŒ¹é…ï¼Œå¹¶ä½¿çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹èƒ½å¤Ÿä¸Mamba2/GDNç­‰å…ˆè¿›çº¿æ€§æ¨¡å‹ç›¸åª²ç¾ã€‚è¿™äº›ç»“æœåœ¨åˆæˆä»»åŠ¡å’Œå­¦æœ¯è§„æ¨¡é¢„è®­ç»ƒä¸­éƒ½å¾—åˆ°äº†éªŒè¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Canon Layerså¯åº”ç”¨äºå„ç§éœ€è¦åºåˆ—å»ºæ¨¡çš„ä»»åŠ¡ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ã€æ—¶é—´åºåˆ—é¢„æµ‹ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒCanon Layersæœ‰æœ›æ”¹å–„æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿç­‰åº”ç”¨çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºçš„åˆæˆé¢„è®­ç»ƒæ–¹æ³•ï¼Œä¸ºæœªæ¥è¯­è¨€æ¨¡å‹æ¶æ„çš„è®¾è®¡å’Œè¯„ä¼°æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term "canon" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.
>   We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by $2\times$), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.

