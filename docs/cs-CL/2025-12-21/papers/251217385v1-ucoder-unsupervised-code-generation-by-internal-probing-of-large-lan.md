---
layout: default
title: UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models
---

# UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17385" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17385v1</a>
  <a href="https://arxiv.org/pdf/2512.17385.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17385v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17385v1', 'UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiajun Wu, Jian Yang, Wei Zhang, Lin Jing, Yuqing Ma, Ensheng Shi, Yuchi Ma, Zhoujun Li, Xianglong Liu

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UCoderï¼šé€šè¿‡å†…éƒ¨æ¢æµ‹å¤§è¯­è¨€æ¨¡å‹å®ç°æ— ç›‘ç£ä»£ç ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç ç”Ÿæˆ` `æ— ç›‘ç£å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `å†…éƒ¨æ¢æµ‹` `è‡ªæ´½æ€§` `è¡¨å¾å­¦ä¹ ` `çŸ¥è¯†æŒ–æ˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä»£ç ç”Ÿæˆæ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æˆ–æœªæ ‡æ³¨æ•°æ®ï¼Œè·å–æˆæœ¬é«˜æ˜‚ä¸”è§„æ¨¡å—é™ã€‚
2. UCoderé€šè¿‡å†…éƒ¨æ¢æµ‹LLMï¼Œæ— éœ€å¤–éƒ¨è¯­æ–™åº“ï¼ŒæŒ–æ˜æ¨¡å‹å†…éƒ¨çŸ¥è¯†å’Œç½®ä¿¡åº¦æ¨¡å¼ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ— ç›‘ç£æ–¹æ³•å¯ä¸ç›‘ç£æ–¹æ³•åª²ç¾ï¼ŒåŒæ—¶é™ä½å¯¹æ•°æ®å’Œè®¡ç®—èµ„æºçš„ä¾èµ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIPCçš„æ— ç›‘ç£ä»£ç ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å†…éƒ¨æ¢æµ‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å®ç°ä»£ç ç”Ÿæˆï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨è¯­æ–™åº“ï¼Œç”šè‡³æœªæ ‡è®°çš„ä»£ç ç‰‡æ®µã€‚IPCé€šè¿‡é—®é¢˜ç©ºé—´æ¢æµ‹ã€æµ‹è¯•ç†è§£æ¢æµ‹ã€è§£ç©ºé—´æ¢æµ‹ä»¥åŠçŸ¥è¯†å·©å›ºå’Œå¼ºåŒ–ç­‰æ‰‹æ®µï¼ŒæŒ–æ˜LLMså†…éƒ¨è•´å«çš„çŸ¥è¯†å’Œç½®ä¿¡åº¦æ¨¡å¼ã€‚æ­¤å¤–ï¼ŒIPCé€šè¿‡è‡ªæ´½æ€§æœºåˆ¶å’ŒåŸºäºè¡¨å¾çš„è´¨é‡ä¼°è®¡æ¥è¯†åˆ«å¯é çš„ä»£ç å€™é€‰ï¼Œä»è€Œè®­ç»ƒUCoderï¼ˆåŸºäºæ— ç›‘ç£å­¦ä¹ çš„ç¼–ç å™¨ï¼‰ã€‚åœ¨å¤šä¸ªä»£ç åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜æ— ç›‘ç£æ–¹æ³•å¯ä»¥è¾¾åˆ°ä¸ç›‘ç£æ–¹æ³•ç›¸åª²ç¾çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å¯¹æ ‡è®°æ•°æ®å’Œè®¡ç®—èµ„æºçš„ä¾èµ–ã€‚åˆ†æå®éªŒè¡¨æ˜ï¼Œå†…éƒ¨æ¨¡å‹çŠ¶æ€åŒ…å«å…³äºä»£ç è´¨é‡å’Œæ­£ç¡®æ€§çš„ä¸°å¯Œä¿¡å·ï¼Œå¹¶ä¸”æ­£ç¡®åˆ©ç”¨è¿™äº›ä¿¡å·èƒ½å¤Ÿä¸ºä»£ç ç”Ÿæˆä»»åŠ¡å®ç°æœ‰æ•ˆçš„æ— ç›‘ç£å­¦ä¹ ï¼Œä¸ºåœ¨èµ„æºå—é™åœºæ™¯ä¸‹è®­ç»ƒä»£ç LLMså¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®æˆ–æœªæ ‡æ³¨ä»£ç ç‰‡æ®µçš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡çš„ç›‘ç£è®­ç»ƒæˆ–æ— ç›‘ç£é¢„è®­ç»ƒï¼Œè¿™åœ¨æ•°æ®è·å–å›°éš¾æˆ–è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹æ˜¯ä¸å¯è¡Œçš„ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨LLMè‡ªèº«è•´å«çš„çŸ¥è¯†ï¼Œåœ¨æ— éœ€å¤–éƒ¨è¯­æ–™çš„æƒ…å†µä¸‹è¿›è¡Œä»£ç ç”Ÿæˆæ˜¯æœ¬ç ”ç©¶è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡â€œå†…éƒ¨æ¢æµ‹â€LLMï¼ŒæŒ–æ˜å…¶å†…éƒ¨çŠ¶æ€ä¸­è•´å«çš„å…³äºä»£ç è´¨é‡å’Œæ­£ç¡®æ€§çš„ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è®¾è®¡ä¸€ç³»åˆ—æ¢æµ‹ä»»åŠ¡ï¼Œä¾‹å¦‚é—®é¢˜ç©ºé—´æ¢æµ‹ã€æµ‹è¯•ç†è§£æ¢æµ‹å’Œè§£ç©ºé—´æ¢æµ‹ï¼Œæ¥æå–LLMå¯¹é—®é¢˜ç†è§£ã€ä»£ç é€»è¾‘å’Œæ­£ç¡®æ€§çš„ç½®ä¿¡åº¦ã€‚ç„¶åï¼Œåˆ©ç”¨è¿™äº›ç½®ä¿¡åº¦ä¿¡æ¯æ¥ç­›é€‰å’Œä¼˜åŒ–ç”Ÿæˆçš„ä»£ç ï¼Œä»è€Œå®ç°æ— ç›‘ç£çš„ä»£ç ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUCoderæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **å†…éƒ¨æ¢æµ‹ (Internal Probing)**ï¼šè®¾è®¡ä¸åŒçš„æ¢æµ‹ä»»åŠ¡ï¼Œæå–LLMå†…éƒ¨çŠ¶æ€ä¸­è•´å«çš„çŸ¥è¯†å’Œç½®ä¿¡åº¦ä¿¡æ¯ã€‚2) **ä»£ç å€™é€‰ç”Ÿæˆ (Code Candidate Generation)**ï¼šåˆ©ç”¨LLMç”Ÿæˆå¤šä¸ªä»£ç å€™é€‰ã€‚3) **ä»£ç è´¨é‡è¯„ä¼° (Code Quality Estimation)**ï¼šåŸºäºè‡ªæ´½æ€§æœºåˆ¶å’Œè¡¨å¾çš„è´¨é‡ä¼°è®¡ï¼Œå¯¹ä»£ç å€™é€‰è¿›è¡Œè¯„ä¼°å’Œç­›é€‰ã€‚4) **UCoderè®­ç»ƒ (UCoder Training)**ï¼šåˆ©ç”¨ç­›é€‰åçš„é«˜è´¨é‡ä»£ç å€™é€‰ï¼Œè®­ç»ƒUCoderæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªå®Œå…¨æ— ç›‘ç£çš„ä»£ç ç”Ÿæˆæ¡†æ¶ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨è¯­æ–™åº“ã€‚é€šè¿‡å†…éƒ¨æ¢æµ‹LLMï¼ŒæŒ–æ˜å…¶å†…éƒ¨çŠ¶æ€ä¸­è•´å«çš„çŸ¥è¯†å’Œç½®ä¿¡åº¦ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥æŒ‡å¯¼ä»£ç ç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•æ‰“ç ´äº†ä¼ ç»Ÿä»£ç ç”Ÿæˆæ–¹æ³•å¯¹å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–ï¼Œä¸ºåœ¨èµ„æºå—é™åœºæ™¯ä¸‹è®­ç»ƒä»£ç LLMæä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **é—®é¢˜ç©ºé—´æ¢æµ‹**ï¼šé€šè¿‡åˆ†æLLMå¯¹ä¸åŒé—®é¢˜çš„ç†è§£ç¨‹åº¦ï¼Œæ¥è¯„ä¼°å…¶å¯¹é—®é¢˜ç©ºé—´çš„æŒæ¡ç¨‹åº¦ã€‚2) **æµ‹è¯•ç†è§£æ¢æµ‹**ï¼šé€šè¿‡åˆ†æLLMå¯¹æµ‹è¯•ç”¨ä¾‹çš„ç†è§£ç¨‹åº¦ï¼Œæ¥è¯„ä¼°å…¶å¯¹ä»£ç é€»è¾‘çš„ç†è§£ç¨‹åº¦ã€‚3) **è§£ç©ºé—´æ¢æµ‹**ï¼šé€šè¿‡åˆ†æLLMç”Ÿæˆçš„ä¸åŒä»£ç å€™é€‰çš„ç½®ä¿¡åº¦ï¼Œæ¥è¯„ä¼°å…¶å¯¹è§£ç©ºé—´çš„æ¢ç´¢èƒ½åŠ›ã€‚4) **è‡ªæ´½æ€§æœºåˆ¶**ï¼šåˆ©ç”¨LLMå¯¹åŒä¸€é—®é¢˜çš„å¤šæ¬¡ç”Ÿæˆç»“æœè¿›è¡Œä¸€è‡´æ€§æ£€éªŒï¼Œç­›é€‰å‡ºé«˜è´¨é‡çš„ä»£ç å€™é€‰ã€‚5) **åŸºäºè¡¨å¾çš„è´¨é‡ä¼°è®¡**ï¼šåˆ©ç”¨LLMå†…éƒ¨çŠ¶æ€çš„è¡¨å¾ä¿¡æ¯ï¼Œå¯¹ä»£ç å€™é€‰çš„è´¨é‡è¿›è¡Œè¯„ä¼°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17385v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17385v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17385v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒUCoderåœ¨å¤šä¸ªä»£ç åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸ç›‘ç£æ–¹æ³•ç›¸åª²ç¾çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å¯¹æ ‡è®°æ•°æ®å’Œè®¡ç®—èµ„æºçš„ä¾èµ–ã€‚ä¾‹å¦‚ï¼Œåœ¨HumanEvalæ•°æ®é›†ä¸Šï¼ŒUCoderçš„æ€§èƒ½è¾¾åˆ°äº†XX%ï¼Œä¸ä½¿ç”¨å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è®­ç»ƒçš„ç›‘ç£æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½å·®è·ç¼©å°è‡³YY%ã€‚è¿™äº›ç»“æœéªŒè¯äº†å†…éƒ¨æ¢æµ‹LLMè¿›è¡Œæ— ç›‘ç£ä»£ç ç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UCoderçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ï¼šåœ¨æ•°æ®ç¨€ç¼ºæˆ–è®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒä¸‹è¿›è¡Œä»£ç ç”Ÿæˆï¼Œä¾‹å¦‚åµŒå…¥å¼ç³»ç»Ÿã€ç§»åŠ¨è®¾å¤‡ç­‰ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºé™ä½äº†ä»£ç ç”Ÿæˆçš„æˆæœ¬å’Œé—¨æ§›ï¼Œä½¿å¾—æ›´å¤šå¼€å‘è€…èƒ½å¤Ÿåˆ©ç”¨LLMè¿›è¡Œä»£ç å¼€å‘ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ï¼Œå®ç°çœŸæ­£çš„æ— ç›‘ç£å­¦ä¹ ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.

