---
layout: default
title: Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory
---

# Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.15055" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.15055v2</a>
  <a href="https://arxiv.org/pdf/2505.15055.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.15055v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.15055v2', 'Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongli Zhou, Hui Huang, Ziqing Zhao, Lvyuan Han, Huicheng Wang, Kehai Chen, Muyun Yang, Wei Bao, Jian Dong, Bing Xu, Conghui Zhu, Hailong Cao, Tiejun Zhao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21 (æ›´æ–°: 2025-08-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPSN-IRTæ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹åŸºå‡†è¯„ä¼°çš„æœ‰æ•ˆæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `é¡¹ç›®ååº”ç†è®º` `ä¼ªåŒèƒèƒç½‘ç»œ` `æ¨¡å‹è¯„ä¼°` `æµ‹é‡è´¨é‡` `äººç±»åå¥½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMåŸºå‡†æµ‹è¯•å­˜åœ¨ä¸åŒæ’è¡Œæ¦œä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œä¸”é¡¶å°–æ¨¡å‹ä¹‹é—´çš„å¯åˆ†æ€§è¾ƒå·®ï¼Œéš¾ä»¥å‡†ç¡®åæ˜ æ¨¡å‹çš„çœŸå®èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºäº†ä¼ªåŒèƒèƒç½‘ç»œï¼ˆPSN-IRTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºçš„é¡¹ç›®ååº”ç†è®ºæ¡†æ¶ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¼°è®¡é¡¹ç›®ç‰¹å¾å’Œæ¨¡å‹èƒ½åŠ›ã€‚
3. é€šè¿‡å¯¹41,871ä¸ªé¡¹ç›®çš„11ä¸ªLLMåŸºå‡†è¿›è¡Œåˆ†æï¼Œå‘ç°äº†æ˜¾è‘—çš„æµ‹é‡è´¨é‡ä¸è¶³ï¼Œå¹¶è¯æ˜äº†PSN-IRTåœ¨æ„å»ºå°å‹åŸºå‡†æ—¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯„ä¼°é€šå¸¸ä¾èµ–äºåŸºå‡†æµ‹è¯•ï¼Œä½†ä¸åŒæ’è¡Œæ¦œä¹‹é—´çš„ä¸ä¸€è‡´æ€§ä»¥åŠé¡¶å°–æ¨¡å‹ä¹‹é—´çš„å¯åˆ†æ€§å·®å¼•å‘äº†å¯¹å…¶çœŸå®èƒ½åŠ›åæ˜ çš„æ‹…å¿§ã€‚æœ¬æ–‡å¯¹åŸºå‡†æµ‹è¯•çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†æ‰¹åˆ¤æ€§åˆ†æï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºçš„é¡¹ç›®ååº”ç†è®ºæ¡†æ¶â€”â€”ä¼ªåŒèƒèƒç½‘ç»œï¼ˆPSN-IRTï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨IRTåŸºç¡€æ¶æ„ä¸­æ•´åˆäº†ä¸°å¯Œçš„é¡¹ç›®å‚æ•°ã€‚é€šè¿‡PSN-IRTï¼Œæˆ‘ä»¬å¯¹11ä¸ªåŒ…å«41,871ä¸ªé¡¹ç›®çš„LLMåŸºå‡†è¿›è¡Œäº†å¹¿æ³›åˆ†æï¼Œæ­ç¤ºäº†å…¶æµ‹é‡è´¨é‡çš„æ˜¾è‘—ä¸è¶³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åˆ©ç”¨PSN-IRTèƒ½å¤Ÿæ„å»ºæ›´å°çš„åŸºå‡†ï¼ŒåŒæ—¶æ›´å¥½åœ°ä¸äººç±»åå¥½å¯¹é½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨çš„è¯„ä¼°ä¸ä¸€è‡´æ€§å’Œæµ‹é‡è´¨é‡ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåŒºåˆ†é¡¶å°–æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å¯é æ€§å—åˆ°è´¨ç–‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„PSN-IRTæ¡†æ¶é€šè¿‡å¼•å…¥ä¸°å¯Œçš„é¡¹ç›®å‚æ•°ï¼Œå¢å¼ºäº†ä¼ ç»Ÿé¡¹ç›®ååº”ç†è®ºçš„èƒ½åŠ›ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹èƒ½åŠ›å’Œé¡¹ç›®ç‰¹å¾çš„æ›´å‡†ç¡®ä¼°è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPSN-IRTæ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯æ•°æ®æ”¶é›†ä¸é¢„å¤„ç†ï¼Œç„¶åæ˜¯åŸºäºIRTçš„æ¨¡å‹æ„å»ºï¼Œæœ€åæ˜¯æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†å¤šç§ç±»å‹çš„åŸºå‡†æ•°æ®ï¼Œæä¾›çµæ´»çš„åˆ†æå·¥å…·ã€‚

**å…³é”®åˆ›æ–°**ï¼šPSN-IRTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å¯¹é¡¹ç›®å‚æ•°çš„ä¸°å¯Œå»ºæ¨¡èƒ½åŠ›ï¼Œä¸ä¼ ç»ŸIRTæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ¨¡å‹ä¸é¡¹ç›®ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨PSN-IRTä¸­ï¼Œè®¾è®¡äº†å¤šç§é¡¹ç›®å‚æ•°è®¾ç½®ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æ„å»ºäº†é€‚åº”æ€§å¼ºçš„ç½‘ç»œç»“æ„ï¼Œä»¥æé«˜è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ©ç”¨PSN-IRTæ¡†æ¶æ„å»ºçš„å°å‹åŸºå‡†åœ¨ä¸äººç±»åå¥½çš„å¯¹é½åº¦ä¸Šæ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºä¼ ç»ŸåŸºå‡†ï¼Œæµ‹é‡è´¨é‡æé«˜äº†çº¦30%ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼ŒPSN-IRTèƒ½å¤Ÿæœ‰æ•ˆè§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•çš„ä¸è¶³ä¹‹å¤„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ•™è‚²æµ‹è¯„å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡åŸºå‡†æµ‹è¯•çš„æœ‰æ•ˆæ€§ï¼ŒPSN-IRTèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å‡†ç¡®åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ï¼Œè¿›è€Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The evaluation of large language models (LLMs) via benchmarks is widespread, yet inconsistencies between different leaderboards and poor separability among top models raise concerns about their ability to accurately reflect authentic model capabilities. This paper provides a critical analysis of benchmark effectiveness, examining mainstream prominent LLM benchmarks using results from diverse models. We first propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced Item Response Theory framework that incorporates a rich set of item parameters within an IRT-grounded architecture. PSN-IRT can be utilized for accurate and reliable estimations of item characteristics and model abilities. Based on PSN-IRT, we conduct extensive analysis on 11 LLM benchmarks comprising 41,871 items, revealing significant and varied shortcomings in their measurement quality. Furthermore, we demonstrate that leveraging PSN-IRT is able to construct smaller benchmarks while maintaining stronger alignment with human preference.

