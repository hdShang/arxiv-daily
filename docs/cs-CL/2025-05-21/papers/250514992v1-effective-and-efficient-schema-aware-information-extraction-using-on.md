---
layout: default
title: Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models
---

# Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14992" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.14992v1</a>
  <a href="https://arxiv.org/pdf/2505.14992.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14992v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14992v1', 'Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhihao Wen, Sheng Liang, Yaxiong Wu, Yongyue Zhang, Yong Liu

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-21

**Â§áÊ≥®**: 5 pages, 2 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DLISC‰ª•Ëß£ÂÜ≥ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÁöÑ‰ø°ÊÅØÊèêÂèñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰ø°ÊÅØÊèêÂèñ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËµÑÊ∫êÂèóÈôêËÆæÂ§á` `Ê®°ÂºèËØÜÂà´` `Â¢ûÈáèÁºìÂ≠ò` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ÊïàÁéáÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äËøõË°å‰ø°ÊÅØÊèêÂèñÊó∂Èù¢‰∏¥ÂπªËßâ„ÄÅ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÈôêÂà∂ÂíåÈ´òÂª∂ËøüÁ≠âÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑDLISCÊñπÊ≥ïÈÄöËøáÂèåLoRAÊ®°ÂùóÂíåÂ¢ûÈáèÊ®°ÂºèÁºìÂ≠òÔºåÊèêÂçá‰∫ÜÊ®°ÂºèËØÜÂà´Âíå‰ø°ÊÅØÊèêÂèñÁöÑÊïàÁéá‰∏éÊïàÊûú„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDLISCÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÈ´ò‰∫Ü‰ø°ÊÅØÊèêÂèñÁöÑÊúâÊïàÊÄßÂíåÊïàÁéáÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰ø°ÊÅØÊèêÂèñÔºàIEÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂÆÉÂ∞ÜÈùûÁªìÊûÑÂåñÊñáÊú¨ËΩ¨Âåñ‰∏∫ÁªìÊûÑÂåñÁü•ËØÜ„ÄÇÂú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÈÉ®ÁΩ≤ËÆ°ÁÆóÂØÜÈõÜÂûãÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøõË°å‰ø°ÊÅØÊèêÂèñÈù¢‰∏¥ËØ∏Â§öÊåëÊàòÔºåÂ¶ÇÂπªËßâ„ÄÅÊúâÈôêÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂíåÈ´òÂª∂ËøüÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂ§öÊ†∑ÂåñÊèêÂèñÊ®°ÂºèÊó∂„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄÇÁî®‰∫éËÆæÂ§áÁ´ØLLMsÁöÑ‰∏§Èò∂ÊÆµ‰ø°ÊÅØÊèêÂèñÊñπÊ≥ïÔºåÁß∞‰∏∫ÂèåLoRA‰∏éÂ¢ûÈáèÊ®°ÂºèÁºìÂ≠òÔºàDLISCÔºâÔºåÂú®ÊúâÊïàÊÄßÂíåÊïàÁéá‰∏äÂ¢ûÂº∫‰∫ÜÊ®°ÂºèËØÜÂà´ÂíåÊ®°ÂºèÊÑüÁü•ÊèêÂèñ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåDLISCÈááÁî®ËØÜÂà´LoRAÊ®°ÂùóÊù•Ê£ÄÁ¥¢‰∏éÁªôÂÆöÊü•ËØ¢ÊúÄÁõ∏ÂÖ≥ÁöÑÊ®°ÂºèÔºåÂπ∂‰ΩøÁî®ÊèêÂèñLoRAÊ®°ÂùóÂü∫‰∫éÂÖàÂâçÈÄâÊã©ÁöÑÊ®°ÂºèËøõË°å‰ø°ÊÅØÊèêÂèñ„ÄÇ‰∏∫Âä†ÈÄüÊèêÂèñÊé®ÁêÜÔºåÂ¢ûÈáèÊ®°ÂºèÁºìÂ≠òË¢´Á∫≥ÂÖ•‰ª•ÂáèÂ∞ëÂÜó‰ΩôËÆ°ÁÆóÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™‰ø°ÊÅØÊèêÂèñÊï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊúâÊïàÊÄßÂíåÊïàÁéáÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äËøõË°å‰ø°ÊÅØÊèêÂèñÊó∂ÁöÑÊïàÁéáÂíåÊúâÊïàÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§öÊ†∑ÂåñÊèêÂèñÊ®°ÂºèÊó∂ÔºåÂ∏∏Â∏∏Èù¢‰∏¥ÂπªËßâ„ÄÅ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÈôêÂà∂ÂíåÈ´òÂª∂ËøüÁ≠âÁóõÁÇπ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDLISCÊñπÊ≥ïÈÄöËøáÂºïÂÖ•ÂèåLoRAÊ®°ÂùóÂíåÂ¢ûÈáèÊ®°ÂºèÁºìÂ≠òÔºåÊó®Âú®ÊèêÈ´ò‰ø°ÊÅØÊèêÂèñÁöÑÊïàÁéáÂíåÊïàÊûú„ÄÇËØÜÂà´LoRAÊ®°ÂùóË¥üË¥£Ê£ÄÁ¥¢Áõ∏ÂÖ≥Ê®°ÂºèÔºåËÄåÊèêÂèñLoRAÊ®°ÂùóÂàôÂü∫‰∫éËøô‰∫õÊ®°ÂºèËøõË°å‰ø°ÊÅØÊèêÂèñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDLISCÁöÑÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÊòØÊ®°ÂºèËØÜÂà´Ôºå‰ΩøÁî®ËØÜÂà´LoRAÊ®°ÂùóËé∑Âèñ‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊ®°ÂºèÔºõÁ¨¨‰∫åÈò∂ÊÆµÊòØ‰ø°ÊÅØÊèêÂèñÔºåÂà©Áî®ÊèêÂèñLoRAÊ®°ÂùóÊ†πÊçÆÈÄâÂÆöÁöÑÊ®°ÂºèËøõË°åÊèêÂèñ„ÄÇÂêåÊó∂ÔºåÂ¢ûÈáèÊ®°ÂºèÁºìÂ≠òÁî®‰∫éÂáèÂ∞ëÂÜó‰ΩôËÆ°ÁÆóÔºåÊèêÂçáÊé®ÁêÜÈÄüÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDLISCÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜÂèåLoRAÊ®°ÂùóÂíåÂ¢ûÈáèÊ®°ÂºèÁºìÂ≠òÔºåÊòæËëóÊèêÈ´ò‰∫Ü‰ø°ÊÅØÊèêÂèñÁöÑÊïàÁéáÂíåÊúâÊïàÊÄß„ÄÇËøô‰∏ÄËÆæËÆ°‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îËµÑÊ∫êÂèóÈôêÁéØÂ¢É„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåDLISC‰ºòÂåñ‰∫ÜLoRAÊ®°ÂùóÁöÑË∂ÖÂèÇÊï∞ÔºåÂπ∂ËÆæËÆ°‰∫ÜÈÄÇÂ∫îÊÄßÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÂ≠¶‰π†ËÉΩÂäõÂíåÊèêÂèñÁ≤æÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDLISCÂú®Â§ö‰∏™‰ø°ÊÅØÊèêÂèñÊï∞ÊçÆÈõÜ‰∏äÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÔºå‰ø°ÊÅØÊèêÂèñÁöÑÊúâÊïàÊÄßÊèêÈ´ò‰∫Ü15%‰ª•‰∏äÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÂçá‰∫Ü30%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéËØ•ÊñπÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØË°åÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÁßªÂä®ËÆæÂ§á‰∏äÁöÑÊô∫ËÉΩÂä©Êâã„ÄÅËæπÁºòËÆ°ÁÆóÁéØÂ¢É‰∏≠ÁöÑÊï∞ÊçÆÂ§ÑÁêÜ‰ª•ÂèäÂÆûÊó∂‰ø°ÊÅØÊèêÂèñÁ≥ªÁªü„ÄÇÈÄöËøáÂú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÂÆûÁé∞È´òÊïàÁöÑ‰ø°ÊÅØÊèêÂèñÔºåDLISCËÉΩÂ§ü‰∏∫ÂêÑÁßçÂ∫îÁî®Âú∫ÊôØÊèê‰æõÊõ¥Âø´„ÄÅÊõ¥ÂáÜÁ°ÆÁöÑÊúçÂä°ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Information extraction (IE) plays a crucial role in natural language processing (NLP) by converting unstructured text into structured knowledge. Deploying computationally intensive large language models (LLMs) on resource-constrained devices for information extraction is challenging, particularly due to issues like hallucinations, limited context length, and high latency-especially when handling diverse extraction schemas. To address these challenges, we propose a two-stage information extraction approach adapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching (DLISC), which enhances both schema identification and schema-aware extraction in terms of effectiveness and efficiency. In particular, DLISC adopts an Identification LoRA module for retrieving the most relevant schemas to a given query, and an Extraction LoRA module for performing information extraction based on the previously selected schemas. To accelerate extraction inference, Incremental Schema Caching is incorporated to reduce redundant computation, substantially improving efficiency. Extensive experiments across multiple information extraction datasets demonstrate notable improvements in both effectiveness and efficiency.

