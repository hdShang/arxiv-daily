---
layout: default
title: "Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective"
---

# Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.15045" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.15045v1</a>
  <a href="https://arxiv.org/pdf/2505.15045.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.15045v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.15045v1', 'Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siyue Zhang, Yilun Zhao, Liyuan Geng, Arman Cohan, Anh Tuan Luu, Chen Zhao

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ‰©æ•£è¯­è¨€æ¨¡å‹ä»¥è§£å†³è‡ªå›å½’æ¨¡å‹åœ¨æ–‡æœ¬åµŒå…¥ä¸­çš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£è¯­è¨€æ¨¡å‹` `æ–‡æœ¬åµŒå…¥` `è‡ªå›å½’æ¨¡å‹` `åŒå‘æ³¨æ„åŠ›` `é•¿æ–‡æ¡£æ£€ç´¢` `æ¨ç†ä»»åŠ¡` `ä¿¡æ¯æ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªå›å½’æ¨¡å‹åœ¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸­ç”±äºå•å‘æ³¨æ„åŠ›çš„ä½¿ç”¨ï¼Œå¯¼è‡´ä¸åŒå‘ç‰¹æ€§ä¸åŒ¹é…ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºæ‰©æ•£è¯­è¨€æ¨¡å‹ä½œä¸ºæ–‡æœ¬åµŒå…¥çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨å…¶åŒå‘æ¶æ„æ¥æ›´å¥½åœ°æ•æ‰æ–‡æœ¬çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰©æ•£è¯­è¨€åµŒå…¥æ¨¡å‹åœ¨é•¿æ–‡æ¡£æ£€ç´¢ä¸Šæå‡20%ï¼Œåœ¨æ¨ç†å¯†é›†å‹æ£€ç´¢ä¸Šæå‡8%ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åµŒå…¥æ¨¡å‹ï¼Œå‡­å€Ÿå¤§è§„æ¨¡çš„é¢„è®­ç»ƒå’Œåè®­ç»ƒï¼Œå·²åœ¨æ–‡æ¡£æ£€ç´¢ç­‰é€šç”¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸­è¶…è¶Šäº†åŸºäºBERTå’ŒT5çš„æ¨¡å‹ã€‚ç„¶è€Œï¼ŒLLMåµŒå…¥çš„ä¸€ä¸ªæ ¹æœ¬é™åˆ¶åœ¨äºè‡ªå›å½’é¢„è®­ç»ƒä¸­ä½¿ç”¨çš„å•å‘æ³¨æ„åŠ›ï¼Œè¿™ä¸æ–‡æœ¬åµŒå…¥ä»»åŠ¡çš„åŒå‘ç‰¹æ€§ä¸åŒ¹é…ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºé‡‡ç”¨æ‰©æ•£è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥ï¼ŒåŸºäºå…¶å›ºæœ‰çš„åŒå‘æ¶æ„åŠåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æˆåŠŸè¡¨ç°ã€‚æˆ‘ä»¬é¦–æ¬¡ç³»ç»Ÿç ”ç©¶äº†æ‰©æ•£è¯­è¨€åµŒå…¥æ¨¡å‹ï¼Œåœ¨é•¿æ–‡æ¡£æ£€ç´¢ä¸Šè¶…è¶ŠLLMåµŒå…¥æ¨¡å‹20%ï¼Œåœ¨æ¨ç†å¯†é›†å‹æ£€ç´¢ä¸Šæå‡8%ï¼Œåœ¨éµå¾ªæŒ‡ä»¤çš„æ£€ç´¢ä¸Šæå‡2%ï¼Œå¹¶åœ¨ä¼ ç»Ÿæ–‡æœ¬åµŒå…¥åŸºå‡†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„åˆ†æéªŒè¯äº†åŒå‘æ³¨æ„åŠ›åœ¨ç¼–ç é•¿æ–‡æœ¬å’Œå¤æ‚æ–‡æœ¬çš„å…¨å±€ä¸Šä¸‹æ–‡ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸­ç”±äºå•å‘æ³¨æ„åŠ›å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ–‡æœ¬å’Œå¤æ‚æ–‡æœ¬çš„å¤„ç†ä¸Šå­˜åœ¨çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶åŒå‘æ³¨æ„åŠ›æœºåˆ¶æ¥æ›´æœ‰æ•ˆåœ°æ•æ‰æ–‡æœ¬çš„å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä»è€Œæå‡æ–‡æœ¬åµŒå…¥çš„è´¨é‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸ä¼˜åŒ–ã€ä»¥åŠåŸºäºè¯¥æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥ç”Ÿæˆã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ–‡æœ¬è¾“å…¥å¤„ç†ã€åŒå‘æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°å’ŒåµŒå…¥å‘é‡çš„ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé¦–æ¬¡å°†æ‰©æ•£è¯­è¨€æ¨¡å‹åº”ç”¨äºæ–‡æœ¬åµŒå…¥ä»»åŠ¡ï¼Œåˆ©ç”¨å…¶åŒå‘ç‰¹æ€§æ˜¾è‘—æå‡äº†é•¿æ–‡æ¡£å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡çš„æ£€ç´¢æ€§èƒ½ï¼Œä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åŒå‘æ³¨æ„åŠ›çš„æ•ˆæœï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”é•¿æ–‡æœ¬çš„å¤„ç†éœ€æ±‚ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å…¨å±€ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£è¯­è¨€åµŒå…¥æ¨¡å‹åœ¨é•¿æ–‡æ¡£æ£€ç´¢ä¸Šè¶…è¶ŠLLMåµŒå…¥æ¨¡å‹20%ï¼Œåœ¨æ¨ç†å¯†é›†å‹æ£€ç´¢ä¸Šæå‡8%ï¼Œåœ¨éµå¾ªæŒ‡ä»¤çš„æ£€ç´¢ä¸Šæå‡2%ã€‚è¿™äº›ç»“æœæ˜¾ç¤ºå‡ºæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ–‡æœ¬æ—¶çš„è¡¨ç°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ–‡æ¡£æ£€ç´¢ã€ä¿¡æ¯æ£€ç´¢å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ï¼Œèƒ½å¤Ÿä¸ºé•¿æ–‡æœ¬å¤„ç†å’Œå¤æ‚æ¨ç†ä»»åŠ¡æä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æœªæ¥ï¼Œæ‰©æ•£è¯­è¨€æ¨¡å‹å¯èƒ½åœ¨æ›´å¤šçš„æ–‡æœ¬ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¤§çš„ä»·å€¼ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirectional attention used during autoregressive pre-training, which misaligns with the bidirectional nature of text embedding tasks. To this end, We propose adopting diffusion language models for text embeddings, motivated by their inherent bidirectional architecture and recent success in matching or surpassing LLMs especially on reasoning tasks. We present the first systematic study of the diffusion language embedding model, which outperforms the LLM-based embedding model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval, 2% on instruction-following retrieval, and achieve competitive performance on traditional text embedding benchmarks. Our analysis verifies that bidirectional attention is crucial for encoding global context in long and complex text.

