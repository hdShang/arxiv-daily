---
layout: default
title: "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning"
---

# MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01110" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01110v1</a>
  <a href="https://arxiv.org/pdf/2505.01110.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01110v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01110v1', 'MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Murtadha Ahmed, Wenbo, Liu yunfeng

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/amurtadha/MateICL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMateICLä»¥è§£å†³å¤§è§„æ¨¡ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›åˆ†æ•£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæ³¨æ„åŠ›` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡ä¸Šä¸‹æ–‡æ—¶é¢ä¸´æ³¨æ„åŠ›åˆ†æ•£çš„é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚
2. MateICLé€šè¿‡å°†ä¸Šä¸‹æ–‡åˆ†å‰²ä¸ºå¤šä¸ªçª—å£å¹¶é‡æ–°æ ¡å‡†æ³¨æ„åŠ›æƒé‡ï¼Œè§£å†³äº†æ³¨æ„åŠ›åˆ†æ•£çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMateICLåœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºäºæ£€ç´¢çš„åŸºçº¿ï¼Œä¸”æ— éœ€å¤–éƒ¨è®­ç»ƒçš„æ£€ç´¢æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„å›ºå®šä½ç½®é•¿åº¦é™åˆ¶äº†ç¤ºä¾‹æ•°é‡ï¼Œå¯¼è‡´æ³¨æ„åŠ›åˆ†æ•£ã€‚æœ¬æ–‡æå‡ºMateICLï¼Œé€šè¿‡å°†ä¸Šä¸‹æ–‡åˆ†å‰²ä¸ºå¤šä¸ªçª—å£å¹¶å¼•å…¥é¢å¤–å±‚æ¥é‡æ–°æ ¡å‡†æ³¨æ„åŠ›æƒé‡ï¼Œä¼˜å…ˆè€ƒè™‘æŸ¥è¯¢æ ‡è®°ï¼Œä»è€Œåœ¨ä¸Šä¸‹æ–‡å¢å¤§æ—¶ä¿æŒæœ‰æ•ˆçš„è‡ªæ³¨æ„åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒMateICLèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡æ¥æå‡ICLæ€§èƒ½ï¼Œä¸”åœ¨è®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒä¸­ä»ç„¶è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤§è§„æ¨¡ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ç”±äºå›ºå®šä½ç½®é•¿åº¦å¯¼è‡´çš„æ³¨æ„åŠ›åˆ†æ•£é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç¤ºä¾‹æ•°é‡å¢åŠ æ—¶ï¼Œæ¨¡å‹çš„è‡ªæ³¨æ„åŠ›æ•ˆæœæ˜¾è‘—ä¸‹é™ï¼Œå½±å“äº†å­¦ä¹ æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMateICLçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¸Šä¸‹æ–‡åˆ†å‰²ä¸ºå¤šä¸ªçª—å£ï¼Œæ¯ä¸ªçª—å£å¡«å……è‡³æ¨¡å‹çš„ä¸Šä¸‹æ–‡å®¹é‡ï¼Œå¹¶å•ç‹¬å¤„ç†è¿™äº›çª—å£ã€‚é€šè¿‡å¼•å…¥é¢å¤–å±‚æ¥é‡æ–°æ ¡å‡†æ³¨æ„åŠ›æƒé‡ï¼Œä¼˜å…ˆè€ƒè™‘æŸ¥è¯¢æ ‡è®°ï¼Œä»è€Œåœ¨ä¸Šä¸‹æ–‡å¢å¤§æ—¶ä¿æŒæœ‰æ•ˆçš„è‡ªæ³¨æ„åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMateICLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸Šä¸‹æ–‡åˆ†å‰²æ¨¡å—ã€çª—å£å¤„ç†æ¨¡å—å’Œæ³¨æ„åŠ›æƒé‡æ ¡å‡†æ¨¡å—ã€‚é¦–å…ˆï¼Œå°†è¾“å…¥ä¸Šä¸‹æ–‡åˆ†å‰²ä¸ºå¤šä¸ªçª—å£ï¼Œç„¶ååˆ†åˆ«å¤„ç†è¿™äº›çª—å£ï¼Œæœ€åé€šè¿‡é¢å¤–å±‚è°ƒæ•´æ³¨æ„åŠ›æƒé‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMateICLçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡çª—å£åŒ–å¤„ç†å’Œæ³¨æ„åŠ›æƒé‡çš„é‡æ–°æ ¡å‡†ï¼Œæœ‰æ•ˆç¼“è§£äº†æ³¨æ„åŠ›åˆ†æ•£çš„é—®é¢˜ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œåè€…é€šå¸¸æ— æ³•æœ‰æ•ˆå¤„ç†å¤§è§„æ¨¡ä¸Šä¸‹æ–‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒMateICLä¼˜åŒ–äº†çª—å£å¤§å°å’Œæ³¨æ„åŠ›æƒé‡çš„æ ¡å‡†ç­–ç•¥ï¼Œç¡®ä¿åœ¨ä¸Šä¸‹æ–‡å¢å¤§æ—¶ä»èƒ½ä¿æŒæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MateICLåœ¨å¤šä¸ªå®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºäºæ£€ç´¢çš„åŸºçº¿ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œä¸”åœ¨è®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒä¸­ä»èƒ½ä¿æŒè‰¯å¥½çš„æ•ˆæœã€‚å…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼ŒMateICLåœ¨å¤„ç†å¤§è§„æ¨¡ä¸Šä¸‹æ–‡æ—¶çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†å……åˆ†éªŒè¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MateICLçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡æå‡å¤§è§„æ¨¡ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ•ˆç‡ï¼ŒMateICLèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¼ºçš„æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated remarkable capabilities in In-Context Learning (ICL). However, the fixed position length constraints in pre-trained models limit the number of demonstration examples. Recent efforts to extend context suffer from attention dispersion as the number of demonstrations increases. In this paper, we introduce Mitigating Attention Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective self-attention as the context size grows. We first split the context into multiple windows, each filled to the model's context capacity, which are processed separately. Then, we introduce an additional layer to recalibrate the attention weights, prioritizing the query tokens as the number of demonstrations increases. Our empirical results show that MateICL can effectively leverage larger contexts to improve ICL performance. Compared to retrieval-based baselines, MateICL consistently achieves better performance without requiring an externally trained retrieval model. Despite recent advances in inference strategies (e.g., 32k token contexts), our results demonstrate that MateICL remains beneficial in computationally resource-constrained settings. The code is publicly available at https://github.com/amurtadha/MateICL.

