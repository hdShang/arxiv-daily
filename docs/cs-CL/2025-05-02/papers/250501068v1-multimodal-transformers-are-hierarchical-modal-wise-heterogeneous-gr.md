---
layout: default
title: Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs
---

# Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01068" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01068v1</a>
  <a href="https://arxiv.org/pdf/2505.01068.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01068v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01068v1', 'Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yijie Jin, Junjie Peng, Xuanchao Lin, Haochen Yuan, Lan Wang, Cangzhi Zheng

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

**æœŸåˆŠ**: https://aclanthology.org/2025.acl-long.109/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›¾ç»“æ„å¤šæ¨¡æ€å˜æ¢å™¨ä»¥æå‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ` `å¤šæ¨¡æ€å˜æ¢å™¨` `å›¾ç»“æ„è¡¨ç¤º` `æ•ˆç‡ä¼˜åŒ–` `å‚æ•°å…±äº«` `äº¤é”™æ©ç ` `å±‚æ¬¡åŒ–æ¨¡æ€å¼‚æ„å›¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å˜æ¢å™¨åœ¨å¤šæ¨¡æ€èåˆè¿‡ç¨‹ä¸­æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹å’Œæ€§èƒ½ç“¶é¢ˆã€‚
2. æœ¬æ–‡æå‡ºå°†å¤šæ¨¡æ€å˜æ¢å™¨è§†ä¸ºå±‚æ¬¡åŒ–æ¨¡æ€å¼‚æ„å›¾ï¼Œå¹¶è®¾è®¡äº†äº¤é”™æ©ç æœºåˆ¶ä»¥ä¼˜åŒ–å‚æ•°å…±äº«ï¼Œæå‡æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGsiTåœ¨å¤šä¸ªä¸»æµæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸMulTsï¼Œä¸”å‚æ•°é‡å‡å°‘è‡³ä¸‰åˆ†ä¹‹ä¸€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆMSAï¼‰æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œæ—¨åœ¨æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ä»¥è¯†åˆ«æƒ…æ„Ÿã€‚ç°æœ‰çš„å¤šæ¨¡æ€å˜æ¢å™¨ï¼ˆMulTsï¼‰åœ¨æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡ä»æ•ˆç‡ä¼˜åŒ–çš„è§’åº¦å‡ºå‘ï¼Œæå‡ºå¹¶è¯æ˜MulTså¯ä»¥è§†ä¸ºå±‚æ¬¡åŒ–çš„æ¨¡æ€å¼‚æ„å›¾ï¼ˆHMHGsï¼‰ï¼Œå¹¶å¼•å…¥äº†å›¾ç»“æ„è¡¨ç¤ºæ¨¡å¼ã€‚åŸºäºæ­¤æ¨¡å¼ï¼Œæå‡ºäº†äº¤é”™æ©ç ï¼ˆIMï¼‰æœºåˆ¶ï¼Œè®¾è®¡äº†å›¾ç»“æ„å’Œäº¤é”™æ©ç å¤šæ¨¡æ€å˜æ¢å™¨ï¼ˆGsiTï¼‰ï¼Œå…¶åœ¨å‚æ•°ä½¿ç”¨ä¸Šä»…ä¸ºçº¯MulTsçš„ä¸‰åˆ†ä¹‹ä¸€ï¼ŒåŒæ—¶é¿å…äº†ä¿¡æ¯æ··ä¹±ï¼Œå®ç°äº†å…¨æ¨¡æ€èåˆã€‚æ­¤å¤–ï¼ŒGsiTåœ¨å¤šä¸ªä¸»æµMSAæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡å’Œå‚æ•°å‡å°‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­å¤šæ¨¡æ€å˜æ¢å™¨ï¼ˆMulTsï¼‰åœ¨æ•ˆç‡ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶å­˜åœ¨è®¡ç®—èµ„æºæµªè´¹å’Œæ€§èƒ½ç“¶é¢ˆçš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºå°†MulTsè§†ä¸ºå±‚æ¬¡åŒ–æ¨¡æ€å¼‚æ„å›¾ï¼ˆHMHGsï¼‰ï¼Œå¹¶é€šè¿‡å¼•å…¥äº¤é”™æ©ç ï¼ˆIMï¼‰æœºåˆ¶ï¼Œè®¾è®¡å‡ºå›¾ç»“æ„å’Œäº¤é”™æ©ç å¤šæ¨¡æ€å˜æ¢å™¨ï¼ˆGsiTï¼‰ï¼Œä»¥å®ç°é«˜æ•ˆçš„å‚æ•°å…±äº«å’Œä¿¡æ¯èåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGsiTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å›¾ç»“æ„è¡¨ç¤ºã€äº¤é”™æ©ç æœºåˆ¶å’Œé«˜æ•ˆçš„æƒé‡å…±äº«æ¨¡å—ã€‚è¯¥æ¡†æ¶é€šè¿‡å›¾ç»“æ„åŒ–çš„æ–¹å¼å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œç¡®ä¿ä¿¡æ¯çš„æœ‰åºèåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†MulTsé‡æ–°å®šä¹‰ä¸ºå±‚æ¬¡åŒ–æ¨¡æ€å¼‚æ„å›¾ï¼Œå¹¶é€šè¿‡IMæœºåˆ¶å®ç°äº†å…¨æ¨¡æ€èåˆçš„é«˜æ•ˆæ€§ï¼Œæ˜¾è‘—é™ä½äº†å‚æ•°é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒGsiTçš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº†é€‚åº”æ€§ç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¤šæ¨¡æ€ä¿¡æ¯èåˆæ—¶çš„ç¨³å®šæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGsiTåœ¨å¤šä¸ªä¸»æµå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å¤šæ¨¡æ€å˜æ¢å™¨ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°æ˜¾è‘—æ°´å¹³ï¼ŒåŒæ—¶å‚æ•°é‡å‡å°‘è‡³ä»…ä¸ºä¸‰åˆ†ä¹‹ä¸€ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æã€ç”¨æˆ·åé¦ˆå¤„ç†å’Œå¸‚åœºè¶‹åŠ¿é¢„æµ‹ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒGsiTèƒ½å¤Ÿä¸ºä¼ä¸šå’Œç ”ç©¶æœºæ„æä¾›æ›´ä¸ºç²¾å‡†çš„æƒ…æ„Ÿæ´å¯Ÿï¼Œæ¨åŠ¨æ™ºèƒ½åˆ†æå·¥å…·çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets.

