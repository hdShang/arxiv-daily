---
layout: default
title: Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System
---

# Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01315" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01315v2</a>
  <a href="https://arxiv.org/pdf/2505.01315.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01315v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01315v2', 'Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sheikh Samit Muhaimin, Spyridon Mastorakis

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02 (æ›´æ–°: 2025-05-05)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¢å¼ºè¿‡æ»¤ä¸æ‘˜è¦ç³»ç»Ÿä»¥ä¿æŠ¤å¤§å‹è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯¹æŠ—æ€§æ”»å‡»` `è‡ªç„¶è¯­è¨€å¤„ç†` `æç¤ºè¿‡æ»¤` `æ‘˜è¦æ¨¡å—` `æ¶æ„è¾“å…¥è¯†åˆ«` `å®‰å…¨é˜²æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é˜²å¾¡æªæ–½é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ä¸”ä¸æ˜“éƒ¨ç½²ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡æç¤ºè¿‡æ»¤å’Œæ‘˜è¦æ¨¡å—ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»è¯†åˆ«å’Œé˜²å¾¡æ¶æ„è¾“å…¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«æœ‰å®³è¾“å…¥æ–¹é¢çš„æˆåŠŸç‡é«˜è¾¾98.71%ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æŠ—æ”»å‡»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬é¢ä¸´ç€å¤æ‚çš„å¯¹æŠ—æ€§æ”»å‡»ã€æ“æ§æ€§æç¤ºå’Œæ¶æ„è¾“å…¥çš„å¨èƒã€‚ç°æœ‰çš„å¯¹ç­–é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œæˆæœ¬é«˜ä¸”ä¸æ˜“éƒ¨ç½²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„é˜²å¾¡èŒƒå¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»è¯†åˆ«ã€è¿‡æ»¤å’Œé˜²å¾¡å¯¹æŠ—æ€§æˆ–æ¶æ„è¾“å…¥ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šä¸€æ˜¯ä½¿ç”¨å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æç¤ºè¿‡æ»¤æ¨¡å—ï¼Œèƒ½å¤Ÿæ£€æµ‹ã€è§£ç å’Œåˆ†ç±»æœ‰å®³è¾“å…¥ï¼›äºŒæ˜¯æ‘˜è¦æ¨¡å—å¤„ç†å’Œæ€»ç»“å¯¹æŠ—æ€§ç ”ç©¶æ–‡çŒ®ï¼Œä¸ºæ¨¡å‹æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é˜²å¾¡çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«æœ‰å®³æ¨¡å¼å’Œæ“æ§æ€§è¯­è¨€ç»“æ„æ–¹é¢çš„æˆåŠŸç‡è¾¾åˆ°98.71%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»å’Œæ¶æ„è¾“å…¥æ—¶çš„è„†å¼±æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºé‡æ–°è®­ç»ƒï¼Œå¯¼è‡´é«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’Œä¸ä¾¿çš„éƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒçš„é˜²å¾¡æ¡†æ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»è¯†åˆ«å’Œè¿‡æ»¤æ¶æ„è¾“å…¥ã€‚é€šè¿‡ç»“åˆæç¤ºè¿‡æ»¤å’Œæ‘˜è¦æ¨¡å—ï¼Œå¢å¼ºæ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæç¤ºè¿‡æ»¤æ¨¡å—å’Œæ‘˜è¦æ¨¡å—ã€‚æç¤ºè¿‡æ»¤æ¨¡å—åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è¿›è¡Œæœ‰å®³è¾“å…¥çš„æ£€æµ‹ä¸åˆ†ç±»ï¼Œæ‘˜è¦æ¨¡å—åˆ™å¤„ç†å¯¹æŠ—æ€§ç ”ç©¶æ–‡çŒ®ï¼Œæä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ— éœ€é‡æ–°è®­ç»ƒçš„é˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡æ–‡æœ¬æå–ã€æ‘˜è¦å’Œæœ‰å®³æç¤ºåˆ†æçš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æŠ—æ”»å‡»èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæç¤ºè¿‡æ»¤æ¨¡å—é‡‡ç”¨é›¶-shotåˆ†ç±»ã€å…³é”®è¯åˆ†æå’Œç¼–ç å†…å®¹æ£€æµ‹ç­‰æŠ€æœ¯ï¼Œç¡®ä¿å¯¹å¤šç§æ¶æ„è¾“å…¥çš„æœ‰æ•ˆè¯†åˆ«ä¸å¤„ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„é›†æˆæ–¹æ³•åœ¨è¯†åˆ«æœ‰å®³æ¨¡å¼ã€æ“æ§æ€§è¯­è¨€ç»“æ„å’Œç¼–ç æç¤ºæ–¹é¢çš„æˆåŠŸç‡é«˜è¾¾98.71%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹å“åº”è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†å¯¹æ¶æ„è¾“å…¥çš„æ‹’ç»ç‡å’ŒæŠ—æ”»å‡»èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºå¿«é€Ÿæ›¿ä»£æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å®‰å…¨é˜²æŠ¤å’Œäººå·¥æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆä¿æŠ¤ç”¨æˆ·å…å—æ¶æ„è¾“å…¥çš„å½±å“ï¼Œæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å¤šç§AIç³»ç»Ÿä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses.

