---
layout: default
title: "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation"
---

# Always Tell Me The Odds: Fine-grained Conditional Probability Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01595" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01595v1</a>
  <a href="https://arxiv.org/pdf/2505.01595.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01595v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01595v1', 'Always Tell Me The Odds: Fine-grained Conditional Probability Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liaoyaqi Wang, Zhengping Jiang, Anqi Liu, Benjamin Van Durme

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç²¾ç»†åŒ–æ¡ä»¶æ¦‚ç‡ä¼°è®¡æ¨¡å‹ä»¥è§£å†³ä¸ç¡®å®šæ€§é¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¡ä»¶æ¦‚ç‡ä¼°è®¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸ç¡®å®šæ€§é¢„æµ‹` `æ•°æ®ç”Ÿæˆ` `æ¨¡å‹è®­ç»ƒ` `ç›‘ç£å­¦ä¹ ` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§å’Œéƒ¨åˆ†ä¿¡æ¯ä¸‹çš„æ¦‚ç‡é¢„æµ‹èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´ä¼°è®¡ç»“æœç²—ç³™ä¸”åå‘å¸¸è§æ•°å­—ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆäººç±»ä¸åˆæˆæ•°æ®ã€æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œæ”¹è¿›ç›‘ç£çš„æ¦‚ç‡ä¼°è®¡æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜é¢„æµ‹çš„ç²¾ç¡®æ€§å’Œå¯é æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ¡ä»¶æ¦‚ç‡ä¼°è®¡ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¾®è°ƒå’Œæç¤ºæ–¹æ³•ï¼Œæå‡å¹…åº¦æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æ¨¡å‹ï¼Œç”¨äºåœ¨ä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹è¿›è¡Œç²¾ç»†åŒ–æ¦‚ç‡ä¼°è®¡ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å®Œæ•´ä¿¡æ¯çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸ç¡®å®šæ€§æˆ–éƒ¨åˆ†ä¿¡æ¯ä¸‹çš„æ¦‚ç‡é¢„æµ‹ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„LLMsæ¦‚ç‡ä¼°è®¡å¾€å¾€ç²—ç³™ä¸”åå‘äºæ›´é¢‘ç¹çš„æ•°å­—ã€‚é€šè¿‡äººç±»å’Œåˆæˆæ•°æ®çš„åˆ›å»ºä¸è¯„ä¼°ã€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ä»¥åŠæ›´å¥½çš„ç›‘ç£ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç³»åˆ—å¼ºå¤§ä¸”ç²¾ç¡®çš„æ¦‚ç‡ä¼°è®¡æ¨¡å‹ï¼Œå¹¶åœ¨ä¾èµ–æ¡ä»¶æ¦‚ç‡ä¼°è®¡çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¾®è°ƒå’Œæç¤ºæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§å’Œéƒ¨åˆ†ä¿¡æ¯æ¡ä»¶ä¸‹è¿›è¡Œæ¦‚ç‡é¢„æµ‹æ—¶çš„ç²—ç³™å’Œåå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›æƒ…å†µæ—¶ç¼ºä¹å¯é æ€§å’Œç²¾ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»“åˆäººç±»å’Œåˆæˆæ•°æ®çš„åˆ›å»ºä¸è¯„ä¼°ï¼Œæ‰©å¤§æ¨¡å‹è§„æ¨¡ï¼Œå¹¶å¼•å…¥æ›´å¥½çš„ç›‘ç£æœºåˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¦‚ç‡ä¼°è®¡æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§ä¸‹çš„é¢„æµ‹èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ä¸å¤„ç†ã€æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ¨¡å—è´Ÿè´£ç”Ÿæˆå’Œæ ‡æ³¨äººç±»ä¸åˆæˆæ•°æ®ï¼Œæ¨¡å‹æ¨¡å—åˆ™åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¯„ä¼°æ¨¡å—ç”¨äºç³»ç»Ÿæ€§åœ°æµ‹è¯•æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç»¼åˆå¤šç§æ•°æ®æºå’Œæ”¹è¿›çš„ç›‘ç£ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æ¡ä»¶æ¦‚ç‡ä¼°è®¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„å¾®è°ƒå’Œæç¤ºæ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…å¾€å¾€ä¾èµ–äºè¾ƒå°‘çš„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¦‚ç‡ä¼°è®¡çš„ç²¾åº¦ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„æ¥å¢å¼ºæ¨¡å‹å¯¹ä¸ç¡®å®šæ€§çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨æ¡ä»¶æ¦‚ç‡ä¼°è®¡ä»»åŠ¡ä¸Šç›¸è¾ƒäºç°æœ‰å¾®è°ƒå’Œæç¤ºæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°æ˜¾è‘—æ°´å¹³ï¼Œå…·ä½“æ•°å€¼æœªæä¾›ï¼Œä½†æ•´ä½“è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å†³ç­–æ”¯æŒç³»ç»Ÿå’Œé£é™©è¯„ä¼°ç­‰ã€‚é€šè¿‡æä¾›æ›´ç²¾ç¡®çš„æ¦‚ç‡ä¼°è®¡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·åœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a state-of-the-art model for fine-grained probability estimation of propositions conditioned on context. Recent advances in large language models (LLMs) have significantly enhanced their reasoning capabilities, particularly on well-defined tasks with complete information. However, LLMs continue to struggle with making accurate and well-calibrated probabilistic predictions under uncertainty or partial information. While incorporating uncertainty into model predictions often boosts performance, obtaining reliable estimates of that uncertainty remains understudied. In particular, LLM probability estimates tend to be coarse and biased towards more frequent numbers. Through a combination of human and synthetic data creation and assessment, scaling to larger models, and better supervision, we propose a set of strong and precise probability estimation models. We conduct systematic evaluations across tasks that rely on conditional probability estimation and show that our approach consistently outperforms existing fine-tuned and prompting-based methods by a large margin.

