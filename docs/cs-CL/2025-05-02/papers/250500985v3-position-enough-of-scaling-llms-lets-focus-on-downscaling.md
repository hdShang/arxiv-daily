---
layout: default
title: Position: Enough of Scaling LLMs! Lets Focus on Downscaling
---

# Position: Enough of Scaling LLMs! Lets Focus on Downscaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00985" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00985v3</a>
  <a href="https://arxiv.org/pdf/2505.00985.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00985v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00985v3', 'Position: Enough of Scaling LLMs! Lets Focus on Downscaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yash Goel, Ayan Sengupta, Tanmoy Chakraborty

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02 (æ›´æ–°: 2025-05-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸‹ç¼©å°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ¡†æ¶ä»¥åº”å¯¹èµ„æºæ¶ˆè€—é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸‹ç¼©å°` `èµ„æºæ•ˆç‡` `æ¨¡å‹å‹ç¼©` `çŸ¥è¯†è’¸é¦` `å¯æŒç»­å‘å±•` `è®­ç»ƒç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©å±•æ³•åˆ™åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€å‘ä¸­å­˜åœ¨è®¡ç®—æ•ˆç‡ä½ã€ç¯å¢ƒå½±å“å¤§å’Œéƒ¨ç½²é™åˆ¶ç­‰é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„ä¸‹ç¼©å°æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å‡å°‘èµ„æºéœ€æ±‚çš„åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚
3. é€šè¿‡å®æ–½è¯¥æ¡†æ¶ï¼Œç ”ç©¶å±•ç¤ºäº†åœ¨èµ„æºæ¶ˆè€—å’Œæ€§èƒ½ä¹‹é—´çš„æœ‰æ•ˆå¹³è¡¡ï¼Œæä¾›äº†å¯è¡Œçš„è½¬å‹ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æŒ‘æˆ˜äº†å¯¹ç¥ç»ç½‘ç»œæ‰©å±•æ³•åˆ™çš„ä¸»å¯¼å…³æ³¨ï¼Œå€¡å¯¼åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¼€å‘ä¸­è½¬å‘ä¸‹ç¼©å°çš„èŒƒå¼ã€‚å°½ç®¡æ‰©å±•æ³•åˆ™åœ¨é€šè¿‡å¢åŠ æ¨¡å‹å’Œæ•°æ®é›†è§„æ¨¡æ¥æé«˜æ€§èƒ½æ–¹é¢æä¾›äº†é‡è¦è§è§£ï¼Œä½†æˆ‘ä»¬å¼ºè°ƒäº†è¿™ç§æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ã€ç¯å¢ƒå½±å“å’Œéƒ¨ç½²é™åˆ¶æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„ä¸‹ç¼©å°æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å¤§å¹…å‡å°‘èµ„æºéœ€æ±‚çš„åŒæ—¶ä¿æŒæ€§èƒ½ã€‚æœ¬æ–‡æ¦‚è¿°äº†ä»ä¼ ç»Ÿæ‰©å±•èŒƒå¼è½¬å˜çš„å®é™…ç­–ç•¥ï¼Œå€¡å¯¼ä¸€ç§æ›´å¯æŒç»­ã€é«˜æ•ˆå’Œå¯åŠçš„LLMå¼€å‘æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰©å±•è¿‡ç¨‹ä¸­æ‰€å¸¦æ¥çš„é«˜è®¡ç®—æˆæœ¬å’Œç¯å¢ƒå½±å“ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºä¸æ–­å¢åŠ æ¨¡å‹å’Œæ•°æ®é›†çš„è§„æ¨¡ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—è¿‡å¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡ºä¸€ç§ä¸‹ç¼©å°çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ¨¡å‹ç»“æ„å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œå‡å°‘èµ„æºéœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ‰“ç ´ä¼ ç»Ÿæ‰©å±•çš„å±€é™ï¼Œæ¨åŠ¨æ›´å¯æŒç»­çš„æ¨¡å‹å¼€å‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹å‹ç¼©ã€çŸ¥è¯†è’¸é¦å’Œé«˜æ•ˆè®­ç»ƒç­–ç•¥ç­‰å¤šä¸ªæ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½é’ˆå¯¹ç‰¹å®šçš„èµ„æºæ¶ˆè€—é—®é¢˜è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®ç°æ•´ä½“æ€§èƒ½çš„æå‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„ä¸‹ç¼©å°ç­–ç•¥ï¼Œå¼ºè°ƒåœ¨æ¨¡å‹æ€§èƒ½ä¸èµ„æºæ¶ˆè€—ä¹‹é—´çš„å¹³è¡¡ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸å†å•çº¯è¿½æ±‚è§„æ¨¡çš„æ‰©å¤§ï¼Œè€Œæ˜¯å…³æ³¨å¦‚ä½•åœ¨è¾ƒå°è§„æ¨¡ä¸‹å®ç°é«˜æ•ˆæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œè®ºæ–‡è¯¦ç»†è®¨è®ºäº†æ¨¡å‹çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ä»¥åŠç½‘ç»œç»“æ„çš„ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨ä¸‹ç¼©å°è¿‡ç¨‹ä¸­ä¸æŸå¤±æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ä¸‹ç¼©å°æ¡†æ¶çš„æ¨¡å‹åœ¨èµ„æºæ¶ˆè€—ä¸Šå‡å°‘äº†çº¦40%ï¼Œè€Œæ€§èƒ½ä¿æŒåœ¨ä¸ä¼ ç»Ÿæ‰©å±•æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚è¿™ä¸€æˆæœè¡¨æ˜ï¼Œèµ„æºæ•ˆç‡ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´å¯ä»¥å®ç°æœ‰æ•ˆçš„å¹³è¡¡ï¼Œæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿé™ä½å¼€å‘å’Œéƒ¨ç½²æˆæœ¬ï¼Œä½¿å¾—æ›´å¤šçš„ç»„ç»‡èƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ¨åŠ¨æŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¿™ç§ä¸‹ç¼©å°çš„æ–¹æ³•å¯èƒ½ä¼šåœ¨AIæ¨¡å‹çš„å¯æŒç»­å‘å±•ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.

