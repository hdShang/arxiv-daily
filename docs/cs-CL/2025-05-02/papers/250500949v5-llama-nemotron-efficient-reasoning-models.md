---
layout: default
title: "Llama-Nemotron: Efficient Reasoning Models"
---

# Llama-Nemotron: Efficient Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00949" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00949v5</a>
  <a href="https://arxiv.org/pdf/2505.00949.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00949v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00949v5', 'Llama-Nemotron: Efficient Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Prasoon Varshney, Makesh Narsimhan, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi Mahabadi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Shaona Ghosh, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Chris Alexiuk, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02 (æ›´æ–°: 2025-09-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLlama-Nemotronç³»åˆ—æ¨¡å‹ä»¥æå‡æ¨ç†æ•ˆç‡ä¸èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨ç†æ¨¡å‹` `åŠ¨æ€æ¨ç†åˆ‡æ¢` `çŸ¥è¯†è’¸é¦` `å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ` `å¼€æºæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†æ¨¡å‹åœ¨æ¨ç†æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³ä¼ä¸šçº§åº”ç”¨éœ€æ±‚ã€‚
2. Llama-Nemotronç³»åˆ—æ¨¡å‹é€šè¿‡å¼‚æ„è®¾è®¡å’ŒåŠ¨æ€æ¨ç†åˆ‡æ¢ï¼Œæå‡äº†æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ï¼Œæ”¯æŒä¼ä¸šä½¿ç”¨ã€‚
3. ä¸æœ€å…ˆè¿›çš„DeepSeek-R1æ¨¡å‹ç›¸æ¯”ï¼ŒLlama-Nemotronåœ¨æ¨ç†ååé‡å’Œå†…å­˜æ•ˆç‡ä¸Šè¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†Llama-Nemotronç³»åˆ—æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼‚æ„æ¨ç†æ¨¡å‹å®¶æ—ï¼Œå…·å¤‡å“è¶Šçš„æ¨ç†èƒ½åŠ›å’Œé«˜æ•ˆçš„æ¨ç†æ€§èƒ½ï¼Œå¹¶å…è®¸ä¼ä¸šä½¿ç”¨ã€‚è¯¥ç³»åˆ—æ¨¡å‹åˆ†ä¸ºä¸‰ç§è§„æ¨¡â€”â€”Nanoï¼ˆ8Bï¼‰ã€Superï¼ˆ49Bï¼‰å’ŒUltraï¼ˆ253Bï¼‰ï¼Œåœ¨æ¨ç†ååé‡å’Œå†…å­˜æ•ˆç‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨ç†æ¨¡å‹DeepSeek-R1ã€‚æœ¬æ–‡è®¨è®ºäº†è¿™äº›æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨Llama 3æ¨¡å‹çš„ç¥ç»æ¶æ„æœç´¢ã€çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒï¼Œéšåè¿›è¡Œä»¥æ¨ç†ä¸ºé‡ç‚¹çš„åè®­ç»ƒé˜¶æ®µï¼ŒåŒ…å«ç›‘ç£å¾®è°ƒå’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ã€‚Llama-Nemotronæ¨¡å‹æ˜¯é¦–ä¸ªæ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢çš„å¼€æºæ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­åœ¨æ ‡å‡†èŠå¤©å’Œæ¨ç†æ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œæ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬å‘å¸ƒäº†Llama-Nemotronæ¨ç†æ¨¡å‹åŠå…¶å®Œæ•´çš„åè®­ç»ƒæ•°æ®é›†å’Œè®­ç»ƒä»£ç åº“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†æ¨¡å‹åœ¨æ¨ç†æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¼ä¸šåº”ç”¨åœºæ™¯ä¸­çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•åœ¨ä¿è¯æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæä¾›è¶³å¤Ÿçš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLlama-Nemotronç³»åˆ—æ¨¡å‹é€šè¿‡å¼‚æ„è®¾è®¡å’ŒåŠ¨æ€æ¨ç†åˆ‡æ¢ï¼Œå…è®¸ç”¨æˆ·åœ¨æ ‡å‡†èŠå¤©å’Œæ¨ç†æ¨¡å¼ä¹‹é—´çµæ´»åˆ‡æ¢ï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ã€‚è¯¥è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­è‡ªé€‚åº”è°ƒæ•´ï¼Œæ»¡è¶³å¤šæ ·åŒ–çš„åº”ç”¨éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯ä½¿ç”¨Llama 3æ¨¡å‹è¿›è¡Œç¥ç»æ¶æ„æœç´¢ä»¥åŠ é€Ÿæ¨ç†ï¼›å…¶æ¬¡æ˜¯çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒï¼›æœ€åæ˜¯æ¨ç†èšç„¦çš„åè®­ç»ƒé˜¶æ®µï¼ŒåŒ…å«ç›‘ç£å¾®è°ƒå’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šLlama-Nemotronæ¨¡å‹çš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶åŠ¨æ€æ¨ç†åˆ‡æ¢åŠŸèƒ½ï¼Œä½¿å…¶æˆä¸ºé¦–ä¸ªæ”¯æŒæ­¤åŠŸèƒ½çš„å¼€æºæ¨ç†æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŠ€æœ¯ç»†èŠ‚ï¼ŒåŒ…æ‹¬ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¨ç†æ—¶çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸DeepSeek-R1çš„å¯¹æ¯”å®éªŒä¸­ï¼ŒLlama-Nemotronç³»åˆ—æ¨¡å‹åœ¨æ¨ç†ååé‡å’Œå†…å­˜æ•ˆç‡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯Ultraç‰ˆæœ¬åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶å±•ç°äº†æ›´é«˜çš„æ€§èƒ½ï¼Œå…·ä½“æ•°æ®æœªæŠ«éœ²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Llama-Nemotronç³»åˆ—æ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦é«˜æ•ˆæ¨ç†çš„ä¼ä¸šçº§åº”ç”¨åœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¢æœã€æ•°æ®åˆ†æå’Œå†³ç­–æ”¯æŒç³»ç»Ÿç­‰ã€‚å…¶å¼€æ”¾çš„è®¸å¯åè®®ä¹Ÿä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†ä¾¿åˆ©ï¼Œä¿ƒè¿›äº†æ¨¡å‹çš„è¿›ä¸€æ­¥å¼€å‘å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.

