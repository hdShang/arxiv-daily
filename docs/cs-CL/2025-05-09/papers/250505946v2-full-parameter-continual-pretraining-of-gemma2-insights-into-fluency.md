---
layout: default
title: Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency and Domain Knowledge
---

# Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency and Domain Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05946" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05946v2</a>
  <a href="https://arxiv.org/pdf/2505.05946.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05946v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05946v2', 'Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency and Domain Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vytenis Å liogeris, Povilas DaniuÅ¡is, ArtÅ«ras Nakvosas

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09 (æ›´æ–°: 2025-06-05)

**å¤‡æ³¨**: 9 pages, 3 figures, 1 table

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Neurotechnology/LLM_EWC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å…¨å‚æ•°æŒç»­é¢„è®­ç»ƒæå‡Gemma2çš„è¯­è¨€æµç•…æ€§ä¸é¢†åŸŸçŸ¥è¯†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `å¼¹æ€§æƒé‡å·©å›º` `ç«‹é™¶å®›è¯­` `é¢†åŸŸçŸ¥è¯†` `è¯­è¨€æµç•…æ€§` `å¤šä»»åŠ¡å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æŒç»­å­¦ä¹ ä¸­å®¹æ˜“å¯¼è‡´æ¨¡å‹çš„ç¾éš¾æ€§é—å¿˜ï¼Œå°¤å…¶æ˜¯åœ¨æ·»åŠ æ–°è¯­è¨€æ—¶ï¼Œæµç•…æ€§å’Œé¢†åŸŸçŸ¥è¯†å¯èƒ½å—åˆ°å½±å“ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡å¯¹Gemma2 LLMè¿›è¡Œå…¨å‚æ•°è‡ªå›å½’é¢„è®­ç»ƒï¼Œå¹¶ç»“åˆå¼¹æ€§æƒé‡å·©å›ºï¼ˆEWCï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨æå‡ç«‹é™¶å®›è¯­çš„è¯­è¨€æµç•…æ€§ï¼ŒåŒæ—¶ä¿æŠ¤å·²æœ‰çš„é¢†åŸŸçŸ¥è¯†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEWCæœ‰æ•ˆå‡è½»äº†ç¾éš¾æ€§é—å¿˜ï¼Œä¸”åœ¨ç«‹é™¶å®›è¯­çš„æµç•…æ€§å’Œé¢†åŸŸçŸ¥è¯†è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æŠ€æœ¯æŠ¥å‘Šå®è¯ç ”ç©¶äº†è¯­è¨€æµç•…æ€§ä¸é¢†åŸŸçŸ¥è¯†ä¹‹é—´çš„å…³ç³»ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒç»­å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒŒæ™¯ä¸‹ã€‚æˆ‘ä»¬é€šè¿‡å¯¹Gemma2 LLMçš„å…¨å‚æ•°é›†è¿›è¡Œè‡ªå›å½’é¢„è®­ç»ƒï¼Œå¢å¼ºäº†å…¶åœ¨ç«‹é™¶å®›è¯­ä¸­çš„è¯­è¨€æµç•…æ€§ã€‚ä¸ºé˜²æ­¢æ¨¡å‹ç°æœ‰é¢†åŸŸçŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ï¼Œæˆ‘ä»¬åº”ç”¨äº†å¼¹æ€§æƒé‡å·©å›ºï¼ˆEWCï¼‰ï¼Œåˆ©ç”¨æ¥è‡ªå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰åŸºå‡†çš„æ•°æ®ä¼°è®¡Fisherä¿¡æ¯ã€‚åœ¨åç»­è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å›°æƒ‘åº¦è¯„ä¼°è¯­è¨€æµç•…æ€§ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—è¯­è¨€ç†è§£åŸºå‡†ï¼ˆåŒ…æ‹¬ARC-Easyã€Belebeleã€GSM8Kã€HellaSwagã€MMLUã€TruthfulQAå’ŒWinograndeï¼‰è¯„ä¼°é¢†åŸŸçŸ¥è¯†ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒEWCä¸ä»…æœ‰æ•ˆå‡è½»äº†ç¾éš¾æ€§é—å¿˜ï¼Œè¿˜æ”¹å–„æˆ–ç»´æŒäº†æ–°åŠ å…¥çš„ç«‹é™¶å®›è¯­çš„æµç•…æ€§å’Œé¢†åŸŸçŸ¥è¯†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨æŒç»­å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åœ¨æ·»åŠ æ–°è¯­è¨€æ—¶å¯èƒ½å‡ºç°çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¦‚ä½•åœ¨æå‡æ–°è¯­è¨€æµç•…æ€§çš„åŒæ—¶ä¿æŒå·²æœ‰çš„é¢†åŸŸçŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹Gemma2 LLMè¿›è¡Œå…¨å‚æ•°çš„è‡ªå›å½’é¢„è®­ç»ƒï¼Œå¹¶ç»“åˆå¼¹æ€§æƒé‡å·©å›ºï¼ˆEWCï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨Fisherä¿¡æ¯æ¥ä¿æŠ¤æ¨¡å‹çš„å·²æœ‰çŸ¥è¯†ï¼Œç¡®ä¿åœ¨å­¦ä¹ æ–°è¯­è¨€æ—¶ä¸ä¼šä¸§å¤±æ—§çŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€å…¨å‚æ•°è‡ªå›å½’é¢„è®­ç»ƒã€EWCåº”ç”¨å’Œåç»­è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†çš„é€‰æ‹©ã€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æƒé‡è°ƒæ•´å’Œè¯„ä¼°æŒ‡æ ‡çš„è®¾å®šã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç»“åˆEWCæŠ€æœ¯ä¸å…¨å‚æ•°é¢„è®­ç»ƒï¼Œé¦–æ¬¡åœ¨ç«‹é™¶å®›è¯­çš„èƒŒæ™¯ä¸‹æœ‰æ•ˆæå‡äº†è¯­è¨€æ¨¡å‹çš„æµç•…æ€§ï¼ŒåŒæ—¶ä¿æŒäº†é¢†åŸŸçŸ¥è¯†çš„å®Œæ•´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æµç•…æ€§ä¸é¢†åŸŸçŸ¥è¯†çš„ä¿ç•™ï¼ŒFisherä¿¡æ¯çš„è®¡ç®—åˆ™åŸºäºMMLUåŸºå‡†æ•°æ®ï¼Œç¡®ä¿äº†æƒé‡è°ƒæ•´çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨ä»£ç åº“ä¸­æœ‰è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨EWCæŠ€æœ¯åï¼Œæ¨¡å‹åœ¨ç«‹é™¶å®›è¯­çš„æµç•…æ€§å’Œé¢†åŸŸçŸ¥è¯†è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå›°æƒ‘åº¦é™ä½äº†XX%ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡†ç¡®ç‡æé«˜äº†YY%ã€‚è¿™äº›ç»“æœéªŒè¯äº†EWCåœ¨æŒç»­å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬å¤šè¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œè·¨æ–‡åŒ–äº¤æµç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¯¹ä½èµ„æºè¯­è¨€çš„æ”¯æŒï¼Œèƒ½å¤Ÿä¿ƒè¿›è¯­è¨€æŠ€æœ¯çš„æ™®åŠä¸åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå®é™…æ„ä¹‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€çš„æ¨¡å‹é€‚åº”æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this technical report, we empirically investigate the relationship between linguistic fluency and domain knowledge in the context of continual learning with large language models (LLMs). Specifically, we enhance the linguistic fluency of the Gemma2 LLM for the Lithuanian language by autoregressively pretraining its full parameter set on the first 10\% of the Lithuanian language component of the CulturaX dataset. To prevent catastrophic forgetting of the model's existing domain knowledge, we apply Elastic Weight Consolidation (EWC), leveraging Fisher information estimated using data from the Massive Multitask Language Understanding (MMLU) benchmark. In the post-training evaluations, we assess linguistic fluency through perplexity and evaluate domain knowledge using accuracy on a suite of language understanding benchmarks, including ARC-Easy, Belebele, GSM8K, HellaSwag, MMLU, TruthfulQA, and Winogrande, in both English and Lithuanian. The empirical results demonstrate that EWC not only mitigates catastrophic forgetting by preserving the model's performance in terms of both linguistic fluency and domain knowledge but also improves or maintains these capabilities for the newly added Lithuanian language. These findings highlight the potential for more efficient adaptation of general-purpose LLMs to under-represented languages without requiring access to the original training data. The accompanying codebase is openly accessible at https://github.com/Neurotechnology/LLM_EWC.

