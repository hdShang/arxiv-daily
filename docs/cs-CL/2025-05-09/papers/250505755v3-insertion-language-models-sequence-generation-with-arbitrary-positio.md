---
layout: default
title: "Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions"
---

# Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05755" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05755v3</a>
  <a href="https://arxiv.org/pdf/2505.05755.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05755v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05755v3', 'Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dhruvesh Patel, Aishwarya Sahoo, Avinash Amballa, Tahira Naseem, Tim G. J. Rudner, Andrew McCallum

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09 (æ›´æ–°: 2025-09-03)

**å¤‡æ³¨**: Additional related work. Code available at: https://dhruveshp.com/projects/ilm

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ’å…¥è¯­è¨€æ¨¡å‹ä»¥è§£å†³åºåˆ—ç”Ÿæˆä¸­çš„å¤æ‚çº¦æŸé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `æ’å…¥è¯­è¨€æ¨¡å‹` `åºåˆ—ç”Ÿæˆ` `è‡ªå›å½’æ¨¡å‹` `æ©è”½æ‰©æ•£æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ–‡æœ¬ç”Ÿæˆ` `ä¾èµ–å…³ç³»å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªå›å½’æ¨¡å‹åœ¨å¤„ç†å¤æ‚çº¦æŸå’Œéé¡ºåºä¾èµ–çš„åºåˆ—ç”Ÿæˆä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚
2. æœ¬æ–‡æå‡ºæ’å…¥è¯­è¨€æ¨¡å‹ï¼ˆILMsï¼‰ï¼Œé€šè¿‡é€ä¸ªæ’å…¥ä»¤ç‰Œçš„æ–¹å¼ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„ä½ç½®è¿›è¡Œæ’å…¥ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ä»¤ç‰Œä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
3. å®éªŒè¯æ˜ï¼ŒILMsåœ¨è§„åˆ’ä»»åŠ¡ä¸Šè¶…è¶Šäº†è‡ªå›å½’æ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨æ— æ¡ä»¶æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†æ›´é«˜çš„çµæ´»æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰åœ¨åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤„ç†å¤æ‚çº¦æŸæˆ–éé¡ºåºä¾èµ–çš„åºåˆ—æ—¶è¡¨ç°ä¸ä½³ã€‚è™½ç„¶æ©è”½æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰éƒ¨åˆ†è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½†åŒæ—¶è§£ç å¤šä¸ªä»¤ç‰Œå¯èƒ½å¯¼è‡´ä¸è¿è´¯ï¼Œå¹¶ä¸”æ— æ³•å¤„ç†æœªçŸ¥æ•°é‡çš„æ’å…¥çº¦æŸã€‚æœ¬æ–‡æå‡ºæ’å…¥è¯­è¨€æ¨¡å‹ï¼ˆILMsï¼‰ï¼Œèƒ½å¤Ÿåœ¨åºåˆ—ä¸­ä»»æ„ä½ç½®æ’å…¥ä»¤ç‰Œï¼Œé€šè¿‡é€ä¸ªæ’å…¥çš„æ–¹å¼ï¼ŒILMsèƒ½å¤Ÿæœ‰æ•ˆè¡¨ç¤ºä»¤ç‰Œä¹‹é—´çš„å¼ºä¾èµ–å…³ç³»ï¼Œå¹¶å‡†ç¡®å»ºæ¨¡ä¸éµå¾ªå·¦åˆ°å³é¡ºåºçš„åºåˆ—ã€‚å®éªŒè¯æ˜ï¼ŒILMsåœ¨å¸¸è§è§„åˆ’ä»»åŠ¡ä¸Šä¼˜äºARMså’ŒMDMsï¼Œå¹¶åœ¨æ— æ¡ä»¶æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ä¸ARMsè¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶åœ¨ä»»æ„é•¿åº¦æ–‡æœ¬å¡«å……ä¸­æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªå›å½’æ¨¡å‹åœ¨åºåˆ—ç”Ÿæˆä¸­æ— æ³•æœ‰æ•ˆå¤„ç†å¤æ‚çº¦æŸå’Œéé¡ºåºä¾èµ–çš„é—®é¢˜ã€‚ç°æœ‰çš„æ©è”½æ‰©æ•£æ¨¡å‹åœ¨åŒæ—¶è§£ç å¤šä¸ªä»¤ç‰Œæ—¶å¯èƒ½å¯¼è‡´ä¸è¿è´¯ï¼Œä¸”æ— æ³•åº”å¯¹æœªçŸ¥æ•°é‡çš„æ’å…¥çº¦æŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ’å…¥è¯­è¨€æ¨¡å‹ï¼ˆILMsï¼‰é€šè¿‡é€ä¸ªæ’å…¥ä»¤ç‰Œçš„æ–¹å¼ï¼Œé€‰æ‹©æ’å…¥ä½ç½®å’Œè¯æ±‡å…ƒç´ ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä»¤ç‰Œä¹‹é—´çš„å¼ºä¾èµ–å…³ç³»ï¼Œå¹¶æ”¯æŒä»»æ„é¡ºåºçš„ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šILMsçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç‰¹å®šçš„ç½‘ç»œå‚æ•°åŒ–è®¾è®¡ï¼Œé‡‡ç”¨ç®€å•çš„å»å™ªç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚æ¨¡å‹é€šè¿‡é€æ­¥æ’å…¥ä»¤ç‰Œï¼Œæ„å»ºå‡ºå®Œæ•´çš„åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šILMsçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨ä»»æ„ä½ç½®æ’å…¥ä»¤ç‰Œï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†å¤æ‚åºåˆ—æ—¶çš„å±€é™æ€§ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿçµæ´»åº”å¯¹å¤šæ ·åŒ–çš„ç”Ÿæˆä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šILMsçš„è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ’å…¥è¿‡ç¨‹ä¸­çš„ä¾èµ–å…³ç³»ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œæ¶æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ’å…¥è¯­è¨€æ¨¡å‹ï¼ˆILMsï¼‰åœ¨å¸¸è§è§„åˆ’ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†è‡ªå›å½’æ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªä»»åŠ¡ä¸­æé«˜äº†ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œåœ¨æ— æ¡ä»¶æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒILMsä¸è‡ªå›å½’æ¨¡å‹è¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶åœ¨ä»»æ„é•¿åº¦æ–‡æœ¬å¡«å……ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¤§çš„çµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

æ’å…¥è¯­è¨€æ¨¡å‹ï¼ˆILMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿå’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶çµæ´»çš„æ’å…¥æœºåˆ¶èƒ½å¤Ÿæ»¡è¶³å¤šç§å¤æ‚çš„ç”Ÿæˆéœ€æ±‚ï¼Œæå‡ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚æœªæ¥ï¼ŒILMsæœ‰æœ›åœ¨æ›´å¤æ‚çš„ç”Ÿæˆä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autoregressive models (ARMs), which predict subsequent tokens one-by-one ``from left to right,'' have achieved significant success across a wide range of sequence generation tasks. However, they struggle to accurately represent sequences that require satisfying sophisticated constraints or whose sequential dependencies are better addressed by out-of-order generation. Masked Diffusion Models (MDMs) address some of these limitations, but the process of unmasking multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs cannot handle arbitrary infilling constraints when the number of tokens to be filled in is not known in advance. In this work, we introduce Insertion Language Models (ILMs), which learn to insert tokens at arbitrary positions in a sequence -- that is, they select jointly both the position and the vocabulary element to be inserted. By inserting tokens one at a time, ILMs can represent strong dependencies between tokens, and their ability to generate sequences in arbitrary order allows them to accurately model sequences where token dependencies do not follow a left-to-right sequential structure. To train ILMs, we propose a tailored network parameterization and use a simple denoising objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs and MDMs on common planning tasks. Furthermore, we show that ILMs outperform MDMs and perform on par with ARMs in an unconditional text generation task while offering greater flexibility than MDMs in arbitrary-length text infilling. The code is available at: https://dhruveshp.com/projects/ilm .

