---
layout: default
title: A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets
---

# A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06150" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06150v2</a>
  <a href="https://arxiv.org/pdf/2505.06150.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06150v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06150v2', 'A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ryan Lagasse, Aidan Kierans, Avijit Ghosh, Shiri Dori-Hacohen

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09 (æ›´æ–°: 2025-06-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°çš„ç¼©æ”¾æ³•åˆ™ä»¥æé«˜LLMå¾®è°ƒçš„ä»¤ç‰Œæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `æ•°æ®ç»„æˆ` `ä»¤ç‰Œæ•ˆç‡` `ç¼©æ”¾æ³•åˆ™` `èµ„æºä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ€»ä»¤ç‰Œæ•°æ¥è¯„ä¼°è®­ç»ƒæ•°æ®ï¼Œå¿½è§†äº†æ•°æ®ç»„æˆå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¼©æ”¾æ³•åˆ™ï¼Œå¼ºè°ƒæ•°æ®é›†çš„ç¤ºä¾‹æ•°é‡å’Œå¹³å‡ä»¤ç‰Œé•¿åº¦åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ•°æ®ç»„æˆæ˜¾è‘—å½±å“ä»¤ç‰Œæ•ˆç‡ï¼Œä¸ºèµ„æºæœ‰é™çš„LLMå¾®è°ƒæä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç¼©æ”¾æ³•åˆ™ï¼Œè¯¥æ³•åˆ™æ˜ç¡®è€ƒè™‘äº†æ•°æ®ç»„æˆã€‚ä¼ ç»Ÿæ–¹æ³•ä»…é€šè¿‡æ€»ä»¤ç‰Œæ•°æ¥è¡¡é‡è®­ç»ƒæ•°æ®ï¼Œè€Œæˆ‘ä»¬æå‡ºçš„â€œæ•°æ®é›†ä½“ç§¯â€æ¦‚å¿µï¼Œå³ç¤ºä¾‹æ•°é‡å’Œå¹³å‡ä»¤ç‰Œé•¿åº¦ï¼Œæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡åœ¨BRICCæ•°æ®é›†å’ŒMMLUæ•°æ®é›†çš„å¤šä¸ªå­é›†ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜æ•°æ®ç»„æˆå¯¹ä»¤ç‰Œæ•ˆç‡æœ‰æ˜¾è‘—å½±å“ã€‚è¿™äº›å‘ç°ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„LLMå¾®è°ƒæä¾›äº†æ›´ç²¾ç»†çš„ç¼©æ”¾æ³•åˆ™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘æ•°æ®ç»„æˆå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“è¿™ä¸€é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–æ€»ä»¤ç‰Œæ•°ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½è¯„ä¼°ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†â€œæ•°æ®é›†ä½“ç§¯â€çš„æ¦‚å¿µï¼Œå¼ºè°ƒç¤ºä¾‹æ•°é‡å’Œå¹³å‡ä»¤ç‰Œé•¿åº¦å¯¹æ¨¡å‹è®­ç»ƒæ•ˆæœçš„é‡è¦æ€§ã€‚é€šè¿‡å¼•å…¥è¿™ä¸€æ¦‚å¿µï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–LLMçš„å¾®è°ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†åŸºäºBRICCæ•°æ®é›†å’ŒMMLUæ•°æ®é›†çš„å®éªŒæ¡†æ¶ï¼Œè¯„ä¼°ä¸åŒå­é›†å’ŒæŠ½æ ·ç­–ç•¥ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é›†å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€æ€§èƒ½è¯„ä¼°å’Œç»“æœåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†æ–°çš„ç¼©æ”¾æ³•åˆ™ï¼Œæ˜ç¡®äº†æ•°æ®ç»„æˆåœ¨ä»¤ç‰Œæ•ˆç‡ä¸­çš„ä½œç”¨ã€‚è¿™ä¸€æ³•åˆ™ä¸ä¼ ç»Ÿæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸å†ä»…ä¾èµ–æ€»ä»¤ç‰Œæ•°ï¼Œè€Œæ˜¯ç»¼åˆè€ƒè™‘æ•°æ®é›†çš„ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„æŠ½æ ·ç­–ç•¥å’Œæ•°æ®é›†ç»„åˆï¼Œä»¥è¯„ä¼°å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©éµå¾ªäº†å·²æœ‰çš„æœ€ä½³å®è·µï¼Œä»¥ç¡®ä¿å®éªŒçš„æœ‰æ•ˆæ€§å’Œå¯é‡å¤æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ•°æ®ç»„æˆå¯¹ä»¤ç‰Œæ•ˆç‡çš„å½±å“æ˜¾è‘—ã€‚åœ¨ä¸åŒçš„æŠ½æ ·ç­–ç•¥ä¸‹ï¼Œæ¨¡å‹æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ–°ç¼©æ”¾æ³•åˆ™çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€å‘ç°ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„LLMå¾®è°ƒæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒå’Œå®è·µæŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–LLMçš„å¾®è°ƒè¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­æå‡æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒå’Œåº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜è®¡ç®—å’Œç§»åŠ¨è®¾å¤‡ç­‰åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \cite{salavati2024reducing} and subsets of the MMLU dataset \cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings.

