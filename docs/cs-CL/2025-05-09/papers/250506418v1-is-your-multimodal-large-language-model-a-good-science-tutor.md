---
layout: default
title: Is your multimodal large language model a good science tutor?
---

# Is your multimodal large language model a good science tutor?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06418" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06418v1</a>
  <a href="https://arxiv.org/pdf/2505.06418.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06418v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06418v1', 'Is your multimodal large language model a good science tutor?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ming Liu, Liwen Wang, Wensheng Zhang

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ¡†æ¶ä»¥æå‡ç§‘å­¦æ•™è‚²æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç§‘å­¦æ•™è‚²` `æ•™å­¦è¯„ä¼°` `æ¨¡å‹ä¼˜åŒ–` `æ•™è‚²æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¿½è§†äº†å…¶åœ¨æ•™è‚²åœºæ™¯ä¸­çš„æ•™å­¦èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡ç»¼åˆæ•™è‚²æ ‡å‡†å’Œæ¨¡æ‹Ÿå­¦ç”Ÿæ¨¡å‹ï¼Œå…¨é¢è¯„ä¼°MLLMsçš„æ•™å­¦è¡¨ç°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼ºå¤§çš„é—®é¢˜è§£å†³èƒ½åŠ›å¹¶ä¸ç­‰åŒäºé«˜è´¨é‡çš„æ•™å­¦ï¼Œä¼˜åŒ–åçš„æ¨¡å‹åœ¨æ•™è‚²å¯¹é½æ–¹é¢è¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¿½è§†äº†æ•™å­¦èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°MLLMsä½œä¸ºç§‘å­¦å¯¼å¸ˆçš„æ¡†æ¶ï¼Œä½¿ç”¨ç»¼åˆæ•™è‚²æ ‡å‡†å’Œæ¨¡æ‹Ÿå­¦ç”Ÿæ¨¡å‹æ¥åˆ¤æ–­å¯¼å¸ˆçš„æ•™å­¦è¡¨ç°ã€‚é€šè¿‡å¯¹å€™é€‰å¯¼å¸ˆçš„è¡¨ç°è¯„åˆ†ï¼Œæˆ‘ä»¬æ„å»ºäº†å¼ºå¼±å¯¼å¸ˆè¾“å‡ºçš„å¯¹æ¯”æ•°æ®é›†ï¼Œå¹¶åº”ç”¨å¤šç§åå¥½ä¼˜åŒ–æ–¹æ³•å¯¹è¡¨ç°ä¸ä½³çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„é—®é¢˜è§£å†³èƒ½åŠ›å¹¶ä¸ä¸€å®šä¿è¯é«˜è´¨é‡çš„è¾…å¯¼ï¼Œè€Œæ€§èƒ½ä¼˜åŒ–å¼•å¯¼çš„æ”¹è¿›å¯ä»¥äº§ç”Ÿæ›´ç¬¦åˆæ•™è‚²éœ€æ±‚çš„å¯¼å¸ˆæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²åº”ç”¨ä¸­ç¼ºä¹å…¨é¢è¯„ä¼°çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¿½è§†äº†å…¶æ•™å­¦èƒ½åŠ›çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªç»¼åˆæ•™è‚²æ ‡å‡†çš„è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆæ¨¡æ‹Ÿå­¦ç”Ÿæ¨¡å‹ï¼Œå…¨é¢è¯„ä¼°MLLMsä½œä¸ºç§‘å­¦å¯¼å¸ˆçš„è¡¨ç°ï¼Œç¡®ä¿ä¸ä»…å…³æ³¨ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œè¿˜å…³æ³¨æ•™å­¦æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬å€™é€‰å¯¼å¸ˆçš„è¡¨ç°è¯„åˆ†ã€æ„å»ºå¼ºå¼±å¯¼å¸ˆè¾“å‡ºçš„å¯¹æ¯”æ•°æ®é›†ï¼Œä»¥åŠåº”ç”¨å¤šç§åå¥½ä¼˜åŒ–æ–¹æ³•å¯¹è¡¨ç°ä¸ä½³çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¯„ä¼°æ ‡å‡†ã€æ¨¡æ‹Ÿå­¦ç”Ÿæ¨¡å‹å’Œä¼˜åŒ–ç®—æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»¼åˆçš„æ•™è‚²è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿè¯†åˆ«å¼ºå¼±å¯¼å¸ˆï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æå‡å…¶æ•™å­¦èƒ½åŠ›ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•å•ä¸€å…³æ³¨ç­”æ¡ˆå‡†ç¡®æ€§æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¼•å¯¼æ¨¡å‹å‘æ•™è‚²ç›®æ ‡é æ‹¢ï¼Œç¡®ä¿æ¨¡å‹ä¸ä»…èƒ½è§£é¢˜ï¼Œè¿˜èƒ½æœ‰æ•ˆæ•™å­¦ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡ä¼˜åŒ–çš„æ¨¡å‹åœ¨æ•™å­¦èƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŸºçº¿æ¨¡å‹çš„å¯¹æ¯”ä¸­ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æ•™è‚²å¯¹é½åº¦å’Œå­¦ç”Ÿæ»¡æ„åº¦ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€åœ¨çº¿å­¦ä¹ å¹³å°å’Œæ™ºèƒ½è¾…å¯¼ç³»ç»Ÿã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ•™å­¦èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºå­¦ç”Ÿæä¾›æ›´æœ‰æ•ˆçš„å­¦ä¹ æ”¯æŒï¼Œä¿ƒè¿›ä¸ªæ€§åŒ–æ•™è‚²çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) demonstrate impressive performance on scientific reasoning tasks (e.g., ScienceQA). However, most existing benchmarks focus narrowly on the accuracy of the final answer while ignoring other metrics. In particular, when applying MLLMs to educational contexts, the goal is not only correctness but also the ability to teach. In this paper, we propose a framework that evaluates MLLMs as science tutors using a comprehensive educational rubric and a simulated student model that judges the teaching performance of the tutors. Given a list of candidate MLLM science tutors, we use rubric-based student judgments to produce a range of tutor performance scores, identifying both strong and weak tutors. Using the training section of the ScienceQA dataset, we then construct a data set of pairwise comparisons between the outputs of strong and weak tutors. This enables us to apply multiple preference optimization methods to fine-tune an underperforming tutor model (Qwen2-VL-2B) into more effective ones. Our results also show that strong problem-solving skills do not guarantee high-quality tutoring and that performance optimization-guided refinements can yield more educationally aligned tutor models. This approach opens avenues for building MLLMs that serve not only as problem solvers, but as genuinely helpful educational assistants.

