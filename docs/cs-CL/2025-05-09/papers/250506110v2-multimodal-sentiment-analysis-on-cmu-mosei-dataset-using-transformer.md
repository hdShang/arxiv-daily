---
layout: default
title: Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models
---

# Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06110" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06110v2</a>
  <a href="https://arxiv.org/pdf/2505.06110.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06110v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06110v2', 'Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jugal Gajjar, Kaustik Ranaware

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09 (æ›´æ–°: 2025-07-15)

**å¤‡æ³¨**: 6 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åŸºäºTransformeræ¨¡å‹çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ–¹æ³•æå‡æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ` `Transformeræ¨¡å‹` `æ—©æœŸèåˆ` `æƒ…æ„Ÿè¯†åˆ«` `BERTç¼–ç å™¨` `è·¨æ¨¡æ€äº¤äº’` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æƒ…æ„Ÿåˆ†ææ–¹æ³•å¾€å¾€å¿½è§†äº†å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆï¼Œå¯¼è‡´æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ä¸è¶³ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ–¹æ³•ï¼Œé€šè¿‡æ—©æœŸèåˆæŠ€æœ¯æœ‰æ•ˆæ•´åˆæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æµ‹è¯•é›†ä¸Šå®ç°äº†97.87%çš„å‡†ç¡®ç‡å’Œ0.9682çš„F1-scoreï¼Œæ˜¾è‘—æå‡äº†æƒ…æ„Ÿåˆ†æçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬é¡¹ç›®åˆ©ç”¨CMU-MOSEIæ•°æ®é›†è¿›è¡Œå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼Œé‡‡ç”¨åŸºäºTransformerçš„æ¨¡å‹ï¼Œé€šè¿‡æ—©æœŸèåˆæ•´åˆæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ã€‚æˆ‘ä»¬ä¸ºæ¯ç§æ¨¡æ€ä½¿ç”¨BERTç¼–ç å™¨æå–åµŒå…¥ï¼Œå¹¶åœ¨åˆ†ç±»å‰è¿›è¡Œæ‹¼æ¥ã€‚æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†97.87%çš„7ç±»å‡†ç¡®ç‡å’Œ0.9682çš„F1-scoreï¼Œå±•ç¤ºäº†æ—©æœŸèåˆåœ¨æ•æ‰è·¨æ¨¡æ€äº¤äº’ä¸­çš„æœ‰æ•ˆæ€§ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†Adamä¼˜åŒ–ï¼ˆlr=1e-4ï¼‰ã€dropoutï¼ˆ0.3ï¼‰å’Œæ—©åœç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒTransformeræ¶æ„åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿå»ºæ¨¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œä½MAEï¼ˆ0.1060ï¼‰è¡¨æ˜æƒ…æ„Ÿå¼ºåº¦é¢„æµ‹çš„ç²¾ç¡®æ€§ã€‚æœªæ¥çš„å·¥ä½œå¯èƒ½ä¼šæ¯”è¾ƒèåˆç­–ç•¥æˆ–å¢å¼ºå¯è§£é‡Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„ä¿¡æ¯èåˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ•´åˆæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¯¼è‡´æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ—©æœŸèåˆæŠ€æœ¯ï¼Œå°†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯åœ¨åˆ†ç±»å‰è¿›è¡Œæ‹¼æ¥ï¼Œåˆ©ç”¨BERTç¼–ç å™¨æå–å„æ¨¡æ€çš„åµŒå…¥ï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹è·¨æ¨¡æ€äº¤äº’çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ–‡æœ¬æ¨¡æ€ã€éŸ³é¢‘æ¨¡æ€å’Œè§†è§‰æ¨¡æ€çš„BERTç¼–ç å™¨ï¼Œéšåå°†æå–çš„åµŒå…¥è¿›è¡Œæ‹¼æ¥ï¼Œæœ€åé€šè¿‡åˆ†ç±»å™¨è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé‡‡ç”¨æ—©æœŸèåˆç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„æ•æ‰èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡æ€æˆ–åæœŸèåˆæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æƒ…æ„Ÿçš„å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆå­¦ä¹ ç‡ä¸º1e-4ï¼‰ã€dropoutç‡è®¾å®šä¸º0.3ï¼Œå¹¶é‡‡ç”¨æ—©åœç­–ç•¥ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç¡®ä¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨CMU-MOSEIæ•°æ®é›†ä¸Šå®ç°äº†97.87%çš„7ç±»å‡†ç¡®ç‡å’Œ0.9682çš„F1-scoreï¼Œè¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œä½MAEï¼ˆ0.1060ï¼‰è¡¨æ˜è¯¥æ¨¡å‹åœ¨æƒ…æ„Ÿå¼ºåº¦é¢„æµ‹æ–¹é¢çš„é«˜ç²¾åº¦ï¼Œå±•ç¤ºäº†Transformeræ¶æ„åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æã€å®¢æˆ·åé¦ˆå¤„ç†å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æœ‰æ•ˆç»“åˆè¯­è¨€ã€éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œè¿›è€Œä¸ºç›¸å…³è¡Œä¸šæä¾›æ›´ç²¾å‡†çš„ç”¨æˆ·æƒ…æ„Ÿæ´å¯Ÿï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This project performs multimodal sentiment analysis using the CMU-MOSEI dataset, using transformer-based models with early fusion to integrate text, audio, and visual modalities. We employ BERT-based encoders for each modality, extracting embeddings that are concatenated before classification. The model achieves strong performance, with 97.87% 7-class accuracy and a 0.9682 F1-score on the test set, demonstrating the effectiveness of early fusion in capturing cross-modal interactions. The training utilized Adam optimization (lr=1e-4), dropout (0.3), and early stopping to ensure generalization and robustness. Results highlight the superiority of transformer architectures in modeling multimodal sentiment, with a low MAE (0.1060) indicating precise sentiment intensity prediction. Future work may compare fusion strategies or enhance interpretability. This approach utilizes multimodal learning by effectively combining linguistic, acoustic, and visual cues for sentiment analysis.

