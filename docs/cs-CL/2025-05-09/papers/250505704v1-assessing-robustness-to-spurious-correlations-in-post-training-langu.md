---
layout: default
title: Assessing Robustness to Spurious Correlations in Post-Training Language Models
---

# Assessing Robustness to Spurious Correlations in Post-Training Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05704" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05704v1</a>
  <a href="https://arxiv.org/pdf/2505.05704.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05704v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05704v1', 'Assessing Robustness to Spurious Correlations in Post-Training Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Julia Shuieh, Prasann Singhal, Apaar Shanker, John Heyer, George Pu, Samuel Denton

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

**å¤‡æ³¨**: ICLR '25 Workshop on Spurious Correlation and Shortcut Learning

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°åè®­ç»ƒè¯­è¨€æ¨¡å‹å¯¹è™šå‡ç›¸å…³æ€§çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è™šå‡ç›¸å…³æ€§` `åè®­ç»ƒ` `è¯­è¨€æ¨¡å‹` `å¾®è°ƒæŠ€æœ¯` `é²æ£’æ€§è¯„ä¼°` `æ•°å­¦æ¨ç†` `åå¥½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¾®è°ƒæ–¹æ³•åœ¨å¤„ç†ç°å®ä¸–ç•Œæ•°æ®ä¸­çš„è™šå‡ç›¸å…³æ€§æ—¶å­˜åœ¨ä¸è¶³ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸‰ç§åè®­ç»ƒç®—æ³•ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°å…¶åœ¨ä¸åŒè™šå‡ç›¸å…³æ€§æ¡ä»¶ä¸‹çš„è¡¨ç°ï¼Œæ¢ç´¢å…¶é²æ£’æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåå¥½åŸºäºçš„æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ç›¸å¯¹é²æ£’ï¼Œè€ŒSFTåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ï¼Œå¼ºè°ƒäº†æ–¹æ³•é€‰æ‹©çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›‘ç£å’ŒåŸºäºåå¥½çš„å¾®è°ƒæŠ€æœ¯å·²æˆä¸ºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç”¨æˆ·æ„å›¾å’Œæ­£ç¡®æ€§æ ‡å‡†çš„çƒ­é—¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„è®­ç»ƒæ•°æ®å¸¸å¸¸è¡¨ç°å‡ºè™šå‡ç›¸å…³æ€§ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½æˆ–æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†ä¸‰ç§åè®­ç»ƒç®—æ³•â€”â€”ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’ŒKTOï¼ˆå¡å°¼æ›¼-ç‰¹æ²ƒæ–¯åŸºä¼˜åŒ–ï¼‰ï¼Œåœ¨å¤šç§åˆæˆä»»åŠ¡å’Œè™šå‡ç›¸å…³æ€§æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è¾ƒé«˜è™šå‡ç›¸å…³æ€§ä¸‹çš„æ€§èƒ½é€šå¸¸ä¼šä¸‹é™ï¼Œä½†åå¥½åŸºäºçš„æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºç›¸å¯¹é²æ£’æ€§ï¼Œè€ŒSFTåœ¨å¤æ‚çš„ä¸Šä¸‹æ–‡å¯†é›†å‹ä»»åŠ¡ä¸­ä¿æŒè¾ƒå¼ºçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ²¡æœ‰å•ä¸€çš„åè®­ç»ƒç­–ç•¥åœ¨æ‰€æœ‰åœºæ™¯ä¸­éƒ½èƒ½è¡¨ç°ä¼˜è¶Šï¼Œæœ€ä½³é€‰æ‹©ä¾èµ–äºç›®æ ‡ä»»åŠ¡çš„ç±»å‹å’Œè™šå‡ç›¸å…³æ€§çš„æ€§è´¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åè®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨é¢å¯¹è™šå‡ç›¸å…³æ€§æ—¶çš„é²æ£’æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ•°æ®ä¸­çš„åè§å’Œä¼ªç‰¹å¾æ—¶ï¼Œå¸¸å¸¸æ— æ³•ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸‰ç§åè®­ç»ƒç®—æ³•ï¼Œæ¢ç´¢ä¸åŒè™šå‡ç›¸å…³æ€§æ¡ä»¶ä¸‹çš„æ¨¡å‹è¡¨ç°ï¼Œæ—¨åœ¨æ‰¾å‡ºæœ€ä½³çš„å¾®è°ƒç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶è®¾è®¡äº†å¤šç§åˆæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨ç†ã€å—é™æŒ‡ä»¤è·Ÿéšå’Œæ–‡æ¡£åŸºç¡€é—®ç­”ï¼Œè¯„ä¼°ä¸åŒç®—æ³•åœ¨10%ä¸90%è™šå‡ç›¸å…³æ€§ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºæ¯”è¾ƒäº†ä¸‰ç§åè®­ç»ƒç®—æ³•åœ¨ä¸åŒè™šå‡ç›¸å…³æ€§æ¡ä»¶ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä¸åŒæ–¹æ³•åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¸åŠ£åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­è®¾ç½®äº†ä¸åŒç¨‹åº¦çš„è™šå‡ç›¸å…³æ€§ï¼Œå¹¶å®šä¹‰äº†â€œç‰¹å¾æ¨¡ç³Šæ€§â€å’Œâ€œåˆ†å¸ƒç‹­çª„â€ä¸¤ç§ä¼ªç‰¹å¾ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨90%è™šå‡ç›¸å…³æ€§æ¡ä»¶ä¸‹ï¼Œåå¥½åŸºäºçš„æ–¹æ³•ï¼ˆDPO/KTOï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºç›¸å¯¹é²æ£’æ€§ï¼Œè€ŒSFTåœ¨å¤æ‚ä»»åŠ¡ä¸­ä¿æŒè¾ƒå¼ºæ€§èƒ½ã€‚æ•´ä½“ä¸Šï¼Œæ¨¡å‹åœ¨ä¸åŒè™šå‡ç›¸å…³æ€§ä¸‹çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå¼ºè°ƒäº†ä»»åŠ¡ç±»å‹å¯¹å¾®è°ƒç­–ç•¥é€‰æ‹©çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹è™šå‡ç›¸å…³æ€§çš„é²æ£’æ€§ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œè¿›è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½å½±å“æ¨¡å‹å¾®è°ƒçš„æ ‡å‡†å®è·µï¼Œæ¨åŠ¨æ›´ä¸ºå¥å£®çš„è¯­è¨€æ¨¡å‹çš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Supervised and preference-based fine-tuning techniques have become popular for aligning large language models (LLMs) with user intent and correctness criteria. However, real-world training data often exhibits spurious correlations -- arising from biases, dataset artifacts, or other "shortcut" features -- that can compromise a model's performance or generalization. In this paper, we systematically evaluate three post-training algorithms -- Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO (Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and spuriousness conditions. Our tasks span mathematical reasoning, constrained instruction-following, and document-grounded question answering. We vary the degree of spurious correlation (10% vs. 90%) and investigate two forms of artifacts: "Feature Ambiguity" and "Distributional Narrowness." Our results show that the models often but not always degrade under higher spuriousness. The preference-based methods (DPO/KTO) can demonstrate relative robustness in mathematical reasoning tasks. By contrast, SFT maintains stronger performance in complex, context-intensive tasks. These findings highlight that no single post-training strategy universally outperforms in all scenarios; the best choice depends on the type of target task and the nature of spurious correlations.

