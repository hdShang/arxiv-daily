---
layout: default
title: Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM
---

# Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05772" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.05772v1</a>
  <a href="https://arxiv.org/pdf/2505.05772.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05772v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05772v1', 'Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zehao Fan, Garrett Gagnon, Zhenyu Liu, Liu Liu

**ÂàÜÁ±ª**: cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-09

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫STARC‰ª•Ëß£ÂÜ≥PIMÊû∂ÊûÑ‰∏ãLLMËß£Á†ÅÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Á®ÄÁñèÊ≥®ÊÑèÂäõ` `Â§ÑÁêÜÂÜÖÂ≠òËÆ°ÁÆó` `Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜ` `Ê∑±Â∫¶Â≠¶‰π†‰ºòÂåñ` `ÂÜÖÂ≠òÊò†Â∞Ñ` `KVÁºìÂ≠ò` `TransformerÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑLLMËß£Á†ÅÊñπÊ≥ïÂú®Èïø‰∏ä‰∏ãÊñáÊÉÖÂÜµ‰∏ãÂØπÂÜÖÂ≠òÂ∏¶ÂÆΩÈÄ†ÊàêÁì∂È¢àÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. STARCÈÄöËøáËØ≠‰πâËÅöÁ±ªKVÂØπÂπ∂‰ºòÂåñÂÜÖÂ≠òÊò†Â∞ÑÔºåÂÆûÁé∞‰∫ÜÈÄâÊã©ÊÄßÊ≥®ÊÑèÂäõÂíåÂπ∂Ë°åÂ§ÑÁêÜÔºåÂáèÂ∞ë‰∫ÜÊï∞ÊçÆÁßªÂä®ÂºÄÈîÄ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSTARCÂú®Âª∂ËøüÂíåËÉΩËÄóÊñπÈù¢ÂàÜÂà´Èôç‰Ωé‰∫Ü19%-31%Âíå19%-27%ÔºåÂú®KVÁºìÂ≠òÈ¢ÑÁÆó‰∏ãÊõ¥ÊòØÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âü∫‰∫éTransformerÁöÑÊ®°ÂûãÂú®Ëá™ÂõûÂΩíËß£Á†ÅËøáÁ®ã‰∏≠ÂØπÂÜÖÂ≠òÁ≥ªÁªüÈÄ†Êàê‰∫ÜÊòæËëóÂéãÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Èïø‰∏ä‰∏ãÊñáÊÉÖÂÜµ‰∏ã„ÄÇÂ§ÑÁêÜÂÜÖÂ≠ò‰∏≠ÁöÑËÆ°ÁÆóÔºàPIMÔºâÊû∂ÊûÑÊèê‰æõ‰∫ÜÈ´òÂ∏¶ÂÆΩÂíåÂπ∂Ë°åËÆ°ÁÆóËÉΩÂäõÔºå‰ΩÜÁé∞ÊúâËÆæËÆ°‰∏ªË¶ÅÈíàÂØπÂØÜÈõÜÊ≥®ÊÑèÂäõÔºåÈöæ‰ª•Â∫îÂØπÁé∞‰ª£ÈîÆÂÄºÁºìÂ≠òÁöÑÁ®ÄÁñèÊÄßÂ∏¶Êù•ÁöÑÂä®ÊÄÅËÆøÈóÆÊ®°Âºè„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫STARCÁöÑÊñ∞ÂûãÁ®ÄÁñè‰ºòÂåñÊï∞ÊçÆÊò†Â∞ÑÊñπÊ°àÔºåÈÄöËøáËØ≠‰πâÁõ∏‰ººÊÄßÂØπKVÂØπËøõË°åËÅöÁ±ªÔºåÂπ∂Â∞ÜÂÖ∂Êò†Â∞ÑÂà∞‰∏éPIMÈì∂Ë°åÁªìÊûÑÂØπÈΩêÁöÑËøûÁª≠ÂÜÖÂ≠òÂå∫ÂüüÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑLLMËß£Á†Å„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSTARCÂú®Èôç‰ΩéÂª∂ËøüÂíåËÉΩËÄóÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰∏éÊúÄÂÖàËøõÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊ®°ÂûãÂáÜÁ°ÆÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®PIMÊû∂ÊûÑ‰∏ãÔºåLLMËß£Á†ÅËøáÁ®ã‰∏≠Áî±‰∫éÈ¢ëÁπÅÂÜÖÂ≠òËÆøÈóÆÂíåKVÁºìÂ≠òÁ®ÄÁñèÊÄßÂØºËá¥ÁöÑÊÄßËÉΩÁì∂È¢à„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂä®ÊÄÅ„ÄÅÈùûËßÑÂàôÁöÑËÆøÈóÆÊ®°ÂºèÊó∂ÔºåÈù¢‰∏¥Â∑•‰ΩúË¥üËΩΩ‰∏çÂùáË°°ÁöÑÈóÆÈ¢òÔºåÂΩ±Âìç‰∫ÜÂêûÂêêÈáèÂíåËµÑÊ∫êÂà©Áî®Áéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSTARCÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËØ≠‰πâÁõ∏‰ººÊÄßÂØπKVÂØπËøõË°åËÅöÁ±ªÔºåÂπ∂Â∞ÜÂÖ∂Êò†Â∞ÑÂà∞ËøûÁª≠ÁöÑÂÜÖÂ≠òÂå∫ÂüüÔºå‰ªéËÄå‰ºòÂåñÂÜÖÂ≠òËÆøÈóÆÊ®°ÂºèÔºåÂáèÂ∞ëÈ¢ëÁπÅÁöÑÈáçËÅöÁ±ªÂíåÊï∞ÊçÆÁßªÂä®ÂºÄÈîÄ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSTARCÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆËÅöÁ±ªÊ®°Âùó„ÄÅÂÜÖÂ≠òÊò†Â∞ÑÊ®°ÂùóÂíåÊü•ËØ¢Â§ÑÁêÜÊ®°Âùó„ÄÇÊï∞ÊçÆËÅöÁ±ªÊ®°ÂùóË¥üË¥£Ê†πÊçÆËØ≠‰πâÁõ∏‰ººÊÄßÂØπKVÂØπËøõË°åËÅöÁ±ªÔºåÂÜÖÂ≠òÊò†Â∞ÑÊ®°ÂùóÂ∞ÜËÅöÁ±ªÁªìÊûúÊò†Â∞ÑÂà∞PIMÁªìÊûÑ‰∏≠ÔºåÊü•ËØ¢Â§ÑÁêÜÊ®°ÂùóÂàôÂú®Ëß£Á†ÅËøáÁ®ã‰∏≠Ê†πÊçÆÈ¢ÑËÆ°ÁÆóÁöÑË¥®ÂøÉËøõË°åÁõ∏ÂÖ≥tokenÁöÑÊ£ÄÁ¥¢„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSTARCÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Á®ÄÁñè‰ºòÂåñÁöÑÊï∞ÊçÆÊò†Â∞ÑÊñπÊ°àÔºåËÉΩÂ§üÊúâÊïàÂ∫îÂØπÁé∞‰ª£KVÁºìÂ≠òÁöÑÁ®ÄÁñèÊÄßÔºåÊèêÂçá‰∫ÜÂÜÖÂ≠òËÆøÈóÆÁöÑÊïàÁéáÔºå‰∏é‰º†ÁªüÁöÑÂü∫‰∫étokenÁöÑÁ®ÄÁñèÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåSTARCÈááÁî®‰∫ÜËÅöÁ±ªÁÆóÊ≥ïÊù•Á°ÆÂÆöKVÂØπÁöÑË¥®ÂøÉÔºåÂπ∂ÈÄöËøáÂØπÈΩêPIMÈì∂Ë°åÁªìÊûÑÊù•‰ºòÂåñÂÜÖÂ≠òÂ∏ÉÂ±Ä„ÄÇÊ≠§Â§ñÔºåÁ≥ªÁªüÂú®KVÁºìÂ≠òÈ¢ÑÁÆó‰∏ãÁöÑÊÄßËÉΩ‰ºòÂåñÁ≠ñÁï•‰πü‰∏∫ÂÆûÁé∞È´òÊïàËß£Á†ÅÊèê‰æõ‰∫Ü‰øùÈöú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSTARCÂú®HBM-PIMÁ≥ªÁªü‰∏äÁõ∏ËæÉ‰∫éÂ∏∏ËßÅÁöÑtokenÁ∫ßÁ®ÄÁñèÊñπÊ≥ïÔºåÊ≥®ÊÑèÂäõÂ±ÇÂª∂ËøüÈôç‰Ωé‰∫Ü19%-31%ÔºåËÉΩËÄóÂáèÂ∞ë‰∫Ü19%-27%„ÄÇÂú®KVÁºìÂ≠òÈ¢ÑÁÆó‰∏∫1024ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂª∂ËøüÂíåËÉΩËÄóÂàÜÂà´Èôç‰Ωé‰∫Ü54%-74%Âíå45%-67%ÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰∏éÊúÄÂÖàËøõÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊ®°ÂûãÂáÜÁ°ÆÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂØπËØùÁ≥ªÁªüÂíåÊô∫ËÉΩÂä©ÊâãÁ≠âÈúÄË¶ÅÈ´òÊïàLLMËß£Á†ÅÁöÑÂú∫ÊôØ„ÄÇSTARCÁöÑËÆæËÆ°‰∏ç‰ªÖÊèêÂçá‰∫ÜLLMÁöÑÊé®ÁêÜÊïàÁéáÔºåËøò‰∏∫Êú™Êù•Âú®PIMÊû∂ÊûÑ‰∏äÂÆûÁé∞Êõ¥Â§çÊùÇÁöÑÊ∑±Â∫¶Â≠¶‰π†‰ªªÂä°Êèê‰æõ‰∫ÜÂü∫Á°ÄÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Transformer-based models are the foundation of modern machine learning, but their execution, particularly during autoregressive decoding in large language models (LLMs), places significant pressure on memory systems due to frequent memory accesses and growing key-value (KV) caches. This creates a bottleneck in memory bandwidth, especially as context lengths increase. Processing-in-memory (PIM) architectures are a promising solution, offering high internal bandwidth and compute parallelism near memory. However, current PIM designs are primarily optimized for dense attention and struggle with the dynamic, irregular access patterns introduced by modern KV cache sparsity techniques. Consequently, they suffer from workload imbalance, reducing throughput and resource utilization. In this work, we propose STARC, a novel sparsity-optimized data mapping scheme tailored specifically for efficient LLM decoding on PIM architectures. STARC clusters KV pairs by semantic similarity and maps them to contiguous memory regions aligned with PIM bank structures. During decoding, queries retrieve relevant tokens at cluster granularity by matching against precomputed centroids, enabling selective attention and parallel processing without frequent reclustering or data movement overhead. Experiments on the HBM-PIM system show that, compared to common token-wise sparsity methods, STARC reduces attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a KV cache budget of 1024, it achieves up to 54%--74% latency reduction and 45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC maintains model accuracy comparable to state-of-the-art sparse attention methods, demonstrating its effectiveness in enabling efficient and hardware-friendly long-context LLM inference on PIM architectures.

