---
layout: default
title: SEM: Reinforcement Learning for Search-Efficient Large Language Models
---

# SEM: Reinforcement Learning for Search-Efficient Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07903v1</a>
  <a href="https://arxiv.org/pdf/2505.07903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07903v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07903v1', 'SEM: Reinforcement Learning for Search-Efficient Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeyang Sha, Shiwen Cui, Weiqiang Wang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEMæ¡†æ¶ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æœç´¢æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æœç´¢ä¼˜åŒ–` `æ¨ç†æ•ˆç‡` `ä¿¡æ¯æ£€ç´¢` `MuSiQue` `MMLU`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ•™ä¼šæ¨¡å‹ä½•æ—¶è°ƒç”¨æœç´¢ä¸ä½•æ—¶ä¾èµ–å†…éƒ¨çŸ¥è¯†æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¯¼è‡´å†—ä½™æœç´¢è¡Œä¸ºã€‚
2. æœ¬æ–‡æå‡ºSEMæ¡†æ¶ï¼Œé€šè¿‡åè®­ç»ƒå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æœç´¢ä½¿ç”¨ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEMæ˜¾è‘—å‡å°‘å†—ä½™æœç´¢æ“ä½œï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¿æŒæˆ–æé«˜äº†å›ç­”å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å’Œè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆç‰¹åˆ«æ˜¯æœç´¢å¼•æ“ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¦‚ä½•æ•™ä¼šæ¨¡å‹åˆ¤æ–­ä½•æ—¶è°ƒç”¨æœç´¢ã€ä½•æ—¶ä¾èµ–å†…éƒ¨çŸ¥è¯†ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¾€å¾€å¯¼è‡´å†—ä½™çš„æœç´¢è¡Œä¸ºï¼Œé€ æˆæ•ˆç‡ä½ä¸‹å’Œæˆæœ¬è¿‡é«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶SEMï¼Œæ˜ç¡®è®­ç»ƒLLMsä¼˜åŒ–æœç´¢ä½¿ç”¨ã€‚é€šè¿‡æ„å»ºç»“åˆMuSiQueå’ŒMMLUçš„å¹³è¡¡æ•°æ®é›†ï¼Œåˆ›å»ºæ¨¡å‹å¿…é¡»å­¦ä¹ åŒºåˆ†å¯ç›´æ¥å›ç­”çš„é—®é¢˜å’Œéœ€è¦å¤–éƒ¨æ£€ç´¢çš„é—®é¢˜çš„åœºæ™¯ã€‚æˆ‘ä»¬è®¾è®¡äº†ç»“æ„åŒ–æ¨ç†æ¨¡æ¿ï¼Œå¹¶é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹æ¨¡å‹çš„æœç´¢è¡Œä¸ºè¿›è¡Œåè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å†—ä½™æœç´¢æ“ä½œï¼ŒåŒæ—¶åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ä¸Šä¿æŒæˆ–æé«˜äº†å›ç­”å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æœç´¢ä½¿ç”¨ä¸Šçš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¸¸å¯¼è‡´å†—ä½™æœç´¢ï¼Œå½±å“æ€§èƒ½å’Œæˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºSEMæ¡†æ¶ï¼Œé€šè¿‡åè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ˜ç¡®è®­ç»ƒæ¨¡å‹ä¼˜åŒ–æœç´¢ä½¿ç”¨ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°åˆ¤æ–­ä½•æ—¶è°ƒç”¨å¤–éƒ¨æœç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ç»“æ„åŒ–æ¨ç†æ¨¡æ¿è®¾è®¡å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆå®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSEMæ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆMuSiQueå’ŒMMLUæ•°æ®é›†ï¼Œåˆ›å»ºç‰¹å®šåœºæ™¯ä»¥è®­ç»ƒæ¨¡å‹åŒºåˆ†å¯ç›´æ¥å›ç­”çš„é—®é¢˜ä¸éœ€è¦æ£€ç´¢çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè®¾è®¡äº†å¥–åŠ±å‡½æ•°ä»¥é¼“åŠ±å‡†ç¡®å›ç­”è€Œä¸è¿›è¡Œä¸å¿…è¦çš„æœç´¢ï¼ŒåŒæ—¶åœ¨éœ€è¦æ—¶ä¿ƒè¿›æœ‰æ•ˆæ£€ç´¢ï¼Œç¡®ä¿æ¨¡å‹çš„æœç´¢è¡Œä¸ºæ›´åŠ é«˜æ•ˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSEMæ¡†æ¶æ˜¾è‘—å‡å°‘äº†å†—ä½™æœç´¢æ“ä½œï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å›ç­”å‡†ç¡®æ€§ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è™šæ‹ŸåŠ©æ‰‹å’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡ä¼˜åŒ–æœç´¢ä½¿ç”¨ï¼ŒSEMæ¡†æ¶å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨å¤šç§å®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in Large Language Models(LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines. However, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. Existing reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. In this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage. By constructing a balanced dataset combining MuSiQue and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval. We design a structured reasoning template and employ Group Relative Policy Optimization(GRPO) to post-train the model's search behaviors. Our reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. Experimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks. This framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge.

