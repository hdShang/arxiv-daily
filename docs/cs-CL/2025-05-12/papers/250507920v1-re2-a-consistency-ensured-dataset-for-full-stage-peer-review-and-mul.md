---
layout: default
title: Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions
---

# Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07920" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07920v1</a>
  <a href="https://arxiv.org/pdf/2505.07920.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07920v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07920v1', 'Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daoze Zhang, Zhijian Bao, Sihang Du, Zhiyi Zhao, Kuangling Zhang, Dezheng Bao, Yang Yang

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: 2 figures, 5 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRe^2æ•°æ®é›†ä»¥è§£å†³åŒè¡Œè¯„å®¡å’Œåé©³è®¨è®ºä¸­çš„æ•°æ®ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒè¡Œè¯„å®¡` `åé©³è®¨è®º` `æ•°æ®é›†` `å¤šè½®å¯¹è¯` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯„å®¡è´¨é‡` `è‡ªæˆ‘è¯„ä¼°å·¥å…·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŒè¡Œè¯„å®¡æ•°æ®é›†é¢ä¸´æ•°æ®å¤šæ ·æ€§ä¸è¶³å’Œè´¨é‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå½±å“äº†è¯„å®¡æ•ˆæœã€‚
2. æœ¬æ–‡æå‡ºRe^2æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡åˆå§‹æäº¤å’Œå¤šè½®åé©³è®¨è®ºï¼Œæ—¨åœ¨æå‡æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚
3. é€šè¿‡æ„å»ºå¤šè½®å¯¹è¯æ¡†æ¶ï¼ŒRe^2ä¸ºä¼ ç»Ÿè¯„å®¡ä»»åŠ¡å’ŒåŠ¨æ€äº¤äº’åŠ©æ‰‹æä¾›äº†æ”¯æŒï¼Œæå‡äº†è¯„å®¡æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŒè¡Œè¯„å®¡æ˜¯ç§‘å­¦è¿›æ­¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†éšç€æŠ•ç¨¿é‡çš„å¿«é€Ÿå¢åŠ ï¼Œè¯„å®¡ç³»ç»Ÿé¢ä¸´å‹åŠ›ï¼Œå¯¼è‡´è¯„å®¡äººå‘˜çŸ­ç¼ºå’Œè¯„å®¡è´¨é‡ä¸‹é™ã€‚ç°æœ‰çš„åŒè¡Œè¯„å®¡æ•°æ®é›†å­˜åœ¨æ•°æ®å¤šæ ·æ€§ä¸è¶³ã€æ•°æ®è´¨é‡ä¸ä¸€è‡´ä»¥åŠå¯¹åé©³å’Œä½œè€…-è¯„å®¡è€…äº’åŠ¨æ”¯æŒä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºRe^2çš„æœ€å¤§ä¸€è‡´æ€§ä¿éšœçš„åŒè¡Œè¯„å®¡å’Œåé©³æ•°æ®é›†ï¼ŒåŒ…å«19,926ä¸ªåˆå§‹æäº¤ã€70,668æ¡è¯„å®¡è¯„è®ºå’Œ53,818æ¡åé©³ï¼Œæ”¯æŒå¤šè½®å¯¹è¯èŒƒå¼ï¼Œæ—¨åœ¨ä¸ºä½œè€…æä¾›æ›´å®ç”¨çš„æŒ‡å¯¼ï¼Œå‡è½»è¯„å®¡è´Ÿæ‹…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŒè¡Œè¯„å®¡æ•°æ®é›†åœ¨æ•°æ®å¤šæ ·æ€§ã€è´¨é‡ä¸ä¸€è‡´åŠå¯¹åé©³æ”¯æŒä¸è¶³ç­‰æ–¹é¢çš„ç—›ç‚¹ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯„å®¡è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºRe^2æ•°æ®é›†ï¼Œé€šè¿‡æ”¶é›†åˆå§‹æäº¤å’Œåé©³è¯„è®ºï¼Œæ„å»ºä¸€ä¸ªå¤šè½®å¯¹è¯æ¡†æ¶ï¼Œä»¥æ”¯æŒåŠ¨æ€äº¤äº’å’Œä¼ ç»Ÿè¯„å®¡ä»»åŠ¡ï¼Œä»è€Œæé«˜æ•°æ®çš„å®ç”¨æ€§å’Œè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRe^2æ•°æ®é›†çš„æ•´ä½“æ¶æ„åŒ…æ‹¬åˆå§‹æäº¤ã€è¯„å®¡è¯„è®ºå’Œåé©³ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œé‡‡ç”¨å¤šè½®å¯¹è¯çš„å½¢å¼æ¥æ¨¡æ‹ŸçœŸå®çš„è¯„å®¡å’Œåé©³è¿‡ç¨‹ï¼Œç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šRe^2æ•°æ®é›†çš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶è§„æ¨¡å’Œä¸€è‡´æ€§ä¿éšœï¼ŒåŒ…å«å¤§é‡åˆå§‹æäº¤å’Œåé©³ï¼Œå¡«è¡¥äº†ç°æœ‰æ•°æ®é›†åœ¨å¤šè½®äº’åŠ¨å’Œåé©³æ”¯æŒæ–¹é¢çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­ï¼Œç¡®ä¿äº†åˆå§‹æäº¤çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œé‡‡ç”¨äº†ä¸¥æ ¼çš„ç­›é€‰æ ‡å‡†ï¼Œç¡®ä¿æ¯æ¡è¯„è®ºå’Œåé©³éƒ½å…·æœ‰é«˜è´¨é‡å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Re^2æ•°æ®é›†åŒ…å«19,926ä¸ªåˆå§‹æäº¤å’Œ70,668æ¡è¯„å®¡è¯„è®ºï¼Œæ˜¾è‘—æå‡äº†æ•°æ®çš„å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡å¤šè½®å¯¹è¯æ¡†æ¶ï¼Œæ”¯æŒåŠ¨æ€äº¤äº’ï¼Œå¸®åŠ©ä½œè€…æ›´å¥½åœ°å®Œå–„ç¨¿ä»¶ï¼Œå‡è½»è¯„å®¡è´Ÿæ‹…ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Re^2æ•°æ®é›†çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å­¦æœ¯è®ºæ–‡çš„åŒè¡Œè¯„å®¡ã€è‡ªåŠ¨åŒ–è¯„å®¡åŠ©æ‰‹çš„å¼€å‘ä»¥åŠå­¦æœ¯å†™ä½œçš„è‡ªæˆ‘è¯„ä¼°å·¥å…·ã€‚å…¶å®é™…ä»·å€¼åœ¨äºæå‡è¯„å®¡æ•ˆç‡å’Œè´¨é‡ï¼Œæœªæ¥å¯èƒ½å¯¹ç§‘å­¦ç ”ç©¶çš„è¿›å±•äº§ç”Ÿç§¯æå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Peer review is a critical component of scientific progress in the fields like AI, but the rapid increase in submission volume has strained the reviewing system, which inevitably leads to reviewer shortages and declines review quality. Besides the growing research popularity, another key factor in this overload is the repeated resubmission of substandard manuscripts, largely due to the lack of effective tools for authors to self-evaluate their work before submission. Large Language Models (LLMs) show great promise in assisting both authors and reviewers, and their performance is fundamentally limited by the quality of the peer review data. However, existing peer review datasets face three major limitations: (1) limited data diversity, (2) inconsistent and low-quality data due to the use of revised rather than initial submissions, and (3) insufficient support for tasks involving rebuttal and reviewer-author interactions. To address these challenges, we introduce the largest consistency-ensured peer review and rebuttal dataset named Re^2, which comprises 19,926 initial submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the rebuttal and discussion stage is framed as a multi-turn conversation paradigm to support both traditional static review tasks and dynamic interactive LLM assistants, providing more practical guidance for authors to refine their manuscripts and helping alleviate the growing review burden. Our data and code are available in https://anonymous.4open.science/r/ReviewBench_anon/.

