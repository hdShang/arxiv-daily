---
layout: default
title: "KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification"
---

# KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07162" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.07162v1</a>
  <a href="https://arxiv.org/pdf/2505.07162.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07162v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07162v1', 'KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hajar Sakai, Sarah S. Lam

**ÂàÜÁ±ª**: cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-12

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫KDH-MLTC‰ª•Ëß£ÂÜ≥ÂåªÁñóÂ§öÊ†áÁ≠æÊñáÊú¨ÂàÜÁ±ªÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `ÂåªÁñóÊñáÊú¨ÂàÜÁ±ª` `Â§öÊ†áÁ≠æÂàÜÁ±ª` `Á≤íÂ≠êÁæ§‰ºòÂåñ` `Ê®°ÂûãÂéãÁº©` `È°∫Â∫èÂæÆË∞É` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂåªÁñóÊñáÊú¨ÂàÜÁ±ªÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÂåªÂ≠¶ÊúØËØ≠Êó∂ÔºåÂæÄÂæÄÈù¢‰∏¥ËÆ°ÁÆóËµÑÊ∫ê‰∏çË∂≥ÂíåÂáÜÁ°ÆÊÄß‰∏çË∂≥ÁöÑÊåëÊàò„ÄÇ
2. KDH-MLTCÈÄöËøáÁü•ËØÜËí∏È¶èÂíåÈ°∫Â∫èÂæÆË∞ÉÔºåÂ∞ÜÂ§çÊùÇÊ®°ÂûãÁöÑÁü•ËØÜËΩ¨ÁßªÂà∞ËΩªÈáèÊ®°ÂûãÔºå‰ºòÂåñËÆ°ÁÆóÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ
3. Âú®‰∏â‰∏™ÂåªÁñóÊñáÁåÆÊï∞ÊçÆÈõÜ‰∏äÔºåKDH-MLTCÁöÑF1ÂàÜÊï∞Ëææ82.70%ÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®Â§ßÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Á™ÅÂá∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂåªÁñóÊñáÊú¨Êï∞ÊçÆÈáèÁöÑÂ¢ûÂä†Ôºå‰∫üÈúÄÈ´òÊïà‰∏îÂáÜÁ°ÆÁöÑÂàÜÁ±ªÊñπÊ≥ïÊù•Â§ÑÁêÜÂ§çÊùÇÁöÑÂåªÂ≠¶ÊúØËØ≠„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂåªÁñóÂ§öÊ†áÁ≠æÊñáÊú¨ÂàÜÁ±ªÁöÑÁü•ËØÜËí∏È¶èÊ°ÜÊû∂KDH-MLTCÔºåËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÊ®°ÂûãÂéãÁº©ÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÇÈÄöËøáÁü•ËØÜËí∏È¶èÂíåÈ°∫Â∫èÂæÆË∞ÉÔºåKDH-MLTCÊúâÊïàÂ∫îÂØπ‰º†ÁªüÂåªÁñóÂ§öÊ†áÁ≠æÊñáÊú¨ÂàÜÁ±ªÁöÑÊåëÊàòÔºåÂπ∂ÈÄöËøáÁ≤íÂ≠êÁæ§‰ºòÂåñÔºàPSOÔºâËøõË°åË∂ÖÂèÇÊï∞Ë∞É‰ºò„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÂ§çÊùÇÁöÑÊïôÂ∏àÊ®°ÂûãÔºàÂ¶ÇBERTÔºâ‰∏≠ÁöÑÁü•ËØÜËΩ¨ÁßªÂà∞ËΩªÈáèÁ∫ßÂ≠¶ÁîüÊ®°ÂûãÔºàÂ¶ÇDistilBERTÔºâÔºåÂú®‰øùÁïôÊïôÂ∏àÂ≠¶‰π†‰ø°ÊÅØÁöÑÂêåÊó∂ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÈúÄÊ±Ç„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåKDH-MLTCÂú®‰∏â‰∏™‰∏çÂêåËßÑÊ®°ÁöÑÂåªÁñóÊñáÁåÆÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®ÊúÄÂ§ßÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü82.70%ÁöÑF1ÂàÜÊï∞ÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂåªÁñóÂ§öÊ†áÁ≠æÊñáÊú¨ÂàÜÁ±ª‰∏≠ÁöÑËÆ°ÁÆóÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÂåªÂ≠¶ÊúØËØ≠Êó∂ÔºåÂæÄÂæÄÈúÄË¶ÅÂ§ßÈáèËÆ°ÁÆóËµÑÊ∫êÔºåÈöæ‰ª•Êª°Ë∂≥ÂÆûÈôÖÂ∫îÁî®ÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöKDH-MLTCÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁü•ËØÜËí∏È¶èÂ∞ÜÂ§çÊùÇÁöÑÊïôÂ∏àÊ®°ÂûãÔºàÂ¶ÇBERTÔºâÁöÑÁü•ËØÜËΩ¨ÁßªÂà∞ËΩªÈáèÁ∫ßÂ≠¶ÁîüÊ®°ÂûãÔºàÂ¶ÇDistilBERTÔºâÔºåÂπ∂ÁªìÂêàÈ°∫Â∫èÂæÆË∞ÉÂíåÁ≤íÂ≠êÁæ§‰ºòÂåñÔºå‰ª•ÊèêÈ´òÂàÜÁ±ªÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöKDH-MLTCÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Áü•ËØÜËí∏È¶èÊ®°Âùó„ÄÅÈ°∫Â∫èÂæÆË∞ÉÊ®°ÂùóÂíåË∂ÖÂèÇÊï∞‰ºòÂåñÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÁü•ËØÜËí∏È¶èÂ∞ÜÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜËΩ¨ÁßªÂà∞Â≠¶ÁîüÊ®°ÂûãÔºåÁÑ∂ÂêéËøõË°åÈ°∫Â∫èÂæÆË∞ÉÔºåÊúÄÂêéÂà©Áî®Á≤íÂ≠êÁæ§‰ºòÂåñËøõË°åË∂ÖÂèÇÊï∞Ë∞É‰ºòÔºå‰ª•Ëé∑ÂæóÊúÄ‰Ω≥Ê®°ÂûãÈÖçÁΩÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöKDH-MLTCÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÁü•ËØÜËí∏È¶è‰∏éÈ°∫Â∫èÂæÆË∞ÉÁõ∏ÁªìÂêàÔºåÂπ∂ÈÄöËøáÁ≤íÂ≠êÁæ§‰ºòÂåñËøõË°åË∂ÖÂèÇÊï∞Ë∞É‰ºò„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖ‰øùÁïô‰∫ÜÊïôÂ∏àÊ®°ÂûãÁöÑÂ≠¶‰π†‰ø°ÊÅØÔºåËøòÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÈúÄÊ±ÇÔºåÈÄÇÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑÂåªÁñóÁéØÂ¢É„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜDistilBERT‰Ωú‰∏∫Â≠¶ÁîüÊ®°ÂûãÔºåÂπ∂ÈÄöËøáÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÁü•ËØÜËí∏È¶èËøáÁ®ã„ÄÇÂêåÊó∂ÔºåÁ≤íÂ≠êÁæ§‰ºòÂåñÁî®‰∫éÂØªÊâæÊúÄ‰Ω≥ÁöÑË∂ÖÂèÇÊï∞ÈÖçÁΩÆÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇÂÆûÈ™å‰∏≠ËøòËøõË°å‰∫ÜÊ∂àËûçÁ†îÁ©∂Ôºå‰ª•È™åËØÅÂêÑ‰∏™Ê®°ÂùóÁöÑÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåKDH-MLTCÂú®‰∏â‰∏™‰∏çÂêåËßÑÊ®°ÁöÑÂåªÁñóÊñáÁåÆÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂Âú®ÊúÄÂ§ßÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü82.70%ÁöÑF1ÂàÜÊï∞ÔºåÊòæËëóÈ´ò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇËøô‰∏ÄÁªìÊûúËØÅÊòé‰∫ÜKDH-MLTCÂú®ÂåªÁñóÂ§öÊ†áÁ≠æÊñáÊú¨ÂàÜÁ±ª‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂêåÊó∂‰πüÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

KDH-MLTCÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÂåªÁñóÊñáÊú¨ÂàÜÁ±ªÈ¢ÜÂüü„ÄÇÂÖ∂È´òÊïàÁöÑÂàÜÁ±ªËÉΩÂäõ‰ΩøÂÖ∂ÈÄÇÁî®‰∫éÂåªÈô¢„ÄÅËØäÊâÄÁ≠âËµÑÊ∫êÂèóÈôêÁöÑÂåªÁñóÁéØÂ¢ÉÔºåËÉΩÂ§üÂ∏ÆÂä©ÂåªÁñó‰∏ì‰∏ö‰∫∫ÂëòÂø´ÈÄü„ÄÅÂáÜÁ°ÆÂú∞Â§ÑÁêÜÂ§ßÈáèÂåªÁñóÊñáÁåÆÔºå‰ªéËÄåÊèêÈ´òÂåªÁñóÂÜ≥Á≠ñÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÁöÑHIPAAÂêàËßÑÊÄßÁ°Æ‰øù‰∫ÜÊÇ£ËÄÖÊï∞ÊçÆÁöÑÂÆâÂÖ®ÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The increasing volume of healthcare textual data requires computationally efficient, yet highly accurate classification approaches able to handle the nuanced and complex nature of medical terminology. This research presents Knowledge Distillation for Healthcare Multi-Label Text Classification (KDH-MLTC), a framework leveraging model compression and Large Language Models (LLMs). The proposed approach addresses conventional healthcare Multi-Label Text Classification (MLTC) challenges by integrating knowledge distillation and sequential fine-tuning, subsequently optimized through Particle Swarm Optimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from a more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e., DistilBERT) through sequential training adapted to MLTC that preserves the teacher's learned information while significantly reducing computational requirements. As a result, the classification is enabled to be conducted locally, making it suitable for healthcare textual data characterized by sensitivity and, therefore, ensuring HIPAA compliance. The experiments conducted on three medical literature datasets of different sizes, sampled from the Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves superior performance compared to existing approaches, particularly for the largest dataset, reaching an F1 score of 82.70%. Additionally, statistical validation and an ablation study are carried out, proving the robustness of KDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process allowed the identification of optimal configurations. The proposed approach contributes to healthcare text classification research, balancing efficiency requirements in resource-constrained healthcare settings with satisfactory accuracy demands.

