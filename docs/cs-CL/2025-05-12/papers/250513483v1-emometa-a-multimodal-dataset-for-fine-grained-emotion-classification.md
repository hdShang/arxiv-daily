---
layout: default
title: "EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors"
---

# EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13483" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13483v1</a>
  <a href="https://arxiv.org/pdf/2505.13483.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13483v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13483v1', 'EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingyuan Lu, Yuxi Liu, Dongyu Zhang, Zhiyao Wu, Jing Ren, Feng Xia

**åˆ†ç±»**: cs.CL, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**DOI**: [10.1145/3701716.3735080](https://doi.org/10.1145/3701716.3735080)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/DUTIR-YSQ/EmoMeta)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEmoMetaæ•°æ®é›†ä»¥è§£å†³ä¸­æ–‡éšå–»æƒ…æ„Ÿåˆ†ç±»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ•°æ®é›†` `éšå–»åˆ†æ` `æƒ…æ„Ÿåˆ†ç±»` `ä¸­æ–‡å¤„ç†` `æƒ…æ„Ÿæ™ºèƒ½` `å¹¿å‘Šç ”ç©¶` `ç»†ç²’åº¦åˆ†ç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­éšå–»æƒ…æ„Ÿåˆ†ç±»ï¼Œç¼ºä¹å¯¹ä¸­æ–‡å¤šæ¨¡æ€éšå–»çš„ç ”ç©¶ï¼Œé™åˆ¶äº†æƒ…æ„Ÿæ™ºèƒ½çš„å‘å±•ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒ…å«5000å¯¹æ–‡æœ¬-å›¾åƒçš„ä¸­æ–‡éšå–»æ•°æ®é›†ï¼Œæ³¨é‡ç»†ç²’åº¦æƒ…æ„Ÿåˆ†ç±»ï¼Œå¡«è¡¥äº†å¤šæ¨¡æ€éšå–»ç ”ç©¶çš„ç©ºç™½ã€‚
3. æ•°æ®é›†çš„å‘å¸ƒå°†ä¿ƒè¿›å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»çš„ç ”ç©¶ï¼Œæ¨åŠ¨æƒ…æ„Ÿæ™ºèƒ½åœ¨ä¸åŒè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšå–»åœ¨æƒ…æ„Ÿè¡¨è¾¾ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå› æ­¤å¯¹æƒ…æ„Ÿæ™ºèƒ½è‡³å…³é‡è¦ã€‚éšç€å¤šæ¨¡æ€æ•°æ®å’Œå¹¿æ³›äº¤æµçš„å‡ºç°ï¼Œå¤šæ¨¡æ€éšå–»çš„å¢å¤šä½¿å¾—æƒ…æ„Ÿåˆ†ç±»çš„å¤æ‚æ€§å¢åŠ ã€‚ç„¶è€Œï¼Œå…³äºæ„å»ºå¤šæ¨¡æ€éšå–»ç»†ç²’åº¦æƒ…æ„Ÿæ•°æ®é›†çš„ç ”ç©¶ç›¸å¯¹ç¨€ç¼ºï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œå¿½è§†äº†ä¸åŒè¯­è¨€é—´æƒ…æ„Ÿç»†å¾®å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«5000å¯¹éšå–»å¹¿å‘Šæ–‡æœ¬-å›¾åƒçš„ä¸­æ–‡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ‰€æœ‰æ¡ç›®å‡ç»è¿‡ç»†è‡´æ³¨é‡Šï¼Œæ¶µç›–éšå–»å‡ºç°ã€é¢†åŸŸå…³ç³»åŠç»†ç²’åº¦æƒ…æ„Ÿåˆ†ç±»ï¼ŒåŒ…æ‹¬å–œæ‚¦ã€çˆ±ã€ä¿¡ä»»ã€ææƒ§ã€æ‚²ä¼¤ã€åŒæ¶ã€æ„¤æ€’ã€æƒŠè®¶ã€æœŸå¾…å’Œä¸­æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œæ—¨åœ¨æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä¸­æ–‡éšå–»æƒ…æ„Ÿåˆ†ç±»ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºå•æ¨¡æ€æˆ–è‹±è¯­éšå–»ï¼Œç¼ºä¹å¯¹ä¸­æ–‡å¤šæ¨¡æ€éšå–»çš„æ·±å…¥ç ”ç©¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«5000å¯¹æ–‡æœ¬-å›¾åƒçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæä¾›ä¸°å¯Œçš„éšå–»æƒ…æ„Ÿä¿¡æ¯ï¼Œæ”¯æŒç»†ç²’åº¦æƒ…æ„Ÿåˆ†ç±»ï¼Œæ—¨åœ¨æå‡æƒ…æ„Ÿæ™ºèƒ½çš„ç ”ç©¶æ·±åº¦å’Œå¹¿åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•°æ®é›†çš„æ„å»ºåŒ…æ‹¬æ•°æ®æ”¶é›†ã€éšå–»æ ‡æ³¨ã€é¢†åŸŸå…³ç³»åˆ†æåŠæƒ…æ„Ÿåˆ†ç±»å››ä¸ªä¸»è¦æ¨¡å—ï¼Œç¡®ä¿æ•°æ®çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ„å»ºäº†ä¸­æ–‡å¤šæ¨¡æ€éšå–»æƒ…æ„Ÿæ•°æ®é›†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ï¼Œå¹¶æä¾›äº†å¤šç§æƒ…æ„Ÿåˆ†ç±»æ ‡ç­¾ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†ä¸­çš„æ¯ä¸€å¯¹æ–‡æœ¬-å›¾åƒå‡ç»è¿‡ä¸“å®¶æ³¨é‡Šï¼Œç¡®ä¿éšå–»çš„å‡†ç¡®æ€§å’Œæƒ…æ„Ÿåˆ†ç±»çš„ç»†è‡´æ€§ï¼Œé‡‡ç”¨å¤šç§æƒ…æ„Ÿæ ‡ç­¾ä»¥æ”¯æŒç»†ç²’åº¦åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨EmoMetaæ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨ç»†ç²’åº¦æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†çº¦15%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€å¹¿å‘Šè®¾è®¡ã€å¿ƒç†å­¦ç ”ç©¶ç­‰ã€‚é€šè¿‡æä¾›å¤šæ¨¡æ€éšå–»æƒ…æ„Ÿæ•°æ®é›†ï¼Œç ”ç©¶è€…å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œåˆ†æä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œæ¨åŠ¨æƒ…æ„Ÿæ™ºèƒ½æŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Metaphors play a pivotal role in expressing emotions, making them crucial for emotional intelligence. The advent of multimodal data and widespread communication has led to a proliferation of multimodal metaphors, amplifying the complexity of emotion classification compared to single-mode scenarios. However, the scarcity of research on constructing multimodal metaphorical fine-grained emotion datasets hampers progress in this domain. Moreover, existing studies predominantly focus on English, overlooking potential variations in emotional nuances across languages. To address these gaps, we introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of metaphorical advertisements. Each entry is meticulously annotated for metaphor occurrence, domain relations and fine-grained emotion classification encompassing joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, and neutral. Our dataset is publicly accessible (https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in this burgeoning field.

