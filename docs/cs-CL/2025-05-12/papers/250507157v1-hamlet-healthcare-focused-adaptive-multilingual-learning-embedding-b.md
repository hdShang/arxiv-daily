---
layout: default
title: "HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling"
---

# HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07157" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07157v1</a>
  <a href="https://arxiv.org/pdf/2505.07157.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07157v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07157v1', 'HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hajar Sakai, Sarah S. Lam

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHAMLETä»¥è§£å†³åŒ»ç–—é¢†åŸŸå¤šè¯­è¨€ä¸»é¢˜å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸»é¢˜å»ºæ¨¡` `å¤šè¯­è¨€å¤„ç†` `åŒ»ç–—æ–‡æœ¬åˆ†æ` `å›¾ç¥ç»ç½‘ç»œ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯­ä¹‰èåˆ` `BERT` `GNN`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¸»é¢˜æ¨¡å‹åœ¨å¤„ç†åŒ»ç–—é¢†åŸŸçš„å¤šè¯­è¨€æ–‡æœ¬æ—¶ï¼Œå¸¸å¸¸æ— æ³•æœ‰æ•ˆæ•æ‰ä¸Šä¸‹æ–‡å’Œè¯ä¹‰çš„ç»†å¾®å·®åˆ«ï¼Œå¯¼è‡´ä¸»é¢˜è´¨é‡ä½ä¸‹ã€‚
2. HAMLETé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸»é¢˜åµŒå…¥ç²¾ç‚¼æ–¹æ³•ï¼Œå¢å¼ºäº†ä¸»é¢˜çš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚
3. åœ¨ä¸¤ä¸ªåŒ»ç–—æ•°æ®é›†çš„å®éªŒä¸­ï¼ŒHAMLETå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤šè¯­è¨€ä¸»é¢˜å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿä¸»é¢˜æ¨¡å‹åœ¨å¤„ç†ä¸Šä¸‹æ–‡ç»†å¾®å·®åˆ«ã€æ­§ä¹‰è¯å’Œç¨€æœ‰è¯æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„ä¸»é¢˜ç¼ºä¹è¿è´¯æ€§å’Œè´¨é‡ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶å¯ä»¥ç”Ÿæˆåˆæ­¥ä¸»é¢˜ï¼Œä½†è¿™äº›ä¸»é¢˜é€šå¸¸ç¼ºä¹ä»£è¡¨æ€§å’Œç²¾ç‚¼åº¦ã€‚æœ¬æ–‡æå‡ºäº†HAMLETï¼Œä¸€ç§åŸºäºå›¾çš„è·¨è¯­è¨€åŒ»ç–—ä¸»é¢˜å»ºæ¨¡æ¶æ„ï¼Œåˆ©ç”¨LLMsç”Ÿæˆä¸»é¢˜åï¼Œé€šè¿‡ç¥ç»å¢å¼ºçš„è¯­ä¹‰èåˆæŠ€æœ¯å¯¹ä¸»é¢˜åµŒå…¥è¿›è¡Œç²¾ç‚¼ã€‚è¯¥æ–¹æ³•ç»“åˆäº†BERTå’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œåœ¨ä¸»é¢˜ç”Ÿæˆåé‡‡ç”¨æ··åˆæŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–ä¸»é¢˜è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHAMLETåœ¨ä¸¤ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰æ•ˆï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿä¸»é¢˜æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸå¤šè¯­è¨€æ–‡æœ¬å¤„ç†ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡ç†è§£ã€æ­§ä¹‰å¤„ç†å’Œç¨€æœ‰è¯æ±‡çš„ä¸»é¢˜ç”Ÿæˆæ–¹é¢å­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¯¼è‡´ä¸»é¢˜ç¼ºä¹è¿è´¯æ€§å’Œè´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHAMLETçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæ­¥ä¸»é¢˜ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå¯¹ä¸»é¢˜åµŒå…¥è¿›è¡Œç²¾ç‚¼ï¼Œä»è€Œæé«˜ä¸»é¢˜çš„è¯­ä¹‰è¡¨ç¤ºå’Œå¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHAMLETçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸»é¢˜ç”Ÿæˆã€ä¸»é¢˜åµŒå…¥ç²¾ç‚¼å’Œä¸»é¢˜ç›¸ä¼¼æ€§è®¡ç®—ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä½¿ç”¨LLMsç”Ÿæˆåˆæ­¥ä¸»é¢˜ï¼Œç„¶åé€šè¿‡BERTå’ŒSBERTè¿›è¡ŒåµŒå…¥ï¼Œæœ€ååˆ©ç”¨GNNå»ºç«‹æ–‡æ¡£ã€ä¸»é¢˜å’Œè¯ä¹‹é—´çš„è¿æ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šHAMLETçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†ç¥ç»å¢å¼ºçš„è¯­ä¹‰èåˆæŠ€æœ¯ï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œå¯¹ä¸»é¢˜åµŒå…¥è¿›è¡Œç²¾ç‚¼ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç»Ÿè®¡å…±ç°å’Œäººå·¥è§£é‡Šæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†BERTå’ŒSBERTè¿›è¡Œä¸»é¢˜åµŒå…¥ï¼ŒGNNç”¨äºå»ºç«‹ä¸»é¢˜ä¹‹é—´çš„ç›¸ä¼¼æ€§å…³ç³»ï¼Œå…³é”®å‚æ•°å’ŒæŸå¤±å‡½æ•°çš„è®¾ç½®æ—¨åœ¨ä¼˜åŒ–ä¸»é¢˜çš„è¯­ä¹‰è¡¨ç¤ºå’Œç›¸ä¼¼æ€§è®¡ç®—ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒHAMLETèƒ½å¤Ÿæœ‰æ•ˆæå–å’Œç²¾ç‚¼ä¸»é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒHAMLETåœ¨ä¸¤ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œä¸»é¢˜çš„è¿è´¯æ€§å’Œè´¨é‡æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªæŠ«éœ²ï¼Œä½†å®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HAMLETåœ¨åŒ»ç–—é¢†åŸŸçš„å¤šè¯­è¨€ä¸»é¢˜å»ºæ¨¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’ŒåŒ»ç–—ä¸“ä¸šäººå£«æ›´å¥½åœ°ç†è§£å’Œåˆ†æè·¨è¯­è¨€çš„åŒ»ç–—æ–‡æœ¬æ•°æ®ã€‚å…¶æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸçš„ä¸»é¢˜å»ºæ¨¡ä»»åŠ¡ï¼Œæå‡æ–‡æœ¬åˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional topic models often struggle with contextual nuances and fail to adequately handle polysemy and rare words. This limitation typically results in topics that lack coherence and quality. Large Language Models (LLMs) can mitigate this issue by generating an initial set of topics. However, these raw topics frequently lack refinement and representativeness, which leads to redundancy without lexical similarity and reduced interpretability. This paper introduces HAMLET, a graph-driven architecture for cross-lingual healthcare topic modeling that uses LLMs. The proposed approach leverages neural-enhanced semantic fusion to refine the embeddings of topics generated by the LLM. Instead of relying solely on statistical co-occurrence or human interpretation to extract topics from a document corpus, this method introduces a topic embedding refinement that uses Bidirectional Encoder Representations from Transformers (BERT) and Graph Neural Networks (GNN). After topic generation, a hybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for embedding. The topic representations are further refined using a GNN, which establishes connections between documents, topics, words, similar topics, and similar words. A novel method is introduced to compute similarities. Consequently, the topic embeddings are refined, and the top k topics are extracted. Experiments were conducted using two healthcare datasets, one in English and one in French, from which six sets were derived. The results demonstrate the effectiveness of HAMLET.

