---
layout: default
title: Learning Dynamics in Continual Pre-Training for Large Language Models
---

# Learning Dynamics in Continual Pre-Training for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07796" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07796v2</a>
  <a href="https://arxiv.org/pdf/2505.07796.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07796v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07796v2', 'Learning Dynamics in Continual Pre-Training for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, Daniel Dajun Zeng

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-06-19)

**å¤‡æ³¨**: Accepted to ICML2025 (Oral)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCPTç¼©æ”¾æ³•åˆ™ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒç»­é¢„è®­ç»ƒ` `å¤§è¯­è¨€æ¨¡å‹` `å­¦ä¹ åŠ¨æ€` `æŸå¤±é¢„æµ‹` `è¶…å‚æ•°ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŒç»­é¢„è®­ç»ƒæ–¹æ³•åœ¨å¤„ç†ä¸åŒé¢†åŸŸæ€§èƒ½å¹³è¡¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å­¦ä¹ åŠ¨æ€çš„ç†è§£ä¸Šä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§CPTç¼©æ”¾æ³•åˆ™ï¼Œé€šè¿‡è§£è€¦åˆ†å¸ƒè½¬ç§»å’Œå­¦ä¹ ç‡é€€ç«çš„å½±å“ï¼Œæä¾›äº†å¯¹æŸå¤±é¢„æµ‹çš„å…¨é¢ç†è§£ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§CPTæ•°æ®é›†ä¸Šæœ‰æ•ˆï¼Œèƒ½å¤Ÿä¼˜åŒ–è®­ç»ƒè¶…å‚æ•°ä»¥å®ç°æ›´å¥½çš„é¢†åŸŸç‰¹å®šæ€§èƒ½ä¸ä¸€èˆ¬æ€§èƒ½çš„å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰å·²æˆä¸ºå°†å¼ºå¤§çš„åŸºç¡€æ¨¡å‹åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹åœ¨CPTè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€ï¼Œé‡ç‚¹å…³æ³¨ä¸€èˆ¬é¢†åŸŸå’Œä¸‹æ¸¸é¢†åŸŸæ€§èƒ½åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­çš„æ¼”å˜ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°CPTæŸå¤±æ›²çº¿æœ¬è´¨ä¸Šè¡¨å¾äº†ä»ä¸€ä¸ªæ›²çº¿åˆ°å¦ä¸€ä¸ªéšè—æ›²çº¿çš„è½¬å˜ï¼Œå¹¶é€šè¿‡è§£è€¦åˆ†å¸ƒè½¬ç§»å’Œå­¦ä¹ ç‡é€€ç«çš„å½±å“æ¥æè¿°ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºç»“åˆè¿™ä¸¤ä¸ªå› ç´ çš„CPTç¼©æ”¾æ³•åˆ™ï¼Œä½¿å¾—èƒ½å¤Ÿé¢„æµ‹åœ¨ä»»ä½•ï¼ˆæŒç»­ï¼‰è®­ç»ƒæ­¥éª¤å’Œå­¦ä¹ ç‡è°ƒåº¦ä¸‹çš„æŸå¤±ã€‚æˆ‘ä»¬çš„å…¬å¼å…¨é¢ç†è§£äº†CPTä¸­çš„å¤šä¸ªå…³é”®å› ç´ ï¼ŒåŒ…æ‹¬æŸå¤±æ½œåŠ›ã€å³°å€¼å­¦ä¹ ç‡ã€è®­ç»ƒæ­¥éª¤ã€é‡æ”¾æ¯”ä¾‹ç­‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç¼©æ”¾æ³•åˆ™åœ¨å„ç§CPTæ•°æ®é›†å’Œè®­ç»ƒè¶…å‚æ•°ä¸­å‡æœ‰æ•ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æŒç»­é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åŠ¨æ€ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¦‚ä½•å¹³è¡¡ä¸€èˆ¬é¢†åŸŸä¸ä¸‹æ¸¸é¢†åŸŸæ€§èƒ½æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæè¿°æŸå¤±æ›²çº¿çš„å˜åŒ–åŠå…¶å½±å“å› ç´ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§CPTç¼©æ”¾æ³•åˆ™ï¼Œé€šè¿‡å°†åˆ†å¸ƒè½¬ç§»å’Œå­¦ä¹ ç‡é€€ç«çš„å½±å“è§£è€¦ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹æŸå¤±å˜åŒ–ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç ”ç©¶è€…èƒ½å¤Ÿåœ¨ä¸åŒçš„è®­ç»ƒæ­¥éª¤å’Œå­¦ä¹ ç‡è°ƒåº¦ä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ€§èƒ½è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šæ ¹æ®CPTç¼©æ”¾æ³•åˆ™åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡å’Œé‡æ”¾æ¯”ä¾‹ï¼Œä»¥ä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ¨å¯¼å‡ºCPTç¼©æ”¾æ³•åˆ™ï¼Œè¯¥æ³•åˆ™ç»“åˆäº†åˆ†å¸ƒè½¬ç§»å’Œå­¦ä¹ ç‡é€€ç«çš„å½±å“ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥ç†è§£å’Œé¢„æµ‹CPTè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºå…¶ç³»ç»Ÿæ€§å’Œå¯é¢„æµ‹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œæœ¬æ–‡è®¾ç½®äº†å¤šä¸ªè¶…å‚æ•°ï¼ŒåŒ…æ‹¬å³°å€¼å­¦ä¹ ç‡ã€è®­ç»ƒæ­¥éª¤å’Œé‡æ”¾æ¯”ä¾‹ç­‰ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒçš„CPTç›®æ ‡ä¸‹èƒ½å¤Ÿè‡ªé€‚åº”è°ƒæ•´ï¼Œä»è€Œå®ç°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„CPTç¼©æ”¾æ³•åˆ™åœ¨å¤šä¸ªCPTæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æŸå¤±å˜åŒ–ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¼˜åŒ–è®­ç»ƒè¶…å‚æ•°å’Œæå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–æŒç»­é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°å°†å¤§è¯­è¨€æ¨¡å‹åº”ç”¨äºç‰¹å®šé¢†åŸŸï¼Œæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œé€‚åº”æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.

