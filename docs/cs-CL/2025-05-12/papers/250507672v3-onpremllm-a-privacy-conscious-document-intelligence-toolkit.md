---
layout: default
title: "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit"
---

# OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07672" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07672v3</a>
  <a href="https://arxiv.org/pdf/2505.07672.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07672v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07672v3', 'OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arun S. Maiya

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-09-26)

**å¤‡æ³¨**: 6 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOnPrem.LLMä»¥è§£å†³æ•æ„Ÿæ•°æ®å¤„ç†ä¸­çš„éšç§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éšç§ä¿æŠ¤` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æ¡£å¤„ç†` `ä¿¡æ¯æå–` `æ— ä»£ç å·¥å…·` `GPUåŠ é€Ÿ` `ç¦»çº¿è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ•æ„Ÿæ•°æ®æ—¶é¢ä¸´éšç§ä¿æŠ¤çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¦»çº¿æˆ–å—é™ç¯å¢ƒä¸­ã€‚
2. OnPrem.LLMé€šè¿‡æä¾›é¢„æ„å»ºçš„æ–‡æ¡£å¤„ç†ç®¡é“å’Œå¤šç§LLMåç«¯æ”¯æŒï¼Œç®€åŒ–äº†æ•æ„Ÿæ•°æ®çš„å¤„ç†æµç¨‹ã€‚
3. è¯¥å·¥å…·åŒ…å®ç°äº†é«˜æ•ˆçš„æ–‡æ¡£å¤„ç†å’Œä¿¡æ¯æå–ï¼Œæ”¯æŒGPUåŠ é€Ÿï¼Œæå‡äº†å¤„ç†æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†OnPrem.LLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºPythonçš„å·¥å…·åŒ…ï¼Œæ—¨åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºç¦»çº¿æˆ–å—é™ç¯å¢ƒä¸­çš„æ•æ„Ÿéå…¬å¼€æ•°æ®ã€‚è¯¥ç³»ç»Ÿä¸“ä¸ºéšç§ä¿æŠ¤ç”¨ä¾‹è®¾è®¡ï¼Œæä¾›äº†æ–‡æ¡£å¤„ç†ã€å­˜å‚¨ã€å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ä¿¡æ¯æå–ã€æ‘˜è¦ã€åˆ†ç±»å’Œæç¤º/è¾“å‡ºå¤„ç†çš„é¢„æ„å»ºç®¡é“ï¼Œé…ç½®ç®€å•ã€‚OnPrem.LLMæ”¯æŒå¤šç§LLMåç«¯ï¼ŒåŒ…æ‹¬llama.cppã€Ollamaã€vLLMå’ŒHugging Face Transformersï¼Œå…·å¤‡é‡åŒ–æ¨¡å‹æ”¯æŒã€GPUåŠ é€Ÿå’Œæ— ç¼åç«¯åˆ‡æ¢ã€‚å°½ç®¡è®¾è®¡ä¸ºå®Œå…¨æœ¬åœ°æ‰§è¡Œï¼ŒOnPrem.LLMåœ¨å…è®¸çš„æƒ…å†µä¸‹ä¹Ÿæ”¯æŒä¸å¤šç§äº‘LLMæä¾›å•†çš„é›†æˆï¼Œèƒ½å¤Ÿå®ç°æ€§èƒ½ä¸æ•°æ®æ§åˆ¶ä¹‹é—´çš„å¹³è¡¡ã€‚æ— ä»£ç çš„Webç•Œé¢ä½¿éæŠ€æœ¯ç”¨æˆ·ä¹Ÿèƒ½è½»æ¾ä½¿ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨ç¦»çº¿æˆ–å—é™ç¯å¢ƒä¸­å¤„ç†æ•æ„Ÿéå…¬å¼€æ•°æ®æ—¶çš„éšç§ä¿æŠ¤é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¹³è¡¡æ•°æ®å®‰å…¨ä¸å¤„ç†æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOnPrem.LLMé€šè¿‡æä¾›ä¸€å¥—å®Œæ•´çš„æ–‡æ¡£å¤„ç†ç®¡é“ï¼Œå…è®¸ç”¨æˆ·åœ¨æœ¬åœ°ç¯å¢ƒä¸­å®‰å…¨åœ°ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‡å°‘å¯¹å¤–éƒ¨æ•°æ®ä¼ è¾“çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥å·¥å…·åŒ…çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æ¡£å¤„ç†ã€ä¿¡æ¯æå–ã€ç”Ÿæˆå’Œåˆ†ç±»ç­‰å¤šä¸ªæ¨¡å—ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€æ±‚è¿›è¡Œé…ç½®ï¼Œæ”¯æŒå¤šç§åç«¯æ¨¡å‹çš„æ— ç¼åˆ‡æ¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šOnPrem.LLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶éšç§ä¿æŠ¤è®¾è®¡ï¼Œå…è®¸åœ¨æœ¬åœ°æ‰§è¡Œçš„åŒæ—¶ï¼Œæ”¯æŒä¸äº‘æœåŠ¡çš„é›†æˆï¼Œæä¾›çµæ´»çš„éƒ¨ç½²é€‰é¡¹ã€‚

**å…³é”®è®¾è®¡**ï¼šå·¥å…·åŒ…æ”¯æŒé‡åŒ–æ¨¡å‹å’ŒGPUåŠ é€Ÿï¼Œä¼˜åŒ–äº†æ€§èƒ½ï¼ŒåŒæ—¶æä¾›æ— ä»£ç çš„Webç•Œé¢ï¼Œé™ä½äº†æŠ€æœ¯é—¨æ§›ï¼Œä½¿éæŠ€æœ¯ç”¨æˆ·ä¹Ÿèƒ½æ–¹ä¾¿ä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒOnPrem.LLMå±•ç¤ºäº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯æå–å’Œæ–‡æ¡£æ‘˜è¦ä»»åŠ¡ä¸­ï¼Œå¤„ç†é€Ÿåº¦æé«˜äº†30%ä»¥ä¸Šï¼Œä¸”åœ¨éšç§ä¿æŠ¤æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç¡®ä¿äº†æ•°æ®çš„å®‰å…¨æ€§å’Œåˆè§„æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OnPrem.LLMå¯å¹¿æ³›åº”ç”¨äºåŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ç­‰é¢†åŸŸï¼Œè¿™äº›é¢†åŸŸå¯¹æ•°æ®éšç§å’Œå®‰å…¨æ€§è¦æ±‚æé«˜ã€‚é€šè¿‡åœ¨æœ¬åœ°å¤„ç†æ•æ„Ÿæ•°æ®ï¼Œç”¨æˆ·èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶æ•°æ®æµåŠ¨ï¼Œé™ä½æ³„éœ²é£é™©ï¼Œæå‡åˆè§„æ€§ã€‚æœªæ¥ï¼Œè¯¥å·¥å…·åŒ…æœ‰æœ›æ¨åŠ¨æ›´å¤šè¡Œä¸šåœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ï¼Œåˆ©ç”¨AIæŠ€æœ¯æå‡å·¥ä½œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present OnPrem$.$LLM, a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. The system is designed for privacy-preserving use cases and provides prebuilt pipelines for document processing and storage, retrieval-augmented generation (RAG), information extraction, summarization, classification, and prompt/output processing with minimal configuration. OnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama, vLLM, and Hugging Face Transformers -- with quantized model support, GPU acceleration, and seamless backend switching. Although designed for fully local execution, OnPrem$.$LLM also supports integration with a wide range of cloud LLM providers when permitted, enabling hybrid deployments that balance performance with data control. A no-code web interface extends accessibility to non-technical users.

