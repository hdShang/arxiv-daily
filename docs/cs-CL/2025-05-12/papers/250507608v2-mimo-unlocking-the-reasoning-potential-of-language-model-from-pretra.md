---
layout: default
title: "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining"
---

# MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07608" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07608v2</a>
  <a href="https://arxiv.org/pdf/2505.07608.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07608v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07608v2', 'MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: LLM-Core Xiaomi, :, Bingquan Xia, Bowen Shen, Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, Kai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-06-05)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xiaomimimo/MiMo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMiMo-7Bä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é¢„å¤„ç†` `å¤šæ ‡è®°é¢„æµ‹` `æ•°å­¦é—®é¢˜` `ç¼–ç¨‹é—®é¢˜` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜ä¸Šã€‚
2. è®ºæ–‡æå‡ºäº†MiMo-7Bï¼Œé€šè¿‡æ”¹è¿›æ•°æ®é¢„å¤„ç†å’Œå¼•å…¥å¤šæ ‡è®°é¢„æµ‹ç›®æ ‡ï¼Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMiMo-7B-Baseåœ¨æ¨ç†èƒ½åŠ›ä¸Šè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„32Bæ¨¡å‹ï¼Œæœ€ç»ˆæ¨¡å‹åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†MiMo-7Bï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ¨ç†ä»»åŠ¡è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µæ¥æå‡å…¶æ¨ç†æ½œåŠ›ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬æ”¹è¿›äº†æ•°æ®é¢„å¤„ç†æµç¨‹ï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥å¢å¼ºåŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚MiMo-7B-Baseåœ¨250ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¼•å…¥äº†å¤šæ ‡è®°é¢„æµ‹ç›®æ ‡ä»¥æé«˜æ€§èƒ½å’ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚åœ¨åè®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬ç­–åˆ’äº†ä¸€ä¸ªåŒ…å«13ä¸‡ä¸ªå¯éªŒè¯çš„æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜çš„æ•°æ®é›†ï¼Œç»“åˆæµ‹è¯•éš¾åº¦é©±åŠ¨çš„ä»£ç å¥–åŠ±æœºåˆ¶ï¼Œä»¥ç¼“è§£ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æˆ˜ç•¥æ€§æ•°æ®é‡é‡‡æ ·æ¥ç¨³å®šè®­ç»ƒã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒMiMo-7B-Baseåœ¨æ¨ç†æ½œåŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§çš„32Bæ¨¡å‹ã€‚æœ€ç»ˆçš„RLè°ƒä¼˜æ¨¡å‹MiMo-7B-RLåœ¨æ•°å­¦ã€ä»£ç å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†OpenAIçš„o1-miniã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨https://github.com/xiaomimimo/MiMoè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜æ—¶çš„è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€é¢ä¸´æ•°æ®ç¨€ç–å’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µï¼Œæå‡æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥å’Œå¤šæ ‡è®°é¢„æµ‹ç›®æ ‡ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œæ¨ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒå’Œåè®­ç»ƒä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨25ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†æµç¨‹æå‡åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨åè®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨130Kçš„æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜æ•°æ®é›†è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆæµ‹è¯•éš¾åº¦é©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å¤šæ ‡è®°é¢„æµ‹ç›®æ ‡å’Œæµ‹è¯•éš¾åº¦é©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ï¼Œè¿™äº›è®¾è®¡æœ‰æ•ˆç¼“è§£äº†ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMiMo-7Båœ¨æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥ï¼Œå¹¶åœ¨åè®­ç»ƒä¸­å®æ–½äº†æˆ˜ç•¥æ€§æ•°æ®é‡é‡‡æ ·ï¼Œä»¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„æŸå¤±å‡½æ•°å’Œå¥–åŠ±æœºåˆ¶ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ä¼˜åŒ–æ¨ç†æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMiMo-7B-Baseåœ¨æ¨ç†èƒ½åŠ›ä¸Šè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„32Bæ¨¡å‹ï¼Œæœ€ç»ˆçš„RLè°ƒä¼˜æ¨¡å‹MiMo-7B-RLåœ¨æ•°å­¦ã€ä»£ç å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†OpenAIçš„o1-miniï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©ã€ç§‘å­¦è®¡ç®—ç­‰ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´é«˜æ•ˆçš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚æœªæ¥ï¼ŒMiMo-7Bæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„æ¨ç†ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.

