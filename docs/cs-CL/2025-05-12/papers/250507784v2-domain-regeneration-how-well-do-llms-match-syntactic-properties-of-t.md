---
layout: default
title: "Domain Regeneration: How well do LLMs match syntactic properties of text domains?"
---

# Domain Regeneration: How well do LLMs match syntactic properties of text domains?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07784" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07784v2</a>
  <a href="https://arxiv.org/pdf/2505.07784.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07784v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07784v2', 'Domain Regeneration: How well do LLMs match syntactic properties of text domains?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Da Ju, Hagen Blix, Adina Williams

**åˆ†ç±»**: cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-06-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬é¢†åŸŸè¯­æ³•ç‰¹æ€§åŒ¹é…çš„æœ‰æ•ˆæ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `è¯­æ³•ç‰¹æ€§` `ç»´åŸºç™¾ç§‘` `æ–°é—»æ–‡æœ¬` `è‡ªç„¶è¯­è¨€å¤„ç†` `ç»Ÿè®¡åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œå¯èƒ½æ— æ³•å‡†ç¡®åŒ¹é…åŸå§‹æ–‡æœ¬çš„è¯­æ³•ç‰¹æ€§ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹çš„è´¨é‡ä¸‹é™ã€‚
2. æœ¬æ–‡é€šè¿‡å¯¹æ¯”ç”Ÿæˆçš„æ–‡æœ¬ä¸ç»´åŸºç™¾ç§‘å’Œæ–°é—»æ–‡æœ¬ï¼Œæ¢è®¨LLMsåœ¨è¯­æ³•ç‰¹æ€§åŒ¹é…æ–¹é¢çš„èƒ½åŠ›ï¼Œé‡‡ç”¨è§‚å¯Ÿæ€§æ–¹æ³•è¿›è¡Œåˆ†æã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆæ–‡æœ¬çš„åˆ†å¸ƒåœ¨å‡å€¼å’Œæ ‡å‡†å·®ä¸Šå‡æœ‰æ˜¾è‘—åç§»ï¼Œä¸”é•¿å°¾ç‰¹æ€§æ˜æ˜¾å‡å¼±ï¼Œè¡¨æ˜LLMsåœ¨è¯­æ³•ç‰¹æ€§ä¸Šå­˜åœ¨å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ€§èƒ½çš„æå‡ï¼Œå®ƒä»¬åœ¨è¿‘ä¼¼è®­ç»ƒæ•°æ®åˆ†å¸ƒæ–¹é¢çš„èƒ½åŠ›ä¹Ÿéšä¹‹å¢å¼ºã€‚æœ¬æ–‡ç ”ç©¶äº†LLMsåœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤Ÿå¿ å®åœ°åŒ¹é…æ–‡æœ¬é¢†åŸŸçš„ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯ç»´åŸºç™¾ç§‘å’Œæ–°é—»æ–‡æœ¬è¿™ä¸¤ä¸ªå¸¸è§çš„è‹±è¯­æ–‡æœ¬é¢†åŸŸã€‚é€šè¿‡å¯¹æ¯”ç”Ÿæˆçš„æ–‡æœ¬ä¸åŸå§‹äººç±»æ–‡æœ¬ï¼Œç ”ç©¶äº†å¥å­é•¿åº¦ã€å¯è¯»æ€§ã€ä¾èµ–æ ‡ç­¾åˆ†å¸ƒç­‰ä¸åŒå±‚æ¬¡çš„è¯­æ³•æŠ½è±¡ç‰¹æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œç”Ÿæˆæ–‡æœ¬çš„åˆ†å¸ƒåœ¨å‡å€¼ã€æ ‡å‡†å·®å’Œé•¿å°¾ç‰¹æ€§ä¸Šå‡ä¸äººç±»æ–‡æœ¬å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œå¦‚ä½•åŒ¹é…ä¸åŒæ–‡æœ¬é¢†åŸŸçš„è¯­æ³•ç‰¹æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨æ­¤æ–¹é¢çš„ä¸è¶³åœ¨äºç¼ºä¹ç³»ç»Ÿçš„æ¯”è¾ƒå’Œåˆ†æï¼Œå¯¼è‡´å¯¹LLMsèƒ½åŠ›çš„ç†è§£ä¸å¤Ÿå…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹LLMsç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œç³»ç»Ÿçš„è¯­æ³•ç‰¹æ€§åˆ†æï¼Œæ¯”è¾ƒå…¶ä¸äººç±»åŸå§‹æ–‡æœ¬çš„ç›¸ä¼¼æ€§ï¼Œæ—¨åœ¨æ­ç¤ºLLMsåœ¨ä¸åŒå±‚æ¬¡è¯­æ³•ç‰¹æ€§ä¸Šçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†è§‚å¯Ÿæ€§æ–¹æ³•ï¼Œé¦–å…ˆé€‰æ‹©ç»´åŸºç™¾ç§‘å’Œæ–°é—»æ–‡æœ¬ä½œä¸ºç ”ç©¶å¯¹è±¡ï¼Œç„¶åä½¿ç”¨å¼€æºLLMç”Ÿæˆæ–‡æœ¬ï¼Œæœ€åå¯¹ç”Ÿæˆæ–‡æœ¬çš„è¯­æ³•ç‰¹æ€§è¿›è¡Œé‡åŒ–åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°åˆ†æäº†LLMsç”Ÿæˆæ–‡æœ¬çš„è¯­æ³•ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¤šå±‚æ¬¡çš„è¯­æ³•æŠ½è±¡è¿›è¡Œæ¯”è¾ƒï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†å¤šä¸ªå‚æ•°ä»¥æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬å¥å­é•¿åº¦ã€å¯è¯»æ€§ã€ä¾èµ–æ ‡ç­¾åˆ†å¸ƒç­‰ï¼Œå¹¶é‡‡ç”¨äº†é€‚å½“çš„ç»Ÿè®¡æ–¹æ³•æ¥åˆ†æç”Ÿæˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬çš„å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆæ–‡æœ¬çš„å‡å€¼åç§»ã€æ ‡å‡†å·®é™ä½å’Œé•¿å°¾ç‰¹æ€§å‡å¼±ï¼Œè¡¨æ˜LLMsåœ¨è¯­æ³•ç‰¹æ€§åŒ¹é…ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚è¿™äº›å‘ç°ä¸ºæ”¹è¿›LLMsçš„ç”Ÿæˆèƒ½åŠ›æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡æ·±å…¥ç†è§£LLMsåœ¨è¯­æ³•ç‰¹æ€§åŒ¹é…ä¸Šçš„å±€é™æ€§ï¼Œå¯ä»¥ä¸ºæœªæ¥çš„æ¨¡å‹æ”¹è¿›æä¾›æŒ‡å¯¼ï¼Œæå‡ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œå¯è¯»æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data -- Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.

