---
layout: default
title: On the Robustness of Reward Models for Language Model Alignment
---

# On the Robustness of Reward Models for Language Model Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07271" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07271v1</a>
  <a href="https://arxiv.org/pdf/2505.07271.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07271v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07271v1', 'On the Robustness of Reward Models for Language Model Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiwoo Hong, Noah Lee, Eunki Kim, Guijin Son, Woojin Chung, Aman Gupta, Shao Tang, James Thorne

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: ICML 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/LinkedIn-XFACT/RM-Robustness)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ‰¹é‡å½’é›¶æ­£åˆ™åŒ–ä»¥è§£å†³å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¥–åŠ±æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `é²æ£’æ€§` `äººç±»åé¦ˆ` `æ¨¡å‹ä¼˜åŒ–` `æ‰¹é‡å½’é›¶æ­£åˆ™åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `åå¥½é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®¹æ˜“å‡ºç°è¿‡åº¦ä¼˜åŒ–ï¼Œå¯¼è‡´å¯¹æ–°è¾“å…¥çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºæ‰¹é‡å½’é›¶æ­£åˆ™åŒ–ï¼ˆBSRï¼‰ï¼Œé€šè¿‡å¼ºåˆ¶æ¯ä¸ªæ‰¹æ¬¡çš„å¥–åŠ±å’Œä¸ºé›¶æ¥è§£å†³è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨BSRçš„å¥–åŠ±æ¨¡å‹åœ¨RLHFè®­ç»ƒä¸­æ›´å¥½åœ°å¯¹é½äº†ç­–ç•¥ï¼Œå¹¶åœ¨å¤æ‚ä»»åŠ¡ä¸­æå‡äº†è¶…è¿‡5%çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Bradley-Terry (BT) æ¨¡å‹åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­å¹¿æ³›åº”ç”¨äºå¥–åŠ±å»ºæ¨¡ã€‚å°½ç®¡å…¶æœ‰æ•ˆæ€§æ˜¾è‘—ï¼Œä½†ä½¿ç”¨BTæ¨¡å‹æŸå¤±è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰å®¹æ˜“è¿‡åº¦ä¼˜åŒ–ï¼Œå¯¼è‡´å¯¹æœªè§è¾“å…¥åˆ†å¸ƒçš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚æœ¬æ–‡ç ”ç©¶äº†RMè®­ç»ƒä¸­è¿‡åº¦ä¼˜åŒ–çš„åŸå› åŠå…¶å¯¹RLHFè¿‡ç¨‹çš„å½±å“ï¼Œå¼ºè°ƒäº†RMsåœ¨æœªè§æ•°æ®ä¸­çš„åˆ†å¸ƒé²æ£’æ€§çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘ç°éšè—çŠ¶æ€èŒƒæ•°çš„è¿‡åº¦åˆ†æ•£æ˜¯è¿‡åº¦ä¼˜åŒ–çš„ä¸»è¦æ¥æºï¼Œå¹¶æå‡ºäº†æ‰¹é‡å½’é›¶æ­£åˆ™åŒ–ï¼ˆBSRï¼‰ï¼Œä»¥å¼ºåˆ¶æ¯ä¸ªæ‰¹æ¬¡çš„å¥–åŠ±å’Œä¸ºé›¶ï¼Œä»è€Œé™åˆ¶æç«¯å¹…åº¦çš„å¥–åŠ±ã€‚é€šè¿‡å››ç§è¿‡åº¦ä¼˜åŒ–åœºæ™¯è¯„ä¼°BSRçš„å½±å“ï¼Œç»“æœè¡¨æ˜BSRåœ¨æé«˜RMsçš„é²æ£’æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ€ç»ˆï¼ŒBSRåœ¨é«˜è´¨é‡æ•°æ®å’Œæ¨¡å‹ä¸Šçš„åº”ç”¨è¶…è¶Šäº†8Bè§„æ¨¡çš„æœ€å…ˆè¿›RMsï¼Œåœ¨å¤æ‚åå¥½é¢„æµ‹ä»»åŠ¡ä¸­æå‡è¶…è¿‡5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¥–åŠ±æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„è¿‡åº¦ä¼˜åŒ–é—®é¢˜ï¼Œç°æœ‰çš„BTæ¨¡å‹æŸå¤±å¯¼è‡´æ¨¡å‹å¯¹æœªè§æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ‰¹é‡å½’é›¶æ­£åˆ™åŒ–ï¼ˆBSRï¼‰ï¼Œé€šè¿‡é™åˆ¶æ¯ä¸ªæ‰¹æ¬¡çš„å¥–åŠ±å’Œä¸ºé›¶ï¼Œæ¥å‡å°‘å¥–åŠ±çš„æç«¯å¹…åº¦ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€BSRæ­£åˆ™åŒ–åº”ç”¨ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚BSRåœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­åº”ç”¨ï¼Œç¡®ä¿å¥–åŠ±çš„åˆ†å¸ƒæ›´ä¸ºå‡åŒ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šBSRæ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼Œä¸ä¼ ç»Ÿçš„BTæ¨¡å‹ç›¸æ¯”ï¼ŒBSRé€šè¿‡æ§åˆ¶å¥–åŠ±çš„åˆ†å¸ƒï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥BSRæ­£åˆ™åŒ–é¡¹ï¼Œç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡çš„å¥–åŠ±å’Œä¸ºé›¶ï¼Œæ­¤å¤–ï¼Œè°ƒæ•´éšè—çŠ¶æ€çš„èŒƒæ•°ä»¥é¿å…è¿‡åº¦åˆ†æ•£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨BSRçš„å¥–åŠ±æ¨¡å‹åœ¨å¤æ‚åå¥½é¢„æµ‹ä»»åŠ¡ä¸­æå‡è¶…è¿‡5%ï¼Œå¹¶ä¸”åœ¨8Bè§„æ¨¡çš„æ¨¡å‹ä¸­ï¼Œé€šè¿‡RLOOè®­ç»ƒï¼Œç”Ÿæˆé•¿åº¦å‡å°‘äº†40%ï¼Œèƒœç‡æé«˜äº†7%ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†é²æ£’æ€§å¯¹RLHFè®­ç»ƒçš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§ï¼Œå¯ä»¥åœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸­å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œè¿›è€Œæ¨åŠ¨RLHFæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss are prone to over-optimization, losing generalizability to unseen input distributions. In this paper, we study the cause of over-optimization in RM training and its downstream effects on the RLHF procedure, accentuating the importance of distributional robustness of RMs in unseen data. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Then, we propose batch-wise sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch, constraining the rewards with extreme magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness. Subsequently, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length by 40% while adding a 7% increase in win rate, further highlighting that robustness in RMs induces robustness in RLHF training. We release the code, data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.

