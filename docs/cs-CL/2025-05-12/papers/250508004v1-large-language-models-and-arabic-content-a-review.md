---
layout: default
title: Large Language Models and Arabic Content: A Review
---

# Large Language Models and Arabic Content: A Review

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08004" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08004v1</a>
  <a href="https://arxiv.org/pdf/2505.08004.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08004v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08004v1', 'Large Language Models and Arabic Content: A Review')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haneh Rhel, Dmitri Roussinov

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: Original language: English This paper has been submitted to the First International Conference on Artificial Intelligence and Generative AI (FICAILY 2025), and it has been accepted for presentation at FICAILY on 9-10/July 2025 and for publication in the Springer Nature. Number of pages: 16 Publication status Accepted/In press - 7 Apr 2025 https://www.gena-ai-libya2025.com/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­å†…å®¹å¤„ç†ä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `é˜¿æ‹‰ä¼¯è¯­å¤„ç†` `è‡ªç„¶è¯­è¨€å¤„ç†` `å¾®è°ƒ` `æç¤ºå·¥ç¨‹` `å¤šè¯­è¨€è¯­æ–™åº“` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é˜¿æ‹‰ä¼¯è¯­NLPä»»åŠ¡é¢ä¸´èµ„æºç¨€ç¼ºå’Œè¯­è¨€å¤æ‚æ€§ç­‰å¤šé‡æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†ã€‚
2. ç ”ç©¶é€šè¿‡ç»¼è¿°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­çš„åº”ç”¨ï¼Œæå‡ºå¾®è°ƒå’Œæç¤ºå·¥ç¨‹ç­‰æ–¹æ³•ä»¥æå‡æ€§èƒ½ã€‚
3. åŸºäºå¤šè¯­è¨€è¯­æ–™åº“çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ¨åŠ¨äº†é˜¿æ‹‰ä¼¯è¯­NLPçš„å‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è¿‡å»ä¸‰å¹´ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•å¯¹äººå·¥æ™ºèƒ½çš„å¤šä¸ªé¢†åŸŸäº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ–¹é¢ã€‚å°½ç®¡é˜¿æ‹‰ä¼¯è¯­æ˜¯27ä¸ªå›½å®¶å¹¿æ³›ä½¿ç”¨çš„è¯­è¨€ï¼Œä½†åœ¨èµ„æºã€æ•°æ®é›†å’Œå·¥å…·æ–¹é¢ä»ç„¶ç¨€ç¼ºã€‚é˜¿æ‹‰ä¼¯è¯­çš„å¤æ‚æ€§ï¼Œå¦‚ä¸°å¯Œçš„å½¢æ€å­¦å’Œå¤šæ ·çš„ä¹¦å†™æ ‡å‡†ï¼Œä½¿å¾—NLPä»»åŠ¡é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå¤šè¯­è¨€è¯­æ–™åº“é¢„è®­ç»ƒçš„LLMsåœ¨é˜¿æ‹‰ä¼¯è¯­NLPä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚æœ¬ç ”ç©¶æ¦‚è¿°äº†LLMsåœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†æ—©æœŸçš„é˜¿æ‹‰ä¼¯è¯­è¨€æ¨¡å‹åŠå…¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶æ¢è®¨äº†å¾®è°ƒå’Œæç¤ºå·¥ç¨‹ç­‰æŠ€æœ¯å¦‚ä½•æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„èµ„æºä¸è¶³å’Œè¯­è¨€å¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é˜¿æ‹‰ä¼¯è¯­çš„ä¸°å¯Œå½¢æ€å’Œå¤šæ ·åŒ–ä¹¦å†™æ ‡å‡†æ—¶å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡ç»¼è¿°å’Œåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šè¯­è¨€è¯­æ–™åº“ä¸Šçš„æˆåŠŸï¼Œæå‡ºå¾®è°ƒå’Œæç¤ºå·¥ç¨‹ä½œä¸ºæå‡æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ‰‹æ®µã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹æ—©æœŸé˜¿æ‹‰ä¼¯è¯­è¨€æ¨¡å‹çš„å›é¡¾ã€å¯¹å¤šæ ·åŒ–ä»»åŠ¡çš„åˆ†æï¼Œä»¥åŠå¯¹å¾®è°ƒå’Œæç¤ºå·¥ç¨‹æŠ€æœ¯çš„æ¢è®¨ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ¨¡å‹è®­ç»ƒã€æ€§èƒ½è¯„ä¼°å’Œåº”ç”¨æ¡ˆä¾‹åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°æ€»ç»“äº†é˜¿æ‹‰ä¼¯è¯­NLPä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡å¾®è°ƒå’Œæç¤ºå·¥ç¨‹æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†å¤šè¯­è¨€è¯­æ–™åº“è¿›è¡Œé¢„è®­ç»ƒï¼Œç»“åˆç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”é˜¿æ‹‰ä¼¯è¯­çš„å¤æ‚æ€§ï¼ŒåŒæ—¶åœ¨å¾®è°ƒé˜¶æ®µå¼•å…¥äº†é’ˆå¯¹æ€§çš„æ•°æ®é›†å’Œå‚æ•°è®¾ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå¤šè¯­è¨€è¯­æ–™åº“çš„é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­NLPä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦å¯è¾¾20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ç­‰ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚é€šè¿‡æå‡é˜¿æ‹‰ä¼¯è¯­NLPçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿä¿ƒè¿›é˜¿æ‹‰ä¼¯åœ°åŒºçš„æ•°å­—åŒ–è½¬å‹å’Œä¿¡æ¯è·å–ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Over the past three years, the rapid advancement of Large Language Models (LLMs) has had a profound impact on multiple areas of Artificial Intelligence (AI), particularly in Natural Language Processing (NLP) across diverse languages, including Arabic. Although Arabic is considered one of the most widely spoken languages across 27 countries in the Arabic world and used as a second language in some other non-Arabic countries as well, there is still a scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face various challenges due to the complexities of the Arabic language, including its rich morphology, intricate structure, and diverse writing standards, among other factors. Researchers have been actively addressing these challenges, demonstrating that pre-trained Large Language Models (LLMs) trained on multilingual corpora achieve significant success in various Arabic NLP tasks. This study provides an overview of using large language models (LLMs) for the Arabic language, highlighting early pre-trained Arabic Language models across various NLP applications and their ability to handle diverse Arabic content tasks and dialects. It also provides an overview of how techniques like finetuning and prompt engineering can enhance the performance of these models. Additionally, the study summarizes common Arabic benchmarks and datasets while presenting our observations on the persistent upward trend in the adoption of LLMs.

