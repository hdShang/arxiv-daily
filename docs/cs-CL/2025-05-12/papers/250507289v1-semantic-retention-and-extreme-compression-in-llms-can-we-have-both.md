---
layout: default
title: Semantic Retention and Extreme Compression in LLMs: Can We Have Both?
---

# Semantic Retention and Extreme Compression in LLMs: Can We Have Both?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07289" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07289v1</a>
  <a href="https://arxiv.org/pdf/2505.07289.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07289v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07289v1', 'Semantic Retention and Extreme Compression in LLMs: Can We Have Both?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Stanislas Laborde, Martin Cousseau, Antoun Yaacoub, Lionel Prevost

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: Accepted for publication in the Proceedings of the 2025 International Joint Conference on Neural Networks (IJCNN); this arXiv version includes an appendix with 6 result tables; 10 pages, 15 figures, 7 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè”åˆå‰ªæä¸é‡åŒ–ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹å‹ç¼©æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `å‰ªæ` `é‡åŒ–` `è¯­ä¹‰ä¿ç•™`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‰ªæå’Œé‡åŒ–æ–¹æ³•åœ¨æ¨¡å‹å‹ç¼©ä¸­å„æœ‰ä¼˜åŠ¿ï¼Œä½†è”åˆåº”ç”¨çš„æ½œåŠ›å°šæœªè¢«å……åˆ†æŒ–æ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡SrCrï¼Œæ—¨åœ¨é‡åŒ–æ¨¡å‹å‹ç¼©ä¸è¯­ä¹‰ä¿ç•™ä¹‹é—´çš„æƒè¡¡ï¼Œä¼˜åŒ–å‰ªæä¸é‡åŒ–çš„ç»„åˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨èçš„å‰ªæä¸é‡åŒ–ç»„åˆåœ¨æ€§èƒ½ä¸Šæ¯”å•ç‹¬é‡åŒ–æ¨¡å‹å¹³å‡æå‡20%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨çš„å¿«é€Ÿå¢é•¿ï¼Œæ¨¡å‹å‹ç¼©æŠ€æœ¯çš„éœ€æ±‚æ—¥ç›Šè¿«åˆ‡ï¼Œä»¥é™ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚è™½ç„¶å‰ªæå’Œé‡åŒ–æ–¹æ³•å·²æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬çš„è”åˆåº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ç ”ç©¶äº†è”åˆå‹ç¼©ï¼Œæå‡ºé€šè¿‡æˆ˜ç•¥æ€§åœ°ç»“åˆå‰ªæå’Œé‡åŒ–æ¥å®ç°æ¯”å•ä¸€æ–¹æ³•æ›´ä¼˜çš„æ€§èƒ½ä¸å‹ç¼©æ¯”ã€‚ä¸ºäº†è§£å†³è¯„ä¼°LLMæ€§èƒ½çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†è¯­ä¹‰ä¿ç•™å‹ç¼©ç‡ï¼ˆSrCrï¼‰è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œé‡åŒ–æ¨¡å‹å‹ç¼©ä¸è¯­ä¹‰ä¿ç•™ä¹‹é—´çš„æƒè¡¡ï¼Œä¿ƒè¿›å‰ªæ-é‡åŒ–é…ç½®çš„ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨èçš„ç»„åˆåœ¨ç›¸åŒç†è®ºå‹ç¼©ç‡ä¸‹ï¼Œå¹³å‡æ€§èƒ½æå‡20%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„æ˜¯å¦‚ä½•åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­æœ‰æ•ˆåœ°è¿›è¡Œæ¨¡å‹å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚ç°æœ‰æ–¹æ³•å¦‚å‰ªæå’Œé‡åŒ–å„è‡ªå­˜åœ¨å±€é™æ€§ï¼Œæœªèƒ½å……åˆ†å‘æŒ¥è”åˆåº”ç”¨çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“åˆå‰ªæå’Œé‡åŒ–æŠ€æœ¯ï¼Œåˆ©ç”¨ä¸¤è€…çš„äº’è¡¥ä¼˜åŠ¿æ¥æå‡æ¨¡å‹çš„å‹ç¼©æ€§èƒ½å’Œè¯­ä¹‰ä¿ç•™èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å®ç°æ›´ä¼˜çš„æ€§èƒ½ä¸å‹ç¼©æ¯”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå‰ªææ¨¡å—å’Œé‡åŒ–æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å‰ªæå‡å°‘æ¨¡å‹çš„å‚æ•°é‡ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œé‡åŒ–ï¼Œä»¥è¿›ä¸€æ­¥é™ä½æ¨¡å‹çš„å­˜å‚¨éœ€æ±‚å’Œè®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯å¼•å…¥äº†è¯­ä¹‰ä¿ç•™å‹ç¼©ç‡ï¼ˆSrCrï¼‰è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿé‡åŒ–æ¨¡å‹å‹ç¼©ä¸è¯­ä¹‰ä¿ç•™ä¹‹é—´çš„æƒè¡¡ï¼Œä»è€Œä¼˜åŒ–å‰ªæä¸é‡åŒ–çš„é…ç½®ã€‚è¿™ä¸€æŒ‡æ ‡ä¸ºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒå‹ç¼©ç­–ç•¥æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå‰ªææ¯”ä¾‹å’Œé‡åŒ–ä½æ•°æ˜¯å…³é”®è®¾è®¡å› ç´ ã€‚æŸå¤±å‡½æ•°çš„é€‰æ‹©ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€ç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­å°½é‡ä¿ç•™æ¨¡å‹çš„è¯­ä¹‰ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨èçš„å‰ªæä¸é‡åŒ–ç»„åˆåœ¨ç›¸åŒç†è®ºå‹ç¼©ç‡ä¸‹ï¼Œå¹³å‡æ€§èƒ½æå‡20%ï¼Œæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨é‡åŒ–çš„æ¨¡å‹ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†è”åˆå‹ç¼©ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„å‹ç¼©æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ›´å¥½åœ°éƒ¨ç½²è¿™äº›æ¨¡å‹ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ã€‚è¿™å°†æ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¿»è¯‘ç­‰åº”ç”¨çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.

