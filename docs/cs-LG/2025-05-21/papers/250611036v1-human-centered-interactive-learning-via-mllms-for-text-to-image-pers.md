---
layout: default
title: Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification
---

# Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11036" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11036v1</a>
  <a href="https://arxiv.org/pdf/2506.11036.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11036v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11036v1', 'Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Qin, Chao Chen, Zhihang Fu, Dezhong Peng, Xi Peng, Peng Hu

**åˆ†ç±»**: cs.LG, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäººæœ¬äº¤äº’å­¦ä¹ æ¡†æ¶ä»¥æå‡æ–‡æœ¬åˆ°å›¾åƒçš„è¡Œäººé‡è¯†åˆ«æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¡Œäººé‡è¯†åˆ«` `è·¨æ¨¡æ€å­¦ä¹ ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `äººæœ¬äº¤äº’` `æ•°æ®å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒè¡Œäººé‡è¯†åˆ«æ–¹æ³•åœ¨å¤„ç†å¤æ‚å€™é€‰å›¾åƒæ—¶å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œä¸»è¦å—é™äºç½‘ç»œæ¶æ„å’Œæ•°æ®è´¨é‡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼è·¨æ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼ˆICLï¼‰ï¼Œé€šè¿‡äººæœ¬äº¤äº’å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æå‡æ–‡æœ¬æŸ¥è¯¢çš„å¯åŒºåˆ†æ€§ã€‚
3. åœ¨CUHK-PEDESã€ICFG-PEDESã€RSTPReidå’ŒUFine6926ç­‰å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è¡Œäººé‡è¯†åˆ«çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è·¨æ¨¡æ€åµŒå…¥æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„è¡Œäººé‡è¯†åˆ«ï¼ˆTIReIDï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨åŒºåˆ†å…·æœ‰æŒ‘æˆ˜æ€§çš„å€™é€‰å›¾åƒæ—¶ä»é¢ä¸´ç½‘ç»œæ¶æ„å’Œæ•°æ®è´¨é‡ç­‰å†…åœ¨é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼è·¨æ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼ˆICLï¼‰ï¼Œé€šè¿‡äººæœ¬äº¤äº’å¢å¼ºæ–‡æœ¬æŸ¥è¯¢çš„å¯åŒºåˆ†æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå³æ’å³ç”¨çš„æµ‹è¯•æ—¶äººæœ¬äº¤äº’æ¨¡å—ï¼ˆTHIï¼‰ï¼Œè¯¥æ¨¡å—ä¸“æ³¨äºäººç±»ç‰¹å¾çš„è§†è§‰é—®ç­”ï¼Œä¿ƒè¿›ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¤šè½®äº¤äº’ï¼Œä»¥å¯¹é½æŸ¥è¯¢æ„å›¾ä¸æ½œåœ¨ç›®æ ‡å›¾åƒã€‚THIæ ¹æ®MLLMçš„åé¦ˆä¼˜åŒ–ç”¨æˆ·æŸ¥è¯¢ï¼Œä»è€Œæé«˜æ’åå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ä½è´¨é‡è®­ç»ƒæ–‡æœ¬çš„é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ•°æ®é‡ç»„å¢å¼ºï¼ˆRDAï¼‰ç­–ç•¥ï¼Œé€šè¿‡ä¸°å¯Œã€åˆ†è§£å’Œé‡ç»„äººç‰©æè¿°æ¥å¢å¼ºæŸ¥è¯¢çš„å¯åŒºåˆ†æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å››ä¸ªTIReIDåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒè¡Œäººé‡è¯†åˆ«ï¼ˆTIReIDï¼‰ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨åŒºåˆ†å¤æ‚å€™é€‰å›¾åƒæ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯ç”±äºç½‘ç»œæ¶æ„å’Œæ•°æ®è´¨é‡å¯¼è‡´çš„è¯†åˆ«å‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„äº¤äº’å¼è·¨æ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼ˆICLï¼‰é€šè¿‡äººæœ¬äº¤äº’æ¥å¢å¼ºæ–‡æœ¬æŸ¥è¯¢çš„å¯åŒºåˆ†æ€§ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œå¤šè½®äº¤äº’ï¼Œä»¥æ›´å¥½åœ°å¯¹é½æŸ¥è¯¢æ„å›¾ä¸ç›®æ ‡å›¾åƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬äººæœ¬äº¤äº’æ¨¡å—ï¼ˆTHIï¼‰å’Œæ•°æ®é‡ç»„å¢å¼ºï¼ˆRDAï¼‰ç­–ç•¥ã€‚THIæ¨¡å—ä¸“æ³¨äºè§†è§‰é—®ç­”ï¼ŒRDAç­–ç•¥åˆ™é€šè¿‡ä¸°å¯Œå’Œé‡ç»„æè¿°æ¥æå‡è®­ç»ƒæ–‡æœ¬è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†THIæ¨¡å—ï¼Œé€šè¿‡ä¸MLLMçš„äº¤äº’ä¼˜åŒ–ç”¨æˆ·æŸ¥è¯¢ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†å›¾åƒåŒ¹é…çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„é™æ€æŸ¥è¯¢å¤„ç†æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨THIæ¨¡å—ä¸­ï¼Œè®¾è®¡äº†å¤šè½®äº¤äº’æœºåˆ¶ä»¥ä¼˜åŒ–æŸ¥è¯¢ï¼ŒåŒæ—¶RDAç­–ç•¥é€šè¿‡ä¿¡æ¯ä¸°å¯Œå’Œå¤šæ ·æ€§å¢å¼ºæ¥æå‡è®­ç»ƒæ–‡æœ¬çš„è´¨é‡ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä½è´¨é‡æ•°æ®ä¸‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨CUHK-PEDESã€ICFG-PEDESã€RSTPReidå’ŒUFine6926ç­‰å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨è¡Œäººé‡è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€å®‰é˜²ç³»ç»Ÿå’Œäººæµç®¡ç†ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è¡Œäººé‡è¯†åˆ«çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–è·¨æ¨¡æ€ä»»åŠ¡ï¼Œå¦‚å›¾åƒæ£€ç´¢å’Œè§†é¢‘åˆ†æï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œå½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite remarkable advancements in text-to-image person re-identification (TIReID) facilitated by the breakthrough of cross-modal embedding models, existing methods often struggle to distinguish challenging candidate images due to intrinsic limitations, such as network architecture and data quality. To address these issues, we propose an Interactive Cross-modal Learning framework (ICL), which leverages human-centered interaction to enhance the discriminability of text queries through external multimodal knowledge. To achieve this, we propose a plug-and-play Test-time Humane-centered Interaction (THI) module, which performs visual question answering focused on human characteristics, facilitating multi-round interactions with a multimodal large language model (MLLM) to align query intent with latent target images. Specifically, THI refines user queries based on the MLLM responses to reduce the gap to the best-matching images, thereby boosting ranking accuracy. Additionally, to address the limitation of low-quality training texts, we introduce a novel Reorganization Data Augmentation (RDA) strategy based on information enrichment and diversity enhancement to enhance query discriminability by enriching, decomposing, and reorganizing person descriptions. Extensive experiments on four TIReID benchmarks, i.e., CUHK-PEDES, ICFG-PEDES, RSTPReid, and UFine6926, demonstrate that our method achieves remarkable performance with substantial improvement.

