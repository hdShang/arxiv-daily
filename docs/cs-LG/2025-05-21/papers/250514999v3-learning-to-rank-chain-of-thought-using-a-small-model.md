---
layout: default
title: "Learning to Rank Chain-of-Thought: Using a Small Model"
---

# Learning to Rank Chain-of-Thought: Using a Small Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14999" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14999v3</a>
  <a href="https://arxiv.org/pdf/2505.14999.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14999v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14999v3', 'Learning to Rank Chain-of-Thought: Using a Small Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eric Hanchen Jiang, Haozheng Luo, Shengyuan Pang, Xiaomin Li, Zhenting Qi, Hengli Li, Cheng-Fu Yang, Zongyu Lin, Xinfeng Li, Hao Xu, Kai-Wei Chang, Ying Nian Wu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºèƒ½é‡ç»“æœå¥–åŠ±æ¨¡å‹ä»¥æé«˜æ•°å­¦æ¨ç†çš„å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `èƒ½é‡ç»“æœå¥–åŠ±æ¨¡å‹` `é“¾å¼æ€ç»´` `æ•°å­¦æ¨ç†` `è½»é‡çº§éªŒè¯å™¨` `é«˜æ•ˆæ¨ç†` `æ¨¡å‹æ³›åŒ–` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä¸”éªŒè¯æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„EORMé€šè¿‡èƒ½é‡æ¡†æ¶å¯¹é“¾å¼æ€ç»´è§£å†³æ–¹æ¡ˆè¿›è¡Œæ’åºï¼Œåˆ©ç”¨ç®€å•çš„ç»“æœæ ‡ç­¾è¿›è¡Œå­¦ä¹ ï¼Œé¿å…äº†æ˜‚è´µçš„æ ‡æ³¨ã€‚
3. EORMåœ¨GSM8kå’ŒMATHæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†90.7%å’Œ63.7%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶å…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯é çš„æ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç°æœ‰çš„éªŒè¯æ–¹æ³•é€šå¸¸è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆã€è½»é‡çº§çš„åéªŒéªŒè¯å™¨â€”â€”èƒ½é‡ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆEORMï¼‰ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚EORMé‡‡ç”¨åŸºäºèƒ½é‡çš„æ¡†æ¶å¯¹é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è§£å†³æ–¹æ¡ˆè¿›è¡Œæ’åºï¼Œä»…ä½¿ç”¨ç®€å•çš„ç»“æœæ ‡ç­¾æ¥åŒºåˆ†æ­£ç¡®ä¸é”™è¯¯çš„æ¨ç†ï¼Œä»è€Œæ¶ˆé™¤æ˜‚è´µçš„æ ‡æ³¨éœ€æ±‚ã€‚EORMå‚æ•°ä»…ä¸º5500ä¸‡ï¼Œæ¯”å…¸å‹å¥–åŠ±æ¨¡å‹å°127å€ï¼Œæ˜¾è‘—æé«˜äº†Llama 3 8Båœ¨GSM8kå’ŒMATHä¸Šçš„å‡†ç¡®ç‡ï¼Œåˆ†åˆ«è¾¾åˆ°äº†90.7%å’Œ63.7%ã€‚è¯¥æ¨¡å‹é€šè¿‡æœ‰æ•ˆé€‰æ‹©å€™é€‰æ¨ç†è·¯å¾„ï¼Œèƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šèµ„æºå¯†é›†å‹çš„æœ€ä½³Né‡‡æ ·æŠ€æœ¯çš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒEORMå¯¹åˆ†å¸ƒå¤–é—®é¢˜å’Œæœªè§æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¡¨æ˜å…¶å­¦ä¹ äº†æœ‰æ•ˆæ¨ç†çš„åŸºæœ¬åŸåˆ™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­çš„å¯é æ€§é—®é¢˜ï¼Œç°æœ‰éªŒè¯æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥å¹¿æ³›åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEORMé€šè¿‡èƒ½é‡æ¡†æ¶å¯¹æ¨ç†ç»“æœè¿›è¡Œæ’åºï¼Œä»…ä¾èµ–ç®€å•çš„ç»“æœæ ‡ç­¾ï¼Œé¿å…äº†å¤æ‚çš„æ ‡æ³¨è¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEORMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥é“¾å¼æ€ç»´è§£å†³æ–¹æ¡ˆï¼Œç»è¿‡èƒ½é‡è®¡ç®—æ¨¡å—è¿›è¡Œæ’åºï¼Œæœ€ç»ˆè¾“å‡ºæœ€ä½³æ¨ç†è·¯å¾„ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬èƒ½é‡è®¡ç®—ã€æ’åºå’Œç»“æœè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šEORMçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶è½»é‡çº§è®¾è®¡å’Œé«˜æ•ˆçš„èƒ½é‡æ’åºæœºåˆ¶ï¼Œä½¿å…¶åœ¨å‡†ç¡®æ€§ä¸Šèƒ½å¤Ÿä¸èµ„æºå¯†é›†å‹çš„æœ€ä½³Né‡‡æ ·æŠ€æœ¯ç›¸åª²ç¾ã€‚

**å…³é”®è®¾è®¡**ï¼šEORMä»…ä½¿ç”¨5500ä¸‡å‚æ•°ï¼Œé‡‡ç”¨ç®€å•çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿åœ¨ä¿æŒé«˜æ•ˆæ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ¨ç†çš„åŸºæœ¬åŸåˆ™ã€‚é€šè¿‡ä¼˜åŒ–å‚æ•°è®¾ç½®ï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EORMåœ¨GSM8kå’ŒMATHæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†90.7%å’Œ63.7%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„éªŒè¯æ–¹æ³•ã€‚ä¸èµ„æºå¯†é›†å‹çš„æœ€ä½³Né‡‡æ ·æŠ€æœ¯ç›¸æ¯”ï¼ŒEORMåœ¨å‡†ç¡®æ€§ä¸ŠåŒ¹é…æˆ–è¶…è¶Šï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›å’Œè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EORMçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨æ•™è‚²ã€é‡‘èå’Œç§‘å­¦è®¡ç®—ç­‰éœ€è¦é«˜å¯é æ€§æ¨ç†çš„åœºæ™¯ä¸­ã€‚å…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ä½¿å…¶æˆä¸ºéƒ¨ç½²æ›´å¯é çš„è¯­è¨€æ¨¡å‹çš„å®ç”¨å·¥å…·ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„ç°å®åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) struggle with reliable mathematical reasoning, and current verification methods are often computationally expensive. This paper introduces the Energy Outcome Reward Model (EORM), a highly efficient, lightweight post-hoc verifier designed to address this challenge. EORM uses an energy-based framework to rank Chain-of-Thought (CoT) solutions, learning to distinguish correct from incorrect reasoning using only simple outcome labels, thus eliminating the need for expensive annotations. With only 55M parameters, over 127 times smaller than typical reward models, EORM boosts the accuracy of Llama 3 8B to 90.7\% on GSM8k and 63.7\% on MATH. This performance is achieved by efficiently selecting the optimal reasoning path from a pool of candidates, allowing it to match or exceed the accuracy of far more resource-intensive Best-of-N sampling techniques. Crucially, our experiments show that EORM generalizes effectively to out-of-distribution problems and unseen models, indicating it learns fundamental principles of valid reasoning. This robustness, combined with its efficiency, establishes EORM as a practical tool for deploying more dependable LLMs in complex, real-world applications.

