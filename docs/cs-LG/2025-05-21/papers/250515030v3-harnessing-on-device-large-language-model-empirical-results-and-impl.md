---
layout: default
title: Harnessing On-Device Large Language Model: Empirical Results and Implications for AI PC
---

# Harnessing On-Device Large Language Model: Empirical Results and Implications for AI PC

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.15030" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.15030v3</a>
  <a href="https://arxiv.org/pdf/2505.15030.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.15030v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.15030v3', 'Harnessing On-Device Large Language Model: Empirical Results and Implications for AI PC')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingyu Song, Peiyu Liao, Wenqian Zhao, Yiwen Wang, Shoubo Hu, Hui-Ling Zhen, Ning Jiang, Mingxuan Yuan

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21 (æ›´æ–°: 2025-06-07)

**å¤‡æ³¨**: 18 pages, 14 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/simmonssong/LLMOnDevice)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç³»ç»ŸåŒ–æ–¹æ³•è¯„ä¼°è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¾¹ç¼˜è®¡ç®—` `é‡åŒ–æ–¹æ³•` `æ€§èƒ½è¯„ä¼°` `éšç§ä¿æŠ¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´æ€§èƒ½é™åˆ¶ï¼Œä¸»è¦ç”±äºæ¨¡å‹å®¹é‡å’Œå‹ç¼©æŠ€æœ¯çš„ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•è®ºï¼Œç»¼åˆè€ƒè™‘æ¨¡å‹èƒ½åŠ›ã€å¼€å‘æ•ˆç‡å’Œç³»ç»Ÿèµ„æºï¼Œä»¥è¯„ä¼°å’Œä¼˜åŒ–è¾¹ç¼˜è®¾å¤‡ä¸Šçš„LLMsã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç³»ç»Ÿçº§æŒ‡æ ‡ä¸æœ‰æ•ˆæ¯”ç‰¹æ¯æƒé‡è¿‘ä¼¼çº¿æ€§å…³ç³»ï¼Œä½BPWé‡åŒ–åœ¨å‡†ç¡®åº¦æŸå¤±å°çš„æƒ…å†µä¸‹å®ç°æ˜¾è‘—å†…å­˜èŠ‚çœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ—¥ç›Šéƒ¨ç½²ï¼Œéšç§ä¿æŠ¤çš„ä¼˜åŠ¿æ„ˆåŠ æ˜æ˜¾ã€‚ç„¶è€Œï¼Œè¿™äº›è®¾å¤‡ä¸Šçš„LLMsç”±äºæ¨¡å‹å®¹é‡å—é™å’Œå‹ç¼©æŠ€æœ¯çš„å¿…è¦æ€§ï¼Œé¢ä¸´æ€§èƒ½é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•è®ºï¼Œæ¶µç›–æ¨¡å‹èƒ½åŠ›ã€å¼€å‘æ•ˆç‡å’Œç³»ç»Ÿèµ„æºï¼Œä»¥è¯„ä¼°è¾¹ç¼˜è®¾å¤‡ä¸Šçš„LLMsã€‚é€šè¿‡å¯¹0.5Bè‡³14Bå‚æ•°æ¨¡å‹åŠä¸ƒç§åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ–¹æ³•çš„ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬è·å¾—äº†è‹¥å¹²å…³é”®è§è§£ï¼ŒåŒ…æ‹¬ç³»ç»Ÿçº§æŒ‡æ ‡ä¸æœ‰æ•ˆæ¯”ç‰¹æ¯æƒé‡ï¼ˆBPWï¼‰è¿‘ä¼¼çº¿æ€§æ‰©å±•ã€çº¦3.5æœ‰æ•ˆBPWçš„å®ç”¨é˜ˆå€¼ï¼Œä»¥åŠä½BPWé‡åŒ–å¸¦æ¥çš„è¾¹é™…å‡†ç¡®åº¦æŸå¤±ä¸æ˜¾è‘—å†…å­˜èŠ‚çœã€‚è¿™äº›å‘ç°ä¸ºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸ŠLLMsçš„é«˜æ•ˆéƒ¨ç½²å’Œä¼˜åŒ–é…ç½®æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¾¹ç¼˜è®¾å¤‡ä¸Šå¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹å®¹é‡å’Œå‹ç¼©æŠ€æœ¯ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´æ— æ³•å……åˆ†å‘æŒ¥LLMsçš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§ç³»ç»ŸåŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œæ¶µç›–æ¨¡å‹èƒ½åŠ›ã€å¼€å‘æ•ˆç‡å’Œç³»ç»Ÿèµ„æºï¼Œæ—¨åœ¨ä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šçš„LLMsæä¾›æœ‰æ•ˆçš„é…ç½®å’Œä¼˜åŒ–æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹è¯„ä¼°ã€é‡åŒ–æ–¹æ³•é€‰æ‹©å’Œç³»ç»Ÿèµ„æºåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆè¯„ä¼°ä¸åŒå‚æ•°è§„æ¨¡æ¨¡å‹çš„æ€§èƒ½ï¼Œç„¶åé€‰æ‹©åˆé€‚çš„åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œæœ€ååˆ†æç³»ç»Ÿèµ„æºçš„ä½¿ç”¨æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå‘ç°äº†æœ‰æ•ˆæ¯”ç‰¹æ¯æƒé‡ï¼ˆBPWï¼‰ä¸ç³»ç»Ÿçº§æŒ‡æ ‡ä¹‹é—´çš„è¿‘çº¿æ€§å…³ç³»ï¼Œå¹¶æå‡ºäº†çº¦3.5æœ‰æ•ˆBPWçš„å®ç”¨é˜ˆå€¼ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç»éªŒæ³•åˆ™æœ‰æ˜¾è‘—åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸ƒç§åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨ä½BPWé‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç¡®ä¿åœ¨å†…å­˜èŠ‚çœçš„åŒæ—¶ï¼Œå°½é‡å‡å°‘å‡†ç¡®åº¦æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç³»ç»Ÿçº§æŒ‡æ ‡ä¸æœ‰æ•ˆæ¯”ç‰¹æ¯æƒé‡ï¼ˆBPWï¼‰ä¹‹é—´å­˜åœ¨è¿‘çº¿æ€§å…³ç³»ï¼Œä¸”åœ¨çº¦3.5æœ‰æ•ˆBPWçš„é˜ˆå€¼ä¸‹ï¼Œå¤§æ¨¡å‹åœ¨ä½æ¯”ç‰¹é‡åŒ–ä¸‹çš„è¡¨ç°ä¼˜äºå°æ¨¡å‹ã€‚ä½BPWé‡åŒ–æ–¹æ³•å®ç°äº†è¾¹é™…å‡†ç¡®åº¦æŸå¤±çš„åŒæ—¶ï¼Œæ˜¾è‘—èŠ‚çœäº†å†…å­˜ï¼Œæä¾›äº†æœ‰æ•ˆçš„èµ„æºåˆ©ç”¨æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€ç‰©è”ç½‘è®¾å¤‡å’Œå…¶ä»–èµ„æºå—é™çš„è¾¹ç¼˜è®¡ç®—ç¯å¢ƒã€‚é€šè¿‡ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„é…ç½®ï¼Œå¯ä»¥åœ¨ä¿è¯éšç§çš„å‰æä¸‹ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œåº”ç”¨æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The increasing deployment of Large Language Models (LLMs) on edge devices, driven by model advancements and hardware improvements, offers significant privacy benefits. However, these on-device LLMs inherently face performance limitations due to reduced model capacity and necessary compression techniques. To address this, we introduce a systematic methodology -- encompassing model capability, development efficiency, and system resources -- for evaluating on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to 14B parameters and seven post-training quantization (PTQ) methods on commodity laptops, yields several critical insights: 1) System-level metrics exhibit near-linear scaling with effective bits-per-weight (BPW). 2) A practical threshold exists around $\sim$3.5 effective BPW, larger models subjected to low-bit quantization consistently outperform smaller models utilizing higher bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but significant memory savings. 4) Determined by low-level implementation specifics power consumption on CPU, where computation-intensive operations spend more power than memory-intensive ones. These findings offer crucial insights and practical guidelines for the efficient deployment and optimized configuration of LLMs on resource-constrained edge devices. Our codebase is available at https://github.com/simmonssong/LLMOnDevice.

