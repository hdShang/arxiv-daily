---
layout: default
title: Why and When Deep is Better than Shallow: An Implementation-Agnostic State-Transition View of Depth Supremacy
---

# Why and When Deep is Better than Shallow: An Implementation-Agnostic State-Transition View of Depth Supremacy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.15064" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.15064v3</a>
  <a href="https://arxiv.org/pdf/2505.15064.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.15064v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.15064v3', 'Why and When Deep is Better than Shallow: An Implementation-Agnostic State-Transition View of Depth Supremacy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda

**åˆ†ç±»**: cs.LG, math.DS, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21 (æ›´æ–°: 2025-11-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ·±åº¦ä¼˜äºæµ…å±‚çš„ç†è®ºæ¡†æ¶ä»¥è§£å†³æ¨¡å‹æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ·±åº¦å­¦ä¹ ` `åå·®-æ–¹å·®æƒè¡¡` `çŠ¶æ€è½¬ç§»åŠç¾¤` `æ³›åŒ–èƒ½åŠ›` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œæµ…å±‚ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›å¾€å¾€ä¸è¶³ï¼Œéš¾ä»¥æ•æ‰æ·±å±‚æ¬¡çš„ç‰¹å¾ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æŠ½è±¡çš„çŠ¶æ€è½¬ç§»åŠç¾¤æ¡†æ¶ï¼Œåˆ†ç¦»å®ç°ä¸çŠ¶æ€è½¬ç§»ï¼Œæ­ç¤ºæ·±åº¦æ¨¡å‹çš„ä¼˜åŠ¿ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ·±åº¦æ¨¡å‹åœ¨ç‰¹å®šæ¡ä»¶ä¸‹å…·æœ‰æ›´ä½çš„æ³›åŒ–è¯¯å·®ï¼Œå°¤å…¶æ˜¯åœ¨ELæ¨¡å¼ä¸‹è¡¨ç°æœ€ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†æ·±åº¦æ¨¡å‹ä¸ºä½•åŠä½•æ—¶ä¼˜äºæµ…å±‚æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§ä¸ç½‘ç»œå®ç°æ— å…³çš„çŠ¶æ€è½¬ç§»æ¡†æ¶ã€‚é€šè¿‡å°†æ·±åº¦æ¨¡å‹è§†ä¸ºä½œç”¨äºä¸€èˆ¬åº¦é‡ç©ºé—´çš„æŠ½è±¡çŠ¶æ€è½¬ç§»åŠç¾¤ï¼Œä½œè€…è¯æ˜äº†åå·®-æ–¹å·®åˆ†è§£å®šç†ï¼Œæ­ç¤ºäº†æ–¹å·®ä¸æ·±åº¦çš„å…³ç³»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œæ·±åº¦æ¨¡å‹çš„æ–¹å·®å¯ä»¥å¤šé¡¹å¼æˆ–å¯¹æ•°å¢é•¿ï¼Œè¿›è€Œè¯†åˆ«å‡ºå››ç§å…¸å‹çš„åå·®-æ–¹å·®æƒè¡¡æ¨¡å¼ï¼Œå¹¶ç»™å‡ºäº†æœ€ä¼˜æ·±åº¦çš„æ˜ç¡®è¡¨è¾¾ï¼Œå°¤å…¶é€‚ç”¨äºè¿­ä»£æˆ–åˆ†å±‚æ¦‚å¿µç±»çš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦æ¨¡å‹ä¸æµ…å±‚æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„å·®å¼‚ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è§£é‡Šæ·±åº¦æ¨¡å‹çš„ä¼˜åŠ¿åŠå…¶é€‚ç”¨æ¡ä»¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ·±åº¦æ¨¡å‹è§†ä¸ºæŠ½è±¡çŠ¶æ€è½¬ç§»åŠç¾¤ï¼Œè®ºæ–‡åˆ†ç¦»äº†æ¨¡å‹å®ç°ä¸çŠ¶æ€è½¬ç§»çš„å…³ç³»ï¼Œæä¾›äº†ä¸€ç§ç†è®ºæ¡†æ¶æ¥åˆ†ææ·±åº¦çš„ä¼˜åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åå·®-æ–¹å·®åˆ†è§£ã€çŠ¶æ€è½¬ç§»åŠç¾¤çš„åº¦é‡ç†µåˆ†æï¼Œä»¥åŠå¯¹ä¸åŒåå·®-æ–¹å·®æƒè¡¡æ¨¡å¼çš„è¯†åˆ«ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç†è®ºè¯æ˜ã€æ¡ä»¶åˆ†æå’Œæœ€ä¼˜æ·±åº¦çš„æ¨å¯¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸å®ç°æ— å…³çš„æ·±åº¦æ¨¡å‹åˆ†ææ¡†æ¶ï¼Œè¯æ˜äº†æ–¹å·®ä¸æ·±åº¦çš„å…³ç³»ï¼Œå¹¶è¯†åˆ«å‡ºå››ç§åå·®-æ–¹å·®æƒè¡¡æ¨¡å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­è®¾å®šäº†æ·±åº¦ç½‘ç»œçš„æŠ½è±¡æ·±åº¦kï¼Œå¹¶é€šè¿‡ç†è®ºæ¨å¯¼æ˜ç¡®äº†åœ¨ä¸åŒåå·®-æ–¹å·®æ¨¡å¼ä¸‹çš„æœ€ä¼˜æ·±åº¦k*ï¼Œä¸ºæ¨¡å‹è®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ELæ¨¡å¼ä¸‹ï¼Œæ·±åº¦æ¨¡å‹çš„æ³›åŒ–è¯¯å·®æ˜¾è‘—ä½äºæµ…å±‚æ¨¡å‹ï¼Œå…·ä½“è¡¨ç°ä¸ºåå·®æŒ‡æ•°è¡°å‡ä¸æ–¹å·®å¯¹æ•°å¢é•¿çš„ç»“åˆã€‚è¿™ä¸€å‘ç°ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®¾è®¡æä¾›äº†ç†è®ºæ”¯æŒï¼Œå¼ºè°ƒäº†æ·±åº¦ç½‘ç»œåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®¾è®¡ä¸ä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡å¦‚ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ã€æ‰©æ•£æ¨¡å‹å’Œé“¾å¼æ¨ç†ç­‰æ–¹é¢ã€‚é€šè¿‡æ˜ç¡®æ·±åº¦æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œç ”ç©¶å¯ä»¥æŒ‡å¯¼å®é™…åº”ç”¨ä¸­çš„æ¨¡å‹é€‰æ‹©ä¸å‚æ•°è°ƒä¼˜ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Why and when is deep better than shallow? We answer this question in a framework that is agnostic to network implementation. We formulate a deep model as an abstract state-transition semigroup acting on a general metric space, and separate the implementation (e.g., ReLU nets, transformers, and chain-of-thought) from the abstract state transition. We prove a bias-variance decomposition in which the variance depends only on the abstract depth-$k$ network and not on the implementation (Theorem 1). We further split the bounds into output and hidden parts to tie the depth dependence of the variance to the metric entropy of the state-transition semigroup (Theorem 2). We then investigate implementation-free conditions under which the variance grow polynomially or logarithmically with depth (Section 4). Combining these with exponential or polynomial bias decay identifies four canonical bias-variance trade-off regimes (EL/EP/PL/PP) and produces explicit optimal depths $k^\ast$. Across regimes, $k^\ast>1$ typically holds, giving a rigorous form of depth supremacy. The lowest generalization error bound is achieved under the EL regime (exp-decay bias + log-growth variance), explaining why and when deep is better, especially for iterative or hierarchical concept classes such as neural ODEs, diffusion/score-matching models, and chain-of-thought reasoning.

