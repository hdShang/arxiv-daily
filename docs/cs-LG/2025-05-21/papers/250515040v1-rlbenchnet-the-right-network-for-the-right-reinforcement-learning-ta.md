---
layout: default
title: RLBenchNet: The Right Network for the Right Reinforcement Learning Task
---

# RLBenchNet: The Right Network for the Right Reinforcement Learning Task

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.15040" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.15040v1</a>
  <a href="https://arxiv.org/pdf/2505.15040.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.15040v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.15040v1', 'RLBenchNet: The Right Network for the Right Reinforcement Learning Task')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ivan Smirnov, Shangding Gu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SafeRL-Lab/RLBenchNet)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLBenchNetä»¥ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„ç½‘ç»œé€‰æ‹©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç¥ç»ç½‘ç»œ` `ç½‘ç»œæ¶æ„` `æ€§èƒ½è¯„ä¼°` `Mambaæ¨¡å‹` `LSTM` `MLP` `å†³ç­–ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°ä¸å‡ï¼Œç¼ºä¹é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç½‘ç»œé€‰æ‹©æŒ‡å¯¼ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯„ä¼°å¤šç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæå‡ºäº†é’ˆå¯¹ä¸åŒRLä»»åŠ¡çš„æœ€ä½³ç½‘ç»œé€‰æ‹©ç­–ç•¥ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLPåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒMambaæ¨¡å‹åœ¨ååé‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„LSTMå’ŒGRUã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šç§ç¥ç»ç½‘ç»œæ¶æ„çš„åº”ç”¨ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†å¤šç§ç¥ç»ç½‘ç»œåœ¨RLä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ã€å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€Mamba/Mamba-2ã€Transformer-XLã€é—¨æ§Transformer-XLå’Œé—¨æ§é€’å½’å•å…ƒï¼ˆGRUï¼‰ã€‚é€šè¿‡å¯¹è¿ç»­æ§åˆ¶ã€ç¦»æ•£å†³ç­–å’ŒåŸºäºè®°å¿†çš„ç¯å¢ƒçš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬è¯†åˆ«äº†æ¶æ„ç‰¹å®šçš„ä¼˜ç¼ºç‚¹ã€‚ç»“æœè¡¨æ˜ï¼šMLPåœ¨å®Œå…¨å¯è§‚å¯Ÿçš„è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼›LSTMå’ŒGRUåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒä¸­è¡¨ç°ç¨³å¥ï¼›Mambaæ¨¡å‹çš„ååé‡æ¯”LSTMé«˜4.5å€ï¼Œæ¯”GRUé«˜3.9å€ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼æ€§èƒ½ï¼›ä»…æœ‰Transformer-XLã€é—¨æ§Transformer-XLå’ŒMamba-2æˆåŠŸè§£å†³äº†æœ€å…·æŒ‘æˆ˜æ€§çš„å†…å­˜å¯†é›†å‹ä»»åŠ¡ï¼Œä¸”Mamba-2æ‰€éœ€å†…å­˜æ¯”Transformer-XLå°‘8å€ã€‚è¿™äº›å‘ç°ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†é€‰æ‹©æ¶æ„çš„ä¾æ®ï¼ŒåŸºäºç‰¹å®šä»»åŠ¡ç‰¹å¾å’Œè®¡ç®—çº¦æŸåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ç½‘ç»œæ¶æ„é€‰æ‹©ä¸å½“çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚è¾ƒå¤§ï¼Œç¼ºä¹ç³»ç»Ÿæ€§æŒ‡å¯¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹å¤šç§ç¥ç»ç½‘ç»œæ¶æ„çš„æ€§èƒ½è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œè¯†åˆ«å„æ¶æ„åœ¨ä¸åŒç±»å‹ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¸å±€é™ï¼Œä»è€Œä¸ºç ”ç©¶äººå‘˜æä¾›æ›´åˆç†çš„ç½‘ç»œé€‰æ‹©ä¾æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶æ¶µç›–äº†å¤šç§ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬MLPã€LSTMã€GRUã€Mambaç³»åˆ—å’ŒTransformerç³»åˆ—ï¼Œé’ˆå¯¹è¿ç»­æ§åˆ¶ã€ç¦»æ•£å†³ç­–å’ŒåŸºäºè®°å¿†çš„ç¯å¢ƒè¿›è¡Œäº†å…¨é¢çš„å®éªŒè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†å¤šç§ç½‘ç»œæ¶æ„åœ¨ä¸åŒRLä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯Mambaæ¨¡å‹åœ¨ååé‡ä¸Šçš„æ˜¾è‘—æå‡ï¼Œæä¾›äº†æ–°çš„ç½‘ç»œé€‰æ‹©è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼ŒMLPåœ¨å®Œå…¨å¯è§‚å¯Ÿä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼ŒLSTMå’ŒGRUåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒä¸­è¡¨ç°ç¨³å¥ï¼Œè€ŒMambaæ¨¡å‹åœ¨ååé‡ä¸Šæ˜¾è‘—ä¼˜äºLSTMå’ŒGRUï¼Œä¸”Mamba-2åœ¨å†…å­˜ä½¿ç”¨ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªè¯¦ç»†æŠ«éœ²ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLPåœ¨å®Œå…¨å¯è§‚å¯Ÿçš„è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒMambaæ¨¡å‹çš„ååé‡æ¯”LSTMé«˜4.5å€ï¼Œæ¯”GRUé«˜3.9å€ï¼Œä¸”Mamba-2åœ¨å†…å­˜ä½¿ç”¨ä¸Šæ¯”Transformer-XLå°‘8å€ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIå’Œæ™ºèƒ½å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–ç½‘ç»œé€‰æ‹©ï¼Œèƒ½å¤Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¸­æé«˜å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has seen significant advancements through the application of various neural network architectures. In this study, we systematically investigate the performance of several neural networks in RL tasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP), Mamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit (GRU). Through comprehensive evaluation across continuous control, discrete decision-making, and memory-based environments, we identify architecture-specific strengths and limitations. Our results reveal that: (1) MLPs excel in fully observable continuous control tasks, providing an optimal balance of performance and efficiency; (2) recurrent architectures like LSTM and GRU offer robust performance in partially observable environments with moderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput compared to LSTM and a 3.9x increase over GRU, all while maintaining comparable performance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2 successfully solve the most challenging memory-intensive tasks, with Mamba-2 requiring 8x less memory than Transformer-XL. These findings provide insights for researchers and practitioners, enabling more informed architecture selection based on specific task characteristics and computational constraints. Code is available at: https://github.com/SafeRL-Lab/RLBenchNet

