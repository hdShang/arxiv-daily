---
layout: default
title: Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions
---

# Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.10947" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.10947v3</a>
  <a href="https://arxiv.org/pdf/2505.10947.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.10947v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.10947v3', 'Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kehan Long, Jorge CortÃ©s, Nikolay Atanasov

**åˆ†ç±»**: cs.LG, cs.RO, eess.SY, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-16 (æ›´æ–°: 2025-12-06)

**å¤‡æ³¨**: NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¹¿ä¹‰æé›…æ™®è¯ºå¤«å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç¨³å®šæ€§è®¤è¯æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æé›…æ™®è¯ºå¤«å‡½æ•°` `ç¨³å®šæ€§è®¤è¯` `ç¥ç»ç½‘ç»œ` `æ§åˆ¶ç†è®º` `å¤šæ­¥æŸå¤±` `ç³»ç»Ÿå®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æé›…æ™®è¯ºå¤«æ–¹æ³•åœ¨æ„å»ºå­¦ä¹ ç­–ç•¥çš„ç¨³å®šæ€§è¯ä¹¦æ—¶é¢ä¸´å›°éš¾ï¼Œé™åˆ¶äº†å…¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å¢å¼ºRLå€¼å‡½æ•°ä¸ç¥ç»ç½‘ç»œæ®‹å·®é¡¹æ¥å­¦ä¹ å¹¿ä¹‰æé›…æ™®è¯ºå¤«å‡½æ•°ï¼Œä»è€Œç®€åŒ–è¯ä¹¦æ„å»ºè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Gymnasiumå’ŒDeepMind ControlåŸºå‡†ä¸ŠæˆåŠŸè®¤è¯äº†RLç­–ç•¥ï¼Œå¹¶æ˜¾è‘—æ‰©å¤§äº†å¸å¼•åŸŸçš„è¿‘ä¼¼èŒƒå›´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥ä¸‹ï¼Œä¸ºé—­ç¯ç³»ç»Ÿå»ºç«‹ç¨³å®šæ€§è¯ä¹¦æ˜¯è¶…è¶Šç»éªŒæ€§èƒ½å¹¶æä¾›ç³»ç»Ÿè¡Œä¸ºä¿è¯çš„å…³é”®ã€‚ä¼ ç»Ÿçš„æé›…æ™®è¯ºå¤«æ–¹æ³•è¦æ±‚æé›…æ™®è¯ºå¤«å‡½æ•°ä¸¥æ ¼é€æ­¥å‡å°ï¼Œä½†å¯¹äºå­¦ä¹ çš„ç­–ç•¥ï¼Œè¿™ç§è¯ä¹¦çš„æ„å»ºè¾ƒä¸ºå›°éš¾ã€‚æœ¬æ–‡é¦–å…ˆç ”ç©¶çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨ï¼ˆLQRï¼‰é—®é¢˜ï¼Œæå‡ºé€šè¿‡æ®‹å·®é¡¹å¢å¼ºLQRç­–ç•¥çš„å€¼å‡½æ•°æ¥è·å¾—æé›…æ™®è¯ºå¤«å‡½æ•°ï¼Œå¹¶æ”¾å®½ä¼ ç»Ÿçš„æé›…æ™®è¯ºå¤«å‡å°è¦æ±‚ï¼Œæå‡ºä»…éœ€åœ¨å¤šä¸ªæ—¶é—´æ­¥ä¸Šå¹³å‡å‡å°çš„å¹¿ä¹‰æé›…æ™®è¯ºå¤«æ¡ä»¶ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬åœ¨éçº¿æ€§è®¾ç½®ä¸‹æå‡ºäº†ä¸€ç§é€šè¿‡å¢å¼ºRLå€¼å‡½æ•°ä¸ç¥ç»ç½‘ç»œæ®‹å·®é¡¹æ¥å­¦ä¹ å¹¿ä¹‰æé›…æ™®è¯ºå¤«å‡½æ•°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•æˆåŠŸè®¤è¯äº†åœ¨Gymnasiumå’ŒDeepMind ControlåŸºå‡†ä¸Šè®­ç»ƒçš„RLç­–ç•¥ï¼Œå¹¶æ‰©å±•è‡³è”åˆè®­ç»ƒç¥ç»æ§åˆ¶å™¨å’Œç¨³å®šæ€§è¯ä¹¦ï¼Œä½¿ç”¨å¤šæ­¥æé›…æ™®è¯ºå¤«æŸå¤±è·å¾—æ›´å¤§çš„å¸å¼•åŸŸå†…è¿‘ä¼¼ã€‚æ•´ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•ä¸ºå¹¿æ³›çš„å­¦ä¹ ç­–ç•¥ç³»ç»Ÿæä¾›äº†ç¨³å®šæ€§è®¤è¯çš„å¯èƒ½ï¼Œä¿ƒè¿›äº†ç»å…¸æ§åˆ¶ç†è®ºä¸ç°ä»£å­¦ä¹ æ–¹æ³•çš„ç»“åˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¸‹é—­ç¯ç³»ç»Ÿçš„ç¨³å®šæ€§è®¤è¯é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºä¼ ç»Ÿæé›…æ™®è¯ºå¤«æ–¹æ³•ï¼Œéš¾ä»¥ä¸ºå­¦ä¹ çš„ç­–ç•¥æ„å»ºæœ‰æ•ˆçš„ç¨³å®šæ€§è¯ä¹¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å°†RLå€¼å‡½æ•°ä¸ç¥ç»ç½‘ç»œæ®‹å·®é¡¹ç»“åˆï¼Œæ¥å­¦ä¹ å¹¿ä¹‰æé›…æ™®è¯ºå¤«å‡½æ•°ï¼Œä»è€Œæ”¾å®½ä¼ ç»Ÿæé›…æ™®è¯ºå¤«å‡å°è¦æ±‚ï¼Œä½¿å¾—ç¨³å®šæ€§è¯ä¹¦çš„æ„å»ºæ›´åŠ ç®€å•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡å¯¹LQRé—®é¢˜çš„åˆ†æï¼Œæ„å»ºåŸºäºå€¼å‡½æ•°çš„æé›…æ™®è¯ºå¤«å‡½æ•°ï¼›å…¶æ¬¡ï¼Œåœ¨éçº¿æ€§è®¾ç½®ä¸­ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œæ®‹å·®é¡¹å¢å¼ºå€¼å‡½æ•°ï¼Œå­¦ä¹ å¹¿ä¹‰æé›…æ™®è¯ºå¤«å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¹¿ä¹‰æé›…æ™®è¯ºå¤«æ¡ä»¶ï¼Œå…è®¸åœ¨å¤šä¸ªæ—¶é—´æ­¥ä¸Šå¹³å‡å‡å°ï¼Œè€Œéä¸¥æ ¼é€æ­¥å‡å°ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—é™ä½äº†ç¨³å®šæ€§è¯ä¹¦çš„æ„å»ºéš¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨å¤šæ­¥æé›…æ™®è¯ºå¤«æŸå¤±ä»¥è”åˆè®­ç»ƒç¥ç»æ§åˆ¶å™¨å’Œç¨³å®šæ€§è¯ä¹¦ï¼Œæ­¤å¤–ï¼Œç½‘ç»œç»“æ„ä¸Šç»“åˆäº†ç¥ç»ç½‘ç»œä»¥å¢å¼ºå€¼å‡½æ•°çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæå‡äº†ç¨³å®šæ€§è®¤è¯çš„æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨Gymnasiumå’ŒDeepMind ControlåŸºå‡†ä¸ŠæˆåŠŸè®¤è¯äº†å¤šä¸ªRLç­–ç•¥ï¼Œå¹¶é€šè¿‡å¤šæ­¥æé›…æ™®è¯ºå¤«æŸå¤±è·å¾—äº†æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å¤§çš„å¸å¼•åŸŸå†…è¿‘ä¼¼ï¼Œæå‡å¹…åº¦æ˜¾è‘—ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦é«˜å¯é æ€§å’Œç¨³å®šæ€§çš„ç³»ç»Ÿã€‚é€šè¿‡æä¾›ç¨³å®šæ€§è®¤è¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¢å¼ºç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é¢„æµ‹æ€§ï¼Œä¿ƒè¿›æ™ºèƒ½ç³»ç»Ÿåœ¨å®é™…ç¯å¢ƒä¸­çš„åº”ç”¨ä¸æ¨å¹¿ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Establishing stability certificates for closed-loop systems under reinforcement learning (RL) policies is essential to move beyond empirical performance and offer guarantees of system behavior. Classical Lyapunov methods require a strict stepwise decrease in the Lyapunov function but such certificates are difficult to construct for learned policies. The RL value function is a natural candidate but it is not well understood how it can be adapted for this purpose. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods.

