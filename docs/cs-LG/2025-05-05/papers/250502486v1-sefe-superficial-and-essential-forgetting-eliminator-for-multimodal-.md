---
layout: default
title: "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning"
---

# SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02486" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02486v1</a>
  <a href="https://arxiv.org/pdf/2505.02486.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02486v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02486v1', 'SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinpeng Chen, Runmin Cong, Yuzhi Zhao, Hongzheng Yang, Guangneng Hu, Horace Ho Shing Ip, Sam Kwong

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSEFEä»¥è§£å†³å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ä¸­çš„é—å¿˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æŒç»­å­¦ä¹ ` `çŸ¥è¯†é—å¿˜` `å¤§è¯­è¨€æ¨¡å‹` `æ­£åˆ™åŒ–æŠ€æœ¯` `ä»»åŠ¡é€‚åº”æ€§` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜æ–¹æ³•åœ¨å¤„ç†ä»»åŠ¡é—´çŸ¥è¯†é—å¿˜æ—¶å­˜åœ¨è¡¨é¢é—å¿˜å’Œæœ¬è´¨é—å¿˜çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºç­”æ¡ˆé£æ ¼å¤šæ ·åŒ–ï¼ˆASDï¼‰èŒƒå¼å’ŒRegLoRAæ–¹æ³•ï¼Œä»¥å‡å°‘è¡¨é¢é—å¿˜å¹¶ç¨³å®šå…³é”®å‚æ•°ï¼Œä»è€Œä¿ç•™å·²æœ‰çŸ¥è¯†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEFEæ–¹æ³•åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ï¼ˆMCITï¼‰æ—¨åœ¨ä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿåœ¨ä¸å‘ç”Ÿç¾éš¾æ€§é—å¿˜çš„æƒ…å†µä¸‹é€æ­¥å­¦ä¹ æ–°ä»»åŠ¡ã€‚æœ¬æ–‡æ¢è®¨äº†é—å¿˜çš„ä¸¤ç§ç±»å‹ï¼šè¡¨é¢é—å¿˜å’Œæœ¬è´¨é—å¿˜ã€‚è¡¨é¢é—å¿˜æŒ‡æ¨¡å‹çš„çŸ¥è¯†æœªçœŸæ­£ä¸§å¤±ï¼Œä½†ç”±äºåç»­ä»»åŠ¡çš„å›ç­”é£æ ¼å½±å“ï¼Œå¯¼è‡´å¯¹å…ˆå‰ä»»åŠ¡çš„å“åº”åç¦»é¢„æœŸæ ¼å¼ã€‚è€Œæœ¬è´¨é—å¿˜åˆ™æ˜¯æŒ‡æ¨¡å‹æä¾›æ ¼å¼æ­£ç¡®ä½†äº‹å®ä¸å‡†ç¡®çš„ç­”æ¡ˆï¼Œè¡¨æ˜çŸ¥è¯†çš„çœŸæ­£ä¸§å¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç­”æ¡ˆé£æ ¼å¤šæ ·åŒ–ï¼ˆASDï¼‰èŒƒå¼å’ŒRegLoRAæ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒSEFEæ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ä¸­çš„é—å¿˜ç°è±¡ï¼Œç‰¹åˆ«æ˜¯è¡¨é¢é—å¿˜å’Œæœ¬è´¨é—å¿˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä»»åŠ¡é—´åˆ‡æ¢æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šå› åç»­ä»»åŠ¡çš„å½±å“è€Œä¸§å¤±å¯¹å…ˆå‰ä»»åŠ¡çš„æœ‰æ•ˆå“åº”ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ç­”æ¡ˆé£æ ¼å¤šæ ·åŒ–ï¼ˆASDï¼‰æ¥ç»Ÿä¸€ä¸åŒä»»åŠ¡çš„æ•°æ®é£æ ¼ï¼Œå‡å°‘è¡¨é¢é—å¿˜çš„å‘ç”Ÿã€‚åŒæ—¶ï¼Œä½¿ç”¨RegLoRAæ–¹æ³•å¯¹å…³é”®å‚æ•°è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥å‡è½»æœ¬è´¨é—å¿˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯ASDæ¨¡å—ï¼Œé€šè¿‡æ ‡å‡†åŒ–çš„è¿‡ç¨‹å°†ä¸åŒä»»åŠ¡çš„æ•°æ®é£æ ¼è½¬åŒ–ä¸ºå¤šæ ·åŒ–çš„å½¢å¼ï¼›å…¶æ¬¡æ˜¯RegLoRAæ¨¡å—ï¼Œé€šè¿‡æ­£åˆ™åŒ–æŠ€æœ¯ç¨³å®šæ¨¡å‹çš„å…³é”®å‚æ•°ï¼Œç¡®ä¿å·²æœ‰çŸ¥è¯†çš„ä¿ç•™ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†è¡¨é¢é—å¿˜å’Œæœ¬è´¨é—å¿˜è¿›è¡Œåˆ†ç±»ï¼Œå¹¶æå‡ºç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚ASDå’ŒRegLoRAçš„ç»“åˆä½¿å¾—æ¨¡å‹åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿ç•™æ—§çŸ¥è¯†ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¸¸è§çš„ç¾éš¾æ€§é—å¿˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ASDä¸­ï¼Œè®¾è®¡äº†æ ‡å‡†åŒ–çš„æ•°æ®è½¬æ¢æµç¨‹ï¼Œä»¥ç¡®ä¿ä¸åŒä»»åŠ¡ä¹‹é—´çš„é£æ ¼ä¸€è‡´æ€§ï¼›åœ¨RegLoRAä¸­ï¼Œå…³é”®å‚æ•°çš„æ­£åˆ™åŒ–ç­–ç•¥è¢«ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘çŸ¥è¯†ä¸¢å¤±çš„é£é™©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSEFEæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å‡å°‘é—å¿˜å’Œæé«˜ä»»åŠ¡é€‚åº”æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æ•™è‚²æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿå¸®åŠ©å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸æ–­å˜åŒ–çš„ä»»åŠ¡ç¯å¢ƒä¸­ä¿æŒé«˜æ•ˆçš„å­¦ä¹ èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´æ™ºèƒ½çš„AIç³»ç»Ÿçš„å‘å±•ï¼Œä½¿å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°æ›´åŠ å‡ºè‰²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting. In this paper, we explore forgetting in this context, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model's knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks' answer styles, making the results unusable. By contrast, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can obscure the model's knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for transforming data styles across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization, enabling the model to retain existing competencies. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.

