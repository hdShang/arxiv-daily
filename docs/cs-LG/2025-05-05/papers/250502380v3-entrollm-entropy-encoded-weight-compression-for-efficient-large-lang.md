---
layout: default
title: EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices
---

# EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02380" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.02380v3</a>
  <a href="https://arxiv.org/pdf/2505.02380.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02380v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02380v3', 'EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Arnab Sanyal, Gourav Datta, Prithwish Mukherjee, Sandeep P. Chinchali, Michael Orshansky

**ÂàÜÁ±ª**: cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-05 (Êõ¥Êñ∞: 2025-07-14)

**Â§áÊ≥®**: 6 pages, 1 reference page

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EntroLLM‰ª•Ëß£ÂÜ≥ËæπÁºòËÆæÂ§á‰∏äÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ËæπÁºòËÆ°ÁÆó` `Ê®°ÂûãÂéãÁº©` `Ê∑∑ÂêàÈáèÂåñ` `ÁÜµÁºñÁ†Å` `ÈúçÂ§´ÊõºÁºñÁ†Å` `Êé®ÁêÜÊïàÁéá` `ÂÆûÊó∂Â§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÂ∫îÁî®ÂèóÂà∞Â≠òÂÇ®ÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑÈôêÂà∂ÔºåÈöæ‰ª•È´òÊïàÊé®ÁêÜ„ÄÇ
2. EntroLLMÈÄöËøáÊ∑∑ÂêàÈáèÂåñÂíåÁÜµÁºñÁ†ÅÁöÑÁªìÂêàÔºå‰ºòÂåñ‰∫ÜÊ®°ÂûãÁöÑÂ≠òÂÇ®ÂíåÊé®ÁêÜÊïàÁéáÔºå‰øùÊåÅ‰∫ÜÂáÜÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåEntroLLMÂú®Â≠òÂÇ®‰∏äÁõ∏ÊØîuint8Ê®°ÂûãÂáèÂ∞ë‰∫Ü30%ÔºåÂπ∂Âú®Êé®ÁêÜÂêûÂêêÈáè‰∏äÊèêÂçá‰∫Ü31.9%Ëá≥146.6%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂Â∫ûÂ§ßÁöÑÂ≠òÂÇ®ÂíåËÆ°ÁÆóÈúÄÊ±ÇÈôêÂà∂‰∫ÜÂú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜEntroLLMÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂéãÁº©Ê°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÊ∑∑ÂêàÈáèÂåñÂíåÁÜµÁºñÁ†ÅÔºå‰ª•ÂáèÂ∞ëÂ≠òÂÇ®ÂºÄÈîÄÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ÈááÁî®ÈÄêÂ±ÇÊ∑∑ÂêàÈáèÂåñÊñπÊ°àÔºåÊ†πÊçÆÂêÑÂ±ÇÊùÉÈáçÂàÜÂ∏ÉÈÄâÊã©ÂØπÁß∞ÊàñÈùûÂØπÁß∞ÈáèÂåñÔºå‰ª•‰ºòÂåñÂèØÂéãÁº©ÊÄß„ÄÇÊé•ÁùÄÔºåÊàë‰ª¨‰ΩøÁî®ÈúçÂ§´ÊõºÁºñÁ†ÅÂØπÈáèÂåñÊùÉÈáçËøõË°åÊó†ÊçüÂéãÁº©ÔºåÊòæËëóÈôç‰ΩéÂÜÖÂ≠òÂ∏¶ÂÆΩÈúÄÊ±Ç„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•Âπ∂Ë°åÈúçÂ§´ÊõºËß£Á†ÅÔºåÁ°Æ‰øùÂú®Êé®ÁêÜËøáÁ®ã‰∏≠È´òÊïàÊ£ÄÁ¥¢ÁºñÁ†ÅÊùÉÈáçÔºåÊúÄÂ∞èÂåñÂª∂ËøüÂΩ±Âìç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEntroLLMÂú®ËæπÁºòÂÖºÂÆπÁöÑLLMs‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ30%ÁöÑÂ≠òÂÇ®ÂáèÂ∞ëÔºåÂπ∂Âú®ÂÜÖÂ≠òÂ∏¶ÂÆΩÂèóÈôêÁöÑËÆæÂ§á‰∏äÊèêÈ´ò‰∫Ü31.9%Ëá≥146.6%ÁöÑÊé®ÁêÜÂêûÂêêÈáè„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂΩìÂâçÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÂ∫îÁî®Èù¢‰∏¥Â≠òÂÇ®ÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑÊåëÊàòÔºåÂØºËá¥Êé®ÁêÜÊïàÁéá‰Ωé‰∏ãÔºåÊó†Ê≥ïÊª°Ë∂≥ÂÆûÊó∂ÈúÄÊ±Ç„ÄÇÁé∞ÊúâÁöÑÂéãÁº©ÊñπÊ≥ïÂæÄÂæÄÊó†Ê≥ïÂÖºÈ°æÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÈÄüÂ∫¶„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEntroLLMÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªìÂêàÊ∑∑ÂêàÈáèÂåñÂíåÁÜµÁºñÁ†ÅÁöÑÂéãÁº©Ê°ÜÊû∂ÔºåÈÄöËøáÈÄêÂ±ÇÈÄâÊã©ÂêàÈÄÇÁöÑÈáèÂåñÊñπÂºèÔºå‰ºòÂåñÊ®°ÂûãÁöÑÂ≠òÂÇ®ÈúÄÊ±ÇÔºåÂπ∂ÈááÁî®ÈúçÂ§´ÊõºÁºñÁ†ÅÂÆûÁé∞Êó†ÊçüÂéãÁº©ÔºåÁ°Æ‰øùÂú®Êé®ÁêÜÊó∂ÁöÑÈ´òÊïàÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØÈÄêÂ±ÇÊ∑∑ÂêàÈáèÂåñÔºåÊ†πÊçÆÊùÉÈáçÂàÜÂ∏ÉÈÄâÊã©ÂØπÁß∞ÊàñÈùûÂØπÁß∞ÈáèÂåñÔºõÂÖ∂Ê¨°ÊòØÈúçÂ§´ÊõºÁºñÁ†ÅÂíåÂπ∂Ë°åËß£Á†ÅÊ®°ÂùóÔºåÁî®‰∫éÈ´òÊïàÂ≠òÂÇ®ÂíåÂø´ÈÄüÊ£ÄÁ¥¢ÈáèÂåñÂêéÁöÑÊùÉÈáç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEntroLLMÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂÖ∂Ê∑∑ÂêàÈáèÂåñÁ≠ñÁï•ÂíåÂπ∂Ë°åÈúçÂ§´ÊõºËß£Á†ÅÁöÑÁªìÂêàÔºåÊòæËëóÊèêÂçá‰∫ÜËæπÁºòËÆæÂ§áÁöÑÊé®ÁêÜÊïàÁéáÂíåÂ≠òÂÇ®Âà©Áî®ÁéáÔºå‰∏é‰º†ÁªüÁöÑÂçï‰∏ÄÈáèÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÊÄßËÉΩË°®Áé∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÈáèÂåñËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÊ†πÊçÆÊØèÂ±ÇÁöÑÊùÉÈáçÂàÜÂ∏ÉÂä®ÊÄÅÈÄâÊã©ÈáèÂåñÊñπÂºèÔºåÈúçÂ§´ÊõºÁºñÁ†ÅÂàôÁ°Æ‰øù‰∫ÜÈáèÂåñÊùÉÈáçÁöÑÊó†ÊçüÂéãÁº©„ÄÇÊ≠§Â§ñÔºåËÆæËÆ°‰∏≠Êú™Â¢ûÂä†È¢ùÂ§ñÁöÑÂÜçËÆ≠ÁªÉÊ≠•È™§Ôºå‰ΩøÂæóËØ•ÊñπÊ≥ï‰∏éÁé∞ÊúâÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ïÂÆåÂÖ®ÂÖºÂÆπ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EntroLLMÂú®ËæπÁºòÂÖºÂÆπÁöÑLLMs‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ30%ÁöÑÂ≠òÂÇ®ÂáèÂ∞ëÔºåÁõ∏ÊØî‰∫éuint4Ê®°ÂûãÊõ¥ÊòØËææÂà∞‰∫Ü65%ÁöÑÂ≠òÂÇ®ÂáèÂ∞ë„ÄÇÂêåÊó∂ÔºåÂú®ÂÜÖÂ≠òÂ∏¶ÂÆΩÂèóÈôêÁöÑËÆæÂ§á‰∏äÔºåÊé®ÁêÜÂêûÂêêÈáèÊèêÂçá‰∫Ü31.9%Ëá≥146.6%ÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EntroLLMÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ËæπÁºòËÆ°ÁÆó„ÄÅÁßªÂä®ËÆæÂ§áÂíåÁâ©ËÅîÁΩëÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéáÂíåÈôç‰ΩéÂ≠òÂÇ®ÈúÄÊ±ÇÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊîØÊåÅÊõ¥Â§öÊô∫ËÉΩÂ∫îÁî®ÁöÑÂÆûÊó∂Â§ÑÁêÜÔºåÊé®Âä®ËæπÁºòÊô∫ËÉΩÁöÑÂèëÂ±ï„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØËÉΩÂú®Êô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®È©æÈ©∂ÂíåÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÂú∫ÊôØ‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) demonstrate exceptional performance across various tasks, but their large storage and computational requirements constrain their deployment on edge devices. To address this, we propose EntroLLM, a novel compression framework that integrates mixed quantization with entropy coding to reduce storage overhead while maintaining model accuracy. Our method applies a layer-wise mixed quantization scheme - choosing between symmetric and asymmetric quantization based on individual layer weight distributions - to optimize compressibility. We then employ Huffman encoding for lossless compression of the quantized weights, significantly reducing memory bandwidth requirements. Furthermore, we introduce parallel Huffman decoding, which enables efficient retrieval of encoded weights during inference, ensuring minimal latency impact. Our experiments on edge-compatible LLMs, including smolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct, demonstrate that EntroLLM achieves up to $30\%$ storage reduction compared to uint8 models and up to $65%$ storage reduction compared to uint4 models, while preserving perplexity and accuracy, on language benchmark tasks. We further show that our method enables $31.9\%$ - $146.6\%$ faster inference throughput on memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by reducing the required data movement. The proposed approach requires no additional re-training and is fully compatible with existing post-training quantization methods, making it a practical solution for edge LLMs.

