---
layout: default
title: Towards Quantifying the Hessian Structure of Neural Networks
---

# Towards Quantifying the Hessian Structure of Neural Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02809" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02809v2</a>
  <a href="https://arxiv.org/pdf/2505.02809.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02809v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02809v2', 'Towards Quantifying the Hessian Structure of Neural Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaorui Dong, Yushun Zhang, Jianfeng Yao, Ruoyu Sun

**åˆ†ç±»**: cs.LG, math.OC, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-09-21)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºç¥ç»ç½‘ç»œHessiançŸ©é˜µçš„è¿‘å—å¯¹è§’ç»“æ„**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `HessiançŸ©é˜µ` `ç¥ç»ç½‘ç»œ` `éšæœºçŸ©é˜µç†è®º` `å—å¯¹è§’ç»“æ„` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¼˜åŒ–ç®—æ³•` `ç†è®ºåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶è™½ç„¶è§‚å¯Ÿåˆ°ç¥ç»ç½‘ç»œHessiançŸ©é˜µçš„è¿‘å—å¯¹è§’ç»“æ„ï¼Œä½†å…¶ç†è®ºåŸºç¡€å°šä¸æ˜ç¡®ï¼Œç¼ºä¹æ·±å…¥åˆ†æã€‚
2. è®ºæ–‡é€šè¿‡ç†è®ºåˆ†ææ­ç¤ºäº†HessiançŸ©é˜µç»“æ„çš„æ¥æºï¼Œæå‡ºäº†é™æ€å’ŒåŠ¨æ€åŠ›é‡çš„æ¦‚å¿µï¼Œæ·±å…¥æ¢è®¨äº†å…¶åœ¨éšæœºåˆå§‹åŒ–ä¸‹çš„è¡¨ç°ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç±»åˆ«æ•°Cæ˜¯å½±å“HessiançŸ©é˜µè¿‘å—å¯¹è§’ç»“æ„çš„ä¸»è¦å› ç´ ï¼Œä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„Hessianç»“æ„æä¾›äº†æ–°æ€è·¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ­ç¤ºäº†ç¥ç»ç½‘ç»œï¼ˆNNsï¼‰HessiançŸ©é˜µçš„è¿‘å—å¯¹è§’ç»“æ„æ¥æºäºä¸¤ç§åŠ›é‡çš„æ··åˆï¼šä¸€ç§æ˜¯æºäºæ¶æ„è®¾è®¡çš„â€œé™æ€åŠ›é‡â€ï¼Œå¦ä¸€ç§æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„â€œåŠ¨æ€åŠ›é‡â€ã€‚æˆ‘ä»¬å¯¹éšæœºåˆå§‹åŒ–ä¸‹çš„â€œé™æ€åŠ›é‡â€è¿›è¡Œäº†ä¸¥æ ¼çš„ç†è®ºåˆ†æï¼Œç ”ç©¶äº†çº¿æ€§æ¨¡å‹å’Œå…·æœ‰ä¸€ä¸ªéšè—å±‚çš„åˆ†ç±»ç½‘ç»œã€‚é€šè¿‡éšæœºçŸ©é˜µç†è®ºï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†HessiançŸ©é˜µå¯¹è§’å—å’Œéå¯¹è§’å—çš„æé™åˆ†å¸ƒï¼Œå‘ç°å½“ç±»åˆ«æ•°Cå¢å¤§æ—¶ï¼Œå—å¯¹è§’ç»“æ„é€æ¸æ˜¾ç°ã€‚è¿™äº›å‘ç°ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„Hessianç»“æ„æä¾›äº†æ–°çš„è§†è§’ï¼Œå°¤å…¶æ˜¯åœ¨Cè¶…è¿‡10^4çš„æƒ…å†µä¸‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œHessiançŸ©é˜µè¿‘å—å¯¹è§’ç»“æ„çš„ç†è®ºåŸºç¡€ä¸æ˜ç¡®çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å…¶æ¥æºçš„æ·±å…¥æ¢è®¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†â€œé™æ€åŠ›é‡â€å’Œâ€œåŠ¨æ€åŠ›é‡â€çš„æ¦‚å¿µï¼Œåˆ†æäº†è¿™ä¸¤ç§åŠ›é‡å¦‚ä½•å…±åŒå½±å“HessiançŸ©é˜µçš„ç»“æ„ï¼Œå°¤å…¶æ˜¯åœ¨éšæœºåˆå§‹åŒ–æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šé¦–å…ˆï¼Œåˆ†æçº¿æ€§æ¨¡å‹å’Œå•éšè—å±‚ç½‘ç»œçš„HessiançŸ©é˜µï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨éšæœºçŸ©é˜µç†è®ºæ¯”è¾ƒå¯¹è§’å—å’Œéå¯¹è§’å—çš„æé™åˆ†å¸ƒï¼Œæ­ç¤ºç±»åˆ«æ•°Cå¯¹ç»“æ„çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†HessiançŸ©é˜µçš„ç»“æ„å½’å› äºé™æ€å’ŒåŠ¨æ€åŠ›é‡çš„ç»“åˆï¼Œç‰¹åˆ«æ˜¯å¼ºè°ƒäº†ç±»åˆ«æ•°Cåœ¨å½¢æˆè¿‘å—å¯¹è§’ç»“æ„ä¸­çš„å…³é”®ä½œç”¨ï¼Œè¿™ä¸ç°æœ‰ç ”ç©¶çš„å•ä¸€è§†è§’å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†éšæœºåˆå§‹åŒ–çš„çº¿æ€§æ¨¡å‹å’Œå•éšè—å±‚ç½‘ç»œï¼Œé‡ç‚¹å…³æ³¨HessiançŸ©é˜µçš„å¯¹è§’å—å’Œéå¯¹è§’å—çš„æé™åˆ†å¸ƒï¼Œç¡®ä¿äº†ç†è®ºåˆ†æçš„ä¸¥è°¨æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“ç±»åˆ«æ•°Cå¢å¤§æ—¶ï¼ŒHessiançŸ©é˜µçš„è¿‘å—å¯¹è§’ç»“æ„æ„ˆåŠ æ˜æ˜¾ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„Hessianç»“æ„æä¾›äº†æ–°çš„è§†è§’ï¼Œå°¤å…¶æ˜¯åœ¨Cè¶…è¿‡10^4çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½å¯¹æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–ç­–ç•¥äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„å‘ç°å¯¹ç†è§£ç¥ç»ç½‘ç»œçš„è®­ç»ƒåŠ¨æ€å’Œä¼˜åŒ–è¿‡ç¨‹å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¾è®¡ä¸åº”ç”¨ä¸­ã€‚é€šè¿‡æ·±å…¥åˆ†æHessiançŸ©é˜µçš„ç»“æ„ï¼Œå¯ä»¥ä¸ºæ¨¡å‹çš„ä¼˜åŒ–å’Œæ€§èƒ½æå‡æä¾›æ–°çš„æ€è·¯ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆçš„ç®—æ³•å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Empirical studies reported that the Hessian matrix of neural networks (NNs) exhibits a near-block-diagonal structure, yet its theoretical foundation remains unclear. In this work, we reveal that the reported Hessian structure comes from a mixture of two forces: a ``static force'' rooted in the architecture design, and a ''dynamic force'' arisen from training. We then provide a rigorous theoretical analysis of ''static force'' at random initialization. We study linear models and 1-hidden-layer networks for classification tasks with $C$ classes. By leveraging random matrix theory, we compare the limit distributions of the diagonal and off-diagonal Hessian blocks and find that the block-diagonal structure arises as $C$ becomes large. Our findings reveal that $C$ is one primary driver of the near-block-diagonal structure. These results may shed new light on the Hessian structure of large language models (LLMs), which typically operate with a large $C$ exceeding $10^4$.

