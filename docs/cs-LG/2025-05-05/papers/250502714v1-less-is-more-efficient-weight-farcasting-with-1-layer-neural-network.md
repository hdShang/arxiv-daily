---
layout: default
title: "Less is More: Efficient Weight Farcasting with 1-Layer Neural Network"
---

# Less is More: Efficient Weight Farcasting with 1-Layer Neural Network

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02714" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02714v1</a>
  <a href="https://arxiv.org/pdf/2505.02714.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02714v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02714v1', 'Less is More: Efficient Weight Farcasting with 1-Layer Neural Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiao Shou, Debarun Bhattacharjya, Yanna Ding, Chen Zhao, Rui Li, Jianxi Gao

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: Accepted to DASFAA '25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆçš„1å±‚ç¥ç»ç½‘ç»œæƒé‡è¿œé¢„æµ‹æ–¹æ³•ä»¥è§£å†³è®­ç»ƒæ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ·±åº¦å­¦ä¹ ` `æƒé‡é¢„æµ‹` `è®¡ç®—æ•ˆç‡` `ç¥ç»ç½‘ç»œ` `æ¨¡å‹è®­ç»ƒ` `æ­£åˆ™åŒ–` `æ—¶é—´åºåˆ—é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒå¤§è§„æ¨¡æ·±åº¦ç¥ç»ç½‘ç»œæ—¶é¢ä¸´è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œéš¾ä»¥æ»¡è¶³ä¸æ–­å¢é•¿çš„æ¨¡å‹éœ€æ±‚ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹æŠ€æœ¯ï¼Œä»…ä¾èµ–åˆå§‹å’Œæœ€ç»ˆæƒé‡å€¼ï¼Œç®€åŒ–äº†æ¨¡å‹æ¶æ„ã€‚
3. å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºæ›´é«˜çš„é¢„æµ‹å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å½“ä»Šæœºå™¨å­¦ä¹ ç ”ç©¶ä¸­ï¼Œè§£å†³å¤§è§„æ¨¡æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„è®¡ç®—æŒ‘æˆ˜ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦è¯¾é¢˜ã€‚å°½ç®¡ä»¥å¾€çš„ç ”ç©¶é€šè¿‡åŠ¨é‡æ¢¯åº¦ä¸‹é™ã€å­¦ä¹ ç‡è°ƒåº¦å’Œæƒé‡æ­£åˆ™åŒ–ç­‰æŠ€æœ¯æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œä½†éšç€æ¨¡å‹è§„æ¨¡çš„ä¸æ–­æ‰©å¤§ï¼Œåˆ›æ–°çš„éœ€æ±‚ä¾ç„¶è¿«åˆ‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹æŠ€æœ¯ï¼Œä¸“æ³¨äºåˆå§‹å’Œæœ€ç»ˆæƒé‡å€¼ï¼Œæä¾›äº†ä¸€ç§ç®€åŒ–çš„å¤æ‚æ¨¡å‹æ¶æ„æ›¿ä»£æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹æ­£åˆ™åŒ–å™¨ï¼Œä»¥å¢å¼ºé¢„æµ‹æ€§èƒ½ã€‚é€šè¿‡å¯¹åˆæˆæƒé‡åºåˆ—å’Œå®é™…æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹DistilBERTï¼‰çš„å®è¯è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”æ‰€éœ€çš„é¢å¤–è®¡ç®—å¼€é”€æå°ï¼Œä¸ºåŠ é€Ÿå¤šç§ä»»åŠ¡å’Œæ¶æ„çš„è®­ç»ƒè¿‡ç¨‹æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤æ‚çš„æ¨¡å‹æ¶æ„å’Œå¤šç§è®­ç»ƒæŠ€å·§ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ç¼“æ…¢ä¸”èµ„æºæ¶ˆè€—å¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„æ¡†æ¶é€šè¿‡é•¿æ—¶é—´åºåˆ—é¢„æµ‹æŠ€æœ¯ï¼Œä¸“æ³¨äºåˆå§‹å’Œæœ€ç»ˆæƒé‡å€¼ï¼Œä»è€Œç®€åŒ–äº†æ¨¡å‹çš„å¤æ‚æ€§ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æƒé‡åˆå§‹åŒ–ã€æƒé‡é¢„æµ‹å’Œæ­£åˆ™åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆå§‹æƒé‡å’Œæœ€ç»ˆæƒé‡çš„å…³ç³»è¿›è¡Œé¢„æµ‹ï¼Œç„¶ååº”ç”¨æ–°å‹æ­£åˆ™åŒ–å™¨ä»¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä»…ä¾èµ–åˆå§‹å’Œæœ€ç»ˆæƒé‡å€¼è¿›è¡Œé¢„æµ‹ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿæ–¹æ³•å¯¹å¤æ‚æ¨¡å‹æ¶æ„çš„ä¾èµ–ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ï¼Œè¿˜é™ä½äº†è®¡ç®—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ç‰¹å®šçš„æ­£åˆ™åŒ–å™¨ä»¥å¢å¼ºé¢„æµ‹æ•ˆæœï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šé‡‡ç”¨äº†å•å±‚ç¥ç»ç½‘ç»œï¼Œç¡®ä¿äº†æ¨¡å‹çš„ç®€æ´æ€§å’Œé«˜æ•ˆæ€§ã€‚å®éªŒä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°ç»è¿‡ç²¾å¿ƒé€‰æ‹©ï¼Œä»¥ä¼˜åŒ–é¢„æµ‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆæˆæƒé‡åºåˆ—å’ŒçœŸå®æ•°æ®é›†ä¸Šå‡æ˜¾è‘—æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹DistilBERTçš„åº”ç”¨ä¸­ï¼Œè¡¨ç°å‡ºæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚é€šè¿‡æé«˜è®­ç»ƒæ•ˆç‡ï¼Œæœ¬æ–¹æ³•èƒ½å¤ŸåŠ é€Ÿæ¨¡å‹å¼€å‘å‘¨æœŸï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Addressing the computational challenges inherent in training large-scale deep neural networks remains a critical endeavor in contemporary machine learning research. While previous efforts have focused on enhancing training efficiency through techniques such as gradient descent with momentum, learning rate scheduling, and weight regularization, the demand for further innovation continues to burgeon as model sizes keep expanding. In this study, we introduce a novel framework which diverges from conventional approaches by leveraging long-term time series forecasting techniques. Our method capitalizes solely on initial and final weight values, offering a streamlined alternative for complex model architectures. We also introduce a novel regularizer that is tailored to enhance the forecasting performance of our approach. Empirical evaluations conducted on synthetic weight sequences and real-world deep learning architectures, including the prominent large language model DistilBERT, demonstrate the superiority of our method in terms of forecasting accuracy and computational efficiency. Notably, our framework showcases improved performance while requiring minimal additional computational overhead, thus presenting a promising avenue for accelerating the training process across diverse tasks and architectures.

