---
layout: default
title: HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models
---

# HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02795" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02795v1</a>
  <a href="https://arxiv.org/pdf/2505.02795.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02795v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02795v1', 'HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zheng Lin, Yuxin Zhang, Zhe Chen, Zihan Fang, Xianhao Chen, Praneeth Vepakomma, Wei Ni, Jun Luo, Yue Gao

**åˆ†ç±»**: cs.LG, cs.AI, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: 16 pages, 22 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHSplitLoRAä»¥è§£å†³å¼‚æ„è®¾å¤‡ä¸Šå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¾®è°ƒ` `è”é‚¦å­¦ä¹ ` `ä½ç§©é€‚åº”` `å¼‚æ„è®¡ç®—` `åˆ†å‰²å­¦ä¹ ` `å‚æ•°é«˜æ•ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå¼‚æ„è®¾å¤‡èµ„æºçš„æŒ‘æˆ˜ã€‚
2. HSplitLoRAé€šè¿‡è¯†åˆ«é‡è¦æƒé‡ã€åŠ¨æ€é…ç½®LoRAé€‚é…å™¨å’Œç¡®å®šæ¨¡å‹åˆ†å‰²ç‚¹æ¥ä¼˜åŒ–å¾®è°ƒè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHSplitLoRAåœ¨è®­ç»ƒå‡†ç¡®æ€§å’Œæ”¶æ•›é€Ÿåº¦ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—çªç ´ã€‚ç”±äºå…¶åºå¤§çš„å‚æ•°è§„æ¨¡ï¼Œä½¿ç”¨ç§æœ‰æ•°æ®å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒå·²æˆä¸ºä¸»æµã€‚å°½ç®¡è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸ºæ— æ•°æ®å…±äº«çš„å¾®è°ƒæä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼Œä½†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬é˜»ç¢äº†å…¶æ™®åŠã€‚æ­¤å¤–ï¼Œç°å®åœºæ™¯ä¸­ï¼Œç§æœ‰å®¢æˆ·ç«¯è®¾å¤‡å¾€å¾€å…·æœ‰å¼‚æ„çš„è®¡ç®—èµ„æºï¼Œè¿›ä¸€æ­¥å¢åŠ äº†å¾®è°ƒçš„å¤æ‚æ€§ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†HSplitLoRAï¼Œä¸€ä¸ªåŸºäºåˆ†å‰²å­¦ä¹ ï¼ˆSLï¼‰å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒçš„å¼‚æ„å‚æ•°é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°åœ¨å¼‚æ„å®¢æˆ·ç«¯è®¾å¤‡ä¸Šå¾®è°ƒLLMsã€‚HSplitLoRAé¦–å…ˆæ ¹æ®æƒé‡å¯¹LLMè®­ç»ƒçš„è´¡çŒ®è¯†åˆ«é‡è¦æƒé‡ï¼Œç„¶ååŠ¨æ€é…ç½®LoRAé€‚é…å™¨çš„åˆ†è§£ç§©ï¼Œå¹¶æ ¹æ®å®¢æˆ·ç«¯è®¾å¤‡çš„è®¡ç®—é¢„ç®—ç¡®å®šæ¨¡å‹åˆ†å‰²ç‚¹ã€‚æœ€åï¼Œè®¾è®¡äº†ä¸€ç§æ— å™ªå£°çš„é€‚é…å™¨èšåˆæœºåˆ¶ï¼Œä»¥æ”¯æŒå¼‚æ„é€‚é…å™¨èšåˆè€Œä¸å¼•å…¥å™ªå£°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHSplitLoRAåœ¨è®­ç»ƒå‡†ç¡®æ€§å’Œæ”¶æ•›é€Ÿåº¦ä¸Šä¼˜äºç°æœ‰çš„åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¼‚æ„å®¢æˆ·ç«¯è®¾å¤‡ä¸Šé«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´çš„è®¡ç®—èµ„æºä¸è¶³å’Œé«˜æˆæœ¬é—®é¢˜ã€‚ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†è¿™äº›é—®é¢˜æ—¶å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œé€‚åº”æ€§å·®çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHSplitLoRAçš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆåˆ†å‰²å­¦ä¹ å’Œä½ç§©é€‚åº”ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°å’Œé€‚é…å™¨é…ç½®æ¥é€‚åº”ä¸åŒè®¾å¤‡çš„è®¡ç®—èƒ½åŠ›ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¾®è°ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œè¯†åˆ«å¯¹è®­ç»ƒè´¡çŒ®å¤§çš„æƒé‡ï¼›å…¶æ¬¡ï¼ŒåŠ¨æ€é…ç½®LoRAé€‚é…å™¨çš„åˆ†è§£ç§©ï¼›æœ€åï¼Œè®¾è®¡æ— å™ªå£°çš„é€‚é…å™¨èšåˆæœºåˆ¶ï¼Œä»¥æ”¯æŒå¼‚æ„é€‚é…å™¨çš„æœ‰æ•ˆæ•´åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šHSplitLoRAçš„åˆ›æ–°åœ¨äºå…¶å¼‚æ„å‚æ•°é«˜æ•ˆå¾®è°ƒçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡åŠ¨æ€é…ç½®é€‚é…å™¨å’Œæ¨¡å‹åˆ†å‰²ç‚¹æ¥é€‚åº”ä¸åŒè®¡ç®—é¢„ç®—ï¼Œè¿™ä¸ä¼ ç»Ÿçš„é™æ€å¾®è°ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒHSplitLoRAé‡‡ç”¨äº†åŸºäºæƒé‡è´¡çŒ®çš„åŠ¨æ€é€‰æ‹©æœºåˆ¶ï¼Œè®¾ç½®äº†é€‚é…å™¨çš„åˆ†è§£ç§©ï¼Œå¹¶å¼•å…¥äº†æ— å™ªå£°èšåˆç®—æ³•ï¼Œä»¥ç¡®ä¿åœ¨å¼‚æ„ç¯å¢ƒä¸‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHSplitLoRAåœ¨è®­ç»ƒå‡†ç¡®æ€§å’Œæ”¶æ•›é€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œå…·ä½“è¡¨ç°ä¸ºè®­ç»ƒå‡†ç¡®æ€§æå‡äº†çº¦15%ï¼Œæ”¶æ•›é€Ÿåº¦æé«˜äº†20%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HSplitLoRAçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„åœºæ™¯ä¸­ï¼Œå¦‚åŒ»ç–—ã€é‡‘èå’Œä¸ªæ€§åŒ–æ¨èç­‰é¢†åŸŸã€‚é€šè¿‡åœ¨å¼‚æ„è®¾å¤‡ä¸Šé«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½çš„åº”ç”¨ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, large language models (LLMs) have achieved remarkable breakthroughs, revolutionizing the natural language processing domain and beyond. Due to immense parameter sizes, fine-tuning these models with private data for diverse downstream tasks has become mainstream. Though federated learning (FL) offers a promising solution for fine-tuning LLMs without sharing raw data, substantial computing costs hinder its democratization. Moreover, in real-world scenarios, private client devices often possess heterogeneous computing resources, further complicating LLM fine-tuning. To combat these challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient fine-tuning (PEFT) framework built on split learning (SL) and low-rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on heterogeneous client devices. HSplitLoRA first identifies important weights based on their contributions to LLM training. It then dynamically configures the decomposition ranks of LoRA adapters for selected weights and determines the model split point according to varying computing budgets of client devices. Finally, a noise-free adapter aggregation mechanism is devised to support heterogeneous adapter aggregation without introducing noise. Extensive experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

