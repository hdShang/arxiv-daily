---
layout: default
title: Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL
---

# Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02391" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02391v1</a>
  <a href="https://arxiv.org/pdf/2505.02391.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02391v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02391v1', 'Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiarui Yao, Yifan Hao, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, Tong Zhang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/RLHFlow/GVM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGVM-RAFTä»¥ä¼˜åŒ–é“¾å¼æ¨ç†æ¨¡å‹çš„æ¢¯åº¦æ–¹å·®**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é“¾å¼æ¨ç†` `éšæœºæ¢¯åº¦` `åŠ¨æ€æ ·æœ¬åˆ†é…` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹è®­ç»ƒ` `æ•ˆç‡æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é“¾å¼æ¨ç†è®­ç»ƒæ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†ä¸åŒæç¤ºçš„éš¾åº¦å’Œæ”¶æ•›è¡Œä¸ºï¼Œå¯¼è‡´éšæœºæ¢¯åº¦ä¼°è®¡æ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºGVM-RAFTï¼Œé€šè¿‡åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥æ¥æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGVM-RAFTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†2-4å€çš„é€Ÿåº¦æå‡å’Œæ˜¾è‘—çš„å‡†ç¡®æ€§æ”¹å–„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“¾å¼æ¨ç†ï¼ˆCoTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¯ä»¥è¢«å½¢å¼åŒ–ä¸ºä¸€ä¸ªæ½œå˜é‡é—®é¢˜ï¼Œæ¨¡å‹éœ€è¦ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚ç°æœ‰æ–¹æ³•å¦‚è¿­ä»£å¥–åŠ±æ’åå¾®è°ƒï¼ˆRAFTï¼‰é€šå¸¸åœ¨æç¤ºä¸Šåº”ç”¨å‡åŒ€çš„æ¨ç†é¢„ç®—ï¼Œæœªèƒ½è€ƒè™‘éš¾åº¦å’Œæ”¶æ•›è¡Œä¸ºçš„å˜å¼‚æ€§ã€‚æœ¬ç ”ç©¶è¯†åˆ«å‡ºCoTè®­ç»ƒçš„ä¸»è¦ç“¶é¢ˆåœ¨äºç”±äºé™æ€é‡‡æ ·ç­–ç•¥å¯¼è‡´çš„éšæœºæ¢¯åº¦ä¼°è®¡æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†GVM-RAFTï¼Œä¸€ç§é’ˆå¯¹æç¤ºçš„åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼Œæ—¨åœ¨åœ¨è®¡ç®—é¢„ç®—çº¦æŸä¸‹æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›‘æ§æç¤ºæ¥å—ç‡å’Œéšæœºæ¢¯åº¦èŒƒæ•°åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œç¡®ä¿æœ€ç»ˆçš„æ¢¯åº¦æ–¹å·®æœ€å°åŒ–ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥åœ¨é€‚å½“æ¡ä»¶ä¸‹èƒ½å¤ŸåŠ é€Ÿæ”¶æ•›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGVM-RAFTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†2-4å€çš„åŠ é€Ÿå’Œæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³é“¾å¼æ¨ç†ï¼ˆCoTï¼‰è®­ç»ƒä¸­éšæœºæ¢¯åº¦ä¼°è®¡æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚RAFTåœ¨å¤„ç†ä¸åŒæç¤ºæ—¶é‡‡ç”¨é™æ€é‡‡æ ·ç­–ç•¥ï¼Œæœªèƒ½æœ‰æ•ˆåº”å¯¹æç¤ºçš„éš¾åº¦å˜åŒ–ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼ˆGVM-RAFTï¼‰ï¼Œé€šè¿‡å®æ—¶ç›‘æ§æç¤ºçš„æ¥å—ç‡å’Œéšæœºæ¢¯åº¦èŒƒæ•°ï¼ŒåŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºçš„åˆ†é…ï¼Œä»¥æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGVM-RAFTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨æ€æ ·æœ¬åˆ†é…æ¨¡å—ã€æ¢¯åº¦è®¡ç®—æ¨¡å—å’Œæ”¶æ•›ç›‘æ§æ¨¡å—ã€‚åŠ¨æ€æ ·æœ¬åˆ†é…æ¨¡å—æ ¹æ®å®æ—¶æ•°æ®è°ƒæ•´è®¡ç®—é¢„ç®—ï¼Œæ¢¯åº¦è®¡ç®—æ¨¡å—è´Ÿè´£ç”Ÿæˆå’Œæ›´æ–°æ¨¡å‹çš„æ¢¯åº¦ï¼Œæ”¶æ•›ç›‘æ§æ¨¡å—åˆ™è¯„ä¼°è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ”¶æ•›æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šGVM-RAFTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®æç¤ºçš„å…·ä½“æƒ…å†µçµæ´»è°ƒæ•´è®¡ç®—èµ„æºã€‚è¿™ä¸€ç­–ç•¥ä¸ç°æœ‰çš„é™æ€é‡‡æ ·æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GVM-RAFTä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬åŠ¨æ€è°ƒæ•´çš„è®¡ç®—é¢„ç®—ã€æç¤ºæ¥å—ç‡çš„é˜ˆå€¼å’Œéšæœºæ¢¯åº¦èŒƒæ•°çš„ç›‘æ§æœºåˆ¶ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†ä¼ ç»Ÿçš„å¥–åŠ±ä¿¡å·ä¸æ–°çš„åŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆæ”¶æ•›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGVM-RAFTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†2-4å€çš„é€Ÿåº¦æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„RAFTæ–¹æ³•ï¼Œå‡†ç¡®æ€§ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚è¿™è¡¨æ˜åŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢å…·æœ‰é‡è¦ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ¨ç†ä»»åŠ¡ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿä»¥åŠå…¶ä»–éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„AIç³»ç»Ÿã€‚é€šè¿‡æé«˜é“¾å¼æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒGVM-RAFTèƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´å¼ºå¤§çš„æ”¯æŒï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.

