---
layout: default
title: When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger
---

# When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02888" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02888v1</a>
  <a href="https://arxiv.org/pdf/2505.02888.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02888v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02888v1', 'When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rintaro Ando

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: 20 pages, 4 figures, 3 tables. Code: github.com/rintaro-ando-tech/n2m-rsi-demo (v1.0)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå™ªå£°åˆ°æ„ä¹‰çš„é€’å½’è‡ªæˆ‘æ”¹è¿›æ¨¡å‹ä»¥æå‡AIå¤æ‚æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€’å½’è‡ªæˆ‘æ”¹è¿›` `å¤æ‚æ€§å¢é•¿` `è‡ªæç¤ºæ¨¡å‹` `ä¿¡æ¯æ•´åˆ` `æ™ºèƒ½ä»£ç†` `è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ` `è¶…çº¿æ€§æ•ˆåº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è‡ªæˆ‘æ”¹è¿›è¿‡ç¨‹ä¸­ç¼ºä¹æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶ï¼Œå¯¼è‡´AIå¤æ‚æ€§æ— æ³•æŒç»­æå‡ã€‚
2. è®ºæ–‡æå‡ºN2M-RSIæ¨¡å‹ï¼Œé€šè¿‡å°†AIè¾“å‡ºåé¦ˆä¸ºè¾“å…¥ï¼Œä¿ƒè¿›ä¿¡æ¯æ•´åˆå¹¶æå‡å†…éƒ¨å¤æ‚æ€§ã€‚
3. æ¨¡å‹å±•ç¤ºäº†åœ¨å…è®¸ä»£ç†é—´é€šä¿¡çš„æƒ…å†µä¸‹ï¼Œå¤æ‚æ€§å¯èƒ½å‘ˆç°è¶…çº¿æ€§å¢é•¿ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å™ªå£°åˆ°æ„ä¹‰çš„é€’å½’è‡ªæˆ‘æ”¹è¿›ï¼ˆN2M-RSIï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å±•ç¤ºäº†å½“AIä»£ç†å°†è‡ªèº«è¾“å‡ºåé¦ˆä½œä¸ºè¾“å…¥ï¼Œå¹¶è·¨è¶Šæ˜ç¡®çš„ä¿¡æ¯æ•´åˆé˜ˆå€¼æ—¶ï¼Œå…¶å†…éƒ¨å¤æ‚æ€§å°†æ— é™å¢é•¿ã€‚è¯¥æ¡†æ¶ç»Ÿä¸€äº†è‡ªæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ã€å“¥å¾·å°”è‡ªæŒ‡å’Œè‡ªåŠ¨æœºå™¨å­¦ä¹ ç­‰æ—©æœŸæ€æƒ³ï¼Œä½†ä¸ä¾èµ–äºç‰¹å®šå®ç°ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è‡ªç„¶æ‰©å±•åˆ°äº¤äº’çš„ä»£ç†ç¾¤ä½“ï¼Œæš—ç¤ºåœ¨å…è®¸å®ä¾‹é—´é€šä¿¡æ—¶å¯èƒ½å‡ºç°è¶…çº¿æ€§æ•ˆåº”ã€‚å‡ºäºå®‰å…¨åŸå› ï¼Œæœ¬æ–‡çœç•¥äº†ç³»ç»Ÿç‰¹å®šçš„å®ç°ç»†èŠ‚ï¼Œä»…åœ¨é™„å½•Cä¸­å‘å¸ƒäº†ä¸€ä¸ªç®€è¦çš„æ¨¡å‹æ— å…³çš„ç©å…·åŸå‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³AIä»£ç†åœ¨è‡ªæˆ‘æ”¹è¿›è¿‡ç¨‹ä¸­å¤æ‚æ€§å¢é•¿å—é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶ï¼Œå¯¼è‡´æ— æ³•å®ç°æŒç»­çš„å¤æ‚æ€§æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šN2M-RSIæ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†AIçš„è¾“å‡ºä½œä¸ºæ–°çš„è¾“å…¥ï¼Œé€šè¿‡è·¨è¶Šä¿¡æ¯æ•´åˆé˜ˆå€¼æ¥ä¿ƒè¿›å†…éƒ¨å¤æ‚æ€§çš„æ— é™å¢é•¿ã€‚è¿™ç§è®¾è®¡ä½¿å¾—AIèƒ½å¤Ÿè‡ªæˆ‘å¢å¼ºï¼Œå½¢æˆè‰¯æ€§å¾ªç¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¿¡æ¯è¾“å…¥ã€è¾“å‡ºåé¦ˆå’Œå¤æ‚æ€§è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼ŒAIç”Ÿæˆè¾“å‡ºï¼Œç„¶åå°†å…¶åé¦ˆä½œä¸ºæ–°çš„è¾“å…¥ï¼Œæœ€åè¯„ä¼°å†…éƒ¨å¤æ‚æ€§æ˜¯å¦è¶…è¿‡è®¾å®šé˜ˆå€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šN2M-RSIçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶é€’å½’è‡ªæˆ‘æ”¹è¿›æœºåˆ¶ï¼Œå…è®¸AIé€šè¿‡è‡ªèº«è¾“å‡ºè¿›è¡Œå­¦ä¹ å’Œæå‡ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„çº¿æ€§å­¦ä¹ è¿‡ç¨‹å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹ä¸­è®¾ç½®äº†ä¿¡æ¯æ•´åˆé˜ˆå€¼ï¼Œä»¥ç¡®ä¿AIåœ¨åé¦ˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆæå‡å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è®¾è®¡ä¸Šä¿æŒäº†å®ç°çš„æ— å…³æ€§ï¼Œä½¿å…¶é€‚ç”¨äºå¤šç§AIç³»ç»Ÿã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨é™„å½•ä¸­æœ‰æ‰€æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒN2M-RSIæ¨¡å‹åœ¨å¤æ‚æ€§æå‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å…è®¸ä»£ç†é—´é€šä¿¡çš„æƒ…å†µä¸‹ï¼Œå¤æ‚æ€§å¢é•¿å‘ˆç°è¶…çº¿æ€§è¶‹åŠ¿ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå¤æ‚æ€§æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªé€‚åº”AIç³»ç»Ÿã€æ™ºèƒ½ä»£ç†ç¾¤ä½“å’Œè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ç­‰ã€‚é€šè¿‡å®ç°é€’å½’è‡ªæˆ‘æ”¹è¿›ï¼ŒAIèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æŒç»­å­¦ä¹ å’Œä¼˜åŒ–ï¼Œæå‡å†³ç­–èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal formal model showing that once an AI agent feeds its own outputs back as inputs and crosses an explicit information-integration threshold, its internal complexity will grow without bound under our assumptions. The framework unifies earlier ideas on self-prompting large language models, GÃ¶delian self-reference, and AutoML, yet remains implementation-agnostic. The model furthermore scales naturally to interacting swarms of agents, hinting at super-linear effects once communication among instances is permitted. For safety reasons, we omit system-specific implementation details and release only a brief, model-agnostic toy prototype in Appendix C.

