---
layout: default
title: RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference
---

# RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02922" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02922v2</a>
  <a href="https://arxiv.org/pdf/2505.02922.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02922v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02922v2', 'RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yaoqi Chen, Jinkai Zhang, Baotong Lu, Qianxi Zhang, Chengruidong Zhang, Jingjia Luo, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-06-30)

**å¤‡æ³¨**: 17 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRetroInferä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿ä¸Šä¸‹æ–‡æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `å‘é‡å­˜å‚¨` `æ³¨æ„åŠ›æœºåˆ¶` `GPUä¼˜åŒ–` `æ¨ç†åŠ é€Ÿ` `ç¨€ç–æ€§æ–¹æ³•` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†é¢ä¸´GPUå†…å­˜å’Œå¸¦å®½é™åˆ¶ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šå­˜åœ¨ä¸è¶³ã€‚
2. RetroInferé€šè¿‡å°†KVç¼“å­˜è§†ä¸ºå‘é‡å­˜å‚¨ç³»ç»Ÿï¼Œåˆ©ç”¨æ³¨æ„åŠ›ç¨€ç–æ€§æ¥åŠ é€Ÿæ¨ç†ï¼Œæ ¸å¿ƒæ˜¯æ³¢åŠ¨ç´¢å¼•å’Œæ³¢åŠ¨ç¼“å†²åŒºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRetroInferåœ¨GPUå†…å­˜é™åˆ¶ä¸‹é€Ÿåº¦æå‡4.5å€ï¼Œåœ¨æ‰©å±•åˆ°CPUå†…å­˜æ—¶é€Ÿåº¦æå‡10.5å€ï¼Œä¸”ä¿æŒé«˜å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œæ¨ç†æ•ˆç‡é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºGPUå†…å­˜å’Œå¸¦å®½çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†RetroInferï¼Œä¸€ä¸ªæ–°é¢–çš„ç³»ç»Ÿï¼Œå°†å…³é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜é‡æ–°æ¦‚å¿µåŒ–ä¸ºå‘é‡å­˜å‚¨ç³»ç»Ÿï¼Œåˆ©ç”¨å†…åœ¨çš„æ³¨æ„åŠ›ç¨€ç–æ€§åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ã€‚å…¶æ ¸å¿ƒæ˜¯æ³¢åŠ¨ç´¢å¼•ï¼ˆwave indexï¼‰ï¼Œä¸€ç§æ³¨æ„åŠ›æ„ŸçŸ¥çš„å‘é‡ç´¢å¼•ï¼Œèƒ½å¤Ÿé€šè¿‡ä¸‰åˆ†æ³¨æ„åŠ›è¿‘ä¼¼ã€ç²¾åº¦å—é™çš„æ³¨æ„åŠ›ä¼°è®¡å’Œåˆ†æ®µèšç±»ç­‰æŠ€æœ¯é«˜æ•ˆå‡†ç¡®åœ°æ£€ç´¢å…³é”®æ ‡è®°ã€‚æ­¤å¤–ï¼Œæ³¢åŠ¨ç¼“å†²åŒºï¼ˆwave bufferï¼‰åè°ƒKVç¼“å­˜çš„æ”¾ç½®ï¼Œå¹¶é‡å GPUå’ŒCPUä¹‹é—´çš„è®¡ç®—ä¸æ•°æ®ä¼ è¾“ï¼Œä»¥ç»´æŒé«˜ååé‡ã€‚ä¸å…ˆå‰çš„ç¨€ç–æ€§æ–¹æ³•ä¸åŒï¼ŒRetroInferåœ¨ä¸å¦¥åæ¨¡å‹å‡†ç¡®æ€§çš„æƒ…å†µä¸‹ï¼Œæä¾›äº†ç¨³å¥çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨GPUå†…å­˜é™åˆ¶ä¸‹ï¼Œç›¸æ¯”å…¨æ³¨æ„åŠ›ï¼Œé€Ÿåº¦æå‡å¯è¾¾4.5å€ï¼Œè€Œåœ¨KVç¼“å­˜æ‰©å±•åˆ°CPUå†…å­˜æ—¶ï¼Œç›¸æ¯”ç¨€ç–æ³¨æ„åŠ›åŸºçº¿ï¼Œé€Ÿåº¦æå‡å¯è¾¾10.5å€ï¼ŒåŒæ—¶ä¿æŒå…¨æ³¨æ„åŠ›çº§åˆ«çš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡ä¸Šä¸‹æ–‡æ—¶ï¼Œå¾€å¾€å—åˆ°GPUå†…å­˜å’Œå¸¦å®½çš„é™åˆ¶ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ç¼“æ…¢å’Œèµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRetroInferçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¼ ç»Ÿçš„KVç¼“å­˜é‡æ–°æ„å»ºä¸ºå‘é‡å­˜å‚¨ç³»ç»Ÿï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„ç¨€ç–æ€§æ¥ä¼˜åŒ–å…³é”®æ ‡è®°çš„æ£€ç´¢æ•ˆç‡ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRetroInferçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ³¢åŠ¨ç´¢å¼•å’Œæ³¢åŠ¨ç¼“å†²åŒºä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚æ³¢åŠ¨ç´¢å¼•è´Ÿè´£é«˜æ•ˆæ£€ç´¢å…³é”®æ ‡è®°ï¼Œè€Œæ³¢åŠ¨ç¼“å†²åŒºåˆ™ä¼˜åŒ–KVç¼“å­˜çš„æ”¾ç½®å’ŒGPUä¸CPUä¹‹é—´çš„è®¡ç®—ä¸æ•°æ®ä¼ è¾“ã€‚

**å…³é”®åˆ›æ–°**ï¼šRetroInferçš„å…³é”®åˆ›æ–°åœ¨äºæ³¢åŠ¨ç´¢å¼•çš„è®¾è®¡ï¼Œé‡‡ç”¨äº†ä¸‰åˆ†æ³¨æ„åŠ›è¿‘ä¼¼å’Œç²¾åº¦å—é™çš„æ³¨æ„åŠ›ä¼°è®¡ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒåŒºåˆ«äºä¼ ç»Ÿç¨€ç–æ€§æ–¹æ³•åœ¨æ ‡è®°é€‰æ‹©å’Œç¡¬ä»¶åè°ƒä¸Šçš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ³¢åŠ¨ç´¢å¼•ä½¿ç”¨äº†åˆ†æ®µèšç±»æŠ€æœ¯ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„æ ‡è®°æ£€ç´¢ï¼›æ³¢åŠ¨ç¼“å†²åŒºåˆ™é€šè¿‡é‡å è®¡ç®—å’Œæ•°æ®ä¼ è¾“ï¼Œæœ€å¤§åŒ–äº†èµ„æºåˆ©ç”¨ç‡ï¼Œæå‡äº†æ•´ä½“æ¨ç†é€Ÿåº¦ã€‚å®éªŒä¸­è¿˜å¯¹å‚æ•°è®¾ç½®è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒç¡¬ä»¶ç¯å¢ƒä¸‹çš„æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRetroInferåœ¨GPUå†…å­˜é™åˆ¶ä¸‹ï¼Œç›¸æ¯”å…¨æ³¨æ„åŠ›æ–¹æ³•å®ç°äº†é«˜è¾¾4.5å€çš„é€Ÿåº¦æå‡ï¼Œè€Œåœ¨KVç¼“å­˜æ‰©å±•åˆ°CPUå†…å­˜æ—¶ï¼Œç›¸æ¯”ç¨€ç–æ³¨æ„åŠ›åŸºçº¿å®ç°äº†é«˜è¾¾10.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å…¨æ³¨æ„åŠ›çº§åˆ«çš„å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RetroInferçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿æ–‡æœ¬æˆ–ä¸Šä¸‹æ–‡çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚å…¶é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›èƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›åº”ç”¨çš„å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤§è§„æ¨¡çš„LLMåœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.

