---
layout: default
title: "LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction"
---

# LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02880" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02880v1</a>
  <a href="https://arxiv.org/pdf/2505.02880.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02880v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02880v1', 'LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zian Liu, Renjun Jia

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: 12 pages, 9figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLM4FTSæ¡†æ¶ä»¥æå‡é‡‘èæ—¶é—´åºåˆ—é¢„æµ‹èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡‘èæ—¶é—´åºåˆ—` `å¤§è¯­è¨€æ¨¡å‹` `åŠ¨æ€å°æ³¢å·ç§¯` `è¡¥ä¸åˆ†å‰²` `æ¨¡å¼è¯†åˆ«` `è‚¡ç¥¨é¢„æµ‹` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ–¹æ³•é€šå¸¸ä¸“æ³¨äºå›ºå®šé•¿åº¦çš„è¡¥ä¸åˆ†æï¼Œå¿½ç•¥äº†å¸‚åœºæ•°æ®çš„å¤šå°ºåº¦æ¨¡å¼ç‰¹å¾ï¼Œå¯¼è‡´é¢„æµ‹æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„LLM4FTSæ¡†æ¶é€šè¿‡å¯å­¦ä¹ çš„è¡¥ä¸åˆ†å‰²å’ŒåŠ¨æ€å°æ³¢å·ç§¯æ¨¡å—ï¼Œå¢å¼ºäº†LLMåœ¨æ—¶é—´åºåˆ—å»ºæ¨¡ä¸­çš„èƒ½åŠ›ã€‚
3. åœ¨çœŸå®é‡‘èæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ•æ‰å¤æ‚å¸‚åœºæ¨¡å¼å’Œè‚¡ç¥¨æ”¶ç›Šé¢„æµ‹ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„ç ”ç©¶æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é‡‘èæ—¶é—´åºåˆ—é¢„æµ‹é¢ä¸´ä½ä¿¡å™ªæ¯”å’Œå¤æ‚æ—¶é—´æ¨¡å¼çš„æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­èƒ½åŠ›æœ‰é™ã€‚æœ¬æ–‡æå‡ºLLM4FTSæ¡†æ¶ï¼Œé€šè¿‡å¯å­¦ä¹ çš„è¡¥ä¸åˆ†å‰²å’ŒåŠ¨æ€å°æ³¢å·ç§¯æ¨¡å—ï¼Œå¢å¼ºå¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—å»ºæ¨¡ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºDTWè·ç¦»çš„K-means++èšç±»è¯†åˆ«å¸‚åœºæ•°æ®ä¸­çš„å°ºåº¦ä¸å˜æ¨¡å¼ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”è¡¥ä¸åˆ†å‰²ä»¥ä¿æŒæ¨¡å¼å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€å°æ³¢å·ç§¯æ¨¡å—æ¨¡æ‹Ÿç¦»æ•£å°æ³¢å˜æ¢ï¼Œçµæ´»æ•æ‰æ—¶é—´-é¢‘ç‡ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ•æ‰å¤æ‚å¸‚åœºæ¨¡å¼å’Œè‚¡ç¥¨æ”¶ç›Šé¢„æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å®é™…äº¤æ˜“ç³»ç»Ÿçš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é‡‘èæ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ä½ä¿¡å™ªæ¯”å’Œå¤æ‚æ—¶é—´æ¨¡å¼é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šå°ºåº¦æ¨¡å¼æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´é¢„æµ‹æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è¡¥ä¸åˆ†å‰²å’ŒåŠ¨æ€å°æ³¢å·ç§¯æ¨¡å—ï¼Œå¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¸‚åœºæ•°æ®ä¸­çš„å¤æ‚ä¾èµ–å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLLM4FTSæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šåŸºäºDTWè·ç¦»çš„K-means++èšç±»ç”¨äºè¯†åˆ«å°ºåº¦ä¸å˜æ¨¡å¼ï¼Œè‡ªé€‚åº”è¡¥ä¸åˆ†å‰²ç”¨äºä¿æŒæ¨¡å¼å®Œæ•´æ€§ï¼Œä»¥åŠåŠ¨æ€å°æ³¢å·ç§¯æ¨¡å—ç”¨äºæ•æ‰æ—¶é—´-é¢‘ç‡ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåŠ¨æ€å°æ³¢å·ç§¯æ¨¡å—çš„è®¾è®¡ï¼Œå®ƒæä¾›äº†æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´é«˜çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿé€‚åº”æ—¶é—´å˜åŒ–çš„é¢‘ç‡ç‰¹å¾ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¡¥ä¸åˆ†å‰²ä¸­ï¼Œé‡‡ç”¨è‡ªé€‚åº”ç®—æ³•ä»¥ç¡®ä¿æœ€å¤§åŒ–æ¨¡å¼å®Œæ•´æ€§ï¼›åŠ¨æ€å°æ³¢å·ç§¯æ¨¡å—çš„å‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼Œä»¥å¢å¼ºå…¶åœ¨æ•æ‰å¤æ‚å¸‚åœºæ¨¡å¼æ—¶çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨çœŸå®é‡‘èæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM4FTSæ¡†æ¶åœ¨è‚¡ç¥¨æ”¶ç›Šé¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œé¢„æµ‹å‡†ç¡®ç‡æå‡äº†15%ä»¥ä¸Šï¼Œå±•ç°äº†å…¶åœ¨å¤æ‚å¸‚åœºæ¨¡å¼æ•æ‰ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºåˆ†æã€æŠ•èµ„ç­–ç•¥ä¼˜åŒ–å’Œé£é™©ç®¡ç†ç­‰ã€‚é€šè¿‡æé«˜é‡‘èæ—¶é—´åºåˆ—é¢„æµ‹çš„å‡†ç¡®æ€§ï¼ŒLLM4FTSæ¡†æ¶èƒ½å¤Ÿä¸ºæŠ•èµ„è€…æä¾›æ›´å¯é çš„å†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Predicting financial time series presents significant challenges due to inherent low signal-to-noise ratios and intricate temporal patterns. Traditional machine learning models exhibit limitations in this forecasting task constrained by their restricted model capacity. Recent advances in large language models (LLMs), with their greatly expanded parameter spaces, demonstrate promising potential for modeling complex dependencies in temporal sequences. However, existing LLM-based approaches typically focus on fixed-length patch analysis due to the Transformer architecture, ignoring market data's multi-scale pattern characteristics. In this study, we propose $LLM4FTS$, a novel framework that enhances LLM capabilities for temporal sequence modeling through learnable patch segmentation and dynamic wavelet convolution modules. Specifically,we first employ K-means++ clustering based on DTW distance to identify scale-invariant patterns in market data. Building upon pattern recognition results, we introduce adaptive patch segmentation that partitions temporal sequences while preserving maximal pattern integrity. To accommodate time-varying frequency characteristics, we devise a dynamic wavelet convolution module that emulates discrete wavelet transformation with enhanced flexibility in capturing time-frequency features. These three modules work together to improve large language model's ability to handle scale-invariant patterns in financial time series. Extensive experiments on real-world financial datasets substantiate the framework's efficacy, demonstrating superior performance in capturing complex market patterns and achieving state-of-the-art results in stock return prediction. The successful deployment in practical trading systems confirms its real-world applicability, representing a significant advancement in LLM applications for financial forecasting.

