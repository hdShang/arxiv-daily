---
layout: default
title: "T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models"
---

# T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02417" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02417v2</a>
  <a href="https://arxiv.org/pdf/2505.02417.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02417v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02417v2', 'T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunfeng Ge, Jiawei Li, Yiji Zhao, Haomin Wen, Zhao Li, Meikang Qiu, Hongyan Li, Ming Jin, Shirui Pan

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-05-08)

**å¤‡æ³¨**: Accepted by the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºT2Sæ¡†æ¶ä»¥è§£å†³æ—¶é—´åºåˆ—ç”Ÿæˆä¸­çš„æ•°æ®ç¨€ç–ä¸ä¸å¹³è¡¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `å˜åˆ†è‡ªç¼–ç å™¨` `å¤šæ¨¡æ€æ•°æ®` `æµåŒ¹é…` `æ–‡æœ¬å¯¹é½` `æ•°æ®ç¨€ç–` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¶é—´åºåˆ—ç”Ÿæˆæ–¹æ³•é¢ä¸´æ•°æ®ç¨€ç–å’Œä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ï¼Œä¸”ç¼ºä¹å¯¹é€šç”¨æ—¶é—´åºåˆ—æè¿°çš„ç³»ç»Ÿæ¢ç´¢ã€‚
2. è®ºæ–‡æå‡ºäº†T2Sæ¡†æ¶ï¼Œé€šè¿‡é•¿åº¦è‡ªé€‚åº”å˜åˆ†è‡ªç¼–ç å™¨å’Œæ‰©æ•£å˜æ¢å™¨æœ‰æ•ˆè¿æ¥æ–‡æœ¬ä¸æ—¶é—´åºåˆ—ã€‚
3. T2Såœ¨13ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆä»»æ„é•¿åº¦æ—¶é—´åºåˆ—æ–¹é¢çš„èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æœ¬åˆ°æ—¶é—´åºåˆ—ç”Ÿæˆåœ¨è§£å†³æ•°æ®ç¨€ç–ã€ä¸å¹³è¡¡å’Œå¤šæ¨¡æ€æ—¶é—´åºåˆ—æ•°æ®é›†æœ‰é™æ€§æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å…¶ä»–ç±»å‹æ•°æ®ç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨æ—¶é—´åºåˆ—ç”Ÿæˆä¸­çš„åº”ç”¨ä»å¤„äºåˆæ­¥é˜¶æ®µã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šä¸€æ˜¯ç¼ºä¹å¯¹é€šç”¨æ—¶é—´åºåˆ—æè¿°çš„ç³»ç»Ÿæ¢ç´¢ï¼ŒäºŒæ˜¯æ— æ³•ç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´åºåˆ—ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡é¦–å…ˆå°†æ—¶é—´åºåˆ—æè¿°åˆ†ä¸ºç‚¹çº§ã€ç‰‡æ®µçº§å’Œå®ä¾‹çº§ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªåŒ…å«è¶…è¿‡60ä¸‡ä¸ªé«˜åˆ†è¾¨ç‡æ—¶é—´åºåˆ—-æ–‡æœ¬å¯¹çš„æ–°æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†T2Sæ¡†æ¶ï¼Œä»¥é¢†åŸŸæ— å…³çš„æ–¹å¼è¿æ¥è‡ªç„¶è¯­è¨€ä¸æ—¶é—´åºåˆ—ï¼Œé‡‡ç”¨é•¿åº¦è‡ªé€‚åº”å˜åˆ†è‡ªç¼–ç å™¨ç¼–ç ä¸åŒé•¿åº¦çš„æ—¶é—´åºåˆ—ï¼Œå¹¶é€šè¿‡æµåŒ¹é…å’Œæ‰©æ•£å˜æ¢å™¨å¯¹é½æ–‡æœ¬è¡¨ç¤ºä¸æ½œåœ¨åµŒå…¥ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒT2Såœ¨12ä¸ªé¢†åŸŸçš„13ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ—¶é—´åºåˆ—ç”Ÿæˆä¸­çš„æ•°æ®ç¨€ç–å’Œä¸å¹³è¡¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•ç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´åºåˆ—ï¼Œé™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºT2Sæ¡†æ¶ï¼Œé€šè¿‡é•¿åº¦è‡ªé€‚åº”å˜åˆ†è‡ªç¼–ç å™¨å°†ä¸åŒé•¿åº¦çš„æ—¶é—´åºåˆ—ç¼–ç ä¸ºä¸€è‡´çš„æ½œåœ¨åµŒå…¥ï¼Œå¹¶åˆ©ç”¨æµåŒ¹é…å¯¹é½æ–‡æœ¬è¡¨ç¤ºä¸æ½œåœ¨åµŒå…¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šT2Sæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ—¶é—´åºåˆ—ç¼–ç æ¨¡å—ã€æ–‡æœ¬è¡¨ç¤ºå¯¹é½æ¨¡å—å’Œç”Ÿæˆæ¨¡å—ï¼Œæ”¯æŒç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šT2Sçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶é¢†åŸŸæ— å…³çš„ç”Ÿæˆæ–¹å¼å’Œé•¿åº¦è‡ªé€‚åº”çš„ç¼–ç èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤šæ ·åŒ–çš„æ—¶é—´åºåˆ—ç”Ÿæˆä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨è¿›è¡Œæ½œåœ¨ç©ºé—´çš„å­¦ä¹ ï¼Œè®¾è®¡äº†æµåŒ¹é…ç®—æ³•ä»¥å¢å¼ºæ–‡æœ¬ä¸æ—¶é—´åºåˆ—çš„å¯¹é½æ•ˆæœï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ä»¥æå‡ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

T2Såœ¨13ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆä»»æ„é•¿åº¦æ—¶é—´åºåˆ—æ–¹é¢çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºåˆ†æã€åŒ»ç–—ç›‘æµ‹ã€æ°”è±¡é¢„æµ‹ç­‰ï¼Œèƒ½å¤Ÿä¸ºæ•°æ®ç¨€ç–å’Œä¸å¹³è¡¡é—®é¢˜æä¾›æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚æœªæ¥ï¼ŒT2Sæ¡†æ¶æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆçš„ç ”ç©¶è¿›å±•ï¼Œä¿ƒè¿›å„é¢†åŸŸçš„æ™ºèƒ½åŒ–åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-to-Time Series generation holds significant potential to address challenges such as data sparsity, imbalance, and limited availability of multimodal time series datasets across domains. While diffusion models have achieved remarkable success in Text-to-X (e.g., vision and audio data) generation, their use in time series generation remains in its nascent stages. Existing approaches face two critical limitations: (1) the lack of systematic exploration of general-proposed time series captions, which are often domain-specific and struggle with generalization; and (2) the inability to generate time series of arbitrary lengths, limiting their applicability to real-world scenarios. In this work, we first categorize time series captions into three levels: point-level, fragment-level, and instance-level. Additionally, we introduce a new fragment-level dataset containing over 600,000 high-resolution time series-text pairs. Second, we propose Text-to-Series (T2S), a diffusion-based framework that bridges the gap between natural language and time series in a domain-agnostic manner. T2S employs a length-adaptive variational autoencoder to encode time series of varying lengths into consistent latent embeddings. On top of that, T2S effectively aligns textual representations with latent embeddings by utilizing Flow Matching and employing Diffusion Transformer as the denoiser. We train T2S in an interleaved paradigm across multiple lengths, allowing it to generate sequences of any desired length. Extensive evaluations demonstrate that T2S achieves state-of-the-art performance across 13 datasets spanning 12 domains.

