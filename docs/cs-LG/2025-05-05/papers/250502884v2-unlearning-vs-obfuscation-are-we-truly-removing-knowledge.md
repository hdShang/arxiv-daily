---
layout: default
title: "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?"
---

# Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02884" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02884v2</a>
  <a href="https://arxiv.org/pdf/2505.02884.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02884v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02884v2', 'Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guangzhi Sun, Potsawee Manakul, Xiao Zhan, Mark Gales

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-09-08)

**å¤‡æ³¨**: To Appear in EMNLP 2025 main conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDF-MCQä»¥è§£å†³çŸ¥è¯†ç§»é™¤çš„æœ‰æ•ˆæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†ç§»é™¤` `æ¨¡ç³Šå¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®éšç§` `KLæ•£åº¦` `æ¢æµ‹è¯„ä¼°` `å¤šé¡¹é€‰æ‹©é¢˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŸ¥è¯†ç§»é™¤æ–¹æ³•å¤šä¾èµ–æ¨¡ç³Šå¤„ç†ï¼Œæœªèƒ½æœ‰æ•ˆå»é™¤æ¨¡å‹ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼Œå¯¼è‡´éšç§é£é™©ã€‚
2. æœ¬æ–‡æå‡ºDF-MCQï¼Œé€šè¿‡KLæ•£åº¦ä¼˜åŒ–æ¨¡å‹é¢„æµ‹åˆ†å¸ƒï¼Œæ—¨åœ¨å®ç°çœŸæ­£çš„çŸ¥è¯†ç§»é™¤ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDF-MCQåœ¨æ‹’ç»ç‡å’Œä¸ç¡®å®šæ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡ç³Šå¤„ç†æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†ç§»é™¤å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ”¯æŒæ•°æ®éšç§å’Œåˆè§„æ€§çš„é‡è¦èƒ½åŠ›ã€‚ç°æœ‰æŠ€æœ¯å¾€å¾€ä¾èµ–äºæ¨¡ç³Šå¤„ç†ï¼Œé€šè¿‡æ³¨å…¥é”™è¯¯æˆ–æ— å…³ä¿¡æ¯æ¥æŠ‘åˆ¶çŸ¥è¯†ï¼Œè¿™å®é™…ä¸Šæ„æˆäº†çŸ¥è¯†çš„æ·»åŠ è€ŒéçœŸæ­£çš„ç§»é™¤ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ¢æµ‹æ—¶ä»ç„¶è„†å¼±ã€‚æœ¬æ–‡æ­£å¼åŒºåˆ†äº†çŸ¥è¯†ç§»é™¤ä¸æ¨¡ç³Šå¤„ç†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºæ¢æµ‹çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°ç°æœ‰æ–¹æ³•æ˜¯å¦çœŸæ­£ç§»é™¤äº†ç›®æ ‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†DF-MCQï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„çŸ¥è¯†ç§»é™¤æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨KLæ•£åº¦å¹³å¦åŒ–æ¨¡å‹å¯¹è‡ªåŠ¨ç”Ÿæˆçš„å¤šé¡¹é€‰æ‹©é¢˜çš„é¢„æµ‹åˆ†å¸ƒï¼Œæœ‰æ•ˆç§»é™¤å…³äºç›®æ ‡ä¸ªä½“çš„çŸ¥è¯†ï¼Œå¹¶è§¦å‘é€‚å½“çš„æ‹’ç»è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDF-MCQåœ¨æ¢æµ‹é—®é¢˜ä¸Šçš„æ‹’ç»ç‡è¶…è¿‡90%ï¼Œä¸”éšæœºé€‰æ‹©æ°´å¹³çš„ä¸ç¡®å®šæ€§æ˜¾è‘—é«˜äºæ¨¡ç³Šå¤„ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰çŸ¥è¯†ç§»é™¤æ–¹æ³•çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯æ¨¡ç³Šå¤„ç†æœªèƒ½çœŸæ­£å»é™¤æ•æ„Ÿä¿¡æ¯çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€é€šè¿‡æ³¨å…¥æ— å…³ä¿¡æ¯æ¥æ©ç›–çŸ¥è¯†ï¼Œä½†è¿™å¹¶ä¸ç­‰åŒäºçœŸæ­£çš„çŸ¥è¯†ç§»é™¤ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ¢æµ‹æ—¶ä»ç„¶å­˜åœ¨é£é™©ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDF-MCQçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡KLæ•£åº¦æ¥å¹³å¦åŒ–æ¨¡å‹å¯¹å¤šé¡¹é€‰æ‹©é¢˜çš„é¢„æµ‹åˆ†å¸ƒï¼Œä»è€Œå®ç°å¯¹ç›®æ ‡ä¸ªä½“çŸ¥è¯†çš„æœ‰æ•ˆç§»é™¤ã€‚è¯¥æ–¹æ³•ä¸ä»…å…³æ³¨çŸ¥è¯†çš„æŠ‘åˆ¶ï¼Œæ›´å¼ºè°ƒçŸ¥è¯†çš„çœŸæ­£æ¶ˆé™¤ï¼Œç¡®ä¿æ¨¡å‹åœ¨é¢å¯¹æ¢æµ‹æ—¶èƒ½å¤Ÿåšå‡ºé€‚å½“çš„æ‹’ç»ååº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDF-MCQçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ä½œä¸ºè¾“å…¥æ•°æ®ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨KLæ•£åº¦ä¼˜åŒ–æ¨¡å‹çš„é¢„æµ‹åˆ†å¸ƒï¼›æœ€åï¼Œé€šè¿‡æ¢æµ‹æ¡†æ¶è¯„ä¼°çŸ¥è¯†ç§»é™¤çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDF-MCQçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é€šè¿‡KLæ•£åº¦å®ç°çŸ¥è¯†ç§»é™¤çš„æœºåˆ¶ï¼Œä¸ä¼ ç»Ÿçš„æ¨¡ç³Šå¤„ç†æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…ä»…ä»…æ˜¯é€šè¿‡æ·»åŠ å™ªå£°æ¥æ©ç›–çŸ¥è¯†ï¼Œè€ŒDF-MCQåˆ™æ˜¯é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ¥å®ç°çœŸæ­£çš„çŸ¥è¯†æ¶ˆé™¤ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DF-MCQä¸­ï¼Œå…³é”®çš„å‚æ•°è®¾ç½®åŒ…æ‹¬KLæ•£åº¦çš„æƒé‡ã€æ¨¡å‹çš„å­¦ä¹ ç‡ä»¥åŠå¤šé¡¹é€‰æ‹©é¢˜çš„ç”Ÿæˆç­–ç•¥ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œå¼ºè°ƒäº†å¯¹ç›®æ ‡çŸ¥è¯†çš„ç§»é™¤æ•ˆæœï¼ŒåŒæ—¶ç¡®ä¿æ¨¡å‹åœ¨æ¢æµ‹æ—¶çš„æ‹’ç»è¡Œä¸ºç¬¦åˆé¢„æœŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDF-MCQåœ¨çŸ¥è¯†ç§»é™¤æ–¹é¢çš„æ‹’ç»ç‡è¶…è¿‡90%ï¼Œè€Œåœ¨æ¢æµ‹é—®é¢˜ä¸Šçš„éšæœºé€‰æ‹©æ°´å¹³çš„ä¸ç¡®å®šæ€§æ˜¾è‘—é«˜äºä¼ ç»Ÿçš„æ¨¡ç³Šå¤„ç†æ–¹æ³•ã€‚è¿™ä¸€ç»“æœè¡¨æ˜DF-MCQåœ¨æœ‰æ•ˆæ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨çŸ¥è¯†ç§»é™¤é¢†åŸŸçš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®éšç§ä¿æŠ¤ã€åˆè§„æ€§è¦æ±‚çš„AIç³»ç»Ÿä»¥åŠéœ€è¦å¤„ç†æ•æ„Ÿä¿¡æ¯çš„æ™ºèƒ½åŠ©æ‰‹ã€‚DF-MCQçš„æœ‰æ•ˆæ€§ä¸ºå¼€å‘æ›´å®‰å…¨çš„AIæ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œæ»¡è¶³æ³•å¾‹å’Œä¼¦ç†è¦æ±‚ï¼Œæœªæ¥å¯èƒ½å¯¹AIçš„å¹¿æ³›åº”ç”¨äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.

