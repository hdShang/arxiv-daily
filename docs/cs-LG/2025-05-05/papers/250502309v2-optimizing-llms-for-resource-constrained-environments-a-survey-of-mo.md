---
layout: default
title: Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques
---

# Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02309" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02309v2</a>
  <a href="https://arxiv.org/pdf/2505.02309.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02309v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02309v2', 'Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sanjay Surendranath Girija, Shashank Kapoor, Lakshit Arora, Dipen Pradhan, Aman Raj, Ankit Shetgaonkar

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-05-08)

**å¤‡æ³¨**: Accepted to IEEE COMPSAC 2025

**æœŸåˆŠ**: 2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)

**DOI**: [10.1109/COMPSAC65507.2025.00224](https://doi.org/10.1109/COMPSAC65507.2025.00224)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°æ¨¡å‹å‹ç¼©æŠ€æœ¯ä»¥ä¼˜åŒ–èµ„æºå—é™ç¯å¢ƒä¸­çš„LLM**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹å‹ç¼©` `çŸ¥è¯†è’¸é¦` `æ¨¡å‹é‡åŒ–` `æ¨¡å‹å‰ªæ` `è¾¹ç¼˜è®¡ç®—` `è‡ªç„¶è¯­è¨€å¤„ç†` `èµ„æºä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²é¢ä¸´å·¨å¤§çš„èµ„æºéœ€æ±‚æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†çŸ¥è¯†è’¸é¦ã€æ¨¡å‹é‡åŒ–å’Œæ¨¡å‹å‰ªæç­‰å‹ç¼©æŠ€æœ¯ï¼Œä»¥æé«˜LLMsåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ¨ç†æ•ˆç‡ã€‚
3. é€šè¿‡å¯¹ä¸åŒå‹ç¼©æŠ€æœ¯çš„è¯„ä¼°ï¼Œå±•ç¤ºäº†åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½æ¨¡å‹å¤§å°å’Œè®¡ç®—éœ€æ±‚çš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½å¤šä¸ªé¢†åŸŸå¼•å‘äº†é©å‘½ï¼Œä½†å…¶å·¨å¤§çš„èµ„æºéœ€æ±‚é™åˆ¶äº†åœ¨ç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚æœ¬æ–‡ç»¼è¿°äº†å‹ç¼©LLMsçš„æŠ€æœ¯ï¼Œä»¥å®ç°èµ„æºå—é™ç¯å¢ƒä¸­çš„é«˜æ•ˆæ¨ç†ã€‚æˆ‘ä»¬è€ƒå¯Ÿäº†ä¸‰ç§ä¸»è¦æ–¹æ³•ï¼šçŸ¥è¯†è’¸é¦ã€æ¨¡å‹é‡åŒ–å’Œæ¨¡å‹å‰ªæã€‚é’ˆå¯¹æ¯ç§æŠ€æœ¯ï¼Œæˆ‘ä»¬è®¨è®ºäº†å…¶åŸºæœ¬åŸç†ã€ä¸åŒå˜ä½“ï¼Œå¹¶æä¾›äº†æˆåŠŸåº”ç”¨çš„å®ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç®€è¦è®¨è®ºäº†æ··åˆä¸“å®¶å’Œæ—©æœŸé€€å‡ºç­–ç•¥ç­‰è¡¥å……æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æœªæ¥çš„æœ‰å¸Œæœ›çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›æœ‰ä»·å€¼çš„èµ„æºï¼Œä»¥ä¼˜åŒ–LLMsåœ¨è¾¹ç¼˜éƒ¨ç½²ä¸­çš„åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²çš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ»¡è¶³ç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡çš„èµ„æºé™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å‹ç¼©æŠ€æœ¯å¦‚çŸ¥è¯†è’¸é¦ã€æ¨¡å‹é‡åŒ–å’Œæ¨¡å‹å‰ªæï¼Œé™ä½æ¨¡å‹çš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šçŸ¥è¯†è’¸é¦ç”¨äºä¼ é€’çŸ¥è¯†ï¼Œæ¨¡å‹é‡åŒ–ç”¨äºå‡å°‘æ•°å€¼è¡¨ç¤ºçš„ä½æ•°ï¼Œæ¨¡å‹å‰ªæç”¨äºå»é™¤å†—ä½™å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°è¯„ä¼°å’Œæ•´åˆå¤šç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œæä¾›äº†ä¸åŒæŠ€æœ¯çš„å˜ä½“å’Œåº”ç”¨å®ä¾‹ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨çŸ¥è¯†è’¸é¦ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹é…ç½®ï¼›åœ¨æ¨¡å‹é‡åŒ–ä¸­ï¼Œä½¿ç”¨äº†åŠ¨æ€èŒƒå›´é‡åŒ–å’Œæƒé‡å…±äº«ï¼›åœ¨æ¨¡å‹å‰ªæä¸­ï¼Œè®¾è®¡äº†åŸºäºé‡è¦æ€§çš„å‰ªæç­–ç•¥ï¼Œä»¥ç¡®ä¿æ€§èƒ½çš„æœ€å°æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨å‹ç¼©æŠ€æœ¯åï¼Œæ¨¡å‹å¤§å°å‡å°‘äº†50%ä»¥ä¸Šï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æå‡äº†30%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œç»è¿‡å‹ç¼©çš„æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¿æŒåœ¨95%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç§»åŠ¨è®¾å¤‡ä¸Šçš„è‡ªç„¶è¯­è¨€å¤„ç†ã€è¾¹ç¼˜è®¡ç®—ä¸­çš„æ™ºèƒ½åŠ©æ‰‹ä»¥åŠä½åŠŸè€—è®¾å¤‡çš„è¯­éŸ³è¯†åˆ«ç­‰ã€‚é€šè¿‡ä¼˜åŒ–LLMsçš„èµ„æºä½¿ç”¨ï¼Œè¯¥ç ”ç©¶å°†æ¨åŠ¨AIæŠ€æœ¯åœ¨æ›´å¹¿æ³›åœºæ™¯ä¸­çš„å®é™…åº”ç”¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.

