---
layout: default
title: "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons"
---

# Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01963" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01963v1</a>
  <a href="https://arxiv.org/pdf/2506.01963.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01963v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01963v1', 'Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Kiruluta, Preethi Raju, Priscilla Burity

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§éæ³¨æ„åŠ›æœºåˆ¶çš„LLMä»¥è§£å†³è¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¶…é•¿ä¸Šä¸‹æ–‡` `éæ³¨æ„åŠ›æœºåˆ¶` `å¤§å‹è¯­è¨€æ¨¡å‹` `çŠ¶æ€ç©ºé—´å—` `å¤šåˆ†è¾¨ç‡å·ç§¯` `æ£€ç´¢å¢å¼ºè®°å¿†` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Transformeræ¨¡å‹åœ¨å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡æ—¶é¢ä¸´äºŒæ¬¡å†…å­˜å’Œè®¡ç®—è´Ÿæ‹…ï¼Œé™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§éæ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„ï¼Œé€šè¿‡å¤šä¸ªäº’è¡¥ç»„ä»¶æœ‰æ•ˆå¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„ç¼ºé™·ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶æ€§èƒ½æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿæ”¯æŒæ•°ç™¾ä¸‡ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡å¤„ç†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„éæ³¨æ„åŠ›æœºåˆ¶æ¶æ„ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†æ•°åä¸‡åˆ°å¯èƒ½æ•°ç™¾ä¸‡ä¸ªæ ‡è®°çš„è¶…é•¿ä¸Šä¸‹æ–‡çª—å£ã€‚ä¸ä¼ ç»Ÿçš„Transformerè®¾è®¡ä¸åŒï¼Œè¯¥æ¨¡å‹å®Œå…¨é¿å…äº†æ ‡è®°é—´çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå…‹æœäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶å¸¦æ¥çš„äºŒæ¬¡å†…å­˜å’Œè®¡ç®—è´Ÿæ‹…ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»“åˆäº†å¤šä¸ªäº’è¡¥ç»„ä»¶ï¼šå—S4å¯å‘çš„çŠ¶æ€ç©ºé—´å—ï¼Œèƒ½å¤Ÿå­¦ä¹ è¿ç»­æ—¶é—´å·ç§¯æ ¸å¹¶åœ¨åºåˆ—é•¿åº¦ä¸Šè¿‘ä¹çº¿æ€§æ‰©å±•ï¼›å¤šåˆ†è¾¨ç‡å·ç§¯å±‚ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒè†¨èƒ€æ°´å¹³ä¸‹æ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡ï¼›è½»é‡çº§é€’å½’ç›‘ç£å™¨ï¼Œç»´æŠ¤è·¨åºåˆ—å—çš„å…¨å±€éšè—çŠ¶æ€ï¼›ä»¥åŠæ£€ç´¢å¢å¼ºçš„å¤–éƒ¨è®°å¿†ï¼Œå­˜å‚¨å’Œæ£€ç´¢é«˜å±‚æ¬¡å—åµŒå…¥è€Œä¸é‡æ–°å¼•å…¥äºŒæ¬¡æ“ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡æ—¶çš„æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰çš„Transformeræ¨¡å‹ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹æ€§ï¼Œå¯¼è‡´å†…å­˜å’Œè®¡ç®—å¼€é”€å‘ˆäºŒæ¬¡å¢é•¿ï¼Œé™åˆ¶äº†å…¶åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„éæ³¨æ„åŠ›æœºåˆ¶æ¶æ„ï¼Œå®Œå…¨é¿å…äº†æ ‡è®°é—´çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œé‡‡ç”¨å¤šä¸ªäº’è¡¥ç»„ä»¶æ¥é«˜æ•ˆå¤„ç†é•¿åºåˆ—ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å‡å°‘è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹ä¸»è¦ç”±ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ç»„æˆï¼šçŠ¶æ€ç©ºé—´å—ï¼ˆå—S4å¯å‘ï¼‰ï¼Œç”¨äºå­¦ä¹ è¿ç»­æ—¶é—´å·ç§¯æ ¸ï¼›å¤šåˆ†è¾¨ç‡å·ç§¯å±‚ï¼Œæ•æ‰ä¸åŒå±‚æ¬¡çš„å±€éƒ¨ä¸Šä¸‹æ–‡ï¼›è½»é‡çº§é€’å½’ç›‘ç£å™¨ï¼Œç»´æŠ¤å…¨å±€éšè—çŠ¶æ€ï¼›ä»¥åŠæ£€ç´¢å¢å¼ºçš„å¤–éƒ¨è®°å¿†ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜å±‚æ¬¡å—åµŒå…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå®Œå…¨å»é™¤äº†ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé‡‡ç”¨çŠ¶æ€ç©ºé—´å’Œå·ç§¯å±‚çš„ç»„åˆï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡æ—¶è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹çš„è®¾è®¡ä¸­ï¼ŒçŠ¶æ€ç©ºé—´å—å’Œå¤šåˆ†è¾¨ç‡å·ç§¯å±‚çš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„æ€§èƒ½ç¨³å®šã€‚æ­¤å¤–ï¼Œè½»é‡çº§é€’å½’ç›‘ç£å™¨çš„è®¾è®¡ä½¿å¾—å…¨å±€çŠ¶æ€çš„ç»´æŠ¤æ›´åŠ é«˜æ•ˆï¼Œå¤–éƒ¨è®°å¿†çš„æ£€ç´¢æœºåˆ¶ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥é¿å…äºŒæ¬¡è®¡ç®—çš„å¼•å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡æ—¶ï¼Œèƒ½å¤Ÿæ”¯æŒæ•°ç™¾ä¸‡ä¸ªæ ‡è®°çš„è¾“å…¥ï¼Œæ€§èƒ½ç›¸æ¯”ä¼ ç»ŸTransformeræ¨¡å‹æå‡æ˜¾è‘—ï¼Œè®¡ç®—æ•ˆç‡æé«˜äº†æ•°å€ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ–‡æœ¬å¤„ç†ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é•¿æ–‡æœ¬ç†è§£ã€å¯¹è¯ç³»ç»Ÿã€ä»¥åŠä¿¡æ¯æ£€ç´¢ç­‰åœºæ™¯ã€‚é€šè¿‡é«˜æ•ˆå¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªå®é™…åº”ç”¨ä¸­æå‡æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a novel non attention based architecture for large language models (LLMs) that efficiently handles very long context windows, on the order of hundreds of thousands to potentially millions of tokens. Unlike traditional Transformer designs, which suffer from quadratic memory and computation overload due to the nature of the self attention mechanism, our model avoids token to token attention entirely. Instead, it combines the following complementary components: State Space blocks (inspired by S4) that learn continuous time convolution kernels and scale near linearly with sequence length, Multi Resolution Convolution layers that capture local context at different dilation levels, a lightweight Recurrent Supervisor to maintain a global hidden state across sequential chunks, and Retrieval Augmented External Memory that stores and retrieves high-level chunk embeddings without reintroducing quadratic operations.

