---
layout: default
title: Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification
---

# Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05744" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05744v1</a>
  <a href="https://arxiv.org/pdf/2505.05744.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05744v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05744v1', 'Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruxue Shi, Hengrui Gu, Xu Shen, Xin Wang

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåˆ©ç”¨LLMè§£é‡Šæå‡è¡¨æ ¼æ•°æ®åˆ†ç±»çš„ä»£ç†æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¡¨æ ¼æ•°æ®åˆ†ç±»` `å¯è§£é‡Šæ€§` `ä»£ç†æ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ¼”ç¤ºé€‰æ‹©` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMæ–¹æ³•åœ¨èµ„æºéœ€æ±‚ã€æ¼”ç¤ºé€‰æ‹©å’Œå¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡åˆ©ç”¨LLMç”Ÿæˆçš„è§£é‡Šï¼ŒæŒ‡å¯¼ä»£ç†æ¨¡å‹è¿›è¡Œå¯è§£é‡Šçš„è¡¨æ ¼æ•°æ®é¢„æµ‹ï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒé¢†åŸŸçš„è¡¨æ ¼æ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†5.31%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºå¢å¼ºè¡¨æ ¼å­¦ä¹ çš„æœ‰å‰æ™¯å·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºLLMçš„æ–¹æ³•é¢ä¸´é«˜èµ„æºéœ€æ±‚ã€æ¼”ç¤ºé€‰æ‹©ä¸ä½³å’Œå¯è§£é‡Šæ€§æœ‰é™ç­‰é—®é¢˜ï¼Œä¸¥é‡å½±å“å…¶é¢„æµ‹æ€§èƒ½å’Œå®é™…åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨LLMç”Ÿæˆçš„è§£é‡Šæ¥æŒ‡å¯¼è¾ƒå°çš„æœ¬åœ°å¯éƒ¨ç½²çš„ä»£ç†è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰è¿›è¡Œå¯è§£é‡Šçš„è¡¨æ ¼é¢„æµ‹ã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šç”Ÿæˆåç½®è§£é‡Šã€åŸºäºè§£é‡Šçš„æ¼”ç¤ºé€‰æ‹©å’ŒåŸºäºè§£é‡Šçš„SLMé¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨æ ¼æ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†5.31%çš„å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMæ–¹æ³•åœ¨è¡¨æ ¼æ•°æ®åˆ†ç±»ä¸­çš„é«˜èµ„æºéœ€æ±‚ã€æ¼”ç¤ºé€‰æ‹©ä¸ä½³å’Œå¯è§£é‡Šæ€§ä¸è¶³ç­‰ç—›ç‚¹ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†LLMåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯ç”¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç”Ÿæˆçš„è§£é‡Šæ¥æŒ‡å¯¼ä¸€ä¸ªè¾ƒå°çš„ã€å¯æœ¬åœ°éƒ¨ç½²çš„ä»£ç†è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ï¼Œä»è€Œå®ç°å¯è§£é‡Šçš„è¡¨æ ¼é¢„æµ‹ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨é™ä½èµ„æºæ¶ˆè€—ï¼ŒåŒæ—¶æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯ç”Ÿæˆåç½®è§£é‡Šï¼Œåˆ©ç”¨LLMä¸ºå€™é€‰æ¼”ç¤ºä¸­çš„é—®ç­”å¯¹ç”Ÿæˆè§£é‡Šï¼›ç¬¬äºŒé˜¶æ®µæ˜¯åŸºäºè§£é‡Šçš„æ¼”ç¤ºé€‰æ‹©ï¼Œåˆ©ç”¨ç”Ÿæˆçš„è§£é‡ŠæŒ‡å¯¼æ¼”ç¤ºé€‰æ‹©è¿‡ç¨‹ï¼›ç¬¬ä¸‰é˜¶æ®µæ˜¯åŸºäºè§£é‡Šçš„SLMé¢„æµ‹ï¼Œä½¿ç”¨ç¬¬äºŒé˜¶æ®µè·å¾—çš„æ¼”ç¤ºä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¹¶åˆå¹¶ç›¸åº”çš„è§£é‡Šä»¥æé«˜SLMçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºåˆ©ç”¨LLMç”Ÿæˆçš„è§£é‡Šæ¥æŒ‡å¯¼æ¼”ç¤ºé€‰æ‹©å’Œæ¨¡å‹é¢„æµ‹ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ç›´æ¥è¾“å…¥æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼Œè®ºæ–‡è¯¦ç»†æè¿°äº†å¦‚ä½•ç”Ÿæˆåç½®è§£é‡Šã€é€‰æ‹©æ¼”ç¤ºçš„æ ‡å‡†ï¼Œä»¥åŠSLMçš„ç»“æ„å’ŒæŸå¤±å‡½æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ¡†æ¶åœ¨å¤šä¸ªè¡¨æ ¼æ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†5.31%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚è¿™ä¸€æå‡è¡¨æ˜ï¼Œåˆ©ç”¨LLMç”Ÿæˆçš„è§£é‡Šèƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºä»£ç†æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èã€åŒ»ç–—å’Œå¸‚åœºåˆ†æç­‰éœ€è¦è¡¨æ ¼æ•°æ®åˆ†ç±»çš„åœºæ™¯ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ï¼Œèƒ½å¤Ÿå¸®åŠ©å†³ç­–è€…æ›´å¥½åœ°ç†è§£æ¨¡å‹è¾“å‡ºï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­åšå‡ºæ›´ä¸ºå‡†ç¡®çš„åˆ¤æ–­ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸæ¨å¹¿åº”ç”¨ï¼Œæå‡æ•°æ®é©±åŠ¨å†³ç­–çš„é€æ˜åº¦å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown remarkable ability in solving complex tasks, making them a promising tool for enhancing tabular learning. However, existing LLM-based methods suffer from high resource requirements, suboptimal demonstration selection, and limited interpretability, which largely hinder their prediction performance and application in the real world. To overcome these problems, we propose a novel in-context learning framework for tabular prediction. The core idea is to leverage the explanations generated by LLMs to guide a smaller, locally deployable Surrogate Language Model (SLM) to make interpretable tabular predictions. Specifically, our framework mainly involves three stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to generate explanations for question-answer pairs in candidate demonstrations, providing insights into the reasoning behind the answer. (ii) Post Hoc Explanation-Guided Demonstrations Selection, which utilizes explanations generated by LLMs to guide the process of demonstration selection from candidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM Prediction, which utilizes the demonstrations obtained in step (ii) as in-context and merges corresponding explanations as rationales to improve the performance of SLM and guide the model to generate interpretable outputs. Experimental results highlight the framework's effectiveness, with an average accuracy improvement of 5.31% across various tabular datasets in diverse domains.

