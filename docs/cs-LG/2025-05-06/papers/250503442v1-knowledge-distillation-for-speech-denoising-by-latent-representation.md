---
layout: default
title: Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance
---

# Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03442" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03442v1</a>
  <a href="https://arxiv.org/pdf/2505.03442.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03442v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03442v1', 'Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Diep Luong, Mikko Heikkinen, Konstantinos Drossos, Tuomas Virtanen

**åˆ†ç±»**: cs.SD, cs.LG, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä½™å¼¦è·ç¦»çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä»¥æ”¹å–„è¯­éŸ³å»å™ªæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯­éŸ³å»å™ª` `çŸ¥è¯†è’¸é¦` `ä½™å¼¦ç›¸ä¼¼æ€§` `å»å™ªè‡ªç¼–ç å™¨` `æ¨¡å‹å‹ç¼©` `ä½èµ„æºç¯å¢ƒ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­éŸ³å»å™ªæ–¹æ³•å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥åœ¨ä½èµ„æºç¯å¢ƒä¸­æœ‰æ•ˆéƒ¨ç½²ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå»å™ªè‡ªç¼–ç å™¨å’Œä½™å¼¦ç›¸ä¼¼æ€§çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šç§ä¸åŒ¹é…åœºæ™¯ä¸‹ï¼Œå­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­éŸ³å»å™ªæ˜¯ä¸€ä¸ªå¹¿æ³›åº”ç”¨ä¸”å½±å“æ·±è¿œçš„ä»»åŠ¡ï¼Œç„¶è€Œç°æœ‰çš„å¼ºå¤§æ–¹æ³•å¾€å¾€è¿‡äºå¤æ‚ï¼Œéš¾ä»¥åœ¨ä½èµ„æºç¯å¢ƒä¸­éƒ¨ç½²ã€‚çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å°†å¤æ‚æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°ç®€å•æ¨¡å‹ä¸­æ¥å‡è½»å¤æ‚æ€§ã€‚ç°æœ‰çš„KDæ–¹æ³•å¯èƒ½é™åˆ¶äº†å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å»å™ªè‡ªç¼–ç å™¨æ¡†æ¶ã€çº¿æ€§åå‘ç“¶é¢ˆå’Œä½™å¼¦ç›¸ä¼¼æ€§ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡å…¬å…±æ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ä¸åŒçš„åŒ¹é…åœºæ™¯ä¸‹ï¼Œå­¦ç”Ÿæ¨¡å‹çš„è¡¨ç°ä¼˜äºæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶èƒ½æ›´å¥½åœ°é€‚åº”æ›´å¤§çš„ä¸åŒ¹é…æ¡ä»¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯è¯­éŸ³å»å™ªä¸­çš„çŸ¥è¯†è’¸é¦é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚æ¨¡å‹ä¸ç®€å•æ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼šé™åˆ¶å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œåˆ©ç”¨å»å™ªè‡ªç¼–ç å™¨æ¡†æ¶å’Œä½™å¼¦ç›¸ä¼¼æ€§ï¼Œæ—¨åœ¨æ”¹å–„å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ æ•ˆæœï¼Œé¿å…ä¿¡æ¯æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„è®¾è®¡ï¼Œé‡‡ç”¨å»å™ªè‡ªç¼–ç å™¨ä½œä¸ºåŸºç¡€ï¼Œç»“åˆçº¿æ€§åå‘ç“¶é¢ˆç»“æ„ï¼Œé€šè¿‡ä½™å¼¦è·ç¦»è¿›è¡ŒçŸ¥è¯†å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä½¿ç”¨ä½™å¼¦ç›¸ä¼¼æ€§æ¥è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„åŸºäºæ¬§æ°è·ç¦»çš„è’¸é¦æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒç‰¹å¾çš„ç›¸å¯¹å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†çº¿æ€§åå‘ç“¶é¢ˆï¼Œä»¥æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œå»å™ªæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ä¸åŒçš„åŒ¹é…åœºæ™¯ä¸‹ï¼Œå­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡æœ‰æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ›´å¤§ä¸åŒ¹é…æ¡ä»¶æ—¶ï¼Œå­¦ç”Ÿæ¨¡å‹çš„é²æ£’æ€§å¾—åˆ°äº†å¢å¼ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è®¾å¤‡ï¼ˆå¦‚æ‰‹æŒè®¾å¤‡ã€æ™ºèƒ½çœ¼é•œå’ŒåŠ©å¬å™¨ï¼‰ä¸­çš„è¯­éŸ³å»å™ªæŠ€æœ¯ã€‚é€šè¿‡é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„è¯­éŸ³å¤„ç†ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Speech denoising is a generally adopted and impactful task, appearing in many common and everyday-life use cases. Although there are very powerful methods published, most of those are too complex for deployment in everyday and low-resources computational environments, like hand-held devices, intelligent glasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for alleviating this complexity mismatch and is based on the transferring/distilling of knowledge from a pre-trained complex model, the teacher, to another less complex one, the student. Existing KD methods for speech denoising are based on processes that potentially hamper the KD by bounding the learning of the student to the distribution, information ordering, and feature dimensionality learned by the teacher. In this paper, we present and assess a method that tries to treat this issue, by exploiting the well-known denoising-autoencoder framework, the linear inverted bottlenecks, and the properties of the cosine similarity. We use a public dataset and conduct repeated experiments with different mismatching scenarios between the teacher and the student, reporting the mean and standard deviation of the metrics of our method and another, state-of-the-art method that is used as a baseline. Our results show that with the proposed method, the student can perform better and can also retain greater mismatching conditions compared to the teacher.

