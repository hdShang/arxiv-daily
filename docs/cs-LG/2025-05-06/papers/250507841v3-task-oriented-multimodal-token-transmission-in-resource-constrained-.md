---
layout: default
title: Task-Oriented Multimodal Token Transmission in Resource-Constrained Multiuser Networks
---

# Task-Oriented Multimodal Token Transmission in Resource-Constrained Multiuser Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07841" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07841v3</a>
  <a href="https://arxiv.org/pdf/2505.07841.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07841v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07841v3', 'Task-Oriented Multimodal Token Transmission in Resource-Constrained Multiuser Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junhe Zhang, Wanli Ni, Pengwei Wang, Dongyu Wang

**åˆ†ç±»**: cs.NI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-11-03)

**DOI**: [10.1109/LWC.2025.3628928](https://doi.org/10.1109/LWC.2025.3628928)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»»åŠ¡å¯¼å‘çš„å¤šæ¨¡æ€ä»¤ç‰Œä¼ è¾“æ–¹æ¡ˆä»¥è§£å†³èµ„æºå—é™ç½‘ç»œä¸­çš„æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ä¿¡æ¯èåˆ` `ä»¤ç‰Œä¼ è¾“` `ä»»åŠ¡å¯¼å‘` `æ»‘åŠ¨çª—å£æ± åŒ–` `ä¼˜åŒ–ç®—æ³•` `èµ„æºå—é™ç½‘ç»œ` `å˜æ¢å™¨æ¶æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºå˜æ¢å™¨çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ä»¤ç‰Œä¼ è¾“ä¸­é¢ä¸´å¸¦å®½å¼€é”€å¤§ã€åŠŸè€—é«˜å’Œå»¶è¿Ÿå¢åŠ ç­‰æŒ‘æˆ˜ã€‚
2. æå‡ºäº†ä¸€ç§ä»»åŠ¡å¯¼å‘çš„å¤šæ¨¡æ€ä»¤ç‰Œä¼ è¾“æ–¹æ¡ˆï¼Œç»“åˆä¸¤é˜¶æ®µè®­ç»ƒç®—æ³•å’Œæ»‘åŠ¨çª—å£æ± åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜ä¼ è¾“æ•ˆç‡ã€‚
3. ä»¿çœŸç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®—æ³•åœ¨ä¸åŒå¸¦å®½å’ŒåŠŸç‡é¢„ç®—ä¸‹å‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§ä¿¡å™ªæ¯”ä¸‹å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½ä½“çš„å‡ºç°ï¼Œå¹¿æ³›é‡‡ç”¨çš„åŸºäºå˜æ¢å™¨çš„æ¶æ„ä¸å¯é¿å…åœ°äº§ç”Ÿè¿‡é•¿çš„ä»¤ç‰ŒåµŒå…¥ï¼Œè¿™å¯èƒ½å¯¼è‡´é«˜å¸¦å®½å¼€é”€ã€å¢åŠ åŠŸè€—å’Œå»¶è¿Ÿã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»»åŠ¡å¯¼å‘çš„å¤šæ¨¡æ€ä»¤ç‰Œä¼ è¾“æ–¹æ¡ˆï¼Œä»¥æé«˜å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆå’Œåˆ©ç”¨æ•ˆç‡ã€‚ä¸ºæé«˜ä»¤ç‰Œä¼ è¾“æ•ˆç‡ï¼Œè®¾è®¡äº†åŒ…æ‹¬è·¨æ¨¡æ€å¯¹é½å’Œä»»åŠ¡å¯¼å‘å¾®è°ƒçš„ä¸¤é˜¶æ®µè®­ç»ƒç®—æ³•ã€‚åŒæ—¶ï¼Œé‡‡ç”¨æ»‘åŠ¨çª—å£æ± åŒ–æ“ä½œè¿›è¡Œä»¤ç‰Œå‹ç¼©ï¼Œä»¥èŠ‚çœé€šä¿¡èµ„æºã€‚ä¸ºå¹³è¡¡å‹ç¼©å¸¦æ¥çš„å»¶è¿Ÿä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œæ„å»ºäº†ä¸€ä¸ªå…³äºå»¶è¿Ÿå’ŒéªŒè¯æŸå¤±çš„åŠ æƒå’Œä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡äº¤æ›¿ä¼˜åŒ–æ–¹æ³•è”åˆä¼˜åŒ–ç”¨æˆ·çš„å¸¦å®½ã€åŠŸç‡åˆ†é…å’Œä»¤ç‰Œé•¿åº¦ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€æç®—æ³•åœ¨ä¸åŒçš„å¸¦å®½å’ŒåŠŸç‡é¢„ç®—ä¸‹ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨èµ„æºå—é™çš„å¤šç”¨æˆ·ç½‘ç»œä¸­ï¼ŒåŸºäºå˜æ¢å™¨çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ä»¤ç‰Œä¼ è¾“æ—¶äº§ç”Ÿçš„é«˜å¸¦å®½å¼€é”€å’Œå»¶è¿Ÿç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ—¶æ•ˆç‡ä½ä¸‹ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ–¹æ¡ˆé€šè¿‡ä»»åŠ¡å¯¼å‘çš„å¤šæ¨¡æ€ä»¤ç‰Œä¼ è¾“ï¼Œç»“åˆä¸¤é˜¶æ®µè®­ç»ƒç®—æ³•ï¼Œä¼˜åŒ–ä»¤ç‰Œçš„ä¼ è¾“æ•ˆç‡ã€‚é€šè¿‡è·¨æ¨¡æ€å¯¹é½å’Œä»»åŠ¡å¯¼å‘å¾®è°ƒï¼Œæå‡äº†ä¿¡æ¯èåˆçš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºè·¨æ¨¡æ€å¯¹é½ï¼Œç¬¬äºŒé˜¶æ®µä¸ºä»»åŠ¡å¯¼å‘å¾®è°ƒã€‚åŒæ—¶ï¼Œé‡‡ç”¨æ»‘åŠ¨çª—å£æ± åŒ–æ“ä½œè¿›è¡Œä»¤ç‰Œå‹ç¼©ï¼ŒèŠ‚çœé€šä¿¡èµ„æºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†åŠ æƒå’Œä¼˜åŒ–é—®é¢˜ï¼Œå¹³è¡¡å»¶è¿Ÿä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œé‡‡ç”¨äº¤æ›¿ä¼˜åŒ–æ–¹æ³•è”åˆä¼˜åŒ–å¸¦å®½ã€åŠŸç‡åˆ†é…å’Œä»¤ç‰Œé•¿åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ»‘åŠ¨çª—å£æ± åŒ–ä½œä¸ºä»¤ç‰Œå‹ç¼©æŠ€æœ¯ï¼Œå¹¶é€šè¿‡åŠ æƒå’ŒæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å»¶è¿Ÿå’ŒéªŒè¯æŸå¤±ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æç®—æ³•åœ¨ä¸åŒå¸¦å®½å’ŒåŠŸç‡é¢„ç®—ä¸‹çš„æ€§èƒ½å‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨ä¿¡å™ªæ¯”å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼Œè¯æ˜äº†ä¸¤é˜¶æ®µè®­ç»ƒç®—æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šã€ç‰©è”ç½‘å’Œæ™ºèƒ½å®¶å±…ç­‰å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†åœºæ™¯ã€‚é€šè¿‡æé«˜ä»¤ç‰Œä¼ è¾“çš„æ•ˆç‡ï¼Œå¯ä»¥æ˜¾è‘—é™ä½ç³»ç»Ÿçš„åŠŸè€—å’Œå»¶è¿Ÿï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the emergence of large model-based agents, widely adopted transformer-based architectures inevitably produce excessively long token embeddings for transmission, which may result in high bandwidth overhead, increased power consumption and latency. In this letter, we propose a task-oriented multimodal token transmission scheme for efficient multimodal information fusion and utilization. To improve the efficiency of token transmission, we design a two-stage training algotithm, including cross-modal alignment and task-oriented fine-tuning, for large model-based token communication. Meanwhile, token compression is performed using a sliding window pooling operation to save communication resources. To balance the trade-off between latency and model performance caused by compression, we formulate a weighted-sum optimization problem over latency and validation loss. We jointly optimizes bandwidth, power allocation, and token length across users by using an alternating optimization method. Simulation results demonstrate that the proposed algorithm outperforms the baseline under different bandwidth and power budgets. Moreover, the two-stage training algorithm achieves higher accuracy across various signal-to-noise ratios than the method without cross-modal alignment.

