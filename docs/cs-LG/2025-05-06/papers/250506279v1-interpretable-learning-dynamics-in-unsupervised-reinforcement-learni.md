---
layout: default
title: Interpretable Learning Dynamics in Unsupervised Reinforcement Learning
---

# Interpretable Learning Dynamics in Unsupervised Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06279" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06279v1</a>
  <a href="https://arxiv.org/pdf/2505.06279.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06279v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06279v1', 'Interpretable Learning Dynamics in Unsupervised Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shashwat Pandey

**åˆ†ç±»**: cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯è§£é‡Šæ€§æ¡†æ¶ä»¥ç†è§£æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ä¸­çš„å†…åœ¨åŠ¨æœº**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ` `å¯è§£é‡Šæ€§` `å†…åœ¨åŠ¨æœº` `æ³¨æ„åŠ›æœºåˆ¶` `ä»£ç†è¡Œä¸ºåˆ†æ` `æ¢ç´¢ç­–ç•¥` `æ·±åº¦å­¦ä¹ ` `åŠ¨æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç¼ºä¹å¯¹ä»£ç†å†…éƒ¨åŠ¨æ€çš„å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥ç†è§£å†…åœ¨åŠ¨æœºå¦‚ä½•å½±å“å…¶è¡Œä¸ºå’Œå­¦ä¹ è¿‡ç¨‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šæ€§æ¡†æ¶ï¼Œé€šè¿‡åˆ†æä»£ç†çš„æ³¨æ„åŠ›å’Œè¡Œä¸ºï¼Œæ­ç¤ºå†…åœ¨åŠ¨æœºå¯¹å…¶å­¦ä¹ åŠ¨æ€çš„å½±å“ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¥½å¥‡å¿ƒçš„ä»£ç†åœ¨æ³¨æ„åŠ›å’Œæ¢ç´¢è¡Œä¸ºä¸Šè¡¨ç°å‡ºæ›´å¹¿æ³›å’ŒåŠ¨æ€çš„ç‰¹å¾ï¼ŒTransformer-RNDåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–ä»£ç†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šæ€§æ¡†æ¶ï¼Œæ—¨åœ¨ç†è§£å†…åœ¨åŠ¨æœºå¦‚ä½•å½±å“æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆURLï¼‰ä»£ç†çš„æ³¨æ„åŠ›ã€è¡Œä¸ºå’Œè¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡åˆ†æäº”ç§ä»£ç†ï¼ˆDQNã€RNDã€ICMã€PPOå’ŒTransformer-RNDå˜ä½“ï¼‰ï¼Œä½¿ç”¨Grad-CAMã€å±‚æ¬¡ç›¸å…³ä¼ æ’­ï¼ˆLRPï¼‰ã€æ¢ç´¢æŒ‡æ ‡å’Œæ½œåœ¨ç©ºé—´èšç±»ï¼Œæ­ç¤ºäº†ä»£ç†å¦‚ä½•æ„ŸçŸ¥å’Œé€‚åº”ç¯å¢ƒã€‚æˆ‘ä»¬å¼•å…¥äº†æ³¨æ„åŠ›å¤šæ ·æ€§å’Œæ³¨æ„åŠ›å˜åŒ–ç‡ä¸¤ä¸ªæŒ‡æ ‡ï¼Œå‘ç°å¥½å¥‡å¿ƒé©±åŠ¨çš„ä»£ç†è¡¨ç°å‡ºæ›´å¹¿æ³›å’ŒåŠ¨æ€çš„æ³¨æ„åŠ›åŠæ¢ç´¢è¡Œä¸ºã€‚Transformer-RNDåœ¨æ³¨æ„åŠ›å¹¿åº¦ã€æ¢ç´¢è¦†ç›–ç‡å’Œç´§å‡‘ç»“æ„çš„æ½œåœ¨è¡¨ç¤ºæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ¶æ„å½’çº³åç½®å’Œè®­ç»ƒä¿¡å·å¯¹ä»£ç†å†…éƒ¨åŠ¨æ€çš„å½±å“ï¼Œæä¾›äº†è¶…è¶Šå¥–åŠ±ä¸­å¿ƒè¯„ä¼°çš„è¯Šæ–­å·¥å…·ï¼Œä¿ƒè¿›äº†RLä»£ç†çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å¯è§£é‡Šæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ­ç¤ºä»£ç†å¦‚ä½•é€šè¿‡å†…åœ¨åŠ¨æœºè¿›è¡Œå­¦ä¹ å’Œé€‚åº”ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ³¨æ„åŠ›å¤šæ ·æ€§å’Œæ³¨æ„åŠ›å˜åŒ–ç‡ä¸¤ä¸ªæ–°æŒ‡æ ‡ï¼Œåˆ†æä»£ç†åœ¨ä¸åŒæ—¶é—´å’Œç©ºé—´ä¸Šçš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œè¿›è€Œç†è§£å†…åœ¨åŠ¨æœºå¯¹ä»£ç†è¡Œä¸ºçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨Grad-CAMå’Œå±‚æ¬¡ç›¸å…³ä¼ æ’­ï¼ˆLRPï¼‰ç­‰æŠ€æœ¯ï¼Œç»“åˆæ¢ç´¢æŒ‡æ ‡å’Œæ½œåœ¨ç©ºé—´èšç±»ï¼Œå¯¹äº”ç§ä¸åŒçš„ä»£ç†è¿›è¡Œåˆ†æï¼Œæ„å»ºäº†ä¸€ä¸ªç»¼åˆçš„å¯è§£é‡Šæ€§æ¡†æ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºçš„æ³¨æ„åŠ›å¤šæ ·æ€§å’Œæ³¨æ„åŠ›å˜åŒ–ç‡æŒ‡æ ‡æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä»£ç†åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ï¼Œä¸ä¼ ç»Ÿçš„å¥–åŠ±ä¸­å¿ƒè¯„ä¼°æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§ä»£ç†æ¶æ„ï¼ˆå¦‚DQNã€RNDã€ICMã€PPOå’ŒTransformer-RNDï¼‰ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒä¿¡å·å’ŒæŸå¤±å‡½æ•°ï¼Œç¡®ä¿ä»£ç†èƒ½å¤Ÿåœ¨ç¨‹åºç”Ÿæˆçš„ç¯å¢ƒä¸­æœ‰æ•ˆå­¦ä¹ ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºå¥½å¥‡å¿ƒçš„ä»£ç†åœ¨æ³¨æ„åŠ›å¤šæ ·æ€§å’Œæ¢ç´¢è¦†ç›–ç‡ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–ä»£ç†ï¼ŒTransformer-RNDåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œå±•ç¤ºäº†æ›´å¹¿æ³›çš„æ³¨æ„åŠ›å’Œæ›´é«˜çš„æ¢ç´¢èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä¸ºç†è§£ä»£ç†çš„å­¦ä¹ åŠ¨æ€æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„å¯è§£é‡Šæ€§æ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºæ— ç›‘ç£å¼ºåŒ–å­¦ä¹ é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç†è§£ä»£ç†è¡Œä¸ºå’Œå†³ç­–è¿‡ç¨‹çš„åœºæ™¯ä¸­ï¼Œå¦‚æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIå’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æ¨åŠ¨RLä»£ç†çš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¿ƒè¿›å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¿¡ä»»åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present an interpretability framework for unsupervised reinforcement learning (URL) agents, aimed at understanding how intrinsic motivation shapes attention, behavior, and representation learning. We analyze five agents DQN, RND, ICM, PPO, and a Transformer-RND variant trained on procedurally generated environments, using Grad-CAM, Layer-wise Relevance Propagation (LRP), exploration metrics, and latent space clustering. To capture how agents perceive and adapt over time, we introduce two metrics: attention diversity, which measures the spatial breadth of focus, and attention change rate, which quantifies temporal shifts in attention. Our findings show that curiosity-driven agents display broader, more dynamic attention and exploratory behavior than their extrinsically motivated counterparts. Among them, TransformerRND combines wide attention, high exploration coverage, and compact, structured latent representations. Our results highlight the influence of architectural inductive biases and training signals on internal agent dynamics. Beyond reward-centric evaluation, the proposed framework offers diagnostic tools to probe perception and abstraction in RL agents, enabling more interpretable and generalizable behavior.

