---
layout: default
title: "Absolute Zero: Reinforced Self-play Reasoning with Zero Data"
---

# Absolute Zero: Reinforced Self-play Reasoning with Zero Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03335" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03335v3</a>
  <a href="https://arxiv.org/pdf/2505.03335.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03335v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03335v3', 'Absolute Zero: Reinforced Self-play Reasoning with Zero Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-10-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAbsolute Zeroä»¥è§£å†³æ— æ•°æ®å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è‡ªæˆ‘å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `æ— ç›‘ç£å­¦ä¹ ` `ä»£ç æ‰§è¡Œ` `ä»»åŠ¡ç”Ÿæˆ` `äººå·¥æ™ºèƒ½` `æ¨¡å‹è‡ªé€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é›¶è®¾ç½®RLVRæ–¹æ³•ä¾èµ–äºäººå·¥ç­–åˆ’çš„æ•°æ®ï¼Œé™åˆ¶äº†å…¶é•¿æœŸæ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„Absolute ZeroèŒƒå¼å…è®¸æ¨¡å‹åœ¨æ²¡æœ‰å¤–éƒ¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè‡ªä¸»ç”Ÿæˆå’Œè§£å†³ä»»åŠ¡ï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›ã€‚
3. AZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¾èµ–æ•°ä¸‡æ¡äººå·¥ç¤ºä¾‹çš„ç°æœ‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚ç°æœ‰çš„é›¶è®¾ç½®RLVRæ–¹æ³•ä¾èµ–äºäººå·¥ç­–åˆ’çš„é—®é¢˜å’Œç­”æ¡ˆè¿›è¡Œè®­ç»ƒï¼Œç¼ºä¹é«˜è´¨é‡çš„äººå·¥ç¤ºä¾‹é™åˆ¶äº†å…¶é•¿æœŸæ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„RLVRèŒƒå¼Absolute Zeroï¼Œå…è®¸æ¨¡å‹åœ¨æ²¡æœ‰å¤–éƒ¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè‡ªä¸»æå‡ºä»»åŠ¡ä»¥æœ€å¤§åŒ–å­¦ä¹ è¿›å±•ï¼Œå¹¶é€šè¿‡è§£å†³è¿™äº›ä»»åŠ¡æ¥æå‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†Absolute Zero Reasonerï¼ˆAZRï¼‰ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä»£ç æ‰§è¡Œå™¨è‡ªæˆ‘è¿›åŒ–è®­ç»ƒè¯¾ç¨‹å’Œæ¨ç†èƒ½åŠ›ï¼ŒéªŒè¯æå‡ºçš„ä»£ç æ¨ç†ä»»åŠ¡å’Œç­”æ¡ˆï¼Œæˆä¸ºå¯éªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºã€‚AZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†SOTAæ€§èƒ½ï¼Œè¶…è¶Šäº†ä¾èµ–å¤§é‡äººå·¥ç­–åˆ’ç¤ºä¾‹çš„ç°æœ‰æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é›¶è®¾ç½®RLVRæ–¹æ³•å¯¹äººå·¥æ•°æ®çš„ä¾èµ–é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œæ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºAbsolute ZeroèŒƒå¼ï¼Œå…è®¸æ¨¡å‹è‡ªä¸»ç”Ÿæˆä»»åŠ¡å¹¶é€šè¿‡è§£å†³è¿™äº›ä»»åŠ¡æ¥æå‡è‡ªèº«çš„æ¨ç†èƒ½åŠ›ï¼Œå®Œå…¨ä¸ä¾èµ–å¤–éƒ¨æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAZRç³»ç»ŸåŒ…å«ä»»åŠ¡ç”Ÿæˆæ¨¡å—ã€ä»£ç æ‰§è¡Œå™¨å’ŒéªŒè¯æ¨¡å—ï¼Œæ¨¡å‹é€šè¿‡è‡ªæˆ‘è¿›åŒ–çš„æ–¹å¼ä¸æ–­ä¼˜åŒ–è®­ç»ƒè¯¾ç¨‹å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šAZRçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å®Œå…¨ä¸ä¾èµ–å¤–éƒ¨æ•°æ®çš„å­¦ä¹ æ–¹å¼ï¼Œé€šè¿‡è‡ªæˆ‘ç”Ÿæˆå’ŒéªŒè¯ä»»åŠ¡æ¥å®ç°å­¦ä¹ ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥ç­–åˆ’ç¤ºä¾‹ã€‚

**å…³é”®è®¾è®¡**ï¼šAZRä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ä»»åŠ¡ç”Ÿæˆå’ŒéªŒè¯è¿‡ç¨‹ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šå…¼å®¹å¤šç§æ¨¡å‹è§„æ¨¡ï¼Œç¡®ä¿å…¶åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†SOTAæ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¾èµ–æ•°ä¸‡æ¡äººå·¥ç­–åˆ’ç¤ºä¾‹çš„ç°æœ‰æ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æ— æ•°æ®å­¦ä¹ ç¯å¢ƒä¸‹çš„å¼ºå¤§èƒ½åŠ›å’Œçµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–ç¼–ç¨‹ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¤æ‚é—®é¢˜è§£å†³ç­‰ã€‚é€šè¿‡è‡ªä¸»å­¦ä¹ å’Œæ¨ç†ï¼ŒAZRèƒ½å¤Ÿåœ¨æ²¡æœ‰äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ä¸æ–­æå‡å…¶èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.

