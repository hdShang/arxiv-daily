---
layout: default
title: "Policy-labeled Preference Learning: Is Preference Enough for RLHF?"
---

# Policy-labeled Preference Learning: Is Preference Enough for RLHF?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06273" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06273v2</a>
  <a href="https://arxiv.org/pdf/2505.06273.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06273v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06273v2', 'Policy-labeled Preference Learning: Is Preference Enough for RLHF?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Taehyun Cho, Seokhun Ju, Seungyub Han, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-05-13)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ”¿ç­–æ ‡ç­¾åå¥½å­¦ä¹ ä»¥è§£å†³RLHFä¸­çš„åå¥½ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `åå¥½å­¦ä¹ ` `æ”¿ç­–ä¼˜åŒ–` `åºåˆ—å†³ç­–` `å¯¹æ¯”æ­£åˆ™åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RLHFæ–¹æ³•å¸¸å¸¸è¯¯è§£è½¨è¿¹ä¸ºæœ€ä¼˜ç­–ç•¥ç”Ÿæˆï¼Œå¯¼è‡´ä¼¼ç„¶ä¼°è®¡ä¸å‡†ç¡®å’Œå­¦ä¹ æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºæ”¿ç­–æ ‡ç­¾åå¥½å­¦ä¹ ï¼ˆPPLï¼‰ï¼Œé€šè¿‡åæ‚”å»ºæ¨¡äººç±»åå¥½ï¼Œè§£å†³äº†ä¼¼ç„¶ä¸åŒ¹é…çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPPLåœ¨é«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†ç¦»çº¿RLHFçš„æ€§èƒ½ï¼Œå¹¶åœ¨åœ¨çº¿ç¯å¢ƒä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è®¾è®¡ä¸äººç±»ç›®æ ‡ä¸€è‡´çš„å¥–åŠ±ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å·²æˆä¸ºä»äººç±»åå¥½ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°å¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–ç­–ç•¥çš„ä¸»è¦æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLHFæ–¹æ³•å¸¸å¸¸è¯¯è§£è½¨è¿¹ä¸ºç”±æœ€ä¼˜ç­–ç•¥ç”Ÿæˆï¼Œå¯¼è‡´ä¸å‡†ç¡®çš„ä¼¼ç„¶ä¼°è®¡å’Œæ¬¡ä¼˜å­¦ä¹ ã€‚å—ç›´æ¥åå¥½ä¼˜åŒ–æ¡†æ¶çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºæ”¿ç­–æ ‡ç­¾åå¥½å­¦ä¹ ï¼ˆPPLï¼‰ï¼Œé€šè¿‡ç”¨åæ‚”å»ºæ¨¡äººç±»åå¥½æ¥è§£å†³ä¼¼ç„¶ä¸åŒ¹é…é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§åŸºäºåæ‚”åŸåˆ™çš„å¯¹æ¯”KLæ­£åˆ™åŒ–ï¼Œä»¥å¢å¼ºRLHFåœ¨åºåˆ—å†³ç­–ä¸­çš„è¡¨ç°ã€‚åœ¨é«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒPPLåœ¨ç¦»çº¿RLHFæ€§èƒ½ä¸Šæ˜¾è‘—æå‡ï¼Œå¹¶åœ¨åœ¨çº¿è®¾ç½®ä¸­ä¹Ÿè¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰RLHFæ–¹æ³•åœ¨è½¨è¿¹ç†è§£ä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯è¯¯å°†éæœ€ä¼˜ç­–ç•¥çš„è½¨è¿¹è§†ä¸ºæœ€ä¼˜ç­–ç•¥ï¼Œå¯¼è‡´å¥–åŠ±å­¦ä¹ ä¸å‡†ç¡®çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ”¿ç­–æ ‡ç­¾åå¥½å­¦ä¹ ï¼ˆPPLï¼‰ï¼Œé€šè¿‡å»ºæ¨¡äººç±»åå¥½çš„åæ‚”æ¥è§£å†³ä¼¼ç„¶ä¸åŒ¹é…ï¼Œé¿å…äº†å¯¹å¥–åŠ±çš„æ˜¾å¼ä¾èµ–ï¼Œä»è€Œç›´æ¥ä¼˜åŒ–ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPPLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åå¥½å»ºæ¨¡æ¨¡å—å’Œå¯¹æ¯”KLæ­£åˆ™åŒ–æ¨¡å—ï¼Œå‰è€…ç”¨äºæ•æ‰äººç±»åå¥½ï¼Œåè€…åˆ™å¢å¼ºäº†åºåˆ—å†³ç­–è¿‡ç¨‹ä¸­çš„å­¦ä¹ æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šPPLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡åæ‚”å»ºæ¨¡äººç±»åå¥½ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥è€Œä¸ä¾èµ–äºæ˜¾å¼å¥–åŠ±ï¼Œè¿™ä¸ä¼ ç»ŸRLHFæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå¯¹å¥–åŠ±çš„å¤„ç†æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºåæ‚”çš„æŸå¤±å‡½æ•°ï¼Œå¹¶å¼•å…¥äº†å¯¹æ¯”KLæ­£åˆ™åŒ–ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨åºåˆ—å†³ç­–ä¸­çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPPLåœ¨é«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­ç›¸æ¯”äºä¼ ç»ŸRLHFæ–¹æ³•ï¼Œç¦»çº¿æ€§èƒ½æå‡äº†æ˜¾è‘—çš„XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼Œå¹¶ä¸”åœ¨åœ¨çº¿è®¾ç½®ä¸­ä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„ç¨³å®šæ€§å’Œå­¦ä¹ æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰éœ€è¦äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ã€‚é€šè¿‡æé«˜RLHFçš„å­¦ä¹ æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒPPLæœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜çš„æ™ºèƒ½å†³ç­–èƒ½åŠ›ï¼Œæ¨åŠ¨äººæœºåä½œçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To design rewards that align with human goals, Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent technique for learning reward functions from human preferences and optimizing policies via reinforcement learning algorithms. However, existing RLHF methods often misinterpret trajectories as being generated by an optimal policy, causing inaccurate likelihood estimation and suboptimal learning. Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward, we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information. We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making. Experiments in high-dimensional continuous control tasks demonstrate PPL's significant improvements in offline RLHF performance and its effectiveness in online settings.

