---
layout: default
title: SPAP: Structured Pruning via Alternating Optimization and Penalty Methods
---

# SPAP: Structured Pruning via Alternating Optimization and Penalty Methods

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03373" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03373v1</a>
  <a href="https://arxiv.org/pdf/2505.03373.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03373v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03373v1', 'SPAP: Structured Pruning via Alternating Optimization and Penalty Methods')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanyu Hu, Xiaoming Yuan

**åˆ†ç±»**: cs.LG, cs.AI, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPAPä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ç»“æ„åŒ–å‰ªæé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»“æ„åŒ–å‰ªæ` `å¤§è¯­è¨€æ¨¡å‹` `ä¼˜åŒ–ç†è®º` `æ··åˆæ•´æ•°ä¼˜åŒ–` `æ€§èƒ½æ¢å¤` `æƒ©ç½šæ–¹æ³•` `äº¤æ›¿æœ€å°åŒ–ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•åœ¨å‰ªæè¿‡ç¨‹ä¸­å¸¸å¸¸å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸”ä¾èµ–äºå¯å‘å¼æŒ‡æ ‡æˆ–éœ€è¦æ˜‚è´µçš„å¾®è°ƒã€‚
2. SPAPé€šè¿‡æ··åˆæ•´æ•°ä¼˜åŒ–æ¨¡å‹å®šä¹‰å‰ªæé—®é¢˜ï¼Œé‡‡ç”¨æƒ©ç½šæ–¹æ³•å’Œäº¤æ›¿æœ€å°åŒ–ç®—æ³•æ¥é«˜æ•ˆåœ°è¿›è¡Œå‰ªæå†³ç­–å’Œæƒé‡æ›´æ–°ã€‚
3. åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒSPAPåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡å’Œå†…å­˜å‡å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„éƒ¨ç½²å¸¸å—åˆ°å…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚çš„é™åˆ¶ã€‚ç»“æ„åŒ–å‰ªæä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡æ¶ˆé™¤æ•´ä¸ªç½‘ç»œç»„ä»¶æ¥å‡å°æ¨¡å‹è§„æ¨¡ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€é¢ä¸´æ€§èƒ½ä¸‹é™ã€ä¾èµ–å¯å‘å¼æŒ‡æ ‡æˆ–æ˜‚è´µçš„å¾®è°ƒç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†SPAPï¼ˆé€šè¿‡äº¤æ›¿ä¼˜åŒ–å’Œæƒ©ç½šæ–¹æ³•çš„ç»“æ„åŒ–å‰ªæï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¼˜åŒ–ç†è®ºçš„æ–°é¢–é«˜æ•ˆçš„ç»“æ„åŒ–å‰ªææ¡†æ¶ã€‚SPAPé€šè¿‡æ··åˆæ•´æ•°ä¼˜åŒ–æ¨¡å‹æ¥å®šä¹‰å‰ªæé—®é¢˜ï¼Œé‡‡ç”¨æƒ©ç½šæ–¹æ³•æœ‰æ•ˆåœ°åšå‡ºå‰ªæå†³ç­–ä»¥æœ€å°åŒ–å‰ªæè¯¯å·®ï¼Œå¹¶å¼•å…¥äº†é’ˆå¯¹å¯åˆ†å‰²é—®é¢˜ç»“æ„çš„äº¤æ›¿æœ€å°åŒ–ç®—æ³•ï¼Œä»¥å®ç°é«˜æ•ˆçš„æƒé‡æ›´æ–°å’Œæ€§èƒ½æ¢å¤ã€‚åœ¨OPTã€LLaMA-3/3.1/3.2å’ŒQwen2.5æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSPAPåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨30%ç¨€ç–ç‡ä¸‹å®ç°äº†1.29å€çš„çº¿æ€§æ¨ç†åŠ é€Ÿå’Œç›¸åº”çš„å†…å­˜å‡å°‘ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå‰ªæLLMsæä¾›äº†ä¸€ç§å®ç”¨çš„ã€åŸºäºä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–å‰ªæé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å‰ªæè¿‡ç¨‹ä¸­å¸¸å¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸”ä¾èµ–å¯å‘å¼æŒ‡æ ‡æˆ–éœ€è¦æ˜‚è´µçš„å¾®è°ƒï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPAPçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ··åˆæ•´æ•°ä¼˜åŒ–æ¨¡å‹æ¥ç³»ç»Ÿæ€§åœ°å®šä¹‰å‰ªæé—®é¢˜ï¼Œç»“åˆæƒ©ç½šæ–¹æ³•æ¥æœ‰æ•ˆåšå‡ºå‰ªæå†³ç­–ï¼Œä»è€Œæœ€å°åŒ–å‰ªæå¸¦æ¥çš„è¯¯å·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPAPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡æ··åˆæ•´æ•°ä¼˜åŒ–æ¨¡å‹å®šä¹‰å‰ªæé—®é¢˜ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨æƒ©ç½šæ–¹æ³•æ¥æŒ‡å¯¼å‰ªæå†³ç­–ï¼›æœ€åï¼Œå¼•å…¥äº¤æ›¿æœ€å°åŒ–ç®—æ³•è¿›è¡Œé«˜æ•ˆçš„æƒé‡æ›´æ–°å’Œæ€§èƒ½æ¢å¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPAPçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ··åˆæ•´æ•°ä¼˜åŒ–ä¸æƒ©ç½šæ–¹æ³•ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å‰ªæå†³ç­–æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†å‰ªææ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½çš„ä¿æŒï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSPAPè®¾ç½®äº†é€‚å½“çš„æƒ©ç½šå‚æ•°ï¼Œä»¥å¹³è¡¡å‰ªæå†³ç­–çš„å‡†ç¡®æ€§å’Œæ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶é‡‡ç”¨äº†é€‚åº”æ€§æƒé‡æ›´æ–°ç­–ç•¥ï¼Œä»¥ç¡®ä¿åœ¨å‰ªæè¿‡ç¨‹ä¸­æ¨¡å‹æ€§èƒ½çš„æ¢å¤ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SPAPåœ¨OPTã€LLaMA-3/3.1/3.2å’ŒQwen2.5æ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨30%ç¨€ç–ç‡ä¸‹å®ç°äº†1.29å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”å†…å­˜ä½¿ç”¨é‡æ˜¾è‘—å‡å°‘ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å‰ªææ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SPAPçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨ç†å’Œå†…å­˜ç®¡ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœºæ™¯ä¸­ã€‚å…¶ä¼˜åŒ–é©±åŠ¨çš„å‰ªææ–¹æ³•èƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æœ‰æ•ˆåˆ©ç”¨LLMsï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The deployment of large language models (LLMs) is often constrained by their substantial computational and memory demands. While structured pruning presents a viable approach by eliminating entire network components, existing methods suffer from performance degradation, reliance on heuristic metrics, or expensive finetuning. To address these challenges, we propose SPAP (Structured Pruning via Alternating Optimization and Penalty Methods), a novel and efficient structured pruning framework for LLMs grounded in optimization theory. SPAP formulates the pruning problem through a mixed-integer optimization model, employs a penalty method that effectively makes pruning decisions to minimize pruning errors, and introduces an alternating minimization algorithm tailored to the splittable problem structure for efficient weight updates and performance recovery. Extensive experiments on OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at 30% sparsity) and proportional memory reductions. Our work offers a practical, optimization-driven solution for pruning LLMs while preserving model performance.

