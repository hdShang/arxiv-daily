---
layout: default
title: A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)
---

# A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03490" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03490v1</a>
  <a href="https://arxiv.org/pdf/2505.03490.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03490v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03490v1', 'A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Faiz Taleb, Ivan Gazeau, Maryline Laurent

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLBRMç®—æ³•ä»¥è§£å†³ç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç”Ÿæˆæ¨¡å‹` `éšç§ä¿æŠ¤` `æˆå‘˜æ¨æ–­` `æ—¶é—´åºåˆ—æ’è¡¥` `æŸå¤±å‡½æ•°` `å‚è€ƒæ¨¡å‹` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ— æ„ä¸­è®°å¿†è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´éšç§æ³„éœ²é£é™©ã€‚
2. æœ¬æ–‡æå‡ºçš„LBRMç®—æ³•é€šè¿‡å‚è€ƒæ¨¡å‹æ¥å¢å¼ºæˆå‘˜æ¨æ–­æ”»å‡»çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLBRMç®—æ³•åœ¨æœªè°ƒä¼˜å’Œè°ƒä¼˜æƒ…å†µä¸‹ï¼ŒAUROCåˆ†åˆ«æå‡çº¦40%å’Œ60%ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆæ¨¡å‹å¯èƒ½æ— æ„ä¸­è®°å¿†è®­ç»ƒæ•°æ®ï¼Œä»è€Œå¸¦æ¥æ˜¾è‘—çš„éšç§é£é™©ã€‚æœ¬æ–‡é’ˆå¯¹æ—¶é—´åºåˆ—æ’è¡¥æ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡ï¼Œæå‡ºäº†åŸºäºæŸå¤±çš„å‚è€ƒæ¨¡å‹ç®—æ³•ï¼ˆLBRMï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å‚è€ƒæ¨¡å‹æé«˜äº†æˆå‘˜æ¨æ–­æ”»å‡»çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šé¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•æ¥æœ‰æ•ˆæå–å’Œè¯†åˆ«è®°å¿†çš„è®­ç»ƒæ•°æ®ï¼Œæ£€æµ‹å‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œæœªè°ƒä¼˜æƒ…å†µä¸‹AUROCå¹³å‡æå‡çº¦40%ï¼Œè°ƒä¼˜åAUROCæå‡çº¦60%ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ä¸¤ç§æ—¶é—´åºåˆ—æ’è¡¥æ¶æ„çš„æˆå‘˜æ¨æ–­æ”»å‡»éªŒè¯äº†è¯¥æ–¹æ³•çš„é²æ£’æ€§å’Œå¤šæ ·æ€§ï¼Œå¼ºè°ƒäº†LBRMåœ¨æé«˜æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢çš„é‡è¦æ€§ï¼Œè§£å†³äº†æ—¶é—´åºåˆ—æ’è¡¥æ¨¡å‹ä¸­çš„éšç§é£é™©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç”Ÿæˆæ¨¡å‹ä¸­è®­ç»ƒæ•°æ®çš„è®°å¿†åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯†åˆ«è®°å¿†æ•°æ®æ—¶å‡†ç¡®æ€§ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆä¿æŠ¤éšç§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLBRMç®—æ³•é€šè¿‡å¼•å…¥å‚è€ƒæ¨¡å‹ï¼Œå¢å¼ºäº†æˆå‘˜æ¨æ–­æ”»å‡»çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ï¼Œä»è€Œæœ‰æ•ˆè¯†åˆ«è®°å¿†æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLBRMç®—æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å‚è€ƒæ¨¡å‹æ„å»ºã€æŸå¤±è®¡ç®—å’Œæˆå‘˜æ¨æ–­å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹è¾“å…¥æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åæ„å»ºå‚è€ƒæ¨¡å‹ä»¥è·å–åŸºå‡†æŸå¤±ï¼Œæ¥ç€è®¡ç®—ç›®æ ‡æ¨¡å‹çš„æŸå¤±ï¼Œå¹¶é€šè¿‡æ¯”è¾ƒæ¥è¿›è¡Œæˆå‘˜æ¨æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šLBRMç®—æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨å‚è€ƒæ¨¡å‹æ¥æå‡æˆå‘˜æ¨æ–­æ”»å‡»çš„å‡†ç¡®æ€§ï¼Œè¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†å¯¹è®°å¿†æ•°æ®çš„è¯†åˆ«èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•å®ç°ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å‚è€ƒæ¨¡å‹çš„é€‰æ‹©ã€æŸå¤±å‡½æ•°çš„è®¾è®¡ä»¥åŠç½‘ç»œç»“æ„çš„ä¼˜åŒ–ã€‚é€šè¿‡åˆç†è®¾ç½®è¿™äº›å‚æ•°ï¼ŒLBRMç®—æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„æ—¶é—´åºåˆ—æ’è¡¥æ¶æ„ä¸­ä¿æŒé«˜æ•ˆçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLBRMç®—æ³•åœ¨æœªè°ƒä¼˜æƒ…å†µä¸‹AUROCå¹³å‡æå‡çº¦40%ï¼Œè€Œåœ¨è°ƒä¼˜åAUROCæå‡çº¦60%ã€‚è¿™äº›æ•°æ®è¡¨æ˜ï¼ŒLBRMç®—æ³•åœ¨æˆå‘˜æ¨æ–­æ”»å‡»ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç°å‡ºå…¶åœ¨ä¸åŒæ¶æ„ä¸‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®éšç§ä¿æŠ¤ã€é‡‘èé¢„æµ‹ã€åŒ»ç–—æ•°æ®åˆ†æç­‰ã€‚é€šè¿‡æé«˜ç”Ÿæˆæ¨¡å‹çš„éšç§ä¿æŠ¤èƒ½åŠ›ï¼ŒLBRMç®—æ³•èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æœ‰æ•ˆé˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå®é™…æ„ä¹‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šéšç§ä¿æŠ¤æŠ€æœ¯çš„å‘å±•ï¼Œä¿ƒè¿›ç”Ÿæˆæ¨¡å‹åœ¨å„é¢†åŸŸçš„å®‰å…¨åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generative models can unintentionally memorize training data, posing significant privacy risks. This paper addresses the memorization phenomenon in time series imputation models, introducing the Loss-Based with Reference Model (LBRM) algorithm. The LBRM method leverages a reference model to enhance the accuracy of membership inference attacks, distinguishing between training and test data. Our contributions are twofold: first, we propose an innovative method to effectively extract and identify memorized training data, significantly improving detection accuracy. On average, without fine-tuning, the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased by approximately 60\%. Second, we validate our approach through membership inference attacks on two types of architectures designed for time series imputation, demonstrating the robustness and versatility of the LBRM approach in different contexts. These results highlight the significant enhancement in detection accuracy provided by the LBRM approach, addressing privacy risks in time series imputation models.

