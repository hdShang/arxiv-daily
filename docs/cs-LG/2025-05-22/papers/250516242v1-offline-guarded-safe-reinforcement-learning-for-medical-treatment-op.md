---
layout: default
title: Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies
---

# Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.16242" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.16242v1</a>
  <a href="https://arxiv.org/pdf/2505.16242.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.16242v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.16242v1', 'Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Runze Yan, Xun Shen, Akifumi Wachi, Sebastien Gros, Anni Zhao, Xiao Hu

**åˆ†ç±»**: cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-05-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOGSRLä»¥è§£å†³åŒ»ç–—å¼ºåŒ–å­¦ä¹ ä¸­çš„OODé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `åŒ»ç–—ä¼˜åŒ–` `è¶…å‡ºåˆ†å¸ƒé—®é¢˜` `å®‰å…¨çº¦æŸ` `æ”¿ç­–æ¢ç´¢` `ç”Ÿç†å®‰å…¨è¾¹ç•Œ` `æ²»ç–—ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åŒ»ç–—åº”ç”¨ä¸­é¢ä¸´è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰é—®é¢˜ï¼Œå¯¼è‡´ä¸å½“çš„ç­–ç•¥å»ºè®®ã€‚
2. æœ¬æ–‡æå‡ºçš„OGSRLæ¡†æ¶é€šè¿‡åŒé‡çº¦æŸæœºåˆ¶ï¼Œç¡®ä¿åœ¨å®‰å…¨åŒºåŸŸå†…æ¢ç´¢æ²»ç–—ç­–ç•¥ï¼Œæé«˜æ”¿ç­–çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOGSRLåœ¨å¤šä¸ªåŒ»ç–—åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°æ¥è¿‘æœ€ä¼˜çš„æ²»ç–—ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨åŒ»ç–—åœºæ™¯ä¸­åº”ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ—¶ï¼Œè¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰é—®é¢˜å¸¦æ¥äº†æ˜¾è‘—é£é™©ï¼Œå› ä¸ºä¸å½“çš„æ³›åŒ–å¯èƒ½å¯¼è‡´æœ‰å®³çš„å»ºè®®ã€‚ç°æœ‰æ–¹æ³•å¦‚ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰ä»…é€šè¿‡æŠ‘åˆ¶ä¸ç¡®å®šåŠ¨ä½œæ¥è§£å†³OODé—®é¢˜ï¼Œä½†è¿™ç§ä»…é™äºåŠ¨ä½œçš„æ­£åˆ™åŒ–æœªèƒ½æœ‰æ•ˆè°ƒæ§åç»­çŠ¶æ€è½¨è¿¹ï¼Œä»è€Œé™åˆ¶äº†é•¿æœŸæ²»ç–—ç­–ç•¥çš„å‘ç°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ç¦»çº¿å—ä¿æŠ¤å®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆOGSRLï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åŒé‡çº¦æŸæœºåˆ¶ï¼Œç¡®ä¿åœ¨å®‰å…¨çš„æ”¿ç­–æ¢ç´¢ä¸­æé«˜ç­–ç•¥çš„å¯é æ€§ã€‚OGSRLå»ºç«‹äº†OODå®ˆæŠ¤è€…ï¼ŒæŒ‡å®šå®‰å…¨çš„æ”¿ç­–æ¢ç´¢åŒºåŸŸï¼Œå¹¶å¼•å…¥å®‰å…¨æˆæœ¬çº¦æŸï¼Œç¼–ç ç”Ÿç†å®‰å…¨è¾¹ç•Œçš„åŒ»å­¦çŸ¥è¯†ï¼Œä»è€Œåœ¨è®­ç»ƒæ•°æ®å¯èƒ½åŒ…å«ä¸å®‰å…¨å¹²é¢„çš„åŒºåŸŸæä¾›é¢†åŸŸç‰¹å®šçš„ä¿æŠ¤ã€‚æˆ‘ä»¬æä¾›äº†å®‰å…¨æ€§å’Œè¿‘ä¼¼æœ€ä¼˜æ€§çš„ç†è®ºä¿è¯ï¼Œç¡®ä¿æ»¡è¶³è¿™äº›çº¦æŸçš„ç­–ç•¥åœ¨å®‰å…¨å¯é çš„åŒºåŸŸå†…ï¼Œå¹¶å®ç°æ¥è¿‘æœ€ä½³æ”¿ç­–çš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨åŒ»ç–—åœºæ™¯ä¸­çš„è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¦‚ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰ä»…é€šè¿‡é™åˆ¶åŠ¨ä½œé€‰æ‹©æ¥åº”å¯¹ï¼Œä½†æœªèƒ½æœ‰æ•ˆè°ƒæ§çŠ¶æ€è½¨è¿¹ï¼Œå¯¼è‡´é•¿æœŸç­–ç•¥çš„å‘ç°å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOGSRLæ¡†æ¶é€šè¿‡å¼•å…¥OODå®ˆæŠ¤è€…å’Œå®‰å…¨æˆæœ¬çº¦æŸï¼Œç¡®ä¿æ”¿ç­–æ¢ç´¢åœ¨ä¸´åºŠéªŒè¯çš„å®‰å…¨åŒºåŸŸå†…è¿›è¡Œï¼Œä»è€Œæé«˜ç­–ç•¥çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOGSRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šOODå®ˆæŠ¤è€…ç”¨äºå®šä¹‰å®‰å…¨æ¢ç´¢åŒºåŸŸï¼Œå®‰å…¨æˆæœ¬çº¦æŸç”¨äºç¼–ç ç”Ÿç†å®‰å…¨è¾¹ç•Œã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å—çš„ååŒä½œç”¨ï¼ŒOGSRLèƒ½å¤Ÿæœ‰æ•ˆæ¢ç´¢è¶…è¶Šä¸´åºŠè¡Œä¸ºçš„æ²»ç–—ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šOGSRLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåŒé‡çº¦æŸæœºåˆ¶çš„å¼•å…¥ï¼ŒåŒºåˆ«äºç°æœ‰æ–¹æ³•ä»…ä¾èµ–äºåŠ¨ä½œé€‰æ‹©çš„æ­£åˆ™åŒ–ï¼ŒOGSRLåŒæ—¶è°ƒæ§çŠ¶æ€å’ŒåŠ¨ä½œï¼Œç¡®ä¿ç­–ç•¥åœ¨å®‰å…¨åŒºåŸŸå†…ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨OGSRLä¸­ï¼ŒOODå®ˆæŠ¤è€…é€šè¿‡å®šä¹‰å®‰å…¨åŒºåŸŸæ¥çº¦æŸä¼˜åŒ–è¿‡ç¨‹ï¼Œè€Œå®‰å…¨æˆæœ¬çº¦æŸåˆ™é€šè¿‡æŸå¤±å‡½æ•°ç¼–ç åŒ»å­¦çŸ¥è¯†ï¼Œç¡®ä¿å³ä½¿åœ¨è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨æ½œåœ¨ä¸å®‰å…¨å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œç­–ç•¥ä¹Ÿèƒ½ä¿æŒå®‰å…¨æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒè®ºæ–‡çš„å…·ä½“å†…å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOGSRLåœ¨å¤šä¸ªåŒ»ç–—åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä¿å®ˆQå­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå®‰å…¨æ€§çš„åŒæ—¶å®ç°æ¥è¿‘æœ€ä½³æ”¿ç­–çš„è¡¨ç°ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨åŒ»ç–—ä¼˜åŒ–ç­–ç•¥ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OGSRLçš„ç ”ç©¶æˆæœåœ¨åŒ»ç–—å†³ç­–æ”¯æŒç³»ç»Ÿä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿä¸ºåŒ»ç”Ÿæä¾›æ›´å®‰å…¨çš„æ²»ç–—å»ºè®®ï¼Œä¼˜åŒ–æ‚£è€…çš„æ²»ç–—æ–¹æ¡ˆã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨åŒ»ç–—é¢†åŸŸçš„æ™ºèƒ½åŒ–å‘å±•ï¼Œæé«˜åŒ»ç–—æœåŠ¡çš„è´¨é‡å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> When applying offline reinforcement learning (RL) in healthcare scenarios, the out-of-distribution (OOD) issues pose significant risks, as inappropriate generalization beyond clinical expertise can result in potentially harmful recommendations. While existing methods like conservative Q-learning (CQL) attempt to address the OOD issue, their effectiveness is limited by only constraining action selection by suppressing uncertain actions. This action-only regularization imitates clinician actions that prioritize short-term rewards, but it fails to regulate downstream state trajectories, thereby limiting the discovery of improved long-term treatment strategies. To safely improve policy beyond clinician recommendations while ensuring that state-action trajectories remain in-distribution, we propose \textit{Offline Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel dual constraint mechanism for improving policy with reliability and safety. First, the OOD guardian is established to specify clinically validated regions for safe policy exploration. By constraining optimization within these regions, it enables the reliable exploration of treatment strategies that outperform clinician behavior by leveraging the full patient state history, without drifting into unsupported state-action trajectories. Second, we introduce a safety cost constraint that encodes medical knowledge about physiological safety boundaries, providing domain-specific safeguards even in areas where training data might contain potentially unsafe interventions. Notably, we provide theoretical guarantees on safety and near-optimality: policies that satisfy these constraints remain in safe and reliable regions and achieve performance close to the best possible policy supported by the data.

