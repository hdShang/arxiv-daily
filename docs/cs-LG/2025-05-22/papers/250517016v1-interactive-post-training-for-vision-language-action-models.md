---
layout: default
title: Interactive Post-Training for Vision-Language-Action Models
---

# Interactive Post-Training for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.17016" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.17016v1</a>
  <a href="https://arxiv.org/pdf/2505.17016.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.17016v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.17016v1', 'Interactive Post-Training for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuhan Tan, Kairan Dou, Yue Zhao, Philipp KrÃ¤henbÃ¼hl

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-22

**å¤‡æ³¨**: Project page: https://ariostgx.github.io/ript_vla/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRIPT-VLAä»¥è§£å†³VLAæ¨¡å‹é€‚åº”æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `å¼ºåŒ–å­¦ä¹ ` `åè®­ç»ƒ` `ç¨€ç–å¥–åŠ±` `æ¨¡å‹å¾®è°ƒ` `åŠ¨æ€å›æ»šé‡‡æ ·` `ä¼˜åŠ¿ä¼°è®¡` `é€‚åº”æ€§å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„VLAè®­ç»ƒæ–¹æ³•è¿‡äºä¾èµ–ä¸“å®¶ç¤ºèŒƒæ•°æ®ï¼Œå¯¼è‡´åœ¨æ–°ä»»åŠ¡å’Œä½æ•°æ®ç¯å¢ƒä¸‹é€‚åº”æ€§ä¸è¶³ã€‚
2. RIPT-VLAé€šè¿‡å¼ºåŒ–å­¦ä¹ çš„äº¤äº’å¼åè®­ç»ƒï¼Œåˆ©ç”¨ç¨€ç–çš„æˆåŠŸå¥–åŠ±è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œæå‡äº†é€‚åº”æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRIPT-VLAåœ¨å¤šä¸ªæ¨¡å‹ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯7B OpenVLA-OFTæ¨¡å‹æˆåŠŸç‡è¾¾åˆ°97.5%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†RIPT-VLAï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„äº¤äº’å¼åè®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡ç¨€ç–çš„äºŒå…ƒæˆåŠŸå¥–åŠ±å¯¹é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç°æœ‰çš„VLAè®­ç»ƒæµç¨‹ä¸¥é‡ä¾èµ–ç¦»çº¿ä¸“å®¶ç¤ºèŒƒæ•°æ®å’Œç›‘ç£æ¨¡ä»¿ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹é€‚åº”æ–°ä»»åŠ¡å’Œæ–°ç¯å¢ƒçš„èƒ½åŠ›ã€‚RIPT-VLAé€šè¿‡åŠ¨æ€å›æ»šé‡‡æ ·å’Œé€ä¸€ä¼˜åŠ¿ä¼°è®¡çš„ç¨³å®šç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚RIPT-VLAé€‚ç”¨äºå¤šç§VLAæ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†è½»é‡çº§QueSTæ¨¡å‹çš„æ€§èƒ½21.2%ï¼Œå¹¶ä½¿7B OpenVLA-OFTæ¨¡å‹çš„æˆåŠŸç‡è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„97.5%ã€‚æ­¤å¤–ï¼ŒRIPT-VLAåœ¨ä»…éœ€ä¸€ä¸ªç¤ºèŒƒçš„æƒ…å†µä¸‹ï¼Œä½¿å¾—åŸæœ¬æ— æ³•å·¥ä½œçš„SFTæ¨¡å‹ï¼ˆ4%ï¼‰åœ¨15æ¬¡è¿­ä»£å†…æˆåŠŸç‡è¾¾åˆ°äº†97%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æ–°ä»»åŠ¡å’Œä½æ•°æ®ç¯å¢ƒä¸‹é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡çš„ç¦»çº¿ä¸“å®¶ç¤ºèŒƒæ•°æ®å’Œç›‘ç£æ¨¡ä»¿ï¼Œé™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRIPT-VLAæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„äº¤äº’å¼åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ç¨€ç–çš„äºŒå…ƒæˆåŠŸå¥–åŠ±è¿›è¡Œå¾®è°ƒï¼Œå…è®¸æ¨¡å‹åœ¨å°‘é‡ç¤ºèŒƒä¸‹è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ï¼Œä»è€Œå¢å¼ºå…¶é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRIPT-VLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨æ€å›æ»šé‡‡æ ·å’Œé€ä¸€ä¼˜åŠ¿ä¼°è®¡ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚åŠ¨æ€å›æ»šé‡‡æ ·ç”¨äºç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼Œè€Œé€ä¸€ä¼˜åŠ¿ä¼°è®¡åˆ™ç”¨äºç¨³å®šç­–ç•¥ä¼˜åŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ”¶æ•›æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šRIPT-VLAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨ä»…æœ‰ç¨€ç–å¥–åŠ±çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„åè®­ç»ƒï¼Œæ˜¾è‘—é™ä½äº†å¯¹å¤§é‡ç¤ºèŒƒæ•°æ®çš„ä¾èµ–ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒRIPT-VLAé‡‡ç”¨äº†åŠ¨æ€å›æ»šé‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜æ ·æœ¬çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§ã€‚åŒæ—¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡äºç¨€ç–å¥–åŠ±çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„ä¿¡æ¯ä¸‹è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RIPT-VLAåœ¨å¤šä¸ªæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„å®éªŒç»“æœï¼Œç‰¹åˆ«æ˜¯è½»é‡çº§QueSTæ¨¡å‹æ€§èƒ½æå‡21.2%ï¼Œè€Œ7B OpenVLA-OFTæ¨¡å‹æˆåŠŸç‡è¾¾åˆ°äº†97.5%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä»…éœ€ä¸€ä¸ªç¤ºèŒƒçš„æƒ…å†µä¸‹ï¼Œä½¿å¾—åŸæœ¬æˆåŠŸç‡ä»…ä¸º4%çš„SFTæ¨¡å‹åœ¨15æ¬¡è¿­ä»£å†…æˆåŠŸç‡æå‡è‡³97%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RIPT-VLAçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚é€šè¿‡å‡å°‘å¯¹å¤§é‡ç¤ºèŒƒæ•°æ®çš„ä¾èµ–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿å¾—æ¨¡å‹åœ¨æ–°ç¯å¢ƒå’Œæ–°ä»»åŠ¡ä¸­å¿«é€Ÿé€‚åº”ï¼Œæå‡å®é™…åº”ç”¨çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚æœªæ¥ï¼ŒRIPT-VLAæœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€å­¦ä¹ å’Œäººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.
>   RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.

