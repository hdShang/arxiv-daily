---
layout: default
title: Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach
---

# Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.16403" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.16403v2</a>
  <a href="https://arxiv.org/pdf/2505.16403.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.16403v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.16403v2', 'Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huazi Pan, Yanjun Zhang, Leo Yu Zhang, Scott Adams, Abbas Kouzani, Suiyang Khoo

**åˆ†ç±»**: cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-05-22 (æ›´æ–°: 2025-05-29)

**å¤‡æ³¨**: This paper is to appear in IJCAI 2025, code available at: https://github.com/Halsey777/FedSA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ»‘æ¨¡æ§åˆ¶æ–¹æ³•ä»¥è§£å†³è”é‚¦å­¦ä¹ ä¸­çš„æ•°æ®æŠ•æ¯’æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `æ•°æ®æŠ•æ¯’` `æ»‘æ¨¡æ§åˆ¶` `æ¨¡å‹å®‰å…¨` `æ¶æ„æ”»å‡»` `éçº¿æ€§æ§åˆ¶` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŠ•æ¯’æ”»å‡»æ–¹æ³•å¤šé›†ä¸­äºå¯¼è‡´æœåŠ¡æ‹’ç»ï¼Œç¼ºä¹å¯¹å…¨å±€æ¨¡å‹å‡†ç¡®ç‡çš„ç²¾ç¡®æ§åˆ¶ã€‚
2. æœ¬æ–‡æå‡ºçš„FedSAæ–¹æ³•åˆ©ç”¨æ»‘æ¨¡æ§åˆ¶ç†è®ºï¼Œèƒ½å¤Ÿä»¥å¯æ§çš„æ–¹å¼å¼•å…¥æŠ•æ¯’ï¼Œç²¾å‡†é™ä½å…¨å±€æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedSAåœ¨è¾ƒå°‘çš„æ¶æ„å®¢æˆ·ç«¯ä¸‹ï¼Œèƒ½å¤Ÿå®ç°é¢„è®¾çš„å…¨å±€å‡†ç¡®ç‡ï¼Œå¹¶ä¿æŒéšè”½æ€§å’Œçµæ´»çš„å­¦ä¹ é€Ÿç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸­ï¼Œå±€éƒ¨è®­ç»ƒæ•°æ®å’Œæ›´æ–°çš„æ“æ§ï¼Œå³æ•°æ®æŠ•æ¯’æ”»å‡»ï¼Œæ˜¯ä¸€ç§ä¸»è¦å¨èƒã€‚ç°æœ‰çš„æŠ•æ¯’æ”»å‡»é€šå¸¸å¯¼è‡´æœåŠ¡æ‹’ç»ï¼ˆDoSï¼‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºè”é‚¦å­¦ä¹ æ»‘æ¨¡æ”»å‡»ï¼ˆFedSAï¼‰ï¼Œæ—¨åœ¨ä»¥å¯æ§çš„æ–¹å¼ç²¾ç¡®å¼•å…¥æŠ•æ¯’ç¨‹åº¦ã€‚FedSAç»“åˆäº†å¼ºå¥çš„éçº¿æ€§æ§åˆ¶ç†è®ºä¸æ¨¡å‹æŠ•æ¯’æ”»å‡»ï¼Œèƒ½å¤Ÿæ“æ§æ¶æ„å®¢æˆ·ç«¯çš„æ›´æ–°ï¼Œä»¥æ§åˆ¶çš„é€Ÿç‡å°†å…¨å±€æ¨¡å‹é©±å‘å¦¥åçŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedSAèƒ½å¤Ÿåœ¨è¾ƒå°‘çš„æ¶æ„å®¢æˆ·ç«¯ä¸‹ï¼Œå‡†ç¡®å®ç°é¢„å®šä¹‰çš„å…¨å±€å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒé«˜éšè”½æ€§å’Œå¯è°ƒçš„å­¦ä¹ é€Ÿç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ ä¸­çš„æ•°æ®æŠ•æ¯’æ”»å‡»é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¯¼è‡´å…¨å±€æ¨¡å‹çš„æœåŠ¡æ‹’ç»ï¼Œè€Œç¼ºä¹å¯¹æ¨¡å‹å‡†ç¡®ç‡çš„ç²¾ç¡®æ“æ§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFedSAæ–¹æ³•ç»“åˆæ»‘æ¨¡æ§åˆ¶ç†è®ºï¼Œå…è®¸æ”»å‡»è€…ä»¥å¯æ§çš„æ–¹å¼å¼•å…¥æŠ•æ¯’ï¼Œè®¾å®šå…¨å±€æ¨¡å‹çš„å‡†ç¡®ç‡ç›®æ ‡ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹çš„ç²¾å‡†æ“æ§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¶æ„å®¢æˆ·ç«¯çš„æ›´æ–°æ“æ§ã€æ»‘æ¨¡æ§åˆ¶çš„å®æ–½ä»¥åŠå…¨å±€æ¨¡å‹çš„åé¦ˆæœºåˆ¶ï¼Œç¡®ä¿æ”»å‡»è¿‡ç¨‹çš„éšè”½æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šFedSAçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ»‘æ¨¡æ§åˆ¶ç†è®ºåº”ç”¨äºæ¨¡å‹æŠ•æ¯’æ”»å‡»ï¼Œå…è®¸æ”»å‡»è€…åœ¨ä¸æ˜¾è‘—å½±å“å­¦ä¹ è¿‡ç¨‹çš„æƒ…å†µä¸‹ï¼Œç²¾ç¡®æ§åˆ¶å…¨å±€æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒFedSAè®¾ç½®äº†é¢„å®šä¹‰çš„ç›®æ ‡å‡†ç¡®ç‡ï¼Œå¹¶é€šè¿‡è°ƒæ•´å­¦ä¹ é€Ÿç‡å’Œæ§åˆ¶å‚æ•°ï¼Œç¡®ä¿æ”»å‡»çš„éšè”½æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒæ—¶å‡å°‘æ‰€éœ€çš„æ¶æ„å®¢æˆ·ç«¯æ•°é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFedSAèƒ½å¤Ÿåœ¨ä»…æœ‰å°‘é‡æ¶æ„å®¢æˆ·ç«¯çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸå°†å…¨å±€æ¨¡å‹çš„å‡†ç¡®ç‡é™ä½è‡³é¢„è®¾ç›®æ ‡ï¼Œä¸”åœ¨éšè”½æ€§å’Œå­¦ä¹ é€Ÿç‡çš„å¯è°ƒæ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨æ”»å‡»æ•ˆæœå’Œæ•ˆç‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨çš„è”é‚¦å­¦ä¹ ç³»ç»Ÿã€åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç¯å¢ƒä»¥åŠéœ€è¦ä¿æŠ¤æ¨¡å‹å…å—æ¶æ„æ”»å‡»çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¯¹æŠ•æ¯’æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¢å¼ºè”é‚¦å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Manipulation of local training data and local updates, i.e., the poisoning attack, is the main threat arising from the collaborative nature of the federated learning (FL) paradigm. Most existing poisoning attacks aim to manipulate local data/models in a way that causes denial-of-service (DoS) issues. In this paper, we introduce a novel attack method, named Federated Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the extent of poisoning in a subtle controlled manner. It operates with a predefined objective, such as reducing global model's prediction accuracy by 10%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC) theory with model poisoning attacks. It can manipulate the updates from malicious clients to drive the global model towards a compromised state, achieving this at a controlled and inconspicuous rate. Additionally, leveraging the robust control properties of FedSA allows precise control over the convergence bounds, enabling the attacker to set the global accuracy of the poisoned model to any desired level. Experimental results demonstrate that FedSA can accurately achieve a predefined global accuracy with fewer malicious clients while maintaining a high level of stealth and adjustable learning rates.

