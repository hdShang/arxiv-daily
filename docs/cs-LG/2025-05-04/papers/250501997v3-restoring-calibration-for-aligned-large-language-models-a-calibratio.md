---
layout: default
title: "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach"
---

# Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01997" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.01997v3</a>
  <a href="https://arxiv.org/pdf/2505.01997.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01997v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01997v3', 'Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jiancong Xiao, Bojian Hou, Zhanliang Wang, Ruochen Jin, Qi Long, Weijie J. Su, Li Shen

**ÂàÜÁ±ª**: cs.LG, cs.AI, stat.ML

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-04 (Êõ¥Êñ∞: 2025-10-16)

**ÊúüÂàä**: ICML 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ê†°ÂáÜÊÑüÁü•ÂæÆË∞ÉÊñπÊ≥ï‰ª•Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÊ†°ÂáÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Ê†°ÂáÜ` `ÂæÆË∞É` `ÂÅèÂ•ΩÂØπÈΩê` `Êú∫Âô®Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÂêéÔºåÊ†°ÂáÜÊÄßËÉΩÊòæËëó‰∏ãÈôçÔºåÂØºËá¥Ê®°ÂûãËøáÂ∫¶Ëá™‰ø°„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÈÄöËøáÈ¢ÜÂüüÁâπÂÆöÁü•ËØÜÁöÑÂæÆË∞ÉÊù•Ëß£ÂÜ≥Ê†°ÂáÜÈóÆÈ¢òÔºåÂπ∂ÂºïÂÖ•Ê†°ÂáÜÊÑüÁü•ÁöÑÂæÆË∞ÉÊñπÊ≥ï„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®Ê†°ÂáÜÊÄßËÉΩ‰∏äÊúâÊòæËëóÊèêÂçáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊàêÂäüÁöÑÂÖ≥ÈîÆÊäÄÊúØ‰πã‰∏ÄÊòØÂÅèÂ•ΩÂØπÈΩê„ÄÇÁÑ∂ËÄåÔºåÂÅèÂ•ΩÂØπÈΩêÁöÑ‰∏Ä‰∏™ÊòæËëóÂâØ‰ΩúÁî®ÊòØÊ†°ÂáÜ‰∏çËâØÔºöÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÈÄöÂ∏∏Ê†°ÂáÜËâØÂ•ΩÔºå‰ΩÜÂú®‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÂêéÔºåLLMsÂæÄÂæÄË°®Áé∞Âá∫ËæÉÂ∑ÆÁöÑÊ†°ÂáÜ„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂÅèÂ•ΩÂØπÈΩêÂ¶Ç‰ΩïÂΩ±ÂìçÊ†°ÂáÜÂèäÂÖ∂Ëß£ÂÜ≥ÊñπÊ°à„ÄÇÊàë‰ª¨ËßÇÂØüÂà∞ÔºåÂÅèÂ•ΩÂ¥©Ê∫ÉÈóÆÈ¢òÂú®Ê†°ÂáÜÂú∫ÊôØ‰∏≠‰∏çËâØÂú∞Ê≥õÂåñÔºåÂØºËá¥LLMsË°®Áé∞Âá∫ËøáÂ∫¶Ëá™‰ø°ÂíåÊ†°ÂáÜ‰∏çËâØ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨Âº∫Ë∞É‰ΩøÁî®È¢ÜÂüüÁâπÂÆöÁü•ËØÜËøõË°åÂæÆË∞ÉÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÁºìËß£ËøáÂ∫¶Ëá™‰ø°ÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨Â∞ÜÊ®°ÂûãÂàÜ‰∏∫ÂèØÊ†°ÂáÜÂíå‰∏çÂèØÊ†°ÂáÜ‰∏§ÁßçÊÉÖÂÜµÔºåÂπ∂ÊèêÂá∫Ê†°ÂáÜÊÑüÁü•ÂæÆË∞ÉÊñπÊ≥ïÔºå‰ª•Âú®‰∏çÂΩ±ÂìçLLMsÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ÈÄÇÂΩìÊ†°ÂáÜ„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÈ™åËØÅ‰∫ÜÊâÄÊèêÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÂêéÂá∫Áé∞ÁöÑÊ†°ÂáÜ‰∏çËâØÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÂÅèÂ•ΩÂØπÈΩêËøáÁ®ã‰∏≠ÂØºËá¥Ê®°ÂûãËøáÂ∫¶Ëá™‰ø°ÔºåÂΩ±ÂìçÂÖ∂Ê†°ÂáÜÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÈÄöËøáÈ¢ÜÂüüÁâπÂÆöÁü•ËØÜÁöÑÂæÆË∞ÉÊù•ÊîπÂñÑÊ®°ÂûãÁöÑÊ†°ÂáÜÊÄßËÉΩÔºåÂº∫Ë∞ÉÊ†°ÂáÜÊÑüÁü•ÂæÆË∞ÉÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÈÅøÂÖçÊ®°ÂûãÂú®ËøΩÊ±ÇÊÄßËÉΩÊó∂ËøõÂÖ•‰∏çÂèØÊ†°ÂáÜÁöÑÁä∂ÊÄÅ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊñπÊ≥ïÂåÖÊã¨‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÊòØÊ†°ÂáÜÊÑüÁü•ÂæÆË∞ÉÔºåÈíàÂØπÂèØÊ†°ÂáÜÊ®°ÂûãËøõË°å‰ºòÂåñÔºõÂÖ∂Ê¨°ÊòØÈíàÂØπ‰∏çÂèØÊ†°ÂáÜÊ®°ÂûãÔºåÈááÁî®Âü∫‰∫éEMÁÆóÊ≥ïÁöÑÊúüÊúõÊ†°ÂáÜËØØÂ∑ÆÔºàECEÔºâÊ≠£ÂàôÂåñÊù•ÊéßÂà∂ÂæÆË∞ÉÊçüÂ§±„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÊ†°ÂáÜÊÑüÁü•ÂæÆË∞ÉÊñπÊ≥ïÂíåECEÊ≠£ÂàôÂåñÁ≠ñÁï•ÔºåËÉΩÂ§üÂú®‰∏çÂêåÁöÑÊ®°ÂûãÁä∂ÊÄÅ‰∏ãÊúâÊïàÂú∞‰øùÊåÅÊ†°ÂáÜÊÄßËÉΩÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠ÔºåËÆæÁΩÆ‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•ÂºïÂÖ•Ê†°ÂáÜËØØÂ∑ÆÁöÑÁ∫¶ÊùüÔºåÂπ∂ÈÄöËøáEMÁÆóÊ≥ï‰ºòÂåñÊ®°ÂûãÂèÇÊï∞ÔºåÁ°Æ‰øùÂú®ÊèêÂçáÊÄßËÉΩÁöÑÂêåÊó∂ÊéßÂà∂Ê†°ÂáÜËØØÂ∑Æ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÊ†°ÂáÜÊÑüÁü•ÂæÆË∞ÉÊñπÊ≥ïÂú®ÂèØÊ†°ÂáÜÊ®°Âûã‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊ†°ÂáÜÊÄßËÉΩÊèêÂçáÔºåÊúüÊúõÊ†°ÂáÜËØØÂ∑ÆÔºàECEÔºâÈôç‰Ωé‰∫Ü20%‰ª•‰∏ä„ÄÇÂêåÊó∂ÔºåÂú®‰øùÊåÅÊ®°ÂûãÊÄßËÉΩÁöÑÂâçÊèê‰∏ãÔºåÊàêÂäüÈÅøÂÖç‰∫ÜËøõÂÖ•‰∏çÂèØÊ†°ÂáÜÁä∂ÊÄÅ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂØπËØùÁ≥ªÁªüÂíåÊé®ËçêÁ≥ªÁªüÁ≠âÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÈù†ÊÄßÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåÈöèÁùÄÊ®°ÂûãËßÑÊ®°ÁöÑ‰∏çÊñ≠Êâ©Â§ßÔºåÊ†°ÂáÜÈóÆÈ¢òÂ∞ÜÊÑàÂèëÈáçË¶ÅÔºåÊú¨ÊñáÁöÑÊñπÊ≥ïÊúâÂä©‰∫éÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.

