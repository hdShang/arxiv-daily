---
layout: default
title: An Empirical Study of Qwen3 Quantization
---

# An Empirical Study of Qwen3 Quantization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02214" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02214v1</a>
  <a href="https://arxiv.org/pdf/2505.02214.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02214v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02214v1', 'An Empirical Study of Qwen3 Quantization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingyu Zheng, Yuye Li, Haoran Chu, Yue Feng, Xudong Ma, Jie Luo, Jinyang Guo, Haotong Qin, Michele Magno, Xianglong Liu

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Efficient-ML/Qwen3-Quantization) | [HUGGINGFACE](https://huggingface.co/collections/Efficient-ML)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç³»ç»Ÿè¯„ä¼°Qwen3é‡åŒ–æŠ€æœ¯ä»¥æå‡èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç„¶è¯­è¨€å¤„ç†` `åè®­ç»ƒé‡åŒ–` `æ€§èƒ½è¯„ä¼°` `èµ„æºå—é™ç¯å¢ƒ` `æ¨¡å‹å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¾€å¾€å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨è¶…ä½ç²¾åº¦è®¾ç½®ä¸‹ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯„ä¼°Qwen3åœ¨ä¸åŒé‡åŒ–è®¾ç½®ä¸‹çš„è¡¨ç°ï¼Œæ¢ç´¢äº†å‹ç¼©æ¨¡å‹çš„æœºé‡ä¸æŒ‘æˆ˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQwen3åœ¨ä¸­ç­‰æ¯”ç‰¹å®½åº¦ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æä½ç²¾åº¦ä¸‹æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼Œå¼ºè°ƒäº†è¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Qwenç³»åˆ—ä½œä¸ºé¢†å…ˆçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚éšç€Qwen3çš„å‘å¸ƒï¼Œå…¶åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œç„¶è€Œåœ¨èµ„æºå—é™ç¯å¢ƒä¸­é«˜æ•ˆéƒ¨ç½²è¿™äº›æ¨¡å‹çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚ä½æ¯”ç‰¹é‡åŒ–è¢«è®¤ä¸ºæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å…¶å¯¹Qwen3æ€§èƒ½çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†Qwen3åœ¨ä¸åŒé‡åŒ–è®¾ç½®ä¸‹çš„é²æ£’æ€§ï¼Œæ—¨åœ¨æ­ç¤ºå‹ç¼©è¿™ä¸€å…ˆè¿›æ¨¡å‹çš„æœºé‡ä¸æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä¸¥æ ¼è¯„ä¼°äº†5ç§ç»å…¸çš„åè®­ç»ƒé‡åŒ–æŠ€æœ¯ï¼Œæ¶µç›–1åˆ°8æ¯”ç‰¹çš„æ¯”ç‰¹å®½åº¦ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°å…¶æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡Qwen3åœ¨ä¸­ç­‰æ¯”ç‰¹å®½åº¦ä¸‹ä¿æŒäº†ç«äº‰åŠ›ï¼Œä½†åœ¨è¶…ä½ç²¾åº¦ä¸‹è¯­è¨€ä»»åŠ¡çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œçªæ˜¾äº†LLMå‹ç¼©ä¸­çš„æŒç»­æŒ‘æˆ˜ã€‚æˆ‘ä»¬æœŸå¾…è¿™ä¸€å®è¯åˆ†æä¸ºæœªæ¥é‡åŒ–æ–¹æ³•çš„æ”¹è¿›æä¾›å¯è¡Œçš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³Qwen3åœ¨ä½æ¯”ç‰¹é‡åŒ–ä¸‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æä½ç²¾åº¦ä¸‹çš„è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿè¯„ä¼°5ç§ç»å…¸çš„åè®­ç»ƒé‡åŒ–æŠ€æœ¯ï¼Œåˆ†æä¸åŒæ¯”ç‰¹å®½åº¦å¯¹Qwen3æ€§èƒ½çš„å½±å“ï¼Œæ—¨åœ¨æ‰¾åˆ°åœ¨å‹ç¼©æ¨¡å‹æ—¶ä¿æŒæ€§èƒ½çš„æœ€ä½³ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤šç§é‡åŒ–æŠ€æœ¯ï¼Œæ¶µç›–1åˆ°8æ¯”ç‰¹çš„æ¯”ç‰¹å®½åº¦ï¼Œè¯„ä¼°å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ¨¡å‹è®­ç»ƒã€é‡åŒ–å®æ–½åŠæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†Qwen3åœ¨ä¸åŒé‡åŒ–è®¾ç½®ä¸‹çš„é²æ£’æ€§ï¼Œæ­ç¤ºäº†åœ¨æä½ç²¾åº¦ä¸‹æ€§èƒ½ä¸‹é™çš„å…·ä½“åŸå› ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„åˆ†æè§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§é‡åŒ–æŠ€æœ¯ï¼Œè®¾ç½®äº†ä¸åŒçš„æ¯”ç‰¹å®½åº¦ï¼Œå¹¶åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç¡®ä¿äº†ç»“æœçš„å…¨é¢æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQwen3åœ¨ä¸­ç­‰æ¯”ç‰¹å®½åº¦ï¼ˆå¦‚4æ¯”ç‰¹ï¼‰ä¸‹ä»èƒ½ä¿æŒç«äº‰åŠ›ï¼Œç„¶è€Œåœ¨è¶…ä½ç²¾åº¦ï¼ˆå¦‚1æ¯”ç‰¹ï¼‰ä¸‹ï¼Œè¯­è¨€ä»»åŠ¡çš„æ€§èƒ½ä¸‹é™æ˜¾è‘—ï¼Œè¡¨æ˜åœ¨æç«¯é‡åŒ–åœºæ™¯ä¸‹ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ä»¥å‡å°‘æ€§èƒ½æŸå¤±ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

æœ¬ç ”ç©¶çš„æˆæœå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—åœºæ™¯ä¸­ã€‚é€šè¿‡ä¼˜åŒ–é‡åŒ–æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¸æ˜¾è‘—æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„å®ç”¨æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¿»è¯‘ç­‰åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.

