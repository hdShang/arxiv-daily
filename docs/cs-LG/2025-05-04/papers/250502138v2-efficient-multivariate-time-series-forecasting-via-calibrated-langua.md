---
layout: default
title: Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation
---

# Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02138" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02138v2</a>
  <a href="https://arxiv.org/pdf/2505.02138.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02138v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02138v2', 'Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenxi Liu, Hao Miao, Qianxiong Xu, Shaowen Zhou, Cheng Long, Yan Zhao, Ziyue Li, Rui Zhao

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04 (æ›´æ–°: 2025-05-06)

**å¤‡æ³¨**: Accepted by ICDE 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTimeKDæ¡†æ¶ä»¥æé«˜å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹` `æ ¡å‡†è¯­è¨€æ¨¡å‹` `ç‰¹æƒçŸ¥è¯†è’¸é¦` `å‡æ³•äº¤å‰æ³¨æ„åŠ›` `é«˜æ•ˆæ¨ç†` `æœºå™¨å­¦ä¹ ` `æ•°æ®åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µæ•ˆç‡ä½ä¸‹ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„TimeKDæ¡†æ¶ç»“åˆæ ¡å‡†è¯­è¨€æ¨¡å‹å’Œç‰¹æƒçŸ¥è¯†è’¸é¦ï¼Œæ—¨åœ¨æå‡é¢„æµ‹æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeKDåœ¨çœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†é¢„æµ‹æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆMTSFï¼‰æ—¨åœ¨æ ¹æ®å†å²æ•°æ®é¢„æµ‹æœªæ¥è§‚å¯Ÿå€¼ï¼Œåœ¨æ—¶é—´åºåˆ—æ•°æ®ç®¡ç†ç³»ç»Ÿä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œè¿‘æœŸç ”ç©¶é€šè¿‡æ–‡æœ¬æç¤ºè°ƒä¼˜å°†LLMsçš„çŸ¥è¯†èå…¥MTSFã€‚ç„¶è€Œï¼ŒLLMsåœ¨æ¨ç†é˜¶æ®µçš„ä½æ•ˆç‡é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TimeKDï¼Œä¸€ä¸ªé«˜æ•ˆçš„MTSFæ¡†æ¶ï¼Œåˆ©ç”¨æ ¡å‡†è¯­è¨€æ¨¡å‹å’Œç‰¹æƒçŸ¥è¯†è’¸é¦ã€‚TimeKDæ—¨åœ¨ä»äº¤å‰æ¨¡æ€æ•™å¸ˆæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æœªæ¥è¡¨ç¤ºï¼Œå¹¶åŸ¹å…»æœ‰æ•ˆçš„å­¦ç”Ÿæ¨¡å‹ã€‚äº¤å‰æ¨¡æ€æ•™å¸ˆæ¨¡å‹é‡‡ç”¨å¸¦æœ‰çœŸå®æç¤ºçš„æ ¡å‡†è¯­è¨€æ¨¡å‹ï¼Œå—åˆ°ç‰¹æƒä¿¡æ¯å­¦ä¹ ï¼ˆLUPIï¼‰èŒƒå¼çš„å¯å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å‡æ³•äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥ä¼˜åŒ–è¿™äº›è¡¨ç¤ºã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒTimeKDåœ¨æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚è¿™ç§ä½æ•ˆç‡é™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å®æ—¶é¢„æµ‹çš„åœºæ™¯ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ ¡å‡†è¯­è¨€æ¨¡å‹å’Œç‰¹æƒçŸ¥è¯†è’¸é¦ï¼Œæå‡å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡äº¤å‰æ¨¡æ€æ•™å¸ˆæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æœªæ¥è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å­¦ç”Ÿæ¨¡å‹è¿›è¡Œæœ‰æ•ˆçš„çŸ¥è¯†ä¼ é€’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTimeKDæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šäº¤å‰æ¨¡æ€æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ã€‚æ•™å¸ˆæ¨¡å‹ä½¿ç”¨æ ¡å‡†è¯­è¨€æ¨¡å‹ç”Ÿæˆæœªæ¥è¡¨ç¤ºï¼Œè€Œå­¦ç”Ÿæ¨¡å‹åˆ™é€šè¿‡ç‰¹æƒçŸ¥è¯†è’¸é¦å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æå‡ºäº†ç‰¹æƒçŸ¥è¯†è’¸é¦ï¼ˆPKDï¼‰æœºåˆ¶ï¼ŒåŒ…æ‹¬ç›¸å…³æ€§å’Œç‰¹å¾è’¸é¦ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿåœ¨æœ€å°åŒ–è¾“å‡ºå·®å¼‚çš„åŒæ—¶å¤åˆ¶æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºã€‚è¿™ä¸€æœºåˆ¶æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†å‡æ³•äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSCAï¼‰æ¥ä¼˜åŒ–è¡¨ç¤ºï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†ç‰¹æƒçŸ¥è¯†è’¸é¦çš„ç›¸å…³æ€§å’Œç‰¹å¾æŸå¤±ï¼Œç¡®ä¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTimeKDæ¡†æ¶åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œé¢„æµ‹å‡†ç¡®ç‡æé«˜äº†15%ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†30%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTimeKDåœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šå‡å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èé¢„æµ‹ã€æ°”è±¡é¢„æŠ¥å’Œæ™ºèƒ½åˆ¶é€ ç­‰å¤šä¸ªéœ€è¦å®æ—¶æ•°æ®åˆ†æå’Œé¢„æµ‹çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„æ•ˆç‡ï¼ŒTimeKDèƒ½å¤Ÿä¸ºå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›æ›´å¿«é€Ÿå’Œå‡†ç¡®çš„é¢„æµ‹ç»“æœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multivariate time series forecasting (MTSF) endeavors to predict future observations given historical data, playing a crucial role in time series data management systems. With advancements in large language models (LLMs), recent studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF. However, the deployment of LLMs often suffers from low efficiency during the inference phase. To address this problem, we introduce TimeKD, an efficient MTSF framework that leverages the calibrated language models and privileged knowledge distillation. TimeKD aims to generate high-quality future representations from the proposed cross-modality teacher model and cultivate an effective student model. The cross-modality teacher model adopts calibrated language models (CLMs) with ground truth prompts, motivated by the paradigm of Learning Under Privileged Information (LUPI). In addition, we design a subtractive cross attention (SCA) mechanism to refine these representations. To cultivate an effective student model, we propose an innovative privileged knowledge distillation (PKD) mechanism including correlation and feature distillation. PKD enables the student to replicate the teacher's behavior while minimizing their output discrepancy. Extensive experiments on real data offer insight into the effectiveness, efficiency, and scalability of the proposed TimeKD.

