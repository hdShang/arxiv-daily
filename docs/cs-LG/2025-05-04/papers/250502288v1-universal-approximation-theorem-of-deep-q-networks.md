---
layout: default
title: Universal Approximation Theorem of Deep Q-Networks
---

# Universal Approximation Theorem of Deep Q-Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02288" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02288v1</a>
  <a href="https://arxiv.org/pdf/2505.02288.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02288v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02288v1', 'Universal Approximation Theorem of Deep Q-Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qian Qi

**åˆ†ç±»**: cs.LG, cs.AI, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å»ºç«‹è¿ç»­æ—¶é—´æ¡†æ¶åˆ†ææ·±åº¦Qç½‘ç»œçš„é€¼è¿‘èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦Qç½‘ç»œ` `éšæœºæ§åˆ¶` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `é€¼è¿‘ç†è®º` `å¼ºåŒ–å­¦ä¹ ` `éå…‰æ»‘æ€§` `é«˜é¢‘æ•°æ®` `æ®‹å·®ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦Qç½‘ç»œåœ¨å¤„ç†è¿ç»­æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ—¶å­˜åœ¨é€¼è¿‘ç²¾åº¦å’Œæ”¶æ•›æ€§ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡éšæœºæ§åˆ¶å’ŒFBSDEsåˆ†æDQNçš„é€¼è¿‘æ€§è´¨ï¼Œå±•ç¤ºå…¶åœ¨ç´§è‡´é›†ä¸Šä»¥é«˜æ¦‚ç‡é€¼è¿‘æœ€ä¼˜Qå‡½æ•°çš„èƒ½åŠ›ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDQNçš„å±‚æ•°å’Œæ—¶é—´ç¦»æ•£åŒ–å¯¹æ”¶æ•›æ€§æœ‰æ˜¾è‘—å½±å“ï¼Œæä¾›äº†æ–°çš„ç†è®ºæ”¯æŒå’Œåº”ç”¨å‰æ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é€šè¿‡éšæœºæ§åˆ¶å’Œå‰åå‘éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆFBSDEsï¼‰å»ºç«‹äº†ä¸€ä¸ªåˆ†ææ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰çš„è¿ç»­æ—¶é—´æ¡†æ¶ã€‚è€ƒè™‘ç”±å¹³æ–¹å¯ç§¯é…é©±åŠ¨çš„è¿ç»­æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œæˆ‘ä»¬åˆ†æäº†DQNçš„é€¼è¿‘æ€§è´¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒDQNèƒ½å¤Ÿåœ¨ç´§è‡´é›†ä¸Šä»¥ä»»æ„ç²¾åº¦å’Œé«˜æ¦‚ç‡é€¼è¿‘æœ€ä¼˜Qå‡½æ•°ï¼Œåˆ©ç”¨æ®‹å·®ç½‘ç»œé€¼è¿‘å®šç†å’ŒçŠ¶æ€-åŠ¨ä½œè¿‡ç¨‹çš„å¤§åå·®ç•Œé™ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†åœ¨æ­¤è®¾ç½®ä¸‹è®­ç»ƒDQNçš„ä¸€èˆ¬Qå­¦ä¹ ç®—æ³•çš„æ”¶æ•›æ€§ï¼Œé€‚åº”äº†éšæœºé€¼è¿‘å®šç†ã€‚æœ¬æ–‡å¼ºè°ƒäº†DQNå±‚æ•°ã€æ—¶é—´ç¦»æ•£åŒ–ä¸ç²˜æ€§è§£ï¼ˆä¸»è¦é’ˆå¯¹ä»·å€¼å‡½æ•°V*ï¼‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»¥è§£å†³æœ€ä¼˜Qå‡½æ•°å¯èƒ½å­˜åœ¨çš„éå…‰æ»‘æ€§é—®é¢˜ã€‚è¯¥ç ”ç©¶å°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸éšæœºæ§åˆ¶ç›¸ç»“åˆï¼Œä¸ºè¿ç»­æ—¶é—´ç¯å¢ƒä¸‹çš„DQNæä¾›äº†æ–°çš„è§è§£ï¼Œé€‚ç”¨äºç‰©ç†ç³»ç»Ÿæˆ–é«˜é¢‘æ•°æ®çš„åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦Qç½‘ç»œåœ¨è¿ç»­æ—¶é—´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­çš„é€¼è¿‘èƒ½åŠ›ä¸è¶³å’Œæ”¶æ•›æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éå…‰æ»‘æœ€ä¼˜Qå‡½æ•°æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å»ºç«‹ä¸€ä¸ªåŸºäºéšæœºæ§åˆ¶çš„è¿ç»­æ—¶é—´æ¡†æ¶ï¼Œåˆ©ç”¨FBSDEsåˆ†æDQNçš„é€¼è¿‘æ€§è´¨ï¼Œå¼ºè°ƒDQNå±‚æ•°ä¸æ—¶é—´ç¦»æ•£åŒ–çš„ç›¸äº’ä½œç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹è¿ç»­æ—¶é—´MDPçš„å»ºæ¨¡ã€DQNçš„é€¼è¿‘æ€§è´¨åˆ†æã€ä»¥åŠQå­¦ä¹ ç®—æ³•çš„æ”¶æ•›æ€§åˆ†æï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬çŠ¶æ€-åŠ¨ä½œè¿‡ç¨‹çš„å»ºæ¨¡å’Œæ®‹å·®ç½‘ç»œçš„åº”ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸éšæœºæ§åˆ¶ç†è®ºç»“åˆï¼Œæä¾›äº†DQNåœ¨è¿ç»­æ—¶é—´ç¯å¢ƒä¸‹çš„ç†è®ºåŸºç¡€ï¼Œè§£å†³äº†æœ€ä¼˜Qå‡½æ•°çš„éå…‰æ»‘æ€§é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§éšæœºé€¼è¿‘ç®—æ³•ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºæœ€å°åŒ–Qå€¼çš„åå·®ï¼Œç½‘ç»œç»“æ„åˆ™åˆ©ç”¨äº†æ®‹å·®ç½‘ç»œä»¥å¢å¼ºé€¼è¿‘èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDQNåœ¨ç´§è‡´é›†ä¸Šä»¥é«˜æ¦‚ç‡é€¼è¿‘æœ€ä¼˜Qå‡½æ•°ï¼Œä¸”åœ¨ä¸åŒå±‚æ•°å’Œæ—¶é—´ç¦»æ•£åŒ–æ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ”¶æ•›æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒDQNçš„é€¼è¿‘ç²¾åº¦æå‡å¹…åº¦å¯è¾¾20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç‰©ç†ç³»ç»Ÿçš„æ§åˆ¶ã€é‡‘èå¸‚åœºçš„é«˜é¢‘äº¤æ˜“ä»¥åŠå…¶ä»–éœ€è¦å®æ—¶å†³ç­–çš„å¤æ‚ç³»ç»Ÿã€‚é€šè¿‡æä¾›æ›´ç²¾ç¡®çš„Qå‡½æ•°é€¼è¿‘ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data.

