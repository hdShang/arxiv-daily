---
layout: default
title: Accelerating RL for LLM Reasoning with Optimal Advantage Regression
---

# Accelerating RL for LLM Reasoning with Optimal Advantage Regression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20686" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20686v1</a>
  <a href="https://arxiv.org/pdf/2505.20686.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20686v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20686v1', 'Accelerating RL for LLM Reasoning with Optimal Advantage Regression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: KiantÃ© Brantley, Mingyu Chen, Zhaolin Gao, Jason D. Lee, Wen Sun, Wenhao Zhan, Xuezhou Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ZhaolinGao/A-PO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºA*-POä»¥è§£å†³RLåœ¨LLMæ¨ç†ä¸­çš„é«˜è®¡ç®—å¼€é”€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ”¿ç­–ä¼˜åŒ–` `æœ€ä¼˜ä¼˜åŠ¿å‡½æ•°` `æœ€å°äºŒä¹˜å›å½’` `KLæ­£åˆ™åŒ–` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ”¿ç­–ä¼˜åŒ–æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ ä¸­é¢ä¸´é«˜è®¡ç®—å¼€é”€å’Œå†…å­˜æ¶ˆè€—çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„A*-POæ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µç­–ç•¥ä¼˜åŒ–ï¼Œç›´æ¥è¿‘ä¼¼æœ€ä¼˜ä¼˜åŠ¿å‡½æ•°ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒA*-POåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘æœ€å¤š2å€ï¼Œå†…å­˜ä½¿ç”¨é™ä½è¶…è¿‡30%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥æå‡å¤æ‚æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆå·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ”¿ç­–ä¼˜åŒ–æ–¹æ³•é€šå¸¸é¢ä¸´é«˜è®¡ç®—å¼€é”€å’Œå†…å­˜æ¶ˆè€—çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºæ¯ä¸ªæç¤ºéœ€è¦å¤šæ¬¡ç”Ÿæˆä»¥åŠä¾èµ–äºå½“å‰æ”¿ç­–çš„è¯„è®ºç½‘ç»œæˆ–ä¼˜åŠ¿ä¼°è®¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ”¿ç­–ä¼˜åŒ–æ¡†æ¶A*-POï¼Œè¯¥æ¡†æ¶ç›´æ¥è¿‘ä¼¼æœ€ä¼˜ä¼˜åŠ¿å‡½æ•°ï¼Œä»è€Œå®ç°LLMsåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„é«˜æ•ˆè®­ç»ƒã€‚é€šè¿‡ç¦»çº¿é‡‡æ ·å‚è€ƒæ”¿ç­–æ¥ä¼°è®¡æœ€ä¼˜ä»·å€¼å‡½æ•°ï¼Œæ¶ˆé™¤äº†æ˜‚è´µçš„åœ¨çº¿ä»·å€¼ä¼°è®¡éœ€æ±‚ï¼Œå¹¶åœ¨ç¬¬äºŒé˜¶æ®µä½¿ç”¨ç®€å•çš„æœ€å°äºŒä¹˜å›å½’æŸå¤±è¿›è¡Œåœ¨çº¿æ›´æ–°ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬å»ºç«‹äº†æ€§èƒ½ä¿è¯ï¼Œå¹¶è¯æ˜KLæ­£åˆ™åŒ–çš„RLç›®æ ‡å¯ä»¥åœ¨ä¸éœ€è¦å¤æ‚æ¢ç´¢ç­–ç•¥çš„æƒ…å†µä¸‹è¿›è¡Œä¼˜åŒ–ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒA*-POåœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´å‡å°‘äº†æœ€å¤š2å€ï¼Œå³°å€¼å†…å­˜ä½¿ç”¨å‡å°‘è¶…è¿‡30%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„é«˜è®¡ç®—å¼€é”€å’Œå†…å­˜æ¶ˆè€—é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤šæ¬¡ç”Ÿæˆå’Œå¤æ‚çš„è¯„è®ºç½‘ç»œï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šA*-POæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä¸¤é˜¶æ®µä¼˜åŒ–ï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨ç¦»çº¿é‡‡æ ·æ¥ä¼°è®¡æœ€ä¼˜ä»·å€¼å‡½æ•°ï¼Œç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡ç®€å•çš„æœ€å°äºŒä¹˜å›å½’è¿›è¡Œåœ¨çº¿æ›´æ–°ï¼Œä»è€Œå‡å°‘è®¡ç®—éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šA*-POçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µè¿›è¡Œç¦»çº¿é‡‡æ ·ä»¥ä¼°è®¡æœ€ä¼˜ä»·å€¼å‡½æ•°V*ï¼Œç¬¬äºŒé˜¶æ®µè¿›è¡ŒåŸºäºå•æ¬¡ç”Ÿæˆçš„åœ¨çº¿æ›´æ–°ï¼Œä½¿ç”¨æœ€å°äºŒä¹˜å›å½’æŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šA*-POçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶ç›´æ¥è¿‘ä¼¼æœ€ä¼˜ä¼˜åŠ¿å‡½æ•°çš„èƒ½åŠ›ï¼Œé¿å…äº†å¤æ‚çš„åœ¨çº¿ä»·å€¼ä¼°è®¡å’Œæ¢ç´¢ç­–ç•¥ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒA*-POé‡‡ç”¨äº†KLæ­£åˆ™åŒ–çš„RLç›®æ ‡ï¼Œç¡®ä¿äº†ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œå¹¶é€šè¿‡ç®€å•çš„æŸå¤±å‡½æ•°è®¾è®¡é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒA*-POåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¾¾åˆ°äº†ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºPPOã€GRPOå’ŒREBELï¼Œè®­ç»ƒæ—¶é—´å‡å°‘æœ€å¤š2å€ï¼Œå³°å€¼å†…å­˜ä½¿ç”¨é™ä½è¶…è¿‡30%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ¨ç†ç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒA*-POèƒ½å¤Ÿåœ¨æ•™è‚²ã€é‡‘èã€åŒ»ç–—ç­‰å¤šä¸ªè¡Œä¸šä¸­æä¾›æ›´é«˜æ•ˆçš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨AIæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$*-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$*, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$*-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at https://github.com/ZhaolinGao/A-PO.

