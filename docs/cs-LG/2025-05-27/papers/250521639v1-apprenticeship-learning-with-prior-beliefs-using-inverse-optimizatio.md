---
layout: default
title: Apprenticeship learning with prior beliefs using inverse optimization
---

# Apprenticeship learning with prior beliefs using inverse optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21639" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21639v1</a>
  <a href="https://arxiv.org/pdf/2505.21639.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21639v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21639v1', 'Apprenticeship learning with prior beliefs using inverse optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mauricio Junca, Esteban Leiva

**åˆ†ç±»**: cs.LG, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€†ä¼˜åŒ–æ¡†æ¶ä»¥å¢å¼ºé€†å¼ºåŒ–å­¦ä¹ çš„å­¦ä¹ èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `é€†ä¼˜åŒ–` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å­¦å¾’å­¦ä¹ ` `æ­£åˆ™åŒ–` `éšæœºé•œåƒä¸‹é™` `æˆæœ¬å‡½æ•°` `ç­–ç•¥å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ—¶å­˜åœ¨ä¸€å®šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æˆæœ¬å‡½æ•°çš„ç»“æ„ä¸æ˜ç¡®æ—¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå…ˆéªŒä¿¡å¿µçš„é€†ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡æ­£åˆ™åŒ–çš„æœ€å°-æœ€å¤§é—®é¢˜æ¥è§£å†³å­¦å¾’å­¦ä¹ ä¸­çš„æ¬¡ä¼˜ä¸“å®¶è®¾ç½®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ­£åˆ™åŒ–åœ¨å­¦ä¹ æˆæœ¬å‘é‡å’Œå­¦å¾’ç­–ç•¥ä¸­èµ·åˆ°äº†è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰å’Œé€†ä¼˜åŒ–ï¼ˆIOï¼‰åœ¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­è§£å†³ç›¸åŒé—®é¢˜ï¼Œä½†æ–‡çŒ®ä¸­å¯¹æ­¤å…³ç³»çš„æ¢è®¨ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†IOæ¡†æ¶ã€IRLå’Œå­¦å¾’å­¦ä¹ ï¼ˆALï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œç»“åˆäº†å¯¹æˆæœ¬å‡½æ•°ç»“æ„çš„å…ˆéªŒä¿¡å¿µï¼Œå±•ç¤ºäº†ALå½¢å¼çš„å‡¸åˆ†æè§†è§’ä½œä¸ºæˆ‘ä»¬æ¡†æ¶çš„æ”¾æ¾ã€‚ç‰¹åˆ«åœ°ï¼Œå½“æ­£åˆ™åŒ–é¡¹ç¼ºå¤±æ—¶ï¼ŒALå½¢å¼æ˜¯æˆ‘ä»¬æ¡†æ¶çš„ç‰¹ä¾‹ã€‚é’ˆå¯¹æ¬¡ä¼˜ä¸“å®¶è®¾ç½®ï¼Œæˆ‘ä»¬å°†ALé—®é¢˜è¡¨è¿°ä¸ºä¸€ä¸ªæ­£åˆ™åŒ–çš„æœ€å°-æœ€å¤§é—®é¢˜ï¼Œæ­£åˆ™åŒ–é¡¹åœ¨å¼•å¯¼å¯è¡Œæˆæœ¬å‡½æ•°çš„æœç´¢ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚ä¸ºäº†è§£å†³å¾—åˆ°çš„æ­£åˆ™åŒ–å‡¸-å‡¹-æœ€å°-æœ€å¤§é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨éšæœºé•œåƒä¸‹é™ï¼ˆSMDï¼‰å¹¶å»ºç«‹äº†æ”¶æ•›ç•Œé™ã€‚æ•°å€¼å®éªŒçªæ˜¾äº†æ­£åˆ™åŒ–åœ¨å­¦ä¹ æˆæœ¬å‘é‡å’Œå­¦å¾’ç­–ç•¥ä¸­çš„é‡è¦ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰ä¸é€†ä¼˜åŒ–ï¼ˆIOï¼‰åœ¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­çš„å…³ç³»æœªè¢«å……åˆ†æ¢è®¨çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æˆæœ¬å‡½æ•°ç»“æ„ä¸æ˜ç¡®æ—¶ï¼Œå¾€å¾€å¯¼è‡´å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºå°†å…ˆéªŒä¿¡å¿µèå…¥IRLå’Œå­¦å¾’å­¦ä¹ ï¼ˆALï¼‰é—®é¢˜ä¸­ï¼Œé€šè¿‡æ­£åˆ™åŒ–çš„æœ€å°-æœ€å¤§æ¡†æ¶æ¥å¼•å¯¼æœç´¢å¯è¡Œçš„æˆæœ¬å‡½æ•°ï¼Œä»è€Œæ”¹å–„å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å°†ALé—®é¢˜è½¬åŒ–ä¸ºæ­£åˆ™åŒ–çš„æœ€å°-æœ€å¤§é—®é¢˜ï¼Œåˆ©ç”¨éšæœºé•œåƒä¸‹é™ï¼ˆSMDï¼‰ç®—æ³•è¿›è¡Œæ±‚è§£ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ­£åˆ™åŒ–é¡¹çš„è®¾è®¡ã€æˆæœ¬å‡½æ•°çš„æœç´¢å’Œç­–ç•¥çš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ­£åˆ™åŒ–å¼•å…¥ALæ¡†æ¶ï¼Œè§£å†³äº†IRLä¸­çš„ä¸é€‚å®šæ€§é—®é¢˜ï¼Œå¹¶ä¸”åœ¨æ²¡æœ‰æ­£åˆ™åŒ–é¡¹æ—¶ï¼ŒALå½¢å¼æˆä¸ºæˆ‘ä»¬æ¡†æ¶çš„ç‰¹ä¾‹ã€‚

**å…³é”®è®¾è®¡**ï¼šæ­£åˆ™åŒ–é¡¹çš„è®¾è®¡æ˜¯å…³é”®ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æœç´¢è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ï¼ˆSMDï¼‰ä¹Ÿä¸ºå®ç°æ”¶æ•›æä¾›äº†ç†è®ºæ”¯æŒã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ­£åˆ™åŒ–çš„å­¦ä¹ æ–¹æ³•åœ¨æˆæœ¬å‘é‡å’Œå­¦å¾’ç­–ç•¥çš„å­¦ä¹ ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ­£åˆ™åŒ–åœ¨è§£å†³IRLé—®é¢˜ä¸­çš„å…³é”®ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡å¼•å…¥å…ˆéªŒä¿¡å¿µå’Œæ­£åˆ™åŒ–æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°å­¦ä¹ ç­–ç•¥ï¼Œæå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å¤šç§å®é™…åœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The relationship between inverse reinforcement learning (IRL) and inverse optimization (IO) for Markov decision processes (MDPs) has been relatively underexplored in the literature, despite addressing the same problem. In this work, we revisit the relationship between the IO framework for MDPs, IRL, and apprenticeship learning (AL). We incorporate prior beliefs on the structure of the cost function into the IRL and AL problems, and demonstrate that the convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a relaxation of our framework. Notably, the AL formalism is a special case in our framework when the regularization term is absent. Focusing on the suboptimal expert setting, we formulate the AL problem as a regularized min-max problem. The regularizer plays a key role in addressing the ill-posedness of IRL by guiding the search for plausible cost functions. To solve the resulting regularized-convex-concave-min-max problem, we use stochastic mirror descent (SMD) and establish convergence bounds for the proposed method. Numerical experiments highlight the critical role of regularization in learning cost vectors and apprentice policies.

