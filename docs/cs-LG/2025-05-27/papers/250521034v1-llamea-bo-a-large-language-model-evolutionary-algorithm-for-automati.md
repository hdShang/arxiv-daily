---
layout: default
title: LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms
---

# LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21034" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21034v1</a>
  <a href="https://arxiv.org/pdf/2505.21034.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21034v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21034v1', 'LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Wenhu Li, Niki van Stein, Thomas B√§ck, Elena Raponi

**ÂàÜÁ±ª**: cs.LG, cs.NE

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Ewendawi/LLaMEA-BO)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LLaMEA-BO‰ª•Ëá™Âä®ÁîüÊàêË¥ùÂè∂ÊñØ‰ºòÂåñÁÆóÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ë¥ùÂè∂ÊñØ‰ºòÂåñ` `Ëá™Âä®ÂåñËÆæËÆ°` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËøõÂåñÁ≠ñÁï•` `ÁÆóÊ≥ïÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑË¥ùÂè∂ÊñØ‰ºòÂåñÁÆóÊ≥ïËÆæËÆ°‰æùËµñ‰∫é‰∏ìÂÆ∂Áü•ËØÜÔºåËøáÁ®ãÁπÅÁêê‰∏îÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËá™Âä®ÁîüÊàêË¥ùÂè∂ÊñØ‰ºòÂåñÁÆóÊ≥ï‰ª£Á†ÅÔºåÁªìÂêàËøõÂåñÁ≠ñÁï•ËøõË°åËø≠‰ª£‰ºòÂåñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÁîüÊàêÁöÑÁÆóÊ≥ïÂú®19‰∏™BBOBÂü∫ÂáÜÂáΩÊï∞‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÊúÄ‰ºòÁÆóÊ≥ïÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ë¥ùÂè∂ÊñØ‰ºòÂåñÔºàBOÔºâÊòØ‰∏ÄÁ±ªÂº∫Â§ßÁöÑÁÆóÊ≥ïÔºåÁî®‰∫é‰ºòÂåñÊòÇË¥µÁöÑÈªëÁÆ±ÂáΩÊï∞Ôºå‰ΩÜËÆæËÆ°ÊúâÊïàÁöÑBOÁÆóÊ≥ï‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™‰æùËµñ‰∏ì‰∏öÁü•ËØÜÁöÑÊâãÂä®‰ªªÂä°„ÄÇÊúÄËøëÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËøõÂ±ï‰∏∫Ëá™Âä®ÂåñÁßëÂ≠¶ÂèëÁé∞ÂºÄËæü‰∫ÜÊñ∞ÈÄîÂæÑÔºåÂåÖÊã¨‰ºòÂåñÁÆóÊ≥ïÁöÑËá™Âä®ËÆæËÆ°„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÂà©Áî®LLMsËá™Âä®ÁîüÊàêÂÆåÊï¥ÁöÑBOÁÆóÊ≥ï‰ª£Á†Å„ÄÇËØ•Ê°ÜÊû∂‰ΩøÁî®ËøõÂåñÁ≠ñÁï•ÂºïÂØºLLMÁîüÊàê‰øùÁïôBOÁÆóÊ≥ïÂÖ≥ÈîÆÁªÑ‰ª∂ÁöÑPython‰ª£Á†Å„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁîüÊàêÁöÑÁÆóÊ≥ïÂú®19‰∏™BBOBÂü∫ÂáÜÂáΩÊï∞‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑBOÂü∫Á∫øÔºåÂπ∂Âú®Êõ¥È´òÁª¥Â∫¶Âíå‰∏çÂêå‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÂ±ïÁ§∫‰∫ÜLLMs‰Ωú‰∏∫ÁÆóÊ≥ïÂÖ±ÂêåËÆæËÆ°ËÄÖÁöÑÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ë¥ùÂè∂ÊñØ‰ºòÂåñÁÆóÊ≥ïËÆæËÆ°ÁöÑÊâãÂä®Âíå‰∏ìÂÆ∂‰æùËµñÊÄßÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÊïàÁéáÂíåËá™Âä®ÂåñÁ®ãÂ∫¶‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÁªìÂêàÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåËøõÂåñÁ≠ñÁï•ÔºåËá™Âä®ÁîüÊàêÁ¨¶ÂêàË¥ùÂè∂ÊñØ‰ºòÂåñÂÖ≥ÈîÆÁªÑ‰ª∂ÁöÑÁÆóÊ≥ï‰ª£Á†ÅÔºå‰ªéËÄåÂÆûÁé∞ÁÆóÊ≥ïÁöÑËá™Âä®ÂåñËÆæËÆ°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂàùÂßãËÆæËÆ°ÁîüÊàê„ÄÅ‰ª£ÁêÜÊ®°ÂûãÊûÑÂª∫ÂíåËé∑ÂèñÂáΩÊï∞ÂÆö‰πâ„ÄÇLLMÁîüÊàêÂ§ö‰∏™ÂÄôÈÄâÁÆóÊ≥ïÔºåÂπ∂ÈÄöËøáBBOBÂü∫ÂáÜÊµãËØïËØÑ‰º∞ÂÖ∂ÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜLLM‰∏éËøõÂåñÁ≠ñÁï•ÁªìÂêàÔºåÂΩ¢Êàê‰∏ÄÁßçÊñ∞ÁöÑÁÆóÊ≥ïÁîüÊàêÊú∫Âà∂ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁÆóÊ≥ïËÆæËÆ°ÁöÑËá™Âä®ÂåñÊ∞¥Âπ≥„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁîüÊàêËøáÁ®ã‰∏≠ÔºåËÆæÁΩÆ‰∫ÜÂ§öÁßçÊèêÁ§∫Âèò‰Ωì‰ª•ÂºïÂØºLLMÁîüÊàê‰∏çÂêåÂÄôÈÄâÁÆóÊ≥ïÔºåÈááÁî®ÊéßÂà∂ÂèòÂºÇÁöÑÊñπÊ≥ïËøõË°åËø≠‰ª£‰ºòÂåñÔºåÁ°Æ‰øùÁîüÊàêÁÆóÊ≥ïÁöÑÂ§öÊ†∑ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLLaMEA-BOÁîüÊàêÁöÑÁÆóÊ≥ïÂú®19‰∏™BBOBÂü∫ÂáÜÂáΩÊï∞‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄ‰ºòË¥ùÂè∂ÊñØ‰ºòÂåñÂü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®5Áª¥ÊÉÖÂÜµ‰∏ãË°®Áé∞Á™ÅÂá∫Ôºå‰∏îÂú®Êõ¥È´òÁª¥Â∫¶Âíå‰∏çÂêå‰ªªÂä°‰∏≠‰πüÂ±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂàõÊñ∞ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™Âä®ÂåñÁÆóÊ≥ïËÆæËÆ°„ÄÅÊú∫Âô®Â≠¶‰π†‰ºòÂåñÂíåÁßëÂ≠¶ËÆ°ÁÆóÁ≠â„ÄÇÈÄöËøáËá™Âä®ÁîüÊàêÈ´òÊïàÁöÑË¥ùÂè∂ÊñØ‰ºòÂåñÁÆóÊ≥ïÔºåÂèØ‰ª•Âä†ÈÄüÁßëÂ≠¶ÂèëÁé∞ÂíåÂ∑•Á®ãÂ∫îÁî®ÔºåÈôç‰ΩéÂØπ‰∏ìÂÆ∂Áü•ËØÜÁöÑ‰æùËµñÔºåÊèêÂçáÁ†îÁ©∂ÊïàÁéá„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊâ©Â±ïÂà∞ÂÖ∂‰ªñÁ±ªÂûãÁöÑ‰ºòÂåñÈóÆÈ¢òÂíåÁÆóÊ≥ïËÆæËÆ°È¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Bayesian optimization (BO) is a powerful class of algorithms for optimizing expensive black-box functions, but designing effective BO algorithms remains a manual, expertise-driven task. Recent advancements in Large Language Models (LLMs) have opened new avenues for automating scientific discovery, including the automatic design of optimization algorithms. While prior work has used LLMs within optimization loops or to generate non-BO algorithms, we tackle a new challenge: Using LLMs to automatically generate full BO algorithm code. Our framework uses an evolution strategy to guide an LLM in generating Python code that preserves the key components of BO algorithms: An initial design, a surrogate model, and an acquisition function. The LLM is prompted to produce multiple candidate algorithms, which are evaluated on the established Black-Box Optimization Benchmarking (BBOB) test suite from the COmparing Continuous Optimizers (COCO) platform. Based on their performance, top candidates are selected, combined, and mutated via controlled prompt variations, enabling iterative refinement. Despite no additional fine-tuning, the LLM-generated algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB functions in dimension 5 and generalize well to higher dimensions, and different tasks (from the Bayesmark framework). This work demonstrates that LLMs can serve as algorithmic co-designers, offering a new paradigm for automating BO development and accelerating the discovery of novel algorithmic combinations. The source code is provided at https://github.com/Ewendawi/LLaMEA-BO.

