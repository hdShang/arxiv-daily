---
layout: default
title: Efficient Large Language Model Inference with Neural Block Linearization
---

# Efficient Large Language Model Inference with Neural Block Linearization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21077" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21077v2</a>
  <a href="https://arxiv.org/pdf/2505.21077.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21077v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21077v2', 'Efficient Large Language Model Inference with Neural Block Linearization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mete Erdogan, Francesco Tonin, Volkan Cevher

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-10-19)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/LIONS-EPFL/NBL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¥ç»å—çº¿æ€§åŒ–ä»¥åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†åŠ é€Ÿ` `ç¥ç»ç½‘ç»œ` `çº¿æ€§è¿‘ä¼¼` `è®¡ç®—æ•ˆç‡` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `å…¸å‹ç›¸å…³åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å˜å‹å™¨åŸºç¡€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶é¢ä¸´é«˜è®¡ç®—éœ€æ±‚ï¼Œå¯¼è‡´éƒ¨ç½²å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºçš„ç¥ç»å—çº¿æ€§åŒ–ï¼ˆNBLï¼‰æ¡†æ¶é€šè¿‡çº¿æ€§è¿‘ä¼¼æ›¿ä»£è‡ªæ³¨æ„åŠ›å±‚ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNBLåœ¨DeepSeek-R1-Distill-Llama-8Bæ¨¡å‹ä¸­å®ç°äº†32%çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œå‡†ç¡®æ€§æŸå¤±ä¸è¶³1%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å˜å‹å™¨åŸºç¡€çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶é¢ä¸´é«˜éœ€æ±‚ï¼Œç»™å…¶éƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶â€”â€”ç¥ç»å—çº¿æ€§åŒ–ï¼ˆNBLï¼‰ï¼Œé€šè¿‡ç”¨çº¿æ€§è¿‘ä¼¼æ›¿ä»£è‡ªæ³¨æ„åŠ›å±‚æ¥åŠ é€Ÿå˜å‹å™¨æ¨¡å‹æ¨ç†ã€‚NBLåˆ©ç”¨å…¸å‹ç›¸å…³åˆ†æè®¡ç®—è¿‘ä¼¼è¯¯å·®çš„ç†è®ºä¸Šé™ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºæ›¿ä»£æ ‡å‡†ï¼Œé€‰æ‹©çº¿æ€§åŒ–è¯¯å·®æœ€ä½çš„LLMå±‚ã€‚NBLå¯ä»¥é«˜æ•ˆåº”ç”¨äºé¢„è®­ç»ƒçš„LLMsï¼Œæ— éœ€å¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼ŒNBLåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šå®ç°äº†æ˜¾è‘—çš„è®¡ç®—åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å˜å‹å™¨åŸºç¡€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶çš„é«˜è®¡ç®—éœ€æ±‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¥ç»å—çº¿æ€§åŒ–ï¼ˆNBLï¼‰æ¡†æ¶ï¼Œç”¨çº¿æ€§è¿‘ä¼¼æ›¿ä»£è‡ªæ³¨æ„åŠ›å±‚ï¼Œä»è€Œå‡å°‘è®¡ç®—å¤æ‚åº¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨çº¿æ€§æœ€å°å‡æ–¹è¯¯å·®ä¼°è®¡å™¨çš„è¿‘ä¼¼ï¼Œæ—¨åœ¨åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æé«˜æ¨ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNBLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è®¡ç®—è¿‘ä¼¼è¯¯å·®çš„ç†è®ºä¸Šé™ã€é€‰æ‹©ä½çº¿æ€§åŒ–è¯¯å·®çš„LLMå±‚ï¼Œå¹¶åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹åº”ç”¨äºé¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬è¯¯å·®è®¡ç®—æ¨¡å—å’Œå±‚é€‰æ‹©æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šNBLçš„ä¸»è¦åˆ›æ–°åœ¨äºåˆ©ç”¨å…¸å‹ç›¸å…³åˆ†ææ¥è®¡ç®—è¿‘ä¼¼è¯¯å·®çš„ç†è®ºä¸Šé™ï¼Œå¹¶ä»¥æ­¤ä¸ºä¾æ®é€‰æ‹©æœ€ä¼˜çš„æ¨¡å‹å±‚ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—ä¸åŒäºä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æ¨ç†åŠ é€Ÿç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒNBLå…³æ³¨äºé€‰æ‹©çº¿æ€§åŒ–è¯¯å·®æœ€ä½çš„å±‚ï¼Œå¹¶é€šè¿‡ç†è®ºä¸Šé™æ¥æŒ‡å¯¼è¿™ä¸€é€‰æ‹©è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•çš„å®ç°ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿äº†åº”ç”¨çš„çµæ´»æ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNBLåœ¨DeepSeek-R1-Distill-Llama-8Bæ¨¡å‹ä¸­æˆåŠŸå°†æ¨ç†é€Ÿåº¦æé«˜äº†32%ï¼Œè€Œå‡†ç¡®æ€§æŸå¤±ä¸è¶³1%ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡è¡¨æ˜NBLåœ¨æ¨ç†æ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå…·æœ‰è¾ƒå¼ºçš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒNBLå¯ä»¥å¸®åŠ©è¿™äº›ç³»ç»Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ›´å¥½åœ°è¿è¡Œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs. The implementation is available at: https://github.com/LIONS-EPFL/NBL.

