---
layout: default
title: Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation
---

# Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20745" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20745v2</a>
  <a href="https://arxiv.org/pdf/2505.20745.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20745v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20745v2', 'Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingping Nie, Dung T. Tran, Karan Thakkar, Vasudha Kowtha, Jon Huang, Carlos Avendano, Erdrin Azemi, Vikramjit Mitra

**åˆ†ç±»**: cs.SD, cs.LG, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-05-29)

**å¤‡æ³¨**: 5 pages, Interspeech 2025 conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨åŸºç¡€æ¨¡å‹éšå«è¡¨ç¤ºè¿›è¡Œå¿ƒç‡ä¼°è®¡ï¼Œæå‡å¬è¯ŠæŠ€æœ¯çš„å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¿ƒç‡ä¼°è®¡` `å¬è¯ŠæŠ€æœ¯` `åŸºç¡€æ¨¡å‹` `å£°å­¦è¡¨ç¤º` `è‡ªç›‘ç£å­¦ä¹ ` `åŒ»ç–—ç›‘æµ‹` `ç”Ÿç‰©ä¿¡å·å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¯¹å¿ƒéŸ³æ•°æ®çš„åˆ©ç”¨ä¸è¶³ï¼Œæœªèƒ½å……åˆ†æŒ–æ˜é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸­çš„ä¿¡æ¯ã€‚
2. è®ºæ–‡é€šè¿‡é€å±‚åˆ†æå…­ç§å£°å­¦è¡¨ç¤ºæ¨¡å‹ï¼Œæ¢ç´¢å…¶åœ¨å¿ƒç‡ä¼°è®¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè‡ªå®¶CLAPæ¨¡å‹çš„éŸ³é¢‘ç¼–ç å™¨åœ¨å¿ƒç‡ä¼°è®¡ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œé™ä½äº†MAEã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¬è¯Šï¼Œå°¤å…¶æ˜¯å¿ƒéŸ³ï¼Œæ˜¯ä¸€ç§éä¾µå…¥æ€§çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿæä¾›é‡è¦çš„ç”Ÿå‘½ä½“å¾ä¿¡æ¯ã€‚è¿‘å¹´æ¥ï¼Œè‡ªç›‘ç£å£°å­¦è¡¨ç¤ºåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰è¢«æå‡ºä»¥æä¾›åŸºäºå£°å­¦çš„ç”Ÿå‘½ä½“å¾çš„è§è§£ã€‚ç„¶è€Œï¼Œå…³äºå¬è¯Šåœ¨è¿™äº›é¢„è®­ç»ƒFMè¡¨ç¤ºä¸­çš„ç¼–ç ç¨‹åº¦çš„æ¢ç´¢ä»ç„¶è¾ƒå°‘ã€‚æœ¬æ–‡ä½¿ç”¨å…¬å¼€çš„å¿ƒéŸ³å›¾ï¼ˆPCGï¼‰æ•°æ®é›†å’Œå¿ƒç‡ï¼ˆHRï¼‰ä¼°è®¡æ¨¡å‹ï¼Œå¯¹å…­ç§å£°å­¦è¡¨ç¤ºFMsè¿›è¡Œäº†é€å±‚ç ”ç©¶ï¼Œå¹¶å®ç°äº†åŸºäºå£°å­¦ç‰¹å¾çš„åŸºçº¿æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºå‘é‡åœ¨æ€§èƒ½ä¸Šä¸åŸºçº¿ç›¸å½“ï¼Œä¸”è‡ªå®¶CLAPæ¨¡å‹çš„éŸ³é¢‘ç¼–ç å™¨åœ¨å¿ƒç‡ä¼°è®¡ä¸­è¡¨ç°ä¼˜äºåŸºçº¿ï¼Œå°½ç®¡å­˜åœ¨é¢†åŸŸä¸åŒ¹é…ï¼Œä»å®ç°äº†æ›´ä½çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¯¹å¿ƒéŸ³æ•°æ®è¿›è¡Œå¿ƒç‡ä¼°è®¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨å£°å­¦ç‰¹å¾æ—¶ï¼Œæœªèƒ½å……åˆ†æŒ–æ˜æ¨¡å‹ä¸­çš„æ½œåœ¨ä¿¡æ¯ï¼Œå¯¼è‡´ä¼°è®¡ç²¾åº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡é€å±‚åˆ†æå…­ç§å£°å­¦è¡¨ç¤ºåŸºç¡€æ¨¡å‹ï¼Œè¯„ä¼°å…¶åœ¨å¿ƒç‡ä¼°è®¡ä¸­çš„è¡¨ç°ï¼Œæ¢ç´¢ä¸åŒæ¨¡å‹çš„éšå«è¡¨ç¤ºå¦‚ä½•å½±å“æœ€ç»ˆç»“æœã€‚è®¾è®¡ä¸Šï¼Œé‡ç‚¹åœ¨äºæ¯”è¾ƒä¸åŒæ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›åŠå…¶å¯¹å¿ƒç‡ä¼°è®¡çš„è´¡çŒ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å…¬å¼€çš„å¿ƒéŸ³å›¾æ•°æ®é›†ï¼Œç»“åˆå¿ƒç‡ä¼°è®¡æ¨¡å‹ï¼Œé€å±‚åˆ†æå…­ç§å£°å­¦è¡¨ç¤ºæ¨¡å‹ï¼ŒåŒ…æ‹¬HuBERTã€wav2vec2ã€wavLMã€Whisperã€CLAPåŠè‡ªå®¶CLAPæ¨¡å‹ã€‚æ¯ç§æ¨¡å‹çš„è¡¨ç¤ºå‘é‡è¢«ç”¨äºå¿ƒç‡ä¼°è®¡ï¼Œå¹¶ä¸åŸºçº¿æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå¯¹é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„é€å±‚åˆ†æï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å¿ƒéŸ³æ•°æ®ä¸­çš„ç¼–ç èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯è‡ªå®¶CLAPæ¨¡å‹åœ¨å¿ƒç‡ä¼°è®¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è§£å’Œåº”ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ†å‰²æ–¹å¼ï¼Œè®¾ç½®äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¿ƒç‡ä¼°è®¡ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè‡ªå®¶CLAPæ¨¡å‹çš„éŸ³é¢‘ç¼–ç å™¨åœ¨å¿ƒç‡ä¼°è®¡ä¸­å®ç°äº†æ¯”åŸºçº¿æ–¹æ³•æ›´ä½çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œå°½ç®¡å­˜åœ¨é¢†åŸŸä¸åŒ¹é…ï¼Œä»å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€å‘ç°ä¸ºå£°å­¦è¡¨ç¤ºæ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ç›‘æµ‹ã€è¿œç¨‹å¥åº·ç®¡ç†å’Œæ™ºèƒ½ç©¿æˆ´è®¾å¤‡ç­‰ã€‚é€šè¿‡æé«˜å¿ƒç‡ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºä¸´åºŠè¯Šæ–­æä¾›æ›´å¯é çš„æ•°æ®æ”¯æŒï¼Œä¿ƒè¿›ä¸ªæ€§åŒ–åŒ»ç–—çš„å‘å±•ã€‚æœªæ¥ï¼Œç ”ç©¶æˆæœæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„ç”Ÿç‰©ä¿¡å·ç›‘æµ‹ä¸­å¾—åˆ°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Auscultation, particularly heart sound, is a non-invasive technique that provides essential vital sign information. Recently, self-supervised acoustic representation foundation models (FMs) have been proposed to offer insights into acoustics-based vital signs. However, there has been little exploration of the extent to which auscultation is encoded in these pre-trained FM representations. In this work, using a publicly available phonocardiogram (PCG) dataset and a heart rate (HR) estimation model, we conduct a layer-wise investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM, Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP model. Additionally, we implement the baseline method from Nie et al., 2024 (which relies on acoustic features) and show that overall, representation vectors from pre-trained foundation models (FMs) offer comparable performance to the baseline. Notably, HR estimation using the representations from the audio encoder of the in-house CLAP model outperforms the results obtained from the baseline, achieving a lower mean absolute error (MAE) across various train/validation/test splits despite the domain mismatch.

