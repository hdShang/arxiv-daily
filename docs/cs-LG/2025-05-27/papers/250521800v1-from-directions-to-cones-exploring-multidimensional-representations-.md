---
layout: default
title: "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs"
---

# From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21800" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21800v1</a>
  <a href="https://arxiv.org/pdf/2505.21800.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21800v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21800v1', 'From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Stanley Yu, Vaidehi Bulusu, Oscar Yasunaga, Clayton Lau, Cole Blondin, Sean O'Brien, Kevin Zhu, Vasu Sharma

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šç»´é”¥ä½“æ¡†æ¶ä»¥æ¢è®¨LLMsä¸­çš„çœŸç†è¡¨ç¤ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `çœŸç†è¡¨ç¤º` `å¤šç»´é”¥ä½“` `å› æœå¹²é¢„` `æ¨¡å‹æ³›åŒ–` `æŠ½è±¡è¡Œä¸ºæ¢æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€çº¿æ€§æ–¹å‘æ¥è¡¨ç¤ºå‘½é¢˜çš„çœŸå®æ€§ï¼Œæ— æ³•å……åˆ†æ•æ‰å…¶å¤æ‚çš„å‡ ä½•ç‰¹å¾ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†å¤šç»´é”¥ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°è¡¨ç¤ºå’Œç†è§£LLMsä¸­çš„çœŸç†ç›¸å…³è¡Œä¸ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå› æœå¹²é¢„å¯ä»¥æœ‰æ•ˆæ”¹å˜æ¨¡å‹å“åº”ï¼Œä¸”å­¦ä¹ åˆ°çš„é”¥ä½“åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„å¯¹è¯èƒ½åŠ›ï¼Œä½†å¸¸å¸¸ç”Ÿæˆè™šå‡ä¿¡æ¯ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç®€å•å‘½é¢˜çš„çœŸå®æ€§å¯ä»¥ç”¨æ¨¡å‹å†…éƒ¨æ¿€æ´»çš„å•ä¸€çº¿æ€§æ–¹å‘è¡¨ç¤ºï¼Œä½†è¿™å¯èƒ½æ— æ³•å®Œå…¨æ•æ‰å…¶åº•å±‚å‡ ä½•ç»“æ„ã€‚æœ¬ç ”ç©¶æ‰©å±•äº†æœ€è¿‘ä¸ºå»ºæ¨¡æ‹’ç»è€Œå¼•å…¥çš„é”¥ä½“æ¦‚å¿µæ¡†æ¶ï¼Œåº”ç”¨äºçœŸç†é¢†åŸŸã€‚æˆ‘ä»¬è¯†åˆ«å‡ºå¤šç»´é”¥ä½“ï¼Œè¿™äº›é”¥ä½“åœ¨å¤šä¸ªLLMå®¶æ—ä¸­å› æœä¸­ä»‹çœŸç†ç›¸å…³è¡Œä¸ºã€‚æˆ‘ä»¬çš„ç»“æœé€šè¿‡ä¸‰æ¡è¯æ®æ”¯æŒï¼šå› æœå¹²é¢„å¯é åœ°ç¿»è½¬æ¨¡å‹å¯¹äº‹å®é™ˆè¿°çš„å“åº”ï¼Œå­¦ä¹ åˆ°çš„é”¥ä½“åœ¨æ¨¡å‹æ¶æ„é—´å…·æœ‰æ³›åŒ–èƒ½åŠ›ï¼Œé”¥ä½“å¹²é¢„ä¿ç•™äº†æ— å…³çš„æ¨¡å‹è¡Œä¸ºã€‚è¿™äº›å‘ç°æ­ç¤ºäº†åœ¨LLMsä¸­æ”¯é…ç®€å•çœŸ/å‡å‘½é¢˜çš„æ›´ä¸°å¯Œçš„å¤šæ–¹å‘ç»“æ„ï¼Œå¹¶å¼ºè°ƒäº†é”¥ä½“æ¦‚å¿µä½œä¸ºæ¢æµ‹æŠ½è±¡è¡Œä¸ºçš„æœ‰å‰æ™¯å·¥å…·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çœŸç†è¡¨ç¤ºçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä»…ç”¨å•ä¸€çº¿æ€§æ–¹å‘æ¥è¡¨ç¤ºå‘½é¢˜çš„çœŸå®æ€§ï¼Œæœªèƒ½åæ˜ å…¶å¤æ‚çš„å¤šç»´å‡ ä½•ç‰¹å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†å¤šç»´é”¥ä½“çš„æ¦‚å¿µï¼Œè®¤ä¸ºè¿™ç§ç»“æ„èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å’Œä¸­ä»‹çœŸç†ç›¸å…³çš„è¡Œä¸ºï¼Œè¿›è€Œæå‡æ¨¡å‹çš„å“åº”å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡å› æœå¹²é¢„æŠ€æœ¯è¯†åˆ«æ¨¡å‹çš„å“åº”å˜åŒ–ï¼›å…¶æ¬¡ï¼Œå­¦ä¹ å¤šç»´é”¥ä½“ä»¥è¡¨ç¤ºä¸åŒçš„çœŸç†çŠ¶æ€ï¼›æœ€åï¼ŒéªŒè¯é”¥ä½“å¹²é¢„å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥å¤šç»´é”¥ä½“æ¦‚å¿µï¼Œçªç ´äº†ä¼ ç»Ÿå•çº¿æ€§æ–¹å‘çš„é™åˆ¶ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æè¿°å’Œç†è§£æ¨¡å‹çš„çœŸç†ç›¸å…³è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å› æœå¹²é¢„ç­–ç•¥ï¼Œè®¾è®¡äº†é€‚åº”ä¸åŒæ¨¡å‹æ¶æ„çš„é”¥ä½“å­¦ä¹ ç®—æ³•ï¼Œå¹¶ç¡®ä¿å¹²é¢„è¿‡ç¨‹ä¸ä¼šå½±å“æ¨¡å‹çš„å…¶ä»–æ— å…³è¡Œä¸ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡å› æœå¹²é¢„ï¼Œæ¨¡å‹å¯¹äº‹å®é™ˆè¿°çš„å“åº”èƒ½å¤Ÿå¯é åœ°ç¿»è½¬ï¼Œä¸”å­¦ä¹ åˆ°çš„å¤šç»´é”¥ä½“åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•åœ¨ä¿ç•™æ— å…³æ¨¡å‹è¡Œä¸ºçš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å¯¹çœŸç†ç›¸å…³è¡Œä¸ºçš„ç†è§£å’Œæ§åˆ¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°ç†è§£å’Œè¡¨ç¤ºçœŸç†ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®å†…å®¹æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿›è€Œæé«˜ç”¨æˆ·ä½“éªŒå’Œä¿¡æ¯çš„å¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å¤šç§è¯­è¨€æ¨¡å‹çš„å¼€å‘å’Œä¼˜åŒ–ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) exhibit strong conversational abilities but often generate falsehoods. Prior work suggests that the truthfulness of simple propositions can be represented as a single linear direction in a model's internal activations, but this may not fully capture its underlying geometry. In this work, we extend the concept cone framework, recently introduced for modeling refusal, to the domain of truth. We identify multi-dimensional cones that causally mediate truth-related behavior across multiple LLM families. Our results are supported by three lines of evidence: (i) causal interventions reliably flip model responses to factual statements, (ii) learned cones generalize across model architectures, and (iii) cone-based interventions preserve unrelated model behavior. These findings reveal the richer, multidirectional structure governing simple true/false propositions in LLMs and highlight concept cones as a promising tool for probing abstract behaviors.

