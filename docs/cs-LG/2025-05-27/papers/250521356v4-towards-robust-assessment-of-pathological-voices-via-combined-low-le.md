---
layout: default
title: Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations
---

# Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21356" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21356v4</a>
  <a href="https://arxiv.org/pdf/2505.21356.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21356v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21356v4', 'Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao

**ÂàÜÁ±ª**: cs.SD, cs.LG, eess.AS

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27 (Êõ¥Êñ∞: 2025-12-11)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VOQANetÂèäÂÖ∂Â¢ûÂº∫Áâà‰ª•Ëß£ÂÜ≥ÁóÖÁêÜÂ£∞Èü≥ËØÑ‰º∞ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â£∞Èü≥Ë¥®ÈáèËØÑ‰º∞` `Ê∑±Â∫¶Â≠¶‰π†` `ËØ≠Èü≥Âü∫Á°ÄÊ®°Âûã` `‰ΩéÂ±ÇÂ£∞Â≠¶ÁâπÂæÅ` `È≤ÅÊ£íÊÄß` `ÂåªÁñóËØäÊñ≠` `Ëá™ÁõëÁù£Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ£∞Èü≥Ë¥®ÈáèËØÑ‰º∞ÊñπÊ≥ï‰æùËµñ‰∫é‰∏ìÂÆ∂ËØÑ‰º∞ÔºåÂ≠òÂú®ËØÑ‰º∞ËÄÖÈó¥ÁöÑÂèòÂºÇÊÄßÔºåÁº∫‰πèÂÆ¢ËßÇÊÄß„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜVOQANetÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåSFMÂµåÂÖ•ÔºåÊèêÂèñÈ´òÂ±ÇÁâπÂæÅ‰ª•ÊèêÈ´òËØÑ‰º∞ÂáÜÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVOQANetÂú®CAPE-VÂíåGRBASÁª¥Â∫¶‰∏ä‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãÔºåVOQANet+Âú®Âô™Â£∞Êù°‰ª∂‰∏ãË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊÑüÁü•Â£∞Èü≥Ë¥®ÈáèËØÑ‰º∞Âú®ËØäÊñ≠ÂíåÁõëÊµãÂ£∞Èü≥ÈöúÁ¢ç‰∏≠Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰º†ÁªüÊñπÊ≥ïÂ¶ÇCAPE-VÂíåGRBAS‰æùËµñ‰∏ìÂÆ∂ËØÑ‰º∞ÔºåÂ≠òÂú®ËØÑ‰º∞ËÄÖÈó¥ÁöÑÂèòÂºÇÊÄßÔºåÂõ†Ê≠§ÈúÄË¶ÅÂÆ¢ËßÇËß£ÂÜ≥ÊñπÊ°à„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜÂ£∞Èü≥Ë¥®ÈáèËØÑ‰º∞ÁΩëÁªúÔºàVOQANetÔºâÔºåËØ•Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂Âà©Áî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåËØ≠Èü≥Âü∫Á°ÄÊ®°ÂûãÔºàSFMÔºâÂµåÂÖ•ÊèêÂèñÈ´òÂ±ÇÁâπÂæÅ„ÄÇ‰∏∫Ëøõ‰∏ÄÊ≠•ÊèêÂçáÊÄßËÉΩÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVOQANet+ÔºåÂ∞ÜËá™ÁõëÁù£SFMÂµåÂÖ•‰∏é‰ΩéÂ±ÇÂ£∞Â≠¶ÊèèËø∞Á¨¶ÔºàÂ¶ÇÊäñÂä®„ÄÅÈó™ÁÉÅÂíåÂô™Â£∞ÊØîÔºâÁªìÂêà„ÄÇ‰∏é‰ª•ÂæÄ‰ªÖÂÖ≥Ê≥®ÂÖÉÈü≥ÂèëÂ£∞ÁöÑÊ®°Âûã‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®ÂÖÉÈü≥Á∫ßÂíåÂè•Â≠êÁ∫ßËØ≠Èü≥‰∏äËøõË°åËØÑ‰º∞Ôºå‰ª•Ê£ÄÈ™åÂÖ∂Ê≥õÂåñËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éÂè•Â≠êÁöÑËæìÂÖ•Âú®ÊÇ£ËÄÖÁ∫ßÂà´‰∏äÂÖ∑ÊúâÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄßÔºåVOQANetÂú®CAPE-VÂíåGRBASÁª¥Â∫¶‰∏äÂùá‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãÔºåVOQANet+Êõ¥ÊòØÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÁóÖÁêÜÂ£∞Èü≥ËØÑ‰º∞‰∏≠Â≠òÂú®ÁöÑ‰∏ªËßÇÊÄßÂíåËØÑ‰º∞ËÄÖÈó¥ÂèòÂºÇÊÄßÁöÑÈóÆÈ¢ò„ÄÇ‰º†ÁªüÊñπÊ≥ï‰æùËµñ‰∏ìÂÆ∂ËØÑ‰º∞ÔºåÁº∫‰πèÂÆ¢ËßÇÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫VOQANetÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåËØ≠Èü≥Âü∫Á°ÄÊ®°ÂûãÔºàSFMÔºâÂµåÂÖ•ÔºåÊèêÂèñÈ´òÂ±ÇÁâπÂæÅÔºåÂêåÊó∂ÂºïÂÖ•‰ΩéÂ±ÇÂ£∞Â≠¶ÊèèËø∞Á¨¶‰ª•Â¢ûÂº∫Ê®°ÂûãÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVOQANetÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÁâπÂæÅÊèêÂèñÊ®°Âùó„ÄÅÊ≥®ÊÑèÂäõÊú∫Âà∂Ê®°ÂùóÂíåËØÑ‰º∞Ê®°Âùó„ÄÇÁâπÂæÅÊèêÂèñÊ®°ÂùóË¥üË¥£‰ªéËæìÂÖ•ËØ≠Èü≥‰∏≠ÊèêÂèñÈ´òÂ±ÇÂíå‰ΩéÂ±ÇÁâπÂæÅÔºåÊ≥®ÊÑèÂäõÊú∫Âà∂Ê®°ÂùóÂ¢ûÂº∫ÈáçË¶ÅÁâπÂæÅÁöÑÊùÉÈáçÔºåËØÑ‰º∞Ê®°ÂùóÂàôËøõË°åÊúÄÁªàÁöÑÂ£∞Èü≥Ë¥®ÈáèËØÑÂàÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜSFMÂµåÂÖ•‰∏é‰ΩéÂ±ÇÂ£∞Â≠¶ÁâπÂæÅÁªìÂêàÔºåÂΩ¢ÊàêVOQANet+ÔºåËøô‰ΩøÂæóÊ®°ÂûãÂú®Â§öÁßçÂèëÂ£∞ÊñπÂºè‰∏ãÂùáËÉΩ‰øùÊåÅÈ´òÂáÜÁ°ÆÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Âè•Â≠êÁ∫ßËØÑ‰º∞‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ®°ÂûãÈááÁî®Ëá™ÁõëÁù£Â≠¶‰π†Á≠ñÁï•ÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏∫ÁªìÂêàRMSEÂíåPearsonÁõ∏ÂÖ≥Á≥ªÊï∞Ôºå‰ª•‰ºòÂåñÊ®°ÂûãÂú®‰∏çÂêåËØÑ‰º∞Áª¥Â∫¶‰∏äÁöÑË°®Áé∞„ÄÇÁΩëÁªúÁªìÊûÑ‰∏äÔºåÈááÁî®Â§öÂ±ÇÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâÂíåÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºàRNNÔºâÁõ∏ÁªìÂêàÁöÑÊñπÂºèÔºå‰ª•ÊçïÊçâÊó∂Èó¥Â∫èÂàóÁâπÂæÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVOQANetÂú®CAPE-VÂíåGRBASÁª¥Â∫¶‰∏äÂùá‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®ÊÇ£ËÄÖÁ∫ßÂà´‰∏äÔºåÂè•Â≠êÁ∫ßËæìÂÖ•ÁöÑÂáÜÁ°ÆÊÄßÊòæËëóÊèêÈ´ò„ÄÇVOQANet+Âú®Âô™Â£∞Êù°‰ª∂‰∏ãË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜËØÑ‰º∞ÁöÑÂèØÈù†ÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂåªÁñóËØäÊñ≠„ÄÅËøúÁ®ãÂÅ•Â∫∑ÁõëÊµãÂíåËØ≠Èü≥Ê≤ªÁñóÁ≠â„ÄÇÈÄöËøáÊèê‰æõÊõ¥ÂÆ¢ËßÇÂíå‰∏ÄËá¥ÁöÑÂ£∞Èü≥Ë¥®ÈáèËØÑ‰º∞Â∑•ÂÖ∑ÔºåËÉΩÂ§üÂ∏ÆÂä©ÂåªÁîüÊõ¥Â•ΩÂú∞ËØäÊñ≠ÂíåÁõëÊµãÂ£∞Èü≥ÈöúÁ¢çÔºåÊèêÂçáÊÇ£ËÄÖÁöÑÊ≤ªÁñóÊïàÊûú„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂú®Êô∫ËÉΩËØ≠Èü≥Âä©ÊâãÂíåËØ≠Èü≥ËØÜÂà´Á≥ªÁªü‰∏≠ÂæóÂà∞Â∫îÁî®ÔºåÊèêÂçáÂÖ∂ÂØπÁóÖÁêÜÂ£∞Èü≥ÁöÑËØÜÂà´ËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Perceptual voice quality assessment plays a vital role in diagnosing and monitoring voice disorders. Traditional methods, such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and the Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS) scales, rely on expert raters and are prone to inter-rater variability, emphasizing the need for objective solutions. This study introduces the Voice Quality Assessment Network (VOQANet), a deep learning framework that employs an attention mechanism and Speech Foundation Model (SFM) embeddings to extract high-level features. To further enhance performance, we propose VOQANet+, which integrates self-supervised SFM embeddings with low-level acoustic descriptors-namely jitter, shimmer, and harmonics-to-noise ratio (HNR). Unlike previous approaches that focus solely on vowel-based phonation (PVQD-A), our models are evaluated on both vowel-level and sentence-level speech (PVQD-S) to assess generalizability. Experimental results demonstrate that sentence-based inputs yield higher accuracy, particularly at the patient level. Overall, VOQANet consistently outperforms baseline models in terms of root mean squared error (RMSE) and Pearson correlation coefficient across CAPE-V and GRBAS dimensions, with VOQANet+ achieving even greater performance gains. Additionally, VOQANet+ maintains consistent performance under noisy conditions, suggesting enhanced robustness for real-world and telehealth applications. This work highlights the value of combining SFM embeddings with low-level features for accurate and robust pathological voice assessment.

