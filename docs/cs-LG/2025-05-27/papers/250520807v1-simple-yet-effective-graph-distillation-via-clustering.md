---
layout: default
title: Simple yet Effective Graph Distillation via Clustering
---

# Simple yet Effective Graph Distillation via Clustering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20807" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20807v1</a>
  <a href="https://arxiv.org/pdf/2505.20807.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20807v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20807v1', 'Simple yet Effective Graph Distillation via Clustering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yurui Lai, Taiyan Zhang, Renchi Yang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: This is the technical report of the paper "Simple yet Effective Graph Distillation via Clustering" accepted by KDD 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºClustGDDä»¥è§£å†³å›¾ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„è®¡ç®—å¼€é”€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å›¾ç¥ç»ç½‘ç»œ` `å›¾æ•°æ®è’¸é¦` `èšç±»æ–¹æ³•` `èŠ‚ç‚¹åˆ†ç±»` `è®¡ç®—æ•ˆç‡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾æ•°æ®è’¸é¦æ–¹æ³•å¤šä¾èµ–å¯å‘å¼ç­–ç•¥ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œç»“æœè´¨é‡ä¸ä½³ã€‚
2. ClustGDDé€šè¿‡å¿«é€Ÿèšç±»æ–¹æ³•åˆæˆå‹ç¼©å›¾ï¼Œä¼˜åŒ–åŒè´¨æ€§ï¼Œæå‡äº†è’¸é¦è´¨é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒClustGDDåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šæ€§èƒ½ä¼˜äºç°æœ‰GDDæ–¹æ³•ï¼Œä¸”è®­ç»ƒé€Ÿåº¦æ˜¾è‘—æé«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å›¾è¡¨ç¤ºå­¦ä¹ åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„è®­ç»ƒä»ç„¶é¢ä¸´å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚æœ€è¿‘ï¼Œå›¾æ•°æ®è’¸é¦ï¼ˆGDDï¼‰ä½œä¸ºä¸€ç§å°†å¤§å‹å›¾è½¬åŒ–ä¸ºç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„å›¾çš„æŠ€æœ¯ï¼Œå±•ç°äº†æé«˜GNNè®­ç»ƒæ•ˆç‡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„GDDæ–¹æ³•å¤šä¾èµ–å¯å‘å¼ç­–ç•¥ï¼Œå¯¼è‡´ç»“æœè´¨é‡ä¸‹é™æˆ–è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„GDDæ–¹æ³•ClustGDDï¼Œé€šè¿‡å¿«é€Ÿä¸”ç†è®ºåŸºç¡€æ‰å®çš„èšç±»æ¥åˆæˆå‹ç¼©å›¾å’ŒèŠ‚ç‚¹å±æ€§ï¼Œä¼˜åŒ–äº†å›¾çš„åŒè´¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºClustGDDç”Ÿæˆçš„å‹ç¼©å›¾è®­ç»ƒçš„GNNåœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¼˜äºæˆ–å¯ä¸æœ€å…ˆè¿›çš„GDDæ–¹æ³•ç›¸åª²ç¾ï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„è®¡ç®—å¼€é”€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å¯å‘å¼ç­–ç•¥ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œç»“æœè´¨é‡ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šClustGDDçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¿«é€Ÿä¸”ç†è®ºåŸºç¡€æ‰å®çš„èšç±»æ–¹æ³•æ¥åˆæˆå‹ç¼©å›¾å’ŒèŠ‚ç‚¹å±æ€§ï¼Œæœ€å¤§åŒ–å›¾çš„åŒè´¨æ€§ï¼Œä»è€Œæé«˜è’¸é¦è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šClustGDDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€èšç±»ç”Ÿæˆå‹ç¼©å›¾ã€èŠ‚ç‚¹å±æ€§ä¼˜åŒ–å’Œæ¨¡å‹è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚èšç±»è¿‡ç¨‹é€šè¿‡æœ€å°åŒ–èšç±»å†…å¹³æ–¹å’Œæ¥å®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šClustGDDçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†èšç±»ä¸è’¸é¦è´¨é‡çš„å…³ç³»è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œåˆ©ç”¨FrÃ©chet Inception Distanceä½œä¸ºè´¨é‡åº¦é‡ï¼Œæ˜¾è‘—æå‡äº†å‹ç¼©å›¾çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨èŠ‚ç‚¹å±æ€§ä¼˜åŒ–ä¸­ï¼ŒClustGDDé‡‡ç”¨äº†ç±»æ„ŸçŸ¥å›¾é‡‡æ ·å’Œä¸€è‡´æ€§æŸå¤±ï¼Œè¿›ä¸€æ­¥æ”¹å–„äº†å‹ç¼©å›¾çš„èŠ‚ç‚¹å±æ€§ï¼Œç¡®ä¿äº†è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºClustGDDç”Ÿæˆçš„å‹ç¼©å›¾è®­ç»ƒçš„GNNåœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¼˜äºæˆ–å¯ä¸æœ€å…ˆè¿›çš„GDDæ–¹æ³•ç›¸åª²ç¾ï¼Œä¸”è®­ç»ƒé€Ÿåº¦æå‡äº†å¤šä¸ªæ•°é‡çº§ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿå’Œç”Ÿç‰©ä¿¡æ¯å­¦ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å›¾ç¥ç»ç½‘ç»œåœ¨å¤§è§„æ¨¡å›¾æ•°æ®ä¸Šçš„è®­ç»ƒæ•ˆç‡ã€‚æœªæ¥ï¼ŒClustGDDæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„å›¾å­¦ä¹ ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite plentiful successes achieved by graph representation learning in various domains, the training of graph neural networks (GNNs) still remains tenaciously challenging due to the tremendous computational overhead needed for sizable graphs in practice. Recently, graph data distillation (GDD), which seeks to distill large graphs into compact and informative ones, has emerged as a promising technique to enable efficient GNN training. However, most existing GDD works rely on heuristics that align model gradients or representation distributions on condensed and original graphs, leading to compromised result quality, expensive training for distilling large graphs, or both. Motivated by this, this paper presents an efficient and effective GDD approach, ClustGDD. Under the hood, ClustGDD resorts to synthesizing the condensed graph and node attributes through fast and theoretically-grounded clustering that minimizes the within-cluster sum of squares and maximizes the homophily on the original graph. The fundamental idea is inspired by our empirical and theoretical findings unveiling the connection between clustering and empirical condensation quality using FrÃ©chet Inception Distance, a well-known quality metric for synthetic images. Furthermore, to mitigate the adverse effects caused by the homophily-based clustering, ClustGDD refines the nodal attributes of the condensed graph with a small augmentation learned via class-aware graph sampling and consistency loss. Our extensive experiments exhibit that GNNs trained over condensed graphs output by ClustGDD consistently achieve superior or comparable performance to state-of-the-art GDD methods in terms of node classification on five benchmark datasets, while being orders of magnitude faster.

