---
layout: default
title: HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling
---

# HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20836" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20836v1</a>
  <a href="https://arxiv.org/pdf/2505.20836.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20836v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20836v1', 'HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hexiong Yang, Mingrui Chen, Huaibo Huang, Junxian Duan, Jie Cao, Zhen Zhou, Ran He

**åˆ†ç±»**: cs.LG, q-bio.GN

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆæ¶æ„è’¸é¦æ–¹æ³•ä»¥æå‡åŸºå› åºåˆ—å»ºæ¨¡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŸºå› åºåˆ—å»ºæ¨¡` `æ··åˆæ¶æ„è’¸é¦` `è‡ªç›‘ç£å­¦ä¹ ` `æ·±åº¦å­¦ä¹ ` `ç”Ÿç‰©ä¿¡æ¯å­¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå› åºåˆ—å»ºæ¨¡æ–¹æ³•ä¾èµ–å¤§é‡é¢„è®­ç»ƒæ•°æ®æˆ–å¤§è§„æ¨¡æ¨¡å‹ï¼Œè®¡ç®—è´Ÿæ‹…é‡ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºæ··åˆæ¶æ„è’¸é¦ï¼ˆHADï¼‰æ–¹æ³•ï¼Œé€šè¿‡è’¸é¦ä¸é‡å»ºä»»åŠ¡ç»“åˆï¼Œæå‡é¢„è®­ç»ƒæ•ˆç‡ä¸æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHADåœ¨Nucleotide Transformer Benchmarkå’ŒGenomic Benchmarkä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å—æ©è”½è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰åœ¨è‡ªç„¶è¯­è¨€é¢†åŸŸæˆåŠŸçš„å¯å‘ï¼Œè‡ªç›‘ç£é¢„è®­ç»ƒå’Œå¾®è°ƒçš„èŒƒå¼åœ¨DNAåºåˆ—å»ºæ¨¡é¢†åŸŸä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡çš„é¢„è®­ç»ƒæ•°æ®æˆ–å¤§è§„æ¨¡çš„åŸºç¡€æ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—è´Ÿæ‹…æ˜¾è‘—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„è’¸é¦ï¼ˆHADï¼‰æ–¹æ³•ï¼Œç»“åˆè’¸é¦å’Œé‡å»ºä»»åŠ¡ä»¥å®ç°æ›´é«˜æ•ˆçš„é¢„è®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨NTv2-500Mä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†åˆ†ç»„æ©è”½ç­–ç•¥ï¼Œä»¥å¯¹é½å¯è§æ ‡è®°çš„ç‰¹å¾åµŒå…¥ï¼ŒåŒæ—¶åœ¨MLMé¢„è®­ç»ƒè¿‡ç¨‹ä¸­é‡å»ºä¸å¯è§æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å‚æ•°ç›¸ä¼¼çš„æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³åœ¨æŸäº›å­ä»»åŠ¡ä¸Šè¶…è¶Šäº†å‚æ•°è¶…è¿‡500å€çš„è’¸é¦æ•™å¸ˆæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºå› åºåˆ—å»ºæ¨¡ä¸­ç°æœ‰æ–¹æ³•å¯¹å¤§é‡é¢„è®­ç»ƒæ•°æ®å’Œå¤§è§„æ¨¡æ¨¡å‹çš„ä¾èµ–ï¼Œå¯¼è‡´çš„è®¡ç®—è´Ÿæ‹…å’Œæ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ··åˆæ¶æ„è’¸é¦ï¼ˆHADï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè’¸é¦å’Œé‡å»ºä»»åŠ¡ï¼Œåˆ©ç”¨æ›´ç´§å‡‘çš„æ¨¡å‹å®ç°é«˜æ•ˆçš„é¢„è®­ç»ƒï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHADæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹NTv2-500Mã€åˆ†ç»„æ©è”½ç­–ç•¥å’ŒMLMé¢„è®­ç»ƒé˜¶æ®µã€‚æ•™å¸ˆæ¨¡å‹æä¾›çŸ¥è¯†è’¸é¦ï¼Œåˆ†ç»„æ©è”½ç­–ç•¥ç”¨äºå¯¹é½å¯è§æ ‡è®°çš„ç‰¹å¾åµŒå…¥ï¼ŒåŒæ—¶é‡å»ºä¸å¯è§æ ‡è®°ã€‚

**å…³é”®åˆ›æ–°**ï¼šHADçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è’¸é¦ä¸é‡å»ºä»»åŠ¡ç»“åˆï¼Œè®¾è®¡åˆ†ç»„æ©è”½ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ ç‰¹å¾è¡¨ç¤ºï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è’¸é¦ä¸é‡å»ºä»»åŠ¡çš„æƒé‡ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥é€‚åº”åŸºå› åºåˆ—çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHADæ–¹æ³•åœ¨Nucleotide Transformer Benchmarkå’ŒGenomic Benchmarkä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å‚æ•°è¶…è¿‡500å€çš„è’¸é¦æ•™å¸ˆæ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨åŸºå› åºåˆ—å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŸºå› ç»„å­¦ã€ä¸ªæ€§åŒ–åŒ»ç–—å’Œç”Ÿç‰©ä¿¡æ¯å­¦ç­‰ã€‚é€šè¿‡æé«˜åŸºå› åºåˆ—å»ºæ¨¡çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒHADæ–¹æ³•èƒ½å¤Ÿä¸ºåŸºå› ç»„æ•°æ®åˆ†ææä¾›æ›´å¼ºå¤§çš„å·¥å…·ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ä¸åº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.

