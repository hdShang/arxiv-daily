---
layout: default
title: Improving LLM-based Global Optimization with Search Space Partitioning
---

# Improving LLM-based Global Optimization with Search Space Partitioning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21372" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21372v1</a>
  <a href="https://arxiv.org/pdf/2505.21372.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21372v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21372v1', 'Improving LLM-based Global Optimization with Search Space Partitioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrej Schwanke, Lyubomir Ivanov, David Salinas, Fabio Ferreira, Aaron Klein, Frank Hutter, Arber Zela

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: 25 pages, 10 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHOLLMç®—æ³•ä»¥è§£å†³é«˜ç»´æœç´¢ç©ºé—´ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…¨çƒä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœç´¢ç©ºé—´åˆ’åˆ†` `è´å¶æ–¯ä¼˜åŒ–` `ä¿¡èµ–åŸŸæ–¹æ³•` `é«˜ç»´ä¼˜åŒ–` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LLMé©±åŠ¨çš„ä¼˜åŒ–æ–¹æ³•åœ¨é«˜ç»´æœç´¢ç©ºé—´ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹é¢†åŸŸçŸ¥è¯†æ—¶ï¼Œå¯¼è‡´ç”Ÿæˆçš„å»ºè®®ç¨€ç–ä¸”ç¼ºä¹ä¿¡æ¯ã€‚
2. æœ¬æ–‡æå‡ºHOLLMç®—æ³•ï¼Œé€šè¿‡å°†æœç´¢ç©ºé—´åˆ’åˆ†ä¸ºæœ‰å‰æ™¯çš„å­åŒºåŸŸï¼Œåˆ©ç”¨èµŒåšæœºçµæ„Ÿçš„è¯„åˆ†æœºåˆ¶æ¥é€‰æ‹©å­åŒºåŸŸï¼Œä»è€Œæé«˜é‡‡æ ·è´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHOLLMåœ¨æ ‡å‡†ä¼˜åŒ–åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šç°æœ‰çš„è´å¶æ–¯ä¼˜åŒ–å’Œä¿¡èµ–åŸŸæ–¹æ³•ï¼Œä¸”æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„LLMé‡‡æ ·ç­–ç•¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘åœ¨å…¨çƒä¼˜åŒ–æ¡†æ¶ä¸­ä½œä¸ºæœ‰æ•ˆçš„æ›¿ä»£æ¨¡å‹å’Œå€™é€‰ç”Ÿæˆå™¨å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼ŒLLMé©±åŠ¨çš„æ–¹æ³•åœ¨é«˜ç»´æœç´¢ç©ºé—´æˆ–ç¼ºä¹é¢†åŸŸç‰¹å®šå…ˆéªŒæ—¶ï¼Œå¸¸å¸¸é¢ä¸´ç¨€ç–æˆ–æ— ä¿¡æ¯çš„å»ºè®®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…¨çƒä¼˜åŒ–ç®—æ³•HOLLMï¼Œé€šè¿‡å°†æœç´¢ç©ºé—´åˆ’åˆ†ä¸ºæœ‰å‰æ™¯çš„å­åŒºåŸŸæ¥å¢å¼ºLLMé©±åŠ¨çš„é‡‡æ ·ã€‚æ¯ä¸ªå­åŒºåŸŸä½œä¸ºä¸€ä¸ªâ€œå…ƒè‡‚â€ï¼Œé€šè¿‡ä¸€ç§çµæ„Ÿæ¥è‡ªèµŒåšæœºçš„è¯„åˆ†æœºåˆ¶è¿›è¡Œé€‰æ‹©ï¼Œæœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚åœ¨æ¯ä¸ªé€‰å®šçš„å­åŒºåŸŸå†…ï¼ŒLLMæå‡ºé«˜è´¨é‡çš„å€™é€‰ç‚¹ï¼Œè€Œæ— éœ€ä»»ä½•æ˜¾å¼çš„é¢†åŸŸçŸ¥è¯†ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒHOLLMåœ¨æ ‡å‡†ä¼˜åŒ–åŸºå‡†ä¸Šå§‹ç»ˆä¸é¢†å…ˆçš„è´å¶æ–¯ä¼˜åŒ–å’Œä¿¡èµ–åŸŸæ–¹æ³•ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶æ˜¾è‘—ä¼˜äºå…¨çƒLLMé©±åŠ¨çš„é‡‡æ ·ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMé©±åŠ¨çš„å…¨çƒä¼˜åŒ–æ–¹æ³•åœ¨é«˜ç»´æœç´¢ç©ºé—´ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹é¢†åŸŸç‰¹å®šå…ˆéªŒçŸ¥è¯†æ—¶ï¼Œå¯¼è‡´ç”Ÿæˆçš„å€™é€‰ç‚¹ç¨€ç–ä¸”æ— ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHOLLMç®—æ³•é€šè¿‡å°†æœç´¢ç©ºé—´åˆ’åˆ†ä¸ºå¤šä¸ªæœ‰å‰æ™¯çš„å­åŒºåŸŸï¼Œåˆ©ç”¨ä¸€ç§åŸºäºèµŒåšæœºçš„è¯„åˆ†æœºåˆ¶æ¥é€‰æ‹©è¿™äº›å­åŒºåŸŸï¼Œä»è€Œåœ¨æ¯ä¸ªå­åŒºåŸŸå†…ç”Ÿæˆé«˜è´¨é‡çš„å€™é€‰ç‚¹ï¼Œå¢å¼ºäº†é‡‡æ ·çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHOLLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æœç´¢ç©ºé—´çš„åˆ’åˆ†æ¨¡å—ï¼Œé€šè¿‡è¯„ä¼°å„ä¸ªå­åŒºåŸŸçš„æ½œåŠ›æ¥é€‰æ‹©â€œå…ƒè‡‚â€ï¼›å…¶æ¬¡æ˜¯LLMç”Ÿæˆæ¨¡å—ï¼Œåœ¨é€‰å®šçš„å­åŒºåŸŸå†…ç”Ÿæˆå€™é€‰ç‚¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šHOLLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†æœç´¢ç©ºé—´çš„åˆ’åˆ†æœºåˆ¶å’ŒåŸºäºèµŒåšæœºçš„è¯„åˆ†ç­–ç•¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å…¨å±€ä¼˜åŒ–æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨HOLLMä¸­ï¼Œè¯„åˆ†æœºåˆ¶çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå®ƒé€šè¿‡åŠ¨æ€è¯„ä¼°å­åŒºåŸŸçš„æ½œåŠ›æ¥æŒ‡å¯¼æœç´¢è¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒLLMçš„ä½¿ç”¨ä¸ä¾èµ–äºä»»ä½•é¢†åŸŸçŸ¥è¯†ï¼Œä½¿å¾—è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HOLLMåœ¨æ ‡å‡†ä¼˜åŒ–åŸºå‡†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶æ€§èƒ½ä¸é¢†å…ˆçš„è´å¶æ–¯ä¼˜åŒ–å’Œä¿¡èµ–åŸŸæ–¹æ³•ç›¸å½“ï¼Œä¸”åœ¨å¤šä¸ªæµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„LLMé©±åŠ¨é‡‡æ ·ç­–ç•¥ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HOLLMç®—æ³•åœ¨é«˜ç»´ä¼˜åŒ–é—®é¢˜ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºéœ€è¦é«˜æ•ˆæœç´¢çš„é¢†åŸŸï¼Œå¦‚å·¥ç¨‹è®¾è®¡ã€æœºå™¨å­¦ä¹ è¶…å‚æ•°ä¼˜åŒ–å’Œå¤æ‚ç³»ç»Ÿçš„ä¼˜åŒ–ã€‚å…¶åˆ›æ–°çš„æœç´¢ç©ºé—´åˆ’åˆ†æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜ä¼˜åŒ–æ•ˆç‡ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ä¸åº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have recently emerged as effective surrogate models and candidate generators within global optimization frameworks for expensive blackbox functions. Despite promising results, LLM-based methods often struggle in high-dimensional search spaces or when lacking domain-specific priors, leading to sparse or uninformative suggestions. To overcome these limitations, we propose HOLLM, a novel global optimization algorithm that enhances LLM-driven sampling by partitioning the search space into promising subregions. Each subregion acts as a ``meta-arm'' selected via a bandit-inspired scoring mechanism that effectively balances exploration and exploitation. Within each selected subregion, an LLM then proposes high-quality candidate points, without any explicit domain knowledge. Empirical evaluation on standard optimization benchmarks shows that HOLLM consistently matches or surpasses leading Bayesian optimization and trust-region methods, while substantially outperforming global LLM-based sampling strategies.

