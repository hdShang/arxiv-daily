---
layout: default
title: PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective
---

# PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21799" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21799v2</a>
  <a href="https://arxiv.org/pdf/2505.21799.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21799v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21799v2', 'PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tim Tsz-Kit Lau, Qi Long, Weijie Su

**ÂàÜÁ±ª**: math.OC, cs.LG, stat.ML

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27 (Êõ¥Êñ∞: 2025-08-02)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PolarGrad‰ª•ÊèêÂçáÊ∑±Â∫¶Â≠¶‰π†‰ºòÂåñÊïàÁéá**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶Â≠¶‰π†` `‰ºòÂåñÊñπÊ≥ï` `È¢ÑÊù°‰ª∂Ê¢ØÂ∫¶` `Áü©ÈòµÁªìÊûÑ` `ÊûÅÂùêÊ†áÂàÜËß£` `Êî∂ÊïõÊÄß` `ËØ≠Ë®ÄÊ®°Âûã` `Êú∫Âô®Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈ¢ÑÊù°‰ª∂‰ºòÂåñÊñπÊ≥ïÂú®Â§ÑÁêÜÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÊó∂ÔºåÂæÄÂæÄÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Ê¢ØÂ∫¶ÁöÑÁü©ÈòµÁªìÊûÑÔºåÂØºËá¥Êî∂ÊïõÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑPolarGradÈÄöËøáÊûÅÂùêÊ†áÂàÜËß£Êù•‰ºòÂåñÁü©ÈòµÂÄºÊ¢ØÂ∫¶ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁªìÊûÑÊÑüÁü•È¢ÑÊù°‰ª∂‰ºòÂåñÊñπÊ≥ï„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåPolarGradÂú®Â§ö‰∏™Áü©Èòµ‰ºòÂåñÈóÆÈ¢òÂíåËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°‰∏≠ÔºåÊÄßËÉΩÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑAdamÂíåMuon‰ºòÂåñÂô®„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÂíåÊï∞ÊçÆÈõÜËßÑÊ®°ÁöÑ‰∏çÊñ≠Êâ©Â§ßÔºåÈ´òÊïàÁöÑ‰ºòÂåñÊñπÊ≥ïÊòæÂæóÂ∞§‰∏∫ÈáçË¶Å„ÄÇÂ∞ΩÁÆ°ÂÉèAdamÂíåAdamWËøôÊ†∑ÁöÑÈ¢ÑÊù°‰ª∂Ê¢ØÂ∫¶ÊñπÊ≥ïÂ∑≤Êàê‰∏∫ËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ†áÂáÜ‰ºòÂåñÂô®Ôºå‰ΩÜÁªìÊûÑÊÑüÁü•ÁöÑÈ¢ÑÊù°‰ª∂‰ºòÂåñÂô®Â¶ÇShampooÂíåMuonÂà©Áî®Ê¢ØÂ∫¶ÁöÑÁü©ÈòµÁªìÊûÑÔºåÊòæÁ§∫Âá∫Êõ¥Âø´ÁöÑÊî∂ÊïõÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÊ°ÜÊû∂Êù•ÂàÜÊûê‚ÄúÁü©ÈòµÊÑüÁü•‚ÄùÈ¢ÑÊù°‰ª∂ÊñπÊ≥ïÔºå‰∏ç‰ªÖÈòêÊòé‰∫ÜMuonÂèäÁõ∏ÂÖ≥‰ºòÂåñÂô®ÁöÑÊúâÊïàÊÄßÔºåËøòÂºïÂÖ•‰∫Ü‰∏ÄÁ±ªÊñ∞ÁöÑÁªìÊûÑÊÑüÁü•È¢ÑÊù°‰ª∂ÊñπÊ≥ï„ÄÇÂÖ≥ÈîÆË¥°ÁåÆÂú®‰∫éÁ≤æÁ°ÆÂå∫ÂàÜ‰∫ÜÂ∞ÜÁ•ûÁªèÁΩëÁªúÊùÉÈáçËßÜ‰∏∫ÂêëÈáèÁöÑÈ¢ÑÂ§ÑÁêÜÁ≠ñÁï•‰∏éËÄÉËôëÂÖ∂Áü©ÈòµÁªìÊûÑÁöÑÁ≠ñÁï•„ÄÇÂü∫‰∫éÊ≠§Ê°ÜÊû∂ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜPolarGradÔºå‰∏ÄÁßçÂü∫‰∫éÁü©ÈòµÂÄºÊ¢ØÂ∫¶ÁöÑÊûÅÂùêÊ†áÂàÜËß£ÁöÑÊñ∞ÂûãÈ¢ÑÊù°‰ª∂‰ºòÂåñÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPolarGradÂú®Â§öÁßçÁü©Èòµ‰ºòÂåñÈóÆÈ¢òÂíåËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°‰∏≠Âùá‰ºò‰∫éAdamÂíåMuon„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊ∑±Â∫¶Â≠¶‰π†‰ºòÂåñÊñπÊ≥ïÂú®Â§ÑÁêÜÊ¢ØÂ∫¶Áü©ÈòµÁªìÊûÑÊó∂ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÊî∂ÊïõÈÄüÂ∫¶ÊÖ¢ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÈ¢ÑÊù°‰ª∂‰ºòÂåñÂô®Â¶ÇAdamÂíåMuonÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Ê¢ØÂ∫¶ÁöÑÁü©ÈòµÁâπÊÄßÔºåÂØºËá¥Âú®Êüê‰∫õ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ¢ÑÊù°‰ª∂‰ºòÂåñÊñπÊ≥ïPolarGradÔºåÂü∫‰∫éÁü©ÈòµÂÄºÊ¢ØÂ∫¶ÁöÑÊûÅÂùêÊ†áÂàÜËß£ÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Ê¢ØÂ∫¶ÁöÑÁü©ÈòµÁªìÊûÑÔºå‰ªéËÄåÊèêÂçá‰ºòÂåñÊïàÁéá„ÄÇÈÄöËøáËøôÁßçËÆæËÆ°ÔºåPolarGradËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÊ¢ØÂ∫¶ÁöÑÂêÑÂêëÂºÇÊÄßÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨ÂØπÊ¢ØÂ∫¶ÁöÑÊûÅÂùêÊ†áÂàÜËß£„ÄÅÈ¢ÑÊù°‰ª∂Á≠ñÁï•ÁöÑËÆæËÆ°‰ª•Âèä‰ºòÂåñÊõ¥Êñ∞ÁöÑËÆ°ÁÆó„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨Ê¢ØÂ∫¶ËÆ°ÁÆó„ÄÅÊûÅÂùêÊ†áÂàÜËß£„ÄÅÈ¢ÑÊù°‰ª∂Êõ¥Êñ∞ÂíåÊî∂ÊïõÊÄßËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂàÜÊûêÊ°ÜÊû∂ÔºåËÉΩÂ§üÂå∫ÂàÜÂ§ÑÁêÜÂêëÈáèÂíåÁü©ÈòµÁöÑÈ¢ÑÂ§ÑÁêÜÁ≠ñÁï•ÔºåÂπ∂ÂºïÂÖ•‰∫ÜPolarGrad‰Ωú‰∏∫Êñ∞ÁöÑ‰ºòÂåñÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜÊî∂ÊïõÈÄüÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®PolarGrad‰∏≠ÔºåÊõ¥Êñ∞Ê≠•È™§ÈÄöËøáÊ†∏ËåÉÊï∞ÂØπÊ¢ØÂ∫¶ËøõË°åÁº©ÊîæÔºåÁ°Æ‰øù‰∫Ü‰ºòÂåñËøáÁ®ãÁöÑÁ®≥ÂÆöÊÄßÂíåÈ´òÊïàÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰πüÁªèËøáÁ≤æÂøÉË∞ÉÊï¥Ôºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑ‰ºòÂåñ‰ªªÂä°„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPolarGradÂú®Â§ö‰∏™Áü©Èòµ‰ºòÂåñÈóÆÈ¢òÂíåËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°‰∏≠ÔºåÂùá‰ºò‰∫éAdamÂíåMuonÔºåÊî∂ÊïõÈÄüÂ∫¶ÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PolarGradÁöÑÁ†îÁ©∂ÊàêÊûúÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂Âú®Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑËÆ≠ÁªÉÂíå‰ºòÂåñ‰∏≠„ÄÇÂÖ∂È´òÊïàÁöÑÊî∂ÊïõÁâπÊÄßÂèØ‰ª•Â∫îÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÅÂõæÂÉèÂ§ÑÁêÜÂíåÂÖ∂‰ªñÈúÄË¶ÅÂø´ÈÄü‰ºòÂåñÁöÑÊú∫Âô®Â≠¶‰π†‰ªªÂä°ÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êõ¥Â§çÊùÇÊ®°ÂûãÁöÑÂºÄÂèë‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The ever-growing scale of deep learning models and datasets underscores the critical importance of efficient optimization methods. While preconditioned gradient methods such as Adam and AdamW are the de facto optimizers for training neural networks and large language models, structure-aware preconditioned optimizers like Shampoo and Muon, which utilize the matrix structure of gradients, have demonstrated promising evidence of faster convergence. In this paper, we introduce a unifying framework for analyzing "matrix-aware" preconditioned methods, which not only sheds light on the effectiveness of Muon and related optimizers but also leads to a class of new structure-aware preconditioned methods. A key contribution of this framework is its precise distinction between preconditioning strategies that treat neural network weights as vectors (addressing curvature anisotropy) versus those that consider their matrix structure (addressing gradient anisotropy). This perspective provides new insights into several empirical phenomena in language model pre-training, including Adam's training instabilities, Muon's accelerated convergence, and the necessity of learning rate warmup for Adam. Building upon this framework, we introduce PolarGrad, a new class of preconditioned optimization methods based on the polar decomposition of matrix-valued gradients. As a special instance, PolarGrad includes Muon with updates scaled by the nuclear norm of the gradients. We provide numerical implementations of these methods, leveraging efficient numerical polar decomposition algorithms for enhanced convergence. Our extensive evaluations across diverse matrix optimization problems and language model pre-training tasks demonstrate that PolarGrad outperforms both Adam and Muon.

