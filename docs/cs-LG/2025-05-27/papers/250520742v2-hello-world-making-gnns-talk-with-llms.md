---
layout: default
title: "'Hello, World!': Making GNNs Talk with LLMs"
---

# 'Hello, World!': Making GNNs Talk with LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20742" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20742v2</a>
  <a href="https://arxiv.org/pdf/2505.20742.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20742v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20742v2', '\'Hello, World!\': Making GNNs Talk with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sunwoo Kim, Soo Yong Lee, Jaemin Yoo, Kijung Shin

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-15)

**å¤‡æ³¨**: Published as a conference paper at EMNLP 2025 Findings. Code and datasets are in https://github.com/kswoo97/GLN-Code

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›¾è¯­è¨€ç½‘ç»œä»¥æå‡å›¾ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šæ€§ä¸æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾ç¥ç»ç½‘ç»œ` `å¯è§£é‡Šæ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `èŠ‚ç‚¹åˆ†ç±»` `é“¾æ¥é¢„æµ‹` `å›¾æ³¨æ„åŠ›æœºåˆ¶` `æ®‹å·®è¿æ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾ç¥ç»ç½‘ç»œç”±äºå…¶é«˜ç»´éšè—è¡¨ç¤ºï¼Œå¯¼è‡´å…¶å†…éƒ¨æœºåˆ¶éš¾ä»¥ç†è§£ï¼Œæˆä¸ºé»‘ç®±ã€‚
2. æœ¬æ–‡æå‡ºçš„å›¾è¯­è¨€ç½‘ç»œï¼ˆGLNï¼‰é€šè¿‡å°†éšè—è¡¨ç¤ºè½¬åŒ–ä¸ºäººç±»å¯è¯»æ–‡æœ¬ï¼Œæå‡äº†å¯è§£é‡Šæ€§ï¼Œå¹¶ç»“åˆäº†å¤šç§GNNæŠ€æœ¯ã€‚
3. GLNåœ¨èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„é›¶-shotæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨å¤šç§å›¾ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é«˜ç»´éšè—è¡¨ç¤ºä½¿å…¶æˆä¸ºé»‘ç®±ã€‚æœ¬æ–‡æå‡ºäº†å›¾è¯­è¨€ç½‘ç»œï¼ˆGLNï¼‰ï¼Œè¯¥ç½‘ç»œåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå…¶éšè—è¡¨ç¤ºä»¥äººç±»å¯è¯»æ–‡æœ¬çš„å½¢å¼å‘ˆç°ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼ŒGLNä¸ä»…ç»“åˆäº†GNNçš„æ¶ˆæ¯ä¼ é€’æ¨¡å—ï¼Œè¿˜èå…¥äº†å›¾æ³¨æ„åŠ›å’Œåˆå§‹æ®‹å·®è¿æ¥ç­‰å…ˆè¿›æŠ€æœ¯ã€‚GLNçš„éšè—è¡¨ç¤ºçš„å¯ç†è§£æ€§ä½¿å¾—å¯¹èŠ‚ç‚¹è¡¨ç¤ºåœ¨ä¸åŒå±‚æ¬¡å’Œå…ˆè¿›GNNæŠ€æœ¯ä¸‹çš„å˜åŒ–è¿›è¡Œç›´è§‚åˆ†ææˆä¸ºå¯èƒ½ï¼Œæ­ç¤ºäº†GNNçš„å†…éƒ¨å·¥ä½œåŸç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†GLNåœ¨èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ä¸Šçš„å¼ºå¤§é›¶-shotæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºäºLLMçš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨é«˜ç»´éšè—è¡¨ç¤ºä¸‹çš„å¯è§£é‡Šæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥æ­ç¤ºå…¶å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå›¾è¯­è¨€ç½‘ç»œï¼ˆGLNï¼‰ï¼Œé€šè¿‡å°†éšè—è¡¨ç¤ºè½¬åŒ–ä¸ºäººç±»å¯è¯»æ–‡æœ¬ï¼Œç»“åˆGNNçš„æ¶ˆæ¯ä¼ é€’æ¨¡å—åŠå…ˆè¿›æŠ€æœ¯ï¼Œæå‡å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGLNçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¶ˆæ¯ä¼ é€’æ¨¡å—ã€å›¾æ³¨æ„åŠ›æœºåˆ¶å’Œåˆå§‹æ®‹å·®è¿æ¥ï¼Œé‡‡ç”¨ç²¾å¿ƒè®¾è®¡çš„æç¤ºæ¥ç”Ÿæˆå¯è¯»æ–‡æœ¬è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šGLNçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†GNNçš„éšè—è¡¨ç¤ºè½¬åŒ–ä¸ºæ–‡æœ¬å½¢å¼ï¼Œä½¿å¾—èŠ‚ç‚¹è¡¨ç¤ºçš„å˜åŒ–æ›´åŠ ç›´è§‚ï¼Œæ˜¾è‘—æå‡äº†å¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒGLNé‡‡ç”¨äº†ç‰¹å®šçš„æç¤ºç­–ç•¥ä»¥ä¼˜åŒ–æ–‡æœ¬ç”Ÿæˆï¼ŒåŒæ—¶ç»“åˆäº†å›¾æ³¨æ„åŠ›æœºåˆ¶å’Œæ®‹å·®è¿æ¥ï¼Œç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ä¸ä¿ç•™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGLNåœ¨èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº†å¼ºå¤§çš„é›¶-shotæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºçº¿æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å›¾æ•°æ®åˆ†æã€ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡å›¾ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šæ€§ï¼ŒGLNèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆæ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–å›¾æ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While graph neural networks (GNNs) have shown remarkable performance across diverse graph-related tasks, their high-dimensional hidden representations render them black boxes. In this work, we propose Graph Lingual Network (GLN), a GNN built on large language models (LLMs), with hidden representations in the form of human-readable text. Through careful prompt design, GLN incorporates not only the message passing module of GNNs but also advanced GNN techniques, including graph attention and initial residual connection. The comprehensibility of GLN's hidden representations enables an intuitive analysis of how node representations change (1) across layers and (2) under advanced GNN techniques, shedding light on the inner workings of GNNs. Furthermore, we demonstrate that GLN achieves strong zero-shot performance on node classification and link prediction, outperforming existing LLM-based baseline methods.

