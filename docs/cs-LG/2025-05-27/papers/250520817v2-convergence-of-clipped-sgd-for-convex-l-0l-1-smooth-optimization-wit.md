---
layout: default
title: Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise
---

# Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20817" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20817v2</a>
  <a href="https://arxiv.org/pdf/2505.20817.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20817v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20817v2', 'Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Savelii Chezhegov, Aleksandr Beznosikov, Samuel HorvÃ¡th, Eduard Gorbunov

**åˆ†ç±»**: math.OC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: 33 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ¦‚ç‡æ”¶æ•›ç•Œé™ä»¥è§£å†³é‡å°¾å™ªå£°ä¸‹çš„ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¢¯åº¦è£å‰ª` `é‡å°¾å™ªå£°` `é«˜æ¦‚ç‡æ”¶æ•›` `æ·±åº¦å­¦ä¹ ` `ä¼˜åŒ–ç®—æ³•` `$(L_0,L_1)$-å…‰æ»‘æ€§` `Clip-SGD`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é‡å°¾å™ªå£°å’Œ$(L_0,L_1)$-å…‰æ»‘æ€§å‡è®¾ä¸‹çš„é«˜æ¦‚ç‡æ”¶æ•›æ€§æœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå­˜åœ¨ç†è®ºç©ºç™½ã€‚
2. è®ºæ–‡æå‡ºäº†Clip-SGDæ–¹æ³•åœ¨é‡å°¾å™ªå£°ä¸‹çš„é«˜æ¦‚ç‡æ”¶æ•›ç•Œé™ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. é€šè¿‡ç†è®ºåˆ†æï¼Œæ¢å¤äº†å·²çŸ¥çš„ç¡®å®šæ€§å’Œéšæœºæƒ…å†µçš„æ”¶æ•›ç•Œé™ï¼Œæ˜¾è‘—æå‡äº†æ–¹æ³•çš„é€‚ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¢¯åº¦è£å‰ªæ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡è½»é‡å°¾å™ªå£°å¯¹å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„å½±å“ã€‚æœ¬æ–‡é¦–æ¬¡ä¸ºåœ¨é‡å°¾å™ªå£°å’Œ$(L_0,L_1)$-å…‰æ»‘æ€§å‡è®¾ä¸‹çš„Clip-SGDæ–¹æ³•å»ºç«‹é«˜æ¦‚ç‡æ”¶æ•›ç•Œé™ï¼Œå¡«è¡¥äº†æ–‡çŒ®ä¸­çš„é‡è¦ç©ºç™½ã€‚æˆ‘ä»¬çš„åˆ†ææ‰©å±•äº†å…ˆå‰çš„ç»“æœï¼Œæ¢å¤äº†ç¡®å®šæ€§æƒ…å†µå’Œéšæœºè®¾ç½®ä¸‹$L_1=0$çš„å·²çŸ¥ç•Œé™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ”¶æ•›é€Ÿç‡é¿å…äº†æŒ‡æ•°çº§çš„å¢å¤§å› å­ï¼Œå¹¶ä¸ä¾èµ–äºé™åˆ¶æ€§çš„æ¬¡é«˜æ–¯å™ªå£°å‡è®¾ï¼Œæ˜¾è‘—æ‹“å®½äº†æ¢¯åº¦è£å‰ªçš„é€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨é‡å°¾å™ªå£°å’Œ$(L_0,L_1)$-å…‰æ»‘æ€§å‡è®¾ä¸‹ï¼ŒClip-SGDæ–¹æ³•çš„é«˜æ¦‚ç‡æ”¶æ•›æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ¢è®¨è¿™ä¸€é¢†åŸŸï¼Œå¯¼è‡´ç†è®ºæ”¯æŒä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡å»ºç«‹é«˜æ¦‚ç‡æ”¶æ•›ç•Œé™ï¼Œå¡«è¡¥äº†é‡å°¾å™ªå£°å’Œ$(L_0,L_1)$-å…‰æ»‘æ€§å‡è®¾ä¸‹çš„ç†è®ºç©ºç™½ï¼Œæä¾›äº†æ›´å¼ºçš„æ”¶æ•›ä¿è¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹Clip-SGDæ–¹æ³•çš„åˆ†æï¼Œåˆ†ä¸ºç†è®ºæ¨å¯¼å’Œå®éªŒéªŒè¯ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚ç†è®ºæ¨å¯¼éƒ¨åˆ†é‡ç‚¹åœ¨äºæ”¶æ•›ç•Œé™çš„å»ºç«‹ï¼Œå®éªŒéªŒè¯åˆ™é€šè¿‡å¯¹æ¯”ä¸åŒå™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé¦–æ¬¡ä¸ºClip-SGDæ–¹æ³•åœ¨é‡å°¾å™ªå£°å’Œ$(L_0,L_1)$-å…‰æ»‘æ€§å‡è®¾ä¸‹æä¾›äº†é«˜æ¦‚ç‡æ”¶æ•›ç•Œé™ï¼Œé¿å…äº†æŒ‡æ•°çº§å¢å¤§å› å­çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè®ºæ–‡æ²¡æœ‰ä¾èµ–äºé™åˆ¶æ€§çš„æ¬¡é«˜æ–¯å™ªå£°å‡è®¾ï¼Œç¡®ä¿äº†æ–¹æ³•åœ¨æ›´å¹¿æ³›åœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒClip-SGDæ–¹æ³•åœ¨é‡å°¾å™ªå£°æ¡ä»¶ä¸‹çš„æ”¶æ•›é€Ÿåº¦æ˜¾è‘—ä¼˜äºä¼ ç»ŸSGDæ–¹æ³•ï¼Œå°¤å…¶åœ¨$(L_0,L_1)$-å…‰æ»‘æ€§å‡è®¾ä¸‹ï¼Œæ”¶æ•›ç•Œé™çš„æå‡å¹…åº¦è¾¾åˆ°äº†ç†è®ºé¢„æœŸçš„æ°´å¹³ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•çš„æ”¹è¿›ç­‰ã€‚é€šè¿‡æä¾›æ›´å¼ºçš„æ”¶æ•›ä¿è¯ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿæå‡æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Gradient clipping is a widely used technique in Machine Learning and Deep Learning (DL), known for its effectiveness in mitigating the impact of heavy-tailed noise, which frequently arises in the training of large language models. Additionally, first-order methods with clipping, such as Clip-SGD, exhibit stronger convergence guarantees than SGD under the $(L_0,L_1)$-smoothness assumption, a property observed in many DL tasks. However, the high-probability convergence of Clip-SGD under both assumptions -- heavy-tailed noise and $(L_0,L_1)$-smoothness -- has not been fully addressed in the literature. In this paper, we bridge this critical gap by establishing the first high-probability convergence bounds for Clip-SGD applied to convex $(L_0,L_1)$-smooth optimization with heavy-tailed noise. Our analysis extends prior results by recovering known bounds for the deterministic case and the stochastic setting with $L_1 = 0$ as special cases. Notably, our rates avoid exponentially large factors and do not rely on restrictive sub-Gaussian noise assumptions, significantly broadening the applicability of gradient clipping.

