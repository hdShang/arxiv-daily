---
layout: default
title: Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals
---

# Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21750" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21750v1</a>
  <a href="https://arxiv.org/pdf/2505.21750.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21750v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21750v1', 'Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vivienne Huiling Wang, Tinghuai Wang, Joni Pajarinen

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸ç¡®å®šæ€§å¼•å¯¼çš„æ‰©æ•£å­ç›®æ ‡ä»¥è§£å†³å±‚æ¬¡å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å±‚æ¬¡å¼ºåŒ–å­¦ä¹ ` `ä¸ç¡®å®šæ€§é‡åŒ–` `æ‰©æ•£æ¨¡å‹` `é«˜æ–¯è¿‡ç¨‹` `å­ç›®æ ‡ç”Ÿæˆ` `è¿ç»­æ§åˆ¶` `å†³ç­–ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å±‚æ¬¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä½çº§ç­–ç•¥å˜åŒ–æ—¶ï¼Œé«˜çº§ç­–ç•¥éš¾ä»¥ç”Ÿæˆæœ‰æ•ˆçš„å­ç›®æ ‡ï¼Œå¯¼è‡´å†³ç­–æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œé«˜æ–¯è¿‡ç¨‹çš„æ¡†æ¶ï¼Œä»¥ç”Ÿæˆå¤æ‚çš„å­ç›®æ ‡å¹¶é‡åŒ–ä¸ç¡®å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„å±‚æ¬¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å±‚æ¬¡å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰åœ¨å¤šä¸ªæ—¶é—´æŠ½è±¡å±‚æ¬¡ä¸Šè¿›è¡Œå†³ç­–å­¦ä¹ ã€‚HRLçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ä½çº§ç­–ç•¥éšæ—¶é—´å˜åŒ–ï¼Œå¯¼è‡´é«˜çº§ç­–ç•¥éš¾ä»¥ç”Ÿæˆæœ‰æ•ˆçš„å­ç›®æ ‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé«˜çº§ç­–ç•¥å¿…é¡»æ•æ‰å¤æ‚çš„å­ç›®æ ‡åˆ†å¸ƒï¼ŒåŒæ—¶è€ƒè™‘å…¶ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹å¹¶ç»“åˆé«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰å…ˆéªŒï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„å­ç›®æ ‡ï¼Œå¹¶åˆ©ç”¨GPçš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä»æ‰©æ•£ç­–ç•¥å’ŒGPçš„é¢„æµ‹å‡å€¼ä¸­é€‰æ‹©å­ç›®æ ‡çš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨æ ·æœ¬æ•ˆç‡å’Œåœ¨æŒ‘æˆ˜æ€§è¿ç»­æ§åˆ¶åŸºå‡†ä¸Šçš„è¡¨ç°ä¸Šè¶…è¶Šäº†ä»¥å¾€çš„HRLæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å±‚æ¬¡å¼ºåŒ–å­¦ä¹ ä¸­ä½çº§ç­–ç•¥å˜åŒ–å¯¼è‡´çš„é«˜çº§ç­–ç•¥ç”Ÿæˆæœ‰æ•ˆå­ç›®æ ‡çš„å›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹ä¸ç¡®å®šæ€§å’Œå¤æ‚å­ç›®æ ‡åˆ†å¸ƒæ—¶å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„å­ç›®æ ‡ï¼Œå¹¶ç»“åˆé«˜æ–¯è¿‡ç¨‹è¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä»¥æé«˜å­ç›®æ ‡çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ¡ä»¶æ‰©æ•£æ¨¡å‹ç”¨äºç”Ÿæˆå­ç›®æ ‡ï¼ŒGPç”¨äºæä¾›ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚é«˜å±‚ç­–ç•¥ä»è¿™ä¸¤ä¸ªæ¨¡å—ä¸­é€‰æ‹©æœ€ä¼˜å­ç›®æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†é«˜æ–¯è¿‡ç¨‹çš„ä¸ç¡®å®šæ€§é‡åŒ–ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä½¿å¾—ç”Ÿæˆçš„å­ç›®æ ‡ä¸ä»…å¤šæ ·åŒ–ä¸”å…·æœ‰è¾ƒé«˜çš„å¯é æ€§ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†ç­–ç•¥çš„é€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥å¹³è¡¡ç”Ÿæˆå­ç›®æ ‡çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç¡®ä¿äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡è°ƒèŠ‚é«˜æ–¯è¿‡ç¨‹çš„è¶…å‚æ•°ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†ä¸ç¡®å®šæ€§ä¼°è®¡çš„ç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªè¿ç»­æ§åˆ¶åŸºå‡†ä¸Šå‡ä¼˜äºä¼ ç»ŸHRLæ–¹æ³•ï¼Œæ ·æœ¬æ•ˆç‡æå‡äº†çº¦30%ï¼Œåœ¨ä»»åŠ¡å®Œæˆç‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æé«˜ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰éœ€è¦é«˜æ•ˆå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡ç”Ÿæˆæ›´ä¸ºæœ‰æ•ˆçš„å­ç›®æ ‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hierarchical reinforcement learning (HRL) learns to make decisions on multiple levels of temporal abstraction. A key challenge in HRL is that the low-level policy changes over time, making it difficult for the high-level policy to generate effective subgoals. To address this issue, the high-level policy must capture a complex subgoal distribution while also accounting for uncertainty in its estimates. We propose an approach that trains a conditional diffusion model regularized by a Gaussian Process (GP) prior to generate a complex variety of subgoals while leveraging principled GP uncertainty quantification. Building on this framework, we develop a strategy that selects subgoals from both the diffusion policy and GP's predictive mean. Our approach outperforms prior HRL methods in both sample efficiency and performance on challenging continuous control benchmarks.

