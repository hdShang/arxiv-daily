---
layout: default
title: Deep Reinforcement Learning Agents are not even close to Human Intelligence
---

# Deep Reinforcement Learning Agents are not even close to Human Intelligence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21731" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21731v1</a>
  <a href="https://arxiv.org/pdf/2505.21731.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21731v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21731v1', 'Deep Reinforcement Learning Agents are not even close to Human Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Quentin Delfosse, Jannis BlÃ¼ml, Fabian Tatai, ThÃ©o Vincent, Bjarne Gregori, Elisabeth Dillies, Jan Peters, Constantin Rothkopf, Kristian Kersting

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: 49 pages in total, 5 main figures, 14 figures total

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHackAtariä»¥è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `é›¶-shot é€‚åº”` `ä»»åŠ¡ç®€åŒ–` `HackAtari` `æ™ºèƒ½è¯„ä¼°` `äººç±»æ™ºèƒ½` `æ€§èƒ½ä¸‹é™`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨ä»»åŠ¡ç®€åŒ–æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œç¼ºä¹äººç±»çš„é›¶-shot é€‚åº”èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºHackAtariï¼Œé€šè¿‡ä¸€ç³»åˆ—ä»»åŠ¡å˜ä½“è¯„ä¼°RLä»£ç†åœ¨ç®€åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºå…¶å¯¹æ·å¾„çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLä»£ç†åœ¨ç®€åŒ–ä»»åŠ¡ä¸Šç³»ç»Ÿæ€§åœ°è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†ä¸äººç±»æ™ºèƒ½çš„å·®è·ï¼Œå‘¼åæ–°çš„è¯„ä¼°æ ‡å‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†åœ¨å¤šç§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ç¼ºä¹é›¶-shot é€‚åº”èƒ½åŠ›ã€‚ç°æœ‰çš„é²æ£’æ€§è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨ä»»åŠ¡å¤æ‚åŒ–ä¸Šï¼Œè€Œå¯¹ä»»åŠ¡ç®€åŒ–çš„è¯„ä¼°å°šæœªè¿›è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†HackAtariï¼Œä¸€ä¸ªåŸºäºè¡—æœºå­¦ä¹ ç¯å¢ƒçš„ä»»åŠ¡å˜ä½“é›†åˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸äººç±»ä¸åŒï¼ŒRLä»£ç†åœ¨è®­ç»ƒä»»åŠ¡çš„ç®€åŒ–ç‰ˆæœ¬ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œæ­ç¤ºäº†ä»£ç†å¯¹æ·å¾„çš„ä¾èµ–ã€‚é€šè¿‡å¯¹å¤šç§ç®—æ³•å’Œæ¶æ„çš„åˆ†æï¼Œå¼ºè°ƒäº†RLä»£ç†ä¸äººç±»è¡Œä¸ºæ™ºèƒ½ä¹‹é—´çš„æŒç»­å·®è·ï¼Œå‘¼åå»ºç«‹æ–°çš„åŸºå‡†å’Œæ–¹æ³•ï¼Œä»¥å¼ºåˆ¶è¿›è¡Œç³»ç»Ÿæ€§çš„æ³›åŒ–æµ‹è¯•ï¼Œè€Œä¸ä»…ä»…ä¾èµ–é™æ€è¯„ä¼°åè®®ã€‚ä»…åœ¨åŒä¸€ç¯å¢ƒä¸­è®­ç»ƒå’Œæµ‹è¯•ä¸è¶³ä»¥åŸ¹å…»å…·æœ‰äººç±»æ™ºèƒ½çš„ä»£ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨ä»»åŠ¡ç®€åŒ–æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»»åŠ¡å¤æ‚åŒ–ï¼Œæœªèƒ½è¯„ä¼°ä»£ç†åœ¨ç®€åŒ–ä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥HackAtariï¼Œç ”ç©¶è€…èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¯„ä¼°RLä»£ç†åœ¨ç®€åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºå…¶å¯¹æ·å¾„çš„ä¾èµ–ï¼Œè¿›è€Œå¼ºè°ƒä¸äººç±»æ™ºèƒ½çš„å·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHackAtariæ˜¯åŸºäºè¡—æœºå­¦ä¹ ç¯å¢ƒçš„ä»»åŠ¡å˜ä½“é›†åˆï¼ŒåŒ…å«å¤šç§ç®€åŒ–ä»»åŠ¡ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”ä¸åŒç®—æ³•å’Œæ¶æ„çš„è¡¨ç°ï¼Œåˆ†æå…¶åœ¨ç®€åŒ–ä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†HackAtariè¿™ä¸€æ–°è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†RLä»£ç†åœ¨ä»»åŠ¡ç®€åŒ–æ—¶çš„æ€§èƒ½ä¸‹é™ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ–°çš„è§†è§’æ¥ç†è§£ä»£ç†çš„æ™ºèƒ½å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†å¤šç§RLç®—æ³•å’Œç½‘ç»œæ¶æ„ï¼Œè¯„ä¼°å…¶åœ¨HackAtariä»»åŠ¡é›†ä¸Šçš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨æ€§èƒ½ä¸‹é™çš„å¹…åº¦å’ŒåŸå› ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLä»£ç†åœ¨HackAtariä»»åŠ¡é›†çš„ç®€åŒ–ç‰ˆæœ¬ä¸Šè¡¨ç°å‡ºé«˜è¾¾50%çš„æ€§èƒ½ä¸‹é™ï¼Œæ˜æ˜¾ä½äºäººç±»çš„é€‚åº”èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†ç°æœ‰RLæ–¹æ³•åœ¨æ™ºèƒ½æ³›åŒ–æ–¹é¢çš„ä¸è¶³ï¼Œå‘¼åæ–°çš„è¯„ä¼°å’Œè®­ç»ƒæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶è€…æ›´å¥½åœ°ç†è§£å’Œæå‡RLä»£ç†çš„æ™ºèƒ½æ°´å¹³ã€‚é€šè¿‡å»ºç«‹æ–°çš„è¯„ä¼°æ ‡å‡†ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å…·äººç±»æ™ºèƒ½ç‰¹å¾çš„AIç³»ç»Ÿçš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep reinforcement learning (RL) agents achieve impressive results in a wide variety of tasks, but they lack zero-shot adaptation capabilities. While most robustness evaluations focus on tasks complexifications, for which human also struggle to maintain performances, no evaluation has been performed on tasks simplifications. To tackle this issue, we introduce HackAtari, a set of task variations of the Arcade Learning Environments. We use it to demonstrate that, contrary to humans, RL agents systematically exhibit huge performance drops on simpler versions of their training tasks, uncovering agents' consistent reliance on shortcuts. Our analysis across multiple algorithms and architectures highlights the persistent gap between RL agents and human behavioral intelligence, underscoring the need for new benchmarks and methodologies that enforce systematic generalization testing beyond static evaluation protocols. Training and testing in the same environment is not enough to obtain agents equipped with human-like intelligence.

