---
layout: default
title: Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders
---

# Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21364" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21364v2</a>
  <a href="https://arxiv.org/pdf/2505.21364.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21364v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21364v2', 'Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: James Oldfield, Shawn Im, Sharon Li, Mihalis A. Nicolaou, Ioannis Patras, Grigorios G Chrysos

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-10-22)

**å¤‡æ³¨**: Accepted at NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/james-oldfield/MxD/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆè§£ç å™¨ä»¥è§£å†³å¤šå±‚æ„ŸçŸ¥æœºå¯è§£é‡Šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šå±‚æ„ŸçŸ¥æœº` `å¯è§£é‡Šæ€§` `æ··åˆè§£ç å™¨` `ç¨€ç–æ€§` `è‡ªç„¶è¯­è¨€å¤„ç†` `æœºå™¨å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å®ç°å¯è§£é‡Šæ€§æ—¶æœªèƒ½å¿ å®é‡å»ºåŸå§‹æ˜ å°„ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å±‚çº§ç¨€ç–æ€§æ¥è§£å†³å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå¼•å…¥æ··åˆè§£ç å™¨ï¼ˆMxDsï¼‰ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMxDsåœ¨ç¨€ç–æ€§-å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å‚æ•°é‡è¾¾åˆ°30äº¿çš„è¯­è¨€æ¨¡å‹ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†å…¶å¯†é›†è¡¨ç¤ºä½¿å¾—ç†è§£ã€ç¼–è¾‘å’Œå¼•å¯¼å˜å¾—å›°éš¾ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ç¥ç»å…ƒçº§ç¨€ç–æ€§å­¦ä¹ å¯è§£é‡Šçš„è¿‘ä¼¼ï¼Œä½†æœªèƒ½å¿ å®é‡å»ºåŸå§‹æ˜ å°„ï¼Œæ˜¾è‘—å¢åŠ æ¨¡å‹çš„ä¸‹ä¸€ä¸ªæ ‡è®°äº¤å‰ç†µæŸå¤±ã€‚æœ¬æ–‡å€¡å¯¼è½¬å‘å±‚çº§ç¨€ç–æ€§ï¼Œä»¥å…‹æœç¨€ç–å±‚è¿‘ä¼¼ä¸­çš„å‡†ç¡®æ€§æƒè¡¡ã€‚æˆ‘ä»¬å¼•å…¥æ··åˆè§£ç å™¨ï¼ˆMxDsï¼‰ï¼Œå®ƒé€šè¿‡çµæ´»çš„å¼ é‡åˆ†è§£ï¼Œå°†é¢„è®­ç»ƒçš„å¯†é›†å±‚æ‰©å±•ä¸ºæ•°ä¸‡ä¸ªä¸“ç”¨å­å±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒMxDsåœ¨è¯­è¨€æ¨¡å‹çš„ç¨€ç–æ€§-å‡†ç¡®æ€§è¾¹ç•Œä¸Šæ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†è®¾è®¡å¯è§£é‡Šä¸”å¿ å®åˆ†è§£çš„æ–°å‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰åœ¨å¯è§£é‡Šæ€§æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•é€šè¿‡ç¥ç»å…ƒçº§ç¨€ç–æ€§æœªèƒ½å¿ å®é‡å»ºåŸå§‹æ˜ å°„ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å±‚çº§ç¨€ç–æ€§æ¥å…‹æœç¨€ç–å±‚è¿‘ä¼¼ä¸­çš„å‡†ç¡®æ€§æƒè¡¡ï¼Œå¼•å…¥æ··åˆè§£ç å™¨ï¼ˆMxDsï¼‰ï¼Œå°†å¯†é›†å±‚æ‰©å±•ä¸ºå¤šä¸ªä¸“ç”¨å­å±‚ï¼Œä»¥ä¿æŒè¡¨è¾¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMxDsé€šè¿‡çµæ´»çš„å¼ é‡åˆ†è§£å®ç°ï¼Œæ¯ä¸ªç¨€ç–æ¿€æ´»çš„MxDå­å±‚æ‰§è¡Œå…¨ç§©æƒé‡çš„çº¿æ€§å˜æ¢ï¼Œæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªå­å±‚å’Œè§£ç å™¨çš„ç»„åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šMxDsçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨é«˜ç¨€ç–æ€§ä¸‹ä¿æŒåŸå§‹è§£ç å™¨çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šMxDsçš„è®¾è®¡åŒ…æ‹¬å¯¹ç¨€ç–æ€§è¿›è¡Œä¼˜åŒ–çš„å‚æ•°è®¾ç½®ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œä»¥åŠç½‘ç»œç»“æ„çš„çµæ´»æ€§ï¼Œä»¥ç¡®ä¿åœ¨ç¨€ç–æ¿€æ´»ä¸‹ä»èƒ½å®ç°æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMxDsåœ¨ç¨€ç–æ€§-å‡†ç¡®æ€§è¾¹ç•Œä¸Šæ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¦‚Transcodersï¼Œå°¤å…¶åœ¨å‚æ•°é‡è¾¾åˆ°30äº¿çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œæå‡å¹…åº¦æ˜æ˜¾ï¼Œå±•ç¤ºäº†å…¶åœ¨å¯è§£é‡Šæ€§ä¸æ€§èƒ½ä¹‹é—´çš„ä¼˜è¶Šå¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æä¾›å¯è§£é‡Šä¸”å¿ å®çš„æ¨¡å‹åˆ†è§£ï¼ŒMxDsæœ‰åŠ©äºæé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯æ§æ€§ï¼Œè¿›è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„ä¿¡ä»»åº¦å’Œæ¥å—åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: https://github.com/james-oldfield/MxD/.

