---
layout: default
title: Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization
---

# Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20881" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20881v1</a>
  <a href="https://arxiv.org/pdf/2505.20881.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20881v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20881v1', 'Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiding Shi, Jianan Zhou, Wen Song, Jieyi Bi, Yaoxin Wu, Jie Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMeta-Optimizationæ¡†æ¶ä»¥è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»„åˆä¼˜åŒ–` `å¯å‘å¼ç®—æ³•` `å…ƒå­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¿›åŒ–è®¡ç®—` `å¤šä»»åŠ¡å­¦ä¹ ` `ä¼˜åŒ–å™¨è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–æ‰‹åŠ¨å®šä¹‰çš„ä¼˜åŒ–å™¨å’Œå•ä»»åŠ¡è®­ç»ƒï¼Œé™åˆ¶äº†å¯å‘å¼ç®—æ³•çš„å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. æå‡ºçš„Meta-Optimizationæ¡†æ¶é€šè¿‡å…ƒå­¦ä¹ åŸç†ï¼Œåˆ©ç”¨LLMsè‡ªä¸»æ„å»ºå¤šæ ·åŒ–çš„ä¼˜åŒ–å™¨ï¼Œæ¶ˆé™¤äº†å¯¹é¢„å®šä¹‰ä¼˜åŒ–å™¨çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoHåœ¨ç»å…¸ç»„åˆä¼˜åŒ–é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨è·¨è§„æ¨¡ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¯å‘å¼è®¾è®¡å·²æˆä¸ºè§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPsï¼‰çš„æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨é¢„å®šä¹‰çš„è¿›åŒ–è®¡ç®—ä¼˜åŒ–å™¨å’Œå•ä»»åŠ¡è®­ç»ƒæ–¹æ¡ˆï¼Œè¿™é™åˆ¶äº†å¤šæ ·åŒ–å¯å‘å¼ç®—æ³•çš„æ¢ç´¢å¹¶é˜»ç¢äº†ç»“æœå¯å‘å¼çš„æ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯å‘å¼çš„å…ƒä¼˜åŒ–ï¼ˆMoHï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¼˜åŒ–å™¨å±‚é¢ä¸Šè¿ä½œï¼Œé€šè¿‡å…ƒå­¦ä¹ åŸç†å‘ç°æœ‰æ•ˆçš„ä¼˜åŒ–å™¨ã€‚å…·ä½“è€Œè¨€ï¼ŒMoHåˆ©ç”¨LLMsè¿­ä»£ä¼˜åŒ–ä¸€ä¸ªå…ƒä¼˜åŒ–å™¨ï¼Œè¯¥ä¼˜åŒ–å™¨é€šè¿‡ï¼ˆè‡ªæˆ‘ï¼‰è°ƒç”¨è‡ªä¸»æ„å»ºå¤šæ ·åŒ–çš„ä¼˜åŒ–å™¨ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹é¢„å®šä¹‰è¿›åŒ–è®¡ç®—ä¼˜åŒ–å™¨çš„ä¾èµ–ã€‚è¿™äº›æ„å»ºçš„ä¼˜åŒ–å™¨éšåä¸ºä¸‹æ¸¸ä»»åŠ¡æ¼”åŒ–å¯å‘å¼ï¼Œä¿ƒè¿›äº†æ›´å¹¿æ³›çš„å¯å‘å¼æ¢ç´¢ã€‚æ­¤å¤–ï¼ŒMoHé‡‡ç”¨å¤šä»»åŠ¡è®­ç»ƒæ–¹æ¡ˆä»¥æå‡å…¶æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ç»å…¸COPsä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMoHæ„å»ºäº†ä¸€ä¸ªæœ‰æ•ˆä¸”å¯è§£é‡Šçš„å…ƒä¼˜åŒ–å™¨ï¼Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è§„æ¨¡è®¾ç½®ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPsï¼‰ä¸­çš„å¯å‘å¼ç®—æ³•è®¾è®¡æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºä¾èµ–äºæ‰‹åŠ¨å®šä¹‰çš„è¿›åŒ–è®¡ç®—ä¼˜åŒ–å™¨å’Œå•ä¸€ä»»åŠ¡è®­ç»ƒï¼Œé™åˆ¶äº†ç®—æ³•çš„å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Meta-Optimizationæ¡†æ¶ï¼ˆMoHï¼‰é€šè¿‡å…ƒå­¦ä¹ çš„æ–¹å¼ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿­ä»£ä¼˜åŒ–ä¸€ä¸ªå…ƒä¼˜åŒ–å™¨ï¼Œèƒ½å¤Ÿè‡ªä¸»æ„å»ºå¤šæ ·åŒ–çš„ä¼˜åŒ–å™¨ï¼Œä»è€Œå®ç°æ›´å¹¿æ³›çš„å¯å‘å¼æ¢ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoHçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å…ƒä¼˜åŒ–å™¨çš„æ„å»ºã€ä¼˜åŒ–å™¨çš„å¤šæ ·åŒ–ç”Ÿæˆä»¥åŠä¸‹æ¸¸ä»»åŠ¡çš„å¯å‘å¼æ¼”åŒ–ã€‚é¦–å…ˆï¼Œåˆ©ç”¨LLMsç”Ÿæˆåˆå§‹ä¼˜åŒ–å™¨ï¼Œç„¶åé€šè¿‡è‡ªæˆ‘è°ƒç”¨ä¸æ–­ä¼˜åŒ–å’Œè°ƒæ•´è¿™äº›ä¼˜åŒ–å™¨ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoHçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åœ¨ä¼˜åŒ–å™¨å±‚é¢è¿›è¡Œæ“ä½œï¼Œæ‰“ç ´äº†ä¼ ç»Ÿæ–¹æ³•å¯¹æ‰‹åŠ¨å®šä¹‰ä¼˜åŒ–å™¨çš„ä¾èµ–ï¼Œä½¿å¾—å¯å‘å¼ç®—æ³•çš„ç”Ÿæˆæ›´åŠ çµæ´»å’Œå¤šæ ·åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒMoHé‡‡ç”¨äº†å¤šä»»åŠ¡è®­ç»ƒæ–¹æ¡ˆï¼Œç¡®ä¿å…ƒä¼˜åŒ–å™¨èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡é—´æœ‰æ•ˆæ³›åŒ–ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ”¯æŒä¼˜åŒ–å™¨çš„è‡ªæˆ‘è°ƒæ•´å’Œæ¼”åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoHåœ¨ç»å…¸ç»„åˆä¼˜åŒ–é—®é¢˜ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è·¨è§„æ¨¡è®¾ç½®ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è°ƒåº¦é—®é¢˜ã€èµ„æºåˆ†é…ã€ç½‘ç»œä¼˜åŒ–ç­‰ç»„åˆä¼˜åŒ–ä»»åŠ¡ã€‚é€šè¿‡æä¾›ä¸€ç§çµæ´»çš„å¯å‘å¼ç®—æ³•ç”Ÿæˆæ–¹æ³•ï¼ŒMoHèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æå‡ä¼˜åŒ–æ•ˆç‡ï¼Œé™ä½äººå·¥å¹²é¢„éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Heuristic design with large language models (LLMs) has emerged as a promising approach for tackling combinatorial optimization problems (COPs). However, existing approaches often rely on manually predefined evolutionary computation (EC) optimizers and single-task training schemes, which may constrain the exploration of diverse heuristic algorithms and hinder the generalization of the resulting heuristics. To address these issues, we propose Meta-Optimization of Heuristics (MoH), a novel framework that operates at the optimizer level, discovering effective optimizers through the principle of meta-learning. Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that autonomously constructs diverse optimizers through (self-)invocation, thereby eliminating the reliance on a predefined EC optimizer. These constructed optimizers subsequently evolve heuristics for downstream tasks, enabling broader heuristic exploration. Moreover, MoH employs a multi-task training scheme to promote its generalization capability. Experiments on classic COPs demonstrate that MoH constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, particularly in cross-size settings.

