---
layout: default
title: TuneComp: Joint Fine-tuning and Compression for Large Foundation Models
---

# TuneComp: Joint Fine-tuning and Compression for Large Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21835" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21835v1</a>
  <a href="https://arxiv.org/pdf/2505.21835.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21835v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21835v1', 'TuneComp: Joint Fine-tuning and Compression for Large Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiangyu Chen, Jing Liu, Ye Wang, Matthew Brand, Pu, Wang, Toshiaki Koike-Akino

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Preliminary Work

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTuneCompä»¥è§£å†³å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹çš„è”åˆå¾®è°ƒä¸å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹å‹ç¼©` `è”åˆå¾®è°ƒ` `çŸ¥è¯†è’¸é¦` `ä½ç§©è¿‘ä¼¼` `å‰ªææŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡å‹å‹ç¼©æ–¹æ³•åœ¨å¾®è°ƒåè¿›è¡Œï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œä¸å¿…è¦çš„æ¨¡å‹å¢å¤§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è”åˆå¾®è°ƒä¸å‹ç¼©çš„æ–¹æ³•ï¼Œç›´æ¥åœ¨ä¸‹æ¸¸ä»»åŠ¡æŒ‡å¯¼ä¸‹æ„å»ºæ›´å°çš„æ¨¡å‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè”åˆå¾®è°ƒå’Œå‹ç¼©æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é¡ºåºå‹ç¼©æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†åœ¨åæœŸè®­ç»ƒä¸­å‡å°‘æ¨¡å‹å¤§å°ï¼Œé€šå¸¸åœ¨å¾®è°ƒæ¨¡å‹ååº”ç”¨å‹ç¼©æ–¹æ³•ï¼Œå¦‚çŸ¥è¯†è’¸é¦ã€ä½ç§©è¿‘ä¼¼å’Œå‰ªæã€‚ç„¶è€Œï¼Œé¡ºåºçš„å¾®è°ƒå’Œå‹ç¼©ä¼šç‰ºç‰²æ€§èƒ½ï¼Œå¹¶åœ¨ä¸­é—´æ­¥éª¤ä¸­åˆ›å»ºä¸€ä¸ªä¸å¿…è¦çš„è¾ƒå¤§æ¨¡å‹ã€‚æœ¬æ–‡æ—¨åœ¨ç¼©å°è¿™ä¸€å·®è·ï¼Œé€šè¿‡åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„æŒ‡å¯¼ä¸‹ç›´æ¥æ„å»ºä¸€ä¸ªæ›´å°çš„æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆå¾®è°ƒå’Œå‹ç¼©çš„æ–¹æ³•ï¼Œé€šè¿‡é€æ­¥è’¸é¦åˆ°å‰ªæçš„ä½ç§©ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œè”åˆå¾®è°ƒå’Œå‹ç¼©æ˜¾è‘—ä¼˜äºå…¶ä»–é¡ºåºå‹ç¼©æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°å‹ç¼©å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨å¾®è°ƒåè¿›è¡Œå‹ç¼©ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™å’Œä¸å¿…è¦çš„æ¨¡å‹å¢å¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è”åˆå¾®è°ƒå’Œå‹ç¼©ï¼Œç›´æ¥åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„æŒ‡å¯¼ä¸‹æ„å»ºä¸€ä¸ªæ›´å°çš„æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•é€šè¿‡é€æ­¥è’¸é¦æ¨¡å‹åˆ°å‰ªæçš„ä½ç§©ç»“æ„ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯å¾®è°ƒé˜¶æ®µï¼Œåœ¨æ­¤é˜¶æ®µæ¨¡å‹æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼›å…¶æ¬¡æ˜¯å‹ç¼©é˜¶æ®µï¼Œé€šè¿‡é€æ­¥è’¸é¦å’Œå‰ªææ¥å‡å°‘æ¨¡å‹çš„å¤æ‚æ€§å’Œå¤§å°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¾®è°ƒä¸å‹ç¼©è¿‡ç¨‹ç»“åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„ä¼˜åŒ–æ¡†æ¶ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œåè€…é€šå¸¸æ˜¯åˆ†å¼€è¿›è¡Œçš„ï¼Œå¯¼è‡´æ€§èƒ½æŸå¤±ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼Œè®ºæ–‡æå‡ºäº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å¾®è°ƒå’Œå‹ç¼©çš„ç›®æ ‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šé‡‡ç”¨äº†ä½ç§©è¿‘ä¼¼å’Œå‰ªææŠ€æœ¯ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å‹ç¼©åçš„æ€§èƒ½ä»ç„¶ä¼˜è¶Šã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTuneCompåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é¡ºåºå‹ç¼©æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨¡å‹å‹ç¼©é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰éœ€è¦å¤§è§„æ¨¡æ¨¡å‹çš„ä»»åŠ¡ã€‚é€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©ï¼ŒTuneCompèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²é«˜æ€§èƒ½æ¨¡å‹ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To reduce model size during post-training, compression methods, including knowledge distillation, low-rank approximation, and pruning, are often applied after fine-tuning the model. However, sequential fine-tuning and compression sacrifices performance, while creating a larger than necessary model as an intermediate step. In this work, we aim to reduce this gap, by directly constructing a smaller model while guided by the downstream task. We propose to jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure. Experiments demonstrate that joint fine-tuning and compression significantly outperforms other sequential compression methods.

