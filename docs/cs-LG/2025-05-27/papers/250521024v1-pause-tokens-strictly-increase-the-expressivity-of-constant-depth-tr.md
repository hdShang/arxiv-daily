---
layout: default
title: Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers
---

# Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21024" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21024v1</a>
  <a href="https://arxiv.org/pdf/2505.21024.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21024v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21024v1', 'Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Charles London, Varun Kanade

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¼•å…¥æš‚åœç¬¦å·ä»¥æå‡å¸¸æ·±åº¦å˜æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å˜æ¢å™¨` `æš‚åœç¬¦å·` `è¡¨è¾¾èƒ½åŠ›` `æ·±åº¦å­¦ä¹ ` `ç†è®ºåˆ†æ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•°å­¦æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å˜æ¢å™¨åœ¨å¤„ç†æŸäº›ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹æœ‰æ•ˆçš„ç¬¦å·æ¥å¢å¼ºå…¶è¡¨è¾¾èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å¼•å…¥æš‚åœç¬¦å·æ¥æå‡å¸¸æ·±åº¦å˜æ¢å™¨çš„è®¡ç®—è¡¨è¾¾èƒ½åŠ›ï¼Œä»è€Œè§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ æš‚åœç¬¦å·åï¼Œå˜æ¢å™¨èƒ½å¤Ÿå­¦ä¹ åˆ°ä¹‹å‰æ— æ³•å­¦ä¹ çš„å‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æš‚åœç¬¦å·ï¼ˆå¦‚"..."ï¼‰åœ¨è¯­è¨€å’Œæ•°å­¦ä»»åŠ¡ä¸­æŒç»­æå‡å˜æ¢å™¨çš„æ€§èƒ½ï¼Œä½†å…¶ç†è®ºæ•ˆæœå°šæœªè§£é‡Šã€‚æœ¬æ–‡é¦–æ¬¡æä¾›äº†å½¢å¼ä¸Šçš„åˆ†ç¦»ç»“æœï¼Œè¯æ˜åœ¨å¸¸æ·±åº¦ã€å¯¹æ•°å®½åº¦çš„å˜æ¢å™¨ä¸­ï¼Œæ·»åŠ æš‚åœç¬¦å·ä¸¥æ ¼å¢åŠ äº†å…¶è®¡ç®—è¡¨è¾¾èƒ½åŠ›ã€‚å¯¹äºæœ‰ç•Œç²¾åº¦çš„æ¿€æ´»å‡½æ•°ï¼Œæœªæ·»åŠ æš‚åœç¬¦å·çš„å˜æ¢å™¨åªèƒ½è®¡ç®—$	ext{AC}^0$å‡½æ•°çš„ä¸¥æ ¼å­é›†ï¼Œè€Œæ·»åŠ å¤šé¡¹å¼æ•°é‡çš„æš‚åœç¬¦å·åˆ™ä½¿å…¶èƒ½å¤Ÿè¡¨è¾¾æ•´ä¸ªç±»ã€‚å¯¹äºå¯¹æ•°ç²¾åº¦çš„å˜æ¢å™¨ï¼Œæ·»åŠ æš‚åœç¬¦å·çš„è¡¨è¾¾èƒ½åŠ›è¾¾åˆ°$	ext{TC}^0$ï¼Œä¸å·²çŸ¥çš„ä¸Šç•Œç›¸åŒ¹é…ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒåŒå±‚å› æœæ©è”½å˜æ¢å™¨åœ¨æä¾›æš‚åœç¬¦å·æ—¶èƒ½å¤Ÿå­¦ä¹ å¥‡å¶æ€§ï¼Œè€Œåœ¨æ²¡æœ‰æš‚åœç¬¦å·æ—¶åˆ™æ— æ³•å­¦ä¹ ã€‚è¿™äº›ç»“æœä¸ºå…ˆå‰çš„å®è¯å‘ç°æä¾›äº†ä¸¥æ ¼çš„ç†è®ºè§£é‡Šï¼Œå¹¶é˜æ˜äº†æš‚åœç¬¦å·å¦‚ä½•ä¸å®½åº¦ã€æ·±åº¦å’Œæ•°å€¼ç²¾åº¦ç›¸äº’ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¸¸æ·±åº¦å˜æ¢å™¨åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹æš‚åœç¬¦å·æ—¶å…¶è®¡ç®—èƒ½åŠ›çš„é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆè¡¨è¾¾æŸäº›å‡½æ•°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æš‚åœç¬¦å·æ¥å¢å¼ºå˜æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè¯æ˜å…¶åœ¨ç†è®ºä¸Šçš„æœ‰æ•ˆæ€§å’Œå®è¯ä¸Šçš„å¯è¡Œæ€§ã€‚é€šè¿‡æ·»åŠ å¤šé¡¹å¼æ•°é‡çš„æš‚åœç¬¦å·ï¼Œå˜æ¢å™¨èƒ½å¤Ÿè¡¨è¾¾æ›´å¹¿æ³›çš„å‡½æ•°ç±»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹å¸¸æ·±åº¦ã€å¯¹æ•°å®½åº¦å˜æ¢å™¨çš„ç†è®ºåˆ†æå’Œå®è¯éªŒè¯ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æš‚åœç¬¦å·çš„å¼•å…¥ã€è¡¨è¾¾èƒ½åŠ›çš„ç†è®ºè¯æ˜ä»¥åŠå®éªŒéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé¦–æ¬¡æä¾›äº†æš‚åœç¬¦å·å¯¹å¸¸æ·±åº¦å˜æ¢å™¨è¡¨è¾¾èƒ½åŠ›çš„ä¸¥æ ¼æå‡è¯æ˜ï¼Œæ˜ç¡®äº†å…¶ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹å˜æ¢å™¨çš„æ¿€æ´»å‡½æ•°è¿›è¡Œæœ‰ç•Œç²¾åº¦è®¾ç½®ï¼Œä»¥åŠåœ¨å®éªŒä¸­ä½¿ç”¨åŒå±‚å› æœæ©è”½ç»“æ„æ¥éªŒè¯æš‚åœç¬¦å·çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒå±‚å› æœæ©è”½å˜æ¢å™¨åœ¨æ·»åŠ æš‚åœç¬¦å·åèƒ½å¤ŸæˆåŠŸå­¦ä¹ å¥‡å¶æ€§å‡½æ•°ï¼Œè€Œåœ¨æ²¡æœ‰æš‚åœç¬¦å·çš„æƒ…å†µä¸‹åˆ™æ— æ³•å®ç°ã€‚è¿™ä¸€å‘ç°éªŒè¯äº†æš‚åœç¬¦å·åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ï¼Œæä¾›äº†ç†è®ºä¸å®è¯çš„åŒé‡æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ•°å­¦æ¨ç†å’Œå…¶ä»–éœ€è¦å¤æ‚æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ã€‚é€šè¿‡æå‡å˜æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç ”ç©¶æˆæœèƒ½å¤Ÿåœ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ¨ç†å’Œæ•™è‚²æŠ€æœ¯ç­‰é¢†åŸŸäº§ç”Ÿå®é™…ä»·å€¼ï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pause tokens, simple filler symbols such as "...", consistently improve Transformer performance on both language and mathematical tasks, yet their theoretical effect remains unexplained. We provide the first formal separation result, proving that adding pause tokens to constant-depth, logarithmic-width Transformers strictly increases their computational expressivity. With bounded-precision activations, Transformers without pause tokens compute only a strict subset of $\mathsf{AC}^0$ functions, while adding a polynomial number of pause tokens allows them to express the entire class. For logarithmic-precision Transformers, we show that adding pause tokens achieves expressivity equivalent to $\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate that two-layer causally masked Transformers can learn parity when supplied with pause tokens, a function that they appear unable to learn without them. Our results provide a rigorous theoretical explanation for prior empirical findings, clarify how pause tokens interact with width, depth, and numeric precision, and position them as a distinct mechanism, complementary to chain-of-thought prompting, for enhancing Transformer reasoning.

