---
layout: default
title: "FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration"
---

# FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20839" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20839v3</a>
  <a href="https://arxiv.org/pdf/2505.20839.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20839v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20839v3', 'FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daehyeon Baek, Jieun Choi, Jimyoung Son, Kyungmin Bin, Seungbeom Choi, Kihyo Moon, Minsung Jang, Hyojung Lee

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-07-18)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFireQä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†åŠ é€Ÿ` `åè®­ç»ƒé‡åŒ–` `INT4` `FP8` `å¼‚å¸¸å¹³æ»‘` `FlashAttention` `RoPE`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­é¢ä¸´å†…å­˜å¸¦å®½é™åˆ¶ï¼Œå¯¼è‡´ååé‡ä¸è¶³ã€‚
2. FireQé€šè¿‡å°†çº¿æ€§å±‚æƒé‡å’Œå…³é”®å€¼é‡åŒ–ä¸ºINT4ï¼Œæ¿€æ´»å€¼å’ŒæŸ¥è¯¢é‡åŒ–ä¸ºFP8ï¼Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚
3. åœ¨Llama2-7Bä¸Šï¼ŒFireQå®ç°äº†1.68å€çš„æ¨ç†åŠ é€Ÿï¼Œåœ¨Llama3-8Bçš„é¢„å¡«å……é˜¶æ®µæ€§èƒ½æå‡1.26å€ï¼Œä¸”ç²¾åº¦æŸå¤±æå°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œå†…å­˜å¸¦å®½é™åˆ¶æ˜¾è‘—å½±å“æ¨ç†ååé‡ï¼Œä¿ƒä½¿åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æŠ€æœ¯çš„å‘å±•ã€‚æœ¬æ–‡æå‡ºäº†FireQï¼Œä¸€ä¸ªå…±åŒè®¾è®¡çš„PTQæ¡†æ¶å’ŒINT4-FP8çŸ©é˜µä¹˜æ³•å†…æ ¸ï¼Œæ—¨åœ¨åŠ é€Ÿæ‰€æœ‰çº¿æ€§å±‚çš„LLMæ¨ç†ã€‚FireQå°†çº¿æ€§å±‚æƒé‡å’Œå…³é”®å€¼é‡åŒ–ä¸ºINT4ï¼Œå°†æ¿€æ´»å€¼å’ŒæŸ¥è¯¢é‡åŒ–ä¸ºFP8ï¼Œä»è€Œæ˜¾è‘—æé«˜ååé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰é˜¶æ®µçš„é¢„å¡«å……ç®¡é“ï¼Œä¿®æ”¹äº†FlashAttention-3å†…æ ¸ï¼Œæœ‰æ•ˆå‡å°‘äº†é¢„å¡«å……é˜¶æ®µçš„é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ã€‚ä¸ºæœ€å°åŒ–é‡åŒ–å¸¦æ¥çš„ç²¾åº¦æŸå¤±ï¼Œæˆ‘ä»¬å¼€å‘äº†é’ˆå¯¹çº¿æ€§å’Œæ³¨æ„åŠ›å±‚çš„ç‹¬ç‰¹å¼‚å¸¸å¹³æ»‘æŠ€æœ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„å†…å­˜å¸¦å®½é™åˆ¶é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¨ç†ååé‡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFireQé€šè¿‡å°†ä¸åŒå±‚çš„æƒé‡å’Œæ¿€æ´»å€¼é‡‡ç”¨ä¸åŒçš„é‡åŒ–æ ¼å¼ï¼ˆINT4å’ŒFP8ï¼‰ï¼Œä»¥æ­¤æ¥æé«˜æ¨ç†æ•ˆç‡å¹¶å‡å°‘å†…å­˜å ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFireQçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é‡åŒ–æ¨¡å—ã€ä¸‰é˜¶æ®µé¢„å¡«å……ç®¡é“å’Œå¼‚å¸¸å¹³æ»‘æŠ€æœ¯ï¼Œåˆ†åˆ«é’ˆå¯¹çº¿æ€§å±‚å’Œæ³¨æ„åŠ›å±‚è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šFireQçš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†INT4å’ŒFP8çš„é‡åŒ–ç­–ç•¥ï¼Œå¹¶é’ˆå¯¹RoPEçš„æŒ‘æˆ˜æå‡ºäº†é¢„åå¹³æ»‘ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†æ¯ä¸ªå¼ é‡çš„ç¼©æ”¾å› å­ä»¥é˜²æ­¢FP8é‡åŒ–å¼•èµ·çš„ä¸‹æº¢ï¼Œå¹¶åœ¨æ³¨æ„åŠ›å±‚ä¸­ç»“åˆäº†é¢„RoPEå’ŒåRoPEçš„ç¼©æ”¾ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FireQåœ¨Llama2-7Bä¸Šå®ç°äº†1.68å€çš„æ¨ç†åŠ é€Ÿï¼Œåœ¨Llama3-8Bçš„é¢„å¡«å……é˜¶æ®µæ€§èƒ½æå‡1.26å€ï¼Œç›¸è¾ƒäºQServeï¼Œä¸”ç²¾åº¦æŸå¤±å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FireQçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†åŠ é€Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜ååé‡å’Œä½å»¶è¿Ÿçš„åœºæ™¯ä¸­ï¼Œå¦‚å®æ—¶å¯¹è¯ç³»ç»Ÿã€æ™ºèƒ½å®¢æœå’Œåœ¨çº¿ç¿»è¯‘ç­‰é¢†åŸŸã€‚å…¶æŠ€æœ¯åˆ›æ–°å°†æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models become increasingly prevalent, memory bandwidth constraints significantly limit inference throughput, motivating post-training quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM inference across all linear layers. Specifically, FireQ quantizes linear layer weights and key-values to INT4, and activations and queries to FP8, significantly enhancing throughput. Additionally, we introduce a three-stage pipelining for the prefill phase, which modifies the FlashAttention-3 kernel, effectively reducing time-to-first-token in the prefill phase. To minimize accuracy loss from quantization, we develop novel outlier smoothing techniques tailored separately for linear and attention layers. In linear layers, we explicitly use per-tensor scaling to prevent underflow caused by the FP8 quantization scaling factor of INT4 quantization, and channel-wise scaling to compensate for coarse granularity of INT4. In attention layers, we address quantization challenges posed by rotary positional embeddings (RoPE) by combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly outperforms state-of-the-art methods, achieving 1.68x faster inference in feed-forward network layers on Llama2-7B and 1.26x faster prefill phase performance on Llama3-8B compared to QServe, with negligible accuracy loss.

