---
layout: default
title: DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models
---

# DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21382" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21382v1</a>
  <a href="https://arxiv.org/pdf/2505.21382.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21382v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21382v1', 'DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nastaran Saadati, Zhanhong Jiang, Joshua R. Waite, Shreyan Ganguly, Aditya Balu, Chinmay Hegde, Soumik Sarkar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeCAFä»¥è§£å†³å»ä¸­å¿ƒåŒ–LoRAçš„å…±è¯†å¹²æ‰°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å»ä¸­å¿ƒåŒ–å­¦ä¹ ` `ä½ç§©é€‚åº”` `è§†è§‰-è¯­è¨€æ¨¡å‹` `çŸ©é˜µåˆ†è§£` `è”é‚¦å­¦ä¹ ` `æœºå™¨å­¦ä¹ ` `ç®—æ³•ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å»ä¸­å¿ƒåŒ–LoRAåœ¨ç†è®ºåŸºç¡€å’Œæ”¶æ•›é€Ÿåº¦ä¸Šå­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¹³æ»‘æ€§ä¿è¯å’Œå…±è¯†å¹²æ‰°é—®é¢˜ã€‚
2. æå‡ºDeCAFç®—æ³•ï¼Œé€šè¿‡ç»“åˆDLoRAä¸TSVDçŸ©é˜µåˆ†è§£ï¼Œç¡®ä¿æ¢¯åº¦å¹³æ»‘æ€§å¹¶è§£å†³å…±è¯†å¹²æ‰°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeCAFåœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†æœ¬åœ°è®­ç»ƒå’Œè”é‚¦å­¦ä¹ çš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å·²æˆä¸ºè®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸€ç§æœ‰æ•ˆä¸”è®¡ç®—å¯è¡Œçš„å¾®è°ƒæ–¹æ³•ã€‚LoRAé€šè¿‡å†»ç»“é¢„è®­ç»ƒæ¨¡å‹æƒé‡å¹¶æ³¨å…¥å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µï¼Œå®ç°äº†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆå­¦ä¹ ã€‚ç„¶è€Œï¼Œå»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸‹çš„LoRAä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è®ºåŸºç¡€æ–¹é¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•DeCAFï¼Œå°†å»ä¸­å¿ƒåŒ–LoRAï¼ˆDLoRAï¼‰ä¸åŸºäºæˆªæ–­å¥‡å¼‚å€¼åˆ†è§£ï¼ˆTSVDï¼‰çš„çŸ©é˜µåˆ†è§£ç›¸ç»“åˆï¼Œä»¥è§£å†³å…±è¯†å¹²æ‰°é—®é¢˜ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒTSVDçš„è¿‘ä¼¼è¯¯å·®æ˜¯æœ‰ç•Œçš„ï¼ŒDLoRAä¸DeCAFä¹‹é—´çš„å…±è¯†å·®å¼‚éšç€ç§©çš„å¢åŠ è€Œæ¶ˆå¤±ï¼Œä»è€Œå®ç°äº†ç›¸åŒçš„æ”¶æ•›é€Ÿåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨è§†è§‰/è¯­è¨€ä»»åŠ¡ä¸Šä¼˜äºæœ¬åœ°è®­ç»ƒï¼Œå¹¶åœ¨IIDå’ŒéIIDæ•°æ®åˆ†å¸ƒä¸‹è¶…è¶Šäº†è”é‚¦å­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å»ä¸­å¿ƒåŒ–LoRAï¼ˆDLoRAï¼‰åœ¨ç†è®ºåŸºç¡€ä¸Šå­˜åœ¨çš„å…±è¯†å¹²æ‰°å’Œæ”¶æ•›é€Ÿåº¦ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å»ä¸­å¿ƒåŒ–è®¾ç½®ä¸‹ç¼ºä¹å¹³æ»‘æ€§ä¿è¯ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºDeCAFç®—æ³•ï¼Œé€šè¿‡å¼•å…¥æˆªæ–­å¥‡å¼‚å€¼åˆ†è§£ï¼ˆTSVDï¼‰æ¥è§£å†³å…±è¯†å¹²æ‰°é—®é¢˜ï¼ŒåŒæ—¶ç¡®ä¿æ¢¯åº¦çš„å¹³æ»‘æ€§ï¼Œä»è€Œæé«˜DLoRAçš„æ”¶æ•›é€Ÿåº¦ï¼Œä½¿å…¶ä¸å»ä¸­å¿ƒåŒ–éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ç›¸åŒ¹é…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDeCAFç®—æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯DLoRAæ¨¡å—ï¼Œè´Ÿè´£ä½ç§©é€‚åº”çš„è®­ç»ƒï¼›å…¶æ¬¡æ˜¯TSVDæ¨¡å—ï¼Œè¿›è¡ŒçŸ©é˜µåˆ†è§£ä»¥æ¶ˆé™¤å…±è¯†å¹²æ‰°ã€‚ç®—æ³•é€šè¿‡è¿­ä»£ä¼˜åŒ–è¿™ä¸¤ä¸ªæ¨¡å—çš„å‚æ•°ï¼Œç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šDeCAFçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†DLoRAä¸TSVDç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å»ä¸­å¿ƒåŒ–è®­ç»ƒæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒDeCAFèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å…±è¯†å¹²æ‰°é—®é¢˜ï¼Œå¹¶åœ¨ç†è®ºä¸Šä¿è¯æ”¶æ•›é€Ÿåº¦çš„æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬ä½ç§©çŸ©é˜µçš„ç§©é€‰æ‹©å’ŒTSVDçš„è¿‘ä¼¼è¯¯å·®æ§åˆ¶ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å…±è¯†å¹²æ‰°çš„å½±å“ï¼Œä»¥ç¡®ä¿æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDeCAFåœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ¬åœ°è®­ç»ƒï¼Œä¸”åœ¨IIDå’ŒéIIDæ•°æ®åˆ†å¸ƒä¸‹çš„è¡¨ç°è¶…è¶Šäº†ä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—ã€æ™ºèƒ½è®¾å¤‡å’Œåˆ†å¸ƒå¼ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡è§†è§‰-è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸‹çš„è®­ç»ƒæ•ˆç‡ã€‚æœªæ¥ï¼ŒDeCAFå¯èƒ½åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œè”é‚¦å­¦ä¹ ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-Rank Adaptation (LoRA) has emerged as one of the most effective, computationally tractable fine-tuning approaches for training Vision-Language Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by freezing the pre-trained model weights and injecting trainable low-rank matrices, allowing for efficient learning of these foundation models even on edge devices. However, LoRA in decentralized settings still remains under explored, particularly for the theoretical underpinnings due to the lack of smoothness guarantee and model consensus interference (defined formally below). This work improves the convergence rate of decentralized LoRA (DLoRA) to match the rate of decentralized SGD by ensuring gradient smoothness. We also introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular value decomposition (TSVD)-based matrix factorization to resolve consensus interference. Theoretical analysis shows TSVD's approximation error is bounded and consensus differences between DLoRA and DeCAF vanish as rank increases, yielding DeCAF's matching convergence rate. Extensive experiments across vision/language tasks demonstrate our algorithms outperform local training and rivals federated learning under both IID and non-IID data distributions.

