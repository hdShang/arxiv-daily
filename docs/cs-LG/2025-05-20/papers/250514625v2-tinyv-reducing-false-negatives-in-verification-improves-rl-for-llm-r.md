---
layout: default
title: TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning
---

# TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14625" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14625v2</a>
  <a href="https://arxiv.org/pdf/2505.14625.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14625v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14625v2', 'TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-22)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/uw-nsl/TinyV)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTinyVä»¥è§£å†³éªŒè¯å™¨å‡é˜´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å‡é˜´æ€§` `æ¨ç†èƒ½åŠ›` `å¥–åŠ±ä¿¡å·` `éªŒè¯å™¨` `æ•°å­¦æ¨ç†` `TinyV`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºéªŒè¯å™¨æä¾›çš„å¥–åŠ±ä¿¡å·ï¼Œä½†éªŒè¯å™¨çš„å‡é˜´æ€§é—®é¢˜ä¸¥é‡å½±å“äº†æ¨¡å‹è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚
2. è®ºæ–‡æå‡ºTinyVï¼Œä¸€ä¸ªè½»é‡çº§çš„LLMåŸºç¡€éªŒè¯å™¨ï¼Œæ—¨åœ¨åŠ¨æ€è¯†åˆ«å‡é˜´æ€§å¹¶æ¢å¤æœ‰æ•ˆçš„æ¨¡å‹è¾“å‡ºï¼Œä»è€Œæé«˜å¥–åŠ±çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé›†æˆTinyVåï¼Œæ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„é€šè¿‡ç‡æé«˜äº†10%ï¼Œå¹¶åŠ å¿«äº†æ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„é‡è¦å·¥å…·ï¼Œä½†å…¶æˆåŠŸä¾èµ–äºéªŒè¯å™¨æä¾›çš„å¯é å¥–åŠ±ä¿¡å·ã€‚æœ¬æ–‡æ­ç¤ºå¹¶åˆ†æäº†ä¸€ä¸ªæ™®éå­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§ï¼Œå³éªŒè¯å™¨é”™è¯¯æ‹’ç»æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚é€šè¿‡å¯¹Big-Math-RL-Verifiedæ•°æ®é›†çš„æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°è¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆå“åº”å­˜åœ¨å‡é˜´æ€§ï¼Œå¯¼è‡´RLè®­ç»ƒå—åˆ°ä¸¥é‡å½±å“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TinyVï¼Œä¸€ä¸ªè½»é‡çº§çš„LLMåŸºç¡€éªŒè¯å™¨ï¼ŒåŠ¨æ€è¯†åˆ«æ½œåœ¨å‡é˜´æ€§å¹¶æ¢å¤æœ‰æ•ˆå“åº”ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTinyVåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šæé«˜äº†é€šè¿‡ç‡ï¼Œå¹¶åŠ é€Ÿäº†æ”¶æ•›é€Ÿåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯éªŒè¯å™¨åœ¨è¯„ä¼°æ¨¡å‹è¾“å‡ºæ—¶äº§ç”Ÿçš„å‡é˜´æ€§ï¼Œå³é”™è¯¯æ‹’ç»æ­£ç¡®ç­”æ¡ˆçš„æƒ…å†µã€‚è¿™ä¸€é—®é¢˜å¯¼è‡´æ¨¡å‹æ— æ³•è·å¾—æœ‰æ•ˆçš„æ¢¯åº¦ä¿¡å·ï¼Œä»è€Œå½±å“å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTinyVçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„LLMåŸºç¡€éªŒè¯å™¨ï¼ŒåŠ¨æ€è¯†åˆ«å’Œçº æ­£å‡é˜´æ€§ï¼Œä»è€Œæé«˜å¥–åŠ±ä¿¡å·çš„å‡†ç¡®æ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œå‡å°‘é”™è¯¯åé¦ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTinyVçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€å‡é˜´æ€§è¯†åˆ«æ¨¡å—å’Œå¥–åŠ±ä¼°è®¡æ¨¡å—ã€‚æ•°æ®è¾“å…¥æ¨¡å—è´Ÿè´£æ¥æ”¶æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºï¼Œå‡é˜´æ€§è¯†åˆ«æ¨¡å—åˆ©ç”¨LLMå¯¹è¾“å‡ºè¿›è¡ŒéªŒè¯ï¼Œæœ€åå¥–åŠ±ä¼°è®¡æ¨¡å—ç”Ÿæˆæ›´å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šTinyVçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è½»é‡çº§è®¾è®¡å’ŒåŠ¨æ€è¯†åˆ«æœºåˆ¶ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å‡é˜´æ€§é—®é¢˜ï¼Œæå‡éªŒè¯çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒTinyVé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¥–åŠ±ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„æ¥æé«˜å‡é˜´æ€§è¯†åˆ«çš„çµæ•åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆTinyVåï¼Œæ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„é€šè¿‡ç‡æé«˜äº†10%ï¼Œå¹¶ä¸”ç›¸è¾ƒäºåŸºçº¿ï¼Œæ”¶æ•›é€Ÿåº¦æ˜¾è‘—åŠ å¿«ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†è§£å†³éªŒè¯å™¨å‡é˜´æ€§é—®é¢˜çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒTinyVå¯ä»¥åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­æä¾›æ›´å¯é çš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

