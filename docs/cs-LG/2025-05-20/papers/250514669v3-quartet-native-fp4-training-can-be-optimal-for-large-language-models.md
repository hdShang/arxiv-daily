---
layout: default
title: Quartet: Native FP4 Training Can Be Optimal for Large Language Models
---

# Quartet: Native FP4 Training Can Be Optimal for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14669" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14669v3</a>
  <a href="https://arxiv.org/pdf/2505.14669.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14669v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14669v3', 'Quartet: Native FP4 Training Can Be Optimal for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-18)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/IST-DASLab/Quartet)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQuartetä»¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„FP4è®­ç»ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä½ç²¾åº¦è®­ç»ƒ` `FP4` `CUDAä¼˜åŒ–` `è®¡ç®—æ•ˆç‡` `NVIDIA Blackwell` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰FP4è®­ç»ƒç®—æ³•åœ¨å‡†ç¡®æ€§ä¸Šå­˜åœ¨æ˜¾è‘—ä¸‹é™ï¼Œä¸”å¸¸å¸¸ä¾èµ–æ··åˆç²¾åº¦å›é€€ï¼Œå½±å“è®­ç»ƒæ•ˆæœã€‚
2. è®ºæ–‡æå‡ºQuartetæ–¹æ³•ï¼Œé€šè¿‡ç¡¬ä»¶æ”¯æŒçš„FP4è®­ç»ƒå®ç°å‡†ç¡®çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œä¼˜åŒ–äº†è®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒQuartetåœ¨å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾æˆäº†è‰¯å¥½çš„å¹³è¡¡ï¼Œä¼˜äºFP16å’ŒFP8è®­ç»ƒæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬è®ºæ–‡æ¢è®¨äº†ç›´æ¥åœ¨ä½ç²¾åº¦ä¸‹è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•ï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜ååé‡å’Œèƒ½æ•ˆã€‚NVIDIAçš„Blackwellæ¶æ„æ”¯æŒéå¸¸ä½ç²¾åº¦çš„FP4æ“ä½œï¼Œä½†ç°æœ‰FP4è®­ç»ƒç®—æ³•é¢ä¸´æ˜¾è‘—çš„å‡†ç¡®æ€§ä¸‹é™ï¼Œå¹¶å¸¸ä¾èµ–æ··åˆç²¾åº¦å›é€€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„FP4è®­ç»ƒæ–¹æ³•Quartetï¼Œæ—¨åœ¨å®ç°å‡†ç¡®çš„ç«¯åˆ°ç«¯FP4è®­ç»ƒï¼Œä¸»è¦è®¡ç®—ï¼ˆå¦‚çº¿æ€§å±‚ï¼‰å‡é‡‡ç”¨ä½ç²¾åº¦ã€‚é€šè¿‡å¯¹Llamaç±»å‹æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸€ç§æ–°çš„ä½ç²¾åº¦ç¼©æ”¾æ³•åˆ™ï¼Œé‡åŒ–äº†ä¸åŒä½å®½å’Œè®­ç»ƒè®¾ç½®ä¹‹é—´çš„æ€§èƒ½æƒè¡¡ã€‚Quartetä½¿ç”¨é’ˆå¯¹Blackwellä¼˜åŒ–çš„CUDAå†…æ ¸å®ç°ï¼Œè¯æ˜äº†å®Œå…¨åŸºäºFP4çš„è®­ç»ƒæ˜¯FP16åŠç²¾åº¦å’ŒFP8è®­ç»ƒçš„æœ‰ç«äº‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å½“å‰FP4è®­ç»ƒç®—æ³•åœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦ä¾èµ–æ··åˆç²¾åº¦å›é€€ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šQuartetæ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡ç¡¬ä»¶æ”¯æŒçš„FP4è®­ç»ƒï¼Œç›´æ¥åœ¨ä½ç²¾åº¦ä¸‹è¿›è¡Œæ‰€æœ‰ä¸»è¦è®¡ç®—ï¼Œä»¥æé«˜è®­ç»ƒçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQuartetçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€FP4è®­ç»ƒæ¨¡å—å’Œä¼˜åŒ–åçš„CUDAå†…æ ¸ï¼Œç¡®ä¿åœ¨Blackwellæ¶æ„ä¸Šé«˜æ•ˆè¿è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„ä½ç²¾åº¦ç¼©æ”¾æ³•åˆ™ï¼Œé‡åŒ–äº†ä¸åŒä½å®½å’Œè®­ç»ƒè®¾ç½®ä¹‹é—´çš„æ€§èƒ½æƒè¡¡ï¼Œæä¾›äº†å‡†ç¡®çš„FP4è®­ç»ƒæ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šQuartetåœ¨å‚æ•°è®¾ç½®ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨FP4è®­ç»ƒä¸­ä¿æŒé«˜å‡†ç¡®æ€§ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬ä¼˜åŒ–çš„çº¿æ€§å±‚è®¡ç®—å’Œé«˜æ•ˆçš„å†…å­˜ç®¡ç†ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQuartetåœ¨Llamaç±»å‹æ¨¡å‹ä¸Šå®ç°äº†ä¸FP16å’ŒFP8è®­ç»ƒç›¸åª²ç¾çš„æ€§èƒ½ï¼Œå‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°X%ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯æ˜äº†FP4è®­ç»ƒçš„å¯è¡Œæ€§å’Œç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡ä¼˜åŒ–FP4è®­ç»ƒï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œå¯æŒç»­æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an "optimal" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.

