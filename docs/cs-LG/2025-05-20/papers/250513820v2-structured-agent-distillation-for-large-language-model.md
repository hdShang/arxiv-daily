---
layout: default
title: Structured Agent Distillation for Large Language Model
---

# Structured Agent Distillation for Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13820" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13820v2</a>
  <a href="https://arxiv.org/pdf/2505.13820.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13820v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13820v2', 'Structured Agent Distillation for Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Liu, Zhenglun Kong, Peiyan Dong, Changdi Yang, Tianqi Li, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„åŒ–ä»£ç†è’¸é¦ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç†è’¸é¦` `æ¨ç†ä¸è¡ŒåŠ¨` `æ¨¡å‹å‹ç¼©` `ç»“æ„æ„ŸçŸ¥ç›‘ç£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œè¡ŒåŠ¨æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é«˜æ¨ç†æˆæœ¬å’Œæ¨¡å‹ä½“ç§¯é™åˆ¶äº†å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºç»“æ„åŒ–ä»£ç†è’¸é¦ï¼Œé€šè¿‡å°†è½¨è¿¹åˆ†ä¸ºæ¨ç†å’Œè¡ŒåŠ¨ä¸¤ä¸ªéƒ¨åˆ†ï¼Œé‡‡ç”¨ç‰¹å®šæŸå¤±å‡½æ•°è¿›è¡Œå¯¹é½ï¼Œä»è€Œå‹ç¼©æ¨¡å‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¼ ç»Ÿçš„ä»¤ç‰Œçº§è’¸é¦å’Œæ¨¡ä»¿å­¦ä¹ ï¼Œå‹ç¼©æ•ˆæœæ˜¾è‘—ä¸”æ€§èƒ½ä¸‹é™æœ€å°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†³ç­–ä»£ç†æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶é«˜æ¨ç†æˆæœ¬å’Œæ¨¡å‹ä½“ç§¯é™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºç»“æ„åŒ–ä»£ç†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡å°†å¤§å‹LLMä»£ç†å‹ç¼©ä¸ºè¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæ¨ç†çš„å‡†ç¡®æ€§å’Œè¡ŒåŠ¨çš„ä¸€è‡´æ€§ã€‚ä¸æ ‡å‡†çš„ä»¤ç‰Œçº§è’¸é¦ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è½¨è¿¹åˆ†ä¸º{[REASON]}å’Œ{[ACT]}ä¸¤ä¸ªéƒ¨åˆ†ï¼Œé’ˆå¯¹æ¯ä¸ªéƒ¨åˆ†åº”ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥å¯¹é½æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºã€‚è¿™ç§ç»“æ„æ„ŸçŸ¥çš„ç›‘ç£ä½¿å¾—ç´§å‡‘çš„ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°å¤åˆ¶æ•™å¸ˆçš„å†³ç­–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ALFWorldã€HotPotQA-ReActå’ŒWebShopä¸Šå‡ä¼˜äºä»¤ç‰Œçº§å’Œæ¨¡ä»¿å­¦ä¹ åŸºçº¿ï¼Œå®ç°äº†æ˜¾è‘—çš„å‹ç¼©å’Œæœ€å°çš„æ€§èƒ½ä¸‹é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ¨ç†æˆæœ¬å’Œæ¨¡å‹ä½“ç§¯é—®é¢˜ã€‚ç°æœ‰çš„è’¸é¦æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆä¿æŒæ¨ç†çš„å‡†ç¡®æ€§å’Œè¡ŒåŠ¨çš„ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç»“æ„åŒ–ä»£ç†è’¸é¦æ¡†æ¶ï¼Œå°†æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹åˆ†ä¸ºæ¨ç†å’Œè¡ŒåŠ¨ä¸¤ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«è¿›è¡Œè’¸é¦ï¼Œä»¥æ›´å¥½åœ°ä¿ç•™æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ¨ç†æ¨¡å—å’Œè¡ŒåŠ¨æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—æ ¹æ®å…¶ç‰¹æ€§åº”ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨ç»“æ„æ„ŸçŸ¥çš„ç›‘ç£æ–¹æ³•ï¼Œé€šè¿‡åˆ†æ®µå¯¹é½æ¨ç†å’Œè¡ŒåŠ¨ï¼Œæ˜¾è‘—æé«˜äº†è’¸é¦æ•ˆæœï¼Œä¸ä¼ ç»Ÿçš„ä»¤ç‰Œçº§è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™å†³ç­–è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé’ˆå¯¹æ¨ç†å’Œè¡ŒåŠ¨åˆ†åˆ«è®¾ç½®äº†ä¸åŒçš„æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ¯ä¸ªéƒ¨åˆ†çš„å¯¹é½æ•ˆæœã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æ”¯æŒè¿™ç§åˆ†æ®µè’¸é¦çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–ä»£ç†è’¸é¦æ–¹æ³•åœ¨ALFWorldã€HotPotQA-ReActå’ŒWebShopä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ä»¤ç‰Œçº§è’¸é¦å’Œæ¨¡ä»¿å­¦ä¹ åŸºçº¿ï¼Œå‹ç¼©ç‡æ˜¾è‘—ï¼Œæ€§èƒ½ä¸‹é™å¹…åº¦æœ€å°ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œæœºå™¨äººæ§åˆ¶ç­‰ã€‚é€šè¿‡æœ‰æ•ˆå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„æ¨ç†å’Œå†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.

