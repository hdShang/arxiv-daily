---
layout: default
title: Scaling Law for Quantization-Aware Training
---

# Scaling Law for Quantization-Aware Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14302" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.14302v1</a>
  <a href="https://arxiv.org/pdf/2505.14302.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14302v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14302v1', 'Scaling Law for Quantization-Aware Training')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, Ping Luo

**ÂàÜÁ±ª**: cs.LG, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-20

**Â§áÊ≥®**: A unified scaling law for QAT that models quantization error as a function of model size, training data volume, and quantization group size

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Áªü‰∏ÄÁº©ÊîæÊ≥ïÂàô‰ª•‰ºòÂåñÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈáèÂåñËØØÂ∑Æ` `Ê∑∑ÂêàÁ≤æÂ∫¶` `Ê®°Âûã‰ºòÂåñ` `ËÆ≠ÁªÉÊï∞ÊçÆÈáè` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÊñπÊ≥ïÂú®Â§ÑÁêÜ4‰ΩçÁ≤æÂ∫¶Êó∂ÁöÑÁº©ÊîæË°å‰∏∫Â∞ö‰∏çÊòéÁ°ÆÔºå‰∏îÂøΩËßÜ‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆÈáèÂíåÈáèÂåñÁ≤íÂ∫¶Á≠âÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÁº©ÊîæÊ≥ïÂàôÔºåÂ∞ÜÈáèÂåñËØØÂ∑Æ‰∏éÊ®°ÂûãËßÑÊ®°„ÄÅËÆ≠ÁªÉÊï∞ÊçÆÈáèÂíåÈáèÂåñÁªÑÂ§ßÂ∞èÂÖ≥ËÅîÔºå‰∏∫QATÊèê‰æõ‰∫ÜÊñ∞ÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇ
3. ÈÄöËøá268‰∏™ÂÆûÈ™åÔºåÂèëÁé∞ÈáèÂåñËØØÂ∑ÆÈöèÁùÄÊ®°ÂûãËßÑÊ®°Â¢ûÂä†ËÄåÂáèÂ∞ëÔºå‰ΩÜËÆ≠ÁªÉÊ†áËÆ∞ÂíåÁ≤óÁ≤íÂ∫¶ÈáèÂåñ‰ºöÂØºËá¥ËØØÂ∑Æ‰∏äÂçáÔºåÊèê‰æõ‰∫Ü‰ºòÂåñÊñπÂêë„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÈúÄË¶ÅÂ§ßÈáèËÆ°ÁÆóÂíåÂÜÖÂ≠òËµÑÊ∫êÔºåÁªôÈÉ®ÁΩ≤Â∏¶Êù•‰∫ÜÊåëÊàò„ÄÇÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâÈÄöËøáÈôç‰ΩéÊ®°ÂûãÁ≤æÂ∫¶Êù•Â∫îÂØπËøô‰∫õÊåëÊàòÔºå‰ΩÜÂÖ∂Áº©ÊîæË°å‰∏∫ÔºåÂ∞§ÂÖ∂ÊòØÂú®4‰ΩçÁ≤æÂ∫¶ÔºàW4A4Ôºâ‰∏ãÔºåÂ∞ö‰∏çÊòéÁ°Æ„ÄÇÁé∞ÊúâÁöÑQATÁº©ÊîæÊ≥ïÂàôÂæÄÂæÄÂøΩËßÜ‰∫ÜËÆ≠ÁªÉÊ†áËÆ∞Êï∞ÈáèÂíåÈáèÂåñÁ≤íÂ∫¶Á≠âÂÖ≥ÈîÆÂõ†Á¥†ÔºåÈôêÂà∂‰∫ÜÂÖ∂ÈÄÇÁî®ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑQATÁº©ÊîæÊ≥ïÂàôÔºåÂ∞ÜÈáèÂåñËØØÂ∑ÆÂª∫Ê®°‰∏∫Ê®°ÂûãËßÑÊ®°„ÄÅËÆ≠ÁªÉÊï∞ÊçÆÈáèÂíåÈáèÂåñÁªÑÂ§ßÂ∞èÁöÑÂáΩÊï∞„ÄÇÈÄöËøá268‰∏™QATÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞ÈáèÂåñËØØÂ∑ÆÈöèÁùÄÊ®°ÂûãËßÑÊ®°ÁöÑÂ¢ûÂä†ËÄåÂáèÂ∞ëÔºå‰ΩÜÈöèÁùÄËÆ≠ÁªÉÊ†áËÆ∞ÁöÑÂ¢ûÂä†ÂíåÈáèÂåñÁ≤íÂ∫¶ÁöÑÁ≤óÂåñËÄå‰∏äÂçá„ÄÇÊàë‰ª¨ËøòÂ∞ÜW4A4ÈáèÂåñËØØÂ∑ÆÂàÜËß£‰∏∫ÊùÉÈáçÂíåÊøÄÊ¥ªÁªÑ‰ª∂ÔºåÂèëÁé∞‰∏§ËÄÖÁöÑÊïèÊÑüÊÄß‰∏çÂêåÔºåÊùÉÈáçÈáèÂåñËØØÂ∑ÆÂØπËÆ≠ÁªÉÊ†áËÆ∞ÁöÑÂ¢ûÂä†ÂèçÂ∫îÊõ¥‰∏∫ÊïèÊÑü„ÄÇËøô‰∫õÂèëÁé∞‰∏∫QATÁöÑÁ†îÁ©∂ÂíåÂºÄÂèëÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑËßÅËß£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâÂú®4‰ΩçÁ≤æÂ∫¶‰∏ãÁöÑÁº©ÊîæË°å‰∏∫‰∏çÊòéÁ°ÆÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÂÖÖÂàÜËÄÉËôëËÆ≠ÁªÉÊ†áËÆ∞Êï∞ÈáèÂíåÈáèÂåñÁ≤íÂ∫¶Á≠âÂÖ≥ÈîÆÂõ†Á¥†ÔºåÂØºËá¥ÂÖ∂ÈÄÇÁî®ÊÄßÂèóÂà∞ÈôêÂà∂„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÁº©ÊîæÊ≥ïÂàôÔºåÂ∞ÜÈáèÂåñËØØÂ∑ÆËßÜ‰∏∫Ê®°ÂûãËßÑÊ®°„ÄÅËÆ≠ÁªÉÊï∞ÊçÆÈáèÂíåÈáèÂåñÁªÑÂ§ßÂ∞èÁöÑÂáΩÊï∞„ÄÇËøô‰∏ÄÊñπÊ≥ïÈÄöËøáÁ≥ªÁªüÊÄßÂÆûÈ™åÈ™åËØÅ‰∫Ü‰∏çÂêåÂõ†Á¥†ÂØπÈáèÂåñËØØÂ∑ÆÁöÑÂΩ±ÂìçÔºåÊèê‰æõ‰∫ÜÊñ∞ÁöÑÁêÜËÆ∫Ê°ÜÊû∂„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÈÄöËøá268‰∏™QATÂÆûÈ™åÔºåÂàÜÊûê‰∫ÜÊ®°ÂûãËßÑÊ®°„ÄÅËÆ≠ÁªÉÊï∞ÊçÆÈáèÂíåÈáèÂåñÁ≤íÂ∫¶ÂØπÈáèÂåñËØØÂ∑ÆÁöÑÂΩ±Âìç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈáèÂåñËØØÂ∑ÆÁöÑÂèòÂåñË∂ãÂäø‰∏éÊ®°ÂûãËßÑÊ®°ÂíåËÆ≠ÁªÉÊï∞ÊçÆÈáèÂØÜÂàáÁõ∏ÂÖ≥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÈáèÂåñËØØÂ∑ÆÂàÜËß£‰∏∫ÊùÉÈáçÂíåÊøÄÊ¥ªÁªÑ‰ª∂ÔºåÂπ∂ÂàÜÊûêÂÖ∂ÂØπËÆ≠ÁªÉÊ†áËÆ∞ÁöÑÊïèÊÑüÊÄß„ÄÇËøô‰∏ÄÂàÜËß£ÊñπÊ≥ïÊè≠Á§∫‰∫ÜW4A4ÈáèÂåñËØØÂ∑ÆÁöÑ‰∏ªË¶ÅÊù•Ê∫êÔºå‰∏∫‰ºòÂåñÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßí„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÆûÈ™å‰∏≠ÔºåÈááÁî®‰∫ÜÊ∑∑ÂêàÁ≤æÂ∫¶ÈáèÂåñÁöÑÊñπÊ≥ïÊù•Ëß£ÂÜ≥ÊøÄÊ¥ªÈáèÂåñËØØÂ∑ÆÁöÑÁì∂È¢àÔºåÂêåÊó∂ÂØπÊùÉÈáçÈáèÂåñËØØÂ∑ÆËøõË°å‰∫ÜÊ∑±ÂÖ•ÂàÜÊûêÔºåÂèëÁé∞ÈöèÁùÄËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ¢ûÂä†ÔºåÊùÉÈáçÈáèÂåñËØØÂ∑ÆÊúÄÁªà‰ºöË∂ÖËøáÊøÄÊ¥ªÈáèÂåñËØØÂ∑Æ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÈöèÁùÄÊ®°ÂûãËßÑÊ®°ÁöÑÂ¢ûÂä†ÔºåÈáèÂåñËØØÂ∑ÆÊòæËëóÈôç‰ΩéÔºåËÄåËÆ≠ÁªÉÊ†áËÆ∞ÁöÑÂ¢ûÂä†ÂíåÁ≤óÁ≤íÂ∫¶ÈáèÂåñÂàôÂØºËá¥ËØØÂ∑Æ‰∏äÂçá„ÄÇÈÄöËøáÊ∑∑ÂêàÁ≤æÂ∫¶ÈáèÂåñÔºåÊùÉÈáçÂíåÊøÄÊ¥ªÈáèÂåñËØØÂ∑ÆÂèØ‰ª•Êî∂ÊïõÂà∞Áõ∏‰ººÊ∞¥Âπ≥ÔºåÊèê‰æõ‰∫ÜÊúâÊïàÁöÑ‰ºòÂåñÁ≠ñÁï•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ºòÂåñÂíåÈÉ®ÁΩ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇÈÄöËøáÊîπËøõÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºåËÉΩÂ§üÂú®‰øùÊåÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂíåÂÜÖÂ≠òÈúÄÊ±ÇÔºåÊé®Âä®AIÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂπøÊ≥õ‰ΩøÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precision (W4A4), is not well understood. Existing QAT scaling laws often ignore key factors such as the number of training tokens and quantization granularity, which limits their applicability. This paper proposes a unified scaling law for QAT that models quantization error as a function of model size, training data volume, and quantization group size. Through 268 QAT experiments, we show that quantization error decreases as model size increases, but rises with more training tokens and coarser quantization granularity. To identify the sources of W4A4 quantization error, we decompose it into weight and activation components. Both components follow the overall trend of W4A4 quantization error, but with different sensitivities. Specifically, weight quantization error increases more rapidly with more training tokens. Further analysis shows that the activation quantization error in the FC2 layer, caused by outliers, is the primary bottleneck of W4A4 QAT quantization error. By applying mixed-precision quantization to address this bottleneck, we demonstrate that weight and activation quantization errors can converge to similar levels. Additionally, with more training data, weight quantization error eventually exceeds activation quantization error, suggesting that reducing weight quantization error is also important in such scenarios. These findings offer key insights for improving QAT research and development.

