---
layout: default
title: "Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions"
---

# Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14543" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14543v1</a>
  <a href="https://arxiv.org/pdf/2505.14543.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14543v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14543v1', 'Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Utsav Dutta, Sina Khoshfetrat Pakazad, Henrik Ohlsson

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCHARMä»¥è§£å†³æ—¶é—´åºåˆ—å»ºæ¨¡çš„å±€é™æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—å»ºæ¨¡` `åŸºç¡€æ¨¡å‹` `å¤šå˜é‡åˆ†æ` `Transformer` `å¯è§£é‡Šæ€§` `æ•°æ®å¢å¼º` `è”åˆåµŒå…¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ—¶é—´åºåˆ—æ¨¡å‹å¾€å¾€ä¾èµ–äºç‰¹å®šä»»åŠ¡å’Œæ•°æ®é›†ï¼Œç¼ºä¹é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ã€‚
2. CHARMæ¨¡å‹é€šè¿‡é›†æˆé€šé“çº§æ–‡æœ¬æè¿°ï¼Œå­¦ä¹ å…±äº«å’Œé¢†åŸŸæ„ŸçŸ¥çš„æ—¶é—´åºåˆ—è¡¨ç¤ºï¼Œå…‹æœäº†ä¼ ç»Ÿæ¨¡å‹çš„å±€é™ã€‚
3. CHARMåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®¾å®šäº†æ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ çš„æ–°åŸºå‡†ï¼Œå±•ç¤ºäº†700ä¸‡å‚æ•°æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„æ—¶é—´åºåˆ—æ¨¡å‹é€šå¸¸æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œä¾èµ–äºæ•°æ®é›†ç‰¹å®šçš„è®­ç»ƒå’Œå¤§é‡ç‰¹å¾å·¥ç¨‹ã€‚å°½ç®¡åŸºäºTransformerçš„æ¶æ„æé«˜äº†å¯æ‰©å±•æ€§ï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢†åŸŸï¼ŒåŸºç¡€æ¨¡å‹çš„åº”ç”¨ä»ç„¶è¾ƒå°‘ï¼Œä¸»è¦é›†ä¸­åœ¨é¢„æµ‹ä»»åŠ¡ä¸Šã€‚æœ¬æ–‡æå‡ºäº†CHARMï¼Œä¸€ä¸ªç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—çš„åŸºç¡€åµŒå…¥æ¨¡å‹ï¼Œèƒ½å¤Ÿå­¦ä¹ å…±äº«ã€å¯è½¬ç§»å’Œé¢†åŸŸæ„ŸçŸ¥çš„è¡¨ç¤ºã€‚CHARMé€šè¿‡é›†æˆé€šé“çº§æ–‡æœ¬æè¿°çš„æ¶æ„åˆ›æ–°ï¼Œè§£å†³äº†æ—¶é—´åºåˆ—åŸºç¡€å­¦ä¹ çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¿æŒå¯¹é€šé“é¡ºåºçš„ä¸å˜æ€§ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆJEPAï¼‰è¿›è¡Œè®­ç»ƒï¼Œç»“åˆæ–°é¢–çš„æ•°æ®å¢å¼ºæ–¹æ¡ˆå’ŒæŸå¤±å‡½æ•°ï¼Œä»¥æé«˜å¯è§£é‡Šæ€§å’Œè®­ç»ƒç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„700ä¸‡å‚æ•°æ¨¡å‹åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºæ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ—¶é—´åºåˆ—æ¨¡å‹åœ¨ä»»åŠ¡ç‰¹å®šæ€§å’Œæ•°æ®é›†ä¾èµ–æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡ç‰¹å¾å·¥ç¨‹ï¼Œéš¾ä»¥å®ç°é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCHARMæ¨¡å‹é€šè¿‡å¼•å…¥é€šé“çº§æ–‡æœ¬æè¿°ï¼Œå­¦ä¹ å…±äº«å’Œé¢†åŸŸæ„ŸçŸ¥çš„è¡¨ç¤ºï¼Œæ—¨åœ¨æé«˜æ—¶é—´åºåˆ—æ•°æ®çš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¯¹é€šé“é¡ºåºçš„ä¸å˜æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCHARMé‡‡ç”¨è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆJEPAï¼‰ï¼Œç»“åˆæ–°é¢–çš„æ•°æ®å¢å¼ºæ–¹æ¡ˆå’ŒæŸå¤±å‡½æ•°ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šCHARMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ¶æ„è®¾è®¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆé€šé“çº§æ–‡æœ¬æè¿°ï¼Œæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹åŒ…å«700ä¸‡å‚æ•°ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¢å¼ºå¯è§£é‡Šæ€§ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šæ³¨é‡é€šé“æè¿°çš„é›†æˆï¼Œç¡®ä¿æ¨¡å‹å¯¹è¾“å…¥é¡ºåºçš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CHARMæ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®¾å®šäº†æ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ çš„æ–°åŸºå‡†ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¨¡å‹åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å°¤ä¸ºçªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CHARMæ¨¡å‹åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†æä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œé€‚ç”¨äºé‡‘èå¸‚åœºé¢„æµ‹ã€æ°”å€™å˜åŒ–ç›‘æµ‹ã€å¥åº·æ•°æ®åˆ†æç­‰é¢†åŸŸã€‚å…¶é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ä½¿å¾—è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒåœºæ™¯ä¸­å¿«é€Ÿé€‚åº”ï¼Œæå‡æ•°æ®åˆ†æçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒCHARMæœ‰æœ›æ¨åŠ¨æ—¶é—´åºåˆ—åˆ†æçš„ç ”ç©¶å’Œåº”ç”¨å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional time series models are task-specific and often depend on dataset-specific training and extensive feature engineering. While Transformer-based architectures have improved scalability, foundation models, commonplace in text, vision, and audio, remain under-explored for time series and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a foundation embedding model for multivariate time series that learns shared, transferable, and domain-aware representations. To address the unique difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates architectural innovations that integrate channel-level textual descriptions while remaining invariant to channel order. The model is trained using a Joint Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a loss function designed to improve interpretability and training stability. Our $7$M-parameter model achieves state-of-the-art performance across diverse downstream tasks, setting a new benchmark for time series representation learning.

