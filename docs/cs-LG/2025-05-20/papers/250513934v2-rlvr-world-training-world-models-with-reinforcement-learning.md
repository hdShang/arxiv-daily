---
layout: default
title: "RLVR-World: Training World Models with Reinforcement Learning"
---

# RLVR-World: Training World Models with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13934" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13934v2</a>
  <a href="https://arxiv.org/pdf/2505.13934.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13934v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13934v2', 'RLVR-World: Training World Models with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jialong Wu, Shaofeng Yin, Ningya Feng, Mingsheng Long

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-25)

**å¤‡æ³¨**: NeurIPS 2025. Code is available at project website: https://thuml.github.io/RLVR-World/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://thuml.github.io/RLVR-World)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLVR-Worldä»¥ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹çš„ä»»åŠ¡ç‰¹å®šç›®æ ‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸–ç•Œæ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¯éªŒè¯å¥–åŠ±` `ä»»åŠ¡ä¼˜åŒ–` `è‡ªå›å½’é¢„æµ‹` `ç”Ÿæˆæ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸–ç•Œæ¨¡å‹è®­ç»ƒæ–¹æ³•å¦‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰ä¸ä»»åŠ¡ç‰¹å®šç›®æ ‡å­˜åœ¨ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. RLVR-Worldæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ç›¸ç»“åˆï¼Œç›´æ¥ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œæå‡ä»»åŠ¡é€‚åº”æ€§ã€‚
3. åœ¨å¤šä¸ªé¢†åŸŸçš„å®éªŒä¸­ï¼ŒRLVR-Worldåœ¨è¯­è¨€å’Œè§†é¢‘åŸºç¡€çš„ä¸–ç•Œæ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸–ç•Œæ¨¡å‹é€šè¿‡é¢„æµ‹çŠ¶æ€è½¬ç§»æ¥å“åº”åŠ¨ä½œï¼Œå¹¿æ³›åº”ç”¨äºå¤šç§æ¨¡æ€ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„è®­ç»ƒç›®æ ‡å¦‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰å¾€å¾€ä¸ä¸–ç•Œæ¨¡å‹çš„ä»»åŠ¡ç‰¹å®šç›®æ ‡ä¸ä¸€è‡´ï¼Œä¾‹å¦‚è½¬ç§»é¢„æµ‹çš„å‡†ç¡®æ€§æˆ–æ„ŸçŸ¥è´¨é‡ã€‚æœ¬æ–‡æå‡ºäº†RLVR-Worldï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ç›´æ¥ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹ï¼Œä»¥æ»¡è¶³è¿™äº›æŒ‡æ ‡ã€‚å°½ç®¡å°†ä¸–ç•Œå»ºæ¨¡å½¢å¼åŒ–ä¸ºæ ‡è®°åºåˆ—çš„è‡ªå›å½’é¢„æµ‹ï¼ŒRLVR-Worldè¯„ä¼°è§£ç é¢„æµ‹çš„æŒ‡æ ‡ä½œä¸ºå¯éªŒè¯å¥–åŠ±ã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬æ¸¸æˆã€ç½‘é¡µå¯¼èˆªå’Œæœºå™¨äººæ“ä½œç­‰é¢†åŸŸå±•ç¤ºäº†è¯­è¨€å’Œè§†é¢‘åŸºç¡€çš„ä¸–ç•Œæ¨¡å‹çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼ŒRLVRä¸ºå¢å¼ºç”Ÿæˆæ¨¡å‹çš„å®ç”¨æ€§æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„åè®­ç»ƒèŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä¸–ç•Œæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚MLEï¼‰ä¸ä»»åŠ¡ç‰¹å®šç›®æ ‡ä¹‹é—´çš„å¯¹é½é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRLVR-Worldæ¡†æ¶é€šè¿‡å¼•å…¥å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œç›´æ¥ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLVR-Worldçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€æ¨¡å‹è®­ç»ƒã€å¥–åŠ±è¯„ä¼°å’Œæ€§èƒ½ä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ¨¡å‹é€šè¿‡è‡ªå›å½’æ–¹å¼è¿›è¡ŒçŠ¶æ€è½¬ç§»é¢„æµ‹ï¼Œå¹¶åœ¨è§£ç åè¯„ä¼°å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLVR-Worldçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ç»“åˆï¼Œç›´æ¥é’ˆå¯¹ä»»åŠ¡ç‰¹å®šç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„MLEæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¯„ä¼°é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬å¥–åŠ±å‡½æ•°çš„è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„è¶…å‚æ•°è°ƒä¼˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªå®éªŒä¸­ï¼ŒRLVR-Worldåœ¨æ–‡æœ¬æ¸¸æˆå’Œè§†é¢‘åŸºç¡€çš„ä¸–ç•Œæ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ä»»åŠ¡å®Œæˆç‡ä¸Šæé«˜äº†20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RLVR-Worldçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ¸¸æˆæ™ºèƒ½ä½“ã€è‡ªåŠ¨åŒ–ç½‘é¡µå¯¼èˆªå’Œæœºå™¨äººæ“ä½œç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–ä¸–ç•Œæ¨¡å‹çš„æ€§èƒ½ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæå‡ç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly. Code, datasets, models, and video samples are available at the project website: https://thuml.github.io/RLVR-World.

