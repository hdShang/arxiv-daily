---
layout: default
title: "LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models"
---

# LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14759" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14759v3</a>
  <a href="https://arxiv.org/pdf/2505.14759.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14759v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14759v3', 'LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yan Wang, Ling Ding, Tien N Nguyen, Shaohua Wang, Yanan Zheng

**åˆ†ç±»**: cs.SE, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-06-08)

**å¤‡æ³¨**: Accepted to ACL 2025 main conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLeanCodeä»¥è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä»£ç ç®€åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `ä»£ç ç®€åŒ–` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `æ³¨æ„åŠ›æœºåˆ¶` `ä»£ç æœç´¢` `ä»£ç æ‘˜è¦` `æ¨¡å‹æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä»£ç åºåˆ—æ—¶è®¡ç®—å¤æ‚åº¦æ˜¾è‘—å¢åŠ ï¼Œå¯¼è‡´è®­ç»ƒå’Œé¢„æµ‹æ—¶é—´å»¶é•¿ã€‚
2. LeanCodeé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ³¨æ„åŠ›åˆ†æ•°é€‰æ‹©æ€§ç§»é™¤ä¸é‡è¦çš„tokenï¼Œä»è€Œå®ç°ä»£ç ç®€åŒ–ï¼Œæå‡æ¨¡å‹æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLeanCodeåœ¨ä»£ç æœç´¢å’Œä»£ç æ‘˜è¦ä»»åŠ¡ä¸Šåˆ†åˆ«æ¯”DietCodeå’ŒSlimcodeæå‡äº†60%ã€16%å’Œ29%ã€27%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä»£ç æ—¶å¸¸é¢ä¸´æ˜¾è‘—çš„è®¡ç®—å¤æ‚åº¦ï¼Œå°¤å…¶æ˜¯è¾“å…¥ä»£ç åºåˆ—é•¿åº¦å¢åŠ æ—¶ã€‚æœ¬æ–‡æå‡ºLeanCodeï¼Œé€šè¿‡åˆ©ç”¨ä»£ç ä¸Šä¸‹æ–‡ä¸­çš„æ³¨æ„åŠ›åˆ†æ•°æ¥ç®€åŒ–ä»£ç ï¼Œä»è€Œå‡å°‘è®­ç»ƒå’Œé¢„æµ‹æ—¶é—´ã€‚æˆ‘ä»¬ä¸»å¼ åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¹³å‡æ³¨æ„åŠ›åˆ†æ•°é€‰æ‹©æ€§åœ°ç§»é™¤ä¸é‡è¦çš„tokenï¼Œè€Œéç®€å•çš„å¹³å‡åˆ†æ•°ã€‚LeanCodeåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨ç¼–ç å™¨ä¸­çš„`CLS` tokençš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œåœ¨åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ä¸­åˆ™åˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨çš„æ³¨æ„åŠ›åˆ†æ•°æ¥è¯„ä¼°tokençš„é‡è¦æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLeanCodeåœ¨ä»£ç æœç´¢å’Œä»£ç æ‘˜è¦ä»»åŠ¡ä¸Šç›¸è¾ƒäºç°æœ‰æ–¹æ³•DietCodeå’ŒSlimcodeåˆ†åˆ«æå‡äº†60%ã€16%å’Œ29%ã€27%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä»£ç åºåˆ—æ—¶çš„è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¾“å…¥æ—¶æœªèƒ½æœ‰æ•ˆè¯„ä¼°tokençš„é‡è¦æ€§ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLeanCodeçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ³¨æ„åŠ›åˆ†æ•°æ¥é€‰æ‹©æ€§åœ°ç§»é™¤ä¸é‡è¦çš„tokenï¼Œè€Œä¸æ˜¯ä¾èµ–äºæ‰€æœ‰è¾“å…¥çš„å¹³å‡åˆ†æ•°ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLeanCodeçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç¼–ç å™¨å’Œè§£ç å™¨ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚åœ¨ç¼–ç å™¨ä¸­ï¼Œä½¿ç”¨`CLS` tokençš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œåˆ†ç±»ä»»åŠ¡ï¼›åœ¨è§£ç å™¨ä¸­ï¼Œåˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨çš„æ³¨æ„åŠ›åˆ†æ•°æ¥è¯„ä¼°tokençš„é‡è¦æ€§ï¼Œé€‚ç”¨äºåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLeanCodeçš„åˆ›æ–°åœ¨äºå¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œtokené€‰æ‹©ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä»£ç æœç´¢å’Œæ‘˜è¦ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒLeanCodeå¯¹æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¢å¼ºæ¨¡å‹å¯¹é‡è¦tokençš„å…³æ³¨ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LeanCodeåœ¨å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç°æœ‰çš„SOTAæ–¹æ³•DietCodeå’ŒSlimcodeï¼Œåœ¨ä»£ç æœç´¢ä»»åŠ¡ä¸Šåˆ†åˆ«æå‡äº†60%å’Œ16%ï¼Œåœ¨ä»£ç æ‘˜è¦ä»»åŠ¡ä¸Šåˆ™æå‡äº†29%å’Œ27%ã€‚è¿™äº›ç»“æœè¡¨æ˜LeanCodeåœ¨æå‡æ¨¡å‹æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LeanCodeçš„ç ”ç©¶æˆæœåœ¨ä»£ç æœç´¢ã€ä»£ç æ‘˜è¦ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æ•ˆç‡ï¼ŒLeanCodeèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å¿«é€Ÿåœ°è·å–å’Œç†è§£ä»£ç ï¼Œä»è€Œæå‡è½¯ä»¶å¼€å‘çš„æ•´ä½“æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models for code often entail significant computational complexity, which grows significantly with the length of the input code sequence. We propose LeanCode for code simplification to reduce training and prediction time, leveraging code contexts in utilizing attention scores to represent the tokens' importance. We advocate for the selective removal of tokens based on the average context-aware attention scores rather than average scores across all inputs. LeanCode uses the attention scores of `CLS' tokens within the encoder for classification tasks, such as code search. It also employs the encoder-decoder attention scores to determine token significance for sequence-to-sequence tasks like code summarization. Our evaluation shows LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements of 60% and 16% for code search, and 29% and 27% for code summarization, respectively.

