---
layout: default
title: Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks
---

# Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14459" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14459v1</a>
  <a href="https://arxiv.org/pdf/2505.14459.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14459v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14459v1', 'Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kamal Singh, Sami Marouani, Ahmad Al Sheikh, Pham Tran Anh Quang, Amaury Habrard

**åˆ†ç±»**: cs.LG, cs.NI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKolmogorov-Arnoldç½‘ç»œä»¥è§£å†³è´Ÿè½½å‡è¡¡çš„å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ` `è´Ÿè½½å‡è¡¡` `Kolmogorov-Arnoldç½‘ç»œ` `ç½‘ç»œæ§åˆ¶` `PPOä»£ç†` `å¤šå±‚æ„ŸçŸ¥å™¨` `ç½‘ç»œæ€§èƒ½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç½‘ç»œæ§åˆ¶ä¸­ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æå–æ§åˆ¶å™¨æ–¹ç¨‹ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºä½¿ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ç»“åˆPPOä»£ç†ï¼Œä»¥å®ç°å¯è§£é‡Šçš„è´Ÿè½½å‡è¡¡ç­–ç•¥å­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒå¥–åŠ±å‡½æ•°ä¸‹æœ‰æ•ˆæé«˜äº†ç½‘ç»œæ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯è§£é‡Šçš„å†³ç­–è¿‡ç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ç½‘ç»œæ§åˆ¶é—®é¢˜ä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„åº”ç”¨ï¼Œä¾‹å¦‚è´Ÿè½½å‡è¡¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLæ–¹æ³•å¾€å¾€ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æå–æ§åˆ¶å™¨æ–¹ç¨‹ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰è¿›è¡Œç½‘ç»œæ§åˆ¶ä¸­çš„å¯è§£é‡ŠRLã€‚æˆ‘ä»¬é‡‡ç”¨PPOä»£ç†ï¼Œç»“åˆ1å±‚çš„KANæ¨¡å‹å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è¯„è®ºç½‘ç»œï¼Œå­¦ä¹ æœ€å¤§åŒ–ååé‡æ•ˆç”¨ã€æœ€å°åŒ–æŸå¤±å’Œå»¶è¿Ÿçš„è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚è¯¥æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»å­¦ä¹ åˆ°çš„ç¥ç»ç½‘ç»œä¸­æå–æ§åˆ¶å™¨æ–¹ç¨‹ï¼Œä»è€Œæ·±å…¥äº†è§£å†³ç­–è¿‡ç¨‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„å¥–åŠ±å‡½æ•°è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œè¯æ˜å…¶åœ¨æé«˜ç½‘ç»œæ€§èƒ½çš„åŒæ—¶æä¾›äº†å¯è§£é‡Šçš„ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç½‘ç»œè´Ÿè½½å‡è¡¡ä¸­ç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ï¼Œå¯¼è‡´éš¾ä»¥æå–æ§åˆ¶å™¨æ–¹ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œç»“åˆPPOä»£ç†ï¼Œè®¾è®¡å‡ºå¯è§£é‡Šçš„è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œå…è®¸ä»ç¥ç»ç½‘ç»œä¸­æå–æ§åˆ¶å™¨æ–¹ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªPPOä»£ç†ï¼Œä½¿ç”¨1å±‚çš„KANä½œä¸ºæ¼”å‘˜æ¨¡å‹ï¼Œå’Œä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä½œä¸ºè¯„è®ºç½‘ç»œï¼ŒååŒå­¦ä¹ è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä½¿ç”¨KANç½‘ç»œå®ç°å¯è§£é‡Šçš„å¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿä»å­¦ä¹ åˆ°çš„æ¨¡å‹ä¸­æå–æ§åˆ¶å™¨æ–¹ç¨‹ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­æ˜¯å‰æ‰€æœªæœ‰çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ååé‡ã€æŸå¤±å’Œå»¶è¿Ÿï¼Œå¹¶é€šè¿‡è°ƒæ•´KANå’ŒMLPçš„ç»“æ„å‚æ•°æ¥ä¼˜åŒ–å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨KANçš„è´Ÿè½½å‡è¡¡ç­–ç•¥åœ¨å¤šä¸ªå¥–åŠ±å‡½æ•°ä¸‹å‡æ˜¾è‘—æé«˜äº†ç½‘ç»œååé‡å’Œé™ä½äº†å»¶è¿Ÿï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”æä¾›äº†å¯è§£é‡Šçš„æ§åˆ¶ç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œæµé‡ç®¡ç†ã€æ•°æ®ä¸­å¿ƒè´Ÿè½½å‡è¡¡å’Œäº‘è®¡ç®—èµ„æºè°ƒåº¦ç­‰ã€‚é€šè¿‡æä¾›å¯è§£é‡Šçš„å†³ç­–è¿‡ç¨‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ç½‘ç»œç®¡ç†å‘˜æ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–ç½‘ç»œæ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has been increasingly applied to network control problems, such as load balancing. However, existing RL approaches often suffer from lack of interpretability and difficulty in extracting controller equations. In this paper, we propose the use of Kolmogorov-Arnold Networks (KAN) for interpretable RL in network control. We employ a PPO agent with a 1-layer actor KAN model and an MLP Critic network to learn load balancing policies that maximise throughput utility, minimize loss as well as delay. Our approach allows us to extract controller equations from the learned neural networks, providing insights into the decision-making process. We evaluate our approach using different reward functions demonstrating its effectiveness in improving network performance while providing interpretable policies.

