---
layout: default
title: Flattening Hierarchies with Policy Bootstrapping
---

# Flattening Hierarchies with Policy Bootstrapping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14975" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14975v2</a>
  <a href="https://arxiv.org/pdf/2505.14975.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14975v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14975v2', 'Flattening Hierarchies with Policy Bootstrapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: John L. Zhou, Jonathan C. Kao

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-15)

**å¤‡æ³¨**: NeurIPS 2025 (Spotlight, top 3.2%)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://johnlyzhou.github.io/saw/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°ç®—æ³•ä»¥è§£å†³é•¿æ—¶é—´ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä¸­çš„å±‚æ¬¡æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ` `å±‚æ¬¡å¼ºåŒ–å­¦ä¹ ` `ä¼˜åŠ¿åŠ æƒé‡è¦æ€§é‡‡æ ·` `é«˜ç»´æ§åˆ¶` `ç­–ç•¥è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶é—´ç›®æ ‡ä»»åŠ¡ä¸­é¢ä¸´ç¨€ç–å¥–åŠ±å’ŒæŠ˜æ‰£çš„æŒ‘æˆ˜ï¼Œéš¾ä»¥æœ‰æ•ˆæ‰©å±•ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡ä¼˜åŠ¿åŠ æƒé‡è¦æ€§é‡‡æ ·å¼•å¯¼å­ç›®æ ‡æ¡ä»¶ç­–ç•¥çš„ç®—æ³•ï¼Œæ—¨åœ¨è®­ç»ƒå¹³å¦çš„ç›®æ ‡æ¡ä»¶ç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§çŠ¶æ€å’Œåƒç´ åŸºç¡€çš„è¿åŠ¨ä¸æ“ä½œåŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ç¦»çº¿GCRLç®—æ³•ï¼Œé€‚åº”å¤æ‚ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGCRLï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¤§è§„æ¨¡æ— å¥–åŠ±è½¨è¿¹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒé€šç”¨ç­–ç•¥ã€‚ç„¶è€Œï¼Œç”±äºç¨€ç–å¥–åŠ±å’ŒæŠ˜æ‰£çš„ç»“åˆï¼Œä½¿å¾—å°†GCRLæ‰©å±•åˆ°æ›´é•¿çš„æ—¶é—´èŒƒå›´å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„å±‚æ¬¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶é—´ç›®æ ‡è¾¾æˆä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å…¶å¯¹æ¨¡å—åŒ–ã€ç‰¹å®šæ—¶é—´å°ºåº¦ç­–ç•¥å’Œå­ç›®æ ‡ç”Ÿæˆçš„ä¾èµ–å¢åŠ äº†å¤æ‚æ€§ï¼Œé™åˆ¶äº†åœ¨é«˜ç»´ç›®æ ‡ç©ºé—´çš„æ‰©å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡ä¼˜åŠ¿åŠ æƒé‡è¦æ€§é‡‡æ ·å¯¹å­ç›®æ ‡æ¡ä»¶ç­–ç•¥è¿›è¡Œå¼•å¯¼çš„ç®—æ³•ï¼Œä»¥è®­ç»ƒå¹³å¦çš„ï¼ˆéå±‚æ¬¡ï¼‰ç›®æ ‡æ¡ä»¶ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç¦»çº¿GCRLç®—æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿æ—¶é—´ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä¸­ç”±äºç¨€ç–å¥–åŠ±å’ŒæŠ˜æ‰£å¯¼è‡´çš„ç­–ç•¥è®­ç»ƒå›°éš¾ï¼Œç°æœ‰å±‚æ¬¡æ–¹æ³•çš„å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨é«˜ç»´ç›®æ ‡ç©ºé—´çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å¯¼å­ç›®æ ‡æ¡ä»¶ç­–ç•¥ï¼Œåˆ©ç”¨ä¼˜åŠ¿åŠ æƒé‡è¦æ€§é‡‡æ ·æ¥è®­ç»ƒä¸€ä¸ªå¹³å¦çš„ç›®æ ‡æ¡ä»¶ç­–ç•¥ï¼Œä»è€Œç®€åŒ–æ¨¡å‹ç»“æ„å¹¶æé«˜æ‰©å±•æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬å¯¹å­ç›®æ ‡çš„æ¡ä»¶ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ä¼˜åŠ¿åŠ æƒé‡è¦æ€§é‡‡æ ·æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œé¿å…äº†å¯¹ç”Ÿæˆæ¨¡å‹çš„éœ€æ±‚ï¼Œé€‚åº”é«˜ç»´æ§åˆ¶ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºæ¶ˆé™¤äº†å¯¹ï¼ˆå­ï¼‰ç›®æ ‡ç©ºé—´ç”Ÿæˆæ¨¡å‹çš„éœ€æ±‚ï¼Œä½¿å¾—åœ¨å¤§çŠ¶æ€ç©ºé—´ä¸­çš„æ‰©å±•æˆä¸ºå¯èƒ½ï¼Œä¸ç°æœ‰å±‚æ¬¡æ–¹æ³•ç›¸æ¯”ï¼Œç®€åŒ–äº†ç­–ç•¥è®¾è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•å®ç°ä¸­ï¼Œé‡‡ç”¨äº†ä¼˜åŠ¿åŠ æƒé‡è¦æ€§é‡‡æ ·ä½œä¸ºæ ¸å¿ƒæŠ€æœ¯ï¼Œè®¾è®¡äº†é€‚åº”é«˜ç»´æ§åˆ¶çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œç¡®ä¿äº†ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šé¡¹çŠ¶æ€å’Œåƒç´ åŸºç¡€çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½ä¸ç°æœ‰æœ€å…ˆè¿›çš„ç¦»çº¿GCRLç®—æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå°¤å…¶åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰é«˜ç»´å¤æ‚ä»»åŠ¡åœºæ™¯ã€‚é€šè¿‡ç®€åŒ–ç­–ç•¥è®­ç»ƒè¿‡ç¨‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿåœ¨é•¿æ—¶é—´ç›®æ ‡è¾¾æˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail. Project page: https://johnlyzhou.github.io/saw/

