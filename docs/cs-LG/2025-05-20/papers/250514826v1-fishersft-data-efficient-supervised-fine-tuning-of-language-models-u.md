---
layout: default
title: "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain"
---

# FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14826" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14826v1</a>
  <a href="https://arxiv.org/pdf/2505.14826.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14826v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14826v1', 'FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rohan Deb, Kiran Thekumparampil, Kousha Kalantari, Gaurush Hiranandani, Shoham Sabach, Branislav Kveton

**åˆ†ç±»**: cs.LG, cs.CL, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFisherSFTä»¥æé«˜è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›‘ç£å¾®è°ƒ` `ä¿¡æ¯å¢ç›Š` `è¯­è¨€æ¨¡å‹` `ç»Ÿè®¡æ•ˆç‡` `å¤šé¡¹å¼é€»è¾‘å›å½’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨è®­ç»ƒç¤ºä¾‹é€‰æ‹©ä¸Šæ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚
2. æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¿¡æ¯å¢ç›Šçš„ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šæ¥ä¼˜åŒ–è®­ç»ƒç¤ºä¾‹çš„é€‰æ‹©ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å¾®è°ƒæ•ˆæœï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”æ–°é¢†åŸŸçš„æ ‡å‡†æ–¹æ³•ã€‚æœ¬æ–‡é€šè¿‡é€‰æ‹©ä¿¡æ¯é‡ä¸°å¯Œçš„è®­ç»ƒç¤ºä¾‹ï¼Œæé«˜äº†SFTçš„ç»Ÿè®¡æ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å›ºå®šçš„è®­ç»ƒç¤ºä¾‹é¢„ç®—ä¸‹ï¼Œé€‰æ‹©æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šçš„ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨å¤šé¡¹å¼é€»è¾‘å›å½’æ¨¡å‹å¯¹LLMæœ€åä¸€å±‚è¿›è¡Œçº¿æ€§åŒ–ï¼Œæ¥é«˜æ•ˆè¿‘ä¼¼HessiançŸ©é˜µã€‚è¯¥æ–¹æ³•åœ¨è®¡ç®—ä¸Šé«˜æ•ˆã€å¯åˆ†æï¼Œå¹¶åœ¨å¤šä¸ªé—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä¸”é€šè¿‡å®šé‡ç»“æœå’ŒLLMè¯„ä¼°æ”¯æŒäº†æˆ‘ä»¬çš„ä¸»å¼ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨è®­ç»ƒç¤ºä¾‹é€‰æ‹©ä¸Šçš„ä½æ•ˆç‡é—®é¢˜ï¼Œå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹å’Œæ¨¡å‹æ€§èƒ½çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é€‰æ‹©æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šçš„è®­ç»ƒç¤ºä¾‹æ¥æé«˜ç»Ÿè®¡æ•ˆç‡ï¼Œä¿¡æ¯å¢ç›Šé€šè¿‡LLMçš„å¯¹æ•°ä¼¼ç„¶çš„HessiançŸ©é˜µè¿›è¡Œåº¦é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆç¡®å®šå›ºå®šçš„è®­ç»ƒç¤ºä¾‹é¢„ç®—ï¼Œç„¶åé€šè¿‡çº¿æ€§åŒ–LLMçš„æœ€åä¸€å±‚ï¼Œä½¿ç”¨å¤šé¡¹å¼é€»è¾‘å›å½’æ¨¡å‹æ¥é«˜æ•ˆè¿‘ä¼¼HessiançŸ©é˜µï¼Œæœ€åé€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„ç¤ºä¾‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºé€šè¿‡ä¿¡æ¯å¢ç›Šé€‰æ‹©è®­ç»ƒç¤ºä¾‹ï¼Œæ˜¾è‘—æé«˜äº†å¾®è°ƒçš„ç»Ÿè®¡æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—ä¸‹è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†å›ºå®šçš„è®­ç»ƒç¤ºä¾‹é¢„ç®—ï¼Œå¹¶é‡‡ç”¨å¤šé¡¹å¼é€»è¾‘å›å½’æ¨¡å‹æ¥è¿‘ä¼¼HessiançŸ©é˜µï¼Œç¡®ä¿äº†è®¡ç®—çš„é«˜æ•ˆæ€§å’Œå¯åˆ†ææ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFisherSFTæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»ŸSFTæ–¹æ³•ï¼Œä¿¡æ¯å¢ç›Šé€‰æ‹©çš„ç¤ºä¾‹èƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ï¼Œå¿«é€Ÿæœ‰æ•ˆåœ°å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Supervised fine-tuning (SFT) is a standard approach to adapting large language models (LLMs) to new domains. In this work, we improve the statistical efficiency of SFT by selecting an informative subset of training examples. Specifically, for a fixed budget of training examples, which determines the computational cost of fine-tuning, we determine the most informative ones. The key idea in our method is to select examples that maximize information gain, measured by the Hessian of the log-likelihood of the LLM. We approximate it efficiently by linearizing the LLM at the last layer using multinomial logistic regression models. Our approach is computationally efficient, analyzable, and performs well empirically. We demonstrate this on several problems, and back our claims with both quantitative results and an LLM evaluation.

