---
layout: default
title: Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity
---

# Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14884" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14884v3</a>
  <a href="https://arxiv.org/pdf/2505.14884.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14884v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14884v3', 'Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Susav Shrestha, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-11)

**å¤‡æ³¨**: NeurIPS 2025, 10 pages, 7 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/susavlsh10/Polar-Sparsity)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPolar Sparsityä»¥è§£å†³å¤§è§„æ¨¡LLMæ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†åŠ é€Ÿ` `ä¸Šä¸‹æ–‡ç¨€ç–æ€§` `é€‰æ‹©æ€§å¤´æ³¨æ„åŠ›` `GPUä¼˜åŒ–` `é«˜ååé‡` `ä½å»¶è¿Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§æ‰¹é‡æ¨ç†æ—¶ï¼ŒMLPå±‚çš„ç¨€ç–æ€§æ¶ˆå¤±ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚
2. æå‡ºPolar Sparsityï¼Œé€šè¿‡é€‰æ‹©æ€§å¤´æ³¨æ„åŠ›å’Œç¨€ç–æ„ŸçŸ¥GPUå†…æ ¸ï¼Œä¼˜åŒ–æ³¨æ„åŠ›å±‚çš„è®¡ç®—æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸åŒæ‰¹é‡å’Œåºåˆ—é•¿åº¦ä¸‹ï¼Œæ¨ç†é€Ÿåº¦æå‡é«˜è¾¾2.2å€ï¼Œä¸”ä¿æŒäº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å¯¹äºéœ€è¦é«˜ååé‡å’Œä½å»¶è¿Ÿçš„å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚ä¸Šä¸‹æ–‡ç¨€ç–æ€§å…è®¸æ¯ä¸ªtokenåŠ¨æ€æ¿€æ´»æ¨¡å‹å‚æ•°çš„ä¸€ä¸ªå°å­é›†ï¼Œä½†åœ¨å¤§æ‰¹é‡æƒ…å†µä¸‹æ— æ³•æ‰©å±•ã€‚æœ¬æ–‡æå‡ºPolar Sparsityï¼Œå¼ºè°ƒåœ¨æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦æ‰©å±•æ—¶ï¼Œç¨€ç–æ€§çš„é‡è¦æ€§ä»MLPå±‚è½¬å‘æ³¨æ„åŠ›å±‚ã€‚æˆ‘ä»¬å¼€å‘äº†é€‰æ‹©æ€§å¤´æ³¨æ„åŠ›å’Œç¡¬ä»¶é«˜æ•ˆçš„ç¨€ç–æ„ŸçŸ¥GPUå†…æ ¸ï¼Œå®ç°äº†åœ¨ä¸åŒæ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦ä¸‹ï¼Œå¯¹OPTã€LLaMA-2 & 3ã€Qwenã€Mistralç­‰æ¨¡å‹çš„ç«¯åˆ°ç«¯é€Ÿåº¦æå‡é«˜è¾¾2.2å€ï¼Œè€Œä¸å½±å“å‡†ç¡®æ€§ã€‚è¿™æ˜¯é¦–æ¬¡è¯æ˜ä¸Šä¸‹æ–‡ç¨€ç–æ€§å¯ä»¥æœ‰æ•ˆæ‰©å±•åˆ°å¤§æ‰¹é‡ï¼Œæ˜¾è‘—åŠ é€Ÿæ¨ç†ï¼Œæå¤§åœ°æé«˜äº†å¤§è§„æ¨¡é«˜ååé‡LLMéƒ¨ç½²ç³»ç»Ÿçš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„è®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤§æ‰¹é‡æƒ…å†µä¸‹ï¼ŒMLPå±‚çš„ç¨€ç–æ€§æ¶ˆå¤±ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹å’Œå»¶è¿Ÿå¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥Polar Sparsityï¼Œå¼ºè°ƒåœ¨æ‰©å±•æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦æ—¶ï¼Œæ³¨æ„åŠ›å±‚çš„ç¨€ç–æ€§ç›¸å¯¹ç¨³å®šï¼Œè€ŒMLPå±‚çš„ç¨€ç–æ€§åˆ™è¿…é€Ÿæ¶ˆå¤±ã€‚é€‰æ‹©æ€§å¤´æ³¨æ„åŠ›çš„è®¾è®¡ä½¿å¾—åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é€‰æ‹©æ€§å¤´æ³¨æ„åŠ›æ¨¡å—å’Œç¨€ç–æ„ŸçŸ¥GPUå†…æ ¸ã€‚é€‰æ‹©æ€§å¤´æ³¨æ„åŠ›æ¨¡å—æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€é€‰æ‹©æ¿€æ´»çš„æ³¨æ„åŠ›å¤´ï¼Œè€Œç¨€ç–æ„ŸçŸ¥GPUå†…æ ¸åˆ™ä¼˜åŒ–äº†è®¡ç®—è¿‡ç¨‹ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé¦–æ¬¡å±•ç¤ºäº†ä¸Šä¸‹æ–‡ç¨€ç–æ€§åœ¨å¤§æ‰¹é‡æ¨ç†ä¸­çš„æœ‰æ•ˆæ‰©å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³¨æ„åŠ›å±‚çš„åº”ç”¨ï¼Œä½¿å¾—æ¨ç†é€Ÿåº¦æ˜¾è‘—æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é€‰æ‹©æ€§å¤´æ³¨æ„åŠ›ä¸­ï¼Œè®¾è®¡äº†åŠ¨æ€æ¿€æ´»æœºåˆ¶ï¼Œç¡®ä¿åªæœ‰å¿…è¦çš„æ³¨æ„åŠ›å¤´è¢«æ¿€æ´»ã€‚åŒæ—¶ï¼ŒGPUå†…æ ¸çš„è®¾è®¡è€ƒè™‘äº†ç¨€ç–æ€§ï¼Œä¼˜åŒ–äº†å†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPolar Sparsityåœ¨ä¸åŒæ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦ä¸‹ï¼Œèƒ½å¤Ÿå®ç°é«˜è¾¾2.2å€çš„ç«¯åˆ°ç«¯é€Ÿåº¦æå‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è§„æ¨¡éƒ¨ç½²ä¸­çš„å®é™…ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡ï¼ŒPolar Sparsityèƒ½å¤Ÿæ”¯æŒå®æ—¶åº”ç”¨ï¼Œæ»¡è¶³é«˜ååé‡å’Œä½å»¶è¿Ÿçš„éœ€æ±‚ï¼Œæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to union of active neurons quickly approaching dense computation. We introduce Polar Sparsity, highlighting a key shift in sparsity importance from MLP to Attention layers as we scale batch size and sequence length. While MLP layers become more compute-efficient under batching, their sparsity vanishes. In contrast, attention becomes increasingly more expensive at scale, while their head sparsity remains stable and batch-invariant. We develop Selective Head Attention with hardware-efficient, sparsity-aware GPU kernels, delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2 \& 3, Qwen, Mistral across various batch sizes and sequence lengths without compromising accuracy. To our knowledge, this is the first work to demonstrate that contextual sparsity can scale effectively to large batch sizes, delivering substantial inference acceleration with minimal changes, making Polar Sparsity practical for large-scale, high-throughput LLM deployment systems. Our code is available at: https://github.com/susavlsh10/Polar-Sparsity.

