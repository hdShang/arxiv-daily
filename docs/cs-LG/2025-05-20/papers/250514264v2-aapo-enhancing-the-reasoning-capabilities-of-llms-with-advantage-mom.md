---
layout: default
title: AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage Momentum
---

# AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage Momentum

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14264" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14264v2</a>
  <a href="https://arxiv.org/pdf/2505.14264.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14264v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14264v2', 'AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage Momentum')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jian Xiong, Jingbo Zhou, Jingyong Ye, Qiang Huang, Dejing Dou

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-09-24)

**å¤‡æ³¨**: 18 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAAPOä»¥è§£å†³ç°æœ‰RLæ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›æå‡ä¸­çš„ä½æ•ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `ä¼˜åŠ¿ä¼°è®¡` `åŠ¨é‡ä¼˜åŒ–` `æ•°å­¦æ¨ç†` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•åœ¨è®­ç»ƒæ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¼˜åŠ¿ä¼°è®¡æ¥è¿‘é›¶æ—¶è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„AAPOç®—æ³•é€šè¿‡åŠ¨é‡å¢å¼ºçš„ä¼˜åŠ¿ä¼°è®¡æ¥ä¼˜åŒ–äº¤å‰ç†µæŸå¤±ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAAPOåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å› é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®æœ‰é™è€Œå—é™çš„åœºæ™¯ä¸­ã€‚ç°æœ‰çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•ï¼Œå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè™½ç„¶ç®€åŒ–äº†è®­ç»ƒï¼Œä½†ä»å­˜åœ¨è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¼˜åŠ¿å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼ˆAAPOï¼‰ï¼Œä¸€ç§æ–°é¢–çš„RLç®—æ³•ï¼Œé€šè¿‡åŠ¨é‡ä¼°è®¡æ–¹æ¡ˆä¼˜åŒ–äº¤å‰ç†µæŸå¤±ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£äº†ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡çš„ä½æ•ˆæ€§ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAAPOè¡¨ç°ä¼˜è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•åœ¨è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¼˜åŠ¿æ¥è¿‘é›¶çš„æƒ…å†µä¸‹ï¼Œå¯¼è‡´çš„æ¨ç†èƒ½åŠ›æå‡å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAAPOç®—æ³•é€šè¿‡å¼•å…¥åŠ¨é‡ä¼°è®¡æ–¹æ¡ˆæ¥å¢å¼ºä¼˜åŠ¿ä¼°è®¡ï¼Œä»è€Œä¼˜åŒ–äº¤å‰ç†µæŸå¤±ï¼Œæå‡è®­ç»ƒæ•ˆç‡å’Œæ¨ç†èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å…‹æœä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹ç¨€ç¼ºæ•°æ®æ—¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAAPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¼˜åŠ¿ä¼°è®¡æ¨¡å—å’Œäº¤å‰ç†µæŸå¤±ä¼˜åŒ–æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åŠ¨é‡æœºåˆ¶è®¡ç®—ä¼˜åŠ¿ï¼Œç„¶åå°†å…¶åº”ç”¨äºäº¤å‰ç†µæŸå¤±çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå®ç°é«˜æ•ˆçš„ç­–ç•¥æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šAAPOçš„ä¸»è¦åˆ›æ–°åœ¨äºåŠ¨é‡å¢å¼ºçš„ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—ç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œä¸ä¼ ç»Ÿçš„PPOç­‰æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æœ¬è´¨ä¸Šçš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨AAPOä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬åŠ¨é‡ç³»æ•°çš„è®¾ç½®ï¼Œä»¥åŠäº¤å‰ç†µæŸå¤±çš„å…·ä½“å®ç°æ–¹å¼ã€‚é€šè¿‡åˆç†çš„å‚æ•°è°ƒæ•´ï¼Œç®—æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¥½çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAAPOç®—æ³•çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†èƒ½åŠ›æå‡æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AAPOç®—æ³•çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ¨ç†ç­‰ã€‚å…¶æå‡çš„æ¨ç†èƒ½åŠ›å¯ä»¥ä¸ºå¤æ‚é—®é¢˜çš„è§£å†³æä¾›æ›´ä¸ºæœ‰æ•ˆçš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼ŒAAPOæœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸä¸­å®ç°æ™ºèƒ½åŒ–çš„æ¨ç†ä¸å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has emerged as an effective approach for enhancing the reasoning capabilities of large language models (LLMs), especially in scenarios where supervised fine-tuning (SFT) falls short due to limited chain-of-thought (CoT) data. Among RL-based post-training methods, group relative advantage estimation, as exemplified by Group Relative Policy Optimization (GRPO), has attracted considerable attention for eliminating the dependency on the value model, thereby simplifying training compared to traditional approaches like Proximal Policy Optimization (PPO). However, we observe that exsiting group relative advantage estimation method still suffers from training inefficiencies, particularly when the estimated advantage approaches zero. To address this limitation, we propose Advantage-Augmented Policy Optimization (AAPO), a novel RL algorithm that optimizes the cross-entropy (CE) loss using advantages enhanced through a momentum-based estimation scheme. This approach effectively mitigates the inefficiencies associated with group relative advantage estimation. Experimental results on multiple mathematical reasoning benchmarks demonstrate the superior performance of AAPO.

