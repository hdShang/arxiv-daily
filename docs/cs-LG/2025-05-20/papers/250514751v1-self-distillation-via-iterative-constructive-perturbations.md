---
layout: default
title: Self Distillation via Iterative Constructive Perturbations
---

# Self Distillation via Iterative Constructive Perturbations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14751" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14751v1</a>
  <a href="https://arxiv.org/pdf/2505.14751.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14751v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14751v1', 'Self Distillation via Iterative Constructive Perturbations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Maheak Dave, Aniket Kumar Singh, Aryan Pareek, Harshita Jha, Debasis Chaudhuri, Manish Pratap Singh

**åˆ†ç±»**: cs.LG, cs.AI, cs.ET

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¾ªç¯ä¼˜åŒ–æ¡†æ¶ä»¥æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å­¦ä¹ ` `è‡ªè’¸é¦` `å¾ªç¯ä¼˜åŒ–` `è¿­ä»£æ„é€ æ‰°åŠ¨` `æ¨¡å‹æ³›åŒ–` `è®¡ç®—æœºè§†è§‰` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éš¾ä»¥å¹³è¡¡æ¨¡å‹æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„å¾ªç¯ä¼˜åŒ–æ¡†æ¶é€šè¿‡è¿­ä»£æ„é€ æ‰°åŠ¨ï¼ˆICPï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–è¾“å…¥æ•°æ®ä¸æ¨¡å‹å‚æ•°ï¼Œé‡æ–°æ€è€ƒä¼ ç»Ÿè®­ç»ƒæ–¹å¼ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†ç¥ç»ç½‘ç»œçš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶åœ¨å¤šç§è®­ç»ƒå˜ä½“ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé‡‡ç”¨å¾ªç¯ä¼˜åŒ–ç­–ç•¥åŒæ—¶ä¼˜åŒ–æ¨¡å‹åŠå…¶è¾“å…¥æ•°æ®ï¼Œä»¥æ”¹å–„è®­ç»ƒæ•ˆæœã€‚æ ¸å¿ƒæ–¹æ³•æ˜¯è¿­ä»£æ„é€ æ‰°åŠ¨ï¼ˆICPï¼‰ï¼Œé€šè¿‡æ¨¡å‹æŸå¤±è¿­ä»£æ‰°åŠ¨è¾“å…¥ï¼Œé€æ­¥æ„å»ºå¢å¼ºè¡¨ç¤ºã€‚è¯¥è¾“å…¥åé¦ˆè‡³æ¨¡å‹ç”Ÿæˆæ”¹è¿›çš„ä¸­é—´ç‰¹å¾ï¼Œä½œä¸ºè‡ªè’¸é¦æ¡†æ¶ä¸­çš„ç›®æ ‡ï¼Œä¸åŸå§‹ç‰¹å¾è¿›è¡Œå¯¹æ¯”ã€‚é€šè¿‡äº¤æ›¿è°ƒæ•´æ¨¡å‹å‚æ•°ä¸æ•°æ®ï¼Œæœ¬æ–‡æœ‰æ•ˆç¼©å°äº†æ‹Ÿåˆä¸æ³›åŒ–ä¹‹é—´çš„å·®è·ï¼Œæå‡äº†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…ç¼“è§£äº†ç¥ç»ç½‘ç»œå¸¸è§çš„æ€§èƒ½ç“¶é¢ˆï¼Œè¿˜åœ¨å¤šç§è®­ç»ƒå˜ä½“ä¸­æ˜¾è‘—æå‡äº†æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„çŸ›ç›¾ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåº”å¯¹æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„å¾ªç¯ä¼˜åŒ–æ¡†æ¶é€šè¿‡è¿­ä»£æ„é€ æ‰°åŠ¨ï¼ˆICPï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ¨¡å‹æŸå¤±è¿­ä»£æ‰°åŠ¨è¾“å…¥æ•°æ®ï¼Œä»è€Œé€æ­¥æ„å»ºæ›´ä¼˜çš„è¾“å…¥è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•é€šè¿‡äº¤æ›¿ä¼˜åŒ–æ¨¡å‹å‚æ•°ä¸è¾“å…¥æ•°æ®ï¼Œæ—¨åœ¨ç¼©å°æ‹Ÿåˆä¸æ³›åŒ–ä¹‹é—´çš„å·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯è¿­ä»£æ„é€ æ‰°åŠ¨æ¨¡å—ï¼Œé€šè¿‡æ¨¡å‹æŸå¤±åé¦ˆä¸æ–­ä¼˜åŒ–è¾“å…¥ï¼›äºŒæ˜¯è‡ªè’¸é¦æ¨¡å—ï¼Œå°†æ”¹è¿›çš„ä¸­é—´ç‰¹å¾ä½œä¸ºç›®æ ‡ï¼Œä¸åŸå§‹ç‰¹å¾è¿›è¡Œå¯¹æ¯”ï¼Œä¿ƒè¿›æ¨¡å‹å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è¿­ä»£æ„é€ æ‰°åŠ¨ï¼ˆICPï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨æ¨¡å‹æŸå¤±åŠ¨æ€è°ƒæ•´è¾“å…¥æ•°æ®ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„é™æ€è®­ç»ƒæ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥æŒ‡å¯¼è¾“å…¥æ‰°åŠ¨ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†è‡ªè’¸é¦æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°æ›´å…·æ³›åŒ–èƒ½åŠ›çš„ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šæå‡äº†çº¦5%-10%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè®­ç»ƒå˜ä½“ä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸç¼“è§£äº†å¸¸è§çš„æ€§èƒ½ç“¶é¢ˆï¼Œå±•ç°äº†è‰¯å¥½çš„é€‚åº”æ€§ä¸ç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†åŠå…¶ä»–éœ€è¦æ·±åº¦å­¦ä¹ çš„ä»»åŠ¡ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æé«˜æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¯é æ€§ä¸å®ç”¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å¤šç§æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä¿ƒè¿›æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep Neural Networks have achieved remarkable achievements across various domains, however balancing performance and generalization still remains a challenge while training these networks. In this paper, we propose a novel framework that uses a cyclic optimization strategy to concurrently optimize the model and its input data for better training, rethinking the traditional training paradigm. Central to our approach is Iterative Constructive Perturbation (ICP), which leverages the model's loss to iteratively perturb the input, progressively constructing an enhanced representation over some refinement steps. This ICP input is then fed back into the model to produce improved intermediate features, which serve as a target in a self-distillation framework against the original features. By alternately altering the model's parameters to the data and the data to the model, our method effectively addresses the gap between fitting and generalization, leading to enhanced performance. Extensive experiments demonstrate that our approach not only mitigates common performance bottlenecks in neural networks but also demonstrates significant improvements across training variations.

