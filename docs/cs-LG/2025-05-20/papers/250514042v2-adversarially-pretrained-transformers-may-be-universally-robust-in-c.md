---
layout: default
title: Adversarially Pretrained Transformers May Be Universally Robust In-Context Learners
---

# Adversarially Pretrained Transformers May Be Universally Robust In-Context Learners

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14042" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14042v2</a>
  <a href="https://arxiv.org/pdf/2505.14042.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14042v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14042v2', 'Adversarially Pretrained Transformers May Be Universally Robust In-Context Learners')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki

**åˆ†ç±»**: cs.LG, cs.CV, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-12-10)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/s-kumano/universally-robust-in-context-learner)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æŠ—é¢„è®­ç»ƒå˜æ¢å™¨ä»¥è§£å†³è½»é‡çº§é²æ£’æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æŠ—è®­ç»ƒ` `å˜æ¢å™¨` `é²æ£’æ€§` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `åŸºç¡€æ¨¡å‹` `æœºå™¨å­¦ä¹ ` `åˆ†ç±»ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹æŠ—è®­ç»ƒæ–¹æ³•åœ¨é˜²å¾¡å¯¹æŠ—æ”»å‡»æ—¶è®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥å¹¿æ³›åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºå¯¹æŠ—é¢„è®­ç»ƒçš„å˜æ¢å™¨ä½œä¸ºé€šç”¨é²æ£’åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿè½»é‡è°ƒä¼˜é€‚åº”å¤šç§ä»»åŠ¡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¯¹æŠ—é¢„è®­ç»ƒçš„å•å±‚çº¿æ€§å˜æ¢å™¨åœ¨æœªè§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·å¤‡è‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æŠ—è®­ç»ƒæ˜¯æœ‰æ•ˆçš„å¯¹æŠ—é˜²å¾¡æ–¹æ³•ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡é¦–æ¬¡ç†è®ºåˆ†æè¡¨æ˜ï¼Œå¯¹æŠ—é¢„è®­ç»ƒçš„å˜æ¢å™¨å¯ä»¥ä½œä¸ºé€šç”¨é²æ£’åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡è½»é‡è°ƒä¼˜é€‚åº”å¤šæ ·çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç»è¿‡å¤šç§åˆ†ç±»ä»»åŠ¡å¯¹æŠ—é¢„è®­ç»ƒçš„å•å±‚çº¿æ€§å˜æ¢å™¨ï¼Œèƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ä»å¹²å‡€ç¤ºä¾‹ä¸­é²æ£’åœ°æ¨å¹¿åˆ°æœªè§çš„åˆ†ç±»ä»»åŠ¡ã€‚è¿™ç§é€šç”¨é²æ£’æ€§æºäºæ¨¡å‹åœ¨ç»™å®šä»»åŠ¡ä¸­è‡ªé€‚åº”èšç„¦äºé²æ£’ç‰¹å¾çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æŒ‡å‡ºäº†å®ç°é²æ£’æ€§çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå‡†ç¡®æ€§ä¸é²æ£’æ€§çš„æƒè¡¡ï¼Œä»¥åŠå¯¹æ ·æœ¬çš„é«˜éœ€æ±‚ã€‚å°½ç®¡è®­ç»ƒæˆæœ¬é«˜ï¼Œä½†æŠ•èµ„æ˜¯å€¼å¾—çš„ï¼Œå› ä¸ºä¸‹æ¸¸ä»»åŠ¡å¯ä»¥äº«å—å…è´¹çš„å¯¹æŠ—é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯¹æŠ—è®­ç»ƒæ–¹æ³•åœ¨è®¡ç®—æˆæœ¬å’Œé€‚åº”æ€§ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ ·åŒ–ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é²æ£’æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¯¹æŠ—é¢„è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹ï¼Œåˆ©ç”¨å…¶è‡ªé€‚åº”èšç„¦äºé²æ£’ç‰¹å¾çš„èƒ½åŠ›ï¼Œæ¥å®ç°å¯¹å¤šç§åˆ†ç±»ä»»åŠ¡çš„é²æ£’é€‚åº”ï¼Œè€Œæ— éœ€é¢å¤–çš„å¯¹æŠ—è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹æŠ—é¢„è®­ç»ƒé˜¶æ®µå’Œä¸Šä¸‹æ–‡å­¦ä¹ é˜¶æ®µã€‚æ¨¡å‹é¦–å…ˆåœ¨å¤šç§åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œå¯¹æŠ—é¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡å¹²å‡€ç¤ºä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å•å±‚çº¿æ€§å˜æ¢å™¨ä½œä¸ºé€šç”¨é²æ£’åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœªè§ä»»åŠ¡ä¸Šè¿›è¡Œæœ‰æ•ˆçš„é²æ£’æ¨å¹¿ï¼Œæ˜¾è‘—é™ä½äº†å¯¹æŠ—è®­ç»ƒçš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­é‡‡ç”¨äº†ç®€å•çš„çº¿æ€§ç»“æ„ï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥å¢å¼ºé²æ£’æ€§ï¼Œå¹¶é€šè¿‡å¤šæ ·åŒ–çš„å¯¹æŠ—æ ·æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¯¹æŠ—é¢„è®­ç»ƒçš„å•å±‚çº¿æ€§å˜æ¢å™¨åœ¨æœªè§åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œé²æ£’æ€§æ˜¾è‘—æå‡ï¼Œä¸”åœ¨å¤šä¸ªåŸºçº¿ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ä¹‹é—´çš„æƒè¡¡å¾—åˆ°äº†æœ‰æ•ˆæ”¹å–„ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒåˆ†ç±»ç­‰å¤šä¸ªæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹æŠ—æ”»å‡»é¢‘å‘çš„åœºæ™¯ä¸­ï¼Œèƒ½å¤Ÿæä¾›æ›´é«˜çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹å¯èƒ½åœ¨å®‰å…¨æ€§è¦æ±‚é«˜çš„åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨å¯¹æŠ—é˜²å¾¡æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Adversarial training is one of the most effective adversarial defenses, but it incurs a high computational cost. In this study, we present the first theoretical analysis suggesting that adversarially pretrained transformers can serve as universally robust foundation models -- models that can robustly adapt to diverse downstream tasks with only lightweight tuning. Specifically, we demonstrate that single-layer linear transformers, after adversarial pretraining across a variety of classification tasks, can robustly generalize to unseen classification tasks through in-context learning from clean demonstrations (i.e., without requiring additional adversarial training or examples). This universal robustness stems from the model's ability to adaptively focus on robust features within given tasks. We also show the two open challenges for attaining robustness: accuracy--robustness trade-off and sample-hungry training. This study initiates the discussion on the utility of universally robust foundation models. While their training is expensive, the investment would prove worthwhile as downstream tasks can enjoy free adversarial robustness. The code is available at https://github.com/s-kumano/universally-robust-in-context-learner.

