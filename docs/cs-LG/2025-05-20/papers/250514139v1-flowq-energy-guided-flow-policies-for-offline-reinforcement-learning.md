---
layout: default
title: FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning
---

# FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14139" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14139v1</a>
  <a href="https://arxiv.org/pdf/2505.14139.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14139v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14139v1', 'FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Marvin Alles, Nutan Chen, Patrick van der Smagt, Botond Cseke

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlowQä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„æŒ‡å¯¼é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `èƒ½é‡å¼•å¯¼` `æµåŒ¹é…` `æ‰©æ•£æ¨¡å‹` `æ¡ä»¶é€Ÿåº¦åœº` `å¤šæ¨¡æ€ç”Ÿæˆ` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹æœ‰æ•ˆçš„æŒ‡å¯¼æœºåˆ¶ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„è´¨é‡å’Œå¤šæ ·æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„èƒ½é‡å¼•å¯¼æµåŒ¹é…æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ æ¡ä»¶é€Ÿåº¦åœºï¼Œä¼˜åŒ–æµç­–ç•¥ï¼Œä»è€Œåœ¨æ¨ç†æ—¶ä¸å†ä¾èµ–å¤–éƒ¨æŒ‡å¯¼ã€‚
3. FlowQç®—æ³•åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒæ—¶é—´ä¸æµé‡‡æ ·æ­¥éª¤æ•°é‡æ— å…³ï¼Œä¸”åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå–å¾—äº†ç«äº‰æ€§æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒæŒ‡å¯¼é‡‡æ ·ä»¥å®ç°æœŸæœ›ç»“æœçš„åº”ç”¨å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œç„¶è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æŒ‡å¯¼çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„èƒ½é‡å¼•å¯¼æµåŒ¹é…æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæµæ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†æ—¶æ¶ˆé™¤å¯¹æŒ‡å¯¼çš„éœ€æ±‚ã€‚é€šè¿‡å°†èƒ½é‡å¼•å¯¼æ¦‚ç‡è·¯å¾„è¿‘ä¼¼ä¸ºé«˜æ–¯è·¯å¾„ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä¸æµç­–ç•¥ç›¸å¯¹åº”çš„æ¡ä»¶é€Ÿåº¦åœºã€‚è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºç›®æ ‡åˆ†å¸ƒç”±æ•°æ®å’Œèƒ½é‡å‡½æ•°ç»„åˆå®šä¹‰çš„ä»»åŠ¡ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºçš„FlowQæ˜¯ä¸€ç§åŸºäºèƒ½é‡å¼•å¯¼æµåŒ¹é…çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨æµé‡‡æ ·æ­¥éª¤æ•°é‡ä¸å˜çš„æƒ…å†µä¸‹å®ç°ç«äº‰æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹æœ‰æ•ˆæŒ‡å¯¼çš„é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„è´¨é‡å’Œå¤šæ ·æ€§ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºåŠ æƒç›®æ ‡æˆ–é€šè¿‡é‡‡æ ·åŠ¨ä½œåå‘ä¼ æ’­æ¢¯åº¦ï¼Œæ•ˆç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºèƒ½é‡å¼•å¯¼æµåŒ¹é…ï¼Œé€šè¿‡å­¦ä¹ æ¡ä»¶é€Ÿåº¦åœºæ¥ä¼˜åŒ–æµç­–ç•¥ï¼Œé¿å…åœ¨æ¨ç†é˜¶æ®µä¾èµ–å¤–éƒ¨æŒ‡å¯¼ï¼Œä»è€Œæå‡æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬èƒ½é‡å¼•å¯¼æµåŒ¹é…æ¨¡å—å’Œæµç­–ç•¥å­¦ä¹ æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡èƒ½é‡å‡½æ•°å®šä¹‰ç›®æ ‡åˆ†å¸ƒï¼Œç„¶åå­¦ä¹ ä¸ä¹‹å¯¹åº”çš„æ¡ä»¶é€Ÿåº¦åœºï¼Œæœ€åä¼˜åŒ–æµç­–ç•¥ä»¥å®ç°é«˜æ•ˆçš„é‡‡æ ·ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥èƒ½é‡å¼•å¯¼çš„æµåŒ¹é…æ–¹æ³•ï¼Œä½¿å¾—åœ¨æ¨ç†æ—¶ä¸å†éœ€è¦å¤–éƒ¨æŒ‡å¯¼ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é«˜æ–¯è·¯å¾„è¿‘ä¼¼æ¥å­¦ä¹ æ¡ä»¶é€Ÿåº¦åœºï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆèƒ½é‡å‡½æ•°å’Œæ•°æ®åˆ†å¸ƒçš„åŠ æƒç›®æ ‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚æ•´ä½“ç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šï¼ŒFlowQç®—æ³•å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè®­ç»ƒæ—¶é—´ä¿æŒä¸å˜ï¼Œä¸”åœ¨æ ·æœ¬æ•ˆç‡å’Œç”Ÿæˆè´¨é‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªå…¬å¼€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆæ™ºèƒ½ä½“ç­‰ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„å†³ç­–å’ŒåŠ¨ä½œç”Ÿæˆã€‚é€šè¿‡ä¼˜åŒ–æµç­–ç•¥ï¼ŒFlowQå¯ä¸ºå¤šæ¨¡æ€ä»»åŠ¡æä¾›æ›´çµæ´»çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The use of guidance to steer sampling toward desired outcomes has been widely explored within diffusion models, especially in applications such as image and trajectory generation. However, incorporating guidance during training remains relatively underexplored. In this work, we introduce energy-guided flow matching, a novel approach that enhances the training of flow models and eliminates the need for guidance at inference time. We learn a conditional velocity field corresponding to the flow policy by approximating an energy-guided probability path as a Gaussian path. Learning guided trajectories is appealing for tasks where the target distribution is defined by a combination of data and an energy function, as in reinforcement learning. Diffusion-based policies have recently attracted attention for their expressive power and ability to capture multi-modal action distributions. Typically, these policies are optimized using weighted objectives or by back-propagating gradients through actions sampled by the policy. As an alternative, we propose FlowQ, an offline reinforcement learning algorithm based on energy-guided flow matching. Our method achieves competitive performance while the policy training time is constant in the number of flow sampling steps.

