---
layout: default
title: Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining
---

# Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.08022" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.08022v3</a>
  <a href="https://arxiv.org/pdf/2506.08022.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.08022v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.08022v3', 'Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenxi Liu, Tianyi Xiong, Yanshuo Chen, Ruibo Chen, Yihan Wu, Junfeng Guo, Tianyi Zhou, Heng Huang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€å¹³è¡¡åå¥½ä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³æ¨¡æ€å¤±è¡¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `åå¥½ä¼˜åŒ–` `æ¨¡æ€å¤±è¡¡` `å¯¹æŠ—æ€§å­¦ä¹ ` `è§†è§‰-è¯­è¨€ä»»åŠ¡` `åœ¨çº¿å­¦ä¹ ` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•æœªèƒ½æœ‰æ•ˆæŠ‘åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒæ•°æ®ä¸­çš„å†…éƒ¨åè§ï¼Œå¯¼è‡´æ¨¡æ€å¤±è¡¡é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¨¡æ€å¹³è¡¡åå¥½ä¼˜åŒ–ï¼ˆMBPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¯¹æŠ—æ€§æ‰°åŠ¨ç”Ÿæˆéš¾è´Ÿæ ·æœ¬ï¼Œæ„å»ºæ›´æœ‰æ•ˆçš„ç¦»çº¿åå¥½æ•°æ®é›†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMBPOåœ¨æŒ‘æˆ˜æ€§çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†å¹»è§‰ç°è±¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„ä»»åŠ¡é€‚åº”æ€§å’Œå¯¹é½èƒ½åŠ›å·²é€šè¿‡æŒ‡ä»¤è°ƒä¼˜å’Œåå¥½ä¼˜åŒ–æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰LMMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»é¢ä¸´æ¨¡æ€å¤±è¡¡çš„é—®é¢˜ï¼Œå¯¼è‡´è¯­è¨€ä¼˜å…ˆåè§è¶…è¿‡è§†è§‰è¾“å…¥ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›å¹¶å¼•å‘å¹»è§‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åå¥½å­¦ä¹ æ¡†æ¶â€”â€”æ¨¡æ€å¹³è¡¡åå¥½ä¼˜åŒ–ï¼ˆMBPOï¼‰ï¼Œé€šè¿‡ç”Ÿæˆå¯¹æŠ—æ€§è´Ÿæ ·æœ¬æ¥æ„å»ºæ›´æœ‰æ•ˆçš„ç¦»çº¿åå¥½æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨åœ¨çº¿ç”Ÿæˆçš„å“åº”è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒMBPOåœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†LMMçš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†å¹»è§‰ç°è±¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„æ¨¡æ€å¤±è¡¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæŠ‘åˆ¶LLMçš„å†…éƒ¨åè§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è§†è§‰è¾“å…¥ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ¨¡æ€å¹³è¡¡åå¥½ä¼˜åŒ–ï¼ˆMBPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå¯¹æŠ—æ€§è´Ÿæ ·æœ¬æ¥å¢å¼ºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶ç»“åˆåœ¨çº¿ç”Ÿæˆçš„å“åº”è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»¥æ”¹å–„æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMBPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡å¯¹æŠ—æ€§æ‰°åŠ¨ç”Ÿæˆéš¾è´Ÿæ ·æœ¬ï¼Œæ„å»ºç¦»çº¿åå¥½æ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨åœ¨çº¿ç”Ÿæˆçš„å“åº”è¿›è¡Œè®­ç»ƒï¼Œç»“åˆGRPOæ–¹æ³•è¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šMBPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç”Ÿæˆå¯¹æŠ—æ€§è´Ÿæ ·æœ¬çš„èƒ½åŠ›ï¼Œè¿™ä¸€æ–¹æ³•æœ‰æ•ˆåœ°å¢å¼ºäº†è®­ç»ƒæ•°æ®çš„ä»£è¡¨æ€§ï¼Œä¸ä¼ ç»Ÿçš„åå¥½ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹æ¨¡æ€å¤±è¡¡é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MBPOä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å¯¹æŠ—æ€§æ‰°åŠ¨çš„å¼ºåº¦å’Œç”Ÿæˆè´Ÿæ ·æœ¬çš„ç­–ç•¥ï¼ŒåŒæ—¶é‡‡ç”¨æ˜“äºéªŒè¯çš„å°é—­å¼ä»»åŠ¡æ¥ç”Ÿæˆåœ¨çº¿å“åº”ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„åé¦ˆæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMBPOåœ¨å¤šä¸ªè§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šå‡†ç¡®ç‡æé«˜äº†10%ä»¥ä¸Šï¼ŒåŒæ—¶æœ‰æ•ˆå‡å°‘äº†å¹»è§‰ç°è±¡çš„å‘ç”Ÿï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆå’Œå¤šæ¨¡æ€æ£€ç´¢ç­‰ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ¨¡æ€æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼ŒMBPOæ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„å¤šæ¨¡æ€åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations.

