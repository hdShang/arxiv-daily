---
layout: default
title: ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs
---

# ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14468" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14468v1</a>
  <a href="https://arxiv.org/pdf/2505.14468.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14468v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14468v1', 'ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Sui, Hao Wang, Hanfei Yu, Yitao Hu, Jianxun Li, Hao Wang

**åˆ†ç±»**: cs.LG, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºServerlessLoRAä»¥è§£å†³LoRA LLMæ¨ç†ä¸­çš„å»¶è¿Ÿä¸æˆæœ¬é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— æœåŠ¡å™¨è®¡ç®—` `ä½ç§©é€‚åº”` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨ç†ä¼˜åŒ–` `GPUèµ„æºç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— æœåŠ¡å™¨æ¶æ„åœ¨å¤„ç†LoRAæ¨ç†æ—¶å­˜åœ¨å‚æ•°å†—ä½™ã€åŠ è½½å»¶è¿Ÿå’Œèµ„æºç«äº‰ç­‰é—®é¢˜ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. ServerlessLoRAé€šè¿‡å…±äº«åç«¯LLMã€é¢„åŠ è½½LoRAå·¥ä»¶å’Œèµ„æºç«äº‰æ„ŸçŸ¥çš„æ‰¹å¤„ç†æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒServerlessLoRAåœ¨TTFTå’Œæˆæœ¬ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æ–¹æ¡ˆæå‡äº†86%å’Œ89%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ— æœåŠ¡å™¨è®¡ç®—å› å…¶æŒ‰éœ€ä»˜è´¹ã€ç»†ç²’åº¦GPUä½¿ç”¨å’Œå¿«é€Ÿæ‰©å±•è€Œè¿…é€Ÿå‘å±•ï¼Œæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„çƒ­é—¨é€‰æ‹©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— æœåŠ¡å™¨æ¶æ„åœ¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨ç†ä¸­å­˜åœ¨ä¸‰å¤§å…³é”®é™åˆ¶ï¼š1ï¼‰å‡½æ•°é—´å­˜åœ¨å¤§é‡å‚æ•°å†—ä½™ï¼Œ99%çš„æƒé‡è¢«ä¸å¿…è¦åœ°é‡å¤ï¼›2ï¼‰åŠ è½½å»¶è¿Ÿé«˜äºLLMåŠ è½½çš„æˆæœ¬ï¼›3ï¼‰åœ¨æœåŠ¡å¤šä¸ªLoRA LLMæ—¶èµ„æºç«äº‰åŠ å‰§ã€‚è¿™äº›ä½æ•ˆå¯¼è‡´GPUæµªè´¹ã€é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰å¢åŠ å’Œé«˜æ˜‚çš„æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ServerlessLoRAï¼Œä¸€ä¸ªæ—¨åœ¨åŠ é€Ÿå’Œé™ä½LoRA LLMæœåŠ¡æˆæœ¬çš„æ— æœåŠ¡å™¨æ¨ç†ç³»ç»Ÿã€‚ServerlessLoRAé€šè¿‡å®‰å…¨å…±äº«åç«¯LLMæ¥å‡å°‘å†—ä½™ï¼Œè®¾è®¡äº†é¢„åŠ è½½æ–¹æ³•ä»¥æœ€å°åŒ–å†·å¯åŠ¨å»¶è¿Ÿï¼Œå¹¶é‡‡ç”¨èµ„æºç«äº‰æ„ŸçŸ¥çš„æ‰¹å¤„ç†å’Œå¸è½½ç­–ç•¥æ¥ç¼“è§£GPUèµ„æºå†²çªã€‚å®éªŒè¡¨æ˜ï¼ŒServerlessLoRAç›¸æ¯”äºç°æœ‰çš„LLMæ¨ç†è§£å†³æ–¹æ¡ˆï¼ŒTTFTå‡å°‘äº†å¤šè¾¾86%ï¼Œæˆæœ¬é™ä½äº†å¤šè¾¾89%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ— æœåŠ¡å™¨æ¨ç†åœ¨å¤„ç†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ—¶çš„æ•ˆç‡é—®é¢˜ï¼Œä¸»è¦ç—›ç‚¹åŒ…æ‹¬å‚æ•°å†—ä½™ã€åŠ è½½å»¶è¿Ÿå’Œèµ„æºç«äº‰ç­‰ï¼Œå¯¼è‡´GPUèµ„æºæµªè´¹å’Œé«˜æˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„ServerlessLoRAç³»ç»Ÿé€šè¿‡å…±äº«åç«¯LLMæ¥å‡å°‘å†—ä½™ï¼Œè®¾è®¡é¢„åŠ è½½æœºåˆ¶ä»¥é™ä½å†·å¯åŠ¨å»¶è¿Ÿï¼Œå¹¶é‡‡ç”¨èµ„æºç«äº‰æ„ŸçŸ¥çš„æ‰¹å¤„ç†ç­–ç•¥æ¥ä¼˜åŒ–GPUèµ„æºçš„ä½¿ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šServerlessLoRAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰åç«¯LLMå…±äº«æ¨¡å—ï¼Œè´Ÿè´£ç®¡ç†å’Œå…±äº«LLMæƒé‡ï¼›2ï¼‰é¢„åŠ è½½æ¨¡å—ï¼Œæå‰åŠ è½½LoRAå·¥ä»¶ä»¥å‡å°‘å†·å¯åŠ¨æ—¶é—´ï¼›3ï¼‰èµ„æºç«äº‰æ„ŸçŸ¥æ¨¡å—ï¼ŒåŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†ç­–ç•¥ä»¥å‡å°‘èµ„æºå†²çªã€‚

**å…³é”®åˆ›æ–°**ï¼šServerlessLoRAçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡å®‰å…¨å…±äº«åç«¯LLMå’Œé¢„åŠ è½½æœºåˆ¶æ˜¾è‘—å‡å°‘äº†å‚æ•°å†—ä½™å’ŒåŠ è½½å»¶è¿Ÿï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç‹¬ç«‹åŠ è½½å’Œå¤„ç†æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒServerlessLoRAé‡‡ç”¨äº†åŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ï¼Œæ ¹æ®å®æ—¶è´Ÿè½½è°ƒæ•´GPUèµ„æºåˆ†é…ï¼Œå¹¶ä½¿ç”¨é«˜æ•ˆçš„ç¼“å­˜æœºåˆ¶æ¥å­˜å‚¨é¢„åŠ è½½çš„LoRAå·¥ä»¶ï¼Œç¡®ä¿å¿«é€Ÿå“åº”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒServerlessLoRAåœ¨å¤„ç†å·¥ä¸šå·¥ä½œè´Ÿè½½æ—¶ï¼Œé¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰å‡å°‘äº†å¤šè¾¾86%ï¼Œè€Œæˆæœ¬é™ä½å¹…åº¦é«˜è¾¾89%ã€‚è¿™äº›æ˜¾è‘—çš„æ€§èƒ½æå‡è¡¨æ˜ï¼ŒServerlessLoRAåœ¨æ— æœåŠ¡å™¨æ¨ç†é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ServerlessLoRAçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ï¼Œå¦‚åœ¨çº¿èŠå¤©æœºå™¨äººã€æ™ºèƒ½å®¢æœç³»ç»Ÿå’Œå®æ—¶æ–‡æœ¬ç”Ÿæˆç­‰ã€‚å…¶é™ä½å»¶è¿Ÿå’Œæˆæœ¬çš„èƒ½åŠ›å°†æ¨åŠ¨æ›´å¤šä¼ä¸šé‡‡ç”¨æ— æœåŠ¡å™¨æ¶æ„è¿›è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜æœ‰æ½œåŠ›æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¨ç†ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Serverless computing has grown rapidly for serving Large Language Model (LLM) inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid scaling. However, our analysis reveals that current serverless can effectively serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to three key limitations: 1) massive parameter redundancy among functions where 99% of weights are unnecessarily duplicated, 2) costly artifact loading latency beyond LLM loading, and 3) magnified resource contention when serving multiple LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased Time-To-First-Token (TTFT), and high monetary costs.
>   We propose ServerlessLoRA, a novel serverless inference system designed for faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM sharing across isolated LoRA functions to reduce redundancy. We design a pre-loading method that pre-loads comprehensive LoRA artifacts to minimize cold-start latency. Furthermore, ServerlessLoRA employs contention aware batching and offloading to mitigate GPU resource conflicts during bursty workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to state-of-the-art LLM inference solutions.

