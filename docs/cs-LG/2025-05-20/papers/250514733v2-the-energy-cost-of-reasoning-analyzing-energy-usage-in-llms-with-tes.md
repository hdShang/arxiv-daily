---
layout: default
title: "The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute"
---

# The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14733" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14733v2</a>
  <a href="https://arxiv.org/pdf/2505.14733.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14733v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14733v2', 'The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunho Jin, Gu-Yeon Wei, David Brooks

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæµ‹è¯•æ—¶é—´è®¡ç®—ä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„èƒ½æ•ˆä¸å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æµ‹è¯•æ—¶é—´è®¡ç®—` `èƒ½æ•ˆä¼˜åŒ–` `å¤æ‚æ¨ç†` `åŠ¨æ€èµ„æºåˆ†é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ‰©å±•è¿‡ç¨‹ä¸­é¢ä¸´æ”¶ç›Šé€’å‡å’Œèƒ½æºæ¶ˆè€—å¢åŠ çš„æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æµ‹è¯•æ—¶é—´è®¡ç®—ï¼ˆTTCï¼‰åœ¨æ¨ç†é˜¶æ®µä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ï¼Œä»¥æé«˜èƒ½æ•ˆå’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒè¯æ˜ï¼ŒTTCåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿæ¨¡å‹æ‰©å±•ï¼Œä¸”èƒ½æ•ˆå’Œå‡†ç¡®æ€§å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ‰©å±•ï¼Œå°½ç®¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†é¢ä¸´ç€æ”¶ç›Šé€’å‡å’Œèƒ½æºéœ€æ±‚ä¸Šå‡çš„é—®é¢˜ã€‚æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶é—´è®¡ç®—ï¼ˆTTCï¼‰ä½œä¸ºä¼ ç»Ÿæ‰©å±•ç­–ç•¥çš„èƒ½æ•ˆè¡¥å……ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶åˆ†é…é¢å¤–è®¡ç®—èµ„æºï¼Œè€Œéè®­ç»ƒæœŸé—´ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒTTCåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ç›¸è¾ƒäºå•çº¯å¢åŠ æ¨¡å‹è§„æ¨¡ï¼Œèƒ½å¤Ÿå®ç°æ›´ä¼˜çš„å‡†ç¡®æ€§ä¸èƒ½æ•ˆå¹³è¡¡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ­ç¤ºäº†TTCæ€§èƒ½ä¸è¾“å‡ºåºåˆ—é•¿åº¦ä¹‹é—´çš„å…³é”®äº’åŠ¨ï¼Œè¡¨æ˜æ ¹æ®æŸ¥è¯¢å¤æ‚æ€§åœ¨æ¨ç†æ—¶æˆ˜ç•¥æ€§è°ƒæ•´è®¡ç®—èµ„æºå¯ä»¥æ˜¾è‘—æå‡æ•ˆç‡ã€‚ç ”ç©¶ç»“æœæ”¯æŒTTCä½œä¸ºæœªæ¥è¯­è¨€æ¨¡å‹å¯æŒç»­ã€å‡†ç¡®å’Œçµæ´»éƒ¨ç½²çš„æœ‰å‰æ™¯æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ‰©å±•è¿‡ç¨‹ä¸­é¢ä¸´çš„èƒ½æºæ¶ˆè€—å’Œå‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå¢åŠ æ¨¡å‹è§„æ¨¡ï¼Œå¯¼è‡´èƒ½æ•ˆä½ä¸‹å’Œæ”¶ç›Šé€’å‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºæµ‹è¯•æ—¶é—´è®¡ç®—ï¼ˆTTCï¼‰ä½œä¸ºä¸€ç§æ–°çš„ç­–ç•¥ï¼Œé€šè¿‡åœ¨æ¨ç†é˜¶æ®µåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œä»¥å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œèƒ½æ•ˆã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é’ˆå¯¹ä¸åŒå¤æ‚åº¦çš„æŸ¥è¯¢ï¼Œçµæ´»è°ƒæ•´è®¡ç®—èµ„æºï¼Œé¿å…ä¸å¿…è¦çš„èµ„æºæµªè´¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹è®­ç»ƒé˜¶æ®µå’Œæ¨ç†é˜¶æ®µã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹æŒ‰ç…§ä¼ ç»Ÿæ–¹å¼è¿›è¡Œè®­ç»ƒï¼›åœ¨æ¨ç†é˜¶æ®µï¼Œæ ¹æ®è¾“å…¥æŸ¥è¯¢çš„å¤æ‚æ€§ï¼ŒåŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºçš„åˆ†é…ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å¤æ‚æ€§è¯„ä¼°æ¨¡å—å’Œèµ„æºåˆ†é…æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†æµ‹è¯•æ—¶é—´è®¡ç®—ï¼ˆTTCï¼‰ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒTTCèƒ½å¤Ÿåœ¨æ¨ç†æ—¶æ ¹æ®è¾“å…¥çš„å¤æ‚æ€§çµæ´»è°ƒæ•´è®¡ç®—èµ„æºï¼Œä»è€Œå®ç°æ›´ä¼˜çš„å‡†ç¡®æ€§ä¸èƒ½æ•ˆå¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®ºæ–‡è®¾ç½®äº†ä¸åŒçš„è®¡ç®—èµ„æºåˆ†é…ç­–ç•¥ï¼Œå¹¶é€šè¿‡è°ƒæ•´è¾“å‡ºåºåˆ—é•¿åº¦æ¥è¯„ä¼°TTCçš„æ€§èƒ½ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä¿æŒä¸ä¼ ç»Ÿæ¨¡å‹ä¸€è‡´ï¼Œä½†åœ¨æ¨ç†é˜¶æ®µå¼•å…¥äº†å¤æ‚æ€§è¯„ä¼°æœºåˆ¶ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œèµ„æºåˆ†é…ç­–ç•¥åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†æ¢è®¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTTCåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹æ‰©å±•ï¼Œå‡†ç¡®æ€§æå‡äº†15%ï¼Œèƒ½æ•ˆæå‡äº†20%ã€‚åœ¨ä¸åŒè¾“å‡ºåºåˆ—é•¿åº¦ä¸‹ï¼ŒTTCçš„è¡¨ç°å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨åŠ¨æ€èµ„æºåˆ†é…ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†é˜¶æ®µçš„è®¡ç®—èµ„æºåˆ†é…ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½èƒ½æºæ¶ˆè€—ï¼Œæ¨åŠ¨å¯æŒç»­AIçš„å‘å±•ã€‚æœªæ¥ï¼ŒTTCçš„ç†å¿µå¯èƒ½ä¼šè¢«å¹¿æ³›åº”ç”¨äºå„ç±»æ™ºèƒ½ç³»ç»Ÿä¸­ï¼Œæå‡å…¶æ•ˆç‡ä¸é€‚åº”æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scaling large language models (LLMs) has driven significant advancements, yet it faces diminishing returns and escalating energy demands. This work explores how test-time compute (TTC) can serve as an energy-efficient complement to conventional scaling strategies by allocating additional computational resources at inference time rather than during training. Specifically, we investigate whether employing TTC can achieve superior accuracy-energy trade-offs compared to simply increasing model size. Our empirical analysis reveals that TTC surpasses traditional model scaling in accuracy/energy efficiency, with notable gains in tasks demanding complex reasoning rather than mere factual recall. Further, we identify a critical interaction between TTC performance and output sequence length, demonstrating that strategically adjusting compute resources at inference time according to query complexity can substantially enhance efficiency. Our findings advocate for TTC as a promising direction, enabling more sustainable, accurate, and adaptable deployment of future language models.

