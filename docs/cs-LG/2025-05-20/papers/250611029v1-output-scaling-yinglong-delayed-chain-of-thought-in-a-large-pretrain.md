---
layout: default
title: Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model
---

# Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11029" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11029v1</a>
  <a href="https://arxiv.org/pdf/2506.11029.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11029v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11029v1', 'Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xue Wang, Tian Zhou, Jinyang Gao, Bolin Ding, Jingren Zhou

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/qcw1314/YingLong_300m)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºYingLongæ¡†æ¶ä»¥æå‡æ—¶é—´åºåˆ—é¢„æµ‹ç²¾åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `éå› æœæ¨¡å‹` `åŒå‘æ³¨æ„åŠ›` `å»¶è¿Ÿæ€ç»´é“¾` `å¤šè¾“å…¥é›†æˆ` `æ¨¡å‹è¯„ä¼°` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•å¤šé‡‡ç”¨ç›´æ¥æˆ–é€’å½’æ–¹å¼ï¼Œå­˜åœ¨å‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§éå› æœçš„åŒå‘æ³¨æ„åŠ›ç¼–ç å™¨YingLongï¼Œé€šè¿‡å»¶è¿Ÿæ€ç»´é“¾æ¨ç†æå‡é¢„æµ‹ç²¾åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒYingLongåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œæœ€ä½³æ€§èƒ½è¶…è¿‡60%ï¼Œå¹¶åœ¨GIFT-EvalåŸºå‡†ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´åºåˆ—é¢„æµ‹çš„è”åˆé¢„æµ‹æ¡†æ¶ï¼Œä¸ä¼ ç»Ÿçš„ç›´æ¥æˆ–é€’å½’æ–¹æ³•å½¢æˆå¯¹æ¯”ã€‚è¯¥æ¡†æ¶åœ¨æˆ‘ä»¬è®¾è®¡çš„åŸºç¡€æ¨¡å‹YingLongä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†ä¸€ç§æ–°çš„ç¼©æ”¾æ•ˆåº”ï¼šæ›´é•¿çš„è¾“å‡ºæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œè¿™å¾—ç›Šäºæˆ‘ä»¬éå› æœæ–¹æ³•ä¸­çš„å»¶è¿Ÿæ€ç»´é“¾æ¨ç†ã€‚YingLongæ˜¯ä¸€ä¸ªéå› æœçš„åŒå‘æ³¨æ„åŠ›ç¼–ç å™¨ï¼Œä»…ä½¿ç”¨å˜æ¢å™¨ç»“æ„ï¼Œé€šè¿‡æ©ç ä»¤ç‰Œæ¢å¤è¿›è¡Œè®­ç»ƒï¼Œæ›´æœ‰æ•ˆåœ°ä¸è¯­è¨€ç†è§£ä»»åŠ¡å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¤šè¾“å…¥é›†æˆæ¥è§£å†³è¾“å‡ºæ–¹å·®ï¼Œä»è€Œæå‡æ€§èƒ½ã€‚æˆ‘ä»¬å‘å¸ƒäº†å››ä¸ªåŸºç¡€æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»6Måˆ°300Mï¼Œåœ¨ETTå’ŒWeatheræ•°æ®é›†çš„é›¶-shotä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼ŒYingLongçš„æœ€ä½³æ€§èƒ½è¶…è¿‡60%ã€‚ä¸ºç¡®ä¿æ¨¡å‹çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨GIFT-EvalåŸºå‡†è¯„ä¼°æ¨¡å‹ï¼Œè¯¥åŸºå‡†åŒ…å«7ä¸ªé¢†åŸŸçš„23ä¸ªæ—¶é—´åºåˆ—æ•°æ®é›†ã€‚YingLongæ˜¾è‘—è¶…è¶Šäº†æœ€ä½³æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å’Œç«¯åˆ°ç«¯è®­ç»ƒæ¨¡å‹ï¼Œåˆ†åˆ«æé«˜äº†14%å’Œ44%çš„æ’åã€‚é¢„è®­ç»ƒçš„300Mæ¨¡å‹å¯åœ¨https://huggingface.co/qcw1314/YingLong_300mè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œçµæ´»æ€§ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç›´æ¥å’Œé€’å½’æ–¹æ³•çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†å¤æ‚çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´é¢„æµ‹æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„YingLongæ¡†æ¶é‡‡ç”¨éå› æœçš„åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å»¶è¿Ÿæ€ç»´é“¾æ¨ç†æ¥å¢å¼ºæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºæ—¶è€ƒè™‘æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šYingLongçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨éƒ¨åˆ†ï¼Œä½¿ç”¨åŒå‘æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä¿¡æ¯å¤„ç†ï¼Œå¹¶é€šè¿‡æ©ç ä»¤ç‰Œæ¢å¤è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šè¾“å…¥é›†æˆç­–ç•¥ï¼Œä»¥å‡å°‘è¾“å‡ºçš„æ–¹å·®å¹¶æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šYingLongçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶éå› æœçš„è®¾è®¡å’Œå»¶è¿Ÿæ€ç»´é“¾æ¨ç†ï¼Œè¿™ä¸ç°æœ‰çš„å› æœæ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¤æ‚å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸åŒè§„æ¨¡çš„å‚æ•°ï¼ˆ6Mè‡³300Mï¼‰ï¼Œå¹¶é‡‡ç”¨äº†é€‚åˆæ—¶é—´åºåˆ—é¢„æµ‹çš„æŸå¤±å‡½æ•°ã€‚æ¨¡å‹é€šè¿‡å¤šè¾“å…¥é›†æˆæ¥ä¼˜åŒ–è¾“å‡ºçš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒYingLongåœ¨ETTå’ŒWeatheræ•°æ®é›†çš„é›¶-shotä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæœ€ä½³æ€§èƒ½è¶…è¿‡60%ã€‚æ­¤å¤–ï¼Œåœ¨GIFT-EvalåŸºå‡†æµ‹è¯•ä¸­ï¼ŒYingLongç›¸æ¯”äºæœ€ä½³æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å’Œç«¯åˆ°ç«¯è®­ç»ƒæ¨¡å‹ï¼Œåˆ†åˆ«æé«˜äº†14%å’Œ44%çš„æ’åï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºé¢„æµ‹ã€æ°”è±¡æ•°æ®åˆ†æä»¥åŠä¾›åº”é“¾ç®¡ç†ç­‰æ—¶é—´åºåˆ—ç›¸å…³çš„ä»»åŠ¡ã€‚YingLongæ¡†æ¶çš„é«˜å‡†ç¡®æ€§å’Œçµæ´»æ€§ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œèƒ½å¤Ÿä¸ºå†³ç­–æä¾›æ›´å¯é çš„æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ—¶é—´åºåˆ—åˆ†æç ”ç©¶ï¼Œæå‡æ™ºèƒ½å†³ç­–ç³»ç»Ÿçš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a joint forecasting framework for time series prediction that contrasts with traditional direct or recursive methods. This framework achieves state-of-the-art performance for our designed foundation model, YingLong, and reveals a novel scaling effect: longer outputs significantly enhance model accuracy due to delayed chain-of-thought reasoning in our non-causal approach. YingLong is a non-causal, bidirectional attention encoder-only transformer trained through masked token recovery, aligning more effectively with language understanding tasks than with generation tasks. Additionally, we boost performance by tackling output variance with a multi-input ensemble. We release four foundation models ranging from 6M to 300M parameters, demonstrating superior results in zero-shot tasks on the ETT and Weather datasets. YingLong achieves more than 60% best performance. To ensure generalizability, we assessed the models using the GIFT-Eval benchmark, which comprises 23 time series datasets across 7 domains. Yinglong significantly outperformed the best time-series foundation models, end-to-end trained models by 14% and 44% in rank respectively.The pretrained 300M model is available at https://huggingface.co/qcw1314/YingLong_300m

