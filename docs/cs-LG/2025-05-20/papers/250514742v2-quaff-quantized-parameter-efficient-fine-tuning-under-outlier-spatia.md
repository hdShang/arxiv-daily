---
layout: default
title: Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis
---

# Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14742" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14742v2</a>
  <a href="https://arxiv.org/pdf/2505.14742.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14742v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14742v2', 'Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hong Huang, Dapeng Wu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-29)

**å¤‡æ³¨**: Accepted by ACL 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Little0o0/Quaff.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQuaffä»¥è§£å†³èµ„æºå—é™è®¾å¤‡ä¸ŠLLMå¾®è°ƒæ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–å¾®è°ƒ` `å¼‚å¸¸ç©ºé—´ç¨³å®šå‡è®¾` `å¤§å‹è¯­è¨€æ¨¡å‹` `å‚æ•°é«˜æ•ˆ` `èµ„æºå—é™è®¾å¤‡` `åŠ¨é‡ç¼©æ”¾` `è®¡ç®—æ•ˆç‡` `å†…å­˜ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–å¾®è°ƒæ–¹æ³•åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéš¾ä»¥å®ç°é«˜æ•ˆæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ¿€æ´»å¼‚å¸¸æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†å¼‚å¸¸ç©ºé—´ç¨³å®šå‡è®¾ï¼ˆOSSHï¼‰ï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†Quaffæ¡†æ¶ï¼Œé€šè¿‡åŠ¨é‡ç¼©æ”¾ä¼˜åŒ–ä½ç²¾åº¦æ¿€æ´»è¡¨ç¤ºã€‚
3. åœ¨GPQAæ¨ç†åŸºå‡†ä¸Šï¼ŒQuaffå®ç°äº†1.73å€çš„å»¶è¿Ÿå‡å°‘å’Œ30%çš„å†…å­˜èŠ‚çœï¼ŒåŒæ—¶åœ¨Phi-3æ¨¡å‹ä¸Šæé«˜äº†0.6%çš„å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†åœ¨èµ„æºå—é™çš„ä¸ªäººè®¾å¤‡ä¸Šéƒ¨ç½²æ—¶ï¼Œä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä»ç„¶é˜»ç¢å…¶åº”ç”¨ã€‚å°½ç®¡é‡åŒ–æä¾›äº†æé«˜æ•ˆç‡çš„é€”å¾„ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ€§èƒ½ä¸å¼€é”€ä¹‹é—´éš¾ä»¥å¹³è¡¡ï¼Œå¾€å¾€å¯¼è‡´é«˜è®¡ç®—/å†…å­˜æˆæœ¬æˆ–æœªèƒ½æœ‰æ•ˆå¤„ç†æ¿€æ´»å¼‚å¸¸ï¼Œè¿™æ˜¯é‡åŒ–å¾®è°ƒä¸­çš„å…³é”®ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¼‚å¸¸ç©ºé—´ç¨³å®šå‡è®¾ï¼ˆOSSHï¼‰ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†Quaffï¼Œä¸€ä¸ªé‡åŒ–çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡æœ‰é’ˆå¯¹æ€§çš„åŠ¨é‡ç¼©æ”¾ä¼˜åŒ–ä½ç²¾åº¦æ¿€æ´»è¡¨ç¤ºã€‚Quaffé€šè¿‡è½»é‡çº§æ“ä½œåŠ¨æ€æŠ‘åˆ¶ä¸å˜é€šé“ä¸­çš„å¼‚å¸¸ï¼Œæ¶ˆé™¤äº†å…¨ç²¾åº¦æƒé‡å­˜å‚¨å’Œå…¨å±€é‡ç¼©æ”¾ï¼ŒåŒæ—¶å‡å°‘äº†é‡åŒ–è¯¯å·®ã€‚å¤§é‡å®éªŒéªŒè¯äº†OSSHï¼Œå¹¶å±•ç¤ºäº†Quaffçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šè¿›è¡Œä»»åŠ¡ç‰¹å®šå¾®è°ƒæ—¶çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚è¿‡é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„é‡åŒ–æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†æ¿€æ´»å¼‚å¸¸ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„å¼‚å¸¸ç©ºé—´ç¨³å®šå‡è®¾ï¼ˆOSSHï¼‰è®¤ä¸ºï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒæŸäº›æ¿€æ´»å¼‚å¸¸é€šé“åœ¨è®­ç»ƒè¿­ä»£ä¸­ä¿æŒç¨³å®šçš„ç©ºé—´ä½ç½®ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼ŒQuaffæ¡†æ¶é€šè¿‡åŠ¨é‡ç¼©æ”¾ä¼˜åŒ–ä½ç²¾åº¦æ¿€æ´»è¡¨ç¤ºï¼Œä¸“æ³¨äºæŠ‘åˆ¶ä¸å˜é€šé“ä¸­çš„å¼‚å¸¸ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQuaffçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¿€æ´»è¡¨ç¤ºçš„é‡åŒ–ã€åŠ¨é‡ç¼©æ”¾çš„åº”ç”¨ä»¥åŠå¼‚å¸¸é€šé“çš„åŠ¨æ€æŠ‘åˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è½»é‡çº§æ“ä½œå®ç°é«˜æ•ˆçš„å¾®è°ƒï¼Œé¿å…äº†å…¨ç²¾åº¦æƒé‡å­˜å‚¨å’Œå…¨å±€é‡ç¼©æ”¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šQuaffçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†OSSHï¼Œå¹¶é€šè¿‡åŠ¨æ€æŠ‘åˆ¶ä¸å˜é€šé“ä¸­çš„å¼‚å¸¸æ¥ä¼˜åŒ–é‡åŒ–è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä¸ä¼ ç»Ÿçš„é‡åŒ–å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Quaffä¸­ï¼ŒåŠ¨é‡ç¼©æ”¾çš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨æŠ‘åˆ¶å¼‚å¸¸çš„åŒæ—¶ï¼Œå°½é‡å‡å°‘é‡åŒ–è¯¯å·®ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†æ¿€æ´»é€šé“çš„ç¨³å®šæ€§ï¼Œä»¥æé«˜å¾®è°ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨GPQAæ¨ç†åŸºå‡†ä¸Šï¼ŒQuaffå®ç°äº†1.73å€çš„å»¶è¿Ÿå‡å°‘å’Œ30%çš„å†…å­˜èŠ‚çœï¼ŒåŒæ—¶åœ¨Phi-3æ¨¡å‹ä¸Šæé«˜äº†0.6%çš„å‡†ç¡®ç‡ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒQuaffåœ¨æ•ˆç‡ã€æ€§èƒ½å’Œå¯éƒ¨ç½²æ€§ä¹‹é—´è¾¾æˆäº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Quaffæ¡†æ¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬ä¸ªäººè®¾å¤‡ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œå¦‚æ™ºèƒ½æ‰‹æœºã€å¹³æ¿ç”µè„‘ç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼ŒQuaffä½¿å¾—ä¸ªæ€§åŒ–çš„LLMéƒ¨ç½²å˜å¾—æ›´åŠ å¯è¡Œï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼ŒQuaffå¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šæ™ºèƒ½è®¾å¤‡çš„æ™ºèƒ½åŒ–åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers, a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations. Building on OSSH, we propose Quaff, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at https://github.com/Little0o0/Quaff.git.

