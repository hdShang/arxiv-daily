---
layout: default
title: "KIPPO: Koopman-Inspired Proximal Policy Optimization"
---

# KIPPO: Koopman-Inspired Proximal Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14566" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14566v1</a>
  <a href="https://arxiv.org/pdf/2505.14566.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14566v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14566v1', 'KIPPO: Koopman-Inspired Proximal Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrei Cozma, Landon Harris, Hairong Qi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: Accepted for IJCAI 2025. This arXiv submission is the full version of the conference paper, including the appendix and supplementary material omitted from the IJCAI proceedings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºKIPPOä»¥è§£å†³å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„ç­–ç•¥ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `Koopmanç®—å­` `éçº¿æ€§åŠ¨æ€` `æ§åˆ¶ç³»ç»Ÿ` `PPO` `ç¨³å®šæ€§` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨å¤æ‚éçº¿æ€§åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´é«˜æ–¹å·®å’Œä¸ç¨³å®šå­¦ä¹ çš„é—®é¢˜ã€‚
2. KIPPOé€šè¿‡å¼•å…¥Koopmanè¿‘ä¼¼è¾…åŠ©ç½‘ç»œï¼Œå­¦ä¹ ç³»ç»ŸåŠ¨æ€çš„çº¿æ€§è¡¨ç¤ºï¼Œå¢å¼ºç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKIPPOåœ¨å¤šç§è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸Šæ€§èƒ½æå‡6-60%ï¼Œæ–¹å·®é™ä½è‡³91%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•å¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å› å…¶åœ¨æ€§èƒ½ã€è®­ç»ƒç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨å¤æ‚å’Œéçº¿æ€§åŠ¨æ€ç¯å¢ƒä¸­å¼€å‘æœ‰æ•ˆçš„æ§åˆ¶ç­–ç•¥ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ¢¯åº¦ä¼°è®¡çš„é«˜æ–¹å·®å’Œéå‡¸ä¼˜åŒ–æ™¯è§‚å¸¸å¯¼è‡´å­¦ä¹ è½¨è¿¹çš„ä¸ç¨³å®šã€‚æœ¬æ–‡æå‡ºäº†KIPPOï¼ˆåŸºäºKoopmançš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¯¥æ–¹æ³•å­¦ä¹ ç³»ç»ŸåŠ¨æ€çš„è¿‘ä¼¼çº¿æ€§æ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼ŒåŒæ—¶ä¿ç•™æœ‰æ•ˆç­–ç•¥å­¦ä¹ æ‰€éœ€çš„åŸºæœ¬ç‰¹å¾ã€‚é€šè¿‡å¼•å…¥Koopmanè¿‘ä¼¼è¾…åŠ©ç½‘ç»œï¼ŒKIPPOå¯ä»¥åœ¨ä¸æ”¹å˜æ ¸å¿ƒç­–ç•¥æˆ–ä»·å€¼å‡½æ•°æ¶æ„çš„æƒ…å†µä¸‹ï¼Œæ·»åŠ åˆ°åŸºçº¿ç­–ç•¥ä¼˜åŒ–ç®—æ³•ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸Šï¼ŒKIPPOç›¸è¾ƒäºPPOåŸºçº¿æ€§èƒ½æå‡äº†6-60%ï¼Œå¹¶å°†æ–¹å·®é™ä½äº†å¤šè¾¾91%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤æ‚éçº¿æ€§åŠ¨æ€ç¯å¢ƒä¸­ï¼Œç°æœ‰ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆå¦‚PPOï¼‰é¢ä¸´çš„é«˜æ–¹å·®å’Œä¸ç¨³å®šå­¦ä¹ è½¨è¿¹çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šKIPPOé€šè¿‡å¼•å…¥Koopmanè¿‘ä¼¼è¾…åŠ©ç½‘ç»œï¼Œå­¦ä¹ ç³»ç»ŸåŠ¨æ€çš„è¿‘ä¼¼çº¿æ€§è¡¨ç¤ºï¼Œä»è€Œç®€åŒ–ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ï¼Œæé«˜å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKIPPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŸºçº¿PPOç®—æ³•å’ŒKoopmanè¿‘ä¼¼è¾…åŠ©ç½‘ç»œã€‚è¯¥è¾…åŠ©ç½‘ç»œé€šè¿‡æ•æ‰ç³»ç»ŸåŠ¨æ€çš„çº¿æ€§ç‰¹å¾ï¼Œå¢å¼ºäº†ç­–ç•¥ä¼˜åŒ–çš„è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šKIPPOçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†Koopmanç®—å­ç†è®ºåº”ç”¨äºç­–ç•¥ä¼˜åŒ–ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥å¤„ç†éçº¿æ€§åŠ¨æ€ç³»ç»Ÿï¼Œæ˜¾è‘—æé«˜äº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šKIPPOçš„è®¾è®¡åŒ…æ‹¬ä¼˜åŒ–æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ç­–ç•¥æ›´æ–°å’ŒåŠ¨æ€å­¦ä¹ ï¼Œç½‘ç»œç»“æ„ä¸Šä¿ç•™äº†PPOçš„æ ¸å¿ƒæ¶æ„ï¼ŒåŒæ—¶å¼•å…¥äº†è¾…åŠ©ç½‘ç»œä»¥æ•æ‰ç³»ç»Ÿçš„çº¿æ€§ç‰¹å¾ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œå±‚æ•°ç­‰ç»†èŠ‚åœ¨å®éªŒä¸­è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒKIPPOç›¸è¾ƒäºPPOåŸºçº¿åœ¨æ€§èƒ½ä¸Šæå‡äº†6-60%ï¼Œå¹¶ä¸”åœ¨æ–¹å·®æ–¹é¢é™ä½äº†å¤šè¾¾91%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒKIPPOåœ¨å¤„ç†å¤æ‚æ§åˆ¶ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

KIPPOçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜ç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼ŒKIPPOèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´ä¸ºå¤æ‚å’ŒåŠ¨æ€çš„æ§åˆ¶ç³»ç»Ÿï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) has made significant strides in various domains, and policy gradient methods like Proximal Policy Optimization (PPO) have gained popularity due to their balance in performance, training stability, and computational efficiency. These methods directly optimize policies through gradient-based updates. However, developing effective control policies for environments with complex and non-linear dynamics remains a challenge. High variance in gradient estimates and non-convex optimization landscapes often lead to unstable learning trajectories. Koopman Operator Theory has emerged as a powerful framework for studying non-linear systems through an infinite-dimensional linear operator that acts on a higher-dimensional space of measurement functions. In contrast with their non-linear counterparts, linear systems are simpler, more predictable, and easier to analyze. In this paper, we present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an approximately linear latent-space representation of the underlying system's dynamics while retaining essential features for effective policy learning. This is achieved through a Koopman-approximation auxiliary network that can be added to the baseline policy optimization algorithms without altering the architecture of the core policy or value function. Extensive experimental results demonstrate consistent improvements over the PPO baseline with 6-60% increased performance while reducing variability by up to 91% when evaluated on various continuous control tasks.

