---
layout: default
title: Byte Pair Encoding for Efficient Time Series Forecasting
---

# Byte Pair Encoding for Efficient Time Series Forecasting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14411" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14411v2</a>
  <a href="https://arxiv.org/pdf/2505.14411.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14411v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14411v2', 'Byte Pair Encoding for Efficient Time Series Forecasting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Leon GÃ¶tz, Marcel Kollovieh, Stephan GÃ¼nnemann, Leo Schwinn

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-08-05)

**å¤‡æ³¨**: 24 pages in total, 17 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¨¡å¼çš„æ—¶é—´åºåˆ—ç¼–ç æ–¹æ³•ä»¥æé«˜é¢„æµ‹æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `æ¨¡å¼è¯†åˆ«` `æ ‡è®°åŒ–æ–¹æ³•` `æ¡ä»¶è§£ç ` `è®¡ç®—æ•ˆç‡` `æœºå™¨å­¦ä¹ ` `æ•°æ®å‹ç¼©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¶é—´åºåˆ—æ ‡è®°åŒ–æ–¹æ³•ç¼ºä¹çµæ´»æ€§ï¼Œå¯¼è‡´åœ¨ç®€å•æ¨¡å¼ä¸‹ç”Ÿæˆè¿‡å¤šæ ‡è®°ï¼Œå¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢‘ç¹æ¨¡å¼çš„æ ‡è®°åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡åˆå¹¶æ ·æœ¬æ¥è‡ªé€‚åº”å‹ç¼©æ—¶é—´åºåˆ—ï¼Œæå‡äº†æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸Šæ€§èƒ½æå‡36%ï¼Œæ•ˆç‡æå‡1990%ï¼Œå¹¶ä¸”æ¡ä»¶è§£ç è¿›ä¸€æ­¥é™ä½äº†å‡æ–¹è¯¯å·®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„æ—¶é—´åºåˆ—æ ‡è®°åŒ–æ–¹æ³•é€šå¸¸å°†å›ºå®šæ•°é‡çš„æ ·æœ¬ç¼–ç ä¸ºå•ç‹¬çš„æ ‡è®°ï¼Œè¿™ç§ä¸çµæ´»çš„æ–¹å¼å¯èƒ½å¯¼è‡´åœ¨ç®€å•æ¨¡å¼ä¸‹ç”Ÿæˆè¿‡å¤šæ ‡è®°ï¼Œä»è€Œå¢åŠ è®¡ç®—å¼€é”€ã€‚å—å­—èŠ‚å¯¹ç¼–ç æˆåŠŸçš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é¦–ä¸ªä»¥æ¨¡å¼ä¸ºä¸­å¿ƒçš„æ—¶é—´åºåˆ—æ ‡è®°åŒ–æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åŸºäºé¢‘ç¹æ¨¡å¼çš„ç¦»æ•£è¯æ±‡ï¼Œå°†å…·æœ‰æ½œåœ¨æ¨¡å¼çš„æ ·æœ¬åˆå¹¶ä¸ºæ ‡è®°ï¼Œä»è€Œè‡ªé€‚åº”åœ°å‹ç¼©æ—¶é—´åºåˆ—ã€‚é€šè¿‡åˆ©ç”¨æœ‰é™çš„æ¨¡å¼é›†å’Œæ—¶é—´åºåˆ—çš„è¿ç»­ç‰¹æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥æ¡ä»¶è§£ç ä½œä¸ºä¸€ç§è½»é‡çº§çš„åå¤„ç†ä¼˜åŒ–æ–¹æ³•ï¼Œæ— éœ€æ¢¯åº¦è®¡ç®—ä¸”ä¸å¢åŠ è®¡ç®—å¼€é”€ã€‚åœ¨æœ€è¿‘çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å¼åŸºç¡€æ ‡è®°åŒ–æé«˜äº†36%çš„é¢„æµ‹æ€§èƒ½ï¼Œå¹¶å¹³å‡æå‡äº†1990%çš„æ•ˆç‡ã€‚æ¡ä»¶è§£ç è¿›ä¸€æ­¥å°†å‡æ–¹è¯¯å·®é™ä½äº†æœ€å¤š44%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ—¶é—´åºåˆ—æ ‡è®°åŒ–æ–¹æ³•é€šå¸¸å°†å›ºå®šæ•°é‡çš„æ ·æœ¬ç¼–ç ä¸ºå•ç‹¬çš„æ ‡è®°ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤„ç†ç®€å•æ¨¡å¼æ—¶ä¼šç”Ÿæˆè¿‡å¤šçš„æ ‡è®°ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€æ˜¾è‘—å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å¼çš„æ ‡è®°åŒ–æ–¹æ¡ˆï¼Œåˆ©ç”¨é¢‘ç¹æ¨¡å¼çš„ç¦»æ•£è¯æ±‡å°†æ ·æœ¬åˆå¹¶ä¸ºæ ‡è®°ï¼Œä»è€Œå®ç°è‡ªé€‚åº”å‹ç¼©ï¼Œæå‡æ—¶é—´åºåˆ—åˆ†æçš„æ•ˆç‡å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å¼è¯†åˆ«ã€æ ·æœ¬åˆå¹¶å’Œæ¡ä»¶è§£ç ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆè¯†åˆ«æ—¶é—´åºåˆ—ä¸­çš„é¢‘ç¹æ¨¡å¼ï¼Œç„¶åå°†ç›¸åº”æ ·æœ¬åˆå¹¶ä¸ºæ ‡è®°ï¼Œæœ€åé€šè¿‡æ¡ä»¶è§£ç è¿›è¡Œåå¤„ç†ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä»¥æ¨¡å¼ä¸ºä¸­å¿ƒçš„æ ‡è®°åŒ–æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ ¹æ®æ—¶é—´åºåˆ—çš„ç‰¹æ€§è‡ªé€‚åº”ç”Ÿæˆæ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„é¢‘ç¹æ¨¡å¼è¯æ±‡ã€å®šä¹‰æ ·æœ¬åˆå¹¶çš„ç­–ç•¥ï¼Œä»¥åŠå®ç°æ¡ä»¶è§£ç çš„å…·ä½“ç®—æ³•ï¼Œè¿™äº›è®¾è®¡ç¡®ä¿äº†æ–¹æ³•çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ¨¡å¼çš„æ ‡è®°åŒ–æ–¹æ³•åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­æ€§èƒ½æå‡äº†36%ï¼Œæ•ˆç‡æå‡è¾¾1990%ã€‚æ­¤å¤–ï¼Œæ¡ä»¶è§£ç æŠ€æœ¯è¿›ä¸€æ­¥å°†å‡æ–¹è¯¯å·®é™ä½äº†æœ€å¤š44%ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºé¢„æµ‹ã€æ°”è±¡æ•°æ®åˆ†æå’Œå·¥ä¸šè®¾å¤‡ç›‘æ§ç­‰ã€‚é€šè¿‡æé«˜æ—¶é—´åºåˆ—é¢„æµ‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿä¸ºå†³ç­–æä¾›æ›´å¯é çš„æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing time series tokenization methods predominantly encode a constant number of samples into individual tokens. This inflexible approach can generate excessive tokens for even simple patterns like extended constant values, resulting in substantial computational overhead. Inspired by the success of byte pair encoding, we propose the first pattern-centric tokenization scheme for time series analysis. Based on a discrete vocabulary of frequent motifs, our method merges samples with underlying patterns into tokens, compressing time series adaptively. Exploiting our finite set of motifs and the continuous properties of time series, we further introduce conditional decoding as a lightweight yet powerful post-hoc optimization method, which requires no gradient computation and adds no computational overhead. On recent time series foundation models, our motif-based tokenization improves forecasting performance by 36% and boosts efficiency by 1990% on average. Conditional decoding further reduces MSE by up to 44%. In an extensive analysis, we demonstrate the adaptiveness of our tokenization to diverse temporal patterns, its generalization to unseen data, and its meaningful token representations capturing distinct time series properties, including statistical moments and trends.

