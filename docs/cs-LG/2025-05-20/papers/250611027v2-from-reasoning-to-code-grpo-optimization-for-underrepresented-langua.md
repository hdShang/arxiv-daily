---
layout: default
title: From Reasoning to Code: GRPO Optimization for Underrepresented Languages
---

# From Reasoning to Code: GRPO Optimization for Underrepresented Languages

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.11027" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.11027v2</a>
  <a href="https://arxiv.org/pdf/2506.11027.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.11027v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.11027v2', 'From Reasoning to Code: GRPO Optimization for Underrepresented Languages')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Federico Pennino, Bianca Raimondi, Massimo Rondelli, Andrea Gurioli, Maurizio Gabbrielli

**åˆ†ç±»**: cs.LG, cs.AI, cs.PL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-06-16)

**å¤‡æ³¨**: Preprint. Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRPOä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³å°ä¼—ç¼–ç¨‹è¯­è¨€ä»£ç ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°ä¼—ç¼–ç¨‹è¯­è¨€` `ä»£ç ç”Ÿæˆ` `ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–` `æ¨ç†æ­¥éª¤` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°ä¼—ç¼–ç¨‹è¯­è¨€çš„ä»£ç ç”Ÿæˆä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºç»“åˆå°è§„æ¨¡Qwen 2.5æ¨¡å‹ä¸GRPOï¼Œé€šè¿‡æ¨ç†æ­¥éª¤ä¼˜åŒ–ä»£ç ç”Ÿæˆè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨é€»è¾‘ä¸€è‡´æ€§å’Œè¯­æ³•å‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨æ•°å­¦é€»è¾‘é—®é¢˜åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆå‡†ç¡®ä¸”å¯æ‰§è¡Œçš„ä»£ç å¯¹äºå…¬å…±è®­ç»ƒæ•°æ®æœ‰é™çš„å°ä¼—ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚Prologï¼‰è€Œè¨€æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„æ–¹æ³•ï¼Œç»“åˆå°è§„æ¨¡çš„Qwen 2.5æ¨¡å‹ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡æ˜ç¡®çš„æ¨ç†æ­¥éª¤å®ç°æœ‰æ•ˆçš„ä»£ç ç”Ÿæˆã€‚ç»è¿‡è®­ç»ƒï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé€»è¾‘ä¸€è‡´ä¸”è¯­æ³•å‡†ç¡®çš„ä»£ç ï¼Œå¹¶åœ¨å¼ºåŒ–å­¦ä¹ å¾ªç¯ä¸­ç›´æ¥æ•´åˆæ¨ç†é©±åŠ¨çš„åé¦ˆã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†è´¨é‡ã€ä»£ç å‡†ç¡®æ€§å’Œé€»è¾‘æ­£ç¡®æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶å¯¹ç¼ºä¹å¹¿æ³›è®­ç»ƒèµ„æºçš„å¤šç§ç¼–ç¨‹è¯­è¨€çš„æ½œåœ¨ç›Šå¤„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°ä¼—ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚Prologï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•å› ç¼ºä¹è®­ç»ƒæ•°æ®è€Œéš¾ä»¥ç”Ÿæˆå¯æ‰§è¡Œä»£ç ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»“åˆå°è§„æ¨¡çš„Qwen 2.5æ¨¡å‹ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåˆ©ç”¨æ¨ç†æ­¥éª¤æ¥æå‡ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸‹ä»èƒ½æœ‰æ•ˆå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†åé¦ˆæ•´åˆä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œä½¿ç”¨å°è§„æ¨¡ä»£ç æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œç„¶ååœ¨ç”Ÿæˆä»£ç æ—¶å¼•å…¥æ¨ç†åé¦ˆï¼Œæœ€åé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ¨ç†é©±åŠ¨çš„åé¦ˆç›´æ¥æ•´åˆå…¥å¼ºåŒ–å­¦ä¹ å¾ªç¯ä¸­ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•çº¯ä¾èµ–æ•°æ®è®­ç»ƒçš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ¨ç†è´¨é‡ä¸ä»£ç å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”å°ä¼—è¯­è¨€çš„ç‰¹ç‚¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡è®­ç»ƒåçš„æ¨¡å‹åœ¨æ¨ç†è´¨é‡ã€ä»£ç å‡†ç¡®æ€§å’Œé€»è¾‘æ­£ç¡®æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é€»è¾‘é—®é¢˜åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹çš„è¡¨ç°è¾ƒåŸºçº¿æå‡äº†20%ä»¥ä¸Šï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©å·¥å…·å’Œè‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ï¼Œå°¤å…¶é€‚ç”¨äºé‚£äº›ç¼ºä¹ä¸°å¯Œè®­ç»ƒæ•°æ®çš„å°ä¼—ç¼–ç¨‹è¯­è¨€ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šç¼–ç¨‹è¯­è¨€çš„æ™ºèƒ½åŒ–å‘å±•ï¼Œæé«˜å¼€å‘æ•ˆç‡å’Œä»£ç è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generating accurate and executable code using large language models (LLMs) is challenging for languages with limited public training data compared to popular languages such as Python. This paper introduces a generalizable approach that uses small-scale code versions of the Qwen 2.5 model combined with Group Relative Policy Optimization (GRPO) to enable effective code generation through explicit reasoning steps, which is particularly beneficial for languages with smaller source code databases. Using Prolog as a representative use case -- given its limited online presence -- the initial model faced challenges in generating executable code. After some training steps, the model successfully produces logically consistent and syntactically accurate code by directly integrating reasoning-driven feedback into the reinforcement learning loop. Experimental evaluations using mathematical logic problem benchmarks illustrate significant improvements in reasoning quality, code accuracy, and logical correctness, underscoring the potential of this approach to benefit a wide range of programming languages lacking extensive training resources.

