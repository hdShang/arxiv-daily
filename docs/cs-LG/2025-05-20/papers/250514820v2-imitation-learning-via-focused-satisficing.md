---
layout: default
title: Imitation Learning via Focused Satisficing
---

# Imitation Learning via Focused Satisficing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14820" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14820v2</a>
  <a href="https://arxiv.org/pdf/2505.14820.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14820v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14820v2', 'Imitation Learning via Focused Satisficing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rushit N. Shah, Nikolaos Agadakos, Synthia Sasulski, Ali Farajzadeh, Sanjiban Choudhury, Brian Ziebart

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-25)

**å¤‡æ³¨**: Accepted for publication at the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºèšç„¦æ»¡æ„åº¦çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä»¥æå‡è¡Œä¸ºæ¥å—åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `æ»¡æ„åº¦ç†è®º` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€æœŸæœ›` `è¡Œä¸ºæ¥å—åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•é€šå¸¸å‡è®¾æ¼”ç¤ºæ¥è¿‘æœ€ä¼˜ï¼Œå¿½è§†äº†äººç±»åœ¨é€‰æ‹©è¡Œä¸ºæ—¶çš„åŠ¨æ€æœŸæœ›æ°´å¹³ã€‚
2. æœ¬æ–‡æå‡ºçš„èšç„¦æ»¡æ„åº¦æ–¹æ³•é€šè¿‡è¾¹é™…ç›®æ ‡å¼•å¯¼æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨è¶…è¶Šæ¼”ç¤ºè€…çš„æœŸæœ›æ°´å¹³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡ä»¿é«˜è´¨é‡æ¼”ç¤ºæ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æé«˜äº†æ¥å—ç‡å’ŒçœŸå®å›æŠ¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ é€šå¸¸å‡è®¾æ¼”ç¤ºæ˜¯åŸºäºæŸä¸ªå›ºå®šä½†æœªçŸ¥çš„æˆæœ¬å‡½æ•°æ¥è¿‘æœ€ä¼˜çš„ã€‚ç„¶è€Œï¼Œæ ¹æ®æ»¡æ„åº¦ç†è®ºï¼Œäººç±»å¾€å¾€æ ¹æ®ä¸ªäººçš„ï¼ˆå¯èƒ½æ˜¯åŠ¨æ€çš„ï¼‰æœŸæœ›æ°´å¹³é€‰æ‹©å¯æ¥å—çš„è¡Œä¸ºï¼Œè€Œä¸æ˜¯è¿½æ±‚ï¼ˆè¿‘ä¹ï¼‰æœ€ä¼˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¾¹é™…ç›®æ ‡çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œèšç„¦æ»¡æ„åº¦çš„æ¨¡ä»¿å­¦ä¹ æ—¨åœ¨å¯»æ‰¾ä¸€ç§ç­–ç•¥ï¼Œä½¿å…¶åœ¨æœªè§æ¼”ç¤ºä¸­è¶…è¶Šæ¼”ç¤ºè€…çš„æœŸæœ›æ°´å¹³ï¼Œè€Œæ— éœ€æ˜ç¡®å­¦ä¹ è¿™äº›æœŸæœ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ¨¡ä»¿é«˜è´¨é‡çš„æ¼”ç¤ºéƒ¨åˆ†ï¼Œæ˜¾è‘—æé«˜æ¼”ç¤ºè€…çš„æ¥å—ç‡ï¼Œå¹¶åœ¨å¤šç§ç¯å¢ƒä¸­å®ç°ç«äº‰æ€§çš„çœŸå®å›æŠ¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯æ¨¡ä»¿å­¦ä¹ ä¸­å¯¹æ¼”ç¤ºè€…æœŸæœ›æ°´å¹³çš„å¿½è§†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æ¼”ç¤ºæ˜¯æœ€ä¼˜çš„ï¼Œå¯¼è‡´åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šèšç„¦æ»¡æ„åº¦çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•é€šè¿‡è¾¹é™…ç›®æ ‡æ¥å¼•å¯¼å­¦ä¹ ï¼Œä½¿å¾—ç­–ç•¥èƒ½å¤Ÿåœ¨æœªè§æ¼”ç¤ºä¸­è¶…è¶Šæ¼”ç¤ºè€…çš„æœŸæœ›ï¼Œè€Œä¸éœ€è¦æ˜ç¡®å­¦ä¹ è¿™äº›æœŸæœ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è¾¹é™…ç›®æ ‡è®¾å®šã€ç­–ç•¥å­¦ä¹ å’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†æ¼”ç¤ºæ•°æ®ï¼Œç„¶åè®¾å®šæœŸæœ›æ°´å¹³ï¼Œæ¥ç€é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥ï¼Œæœ€åè¯„ä¼°ç­–ç•¥çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ»¡æ„åº¦ç†è®ºï¼Œä½¿å¾—æ¨¡ä»¿å­¦ä¹ ä¸å†ä»…ä»…è¿½æ±‚æœ€ä¼˜è§£ï¼Œè€Œæ˜¯å…³æ³¨äºæ»¡è¶³æ¼”ç¤ºè€…çš„æœŸæœ›ï¼Œä»è€Œæé«˜äº†ç­–ç•¥çš„å®é™…å¯æ¥å—æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†è¾¹é™…ç›®æ ‡ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œè®¾è®¡äº†é€‚åº”æ€§ç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰æ¼”ç¤ºè€…çš„åŠ¨æ€æœŸæœ›æ°´å¹³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œèšç„¦æ»¡æ„åº¦çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨å¤šç§ç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†æ¼”ç¤ºè€…çš„æ¥å—ç‡ï¼Œè¾¾åˆ°äº†æ¯”ç°æœ‰æ–¹æ³•æ›´é«˜çš„æ¨¡ä»¿è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åœ¨æŸäº›ä»»åŠ¡ä¸­å®ç°äº†è¶…è¿‡20%çš„æ¥å—ç‡æå‡ï¼Œå¹¶åœ¨çœŸå®å›æŠ¥ä¸Šä¸åŸºçº¿æ–¹æ³•æŒå¹³æˆ–æ›´ä¼˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œç†è§£å’Œæ¨¡ä»¿äººç±»çš„è¡Œä¸ºæ¨¡å¼è‡³å…³é‡è¦ï¼Œèšç„¦æ»¡æ„åº¦çš„æ–¹æ³•èƒ½å¤Ÿæå‡ç³»ç»Ÿçš„å¯æ¥å—æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning often assumes that demonstrations are close to optimal according to some fixed, but unknown, cost function. However, according to satisficing theory, humans often choose acceptable behavior based on their personal (and potentially dynamic) levels of aspiration, rather than achieving (near-) optimality. For example, a lunar lander demonstration that successfully lands without crashing might be acceptable to a novice despite being slow or jerky. Using a margin-based objective to guide deep reinforcement learning, our focused satisficing approach to imitation learning seeks a policy that surpasses the demonstrator's aspiration levels -- defined over trajectories or portions of trajectories -- on unseen demonstrations without explicitly learning those aspirations. We show experimentally that this focuses the policy to imitate the highest quality (portions of) demonstrations better than existing imitation learning methods, providing much higher rates of guaranteed acceptability to the demonstrator, and competitive true returns on a range of environments.

