---
layout: default
title: Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs
---

# Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14620" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14620v1</a>
  <a href="https://arxiv.org/pdf/2505.14620.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14620v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14620v1', 'Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Morgan Lindsay Heisler, Linzi Xing, Ge Shi, Hanieh Sadri, Gursimran Singh, Weiwei Zhang, Tao Ye, Ying Xiong, Yong Zhang, Zhenan Fan

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: Accepted at ACM KDD 2025

**DOI**: [10.1145/3711896.3737215](https://doi.org/10.1145/3711896.3737215)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æ¯”LoRAè§£ç ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æ¯”è§£ç ` `LoRAé€‚é…` `å¤§è¯­è¨€æ¨¡å‹` `ä»»åŠ¡ç‰¹å®šçŸ¥è¯†` `åä¸ºAscend NPU` `è®¡ç®—æ•ˆç‡` `å¾®è°ƒæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§£ç æ–¹æ³•å¦‚è´ªå©ªæœç´¢å’ŒæŸæœç´¢åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¸¸å—åˆ°åŸºæ¨¡å‹åè§çš„å½±å“ï¼Œå¯¼è‡´ç”Ÿæˆçš„å“åº”ä¸å¤Ÿç‰¹å®šå’Œå‡†ç¡®ã€‚
2. æœ¬æ–‡æå‡ºçš„å¯¹æ¯”LoRAè§£ç ï¼ˆCoLDï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”è§£ç æŠ€æœ¯ï¼Œä¼˜åŒ–äº†LoRAé€‚é…æ¨¡å‹çš„ä»»åŠ¡ç‰¹å®šçŸ¥è¯†åˆ©ç”¨ï¼Œæå‡äº†æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLDåœ¨ä»»åŠ¡å‡†ç¡®ç‡ä¸Šæé«˜äº†5.54%ï¼ŒåŒæ—¶å°†ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†28%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åä¸ºäº‘ç”¨æˆ·åˆ©ç”¨LoRAï¼ˆä½ç§©é€‚é…ï¼‰ä½œä¸ºä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œé’ˆå¯¹ç‰¹å®šåº”ç”¨éœ€æ±‚å¾®è°ƒå’Œå®šåˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç„¶è€Œï¼Œå¤æ‚æ¨ç†æˆ–æ·±åº¦ä¸Šä¸‹æ–‡ç†è§£çš„ä»»åŠ¡å¸¸å¸¸å—åˆ°åŸºæ¨¡å‹åè§æˆ–å¹²æ‰°çš„å½±å“ï¼Œå¯¼è‡´ç”Ÿæˆçš„å“åº”è¿‡äºé€šç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹æ¯”LoRAè§£ç ï¼ˆCoLDï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æœ€å¤§åŒ–åˆ©ç”¨LoRAé€‚é…æ¨¡å‹ä¸­çš„ä»»åŠ¡ç‰¹å®šçŸ¥è¯†ï¼Œä»è€Œæå‡ä¸‹æ¸¸æ€§èƒ½ã€‚CoLDé€šè¿‡å¯¹å€™é€‰æ ‡è®°è¿›è¡Œå¯¹æ¯”è§£ç ï¼ŒåŸºäºLoRAé€‚é…ä¸“å®¶æ¨¡å‹ä¸åŸºæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒå·®å¼‚è¿›è¡Œè¯„åˆ†ï¼Œä¼˜å…ˆé€‰æ‹©ä¸LoRAå­¦ä¹ è¡¨ç¤ºæ›´ä¸€è‡´çš„æ ‡è®°ã€‚å°½ç®¡æœ‰æ•ˆï¼Œä½†CoLDçš„ç®€å•å®ç°è®¡ç®—å¼€é”€è¾ƒå¤§ï¼Œå› æ­¤æˆ‘ä»¬ä¸ºåä¸ºAscend NPUå¼€å‘äº†ä¼˜åŒ–å†…æ ¸ã€‚ä¸è´ªå©ªè§£ç ç›¸æ¯”ï¼ŒCoLDåœ¨ä»»åŠ¡å‡†ç¡®ç‡ä¸Šæé«˜äº†5.54%ï¼Œå¹¶å°†ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†28%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§£ç æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¸¸å¸¸å—åˆ°åŸºæ¨¡å‹çš„åè§å½±å“ï¼Œå¯¼è‡´ç”Ÿæˆçš„å“åº”è¿‡äºé€šç”¨ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨LoRAé€‚é…æ¨¡å‹çš„ç‰¹å®šçŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„å¯¹æ¯”LoRAè§£ç ï¼ˆCoLDï¼‰é€šè¿‡å¯¹æ¯”è§£ç æŠ€æœ¯ï¼ŒåŸºäºLoRAé€‚é…ä¸“å®¶æ¨¡å‹ä¸åŸºæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒå·®å¼‚ï¼Œä¼˜å…ˆé€‰æ‹©ä¸LoRAå­¦ä¹ è¡¨ç¤ºä¸€è‡´çš„å€™é€‰æ ‡è®°ï¼Œä»è€Œæå‡ä»»åŠ¡ç‰¹å®šæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoLDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å€™é€‰æ ‡è®°è¯„åˆ†æ¨¡å—å’Œä¼˜åŒ–è§£ç æ¨¡å—ã€‚è¯„åˆ†æ¨¡å—è¯„ä¼°æ¯ä¸ªå€™é€‰æ ‡è®°çš„é€‚åº”æ€§ï¼Œä¼˜åŒ–è§£ç æ¨¡å—åˆ™è´Ÿè´£é«˜æ•ˆç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šCoLDçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯¹æ¯”è§£ç ç­–ç•¥ï¼Œé€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ˜¾è‘—æå‡äº†ä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„åˆ©ç”¨æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿçš„è´ªå©ªè§£ç æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å…·é’ˆå¯¹æ€§çš„å“åº”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°CoLDæ—¶ï¼Œé‡‡ç”¨äº†ä¼˜åŒ–çš„å†…æ ¸è®¾è®¡ä»¥é€‚åº”åä¸ºAscend NPUï¼Œç¡®ä¿åœ¨è®¡ç®—æ•ˆç‡å’Œè§£ç æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼ŒåŒæ—¶è®¾ç½®äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´æœ‰æ•ˆçš„è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoLDåœ¨ä»»åŠ¡å‡†ç¡®ç‡ä¸Šæé«˜äº†5.54%ï¼ŒåŒæ—¶å°†ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†28%ã€‚ä¸ä¼ ç»Ÿçš„è´ªå©ªè§£ç æ–¹æ³•ç›¸æ¯”ï¼ŒCoLDæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äº‘è®¡ç®—ç¯å¢ƒä¸­çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆè§£ç å’Œå¿«é€Ÿå“åº”çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¢æœã€å†…å®¹ç”Ÿæˆå’Œæ•°æ®åˆ†æç­‰ã€‚å…¶ä¼˜åŒ–çš„è§£ç ç­–ç•¥èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æä¾›æ›´å¥½çš„æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and scalable method to fine-tune and customize large language models (LLMs) for application-specific needs. However, tasks that require complex reasoning or deep contextual understanding are often hindered by biases or interference from the base model when using typical decoding methods like greedy or beam search. These biases can lead to generic or task-agnostic responses from the base model instead of leveraging the LoRA-specific adaptations. In this paper, we introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed to maximize the use of task-specific knowledge in LoRA-adapted models, resulting in better downstream performance. CoLD uses contrastive decoding by scoring candidate tokens based on the divergence between the probability distributions of a LoRA-adapted expert model and the corresponding base model. This approach prioritizes tokens that better align with the LoRA's learned representations, enhancing performance for specialized tasks. While effective, a naive implementation of CoLD is computationally expensive because each decoding step requires evaluating multiple token candidates across both models. To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD achieves up to a 5.54% increase in task accuracy while reducing end-to-end latency by 28% compared to greedy decoding. This work provides practical and efficient decoding strategies for fine-tuned LLMs in resource-constrained environments and has broad implications for applied data science in both cloud and on-premises settings.

