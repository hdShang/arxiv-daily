---
layout: default
title: Energy-Efficient Deep Reinforcement Learning with Spiking Transformers
---

# Energy-Efficient Deep Reinforcement Learning with Spiking Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14533" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14533v1</a>
  <a href="https://arxiv.org/pdf/2505.14533.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14533v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14533v1', 'Energy-Efficient Deep Reinforcement Learning with Spiking Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Irfan Uddin, Nishad Tasnim, Md Omor Faruk, Zejian Zhou

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpike-Transformerå¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥è§£å†³èƒ½è€—é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è„‰å†²ç¥ç»ç½‘ç»œ` `å¼ºåŒ–å­¦ä¹ ` `Transformer` `èƒ½æ•ˆä¼˜åŒ–` `ç”Ÿç‰©å¯å‘æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºä»£ç†çš„Transformeråœ¨å¼ºåŒ–å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é«˜èƒ½è€—é™åˆ¶äº†å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„Spike-Transformerå¼ºåŒ–å­¦ä¹ ç®—æ³•ç»“åˆäº†è„‰å†²ç¥ç»ç½‘ç»œçš„èƒ½æ•ˆä¸å¼ºåŒ–å­¦ä¹ çš„å†³ç­–èƒ½åŠ›ï¼Œæ—¨åœ¨é™ä½èƒ½è€—ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ”¿ç­–æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†æ›´é«˜çš„èƒ½æ•ˆå’Œæ”¿ç­–æœ€ä¼˜æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºä»£ç†çš„Transformeråœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶é«˜è®¡ç®—å¤æ‚åº¦å¯¼è‡´æ˜¾è‘—çš„èƒ½è€—ï¼Œé™åˆ¶äº†åœ¨ç°å®ä¸–ç•Œè‡ªä¸»ç³»ç»Ÿä¸­çš„éƒ¨ç½²ã€‚è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰ä»¥å…¶ç”Ÿç‰©å¯å‘çš„ç»“æ„æä¾›äº†ä¸€ç§èƒ½æ•ˆæ›´é«˜çš„æœºå™¨å­¦ä¹ æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„Spike-Transformerå¼ºåŒ–å­¦ä¹ ï¼ˆSTRLï¼‰ç®—æ³•ï¼Œç»“åˆäº†SNNçš„èƒ½æ•ˆä¸å¼ºåŒ–å­¦ä¹ çš„å†³ç­–èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œè®¾è®¡äº†ä¸€ç§ä½¿ç”¨å¤šæ­¥æ³„æ¼ç§¯åˆ†å‘å°„ï¼ˆLIFï¼‰ç¥ç»å…ƒå’Œæ³¨æ„åŠ›æœºåˆ¶çš„SNNï¼Œèƒ½å¤Ÿå¤„ç†å¤šä¸ªæ—¶é—´æ­¥çš„æ—¶ç©ºæ¨¡å¼ã€‚é€šè¿‡çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ç¼–ç è¿›ä¸€æ­¥å¢å¼ºæ¶æ„ï¼Œåˆ›å»ºäº†ä¸€ä¸ªä¼˜åŒ–ç”¨äºå¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„ç±»ä¼¼Transformerçš„ç»“æ„ã€‚ç»¼åˆæ•°å€¼å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SNN Transformeråœ¨æ”¿ç­–æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºä»£ç†çš„Transformerï¼Œå±•ç¤ºäº†åœ¨å¤æ‚ç°å®å†³ç­–åœºæ™¯ä¸­éƒ¨ç½²ç”Ÿç‰©å¯å‘çš„ä½æˆæœ¬æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºäºä»£ç†çš„Transformeråœ¨å¼ºåŒ–å­¦ä¹ ä¸­é¢ä¸´çš„é«˜èƒ½è€—é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…è‡ªä¸»ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºSpike-Transformerå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡ç»“åˆè„‰å†²ç¥ç»ç½‘ç»œçš„èƒ½æ•ˆä¸å¼ºåŒ–å­¦ä¹ çš„å†³ç­–èƒ½åŠ›ï¼Œè®¾è®¡å‡ºä¸€ç§æ–°å‹çš„ç½‘ç»œç»“æ„ï¼Œä»¥é™ä½èƒ½è€—å¹¶æé«˜å†³ç­–æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä½¿ç”¨å¤šæ­¥LIFç¥ç»å…ƒçš„è„‰å†²ç¥ç»ç½‘ç»œï¼Œç»“åˆæ³¨æ„åŠ›æœºåˆ¶å¤„ç†æ—¶ç©ºæ¨¡å¼ï¼Œå¹¶é€šè¿‡çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ç¼–ç å¢å¼ºç½‘ç»œæ€§èƒ½ï¼Œå½¢æˆç±»ä¼¼Transformerçš„ç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†ç”Ÿç‰©å¯å‘çš„è„‰å†²ç¥ç»ç½‘ç»œä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„ç½‘ç»œæ¶æ„ï¼Œæ˜¾è‘—æé«˜äº†èƒ½æ•ˆå’Œå†³ç­–èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œè®¾è®¡ä¸­ï¼Œé‡‡ç”¨å¤šæ­¥LIFç¥ç»å…ƒå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°ï¼Œä»¥ä¼˜åŒ–ç½‘ç»œåœ¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Spike-Transformeråœ¨æ”¿ç­–æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºä»£ç†çš„Transformerï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ”¿ç­–æˆåŠŸç‡æé«˜äº†20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨èƒ½æ•ˆå’Œå†³ç­–èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰éœ€è¦é«˜æ•ˆå†³ç­–çš„è‡ªä¸»ç³»ç»Ÿã€‚é€šè¿‡é™ä½èƒ½è€—ï¼ŒSpike-Transformerç®—æ³•èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œæ¨åŠ¨ç”Ÿç‰©å¯å‘æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Agent-based Transformers have been widely adopted in recent reinforcement learning advances due to their demonstrated ability to solve complex tasks. However, the high computational complexity of Transformers often results in significant energy consumption, limiting their deployment in real-world autonomous systems. Spiking neural networks (SNNs), with their biologically inspired structure, offer an energy-efficient alternative for machine learning. In this paper, a novel Spike-Transformer Reinforcement Learning (STRL) algorithm that combines the energy efficiency of SNNs with the powerful decision-making capabilities of reinforcement learning is developed. Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons and attention mechanisms capable of processing spatio-temporal patterns over multiple time steps is designed. The architecture is further enhanced with state, action, and reward encodings to create a Transformer-like structure optimized for reinforcement learning tasks. Comprehensive numerical experiments conducted on state-of-the-art benchmarks demonstrate that the proposed SNN Transformer achieves significantly improved policy performance compared to conventional agent-based Transformers. With both enhanced energy efficiency and policy optimality, this work highlights a promising direction for deploying bio-inspired, low-cost machine learning models in complex real-world decision-making scenarios.

