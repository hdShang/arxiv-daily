---
layout: default
title: Table Foundation Models: on knowledge pre-training for tabular learning
---

# Table Foundation Models: on knowledge pre-training for tabular learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14415" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14415v2</a>
  <a href="https://arxiv.org/pdf/2505.14415.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14415v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14415v2', 'Table Foundation Models: on knowledge pre-training for tabular learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Myung Jun Kim, FÃ©lix Lefebvre, GaÃ«tan Brison, Alexandre Perez-Lebel, GaÃ«l Varoquaux

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-06-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTARTEæ¨¡å‹ä»¥è§£å†³è¡¨æ ¼å­¦ä¹ ä¸­çš„çŸ¥è¯†é¢„è®­ç»ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¡¨æ ¼å­¦ä¹ ` `çŸ¥è¯†é¢„è®­ç»ƒ` `å‘é‡è¡¨ç¤º` `æ•°æ®ç§‘å­¦` `æ¨¡å‹ä¼˜åŒ–` `æ·±åº¦å­¦ä¹ ` `è¯­ä¹‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¡¨æ ¼å­¦ä¹ æ¨¡å‹åœ¨æ•°æ®è¯­ä¹‰ç†è§£ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯æ•°å€¼æ¡ç›®çš„ä¸Šä¸‹æ–‡ä¾èµ–æ€§ä½¿å¾—æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å—åˆ°é™åˆ¶ã€‚
2. TARTEæ¨¡å‹é€šè¿‡å°†è¡¨æ ¼æ•°æ®è½¬åŒ–ä¸ºçŸ¥è¯†å¢å¼ºçš„å‘é‡è¡¨ç¤ºï¼Œåˆ©ç”¨å­—ç¬¦ä¸²ä¿¡æ¯æ•æ‰è¯­ä¹‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTARTEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†é¢„æµ‹æ€§èƒ½ï¼Œå¹¶åœ¨è®¡ç®—æ•ˆç‡ä¸é¢„æµ‹å‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¡¨æ ¼åŸºç¡€æ¨¡å‹ä¸ºæ•°æ®ç§‘å­¦å¸¦æ¥äº†æ–°çš„å¸Œæœ›ï¼šé€šè¿‡åœ¨è¡¨æ ¼æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè·å–çŸ¥è¯†æˆ–å…ˆéªŒä¿¡æ¯ï¼Œä»è€Œä¿ƒè¿›ä¸‹æ¸¸ä»»åŠ¡çš„æ‰§è¡Œã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨æ•°æ®è¯­ä¹‰ç†è§£æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œæ•°å€¼æ¡ç›®çš„æ„ä¹‰ä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œå¦‚åˆ—åã€‚å°½ç®¡æœ€è¿‘çš„é¢„è®­ç»ƒç¥ç»ç½‘ç»œé€šè¿‡è”åˆå»ºæ¨¡åˆ—åå’Œè¡¨æ ¼æ¡ç›®æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬åœ¨ä½¿ç”¨å’Œç»„åˆä¸Šä»ç„¶ä¸å¤Ÿæ–¹ä¾¿ã€‚æœ¬æ–‡æå‡ºçš„TARTEæ¨¡å‹é€šè¿‡å°†è¡¨æ ¼è½¬æ¢ä¸ºçŸ¥è¯†å¢å¼ºçš„å‘é‡è¡¨ç¤ºï¼Œåˆ©ç”¨å­—ç¬¦ä¸²æ•æ‰è¯­ä¹‰ï¼Œé¢„è®­ç»ƒäºå¤§è§„æ¨¡å…³ç³»æ•°æ®ï¼Œèƒ½å¤Ÿä»¥è¾ƒä½çš„é¢å¤–æˆæœ¬ä¿ƒè¿›åç»­å­¦ä¹ ï¼Œå¹¶å¯ä¸å…¶ä»–å­¦ä¹ å™¨ç»“åˆï¼Œæ˜¾è‘—æå‡é¢„æµ‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¡¨æ ¼å­¦ä¹ ä¸­çŸ¥è¯†é¢„è®­ç»ƒçš„ä¸è¶³ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†æ•°å€¼æ¡ç›®è¯­ä¹‰æ—¶ç¼ºä¹æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ç†è§£ï¼Œå¯¼è‡´é¢„æµ‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTARTEæ¨¡å‹é€šè¿‡å°†è¡¨æ ¼æ•°æ®è½¬åŒ–ä¸ºçŸ¥è¯†å¢å¼ºçš„å‘é‡è¡¨ç¤ºï¼Œåˆ©ç”¨å­—ç¬¦ä¸²ä¿¡æ¯æ¥æ•æ‰æ•°æ®çš„è¯­ä¹‰ï¼Œä»è€Œæå‡æ¨¡å‹çš„ç†è§£èƒ½åŠ›å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTARTEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€çŸ¥è¯†å¢å¼ºå‘é‡ç”Ÿæˆå’Œåç»­å­¦ä¹ æ¨¡å—ã€‚é¦–å…ˆå¯¹è¡¨æ ¼æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åé€šè¿‡é¢„è®­ç»ƒç”Ÿæˆå‘é‡è¡¨ç¤ºï¼Œæœ€åä¸å…¶ä»–å­¦ä¹ å™¨ç»“åˆè¿›è¡Œä»»åŠ¡ç‰¹å®šçš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šTARTEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶çŸ¥è¯†å¢å¼ºçš„å‘é‡è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è¡¨æ ¼æ•°æ®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹åœ¨è®¾è®¡ä¸Šé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å‘é‡è¡¨ç¤ºçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†å¤šå±‚æ¬¡çš„ç‰¹å¾æå–æ¨¡å—ï¼Œä»¥å¢å¼ºå¯¹å¤æ‚è¡¨æ ¼æ•°æ®çš„ç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTARTEæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹æå‡äº†é¢„æµ‹å‡†ç¡®æ€§ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šæé«˜äº†10%ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¹Ÿå®ç°äº†æ˜¾è‘—ä¼˜åŒ–ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ä¸æ•ˆç‡å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TARTEæ¨¡å‹åœ¨æ•°æ®ç§‘å­¦ã€é‡‘èåˆ†æã€åŒ»ç–—æ•°æ®å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡è¡¨æ ¼æ•°æ®çš„ç†è§£å’Œå¤„ç†èƒ½åŠ›ï¼ŒTARTEèƒ½å¤Ÿä¸ºå†³ç­–æ”¯æŒç³»ç»Ÿã€é¢„æµ‹åˆ†æç­‰æä¾›æ›´ä¸ºç²¾å‡†çš„ç»“æœï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨è¡Œä¸šçš„æ™ºèƒ½åŒ–è½¬å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Table foundation models bring high hopes to data science: pre-trained on tabular data to embark knowledge or priors, they should facilitate downstream tasks on tables. One specific challenge is that of data semantics: numerical entries take their meaning from context, e.g., column name. Pre-trained neural networks that jointly model column names and table entries have recently boosted prediction accuracy. While these models outline the promises of world knowledge to interpret table values, they lack the convenience of popular foundation models in text or vision. Indeed, they must be fine-tuned to bring benefits, come with sizeable computation costs, and cannot easily be reused or combined with other architectures. Here we introduce TARTE, a foundation model that transforms tables to knowledge-enhanced vector representations using the string to capture semantics. Pre-trained on large relational data, TARTE yields representations that facilitate subsequent learning with little additional cost. These representations can be fine-tuned or combined with other learners, giving models that push the state-of-the-art prediction performance and improve the prediction/computation performance trade-off. Specialized to a task or a domain, TARTE gives domain-specific representations that facilitate further learning. Our study demonstrates an effective approach to knowledge pre-training for tabular learning.

