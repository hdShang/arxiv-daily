---
layout: default
title: Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study
---

# Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14185" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14185v2</a>
  <a href="https://arxiv.org/pdf/2505.14185.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14185v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14185v2', 'Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaustubh Ponkshe, Shaan Shah, Raghav Singhal, Praneeth Vepakomma

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-04)

**å¤‡æ³¨**: Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally to this work

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/CERT-Lab/safety-subspaces)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶å®‰å…¨å­ç©ºé—´ä¸çº¿æ€§ç‹¬ç«‹æ€§ï¼Œæ­ç¤ºæ¨¡å‹å®‰å…¨æ€§æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®‰å…¨å¯¹é½` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¾®è°ƒç­–ç•¥` `æ¿€æ´»æ¨¡å¼` `å¯¹æŠ—æ€§è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å®‰å…¨å¯¹é½æ–¹é¢å­˜åœ¨è„†å¼±æ€§ï¼Œå¾®è°ƒåå¯èƒ½å¯¼è‡´æœ‰å®³è¡Œä¸ºçš„é‡æ–°å‡ºç°ã€‚
2. è®ºæ–‡é€šè¿‡å®è¯ç ”ç©¶æ¢è®¨å®‰å…¨è¡Œä¸ºä¸é€šç”¨å­¦ä¹ çš„å…³ç³»ï¼Œæå‡ºå®‰å…¨æ€§ä¸å­¦ä¹ ç»„ä»¶é«˜åº¦çº ç¼ çš„è§‚ç‚¹ã€‚
3. ç ”ç©¶åœ¨äº”ä¸ªå¼€æºLLMä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†å®‰å…¨æ€§ä¸æœ‰ç”¨æ€§è¡Œä¸ºçš„é‡å æ€§ï¼Œæå‡ºäº†æ–°çš„é˜²å¾¡ç­–ç•¥éœ€æ±‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¾èµ–å®‰å…¨å¯¹é½ä»¥ç”Ÿæˆç¤¾ä¼šå¯æ¥å—çš„å“åº”ã€‚ç„¶è€Œï¼Œè¿™ç§è¡Œä¸ºå¾€å¾€è„†å¼±ï¼šå³ä½¿åœ¨è‰¯æ€§æˆ–è½»åº¦æ±¡æŸ“çš„æ•°æ®ä¸Šè¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒï¼Œä¹Ÿå¯èƒ½å¯¼è‡´å®‰å…¨æ€§ä¸‹é™å¹¶é‡æ–°å¼•å…¥æœ‰å®³è¡Œä¸ºã€‚æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶æ¢è®¨å®‰å…¨è¡Œä¸ºæ˜¯å¦é›†ä¸­åœ¨ç‰¹å®šçº¿æ€§å­ç©ºé—´ï¼Œæ˜¯å¦å¯ä»¥ä¸é€šç”¨å­¦ä¹ åˆ†ç¦»ï¼Œä»¥åŠæœ‰å®³æ€§æ˜¯å¦æºäºå¯åŒºåˆ†çš„æ¿€æ´»æ¨¡å¼ã€‚ç ”ç©¶å‘ç°ï¼Œæ”¾å¤§å®‰å…¨è¡Œä¸ºçš„å­ç©ºé—´åŒæ ·æ”¾å¤§æœ‰ç”¨è¡Œä¸ºï¼Œä¸”ä¸åŒå®‰å…¨å«ä¹‰çš„æç¤ºæ¿€æ´»é‡å çš„è¡¨ç¤ºã€‚è¿™è¡¨æ˜å®‰å…¨æ€§ä¸æ¨¡å‹çš„é€šç”¨å­¦ä¹ ç»„ä»¶é«˜åº¦çº ç¼ ï¼Œå¼ºè°ƒäº†åŸºäºå­ç©ºé—´çš„é˜²å¾¡é¢ä¸´çš„æ ¹æœ¬é™åˆ¶ï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ›¿ä»£ç­–ç•¥ä»¥åœ¨æŒç»­è®­ç»ƒä¸­ä¿æŒå®‰å…¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å¯¹é½æ–¹é¢çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒåå¯èƒ½å¯¼è‡´å®‰å…¨æ€§ä¸‹é™å’Œæœ‰å®³è¡Œä¸ºçš„é‡æ–°å‡ºç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å®è¯ç ”ç©¶ï¼Œæ¢è®¨å®‰å…¨è¡Œä¸ºæ˜¯å¦é›†ä¸­åœ¨ç‰¹å®šçº¿æ€§å­ç©ºé—´ï¼Œæ­ç¤ºå®‰å…¨æ€§ä¸é€šç”¨å­¦ä¹ ç»„ä»¶çš„é«˜åº¦çº ç¼ ï¼Œå¼ºè°ƒåŸºäºå­ç©ºé—´çš„é˜²å¾¡ç­–ç•¥çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤šç§å®éªŒæ–¹æ³•ï¼Œåˆ†æäº†äº”ä¸ªå¼€æºLLMï¼ˆå¦‚Llamaå’ŒQwenç³»åˆ—ï¼‰çš„æƒé‡å’Œæ¿€æ´»ç©ºé—´ï¼Œæ¯”è¾ƒäº†å®‰å…¨è¡Œä¸ºä¸æœ‰ç”¨è¡Œä¸ºçš„é‡å æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå‘ç°å®‰å…¨æ€§å¹¶ä¸åœ¨ç‹¬ç«‹çš„çº¿æ€§å­ç©ºé—´ä¸­ï¼Œè€Œæ˜¯ä¸æ¨¡å‹çš„é€šç”¨å­¦ä¹ ç»„ä»¶é«˜åº¦çº ç¼ ï¼Œè¿™ä¸ç°æœ‰ç†è®ºç›¸æ‚–ã€‚

**å…³é”®è®¾è®¡**ï¼šå®éªŒä¸­ä½¿ç”¨äº†å¤šç§å¾®è°ƒç­–ç•¥å’Œæ•°æ®é›†ï¼Œé‡ç‚¹å…³æ³¨æƒé‡å’Œæ¿€æ´»æ¨¡å¼çš„åˆ†æï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚å®éªŒä»£ç å·²å…¬å¼€ï¼Œä¾¿äºåç»­ç ”ç©¶è€…éªŒè¯å’Œæ‰©å±•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå®‰å…¨è¡Œä¸ºä¸æœ‰ç”¨è¡Œä¸ºåœ¨æƒé‡å’Œæ¿€æ´»ç©ºé—´ä¸­é«˜åº¦é‡å ï¼Œæç¤ºä¸åŒå®‰å…¨å«ä¹‰çš„è¾“å…¥æ¿€æ´»ç›¸ä¼¼çš„è¡¨ç¤ºã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„å®‰å…¨å­ç©ºé—´ç†è®ºï¼Œå¼ºè°ƒäº†åŸºäºå­ç©ºé—´çš„é˜²å¾¡ç­–ç•¥çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æå‡ã€å¯¹æŠ—æ€§è®­ç»ƒå’Œæ¨¡å‹å¾®è°ƒç­–ç•¥çš„ä¼˜åŒ–ã€‚é€šè¿‡æ·±å…¥ç†è§£å®‰å…¨æ€§ä¸é€šç”¨å­¦ä¹ çš„å…³ç³»ï¼Œç ”ç©¶ä¸ºå¼€å‘æ›´ç¨³å¥çš„AIç³»ç»Ÿæä¾›äº†ç†è®ºåŸºç¡€ï¼Œæœªæ¥å¯èƒ½å½±å“AIåœ¨ç¤¾ä¼šåº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. However, this behavior is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this perspective. We examine whether safety-relevant behavior is concentrated in specific linear subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in activations. Across both weight and activation spaces, our findings are consistent: subspaces that amplify safe behaviors also amplify useful ones, and prompts with different safety implications activate overlapping representations. Rather than residing in distinct directions, we show that safety is highly entangled with the general learning components of the model. This suggests that subspace-based defenses face fundamental limitations and underscores the need for alternative strategies to preserve safety under continued training. We corroborate these findings with multiple experiments on five open-source LLMs from the Llama and Qwen families. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces.

