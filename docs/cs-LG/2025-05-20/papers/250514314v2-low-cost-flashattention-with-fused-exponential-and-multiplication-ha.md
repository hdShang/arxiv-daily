---
layout: default
title: Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators
---

# Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14314" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14314v2</a>
  <a href="https://arxiv.org/pdf/2505.14314.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14314v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14314v2', 'Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos

**åˆ†ç±»**: cs.AR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-30)

**å¤‡æ³¨**: IEEE Computer Society Annual Symposium on VLSI (ISVLSI 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºèåˆæŒ‡æ•°ä¸ä¹˜æ³•è¿ç®—çš„ç¡¬ä»¶æ“ä½œä»¥ä¼˜åŒ–FlashAttention**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ³¨æ„åŠ›æœºåˆ¶` `FlashAttention` `ç¡¬ä»¶åŠ é€Ÿ` `æŒ‡æ•°è®¡ç®—` `å‘é‡ä¹˜æ³•` `æ·±åº¦å­¦ä¹ ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—æ–¹æ³•åœ¨å¤„ç†é•¿åºåˆ—æ—¶é¢ä¸´æ€§èƒ½ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨ç¡¬ä»¶å®ç°ä¸Šå­˜åœ¨é¢ç§¯å’ŒåŠŸè€—çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¡¬ä»¶æ“ä½œç¬¦ExpMulï¼Œé€šè¿‡èåˆæŒ‡æ•°è®¡ç®—å’Œå‘é‡ä¹˜æ³•ï¼Œä¼˜åŒ–äº†FlashAttentionçš„å†…æ ¸ã€‚
3. åœ¨28nm ASICæŠ€æœ¯ä¸‹ï¼Œæ‰€ææ–¹æ³•åœ¨é¢ç§¯å’ŒåŠŸè€—æ–¹é¢åˆ†åˆ«æå‡äº†28.8%å’Œ17.6%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨Transformeræ¶æ„å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå·²å½»åº•æ”¹å˜äº†æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„åºåˆ—å»ºæ¨¡ã€‚ä¸ºäº†è®¡ç®—è¶Šæ¥è¶Šé•¿çš„åºåˆ—çš„æ³¨æ„åŠ›ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†ä¸“ç”¨åŠ é€Ÿå™¨ï¼Œç›´æ¥åœ¨ç¡¬ä»¶ä¸­æ‰§è¡Œå…³é”®çš„æ³¨æ„åŠ›æ­¥éª¤ã€‚æœ¬æ–‡èšç„¦äºä¼˜åŒ–åŸºäºæµ®ç‚¹æ•°çš„FlashAttentionå†…æ ¸ï¼Œé‡‡ç”¨æ–°çš„ç¡¬ä»¶æ“ä½œç¬¦èåˆæŒ‡æ•°å’Œå‘é‡ä¹˜æ³•çš„è®¡ç®—ã€‚æ‰€æå‡ºçš„ExpMulç¡¬ä»¶æ“ä½œç¬¦æ˜¾è‘—é™ä½äº†åŸºäºFlashAttentionçš„ç¡¬ä»¶åŠ é€Ÿå™¨çš„é¢ç§¯å’ŒåŠŸè€—ã€‚åœ¨28nm ASICæŠ€æœ¯ä¸­å®ç°æ—¶ï¼Œä¸é‡‡ç”¨ç‹¬ç«‹æŒ‡æ•°å’Œå‘é‡ä¹˜æ³•ç¡¬ä»¶æ“ä½œç¬¦çš„æœ€å…ˆè¿›ç¡¬ä»¶æ¶æ„ç›¸æ¯”ï¼Œå¹³å‡åœ¨é¢ç§¯ä¸Šæå‡äº†28.8%ï¼Œåœ¨åŠŸè€—ä¸Šæå‡äº†17.6%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰FlashAttentionç®—æ³•åœ¨ç¡¬ä»¶å®ç°ä¸­é¢ç§¯å’ŒåŠŸè€—è¿‡é«˜çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„æ•ˆç‡ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ–°çš„ç¡¬ä»¶æ“ä½œç¬¦ExpMulï¼Œå°†æŒ‡æ•°è®¡ç®—ä¸å‘é‡ä¹˜æ³•èåˆåœ¨ä¸€èµ·ï¼Œä»è€Œå‡å°‘è®¡ç®—æ­¥éª¤å’Œèµ„æºæ¶ˆè€—ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨æé«˜è®¡ç®—æ•ˆç‡å¹¶é™ä½ç¡¬ä»¶æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥åºåˆ—çš„é¢„å¤„ç†ã€æŒ‡æ•°è®¡ç®—ä¸å‘é‡ä¹˜æ³•çš„èåˆã€ä»¥åŠæœ€ç»ˆçš„æ³¨æ„åŠ›è¾“å‡ºç”Ÿæˆã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®è¾“å…¥ã€ExpMulæ“ä½œã€ä»¥åŠç»“æœè¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºExpMulæ“ä½œç¬¦çš„æå‡ºï¼Œå®ƒå°†ä¸¤ä¸ªè®¡ç®—æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªï¼Œä»è€Œæ˜¾è‘—é™ä½äº†ç¡¬ä»¶çš„é¢ç§¯å’ŒåŠŸè€—ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¸­åˆ†å¼€å¤„ç†çš„æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†28nm ASICæŠ€æœ¯ï¼Œä¼˜åŒ–äº†ç¡¬ä»¶å¸ƒå±€å’Œç”µæºç®¡ç†ï¼Œç¡®ä¿åœ¨å®ç°ExpMulæ“ä½œæ—¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨èµ„æºï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„è®¡ç®—æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„ExpMulç¡¬ä»¶æ“ä½œç¬¦åœ¨28nm ASICæŠ€æœ¯ä¸‹ï¼Œå¹³å‡åœ¨é¢ç§¯ä¸Šæå‡äº†28.8%ï¼Œåœ¨åŠŸè€—ä¸Šæå‡äº†17.6%ã€‚è¿™äº›ç»“æœç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„ç¡¬ä»¶æ¶æ„ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ä»¥åŠå…¶ä»–éœ€è¦é«˜æ•ˆåºåˆ—å»ºæ¨¡çš„äººå·¥æ™ºèƒ½ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–ç¡¬ä»¶å®ç°ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹æ¨ç†ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Attention mechanisms, particularly within Transformer architectures and large language models (LLMs), have revolutionized sequence modeling in machine learning and artificial intelligence applications. To compute attention for increasingly long sequences, specialized accelerators have been proposed to execute key attention steps directly in hardware. Among the various recently proposed architectures, those based on variants of the FlashAttention algorithm, originally designed for GPUs, stand out due to their optimized computation, tiling capabilities, and reduced memory traffic. In this work, we focus on optimizing the kernel of floating-point-based FlashAttention using new hardware operators that fuse the computation of exponentials and vector multiplications, e.g., e^x, V. The proposed ExpMul hardware operators significantly reduce the area and power costs of FlashAttention-based hardware accelerators. When implemented in a 28nm ASIC technology, they achieve improvements of 28.8% in area and 17.6% in power, on average, compared to state-of-the-art hardware architectures with separate exponentials and vector multiplications hardware operators.

