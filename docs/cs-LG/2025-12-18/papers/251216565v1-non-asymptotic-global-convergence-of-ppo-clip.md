---
layout: default
title: Non-Asymptotic Global Convergence of PPO-Clip
---

# Non-Asymptotic Global Convergence of PPO-Clip

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16565" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16565v1</a>
  <a href="https://arxiv.org/pdf/2512.16565.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16565v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16565v1', 'Non-Asymptotic Global Convergence of PPO-Clip')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yin Liu, Qiming Dai, Junyu Zhang, Zaiwen Wen

**åˆ†ç±»**: math.OC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPPO-Clipç®—æ³•çš„éæ¸è¿‘å…¨å±€æ”¶æ•›æ€§åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `PPOç®—æ³•` `KLæ•£åº¦` `æ”¶æ•›æ€§åˆ†æ` `ç†è®ºç ”ç©¶` `ç®—æ³•ç¨³å®šæ€§` `äººç±»åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„PPOç®—æ³•åœ¨ç†è®ºåˆ†æä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ”¶æ•›æ€§å’Œç¨³å®šæ€§æ–¹é¢çš„ç†è§£è¾ƒä¸ºæœ‰é™ã€‚
2. è®ºæ–‡é€šè¿‡åˆ†æç¡®å®šæ€§PPOç®—æ³•ï¼Œç»“åˆf-æ•£åº¦æ­£åˆ™åŒ–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºæ¡†æ¶ï¼Œå¢å¼ºäº†å¯¹PPO-Clipç®—æ³•çš„ç†è§£ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œå‰å‘KLæ­£åˆ™åŒ–å™¨å¯ä»¥å®ç°éæ¸è¿‘çº¿æ€§æ”¶æ•›ï¼Œè€Œåå‘KLæ­£åˆ™åŒ–å™¨åˆ™å…·å¤‡å¹³ç¨³å’Œå±€éƒ¨çº¿æ€§æ”¶æ•›æ€§ï¼Œæå‡äº†ç®—æ³•çš„ç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å› å…¶åœ¨é€šè¿‡äººç±»åé¦ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¯¹é½çš„èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚æœ¬æ–‡é’ˆå¯¹ç°æœ‰çš„PPOç®—æ³•åœ¨ç†è®ºç†è§£ä¸Šçš„ä¸è¶³ï¼Œåˆ†æäº†åœ¨ä¸€èˆ¬RLè®¾ç½®ä¸‹ï¼Œé‡‡ç”¨è½¯æœ€å¤§ç­–ç•¥å‚æ•°åŒ–çš„ç¡®å®šæ€§PPOç®—æ³•ï¼Œå¹¶å¼•å…¥äº†f-æ•£åº¦æ­£åˆ™åŒ–ã€‚é€šè¿‡æ¨å¯¼éå‡åŒ€Lipschitzå…‰æ»‘æ€§æ¡ä»¶å’ŒÅojasiewiczä¸ç­‰å¼ï¼Œå»ºç«‹äº†å‰å‘KLæ­£åˆ™åŒ–å™¨çš„éæ¸è¿‘çº¿æ€§æ”¶æ•›ç‡ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å¯¼äº†åå‘KLæ­£åˆ™åŒ–å™¨çš„å¹³ç¨³æ”¶æ•›å’Œå±€éƒ¨çº¿æ€§æ”¶æ•›æ€§ï¼Œä¸ºPPO-Clipç®—æ³•çš„ç†è®ºåŸºç¡€æä¾›äº†é‡è¦æ”¯æŒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³PPOç®—æ³•åœ¨ç†è®ºåˆ†æä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¯¹å…¶æ”¶æ•›æ€§å’Œç¨³å®šæ€§çš„ä¸¥æ ¼ç†è§£ã€‚ç°æœ‰æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹ç³»ç»Ÿçš„ç†è®ºæ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡åˆ†æç¡®å®šæ€§PPOç®—æ³•ï¼Œç»“åˆf-æ•£åº¦æ­£åˆ™åŒ–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨æä¾›å¯¹PPO-Clipç®—æ³•çš„æ·±å…¥ç†è§£ï¼Œå¹¶ç¡®ä¿å…¶æ”¶æ•›æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹PPOç®—æ³•çš„ç†è®ºåˆ†æï¼Œæ¨å¯¼éå‡åŒ€Lipschitzå…‰æ»‘æ€§æ¡ä»¶å’ŒÅojasiewiczä¸ç­‰å¼ï¼Œè¿›è€Œå»ºç«‹æ”¶æ•›æ€§ç»“æœã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç®—æ³•è®¾è®¡ã€æ­£åˆ™åŒ–ç­–ç•¥å’Œæ”¶æ•›æ€§åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé¦–æ¬¡ä¸ºPPO-Clipç®—æ³•æä¾›äº†éæ¸è¿‘å…¨å±€æ”¶æ•›æ€§çš„ç†è®ºåˆ†æï¼Œå°¤å…¶æ˜¯åœ¨å‰å‘å’Œåå‘KLæ­£åˆ™åŒ–å™¨ä¸‹çš„ä¸åŒæ”¶æ•›ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å¼•å…¥äº†f-æ•£åº¦æ­£åˆ™åŒ–ï¼Œé‡‡ç”¨è½¯æœ€å¤§ç­–ç•¥å‚æ•°åŒ–ï¼Œå¹¶æ¨å¯¼äº†ç›¸å…³çš„å…‰æ»‘æ€§æ¡ä»¶å’Œä¸ç­‰å¼ï¼Œä¸ºç®—æ³•çš„æ”¶æ•›æ€§æä¾›äº†ç†è®ºä¾æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå‰å‘KLæ­£åˆ™åŒ–å™¨å®ç°äº†éæ¸è¿‘çº¿æ€§æ”¶æ•›ï¼Œè€Œåå‘KLæ­£åˆ™åŒ–å™¨åˆ™å±•ç°äº†å¹³ç¨³å’Œå±€éƒ¨çº¿æ€§æ”¶æ•›æ€§ã€‚è¿™äº›ç»“æœä¸ºPPO-Clipç®—æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§æä¾›äº†å¼ºæœ‰åŠ›çš„ç†è®ºæ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨äººæ§åˆ¶å’Œå…¶ä»–éœ€è¦é€šè¿‡äººç±»åé¦ˆè¿›è¡Œå­¦ä¹ çš„æ™ºèƒ½ç³»ç»Ÿã€‚é€šè¿‡å¢å¼ºPPO-Clipç®—æ³•çš„ç†è®ºåŸºç¡€ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æé«˜ç®—æ³•çš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ï¼Œè¿›è€Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \(f\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \(f\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Åojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.

