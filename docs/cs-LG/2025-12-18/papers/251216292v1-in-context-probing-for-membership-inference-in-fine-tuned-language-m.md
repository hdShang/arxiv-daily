---
layout: default
title: In-Context Probing for Membership Inference in Fine-Tuned Language Models
---

# In-Context Probing for Membership Inference in Fine-Tuned Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16292" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16292v1</a>
  <a href="https://arxiv.org/pdf/2512.16292.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16292v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16292v1', 'In-Context Probing for Membership Inference in Fine-Tuned Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhexi Lu, Hongliang Chi, Nathalie Baracaldo, Swanand Ravindra Kadhe, Yuseok Jeon, Lei Yu

**åˆ†ç±»**: cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºICP-MIAæ¡†æ¶ä»¥è§£å†³ç»†è°ƒè¯­è¨€æ¨¡å‹çš„æˆå‘˜æ¨æ–­æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æˆå‘˜æ¨æ–­æ”»å‡»` `éšç§ä¿æŠ¤` `è¯­è¨€æ¨¡å‹` `ä¼˜åŒ–å·®è·` `ä¸Šä¸‹æ–‡æ¢æµ‹` `é»‘ç®±æ”»å‡»` `æ¨¡å‹å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é»‘ç®±æˆå‘˜æ¨æ–­æ”»å‡»æ–¹æ³•ä¾èµ–äºç½®ä¿¡åº¦åˆ†æ•°ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®å’Œä¿¡å™ªæ¯”ä½ã€‚
2. æœ¬æ–‡æå‡ºICP-MIAæ¡†æ¶ï¼Œåˆ©ç”¨ä¼˜åŒ–å·®è·ä½œä¸ºæˆå‘˜èµ„æ ¼ä¿¡å·ï¼Œè®¾è®¡ä¸Šä¸‹æ–‡æ¢æµ‹æ–¹æ³•ä»¥ä¼°è®¡è¯¥å·®è·ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒICP-MIAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ”»å‡»æ•ˆæœï¼Œå°¤å…¶åœ¨ä½å‡é˜³æ€§ç‡ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰å¯¹ç»†è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ„æˆäº†ä¸¥é‡çš„éšç§å¨èƒï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æ•æ„Ÿæ•°æ®è¿›è¡Œé¢†åŸŸç‰¹å®šä»»åŠ¡é€‚é…æ—¶ã€‚ä»¥å¾€çš„é»‘ç®±MIAæŠ€æœ¯ä¾èµ–äºç½®ä¿¡åº¦åˆ†æ•°æˆ–æ ‡è®°ä¼¼ç„¶æ€§ï¼Œä½†è¿™äº›ä¿¡å·å¸¸å¸¸ä¸æ ·æœ¬çš„å†…åœ¨å±æ€§äº¤ç»‡åœ¨ä¸€èµ·ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®å’Œä¿¡å™ªæ¯”ä½ã€‚æœ¬æ–‡æå‡ºäº†ICP-MIAï¼Œä¸€ä¸ªåŸºäºè®­ç»ƒåŠ¨æ€ç†è®ºçš„æ–°å‹MIAæ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æ”¶ç›Šé€’å‡ç°è±¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¼˜åŒ–å·®è·ä½œä¸ºæˆå‘˜èµ„æ ¼çš„åŸºæœ¬ä¿¡å·ï¼šåœ¨æ”¶æ•›æ—¶ï¼Œæˆå‘˜æ ·æœ¬çš„å‰©ä½™æŸå¤±å‡å°‘æ½œåŠ›æœ€å°ï¼Œè€Œéæˆå‘˜åˆ™ä¿ç•™æ˜¾è‘—çš„è¿›ä¸€æ­¥ä¼˜åŒ–æ½œåŠ›ã€‚ä¸ºåœ¨é»‘ç®±è®¾ç½®ä¸­ä¼°è®¡è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡æ¢æµ‹ï¼ˆICPï¼‰ï¼Œä¸€ç§é€šè¿‡æˆ˜ç•¥æ€§æ„å»ºè¾“å…¥ä¸Šä¸‹æ–‡æ¨¡æ‹Ÿç»†è°ƒè¡Œä¸ºçš„æ— è®­ç»ƒæ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒICP-MIAåœ¨å¤šä¸ªLLMä¸Šæ˜¾è‘—ä¼˜äºä»¥å¾€çš„é»‘ç®±MIAï¼Œå°¤å…¶æ˜¯åœ¨ä½å‡é˜³æ€§ç‡ä¸‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯ç»†è°ƒè¯­è¨€æ¨¡å‹ä¸­çš„æˆå‘˜æ¨æ–­æ”»å‡»é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºç½®ä¿¡åº¦åˆ†æ•°ï¼Œå¯¼è‡´ä¿¡å·ä¸æ ·æœ¬å±æ€§äº¤ç»‡ï¼Œå½±å“æ”»å‡»æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºICP-MIAæ¡†æ¶ï¼Œåˆ©ç”¨ä¼˜åŒ–å·®è·ä½œä¸ºæˆå‘˜èµ„æ ¼ä¿¡å·ï¼Œè®¾è®¡ä¸Šä¸‹æ–‡æ¢æµ‹æ–¹æ³•ä»¥åœ¨é»‘ç®±ç¯å¢ƒä¸­ä¼°è®¡è¯¥å·®è·ï¼Œä»è€Œæé«˜æ”»å‡»çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šICP-MIAæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¼˜åŒ–å·®è·çš„å®šä¹‰ä¸ä¼°è®¡ï¼Œä»¥åŠä¸Šä¸‹æ–‡æ¢æµ‹ç­–ç•¥çš„å®æ–½ã€‚ä¸Šä¸‹æ–‡æ¢æµ‹é€šè¿‡æ„å»ºå‚è€ƒæ•°æ®å’Œè‡ªæ‰°åŠ¨ä¸¤ç§ç­–ç•¥æ¥æ¨¡æ‹Ÿç»†è°ƒè¡Œä¸ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥ä¼˜åŒ–å·®è·ä½œä¸ºæˆå‘˜èµ„æ ¼çš„ä¿¡å·ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡æ¢æµ‹æ–¹æ³•åœ¨æ— è®­ç»ƒçš„æƒ…å†µä¸‹æœ‰æ•ˆä¼°è®¡è¿™ä¸€å·®è·ï¼Œæ˜¾è‘—æå‡äº†æ”»å‡»çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å‚è€ƒæ•°æ®çš„é€‰æ‹©ï¼ˆä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼çš„å…¬å…±æ ·æœ¬ï¼‰å’Œè‡ªæ‰°åŠ¨ç­–ç•¥ï¼ˆé€šè¿‡æ©è”½æˆ–ç”Ÿæˆï¼‰ï¼Œç¡®ä¿æ¢æµ‹è¿‡ç¨‹çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒä¸­è¿˜è€ƒè™‘äº†æ¨¡å‹ç±»å‹ã€PEFTé…ç½®å’Œè®­ç»ƒè®¡åˆ’å¯¹æ”»å‡»æ•ˆæœçš„å½±å“ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16292v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16292v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16292v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒICP-MIAåœ¨ä¸‰ä¸ªä»»åŠ¡å’Œå¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºä»¥å¾€çš„é»‘ç®±MIAæ–¹æ³•ï¼Œå°¤å…¶åœ¨ä½å‡é˜³æ€§ç‡ä¸‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éšç§å®¡è®¡ã€æ¨¡å‹å®‰å…¨æ€§è¯„ä¼°ä»¥åŠæ•æ„Ÿæ•°æ®å¤„ç†ç­‰ã€‚ICP-MIAæ¡†æ¶ä¸ºè¯„ä¼°å’Œé™ä½éƒ¨ç½²è¯­è¨€æ¨¡å‹çš„éšç§é£é™©æä¾›äº†ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ï¼Œæœªæ¥å¯å¹¿æ³›åº”ç”¨äºå„ç±»éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„AIç³»ç»Ÿä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.

