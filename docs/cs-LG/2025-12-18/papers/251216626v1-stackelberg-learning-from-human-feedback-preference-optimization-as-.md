---
layout: default
title: Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game
---

# Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16626" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16626v1</a>
  <a href="https://arxiv.org/pdf/2512.16626.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16626v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16626v1', 'Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Barna PÃ¡sztor, Thomas Kleine Buening, Andreas Krause

**åˆ†ç±»**: cs.LG, cs.AI, cs.GT, cs.MA, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: 10 pages, 5 tables, 1 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStackelbergå­¦ä¹ æ¡†æ¶SLHFï¼Œé€šè¿‡åºè´¯åšå¼ˆä¼˜åŒ–äººç±»åé¦ˆåå¥½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `åå¥½å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `Stackelbergåšå¼ˆ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å’Œçº³ä»€å­¦ä¹ ï¼ˆNLHFï¼‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åå¥½ç»“æ„æ—¶å­˜åœ¨å±€é™æ€§ã€‚
2. SLHFå°†åå¥½ä¼˜åŒ–å»ºæ¨¡ä¸ºé¢†å¯¼è€…-è·Ÿéšè€…çš„åºè´¯åšå¼ˆï¼Œåˆ©ç”¨åºè´¯åšå¼ˆçš„ä¸å¯¹ç§°æ€§æ¥æ•æ‰æ›´ä¸°å¯Œçš„åå¥½ç»“æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSLHFåœ¨ä¸åŒè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†å¼ºå¤§çš„å¯¹é½ï¼Œå¹¶èƒ½è¿›è¡Œè·¨æ¨¡å‹è¿ç§»çš„æ¨ç†æ—¶ä¼˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åå¥½ä¼˜åŒ–æ¡†æ¶â€”â€”Stackelbergå­¦ä¹ æ¡†æ¶ï¼ˆSLHFï¼‰ã€‚SLHFå°†å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºä¸¤ä¸ªç­–ç•¥ä¹‹é—´çš„åºè´¯åšå¼ˆï¼šé¢†å¯¼è€…ï¼ˆLeaderï¼‰æ‰¿è¯ºä¸€ä¸ªåŠ¨ä½œï¼Œè·Ÿéšè€…ï¼ˆFollowerï¼‰æ ¹æ®é¢†å¯¼è€…çš„åŠ¨ä½œåšå‡ºå“åº”ã€‚è¿™ç§æ–¹æ³•å°†åå¥½ä¼˜åŒ–åˆ†è§£ä¸ºè·Ÿéšè€…çš„ä¼˜åŒ–é—®é¢˜å’Œé¢†å¯¼è€…å¯¹æŠ—æ€§ä¼˜åŒ–é—®é¢˜ã€‚ä¸ä¸ºåŠ¨ä½œåˆ†é…æ ‡é‡å¥–åŠ±çš„RLHFæˆ–å¯»æ±‚åŒæ­¥åšå¼ˆå‡è¡¡çš„NLHFä¸åŒï¼ŒSLHFåˆ©ç”¨åºè´¯åšå¼ˆçš„ä¸å¯¹ç§°æ€§æ¥æ•è·æ›´ä¸°å¯Œçš„åå¥½ç»“æ„ã€‚SLHFçš„åºè´¯è®¾è®¡è‡ªç„¶åœ°å®ç°äº†æ¨ç†æ—¶ä¼˜åŒ–ï¼Œå› ä¸ºè·Ÿéšè€…å­¦ä¼šæ”¹è¿›é¢†å¯¼è€…çš„åŠ¨ä½œï¼Œå¹¶ä¸”è¿™äº›æ”¹è¿›å¯ä»¥é€šè¿‡è¿­ä»£é‡‡æ ·æ¥åˆ©ç”¨ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†SLHFã€RLHFå’ŒNLHFçš„è§£æ¦‚å¿µï¼Œå¹¶é˜è¿°äº†åœ¨ä¸€è‡´æ€§ã€æ•°æ®æ•æ„Ÿæ€§å’Œå¯¹éä¼ é€’åå¥½çš„é²æ£’æ€§æ–¹é¢çš„å…³é”®ä¼˜åŠ¿ã€‚å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼ŒSLHFåœ¨ä¸åŒçš„åå¥½æ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„å¯¹é½ï¼Œå¯ä»¥ä»0.5Bæ‰©å±•åˆ°8Bå‚æ•°ï¼Œå¹¶äº§ç”Ÿäº†å¯ä»¥åœ¨æ¨¡å‹ç³»åˆ—ä¹‹é—´è½¬ç§»è€Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒçš„æ¨ç†æ—¶ä¼˜åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•é€šå¸¸å°†äººç±»åé¦ˆè½¬åŒ–ä¸ºæ ‡é‡å¥–åŠ±ï¼Œç®€åŒ–äº†å¤æ‚çš„åå¥½ç»“æ„ã€‚çº³ä»€å­¦ä¹ ï¼ˆNLHFï¼‰åˆ™å‡è®¾ç­–ç•¥åŒæ­¥è¿›è¡Œï¼Œå¿½ç•¥äº†ç­–ç•¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¿™äº›æ–¹æ³•åœ¨å¤„ç†éä¼ é€’åå¥½æˆ–éœ€è¦ç»†ç²’åº¦è°ƒæ•´çš„åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSLHFçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†åå¥½å­¦ä¹ å»ºæ¨¡ä¸ºä¸€ä¸ªStackelbergåšå¼ˆï¼Œå…¶ä¸­é¢†å¯¼è€…ï¼ˆLeaderï¼‰ç­–ç•¥å…ˆè¡ŒåŠ¨ï¼Œè·Ÿéšè€…ï¼ˆFollowerï¼‰ç­–ç•¥æ ¹æ®é¢†å¯¼è€…çš„è¡ŒåŠ¨åšå‡ºååº”ã€‚è¿™ç§åºè´¯åšå¼ˆç»“æ„èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç­–ç•¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶å…è®¸è·Ÿéšè€…å¯¹é¢†å¯¼è€…çš„è¡Œä¸ºè¿›è¡Œç»†åŒ–å’Œæ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSLHFçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè·Ÿéšè€…å­¦ä¹ å’Œé¢†å¯¼è€…ä¼˜åŒ–ã€‚é¦–å…ˆï¼Œè·Ÿéšè€…é€šè¿‡å­¦ä¹ äººç±»åé¦ˆï¼Œå­¦ä¹ å¦‚ä½•æ ¹æ®é¢†å¯¼è€…çš„åŠ¨ä½œè¿›è¡Œæ”¹è¿›ã€‚ç„¶åï¼Œé¢†å¯¼è€…é€šè¿‡å¯¹æŠ—æ€§ä¼˜åŒ–ï¼Œå­¦ä¹ å¦‚ä½•ç”Ÿæˆèƒ½å¤Ÿæœ€å¤§åŒ–äººç±»åå¥½çš„åŠ¨ä½œï¼ŒåŒæ—¶è€ƒè™‘åˆ°è·Ÿéšè€…çš„ååº”ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè·Ÿéšè€…å¯ä»¥å¯¹é¢†å¯¼è€…çš„è¾“å‡ºè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSLHFçš„å…³é”®åˆ›æ–°åœ¨äºå°†åå¥½å­¦ä¹ é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªåºè´¯åšå¼ˆï¼Œä»è€Œèƒ½å¤Ÿæ•æ‰æ›´ä¸°å¯Œçš„åå¥½ç»“æ„ã€‚ä¸RLHFå’ŒNLHFç›¸æ¯”ï¼ŒSLHFèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†éä¼ é€’åå¥½ï¼Œå¹¶ä¸”å…è®¸åœ¨æ¨ç†æ—¶è¿›è¡Œç»†ç²’åº¦ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒSLHFçš„åºè´¯è®¾è®¡ä½¿å¾—è·Ÿéšè€…å­¦ä¹ åˆ°çš„çŸ¥è¯†å¯ä»¥è·¨æ¨¡å‹è¿ç§»ã€‚

**å…³é”®è®¾è®¡**ï¼šSLHFçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨åˆé€‚çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒè·Ÿéšè€…ï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹äººç±»å¯¹é¢†å¯¼è€…åŠ¨ä½œçš„åå¥½ï¼›2) è®¾è®¡æœ‰æ•ˆçš„å¯¹æŠ—æ€§ä¼˜åŒ–ç®—æ³•æ¥è®­ç»ƒé¢†å¯¼è€…ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆèƒ½å¤Ÿæœ€å¤§åŒ–äººç±»åå¥½çš„åŠ¨ä½œï¼›3) æ¢ç´¢ä¸åŒçš„è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æé«˜æ¨ç†æ—¶çš„æ€§èƒ½ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16626v1/x1.png" alt="fig_0" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSLHFåœ¨ä¸åŒçš„åå¥½æ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„å¯¹é½ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ‰©å±•åˆ°ä¸åŒè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ï¼ˆä»0.5Båˆ°8Bå‚æ•°ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒSLHFäº§ç”Ÿçš„æ¨ç†æ—¶ä¼˜åŒ–å¯ä»¥è·¨æ¨¡å‹ç³»åˆ—è½¬ç§»ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒï¼Œè¿™è¡¨æ˜SLHFå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚SLHFåœ¨ä¸€è‡´æ€§ã€æ•°æ®æ•æ„Ÿæ€§å’Œå¯¹éä¼ é€’åå¥½çš„é²æ£’æ€§æ–¹é¢ä¼˜äºRLHFå’ŒNLHFã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SLHFå¯åº”ç”¨äºå„ç§éœ€è¦ä»äººç±»åé¦ˆä¸­å­¦ä¹ çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆå’Œæœºå™¨äººæ§åˆ¶ã€‚é€šè¿‡åˆ©ç”¨SLHFï¼Œå¯ä»¥è®­ç»ƒå‡ºæ›´ç¬¦åˆäººç±»åå¥½ã€æ›´å®‰å…¨å¯é çš„æ™ºèƒ½ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒSLHFçš„æ¨ç†æ—¶ä¼˜åŒ–èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç”¨æˆ·éœ€æ±‚å’Œç¯å¢ƒå˜åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

