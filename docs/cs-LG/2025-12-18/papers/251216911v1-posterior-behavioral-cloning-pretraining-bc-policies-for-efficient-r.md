---
layout: default
title: Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning
---

# Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16911" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16911v1</a>
  <a href="https://arxiv.org/pdf/2512.16911.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16911v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16911v1', 'Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)æ–¹æ³•ï¼Œæå‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„é¢„è®­ç»ƒç­–ç•¥æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åéªŒè¡Œä¸ºå…‹éš†` `å¼ºåŒ–å­¦ä¹ å¾®è°ƒ` `é¢„è®­ç»ƒç­–ç•¥` `æœºå™¨äººæ§åˆ¶` `è¡Œä¸ºå…‹éš†` `ç”Ÿæˆæ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡Œä¸ºå…‹éš†(BC)æ–¹æ³•åœ¨é¢„è®­ç»ƒç­–ç•¥æ—¶ï¼Œéš¾ä»¥ä¿è¯å¯¹æ¼”ç¤ºè€…è¡Œä¸ºçš„å……åˆ†è¦†ç›–ï¼Œé™åˆ¶äº†åç»­å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)æ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼Œç¡®ä¿é¢„è®­ç»ƒç­–ç•¥èƒ½å¤Ÿè¦†ç›–æ¼”ç¤ºè€…çš„è¡Œä¸ºç©ºé—´ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPostBCåœ¨çœŸå®æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æå‡äº†å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ€§èƒ½ï¼Œä¼˜äºæ ‡å‡†è¡Œä¸ºå…‹éš†æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†é¢„è®­ç»ƒç­–ç•¥å¯¹å¼ºåŒ–å­¦ä¹ (RL)å¾®è°ƒæ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•é¢„è®­ç»ƒç­–ç•¥ä»¥ç¡®ä¿å…¶ä½œä¸ºæœ‰æ•ˆçš„å¾®è°ƒåˆå§‹åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ ‡å‡†çš„è¡Œä¸ºå…‹éš†(BC)æ— æ³•ä¿è¯å¯¹æ¼”ç¤ºè€…è¡Œä¸ºçš„è¦†ç›–ï¼Œè¿™æ˜¯æœ‰æ•ˆRLå¾®è°ƒçš„å¿…è¦æ¡ä»¶ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºåéªŒè¡Œä¸ºå…‹éš†(PostBC)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è®­ç»ƒæ¨¡å‹ä»¥æ¨¡æ‹Ÿæ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼Œä»è€Œç¡®ä¿å¯¹æ¼”ç¤ºè€…è¡Œä¸ºçš„è¦†ç›–ï¼Œå¹¶å®ç°æ›´æœ‰æ•ˆçš„å¾®è°ƒã€‚PostBCåœ¨ä¿è¯é¢„è®­ç»ƒæ€§èƒ½ä¸ä½äºBCç­–ç•¥çš„åŒæ—¶ï¼Œå¯ä»¥é€šè¿‡ç°ä»£ç”Ÿæˆæ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸå®ç°ï¼Œå¹¶ä¸”åœ¨çœŸå®çš„æœºå™¨äººæ§åˆ¶åŸºå‡†å’Œå®é™…æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä¸æ ‡å‡†è¡Œä¸ºå…‹éš†ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†RLå¾®è°ƒæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰æ–¹æ³•æ—¨åœ¨ç›´æ¥æ¨¡ä»¿æ¼”ç¤ºæ•°æ®ä¸­çš„åŠ¨ä½œï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½æ— æ³•å®Œå…¨è¦†ç›–æ¼”ç¤ºè€…è¡Œä¸ºçš„åˆ†å¸ƒã€‚è¿™æ„å‘³ç€åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒé˜¶æ®µï¼Œç­–ç•¥å¯èƒ½æ— æ³•æ¢ç´¢åˆ°æ¼”ç¤ºè€…æ›¾ç»é‡‡å–è¿‡çš„å…³é”®åŠ¨ä½œï¼Œä»è€Œé™åˆ¶äº†å¾®è°ƒçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•é¢„è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆè¦†ç›–æ¼”ç¤ºè€…è¡Œä¸ºåˆ†å¸ƒçš„ç­–ç•¥ï¼Œæˆä¸ºäº†ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œä¸å…¶ç›´æ¥æ¨¡ä»¿æ¼”ç¤ºæ•°æ®ä¸­çš„åŠ¨ä½œï¼Œä¸å¦‚å­¦ä¹ æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒã€‚è¿™æ„å‘³ç€æ¨¡å‹éœ€è¦å­¦ä¹ åœ¨ç»™å®šæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œæ¼”ç¤ºè€…é‡‡å–å„ç§åŠ¨ä½œçš„æ¦‚ç‡ã€‚é€šè¿‡å­¦ä¹ åéªŒåˆ†å¸ƒï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„çŠ¶æ€ï¼Œå¹¶ç¡®ä¿åœ¨RLå¾®è°ƒé˜¶æ®µèƒ½å¤Ÿæ¢ç´¢åˆ°æ›´å¹¿æ³›çš„è¡Œä¸ºç©ºé—´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPostBCçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æ”¶é›†æ¼”ç¤ºæ•°æ®é›†ï¼›2) ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚å˜åˆ†è‡ªç¼–ç å™¨VAEæˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œGANï¼‰å­¦ä¹ æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼›3) ä½¿ç”¨å­¦ä¹ åˆ°çš„åéªŒåˆ†å¸ƒç”Ÿæˆç­–ç•¥ï¼›4) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹ç­–ç•¥è¿›è¡Œå¾®è°ƒã€‚å…³é”®åœ¨äºç¬¬äºŒæ­¥ï¼Œå³å¦‚ä½•æœ‰æ•ˆåœ°å­¦ä¹ åéªŒåˆ†å¸ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šPostBCæœ€é‡è¦çš„åˆ›æ–°åœ¨äºï¼Œå®ƒå°†é¢„è®­ç»ƒç­–ç•¥çš„ç›®æ ‡ä»ç›´æ¥æ¨¡ä»¿åŠ¨ä½œï¼Œè½¬å˜ä¸ºå­¦ä¹ åŠ¨ä½œçš„åéªŒåˆ†å¸ƒã€‚è¿™ç§è½¬å˜ä½¿å¾—é¢„è®­ç»ƒç­–ç•¥èƒ½å¤Ÿæ›´å¥½åœ°è¦†ç›–æ¼”ç¤ºè€…è¡Œä¸ºç©ºé—´ï¼Œä»è€Œä¸ºåç»­çš„RLå¾®è°ƒæä¾›æ›´å¥½çš„åˆå§‹åŒ–ã€‚ä¸ä¼ ç»Ÿçš„BCæ–¹æ³•ç›¸æ¯”ï¼ŒPostBCèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„çŠ¶æ€ï¼Œå¹¶æ¢ç´¢åˆ°æ›´å¹¿æ³›çš„è¡Œä¸ºç©ºé—´ã€‚

**å…³é”®è®¾è®¡**ï¼šPostBCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨åˆé€‚çš„ç”Ÿæˆæ¨¡å‹æ¥å­¦ä¹ åéªŒåˆ†å¸ƒï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ã€‚2) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨KLæ•£åº¦æ¥è¡¡é‡ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åéªŒåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚3) åœ¨RLå¾®è°ƒé˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨å„ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨PPOæˆ–SACç­‰ç®—æ³•ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/im/corn_in_pot2.jpg" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16911v1/x2.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çœŸå®çš„æœºå™¨äººæ§åˆ¶åŸºå‡†å’Œå®é™…æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒPostBCæ˜¾è‘—æé«˜äº†RLå¾®è°ƒçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸé¡¹æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä½¿ç”¨PostBCé¢„è®­ç»ƒçš„ç­–ç•¥ï¼Œç»è¿‡RLå¾®è°ƒåï¼ŒæˆåŠŸç‡æ¯”ä½¿ç”¨æ ‡å‡†BCé¢„è®­ç»ƒçš„ç­–ç•¥æé«˜äº†15%ã€‚è¿™è¡¨æ˜PostBCèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜RLå¾®è°ƒçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PostBCæ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œé€šå¸¸éœ€è¦å…ˆä½¿ç”¨å¤§é‡æ¼”ç¤ºæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚PostBCå¯ä»¥ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œæé«˜å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œä»è€ŒåŠ é€Ÿè¿™äº›é¢†åŸŸçš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

