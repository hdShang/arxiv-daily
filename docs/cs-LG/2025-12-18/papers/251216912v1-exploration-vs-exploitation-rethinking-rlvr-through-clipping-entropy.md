---
layout: default
title: Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward
---

# Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16912" class="toolbar-btn" target="_blank">📄 arXiv: 2512.16912v1</a>
  <a href="https://arxiv.org/pdf/2512.16912.pdf" class="toolbar-btn" target="_blank">📥 PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16912v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16912v1', 'Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward')" title="添加到收藏夹">☆ 收藏</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">🔗 分享</button>
</div>


**作者**: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin

**分类**: cs.LG, cs.AI, cs.CL

**发布日期**: 2025-12-18

**备注**: 35 pages

---

## 💡 一句话要点

**通过裁剪、熵和虚假奖励重新思考RLVR，提升LLM数学推理能力**

🎯 **匹配领域**: **支柱二：RL算法与架构 (RL & Architecture)** **支柱九：具身大模型 (Embodied Foundation Models)**

**关键词**: `强化学习` `大型语言模型` `探索-利用权衡` `虚假奖励` `策略熵` `裁剪偏差` `数学推理` `奖励模型`

## 📋 核心要点

1. 现有RLVR方法在探索-利用平衡上存在困境，抑制探索和利用均能提升LLM推理能力，但内在机制不明。
2. 论文提出通过裁剪偏差降低策略熵，使模型输出更自信和确定，从而提升性能，并构建奖励-错位模型。
3. 实验结果表明，虚假奖励下的裁剪偏差能有效降低策略熵，提升模型性能，并解释了虚假奖励的有效性。

## 📝 摘要（中文）

本文研究了具有可验证奖励的强化学习（RLVR）中的探索-利用权衡，RLVR是一种用于改进大型语言模型（LLM）推理的框架。最近的研究表明，RLVR可以通过两种看似矛盾的机制来激发LLM强大的数学推理能力：虚假奖励，通过奖励与ground truth无关的结果来抑制利用；以及熵最小化，通过将模型推向更自信和确定性的输出来抑制探索。这突出了一种令人困惑的动态：抑制利用和抑制探索都能提高推理性能，但协调这些影响的潜在原则仍然知之甚少。我们关注两个基本问题：（i）策略熵如何与性能相关，以及（ii）虚假奖励是否能带来收益，可能是通过裁剪偏差和模型污染的相互作用。我们的结果表明，虚假奖励下的裁剪偏差降低了策略熵，从而产生更自信和确定性的输出，而仅靠熵最小化不足以改进。我们进一步提出了一个奖励-错位模型，解释了为什么虚假奖励可以提高超出污染环境的性能。我们的发现阐明了虚假奖励益处背后的机制，并为更有效的RLVR训练提供了原则。

## 🔬 方法详解

**问题定义**：现有RLVR方法在利用强化学习提升大型语言模型（LLM）的数学推理能力时，存在探索与利用的权衡难题。一方面，虚假奖励（spurious rewards）通过奖励与真实目标无关的结果来抑制模型的过度利用；另一方面，熵最小化通过鼓励模型产生更自信和确定的输出，来抑制模型的过度探索。然而，同时抑制探索和利用却都能提升推理性能，这其中的内在机制尚不明确。现有方法缺乏对策略熵与性能之间关系的深入理解，以及对虚假奖励有效性的合理解释。

**核心思路**：本文的核心思路是，虚假奖励通过引入裁剪偏差（clipping bias）来降低策略熵，从而使得模型产生更自信和确定的输出，进而提升性能。此外，论文提出了一个奖励-错位模型（reward-misalignment model），用于解释虚假奖励在非污染环境下的有效性。通过分析裁剪偏差、策略熵和虚假奖励之间的关系，揭示了RLVR中探索-利用权衡的内在机制。

**技术框架**：论文主要通过理论分析和实验验证来研究RLVR中的探索-利用权衡。具体而言，首先分析了裁剪偏差对策略熵的影响，然后通过实验验证了虚假奖励下裁剪偏差降低策略熵的效果。此外，论文还提出了奖励-错位模型，并基于该模型解释了虚假奖励的有效性。整个研究框架围绕着策略熵、裁剪偏差和虚假奖励展开，旨在揭示RLVR中提升LLM推理能力的内在机制。

**关键创新**：论文的关键创新在于：1) 揭示了虚假奖励通过裁剪偏差降低策略熵，从而提升LLM推理能力的机制；2) 提出了奖励-错位模型，解释了虚假奖励在非污染环境下的有效性。这些发现为理解和改进RLVR训练提供了新的视角和理论依据。与现有方法相比，本文更深入地探讨了策略熵、裁剪偏差和虚假奖励之间的关系，并提出了更具解释性的模型。

**关键设计**：论文的关键设计包括：1) 裁剪偏差的引入，通过限制奖励的范围来影响策略熵；2) 策略熵的计算和分析，用于评估模型的探索程度；3) 奖励-错位模型的构建，用于解释虚假奖励的有效性。此外，论文还设计了一系列实验，用于验证理论分析的正确性，并评估不同方法在提升LLM推理能力方面的性能。

## 🖼️ 关键图片

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16912v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16912v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16912v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## 📊 实验亮点

实验结果表明，在虚假奖励下，裁剪偏差能够有效降低策略熵，使得模型输出更加自信和确定，从而提升了LLM的推理性能。此外，奖励-错位模型成功解释了虚假奖励在非污染环境下的有效性，为RLVR训练提供了新的理论依据。这些发现为改进RLVR训练策略，提升LLM的推理能力提供了重要的指导。

## 🎯 应用场景

该研究成果可应用于提升大型语言模型在数学推理、代码生成等领域的性能。通过合理设计奖励机制，平衡探索与利用，可以训练出更可靠、更高效的LLM。此外，该研究对于理解和改进其他基于强化学习的LLM训练方法也具有重要的参考价值，有助于推动人工智能技术的发展。

## 📄 摘要（原文）

> This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

