---
layout: default
title: Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling
---

# Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03799" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03799v1</a>
  <a href="https://arxiv.org/pdf/2505.03799.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03799v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03799v1', 'Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hyun Lee, Chris Yi, Maminur Islam, B. D. S. Aritra

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

**å¤‡æ³¨**: To be published in International Joint Conference on Neural Networks (IJCNN), 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSDM-InstructGLMä»¥è§£å†³å¤§è§„æ¨¡å›¾å¤„ç†çš„å¯æ‰©å±•æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾è¯­è¨€æ¨¡å‹` `å›¾ç¥ç»ç½‘ç»œ` `è‡ªç„¶è¯­è¨€å¤„ç†` `èŠ‚ç‚¹åˆ†ç±»` `é“¾æ¥é¢„æµ‹` `ç›¸ä¼¼åº¦è®¡ç®—` `éšæœºæ¸¸èµ°` `å¯æ‰©å±•æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç»“åˆï¼Œå¯¼è‡´åœ¨å¤„ç†å¤§è§„æ¨¡å›¾æ—¶é¢ä¸´å¯æ‰©å±•æ€§å’Œä¿¡æ¯æŸå¤±é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºSDM-InstructGLMï¼Œé€šè¿‡åŸºäºç›¸ä¼¼åº¦å’Œåº¦ä¸­å¿ƒæ€§çš„åç½®éšæœºæ¸¸èµ°æœºåˆ¶ï¼Œç›´æ¥åœ¨LLMsä¸­ç¼–ç å›¾ç»“æ„ï¼Œæå‡äº†å›¾ä¿¡æ¯çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSDM-InstructGLMåœ¨èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ˜¾è‘—æé«˜äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å›¾ç›¸å…³é—®é¢˜ä¸Šçš„åº”ç”¨å—åˆ°å¯æ‰©å±•æ€§é™åˆ¶å’Œç¼ºä¹ä¸“é—¨å¤„ç†å›¾ç»“æ„æœºåˆ¶çš„å½±å“ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å°†LLMsä¸å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ç»“åˆï¼Œä½¿ç”¨GNNsä½œä¸ºç‰¹å¾ç¼–ç å™¨æˆ–è¾…åŠ©ç»„ä»¶ã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡å›¾çš„èƒŒæ™¯ä¸‹ï¼Œç›´æ¥åœ¨LLMsä¸­ç¼–ç å›¾ç»“æ„çš„ç ”ç©¶å°šä¸å……åˆ†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æŒ‡ä»¤è°ƒä¼˜å›¾è¯­è¨€æ¨¡å‹æ¡†æ¶SDM-InstructGLMï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åŸºäºç›¸ä¼¼åº¦å’Œåº¦ä¸­å¿ƒæ€§çš„åç½®éšæœºæ¸¸èµ°æœºåˆ¶ï¼Œé€‰æ‹©æ€§åœ°é‡‡æ ·å’Œç¼–ç å›¾ä¿¡æ¯ï¼Œä»è€Œåœ¨LLMsä¸­ç¡®ä¿è‡ªé€‚åº”å’Œç»“æ„åŒ–çš„è¡¨ç¤ºã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ä»¤ç‰Œæ•ˆç‡ï¼Œå‡è½»äº†éšæœºé‡‡æ ·å¯¼è‡´çš„ä¿¡æ¯æŸå¤±ï¼Œå¹¶åœ¨èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ç­‰å›¾ä»»åŠ¡ä¸Šæå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å›¾ç»“æ„æ—¶çš„å¯æ‰©å±•æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ï¼Œåœ¨å¤§è§„æ¨¡å›¾çš„èƒŒæ™¯ä¸‹å­˜åœ¨ä»¤ç‰Œé™åˆ¶å’Œä¿¡æ¯æŸå¤±çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºSDM-InstructGLMæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åŸºäºç›¸ä¼¼åº¦å’Œåº¦ä¸­å¿ƒæ€§çš„åç½®éšæœºæ¸¸èµ°æœºåˆ¶ï¼Œé€‰æ‹©æ€§åœ°é‡‡æ ·å›¾ä¿¡æ¯ï¼Œç¡®ä¿åœ¨LLMsä¸­å®ç°é«˜æ•ˆçš„å›¾ç»“æ„è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç›¸ä¼¼åº¦è®¡ç®—ã€åç½®éšæœºæ¸¸èµ°å’Œå›¾ä¿¡æ¯ç¼–ç ç­‰ä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„å›¾å¤„ç†æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºç›¸ä¼¼åº¦å’Œåº¦ä¸­å¿ƒæ€§çš„é‡‡æ ·æœºåˆ¶ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿç‹¬ç«‹äºGNNsè¿›è¡Œå›¾ä¿¡æ¯å¤„ç†ï¼Œæ˜¾è‘—æå‡äº†å›¾ä»»åŠ¡çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„ç›¸ä¼¼åº¦åº¦é‡å’Œä¸­å¿ƒæ€§è®¡ç®—æ–¹æ³•ï¼Œç¡®ä¿äº†é‡‡æ ·çš„æœ‰æ•ˆæ€§å’Œä¿¡æ¯çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSDM-InstructGLMåœ¨èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†15%-30%çš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨ä»¤ç‰Œä½¿ç”¨æ•ˆç‡ä¸Šæé«˜äº†20%ï¼ŒéªŒè¯äº†å…¶åœ¨å¤§è§„æ¨¡å›¾å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ„å»ºç­‰ï¼Œèƒ½å¤Ÿä¸ºå›¾æ•°æ®çš„å¤„ç†æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æœªæ¥ï¼Œéšç€LLMsåœ¨å›¾å­¦ä¹ ä¸­çš„è¿›ä¸€æ­¥åº”ç”¨ï¼Œå¯èƒ½ä¼šæ¨åŠ¨æ— GNNæ–¹æ³•çš„å‘å±•ï¼Œæå‡å›¾æ¨ç†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.

