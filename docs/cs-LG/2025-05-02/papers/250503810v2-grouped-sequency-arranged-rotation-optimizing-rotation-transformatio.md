---
layout: default
title: "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free"
---

# Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03810" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03810v2</a>
  <a href="https://arxiv.org/pdf/2505.03810.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03810v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03810v2', 'Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Euntae Choi, Sumin Song, Woosang Lim, Sungjoo Yoo

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02 (æ›´æ–°: 2025-08-14)

**å¤‡æ³¨**: 7 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGrouped Sequency-arranged Rotationä»¥ä¼˜åŒ–ä½æ¯”ç‰¹é‡åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `æ—‹è½¬çŸ©é˜µ` `Walsh-Hadamardå˜æ¢` `ä½æ¯”ç‰¹å®½åº¦` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–` `æ— è®­ç»ƒæ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ—‹è½¬åŸºæ–¹æ³•åœ¨æä½æ¯”ç‰¹å®½åº¦ä¸‹ï¼ˆå¦‚2æ¯”ç‰¹ï¼‰é‡åŒ–æ€§èƒ½ä¸è¶³ï¼Œå¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„Grouped Sequency-arranged Rotationï¼ˆGSRï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨Walsh-Hadamardå˜æ¢ä¼˜åŒ–æ—‹è½¬çŸ©é˜µï¼Œå‡å°‘é‡åŒ–è¯¯å·®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGSRåœ¨æ¨ç†ä»»åŠ¡å’ŒPPLè¯„åˆ†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨ç°æœ‰å­¦ä¹ æ—‹è½¬æŠ€æœ¯ä¸Šä¹Ÿèƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éƒ¨ç½²æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬çš„æŒ‘æˆ˜ï¼Œå°½ç®¡åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰åŸºäºæ—‹è½¬çš„æ–¹æ³•åœ¨æä½æ¯”ç‰¹å®½åº¦ï¼ˆå¦‚2æ¯”ç‰¹ï¼‰ä¸‹è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ”¹è¿›æ—‹è½¬çŸ©é˜µæ¥è§£å†³å½“å‰æ–¹æ³•çš„å±€é™æ€§ã€‚å…³é”®è´¡çŒ®åŒ…æ‹¬åˆ©ç”¨å…·æœ‰åºåˆ—æ’åºçš„Walsh-Hadamardå˜æ¢ï¼Œèšç±»ç›¸ä¼¼é¢‘ç‡æˆåˆ†ä»¥å‡å°‘é‡åŒ–è¯¯å·®ï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å°Walshå—çš„åˆ†ç»„åºåˆ—æ’åˆ—æ—‹è½¬ï¼ˆGSRï¼‰ï¼Œæœ‰æ•ˆéš”ç¦»å¼‚å¸¸å€¼å½±å“ï¼Œå®ç°ä¸åŸºäºä¼˜åŒ–çš„æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œè€Œæ— éœ€ä»»ä½•è®­ç»ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨æ¨ç†ä»»åŠ¡å’ŒWikiText-2çš„å›°æƒ‘åº¦ï¼ˆPPLï¼‰è¯„åˆ†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ç°æœ‰å­¦ä¹ æ—‹è½¬æŠ€æœ¯ä¸Šè¿›ä¸€æ­¥æå‡äº†ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½æ¯”ç‰¹é‡åŒ–æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œç°æœ‰æ—‹è½¬åŸºæ–¹æ³•åœ¨2æ¯”ç‰¹å®½åº¦ä¸‹çš„é‡åŒ–æ•ˆæœä¸ç†æƒ³ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„Grouped Sequency-arranged Rotationï¼ˆGSRï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å…·æœ‰åºåˆ—æ’åºçš„Walsh-Hadamardå˜æ¢ï¼Œèšç±»ç›¸ä¼¼é¢‘ç‡æˆåˆ†ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘é‡åŒ–è¯¯å·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æ„å»ºæ”¹è¿›çš„æ—‹è½¬çŸ©é˜µï¼Œå…¶æ¬¡æ˜¯åº”ç”¨åˆ†ç»„åºåˆ—æ’åˆ—çš„æ—‹è½¬ï¼Œåˆ©ç”¨å—å¯¹è§’çŸ©é˜µæ¥éš”ç¦»å¼‚å¸¸å€¼çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç»“åˆäº†Walsh-Hadamardå˜æ¢ä¸åºåˆ—æ’åºï¼Œæ˜¾è‘—æå‡äº†é‡åŒ–æ€§èƒ½ï¼Œå¹¶ä¸”GSRæ–¹æ³•åœ¨ä¸éœ€è¦è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸ä¼˜åŒ–æ–¹æ³•ç›¸å½“çš„æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨å°Walshå—æ„å»ºå—å¯¹è§’çŸ©é˜µï¼Œè®¾è®¡äº†é€‚åº”ä½æ¯”ç‰¹é‡åŒ–çš„æŸå¤±å‡½æ•°ï¼Œç¡®ä¿åœ¨é‡åŒ–è¿‡ç¨‹ä¸­ä¿æŒä¿¡æ¯çš„å®Œæ•´æ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒGSRæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹ä½æ¯”ç‰¹é‡åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGSRæ–¹æ³•åœ¨WikiText-2æ•°æ®é›†ä¸Šçš„å›°æƒ‘åº¦ï¼ˆPPLï¼‰è¯„åˆ†æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ—‹è½¬åŸºæ–¹æ³•ï¼Œä¸”åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨ä½æ¯”ç‰¹é‡åŒ–ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜æ˜¾ï¼Œå±•ç¤ºäº†æ— è®­ç»ƒæ–¹æ³•çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²ã€‚é€šè¿‡ä¼˜åŒ–é‡åŒ–è¿‡ç¨‹ï¼ŒGSRæ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„æ¨¡å‹æ¨ç†ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) face deployment challenges due to high computational costs, and while Post-Training Quantization (PTQ) offers a solution, existing rotation-based methods struggle at very low bit-widths like 2-bit. We introduce a novel, training-free approach to construct an improved rotation matrix, addressing the limitations of current methods. The key contributions include leveraging the Walsh-Hadamard transform with sequency ordering, which clusters similar frequency components to reduce quantization error compared to standard Hadamard matrices, significantly improving performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices with smaller Walsh blocks, effectively isolating outlier impacts and achieving performance comparable to optimization-based methods without requiring any training. Our method demonstrates robust performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our method also enhances results even when applied over existing learned rotation techniques.

