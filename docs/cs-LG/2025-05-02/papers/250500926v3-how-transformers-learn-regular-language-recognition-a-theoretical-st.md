---
layout: default
title: How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias
---

# How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00926" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00926v3</a>
  <a href="https://arxiv.org/pdf/2505.00926.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00926v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00926v3', 'How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruiquan Huang, Yingbin Liang, Jing Yang

**åˆ†ç±»**: cs.LG, cs.CL, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02 (æ›´æ–°: 2025-05-28)

**å¤‡æ³¨**: accepted by ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€å±‚å˜æ¢å™¨å­¦ä¹ æ­£åˆ™è¯­è¨€è¯†åˆ«çš„è®­ç»ƒåŠ¨æ€åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ­£åˆ™è¯­è¨€è¯†åˆ«` `å˜æ¢å™¨` `æ¢¯åº¦ä¸‹é™` `æ€ç»´é“¾` `è®­ç»ƒåŠ¨æ€` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹æ€§èƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­£åˆ™è¯­è¨€è¯†åˆ«ä»»åŠ¡æ—¶ï¼Œå°¤å…¶æ˜¯å¥‡å¶æ ¡éªŒä»»åŠ¡ï¼Œé¢ä¸´ç€æœ‰æ•ˆæ€§å’Œæ•ˆç‡çš„æŒ‘æˆ˜ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡ä¸€å±‚å˜æ¢å™¨ç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¥è§£å†³å¶å¯¹å’Œå¥‡å¶æ ¡éªŒä»»åŠ¡ï¼Œåˆ†æå…¶è®­ç»ƒåŠ¨æ€ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚çš„è”åˆè®­ç»ƒèƒ½å¤Ÿæœ‰æ•ˆé™ä½æŸå¤±ï¼Œå¹¶å®ç°æ ·æœ¬çš„æ­£ç¡®åˆ†ç¦»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¯­è¨€è¯†åˆ«ä»»åŠ¡åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è‡³å…³é‡è¦ï¼Œå¹¿æ³›ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶èšç„¦äºä¸¤ä¸ªæ­£åˆ™è¯­è¨€è¯†åˆ«ä»»åŠ¡ï¼Œå³â€œå¶å¯¹â€å’Œâ€œå¥‡å¶æ ¡éªŒâ€ï¼Œæ—¨åœ¨æ¢è®¨ä¸€å±‚å˜æ¢å™¨å¦‚ä½•é€šè¿‡ç†è®ºåˆ†æå…¶åœ¨æ¢¯åº¦ä¸‹é™ä¸‹çš„è®­ç»ƒåŠ¨æ€æ¥è§£å†³è¿™äº›ä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¶å¯¹ä»»åŠ¡å¯ä»¥ç›´æ¥ç”±ä¸€å±‚å˜æ¢å™¨è§£å†³ï¼Œè€Œå¥‡å¶æ ¡éªŒä»»åŠ¡åˆ™éœ€è¦å°†æ€ç»´é“¾ï¼ˆCoTï¼‰æ•´åˆåˆ°å˜æ¢å™¨çš„æ¨ç†é˜¶æ®µæˆ–è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œæ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚çš„è”åˆè®­ç»ƒç»å†ä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µæ³¨æ„åŠ›å±‚å¿«é€Ÿå¢é•¿ï¼Œç¬¬äºŒé˜¶æ®µåˆ™è¶‹äºç¨³å®šï¼Œçº¿æ€§å±‚ä»¥å¯¹æ•°é€Ÿç‡å¢é•¿ï¼Œæœ€ç»ˆå®ç°æ­£ç¡®çš„æ ·æœ¬åˆ†ç¦»ã€‚å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æ­£åˆ™è¯­è¨€è¯†åˆ«ä¸­çš„å¶å¯¹å’Œå¥‡å¶æ ¡éªŒä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¥‡å¶æ ¡éªŒæ—¶æ•ˆç‡ä½ä¸‹ï¼Œæ— æ³•ç›´æ¥åº”ç”¨ä¸€å±‚å˜æ¢å™¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç†è®ºåˆ†æä¸€å±‚å˜æ¢å™¨çš„è®­ç»ƒåŠ¨æ€ï¼Œç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæœ‰æ•ˆè§£å†³å¥‡å¶æ ¡éªŒä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªæ³¨æ„åŠ›å±‚å’Œä¸€ä¸ªçº¿æ€§å±‚ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ç»å†ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ³¨æ„åŠ›å±‚å¿«é€Ÿå¢é•¿ï¼Œç¬¬äºŒé˜¶æ®µæ³¨æ„åŠ›å±‚ç¨³å®šï¼Œçº¿æ€§å±‚é€æ¸æ¥è¿‘æœ€å¤§è¾¹é™…è¶…å¹³é¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°åœ¨äºæ­ç¤ºäº†æ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚çš„è”åˆè®­ç»ƒåŠ¨æ€ï¼Œç‰¹åˆ«æ˜¯å…¶åœ¨ä¸åŒé˜¶æ®µçš„è¡¨ç°å·®å¼‚ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è®ºç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒæŸå¤±å‡½æ•°ä»¥$O(1/t)$çš„é€Ÿç‡ä¸‹é™ï¼Œæ³¨æ„åŠ›å±‚çš„è¾“å‡ºè¢«æ˜ å°„ä¸ºå¯åˆ†ç¦»çš„å‘é‡ï¼Œçº¿æ€§å±‚åˆ™é€šè¿‡å¯¹æ•°å¢é•¿æ¥è¿‘æ­£ç¡®çš„æ ·æœ¬åˆ†ç¦»ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒä¸­è¿›è¡Œäº†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è”åˆè®­ç»ƒçš„å˜æ¢å™¨åœ¨å¶å¯¹å’Œå¥‡å¶æ ¡éªŒä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å¥‡å¶æ ¡éªŒä»»åŠ¡ä¸­ï¼ŒæŸå¤±ä»¥$O(1/t)$çš„é€Ÿç‡ä¸‹é™ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ï¼Œæå‡äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¯­è¨€è¯†åˆ«ç³»ç»Ÿã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬åˆ†æå·¥å…·ã€‚é€šè¿‡æå‡å˜æ¢å™¨åœ¨æ­£åˆ™è¯­è¨€è¯†åˆ«ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œèƒ½å¤Ÿä¸ºç›¸å…³åº”ç”¨æä¾›æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤æ‚è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as `even pairs' and `parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.

