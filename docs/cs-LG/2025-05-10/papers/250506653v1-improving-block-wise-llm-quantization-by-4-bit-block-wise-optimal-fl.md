---
layout: default
title: "Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations"
---

# Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06653" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.06653v1</a>
  <a href="https://arxiv.org/pdf/2505.06653.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06653v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06653v1', 'Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Patrick Blumenberg, Thomas Graave, Tim Fingscheidt

**ÂàÜÁ±ª**: cs.LG, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-10

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫BOF4‰ºòÂåñÂùóÁ∫ßÈáèÂåñ‰ª•Èôç‰ΩéLLMÂÜÖÂ≠òÈúÄÊ±Ç**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂùóÁ∫ßÈáèÂåñ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈáèÂåñ‰ºòÂåñ` `Ê∑∑ÂêàÁ≤æÂ∫¶` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `Ê®°ÂûãÂéãÁº©`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂùóÁ∫ßÈáèÂåñÊñπÊ≥ïÂú®ÈáèÂåñËøáÁ®ã‰∏≠‰∫ßÁîü‰∫ÜÊ¨°‰ºòÁöÑÈáèÂåñËØØÂ∑ÆÔºåÂΩ±Âìç‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂùóÁ∫ßÈáèÂåñ‰ºòÂåñÊñπÊ≥ïBOF4ÔºåÊó®Âú®ÂáèÂ∞ëÈáèÂåñËØØÂ∑ÆÂπ∂ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBOF4ÂèäÂÖ∂Âèò‰ΩìÂú®ËØ≠Ë®ÄÂª∫Ê®°‰ªªÂä°‰∏≠Áõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÊòæËëóÈôç‰Ωé‰∫ÜÂõ∞ÊÉëÂ∫¶ÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÊïàÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂæÆË∞ÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠ÈúÄË¶ÅÂ§ßÈáèÂÜÖÂ≠ò„ÄÇÁé∞ÊúâÁöÑÂùóÁ∫ßÈáèÂåñÊñπÊ≥ïÂ¶ÇNF4ÂíåAF4Â≠òÂú®ÈáèÂåñËØØÂ∑ÆÁöÑ‰∏çË∂≥„ÄÇÊú¨ÊñáÈ¶ñÊ¨°ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂùóÁ∫ßÈáèÂåñÁöÑ‰ºòÂåñÊñπÊ≥ïÔºåËÆæËÆ°‰∫Ü‰∏ÄÁßçÂêç‰∏∫4-bitÂùóÁ∫ßÊúÄ‰ºòÊµÆÁÇπÔºàBOF4ÔºâÁöÑÈáèÂåñÂô®ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÈáèÂåñËØØÂ∑Æ„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫‰∫ÜÂü∫‰∫éÊúâÁ¨¶Âè∑ÁªùÂØπÂùóÊúÄÂ§ßÂÄºÁöÑÂΩí‰∏ÄÂåñÊñπÊ≥ïÔºàBOF4-SÔºâÔºåËøõ‰∏ÄÊ≠•ÂáèÂ∞ëÈáèÂåñËØØÂ∑ÆÂπ∂ÊèêÈ´òËØ≠Ë®ÄÂª∫Ê®°ÊÄßËÉΩ„ÄÇÈÄöËøáÂÆûÈ™åÁ†îÁ©∂ÔºåÊé¢Á¥¢‰∫ÜÂáÜÁ°ÆË°®Á§∫Èõ∂ÂíåÂ§ßÂπÖÂ∫¶ÊùÉÈáçÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÁ≤æÂ∫¶ÈáèÂåñÁ≠ñÁï•ÔºàOPQÔºâÔºåÂú®4-bitÂùóÁ∫ßÈáèÂåñÊäÄÊúØ‰∏≠ÂÆûÁé∞‰∫ÜÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂùóÁ∫ßÈáèÂåñÊñπÊ≥ïÔºàÂ¶ÇNF4ÂíåAF4ÔºâÂú®ÈáèÂåñËøáÁ®ã‰∏≠‰∫ßÁîüÁöÑÊ¨°‰ºòÈáèÂåñËØØÂ∑ÆÔºåÂØºËá¥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÜÖÂ≠ò‰ΩøÁî®ÂíåÊÄßËÉΩ‰∏äÁöÑ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ºòÂåñÂùóÁ∫ßÈáèÂåñÁöÑÊñπÊ≥ïÔºåËÆæËÆ°‰∫Ü4-bitÂùóÁ∫ßÊúÄ‰ºòÊµÆÁÇπÔºàBOF4ÔºâÈáèÂåñÂô®ÔºåÊó®Âú®ÈÄöËøá‰ºòÂåñÈáèÂåñËøáÁ®ãÊù•ÂáèÂ∞ëÈáèÂåñËØØÂ∑Æ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÈáèÂåñÂô®ËÆæËÆ°„ÄÅÂΩí‰∏ÄÂåñÊñπÊ≥ïÁöÑÊîπËøõÔºàBOF4-SÔºâÔºå‰ª•ÂèäÂØπ‰∏çÂêåÈáèÂåñÂèò‰ΩìÁöÑÂÆûÈ™åÁ†îÁ©∂„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨ÈáèÂåñËØØÂ∑Æ‰ºòÂåñ„ÄÅÂΩí‰∏ÄÂåñÂ§ÑÁêÜÂíåÊ∑∑ÂêàÁ≤æÂ∫¶ÈáèÂåñÁ≠ñÁï•ÁöÑÂÆûÊñΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜBOF4ÈáèÂåñÂô®ÂíåBOF4-SÂΩí‰∏ÄÂåñÊñπÊ≥ïÔºåËøô‰∫õÊñπÊ≥ïÂú®ÈáèÂåñËØØÂ∑Æ‰∏ä‰ºò‰∫éÁé∞ÊúâÊäÄÊúØÔºå‰∏îÂú®ËØ≠Ë®ÄÂª∫Ê®°ÊÄßËÉΩ‰∏äË°®Áé∞Êõ¥‰Ω≥„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÊúâÁ¨¶Âè∑ÁªùÂØπÂùóÊúÄÂ§ßÂÄºËøõË°åÂΩí‰∏ÄÂåñÔºå‰ºòÂåñ‰∫ÜÈáèÂåñËøáÁ®ãÔºåÂπ∂ÂºïÂÖ•‰∫ÜÊ∑∑ÂêàÁ≤æÂ∫¶ÈáèÂåñÁ≠ñÁï•ÔºàOPQÔºâÔºå‰ª•Â∫îÂØπÂùóÁ∫ßÈáèÂåñ‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÂàÜÂ∏ÉÈóÆÈ¢ò„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÈááÁî®BOF4ÂèäÂÖ∂Âèò‰ΩìÁöÑÊ®°ÂûãÂú®Âõ∞ÊÉëÂ∫¶‰∏äÁõ∏ËæÉ‰∫é‰º†Áªü4-bitÂùóÁ∫ßÈáèÂåñÊñπÊ≥ïÊúâÊòæËëóÊèêÂçáÔºåÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆË°®ÊòéÔºåBOF4Âú®ÈáèÂåñËØØÂ∑Æ‰∏äÂáèÂ∞ë‰∫ÜXX%ÔºåÂπ∂Âú®ËØ≠Ë®ÄÂª∫Ê®°‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄ‰Ω≥ÊïàÊûú„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊú∫Âô®ÁøªËØëÂíåÂØπËØùÁ≥ªÁªüÁ≠âÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂºÄÂèë‰∏é‰ºòÂåñ„ÄÇÈÄöËøáÈôç‰ΩéÂÜÖÂ≠òÈúÄÊ±ÇÂíåÊèêÈ´òÊ®°ÂûãÊÄßËÉΩÔºåËÉΩÂ§ü‰ΩøÂæóËøô‰∫õÊ®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠Êõ¥‰∏∫È´òÊïàÂú∞ËøêË°åÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) demand extensive memory capacity during both fine-tuning and inference. To enable memory-efficient fine-tuning, existing methods apply block-wise quantization techniques, such as NF4 and AF4, to the network weights. We show that these quantization techniques incur suboptimal quantization errors. Therefore, as a first novelty, we propose an optimization approach for block-wise quantization. Using this method, we design a family of quantizers named 4-bit block-wise optimal float (BOF4), which consistently reduces the quantization error compared to both baseline methods. We provide both a theoretical and a data-driven solution for the optimization process and prove their practical equivalence. Secondly, we propose a modification to the employed normalization method based on the signed absolute block maximum (BOF4-S), enabling further reduction of the quantization error and empirically achieving less degradation in language modeling performance. Thirdly, we explore additional variations of block-wise quantization methods applied to LLMs through an experimental study on the importance of accurately representing zero and large-amplitude weights on the one hand, and optimization towards various error metrics on the other hand. Lastly, we introduce a mixed-precision quantization strategy dubbed outlier-preserving quantization (OPQ) to address the distributional mismatch induced by outlier weights in block-wise quantization. By storing outlier weights in 16-bit precision (OPQ) while applying BOF4-S, we achieve top performance among 4-bit block-wise quantization techniques w.r.t. perplexity.

