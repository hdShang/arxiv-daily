---
layout: default
title: FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference
---

# FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13109" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13109v3</a>
  <a href="https://arxiv.org/pdf/2505.13109.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13109v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13109v3', 'FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guangda Liu, Chengwei Li, Zhenyu Ning, Jing Lin, Yiwu Yao, Danning Ke, Minyi Guo, Jieru Zhao

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-12-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFreeKVä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡KVç¼“å­˜æ£€ç´¢æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `KVç¼“å­˜` `æ£€ç´¢æ•ˆç‡` `æ¨æµ‹æ€§æ£€ç´¢` `ç®—æ³•ä¼˜åŒ–` `ç³»ç»Ÿè®¾è®¡` `åŒç¼“å†²æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿ä¸Šä¸‹æ–‡çš„KVç¼“å­˜å¢é•¿å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´éš¾ä»¥å¹³è¡¡ã€‚
2. FreeKVé€šè¿‡æ¨æµ‹æ€§æ£€ç´¢å’Œç»†ç²’åº¦æ ¡æ­£ï¼Œä¼˜åŒ–KVé€‰æ‹©å’Œå›å¿†è¿‡ç¨‹ï¼Œæå‡æ£€ç´¢æ•ˆç‡å¹¶ä¿æŒå‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFreeKVåœ¨å¤šç§åœºæ™¯ä¸‹å®ç°äº†è¿‘ä¹æ— æŸçš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦æå‡å¯è¾¾13å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ”¯æŒæ—¥ç›Šå¤æ‚çš„åº”ç”¨æ—¶ï¼Œé¢ä¸´ç€é•¿ä¸Šä¸‹æ–‡å¸¦æ¥çš„æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯KVç¼“å­˜çš„å¤§å°éšä¸Šä¸‹æ–‡é•¿åº¦æˆæ¯”ä¾‹å¢é•¿ã€‚å°½ç®¡å·²æœ‰KVç¼“å­˜å‹ç¼©æ–¹æ³•ï¼Œä½†KVä¸¢å¼ƒæ–¹æ³•ä¼šå¯¼è‡´æ˜¾è‘—çš„å‡†ç¡®æ€§æŸå¤±ï¼Œè€ŒKVæ£€ç´¢æ–¹æ³•åˆ™å­˜åœ¨æ•ˆç‡ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FreeKVï¼Œä¸€ä¸ªç®—æ³•-ç³»ç»ŸååŒä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜KVæ£€ç´¢æ•ˆç‡å¹¶ä¿æŒå‡†ç¡®æ€§ã€‚FreeKVåœ¨ç®—æ³•å±‚é¢å¼•å…¥äº†æ¨æµ‹æ€§æ£€ç´¢ï¼Œå°†KVé€‰æ‹©å’Œå›å¿†è¿‡ç¨‹ç§»å‡ºå…³é”®è·¯å¾„ï¼Œå¹¶ç»“åˆç»†ç²’åº¦æ ¡æ­£ä»¥ç¡®ä¿å‡†ç¡®æ€§ã€‚åœ¨ç³»ç»Ÿå±‚é¢ï¼ŒFreeKVé‡‡ç”¨è·¨CPUå’ŒGPUå†…å­˜çš„æ··åˆKVå¸ƒå±€ï¼Œæ¶ˆé™¤æ•°æ®ä¼ è¾“ç¢ç‰‡ï¼Œå¹¶åˆ©ç”¨åŒç¼“å†²æµå¼å›å¿†è¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒFreeKVåœ¨å„ç§åœºæ™¯å’Œæ¨¡å‹ä¸­å®ç°äº†è¿‘ä¹æ— æŸçš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„KVæ£€ç´¢æ–¹æ³•ï¼Œé€Ÿåº¦æå‡å¯è¾¾13å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹KVç¼“å­˜æ£€ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰çš„KVä¸¢å¼ƒæ–¹æ³•ä¼šå¯¼è‡´å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼Œè€ŒKVæ£€ç´¢æ–¹æ³•åˆ™é¢ä¸´æ•ˆç‡ç“¶é¢ˆï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFreeKVçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¨æµ‹æ€§æ£€ç´¢å°†KVé€‰æ‹©å’Œå›å¿†è¿‡ç¨‹ç§»å‡ºå…³é”®è·¯å¾„ï¼Œä»è€Œæé«˜æ£€ç´¢æ•ˆç‡ï¼ŒåŒæ—¶ç»“åˆç»†ç²’åº¦æ ¡æ­£ä»¥ç¡®ä¿æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å‡å°‘æ£€ç´¢è¿‡ç¨‹ä¸­çš„å»¶è¿Ÿï¼Œæå‡æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFreeKVçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç®—æ³•å±‚å’Œç³»ç»Ÿå±‚ã€‚åœ¨ç®—æ³•å±‚ï¼Œé‡‡ç”¨æ¨æµ‹æ€§æ£€ç´¢å’Œç»†ç²’åº¦æ ¡æ­£ï¼›åœ¨ç³»ç»Ÿå±‚ï¼Œä½¿ç”¨æ··åˆKVå¸ƒå±€å’ŒåŒç¼“å†²æµå¼å›å¿†ï¼Œä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæ•°æ®ä¼ è¾“æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šFreeKVçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç®—æ³•-ç³»ç»ŸååŒä¼˜åŒ–çš„æ¡†æ¶ï¼Œé€šè¿‡å°†KVæ£€ç´¢è¿‡ç¨‹çš„å…³é”®è·¯å¾„å¤–ç§»ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢æ•ˆç‡ï¼Œå¹¶ä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒFreeKVé‡‡ç”¨äº†æ··åˆKVå¸ƒå±€ä»¥ä¼˜åŒ–CPUå’ŒGPUå†…å­˜çš„ä½¿ç”¨ï¼Œå‡å°‘æ•°æ®ä¼ è¾“çš„ç¢ç‰‡åŒ–ã€‚åŒæ—¶ï¼ŒåŒç¼“å†²æµå¼å›å¿†æŠ€æœ¯çš„å¼•å…¥è¿›ä¸€æ­¥æå‡äº†æ£€ç´¢æ•ˆç‡ï¼Œç¡®ä¿äº†åœ¨é«˜è´Ÿè½½æƒ…å†µä¸‹çš„ç¨³å®šæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFreeKVåœ¨å¤šç§åœºæ™¯ä¸‹å®ç°äº†è¿‘ä¹æ— æŸçš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„KVæ£€ç´¢æ–¹æ³•ï¼Œé€Ÿåº¦æå‡å¯è¾¾13å€ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡å±•ç¤ºäº†FreeKVåœ¨å®é™…åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FreeKVçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„KVæ£€ç´¢æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å“åº”é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.

