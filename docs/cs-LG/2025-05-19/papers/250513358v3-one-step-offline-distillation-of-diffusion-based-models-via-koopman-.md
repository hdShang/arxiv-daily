---
layout: default
title: One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling
---

# One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13358v3</a>
  <a href="https://arxiv.org/pdf/2505.13358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13358v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13358v3', 'One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nimrod Berman, Ilan Naiman, Moshe Eliasof, Hedi Zisling, Omri Azencot

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-10-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºKoopmanå»ºæ¨¡çš„ä¸€æ­¥ç¦»çº¿è’¸é¦æ–¹æ³•ä»¥æå‡æ‰©æ•£æ¨¡å‹æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `ç¦»çº¿è’¸é¦` `Koopmanç†è®º` `ç”Ÿæˆæ¨¡å‹` `è®¡ç®—æ•ˆç‡` `è¯­ä¹‰ä¸€è‡´æ€§` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éœ€è¦å¤šæ¬¡è¿­ä»£é‡‡æ ·ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„Koopmanè’¸é¦æ¨¡å‹ï¼ˆKDMï¼‰åˆ©ç”¨Koopmanç†è®ºï¼Œé€šè¿‡å°†è¾“å…¥æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ï¼Œå®ç°å•æ­¥ç”Ÿæˆï¼Œæå‡äº†æ•ˆç‡ã€‚
3. KDMåœ¨å¤šä¸ªæ ‡å‡†ç¦»çº¿è’¸é¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”çš„ç«äº‰åŠ›å’Œä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶è¿­ä»£é‡‡æ ·è¿‡ç¨‹è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºKoopmanç†è®ºçš„ç¦»çº¿è’¸é¦æ¡†æ¶ï¼Œç§°ä¸ºKoopmanè’¸é¦æ¨¡å‹ï¼ˆKDMï¼‰ï¼Œé€šè¿‡å°†å™ªå£°è¾“å…¥ç¼–ç åˆ°åµŒå…¥ç©ºé—´ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„çº¿æ€§ç®—å­è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œæœ€åé€šè¿‡è§£ç å™¨é‡æ„å¹²å‡€æ ·æœ¬ï¼Œä»è€Œå®ç°å•æ­¥ç”Ÿæˆå¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºæ”¯æŒï¼Œè¯æ˜åœ¨ä¸€å®šå‡è®¾ä¸‹ï¼Œå­¦ä¹ åˆ°çš„æ‰©æ•£åŠ¨æ€å¯ä»¥ç”¨æœ‰é™ç»´çš„Koopmanè¡¨ç¤ºï¼Œä¸”Koopmanæ½œåœ¨ç©ºé—´çš„æ¥è¿‘æ€§ä¸ç”Ÿæˆè¾“å‡ºçš„è¯­ä¹‰ç›¸ä¼¼æ€§ç›¸å…³è”ã€‚KDMåœ¨æ ‡å‡†ç¦»çº¿è’¸é¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–äºè¿­ä»£é‡‡æ ·ï¼Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„KDMé€šè¿‡Koopmanç†è®ºï¼Œå°†å™ªå£°è¾“å…¥æ˜ å°„åˆ°ä¸€ä¸ªåµŒå…¥ç©ºé—´ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„çº¿æ€§ç®—å­è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œä»è€Œå®ç°å•æ­¥ç”Ÿæˆï¼Œä¿æŒç”Ÿæˆæ ·æœ¬çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKDMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¾“å…¥ç¼–ç æ¨¡å—ã€çº¿æ€§ä¼ æ’­æ¨¡å—å’Œè§£ç å™¨æ¨¡å—ã€‚è¾“å…¥ç¼–ç æ¨¡å—å°†å™ªå£°è¾“å…¥æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ï¼Œçº¿æ€§ä¼ æ’­æ¨¡å—åˆ©ç”¨å­¦ä¹ åˆ°çš„ç®—å­è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œæœ€åè§£ç å™¨æ¨¡å—é‡æ„å‡ºå¹²å‡€çš„æ ·æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šKDMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†Koopmanç†è®ºåº”ç”¨äºæ‰©æ•£æ¨¡å‹çš„è’¸é¦è¿‡ç¨‹ï¼Œåˆ©ç”¨çº¿æ€§åŒ–çš„åŠ¨æ€è¡¨ç¤ºæ¥æé«˜ç”Ÿæˆæ•ˆç‡ï¼Œè¿™ä¸ä¼ ç»Ÿçš„è¿­ä»£é‡‡æ ·æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒKDMé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç¡®ä¿åœ¨æ½œåœ¨ç©ºé—´ä¸­ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

KDMåœ¨æ ‡å‡†ç¦»çº¿è’¸é¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç°å‡ºä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”çš„ç«äº‰åŠ›ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜å…¶åœ¨ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç”Ÿæˆã€è§†é¢‘åˆæˆä»¥åŠå…¶ä»–éœ€è¦é«˜æ•ˆç”Ÿæˆçš„ä»»åŠ¡ã€‚é€šè¿‡æå‡æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ï¼ŒKDMèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—é™ä½è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œæ¨åŠ¨ç”Ÿæˆæ¨¡å‹åœ¨å·¥ä¸šç•Œçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion-based generative models have demonstrated exceptional performance, yet their iterative sampling procedures remain computationally expensive. A prominent strategy to mitigate this cost is distillation, with offline distillation offering particular advantages in terms of efficiency, modularity, and flexibility. In this work, we identify two key observations that motivate a principled distillation framework: (1) while diffusion models have been viewed through the lens of dynamical systems theory, powerful and underexplored tools can be further leveraged; and (2) diffusion models inherently impose structured, semantically coherent trajectories in latent space. Building on these observations, we introduce the Koopman Distillation Model (KDM), a novel offline distillation approach grounded in Koopman theory - a classical framework for representing nonlinear dynamics linearly in a transformed space. KDM encodes noisy inputs into an embedded space where a learned linear operator propagates them forward, followed by a decoder that reconstructs clean samples. This enables single-step generation while preserving semantic fidelity. We provide theoretical justification for our approach: (1) under mild assumptions, the learned diffusion dynamics admit a finite-dimensional Koopman representation; and (2) proximity in the Koopman latent space correlates with semantic similarity in the generated outputs, allowing for effective trajectory alignment. KDM achieves highly competitive performance across standard offline distillation benchmarks.

