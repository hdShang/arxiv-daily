---
layout: default
title: Optimizing Anytime Reasoning via Budget Relative Policy Optimization
---

# Optimizing Anytime Reasoning via Budget Relative Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13438" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.13438v3</a>
  <a href="https://arxiv.org/pdf/2505.13438.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13438v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13438v3', 'Optimizing Anytime Reasoning via Budget Relative Policy Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-19 (Êõ¥Êñ∞: 2025-11-07)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AnytimeReasoner‰ª•‰ºòÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂç≥Êó∂Êé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âç≥Êó∂Êé®ÁêÜ` `Âº∫ÂåñÂ≠¶‰π†` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÂèØÈ™åËØÅÂ•ñÂä±` `Á≠ñÁï•‰ºòÂåñ` `Êï∞Â≠¶Êé®ÁêÜ` `‰ª§ÁâåÈ¢ÑÁÆó` `ÊïàÁéáÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Âõ∫ÂÆö‰ª§ÁâåÈ¢ÑÁÆó‰∏ã‰ºòÂåñÊé®ÁêÜÊÄßËÉΩÔºåÂØºËá¥ËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤ÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. ÊèêÂá∫AnytimeReasonerÊ°ÜÊû∂ÔºåÈÄöËøáÊà™Êñ≠ÊÄùËÄÉËøáÁ®ãÂπ∂ÂºïÂÖ•ÂèØÈ™åËØÅÂ•ñÂä±Êù•‰ºòÂåñÂç≥Êó∂Êé®ÁêÜÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åËØÅÊòéËØ•ÊñπÊ≥ïÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Ë∂ÖË∂äGRPOÔºåÊèêÂçá‰∫ÜËÆ≠ÁªÉÂíå‰ª§ÁâåÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊé®ÁêÜËÉΩÂäõÁöÑËøáÁ®ã‰∏≠ÔºåÊµãËØïÊó∂ËÆ°ÁÆóËµÑÊ∫êÁöÑÊâ©Â±ïËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊù•ÊúÄÂ§ßÂåñÂú®Êé®ÁêÜËΩ®ËøπÁªìÊùüÊó∂Ëé∑ÂæóÁöÑÂèØÈ™åËØÅÂ•ñÂä±Ôºå‰ΩÜËøô‰∫õÊñπÊ≥ï‰ªÖÂú®Âõ∫ÂÆöÁöÑ‰ª§ÁâåÈ¢ÑÁÆó‰∏ã‰ºòÂåñÊúÄÁªàÊÄßËÉΩÔºåÈôêÂà∂‰∫ÜËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤ÁöÑÊïàÁéá„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂AnytimeReasonerÔºåÊó®Âú®‰ºòÂåñÂç≥Êó∂Êé®ÁêÜÊÄßËÉΩÔºåÊèêÈ´ò‰ª§ÁâåÊïàÁéáÂíåÂú®‰∏çÂêå‰ª§ÁâåÈ¢ÑÁÆóÁ∫¶Êùü‰∏ãÁöÑÊé®ÁêÜÁÅµÊ¥ªÊÄß„ÄÇÈÄöËøá‰ªéÂÖàÈ™åÂàÜÂ∏É‰∏≠ÊäΩÊ†∑‰ª§ÁâåÈ¢ÑÁÆóÔºåÊà™Êñ≠ÂÆåÊï¥ÁöÑÊÄùËÄÉËøáÁ®ãÔºå‰øÉ‰ΩøÊ®°Âûã‰∏∫ÊØè‰∏™Êà™Êñ≠ÁöÑÊÄùËÄÉÊÄªÁªìÊúÄ‰Ω≥Á≠îÊ°àÂπ∂ËøõË°åÈ™åËØÅÔºå‰ªéËÄåÂºïÂÖ•ÂèØÈ™åËØÅÁöÑÂØÜÈõÜÂ•ñÂä±Ôºå‰øÉËøõRL‰ºòÂåñ‰∏≠ÁöÑÊúâÊïà‰ø°Áî®ÂàÜÈÖç„ÄÇÂÆûÈ™åËØÅÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Âú®ÂêÑÁßçÊÄùËÄÉÈ¢ÑÁÆó‰∏ãÂùá‰ºò‰∫éGRPOÔºåÊèêÂçá‰∫ÜËÆ≠ÁªÉÂíå‰ª§ÁâåÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ïÂú®Âõ∫ÂÆö‰ª§ÁâåÈ¢ÑÁÆó‰∏ã‰ºòÂåñÊé®ÁêÜÊÄßËÉΩÁöÑ‰∏çË∂≥ÔºåÂØºËá¥ËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫AnytimeReasonerÊ°ÜÊû∂ÔºåÈÄöËøáÊà™Êñ≠ÊÄùËÄÉËøáÁ®ãÂπ∂ÂºïÂÖ•ÂèØÈ™åËØÅÁöÑÂØÜÈõÜÂ•ñÂä±Ôºå‰øÉ‰ΩøÊ®°ÂûãÂú®‰∏çÂêå‰ª§ÁâåÈ¢ÑÁÆó‰∏ãËøõË°åÊúâÊïàÊé®ÁêÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÊÄùËÄÉËøáÁ®ãÁöÑÊà™Êñ≠„ÄÅÂ•ñÂä±ÁöÑÂºïÂÖ•Âíå‰ºòÂåñÁ≠ñÁï•ÁöÑËß£ËÄ¶„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨ÊÄùËÄÉÁ≠ñÁï•ÂíåÊÄªÁªìÁ≠ñÁï•ÁöÑ‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂºïÂÖ•‰∫ÜÈ¢ÑÁÆóÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàBRPOÔºâÊäÄÊúØÔºåÂ¢ûÂº∫‰∫ÜÂ≠¶‰π†ËøáÁ®ãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊïàÁéáÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËøõË°å‰ø°Áî®ÂàÜÈÖç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÔºåÈááÁî®‰∫ÜÂèØÈ™åËØÅÁöÑÂØÜÈõÜÂ•ñÂä±Êú∫Âà∂ÔºåÂèÇÊï∞ËÆæÁΩÆ‰∏äÊ†πÊçÆÂÖàÈ™åÂàÜÂ∏ÉËøõË°å‰ª§ÁâåÈ¢ÑÁÆóÁöÑÊäΩÊ†∑ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®‰∏çÂêåÈ¢ÑÁÆó‰∏ãÁöÑÁÅµÊ¥ªÊÄßÂíåÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAnytimeReasonerÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÔºåÂú®ÊâÄÊúâÊÄùËÄÉÈ¢ÑÁÆó‰∏ãÂùá‰ºò‰∫éGRPOÔºåÊèêÂçáÂπÖÂ∫¶ÊòæËëóÔºåÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Êèê‰æõÔºå‰ΩÜÊï¥‰ΩìËÆ≠ÁªÉÂíå‰ª§ÁâåÊïàÁéáÂùáÊúâÊâÄÊèêÈ´ò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÂíåÂÆûÊó∂ÂÜ≥Á≠ñÊîØÊåÅÁ≠â„ÄÇÈÄöËøá‰ºòÂåñÊé®ÁêÜËøáÁ®ãÔºåÊèêÂçá‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊïàÁéáÂíåÁÅµÊ¥ªÊÄßÔºåÊú™Êù•ÂèØËÉΩÂØπÊô∫ËÉΩÂä©ÊâãÂíåËá™Âä®ÂåñÁ≥ªÁªü‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.

