---
layout: default
title: Incentivizing Truthful Language Models via Peer Elicitation Games
---

# Incentivizing Truthful Language Models via Peer Elicitation Games

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13636" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13636v2</a>
  <a href="https://arxiv.org/pdf/2505.13636.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13636v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13636v2', 'Incentivizing Truthful Language Models via Peer Elicitation Games')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Baiting Chen, Tong Zhu, Jiale Han, Lexin Li, Gang Li, Xiaowu Dai

**åˆ†ç±»**: cs.LG, cs.AI, cs.GT

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-10-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒè¡Œå¼•å¯¼æ¸¸æˆä»¥è§£å†³è¯­è¨€æ¨¡å‹çš„çœŸå®æŠ¥å‘Šé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `åšå¼ˆè®º` `åŒè¡Œè¯„ä¼°` `çœŸå®æŠ¥å‘Š` `äº’ä¿¡æ¯è¯„åˆ†` `æ— ç›‘ç£å­¦ä¹ ` `çº³ä»€å‡è¡¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶å¸¸å¸¸å‡ºç°ä¸ä¸€è‡´å’Œå¹»è§‰ï¼Œå¯¼è‡´å…¶è¾“å‡ºçš„å¯é æ€§å—åˆ°è´¨ç–‘ã€‚
2. æœ¬æ–‡æå‡ºçš„åŒè¡Œå¼•å¯¼æ¸¸æˆï¼ˆPEGï¼‰é€šè¿‡åšå¼ˆè®ºæ¡†æ¶å’ŒåŒè¡Œè¯„ä¼°æœºåˆ¶ï¼Œæ—¨åœ¨æ¿€åŠ±è¯­è¨€æ¨¡å‹çœŸå®æŠ¥å‘Šè€Œæ— éœ€çœŸå®æ ‡ç­¾ã€‚
3. å®éªŒè¯æ˜ï¼ŒPEGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†äº‹å®å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æ— ç›‘ç£ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°äº†å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†ä»ç„¶å®¹æ˜“å‡ºç°ä¸ä¸€è‡´æ€§å’Œå¹»è§‰ç°è±¡ã€‚æœ¬æ–‡æå‡ºäº†åŒè¡Œå¼•å¯¼æ¸¸æˆï¼ˆPEGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— è®­ç»ƒã€åŸºäºåšå¼ˆè®ºçš„æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå™¨å’Œå¤šä¸ªæ¥è‡ªä¸åŒåŸºç¡€æ¨¡å‹çš„é‰´åˆ«å™¨çš„åŒè¡Œå¼•å¯¼æœºåˆ¶æ¥å¯¹é½LLMsã€‚é‰´åˆ«å™¨åœ¨åŒè¡Œè¯„ä¼°ç¯å¢ƒä¸­äº’åŠ¨ï¼Œæ•ˆç”¨é€šè¿‡åŸºäºè¡Œåˆ—å¼çš„äº’ä¿¡æ¯è¯„åˆ†è®¡ç®—ï¼Œè¯æ˜èƒ½å¤Ÿæ¿€åŠ±çœŸå®æŠ¥å‘Šè€Œæ— éœ€çœŸå®æ ‡ç­¾ã€‚æˆ‘ä»¬å»ºç«‹äº†ç†è®ºä¿è¯ï¼Œè¡¨æ˜æ¯ä¸ªä»£ç†é€šè¿‡åœ¨çº¿å­¦ä¹ å®ç°äºšçº¿æ€§é—æ†¾ï¼Œå…¶ç´¯ç§¯è¡¨ç°æ¥è¿‘æœ€ä½³å›ºå®šçœŸå®ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€åè¿­ä»£æ”¶æ•›åˆ°çœŸå®çš„çº³ä»€å‡è¡¡ï¼Œç¡®ä¿ä»£ç†çš„å®é™…ç­–ç•¥éšæ—¶é—´æ”¶æ•›åˆ°ç¨³å®šå’ŒçœŸå®çš„è¡Œä¸ºã€‚å¤šé¡¹åŸºå‡†çš„å®è¯è¯„ä¼°æ˜¾ç¤ºå‡ºäº‹å®å‡†ç¡®æ€§çš„æ˜¾è‘—æå‡ã€‚è¿™äº›ç»“æœä½¿PEGæˆä¸ºä¸€ç§åœ¨æ²¡æœ‰ç›‘ç£æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹å¼•å¯¼LLMsçœŸå®è¡Œä¸ºçš„å®ç”¨æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ—¶çš„çœŸå®æŠ¥å‘Šé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºçœŸå®æ ‡ç­¾ï¼Œé™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥åŒè¡Œå¼•å¯¼æ¸¸æˆï¼ˆPEGï¼‰ï¼Œåˆ©ç”¨åšå¼ˆè®ºä¸­çš„åŒè¡Œè¯„ä¼°æœºåˆ¶ï¼Œæ¿€åŠ±æ¨¡å‹ç”ŸæˆçœŸå®çš„è¾“å‡ºï¼Œè€Œæ— éœ€ä¾èµ–çœŸå®æ ‡ç­¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPEGæ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç”Ÿæˆå™¨å’Œå¤šä¸ªæ¥è‡ªä¸åŒåŸºç¡€æ¨¡å‹çš„é‰´åˆ«å™¨ï¼Œé‰´åˆ«å™¨åœ¨åŒè¡Œè¯„ä¼°ç¯å¢ƒä¸­äº’åŠ¨ï¼Œæ•ˆç”¨é€šè¿‡äº’ä¿¡æ¯è¯„åˆ†è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ä¸ä¾èµ–çœŸå®æ ‡ç­¾çš„æ¿€åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨åšå¼ˆè®ºçš„åŸç†ç¡®ä¿æ¨¡å‹çš„çœŸå®æŠ¥å‘Šã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºè¡Œåˆ—å¼çš„äº’ä¿¡æ¯è¯„åˆ†ä½œä¸ºæ•ˆç”¨è®¡ç®—æ–¹å¼ï¼Œç¡®ä¿æ¯ä¸ªä»£ç†é€šè¿‡åœ¨çº¿å­¦ä¹ å®ç°äºšçº¿æ€§é—æ†¾ï¼Œå¹¶æœ€ç»ˆæ”¶æ•›åˆ°çœŸå®çš„çº³ä»€å‡è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPEGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†äº‹å®å‡†ç¡®æ€§ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜è¯­è¨€æ¨¡å‹çš„çœŸå®æŠ¥å‘Šèƒ½åŠ›ï¼Œèƒ½å¤Ÿå¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨ç”Ÿæˆå†…å®¹çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated strong generative capabilities but remain prone to inconsistencies and hallucinations. We introduce Peer Elicitation Games (PEG), a training-free, game-theoretic framework for aligning LLMs through a peer elicitation mechanism involving a generator and multiple discriminators instantiated from distinct base models. Discriminators interact in a peer evaluation setting, where utilities are computed using a determinant-based mutual information score that provably incentivizes truthful reporting without requiring ground-truth labels. We establish theoretical guarantees showing that each agent, via online learning, achieves sublinear regret in the sense their cumulative performance approaches that of the best fixed truthful strategy in hindsight. Moreover, we prove last-iterate convergence to a truthful Nash equilibrium, ensuring that the actual policies used by agents converge to stable and truthful behavior over time. Empirical evaluations across multiple benchmarks demonstrate significant improvements in factual accuracy. These results position PEG as a practical approach for eliciting truthful behavior from LLMs without supervision or fine-tuning.

