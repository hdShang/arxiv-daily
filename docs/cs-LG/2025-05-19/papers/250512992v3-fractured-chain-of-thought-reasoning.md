---
layout: default
title: Fractured Chain-of-Thought Reasoning
---

# Fractured Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12992" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12992v3</a>
  <a href="https://arxiv.org/pdf/2505.12992.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12992v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12992v3', 'Fractured Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-06-18)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/BaohaoLiao/frac-cot)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFractured Samplingä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†æ•ˆç‡` `å¤§è¯­è¨€æ¨¡å‹` `Chain-of-Thought` `Fractured Sampling` `è‡ªç„¶è¯­è¨€å¤„ç†` `å®æ—¶åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Chain-of-Thoughtæ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¶ˆè€—å¤§é‡tokenï¼Œé™åˆ¶äº†å…¶åœ¨å»¶è¿Ÿæ•æ„Ÿåœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºFractured Samplingï¼Œé€šè¿‡æˆªæ–­æ¨ç†è¿‡ç¨‹å¹¶åœ¨å¤šä¸ªç»´åº¦ä¸Šè¿›è¡Œæ’å€¼ï¼Œå‡å°‘tokenæ¶ˆè€—åŒæ—¶ä¿æŒæ¨ç†å‡†ç¡®æ€§ã€‚
3. åœ¨äº”ä¸ªä¸åŒçš„æ¨ç†åŸºå‡†ä¸Šï¼ŒFractured Samplingå±•ç¤ºäº†ä¼˜è¶Šçš„å‡†ç¡®æ€§ä¸æˆæœ¬å¹³è¡¡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†æ—¶çš„æ‰©å±•æŠ€æœ¯æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰çš„Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºæ–¹æ³•åœ¨å»¶è¿Ÿæ•æ„Ÿçš„åœºæ™¯ä¸­å­˜åœ¨é«˜ä»£ä»·é—®é¢˜ã€‚æœ¬æ–‡é¦–å…ˆå±•ç¤ºäº†æˆªæ–­CoTçš„æœ‰æ•ˆæ€§ï¼Œéšåæå‡ºFractured Samplingç­–ç•¥ï¼Œé€šè¿‡åœ¨æ¨ç†è½¨è¿¹æ•°é‡ã€æ¯æ¡è½¨è¿¹çš„æœ€ç»ˆè§£æ•°é‡å’Œæ¨ç†æ·±åº¦ç­‰ç»´åº¦ä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ä¸æˆæœ¬çš„å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒFractured Samplingåœ¨å¤šé¡¹æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæä¾›äº†æ›´é«˜æ•ˆçš„LLMæ¨ç†æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰Chain-of-Thoughtæ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­é«˜tokenæ¶ˆè€—çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºFractured Samplingç­–ç•¥ï¼Œé€šè¿‡æˆªæ–­æ¨ç†è¿‡ç¨‹ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œä»è€Œå‡å°‘tokenä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒæ¨ç†çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFractured Samplingåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡ä¸‰ä¸ªç»´åº¦è¿›è¡Œæ’å€¼ï¼šæ¨ç†è½¨è¿¹æ•°é‡ã€æ¯æ¡è½¨è¿¹çš„æœ€ç»ˆè§£æ•°é‡å’Œæ¨ç†æ·±åº¦ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ç”Ÿæˆå¤šä¸ªæ¨ç†è½¨è¿¹ï¼Œé€‰æ‹©æœ€ç»ˆè§£å¹¶è¿›è¡Œæˆªæ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šFractured Samplingçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»è°ƒæ•´å„ä¸ªç»´åº¦çš„ç­–ç•¥ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ï¼Œä¸ä¼ ç»Ÿçš„å®Œæ•´CoTæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†tokenæ¶ˆè€—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Fractured Samplingä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ¨ç†è½¨è¿¹çš„æ•°é‡å’Œæˆªæ–­æ·±åº¦ï¼Œè¿™äº›å‚æ•°çš„é€‰æ‹©ç›´æ¥å½±å“æœ€ç»ˆçš„æ¨ç†æ•ˆæœå’Œtokençš„ä½¿ç”¨æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFractured Samplingåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„Chain-of-Thoughtæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ç›¸åŒtokené¢„ç®—ä¸‹ï¼ŒPass@kæŒ‡æ ‡æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†æ›´é«˜çš„å‡†ç¡®æ€§ä¸æˆæœ¬æ•ˆç›Šæ¯”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡æ¨ç†æ•ˆç‡ï¼ŒFractured Samplingèƒ½å¤Ÿä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®æ—¶åº”ç”¨ä¸­æ›´å…·ç«äº‰åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning. Code is available at https://github.com/BaohaoLiao/frac-cot.

