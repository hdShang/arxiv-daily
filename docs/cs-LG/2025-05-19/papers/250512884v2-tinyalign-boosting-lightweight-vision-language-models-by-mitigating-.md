---
layout: default
title: "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks"
---

# TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12884" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12884v2</a>
  <a href="https://arxiv.org/pdf/2505.12884.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12884v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12884v2', 'TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanze Hu, Zhaoxin Fan, Xinyu Wang, Gen Li, Ye Qiu, Zhichao Yang, Wenjun Wu, Kejian Wu, Yifan Sun, Xiaotie Deng, Jin Dong

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-06-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTinyAlignä»¥è§£å†³è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹å¯¹é½ç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è½»é‡çº§æ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¯¹é½ç“¶é¢ˆ` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¤šæ¨¡æ€è¾“å…¥` `æœ‰æ•ˆäº’ä¿¡æ¯` `æ•°æ®æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•ä¾èµ–äºè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹é½è´¨é‡å—é™ã€‚
2. TinyAligné€šè¿‡ä»è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºå¤šæ¨¡æ€è¾“å…¥çš„å¯¹é½æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTinyAlignæ˜¾è‘—é™ä½è®­ç»ƒæŸå¤±ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼Œå¹¶åœ¨æ•°æ®ä½¿ç”¨ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨èµ„æºå—é™çš„åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å¯¹é½æ–¹æ³•é€šå¸¸åœ¨è®­ç»ƒå°å‹è¿æ¥æ¨¡å—æ—¶å†»ç»“è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ï¼Œè¿™ç§ç­–ç•¥ä¾èµ–äºè¯­è¨€æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›ï¼Œå¯èƒ½å¯¹è½»é‡çº§æ¨¡å‹çš„è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡é€šè¿‡äº’ä¿¡æ¯çš„è§†è§’æ¢è®¨äº†è¿™ä¸€å¯¹é½ç“¶é¢ˆï¼Œè¡¨æ˜è¯­è¨€æ¨¡å‹çš„å—é™èƒ½åŠ›é™åˆ¶äº†å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æœ‰æ•ˆäº’ä¿¡æ¯ï¼ˆEMIï¼‰ï¼Œä»è€Œå½±å“å¯¹é½è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TinyAlignï¼Œä¸€ä¸ªå—æ£€ç´¢å¢å¼ºç”Ÿæˆå¯å‘çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡ä»è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡æ¥ä¸°å¯Œå¤šæ¨¡æ€è¾“å…¥ï¼Œå¢å¼ºå…¶å¯¹é½æ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒTinyAlignæ˜¾è‘—é™ä½è®­ç»ƒæŸå¤±ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼Œå¹¶æå‡ä»»åŠ¡æ€§èƒ½ï¼Œä¸”ä»…éœ€40%çš„å¾®è°ƒæ•°æ®ä¾¿å¯è¾¾åˆ°åŸºçº¿æ°´å¹³çš„è¡¨ç°ï¼Œå±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¯¹é½è¿‡ç¨‹ä¸­é¢ä¸´çš„ç“¶é¢ˆï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹é½è´¨é‡ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTinyAlignçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡æ¥å¢å¼ºå¤šæ¨¡æ€è¾“å…¥ï¼Œä»è€Œæé«˜å¯¹é½è´¨é‡ï¼Œå…‹æœè¯­è¨€æ¨¡å‹èƒ½åŠ›çš„é™åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTinyAlignæ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªè®°å¿†åº“ï¼Œç”¨äºå­˜å‚¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥åŠä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œè´Ÿè´£ä»è®°å¿†åº“ä¸­æå–ç›¸å…³ä¿¡æ¯ä»¥ä¸°å¯Œè¾“å…¥ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆé€šè¿‡è§†è§‰å’Œè¯­è¨€æ¨¡å‹æå–ç‰¹å¾ï¼Œå†ç»“åˆæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡è¿›è¡Œå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTinyAlignçš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ€æƒ³ï¼Œé€šè¿‡å¤–éƒ¨è®°å¿†åº“æ¥æå‡å¤šæ¨¡æ€è¾“å…¥çš„æœ‰æ•ˆäº’ä¿¡æ¯ï¼Œæ˜¾è‘—æ”¹å–„äº†å¯¹é½æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒTinyAligné‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯¹é½è´¨é‡ï¼Œå¹¶é€šè¿‡è°ƒèŠ‚è®°å¿†åº“çš„å¤§å°å’Œæ£€ç´¢ç­–ç•¥æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTinyAlignåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾è‘—é™ä½äº†æŸå¤±ï¼Œæ”¶æ•›é€Ÿåº¦åŠ å¿«ï¼Œå¹¶åœ¨ä»»åŠ¡æ€§èƒ½ä¸Šè¶…è¿‡äº†åŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨ä»…ä½¿ç”¨40%çš„å¾®è°ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»èƒ½è¾¾åˆ°åŸºçº¿æ°´å¹³çš„è¡¨ç°ï¼Œå±•ç°å‡ºæé«˜çš„æ•°æ®æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TinyAlignçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºèµ„æºå—é™çš„åœºæ™¯ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„æ•°æ®åˆ©ç”¨ç‡å’Œå¯¹é½èƒ½åŠ›å°†æ¨åŠ¨è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Lightweight Vision-Language Models (VLMs) are indispensable for resource-constrained applications. The prevailing approach to aligning vision and language models involves freezing both the vision encoder and the language model while training small connector modules. However, this strategy heavily depends on the intrinsic capabilities of the language model, which can be suboptimal for lightweight models with limited representational capacity. In this work, we investigate this alignment bottleneck through the lens of mutual information, demonstrating that the constrained capacity of the language model inherently limits the Effective Mutual Information (EMI) between multimodal inputs and outputs, thereby compromising alignment quality. To address this challenge, we propose TinyAlign, a novel framework inspired by Retrieval-Augmented Generation, which strategically retrieves relevant context from a memory bank to enrich multimodal inputs and enhance their alignment. Extensive empirical evaluations reveal that TinyAlign significantly reduces training loss, accelerates convergence, and enhances task performance. Remarkably, it allows models to achieve baseline-level performance with only 40\% of the fine-tuning data, highlighting exceptional data efficiency. Our work thus offers a practical pathway for developing more capable lightweight VLMs while introducing a fresh theoretical lens to better understand and address alignment bottlenecks in constrained multimodal systems.

