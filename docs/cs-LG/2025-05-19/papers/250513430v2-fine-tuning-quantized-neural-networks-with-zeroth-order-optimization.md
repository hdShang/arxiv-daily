---
layout: default
title: Fine-tuning Quantized Neural Networks with Zeroth-order Optimization
---

# Fine-tuning Quantized Neural Networks with Zeroth-order Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13430" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.13430v2</a>
  <a href="https://arxiv.org/pdf/2505.13430.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13430v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13430v2', 'Fine-tuning Quantized Neural Networks with Zeroth-order Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Sifeng Shang, Jiayi Zhou, Chenyu Lin, Minxian Li, Kaiyang Zhou

**ÂàÜÁ±ª**: cs.LG, cs.CL, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-19 (Êõ¥Êñ∞: 2025-09-01)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÈáèÂåñÁ•ûÁªèÁΩëÁªúÁöÑÈõ∂Èò∂‰ºòÂåñÊñπÊ≥ï‰ª•Ëß£ÂÜ≥ÂÜÖÂ≠òÁì∂È¢àÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈáèÂåñÁ•ûÁªèÁΩëÁªú` `Èõ∂Èò∂‰ºòÂåñ` `ÂÜÖÂ≠òÈ´òÊïàËÆ≠ÁªÉ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `GPU‰ºòÂåñ` `Ê®°ÂûãÂæÆË∞É` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂Èù¢‰∏¥GPUÂÜÖÂ≠òÁì∂È¢àÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÈÄÇÂ∫îÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈáèÂåñÈõ∂Èò∂‰ºòÂåñÔºàQZOÔºâÊñπÊ≥ïÔºåÈÄöËøáÊâ∞Âä®ÈáèÂåñÂ∞∫Â∫¶Êù•Ëøë‰ººÊ¢ØÂ∫¶ÔºåÊ∂àÈô§Ê¢ØÂ∫¶Âíå‰ºòÂåñÂô®Áä∂ÊÄÅÁöÑÂÜÖÂ≠òÈúÄÊ±Ç„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQZOÂú®4‰ΩçLLM‰∏≠Â∞ÜÂÜÖÂ≠òÊàêÊú¨Èôç‰ΩéË∂ÖËøá18ÂÄçÔºåËÉΩÂ§üÂú®Âçï‰∏™24GB GPU‰∏äÊàêÂäüÂæÆË∞ÉÂ§ßÂûãÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËßÑÊ®°ÁöÑÊåáÊï∞Â¢ûÈïøÔºåGPUÂÜÖÂ≠òÊàê‰∏∫ÈÄÇÂ∫î‰∏ãÊ∏∏‰ªªÂä°ÁöÑÁì∂È¢à„ÄÇÊú¨ÊñáÊó®Âú®ÈÄöËøáÂú®Áªü‰∏ÄÊ°ÜÊû∂ÂÜÖÊúÄÂ∞èÂåñÊ®°ÂûãÊùÉÈáç„ÄÅÊ¢ØÂ∫¶Âíå‰ºòÂåñÂô®Áä∂ÊÄÅÁöÑÂÜÖÂ≠ò‰ΩøÁî®ÔºåÊé®Âä®ÂÜÖÂ≠òÈ´òÊïàËÆ≠ÁªÉÁöÑÊûÅÈôê„ÄÇÊàë‰ª¨ÊèêÂá∫Èõ∂Èò∂‰ºòÂåñÊñπÊ≥ïÔºåÈÄöËøáÂú®ÂâçÂêë‰º†Êí≠‰∏≠Êâ∞Âä®ÊùÉÈáçÊù•Ëøë‰ººÊ¢ØÂ∫¶Ôºå‰ªéËÄåÊ∂àÈô§Ê¢ØÂ∫¶Âíå‰ºòÂåñÂô®Áä∂ÊÄÅ„ÄÇ‰∏∫ÂáèÂ∞ëÊùÉÈáçÁöÑÂÜÖÂ≠ò‰ΩøÁî®ÔºåÊàë‰ª¨ÈááÁî®Ê®°ÂûãÈáèÂåñÊäÄÊúØ„ÄÇÁÑ∂ËÄåÔºåÁõ¥Êé•Â∞ÜÈõ∂Èò∂‰ºòÂåñÂ∫îÁî®‰∫éÈáèÂåñÊùÉÈáçÊòØ‰∏çÂèØË°åÁöÑÔºåÂõ†Ê≠§Êàë‰ª¨ÊèêÂá∫‰∫ÜÈáèÂåñÈõ∂Èò∂‰ºòÂåñÔºàQZOÔºâÔºåËØ•ÊñπÊ≥ïÈÄöËøáÊâ∞Âä®ËøûÁª≠ÈáèÂåñÂ∞∫Â∫¶Êù•‰º∞ËÆ°Ê¢ØÂ∫¶ÔºåÂπ∂‰ΩøÁî®ÊñπÂêëÂØºÊï∞Ë£ÅÂâ™ÊñπÊ≥ïÊù•Á®≥ÂÆöËÆ≠ÁªÉ„ÄÇ‰∏é16‰ΩçÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÁõ∏ÊØîÔºåQZOÂú®4‰ΩçLLM‰∏≠ÂèØÂ∞ÜÊÄªÂÜÖÂ≠òÊàêÊú¨Èôç‰ΩéË∂ÖËøá18ÂÄçÔºåÂπ∂ËÉΩÂ§üÂú®Âçï‰∏™24GB GPU‰∏äÂæÆË∞ÉLlama-2-13B„ÄÇ‰ª£Á†ÅÂ∞ÜÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠GPUÂÜÖÂ≠ò‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÊùÉÈáç„ÄÅÊ¢ØÂ∫¶Âíå‰ºòÂåñÂô®Áä∂ÊÄÅÁöÑÂÜÖÂ≠ò‰ΩøÁî®‰∏äÂ≠òÂú®ÊòæËëóÁì∂È¢à„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÈáèÂåñÈõ∂Èò∂‰ºòÂåñÔºàQZOÔºâÊñπÊ≥ïÔºåÈÄöËøáÊâ∞Âä®ÈáèÂåñÂ∞∫Â∫¶Êù•Ëøë‰ººÊ¢ØÂ∫¶ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÊñπÊ≥ï‰∏≠ÂØπÊ¢ØÂ∫¶Âíå‰ºòÂåñÂô®Áä∂ÊÄÅÁöÑÂÜÖÂ≠òÈúÄÊ±ÇÔºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑÂÜÖÂ≠ò‰ΩøÁî®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÊùÉÈáçÈáèÂåñ„ÄÅÈõ∂Èò∂‰ºòÂåñÂíåÊñπÂêëÂØºÊï∞Ë£ÅÂâ™‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÂØπÊ®°ÂûãÊùÉÈáçËøõË°åÈáèÂåñÔºåÁÑ∂ÂêéÂú®ÂâçÂêë‰º†Êí≠‰∏≠Êâ∞Âä®ÊùÉÈáç‰ª•‰º∞ËÆ°Ê¢ØÂ∫¶ÔºåÊúÄÂêéÈÄöËøáË£ÅÂâ™ÊñπÊ≥ïÁ®≥ÂÆöËÆ≠ÁªÉËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöQZOÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÈÄöËøáÊâ∞Âä®ÈáèÂåñÂ∞∫Â∫¶Êù•ÂÆûÁé∞Ê¢ØÂ∫¶‰º∞ËÆ°ÔºåËøô‰∏ÄÊñπÊ≥ï‰∏éÁé∞ÊúâÁöÑÂü∫‰∫éÊ†áÈáèÂíå‰ª£Á†ÅÊú¨ÁöÑÂêéËÆ≠ÁªÉÈáèÂåñÊñπÊ≥ïÊòØÊ≠£‰∫§ÁöÑÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊÄùË∑ØÊù•Ëß£ÂÜ≥ÈáèÂåñÊ®°ÂûãÁöÑËÆ≠ÁªÉÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®QZO‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊâ∞Âä®Á≠ñÁï•ÂíåÊñπÂêëÂØºÊï∞Ë£ÅÂâ™ÊäÄÊúØÔºå‰ª•Á°Æ‰øùÂú®ÈáèÂåñÊùÉÈáçÁöÑÊÉÖÂÜµ‰∏ã‰ªçËÉΩÊúâÊïà‰º∞ËÆ°Ê¢ØÂ∫¶Âπ∂Á®≥ÂÆöËÆ≠ÁªÉËøáÁ®ã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰∏é16‰ΩçÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÁõ∏ÊØîÔºåQZOÂú®4‰ΩçLLM‰∏≠Â∞ÜÂÜÖÂ≠òÊàêÊú¨Èôç‰ΩéË∂ÖËøá18ÂÄçÔºåËÉΩÂ§üÂú®Âçï‰∏™24GB GPU‰∏äÊàêÂäüÂæÆË∞ÉLlama-2-13BÔºåÂ±ïÁé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅËÆ°ÁÆóÊú∫ËßÜËßâÁ≠âÈúÄË¶ÅÂ§ßËßÑÊ®°Ê®°ÂûãÁöÑ‰ªªÂä°„ÄÇÈÄöËøáÈôç‰ΩéÂÜÖÂ≠òÈúÄÊ±ÇÔºåQZOÊñπÊ≥ï‰ΩøÂæóÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÂ§ßÂûãÊ®°ÂûãÊàê‰∏∫ÂèØËÉΩÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a simple yet effective approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in 16 bits, QZO can reduce the total memory cost by more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B within a single 24GB GPU. Code will be released publicly.

