---
layout: default
title: Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression
---

# Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13563" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13563v3</a>
  <a href="https://arxiv.org/pdf/2505.13563.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13563v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13563v3', 'Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaohui Wang, Peng Ye, Chenyu Huang, Shenghe Zheng, Bo Zhang, Lei Bai, Wanli Ouyang, Tao Chen

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-10-13)

**å¤‡æ³¨**: Accepted at NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xiaohuiwang000/UltraDelta)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUltraDeltaä»¥è§£å†³æ•°æ®ä¾èµ–çš„è¶…é«˜æ•ˆå¢é‡å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¢é‡å‹ç¼©` `æ¨¡å‹å‹ç¼©` `æ•°æ®æ— å…³` `è¶…é«˜å‹ç¼©` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºè§†è§‰` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¢é‡å‹ç¼©æ–¹æ³•åœ¨é«˜å‹ç¼©ç‡ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´éš¾ä»¥å–å¾—å¹³è¡¡ï¼Œå¹¶ä¸”é€šå¸¸ä¾èµ–äºè®­ç»ƒæ•°æ®ã€‚
2. UltraDeltaé€šè¿‡æ•°æ®æ— å…³çš„æ–¹å¼å®ç°è¶…é«˜å‹ç¼©ï¼Œé‡‡ç”¨åŸºäºæ–¹å·®çš„ç¨€ç–åˆ†é…ã€åˆ†å¸ƒæ„ŸçŸ¥å‹ç¼©å’Œè¿¹èŒƒæ•°å¼•å¯¼é‡æ ‡å®šç­‰æŠ€æœ¯ã€‚
3. åœ¨å¤šä¸ªæ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒUltraDeltaåœ¨å‹ç¼©ç‡é«˜è¾¾224å€çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨è¯­è¨€å’Œè§†è§‰æ¨¡å‹ä¸Šè¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¾®è°ƒé¢„è®­ç»ƒèŒƒå¼çš„å…´èµ·ï¼Œå­˜å‚¨å¤šä¸ªå¾®è°ƒæ¨¡å‹å¸¦æ¥äº†æ˜¾è‘—çš„å­˜å‚¨å¼€é”€ã€‚å¢é‡å‹ç¼©é€šè¿‡ä»…å­˜å‚¨é¢„è®­ç»ƒæ¨¡å‹å’Œé«˜åº¦å‹ç¼©çš„å¢é‡æƒé‡æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é«˜å‹ç¼©ç‡å’Œæ€§èƒ½ä¹‹é—´éš¾ä»¥å¹³è¡¡ï¼Œå¹¶ä¸”é€šå¸¸ä¾èµ–äºæ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†UltraDeltaï¼Œè¿™æ˜¯é¦–ä¸ªå®ç°è¶…é«˜å‹ç¼©å’Œå¼ºæ€§èƒ½çš„æ•°æ®æ— å…³å¢é‡å‹ç¼©ç®¡é“ã€‚UltraDeltaé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶è®¾è®¡æ¥æœ€å°åŒ–å†—ä½™ã€æœ€å¤§åŒ–ä¿¡æ¯å¹¶ç¨³å®šæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒUltraDeltaåœ¨å¤šç§æ¨¡å‹ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨è¶…é«˜å‹ç¼©æƒ…å†µä¸‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¢é‡å‹ç¼©æ–¹æ³•åœ¨é«˜å‹ç¼©ç‡ä¸æ€§èƒ½ä¹‹é—´çš„çŸ›ç›¾ï¼Œå°¤å…¶æ˜¯å…¶å¯¹æ•°æ®çš„ä¾èµ–æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å®ç°é«˜å‹ç¼©ç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUltraDeltaçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ•°æ®æ— å…³çš„æ–¹å¼å®ç°è¶…é«˜å‹ç¼©ï¼Œè®¾è®¡äº†ä¸‰å¤§å…³é”®ç»„ä»¶ï¼Œåˆ†åˆ«é’ˆå¯¹ä¸åŒå±‚æ¬¡çš„ä¿¡æ¯å†—ä½™è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæå‡å‹ç¼©æ•ˆæœå’Œæ¨¡å‹ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUltraDeltaçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŸºäºæ–¹å·®çš„æ··åˆç¨€ç–åˆ†é…ï¼Œ2) åˆ†å¸ƒæ„ŸçŸ¥å‹ç¼©ï¼Œ3) è¿¹èŒƒæ•°å¼•å¯¼é‡æ ‡å®šã€‚æ¯ä¸ªæ¨¡å—é’ˆå¯¹ä¸åŒçš„å‹ç¼©éœ€æ±‚è¿›è¡Œä¼˜åŒ–ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„å‹ç¼©ç®¡é“ã€‚

**å…³é”®åˆ›æ–°**ï¼šUltraDeltaçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ•°æ®æ— å…³æ€§å’Œé’ˆå¯¹æ€§è®¾è®¡ï¼Œå°¤å…¶æ˜¯åŸºäºæ–¹å·®çš„ç¨€ç–åˆ†é…å’Œè¿¹èŒƒæ•°å¼•å¯¼é‡æ ‡å®šï¼Œè¿™äº›è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨é«˜å‹ç¼©ç‡ä¸‹ä»èƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒUltraDeltaé‡‡ç”¨äº†åŸºäºæ–¹å·®çš„ç¨€ç–åˆ†é…ç­–ç•¥ï¼Œç¡®ä¿é«˜æ–¹å·®å±‚ä¿ç•™æ›´å¤šä¿¡æ¯ï¼›åˆ†å¸ƒæ„ŸçŸ¥å‹ç¼©é€šè¿‡å‡åŒ€é‡åŒ–å’Œåˆ†ç»„ä¿®å‰ªæ¥ä¼˜åŒ–å‚æ•°åˆ†å¸ƒï¼›è¿¹èŒƒæ•°å¼•å¯¼é‡æ ‡å®šåˆ™é€šè¿‡å…¨å±€é‡æ ‡å®šå› å­æ¥æå‡æ¨¡å‹ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUltraDeltaåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LLaMA-2ï¼‰ä¸Šå®ç°äº†é«˜è¾¾50å€çš„å‹ç¼©ï¼Œåœ¨é€šç”¨NLPæ¨¡å‹ï¼ˆå¦‚RoBERTaå’ŒT5ï¼‰ä¸Šè¾¾åˆ°äº†224å€çš„å‹ç¼©ç‡ï¼Œè€Œåœ¨è§†è§‰æ¨¡å‹ï¼ˆå¦‚ViTï¼‰ä¸Šä¹Ÿå®ç°äº†132å€çš„å‹ç¼©ï¼Œå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨è¶…é«˜å‹ç¼©æƒ…å†µä¸‹è¡¨ç°çªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UltraDeltaçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å­˜å‚¨å’Œéƒ¨ç½²å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„åœºæ™¯ä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚å…¶é«˜æ•ˆçš„å¢é‡å‹ç¼©æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½å­˜å‚¨æˆæœ¬ï¼Œæå‡æ¨¡å‹çš„å¯ç”¨æ€§å’Œçµæ´»æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rise of the fine-tuned-pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead. Delta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). However, existing methods fail to maintain both high compression and performance, and often rely on data. To address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. UltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components: (1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information. (2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution. (3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression. Extensive experiments across (a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 50x compression, (b) general NLP models (RoBERTa-base, T5-base) with up to 224x compression, (c) vision models (ViT-B/32, ViT-L/14) with up to 132x compression, and (d) multi-modal models (BEiT-3) with 18x compression, demonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression. Code is available at https://github.com/xiaohuiwang000/UltraDelta.

