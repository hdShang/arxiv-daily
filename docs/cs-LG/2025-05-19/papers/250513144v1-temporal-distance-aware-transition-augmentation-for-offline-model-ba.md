---
layout: default
title: Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning
---

# Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13144" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13144v1</a>
  <a href="https://arxiv.org/pdf/2505.13144.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13144v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13144v1', 'Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dongsu Lee, Minhae Kwon

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**å¤‡æ³¨**: 2025 ICML

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTempDATAä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ ` `æ—¶é—´è·ç¦»æ„ŸçŸ¥` `å¢å¼ºè½¬ç§»` `ç¨€ç–å¥–åŠ±` `é•¿æ—¶é—´è·¨åº¦ä»»åŠ¡` `åŠ¨æ€æ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿MBRLæ–¹æ³•åœ¨å¤„ç†ç¨€ç–å¥–åŠ±å’Œé•¿æ—¶é—´è·¨åº¦ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„TempDATAæ¡†æ¶é€šè¿‡åœ¨æ—¶é—´ç»“æ„çš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå¢å¼ºè½¬ç§»ï¼Œæ¥æœ‰æ•ˆå»ºæ¨¡é•¿æ—¶é—´è·¨åº¦è¡Œä¸ºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTempDATAåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ç¦»çº¿MBRLæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯ä»å›ºå®šæ•°æ®é›†ä¸­æå–é«˜æ€§èƒ½ç­–ç•¥ï¼Œæœ€å°åŒ–ç”±äºåˆ†å¸ƒå¤–æ ·æœ¬å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚ç¦»çº¿æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰é€šè¿‡åˆ©ç”¨å­¦ä¹ çš„åŠ¨æ€æ¨¡å‹åˆæˆçš„å¢å¼ºçŠ¶æ€-åŠ¨ä½œè½¬ç§»æ¥æ”¹å–„åˆ†å¸ƒå¤–é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¦»çº¿MBRLæ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±å’Œé•¿æ—¶é—´è·¨åº¦ä»»åŠ¡ä¸­å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„MBRLæ¡†æ¶ï¼Œç§°ä¸ºæ—¶é—´è·ç¦»æ„ŸçŸ¥è½¬ç§»å¢å¼ºï¼ˆTempDATAï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨æ—¶é—´ç»“æ„çš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå¢å¼ºè½¬ç§»ï¼Œè€Œéåœ¨åŸå§‹çŠ¶æ€ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempDATAåœ¨D4RLçš„AntMazeã€FrankaKitchenã€CALVINå’ŒåŸºäºåƒç´ çš„FrankaKitchenä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„ç¦»çº¿MBRLæ–¹æ³•ï¼Œå¹¶ä¸åŸºäºæ‰©æ•£çš„è½¨è¿¹å¢å¼ºå’Œç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç”±äºç¨€ç–å¥–åŠ±å’Œé•¿æ—¶é—´è·¨åº¦ä»»åŠ¡å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰çš„ç¦»çº¿MBRLæ–¹æ³•åœ¨å¤„ç†è¿™äº›ä»»åŠ¡æ—¶å¸¸å¸¸æ•ˆæœä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å›ºå®šæ•°æ®é›†ä¸­çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTempDATAæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åœ¨æ—¶é—´ç»“æ„çš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå¢å¼ºè½¬ç§»ï¼Œæ¥æ•æ‰çŠ¶æ€ç©ºé—´ä¸­è½¨è¿¹å’Œè½¬ç§»å±‚é¢çš„æ—¶é—´è·ç¦»ï¼Œä»è€Œæ›´å¥½åœ°å»ºæ¨¡é•¿æ—¶é—´è·¨åº¦çš„è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTempDATAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ½œåœ¨ç©ºé—´å»ºæ¨¡å’Œå¢å¼ºè½¬ç§»ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å­¦ä¹ çš„åŠ¨æ€æ¨¡å‹å¯¹çŠ¶æ€-åŠ¨ä½œè½¬ç§»è¿›è¡Œå»ºæ¨¡ï¼Œç„¶ååœ¨æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå¢å¼ºè½¬ç§»ï¼Œæœ€åå°†è¿™äº›å¢å¼ºè½¬ç§»ç”¨äºç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šTempDATAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åœ¨æ—¶é—´ç»“æ„çš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆå¢å¼ºè½¬ç§»çš„èƒ½åŠ›ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰é•¿æ—¶é—´è·¨åº¦çš„è¡Œä¸ºç‰¹å¾ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•åœ¨åŸå§‹çŠ¶æ€ç©ºé—´ä¸­ç”Ÿæˆå¢å¼ºè½¬ç§»çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒTempDATAé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ½œåœ¨ç©ºé—´çš„å­¦ä¹ ï¼Œå¹¶ä½¿ç”¨äº†é€‚åº”æ€§çš„ç½‘ç»œç»“æ„æ¥å¤„ç†ä¸åŒä»»åŠ¡çš„å¤æ‚æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTempDATAåœ¨D4RLåŸºå‡†ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ç¦»çº¿MBRLæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨AntMazeã€FrankaKitchenå’ŒCALVINä»»åŠ¡ä¸­æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”åœ¨æŸäº›ä»»åŠ¡ä¸Šä¸åŸºäºæ‰©æ•£çš„è½¨è¿¹å¢å¼ºå’Œç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ç›¸å½“æˆ–æ›´ä¼˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæ¸¸æˆæ™ºèƒ½ç­‰éœ€è¦é«˜æ•ˆå†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æå‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ï¼ŒTempDATAèƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å®ç°æ›´ä¼˜çš„ç­–ç•¥å­¦ä¹ ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The goal of offline reinforcement learning (RL) is to extract a high-performance policy from the fixed datasets, minimizing performance degradation due to out-of-distribution (OOD) samples. Offline model-based RL (MBRL) is a promising approach that ameliorates OOD issues by enriching state-action transitions with augmentations synthesized via a learned dynamics model. Unfortunately, seminal offline MBRL methods often struggle in sparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL framework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA), that generates augmented transitions in a temporally structured latent space rather than in raw state space. To model long-horizon behavior, TempDATA learns a latent abstraction that captures a temporal distance from both trajectory and transition levels of state space. Our experiments confirm that TempDATA outperforms previous offline MBRL methods and achieves matching or surpassing the performance of diffusion-based trajectory augmentation and goal-conditioned RL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.

