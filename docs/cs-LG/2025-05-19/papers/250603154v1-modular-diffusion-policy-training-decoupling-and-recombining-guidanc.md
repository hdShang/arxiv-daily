---
layout: default
title: Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL
---

# Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03154" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03154v1</a>
  <a href="https://arxiv.org/pdf/2506.03154.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03154v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03154v1', 'Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaoyang Chen, Cody Fleming

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨¡å—åŒ–æ‰©æ•£ç­–ç•¥è®­ç»ƒä»¥ä¼˜åŒ–ç¦»çº¿å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ¨¡å—åŒ–è®­ç»ƒ` `æ‰©æ•£æ¨¡å‹` `å¼•å¯¼æ¨¡å—` `æ ·æœ¬æ•ˆç‡` `å¯è½¬ç§»æ€§` `ä»·å€¼ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ—©æœŸé˜¶æ®µä¾èµ–äºå¼•å¯¼æ¨¡å—å’Œæ‰©æ•£æ¨¡å‹çš„è”åˆè®­ç»ƒï¼Œå¯¼è‡´å¼•å¯¼ä¸å‡†ç¡®å’Œå­¦ä¹ ä¿¡å·å™ªå£°è¿‡å¤§ã€‚
2. è®ºæ–‡æå‡ºå°†å¼•å¯¼æ¨¡å—ä¸æ‰©æ•£æ¨¡å‹è§£è€¦çš„æ¨¡å—åŒ–è®­ç»ƒæ–¹æ³•ï¼Œé¦–å…ˆç‹¬ç«‹è®­ç»ƒå¼•å¯¼æ¨¡å—ä½œä¸ºä»·å€¼ä¼°è®¡å™¨ï¼Œç„¶åå¼•å¯¼æ‰©æ•£æ¨¡å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹è®­ç»ƒçš„å¼•å¯¼æ¨¡å‹æ˜¾è‘—é™ä½äº†æ ‡å‡†åŒ–å¾—åˆ†æ–¹å·®ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ¨¡å—åŒ–å’Œå¯è½¬ç§»æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ— åˆ†ç±»å™¨å¼•å¯¼åœ¨åŸºäºæ‰©æ•£çš„å¼ºåŒ–å­¦ä¹ ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå¼•å¯¼æ¨¡å—å’Œæ‰©æ•£æ¨¡å‹çš„è”åˆè®­ç»ƒï¼Œè¿™åœ¨æ—©æœŸé˜¶æ®µå¯èƒ½å¯¼è‡´ä¸ç†æƒ³çš„ç»“æœã€‚æœ¬æ–‡æå‡ºæ¨¡å—åŒ–è®­ç»ƒæ–¹æ³•ï¼Œå°†å¼•å¯¼æ¨¡å—ä¸æ‰©æ•£æ¨¡å‹è§£è€¦ï¼ŒåŸºäºä¸‰ä¸ªå…³é”®å‘ç°ï¼šå¼•å¯¼çš„å¿…è¦æ€§ã€å¼•å¯¼ä¼˜å…ˆçš„æ‰©æ•£è®­ç»ƒå’Œè·¨æ¨¡å—çš„å¯è½¬ç§»æ€§ã€‚é€šè¿‡ç‹¬ç«‹è®­ç»ƒçš„å¼•å¯¼æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†æ ‡å‡†åŒ–å¾—åˆ†æ–¹å·®ï¼Œå±•ç¤ºäº†æ¨¡å—åŒ–å’Œå¯é‡ç”¨æ€§çš„å¼ºå¤§æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨D4RLåŸºå‡†ä¸Šæä¾›äº†ç†è®ºä¾æ®å’Œå®è¯éªŒè¯ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒèŒƒå¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³äº†ç°æœ‰ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸­å¼•å¯¼æ¨¡å—ä¸æ‰©æ•£æ¨¡å‹è”åˆè®­ç»ƒçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸé˜¶æ®µå¼•å¯¼ä¸å‡†ç¡®å¯¼è‡´çš„å­¦ä¹ ä¿¡å·å™ªå£°é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼•å¯¼æ¨¡å—ä¸æ‰©æ•£æ¨¡å‹è§£è€¦ï¼Œé€šè¿‡ç‹¬ç«‹è®­ç»ƒå¼•å¯¼æ¨¡å—æ¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆç‹¬ç«‹è®­ç»ƒå¼•å¯¼æ¨¡å—ä½œä¸ºä»·å€¼ä¼°è®¡å™¨ï¼›ç„¶åå°†å…¶å†»ç»“å¹¶ç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ï¼›æœ€ååœ¨æ¨ç†é˜¶æ®µåº”ç”¨ç‹¬ç«‹è®­ç»ƒçš„å¼•å¯¼æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å¯¼æ¨¡å—çš„ç‹¬ç«‹è®­ç»ƒå’Œè·¨æ¨¡å—çš„å¯è½¬ç§»æ€§ï¼Œå…è®¸ä¸åŒç®—æ³•é—´çš„å¼•å¯¼æ¨¡å—é‡ç”¨ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¼•å¯¼æ¨¡å—çš„æŸå¤±å‡½æ•°è®¾ç½®ã€ç½‘ç»œç»“æ„çš„é€‰æ‹©ï¼Œä»¥åŠåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„å‚æ•°è°ƒæ•´ï¼Œä»¥ç¡®ä¿å¼•å¯¼æ¨¡å—çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ç‹¬ç«‹è®­ç»ƒçš„å¼•å¯¼æ¨¡å‹å¯ä»¥å°†æ ‡å‡†åŒ–å¾—åˆ†æ–¹å·®é™ä½86%ï¼Œå¹¶ä¸”ä¸åŒç®—æ³•é—´çš„å¼•å¯¼æ¨¡å—å¯ä»¥ç›´æ¥é‡ç”¨ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ¨¡å—åŒ–å’Œå¯è½¬ç§»æ€§ï¼Œæ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆæ™ºèƒ½ä½“ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œæ¨¡å—åŒ–çš„è®­ç»ƒç®¡é“å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤æ‚ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆï¼Œä¿ƒè¿›æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Classifier free guidance has shown strong potential in diffusion-based reinforcement learning. However, existing methods rely on joint training of the guidance module and the diffusion model, which can be suboptimal during the early stages when the guidance is inaccurate and provides noisy learning signals. In offline RL, guidance depends solely on offline data: observations, actions, and rewards, and is independent of the policy module's behavior, suggesting that joint training is not required. This paper proposes modular training methods that decouple the guidance module from the diffusion model, based on three key findings:
>   Guidance Necessity: We explore how the effectiveness of guidance varies with the training stage and algorithm choice, uncovering the roles of guidance and diffusion. A lack of good guidance in the early stage presents an opportunity for optimization.
>   Guidance-First Diffusion Training: We introduce a method where the guidance module is first trained independently as a value estimator, then frozen to guide the diffusion model using classifier-free reward guidance. This modularization reduces memory usage, improves computational efficiency, and enhances both sample efficiency and final performance.
>   Cross-Module Transferability: Applying two independently trained guidance models, one during training and the other during inference, can significantly reduce normalized score variance (e.g., reducing IQR by 86%). We show that guidance modules trained with one algorithm (e.g., IDQL) can be directly reused with another (e.g., DQL), with no additional training required, demonstrating baseline-level performance as well as strong modularity and transferability.
>   We provide theoretical justification and empirical validation on bullet D4RL benchmarks. Our findings suggest a new paradigm for offline RL: modular, reusable, and composable training pipelines.

