---
layout: default
title: RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs
---

# RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13697" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13697v3</a>
  <a href="https://arxiv.org/pdf/2505.13697.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13697v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13697v3', 'RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Soumya Rani Samineni, Durgesh Kalwar, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-11-10)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ†æå¼ºåŒ–å­¦ä¹ åè®­ç»ƒåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç»“æ„å‡è®¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç›‘ç£å­¦ä¹ ` `æ¨¡å‹å‡è®¾` `æ€§èƒ½è¯„ä¼°` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•åœ¨ç»“æ„å‡è®¾ä¸Šå­˜åœ¨ç®€åŒ–ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½çš„è´¨ç–‘ã€‚
2. è®ºæ–‡é€šè¿‡åˆ†æå¼ºåŒ–å­¦ä¹ åè®­ç»ƒçš„ç»“æ„å‡è®¾ï¼Œæå‡ºäº†å°†LLMè®­ç»ƒè§†ä¸ºç›‘ç£å­¦ä¹ çš„è§‚ç‚¹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¿­ä»£çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¡¨ç°å‡ºä¸GRPOè®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨DeepSeek R1å‘å¸ƒåã€‚æœ¬æ–‡æ‰¹åˆ¤æ€§åœ°åˆ†æäº†è¿™äº›æ–¹æ³•çš„æ¨¡å‹å‡è®¾ï¼ŒæŒ‡å‡ºå°†LLMè®­ç»ƒå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ‰€åšçš„ç®€åŒ–å‡è®¾å¯¼è‡´äº†ä¸€ä¸ªé€€åŒ–çš„MDPï¼Œå®é™…ä¸Šå¹¶ä¸éœ€è¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆ–GRPOçš„æ¡†æ¶ã€‚é€šè¿‡å¯¹GSM8Kå’ŒCountdownåŸºå‡†çš„å®éªŒï¼Œå‘ç°è¿­ä»£çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¸GRPOè®­ç»ƒç›¸å½“ï¼Œè¡¨æ˜ç°æœ‰çš„RLæ¡†æ¶å’Œè§£é‡Šå­˜åœ¨è´¨ç–‘çš„ç©ºé—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•åœ¨ç»“æ„å‡è®¾ä¸Šçš„ä¸è¶³ï¼ŒæŒ‡å‡ºè¿™äº›å‡è®¾å¯¼è‡´äº†æ¨¡å‹çš„é€€åŒ–ï¼Œå½±å“äº†å…¶æœ‰æ•ˆæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºå°†LLMè®­ç»ƒè§†ä¸ºç›‘ç£å­¦ä¹ ï¼Œè€Œéä¾èµ–äºå¤æ‚çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¼ºè°ƒäº†ç®€åŒ–å‡è®¾çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆåˆ†æäº†å°†LLMè®­ç»ƒå»ºæ¨¡ä¸ºMDPçš„å¸¸è§å‡è®¾ï¼Œç„¶åé€šè¿‡å®éªŒéªŒè¯äº†è¿­ä»£ç›‘ç£å¾®è°ƒçš„æœ‰æ•ˆæ€§ï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ­ç¤ºäº†ç°æœ‰RLæ¡†æ¶çš„ç®€åŒ–å‡è®¾ä½¿å¾—å…¶ä¸ç›‘ç£å­¦ä¹ ç­‰æ•ˆï¼Œè´¨ç–‘äº†RLåœ¨LLMè®­ç»ƒä¸­çš„å¿…è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†æ­£è´Ÿæ ·æœ¬çš„è¿­ä»£å¾®è°ƒç­–ç•¥ï¼Œä½¿ç”¨äº†GSM8Kå’ŒCountdownç­‰åŸºå‡†æ•°æ®é›†ï¼Œç¡®ä¿äº†æ¨¡å‹çš„å…¨é¢è¯„ä¼°ã€‚å®éªŒä¸­å¯¹å¥–åŠ±çš„åˆ†é…å’ŒçŠ¶æ€çš„å®šä¹‰è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è¿­ä»£ç›‘ç£å¾®è°ƒçš„æ¨¡å‹åœ¨GSM8Kå’ŒCountdownåŸºå‡†ä¸Šè¾¾åˆ°äº†ä¸GRPOè®­ç»ƒç›¸å½“çš„æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯è¡Œæ€§ï¼Œè¡¨æ˜åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ çš„å¤æ‚æ€§å¹¶éå¿…è¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥æé«˜å…¶æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ï¼Œè¿›è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of "RL generating longer thinking traces." While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.

