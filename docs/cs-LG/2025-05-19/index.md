---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.LG - 2025-05-19
---

# cs.LGï¼ˆ2025-05-19ï¼‰

ğŸ“Š å…± **19** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250512992v3-fractured-chain-of-thought-reasoning.html">Fractured Chain-of-Thought Reasoning</a></td>
  <td>æå‡ºFractured Samplingä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12992v3" data-paper-url="./papers/250512992v3-fractured-chain-of-thought-reasoning.html" onclick="toggleFavorite(this, '2505.12992v3', 'Fractured Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250513081v1-walking-the-tightrope-disentangling-beneficial-and-detrimental-drift.html">Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning</a></td>
  <td>æå‡ºåäº‹å®åå¥½ä¼˜åŒ–ä»¥è§£å†³éå¹³ç¨³ç¯å¢ƒä¸­çš„æœ‰å®³æ¦‚å¿µæ¼‚ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13081v1" data-paper-url="./papers/250513081v1-walking-the-tightrope-disentangling-beneficial-and-detrimental-drift.html" onclick="toggleFavorite(this, '2505.13081v1', 'Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250513308v2-seek-in-the-dark-reasoning-via-test-time-instance-level-policy-gradi.html">Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space</a></td>
  <td>æå‡ºLatentSeekä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13308v2" data-paper-url="./papers/250513308v2-seek-in-the-dark-reasoning-via-test-time-instance-level-policy-gradi.html" onclick="toggleFavorite(this, '2505.13308v2', 'Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250513430v2-fine-tuning-quantized-neural-networks-with-zeroth-order-optimization.html">Fine-tuning Quantized Neural Networks with Zeroth-order Optimization</a></td>
  <td>æå‡ºé‡åŒ–ç¥ç»ç½‘ç»œçš„é›¶é˜¶ä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³å†…å­˜ç“¶é¢ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13430v2" data-paper-url="./papers/250513430v2-fine-tuning-quantized-neural-networks-with-zeroth-order-optimization.html" onclick="toggleFavorite(this, '2505.13430v2', 'Fine-tuning Quantized Neural Networks with Zeroth-order Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250513563v3-breaking-the-compression-ceiling-data-free-pipeline-for-ultra-effici.html">Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression</a></td>
  <td>æå‡ºUltraDeltaä»¥è§£å†³æ•°æ®ä¾èµ–çš„è¶…é«˜æ•ˆå¢é‡å‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13563v3" data-paper-url="./papers/250513563v3-breaking-the-compression-ceiling-data-free-pipeline-for-ultra-effici.html" onclick="toggleFavorite(this, '2505.13563v3', 'Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250512884v2-tinyalign-boosting-lightweight-vision-language-models-by-mitigating-.html">TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks</a></td>
  <td>æå‡ºTinyAlignä»¥è§£å†³è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹å¯¹é½ç“¶é¢ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12884v2" data-paper-url="./papers/250512884v2-tinyalign-boosting-lightweight-vision-language-models-by-mitigating-.html" onclick="toggleFavorite(this, '2505.12884v2', 'TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250513775v3-beyond-semantics-the-unreasonable-effectiveness-of-reasonless-interm.html">Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens</a></td>
  <td>æå‡ºæ— è¯­ä¹‰ä¸­é—´æ ‡è®°ä»¥æŒ‘æˆ˜æ¨ç†æ¨¡å‹çš„ä¼ ç»Ÿç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13775v3" data-paper-url="./papers/250513775v3-beyond-semantics-the-unreasonable-effectiveness-of-reasonless-interm.html" onclick="toggleFavorite(this, '2505.13775v3', 'Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250513755v2-panda-a-pretrained-forecast-model-for-chaotic-dynamics.html">Panda: A pretrained forecast model for chaotic dynamics</a></td>
  <td>æå‡ºPandaæ¨¡å‹ä»¥è§£å†³æ··æ²ŒåŠ¨åŠ›å­¦é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13755v2" data-paper-url="./papers/250513755v2-panda-a-pretrained-forecast-model-for-chaotic-dynamics.html" onclick="toggleFavorite(this, '2505.13755v2', 'Panda: A pretrained forecast model for chaotic dynamics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250513636v2-incentivizing-truthful-language-models-via-peer-elicitation-games.html">Incentivizing Truthful Language Models via Peer Elicitation Games</a></td>
  <td>æå‡ºåŒè¡Œå¼•å¯¼æ¸¸æˆä»¥è§£å†³è¯­è¨€æ¨¡å‹çš„çœŸå®æŠ¥å‘Šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13636v2" data-paper-url="./papers/250513636v2-incentivizing-truthful-language-models-via-peer-elicitation-games.html" onclick="toggleFavorite(this, '2505.13636v2', 'Incentivizing Truthful Language Models via Peer Elicitation Games')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250513109v3-freekv-boosting-kv-cache-retrieval-for-efficient-llm-inference.html">FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</a></td>
  <td>æå‡ºFreeKVä»¥è§£å†³é•¿ä¸Šä¸‹æ–‡KVç¼“å­˜æ£€ç´¢æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13109v3" data-paper-url="./papers/250513109v3-freekv-boosting-kv-cache-retrieval-for-efficient-llm-inference.html" onclick="toggleFavorite(this, '2505.13109v3', 'FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250513709v2-policy-driven-world-model-adaptation-for-robust-offline-model-based-.html">Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning</a></td>
  <td>æå‡ºåŠ¨æ€é€‚åº”ä¸–ç•Œæ¨¡å‹ä»¥å¢å¼ºç¦»çº¿æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">offline reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13709v2" data-paper-url="./papers/250513709v2-policy-driven-world-model-adaptation-for-robust-offline-model-based-.html" onclick="toggleFavorite(this, '2505.13709v2', 'Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250603154v1-modular-diffusion-policy-training-decoupling-and-recombining-guidanc.html">Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL</a></td>
  <td>æå‡ºæ¨¡å—åŒ–æ‰©æ•£ç­–ç•¥è®­ç»ƒä»¥ä¼˜åŒ–ç¦»çº¿å¼ºåŒ–å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline RL</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03154v1" data-paper-url="./papers/250603154v1-modular-diffusion-policy-training-decoupling-and-recombining-guidanc.html" onclick="toggleFavorite(this, '2506.03154v1', 'Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250513144v1-temporal-distance-aware-transition-augmentation-for-offline-model-ba.html">Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning</a></td>
  <td>æå‡ºTempDATAä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">offline reinforcement learning</span> <span class="paper-tag">model-based RL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13144v1" data-paper-url="./papers/250513144v1-temporal-distance-aware-transition-augmentation-for-offline-model-ba.html" onclick="toggleFavorite(this, '2505.13144v1', 'Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250514725v1-hr-vilage-3k3m-a-human-respiratory-viral-immunization-longitudinal-g.html">HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene Expression Dataset for Systems Immunity</a></td>
  <td>æ„å»ºHR-VILAGE-3K3Mæ•°æ®é›†ä»¥è§£å†³å‘¼å¸ç—…æ¯’å…ç–«ç ”ç©¶ä¸­çš„æ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">predictive model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14725v1" data-paper-url="./papers/250514725v1-hr-vilage-3k3m-a-human-respiratory-viral-immunization-longitudinal-g.html" onclick="toggleFavorite(this, '2505.14725v1', 'HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene Expression Dataset for Systems Immunity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250513638v1-4hammer-a-board-game-reinforcement-learning-environment-for-the-hour.html">4Hammer: a board-game reinforcement learning environment for the hour long time frame</a></td>
  <td>æå‡º4Hammerä»¥è§£å†³é•¿æ—¶é—´æ¡†æ¶ä¸‹å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13638v1" data-paper-url="./papers/250513638v1-4hammer-a-board-game-reinforcement-learning-environment-for-the-hour.html" onclick="toggleFavorite(this, '2505.13638v1', '4Hammer: a board-game reinforcement learning environment for the hour long time frame')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250513447v1-mean-flows-for-one-step-generative-modeling.html">Mean Flows for One-step Generative Modeling</a></td>
  <td>æå‡ºMeanFlowæ¨¡å‹ä»¥è§£å†³ä¸€é˜¶æ®µç”Ÿæˆå»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">curriculum learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13447v1" data-paper-url="./papers/250513447v1-mean-flows-for-one-step-generative-modeling.html" onclick="toggleFavorite(this, '2505.13447v1', 'Mean Flows for One-step Generative Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250513697v3-rl-in-name-only-analyzing-the-structural-assumptions-in-rl-post-trai.html">RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs</a></td>
  <td>åˆ†æå¼ºåŒ–å­¦ä¹ åè®­ç»ƒåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç»“æ„å‡è®¾</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13697v3" data-paper-url="./papers/250513697v3-rl-in-name-only-analyzing-the-structural-assumptions-in-rl-post-trai.html" onclick="toggleFavorite(this, '2505.13697v3', 'RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250513438v3-optimizing-anytime-reasoning-via-budget-relative-policy-optimization.html">Optimizing Anytime Reasoning via Budget Relative Policy Optimization</a></td>
  <td>æå‡ºAnytimeReasonerä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„å³æ—¶æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13438v3" data-paper-url="./papers/250513438v3-optimizing-anytime-reasoning-via-budget-relative-policy-optimization.html" onclick="toggleFavorite(this, '2505.13438v3', 'Optimizing Anytime Reasoning via Budget Relative Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250513358v3-one-step-offline-distillation-of-diffusion-based-models-via-koopman-.html">One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling</a></td>
  <td>æå‡ºåŸºäºKoopmanå»ºæ¨¡çš„ä¸€æ­¥ç¦»çº¿è’¸é¦æ–¹æ³•ä»¥æå‡æ‰©æ•£æ¨¡å‹æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13358v3" data-paper-url="./papers/250513358v3-one-step-offline-distillation-of-diffusion-based-models-via-koopman-.html" onclick="toggleFavorite(this, '2505.13358v3', 'One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.LG é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)