---
layout: default
title: "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models"
---

# BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15689" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15689v2</a>
  <a href="https://arxiv.org/pdf/2506.15689.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15689v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15689v2', 'BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liulu He, Shenli Zheng, Karwei Sun, Yijiang Liu, Yufei Zhao, Chongkang Tan, Huanrui Yang, Yuan Du, Li Du

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-08-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBASE-Qä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é‡åŒ–ä¸­çš„åå·®ä¸å‰ªåˆ‡è¯¯å·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `å¤§è¯­è¨€æ¨¡å‹` `æ—‹è½¬é‡åŒ–` `åå·®ä¿®æ­£` `éå¯¹ç§°ç¼©æ”¾` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ—‹è½¬é‡åŒ–æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šæœªèƒ½å¯¹é½é€šé“å‡å€¼å’Œå¯¼è‡´æ¿€æ´»åˆ†å¸ƒæ›´é«˜æ–¯åŒ–ï¼Œå¢åŠ äº†èˆå…¥å’Œå‰ªåˆ‡è¯¯å·®ã€‚
2. BASE-Qé€šè¿‡ç»“åˆåå·®ä¿®æ­£å’Œéå¯¹ç§°ç¼©æ”¾ï¼Œæ—¨åœ¨æœ‰æ•ˆå‡å°‘é‡åŒ–è¿‡ç¨‹ä¸­çš„èˆå…¥å’Œå‰ªåˆ‡è¯¯å·®ï¼ŒåŒæ—¶æ”¯æŒå—çº§ä¼˜åŒ–ä»¥é™ä½å†…å­˜æ¶ˆè€—ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBASE-Qåœ¨å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ä¸Šç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ˆå¦‚QuaRotã€SpinQuantå’ŒOSTQuantï¼‰åˆ†åˆ«ç¼©å°äº†50.5%ã€42.9%å’Œ29.2%çš„å‡†ç¡®æ€§å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ—‹è½¬åœ¨å¤§è¯­è¨€æ¨¡å‹çš„é‡åŒ–ç®¡é“ä¸­å˜å¾—è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¹³æ»‘æƒé‡å’Œæ¿€æ´»å€¼ä¸­çš„å¼‚å¸¸å€¼ã€‚ç„¶è€Œï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ—‹è½¬å‚æ•°çš„æ•ˆæœæœ‰é™ï¼Œå¹¶ä¸”å¼•å…¥äº†æ˜¾è‘—çš„è®­ç»ƒå¼€é”€ã€‚æœ¬æ–‡è¯†åˆ«äº†å½“å‰æ—‹è½¬é‡åŒ–æ–¹æ³•çš„ä¸¤ä¸ªåŸºæœ¬å±€é™æ€§ï¼šæ—‹è½¬æœªèƒ½å¯¹é½é€šé“å‡å€¼ï¼Œå¯¼è‡´é‡åŒ–èŒƒå›´æ›´å®½å’Œèˆå…¥è¯¯å·®å¢åŠ ï¼›æ—‹è½¬ä½¿æ¿€æ´»åˆ†å¸ƒæ›´æ¥è¿‘é«˜æ–¯åˆ†å¸ƒï¼Œå¢åŠ äº†å› å‰ªåˆ‡è¯¯å·®é€ æˆçš„èƒ½é‡æŸå¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BASE-Qï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆåå·®ä¿®æ­£å’Œéå¯¹ç§°ç¼©æ”¾çš„ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘èˆå…¥å’Œå‰ªåˆ‡è¯¯å·®ã€‚æ­¤å¤–ï¼ŒBASE-Qæ”¯æŒå—çº§ä¼˜åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹å†…å­˜å¯†é›†å‹å…¨æ¨¡å‹åå‘ä¼ æ’­çš„éœ€æ±‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBASE-Qåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰æ—‹è½¬é‡åŒ–æ–¹æ³•åœ¨å¯¹é½é€šé“å‡å€¼å’Œæ¿€æ´»åˆ†å¸ƒæ–¹é¢çš„ä¸è¶³ï¼Œå¯¼è‡´çš„é‡åŒ–è¯¯å·®å’Œèƒ½é‡æŸå¤±é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¼˜åŒ–æ—‹è½¬å‚æ•°æ—¶ï¼Œéœ€åŠ è½½å…¨æ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œé€ æˆæ˜¾è‘—çš„å†…å­˜æ¶ˆè€—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBASE-Qçš„æ ¸å¿ƒæ€æƒ³æ˜¯ç»“åˆåå·®ä¿®æ­£å’Œéå¯¹ç§°ç¼©æ”¾ï¼Œé€šè¿‡ä¼˜åŒ–é‡åŒ–è¿‡ç¨‹ä¸­çš„å‚æ•°è®¾ç½®ï¼Œå‡å°‘èˆå…¥å’Œå‰ªåˆ‡è¯¯å·®ï¼ŒåŒæ—¶æ”¯æŒå—çº§ä¼˜åŒ–ï¼Œé™ä½å†…å­˜éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBASE-Qçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åå·®ä¿®æ­£æ¨¡å—å’Œéå¯¹ç§°ç¼©æ”¾æ¨¡å—ï¼Œé¦–å…ˆå¯¹æƒé‡å’Œæ¿€æ´»å€¼è¿›è¡Œåå·®ä¿®æ­£ï¼Œç„¶ååº”ç”¨éå¯¹ç§°ç¼©æ”¾ä»¥ä¼˜åŒ–é‡åŒ–è¿‡ç¨‹ï¼Œæœ€åé€šè¿‡å—çº§ä¼˜åŒ–å®ç°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šBASE-Qçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åå·®ä¿®æ­£å’Œéå¯¹ç§°ç¼©æ”¾çš„ç»“åˆï¼Œæ˜¾è‘—æ”¹å–„äº†æ—‹è½¬é‡åŒ–çš„æ•ˆæœï¼Œå¹¶ä¸”æ”¯æŒå—çº§ä¼˜åŒ–ï¼Œé¿å…äº†å…¨æ¨¡å‹åå‘ä¼ æ’­çš„å†…å­˜ç“¶é¢ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒBASE-Qé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡èˆå…¥å’Œå‰ªåˆ‡è¯¯å·®ï¼ŒåŒæ—¶åœ¨å‚æ•°è®¾ç½®ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡èƒ½æœ‰æ•ˆè¿è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBASE-Qåœ¨å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ä¸Šç›¸è¾ƒäºç°æœ‰çš„é‡åŒ–æ–¹æ³•ï¼ˆå¦‚QuaRotã€SpinQuantå’ŒOSTQuantï¼‰åˆ†åˆ«ç¼©å°äº†50.5%ã€42.9%å’Œ29.2%çš„å‡†ç¡®æ€§å·®è·ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BASE-Qçš„ç ”ç©¶æˆæœåœ¨å¤§è¯­è¨€æ¨¡å‹çš„é‡åŒ–é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½å†…å­˜æ¶ˆè€—å¹¶æé«˜æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯æ‰©å±•è‡³å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œä»»åŠ¡ï¼Œæ¨åŠ¨é‡åŒ–æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Rotations have become essential to state-of-the-art quantization pipelines for large language models (LLMs) by effectively smoothing outliers in weights and activations. However, further optimizing the rotation parameters offers only limited performance gains and introduces significant training overhead: due to rotation parameter sharing, full-model must be loaded simultaneously to enable backpropagation, resulting in substantial memory consumption and limited practical utility. In this work, we identify two fundamental limitations of current rotational quantization methods: (i) rotation fails to align channel means, resulting in wider quantization bounds and increased rounding errors; and (ii) rotation makes the activation distribution more Gaussian-like, increasing energy loss caused by clipping errors. To address these issues, we introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias correction and asymmetric scaling to effectively reduce rounding and clipping errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the need for memory-intensive full-model backpropagation. Extensive experiments on various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\% compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be released soon.

