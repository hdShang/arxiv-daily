---
layout: default
title: The challenge of hidden gifts in multi-agent reinforcement learning
---

# The challenge of hidden gifts in multi-agent reinforcement learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20579" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20579v5</a>
  <a href="https://arxiv.org/pdf/2505.20579.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20579v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20579v5', 'The challenge of hidden gifts in multi-agent reinforcement learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dane Malenfant, Blake A. Richards

**åˆ†ç±»**: cs.LG, cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: Added LOLA baselines to appendix, new corollary proof on correction term not conflicting with individual objectives, related works on multi-objective RL and coordination MARL, expanded the contraposition appendix experiment, moved key drop rate experiments to appendix and aligned first success plots with key-drop plots

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­éšè—ç¤¼ç‰©é—®é¢˜çš„æ–°æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `éšè—ç¤¼ç‰©` `ä¿¡ç”¨åˆ†é…` `å»ä¸­å¿ƒåŒ–å­¦ä¹ ` `æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•` `å­¦ä¹ æ„è¯†` `é›†ä½“å¥–åŠ±` `æ™ºèƒ½ä½“åä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. éšè—ç¤¼ç‰©é—®é¢˜ä½¿å¾—å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡ç”¨åˆ†é…å˜å¾—å¤æ‚ï¼Œç°æœ‰ç®—æ³•åœ¨æ­¤åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æä¾›æ™ºèƒ½ä½“è‡ªèº«çš„è¡ŒåŠ¨å†å²ä¿¡æ¯ï¼Œæ”¹å–„å»ä¸­å¿ƒåŒ–æ¼”å‘˜-è¯„è®ºå®¶ç­–ç•¥æ¢¯åº¦æ™ºèƒ½ä½“çš„å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¿®æ­£åçš„ç­–ç•¥æ¢¯åº¦æ™ºèƒ½ä½“åœ¨ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿ç®—æ³•æ˜¾è‘—æé«˜äº†é›†ä½“å¥–åŠ±çš„è·å–ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ï¼Œéšè—ç¤¼ç‰©çš„æ¦‚å¿µæŒ‡çš„æ˜¯ä¸ªä½“åœ¨ä¸çŸ¥æƒ…çš„æƒ…å†µä¸‹ï¼Œä»ä»–äººçš„è¡ŒåŠ¨ä¸­è·ç›Šã€‚æœ¬æ–‡ç ”ç©¶äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„MARLä»»åŠ¡ï¼Œå…¶ä¸­æ™ºèƒ½ä½“åœ¨ä¸€ä¸ªç½‘æ ¼ç¯å¢ƒä¸­è§£é”å„è‡ªçš„é—¨ä»¥è·å¾—å¥–åŠ±ã€‚æ‰€æœ‰æ™ºèƒ½ä½“è§£é”é—¨åï¼Œèƒ½å¤Ÿè·å¾—æ›´å¤§çš„é›†ä½“å¥–åŠ±ï¼Œä½†åªæœ‰ä¸€ä¸ªé’¥åŒ™å¯ä¾›ä½¿ç”¨ï¼Œä¸”æ™ºèƒ½ä½“æ— æ³•å¾—çŸ¥ä»–äººæ˜¯å¦å·²æ”¾ä¸‹é’¥åŒ™ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å¤šç§MARLç®—æ³•åœ¨æ­¤ä»»åŠ¡ä¸­æ— æ³•æœ‰æ•ˆå­¦ä¹ ï¼Œè€Œå»ä¸­å¿ƒåŒ–çš„æ¼”å‘˜-è¯„è®ºå®¶ç­–ç•¥æ¢¯åº¦æ™ºèƒ½ä½“åœ¨æä¾›è‡ªèº«è¡ŒåŠ¨å†å²ä¿¡æ¯åèƒ½å¤ŸæˆåŠŸå®Œæˆä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ æ„è¯†çš„æ–¹æ³•ä¿®æ­£ç­–ç•¥æ¢¯åº¦æ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†æ”¶æ•›æ€§å’ŒæˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­éšè—ç¤¼ç‰©å¸¦æ¥çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ä»–äººè¡ŒåŠ¨çš„éšè”½æ€§æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆå­¦ä¹ å¦‚ä½•è·å¾—é›†ä½“å¥–åŠ±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥æ™ºèƒ½ä½“è‡ªèº«çš„è¡ŒåŠ¨å†å²ä¿¡æ¯ï¼Œå¢å¼ºæ™ºèƒ½ä½“çš„å­¦ä¹ æ„è¯†ï¼Œä»è€Œæ”¹å–„å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªç½‘æ ¼ç¯å¢ƒï¼Œæ™ºèƒ½ä½“é€šè¿‡è§£é”å„è‡ªçš„é—¨è·å¾—å¥–åŠ±ï¼ŒåŒæ—¶éœ€è¦å…±äº«é’¥åŒ™ä»¥å®ç°é›†ä½“å¥–åŠ±ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ™ºèƒ½ä½“çš„å†³ç­–ç½‘ç»œå’Œå†å²ä¿¡æ¯å¤„ç†æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ä¿®æ­£é¡¹ï¼ŒåŸºäºå­¦ä¹ æ„è¯†çš„æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ–¹å·®ï¼Œæé«˜äº†æ”¶æ•›æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´æœ‰æ•ˆåœ°å¤„ç†äº†éšè—ç¤¼ç‰©é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹æ™ºèƒ½ä½“çš„è¡ŒåŠ¨å†å²è¿›è¡Œç¼–ç ï¼Œé‡‡ç”¨å»ä¸­å¿ƒåŒ–çš„æ¼”å‘˜-è¯„è®ºå®¶æ¶æ„ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ä¿®æ­£é¡¹ï¼Œä»¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¿®æ­£åçš„å»ä¸­å¿ƒåŒ–æ¼”å‘˜-è¯„è®ºå®¶ç­–ç•¥æ¢¯åº¦æ™ºèƒ½ä½“åœ¨ä»»åŠ¡ä¸­æˆåŠŸè·å–é›†ä½“å¥–åŠ±çš„æ¦‚ç‡æ˜¾è‘—æé«˜ï¼Œç›¸è¾ƒäºä¼ ç»ŸMARLç®—æ³•ï¼ŒæˆåŠŸç‡æå‡äº†çº¦30%ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å­¦ä¹ æ„è¯†åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œä»»åŠ¡ï¼Œå¦‚æ— äººæœºç¼–é˜Ÿã€æ™ºèƒ½äº¤é€šç®¡ç†å’Œæœºå™¨äººå›¢é˜Ÿåˆä½œç­‰ã€‚é€šè¿‡æ”¹å–„æ™ºèƒ½ä½“çš„å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„åä½œï¼Œæå‡ç³»ç»Ÿæ•´ä½“æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These ``hidden gifts'' represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus this act for others is a ``hidden gift''. We show that several different state-of-the-art MARL algorithms, including MARL specific architectures, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that decentralized actor-critic policy gradient agents can succeed when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for policy gradient agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of ``hidden gifts'', and demonstrate that self learning-awareness in decentralized agents can benefit these settings.

