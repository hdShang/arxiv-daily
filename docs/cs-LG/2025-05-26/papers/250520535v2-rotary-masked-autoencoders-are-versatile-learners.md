---
layout: default
title: Rotary Masked Autoencoders are Versatile Learners
---

# Rotary Masked Autoencoders are Versatile Learners

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20535" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20535v2</a>
  <a href="https://arxiv.org/pdf/2505.20535.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20535v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20535v2', 'Rotary Masked Autoencoders are Versatile Learners')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Uros Zivanovic, Serafina Di Gioia, Andre Scaffidi, MartÃ­n de los Rios, Gabriella Contardo, Roberto Trotta

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-11-08)

**å¤‡æ³¨**: NeurIPS 2025 Camera Ready

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRotary Masked Autoencoderä»¥è§£å†³æ—¶é—´åºåˆ—å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—å­¦ä¹ ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦å­¦ä¹ ` `è¡¨ç¤ºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Transformeræ–¹æ³•åœ¨å¤„ç†ä¸è§„åˆ™æ—¶é—´åºåˆ—æ—¶éœ€è¦ç‰¹æ®ŠåŒ–æ¶æ„ï¼Œå¯¼è‡´è®¡ç®—å¤æ‚æ€§å¢åŠ ã€‚
2. RoMAEé€šè¿‡å¼•å…¥Rotary Positional Embeddingï¼Œæ‰©å±•äº†Masked Autoencoderï¼Œæ”¯æŒå¤šç»´è¿ç»­ä½ç½®çš„å­¦ä¹ ã€‚
3. RoMAEåœ¨å¤šç§æ¨¡æ€ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¤æ‚æ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¸“é—¨çš„æ—¶é—´åºåˆ—æ¶æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„Transformeråœ¨å¤„ç†ä¸è§„åˆ™æ—¶é—´åºåˆ—æ—¶é€šå¸¸éœ€è¦å¯¹å…¶åŸºç¡€æ¶æ„è¿›è¡Œç‰¹æ®ŠåŒ–ï¼Œè¿™ä¼šå¯¼è‡´é¢å¤–çš„è®¡ç®—å¼€é”€å’Œæ–¹æ³•å¤æ‚æ€§ã€‚æœ¬æ–‡æå‡ºäº†Rotary Masked Autoencoderï¼ˆRoMAEï¼‰ï¼Œåˆ©ç”¨æµè¡Œçš„Rotary Positional Embeddingï¼ˆRoPEï¼‰æ–¹æ³•æ¥å¤„ç†è¿ç»­ä½ç½®ã€‚RoMAEæ˜¯å¯¹Masked Autoencoderï¼ˆMAEï¼‰çš„æ‰©å±•ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦æ—¶é—´åºåˆ—ç‰¹å®šæ¶æ„çš„æƒ…å†µä¸‹ï¼Œå®ç°å¤šç»´è¿ç»­ä½ç½®çš„ä¿¡æ¯æ’å€¼å’Œè¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬å±•ç¤ºäº†RoMAEåœ¨å¤šç§æ¨¡æ€ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ä¸è§„åˆ™å’Œå¤šå˜é‡æ—¶é—´åºåˆ—ã€å›¾åƒå’ŒéŸ³é¢‘ï¼Œè¯æ˜RoMAEåœ¨å¤æ‚æ•°æ®é›†ï¼ˆå¦‚DESC ELAsTiCCæŒ‘æˆ˜ï¼‰ä¸Šè¶…è¶Šäº†ä¸“é—¨çš„æ—¶é—´åºåˆ—æ¶æ„ï¼ŒåŒæ—¶åœ¨å…¶ä»–æ¨¡æ€ä¸Šä¿æŒäº†MAEçš„å¸¸è§„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†RoMAEé‡å»ºåµŒå…¥è¿ç»­ä½ç½®çš„èƒ½åŠ›ï¼Œè¡¨æ˜åœ¨è¾“å…¥åºåˆ—ä¸­åŒ…å«å­¦ä¹ åˆ°çš„åµŒå…¥ä¼šç ´åRoPEçš„ç›¸å¯¹ä½ç½®å±æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„Transformeræ¶æ„åœ¨å¤„ç†ä¸è§„åˆ™æ—¶é—´åºåˆ—æ—¶ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œç‰¹æ®ŠåŒ–è®¾è®¡ï¼Œè¿™å¯¼è‡´äº†è®¡ç®—å¼€é”€çš„å¢åŠ å’Œæ–¹æ³•çš„å¤æ‚æ€§ã€‚RoMAEæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæä¾›ä¸€ç§é€šç”¨çš„å­¦ä¹ æ¡†æ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoMAEçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨Rotary Positional Embeddingï¼ˆRoPEï¼‰æ¥å¤„ç†è¿ç»­ä½ç½®ï¼Œä»è€Œé¿å…æ—¶é—´åºåˆ—ç‰¹å®šçš„æ¶æ„è®¾è®¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒRoMAEèƒ½å¤Ÿåœ¨å¤šç§æ¨¡æ€ä¸‹è¿›è¡Œæœ‰æ•ˆçš„è¡¨ç¤ºå­¦ä¹ å’Œæ’å€¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoMAEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ•°æ®çš„åµŒå…¥ã€ä½ç½®ç¼–ç çš„å¼•å…¥ã€ä»¥åŠé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾å­¦ä¹ ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†ä¸è§„åˆ™å’Œå¤šç»´æ•°æ®ï¼Œä¿æŒäº†MAEçš„åŸºæœ¬ç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šRoMAEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯¹Rotary Positional Embeddingçš„åº”ç”¨ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸å¢åŠ å¤æ‚æ€§çš„æƒ…å†µä¸‹ï¼Œå¤„ç†å¤šç»´è¿ç»­ä½ç½®çš„ä¿¡æ¯ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ—¶é—´åºåˆ—ä¸“ç”¨æ¶æ„æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šRoMAEåœ¨å‚æ•°è®¾ç½®ä¸Šä¿æŒäº†MAEçš„è®¾è®¡ç†å¿µï¼Œé‡‡ç”¨äº†ç›¸ä¼¼çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼ŒåŒæ—¶å¼•å…¥äº†å­¦ä¹ åˆ°çš„åµŒå…¥ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RoMAEåœ¨DESC ELAsTiCCæŒ‘æˆ˜æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¸“é—¨è®¾è®¡çš„æ—¶é—´åºåˆ—æ¶æ„ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoMAEåœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­ä¿æŒäº†MAEçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºåˆ†æã€åŒ»ç–—å¥åº·ç›‘æµ‹ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸è§„åˆ™å’Œå¤šç»´æ—¶é—´åºåˆ—æ•°æ®ã€‚RoMAEçš„é€šç”¨æ€§å’Œé«˜æ•ˆæ€§ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Applying Transformers to irregular time-series typically requires specializations to their baseline architecture, which can result in additional computational overhead and increased method complexity. We present the Rotary Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional Embedding (RoPE) method for continuous positions. RoMAE is an extension to the Masked Autoencoder (MAE) that enables interpolation and representation learning with multidimensional continuous positional information while avoiding any time-series-specific architectural specializations. We showcase RoMAE's performance on a variety of modalities including irregular and multivariate time-series, images, and audio, demonstrating that RoMAE surpasses specialized time-series architectures on difficult datasets such as the DESC ELAsTiCC Challenge while maintaining MAE's usual performance across other modalities. In addition, we investigate RoMAE's ability to reconstruct the embedded continuous positions, demonstrating that including learned embeddings in the input sequence breaks RoPE's relative position property.

