---
layout: default
title: JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning
---

# JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19698" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19698v2</a>
  <a href="https://arxiv.org/pdf/2505.19698.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19698v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19698v2', 'JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jing Yu Lim, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo Liu

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-05-28)

**å¤‡æ³¨**: Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºJEDIä»¥è§£å†³æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ ä¸­çš„äººæœºæ€§èƒ½ä¸å¯¹ç§°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡å‹åŸºç¡€å¼ºåŒ–å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `äººæœºæ€§èƒ½å¯¹ç§°æ€§` `æ½œåœ¨ç©ºé—´` `è‡ªä¸€è‡´æ€§ç›®æ ‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°ä¸å‡ï¼Œå¯¼è‡´äººæœºæ€§èƒ½ä¸å¯¹ç§°ï¼Œå½±å“æ•´ä½“è¯„ä¼°ã€‚
2. æœ¬æ–‡æå‡ºJEDIï¼Œé€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åƒç´ åŸºç¡€ä»£ç†ä¸­çš„æ—¶é—´ç»“æ„ç¼ºå¤±é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒJEDIåœ¨äººç±»æœ€ä¼˜ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶åœ¨Atari100kåŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰åœ¨Atari100kåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†è¶…äººç±»æ°´å¹³çš„è¡¨ç°ï¼Œå¾—ç›Šäºå¼ºå¤§çš„æ‰©æ•£ä¸–ç•Œæ¨¡å‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„èšåˆæŒ‡æ ‡æ©ç›–äº†ä¸€ä¸ªä¸»è¦çš„æ€§èƒ½ä¸å¯¹ç§°æ€§ï¼šåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼ŒMBRLä»£ç†æ˜¾è‘—è¶…è¶Šäººç±»ï¼Œè€Œåœ¨å…¶ä»–ä»»åŠ¡ä¸­å´è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡é€šè¿‡å°†ä»»åŠ¡åˆ’åˆ†ä¸ºä»£ç†æœ€ä¼˜å’Œäººç±»æœ€ä¼˜ï¼Œæå‡ºäº†Joint Embedding DIffusionï¼ˆJEDIï¼‰ï¼Œä¸€ç§ç«¯åˆ°ç«¯è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨ç¼“è§£è¿™ç§ä¸å¯¹ç§°æ€§ã€‚JEDIåœ¨äººç±»æœ€ä¼˜ä»»åŠ¡ä¸­è¶…è¶Šäº†æœ€æ–°çš„æ¨¡å‹ï¼ŒåŒæ—¶åœ¨Atari100kåŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ï¼Œä¸”è¿è¡Œé€Ÿåº¦æé«˜äº†ä¸‰å€ï¼Œå†…å­˜ä½¿ç”¨é™ä½äº†43%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åƒç´ åŸºç¡€çš„ä»£ç†åœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºäººç±»ï¼Œè€Œåœ¨å…¶ä»–ä»»åŠ¡ä¸­åˆ™è¿œä¸åŠäººç±»ï¼Œå¯¼è‡´æ€§èƒ½è¯„ä¼°çš„åå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„JEDIæ¨¡å‹é€šè¿‡å¼•å…¥ç«¯åˆ°ç«¯çš„æ½œåœ¨æ‰©æ•£è®­ç»ƒï¼Œæ—¨åœ¨å»ºç«‹ä¸€ä¸ªå…·æœ‰æ—¶é—´ç»“æ„çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥æ›´å¥½åœ°é€‚åº”ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œä»è€Œå‡å°‘äººæœºæ€§èƒ½çš„ä¸å¯¹ç§°æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šJEDIæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ½œåœ¨ç©ºé—´çš„æ„å»ºã€æ‰©æ•£è¿‡ç¨‹çš„ä¼˜åŒ–å’Œè‡ªä¸€è‡´æ€§ç›®æ ‡çš„å®ç°ã€‚æ¨¡å‹é€šè¿‡è‡ªä¸€è‡´æ€§ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿ç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä»»åŠ¡çš„åŠ¨æ€ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šJEDIçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹å¼å’Œè‡ªä¸€è‡´æ€§ç›®æ ‡çš„å¼•å…¥ï¼Œè¿™ä¸ä¼ ç»Ÿçš„åˆ†é˜¶æ®µè®­ç»ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†ä»»åŠ¡ä¸­çš„æ—¶é—´ä¾èµ–æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒJEDIä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ½œåœ¨ç©ºé—´çš„ç»“æ„ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ¥æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒJEDIåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†æœ€æ–°çš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨äººç±»æœ€ä¼˜ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚åŒæ—¶ï¼ŒJEDIçš„è¿è¡Œé€Ÿåº¦æé«˜äº†ä¸‰å€ï¼Œå†…å­˜ä½¿ç”¨é™ä½äº†43%ï¼Œåœ¨Atari100kåŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç«äº‰åŠ›ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æ”¹å–„äººæœºæ€§èƒ½çš„å¯¹ç§°æ€§ï¼ŒJEDIå¯ä»¥åœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸­å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ å’Œå†³ç­–ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼ŒJEDIçš„è®¾è®¡ç†å¿µä¹Ÿå¯èƒ½è¢«åº”ç”¨äºå…¶ä»–é¢†åŸŸçš„æ¨¡å‹è®­ç»ƒä¸­ï¼Œæå‡æ¨¡å‹çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in model-based reinforcement learning (MBRL) have achieved super-human level performance on the Atari100k benchmark, driven by reinforcement learning agents trained on powerful diffusion world models. However, we identify that the current aggregates mask a major performance asymmetry: MBRL agents dramatically outperform humans in some tasks despite drastically underperforming in others, with the former inflating the aggregate metrics. This is especially pronounced in pixel-based agents trained with diffusion world models. In this work, we address the pronounced asymmetry observed in pixel-based agents as an initial attempt to reverse the worrying upward trend observed in them. We address the problematic aggregates by delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal importance on metrics from both sets. Next, we hypothesize this pronounced asymmetry is due to the lack of temporally-structured latent space trained with the World Model objective in pixel-based methods. Lastly, to address this issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective. JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline. Overall, our work rethinks what it truly means to cross human-level performance in Atari100k.

