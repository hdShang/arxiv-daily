---
layout: default
title: "HoPE: Hybrid of Position Embedding for Long Context Vision-Language Models"
---

# HoPE: Hybrid of Position Embedding for Long Context Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20444" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.20444v2</a>
  <a href="https://arxiv.org/pdf/2505.20444.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20444v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20444v2', 'HoPE: Hybrid of Position Embedding for Long Context Vision-Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Haoran Li, Yingjie Qin, Baoyuan Ou, Lai Xu, Ruiwen Xu

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-26 (Êõ¥Êñ∞: 2025-10-08)

**Â§áÊ≥®**: NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/hrlics/HoPE)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HoPE‰ª•Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÁêÜËß£‰∏≠ÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøËßÜÈ¢ëÁêÜËß£` `Â§öÊ®°ÊÄÅÊ®°Âûã` `‰ΩçÁΩÆÁºñÁ†Å` `Êó∂Á©∫‰æùËµñ` `Âä®ÊÄÅÊó∂Èó¥Áº©Êîæ` `Ê∑∑ÂêàÈ¢ëÁéáÂàÜÈÖç` `ËßÜÈ¢ëÊ£ÄÁ¥¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅRoPEÊñπÊ≥ïÂú®Èïø‰∏ä‰∏ãÊñáÂú∫ÊôØ‰∏≠Êó†Ê≥ïÊúâÊïàÊçïÊçâËØ≠‰πâÁõ∏‰ººÊÄßÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. HoPEÈÄöËøáÊ∑∑ÂêàÈ¢ëÁéáÂàÜÈÖçÁ≠ñÁï•ÂíåÂä®ÊÄÅÊó∂Èó¥Áº©ÊîæÊú∫Âà∂ÔºåÊó®Âú®ÊèêÂçáVLMsÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠ÁöÑË°®Áé∞„ÄÇ
3. Âú®Âõõ‰∏™ËßÜÈ¢ëÂü∫ÂáÜ‰∏äËøõË°åÁöÑÂÆûÈ™åË°®ÊòéÔºåHoPEÂú®ÈïøËßÜÈ¢ëÁêÜËß£ÂíåÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®Èïø‰∏ä‰∏ãÊñáÂú∫ÊôØ‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÈïøËßÜÈ¢ë‰∏≠ÔºåÂÖ∂ÊÄßËÉΩÂæÄÂæÄ‰∏ãÈôç„ÄÇÂ∞ΩÁÆ°ÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÔºàRoPEÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠Ë¢´ÂπøÊ≥õÈááÁî®‰ª•ÂÆûÁé∞ÈïøÂ∫¶Ê≥õÂåñÔºå‰ΩÜÂ∞ÜÂÖ∂Êâ©Â±ïÂà∞ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑÂ§çÊùÇÊó∂Á©∫‰æùËµñ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™Ëß£ÂÜ≥ÁöÑÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âú®RoPE‰∏≠ÂàÜÈÖç‰∏çÂêåÈ¢ëÁéá‰ª•ÁºñÁ†Å3D‰ΩçÁΩÆ‰ø°ÊÅØÔºå‰ΩÜËøô‰∫õÁ≠ñÁï•‰∏ªË¶Å‰æùËµñÂêØÂèëÂºèÊñπÊ≥ïÔºåÁº∫‰πèÊ∑±ÂÖ•ÁöÑÁêÜËÆ∫ÂàÜÊûê„ÄÇÊú¨ÊñáÈ¶ñÂÖàÁ†îÁ©∂‰∏çÂêåÂàÜÈÖçÁ≠ñÁï•ÂØπVLMsÈïø‰∏ä‰∏ãÊñáËÉΩÂäõÁöÑÂΩ±ÂìçÔºåÂèëÁé∞ÂΩìÂâçÁöÑÂ§öÊ®°ÊÄÅRoPEÊó†Ê≥ïÂèØÈù†Âú∞ÊçïÊçâÊâ©Â±ï‰∏ä‰∏ãÊñá‰∏≠ÁöÑËØ≠‰πâÁõ∏‰ººÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜHoPEÔºå‰∏ÄÁßçÊ∑∑Âêà‰ΩçÁΩÆÁºñÁ†ÅÔºåÊó®Âú®ÊèêÈ´òVLMsÁöÑÈïø‰∏ä‰∏ãÊñáËÉΩÂäõ„ÄÇHoPEÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÈ¢ëÁéáÂàÜÈÖçÁ≠ñÁï•Ôºå‰ª•ÂÆûÁé∞ÂØπ‰ªªÊÑèÈïø‰∏ä‰∏ãÊñáÁöÑÂèØÈù†ËØ≠‰πâÂª∫Ê®°ÔºåÂπ∂ÈááÁî®Âä®ÊÄÅÊó∂Èó¥Áº©ÊîæÊú∫Âà∂Ôºå‰ª•‰øÉËøõÂú®‰∏çÂêå‰∏ä‰∏ãÊñáÈïøÂ∫¶‰∏ãÁöÑÁ®≥ÂÅ•Â≠¶‰π†ÂíåÁÅµÊ¥ªÊé®ÁêÜ„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåHoPEÂú®Âõõ‰∏™ËßÜÈ¢ëÂü∫ÂáÜ‰∏äÁöÑÈïøËßÜÈ¢ëÁêÜËß£ÂíåÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÂßãÁªà‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§öÊ®°ÊÄÅRoPEÂú®ÈïøËßÜÈ¢ëÁêÜËß£‰∏≠Êó†Ê≥ïÊúâÊïàÊçïÊçâÊó∂Á©∫‰æùËµñÁöÑÈóÆÈ¢òÔºåÂØºËá¥Èïø‰∏ä‰∏ãÊñáÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHoPEÈÄöËøáÂºïÂÖ•Ê∑∑ÂêàÈ¢ëÁéáÂàÜÈÖçÁ≠ñÁï•ÂíåÂä®ÊÄÅÊó∂Èó¥Áº©ÊîæÊú∫Âà∂ÔºåÊó®Âú®ÂÆûÁé∞ÂØπ‰ªªÊÑèÈïø‰∏ä‰∏ãÊñáÁöÑÂèØÈù†ËØ≠‰πâÂª∫Ê®°Ôºå‰ªéËÄåÊèêÂçáVLMsÁöÑÈïø‰∏ä‰∏ãÊñáËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHoPEÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊ∑∑ÂêàÈ¢ëÁéáÂàÜÈÖçÊ®°ÂùóÂíåÂä®ÊÄÅÊó∂Èó¥Áº©ÊîæÊ®°Âùó„ÄÇÂâçËÄÖË¥üË¥£Ê†πÊçÆ‰∏ä‰∏ãÊñáÈïøÂ∫¶Âä®ÊÄÅË∞ÉÊï¥È¢ëÁéáÂàÜÈÖçÔºåÂêéËÄÖÂàôÈÄöËøáÊó∂Èó¥Áº©ÊîæÊú∫Âà∂Â¢ûÂº∫Ê®°ÂûãÁöÑÂ≠¶‰π†ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHoPEÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂Ê∑∑ÂêàÈ¢ëÁéáÂàÜÈÖçÁ≠ñÁï•ÔºåËÉΩÂ§üÊ†πÊçÆ‰∏ä‰∏ãÊñáÁöÑ‰∏çÂêåÁâπÊÄßÁÅµÊ¥ªË∞ÉÊï¥È¢ëÁéáÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÂõ∫ÂÆöÈ¢ëÁéáÂàÜÈÖçÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåHoPEÈááÁî®‰∫ÜÂä®ÊÄÅË∞ÉÊï¥ÁöÑÈ¢ëÁéáÂàÜÈÖçÁ≠ñÁï•ÔºåÂπ∂ËÆæËÆ°‰∫ÜÈÄÇÂ∫î‰∏çÂêå‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•Á°Æ‰øùÊ®°ÂûãÂú®ÂêÑÁßçÂú∫ÊôØ‰∏ãÁöÑÁ®≥ÂÅ•ÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÁªÜËäÇÂíåËÆ≠ÁªÉËøáÁ®ãÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Âõõ‰∏™ËßÜÈ¢ëÂü∫ÂáÜ‰∏äÔºåHoPEÂú®ÈïøËßÜÈ¢ëÁêÜËß£ÂíåÊ£ÄÁ¥¢‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÂæÖË°•ÂÖÖÔºâÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Èïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HoPEÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ÈïøËßÜÈ¢ëÁêÜËß£„ÄÅËßÜÈ¢ëÊ£ÄÁ¥¢Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÂçáVLMsÂú®Èïø‰∏ä‰∏ãÊñáÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞ÔºåHoPEÂèØ‰ª•‰∏∫ËßÜÈ¢ëÂàÜÊûê„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅÂÜÖÂÆπÊé®ËçêÁ≠âÂÆûÈôÖÂ∫îÁî®Êèê‰æõÊõ¥‰∏∫Á≤æÂáÜÁöÑÊîØÊåÅÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long contexts, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Our code is available at https://github.com/hrlics/HoPE.

