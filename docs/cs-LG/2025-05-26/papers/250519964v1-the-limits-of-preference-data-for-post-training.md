---
layout: default
title: The Limits of Preference Data for Post-Training
---

# The Limits of Preference Data for Post-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19964" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19964v1</a>
  <a href="https://arxiv.org/pdf/2505.19964.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19964v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19964v1', 'The Limits of Preference Data for Post-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eric Zhao, Jessica Dai, Pranjal Awasthi

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.GT

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶åå¥½æ•°æ®å¯¹åè®­ç»ƒä¼˜åŒ–çš„é™åˆ¶åŠå…¶å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åå¥½æ•°æ®` `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `æŠ•ç¥¨ç†è®º` `åºæ•°åé¦ˆ` `æ¨¡å‹ä¼˜åŒ–` `æ¨ç†è¡Œä¸º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨éœ€è¦äººç±»åé¦ˆçš„é¢†åŸŸä¸­ï¼Œåå¥½æ•°æ®é™åˆ¶äº†ç»“æœä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æŠ•ç¥¨ç†è®ºå½¢å¼åŒ–åå¥½æ•°æ®çš„å±€é™æ€§ï¼Œå¼ºè°ƒäººç±»è¯„åˆ†å’Œç®—æ³•åˆ›æ–°çš„é‡è¦æ€§ã€‚
3. ç ”ç©¶å‘ç°åå¥½æ•°æ®å¯¹å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨æ¨ç†è¡Œä¸ºçš„å¼•å¯¼ä¸Šå½±å“æ˜¾è‘—ï¼ŒæŠ‘åˆ¶äº†å…¶èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨å¯è‡ªåŠ¨éªŒè¯ç»“æœçš„é¢†åŸŸã€‚ç„¶è€Œï¼Œåœ¨éœ€è¦äººç±»åé¦ˆçš„é¢†åŸŸï¼Œç»“æœè¯„ä¼°å¾€å¾€æ˜¯å®šæ€§çš„ï¼Œå­˜åœ¨å¤šç§æˆåŠŸçš„å¯èƒ½æ€§ã€‚æœ¬æ–‡ç ”ç©¶äº†åå¥½æ•°æ®åœ¨ç»“æœä¼˜åŒ–ä¸­çš„æ ¹æœ¬é™åˆ¶ï¼Œå‘ç°å³ä½¿åœ¨ç†æƒ³åŒ–çš„æ¡ä»¶ä¸‹ï¼Œä½¿ç”¨åºæ•°åé¦ˆä¹Ÿå¯èƒ½æ— æ³•è·å¾—è¿‘ä¼¼æœ€ä¼˜è§£ã€‚é€šè¿‡æŠ•ç¥¨ç†è®ºçš„ç±»æ¯”ï¼Œæœ¬æ–‡æŒ‡å‡ºéœ€è¦æ›´æ‰å®çš„äººç±»è¯„åˆ†å’Œç®—æ³•åˆ›æ–°ï¼Œä»¥æ‰©å±•å¼ºåŒ–å­¦ä¹ åè®­ç»ƒçš„æˆåŠŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„é—®é¢˜æ˜¯åå¥½æ•°æ®åœ¨åè®­ç»ƒä¼˜åŒ–ä¸­çš„å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦äººç±»åé¦ˆçš„ä»»åŠ¡æ—¶ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°ç»“æœï¼Œå¯¼è‡´ä¼˜åŒ–æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æŠ•ç¥¨ç†è®ºå½¢å¼åŒ–åå¥½æ•°æ®çš„é™åˆ¶ï¼ŒæŒ‡å‡ºå³ä½¿åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œåºæ•°åé¦ˆä¹Ÿæ— æ³•ä¿è¯æ¥è¿‘æœ€ä¼˜è§£ï¼Œä»è€Œå¼ºè°ƒäº†äººç±»è¯„åˆ†çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) åå¥½æ•°æ®æ”¶é›†ï¼Œ2) ç»“æœè¯„ä¼°ä¸ä¼˜åŒ–ï¼Œ3) äººç±»åé¦ˆæ•´åˆã€‚æ¯ä¸ªæ¨¡å—éƒ½æ—¨åœ¨è§£å†³ç‰¹å®šçš„ä¼˜åŒ–æŒ‘æˆ˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†æŠ•ç¥¨ç†è®ºåº”ç”¨äºåå¥½æ•°æ®çš„åˆ†æï¼Œæ­ç¤ºäº†å…¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æ ¹æœ¬é™åˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œè®ºæ–‡å¼ºè°ƒäº†ç†æƒ³åŒ–åå¥½æ•°æ®çš„ç‰¹å¾ï¼Œå¦‚æ— é™ã€æ— å™ªå£°å’Œåœ¨çº¿æ”¶é›†ï¼ŒåŒæ—¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡äººç±»è¯„åˆ†æ¥è¡¥å……åºæ•°åé¦ˆçš„ä¸è¶³ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†è¯´æ˜ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåå¥½æ•°æ®çš„å±€é™æ€§æ˜¾è‘—æŠ‘åˆ¶äº†å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨æ¨ç†è¡Œä¸ºå¼•å¯¼ä¸Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„è¡¨ç°æœªèƒ½è¾¾åˆ°é¢„æœŸçš„ä¼˜åŒ–æ•ˆæœï¼Œå¼ºè°ƒäº†äººç±»è¯„åˆ†çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éœ€è¦äººç±»åé¦ˆçš„å¤æ‚ä»»åŠ¡ï¼Œå¦‚æ·±åº¦ç ”ç©¶å’Œæ—…è¡Œè§„åˆ’ã€‚é€šè¿‡æ”¹è¿›äººç±»åé¦ˆçš„æ”¶é›†å’Œæ•´åˆæ–¹å¼ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨è¿™äº›é¢†åŸŸçš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent progress in strengthening the capabilities of large language models has stemmed from applying reinforcement learning to domains with automatically verifiable outcomes. A key question is whether we can similarly use RL to optimize for outcomes in domains where evaluating outcomes inherently requires human feedback; for example, in tasks like deep research and trip planning, outcome evaluation is qualitative and there are many possible degrees of success. One attractive and scalable modality for collecting human feedback is preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$ given outcomes, which one is preferred. In this work, we study a critical roadblock: preference data fundamentally and significantly limits outcome-based optimization. Even with idealized preference data (infinite, noiseless, and online), the use of ordinal feedback can prevent obtaining even approximately optimal solutions. We formalize this impossibility using voting theory, drawing an analogy between how a model chooses to answer a query with how voters choose a candidate to elect. This indicates that grounded human scoring and algorithmic innovations are necessary for extending the success of RL post-training to domains demanding human feedback. We also explore why these limitations have disproportionately impacted RLHF when it comes to eliciting reasoning behaviors (e.g., backtracking) versus situations where RLHF has been historically successful (e.g., instruction-tuning and safety training), finding that the limitations of preference data primarily suppress RLHF's ability to elicit robust strategies -- a class that encompasses most reasoning behaviors.

