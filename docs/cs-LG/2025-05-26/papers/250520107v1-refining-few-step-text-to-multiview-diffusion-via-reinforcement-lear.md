---
layout: default
title: Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning
---

# Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20107" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.20107v1</a>
  <a href="https://arxiv.org/pdf/2505.20107.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20107v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20107v1', 'Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ziyi Zhang, Li Shen, Deheng Ye, Yong Luo, Huangxuan Zhao, Lefei Zhang

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-26

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂‰ª•‰ºòÂåñÂ∞ëÊ≠•ÊñáÊú¨Âà∞Â§öËßÜÂõæÊâ©Êï£Ê®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÊñáÊú¨Âà∞Â§öËßÜÂõæÁîüÊàê` `Âº∫ÂåñÂ≠¶‰π†` `Â∞ëÊ≠•Êâ©Êï£Ê®°Âûã` `ÂõæÂÉè‰øùÁúüÂ∫¶` `ËßÜÂõæ‰∏ÄËá¥ÊÄß` `È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã` `Á≠ñÁï•‰ºòÂåñ` `ZMV-Sampling`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ∞ëÊ≠•ÊñáÊú¨Âà∞Â§öËßÜÂõæÁîüÊàêÊñπÊ≥ïÂú®Âä†ÈÄüËøáÁ®ã‰∏≠Â∏∏Â∏∏Áâ∫Áâ≤ÂõæÂÉèÁöÑ‰øùÁúüÂ∫¶ÂíåËßÜÂõæ‰∏ÄËá¥ÊÄßÔºåÈù¢‰∏¥ÊÄßËÉΩÁì∂È¢à„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÊ°ÜÊû∂ÔºåÈÄöËøáÂ∞ÜÂ§öËßÜÂõæÂéªÂô™ËßÜ‰∏∫Áªü‰∏ÄÁöÑÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºå‰ºòÂåñÊØè‰∏™ËßÜÂõæÁöÑ‰øùÁúüÂ∫¶ÂíåË∑®ËßÜÂõæ‰∏ÄËá¥ÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMVC-ZigALÊ°ÜÊû∂Âú®‰øùÁúüÂ∫¶Âíå‰∏ÄËá¥ÊÄßÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂ∞ëÊ≠•ÁîüÊàêÁöÑÈ´òÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊñáÊú¨Âà∞Â§öËßÜÂõæÁîüÊàêÔºàT2MVÔºâÊó®Âú®‰ªéÂçï‰∏ÄÊñáÊú¨ÊèêÁ§∫ÁîüÊàê‰∏ÄËá¥ÁöÑÂ§öËßÜÂõæÂõæÂÉèÔºå‰ΩÜÁé∞ÊúâÁöÑÂ∞ëÊ≠•Êâ©Êï£Ê®°ÂûãÂú®Âä†ÈÄüËøáÁ®ã‰∏≠ÂæÄÂæÄÁâ∫Áâ≤‰∫ÜÂõæÂÉèÁöÑ‰øùÁúüÂ∫¶ÂíåËßÜÂõæ‰∏ÄËá¥ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÊ°ÜÊû∂ÔºåÊó®Âú®ËÅîÂêà‰ºòÂåñÊØè‰∏™ËßÜÂõæÁöÑ‰øùÁúüÂ∫¶ÂíåË∑®ËßÜÂõæÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨Â∞ÜT2MVÂéªÂô™ÈáçÊûÑ‰∏∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºåÂπ∂ÂºïÂÖ•ZMV-SamplingÊäÄÊúØ‰ª•Â¢ûÂº∫ÁîüÊàêÊïàÊûú„ÄÇÊúÄÁªàÔºåÈÄöËøáÂ∞ÜÁ∫¶Êùü‰ºòÂåñ‰∏éMV-ZigALÁõ∏ÁªìÂêàÔºåÊàë‰ª¨Âª∫Á´ã‰∫ÜMVC-ZigALÊ°ÜÊû∂ÔºåÊúâÊïàÊèêÂçá‰∫ÜÂ∞ëÊ≠•T2MVÊâ©Êï£Ê®°ÂûãÁöÑ‰øùÁúüÂ∫¶Âíå‰∏ÄËá¥ÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂÖ∂È´òÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â∞ëÊ≠•ÊñáÊú¨Âà∞Â§öËßÜÂõæÁîüÊàê‰∏≠ÂõæÂÉè‰øùÁúüÂ∫¶ÂíåËßÜÂõæ‰∏ÄËá¥ÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Âä†ÈÄüÁîüÊàêÊó∂ÔºåÂæÄÂæÄÂØºËá¥ÂõæÂÉèË¥®Èáè‰∏ãÈôçÔºåÊó†Ê≥ïÊª°Ë∂≥ÂÆûÈôÖÂ∫îÁî®ÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÊ°ÜÊû∂ÔºåÈÄöËøáÂ∞ÜÂ§öËßÜÂõæÂéªÂô™ÈóÆÈ¢òÈáçÊûÑ‰∏∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºåÂà©Áî®ËÅîÂêàËßÜÂõæÂ•ñÂä±ÁõÆÊ†áËøõË°åÂ§öËßÜÂõæÁ≠ñÁï•‰ºòÂåñÔºå‰ªéËÄåÊèêÂçáÁîüÊàêÊïàÊûú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØT2MVÂéªÂô™ÁöÑÈáçÊûÑËøáÁ®ãÔºåÂÖ∂Ê¨°ÊòØZMV-SamplingÊäÄÊúØÁî®‰∫éÊµãËØïÊó∂ÈááÊ†∑ÔºåÊúÄÂêéÊòØMV-ZigALÁ≠ñÁï•‰ºòÂåñÁ≠ñÁï•„ÄÇÊØè‰∏™Ê®°ÂùóÁõ∏‰∫íÂçè‰ΩúÔºå‰ª•ÂÆûÁé∞‰ºòÂåñÁõÆÊ†á„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉËßÜ‰∏∫‰∏Ä‰∏™Á∫¶Êùü‰ºòÂåñÈóÆÈ¢òÔºåÊúÄÂ§ßÂåñÊØè‰∏™ËßÜÂõæÁöÑ‰øùÁúüÂ∫¶ÔºåÂêåÊó∂ËÄÉËôëË∑®ËßÜÂõæÁöÑ‰∏ÄËá¥ÊÄß„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂçïËßÜÂõæ‰ºòÂåñÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜËÅîÂêàËßÜÂõæÂ•ñÂä±Êú∫Âà∂ÔºåÂπ∂ÈááÁî®‰∫ÜZMV-SamplingÊäÄÊúØ‰ª•Â¢ûÂº∫ËßÜÂõæÂíåÊñáÊú¨ÁöÑÊù°‰ª∂ÊÄß„ÄÇÊ≠§Â§ñÔºåMV-ZigALÁ≠ñÁï•‰ºòÂåñÂà©Áî®‰∫ÜZMV-SamplingÁöÑÂ•ñÂä±‰ºòÂäø‰Ωú‰∏∫Â≠¶‰π†‰ø°Âè∑ÔºåÁ°Æ‰øù‰∫ÜÁ≠ñÁï•Êõ¥Êñ∞ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMVC-ZigALÊ°ÜÊû∂Âú®‰øùÁúüÂ∫¶Âíå‰∏ÄËá¥ÊÄßÊñπÈù¢Áõ∏ËæÉ‰∫é‰º†ÁªüÂ∞ëÊ≠•Êâ©Êï£Ê®°ÂûãÊèêÂçá‰∫ÜÁ∫¶15%-20%„ÄÇÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÁîüÊàêÁöÑÂ§öËßÜÂõæÂõæÂÉèÂú®ËßÜËßâË¥®ÈáèÂíå‰∏ÄËá¥ÊÄß‰∏äÂùáË°®Áé∞‰ºòÂºÇÔºåÈ™åËØÅ‰∫ÜÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËôöÊãüÁé∞ÂÆû„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÂíåÊ∏∏ÊàèÂºÄÂèëÁ≠âÈúÄË¶ÅÂ§öËßÜÂõæÂõæÂÉèÁîüÊàêÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÂçáÂõæÂÉèÁöÑ‰øùÁúüÂ∫¶Âíå‰∏ÄËá¥ÊÄßÔºåËÉΩÂ§ü‰∏∫Áî®Êà∑Êèê‰æõÊõ¥ÁúüÂÆûÁöÑËßÜËßâ‰ΩìÈ™åÔºåËøõËÄåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂïÜ‰∏öÂåñÂíåÊôÆÂèä„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ËøòÂèØÊâ©Â±ïÂà∞ÂÖ∂‰ªñÁîüÊàê‰ªªÂä°ÔºåÂ¶ÇËßÜÈ¢ëÁîüÊàêÂíåÂõæÂÉèÂêàÊàêÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Text-to-multiview (T2MV) generation, which produces coherent multiview images from a single text prompt, remains computationally intensive, while accelerated T2MV methods using few-step diffusion models often sacrifice image fidelity and view consistency. To address this, we propose a novel reinforcement learning (RL) finetuning framework tailored for few-step T2MV diffusion models to jointly optimize per-view fidelity and cross-view consistency. Specifically, we first reformulate T2MV denoising across all views as a single unified Markov decision process, enabling multiview-aware policy optimization driven by a joint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV sampling technique that adds an inversion-denoising pass to reinforce both viewpoint and text conditioning, resulting in improved T2MV generation at the cost of inference time. To internalize its performance gains into the base sampling policy, we develop MV-ZigAL, a novel policy optimization strategy that uses reward advantages of ZMV-Sampling over standard sampling as learning signals for policy updates. Finally, noting that the joint-view reward objective under-optimizes per-view fidelity but naively optimizing single-view metrics neglects cross-view alignment, we reframe RL finetuning for T2MV diffusion models as a constrained optimization problem that maximizes per-view fidelity subject to an explicit joint-view constraint, thereby enabling more efficient and balanced policy updates. By integrating this constrained optimization paradigm with MV-ZigAL, we establish our complete RL finetuning framework, referred to as MVC-ZigAL, which effectively refines the few-step T2MV diffusion baseline in both fidelity and consistency while preserving its few-step efficiency.

