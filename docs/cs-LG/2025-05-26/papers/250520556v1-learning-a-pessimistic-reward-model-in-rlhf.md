---
layout: default
title: Learning a Pessimistic Reward Model in RLHF
---

# Learning a Pessimistic Reward Model in RLHF

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20556" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20556v1</a>
  <a href="https://arxiv.org/pdf/2505.20556.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20556v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20556v1', 'Learning a Pessimistic Reward Model in RLHF')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yinglun Xu, Hangoo Kang, Tarun Suresh, Yuxuan Wan, Gagandeep Singh

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPETæ–¹æ³•ä»¥è§£å†³ç¦»çº¿RLHFä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `å¥–åŠ±å»ºæ¨¡` `æ‚²è§‚å¥–åŠ±` `ç­–ç•¥ä¼˜åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ–‡æœ¬æ‘˜è¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±å»ºæ¨¡æŠ€æœ¯åœ¨RLHFä¸­å­˜åœ¨ä¸å®Œç¾çš„å¥–åŠ±æ¨¡å‹ï¼Œå¯¼è‡´å¥–åŠ±é»‘å®¢é—®é¢˜ä¾ç„¶ä¸¥é‡ã€‚
2. è®ºæ–‡æå‡ºçš„PETæ–¹æ³•é€šè¿‡å¾®è°ƒæ‚²è§‚å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹ä¼˜åŒ–ç­–ç•¥ï¼Œé˜²æ­¢å¥–åŠ±é»‘å®¢ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ‚²è§‚å¥–åŠ±æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°é«˜è´¨é‡çš„ç­–ç•¥ï¼Œä¸”ä¸æ•°æ®é›†åˆ†å¸ƒçš„KLæ•£åº¦è¾ƒå¤§ï¼Œä½†å®é™…æ€§èƒ½ä¾ç„¶ä¼˜ç§€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ‚²è§‚å¥–åŠ±å¾®è°ƒæ–¹æ³•PETï¼Œä»¥å­¦ä¹ ä¸€ç§å¯¹å¥–åŠ±é»‘å®¢å…·æœ‰é²æ£’æ€§çš„æ‚²è§‚å¥–åŠ±æ¨¡å‹ï¼Œé€‚ç”¨äºç¦»çº¿äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ä¼ ç»Ÿçš„å¥–åŠ±å»ºæ¨¡æŠ€æœ¯åœ¨RLHFä¸­è®­ç»ƒä¸å®Œç¾çš„å¥–åŠ±æ¨¡å‹ï¼ŒKLæ­£åˆ™åŒ–åœ¨ä¼˜åŒ–ç­–ç•¥æ—¶èµ·ç€å…³é”®ä½œç”¨ä»¥å‡è½»å¥–åŠ±é»‘å®¢çš„å½±å“ã€‚ç„¶è€Œï¼Œè¿™ç§åŸºäºç›´è§‰çš„æ–¹æ³•ä»ç„¶é¢ä¸´å¥–åŠ±é»‘å®¢çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ’é™¤äº†ä¸æ•°æ®é›†åˆ†å¸ƒå…·æœ‰è¾ƒå¤§KLæ•£åº¦çš„ç­–ç•¥ã€‚ç›¸åï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡PETå¾®è°ƒçš„æ‚²è§‚å¥–åŠ±æ¨¡å‹ä¼˜åŒ–ç­–ç•¥æ—¶ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–ä»»ä½•æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹é˜²æ­¢å¥–åŠ±é»‘å®¢ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†çš„TL;DRæ‘˜è¦æ•°æ®é›†ä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå‘ç°å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹å­¦ä¹ é«˜è´¨é‡çš„ç­–ç•¥ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†å­¦ä¹ é’ˆå¯¹å¥–åŠ±é»‘å®¢çš„æ‚²è§‚å¥–åŠ±æ¨¡å‹çš„å¯è¡Œæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºKLæ­£åˆ™åŒ–æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ä»ç„¶æ— æ³•å®Œå…¨æ¶ˆé™¤å¥–åŠ±é»‘å®¢çš„å½±å“ï¼Œä¸”æ’é™¤äº†ä¸æ•°æ®é›†åˆ†å¸ƒå·®å¼‚è¾ƒå¤§çš„ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„PETæ–¹æ³•é€šè¿‡å¾®è°ƒæ‚²è§‚å¥–åŠ±æ¨¡å‹ï¼Œä½¿å¾—åœ¨ä¼˜åŒ–ç­–ç•¥æ—¶èƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢å¥–åŠ±é»‘å®¢ï¼Œè€Œä¸éœ€è¦ä¾èµ–ä»»ä½•æ­£åˆ™åŒ–æ‰‹æ®µã€‚è¿™ç§è®¾è®¡ä½¿å¾—ä»£ç†èƒ½å¤Ÿè´ªå©ªåœ°æœç´¢é«˜æ‚²è§‚å¥–åŠ±çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œç­–ç•¥ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆæ”¶é›†äººç±»åé¦ˆæ•°æ®ï¼Œç„¶åè®­ç»ƒæ‚²è§‚å¥–åŠ±æ¨¡å‹ï¼Œæœ€ååœ¨è¯¥æ¨¡å‹ä¸Šä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†PETæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒæ‚²è§‚å¥–åŠ±æ¨¡å‹æ¥æ›¿ä»£ä¼ ç»Ÿçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»æ ¹æœ¬ä¸Šæ”¹å˜äº†ç­–ç•¥ä¼˜åŒ–çš„æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ç¡®ä¿æ‚²è§‚å¥–åŠ±çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶åœ¨ç­–ç•¥ä¼˜åŒ–é˜¶æ®µï¼Œå…è®¸è¾ƒå¤§çš„KLæ•£åº¦ï¼Œä»¥ä¾¿æ¢ç´¢æ›´ä¼˜çš„ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨PETæ–¹æ³•è®­ç»ƒçš„æ‚²è§‚å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ä»»ä½•æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹å­¦ä¹ åˆ°é«˜è´¨é‡çš„ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç­–ç•¥åœ¨å®è·µä¸­è¡¨ç°å‡ºè‰²ï¼Œå°½ç®¡ä¸æ•°æ®é›†åˆ†å¸ƒçš„KLæ•£åº¦è¾ƒå¤§ï¼Œä¾ç„¶èƒ½å¤Ÿå®ç°é«˜æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬æ‘˜è¦ã€å¯¹è¯ç³»ç»Ÿä»¥åŠå…¶ä»–éœ€è¦äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡æœ‰æ•ˆé˜²æ­¢å¥–åŠ±é»‘å®¢ï¼ŒPETæ–¹æ³•èƒ½å¤Ÿæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå®é™…åº”ç”¨æ•ˆæœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work proposes `PET', a novel pessimistic reward fine-tuning method, to learn a pessimistic reward model robust against reward hacking in offline reinforcement learning from human feedback (RLHF). Traditional reward modeling techniques in RLHF train an imperfect reward model, on which a KL regularization plays a pivotal role in mitigating reward hacking when optimizing a policy. Such an intuition-based method still suffers from reward hacking, and the policies with large KL divergence from the dataset distribution are excluded during learning. In contrast, we show that when optimizing a policy on a pessimistic reward model fine-tuned through PET, reward hacking can be prevented without relying on any regularization. We test our methods on the standard TL;DR summarization dataset. We find that one can learn a high-quality policy on our pessimistic reward without using any regularization. Such a policy has a high KL divergence from the dataset distribution while having high performance in practice. In summary, our work shows the feasibility of learning a pessimistic reward model against reward hacking. The agent can greedily search for the policy with a high pessimistic reward without suffering from reward hacking.

