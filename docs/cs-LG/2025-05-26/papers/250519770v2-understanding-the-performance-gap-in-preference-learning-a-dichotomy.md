---
layout: default
title: Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO
---

# Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19770" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19770v2</a>
  <a href="https://arxiv.org/pdf/2505.19770.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19770v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19770v2', 'Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruizhe Shi, Minhak Song, Runlong Zhou, Zihan Zhang, Maryam Fazel, Simon S. Du

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-10-03)

**å¤‡æ³¨**: 30 pages, 5 figures. Improved proofs, and typo fixes

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»†è‡´ç†è®ºåˆ†æä»¥ç†è§£RLHFä¸DPOé—´çš„æ€§èƒ½å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ` `ç›´æ¥åå¥½ä¼˜åŒ–` `æ€§èƒ½å·®è·` `è¡¨ç¤ºå·®è·` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ä¸ç›´æ¥åå¥½ä¼˜åŒ–æ—¶å­˜åœ¨æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨è¡¨ç¤ºå·®è·çš„å½±å“ä¸‹ã€‚
2. è®ºæ–‡é€šè¿‡ç†è®ºåˆ†æå°†æ€§èƒ½å·®è·åˆ†è§£ä¸ºæ˜¾å¼å’Œéšå¼è¡¨ç¤ºå·®è·ï¼Œå¹¶æ¢è®¨ä¸åŒæ¨¡å‹æŒ‡å®šå¯¹ç­–ç•¥è´¨é‡çš„å½±å“ã€‚
3. ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œåœ¨çº¿DPOèƒ½å¤Ÿè¶…è¶ŠRLHFå’Œæ ‡å‡†DPOï¼Œä¸”RLHFåœ¨æ ·æœ¬æ•ˆç‡ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¯¹äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨è¡¨ç¤ºå·®è·ä¸‹çš„æ€§èƒ½å·®è·è¿›è¡Œäº†ç»†è‡´çš„ç†è®ºåˆ†æã€‚ç ”ç©¶å°†è¿™ä¸€å·®è·åˆ†è§£ä¸ºä¸¤ä¸ªæ¥æºï¼šåœ¨ç²¾ç¡®ä¼˜åŒ–ä¸‹çš„æ˜¾å¼è¡¨ç¤ºå·®è·å’Œåœ¨æœ‰é™æ ·æœ¬ä¸‹çš„éšå¼è¡¨ç¤ºå·®è·ã€‚åœ¨ç²¾ç¡®ä¼˜åŒ–è®¾ç½®ä¸­ï¼Œè®ºæ–‡è¡¨å¾äº†å¥–åŠ±æ¨¡å‹å’Œç­–ç•¥æ¨¡å‹ç±»åˆ«çš„ç›¸å¯¹èƒ½åŠ›å¦‚ä½•å½±å“æœ€ç»ˆç­–ç•¥è´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLHFã€DPOæˆ–åœ¨çº¿DPOçš„è¡¨ç°å–å†³äºæ¨¡å‹çš„é”™è¯¯æŒ‡å®šç±»å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“å¥–åŠ±å’Œç­–ç•¥æ¨¡å‹ç±»åˆ«åŒæ„ä¸”å‡è¢«é”™è¯¯æŒ‡å®šæ—¶ï¼Œåœ¨çº¿DPOå¯ä»¥è¶…è¶ŠRLHFå’Œæ ‡å‡†DPOã€‚åœ¨è¿‘ä¼¼ä¼˜åŒ–è®¾ç½®ä¸­ï¼Œè®ºæ–‡æä¾›äº†ä¸€ä¸ªå…·ä½“æ„é€ ï¼Œè¡¨æ˜RLHFåœ¨æ¢å¤æœ‰æ•ˆå¥–åŠ±æ¨¡å‹æ—¶æ‰€éœ€çš„æ ·æœ¬æ˜¾è‘—å°‘äºDPOï¼Œçªæ˜¾äº†ä¸¤é˜¶æ®µå­¦ä¹ çš„ç»Ÿè®¡ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœä¸ºç†è§£RLHFä¸DPOé—´çš„æ€§èƒ½å·®è·æä¾›äº†å…¨é¢çš„è§†è§’ï¼Œå¹¶ä¸ºé€‰æ‹©åˆé€‚çš„æ–¹æ³•æä¾›äº†å®ç”¨çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨è¡¨ç¤ºå·®è·çš„å½±å“ä¸‹ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹æŒ‡å®šæƒ…å†µä¸‹çš„è¡¨ç°ä¸å°½ç›¸åŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡ç»†è‡´çš„ç†è®ºåˆ†æï¼Œå°†æ€§èƒ½å·®è·åˆ†è§£ä¸ºæ˜¾å¼å’Œéšå¼è¡¨ç¤ºå·®è·ï¼Œæ¢è®¨å¦‚ä½•é€šè¿‡ä¼˜åŒ–æ¨¡å‹ç±»åˆ«çš„ç›¸å¯¹èƒ½åŠ›æ¥æ”¹å–„ç­–ç•¥è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå®šä¹‰äº†ç²¾ç¡®ä¼˜åŒ–å’Œè¿‘ä¼¼ä¼˜åŒ–çš„è®¾ç½®ï¼Œåˆ†æäº†å¥–åŠ±æ¨¡å‹å’Œç­–ç•¥æ¨¡å‹çš„ç›¸å¯¹èƒ½åŠ›ï¼Œç„¶åé€šè¿‡æ„é€ ç¤ºä¾‹å±•ç¤ºäº†RLHFå’ŒDPOåœ¨æ ·æœ¬æ•ˆç‡ä¸Šçš„å·®å¼‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ€§èƒ½å·®è·ç»†åˆ†ä¸ºä¸¤ç§ç±»å‹çš„è¡¨ç¤ºå·®è·ï¼Œå¹¶æ­ç¤ºäº†åœ¨çº¿DPOåœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„ä¼˜è¶Šæ€§ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€è§†è§’å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­è®¾è®¡äº†å…·ä½“çš„å¥–åŠ±æ¨¡å‹å’Œç­–ç•¥æ¨¡å‹ç±»åˆ«ï¼Œå¹¶é€šè¿‡ç†è®ºæ¨å¯¼å’Œæ„é€ ç¤ºä¾‹éªŒè¯äº†RLHFåœ¨æ ·æœ¬æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ï¼Œå¼ºè°ƒäº†ä¸¤é˜¶æ®µå­¦ä¹ çš„ç»Ÿè®¡ä¼˜åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œåœ¨çº¿DPOçš„æ€§èƒ½è¶…è¶Šäº†RLHFå’Œæ ‡å‡†DPOï¼Œå°¤å…¶æ˜¯åœ¨å¥–åŠ±å’Œç­–ç•¥æ¨¡å‹åŒæ„ä¸”å‡è¢«é”™è¯¯æŒ‡å®šçš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼ŒRLHFåœ¨æ ·æœ¬æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºDPOï¼Œå±•ç¤ºäº†åœ¨æ¢å¤æœ‰æ•ˆå¥–åŠ±æ¨¡å‹æ—¶æ‰€éœ€æ ·æœ¬æ•°é‡çš„æ˜¾è‘—å‡å°‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€æ¨èç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–å†³ç­–ç­‰ã€‚é€šè¿‡ç†è§£ä¸åŒå­¦ä¹ æ–¹æ³•çš„æ€§èƒ½å·®è·ï¼Œç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆå¯ä»¥æ›´æœ‰æ•ˆåœ°é€‰æ‹©åˆé€‚çš„ç®—æ³•æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä»è€Œæå‡å®é™…åº”ç”¨çš„æ•ˆæœå’Œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å­¦ä¹ ç³»ç»Ÿçš„å‘å±•ï¼Œæ”¹å–„äººç±»ä¸æœºå™¨çš„åä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a fine-grained theoretical analysis of the performance gap between reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under a representation gap. Our study decomposes this gap into two sources: an explicit representation gap under exact optimization and an implicit representation gap under finite samples. In the exact optimization setting, we characterize how the relative capacities of the reward and policy model classes influence the final policy qualities. We show that RLHF, DPO, or online DPO can outperform one another depending on type of model mis-specifications. Notably, online DPO can outperform both RLHF and standard DPO when the reward and policy model classes are isomorphic and both mis-specified. In the approximate optimization setting, we provide a concrete construction where the ground-truth reward is implicitly sparse and show that RLHF requires significantly fewer samples than DPO to recover an effective reward model -- highlighting a statistical advantage of two-stage learning. Together, these results provide a comprehensive understanding of the performance gap between RLHF and DPO under various settings, and offer practical insights into when each method is preferred.

