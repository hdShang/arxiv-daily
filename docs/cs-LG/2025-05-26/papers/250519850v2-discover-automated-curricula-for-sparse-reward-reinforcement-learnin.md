---
layout: default
title: "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning"
---

# DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19850" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.19850v2</a>
  <a href="https://arxiv.org/pdf/2505.19850.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19850v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19850v2', 'DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Leander Diaz-Bone, Marco Bagatella, Jonas H√ºbotter, Andreas Krause

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-26 (Êõ¥Êñ∞: 2025-10-20)

**Â§áÊ≥®**: NeurIPS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DISCOVER‰ª•Ëß£ÂÜ≥Á®ÄÁñèÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÊé¢Á¥¢ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Á®ÄÁñèÂ•ñÂä±` `Âº∫ÂåñÂ≠¶‰π†` `Êé¢Á¥¢Á≠ñÁï•` `ÁõÆÊ†áÈÄâÊã©` `È´òÁª¥ÁéØÂ¢É` `Ëá™ÊàëÊîπËøõ‰ª£ÁêÜ` `‰∫∫Â∑•Êô∫ËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ®ÄÁñèÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®È´òÁª¥„ÄÅÈïøÊó∂Èó¥‰ªªÂä°ÁöÑÊé¢Á¥¢‰∏äÈù¢‰∏¥Â∑®Â§ßÊåëÊàòÔºåÂØºËá¥Ëß£ÂÜ≥ÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑDISCOVERÊñπÊ≥ïÈÄöËøáÈÄâÊã©‰∏éÁõÆÊ†á‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁÆÄÂçï‰ªªÂä°Êù•ÊåáÂØºÊé¢Á¥¢ÔºåÊèêÂçá‰∫ÜÂ≠¶‰π†ÊïàÁéá„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDISCOVERÂú®È´òÁª¥ÁéØÂ¢É‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËß£ÂÜ≥‰∫Ü‰ª•ÂæÄÊñπÊ≥ïÊó†Ê≥ïÂ§ÑÁêÜÁöÑÊé¢Á¥¢ÈóÆÈ¢ò„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á®ÄÁñèÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËÉΩÂ§üÂª∫Ê®°Â§öÁßçÂ§çÊùÇ‰ªªÂä°Ôºå‰ΩÜËß£ÂÜ≥Á®ÄÁñèÂ•ñÂä±‰ªªÂä°ÁöÑÊ†∏ÂøÉÂú®‰∫éÈ´òÊïàÊé¢Á¥¢ÂíåÈïøÊó∂Èó¥‰ø°Áî®ÂàÜÈÖç„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÂÖ≥Ê≥®Ëß£ÂÜ≥Â§ö‰∏™Á®ÄÁñèÂ•ñÂä±‰ªªÂä°ÔºåÂØºËá¥‰∏™Âà´È´òÁª¥„ÄÅÈïøÊó∂Èó¥‰ªªÂä°ÁöÑÊé¢Á¥¢ÂèòÂæó‰∏çÂèØË°å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïDISCOVERÔºåÊó®Âú®ÈÄöËøáÈÄâÊã©‰∏éÁõÆÊ†á‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁÆÄÂçï‰ªªÂä°Êù•ÊåáÂØºÊé¢Á¥¢Ôºå‰ªéËÄåÊèêÈ´òËß£ÂÜ≥Â§çÊùÇ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨Â∞ÜDISCOVER‰∏éÊúâÂéüÂàôÁöÑÊé¢Á¥¢ÊñπÊ≥ïÁõ∏ËøûÊé•ÔºåÁêÜËÆ∫‰∏äÁïåÂÆö‰∫ÜËææÂà∞ÁõÆÊ†á‰ªªÂä°ÊâÄÈúÄÁöÑÊó∂Èó¥ÔºåÂπ∂Âú®È´òÁª¥ÁéØÂ¢É‰∏≠ËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÁªìÊûúË°®ÊòéËØ•ÊñπÊ≥ïÂú®Êé¢Á¥¢ÈóÆÈ¢ò‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Á®ÄÁñèÂ•ñÂä±Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÊé¢Á¥¢ÊïàÁéáÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Èù¢ÂØπÈ´òÁª¥„ÄÅÈïøÊó∂Èó¥‰ªªÂä°Êó∂ÔºåÂæÄÂæÄÊó†Ê≥ïÊúâÊïàÊåáÂØºÊé¢Á¥¢ÔºåÂØºËá¥Â≠¶‰π†ËøáÁ®ãÁºìÊÖ¢‰∏î‰ΩéÊïà„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑDISCOVERÊñπÊ≥ïÈÄöËøáÈÄâÊã©‰∏éÁõÆÊ†á‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁÆÄÂçï‰ªªÂä°Êù•ÂºïÂØºÊé¢Á¥¢ÔºåÂ∏ÆÂä©‰ª£ÁêÜÂ≠¶‰π†Ëß£ÂÜ≥Â§çÊùÇ‰ªªÂä°ÊâÄÈúÄÁöÑÊäÄËÉΩ„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰æùËµñ‰∫é‰ªª‰ΩïÂÖàÈ™å‰ø°ÊÅØÔºåËÉΩÂ§ü‰ªéÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï‰∏≠ÊèêÂèñÊé¢Á¥¢ÊñπÂêë„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDISCOVERÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÁõÆÊ†áÈÄâÊã©Ê®°ÂùóÂíåÊé¢Á¥¢Á≠ñÁï•Ê®°Âùó„ÄÇÁõÆÊ†áÈÄâÊã©Ê®°ÂùóË¥üË¥£‰ªéÁÆÄÂçï‰ªªÂä°‰∏≠ÈÄâÊã©‰∏éÁõÆÊ†á‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÊé¢Á¥¢ÁõÆÊ†áÔºåËÄåÊé¢Á¥¢Á≠ñÁï•Ê®°ÂùóÂàôÊ†πÊçÆËøô‰∫õÁõÆÊ†áÊåáÂØº‰ª£ÁêÜÁöÑÂ≠¶‰π†ËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDISCOVERÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÁõÆÊ†áÈÄâÊã©‰∏éÊúâÂéüÂàôÁöÑÊé¢Á¥¢ÊñπÊ≥ïÁõ∏ÁªìÂêàÔºåÁêÜËÆ∫‰∏äÁïåÂÆö‰∫ÜËææÂà∞ÁõÆÊ†á‰ªªÂä°ÊâÄÈúÄÁöÑÊó∂Èó¥„ÄÇËøô‰∏ÄÊñπÊ≥ïÁöÑÁã¨Áâπ‰πãÂ§ÑÂú®‰∫éÂÆÉ‰∏ç‰æùËµñ‰∫é‰ªªÂä°Á©∫Èó¥ÁöÑ‰ΩìÁßØÔºåËÄåÊòØÂÖ≥Ê≥®‰ª£ÁêÜ‰∏éÁõÆÊ†á‰πãÈó¥ÁöÑÂàùÂßãË∑ùÁ¶ª„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåDISCOVERÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÁõÆÊ†áÈÄâÊã©ËøáÁ®ãÔºåÂπ∂‰ΩøÁî®‰∫ÜÈ´òÁª¥ÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÊé¢Á¥¢Á≠ñÁï•„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑËÆæËÆ°Âú®ÂÆûÈ™å‰∏≠ÁªèËøáÂ§öÊ¨°Ë∞ÉÊï¥Ôºå‰ª•Á°Æ‰øùÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDISCOVERÂú®È´òÁª¥ÁéØÂ¢É‰∏≠ÁöÑÊé¢Á¥¢ÊïàÁéáÊòæËëóÊèêÂçáÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊé¢Á¥¢ÊñπÊ≥ïÔºåËß£ÂÜ≥‰∫ÜÊõ¥Â§öÂ§çÊùÇ‰ªªÂä°ÔºåË°®Áé∞Âá∫Êõ¥Âø´ÁöÑÂ≠¶‰π†ÈÄüÂ∫¶ÂíåÊõ¥È´òÁöÑÊàêÂäüÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÊ∏∏ÊàèAI„ÄÅËá™Âä®È©æÈ©∂Á≠âÈúÄË¶ÅËß£ÂÜ≥Â§çÊùÇÂÜ≥Á≠ñ‰ªªÂä°ÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÈ´òÁ®ÄÁñèÂ•ñÂä±‰ªªÂä°ÁöÑÂ≠¶‰π†ÊïàÁéáÔºåDISCOVERÊúâÂä©‰∫éÊûÑÂª∫Êõ¥Êô∫ËÉΩÁöÑËá™ÊàëÊîπËøõ‰ª£ÁêÜÔºåÊé®Âä®‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁöÑËøõÊ≠•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Sparse-reward reinforcement learning (RL) can model a wide range of highly complex tasks. Solving sparse-reward tasks is RL's core premise, requiring efficient exploration coupled with long-horizon credit assignment, and overcoming these challenges is key for building self-improving agents with superhuman ability. Prior work commonly explores with the objective of solving many sparse-reward tasks, making exploration of individual high-dimensional, long-horizon tasks intractable. We argue that solving such challenging tasks requires solving simpler tasks that are relevant to the target task, i.e., whose achieval will teach the agent skills required for solving the target task. We demonstrate that this sense of direction, necessary for effective exploration, can be extracted from existing RL algorithms, without leveraging any prior information. To this end, we propose a method for directed sparse-reward goal-conditioned very long-horizon RL (DISCOVER), which selects exploratory goals in the direction of the target task. We connect DISCOVER to principled exploration in bandits, formally bounding the time until the target task becomes achievable in terms of the agent's initial distance to the target, but independent of the volume of the space of all tasks. We then perform a thorough evaluation in high-dimensional environments. We find that the directed goal selection of DISCOVER solves exploration problems that are beyond the reach of prior state-of-the-art exploration methods in RL.

