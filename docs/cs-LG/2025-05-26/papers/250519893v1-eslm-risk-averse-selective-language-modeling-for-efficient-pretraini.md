---
layout: default
title: "ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining"
---

# ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19893" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19893v1</a>
  <a href="https://arxiv.org/pdf/2505.19893.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19893v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19893v1', 'ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Melis Ilayda Bal, Volkan Cevher, Michael Muehlebach

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºESLMä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ•ˆç‡ä¸é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `é¢„è®­ç»ƒ` `é£é™©æ„è¯†` `é€‰æ‹©æœºåˆ¶` `è®¡ç®—æ•ˆç‡` `åˆ†å¸ƒé²æ£’æ€§` `è‡ªé€‚åº”è°ƒæ•´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•è®¡ç®—èµ„æºæ¶ˆè€—å¤§ï¼Œä¸”è®¸å¤štokenå¯¹å­¦ä¹ è´¡çŒ®æœ‰é™ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºçš„ESLMç®—æ³•é€šè¿‡åœ¨çº¿é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„tokenï¼Œåˆ©ç”¨æ¯ä¸ªtokençš„ç»Ÿè®¡ä¿¡æ¯æ¥æé«˜è®­ç»ƒæ•ˆç‡å’Œé²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒESLMåœ¨GPT-2é¢„è®­ç»ƒä¸­æ˜¾è‘—é™ä½äº†è®­ç»ƒFLOPsï¼ŒåŒæ—¶åœ¨å›°æƒ‘åº¦å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè®¡ç®—å¯†é›†ï¼Œä½†è®¸å¤štokenå¯¹å­¦ä¹ çš„è´¡çŒ®å¾®ä¹å…¶å¾®ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†é«˜æ•ˆé€‰æ‹©è¯­è¨€å»ºæ¨¡ï¼ˆESLMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é£é™©æ„è¯†ç®—æ³•ï¼Œé€šè¿‡åœ¨çº¿tokençº§æ‰¹æ¬¡é€‰æ‹©æ¥æé«˜è®­ç»ƒæ•ˆç‡å’Œåˆ†å¸ƒé²æ£’æ€§ã€‚ESLMåˆ©ç”¨æ¯ä¸ªtokençš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚ç†µæˆ–æŸå¤±ï¼‰ï¼Œå¹¶åº”ç”¨é£é™©ä»·å€¼é˜ˆå€¼ï¼Œä»…ä¿ç•™æ¯ä¸ªæ‰¹æ¬¡ä¸­æœ€å…·ä¿¡æ¯é‡çš„tokenã€‚è¿™ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æœºåˆ¶é‡å¡‘äº†è®­ç»ƒæŸå¤±ï¼Œä¼˜å…ˆè€ƒè™‘é«˜é£é™©tokenï¼Œæ¶ˆé™¤å†—ä½™çš„æ¢¯åº¦è®¡ç®—ã€‚æˆ‘ä»¬å°†ESLMæ¡†æ¶è§†ä¸ºä¸€ä¸ªåŒå±‚åšå¼ˆï¼šæ¨¡å‹ä¸é€‰æ‹©æœ€åæƒ…å†µtokenå­é›†çš„æ©è”½å¯¹æ‰‹ç«äº‰ã€‚åœ¨åŸºäºæŸå¤±çš„è®¾ç½®ä¸­ï¼ŒESLMå®ç°äº†æ¡ä»¶ä»·å€¼-at-riskæŸå¤±æœ€å°åŒ–ï¼Œæä¾›äº†ä¸åˆ†å¸ƒé²æ£’ä¼˜åŒ–çš„åŸåˆ™æ€§è”ç³»ã€‚æˆ‘ä»¬å°†æ–¹æ³•æ‰©å±•åˆ°è‡ªé€‚åº”ESLMï¼ˆAda-ESLMï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”è°ƒæ•´é€‰æ‹©ç½®ä¿¡åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒESLMæ˜¾è‘—å‡å°‘è®­ç»ƒFLOPsï¼ŒåŒæ—¶ä¿æŒæˆ–æ”¹å–„å›°æƒ‘åº¦å’Œä¸‹æ¸¸æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ‰€æœ‰tokenï¼Œå¯¼è‡´èµ„æºæµªè´¹å’Œè®­ç»ƒæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šESLMé€šè¿‡åœ¨çº¿é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„tokenï¼Œé‡‡ç”¨é£é™©ä»·å€¼é˜ˆå€¼ç­–ç•¥ï¼Œä¼˜å…ˆä¿ç•™é«˜é£é™©tokenï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å¹¶å‡å°‘å†—ä½™è®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šESLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬tokené€‰æ‹©æ¨¡å—å’ŒæŸå¤±é‡å¡‘æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è®¡ç®—æ¯ä¸ªtokençš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚ç†µæˆ–æŸå¤±ï¼‰è¿›è¡Œåœ¨çº¿é€‰æ‹©ï¼Œç„¶åæ ¹æ®é€‰æ‹©ç»“æœè°ƒæ•´è®­ç»ƒæŸå¤±ã€‚

**å…³é”®åˆ›æ–°**ï¼šESLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†tokené€‰æ‹©è§†ä¸ºä¸€ä¸ªåŒå±‚åšå¼ˆï¼Œæ¨¡å‹ä¸æ©è”½å¯¹æ‰‹ç«äº‰ï¼Œç¡®ä¿é€‰æ‹©çš„tokenåœ¨æœ€åæƒ…å†µä¸‹ä»èƒ½ä¿æŒæœ‰æ•ˆæ€§ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ ¹æœ¬åŒºåˆ«åœ¨äºå…¶é£é™©æ„è¯†å’ŒåŠ¨æ€è°ƒæ•´æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒESLMä½¿ç”¨æ¡ä»¶ä»·å€¼-at-riskæŸå¤±æœ€å°åŒ–ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”è°ƒæ•´é€‰æ‹©ç½®ä¿¡åº¦çš„Ada-ESLMæ‰©å±•ï¼Œç¡®ä¿åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µçš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä¸çŸ¥è¯†è’¸é¦è‡ªç„¶ç»“åˆï¼Œé€‚åº”ä¸åŒæ¨¡å‹è§„æ¨¡å’Œé¢„è®­ç»ƒè¯­æ–™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒESLMåœ¨GPT-2é¢„è®­ç»ƒä¸­æ˜¾è‘—å‡å°‘äº†è®­ç»ƒFLOPsï¼Œå…·ä½“å‡å°‘å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼ŒåŒæ—¶åœ¨å›°æƒ‘åº¦å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜é¢„è®­ç»ƒçš„æ•ˆç‡ï¼ŒESLMèƒ½å¤ŸåŠ é€Ÿæ¨¡å‹å¼€å‘å’Œéƒ¨ç½²ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œæ¨åŠ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠä¸åº”ç”¨ã€‚æœªæ¥ï¼ŒESLMçš„æ€æƒ³ä¹Ÿå¯èƒ½æ‰©å±•åˆ°å…¶ä»–æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œæå‡å…¶è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model pretraining is compute-intensive, yet many tokens contribute marginally to learning, resulting in inefficiency. We introduce Efficient Selective Language Modeling (ESLM), a risk-aware algorithm that improves training efficiency and distributional robustness by performing online token-level batch selection. ESLM leverages per-token statistics (e.g., entropy or loss) and applies value-at-risk thresholding to retain only the most informative tokens per batch. This data-centric mechanism reshapes the training loss, prioritizing high-risk tokens and eliminating redundant gradient computation. We frame ESLM as a bilevel game: the model competes with a masking adversary that selects worst-case token subsets under a constrained thresholding rule. In the loss-based setting, ESLM recovers conditional value-at-risk loss minimization, providing a principled connection to distributionally robust optimization. We extend our approach to Ada-ESLM, which adaptively tunes the selection confidence during training. Experiments on GPT-2 pretraining show that ESLM significantly reduces training FLOPs while maintaining or improving both perplexity and downstream performance compared to baselines. Our approach also scales across model sizes, pretraining corpora, and integrates naturally with knowledge distillation.

