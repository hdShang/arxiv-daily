---
layout: default
title: Characterizing Pattern Matching and Its Limits on Compositional Task Structures
---

# Characterizing Pattern Matching and Its Limits on Compositional Task Structures

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20278" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20278v2</a>
  <a href="https://arxiv.org/pdf/2505.20278.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20278v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20278v2', 'Characterizing Pattern Matching and Its Limits on Compositional Task Structures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hoyeon Chang, Jinho Park, Hanseul Cho, Sohee Yang, Miyoung Ko, Hyeonbin Hwang, Seungpil Won, Dohaeng Lee, Youbin Ahn, Minjoon Seo

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-11-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨¡å¼åŒ¹é…çš„å½¢å¼åŒ–æ¡†æ¶ä»¥è§£å†³ç»„åˆä»»åŠ¡ä¸­çš„æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å¼åŒ¹é…` `åŠŸèƒ½ç­‰ä»·` `ç»„åˆä»»åŠ¡` `è¶…å‡ºåˆ†å¸ƒæ³›åŒ–` `è·¯å¾„æ¨¡ç³Šæ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ ·æœ¬å¤æ‚åº¦` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç»„åˆä»»åŠ¡ä¸­å­˜åœ¨å¤šç§æ³›åŒ–æ¥æºï¼Œå¯¼è‡´å¯¹LLMsæ¨¡å¼åŒ¹é…èƒ½åŠ›çš„è¯„ä¼°ä¸å¤Ÿç²¾ç¡®ã€‚
2. æœ¬æ–‡é€šè¿‡å°†æ¨¡å¼åŒ¹é…å½¢å¼åŒ–ä¸ºåŠŸèƒ½ç­‰ä»·ï¼Œç³»ç»Ÿç ”ç©¶äº†æ¨¡å‹åœ¨æ§åˆ¶ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæä¾›äº†æ¸…æ™°çš„ç†è®ºæ¡†æ¶ã€‚
3. å®éªŒè¯æ˜ï¼Œæ¨¡å¼åŒ¹é…çš„å®ä¾‹æˆåŠŸç‡ä¸è§è¯ç›¸å…³åŠŸèƒ½ç­‰ä»·çš„ä¸Šä¸‹æ–‡æ•°é‡å¯†åˆ‡ç›¸å…³ï¼Œä¸”è·¯å¾„æ¨¡ç³Šæ€§å½±å“æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å…¶æˆåŠŸå¾€å¾€ä¾èµ–äºæ¨¡å¼åŒ¹é…è¡Œä¸ºï¼Œè¿™ä¸ç»„åˆä»»åŠ¡ä¸­çš„è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰æ³›åŒ–å¤±è´¥ç›¸å…³ã€‚ç°æœ‰çš„è¡Œä¸ºç ”ç©¶å¸¸å¸¸ä½¿ç”¨å¤šç§æ³›åŒ–æ¥æºçš„ä»»åŠ¡è®¾ç½®ï¼Œæ¨¡ç³Šäº†LLMsåœ¨æ¨¡å¼åŒ¹é…ä¸­çš„è¡¨ç°åŠå…¶å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ¨¡ç³Šæ€§ï¼Œæœ¬æ–‡é¦–å…ˆå°†æ¨¡å¼åŒ¹é…å½¢å¼åŒ–ä¸ºåŠŸèƒ½ç­‰ä»·ï¼Œå³è¯†åˆ«è¾“å…¥å­åºåˆ—å¯¹åœ¨å…¶ä»–è¾“å…¥ä¿æŒä¸å˜æ—¶å§‹ç»ˆå¯¼è‡´ç›¸åŒç»“æœçš„æƒ…å†µã€‚ç„¶åï¼Œæˆ‘ä»¬ç³»ç»Ÿç ”ç©¶äº†ä»…è§£ç å™¨Transformerå’ŒMambaåœ¨æ§åˆ¶ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡çš„ç»„åˆç»“æ„éš”ç¦»äº†è¿™ä¸€æœºåˆ¶ã€‚æˆ‘ä»¬çš„å½¢å¼åŒ–æ–¹æ³•æä¾›äº†é¢„æµ‹æ€§å’Œå®šé‡çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMsåœ¨ç»„åˆä»»åŠ¡ä¸­æ¨¡å¼åŒ¹é…èƒ½åŠ›çš„è¯„ä¼°ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æ¸…æ™°åŒºåˆ†å¤šç§æ³›åŒ–æ¥æºå¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†æ¨¡å¼åŒ¹é…å½¢å¼åŒ–ä¸ºåŠŸèƒ½ç­‰ä»·ï¼Œè¯†åˆ«è¾“å…¥å­åºåˆ—å¯¹ï¼Œä»è€Œæä¾›ä¸€ä¸ªå¯é¢„æµ‹å’Œå¯éªŒè¯çš„æ¡†æ¶æ¥åˆ†ææ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨æ§åˆ¶ä»»åŠ¡çš„ç»„åˆç»“æ„ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬åŠŸèƒ½ç­‰ä»·çš„è¯†åˆ«ã€æ ·æœ¬å¤æ‚åº¦çš„ç•Œå®šä»¥åŠè·¯å¾„æ¨¡ç³Šæ€§çš„åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†åŠŸèƒ½ç­‰ä»·çš„å½¢å¼åŒ–å®šä¹‰ï¼Œæ­ç¤ºäº†æ¨¡å¼åŒ¹é…çš„å®ä¾‹æˆåŠŸç‡ä¸ä¸Šä¸‹æ–‡æ•°é‡ä¹‹é—´çš„å…³ç³»ï¼Œæä¾›äº†å¯¹è·¯å¾„æ¨¡ç³Šæ€§çš„æ·±å…¥åˆ†æã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†20å€å‚æ•°æ‰©å±•çš„è®¾ç½®ï¼ŒéªŒè¯äº†ç†è®ºé¢„æµ‹ï¼Œå¹¶é€šè¿‡ä¸åŒæ¶æ„çš„æ¯”è¾ƒï¼Œåˆ†æäº†æ ·æœ¬å¤æ‚åº¦å’Œè·¯å¾„æ¨¡ç³Šæ€§å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å¼åŒ¹é…çš„å®ä¾‹æˆåŠŸç‡ä¸è§è¯åŠŸèƒ½ç­‰ä»·çš„ä¸Šä¸‹æ–‡æ•°é‡å‘ˆæ­£ç›¸å…³ï¼Œä¸”åœ¨20å€å‚æ•°æ‰©å±•ä¸‹ï¼Œæ¨¡å‹çš„è¡¨ç°ä¸ç†è®ºé¢„æµ‹é«˜åº¦ä¸€è‡´ã€‚æ­¤å¤–ï¼Œè·¯å¾„æ¨¡ç³Šæ€§è¢«è¯æ˜æ˜¯å½±å“æ¨¡å‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§çš„å…³é”®å› ç´ ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºç†è§£å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»„åˆä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†ç†è®ºåŸºç¡€ï¼Œæ½œåœ¨åº”ç”¨åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨äººæ§åˆ¶å’Œå¤æ‚ç³»ç»Ÿå»ºæ¨¡ç­‰é¢†åŸŸã€‚é€šè¿‡æ˜ç¡®æ¨¡å¼åŒ¹é…çš„å±€é™æ€§ï¼Œæœªæ¥å¯ä»¥è®¾è®¡å‡ºæ›´å…·æ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹ï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite impressive capabilities, LLMs' successes often rely on pattern-matching behaviors, yet these are also linked to OOD generalization failures in compositional tasks. However, behavioral studies commonly employ task setups that allow multiple generalization sources (e.g., algebraic invariances, structural repetition), obscuring a precise and testable account of how well LLMs perform generalization through pattern matching and their limitations. To address this ambiguity, we first formalize pattern matching as functional equivalence, i.e., identifying pairs of subsequences of inputs that consistently lead to identical results when the rest of the input is held constant. Then, we systematically study how decoder-only Transformer and Mamba behave in controlled tasks with compositional structures that isolate this mechanism. Our formalism yields predictive and quantitative insights: (1) Instance-wise success of pattern matching is well predicted by the number of contexts witnessing the relevant functional equivalence. (2) We prove a tight sample complexity bound of learning a two-hop structure by identifying the exponent of the data scaling law for perfect in-domain generalization. Our empirical results align with the theoretical prediction, under 20x parameter scaling and across architectures. (3) Path ambiguity is a structural barrier: when a variable influences the output via multiple paths, models fail to form unified intermediate state representations, impairing accuracy and interpretability. (4) Chain-of-Thought reduces data requirements yet does not resolve path ambiguity. Hence, we provide a predictive, falsifiable boundary for pattern matching and a foundational diagnostic for disentangling mixed generalization mechanisms.

