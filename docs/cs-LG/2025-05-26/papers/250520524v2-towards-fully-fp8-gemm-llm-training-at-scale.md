---
layout: default
title: Towards Fully FP8 GEMM LLM Training at Scale
---

# Towards Fully FP8 GEMM LLM Training at Scale

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20524" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20524v2</a>
  <a href="https://arxiv.org/pdf/2505.20524.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20524v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20524v2', 'Towards Fully FP8 GEMM LLM Training at Scale')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alejandro HernÃ¡ndez-Cano, Dhia Garbaya, Imanol Schlag, Martin Jaggi

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: 19 pages including appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…¨FP8 GEMM LLMè®­ç»ƒæ¶æ„ä»¥æå‡å¤§è§„æ¨¡è®­ç»ƒæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `FP8è®­ç»ƒ` `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `çŸ©é˜µä¹˜æ³•` `å˜æ¢å™¨æ¶æ„` `ä½ç²¾åº¦è®¡ç®—` `ååé‡æå‡` `ç¨³å®šæ€§ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§è§„æ¨¡FP8è®­ç»ƒä¸­é¢ä¸´ç¨³å®šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´å…¶åº”ç”¨å—åˆ°é™åˆ¶ã€‚
2. æœ¬æ–‡æå‡ºçš„æ–°æ¶æ„é¦–æ¬¡æ”¯æŒåœ¨å˜æ¢å™¨å—å†…çš„æ‰€æœ‰GEMMè¿›è¡ŒFP8è®¡ç®—ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨ååé‡ä¸Šå®ç°äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸BF16è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡FP8æ•°æ®æ ¼å¼åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†ç”±äºåœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ä¿æŒç¨³å®šæ€§çš„æŒ‘æˆ˜ï¼Œå…¶é‡‡ç”¨å—åˆ°é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ¬¡ä¼˜çš„ç»†ç²’åº¦FP8å†…æ ¸ï¼Œæˆ–åœ¨æ•æ„Ÿç»„ä»¶ï¼ˆå¦‚æ³¨æ„åŠ›æŠ•å½±ï¼‰ä¸­å›é€€åˆ°æ›´é«˜ç²¾åº¦çš„çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰ï¼Œä»è€Œå¦¨ç¢äº†æ½œåœ¨çš„ååé‡æå‡ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºäº†ä¸€ç±»æ–°çš„LLMæ¶æ„ï¼Œæ”¯æŒåœ¨å˜æ¢å™¨å—å†…çš„æ‰€æœ‰GEMMè¿›è¡ŒFP8è®¡ç®—ï¼Œæ¶µç›–å‰å‘å’Œåå‘ä¼ æ’­ã€‚è¿™ä½¿å¾—åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­å®ç°å‰æ‰€æœªæœ‰çš„ååé‡æå‡ï¼ŒåŒæ—¶åŒ¹é…æ ‡å‡†BF16è®­ç»ƒçš„ä¸‹æ¸¸æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¶æ„è®¾è®¡å‡å°‘äº†å¤§å¹…åº¦çš„å¼‚å¸¸æ¿€æ´»ï¼Œä¿ƒè¿›äº†é•¿æœŸç¨³å®šçš„FP8è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯†åˆ«äº†ç›‘æ§ä½ç²¾åº¦è®­ç»ƒçš„å…³é”®æŒ‡æ ‡ï¼Œå¹¶é¢„æµ‹æ½œåœ¨çš„æœªæ¥å‘æ•£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³FP8æ•°æ®æ ¼å¼åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ç¨³å®šæ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç»†ç²’åº¦FP8å†…æ ¸æˆ–å›é€€åˆ°é«˜ç²¾åº¦GEMMï¼Œå¯¼è‡´ååé‡æå‡å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ¶æ„è®¾è®¡å…è®¸åœ¨å˜æ¢å™¨å—å†…çš„æ‰€æœ‰GEMMä½¿ç”¨FP8è®¡ç®—ï¼Œæ¶µç›–å‰å‘å’Œåå‘ä¼ æ’­ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å‡å°‘å¤§å¹…åº¦çš„å¼‚å¸¸æ¿€æ´»ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œä¸»è¦åŒ…æ‹¬FP8è®¡ç®—å•å…ƒã€å˜æ¢å™¨å—å’Œç›‘æ§æœºåˆ¶ã€‚FP8è®¡ç®—å•å…ƒè´Ÿè´£æ‰§è¡Œä½ç²¾åº¦çš„çŸ©é˜µä¹˜æ³•ï¼Œè€Œç›‘æ§æœºåˆ¶åˆ™ç”¨äºè·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé¦–æ¬¡å®ç°äº†å…¨FP8æ”¯æŒçš„GEMMè®¡ç®—ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­é«˜ç²¾åº¦å›é€€çš„é—®é¢˜ï¼Œä»è€Œå®ç°äº†æ›´é«˜çš„ååé‡å’Œç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬FP8çš„æ•°å€¼èŒƒå›´å’Œç²¾åº¦æ§åˆ¶ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´ç­–ç•¥ï¼Œä»¥ç¡®ä¿ä½ç²¾åº¦è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ¶æ„åœ¨ååé‡ä¸Šå®ç°äº†æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œä¸ä¼ ç»ŸBF16è®­ç»ƒç›¸æ¯”ï¼ŒFP8è®­ç»ƒåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šæå‡äº†çº¦30%çš„è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸ä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆè®¡ç®—èµ„æºçš„åœºæ™¯ä¸­ã€‚é€šè¿‡æå‡FP8è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œæœªæ¥å¯ä»¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸå®ç°æ›´å¿«é€Ÿçš„æ¨¡å‹è¿­ä»£ä¸éƒ¨ç½²ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains. We introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. In addition, we identify key metrics to monitor low-precision training and predict potential future divergences.

