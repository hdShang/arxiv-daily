---
layout: default
title: GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance
---

# GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07004" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07004v4</a>
  <a href="https://arxiv.org/pdf/2505.07004.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07004v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07004v4', 'GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinuk Kim, Marwa El Halabi, Wonpyo Park, Clemens JS Schaefer, Deokjae Lee, Yeonhong Park, Jae W. Lee, Hyun Oh Song

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-11 (æ›´æ–°: 2025-09-22)

**å¤‡æ³¨**: ICML 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/snu-mllab/GuidedQuant)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGuidedQuantä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹é‡åŒ–ä¸­çš„ç‰¹å¾é‡è¦æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åè®­ç»ƒé‡åŒ–` `å¤§è¯­è¨€æ¨¡å‹` `é‡åŒ–ç®—æ³•` `ç‰¹å¾é‡è¦æ€§` `æ¨¡å‹å‹ç¼©` `éå‡åŒ€é‡åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åè®­ç»ƒé‡åŒ–æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘éšè—ç‰¹å¾å¯¹æœ€ç»ˆæŸå¤±çš„é‡è¦æ€§å˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. GuidedQuanté€šè¿‡æ•´åˆæœ€ç»ˆæŸå¤±çš„æ¢¯åº¦ä¿¡æ¯åˆ°é‡åŒ–ç›®æ ‡ä¸­ï¼ŒåŒæ—¶ä¿æŒæƒé‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„é‡åŒ–æ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGuidedQuantåœ¨å¤šç§é‡åŒ–åœºæ™¯ä¸‹å‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åè®­ç»ƒé‡åŒ–æ˜¯å‡å°‘å¤§è¯­è¨€æ¨¡å‹å†…å­˜å’Œæ¨ç†å»¶è¿Ÿçš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆæœªèƒ½è€ƒè™‘éšè—ç‰¹å¾å¯¹æœ€ç»ˆæŸå¤±çš„é‡è¦æ€§å˜åŒ–ï¼Œè¦ä¹ˆåœ¨è€ƒè™‘æœ€ç»ˆæŸå¤±æ—¶å¿½è§†äº†æ¨¡å‹æƒé‡ä¹‹é—´çš„å…³é”®äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†GuidedQuantï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é‡åŒ–æ–¹æ³•ï¼Œå®ƒå°†æœ€ç»ˆæŸå¤±çš„æ¢¯åº¦ä¿¡æ¯æ•´åˆåˆ°é‡åŒ–ç›®æ ‡ä¸­ï¼ŒåŒæ—¶ä¿ç•™è¾“å‡ºé€šé“å†…çš„è·¨æƒé‡ä¾èµ–å…³ç³»ã€‚GuidedQuantåœ¨æƒé‡ä»…æ ‡é‡ã€æƒé‡ä»…å‘é‡å’Œæƒé‡ä¸æ¿€æ´»é‡åŒ–çš„æœ€å…ˆè¿›æ–¹æ³•ä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„éå‡åŒ€æ ‡é‡é‡åŒ–ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¿è¯é‡åŒ–ç›®æ ‡å€¼å•è°ƒä¸‹é™ï¼Œå¹¶åœ¨è¯¥ç±»åˆ«ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åè®­ç»ƒé‡åŒ–æ–¹æ³•åœ¨è€ƒè™‘ç‰¹å¾é‡è¦æ€§å’Œæƒé‡äº¤äº’æ—¶çš„ä¸è¶³ï¼Œå¯¼è‡´é‡åŒ–æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGuidedQuantçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æœ€ç»ˆæŸå¤±çš„æ¢¯åº¦ä¿¡æ¯èå…¥é‡åŒ–ç›®æ ‡ä¸­ï¼Œä»¥æ›´å¥½åœ°åæ˜ ç‰¹å¾çš„é‡è¦æ€§ï¼ŒåŒæ—¶ä¿æŒæƒé‡ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGuidedQuantçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é‡åŒ–ç›®æ ‡çš„å®šä¹‰ã€æ¢¯åº¦ä¿¡æ¯çš„æå–å’Œæƒé‡ä¾èµ–å…³ç³»çš„å»ºæ¨¡ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬é‡åŒ–ç®—æ³•å’Œéå‡åŒ€æ ‡é‡é‡åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šGuidedQuantçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºæœ€ç»ˆæŸå¤±çš„æ¢¯åº¦ä¿¡æ¯è¿›è¡Œé‡åŒ–ï¼ŒåŒæ—¶è®¾è®¡äº†éå‡åŒ€æ ‡é‡é‡åŒ–ç®—æ³•ï¼Œç¡®ä¿é‡åŒ–ç›®æ ‡å€¼å•è°ƒä¸‹é™ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒGuidedQuanté‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é‡åŒ–ç›®æ ‡ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å‚æ•°è®¾ç½®æ¥ç¡®ä¿æƒé‡ä¹‹é—´çš„ä¾èµ–å…³ç³»å¾—åˆ°æœ‰æ•ˆä¿ç•™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGuidedQuantåœ¨æƒé‡ä»…æ ‡é‡ã€æƒé‡ä»…å‘é‡å’Œæƒé‡ä¸æ¿€æ´»é‡åŒ–çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—æå‡æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œé‡åŒ–ç›®æ ‡å€¼å•è°ƒä¸‹é™ï¼Œä¸”åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰éœ€è¦é«˜æ•ˆæ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡æå‡é‡åŒ–æ€§èƒ½ï¼ŒGuidedQuantèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜å ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.

