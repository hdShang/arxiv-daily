---
layout: default
title: Turning LLM Activations Quantization-Friendly
---

# Turning LLM Activations Quantization-Friendly

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.01967" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.01967v1</a>
  <a href="https://arxiv.org/pdf/2506.01967.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.01967v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.01967v1', 'Turning LLM Activations Quantization-Friendly')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Patrik CzakÃ³, GÃ¡bor KertÃ©sz, SÃ¡ndor SzÃ©nÃ¡si

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-11

**å¤‡æ³¨**: 6 pages, 5 figures. Accepted to SACI 2025 conference proceedings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé‡åŒ–å‹å¥½çš„æ¿€æ´»æ–¹æ³•ä»¥é™ä½LLMæœåŠ¡æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼‚å¸¸å€¼å¤„ç†` `é€šé“ç¼©æ”¾` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´æ˜¾è‘—çš„é‡åŒ–è¯¯å·®ï¼Œä¸»è¦ç”±äºæ¨¡å‹ä¸­çš„å¼‚å¸¸å€¼ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–éš¾åº¦åº¦é‡æ ‡å‡†ï¼Œå¹¶å¼•å…¥é€šé“ç¼©æ”¾ä¸æ—‹è½¬ç›¸ç»“åˆçš„æ··åˆæ–¹æ³•æ¥æ”¹å–„é‡åŒ–æ•ˆæœã€‚
3. é€šè¿‡å®éªŒéªŒè¯ï¼Œæå‡ºçš„æ–¹æ³•åœ¨é‡åŒ–è¯¯å·®ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„é™ä½ï¼Œæå‡äº†æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é‡åŒ–é€šè¿‡å‹ç¼©å‚æ•°åŠ é€Ÿæ•°æ®ä¼ è¾“å¹¶åˆ©ç”¨æ•´æ•°è¿ç®—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿è¡Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œæ¿€æ´»æ•´æ•°è¿ç®—éœ€è¦å¯¹æƒé‡å’Œæ¿€æ´»è¿›è¡Œé‡åŒ–ï¼Œè¿™åœ¨LLMsä¸­ç”±äºæ˜¾è‘—çš„å¼‚å¸¸å€¼è€Œå¢åŠ äº†é‡åŒ–è¯¯å·®ã€‚æœ¬æ–‡ç ”ç©¶äº†è¿™äº›å¼‚å¸¸å€¼å¯¹å±‚çº§é‡åŒ–è¯¯å·®çš„å½±å“ï¼Œå¹¶æ¢è®¨äº†å¹³æ»‘å’Œæ—‹è½¬å¦‚ä½•è½¬å˜è§‚å¯Ÿå€¼ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬å¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†æ¥åŸºäºé€šé“å¹…åº¦æµ‹é‡å’Œå¯è§†åŒ–é‡åŒ–éš¾åº¦ï¼Œå¹¶æå‡ºäº†ä¸€ç§åœ¨æ—‹è½¬å‰åº”ç”¨é€šé“ç¼©æ”¾çš„æ··åˆæ–¹æ³•ï¼Œæ”¯æŒå…¶ç›Šå¤„çš„æ•°å­¦å…¬å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡åŒ–è¿‡ç¨‹ä¸­ç”±äºå¼‚å¸¸å€¼å¯¼è‡´çš„é‡åŒ–è¯¯å·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›å¼‚å¸¸å€¼æ—¶æ•ˆæœä¸ä½³ï¼Œå½±å“äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ–°çš„åº¦é‡æ ‡å‡†æ¥è¯„ä¼°é‡åŒ–éš¾åº¦ï¼Œå¹¶ç»“åˆé€šé“ç¼©æ”¾ä¸æ—‹è½¬çš„æ··åˆæ–¹æ³•ï¼Œä»¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ›´å¥½åœ°å¤„ç†æ¨¡å‹ä¸­çš„å¼‚å¸¸å€¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ†æå±‚çº§é‡åŒ–è¯¯å·®ï¼›å…¶æ¬¡ï¼Œåº”ç”¨é€šé“ç¼©æ”¾ä»¥å¹³æ»‘æ•°æ®ï¼›æœ€åï¼Œè¿›è¡Œæ—‹è½¬ä»¥ä¼˜åŒ–é‡åŒ–æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–éš¾åº¦åº¦é‡æ ‡å‡†ï¼Œå¹¶é€šè¿‡æ•°å­¦å…¬å¼è¯æ˜äº†é€šé“ç¼©æ”¾ä¸æ—‹è½¬ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºæ›´ç³»ç»Ÿåœ°å¤„ç†å¼‚å¸¸å€¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€šé“å¹…åº¦ä½œä¸ºé‡åŒ–éš¾åº¦çš„ä¾æ®ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–é‡åŒ–è¿‡ç¨‹ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œç»“åˆäº†é€šé“ç¼©æ”¾å’Œæ—‹è½¬çš„æ¨¡å—ï¼Œä»¥æé«˜æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ··åˆæ–¹æ³•åœ¨é‡åŒ–è¯¯å·®ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•é™ä½äº†çº¦15%ï¼ŒåŒæ—¶åœ¨æ¨¡å‹æ¨ç†é€Ÿåº¦ä¸Šæå‡äº†20%ã€‚è¿™äº›ç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–ã€‚é€šè¿‡é™ä½é‡åŒ–è¯¯å·®ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ï¼Œå‡å°‘è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦çš„ç»æµä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Quantization effectively reduces the serving costs of Large Language Models (LLMs) by speeding up data movement through compressed parameters and enabling faster operations via integer arithmetic. However, activating integer arithmetic requires quantizing both weights and activations, which poses challenges due to the significant outliers in LLMs that increase quantization error. In this work, we investigate these outliers with an emphasis on their effect on layer-wise quantization error, then examine how smoothing and rotation transform the observed values. Our primary contributions include introducing a new metric to measure and visualize quantization difficulty based on channel magnitudes, as well as proposing a hybrid approach that applies channel-wise scaling before rotation, supported by a mathematical formulation of its benefits.

