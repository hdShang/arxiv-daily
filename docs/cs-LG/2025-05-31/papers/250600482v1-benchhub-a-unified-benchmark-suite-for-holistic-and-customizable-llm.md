---
layout: default
title: BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation
---

# BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00482" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00482v1</a>
  <a href="https://arxiv.org/pdf/2506.00482.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00482v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00482v1', 'BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eunsu Kim, Haneul Yoo, Guijin Son, Hitesh Patel, Amit Agarwal, Alice Oh

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBenchHubä»¥è§£å†³LLMè¯„ä¼°åŸºå‡†åˆ†æ•£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯„ä¼°åŸºå‡†` `åŠ¨æ€åŸºå‡†åº“` `é¢†åŸŸç‰¹å®šè¯„ä¼°` `æ•°æ®é›†æ•´åˆ` `æ¨¡å‹æ¯”è¾ƒ` `å¯æ‰©å±•æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMè¯„ä¼°åŸºå‡†åˆ†æ•£ä¸”éš¾ä»¥ç®¡ç†ï¼Œæ— æ³•æ»¡è¶³ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°éœ€æ±‚ã€‚
2. BenchHubæ˜¯ä¸€ä¸ªåŠ¨æ€çš„åŸºå‡†åº“ï¼Œæ•´åˆäº†æ¥è‡ªå¤šä¸ªé¢†åŸŸçš„è¯„ä¼°æ•°æ®é›†ï¼Œæ”¯æŒçµæ´»çš„å®šåˆ¶åŒ–è¯„ä¼°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨é¢†åŸŸç‰¹å®šå­é›†ä¸Šçš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œå¼ºè°ƒäº†é¢†åŸŸæ„ŸçŸ¥åŸºå‡†çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸æ–­å‘å±•ï¼ŒåŠæ—¶ä¸”æœ‰ç»„ç»‡çš„åŸºå‡†è¯„ä¼°å˜å¾—æ„ˆå‘é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†åˆ†æ•£ä¸”éš¾ä»¥ç®¡ç†ï¼Œå¯¼è‡´åœ¨ç‰¹å®šéœ€æ±‚æˆ–é¢†åŸŸä¸‹è¿›è¡Œè¯„ä¼°æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†BenchHubï¼Œä¸€ä¸ªåŠ¨æ€åŸºå‡†åº“ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´æœ‰æ•ˆåœ°è¯„ä¼°LLMsã€‚BenchHubæ•´åˆå¹¶è‡ªåŠ¨åˆ†ç±»æ¥è‡ªä¸åŒé¢†åŸŸçš„åŸºå‡†æ•°æ®é›†ï¼Œé›†æˆäº†38ä¸ªåŸºå‡†ä¸­çš„303Kä¸ªé—®é¢˜ã€‚è¯¥ç³»ç»Ÿæ”¯æŒæŒç»­æ›´æ–°å’Œå¯æ‰©å±•çš„æ•°æ®ç®¡ç†ï¼Œèƒ½å¤Ÿçµæ´»å®šåˆ¶è¯„ä¼°ä»¥é€‚åº”å„ç§é¢†åŸŸæˆ–ç”¨ä¾‹ã€‚é€šè¿‡å¯¹ä¸åŒLLMå®¶æ—çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ¨¡å‹æ€§èƒ½åœ¨é¢†åŸŸç‰¹å®šå­é›†ä¸­çš„æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†é¢†åŸŸæ„ŸçŸ¥åŸºå‡†çš„é‡è¦æ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡BenchHubèƒ½å¤Ÿä¿ƒè¿›æ•°æ®é›†çš„é‡ç”¨ã€æ¨¡å‹æ¯”è¾ƒçš„é€æ˜æ€§ï¼Œå¹¶æ›´å®¹æ˜“è¯†åˆ«ç°æœ‰åŸºå‡†ä¸­çš„ä¸è¶³ä¹‹å¤„ï¼Œä¸ºLLMè¯„ä¼°ç ”ç©¶æä¾›å…³é”®åŸºç¡€è®¾æ–½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMè¯„ä¼°åŸºå‡†åˆ†æ•£ã€éš¾ä»¥ç®¡ç†çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨ç‰¹å®šé¢†åŸŸè¿›è¡Œè¯„ä¼°æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBenchHubé€šè¿‡æ•´åˆå’Œè‡ªåŠ¨åˆ†ç±»æ¥è‡ªä¸åŒé¢†åŸŸçš„åŸºå‡†æ•°æ®é›†ï¼Œæä¾›ä¸€ä¸ªåŠ¨æ€çš„è¯„ä¼°å¹³å°ï¼Œæ”¯æŒçµæ´»çš„å®šåˆ¶åŒ–è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBenchHubçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†èšåˆæ¨¡å—ã€è‡ªåŠ¨åˆ†ç±»æ¨¡å—å’Œç”¨æˆ·è‡ªå®šä¹‰è¯„ä¼°æ¨¡å—ï¼Œæ”¯æŒæŒç»­æ›´æ–°å’Œå¯æ‰©å±•çš„æ•°æ®ç®¡ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šBenchHubçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€æ›´æ–°èƒ½åŠ›å’Œé¢†åŸŸç‰¹å®šçš„è¯„ä¼°æ”¯æŒï¼Œä¸ç°æœ‰é™æ€åŸºå‡†ç›¸æ¯”ï¼Œæä¾›äº†æ›´çµæ´»å’Œé«˜æ•ˆçš„è¯„ä¼°æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒBenchHubé‡‡ç”¨äº†è‡ªåŠ¨åˆ†ç±»ç®—æ³•æ¥å¤„ç†æ•°æ®é›†ï¼Œå¹¶å…è®¸ç”¨æˆ·æ ¹æ®éœ€æ±‚è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†å’ŒæŒ‡æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBenchHubåœ¨å¤šä¸ªé¢†åŸŸçš„è¯„ä¼°ä¸­ï¼Œæ¨¡å‹æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†é¢†åŸŸæ„ŸçŸ¥åŸºå‡†çš„é‡è¦æ€§ã€‚é€šè¿‡ä¸ä¼ ç»ŸåŸºå‡†çš„å¯¹æ¯”ï¼ŒBenchHubåœ¨çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä¸Šè¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œæ”¯æŒæ›´ä¸ºç²¾å‡†çš„æ¨¡å‹è¯„ä¼°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BenchHubçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ç­‰å¤šä¸ªé¢†åŸŸï¼Œèƒ½å¤Ÿä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªé«˜æ•ˆçš„è¯„ä¼°å·¥å…·ï¼Œä¿ƒè¿›æ¨¡å‹çš„å¼€å‘å’Œä¼˜åŒ–ã€‚æœªæ¥ï¼ŒBenchHubå¯èƒ½æˆä¸ºLLMè¯„ä¼°çš„æ ‡å‡†å¹³å°ï¼Œæ¨åŠ¨ç›¸å…³ç ”ç©¶çš„æ·±å…¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.

