---
layout: default
title: Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models
---

# Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00653" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00653v3</a>
  <a href="https://arxiv.org/pdf/2506.00653.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00653v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00653v3', 'Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Femi Bello, Anubrata Das, Fanzhi Zeng, Fangcong Yin, Liu Leqi

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-06-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºçº¿æ€§è¡¨ç¤ºå¯è½¬ç§»å‡è®¾ä»¥å¼•å¯¼å¤§æ¨¡å‹è¡Œä¸º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¡¨ç¤ºå­¦ä¹ ` `çŸ¥è¯†è½¬ç§»` `ä»¿å°„å˜æ¢` `æ¨¡å‹è’¸é¦` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¹‹é—´ç¼ºä¹æœ‰æ•ˆçš„è¡¨ç¤ºå¯¹é½æœºåˆ¶ï¼Œå¯¼è‡´å°æ¨¡å‹çš„çŸ¥è¯†æ— æ³•æœ‰æ•ˆè½¬ç§»åˆ°å¤§æ¨¡å‹ã€‚
2. è®ºæ–‡æå‡ºçº¿æ€§è¡¨ç¤ºå¯è½¬ç§»å‡è®¾ï¼Œè®¤ä¸ºä¸åŒæ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ä¹‹é—´å­˜åœ¨ä»¿å°„å˜æ¢ï¼Œä»è€Œå®ç°å°æ¨¡å‹å¯¹å¤§æ¨¡å‹çš„å¼•å¯¼ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå­¦ä¹ åˆ°çš„ä»¿å°„æ˜ å°„èƒ½å¤Ÿæœ‰æ•ˆä¿æŒå¼•å¯¼å‘é‡çš„è¯­ä¹‰æ•ˆæœï¼Œæ”¯æŒäº†LRTå‡è®¾çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å‡è®¾ç›¸ä¼¼æ¶æ„çš„ç¥ç»ç½‘ç»œåœ¨ç›¸ä¼¼æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°ä¸å­¦ä¹ ä»»åŠ¡ç›¸å…³çš„å…±äº«è¡¨ç¤ºã€‚æˆ‘ä»¬æ‰©å±•äº†è¿™ä¸€æ¦‚å¿µæ¡†æ¶ï¼Œæå‡ºäº†çº¿æ€§è¡¨ç¤ºå¯è½¬ç§»ï¼ˆLRTï¼‰å‡è®¾ï¼Œå³ä¸åŒæ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ä¹‹é—´å­˜åœ¨ä»¿å°„å˜æ¢ã€‚é€šè¿‡å­¦ä¹ ä¸åŒè§„æ¨¡æ¨¡å‹çš„éšè—çŠ¶æ€ä¹‹é—´çš„ä»¿å°„æ˜ å°„ï¼Œæˆ‘ä»¬éªŒè¯äº†å°æ¨¡å‹çš„å¼•å¯¼å‘é‡åœ¨è½¬ç§»åˆ°å¤§æ¨¡å‹æ—¶èƒ½å¤Ÿä¿æŒå…¶è¯­ä¹‰æ•ˆæœã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå°æ¨¡å‹å­¦ä¹ çš„è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆå¼•å¯¼å¤§æ¨¡å‹çš„è¡Œä¸ºï¼Œå¹¶ä¸ºç†è§£ä¸åŒè§„æ¨¡æ¨¡å‹ä¹‹é—´çš„è¡¨ç¤ºå¯¹é½æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸åŒè§„æ¨¡æ¨¡å‹ä¹‹é—´è¡¨ç¤ºå¯¹é½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨å°æ¨¡å‹çš„çŸ¥è¯†æ¥æŒ‡å¯¼å¤§æ¨¡å‹çš„è¡Œä¸ºï¼Œå¯¼è‡´çŸ¥è¯†è½¬ç§»æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçº¿æ€§è¡¨ç¤ºå¯è½¬ç§»å‡è®¾ï¼Œè®¤ä¸ºä¸åŒæ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ä¹‹é—´å­˜åœ¨ä»¿å°„å˜æ¢ã€‚é€šè¿‡å­¦ä¹ è¿™äº›å˜æ¢ï¼Œå¯ä»¥å°†å°æ¨¡å‹çš„å¼•å¯¼å‘é‡æœ‰æ•ˆè½¬ç§»åˆ°å¤§æ¨¡å‹ä¸­ï¼Œä»è€Œå®ç°çŸ¥è¯†çš„æœ‰æ•ˆåˆ©ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹è®­ç»ƒã€éšè—çŠ¶æ€æå–ã€ä»¿å°„æ˜ å°„å­¦ä¹ å’Œå¼•å¯¼å‘é‡è½¬ç§»å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆè®­ç»ƒä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œç„¶åæå–å…¶éšè—çŠ¶æ€ï¼Œæ¥ç€å­¦ä¹ éšè—çŠ¶æ€ä¹‹é—´çš„ä»¿å°„æ˜ å°„ï¼Œæœ€åéªŒè¯å¼•å¯¼å‘é‡çš„è½¬ç§»æ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†çº¿æ€§è¡¨ç¤ºå¯è½¬ç§»å‡è®¾ï¼Œå¹¶é€šè¿‡å®éªŒè¯å®äº†å°æ¨¡å‹çš„å¼•å¯¼å‘é‡åœ¨å¤§æ¨¡å‹ä¸­ä¿æŒè¯­ä¹‰æ•ˆæœçš„èƒ½åŠ›ã€‚è¿™ä¸€åˆ›æ–°ä¸ºæ¨¡å‹é—´çš„çŸ¥è¯†è½¬ç§»æä¾›äº†æ–°çš„ç†è®ºåŸºç¡€ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ä»¿å°„æ˜ å°„çš„å­¦ä¹ ï¼Œå¹¶è®¾è®¡äº†é€‚åº”ä¸åŒè§„æ¨¡æ¨¡å‹çš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿å¼•å¯¼å‘é‡çš„æœ‰æ•ˆè½¬ç§»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå­¦ä¹ åˆ°çš„ä»¿å°„æ˜ å°„èƒ½å¤Ÿæœ‰æ•ˆä¿æŒå¼•å¯¼å‘é‡çš„è¯­ä¹‰æ•ˆæœï¼Œæ”¯æŒäº†LRTå‡è®¾çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œè½¬ç§»åçš„å¼•å¯¼å‘é‡åœ¨å¤§æ¨¡å‹ä¸­çš„è¡¨ç°ä¸å°æ¨¡å‹ç›¸ä¼¼ï¼ŒéªŒè¯äº†å°æ¨¡å‹çŸ¥è¯†è½¬ç§»çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰éœ€è¦æ¨¡å‹çŸ¥è¯†è½¬ç§»çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨å°æ¨¡å‹çš„çŸ¥è¯†ï¼Œå¯ä»¥åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹æå‡å¤§æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.

