---
layout: default
title: FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts
---

# FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00495" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00495v1</a>
  <a href="https://arxiv.org/pdf/2506.00495.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00495v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00495v1', 'FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyi Wang, Lirong Gao, Haobo Wang, Yiming Zhang, Junbo Zhao

**åˆ†ç±»**: cs.LG, cs.CL, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: 17 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFLoEä»¥è§£å†³ä½ç§©ä¸“å®¶é€‚åº”ä¸­çš„å±‚é€‰æ‹©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ä½ç§©é€‚åº”` `Fisherä¿¡æ¯` `è´å¶æ–¯ä¼˜åŒ–` `ç¨€ç–é€‚é…å™¨` `å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹` `å±‚é€‰æ‹©` `æ¨¡å‹é€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰PEFTæ–¹æ³•åœ¨æ‰€æœ‰å±‚ä¸­å‡åŒ€éƒ¨ç½²LoRAé€‚é…å™¨ï¼Œå¿½è§†äº†å±‚è´¡çŒ®çš„å¼‚è´¨æ€§ï¼Œå¯¼è‡´å†—ä½™å‚æ•°å’Œæ¬¡ä¼˜é€‚åº”æ•ˆç‡ã€‚
2. FLoEå¼•å…¥äº†åŸºäºFisherä¿¡æ¯çš„å±‚é‡è¦æ€§è¯„åˆ†æœºåˆ¶å’Œè´å¶æ–¯ä¼˜åŒ–çš„ç§©åˆ†é…å™¨ï¼Œä»¥å®ç°ç¨€ç–é€‚é…å™¨çš„åŠ¨æ€éƒ¨ç½²å’Œæœ€ä½³ç§©é€‰æ‹©ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFLoEåœ¨å¤šä¸ªLLMså’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†é€‚åº”æ•ˆç‡ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å·²æˆä¸ºå°†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”äºä¸‹æ¸¸ä»»åŠ¡çš„å¹¿æ³›é‡‡ç”¨ç­–ç•¥ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰PEFTæŠ€æœ¯åœ¨æ‰€æœ‰å±‚ä¸­å‡åŒ€éƒ¨ç½²LoRAé€‚é…å™¨ï¼Œå¿½è§†äº†å±‚è´¡çŒ®çš„å†…åœ¨å¼‚è´¨æ€§å’Œä»»åŠ¡ç‰¹å®šçš„ç§©è¦æ±‚ã€‚è¿™ç§å‡åŒ€çš„èŒƒå¼å¯¼è‡´äº†å†—ä½™çš„å‚æ•°åˆ†é…å’Œæ¬¡ä¼˜çš„é€‚åº”æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FLoEï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„PEFTæ¡†æ¶ï¼Œä»‹ç»äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šï¼ˆiï¼‰åŸºäºFisherä¿¡æ¯çš„å±‚é‡è¦æ€§è¯„åˆ†æœºåˆ¶ï¼ŒåŠ¨æ€è¯†åˆ«ä»»åŠ¡å…³é”®çš„å˜æ¢å±‚ä»¥å®ç°åŸºäºMoEçš„ä½ç§©é€‚åº”ï¼Œæ”¯æŒç¨€ç–é€‚é…å™¨çš„éƒ¨ç½²ï¼›ï¼ˆiiï¼‰åŸºäºè´å¶æ–¯ä¼˜åŒ–çš„ç§©åˆ†é…å™¨ï¼Œè‡ªåŠ¨ç¡®å®šç‰¹å®šæ•°æ®é›†ä¸Šçš„æœ€ä½³LoRAç§©ï¼Œè€Œæ— éœ€è€—æ—¶çš„ç½‘æ ¼æœç´¢ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒFLoEåœ¨ä¸åŒçš„LLMså’ŒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆç‡-å‡†ç¡®æ€§æƒè¡¡ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦å¿«é€Ÿé€‚åº”çš„èµ„æºå—é™ç¯å¢ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰PEFTæ–¹æ³•åœ¨å±‚é€‰æ‹©ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å‡åŒ€éƒ¨ç½²LoRAé€‚é…å™¨å¯¼è‡´çš„å†—ä½™å‚æ•°å’Œé€‚åº”æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFLoEé€šè¿‡å¼•å…¥Fisherä¿¡æ¯æŒ‡å¯¼çš„å±‚é‡è¦æ€§è¯„åˆ†æœºåˆ¶ï¼ŒåŠ¨æ€è¯†åˆ«å…³é”®å±‚ï¼Œå¹¶ç»“åˆè´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨ç¡®å®šæœ€ä½³LoRAç§©ï¼Œä»è€Œæé«˜é€‚åº”æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFLoEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯åŸºäºFisherä¿¡æ¯çš„å±‚é‡è¦æ€§è¯„åˆ†æœºåˆ¶ï¼ŒäºŒæ˜¯è´å¶æ–¯ä¼˜åŒ–é©±åŠ¨çš„ç§©åˆ†é…å™¨ï¼ŒäºŒè€…ååŒå·¥ä½œä»¥å®ç°é«˜æ•ˆçš„ç¨€ç–é€‚é…å™¨éƒ¨ç½²ã€‚

**å…³é”®åˆ›æ–°**ï¼šFLoEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŠ¨æ€å±‚é€‰æ‹©å’Œç§©åˆ†é…æœºåˆ¶ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å‡åŒ€é€‚é…å™¨éƒ¨ç½²ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚çµæ´»è°ƒæ•´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒFLoEé‡‡ç”¨äº†åŸºäºFisherä¿¡æ¯çš„è¯„åˆ†æ–¹æ³•æ¥è¯„ä¼°å±‚çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡è´å¶æ–¯ä¼˜åŒ–ç®—æ³•è‡ªåŠ¨é€‰æ‹©é€‚åˆç‰¹å®šæ•°æ®é›†çš„LoRAç§©ï¼Œé¿å…äº†ç¹ççš„ç½‘æ ¼æœç´¢è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šé¡¹å®éªŒä¸­ï¼ŒFLoEåœ¨ä¸åŒçš„LLMså’ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»ŸPEFTæ–¹æ³•ï¼Œæ•ˆç‡æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FLoEçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ï¼Œèƒ½å¤Ÿå¿«é€Ÿé€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒFLoEçš„æ¡†æ¶ä¹Ÿå¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æ¨¡å‹é€‚åº”å’Œä¼˜åŒ–åœºæ™¯ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely adopted strategy for adapting pre-trained Large Language Models (LLMs) to downstream tasks, significantly reducing memory and computational costs. However, most existing PEFT techniques uniformly deploy LoRA adapters across all layers, disregarding the intrinsic heterogeneity of layer contributions and task-specific rank requirements. This uniform paradigm leads to redundant parameter allocation and suboptimal adaptation efficiency. To address these limitations, we propose FLoE, a novel PEFT framework that introduces two key innovations: (i) a Fisher information-guided importance scoring mechanism to dynamically identify task-critical transformer layers for MoE-based low-rank adaptation, enabling sparse adapter deployment; and (ii) a Bayesian optimization-driven rank allocator that automatically determines optimal LoRA ranks on specific datasets without exhaustive grid search. Extensive experiments across diverse LLMs and benchmarks reveal that FLoE achieves impressive efficiency-accuracy trade-offs, making FLoE particularly advantageous in resource-constrained environments that necessitate rapid adaptation.

