---
layout: default
title: MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning
---

# MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00555" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00555v2</a>
  <a href="https://arxiv.org/pdf/2506.00555.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00555v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00555v2', 'MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie Liu, Yan Lu, Huaxiu Yao

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-06-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMedAgent-RLä»¥è§£å†³å¤šæ¨¡æ€åŒ»ç–—æ¨ç†ä¸­çš„åä½œé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŒ»ç–—æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šä»£ç†åä½œ` `åŒ»ç–—å†³ç­–æ”¯æŒ` `è¯¾ç¨‹å­¦ä¹ ` `åˆ†è¯Šç³»ç»Ÿ` `æ™ºèƒ½åŒ»ç–—åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å•ä¸€ä»£ç†æ¨¡å‹åœ¨åŒ»ç–—ä¸“ä¸šä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. æå‡ºMMedAgent-RLï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°åŒ»ç–—ä»£ç†çš„åŠ¨æ€ä¼˜åŒ–åä½œï¼Œæå‡æ¨ç†èƒ½åŠ›ã€‚
3. åœ¨äº”ä¸ªåŒ»ç–—VQAåŸºå‡†ä¸Šï¼ŒMMedAgent-RLè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰Med-LVLMsï¼Œå¹³å‡æ€§èƒ½æå‡20.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŒ»ç–—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-LVLMsï¼‰åœ¨å¤šæ¨¡æ€è¯Šæ–­ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å•ä¸€ä»£ç†æ¨¡å‹åœ¨ä¸åŒåŒ»ç–—ä¸“ä¸šä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MMedAgent-RLï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šä»£ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°åŒ»ç–—ä»£ç†ä¹‹é—´çš„åŠ¨æ€ä¼˜åŒ–åä½œã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ä¸ªåŸºäºQwen2.5-VLçš„GPä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£æ‚£è€…åˆ†è¯Šå’Œæœ€ç»ˆå†³ç­–ã€‚é€šè¿‡å¼•å…¥è¯¾ç¨‹å­¦ä¹ æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥æ•™å¯¼ä¸»æ²»åŒ»ç”Ÿåœ¨æ¨¡ä»¿ä¸“å®¶ä¸çº æ­£é”™è¯¯ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMedAgent-RLåœ¨äº”ä¸ªåŒ»ç–—VQAåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æ€§èƒ½æå‡è¾¾åˆ°20.7%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å•ä¸€ä»£ç†æ¨¡å‹åœ¨å¤šæ¨¡æ€åŒ»ç–—æ¨ç†ä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´çš„æ€§èƒ½é™åˆ¶ã€‚ç°æœ‰çš„å¤šä»£ç†åä½œæ¡†æ¶ç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å¤æ‚çš„åŒ»ç–—å†³ç­–åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMMedAgent-RLé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°åŒ»ç–—ä»£ç†ä¹‹é—´çš„åŠ¨æ€åä½œï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è®­ç»ƒåˆ†è¯ŠåŒ»ç”Ÿå’Œä¸»æ²»åŒ»ç”Ÿï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒåŒ»ç–—ä¸“ä¸šä¹‹é—´é«˜æ•ˆäº’åŠ¨ï¼Œä»è€Œæå‡æ•´ä½“å†³ç­–è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåˆ†è¯ŠåŒ»ç”Ÿè´Ÿè´£å°†æ‚£è€…åˆ†é…ç»™åˆé€‚çš„ä¸“ä¸šï¼Œè€Œä¸»æ²»åŒ»ç”Ÿåˆ™æ•´åˆå¤šä½ä¸“å®¶çš„åˆ¤æ–­ä¸è‡ªèº«çŸ¥è¯†è¿›è¡Œæœ€ç»ˆå†³ç­–ã€‚é€šè¿‡è¯¾ç¨‹å­¦ä¹ æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä¸»æ²»åŒ»ç”Ÿé€æ­¥å­¦ä¹ å¦‚ä½•åœ¨æ¨¡ä»¿ä¸“å®¶ä¸çº æ­£é”™è¯¯ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è¯¾ç¨‹å­¦ä¹ æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä½¿å¾—ä¸»æ²»åŒ»ç”Ÿèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­çµæ´»è°ƒæ•´å†³ç­–ç­–ç•¥ï¼Œä»è€Œå…‹æœä¸“å®¶è¾“å‡ºä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŸºäºQwen2.5-VLçš„æ¨¡å‹æ¶æ„ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥å¹³è¡¡æ¨¡ä»¿ä¸“å®¶ä¸çº æ­£é”™è¯¯çš„å­¦ä¹ è¿‡ç¨‹ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œåˆ†è¯ŠåŒ»ç”Ÿå’Œä¸»æ²»åŒ»ç”Ÿçš„äº¤äº’æœºåˆ¶ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ä¸æ•´åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMMedAgent-RLåœ¨äº”ä¸ªåŒ»ç–—VQAåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰Med-LVLMsï¼Œå¹³å‡æ€§èƒ½æå‡è¾¾åˆ°20.7%ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æå‡ï¼Œè¿˜å±•ç°å‡ºæ›´æ¥è¿‘äººç±»çš„æ¨ç†æ¨¡å¼ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—è¯Šæ–­æ”¯æŒç³»ç»Ÿã€æ™ºèƒ½åŒ»ç–—åŠ©æ‰‹å’Œä¸´åºŠå†³ç­–æ”¯æŒå·¥å…·ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€åŒ»ç–—æ¨ç†çš„åä½œèƒ½åŠ›ï¼ŒMMedAgent-RLèƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´é«˜æ•ˆåœ°è¿›è¡Œè¯Šæ–­å’Œæ²»ç–—å†³ç­–ï¼Œæœ€ç»ˆæé«˜æ‚£è€…çš„åŒ»ç–—æœåŠ¡è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åŒ»ç–—åœºæ™¯ä¸­æ¨å¹¿åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½åŒ»ç–—çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 20.7% over supervised fine-tuning baselines.

