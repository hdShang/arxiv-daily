---
layout: default
title: Existing Large Language Model Unlearning Evaluations Are Inconclusive
---

# Existing Large Language Model Unlearning Evaluations Are Inconclusive

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00688" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00688v1</a>
  <a href="https://arxiv.org/pdf/2506.00688.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00688v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00688v1', 'Existing Large Language Model Unlearning Evaluations Are Inconclusive')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhili Feng, Yixuan Even Xu, Alexander Robey, Robert Kirk, Xander Davies, Yarin Gal, Avi Schwarzschild, J. Zico Kolter

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–°è¯„ä¼°åŸåˆ™ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å»å­¦ä¹ è¯„ä¼°ä¸ç¡®å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨å»å­¦ä¹ ` `è¯„ä¼°æ–¹æ³•` `æ•°æ®éšç§` `æ¨¡å‹å®‰å…¨` `ä¿¡æ¯æ³¨å…¥` `ä¸‹æ¸¸ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å»å­¦ä¹ è¯„ä¼°æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œå¯èƒ½å¯¼è‡´å¯¹å»å­¦ä¹ æ•ˆæœçš„è¯¯è§£ã€‚
2. è®ºæ–‡æå‡ºæœ€å°ä¿¡æ¯æ³¨å…¥å’Œä¸‹æ¸¸ä»»åŠ¡æ„è¯†ä¸¤ä¸ªåŸåˆ™ï¼Œä»¥æ”¹å–„å»å­¦ä¹ è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
3. é€šè¿‡å®éªŒéªŒè¯ï¼Œå‘ç°è¿åè¿™äº›åŸåˆ™ä¼šå¯¼è‡´è¯¯å¯¼æ€§ç»“è®ºï¼Œå¼ºè°ƒäº†æ–°è¯„ä¼°æ–¹æ³•çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨å»å­¦ä¹ æ—¨åœ¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç§»é™¤æ•æ„Ÿæˆ–ä¸å¿…è¦çš„æ•°æ®ã€‚ç„¶è€Œï¼Œè¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå»å­¦ä¹ çš„æ•ˆæœå¾€å¾€è¾ƒä¸ºè¡¨é¢ï¼Œç§»é™¤çš„çŸ¥è¯†å¯èƒ½å®¹æ˜“è¢«æ¢å¤ã€‚æœ¬æ–‡æ‰¹åˆ¤æ€§åœ°å®¡è§†äº†ç°æœ‰çš„å»å­¦ä¹ è¯„ä¼°å®è·µï¼Œæ­ç¤ºäº†å…³é”®çš„å±€é™æ€§ï¼Œå½±å“äº†æˆ‘ä»¬å¯¹è¿™äº›ç ”ç©¶ç»“æœçš„ä¿¡ä»»ã€‚æˆ‘ä»¬å‘ç°ï¼ŒæŸäº›è¯„ä¼°åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­å¼•å…¥äº†å¤§é‡æ–°ä¿¡æ¯ï¼Œå¯èƒ½æ©ç›–çœŸå®çš„å»å­¦ä¹ è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯„ä¼°ç»“æœåœ¨ä¸åŒä»»åŠ¡é—´æ˜¾è‘—å˜åŒ–ï¼Œå‰Šå¼±äº†å½“å‰è¯„ä¼°ç¨‹åºçš„æ™®é€‚æ€§ã€‚æœ€åï¼Œè®¸å¤šè¯„ä¼°ä¾èµ–äºè™šå‡çš„ç›¸å…³æ€§ï¼Œä½¿å¾—ç»“æœéš¾ä»¥ä¿¡ä»»å’Œè§£é‡Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æœªæ¥å»å­¦ä¹ è¯„ä¼°çš„ä¸¤ä¸ªåŸåˆ™ï¼šæœ€å°ä¿¡æ¯æ³¨å…¥å’Œä¸‹æ¸¸ä»»åŠ¡æ„è¯†ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—é’ˆå¯¹æ€§å®éªŒéªŒè¯äº†è¿™äº›åŸåˆ™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å»å­¦ä¹ è¯„ä¼°æ–¹æ³•çš„ä¸ç¡®å®šæ€§å’Œå±€é™æ€§ï¼ŒæŒ‡å‡ºè¿™äº›æ–¹æ³•å¯èƒ½ä¼šå¤¸å¤§æˆ–ä½ä¼°å»å­¦ä¹ çš„æˆåŠŸç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæœ€å°ä¿¡æ¯æ³¨å…¥å’Œä¸‹æ¸¸ä»»åŠ¡æ„è¯†çš„åŸåˆ™ï¼Œä»¥ç¡®ä¿è¯„ä¼°è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œé¿å…åœ¨æµ‹è¯•ä¸­å¼•å…¥æ–°ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶è®¾è®¡äº†ä¸€ç³»åˆ—å®éªŒï¼Œåˆ†åˆ«æµ‹è¯•ä¸åŒè¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåˆ†æå…¶å¯¹å»å­¦ä¹ æ•ˆæœçš„å½±å“ï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„å¯è§£é‡Šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†æ–°çš„è¯„ä¼°åŸåˆ™ï¼Œå¼ºè°ƒäº†ä¿¡æ¯æ³¨å…¥çš„æ§åˆ¶å’Œä»»åŠ¡ç›¸å…³æ€§çš„è€ƒè™‘ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è¯„ä¼°æ ‡å‡†æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„ä¿¡æ¯æ³¨å…¥é‡å’Œä»»åŠ¡ç±»å‹ï¼Œä½¿ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡å»å­¦ä¹ çš„æ•ˆæœï¼Œç¡®ä¿ç»“æœçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¿åæœ€å°ä¿¡æ¯æ³¨å…¥åŸåˆ™çš„è¯„ä¼°æ–¹æ³•å¯¼è‡´å»å­¦ä¹ æ•ˆæœçš„è¯¯åˆ¤ï¼Œä¸”åœ¨ä¸åŒä»»åŠ¡ä¸‹è¯„ä¼°ç»“æœå·®å¼‚æ˜¾è‘—ï¼Œå¼ºè°ƒäº†æ–°è¯„ä¼°åŸåˆ™çš„é‡è¦æ€§ã€‚å…·ä½“å®éªŒä¸­ï¼ŒæŸäº›è¯„ä¼°æ–¹æ³•çš„å»å­¦ä¹ æˆåŠŸç‡è¢«é«˜ä¼°äº†20%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®éšç§ä¿æŠ¤ã€æœºå™¨å­¦ä¹ æ¨¡å‹çš„åˆè§„æ€§ä»¥åŠå®‰å…¨æ€§è¯„ä¼°ã€‚é€šè¿‡æ”¹è¿›å»å­¦ä¹ è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°ç®¡ç†å’Œç§»é™¤æ•æ„Ÿæ•°æ®ï¼Œæå‡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œå®é™…å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.

