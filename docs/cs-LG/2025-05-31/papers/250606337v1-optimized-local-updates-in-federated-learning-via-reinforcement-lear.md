---
layout: default
title: Optimized Local Updates in Federated Learning via Reinforcement Learning
---

# Optimized Local Updates in Federated Learning via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.06337" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.06337v1</a>
  <a href="https://arxiv.org/pdf/2506.06337.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.06337v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.06337v1', 'Optimized Local Updates in Federated Learning via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ali Murad, Bo Hui, Wei-Shinn Ku

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: This paper is accepted at IEEE IJCNN 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/amuraddd/optimized_client_training.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è”é‚¦å­¦ä¹ ä¸­çš„æœ¬åœ°æ›´æ–°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `éIIDæ•°æ®` `æ¨¡å‹èšåˆ` `å®¢æˆ·ç«¯è®­ç»ƒ` `æ•°æ®éšç§` `ä¼˜åŒ–ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹éIIDæ•°æ®æ—¶ï¼Œé›†ä¸­èšåˆå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä¸”å®¢æˆ·ç«¯è¿‡åº¦è®­ç»ƒå¹¶ä¸ä¼šæå‡æ•´ä½“æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†æ¥ä¼˜åŒ–å®¢æˆ·ç«¯æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡ï¼Œä»è€Œé¿å…ä¿¡æ¯è¿‡åº¦å…±äº«ã€‚
3. é€šè¿‡å¤§é‡å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†FLå®¢æˆ·ç«¯çš„æ€§èƒ½ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§åˆ†å¸ƒå¼æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åä½œæ¨¡å‹è®­ç»ƒæ¥å¤„ç†å¤§è§„æ¨¡åˆ†å¸ƒå¼æ•°æ®ï¼ŒåŒæ—¶ä¿æŒå®¢æˆ·ç«¯æ•°æ®éšç§ã€‚ç„¶è€Œï¼Œåœ¨ä¸åŒå®¢æˆ·ç«¯å­˜åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé›†ä¸­æœåŠ¡å™¨çš„æ¨¡å‹èšåˆå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ä»£ç†é€‰æ‹©å¿…è¦çš„è®­ç»ƒæ•°æ®é‡ï¼Œä»¥ä¼˜åŒ–å®¢æˆ·ç«¯æ¨¡å‹çš„è®­ç»ƒï¼Œè€Œä¸å‘æœåŠ¡å™¨è¿‡åº¦å…±äº«ä¿¡æ¯ã€‚DRLä»£ç†é€šè¿‡è®­ç»ƒæŸå¤±çš„å˜åŒ–ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå­¦ä¹ ä¼˜åŒ–è®­ç»ƒæ•°æ®çš„ä½¿ç”¨ï¼Œè¿›è€Œåœ¨æ¯æ¬¡èšåˆåè¾“å‡ºä¼˜åŒ–çš„æƒé‡ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥ç®—æ³•è®­ç»ƒçš„FLå®¢æˆ·ç«¯åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’ŒFLæ¡†æ¶ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ ä¸­ç”±äºéIIDæ•°æ®å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å®¢æˆ·ç«¯è®­ç»ƒæ—¶æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ•°æ®ï¼Œé€ æˆèµ„æºæµªè´¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼ŒåŠ¨æ€é€‰æ‹©æ¯ä¸ªå®¢æˆ·ç«¯æ‰€éœ€çš„è®­ç»ƒæ•°æ®é‡ï¼Œä¼˜åŒ–æœ¬åœ°è®­ç»ƒè¿‡ç¨‹ï¼Œå‡å°‘ä¿¡æ¯å†—ä½™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆï¼ŒDRLä»£ç†åœ¨æ¯è½®èšåˆåè¯„ä¼°å½“å‰å®¢æˆ·ç«¯çš„æ€§èƒ½ï¼›å…¶æ¬¡ï¼ŒåŸºäºè®­ç»ƒæŸå¤±å˜åŒ–ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè¾“å‡ºä¼˜åŒ–çš„è®­ç»ƒæ•°æ®æƒé‡ï¼›æœ€åï¼Œå®¢æˆ·ç«¯åˆ©ç”¨æ•´ä¸ªæœ¬åœ°æ•°æ®é›†è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å¼ºåŒ–å­¦ä¹ åŠ¨æ€è°ƒæ•´è®­ç»ƒæ•°æ®çš„ä½¿ç”¨ç­–ç•¥ï¼Œæ˜¾è‘—æ”¹å–„äº†å®¢æˆ·ç«¯æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœï¼Œä¸ä¼ ç»Ÿé™æ€æ•°æ®é€‰æ‹©æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒDRLä»£ç†çš„å¥–åŠ±ä¿¡å·åŸºäºè®­ç»ƒæŸå¤±çš„å˜åŒ–ï¼Œé‡‡ç”¨é€‚åº”æ€§ç­–ç•¥æ¥ä¼˜åŒ–æ•°æ®é€‰æ‹©ï¼Œç¡®ä¿æ¯è½®è®­ç»ƒéƒ½èƒ½æœ‰æ•ˆåˆ©ç”¨æœ€ç›¸å…³çš„æ•°æ®ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†è°ƒä¼˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æœ¬æ–‡æå‡ºçš„ç®—æ³•åï¼ŒFLå®¢æˆ·ç«¯åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›æ•°æ®é›†ä¸Šå‡†ç¡®ç‡æé«˜äº†5%è‡³10%ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ™ºèƒ½è®¾å¤‡ç­‰éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–æœ¬åœ°æ›´æ–°ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯æ•°æ®éšç§çš„å‰æä¸‹ï¼Œæå‡æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated Learning (FL) is a distributed framework for collaborative model training over large-scale distributed data, enabling higher performance while maintaining client data privacy. However, the nature of model aggregation at the centralized server can result in a performance drop in the presence of non-IID data across different clients. We remark that training a client locally on more data than necessary does not benefit the overall performance of all clients. In this paper, we devise a novel framework that leverages a Deep Reinforcement Learning (DRL) agent to select an optimized amount of data necessary to train a client model without oversharing information with the server. Starting without awareness of the client's performance, the DRL agent utilizes the change in training loss as a reward signal and learns to optimize the amount of training data necessary for improving the client's performance. Specifically, after each aggregation round, the DRL algorithm considers the local performance as the current state and outputs the optimized weights for each class, in the training data, to be used during the next round of local training. In doing so, the agent learns a policy that creates an optimized partition of the local training dataset during the FL rounds. After FL, the client utilizes the entire local training dataset to further enhance its performance on its own data distribution, mitigating the non-IID effects of aggregation. Through extensive experiments, we demonstrate that training FL clients through our algorithm results in superior performance on multiple benchmark datasets and FL frameworks. Our code is available at https://github.com/amuraddd/optimized_client_training.git.

