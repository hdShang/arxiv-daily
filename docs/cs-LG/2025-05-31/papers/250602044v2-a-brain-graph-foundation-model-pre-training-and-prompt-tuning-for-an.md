---
layout: default
title: A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder
---

# A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.02044" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.02044v2</a>
  <a href="https://arxiv.org/pdf/2506.02044.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.02044v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.02044v2', 'A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinxu Wei, Kanhao Zhao, Yong Jiao, Lifang He, Yu Zhang

**åˆ†ç±»**: q-bio.NC, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-08-03)

**å¤‡æ³¨**: 30pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/weixinxu666/BrainGFM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè„‘å›¾åŸºç¡€æ¨¡å‹ä»¥è§£å†³ç¥ç»ç§‘å­¦é¢†åŸŸçš„å¤šæ ·æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è„‘å›¾æ¨¡å‹` `å›¾å¯¹æ¯”å­¦ä¹ ` `fMRIé¢„è®­ç»ƒ` `ç¥ç»ç§‘å­¦` `æœºå™¨å­¦ä¹ ` `å›¾æ©ç è‡ªç¼–ç å™¨` `å°‘æ ·æœ¬å­¦ä¹ ` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è„‘åŸºç¡€æ¨¡å‹ä¸»è¦ä¾èµ–æ—¶é—´åºåˆ—ä¿¡å·æˆ–è¿æ¥ç»„ç‰¹å¾ï¼Œç¼ºä¹å¯¹å¤šæ ·åŒ–è„‘å›¾è°±çš„æœ‰æ•ˆåˆ©ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„BrainGFMæ¨¡å‹ç»“åˆå›¾å¯¹æ¯”å­¦ä¹ å’Œå›¾æ©ç è‡ªç¼–ç å™¨ï¼Œæ”¯æŒå¤šç§è„‘å›¾è°±å’Œç–¾ç—…çš„çµæ´»é€‚åº”ã€‚
3. BrainGFMåœ¨27ä¸ªç¥ç»å½±åƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¦†ç›–25ç§å¸¸è§ç¥ç»ç²¾ç¥ç–¾ç—…ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­çš„é©å‘½æ€§è¿›å±•ï¼Œæ„å»ºå¤§è§„æ¨¡è„‘åŸºç¡€æ¨¡å‹ä»¥æ¨åŠ¨ç¥ç»ç§‘å­¦çš„å…´è¶£æ—¥ç›Šå¢é•¿ã€‚ç°æœ‰çš„è„‘åŸºç¡€æ¨¡å‹å¤§å¤šåŸºäºæ—¶é—´åºåˆ—ä¿¡å·æˆ–è¿æ¥ç»„ç‰¹å¾è¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå›¾çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œæ„å»ºè„‘å›¾åŸºç¡€æ¨¡å‹ï¼ˆBrainGFMï¼‰ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å›¾å¯¹æ¯”å­¦ä¹ å’Œå›¾æ©ç è‡ªç¼–ç å™¨è¿›è¡Œå¤§è§„æ¨¡fMRIé¢„è®­ç»ƒï¼Œæ”¯æŒå¤šç§è„‘å›¾è°±å’Œç¥ç»ç²¾ç¥ç–¾ç—…çš„é€‚åº”æ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¼‚è´¨fMRIè¡ç”Ÿè„‘è¡¨å¾ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è„‘åŸºç¡€æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–è„‘å›¾è°±å’Œç–¾ç—…æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯å¯¹å¼‚è´¨fMRIæ•°æ®çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºBrainGFMæ¨¡å‹ï¼Œé€šè¿‡å›¾å¯¹æ¯”å­¦ä¹ å’Œå›¾æ©ç è‡ªç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹å¯¹ä¸åŒè„‘å›¾è°±å’Œç–¾ç—…çš„é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBrainGFMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒé˜¶æ®µå’Œä¸‹æ¸¸ä»»åŠ¡é€‚åº”é˜¶æ®µï¼Œå‰è€…åˆ©ç”¨å¤šç§è„‘å›¾è°±è¿›è¡Œå¤§è§„æ¨¡fMRIæ•°æ®çš„å­¦ä¹ ï¼Œåè€…é€šè¿‡å›¾æç¤ºå’Œè¯­è¨€æç¤ºå®ç°çµæ´»è½¬ç§»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆå›¾å­¦ä¹ ä¸è¯­è¨€æç¤ºï¼Œæ”¯æŒåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ æ¡ä»¶ä¸‹çš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿçš„æ—¶é—´åºåˆ—æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­é‡‡ç”¨äº†å¤šç§è„‘å›¾è°±çš„æ··åˆï¼Œä½¿ç”¨äº†ä¼˜åŒ–çš„å›¾æç¤ºå’Œè¯­è¨€æç¤ºï¼ŒæŸå¤±å‡½æ•°ç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’Œè‡ªç¼–ç å™¨çš„ç‰¹ç‚¹ï¼Œç¡®ä¿äº†æ¨¡å‹çš„é«˜æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨27ä¸ªç¥ç»å½±åƒæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒBrainGFMæ¨¡å‹åœ¨å¤„ç†25ç§å¸¸è§ç¥ç»ç²¾ç¥ç–¾ç—…æ—¶ï¼Œè¡¨ç°å‡ºä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ æ¡ä»¶ä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BrainGFMæ¨¡å‹çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬ç¥ç»ç§‘å­¦ç ”ç©¶ã€ä¸´åºŠè¯Šæ–­å’Œä¸ªæ€§åŒ–åŒ»ç–—ç­‰é¢†åŸŸã€‚é€šè¿‡å¯¹ä¸åŒè„‘å›¾è°±å’Œç–¾ç—…çš„é€‚åº”æ€§ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£è„‘åŠŸèƒ½å’Œç–¾ç—…æœºåˆ¶ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) continue to revolutionize AI research, there is a growing interest in building large-scale brain foundation models to advance neuroscience. While most existing brain foundation models are pre-trained on time-series signals or connectome features, we propose a novel graph-based pre-training paradigm for constructing a brain graph foundation model. In this paper, we introduce the Brain Graph Foundation Model, termed BrainGFM, a unified framework that leverages graph contrastive learning and graph masked autoencoders for large-scale fMRI-based pre-training. BrainGFM is pre-trained on a diverse mixture of brain atlases with varying parcellations, significantly expanding the pre-training corpus and enhancing the model's ability to generalize across heterogeneous fMRI-derived brain representations. To support efficient and versatile downstream transfer, we integrate both graph prompts and language prompts into the model design, enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological and psychiatric disorders, and task settings. Furthermore, we employ meta-learning to optimize the graph prompts, facilitating strong generalization to previously unseen disorders under both few-shot and zero-shot learning conditions via language-guided prompting. BrainGFM is pre-trained on 27 neuroimaging datasets spanning 25 common neurological and psychiatric disorders, encompassing 2 types of brain atlases (functional and anatomical) across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000 fMRI scans, and a total of 400,000 graph samples aggregated across all atlases and parcellations. The code is available at: https://github.com/weixinxu666/BrainGFM

