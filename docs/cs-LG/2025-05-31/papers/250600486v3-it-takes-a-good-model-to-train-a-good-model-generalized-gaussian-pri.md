---
layout: default
title: "It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs"
---

# It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00486" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00486v3</a>
  <a href="https://arxiv.org/pdf/2506.00486.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00486v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00486v3', 'It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Wu, Yirong Xiong, Jiangtao Wen, Yuxing Han

**åˆ†ç±»**: cs.LG, cs.AI, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-06-04)

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/spaces/shifeng3711)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¹¿ä¹‰é«˜æ–¯å…ˆéªŒçš„ä¼˜åŒ–æ¡†æ¶ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¹¿ä¹‰é«˜æ–¯åˆ†å¸ƒ` `æ¨¡å‹å‹ç¼©` `è®­ç»ƒä¼˜åŒ–` `åè®­ç»ƒæ­£åˆ™åŒ–` `ç¡¬ä»¶é«˜æ•ˆ` `æ¨ç†é€Ÿåº¦` `ç»Ÿè®¡å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¯¹å¤§è¯­è¨€æ¨¡å‹å‚æ•°çš„ç»Ÿè®¡åˆ†å¸ƒå…³æ³¨ä¸è¶³ï¼Œå½±å“åˆå§‹åŒ–å’Œè®­ç»ƒæ•ˆç‡ã€‚
2. æå‡ºåŸºäºå¹¿ä¹‰é«˜æ–¯åˆ†å¸ƒçš„åˆå§‹åŒ–å’Œæ­£åˆ™åŒ–æ–¹æ³•ï¼Œä¼˜åŒ–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¡†æ¶åœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸‹å‡èƒ½æ˜¾è‘—æå‡æ¨¡å‹å‹ç¼©ç‡å’Œæ¨ç†é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç ”ç©¶å’Œåº”ç”¨è¿…é€Ÿå‘å±•ï¼Œä½†æ¨¡å‹å‚æ•°çš„ç»Ÿè®¡åˆ†å¸ƒåŠå…¶å¯¹åˆå§‹åŒ–ã€è®­ç»ƒåŠ¨æ€å’Œä¸‹æ¸¸æ•ˆç‡çš„å½±å“å´é²œæœ‰å…³æ³¨ã€‚è¿‘æœŸç ”ç©¶æå‡ºäº†BackSlashè®­ç»ƒæ—¶å‹ç¼©ç®—æ³•ï¼Œé¦–æ¬¡è¡¨æ˜é¢„è®­ç»ƒLLMå‚æ•°æ›´ç¬¦åˆå¹¿ä¹‰é«˜æ–¯åˆ†å¸ƒï¼ˆGGDsï¼‰ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–GGå…ˆéªŒï¼ŒBackSlashèƒ½å¤Ÿåœ¨æ€§èƒ½æŸå¤±æœ€å°çš„æƒ…å†µä¸‹å‡å°‘å¤šè¾¾90%çš„å‚æ•°ã€‚åŸºäºè¿™ä¸€åŸºç¡€æ´å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç«¯åˆ°ç«¯LLMä¼˜åŒ–æ¡†æ¶ï¼Œä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š1ï¼‰åŸºäºGGçš„åˆå§‹åŒ–æ–¹æ¡ˆï¼Œæå‡æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼›2ï¼‰DeepShapeåè®­ç»ƒæ­£åˆ™åŒ–æ–¹æ³•ï¼Œé‡å¡‘æƒé‡åˆ†å¸ƒä»¥åŒ¹é…GGè½®å»“ï¼Œæ”¹å–„å‹ç¼©æ€§ï¼›3ï¼‰RF8ï¼Œä¸€ç§ç´§å‡‘ä¸”ç¡¬ä»¶é«˜æ•ˆçš„8ä½æµ®ç‚¹æ ¼å¼ï¼Œæ”¯æŒGGåˆ†å¸ƒåˆå§‹åŒ–çš„BackSlashè®­ç»ƒï¼Œé™ä½æ¨ç†æˆæœ¬è€Œä¸å½±å“å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸‹å‡èƒ½äº§ç”Ÿæ›´å°æ›´å¿«çš„æ¨¡å‹ï¼Œä¸”æ€§èƒ½ä¸æ ‡å‡†è®­ç»ƒåŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­å¯¹å‚æ•°ç»Ÿè®¡åˆ†å¸ƒå…³æ³¨ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åˆå§‹åŒ–å’Œè®­ç»ƒåŠ¨æ€ä¸Šå­˜åœ¨æ•ˆç‡ä½ä¸‹çš„ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¹¿ä¹‰é«˜æ–¯åˆ†å¸ƒï¼ˆGGDï¼‰ä½œä¸ºæ¨¡å‹å‚æ•°çš„å…ˆéªŒï¼Œä¼˜åŒ–åˆå§‹åŒ–å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥å®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1ï¼‰GGåˆå§‹åŒ–æ–¹æ¡ˆï¼Œ2ï¼‰DeepShapeåè®­ç»ƒæ­£åˆ™åŒ–ï¼Œ3ï¼‰RF8ç¡¬ä»¶é«˜æ•ˆæ ¼å¼ã€‚æ¯ä¸ªæ¨¡å—ç›¸äº’é…åˆï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„ä¼˜åŒ–æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†GGå…ˆéªŒåº”ç”¨äºæ¨¡å‹è®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‹ç¼©æ€§å’Œæ¨ç†æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…å‡å°‘å‚æ•°é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åˆå§‹åŒ–é˜¶æ®µï¼Œé‡‡ç”¨ä¸è®­ç»ƒæ¨¡å‹ç»Ÿè®¡ç»“æ„ä¸€è‡´çš„GGåˆ†å¸ƒï¼›DeepShapeé€šè¿‡è°ƒæ•´æƒé‡åˆ†å¸ƒæ¥åŒ¹é…GGè½®å»“ï¼›RF8æ ¼å¼åˆ™ä¼˜åŒ–äº†å­˜å‚¨å’Œè®¡ç®—æ•ˆç‡ï¼Œç¡®ä¿ä½æˆæœ¬æ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸‹å‡èƒ½å®ç°å‚æ•°å‡å°‘é«˜è¾¾90%ï¼ŒåŒæ—¶ä¿æŒä¸æ ‡å‡†è®­ç»ƒåŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†åœ¨å‹ç¼©æ€§å’Œæ¨ç†é€Ÿåº¦ä¸Šçš„æ˜¾è‘—æå‡ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå“åº”é€Ÿåº¦ï¼Œæ¨åŠ¨æ›´æ™ºèƒ½çš„AIç³»ç»Ÿå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: https://huggingface.co/spaces/shifeng3711/gg_prior.

