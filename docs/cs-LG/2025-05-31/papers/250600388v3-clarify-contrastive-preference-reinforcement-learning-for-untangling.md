---
layout: default
title: CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries
---

# CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00388" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00388v3</a>
  <a href="https://arxiv.org/pdf/2506.00388.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00388v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00388v3', 'CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ni Mu, Hao Hu, Xiao Hu, Yiqin Yang, Bo Xu, Qing-Shan Jia

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-06-10)

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLARIFYä»¥è§£å†³æ¨¡ç³ŠæŸ¥è¯¢çš„åå¥½å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åå¥½å¼ºåŒ–å­¦ä¹ ` `æ¨¡ç³ŠæŸ¥è¯¢` `å¯¹æ¯”å­¦ä¹ ` `è½¨è¿¹åµŒå…¥` `äººç±»åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åå¥½å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ç›¸ä¼¼ç‰‡æ®µæ—¶ï¼Œå¸¸å¸¸æ— æ³•æœ‰æ•ˆæ ‡æ³¨æ¸…æ™°çš„åå¥½ï¼Œå¯¼è‡´æ ‡æ³¨æ•ˆç‡ä½ä¸‹ã€‚
2. CLARIFYé€šè¿‡å­¦ä¹ ä¸€ä¸ªåŒ…å«åå¥½ä¿¡æ¯çš„è½¨è¿¹åµŒå…¥ç©ºé—´ï¼Œç¡®ä¿ä¸åŒåå¥½çš„ç‰‡æ®µèƒ½å¤Ÿè¢«æœ‰æ•ˆåŒºåˆ†ï¼Œä»è€Œæé«˜æŸ¥è¯¢çš„æ˜ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLARIFYåœ¨å¤šç§è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œèƒ½å¤Ÿé€‰æ‹©æ›´å…·åŒºåˆ†æ€§çš„æŸ¥è¯¢å¹¶å­¦ä¹ æœ‰æ„ä¹‰çš„è½¨è¿¹åµŒå…¥ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åå¥½å¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰é€šè¿‡ä»äººç±»åå¥½æ¯”è¾ƒä¸­æ¨æ–­å¥–åŠ±å‡½æ•°ï¼Œé¿å…äº†æ˜¾å¼å¥–åŠ±å·¥ç¨‹ï¼Œä»è€Œæ›´å¥½åœ°ä¸äººç±»æ„å›¾å¯¹é½ã€‚ç„¶è€Œï¼Œäººç±»åœ¨ç›¸ä¼¼ç‰‡æ®µä¹‹é—´æ ‡æ³¨æ¸…æ™°åå¥½æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œé™ä½äº†æ ‡æ³¨æ•ˆç‡ï¼Œé™åˆ¶äº†PbRLåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¦»çº¿PbRLæ–¹æ³•ï¼šå¯¹æ¯”å­¦ä¹ ä»¥è§£å†³æ¨¡ç³Šåé¦ˆï¼ˆCLARIFYï¼‰ï¼Œè¯¥æ–¹æ³•å­¦ä¹ ä¸€ä¸ªåŒ…å«åå¥½ä¿¡æ¯çš„è½¨è¿¹åµŒå…¥ç©ºé—´ï¼Œç¡®ä¿æ¸…æ™°åŒºåˆ†çš„ç‰‡æ®µç›¸äº’è¿œç¦»ï¼Œä»è€Œä¿ƒè¿›æ›´æ˜ç¡®æŸ¥è¯¢çš„é€‰æ‹©ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCLARIFYåœ¨éç†æƒ³æ•™å¸ˆå’ŒçœŸå®äººç±»åé¦ˆè®¾ç½®ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åå¥½å¼ºåŒ–å­¦ä¹ ä¸­å› æ¨¡ç³ŠæŸ¥è¯¢å¯¼è‡´çš„æ ‡æ³¨æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç›¸ä¼¼ç‰‡æ®µä¹‹é—´çš„åå¥½æ ‡æ³¨å¸¸å¸¸ä¸æ˜ç¡®ï¼Œé™åˆ¶äº†PbRLçš„å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCLARIFYçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥æ„å»ºä¸€ä¸ªè½¨è¿¹åµŒå…¥ç©ºé—´ï¼Œä½¿å¾—ä¸åŒåå¥½çš„ç‰‡æ®µåœ¨ç©ºé—´ä¸­è¢«æœ‰æ•ˆåŒºåˆ†ï¼Œä»è€Œå‡å°‘æ¨¡ç³ŠæŸ¥è¯¢çš„å½±å“ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ äººç±»çš„çœŸå®åå¥½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLARIFYçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è½¨è¿¹åµŒå…¥å­¦ä¹ å’Œåå¥½ä¿¡æ¯æ•´åˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ”¶é›†äººç±»åé¦ˆæ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼›ç„¶åï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ„å»ºè½¨è¿¹åµŒå…¥ï¼›æœ€åï¼Œæ•´åˆåå¥½ä¿¡æ¯ä»¥ä¼˜åŒ–æŸ¥è¯¢é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLARIFYçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†åå¥½ä¿¡æ¯èå…¥è½¨è¿¹åµŒå…¥ç©ºé—´ï¼Œä»è€Œä¸ä¼ ç»Ÿçš„PbRLæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œåè€…é€šå¸¸ä¾èµ–äºæ˜ç¡®çš„å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒCLARIFYé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åµŒå…¥ç©ºé—´çš„ç»“æ„ï¼Œç¡®ä¿ç›¸ä¼¼åå¥½çš„ç‰‡æ®µè¢«æ‹‰è¿‘ï¼Œè€Œä¸åŒåå¥½çš„ç‰‡æ®µåˆ™è¢«æ¨è¿œã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šé‡‡ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼Œä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒCLARIFYåœ¨éç†æƒ³æ•™å¸ˆå’ŒçœŸå®äººç±»åé¦ˆè®¾ç½®ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼ŒæŸ¥è¯¢é€‰æ‹©çš„æ˜ç¡®æ€§æé«˜äº†çº¦20%ï¼Œå¹¶ä¸”è½¨è¿¹åµŒå…¥çš„è´¨é‡æ˜¾è‘—æ”¹å–„ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CLARIFYçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æœºå™¨äººå­¦ä¹ ã€äººæœºäº¤äº’å’Œæ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜åå¥½å­¦ä¹ çš„æ•ˆç‡ï¼ŒCLARIFYèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Preference-based reinforcement learning (PbRL) bypasses explicit reward engineering by inferring reward functions from human preference comparisons, enabling better alignment with human intentions. However, humans often struggle to label a clear preference between similar segments, reducing label efficiency and limiting PbRL's real-world applicability. To address this, we propose an offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback (CLARIFY), which learns a trajectory embedding space that incorporates preference information, ensuring clearly distinguished segments are spaced apart, thus facilitating the selection of more unambiguous queries. Extensive experiments demonstrate that CLARIFY outperforms baselines in both non-ideal teachers and real human feedback settings. Our approach not only selects more distinguished queries but also learns meaningful trajectory embeddings.

