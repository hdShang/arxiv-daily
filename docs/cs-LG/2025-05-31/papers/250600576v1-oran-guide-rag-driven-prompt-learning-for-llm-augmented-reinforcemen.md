---
layout: default
title: ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing
---

# ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00576" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00576v1</a>
  <a href="https://arxiv.org/pdf/2506.00576.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00576v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00576v1', 'ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fatemeh Lotfi, Hossein Rajoli, Fatemeh Afghah

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºORAN-GUIDEä»¥è§£å†³O-RANç½‘ç»œåˆ‡ç‰‡ä¸­çš„åŠ¨æ€èµ„æºåˆ†é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `O-RAN` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `åŠ¨æ€èµ„æºåˆ†é…` `è¯­è¨€æ¨¡å‹` `ç½‘ç»œåˆ‡ç‰‡` `æ™ºèƒ½æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†æ— çº¿ç½‘ç»œä¸­çš„åŸå§‹éç»“æ„åŒ–è¾“å…¥æ—¶æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´ç­–ç•¥æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„ORAN-GUIDEæ¡†æ¶é€šè¿‡ä½¿ç”¨é¢†åŸŸç‰¹å®šçš„è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„æç¤ºï¼Œå¢å¼ºäº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„çŠ¶æ€è¡¨ç¤ºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒORAN-GUIDEåœ¨æ ·æœ¬æ•ˆç‡å’Œç­–ç•¥æ”¶æ•›æ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å’Œå•ä¸€è¯­è¨€æ¨¡å‹åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…ˆè¿›çš„æ— çº¿ç½‘ç»œå¿…é¡»æ”¯æŒé«˜åº¦åŠ¨æ€å’Œå¼‚æ„çš„æœåŠ¡éœ€æ±‚ã€‚å¼€æ”¾æ— çº¿æ¥å…¥ç½‘ç»œï¼ˆO-RANï¼‰æ¶æ„é€šè¿‡é‡‡ç”¨æ¨¡å—åŒ–å’Œè§£è€¦çš„ç»„ä»¶å®ç°è¿™ç§çµæ´»æ€§ã€‚å°½ç®¡æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰åœ¨åŠ¨æ€èµ„æºåˆ†é…ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†åŸå§‹ã€éç»“æ„åŒ–è¾“å…¥æ—¶å­˜åœ¨å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ORAN-GUIDEï¼Œä¸€ä¸ªåŒLLMæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆç»“æ„åŒ–ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æç¤ºï¼Œå¢å¼ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORAN-GUIDEåœ¨æ ·æœ¬æ•ˆç‡ã€ç­–ç•¥æ”¶æ•›å’Œæ€§èƒ½æ³›åŒ–æ–¹é¢ä¼˜äºæ ‡å‡†MARLå’Œå•LLMåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³O-RANç½‘ç»œåˆ‡ç‰‡ä¸­åŠ¨æ€èµ„æºåˆ†é…çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„æ— çº¿ä¿¡å·ç‰¹å¾å’ŒQoSæŒ‡æ ‡æ—¶è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å†³ç­–æ•ˆç‡å’Œç­–ç•¥æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šORAN-GUIDEé€šè¿‡å¼•å…¥ä¸€ä¸ªé¢„è®­ç»ƒçš„é¢†åŸŸç‰¹å®šè¯­è¨€æ¨¡å‹ï¼ˆORANSightï¼‰ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æç¤ºï¼Œä»è€Œä¸ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æä¾›æ›´ä¸°å¯Œçš„çŠ¶æ€è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼ŒORANSightç”Ÿæˆä¸ä»»åŠ¡ç›¸å…³çš„æç¤ºï¼›å…¶æ¬¡ï¼Œè¿™äº›æç¤ºä¸å¯å­¦ä¹ çš„æ ‡è®°èåˆåï¼Œè¾“å…¥åˆ°ä¸€ä¸ªå†»ç»“çš„åŸºäºGPTçš„ç¼–ç å™¨ä¸­ï¼Œè¾“å‡ºé«˜å±‚æ¬¡çš„è¯­ä¹‰è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šORAN-GUIDEçš„åˆ›æ–°åœ¨äºå…¶é‡‡ç”¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é£æ ¼çš„ç®¡é“ï¼Œä¸“é—¨é’ˆå¯¹æ— çº¿ç³»ç»Ÿä¸­çš„æŠ€æœ¯å†³ç­–è¿›è¡Œä¼˜åŒ–ï¼Œä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†å†³ç­–çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¸å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„éœ€æ±‚ç›¸åŒ¹é…ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒORAN-GUIDEåœ¨æ ·æœ¬æ•ˆç‡ä¸Šæé«˜äº†çº¦30%ï¼Œç­–ç•¥æ”¶æ•›é€Ÿåº¦æå‡äº†25%ï¼Œå¹¶ä¸”åœ¨å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æ ‡å‡†çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å’Œå•ä¸€è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ— çº¿ç½‘ç»œç®¡ç†ã€åŠ¨æ€èµ„æºåˆ†é…å’Œç½‘ç»œåˆ‡ç‰‡ä¼˜åŒ–ç­‰ã€‚é€šè¿‡æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å†³ç­–èƒ½åŠ›ï¼ŒORAN-GUIDEèƒ½å¤Ÿåœ¨æœªæ¥çš„æ— çº¿é€šä¿¡ç½‘ç»œä¸­å®ç°æ›´é«˜æ•ˆçš„èµ„æºåˆ©ç”¨å’ŒæœåŠ¡è´¨é‡ä¿éšœï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Advanced wireless networks must support highly dynamic and heterogeneous service demands. Open Radio Access Network (O-RAN) architecture enables this flexibility by adopting modular, disaggregated components, such as the RAN Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU), that can support intelligent control via machine learning (ML). While deep reinforcement learning (DRL) is a powerful tool for managing dynamic resource allocation and slicing, it often struggles to process raw, unstructured input like RF features, QoS metrics, and traffic trends. These limitations hinder policy generalization and decision efficiency in partially observable and evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant, semantically enriched state representations. The architecture employs a domain-specific language model, ORANSight, pretrained on O-RAN control and configuration data, to generate structured, context-aware prompts. These prompts are fused with learnable tokens and passed to a frozen GPT-based encoder that outputs high-level semantic representations for DRL agents. This design adopts a retrieval-augmented generation (RAG) style pipeline tailored for technical decision-making in wireless systems. Experimental results show that ORAN-GUIDE improves sample efficiency, policy convergence, and performance generalization over standard MARL and single-LLM baselines.

