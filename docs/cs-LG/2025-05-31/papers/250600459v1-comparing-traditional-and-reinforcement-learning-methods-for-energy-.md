---
layout: default
title: Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control
---

# Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00459" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00459v1</a>
  <a href="https://arxiv.org/pdf/2506.00459.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00459v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00459v1', 'Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Elinor Ginzburg, Itay Segev, Yoash Levron, Sarah Keren

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¯”è¾ƒä¼ ç»Ÿä¸å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨èƒ½æºå­˜å‚¨æ§åˆ¶ä¸­çš„åº”ç”¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `èƒ½æºå­˜å‚¨` `å¼ºåŒ–å­¦ä¹ ` `ä¼ ç»Ÿæ–¹æ³•` `å¾®ç”µç½‘` `ä¼˜åŒ–ç­–ç•¥` `å¯å†ç”Ÿèƒ½æº` `æ™ºèƒ½ç”µç½‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„èƒ½æºå­˜å‚¨æ§åˆ¶æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ€§å’ŒåŠ¨æ€ç¯å¢ƒæ—¶å­˜åœ¨æ€§èƒ½æŸå¤±ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ç”Ÿæˆæ€§RLç­–ç•¥æ—¶ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ç®€åŒ–å¾®ç”µç½‘æ¨¡å‹ï¼Œç³»ç»Ÿæ¯”è¾ƒä¼ ç»Ÿæ–¹æ³•ä¸RLæ–¹æ³•åœ¨ä¸åŒå¤æ‚æ€§åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œä»¥ä¼˜åŒ–èƒ½æºå­˜å‚¨ç®¡ç†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç‰¹å®šåœºæ™¯ä¸‹ï¼ŒRLæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡æ§åˆ¶ç­–ç•¥çš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œå°½ç®¡åœ¨æŸäº›æƒ…å†µä¸‹ä¼ ç»Ÿæ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨æ·±å…¥ç†è§£ä¼ ç»Ÿæ–¹æ³•ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨èƒ½æºå­˜å‚¨ç®¡ç†ä¸­çš„æƒè¡¡ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨ç”Ÿæˆæ€§RLç­–ç•¥æ—¶ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•åœ¨ç‰¹å®šå®ä¾‹ä¸­å¯»æ‰¾æœ€ä¼˜æ§åˆ¶ç­–ç•¥æ‰€å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚åŸºäºç®€åŒ–çš„å¾®ç”µç½‘æ¨¡å‹ï¼Œç ”ç©¶è€ƒå¯Ÿäº†ç†æƒ³å­˜å‚¨ã€æŸè€—å­˜å‚¨è®¾å¤‡åŠæŸè€—å­˜å‚¨è®¾å¤‡ä¸ä¼ è¾“æŸè€—çš„ä¸‰ç§å¤æ‚æ€§é€æ¸å¢åŠ çš„ä½¿ç”¨æ¡ˆä¾‹ã€‚é€šè¿‡è¯¦ç»†çš„æ¡ˆä¾‹æè¿°å’Œä¼˜åŒ–æŒ‘æˆ˜åˆ†æï¼Œæ¯”è¾ƒäº†ä¼ ç»Ÿä¸RLæ–¹æ³•çš„æ€§èƒ½ï¼Œè®¨è®ºäº†å„è‡ªçš„é€‚ç”¨åœºæ™¯ï¼Œå¹¶æå‡ºæœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶èšç„¦äºèƒ½æºå­˜å‚¨æ§åˆ¶ä¸­çš„æ€§èƒ½æŸå¤±é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ç”Ÿæˆæ€§å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ—¶ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é€‚åº”æ€§å’Œæ•ˆç‡äºŸå¾…æé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡é€šè¿‡å»ºç«‹ç®€åŒ–çš„å¾®ç”µç½‘æ¨¡å‹ï¼Œæ¯”è¾ƒä¸åŒå¤æ‚æ€§åœºæ™¯ä¸‹ä¼ ç»Ÿä¸RLæ–¹æ³•çš„æ€§èƒ½ï¼Œæ—¨åœ¨ä¸ºRLæ–¹æ³•åœ¨èƒ½æºç®¡ç†ä¸­çš„åº”ç”¨æä¾›ç†è®ºæ”¯æŒå’Œå®è·µæŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆå®šä¹‰äº†ä¸‰ç§ä½¿ç”¨æ¡ˆä¾‹ï¼Œåˆ†åˆ«ä¸ºç†æƒ³å­˜å‚¨ã€æŸè€—å­˜å‚¨è®¾å¤‡å’Œå¸¦æœ‰ä¼ è¾“æŸè€—çš„æŸè€—å­˜å‚¨è®¾å¤‡ã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½è¯¦ç»†æè¿°äº†ä¼˜åŒ–æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡å®éªŒè¯„ä¼°ä¸¤ç§æ–¹æ³•çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°ä¹‹å¤„åœ¨äºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†ä¼ ç»Ÿä¸RLæ–¹æ³•åœ¨èƒ½æºå­˜å‚¨æ§åˆ¶ä¸­çš„é€‚ç”¨æ€§ï¼Œæ­ç¤ºäº†åœ¨ä¸åŒåœºæ™¯ä¸‹å„è‡ªçš„ä¼˜åŠ¿ä¸åŠ£åŠ¿ï¼Œæ¨åŠ¨äº†RLæ–¹æ³•åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒå¤æ‚æ€§åœºæ™¯ä¸‹çš„ä¼˜åŒ–æ•ˆæœï¼Œå…·ä½“ç»†èŠ‚åŒ…æ‹¬å¯¹å­˜å‚¨è®¾å¤‡æ€§èƒ½çš„å»ºæ¨¡å’Œå¯¹ä¼ è¾“æŸè€—çš„è€ƒè™‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç†æƒ³å­˜å‚¨åœºæ™¯ä¸‹ï¼ŒRLæ–¹æ³•çš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°15%ï¼Œè€Œåœ¨æŸè€—å­˜å‚¨è®¾å¤‡åœºæ™¯ä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•ä»ç„¶è¡¨ç°æ›´ä¼˜ï¼Œæå‡å¹…åº¦ä¸º10%ã€‚è¿™äº›ç»“æœä¸ºä¸åŒåœºæ™¯ä¸‹æ–¹æ³•é€‰æ‹©æä¾›äº†å®è¯ä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç”µç½‘ã€å¯å†ç”Ÿèƒ½æºç®¡ç†å’Œç”µåŠ›ç³»ç»Ÿä¼˜åŒ–ç­‰ã€‚é€šè¿‡ä¼˜åŒ–èƒ½æºå­˜å‚¨æ§åˆ¶ç­–ç•¥ï¼Œå¯ä»¥æé«˜èƒ½æºåˆ©ç”¨æ•ˆç‡ï¼Œé™ä½è¿è¥æˆæœ¬ï¼Œæ¨åŠ¨å¯æŒç»­å‘å±•ã€‚æœªæ¥ï¼Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„åº”ç”¨å¯èƒ½ä¼šåœ¨æ›´å¤æ‚çš„èƒ½æºç®¡ç†ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We aim to better understand the tradeoffs between traditional and reinforcement learning (RL) approaches for energy storage management. More specifically, we wish to better understand the performance loss incurred when using a generative RL policy instead of using a traditional approach to find optimal control policies for specific instances. Our comparison is based on a simplified micro-grid model, that includes a load component, a photovoltaic source, and a storage device. Based on this model, we examine three use cases of increasing complexity: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses. With the aim of promoting the principled use RL based methods in this challenging and important domain, we provide a detailed formulation of each use case and a detailed description of the optimization challenges. We then compare the performance of traditional and RL methods, discuss settings in which it is beneficial to use each method, and suggest avenues for future investigation.

