---
layout: default
title: "RLAE: Reinforcement Learning-Assisted Ensemble for LLMs"
---

# RLAE: Reinforcement Learning-Assisted Ensemble for LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00439" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00439v1</a>
  <a href="https://arxiv.org/pdf/2506.00439.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00439v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00439v1', 'RLAE: Reinforcement Learning-Assisted Ensemble for LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuqian Fu, Yuanheng Zhu, Jiajun Chai, Guojun Yin, Wei Lin, Qichao Zhang, Dongbin Zhao

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRLAEä»¥è§£å†³LLMé›†æˆåŠ¨æ€æƒé‡è°ƒæ•´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `é›†æˆå­¦ä¹ ` `åŠ¨æ€æƒé‡` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ¨¡å‹æ³›åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMé›†æˆæ–¹æ³•ä¾èµ–å›ºå®šæƒé‡ç­–ç•¥ï¼Œæ— æ³•é€‚åº”æ¨¡å‹èƒ½åŠ›çš„åŠ¨æ€å˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„RLAEæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ åŠ¨æ€è°ƒæ•´é›†æˆæƒé‡ï¼Œè€ƒè™‘è¾“å…¥ä¸Šä¸‹æ–‡å’Œç”ŸæˆçŠ¶æ€ï¼Œæå‡äº†é›†æˆæ•ˆæœã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRLAEåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†æœ€é«˜3.3%çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿæœ‰æ•ˆç»“åˆä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä»è€Œæå‡å¤šç§ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å›ºå®šçš„æƒé‡ç­–ç•¥ï¼Œæ— æ³•é€‚åº”LLMèƒ½åŠ›çš„åŠ¨æ€å’Œä¸Šä¸‹æ–‡ä¾èµ–ç‰¹æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶â€”â€”å¼ºåŒ–å­¦ä¹ è¾…åŠ©é›†æˆï¼ˆRLAEï¼‰ï¼Œé€šè¿‡é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰é‡æ–°æ„å»ºLLMé›†æˆã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»£ç†ï¼ŒåŠ¨æ€è°ƒæ•´é›†æˆæƒé‡ï¼Œè€ƒè™‘è¾“å…¥ä¸Šä¸‹æ–‡å’Œä¸­é—´ç”ŸæˆçŠ¶æ€ï¼Œå¹¶é€šè¿‡ä¸æœ€ç»ˆè¾“å‡ºè´¨é‡ç›´æ¥ç›¸å…³çš„å¥–åŠ±è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLAEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»Ÿé›†æˆæ–¹æ³•æå‡äº†æœ€é«˜3.3%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ›´ä½çš„æ—¶é—´å»¶è¿Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„LLMé›†æˆæ–¹æ³•é€šå¸¸ä½¿ç”¨å›ºå®šçš„æƒé‡ç­–ç•¥ï¼Œæ— æ³•æ ¹æ®è¾“å…¥çš„ä¸Šä¸‹æ–‡å’Œæ¨¡å‹çš„åŠ¨æ€çŠ¶æ€è¿›è¡Œè°ƒæ•´ï¼Œå¯¼è‡´æ€§èƒ½çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRLAEæ¡†æ¶é€šè¿‡å°†LLMé›†æˆè§†ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¼•å…¥å¼ºåŒ–å­¦ä¹ ä»£ç†æ¥åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ä¸åŒä»»åŠ¡å’Œä¸Šä¸‹æ–‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLAEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥ä¸Šä¸‹æ–‡çš„åˆ†æã€ç”ŸæˆçŠ¶æ€çš„è¯„ä¼°å’Œå¼ºåŒ–å­¦ä¹ ä»£ç†çš„è®­ç»ƒã€‚ä»£ç†æ ¹æ®æœ€ç»ˆè¾“å‡ºçš„è´¨é‡è¿›è¡Œæƒé‡è°ƒæ•´ï¼Œå½¢æˆé—­ç¯åé¦ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šRLAEçš„ä¸»è¦åˆ›æ–°åœ¨äºä½¿ç”¨å¼ºåŒ–å­¦ä¹ åŠ¨æ€è°ƒæ•´é›†æˆæƒé‡ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€æƒé‡ç­–ç•¥å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œä½¿å¾—é›†æˆæ¨¡å‹èƒ½å¤Ÿæ›´çµæ´»åœ°é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RLAEä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ä»£ç†çš„è®­ç»ƒä½¿ç”¨ä¸è¾“å‡ºè´¨é‡ç›´æ¥ç›¸å…³çš„å¥–åŠ±ä¿¡å·ï¼Œé‡‡ç”¨äº†å•ä»£ç†å’Œå¤šä»£ç†çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚RLAE_PPOå’ŒRLAE_MAPPOï¼‰ï¼Œä»¥ä¼˜åŒ–é›†æˆæ•ˆæœã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šé¡¹ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒRLAEç›¸è¾ƒäºä¼ ç»Ÿé›†æˆæ–¹æ³•æå‡äº†æœ€é«˜3.3%çš„å‡†ç¡®ç‡ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æ—¶é—´å»¶è¿Ÿæ–¹é¢è¡¨ç°æ›´ä¼˜ï¼Œè¯æ˜äº†å…¶åœ¨LLMé›†æˆä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RLAEæ¡†æ¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´é›†æˆæƒé‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­æä¾›æ›´é«˜çš„å‡†ç¡®æ€§å’Œå“åº”é€Ÿåº¦ï¼Œå…·æœ‰æ˜¾è‘—çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.

