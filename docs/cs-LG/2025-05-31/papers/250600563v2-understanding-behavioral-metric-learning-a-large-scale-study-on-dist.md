---
layout: default
title: Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments
---

# Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00563" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00563v2</a>
  <a href="https://arxiv.org/pdf/2506.00563.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00563v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00563v2', 'Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyan Luo, Tianwei Ni, Pierre-Luc Bacon, Doina Precup, Xujie Si

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-09-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°çš„åº¦é‡å­¦ä¹ æ–¹æ³•ä»¥åº”å¯¹å¼ºåŒ–å­¦ä¹ ä¸­çš„å¹²æ‰°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¡Œä¸ºåº¦é‡å­¦ä¹ ` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `å»å™ªå› å­` `çŠ¶æ€æŠ½è±¡` `å¼€æºä»£ç åº“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¡Œä¸ºåº¦é‡å­¦ä¹ æ–¹æ³•åœ¨å‡†ç¡®ä¼°è®¡åº¦é‡æ—¶é¢ä¸´è®¾è®¡é€‰æ‹©å¸¦æ¥çš„ç†è®ºä¸å®è·µä¹‹é—´çš„å·®è·ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿè¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡å¯¹äº”ç§åº¦é‡å­¦ä¹ æ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ¢ç´¢å…¶åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°çš„å»å™ªå› å­è¯„ä¼°æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé‡åŒ–ç¼–ç å™¨çš„å¹²æ‰°è¿‡æ»¤èƒ½åŠ›ï¼Œæå‡äº†å­¦ä¹ åº¦é‡çš„è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶èšç„¦äºçŠ¶æ€æŠ½è±¡ä¸­çš„è¡Œä¸ºåº¦é‡å­¦ä¹ ï¼Œå°¤å…¶æ˜¯è§‚å¯Ÿç©ºé—´ä¸­çš„åŒä»¿å°„åº¦é‡çš„è¿‘ä¼¼ã€‚å°½ç®¡å…ˆå‰ç ”ç©¶è¡¨æ˜è¯¥æ–¹æ³•åœ¨åº”å¯¹ä»»åŠ¡æ— å…³å™ªå£°æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å‡†ç¡®ä¼°è®¡è¿™äº›åº¦é‡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨æœ€ç»ˆå›æŠ¥ä¸Šï¼Œæœªèƒ½æ¸…æ™°æ­ç¤ºå­¦ä¹ åº¦é‡çš„è´¨é‡åŠå…¶æ€§èƒ½æå‡çš„æ¥æºã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†äº”ç§æœ€è¿‘çš„åº¦é‡å­¦ä¹ æ–¹æ³•ï¼Œå¹¶åœ¨20ä¸ªåŸºäºçŠ¶æ€å’Œ14ä¸ªåŸºäºåƒç´ çš„ä»»åŠ¡ä¸­è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–370ç§ä»»åŠ¡é…ç½®åŠå¤šæ ·çš„å™ªå£°è®¾ç½®ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†å»å™ªå› å­çš„è¯„ä¼°ï¼Œä»¥é‡åŒ–ç¼–ç å™¨è¿‡æ»¤å¹²æ‰°çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå­¤ç«‹çš„åº¦é‡ä¼°è®¡è®¾ç½®ï¼Œä»¥è¿›ä¸€æ­¥éš”ç¦»åº¦é‡å­¦ä¹ çš„å½±å“ã€‚æœ€åï¼Œç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†ä¸€ä¸ªå¼€æºæ¨¡å—åŒ–ä»£ç åº“ï¼Œä»¥æé«˜å¯é‡å¤æ€§å¹¶æ”¯æŒæœªæ¥çš„åº¦é‡å­¦ä¹ ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­ï¼Œå¦‚ä½•å‡†ç¡®ä¼°è®¡è¡Œä¸ºåº¦é‡ï¼ˆå¦‚åŒä»¿å°„åº¦é‡ï¼‰çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®¾è®¡é€‰æ‹©ä¸Šå­˜åœ¨ç†è®ºä¸å®è·µä¹‹é—´çš„å·®è·ï¼Œå¯¼è‡´åº¦é‡å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡é€šè¿‡ç³»ç»Ÿè¯„ä¼°äº”ç§ä¸åŒçš„åº¦é‡å­¦ä¹ æ–¹æ³•ï¼Œç»Ÿä¸€ä¸ºç­‰è·åµŒå…¥ï¼Œæ¢ç´¢å…¶åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥å»å™ªå› å­è¯„ä¼°ï¼Œé‡åŒ–ç¼–ç å™¨çš„å¹²æ‰°è¿‡æ»¤èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥æå‡å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬äº”ç§åº¦é‡å­¦ä¹ æ–¹æ³•çš„æ¯”è¾ƒï¼ŒåŸºäº20ä¸ªçŠ¶æ€ä»»åŠ¡å’Œ14ä¸ªåƒç´ ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–370ç§ä»»åŠ¡é…ç½®ã€‚è¯„ä¼°è¿‡ç¨‹åˆ†ä¸ºæœ€ç»ˆå›æŠ¥å’Œå»å™ªå› å­ä¸¤ä¸ªé˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥å»å™ªå› å­çš„è¯„ä¼°æ–¹æ³•ï¼Œèƒ½å¤Ÿé‡åŒ–ç¼–ç å™¨åœ¨é¢å¯¹å¹²æ‰°æ—¶çš„è¡¨ç°ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶ä¸­å¯¹å­¦ä¹ åº¦é‡è´¨é‡è¯„ä¼°çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è®¾è®¡é€‰æ‹©ï¼ŒåŒ…æ‹¬ä¸åŒçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿å¯¹åº¦é‡å­¦ä¹ çš„å…¨é¢è¯„ä¼°ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œæ¶æ„ç»†èŠ‚åœ¨å¼€æºä»£ç åº“ä¸­æä¾›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–°çš„åº¦é‡å­¦ä¹ æ–¹æ³•åœ¨å»å™ªèƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚å™ªå£°ç¯å¢ƒæ—¶ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°ï¼Œæ˜ç¡®äº†ä¸åŒè®¾è®¡é€‰æ‹©å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆæ™ºèƒ½ä½“ç­‰éœ€è¦å¤„ç†å¤æ‚ç¯å¢ƒä¸­çš„å¹²æ‰°çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡æé«˜åº¦é‡å­¦ä¹ çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿå¢å¼ºæ™ºèƒ½ä½“åœ¨çœŸå®ä¸–ç•Œä¸­çš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A key approach to state abstraction is approximating behavioral metrics (notably, bisimulation metrics) in the observation space and embedding these learned distances in the representation space. While promising for robustness to task-irrelevant noise, as shown in prior work, accurately estimating these metrics remains challenging, requiring various design choices that create gaps between theory and practice. Prior evaluations focus mainly on final returns, leaving the quality of learned metrics and the source of performance gains unclear. To systematically assess how metric learning works in deep reinforcement learning (RL), we evaluate five recent approaches, unified conceptually as isometric embeddings with varying design choices. We benchmark them with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, we introduce the evaluation of a denoising factor to quantify the encoder's ability to filter distractions. To further isolate the effect of metric learning, we propose and evaluate an isolated metric estimation setting, in which the encoder is influenced solely by the metric loss. Finally, we release an open-source, modular codebase to improve reproducibility and support future research on metric learning in deep RL.

