---
layout: default
title: Power-of-Two (PoT) Weights in Large Language Models (LLMs)
---

# Power-of-Two (PoT) Weights in Large Language Models (LLMs)

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00315" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00315v1</a>
  <a href="https://arxiv.org/pdf/2506.00315.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00315v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00315v1', 'Power-of-Two (PoT) Weights in Large Language Models (LLMs)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mahmoud Elgenedy

**åˆ†ç±»**: eess.SP, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPoTæƒé‡ä»¥é™ä½å¤§è¯­è¨€æ¨¡å‹çš„å¤æ‚æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `é‡åŒ–æŠ€æœ¯` `äºŒæ¬¡å¹‚` `è®¡ç®—å¤æ‚æ€§` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹å‚æ•°æ•°é‡æ€¥å‰§å¢åŠ ï¼Œå¯¼è‡´æ¨¡å‹å¤æ‚æ€§ä¸Šå‡ï¼Œå°¤å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°é¢ä¸´å†…å­˜å’Œè®¡ç®—èƒ½åŠ›çš„é™åˆ¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§äºŒæ¬¡å¹‚ï¼ˆPoTï¼‰é‡åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç‰¹æ®Šçš„é‡åŒ–æŠ€æœ¯é™ä½å¤§è¯­è¨€æ¨¡å‹çš„å¤æ‚æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æƒé‡å’Œå˜æ¢è¡¨ä¸­ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPoTé‡åŒ–åœ¨Nano-GPTå’Œ124-M GPT-2æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œäº¤å‰ç†µæŸå¤±æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‚æ•°æ•°é‡çš„è¿…é€Ÿå¢åŠ ï¼Œæ¨¡å‹å¤æ‚æ€§ä¹Ÿåœ¨æ€¥å‰§ä¸Šå‡ï¼Œç»™è¾¹ç¼˜è®¾å¤‡çš„å®ç°å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§ç‰¹æ®Šçš„é‡åŒ–æ–¹æ³•â€”â€”äºŒæ¬¡å¹‚ï¼ˆPoTï¼‰é‡åŒ–ï¼Œæ—¨åœ¨å‡å°‘LLMsçš„å¤æ‚æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¿æ€§å±‚æƒé‡å’Œå˜æ¢è¡¨ä¸­ã€‚PoTä¸ä»…èƒ½å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œæ›´é‡è¦çš„æ˜¯é€šè¿‡å°†ä¹˜æ³•è½¬æ¢ä¸ºä½ç§»æ“ä½œæ˜¾è‘—é™ä½è®¡ç®—é‡ã€‚é€šè¿‡å¯¹Nano-GPTåœ¨èå£«æ¯”äºšæ•°æ®é›†ä¸Šçš„åˆæ­¥å®éªŒï¼Œç»“æœæ˜¾ç¤ºPoTé‡åŒ–åœ¨124-M GPT-2æ¨¡å‹ä¸Šçš„è¡¨ç°ä¹Ÿéå¸¸æœ‰å‰æ™¯ï¼Œäº¤å‰ç†µæŸå¤±é™å¹…çº¦ä¸º[1.3-0.88]ï¼Œä½¿ç”¨4åˆ°6ä½è¡¨ç¤ºå¹‚çº§åˆ«ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°æ—¶é¢ä¸´çš„å¤æ‚æ€§å’Œèµ„æºé™åˆ¶é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ¨¡å‹æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆé™ä½å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„äºŒæ¬¡å¹‚ï¼ˆPoTï¼‰é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡å°†æƒé‡è¡¨ç¤ºä¸ºäºŒæ¬¡å¹‚å½¢å¼ï¼Œåˆ©ç”¨ä½ç§»æ“ä½œæ›¿ä»£ä¹˜æ³•ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜å ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€PoTé‡åŒ–å®æ–½å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶ååº”ç”¨PoTé‡åŒ–æŠ€æœ¯äºçº¿æ€§å±‚æƒé‡å’Œå˜æ¢è¡¨ï¼Œæœ€åè¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥PoTé‡åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨ä½ç§»æ“ä½œæ›¿ä»£ä¼ ç»Ÿä¹˜æ³•ï¼Œæ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„é‡åŒ–æŠ€æœ¯ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨PoTé‡åŒ–ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®ä¸º4åˆ°6ä½ç”¨äºè¡¨ç¤ºå¹‚çº§åˆ«ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç¡®ä¿æ¨¡å‹è®­ç»ƒçš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPoTé‡åŒ–åœ¨Nano-GPTå’Œ124-M GPT-2æ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œäº¤å‰ç†µæŸå¤±é™å¹…çº¦ä¸º[1.3-0.88]ï¼Œä½¿ç”¨4åˆ°6ä½è¡¨ç¤ºå¹‚çº§åˆ«ï¼Œå±•ç°å‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨è®¾å¤‡å’Œèµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡é™ä½æ¨¡å‹å¤æ‚æ€§ï¼ŒPoTé‡åŒ–æ–¹æ³•èƒ½å¤Ÿä½¿å¾—é«˜æ€§èƒ½çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡åœ¨æ›´å¹¿æ³›çš„è®¾å¤‡ä¸Šå®ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Complexity of Neural Networks is increasing rapidly due to the massive increase in model parameters. Specifically, in Large Language Models (LLMs), the number of model parameters has grown exponentially in the past few years, for example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This raises a significant challenge for implementation, especially for Edge devices where memory and processing power are very limited. In this work, we investigate reducing LLM complexity with special type of quantization, power of two (PoT), for linear layers weights and transformer tables. PoT not only provides memory reduction but more importantly provides significant computational reduction through converting multiplication to bit shifting. We obtained preliminary results of PoT quantization on Nano-GPT implementation using Shakespeare dataset. We then extended results to 124-M GPT-2 model. The PoT quantization results are shown to be very promising with cross entropy loss degradation $\approx$[1.3-0.88] with number of bits range [4-6] to represent power levels.

