---
layout: default
title: Reinforcement Learning for Hanabi
---

# Reinforcement Learning for Hanabi

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00458" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00458v1</a>
  <a href="https://arxiv.org/pdf/2506.00458.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00458v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00458v1', 'Reinforcement Learning for Hanabi')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nina Cohen, Kordel K. France

**åˆ†ç±»**: cs.LG, cs.AI, cs.GT, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢ç´¢å¼ºåŒ–å­¦ä¹ åœ¨Hanabiæ¸¸æˆä¸­çš„åº”ç”¨ä¸è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `åˆä½œæ¸¸æˆ` `ä¸å®Œå…¨ä¿¡æ¯` `æ—¶é—´å·®åˆ†ç®—æ³•` `æ·±åº¦å­¦ä¹ ` `ä»£ç†å¯¹æŠ—` `æ€§èƒ½è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Hanabiæ¸¸æˆçš„ç¯å¢ƒä¿¡æ¯ä¸å®Œå…¨æ€§ç»™å¼ºåŒ–å­¦ä¹ ä»£ç†å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é€‚åº”æ€§å’Œè¡¨ç°ä¸Šå­˜åœ¨ä¸è¶³ã€‚
2. æœ¬æ–‡é€šè¿‡æ¯”è¾ƒå¤šç§è¡¨æ ¼å‹å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ¢ç´¢ä¸åŒç®—æ³•åœ¨ä¸åŒç±»åŠå¼‚ç±»ä»£ç†å¯¹æŠ—ä¸­çš„è¡¨ç°å·®å¼‚ã€‚
3. ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ—¶é—´å·®åˆ†ç®—æ³•åœ¨æ•´ä½“è¡¨ç°ä¸Šä¼˜äºè¡¨æ ¼å‹ä»£ç†ï¼Œå°¤å…¶æ˜¯æœŸæœ›SARSAå’Œæ·±åº¦Qå­¦ä¹ ç®—æ³•è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Hanabiä½œä¸ºä¸€ç§åˆä½œå¡ç‰Œæ¸¸æˆï¼Œå› å…¶ç¯å¢ƒä¿¡æ¯çš„ä¸å®Œå…¨æ€§è€Œæˆä¸ºå¼ºåŒ–å­¦ä¹ ç ”ç©¶çš„çƒ­é—¨é¢†åŸŸã€‚æœ¬æ–‡æ¢è®¨äº†å¤šç§è¡¨æ ¼å‹å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ä¸åŒç±»åŠå¼‚ç±»ä»£ç†å¯¹æŠ—ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒç±»å‹çš„ä»£ç†åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è¡¨ç°æœ€ä½³ï¼Œå¹¶ä¸”æŸäº›ä»£ç†èƒ½å¤Ÿé€šè¿‡é€‚åº”å¯¹æ‰‹çš„è¡Œä¸ºè·å¾—æ›´é«˜çš„å¹³å‡åˆ†æ•°ã€‚æœ€ç»ˆï¼Œæ—¶é—´å·®åˆ†ï¼ˆTDï¼‰ç®—æ³•åœ¨æ•´ä½“è¡¨ç°å’Œæ¸¸æˆç±»å‹å¹³è¡¡æ–¹é¢ä¼˜äºè¡¨æ ¼å‹ä»£ç†ï¼Œå°¤å…¶æ˜¯è¡¨æ ¼å‹çš„æœŸæœ›SARSAå’Œæ·±åº¦Qå­¦ä¹ ä»£ç†è¡¨ç°æœ€ä½³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Hanabiæ¸¸æˆä¸­å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨é¢å¯¹ä¸å®Œå…¨ä¿¡æ¯æ—¶çš„é€‚åº”æ€§å’Œè¡¨ç°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¸åŒç±»å‹ä»£ç†å¯¹æŠ—ä¸­è¡¨ç°ä¸å‡ï¼Œç¼ºä¹ç³»ç»Ÿæ€§åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¯”è¾ƒä¸åŒçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå°¤å…¶æ˜¯è¡¨æ ¼å‹ä¸æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œæ¥è¯†åˆ«åœ¨ç‰¹å®šå¯¹æŠ—æ¡ä»¶ä¸‹çš„æœ€ä½³è¡¨ç°ç­–ç•¥ï¼Œè¿›è€Œä¼˜åŒ–ä»£ç†çš„å­¦ä¹ ä¸é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬è¡¨æ ¼å‹çš„æœŸæœ›SARSAå’Œæ·±åº¦Qå­¦ä¹ ï¼Œæ„å»ºäº†ä¸€ä¸ªå®éªŒæ¡†æ¶æ¥è¯„ä¼°è¿™äº›ç®—æ³•åœ¨Hanabiæ¸¸æˆä¸­çš„è¡¨ç°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ä»£ç†è¡Œä¸ºæ¨¡å‹ã€ç¯å¢ƒæ¨¡æ‹Ÿå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°é‡åŒ–ä¸åŒç®—æ³•åœ¨å¤šç§å¯¹æŠ—æ¡ä»¶ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä»£ç†é—´çš„äº’åŠ¨å…³ç³»åŠå…¶å¯¹å¾—åˆ†çš„å½±å“ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œå¥–åŠ±æœºåˆ¶ï¼Œä¼˜åŒ–äº†ä»£ç†çš„å†³ç­–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å¤šè½®å®éªŒéªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ä¸ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ—¶é—´å·®åˆ†ç®—æ³•åœ¨æ•´ä½“è¡¨ç°ä¸Šä¼˜äºè¡¨æ ¼å‹ä»£ç†ï¼Œå…·ä½“è€Œè¨€ï¼Œè¡¨æ ¼å‹çš„æœŸæœ›SARSAå’Œæ·±åº¦Qå­¦ä¹ ä»£ç†åœ¨ä¸åŒç±»åŠå¼‚ç±»ä»£ç†å¯¹æŠ—ä¸­è·å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨é€‚åº”æ€§å¾—åˆ†ä¸Šè¡¨ç°çªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ï¼Œå°¤å…¶æ˜¯åœ¨åˆä½œä¸å¯¹æŠ—åœºæ™¯ä¸­ã€‚å…¶æ–¹æ³•è®ºå¯æ‰©å±•è‡³å…¶ä»–å…·æœ‰ä¸å®Œå…¨ä¿¡æ¯çš„æ¸¸æˆæˆ–å†³ç­–ç³»ç»Ÿï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç ”ç©¶æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hanabi has become a popular game for research when it comes to reinforcement learning (RL) as it is one of the few cooperative card games where you have incomplete knowledge of the entire environment, thus presenting a challenge for a RL agent. We explored different tabular and deep reinforcement learning algorithms to see which had the best performance both against an agent of the same type and also against other types of agents. We establish that certain agents played their highest scoring games against specific agents while others exhibited higher scores on average by adapting to the opposing agent's behavior. We attempted to quantify the conditions under which each algorithm provides the best advantage and identified the most interesting interactions between agents of different types. In the end, we found that temporal difference (TD) algorithms had better overall performance and balancing of play types compared to tabular agents. Specifically, tabular Expected SARSA and deep Q-Learning agents showed the best performance.

