---
layout: default
title: Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders
---

# Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08080" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08080v2</a>
  <a href="https://arxiv.org/pdf/2505.08080.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08080v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08080v2', 'Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dong Shu, Xuansheng Wu, Haiyan Zhao, Mengnan Du, Ninghao Liu

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-09-23)

**å¤‡æ³¨**: EMNLP 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGradient Sparse Autoencoderä»¥è¯†åˆ«å½±å“æ¨¡å‹è¾“å‡ºçš„æ½œåœ¨ç‰¹å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–è‡ªç¼–ç å™¨` `æ½œåœ¨ç‰¹å¾` `æ¨¡å‹å¯è§£é‡Šæ€§` `å› æœå½±å“` `æ¢¯åº¦ä¿¡æ¯` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¨€ç–è‡ªç¼–ç å™¨åˆ†ææ–¹æ³•ä¸»è¦ä¾èµ–è¾“å…¥ä¾§æ¿€æ´»ï¼Œæœªè€ƒè™‘æ½œåœ¨ç‰¹å¾ä¸è¾“å‡ºä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¯¼è‡´åˆ†æç»“æœçš„å±€é™æ€§ã€‚
2. æœ¬ç ”ç©¶æå‡ºGradient Sparse Autoencoderï¼Œé€šè¿‡å¼•å…¥è¾“å‡ºä¾§æ¢¯åº¦ä¿¡æ¯ï¼Œè¯†åˆ«å¯¹æ¨¡å‹è¾“å‡ºå½±å“æœ€å¤§çš„æ½œåœ¨ç‰¹å¾ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¼•å¯¼èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGradSAEåœ¨è¯†åˆ«æ½œåœ¨ç‰¹å¾çš„æœ‰æ•ˆæ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ½œåœ¨ç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„è´¡çŒ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰è¿‘å¹´æ¥æˆä¸ºè§£é‡Šå’Œå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å†…éƒ¨è¡¨ç¤ºçš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„SAEåˆ†ææ–¹æ³•é€šå¸¸ä»…ä¾èµ–è¾“å…¥ä¾§çš„æ¿€æ´»ï¼Œè€Œå¿½è§†äº†æ¯ä¸ªæ½œåœ¨ç‰¹å¾ä¸æ¨¡å‹è¾“å‡ºä¹‹é—´çš„å› æœå½±å“ã€‚æœ¬æ–‡åŸºäºä¸¤ä¸ªå…³é”®å‡è®¾ï¼šæ¿€æ´»çš„æ½œåœ¨ç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„æ„å»ºè´¡çŒ®ä¸å‡ç­‰ï¼Œä¸”åªæœ‰å…·æœ‰é«˜å› æœå½±å“çš„æ½œåœ¨ç‰¹å¾æ‰å¯¹æ¨¡å‹å¼•å¯¼æœ‰æ•ˆã€‚ä¸ºéªŒè¯è¿™äº›å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºäº†Gradient Sparse Autoencoderï¼ˆGradSAEï¼‰ï¼Œä¸€ç§é€šè¿‡ç»“åˆè¾“å‡ºä¾§æ¢¯åº¦ä¿¡æ¯æ¥è¯†åˆ«æœ€å…·å½±å“åŠ›æ½œåœ¨ç‰¹å¾çš„ç®€å•æœ‰æ•ˆæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿç¨€ç–è‡ªç¼–ç å™¨åœ¨åˆ†ææ½œåœ¨ç‰¹å¾æ—¶å¿½è§†è¾“å‡ºä¾§å› æœå½±å“çš„é—®é¢˜ï¼Œå¯¼è‡´å¯¹æ¨¡å‹è¾“å‡ºçš„ç†è§£ä¸å¤Ÿå…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºGradient Sparse Autoencoderï¼Œé€šè¿‡ç»“åˆè¾“å‡ºä¾§çš„æ¢¯åº¦ä¿¡æ¯ï¼Œè¯†åˆ«å‡ºå¯¹æ¨¡å‹è¾“å‡ºå½±å“æœ€å¤§çš„æ½œåœ¨ç‰¹å¾ï¼Œä»¥æ­¤æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¼•å¯¼èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGradSAEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å±‚ã€ç¨€ç–ç¼–ç å±‚å’Œè¾“å‡ºå±‚ã€‚é¦–å…ˆï¼Œè¾“å…¥æ•°æ®ç»è¿‡ç¨€ç–ç¼–ç å±‚è¿›è¡Œç‰¹å¾æå–ï¼Œç„¶åé€šè¿‡è¾“å‡ºå±‚è®¡ç®—æ¨¡å‹è¾“å‡ºï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦ä¿¡æ¯è¿›è¡Œæ½œåœ¨ç‰¹å¾çš„å½±å“åŠ›è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šGradSAEçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥è¾“å‡ºä¾§æ¢¯åº¦ä¿¡æ¯æ¥è¯„ä¼°æ½œåœ¨ç‰¹å¾çš„å½±å“åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–è¾“å…¥ä¾§æ¿€æ´»çš„åˆ†ææ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨GradSAEä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡é‡æ„è¯¯å·®ä¸ç¨€ç–æ€§çº¦æŸï¼ŒåŒæ—¶é‡‡ç”¨äº†æ”¹è¿›çš„ç½‘ç»œç»“æ„ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGradSAEåœ¨è¯†åˆ«æ½œåœ¨ç‰¹å¾çš„æœ‰æ•ˆæ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼Œæ¨¡å‹è¾“å‡ºçš„å¯è§£é‡Šæ€§æå‡äº†çº¦20%ï¼Œå¹¶ä¸”åœ¨ç‰¹å¾å¼•å¯¼æ¨¡å‹æ€§èƒ½æ–¹é¢ä¹Ÿå–å¾—äº†æ˜æ˜¾çš„æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒè¯†åˆ«ç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§åˆ†æã€‚é€šè¿‡è¯†åˆ«å½±å“æ¨¡å‹è¾“å‡ºçš„å…³é”®æ½œåœ¨ç‰¹å¾ï¼ŒGradSAEå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¥½åœ°ç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­æå‡æ¨¡å‹çš„å¯é æ€§å’Œé€æ˜åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sparse Autoencoders (SAEs) have recently emerged as powerful tools for interpreting and steering the internal representations of large language models (LLMs). However, conventional approaches to analyzing SAEs typically rely solely on input-side activations, without considering the causal influence between each latent feature and the model's output. This work is built on two key hypotheses: (1) activated latents do not contribute equally to the construction of the model's output, and (2) only latents with high causal influence are effective for model steering. To validate these hypotheses, we propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method that identifies the most influential latents by incorporating output-side gradient information.

