---
layout: default
title: Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains
---

# Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07274" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07274v1</a>
  <a href="https://arxiv.org/pdf/2505.07274.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07274v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07274v1', 'Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**æœŸåˆŠ**: Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¼“å­˜é«˜æ•ˆçš„åéªŒé‡‡æ ·æ¡†æ¶ä»¥é™ä½RLè®¡ç®—æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºå¼ºåŒ–å­¦ä¹ æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç¼“å­˜æœºåˆ¶ï¼Œé€šè¿‡å…ƒä¼˜åŒ–ç¼“å­˜å‚æ•°ï¼Œæ˜¾è‘—æé«˜äº†åéªŒé‡‡æ ·çš„æ•ˆç‡ï¼Œé€‚ç”¨äºç¦»æ•£å’Œè¿ç»­æ§åˆ¶é¢†åŸŸã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†3.8-4.7å€çš„LLMæŸ¥è¯¢å‡å°‘å’Œ4.0-12.0å€çš„å»¶è¿Ÿé™ä½ï¼ŒåŒæ—¶ä¿æŒäº†96-98%çš„æ€§èƒ½ã€‚
4. method_zhâ€: â€œ**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå…ˆéªŒä¿¡æ¯æ—¶çš„é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç¯å¢ƒæ—¶ï¼Œå¾€å¾€éœ€è¦é¢‘ç¹è°ƒç”¨LLMï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚\n\n**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”ç¼“å­˜çš„åéªŒé‡‡æ ·æ¡†æ¶ï¼Œé€šè¿‡å…ƒä¼˜åŒ–ç¼“å­˜å‚æ•°ï¼Œåˆ©ç”¨ä»£ç†æ¢¯åº¦æ¥æå‡æ€§èƒ½ï¼Œä»è€Œé™ä½LLMçš„æŸ¥è¯¢é¢‘ç‡å’Œå»¶è¿Ÿã€‚\n\n**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è‡ªé€‚åº”ç¼“å­˜æœºåˆ¶ã€åéªŒé‡‡æ ·æ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚è‡ªé€‚åº”ç¼“å­˜æœºåˆ¶æ ¹æ®ç­–ç•¥æ€§èƒ½åŠ¨æ€è°ƒæ•´ç¼“å­˜å‚æ•°ï¼Œç¡®ä¿é«˜æ•ˆæ¨ç†ã€‚\n\n**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè‡ªé€‚åº”ç¼“å­˜æœºåˆ¶çš„è®¾è®¡ï¼Œä½¿å¾—åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹LLMçš„æŸ¥è¯¢éœ€æ±‚ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„é™æ€ç¼“å­˜ç­–ç•¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚\n\n**å…³é”®è®¾è®¡**ï¼šå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬ç¼“å­˜å¤§å°ã€æ›´æ–°é¢‘ç‡ç­‰ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº†åŸºäºKLæ•£åº¦çš„è¿‘ä¼¼è´¨é‡è¯„ä¼°ï¼Œç¡®ä¿äº†åœ¨ä¸åŒç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚
5. application_zhâ€: â€œè¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨äººæ§åˆ¶å’Œæ¸¸æˆAIç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚â€
6. highlight_zhâ€: â€œå®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ¡†æ¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†3.8-4.7å€çš„LLMæŸ¥è¯¢å‡å°‘å’Œ4.0-12.0å€çš„å»¶è¿Ÿé™ä½ï¼ŒåŒæ—¶ä¿æŒäº†96-98%çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨ç¦»çº¿RLä¸­ï¼ŒCQL-Priorå˜ä½“æå‡äº†14-29%çš„æ€§èƒ½ï¼Œå¹¶å‡å°‘äº†38-40%çš„è®­ç»ƒæ—¶é—´ã€‚â€
7. tags_zhâ€: [
8. ç¼“å­˜æœºåˆ¶
9. åéªŒé‡‡æ ·
10. å¼ºåŒ–å­¦ä¹ 
11. å¤§å‹è¯­è¨€æ¨¡å‹
12. è‡ªé€‚åº”ä¼˜åŒ–
13. ç¦»çº¿å¼ºåŒ–å­¦ä¹ 
14. æ€§èƒ½æå‡

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å…ˆéªŒä¿¡æ¯ï¼Œè™½ç„¶èƒ½å¸¦æ¥æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†ä¹Ÿä¼´éšé«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸåˆ™æ€§çš„ç¼“å­˜é«˜æ•ˆæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”ç¼“å­˜æœºåˆ¶ï¼Œæ˜¾è‘—é™ä½äº†LLMæŸ¥è¯¢æ¬¡æ•°å’Œå»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ç¦»çº¿RLä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œæå‡äº†14-29%çš„æ€§èƒ½ï¼Œå¹¶å‡å°‘äº†38-40%çš„è®­ç»ƒæ—¶é—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Integrating large language models (LLMs) as priors in reinforcement learning (RL) offers significant advantages but comes with substantial computational costs. We present a principled cache-efficient framework for posterior sampling with LLM-derived priors that dramatically reduces these costs while maintaining high performance. At the core of our approach is an adaptive caching mechanism, where cache parameters are meta-optimized using surrogate gradients derived from policy performance. This design enables efficient inference across both discrete text environments (e.g., TextWorld, ALFWorld) and continuous control domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU) while retaining 96--98\% of uncached performance. Our theoretical analysis provides KL divergence bounds on approximation quality, validated empirically. The framework extends to offline RL, where our CQL-Prior variant improves performance by 14--29\% and reduces training time by 38--40\%. Extensive evaluations across a diverse suite of eight tasks demonstrate the generalizability and practical viability of LLM-guided RL in resource-constrained settings.

