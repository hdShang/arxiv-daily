---
layout: default
title: Simple yet Effective Semi-supervised Knowledge Distillation from Vision-Language Models via Dual-Head Optimization
---

# Simple yet Effective Semi-supervised Knowledge Distillation from Vision-Language Models via Dual-Head Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07675" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07675v2</a>
  <a href="https://arxiv.org/pdf/2505.07675.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07675v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07675v2', 'Simple yet Effective Semi-supervised Knowledge Distillation from Vision-Language Models via Dual-Head Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seongjae Kang, Dong Bok Lee, Hyungjoon Jang, Sung Ju Hwang

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: 38 pages, 17 figures, preprint

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/erjui/DHO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒå¤´ä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³çŸ¥è¯†è’¸é¦ä¸­çš„æ¢¯åº¦å†²çªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŠç›‘ç£å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `è§†è§‰-è¯­è¨€æ¨¡å‹` `åŒå¤´ä¼˜åŒ–` `ç‰¹å¾å­¦ä¹ ` `æ¨¡å‹æ³›åŒ–` `è®¡ç®—æœºè§†è§‰` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨ç›‘ç£æŸå¤±ä¸è’¸é¦æŸå¤±ä¹‹é—´å­˜åœ¨æ¢¯åº¦å†²çªï¼Œå½±å“äº†æ¨¡å‹æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºåŒå¤´ä¼˜åŒ–ï¼ˆDHOï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åŒé‡é¢„æµ‹å¤´æ¥è§£å†³æ¢¯åº¦å†²çªé—®é¢˜ï¼Œä»è€Œæå‡ç‰¹å¾å­¦ä¹ æ•ˆæœã€‚
3. åœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDHOæ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦åŸºçº¿ï¼Œä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é€šè¿‡åˆ©ç”¨æœªæ ‡è®°æ•°æ®æ¥åº”å¯¹æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¿‘å¹´æ¥ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤§é‡å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„SSLæ–¹æ³•ã€‚ä¸ºæœ‰æ•ˆåˆ©ç”¨VLMçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†åŒå¤´ä¼˜åŒ–ï¼ˆDHOï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç›‘ç£æŸå¤±ä¸è’¸é¦æŸå¤±ä¹‹é—´çš„æ¢¯åº¦å†²çªã€‚DHOé€šè¿‡ä¸ºæ¯ä¸ªä¿¡å·å¼•å…¥åŒé‡é¢„æµ‹å¤´ï¼Œæ˜¾è‘—æ”¹å–„äº†ç‰¹å¾å­¦ä¹ æ•ˆæœï¼Œä¸”åœ¨è®¡ç®—å¼€é”€å’Œæµ‹è¯•æ—¶è¶…å‚æ•°è°ƒä¼˜æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDHOåœ¨15ä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºä¼ ç»ŸKDåŸºçº¿ï¼Œä¸”åœ¨ImageNetåŠç›‘ç£å­¦ä¹ å’Œè·¨ImageNetå˜ä½“çš„å¤–éƒ¨æ³›åŒ–ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä¼˜æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­ç›‘ç£æŸå¤±ä¸è’¸é¦æŸå¤±ä¹‹é—´çš„æ¢¯åº¦å†²çªé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœªæ ‡è®°æ•°æ®æ—¶ï¼Œå¸¸å¸¸æ— æ³•æœ‰æ•ˆåˆ©ç”¨VLMçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŒå¤´ä¼˜åŒ–ï¼ˆDHOï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªä¿¡å·å¼•å…¥åŒé‡é¢„æµ‹å¤´ï¼Œåˆ†åˆ«å¤„ç†ç›‘ç£å’Œè’¸é¦ä¿¡å·ï¼Œä»è€Œé¿å…æ¢¯åº¦å†²çªï¼Œæå‡ç‰¹å¾å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDHOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€ä¸ªç”¨äºç›‘ç£å­¦ä¹ çš„é¢„æµ‹å¤´ï¼Œå¦ä¸€ä¸ªç”¨äºçŸ¥è¯†è’¸é¦çš„é¢„æµ‹å¤´ã€‚æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶ä¼˜åŒ–è¿™ä¸¤ä¸ªå¤´ï¼Œç¡®ä¿å„è‡ªä¿¡å·çš„ç‹¬ç«‹æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDHOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥åŒé‡é¢„æµ‹å¤´è®¾è®¡ï¼Œè§£å†³äº†ä¼ ç»Ÿå•å¤´è’¸é¦æ–¹æ³•ä¸­å­˜åœ¨çš„æ¢¯åº¦å†²çªé—®é¢˜ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ç‰¹å¾å­¦ä¹ ä¸Šè¡¨ç°æ›´ä¸ºä¼˜è¶Šã€‚

**å…³é”®è®¾è®¡**ï¼šDHOåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šé‡‡ç”¨äº†ç‹¬ç«‹çš„ç›‘ç£æŸå¤±å’Œè’¸é¦æŸå¤±ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šå®ç°äº†åŒå¤´æ¶æ„ã€‚è¯¥æ–¹æ³•åœ¨è®¡ç®—å¼€é”€ä¸Šä¿æŒäº†ä½æˆæœ¬ï¼ŒåŒæ—¶æ”¯æŒæµ‹è¯•æ—¶çš„è¶…å‚æ•°è°ƒä¼˜ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDHOåœ¨15ä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦åŸºçº¿ï¼Œä¸”åœ¨ImageNetåŠç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒDHOæ–¹æ³•çš„æ€§èƒ½ç”šè‡³è¶…è¿‡äº†è¾ƒå¤§çš„æ•™å¸ˆæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å°æ¨¡å‹è®­ç»ƒä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†åŠå…¶äº¤å‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼ŒDHOæ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Semi-supervised learning (SSL) has emerged as a practical solution for addressing data scarcity challenges by leveraging unlabeled data. Recently, vision-language models (VLMs), pre-trained on massive image-text pairs, have demonstrated remarkable zero-/few-shot performance that often surpasses SSL approaches due to their exceptional generalization capabilities. This gap motivates us to question: how can we effectively harness the powerful generalization capabilities of VLMs into task-specific models? Knowledge distillation (KD) offers a natural framework for transferring VLM capabilities, but we identify that it suffers from gradient conflicts between supervised and distillation losses. To address this challenge, we propose Dual-Head Optimization (DHO), which introduces dual prediction heads for each distinct signal. We observe that DHO resolves gradient conflicts, enabling improved feature learning compared to single-head KD baselines, with practical benefits of minimal computational overhead and test-time hyperparameter tuning without retraining. Extensive experiments across 15 datasets show that DHO consistently outperforms KD baselines, often outperforming teacher models with smaller student models. DHO also achieves new state-of-the-art performance on both in-distribution ImageNet semi-supervised learning and out-of-distribution generalization across ImageNet variants. We publicly release our code and model checkpoints to facilitate future research at https://github.com/erjui/DHO.

