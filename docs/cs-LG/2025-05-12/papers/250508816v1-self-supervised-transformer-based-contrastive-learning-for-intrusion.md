---
layout: default
title: Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems
---

# Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08816" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08816v1</a>
  <a href="https://arxiv.org/pdf/2505.08816.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08816v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08816v1', 'Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ippokratis Koukoulis, Ilias Syrigos, Thanasis Korakis

**åˆ†ç±»**: cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: Accepted at IFIP Networking 2025. Code available at https://github.com/koukipp/contrastive_transformers_ids

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„å˜æ¢å™¨æ¨¡å‹ä»¥æå‡å…¥ä¾µæ£€æµ‹ç³»ç»Ÿæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å…¥ä¾µæ£€æµ‹` `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `å˜æ¢å™¨æ¨¡å‹` `ç½‘ç»œå®‰å…¨` `å¼‚å¸¸æ£€æµ‹` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿä¾èµ–æ ‡æ³¨æ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥å¤„ç†æœªè§æµé‡æ¨¡å¼ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå˜æ¢å™¨çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ•°æ®åŒ…çº§æ•°æ®å¢å¼ºè‡ªåŠ¨å­¦ä¹ æµé‡è¡¨ç¤ºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨åŒä¸€æ•°æ®é›†ä¸Šå¼‚å¸¸æ£€æµ‹AUCæå‡3%ï¼Œè·¨æ•°æ®é›†æå‡å¯è¾¾20%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ•°å­—ç¯å¢ƒæ—¥ç›Šäº’è”ï¼Œé›¶æ—¥æ”»å‡»çš„é¢‘ç‡å’Œä¸¥é‡æ€§æ˜¾è‘—å¢åŠ ï¼Œè¿«åˆ‡éœ€è¦åˆ›æ–°çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿï¼ˆIDSï¼‰ã€‚åŸºäºæœºå™¨å­¦ä¹ çš„IDSèƒ½å¤Ÿä»ç½‘ç»œæµé‡ç‰¹å¾ä¸­å­¦ä¹ å¹¶åŒºåˆ†æ”»å‡»æ¨¡å¼ä¸æ­£å¸¸æµé‡ï¼Œæä¾›äº†æ¯”ä¼ ç»ŸåŸºäºç­¾åçš„IDSæ›´å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨é‡åˆ°æœªè§æµé‡æ¨¡å¼æ—¶çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå˜æ¢å™¨ç¼–ç å™¨çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä¸“é—¨é’ˆå¯¹åŸå§‹æ•°æ®åŒ…åºåˆ—çš„å¯æ³›åŒ–å…¥ä¾µæ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆæ•°æ®åŒ…çº§æ•°æ®å¢å¼ºç­–ç•¥ä¸å˜æ¢å™¨æ¶æ„ï¼Œè‡ªåŠ¨å­¦ä¹ å…¨é¢çš„æ•°æ®åŒ…åºåˆ—è¡¨ç¤ºï¼Œæ˜¾è‘—æå‡å¼‚å¸¸è¯†åˆ«ä»»åŠ¡å’Œå…¥ä¾µæ£€æµ‹çš„ç›‘ç£å­¦ä¹ æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å…¥ä¾µæ£€æµ‹ç³»ç»Ÿåœ¨å¤„ç†æœªè§æµé‡æ¨¡å¼æ—¶çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„ç»Ÿè®¡ç‰¹å¾ï¼Œéš¾ä»¥é€‚åº”å¿«é€Ÿå˜åŒ–çš„ç½‘ç»œç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œç»“åˆå˜æ¢å™¨æ¶æ„ï¼Œè‡ªåŠ¨å­¦ä¹ æ•°æ®åŒ…åºåˆ—çš„æ·±å±‚è¡¨ç¤ºï¼Œä»è€Œæé«˜å¯¹å¼‚å¸¸æµé‡çš„æ£€æµ‹èƒ½åŠ›ã€‚æ­¤è®¾è®¡æ—¨åœ¨å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®åŒ…çº§æ•°æ®å¢å¼ºæ¨¡å—å’Œå˜æ¢å™¨ç¼–ç å™¨ã€‚æ•°æ®å¢å¼ºæ¨¡å—ç”Ÿæˆå¤šæ ·åŒ–çš„è¾“å…¥ï¼Œå˜æ¢å™¨ç¼–ç å™¨åˆ™è´Ÿè´£æå–å’Œç”Ÿæˆæµé‡çš„æœ‰æ„ä¹‰è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†è‡ªç›‘ç£å­¦ä¹ ä¸å˜æ¢å™¨æ¨¡å‹ç»“åˆï¼Œè‡ªåŠ¨å­¦ä¹ æµé‡è¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜äº†å¼‚å¸¸æ£€æµ‹çš„æ€§èƒ½ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„åŸºäºNetFlowçš„æ‰‹å·¥ç‰¹å¾æå–æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¯¹æ¯”å­¦ä¹ æ•ˆæœï¼Œå¹¶åœ¨å˜æ¢å™¨æ¶æ„ä¸­è°ƒæ•´äº†å±‚æ•°å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥é€‚åº”æ•°æ®åŒ…åºåˆ—çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åœ¨åŒä¸€æ•°æ®é›†ä¸Šå¼‚å¸¸æ£€æµ‹çš„AUCæå‡äº†3%ï¼Œåœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­æå‡äº†é«˜è¾¾20%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„ç›‘ç£å­¦ä¹ è¡¨ç°ä¹Ÿä¼˜äºç°æœ‰çš„è‡ªç›‘ç£NetFlowæ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°1.5%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œå®‰å…¨ã€å®æ—¶å…¥ä¾µæ£€æµ‹å’Œæµé‡åˆ†æç­‰ã€‚é€šè¿‡æå‡å…¥ä¾µæ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²èŒƒç½‘ç»œæ”»å‡»ï¼Œä¿æŠ¤ç”¨æˆ·æ•°æ®å®‰å…¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œé•¿è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As the digital landscape becomes more interconnected, the frequency and severity of zero-day attacks, have significantly increased, leading to an urgent need for innovative Intrusion Detection Systems (IDS). Machine Learning-based IDS that learn from the network traffic characteristics and can discern attack patterns from benign traffic offer an advanced solution to traditional signature-based IDS. However, they heavily rely on labeled datasets, and their ability to generalize when encountering unseen traffic patterns remains a challenge. This paper proposes a novel self-supervised contrastive learning approach based on transformer encoders, specifically tailored for generalizable intrusion detection on raw packet sequences. Our proposed learning scheme employs a packet-level data augmentation strategy combined with a transformer-based architecture to extract and generate meaningful representations of traffic flows. Unlike traditional methods reliant on handcrafted statistical features (NetFlow), our approach automatically learns comprehensive packet sequence representations, significantly enhancing performance in anomaly identification tasks and supervised learning for intrusion detection. Our transformer-based framework exhibits better performance in comparison to existing NetFlow self-supervised methods. Specifically, we achieve up to a 3% higher AUC in anomaly detection for intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset evaluation. Moreover, our model provides a strong baseline for supervised intrusion detection with limited labeled data, exhibiting an improvement over self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated on the same dataset. Additionally, we show the adaptability of our pretrained model when fine-tuned across different datasets, demonstrating strong performance even when lacking benign data from the target domain.

