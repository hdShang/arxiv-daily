---
layout: default
title: Online Episodic Convex Reinforcement Learning
---

# Online Episodic Convex Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07303" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07303v1</a>
  <a href="https://arxiv.org/pdf/2505.07303.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07303v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07303v1', 'Online Episodic Convex Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bianca Marin Moreno, Khaled Eldowa, Pierre Gaillard, Margaux BrÃ©gÃ¨re, Nadia Oudjane

**åˆ†ç±»**: cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ¨çº¿å‡¸å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥è§£å†³CURLé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å‡¸å¼ºåŒ–å­¦ä¹ ` `åœ¨çº¿å­¦ä¹ ` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å¸¦å­ä¼˜åŒ–` `æ¢ç´¢å¥–åŠ±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å‡¸æŸå¤±æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç»å…¸è´å°”æ›¼æ–¹ç¨‹ä¸å†é€‚ç”¨ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†ä¸€ç§åœ¨çº¿é•œåƒä¸‹é™ç®—æ³•ï¼Œç»“åˆå˜åŒ–çš„çº¦æŸé›†å’Œæ¢ç´¢å¥–åŠ±ï¼Œè§£å†³CURLé—®é¢˜ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨æ²¡æœ‰è¿‡æ¸¡å‡½æ•°å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†è¿‘ä¼¼æœ€ä¼˜çš„é—æ†¾ç•Œé™ï¼Œå¹¶åœ¨å¸¦å­ç‰ˆæœ¬ä¸­å–å¾—äºšçº¿æ€§é—æ†¾ç•Œé™ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ç ”ç©¶äº†åœ¨æœ‰é™æ—¶é—´çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­è¿›è¡Œåœ¨çº¿å­¦ä¹ çš„æƒ…å½¢ï¼Œç‰¹åˆ«æ˜¯å‡¸ç›®æ ‡å‡½æ•°ä¸‹çš„å­¦ä¹ é—®é¢˜ï¼Œå³å‡¹æ•ˆç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆCURLï¼‰é—®é¢˜ã€‚è¯¥è®¾ç½®å°†å¼ºåŒ–å­¦ä¹ ä»çº¿æ€§æŸå¤±æ¨å¹¿åˆ°å‡¸æŸå¤±ï¼Œéçº¿æ€§ç‰¹æ€§ä½¿å¾—ç»å…¸çš„è´å°”æ›¼æ–¹ç¨‹å¤±æ•ˆï¼Œéœ€é‡‡ç”¨æ–°çš„ç®—æ³•æ–¹æ³•ã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œåœ¨æ²¡æœ‰è¿‡æ¸¡å‡½æ•°å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†åœ¨çº¿CURLçš„è¿‘ä¼¼æœ€ä¼˜é—æ†¾ç•Œé™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å…·æœ‰å˜åŒ–çº¦æŸé›†çš„åœ¨çº¿é•œåƒä¸‹é™ç®—æ³•ï¼Œå¹¶è®¾è®¡äº†ç²¾å¿ƒçš„æ¢ç´¢å¥–åŠ±ã€‚æˆ‘ä»¬è¿˜é¦–æ¬¡å¤„ç†äº†CURLçš„å¸¦å­ç‰ˆæœ¬ï¼Œä»…ä¾èµ–äºä»£ç†ç­–ç•¥æ‰€è¯±å¯¼çš„çŠ¶æ€-åŠ¨ä½œåˆ†å¸ƒä¸Šçš„ç›®æ ‡å‡½æ•°å€¼åé¦ˆã€‚é€šè¿‡å°†å¸¦å­å‡¸ä¼˜åŒ–æŠ€æœ¯é€‚åº”äºMDPè®¾ç½®ï¼Œæˆ‘ä»¬ä¸ºè¿™ä¸€æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜å®ç°äº†äºšçº¿æ€§é—æ†¾ç•Œé™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨æœ‰é™æ—¶é—´çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­ï¼Œå‡¸ç›®æ ‡å‡½æ•°ä¸‹çš„åœ¨çº¿å­¦ä¹ é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éçº¿æ€§æŸå¤±æ—¶ï¼Œç»å…¸çš„è´å°”æ›¼æ–¹ç¨‹å¤±æ•ˆï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆå­¦ä¹ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœ¨çº¿é•œåƒä¸‹é™ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰è¿‡æ¸¡å‡½æ•°å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹CURLé—®é¢˜å®ç°è¿‘ä¼¼æœ€ä¼˜çš„é—æ†¾ç•Œé™ã€‚é€šè¿‡è®¾è®¡å˜åŒ–çš„çº¦æŸé›†å’Œæ¢ç´¢å¥–åŠ±ï¼Œå¢å¼ºäº†ç®—æ³•çš„æ¢ç´¢èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åœ¨çº¿é•œåƒä¸‹é™ç®—æ³•çš„å®ç°ï¼Œåˆ†ä¸ºåˆå§‹åŒ–ã€ç­–ç•¥æ›´æ–°ã€çº¦æŸé›†è°ƒæ•´å’Œå¥–åŠ±è®¾è®¡å‡ ä¸ªä¸»è¦æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—ç›¸äº’é…åˆï¼Œç¡®ä¿ç®—æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­æœ‰æ•ˆå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé¦–æ¬¡æå‡ºäº†é’ˆå¯¹CURLé—®é¢˜çš„åœ¨çº¿å­¦ä¹ ç®—æ³•ï¼Œå¹¶åœ¨å¸¦å­ç‰ˆæœ¬ä¸­å®ç°äº†äºšçº¿æ€§é—æ†¾ç•Œé™ã€‚è¿™ä¸€åˆ›æ–°çªç ´äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç®—æ³•ä¸­å…³é”®å‚æ•°åŒ…æ‹¬å˜åŒ–çš„çº¦æŸé›†è®¾è®¡å’Œæ¢ç´¢å¥–åŠ±çš„è®¡ç®—æ–¹å¼ï¼Œç¡®ä¿åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä¼˜åŒ–å­¦ä¹ æ•ˆç‡ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”å‡¸æŸå¤±çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•åœ¨æ²¡æœ‰è¿‡æ¸¡å‡½æ•°å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸå®ç°äº†è¿‘ä¼¼æœ€ä¼˜çš„é—æ†¾ç•Œé™ï¼Œä¸”åœ¨å¸¦å­ç‰ˆæœ¬ä¸­å–å¾—äº†äºšçº¿æ€§é—æ†¾ç•Œé™ã€‚è¿™äº›ç»“æœç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’Œé‡‘èå†³ç­–ç­‰éœ€è¦å®æ—¶å†³ç­–çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆå¤„ç†å‡¸æŸå¤±é—®é¢˜ï¼Œèƒ½å¤Ÿæå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡å’Œå†³ç­–è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study online learning in episodic finite-horizon Markov decision processes (MDPs) with convex objective functions, known as the concave utility reinforcement learning (CURL) problem. This setting generalizes RL from linear to convex losses on the state-action distribution induced by the agent's policy. The non-linearity of CURL invalidates classical Bellman equations and requires new algorithmic approaches. We introduce the first algorithm achieving near-optimal regret bounds for online CURL without any prior knowledge on the transition function. To achieve this, we use an online mirror descent algorithm with varying constraint sets and a carefully designed exploration bonus. We then address for the first time a bandit version of CURL, where the only feedback is the value of the objective function on the state-action distribution induced by the agent's policy. We achieve a sub-linear regret bound for this more challenging problem by adapting techniques from bandit convex optimization to the MDP setting.

