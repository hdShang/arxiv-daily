---
layout: default
title: "TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining"
---

# TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07609" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07609v1</a>
  <a href="https://arxiv.org/pdf/2505.07609.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07609v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07609v1', 'TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Paul Primus, Florian Schmid, Gerhard Widmer

**åˆ†ç±»**: eess.AS, cs.LG, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: submitted to the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2025. Dataset (Zenodo): https://zenodo.org/records/15379789, Implementation (GitHub): https://github.com/OptimusPrimus/tacos

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTACOSä»¥è§£å†³éŸ³é¢‘ä¸æ–‡æœ¬æè¿°çš„æ—¶é—´å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘å¤„ç†` `æ–‡æœ¬æè¿°` `æ—¶é—´å¯¹é½` `å¤šæ¨¡æ€å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `æ•°æ®é›†æ„å»º` `å¸§çº§è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯¹æ¯”è¯­è¨€-éŸ³é¢‘æ¨¡å‹åœ¨è®­ç»ƒæ—¶ä½¿ç”¨å…¨å±€æè¿°ï¼Œå¯¼è‡´æ—¶é—´ç›‘ç£ä¸è¶³ï¼Œå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œå¸§çº§å¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºéŸ³é¢‘ä¸æ–‡æœ¬æè¿°çš„æ—¶é—´å¯¹é½èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ¨¡å‹åœ¨AudioSet StrongåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä»…ä½¿ç”¨å…¨å±€æè¿°çš„æ¨¡å‹ï¼Œæå‡äº†æ—¶é—´å¯¹é½çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨å­¦ä¹ éŸ³é¢‘ä¸æ–‡æœ¬æè¿°ä¹‹é—´çš„å…³è”ï¼Œè¿™å¯¹äºé¢„è®­ç»ƒã€é›¶æ ·æœ¬åˆ†ç±»ã€éŸ³é¢‘æ£€ç´¢ã€éŸ³é¢‘å­—å¹•ç”Ÿæˆç­‰ä»»åŠ¡å…·æœ‰é‡è¦ä»·å€¼ã€‚ç°æœ‰çš„å¯¹æ¯”è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒæ¨¡å‹é€šå¸¸ä½¿ç”¨å…¨å±€ç‰‡æ®µçº§æè¿°ï¼Œæä¾›çš„æ—¶é—´ç›‘ç£è¾ƒå¼±ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«çº¦12000ä¸ªéŸ³é¢‘å½•éŸ³ï¼Œæ¯ä¸ªå½•éŸ³éƒ½é™„æœ‰ä¸ç‰¹å®šæ—¶é—´æ®µç›¸å…³çš„å•å¥è‡ªç”±æ–‡æœ¬æè¿°ã€‚é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¸…ç†è¿™äº›æ³¨é‡Šï¼Œå»é™¤éå¯å¬äº‹ä»¶ã€è½¬å½•è¯­éŸ³ã€æ‹¼å†™é”™è¯¯å’Œæ³¨é‡Šè€…è¯­è¨€åè§ï¼Œè¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å¸§çº§å¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºæ–‡æœ¬æè¿°ä¸éŸ³é¢‘å½•éŸ³æ—¶é—´åŒºåŸŸçš„å¯¹é½èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨å…¨å±€å­—å¹•è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæœ¬æ–‡æ¨¡å‹åœ¨AudioSet StrongåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ—¶é—´å¯¹é½èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¯¹æ¯”è¯­è¨€-éŸ³é¢‘æ¨¡å‹åœ¨æ—¶é—´å¯¹é½æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å…¨å±€æè¿°å¯¼è‡´çš„å¼±æ—¶é—´ç›‘ç£é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«æ—¶é—´æ ‡æ³¨çš„éŸ³é¢‘-æ–‡æœ¬æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨å¸§çº§å¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼Œå¢å¼ºæ¨¡å‹åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ³¨é‡Šæ¸…ç†ã€å¸§çº§å¯¹æ¯”è®­ç»ƒå’Œæ¨¡å‹è¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æä¾›äº†ä¸°å¯Œçš„éŸ³é¢‘å’Œæ–‡æœ¬å¯¹ï¼Œæ³¨é‡Šæ¸…ç†ç¡®ä¿äº†æ•°æ®è´¨é‡ï¼Œå¸§çº§å¯¹æ¯”è®­ç»ƒåˆ™æ˜¯æ ¸å¿ƒå­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†å¸§çº§å¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ›´ç»†ç²’åº¦çš„æ—¶é—´å°ºåº¦ä¸Šå­¦ä¹ éŸ³é¢‘ä¸æ–‡æœ¬çš„å¯¹é½å…³ç³»ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å…¨å±€æè¿°è®­ç»ƒå½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ³¨é‡Šæ¸…ç†ï¼Œä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¸§çº§å¯¹é½ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†é€‚åº”æ€§å­¦ä¹ ç‡è°ƒæ•´æœºåˆ¶ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åœ¨AudioSet StrongåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºä»…ä½¿ç”¨å…¨å±€å­—å¹•çš„æ¨¡å‹ï¼Œæ—¶é—´å¯¹é½èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ï¼Œä½†æå‡å¹…åº¦æ˜æ˜¾ï¼ŒéªŒè¯äº†å¸§çº§å¯¹æ¯”è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éŸ³é¢‘æ£€ç´¢ã€éŸ³é¢‘å­—å¹•ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶éŸ³é¢‘ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜éŸ³é¢‘ä¸æ–‡æœ¬æè¿°çš„æ—¶é—´å¯¹é½èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œäººæœºäº¤äº’ç­‰æ–¹é¢å¸¦æ¥æ›´å¥½çš„ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning to associate audio with textual descriptions is valuable for a range of tasks, including pretraining, zero-shot classification, audio retrieval, audio captioning, and text-conditioned audio generation. Existing contrastive language-audio pretrained models are typically trained using global, clip-level descriptions, which provide only weak temporal supervision. We hypothesize that CLAP-like language-audio models - particularly, if they are expected to produce frame-level embeddings - can benefit from a stronger temporal supervision. To confirm our hypothesis, we curate a novel dataset of approximately 12,000 audio recordings from Freesound, each annotated with single-sentence free-text descriptions linked to a specific temporal segment in an audio recording. We use large language models to clean these annotations by removing references to non-audible events, transcribed speech, typos, and annotator language bias. We further propose a frame-wise contrastive training strategy that learns to align text descriptions with temporal regions in an audio recording and demonstrate that our model has better temporal text-audio alignment abilities compared to models trained only on global captions when evaluated on the AudioSet Strong benchmark. The dataset and our source code are available on Zenodo and GitHub, respectively.

