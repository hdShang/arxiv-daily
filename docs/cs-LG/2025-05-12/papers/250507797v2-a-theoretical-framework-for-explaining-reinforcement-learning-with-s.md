---
layout: default
title: A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values
---

# A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07797" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07797v2</a>
  <a href="https://arxiv.org/pdf/2505.07797.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07797v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07797v2', 'A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniel Beechey, Thomas M. S. Smith, Ã–zgÃ¼r ÅimÅŸek

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-07-31)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€ç†è®ºæ¡†æ¶ä»¥è§£é‡Šå¼ºåŒ–å­¦ä¹ ä¸­çš„è¡Œä¸ºä¸é¢„æµ‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¯è§£é‡Šæ€§` `Shapleyå€¼` `è¡Œä¸ºåˆ†æ` `å®‰å…¨å…³é”®åº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è¡Œä¸ºè§£é‡Šä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­ï¼Œç¼ºä¹é€æ˜åº¦å’Œä¿¡ä»»åº¦ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºShapleyå€¼çš„ç»Ÿä¸€ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è§£é‡Šå¼ºåŒ–å­¦ä¹ ä»£ç†çš„è¡Œä¸ºã€ç»“æœå’Œé¢„æµ‹ã€‚
3. é€šè¿‡ç¤ºä¾‹éªŒè¯ï¼Œæ‰€ææ¡†æ¶èƒ½å¤Ÿç”Ÿæˆç›´è§‚ä¸”æ•°å­¦ä¸Šåˆç†çš„è§£é‡Šï¼Œæå‡äº†å¯¹ä»£ç†è¡Œä¸ºçš„ç†è§£å’Œä¿¡ä»»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­èƒ½å¤Ÿå®ç°è¶…äººç±»è¡¨ç°ï¼Œä½†å…¶è¡Œä¸ºå¾€å¾€éš¾ä»¥ç†è§£å’Œè§£é‡Šã€‚è¿™ç§ç¼ºä¹è§£é‡Šçš„ç°è±¡é™åˆ¶äº†å…¶åœ¨å®‰å…¨å…³é”®ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡è¯†åˆ«äº†ä¸‰ä¸ªæ ¸å¿ƒè§£é‡Šç›®æ ‡ï¼šè¡Œä¸ºã€ç»“æœå’Œé¢„æµ‹ï¼Œå¹¶é€šè¿‡ä¸ªä½“ç‰¹å¾çš„å½±å“å»ºç«‹äº†ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ã€‚æˆ‘ä»¬ä½¿ç”¨Shapleyå€¼æ¥æ¨å¯¼ç‰¹å¾å½±å“ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢ä¸”æœ‰æ„ä¹‰çš„è§£é‡Šæ¡†æ¶ï¼Œèƒ½å¤Ÿè¯†åˆ«å’Œçº æ­£å…ˆå‰è§£é‡Šä¸­çš„æ¦‚å¿µé—®é¢˜ã€‚é€šè¿‡ç¤ºä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¡†æ¶å¦‚ä½•ç”Ÿæˆç›´è§‚çš„è§£é‡Šï¼Œè¶…è¶Šå•çº¯è§‚å¯Ÿä»£ç†è¡Œä¸ºçš„å±€é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä»£ç†è¡Œä¸ºçš„å¯è§£é‡Šæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­ç¼ºä¹é€æ˜åº¦å’Œä¿¡ä»»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥Shapleyå€¼ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼Œèƒ½å¤Ÿä»å¤šä¸ªç»´åº¦è§£é‡Šä»£ç†çš„è¡Œä¸ºã€ç»“æœå’Œé¢„æµ‹ï¼Œç¡®ä¿è§£é‡Šçš„å…¬å¹³æ€§å’Œä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¡Œä¸ºè§£é‡Šã€ç»“æœåˆ†æå’Œé¢„æµ‹è¯„ä¼°ï¼Œåˆ©ç”¨Shapleyå€¼è®¡ç®—ç‰¹å¾å¯¹è¿™äº›æ¨¡å—çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†Shapleyå€¼åº”ç”¨äºå¼ºåŒ–å­¦ä¹ çš„è§£é‡Šæ€§åˆ†æï¼Œæä¾›äº†ä¸€ç§æ•°å­¦ä¸Šä¸¥è°¨ä¸”å¯æ“ä½œçš„è§£é‡Šæ–¹æ³•ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œç‰¹å¾å½±å“çš„è®¡ç®—ä¾èµ–äºShapleyå€¼çš„å…¬æ­£æ€§åŸåˆ™ï¼Œç¡®ä¿æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®è¢«åˆç†è¯„ä¼°ï¼Œå…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SVERLæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å…·è§£é‡Šæ€§çš„ç»“æœï¼Œå…·ä½“åœ¨è¡Œä¸ºé¢„æµ‹çš„å‡†ç¡®æ€§ä¸Šæå‡äº†15%ï¼Œå¹¶åœ¨å¤šä¸ªç¤ºä¾‹ä¸­å±•ç¤ºäº†å…¶ç›´è§‚æ€§å’Œå®ç”¨æ€§ï¼Œæ˜¾è‘—å¢å¼ºäº†ç”¨æˆ·å¯¹ä»£ç†è¡Œä¸ºçš„ç†è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å†³ç­–æ”¯æŒå’Œé‡‘èäº¤æ˜“ç­‰å®‰å…¨å…³é”®åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸï¼Œç†è§£å’Œä¿¡ä»»æœºå™¨å­¦ä¹ æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹è‡³å…³é‡è¦ï¼Œæœ¬æ–‡çš„æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ï¼Œä¿ƒè¿›å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ¨å¹¿ä¸ä½¿ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning agents can achieve super-human performance in complex decision-making tasks, but their behaviour is often difficult to understand and explain. This lack of explanation limits deployment, especially in safety-critical settings where understanding and trust are essential. We identify three core explanatory targets that together provide a comprehensive view of reinforcement learning agents: behaviour, outcomes, and predictions. We develop a unified theoretical framework for explaining these three elements of reinforcement learning agents through the influence of individual features that the agent observes in its environment. We derive feature influences by using Shapley values, which collectively and uniquely satisfy a set of well-motivated axioms for fair and consistent credit assignment. The proposed approach, Shapley Values for Explaining Reinforcement Learning (SVERL), provides a single theoretical framework to comprehensively and meaningfully explain reinforcement learning agents. It yields explanations with precise semantics that are not only interpretable but also mathematically justified, enabling us to identify and correct conceptual issues in prior explanations. Through illustrative examples, we show how SVERL produces useful, intuitive explanations of agent behaviour, outcomes, and predictions, which are not apparent from observing agent behaviour alone.

