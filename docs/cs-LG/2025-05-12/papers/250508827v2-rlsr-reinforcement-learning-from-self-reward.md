---
layout: default
title: "RLSR: Reinforcement Learning from Self Reward"
---

# RLSR: Reinforcement Learning from Self Reward

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08827" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08827v2</a>
  <a href="https://arxiv.org/pdf/2505.08827.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08827v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08827v2', 'RLSR: Reinforcement Learning from Self Reward')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Toby Simonds, Kevin Lopez, Akira Yoshiyama, Dominique Garmier

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-08-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæˆ‘å¥–åŠ±å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³å¥–åŠ±å·¥ç¨‹æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘å¥–åŠ±` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªæˆ‘åˆ¤æ–­` `è‡ªæˆ‘æ”¹è¿›` `æ— ç›‘ç£å­¦ä¹ ` `å¥–åŠ±ä¿¡å·` `å¤æ‚é—®é¢˜æ±‚è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºå¯éªŒè¯çš„å¥–åŠ±ï¼Œè¿™åœ¨è®¸å¤šé¢†åŸŸä¸­éš¾ä»¥å®ç°ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå…è®¸æ¨¡å‹é€šè¿‡è‡ªæˆ‘åˆ¤æ–­ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°è‡ªæˆ‘æ”¹è¿›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨æ²¡æœ‰çœŸå®ç­”æ¡ˆçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨Countdownè°œé¢˜å’Œç§¯åˆ†é—®é¢˜ä¸Šè¾¾åˆ°ä¸æ­£å¼éªŒè¯ç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆå¤æ‚é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒé€šå¸¸éœ€è¦å¯éªŒè¯çš„å¥–åŠ±ï¼Œè¿™äº›å¥–åŠ±çš„åˆ›å»ºæˆæœ¬é«˜ä¸”åœ¨æŸäº›é¢†åŸŸä¸å¯è¡Œã€‚æœ¬æ–‡å±•ç¤ºäº†LLMså¦‚ä½•é€šè¿‡è‡ªæˆ‘åˆ¤æ–­è¿›è¡Œæœ‰æ•ˆçš„è‡ªæˆ‘æ”¹è¿›ï¼Œåˆ©ç”¨ç”Ÿæˆä¸éªŒè¯è§£å†³æ–¹æ¡ˆä¹‹é—´çš„å†…åœ¨ä¸å¯¹ç§°æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®ç­”æ¡ˆçš„æƒ…å†µä¸‹æä¾›å¯é çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œåœ¨å¯éªŒè¯å¥–åŠ±ä¸åˆ‡å®é™…çš„é¢†åŸŸå®ç°å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡åœ¨Countdownè°œé¢˜å’Œç§¯åˆ†é—®é¢˜ä¸Šå®æ–½è‡ªæˆ‘åˆ¤æ–­ï¼Œæˆ‘ä»¬çš„æ€§èƒ½ä¸æ­£å¼éªŒè¯ç›¸å½“ï¼Œä¸”æ— éœ€çœŸå®ç­”æ¡ˆã€‚æœ€æ˜¾è‘—çš„æ˜¯ï¼Œç»è¿‡è‡ªæˆ‘å¥–åŠ±è®­ç»ƒçš„Qwen 2.5 7B DeepSeek Distilledæ¨¡å‹æœ‰èµ„æ ¼å‚åŠ MITç§¯åˆ†ç«èµ›ï¼Œå±•ç°å‡ºè‡ªæˆ‘ç›‘ç£æ”¹è¿›çš„èƒ½åŠ›ã€‚ç»“åˆåˆæˆé—®é¢˜ç”Ÿæˆï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå®Œæ•´çš„è‡ªæˆ‘æ”¹è¿›å¾ªç¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆç»ƒä¹ é—®é¢˜ã€è§£å†³é—®é¢˜å¹¶è¯„ä¼°è‡ªèº«è¡¨ç°ï¼Œæ— éœ€å¤–éƒ¨éªŒè¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMè¯„åˆ¤è€…å¯ä»¥ä¸ºè®­ç»ƒæä¾›æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ï¼Œå¼€å¯äº†åœ¨å¥–åŠ±å·¥ç¨‹æŒ‘æˆ˜é™åˆ¶ä¸‹çš„å¼ºåŒ–å­¦ä¹ æ–°é¢†åŸŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è®¸å¤šé¢†åŸŸä¸­å¯éªŒè¯å¥–åŠ±éš¾ä»¥è·å–çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ­¤æƒ…å†µä¸‹æ— æ³•æœ‰æ•ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è‡ªæˆ‘åˆ¤æ–­ï¼ŒLLMsèƒ½å¤Ÿåœ¨æ²¡æœ‰å¤–éƒ¨å‚è€ƒç­”æ¡ˆçš„æƒ…å†µä¸‹ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°è‡ªæˆ‘æ”¹è¿›å’Œå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ¨¡å‹ç”Ÿæˆé—®é¢˜ã€è§£å†³é—®é¢˜ã€è¯„ä¼°è‡ªèº«è¡¨ç°ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯çš„è‡ªæˆ‘æ”¹è¿›ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å¯¹çœŸå®å¥–åŠ±çš„ä¾èµ–ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤æ‚é¢†åŸŸçš„é€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„å¥–åŠ±ä¿¡å·è¿›è¡Œä¼˜åŒ–ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥é€‚åº”è‡ªæˆ‘åˆ¤æ–­çš„åé¦ˆæœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è‡ªæˆ‘å¥–åŠ±è®­ç»ƒçš„Qwen 2.5 7B DeepSeek Distilledæ¨¡å‹åœ¨Countdownè°œé¢˜å’Œç§¯åˆ†é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸ä¼ ç»Ÿçš„æ­£å¼éªŒè¯æ–¹æ³•ç›¸å½“ï¼Œä¸”æ— éœ€çœŸå®ç­”æ¡ˆï¼Œå±•ç¤ºäº†è‡ªæˆ‘ç›‘ç£å­¦ä¹ çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ¸¸æˆå’Œå¤æ‚é—®é¢˜æ±‚è§£ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿå¸®åŠ©æ¨¡å‹åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚æœªæ¥ï¼Œè¿™ç§è‡ªæˆ‘å¥–åŠ±æœºåˆ¶å¯èƒ½æ¨åŠ¨è‡ªä¸»AIç³»ç»Ÿçš„å‘å±•ï¼Œä½¿å…¶åœ¨å¤šç§é¢†åŸŸä¸­å®ç°æŒç»­è‡ªæˆ‘æ”¹è¿›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models can generate solutions to complex problems, but training them with reinforcement learning typically requires verifiable rewards that are expensive to create and not possible for all domains. We demonstrate that LLMs can effectively self-improve through self-judging without reference solutions, leveraging the inherent asymmetry between generating and verifying solutions. Our experiments show that models can provide reliable reward signals without ground truth answers, enabling reinforcement learning in domains where verifiable rewards are impractical. By implementing self-judging across Countdown puzzles and integration problems, we achieve performance comparable to formal verification without ground truth solutions. Most notably, Qwen 2.5 7B DeepSeek Distilled trained with self-rewards qualifies for the prestigious MIT Integration Bee competition, performance through self-supervised improvement. When combined with synthetic question generation, we establish a complete self-improvement loop where models generate practice problems, solve them, and evaluate their own performance without any external validation. Our findings demonstrate that LLM judges can provide effective reward signals for training, unlocking reinforcement learning in countless domains previously limited by reward engineering challenges. This work represents a significant step toward autonomous AI systems that continuously improve through self-directed learning rather than human-guided training, potentially accelerating progress across domains where training data is scarce or evaluation is complex.

