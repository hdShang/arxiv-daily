---
layout: default
title: An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits
---

# An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08823" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08823v1</a>
  <a href="https://arxiv.org/pdf/2505.08823.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08823v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08823v1', 'An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cody Steinmetz, Gavin Childress, Aaron Herbst, Gavin Jones, Jasdeep Singh, Eli Vang, Keagan Weinstock

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRMSNormä»¥ç¨³å®šå¾®è°ƒè‡³1.58ä½çš„ä½æ¯”ç‰¹é‡åŒ–æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æ¨¡å‹` `RMSå½’ä¸€åŒ–` `è‡ªç„¶è¯­è¨€å¤„ç†` `ä½æ¯”ç‰¹æ¨ç†` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨é™ä½æ¨¡å‹å¤æ‚åº¦çš„åŒæ—¶ï¼Œå¾€å¾€å¯¼è‡´å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ä¸‰å…ƒé‡åŒ–æ—¶æ›´ä¸ºæ˜æ˜¾ã€‚
2. æœ¬æ–‡æå‡ºåœ¨æ¯ä¸ªçº¿æ€§æŠ•å½±å‰æ’å…¥RMSå½’ä¸€åŒ–ï¼Œå¹¶é‡‡ç”¨é€å±‚é‡åŒ–è°ƒåº¦ï¼Œä»¥ç¨³å®šå¾®è°ƒå…¨ç²¾åº¦æ¨¡å‹è‡³ä½æ¯”ç‰¹é‡åŒ–æ¨¡å‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šä¸å¤æ‚çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼Œä¸”æœªå¢åŠ æ¨¡å‹å¤æ‚æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥äº†å˜é©ï¼Œä½†å…¶è§„æ¨¡ä½¿å¾—å®é™…éƒ¨ç½²æˆæœ¬é«˜æ˜‚ã€‚åè®­ç»ƒé‡åŒ–è™½ç„¶èƒ½å‡å°‘å†…å­˜å’Œè®¡ç®—éœ€æ±‚ï¼Œä½†å¸¸å¸¸å¯¼è‡´å‡†ç¡®æ€§ä¸‹é™ï¼Œè€Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒåˆ™éœ€è¦é¢å¤–çš„è®­ç»ƒæ—¶é—´ã€‚å°†é‡åŒ–æ¨å‘ä¸‰å…ƒï¼ˆ2ä½ï¼‰èŒƒå›´å¯ä»¥è·å¾—æ›´å¤§çš„èŠ‚çœï¼Œä½†é€šå¸¸ä¸ç¨³å®šã€‚åŸºäºè¿‘æœŸç ”ç©¶ï¼Œæœ¬æ–‡å±•ç¤ºäº†åœ¨æ¯ä¸ªçº¿æ€§æŠ•å½±å‰æ’å…¥RMSå½’ä¸€åŒ–ï¼Œå¹¶åº”ç”¨é€å±‚é‡åŒ–è°ƒåº¦ï¼Œå¯ä»¥ç¨³å®šåœ°å°†å…¨ç²¾åº¦æ£€æŸ¥ç‚¹å¾®è°ƒä¸ºä¸‰å…ƒLLMsã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šä¸æ›´å¤æ‚çš„çŸ¥è¯†è’¸é¦ç®¡é“ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œè€Œæ— éœ€å¢åŠ æ¨¡å‹å¤æ‚æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»†è‡´çš„å½’ä¸€åŒ–å¯ä»¥ç¼©å°ä¸‰å…ƒä¸å…¨ç²¾åº¦LLMsä¹‹é—´çš„å‡†ç¡®æ€§å·®è·ï¼Œä½¿è¶…ä½æ¯”ç‰¹æ¨ç†å˜å¾—å¯è¡Œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å‡†ç¡®æ€§ä¸‹é™çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨å‘ä¸‰å…ƒé‡åŒ–æ—¶çš„ç¨³å®šæ€§ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•åœ¨é™ä½æ¨¡å‹å¤æ‚åº¦çš„åŒæ—¶ï¼Œå¸¸å¸¸å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåœ¨æ¯ä¸ªçº¿æ€§æŠ•å½±å‰æ’å…¥RMSå½’ä¸€åŒ–ï¼Œå¹¶é‡‡ç”¨é€å±‚é‡åŒ–è°ƒåº¦ï¼Œä»¥ç¨³å®šåœ°å°†å…¨ç²¾åº¦æ£€æŸ¥ç‚¹å¾®è°ƒä¸ºä¸‰å…ƒLLMsã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨é€šè¿‡ç»†è‡´çš„å½’ä¸€åŒ–æ¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å…¨ç²¾åº¦æ¨¡å‹çš„åˆå§‹åŒ–ã€RMSå½’ä¸€åŒ–çš„æ’å…¥ã€é€å±‚é‡åŒ–è°ƒåº¦çš„å®æ–½ï¼Œä»¥åŠæœ€ç»ˆçš„å¾®è°ƒè¿‡ç¨‹ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒè¿‡ç¨‹å’Œè¯„ä¼°é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé€šè¿‡ç®€å•çš„RMSå½’ä¸€åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†ä¸‰å…ƒé‡åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼Œç¼©å°äº†ä¸å…¨ç²¾åº¦æ¨¡å‹ä¹‹é—´çš„å‡†ç¡®æ€§å·®è·ã€‚è¿™ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒRMSå½’ä¸€åŒ–è¢«åº”ç”¨äºæ¯ä¸ªçº¿æ€§æŠ•å½±å±‚ï¼Œé‡åŒ–è°ƒåº¦åˆ™æ˜¯é€å±‚è¿›è¡Œï¼Œç¡®ä¿æ¯ä¸€å±‚çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œè®­ç»ƒç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥é€‚åº”ä½æ¯”ç‰¹é‡åŒ–çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨RMSå½’ä¸€åŒ–çš„å¾®è°ƒæ–¹æ³•åœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šè¾¾åˆ°äº†1.58ä½çš„ç²¾åº¦ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šä¸æ›´å¤æ‚çš„çŸ¥è¯†è’¸é¦ç®¡é“ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æå‡å¹…åº¦ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨ç¿»è¯‘ç­‰éœ€è¦é«˜æ•ˆæ¨ç†çš„åœºæ™¯ã€‚é€šè¿‡å®ç°è¶…ä½æ¯”ç‰¹é‡åŒ–ï¼Œæ¨¡å‹çš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚å¤§å¹…é™ä½ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have transformed natural-language processing, yet their scale makes real-world deployment costly. Post-training quantization reduces memory and computation but often degrades accuracy, while quantization-aware training can recover performance at the cost of extra training. Pushing quantization to the ternary (2-bit) regime yields even larger savings but is notoriously unstable. Building on recent work showing that a bias-free, RMS-normalized Transformer with straight-through estimation can reach 1.58-bit precision, we demonstrate that simply inserting RMS normalization before every linear projection and applying a gradual, layer-wise quantization schedule stably fine-tunes full-precision checkpoints into ternary LLMs. Our approach matches or surpasses more elaborate knowledge-distillation pipelines on standard language-modeling benchmarks without adding model complexity. These results indicate that careful normalization alone can close much of the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.

