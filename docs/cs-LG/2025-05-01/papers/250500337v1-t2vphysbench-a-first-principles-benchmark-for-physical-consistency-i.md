---
layout: default
title: "T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation"
---

# T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00337" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00337v1</a>
  <a href="https://arxiv.org/pdf/2505.00337.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00337v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00337v1', 'T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, Jiale Zhao

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºT2VPhysBenchä»¥è¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ç†ä¸€è‡´æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ` `ç‰©ç†ä¸€è‡´æ€§` `è¯„ä¼°åŸºå‡†` `äººç±»è¯„ä¼°` `ç‰›é¡¿åŠ›å­¦` `èƒ½é‡å®ˆæ’` `ç°è±¡æ•ˆåº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨éµå¾ªåŸºæœ¬ç‰©ç†æ³•åˆ™æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ä¸çœŸå®æˆ–è¯¯å¯¼ã€‚
2. T2VPhysBenchåŸºå‡†é€šè¿‡ç³»ç»Ÿè¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¯å¦éµå¾ªæ ¸å¿ƒç‰©ç†æ³•åˆ™ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ç©ºç™½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰æ¨¡å‹åœ¨ç‰©ç†æ³•åˆ™çš„åˆè§„æ€§è¯„ä¼°ä¸­å¹³å‡å¾—åˆ†ä½äº0.60ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œå…¼é¡¾ç¾å­¦å’ŒæŒ‡ä»¤éµå¾ªã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨éµå¾ªåŸºæœ¬ç‰©ç†æ³•åˆ™æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æµ‹è¯•ï¼Œè®¸å¤šè¾“å‡ºä»ç„¶è¿ååŸºæœ¬çº¦æŸï¼Œå¦‚åˆšä½“ç¢°æ’ã€èƒ½é‡å®ˆæ’å’Œé‡åŠ›åŠ¨æ€ã€‚ç°æœ‰çš„ç‰©ç†è¯„ä¼°åŸºå‡†é€šå¸¸ä¾èµ–äºè‡ªåŠ¨åŒ–çš„åƒç´ çº§æŒ‡æ ‡ï¼Œå¿½è§†äº†äººç±»åˆ¤æ–­å’Œç¬¬ä¸€æ€§åŸç†ç‰©ç†ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†T2VPhysBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿè¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘ç³»ç»Ÿæ˜¯å¦éµå¾ªåŒ…æ‹¬ç‰›é¡¿åŠ›å­¦ã€å®ˆæ’åŸåˆ™å’Œç°è±¡æ•ˆåº”åœ¨å†…çš„åäºŒé¡¹æ ¸å¿ƒç‰©ç†æ³•åˆ™çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„åŸºå‡†é‡‡ç”¨ä¸¥æ ¼çš„äººç±»è¯„ä¼°åè®®ï¼Œå¹¶åŒ…æ‹¬ä¸‰é¡¹é’ˆå¯¹æ€§ç ”ç©¶ï¼Œç»“æœæ­ç¤ºäº†å½“å‰æ¶æ„çš„æŒç»­å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å…·ä½“çš„æŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨éµå¾ªç‰©ç†æ³•åˆ™æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–äºç®€å•çš„åƒç´ çº§è¯„ä¼°ï¼Œç¼ºä¹å¯¹äººç±»åˆ¤æ–­å’Œç‰©ç†åŸåˆ™çš„è€ƒé‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºT2VPhysBenchåŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°æ¨¡å‹æ˜¯å¦éµå¾ªåäºŒé¡¹æ ¸å¿ƒç‰©ç†æ³•åˆ™ï¼Œé‡‡ç”¨ä¸¥æ ¼çš„äººç±»è¯„ä¼°æ–¹æ³•ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥åŸºå‡†åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ•´ä½“åˆè§„æ€§è¯„ä¼°ã€æç¤ºæ¶ˆèå®éªŒå’Œåäº‹å®é²æ£’æ€§æµ‹è¯•ï¼Œæ—¨åœ¨å…¨é¢åˆ†ææ¨¡å‹åœ¨ç‰©ç†æ³•åˆ™éµå¾ªæ–¹é¢çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šT2VPhysBenchçš„åˆ›æ–°åœ¨äºå…¶ç³»ç»Ÿæ€§å’Œç¬¬ä¸€æ€§åŸç†çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒºåˆ«äºç°æœ‰çš„è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œå¼ºè°ƒäº†äººç±»è¯„ä¼°çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†è¯¦ç»†çš„æç¤ºå’Œè¯„ä¼°æ ‡å‡†ï¼Œç¡®ä¿èƒ½å¤Ÿå‡†ç¡®æ•æ‰æ¨¡å‹åœ¨ç‰©ç†æ³•åˆ™éµå¾ªä¸Šçš„è¡¨ç°ï¼Œå¹¶é€šè¿‡å¤šè½®è¯„ä¼°æé«˜ç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹åœ¨ç‰©ç†æ³•åˆ™çš„åˆè§„æ€§ä¸Šå¹³å‡å¾—åˆ†ä½äº0.60ï¼Œè¡¨æ˜å½“å‰æŠ€æœ¯åœ¨éµå¾ªç‰©ç†æ³•åˆ™æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚æ­¤å¤–ï¼Œè¯¦ç»†çš„æç¤ºå¹¶æœªæ˜¾è‘—æ”¹å–„æ¨¡å‹çš„ç‰©ç†ä¸€è‡´æ€§ï¼Œæ­ç¤ºäº†æ¨¡å‹æ¶æ„çš„æ ¹æœ¬å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°å­—è‰ºæœ¯åˆ›ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…ç”Ÿæˆæ›´ç¬¦åˆç‰©ç†è§„å¾‹çš„å†…å®¹ï¼Œæé«˜ç”¨æˆ·ä½“éªŒå’Œå‚ä¸åº¦ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†æœ‰æœ›æ¨åŠ¨æ–‡æœ¬åˆ°è§†é¢‘ç”ŸæˆæŠ€æœ¯å‘æ›´é«˜çš„ç‰©ç†ä¸€è‡´æ€§å’ŒçœŸå®æ„Ÿå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. Yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. Existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. To fill this gap, we introduce \textbf{T2VPhysBench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including Newtonian mechanics, conservation principles, and phenomenological effects. Our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. The results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.

