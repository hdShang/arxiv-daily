---
layout: default
title: Variational OOD State Correction for Offline Reinforcement Learning
---

# Variational OOD State Correction for Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00503" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00503v3</a>
  <a href="https://arxiv.org/pdf/2505.00503.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00503v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00503v3', 'Variational OOD State Correction for Offline Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ke Jiang, Wen Jiang, Xiaoyang Tan

**åˆ†ç±»**: cs.LG, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01 (æ›´æ–°: 2025-07-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDensity-Aware Safety Perceptionä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„çŠ¶æ€åˆ†å¸ƒåç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `çŠ¶æ€åˆ†å¸ƒåç§»` `OODçŠ¶æ€æ ¡æ­£` `å¯†åº¦æ„ŸçŸ¥` `å®‰å…¨å†³ç­–` `å˜åˆ†æ¡†æ¶` `æ™ºèƒ½ä½“å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»çº¿å¼ºåŒ–å­¦ä¹ é¢ä¸´çŠ¶æ€åˆ†å¸ƒåç§»çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†OODçŠ¶æ€æ—¶æ•ˆæœä¸ä½³ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„DASPæ–¹æ³•é€šè¿‡ä¼˜åŒ–å†³ç­–ç»“æœçš„å¯†åº¦ï¼Œé¼“åŠ±æ™ºèƒ½ä½“é€‰æ‹©æ›´å®‰å…¨çš„åŠ¨ä½œï¼Œä»è€Œæ”¹å–„çŠ¶æ€æ ¡æ­£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDASPåœ¨ç¦»çº¿MuJoCoå’ŒAntMazeç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„è¡¨ç°ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½å—åˆ°çŠ¶æ€åˆ†å¸ƒåç§»é—®é¢˜çš„æ˜¾è‘—å½±å“ï¼Œè€Œç¦»ç¾¤çŠ¶æ€ï¼ˆOODï¼‰æ ¡æ­£æ˜¯è§£å†³è¯¥é—®é¢˜çš„å¸¸ç”¨æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç§°ä¸ºå¯†åº¦æ„ŸçŸ¥å®‰å…¨æ„ŸçŸ¥ï¼ˆDASPï¼‰ï¼Œç”¨äºOODçŠ¶æ€æ ¡æ­£ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¼“åŠ±æ™ºèƒ½ä½“ä¼˜å…ˆé€‰æ‹©å¯¼è‡´é«˜æ•°æ®å¯†åº¦ç»“æœçš„åŠ¨ä½œï¼Œä»è€Œä¿ƒè¿›å…¶åœ¨åˆ†å¸ƒå†…ï¼ˆå®‰å…¨ï¼‰åŒºåŸŸå†…æ“ä½œæˆ–è¿”å›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨å˜åˆ†æ¡†æ¶å†…ä¼˜åŒ–ç›®æ ‡ï¼ŒåŒæ—¶è€ƒè™‘å†³ç­–çš„æ½œåœ¨ç»“æœåŠå…¶å¯†åº¦ï¼Œä»è€Œä¸ºå®‰å…¨å†³ç­–æä¾›é‡è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ç¦»çº¿MuJoCoå’ŒAntMazeå¥—ä»¶ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯è¡Œæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„çŠ¶æ€åˆ†å¸ƒåç§»é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†OODçŠ¶æ€æ—¶å¾€å¾€æ— æ³•æœ‰æ•ˆæ ¡æ­£ï¼Œå¯¼è‡´æ™ºèƒ½ä½“æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDASPæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¼•å¯¼æ™ºèƒ½ä½“ä¼˜å…ˆé€‰æ‹©é«˜æ•°æ®å¯†åº¦çš„åŠ¨ä½œï¼Œç¡®ä¿å…¶åœ¨å®‰å…¨åŒºåŸŸå†…æ“ä½œï¼Œä»è€Œå‡å°‘OODçŠ¶æ€çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨å˜åˆ†æ¡†æ¶ï¼Œä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè¯„ä¼°å†³ç­–çš„æ½œåœ¨ç»“æœåŠå…¶æ•°æ®å¯†åº¦ï¼Œå…¶æ¬¡ä¼˜åŒ–æ™ºèƒ½ä½“çš„å†³ç­–ç­–ç•¥ä»¥æé«˜å®‰å…¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDASPçš„åˆ›æ–°ä¹‹å¤„åœ¨äºåŒæ—¶è€ƒè™‘å†³ç­–ç»“æœçš„å¯†åº¦å’Œæ½œåœ¨åæœï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•å•ä¸€å…³æ³¨ç»“æœä¸åŒï¼Œæä¾›äº†æ›´å…¨é¢çš„å®‰å…¨å†³ç­–æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸­ï¼ŒDASPä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å®‰å…¨æ€§ä¸æ¢ç´¢æ€§ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§è°ƒæ•´çš„ç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿åœ¨ä¸åŒç¯å¢ƒä¸­çµæ´»åº”ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDASPæ–¹æ³•åœ¨ç¦»çº¿MuJoCoå’ŒAntMazeç¯å¢ƒä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾è‘—æé«˜äº†æ™ºèƒ½ä½“åœ¨OODçŠ¶æ€ä¸‹çš„å†³ç­–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæ¸¸æˆAIç­‰åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆçš„OODçŠ¶æ€æ ¡æ­£ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å®‰å…¨åœ°è¿›è¡Œå†³ç­–ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤„ç†çŠ¶æ€åˆ†å¸ƒå˜åŒ–çš„é¢†åŸŸï¼Œå¦‚åŒ»ç–—å†³ç­–å’Œé‡‘èé¢„æµ‹ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites.

