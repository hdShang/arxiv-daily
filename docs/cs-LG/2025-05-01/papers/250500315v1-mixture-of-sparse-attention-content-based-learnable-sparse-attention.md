---
layout: default
title: "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing"
---

# Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00315" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00315v1</a>
  <a href="https://arxiv.org/pdf/2505.00315.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00315v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00315v1', 'Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Piotr PiÄ™kos, RÃ³bert CsordÃ¡s, JÃ¼rgen Schmidhuber

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä»¥è§£å†³è‡ªæ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¨€ç–æ³¨æ„åŠ›` `æ··åˆä¸“å®¶` `è®¡ç®—å¤æ‚åº¦` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨è®¡ç®—å¤æ‚åº¦ä¸Šå­˜åœ¨äºŒæ¬¡æ–¹çš„å¼€é”€ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„æ··åˆç¨€ç–æ³¨æ„åŠ›ï¼ˆMoSAï¼‰é€šè¿‡åŠ¨æ€é€‰æ‹©ä»¤ç‰Œï¼Œå…è®¸çµæ´»çš„ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoSAåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹çš„å›°æƒ‘åº¦æ¯”å¯†é›†åŸºçº¿æé«˜äº†27%ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ—¶æ›´å¿«ã€å†…å­˜å ç”¨æ›´å°‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•çªæ˜¾äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡æ–¹è®¡ç®—æˆæœ¬ã€‚å°½ç®¡å·²æœ‰å¤§é‡ç ”ç©¶åŠªåŠ›ï¼ŒäºšäºŒæ¬¡æ–¹æ³¨æ„åŠ›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­ä»è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡å‡è®¾åŠ¨æ€å­¦ä¹ çš„åŸºäºå†…å®¹çš„ç¨€ç–æ€§å¯ä»¥æé«˜æ³¨æ„åŠ›æœºåˆ¶çš„æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†æ··åˆç¨€ç–æ³¨æ„åŠ›ï¼ˆMoSAï¼‰ï¼Œè¿™ä¸€æ–°æ–¹æ³•å—åˆ°äº†ä¸“å®¶é€‰æ‹©è·¯ç”±çš„å¯å‘ã€‚MoSAåŠ¨æ€é€‰æ‹©æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ä»¤ç‰Œï¼Œå…è®¸ä»»æ„ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ã€‚é€šè¿‡ä»é•¿åº¦ä¸ºTçš„åºåˆ—ä¸­é€‰æ‹©kä¸ªä»¤ç‰Œï¼ŒMoSAå°†æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è®¡ç®—å¤æ‚åº¦ä»O(T^2)é™ä½åˆ°O(k^2 + T)ï¼Œä»è€Œåœ¨ç›¸åŒè®¡ç®—é¢„ç®—å†…ä½¿ç”¨æ›´å¤šçš„å¤´ï¼Œæå‡äº†ä¸“é—¨åŒ–æ°´å¹³ã€‚å®éªŒè¡¨æ˜ï¼ŒMoSAåœ¨æµ‹è¯•çš„ç¨€ç–æ³¨æ„åŠ›å˜ä½“ä¸­æ˜¯å”¯ä¸€èƒ½å¤Ÿè¶…è¶Šå¯†é›†åŸºçº¿çš„æ–¹æ¡ˆï¼ŒæŸäº›æƒ…å†µä¸‹åœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹çš„å›°æƒ‘åº¦æå‡è¾¾27%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é«˜è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œç°æœ‰çš„äºšäºŒæ¬¡æ–¹æ³¨æ„åŠ›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºæ··åˆç¨€ç–æ³¨æ„åŠ›ï¼ˆMoSAï¼‰ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©ä»¤ç‰Œæ¥å®ç°åŸºäºå†…å®¹çš„ç¨€ç–æ€§ï¼Œä»è€Œé™ä½æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è®¡ç®—å¤æ‚åº¦ï¼Œæå‡æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoSAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ¯ä¸ªå¤´æ ¹æ®è¾“å…¥åºåˆ—åŠ¨æ€é€‰æ‹©kä¸ªä»¤ç‰Œï¼Œè®¡ç®—å¤æ‚åº¦ä»O(T^2)é™ä½åˆ°O(k^2 + T)ï¼Œä½¿å¾—åœ¨ç›¸åŒé¢„ç®—ä¸‹å¯ä»¥ä½¿ç”¨æ›´å¤šçš„æ³¨æ„åŠ›å¤´ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoSAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€é€‰æ‹©æœºåˆ¶ï¼Œå…è®¸ä»»æ„ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¯†é›†æ³¨æ„åŠ›æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸­ï¼ŒMoSAä½¿ç”¨äº†torchæ¡†æ¶ï¼Œå°½ç®¡æ²¡æœ‰ä¼˜åŒ–çš„å†…æ ¸ï¼Œä½†åœ¨å›°æƒ‘åº¦åŒ¹é…çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨å¢™é’Ÿæ—¶é—´ä¸Šæ›´å¿«ï¼Œè®­ç»ƒæ—¶å†…å­˜éœ€æ±‚æ›´å°‘ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†KVç¼“å­˜çš„å¤§å°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoSAåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹çš„å›°æƒ‘åº¦æ¯”å¯†é›†åŸºçº¿æé«˜äº†27%ã€‚æ­¤å¤–ï¼ŒMoSAåœ¨è®­ç»ƒæ—¶é€Ÿåº¦æ›´å¿«ï¼Œå†…å­˜å ç”¨æ›´å°‘ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†KVç¼“å­˜çš„å¤§å°ï¼Œå±•ç¤ºäº†å…¶åœ¨èµ„æºä½¿ç”¨ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜æ³¨æ„åŠ›æœºåˆ¶çš„æ•ˆç‡ï¼ŒMoSAèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.

