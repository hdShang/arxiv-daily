---
layout: default
title: "R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training"
---

# R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00358v1</a>
  <a href="https://arxiv.org/pdf/2505.00358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00358v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00358v1', 'R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Albert Ge, Tzu-Heng Huang, John Cooper, Avi Trost, Ziyi Chu, Satya Sai Srinath Namburi GNVV, Ziyang Cai, Kendall Park, Nicholas Roberts, Frederic Sala

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR&Bæ¡†æ¶ä»¥è§£å†³æ•°æ®æ··åˆè®­ç»ƒä¸­çš„æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®æ··åˆ` `æ¨¡å‹è®­ç»ƒ` `è¯­ä¹‰ç›¸ä¼¼æ€§` `è®¡ç®—æ•ˆç‡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°æ®æ··åˆæ–¹æ³•ä¾èµ–äºé¢„è®¾çš„æ•°æ®åŸŸï¼Œæ— æ³•å……åˆ†æ•æ‰è¯­ä¹‰ç»†èŠ‚ï¼Œå¯¼è‡´æ€§èƒ½æœªèƒ½è¾¾åˆ°æœ€ä½³ã€‚
2. R&Bæ¡†æ¶é€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§é‡æ–°åˆ’åˆ†è®­ç»ƒæ•°æ®ï¼Œå¹¶åˆ©ç”¨åŸŸæ¢¯åº¦çš„GramçŸ©é˜µä¼˜åŒ–æ•°æ®ç»„æˆï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡ã€‚
3. åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šï¼ŒR&Bä»¥ä»…0.01%çš„é¢å¤–è®¡ç®—å¼€é”€ï¼Œè¾¾åˆ°äº†æˆ–è¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ•°æ®æ··åˆç­–ç•¥çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®æ··åˆç­–ç•¥åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­æˆåŠŸé™ä½äº†æˆæœ¬ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºé™·ï¼šä¸€æ˜¯ä¾èµ–äºé¢„è®¾çš„æ•°æ®åŸŸï¼Œå¯èƒ½æ— æ³•æ•æ‰å…³é”®çš„è¯­ä¹‰ç»†å¾®å·®åˆ«ï¼›äºŒæ˜¯éšç€åŸŸæ•°çš„å¢åŠ ï¼Œè®¡ç®—å¼€é”€æ˜¾è‘—å¢åŠ ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†R&Bæ¡†æ¶ï¼Œé€šè¿‡åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§é‡æ–°åˆ’åˆ†è®­ç»ƒæ•°æ®ï¼ˆRegroupï¼‰ï¼Œåˆ›å»ºæ›´ç»†ç²’åº¦çš„åŸŸï¼Œå¹¶é€šè¿‡åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—çš„åŸŸæ¢¯åº¦çš„GramçŸ©é˜µé«˜æ•ˆä¼˜åŒ–æ•°æ®ç»„æˆï¼ˆBalanceï¼‰ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒR&Bä¸éœ€è¦é¢å¤–çš„è®¡ç®—æ¥è·å–è¯„ä¼°ä¿¡æ¯ï¼Œå¦‚æŸå¤±æˆ–æ¢¯åº¦ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†æ­£åˆ™æ¡ä»¶ä¸‹åˆ†æäº†è¯¥æŠ€æœ¯ï¼Œå¹¶æä¾›äº†ç†è®ºè§è§£ï¼Œè¯æ˜R&Bç›¸è¾ƒäºéè‡ªé€‚åº”æ··åˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¯æ˜ï¼ŒR&Båœ¨äº”ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œé¢å¤–è®¡ç®—å¼€é”€ä»…ä¸º0.01%ï¼Œä¸”æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ•°æ®æ··åˆç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®æ··åˆç­–ç•¥åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶çš„æ•ˆç‡é—®é¢˜ï¼Œå°¤å…¶æ˜¯å…¶å¯¹é¢„è®¾æ•°æ®åŸŸçš„ä¾èµ–å’Œè®¡ç®—å¼€é”€çš„å¢åŠ ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šR&Bæ¡†æ¶é€šè¿‡é‡æ–°åˆ’åˆ†è®­ç»ƒæ•°æ®ä»¥æ•æ‰æ›´ç»†ç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—çš„åŸŸæ¢¯åº¦ä¿¡æ¯æ¥ä¼˜åŒ–æ•°æ®ç»„æˆï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR&Bæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ•°æ®é‡æ–°åˆ’åˆ†ï¼ˆRegroupï¼‰å’Œæ•°æ®ç»„æˆä¼˜åŒ–ï¼ˆBalanceï¼‰ã€‚Regroupæ¨¡å—æ ¹æ®è¯­ä¹‰ç›¸ä¼¼æ€§å¯¹æ•°æ®è¿›è¡Œç»†åˆ†ï¼Œè€ŒBalanceæ¨¡å—åˆ™é€šè¿‡GramçŸ©é˜µæ¥ä¼˜åŒ–æ•°æ®çš„ç»„åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šR&Bçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ¶ˆé™¤äº†è·å–è¯„ä¼°ä¿¡æ¯ï¼ˆå¦‚æŸå¤±æˆ–æ¢¯åº¦ï¼‰æ‰€éœ€çš„é¢å¤–è®¡ç®—ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼ŒR&Bèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹æå‡æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šR&Bçš„è®¾è®¡ä¸­ï¼ŒGramçŸ©é˜µçš„è®¡ç®—æ˜¯åŸºäºè®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è·å¾—çš„åŸŸæ¢¯åº¦ï¼Œç¡®ä¿äº†æ•°æ®ç»„æˆçš„è‡ªé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæ¡†æ¶åœ¨å‚æ•°è®¾ç½®ä¸Šä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒç±»å‹çš„æ•°æ®é›†å’Œä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨äº”ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šï¼ŒR&Bæ¡†æ¶ä»¥ä»…0.01%çš„é¢å¤–è®¡ç®—å¼€é”€ï¼Œè¾¾åˆ°äº†æˆ–è¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ•°æ®æ··åˆç­–ç•¥çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R&Bæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨ç†å’Œå¤šæ¨¡æ€ä»»åŠ¡ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒR&Bèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ï¼Œå¿«é€Ÿè¿­ä»£å’Œä¼˜åŒ–æ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.

