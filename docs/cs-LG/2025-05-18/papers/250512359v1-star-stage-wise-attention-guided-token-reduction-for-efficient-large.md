---
layout: default
title: "STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference"
---

# STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12359" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12359v1</a>
  <a href="https://arxiv.org/pdf/2505.12359.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12359v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12359v1', 'STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yichen Guo, Hanze Li, Zonghao Zhang, Jinhao You, Kai Tang, Xiande Huang

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTARæ¡†æ¶ä»¥è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æ ‡è®°å‰ªæ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ ‡è®°å‰ªææ–¹æ³•å¤šé‡‡ç”¨å•ä¸€é˜¶æ®µç­–ç•¥ï¼Œå¿½è§†äº†æ¨¡å‹å†…éƒ¨çš„ä¿¡æ¯æµï¼Œå¯¼è‡´é«˜å‰ªææ¯”ç‡ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
2. STARæ¡†æ¶é€šè¿‡é˜¶æ®µæ€§æ³¨æ„åŠ›å¼•å¯¼çš„æ–¹å¼è¿›è¡Œæ ‡è®°å‰ªæï¼Œåˆ†ä¸ºæ—©æœŸå’ŒåæœŸä¸¤ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«å»é™¤å†—ä½™ç‰¹å¾å’Œæ— å…³æ ‡è®°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTARåœ¨å¤šä¸ªLVLMæ¶æ„ä¸Šå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨æŸäº›æƒ…å†µä¸‹æ€§èƒ½æœ‰æ‰€æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åˆ©ç”¨ä¸°å¯Œçš„è§†è§‰æ ‡è®°è¡¨ç¤ºåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½è¡¨ç°ï¼Œä½†è¿™äº›æ ‡è®°åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¹Ÿå¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ã€‚ç°æœ‰çš„æ— è®­ç»ƒæ ‡è®°å‰ªææ–¹æ³•é€šå¸¸é‡‡ç”¨å•é˜¶æ®µç­–ç•¥ï¼Œå…³æ³¨è§†è§‰è‡ªæ³¨æ„åŠ›æˆ–è§†è§‰-æ–‡æœ¬äº¤å‰æ³¨æ„åŠ›ï¼Œå¯¼è‡´ä¿¡æ¯æµçš„å±€éƒ¨è§†è§’å¿½è§†ï¼Œå°¤å…¶åœ¨é«˜å‰ªææ¯”ç‡ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†STARï¼ˆé˜¶æ®µæ€§æ³¨æ„åŠ›å¼•å¯¼çš„æ ‡è®°å‡å°‘ï¼‰ï¼Œä¸€ä¸ªæ— è®­ç»ƒã€å³æ’å³ç”¨çš„æ¡†æ¶ï¼Œä»å…¨å±€è§†è§’è¿›è¡Œæ ‡è®°å‰ªæã€‚STARåœ¨ä¸¤ä¸ªäº’è¡¥é˜¶æ®µè¿›è¡Œæ³¨æ„åŠ›å¼•å¯¼çš„å‡å°‘ï¼Œæ—©æœŸé˜¶æ®µåŸºäºè§†è§‰è‡ªæ³¨æ„åŠ›å»é™¤å†—ä½™ä½çº§ç‰¹å¾ï¼ŒåæœŸé˜¶æ®µé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å¼•å¯¼ä¸¢å¼ƒä¸ä»»åŠ¡æ— å…³çš„æ ‡è®°ã€‚è¿™ç§æ•´ä½“æ–¹æ³•æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶æ›´å¥½åœ°ä¿ç•™äº†ä»»åŠ¡å…³é”®çš„ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSTARåœ¨å¤šä¸ªLVLMæ¶æ„å’ŒåŸºå‡†ä¸Šå®ç°äº†å¼ºå¤§çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¯”çš„ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±äºæ ‡è®°æ•°é‡åºå¤§è€Œå¯¼è‡´çš„è®¡ç®—å¼€é”€é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å•é˜¶æ®µå‰ªæï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å…¨å±€ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSTARæ¡†æ¶é€šè¿‡é˜¶æ®µæ€§æ³¨æ„åŠ›å¼•å¯¼çš„æ–¹å¼è¿›è¡Œæ ‡è®°å‰ªæï¼Œé¦–å…ˆåœ¨æ—©æœŸé˜¶æ®µå»é™¤å†—ä½™çš„ä½çº§ç‰¹å¾ï¼Œç„¶ååœ¨åæœŸé˜¶æ®µåŸºäºè·¨æ¨¡æ€æ³¨æ„åŠ›ä¸¢å¼ƒä¸ä»»åŠ¡æ— å…³çš„æ ‡è®°ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSTARçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µåˆ©ç”¨è§†è§‰è‡ªæ³¨æ„åŠ›è¿›è¡Œæ—©æœŸå‰ªæï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›è¿›è¡ŒåæœŸå‰ªæã€‚æ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹ç‰¹å®šçš„ç‰¹å¾è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆä¿ç•™ã€‚

**å…³é”®åˆ›æ–°**ï¼šSTARçš„åˆ›æ–°åœ¨äºå…¶é˜¶æ®µæ€§å‰ªæç­–ç•¥ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å•é˜¶æ®µæ–¹æ³•ï¼Œé€šè¿‡å…¨å±€è§†è§’ä¼˜åŒ–å‰ªæè¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†å‰ªææ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSTARé‡‡ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥æŒ‡å¯¼å‰ªæè¿‡ç¨‹ï¼Œç¡®ä¿åœ¨æ¯ä¸ªé˜¶æ®µéƒ½èƒ½æœ‰æ•ˆè¯†åˆ«å’Œå»é™¤å†—ä½™ä¿¡æ¯ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œå¯èƒ½ä¸ºæœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSTARåœ¨å¤šä¸ªLVLMæ¶æ„ä¸Šå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿï¼Œæ¨ç†é€Ÿåº¦æå‡å¯è¾¾XX%ï¼ˆå…·ä½“æ•°æ®éœ€æŸ¥é˜…åŸæ–‡ï¼‰ï¼ŒåŒæ—¶åœ¨æŸäº›åŸºå‡†ä¸Šæ€§èƒ½ä¿æŒå¯æ¯”æˆ–æœ‰æ‰€æå‡ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬çš„è”åˆç†è§£ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿä»¥åŠå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜æ¨ç†æ•ˆç‡ï¼ŒSTARæ¡†æ¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´å¿«é€Ÿçš„å“åº”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although large vision-language models (LVLMs) leverage rich visual token representations to achieve strong performance on multimodal tasks, these tokens also introduce significant computational overhead during inference. Existing training-free token pruning methods typically adopt a single-stage strategy, focusing either on visual self-attention or visual-textual cross-attention. However, such localized perspectives often overlook the broader information flow across the model, leading to substantial performance degradation, especially under high pruning ratios. In this work, we propose STAR (Stage-wise Attention-guided token Reduction), a training-free, plug-and-play framework that approaches token pruning from a global perspective. Instead of pruning at a single point, STAR performs attention-guided reduction in two complementary stages: an early-stage pruning based on visual self-attention to remove redundant low-level features, and a later-stage pruning guided by cross-modal attention to discard task-irrelevant tokens. This holistic approach allows STAR to significantly reduce computational cost while better preserving task-critical information. Extensive experiments across multiple LVLM architectures and benchmarks show that STAR achieves strong acceleration while maintaining comparable, and in some cases even improved performance.

