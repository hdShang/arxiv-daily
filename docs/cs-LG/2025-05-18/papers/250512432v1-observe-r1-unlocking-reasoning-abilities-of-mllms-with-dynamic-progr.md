---
layout: default
title: "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning"
---

# Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12432" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12432v1</a>
  <a href="https://arxiv.org/pdf/2505.12432.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12432v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12432v1', 'Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zirun Guo, Minjie Hong, Tao Jin

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-18

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zrguo/Observe-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºObserve-R1ä»¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `æ•°æ®é›†æ„å»º` `åŠ¨æ€åŠ æƒæœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤šæ¨¡æ€æ•°æ®æ—¶é¢ä¸´é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†èƒ½åŠ›æå‡æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºObserve-R1æ¡†æ¶ï¼Œé€šè¿‡é€æ­¥å­¦ä¹ å’Œå¤šæ¨¡æ€æ ¼å¼çº¦æŸï¼Œå¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒObserve-R1åœ¨20,000ä¸ªæ ·æœ¬ä¸Šè¶…è¶Šäº†å¤šä¸ªå¤§å‹æ¨ç†æ¨¡å‹ï¼Œæ¨ç†é“¾çš„æ¸…æ™°åº¦å’Œç®€æ´æ€§æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†RLé€‚åº”äºå¤šæ¨¡æ€æ•°æ®å’Œæ ¼å¼çš„å…·ä½“æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†Observe-R1ï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬å€Ÿé‰´äººç±»å­¦ä¹ çš„æ¸è¿›è¿‡ç¨‹ï¼Œæå‡ºäº†ä¸€ç§é€æ­¥å­¦ä¹ çš„èŒƒå¼ï¼Œå¹¶æ„å»ºäº†NeuraLadderæ•°æ®é›†ï¼Œä»¥æ•°æ®æ ·æœ¬çš„éš¾åº¦å’Œå¤æ‚æ€§ä¸ºåŸºç¡€è¿›è¡ŒRLè®­ç»ƒã€‚é€šè¿‡å¼•å…¥å¤šæ¨¡æ€æ ¼å¼çº¦æŸå’Œå¥–åŠ±ç³»ç»Ÿï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒObserve-R1åœ¨æ¨ç†å’Œä¸€èˆ¬åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¸€ç³»åˆ—æ›´å¤§çš„æ¨ç†æ¨¡å‹ï¼Œå–å¾—äº†æ›´æ¸…æ™°å’Œç®€æ´çš„æ¨ç†é“¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æå‡ä¸­çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é€‚åº”å¤šæ¨¡æ€æ•°æ®æ—¶å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šObserve-R1æ¡†æ¶å€Ÿé‰´äººç±»å­¦ä¹ çš„æ¸è¿›è¿‡ç¨‹ï¼Œæå‡ºé€æ­¥å­¦ä¹ çš„èŒƒå¼ï¼Œé€šè¿‡æ„å»ºNeuraLadderæ•°æ®é›†æ¥ç»„ç»‡å’Œé‡‡æ ·æ•°æ®ï¼Œç¡®ä¿RLè®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€å¥–åŠ±ç³»ç»Ÿè®¾è®¡å’ŒåŠ¨æ€åŠ æƒæœºåˆ¶ã€‚æ•°æ®é›†æ ¹æ®æ ·æœ¬çš„éš¾åº¦å’Œå¤æ‚æ€§è¿›è¡Œç»„ç»‡ï¼Œå¥–åŠ±ç³»ç»Ÿé¼“åŠ±ç®€æ´å’Œæ­£ç¡®çš„å›ç­”ï¼ŒåŠ¨æ€åŠ æƒæœºåˆ¶åˆ™ä¼˜å…ˆè€ƒè™‘ä¸ç¡®å®šå’Œä¸­ç­‰éš¾åº¦çš„é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šObserve-R1çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šæ¨¡æ€æ ¼å¼çº¦æŸå’ŒåŠ¨æ€åŠ æƒæœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€è®­ç»ƒæ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¥–åŠ±ç³»ç»Ÿä¸­ï¼Œè®¾ç½®äº†é•¿åº¦çº¦æŸä»¥é¼“åŠ±ç®€æ´å›ç­”ï¼ŒåŒæ—¶é€šè¿‡åŠ¨æ€åŠ æƒæœºåˆ¶ç¡®ä¿æ›´å…·ä¿¡æ¯é‡çš„æ ·æœ¬å¯¹è®­ç»ƒçš„å½±å“æ›´å¤§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒObserve-R1åœ¨20,000ä¸ªæ ·æœ¬çš„æµ‹è¯•ä¸­ï¼Œè¶…è¶Šäº†å¤šä¸ªå¤§å‹æ¨ç†æ¨¡å‹ï¼Œå°¤å…¶åœ¨æ¨ç†é“¾çš„æ¸…æ™°åº¦å’Œç®€æ´æ€§æ–¹é¢è¡¨ç°çªå‡ºï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å›¾åƒç†è§£å’Œå¤šæ¨¡æ€äº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒObserve-R1å¯åœ¨æ•™è‚²ã€åŒ»ç–—å’Œè‡ªåŠ¨åŒ–å®¢æœç­‰å¤šä¸ªè¡Œä¸šä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) has shown promise in improving the reasoning abilities of Large Language Models (LLMs). However, the specific challenges of adapting RL to multimodal data and formats remain relatively unexplored. In this work, we present Observe-R1, a novel framework aimed at enhancing the reasoning capabilities of multimodal large language models (MLLMs). We draw inspirations from human learning progression--from simple to complex and easy to difficult, and propose a gradual learning paradigm for MLLMs. To this end, we construct the NeuraLadder dataset, which is organized and sampled according to the difficulty and complexity of data samples for RL training. To tackle multimodal tasks, we introduce a multimodal format constraint that encourages careful observation of images, resulting in enhanced visual abilities and clearer and more structured responses. Additionally, we implement a bonus reward system that favors concise, correct answers within a length constraint, alongside a dynamic weighting mechanism that prioritizes uncertain and medium-difficulty problems, ensuring that more informative samples have a greater impact on training. Our experiments with the Qwen2.5-VL-3B and Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that Observe-R1 outperforms a series of larger reasoning models on both reasoning and general benchmarks, achieving superior clarity and conciseness in reasoning chains. Ablation studies validate the effectiveness of our strategies, highlighting the robustness and generalization of our approach. The dataset and code will be released at https://github.com/zrguo/Observe-R1.

