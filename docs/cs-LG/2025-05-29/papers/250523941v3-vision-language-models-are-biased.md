---
layout: default
title: Vision Language Models are Biased
---

# Vision Language Models are Biased

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23941" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23941v3</a>
  <a href="https://arxiv.org/pdf/2505.23941.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23941v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23941v3', 'Vision Language Models are Biased')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, Daeyoung Kim

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-11-30)

**å¤‡æ³¨**: Code and qualitative examples are available at: vlmsarebiased.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹çš„åè§åŠå…¶å¯¹è¯†åˆ«å‡†ç¡®æ€§çš„å½±å“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æ¨¡å‹åè§` `è®¡æ•°ä»»åŠ¡` `å›¾åƒå¤„ç†` `æ¨ç†åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ ‡å‡†è§†è§‰ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“å—åˆ°å…ˆå‰çŸ¥è¯†çš„å½±å“ï¼Œå¯¼è‡´è¾“å‡ºåè§å’Œä¸å‡†ç¡®ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡å»é™¤å›¾åƒèƒŒæ™¯å’Œåˆ†ææ¨ç†æ¨¡å¼ï¼Œæ¢è®¨äº†å¦‚ä½•æé«˜è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è®¡æ•°å’Œè¯†åˆ«ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå»é™¤èƒŒæ™¯åå‡†ç¡®ç‡æå‡äº†21.09ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”æ€è€ƒæ ‡è®°çš„ä½¿ç”¨å¯¹å‡†ç¡®ç‡æœ‰æ˜¾è‘—å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»äº’è”ç½‘è®°å¿†äº†å¤§é‡å…ˆå‰çŸ¥è¯†ï¼Œè¿™æœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´è¾“å‡ºåå‘é”™è¯¯æˆ–æœ‰åè§çš„ç­”æ¡ˆã€‚æœ¬ç ”ç©¶æµ‹è¯•äº†å…³äºæµè¡Œä¸»é¢˜çš„çŸ¥è¯†å¦‚ä½•å½±å“è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è®¡æ•°å’Œè¯†åˆ«ç­‰æ ‡å‡†è§†è§‰ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„VLMså­˜åœ¨æ˜¾è‘—åè§ï¼Œè®¡æ•°å‡†ç¡®ç‡åœ¨ä¸ƒä¸ªä¸åŒé¢†åŸŸä¸­å¹³å‡ä»…ä¸º17.05%ã€‚å»é™¤å›¾åƒèƒŒæ™¯å‡ ä¹ä½¿å‡†ç¡®ç‡ç¿»å€ï¼Œæ˜¾ç¤ºä¸Šä¸‹æ–‡è§†è§‰çº¿ç´¢è§¦å‘äº†è¿™äº›åè§ååº”ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒVLMsçš„è®¡æ•°å‡†ç¡®ç‡éšç€æ€è€ƒæ ‡è®°çš„å¢åŠ è€Œä¸Šå‡ï¼Œè¾¾åˆ°çº¦40%ï¼Œä½†åœ¨è¿‡åº¦æ¨ç†åä¸‹é™ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†VLMsçš„ä¸€ä¸ªæœ‰è¶£å¤±è´¥æ¨¡å¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§äººç±»ç›‘ç£çš„è‡ªåŠ¨åŒ–æ¡†æ¶æ¥æµ‹è¯•VLMçš„åè§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è®¡æ•°å’Œè¯†åˆ«ä»»åŠ¡ä¸­å­˜åœ¨çš„åè§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æµè¡Œä¸»é¢˜æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„å‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å»é™¤å›¾åƒèƒŒæ™¯å’Œåˆ†ææ¨ç†è¿‡ç¨‹ï¼Œæ¢ç´¢å¦‚ä½•å‡å°‘æ¨¡å‹è¾“å‡ºçš„åè§ï¼Œæé«˜å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé¦–å…ˆè¿›è¡Œå›¾åƒèƒŒæ™¯å»é™¤ï¼Œç„¶åé€šè¿‡æ€è€ƒæ ‡è®°åˆ†ææ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œæœ€ç»ˆè¯„ä¼°æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°æµ‹è¯•è§†è§‰è¯­è¨€æ¨¡å‹çš„åè§ï¼Œå¹¶æ­ç¤ºå…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„å¤±è´¥æ¨¡å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œå»é™¤èƒŒæ™¯çš„å¤„ç†æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è®¡æ•°å‡†ç¡®ç‡ï¼Œæ­¤å¤–ï¼Œæ€è€ƒæ ‡è®°çš„æ•°é‡ä¸å‡†ç¡®ç‡ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³ç³»ï¼ŒåˆæœŸæå‡åå‡ºç°ä¸‹é™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå»é™¤å›¾åƒèƒŒæ™¯åï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è®¡æ•°å‡†ç¡®ç‡ä»17.05%æå‡è‡³38.14%ï¼Œæå‡å¹…åº¦è¾¾21.09ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨ä½¿ç”¨æ€è€ƒæ ‡è®°æ—¶ï¼Œå‡†ç¡®ç‡åˆæœŸä¸Šå‡è‡³çº¦40%ï¼Œä½†åœ¨è¿‡åº¦æ¨ç†åå‡ºç°ä¸‹é™ï¼Œæ­ç¤ºäº†æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„å¤æ‚æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€äººå·¥æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡è¯†åˆ«å’Œå‡å°‘æ¨¡å‹åè§ï¼Œå¯ä»¥æå‡è¿™äº›ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œè¿›è€Œå¢å¼ºç”¨æˆ·ä½“éªŒå’Œä¿¡ä»»åº¦ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´å…¬å¹³å’Œé€æ˜çš„AIç³»ç»Ÿè®¾è®¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that helps them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g., unable to recognize the 4th stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Removing image backgrounds nearly doubles accuracy (21.09 percentage points), revealing that contextual visual cues trigger these biased responses. Further analysis of VLMs' reasoning patterns shows that counting accuracy initially rises with thinking tokens, reaching ~40%, before declining with excessive reasoning. Our work presents an interesting failure mode in VLMs and a human-supervised automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.

