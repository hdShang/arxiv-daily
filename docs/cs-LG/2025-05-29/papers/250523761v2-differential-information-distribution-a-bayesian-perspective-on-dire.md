---
layout: default
title: "Differential Information Distribution: A Bayesian Perspective on Direct Preference Optimization"
---

# Differential Information Distribution: A Bayesian Perspective on Direct Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23761" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23761v2</a>
  <a href="https://arxiv.org/pdf/2505.23761.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23761v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23761v2', 'Differential Information Distribution: A Bayesian Perspective on Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-10-02)

**å¤‡æ³¨**: Preprint, under review. 39 pages, 12 figures. Updates from v1: Added new theoretical results on DPO training dynamics and policy exploration, included experiments with Qwen3-4B, and refined the discussion of log-margin dynamics

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå·®å¼‚ä¿¡æ¯åˆ†å¸ƒæ–¹æ³•ä»¥ä¼˜åŒ–ç›´æ¥åå¥½å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›´æ¥åå¥½ä¼˜åŒ–` `å·®å¼‚ä¿¡æ¯åˆ†å¸ƒ` `è´å¶æ–¯å­¦ä¹ ` `ç­–ç•¥æ›´æ–°` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ¨¡å‹å¯¹é½` `è®­ç»ƒåŠ¨æ€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨å¥–åŠ±è®¾è®¡å’Œè®­ç»ƒåŠ¨æ€æ–¹é¢å­˜åœ¨ä¸æ˜ç¡®æ€§ï¼Œå½±å“äº†å…¶ä¸‹æ¸¸èƒ½åŠ›çš„æå‡ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å·®å¼‚ä¿¡æ¯åˆ†å¸ƒï¼ˆDIDï¼‰æ¥ç†è§£åå¥½ä¼˜åŒ–ï¼Œè®¤ä¸ºå…¶ç›®æ ‡æ˜¯å­¦ä¹ æ›´æ–°ç­–ç•¥æ‰€éœ€çš„å·®å¼‚ä¿¡æ¯ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå­¦ä¹ é«˜ç†µDIDèƒ½æ˜¾è‘—æé«˜å¼€æ”¾å¼æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œè€Œä½ç†µDIDåˆ™åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”ä¸­è¡¨ç°æ›´ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¹¿æ³›åº”ç”¨äºå°†è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å…³é”®é—®é¢˜æœªå¾—åˆ°è§£å†³ã€‚æœ¬æ–‡ä»è´å¶æ–¯çš„è§’åº¦å‡ºå‘ï¼Œæå‡ºå·®å¼‚ä¿¡æ¯åˆ†å¸ƒï¼ˆDIDï¼‰ä½œä¸ºæ›´æ–°å‚è€ƒç­–ç•¥åˆ°ç›®æ ‡ç­–ç•¥æ‰€éœ€çš„è´å¶æ–¯è¯æ®çš„æ ·æœ¬åˆ†å¸ƒã€‚é€šè¿‡DIDçš„è§†è§’ï¼Œæˆ‘ä»¬å‘ç°DPOçš„å¯¹æ•°æ¯”å¥–åŠ±åœ¨åå¥½ç¼–ç äº†æ›´æ–°æ‰€éœ€çš„å·®å¼‚ä¿¡æ¯æ—¶å…·æœ‰ç‹¬ç‰¹çš„åˆç†æ€§ã€‚æ­¤å¤–ï¼ŒDIDçš„ç†µä½œä¸ºå­¦ä¹ ä¿¡æ¯çš„ä¸ç¡®å®šæ€§åº¦é‡ï¼Œå½±å“ä¸‹æ¸¸æ€§èƒ½ï¼Œå­¦ä¹ é«˜ç†µDIDæœ‰åŠ©äºå¼€æ”¾å¼æŒ‡ä»¤è·Ÿéšï¼Œè€Œä½ç†µDIDåˆ™æœ‰åˆ©äºçŸ¥è¯†å¯†é›†å‹é—®ç­”ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºåå¥½å¯¹é½æä¾›äº†ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸­å¯¹æ•°æ¯”å¥–åŠ±çš„åˆç†æ€§ã€åå¥½æ•°æ®é›†çš„ç»Ÿè®¡ç»“æ„å¯¹è®­ç»ƒåŠ¨æ€çš„å½±å“ï¼Œä»¥åŠè¿™äº›åŠ¨æ€å¦‚ä½•å½±å“ä¸‹æ¸¸èƒ½åŠ›ç­‰å…³é”®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™äº›æ–¹é¢å­˜åœ¨ä¸æ˜ç¡®æ€§ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬ä»è´å¶æ–¯è§†è§’å‡ºå‘ï¼Œæå‡ºå·®å¼‚ä¿¡æ¯åˆ†å¸ƒï¼ˆDIDï¼‰ï¼Œå°†åå¥½ä¼˜åŒ–è§†ä¸ºå­¦ä¹ æ›´æ–°å‚è€ƒç­–ç•¥åˆ°ç›®æ ‡ç­–ç•¥æ‰€éœ€çš„å·®å¼‚ä¿¡æ¯ã€‚è¿™ä¸€è§†è§’ä¸ºç†è§£DPOçš„å¥–åŠ±è®¾è®¡å’Œè®­ç»ƒåŠ¨æ€æä¾›äº†æ–°çš„ç†è®ºåŸºç¡€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡DIDå®šä¹‰åå¥½æ•°æ®çš„åˆ†å¸ƒï¼›å…¶æ¬¡ï¼Œåˆ†æDPOçš„è®­ç»ƒåŠ¨æ€ä¸DIDä¹‹é—´çš„å…³ç³»ï¼›æœ€åï¼Œåˆ©ç”¨DIDçš„ç†µæ¥è¯„ä¼°å…¶å¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†DIDè¿™ä¸€æ¦‚å¿µï¼Œæ˜ç¡®äº†DPOçš„å¥–åŠ±è®¾è®¡ä¸å­¦ä¹ å·®å¼‚ä¿¡æ¯ä¹‹é—´çš„å…³ç³»ï¼Œæä¾›äº†å¯¹ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«å’Œç†è®ºæ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒDIDçš„ç†µè¢«ç”¨ä½œä¸ç¡®å®šæ€§åº¦é‡ï¼Œå½±å“è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç­–ç•¥æ¢ç´¢å’Œå¯¹æ•°ä¼¼ç„¶å˜åŒ–ï¼Œå…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡ä¸DIDçš„ç‰¹æ€§å¯†åˆ‡ç›¸å…³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå­¦ä¹ é«˜ç†µDIDçš„æ¨¡å‹åœ¨å¼€æ”¾å¼æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ã€‚è€Œä½ç†µDIDåˆ™åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†DIDå¯¹ä¸‹æ¸¸èƒ½åŠ›çš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„äººæœºäº¤äº’ã€æ™ºèƒ½åŠ©æ‰‹å’Œæ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡ä¼˜åŒ–åå¥½å¯¹é½ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¢å¼ºç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒDIDçš„ç†è®ºæ¡†æ¶å¯èƒ½ä¸ºå…¶ä»–æœºå™¨å­¦ä¹ é¢†åŸŸæä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Direct Preference Optimization (DPO) has been widely used for aligning language models with human preferences in a supervised manner. However, several key questions remain unresolved: the rationale behind its log-ratio reward, how the statistical structure of preference datasets shapes its training dynamics, and how those dynamics impact downstream capabilities. We approach these questions from a Bayesian perspective, interpreting the goal of preference optimization as learning the differential information required to update a reference policy into a target policy. To formalize this view, we introduce the Differential Information Distribution (DID), defined as the distribution over samples that carry the Bayesian evidence required to update policies. We introduce three complementary insights by viewing preference optimization through the DID. First, we find that DPO's log-ratio reward is uniquely justified when preferences encode the Differential Information needed to update a reference policy into the target policy. Second, we discuss how commonly observed training dynamics in DPO, including changes in log-likelihood and policy exploration, stem from a power-law DID relationship. Finally, we analyze how training dynamics influence downstream performance using the entropy of DID, a principled measure of uncertainty in the learned information. We observe that learning high-entropy DID improves open-ended instruction-following, while low-entropy DID benefits knowledge-intensive QA. Taken together, our results show that DPO's reward design, training dynamics, and downstream capabilities all emerge as natural consequences of learning Differential Information, offering both a principled theoretical foundation and practical guidance for preference-based alignment.

