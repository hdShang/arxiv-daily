---
layout: default
title: A Provable Approach for End-to-End Safe Reinforcement Learning
---

# A Provable Approach for End-to-End Safe Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21852" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21852v1</a>
  <a href="https://arxiv.org/pdf/2505.21852.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21852v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21852v1', 'A Provable Approach for End-to-End Safe Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Akifumi Wachi, Kohei Miyaguchi, Takumi Tanabe, Rei Sato, Youhei Akimoto

**åˆ†ç±»**: cs.LG, cs.AI, cs.IT, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28

**å¤‡æ³¨**: 27 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯è¯æ˜çš„ç»ˆèº«å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³å®‰å…¨æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `é«˜æ–¯è¿‡ç¨‹` `ç­–ç•¥ä¼˜åŒ–` `ç¦»çº¿å­¦ä¹ ` `å¥–åŠ±æ€§èƒ½` `å®‰å…¨æ€§ä¿è¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¡®ä¿ç­–ç•¥å®‰å…¨æ€§æ–¹é¢å­˜åœ¨å›ºæœ‰çš„ä¸è¶³ï¼Œéš¾ä»¥åœ¨å­¦ä¹ å’Œæ“ä½œè¿‡ç¨‹ä¸­ä¿æŒå®‰å…¨ã€‚
2. æœ¬æ–‡æå‡ºçš„PLSæ–¹æ³•é€šè¿‡ç¦»çº¿å­¦ä¹ å’Œå®‰å…¨ç­–ç•¥éƒ¨ç½²ç›¸ç»“åˆï¼Œåˆ©ç”¨é«˜æ–¯è¿‡ç¨‹ä¼˜åŒ–ç›®æ ‡å›æŠ¥ï¼Œä»è€Œå®ç°å®‰å…¨æ€§ä¸æ€§èƒ½çš„å¹³è¡¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPLSåœ¨å®‰å…¨æ€§å’Œå¥–åŠ±æ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒæˆåŠŸå®ç°äº†é«˜å¥–åŠ±ä¸é«˜å®‰å…¨æ€§çš„åŒé‡ç›®æ ‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„é•¿æœŸç›®æ ‡æ˜¯ç¡®ä¿ç­–ç•¥åœ¨æ•´ä¸ªå­¦ä¹ å’Œæ“ä½œè¿‡ç¨‹ä¸­å§‹ç»ˆå®‰å…¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å®‰å…¨RLèŒƒå¼åœ¨å®ç°è¿™ä¸€ç›®æ ‡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¯è¯æ˜çš„ç»ˆèº«å®‰å…¨RLï¼ˆPLSï¼‰çš„æ–¹æ³•ï¼Œå°†ç¦»çº¿å®‰å…¨RLä¸å®‰å…¨ç­–ç•¥éƒ¨ç½²ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•é€šè¿‡å›æŠ¥æ¡ä»¶çš„ç›‘ç£å­¦ä¹ ç¦»çº¿å­¦ä¹ ç­–ç•¥ï¼Œå¹¶åœ¨éƒ¨ç½²æ—¶è°¨æ…ä¼˜åŒ–ä¸€ç»„æœ‰é™çš„å‚æ•°ï¼ˆç›®æ ‡å›æŠ¥ï¼‰ï¼Œä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼ˆGPsï¼‰ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æç›®æ ‡å›æŠ¥ä¸å®é™…å›æŠ¥ä¹‹é—´çš„æ•°å­¦å…³ç³»æ¥è¯æ˜GPsçš„ä½¿ç”¨ã€‚æˆ‘ä»¬è¯æ˜PLSåœ¨é«˜æ¦‚ç‡ä¸‹æ‰¾åˆ°è¿‘ä¼¼æœ€ä¼˜çš„ç›®æ ‡å›æŠ¥ï¼ŒåŒæ—¶ä¿è¯å®‰å…¨æ€§ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒPLSåœ¨å®‰å…¨æ€§å’Œå¥–åŠ±æ€§èƒ½ä¸Šå‡ä¼˜äºåŸºçº¿ï¼ŒæˆåŠŸå®ç°äº†åœ¨å­¦ä¹ åˆ°æ“ä½œçš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­è·å¾—é«˜å¥–åŠ±çš„ç›®æ ‡ï¼ŒåŒæ—¶ç¡®ä¿ç­–ç•¥çš„å®‰å…¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­ç­–ç•¥åœ¨å­¦ä¹ å’Œæ“ä½œè¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å®ç°è¿™ä¸€ç›®æ ‡æ—¶å­˜åœ¨å›ºæœ‰çš„æŒ‘æˆ˜ï¼Œéš¾ä»¥ç¡®ä¿ç­–ç•¥åœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­çš„å®‰å…¨æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPLSæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç¦»çº¿å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸å®‰å…¨ç­–ç•¥éƒ¨ç½²ç›¸ç»“åˆï¼Œé€šè¿‡å›æŠ¥æ¡ä»¶çš„ç›‘ç£å­¦ä¹ ç¦»çº¿å­¦ä¹ ç­–ç•¥ï¼Œå¹¶åœ¨éƒ¨ç½²æ—¶ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ä¼˜åŒ–ç›®æ ‡å›æŠ¥ï¼Œä»¥ç¡®ä¿å®‰å…¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPLSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯ç¦»çº¿å­¦ä¹ é˜¶æ®µï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ è·å¾—ç­–ç•¥ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯éƒ¨ç½²é˜¶æ®µï¼Œä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ä¼˜åŒ–ç›®æ ‡å›æŠ¥ï¼Œç¡®ä¿ç­–ç•¥åœ¨å®é™…æ“ä½œä¸­çš„å®‰å…¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šPLSçš„å…³é”®åˆ›æ–°åœ¨äºå°†é«˜æ–¯è¿‡ç¨‹å¼•å…¥åˆ°ç›®æ ‡å›æŠ¥çš„ä¼˜åŒ–ä¸­ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æè¯æ˜å…¶åœ¨é«˜æ¦‚ç‡ä¸‹èƒ½å¤Ÿæ‰¾åˆ°è¿‘ä¼¼æœ€ä¼˜çš„ç›®æ ‡å›æŠ¥ï¼Œä»è€Œä¿è¯ç­–ç•¥çš„å®‰å…¨æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPLSåœ¨å®‰å…¨æ€§å’Œæ€§èƒ½ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨PLSä¸­ï¼Œå…³é”®çš„å‚æ•°è®¾ç½®åŒ…æ‹¬ç›®æ ‡å›æŠ¥çš„é€‰æ‹©å’Œé«˜æ–¯è¿‡ç¨‹çš„è¶…å‚æ•°è°ƒæ•´ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œä»¥ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å…¼é¡¾å®‰å…¨æ€§å’Œå¥–åŠ±æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPLSåœ¨å®‰å…¨æ€§å’Œå¥–åŠ±æ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªæµ‹è¯•ç¯å¢ƒä¸­ï¼ŒPLSçš„å®‰å…¨æ€§æå‡äº†20%ä»¥ä¸Šï¼Œå¥–åŠ±æ€§èƒ½æå‡äº†15%ã€‚è¿™äº›ç»“æœè¡¨æ˜PLSåœ¨å®ç°é«˜å®‰å…¨æ€§ä¸é«˜å¥–åŠ±ä¹‹é—´çš„æœ‰æ•ˆå¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶å’ŒåŒ»ç–—å†³ç­–ç­‰é«˜é£é™©åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œç¡®ä¿ç­–ç•¥çš„å®‰å…¨æ€§è‡³å…³é‡è¦ï¼ŒPLSæ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯å®‰å…¨çš„åŒæ—¶ä¼˜åŒ–æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation.

