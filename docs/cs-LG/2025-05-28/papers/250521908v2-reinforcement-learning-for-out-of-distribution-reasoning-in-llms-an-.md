---
layout: default
title: Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding
---

# Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21908" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21908v2</a>
  <a href="https://arxiv.org/pdf/2505.21908.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21908v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21908v2', 'Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hanyin Wang, Zhenbang Wu, Gururaj Kolar, Hariprasad Korsapati, Brian Bartlett, Bryan Hull, Jimeng Sun

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-28 (Êõ¥Êñ∞: 2025-10-14)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DRG-Sapphire‰ª•Ëß£ÂÜ≥‰∏¥Â∫äÁ¨îËÆ∞‰∏≠ÁöÑDRGÁºñÁ†ÅÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ËØäÊñ≠Áõ∏ÂÖ≥ÁªÑ` `Ëá™Âä®ÂåñÁºñÁ†Å` `‰∏¥Â∫äÁ¨îËÆ∞` `ÂèØËß£ÈáäÊÄß` `È¢ÜÂüüÁâπÂÆöÊåëÊàò` `Qwen2.5-7B` `ÂåªÁñóÊï∞ÊçÆÂàÜÊûê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®DRGÁºñÁ†Å‰∏≠Èù¢‰∏¥ÁöÑ‰∏ªË¶ÅÊåëÊàòÊòØÁº∫‰πèË∂≥Â§üÁöÑÈ¢ÜÂüüÁü•ËØÜÔºåÂØºËá¥LLMsÂú®Â§ÑÁêÜ‰∏¥Â∫äÊï∞ÊçÆÊó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÁöÑDRG-SapphireÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÂíåÂü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÊó®Âú®ÊèêÈ´òDRGÁºñÁ†ÅÁöÑËá™Âä®ÂåñÂíåÂáÜÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDRG-SapphireÂú®MIMIC-IVÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÁéáÔºåÂπ∂Êèê‰æõ‰∫ÜÂèØËß£ÈáäÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËØäÊñ≠Áõ∏ÂÖ≥ÁªÑÔºàDRGÔºâÁºñÁ†ÅÂØπÂåªÈô¢ÁöÑÊä•ÈîÄÂíåËøêËê•Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÂÖ∂ÂàÜÈÖçËøáÁ®ãÂä≥Âä®ÂØÜÈõÜ„ÄÇÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®DRGÁºñÁ†ÅÊñπÈù¢Èù¢‰∏¥ÊåëÊàòÔºåÂõ†‰∏∫È¢ÑËÆ≠ÁªÉËØ≠ÊñôÂ∫ì‰∏≠ÂæàÂ∞ëÂåÖÂê´ÁßÅÊúâ‰∏¥Â∫äÊàñË¥¶ÂçïÊï∞ÊçÆ„ÄÇÊú¨ÊñáÊèêÂá∫DRG-SapphireÔºåÂà©Áî®Â§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰ªé‰∏¥Â∫äÁ¨îËÆ∞‰∏≠Ëá™Âä®ËøõË°åDRGÁºñÁ†Å„ÄÇËØ•Ê®°ÂûãÂü∫‰∫éQwen2.5-7BÔºåÈááÁî®Âü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±ËøõË°åÁªÑÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâËÆ≠ÁªÉÔºåÈíàÂØπ‰ª•ÂæÄÊï∞Â≠¶‰ªªÂä°Êú™ËßÅÁöÑÈ¢ÜÂüüÁâπÂÆöÊåëÊàòÂºïÂÖ•‰∫Ü‰∏ÄÁ≥ªÂàóRLÂ¢ûÂº∫„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®MIMIC-IVÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÊÄßÔºåÂπ∂ÁîüÊàê‰∫ÜÁªèËøáÂåªÁîüÈ™åËØÅÁöÑDRGÂàÜÈÖçÊé®ÁêÜÔºåÊòæËëóÂ¢ûÂº∫‰∫ÜÂèØËß£ÈáäÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥DRGÁºñÁ†Å‰∏≠ÁöÑËá™Âä®ÂåñÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÁî±‰∫éÁº∫‰πèÈ¢ÜÂüüÁâπÂÆöÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥LLMsÂú®Ê≠§‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDRG-SapphireÈÄöËøáÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ÔºåÁªìÂêà‰∏¥Â∫äÁ¨îËÆ∞‰∏≠ÁöÑ‰ø°ÊÅØÔºåÂà©Áî®ËßÑÂàôÂ•ñÂä±Êú∫Âà∂Êù•‰ºòÂåñÁºñÁ†ÅËøáÁ®ãÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê®°ÂûãÂü∫‰∫éQwen2.5-7BÊû∂ÊûÑÔºåÈááÁî®ÁªÑÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâËøõË°åËÆ≠ÁªÉÔºåÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÊ®°ÂûãËÆ≠ÁªÉ„ÄÅÂ•ñÂä±ËÆ°ÁÆóÂíåÊé®ÁêÜÁîüÊàêÁ≠â‰∏ªË¶ÅÊ®°Âùó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDRG-SapphireÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÈíàÂØπÈ¢ÜÂüüÁâπÂÆöÊåëÊàòÁöÑÂº∫ÂåñÂ≠¶‰π†Â¢ûÂº∫ÔºåËß£ÂÜ≥‰∫Ü‰ª•ÂæÄÊñπÊ≥ïÂú®Â§ÑÁêÜOOD‰ªªÂä°Êó∂ÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ®°ÂûãÁöÑËÆ≠ÁªÉËøáÁ®ã‰∏≠ÈááÁî®‰∫ÜÂü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåËÆæÁΩÆ‰∫ÜÈÄÇÂΩìÁöÑË∂ÖÂèÇÊï∞‰ª•Âπ≥Ë°°Êé¢Á¥¢‰∏éÂà©Áî®ÔºåÁ°Æ‰øùÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂ≠¶‰π†DRGÁºñÁ†ÅÁöÑÂ§çÊùÇÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DRG-SapphireÂú®MIMIC-IVÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÁéáÔºåÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆË°®ÊòéÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÊèêÂçá‰∫ÜÊòæËëóÁöÑÂπÖÂ∫¶Ôºå‰∏îÁîüÊàêÁöÑÊé®ÁêÜËøáÁ®ãÂæóÂà∞‰∫ÜÂåªÁîüÁöÑÈ™åËØÅÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂåªÈô¢ÁÆ°ÁêÜ„ÄÅÂåªÁñóË¥¶ÂçïÂ§ÑÁêÜÂíå‰∏¥Â∫äÊï∞ÊçÆÂàÜÊûêÁ≠â„ÄÇÈÄöËøáËá™Âä®ÂåñDRGÁºñÁ†ÅÔºåÂåªÈô¢ÂèØ‰ª•ÊèêÈ´òËøêËê•ÊïàÁéáÔºåÂáèÂ∞ë‰∫∫Â∑•ÊàêÊú¨ÔºåÂêåÊó∂ÊèêÂçáÁºñÁ†ÅÁöÑÂáÜÁ°ÆÊÄßÂíå‰∏ÄËá¥ÊÄßÔºåÊú™Êù•ÂèØËÉΩÂØπÂåªÁñóË°å‰∏ö‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.

