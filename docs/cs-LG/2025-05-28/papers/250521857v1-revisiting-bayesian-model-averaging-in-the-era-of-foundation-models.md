---
layout: default
title: Revisiting Bayesian Model Averaging in the Era of Foundation Models
---

# Revisiting Bayesian Model Averaging in the Era of Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21857" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21857v1</a>
  <a href="https://arxiv.org/pdf/2505.21857.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21857v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21857v1', 'Revisiting Bayesian Model Averaging in the Era of Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mijung Park

**åˆ†ç±»**: cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè´å¶æ–¯æ¨¡å‹å¹³å‡çš„çº¿æ€§åˆ†ç±»å™¨ä»¥æå‡åˆ†ç±»æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è´å¶æ–¯æ¨¡å‹å¹³å‡` `çº¿æ€§åˆ†ç±»å™¨` `åŸºç¡€æ¨¡å‹` `æ¨¡å‹é›†æˆ` `åˆ†ç±»æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡å‹é›†æˆæ–¹æ³•åœ¨å¤„ç†åŸºç¡€æ¨¡å‹æ—¶é¢ä¸´è®¡ç®—å¤æ‚æ€§å’Œæ€§èƒ½æå‡çš„æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥å¯è®­ç»ƒçš„çº¿æ€§åˆ†ç±»å™¨å’Œä¼˜åŒ–æ¨¡å‹å¹³å‡æ–¹æ¡ˆæ¥è§£å†³BMAçš„å¯è¡Œæ€§é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é‡æ–°å®¡è§†äº†ç»å…¸çš„è´å¶æ–¯æ¨¡å‹å¹³å‡ï¼ˆBMAï¼‰èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆé¢„è®­ç»ƒå’Œè½»å¾®å¾®è°ƒçš„åŸºç¡€æ¨¡å‹æ¥å¢å¼ºå›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„åˆ†ç±»æ€§èƒ½ã€‚ä¸ºä½¿BMAåœ¨åŸºç¡€æ¨¡å‹ä¸‹å¯è¡Œï¼Œä½œè€…å¼•å…¥äº†å¯è®­ç»ƒçš„çº¿æ€§åˆ†ç±»å™¨ï¼Œè¿™äº›åˆ†ç±»å™¨ä»¥å†»ç»“çš„åŸºç¡€æ¨¡å‹ç‰¹å¾ä½œä¸ºè¾“å…¥ã€‚æ¨¡å‹åéªŒåˆ†å¸ƒèƒ½å¤ŸæŒ‡ç¤ºå“ªäº›çº¿æ€§å¤´å’Œå†»ç»“ç‰¹å¾æ›´é€‚åˆç‰¹å®šæ•°æ®é›†ï¼Œä»è€Œå½¢æˆä¸€ç§æœ‰åŸåˆ™çš„æ¨¡å‹é›†æˆæ–¹æ³•ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è®¡ç®—æˆæœ¬æ›´ä½ã€å¯ä¼˜åŒ–çš„æ¨¡å‹å¹³å‡æ–¹æ¡ˆï¼ˆOMAï¼‰ï¼Œé€šè¿‡å‡å°‘æ¥è‡ªé›†æˆæ¨¡å‹é¢„æµ‹çš„æƒŠè®¶é‡ï¼ˆé¢„æµ‹çš„æœŸæœ›ç†µï¼‰æ¥ç›´æ¥ä¼˜åŒ–æ¨¡å‹é›†æˆæƒé‡ã€‚è¿™äº›æ–¹æ³•å°†ä¸ºæœªæ¥æ›´ä¼˜çš„åŸºç¡€æ¨¡å‹çš„åº”ç”¨æä¾›å¯èƒ½ï¼Œæå‡æŒ‘æˆ˜æ€§åˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨åŸºç¡€æ¨¡å‹ä¸‹è¿›è¡Œè´å¶æ–¯æ¨¡å‹å¹³å‡ï¼ˆBMAï¼‰æ—¶çš„è®¡ç®—å¤æ‚æ€§å’Œæ€§èƒ½æå‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¾€å¾€éš¾ä»¥æœ‰æ•ˆé›†æˆä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šä½œè€…æå‡ºé€šè¿‡å¼•å…¥å¯è®­ç»ƒçš„çº¿æ€§åˆ†ç±»å™¨ï¼Œä»¥å†»ç»“çš„åŸºç¡€æ¨¡å‹ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œæ¥å®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹é›†æˆã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹é›†æˆæƒé‡ï¼Œå‡å°‘é¢„æµ‹çš„æƒŠè®¶é‡ï¼Œä»è€Œæå‡åˆ†ç±»æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯çº¿æ€§åˆ†ç±»å™¨æ¨¡å—ï¼Œè´Ÿè´£å¤„ç†å†»ç»“ç‰¹å¾å¹¶è¿›è¡Œåˆ†ç±»ï¼›å…¶æ¬¡æ˜¯æ¨¡å‹å¹³å‡æ¨¡å—ï¼Œé€šè¿‡ä¼˜åŒ–é›†æˆæƒé‡æ¥æå‡æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¯è®­ç»ƒçš„çº¿æ€§åˆ†ç±»å™¨å’Œä¼˜åŒ–æ¨¡å‹å¹³å‡æ–¹æ¡ˆï¼ˆOMAï¼‰ï¼Œä½¿å¾—BMAåœ¨åŸºç¡€æ¨¡å‹ä¸‹å˜å¾—å¯è¡Œä¸”é«˜æ•ˆã€‚è¿™ä¸ä¼ ç»Ÿçš„BMAæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œä½œè€…è®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹é›†æˆæƒé‡ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†ä¸åŒå‚æ•°è®¾ç½®å¯¹åˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„æ¨¡å‹é›†æˆæ–¹æ³•ï¼Œåˆ†ç±»å‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°5%è‡³10%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œä¼˜åŒ–åçš„æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°æ®æ—¶è¡¨ç°å‡ºæ›´ä½çš„é¢„æµ‹ç†µï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ç­‰å¤šä¸ªæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†å¤§é‡é¢„è®­ç»ƒæ¨¡å‹çš„åœºæ™¯ä¸­ã€‚é€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹é›†æˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡åˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We revisit the classical, full-fledged Bayesian model averaging (BMA) paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to enhance the classification performance on image and text data. To make BMA tractable under foundation models, we introduce trainable linear classifiers that take frozen features from the pre-trained foundation models as inputs. The model posteriors over the linear classifiers tell us which linear heads and frozen features are better suited for a given dataset, resulting in a principled model ensembling method. Furthermore, we propose a computationally cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize the model ensemble weights, just like those weights based on model posterior distributions in BMA, by reducing the amount of surprise (expected entropy of the predictions) we get from predictions of ensembled models. With the rapid development of foundation models, these approaches will enable the incorporation of future, possibly significantly better foundation models to enhance the performance of challenging classification tasks.

