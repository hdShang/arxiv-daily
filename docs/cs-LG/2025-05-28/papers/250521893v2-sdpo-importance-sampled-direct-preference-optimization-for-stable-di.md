---
layout: default
title: "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training"
---

# SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21893" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21893v2</a>
  <a href="https://arxiv.org/pdf/2505.21893.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21893v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21893v2', 'SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaomeng Yang, Zhiyu Tan, Junyan Wang, Zhijian Zhou, Hao Li

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28 (æ›´æ–°: 2025-09-25)

**å¤‡æ³¨**: This version contains a critical error in the main theorem and proof design that affects the validity of the results

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSDPOä»¥è§£å†³æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­çš„åå·®å’Œä¸ç¨³å®šé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `åå¥½å­¦ä¹ ` `é‡è¦æ€§é‡‡æ ·` `è®­ç»ƒç¨³å®šæ€§` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Diffusion-DPOæ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´æ—¶é—´æ­¥ä¾èµ–çš„ä¸ç¨³å®šæ€§å’Œç¦»ç­–ç•¥åå·®ç­‰æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºDPO-C&Mç­–ç•¥ï¼Œé€šè¿‡å‰ªåˆ‡å’Œå±è”½æ— ä¿¡æ¯æ—¶é—´æ­¥æ¥æ”¹å–„ç¨³å®šæ€§ï¼Œå¹¶å¼•å…¥SDPOæ¡†æ¶ä»¥çº æ­£ç¦»ç­–ç•¥åå·®ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSDPOåœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Šäº†æ ‡å‡†Diffusion-DPOï¼Œè¡¨ç°å‡ºæ›´å¥½çš„åå¥½å¯¹é½å’Œè®­ç»ƒé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åå¥½å­¦ä¹ å·²æˆä¸ºå¯¹é½ç”Ÿæˆæ¨¡å‹ä¸äººç±»æœŸæœ›çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæœ€è¿‘é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ‰©å±•åˆ°æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¦‚Diffusion-DPOé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæ—¶é—´æ­¥ä¾èµ–çš„ä¸ç¨³å®šæ€§å’Œç”±ä¼˜åŒ–ä¸æ•°æ®æ”¶é›†ç­–ç•¥ä¸åŒ¹é…å¼•èµ·çš„ç¦»ç­–ç•¥åå·®ã€‚æœ¬æ–‡åˆ†æäº†åå‘æ‰©æ•£è½¨è¿¹ï¼Œå‘ç°ä¸ç¨³å®šæ€§ä¸»è¦å‘ç”Ÿåœ¨ä½é‡è¦æ€§æƒé‡çš„æ—©æœŸæ—¶é—´æ­¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†DPO-C&Mç­–ç•¥ï¼Œé€šè¿‡å‰ªåˆ‡å’Œå±è”½æ— ä¿¡æ¯æ—¶é—´æ­¥æ¥æ”¹å–„ç¨³å®šæ€§ï¼ŒåŒæ—¶éƒ¨åˆ†ç¼“è§£ç¦»ç­–ç•¥åå·®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†SDPOï¼ˆé‡è¦æ€§é‡‡æ ·ç›´æ¥åå¥½ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†é‡è¦æ€§é‡‡æ ·çº³å…¥ç›®æ ‡çš„åŸåˆ™æ€§æ¡†æ¶ï¼Œæ—¨åœ¨å®Œå…¨çº æ­£ç¦»ç­–ç•¥åå·®å¹¶å¼ºè°ƒæ‰©æ•£è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSDPOåœ¨VBenchè¯„åˆ†ã€äººç±»åå¥½å¯¹é½å’Œè®­ç»ƒé²æ£’æ€§æ–¹é¢å‡ä¼˜äºæ ‡å‡†Diffusion-DPOã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§å’Œç¦»ç­–ç•¥åå·®é—®é¢˜ã€‚ç°æœ‰çš„Diffusion-DPOæ–¹æ³•åœ¨æ—©æœŸæ—¶é—´æ­¥è¡¨ç°å‡ºé«˜æ¢¯åº¦æ–¹å·®å’Œä¸åŒ¹é…çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ†æåå‘æ‰©æ•£è½¨è¿¹ï¼Œå‘ç°ä¸ç¨³å®šæ€§ä¸»è¦é›†ä¸­åœ¨ä½é‡è¦æ€§æƒé‡çš„æ—©æœŸæ—¶é—´æ­¥ã€‚æå‡ºDPO-C&Mç­–ç•¥ä»¥æ”¹å–„ç¨³å®šæ€§ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼•å…¥SDPOæ¡†æ¶ï¼Œåˆ©ç”¨é‡è¦æ€§é‡‡æ ·æ¥çº æ­£ç¦»ç­–ç•¥åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSDPOæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯DPO-C&Mç­–ç•¥ç”¨äºç¨³å®šè®­ç»ƒï¼ŒäºŒæ˜¯é‡è¦æ€§é‡‡æ ·æœºåˆ¶ç”¨äºä¼˜åŒ–ç›®æ ‡ï¼Œå¼ºè°ƒä¿¡æ¯æ›´æ–°ã€‚æ•´ä½“æµç¨‹ä»æ•°æ®æ”¶é›†åˆ°ä¼˜åŒ–ç›®æ ‡å†åˆ°æ¨¡å‹æ›´æ–°ï¼Œç¡®ä¿æ¯ä¸€æ­¥éƒ½èƒ½æœ‰æ•ˆå¯¹é½äººç±»åå¥½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSDPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†é‡è¦æ€§é‡‡æ ·å¼•å…¥åå¥½ä¼˜åŒ–ä¸­ï¼Œå®Œå…¨çº æ­£äº†ç¦»ç­–ç•¥åå·®ï¼Œå¼ºè°ƒäº†ä¿¡æ¯æ›´æ–°çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¯¹æ—¶é—´æ­¥çš„æ•æ„Ÿæ€§å’Œä¼˜åŒ–ç­–ç•¥çš„è°ƒæ•´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SDPOä¸­ï¼Œé‡è¦æ€§æƒé‡çš„è®¡ç®—å’Œæ—¶é—´æ­¥çš„é€‰æ‹©æ˜¯å…³é”®è®¾è®¡å› ç´ ã€‚æŸå¤±å‡½æ•°ç»è¿‡è°ƒæ•´ï¼Œä»¥ç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é‡è¦æ—¶é—´æ­¥çš„æ›´æ–°ç»™äºˆæ›´å¤šå…³æ³¨ï¼ŒåŒæ—¶é‡‡ç”¨äº†å‰ªåˆ‡å’Œå±è”½ç­–ç•¥ä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSDPOåœ¨VBenchè¯„åˆ†ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†Diffusion-DPOï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚æ­¤å¤–ï¼ŒSDPOåœ¨ä¸äººç±»åå¥½çš„å¯¹é½å’Œè®­ç»ƒé²æ£’æ€§æ–¹é¢ä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€è§†é¢‘ç”Ÿæˆã€å›¾åƒåˆæˆç­‰ã€‚é€šè¿‡æé«˜æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œå¯¹äººç±»åå¥½çš„å¯¹é½èƒ½åŠ›ï¼ŒSDPOèƒ½å¤Ÿåœ¨å®é™…ç”Ÿæˆä»»åŠ¡ä¸­æä¾›æ›´é«˜è´¨é‡çš„è¾“å‡ºï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Preference learning has become a central technique for aligning generative models with human expectations. Recently, it has been extended to diffusion models through methods like Direct Preference Optimization (DPO). However, existing approaches such as Diffusion-DPO suffer from two key challenges: timestep-dependent instability, caused by a mismatch between the reverse and forward diffusion processes and by high gradient variance in early noisy timesteps, and off-policy bias arising from the mismatch between optimization and data collection policies. We begin by analyzing the reverse diffusion trajectory and observe that instability primarily occurs at early timesteps with low importance weights. To address these issues, we first propose DPO-C\&M, a practical strategy that improves stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Building on this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a principled framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO, with SDPO achieving superior VBench scores, human preference alignment, and training robustness. These results highlight the importance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning.

