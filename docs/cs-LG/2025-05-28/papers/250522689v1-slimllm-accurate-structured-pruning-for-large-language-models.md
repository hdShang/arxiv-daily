---
layout: default
title: SlimLLM: Accurate Structured Pruning for Large Language Models
---

# SlimLLM: Accurate Structured Pruning for Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.22689" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.22689v1</a>
  <a href="https://arxiv.org/pdf/2505.22689.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.22689v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.22689v1', 'SlimLLM: Accurate Structured Pruning for Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSlimLLMä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–å‰ªæé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–å‰ªæ` `æ¨¡å‹å‹ç¼©` `æ€§èƒ½æ¢å¤` `é€šé“é‡è¦æ€§è¯„ä¼°` `æ³¨æ„åŠ›å¤´å‰ªæ` `çº¿æ€§å›å½’ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•åœ¨è¯„ä¼°å­æ¨¡å—é‡è¦æ€§æ—¶ï¼Œå¾€å¾€åªå…³æ³¨å•ä¸ªå…ƒç´ ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘å…ƒç´ é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚
2. SlimLLMé€šè¿‡æ•´ä½“è¯„ä¼°é€šé“å’Œæ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å‰ªæç­–ç•¥ï¼Œå¹¶å¼•å…¥çº¿æ€§å›å½’ä»¥å¿«é€Ÿæ¢å¤æ¨¡å‹æ€§èƒ½ã€‚
3. åœ¨LLaMAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSlimLLMçš„è¡¨ç°ä¼˜äºå…¶ä»–å‰ªææ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è¯­è¨€æ¨¡å‹å‹ç¼©ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶å·¨å¤§çš„è®¡ç®—æˆæœ¬è€Œé™åˆ¶äº†å…¶éƒ¨ç½²å’Œåº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç»“æ„åŒ–å‰ªææˆä¸ºå‹ç¼©LLMså‚æ•°çš„æœ‰æ•ˆæ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSlimLLMçš„å¿«é€Ÿç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œé€šè¿‡è¯„ä¼°æ•´ä¸ªé€šé“æˆ–æ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œè€ƒè™‘äº†å­æ¨¡å—å†…å…ƒç´ ä¹‹é—´çš„ç›¸äº’ä¾èµ–ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ç®€å•çš„çº¿æ€§å›å½’ç­–ç•¥ä»¥å¿«é€Ÿæ¢å¤æ€§èƒ½ï¼Œå¹¶æå‡ºåŸºäºå±‚çš„é‡è¦æ€§æ¯”ç‡æ¥ç¡®å®šæ¯å±‚çš„å‰ªææ¯”ä¾‹ã€‚åŸºäºLLaMAåŸºå‡†æµ‹è¯•ç»“æœï¼ŒSlimLLMåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–å‰ªæé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°å­æ¨¡å—é‡è¦æ€§æ—¶å­˜åœ¨å±€é™ï¼Œæœªèƒ½è€ƒè™‘å…ƒç´ é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œå¯¼è‡´æ€§èƒ½æŸå¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSlimLLMé€šè¿‡æ•´ä½“è¯„ä¼°é€šé“å’Œæ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œé¿å…äº†ä»…ä¾èµ–å•ä¸ªå…ƒç´ çš„é‡è¦æ€§èšåˆï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å‰ªæã€‚åŒæ—¶ï¼Œè®¾è®¡äº†çº¿æ€§å›å½’ç­–ç•¥ä»¥å¿«é€Ÿæ¢å¤æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSlimLLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é‡è¦æ€§è¯„ä¼°æ¨¡å—ã€å‰ªæå†³ç­–æ¨¡å—å’Œæ€§èƒ½æ¢å¤æ¨¡å—ã€‚é‡è¦æ€§è¯„ä¼°æ¨¡å—è´Ÿè´£è®¡ç®—é€šé“å’Œæ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œå‰ªæå†³ç­–æ¨¡å—æ ¹æ®å±‚çš„é‡è¦æ€§æ¯”ç‡ç¡®å®šå‰ªææ¯”ä¾‹ï¼Œæ€§èƒ½æ¢å¤æ¨¡å—åˆ™é€šè¿‡çº¿æ€§å›å½’å¿«é€Ÿæ¢å¤æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šSlimLLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ•´ä½“é‡è¦æ€§è¯„ä¼°æ–¹æ³•ï¼Œè€ƒè™‘äº†å­æ¨¡å—å†…å…ƒç´ çš„ç›¸äº’ä¾èµ–æ€§ï¼Œæ˜¾è‘—æé«˜äº†å‰ªæçš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹çš„ä¿ç•™æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒSlimLLMé‡‡ç”¨äº†åŸºäºå±‚çš„é‡è¦æ€§æ¯”ç‡æ¥åŠ¨æ€è°ƒæ•´æ¯å±‚çš„å‰ªææ¯”ä¾‹ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šåˆ™ç»“åˆäº†çº¿æ€§å›å½’ä»¥ä¼˜åŒ–è¾“å‡ºçŸ©é˜µçš„æ¢å¤æ•ˆæœã€‚æ•´ä½“ç½‘ç»œç»“æ„ä¿æŒäº†å¤§è¯­è¨€æ¨¡å‹çš„åŸºæœ¬æ¶æ„ï¼Œç¡®ä¿äº†å‰ªæåçš„æ¨¡å‹ä»å…·å¤‡è‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨LLaMAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSlimLLMçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„å‰ªææ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚è¯¥æ–¹æ³•åœ¨å‰ªææ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½æ¢å¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è¯­è¨€æ¨¡å‹å‹ç¼©é¢†åŸŸçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SlimLLMçš„ç ”ç©¶æˆæœåœ¨å¤§è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ã€‚é€šè¿‡æœ‰æ•ˆçš„ç»“æ„åŒ–å‰ªæï¼ŒSlimLLMèƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ï¼Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠå’Œæ¨å¹¿ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.

