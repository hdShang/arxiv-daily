---
layout: default
title: "CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts"
---

# CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05063" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05063v1</a>
  <a href="https://arxiv.org/pdf/2505.05063.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05063v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05063v1', 'CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Manik Sheokand, Parth Sawant

**åˆ†ç±»**: cs.LG, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCodeMixBenchä»¥è§£å†³å¤šè¯­è¨€ä»£ç ç”Ÿæˆè¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç ç”Ÿæˆ` `å¤šè¯­è¨€å¤„ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯„ä¼°åŸºå‡†` `ä»£ç æ··åˆè¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æç¤ºï¼Œå¿½è§†äº†å¤šè¯­è¨€å¼€å‘è€…çš„å®é™…éœ€æ±‚ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¤Ÿå…¨é¢ã€‚
2. æå‡ºCodeMixBenchï¼Œé€šè¿‡å¼•å…¥æ§åˆ¶ä»£ç æ··åˆï¼ˆCMDï¼‰æ¥è¯„ä¼°LLMsåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä»£ç æ··åˆæç¤ºçš„Pass@1æ€§èƒ½æ˜¾è‘—ä½äºè‹±è¯­æç¤ºï¼Œå°¤å…¶æ˜¯å°æ¨¡å‹åœ¨é«˜CMDæ°´å¹³ä¸‹è¡¨ç°æ›´å·®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå¹¿æ³›åº”ç”¨äºä»£ç è¡¥å…¨ã€è°ƒè¯•å’Œç¼–ç¨‹è¾…åŠ©ç­‰åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†å¦‚HumanEvalã€MBPPå’ŒBigCodeBenchä¸»è¦é›†ä¸­åœ¨è‹±è¯­æç¤ºä¸Šï¼Œå¿½è§†äº†å¤šè¯­è¨€å¼€å‘è€…åœ¨ä¸LLMsäº¤äº’æ—¶å¸¸ç”¨çš„ä»£ç æ··åˆè¯­è¨€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CodeMixBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMsåœ¨ä»£ç æ··åˆæç¤ºä¸‹çš„ç”Ÿæˆèƒ½åŠ›çš„æ–°åŸºå‡†ã€‚CodeMixBenchåŸºäºBigCodeBenchæ„å»ºï¼Œå¼•å…¥äº†æ§åˆ¶ä»£ç æ··åˆï¼ˆCMDï¼‰ï¼Œæ¶µç›–äº†ä¸‰ç§è¯­è¨€å¯¹ï¼šHinglishï¼ˆå°åœ°è¯­-è‹±è¯­ï¼‰ã€è¥¿ç­ç‰™è¯­-è‹±è¯­å’Œæ±‰è¯­æ‹¼éŸ³-è‹±è¯­ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸ä»…ä½¿ç”¨è‹±è¯­çš„æç¤ºç›¸æ¯”ï¼Œä»£ç æ··åˆæç¤ºåœ¨Pass@1æ€§èƒ½ä¸ŠæŒç»­ä¸‹é™ï¼Œä¸”åœ¨è¾ƒé«˜çš„CMDæ°´å¹³ä¸‹ï¼Œå°æ¨¡å‹çš„æ€§èƒ½ä¸‹é™å¹…åº¦æ›´å¤§ã€‚CodeMixBenchä¸ºç ”ç©¶å¤šè¯­è¨€ä»£ç ç”Ÿæˆæä¾›äº†ç°å®çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶çªå‡ºäº†æ„å»ºèƒ½å¤Ÿåœ¨å¤šæ ·è¯­è¨€ç¯å¢ƒä¸­è‰¯å¥½æ³›åŒ–çš„ç¨³å¥ä»£ç ç”Ÿæˆæ¨¡å‹çš„æ–°æŒ‘æˆ˜å’Œæ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä»£ç ç”Ÿæˆè¯„ä¼°åŸºå‡†å¯¹å¤šè¯­è¨€æç¤ºçš„å¿½è§†ï¼Œç‰¹åˆ«æ˜¯ä»£ç æ··åˆè¯­è¨€çš„ä½¿ç”¨ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½åæ˜ çœŸå®å¼€å‘ç¯å¢ƒä¸­çš„å¤šæ ·æ€§ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ§åˆ¶ä»£ç æ··åˆï¼ˆCMDï¼‰ï¼Œæœ¬æ–‡è®¾è®¡äº†CodeMixBenchï¼Œä»¥è¯„ä¼°LLMsåœ¨å¤„ç†å¤šè¯­è¨€æç¤ºæ—¶çš„ç”Ÿæˆèƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ›´å¥½åœ°æ¨¡æ‹Ÿå¤šè¯­è¨€å¼€å‘è€…çš„å®é™…ä½¿ç”¨åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCodeMixBenchåŸºäºBigCodeBenchæ„å»ºï¼Œæ¶µç›–ä¸‰ç§è¯­è¨€å¯¹çš„ä»£ç æ··åˆæç¤ºã€‚è¯„ä¼°æµç¨‹åŒ…æ‹¬ç”Ÿæˆä»£ç ã€è®¡ç®—æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚Pass@1ï¼‰ä»¥åŠå¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨ä¸åŒCMDæ°´å¹³ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ§åˆ¶ä»£ç æ··åˆçš„æœºåˆ¶ï¼Œä½¿å¾—è¯„ä¼°ä¸ä»…é™äºå•ä¸€è¯­è¨€æç¤ºï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°åæ˜ æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCodeMixBenchæä¾›äº†æ›´å…·ç°å®æ„ä¹‰çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„CMDæ°´å¹³ï¼Œå¹¶è¯„ä¼°äº†1.5Båˆ°15Bå‚æ•°çš„å¤šç§å¼€æºä»£ç ç”Ÿæˆæ¨¡å‹ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„å…·ä½“ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»£ç æ··åˆæç¤ºçš„Pass@1æ€§èƒ½æ™®éä½äºè‹±è¯­æç¤ºï¼Œå°¤å…¶æ˜¯åœ¨é«˜CMDæ°´å¹³ä¸‹ï¼Œå°æ¨¡å‹çš„æ€§èƒ½ä¸‹é™å¹…åº¦æ›´å¤§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å¤šè¯­è¨€ç¯å¢ƒä¸‹ä»£ç ç”Ÿæˆæ¨¡å‹çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šè¯­è¨€ç¼–ç¨‹ç¯å¢ƒä¸­çš„ä»£ç ç”Ÿæˆã€æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–è°ƒè¯•å·¥å…·ã€‚é€šè¿‡æä¾›æ›´çœŸå®çš„è¯„ä¼°æ¡†æ¶ï¼ŒCodeMixBenchèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…å’Œç ”ç©¶äººå‘˜ç†è§£å’Œæ”¹è¿›LLMsåœ¨å¤šè¯­è¨€åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œä»è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have achieved remarkable success in code generation tasks, powering various applications like code completion, debugging, and programming assistance. However, existing benchmarks such as HumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only prompts, overlooking the real-world scenario where multilingual developers often use code-mixed language while interacting with LLMs. To address this gap, we introduce CodeMixBench, a novel benchmark designed to evaluate the robustness of LLMs on code generation from code-mixed prompts. Built upon BigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the natural language parts of prompts across three language pairs: Hinglish (Hindi-English), Spanish-English, and Chinese Pinyin-English. We comprehensively evaluate a diverse set of open-source code generation models ranging from 1.5B to 15B parameters. Our results show that code-mixed prompts consistently degrade Pass@1 performance compared to their English-only counterparts, with performance drops increasing under higher CMD levels for smaller models. CodeMixBench provides a realistic evaluation framework for studying multilingual code generation and highlights new challenges and directions for building robust code generation models that generalize well across diverse linguistic settings.

