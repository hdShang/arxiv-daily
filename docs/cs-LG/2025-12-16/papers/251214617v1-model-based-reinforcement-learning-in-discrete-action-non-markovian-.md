---
layout: default
title: Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes
---

# Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes

**arXiv**: [2512.14617v1](https://arxiv.org/abs/2512.14617) | [PDF](https://arxiv.org/pdf/2512.14617.pdf)

**ä½œè€…**: Alessandro Trapasso, Luca Iocchi, Fabio Patrizi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 19 pages, 32 figures, includes appendix

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQR-MAXç®—æ³•ï¼Œè§£å†³ç¦»æ•£åŠ¨ä½œéžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹ä¸­çš„æ¨¡åž‹å­¦ä¹ ä¸Žç­–ç•¥ä¼˜åŒ–é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)** **åŠ¨ä½œç”Ÿæˆä¸Žç‰©ç†åŠ¨ç”» (Animation & Physics)** **ä¸–ç•Œæ¨¡åž‹ä¸Žé¢„æµ‹ (World Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `éžé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `æ¨¡åž‹å­¦ä¹ ` `å¥–åŠ±æœºå™¨` `æ ·æœ¬æ•ˆçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿé©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ éš¾ä»¥å¤„ç†å¥–åŠ±ä¾èµ–åŽ†å²çš„ä»»åŠ¡ï¼Œéžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹ç¼ºä¹æœ€ä¼˜æ€§å’Œæ ·æœ¬æ•ˆçŽ‡çš„ä¿è¯ã€‚
2. QR-MAXç®—æ³•é€šè¿‡å¥–åŠ±æœºå™¨åˆ†è§£é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ å’Œéžé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†ï¼Œå®žçŽ°é«˜æ•ˆå­¦ä¹ ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒQR-MAXåœ¨æ ·æœ¬æ•ˆçŽ‡å’Œå¯»æ‰¾æœ€ä¼˜ç­–ç•¥çš„é²æ£’æ€§æ–¹é¢ä¼˜äºŽçŽ°æœ‰åŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå®žé™…å†³ç­–é—®é¢˜ä¾èµ–äºŽæ•´ä¸ªç³»ç»ŸåŽ†å²ï¼Œè€Œéžä»…ä¾èµ–äºŽè¾¾åˆ°å…·æœ‰æœŸæœ›å±žæ€§çš„çŠ¶æ€ã€‚é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ (RL)æ–¹æ³•ä¸é€‚ç”¨äºŽæ­¤ç±»ä»»åŠ¡ï¼Œè€Œéžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹(NMRDPs)çš„RLä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå¤„ç†æ—¶é—´ä¾èµ–æ€§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é•¿æœŸä»¥æ¥ç¼ºä¹å…³äºŽ(è¿‘)æœ€ä¼˜æ€§å’Œæ ·æœ¬æ•ˆçŽ‡çš„å½¢å¼ä¿è¯ã€‚æˆ‘ä»¬æå‡ºäº†QR-MAXï¼Œä¸€ç§ç”¨äºŽç¦»æ•£NMRDPsçš„æ–°åž‹åŸºäºŽæ¨¡åž‹çš„ç®—æ³•ï¼Œå®ƒé€šè¿‡å¥–åŠ±æœºå™¨å°†é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ ä¸Žéžé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†åˆ†è§£å¼€æ¥ï¼Œä»Žè€Œè§£å†³äº†è¿™ä¸¤ä¸ªé—®é¢˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºŽç¦»æ•£åŠ¨ä½œNMRDPsçš„åŸºäºŽæ¨¡åž‹çš„RLç®—æ³•ï¼Œå®ƒåˆ©ç”¨è¿™ç§åˆ†è§£æ¥èŽ·å¾—PACæ”¶æ•›åˆ°å…·æœ‰å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦çš„Îµ-æœ€ä¼˜ç­–ç•¥ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†QR-MAXæ‰©å±•åˆ°å…·æœ‰Bucket-QR-MAXçš„è¿žç»­çŠ¶æ€ç©ºé—´ï¼ŒBucket-QR-MAXæ˜¯ä¸€ç§åŸºäºŽSimHashçš„ç¦»æ•£å™¨ï¼Œå®ƒä¿ç•™äº†ç›¸åŒçš„åˆ†è§£ç»“æž„ï¼Œå¹¶åœ¨æ²¡æœ‰æ‰‹åŠ¨ç½‘æ ¼åˆ’åˆ†æˆ–å‡½æ•°é€¼è¿‘çš„æƒ…å†µä¸‹å®žçŽ°äº†å¿«é€Ÿç¨³å®šçš„å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨å¤æ‚åº¦ä¸æ–­å¢žåŠ çš„çŽ¯å¢ƒä¸­ï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ŽçŽ°ä»£æœ€å…ˆè¿›çš„åŸºäºŽæ¨¡åž‹çš„RLæ–¹æ³•è¿›è¡Œäº†å®žéªŒæ¯”è¾ƒï¼Œç»“æžœè¡¨æ˜Žåœ¨æ ·æœ¬æ•ˆçŽ‡æ–¹é¢æœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨å¯»æ‰¾æœ€ä¼˜ç­–ç•¥æ–¹é¢å…·æœ‰æ›´é«˜çš„é²æ£’æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¦»æ•£åŠ¨ä½œéžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹(NMRDPs)ä¸­çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚ä¼ ç»Ÿçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)å‡è®¾å½“å‰çŠ¶æ€åŒ…å«äº†åšå‡ºæœ€ä¼˜å†³ç­–æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä½†çŽ°å®žä¸–ç•Œä¸­è®¸å¤šä»»åŠ¡çš„å¥–åŠ±ä¾èµ–äºŽæ•´ä¸ªåŽ†å²è½¨è¿¹ï¼Œè€Œéžå½“å‰çŠ¶æ€ã€‚çŽ°æœ‰çš„NMRDPsæ–¹æ³•ç¼ºä¹ç†è®ºä¿è¯ï¼Œå°¤å…¶æ˜¯åœ¨æ ·æœ¬æ•ˆçŽ‡å’Œæ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ ä¸Žéžé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†è¿›è¡Œè§£è€¦ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨å¥–åŠ±æœºå™¨(Reward Machine)æ¥å¤„ç†éžé©¬å°”å¯å¤«å¥–åŠ±å‡½æ•°ï¼ŒåŒæ—¶ä½¿ç”¨ä¼ ç»Ÿçš„é©¬å°”å¯å¤«æ¨¡åž‹æ¥å­¦ä¹ çŠ¶æ€è½¬ç§»æ¦‚çŽ‡ã€‚è¿™ç§åˆ†è§£ä½¿å¾—ç®—æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ•°æ®ï¼Œå¹¶èŽ·å¾—æ›´å¥½çš„ç†è®ºä¿è¯ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šQR-MAXç®—æ³•çš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é©¬å°”å¯å¤«è½¬ç§»æ¨¡åž‹å­¦ä¹ ï¼šä½¿ç”¨æ ‡å‡†çš„æ¨¡åž‹å­¦ä¹ æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œé¢‘çŽ‡è®¡æ•°ï¼‰æ¥ä¼°è®¡çŠ¶æ€è½¬ç§»æ¦‚çŽ‡ã€‚2) å¥–åŠ±æœºå™¨å­¦ä¹ ï¼šå­¦ä¹ å¥–åŠ±æœºå™¨çš„çŠ¶æ€è½¬ç§»å’Œå¥–åŠ±å‡½æ•°ï¼Œå¥–åŠ±æœºå™¨çš„çŠ¶æ€åŸºäºŽåŽ†å²ä¿¡æ¯ã€‚3) ç­–ç•¥ä¼˜åŒ–ï¼šä½¿ç”¨Q-learningç®—æ³•æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œå…¶ä¸­Qå‡½æ•°åŸºäºŽé©¬å°”å¯å¤«çŠ¶æ€å’Œå¥–åŠ±æœºå™¨çŠ¶æ€ã€‚4) è¿žç»­çŠ¶æ€ç©ºé—´æ‰©å±•ï¼šä½¿ç”¨Bucket-QR-MAXï¼Œä¸€ç§åŸºäºŽSimHashçš„ç¦»æ•£åŒ–æ–¹æ³•ï¼Œå°†è¿žç»­çŠ¶æ€ç©ºé—´ç¦»æ•£åŒ–ï¼Œä»¥ä¾¿åº”ç”¨QR-MAXç®—æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽå°†é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ ä¸Žéžé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†è¿›è¡Œåˆ†è§£ï¼Œå¹¶åˆ©ç”¨å¥–åŠ±æœºå™¨æ¥å¤„ç†éžé©¬å°”å¯å¤«æ€§ã€‚è¿™ç§åˆ†è§£ä½¿å¾—ç®—æ³•èƒ½å¤ŸèŽ·å¾—PACæ”¶æ•›åˆ°Îµ-æœ€ä¼˜ç­–ç•¥çš„ç†è®ºä¿è¯ï¼Œå¹¶ä¸”åœ¨æ ·æœ¬æ•ˆçŽ‡æ–¹é¢ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒBucket-QR-MAXé€šè¿‡SimHashè¿›è¡Œç¦»æ•£åŒ–ï¼Œé¿å…äº†æ‰‹åŠ¨ç½‘æ ¼åˆ’åˆ†å’Œå‡½æ•°é€¼è¿‘ï¼Œæé«˜äº†ç®—æ³•çš„é€‚ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šQR-MAXç®—æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¥–åŠ±æœºå™¨çš„è¡¨ç¤ºå’Œå­¦ä¹ æ–¹æ³•ã€‚2) Q-learningç®—æ³•çš„æ›´æ–°è§„åˆ™ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘é©¬å°”å¯å¤«çŠ¶æ€å’Œå¥–åŠ±æœºå™¨çŠ¶æ€ã€‚3) Bucket-QR-MAXä¸­SimHashå‡½æ•°çš„é€‰æ‹©å’Œå‚æ•°è®¾ç½®ï¼Œä»¥åŠæ¡¶å¤§å°çš„ç¡®å®šã€‚è®ºæ–‡ä¸­å¹¶æ²¡æœ‰è¯¦ç»†è¯´æ˜Žå…·ä½“çš„æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æž„ï¼Œå› ä¸ºè¯¥ç®—æ³•ä¸»è¦å…³æ³¨äºŽç¦»æ•£åŠ¨ä½œç©ºé—´å’Œæ¨¡åž‹å­¦ä¹ ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒQR-MAXç®—æ³•åœ¨å¤šä¸ªå¤æ‚çŽ¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆçŽ‡ï¼Œå¹¶å¢žå¼ºäº†å¯»æ‰¾æœ€ä¼˜ç­–ç•¥çš„é²æ£’æ€§ã€‚ä¸ŽçŽ°æœ‰çš„åŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒQR-MAXèƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶ä¸”åœ¨é¢å¯¹çŽ¯å¢ƒå˜åŒ–æ—¶è¡¨çŽ°å‡ºæ›´å¥½çš„é€‚åº”æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽéœ€è¦è€ƒè™‘åŽ†å²ä¿¡æ¯çš„å†³ç­–é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€ä»»åŠ¡è§„åˆ’ã€æ¸¸æˆAIç­‰ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ™ºèƒ½ä½“çš„æˆåŠŸä¸ä»…å–å†³äºŽå½“å‰çŠ¶æ€ï¼Œè¿˜å–å†³äºŽä¹‹å‰çš„è¡Œä¸ºåºåˆ—ã€‚è¯¥ç®—æ³•çš„å®žé™…ä»·å€¼åœ¨äºŽæé«˜æ ·æœ¬æ•ˆçŽ‡å’Œç­–ç•¥çš„é²æ£’æ€§ï¼Œä»Žè€Œé™ä½Žè®­ç»ƒæˆæœ¬å’Œæé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„çŽ¯å¢ƒå’Œä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚éƒ¨åˆ†å¯è§‚æµ‹çŽ¯å¢ƒå’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

