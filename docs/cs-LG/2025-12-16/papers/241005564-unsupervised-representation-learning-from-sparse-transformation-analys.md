---
layout: default
title: Unsupervised Representation Learning from Sparse Transformation Analysis
---

# Unsupervised Representation Learning from Sparse Transformation Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2410.05564" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2410.05564</a>
  <a href="https://arxiv.org/pdf/2410.05564.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2410.05564" onclick="toggleFavorite(this, '2410.05564', 'Unsupervised Representation Learning from Sparse Transformation Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yue Song, Thomas Anderson Keller, Yisong Yue, Pietro Perona, Max Welling

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç¨€ç–å˜æ¢åˆ†æçš„æ— ç›‘ç£è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè§£è€¦åºåˆ—æ•°æ®ä¸­çš„æ½œåœ¨å› ç´ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— ç›‘ç£å­¦ä¹ ` `è¡¨å¾å­¦ä¹ ` `è§£è€¦è¡¨å¾` `ç¨€ç–å˜æ¢` `æ¦‚ç‡æµæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¡¨å¾å­¦ä¹ æ–¹æ³•åœ¨ç¼–ç æ•ˆç‡ã€ç»Ÿè®¡ç‹¬ç«‹æ€§ç­‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆè§£è€¦åºåˆ—æ•°æ®ä¸­çš„æ½œåœ¨å› ç´ ã€‚
2. è¯¥æ–¹æ³•é€šè¿‡å°†æ½œåœ¨å˜é‡çš„å˜æ¢åˆ†è§£ä¸ºç¨€ç–åˆ†é‡ï¼Œå­¦ä¹ åºåˆ—æ•°æ®çš„è¡¨å¾ï¼Œä»è€Œå®ç°æ›´å¥½çš„è§£è€¦ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ•°æ®ä¼¼ç„¶å’Œæ— ç›‘ç£è¿‘ä¼¼ç­‰å˜è¯¯å·®æ–¹é¢å‡å–å¾—äº†é¢†å…ˆæ°´å¹³ï¼ŒéªŒè¯äº†è§£è€¦è¡¨å¾çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç¨€ç–å˜æ¢åˆ†æçš„è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºä»åºåˆ—æ•°æ®ä¸­è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ã€‚è¯¥æ–¹æ³•é¦–å…ˆå°†è¾“å…¥æ•°æ®ç¼–ç ä¸ºæ½œåœ¨æ¿€æ´»åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨æ¦‚ç‡æµæ¨¡å‹å¯¹è¿™äº›åˆ†å¸ƒè¿›è¡Œå˜æ¢ï¼Œæœ€åè§£ç ä»¥é¢„æµ‹æœªæ¥çš„è¾“å…¥çŠ¶æ€ã€‚æ¦‚ç‡æµæ¨¡å‹è¢«åˆ†è§£ä¸ºè‹¥å¹²æ—‹è½¬ï¼ˆæ— æ•£åº¦ï¼‰å‘é‡åœºå’Œè‹¥å¹²åŠ¿æµï¼ˆæ— æ—‹åº¦ï¼‰åœºã€‚é€šè¿‡æ–½åŠ ç¨€ç–æ€§å…ˆéªŒï¼Œé¼“åŠ±åªæœ‰å°‘é‡åœºåœ¨ä»»ä½•æ—¶åˆ»å¤„äºæ´»è·ƒçŠ¶æ€ï¼Œå¹¶æ¨æ–­æ¦‚ç‡æ²¿è¿™äº›åœºæµåŠ¨çš„é€Ÿåº¦ã€‚è¯¥æ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„å˜åˆ†ç›®æ ‡è¿›è¡Œå®Œå…¨æ— ç›‘ç£çš„è®­ç»ƒï¼Œä»è€Œäº§ç”Ÿä¸€ç§æ–°çš„è§£è€¦è¡¨å¾å½¢å¼ï¼Œå…¶ä¸­è¾“å…¥ä¸ä»…ç”±ç‹¬ç«‹å› ç´ çš„ç»„åˆè¡¨ç¤ºï¼Œè¿˜ç”±å­¦ä¹ åˆ°çš„æµåœºç»™å‡ºçš„ç‹¬ç«‹å˜æ¢åŸè¯­çš„ç»„åˆè¡¨ç¤ºã€‚å½“å°†å˜æ¢è§†ä¸ºå¯¹ç§°æ€§æ—¶ï¼Œå¯ä»¥å°†å…¶è§£é‡Šä¸ºå­¦ä¹ è¿‘ä¼¼ç­‰å˜è¡¨å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”±åºåˆ—å˜æ¢ç»„æˆçš„æ•°æ®é›†ä¸Šï¼Œåœ¨æ•°æ®ä¼¼ç„¶å’Œæ— ç›‘ç£è¿‘ä¼¼ç­‰å˜è¯¯å·®æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¯¥è®ºæ–‡æ—¨åœ¨è§£å†³ä»åºåˆ—æ•°æ®ä¸­æ— ç›‘ç£åœ°å­¦ä¹ è§£è€¦è¡¨å¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°å°†è¾“å…¥æ•°æ®åˆ†è§£ä¸ºç‹¬ç«‹çš„æ½œåœ¨å› ç´ ï¼Œå¹¶ä¸”éš¾ä»¥æ•æ‰æ•°æ®ä¸­çš„å˜æ¢å…³ç³»ã€‚è¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•å­¦ä¹ åˆ°æ—¢ç‹¬ç«‹åˆå…·æœ‰æ˜ç¡®ç‰©ç†æ„ä¹‰çš„å˜æ¢åŸè¯­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åºåˆ—æ•°æ®çš„å˜æ¢åˆ†è§£ä¸ºç¨€ç–çš„ã€ç‹¬ç«‹çš„å˜æ¢åŸè¯­ã€‚é€šè¿‡å­¦ä¹ æ¦‚ç‡æµæ¨¡å‹ï¼Œå°†æ½œåœ¨å˜é‡çš„å˜æ¢è¡¨ç¤ºä¸ºæ—‹è½¬å‘é‡åœºå’ŒåŠ¿æµåœºçš„ç»„åˆï¼Œå¹¶åˆ©ç”¨ç¨€ç–æ€§å…ˆéªŒæ¥é¼“åŠ±åªæœ‰å°‘é‡åœºåœ¨ä»»ä½•æ—¶åˆ»å¤„äºæ´»è·ƒçŠ¶æ€ã€‚è¿™ç§ç¨€ç–æ€§çº¦æŸæœ‰åŠ©äºè§£è€¦ä¸åŒçš„å˜æ¢å› ç´ ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å…·è§£é‡Šæ€§çš„è¡¨å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **ç¼–ç å™¨**ï¼šå°†è¾“å…¥æ•°æ®ç¼–ç ä¸ºæ½œåœ¨æ¿€æ´»åˆ†å¸ƒã€‚2) **æ¦‚ç‡æµæ¨¡å‹**ï¼šå°†æ½œåœ¨æ¿€æ´»åˆ†å¸ƒè¿›è¡Œå˜æ¢ï¼Œè¯¥æ¨¡å‹ç”±æ—‹è½¬å‘é‡åœºå’ŒåŠ¿æµåœºç»„æˆã€‚3) **è§£ç å™¨**ï¼šå°†å˜æ¢åçš„æ½œåœ¨æ¿€æ´»åˆ†å¸ƒè§£ç ä¸ºæœªæ¥çš„è¾“å…¥çŠ¶æ€ã€‚æ•´ä¸ªæ¡†æ¶ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„ç»“æ„è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æ•°æ®çš„ä¼¼ç„¶å‡½æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å˜æ¢åˆ†è§£ä¸ºç¨€ç–çš„ã€ç‹¬ç«‹çš„å˜æ¢åŸè¯­ã€‚é€šè¿‡å­¦ä¹ æ¦‚ç‡æµæ¨¡å‹ï¼Œå¹¶æ–½åŠ ç¨€ç–æ€§å…ˆéªŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£è€¦ä¸åŒçš„å˜æ¢å› ç´ ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å…·è§£é‡Šæ€§çš„è¡¨å¾ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿå­¦ä¹ åˆ°ç‹¬ç«‹çš„æ½œåœ¨å› ç´ ï¼Œè¿˜èƒ½å¤Ÿå­¦ä¹ åˆ°ç‹¬ç«‹çš„å˜æ¢åŸè¯­ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰æ•°æ®ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¦‚ç‡æµæ¨¡å‹è¢«åˆ†è§£ä¸ºè‹¥å¹²æ—‹è½¬ï¼ˆæ— æ•£åº¦ï¼‰å‘é‡åœºå’Œè‹¥å¹²åŠ¿æµï¼ˆæ— æ—‹åº¦ï¼‰åœºã€‚ç¨€ç–æ€§å…ˆéªŒé€šè¿‡L1æ­£åˆ™åŒ–æ¥å®ç°ï¼Œé¼“åŠ±åªæœ‰å°‘é‡åœºåœ¨ä»»ä½•æ—¶åˆ»å¤„äºæ´»è·ƒçŠ¶æ€ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æ„æŸå¤±å’ŒKLæ•£åº¦æŸå¤±ï¼Œç”¨äºä¿è¯é‡æ„çš„å‡†ç¡®æ€§å’Œæ½œåœ¨å˜é‡çš„åˆ†å¸ƒä¸å…ˆéªŒåˆ†å¸ƒçš„æ¥è¿‘ç¨‹åº¦ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®å–å†³äºå…·ä½“çš„æ•°æ®é›†å’Œä»»åŠ¡ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2410.05564/imgs/teaser5.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2410.05564/x1.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2410.05564/x2.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”±åºåˆ—å˜æ¢ç»„æˆçš„æ•°æ®é›†ä¸Šï¼Œåœ¨æ•°æ®ä¼¼ç„¶å’Œæ— ç›‘ç£è¿‘ä¼¼ç­‰å˜è¯¯å·®æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶å­¦ä¹ è§£è€¦è¡¨å¾çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¨¡å‹è¿˜èƒ½å¤Ÿå­¦ä¹ åˆ°å…·æœ‰æ˜ç¡®ç‰©ç†æ„ä¹‰çš„å˜æ¢åŸè¯­ï¼Œä¾‹å¦‚æ—‹è½¬ã€å¹³ç§»ç­‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç†è§£ã€æœºå™¨äººæ§åˆ¶ã€ç‰©ç†å»ºæ¨¡ç­‰ã€‚é€šè¿‡å­¦ä¹ è§£è€¦çš„è¡¨å¾ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£è§†é¢‘ä¸­çš„ç‰©ä½“è¿åŠ¨ã€æ§åˆ¶æœºå™¨äººçš„åŠ¨ä½œã€ä»¥åŠå»ºæ¨¡ç‰©ç†ç³»ç»Ÿçš„åŠ¨æ€è¡Œä¸ºã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºç”Ÿæˆæ–°çš„åºåˆ—æ•°æ®ï¼Œä¾‹å¦‚é€šè¿‡æ”¹å˜å˜æ¢åŸè¯­æ¥ç”Ÿæˆä¸åŒçš„è§†é¢‘ç‰‡æ®µæˆ–æœºå™¨äººåŠ¨ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> There is a vast literature on representation learning based on principles such as coding efficiency, statistical independence, causality, controllability, or symmetry. In this paper we propose to learn representations from sequence data by factorizing the transformations of the latent variables into sparse components. Input data are first encoded as distributions of latent activations and subsequently transformed using a probability flow model, before being decoded to predict a future input state. The flow model is decomposed into a number of rotational (divergence-free) vector fields and a number of potential flow (curl-free) fields. Our sparsity prior encourages only a small number of these fields to be active at any instant and infers the speed with which the probability flows along these fields. Training this model is completely unsupervised using a standard variational objective and results in a new form of disentangled representations where the input is not only represented by a combination of independent factors, but also by a combination of independent transformation primitives given by the learned flow fields. When viewing the transformations as symmetries one may interpret this as learning approximately equivariant representations. Empirically we demonstrate that this model achieves state of the art in terms of both data likelihood and unsupervised approximate equivariance errors on datasets composed of sequence transformations.

