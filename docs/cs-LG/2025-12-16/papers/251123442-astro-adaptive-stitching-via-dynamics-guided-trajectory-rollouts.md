---
layout: default
title: ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts
---

# ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.23442" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.23442</a>
  <a href="https://arxiv.org/pdf/2511.23442.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.23442" onclick="toggleFavorite(this, '2511.23442', 'ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hang Yu, Di Zhang, Qiwei Du, Yanping Zhao, Hai Zhang, Guang Chen, Eduardo E. Veas, Junqiao Zhao

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ASTROï¼šé€šè¿‡åŠ¨æ€å¼•å¯¼è½¨è¿¹å±•å¼€å®ç°è‡ªé€‚åº”æ‹¼æ¥ï¼Œæå‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `æ•°æ®å¢å¼º` `è½¨è¿¹æ‹¼æ¥` `åŠ¨æ€å¼•å¯¼` `Rollout Deviation Feedback`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»çº¿å¼ºåŒ–å­¦ä¹ å—é™äºæ•°æ®é›†è´¨é‡ï¼Œæ¬¡ä¼˜æˆ–ç¢ç‰‡åŒ–è½¨è¿¹å¯¼è‡´å¥–åŠ±ä¼ æ’­å›°éš¾ï¼Œå½±å“ç­–ç•¥å­¦ä¹ ã€‚
2. ASTROé€šè¿‡å­¦ä¹ æ—¶é—´è·ç¦»è¡¨ç¤ºå’ŒåŠ¨æ€å¼•å¯¼çš„æ‹¼æ¥è§„åˆ’å™¨ï¼Œç”Ÿæˆæ–°é¢–ä¸”åŠ¨æ€ä¸€è‡´çš„è½¨è¿¹ï¼Œå¢å¼ºæ•°æ®é›†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒASTROåœ¨OGBenchå’ŒD4RLç­‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰ç¦»çº¿RLå¢å¼ºæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ (RL)ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä»é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚ç„¶è€Œï¼ŒåŒ…å«æ¬¡ä¼˜å’Œç¢ç‰‡åŒ–è½¨è¿¹çš„æ•°æ®é›†ç»™å¥–åŠ±ä¼ æ’­å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¯¼è‡´ä¸å‡†ç¡®çš„ä»·å€¼ä¼°è®¡å’Œé™ä½çš„ç­–ç•¥æ€§èƒ½ã€‚è™½ç„¶é€šè¿‡ç”Ÿæˆæ¨¡å‹è¿›è¡Œè½¨è¿¹æ‹¼æ¥æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„å¢å¼ºæ–¹æ³•ç»å¸¸äº§ç”Ÿå±€é™äºè¡Œä¸ºç­–ç•¥æ”¯æŒæˆ–è¿ååº•å±‚åŠ¨æ€çš„è½¨è¿¹ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨ç­–ç•¥æ”¹è¿›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ASTROï¼Œä¸€ä¸ªæ•°æ®å¢å¼ºæ¡†æ¶ï¼Œä¸ºç¦»çº¿RLç”Ÿæˆåˆ†å¸ƒä¸Šæ–°é¢–ä¸”åŠ¨æ€ä¸€è‡´çš„è½¨è¿¹ã€‚ASTROé¦–å…ˆå­¦ä¹ ä¸€ä¸ªæ—¶é—´è·ç¦»è¡¨ç¤ºï¼Œä»¥è¯†åˆ«ä¸åŒçš„å’Œå¯åˆ°è¾¾çš„æ‹¼æ¥ç›®æ ‡ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªåŠ¨æ€å¼•å¯¼çš„æ‹¼æ¥è§„åˆ’å™¨ï¼Œé€šè¿‡Rollout Deviation Feedbackè‡ªé€‚åº”åœ°ç”Ÿæˆè¿æ¥åŠ¨ä½œåºåˆ—ï¼ŒRollout Deviation Feedbackè¢«å®šä¹‰ä¸ºç›®æ ‡çŠ¶æ€åºåˆ—ä¸æ‰§è¡Œé¢„æµ‹åŠ¨ä½œåå®é™…åˆ°è¾¾çŠ¶æ€åºåˆ—ä¹‹é—´çš„å·®è·ï¼Œä»¥æé«˜è½¨è¿¹æ‹¼æ¥çš„å¯è¡Œæ€§å’Œå¯è¾¾æ€§ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ‹¼æ¥ä¿ƒè¿›äº†æœ‰æ•ˆçš„å¢å¼ºï¼Œå¹¶æœ€ç»ˆå¢å¼ºäº†ç­–ç•¥å­¦ä¹ ã€‚ASTROåœ¨å„ç§ç®—æ³•ä¸­ä¼˜äºå…ˆå‰çš„ç¦»çº¿RLå¢å¼ºæ–¹æ³•ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„OGBenchå¥—ä»¶ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨æ ‡å‡†çš„ç¦»çº¿RLåŸºå‡†ï¼ˆå¦‚D4RLï¼‰ä¸Šå±•ç¤ºäº†ä¸€è‡´çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ é¢ä¸´æ•°æ®é›†è´¨é‡çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“æ•°æ®é›†ä¸­åŒ…å«å¤§é‡æ¬¡ä¼˜æˆ–ä¸å®Œæ•´çš„è½¨è¿¹æ—¶ã€‚è¿™äº›è½¨è¿¹ä¼šå¯¼è‡´å¥–åŠ±éš¾ä»¥å‡†ç¡®ä¼ æ’­ï¼Œä»è€Œå½±å“ä»·å€¼ä¼°è®¡å’Œç­–ç•¥å­¦ä¹ ã€‚ç°æœ‰çš„è½¨è¿¹æ‹¼æ¥æ–¹æ³•è¦ä¹ˆç”Ÿæˆçš„è½¨è¿¹è¿‡äºä¿å®ˆï¼Œå±€é™äºåŸå§‹æ•°æ®é›†çš„åˆ†å¸ƒï¼Œè¦ä¹ˆç”Ÿæˆçš„è½¨è¿¹è¿åç¯å¢ƒåŠ¨åŠ›å­¦ï¼Œå¯¼è‡´ç­–ç•¥æ€§èƒ½æå‡æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šASTROçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç”Ÿæˆæ—¢æ–°é¢–åˆç¬¦åˆç¯å¢ƒåŠ¨åŠ›å­¦çš„è½¨è¿¹æ¥å¢å¼ºç¦»çº¿æ•°æ®é›†ã€‚å®ƒé€šè¿‡å­¦ä¹ è½¨è¿¹ä¹‹é—´çš„æ—¶é—´è·ç¦»è¡¨ç¤ºæ¥ç¡®å®šåˆé€‚çš„æ‹¼æ¥ç›®æ ‡ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€å¼•å¯¼çš„æ‹¼æ¥è§„åˆ’å™¨æ¥ç”Ÿæˆè¿æ¥è¿™äº›ç›®æ ‡çš„åŠ¨ä½œåºåˆ—ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæé«˜è½¨è¿¹æ‹¼æ¥çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šASTROæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ—¶é—´è·ç¦»è¡¨ç¤ºå­¦ä¹ æ¨¡å—ï¼Œç”¨äºè¯†åˆ«å¯è¡Œçš„æ‹¼æ¥ç›®æ ‡ï¼›2) åŠ¨æ€å¼•å¯¼çš„æ‹¼æ¥è§„åˆ’å™¨ï¼Œç”¨äºç”Ÿæˆè¿æ¥è½¨è¿¹çš„åŠ¨ä½œåºåˆ—ã€‚æ‹¼æ¥è§„åˆ’å™¨ä½¿ç”¨Rollout Deviation Feedbackï¼Œå³ç›®æ ‡çŠ¶æ€åºåˆ—ä¸å®é™…åˆ°è¾¾çŠ¶æ€åºåˆ—ä¹‹é—´çš„å·®è·ï¼Œæ¥æŒ‡å¯¼åŠ¨ä½œåºåˆ—çš„ç”Ÿæˆï¼Œä»è€Œç¡®ä¿ç”Ÿæˆçš„è½¨è¿¹ç¬¦åˆç¯å¢ƒåŠ¨åŠ›å­¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šASTROçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŠ¨æ€å¼•å¯¼çš„æ‹¼æ¥è§„åˆ’å™¨å’ŒRollout Deviation Feedbackæœºåˆ¶ã€‚ä¼ ç»Ÿçš„è½¨è¿¹æ‹¼æ¥æ–¹æ³•é€šå¸¸ä¾èµ–äºç”Ÿæˆæ¨¡å‹æˆ–ç®€å•çš„æ’å€¼æ–¹æ³•ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆè½¨è¿¹çš„åŠ¨åŠ›å­¦ä¸€è‡´æ€§ã€‚ASTROé€šè¿‡Rollout Deviation Feedbackï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´ç”Ÿæˆçš„åŠ¨ä½œåºåˆ—ï¼Œä½¿å…¶æ›´æ¥è¿‘ç›®æ ‡çŠ¶æ€åºåˆ—ï¼Œä»è€Œæé«˜è½¨è¿¹æ‹¼æ¥çš„æˆåŠŸç‡å’Œç­–ç•¥å­¦ä¹ çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šASTROä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ—¶é—´è·ç¦»è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ï¼‰æ¥ç”Ÿæˆè¿æ¥è½¨è¿¹çš„åŠ¨ä½œåºåˆ—ã€‚Rollout Deviation Feedbackè¢«ç”¨ä½œä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°ï¼Œå¼•å¯¼åŠ¨ä½œåºåˆ—çš„ç”Ÿæˆã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2511.23442/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2511.23442/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2511.23442/fig/ori_heatmap.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

ASTROåœ¨OGBenchå¥—ä»¶ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨D4RLç­‰æ ‡å‡†ç¦»çº¿RLåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†ä¸€è‡´çš„æ”¹è¿›ã€‚å…·ä½“è€Œè¨€ï¼ŒASTROåœ¨æŸäº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•çš„10%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ•°æ®å¢å¼ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ASTROå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚æˆ–éš¾ä»¥è¿›è¡Œåœ¨çº¿æ¢ç´¢çš„åœºæ™¯ä¸‹ã€‚é€šè¿‡å¢å¼ºç¦»çº¿æ•°æ®é›†ï¼ŒASTROèƒ½å¤Ÿæé«˜æ™ºèƒ½ä½“çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ï¼Œé™ä½å¯¹å¤§é‡é«˜è´¨é‡æ•°æ®çš„ä¾èµ–ï¼ŒåŠ é€Ÿæ™ºèƒ½ä½“çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.

