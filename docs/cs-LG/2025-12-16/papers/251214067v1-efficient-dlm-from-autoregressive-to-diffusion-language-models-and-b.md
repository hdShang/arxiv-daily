---
layout: default
title: Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed
---

# Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed

**arXiv**: [2512.14067v1](https://arxiv.org/abs/2512.14067) | [PDF](https://arxiv.org/pdf/2512.14067.pdf)

**ä½œè€…**: Yonggan Fu, Lexington Whalen, Zhifan Ye, Xin Dong, Shizhe Diao, Jingyu Liu, Chengyue Wu, Hao Zhang, Enze Xie, Song Han, Maksim Khadkevich, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov

**åˆ†ç±»**: cs.CL, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEfficient-DLMæ¡†æž¶ï¼Œé€šè¿‡æ”¹è¿›AR-to-dLMè½¬æ¢æ–¹æ³•ï¼Œå®žçŽ°é«˜æ•ˆæ‰©æ•£è¯­è¨€æ¨¡åž‹ï¼Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å¤§å¹…æå‡ç”Ÿæˆé€Ÿåº¦ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `æ‰©æ•£è¯­è¨€æ¨¡åž‹` `è‡ªå›žå½’æ¨¡åž‹è½¬æ¢` `æ³¨æ„åŠ›æ¨¡å¼ä¼˜åŒ–` `å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆ` `è®­ç»ƒç­–ç•¥æ”¹è¿›` `æ¨¡åž‹æ•ˆçŽ‡æå‡` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ·±åº¦å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰AR-to-dLMè½¬æ¢æ–¹æ³•åœ¨æ³¨æ„åŠ›æ¨¡å¼å’Œè®­ç»ƒç›®æ ‡ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´è½¬æ¢åŽæ¨¡åž‹éš¾ä»¥åœ¨ä¿æŒARæ¨¡åž‹å‡†ç¡®æ€§çš„åŒæ—¶å®žçŽ°é«˜æ•ˆå¹¶è¡Œç”Ÿæˆã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºåŸºäºŽå—çŠ¶æ³¨æ„åŠ›æ¨¡å¼çš„æŒç»­é¢„è®­ç»ƒæ–¹æ¡ˆå’Œä½ç½®ç›¸å…³æŽ©ç ç­–ç•¥ï¼Œä¼˜åŒ–AR-to-dLMè½¬æ¢è¿‡ç¨‹ï¼Œå®žçŽ°å‡†ç¡®æ€§ä¸Žæ•ˆçŽ‡çš„å¹³è¡¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šEfficient-DLM 8Bæ¨¡åž‹åœ¨å‡†ç¡®çŽ‡å’Œåžåé‡ä¸Šå‡æ˜¾è‘—è¶…è¶ŠDream 7Bå’ŒQwen3 4Bç­‰åŸºçº¿æ¨¡åž‹ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£è¯­è¨€æ¨¡åž‹ï¼ˆdLMsï¼‰ä½œä¸ºä¸€ç§æ”¯æŒå¹¶è¡Œã€éžè‡ªå›žå½’ç”Ÿæˆçš„æ–°èŒƒå¼ï¼Œå±•çŽ°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶ä»Žå¤´å¼€å§‹è®­ç»ƒæ—¶çš„å­¦ä¹ æ•ˆçŽ‡ä»è½åŽäºŽè‡ªå›žå½’ï¼ˆARï¼‰è¯­è¨€æ¨¡åž‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ç ”ç©¶AR-to-dLMè½¬æ¢æ–¹æ³•ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„ARæ¨¡åž‹è½¬åŒ–ä¸ºé«˜æ•ˆçš„dLMsï¼Œåœ¨ä¿æŒARæ¨¡åž‹ä»»åŠ¡å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—æå‡ç”Ÿæˆé€Ÿåº¦ã€‚æˆ‘ä»¬é€šè¿‡åˆ†æžçŽ°æœ‰AR-to-dLMæ–¹æ³•åœ¨æ³¨æ„åŠ›æ¨¡å¼å’Œç›®æ ‡å‡½æ•°ä¸Šçš„å±€é™æ€§ï¼Œæå‡ºäº†æ›´æœ‰æ•ˆçš„è½¬æ¢åŽŸåˆ™å’Œæ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œé¦–å…ˆç³»ç»Ÿæ¯”è¾ƒäº†ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå‘çŽ°ä¿æŒé¢„è®­ç»ƒARæƒé‡åˆ†å¸ƒå¯¹æœ‰æ•ˆè½¬æ¢è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºŽå—çŠ¶æ³¨æ„åŠ›æ¨¡å¼çš„æŒç»­é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨å—é—´ä¿æŒå› æžœæ€§ï¼ŒåŒæ—¶åœ¨å—å†…å®žçŽ°åŒå‘å»ºæ¨¡ã€‚æˆ‘ä»¬å‘çŽ°è¿™ç§æ–¹æ³•æ¯”å®Œå…¨åŒå‘å»ºæ¨¡èƒ½æ›´å¥½åœ°ä¿ç•™é¢„è®­ç»ƒARæ¨¡åž‹çš„æƒé‡åˆ†å¸ƒï¼Œå¹¶å…·æœ‰æ”¯æŒKVç¼“å­˜çš„ä¼˜åŠ¿ï¼Œå®žçŽ°äº†å‡†ç¡®æ€§å’Œæ•ˆçŽ‡çš„åŒèµ¢ã€‚å…¶æ¬¡ï¼Œä¸ºç¼“è§£è®­ç»ƒä¸Žæµ‹è¯•æ—¶æŽ©ç æ ‡è®°åˆ†å¸ƒï¼ˆå‡åŒ€åˆ†å¸ƒä¸Žé«˜åº¦ä»Žå·¦åˆ°å³åˆ†å¸ƒï¼‰ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½ç½®ç›¸å…³çš„æ ‡è®°æŽ©ç ç­–ç•¥ï¼Œåœ¨è®­ç»ƒæ—¶å¯¹åŽç»­æ ‡è®°èµ‹äºˆæ›´é«˜çš„æŽ©ç æ¦‚çŽ‡ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿæµ‹è¯•æ—¶çš„è¡Œä¸ºã€‚åŸºäºŽæ­¤æ¡†æž¶ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†dLMsçš„æ³¨æ„åŠ›æ¨¡å¼ã€è®­ç»ƒåŠ¨æ€å’Œå…¶ä»–è®¾è®¡é€‰æ‹©ï¼Œä¸ºå¯æ‰©å±•çš„AR-to-dLMè½¬æ¢æä¾›äº†å®žç”¨è§è§£ã€‚è¿™äº›ç ”ç©¶å‚¬ç”Ÿäº†Efficient-DLMç³»åˆ—æ¨¡åž‹ï¼Œå…¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„ARæ¨¡åž‹å’ŒdLMsã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„Efficient-DLM 8Bæ¨¡åž‹åœ¨å‡†ç¡®çŽ‡ä¸Šæ¯”Dream 7Bå’ŒQwen3 4Båˆ†åˆ«é«˜å‡º+5.4%å’Œ+2.7%ï¼ŒåŒæ—¶åžåé‡åˆ†åˆ«æå‡äº†4.5å€å’Œ2.7å€ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•å°†é¢„è®­ç»ƒçš„è‡ªå›žå½’ï¼ˆARï¼‰è¯­è¨€æ¨¡åž‹é«˜æ•ˆè½¬æ¢ä¸ºæ‰©æ•£è¯­è¨€æ¨¡åž‹ï¼ˆdLMï¼‰ï¼Œä»¥åœ¨ä¿æŒARæ¨¡åž‹ä»»åŠ¡å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œåˆ©ç”¨dLMçš„å¹¶è¡Œç”Ÿæˆä¼˜åŠ¿æå‡é€Ÿåº¦ã€‚çŽ°æœ‰AR-to-dLMæ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽå…¶æ³¨æ„åŠ›æ¨¡å¼å’Œè®­ç»ƒç›®æ ‡è®¾è®¡ä¸å½“ï¼Œå¯¼è‡´è½¬æ¢åŽæ¨¡åž‹æƒé‡åˆ†å¸ƒåç¦»é¢„è®­ç»ƒARæ¨¡åž‹ï¼Œä¸”è®­ç»ƒä¸Žæµ‹è¯•æ—¶çš„æŽ©ç åˆ†å¸ƒä¸åŒ¹é…ï¼Œå½±å“æ¨¡åž‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›æ¨¡å¼å’ŒæŽ©ç ç­–ç•¥ï¼Œå®žçŽ°æ›´æœ‰æ•ˆçš„AR-to-dLMè½¬æ¢ã€‚å…·ä½“åŒ…æ‹¬ï¼š1ï¼‰é‡‡ç”¨å—çŠ¶æ³¨æ„åŠ›æ¨¡å¼ï¼Œåœ¨å—å†…è¿›è¡ŒåŒå‘å»ºæ¨¡ä»¥æ•æ‰ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶åœ¨å—é—´ä¿æŒå› æžœæ€§ä»¥ä¿ç•™é¢„è®­ç»ƒæƒé‡åˆ†å¸ƒï¼›2ï¼‰å¼•å…¥ä½ç½®ç›¸å…³çš„æŽ©ç ç­–ç•¥ï¼Œä½¿è®­ç»ƒæ—¶çš„æŽ©ç åˆ†å¸ƒæ›´æŽ¥è¿‘æµ‹è¯•æ—¶çš„ä»Žå·¦åˆ°å³æ¨¡å¼ï¼Œå‡å°‘è®­ç»ƒ-æµ‹è¯•å·®è·ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼ŒåŸºäºŽé¢„è®­ç»ƒçš„ARæ¨¡åž‹ï¼Œé€šè¿‡æŒç»­é¢„è®­ç»ƒè¿›è¡ŒAR-to-dLMè½¬æ¢ï¼Œä½¿ç”¨å—çŠ¶æ³¨æ„åŠ›æ¨¡å¼å’Œä½ç½®ç›¸å…³æŽ©ç ç­–ç•¥ä¼˜åŒ–æ¨¡åž‹ï¼›å…¶æ¬¡ï¼Œåœ¨è½¬æ¢åŽçš„dLMä¸Šè¿›è¡ŒæŽ¨ç†ï¼Œåˆ©ç”¨å…¶å¹¶è¡Œç”Ÿæˆèƒ½åŠ›å®žçŽ°é«˜é€Ÿæ–‡æœ¬ç”Ÿæˆã€‚æ¡†æž¶è¿˜åŒ…æ‹¬å¯¹æ³¨æ„åŠ›æ¨¡å¼ã€è®­ç»ƒåŠ¨æ€çš„ç³»ç»Ÿåˆ†æžï¼Œä»¥æŒ‡å¯¼è®¾è®¡é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯å—çŠ¶æ³¨æ„åŠ›æ¨¡å¼å’Œä½ç½®ç›¸å…³æŽ©ç ç­–ç•¥çš„ç»“åˆã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå—çŠ¶æ³¨æ„åŠ›æ¨¡å¼åœ¨ä¿ç•™é¢„è®­ç»ƒæƒé‡åˆ†å¸ƒå’Œæ”¯æŒKVç¼“å­˜æ–¹é¢æ›´å…·ä¼˜åŠ¿ï¼Œè€Œä½ç½®ç›¸å…³æŽ©ç ç­–ç•¥ç›´æŽ¥é’ˆå¯¹è®­ç»ƒ-æµ‹è¯•åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œæå‡äº†æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰å—çŠ¶æ³¨æ„åŠ›æ¨¡å¼ï¼šå°†è¾“å…¥åºåˆ—åˆ’åˆ†ä¸ºå—ï¼Œå—å†…ä½¿ç”¨åŒå‘æ³¨æ„åŠ›ï¼Œå—é—´ä½¿ç”¨å› æžœæ³¨æ„åŠ›ï¼Œå…·ä½“å—å¤§å°æ ¹æ®å®žéªŒä¼˜åŒ–ï¼›2ï¼‰ä½ç½®ç›¸å…³æŽ©ç ç­–ç•¥ï¼šåœ¨è®­ç»ƒæ—¶ï¼Œå¯¹åºåˆ—ä¸­é åŽçš„æ ‡è®°èµ‹äºˆæ›´é«˜çš„æŽ©ç æ¦‚çŽ‡ï¼Œæ¨¡æ‹Ÿæµ‹è¯•æ—¶ä»Žå·¦åˆ°å³çš„ç”Ÿæˆè¿‡ç¨‹ï¼›3ï¼‰æŸå¤±å‡½æ•°ï¼šåŸºäºŽæ‰©æ•£æ¨¡åž‹çš„åŽ»å™ªç›®æ ‡ï¼Œç»“åˆæŽ©ç é¢„æµ‹ä»»åŠ¡ï¼Œå…·ä½“å‚æ•°å¦‚å­¦ä¹ çŽ‡ã€æ‰¹å¤§å°é€šè¿‡ç½‘æ ¼æœç´¢ç¡®å®šï¼›4ï¼‰ç½‘ç»œç»“æž„ï¼šæ²¿ç”¨é¢„è®­ç»ƒARæ¨¡åž‹çš„Transformeræž¶æž„ï¼Œä½†è°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ä»¥é€‚åº”dLMéœ€æ±‚ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒEfficient-DLMç³»åˆ—æ¨¡åž‹åœ¨æ€§èƒ½å’Œæ•ˆçŽ‡ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ã€‚ä»¥Efficient-DLM 8Bä¸ºä¾‹ï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢ï¼Œç›¸æ¯”Dream 7Bå’ŒQwen3 4Bï¼Œåˆ†åˆ«æé«˜äº†+5.4%å’Œ+2.7%ï¼›åœ¨åžåé‡æ–¹é¢ï¼Œåˆ†åˆ«è¾¾åˆ°4.5å€å’Œ2.7å€çš„æå‡ã€‚è¿™äº›ç»“æžœåŸºäºŽæ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨AR-to-dLMè½¬æ¢ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåŽç»­ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›åŸºçº¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜é€Ÿæ–‡æœ¬ç”Ÿæˆçš„åœºæ™¯ä¸­ï¼Œå¦‚å®žæ—¶å¯¹è¯ç³»ç»Ÿã€å†…å®¹åˆ›ä½œè¾…åŠ©ã€ä»£ç ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ã€‚Efficient-DLMæ¡†æž¶é€šè¿‡æå‡æ‰©æ•£è¯­è¨€æ¨¡åž‹çš„æ•ˆçŽ‡ï¼Œä¸ºå®žé™…éƒ¨ç½²æä¾›äº†æ›´å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½æŽ¨åŠ¨å¹¶è¡Œç”Ÿæˆæ¨¡åž‹åœ¨è¾¹ç¼˜è®¾å¤‡å’Œå¤§è§„æ¨¡æœåŠ¡ä¸­çš„æ™®åŠï¼Œé™ä½Žè®¡ç®—æˆæœ¬å¹¶æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.

