---
layout: default
title: Scaling and Transferability of Annealing Strategies in Large Language Model Training
---

# Scaling and Transferability of Annealing Strategies in Large Language Model Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13705" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13705</a>
  <a href="https://arxiv.org/pdf/2512.13705.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13705" onclick="toggleFavorite(this, '2512.13705', 'Scaling and Transferability of Annealing Strategies in Large Language Model Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siqi Wang, Zhengyu Chen, Teng Xiao, Zheqi Lv, Jinluan Yang, Xunliang Cai, Jingang Wang, Xiaomeng Li

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¯è¿ç§»çš„å­¦ä¹ ç‡é€€ç«ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å­¦ä¹ ç‡è°ƒåº¦` `é€€ç«ç­–ç•¥` `æ¨¡å‹è¿ç§»` `è¶…å‚æ•°ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ï¼Œå­¦ä¹ ç‡é€€ç«ç­–ç•¥çš„ä¼˜åŒ–é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹æœ‰æ•ˆçš„è·¨æ¨¡å‹é…ç½®è¿ç§»æ–¹æ³•ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ”¹è¿›çš„é¢„æµ‹æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè®­ç»ƒæ­¥æ•°ã€æœ€å¤§å­¦ä¹ ç‡å’Œé€€ç«è¡Œä¸ºï¼Œä¼˜åŒ–Warmup-Steady-Decayè°ƒåº¦å™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæœ€ä¼˜é€€ç«æ¯”ç‡å…·æœ‰ä¸€è‡´æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒè®­ç»ƒé…ç½®é—´è¿ç§»ï¼Œå°æ¨¡å‹å¯ä½œä¸ºå¤§æ¨¡å‹è®­ç»ƒåŠ¨æ€ä¼˜åŒ–çš„ä»£ç†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­¦ä¹ ç‡è°ƒåº¦å¯¹äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†ç†è§£ä¸åŒæ¨¡å‹é…ç½®ä¸‹çš„æœ€ä¼˜é€€ç«ç­–ç•¥ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­é€€ç«åŠ¨æ€çš„å¯è¿ç§»æ€§ï¼Œå¹¶æ”¹è¿›äº†ä¸€ä¸ªå¹¿ä¹‰é¢„æµ‹æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–Warmup-Steady-Decay (WSD)è°ƒåº¦å™¨ä¸‹çš„é€€ç«ç­–ç•¥ã€‚æ”¹è¿›åçš„æ¡†æ¶ç»“åˆäº†è®­ç»ƒæ­¥æ•°ã€æœ€å¤§å­¦ä¹ ç‡å’Œé€€ç«è¡Œä¸ºï¼Œä»è€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å­¦ä¹ ç‡è®¡åˆ’ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºé€‰æ‹©æœ€ä¼˜é€€ç«ç­–ç•¥æä¾›äº†å®ç”¨çš„æŒ‡å¯¼ï¼Œæ— éœ€è¯¦å°½çš„è¶…å‚æ•°æœç´¢ï¼Œè¯æ˜äº†è¾ƒå°çš„æ¨¡å‹å¯ä»¥ä½œä¸ºä¼˜åŒ–è¾ƒå¤§æ¨¡å‹è®­ç»ƒåŠ¨æ€çš„å¯é ä»£ç†ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¨ å¯†æ¨¡å‹å’Œæ··åˆä¸“å®¶(MoE)æ¨¡å‹è¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ï¼Œè¡¨æ˜æœ€ä¼˜é€€ç«æ¯”ç‡éµå¾ªä¸€è‡´çš„æ¨¡å¼ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„è®­ç»ƒé…ç½®ä¹‹é—´è½¬ç§»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­å­¦ä¹ ç‡é€€ç«ç­–ç•¥ä¼˜åŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è¶…å‚æ•°æœç´¢ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”éš¾ä»¥åœ¨ä¸åŒæ¨¡å‹é…ç½®ä¹‹é—´è¿ç§»æœ€ä¼˜ç­–ç•¥ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆåœ°æ‰¾åˆ°é€‚ç”¨äºä¸åŒè§„æ¨¡å’Œæ¶æ„æ¨¡å‹çš„å­¦ä¹ ç‡é€€ç«ç­–ç•¥æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç ”ç©¶å­¦ä¹ ç‡é€€ç«åŠ¨æ€åœ¨ä¸åŒæ¨¡å‹é…ç½®ä¹‹é—´çš„å¯è¿ç§»æ€§ã€‚é€šè¿‡è§‚å¯Ÿå’Œåˆ†æä¸åŒæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ ç‡å˜åŒ–è§„å¾‹ï¼Œå‘ç°æœ€ä¼˜é€€ç«æ¯”ç‡å…·æœ‰ä¸€è‡´æ€§ï¼Œä»è€Œå¯ä»¥å°†å°æ¨¡å‹ä¸Šä¼˜åŒ–å¾—åˆ°çš„é€€ç«ç­–ç•¥è¿ç§»åˆ°å¤§æ¨¡å‹ä¸Šï¼Œå‡å°‘è¶…å‚æ•°æœç´¢çš„æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å¹¿ä¹‰é¢„æµ‹æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–Warmup-Steady-Decay (WSD) è°ƒåº¦å™¨ä¸‹çš„é€€ç«ç­–ç•¥ã€‚2) è¯¥æ¡†æ¶ç»“åˆäº†è®­ç»ƒæ­¥æ•°ã€æœ€å¤§å­¦ä¹ ç‡å’Œé€€ç«è¡Œä¸ºç­‰å…³é”®å› ç´ ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹æœ€ä¼˜å­¦ä¹ ç‡è®¡åˆ’ã€‚3) é€šè¿‡å¤§é‡çš„å®éªŒéªŒè¯äº†é€€ç«ç­–ç•¥çš„å¯è¿ç§»æ€§ï¼Œå¹¶è¯æ˜äº†å°æ¨¡å‹å¯ä»¥ä½œä¸ºå¤§æ¨¡å‹è®­ç»ƒåŠ¨æ€ä¼˜åŒ–çš„å¯é ä»£ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå‘ç°äº†å­¦ä¹ ç‡é€€ç«åŠ¨æ€çš„å¯è¿ç§»æ€§ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ã€‚è¿™ä½¿å¾—å¯ä»¥åœ¨å°æ¨¡å‹ä¸Šè¿›è¡Œé«˜æ•ˆçš„è¶…å‚æ•°æœç´¢ï¼Œç„¶åå°†ä¼˜åŒ–åçš„ç­–ç•¥è¿ç§»åˆ°å¤§æ¨¡å‹ä¸Šï¼Œä»è€Œå¤§å¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œæ”¹è¿›çš„é¢„æµ‹æ¡†æ¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹æœ€ä¼˜å­¦ä¹ ç‡è®¡åˆ’ï¼Œè¿›ä¸€æ­¥æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Warmup-Steady-Decay (WSD) è°ƒåº¦å™¨ä½œä¸ºå­¦ä¹ ç‡è°ƒæ•´çš„åŸºç¡€ã€‚2) æ”¹è¿›çš„é¢„æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è€ƒè™‘äº†è®­ç»ƒæ­¥æ•°ã€æœ€å¤§å­¦ä¹ ç‡å’Œé€€ç«è¡Œä¸ºç­‰å…³é”®å› ç´ ã€‚3) é€šè¿‡å¤§é‡çš„å®éªŒï¼Œæ¢ç´¢äº†ä¸åŒæ¨¡å‹é…ç½®ä¸‹çš„æœ€ä¼˜é€€ç«æ¯”ç‡ï¼Œå¹¶éªŒè¯äº†å…¶å¯è¿ç§»æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13705/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13705/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13705/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€ä¼˜é€€ç«æ¯”ç‡åœ¨ä¸åŒæ¨¡å‹é…ç½®ä¹‹é—´å…·æœ‰ä¸€è‡´æ€§ï¼Œå¹¶ä¸”å°æ¨¡å‹å¯ä»¥ä½œä¸ºå¤§æ¨¡å‹è®­ç»ƒåŠ¨æ€ä¼˜åŒ–çš„å¯é ä»£ç†ã€‚é€šè¿‡å°†å°æ¨¡å‹ä¸Šä¼˜åŒ–åçš„é€€ç«ç­–ç•¥è¿ç§»åˆ°å¤§æ¨¡å‹ä¸Šï¼Œå¯ä»¥æ˜¾è‘—é™ä½è¶…å‚æ•°æœç´¢çš„æˆæœ¬ï¼Œå¹¶æå‡è®­ç»ƒæ•ˆç‡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥åˆ©ç”¨å°æ¨¡å‹ä¼˜åŒ–åçš„é€€ç«ç­–ç•¥æ¥åŠ é€Ÿå¤§æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ï¼Œæœ‰åŠ©äºæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.

