---
layout: default
title: A First-Order Logic-Based Alternative to Reward Models in RLHF
---

# A First-Order Logic-Based Alternative to Reward Models in RLHF

**arXiv**: [2512.14100v1](https://arxiv.org/abs/2512.14100) | [PDF](https://arxiv.org/pdf/2512.14100.pdf)

**ä½œè€…**: Chunjin Jian, Xinhua Zhu

**åˆ†ç±»**: cs.LG, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ChunjinJiang/sgrpo)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé€»è¾‘ç›¸ä¼¼åº¦çš„å¥–åŠ±æœºåˆ¶S-GRPOï¼Œæå‡RLHFä¸­LLMå¯¹é½æ•ˆæžœ**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)** **3Dæ„ŸçŸ¥ä¸ŽçŠ¶æ€ä¼°è®¡ (Perception & State Est)**

**å…³é”®è¯**: `RLHF` `å¤§è¯­è¨€æ¨¡åž‹å¯¹é½` `é€»è¾‘æŽ¨ç†` `å¥–åŠ±æ¨¡åž‹` `åå¥½å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰RLHFæ–¹æ³•ä¾èµ–å¥–åŠ±æ¨¡åž‹å¼•å¯¼LLMå¯¹é½ï¼Œä½†å¥–åŠ±æ¨¡åž‹çš„è´¨é‡å’Œç¨³å®šæ€§æ˜¯ç“¶é¢ˆã€‚
2. æå‡ºåŸºäºŽé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§å¼•å¯¼æ¨¡åž‹å¯¹é½äººç±»åå¥½ã€‚
3. å¼•å…¥S-GRPOï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ ç»„ä»¶é˜²æ­¢æ¨¡åž‹å´©æºƒï¼Œå®žéªŒè¡¨æ˜Žå…¶ä¼˜äºŽSFTï¼Œå¹¶æ‰©å±•äº†çŽ°æœ‰æ¡†æž¶ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºŽé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œä½œä¸ºå¼ºåŒ–å­¦ä¹ ä»Žäººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­ä¼ ç»Ÿå¥–åŠ±æ¨¡åž‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºŽå¯å‘å¼å¥–åŠ±ä¼°è®¡ï¼Œè€Œæ˜¯åˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ¥å¼•å¯¼æ¨¡åž‹ä¸Žäººç±»åå¥½å¯¹é½ã€‚è€ƒè™‘åˆ°çŽ°å®žä¸–ç•Œçš„é—®é¢˜å¯ä»¥ä»Žå¤šä¸ªè§’åº¦è§£é‡Šï¼Œä¸ºé˜²æ­¢åŸºäºŽé€»è¾‘çš„å¼ºåŒ–å­¦ä¹ å¯¼è‡´æ¨¡åž‹å´©æºƒï¼Œå¼•å…¥äº†S-GRPOï¼Œä¸€ç§GRPOæ¡†æž¶çš„ç›‘ç£å˜ä½“ã€‚S-GRPOåŒ…å«ä¸€ä¸ªé¢å¤–çš„ç›‘ç£ç»„ä»¶ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–å’ŒåŸºäºŽæ ‡ç­¾çš„ç›®æ ‡ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å§‹ç»ˆä¼˜äºŽæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¹¶æ‰©å±•äº†çŽ°æœ‰çš„åå¥½å­¦ä¹ æ¡†æž¶ï¼Œå¦‚GRPOå’ŒDPOï¼Œä¸ºå¯¹é½è®­ç»ƒæä¾›äº†ä¸€ç§æ›´çµæ´»å’Œä»»åŠ¡è‡ªé€‚åº”çš„æ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰RLHFæ–¹æ³•ä¸¥é‡ä¾èµ–å¥–åŠ±æ¨¡åž‹ï¼Œè€Œå¥–åŠ±æ¨¡åž‹çš„è®­ç»ƒå’Œæ³›åŒ–èƒ½åŠ›ç›´æŽ¥å½±å“æœ€ç»ˆçš„å¯¹é½æ•ˆæžœã€‚ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡åž‹é€šå¸¸åŸºäºŽå¯å‘å¼æ–¹æ³•è¿›è¡Œå¥–åŠ±ä¼°è®¡ï¼Œè¿™å¯èƒ½å¯¼è‡´å¥–åŠ±ä¿¡å·ä¸å‡†ç¡®æˆ–ä¸ç¨³å®šï¼Œä»Žè€Œå½±å“LLMçš„å­¦ä¹ æ•ˆæžœã€‚æ­¤å¤–ï¼ŒçŽ°å®žä¸–ç•Œçš„é—®é¢˜å¾€å¾€å…·æœ‰å¤šé¢æ€§ï¼Œå•ä¸€çš„å¥–åŠ±ä¿¡å·å¯èƒ½å¯¼è‡´æ¨¡åž‹é™·å…¥å±€éƒ¨æœ€ä¼˜æˆ–äº§ç”Ÿåå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å½¢å¼é€»è¾‘çš„ä¸€è‡´æ€§æ¥æ›¿ä»£ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡åž‹ã€‚é€šè¿‡å°†äººç±»åå¥½è½¬åŒ–ä¸ºé€»è¾‘è§„åˆ™ï¼Œå¹¶è®¡ç®—æ¨¡åž‹ç”Ÿæˆç»“æžœä¸Žè¿™äº›è§„åˆ™çš„ç›¸ä¼¼åº¦ï¼Œä»Žè€Œå¾—åˆ°å¥–åŠ±ä¿¡å·ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¥–åŠ±æ¨¡åž‹çš„ä¾èµ–ï¼Œå¹¶èƒ½å¤Ÿæ›´ç›´æŽ¥åœ°åæ˜ äººç±»çš„åå¥½ã€‚ä¸ºäº†é˜²æ­¢æ¨¡åž‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å´©æºƒï¼Œå¼•å…¥äº†ç›‘ç£å­¦ä¹ ç»„ä»¶ï¼Œä»¥ç¡®ä¿æ¨¡åž‹ç”Ÿæˆçš„ç»“æžœåœ¨é€»è¾‘ä¸Šæ˜¯åˆç†çš„ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šS-GRPOæ¡†æž¶åœ¨GRPOçš„åŸºç¡€ä¸Šå¢žåŠ äº†ä¸€ä¸ªç›‘ç£å­¦ä¹ åˆ†æ”¯ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œæ¨¡åž‹ç”Ÿæˆä¸€ä¸ªå€™é€‰ç­”æ¡ˆã€‚ç„¶åŽï¼Œè®¡ç®—è¯¥ç­”æ¡ˆä¸Žé¢„å®šä¹‰çš„é€»è¾‘è§„åˆ™çš„ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°é€»è¾‘å¥–åŠ±ã€‚åŒæ—¶ï¼Œä½¿ç”¨ç›‘ç£å­¦ä¹ åˆ†æ”¯ï¼Œæ ¹æ®äººå·¥æ ‡æ³¨çš„æ ‡ç­¾è®¡ç®—ç›‘ç£æŸå¤±ã€‚æœ€åŽï¼Œå°†é€»è¾‘å¥–åŠ±å’Œç›‘ç£æŸå¤±ç»“åˆèµ·æ¥ï¼Œä½œä¸ºæ¨¡åž‹çš„æœ€ç»ˆä¼˜åŒ–ç›®æ ‡ã€‚æ¨¡åž‹é€šè¿‡è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–å’ŒåŸºäºŽæ ‡ç­¾çš„ç›®æ ‡ï¼Œå®žçŽ°ä¸Žäººç±»åå¥½çš„å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽä½¿ç”¨é€»è¾‘ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡åž‹ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´ç›´æŽ¥åœ°åæ˜ äººç±»çš„åå¥½ï¼Œå¹¶é¿å…äº†å¯¹å¥–åŠ±æ¨¡åž‹çš„ä¾èµ–ã€‚æ­¤å¤–ï¼ŒS-GRPOæ¡†æž¶é€šè¿‡å¼•å…¥ç›‘ç£å­¦ä¹ ç»„ä»¶ï¼Œæœ‰æ•ˆåœ°é˜²æ­¢äº†æ¨¡åž‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å´©æºƒï¼Œæé«˜äº†æ¨¡åž‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šS-GRPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1)é€»è¾‘ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼šå…·ä½“å¦‚ä½•å°†äººç±»åå¥½è½¬åŒ–ä¸ºé€»è¾‘è§„åˆ™ï¼Œä»¥åŠå¦‚ä½•è®¡ç®—æ¨¡åž‹ç”Ÿæˆç»“æžœä¸Žè¿™äº›è§„åˆ™çš„ç›¸ä¼¼åº¦ï¼ˆå…·ä½“è®¡ç®—å…¬å¼æœªçŸ¥ï¼‰ã€‚2)ç›‘ç£å­¦ä¹ åˆ†æ”¯çš„æŸå¤±å‡½æ•°ï¼šå¦‚ä½•æ ¹æ®äººå·¥æ ‡æ³¨çš„æ ‡ç­¾è®¡ç®—ç›‘ç£æŸå¤±ï¼ˆå…·ä½“æŸå¤±å‡½æ•°æœªçŸ¥ï¼‰ã€‚3)é€»è¾‘å¥–åŠ±å’Œç›‘ç£æŸå¤±çš„æƒé‡ï¼šå¦‚ä½•å¹³è¡¡é€»è¾‘å¥–åŠ±å’Œç›‘ç£æŸå¤±åœ¨æœ€ç»ˆä¼˜åŒ–ç›®æ ‡ä¸­çš„ä½œç”¨ï¼ˆå…·ä½“æƒé‡è®¾ç½®æœªçŸ¥ï¼‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å§‹ç»ˆä¼˜äºŽæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒS-GRPOèƒ½å¤Ÿæ‰©å±•çŽ°æœ‰çš„åå¥½å­¦ä¹ æ¡†æž¶ï¼Œå¦‚GRPOå’ŒDPOï¼Œä¸ºå¯¹é½è®­ç»ƒæä¾›äº†ä¸€ç§æ›´çµæ´»å’Œä»»åŠ¡è‡ªé€‚åº”çš„æ–¹æ³•ã€‚ä»£ç å·²å¼€æºï¼Œæ–¹ä¾¿å¤çŽ°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦ä¸Žäººç±»åå¥½å¯¹é½çš„å¤§è¯­è¨€æ¨¡åž‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ã€ä»£ç ç”Ÿæˆç­‰ã€‚é€šè¿‡ä½¿ç”¨åŸºäºŽé€»è¾‘ç›¸ä¼¼åº¦çš„å¥–åŠ±æœºåˆ¶ï¼Œå¯ä»¥æé«˜æ¨¡åž‹çš„å®‰å…¨æ€§ã€å¯é æ€§å’Œå¯æŽ§æ€§ï¼Œä½¿å…¶æ›´å¥½åœ°æœåŠ¡äºŽäººç±»ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»åž‹çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¸ºè§£å†³å¤æ‚çš„å¯¹é½é—®é¢˜æä¾›æ–°çš„æ€è·¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
>   In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
>   Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

