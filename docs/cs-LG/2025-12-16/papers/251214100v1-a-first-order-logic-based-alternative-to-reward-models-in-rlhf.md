---
layout: default
title: A First-Order Logic-Based Alternative to Reward Models in RLHF
---

# A First-Order Logic-Based Alternative to Reward Models in RLHF

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14100" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14100v1</a>
  <a href="https://arxiv.org/pdf/2512.14100.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14100v1" onclick="toggleFavorite(this, '2512.14100v1', 'A First-Order Logic-Based Alternative to Reward Models in RLHF')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunjin Jian, Xinhua Zhu

**åˆ†ç±»**: cs.LG, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ChunjinJiang/sgrpo)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé€»è¾‘ç›¸ä¼¼åº¦çš„å¥–åŠ±æœºåˆ¶S-GRPOï¼Œæå‡RLHFä¸­LLMå¯¹é½çš„æ€§èƒ½ä¸é²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººç±»åé¦ˆ` `å¤§è¯­è¨€æ¨¡å‹` `é€»è¾‘æ¨ç†` `æ¨¡å‹å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RLHFæ–¹æ³•ä¾èµ–å¥–åŠ±æ¨¡å‹å¼•å¯¼LLMå¯¹é½ï¼Œä½†å¥–åŠ±æ¨¡å‹çš„è´¨é‡å’Œç¨³å®šæ€§æ˜¯å…³é”®æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºåŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§å¼•å¯¼æ¨¡å‹å¯¹é½äººç±»åå¥½ã€‚
3. S-GRPOé€šè¿‡å¼•å…¥ç›‘ç£ç»„ä»¶ï¼Œè”åˆä¼˜åŒ–ç”Ÿæˆã€KLæ•£åº¦å’Œæ ‡ç­¾ç›®æ ‡ï¼Œæå‡æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€»è¾‘ç›¸ä¼¼æ€§çš„å¥–åŠ±æœºåˆ¶ï¼Œä½œä¸ºå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºå¯å‘å¼å¥–åŠ±ä¼°è®¡ï¼Œè€Œæ˜¯åˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ¥å¼•å¯¼æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚è€ƒè™‘åˆ°ç°å®ä¸–ç•Œçš„é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦è§£é‡Šï¼Œä¸ºäº†é¿å…åŸºäºé€»è¾‘çš„å¼ºåŒ–å­¦ä¹ å¯¼è‡´æ¨¡å‹å´©æºƒï¼Œå¼•å…¥äº†S-GRPOï¼Œä¸€ç§GRPOæ¡†æ¶çš„ç›‘ç£å˜ä½“ã€‚S-GRPOç»“åˆäº†é¢å¤–çš„ç›‘ç£ç»„ä»¶ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´è”åˆä¼˜åŒ–ç”Ÿæˆé¡¹ã€KLæ•£åº¦æ­£åˆ™åŒ–å’ŒåŸºäºæ ‡ç­¾çš„ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¹¶æ‰©å±•äº†ç°æœ‰çš„åå¥½å­¦ä¹ æ¡†æ¶ï¼Œå¦‚GRPOå’ŒDPOï¼Œä¸ºå¯¹é½è®­ç»ƒæä¾›äº†ä¸€ç§æ›´çµæ´»å’Œä»»åŠ¡è‡ªé€‚åº”çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰RLHFæ–¹æ³•ä¸¥é‡ä¾èµ–å¥–åŠ±æ¨¡å‹ï¼Œè€Œå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒè´¨é‡ç›´æ¥å½±å“æœ€ç»ˆçš„å¯¹é½æ•ˆæœã€‚ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ä¾èµ–äºå¯å‘å¼å¥–åŠ±ä¼°è®¡ï¼Œå¯èƒ½å­˜åœ¨åå·®ï¼Œå¹¶ä¸”éš¾ä»¥ä¿è¯æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç”±äºçœŸå®ä¸–ç•Œé—®é¢˜çš„å¤æ‚æ€§ï¼Œå•ä¸€çš„å¥–åŠ±ä¿¡å·å¯èƒ½å¯¼è‡´æ¨¡å‹å´©æºƒï¼Œæ— æ³•æ•æ‰äººç±»åå¥½çš„å¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å½¢å¼é€»è¾‘ä¸€è‡´æ€§æ¥æ›¿ä»£ä¼ ç»Ÿçš„å¯å‘å¼å¥–åŠ±ä¼°è®¡ã€‚é€šè¿‡å°†äººç±»åå¥½è½¬åŒ–ä¸ºé€»è¾‘è§„åˆ™ï¼Œå¹¶è®¡ç®—æ¨¡å‹ç”Ÿæˆç»“æœä¸è¿™äº›è§„åˆ™çš„ç›¸ä¼¼åº¦ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å­¦ä¹ ç¬¦åˆäººç±»ä»·å€¼è§‚çš„è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¥–åŠ±æ¨¡å‹çš„ä¾èµ–ï¼Œå¹¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººç±»åå¥½çš„æœ¬è´¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS-GRPOæ¡†æ¶åœ¨GRPOçš„åŸºç¡€ä¸Šå¼•å…¥äº†ç›‘ç£å­¦ä¹ ç»„ä»¶ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç›‘ç£æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼›2) ä½¿ç”¨é€»è¾‘ç›¸ä¼¼åº¦è®¡ç®—å¥–åŠ±ä¿¡å·ï¼›3) ä½¿ç”¨GRPOæ¡†æ¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ŒåŒæ—¶ç»“åˆç›‘ç£å­¦ä¹ ç›®æ ‡è¿›è¡Œè”åˆä¼˜åŒ–ã€‚GRPOæ¡†æ¶åŒ…å«ç”Ÿæˆæ¨¡å‹ã€KLæ•£åº¦æ­£åˆ™åŒ–é¡¹å’Œå¥–åŠ±å‡½æ•°ã€‚S-GRPOçš„å…³é”®åœ¨äºå°†é€»è¾‘ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œå¹¶å¼•å…¥ç›‘ç£å­¦ä¹ ç›®æ ‡ä»¥é˜²æ­¢æ¨¡å‹å´©æºƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨é€»è¾‘ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´ç›´æ¥åœ°åæ˜ äººç±»åå¥½ï¼Œå¹¶é¿å…äº†å¥–åŠ±æ¨¡å‹å¸¦æ¥çš„åå·®ã€‚æ­¤å¤–ï¼ŒS-GRPOé€šè¿‡å¼•å…¥ç›‘ç£å­¦ä¹ ç›®æ ‡ï¼Œè§£å†³äº†åŸºäºé€»è¾‘çš„å¼ºåŒ–å­¦ä¹ å¯èƒ½å¯¼è‡´çš„æ¨¡å‹å´©æºƒé—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šS-GRPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€»è¾‘ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼šå…·ä½“å¦‚ä½•å°†äººç±»åå¥½è½¬åŒ–ä¸ºé€»è¾‘è§„åˆ™ï¼Œä»¥åŠå¦‚ä½•è®¡ç®—æ¨¡å‹ç”Ÿæˆç»“æœä¸è¿™äº›è§„åˆ™çš„ç›¸ä¼¼åº¦ï¼ˆè®ºæ–‡ä¸­æœªæ˜ç¡®è¯´æ˜ï¼Œå±äºæœªçŸ¥ç»†èŠ‚ï¼‰ï¼›2) ç›‘ç£å­¦ä¹ ç›®æ ‡çš„å…·ä½“å½¢å¼ï¼šè®ºæ–‡ä¸­æåˆ°ä½¿ç”¨äº†åŸºäºæ ‡ç­¾çš„ç›®æ ‡ï¼Œä½†æœªæ˜ç¡®è¯´æ˜å…·ä½“å½¢å¼ï¼ˆä¾‹å¦‚äº¤å‰ç†µæŸå¤±ç­‰ï¼‰ï¼›3) å„ä¸ªæŸå¤±é¡¹çš„æƒé‡ï¼šå¦‚ä½•å¹³è¡¡ç”ŸæˆæŸå¤±ã€KLæ•£åº¦æŸå¤±å’Œç›‘ç£å­¦ä¹ æŸå¤±ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„è®­ç»ƒæ•ˆæœï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒS-GRPOåœ¨æ€§èƒ½å’Œé²æ£’æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚è™½ç„¶è®ºæ–‡ä¸­æ²¡æœ‰ç»™å‡ºå…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦ï¼Œä½†å¼ºè°ƒäº†S-GRPOåœ¨å„ç§ä»»åŠ¡ä¸Šçš„ä¸€è‡´æ€§è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºå…¶æ‰©å±•äº†ç°æœ‰çš„åå¥½å­¦ä¹ æ¡†æ¶ï¼Œå¦‚GRPOå’ŒDPOï¼Œä¸ºå¯¹é½è®­ç»ƒæä¾›äº†ä¸€ç§æ›´çµæ´»å’Œä»»åŠ¡è‡ªé€‚åº”çš„æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€æ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡ä½¿ç”¨åŸºäºé€»è¾‘ç›¸ä¼¼åº¦çš„å¥–åŠ±æœºåˆ¶ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å®‰å…¨æ€§ã€å¯é æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ï¼Œå¹¶å‡å°‘æ¨¡å‹äº§ç”Ÿæœ‰å®³æˆ–ä¸å½“å†…å®¹çš„é£é™©ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œé¢†åŸŸï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´äººæ€§åŒ–çš„AIç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
>   In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
>   Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

