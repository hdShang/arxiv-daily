---
layout: default
title: Differentially Private Knowledge Distillation via Synthetic Text Generation
---

# Differentially Private Knowledge Distillation via Synthetic Text Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2403.00932" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2403.00932</a>
  <a href="https://arxiv.org/pdf/2403.00932.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2403.00932" onclick="toggleFavorite(this, '2403.00932', 'Differentially Private Knowledge Distillation via Synthetic Text Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: James Flemings, Murali Annavaram

**åˆ†ç±»**: cs.LG, cs.CL, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDistilDPä»¥è§£å†³å·®åˆ†éšç§ä¸çŸ¥è¯†è’¸é¦çš„å¹³è¡¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å·®åˆ†éšç§` `çŸ¥è¯†è’¸é¦` `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆæˆæ•°æ®` `æ¨¡å‹å‹ç¼©` `éšç§ä¿æŠ¤` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å®ç°å·®åˆ†éšç§å’Œæ¨¡å‹å‹ç¼©æ—¶ï¼Œå¾€å¾€é¢ä¸´æ•ˆç”¨æŸå¤±çš„æƒè¡¡ï¼Œä¸”åŒæ—¶åº”ç”¨ä¸¤è€…å¯èƒ½å¯¼è‡´æ›´å¤§çš„æ•ˆç”¨ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºDistilDPç®—æ³•ï¼Œé€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆå’Œæ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒï¼Œè¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œä»è€Œæœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDistilDPåœ¨Big Patentæ•°æ®é›†ä¸Šç›¸è¾ƒäºç°æœ‰åŸºçº¿è‡³å°‘æå‡äº†9.0 PPLï¼Œå±•ç°å‡ºå¼ºå¤§çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ•°æ®éšç§çš„æ—¥ç›Šç´§è¿«æ€§è¦æ±‚åœ¨ç§æœ‰æ•°æ®ä¸Šä½¿ç”¨å·®åˆ†éšç§ï¼ˆDPï¼‰è¿›è¡Œè®­ç»ƒã€‚åŒæ—¶ï¼ŒLLMså‚æ•°è§„æ¨¡çš„æŒ‡æ•°å¢é•¿ä¹Ÿéœ€è¦åœ¨èµ„æºå—é™æˆ–å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨ä¸­è¿›è¡Œæ¨¡å‹å‹ç¼©ã€‚å·®åˆ†éšç§å’Œæ¨¡å‹å‹ç¼©é€šå¸¸éœ€è¦åœ¨æ•ˆç”¨æŸå¤±ä¸Šè¿›è¡Œæƒè¡¡ï¼Œä¸”åŒæ—¶åº”ç”¨ä¸¤è€…å¯èƒ½ä¼šåŠ å‰§æ•ˆç”¨ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å·®åˆ†éšç§çŸ¥è¯†è’¸é¦ç®—æ³•DistilDPï¼Œè¯¥ç®—æ³•åˆ©ç”¨ç”±å·®åˆ†éšç§æ•™å¸ˆLLMç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡ŒçŸ¥è¯†ä¼ é€’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDistilDPåœ¨Big Patentæ•°æ®é›†ä¸Šç›¸è¾ƒäºç°æœ‰åŸºçº¿æ˜¾è‘—æé«˜äº†æ•ˆç”¨ï¼Œéšç§å‚æ•°ä¸ºÎµ=2æ—¶è‡³å°‘æå‡äº†9.0 PPLã€‚è¿™äº›ç»“æœæ¨åŠ¨äº†è‡ªå›å½’LLMsçš„éšç§ä¿æŠ¤å‹ç¼©è¿›ç¨‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç§æœ‰æ•°æ®ä¸Šè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¦‚ä½•åœ¨ä¿è¯å·®åˆ†éšç§çš„åŒæ—¶è¿›è¡Œæœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©ã€‚ç°æœ‰æ–¹æ³•åœ¨åŒæ—¶åº”ç”¨å·®åˆ†éšç§å’ŒçŸ¥è¯†è’¸é¦æ—¶ï¼Œå¾€å¾€å¯¼è‡´æ•ˆç”¨æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDistilDPç®—æ³•é€šè¿‡åˆ©ç”¨å·®åˆ†éšç§æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œè¿›è¡ŒçŸ¥è¯†çš„åŒé‡ä¼ é€’ï¼šä¸€æ–¹é¢é€šè¿‡åˆæˆæ•°æ®çš„ç¡¬æ ‡ç­¾ï¼Œå¦ä¸€æ–¹é¢é€šè¿‡æ•™å¸ˆæ¨¡å‹åœ¨åˆæˆæ•°æ®ä¸Šçš„è¾“å‡ºåˆ†å¸ƒï¼ˆè½¯æ ‡ç­¾ï¼‰ï¼Œä»è€Œæå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDistilDPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ•™å¸ˆæ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®å’Œå­¦ç”Ÿæ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚æ•™å¸ˆæ¨¡å‹é¦–å…ˆç”Ÿæˆåˆæˆæ•°æ®ï¼Œç„¶åå­¦ç”Ÿæ¨¡å‹é€šè¿‡å­¦ä¹ ç¡¬æ ‡ç­¾å’Œè½¯æ ‡ç­¾æ¥è¿›è¡ŒçŸ¥è¯†çš„å¸æ”¶å’Œè½¬ç§»ã€‚

**å…³é”®åˆ›æ–°**ï¼šDistilDPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆäº†å·®åˆ†éšç§å’ŒçŸ¥è¯†è’¸é¦çš„æ€æƒ³ï¼Œé€šè¿‡åˆæˆæ•°æ®çš„ç”Ÿæˆå’Œæ ‡ç­¾çš„åŒé‡åˆ©ç”¨ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ•ˆç”¨ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„æ¶æ„ç›¸ä¼¼ï¼Œä»¥ä¾¿äºå¯¹éšè—è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚æ­¤å¤–ï¼Œéšç§å‚æ•°Îµçš„è®¾ç½®ä¸º2ï¼Œç¡®ä¿äº†åœ¨ä¿è¯éšç§çš„å‰æä¸‹ï¼Œæ¨¡å‹çš„æ€§èƒ½å¾—ä»¥æå‡ã€‚å®éªŒä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æœ€å¤§åŒ–çŸ¥è¯†çš„è½¬ç§»æ•ˆæœã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2403.00932/figures/dpkd_overview.png" alt="fig_0" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDistilDPåœ¨Big Patentæ•°æ®é›†ä¸Šç›¸è¾ƒäºç°æœ‰åŸºçº¿è‡³å°‘æå‡äº†9.0 PPLï¼Œå±•ç°å‡ºå¼ºå¤§çš„éšç§ä¿æŠ¤èƒ½åŠ›ï¼Œéšç§å‚æ•°è®¾ç½®ä¸ºÎµ=2ï¼Œè¯æ˜äº†å…¶åœ¨éšç§ä¿æŠ¤ä¸‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚åŒ»ç–—è®°å½•åˆ†æã€é‡‘èæ•°æ®å¤„ç†ç­‰ã€‚é€šè¿‡åœ¨ä¿è¯éšç§çš„åŒæ—¶æå‡æ¨¡å‹æ€§èƒ½ï¼ŒDistilDPä¸ºå®é™…åº”ç”¨æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy puts pressure on practitioners to train LLMs with Differential Privacy (DP) on private data. Concurrently, the exponential growth in parameter size of LLMs necessitates model compression before deployment of LLMs on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, simultaneously applying both schemes can compound the utility degradation. To this end, we propose DistilDP: a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private teacher LLM. The knowledge of a teacher LLM is transferred onto the student in two ways: one way from the synthetic data itself -- the hard labels, and the other way by the output distribution of the teacher evaluated on the synthetic data -- the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by aligning the hidden representations between both. Our experimental results demonstrate that DistilDP can substantially improve the utility over existing baselines, at least $9.0$ PPL on the Big Patent dataset, with strong privacy parameters, $\epsilon=2$. These promising results progress privacy-preserving compression of autoregressive LLMs. Our code can be accessed here:this https URL.

