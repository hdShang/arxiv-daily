---
layout: default
title: Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training
---

# Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13770" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13770</a>
  <a href="https://arxiv.org/pdf/2512.13770.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13770" onclick="toggleFavorite(this, '2512.13770', 'Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huaiyuan Xiao, Fadi Dornaika, Jingjun Bi

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMV-SupGCNï¼Œé€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ å’Œè‡ªè®­ç»ƒå¢å¼ºåŠç›‘ç£å¤šè§†å›¾å›¾å·ç§¯ç½‘ç»œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šè§†å›¾å­¦ä¹ ` `å›¾å·ç§¯ç½‘ç»œ` `åŠç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `è‡ªè®­ç»ƒ` `ä¼ªæ ‡ç­¾` `å›¾ç¥ç»ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºGCNçš„å¤šè§†å›¾å­¦ä¹ æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è§†å›¾é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´ç‰¹å¾è¡¨ç¤ºæ¬¡ä¼˜å’Œæ€§èƒ½å—é™ã€‚
2. MV-SupGCNé€šè¿‡ç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ ã€å¤šå›¾æ„å»ºå’Œè‡ªè®­ç»ƒï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œå¤šè§†å›¾è¯­ä¹‰å¯¹é½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMV-SupGCNåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä½³æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMV-SupGCNçš„åŠç›‘ç£å›¾å·ç§¯ç½‘ç»œæ¨¡å‹ï¼Œæ—¨åœ¨æ•´åˆäº’è¡¥ç»„ä»¶ï¼Œæœ‰æ•ˆå»ºæ¨¡å¤æ‚çš„å¤šè§†å›¾æ•°æ®ã€‚è¯¥æ¨¡å‹ç»“åˆäº¤å‰ç†µæŸå¤±å’Œç›‘ç£å¯¹æ¯”æŸå¤±çš„è”åˆæŸå¤±å‡½æ•°ï¼Œä»¥æœ€å°åŒ–ç±»å†…æ–¹å·®å¹¶æœ€å¤§åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„ç±»é—´å¯åˆ†æ€§ï¼Œä»è€Œæ›´å¥½åœ°æ•è·åˆ¤åˆ«æ€§ç‰¹å¾å¹¶æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†åŸºäºKNNå’ŒåŠç›‘ç£çš„å›¾æ„å»ºæ–¹æ³•ï¼Œå¢å¼ºäº†æ•°æ®ç»“æ„è¡¨ç¤ºçš„é²æ£’æ€§ï¼Œå¹¶å‡å°‘äº†æ³›åŒ–è¯¯å·®ã€‚æœ€åï¼Œä¸ºäº†æœ‰æ•ˆåˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾æ•°æ®å¹¶å¢å¼ºå¤šè§†å›¾ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œè¯¥æ¨¡å‹æ•´åˆäº†å¯¹æ¯”å­¦ä¹ ï¼ˆç”¨äºå¼ºåˆ¶å¤šè§†å›¾åµŒå…¥ä¹‹é—´çš„ä¸€è‡´æ€§å¹¶æ•è·æœ‰æ„ä¹‰çš„è§†å›¾é—´å…³ç³»ï¼‰å’Œä¼ªæ ‡ç­¾ï¼ˆä¸ºäº¤å‰ç†µå’Œå¯¹æ¯”æŸå¤±å‡½æ•°æä¾›é¢å¤–çš„ç›‘ç£ï¼Œä»¥å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMV-SupGCNåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒéªŒè¯äº†è¯¥é›†æˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åŸºäºå›¾å·ç§¯ç½‘ç»œçš„å¤šè§†å›¾å­¦ä¹ æ–¹æ³•ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨ä¸åŒè§†å›¾ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„ç‰¹å¾è¡¨ç¤ºä¸å¤Ÿä¼˜ç§€ï¼Œæ¨¡å‹æ€§èƒ½å—åˆ°é™åˆ¶ã€‚å°¤å…¶æ˜¯åœ¨åŠç›‘ç£å­¦ä¹ åœºæ™¯ä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾æ•°æ®æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ ã€å¤šå›¾æ„å»ºå’Œè‡ªè®­ç»ƒï¼Œæ¥å¢å¼ºæ¨¡å‹å¯¹å¤šè§†å›¾æ•°æ®çš„ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç›‘ç£å¯¹æ¯”å­¦ä¹ æ—¨åœ¨æ‹‰è¿‘åŒç±»æ ·æœ¬çš„è·ç¦»ï¼Œæ¨è¿œä¸åŒç±»æ ·æœ¬çš„è·ç¦»ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å…·åˆ¤åˆ«æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚å¤šå›¾æ„å»ºæ—¨åœ¨æé«˜å›¾ç»“æ„çš„é²æ£’æ€§ã€‚è‡ªè®­ç»ƒåˆ™åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMV-SupGCNçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨å›¾å·ç§¯ç½‘ç»œä»æ¯ä¸ªè§†å›¾ä¸­æå–ç‰¹å¾ã€‚2) å›¾æ„å»ºæ¨¡å—ï¼šç»“åˆKNNå›¾å’ŒåŠç›‘ç£å›¾æ„å»ºæ–¹æ³•ï¼Œä¸ºæ¯ä¸ªè§†å›¾æ„å»ºå›¾ç»“æ„ã€‚3) ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼šä½¿ç”¨ç›‘ç£å¯¹æ¯”æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚4) è‡ªè®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨ä¼ªæ ‡ç­¾æ¥ä¸ºæ— æ ‡ç­¾æ•°æ®æä¾›é¢å¤–çš„ç›‘ç£ä¿¡æ¯ã€‚5) å¤šè§†å›¾èåˆæ¨¡å—ï¼šå°†ä¸åŒè§†å›¾çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ç›‘ç£å¯¹æ¯”å­¦ä¹ ã€å¤šå›¾æ„å»ºå’Œè‡ªè®­ç»ƒé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®ï¼Œå¹¶å¢å¼ºäº†å¤šè§†å›¾ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMV-SupGCNèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…·åˆ¤åˆ«æ€§å’Œé²æ£’æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼ŒMV-SupGCNä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å’Œç›‘ç£å¯¹æ¯”æŸå¤±çš„åŠ æƒå’Œã€‚ç›‘ç£å¯¹æ¯”æŸå¤±çš„æ¸©åº¦å‚æ•°Ï„æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´ã€‚åœ¨å›¾æ„å»ºæ–¹é¢ï¼ŒKNNå›¾çš„è¿‘é‚»æ•°é‡kå’ŒåŠç›‘ç£å›¾çš„å‚æ•°Î±ä¹Ÿéœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚ä¼ªæ ‡ç­¾çš„ç½®ä¿¡åº¦é˜ˆå€¼ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼Œç”¨äºæ§åˆ¶ä¼ªæ ‡ç­¾çš„è´¨é‡ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13770/MVSupGCNv5.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13770/vennpseudoceloss.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13770/vennpseudocesuploss.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMV-SupGCNåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚åœ¨CiteSeeræ•°æ®é›†ä¸Šï¼ŒMV-SupGCNçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†2%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒéªŒè¯äº†ç›‘ç£å¯¹æ¯”å­¦ä¹ ã€å¤šå›¾æ„å»ºå’Œè‡ªè®­ç»ƒç­‰å„ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMV-SupGCNæ˜¯ä¸€ç§æœ‰æ•ˆä¸”é€šç”¨çš„å¤šè§†å›¾å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§å¤šè§†å›¾æ•°æ®åˆ†æä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ã€ç¤¾äº¤ç½‘ç»œåˆ†æå’Œç”Ÿç‰©ä¿¡æ¯å­¦ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨å¤šè§†å›¾æ•°æ®ä¸­çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œä»è€Œä¸ºå®é™…åº”ç”¨å¸¦æ¥æ›´å¤§çš„ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤„ç†æ›´å¤æ‚çš„å¤šè§†å›¾æ•°æ®ï¼Œä¾‹å¦‚å…·æœ‰ç¼ºå¤±è§†å›¾æˆ–å™ªå£°è§†å›¾çš„æ•°æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available atthis https URL

