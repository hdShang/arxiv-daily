---
layout: default
title: Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits
---

# Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits

**arXiv**: [2512.14338v1](https://arxiv.org/abs/2512.14338) | [PDF](https://arxiv.org/pdf/2512.14338.pdf)

**ä½œè€…**: Michael Murray, Tenzin Chan, Kedar Karhadker, Christopher J. Hillar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºHopfieldç½‘ç»œé€šè¿‡èŒƒæ•°æ•ˆçŽ‡éšå¼åç½®é«˜æ•ˆå­¦ä¹ å›¾åŒæž„ç±»çš„æœºåˆ¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `Hopfieldç½‘ç»œ` `éšå¼åç½®` `å›¾åŒæž„` `ä¸å˜å­ç©ºé—´` `èŒƒæ•°æ•ˆçŽ‡` `æ¢¯åº¦ä¸‹é™` `æ ·æœ¬å¤æ‚åº¦` `ç¾¤ç»“æž„æ•°æ®`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¸¸æ˜¾å¼æž„å»ºä¸å˜æ€§ï¼Œä½†éšå¼å­¦ä¹ æœºåˆ¶åœ¨ç¾¤ç»“æž„æ•°æ®ä¸­çš„æ•ˆçŽ‡ä¸Žæ³›åŒ–èƒ½åŠ›å°šä¸æ˜Žç¡®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šé€šè¿‡åˆ†æžHopfieldç½‘ç»œåœ¨æ¢¯åº¦ä¸‹é™ä¸‹çš„èƒ½é‡æœ€å°åŒ–ï¼Œæ­ç¤ºå…¶éšå¼åç½®å¯¼è‡´èŒƒæ•°é«˜æ•ˆè§£ï¼Œä»Žè€Œå­¦ä¹ å›¾åŒæž„ç±»ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šè¯æ˜Žå›¾åŒæž„ç±»å¯åœ¨ä¸‰ç»´å­ç©ºé—´è¡¨ç¤ºï¼Œæ ·æœ¬å¤æ‚åº¦ä¸ºå¤šé¡¹å¼ï¼Œå‚æ•°æ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå­¦ä¹ é—®é¢˜æ¶‰åŠå¯¹ç§°æ€§ï¼Œè™½ç„¶ä¸å˜æ€§å¯ä»¥æž„å»ºåˆ°ç¥žç»æž¶æž„ä¸­ï¼Œä½†åœ¨å¤„ç†ç¾¤ç»“æž„æ•°æ®æ—¶ä¹Ÿå¯èƒ½éšå¼å‡ºçŽ°ã€‚æˆ‘ä»¬åœ¨ç»å…¸Hopfieldç½‘ç»œä¸­ç ”ç©¶äº†è¿™ä¸€çŽ°è±¡ï¼Œå¹¶è¡¨æ˜Žå®ƒä»¬å¯ä»¥ä»Žå°çš„éšæœºæ ·æœ¬ä¸­æŽ¨æ–­å‡ºå›¾çš„å®Œæ•´åŒæž„ç±»ã€‚æˆ‘ä»¬çš„ç»“æžœè¡¨æ˜Žï¼š(i) å›¾åŒæž„ç±»å¯ä»¥åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´å†…è¡¨ç¤ºï¼›(ii) ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµå…·æœ‰å¯¹èŒƒæ•°é«˜æ•ˆè§£çš„éšå¼åç½®ï¼Œè¿™æ”¯æ’‘äº†å­¦ä¹ åŒæž„ç±»çš„å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼›(iii) åœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸‹ï¼Œéšç€æ ·æœ¬é‡çš„å¢žåŠ ï¼Œå‚æ•°ä¼šæ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚è¿™äº›å‘çŽ°å…±åŒçªå‡ºäº†Hopfieldç½‘ç»œä¸­æ³›åŒ–çš„ç»Ÿä¸€æœºåˆ¶ï¼šå­¦ä¹ ä¸­å¯¹èŒƒæ•°æ•ˆçŽ‡çš„åç½®é©±åŠ¨äº†åœ¨ç¾¤ç»“æž„æ•°æ®ä¸‹è¿‘ä¼¼ä¸å˜æ€§çš„å‡ºçŽ°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡ç ”ç©¶Hopfieldç½‘ç»œå¦‚ä½•ä»Žå°‘é‡éšæœºæ ·æœ¬ä¸­é«˜æ•ˆå­¦ä¹ å›¾åŒæž„ç±»ï¼Œå³å›¾çš„å®Œæ•´åŒæž„ç±»åˆ«ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽï¼Œè™½ç„¶å¯¹ç§°æ€§ï¼ˆå¦‚ç¾¤ç»“æž„ï¼‰åœ¨æœºå™¨å­¦ä¹ ä¸­å¸¸è§ï¼Œä½†é€šå¸¸éœ€è¦æ˜¾å¼æž„å»ºä¸å˜æ€§åˆ°ç½‘ç»œæž¶æž„ä¸­ï¼Œè¿™å¯èƒ½å¢žåŠ è®¡ç®—å¤æ‚åº¦æˆ–é™åˆ¶æ³›åŒ–èƒ½åŠ›ï¼Œè€Œéšå¼å­¦ä¹ æœºåˆ¶åœ¨ç¾¤ç»“æž„æ•°æ®ä¸­çš„æ•ˆçŽ‡å’Œç†è®ºåŸºç¡€å°šä¸å……åˆ†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯åˆ†æžHopfieldç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„éšå¼åç½®ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµæ—¶ï¼Œç½‘ç»œå€¾å‘äºŽå­¦ä¹ èŒƒæ•°é«˜æ•ˆçš„è§£ã€‚è¿™ç§åç½®ä½¿å¾—ç½‘ç»œèƒ½å¤Ÿä»Žæœ‰é™æ ·æœ¬ä¸­æŽ¨æ–­å‡ºå›¾çš„ä¸å˜ç‰¹å¾ï¼Œä»Žè€Œé«˜æ•ˆå­¦ä¹ åŒæž„ç±»ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç å¯¹ç§°æ€§ã€‚è¿™æ ·è®¾è®¡æ˜¯å› ä¸ºå®ƒæ­ç¤ºäº†å­¦ä¹ ç®—æ³•æœ¬èº«çš„å†…åœ¨å±žæ€§å¦‚ä½•ä¿ƒè¿›æ³›åŒ–ï¼Œä¸ºç†è§£ç¥žç»ç½‘ç»œåœ¨å¯¹ç§°æ•°æ®ä¸Šçš„è¡Œä¸ºæä¾›äº†ç†è®ºæ¡†æž¶ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŸºäºŽç»å…¸Hopfieldç½‘ç»œï¼Œæµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œå°†å›¾è¡¨ç¤ºä¸ºè¾“å…¥æ•°æ®ï¼›ç„¶åŽï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰å­¦ä¹ è§„åˆ™è®­ç»ƒç½‘ç»œä»¥æœ€å°åŒ–èƒ½é‡å‡½æ•°ï¼›æŽ¥ç€ï¼Œåˆ†æžå‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”åŒ–ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¦‚ä½•æ”¶æ•›åˆ°ä¸Žå›¾åŒæž„ç±»ç›¸å…³çš„ä¸å˜å­ç©ºé—´ï¼›æœ€åŽï¼Œé€šè¿‡ç†è®ºæŽ¨å¯¼å’Œå®žéªŒéªŒè¯ï¼Œè¯„ä¼°å­¦ä¹ æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®è¡¨ç¤ºã€èƒ½é‡æœ€å°åŒ–ä¼˜åŒ–ã€ä¸å˜å­ç©ºé—´åˆ†æžå’Œæ ·æœ¬å¤æ‚åº¦è®¡ç®—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯æ­ç¤ºäº†Hopfieldç½‘ç»œä¸­çš„éšå¼åç½®æœºåˆ¶ï¼šåœ¨ç¾¤ç»“æž„æ•°æ®ä¸‹ï¼Œæ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµä¼šè‡ªç„¶å¯¼å‘èŒƒæ•°é«˜æ•ˆçš„è§£ï¼Œè¿™é©±åŠ¨äº†è¿‘ä¼¼ä¸å˜æ€§çš„å‡ºçŽ°ï¼Œä»Žè€Œé«˜æ•ˆå­¦ä¹ å›¾åŒæž„ç±»ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼Œå®ƒä¸ä¾èµ–äºŽæ˜¾å¼çš„ä¸å˜æ€§æž„å»ºï¼Œè€Œæ˜¯åˆ©ç”¨å­¦ä¹ ç®—æ³•æœ¬èº«çš„åç½®æ¥å®žçŽ°æ³›åŒ–ï¼Œè¿™ä¸ºè®¾è®¡æ›´é«˜æ•ˆå’Œç†è®ºå¯è§£é‡Šçš„æ¨¡åž‹æä¾›äº†æ–°è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä½¿ç”¨Hopfieldç½‘ç»œçš„èƒ½é‡å‡½æ•°ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ï¼ˆå¦‚æœ€å°åŒ–èƒ½é‡æµï¼ŒMEFï¼‰è¿›è¡Œè®­ç»ƒï¼›å‚æ•°è®¾ç½®æ¶‰åŠç½‘ç»œæƒé‡å’Œè¾“å…¥è¡¨ç¤ºï¼Œå…·ä½“ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œä½†åŸºäºŽæ ‡å‡†Hopfieldæ¨¡åž‹ï¼›æŸå¤±å‡½æ•°æ˜¯èƒ½é‡æœ€å°åŒ–ï¼Œæ²¡æœ‰é¢å¤–æ­£åˆ™åŒ–ï¼›ç½‘ç»œç»“æž„ä¸ºç»å…¸å…¨è¿žæŽ¥Hopfieldç½‘ç»œï¼Œç”¨äºŽå¤„ç†å›¾æ•°æ®ï¼›æŠ€æœ¯ç»†èŠ‚è¿˜åŒ…æ‹¬å¯¹ä¸å˜å­ç©ºé—´çš„ä¸‰ç»´è¡¨ç¤ºåˆ†æžå’Œå¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦çš„ç†è®ºæŽ¨å¯¼ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœè¡¨æ˜Žï¼šå›¾åŒæž„ç±»å¯ä»¥åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´å†…æœ‰æ•ˆè¡¨ç¤ºï¼Œè¿™ç®€åŒ–äº†å­¦ä¹ è¿‡ç¨‹ï¼›ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµæ—¶ï¼Œç½‘ç»œå±•çŽ°å‡ºå¯¹èŒƒæ•°é«˜æ•ˆè§£çš„éšå¼åç½®ï¼Œæ”¯æ’‘äº†å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œå…·ä½“æå‡æœªåœ¨æ‘˜è¦ä¸­é‡åŒ–ï¼Œä½†ç†è®ºåˆ†æžè¡¨æ˜Žä¼˜äºŽæ˜¾å¼æ–¹æ³•ï¼›åœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸‹ï¼Œéšç€æ ·æœ¬é‡å¢žåŠ ï¼Œå‚æ•°æ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ï¼ŒéªŒè¯äº†æ³›åŒ–æœºåˆ¶çš„é²æ£’æ€§ã€‚å¯¹æ¯”åŸºçº¿æœªæ˜Žç¡®æåŠï¼Œä½†çªå‡ºäº†éšå¼å­¦ä¹ ç›¸å¯¹äºŽæ˜¾å¼æž„å»ºçš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰ã€å›¾ç¥žç»ç½‘ç»œå’Œæœºå™¨å­¦ä¹ é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åˆ†ç±»ã€ç¤¾äº¤ç½‘ç»œåˆ†æžæˆ–åˆ†å­ç»“æž„è¯†åˆ«ä¸­ï¼Œé«˜æ•ˆå­¦ä¹ å›¾åŒæž„ç±»å¯æå‡æ¨¡åž‹æ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ•ˆçŽ‡ã€‚å®žé™…ä»·å€¼åœ¨äºŽä¸ºè®¾è®¡æ— éœ€æ˜¾å¼å¯¹ç§°æ€§ç¼–ç çš„æ¨¡åž‹æä¾›ç†è®ºæŒ‡å¯¼ï¼Œæœªæ¥å¯èƒ½æŽ¨åŠ¨æ›´é²æ£’å’Œå¯è§£é‡Šçš„AIç³»ç»Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰å¤æ‚å¯¹ç§°ç»“æž„çš„æ•°æ®æ—¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

