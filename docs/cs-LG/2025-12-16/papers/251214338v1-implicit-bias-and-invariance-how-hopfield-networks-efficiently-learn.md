---
layout: default
title: Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits
---

# Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits

**arXiv**: [2512.14338v1](https://arxiv.org/abs/2512.14338) | [PDF](https://arxiv.org/pdf/2512.14338.pdf)

**ä½œè€…**: Michael Murray, Tenzin Chan, Kedar Karhadker, Christopher J. Hillar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºHopfieldç½‘ç»œé€šè¿‡èŒƒæ•°æ•ˆçŽ‡åç½®é«˜æ•ˆå­¦ä¹ å›¾åŒæž„ç±»çš„éšå¼æœºåˆ¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `Hopfieldç½‘ç»œ` `å›¾åŒæž„ç±»` `éšå¼åç½®` `ä¸å˜å­ç©ºé—´` `èŒƒæ•°æ•ˆçŽ‡` `ç¾¤ç»“æž„æ•°æ®` `æ ·æœ¬å¤æ‚åº¦` `èƒ½é‡æœ€å°åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨å¤„ç†å›¾å¯¹ç§°æ€§æ—¶é€šå¸¸æ˜¾å¼æž„å»ºä¸å˜æ€§ï¼Œä½†éšå¼å­¦ä¹ æœºåˆ¶å°šä¸æ˜Žç¡®ï¼Œéš¾ä»¥é«˜æ•ˆåˆ©ç”¨ç¾¤ç»“æž„æ•°æ®ã€‚
2. è®ºæ–‡æå‡ºHopfieldç½‘ç»œé€šè¿‡æœ€å°åŒ–èƒ½é‡æµçš„æ¢¯åº¦ä¸‹é™ï¼Œéšå¼åç½®èŒƒæ•°é«˜æ•ˆè§£ï¼Œä»Žè€Œåœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´è¡¨ç¤ºå›¾åŒæž„ç±»ã€‚
3. å®žéªŒè¡¨æ˜Žè¯¥æ–¹æ³•èƒ½ä»¥å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦å­¦ä¹ åŒæž„ç±»ï¼Œå‚æ•°æ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ï¼ŒéªŒè¯äº†éšå¼åç½®é©±åŠ¨è¿‘ä¼¼ä¸å˜æ€§çš„æœºåˆ¶ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå­¦ä¹ é—®é¢˜æ¶‰åŠå¯¹ç§°æ€§ï¼Œè™½ç„¶ä¸å˜æ€§å¯ä»¥æž„å»ºåˆ°ç¥žç»æž¶æž„ä¸­ï¼Œä½†åœ¨å¤„ç†ç¾¤ç»“æž„æ•°æ®æ—¶ä¹Ÿå¯èƒ½éšå¼å‡ºçŽ°ã€‚æˆ‘ä»¬åœ¨ç»å…¸Hopfieldç½‘ç»œä¸­ç ”ç©¶è¿™ä¸€çŽ°è±¡ï¼Œå¹¶è¯æ˜Žå®ƒä»¬èƒ½å¤Ÿä»Žå°çš„éšæœºæ ·æœ¬ä¸­æŽ¨æ–­å‡ºå›¾çš„å®Œæ•´åŒæž„ç±»ã€‚æˆ‘ä»¬çš„ç»“æžœè¡¨æ˜Žï¼š(i) å›¾åŒæž„ç±»å¯ä»¥åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´å†…è¡¨ç¤ºï¼›(ii) ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰å…·æœ‰å¯¹èŒƒæ•°é«˜æ•ˆè§£çš„éšå¼åç½®ï¼Œè¿™æ”¯æ’‘äº†å­¦ä¹ åŒæž„ç±»çš„å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼›(iii) åœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸‹ï¼Œéšç€æ ·æœ¬é‡çš„å¢žåŠ ï¼Œå‚æ•°ä¼šæ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚è¿™äº›å‘çŽ°å…±åŒçªå‡ºäº†Hopfieldç½‘ç»œä¸­æ³›åŒ–çš„ç»Ÿä¸€æœºåˆ¶ï¼šå­¦ä¹ ä¸­å¯¹èŒƒæ•°æ•ˆçŽ‡çš„åç½®é©±åŠ¨äº†åœ¨ç¾¤ç»“æž„æ•°æ®ä¸‹è¿‘ä¼¼ä¸å˜æ€§çš„å‡ºçŽ°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•é«˜æ•ˆå­¦ä¹ å›¾åŒæž„ç±»çš„é—®é¢˜ï¼Œå³ä»Žå›¾çš„éšæœºæ ·æœ¬ä¸­æŽ¨æ–­å…¶å®Œæ•´åŒæž„ç±»ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽï¼Œè™½ç„¶å¯ä»¥é€šè¿‡æ˜¾å¼æž„å»ºä¸å˜æ€§ï¼ˆå¦‚ä½¿ç”¨å›¾ç¥žç»ç½‘ç»œä¸­çš„ä¸å˜å±‚ï¼‰æ¥å¤„ç†å¯¹ç§°æ€§ï¼Œä½†è¿™å¯èƒ½å¢žåŠ è®¡ç®—å¤æ‚åº¦æˆ–é™åˆ¶æ¨¡åž‹çµæ´»æ€§ï¼Œä¸”éšå¼å­¦ä¹ æœºåˆ¶åœ¨ç¾¤ç»“æž„æ•°æ®ä¸‹çš„ä½œç”¨æœºåˆ¶ä¸æ˜Žç¡®ï¼Œå¯¼è‡´æ ·æœ¬æ•ˆçŽ‡ä½Žä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯ç ”ç©¶Hopfieldç½‘ç»œåœ¨å¤„ç†ç¾¤ç»“æž„æ•°æ®æ—¶çš„éšå¼å­¦ä¹ è¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰çš„æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ï¼Œæ­ç¤ºå…¶å¯¹èŒƒæ•°é«˜æ•ˆè§£çš„éšå¼åç½®ã€‚è¿™æ ·è®¾è®¡æ˜¯å› ä¸ºHopfieldç½‘ç»œä½œä¸ºç»å…¸æ¨¡åž‹ï¼Œå…·æœ‰ç®€å•çš„åŠ¨åŠ›å­¦å’Œèƒ½é‡å‡½æ•°ï¼Œä¾¿äºŽç†è®ºåˆ†æžï¼Œèƒ½å¤Ÿæ·±å…¥æŽ¢è®¨å¯¹ç§°æ€§å­¦ä¹ ä¸­çš„åŸºæœ¬æœºåˆ¶ï¼Œè€Œä¸å—å¤æ‚æž¶æž„çš„å¹²æ‰°ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŸºäºŽç»å…¸Hopfieldç½‘ç»œï¼Œæµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆï¼Œå°†å›¾è¡¨ç¤ºä¸ºèŠ‚ç‚¹ç‰¹å¾æˆ–é‚»æŽ¥çŸ©é˜µï¼Œä½œä¸ºè¾“å…¥æ•°æ®ï¼›ç„¶åŽï¼Œä½¿ç”¨Hopfieldç½‘ç»œé€šè¿‡èƒ½é‡æœ€å°åŒ–è¿‡ç¨‹è¿›è¡Œå­¦ä¹ ï¼Œå…¶ä¸­èƒ½é‡å‡½æ•°å®šä¹‰ä¸ºç½‘ç»œçŠ¶æ€ä¸Žæƒé‡çŸ©é˜µçš„å‡½æ•°ï¼›æŽ¥ç€ï¼Œåº”ç”¨æ¢¯åº¦ä¸‹é™ï¼ˆæˆ–å…¶ä»–å­¦ä¹ è§„åˆ™ï¼‰æ¥æœ€å°åŒ–èƒ½é‡æµï¼Œä¼˜åŒ–ç½‘ç»œå‚æ•°ï¼›æœ€åŽï¼Œåˆ†æžå‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”åŒ–ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¦‚ä½•æ”¶æ•›åˆ°è¡¨ç¤ºå›¾åŒæž„ç±»çš„ä¸å˜å­ç©ºé—´ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®è¡¨ç¤ºæ¨¡å—ã€Hopfieldç½‘ç»œæ¨¡åž‹ã€èƒ½é‡æœ€å°åŒ–ä¼˜åŒ–æ¨¡å—å’Œç†è®ºåˆ†æžæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯æ­ç¤ºäº†Hopfieldç½‘ç»œå­¦ä¹ ä¸­çš„éšå¼åç½®æœºåˆ¶ï¼šé€šè¿‡æœ€å°åŒ–èƒ½é‡æµçš„æ¢¯åº¦ä¸‹é™ï¼Œç½‘ç»œè‡ªç„¶åœ°å€¾å‘äºŽèŒƒæ•°é«˜æ•ˆçš„è§£ï¼Œè¿™é©±åŠ¨äº†å‚æ•°å‘ä¸å˜å­ç©ºé—´æ”¶æ•›ï¼Œä»Žè€Œå®žçŽ°è¿‘ä¼¼ä¸å˜æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼Œä¸ä¾èµ–æ˜¾å¼çš„ä¸å˜æ€§æž„å»ºï¼ˆå¦‚å¯¹ç§°ç¾¤æ“ä½œæˆ–ç‰¹å®šç½‘ç»œå±‚ï¼‰ï¼Œè€Œæ˜¯åˆ©ç”¨ä¼˜åŒ–è¿‡ç¨‹çš„éšå¼ç‰¹æ€§ï¼Œåœ¨ç®€å•æ¨¡åž‹ä¸­è‡ªå‘æ¶ŒçŽ°å¯¹ç§°æ€§å­¦ä¹ èƒ½åŠ›ï¼Œè¿™ä¸ºç†è§£æ›´å¤æ‚ç¥žç»ç½‘ç»œä¸­çš„æ³›åŒ–æä¾›äº†æ–°è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ä½¿ç”¨ç»å…¸Hopfieldç½‘ç»œçš„è¿žç»­æˆ–ç¦»æ•£ç‰ˆæœ¬ï¼Œèƒ½é‡å‡½æ•°åŸºäºŽèµ«å¸ƒå­¦ä¹ è§„åˆ™æˆ–ç±»ä¼¼å½¢å¼ï¼›æŸå¤±å‡½æ•°é—´æŽ¥é€šè¿‡èƒ½é‡æœ€å°åŒ–ä½“çŽ°ï¼Œå…·ä½“ä¸ºæœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰ï¼Œè¿™æ¶‰åŠæ¢¯åº¦ä¸‹é™æ›´æ–°æƒé‡ï¼›ç½‘ç»œç»“æž„ç®€å•ï¼Œä¸»è¦ä¸ºå…¨è¿žæŽ¥æƒé‡çŸ©é˜µï¼Œè¾“å…¥ç»´åº¦å¯¹åº”å›¾è¡¨ç¤ºï¼›å‚æ•°è®¾ç½®ä¸­ï¼Œæ ·æœ¬å¤æ‚åº¦åˆ†æžåŸºäºŽç†è®ºæŽ¨å¯¼ï¼Œå¾—å‡ºå¤šé¡¹å¼ç•Œé™ï¼Œå…·ä½“æ•°å€¼æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†å¼ºè°ƒä¸Žç¾¤ç»“æž„ç›¸å…³ï¼›å­¦ä¹ è§„åˆ™å¯èƒ½åŒ…æ‹¬æ ‡å‡†æ¢¯åº¦ä¸‹é™æˆ–å…¶ä»–å˜ä½“ï¼Œä»¥éªŒè¯ä¸åŒè§„åˆ™ä¸‹å‚æ•°æ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´çš„æ™®éæ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒHopfieldç½‘ç»œèƒ½å¤Ÿä»Žå°çš„éšæœºæ ·æœ¬ä¸­æŽ¨æ–­å›¾åŒæž„ç±»ï¼Œå…·ä½“æ€§èƒ½åŒ…æ‹¬ï¼šå›¾åŒæž„ç±»å¯åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´å†…è¡¨ç¤ºï¼›ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰æ—¶ï¼Œéšå¼åç½®å¯¼è‡´èŒƒæ•°é«˜æ•ˆè§£ï¼Œæ”¯æ’‘äº†å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼ˆå…·ä½“æ•°å€¼æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†å¼ºè°ƒä¸ºå¤šé¡¹å¼çº§ï¼‰ï¼›åœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸‹ï¼Œå‚æ•°éšæ ·æœ¬é‡å¢žåŠ æ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ï¼ŒéªŒè¯äº†éšå¼åç½®é©±åŠ¨è¿‘ä¼¼ä¸å˜æ€§çš„æœºåˆ¶ï¼Œç›¸æ¯”æ˜¾å¼æž„å»ºä¸å˜æ€§çš„æ–¹æ³•ï¼Œå¯èƒ½åœ¨æ ·æœ¬æ•ˆçŽ‡ä¸Šæœ‰æ‰€æå‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰ã€å›¾å­¦ä¹ å’Œæœºå™¨äººé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œä¾‹å¦‚ç”¨äºŽå›¾åƒè¯†åˆ«ä¸­çš„å¯¹ç§°æ€§å¤„ç†ã€ç¤¾äº¤ç½‘ç»œæˆ–åˆ†å­å›¾çš„ç»“æž„åˆ†æžï¼Œä»¥åŠæœºå™¨äººæ„ŸçŸ¥ä¸­çš„ç‰©ä½“è¯†åˆ«ã€‚å®žé™…ä»·å€¼åœ¨äºŽæä¾›äº†ä¸€ç§é«˜æ•ˆå­¦ä¹ å¯¹ç§°æ€§çš„éšå¼æœºåˆ¶ï¼Œå¯é™ä½Žæ¨¡åž‹å¤æ‚åº¦å’Œæ ·æœ¬éœ€æ±‚ï¼Œæœªæ¥å¯èƒ½å¯å‘æ›´é²æ£’çš„ç¥žç»ç½‘ç»œè®¾è®¡ï¼ŒæŽ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨ç¾¤ç»“æž„æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›æå‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

