---
layout: default
title: Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits
---

# Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits

**arXiv**: [2512.14338v1](https://arxiv.org/abs/2512.14338) | [PDF](https://arxiv.org/pdf/2512.14338.pdf)

**ä½œè€…**: Michael Murray, Tenzin Chan, Kedar Karhadker, Christopher J. Hillar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Hopfieldç½‘ç»œé€šè¿‡éšå¼åç½®é«˜æ•ˆå­¦ä¹ å›¾åŒæž„ç±»**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **åŠ¨ä½œç”Ÿæˆä¸Žç‰©ç†åŠ¨ç”» (Animation & Physics)**

**å…³é”®è¯**: `Hopfieldç½‘ç»œ` `å›¾åŒæž„` `éšå¼åç½®` `ä¸å˜æ€§` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å¯¹ç§°æ€§çš„å­¦ä¹ é—®é¢˜æ—¶ï¼Œé€šå¸¸éœ€è¦æ‰‹åŠ¨è®¾è®¡ä¸å˜æ€§ï¼Œå¢žåŠ äº†å¤æ‚æ€§ã€‚
2. è¯¥è®ºæ–‡è¡¨æ˜ŽHopfieldç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šéšå¼åœ°å­¦ä¹ åˆ°å›¾çš„åŒæž„ç±»ï¼Œæ— éœ€æ˜¾å¼è®¾è®¡ä¸å˜æ€§ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒHopfieldç½‘ç»œèƒ½å¤Ÿä»Žå°æ ·æœ¬ä¸­å­¦ä¹ å›¾çš„åŒæž„ç±»ï¼Œå¹¶æ­ç¤ºäº†å…¶æ³›åŒ–èƒ½åŠ›çš„å†…åœ¨æœºåˆ¶ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå­¦ä¹ é—®é¢˜æ¶‰åŠå¯¹ç§°æ€§ã€‚ä¸å˜æ€§æ—¢å¯ä»¥æž„å»ºåˆ°ç¥žç»ç½‘ç»œæž¶æž„ä¸­ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå…·æœ‰ç¾¤ç»“æž„çš„æ•°æ®æ—¶éšå¼åœ°å‡ºçŽ°ã€‚æœ¬æ–‡ç ”ç©¶äº†ç»å…¸Hopfieldç½‘ç»œä¸­çš„è¿™ç§çŽ°è±¡ï¼Œå¹¶è¡¨æ˜Žå®ƒä»¬å¯ä»¥ä»Žå°è§„æ¨¡éšæœºæ ·æœ¬ä¸­æŽ¨æ–­å‡ºå›¾çš„å®Œæ•´åŒæž„ç±»ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼šï¼ˆiï¼‰å›¾åŒæž„ç±»å¯ä»¥åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´å†…è¡¨ç¤ºï¼›ï¼ˆiiï¼‰ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰å…·æœ‰å¯¹èŒƒæ•°é«˜æ•ˆè§£çš„éšå¼åç½®ï¼Œè¿™ä¸ºå­¦ä¹ åŒæž„ç±»çš„å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ç•Œé™æä¾›äº†åŸºç¡€ï¼›ï¼ˆiiiï¼‰åœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸­ï¼Œå‚æ•°éšç€æ ·æœ¬é‡çš„å¢žåŠ è€Œæ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚æ€»ä¹‹ï¼Œè¿™äº›å‘çŽ°çªå‡ºäº†Hopfieldç½‘ç»œä¸­æ³›åŒ–çš„ç»Ÿä¸€æœºåˆ¶ï¼šå­¦ä¹ ä¸­å¯¹èŒƒæ•°æ•ˆçŽ‡çš„åç½®é©±åŠ¨äº†åœ¨ç¾¤ç»“æž„æ•°æ®ä¸‹è¿‘ä¼¼ä¸å˜æ€§çš„å‡ºçŽ°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•è®©ç¥žç»ç½‘ç»œé«˜æ•ˆåœ°å­¦ä¹ å…·æœ‰å¯¹ç§°æ€§çš„æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å›¾çš„åŒæž„ç±»ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦æ‰‹åŠ¨è®¾è®¡ç½‘ç»œç»“æž„æˆ–æŸå¤±å‡½æ•°æ¥ä¿è¯ä¸å˜æ€§ï¼Œè¿™å¢žåŠ äº†æ¨¡åž‹çš„å¤æ‚åº¦å’Œè®¾è®¡éš¾åº¦ã€‚è®ºæ–‡å…³æ³¨çš„æ˜¯ï¼Œæ˜¯å¦å¯ä»¥é€šè¿‡éšå¼çš„æ–¹å¼ï¼Œè®©ç¥žç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ åˆ°è¿™ç§ä¸å˜æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œé€šè¿‡ç ”ç©¶Hopfieldç½‘ç»œåœ¨å­¦ä¹ å›¾åŒæž„ç±»æ—¶çš„è¡Œä¸ºï¼Œæ­ç¤ºå…¶éšå¼åç½®å’Œä¸å˜æ€§ä¹‹é—´çš„å…³ç³»ã€‚è®ºæ–‡è®¤ä¸ºï¼ŒHopfieldç½‘ç»œåœ¨æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰æ—¶ï¼Œä¼šå€¾å‘äºŽé€‰æ‹©èŒƒæ•°é«˜æ•ˆçš„è§£ï¼Œè¿™ç§åç½®é©±åŠ¨äº†ç½‘ç»œå­¦ä¹ åˆ°å¯¹å›¾åŒæž„å˜æ¢çš„ä¸å˜æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè®ºæ–‡ä¸»è¦ç ”ç©¶äº†ç»å…¸Hopfieldç½‘ç»œåœ¨å­¦ä¹ å›¾åŒæž„ç±»æ—¶çš„è¡Œä¸ºã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡é¦–å…ˆè¯æ˜Žäº†å›¾åŒæž„ç±»å¯ä»¥åœ¨ä¸€ä¸ªä¸‰ç»´ä¸å˜å­ç©ºé—´ä¸­è¡¨ç¤ºã€‚ç„¶åŽï¼Œè®ºæ–‡åˆ†æžäº†ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå‘çŽ°å…¶å…·æœ‰å¯¹èŒƒæ•°é«˜æ•ˆè§£çš„éšå¼åç½®ã€‚æœ€åŽï¼Œè®ºæ–‡é€šè¿‡å®žéªŒéªŒè¯äº†ï¼Œåœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸‹ï¼Œç½‘ç»œå‚æ•°ä¼šéšç€æ ·æœ¬é‡çš„å¢žåŠ è€Œæ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽï¼Œæ­ç¤ºäº†Hopfieldç½‘ç»œåœ¨å­¦ä¹ å…·æœ‰ç¾¤ç»“æž„çš„æ•°æ®æ—¶ï¼Œä¼šé€šè¿‡éšå¼åç½®è‡ªåŠ¨å­¦ä¹ åˆ°ä¸å˜æ€§ã€‚è¿™ç§éšå¼å­¦ä¹ æœºåˆ¶é¿å…äº†æ‰‹åŠ¨è®¾è®¡ä¸å˜æ€§çš„å¤æ‚æ€§ï¼Œå¹¶ä¸ºç†è§£ç¥žç»ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) ä½¿ç”¨ç»å…¸Hopfieldç½‘ç»œä½œä¸ºç ”ç©¶å¯¹è±¡ï¼Œå› ä¸ºå®ƒå…·æœ‰ç®€å•çš„ç»“æž„å’Œæ˜Žç¡®çš„èƒ½é‡å‡½æ•°ï¼›(2) å®šä¹‰äº†èƒ½é‡æµï¼ˆMEFï¼‰ä½œä¸ºå­¦ä¹ çš„ç›®æ ‡å‡½æ•°ï¼Œå¹¶é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ï¼›(3) åˆ†æžäº†æ¢¯åº¦ä¸‹é™çš„éšå¼åç½®ï¼Œè¯æ˜Žå…¶å€¾å‘äºŽé€‰æ‹©èŒƒæ•°é«˜æ•ˆçš„è§£ï¼›(4) é€šè¿‡å®žéªŒéªŒè¯äº†ç½‘ç»œå‚æ•°ä¼šæ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è®ºæ–‡è¯æ˜Žäº†å›¾åŒæž„ç±»å¯ä»¥åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´ä¸­è¡¨ç¤ºï¼Œå¹¶æ­ç¤ºäº†Hopfieldç½‘ç»œé€šè¿‡æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰å­¦ä¹ å›¾åŒæž„ç±»çš„è¿‡ç¨‹å…·æœ‰å¯¹èŒƒæ•°é«˜æ•ˆè§£çš„éšå¼åç½®ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œç½‘ç»œå‚æ•°ä¼šéšç€æ ·æœ¬é‡çš„å¢žåŠ è€Œæ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ï¼ŒéªŒè¯äº†ç†è®ºåˆ†æžçš„æ­£ç¡®æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå›¾ç¥žç»ç½‘ç»œçš„è®¾è®¡ï¼Œé€šè¿‡å€Ÿé‰´Hopfieldç½‘ç»œçš„éšå¼åç½®æœºåˆ¶ï¼Œå¯ä»¥è®¾è®¡å‡ºæ›´é«˜æ•ˆã€æ›´å…·æœ‰æ³›åŒ–èƒ½åŠ›çš„å›¾ç¥žç»ç½‘ç»œæ¨¡åž‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¯¹äºŽç†è§£ç¥žç»ç½‘ç»œåœ¨å¤„ç†å…·æœ‰å¯¹ç§°æ€§æ•°æ®æ—¶çš„è¡Œä¸ºï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨éšå¼åç½®æ¥æé«˜æ¨¡åž‹çš„æ€§èƒ½å…·æœ‰é‡è¦çš„ç†è®ºä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

