---
layout: default
title: Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits
---

# Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits

**arXiv**: [2512.14338v1](https://arxiv.org/abs/2512.14338) | [PDF](https://arxiv.org/pdf/2512.14338.pdf)

**ä½œè€…**: Michael Murray, Tenzin Chan, Kedar Karhadker, Christopher J. Hillar

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºHopfieldç½‘ç»œé€šè¿‡èŒƒæ•°æ•ˆçŽ‡éšå¼å­¦ä¹ å›¾åŒæž„ç±»ï¼Œå®žçŽ°å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `Hopfieldç½‘ç»œ` `éšå¼åç½®` `å›¾åŒæž„` `ä¸å˜å­ç©ºé—´` `èŒƒæ•°æ•ˆçŽ‡` `æ ·æœ¬å¤æ‚åº¦` `ç¾¤ç»“æž„æ•°æ®` `èƒ½é‡æœ€å°åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰æ–¹æ³•å¸¸æ˜¾å¼æž„å»ºä¸å˜æ€§ï¼Œä½†éšå¼å­¦ä¹ æœºåˆ¶åœ¨ç¾¤ç»“æž„æ•°æ®ä¸­çš„æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›å°šä¸æ˜Žç¡®ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šåˆ©ç”¨Hopfieldç½‘ç»œï¼Œé€šè¿‡æœ€å°åŒ–èƒ½é‡æµçš„æ¢¯åº¦ä¸‹é™ï¼Œéšå¼åç½®èŒƒæ•°æ•ˆçŽ‡è§£ï¼Œå®žçŽ°å›¾åŒæž„ç±»çš„é«˜æ•ˆå­¦ä¹ ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šå®žéªŒè¡¨æ˜Žç½‘ç»œèƒ½åœ¨ä¸‰ç»´å­ç©ºé—´è¡¨ç¤ºåŒæž„ç±»ï¼Œæ ·æœ¬å¤æ‚åº¦ä¸ºå¤šé¡¹å¼çº§ï¼Œå‚æ•°æ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå­¦ä¹ é—®é¢˜æ¶‰åŠå¯¹ç§°æ€§ï¼Œè™½ç„¶ä¸å˜æ€§å¯ä»¥å†…ç½®åˆ°ç¥žç»æž¶æž„ä¸­ï¼Œä½†åœ¨ç¾¤ç»“æž„æ•°æ®ä¸Šè®­ç»ƒæ—¶ä¹Ÿå¯èƒ½éšå¼å‡ºçŽ°ã€‚æˆ‘ä»¬ç ”ç©¶äº†ç»å…¸Hopfieldç½‘ç»œä¸­çš„è¿™ä¸€çŽ°è±¡ï¼Œå¹¶è¡¨æ˜Žå®ƒä»¬å¯ä»¥ä»Žå°çš„éšæœºæ ·æœ¬ä¸­æŽ¨æ–­å‡ºå›¾çš„å®Œæ•´åŒæž„ç±»ã€‚æˆ‘ä»¬çš„ç»“æžœæ˜¾ç¤ºï¼š(i) å›¾åŒæž„ç±»å¯ä»¥åœ¨ä¸‰ç»´ä¸å˜å­ç©ºé—´ä¸­è¡¨ç¤ºï¼Œ(ii) ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰å…·æœ‰å¯¹èŒƒæ•°æ•ˆçŽ‡è§£çš„éšå¼åç½®ï¼Œè¿™æ”¯æ’‘äº†å­¦ä¹ åŒæž„ç±»çš„å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œä»¥åŠ(iii) åœ¨å¤šç§å­¦ä¹ è§„åˆ™ä¸‹ï¼Œå‚æ•°éšç€æ ·æœ¬é‡å¢žé•¿è€Œæ”¶æ•›åˆ°ä¸å˜å­ç©ºé—´ã€‚è¿™äº›å‘çŽ°å…±åŒçªå‡ºäº†Hopfieldç½‘ç»œä¸­æ³›åŒ–çš„ç»Ÿä¸€æœºåˆ¶ï¼šå­¦ä¹ ä¸­å¯¹èŒƒæ•°æ•ˆçŽ‡çš„åç½®é©±åŠ¨äº†åœ¨ç¾¤ç»“æž„æ•°æ®ä¸‹è¿‘ä¼¼ä¸å˜æ€§çš„å‡ºçŽ°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡åŸºäºŽç»å…¸Hopfieldç½‘ç»œæ¡†æž¶ï¼Œç ”ç©¶å…¶åœ¨å›¾åŒæž„ç±»å­¦ä¹ ä¸­çš„éšå¼åç½®æœºåˆ¶ã€‚æ•´ä½“æ¡†æž¶æ¶‰åŠä½¿ç”¨æ¢¯åº¦ä¸‹é™æœ€å°åŒ–èƒ½é‡æµï¼ˆMEFï¼‰ä½œä¸ºå­¦ä¹ è§„åˆ™ï¼Œä»¥ä¼˜åŒ–ç½‘ç»œå‚æ•°ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæ­ç¤ºäº†MEFå…·æœ‰å¯¹èŒƒæ•°æ•ˆçŽ‡è§£çš„éšå¼åç½®ï¼Œè¿™ä¿ƒä½¿ç½‘ç»œå‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨æ”¶æ•›åˆ°ä¸‰ç»´ä¸å˜å­ç©ºé—´ï¼Œä»Žè€Œé«˜æ•ˆè¡¨ç¤ºå›¾åŒæž„ç±»ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œä¸ä¾èµ–æ˜¾å¼çš„ä¸å˜æ€§è®¾è®¡ï¼Œè€Œæ˜¯é€šè¿‡éšå¼å­¦ä¹ æœºåˆ¶åœ¨ç¾¤ç»“æž„æ•°æ®ä¸­è‡ªç„¶æ¶ŒçŽ°è¿‘ä¼¼ä¸å˜æ€§ï¼Œç®€åŒ–äº†æž¶æž„å¹¶æå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒHopfieldç½‘ç»œèƒ½ä»Žå°‘é‡éšæœºæ ·æœ¬ä¸­å­¦ä¹ å›¾åŒæž„ç±»ï¼Œæ ·æœ¬å¤æ‚åº¦ä¸ºå¤šé¡¹å¼ç•Œé™ï¼Œå‚æ•°æ”¶æ•›åˆ°ä¸‰ç»´ä¸å˜å­ç©ºé—´ï¼ŒéªŒè¯äº†éšå¼åç½®èŒƒæ•°æ•ˆçŽ‡åœ¨é©±åŠ¨è¿‘ä¼¼ä¸å˜æ€§ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽå›¾ç»“æž„æ•°æ®åˆ†æžã€ç¤¾äº¤ç½‘ç»œå»ºæ¨¡ã€åŒ–å­¦åˆ†å­è¯†åˆ«ç­‰é¢†åŸŸï¼Œé€šè¿‡éšå¼å­¦ä¹ å¯¹ç§°æ€§ï¼Œæé«˜æ¨¡åž‹åœ¨å¤æ‚æ•°æ®ä¸­çš„æ³›åŒ–æ•ˆçŽ‡å’Œé²æ£’æ€§ï¼Œä¸ºè®¾è®¡æ›´ç®€æ´çš„ç¥žç»ç½‘ç»œæä¾›ç†è®ºæŒ‡å¯¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

