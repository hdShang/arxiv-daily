---
layout: default
title: Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting
---

# Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting

**arXiv**: [2512.14115v1](https://arxiv.org/abs/2512.14115) | [PDF](https://arxiv.org/pdf/2512.14115.pdf)

**ä½œè€…**: Ramesh Gundluru, Shubham Gupta, Sri Rama Murty K

**åˆ†ç±»**: cs.SD, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè”åˆå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œä»¥è§£å†³å£°å­¦è¯åµŒå…¥åœ¨è¯­éŸ³æ£€ç´¢ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œæå‡å£è¯­è¯æ£€æµ‹å’Œå…³é”®è¯è¯†åˆ«çš„é²æ£’æ€§ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ` `å£°å­¦è¯åµŒå…¥` `å£è¯­è¯æ£€æµ‹` `å…³é”®è¯è¯†åˆ«` `éŸ³é¢‘-æ–‡æœ¬å¯¹é½` `æ·±åº¦è¯åˆ¤åˆ«` `è¯­éŸ³æ£€ç´¢` `å…±äº«åµŒå…¥ç©ºé—´`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å£°å­¦è¯åµŒå…¥æ–¹æ³•ä¾èµ–å•æ¨¡æ€ç›‘ç£ï¼ŒéŸ³é¢‘-éŸ³é¢‘å’ŒéŸ³é¢‘-æ–‡æœ¬å¯¹é½ä¼˜åŒ–åˆ†ç¦»ï¼Œå¯¼è‡´è¯­éŸ³æ£€ç´¢ä»»åŠ¡æ•ˆçŽ‡å—é™ã€‚
2. è®ºæ–‡æå‡ºè”åˆå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œç»Ÿä¸€éŸ³é¢‘-æ–‡æœ¬å’ŒéŸ³é¢‘-éŸ³é¢‘ç›‘ç£ï¼Œåœ¨å…±äº«åµŒå…¥ç©ºé—´åŒæ—¶ä¼˜åŒ–å¯¹é½å’Œåˆ¤åˆ«ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨è¯åˆ¤åˆ«ä»»åŠ¡ä¸Šä¼˜äºŽåŸºçº¿ï¼Œå¹¶çµæ´»æ”¯æŒå£è¯­è¯æ£€æµ‹å’Œå…³é”®è¯è¯†åˆ«ï¼Œæå‡é²æ£’æ€§å’Œæ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å£°å­¦è¯åµŒå…¥ï¼ˆAWEsï¼‰æé«˜äº†è¯­éŸ³æ£€ç´¢ä»»åŠ¡ï¼ˆå¦‚å£è¯­è¯æ£€æµ‹å’Œå…³é”®è¯è¯†åˆ«ï¼‰çš„æ•ˆçŽ‡ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ç›‘ç£ã€éŸ³é¢‘-éŸ³é¢‘å’ŒéŸ³é¢‘-æ–‡æœ¬å¯¹é½çš„åˆ†ç¦»ä¼˜åŒ–ï¼Œä»¥åŠéœ€è¦ä»»åŠ¡ç‰¹å®šæ¨¡åž‹ã€‚ä¸ºè§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè”åˆå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­ç»Ÿä¸€äº†å£°å­¦å’Œè·¨æ¨¡æ€ç›‘ç£ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒæ—¶ä¼˜åŒ–ï¼šï¼ˆiï¼‰éŸ³é¢‘-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼Œå—CLAPæŸå¤±å¯å‘ï¼Œä»¥å¯¹é½éŸ³é¢‘å’Œæ–‡æœ¬è¡¨ç¤ºï¼›ï¼ˆiiï¼‰éŸ³é¢‘-éŸ³é¢‘å¯¹æ¯”å­¦ä¹ ï¼Œé€šè¿‡æ·±åº¦è¯åˆ¤åˆ«æŸå¤±ï¼Œä»¥å¢žå¼ºç±»å†…ç´§å‡‘æ€§å’Œç±»é—´åˆ†ç¦»æ€§ã€‚æ‰€ææ–¹æ³•åœ¨è¯åˆ¤åˆ«ä»»åŠ¡ä¸Šä¼˜äºŽçŽ°æœ‰AWEåŸºçº¿ï¼ŒåŒæ—¶çµæ´»æ”¯æŒå£è¯­è¯æ£€æµ‹å’Œå…³é”®è¯è¯†åˆ«ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªæ­¤ç±»ç»¼åˆæ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æ ¸å¿ƒæ–¹æ³•æ˜¯ä¸€ä¸ªè”åˆå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œæ•´ä½“æž¶æž„åŸºäºŽå…±äº«åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶æ•´åˆéŸ³é¢‘-æ–‡æœ¬å’ŒéŸ³é¢‘-éŸ³é¢‘ç›‘ç£ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šé‡‡ç”¨CLAPæŸå¤±è¿›è¡ŒéŸ³é¢‘-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼Œä»¥å¯¹é½è·¨æ¨¡æ€è¡¨ç¤ºï¼›ç»“åˆæ·±åº¦è¯åˆ¤åˆ«æŸå¤±è¿›è¡ŒéŸ³é¢‘-éŸ³é¢‘å¯¹æ¯”å­¦ä¹ ï¼Œå¢žå¼ºç±»å†…ç´§å‡‘æ€§å’Œç±»é—´åˆ†ç¦»æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œå®ƒç»Ÿä¸€äº†å¤šæ¨¡æ€ç›‘ç£ï¼Œé¿å…äº†åˆ†ç¦»ä¼˜åŒ–ï¼Œä»Žè€Œæé«˜äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œä»»åŠ¡é€‚åº”æ€§ï¼Œæ— éœ€ä¾èµ–ä»»åŠ¡ç‰¹å®šæ¨¡åž‹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨è¯åˆ¤åˆ«ä»»åŠ¡ä¸Šä¼˜äºŽçŽ°æœ‰å£°å­¦è¯åµŒå…¥åŸºçº¿ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚åŒæ—¶ï¼Œå®ƒçµæ´»æ”¯æŒå£è¯­è¯æ£€æµ‹å’Œå…³é”®è¯è¯†åˆ«ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šæ¨¡åž‹ï¼ŒéªŒè¯äº†è”åˆå¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸»è¦åº”ç”¨äºŽè¯­éŸ³æ£€ç´¢é¢†åŸŸï¼Œå¦‚å£è¯­è¯æ£€æµ‹å’Œå…³é”®è¯è¯†åˆ«ï¼Œå¯æå‡æ™ºèƒ½åŠ©æ‰‹ã€è¯­éŸ³æœç´¢å’ŒéŸ³é¢‘å†…å®¹åˆ†æžç³»ç»Ÿçš„æ•ˆçŽ‡å’Œå‡†ç¡®æ€§ã€‚æ½œåœ¨ä»·å€¼åŒ…æ‹¬æ”¯æŒå¤šè¯­è¨€å¤„ç†ã€ä½Žèµ„æºåœºæ™¯ä¸‹çš„é²æ£’æ€§ï¼Œä»¥åŠè·¨æ¨¡æ€ä¿¡æ¯èžåˆçš„å®žé™…åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.

