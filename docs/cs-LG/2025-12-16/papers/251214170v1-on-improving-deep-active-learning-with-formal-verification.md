---
layout: default
title: On Improving Deep Active Learning with Formal Verification
---

# On Improving Deep Active Learning with Formal Verification

**arXiv**: [2512.14170v1](https://arxiv.org/abs/2512.14170) | [PDF](https://arxiv.org/pdf/2512.14170.pdf)

**ä½œè€…**: Jonathan Spiegelman, Guy Amir, Guy Katz

**åˆ†ç±»**: cs.LG, cs.LO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå½¢å¼éªŒè¯çš„å¯¹æŠ—æ ·æœ¬å¢žå¼ºæ–¹æ³•ï¼Œä»¥æå‡æ·±åº¦ä¸»åŠ¨å­¦ä¹ çš„æ¨¡åž‹æ³›åŒ–æ€§èƒ½ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `æ·±åº¦ä¸»åŠ¨å­¦ä¹ ` `å½¢å¼éªŒè¯` `å¯¹æŠ—æ ·æœ¬` `æ•°æ®å¢žå¼º` `æ¨¡åž‹æ³›åŒ–` `é²æ£’æ€§çº¦æŸ` `ç¥žç»ç½‘ç»œè®­ç»ƒ` `æ ‡æ³¨æˆæœ¬é™ä½Ž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ·±åº¦ä¸»åŠ¨å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®å¢žå¼ºæ–¹é¢ä¾èµ–æ ‡å‡†åˆæˆè¾“å…¥ï¼Œå¯èƒ½å¿½ç•¥å¯¹æŠ—æ€§æ‰°åŠ¨å¯¹æ¨¡åž‹é²æ£’æ€§çš„å½±å“ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨å½¢å¼éªŒè¯ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬èƒ½æ›´æœ‰æ•ˆåœ°è¿åæ¨¡åž‹é²æ£’æ€§çº¦æŸï¼Œä½œä¸ºè®­ç»ƒæ•°æ®å¢žå¼ºæ‰‹æ®µï¼Œæå‡ä¸»åŠ¨å­¦ä¹ æ•ˆçŽ‡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªDALæŠ€æœ¯å’Œæ–°æå‡ºçš„æŠ€æœ¯ä¸Šåº”ç”¨åŽï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡åž‹æ³›åŒ–æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ä¸»åŠ¨å­¦ä¹ ï¼ˆDALï¼‰æ—¨åœ¨é€šè¿‡ä¼˜å…ˆæ ‡æ³¨ä¿¡æ¯é‡æœ€å¤§çš„æœªæ ‡è®°æ ·æœ¬æ¥é™ä½Žç¥žç»ç½‘ç»œè®­ç»ƒä¸­çš„æ ‡æ³¨æˆæœ¬ã€‚é™¤äº†é€‰æ‹©å“ªäº›æ ·æœ¬è¿›è¡Œæ ‡æ³¨å¤–ï¼Œä¸€äº›DALæ–¹æ³•è¿˜é€šè¿‡æ·»åŠ æ— éœ€é¢å¤–æ‰‹åŠ¨æ ‡æ³¨çš„åˆæˆè¾“å…¥æ¥å¢žå¼ºè®­ç»ƒé›†ï¼Œè¿›ä¸€æ­¥æé«˜æ•°æ®æ•ˆçŽ‡ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æŽ¢è®¨äº†å¦‚ä½•é€šè¿‡æ·»åŠ è¿åé²æ£’æ€§çº¦æŸçš„å¯¹æŠ—æ ·æœ¬æ¥å¢žå¼ºè®­ç»ƒæ•°æ®ï¼Œä»¥æå‡DALæ€§èƒ½ã€‚æˆ‘ä»¬è¡¨æ˜Žï¼Œé€šè¿‡å½¢å¼éªŒè¯ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬æ¯”åŸºäºŽæ ‡å‡†æ¢¯åº¦æ”»å‡»ç”Ÿæˆçš„æ ·æœ¬è´¡çŒ®æ›´å¤§ã€‚æˆ‘ä»¬å°†æ­¤æ‰©å±•åº”ç”¨äºŽå¤šç§çŽ°ä»£DALæŠ€æœ¯ä»¥åŠæˆ‘ä»¬æå‡ºçš„ä¸€ç§æ–°æŠ€æœ¯ï¼Œå¹¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæ¨¡åž‹æ³›åŒ–èƒ½åŠ›çš„æ˜¾è‘—æå‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡çš„æ ¸å¿ƒæ–¹æ³•æ˜¯å°†å½¢å¼éªŒè¯ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬é›†æˆåˆ°æ·±åº¦ä¸»åŠ¨å­¦ä¹ æ¡†æž¶ä¸­ã€‚æ•´ä½“æ¡†æž¶åŒ…æ‹¬ï¼šåœ¨ä¸»åŠ¨å­¦ä¹ å¾ªçŽ¯ä¸­ï¼Œä¸ä»…é€‰æ‹©ä¿¡æ¯é‡å¤§çš„æœªæ ‡è®°æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œè¿˜é€šè¿‡å½¢å¼éªŒè¯å·¥å…·ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ä½œä¸ºé¢å¤–è®­ç»ƒæ•°æ®ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽä½¿ç”¨å½¢å¼éªŒè¯è€Œéžä¼ ç»Ÿæ¢¯åº¦æ”»å‡»æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè¿™èƒ½æ›´ç³»ç»Ÿåœ°æŽ¢ç´¢æ¨¡åž‹å†³ç­–è¾¹ç•Œï¼Œç¡®ä¿æ ·æœ¬è¿åé²æ£’æ€§çº¦æŸã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒçŽ°æœ‰DALæ–¹æ³•é€šå¸¸ä¾èµ–éšæœºæˆ–åŸºäºŽæ¢¯åº¦çš„æ•°æ®å¢žå¼ºï¼Œè€Œæœ¬æ–¹æ³•å¼•å…¥å½¢å¼éªŒè¯çš„ç²¾ç¡®æ€§æ¥ç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§çš„å¯¹æŠ—æ ·æœ¬ï¼Œä»Žè€Œæ›´æœ‰æ•ˆåœ°æå‡æ¨¡åž‹é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒåŸºäºŽå½¢å¼éªŒè¯çš„å¯¹æŠ—æ ·æœ¬å¢žå¼ºåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡åž‹æ³›åŒ–æ€§èƒ½ï¼Œç›¸æ¯”ä¼ ç»Ÿæ¢¯åº¦æ”»å‡»æ–¹æ³•ï¼Œè´¡çŒ®æ›´å¤§ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ·±åº¦ä¸»åŠ¨å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽéœ€è¦é«˜æ•°æ®æ•ˆçŽ‡çš„æœºå™¨å­¦ä¹ åœºæ™¯ï¼Œå¦‚åŒ»ç–—å›¾åƒåˆ†æžã€è‡ªåŠ¨é©¾é©¶å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå…¶ä¸­æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”æ¨¡åž‹é²æ£’æ€§è‡³å…³é‡è¦ã€‚é€šè¿‡å‡å°‘æ ‡æ³¨éœ€æ±‚å¹¶æå‡æ³›åŒ–æ€§èƒ½ï¼Œæœ‰åŠ©äºŽåœ¨å®žé™…éƒ¨ç½²ä¸­æé«˜AIç³»ç»Ÿçš„å¯é æ€§å’Œæ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

