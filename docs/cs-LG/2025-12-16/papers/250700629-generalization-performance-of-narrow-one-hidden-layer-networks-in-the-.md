---
layout: default
title: Generalization performance of narrow one-hidden layer networks in the teacher-student setting
---

# Generalization performance of narrow one-hidden layer networks in the teacher-student setting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.00629" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.00629</a>
  <a href="https://arxiv.org/pdf/2507.00629.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.00629" onclick="toggleFavorite(this, '2507.00629', 'Generalization performance of narrow one-hidden layer networks in the teacher-student setting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jean Barbier, Federica Gerace, Alessandro Ingrosso, Clarissa Lauditi, Enrico M. Malatesta, Gibbs Nwemadji, Rodrigo PÃ©rez Ortiz

**åˆ†ç±»**: cs.LG, math.PR, math.ST

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹çª„å•éšå±‚ç½‘ç»œï¼Œæå‡ºåŸºäºå¸ˆç”Ÿæ¡†æ¶çš„æ³›åŒ–æ€§èƒ½ç†è®ºåˆ†ææ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¥ç»ç½‘ç»œæ³›åŒ–` `å¸ˆç”Ÿæ¡†æ¶` `ç»Ÿè®¡ç‰©ç†` `çª„ç½‘ç»œ` `å•éšå±‚ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å…·æœ‰é€šç”¨æ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥å•éšå±‚ç½‘ç»œæ³›åŒ–æ€§èƒ½çš„å®Œæ•´ç†è®ºæè¿°ï¼Œå°¤å…¶æ˜¯åœ¨çª„ç½‘ç»œçš„æƒ…å†µä¸‹ã€‚
2. è¯¥è®ºæ–‡åˆ©ç”¨ç»Ÿè®¡ç‰©ç†å­¦çš„æ–¹æ³•ï¼Œä¸ºçª„ç½‘ç»œçš„æ³›åŒ–æ€§èƒ½æä¾›äº†é—­å¼è¡¨è¾¾å¼ï¼Œå¹¶æ­ç¤ºäº†éšè—ç¥ç»å…ƒä¸“ä¸šåŒ–è½¬å˜çš„ç°è±¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç†è®ºèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ç¥ç»ç½‘ç»œåœ¨å›å½’æˆ–åˆ†ç±»ä»»åŠ¡ä¸­çš„æ³›åŒ–è¯¯å·®ï¼ŒéªŒè¯äº†ç†è®ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç†è§£ç¥ç»ç½‘ç»œåœ¨ç®€å•è¾“å…¥è¾“å‡ºåˆ†å¸ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹äºè§£é‡Šå…¶åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å­¦ä¹ æ€§èƒ½è‡³å…³é‡è¦ã€‚ç»å…¸çš„å¸ˆç”Ÿæ¡†æ¶æä¾›äº†ä¸€ä¸ªå®Œç¾çš„ç†è®ºæµ‹è¯•å¹³å°ï¼Œå…¶ä¸­å­¦ç”Ÿç½‘ç»œä»ç”±æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„æ•°æ®ä¸­å­¦ä¹ ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹å¯¹å…·æœ‰é€šç”¨æ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥å•éšå±‚ç½‘ç»œæ€§èƒ½çš„å®Œæ•´ç†è®ºæè¿°ã€‚æœ¬æ–‡é’ˆå¯¹çª„ç½‘ç»œï¼ˆå³éšè—å•å…ƒæ•°é‡è¿œå°äºè¾“å…¥ç»´åº¦ï¼‰æå‡ºäº†è¿™æ ·çš„é€šç”¨ç†è®ºã€‚åˆ©ç”¨ç»Ÿè®¡ç‰©ç†å­¦çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä¸ºæœ‰é™æ¸©åº¦ï¼ˆè´å¶æ–¯ï¼‰å’Œç»éªŒé£é™©æœ€å°åŒ–ä¼°è®¡å™¨çš„å…¸å‹æ€§èƒ½æä¾›äº†é—­å¼è¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼ä»…ä¾èµ–äºå°‘é‡çš„ç»Ÿè®¡é‡ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œå½“æ ·æœ¬æ•°é‡è¶³å¤Ÿå¤§ä¸”ä¸ç½‘ç»œå‚æ•°æ•°é‡æˆæ¯”ä¾‹æ—¶ï¼Œéšè—ç¥ç»å…ƒä¼šå‘ç”Ÿä¸“ä¸šåŒ–è½¬å˜ã€‚æˆ‘ä»¬çš„ç†è®ºèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ç¥ç»ç½‘ç»œåœ¨å›å½’æˆ–åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨å™ªå£°å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼‰æˆ–å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™è®­ç»ƒæ—¶çš„æ³›åŒ–è¯¯å·®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè¯¥è®ºæ–‡æ—¨åœ¨è§£å†³çª„å•éšå±‚ç¥ç»ç½‘ç»œåœ¨å¸ˆç”Ÿæ¡†æ¶ä¸‹çš„æ³›åŒ–æ€§èƒ½åˆ†æé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å…·æœ‰é€šç”¨æ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥å•éšå±‚ç½‘ç»œæ€§èƒ½çš„å®Œæ•´ç†è®ºæè¿°ï¼Œå°¤å…¶æ˜¯åœ¨éšè—å•å…ƒæ•°é‡è¿œå°äºè¾“å…¥ç»´åº¦çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥å‡†ç¡®é¢„æµ‹ç½‘ç»œçš„æ³›åŒ–è¯¯å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­çš„æ–¹æ³•ï¼Œå¯¹çª„ç½‘ç»œçš„æ³›åŒ–æ€§èƒ½è¿›è¡Œç†è®ºåˆ†æã€‚é€šè¿‡å°†ç¥ç»ç½‘ç»œçš„å­¦ä¹ è¿‡ç¨‹ç±»æ¯”äºç‰©ç†ç³»ç»Ÿï¼Œå¯ä»¥æ¨å¯¼å‡ºæ³›åŒ–è¯¯å·®çš„é—­å¼è¡¨è¾¾å¼ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ç½‘ç»œçš„å­¦ä¹ è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ•æ‰åˆ°ç½‘ç»œä¸­çš„ä¸€äº›å…³é”®ç°è±¡ï¼Œä¾‹å¦‚éšè—ç¥ç»å…ƒçš„ä¸“ä¸šåŒ–è½¬å˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥è®ºæ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) å»ºç«‹å¸ˆç”Ÿæ¡†æ¶ä¸‹çš„çª„å•éšå±‚ç¥ç»ç½‘ç»œæ¨¡å‹ï¼›2) åˆ©ç”¨ç»Ÿè®¡ç‰©ç†å­¦ä¸­çš„å‰¯æœ¬æ–¹æ³•æˆ–è…”æ–¹æ³•ï¼Œè®¡ç®—ç½‘ç»œçš„è‡ªç”±èƒ½æˆ–é…åˆ†å‡½æ•°ï¼›3) ä»è‡ªç”±èƒ½æˆ–é…åˆ†å‡½æ•°ä¸­æ¨å¯¼å‡ºæ³›åŒ–è¯¯å·®çš„é—­å¼è¡¨è¾¾å¼ï¼›4) é€šè¿‡å®éªŒéªŒè¯ç†è®ºé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†é’ˆå¯¹çª„å•éšå±‚ç½‘ç»œçš„æ³›åŒ–æ€§èƒ½çš„å®Œæ•´ç†è®ºåˆ†ææ–¹æ³•ï¼›2) åˆ©ç”¨ç»Ÿè®¡ç‰©ç†å­¦çš„æ–¹æ³•ï¼Œæ¨å¯¼å‡ºäº†æ³›åŒ–è¯¯å·®çš„é—­å¼è¡¨è¾¾å¼ï¼Œä¸ºç†è§£ç½‘ç»œçš„å­¦ä¹ è¡Œä¸ºæä¾›äº†æ–°çš„è§†è§’ï¼›3) æ­ç¤ºäº†éšè—ç¥ç»å…ƒä¸“ä¸šåŒ–è½¬å˜çš„ç°è±¡ï¼Œå¹¶åˆ†æäº†å…¶å¯¹æ³›åŒ–æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) å‡è®¾éšè—å•å…ƒçš„æ•°é‡è¿œå°äºè¾“å…¥ç»´åº¦ï¼Œä»è€Œç®€åŒ–äº†ç†è®ºåˆ†æï¼›2) ä½¿ç”¨é€šç”¨æ¿€æ´»å‡½æ•°ï¼Œä½¿å¾—ç†è®ºåˆ†æå…·æœ‰æ›´å¹¿æ³›çš„é€‚ç”¨æ€§ï¼›3) è€ƒè™‘äº†æœ‰é™æ¸©åº¦ï¼ˆè´å¶æ–¯ï¼‰å’Œç»éªŒé£é™©æœ€å°åŒ–ä¸¤ç§ä¸åŒçš„å­¦ä¹ æ–¹å¼ï¼›4) é€šè¿‡å™ªå£°å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼‰æˆ–å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2507.00629/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2507.00629/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2507.00629/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†ç†è®ºé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç†è®ºèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ç¥ç»ç½‘ç»œåœ¨å›å½’æˆ–åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨å™ªå£°å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼‰æˆ–å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™è®­ç»ƒæ—¶çš„æ³›åŒ–è¯¯å·®ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†éšè—ç¥ç»å…ƒä¸“ä¸šåŒ–è½¬å˜ç°è±¡çš„å­˜åœ¨ï¼Œå¹¶åˆ†æäº†å…¶å¯¹æ³›åŒ–æ€§èƒ½çš„å½±å“ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç†è§£å’Œä¼˜åŒ–ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸‹ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œå¯ä»¥æ›´å¥½åœ°é€‰æ‹©åˆé€‚çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒå‚æ•°ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„ç¥ç»ç½‘ç»œæ¶æ„æä¾›ç†è®ºæŒ‡å¯¼ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡æ§åˆ¶éšè—ç¥ç»å…ƒçš„ä¸“ä¸šåŒ–ç¨‹åº¦æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding the generalization abilities of neural networks for simple input-output distributions is crucial to account for their learning performance on real datasets. The classical teacher-student setting, where a network is trained from data obtained thanks to a label-generating teacher model, serves as a perfect theoretical test bed. In this context, a complete theoretical account of the performance of fully connected one-hidden layer networks in the presence of generic activation functions is lacking. In this work, we develop such a general theory for narrow networks, i.e. with a large number of hidden units, yet much smaller than the input dimension. Using methods from statistical physics, we provide closed-form expressions for the typical performance of both finite temperature (Bayesian) and empirical risk minimization estimators, in terms of a small number of summary statistics. In doing so, we highlight the presence of a transition where hidden neurons specialize when the number of samples is sufficiently large and proportional to the number of parameters of the network. Our theory accurately predicts the generalization error of neural networks trained on regression or classification tasks with either noisy full-batch gradient descent (Langevin dynamics) or full-batch gradient descent.

