---
layout: default
title: TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines
---

# TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines

**arXiv**: [2512.14645v1](https://arxiv.org/abs/2512.14645) | [PDF](https://arxiv.org/pdf/2512.14645.pdf)

**ä½œè€…**: David Schulmeister, Valentin Hartmann, Lars Klein, Robert West

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTiMEï¼ˆTiny Monolingual Encodersï¼‰ä»¥è§£å†³å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨æ•ˆçŽ‡å…³é”®åº”ç”¨ä¸­é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜çš„é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å°åž‹è¯­è¨€æ¨¡åž‹` `å•è¯­è¨€ç¼–ç å™¨` `è’¸é¦è®­ç»ƒ` `æ•ˆçŽ‡ä¼˜åŒ–` `ä½Žèµ„æºè¯­è¨€` `èƒ½è€—é™ä½Ž` `å®žæ—¶NLP` `æ¨¡åž‹åŽ‹ç¼©`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹é€šç”¨è¯­è¨€æ¨¡åž‹åœ¨NLPæµæ°´çº¿ä¸­é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜ï¼Œä¸é€‚åˆæ•ˆçŽ‡å…³é”®åº”ç”¨å’Œä½Žèµ„æºè®¾å¤‡éƒ¨ç½²ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºTiMEæ¨¡åž‹ï¼Œé€šè¿‡è’¸é¦æŠ€æœ¯è®­ç»ƒå°åž‹å•è¯­è¨€ç¼–ç å™¨ï¼Œæ”¯æŒä½Žèµ„æºè¯­è¨€ï¼Œä¼˜åŒ–æ€§èƒ½ä¸Žæ•ˆçŽ‡çš„æƒè¡¡ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§NLPä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒTiMEåœ¨åŸºå‡†æ€§èƒ½ã€åžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—æ–¹é¢è¡¨çŽ°æ›´ä¼˜ï¼ŒéªŒè¯äº†è’¸é¦æ–¹æ³•çš„å¯è¡Œæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰è¯­è¨€æ¨¡åž‹ç ”ç©¶å¤šé›†ä¸­äºŽå¤§åž‹é€šç”¨æ¨¡åž‹ï¼Œä½†è®¸å¤šNLPæµæ°´çº¿ä»…éœ€å…·å¤‡æ˜Žç¡®ã€å°è§„æ¨¡èƒ½åŠ›çš„æ¨¡åž‹ã€‚å¤§åž‹æ¨¡åž‹è™½èƒ½æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œä½†å¤„ç†å¤§é‡æ•°æ®æˆ–æä¾›å®žæ—¶å“åº”æ—¶é€Ÿåº¦ä¸è¶³ï¼Œä¸”èƒ½è€—è¿‡é«˜ï¼Œå¯¼è‡´å¯æŒç»­æ€§é—®é¢˜ï¼Œåœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ã€‚æœ¬å·¥ä½œå±•ç¤ºäº†å¦‚ä½•ä¸ºè¿™ç±»æ•ˆçŽ‡å…³é”®åº”ç”¨è®­ç»ƒå°åž‹æ¨¡åž‹ã€‚ä¸Žè®¸å¤šçŽ°æˆNLPæµæ°´çº¿ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡åž‹é‡‡ç”¨è’¸é¦ç­‰çŽ°ä»£è®­ç»ƒæŠ€æœ¯ï¼Œå¹¶æ”¯æŒä½Žèµ„æºè¯­è¨€ã€‚æˆ‘ä»¬ç§°è¿™äº›æ¨¡åž‹ä¸ºTiMEï¼ˆTiny Monolingual Encodersï¼‰ï¼Œå¹¶åœ¨å¤šç§å¸¸è§NLPä»»åŠ¡ä¸Šå…¨é¢è¯„ä¼°ï¼Œè§‚å¯Ÿåˆ°å…¶åœ¨åŸºå‡†æ€§èƒ½ä¸Žåžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´å®žçŽ°äº†æ›´å¥½çš„æƒè¡¡ã€‚è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¯æ˜Žäº†ä»Žå¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è’¸é¦å•è¯­è¨€æ¨¡åž‹æ˜¯å¯è¡Œçš„ï¼ŒåŒæ ·å¯ä»¥ä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡åž‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡åž‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

TiMEæ¨¡åž‹é‡‡ç”¨åŸºäºŽè’¸é¦çš„æ•´ä½“æ¡†æž¶ï¼Œä»Žå¤§åž‹å¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è’¸é¦å‡ºå°åž‹å•è¯­è¨€ç¼–ç å™¨ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šä½¿ç”¨çŽ°ä»£è’¸é¦æŠ€æœ¯ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œæ”¯æŒä»Žå¤šè¯­è¨€æ•™å¸ˆè’¸é¦å•è¯­è¨€æ¨¡åž‹ï¼Œä»¥åŠä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆè’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡åž‹ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼ŒTiMEä¸“æ³¨äºŽæ•ˆçŽ‡å…³é”®åº”ç”¨ï¼Œé€šè¿‡å°åž‹åŒ–è®¾è®¡å‡å°‘æ¨¡åž‹å‚æ•°ï¼Œç»“åˆè’¸é¦æå‡æ€§èƒ½ï¼Œè€Œä¼ ç»ŸNLPæµæ°´çº¿å¾€å¾€ä¾èµ–å¤§åž‹æ¨¡åž‹æˆ–ç¼ºä¹é«˜æ•ˆè®­ç»ƒæŠ€æœ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒTiMEåœ¨å¤šç§NLPä»»åŠ¡ä¸Šå®žçŽ°äº†åŸºå‡†æ€§èƒ½ä¸Žåžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—çš„æ›´å¥½æƒè¡¡ï¼ŒéªŒè¯äº†ä»Žå¤šè¯­è¨€æ•™å¸ˆè’¸é¦å•è¯­è¨€æ¨¡åž‹ä»¥åŠä»Žç›¸å¯¹ä½ç½®åµŒå…¥æ•™å¸ˆè’¸é¦ç»å¯¹ä½ç½®åµŒå…¥æ¨¡åž‹çš„å¯è¡Œæ€§ï¼Œæå‡äº†å°åž‹æ¨¡åž‹çš„å®žç”¨ä»·å€¼ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TiMEé€‚ç”¨äºŽéœ€è¦é«˜æ•ˆNLPå¤„ç†çš„åœºæ™¯ï¼Œå¦‚å®žæ—¶å“åº”ç³»ç»Ÿã€å¤§è§„æ¨¡æ•°æ®å¤„ç†ã€ä½Žèµ„æºè¯­è¨€æ”¯æŒï¼Œä»¥åŠåœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ï¼ˆå¦‚ç§»åŠ¨è®¾å¤‡æˆ–ç‰©è”ç½‘è®¾å¤‡ï¼‰ä¸Šçš„éƒ¨ç½²ï¼Œæœ‰åŠ©äºŽé™ä½Žèƒ½è€—å’Œæå‡å¯æŒç»­æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

