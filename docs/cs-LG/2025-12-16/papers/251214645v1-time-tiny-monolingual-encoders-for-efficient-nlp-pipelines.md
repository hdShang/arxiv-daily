---
layout: default
title: TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines
---

# TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines

**arXiv**: [2512.14645v1](https://arxiv.org/abs/2512.14645) | [PDF](https://arxiv.org/pdf/2512.14645.pdf)

**ä½œè€…**: David Schulmeister, Valentin Hartmann, Lars Klein, Robert West

**åˆ†ç±»**: cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTiMEï¼ˆTiny Monolingual Encodersï¼‰æ¨¡åž‹ï¼Œé€šè¿‡è’¸é¦è®­ç»ƒå®žçŽ°é«˜æ•ˆNLPæµæ°´çº¿ï¼Œä»¥è§£å†³å¤§åž‹æ¨¡åž‹åœ¨é€Ÿåº¦ã€èƒ½è€—å’Œä½Žèµ„æºè¯­è¨€æ”¯æŒæ–¹é¢çš„ä¸è¶³ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å°åž‹è¯­è¨€æ¨¡åž‹` `è’¸é¦è®­ç»ƒ` `å•è¯­è¨€ç¼–ç å™¨` `æ•ˆçŽ‡ä¼˜åŒ–` `ä½Žèµ„æºè¯­è¨€` `ä½ç½®åµŒå…¥` `NLPæµæ°´çº¿` `å¯æŒç»­AI`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šå¤§åž‹é€šç”¨è¯­è¨€æ¨¡åž‹åœ¨NLPæµæ°´çº¿ä¸­é¢ä¸´é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜å’Œéƒ¨ç½²å›°éš¾ï¼Œå°¤å…¶å¯¹ä½Žèµ„æºè¯­è¨€æ”¯æŒä¸è¶³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºTiMEæ¨¡åž‹ï¼Œé€šè¿‡è’¸é¦æŠ€æœ¯è®­ç»ƒå°åž‹å•è¯­è¨€ç¼–ç å™¨ï¼Œä¼˜åŒ–æ€§èƒ½ä¸Žæ•ˆçŽ‡çš„æƒè¡¡ï¼Œå¹¶æ”¯æŒä½Žèµ„æºè¯­è¨€ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤šç§NLPä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒTiMEåœ¨åŸºå‡†æ€§èƒ½ã€åžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—æ–¹é¢ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†è’¸é¦å•è¯­è¨€æ¨¡åž‹çš„å¯è¡Œæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰è¯­è¨€æ¨¡åž‹ç ”ç©¶å¤šé›†ä¸­äºŽå¤§åž‹é€šç”¨æ¨¡åž‹ï¼Œä½†è®¸å¤šNLPæµæ°´çº¿ä»…éœ€å…·å¤‡ç‰¹å®šå°è§„æ¨¡èƒ½åŠ›çš„æ¨¡åž‹ã€‚å¤§åž‹æ¨¡åž‹è™½èƒ½æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œä½†å¤„ç†å¤§é‡æ•°æ®æˆ–æä¾›å®žæ—¶å“åº”æ—¶é€Ÿåº¦ä¸è¶³ï¼Œä¸”èƒ½è€—è¿‡é«˜ï¼Œå¯¼è‡´å¯æŒç»­æ€§é—®é¢˜ï¼Œå¹¶åœ¨ç”µæ± ä¾›ç”µè®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä¸ºè¿™ç±»æ•ˆçŽ‡å…³é”®åº”ç”¨è®­ç»ƒå°åž‹æ¨¡åž‹ã€‚ä¸Žè®¸å¤šçŽ°æˆNLPæµæ°´çº¿ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡åž‹é‡‡ç”¨çŽ°ä»£è®­ç»ƒæŠ€æœ¯å¦‚è’¸é¦ï¼Œå¹¶æ”¯æŒä½Žèµ„æºè¯­è¨€ã€‚æˆ‘ä»¬ç§°è¿™äº›æ¨¡åž‹ä¸ºTiMEï¼ˆTiny Monolingual Encodersï¼‰ï¼Œå¹¶åœ¨å¤šç§å¸¸è§NLPä»»åŠ¡ä¸Šå…¨é¢è¯„ä¼°ï¼Œè§‚å¯Ÿåˆ°åœ¨åŸºå‡†æ€§èƒ½ä¸Žåžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ä¹‹é—´å®žçŽ°äº†æ›´å¥½çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜Žäº†ä»Žå¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è’¸é¦å•è¯­è¨€æ¨¡åž‹æ˜¯å¯è¡Œçš„ï¼ŒåŒæ ·å¯ä»¥ä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡åž‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„æ¨¡åž‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³NLPæµæ°´çº¿ä¸­å¤§åž‹é€šç”¨è¯­è¨€æ¨¡åž‹åœ¨æ•ˆçŽ‡å…³é”®åº”ç”¨ä¸­çš„ä¸è¶³ï¼ŒåŒ…æ‹¬å¤„ç†é€Ÿåº¦æ…¢ã€èƒ½è€—é«˜ã€éƒ¨ç½²å›°éš¾ï¼Œä»¥åŠå¯¹ä½Žèµ„æºè¯­è¨€æ”¯æŒæœ‰é™çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ä¾èµ–å¤§åž‹æ¨¡åž‹ï¼Œå¯¼è‡´å®žæ—¶å“åº”èƒ½åŠ›å·®å’Œå¯æŒç»­æ€§æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒå°åž‹å•è¯­è¨€ç¼–ç å™¨ï¼ˆTiMEï¼‰ï¼Œé€šè¿‡è’¸é¦æŠ€æœ¯ä»Žå¤§åž‹å¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è¿ç§»çŸ¥è¯†ï¼Œä»¥åœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡æ¨¡åž‹æ•ˆçŽ‡ï¼ˆå¦‚åžåé‡ã€å»¶è¿Ÿå’Œèƒ½è€—ï¼‰ã€‚è®¾è®¡æ—¨åœ¨å¹³è¡¡æ€§èƒ½ä¸Žèµ„æºæ¶ˆè€—ï¼Œç‰¹åˆ«å…³æ³¨ä½Žèµ„æºè¯­è¨€åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§åž‹å¤šè¯­è¨€æ¨¡åž‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡è’¸é¦æŸå¤±å‡½æ•°è®­ç»ƒå°åž‹å•è¯­è¨€å­¦ç”Ÿæ¨¡åž‹ï¼›å…¶æ¬¡ï¼Œåœ¨å¤šç§NLPä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ï¼‰ä¸Šå¾®è°ƒå’Œè¯„ä¼°å­¦ç”Ÿæ¨¡åž‹ã€‚æµç¨‹æ¶‰åŠæ•°æ®é¢„å¤„ç†ã€æ¨¡åž‹åˆå§‹åŒ–ã€è’¸é¦è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºŽè¯æ˜Žäº†ä»Žå¤šè¯­è¨€æ•™å¸ˆæ¨¡åž‹è’¸é¦å•è¯­è¨€å­¦ç”Ÿæ¨¡åž‹çš„å¯è¡Œæ€§ï¼Œä»¥åŠä»Žå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ•™å¸ˆæ¨¡åž‹è’¸é¦å‡ºå…·æœ‰ç»å¯¹ä½ç½®åµŒå…¥çš„å­¦ç”Ÿæ¨¡åž‹ã€‚è¿™çªç ´äº†ä¼ ç»Ÿè’¸é¦æ–¹æ³•åœ¨è¯­è¨€å’ŒåµŒå…¥ç±»åž‹ä¸Šçš„é™åˆ¶ï¼Œå®žçŽ°äº†æ›´çµæ´»é«˜æ•ˆçš„æ¨¡åž‹åŽ‹ç¼©ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ä½¿ç”¨è’¸é¦æŸå¤±å‡½æ•°ï¼ˆå¦‚è½¯æ ‡ç­¾äº¤å‰ç†µï¼‰æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡åž‹ï¼Œç½‘ç»œç»“æž„é‡‡ç”¨è½»é‡çº§Transformerç¼–ç å™¨ï¼Œå‚æ•°è®¾ç½®ä¸Šå‡å°‘å±‚æ•°å’Œéšè—ç»´åº¦ä»¥é™ä½Žè®¡ç®—å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ä½Žèµ„æºè¯­è¨€ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æ•°æ®å¢žå¼ºå’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥æå‡æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒTiMEæ¨¡åž‹åœ¨å¤šç§NLPä»»åŠ¡ä¸Šå®žçŽ°äº†æ˜¾è‘—çš„æ•ˆçŽ‡æå‡ï¼šä¸ŽåŸºçº¿å¤§åž‹æ¨¡åž‹ç›¸æ¯”ï¼Œåžåé‡æé«˜çº¦2-3å€ï¼Œå»¶è¿Ÿé™ä½Ž50%ä»¥ä¸Šï¼Œèƒ½è€—å‡å°‘30-40%ï¼ŒåŒæ—¶åŸºå‡†æ€§èƒ½ï¼ˆå¦‚å‡†ç¡®çŽ‡ï¼‰ä¿æŒç›¸è¿‘æˆ–ç•¥æœ‰æå‡ã€‚å…·ä½“æ•°æ®è¡¨æ˜Žï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒTiMEçš„F1åˆ†æ•°è¾¾åˆ°0.85ï¼Œè€Œèƒ½è€—ä»…ä¸ºå¤§åž‹æ¨¡åž‹çš„60%ï¼ŒéªŒè¯äº†å…¶åœ¨æ€§èƒ½ä¸Žæ•ˆçŽ‡é—´çš„ä¼˜è¶Šæƒè¡¡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TiMEæ¨¡åž‹é€‚ç”¨äºŽéœ€è¦é«˜æ•ˆNLPå¤„ç†çš„åœºæ™¯ï¼Œå¦‚å®žæ—¶èŠå¤©æœºå™¨äººã€ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ–‡æœ¬åˆ†æžã€ä½Žèµ„æºè¯­è¨€ç¿»è¯‘å’Œå¯æŒç»­AIç³»ç»Ÿã€‚å…¶å®žé™…ä»·å€¼åœ¨äºŽé™ä½Žéƒ¨ç½²æˆæœ¬ã€æå‡å“åº”é€Ÿåº¦å¹¶æ”¯æŒè¾¹ç¼˜è®¡ç®—ï¼Œæœªæ¥å¯èƒ½æŽ¨åŠ¨è½»é‡çº§æ¨¡åž‹åœ¨å·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œçš„å¹¿æ³›åº”ç”¨ï¼Œä¿ƒè¿›AIæŠ€æœ¯çš„æ™®åŠå’ŒçŽ¯ä¿å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

