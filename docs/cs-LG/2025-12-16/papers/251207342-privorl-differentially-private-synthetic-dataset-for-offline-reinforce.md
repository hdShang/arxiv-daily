---
layout: default
title: PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning
---

# PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.07342" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.07342</a>
  <a href="https://arxiv.org/pdf/2512.07342.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.07342" onclick="toggleFavorite(this, '2512.07342', 'PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chen Gong, Zheng Liu, Kecen Li, Tianhao Wang

**åˆ†ç±»**: cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PrivORLï¼šä¸€ç§å·®åˆ†éšç§ç¦»çº¿å¼ºåŒ–å­¦ä¹ åˆæˆæ•°æ®é›†ç”Ÿæˆæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `å·®åˆ†éšç§` `æ•°æ®é›†åˆæˆ` `æ‰©æ•£æ¨¡å‹` `æ‰©æ•£Transformer` `éšç§ä¿æŠ¤` `æ•°æ®æ•ˆç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ•°æ®é›†å­˜åœ¨éšç§æ³„éœ²é£é™©ï¼Œéœ€è¦ä¿æŠ¤æ•°æ®é›†ä¸­åŒ…å«çš„æ•æ„Ÿä¿¡æ¯ã€‚
2. PrivORLåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£Transformerï¼Œåœ¨å·®åˆ†éšç§ä¿æŠ¤ä¸‹åˆæˆé«˜è´¨é‡çš„è½¬ç§»å’Œè½¨è¿¹æ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPrivORLåœ¨æ•ˆç”¨æ€§å’Œä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸçš„åˆæˆæ•°æ®é›†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿å¼ºåŒ–å­¦ä¹ (RL)å·²æˆä¸ºä¸€ç§æµè¡Œçš„RLèŒƒå¼ã€‚åœ¨ç¦»çº¿RLä¸­ï¼Œæ•°æ®æä¾›è€…å…±äº«é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†ï¼ˆå•ä¸ªè½¬ç§»æˆ–å½¢æˆè½¨è¿¹çš„è½¬ç§»åºåˆ—ï¼‰ï¼Œä»¥æ”¯æŒRLæ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºæ™ºèƒ½ä½“ï¼‰çš„è®­ç»ƒï¼Œè€Œæ— éœ€ä¸ç¯å¢ƒç›´æ¥äº¤äº’ã€‚ç¦»çº¿RLèŠ‚çœäº†ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œå¹¶ä¸”åœ¨å¯¼èˆªç­‰å…³é”®é¢†åŸŸéå¸¸æœ‰æ•ˆã€‚ä¸æ­¤åŒæ—¶ï¼Œç¦»çº¿RLæ•°æ®é›†ä¸­çš„éšç§æ³„éœ²é—®é¢˜æ—¥ç›Šçªå‡ºã€‚ä¸ºäº†ä¿æŠ¤ç¦»çº¿RLæ•°æ®é›†ä¸­çš„éšç§ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªå·®åˆ†éšç§(DP)ç¦»çº¿æ•°æ®é›†åˆæˆæ–¹æ³•PrivORLï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£Transformeråˆ†åˆ«åœ¨DPä¸‹åˆæˆè½¬ç§»å’Œè½¨è¿¹ã€‚ç„¶åï¼Œå¯ä»¥å®‰å…¨åœ°å‘å¸ƒåˆæˆæ•°æ®é›†ï¼Œç”¨äºä¸‹æ¸¸åˆ†æå’Œç ”ç©¶ã€‚PrivORLé‡‡ç”¨äº†ä¸€ç§æµè¡Œçš„æ–¹æ³•ï¼Œå³åœ¨å…¬å…±æ•°æ®é›†ä¸Šé¢„è®­ç»ƒåˆæˆå™¨ï¼Œç„¶åä½¿ç”¨DPéšæœºæ¢¯åº¦ä¸‹é™(DP-SGD)åœ¨æ•æ„Ÿæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼ŒPrivORLå¼•å…¥äº†å¥½å¥‡å¿ƒé©±åŠ¨çš„é¢„è®­ç»ƒï¼Œå®ƒåˆ©ç”¨æ¥è‡ªå¥½å¥‡å¿ƒæ¨¡å—çš„åé¦ˆæ¥å¤šæ ·åŒ–åˆæˆæ•°æ®é›†ï¼Œä»è€Œå¯ä»¥ç”Ÿæˆä¸æ•æ„Ÿæ•°æ®é›†éå¸¸ç›¸ä¼¼çš„å„ç§åˆæˆè½¬ç§»å’Œè½¨è¿¹ã€‚åœ¨äº”ä¸ªæ•æ„Ÿç¦»çº¿RLæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨DPè½¬ç§»å’Œè½¨è¿¹åˆæˆä¸­éƒ½å®ç°äº†æ›´å¥½çš„æ•ˆç”¨å’Œä¿çœŸåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­æ•°æ®é›†çš„éšç§æ³„éœ²é—®é¢˜ã€‚ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºé¢„å…ˆæ”¶é›†çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ã€‚ç›´æ¥ä½¿ç”¨è¿™äº›æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒä¼šå¸¦æ¥éšç§æ³„éœ²çš„é£é™©ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–¹æ³•æ¥ç”Ÿæˆæ—¢èƒ½ä¿æŠ¤éšç§åˆèƒ½ä¿ç•™æ•°æ®é›†ç‰¹å¾çš„åˆæˆæ•°æ®é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å·®åˆ†éšç§(DP)æŠ€æœ¯ï¼Œç»“åˆæ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£Transformerï¼Œç”Ÿæˆæ—¢èƒ½ä¿æŠ¤éšç§åˆèƒ½ä¿ç•™åŸå§‹æ•°æ®é›†ç‰¹å¾çš„åˆæˆæ•°æ®é›†ã€‚é€šè¿‡åœ¨å…¬å…±æ•°æ®é›†ä¸Šé¢„è®­ç»ƒåˆæˆå™¨ï¼Œç„¶ååœ¨æ•æ„Ÿæ•°æ®é›†ä¸Šè¿›è¡Œå·®åˆ†éšç§å¾®è°ƒï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡éšç§ä¿æŠ¤å’Œæ•°æ®æ•ˆç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPrivORLçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) åœ¨å…¬å…±æ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£Transformerï¼›2) åœ¨æ•æ„Ÿæ•°æ®é›†ä¸Šä½¿ç”¨DP-SGDå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›3) ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆåˆæˆçš„è½¬ç§»å’Œè½¨è¿¹æ•°æ®ã€‚å…¶ä¸­ï¼Œæ‰©æ•£æ¨¡å‹ç”¨äºç”Ÿæˆå•ä¸ªè½¬ç§»ï¼Œæ‰©æ•£Transformerç”¨äºç”Ÿæˆè½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šPrivORLçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) é¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£Transformeråº”ç”¨äºå·®åˆ†éšç§ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ•°æ®é›†åˆæˆï¼›2) æå‡ºäº†å¥½å¥‡å¿ƒé©±åŠ¨çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨å¥½å¥‡å¿ƒæ¨¡å—çš„åé¦ˆæ¥å¤šæ ·åŒ–åˆæˆæ•°æ®é›†ï¼Œæé«˜æ•°æ®è´¨é‡ï¼›3) å®ç°äº†åœ¨å·®åˆ†éšç§ä¿æŠ¤ä¸‹ç”Ÿæˆé«˜è´¨é‡çš„è½¬ç§»å’Œè½¨è¿¹æ•°æ®ï¼Œæœ‰æ•ˆå¹³è¡¡äº†éšç§ä¿æŠ¤å’Œæ•°æ®æ•ˆç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šPrivORLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨DP-SGDè¿›è¡Œå¾®è°ƒï¼Œä»¥ä¿è¯å·®åˆ†éšç§ï¼›2) è®¾è®¡å¥½å¥‡å¿ƒæ¨¡å—ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ›´å¤šæ ·åŒ–çš„çŠ¶æ€ç©ºé—´ï¼›3) ä½¿ç”¨æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£Transformeråˆ†åˆ«ç”Ÿæˆè½¬ç§»å’Œè½¨è¿¹ï¼Œå……åˆ†åˆ©ç”¨äº†ä¸¤ç§æ¨¡å‹çš„ä¼˜åŠ¿ï¼›4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œæ—¨åœ¨æœ€å°åŒ–åˆæˆæ•°æ®ä¸åŸå§‹æ•°æ®ä¹‹é—´çš„å·®å¼‚ï¼ŒåŒæ—¶æ»¡è¶³å·®åˆ†éšç§çº¦æŸã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.07342/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.07342/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.07342/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPrivORLåœ¨äº”ä¸ªæ•æ„Ÿç¦»çº¿RLæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒPrivORLåœ¨DPè½¬ç§»å’Œè½¨è¿¹åˆæˆä¸­éƒ½å®ç°äº†æ›´å¥½çš„æ•ˆç”¨å’Œä¿çœŸåº¦ã€‚å…·ä½“æ¥è¯´ï¼ŒPrivORLèƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸçš„åˆæˆæ•°æ®é›†ï¼Œä½¿å¾—åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒçš„RLæ™ºèƒ½ä½“èƒ½å¤Ÿè·å¾—ä¸åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ™ºèƒ½ä½“ç›¸è¿‘çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PrivORLå¯åº”ç”¨äºå„ç§éœ€è¦ä¿æŠ¤éšç§çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—ã€é‡‘èå’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡ç”Ÿæˆå·®åˆ†éšç§çš„åˆæˆæ•°æ®é›†ï¼Œå¯ä»¥å®‰å…¨åœ°å…±äº«æ•°æ®ï¼Œä¿ƒè¿›ç®—æ³•ç ”ç©¶å’Œæ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶é¿å…æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²ã€‚è¯¥æ–¹æ³•æœ‰åŠ©äºæ¨åŠ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨éšç§æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.

