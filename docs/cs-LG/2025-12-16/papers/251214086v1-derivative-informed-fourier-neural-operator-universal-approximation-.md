---
layout: default
title: Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization
---

# Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization

**arXiv**: [2512.14086v1](https://arxiv.org/abs/2512.14086) | [PDF](https://arxiv.org/pdf/2512.14086.pdf)

**‰ΩúËÄÖ**: Boyuan Yao, Dingcheng Luo, Lianghao Cao, Nikola Kovachki, Thomas O'Leary-Roseberry, Omar Ghattas

**ÂàÜÁ±ª**: cs.LG, math.NA

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂØºÊï∞‰ø°ÊÅØÂÇÖÈáåÂè∂Á•ûÁªèÁÆóÂ≠êÔºåÈÄöËøáËÅîÂêà‰ºòÂåñËæìÂá∫ÂíåÂØºÊï∞Ê†∑Êú¨ÔºåÊèêÂçáPDEÁ∫¶Êùü‰ºòÂåñÁöÑÁ≤æÂ∫¶ÂíåÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **Âº∫ÂåñÂ≠¶‰π†**

**ÂÖ≥ÈîÆËØç**: `ÂÇÖÈáåÂè∂Á•ûÁªèÁÆóÂ≠ê` `ÂØºÊï∞‰ø°ÊÅØÂ≠¶‰π†` `PDEÁ∫¶Êùü‰ºòÂåñ` `ÈÄöÁî®Ëøë‰ººÁêÜËÆ∫` `Fr√©chetÂØºÊï∞` `Ê†∑Êú¨Â§çÊùÇÂ∫¶` `Â§öÂàÜËæ®ÁéáËÆ≠ÁªÉ` `Êó†ÈôêÁª¥ÈÄÜÈóÆÈ¢ò`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂÇÖÈáåÂè∂Á•ûÁªèÁÆóÂ≠êÂú®PDEÁ∫¶Êùü‰ºòÂåñ‰∏≠ÔºåÁî±‰∫éÁº∫‰πèÁ≤æÁ°ÆÂØºÊï∞‰ø°ÊÅØÔºåÂØºËá¥‰ª£ÁêÜÊ®°ÂûãÈ©±Âä®‰ºòÂåñÊó∂Á≤æÂ∫¶‰∏çË∂≥„ÄÇ
2. ÊèêÂá∫ÂØºÊï∞‰ø°ÊÅØÂÇÖÈáåÂè∂Á•ûÁªèÁÆóÂ≠êÔºåÈÄöËøáËÅîÂêàËÆ≠ÁªÉËæìÂá∫ÂíåFr√©chetÂØºÊï∞Ê†∑Êú¨ÔºåÂÆûÁé∞ÁÆóÂ≠êÂíåÂØºÊï∞ÁöÑÂêåÊó∂È´òÁ≤æÂ∫¶Ëøë‰ºº„ÄÇ
3. ÂÆûÈ™åÊòæÁ§∫DIFNOÂú®‰ΩéÊ†∑Êú¨Èáè‰∏ãÊòæËëóÊèêÂçá‰ºòÂåñÁ≤æÂ∫¶ÔºåÂπ∂Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÔºåÈ™åËØÅ‰∫ÜÂÖ∂È´òÊïàÊÄßÂíåÈÄöÁî®Ëøë‰ººËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Êñá‰ªãÁªç‰∫ÜÂØºÊï∞‰ø°ÊÅØÂÇÖÈáåÂè∂Á•ûÁªèÁÆóÂ≠êÁöÑËøë‰ººÁêÜËÆ∫ÂíåÈ´òÊïàËÆ≠ÁªÉÊñπÊ≥ïÔºåÂ∫îÁî®‰∫éÂÅèÂæÆÂàÜÊñπÁ®ãÁ∫¶Êùü‰ºòÂåñ„ÄÇDIFNOÊòØ‰∏ÄÁßçÂÇÖÈáåÂè∂Á•ûÁªèÁÆóÂ≠êÔºåÈÄöËøáÊúÄÂ∞èÂåñÂÖ∂Âú®È´ò‰øùÁúüÁÆóÂ≠êËæìÂá∫ÂíåFr√©chetÂØºÊï∞Ê†∑Êú¨‰∏äÁöÑÈ¢ÑÊµãËØØÂ∑ÆËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄå‰∏ç‰ªÖËÉΩÁ¥ßÂØÜÊ®°ÊãüÈ´ò‰øùÁúüÁÆóÂ≠êÁöÑÂìçÂ∫îÔºåËøòËÉΩÊ®°ÊãüÂÖ∂ÊïèÊÑüÊÄß„ÄÇ‰∏∫ËØÅÊòéDIFNO‰ºò‰∫é‰º†ÁªüFNO‰Ωú‰∏∫‰ª£ÁêÜÊ®°ÂûãÔºåÊàë‰ª¨ÊåáÂá∫Á≤æÁ°ÆÁöÑ‰ª£ÁêÜÈ©±Âä®PDEÁ∫¶Êùü‰ºòÂåñÈúÄË¶ÅÁ≤æÁ°ÆÁöÑ‰ª£ÁêÜFr√©chetÂØºÊï∞„ÄÇÂØπ‰∫éËøûÁª≠ÂèØÂæÆÁÆóÂ≠êÔºåÊàë‰ª¨Âª∫Á´ã‰∫ÜÔºàiÔºâFNOÂèäÂÖ∂Fr√©chetÂØºÊï∞Âú®Á¥ßÈõÜ‰∏äÁöÑÂêåÊó∂ÈÄöÁî®Ëøë‰ººÊÄßÔºå‰ª•ÂèäÔºàiiÔºâFNOÂú®ÂÖ∑ÊúâÊó†ÁïåÊîØÊåÅÁöÑËæìÂÖ•ÊµãÂ∫¶ÁöÑÂä†ÊùÉSobolevÁ©∫Èó¥‰∏≠ÁöÑÈÄöÁî®Ëøë‰ººÊÄß„ÄÇËøô‰∫õÁêÜËÆ∫ÁªìÊûúÈ™åËØÅ‰∫ÜFNOÂú®Á≤æÁ°ÆÂØºÊï∞‰ø°ÊÅØÁÆóÂ≠êÂ≠¶‰π†ÂíåÁ≤æÁ°ÆÊ±ÇËß£PDEÁ∫¶Êùü‰ºòÂåñÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰ΩøÁî®ÈôçÁª¥ÂíåÂ§öÂàÜËæ®ÁéáÊäÄÊúØÁöÑÈ´òÊïàËÆ≠ÁªÉÊñπÊ°àÔºåÊòæËëóÈôç‰Ωé‰∫ÜFr√©chetÂØºÊï∞Â≠¶‰π†ÁöÑÂÜÖÂ≠òÂíåËÆ°ÁÆóÊàêÊú¨„ÄÇÂú®ÈùûÁ∫øÊÄßÊâ©Êï£-ÂèçÂ∫î„ÄÅHelmholtzÂíåNavier-StokesÊñπÁ®ã‰∏äÁöÑÊï∞ÂÄºÂÆûÈ™åË°®ÊòéÔºåDIFNOÂú®ÁÆóÂ≠êÂ≠¶‰π†ÂíåÊ±ÇËß£Êó†ÈôêÁª¥PDEÁ∫¶ÊùüÈÄÜÈóÆÈ¢òÁöÑÊ†∑Êú¨Â§çÊùÇÂ∫¶ÊñπÈù¢Ë°®Áé∞‰ºòË∂äÔºåËÉΩÂú®‰ΩéËÆ≠ÁªÉÊ†∑Êú¨Èáè‰∏ãÂÆûÁé∞È´òÁ≤æÂ∫¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

DIFNOÂü∫‰∫éÂÇÖÈáåÂè∂Á•ûÁªèÁÆóÂ≠êÊ°ÜÊû∂ÔºåÈÄöËøáÊúÄÂ∞èÂåñÈ´ò‰øùÁúüÁÆóÂ≠êÁöÑËæìÂá∫ÂíåFr√©chetÂØºÊï∞Ê†∑Êú¨ÁöÑËÅîÂêàÊçüÂ§±ÂáΩÊï∞ËøõË°åËÆ≠ÁªÉ„ÄÇÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•ÂØºÊï∞‰ø°ÊÅØÂ≠¶‰π†ÔºåÂà©Áî®ÈôçÁª¥ÂíåÂ§öÂàÜËæ®ÁéáÊäÄÊúØ‰ºòÂåñËÆ≠ÁªÉËøáÁ®ãÔºåÂáèÂ∞ëÂÜÖÂ≠òÂíåËÆ°ÁÆóÂºÄÈîÄ„ÄÇ‰∏éÁé∞ÊúâFNOÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÔºåDIFNO‰∏ç‰ªÖÂ≠¶‰π†ÁÆóÂ≠êÊò†Â∞ÑÔºåËøòÂ≠¶‰π†ÂÖ∂ÂØºÊï∞Ôºå‰ªéËÄåÂú®PDEÁ∫¶Êùü‰ºòÂåñ‰∏≠Êèê‰æõÊõ¥Á≤æÁ°ÆÁöÑÊïèÊÑüÊÄß‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Êï∞ÂÄºÂÆûÈ™åË°®ÊòéÔºåDIFNOÂú®ÈùûÁ∫øÊÄßÊâ©Êï£-ÂèçÂ∫î„ÄÅHelmholtzÂíåNavier-StokesÊñπÁ®ã‰∏äÔºåÁõ∏ÊØî‰º†ÁªüFNOÔºåÊ†∑Êú¨Â§çÊùÇÂ∫¶ÊòæËëóÈôç‰ΩéÔºåËÉΩÂú®Â∞ëÈáèËÆ≠ÁªÉÊ†∑Êú¨‰∏ãÂÆûÁé∞È´òÁ≤æÂ∫¶‰ºòÂåñÔºåÈ™åËØÅ‰∫ÜÂÖ∂È´òÊïàÊÄßÂíåÁêÜËÆ∫‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂‰∏ªË¶ÅÂ∫îÁî®‰∫éÂÅèÂæÆÂàÜÊñπÁ®ãÁ∫¶Êùü‰ºòÂåñÈóÆÈ¢òÔºåÂ¶ÇÈùûÁ∫øÊÄßÊâ©Êï£-ÂèçÂ∫î„ÄÅHelmholtzÂíåNavier-StokesÊñπÁ®ãÁöÑÈÄÜÈóÆÈ¢òÊ±ÇËß£ÔºåÂú®Â∑•Á®ãÂíåÁßëÂ≠¶ËÆ°ÁÆó‰∏≠ÂÖ∑ÊúâÂπøÊ≥õ‰ª∑ÂÄºÔºåÂèØÊèêÂçá‰ºòÂåñÊïàÁéáÂíåÁ≤æÂ∫¶„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fr√©chet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fr√©chet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fr√©chet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fr√©chet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.

