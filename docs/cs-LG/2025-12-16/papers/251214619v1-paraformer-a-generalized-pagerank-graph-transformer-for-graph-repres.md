---
layout: default
title: ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning
---

# ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14619" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14619v1</a>
  <a href="https://arxiv.org/pdf/2512.14619.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14619v1" onclick="toggleFavorite(this, '2512.14619v1', 'ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chaohao Yuan, Zhenjie Song, Ercan Engin Kuruoglu, Kangfei Zhao, Yang Liu, Deli Zhao, Hong Cheng, Yu Rong

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted by WSDM 2026

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/chaohaoyuan/ParaFormer)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºParaFormerï¼Œä¸€ç§åŸºäºPageRankå¢å¼ºçš„å›¾Transformerï¼Œç¼“è§£å›¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„è¿‡å¹³æ»‘é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å›¾ç¥ç»ç½‘ç»œ` `å›¾Transformer` `PageRank` `è¿‡å¹³æ»‘` `å›¾è¡¨ç¤ºå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ·±åº¦å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å­˜åœ¨è¿‡å¹³æ»‘é—®é¢˜ï¼Œå¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºéš¾ä»¥åŒºåˆ†ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. ParaFormeré€šè¿‡å¼•å…¥PageRankå¢å¼ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¨¡ä»¿æ·±åº¦Transformerçš„è¡Œä¸ºï¼Œç¼“è§£è¿‡å¹³æ»‘é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒParaFormeråœ¨èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾Transformer (GTs) ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„å›¾å­¦ä¹ å·¥å…·ï¼Œåˆ©ç”¨å…¶å…¨è¿æ¥ç‰¹æ€§æœ‰æ•ˆåœ°æ•è·å…¨å±€ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³æ·±åº¦GNNä¸­çš„è¿‡å¹³æ»‘é—®é¢˜ï¼Œæœ€åˆå¼•å…¥äº†å…¨å±€æ³¨æ„åŠ›ï¼Œä»è€Œæ¶ˆé™¤äº†ä½¿ç”¨æ·±åº¦GNNçš„å¿…è¦æ€§ã€‚ç„¶è€Œï¼Œé€šè¿‡å®è¯å’Œç†è®ºåˆ†æï¼Œæˆ‘ä»¬éªŒè¯äº†å¼•å…¥çš„å…¨å±€æ³¨æ„åŠ›è¡¨ç°å‡ºä¸¥é‡çš„è¿‡å¹³æ»‘ç°è±¡ï¼Œç”±äºå…¶å›ºæœ‰çš„ä½é€šæ»¤æ³¢ç‰¹æ€§ï¼Œå¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºå˜å¾—éš¾ä»¥åŒºåˆ†ã€‚è¿™ç§æ•ˆåº”ç”šè‡³æ¯”åœ¨GNNä¸­è§‚å¯Ÿåˆ°çš„æ›´å¼ºã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PageRank Transformer (ParaFormer)ï¼Œå®ƒå…·æœ‰PageRankå¢å¼ºçš„æ³¨æ„åŠ›æ¨¡å—ï¼Œæ—¨åœ¨æ¨¡ä»¿æ·±åº¦Transformerçš„è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨ç†è®ºä¸Šå’Œå®éªŒä¸Šè¯æ˜äº†ParaFormeré€šè¿‡å……å½“è‡ªé€‚åº”é€šæ»¤æ³¢å™¨æ¥ç¼“è§£è¿‡å¹³æ»‘ã€‚å®éªŒè¡¨æ˜ï¼ŒParaFormeråœ¨æ•°åƒåˆ°æ•°ç™¾ä¸‡ä¸ªèŠ‚ç‚¹çš„11ä¸ªæ•°æ®é›†ä¸Šçš„èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å›¾ç¥ç»ç½‘ç»œä¸­ç”±äºå…¨å±€æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥è€Œå¯¼è‡´çš„è¿‡å¹³æ»‘é—®é¢˜ã€‚ç°æœ‰çš„å›¾Transformerè™½ç„¶èƒ½å¤Ÿæ•è·å…¨å±€ä¿¡æ¯ï¼Œä½†å…¶å…¨å±€æ³¨æ„åŠ›æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä½é€šæ»¤æ³¢å™¨ï¼Œå¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºè¶‹äºä¸€è‡´ï¼Œéš¾ä»¥åŒºåˆ†ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ç§è¿‡å¹³æ»‘ç°è±¡ç”šè‡³æ¯”ä¼ ç»ŸGNNæ›´ä¸¥é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥PageRankç®—æ³•æ¥å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¼ é€’ä¿¡æ¯ï¼Œä»è€Œç¼“è§£è¿‡å¹³æ»‘é—®é¢˜ã€‚PageRankç®—æ³•å¯ä»¥æ ¹æ®èŠ‚ç‚¹çš„é‡è¦æ€§æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä½¿å¾—é‡è¦çš„èŠ‚ç‚¹èƒ½å¤Ÿå¯¹å…¶ä»–èŠ‚ç‚¹äº§ç”Ÿæ›´å¤§çš„å½±å“ï¼Œä»è€Œä¿æŒèŠ‚ç‚¹è¡¨ç¤ºçš„å¤šæ ·æ€§ã€‚è¿™ç§è®¾è®¡æ¨¡ä»¿äº†æ·±åº¦Transformerçš„è¡Œä¸ºï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ•è·å›¾çš„ç»“æ„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šParaFormerçš„æ•´ä½“æ¶æ„åŸºäºTransformerï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼šè¾“å…¥åµŒå…¥å±‚ã€PageRankå¢å¼ºçš„æ³¨æ„åŠ›æ¨¡å—ã€å‰é¦ˆç¥ç»ç½‘ç»œå’Œè¾“å‡ºå±‚ã€‚è¾“å…¥åµŒå…¥å±‚å°†èŠ‚ç‚¹ç‰¹å¾è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚PageRankå¢å¼ºçš„æ³¨æ„åŠ›æ¨¡å—æ˜¯æ ¸å¿ƒæ¨¡å—ï¼Œå®ƒåˆ©ç”¨PageRankç®—æ³•è®¡ç®—èŠ‚ç‚¹çš„é‡è¦æ€§ï¼Œå¹¶å°†å…¶èå…¥åˆ°æ³¨æ„åŠ›æƒé‡ä¸­ã€‚å‰é¦ˆç¥ç»ç½‘ç»œç”¨äºè¿›ä¸€æ­¥å¤„ç†èŠ‚ç‚¹è¡¨ç¤ºã€‚è¾“å‡ºå±‚æ ¹æ®ä»»åŠ¡ç±»å‹è¾“å‡ºèŠ‚ç‚¹åˆ†ç±»æˆ–å›¾åˆ†ç±»ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šParaFormerçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†PageRankå¢å¼ºçš„æ³¨æ„åŠ›æ¨¡å—ã€‚ä¸ä¼ ç»Ÿçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿæ ¹æ®èŠ‚ç‚¹çš„é‡è¦æ€§è‡ªé€‚åº”åœ°è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œç¼“è§£è¿‡å¹³æ»‘é—®é¢˜ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•è·å›¾çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¿æŒèŠ‚ç‚¹è¡¨ç¤ºçš„å¤šæ ·æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šPageRankå¢å¼ºçš„æ³¨æ„åŠ›æ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•å°†PageRankå€¼èå…¥åˆ°æ³¨æ„åŠ›æƒé‡ä¸­ã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŠ æƒå¹³å‡çš„æ–¹å¼ï¼Œå°†PageRankå€¼ä½œä¸ºæƒé‡ï¼Œå¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡Œè°ƒæ•´ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºèŠ‚ç‚¹iå’ŒèŠ‚ç‚¹jï¼Œå…¶æ³¨æ„åŠ›æƒé‡ä¸ºï¼šattention(i,j) = softmax(Q_i * K_j^T + alpha * PageRank(i))ï¼Œå…¶ä¸­Q_iå’ŒK_jåˆ†åˆ«æ˜¯èŠ‚ç‚¹iå’ŒèŠ‚ç‚¹jçš„æŸ¥è¯¢å‘é‡å’Œé”®å‘é‡ï¼Œalphaæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ï¼Œç”¨äºæ§åˆ¶PageRankå€¼çš„å½±å“ç¨‹åº¦ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºèŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒParaFormeråœ¨11ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒParaFormeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å›¾ç¥ç»ç½‘ç»œå’Œå›¾Transformeræ¨¡å‹ã€‚åœ¨å›¾åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒParaFormerä¹Ÿå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒParaFormerçš„æ€§èƒ½æå‡è¶…è¿‡äº†5%ã€‚è¿™äº›ç»“æœéªŒè¯äº†ParaFormerçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£è¿‡å¹³æ»‘é—®é¢˜ï¼Œå¹¶æé«˜å›¾è¡¨ç¤ºå­¦ä¹ çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ParaFormerå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºç¤¾äº¤ç½‘ç»œåˆ†æã€ç”Ÿç‰©ä¿¡æ¯å­¦ã€åŒ–å­¦ä¿¡æ¯å­¦ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ç¤¾äº¤ç½‘ç»œåˆ†æä¸­ï¼Œå¯ä»¥åˆ©ç”¨ParaFormerè¿›è¡Œç”¨æˆ·åˆ†ç±»ã€ç¤¾åŒºæ£€æµ‹ç­‰ä»»åŠ¡ã€‚åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­ï¼Œå¯ä»¥åˆ©ç”¨ParaFormerè¿›è¡Œè›‹ç™½è´¨ç»“æ„é¢„æµ‹ã€è¯ç‰©å‘ç°ç­‰ä»»åŠ¡ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæé«˜äº†å›¾è¡¨ç¤ºå­¦ä¹ çš„æ€§èƒ½ï¼Œä¸ºè§£å†³å®é™…é—®é¢˜æä¾›äº†æ›´æœ‰æ•ˆçš„å·¥å…·ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶ParaFormeråœ¨æ›´å¤§è§„æ¨¡å›¾æ•°æ®ä¸Šçš„åº”ç”¨ï¼Œå¹¶æ¢ç´¢å…¶ä¸å…¶ä»–å›¾å­¦ä¹ æŠ€æœ¯çš„ç»“åˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

