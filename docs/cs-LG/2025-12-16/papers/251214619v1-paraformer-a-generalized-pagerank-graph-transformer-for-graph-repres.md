---
layout: default
title: ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning
---

# ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning

**arXiv**: [2512.14619v1](https://arxiv.org/abs/2512.14619) | [PDF](https://arxiv.org/pdf/2512.14619.pdf)

**ä½œè€…**: Chaohao Yuan, Zhenjie Song, Ercan Engin Kuruoglu, Kangfei Zhao, Yang Liu, Deli Zhao, Hong Cheng, Yu Rong

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted by WSDM 2026

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/chaohaoyuan/ParaFormer)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºParaFormerï¼Œä¸€ç§åŸºäºŽPageRankå¢žå¼ºçš„å›¾Transformerï¼Œç¼“è§£å›¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„è¿‡å¹³æ»‘é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å…·èº«æ™ºèƒ½ä¸Žè¡¨å¾å­¦ä¹  (Embodied AI & Representation)** **3Dæ„ŸçŸ¥ä¸ŽçŠ¶æ€ä¼°è®¡ (Perception & State Est)**

**å…³é”®è¯**: `å›¾ç¥žç»ç½‘ç»œ` `å›¾Transformer` `è¿‡å¹³æ»‘` `PageRank` `æ³¨æ„åŠ›æœºåˆ¶` `å›¾è¡¨ç¤ºå­¦ä¹ ` `èŠ‚ç‚¹åˆ†ç±»` `å›¾åˆ†ç±»`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ·±åº¦GNNå’Œå›¾Transformerå­˜åœ¨è¿‡å¹³æ»‘é—®é¢˜ï¼Œå¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºåŒºåˆ†åº¦é™ä½Žï¼Œé™åˆ¶äº†æ¨¡åž‹æ€§èƒ½ã€‚
2. ParaFormeré€šè¿‡å¼•å…¥PageRankå¢žå¼ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¨¡æ‹Ÿæ·±åº¦Transformerçš„è¡Œä¸ºï¼Œç¼“è§£è¿‡å¹³æ»‘é—®é¢˜ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒParaFormeråœ¨èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾Transformer (GTs) ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„å›¾å­¦ä¹ å·¥å…·ï¼Œåˆ©ç”¨å…¶å…¨è¿žæŽ¥ç‰¹æ€§æœ‰æ•ˆåœ°æ•èŽ·å…¨å±€ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³æ·±åº¦GNNä¸­çš„è¿‡å¹³æ»‘é—®é¢˜ï¼Œæœ€åˆå¼•å…¥äº†å…¨å±€æ³¨æ„åŠ›ï¼Œä»Žè€Œæ¶ˆé™¤äº†ä½¿ç”¨æ·±åº¦GNNçš„å¿…è¦æ€§ã€‚ç„¶è€Œï¼Œé€šè¿‡å®žè¯å’Œç†è®ºåˆ†æžï¼Œæˆ‘ä»¬éªŒè¯äº†å¼•å…¥çš„å…¨å±€æ³¨æ„åŠ›è¡¨çŽ°å‡ºä¸¥é‡çš„è¿‡å¹³æ»‘çŽ°è±¡ï¼Œç”±äºŽå…¶å›ºæœ‰çš„ä½Žé€šæ»¤æ³¢ç‰¹æ€§ï¼Œå¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºå˜å¾—éš¾ä»¥åŒºåˆ†ã€‚è¿™ç§æ•ˆåº”ç”šè‡³æ¯”åœ¨GNNä¸­è§‚å¯Ÿåˆ°çš„æ›´å¼ºã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PageRank Transformer (ParaFormer)ï¼Œå®ƒå…·æœ‰PageRankå¢žå¼ºçš„æ³¨æ„åŠ›æ¨¡å—ï¼Œæ—¨åœ¨æ¨¡ä»¿æ·±åº¦Transformerçš„è¡Œä¸ºã€‚æˆ‘ä»¬ä»Žç†è®ºä¸Šå’Œå®žéªŒä¸Šè¯æ˜Žäº†ParaFormeré€šè¿‡å……å½“è‡ªé€‚åº”é€šæ»¤æ³¢å™¨æ¥ç¼“è§£è¿‡å¹³æ»‘ã€‚å®žéªŒè¡¨æ˜Žï¼ŒParaFormeråœ¨æ•°åƒåˆ°æ•°ç™¾ä¸‡ä¸ªèŠ‚ç‚¹çš„11ä¸ªæ•°æ®é›†ä¸Šçš„èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å›¾ç¥žç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œå›¾Transformerï¼ˆGTsï¼‰ä¸­æ™®éå­˜åœ¨çš„è¿‡å¹³æ»‘é—®é¢˜ã€‚è¿‡å¹³æ»‘å¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºå˜å¾—éš¾ä»¥åŒºåˆ†ï¼Œä»Žè€Œé™åˆ¶äº†æ¨¡åž‹åœ¨å›¾è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚çŽ°æœ‰çš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶è™½ç„¶è¯•å›¾è§£å†³è¿‡å¹³æ»‘ï¼Œä½†å®žé™…ä¸ŠåŠ å‰§äº†è¿™ä¸€é—®é¢˜ï¼Œè¡¨çŽ°å‡ºæ¯”ä¼ ç»ŸGNNæ›´å¼ºçš„ä½Žé€šæ»¤æ³¢ç‰¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šParaFormerçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥PageRankå¢žå¼ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ èŠ‚ç‚¹ä¹‹é—´çš„é‡è¦æ€§ï¼Œä»Žè€Œç¼“è§£è¿‡å¹³æ»‘ã€‚PageRankç®—æ³•èƒ½å¤Ÿè¡¡é‡èŠ‚ç‚¹åœ¨å›¾ä¸­çš„é‡è¦æ€§ï¼Œå°†å…¶èžå…¥æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ä½¿æ¨¡åž‹æ›´åŠ å…³æ³¨é‡è¦çš„èŠ‚ç‚¹ï¼Œå‡å°‘å¯¹ä¸é‡è¦èŠ‚ç‚¹çš„è¿‡åº¦å¹³æ»‘ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šParaFormerçš„æ•´ä½“æž¶æž„åŸºäºŽTransformerï¼Œä½†å…¶å…³é”®åœ¨äºŽPageRankå¢žå¼ºçš„æ³¨æ„åŠ›æ¨¡å—ã€‚è¯¥æ¨¡å—é¦–å…ˆè®¡ç®—èŠ‚ç‚¹ä¹‹é—´çš„PageRankå€¼ï¼Œç„¶åŽå°†PageRankå€¼èžå…¥åˆ°æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—ä¸­ã€‚å…·ä½“æ¥è¯´ï¼ŒPageRankå€¼è¢«ç”¨æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä½¿å¾—ä¸Žé‡è¦èŠ‚ç‚¹ç›¸å…³çš„æ³¨æ„åŠ›æƒé‡æ›´é«˜ï¼Œä»Žè€Œå‡å°‘å¯¹ä¸é‡è¦èŠ‚ç‚¹çš„è¿‡åº¦å¹³æ»‘ã€‚æ•´ä¸ªæ¨¡åž‹å¯ä»¥ç«¯åˆ°ç«¯åœ°è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šParaFormerçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†PageRankç®—æ³•ä¸ŽTransformerçš„æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆã€‚è¿™ç§ç»“åˆä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ èŠ‚ç‚¹çš„é‡è¦æ€§ï¼Œä»Žè€Œæœ‰æ•ˆåœ°ç¼“è§£è¿‡å¹³æ»‘é—®é¢˜ã€‚ä¸Žä¼ ç»Ÿçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒParaFormerèƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒèŠ‚ç‚¹è¡¨ç¤ºçš„åŒºåˆ†åº¦ï¼Œä»Žè€Œæé«˜æ¨¡åž‹åœ¨å›¾è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šParaFormerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) PageRankå€¼çš„è®¡ç®—æ–¹æ³•ï¼šè®ºæ–‡é‡‡ç”¨äº†æ ‡å‡†çš„PageRankç®—æ³•ï¼Œå¹¶å¯¹PageRankå€¼è¿›è¡Œäº†å½’ä¸€åŒ–å¤„ç†ã€‚2) PageRankå€¼èžå…¥æ³¨æ„åŠ›æƒé‡çš„å…·ä½“æ–¹å¼ï¼šè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŠ æƒçš„æ–¹å¼ï¼Œå°†PageRankå€¼ä¸Žæ³¨æ„åŠ›æƒé‡ç›¸åŠ ã€‚3) æ¨¡åž‹çš„è®­ç»ƒæ–¹å¼ï¼šè®ºæ–‡é‡‡ç”¨äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹å¼ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

ParaFormeråœ¨11ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®žéªŒï¼ŒåŒ…æ‹¬èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒParaFormeråœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒParaFormerçš„å¹³å‡å‡†ç¡®çŽ‡æ¯”åŸºçº¿æ¨¡åž‹æé«˜äº†5%ä»¥ä¸Šã€‚åœ¨å›¾åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒParaFormerçš„å¹³å‡å‡†ç¡®çŽ‡æ¯”åŸºçº¿æ¨¡åž‹æé«˜äº†3%ä»¥ä¸Šã€‚è¿™äº›ç»“æžœéªŒè¯äº†ParaFormerçš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

ParaFormerå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºŽç¤¾äº¤ç½‘ç»œåˆ†æžã€çŸ¥è¯†å›¾è°±æŽ¨ç†ã€ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ç¤¾äº¤ç½‘ç»œåˆ†æžä¸­ï¼ŒParaFormerå¯ä»¥ç”¨äºŽè¯†åˆ«å…³é”®ç”¨æˆ·å’Œç¤¾åŒºç»“æž„ï¼›åœ¨çŸ¥è¯†å›¾è°±æŽ¨ç†ä¸­ï¼ŒParaFormerå¯ä»¥ç”¨äºŽé¢„æµ‹å®žä½“ä¹‹é—´çš„å…³ç³»ï¼›åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­ï¼ŒParaFormerå¯ä»¥ç”¨äºŽé¢„æµ‹è›‹ç™½è´¨çš„åŠŸèƒ½ã€‚è¯¥ç ”ç©¶çš„å®žé™…ä»·å€¼åœ¨äºŽæé«˜äº†å›¾è¡¨ç¤ºå­¦ä¹ çš„æ€§èƒ½ï¼Œä¸ºè§£å†³å®žé™…é—®é¢˜æä¾›äº†æ›´æœ‰æ•ˆçš„å·¥å…·ã€‚æœªæ¥ï¼ŒParaFormerå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤„ç†æ›´å¤§è§„æ¨¡çš„å›¾æ•°æ®ï¼Œå¹¶ä¸Žå…¶ä»–å›¾å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

