---
layout: default
title: HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network
---

# HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09767" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09767</a>
  <a href="https://arxiv.org/pdf/2510.09767.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09767" onclick="toggleFavorite(this, '2510.09767', 'HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Lu, Ziyun Zou, Belal Alsinglawi, Islam Al-Qudah, Izzat Alsmadi, Feilong Tang, Pengfei Jiao, Shoaib Jameel, Imran Razzak

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHeSRNï¼šä¸€ç§åŸºäºSlot-Aware Retentiveç½‘ç»œçš„å¼‚æ„å›¾è¡¨ç¤ºå­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼‚æ„å›¾è¡¨ç¤ºå­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `Retentiveç½‘ç»œ` `Slot-Awareæœºåˆ¶` `èŠ‚ç‚¹åˆ†ç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾Transformeræ–¹æ³•åœ¨å¼‚æ„å›¾ä¸Šå­˜åœ¨è®¡ç®—å¤æ‚åº¦é«˜ã€éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡å¼‚æ„è¯­ä¹‰ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. HeSRNé€šè¿‡Slot-Awareç»“æ„ç¼–ç å™¨è§£è€¦èŠ‚ç‚¹ç±»å‹è¯­ä¹‰ï¼Œå¹¶ä½¿ç”¨Retentiveç¼–ç å™¨ä»¥çº¿æ€§å¤æ‚åº¦å»ºæ¨¡ç»“æ„å’Œä¸Šä¸‹æ–‡ä¾èµ–ï¼Œæå‡æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHeSRNåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰å¼‚æ„å›¾ç¥ç»ç½‘ç»œå’Œå›¾Transformerï¼Œå¹¶åœ¨è®¡ç®—å¤æ‚åº¦ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼‚æ„å›¾è¡¨ç¤ºå­¦ä¹ æ–¹æ³•HeSRNï¼Œå³å¼‚æ„Slot-Aware Retentiveç½‘ç»œï¼Œæ—¨åœ¨é«˜æ•ˆä¸”å¯Œæœ‰è¡¨ç°åŠ›åœ°å­¦ä¹ å¼‚æ„å›¾è¡¨ç¤ºã€‚é’ˆå¯¹å›¾Transformeråœ¨å¼‚æ„å›¾ä¸Šçš„è®¡ç®—å¤æ‚åº¦å’Œè¯­ä¹‰å»ºæ¨¡é—®é¢˜ï¼ŒHeSRNå¼•å…¥äº†Slot-Awareç»“æ„ç¼–ç å™¨ï¼Œé€šè¿‡å°†å¼‚æ„ç‰¹å¾æŠ•å½±åˆ°ç‹¬ç«‹çš„Slotä¸­ï¼Œå¹¶é€šè¿‡Slotå½’ä¸€åŒ–å’ŒåŸºäºRetentiveçš„èåˆæ¥æ˜¾å¼åœ°è§£è€¦èŠ‚ç‚¹ç±»å‹è¯­ä¹‰ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£äº†å…ˆå‰åŸºäºTransformerçš„æ¨¡å‹ä¸­å¼ºåˆ¶ç‰¹å¾ç©ºé—´ç»Ÿä¸€å¼•èµ·çš„è¯­ä¹‰çº ç¼ ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ç”¨åŸºäºRetentiveçš„ç¼–ç å™¨å–ä»£äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥çº¿æ€§æ—¶é—´å¤æ‚åº¦å¯¹ç»“æ„å’Œä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚å¼‚æ„Retentiveç¼–ç å™¨è¿›ä¸€æ­¥ç”¨äºé€šè¿‡å¤šå°ºåº¦Retentiveå±‚è”åˆæ•è·å±€éƒ¨ç»“æ„ä¿¡å·å’Œå…¨å±€å¼‚æ„è¯­ä¹‰ã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œçš„å¼‚æ„å›¾æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHeSRNåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„å¼‚æ„å›¾ç¥ç»ç½‘ç»œå’Œå›¾TransformeråŸºçº¿ï¼Œä»¥æ˜¾è‘—é™ä½çš„è®¡ç®—å¤æ‚åº¦å®ç°äº†å“è¶Šçš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºTransformerçš„å›¾ç¥ç»ç½‘ç»œåœ¨å¤„ç†å¼‚æ„å›¾æ—¶ï¼Œç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦æ˜¯èŠ‚ç‚¹æ•°é‡çš„å¹³æ–¹çº§åˆ«ï¼Œå› æ­¤åœ¨å¤§è§„æ¨¡å›¾ä¸Šæ•ˆç‡ä½ä¸‹ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸å°†ä¸åŒç±»å‹çš„èŠ‚ç‚¹ç‰¹å¾å¼ºåˆ¶ç»Ÿä¸€åˆ°åŒä¸€ä¸ªç‰¹å¾ç©ºé—´ï¼Œå¯¼è‡´è¯­ä¹‰çº ç¼ ï¼Œå½±å“è¡¨ç¤ºå­¦ä¹ çš„è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHeSRNçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥Slot-Awareæœºåˆ¶æ¥æ˜¾å¼åœ°è§£è€¦ä¸åŒèŠ‚ç‚¹ç±»å‹çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨Retentiveç½‘ç»œæ›¿ä»£è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡å°†å¼‚æ„ç‰¹å¾æŠ•å½±åˆ°ç‹¬ç«‹çš„Slotä¸­ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–å’Œèåˆï¼Œå¯ä»¥æœ‰æ•ˆç¼“è§£è¯­ä¹‰çº ç¼ é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHeSRNçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) Slot-Awareç»“æ„ç¼–ç å™¨ï¼šå°†å¼‚æ„èŠ‚ç‚¹ç‰¹å¾æŠ•å½±åˆ°ä¸åŒçš„Slotä¸­ï¼Œæ¯ä¸ªSlotå¯¹åº”ä¸€ç§èŠ‚ç‚¹ç±»å‹ã€‚2) Slotå½’ä¸€åŒ–ï¼šå¯¹æ¯ä¸ªSlotä¸­çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥å¯¹é½ä¸åŒSlotçš„åˆ†å¸ƒã€‚3) åŸºäºRetentiveçš„èåˆï¼šä½¿ç”¨Retentiveæœºåˆ¶èåˆä¸åŒSlotä¸­çš„ç‰¹å¾ï¼Œæ•æ‰èŠ‚ç‚¹ç±»å‹ä¹‹é—´çš„å…³ç³»ã€‚4) å¼‚æ„Retentiveç¼–ç å™¨ï¼šé€šè¿‡å¤šå°ºåº¦Retentiveå±‚ï¼Œè”åˆæ•è·å±€éƒ¨ç»“æ„ä¿¡å·å’Œå…¨å±€å¼‚æ„è¯­ä¹‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šHeSRNçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) Slot-Awareç»“æ„ç¼–ç å™¨ï¼Œå®ƒæ˜¾å¼åœ°è§£è€¦äº†èŠ‚ç‚¹ç±»å‹è¯­ä¹‰ï¼Œé¿å…äº†å¼ºåˆ¶ç‰¹å¾ç©ºé—´ç»Ÿä¸€å¸¦æ¥çš„è¯­ä¹‰çº ç¼ ã€‚2) ä½¿ç”¨Retentiveç½‘ç»œæ›¿ä»£è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è®¡ç®—å¤æ‚åº¦é™ä½åˆ°çº¿æ€§çº§åˆ«ï¼Œæé«˜äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚3) å¼‚æ„Retentiveç¼–ç å™¨ï¼Œèƒ½å¤ŸåŒæ—¶æ•æ‰å±€éƒ¨ç»“æ„ä¿¡å·å’Œå…¨å±€å¼‚æ„è¯­ä¹‰ã€‚

**å…³é”®è®¾è®¡**ï¼šSlot-Awareç»“æ„ç¼–ç å™¨ä½¿ç”¨çº¿æ€§æŠ•å½±å°†å¼‚æ„èŠ‚ç‚¹ç‰¹å¾æ˜ å°„åˆ°ä¸åŒçš„Slotä¸­ã€‚Slotå½’ä¸€åŒ–é‡‡ç”¨Layer Normalizationã€‚Retentiveç½‘ç»œä½¿ç”¨å¹¶è¡ŒåŒ–çš„chunk-wise recurrent è®¡ç®—æ–¹å¼ï¼ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚å¼‚æ„Retentiveç¼–ç å™¨å †å äº†å¤šä¸ªRetentiveå±‚ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„å°ºåº¦æ¥æ•æ‰ä¸åŒèŒƒå›´çš„ä¾èµ–å…³ç³»ã€‚æŸå¤±å‡½æ•°æ ¹æ®å…·ä½“çš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œé€‰æ‹©ï¼Œä¾‹å¦‚èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡é€šå¸¸ä½¿ç”¨äº¤å‰ç†µæŸå¤±ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png" alt="img_0" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHeSRNåœ¨å››ä¸ªçœŸå®ä¸–ç•Œçš„å¼‚æ„å›¾æ•°æ®é›†ä¸Šï¼Œç›¸æ¯”äºæœ€å…ˆè¿›çš„å¼‚æ„å›¾ç¥ç»ç½‘ç»œå’Œå›¾TransformeråŸºçº¿ï¼Œåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒHeSRNçš„å‡†ç¡®ç‡æé«˜äº†5%ä»¥ä¸Šï¼ŒåŒæ—¶è®¡ç®—å¤æ‚åº¦æ˜¾è‘—é™ä½ã€‚è¿™äº›ç»“æœéªŒè¯äº†HeSRNçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HeSRNå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ç¤¾äº¤ç½‘ç»œåˆ†æã€çŸ¥è¯†å›¾è°±æ¨ç†ã€ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºèŠ‚ç‚¹åˆ†ç±»ã€é“¾æ¥é¢„æµ‹ã€å›¾åˆ†ç±»ç­‰ä»»åŠ¡ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæé«˜äº†å¼‚æ„å›¾è¡¨ç¤ºå­¦ä¹ çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œä¸ºå¤§è§„æ¨¡å¼‚æ„å›¾æ•°æ®çš„åˆ†æå’Œåº”ç”¨æä¾›äº†æ–°çš„å·¥å…·ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢HeSRNåœ¨å…¶ä»–å›¾ç»“æ„æ•°æ®ä¸Šçš„åº”ç”¨ï¼Œå¹¶ç ”ç©¶å¦‚ä½•å°†å…¶ä¸å…¶ä»–æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥è§£å†³æ›´å¤æ‚çš„é—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Graph Transformers have recently achieved remarkable progress in graph representation learning by capturing long-range dependencies through self-attention. However, their quadratic computational complexity and inability to effectively model heterogeneous semantics severely limit their scalability and generalization on real-world heterogeneous graphs. To address these issues, we propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network for efficient and expressive heterogeneous graph representation learning. HeSRN introduces a slot-aware structure encoder that explicitly disentangles node-type semantics by projecting heterogeneous features into independent slots and aligning their distributions through slot normalization and retention-based fusion, effectively mitigating the semantic entanglement caused by forced feature-space unification in previous Transformer-based models. Furthermore, we replace the self-attention mechanism with a retention-based encoder, which models structural and contextual dependencies in linear time complexity while maintaining strong expressive power. A heterogeneous retentive encoder is further employed to jointly capture both local structural signals and global heterogeneous semantics through multi-scale retention layers. Extensive experiments on four real-world heterogeneous graph datasets demonstrate that HeSRN consistently outperforms state-of-the-art heterogeneous graph neural networks and Graph Transformer baselines on node classification tasks, achieving superior accuracy with significantly lower computational complexity.

