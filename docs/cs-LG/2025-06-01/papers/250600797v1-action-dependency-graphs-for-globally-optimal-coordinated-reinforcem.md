---
layout: default
title: Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning
---

# Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00797" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00797v1</a>
  <a href="https://arxiv.org/pdf/2506.00797.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00797v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00797v1', 'Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianglin Ding, Jingcheng Tang, Gangshan Jing

**åˆ†ç±»**: cs.LG, cs.AI, eess.SY, math.OC

**å‘å¸ƒæ—¥æœŸ**: 2025-06-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨ä½œä¾èµ–å›¾ä»¥è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å…¨å±€æœ€ä¼˜é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `åŠ¨ä½œä¾èµ–å›¾` `å…¨å±€æœ€ä¼˜` `ç­–ç•¥è¿­ä»£` `è®¡ç®—å¤æ‚æ€§` `åè°ƒæ§åˆ¶` `æ™ºèƒ½äº¤é€š` `æœºå™¨äººåä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªå›å½’åŠ¨ä½œä¾èµ–ç­–ç•¥åœ¨æ™ºèƒ½ä½“æ•°é‡å¢åŠ æ—¶å¯¼è‡´è®¡ç®—å¤æ‚åº¦æ˜¾è‘—ä¸Šå‡ï¼Œé™åˆ¶äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨ä½œä¾èµ–å›¾ï¼ˆADGï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°è¡¨ç¤ºæ™ºèƒ½ä½“é—´çš„åŠ¨ä½œä¾èµ–å…³ç³»ï¼Œä»è€Œå®ç°å…¨å±€æœ€ä¼˜ç­–ç•¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŠ¨ä½œä¾èµ–çš„ä¸ªä½“ç­–ç•¥ï¼Œç»“åˆç¯å¢ƒçŠ¶æ€å’Œå…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œè¿›è¡Œå†³ç­–ï¼Œå·²æˆä¸ºå®ç°å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰å…¨å±€æœ€ä¼˜çš„æœ‰å‰æ™¯çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®é€šå¸¸é‡‡ç”¨è‡ªå›å½’çš„åŠ¨ä½œä¾èµ–ç­–ç•¥ï¼Œä½¿å¾—æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ä¾èµ–äºæ‰€æœ‰å‰ç½®æ™ºèƒ½ä½“çš„åŠ¨ä½œï¼Œè¿™åœ¨æ™ºèƒ½ä½“æ•°é‡å¢åŠ æ—¶ä¼šå¯¼è‡´è®¡ç®—å¤æ‚åº¦æ˜¾è‘—ä¸Šå‡ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡è€ƒè™‘äº†ä¸€ç±»æ›´ä¸ºå¹¿æ³›çš„åŠ¨ä½œä¾èµ–ç­–ç•¥ï¼Œä¸å¿…éµå¾ªè‡ªå›å½’å½¢å¼ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨åŠ¨ä½œä¾èµ–å›¾ï¼ˆADGï¼‰æ¥å»ºæ¨¡æ™ºèƒ½ä½“é—´çš„åŠ¨ä½œä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬è¯æ˜äº†åœ¨ç”±åè°ƒå›¾ç»“æ„åŒ–çš„MARLé—®é¢˜ä¸­ï¼Œæ»¡è¶³ç‰¹å®šæ¡ä»¶çš„ç¨€ç–ADGçš„åŠ¨ä½œä¾èµ–ç­–ç•¥å¯ä»¥å®ç°å…¨å±€æœ€ä¼˜ã€‚åŸºäºæ­¤ç†è®ºåŸºç¡€ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å…·æœ‰å…¨å±€æœ€ä¼˜ä¿è¯çš„è¡¨æ ¼ç­–ç•¥è¿­ä»£ç®—æ³•ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ¡†æ¶æ•´åˆåˆ°å¤šä¸ªæœ€å…ˆè¿›çš„ç®—æ³•ä¸­ï¼Œå®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ›´ä¸€èˆ¬åœºæ™¯ä¸­çš„é²æ£’æ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­è‡ªå›å½’åŠ¨ä½œä¾èµ–ç­–ç•¥å¸¦æ¥çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ™ºèƒ½ä½“æ•°é‡å¢åŠ æ—¶ï¼Œç­–ç•¥çš„è®¡ç®—è´Ÿæ‹…æ˜¾è‘—åŠ é‡ï¼Œå½±å“äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨ä½œä¾èµ–å›¾ï¼ˆADGï¼‰æ¨¡å‹ï¼Œå…è®¸æ™ºèƒ½ä½“ä¹‹é—´çš„åŠ¨ä½œä¾èµ–å…³ç³»ä¸å†éµå¾ªè‡ªå›å½’å½¢å¼ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦å¹¶æé«˜ç­–ç•¥çš„çµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŠ¨ä½œä¾èµ–å›¾çš„æ„å»ºã€ç­–ç•¥çš„è®¾è®¡ä¸ä¼˜åŒ–ï¼Œä»¥åŠåŸºäºç¨€ç–ADGçš„å…¨å±€æœ€ä¼˜ä¿è¯çš„ç­–ç•¥è¿­ä»£ç®—æ³•ã€‚å…·ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆæ„å»ºADGï¼Œç„¶ååˆ©ç”¨è¯¥å›¾è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œæœ€åé€šè¿‡è¿­ä»£ç®—æ³•å®ç°å…¨å±€æœ€ä¼˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŠ¨ä½œä¾èµ–å›¾ï¼ˆADGï¼‰ï¼Œä½¿å¾—ç­–ç•¥è®¾è®¡ä¸å†å±€é™äºè‡ªå›å½’å½¢å¼ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä¿è¯å…¨å±€æœ€ä¼˜æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç¨€ç–ADGä»¥å‡å°‘è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®æ¥ä¼˜åŒ–ç­–ç•¥çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­ç›¸è¾ƒäºåŸºçº¿ç®—æ³•è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨å…¨å±€æœ€ä¼˜æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åè°ƒæ§åˆ¶ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿã€æœºå™¨äººç¾¤ä½“åä½œç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ™ºèƒ½ä½“é—´çš„å†³ç­–è¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—æå‡ç³»ç»Ÿçš„æ•´ä½“æ•ˆç‡å’Œæ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Action-dependent individual policies, which incorporate both environmental states and the actions of other agents in decision-making, have emerged as a promising paradigm for achieving global optimality in multi-agent reinforcement learning (MARL). However, the existing literature often adopts auto-regressive action-dependent policies, where each agent's policy depends on the actions of all preceding agents. This formulation incurs substantial computational complexity as the number of agents increases, thereby limiting scalability. In this work, we consider a more generalized class of action-dependent policies, which do not necessarily follow the auto-regressive form. We propose to use the `action dependency graph (ADG)' to model the inter-agent action dependencies. Within the context of MARL problems structured by coordination graphs, we prove that an action-dependent policy with a sparse ADG can achieve global optimality, provided the ADG satisfies specific conditions specified by the coordination graph. Building on this theoretical foundation, we develop a tabular policy iteration algorithm with guaranteed global optimality. Furthermore, we integrate our framework into several SOTA algorithms and conduct experiments in complex environments. The empirical results affirm the robustness and applicability of our approach in more general scenarios, underscoring its potential for broader MARL challenges.

