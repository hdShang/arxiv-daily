---
layout: default
title: Generalized Linear Markov Decision Process
---

# Generalized Linear Markov Decision Process

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00818" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00818v1</a>
  <a href="https://arxiv.org/pdf/2506.00818.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00818v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00818v1', 'Generalized Linear Markov Decision Process')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sinian Zhang, Kaicheng Zhang, Ziping Xu, Tianxi Cai, Doudou Zhou

**åˆ†ç±»**: stat.ML, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-01

**å¤‡æ³¨**: 34 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¹¿ä¹‰çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä»¥è§£å†³å¥–åŠ±ä¿¡å·éçº¿æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¹¿ä¹‰çº¿æ€§æ¨¡å‹` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å»ºæ¨¡` `ç¦»çº¿å­¦ä¹ ` `æ ·æœ¬æ•ˆç‡` `åŒ»ç–—åº”ç”¨` `ç”µå­å•†åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å‡è®¾è½¬ç§»åŠ¨æ€å’Œå¥–åŠ±å‡½æ•°åœ¨åŒä¸€ç‰¹å¾ç©ºé—´ä¸­æ˜¯çº¿æ€§çš„ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„å¹¿ä¹‰çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆGLMDPï¼‰é€šè¿‡ä½¿ç”¨å¹¿ä¹‰çº¿æ€§æ¨¡å‹æ¥å»ºæ¨¡å¥–åŠ±ï¼Œå…‹æœäº†çº¿æ€§å‡è®¾çš„å±€é™æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨å¥–åŠ±æ ‡ç­¾ç¨€ç¼ºçš„æƒ…å†µä¸‹è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€å’Œæ ·æœ¬æ•ˆç‡ï¼Œä½†å…¶å‡è®¾è¿‡äºä¸¥æ ¼ï¼Œé™åˆ¶äº†åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¹¿ä¹‰çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆGLMDPï¼‰ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMsï¼‰æ¥å»ºæ¨¡å¥–åŠ±ï¼ŒåŒæ—¶ä¿æŒçº¿æ€§è½¬ç§»åŠ¨æ€ã€‚æˆ‘ä»¬å»ºç«‹äº†GLMDPçš„è´å°”æ›¼å®Œå¤‡æ€§ï¼Œå¹¶å¼€å‘äº†ä¸¤ç§ç¦»çº¿RLç®—æ³•ï¼šå¹¿ä¹‰æ‚²è§‚ä»·å€¼è¿­ä»£ï¼ˆGPEVIï¼‰å’Œä¸€ç§åˆ©ç”¨æ ‡è®°ä¸æœªæ ‡è®°è½¨è¿¹çš„åŠç›‘ç£å˜ä½“ï¼ˆSS-GPEVIï¼‰ã€‚è¿™äº›ç®—æ³•åœ¨æ”¿ç­–æ¬¡ä¼˜æ€§ä¸Šæä¾›äº†ç†è®ºä¿è¯ï¼Œå¹¶åœ¨å¥–åŠ±æ ‡ç­¾ç¨€ç¼ºçš„æƒ…å†µä¸‹å±•ç¤ºäº†æ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åœ¨é¢å¯¹éçº¿æ€§æˆ–ç¦»æ•£å¥–åŠ±ç»“æ„æ—¶çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å‡è®¾è½¬ç§»åŠ¨æ€å’Œå¥–åŠ±å‡½æ•°å‡ä¸ºçº¿æ€§ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡å¤æ‚çš„å¥–åŠ±ä¿¡å·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¹¿ä¹‰çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆGLMDPï¼‰ï¼Œé€šè¿‡å¼•å…¥å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMsï¼‰æ¥å»ºæ¨¡å¥–åŠ±ï¼ŒåŒæ—¶ä¿æŒçº¿æ€§è½¬ç§»åŠ¨æ€ï¼Œä»è€Œæ‰©å±•äº†çº¿æ€§MDPçš„é€‚ç”¨èŒƒå›´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGLMDPæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯åŸºäºå¹¿ä¹‰çº¿æ€§æ¨¡å‹çš„å¥–åŠ±å»ºæ¨¡ï¼ŒäºŒæ˜¯çº¿æ€§è½¬ç§»åŠ¨æ€çš„ä¿æŒã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å—çš„ç»“åˆï¼ŒGLMDPèƒ½å¤Ÿå¤„ç†éçº¿æ€§å¥–åŠ±çš„æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šGLMDPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¹¿ä¹‰çº¿æ€§æ¨¡å‹å¼•å…¥åˆ°å¥–åŠ±å»ºæ¨¡ä¸­ï¼Œçªç ´äº†ä¼ ç»Ÿçº¿æ€§å‡è®¾çš„é™åˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ›´å¤æ‚çš„å¥–åŠ±ç»“æ„ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGLMDPåœ¨ç†è®ºä¸Šæä¾›äº†æ›´å¼ºçš„çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼ŒGPEVIå’ŒSS-GPEVIåˆ†åˆ«é‡‡ç”¨äº†æ‚²è§‚ä»·å€¼è¿­ä»£å’ŒåŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œç¡®ä¿åœ¨æ ·æœ¬ç¨€ç¼ºçš„æƒ…å†µä¸‹ä»èƒ½æœ‰æ•ˆå­¦ä¹ ã€‚å…³é”®å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å‡è€ƒè™‘äº†å¥–åŠ±çš„éçº¿æ€§ç‰¹å¾ï¼Œä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„GPEVIå’ŒSS-GPEVIç®—æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçº¿æ€§MDPæ–¹æ³•ï¼Œå°¤å…¶åœ¨å¥–åŠ±æ ‡ç­¾ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œæ”¿ç­–æ¬¡ä¼˜æ€§å¾—åˆ°äº†ç†è®ºä¿è¯ï¼Œæå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

å¹¿ä¹‰çº¿æ€§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆGLMDPï¼‰åœ¨åŒ»ç–—å¥åº·å’Œç”µå­å•†åŠ¡ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ•°æ®å¾€å¾€ç¨€ç¼ºï¼Œå¥–åŠ±ä¿¡å·å¯èƒ½æ˜¯äºŒå…ƒæˆ–è®¡æ•°å€¼çš„ï¼ŒGLMDPèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è¿™äº›å¤æ‚çš„å¥–åŠ±ç»“æ„ï¼Œä»è€Œæå‡å†³ç­–è´¨é‡å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The linear Markov Decision Process (MDP) framework offers a principled foundation for reinforcement learning (RL) with strong theoretical guarantees and sample efficiency. However, its restrictive assumption-that both transition dynamics and reward functions are linear in the same feature space-limits its applicability in real-world domains, where rewards often exhibit nonlinear or discrete structures. Motivated by applications such as healthcare and e-commerce, where data is scarce and reward signals can be binary or count-valued, we propose the Generalized Linear MDP (GLMDP) framework-an extension of the linear MDP framework-that models rewards using generalized linear models (GLMs) while maintaining linear transition dynamics. We establish the Bellman completeness of GLMDPs with respect to a new function class that accommodates nonlinear rewards and develop two offline RL algorithms: Generalized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant (SS-GPEVI) that utilizes both labeled and unlabeled trajectories. Our algorithms achieve theoretical guarantees on policy suboptimality and demonstrate improved sample efficiency in settings where reward labels are expensive or limited.

