---
layout: default
title: "Uni-LoRA: One Vector is All You Need"
---

# Uni-LoRA: One Vector is All You Need

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00799" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00799v3</a>
  <a href="https://arxiv.org/pdf/2506.00799.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00799v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00799v3', 'Uni-LoRA: One Vector is All You Need')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaiyang Li, Shaobo Han, Qing Su, Wei Li, Zhipeng Cai, Shihao Ji

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-01 (æ›´æ–°: 2025-10-28)

**å¤‡æ³¨**: NeurIPS 2025 Spotlight

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/KaiyangLi1992/Uni-LoRA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUni-LoRAæ¡†æ¶ä»¥å®ç°é«˜æ•ˆçš„å‚æ•°å…±äº«ä¸å¾®è°ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä½ç§©é€‚åº”` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å…¨å±€å‚æ•°å…±äº«` `ç­‰è·æŠ•å½±çŸ©é˜µ` `ç»Ÿä¸€æ¡†æ¶` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LoRAå˜ä½“åœ¨å‚æ•°å…±äº«ä¸Šå­˜åœ¨å±€é™ï¼Œå¯¼è‡´å‚æ•°æ•ˆç‡é™ä½ï¼Œå°¤å…¶æ˜¯å±‚é—´å…±äº«å—é™ã€‚
2. æå‡ºçš„Uni-LoRAæ¡†æ¶é€šè¿‡ä½¿ç”¨ç­‰è·æŠ•å½±çŸ©é˜µï¼Œå®ç°äº†å…¨å±€å‚æ•°å…±äº«ï¼Œä»…éœ€ä¸€ä¸ªå¯è®­ç»ƒå‘é‡é‡æ„æ•´ä¸ªLLMçš„LoRAå‚æ•°ã€‚
3. åœ¨GLUEç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒUni-LoRAå±•ç°å‡ºå“è¶Šçš„å‚æ•°æ•ˆç‡ï¼Œå¹¶åœ¨é¢„æµ‹æ€§èƒ½ä¸Šä¸ç°æœ‰æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ ‡å‡†æ–¹æ³•ï¼Œé€šè¿‡å°†æƒé‡æ›´æ–°é™åˆ¶ä¸ºä½ç§©çŸ©é˜µã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶Uni-LoRAï¼Œå±•ç¤ºäº†ä¸åŒLoRAå˜ä½“çš„å‚æ•°ç©ºé—´ç¼©å‡ç­–ç•¥å¯ä»¥åœ¨æ­¤æ¡†æ¶ä¸‹è¿›è¡Œç»Ÿä¸€è¡¨è¿°ã€‚Uni-LoRAé€šè¿‡ä»ä½ç»´å­ç©ºé—´æŠ•å½±é‡æ„é«˜ç»´å‚æ•°ç©ºé—´ï¼Œé‡‡ç”¨ç­‰è·æŠ•å½±çŸ©é˜µå®ç°å…¨å±€å‚æ•°å…±äº«ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-LoRAåœ¨GLUEã€æ•°å­¦æ¨ç†å’ŒæŒ‡ä»¤å¾®è°ƒåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‚æ•°æ•ˆç‡ï¼ŒåŒæ—¶åœ¨é¢„æµ‹æ€§èƒ½ä¸Šè¶…è¶Šæˆ–åŒ¹é…äº†å…ˆå‰çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LoRAå˜ä½“åœ¨å‚æ•°å…±äº«æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å±‚é—´å…±äº«å—é™çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å‚æ•°æ•ˆç‡çš„æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUni-LoRAæ¡†æ¶é€šè¿‡å¼•å…¥ç­‰è·æŠ•å½±çŸ©é˜µï¼Œå…è®¸å…¨å±€å‚æ•°å…±äº«ï¼Œä»è€Œä»…éœ€ä¸€ä¸ªå¯è®­ç»ƒå‘é‡å³å¯é‡æ„LoRAå‚æ•°ï¼Œç®€åŒ–äº†å¾®è°ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶é¦–å…ˆå°†LoRAå‚æ•°ç©ºé—´è§†ä¸ºé«˜ç»´å‘é‡ç©ºé—´ï¼Œé€šè¿‡ä»ä½ç»´å­ç©ºé—´è¿›è¡ŒæŠ•å½±æ¥å®ç°å‚æ•°é‡æ„ï¼Œæ ¸å¿ƒåœ¨äºé€‰æ‹©åˆé€‚çš„æŠ•å½±çŸ©é˜µã€‚

**å…³é”®åˆ›æ–°**ï¼šUni-LoRAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„å‚æ•°ç©ºé—´é‡æ„æ–¹æ³•å’Œç­‰è·æŠ•å½±çŸ©é˜µçš„å¼•å…¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å±‚çº§æˆ–ç»“æ„ç‰¹å®šæŠ•å½±å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒUni-LoRAä»…éœ€ä¸€ä¸ªå¯è®­ç»ƒçš„å‘é‡ï¼Œä¸”é€šè¿‡ä¼˜åŒ–æŠ•å½±çŸ©é˜µçš„é€‰æ‹©ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨GLUEã€æ•°å­¦æ¨ç†å’ŒæŒ‡ä»¤å¾®è°ƒåŸºå‡†ä¸Šï¼ŒUni-LoRAå±•ç°å‡ºæœ€å…ˆè¿›çš„å‚æ•°æ•ˆç‡ï¼Œå…·ä½“å®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šæˆ–ä¸ç°æœ‰æ–¹æ³•æŒå¹³ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Uni-LoRAæ¡†æ¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆå‚æ•°è°ƒæ•´çš„åœºæ™¯ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚å…¶é«˜æ•ˆçš„å‚æ•°å…±äº«æœºåˆ¶èƒ½å¤Ÿé™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œæå‡æ¨¡å‹çš„é€‚åº”æ€§å’Œçµæ´»æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„ç ”ç©¶ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space $R^D$, can be reconstructed through a projection from a subspace R^d, with $d \ll D$. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, $P \in R^{D \times d}$.Most existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM - making Uni-LoRA both a unified framework and a "one-vector-only" solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance. Our code is available at https://github.com/KaiyangLi1992/Uni-LoRA.

