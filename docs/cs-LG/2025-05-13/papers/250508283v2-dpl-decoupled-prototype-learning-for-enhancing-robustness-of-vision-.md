---
layout: default
title: DPL: Decoupled Prototype Learning for Enhancing Robustness of Vision-Language Transformers to Missing Modalities
---

# DPL: Decoupled Prototype Learning for Enhancing Robustness of Vision-Language Transformers to Missing Modalities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08283" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08283v2</a>
  <a href="https://arxiv.org/pdf/2505.08283.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08283v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08283v2', 'DPL: Decoupled Prototype Learning for Enhancing Robustness of Vision-Language Transformers to Missing Modalities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jueqing Lu, Yuanyuan Qi, Xiaohao Yang, Shuaicheng Niu, Fucai Ke, Shujie Zhou, Wei Tan, Jionghao Lin, Wray Buntine, Hamid Rezatofighi, Lan Du

**åˆ†ç±»**: cs.LG, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-11-15)

**å¤‡æ³¨**: Updates to v1. Added new coauthors and extended the experimental section

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDPLä»¥è§£å†³è§†è§‰è¯­è¨€å˜æ¢å™¨ç¼ºå¤±æ¨¡æ€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å˜æ¢å™¨` `ç¼ºå¤±æ¨¡æ€` `å»è€¦åŸå‹å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `é²æ£’æ€§æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¼ºå¤±æ„ŸçŸ¥æç¤ºæ–¹æ³•åœ¨å¤„ç†æ¨¡æ€ç¼ºå¤±æ—¶ä»ä¾èµ–äºä¼ ç»Ÿçš„é¢„æµ‹å¤´ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºå»è€¦åŸå‹å­¦ä¹ ï¼ˆDPLï¼‰ï¼Œé€šè¿‡é€‰æ‹©ç‰¹å®šåŸå‹å’Œåˆ†è§£ç»„ä»¶æ¥é€‚åº”ç¼ºå¤±æ¨¡æ€çš„å†³ç­–è¿‡ç¨‹ã€‚
3. åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°æ®é›†ä¸Šï¼ŒDPLçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†ç¼ºå¤±æ¨¡æ€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€å˜æ¢å™¨åœ¨è¾“å…¥æ¨¡æ€ï¼ˆå¦‚å›¾åƒï¼‰ç¼ºå¤±æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå› ä¸ºæ¨¡å‹è¢«è¿«ä½¿ç”¨ä¸å®Œæ•´çš„ä¿¡æ¯è¿›è¡Œé¢„æµ‹ã€‚ç°æœ‰çš„ç¼ºå¤±æ„ŸçŸ¥æç¤ºæ–¹æ³•è™½ç„¶èƒ½å‡è½»è¿™ç§æ€§èƒ½ä¸‹é™ï¼Œä½†ä»ä¾èµ–äºä¼ ç»Ÿçš„é¢„æµ‹å¤´ï¼ˆå¦‚å…¨è¿æ¥å±‚ï¼‰ï¼Œæ— è®ºå“ªç§æ¨¡æ€ç¼ºå¤±ï¼Œè®¡ç®—ç±»åˆ«åˆ†æ•°çš„æ–¹å¼éƒ½ç›¸åŒã€‚æœ¬æ–‡æå‡ºäº†å»è€¦åŸå‹å­¦ä¹ ï¼ˆDPLï¼‰ï¼Œä¸€ç§æ–°çš„é¢„æµ‹å¤´æ¶æ„ï¼Œèƒ½å¤Ÿæ ¹æ®è§‚å¯Ÿåˆ°çš„è¾“å…¥æ¨¡æ€æ˜¾å¼è°ƒæ•´å†³ç­–è¿‡ç¨‹ã€‚DPLä¸ºæ¯ä¸ªç±»åˆ«é€‰æ‹©ä¸€ç»„ç‰¹å®šäºå½“å‰ç¼ºå¤±æ¨¡æ€æƒ…å†µçš„åŸå‹ï¼Œå¹¶å°†æ¯ä¸ªåŸå‹åˆ†è§£ä¸ºå›¾åƒç‰¹å®šå’Œæ–‡æœ¬ç‰¹å®šçš„ç»„ä»¶ï¼Œä»è€Œä½¿å†³ç­–ä¾èµ–äºå®é™…å­˜åœ¨çš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDPLåœ¨MM-IMDbã€UPMC Food-101å’ŒHateful Memesç­‰å¤šæ¨¡æ€å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€å˜æ¢å™¨åœ¨è¾“å…¥æ¨¡æ€ç¼ºå¤±æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¼ºå¤±æ¨¡æ€æ—¶ï¼Œä¾èµ–äºç»Ÿä¸€çš„é¢„æµ‹å¤´ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å¯ç”¨ä¿¡æ¯ï¼Œå¯¼è‡´é¢„æµ‹å‡†ç¡®æ€§é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDPLé€šè¿‡ä¸ºæ¯ä¸ªç±»åˆ«é€‰æ‹©ç‰¹å®šäºå½“å‰ç¼ºå¤±æ¨¡æ€çš„åŸå‹ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºå›¾åƒå’Œæ–‡æœ¬ç‰¹å®šç»„ä»¶ï¼Œæ¥é€‚åº”ä¸åŒçš„è¾“å…¥æƒ…å†µã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å®é™…å­˜åœ¨çš„ä¿¡æ¯è¿›è¡Œå†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDPLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªæ–°çš„é¢„æµ‹å¤´ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥æ¨¡æ€çš„ä¸åŒé€‰æ‹©ç›¸åº”çš„åŸå‹ã€‚æ¯ä¸ªåŸå‹è¢«åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«é’ˆå¯¹å›¾åƒå’Œæ–‡æœ¬ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„å†³ç­–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šDPLçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å»è€¦çš„åŸå‹é€‰æ‹©æœºåˆ¶ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒDPLèƒ½å¤Ÿæ ¹æ®ç¼ºå¤±æ¨¡æ€åŠ¨æ€è°ƒæ•´å†³ç­–è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDPLçš„è®¾è®¡åŒ…æ‹¬é€‰æ‹©é€‚åº”ä¸åŒç¼ºå¤±æƒ…å†µçš„åŸå‹é›†ï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ç¡®ä¿ä¸ç°æœ‰åŸºäºæç¤ºçš„æ–¹æ³•å…¼å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDPLåœ¨MM-IMDbã€UPMC Food-101å’ŒHateful Memesç­‰æ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°5%-10%ã€‚è¿™äº›ç»“æœéªŒè¯äº†DPLåœ¨å¤„ç†ç¼ºå¤±æ¨¡æ€æ—¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€å›¾åƒæè¿°ç”Ÿæˆå’Œè·¨æ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æé«˜è§†è§‰è¯­è¨€å˜æ¢å™¨åœ¨ç¼ºå¤±æ¨¡æ€æƒ…å†µä¸‹çš„é²æ£’æ€§ï¼ŒDPLèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å¯é çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ä¿¡æ¯ä¸å®Œæ•´çš„åœºæ™¯ä¸­ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The performance of Visio-Language Transformers drops sharply when an input modality (e.g., image) is missing, because the model is forced to make predictions using incomplete information. Existing missing-aware prompt methods help reduce this degradation, but they still rely on conventional prediction heads (e.g., a Fully-Connected layer) that compute class scores in the same way regardless of which modality is present or absent. We introduce Decoupled Prototype Learning (DPL), a new prediction head architecture that explicitly adjusts its decision process to the observed input modalities. For each class, DPL selects a set of prototypes specific to the current missing-modality cases (image-missing, text-missing, or mixed-missing). Each prototype is then decomposed into image-specific and text-specific components, enabling the head to make decisions that depend on the information actually present. This adaptive design allows DPL to handle inputs with missing modalities more effectively while remaining fully compatible with existing prompt-based frameworks. Extensive experiments on MM-IMDb, UPMC Food-101, and Hateful Memes demonstrate that DPL outperforms state-of-the-art approaches across all widely used multimodal imag-text datasets and various missing cases.

