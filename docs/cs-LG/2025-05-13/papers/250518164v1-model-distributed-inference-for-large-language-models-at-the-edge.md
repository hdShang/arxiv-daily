---
layout: default
title: Model-Distributed Inference for Large Language Models at the Edge
---

# Model-Distributed Inference for Large Language Models at the Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.18164" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.18164v1</a>
  <a href="https://arxiv.org/pdf/2505.18164.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.18164v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.18164v1', 'Model-Distributed Inference for Large Language Models at the Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Davide Macario, Hulya Seferoglu, Erdem Koyuncu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMDI-LLMä»¥è§£å†³è¾¹ç¼˜è®¾å¤‡ä¸Šå¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `è¾¹ç¼˜è®¡ç®—` `æ¨¡å‹åˆ†å¸ƒå¼æ¨ç†` `é€’å½’ç®¡é“å¹¶è¡Œæ€§` `ä½åŠŸè€—è®¾å¤‡` `ååŒè®¡ç®—` `æ¨ç†æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹é¢ä¸´å†…å­˜é™åˆ¶å’Œè®¡ç®—èµ„æºä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. MDI-LLMé€šè¿‡å°†æ¨¡å‹åˆ’åˆ†å¹¶åˆ†é…åˆ°å¤šä¸ªè®¾å¤‡ï¼Œå®ç°ååŒè®¡ç®—å’Œå¹¶è¡Œæ¨ç†ï¼Œå…‹æœäº†å•è®¾å¤‡çš„é™åˆ¶ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMDI-LLMåœ¨å¤šä¸ªè®¾å¤‡å‚ä¸æ—¶æ˜¾è‘—æé«˜äº†ä»¤ç‰Œç”Ÿæˆååé‡ï¼Œå¹¶å‡å°‘äº†æ¯ä¸ªè®¾å¤‡çš„å†…å­˜ä½¿ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†æ¨¡å‹åˆ†å¸ƒå¼æ¨ç†æ¡†æ¶MDI-LLMï¼Œæ—¨åœ¨ä¿ƒè¿›æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜ä½åŠŸè€—è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ¨¡å‹åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå¹¶å°†å…¶åˆ†é…ç»™ç½‘ç»œä¸­çš„ä¸åŒè®¾å¤‡/èŠ‚ç‚¹æ¥å®ç°ã€‚è¿™äº›èŠ‚ç‚¹é€šè¿‡è®¾å¤‡é—´é“¾æ¥äº¤æ¢ä¸­é—´æ¿€æ´»å‘é‡ï¼Œä»è€Œå®ç°ååŒè®¡ç®—ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†â€œé€’å½’ç®¡é“å¹¶è¡Œæ€§â€æŠ€æœ¯ï¼Œå‡å°‘æ¯ä¸ªè®¾å¤‡çš„ç©ºé—²æ—¶é—´ï¼Œå¹¶åœ¨ç”Ÿæˆå¤šä¸ªæ–‡æœ¬åºåˆ—æ—¶å®ç°å¹¶è¡Œæ¨ç†ã€‚MDI-LLMåˆ©ç”¨å¤šä¸ªè¾¹ç¼˜è®¾å¤‡çš„è®¡ç®—èµ„æºï¼Œä½¿å¾—è¶…å‡ºå•ä¸ªè®¾å¤‡å†…å­˜å®¹é‡çš„å¤§è¯­è¨€æ¨¡å‹å¾—ä»¥éƒ¨ç½²ï¼Œä»è€Œåœ¨ä½æˆæœ¬ç¡¬ä»¶ä¸Šè¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼Œéšç€å‚ä¸è®¾å¤‡æ•°é‡çš„å¢åŠ ï¼ŒMDI-LLMæé«˜äº†ä»¤ç‰Œç”Ÿæˆååé‡ï¼Œé™ä½äº†æ¯ä¸ªè®¾å¤‡çš„å†…å­˜æ¶ˆè€—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œç”±äºå†…å­˜å’Œè®¡ç®—èƒ½åŠ›çš„é™åˆ¶ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨è¾¹ç¼˜è®¾å¤‡çš„è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMDI-LLMçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤§è¯­è¨€æ¨¡å‹åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå¹¶å°†è¿™äº›éƒ¨åˆ†åˆ†é…åˆ°ä¸åŒçš„è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œé€šè¿‡è®¾å¤‡é—´çš„åä½œå®ç°é«˜æ•ˆæ¨ç†ã€‚è¿™ç§è®¾è®¡ä½¿å¾—å¤šä¸ªè®¾å¤‡å¯ä»¥å…±åŒæ‰¿æ‹…è®¡ç®—è´Ÿæ‹…ï¼Œä»è€Œçªç ´å•ä¸ªè®¾å¤‡çš„å†…å­˜é™åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMDI-LLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹åˆ’åˆ†ã€è®¾å¤‡åˆ†é…ã€æ¿€æ´»å‘é‡äº¤æ¢å’Œé€’å½’ç®¡é“å¹¶è¡Œæ€§å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ¨¡å‹é¦–å…ˆè¢«åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œç„¶ååˆ†é…ç»™ç½‘ç»œä¸­çš„ä¸åŒè®¾å¤‡ï¼Œè®¾å¤‡é—´é€šè¿‡é“¾æ¥äº¤æ¢ä¸­é—´æ¿€æ´»å‘é‡ï¼Œæœ€ååˆ©ç”¨é€’å½’ç®¡é“å¹¶è¡Œæ€§æŠ€æœ¯è¿›è¡Œé«˜æ•ˆæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šMDI-LLMçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†â€œé€’å½’ç®¡é“å¹¶è¡Œæ€§â€æŠ€æœ¯ï¼Œæ˜¾è‘—å‡å°‘äº†è®¾å¤‡çš„ç©ºé—²æ—¶é—´ï¼Œå¹¶å®ç°äº†åœ¨ç”Ÿæˆå¤šä¸ªæ–‡æœ¬åºåˆ—æ—¶çš„å¹¶è¡Œæ¨ç†ã€‚è¿™ä¸€åˆ›æ–°ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è¾¹ç¼˜è®¾å¤‡çš„è®¡ç®—èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMDI-LLMå…³æ³¨äºè®¾å¤‡é—´çš„é«˜æ•ˆé€šä¿¡å’Œè®¡ç®—è´Ÿè½½çš„å‡è¡¡åˆ†é…ï¼Œç¡®ä¿æ¯ä¸ªè®¾å¤‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­éƒ½èƒ½å‘æŒ¥æœ€å¤§æ•ˆèƒ½ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†è®¨è®ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMDI-LLMåœ¨å¤šä¸ªè®¾å¤‡å‚ä¸æ—¶ï¼Œä»¤ç‰Œç”Ÿæˆååé‡æé«˜äº†50%ä»¥ä¸Šï¼ŒåŒæ—¶æ¯ä¸ªè®¾å¤‡çš„å†…å­˜æ¶ˆè€—é™ä½äº†30%ã€‚ä¸ä¼ ç»Ÿå•è®¾å¤‡æ¨ç†æ–¹æ³•ç›¸æ¯”ï¼ŒMDI-LLMæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MDI-LLMçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ™ºèƒ½å®¶å±…ã€ç‰©è”ç½‘å’Œç§»åŠ¨è®¾å¤‡ç­‰é¢†åŸŸã€‚é€šè¿‡åœ¨ä½åŠŸè€—ç¡¬ä»¶ä¸Šå®ç°å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´æ™ºèƒ½çš„äº¤äº’ä½“éªŒï¼Œæ¨åŠ¨è¾¹ç¼˜è®¡ç®—çš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šåœ¨å®æ—¶è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM), a novel framework designed to facilitate the deployment of state-of-the-art large-language models (LLMs) across low-power devices at the edge. This is accomplished by dividing the model into multiple partitions, which are then assigned to different devices/nodes within the network. These nodes exchange intermediate activation vectors via device-to-device links, enabling collaborative computation. To enhance the efficiency of this process, we propose the "recurrent pipeline parallelism" technique, which reduces idle time on each device and facilitates parallel inference during the generation of multiple text sequences. By leveraging the combined computational resources of multiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the memory capacity of individual devices, making it possible to perform inference on low-cost hardware. Furthermore, as the number of participating devices increases, MDI-LLM boosts token generation throughput and reduces memory consumption per device.

