---
layout: default
title: "InfoPO: On Mutual Information Maximization for Large Language Model Alignment"
---

# InfoPO: On Mutual Information Maximization for Large Language Model Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08507" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08507v1</a>
  <a href="https://arxiv.org/pdf/2505.08507.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08507v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08507v1', 'InfoPO: On Mutual Information Maximization for Large Language Model Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Teng Xiao, Zhen Ge, Sujay Sanghavi, Tian Wang, Julian Katz-Samuels, Marc Versage, Qingjun Cui, Trishul Chilimbi

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: NAACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInfoPOä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹å¯¹é½ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `åå¥½ä¼˜åŒ–` `äº’ä¿¡æ¯æœ€å¤§åŒ–` `æ¨ç†ä»»åŠ¡` `æ¨¡å‹å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ä¾èµ–äºBTæ¨¡å‹ï¼Œå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå°¤å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºçš„InfoPOç®—æ³•é€šè¿‡æ¶ˆé™¤å¯¹BTæ¨¡å‹çš„ä¾èµ–ï¼Œæä¾›äº†ä¸€ç§æ–°çš„åå¥½å¾®è°ƒæ–¹æ³•ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInfoPOåœ¨å¤šä¸ªå¼€æ”¾åŸºå‡†ä¸Šï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨äººç±»åå¥½æ•°æ®å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œåè®­ç»ƒçš„é—®é¢˜ã€‚è¿‘æœŸçš„ç›´æ¥åå¥½ä¼˜åŒ–åŠå…¶å˜ä½“åœ¨å¯¹é½è¯­è¨€æ¨¡å‹æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ½œåŠ›ï¼Œæ¶ˆé™¤äº†å¯¹å¥–åŠ±æ¨¡å‹å’Œåœ¨çº¿é‡‡æ ·çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºBradley-Terryï¼ˆBTï¼‰æ¨¡å‹çš„æ˜¾å¼å‡è®¾ï¼Œå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå¹¶åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºInfoPOçš„åŸåˆ™æ€§åå¥½å¾®è°ƒç®—æ³•ï¼Œè¯¥ç®—æ³•æœ‰æ•ˆä¸”é«˜æ•ˆåœ°ä½¿ç”¨åå¥½æ•°æ®å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹é½ã€‚InfoPOæ¶ˆé™¤äº†å¯¹BTæ¨¡å‹çš„ä¾èµ–ï¼Œå¹¶é˜²æ­¢æ‰€é€‰å“åº”çš„å¯èƒ½æ€§ä¸‹é™ã€‚å¤§é‡å®éªŒç¡®è®¤ï¼ŒInfoPOåœ¨å¹¿æ³›ä½¿ç”¨çš„å¼€æ”¾åŸºå‡†ä¸Šï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œå§‹ç»ˆä¼˜äºå·²å»ºç«‹çš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹é½è¿‡ç¨‹ä¸­å› ä¾èµ–Bradley-Terryæ¨¡å‹è€Œå¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šInfoPOç®—æ³•é€šè¿‡æœ€å¤§åŒ–äº’ä¿¡æ¯æ¥å¯¹é½è¯­è¨€æ¨¡å‹ï¼Œé¿å…äº†BTæ¨¡å‹çš„å‡è®¾ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€åå¥½å»ºæ¨¡å’Œæ¨¡å‹å¾®è°ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼Œç¡®ä¿äº†åå¥½æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šInfoPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ä¸å†ä¾èµ–BTæ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡äº’ä¿¡æ¯æœ€å¤§åŒ–æ¥ä¼˜åŒ–æ¨¡å‹çš„å“åº”é€‰æ‹©ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒInfoPOé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–äº’ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è°ƒæ•´è¶…å‚æ•°æ¥å¹³è¡¡æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œç¡®ä¿å“åº”çš„å¯èƒ½æ€§ä¸ä¼šä¸‹é™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒInfoPOåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»ŸåŸºçº¿æå‡äº†çº¦15%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼ŒInfoPOèƒ½å¤Ÿå¢å¼ºè¿™äº›ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study the post-training of large language models (LLMs) with human preference data. Recently, direct preference optimization and its variants have shown considerable promise in aligning language models, eliminating the need for reward models and online sampling. Despite these benefits, these methods rely on explicit assumptions about the Bradley-Terry (BT) model, which makes them prone to overfitting and results in suboptimal performance, particularly on reasoning-heavy tasks. To address these challenges, we propose a principled preference fine-tuning algorithm called InfoPO, which effectively and efficiently aligns large language models using preference data. InfoPO eliminates the reliance on the BT model and prevents the likelihood of the chosen response from decreasing. Extensive experiments confirm that InfoPO consistently outperforms established baselines on widely used open benchmarks, particularly in reasoning tasks.

