---
layout: default
title: Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression
---

# Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.18166" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.18166v1</a>
  <a href="https://arxiv.org/pdf/2505.18166.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.18166v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.18166v1', 'Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jacob Sander, David Moe, Achraf Cohen, Brent Venable, Venkat Dasari, Brian Jalaian

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: 9 Pages, 2 Figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè‡ªè’¸é¦çš„LLMå‹ç¼©æ–¹æ³•ä»¥åº”å¯¹è¾¹ç¼˜è®¡ç®—é™åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¾¹ç¼˜è®¡ç®—` `æ¨¡å‹å‹ç¼©` `è‡ªè’¸é¦` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‹ç¼©æ–¹æ³•é€šå¸¸ä¾èµ–äºå…¨Transformerçš„å‰ªæï¼Œéš¾ä»¥æ»¡è¶³è¾¹ç¼˜è®¡ç®—çš„èµ„æºé™åˆ¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹MLPæ¨¡å—çš„é€å±‚L2èŒƒæ•°å‰ªææ–¹æ³•ï¼Œå¹¶æ¯”è¾ƒäº†å¾®è°ƒä¸è‡ªè’¸é¦çš„æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç›¸åŒå‰ªææ¡ä»¶ä¸‹ï¼Œè‡ªè’¸é¦æ–¹æ³•åœ¨æµ‹è¯•å‡†ç¡®æ€§ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£åŸºç¡€æ¨¡å‹é€šå¸¸é€šè¿‡ç»“æ„åŒ–å‰ªæå’Œå†è®­ç»ƒçš„ç»„åˆè¿›è¡Œå‹ç¼©ï¼Œä»¥æ»¡è¶³è¾¹ç¼˜éƒ¨ç½²çš„ä¸¥æ ¼è®¡ç®—ã€å†…å­˜å’Œè¿æ¥æ€§é™åˆ¶ã€‚æœ¬æ–‡é‡‡ç”¨ç®€å•çš„é€å±‚L2èŒƒæ•°å‰ªæï¼Œä»…é’ˆå¯¹MLPæ¨¡å—ä½œä¸ºå›ºå®šåŸºçº¿ã€‚ç ”ç©¶çš„é‡ç‚¹åœ¨äºéš”ç¦»å†è®­ç»ƒæŸå¤±å‡½æ•°çš„å½±å“ï¼šå³ä½¿ç”¨äº¤å‰ç†µçš„å¾®è°ƒï¼ˆL2PFTï¼‰éœ€è¦æ ‡è®°æ•°æ®ï¼Œè€Œè‡ªè’¸é¦çš„KLæ•£åº¦ï¼ˆL2PSDï¼‰ä»…åˆ©ç”¨æ•™å¸ˆè¾“å‡ºï¼ˆæ— æ ‡ç­¾ï¼‰ã€‚åœ¨ç›¸åŒçš„å‰ªæè®¡åˆ’ä¸‹ï¼ŒåŸºäºKLçš„è’¸é¦åœ¨æµ‹è¯•å‡†ç¡®æ€§ä¸Šä¸äº¤å‰ç†µå¾®è°ƒç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œè¡¨æ˜å³ä½¿åœ¨åŸºæœ¬çš„MLPå‰ªæä¸‹ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©å¯¹èµ„æºå—é™ç¯å¢ƒä¸­çš„å‹ç¼©æ¨¡å‹æ¢å¤æœ‰é‡è¦å½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥æ»¡è¶³è®¡ç®—å’Œå†…å­˜é™åˆ¶çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå…¨æ¨¡å‹å‰ªæï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹å®ç°æœ€ä½³æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„é€å±‚L2èŒƒæ•°å‰ªææ–¹æ³•ï¼Œä¸“æ³¨äºMLPæ¨¡å—ï¼Œå¹¶æ¯”è¾ƒäº†ä¸¤ç§ä¸åŒçš„å†è®­ç»ƒç­–ç•¥ï¼šäº¤å‰ç†µå¾®è°ƒå’Œè‡ªè’¸é¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æŸå¤±å‡½æ•°å¯¹æ¨¡å‹æ¢å¤çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œé€å±‚L2èŒƒæ•°å‰ªæï¼Œç„¶ååˆ†åˆ«åº”ç”¨äº¤å‰ç†µå¾®è°ƒå’Œè‡ªè’¸é¦ç­–ç•¥è¿›è¡Œå†è®­ç»ƒã€‚æ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹ç‰¹å®šçš„æ¨¡å‹ç»„ä»¶è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡è‡ªè’¸é¦æ–¹æ³•ï¼Œåˆ©ç”¨æ•™å¸ˆè¾“å‡ºè€Œéæ ‡è®°æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»è€Œåœ¨èµ„æºå—é™ç¯å¢ƒä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä¾èµ–æ ‡è®°æ•°æ®çš„å¾®è°ƒæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç›¸åŒçš„å‰ªæè®¡åˆ’ï¼ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼ˆäº¤å‰ç†µä¸KLæ•£åº¦ï¼‰æˆä¸ºå½±å“æœ€ç»ˆæ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚MLPæ¨¡å—çš„è®¾è®¡å’Œå‰ªæç­–ç•¥ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿åœ¨å‹ç¼©åä»èƒ½ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„å‰ªææ¡ä»¶ä¸‹ï¼ŒåŸºäºKLæ•£åº¦çš„è‡ªè’¸é¦æ–¹æ³•åœ¨æµ‹è¯•å‡†ç¡®æ€§ä¸Šä¸äº¤å‰ç†µå¾®è°ƒç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œå±•ç¤ºäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­ï¼ŒæŸå¤±å‡½æ•°é€‰æ‹©å¯¹æ¨¡å‹æ¢å¤çš„é‡è¦æ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒKLæ•£åº¦æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸Šæå‡å¹…åº¦æ˜¾è‘—ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è¾¹ç¼˜è®¡ç®—åœºæ™¯ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ™ºèƒ½æ‰‹æœºã€ç‰©è”ç½‘è®¾å¤‡å’Œè¾¹ç¼˜æœåŠ¡å™¨ç­‰ã€‚é€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©å’Œå†è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„æ¨ç†å’Œå“åº”ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¿™ä¸€æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½è®¾å¤‡çš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern foundational models are often compressed via a combination of structured pruning and re-training to meet the strict compute, memory, and connectivity constraints of edge deployments. While state-of-the-art pruning schemes target the entire Transformer, we adopt a simple, layer-wise L2-norm pruning on only the MLP blocks as a fixed baseline. Our focus is not on achieving maximal compression, but on isolating the impact of the re-training loss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires labeled data, versus (ii) Self-Distillation with KL-divergence, which leverages only teacher logits (no labels) (L2PSD). We evaluate both pipelines on the OLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied connectivity scenarios typical of edge networks. Under identical pruning schedules, KL-based distillation matches or exceeds CE fine-tuning in test accuracy, demonstrating that, even with a basic MLP-only pruning, the choice of loss function materially affects compressed model recovery in resource-constrained environments.

