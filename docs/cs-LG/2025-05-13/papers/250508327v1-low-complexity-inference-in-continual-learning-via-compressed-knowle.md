---
layout: default
title: Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer
---

# Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08327" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08327v1</a>
  <a href="https://arxiv.org/pdf/2505.08327.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08327v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08327v1', 'Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenrong Liu, Janne M. J. Huttunen, Mikko Honkala

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä½å¤æ‚åº¦æ¨ç†æ¡†æ¶ä»¥è§£å†³æŒç»­å­¦ä¹ ä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `æ¨¡å‹å‹ç¼©` `çŸ¥è¯†è’¸é¦` `å‰ªææŠ€æœ¯` `ç±»å¢é‡å­¦ä¹ ` `æ¨ç†å¤æ‚åº¦` `é«˜æ•ˆæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŒç»­å­¦ä¹ æ–¹æ³•åœ¨æ¨ç†æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬ï¼Œé™åˆ¶äº†å…¶åœ¨ä½å»¶è¿Ÿå’Œèƒ½æ•ˆè¦æ±‚é«˜çš„å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†åŸºäºå‰ªæå’ŒçŸ¥è¯†è’¸é¦çš„ä¸¤ç§é«˜æ•ˆæ¡†æ¶ï¼Œæ—¨åœ¨é™ä½æ¨ç†å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚
3. åœ¨å¤šä¸ªç±»å¢é‡å­¦ä¹ åŸºå‡†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œæ¨ç†å¤æ‚æ€§ä¹‹é—´å–å¾—äº†æ›´ä¼˜çš„å¹³è¡¡ï¼Œè¶…è¶Šäº†ç°æœ‰å¼ºåŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æ—¨åœ¨è®­ç»ƒèƒ½å¤Ÿåœ¨ä¸é—å¿˜å…ˆå‰çŸ¥è¯†çš„æƒ…å†µä¸‹å­¦ä¹ ä¸€ç³»åˆ—ä»»åŠ¡çš„æ¨¡å‹ã€‚CLçš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¹³è¡¡ç¨³å®šæ€§ï¼ˆä¿æŒæ—§ä»»åŠ¡çš„æ€§èƒ½ï¼‰å’Œå¯å¡‘æ€§ï¼ˆé€‚åº”æ–°ä»»åŠ¡ï¼‰ã€‚å°½ç®¡å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨CLä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ¨ç†æ—¶çš„é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ¢ç´¢äº†æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼ŒåŒ…æ‹¬å‰ªæå’ŒçŸ¥è¯†è’¸é¦ï¼Œæå‡ºäº†ä¸¤ç§é«˜æ•ˆæ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹ç±»å¢é‡å­¦ä¹ ï¼ˆCILï¼‰è¿™ä¸€æŒ‘æˆ˜æ€§CLè®¾ç½®ã€‚é€šè¿‡åœ¨å¤šä¸ªCILåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†æ‰€ææ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œæ¨ç†å¤æ‚æ€§ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä¸”å§‹ç»ˆä¼˜äºå¼ºåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æŒç»­å­¦ä¹ ä¸­æ¨ç†æ—¶çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç±»å¢é‡å­¦ä¹ æ—¶é¢ä¸´æ€§èƒ½å’Œæ•ˆç‡çš„åŒé‡æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯å‰ªæå’ŒçŸ¥è¯†è’¸é¦ï¼Œæ¥é™ä½æ¨ç†å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„é€‚åº”èƒ½åŠ›å’Œæ—§ä»»åŠ¡çš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¡†æ¶ï¼šå‰ªææ¡†æ¶å’ŒçŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚å‰ªææ¡†æ¶åœ¨è®­ç»ƒçš„ä¸åŒé˜¶æ®µåº”ç”¨é¢„å‰ªæå’Œåå‰ªæç­–ç•¥ï¼Œè€ŒçŸ¥è¯†è’¸é¦æ¡†æ¶åˆ™é‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¶æ„ï¼Œå°†å¤§å‹é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†é’ˆå¯¹ç±»å¢é‡å­¦ä¹ çš„é«˜æ•ˆæ¨ç†æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†æ—¶ä»»åŠ¡èº«ä»½ä¸å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡ŒçŸ¥è¯†è½¬ç§»å’Œæ¨¡å‹å‹ç¼©ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‰ªææ¡†æ¶ä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„å‰ªææ¯”ä¾‹å’Œç­–ç•¥ï¼›åœ¨çŸ¥è¯†è’¸é¦æ¡†æ¶ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ•™å¸ˆä¸å­¦ç”Ÿä¹‹é—´çš„çŸ¥è¯†ä¼ é€’ï¼Œç¡®ä¿å­¦ç”Ÿæ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‰ªæå’ŒçŸ¥è¯†è’¸é¦æ¡†æ¶åœ¨å¤šä¸ªç±»å¢é‡å­¦ä¹ åŸºå‡†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®æ€§ä¸æ¨ç†å¤æ‚æ€§ä¹‹é—´çš„å¹³è¡¡æ˜æ˜¾ä¼˜äºç°æœ‰å¼ºåŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰éœ€è¦å®æ—¶å­¦ä¹ å’Œé€‚åº”æ–°ç¯å¢ƒçš„ç³»ç»Ÿã€‚é€šè¿‡é™ä½æ¨ç†å¤æ‚åº¦ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œï¼Œæå‡å®é™…åº”ç”¨çš„å¯è¡Œæ€§å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ›´å¤šæ™ºèƒ½ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ï¼Œä¿ƒè¿›æŒç»­å­¦ä¹ æŠ€æœ¯çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Continual learning (CL) aims to train models that can learn a sequence of tasks without forgetting previously acquired knowledge. A core challenge in CL is balancing stability -- preserving performance on old tasks -- and plasticity -- adapting to new ones. Recently, large pre-trained models have been widely adopted in CL for their ability to support both, offering strong generalization for new tasks and resilience against forgetting. However, their high computational cost at inference time limits their practicality in real-world applications, especially those requiring low latency or energy efficiency. To address this issue, we explore model compression techniques, including pruning and knowledge distillation (KD), and propose two efficient frameworks tailored for class-incremental learning (CIL), a challenging CL setting where task identities are unavailable during inference. The pruning-based framework includes pre- and post-pruning strategies that apply compression at different training stages. The KD-based framework adopts a teacher-student architecture, where a large pre-trained teacher transfers downstream-relevant knowledge to a compact student. Extensive experiments on multiple CIL benchmarks demonstrate that the proposed frameworks achieve a better trade-off between accuracy and inference complexity, consistently outperforming strong baselines. We further analyze the trade-offs between the two frameworks in terms of accuracy and efficiency, offering insights into their use across different scenarios.

