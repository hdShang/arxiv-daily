---
layout: default
title: DSADF: Thinking Fast and Slow for Decision Making
---

# DSADF: Thinking Fast and Slow for Decision Making

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08189" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08189v2</a>
  <a href="https://arxiv.org/pdf/2505.08189.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08189v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08189v2', 'DSADF: Thinking Fast and Slow for Decision Making')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhihao Dou, Dongfei Cui, Jun Yan, Weida Wang, Benteng Chen, Haoming Wang, Zeke Xie, Shufei Zhang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-08-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ä»¥æå‡RLæ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å†³ç­–æ¡†æ¶` `è§†è§‰è¯­è¨€æ¨¡å‹` `åŠ¨æ€ç¯å¢ƒ` `æ™ºèƒ½ä½“` `æ·±åº¦æ¨ç†` `å¿«é€Ÿå†³ç­–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´å†³ç­–æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„åŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼ˆDSADFï¼‰ç»“åˆäº†RLæ™ºèƒ½ä½“å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°å¿«é€Ÿä¸æ·±åº¦æ¨ç†çš„ååŒã€‚
3. åœ¨Crafterå’ŒHousekeepç­‰è§†é¢‘æ¸¸æˆç¯å¢ƒä¸­ï¼ŒDSADFæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“åœ¨å·²çŸ¥å’ŒæœªçŸ¥ä»»åŠ¡ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“åœ¨æ˜ç¡®ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€ç¯å¢ƒä¸­ç”±äºä¾èµ–è¯•é”™äº¤äº’è€Œéš¾ä»¥æ¨å¹¿å…¶å­¦ä¹ ç­–ç•¥ã€‚è¿‘æœŸç ”ç©¶å°è¯•é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥å¢å¼ºRLæ™ºèƒ½ä½“çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ç¼ºä¹ä¸åŸºç¡€æ¨¡å‹çš„æ— ç¼åè°ƒï¼Œå¯¼è‡´åœ¨ä¸ç†Ÿæ‚‰ç¯å¢ƒä¸­çš„å†³ç­–ä¸åˆç†ä¸”æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼ˆDSADFï¼‰ï¼Œç»“åˆäº†RLæ™ºèƒ½ä½“å’ŒVLMçš„ä¼˜åŠ¿ï¼Œå®ç°å¿«é€Ÿç›´è§‰ä¸æ·±åº¦æ¨ç†çš„å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘æ¸¸æˆç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†å†³ç­–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­å†³ç­–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹ä¸åŸºç¡€æ¨¡å‹çš„æœ‰æ•ˆåè°ƒï¼Œå¯¼è‡´å†³ç­–ä¸åˆç†å’Œæ•ˆç‡ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼ˆDSADFï¼‰ï¼Œé€šè¿‡ç»“åˆå¿«é€Ÿç›´è§‰çš„RLæ™ºèƒ½ä½“å’Œæ·±åº¦æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ç°é«˜æ•ˆçš„å†³ç­–è¿‡ç¨‹ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ï¼Œæå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDSADFæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç³»ç»Ÿ1ï¼ˆRLæ™ºèƒ½ä½“åŠå…¶è®°å¿†ç©ºé—´ï¼‰ç”¨äºå¿«é€Ÿç›´è§‰å†³ç­–ï¼Œç³»ç»Ÿ2ï¼ˆVLMï¼‰ç”¨äºæ·±åº¦åˆ†ææ¨ç†ã€‚ä¸¤è€…é€šè¿‡æœ‰æ•ˆçš„äº¤äº’å½¢æˆä¸€ä¸ªåŒç³»ç»Ÿå†³ç­–æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šDSADFçš„åˆ›æ–°ç‚¹åœ¨äºå°†RLæ™ºèƒ½ä½“ä¸VLMçš„æ¨ç†èƒ½åŠ›ç»“åˆï¼Œå½¢æˆä¸€ä¸ªåŠ¨æ€é€‚åº”çš„å†³ç­–ç³»ç»Ÿã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„RLæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨DSADFä¸­ï¼ŒRLæ™ºèƒ½ä½“çš„è®°å¿†ç©ºé—´ç”¨äºå­˜å‚¨å†å²å†³ç­–ä¿¡æ¯ï¼ŒVLMåˆ™é€šè¿‡åˆ†æç¯å¢ƒä¿¡æ¯æä¾›å†³ç­–æ”¯æŒã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œæœªæ¥ç ”ç©¶å¯è¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›æŠ€æœ¯ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDSADFåœ¨Crafterå’ŒHousekeepç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œå†³ç­–æ•ˆç‡æé«˜äº†XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œåœ¨å·²çŸ¥å’ŒæœªçŸ¥ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ¸¸æˆã€æœºå™¨äººæ§åˆ¶å’Œè‡ªåŠ¨é©¾é©¶ç­‰åŠ¨æ€ç¯å¢ƒä¸­çš„å†³ç­–ç³»ç»Ÿã€‚é€šè¿‡æå‡æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›ï¼ŒDSADFæœ‰æœ›åœ¨å¤æ‚åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–å†³ç­–ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although Reinforcement Learning (RL) agents are effective in well-defined environments, they often struggle to generalize their learned policies to dynamic settings due to their reliance on trial-and-error interactions. Recent work has explored applying Large Language Models (LLMs) or Vision Language Models (VLMs) to boost the generalization of RL agents through policy optimization guidance or prior knowledge. However, these approaches often lack seamless coordination between the RL agent and the foundation model, leading to unreasonable decision-making in unfamiliar environments and efficiency bottlenecks. Making full use of the inferential capabilities of foundation models and the rapid response capabilities of RL agents and enhancing the interaction between the two to form a dual system is still a lingering scientific question. To address this problem, we draw inspiration from Kahneman's theory of fast thinking (System 1) and slow thinking (System 2), demonstrating that balancing intuition and deep reasoning can achieve nimble decision-making in a complex world. In this study, we propose a Dual-System Adaptive Decision Framework (DSADF), integrating two complementary modules: System 1, comprising an RL agent and a memory space for fast and intuitive decision making, and System 2, driven by a VLM for deep and analytical reasoning. DSADF facilitates efficient and adaptive decision-making by combining the strengths of both systems. The empirical study in the video game environment: Crafter and Housekeep demonstrates the effectiveness of our proposed method, showing significant improvements in decision abilities for both unseen and known tasks.

