---
layout: default
title: "AI Accelerators for Large Language Model Inference: Architecture Analysis and Scaling Strategies"
---

# AI Accelerators for Large Language Model Inference: Architecture Analysis and Scaling Strategies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00008" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00008v1</a>
  <a href="https://arxiv.org/pdf/2506.00008.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00008v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00008v1', 'AI Accelerators for Large Language Model Inference: Architecture Analysis and Scaling Strategies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amit Sharma

**åˆ†ç±»**: cs.AR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„AIåŠ é€Ÿå™¨æ¶æ„åˆ†æä¸æ‰©å±•ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `AIåŠ é€Ÿå™¨` `æ€§èƒ½åˆ†æ` `æ¶æ„è®¾è®¡` `æ‰©å±•ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„AIåŠ é€Ÿå™¨åœ¨å¤„ç†å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ—¶å­˜åœ¨æ€§èƒ½å·®å¼‚ï¼Œæ— æ³•æœ‰æ•ˆåŒ¹é…ä¸åŒå·¥ä½œè´Ÿè½½ã€‚
2. æœ¬æ–‡é€šè¿‡å¯¹å¤šç§å•†ä¸šAIåŠ é€Ÿå™¨çš„æ€§èƒ½è¿›è¡Œæ¯”è¾ƒï¼Œæå‡ºäº†ä¼˜åŒ–åŒ¹é…å·¥ä½œè´Ÿè½½ä¸ç¡¬ä»¶æ¶æ„çš„ç­–ç•¥ã€‚
3. ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ä¸“å®¶å¹¶è¡Œæ€§å¯æ˜¾è‘—æé«˜å‚æ•°è®¡ç®—æ•ˆç‡ï¼Œä½†å»¶è¿Ÿæ³¢åŠ¨è¾ƒå¤§ï¼Œæä¾›äº†è®¾è®¡æ”¹è¿›çš„æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä¸“ç”¨ç¡¬ä»¶çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚æœ¬æ–‡é¦–æ¬¡è¿›è¡Œäº†ä»¥å·¥ä½œè´Ÿè½½ä¸ºä¸­å¿ƒçš„å•†ä¸šAIåŠ é€Ÿå™¨è·¨æ¶æ„æ€§èƒ½ç ”ç©¶ï¼Œæ¶µç›–äº†åŸºäºGPUçš„èŠ¯ç‰‡ã€æ··åˆå°è£…å’Œæ™¶åœ†çº§å¼•æ“ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†å†…å­˜å±‚æ¬¡ã€è®¡ç®—æ¶æ„å’Œç‰‡ä¸Šäº’è¿ï¼Œå¹¶è§‚å¯Ÿåˆ°åœ¨æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦å˜åŒ–æ—¶ï¼Œæ¶æ„ä¹‹é—´çš„æ€§èƒ½å·®å¼‚å¯è¾¾3.7å€ã€‚ç ”ç©¶è¿˜è€ƒå¯Ÿäº†é’ˆå¯¹ä¸‡äº¿å‚æ•°æ¨¡å‹çš„å››ç§æ‰©å±•æŠ€æœ¯ï¼›ä¸“å®¶å¹¶è¡Œæ€§æä¾›äº†8.4å€çš„å‚æ•°ä¸è®¡ç®—ä¼˜åŠ¿ï¼Œä½†å…¶å»¶è¿Ÿæ–¹å·®æ¯”å¼ é‡å¹¶è¡Œæ€§é«˜å‡º2.1å€ã€‚è¿™äº›å‘ç°ä¸ºå°†å·¥ä½œè´Ÿè½½ä¸åŠ é€Ÿå™¨åŒ¹é…æä¾›äº†å®šé‡æŒ‡å¯¼ï¼Œå¹¶æ­ç¤ºäº†ä¸‹ä¸€ä»£è®¾è®¡å¿…é¡»è§£å†³çš„æ¶æ„å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­ä¸åŒAIåŠ é€Ÿå™¨æ€§èƒ½ä¸å‡çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¶æ„é€‰æ‹©ä¸Šç¼ºä¹ç³»ç»Ÿæ€§åˆ†æï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹å¤šç§AIåŠ é€Ÿå™¨çš„æ€§èƒ½è¿›è¡Œå·¥ä½œè´Ÿè½½ä¸­å¿ƒçš„æ¯”è¾ƒï¼Œæå‡ºäº†é’ˆå¯¹ä¸åŒæ¨¡å‹éœ€æ±‚çš„ç¡¬ä»¶åŒ¹é…ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–æ¨ç†æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆåˆ†æäº†ä¸åŒæ¶æ„çš„å†…å­˜å±‚æ¬¡ã€è®¡ç®—ç»“æ„å’Œç‰‡ä¸Šäº’è¿ï¼Œæ¥ç€è¯„ä¼°äº†å››ç§æ‰©å±•æŠ€æœ¯çš„æ€§èƒ½ï¼Œæœ€åæä¾›äº†é’ˆå¯¹æœªæ¥è®¾è®¡çš„å»ºè®®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†å¤šç§AIåŠ é€Ÿå™¨çš„æ€§èƒ½ï¼Œæ­ç¤ºäº†åœ¨ä¸åŒå·¥ä½œè´Ÿè½½ä¸‹çš„æ€§èƒ½å·®å¼‚ï¼Œä¸ºç¡¬ä»¶è®¾è®¡æä¾›äº†æ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒçš„æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦è¿›è¡Œæ€§èƒ½æµ‹è¯•ï¼Œä¸“å®¶å¹¶è¡Œæ€§å’Œå¼ é‡å¹¶è¡Œæ€§è¢«ç”¨ä½œä¸»è¦çš„æ‰©å±•æŠ€æœ¯ï¼Œç»“æœæ˜¾ç¤ºä¸“å®¶å¹¶è¡Œæ€§åœ¨å‚æ•°è®¡ç®—ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸åŒæ¶æ„ä¸‹ï¼Œæ€§èƒ½å·®å¼‚å¯è¾¾3.7å€ã€‚ä¸“å®¶å¹¶è¡Œæ€§åœ¨å‚æ•°è®¡ç®—æ–¹é¢æä¾›äº†8.4å€çš„ä¼˜åŠ¿ï¼Œä½†å»¶è¿Ÿæ³¢åŠ¨é«˜è¾¾2.1å€ï¼Œæ˜¾ç¤ºå‡ºä¸åŒæ‰©å±•æŠ€æœ¯çš„æƒè¡¡ä¸é€‰æ‹©çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿç­‰é¢†åŸŸã€‚é€šè¿‡ä¼˜åŒ–AIåŠ é€Ÿå™¨çš„æ¶æ„è®¾è®¡ï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„ä¸æ–­æ‰©å¤§ï¼Œæœ¬æ–‡çš„ç ”ç©¶æˆæœå°†ä¸ºç¡¬ä»¶è®¾è®¡æä¾›é‡è¦çš„å‚è€ƒä¾æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid growth of large-language models (LLMs) is driving a new wave of specialized hardware for inference. This paper presents the first workload-centric, cross-architectural performance study of commercial AI accelerators, spanning GPU-based chips, hybrid packages, and wafer-scale engines. We compare memory hierarchies, compute fabrics, and on-chip interconnects, and observe up to 3.7x performance variation across architectures as batch size and sequence length change. Four scaling techniques for trillion-parameter models are examined; expert parallelism offers an 8.4x parameter-to-compute advantage but incurs 2.1x higher latency variance than tensor parallelism. These findings provide quantitative guidance for matching workloads to accelerators and reveal architectural gaps that next-generation designs must address.

