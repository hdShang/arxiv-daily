---
layout: default
title: "DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update"
---

# DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09017" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.09017v2</a>
  <a href="https://arxiv.org/pdf/2505.09017.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09017v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09017v2', 'DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Bizhan Alipour Pijan, Serdar Bozdag

**ÂàÜÁ±ª**: cs.LG, cs.SI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-13 (Êõ¥Êñ∞: 2025-12-21)

**Â§áÊ≥®**: Published in LOG conference, 2025. This version corresponds to the published article

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DyGSSM‰ª•Ëß£ÂÜ≥Âä®ÊÄÅÂõæË°®Á§∫Â≠¶‰π†‰∏≠‰ø°ÊÅØÊèêÂèñ‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âä®ÊÄÅÂõæË°®Á§∫Â≠¶‰π†` `ÂõæÂç∑ÁßØÁΩëÁªú` `Èó®ÊéßÂæ™ÁéØÂçïÂÖÉ` `Áä∂ÊÄÅÁ©∫Èó¥Ê®°Âûã` `HiPPOÁÆóÊ≥ï` `ÁâπÂæÅÊèêÂèñ` `Êó∂Èó¥‰æùËµñÁÆ°ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂä®ÊÄÅÂõæË°®Á§∫Â≠¶‰π†ÊñπÊ≥ïÂú®‰ø°ÊÅØÊèêÂèñ‰∏äÂ≠òÂú®Â±ÄÈôêÔºåÊó†Ê≥ïÂêåÊó∂ÊçïÊçâÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÁâπÂæÅ„ÄÇ
2. DyGSSMÊñπÊ≥ïÁªìÂêàGCNÂíåGRUËøõË°åÁâπÂæÅÊèêÂèñÔºåÂπ∂‰ΩøÁî®HiPPOÁÆóÊ≥ï‰ºòÂåñÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÊõ¥Êñ∞„ÄÇ
3. Âú®‰∫î‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåDyGSSMÂú®17‰∏™Ê°à‰æã‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫Á∫øÂíåÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂ§öÊï∞Âä®ÊÄÅÂõæË°®Á§∫Â≠¶‰π†ÊñπÊ≥ïÂ∞ÜÂä®ÊÄÅÂõæÂàíÂàÜ‰∏∫Á¶ªÊï£Âø´ÁÖßÔºå‰ª•ÊçïÊçâËäÇÁÇπÈöèÊó∂Èó¥ÊºîÂèòÁöÑË°å‰∏∫„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈÄöËøáÊ∂àÊÅØ‰º†ÈÄíÂíåÈöèÊú∫Ê∏∏Ëµ∞ÊñπÊ≥ïÊçïÊçâÊØè‰∏™Âø´ÁÖß‰∏≠ËäÇÁÇπÁöÑÂ±ÄÈÉ®ÊàñÂÖ®Â±ÄÁªìÊûÑÔºåÈöèÂêéÂà©Áî®Âü∫‰∫éÂ∫èÂàóÁöÑÊ®°ÂûãÔºàÂ¶ÇÂèòÊç¢Âô®ÔºâÂØπËäÇÁÇπÂµåÂÖ•ÁöÑÊó∂Èó¥ÊºîÂèòËøõË°åÁºñÁ†ÅÔºåÂπ∂‰ΩøÁî®ÂÖÉÂ≠¶‰π†ÊäÄÊúØÊõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÂ≠òÂú®‰∏§‰∏™‰∏ªË¶ÅÂ±ÄÈôêÊÄßÔºö‰∏ÄÊòØÊú™ËÉΩÂêåÊó∂ÊèêÂèñÊØè‰∏™Âø´ÁÖß‰∏≠ÁöÑÂÖ®Â±ÄÂíåÂ±ÄÈÉ®‰ø°ÊÅØÔºõ‰∫åÊòØÂú®ÂèÇÊï∞Êõ¥Êñ∞Êó∂Êú™ËÄÉËôëÂΩìÂâçÂø´ÁÖßÁöÑÊ®°ÂûãÊÄßËÉΩÔºåÂØºËá¥Áº∫‰πèÊó∂Èó¥‰æùËµñÁÆ°ÁêÜ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïDyGSSMÔºåÁªìÂêàÂõæÂç∑ÁßØÁΩëÁªúÔºàGCNÔºâÂíåÈó®ÊéßÂæ™ÁéØÂçïÂÖÉÔºàGRUÔºâËøõË°åÁâπÂæÅÊèêÂèñÔºåÂπ∂ÂºïÂÖ•Âü∫‰∫éHiPPOÁÆóÊ≥ïÁöÑÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMÔºâÊù•Â§ÑÁêÜÈïøÊúü‰æùËµñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®20‰∏™Ê°à‰æã‰∏≠Êúâ17‰∏™Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫Á∫øÂíåÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âä®ÊÄÅÂõæË°®Á§∫Â≠¶‰π†‰∏≠‰ø°ÊÅØÊèêÂèñ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÂêåÊó∂ÊèêÂèñÂø´ÁÖß‰∏≠ÁöÑÂÖ®Â±ÄÂíåÂ±ÄÈÉ®‰ø°ÊÅØÔºåÂπ∂‰∏îÂú®ÂèÇÊï∞Êõ¥Êñ∞Êó∂Êú™ËÄÉËôëÂΩìÂâçÂø´ÁÖßÁöÑÊ®°ÂûãÊÄßËÉΩÔºåÁº∫‰πèÊó∂Èó¥‰æùËµñÁÆ°ÁêÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫ÁöÑDyGSSMÊñπÊ≥ïÈÄöËøáÁªìÂêàÂõæÂç∑ÁßØÁΩëÁªúÔºàGCNÔºâËøõË°åÂ±ÄÈÉ®ÁâπÂæÅÊèêÂèñÂíåÈó®ÊéßÂæ™ÁéØÂçïÂÖÉÔºàGRUÔºâËøõË°åÂÖ®Â±ÄÁâπÂæÅÊèêÂèñÔºåÂà©Áî®‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Êï¥ÂêàÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæÅÔºåÂêåÊó∂ÂºïÂÖ•Âü∫‰∫éHiPPOÁÆóÊ≥ïÁöÑÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMÔºâÊù•Â§ÑÁêÜÈïøÊúü‰æùËµñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDyGSSMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂ±ÄÈÉ®ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºàGCNÔºâ„ÄÅÂÖ®Â±ÄÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºàGRUÔºâÂíåÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÊõ¥Êñ∞Ê®°Âùó„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáGCNÊèêÂèñÊØè‰∏™Âø´ÁÖßÁöÑÂ±ÄÈÉ®ÁâπÂæÅÔºåÁÑ∂Âêé‰ΩøÁî®GRUÊèêÂèñÂÖ®Â±ÄÁâπÂæÅÔºåÊúÄÂêéÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Êï¥ÂêàËøô‰∫õÁâπÂæÅÔºåÂπ∂Âà©Áî®SSMËøõË°åÂèÇÊï∞Êõ¥Êñ∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDyGSSMÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂêåÊó∂ÊèêÂèñÂø´ÁÖß‰∏≠ÁöÑÂÖ®Â±ÄÂíåÂ±ÄÈÉ®‰ø°ÊÅØÔºåÂπ∂ÈÄöËøáÂºïÂÖ•HiPPOÁÆóÊ≥ï‰ºòÂåñÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÁöÑÂèÇÊï∞Êõ¥Êñ∞ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®ÊØè‰∏™Âø´ÁÖßÁöÑÊÄßËÉΩËÉΩÂ§üÂΩ±ÂìçÂêéÁª≠Êõ¥Êñ∞ÔºåËøôÂú®Áé∞ÊúâÊñπÊ≥ï‰∏≠ÊòØÊú™ÊõæÂÆûÁé∞ÁöÑ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®DyGSSM‰∏≠ÔºåGCNÂíåGRUÁöÑÁΩëÁªúÁªìÊûÑÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùÁâπÂæÅÊèêÂèñÁöÑÊúâÊïàÊÄßÔºõÊçüÂ§±ÂáΩÊï∞ÈááÁî®‰∫ÜÁªìÂêàÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæÅÁöÑÂ§çÂêàÊçüÂ§±Ôºå‰ª•ÊèêÂçáÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®‰∫î‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDyGSSMÂú®17‰∏™Ê°à‰æã‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫Á∫øÂíåÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Âä®ÊÄÅÂõæË°®Á§∫Â≠¶‰π†‰∏≠ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄßÔºåÂ±ïÁ§∫‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DyGSSMÊñπÊ≥ïÂú®Á§æ‰∫§ÁΩëÁªúÂàÜÊûê„ÄÅ‰∫§ÈÄöÊµÅÈáèÈ¢ÑÊµãÂíåÁîüÁâ©‰ø°ÊÅØÂ≠¶Á≠âÂ§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊúâÊïàÊçïÊçâÂä®ÊÄÅÂõæ‰∏≠ÁöÑËäÇÁÇπÊºîÂèòÂíåÂÖ≥Á≥ªÂèòÂåñÔºåËØ•ÊñπÊ≥ïËÉΩÂ§ü‰∏∫ÂÆûÊó∂ÂÜ≥Á≠ñÊèê‰æõÊîØÊåÅÔºåÂπ∂Êé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Most of the dynamic graph representation learning methods involve dividing a dynamic graph into discrete snapshots to capture the evolving behavior of nodes over time. Existing methods primarily capture only local or global structures of each node within a snapshot using message-passing and random walk-based methods. Then, they utilize sequence-based models (e.g., transformers) to encode the temporal evolution of node embeddings, and meta-learning techniques to update the model parameters. However, these approaches have two limitations. First, they neglect the extraction of global and local information simultaneously in each snapshot. Second, they fail to consider the model's performance in the current snapshot during parameter updates, resulting in a lack of temporal dependency management. Recently, HiPPO (High-order Polynomial Projection Operators) algorithm has gained attention for their ability to optimize and preserve sequence history in State Space Model (SSM). To address the aforementioned limitations in dynamic graph representation learning, we propose a novel method called Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. We then integrate the local and global features using a cross-attention mechanism. Additionally, we incorporate an SSM based on HiPPO algorithm to account for long-term dependencies when updating model parameters, ensuring that model performance in each snapshot informs subsequent updates. Experiments on five public datasets show that our method outperforms existing baseline and state-of-the-art (SOTA) methods in 17 out of 20 cases.

