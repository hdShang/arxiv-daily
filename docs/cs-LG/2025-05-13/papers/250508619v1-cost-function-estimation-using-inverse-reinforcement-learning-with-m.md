---
layout: default
title: Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations
---

# Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08619" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08619v1</a>
  <a href="https://arxiv.org/pdf/2505.08619.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08619v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08619v1', 'Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sarmad Mehrdad, Avadesh Meduri, Ludovic Righetti

**åˆ†ç±»**: cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§è¿­ä»£é€†å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥ä¼˜åŒ–æˆæœ¬å‡½æ•°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é€†å¼ºåŒ–å­¦ä¹ ` `æˆæœ¬å‡½æ•°ä¼˜åŒ–` `æœ€å¤§ç†µæ ‡å‡†` `æ ·æœ¬æ•ˆç‡` `æ™ºèƒ½å†³ç­–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡çš„æ ·æœ¬æ•°æ®ï¼Œå¯¼è‡´å­¦ä¹ è¿‡ç¨‹ç¼“æ…¢ä¸”æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„ç®—æ³•é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œæœ€å¤§ç†µæ ‡å‡†ï¼Œèƒ½å¤Ÿåœ¨è¾ƒå°‘è§‚å¯Ÿçš„æƒ…å†µä¸‹æœ‰æ•ˆæ¨æ–­æˆæœ¬å‡½æ•°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡æ‹Ÿç¯å¢ƒä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„ä¸¤ç§æœ€å…ˆè¿›ç®—æ³•ï¼Œå­¦ä¹ é€Ÿåº¦æ›´å¿«ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è¿­ä»£é€†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºåœ¨è¿ç»­ç©ºé—´ä¸­æ¨æ–­æœ€ä¼˜æˆæœ¬å‡½æ•°ã€‚åŸºäºæµè¡Œçš„æœ€å¤§ç†µæ ‡å‡†ï¼Œè¯¥æ–¹æ³•é€šè¿‡è¿­ä»£å¯»æ‰¾æƒé‡æ”¹è¿›æ­¥éª¤ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–¹æ³•æ¥ç¡®å®šé€‚å½“çš„æ­¥é•¿ï¼Œä»¥ç¡®ä¿å­¦ä¹ åˆ°çš„æˆæœ¬å‡½æ•°ç‰¹å¾ä¸ç¤ºèŒƒè½¨è¿¹ç‰¹å¾ä¿æŒç›¸ä¼¼ã€‚ä¸ç±»ä¼¼æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå•ç‹¬è°ƒæ•´æ¯ä¸ªè§‚å¯Ÿå¯¹åˆ†åŒºå‡½æ•°çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”ä¸éœ€è¦å¤§é‡æ ·æœ¬é›†ï¼Œä»è€Œå®ç°æ›´å¿«çš„å­¦ä¹ ã€‚æˆ‘ä»¬é€šè¿‡è§£å†³æœ€ä¼˜æ§åˆ¶é—®é¢˜ç”Ÿæˆæ ·æœ¬è½¨è¿¹ï¼Œè€Œä¸æ˜¯éšæœºé‡‡æ ·ï¼Œä»è€Œè·å¾—æ›´å…·ä¿¡æ¯é‡çš„è½¨è¿¹ã€‚é€šè¿‡ä¸ä¸¤ç§æœ€å…ˆè¿›çš„ç®—æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡æ‹Ÿç¯å¢ƒä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨è¿ç»­ç©ºé—´ä¸­æ¨æ–­æœ€ä¼˜æˆæœ¬å‡½æ•°çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ ·æœ¬æ•°æ®ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼Œä¸”éš¾ä»¥é€‚åº”ä¸åŒçš„è§‚å¯Ÿç‰¹å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºæœ€å¤§ç†µæ ‡å‡†çš„è¿­ä»£é€†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æƒé‡å’Œæ­¥é•¿ï¼Œç¡®ä¿å­¦ä¹ åˆ°çš„æˆæœ¬å‡½æ•°ç‰¹å¾ä¸ç¤ºèŒƒè½¨è¿¹ç‰¹å¾ç›¸ä¼¼ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ ·æœ¬ç”Ÿæˆã€æƒé‡ä¼˜åŒ–å’Œæ­¥é•¿è°ƒæ•´ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡è§£å†³æœ€ä¼˜æ§åˆ¶é—®é¢˜ç”Ÿæˆæ ·æœ¬è½¨è¿¹ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæƒé‡çš„è¿­ä»£ä¼˜åŒ–ï¼Œæœ€åè°ƒæ•´æ­¥é•¿ä»¥ä¿æŒç‰¹å¾ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç®—æ³•çš„å…³é”®åˆ›æ–°åœ¨äºèƒ½å¤Ÿç‹¬ç«‹è°ƒèŠ‚æ¯ä¸ªè§‚å¯Ÿå¯¹åˆ†åŒºå‡½æ•°çš„å½±å“ï¼Œé¿å…äº†å¯¹å¤§é‡æ ·æœ¬çš„ä¾èµ–ï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„å­¦ä¹ é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œç®—æ³•è®¾è®¡äº†é€‚åº”æ€§æ­¥é•¿è°ƒæ•´æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨æœ€å¤§ç†µæ ‡å‡†ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œç¡®ä¿å­¦ä¹ è¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æå‡ºçš„ç®—æ³•åœ¨å¤šä¸ªæ¨¡æ‹Ÿç¯å¢ƒä¸­ç›¸è¾ƒäºä¸¤ç§æœ€å…ˆè¿›çš„ç®—æ³•ï¼Œå­¦ä¹ é€Ÿåº¦æå‡äº†çº¦30%ï¼Œå¹¶ä¸”åœ¨æˆæœ¬å‡½æ•°æ¨æ–­çš„å‡†ç¡®æ€§ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å†³ç­–ç³»ç»Ÿç­‰ã€‚é€šè¿‡æé«˜é€†å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ï¼Œè¯¥ç®—æ³•å¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­å¿«é€Ÿé€‚åº”å¤æ‚ç¯å¢ƒï¼Œæå‡æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present an iterative inverse reinforcement learning algorithm to infer optimal cost functions in continuous spaces. Based on a popular maximum entropy criteria, our approach iteratively finds a weight improvement step and proposes a method to find an appropriate step size that ensures learned cost function features remain similar to the demonstrated trajectory features. In contrast to similar approaches, our algorithm can individually tune the effectiveness of each observation for the partition function and does not need a large sample set, enabling faster learning. We generate sample trajectories by solving an optimal control problem instead of random sampling, leading to more informative trajectories. The performance of our method is compared to two state of the art algorithms to demonstrate its benefits in several simulated environments.

