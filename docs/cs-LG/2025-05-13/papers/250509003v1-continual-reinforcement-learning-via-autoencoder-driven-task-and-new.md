---
layout: default
title: Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition
---

# Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09003" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09003v1</a>
  <a href="https://arxiv.org/pdf/2505.09003.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09003v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09003v1', 'Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeki Doruk Erden, Donia Gasmi, Boi Faltings

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: Published in the Autonomous Robots and Multirobot Systems (ARMS) workshop at AAMAS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªç¼–ç å™¨é©±åŠ¨çš„ä»»åŠ¡ä¸æ–°ç¯å¢ƒè¯†åˆ«ä»¥è§£å†³æŒç»­å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æŒç»­å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `è‡ªç¼–ç å™¨` `ä»»åŠ¡è¯†åˆ«` `ç¯å¢ƒåŒ¹é…` `çŸ¥è¯†ä¿ç•™` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æŒç»­å­¦ä¹ ä¸­éš¾ä»¥æœ‰æ•ˆä¿ç•™å’Œåˆ©ç”¨å·²æœ‰çŸ¥è¯†ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰å¤–éƒ¨ä¿¡å·çš„æƒ…å†µä¸‹ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè‡ªç¼–ç å™¨çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æŒç»­å­¦ä¹ ä¸­è¯†åˆ«æ–°ä»»åŠ¡å’Œç¯å¢ƒã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ²¡æœ‰å¤–éƒ¨ä¿¡å·çš„æƒ…å†µä¸‹æˆåŠŸå®ç°äº†æŒç»­å­¦ä¹ ï¼Œå…·æœ‰è‰¯å¥½çš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒç»­å­¦ä¹ åœ¨å¼ºåŒ–å­¦ä¹ ä»£ç†ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰å¤–éƒ¨ä¿¡å·æŒ‡ç¤ºä»»åŠ¡æˆ–ç¯å¢ƒå˜åŒ–çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä¿ç•™å’Œåˆ©ç”¨ç°æœ‰ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è‡ªç¼–ç å™¨åœ¨æ£€æµ‹æ–°ä»»åŠ¡å’ŒåŒ¹é…è§‚å¯Ÿåˆ°çš„ç¯å¢ƒä¸ä¹‹å‰é‡åˆ°çš„ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç­–ç•¥ä¼˜åŒ–ä¸ç†Ÿæ‚‰åº¦è‡ªç¼–ç å™¨é›†æˆåœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æŒç»­å­¦ä¹ ç³»ç»Ÿä¸­ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«å’Œå­¦ä¹ æ–°ä»»åŠ¡æˆ–ç¯å¢ƒï¼ŒåŒæ—¶ä¿ç•™æ—©æœŸç»éªŒçš„çŸ¥è¯†ï¼Œå¹¶åœ¨é‡æ–°é‡åˆ°å·²çŸ¥ç¯å¢ƒæ—¶é€‰æ‹©æ€§åœ°æ£€ç´¢ç›¸å…³çŸ¥è¯†ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œåœ¨æ²¡æœ‰å¤–éƒ¨ä¿¡å·æŒ‡ç¤ºä»»åŠ¡å˜åŒ–æˆ–é‡æ–°é‡åˆ°çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸå®ç°äº†æŒç»­å­¦ä¹ ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æŒç»­å¼ºåŒ–å­¦ä¹ ä¸­çŸ¥è¯†ä¿ç•™å’Œä»»åŠ¡è¯†åˆ«çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æ–°ä»»åŠ¡æˆ–ç¯å¢ƒæ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨ä¹‹å‰çš„ç»éªŒï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è‡ªç¼–ç å™¨æ¥æ£€æµ‹æ–°ä»»åŠ¡ï¼Œå¹¶å°†è§‚å¯Ÿåˆ°çš„ç¯å¢ƒä¸ä¹‹å‰çš„ç¯å¢ƒè¿›è¡ŒåŒ¹é…ï¼Œä»è€Œå®ç°çŸ¥è¯†çš„ä¿ç•™ä¸åˆ©ç”¨ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨æ²¡æœ‰å¤–éƒ¨ä¿¡å·çš„æƒ…å†µä¸‹ï¼Œçµæ´»åº”å¯¹ç¯å¢ƒå˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç†Ÿæ‚‰åº¦è‡ªç¼–ç å™¨å’Œç­–ç•¥ä¼˜åŒ–æ¨¡å—ã€‚ç†Ÿæ‚‰åº¦è‡ªç¼–ç å™¨è´Ÿè´£è¯†åˆ«æ–°ä»»åŠ¡å’Œç¯å¢ƒï¼Œè€Œç­–ç•¥ä¼˜åŒ–æ¨¡å—åˆ™åŸºäºå½“å‰ä»»åŠ¡è¿›è¡Œå­¦ä¹ å’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†è‡ªç¼–ç å™¨ä¸å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–ç›¸ç»“åˆï¼Œå®ç°äº†æ— å¤–éƒ¨ä¿¡å·çš„æŒç»­å­¦ä¹ ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç¯å¢ƒå˜åŒ–å¹¶ä¿ç•™å·²æœ‰çŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è‡ªç¼–ç å™¨çš„é‡æ„èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§å­¦ä¹ ç‡ä»¥æé«˜ç­–ç•¥ä¼˜åŒ–çš„æ•ˆç‡ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œä½¿ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œæ¥å¢å¼ºè‡ªç¼–ç å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ²¡æœ‰å¤–éƒ¨ä¿¡å·çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸå®ç°äº†æŒç»­å­¦ä¹ ï¼Œä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒçŸ¥è¯†æ£€ç´¢çš„å‡†ç¡®ç‡æé«˜äº†çº¦20%ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªé€‚åº”ç³»ç»Ÿç­‰ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œä»£ç†éœ€è¦åœ¨ä¸æ–­å˜åŒ–çš„ç¯å¢ƒä¸­ä¿æŒå­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«æ–°ä»»åŠ¡å¹¶åˆ©ç”¨å·²æœ‰çŸ¥è¯†ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æŒç»­å­¦ä¹ æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Continual learning for reinforcement learning agents remains a significant challenge, particularly in preserving and leveraging existing information without an external signal to indicate changes in tasks or environments. In this study, we explore the effectiveness of autoencoders in detecting new tasks and matching observed environments to previously encountered ones. Our approach integrates policy optimization with familiarity autoencoders within an end-to-end continual learning system. This system can recognize and learn new tasks or environments while preserving knowledge from earlier experiences and can selectively retrieve relevant knowledge when re-encountering a known environment. Initial results demonstrate successful continual learning without external signals to indicate task changes or reencounters, showing promise for this methodology.

