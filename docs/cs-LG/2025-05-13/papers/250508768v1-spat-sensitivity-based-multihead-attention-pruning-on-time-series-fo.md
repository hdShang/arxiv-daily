---
layout: default
title: "SPAT: Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models"
---

# SPAT: Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08768v1</a>
  <a href="https://arxiv.org/pdf/2505.08768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08768v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08768v1', 'SPAT: Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Suhan Guo, Jiahong Deng, Mengjun Yi, Furao Shen, Jian Zhao

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSPATæ–¹æ³•ä»¥ä¼˜åŒ–æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„è®¡ç®—æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—é¢„æµ‹` `æ³¨æ„åŠ›æœºåˆ¶` `æ¨¡å‹å‰ªæ` `è®¡ç®—æ•ˆç‡` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­è™½ç„¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ï¼Œå¯¼è‡´æ¨¡å‹æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºSPATæ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–å‰ªæé€‰æ‹©æ€§å»é™¤å†—ä½™çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œé™ä½æ¨¡å‹å¤æ‚åº¦å¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSPATå‰ªææ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”åœ¨æ ‡å‡†å’Œé›¶æ ·æœ¬æ¨ç†ä¸­è¶…è¶Šäº†ç°æœ‰çš„è½»é‡çº§æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºæ³¨æ„åŠ›çš„æ¶æ„åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–å‰ªææ–¹æ³•SPATï¼ˆæ•æ„Ÿæ€§å‰ªææ³¨æ„åŠ›ï¼‰ï¼Œè¯¥æ–¹æ³•é€‰æ‹©æ€§åœ°å»é™¤å†—ä½™çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œç”Ÿæˆé«˜æ•ˆæ¨¡å‹ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒSPATæ—¨åœ¨å®Œå…¨ç§»é™¤æ•´ä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ï¼Œå¹¶åœ¨ä¸ä¾èµ–ä¸“ç”¨ç¡¬ä»¶çš„æƒ…å†µä¸‹åŠ é€Ÿæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€æ•æ„Ÿæ€§åº¦é‡â€”â€”æ•æ„Ÿæ€§å¢å¼ºå½’ä¸€åŒ–ç¦»æ•£åº¦ï¼ˆSENDï¼‰ï¼Œç”¨äºåœ¨é¢„è®­ç»ƒé˜¶æ®µè¯„ä¼°æ¯ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„é‡è¦æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPATå‰ªææ¨¡å‹åœ¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä¸Šå‡å°‘äº†2.842%ï¼Œåœ¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸Šå‡å°‘äº†1.996%ï¼Œå¹¶åœ¨æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰ä¸Šå‡å°‘äº†35.274%ã€‚æ­¤å¤–ï¼ŒSPATå‰ªææ¨¡å‹åœ¨æ ‡å‡†å’Œé›¶æ ·æœ¬æ¨ç†ä¸­å‡ä¼˜äºç°æœ‰çš„è½»é‡çº§ã€åŸºäºMambaå’ŒLLMçš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œçªæ˜¾äº†ä¿ç•™æœ€æœ‰æ•ˆæ³¨æ„åŠ›æœºåˆ¶çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰çš„æ³¨æ„åŠ›æœºåˆ¶è™½ç„¶åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å…¶é«˜è®¡ç®—å¼€é”€å’Œå¤æ‚æ€§é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ç°æœ‰çš„å‰ªææŠ€æœ¯å¾€å¾€æ— æ³•æœ‰æ•ˆå»é™¤å†—ä½™æ¨¡å—ï¼Œå¯¼è‡´æ¨¡å‹ä»ç„¶è¾ƒä¸ºåºå¤§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPATæ–¹æ³•é€šè¿‡åŠ¨æ€è¯„ä¼°æ¯ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„é‡è¦æ€§ï¼Œé€‰æ‹©æ€§åœ°å»é™¤å†—ä½™æ¨¡å—ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ¨¡å‹å‰ªæã€‚æ­¤æ–¹æ³•ä¸ä»…é™ä½äº†è¿‡æ‹Ÿåˆé£é™©ï¼Œè¿˜èƒ½åœ¨ä¸ä¾èµ–ä¸“ç”¨ç¡¬ä»¶çš„æƒ…å†µä¸‹åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPATçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒé˜¶æ®µçš„æ•æ„Ÿæ€§è¯„ä¼°å’Œå‰ªæé˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡SENDåº¦é‡æ¯ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„æ•æ„Ÿæ€§ï¼Œç„¶åæ ¹æ®è¯„ä¼°ç»“æœé€‰æ‹©æ€§å»é™¤å†—ä½™æ¨¡å—ï¼Œæœ€ç»ˆå¾—åˆ°ä¼˜åŒ–åçš„æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPATçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†SENDåº¦é‡ï¼Œèƒ½å¤ŸåŠ¨æ€è¯„ä¼°æ³¨æ„åŠ›æ¨¡å—çš„é‡è¦æ€§ï¼Œå¹¶åœ¨å‰ªæè¿‡ç¨‹ä¸­å®Œå…¨ç§»é™¤å†—ä½™æ¨¡å—ï¼Œè¿™ä¸ä»¥å¾€æ–¹æ³•çš„éƒ¨åˆ†å‰ªæç­–ç•¥å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒSENDåº¦é‡çš„è®¡ç®—æ–¹å¼å’Œå‰ªæç­–ç•¥çš„é€‰æ‹©è‡³å…³é‡è¦ã€‚é€šè¿‡åˆç†è®¾ç½®å‰ªæé˜ˆå€¼å’ŒæŸå¤±å‡½æ•°ï¼Œç¡®ä¿å‰ªæåçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä»ç„¶ä¿æŒç«äº‰åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSPATå‰ªææ¨¡å‹åœ¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä¸Šå‡å°‘äº†2.842%ï¼Œåœ¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸Šå‡å°‘äº†1.996%ï¼Œå¹¶åœ¨æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰ä¸Šå‡å°‘äº†35.274%ã€‚æ­¤å¤–ï¼ŒSPATæ–¹æ³•åœ¨æ ‡å‡†å’Œé›¶æ ·æœ¬æ¨ç†ä¸­å‡è¶…è¶Šäº†ç°æœ‰çš„è½»é‡çº§ã€åŸºäºMambaå’ŒLLMçš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é‡‘èå¸‚åœºé¢„æµ‹ã€æ°”è±¡æ•°æ®åˆ†æå’Œæ™ºèƒ½åˆ¶é€ ç­‰å¤šç§æ—¶é—´åºåˆ—é¢„æµ‹åœºæ™¯ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼ŒSPATæ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°å®æ—¶é¢„æµ‹ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Attention-based architectures have achieved superior performance in multivariate time series forecasting but are computationally expensive. Techniques such as patching and adaptive masking have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method, SPAT ($\textbf{S}$ensitivity $\textbf{P}$runer for $\textbf{At}$tention), which selectively removes redundant attention mechanisms and yields highly effective models. Different from previous approaches, SPAT aims to remove the entire attention module, which reduces the risk of overfitting and enables speed-up without demanding specialized hardware. We propose a dynamic sensitivity metric, $\textbf{S}$ensitivity $\textbf{E}$nhanced $\textbf{N}$ormalized $\textbf{D}$ispersion (SEND) that measures the importance of each attention module during the pre-training phase. Experiments on multivariate datasets demonstrate that SPAT-pruned models achieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs. Furthermore, SPAT-pruned models outperform existing lightweight, Mamba-based and LLM-based SOTA methods in both standard and zero-shot inference, highlighting the importance of retaining only the most effective attention mechanisms. We have made our code publicly available https://anonymous.4open.science/r/SPAT-6042.

