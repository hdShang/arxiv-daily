---
layout: default
title: "ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse"
---

# ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2509.23183" class="toolbar-btn" target="_blank">üìÑ arXiv: 2509.23183v1</a>
  <a href="https://arxiv.org/pdf/2509.23183.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2509.23183v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2509.23183v1', 'ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Guohao Chen, Shuaicheng Niu, Deyu Chen, Jiahao Yang, Zitian Zhang, Mingkui Tan, Pengcheng Wu, Zhiqi Shen

**ÂàÜÁ±ª**: cs.LG, cs.NI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-09-27

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ZeroSiamÔºåÈÄöËøáSiameseÊû∂ÊûÑÂíåÁÜµ‰ºòÂåñËß£ÂÜ≥ÊµãËØïÊó∂Ê®°ÂûãÂùçÂ°åÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÊµãËØïÊó∂Ëá™ÈÄÇÂ∫î` `ÁÜµÊúÄÂ∞èÂåñ` `SiameseÁΩëÁªú` `Ê®°ÂûãÂùçÂ°å` `ÈùûÂØπÁß∞Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÊµãËØïÊó∂ÁÜµÊúÄÂ∞èÂåñËÉΩ‰ΩøÊ®°ÂûãÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÊòìÂØºËá¥Ê®°ÂûãÂùçÂ°å„ÄÇ
2. ZeroSiamÈÄöËøáÈùûÂØπÁß∞SiameseÊû∂ÊûÑÂíåÊï£Â∫¶ÂØπÈΩêÔºåÊúâÊïàÈò≤Ê≠¢Ê®°ÂûãÂùçÂ°åÔºåÂπ∂Ê≠£ÂàôÂåñÂ≠¶‰π†‰ø°Âè∑„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåZeroSiamÂú®ËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°‰∏äÔºåÂØπÂ§öÁßçÊ®°ÂûãÂùáË°®Áé∞Âá∫Á®≥ÂÆö‰∏î‰ºòË∂äÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫ZeroSiamÔºå‰∏ÄÁßçÈ´òÊïàÁöÑÈùûÂØπÁß∞SiameseÊû∂ÊûÑÔºå‰∏ì‰∏∫ÊµãËØïÊó∂ÁÜµÊúÄÂ∞èÂåñËÆæËÆ°„ÄÇÁ∫ØÁ≤πÁöÑÁÜµÊúÄÂ∞èÂåñÂèØËÉΩÂÄæÂêë‰∫éÈùûÊ≥õÂåñÁöÑÊç∑ÂæÑÔºå‰æãÂ¶ÇËÜ®ËÉÄlogitËåÉÊï∞Âπ∂Â∞ÜÊâÄÊúâÈ¢ÑÊµãÈ©±Âä®Âà∞‰∏ªÂØºÁ±ªÂà´‰ª•ÂáèÂ∞ëÁÜµÔºå‰ªéËÄåÂØºËá¥Ê®°ÂûãÂùçÂ°å„ÄÇZeroSiamÈÄöËøáÈùûÂØπÁß∞Êï£Â∫¶ÂØπÈΩêÊù•Èò≤Ê≠¢ÂùçÂ°åÔºåËøôÈÄöËøáÂèØÂ≠¶‰π†ÁöÑÈ¢ÑÊµãÂô®ÂíåÂàÜÁ±ªÂô®ÂâçÁöÑÂÅúÊ≠¢Ê¢ØÂ∫¶ÁÆóÂ≠êÊúâÊïàÂÆûÁé∞„ÄÇÁªèÈ™åÂíåÁêÜËÆ∫ËØÅÊçÆË°®ÊòéÔºåZeroSiam‰∏ç‰ªÖÂèØ‰ª•Èò≤Ê≠¢ÂùçÂ°åÔºåËøòÂèØ‰ª•Âê∏Êî∂ÂíåÊ≠£ÂàôÂåñÊúâÂÅèÁöÑÂ≠¶‰π†‰ø°Âè∑Ôºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩÔºåÂç≥‰ΩøÊ≤°ÊúâÂèëÁîüÂùçÂ°å„ÄÇÂ§ßÈáèÁªìÊûúË°®ÊòéÔºåZeroSiamÂú®‰ΩøÁî®ÂèØÂøΩÁï•ÁöÑÂºÄÈîÄÁöÑÊÉÖÂÜµ‰∏ãÔºåÊØîÂÖàÂâçÁöÑÊñπÊ≥ïË°®Áé∞Êõ¥Á®≥ÂÆöÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊµãËØïÂú∫ÊôØÂíåÂêÑÁßçÊ®°ÂûãÔºàÂåÖÊã¨ÁâπÂà´ÂÆπÊòìÂèëÁîüÂùçÂ°åÁöÑÂ∞èÂûãÊ®°ÂûãÔºâ‰∏äÁöÑËßÜËßâÈÄÇÂ∫îÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÊó®Âú®Âà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑÈ¢ÑÊµãÂú®Êé®ÁêÜÈò∂ÊÆµÊåÅÁª≠ÊîπËøõÔºå‰ΩÜÁõ¥Êé•ÊúÄÂ∞èÂåñÈ¢ÑÊµãÁÜµÂÆπÊòìÂØºËá¥Ê®°ÂûãÂùçÂ°åÔºåÂç≥ËæìÂá∫ÊÅíÂÆöÁöÑone-hotÂêëÈáèÔºå‰ªéËÄåËé∑ÂæóÊûÅ‰ΩéÁöÑÁÜµÂÄºÔºå‰ΩÜ‰∏ßÂ§±‰∫ÜÊ≥õÂåñËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÈò≤Ê≠¢ËøôÁßçÂùçÂ°åÁé∞Ë±°ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∞èÂûãÊ®°Âûã‰∏äÊõ¥‰∏∫ÊòéÊòæ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöZeroSiamÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂºïÂÖ•ÈùûÂØπÁß∞ÁöÑSiameseÁΩëÁªúÁªìÊûÑÔºåÈÄöËøá‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÈ¢ÑÊµãÂô®Âíå‰∏Ä‰∏™ÂÅúÊ≠¢Ê¢ØÂ∫¶Êìç‰ΩúÔºåÂÆûÁé∞ÈùûÂØπÁß∞ÁöÑÊï£Â∫¶ÂØπÈΩê„ÄÇËøôÁßçËÆæËÆ°ËÉΩÂ§üÊúâÊïàÂú∞Èò≤Ê≠¢Ê®°ÂûãÂùçÂ°åÔºåÂêåÊó∂ËøòËÉΩÂê∏Êî∂ÂíåÊ≠£ÂàôÂåñÊúâÂÅèÁöÑÂ≠¶‰π†‰ø°Âè∑Ôºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöZeroSiamÂåÖÂê´‰∏§‰∏™ÂàÜÊîØÔºå‰∏Ä‰∏™ÂàÜÊîØÊòØÂéüÂßãÊ®°ÂûãÔºåÂè¶‰∏Ä‰∏™ÂàÜÊîØÊòØÂ∏¶ÊúâÂèØÂ≠¶‰π†È¢ÑÊµãÂô®ÁöÑÊ®°Âûã„ÄÇÂéüÂßãÊ®°ÂûãÁöÑËæìÂá∫ÁªèËøáÂàÜÁ±ªÂô®ÂæóÂà∞È¢ÑÊµãÁªìÊûúÔºåËÄåÂè¶‰∏Ä‰∏™ÂàÜÊîØÁöÑËæìÂá∫ÁªèËøáÈ¢ÑÊµãÂô®ÂêéÔºå‰∏éÂéüÂßãÊ®°ÂûãÁöÑËæìÂá∫ËøõË°åÊï£Â∫¶ÂØπÈΩê„ÄÇÂú®È¢ÑÊµãÂô®‰πãÂâçÔºå‰ΩøÁî®ÂÅúÊ≠¢Ê¢ØÂ∫¶Êìç‰ΩúÔºåÈòªÊ≠¢Ê¢ØÂ∫¶‰ªéÂéüÂßãÊ®°ÂûãÊµÅÂêëÈ¢ÑÊµãÂô®Ôºå‰ªéËÄåÂÆûÁé∞ÈùûÂØπÁß∞ÊÄß„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÂú®ÊµãËØïÊó∂ÔºåÂà©Áî®È¢ÑÊµãÁªìÊûúÁöÑÁÜµÂíåÊï£Â∫¶ÂØπÈΩêÊçüÂ§±Êù•Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöZeroSiamÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÈùûÂØπÁß∞ÁöÑSiameseÊû∂ÊûÑÂíåÈùûÂØπÁß∞Êï£Â∫¶ÂØπÈΩêÊñπÂºè„ÄÇ‰º†ÁªüÁöÑSiameseÁΩëÁªúÈÄöÂ∏∏ÊòØÂØπÁß∞ÁöÑÔºåËÄåZeroSiamÈÄöËøáÂºïÂÖ•ÂèØÂ≠¶‰π†ÁöÑÈ¢ÑÊµãÂô®ÂíåÂÅúÊ≠¢Ê¢ØÂ∫¶Êìç‰ΩúÔºåÊâìÁ†¥‰∫ÜËøôÁßçÂØπÁß∞ÊÄßÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Èò≤Ê≠¢‰∫ÜÊ®°ÂûãÂùçÂ°å„ÄÇÊ≠§Â§ñÔºåÈùûÂØπÁß∞ÁöÑÊï£Â∫¶ÂØπÈΩêËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âê∏Êî∂ÂíåÊ≠£ÂàôÂåñÊúâÂÅèÁöÑÂ≠¶‰π†‰ø°Âè∑„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöZeroSiamÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÂèØÂ≠¶‰π†ÁöÑÈ¢ÑÊµãÂô®ÔºåÁî®‰∫éÂ∞Ü‰∏Ä‰∏™ÂàÜÊîØÁöÑËæìÂá∫Êò†Â∞ÑÂà∞Âè¶‰∏Ä‰∏™ÂàÜÊîØÁöÑËæìÂá∫Á©∫Èó¥Ôºõ2) ÂÅúÊ≠¢Ê¢ØÂ∫¶Êìç‰ΩúÔºåÈòªÊ≠¢Ê¢ØÂ∫¶‰ªéÂéüÂßãÊ®°ÂûãÊµÅÂêëÈ¢ÑÊµãÂô®ÔºåÂÆûÁé∞ÈùûÂØπÁß∞ÊÄßÔºõ3) Êï£Â∫¶ÂØπÈΩêÊçüÂ§±ÔºåÁî®‰∫éË°°Èáè‰∏§‰∏™ÂàÜÊîØËæìÂá∫‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂπ∂‰øÉ‰ΩøÊ®°ÂûãÂ≠¶‰π†Âà∞Êõ¥Âä†È≤ÅÊ£íÁöÑÁâπÂæÅË°®Á§∫„ÄÇÊçüÂ§±ÂáΩÊï∞ÈÄöÂ∏∏ÂåÖÂê´ÁÜµÊçüÂ§±ÂíåÊï£Â∫¶ÂØπÈΩêÊçüÂ§±‰∏§ÈÉ®ÂàÜÔºåÈÄöËøáË∞ÉÊï¥‰∏§ËÄÖÁöÑÊùÉÈáçÊù•Âπ≥Ë°°ÁÜµÊúÄÂ∞èÂåñÂíåÈò≤Ê≠¢ÂùçÂ°å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåZeroSiamÂú®ËßÜËßâÈÄÇÂ∫îÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ‰ªªÂä°‰∏äÂùáË°®Áé∞Âá∫Ëâ≤„ÄÇÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÔºåZeroSiamÁõ∏ÊØî‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂú®Èò≤Ê≠¢Ê®°ÂûãÂùçÂ°åÁöÑÂêåÊó∂ÔºåÂèñÂæó‰∫ÜÊõ¥È´òÁöÑÂáÜÁ°ÆÁéá„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Â∞èÂûãÊ®°Âûã‰∏äÔºåZeroSiamÁöÑ‰ºòÂäøÊõ¥Âä†ÊòéÊòæÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ZeroSiamÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂõæÂÉèËØÜÂà´„ÄÅÁõÆÊ†áÊ£ÄÊµã„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠â„ÄÇÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆæÂ§áÊàñÂ∞èÂûãÊ®°ÂûãÔºåËÉΩÂ§üÊèêÂçáÊ®°ÂûãÂú®Êú™Áü•ÁéØÂ¢É‰∏ãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂú®ÂåªÁñóËØäÊñ≠„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®ÁöÑÂ∫îÁî®‰ª∑ÂÄºÔºåÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÂú®Â§çÊùÇÂíåÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂèØÈù†ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Test-time entropy minimization helps adapt a model to novel environments and incentivize its reasoning capability, unleashing the model's potential during inference by allowing it to evolve and improve in real-time using its own predictions, achieving promising performance. However, pure entropy minimization can favor non-generalizable shortcuts, such as inflating the logit norm and driving all predictions to a dominant class to reduce entropy, risking collapsed solutions (e.g., constant one-hot outputs) that trivially minimize the objective without meaningful learning. In this paper, we introduce ZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time entropy minimization. ZeroSiam prevents collapse through asymmetric divergence alignment, which is efficiently achieved by a learnable predictor and a stop-gradient operator before the classifier. We provide empirical and theoretical evidence that ZeroSiam not only prevents collapse solutions, but also absorbs and regularizes biased learning signals, enhancing performance even when no collapse occurs. Despite its simplicity, extensive results show that ZeroSiam performs more stably over prior methods using negligible overhead, demonstrating efficacy on both vision adaptation and large language model reasoning tasks across challenging test scenarios and diverse models, including tiny models that are particularly collapse-prone.

