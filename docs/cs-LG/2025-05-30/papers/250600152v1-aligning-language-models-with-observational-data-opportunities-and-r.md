---
layout: default
title: "Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective"
---

# Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00152" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00152v1</a>
  <a href="https://arxiv.org/pdf/2506.00152.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00152v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00152v1', 'Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Erfan Loghmani

**åˆ†ç±»**: cs.LG, econ.EM, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: 10+12 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeconfoundLMä»¥è§£å†³è§‚å¯Ÿæ•°æ®å¯¹è¯­è¨€æ¨¡å‹å¾®è°ƒçš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `è§‚å¯Ÿæ•°æ®` `å› æœæ¨æ–­` `å¾®è°ƒæ–¹æ³•` `æ··æ‚å› ç´ ` `å†…å®¹ç”Ÿæˆ` `å¸‚åœºè¥é”€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨ä¸äººç±»åå¥½å’Œå•†ä¸šç›®æ ‡å¯¹é½æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œç›´æ¥å¾®è°ƒå¯èƒ½å¯¼è‡´è™šå‡ç›¸å…³æ€§ã€‚
2. è®ºæ–‡æå‡ºDeconfoundLMæ–¹æ³•ï¼Œé€šè¿‡æ˜¾å¼å»é™¤æ··æ‚å› ç´ çš„å½±å“ï¼Œæ”¹å–„è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ•ˆæœã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDeconfoundLMåœ¨æ¢å¤å› æœå…³ç³»æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„å¤±è´¥æ¨¡å¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„è¡Œä¸šè¢«å¹¿æ³›åº”ç”¨äºç”Ÿæˆå†…å®¹ï¼Œä»¥ç›´æ¥å½±å“å…³é”®ç»©æ•ˆæŒ‡æ ‡ï¼Œå¦‚è½¬åŒ–ç‡ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸äººç±»åå¥½æˆ–å•†ä¸šç›®æ ‡å¯¹é½æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œå› æ­¤éœ€è¦é€šè¿‡é«˜è´¨é‡æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒã€‚å°½ç®¡æ§åˆ¶å®éªŒï¼ˆå¦‚A/Bæµ‹è¯•ï¼‰èƒ½å¤Ÿæä¾›æ­¤ç±»æ•°æ®ï¼Œä½†å…¶æˆæœ¬é«˜æ˜‚ä¸”é¢ä¸´å·¥ç¨‹å’Œåå‹¤æŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨è§‚å¯Ÿæ•°æ®å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ä¸æœºé‡ï¼ŒæŒ‡å‡ºç›´æ¥åœ¨è§‚å¯Ÿæ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹å¯èƒ½å¯¼è‡´å­¦ä¹ è™šå‡ç›¸å…³æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDeconfoundLMçš„æ–¹æ³•ï¼Œæ˜¾å¼å»é™¤å·²çŸ¥æ··æ‚å› ç´ å¯¹å¥–åŠ±ä¿¡å·çš„å½±å“ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿå®éªŒå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ¢å¤å› æœå…³ç³»å’Œå‡è½»å¾®è°ƒæ–¹æ³•å¤±è´¥æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡è§‚å¯Ÿæ•°æ®å­˜åœ¨é£é™©ï¼Œä½†é€šè¿‡é€‚å½“çš„å› æœä¿®æ­£ï¼Œå®ƒå¯ä»¥æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½çš„å¼ºå¤§ä¿¡å·æ¥æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨è§‚å¯Ÿæ•°æ®è¿›è¡Œå¾®è°ƒæ—¶å¯èƒ½å¯¼è‡´çš„è™šå‡ç›¸å…³æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æœªèƒ½æœ‰æ•ˆå¤„ç†æ··æ‚å› ç´ ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„DeconfoundLMæ–¹æ³•é€šè¿‡æ˜¾å¼å»é™¤å·²çŸ¥æ··æ‚å› ç´ çš„å½±å“ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å‡†ç¡®åœ°å­¦ä¹ å› æœå…³ç³»ï¼Œè€Œéè™šå‡ç›¸å…³æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ··æ‚å› ç´ è¯†åˆ«ã€å¥–åŠ±ä¿¡å·ä¿®æ­£å’Œæ¨¡å‹å¾®è°ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†æå†å²æ•°æ®è¯†åˆ«æ··æ‚å› ç´ ï¼Œç„¶ååœ¨å¥–åŠ±ä¿¡å·ä¸­è¿›è¡Œä¿®æ­£ï¼Œæœ€åè¿›è¡Œæ¨¡å‹çš„å¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDeconfoundLMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ˜¾å¼å»é™¤æ··æ‚å› ç´ çš„æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„éšå¼å¤„ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹å› æœå…³ç³»çš„å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒDeconfoundLMé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥é‡åŒ–æ··æ‚å› ç´ çš„å½±å“ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–ç®—æ³•è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¿æŒå¯¹å› æœå…³ç³»çš„æ•æ„Ÿæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeconfoundLMåœ¨æ¢å¤å› æœå…³ç³»æ–¹é¢çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹åœ¨å¤„ç†æ··æ‚å› ç´ æ—¶çš„å¤±è´¥ç‡ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰è¾ƒé«˜çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†…å®¹ç”Ÿæˆã€å¸‚åœºè¥é”€å’Œç”¨æˆ·è¡Œä¸ºåˆ†æç­‰ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨è§‚å¯Ÿæ•°æ®ï¼Œä¼ä¸šå¯ä»¥æ›´å¥½åœ°å¯¹é½è¯­è¨€æ¨¡å‹ä¸ç”¨æˆ·éœ€æ±‚ï¼Œä»è€Œæå‡è½¬åŒ–ç‡å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸæ¨å¹¿ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„ä¼˜åŒ–ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models are being widely used across industries to generate content that contributes directly to key performance metrics, such as conversion rates. Pretrained models, however, often fall short when it comes to aligning with human preferences or optimizing for business objectives. As a result, fine-tuning with good-quality labeled data is essential to guide models to generate content that achieves better results. Controlled experiments, like A/B tests, can provide such data, but they are often expensive and come with significant engineering and logistical challenges. Meanwhile, companies have access to a vast amount of historical (observational) data that remains underutilized. In this work, we study the challenges and opportunities of fine-tuning LLMs using observational data. We show that while observational outcomes can provide valuable supervision, directly fine-tuning models on such data can lead them to learn spurious correlations. We present empirical evidence of this issue using various real-world datasets and propose DeconfoundLM, a method that explicitly removes the effect of known confounders from reward signals. Using simulation experiments, we demonstrate that DeconfoundLM improves the recovery of causal relationships and mitigates failure modes found in fine-tuning methods that ignore or naively incorporate confounding variables. Our findings highlight that while observational data presents risks, with the right causal corrections, it can be a powerful source of signal for LLM alignment. Please refer to the project page for code and related resources.

