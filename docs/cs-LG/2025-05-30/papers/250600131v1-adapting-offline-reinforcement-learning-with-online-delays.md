---
layout: default
title: Adapting Offline Reinforcement Learning with Online Delays
---

# Adapting Offline Reinforcement Learning with Online Delays

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00131" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00131v1</a>
  <a href="https://arxiv.org/pdf/2506.00131.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00131v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00131v1', 'Adapting Offline Reinforcement Learning with Online Delays')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Simon Sinong Zhan, Qingyuan Wu, Frank Yang, Xiangyu Shi, Chao Huang, Qi Zhu

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDT-CORLä»¥è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„å»¶è¿Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `å»¶è¿ŸåŠ¨æ€` `ä¿¡å¿µé¢„æµ‹` `å˜æ¢å™¨` `æ ·æœ¬æ•ˆç‡` `æ™ºèƒ½å†³ç­–` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹ç°å®ç¯å¢ƒä¸­çš„å»¶è¿Ÿå’Œä¸ç¡®å®šæ€§æ—¶ï¼Œè¡¨ç°å‡ºè¾ƒå·®çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. DT-CORLé€šè¿‡å¼•å…¥åŸºäºå˜æ¢å™¨çš„ä¿¡å¿µé¢„æµ‹å™¨ï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡å»¶è¿Ÿè§‚æµ‹çš„æƒ…å†µä¸‹ç”Ÿæˆå»¶è¿Ÿç¨³å¥çš„åŠ¨ä½œï¼Œä»è€Œæœ‰æ•ˆåº”å¯¹å»¶è¿ŸåŠ¨æ€ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDT-CORLåœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨ä¸åŒå»¶è¿Ÿè®¾ç½®ä¸‹å‡ä¼˜äºå†å²å¢å¼ºå’Œæ™®é€šä¿¡å¿µæ–¹æ³•ï¼Œæå‡äº†æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç¦»çº¿åˆ°åœ¨çº¿çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†éœ€è¦è§£å†³ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ï¼Œç°å®ç³»ç»Ÿä¸­å­˜åœ¨å»¶è¿Ÿå’Œå…¶ä»–ä¸å®Œç¾å› ç´ ï¼›äºŒæ˜¯äº¤äº’å·®è·ï¼Œçº¯ç¦»çº¿è®­ç»ƒçš„ç­–ç•¥åœ¨åœ¨çº¿æ‰§è¡Œæ—¶é¢ä¸´åˆ†å¸ƒå¤–çŠ¶æ€ã€‚ä¸ºæ­¤ï¼Œä»£ç†å¿…é¡»ä»é™æ€ã€æ— å»¶è¿Ÿçš„æ•°æ®é›†ä¸­æ¨å¹¿åˆ°åŠ¨æ€ã€æ˜“å—å»¶è¿Ÿå½±å“çš„ç¯å¢ƒã€‚æ ‡å‡†çš„ç¦»çº¿RLä»æ— å»¶è¿Ÿçš„æ—¥å¿—ä¸­å­¦ä¹ ï¼Œä½†åœ¨å»¶è¿Ÿä¸‹æ‰§è¡Œæ—¶ä¼šç ´åé©¬å°”å¯å¤«å‡è®¾ï¼Œå½±å“æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºDT-CORLï¼ˆå»¶è¿Ÿå˜æ¢å™¨ä¿¡å¿µç­–ç•¥çº¦æŸç¦»çº¿RLï¼‰ï¼Œæ—¨åœ¨åº”å¯¹éƒ¨ç½²ä¸­çš„å»¶è¿ŸåŠ¨æ€ã€‚DT-CORLé€šè¿‡åŸºäºå˜æ¢å™¨çš„ä¿¡å¿µé¢„æµ‹å™¨ç”Ÿæˆå»¶è¿Ÿç¨³å¥çš„åŠ¨ä½œï¼Œå°½ç®¡åœ¨è®­ç»ƒä¸­æœªè§è¿‡å»¶è¿Ÿè§‚æµ‹ï¼ŒåŒæ—¶åœ¨æ ·æœ¬æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç®€å•çš„å†å²å¢å¼ºåŸºçº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDT-CORLåœ¨å¤šä¸ªå»¶è¿Ÿè®¾ç½®ä¸‹çš„D4RLåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå†å²å¢å¼ºå’Œæ™®é€šä¿¡å¿µæ–¹æ³•ï¼Œç¼©å°äº†æ¨¡æ‹Ÿä¸ç°å®çš„å»¶è¿Ÿå·®è·ï¼ŒåŒæ—¶ä¿æŒæ•°æ®æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨åœ¨çº¿éƒ¨ç½²æ—¶é¢ä¸´çš„å»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒæ—¶æœªè€ƒè™‘å»¶è¿Ÿï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€ç¯å¢ƒä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDT-CORLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å˜æ¢å™¨ç»“æ„æ„å»ºä¿¡å¿µé¢„æµ‹å™¨ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨æœªè§è¿‡å»¶è¿Ÿè§‚æµ‹çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½å¤Ÿç”Ÿæˆç¨³å¥çš„åŠ¨ä½œï¼Œä»è€Œæœ‰æ•ˆåº”å¯¹å»¶è¿Ÿå¸¦æ¥çš„æŒ‘æˆ˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDT-CORLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€ä¿¡å¿µé¢„æµ‹ã€ç­–ç•¥ç”Ÿæˆå’Œæ‰§è¡Œå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»ç¦»çº¿æ•°æ®é›†ä¸­æ”¶é›†æ— å»¶è¿Ÿçš„æ ·æœ¬ï¼›ç„¶åï¼Œé€šè¿‡ä¿¡å¿µé¢„æµ‹å™¨å¤„ç†è¿™äº›æ ·æœ¬ä»¥ç”Ÿæˆå»¶è¿Ÿç¨³å¥çš„ç­–ç•¥ï¼›æœ€åï¼Œåœ¨å®é™…ç¯å¢ƒä¸­æ‰§è¡Œè¿™äº›ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šDT-CORLçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºå˜æ¢å™¨çš„ä¿¡å¿µé¢„æµ‹å™¨ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§è¿‡å»¶è¿Ÿè§‚æµ‹çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å»¶è¿ŸåŠ¨æ€ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„ç¦»çº¿RLæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒDT-CORLé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ä¿¡å¿µé¢„æµ‹å™¨çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šä½¿ç”¨äº†å˜æ¢å™¨ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨é‡æ ·æœ¬æ•ˆç‡ï¼Œç¡®ä¿åœ¨æœ‰é™çš„æ•°æ®ä¸‹è·å¾—æœ€ä½³çš„ç­–ç•¥è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDT-CORLåœ¨å¤šä¸ªå»¶è¿Ÿè®¾ç½®ä¸‹çš„è¡¨ç°å‡ä¼˜äºå†å²å¢å¼ºå’Œæ™®é€šä¿¡å¿µæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾è‘—ç¼©å°äº†æ¨¡æ‹Ÿä¸ç°å®çš„å»¶è¿Ÿå·®è·ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æ•°æ®æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DT-CORLçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰éœ€è¦å®æ—¶å†³ç­–çš„åœºæ™¯ä¸­ã€‚é€šè¿‡æé«˜ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šåœ¨æ›´å¹¿æ³›çš„é¢†åŸŸä¸­å¾—åˆ°æ¨å¹¿ï¼Œä¿ƒè¿›æ™ºèƒ½å†³ç­–ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline-to-online deployment of reinforcement-learning (RL) agents must bridge two gaps: (1) the sim-to-real gap, where real systems add latency and other imperfections not present in simulation, and (2) the interaction gap, where policies trained purely offline face out-of-distribution states during online execution because gathering new interaction data is costly or risky. Agents therefore have to generalize from static, delay-free datasets to dynamic, delay-prone environments. Standard offline RL learns from delay-free logs yet must act under delays that break the Markov assumption and hurt performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained Offline RL), an offline-RL framework built to cope with delayed dynamics at deployment. DT-CORL (i) produces delay-robust actions with a transformer-based belief predictor even though it never sees delayed observations during training, and (ii) is markedly more sample-efficient than naÃ¯ve history-augmentation baselines. Experiments on D4RL benchmarks with several delay settings show that DT-CORL consistently outperforms both history-augmentation and vanilla belief-based methods, narrowing the sim-to-real latency gap while preserving data efficiency.

