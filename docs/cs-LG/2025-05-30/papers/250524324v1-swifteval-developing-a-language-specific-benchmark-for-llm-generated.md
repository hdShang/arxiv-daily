---
layout: default
title: SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation
---

# SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24324" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24324v1</a>
  <a href="https://arxiv.org/pdf/2505.24324.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24324v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24324v1', 'SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ivan Petrukha, Yana Kurliak, Nataliia Stulova

**åˆ†ç±»**: cs.LG, cs.CL, cs.PL, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: Accepted to FORGE'25 Benchmarking on 15.01.2025, to be published by IEEE under the CC BY-NC-ND 4.0 license. This is the accepted version of the article (5 pages, 2 figures, 1 table). DOI will be added upon publication

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSwiftEvalä»¥è§£å†³Swiftä»£ç è¯„ä¼°çš„ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `Swiftè¯„ä¼°` `å¤šè¯­è¨€åŸºå‡†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»£ç è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­åœ¨Pythonï¼Œå¯¼è‡´å¯¹Swiftç­‰å…¶ä»–è¯­è¨€çš„è¯„ä¼°è´¨é‡ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºäº†SwiftEvalåŸºå‡†ï¼ŒåŒ…å«28ä¸ªæ‰‹å·¥è®¾è®¡çš„é—®é¢˜ï¼Œä¸“æ³¨äºSwiftè¯­è¨€ç‰¹æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨éœ€è¦è¯­è¨€ç‰¹å®šç‰¹å¾çš„é—®é¢˜ä¸Šå¾—åˆ†æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯å°å‹æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­åœ¨Pythonä¸Šï¼Œè¿™ä½¿å¾—å¯¹å…¶ä»–ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚Swiftï¼‰çš„é«˜è´¨é‡è¯„ä¼°å˜å¾—å›°éš¾ã€‚é€šè¿‡å¯¹HumanEval-XLå’ŒMultiPL-Eç­‰å¤šè¯­è¨€åŸºå‡†çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å…¶Swiftç»„ä»¶å­˜åœ¨å…³é”®é—®é¢˜ï¼Œå¯¼è‡´å…¶åœ¨è¯„ä¼°LLMç¼–ç èƒ½åŠ›æ—¶ä¸å¤Ÿå……åˆ†æˆ–ç”šè‡³æ— å…³ã€‚ä¸è¿™äº›ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è´¨é‡ä¼˜å…ˆçš„ç­–ç•¥ï¼Œæå‡ºäº†SwiftEvalï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»¥Swiftä¸ºå¯¼å‘çš„åŸºå‡†ï¼ŒåŒ…å«28ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¯„ä¼°äº†44ä¸ªæµè¡Œçš„ä»£ç LLMã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œå¯¹äºéœ€è¦è¯­è¨€ç‰¹å®šç‰¹å¾çš„é—®é¢˜ï¼ŒLLMçš„å¾—åˆ†æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒå°æ¨¡å‹ä¸­è¡¨ç°æœ€ä¸ºæ˜æ˜¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šè¯­è¨€ä»£ç è¯„ä¼°åŸºå‡†åœ¨Swiftè¯­è¨€è¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯HumanEval-XLå’ŒMultiPL-Eåœ¨Swiftç»„ä»¶ä¸Šçš„å…³é”®é—®é¢˜ï¼Œä½¿å¾—è¯„ä¼°ç»“æœä¸å¤Ÿå‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†SwiftEvalåŸºå‡†ï¼Œä¸“æ³¨äºSwiftè¯­è¨€çš„ç‰¹æ€§ï¼Œé‡‡ç”¨è´¨é‡ä¼˜å…ˆçš„ç­–ç•¥ï¼Œè®¾è®¡äº†28ä¸ªæ‰‹å·¥åˆ¶ä½œçš„é—®é¢˜ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°LLMåœ¨Swiftä»£ç ç”Ÿæˆä¸­çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSwiftEvalçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é—®é¢˜è®¾è®¡ã€LLMè¯„ä¼°å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œè®¾è®¡é—®é¢˜ä»¥æ¶µç›–Swiftç‰¹æœ‰çš„è¯­è¨€ç‰¹æ€§ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨44ä¸ªæµè¡Œçš„ä»£ç LLMè¿›è¡Œè¯„ä¼°ï¼›æœ€åï¼Œåˆ†ææ¨¡å‹åœ¨ä¸åŒé—®é¢˜ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šSwiftEvalæ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºSwiftçš„ä»£ç è¯„ä¼°åŸºå‡†ï¼ŒåŒºåˆ«äºç°æœ‰æ–¹æ³•çš„åœ°æ–¹åœ¨äºå…¶æ‰‹å·¥è®¾è®¡çš„é—®é¢˜èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ Swiftè¯­è¨€çš„ç‰¹æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯é€šè¿‡ç¿»è¯‘PythonåŸºå‡†æ¥å®ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é—®é¢˜è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬ç‰¹åˆ«å…³æ³¨Swiftçš„è¯­æ³•å’Œç‰¹æ€§ï¼Œç¡®ä¿æ¯ä¸ªé—®é¢˜éƒ½èƒ½æœ‰æ•ˆè¯„ä¼°LLMåœ¨Swiftç¼–ç¨‹ä¸­çš„èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMåœ¨éœ€è¦ç‰¹å®šè¯­è¨€ç‰¹å¾çš„é—®é¢˜ä¸Šå¾—åˆ†æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯å°å‹æ¨¡å‹çš„è¡¨ç°æœ€ä¸ºæ˜æ˜¾ã€‚è¿™è¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¤„ç†Swiftè¯­è¨€ç‰¹æ€§æ—¶å­˜åœ¨æ˜¾è‘—çš„ä¸è¶³ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹æ”¹è¿›æä¾›äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è½¯ä»¶å¼€å‘å’Œè‡ªåŠ¨åŒ–æµ‹è¯•ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªä¸“é—¨é’ˆå¯¹Swiftçš„è¯„ä¼°åŸºå‡†ï¼Œå¼€å‘è€…å’Œç ”ç©¶äººå‘˜å¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°å’Œæ”¹è¿›LLMåœ¨Swiftä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ï¼Œä»è€Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, large language models (LLMs) have showcased significant advancements in code generation. However, most evaluation benchmarks are primarily oriented towards Python, making it difficult to evaluate other programming languages, such as Swift, with high quality. By examining widely established multilingual benchmarks like HumanEval-XL and MultiPL-E, we identified critical issues specific to their Swift components, making them insufficient or even irrelevant for assessing LLM coding capabilities on Swift. Unlike these existing approaches, which prioritize rapid scaling and generalization by automatically translating Python-centric benchmarks with LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluate 44 popular Code LLMs on it. Our results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes.

