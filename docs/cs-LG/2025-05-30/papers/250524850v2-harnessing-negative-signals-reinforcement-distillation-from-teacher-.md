---
layout: default
title: "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning"
---

# Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24850" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24850v2</a>
  <a href="https://arxiv.org/pdf/2505.24850.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24850v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24850v2', 'Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-12-14)

**å¤‡æ³¨**: 22 pages, 10 figures. Code available at https://github.com/Tim-Siu/reinforcement-distillation

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè´Ÿä¿¡å·è’¸é¦æ–¹æ³•ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è´Ÿä¿¡å·è’¸é¦` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†æ€§èƒ½` `ç›‘ç£å¾®è°ƒ` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡å‹è’¸é¦æ–¹æ³•é€šå¸¸å¿½è§†é”™è¯¯æ¨ç†è½¨è¿¹ï¼Œå¯¼è‡´æ½œåœ¨çš„æœ‰ä»·å€¼æ•°æ®æœªè¢«åˆ©ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆåœ¨æ­£è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶åç»“åˆæ­£è´Ÿè½¨è¿¹è¿›è¡Œç²¾ç‚¼ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒQwen-REDI-1.5Bæ¨¡å‹åœ¨MATH-500ä¸Šå–å¾—äº†83.1%çš„å¾—åˆ†ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•°æ®æ•ˆç‡æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ¨¡å‹è’¸é¦çš„è¿›å±•è¡¨æ˜ï¼Œæ¥è‡ªå…ˆè¿›æ¨ç†æ¨¡å‹çš„æ•°æ®å¯ä»¥æœ‰æ•ˆè®­ç»ƒè¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ã€‚ç„¶è€Œï¼Œæ ‡å‡†å®è·µå¾€å¾€å¿½è§†é”™è¯¯æ¨ç†è½¨è¿¹ï¼Œè¿™äº›æ•°æ®è™½æœ‰ä»·å€¼å´æœªè¢«å……åˆ†åˆ©ç”¨ã€‚æœ¬æ–‡æ¢è®¨å¦‚ä½•åœ¨ç¦»çº¿ç¯å¢ƒä¸­æœ‰æ•ˆåˆ©ç”¨æ­£è´Ÿè’¸é¦æ¨ç†è½¨è¿¹ï¼Œä»¥æœ€å¤§åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆåœ¨æ­£è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç„¶ååœ¨æ­£è´Ÿè½¨è¿¹ä¸Šè¿›è¡Œç²¾ç‚¼é˜¶æ®µã€‚æˆ‘ä»¬å‘ç°ï¼Œç®€å•çš„REINFORCEé£æ ¼ç›®æ ‡ï¼Œå³æˆ‘ä»¬ç§°ä¹‹ä¸ºå¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰ç›®æ ‡ï¼Œåœ¨è’¸é¦ä¸Šä¸‹æ–‡ä¸­ä¼˜äºç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚DPOå’ŒSimPOã€‚æˆ‘ä»¬çš„å®è¯è¯„ä¼°å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„Qwen-REDI-1.5Bæ¨¡å‹åœ¨ä»…ä½¿ç”¨131kè½¨è¿¹çš„æƒ…å†µä¸‹ï¼Œåœ¨MATH-500ä¸Šå–å¾—äº†83.1%çš„å¾—åˆ†ï¼Œå…¶æ€§èƒ½ä¸åœ¨800kä¸“æœ‰æ•°æ®ä¸Šè®­ç»ƒçš„DeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹ç›¸å½“ã€‚è¿™ä¸€ç»“æœå±•ç¤ºäº†åˆ©ç”¨ä¹‹å‰è¢«ä¸¢å¼ƒçš„è´Ÿè½¨è¿¹çš„æ˜¾è‘—æ•°æ®æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ­£è´Ÿè’¸é¦æ¨ç†è½¨è¿¹ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å¿½è§†é”™è¯¯æ¨ç†è½¨è¿¹ï¼Œå¯¼è‡´æ½œåœ¨ä¿¡æ¯çš„ä¸¢å¤±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆåœ¨æ­£è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç„¶ååœ¨ç²¾ç‚¼é˜¶æ®µç»“åˆæ­£è´Ÿè½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œä»¥å……åˆ†åˆ©ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºç›‘ç£å¾®è°ƒï¼Œä¸“æ³¨äºæ­£è½¨è¿¹ï¼›ç¬¬äºŒé˜¶æ®µä¸ºç²¾ç‚¼é˜¶æ®µï¼Œç»“åˆæ­£è´Ÿè½¨è¿¹è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æå‡ºäº†å¼ºåŒ–è’¸é¦ï¼ˆREDIï¼‰ç›®æ ‡ï¼Œè¯¥ç›®æ ‡åœ¨è’¸é¦ä¸Šä¸‹æ–‡ä¸­ä¼˜äºä¼ ç»Ÿçš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚DPOå’ŒSimPOã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ­£è´Ÿè½¨è¿¹çš„å½±å“ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§å­¦ä¹ ç‡ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQwen-REDI-1.5Bæ¨¡å‹åœ¨MATH-500ä¸Šå–å¾—äº†83.1%çš„å¾—åˆ†ï¼Œè¡¨ç°ä¸åœ¨800kä¸“æœ‰æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼Œå±•ç¤ºäº†åœ¨æ•°æ®æ•ˆç‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨è´Ÿä¿¡å·ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ›´å°‘çš„æ•°æ®ä¸Šå®ç°æ›´é«˜çš„æ¨ç†æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in model distillation show that data from advanced reasoning models can effectively train smaller student models. However, standard practices discard incorrect reasoning traces -- valuable, yet underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? We employ a two-stage training recipe: first, Supervised Fine-Tuning (SFT) on positive traces, followed by a refinement stage using both positive and negative traces. We find that a simple REINFORCE-style objective, which we term the Reinforcement Distillation (REDI) objective, outperforms established preference optimization methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate the effectiveness of this approach. Notably, our Qwen-REDI-1.5B model, trained on just 131k traces from the open Open-R1 dataset, achieves an 83.1% score on MATH-500. Its performance matches that of DeepSeek-R1-Distill-Qwen-1.5B, a model trained on 800k proprietary data. This result showcases the remarkable data efficiency of utilizing previously discarded negative traces.

