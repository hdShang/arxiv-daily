---
layout: default
title: Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding
---

# Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15704" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.15704v1</a>
  <a href="https://arxiv.org/pdf/2506.15704.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15704v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15704v1', 'Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Feiyu Yao, Qian Wang

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LFPS‰ª•Ëß£ÂÜ≥Èïø‰∏ä‰∏ãÊñáËß£Á†Å‰∏≠ÁöÑÁ®ÄÁñèÁ¥¢ÂºïÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Á®ÄÁñèÊ≥®ÊÑèÂäõ` `Ëß£Á†Å‰ºòÂåñ` `ÂéÜÂè≤‰ø°ÊÅØÂà©Áî®` `Èïø‰∏ä‰∏ãÊñá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂Âú®Ëß£Á†ÅËøáÁ®ã‰∏≠ÈúÄË¶ÅÈÅçÂéÜÊâÄÊúâÈîÆÂêëÈáèÔºåÂØºËá¥ËÆ°ÁÆóÂíåÊï∞ÊçÆ‰º†ËæìÂºÄÈîÄËøáÂ§ß„ÄÇ
2. LFPSÊñπÊ≥ïÈÄöËøáÂä®ÊÄÅÊûÑÂª∫Á®ÄÁñèÁ¥¢ÂºïÂÄôÈÄâÔºåÂà©Áî®ÂéÜÂè≤Ê≥®ÊÑèÂäõÊ®°Âºè‰∏≠ÁöÑÊó∂Èó¥Áõ∏ÂÖ≥ÊÄßÊù•‰ºòÂåñÁ¥¢ÂºïÊ£ÄÁ¥¢„ÄÇ
3. Âú®Èïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠ÔºåLFPSÂú®RTX 4090 GPU‰∏äÂÆûÁé∞‰∫Ü22.8ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçáÔºåÂêåÊó∂‰øùÊåÅÁîüÊàêÂáÜÁ°ÆÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂØπÊõ¥Èïø‰∏ä‰∏ãÊñáÁöÑÊîØÊåÅÔºåËß£Á†ÅËøáÁ®ã‰∏≠ÂØπÈîÆÂÄºÔºàKVÔºâÁºìÂ≠òÁöÑÂÜÖÂ≠òÈúÄÊ±ÇËøÖÈÄüÂ¢ûÈïøÔºåÊàê‰∏∫GPUÂÜÖÂ≠òÂÆπÈáèÂíåPCIeÂ∏¶ÂÆΩÁöÑÂÖ≥ÈîÆÁì∂È¢à„ÄÇÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÈÄöËøá‰ªÖËÆ°ÁÆóÈÄâÂÆöÈîÆÂÄºÂØπÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÊù•ÁºìËß£Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Á¥¢ÂºïËÆ°ÁÆóÊó∂ÈÄöÂ∏∏ÈúÄË¶ÅÈÅçÂéÜÊâÄÊúâÈîÆÂêëÈáèÔºåÂØºËá¥ÊòæËëóÁöÑËÆ°ÁÆóÂíåÊï∞ÊçÆ‰º†ËæìÂºÄÈîÄ„ÄÇ‰∏∫Èôç‰ΩéÁ¥¢ÂºïÊ£ÄÁ¥¢ÊàêÊú¨ÔºåÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÂ∞ÜÊØè‰∏™Ëß£Á†ÅÊ≠•È™§ËßÜ‰∏∫Áã¨Á´ãËøáÁ®ãÔºåÊú™ËÉΩÂà©Áî®ÂéÜÂè≤Ëß£Á†Å‰ø°ÊÅØ‰∏≠ÁöÑÊó∂Èó¥Áõ∏ÂÖ≥ÊÄß„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫LFPSÔºàLearn From the Past for Sparse IndexingÔºâÔºå‰∏ÄÁßçÂä†ÈÄüÊñπÊ≥ïÔºåÂü∫‰∫éÂéÜÂè≤Ê≥®ÊÑèÂäõÊ®°ÂºèÂä®ÊÄÅÊûÑÂª∫Á®ÄÁñèÁ¥¢ÂºïÂÄôÈÄâ„ÄÇLFPSÊçïÊçâËß£Á†ÅÂô®Ê≥®ÊÑèÂäõ‰∏≠ÁöÑ‰∏§ÁßçÊôÆÈÅçË∂ãÂäøÔºåÂπ∂ÁªìÂêà‰ΩçÁΩÆÊâ©Â±ïÁ≠ñÁï•ÊúâÊïàÈ¢ÑÊµãÂΩìÂâçÊ≠•È™§ÁöÑTop-kÁ¥¢Âºï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLFPSÂú®Èïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÂä†ÈÄüÊïàÊûúÔºåÂêåÊó∂‰øùÊåÅÁîüÊàêÂáÜÁ°ÆÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáËß£ÂÜ≥ÁöÑÊòØÂú®Èïø‰∏ä‰∏ãÊñáËß£Á†Å‰∏≠ÔºåÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂Áî±‰∫éÈÅçÂéÜÊâÄÊúâÈîÆÂêëÈáèËÄåÂØºËá¥ÁöÑËÆ°ÁÆóÂíåÊï∞ÊçÆ‰º†ËæìÂºÄÈîÄËøáÂ§ßÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂà©Áî®ÂéÜÂè≤Ëß£Á†Å‰ø°ÊÅØ‰∏≠ÁöÑÊó∂Èó¥Áõ∏ÂÖ≥ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLFPSÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂü∫‰∫éÂéÜÂè≤Ê≥®ÊÑèÂäõÊ®°ÂºèÂä®ÊÄÅÊûÑÂª∫Á®ÄÁñèÁ¥¢ÂºïÂÄôÈÄâÔºåÊçïÊçâËß£Á†ÅÂô®Ê≥®ÊÑèÂäõ‰∏≠ÁöÑÂûÇÁõ¥Ê®°ÂºèÂíåÊñúÁ∫øÊ®°ÂºèÔºåÂπ∂ÈÄöËøá‰ΩçÁΩÆÊâ©Â±ïÁ≠ñÁï•È¢ÑÊµãÂΩìÂâçÊ≠•È™§ÁöÑTop-kÁ¥¢Âºï„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLFPSÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂéÜÂè≤Ê≥®ÊÑèÂäõÊ®°ÂºèÁöÑÊèêÂèñ„ÄÅÁ®ÄÁñèÁ¥¢ÂºïÂÄôÈÄâÁöÑÊûÑÂª∫ÂíåTop-kÁ¥¢ÂºïÁöÑÈ¢ÑÊµã‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºå‰ªéÂéÜÂè≤Ëß£Á†Å‰∏≠ÊèêÂèñÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåÁÑ∂ÂêéÊ†πÊçÆËøô‰∫õÊ®°ÂºèÂä®ÊÄÅÁîüÊàêÁ¥¢ÂºïÂÄôÈÄâÔºåÊúÄÂêéËøõË°åTop-kÁ¥¢ÂºïÁöÑÈÄâÊã©„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLFPSÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂà©Áî®ÂéÜÂè≤Ëß£Á†Å‰ø°ÊÅØ‰∏≠ÁöÑÊó∂Èó¥Áõ∏ÂÖ≥ÊÄßÔºåÂä®ÊÄÅÊûÑÂª∫Á®ÄÁñèÁ¥¢ÂºïÂÄôÈÄâÔºå‰ªéËÄåÊòæËëóÈôç‰ΩéÁ¥¢ÂºïÊ£ÄÁ¥¢ÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÂ∞ÜÊØè‰∏™Ëß£Á†ÅÊ≠•È™§ËßÜ‰∏∫Áã¨Á´ãËøáÁ®ãÁöÑÂÅöÊ≥ïÊú¨Ë¥®‰∏ä‰∏çÂêå„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöLFPSÂú®ËÆæËÆ°‰∏äÁªìÂêà‰∫Ü‰ΩçÁΩÆÊâ©Â±ïÁ≠ñÁï•Ôºå‰ª•ÊúâÊïàÈ¢ÑÊµãTop-kÁ¥¢ÂºïÔºåÂπ∂Âú®ÂÆûÈ™å‰∏≠‰ΩøÁî®‰∫ÜLlama-3.1-8B-Instruct‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºåÁ°Æ‰øù‰∫ÜÁîüÊàêÁöÑÂáÜÁ°ÆÊÄß‰∏éÈÄüÂ∫¶ÁöÑÂπ≥Ë°°„ÄÇÂÆûÈ™å‰∏≠Ëøò‰ΩøÁî®‰∫ÜRTX 4090 GPUÂíåXeon Gold 6430 CPUËøõË°åÊÄßËÉΩËØÑ‰º∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLFPSÂú®Èïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÈ´òËææ22.8ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçáÔºåÁõ∏ËæÉ‰∫éÂÆåÊï¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåÁ≤æÁ°ÆTop-kÊ£ÄÁ¥¢ÂàÜÂà´ÊèêÂçá‰∫Ü9.6ÂÄç„ÄÇËøô‰∫õÁªìÊûúÂ±ïÁ§∫‰∫ÜLFPSÂú®Ëß£Á†Å‰ºòÂåñ‰∏≠ÁöÑÂÆûÁî®ÊÄßÂíåÈ´òÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LFPSÊñπÊ≥ïÂú®Èïø‰∏ä‰∏ãÊñáÁöÑËØ≠Ë®ÄÊ®°ÂûãËß£Á†Å‰∏≠ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´òÊïàÂ§ÑÁêÜÂ§ßËßÑÊ®°ÊñáÊú¨ÁîüÊàê‰ªªÂä°ÁöÑÂú∫ÊôØ‰∏≠ÔºåÂ¶ÇÂØπËØùÁ≥ªÁªü„ÄÅÊñáÊú¨ÊëòË¶ÅÂíåÊú∫Âô®ÁøªËØëÁ≠â„ÄÇÂÖ∂È´òÊïàÁöÑÁ¥¢ÂºïÊ£ÄÁ¥¢Êú∫Âà∂ËÉΩÂ§üÊòæËëóÊèêÂçáËß£Á†ÅÈÄüÂ∫¶ÔºåÈôç‰ΩéËÆ°ÁÆóËµÑÊ∫êÊ∂àËÄóÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> As large language models (LLMs) continue to support increasingly longer contexts, the memory demand for key-value (KV) caches during decoding grows rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe bandwidth. Sparse attention mechanisms alleviate this issue by computing attention weights only for selected key-value pairs. However, their indexing computation typically requires traversing all key vectors, resulting in significant computational and data transfer overhead. To reduce the cost of index retrieval, existing methods often treat each decoding step as an independent process, failing to exploit the temporal correlations embedded in historical decoding information. To this end, we propose LFPS(Learn From the Past for Sparse Indexing), an acceleration method that dynamically constructs sparse indexing candidates based on historical attention patterns. LFPS captures two prevalent trends in decoder attention -vertical patterns (attending to fixed positions) and slash patterns (attending to relative positions) -and incorporates a positional expansion strategy to effectively predict the Top-k indices for the current step. We validate LFPS on challenging long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as the base model. Experimental results show that LFPS achieves up to 22.8$\times$ speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively, while preserving generation accuracy. These results demonstrate that LFPS offers a practical and efficient solution for decoding optimization in long-context LLM inference.

