---
layout: default
title: HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts
---

# HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24722" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.24722v2</a>
  <a href="https://arxiv.org/pdf/2505.24722.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24722v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24722v2', 'HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying

**ÂàÜÁ±ª**: cs.LG, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-30 (Êõ¥Êñ∞: 2025-11-06)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫HELM‰ª•Ëß£ÂÜ≥Áé∞ÊúâËØ≠Ë®ÄÊ®°ÂûãÂá†‰ΩïÁªìÊûÑ‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ë∂ÖÊõ≤ÁéáÊ®°Âûã` `ËØ≠Ë®ÄÊ®°Âûã` `Âá†‰ΩïÁªìÊûÑ` `Transformer` `Ê∑∑ÂêàÊõ≤Áéá‰∏ìÂÆ∂` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `Ê®°ÂûãËÆ≠ÁªÉ` `Êé®ÁêÜËÉΩÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂá†‰ΩïÁªìÊûÑÔºåÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÂíåÁîüÊàêËÉΩÂäõ‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫HELMÔºåÈÄöËøáÂú®Ë∂ÖÊõ≤ÁéáÁ©∫Èó¥‰∏≠Êìç‰ΩúÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊ®°ÂûãÁöÑË°®Á§∫ÁÅµÊ¥ªÊÄß‰∏çË∂≥ÂíåÊâ©Â±ïÊÄßÂ∑ÆÁöÑÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHELMÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Áõ∏ËæÉ‰∫é‰º†ÁªüÊ¨ßÂá†ÈáåÂæóÊû∂ÊûÑÊèêÂçá‰∫ÜÊúÄÈ´ò4%ÁöÑÊÄßËÉΩÔºåÊòæÁ§∫Âá∫Ë∂ÖÊõ≤ÁéáÂá†‰ΩïÁöÑ‰ºòÂäø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊñáÊú¨Âª∫Ê®°‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÁî±‰∫é‰æùËµñÊ¨ßÂá†ÈáåÂæóÊìç‰ΩúÔºåÊú™ËÉΩÂÆåÂÖ®ÊçïÊçâËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂõ∫ÊúâËØ≠‰πâÂ±ÇÊ¨°ÂíåÂá†‰ΩïÁªìÊûÑ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂøΩËßÜÊ†áËÆ∞ÂµåÂÖ•ÁöÑÂá†‰ΩïÁâπÊÄß‰ºöÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÂíåÁîüÊàêËÉΩÂäõ‰∏ãÈôç„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫Âú®Ë∂ÖÊõ≤ÁéáÁ©∫Èó¥‰∏≠ÂÆåÂÖ®Êìç‰ΩúÁöÑHELMÔºàHypErbolic Large Language ModelsÔºâÔºåÂπ∂ÂºïÂÖ•Ê∑∑ÂêàÊõ≤Áéá‰∏ìÂÆ∂Ê®°ÂûãHELM-MICEÔºå‰ª•Êõ¥ÁªÜËá¥Âú∞ÁºñÁ†ÅÊñáÊú¨ÁöÑÂá†‰ΩïÁªìÊûÑ„ÄÇÊàë‰ª¨È¶ñÊ¨°Âú®ÂçÅ‰∫øÂèÇÊï∞ËßÑÊ®°‰∏ãËÆ≠ÁªÉÂÆåÂÖ®Ë∂ÖÊõ≤ÁéáÁöÑLLMsÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ËØÑ‰º∞ÂÖ∂ÊÄßËÉΩÔºåÁªìÊûúÊòæÁ§∫HELMÊû∂ÊûÑÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÁõ∏ËæÉ‰∫éÊµÅË°åÁöÑÊ¨ßÂá†ÈáåÂæóÊû∂ÊûÑÊúâÊòæËëóÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜËá™ÁÑ∂ËØ≠Ë®ÄÊó∂Êú™ËÉΩÂÖÖÂàÜËÄÉËôëÂÖ∂Âá†‰ΩïÁªìÊûÑÁöÑÈóÆÈ¢òÔºåÂØºËá¥Ê®°ÂûãËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÂíåÁîüÊàêËÉΩÂäõ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫Âú®Ë∂ÖÊõ≤ÁéáÁ©∫Èó¥‰∏≠ÂÆåÂÖ®Êìç‰ΩúÁöÑHELMÊ®°ÂûãÔºåÂà©Áî®Ë∂ÖÊõ≤ÁéáÁöÑÊâ©Â±ïÊÄßÂíå‰ΩéÂ§±ÁúüÁâπÊÄßÔºåÈáçÊñ∞ÊÄùËÄÉÂü∫‰∫éTransformerÁöÑËØ≠Ë®ÄÊ®°ÂûãÊû∂ÊûÑ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHELMÂåÖÊã¨HELM-MICEÂíåHELM-D‰∏§‰∏™Ê®°ÂûãÔºåHELM-MICEÈááÁî®Ê∑∑ÂêàÊõ≤Áéá‰∏ìÂÆ∂Ê®°ÂûãÔºåÊØè‰∏™‰∏ìÂÆ∂Âú®‰∏çÂêåÁöÑÊõ≤ÁéáÁ©∫Èó¥‰∏≠Êìç‰ΩúÔºå‰ª•ÁºñÁ†ÅÊõ¥ÁªÜËá¥ÁöÑÂá†‰ΩïÁªìÊûÑ„ÄÇHELM-DÂàô‰∏∫ÂØÜÈõÜÊ®°ÂûãÔºå‰∫åËÄÖÂùáÂºïÂÖ•Ë∂ÖÊõ≤ÁéáÁöÑÂ§öÂ§¥ÊΩúÂú®Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºàHMLAÔºâ‰ª•ÊèêÈ´òËÆ≠ÁªÉÂíåÊé®ÁêÜÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÈ¶ñÊ¨°Âú®ÂçÅ‰∫øÂèÇÊï∞ËßÑÊ®°‰∏ãËÆ≠ÁªÉÂÆåÂÖ®Ë∂ÖÊõ≤ÁéáÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÂπ∂ÂºÄÂèë‰∫ÜË∂ÖÊõ≤ÁéáÁöÑÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÂíåRMSÂΩí‰∏ÄÂåñÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöHELMÊ®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÊ∑∑ÂêàÊõ≤Áéá‰∏ìÂÆ∂Êú∫Âà∂Ôºå‰ºòÂåñ‰∫ÜÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®‰∏çÂêåÂá†‰ΩïÁ©∫Èó¥‰∏≠ÊúâÊïàÂ≠¶‰π†ÊñáÊú¨ÁöÑÁªìÊûÑÁâπÂæÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåHELMÊ®°ÂûãÂú®MMLUÂíåARCÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÁõ∏ËæÉ‰∫éLLaMAÂíåDeepSeekÁ≠âÊµÅË°åÁöÑÊ¨ßÂá†ÈáåÂæóÊû∂ÊûÑÔºåÊÄßËÉΩÊèêÂçáÊúÄÈ´òÂèØËææ4%„ÄÇËøô‰∏ÄÁªìÊûúÁ™ÅÊòæ‰∫ÜË∂ÖÊõ≤ÁéáÂá†‰ΩïÂú®Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÂ¢ûÂº∫ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HELMÊ®°ÂûãÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÊ∑±Â∫¶ÁêÜËß£ÊñáÊú¨Âá†‰ΩïÁªìÊûÑÁöÑ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÁü•ËØÜÈóÆÁ≠î„ÄÅÊñáÊú¨ÁîüÊàêÂíåËØ≠‰πâÁêÜËß£Á≠â„ÄÇÂÖ∂ÊîπËøõÁöÑÊé®ÁêÜËÉΩÂäõÂíåÁ®≥ÂÆöÊÄßÂ∞ÜÊé®Âä®Êõ¥Â§çÊùÇÁöÑËØ≠Ë®ÄÊ®°ÂûãÁöÑÂºÄÂèëÔºåÊú™Êù•ÂèØËÉΩÂú®Â§öÁßçÊô∫ËÉΩÂ∫îÁî®‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.

