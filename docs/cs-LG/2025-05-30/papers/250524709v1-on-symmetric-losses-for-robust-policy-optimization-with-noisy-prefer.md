---
layout: default
title: On Symmetric Losses for Robust Policy Optimization with Noisy Preferences
---

# On Symmetric Losses for Robust Policy Optimization with Noisy Preferences

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24709" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24709v1</a>
  <a href="https://arxiv.org/pdf/2505.24709.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24709v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24709v1', 'On Symmetric Losses for Robust Policy Optimization with Noisy Preferences')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, Masashi Sugiyama

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æŠ—å™ªå£°åå¥½çš„ç¨³å¥ç­–ç•¥ä¼˜åŒ–æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç­–ç•¥ä¼˜åŒ–` `äººç±»åé¦ˆ` `å¥–åŠ±å»ºæ¨¡` `å¯¹ç§°æŸå¤±` `å™ªå£°é²æ£’æ€§` `å¼ºåŒ–å­¦ä¹ ` `åå¥½å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾åå¥½æ•°æ®çš„æ ‡æ³¨æ˜¯å‡†ç¡®çš„ï¼Œä½†å®é™…æƒ…å†µä¸­æ•°æ®å¸¸å¸¸å—åˆ°å™ªå£°çš„å½±å“ï¼Œå¯¼è‡´ç­–ç•¥ä¼˜åŒ–æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºåˆ†ç±»é—®é¢˜ï¼Œåˆ©ç”¨å¯¹ç§°æŸå¤±çš„é²æ£’æ€§æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä»è€Œåº”å¯¹å™ªå£°åå¥½çš„æŒ‘æˆ˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSymPOæ–¹æ³•åœ¨å¤šç§åˆæˆå’ŒçœŸå®ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼˜åŒ–åŸºäºäººç±»åå¥½çš„ç­–ç•¥æ˜¯ä½¿è¯­è¨€æ¨¡å‹ä¸äººç±»æ„å›¾å¯¹é½çš„å…³é”®ã€‚æœ¬æ–‡èšç„¦äºå¥–åŠ±å»ºæ¨¡ï¼Œè¿™æ˜¯ä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å‡è®¾æ ‡æ³¨å‡†ç¡®ï¼Œä½†ç°å®ä¸­çš„åå¥½æ•°æ®å¸¸å› äººä¸ºé”™è¯¯æˆ–åè§è€Œå«æœ‰å™ªå£°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨å™ªå£°åå¥½ä¸‹ç¨³å¥ç­–ç•¥ä¼˜åŒ–çš„æ¡†æ¶ï¼Œå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºåˆ†ç±»é—®é¢˜ï¼Œåˆ©ç”¨å¯¹æ ‡ç­¾å™ªå£°å…·æœ‰é²æ£’æ€§çš„å¯¹ç§°æŸå¤±ï¼Œæå‡ºäº†å¯¹ç§°åå¥½ä¼˜åŒ–ï¼ˆSymPOï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜äº†å¯¹ç§°æŸå¤±èƒ½å¤Ÿåœ¨æ ‡ç­¾å™ªå£°ä¸‹æˆåŠŸä¼˜åŒ–ç­–ç•¥ï¼Œå› ä¸ºæ‰€å¾—åˆ°çš„å¥–åŠ±ä¿æŒæ’åä¸å˜ï¼Œè¿™ä¸€ç‰¹æ€§è¶³ä»¥å®ç°ç­–ç•¥æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSymPOåœ¨åˆæˆå’ŒçœŸå®ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å™ªå£°åå¥½ä¸‹è¿›è¡Œç­–ç•¥ä¼˜åŒ–çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾åå¥½æ ‡æ³¨å‡†ç¡®ï¼Œå¯¼è‡´åœ¨çœŸå®åœºæ™¯ä¸­æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬å°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºåˆ†ç±»é—®é¢˜ï¼Œåˆ©ç”¨å¯¹ç§°æŸå¤±çš„ç‰¹æ€§æ¥æé«˜å¯¹æ ‡ç­¾å™ªå£°çš„é²æ£’æ€§ï¼Œä»è€Œå®ç°ç¨³å¥çš„ç­–ç•¥ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å¯¹ç§°æŸå¤±è®¡ç®—ã€ç­–ç•¥ä¼˜åŒ–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹åå¥½æ•°æ®è¿›è¡Œå¤„ç†ï¼Œç„¶åé€šè¿‡å¯¹ç§°æŸå¤±è¿›è¡Œå¥–åŠ±å»ºæ¨¡ï¼Œæœ€åè¿›è¡Œç­–ç•¥æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å¯¹ç§°æŸå¤±æ¥å¤„ç†æ ‡ç­¾å™ªå£°ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå‡†ç¡®æ ‡æ³¨çš„å‡è®¾å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨å¯¹ç§°æŸå¤±ï¼Œç¡®ä¿åœ¨æ ‡ç­¾å™ªå£°å­˜åœ¨æ—¶ä»èƒ½ä¿æŒå¥–åŠ±çš„æ’åç‰¹æ€§ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å™ªå£°çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSymPOæ–¹æ³•åœ¨åˆæˆä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜äº†20%çš„ç­–ç•¥ä¼˜åŒ–æ•ˆæœï¼Œåœ¨çœŸå®ä»»åŠ¡ä¸­ä¹Ÿæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†å™ªå£°åå¥½æ—¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€æ¨èç³»ç»Ÿå’Œè‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ç³»ç»Ÿåœ¨é¢å¯¹ä¸ç¡®å®šæ€§å’Œå™ªå£°æ•°æ®æ—¶çš„å†³ç­–èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ›´æ™ºèƒ½çš„ç³»ç»Ÿè®¾è®¡ï¼Œä½¿å…¶æ›´å¥½åœ°ç†è§£å’Œå“åº”äººç±»åå¥½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Optimizing policies based on human preferences is key to aligning language models with human intent. This work focuses on reward modeling, a core component in reinforcement learning from human feedback (RLHF), and offline preference optimization, such as direct preference optimization. Conventional approaches typically assume accurate annotations. However, real-world preference data often contains noise due to human errors or biases. We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem. This allows us to leverage symmetric losses, known for their robustness to label noise in classification, leading to our Symmetric Preference Optimization (SymPO) method. We prove that symmetric losses enable successful policy optimization even under noisy labels, as the resulting reward remains rank-preserving -- a property sufficient for policy improvement. Experiments on synthetic and real-world tasks demonstrate the effectiveness of SymPO.

