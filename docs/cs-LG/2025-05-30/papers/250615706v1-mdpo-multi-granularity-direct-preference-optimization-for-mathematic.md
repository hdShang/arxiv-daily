---
layout: default
title: "MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning"
---

# MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15706" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15706v1</a>
  <a href="https://arxiv.org/pdf/2506.15706.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15706v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15706v1', 'MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunze Lin

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMDPOä»¥è§£å†³é•¿é“¾æ•°å­¦æ¨ç†ä¸­çš„é”™è¯¯è¾“å‡ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°å­¦æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç›´æ¥åå¥½ä¼˜åŒ–` `å¤šç²’åº¦ä¼˜åŒ–` `æœºå™¨å­¦ä¹ ` `æ¨ç†èƒ½åŠ›æå‡` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨é•¿é“¾æ•°å­¦æ¨ç†ä¸­æ•ˆæœæœ‰é™ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰åå¥½æ•°æ®ä¸­çš„æ¥å—ä¸æ‹’ç»ç­”æ¡ˆä¹‹é—´çš„å·®å¼‚ã€‚
2. æœ¬æ–‡æå‡ºçš„MDPOæ–¹æ³•ä»è§£å†³æ–¹æ¡ˆã€æ¨ç†å’Œæ­¥éª¤ä¸‰ä¸ªç²’åº¦ä¼˜åŒ–LLMsçš„æ•°å­¦æ¨ç†ï¼Œç»Ÿä¸€è®­ç»ƒç›®æ ‡ä»¥å¯¹é½ç”ŸæˆæŒ‡æ ‡ã€‚
3. åœ¨GSM8Kå’ŒMATHæ•°æ®é›†ä¸Šï¼ŒMDPOæ–¹æ³•åˆ†åˆ«æå‡äº†1.7%å’Œ2.3%çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†DPOåŠå…¶å˜ä½“æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°å­¦æ¨ç†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒè¦æ±‚ç¡®ä¿æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚å°½ç®¡ç ”ç©¶è€…é€šè¿‡ç›‘ç£å¾®è°ƒå¢å¼ºäº†LLMsçš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†ç”±äºæ— æ³•æŠ‘åˆ¶é”™è¯¯è¾“å‡ºï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ã€‚æœ€è¿‘ï¼Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¢«å¹¿æ³›é‡‡ç”¨ä»¥å¯¹é½äººç±»æ„å›¾ï¼Œä½†åœ¨é•¿é“¾æ•°å­¦æ¨ç†ä¸­æ•ˆæœæœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å¤šç²’åº¦ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆMDPOï¼‰æ–¹æ³•ï¼Œä»è§£å†³æ–¹æ¡ˆã€æ¨ç†å’Œæ­¥éª¤ä¸‰ä¸ªç²’åº¦ä¼˜åŒ–LLMsçš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾æ•°å­¦æ¨ç†ä¸­ç”Ÿæˆé”™è¯¯è¾“å‡ºçš„é—®é¢˜ã€‚ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•åœ¨å¤„ç†é•¿é“¾æ•°æ®æ—¶ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰åå¥½æ•°æ®ä¸­æ¥å—ä¸æ‹’ç»ç­”æ¡ˆçš„å·®å¼‚ï¼Œå¯¼è‡´æ¨ç†æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMDPOæ–¹æ³•é€šè¿‡ä»ä¸‰ä¸ªç²’åº¦ï¼ˆè§£å†³æ–¹æ¡ˆã€æ¨ç†ã€æ­¥éª¤ï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œæå‡LLMsçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æ¯ä¸ªç²’åº¦å…³æ³¨ä¸åŒçš„æ¨ç†å±‚é¢ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMDPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šSolution2Solutionï¼ˆå…³æ³¨æ•´ä¸ªé•¿é“¾æ¨ç†çš„æ­£ç¡®æ€§ï¼‰ã€Inference2Inferenceï¼ˆä¸“æ³¨äºæ­¥éª¤é—´çš„é€»è¾‘æ¨ç†ï¼‰å’ŒStep2Stepï¼ˆçº æ­£æ­¥éª¤ä¸­çš„è®¡ç®—é”™è¯¯ï¼‰ã€‚è¿™ä¸‰ä¸ªæ¨¡å—çš„è®­ç»ƒç›®æ ‡ç»Ÿä¸€ï¼Œä»¥å¯¹é½ç”ŸæˆæŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šMDPOçš„åˆ›æ–°åœ¨äºå…¶å¤šç²’åº¦ä¼˜åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒå±‚é¢ä¸Šæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„DPOæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰æ¨ç†è¿‡ç¨‹ä¸­çš„ç»†å¾®å·®å¼‚ï¼Œä»è€Œæœ‰æ•ˆæŠ‘åˆ¶é”™è¯¯è¾“å‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MDPOä¸­ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ç»Ÿä¸€çš„æŸå¤±å‡½æ•°è®¾è®¡ï¼Œä»¥ç¡®ä¿ä¸åŒç²’åº¦çš„ä¼˜åŒ–ç›®æ ‡èƒ½å¤ŸååŒå·¥ä½œã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒç²’åº¦çš„æ¨ç†éœ€æ±‚ï¼Œæå‡è®¡ç®—èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMDPOæ–¹æ³•åœ¨GSM8Kæ•°æ®é›†ä¸Šæå‡äº†1.7%ï¼Œåœ¨MATHæ•°æ®é›†ä¸Šæå‡äº†2.3%ã€‚è¿™äº›ç»“æœå‡è¶…è¿‡äº†ä¼ ç»ŸDPOåŠå…¶å˜ä½“æ–¹æ³•ï¼Œè¡¨æ˜MDPOåœ¨é•¿é“¾æ•°å­¦æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç§‘å­¦è®¡ç®—å’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼ŒMDPOå¯ä»¥ä¸ºæ•™è‚²å·¥å…·ã€æ™ºèƒ½åŠ©æ‰‹å’Œç§‘ç ”è®¡ç®—æä¾›æ›´å‡†ç¡®çš„æ”¯æŒï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) as it requires ensuring the correctness of each reasoning step. Researchers have been strengthening the mathematical reasoning abilities of LLMs through supervised fine-tuning, but due to the inability to suppress incorrect outputs, illusions can easily arise. Recently, Direct Preference Optimization (DPO) has been widely adopted for aligning human intent by using preference data to prevent LLMs from generating incorrect outputs. However, it has shown limited benefits in long-chain mathematical reasoning, mainly because DPO struggles to effectively capture the differences between accepted and rejected answers from preferences in long-chain data. The inconsistency between DPO training and LLMs' generation metrics also affects the effectiveness of suppressing incorrect outputs. We propose the Multi-Granularity Direct Preference Optimization (MDPO) method, optimizing the mathematical reasoning of LLMs at three granularities: Solution2Solution, Inference2Inference, and Step2Step. Solution2Solution focuses on the correctness of entire long-chain reasoning; Inference2Inference concentrates on logical reasoning between steps; Step2Step corrects computational errors in steps, enhancing the computational capabilities of LLMs. Additionally, we unify the training objectives of the three granularities to align with the generation metrics. We conducted experiments on the open-source models Qwen2 and Llama3, achieving improvements of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset, outperforming DPO and other DPO variant methods. Furthermore, we also provide a pipeline for constructing MDPO training data that is simple and does not require manual annotation costs.

