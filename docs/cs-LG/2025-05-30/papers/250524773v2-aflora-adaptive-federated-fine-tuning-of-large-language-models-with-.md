---
layout: default
title: "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption"
---

# AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24773" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24773v2</a>
  <a href="https://arxiv.org/pdf/2505.24773.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24773v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24773v2', 'AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yajie Zhou, Xiaoyi Pang, Zhibo Wang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-08-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAFLoRAä»¥è§£å†³å¼‚æ„ç¯å¢ƒä¸‹å¤§è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§å¾®è°ƒé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è”é‚¦å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `ä½ç§©é€‚åº”` `èµ„æºä¼˜åŒ–` `æ•°æ®å¼‚æ„æ€§` `æ¨¡å‹å¾®è°ƒ` `è‡ªé€‚åº”ç®—æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è”é‚¦å¾®è°ƒæ–¹æ³•åœ¨å¼‚æ„å®¢æˆ·ç«¯ä¸Šé¢ä¸´é«˜è®¡ç®—å’Œé€šä¿¡éœ€æ±‚ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
2. AFLoRAé€šè¿‡è§£è€¦å…±äº«å’Œå®¢æˆ·ç«¯ç‰¹å®šæ›´æ–°ï¼Œç»“åˆå¯¹è§’çŸ©é˜µå‰ªæå’Œç§©æ„ŸçŸ¥èšåˆï¼Œä¼˜åŒ–äº†å¾®è°ƒè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAFLoRAåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›äº†æ›´å¥½çš„é€‚åº”æ€§è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è”é‚¦å¾®è°ƒä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨å»ä¸­å¿ƒåŒ–æ•°æ®å°†åŸºç¡€æ¨¡å‹é€‚åº”äºä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºå®¢æˆ·ç«¯çš„æ•°æ®å’Œç³»ç»Ÿèµ„æºå¼‚æ„ä¸”å—é™ï¼Œå®é™…éƒ¨ç½²é¢ä¸´é«˜è®¡ç®—å’Œé€šä¿¡éœ€æ±‚çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶é‡‡ç”¨äº†ä½ç§©é€‚åº”ç­‰å‚æ•°é«˜æ•ˆæŠ€æœ¯ï¼Œä½†åœ¨ç¡®ä¿ä½ç§©æ›´æ–°çš„å‡†ç¡®èšåˆå’Œç»´æŒä½ç³»ç»Ÿæˆæœ¬æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºAFLoRAï¼Œä¸€ä¸ªè‡ªé€‚åº”ä¸”è½»é‡çš„è”é‚¦å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å¼€é”€å¹¶æé«˜èšåˆå‡†ç¡®æ€§ã€‚AFLoRAé€šè¿‡è§£è€¦å…±äº«å’Œå®¢æˆ·ç«¯ç‰¹å®šæ›´æ–°ã€å¼•å…¥å¯¹è§’çŸ©é˜µåŸºç¡€çš„ç§©å‰ªæä»¥åŠé‡‡ç”¨ç§©æ„ŸçŸ¥èšåˆä¸å…¬å…±æ•°æ®ç²¾ç‚¼ï¼Œå¢å¼ºäº†åœ¨æ•°æ®å¼‚æ„æ€§ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAFLoRAåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸ºå¼‚æ„ç¯å¢ƒä¸­é«˜æ•ˆçš„LLMé€‚åº”æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¼‚æ„å®¢æˆ·ç«¯ä¸Šè¿›è¡Œå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ—¶çš„é«˜è®¡ç®—å’Œé€šä¿¡éœ€æ±‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½ç§©æ›´æ–°çš„èšåˆå‡†ç¡®æ€§å’Œç³»ç»Ÿæˆæœ¬æ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´æ•´ä½“æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAFLoRAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è§£è€¦å…±äº«å’Œå®¢æˆ·ç«¯ç‰¹å®šçš„æ›´æ–°æ¥é™ä½å¼€é”€ï¼ŒåŒæ—¶å¼•å…¥å¯¹è§’çŸ©é˜µåŸºç¡€çš„ç§©å‰ªææŠ€æœ¯ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æœ¬åœ°èµ„æºï¼Œå¹¶é€šè¿‡ç§©æ„ŸçŸ¥èšåˆå¢å¼ºåœ¨æ•°æ®å¼‚æ„æ€§ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAFLoRAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå…±äº«æ›´æ–°æ¨¡å—ã€å®¢æˆ·ç«¯ç‰¹å®šæ›´æ–°æ¨¡å—å’Œèšåˆæ¨¡å—ã€‚å…±äº«æ›´æ–°æ¨¡å—è´Ÿè´£å¤„ç†å…¨å±€æ¨¡å‹çš„æ›´æ–°ï¼Œè€Œå®¢æˆ·ç«¯ç‰¹å®šæ›´æ–°æ¨¡å—åˆ™å…³æ³¨äºæœ¬åœ°æ•°æ®çš„é€‚åº”æ€§è°ƒæ•´ï¼Œæœ€åé€šè¿‡èšåˆæ¨¡å—æ•´åˆå„å®¢æˆ·ç«¯çš„æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šAFLoRAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è‡ªé€‚åº”çš„ä½ç§©é€‚åº”ç­–ç•¥å’Œç§©æ„ŸçŸ¥èšåˆæœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å›ºå®šå‚æ•°æ›´æ–°æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹å®¢æˆ·ç«¯èµ„æºçš„å¼‚æ„æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒAFLoRAé‡‡ç”¨äº†å¯¹è§’çŸ©é˜µå‰ªææ¥ä¼˜åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†èšåˆå‡†ç¡®æ€§çš„çº¦æŸï¼Œä»¥ç¡®ä¿ä½ç§©æ›´æ–°çš„æœ‰æ•ˆæ•´åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AFLoRAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…¶å‡†ç¡®æ€§æé«˜äº†çº¦10%ï¼ŒåŒæ—¶è®¡ç®—å’Œé€šä¿¡å¼€é”€é™ä½äº†15%ã€‚è¿™äº›ç»“æœè¡¨æ˜AFLoRAåœ¨èµ„æºå—é™çš„å¼‚æ„ç¯å¢ƒä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AFLoRAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€é‡‘èå’Œæ™ºèƒ½åˆ¶é€ ç­‰è¡Œä¸šã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ•°æ®é€šå¸¸æ˜¯åˆ†æ•£çš„ä¸”å…·æœ‰éšç§ä¿æŠ¤éœ€æ±‚ï¼ŒAFLoRAèƒ½å¤Ÿåœ¨ä¿è¯æ•°æ®å®‰å…¨çš„å‰æä¸‹ï¼Œå®ç°é«˜æ•ˆçš„æ¨¡å‹é€‚åº”ï¼Œæå‡å„ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚æœªæ¥ï¼Œéšç€å¤§è¯­è¨€æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼ŒAFLoRAçš„æ¡†æ¶å¯èƒ½ä¼šæˆä¸ºè”é‚¦å­¦ä¹ é¢†åŸŸçš„é‡è¦å‚è€ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Federated fine-tuning has emerged as a promising approach to adapt foundation models to downstream tasks using decentralized data. However, real-world deployment remains challenging due to the high computational and communication demands of fine-tuning Large Language Models (LLMs) on clients with data and system resources that are heterogeneous and constrained. In such settings, the global model's performance is often bottlenecked by the weakest clients and further degraded by the non-IID nature of local data. Although existing methods leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to reduce communication and computation overhead, they often fail to simultaneously ensure accurate aggregation of low-rank updates and maintain low system costs, thereby hindering overall performance. To address these challenges, we propose AFLoRA, an adaptive and lightweight federated fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific updates to reduce overhead and improve aggregation accuracy, incorporates diagonal matrix-based rank pruning to better utilize local resources, and employs rank-aware aggregation with public data refinement to strengthen generalization under data heterogeneity. Extensive experiments demonstrate that AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency, providing a practical solution for efficient LLM adaptation in heterogeneous environments in the real world.

