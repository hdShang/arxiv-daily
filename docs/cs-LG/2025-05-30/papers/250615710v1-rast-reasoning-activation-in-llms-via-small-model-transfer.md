---
layout: default
title: RAST: Reasoning Activation in LLMs via Small-model Transfer
---

# RAST: Reasoning Activation in LLMs via Small-model Transfer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.15710" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.15710v1</a>
  <a href="https://arxiv.org/pdf/2506.15710.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.15710v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.15710v1', 'RAST: Reasoning Activation in LLMs via Small-model Transfer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siru Ouyang, Xinyu Zhu, Zilin Xiao, Minhao Jiang, Yu Meng, Jiawei Han

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://ozyyshr.github.io/RAST/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRASTä»¥é«˜æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `æ¨¡å‹è½¬ç§»` `è®¡ç®—æ•ˆç‡` `æ•°å­¦æ¨ç†` `å°æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§è§„æ¨¡åº”ç”¨æ—¶èµ„æºæ¶ˆè€—å·¨å¤§ï¼Œé™åˆ¶äº†å…¶æ¨å¹¿ã€‚
2. æœ¬æ–‡æå‡ºRASTï¼Œé€šè¿‡å°æ¨¡å‹çš„RLè®­ç»ƒè½¬ç§»æ¦‚ç‡è°ƒæ•´ï¼Œæå‡å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRASTåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ï¼Œä¸”GPUå†…å­˜æ¶ˆè€—æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å…¶åœ¨è§„æ¨¡åº”ç”¨æ—¶èµ„æºæ¶ˆè€—å·¨å¤§ã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼ŒRLå¹¶æœªä¸ºæ¨¡å‹èµ‹äºˆæ–°çŸ¥è¯†ï¼Œè€Œæ˜¯é‡å¡‘äº†æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡å‡è®¾RLå¼•èµ·çš„è¾“å‡ºæ¦‚ç‡å˜åŒ–åœ¨æ¨¡å‹è§„æ¨¡ä¸Šæ˜¯ç›¸å¯¹ä¸å˜çš„ï¼Œä»è€Œæå‡ºRASTæ–¹æ³•ï¼Œé€šè¿‡å°†å°æ¨¡å‹ä¸­RLè®­ç»ƒå¼•èµ·çš„æ¦‚ç‡è°ƒæ•´è½¬ç§»åˆ°å¤§æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡å…¶æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRASTåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ‰€éœ€GPUå†…å­˜æ˜¾è‘—ä½äºç›´æ¥RLè®­ç»ƒï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†RLè®­ç»ƒçš„æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºRLé©±åŠ¨çš„æ¨ç†æä¾›äº†æ–°è§è§£ï¼Œå¹¶æå‡ºäº†åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æ‰©å±•å…¶ä¼˜åŠ¿çš„å®ç”¨ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ä¸­çš„èµ„æºæ¶ˆè€—é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤šä¸ªæ¨¡å‹å‰¯æœ¬å’Œå¤§é‡GPUè®¡ç®—ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„RASTæ–¹æ³•å‡è®¾RLå¼•èµ·çš„è¾“å‡ºæ¦‚ç‡å˜åŒ–åœ¨ä¸åŒè§„æ¨¡æ¨¡å‹ä¸­æ˜¯ç›¸å¯¹ä¸å˜çš„ï¼Œå› æ­¤å¯ä»¥é€šè¿‡è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹å¹¶å°†å…¶æ¦‚ç‡è°ƒæ•´è½¬ç§»åˆ°å¤§æ¨¡å‹ä¸­ï¼Œä»è€Œé«˜æ•ˆæå‡æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRASTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹ï¼›å…¶æ¬¡ï¼Œå°†å°æ¨¡å‹ä¸­è·å¾—çš„æ¦‚ç‡è°ƒæ•´åº”ç”¨äºæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œä»¥å¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šRASTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å°æ¨¡å‹çš„RLè®­ç»ƒå®ç°æ¦‚ç‡è°ƒæ•´çš„è½¬ç§»ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›´æ¥åœ¨å¤§æ¨¡å‹ä¸Šè¿›è¡ŒRLè®­ç»ƒçš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒRASTå…³æ³¨äºå°æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å’Œè¾“å‡ºæ¦‚ç‡çš„å¯¹é½ï¼Œç¡®ä¿åœ¨è½¬ç§»è¿‡ç¨‹ä¸­ä¿æŒé«˜ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†æŸå¤±å‡½æ•°çš„é€‰æ‹©å’Œæ¨¡å‹å‚æ•°çš„è®¾ç½®ï¼Œä»¥ä¼˜åŒ–è½¬ç§»æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRASTåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ‰€éœ€GPUå†…å­˜æ¯”ç›´æ¥RLè®­ç»ƒä½å¾—å¤šã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒRASTçš„æ€§èƒ½ç”šè‡³è¶…è¿‡äº†ç»è¿‡RLè®­ç»ƒçš„æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œè‡ªåŠ¨æ¨ç†ç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒRASTå¯ä»¥åœ¨æ•™è‚²ã€é‡‘èåˆ†æå’Œç§‘å­¦ç ”ç©¶ç­‰å¤šä¸ªé¢†åŸŸæä¾›æ›´ä¸ºç²¾å‡†çš„å†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.

