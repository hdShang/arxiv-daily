---
layout: default
title: Hyperbolic Dataset Distillation
---

# Hyperbolic Dataset Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24623" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24623v2</a>
  <a href="https://arxiv.org/pdf/2505.24623.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24623v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24623v2', 'Hyperbolic Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenyuan Li, Guang Li, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama

**åˆ†ç±»**: cs.LG, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-10-17)

**å¤‡æ³¨**: Accepted to NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Guang000/HDD)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¶…æ›²é¢æ•°æ®é›†è’¸é¦æ–¹æ³•ä»¥è§£å†³å¤§è§„æ¨¡æ•°æ®é›†æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `è¶…æ›²é¢ç©ºé—´` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹å‹ç¼©` `åˆ†å¸ƒåŒ¹é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°æ®é›†è’¸é¦æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°æ®åˆ†å¸ƒæ—¶ï¼Œå¾€å¾€å¿½è§†äº†æ•°æ®ä¹‹é—´çš„å‡ ä½•å’Œå±‚æ¬¡å…³ç³»ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„HDDæ–¹æ³•åˆ©ç”¨è¶…æ›²é¢ç©ºé—´çš„ç‰¹æ€§ï¼Œé€šè¿‡ä¼˜åŒ–åˆæˆæ•°æ®ä¸åŸå§‹æ•°æ®çš„è´¨å¿ƒä¹‹é—´çš„è¶…æ›²é¢è·ç¦»ï¼Œå¢å¼ºäº†å±‚æ¬¡ç»“æ„çš„å»ºæ¨¡èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHDDæ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œä»…éœ€20%çš„è’¸é¦æ ¸å¿ƒé›†å³å¯å®ç°æ˜¾è‘—çš„è®­ç»ƒç¨³å®šæ€§æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è§£å†³æ·±åº¦å­¦ä¹ ä¸­å¤§è§„æ¨¡æ•°æ®é›†å¸¦æ¥çš„è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ï¼Œæ•°æ®é›†è’¸é¦è¢«æå‡ºä»¥åˆæˆä¸€ä¸ªç´§å‡‘çš„æ•°æ®é›†ï¼Œæ›¿ä»£åŸå§‹æ•°æ®é›†ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„æ¨¡å‹æ€§èƒ½ã€‚ä¸éœ€è¦æ˜‚è´µçš„åŒå±‚ä¼˜åŒ–çš„ä¼˜åŒ–æ–¹æ³•ä¸åŒï¼Œåˆ†å¸ƒåŒ¹é…æ–¹æ³•é€šè¿‡å¯¹é½åˆæˆæ•°æ®å’ŒåŸå§‹æ•°æ®çš„åˆ†å¸ƒæ¥æé«˜æ•ˆç‡ï¼Œä»è€Œæ¶ˆé™¤äº†åµŒå¥—ä¼˜åŒ–ã€‚ç°æœ‰çš„åˆ†å¸ƒåŒ¹é…æ–¹æ³•å±€é™äºæ¬§å‡ é‡Œå¾—ç©ºé—´ï¼Œå¿½è§†äº†å¤æ‚çš„å‡ ä½•å’Œå±‚æ¬¡å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¶…æ›²é¢æ•°æ®é›†è’¸é¦æ–¹æ³•HDDï¼Œåˆ©ç”¨è´Ÿæ›²ç‡çš„è¶…æ›²é¢ç©ºé—´è‡ªç„¶å»ºæ¨¡å±‚æ¬¡å’Œæ ‘çŠ¶ç»“æ„ã€‚HDDå°†ç‰¹å¾åµŒå…¥åˆ°æ´›ä¼¦å…¹è¶…æ›²é¢ç©ºé—´ï¼Œé€šè¿‡ä¼˜åŒ–åˆæˆæ•°æ®å’ŒåŸå§‹æ•°æ®çš„è´¨å¿ƒä¹‹é—´çš„è¶…æ›²é¢è·ç¦»ï¼Œæ˜¾å¼åœ°å°†å±‚æ¬¡ç»“æ„æ•´åˆåˆ°è’¸é¦è¿‡ç¨‹ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†è’¸é¦æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°æ®åˆ†å¸ƒæ—¶çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹æ•°æ®ä¹‹é—´å‡ ä½•å’Œå±‚æ¬¡å…³ç³»çš„å¿½è§†ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–äºåŒå±‚ä¼˜åŒ–ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHDDæ–¹æ³•é€šè¿‡å¼•å…¥è¶…æ›²é¢ç©ºé—´ï¼Œåˆ©ç”¨å…¶è´Ÿæ›²ç‡ç‰¹æ€§æ¥å»ºæ¨¡å±‚æ¬¡ç»“æ„ï¼Œä¼˜åŒ–åˆæˆæ•°æ®ä¸åŸå§‹æ•°æ®çš„è´¨å¿ƒä¹‹é—´çš„è¶…æ›²é¢è·ç¦»ï¼Œä»è€Œåœ¨è’¸é¦è¿‡ç¨‹ä¸­æ˜¾å¼æ•´åˆå±‚æ¬¡ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHDDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾æå–æ¨¡å—ã€è¶…æ›²é¢åµŒå…¥æ¨¡å—å’Œè·ç¦»ä¼˜åŒ–æ¨¡å—ã€‚ç‰¹å¾æå–æ¨¡å—ä½¿ç”¨æµ…å±‚ç½‘ç»œæå–æ•°æ®ç‰¹å¾ï¼Œéšåå°†è¿™äº›ç‰¹å¾åµŒå…¥åˆ°æ´›ä¼¦å…¹è¶…æ›²é¢ç©ºé—´ä¸­ï¼Œæœ€åé€šè¿‡ä¼˜åŒ–è´¨å¿ƒä¹‹é—´çš„è¶…æ›²é¢è·ç¦»æ¥å®ç°æ•°æ®è’¸é¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šHDDæ˜¯é¦–ä¸ªå°†è¶…æ›²é¢ç©ºé—´å¼•å…¥æ•°æ®é›†è’¸é¦è¿‡ç¨‹çš„æ–¹æ³•ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®çš„å±‚æ¬¡ç»“æ„å’Œå‡ ä½•ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨HDDä¸­ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºè´¨å¿ƒä¹‹é—´çš„è¶…æ›²é¢è·ç¦»ï¼Œç½‘ç»œç»“æ„é‡‡ç”¨æµ…å±‚ç½‘ç»œä»¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶åœ¨è¶…æ›²é¢ç©ºé—´ä¸­è¿›è¡Œæ•°æ®å‰ªæï¼Œä»…éœ€20%çš„è’¸é¦æ ¸å¿ƒé›†å³å¯ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHDDæ–¹æ³•åœ¨ä»…ä½¿ç”¨20%çš„è’¸é¦æ ¸å¿ƒé›†çš„æƒ…å†µä¸‹ï¼Œä»èƒ½ä¿æŒæ¨¡å‹æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚è¿™ä¸€ç»“æœç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†HDDåœ¨æ•°æ®é›†è’¸é¦ä¸­çš„é«˜æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå…¶ä»–éœ€è¦å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡æœ‰æ•ˆçš„è’¸é¦æ–¹æ³•ï¼ŒHDDå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è®­ç»ƒé«˜æ€§èƒ½æ¨¡å‹ï¼Œæå‡æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To address the computational and storage challenges posed by large-scale datasets in deep learning, dataset distillation has been proposed to synthesize a compact dataset that replaces the original while maintaining comparable model performance. Unlike optimization-based approaches that require costly bi-level optimization, distribution matching (DM) methods improve efficiency by aligning the distributions of synthetic and original data, thereby eliminating nested optimization. DM achieves high computational efficiency and has emerged as a promising solution. However, existing DM methods, constrained to Euclidean space, treat data as independent and identically distributed points, overlooking complex geometric and hierarchical relationships. To overcome this limitation, we propose a novel hyperbolic dataset distillation method, termed HDD. Hyperbolic space, characterized by negative curvature and exponential volume growth with distance, naturally models hierarchical and tree-like structures. HDD embeds features extracted by a shallow network into the Lorentz hyperbolic space, where the discrepancy between synthetic and original data is measured by the hyperbolic (geodesic) distance between their centroids. By optimizing this distance, the hierarchical structure is explicitly integrated into the distillation process, guiding synthetic samples to gravitate towards the root-centric regions of the original data distribution while preserving their underlying geometric characteristics. Furthermore, we find that pruning in hyperbolic space requires only 20% of the distilled core set to retain model performance, while significantly improving training stability. To the best of our knowledge, this is the first work to incorporate the hyperbolic space into the dataset distillation process. The code is available at https://github.com/Guang000/HDD.

