---
layout: default
title: Logits-Based Finetuning
---

# Logits-Based Finetuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24461" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24461v2</a>
  <a href="https://arxiv.org/pdf/2505.24461.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24461v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24461v2', 'Logits-Based Finetuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingyao Li, Senqiao Yang, Sitong Wu, Han Shi, Chuanyang Zheng, Hong Xu, Jiaya Jia

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-11)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dvlab-research/Logits-Based-Finetuning)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºlogitsçš„å¾®è°ƒæ–¹æ³•ä»¥è§£å†³ä¼ ç»ŸSFTçš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç›‘ç£å¾®è°ƒ` `çŸ¥è¯†è’¸é¦` `logits` `æ¨¡å‹è®­ç»ƒ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ•°å­¦åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰tokençº§åˆ«çš„ä¾èµ–å’Œè¯­è¨€å¤šæ ·æ€§ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºçš„åŸºäºlogitsçš„å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ•™å¸ˆlogitsä¸çœŸå®æ ‡ç­¾ï¼Œå¢å¼ºè®­ç»ƒç›®æ ‡çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æå‡è¾¾åˆ°7.28%ï¼Œå¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¼€å‘ç´§å‡‘é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¾èµ–å•ä¸€çš„çœŸå®æ ‡ç­¾ï¼Œå¾€å¾€æ— æ³•æ•æ‰åˆ°tokençº§åˆ«çš„ä¾èµ–å…³ç³»å’Œè¯­è¨€å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºlogitsçš„å¾®è°ƒæ¡†æ¶ï¼Œç»“åˆäº†ç›‘ç£å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦çš„ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ•™å¸ˆlogitsä¸çœŸå®æ ‡ç­¾ç»“åˆï¼Œæ„å»ºä¸°å¯Œçš„è®­ç»ƒç›®æ ‡ï¼Œä»è€Œç¡®ä¿è®­ç»ƒçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä»¥å¾€çš„SFTæ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡è¾¾åˆ°18%å’Œ22.7%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨æ•æ‰tokençº§åˆ«ä¾èµ–å’Œè¯­è¨€å¤šæ ·æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåŸºäºlogitsçš„å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ•™å¸ˆæ¨¡å‹çš„logitsä¸çœŸå®æ ‡ç­¾ï¼Œæ„å»ºæ›´ä¸°å¯Œçš„è®­ç»ƒç›®æ ‡ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€logitsç”Ÿæˆã€è®­ç»ƒç›®æ ‡æ„å»ºå’Œæ¨¡å‹è®­ç»ƒå››ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œæ„å»º1.2Mçš„logitsæ•°æ®é›†ï¼Œç„¶ååˆ©ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆlogitsï¼Œæœ€åç»“åˆçœŸå®æ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡ç»“åˆæ•™å¸ˆlogitsä¸çœŸå®æ ‡ç­¾ï¼Œå½¢æˆäº†æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œè¿™ä¸€æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè¯­è¨€å¤šæ ·æ€§ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å•ä¸€æ ‡ç­¾è®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†ç»“åˆlogitså’ŒçœŸå®æ ‡ç­¾çš„å¤åˆæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ—¢ä¿æŒå‡†ç¡®æ€§åˆå¢å¼ºå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºlogitsçš„å¾®è°ƒæ–¹æ³•åœ¨Mawpså’ŒTabMWPä»»åŠ¡ä¸Šåˆ†åˆ«æå‡äº†18%å’Œ22.7%çš„å‡†ç¡®ç‡ã€‚åœ¨ä¹ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å¹³å‡æå‡è¾¾7.28%ï¼Œæ˜¾è‘—ä¼˜äºä»¥å¾€çš„SFTæ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ•™è‚²æŠ€æœ¯å’Œç§‘å­¦è®¡ç®—ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥æ›´å¥½åœ°æ”¯æŒå¤æ‚çš„è¯­è¨€ç†è§£ä»»åŠ¡ï¼Œæ¨åŠ¨æ™ºèƒ½æ•™è‚²å’Œè‡ªåŠ¨åŒ–ç§‘å­¦ç ”ç©¶çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, developing compact and efficient large language models (LLMs) has emerged as a thriving area of research. Traditional Supervised Fine-Tuning (SFT), which relies on singular ground truth labels, often fails to capture token-level dependencies and linguistic diversity. To address these limitations, we propose a logits-based fine-tuning framework that integrates the strengths of supervised learning and knowledge distillation. Our approach constructs enriched training targets by combining teacher logits with ground truth labels, preserving both correctness and linguistic diversity. This ensures more reliable and effective training. We constructed a large-scale 1.2M logits dataset and trained a series of science-focused models. Experimental results demonstrate that our method achieves significant improvements, with accuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used mathematical benchmarks, our method consistently outperforms prior SFT models, achieving an average improvement of 7.28%. Codes are available at https://github.com/dvlab-research/Logits-Based-Finetuning.

