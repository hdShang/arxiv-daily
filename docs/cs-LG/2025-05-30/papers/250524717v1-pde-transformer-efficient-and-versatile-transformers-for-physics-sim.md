---
layout: default
title: "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations"
---

# PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24717" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24717v1</a>
  <a href="https://arxiv.org/pdf/2505.24717.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24717v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24717v1', 'PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Benjamin Holzschuh, Qiang Liu, Georg Kohl, Nils Thuerey

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: ICML 2025. Code available at https://github.com/tum-pbs/pde-transformer

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPDE-Transformerä»¥æå‡ç‰©ç†ä»¿çœŸå»ºæ¨¡æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç‰©ç†ä»¿çœŸ` `å˜æ¢å™¨æ¶æ„` `åå¾®åˆ†æ–¹ç¨‹` `ä»£ç†å»ºæ¨¡` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `å¤§è§„æ¨¡æ•°æ®å¤„ç†` `æ—¶ç©ºæ ‡è®°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç‰©ç†ä»¿çœŸå»ºæ¨¡æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥åŒæ—¶å­¦ä¹ å¤šç§ç±»å‹çš„åå¾®åˆ†æ–¹ç¨‹ã€‚
2. PDE-Transformeré€šè¿‡å°†ä¸åŒç‰©ç†é€šé“åµŒå…¥ä¸ºæ—¶ç©ºæ ‡è®°ï¼Œå¹¶åˆ©ç”¨é€šé“é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ã€‚
3. é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä»å¤´è®­ç»ƒï¼Œæ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶è¶…è¶Šäº†å…¶ä»–ç‰©ç†ä»¿çœŸåŸºç¡€æ¨¡å‹æ¶æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†PDE-Transformerï¼Œè¿™æ˜¯ä¸€ç§æ”¹è¿›çš„åŸºäºå˜æ¢å™¨çš„æ¶æ„ï¼Œæ—¨åœ¨å¯¹è§„åˆ™ç½‘æ ¼ä¸Šçš„ç‰©ç†ä»¿çœŸè¿›è¡Œä»£ç†å»ºæ¨¡ã€‚é€šè¿‡ç»“åˆæ‰©æ•£å˜æ¢å™¨çš„æœ€æ–°æ¶æ„æ”¹è¿›ä»¥åŠé’ˆå¯¹å¤§è§„æ¨¡ä»¿çœŸçš„ç‰¹å®šè°ƒæ•´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç§æ›´å…·å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§çš„å˜æ¢å™¨æ¶æ„ï¼Œå¯ä½œä¸ºç‰©ç†ç§‘å­¦é¢†åŸŸå¤§å‹åŸºç¡€æ¨¡å‹çš„æ”¯æ’‘ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¶æ„åœ¨16ç§ä¸åŒç±»å‹çš„åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰çš„å¤§å‹æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„å˜æ¢å™¨æ¶æ„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç‰©ç†ä»¿çœŸå»ºæ¨¡æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶çš„ä½æ•ˆé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨åŒæ—¶å­¦ä¹ å¤šç§ç±»å‹çš„åå¾®åˆ†æ–¹ç¨‹æ—¶ï¼Œä¿¡æ¯å¯†åº¦ä¸å‡åŒ€å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPDE-Transformeré€šè¿‡å°†ä¸åŒç‰©ç†é€šé“ä½œä¸ºç‹¬ç«‹çš„æ—¶ç©ºæ ‡è®°è¿›è¡ŒåµŒå…¥ï¼Œåˆ©ç”¨é€šé“é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¿æŒäº†ä¿¡æ¯å¯†åº¦çš„ä¸€è‡´æ€§ï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¶æ„ä¸»è¦åŒ…æ‹¬æ•°æ®é¢„å¤„ç†æ¨¡å—ã€æ—¶ç©ºæ ‡è®°åµŒå…¥æ¨¡å—ã€é€šé“é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥åŠè¾“å‡ºç”Ÿæˆæ¨¡å—ã€‚é€šè¿‡è¿™äº›æ¨¡å—çš„ååŒå·¥ä½œï¼ŒPDE-Transformerèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„ç‰©ç†ä»¿çœŸä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šPDE-Transformerçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ç‹¬ç‰¹çš„æ—¶ç©ºæ ‡è®°åµŒå…¥æ–¹å¼å’Œé€šé“é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å˜æ¢å™¨æ¶æ„ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†å¯¹å¤šç§PDEçš„å»ºæ¨¡èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šä»»åŠ¡å­¦ä¹ æ•ˆæœï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”å¤§è§„æ¨¡ç‰©ç†ä»¿çœŸæ•°æ®çš„å¤„ç†éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒPDE-Transformeråœ¨16ç§ä¸åŒç±»å‹çš„åå¾®åˆ†æ–¹ç¨‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„å˜æ¢å™¨æ¶æ„ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PDE-Transformerçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ°”å€™æ¨¡æ‹Ÿã€æµä½“åŠ¨åŠ›å­¦ã€ææ–™ç§‘å­¦ç­‰ç‰©ç†ç§‘å­¦é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„å»ºæ¨¡èƒ½åŠ›èƒ½å¤ŸåŠ é€Ÿç§‘å­¦ç ”ç©¶å’Œå·¥ç¨‹åº”ç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ä¸åˆ›æ–°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.

