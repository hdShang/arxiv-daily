---
layout: default
title: Adversarial Preference Learning for Robust LLM Alignment
---

# Adversarial Preference Learning for Robust LLM Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24369" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24369v1</a>
  <a href="https://arxiv.org/pdf/2505.24369.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24369v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24369v1', 'Adversarial Preference Learning for Robust LLM Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, Mingchuan Yang

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: Accepted at ACL2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æŠ—åå¥½å­¦ä¹ ä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯¹æŠ—å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `é²æ£’æ€§` `äººç±»åé¦ˆ` `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨åŒ–åé¦ˆ` `ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ` `å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¯¹æŠ—æ”»å‡»æ—¶æ•ˆç‡ä½ä¸”æˆæœ¬é«˜ï¼Œä¸”å®¹æ˜“å—åˆ°åé¦ˆåå·®å½±å“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºå¯¹æŠ—åå¥½å­¦ä¹ ï¼ˆAPLï¼‰ï¼Œé€šè¿‡ç›´æ¥æœ‰å®³æ€§åº¦é‡å’Œæ¡ä»¶ç”Ÿæˆæ”»å‡»è€…æ¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šAPLåœ¨Mistral-7B-Instruct-v0.3ä¸Šå®ç°äº†83.33%çš„æ— å®³æ€§èƒœç‡ï¼Œæœ‰å®³è¾“å‡ºå‡å°‘è‡³0.43%ï¼Œæ”»å‡»æˆåŠŸç‡é™ä½65%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£è¯­è¨€æ¨¡å‹é€šå¸¸ä¾èµ–äºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ¥ä¿ƒè¿›å®‰å…¨è¡Œä¸ºã€‚ç„¶è€Œï¼Œç”±äºäººç±»æ ‡æ³¨æ•ˆç‡ä½ã€æ½œåœ¨å¯¹æŠ—æ”»å‡»å¤šæ ·æ€§ä»¥åŠåé¦ˆåå·®å’Œå¥–åŠ±æ“æ§é£é™©ç­‰ä¸‰å¤§é™åˆ¶ï¼Œå®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ”»å‡»ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯¹æŠ—åå¥½å­¦ä¹ ï¼ˆAPLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¿­ä»£çš„å¯¹æŠ—è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼šåŸºäºæ¨¡å‹å†…åœ¨åå¥½æ¦‚ç‡çš„ç›´æ¥æœ‰å®³æ€§åº¦é‡ã€åˆæˆè¾“å…¥ç‰¹å®šå¯¹æŠ—å˜ä½“çš„æ¡ä»¶ç”Ÿæˆæ”»å‡»è€…ï¼Œä»¥åŠé€šè¿‡è‡ªåŠ¨é—­ç¯åé¦ˆå®ç°çš„è¿­ä»£æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPLæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œå‡å°‘äº†æœ‰å®³è¾“å‡ºï¼Œå¹¶ä¿æŒäº†ç«äº‰åŠ›çš„å®ç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†äººç±»åé¦ˆæ—¶å­˜åœ¨æ•ˆç‡ä½ã€æˆæœ¬é«˜å’Œåé¦ˆåå·®ç­‰ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¯¹æŠ—åå¥½å­¦ä¹ ï¼ˆAPLï¼‰ï¼Œé€šè¿‡å¼•å…¥ç›´æ¥æœ‰å®³æ€§åº¦é‡å’Œæ¡ä»¶ç”Ÿæˆæ”»å‡»è€…ï¼Œå‡å°‘å¯¹å¤–éƒ¨è¯„ä¼°çš„ä¾èµ–ï¼Œä»è€Œæå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAPLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç›´æ¥æœ‰å®³æ€§åº¦é‡ã€æ¡ä»¶ç”Ÿæˆæ”»å‡»è€…å’Œè¿­ä»£åé¦ˆæœºåˆ¶ã€‚ç›´æ¥æœ‰å®³æ€§åº¦é‡ç”¨äºè¯„ä¼°æ¨¡å‹è¾“å‡ºçš„æ½œåœ¨å±å®³ï¼Œæ¡ä»¶ç”Ÿæˆæ”»å‡»è€…åˆ™æ ¹æ®è¾“å…¥ç”Ÿæˆç‰¹å®šçš„å¯¹æŠ—æ ·æœ¬ï¼Œè¿­ä»£åé¦ˆæœºåˆ¶åˆ™é€šè¿‡é—­ç¯åé¦ˆä¸æ–­ä¼˜åŒ–æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šAPLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºæ¨¡å‹å†…åœ¨åå¥½æ¦‚ç‡çš„æœ‰å®³æ€§åº¦é‡å’Œæ¡ä»¶ç”Ÿæˆæ”»å‡»è€…ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤–éƒ¨è¯„ä¼°çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è‡ªåŠ¨åŒ–çš„é—­ç¯åé¦ˆæœºåˆ¶ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨å‘ç°è„†å¼±æ€§åè¿›è¡ŒæŒç»­é€‚åº”ï¼Œæ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¯¹æŠ—æ ·æœ¬çš„ç”Ÿæˆå’Œè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAPLåœ¨Mistral-7B-Instruct-v0.3ä¸Šå®ç°äº†83.33%çš„æ— å®³æ€§èƒœç‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæœ‰å®³è¾“å‡ºä»5.88%é™è‡³0.43%ï¼Œæ”»å‡»æˆåŠŸç‡é™ä½äº†65%ã€‚åŒæ—¶ï¼ŒAPLåœ¨å®ç”¨æ€§æ–¹é¢ä¿æŒç«äº‰åŠ›ï¼ŒMT-Benchå¾—åˆ†ä¸º6.59ï¼Œæ¥è¿‘åŸºçº¿çš„6.78ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ€§è¦æ±‚é«˜çš„å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨å†…å®¹å®¡æ ¸å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ï¼ŒAPLèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æœ‰å®³è¾“å‡ºï¼Œå¢å¼ºç”¨æˆ·ä¿¡ä»»ï¼Œæœªæ¥å¯èƒ½åœ¨å¤šä¸ªè¡Œä¸šä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.

