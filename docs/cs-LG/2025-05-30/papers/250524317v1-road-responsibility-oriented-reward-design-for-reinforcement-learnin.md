---
layout: default
title: ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving
---

# ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24317" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24317v1</a>
  <a href="https://arxiv.org/pdf/2505.24317.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24317v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24317v1', 'ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongming Chen, Miner Chen, Liewen Liao, Mingyang Jiang, Xiang Zuo, Hengrui Zhang, Yuchen Xi, Songan Zhang

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè´£ä»»å¯¼å‘å¥–åŠ±è®¾è®¡ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„å¥–åŠ±å‡½æ•°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶` `å¥–åŠ±å‡½æ•°` `äº¤é€šæ³•è§„` `çŸ¥è¯†å›¾è°±` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ£€ç´¢å¢å¼ºç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±å‡½æ•°è®¾è®¡æ–¹æ³•ä¾èµ–äºäººå·¥è®¾è®¡ï¼Œéš¾ä»¥é€‚åº”å¤æ‚çš„è‡ªåŠ¨é©¾é©¶åœºæ™¯ï¼Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è´£ä»»å¯¼å‘çš„å¥–åŠ±å‡½æ•°ï¼Œé€šè¿‡å¼•å…¥äº¤é€šæ³•è§„çŸ¥è¯†å›¾è°±å’Œè‡ªåŠ¨åŒ–å¥–åŠ±åˆ†é…æœºåˆ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨äº‹æ•…è´£ä»»åˆ†é…å‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†ä»£ç†çš„äº¤é€šäº‹æ•…è´£ä»»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é‡‡ç”¨è¯•é”™æœºåˆ¶ä»¥å¢å¼ºåœ¨ä¸å¯é¢„æµ‹ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œè®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ‰‹åŠ¨è®¾è®¡ä¸”åœ¨å¤æ‚åœºæ™¯ä¸­æ•ˆæœæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è´£ä»»å¯¼å‘çš„å¥–åŠ±å‡½æ•°ï¼Œæ˜ç¡®å°†äº¤é€šæ³•è§„çº³å…¥RLæ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†äº¤é€šæ³•è§„çŸ¥è¯†å›¾è°±ï¼Œå¹¶ç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹åŠæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯æ¥è‡ªåŠ¨åŒ–å¥–åŠ±åˆ†é…ã€‚è¿™ç§é›†æˆæ–¹æ³•æŒ‡å¯¼ä»£ç†ä¸¥æ ¼éµå®ˆäº¤é€šæ³•å¾‹ï¼Œä»è€Œæœ€å°åŒ–è§„åˆ™è¿åå¹¶ä¼˜åŒ–å¤šæ ·åŒ–é©¾é©¶æ¡ä»¶ä¸‹çš„å†³ç­–æ€§èƒ½ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†äº‹æ•…è´£ä»»åˆ†é…çš„å‡†ç¡®æ€§ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†ä»£ç†åœ¨äº¤é€šäº‹ä»¶ä¸­çš„è´£ä»»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°è®¾è®¡çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äººå·¥è®¾è®¡ï¼Œéš¾ä»¥é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„äº¤é€šç¯å¢ƒï¼Œå¯¼è‡´ä»£ç†çš„å†³ç­–æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§è´£ä»»å¯¼å‘çš„å¥–åŠ±å‡½æ•°ï¼Œæ˜ç¡®å°†äº¤é€šæ³•è§„çº³å…¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–çš„æ–¹å¼ä¼˜åŒ–å¥–åŠ±åˆ†é…ï¼Œç¡®ä¿ä»£ç†éµå¾ªäº¤é€šæ³•å¾‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬äº¤é€šæ³•è§„çŸ¥è¯†å›¾è°±çš„æ„å»ºã€è§†è§‰-è¯­è¨€æ¨¡å‹çš„åº”ç”¨ä»¥åŠæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯çš„é›†æˆã€‚é¦–å…ˆï¼Œé€šè¿‡çŸ¥è¯†å›¾è°±æå–äº¤é€šæ³•è§„ä¿¡æ¯ï¼Œç„¶ååˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è§£æç¯å¢ƒä¿¡æ¯ï¼Œæœ€åé€šè¿‡æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯è‡ªåŠ¨åˆ†é…å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†äº¤é€šæ³•è§„ç³»ç»ŸåŒ–åœ°èå…¥åˆ°å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±è®¾è®¡ä¸­ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ï¼Œä½¿å¾—å¥–åŠ±åˆ†é…æ›´åŠ æ™ºèƒ½å’Œè‡ªåŠ¨åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®ºæ–‡è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¥–åŠ±åˆ†é…ï¼Œå¹¶é€‰æ‹©äº†é€‚åˆçš„ç½‘ç»œç»“æ„ä»¥æ”¯æŒè§†è§‰-è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆè¿è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„è´£ä»»å¯¼å‘å¥–åŠ±è®¾è®¡åœ¨äº‹æ•…è´£ä»»åˆ†é…å‡†ç¡®æ€§ä¸Šæé«˜äº†çº¦30%ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†ä»£ç†åœ¨äº¤é€šäº‹ä»¶ä¸­çš„è´£ä»»ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¥–åŠ±è®¾è®¡æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿä»¥åŠæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œèƒ½å¤Ÿæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå†³ç­–æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) in autonomous driving employs a trial-and-error mechanism, enhancing robustness in unpredictable environments. However, crafting effective reward functions remains challenging, as conventional approaches rely heavily on manual design and demonstrate limited efficacy in complex scenarios. To address this issue, this study introduces a responsibility-oriented reward function that explicitly incorporates traffic regulations into the RL framework. Specifically, we introduced a Traffic Regulation Knowledge Graph and leveraged Vision-Language Models alongside Retrieval-Augmented Generation techniques to automate reward assignment. This integration guides agents to adhere strictly to traffic laws, thus minimizing rule violations and optimizing decision-making performance in diverse driving conditions. Experimental validations demonstrate that the proposed methodology significantly improves the accuracy of assigning accident responsibilities and effectively reduces the agent's liability in traffic incidents.

