---
layout: default
title: Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking
---

# Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24857" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24857v1</a>
  <a href="https://arxiv.org/pdf/2505.24857.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24857v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24857v1', 'Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, Brian Karrer

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEB-Samplerä»¥åŠ é€Ÿä»æ©è”½æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `æ©è”½æ‰©æ•£æ¨¡å‹` `é«˜æ•ˆé‡‡æ ·` `ç†µæœ‰ç•Œè§£æ©è”½` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ©è”½æ‰©æ•£æ¨¡å‹åœ¨é‡‡æ ·æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†éƒ¨åˆ†æ©è”½åºåˆ—æ—¶æœªèƒ½å……åˆ†åˆ©ç”¨ä¿¡æ¯ã€‚
2. è®ºæ–‡æå‡ºEB-Samplerï¼Œé€šè¿‡ç†µæœ‰ç•Œçš„è§£æ©è”½æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸€æ¬¡è¯„ä¼°ä¸­è§£æ©è”½å¤šä¸ªæ ‡è®°ï¼Œæé«˜é‡‡æ ·æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEB-Sampleråœ¨æ ‡å‡†åŸºå‡†ä¸ŠåŠ é€Ÿé‡‡æ ·2-3å€ï¼ŒåŒæ—¶åœ¨å°å‹æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸçš„æ©è”½æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é«˜æ•ˆé‡‡æ ·çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ï¼Œéƒ¨åˆ†æ©è”½çš„åºåˆ—å¯ä»¥ç¡®å®šå¤šä¸ªæœªçŸ¥æ ‡è®°çš„å€¼ï¼Œå› æ­¤å•æ¬¡é¢„æµ‹åŒ…å«æœªè¢«æ ‡å‡†é‡‡æ ·ç¨‹åºåˆ©ç”¨çš„é¢å¤–ä¿¡æ¯ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†EB-Samplerï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ›¿ä»£ç°æœ‰é‡‡æ ·å™¨çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç†µæœ‰ç•Œçš„è§£æ©è”½ç¨‹åºï¼Œåœ¨ä¸€æ¬¡å‡½æ•°è¯„ä¼°ä¸­åŠ¨æ€è§£æ©è”½å¤šä¸ªæ ‡è®°ï¼Œå¹¶è®¾å®šäº†è¿‘ä¼¼è¯¯å·®å®¹å¿åº¦ã€‚EB-Sampleråœ¨æ ‡å‡†ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†ä¸ŠåŠ é€Ÿé‡‡æ ·é€Ÿåº¦çº¦2-3å€ï¼Œä¸”æ€§èƒ½æ— æŸï¼ŒåŒæ—¶åœ¨è¿·å®«å¯¼èˆªå’Œæ•°ç‹¬ç­‰è¾ƒå°æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°è‰¯å¥½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»æ©è”½æ‰©æ•£æ¨¡å‹ä¸­é«˜æ•ˆé‡‡æ ·çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éƒ¨åˆ†æ©è”½åºåˆ—æ—¶ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å•æ¬¡é¢„æµ‹æ‰€åŒ…å«çš„é¢å¤–ä¿¡æ¯ï¼Œå¯¼è‡´é‡‡æ ·æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç†µæœ‰ç•Œçš„è§£æ©è”½ç¨‹åºï¼ŒåŠ¨æ€è§£æ©è”½å¤šä¸ªæ ‡è®°ï¼Œä»è€Œåœ¨ä¸€æ¬¡å‡½æ•°è¯„ä¼°ä¸­æé«˜é‡‡æ ·æ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿå……åˆ†åˆ©ç”¨éƒ¨åˆ†æ©è”½åºåˆ—æ‰€è•´å«çš„ä¿¡æ¯ï¼Œå‡å°‘é‡‡æ ·æ‰€éœ€çš„è®¡ç®—é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEB-Samplerçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥éƒ¨åˆ†æ©è”½åºåˆ—ã€ç†µæœ‰ç•Œè§£æ©è”½æ¨¡å—å’Œè¾“å‡ºè§£æ©è”½ç»“æœã€‚è¯¥æ¡†æ¶å…è®¸åœ¨ä¸€æ¬¡è¯„ä¼°ä¸­åŒæ—¶è§£æ©è”½å¤šä¸ªæ ‡è®°ï¼Œæå‡äº†é‡‡æ ·é€Ÿåº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šEB-Samplerçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç†µæœ‰ç•Œçš„è§£æ©è”½æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æé«˜é‡‡æ ·é€Ÿåº¦ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„é€æ­¥é‡‡æ ·æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…é€šå¸¸åªå…³æ³¨å•ä¸ªæ ‡è®°çš„é¢„æµ‹ã€‚

**å…³é”®è®¾è®¡**ï¼šEB-Samplerçš„è®¾è®¡ä¸­ï¼Œè®¾å®šäº†è¿‘ä¼¼è¯¯å·®å®¹å¿åº¦ï¼Œä»¥ç¡®ä¿åœ¨è§£æ©è”½è¿‡ç¨‹ä¸­ä¸ä¼šæŸå¤±é‡è¦ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ¨¡å—çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ä¼˜åŒ–æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEB-Sampleråœ¨æ ‡å‡†ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†ä¸ŠåŠ é€Ÿé‡‡æ ·é€Ÿåº¦çº¦2-3å€ï¼Œä¸”åœ¨å°å‹æ¨ç†ä»»åŠ¡å¦‚è¿·å®«å¯¼èˆªå’Œæ•°ç‹¬ä¸­è¡¨ç°è‰¯å¥½ï¼Œå±•ç°å‡ºè¾ƒå¼ºçš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰ã€‚é€šè¿‡æé«˜æ©è”½æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·æ•ˆç‡ï¼ŒEB-Samplerèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´å¿«çš„å“åº”æ—¶é—´å’Œæ›´é«˜çš„å¤„ç†èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent masked diffusion models (MDMs) have shown competitive performance compared to autoregressive models (ARMs) for language modeling. While most literature has focused on performance enhancing sampling procedures, efficient sampling from MDMs has been scarcely explored. We make the observation that often a given sequence of partially masked tokens determines the values of multiple unknown tokens deterministically, meaning that a single prediction of a masked model holds additional information unused by standard sampling procedures. Based on this observation, we introduce EB-Sampler, a simple drop-in replacement for existing samplers, utilizing an Entropy Bounded unmasking procedure that dynamically unmasks multiple tokens in one function evaluation with predefined approximate error tolerance. We formulate the EB-Sampler as part of a broad family of adaptive samplers for which we provide an error analysis that motivates our algorithmic choices. EB-Sampler accelerates sampling from current state of the art MDMs by roughly 2-3x on standard coding and math reasoning benchmarks without loss in performance. We also validate the same procedure works well on smaller reasoning tasks including maze navigation and Sudoku, tasks ARMs often struggle with.

