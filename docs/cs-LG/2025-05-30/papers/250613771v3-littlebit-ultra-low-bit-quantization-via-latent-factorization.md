---
layout: default
title: LittleBit: Ultra Low-Bit Quantization via Latent Factorization
---

# LittleBit: Ultra Low-Bit Quantization via Latent Factorization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.13771" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.13771v3</a>
  <a href="https://arxiv.org/pdf/2506.13771.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.13771v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.13771v3', 'LittleBit: Ultra Low-Bit Quantization via Latent Factorization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Banseok Lee, Dongkyu Kim, Youngcheon You, Youngmin Kim

**åˆ†ç±»**: cs.LG, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-12-04)

**å¤‡æ³¨**: Accepted to NeurIPS 2025. Banseok Lee and Dongkyu Kim contributed equally

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SamsungLabs/LittleBit)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLittleBitä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹è¶…ä½æ¯”ç‰¹é‡åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é‡åŒ–æŠ€æœ¯` `å¤§è¯­è¨€æ¨¡å‹` `ä½æ¯”ç‰¹é‡åŒ–` `æ½œåœ¨çŸ©é˜µåˆ†è§£` `æ¨¡å‹å‹ç¼©` `è®¡ç®—æ•ˆç‡` `èµ„æºä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨äºš1æ¯”ç‰¹é‡åŒ–ä¸­é¢ä¸´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œé™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚
2. LittleBité€šè¿‡æ½œåœ¨çŸ©é˜µåˆ†è§£å’Œå¤šå°ºåº¦è¡¥å¿æœºåˆ¶ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æç«¯é‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å†…å­˜å ç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLittleBitåœ¨Llama2-7Bä¸Šçš„0.1 BPWæ€§èƒ½ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•çš„0.7 BPWï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ï¼Œé€šå¸¸é¢ä¸´æ˜¾è‘—çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚é‡åŒ–æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨äºš1æ¯”ç‰¹èŒƒå›´å†…çš„æ€§èƒ½ä¸‹é™å°¤å…¶å›°éš¾ã€‚æœ¬æ–‡ä»‹ç»äº†LittleBitï¼Œè¿™æ˜¯ä¸€ç§æç«¯LLMå‹ç¼©çš„æ–°æ–¹æ³•ï¼Œç›®æ ‡æ˜¯æ¯ä¸ªæƒé‡0.1æ¯”ç‰¹ï¼ˆBPWï¼‰ï¼Œå®ç°äº†è¿‘31å€çš„å†…å­˜å‡å°‘ï¼Œä¾‹å¦‚å°†Llama2-13Bå‹ç¼©è‡³0.9 GBä»¥ä¸‹ã€‚LittleBité€šè¿‡æ½œåœ¨çŸ©é˜µåˆ†è§£ä»¥ä½ç§©å½¢å¼è¡¨ç¤ºæƒé‡ï¼Œå¹¶éšåå¯¹è¿™äº›å› å­è¿›è¡ŒäºŒå€¼åŒ–ã€‚ä¸ºæŠµæ¶ˆæç«¯ç²¾åº¦å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ï¼Œå®ƒé›†æˆäº†å¤šå°ºåº¦è¡¥å¿æœºåˆ¶ï¼ŒåŒ…æ‹¬è¡Œã€åˆ—å’Œé¢å¤–çš„æ½œåœ¨ç»´åº¦ï¼Œä»¥å­¦ä¹ æ¯ä¸ªç§©çš„é‡è¦æ€§ã€‚å®éªŒç»“æœç¡®è®¤LittleBitåœ¨äºš1æ¯”ç‰¹é‡åŒ–ä¸­çš„ä¼˜è¶Šæ€§ï¼Œå…¶åœ¨Llama2-7Bä¸Šçš„0.1 BPWæ€§èƒ½è¶…è¿‡äº†é¢†å…ˆæ–¹æ³•çš„0.7 BPWã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡åŒ–è¿‡ç¨‹ä¸­é¢ä¸´çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨äºš1æ¯”ç‰¹é‡åŒ–æ—¶æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æä½æ¯”ç‰¹é‡åŒ–æ—¶å¾€å¾€æ— æ³•ä¿æŒæ¨¡å‹æ€§èƒ½ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLittleBitçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ½œåœ¨çŸ©é˜µåˆ†è§£å°†æƒé‡è¡¨ç¤ºä¸ºä½ç§©å½¢å¼ï¼Œå¹¶å¯¹è¿™äº›å› å­è¿›è¡ŒäºŒå€¼åŒ–ã€‚é€šè¿‡å¼•å…¥å¤šå°ºåº¦è¡¥å¿æœºåˆ¶ï¼ŒLittleBitèƒ½å¤Ÿæœ‰æ•ˆæŠµæ¶ˆå› æç«¯é‡åŒ–å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ï¼Œä»è€Œä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLittleBitçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æƒé‡çš„ä½ç§©è¡¨ç¤ºã€å› å­çš„äºŒå€¼åŒ–ä»¥åŠå¤šå°ºåº¦è¡¥å¿æœºåˆ¶ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹é¦–å…ˆé€šè¿‡æ½œåœ¨çŸ©é˜µåˆ†è§£è·å–ä½ç§©è¡¨ç¤ºï¼Œç„¶åå¯¹è¿™äº›è¡¨ç¤ºè¿›è¡ŒäºŒå€¼åŒ–ï¼Œæœ€åé€šè¿‡è¡Œã€åˆ—å’Œé¢å¤–çš„æ½œåœ¨ç»´åº¦è¿›è¡Œè¡¥å¿ï¼Œä»¥å­¦ä¹ æ¯ä¸ªç§©çš„é‡è¦æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŒç¬¦å·å€¼ç‹¬ç«‹åˆ†è§£ï¼ˆDual-SVIDï¼‰ç”¨äºé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰åˆå§‹åŒ–ï¼Œä»¥åŠé›†æˆæ®‹å·®è¡¥å¿æœºåˆ¶ä»¥å‡è½»è¯¯å·®ã€‚è¿™äº›åˆ›æ–°ä½¿å¾—åœ¨æä½æ¯”ç‰¹é‡åŒ–ä¸‹ä»èƒ½ä¿æŒè¾ƒé«˜çš„æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒLittleBité‡‡ç”¨äº†ä½ç§©çŸ©é˜µåˆ†è§£çš„ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†é‡åŒ–è¯¯å·®å’Œè¡¥å¿æœºåˆ¶çš„ç»“åˆï¼Œç½‘ç»œç»“æ„åˆ™é€šè¿‡å¼•å…¥å¤šå°ºåº¦è¡¥å¿æ¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLittleBitåœ¨Llama2-7Bä¸Šçš„0.1 BPWæ€§èƒ½è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•çš„0.7 BPWï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒLittleBitåœ¨å†…å­˜å ç”¨ä¸Šå®ç°äº†è¿‘31å€çš„å‡å°‘ï¼Œå¹¶åœ¨å†…æ ¸çº§åˆ«ä¸Šæä¾›äº†11.6å€çš„é€Ÿåº¦æå‡ï¼Œç›¸è¾ƒäºFP16ï¼Œæå¤§åœ°æ”¹å–„äº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LittleBitçš„ç ”ç©¶æˆæœåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—å’ŒåµŒå…¥å¼ç³»ç»Ÿä¸­ã€‚é€šè¿‡æç«¯é‡åŒ–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½å†…å­˜å’Œè®¡ç®—éœ€æ±‚ï¼Œä»è€Œæ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„æ™®åŠå’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. LittleBit establishes a new, viable size-performance trade-off--unlocking a potential 11.6$\times$ speedup over FP16 at the kernel level--and makes powerful LLMs practical for resource-constrained environments. Our code can be found at https://github.com/SamsungLabs/LittleBit.

