---
layout: default
title: "Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents"
---

# Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00172" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00172v1</a>
  <a href="https://arxiv.org/pdf/2506.00172.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00172v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00172v1', 'Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaivalya Hariharan, Uzay Girit, Atticus Wang, Jacob Andreas

**åˆ†ç±»**: cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: 21 pages, 14 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBreakpointä»¥è§£å†³LLMä»£ç ä»£ç†çš„ç³»ç»Ÿçº§æ¨ç†è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç³»ç»Ÿçº§æ¨ç†` `ä»£ç ä¿®å¤` `åŸºå‡†æµ‹è¯•` `è‡ªåŠ¨åŒ–ç”Ÿæˆä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨çŸ­æœŸæ¨ç†ï¼Œéš¾ä»¥è¯„ä¼°é•¿æ—¶é—´è·¨åº¦çš„ç³»ç»Ÿçº§æ¨ç†èƒ½åŠ›ã€‚
2. Breakpointæ–¹æ³•é€šè¿‡å¯¹æŠ—æ€§ç ´åçœŸå®è½¯ä»¶åº“ä¸­çš„å‡½æ•°ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„ä»£ç ä¿®å¤ä»»åŠ¡ï¼Œç³»ç»Ÿæ§åˆ¶ä»»åŠ¡éš¾åº¦ã€‚
3. åœ¨900å¤šä¸ªç”Ÿæˆä»»åŠ¡çš„å®éªŒä¸­ï¼ŒBreakpointå±•ç¤ºäº†å…¶åœ¨éš¾åº¦æ‰©å±•ä¸Šçš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸç‡ä»55%é™è‡³0%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°çŸ­æœŸã€å±€éƒ¨æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„é•¿æ—¶é—´è·¨åº¦åŸºå‡†ï¼ˆå¦‚SWE-benchï¼‰ä¾èµ–äººå·¥ç­–åˆ’çš„é—®é¢˜ï¼Œæ‰©å±•æˆ–è°ƒæ•´éš¾åº¦éœ€è¦æ˜‚è´µçš„äººåŠ›æˆæœ¬ï¼Œä¸”è¯„ä¼°å¾ˆå¿«è¾¾åˆ°é¥±å’Œã€‚ç„¶è€Œï¼Œè®¸å¤šç°å®ä»»åŠ¡ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹æˆ–ç§‘å­¦ç ”ç©¶ï¼Œè¦æ±‚ä»£ç†å¿«é€Ÿç†è§£å’ŒåŠ¨æ€å¤„ç†æ–°é¢–å¤æ‚çš„ç»“æ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Breakpointï¼Œä¸€ç§é€šè¿‡å¯¹çœŸå®è½¯ä»¶åº“ä¸­çš„å‡½æ•°è¿›è¡Œå¯¹æŠ—æ€§ç ´åï¼Œè‡ªåŠ¨ç”Ÿæˆä»£ç ä¿®å¤ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ã€‚Breakpointç³»ç»Ÿåœ°æ§åˆ¶ä»»åŠ¡éš¾åº¦ï¼Œæ¶µç›–å±€éƒ¨æ¨ç†å’Œç³»ç»Ÿçº§æ¨ç†ä¸¤ä¸ªç»´åº¦ã€‚åœ¨900å¤šä¸ªç”Ÿæˆä»»åŠ¡çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿæ‰©å±•åˆ°ä»»æ„éš¾åº¦ï¼Œæœ€å…ˆè¿›æ¨¡å‹çš„æˆåŠŸç‡ä»æœ€ç®€å•ä»»åŠ¡çš„55%ä¸‹é™åˆ°æœ€å›°éš¾ä»»åŠ¡çš„0%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•æ–¹æ³•åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç³»ç»Ÿçº§æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯éš¾ä»¥ç”Ÿæˆå¤šæ ·åŒ–å’ŒåŠ¨æ€å˜åŒ–çš„ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBreakpointé€šè¿‡å¯¹çœŸå®è½¯ä»¶åº“ä¸­çš„å‡½æ•°è¿›è¡Œå¯¹æŠ—æ€§ç ´åï¼Œè‡ªåŠ¨ç”Ÿæˆä»£ç ä¿®å¤ä»»åŠ¡ï¼Œä»è€Œå®ç°ä»»åŠ¡çš„å¤šæ ·æ€§å’Œéš¾åº¦æ§åˆ¶ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—è¯„ä¼°è¿‡ç¨‹ä¸å†ä¾èµ–äººå·¥ç­–åˆ’ï¼Œé™ä½äº†æˆæœ¬å¹¶æé«˜äº†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBreakpointçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡ç”Ÿæˆæ¨¡å—ã€éš¾åº¦æ§åˆ¶æ¨¡å—å’Œè¯„ä¼°æ¨¡å—ã€‚ä»»åŠ¡ç”Ÿæˆæ¨¡å—è´Ÿè´£å¯¹æŠ—æ€§ç ´åå‡½æ•°ï¼Œéš¾åº¦æ§åˆ¶æ¨¡å—é€šè¿‡å±€éƒ¨æ¨ç†å’Œç³»ç»Ÿçº§æ¨ç†çš„æŒ‡æ ‡æ¥è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œè¯„ä¼°æ¨¡å—åˆ™ç”¨äºæµ‹è¯•æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šBreakpointçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è‡ªåŠ¨åŒ–ç”Ÿæˆä»»åŠ¡çš„èƒ½åŠ›ï¼Œä»¥åŠç³»ç»Ÿåœ°æ§åˆ¶ä»»åŠ¡éš¾åº¦çš„æœºåˆ¶ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„äººå·¥ç­–åˆ’æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æé«˜äº†åŸºå‡†æµ‹è¯•çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå±€éƒ¨æ¨ç†é€šè¿‡ä»£ç å¤æ‚åº¦æŒ‡æ ‡ï¼ˆå¦‚åœˆå¤æ‚åº¦ï¼‰è¿›è¡Œé‡åŒ–ï¼Œç³»ç»Ÿçº§æ¨ç†åˆ™é€šè¿‡è°ƒç”¨å›¾ä¸­å¿ƒæ€§å’ŒåŒæ—¶ç ´åçš„ç›¸äº’ä¾èµ–å‡½æ•°æ•°é‡æ¥è¡¡é‡ã€‚è¿™æ ·çš„è®¾è®¡ç¡®ä¿äº†ä»»åŠ¡çš„å¤šæ ·æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨900å¤šä¸ªç”Ÿæˆä»»åŠ¡çš„å®éªŒä¸­ï¼ŒBreakpointå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ‰©å±•èƒ½åŠ›ï¼Œæœ€å…ˆè¿›æ¨¡å‹åœ¨æœ€ç®€å•ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡è¾¾åˆ°55%ï¼Œè€Œåœ¨æœ€å›°éš¾ä»»åŠ¡ä¸Šåˆ™é™è‡³0%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒBreakpointèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°å’ŒæŒ‘æˆ˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿçº§æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Breakpointçš„ç ”ç©¶æˆæœåœ¨è½¯ä»¶å·¥ç¨‹ã€ç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›ä¸€ç§é«˜æ•ˆçš„è¯„ä¼°æœºåˆ¶ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘æ›´å¼ºå¤§çš„ä»£ç ç”Ÿæˆå’Œä¿®å¤å·¥å…·ï¼Œæå‡è‡ªåŠ¨åŒ–ç¼–ç¨‹çš„èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹çš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ï¼Œå¦‚è‡ªåŠ¨åŒ–æµ‹è¯•å’Œç³»ç»Ÿä¼˜åŒ–ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Benchmarks for large language models (LLMs) have predominantly assessed short-horizon, localized reasoning. Existing long-horizon suites (e.g. SWE-bench) rely on manually curated issues, so expanding or tuning difficulty demands expensive human effort and evaluations quickly saturate. However, many real-world tasks, such as software engineering or scientific research, require agents to rapidly comprehend and manipulate novel, complex structures dynamically; evaluating these capabilities requires the ability to construct large and varied sets of problems for agents to solve. We introduce Breakpoint, a benchmarking methodology that automatically generates code-repair tasks by adversarially corrupting functions within real-world software repositories. Breakpoint systematically controls task difficulty along two clear dimensions: local reasoning (characterized by code complexity metrics such as cyclomatic complexity) and system-level reasoning (characterized by call-graph centrality and the number of simultaneously corrupted interdependent functions). In experiments across more than 900 generated tasks we demonstrate that our methodology can scale to arbitrary difficulty, with state-of-the-art models' success rates ranging from 55% on the easiest tasks down to 0% on the hardest.

