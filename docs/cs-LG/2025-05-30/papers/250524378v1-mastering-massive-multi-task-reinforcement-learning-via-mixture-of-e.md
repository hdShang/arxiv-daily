---
layout: default
title: Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer
---

# Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24378" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24378v1</a>
  <a href="https://arxiv.org/pdf/2505.24378.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24378v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24378v1', 'Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: ICML 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºM3DTæ¡†æ¶ä»¥è§£å†³å¤§è§„æ¨¡å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ` `æ··åˆä¸“å®¶` `å†³ç­–Transformer` `æ¨¡å‹å¯æ‰©å±•æ€§` `ä¸‰é˜¶æ®µè®­ç»ƒæœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä»»åŠ¡æ•°é‡æ‰©å±•ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œç®€å•å¢åŠ å‚æ•°æ— æ³•æœ‰æ•ˆæå‡æ€§èƒ½ã€‚
2. æå‡ºM3DTæ¡†æ¶ï¼Œé€šè¿‡æ··åˆä¸“å®¶æœºåˆ¶å¢å¼ºæ¨¡å‹å‚æ•°çš„å¯æ‰©å±•æ€§ï¼Œä¼˜åŒ–å†³ç­–Transformerä»¥åº”å¯¹ä»»åŠ¡è´Ÿè½½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM3DTåœ¨å›ºå®šä»»åŠ¡æ•°é‡ä¸‹æ€§èƒ½æŒç»­æå‡ï¼Œå¹¶æˆåŠŸæ‰©å±•åˆ°160ä¸ªä»»åŠ¡ï¼Œè¡¨ç°ä¼˜è¶Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è¿‘æœŸç¦»çº¿å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼ˆMTRLï¼‰åœ¨Transformeræ¶æ„çš„åº”ç”¨ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†å¤§è§„æ¨¡ä»»åŠ¡çš„æ‰©å±•ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡é¦–å…ˆé‡æ–°å®¡è§†ä»»åŠ¡æ•°é‡å¯¹ç°æœ‰MTRLæ–¹æ³•çš„å½±å“ï¼Œå¹¶æŒ‡å‡ºç®€å•æ‰©å±•å‚æ•°ä¸è¶³ä»¥åº”å¯¹ä»»åŠ¡æ•°é‡å¢åŠ å¸¦æ¥çš„æ€§èƒ½ä¸‹é™ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¡†æ¶M3DTï¼Œé€šè¿‡å¢å¼ºå†³ç­–Transformerï¼ˆDTï¼‰éª¨å¹²ç½‘ç»œï¼Œå‡å°‘å‚æ•°å­é›†ä¸Šçš„ä»»åŠ¡è´Ÿè½½ï¼Œå¹¶å¼•å…¥ä¸‰é˜¶æ®µè®­ç»ƒæœºåˆ¶ä»¥å®ç°é«˜æ•ˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM3DTåœ¨ä»»åŠ¡æ•°é‡å¢åŠ æ—¶æ€§èƒ½æŒç»­æå‡ï¼ŒæˆåŠŸæ‰©å±•è‡³160ä¸ªä»»åŠ¡å¹¶è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹å¤§é‡ä»»åŠ¡æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä»»åŠ¡æ•°é‡å¢åŠ æ—¶ï¼Œç®€å•æ‰©å±•æ¨¡å‹å‚æ•°å¹¶æœªèƒ½æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œå¯¼è‡´ä»»åŠ¡å¤„ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„M3DTæ¡†æ¶é€šè¿‡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æœºåˆ¶æ¥å¢å¼ºæ¨¡å‹çš„å‚æ•°å¯æ‰©å±•æ€§ï¼Œæ—¨åœ¨å‡è½»æ¯ä¸ªä¸“å®¶çš„ä»»åŠ¡è´Ÿè½½ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚é€šè¿‡ä¼˜åŒ–å†³ç­–Transformerï¼ˆDTï¼‰éª¨å¹²ç½‘ç»œï¼ŒM3DTèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¤šä»»åŠ¡åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM3DTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªä¸“å®¶æ¨¡å—ï¼Œæ¯ä¸ªä¸“å®¶è´Ÿè´£å¤„ç†ç‰¹å®šçš„ä»»åŠ¡å­é›†ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸‰é˜¶æ®µè®­ç»ƒæœºåˆ¶ï¼Œåˆ†åˆ«ä¸ºé¢„è®­ç»ƒã€å¾®è°ƒå’Œæœ€ç»ˆä¼˜åŒ–ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒé˜¶æ®µçš„é«˜æ•ˆå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šM3DTçš„ä¸»è¦åˆ›æ–°åœ¨äºç»“åˆäº†æ··åˆä¸“å®¶æœºåˆ¶ä¸å†³ç­–Transformerï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤§è§„æ¨¡ä»»åŠ¡ä¸Šçš„å¤„ç†èƒ½åŠ›ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ ¹æœ¬åŒºåˆ«åœ¨äºé€šè¿‡ä¸“å®¶åˆ†å·¥æ¥ä¼˜åŒ–ä»»åŠ¡å¤„ç†ï¼Œè€Œéå•ä¸€æ¨¡å‹çš„å‚æ•°æ‰©å±•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒM3DTé‡‡ç”¨äº†åŠ¨æ€ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼Œæ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€æ¿€æ´»ä¸åŒçš„ä¸“å®¶ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†ä»»åŠ¡é—´çš„ç›¸äº’å½±å“ï¼Œä»¥ä¼˜åŒ–æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒM3DTåœ¨ä»»åŠ¡æ•°é‡å¢åŠ æ—¶æ€§èƒ½æŒç»­æå‡ï¼ŒæˆåŠŸæ‰©å±•è‡³160ä¸ªä»»åŠ¡ï¼Œè¾ƒåŸºçº¿æ–¹æ³•åœ¨ç›¸åŒä»»åŠ¡æ•°é‡ä¸‹æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„ä»»åŠ¡å¯æ‰©å±•æ€§å’Œå¤„ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ™ºèƒ½æ¨èç³»ç»Ÿå’Œè‡ªåŠ¨é©¾é©¶ç­‰å¤šä»»åŠ¡åœºæ™¯ã€‚M3DTæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤§è§„æ¨¡ä»»åŠ¡ï¼Œæå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.

