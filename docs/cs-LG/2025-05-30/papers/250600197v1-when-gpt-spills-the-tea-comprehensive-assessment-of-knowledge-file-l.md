---
layout: default
title: "When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs"
---

# When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00197" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00197v1</a>
  <a href="https://arxiv.org/pdf/2506.00197.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00197v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00197v1', 'When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyue Shen, Yun Shen, Michael Backes, Yang Zhang

**åˆ†ç±»**: cs.CR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…¨é¢è¯„ä¼°GPTçŸ¥è¯†æ–‡ä»¶æ³„éœ²é£é™©çš„æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†æ–‡ä»¶æ³„éœ²` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®å®‰å…¨` `é£é™©è¯„ä¼°` `ç‰¹æƒæå‡æ¼æ´` `æ•°æ®æµåˆ†æ` `å®‰å…¨ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡æ¶æ„æç¤ºè¯±å¯¼GPTæ³„éœ²çŸ¥è¯†æ–‡ä»¶ï¼Œä½†æœªå…¨é¢è¯„ä¼°å…¶ä»–æ½œåœ¨æ³„éœ²é€”å¾„ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é£é™©è¯„ä¼°å·¥ä½œæµç¨‹ï¼Œç»“åˆæ•°æ®å®‰å…¨æ€åŠ¿ç®¡ç†ï¼ˆDSPMï¼‰ç†å¿µï¼Œç³»ç»Ÿåˆ†æçŸ¥è¯†æ–‡ä»¶æ³„éœ²çš„å¤šç§å‘é‡ã€‚
3. é€šè¿‡å¯¹å¤§é‡æ•°æ®çš„åˆ†æï¼Œå‘ç°äº†äº”ç§æ³„éœ²å‘é‡ï¼Œå¹¶æ­ç¤ºäº†é«˜è¾¾95.95%çš„æˆåŠŸç‡ä¸‹è½½åŸå§‹çŸ¥è¯†æ–‡ä»¶çš„æ¼æ´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†æ–‡ä»¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä»¥æé«˜å“åº”è´¨é‡ã€‚ç„¶è€Œï¼Œå…³äºçŸ¥è¯†æ–‡ä»¶æ½œåœ¨æ³„éœ²çš„æ‹…å¿§æ—¥ç›Šå¢åŠ ã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œæ¶æ„æç¤ºå¯ä»¥è¯±ä½¿GPTæ³„éœ²çŸ¥è¯†æ–‡ä»¶å†…å®¹ï¼Œä½†æ˜¯å¦å­˜åœ¨å…¶ä»–æ³„éœ²é€”å¾„ä»ä¸ç¡®å®šã€‚æœ¬æ–‡é€šè¿‡åˆ†æ651,022ä¸ªGPTå…ƒæ•°æ®ã€11,820ä¸ªæ•°æ®æµå’Œ1,466ä¸ªå“åº”ï¼Œè¯†åˆ«å‡ºäº”ç§æ³„éœ²å‘é‡ï¼Œå¹¶æå‡ºäº†åŸºäºæ•°æ®å®‰å…¨æ€åŠ¿ç®¡ç†ï¼ˆDSPMï¼‰çš„æ–°å·¥ä½œæµç¨‹ï¼Œæä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆä»¥ä¿æŠ¤GPTæ•°æ®ä¾›åº”é“¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çŸ¥è¯†æ–‡ä»¶æ³„éœ²çš„é£é™©ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å…¨é¢è¯†åˆ«æ‰€æœ‰æ½œåœ¨çš„æ³„éœ²é€”å¾„ï¼Œå¯¼è‡´å®‰å…¨éšæ‚£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ•°æ®å®‰å…¨æ€åŠ¿ç®¡ç†ï¼ˆDSPMï¼‰ç†å¿µï¼Œæ„å»ºå…¨é¢çš„é£é™©è¯„ä¼°æ¡†æ¶ï¼Œåˆ†æä¸åŒçš„æ•°æ®æµå’Œæ³„éœ²å‘é‡ï¼Œä»¥è¯†åˆ«å’Œç¼“è§£çŸ¥è¯†æ–‡ä»¶æ³„éœ²çš„é£é™©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†æ•°æ®æµåˆ†æçš„æ–¹æ³•ï¼Œåˆ†æäº†651,022ä¸ªGPTå…ƒæ•°æ®ã€11,820ä¸ªæ•°æ®æµå’Œ1,466ä¸ªå“åº”ï¼Œè¯†åˆ«å‡ºäº”ç§ä¸»è¦çš„æ³„éœ²å‘é‡ï¼ŒåŒ…æ‹¬å…ƒæ•°æ®ã€GPTåˆå§‹åŒ–ã€æ£€ç´¢ã€æ²™ç®±æ‰§è¡Œç¯å¢ƒå’Œæç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºç³»ç»Ÿæ€§åœ°è¯†åˆ«å’Œåˆ†æäº†å¤šç§çŸ¥è¯†æ–‡ä»¶æ³„éœ²å‘é‡ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ¿€æ´»å†…ç½®å·¥å…·Code Interpreterå¯¼è‡´çš„ç‰¹æƒæå‡æ¼æ´ï¼ŒæˆåŠŸç‡é«˜è¾¾95.95%ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®åˆ†æè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤§è§„æ¨¡æ•°æ®é›†å’Œå¤šç»´åº¦åˆ†ææ–¹æ³•ï¼Œç¡®ä¿äº†ç»“æœçš„å¯é æ€§å’Œå…¨é¢æ€§ï¼ŒåŒæ—¶æå‡ºäº†é’ˆå¯¹æ€§å¼ºçš„å®‰å…¨è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯†åˆ«å‡ºçš„äº”ç§æ³„éœ²å‘é‡ä¸­ï¼Œæ¿€æ´»Code Interpreterå·¥å…·å¯¼è‡´çš„ç‰¹æƒæå‡æ¼æ´å…·æœ‰95.95%çš„æˆåŠŸç‡ï¼Œä¸”28.80%çš„æ³„éœ²æ–‡ä»¶ä¸ºå—ç‰ˆæƒä¿æŠ¤çš„ææ–™ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†çŸ¥è¯†æ–‡ä»¶æ³„éœ²çš„ä¸¥é‡æ€§åŠå…¶å¯¹çŸ¥è¯†äº§æƒçš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æå‡ã€çŸ¥è¯†ç®¡ç†ç³»ç»Ÿçš„ä¿æŠ¤ä»¥åŠæ•°æ®éšç§åˆè§„æ€§ã€‚é€šè¿‡è¯†åˆ«å’Œç¼“è§£çŸ¥è¯†æ–‡ä»¶æ³„éœ²é£é™©ï¼Œèƒ½å¤Ÿä¸ºGPTæ„å»ºè€…å’Œå¹³å°æä¾›å•†æä¾›æœ‰æ•ˆçš„å®‰å…¨ç­–ç•¥ï¼Œç¡®ä¿ç”¨æˆ·æ•°æ®çš„å®‰å…¨æ€§å’Œéšç§æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge files have been widely used in large language model (LLM) agents, such as GPTs, to improve response quality. However, concerns about the potential leakage of knowledge files have grown significantly. Existing studies demonstrate that adversarial prompts can induce GPTs to leak knowledge file content. Yet, it remains uncertain whether additional leakage vectors exist, particularly given the complex data flows across clients, servers, and databases in GPTs. In this paper, we present a comprehensive risk assessment of knowledge file leakage, leveraging a novel workflow inspired by Data Security Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820 flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT initialization, retrieval, sandboxed execution environments, and prompts. These vectors enable adversaries to extract sensitive knowledge file data such as titles, content, types, and sizes. Notably, the activation of the built-in tool Code Interpreter leads to a privilege escalation vulnerability, enabling adversaries to directly download original knowledge files with a 95.95% success rate. Further analysis reveals that 28.80% of leaked files are copyrighted, including digital copies from major publishers and internal materials from a listed company. In the end, we provide actionable solutions for GPT builders and platform providers to secure the GPT data supply chain.

