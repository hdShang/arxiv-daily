---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-06
---

# cs.CVï¼ˆ2025-05-06ï¼‰

ğŸ“Š å…± **23** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250503310v2-3d-gaussian-splatting-data-compression-with-mixture-of-priors.html">3D Gaussian Splatting Data Compression with Mixture of Priors</a></td>
  <td>æå‡ºæ··åˆå…ˆéªŒç­–ç•¥ä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘å‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03310v2" data-paper-url="./papers/250503310v2-3d-gaussian-splatting-data-compression-with-mixture-of-priors.html" onclick="toggleFavorite(this, '2505.03310v2', '3D Gaussian Splatting Data Compression with Mixture of Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250503422v1-liftfeat-3d-geometry-aware-local-feature-matching.html">LiftFeat: 3D Geometry-Aware Local Feature Matching</a></td>
  <td>æå‡ºLiftFeatä»¥è§£å†³3Då‡ ä½•æ„ŸçŸ¥ä¸‹çš„å±€éƒ¨ç‰¹å¾åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">feature matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03422v1" data-paper-url="./papers/250503422v1-liftfeat-3d-geometry-aware-local-feature-matching.html" onclick="toggleFavorite(this, '2505.03422v1', 'LiftFeat: 3D Geometry-Aware Local Feature Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250506280v1-show-or-tell-a-benchmark-to-evaluate-visual-and-textual-prompts-in-s.html">Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation</a></td>
  <td>æå‡ºShow or TellåŸºå‡†ä»¥è¯„ä¼°è¯­ä¹‰åˆ†å‰²ä¸­çš„è§†è§‰ä¸æ–‡æœ¬æç¤º</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06280v1" data-paper-url="./papers/250506280v1-show-or-tell-a-benchmark-to-evaluate-visual-and-textual-prompts-in-s.html" onclick="toggleFavorite(this, '2505.06280v1', 'Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250503334v2-os-w2s-an-automatic-labeling-engine-for-language-guided-open-set-aer.html">OS-W2S: An Automatic Labeling Engine for Language-Guided Open-Set Aerial Object Detection</a></td>
  <td>æå‡ºOS-W2Så¼•æ“ä»¥è§£å†³è¯­è¨€å¼•å¯¼çš„å¼€æ”¾é›†ç©ºä¸­ç‰©ä½“æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">visual grounding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03334v2" data-paper-url="./papers/250503334v2-os-w2s-an-automatic-labeling-engine-for-language-guided-open-set-aer.html" onclick="toggleFavorite(this, '2505.03334v2', 'OS-W2S: An Automatic Labeling Engine for Language-Guided Open-Set Aerial Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250503116v1-timetracker-event-based-continuous-point-tracking-for-video-frame-in.html">TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion</a></td>
  <td>æå‡ºTimeTrackerä»¥è§£å†³éçº¿æ€§è¿åŠ¨ä¸‹çš„è§†é¢‘å¸§æ’å€¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03116v1" data-paper-url="./papers/250503116v1-timetracker-event-based-continuous-point-tracking-for-video-frame-in.html" onclick="toggleFavorite(this, '2505.03116v1', 'TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250503554v1-read-my-ears-horse-ear-movement-detection-for-equine-affective-state.html">Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment</a></td>
  <td>æå‡ºè€³æœµè¿åŠ¨æ£€æµ‹æ–¹æ³•ä»¥è§£å†³é©¬åŒ¹æƒ…æ„ŸçŠ¶æ€è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03554v1" data-paper-url="./papers/250503554v1-read-my-ears-horse-ear-movement-detection-for-equine-affective-state.html" onclick="toggleFavorite(this, '2505.03554v1', 'Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250503362v1-3d-surface-reconstruction-with-enhanced-high-frequency-details.html">3D Surface Reconstruction with Enhanced High-Frequency Details</a></td>
  <td>æå‡ºFreNeuSä»¥è§£å†³3Dé‡å»ºè¡¨é¢ç»†èŠ‚ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">implicit representation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03362v1" data-paper-url="./papers/250503362v1-3d-surface-reconstruction-with-enhanced-high-frequency-details.html" onclick="toggleFavorite(this, '2505.03362v1', '3D Surface Reconstruction with Enhanced High-Frequency Details')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250503318v3-unified-multimodal-chain-of-thought-reward-model-through-reinforceme.html">Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning</a></td>
  <td>æå‡ºç»Ÿä¸€å¤šæ¨¡æ€é“¾å¼æ€ç»´å¥–åŠ±æ¨¡å‹ä»¥æå‡è§†è§‰ä»»åŠ¡çš„å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03318v3" data-paper-url="./papers/250503318v3-unified-multimodal-chain-of-thought-reward-model-through-reinforceme.html" onclick="toggleFavorite(this, '2505.03318v3', 'Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250503621v1-physllm-harnessing-large-language-models-for-cross-modal-remote-phys.html">PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing</a></td>
  <td>æå‡ºPhysLLMä»¥è§£å†³è¿œç¨‹ç”Ÿç†ä¿¡å·æµ‹é‡ä¸­çš„å™ªå£°æ•æ„Ÿé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03621v1" data-paper-url="./papers/250503621v1-physllm-harnessing-large-language-models-for-cross-modal-remote-phys.html" onclick="toggleFavorite(this, '2505.03621v1', 'PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250503494v1-upmad-net-a-brain-tumor-segmentation-network-with-uncertainty-guidan.html">UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion</a></td>
  <td>æå‡ºUPMAD-Netä»¥è§£å†³è„‘è‚¿ç˜¤åˆ†å‰²ä¸­çš„ä¸ç¡®å®šæ€§ä¸å¤šæ¨¡æ€ç‰¹å¾èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03494v1" data-paper-url="./papers/250503494v1-upmad-net-a-brain-tumor-segmentation-network-with-uncertainty-guidan.html" onclick="toggleFavorite(this, '2505.03494v1', 'UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250503299v1-towards-efficient-benchmarking-of-foundation-models-in-remote-sensin.html">Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach</a></td>
  <td>æå‡ºèƒ½åŠ›ç¼–ç æ–¹æ³•ä»¥é«˜æ•ˆè¯„ä¼°é¥æ„ŸåŸºç¡€æ¨¡å‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03299v1" data-paper-url="./papers/250503299v1-towards-efficient-benchmarking-of-foundation-models-in-remote-sensin.html" onclick="toggleFavorite(this, '2505.03299v1', 'Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250503991v3-deep-learning-for-sports-video-event-detection-tasks-datasets-method.html">Deep Learning for Sports Video Event Detection: Tasks, Datasets, Methods, and Challenges</a></td>
  <td>æå‡ºæ·±åº¦å­¦ä¹ æ¡†æ¶ä»¥è§£å†³ä½“è‚²è§†é¢‘äº‹ä»¶æ£€æµ‹çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03991v3" data-paper-url="./papers/250503991v3-deep-learning-for-sports-video-event-detection-tasks-datasets-method.html" onclick="toggleFavorite(this, '2505.03991v3', 'Deep Learning for Sports Video Event Detection: Tasks, Datasets, Methods, and Challenges')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250503735v2-multi-agent-system-for-comprehensive-soccer-understanding.html">Multi-Agent System for Comprehensive Soccer Understanding</a></td>
  <td>æå‡ºç»¼åˆæ¡†æ¶ä»¥è§£å†³è¶³çƒç†è§£çš„å±€é™æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03735v2" data-paper-url="./papers/250503735v2-multi-agent-system-for-comprehensive-soccer-understanding.html" onclick="toggleFavorite(this, '2505.03735v2', 'Multi-Agent System for Comprehensive Soccer Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250503319v2-sd-vsum-a-method-and-dataset-for-script-driven-video-summarization.html">SD-VSum: A Method and Dataset for Script-Driven Video Summarization</a></td>
  <td>æå‡ºSD-VSumä»¥è§£å†³è„šæœ¬é©±åŠ¨çš„è§†é¢‘æ‘˜è¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03319v2" data-paper-url="./papers/250503319v2-sd-vsum-a-method-and-dataset-for-script-driven-video-summarization.html" onclick="toggleFavorite(this, '2505.03319v2', 'SD-VSum: A Method and Dataset for Script-Driven Video Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250503123v1-stg-spatiotemporal-graph-neural-network-with-fusion-and-spatiotempor.html">STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis</a></td>
  <td>æå‡ºSTGæ¡†æ¶ä»¥è§£å†³ç»“ç›´è‚ ç™Œè‚è½¬ç§»é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03123v1" data-paper-url="./papers/250503123v1-stg-spatiotemporal-graph-neural-network-with-fusion-and-spatiotempor.html" onclick="toggleFavorite(this, '2505.03123v1', 'STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250503703v1-fill-the-gap-quantifying-and-reducing-the-modality-gap-in-image-text.html">Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning</a></td>
  <td>æå‡ºæ–°æ–¹æ³•é‡åŒ–ä¸å‡å°‘å›¾åƒ-æ–‡æœ¬è¡¨ç¤ºå­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03703v1" data-paper-url="./papers/250503703v1-fill-the-gap-quantifying-and-reducing-the-modality-gap-in-image-text.html" onclick="toggleFavorite(this, '2505.03703v1', 'Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250515822v1-mambastyle-efficient-stylegan-inversion-for-real-image-editing-with-.html">MambaStyle: Efficient StyleGAN Inversion for Real Image Editing with State-Space Models</a></td>
  <td>æå‡ºMambaStyleä»¥è§£å†³GANåæ¼”ä¸ç¼–è¾‘æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.15822v1" data-paper-url="./papers/250515822v1-mambastyle-efficient-stylegan-inversion-for-real-image-editing-with-.html" onclick="toggleFavorite(this, '2505.15822v1', 'MambaStyle: Efficient StyleGAN Inversion for Real Image Editing with State-Space Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250503562v1-real-time-person-image-synthesis-using-a-flow-matching-model.html">Real-Time Person Image Synthesis Using a Flow Matching Model</a></td>
  <td>æå‡ºåŸºäºæµåŒ¹é…æ¨¡å‹çš„å®æ—¶äººç‰©å›¾åƒåˆæˆæ–¹æ³•ä»¥è§£å†³ç”Ÿæˆé€Ÿåº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03562v1" data-paper-url="./papers/250503562v1-real-time-person-image-synthesis-using-a-flow-matching-model.html" onclick="toggleFavorite(this, '2505.03562v1', 'Real-Time Person Image Synthesis Using a Flow Matching Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250503176v2-seq-jepa-autoregressive-predictive-learning-of-invariant-equivariant.html">seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models</a></td>
  <td>æå‡ºseq-JEPAä»¥è§£å†³è‡ªç›‘ç£å­¦ä¹ ä¸­çš„è¡¨ç¤ºçµæ´»æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03176v2" data-paper-url="./papers/250503176v2-seq-jepa-autoregressive-predictive-learning-of-invariant-equivariant.html" onclick="toggleFavorite(this, '2505.03176v2', 'seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250503597v1-fixed-length-dense-fingerprint-representation.html">Fixed-Length Dense Fingerprint Representation</a></td>
  <td>æå‡ºå›ºå®šé•¿åº¦å¯†é›†æŒ‡çº¹è¡¨ç¤ºä»¥è§£å†³æŒ‡çº¹åŒ¹é…æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03597v1" data-paper-url="./papers/250503597v1-fixed-length-dense-fingerprint-representation.html" onclick="toggleFavorite(this, '2505.03597v1', 'Fixed-Length Dense Fingerprint Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250503470v4-blending-3d-geometry-and-machine-learning-for-multi-view-stereopsis.html">Blending 3D Geometry and Machine Learning for Multi-View Stereopsis</a></td>
  <td>æå‡ºGC MVSNet++ä»¥è§£å†³å¤šè§†å›¾ç«‹ä½“è§†è§‰ä¸­çš„å‡ ä½•ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03470v4" data-paper-url="./papers/250503470v4-blending-3d-geometry-and-machine-learning-for-multi-view-stereopsis.html" onclick="toggleFavorite(this, '2505.03470v4', 'Blending 3D Geometry and Machine Learning for Multi-View Stereopsis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250503154v2-stablemotion-training-motion-cleanup-models-with-unpaired-corrupted-.html">StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</a></td>
  <td>æå‡ºStableMotionä»¥è§£å†³è¿åŠ¨æ•æ‰æ•°æ®æ¸…ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03154v2" data-paper-url="./papers/250503154v2-stablemotion-training-motion-cleanup-models-with-unpaired-corrupted-.html" onclick="toggleFavorite(this, '2505.03154v2', 'StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250503351v2-guava-generalizable-upper-body-3d-gaussian-avatar.html">GUAVA: Generalizable Upper Body 3D Gaussian Avatar</a></td>
  <td>æå‡ºGUAVAæ¡†æ¶ä»¥è§£å†³å•å›¾åƒé‡å»º3Däººç±»å¤´åƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SMPL-X</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03351v2" data-paper-url="./papers/250503351v2-guava-generalizable-upper-body-3d-gaussian-avatar.html" onclick="toggleFavorite(this, '2505.03351v2', 'GUAVA: Generalizable Upper Body 3D Gaussian Avatar')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)