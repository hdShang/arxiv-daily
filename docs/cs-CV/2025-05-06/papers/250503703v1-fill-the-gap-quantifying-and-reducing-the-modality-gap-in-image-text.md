---
layout: default
title: "Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning"
---

# Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03703" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03703v1</a>
  <a href="https://arxiv.org/pdf/2505.03703.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03703v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03703v1', 'Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: FranÃ§ois Role, SÃ©bastien Meyer, Victor Amblard

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–°æ–¹æ³•é‡åŒ–ä¸å‡å°‘å›¾åƒ-æ–‡æœ¬è¡¨ç¤ºå­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æ¨¡æ€å·®è·` `å¤šæ¨¡æ€æ£€ç´¢` `æœ€ä¼˜ä¼ è¾“` `è°±æ–¹æ³•` `åµŒå…¥å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹å­˜åœ¨æ¨¡æ€å·®è·ï¼Œå¯¼è‡´æ–‡æœ¬å’Œå›¾åƒçš„åµŒå…¥åœ¨è¡¨ç¤ºç©ºé—´ä¸­æ˜æ˜¾åˆ†ç¦»ï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºåŸºäºè°±å’Œæœ€ä¼˜ä¼ è¾“çš„æ–¹æ³•æ¥é‡åŒ–å’Œå‡å°‘æ¨¡æ€å·®è·ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€ä»»åŠ¡çš„æ•ˆæœã€‚
3. é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨å¤šæ¨¡æ€æ£€ç´¢å’Œåˆ†ç±»ç­‰ä»»åŠ¡ä¸­çš„æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿå°†æ–‡æœ¬å’Œå›¾åƒåµŒå…¥åˆ°å…±äº«çš„è¡¨ç¤ºç©ºé—´ä¸­ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜è¿™äº›æ¨¡å‹å­˜åœ¨æ¨¡æ€å·®è·ç°è±¡ï¼Œå³ä¸åŒæ¨¡æ€çš„åµŒå…¥åœ¨è¡¨ç¤ºç©ºé—´ä¸­å­˜åœ¨æ˜æ˜¾çš„åˆ†ç¦»ã€‚è¿™ç§ä¸å¯¹é½å¯¹å¤šæ¨¡æ€æ£€ç´¢ã€å¤šæ¨¡æ€èšç±»å’Œé›¶-shot åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡äº§ç”Ÿä¸åˆ©å½±å“ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†æ–°çš„åº¦é‡æ ‡å‡†å’Œæœ‰æ•ˆæŠ€æœ¯ï¼ˆåŸºäºè°±å’Œæœ€ä¼˜ä¼ è¾“çš„æ–¹æ³•ï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚é€šè¿‡åœ¨å¤šä¸ªå›¾åƒ-æ–‡æœ¬æ•°æ®é›†å’Œæ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§åŠå…¶å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„ç§¯æå½±å“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„æ¨¡æ€å·®è·é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆé‡åŒ–å’Œå‡å°‘ä¸åŒæ¨¡æ€ä¹‹é—´çš„åµŒå…¥ä¸å¯¹é½ï¼Œå¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†å’ŒæŠ€æœ¯ï¼Œåˆ©ç”¨è°±æ–¹æ³•å’Œæœ€ä¼˜ä¼ è¾“ç†è®ºæ¥é‡åŒ–æ¨¡æ€å·®è·ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–åµŒå…¥ç©ºé—´ä¸­çš„å¯¹é½æ¥å‡å°‘è¿™ä¸€å·®è·ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡æ€é—´çš„å…³ç³»ï¼Œå¹¶æœ‰æ•ˆæ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯æ¨¡æ€å·®è·çš„é‡åŒ–æ¨¡å—ï¼Œé€šè¿‡è°±åˆ†æå’Œæœ€ä¼˜ä¼ è¾“æ–¹æ³•è®¡ç®—æ¨¡æ€é—´çš„è·ç¦»ï¼›å…¶æ¬¡æ˜¯ä¼˜åŒ–æ¨¡å—ï¼ŒåŸºäºè®¡ç®—ç»“æœè°ƒæ•´åµŒå…¥ç©ºé—´ï¼Œå¢å¼ºæ¨¡æ€é—´çš„å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•æ¥é‡åŒ–æ¨¡æ€å·®è·ï¼Œå¹¶é€šè¿‡æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µå‡å°‘è¿™ä¸€å·®è·ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºç²¾ç¡®å’Œå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®ºæ–‡è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡æ€å¯¹é½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å‚æ•°è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°15%ä»¥ä¸Šï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€æ£€ç´¢ã€å›¾åƒ-æ–‡æœ¬åŒ¹é…ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿç­‰ã€‚é€šè¿‡å‡å°‘æ¨¡æ€å·®è·ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›åº”ç”¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…è½åœ°å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) allow to embed texts and images in a shared representation space. However, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. While this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc. no generic and practical methods have so far been proposed to assess it precisely and even reduce it. We therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. Extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. Our code is available at the URL provided in the paper's abstract.

