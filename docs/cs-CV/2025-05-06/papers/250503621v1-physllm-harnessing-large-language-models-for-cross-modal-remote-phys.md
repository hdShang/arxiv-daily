---
layout: default
title: PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing
---

# PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03621" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.03621v1</a>
  <a href="https://arxiv.org/pdf/2505.03621.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03621v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03621v1', 'PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yiping Xie, Bo Zhao, Mingtong Dai, Jian-Ping Zhou, Yue Sun, Tao Tan, Weicheng Xie, Linlin Shen, Zitong Yu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PhysLLM‰ª•Ëß£ÂÜ≥ËøúÁ®ãÁîüÁêÜ‰ø°Âè∑ÊµãÈáè‰∏≠ÁöÑÂô™Â£∞ÊïèÊÑüÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËøúÁ®ãÁîüÁêÜÁõëÊµã` `ÂÖâÁîµÂÆπÁßØÊèèËÆ∞Ê≥ï` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `‰ø°Âè∑Â§ÑÁêÜ` `ÁîüÁêÜÁªüËÆ°` `ÁéØÂ¢É‰∏ä‰∏ãÊñá` `Âä®ÊÄÅÈÄÇÂ∫î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËøúÁ®ãÁîüÁêÜ‰ø°Âè∑ÊµãÈáèÊñπÊ≥ïÂú®ÂÖâÁÖßÂèòÂåñÂíåËøêÂä®‰º™ÂΩ±‰∏ãË°®Áé∞‰∏ç‰Ω≥ÔºåÂØºËá¥ÊµãÈáèÁªìÊûú‰∏çÁ®≥ÂÆö„ÄÇ
2. Êú¨ÊñáÊèêÂá∫PhysLLMÔºåÈÄöËøáÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏érPPGÁªÑ‰ª∂ÁªìÂêàÔºåÂà©Áî®ÊñáÊú¨ÂéüÂûãÂºïÂØºÁ≠ñÁï•ÂÆûÁé∞Ë∑®Ê®°ÊÄÅÂØπÈΩê„ÄÇ
3. Âú®Âõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåPhysLLMÂú®ÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøúÁ®ãÂÖâÁîµÂÆπÁßØÊèèËÆ∞Ê≥ïÔºàrPPGÔºâËÉΩÂ§üÂÆûÁé∞ÈùûÊé•Ëß¶ÂºèÁîüÁêÜÊµãÈáèÔºå‰ΩÜÂØπÂÖâÁÖßÂèòÂåñ„ÄÅËøêÂä®‰º™ÂΩ±ÂíåÊó∂Èó¥Âª∫Ê®°ËÉΩÂäõÊúâÈôê„ÄÇÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊçïÊçâÈïøË∑ùÁ¶ª‰æùËµñÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁî±‰∫éÂÖ∂ÊñáÊú¨‰∏≠ÂøÉËÆæËÆ°ÔºåÈöæ‰ª•Â§ÑÁêÜËøûÁª≠‰∏îÂØπÂô™Â£∞ÊïèÊÑüÁöÑrPPG‰ø°Âè∑„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫PhysLLMÔºå‰∏Ä‰∏™ÂçèÂêå‰ºòÂåñÊ°ÜÊû∂ÔºåÂ∞ÜLLMs‰∏éÁâπÂÆöÈ¢ÜÂüüÁöÑrPPGÁªÑ‰ª∂ÁªìÂêà„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊèêÂá∫‰∫ÜÊñáÊú¨ÂéüÂûãÂºïÂØºÔºàTPGÔºâÁ≠ñÁï•ÔºåÈÄöËøáÂ∞ÜË°ÄÊµÅÂä®ÂäõÂ≠¶ÁâπÂæÅÊäïÂΩ±Âà∞LLMÂèØËß£ÈáäÁöÑËØ≠‰πâÁ©∫Èó¥‰∏≠ÔºåÂª∫Á´ãË∑®Ê®°ÊÄÅÂØπÈΩê„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåÂüüÈùôÊÄÅÔºàDDSÔºâÁÆóÊ≥ïÔºåÈÄöËøáËá™ÈÄÇÂ∫îÊó∂È¢ëÂüüÁâπÂæÅÈáçÂä†ÊùÉËß£ÂÜ≥‰ø°Âè∑‰∏çÁ®≥ÂÆöÊÄß„ÄÇÊúÄÂêéÔºåÈÄöËøáÁîüÁêÜÁªüËÆ°„ÄÅÁéØÂ¢É‰∏ä‰∏ãÊñáÂõûÁ≠îÂíå‰ªªÂä°ÊèèËø∞ÔºåÁ≥ªÁªüÊÄßÂú∞Ê≥®ÂÖ•ÁîüÁêÜÂÖàÈ™åÔºåÂà©Áî®Ë∑®Ê®°ÊÄÅÂ≠¶‰π†Êï¥ÂêàËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂä®ÊÄÅÈÄÇÂ∫îÂÖâÁÖßÂèòÂåñÂíåÂèóËØïËÄÖËøêÂä®Á≠âÊåëÊàòÂú∫ÊôØ„ÄÇÁªèËøáÂõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜÁöÑËØÑ‰º∞ÔºåPhysLLMÂú®ÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥ÔºåÂ±ïÁ§∫‰∫ÜÂú®ÂÖâÁÖßÂèòÂåñÂíåËøêÂä®Âú∫ÊôØ‰∏ãÁöÑ‰ºòË∂äÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËøúÁ®ãÂÖâÁîµÂÆπÁßØÊèèËÆ∞Ê≥ïÔºàrPPGÔºâÂú®ÂÖâÁÖßÂèòÂåñÂíåËøêÂä®‰º™ÂΩ±‰∏ãÁöÑ‰ø°Âè∑‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜËøûÁª≠‰∏îÂØπÂô™Â£∞ÊïèÊÑüÁöÑÁîüÁêÜ‰ø°Âè∑Êó∂ÔºåÂ∏∏Â∏∏ÂèóÂà∞ÈôêÂà∂ÔºåÂØºËá¥ÊµãÈáèÁªìÊûúÁöÑÂáÜÁ°ÆÊÄß‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏éÈ¢ÜÂüüÁâπÂÆöÁöÑrPPGÁªÑ‰ª∂ÁªìÂêàÔºåÈÄöËøáÊñáÊú¨ÂéüÂûãÂºïÂØºÔºàTPGÔºâÁ≠ñÁï•ÂÆûÁé∞Ë∑®Ê®°ÊÄÅÂØπÈΩêÔºå‰ªéËÄåÊúâÊïàÂú∞Â∞ÜÁîüÁêÜ‰ø°Âè∑‰∏éËØ≠Ë®ÄÁ¨¶Âè∑‰πãÈó¥ÁöÑË°®Á§∫Â∑ÆË∑ùÁº©Â∞è„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÔºåÈÄöËøáTPGÁ≠ñÁï•Â∞ÜË°ÄÊµÅÂä®ÂäõÂ≠¶ÁâπÂæÅÊò†Â∞ÑÂà∞LLMÂèØËß£ÈáäÁöÑËØ≠‰πâÁ©∫Èó¥ÔºõÂÖ∂Ê¨°ÔºåÈááÁî®ÂèåÂüüÈùôÊÄÅÔºàDDSÔºâÁÆóÊ≥ïËøõË°å‰ø°Âè∑ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂ§ÑÁêÜÔºõÊúÄÂêéÔºåÈÄöËøá‰ªªÂä°ÁâπÂÆöÁöÑÁ∫øÁ¥¢Ê≥®ÂÖ•ÁîüÁêÜÂÖàÈ™å‰ø°ÊÅØÔºåÊï¥ÂêàËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éTPGÁ≠ñÁï•ÂíåDDSÁÆóÊ≥ïÁöÑÁªìÂêàÔºåÂâçËÄÖÂÆûÁé∞‰∫ÜË∑®Ê®°ÊÄÅÂØπÈΩêÔºåÂêéËÄÖÂàôÈÄöËøáËá™ÈÄÇÂ∫îÁâπÂæÅÈáçÂä†ÊùÉËß£ÂÜ≥‰∫Ü‰ø°Âè∑ÁöÑ‰∏çÁ®≥ÂÆöÊÄß„ÄÇËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåPhysLLMËÉΩÂ§üÂä®ÊÄÅÈÄÇÂ∫îÂ§çÊùÇÁéØÂ¢É‰∏ãÁöÑÁîüÁêÜ‰ø°Âè∑ÂèòÂåñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®‰∫ÜËá™ÈÄÇÂ∫îÊó∂È¢ëÂüüÁâπÂæÅÈáçÂä†ÊùÉÁ≠ñÁï•ÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°ËÄÉËôë‰∫ÜÁîüÁêÜÁªüËÆ°ÂíåÁéØÂ¢É‰∏ä‰∏ãÊñáÁöÑÂΩ±ÂìçÔºåÁΩëÁªúÁªìÊûÑÂàôÁªìÂêà‰∫ÜLLMs‰∏érPPGÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºåÁ°Æ‰øù‰∫Ü‰ø°ÊÅØÁöÑÊúâÊïàËûçÂêà„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Âõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåPhysLLMÂú®ÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥ÔºåÂ∞§ÂÖ∂Âú®ÂÖâÁÖßÂèòÂåñÂíåËøêÂä®Âú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõÊòæËëóÊèêÂçáÔºåÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™ËØ¶Ëø∞Ôºå‰ΩÜÊï¥‰ΩìË°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂåªÁñóÁõëÊµã„ÄÅËøêÂä®ÂÅ•Â∫∑ÁÆ°ÁêÜÂíåËøúÁ®ãÁîüÁêÜÊï∞ÊçÆÈááÈõÜÁ≠â„ÄÇÈÄöËøáÊèêÈ´òrPPG‰ø°Âè∑ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåPhysLLMËÉΩÂ§üÂú®Â§öÁßçÂ§çÊùÇÁéØÂ¢É‰∏ãÂÆûÁé∞ÂèØÈù†ÁöÑÁîüÁêÜÁõëÊµãÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Remote photoplethysmography (rPPG) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rPPG signals due to their text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative optimization framework that synergizes LLMs with domain-specific rPPG components. Specifically, the Text Prototype Guidance (TPG) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into LLM-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. Finally, rPPG task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. Evaluation on four benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios.

