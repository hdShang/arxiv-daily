---
layout: default
title: Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning
---

# Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03318" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.03318v3</a>
  <a href="https://arxiv.org/pdf/2505.03318.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03318v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03318v3', 'Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-06 (Êõ¥Êñ∞: 2025-10-29)

**Â§áÊ≥®**: [NeurIPS2025] Project Page: https://codegoat24.github.io/UnifiedReward/think

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Áªü‰∏ÄÂ§öÊ®°ÊÄÅÈìæÂºèÊÄùÁª¥Â•ñÂä±Ê®°Âûã‰ª•ÊèêÂçáËßÜËßâ‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ•ñÂä±Ê®°Âûã` `ÈìæÂºèÊÄùÁª¥` `Âº∫ÂåñÂ≠¶‰π†` `Êé®ÁêÜËÉΩÂäõ` `ËßÜËßâÁêÜËß£` `ÁîüÊàê‰ªªÂä°` `Êé¢Á¥¢È©±Âä®` `Á≠ñÁï•‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÂú®Êé®ÁêÜÊ∑±Â∫¶ÂíåÂáÜÁ°ÆÊÄß‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÂØºËá¥Â•ñÂä±‰ø°Âè∑‰∏çÂèØÈù†„ÄÇ
2. Êú¨ÊñáÊèêÂá∫UnifiedReward-ThinkÔºåÈÄöËøáÂºïÂÖ•ÈìæÂºèÊÄùÁª¥Â¢ûÂº∫Â•ñÂä±Êé®ÁêÜËøáÁ®ãÁöÑÂèØÈù†ÊÄßÔºåÂπ∂ÊèêÂçáÁõ¥Êé•ÂìçÂ∫îÁöÑÂáÜÁ°ÆÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUnifiedReward-ThinkÂú®Â§öÁßçËßÜËßâÂ•ñÂä±‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÂú®Â∞ÜËßÜËßâÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÊñπÈù¢Â±ïÁé∞Âá∫ÊòæËëóÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊ®°ÂûãÈÄöÂ∏∏‰ªÖÊèê‰æõÁõ¥Êé•ÂìçÂ∫îÊàñËøõË°åÊµÖÂ±ÇÊé®ÁêÜÔºåÂØºËá¥Â•ñÂä±‰ø°Âè∑‰∏çÂáÜÁ°Æ„ÄÇÊú¨ÊñáÊèêÂá∫UnifiedReward-ThinkÔºåËøôÊòØÈ¶ñ‰∏™Áªü‰∏ÄÁöÑÂü∫‰∫éÈìæÂºèÊÄùÁª¥ÁöÑÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÔºåËÉΩÂ§üËøõË°åÂ§öÁª¥Â∫¶„ÄÅÈÄêÊ≠•ÁöÑÈïøÈìæÊé®ÁêÜÔºåÈÄÇÁî®‰∫éËßÜËßâÁêÜËß£ÂíåÁîüÊàê‰ªªÂä°„ÄÇÈÄöËøáÊé¢Á¥¢È©±Âä®ÁöÑÂº∫ÂåñÂæÆË∞ÉÊñπÊ≥ïÔºåÊ®°ÂûãËÉΩÂ§üÊøÄÂèëÂÖ∂ÊΩúÂú®ÁöÑÂ§çÊùÇÊé®ÁêÜËÉΩÂäõÔºåÊúÄÁªàÂú®ÂêÑÁßçËßÜËßâÂ•ñÂä±‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫‰ºòË∂äÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÂú®Êé®ÁêÜÊ∑±Â∫¶ÂíåÂáÜÁ°ÆÊÄß‰∏äÁöÑ‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄÂè™ËÉΩÊèê‰æõË°®Â±ÇÁöÑÂìçÂ∫îÔºåÂØºËá¥Â•ñÂä±‰ø°Âè∑‰∏çÂ§üÂèØÈù†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫UnifiedReward-ThinkÔºåÈÄöËøáÂºïÂÖ•ÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊù•Â¢ûÂº∫Â•ñÂä±Êé®ÁêÜÁöÑÂèØÈù†ÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂêåÊó∂ÊèêÂçáÊ®°ÂûãÁöÑÁõ¥Êé•ÂìçÂ∫îÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖà‰ΩøÁî®Â∞ëÈáèÂõæÂÉèÁîüÊàêÂÅèÂ•ΩÊï∞ÊçÆËøõË°åÂÜ∑ÂêØÂä®ÔºåÂ≠¶‰π†ÈìæÂºèÊÄùÁª¥ÁöÑÊ†ºÂºèÂíåÁªìÊûÑÔºõÂÖ∂Ê¨°ÔºåÂà©Áî®Ê®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜÂáÜÂ§áÂ§ßËßÑÊ®°ÁöÑÂ§öÊ®°ÊÄÅÂÅèÂ•ΩÊï∞ÊçÆÔºåÊøÄÂèëÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ãÔºõÊúÄÂêéÔºåÈÄöËøáÂØπÈîôËØØÈ¢ÑÊµãÊ†∑Êú¨ËøõË°åÂº∫ÂåñÂæÆË∞ÉÔºå‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÈ¶ñÊ¨°Â∞ÜÈìæÂºèÊÄùÁª¥ÂºïÂÖ•Â§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üËøõË°åÂ§çÊùÇÁöÑÊé®ÁêÜËøáÁ®ãÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜÁöÑÊ∑±Â∫¶ÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®‰∫ÜÊé¢Á¥¢È©±Âä®ÁöÑÂº∫ÂåñÂæÆË∞ÉÁ≠ñÁï•ÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÁªìÂêà‰∫ÜÊãíÁªùÈááÊ†∑ÂíåÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâÔºåÁ°Æ‰øùÊ®°ÂûãËÉΩÂ§üÊúâÊïàÊé¢Á¥¢Â§öÊ†∑ÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåUnifiedReward-ThinkÂú®Â§ö‰∏™ËßÜËßâÂ•ñÂä±‰ªªÂä°‰∏≠Áõ∏ËæÉ‰∫éÂü∫Á∫øÊ®°ÂûãÊèêÂçá‰∫ÜÊé®ÁêÜÂáÜÁ°ÆÊÄßÔºåÂÖ∑‰ΩìÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÈ™åËØÅ‰∫ÜÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜÂú∫ÊôØ‰∏ãÁöÑÊúâÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂõæÂÉèÁîüÊàêÁ≠âÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáËßÜËßâ‰ªªÂä°ÁöÑÂ§ÑÁêÜËÉΩÂäõÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåÈöèÁùÄÊ®°ÂûãÁöÑËøõ‰∏ÄÊ≠•‰ºòÂåñÔºåÂèØËÉΩÂú®Êõ¥ÂπøÊ≥õÁöÑÂ§öÊ®°ÊÄÅ‰∫§‰∫íÂú∫ÊôØ‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.

