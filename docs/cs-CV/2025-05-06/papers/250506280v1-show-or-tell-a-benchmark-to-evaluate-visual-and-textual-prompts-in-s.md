---
layout: default
title: Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation
---

# Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06280" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06280v1</a>
  <a href="https://arxiv.org/pdf/2505.06280.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06280v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06280v1', 'Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gabriele Rosi, Fabio Cermelli

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

**å¤‡æ³¨**: Accepted to PixFoundation workshop at CVPR2025. Code: https://github.com/FocoosAI/ShowOrTell

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºShow or TellåŸºå‡†ä»¥è¯„ä¼°è¯­ä¹‰åˆ†å‰²ä¸­çš„è§†è§‰ä¸æ–‡æœ¬æç¤º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­ä¹‰åˆ†å‰²` `è§†è§‰æç¤º` `æ–‡æœ¬æç¤º` `å¼€æ”¾è¯æ±‡` `åŸºå‡†è¯„ä¼°` `å¤šæ¨¡æ€å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è¯­ä¹‰åˆ†å‰²åŸºå‡†å¾€å¾€å­¤ç«‹è¯„ä¼°æ–‡æœ¬ä¸è§†è§‰æç¤ºï¼Œç¼ºä¹åœ¨ç›¸åŒæ¡ä»¶ä¸‹çš„ç›´æ¥æ¯”è¾ƒï¼Œé™åˆ¶äº†ç ”ç©¶çš„æ·±å…¥ã€‚
2. è®ºæ–‡æå‡ºShow or TellåŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°è§†è§‰ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„è¡¨ç°ï¼Œæ¶µç›–å¤šç§æ•°æ®é›†å’Œé¢†åŸŸã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼€æ”¾è¯æ±‡æ–¹æ³•åœ¨ç®€å•æ¦‚å¿µä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚é¢†åŸŸä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œè€Œè§†è§‰æç¤ºæ–¹æ³•åˆ™è¡¨ç°å‡ºè¾ƒé«˜çš„ç»“æœå˜å¼‚æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æç¤ºå·¥ç¨‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„ç³»ç»Ÿæ¢ç´¢ä»ç„¶æœ‰é™ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä¸­ï¼Œæ–‡æœ¬æç¤ºå’Œè§†è§‰æç¤ºå„å…·ä¼˜åŠ¿ï¼šæ–‡æœ¬æç¤ºé€šè¿‡å¼€æ”¾è¯æ±‡æ–¹æ³•å…è®¸å¯¹ä»»æ„ç±»åˆ«è¿›è¡Œåˆ†å‰²ï¼Œè€Œè§†è§‰å‚è€ƒæç¤ºåˆ™æä¾›ç›´è§‚çš„å‚è€ƒç¤ºä¾‹ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†åœ¨ä¸åŒæ¡ä»¶ä¸‹å¯¹è¿™äº›æ¨¡æ€çš„è¯„ä¼°ç¼ºä¹ç›´æ¥æ¯”è¾ƒã€‚æˆ‘ä»¬æå‡ºäº†Show or Tellï¼ˆSoTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°è·¨è¶Š7ä¸ªä¸åŒé¢†åŸŸçš„14ä¸ªæ•°æ®é›†ä¸­çš„è§†è§‰å’Œæ–‡æœ¬æç¤ºã€‚æˆ‘ä»¬è¯„ä¼°äº†5ç§å¼€æ”¾è¯æ±‡æ–¹æ³•å’Œ4ç§è§†è§‰å‚è€ƒæç¤ºæ–¹æ³•ï¼Œå¹¶é€šè¿‡åŸºäºç½®ä¿¡åº¦çš„æ©è†œåˆå¹¶ç­–ç•¥è°ƒæ•´åè€…ä»¥å¤„ç†å¤šç±»åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æ”¾è¯æ±‡æ–¹æ³•åœ¨å¸¸è§æ¦‚å¿µä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å·¥å…·ç­‰å¤æ‚é¢†åŸŸä¸­è¡¨ç°ä¸ä½³ï¼Œè€Œè§†è§‰å‚è€ƒæç¤ºæ–¹æ³•åˆ™åœ¨è¾“å…¥æç¤ºçš„ä¸åŒæƒ…å†µä¸‹è¡¨ç°å‡ºè¾ƒé«˜çš„å˜å¼‚æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯­ä¹‰åˆ†å‰²åŸºå‡†åœ¨è¯„ä¼°è§†è§‰ä¸æ–‡æœ¬æç¤ºæ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯ç¼ºä¹åœ¨ç›¸åŒæ¡ä»¶ä¸‹çš„ç›´æ¥æ¯”è¾ƒï¼Œå¯¼è‡´å¯¹ä¸åŒæç¤ºæ¨¡æ€çš„ç†è§£ä¸å¤Ÿå…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºShow or TellåŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°è¯„ä¼°è§†è§‰ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„è¡¨ç°ï¼Œå¸®åŠ©ç ”ç©¶è€…æ›´å¥½åœ°ç†è§£å„è‡ªçš„ä¼˜åŠ¿ä¸åŠ£åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€æç¤ºæ–¹æ³•è¯„ä¼°å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æˆ‘ä»¬é€‰æ‹©äº†14ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–7ä¸ªé¢†åŸŸï¼Œè¯„ä¼°5ç§å¼€æ”¾è¯æ±‡æ–¹æ³•å’Œ4ç§è§†è§‰å‚è€ƒæç¤ºæ–¹æ³•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†ï¼Œèƒ½å¤Ÿåœ¨ç›¸åŒæ¡ä»¶ä¸‹æ¯”è¾ƒä¸åŒçš„æç¤ºæ¨¡æ€ï¼Œå¹¶é€šè¿‡ç½®ä¿¡åº¦æ©è†œåˆå¹¶ç­–ç•¥ä½¿è§†è§‰æç¤ºæ–¹æ³•é€‚åº”å¤šç±»åˆ†å‰²ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å¯¹å¼€æ”¾è¯æ±‡æ–¹æ³•å’Œè§†è§‰å‚è€ƒæç¤ºæ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æ”¾è¯æ±‡æ–¹æ³•åœ¨å¸¸è§æ¦‚å¿µçš„åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡è¾¾åˆ°XX%ï¼Œè€Œåœ¨å¤æ‚é¢†åŸŸå¦‚å·¥å…·çš„åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå‡†ç¡®ç‡ä¸‹é™è‡³YY%ã€‚è§†è§‰å‚è€ƒæç¤ºæ–¹æ³•çš„å¹³å‡ç»“æœè‰¯å¥½ï¼Œä½†åœ¨ä¸åŒè¾“å…¥æç¤ºä¸‹è¡¨ç°å‡ºé«˜è¾¾ZZ%çš„å˜å¼‚æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶å¯¹è¾“å…¥è´¨é‡çš„æ•æ„Ÿæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æå’Œæ™ºèƒ½ç›‘æ§ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©æå‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚é€šè¿‡å¯¹ä¸åŒæç¤ºæ¨¡æ€çš„æ·±å…¥ç†è§£ï¼Œæœªæ¥çš„è§†è§‰åŸºç¡€æ¨¡å‹å¯ä»¥æ›´å¥½åœ°é€‚åº”å¤æ‚çš„å®é™…åœºæ™¯ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Prompt engineering has shown remarkable success with large language models, yet its systematic exploration in computer vision remains limited. In semantic segmentation, both textual and visual prompts offer distinct advantages: textual prompts through open-vocabulary methods allow segmentation of arbitrary categories, while visual reference prompts provide intuitive reference examples. However, existing benchmarks evaluate these modalities in isolation, without direct comparison under identical conditions. We present Show or Tell (SoT), a novel benchmark specifically designed to evaluate both visual and textual prompts for semantic segmentation across 14 datasets spanning 7 diverse domains (common scenes, urban, food, waste, parts, tools, and land-cover). We evaluate 5 open-vocabulary methods and 4 visual reference prompt approaches, adapting the latter to handle multi-class segmentation through a confidence-based mask merging strategy. Our extensive experiments reveal that open-vocabulary methods excel with common concepts easily described by text but struggle with complex domains like tools, while visual reference prompt methods achieve good average results but exhibit high variability depending on the input prompt. Through comprehensive quantitative and qualitative analysis, we identify the strengths and weaknesses of both prompting modalities, providing valuable insights to guide future research in vision foundation models for segmentation tasks.

