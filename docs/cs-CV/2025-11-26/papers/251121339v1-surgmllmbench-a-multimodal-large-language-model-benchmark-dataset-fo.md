---
layout: default
title: SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding
---

# SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.21339" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.21339v1</a>
  <a href="https://arxiv.org/pdf/2511.21339.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.21339v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.21339v1', 'SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tae-Min Choi, Tae Kyeong Jeong, Garam Kim, Jaemin Lee, Yeongyoon Koh, In Cheul Choi, Jae-Ho Chung, Jong Woong Park, Juyoun Park

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

**å¤‡æ³¨**: 10 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SurgMLLMBenchï¼šç”¨äºæ‰‹æœ¯åœºæ™¯ç†è§£çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æ•°æ®é›†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `æ‰‹æœ¯åœºæ™¯ç†è§£` `è§†è§‰é—®ç­”` `åƒç´ çº§åˆ†å‰²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰‹æœ¯æ•°æ®é›†ä¸»è¦é‡‡ç”¨VQAæ ¼å¼ï¼Œåˆ†ç±»ä½“ç³»ä¸ç»Ÿä¸€ï¼Œç¼ºä¹åƒç´ çº§åˆ†å‰²æ”¯æŒï¼Œé™åˆ¶äº†å¤šæ¨¡æ€LLMçš„è¯„ä¼°å’Œåº”ç”¨ã€‚
2. SurgMLLMBenché€šè¿‡ç»Ÿä¸€çš„åˆ†ç±»ä½“ç³»æ•´åˆäº†å¤šç§æ‰‹æœ¯æ–¹å¼çš„åƒç´ çº§å™¨æ¢°åˆ†å‰²æ©ç å’Œç»“æ„åŒ–VQAæ³¨é‡Šï¼Œæ”¯æŒæ›´å…¨é¢çš„è¯„ä¼°å’Œäº¤äº’ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåœ¨SurgMLLMBenchä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ä¸åŒæ‰‹æœ¯é¢†åŸŸè¡¨ç°ä¸€è‡´ï¼Œå¹¶èƒ½æœ‰æ•ˆæ³›åŒ–åˆ°æ–°çš„æ•°æ®é›†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å‡¸æ˜¾äº†å…¶åœ¨åŒ»ç–—å’Œå¤–ç§‘åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰‹æœ¯æ•°æ®é›†ä¸»è¦é‡‡ç”¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ ¼å¼ï¼Œå…·æœ‰å¼‚æ„çš„åˆ†ç±»ä½“ç³»ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹åƒç´ çº§åˆ†å‰²çš„æ”¯æŒï¼Œé™åˆ¶äº†ä¸€è‡´çš„è¯„ä¼°å’Œé€‚ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†SurgMLLMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œä¸“é—¨ç”¨äºå¼€å‘å’Œè¯„ä¼°ç”¨äºæ‰‹æœ¯åœºæ™¯ç†è§£çš„äº¤äº’å¼å¤šæ¨¡æ€LLMï¼ŒåŒ…æ‹¬æ–°æ”¶é›†çš„æ˜¾å¾®å¤–ç§‘äººå·¥è¡€ç®¡å»åˆï¼ˆMAVISï¼‰æ•°æ®é›†ã€‚å®ƒåœ¨ç»Ÿä¸€çš„åˆ†ç±»ä½“ç³»ä¸‹æ•´åˆäº†è…¹è…”é•œã€æœºå™¨äººè¾…åŠ©å’Œæ˜¾å¾®å¤–ç§‘é¢†åŸŸçš„åƒç´ çº§å™¨æ¢°åˆ†å‰²æ©ç å’Œç»“æ„åŒ–VQAæ³¨é‡Šï¼Œä»è€Œèƒ½å¤Ÿè¿›è¡Œè¶…è¶Šä¼ ç»ŸVQAä»»åŠ¡çš„å…¨é¢è¯„ä¼°ï¼Œå¹¶å®ç°æ›´ä¸°å¯Œçš„è§†è§‰å¯¹è¯äº¤äº’ã€‚å¹¿æ³›çš„åŸºçº¿å®éªŒè¡¨æ˜ï¼Œåœ¨SurgMLLMBenchä¸Šè®­ç»ƒçš„å•ä¸ªæ¨¡å‹å¯ä»¥åœ¨ä¸åŒé¢†åŸŸå®ç°ä¸€è‡´çš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„æ•°æ®é›†ã€‚SurgMLLMBenchå°†å…¬å¼€å‘å¸ƒï¼Œä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„èµ„æºï¼Œä»¥æ¨è¿›å¤šæ¨¡æ€æ‰‹æœ¯AIç ”ç©¶ï¼Œæ”¯æŒå¯é‡å¤çš„è¯„ä¼°å’Œäº¤äº’å¼æ‰‹æœ¯æ¨ç†æ¨¡å‹çš„å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ‰‹æœ¯æ•°æ®é›†ä¸»è¦é‡‡ç”¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ ¼å¼ï¼Œä¸”ä¸åŒæ•°æ®é›†çš„æ ‡æ³¨ä½“ç³»ä¸ç»Ÿä¸€ï¼Œç¼ºä¹åƒç´ çº§åˆ«çš„åˆ†å‰²ä¿¡æ¯ï¼Œè¿™ä½¿å¾—è®­ç»ƒå’Œè¯„ä¼°ç”¨äºæ‰‹æœ¯åœºæ™¯ç†è§£çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å˜å¾—å›°éš¾ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥è¿›è¡Œä¸€è‡´æ€§è¯„ä¼°ï¼Œå¹¶ä¸”é™åˆ¶äº†æ¨¡å‹åœ¨ä¸åŒæ‰‹æœ¯åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†SurgMLLMBenchï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šç§æ‰‹æœ¯æ–¹å¼ï¼ˆè…¹è…”é•œã€æœºå™¨äººè¾…åŠ©ã€æ˜¾å¾®å¤–ç§‘ï¼‰çš„å›¾åƒå’Œè§†é¢‘æ•°æ®ï¼Œå¹¶æä¾›ç»Ÿä¸€çš„æ ‡æ³¨ä½“ç³»ï¼ŒåŒ…æ‹¬åƒç´ çº§åˆ«çš„å™¨æ¢°åˆ†å‰²æ©ç å’Œç»“æ„åŒ–çš„VQAæ³¨é‡Šã€‚é€šè¿‡ç»Ÿä¸€çš„æ•°æ®æ ¼å¼å’Œæ ‡æ³¨ï¼Œå¯ä»¥æ›´æ–¹ä¾¿åœ°è®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€LLMåœ¨æ‰‹æœ¯åœºæ™¯ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSurgMLLMBenchæ•°æ®é›†åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šè…¹è…”é•œæ‰‹æœ¯æ•°æ®ã€æœºå™¨äººè¾…åŠ©æ‰‹æœ¯æ•°æ®å’Œæ˜¾å¾®å¤–ç§‘æ‰‹æœ¯æ•°æ®ï¼ˆMAVISï¼‰ã€‚å¯¹äºæ¯ç§æ‰‹æœ¯æ•°æ®ï¼Œéƒ½æä¾›äº†åƒç´ çº§åˆ«çš„å™¨æ¢°åˆ†å‰²æ©ç å’Œç»“æ„åŒ–çš„VQAæ³¨é‡Šã€‚æ•°æ®é›†çš„æ„å»ºæµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ ‡æ³¨å’ŒéªŒè¯ä¸‰ä¸ªé˜¶æ®µã€‚æ ‡æ³¨è¿‡ç¨‹é‡‡ç”¨ç»Ÿä¸€çš„åˆ†ç±»ä½“ç³»ï¼Œç¡®ä¿ä¸åŒæ‰‹æœ¯æ–¹å¼çš„æ•°æ®å…·æœ‰å¯æ¯”æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSurgMLLMBenchçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ï¼Œå®ƒæ•´åˆäº†å¤šç§æ‰‹æœ¯æ–¹å¼çš„æ•°æ®ï¼Œå¹¶æä¾›äº†åƒç´ çº§åˆ«çš„å™¨æ¢°åˆ†å‰²æ©ç å’Œç»“æ„åŒ–çš„VQAæ³¨é‡Šã€‚è¿™ç§ç»Ÿä¸€çš„æ•°æ®æ ¼å¼å’Œæ ‡æ³¨ä½“ç³»ä½¿å¾—å¯ä»¥æ›´æ–¹ä¾¿åœ°è®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€LLMåœ¨æ‰‹æœ¯åœºæ™¯ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ä¿ƒè¿›äº†ä¸åŒæ¨¡å‹ä¹‹é—´çš„æ¯”è¾ƒå’Œåˆ†æã€‚æ­¤å¤–ï¼Œæ–°æ”¶é›†çš„MAVISæ•°æ®é›†ä¹Ÿä¸ºæ˜¾å¾®å¤–ç§‘æ‰‹æœ¯åœºæ™¯ç†è§£æä¾›äº†æ–°çš„æ•°æ®èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šSurgMLLMBenchæ•°æ®é›†çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç»Ÿä¸€çš„åˆ†ç±»ä½“ç³»ï¼Œç”¨äºæ ‡æ³¨ä¸åŒæ‰‹æœ¯æ–¹å¼çš„æ•°æ®ï¼›2) åƒç´ çº§åˆ«çš„å™¨æ¢°åˆ†å‰²æ©ç ï¼Œç”¨äºæä¾›æ›´ç²¾ç»†çš„è§†è§‰ä¿¡æ¯ï¼›3) ç»“æ„åŒ–çš„VQAæ³¨é‡Šï¼Œç”¨äºè¯„ä¼°æ¨¡å‹å¯¹æ‰‹æœ¯åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºä½¿ç”¨çš„å¤šæ¨¡æ€LLMæ¨¡å‹ï¼Œè®ºæ–‡ä¸»è¦å…³æ³¨æ•°æ®é›†çš„æ„å»ºå’Œè¯„ä¼°ï¼Œè€Œéç‰¹å®šæ¨¡å‹çš„ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨SurgMLLMBenchä¸Šè®­ç»ƒçš„å•ä¸ªæ¨¡å‹å¯ä»¥åœ¨ä¸åŒæ‰‹æœ¯é¢†åŸŸå®ç°ä¸€è‡´çš„æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„æ•°æ®é›†ã€‚è¿™è¡¨æ˜SurgMLLMBenchæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„åŸºå‡†æ•°æ®é›†ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€LLMåœ¨æ‰‹æœ¯åœºæ™¯ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ™ºèƒ½æ‰‹æœ¯è¾…åŠ©ç³»ç»Ÿï¼Œä¾‹å¦‚ï¼Œé€šè¿‡ç†è§£æ‰‹æœ¯åœºæ™¯ï¼Œä¸ºåŒ»ç”Ÿæä¾›å®æ—¶çš„å™¨æ¢°å®šä½ã€æ“ä½œå»ºè®®å’Œé£é™©é¢„è­¦ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜å¯ä»¥ç”¨äºè®­ç»ƒæœºå™¨äººæ‰‹æœ¯ç³»ç»Ÿï¼Œæé«˜æ‰‹æœ¯çš„ç²¾å‡†æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨æ‰‹æœ¯AIçš„æ™ºèƒ½åŒ–å‘å±•ï¼Œæå‡åŒ»ç–—æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.

