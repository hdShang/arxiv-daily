---
layout: default
title: "BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data"
---

# BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.21194" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.21194v1</a>
  <a href="https://arxiv.org/pdf/2511.21194.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.21194v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.21194v1', 'BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Selene Cerna, Sara Si-Moussi, Wilfried Thuiller, Hadrien Hendrikx, Vincent Miele

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BotaCLIPï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ å®ç°åœ°çƒè§‚æµ‹æ•°æ®çš„æ¤ç‰©å­¦æ„ŸçŸ¥è¡¨å¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `åœ°çƒè§‚æµ‹` `æ¤ç‰©å­¦` `é¢†åŸŸçŸ¥è¯†` `è¡¨å¾å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å°†é¢†åŸŸçŸ¥è¯†æœ‰æ•ˆèå…¥é¢„è®­ç»ƒçš„åœ°çƒè§‚æµ‹æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶åœ¨ç”Ÿæ€å­¦ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚
2. BotaCLIPé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†é«˜åˆ†è¾¨ç‡èˆªæ‹å›¾åƒä¸æ¤ç‰©ç¾¤è½è°ƒæŸ¥æ•°æ®å¯¹é½ï¼Œä»è€Œä½¿æ¨¡å‹å…·å¤‡æ¤ç‰©å­¦æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBotaCLIPåœ¨æ¤ç‰©å­˜åœ¨é¢„æµ‹ã€è´è¶å‡ºç°å»ºæ¨¡å’ŒåœŸå£¤è¥å…»ç¾¤ä¸°åº¦ä¼°è®¡ç­‰ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹å·²å±•ç°å‡ºå­¦ä¹ è·¨å›¾åƒã€æ–‡æœ¬å’ŒéŸ³é¢‘ç­‰å¤šç§æ¨¡æ€çš„ä¸°å¯Œã€å¯è¿ç§»è¡¨å¾çš„å“è¶Šèƒ½åŠ›ã€‚åœ¨ç°ä»£æœºå™¨å­¦ä¹ æµç¨‹ä¸­ï¼Œè¿™äº›è¡¨å¾é€šå¸¸å–ä»£åŸå§‹æ•°æ®ï¼Œä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„ä¸»è¦è¾“å…¥ã€‚æœ¬æ–‡è‡´åŠ›äºè§£å†³å¦‚ä½•è°ƒæ•´é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä»¥æ³¨å…¥é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„æŒ‘æˆ˜ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒæˆ–äº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†BotaCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§å¤šæ¨¡æ€å¯¹æ¯”æ¡†æ¶ï¼Œé€šè¿‡å°†é«˜åˆ†è¾¨ç‡èˆªç©ºå›¾åƒä¸æ¤ç‰©ç¾¤è½è°ƒæŸ¥æ•°æ®å¯¹é½ï¼Œæ¥è°ƒæ•´é¢„è®­ç»ƒçš„åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹ï¼ˆDOFAï¼‰ã€‚ä¸é€šç”¨åµŒå…¥ä¸åŒï¼ŒBotaCLIPé€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œç¼“è§£ç¾éš¾æ€§é—å¿˜çš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œå°†ç”Ÿæ€ç»“æ„å†…åœ¨åŒ–ã€‚è®­ç»ƒå®Œæˆåï¼Œç”Ÿæˆçš„åµŒå…¥å¯ä½œä¸ºä¸‹æ¸¸é¢„æµ‹å™¨çš„å¯è¿ç§»è¡¨å¾ã€‚å—ç”Ÿç‰©å¤šæ ·æ€§å»ºæ¨¡ä¸­å®é™…åº”ç”¨çš„é©±åŠ¨ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªç”Ÿæ€ä»»åŠ¡ä¸­è¯„ä¼°äº†BotaCLIPè¡¨å¾ï¼šæ¤ç‰©å­˜åœ¨é¢„æµ‹ã€è´è¶å‡ºç°å»ºæ¨¡å’ŒåœŸå£¤è¥å…»ç¾¤ä¸°åº¦ä¼°è®¡ã€‚ç»“æœè¡¨æ˜ï¼Œç›¸å¯¹äºDOFAå’Œç›‘ç£åŸºçº¿ï¼Œæ€§èƒ½å¾—åˆ°äº†æŒç»­æå‡ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œè¿™é¡¹å·¥ä½œå±•ç¤ºäº†é¢†åŸŸæ„ŸçŸ¥çš„æ¨¡å‹è°ƒæ•´å¦‚ä½•å°†ä¸“å®¶çŸ¥è¯†æ³¨å…¥åˆ°æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­ï¼Œä»è€Œå®ç°èŠ‚ä¿­çš„è¡¨å¾å­¦ä¹ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•å°†é¢†åŸŸçŸ¥è¯†ï¼ˆç‰¹åˆ«æ˜¯æ¤ç‰©å­¦çŸ¥è¯†ï¼‰æœ‰æ•ˆåœ°èå…¥åˆ°é¢„è®­ç»ƒçš„åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿æ›´å¥½åœ°åº”ç”¨äºç”Ÿæ€å­¦ç›¸å…³ä»»åŠ¡ã€‚ç°æœ‰çš„é€šç”¨è¡¨å¾å­¦ä¹ æ–¹æ³•ç¼ºä¹å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ç†è§£ï¼Œç›´æ¥åº”ç”¨æ•ˆæœä¸ä½³ã€‚ä»å¤´å¼€å§‹è®­ç»ƒé¢†åŸŸç‰¹å®šæ¨¡å‹æˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥åˆ©ç”¨å·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œå°†é«˜åˆ†è¾¨ç‡èˆªæ‹å›¾åƒä¸æ¤ç‰©ç¾¤è½è°ƒæŸ¥æ•°æ®è¿›è¡Œå¯¹é½ï¼Œä»è€Œä½¿æ¨¡å‹å­¦ä¹ åˆ°å›¾åƒä¸æ¤ç‰©å­¦ä¿¡æ¯ä¹‹é—´çš„å…³è”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿç†è§£å›¾åƒä¸­çš„æ¤ç‰©åˆ†å¸ƒå’Œç”Ÿæ€ç»“æ„ï¼Œä»è€Œå…·å¤‡æ¤ç‰©å­¦æ„ŸçŸ¥èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶èƒ½æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨è¡¨å¾èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBotaCLIPçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å›¾åƒç¼–ç å™¨ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹ï¼ˆDOFAï¼‰ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œæå–èˆªæ‹å›¾åƒçš„ç‰¹å¾ã€‚2) æ–‡æœ¬ç¼–ç å™¨ï¼šä½¿ç”¨ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚å°†æ¤ç‰©ç¾¤è½è°ƒæŸ¥æ•°æ®ç¼–ç ä¸ºå‘é‡è¡¨ç¤ºã€‚3) å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼šä½¿ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œä¿ƒä½¿æ¥è‡ªåŒä¸€åœ°ç‚¹çš„å›¾åƒå’Œæ–‡æœ¬åµŒå…¥åœ¨åµŒå…¥ç©ºé—´ä¸­é è¿‘ï¼Œè€Œæ¥è‡ªä¸åŒåœ°ç‚¹çš„åµŒå…¥åˆ™è¿œç¦»ã€‚4) æ­£åˆ™åŒ–æ¨¡å—ï¼šä¸ºäº†ç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œå¼•å…¥æ­£åˆ™åŒ–é¡¹ï¼Œä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨è¡¨å¾èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šBotaCLIPçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†é¢†åŸŸçŸ¥è¯†æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„åœ°çƒè§‚æµ‹æ¨¡å‹ä¸­ã€‚2) é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°å›¾åƒä¸æ¤ç‰©å­¦ä¿¡æ¯ä¹‹é—´çš„å…³è”ï¼Œä»è€Œå…·å¤‡æ¤ç‰©å­¦æ„ŸçŸ¥èƒ½åŠ›ã€‚3) å¼•å…¥æ­£åˆ™åŒ–ç­–ç•¥ï¼Œç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¿æŒäº†é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨è¡¨å¾èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œä½¿ç”¨äº†InfoNCEæŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°é€šè¿‡softmaxæ“ä½œè®¡ç®—æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦ä¸å…¶ä»–è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦ä¹‹é—´çš„æ¯”ä¾‹ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹ã€‚æ­£åˆ™åŒ–é¡¹é‡‡ç”¨L2æ­£åˆ™åŒ–ï¼Œä½œç”¨äºDOFAæ¨¡å‹çš„å‚æ•°ï¼Œé˜²æ­¢æ¨¡å‹å‚æ•°å‘ç”Ÿè¿‡å¤§çš„å˜åŒ–ã€‚å›¾åƒç¼–ç å™¨é‡‡ç”¨DOFAæ¨¡å‹ï¼Œæ–‡æœ¬ç¼–ç å™¨é‡‡ç”¨çº¿æ€§å±‚ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º1e-4ï¼Œbatch sizeè®¾ç½®ä¸º64ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBotaCLIPåœ¨æ¤ç‰©å­˜åœ¨é¢„æµ‹ã€è´è¶å‡ºç°å»ºæ¨¡å’ŒåœŸå£¤è¥å…»ç¾¤ä¸°åº¦ä¼°è®¡ç­‰ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¤ç‰©å­˜åœ¨é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒBotaCLIPçš„AUCæŒ‡æ ‡æ¯”DOFAæé«˜äº†5%-10%ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹æ¤ç‰©çš„åˆ†å¸ƒæƒ…å†µã€‚æ­¤å¤–ï¼ŒBotaCLIPåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BotaCLIPåœ¨ç”Ÿç‰©å¤šæ ·æ€§å»ºæ¨¡ã€ç”Ÿæ€ç¯å¢ƒç›‘æµ‹ã€ç²¾å‡†å†œä¸šç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºé¢„æµ‹æ¤ç‰©åˆ†å¸ƒã€è¯„ä¼°ç”Ÿæ€ç³»ç»Ÿå¥åº·çŠ¶å†µã€ä¼˜åŒ–å†œä½œç‰©ç§æ¤æ–¹æ¡ˆç­‰ã€‚é€šè¿‡å°†é¢†åŸŸçŸ¥è¯†èå…¥åœ°çƒè§‚æµ‹æ•°æ®åˆ†æä¸­ï¼ŒBotaCLIPèƒ½å¤Ÿä¸ºç”Ÿæ€ç¯å¢ƒä¿æŠ¤å’Œå¯æŒç»­å‘å±•æä¾›æ›´å‡†ç¡®ã€æ›´å¯é çš„ä¿¡æ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models have demonstrated a remarkable ability to learn rich, transferable representations across diverse modalities such as images, text, and audio. In modern machine learning pipelines, these representations often replace raw data as the primary input for downstream tasks. In this paper, we address the challenge of adapting a pre-trained foundation model to inject domain-specific knowledge, without retraining from scratch or incurring significant computational costs. To this end, we introduce BotaCLIP, a lightweight multimodal contrastive framework that adapts a pre-trained Earth Observation foundation model (DOFA) by aligning high-resolution aerial imagery with botanical relevÃ©s. Unlike generic embeddings, BotaCLIP internalizes ecological structure through contrastive learning with a regularization strategy that mitigates catastrophic forgetting. Once trained, the resulting embeddings serve as transferable representations for downstream predictors. Motivated by real-world applications in biodiversity modeling, we evaluated BotaCLIP representations in three ecological tasks: plant presence prediction, butterfly occurrence modeling, and soil trophic group abundance estimation. The results showed consistent improvements over those derived from DOFA and supervised baselines. More broadly, this work illustrates how domain-aware adaptation of foundation models can inject expert knowledge into data-scarce settings, enabling frugal representation learning.

