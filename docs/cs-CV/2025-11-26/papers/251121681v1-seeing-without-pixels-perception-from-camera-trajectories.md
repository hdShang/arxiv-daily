---
layout: default
title: Seeing without Pixels: Perception from Camera Trajectories
---

# Seeing without Pixels: Perception from Camera Trajectories

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.21681" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.21681v1</a>
  <a href="https://arxiv.org/pdf/2511.21681.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.21681v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.21681v1', 'Seeing without Pixels: Perception from Camera Trajectories')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihui Xue, Kristen Grauman, Dima Damen, Andrew Zisserman, Tengda Han

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

**å¤‡æ³¨**: Project website: https://sites.google.com/view/seeing-without-pixels

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ä»…å‡­ç›¸æœºè½¨è¿¹æ„ŸçŸ¥è§†é¢‘å†…å®¹ï¼šæå‡ºCamFormerå¯¹æ¯”å­¦ä¹ æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç›¸æœºè½¨è¿¹` `è§†é¢‘ç†è§£` `å¯¹æ¯”å­¦ä¹ ` `Transformer` `è·¨æ¨¡æ€å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–åƒç´ ä¿¡æ¯è¿›è¡Œè§†é¢‘å†…å®¹ç†è§£ï¼Œå¿½ç•¥äº†ç›¸æœºè¿åŠ¨è½¨è¿¹ä¸­è•´å«çš„ä¿¡æ¯ã€‚
2. æå‡ºCamFormerï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†ç›¸æœºè½¨è¿¹ç¼–ç åˆ°è”åˆåµŒå…¥ç©ºé—´ï¼Œä¸è‡ªç„¶è¯­è¨€å¯¹é½ï¼Œä»è€Œç†è§£è§†é¢‘å†…å®¹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCamFormeråœ¨è·¨æ¨¡æ€å¯¹é½ã€åˆ†ç±»å’Œæ—¶é—´åˆ†æç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸”å¯¹ä¸åŒç›¸æœºå§¿æ€ä¼°è®¡æ–¹æ³•å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†ä¸€ä¸ªçœ‹ä¼¼ä¸å¯èƒ½çš„é—®é¢˜ï¼šä»…é€šè¿‡ç›¸æœºè½¨è¿¹ï¼ˆå³ç›¸æœºåœ¨ç©ºé—´ä¸­ç§»åŠ¨çš„è·¯å¾„ï¼‰è€Œéåƒç´ æ¥æ„ŸçŸ¥è§†é¢‘å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶æ¥è®­ç»ƒCamFormerï¼Œä¸€ä¸ªä¸“é—¨çš„ç¼–ç å™¨ï¼Œç”¨äºå°†ç›¸æœºå§¿æ€è½¨è¿¹æŠ•å½±åˆ°ä¸€ä¸ªè”åˆåµŒå…¥ç©ºé—´ï¼Œå¹¶ä½¿å…¶ä¸è‡ªç„¶è¯­è¨€å¯¹é½ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸è¡¨é¢ä¸Šçš„ç®€å•æ€§ç›¸åï¼Œç›¸æœºè½¨è¿¹æ˜¯æ­ç¤ºè§†é¢‘å†…å®¹çš„ä¸€ä¸ªéå¸¸ä¸°å¯Œçš„ä¿¡æ¯æ¥æºã€‚æ¢å¥è¯è¯´ï¼Œâ€œä½ çš„ç§»åŠ¨æ–¹å¼â€ç¡®å®å¯ä»¥æ­ç¤ºâ€œä½ åœ¨åšä»€ä¹ˆâ€ï¼ˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼‰æˆ–â€œåœ¨è§‚å¯Ÿä»€ä¹ˆâ€ï¼ˆä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç¤ºäº†æˆ‘ä»¬å­¦ä¹ åˆ°çš„CamFormeråµŒå…¥çš„å¤šåŠŸèƒ½æ€§ï¼Œä»è·¨æ¨¡æ€å¯¹é½åˆ°åˆ†ç±»å’Œæ—¶é—´åˆ†æã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è¡¨ç¤ºåœ¨ä¸åŒçš„ç›¸æœºå§¿æ€ä¼°è®¡æ–¹æ³•ä¸­å…·æœ‰é²æ£’æ€§ï¼ŒåŒ…æ‹¬é«˜ä¿çœŸå¤šä¼ æ„Ÿå™¨å’Œæ ‡å‡†RGB-onlyä¼°è®¡å™¨ã€‚æˆ‘ä»¬çš„å‘ç°ç¡®ç«‹äº†ç›¸æœºè½¨è¿¹ä½œä¸ºä¸€ç§è½»é‡çº§ã€é²æ£’ä¸”é€šç”¨çš„æ„ŸçŸ¥è§†é¢‘å†…å®¹çš„æ–¹å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘å†…å®¹ç†è§£æ–¹æ³•ä¸»è¦ä¾èµ–äºåƒç´ ä¿¡æ¯ï¼Œå¿½ç•¥äº†ç›¸æœºè¿åŠ¨è½¨è¿¹æ‰€è•´å«çš„ä¸°å¯Œä¿¡æ¯ã€‚å¦‚ä½•ä»…é€šè¿‡ç›¸æœºè½¨è¿¹æ¥æ„ŸçŸ¥è§†é¢‘å†…å®¹ï¼Œæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨ç›¸æœºè½¨è¿¹ä¿¡æ¯è¿›è¡Œè§†é¢‘ç†è§£ï¼Œç¼ºä¹æœ‰æ•ˆçš„è½¨è¿¹ç¼–ç å’Œè¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç›¸æœºè½¨è¿¹ä½œä¸ºä¸€ç§ç‹¬ç«‹çš„æ¨¡æ€ï¼Œé€šè¿‡å­¦ä¹ ç›¸æœºè½¨è¿¹çš„åµŒå…¥è¡¨ç¤ºæ¥ç†è§£è§†é¢‘å†…å®¹ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†ç›¸æœºè½¨è¿¹çš„åµŒå…¥è¡¨ç¤ºä¸è‡ªç„¶è¯­è¨€æè¿°å¯¹é½ï¼Œä»è€Œå»ºç«‹ç›¸æœºè¿åŠ¨ä¸è§†é¢‘è¯­ä¹‰ä¹‹é—´çš„è”ç³»ã€‚è¿™ç§æ–¹æ³•æ— éœ€ç›´æ¥åˆ†æåƒç´ ä¿¡æ¯ï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶æä¾›äº†ä¸€ç§æ–°çš„è§†é¢‘ç†è§£è§†è§’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCamFormeræ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ç›¸æœºå§¿æ€ä¼°è®¡æ¨¡å—ï¼šç”¨äºä»è§†é¢‘ä¸­æå–ç›¸æœºè½¨è¿¹ï¼›2) è½¨è¿¹ç¼–ç å™¨ï¼šCamFormerï¼Œç”¨äºå°†ç›¸æœºè½¨è¿¹ç¼–ç æˆåµŒå…¥å‘é‡ï¼›3) è‡ªç„¶è¯­è¨€ç¼–ç å™¨ï¼šç”¨äºå°†æ–‡æœ¬æè¿°ç¼–ç æˆåµŒå…¥å‘é‡ï¼›4) å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œå°†ç›¸æœºè½¨è¿¹åµŒå…¥å‘é‡ä¸å¯¹åº”çš„æ–‡æœ¬æè¿°åµŒå…¥å‘é‡æ‹‰è¿‘ï¼Œä¸å…¶ä»–æ–‡æœ¬æè¿°çš„åµŒå…¥å‘é‡æ¨è¿œã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œç»™å®šè§†é¢‘ï¼Œé¦–å…ˆæå–ç›¸æœºè½¨è¿¹ï¼Œç„¶åä½¿ç”¨CamFormerå°†å…¶ç¼–ç æˆåµŒå…¥å‘é‡ï¼Œå†é€šè¿‡å¯¹æ¯”å­¦ä¹ ä¸è‡ªç„¶è¯­è¨€æè¿°å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†CamFormerï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºç¼–ç ç›¸æœºè½¨è¿¹çš„Transformeræ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„è§†é¢‘ç†è§£æ–¹æ³•ä¸åŒï¼ŒCamFormerç›´æ¥å¤„ç†ç›¸æœºè½¨è¿¹æ•°æ®ï¼Œæ— éœ€åƒç´ ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†ç›¸æœºè½¨è¿¹åµŒå…¥ä¸è‡ªç„¶è¯­è¨€æè¿°å¯¹é½ï¼Œå®ç°äº†è·¨æ¨¡æ€çš„è§†é¢‘ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šCamFormeré‡‡ç”¨Transformeræ¶æ„ï¼Œè¾“å…¥æ˜¯ç›¸æœºå§¿æ€åºåˆ—ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œä¾‹å¦‚InfoNCE lossï¼Œæ—¨åœ¨æœ€å¤§åŒ–æ­£æ ·æœ¬ï¼ˆç›¸æœºè½¨è¿¹ä¸å…¶å¯¹åº”çš„æ–‡æœ¬æè¿°ï¼‰ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œæœ€å°åŒ–è´Ÿæ ·æœ¬ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚ç›¸æœºå§¿æ€ä¼°è®¡å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå¤šä¼ æ„Ÿå™¨èåˆçš„æ–¹æ³•å’ŒåŸºäºRGBå›¾åƒçš„æ–¹æ³•ã€‚è®ºæ–‡éªŒè¯äº†CamFormerå¯¹ä¸åŒç›¸æœºå§¿æ€ä¼°è®¡æ–¹æ³•çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCamFormeråœ¨è·¨æ¨¡æ€å¯¹é½ã€è§†é¢‘åˆ†ç±»å’Œæ—¶é—´åˆ†æç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒCamFormerèƒ½å¤Ÿå‡†ç¡®åœ°å°†ç›¸æœºè½¨è¿¹ä¸å¯¹åº”çš„æ–‡æœ¬æè¿°åŒ¹é…ã€‚æ­¤å¤–ï¼ŒCamFormerå¯¹ä¸åŒçš„ç›¸æœºå§¿æ€ä¼°è®¡æ–¹æ³•å…·æœ‰é²æ£’æ€§ï¼Œå³ä½¿ä½¿ç”¨åŸºäºRGBå›¾åƒçš„å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œä¹Ÿèƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç›‘æ§ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥ä»…é€šè¿‡è‡ªèº«çš„è¿åŠ¨è½¨è¿¹æ¥ç†è§£å‘¨å›´ç¯å¢ƒï¼Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥é€šè¿‡åˆ†æå…¶ä»–è½¦è¾†çš„è¿åŠ¨è½¨è¿¹æ¥é¢„æµ‹å…¶è¡Œä¸ºï¼Œè§†é¢‘ç›‘æ§ç³»ç»Ÿå¯ä»¥é€šè¿‡åˆ†ææ‘„åƒå¤´çš„è¿åŠ¨è½¨è¿¹æ¥è¯†åˆ«å¼‚å¸¸äº‹ä»¶ã€‚è¯¥ç ”ç©¶ä¸ºè½»é‡çº§ã€é²æ£’çš„è§†é¢‘ç†è§£æä¾›äº†ä¸€ç§æ–°çš„é€”å¾„ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

