---
layout: default
title: A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation
---

# A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04794" target="_blank" class="toolbar-btn">arXiv: 2510.04794v1</a>
    <a href="https://arxiv.org/pdf/2510.04794.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04794v1" 
            onclick="toggleFavorite(this, '2510.04794v1', 'A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Alon Kaya, Igal Bilik, Inna Stainvas

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÂØπÊØîViT‰∏éCNNÂú®Â∞ëÊ†∑Êú¨ÂàöÊÄßÂèòÊç¢ÂíåÊú¨Ë¥®Áü©Èòµ‰º∞ËÆ°‰∏≠ÁöÑÊÄßËÉΩÔºåÊè≠Á§∫‰∏çÂêåÊï∞ÊçÆËßÑÊ®°‰∏ãÁöÑÊû∂ÊûÑÈÄâÊã©Á≠ñÁï•„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâTransformer` `Âç∑ÁßØÁ•ûÁªèÁΩëÁªú` `Â∞ëÊ†∑Êú¨Â≠¶‰π†` `ÂàöÊÄßÂèòÊç¢‰º∞ËÆ°` `Êú¨Ë¥®Áü©Èòµ‰º∞ËÆ°` `Âá†‰Ωï‰º∞ËÆ°` `ËøÅÁßªÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®È´òÁ≤æÂ∫¶Âá†‰Ωï‰º∞ËÆ°‰ªªÂä°‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÈáèËæÉÂ∞ëÁöÑÊÉÖÂÜµ‰∏ãÔºåÂØπÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæÅÁöÑÂπ≥Ë°°Â≠òÂú®ÊåëÊàò„ÄÇ
2. ÈÄöËøáÂØπÊØîViTÂíåCNNÂú®‰∏çÂêåÊï∞ÊçÆËßÑÊ®°‰∏ãÁöÑÊÄßËÉΩÔºåÊé¢Á¥¢ÈÄÇÁî®‰∫éÂ∞ëÊ†∑Êú¨Âá†‰Ωï‰º∞ËÆ°‰ªªÂä°ÁöÑÈ™®Âπ≤ÁΩëÁªúÊû∂ÊûÑ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåViTÂú®Â§ßÊï∞ÊçÆÂæÆË∞ÉÂíåË∑®ÂüüÊ≥õÂåñÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËÄåCNNÂú®Â∞èÊï∞ÊçÆÂú∫ÊôØ‰∏ãÂÖ∑ÊúâÁ´û‰∫âÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâTransformer (ViT) ÂíåÂ§ßËßÑÊ®°Âç∑ÁßØÁ•ûÁªèÁΩëÁªú (CNN) ÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÁâπÂæÅË°®Á§∫ÈáçÂ°ë‰∫ÜËÆ°ÁÆóÊú∫ËßÜËßâÔºå‰∏∫ÂêÑÁßç‰ªªÂä°ÂÆûÁé∞‰∫ÜÂº∫Â§ßÁöÑËøÅÁßªÂ≠¶‰π†„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨‰Ωú‰∏∫È™®Âπ≤Êû∂ÊûÑÂú®‰ΩéÊï∞ÊçÆÊÉÖÂÜµ‰∏ãÔºåÂ§ÑÁêÜÊ∂âÂèäÂõæÂÉèÂΩ¢ÂèòÁöÑÂá†‰Ωï‰º∞ËÆ°‰ªªÂä°ÁöÑÊïàÁéá‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÊÇ¨ËÄåÊú™ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇÊú¨ÊñáËÄÉËôë‰∫Ü‰∏§‰∏™ËøôÊ†∑ÁöÑ‰ªªÂä°Ôºö1) ‰º∞ËÆ°ÂõæÂÉèÂØπ‰πãÈó¥ÁöÑ2DÂàöÊÄßÂèòÊç¢Ôºõ2) È¢ÑÊµãÁ´ã‰ΩìÂõæÂÉèÂØπÁöÑÊú¨Ë¥®Áü©ÈòµÔºåËøôÊòØËá™‰∏ªÁßªÂä®„ÄÅÊú∫Âô®‰∫∫Âíå3DÂú∫ÊôØÈáçÂª∫Á≠âÂêÑÁßçÂ∫îÁî®‰∏≠ÁöÑÈáçË¶ÅÈóÆÈ¢ò„ÄÇÊú¨ÊñáÁ≥ªÁªüÂú∞ÊØîËæÉ‰∫ÜÂ§ßËßÑÊ®°CNNÔºàResNet„ÄÅEfficientNet„ÄÅCLIP-ResNetÔºâ‰∏éÂü∫‰∫éViTÁöÑÂü∫Á°ÄÊ®°ÂûãÔºàCLIP-ViTÂèò‰ΩìÂíåDINOÔºâÂú®ÂêÑÁßçÊï∞ÊçÆËßÑÊ®°ËÆæÁΩÆÔºàÂåÖÊã¨Â∞ëÊ†∑Êú¨Âú∫ÊôØÔºâ‰∏ãÁöÑÊÄßËÉΩ„ÄÇËøô‰∫õÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÈíàÂØπÂàÜÁ±ªÊàñÂØπÊØîÂ≠¶‰π†ËøõË°å‰∫Ü‰ºòÂåñÔºåÈºìÂä±ÂÆÉ‰ª¨‰∏ªË¶ÅÂÖ≥Ê≥®È´òÂ±ÇËØ≠‰πâ„ÄÇÊâÄËÄÉËôëÁöÑ‰ªªÂä°ÈúÄË¶Å‰∏çÂêåÂú∞Âπ≥Ë°°Â±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæÅÔºåËøôÁªôÁõ¥Êé•ÈááÁî®Ëøô‰∫õÊ®°Âûã‰Ωú‰∏∫È™®Âπ≤Â∏¶Êù•‰∫ÜÊåëÊàò„ÄÇÂÆûËØÅÊØîËæÉÂàÜÊûêË°®ÊòéÔºå‰∏é‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÁ±ª‰ººÔºåViTÂú®Â§ßÂûã‰∏ãÊ∏∏Êï∞ÊçÆÂú∫ÊôØ‰∏≠ÁöÑÂæÆË∞É‰ºò‰∫éCNN„ÄÇÁÑ∂ËÄåÔºåÂú®Â∞èÊï∞ÊçÆÂú∫ÊôØ‰∏≠ÔºåCNNÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÂíåËæÉÂ∞èÁöÑÂÆπÈáèÊîπÂñÑ‰∫ÜÂÆÉ‰ª¨ÁöÑÊÄßËÉΩÔºå‰ΩøÂÖ∂ËÉΩÂ§ü‰∏éViTÁõ∏ÂåπÈÖç„ÄÇÊ≠§Â§ñÔºåViTÂú®Êï∞ÊçÆÂàÜÂ∏ÉÂèëÁîüÂèòÂåñÁöÑË∑®ÂüüËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËøô‰∫õÁªìÊûúÂº∫Ë∞É‰∫Ü‰ªîÁªÜÈÄâÊã©Ê®°ÂûãÊû∂ÊûÑËøõË°åÂæÆË∞ÉÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂Êé®Âä®Êú™Êù•Á†îÁ©∂Ê∑∑ÂêàÊû∂ÊûÑÔºå‰ª•Âπ≥Ë°°Â±ÄÈÉ®ÂíåÂÖ®Â±ÄË°®Á§∫„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®Â∞ëÊ†∑Êú¨ÊÉÖÂÜµ‰∏ãÔºåÂ¶Ç‰ΩïÈÄâÊã©ÂêàÈÄÇÁöÑÈ™®Âπ≤ÁΩëÁªúÔºàViTÊàñCNNÔºâÊù•ËøõË°åÂõæÂÉèÂØπ‰πãÈó¥ÁöÑ2DÂàöÊÄßÂèòÊç¢‰º∞ËÆ°‰ª•ÂèäÁ´ã‰ΩìÂõæÂÉèÂØπÁöÑÊú¨Ë¥®Áü©ÈòµÈ¢ÑÊµãÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁõ¥Êé•Â∫îÁî®È¢ÑËÆ≠ÁªÉÁöÑViTÊàñCNNÔºåÂøΩÁï•‰∫ÜÂú®‰ΩéÊï∞ÊçÆÂú∫ÊôØ‰∏ãÔºåÊ®°ÂûãÊû∂ÊûÑÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÂíåÂÆπÈáèÂØπÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂØπÊØîÂÆûÈ™åÔºåÂàÜÊûêViTÂíåCNNÂú®‰∏çÂêåÊï∞ÊçÆËßÑÊ®°‰∏ãÁöÑÊÄßËÉΩÂ∑ÆÂºÇÔºå‰ªéËÄå‰∏∫Âá†‰Ωï‰º∞ËÆ°‰ªªÂä°ÈÄâÊã©ÂêàÈÄÇÁöÑÈ™®Âπ≤ÁΩëÁªúÊèê‰æõÊåáÂØº„ÄÇËÆ∫ÊñáËÆ§‰∏∫ÔºåViTÂíåCNNÂú®Â±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæÅÁöÑÊèêÂèñËÉΩÂäõ‰∏äÂ≠òÂú®Â∑ÆÂºÇÔºåËÄåÂá†‰Ωï‰º∞ËÆ°‰ªªÂä°ÈúÄË¶ÅÂπ≥Ë°°Ëøô‰∏§ÁßçÁâπÂæÅ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÁöÑÊäÄÊúØÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö1) ÈÄâÊã©È¢ÑËÆ≠ÁªÉÁöÑViTÔºàCLIP-ViTÂíåDINOÔºâÂíåCNNÔºàResNet„ÄÅEfficientNet„ÄÅCLIP-ResNetÔºâ‰Ωú‰∏∫È™®Âπ≤ÁΩëÁªúÔºõ2) Âú®2DÂàöÊÄßÂèòÊç¢‰º∞ËÆ°ÂíåÊú¨Ë¥®Áü©ÈòµÈ¢ÑÊµã‰∏§‰∏™‰ªªÂä°‰∏äËøõË°åÂÆûÈ™åÔºõ3) ËØÑ‰º∞‰∏çÂêåÊï∞ÊçÆËßÑÊ®°‰∏ãÔºåViTÂíåCNNÁöÑÊÄßËÉΩÔºõ4) ÂàÜÊûêÂÆûÈ™åÁªìÊûúÔºåÊÄªÁªìViTÂíåCNNÁöÑ‰ºòÁº∫ÁÇπ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÁ≥ªÁªüÂú∞ÂØπÊØî‰∫ÜViTÂíåCNNÂú®Â∞ëÊ†∑Êú¨Âá†‰Ωï‰º∞ËÆ°‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºåÂπ∂Êè≠Á§∫‰∫Ü‰∏çÂêåÊï∞ÊçÆËßÑÊ®°‰∏ãÔºåÊ®°ÂûãÊû∂ÊûÑÈÄâÊã©ÁöÑÈáçË¶ÅÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåËÆ∫ÊñáÊ≤°ÊúâÁõ¥Êé•Â∫îÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåËÄåÊòØÂÖ≥Ê≥®Ê®°ÂûãÊû∂ÊûÑÊú¨Ë∫´ÂØπÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÈÄâÊã©ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑViTÂíåCNNÊ®°ÂûãÔºõ2) Âú®‰∏§‰∏™‰∏çÂêåÁöÑÂá†‰Ωï‰º∞ËÆ°‰ªªÂä°‰∏äËøõË°åÂÆûÈ™åÔºõ3) ÈááÁî®‰∏çÂêåÁöÑÊï∞ÊçÆËßÑÊ®°ËÆæÁΩÆÔºåÂåÖÊã¨Â∞ëÊ†∑Êú¨Âú∫ÊôØÔºõ4) ‰ΩøÁî®Ê†áÂáÜÁöÑËØÑ‰º∞ÊåáÊ†áÔºåÂ¶ÇÂπ≥ÂùáÁ´ØÁÇπËØØÂ∑ÆÔºàAverage Endpoint ErrorÔºâÂíåSampsonË∑ùÁ¶ª„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Â§ßÂûã‰∏ãÊ∏∏Êï∞ÊçÆÂú∫ÊôØ‰∏≠ÔºåViTÂú®ÂæÆË∞ÉÂêé‰ºò‰∫éCNN„ÄÇÁÑ∂ËÄåÔºåÂú®Â∞èÊï∞ÊçÆÂú∫ÊôØ‰∏≠ÔºåCNNÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÂíåËæÉÂ∞èÁöÑÂÆπÈáè‰ΩøÂÖ∂ÊÄßËÉΩ‰∏éViTÁõ∏ÂåπÈÖç„ÄÇÊ≠§Â§ñÔºåViTÂú®Ë∑®ÂüüËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËøô‰∫õÁªìÊûúÂº∫Ë∞É‰∫ÜÂú®ÂæÆË∞ÉÊó∂‰ªîÁªÜÈÄâÊã©Ê®°ÂûãÊû∂ÊûÑÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËá™‰∏ªÁßªÂä®„ÄÅÊú∫Âô®‰∫∫Âíå3DÂú∫ÊôØÈáçÂª∫Á≠âÈ¢ÜÂüüÔºå‰∏∫Ëøô‰∫õÂ∫îÁî®‰∏≠ÁöÑÂá†‰Ωï‰º∞ËÆ°‰ªªÂä°Êèê‰æõÊõ¥ÊúâÊïàÁöÑÈ™®Âπ≤ÁΩëÁªúÈÄâÊã©Á≠ñÁï•„ÄÇÈÄöËøáÈÄâÊã©ÂêàÈÄÇÁöÑÊ®°ÂûãÊû∂ÊûÑÔºåÂèØ‰ª•ÊèêÈ´òÂá†‰Ωï‰º∞ËÆ°ÁöÑÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÔºå‰ªéËÄåÊèêÂçáÁõ∏ÂÖ≥Â∫îÁî®ÁöÑÊÄßËÉΩÂíåÂèØÈù†ÊÄß„ÄÇÊú™Êù•ÁöÑÁ†îÁ©∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êé¢Á¥¢Ê∑∑ÂêàÊû∂ÊûÑÔºå‰ª•Êõ¥Â•ΩÂú∞Âπ≥Ë°°Â±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÂæÅÔºå‰ªéËÄåÂú®ÂêÑÁßçÊï∞ÊçÆËßÑÊ®°‰∏ãÂÆûÁé∞Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs) have reshaped computer vision through pretrained feature representations that enable strong transfer learning for diverse tasks. However, their efficiency as backbone architectures for geometric estimation tasks involving image deformations in low-data regimes remains an open question. This work considers two such tasks: 1) estimating 2D rigid transformations between pairs of images and 2) predicting the fundamental matrix for stereo image pairs, an important problem in various applications, such as autonomous mobility, robotics, and 3D scene reconstruction. Addressing this intriguing question, this work systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet) with ViT-based foundation models (CLIP-ViT variants and DINO) in various data size settings, including few-shot scenarios. These pretrained models are optimized for classification or contrastive learning, encouraging them to focus mostly on high-level semantics. The considered tasks require balancing local and global features differently, challenging the straightforward adoption of these models as the backbone. Empirical comparative analysis shows that, similar to training from scratch, ViTs outperform CNNs during refinement in large downstream-data scenarios. However, in small data scenarios, the inductive bias and smaller capacity of CNNs improve their performance, allowing them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in cross-domain evaluation where the data distribution changes. These results emphasize the importance of carefully selecting model architectures for refinement, motivating future research towards hybrid architectures that balance local and global representations.

