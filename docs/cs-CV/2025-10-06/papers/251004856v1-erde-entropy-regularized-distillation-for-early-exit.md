---
layout: default
title: ERDE: Entropy-Regularized Distillation for Early-exit
---

# ERDE: Entropy-Regularized Distillation for Early-exit

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04856" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.04856v1</a>
  <a href="https://arxiv.org/pdf/2510.04856.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04856v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.04856v1', 'ERDE: Entropy-Regularized Distillation for Early-exit')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Martial Guidez, Stefan Duffner, Yannick Alpou, Oscar R√∂th, Christophe Garcia

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÁÜµÊ≠£ÂàôÂåñÁöÑÁü•ËØÜËí∏È¶èÊó©ÊúüÈÄÄÂá∫ÊñπÊ≥ïÔºåÊèêÂçáËæπÁºòËÆæÂ§áÂõæÂÉèÂàÜÁ±ªÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `Êó©ÊúüÈÄÄÂá∫` `Ê®°ÂûãÂéãÁº©` `ËæπÁºòËÆ°ÁÆó` `ÂõæÂÉèÂàÜÁ±ª`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúËÆ°ÁÆóÊàêÊú¨È´òÔºåÈöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆæÂ§á‰∏äÈÉ®ÁΩ≤ÔºåÈúÄË¶ÅÂéãÁº©ÊäÄÊúØ„ÄÇ
2. ÊèêÂá∫ÁÜµÊ≠£ÂàôÂåñÁöÑÁü•ËØÜËí∏È¶èÊó©ÊúüÈÄÄÂá∫ÊñπÊ≥ïÔºåÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãËÆ≠ÁªÉÔºå‰ºòÂåñÁ≤æÂ∫¶ÂíåÊïàÁéá„ÄÇ
3. Âú®CIFAR10Á≠âÊï∞ÊçÆÈõÜ‰∏äÈ™åËØÅÔºåËØ•ÊñπÊ≥ïÂú®Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜÂàÜÁ±ªÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑±Â∫¶Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂú®ÂõæÂÉèÂàÜÁ±ª‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏çÈÄÇÁî®‰∫éÂÆûÊó∂ÂíåËæπÁºòÂ∫îÁî®„ÄÇÊú¨ÊñáÊèêÂá∫‰∏ÄÁßçÊúâÊïàÁöÑ‰ºòÂåñÊñπÊ≥ïÔºåÁªìÂêà‰∫ÜÊó©ÊúüÈÄÄÂá∫ÂíåÁü•ËØÜËí∏È¶è‰∏§ÁßçÊäÄÊúØÔºåÂà©Áî®Êõ¥Â§çÊùÇÁöÑÊïôÂ∏àÊó©ÊúüÈÄÄÂá∫Ê®°ÂûãËÆ≠ÁªÉÁ≤æÁÆÄÁöÑÂ≠¶ÁîüÊó©ÊúüÈÄÄÂá∫Ê®°Âûã„ÄÇ‰∏ªË¶ÅË¥°ÁåÆÂú®‰∫éÂ≠¶ÁîüÊ®°ÂûãËÆ≠ÁªÉÊñπÊ≥ïÔºå‰∏é‰º†ÁªüÁü•ËØÜËí∏È¶èÊçüÂ§±Áõ∏ÊØîÔºåÈíàÂØπÊïôÂ∏àÂàÜÁ±ªÈîôËØØÁöÑÂõæÂÉèÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫‰∫éÁÜµÁöÑÊçüÂ§±„ÄÇËØ•ÊñπÊ≥ï‰ºòÂåñ‰∫ÜÁ≤æÂ∫¶ÂíåÊïàÁéá‰πãÈó¥ÁöÑÂπ≥Ë°°ÔºåÂú®‰∏çÂΩ±ÂìçÂàÜÁ±ªÊÄßËÉΩÁöÑÂâçÊèê‰∏ãÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÂú®CIFAR10„ÄÅCIFAR100ÂíåSVHNÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÂπ∂‰∏∫Áü•ËØÜËí∏È¶èÂú®ÂÖ∂‰ªñÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®ÂºÄËæü‰∫ÜÊñ∞ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúËÆ°ÁÆóÈáèÂ§ßÔºåÈöæ‰ª•Âú®ËæπÁºòËÆæÂ§á‰∏äÂÆûÊó∂ËøêË°å„ÄÇÁü•ËØÜËí∏È¶èÂíåÊó©ÊúüÈÄÄÂá∫ÊòØ‰∏§ÁßçÂ∏∏Áî®ÁöÑÊ®°ÂûãÂéãÁº©ÂíåÂä†ÈÄüÊäÄÊúØÔºå‰ΩÜÂ¶Ç‰ΩïÊúâÊïàÂú∞ÁªìÂêàËøô‰∏§ÁßçÊäÄÊúØÔºåÂú®‰øùËØÅÁ≤æÂ∫¶ÁöÑÂâçÊèê‰∏ãËøõ‰∏ÄÊ≠•Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ïÂú®ÊïôÂ∏àÊ®°ÂûãÈ¢ÑÊµãÈîôËØØÊó∂ÔºåÊó†Ê≥ïÊúâÊïàÂà©Áî®Ëøô‰∫õÈîôËØØ‰ø°ÊÅØÊù•ÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãÁöÑÂ≠¶‰π†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÈ¢ÑÊµãÈîôËØØÁöÑÊ†∑Êú¨ÔºåÈÄöËøáÁÜµÊ≠£ÂàôÂåñÁöÑÊñπÂºèÔºåÂºïÂØºÂ≠¶ÁîüÊ®°ÂûãÊõ¥Â•ΩÂú∞Â≠¶‰π†„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂΩìÊïôÂ∏àÊ®°ÂûãÈ¢ÑÊµãÈîôËØØÊó∂ÔºåÂºïÂÖ•Âü∫‰∫éÁÜµÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÈºìÂä±Â≠¶ÁîüÊ®°ÂûãÂØπËøô‰∫õÊ†∑Êú¨‰∫ßÁîüÊõ¥‰∏çÁ°ÆÂÆöÁöÑÈ¢ÑÊµãÔºå‰ªéËÄåÈÅøÂÖçÂ≠¶ÁîüÊ®°ÂûãÁõ≤ÁõÆÂú∞Ê®°‰ªøÊïôÂ∏àÊ®°ÂûãÁöÑÈîôËØØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏Ä‰∏™ÊïôÂ∏àÊó©ÊúüÈÄÄÂá∫Ê®°ÂûãÂíå‰∏Ä‰∏™Â≠¶ÁîüÊó©ÊúüÈÄÄÂá∫Ê®°Âûã„ÄÇÈ¶ñÂÖàËÆ≠ÁªÉ‰∏Ä‰∏™È´òÊÄßËÉΩÁöÑÊïôÂ∏àÊ®°ÂûãÔºåÁÑ∂ÂêéÂà©Áî®ËØ•ÊïôÂ∏àÊ®°ÂûãÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÇÂ≠¶ÁîüÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂêåÊó∂ÊúÄÂ∞èÂåñ‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊçüÂ§±ÂíåÂü∫‰∫éÁÜµÁöÑÊçüÂ§±„ÄÇÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÈò∂ÊÆµÔºö1) ÊïôÂ∏àÊ®°ÂûãËÆ≠ÁªÉÔºõ2) Â≠¶ÁîüÊ®°ÂûãÂàùÂßãÂåñÔºõ3) Âü∫‰∫éÁü•ËØÜËí∏È¶èÂíåÁÜµÊ≠£ÂàôÂåñÁöÑÂ≠¶ÁîüÊ®°ÂûãËÆ≠ÁªÉÔºõ4) Ê®°ÂûãËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂºïÂÖ•‰∫ÜÂü∫‰∫éÁÜµÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éÂ§ÑÁêÜÊïôÂ∏àÊ®°ÂûãÈ¢ÑÊµãÈîôËØØÁöÑÊ†∑Êú¨„ÄÇ‰∏é‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ï‰∏çÂêåÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÁöÑÈîôËØØ‰ø°ÊÅØÔºåÂºïÂØºÂ≠¶ÁîüÊ®°ÂûãÊõ¥Â•ΩÂú∞Â≠¶‰π†„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§ü‰ºòÂåñÁ≤æÂ∫¶ÂíåÊïàÁéá‰πãÈó¥ÁöÑÂπ≥Ë°°ÔºåÂú®‰∏çÂΩ±ÂìçÂàÜÁ±ªÊÄßËÉΩÁöÑÂâçÊèê‰∏ãÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Êó©ÊúüÈÄÄÂá∫Êú∫Âà∂ÔºöÂú®ÁΩëÁªúÁöÑ‰∏≠Èó¥Â±ÇËÆæÁΩÆÂ§ö‰∏™ÈÄÄÂá∫ÁÇπÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®ËææÂà∞‰∏ÄÂÆöÁΩÆ‰ø°Â∫¶Êó∂ÊèêÂâçËæìÂá∫ÁªìÊûúÔºå‰ªéËÄåÈôç‰ΩéËÆ°ÁÆóÈáè„ÄÇ2) ÁÜµÊ≠£ÂàôÂåñÊçüÂ§±ÂáΩÊï∞ÔºöÂΩìÊïôÂ∏àÊ®°ÂûãÈ¢ÑÊµãÈîôËØØÊó∂Ôºå‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÔºåÁõÆÊ†áÊòØÊúÄÂ§ßÂåñÂ≠¶ÁîüÊ®°ÂûãËæìÂá∫Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÁÜµÔºåÈºìÂä±Â≠¶ÁîüÊ®°ÂûãÂØπËøô‰∫õÊ†∑Êú¨‰∫ßÁîüÊõ¥‰∏çÁ°ÆÂÆöÁöÑÈ¢ÑÊµã„ÄÇ3) Áü•ËØÜËí∏È¶èÊçüÂ§±ÂáΩÊï∞Ôºö‰ΩøÁî®‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊçüÂ§±ÂáΩÊï∞Ôºå‰æãÂ¶ÇKLÊï£Â∫¶ÔºåÁî®‰∫éË°°ÈáèÂ≠¶ÁîüÊ®°ÂûãÂíåÊïôÂ∏àÊ®°ÂûãËæìÂá∫Ê¶ÇÁéáÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®CIFAR10„ÄÅCIFAR100ÂíåSVHNÊï∞ÊçÆÈõÜ‰∏äÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂú®ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÁöÑÂêåÊó∂Ôºå‰øùÊåÅÁîöËá≥Áï•ÂæÆÊèêÂçáÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇ‰∏é‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®ÊïôÂ∏àÊ®°ÂûãÁöÑÈîôËØØ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÂ≠¶ÁîüÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑËæπÁºòËÆæÂ§áÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á„ÄÅÂµåÂÖ•ÂºèÁ≥ªÁªüÂíåÁâ©ËÅîÁΩëËÆæÂ§á„ÄÇÈÄöËøáÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Âø´ÁöÑÂõæÂÉèÂàÜÁ±ªÈÄüÂ∫¶ÂíåÊõ¥‰ΩéÁöÑÂäüËÄóÔºå‰ªéËÄåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÂíåÂª∂ÈïøËÆæÂ§áÁª≠Ëà™Êó∂Èó¥„ÄÇËØ•ÊñπÊ≥ïËøòÂèØÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÁõëÊéßÁ≠âÈ¢ÜÂüüÔºåÊèêÈ´òÂÆûÊó∂ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Although deep neural networks and in particular Convolutional Neural Networks have demonstrated state-of-the-art performance in image classification with relatively high efficiency, they still exhibit high computational costs, often rendering them impractical for real-time and edge applications. Therefore, a multitude of compression techniques have been developed to reduce these costs while maintaining accuracy. In addition, dynamic architectures have been introduced to modulate the level of compression at execution time, which is a desirable property in many resource-limited application scenarios. The proposed method effectively integrates two well-established optimization techniques: early exits and knowledge distillation, where a reduced student early-exit model is trained from a more complex teacher early-exit model. The primary contribution of this research lies in the approach for training the student early-exit model. In comparison to the conventional Knowledge Distillation loss, our approach incorporates a new entropy-based loss for images where the teacher's classification was incorrect. The proposed method optimizes the trade-off between accuracy and efficiency, thereby achieving significant reductions in computational complexity without compromising classification performance. The validity of this approach is substantiated by experimental results on image classification datasets CIFAR10, CIFAR100 and SVHN, which further opens new research perspectives for Knowledge Distillation in other contexts.

