---
layout: default
title: General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks
---

# General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06277" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.06277v1</a>
  <a href="https://arxiv.org/pdf/2510.06277.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06277v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.06277v1', 'General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fahim Shahriar, Cheryl Wang, Alireza Azimi, Gautham Vasan, Hany Hamed Elanwar, A. Rupam Mahmood, Colin Bellinger

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¯¹è±¡æ— å…³æ©ç çš„è§†è§‰ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡æ³›åŒ–æ€§å’Œæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ` `è§†è§‰å¼ºåŒ–å­¦ä¹ ` `å¯¹è±¡æ— å…³æ©ç ` `æœºå™¨äººæ“ä½œ` `Sim-to-Realè¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç›®æ ‡è¡¨ç¤ºä¸Šå­˜åœ¨æ³›åŒ–æ€§å·®ã€æ”¶æ•›æ…¢ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºä½¿ç”¨å¯¹è±¡æ— å…³çš„æ©ç ä½œä¸ºç›®æ ‡è¡¨ç¤ºï¼Œä¸ºæ™ºèƒ½ä½“æä¾›æ›´é²æ£’çš„è§†è§‰çº¿ç´¢ï¼Œä»è€Œæå‡å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­å‡å–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨å¯¹è±¡æ‹¾å–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ©ç çš„ç›®æ ‡è¡¨ç¤ºç³»ç»Ÿï¼Œç”¨äºé€šç”¨ä¸”é«˜æ•ˆçš„è§†è§‰ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ (GCRL)ã€‚è¯¥ç³»ç»Ÿä¸ºæ™ºèƒ½ä½“æä¾›å¯¹è±¡æ— å…³çš„è§†è§‰çº¿ç´¢ï¼Œä»è€Œå®ç°é«˜æ•ˆå­¦ä¹ å’Œå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„ç›®æ ‡è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚ç›®æ ‡çŠ¶æ€å›¾åƒã€3Dåæ ‡å’Œone-hotå‘é‡ï¼‰ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…‹æœäº†æ³›åŒ–èƒ½åŠ›å·®ã€æ”¶æ•›é€Ÿåº¦æ…¢ä»¥åŠéœ€è¦ç‰¹æ®Šç›¸æœºç­‰é—®é¢˜ã€‚æ©ç å¯ä»¥è¢«å¤„ç†ä»¥ç”Ÿæˆå¯†é›†çš„å¥–åŠ±ï¼Œè€Œæ— éœ€å®¹æ˜“å‡ºé”™çš„è·ç¦»è®¡ç®—ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ä½¿ç”¨ground truthæ©ç è¿›è¡Œå­¦ä¹ ï¼Œåœ¨è®­ç»ƒå’Œæœªè§è¿‡çš„æµ‹è¯•å¯¹è±¡ä¸Šè¾¾åˆ°äº†99.9%çš„åˆ°è¾¾ç²¾åº¦ã€‚è¯¥æ–¹æ³•æ— éœ€ç›®æ ‡ä½ç½®ä¿¡æ¯å³å¯é«˜ç²¾åº¦åœ°æ‰§è¡Œæ‹¾å–ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†ä½¿ç”¨é¢„è®­ç»ƒçš„å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹æ¨¡å‹ç”Ÿæˆæ©ç ï¼Œåœ¨ä¸¤ç§ä¸åŒçš„ç‰©ç†æœºå™¨äººä¸Šä»å¤´å¼€å§‹å­¦ä¹ å’Œä»æ¨¡æ‹Ÿåˆ°çœŸå®çš„è¿ç§»åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGCRLï¼‰æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨ç›®æ ‡çŠ¶æ€å›¾åƒã€3Dåæ ‡æˆ–one-hotå‘é‡ä½œä¸ºç›®æ ‡è¡¨ç¤ºï¼Œå­˜åœ¨æ³›åŒ–èƒ½åŠ›å·®ã€æ”¶æ•›é€Ÿåº¦æ…¢ä»¥åŠå¯¹ç‰¹å®šç¡¬ä»¶ï¼ˆå¦‚ç‰¹æ®Šç›¸æœºï¼‰çš„ä¾èµ–ç­‰é—®é¢˜ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†GCRLåœ¨å®é™…æœºå™¨äººä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æœªè§è¿‡çš„å¯¹è±¡æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä½¿ç”¨å¯¹è±¡æ— å…³çš„æ©ç ï¼ˆObject-Agnostic Masksï¼‰ä½œä¸ºç›®æ ‡è¡¨ç¤ºã€‚æ©ç èƒ½å¤Ÿæä¾›å…³äºç›®æ ‡å½¢çŠ¶å’Œä½ç½®çš„è§†è§‰çº¿ç´¢ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°æŒ‡å®šç›®æ ‡çš„ç±»åˆ«æˆ–3Dåæ ‡ã€‚è¿™ç§è¡¨ç¤ºæ–¹å¼æ›´å…·é€šç”¨æ€§ï¼Œå¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„å¯¹è±¡ã€‚æ­¤å¤–ï¼Œæ©ç å¯ä»¥ç”¨äºç”Ÿæˆå¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€ŒåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰æ„ŸçŸ¥æ¨¡å—ï¼šä½¿ç”¨å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œé¢„è®­ç»ƒçš„å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼‰ä»å›¾åƒä¸­æå–ç›®æ ‡çš„æ©ç ã€‚2) ç›®æ ‡è¡¨ç¤ºæ¨¡å—ï¼šå°†æå–çš„æ©ç ä½œä¸ºç›®æ ‡çŠ¶æ€çš„è¡¨ç¤ºã€‚3) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šä½¿ç”¨GCRLç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®å½“å‰çŠ¶æ€å’Œç›®æ ‡æ©ç é‡‡å–è¡ŒåŠ¨ã€‚4) å¥–åŠ±å‡½æ•°è®¾è®¡ï¼šåŸºäºå½“å‰çŠ¶æ€çš„æ©ç å’Œç›®æ ‡æ©ç ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ™ºèƒ½ä½“æœç€ç›®æ ‡çŠ¶æ€ç§»åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨å¯¹è±¡æ— å…³çš„æ©ç ä½œä¸ºç›®æ ‡è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„åŸºäºåæ ‡æˆ–å›¾åƒçš„ç›®æ ‡è¡¨ç¤ºæ–¹æ³•ç›¸æ¯”ï¼Œæ©ç èƒ½å¤Ÿæä¾›æ›´é²æ£’çš„è§†è§‰çº¿ç´¢ï¼Œå¹¶ä¸”æ›´å®¹æ˜“æ³›åŒ–åˆ°æœªè§è¿‡çš„å¯¹è±¡ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ©ç å¯ä»¥é¿å…å¤æ‚çš„è·ç¦»è®¡ç®—ï¼Œä»è€Œç®€åŒ–äº†å¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®çš„è®¾è®¡ç»†èŠ‚åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹æ¨¡å‹æ¥æå–æ©ç ï¼Œè¿™ä½¿å¾—è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å„ç§ä¸åŒçš„å¯¹è±¡ã€‚2) è®¾è®¡åŸºäºæ©ç ç›¸ä¼¼åº¦çš„å¥–åŠ±å‡½æ•°ï¼Œä¾‹å¦‚ä½¿ç”¨IoUï¼ˆIntersection over Unionï¼‰ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡ã€‚3) ä½¿ç”¨æ ‡å‡†çš„GCRLç®—æ³•ï¼Œä¾‹å¦‚Hindsight Experience Replay (HER)ï¼Œæ¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚4) åœ¨sim-to-realè¿ç§»ä¸­ï¼Œä½¿ç”¨åŸŸéšæœºåŒ–ç­‰æŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¾¾åˆ°äº†99.9%çš„åˆ°è¾¾ç²¾åº¦ï¼Œå¹¶ä¸”èƒ½å¤ŸæˆåŠŸåœ°è¿ç§»åˆ°çœŸå®æœºå™¨äººç¯å¢ƒä¸­ã€‚ä¸ä¼ ç»Ÿçš„GCRLæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›å’Œå­¦ä¹ æ•ˆç‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚ç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡æ‹¾å–ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•æ— éœ€ç›®æ ‡ä½ç½®ä¿¡æ¯å³å¯å®ç°é«˜ç²¾åº¦æ“ä½œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€æ”¾ç½®ã€ç»„è£…ç­‰ã€‚ç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤„ç†å¤šç§ä¸åŒå¯¹è±¡æˆ–åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ“ä½œçš„åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸï¼Œç”¨äºç›®æ ‡æ£€æµ‹ã€è·Ÿè¸ªå’Œè¡Œä¸ºé¢„æµ‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse objectives using a unified policy. The success of GCRL, however, is contingent on the choice of goal representation. In this work, we propose a mask-based goal representation system that provides object-agnostic visual cues to the agent, enabling efficient learning and superior generalization. In contrast, existing goal representation methods, such as target state images, 3D coordinates, and one-hot vectors, face issues of poor generalization to unseen objects, slow convergence, and the need for special cameras. Masks can be processed to generate dense rewards without requiring error-prone distance calculations. Learning with ground truth masks in simulation, we achieved 99.9% reaching accuracy on training and unseen test objects. Our proposed method can be utilized to perform pick-up tasks with high accuracy, without using any positional information of the target. Moreover, we demonstrate learning from scratch and sim-to-real transfer applications using two different physical robots, utilizing pretrained open vocabulary object detection models for mask generation.

