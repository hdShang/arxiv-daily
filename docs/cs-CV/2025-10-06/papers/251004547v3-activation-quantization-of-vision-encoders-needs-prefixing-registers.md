---
layout: default
title: Activation Quantization of Vision Encoders Needs Prefixing Registers
---

# Activation Quantization of Vision Encoders Needs Prefixing Registers

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04547" target="_blank" class="toolbar-btn">arXiv: 2510.04547v3</a>
    <a href="https://arxiv.org/pdf/2510.04547.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04547v3" 
            onclick="toggleFavorite(this, '2510.04547v3', 'Activation Quantization of Vision Encoders Needs Prefixing Registers')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-06 (Êõ¥Êñ∞: 2025-11-28)

**Â§áÊ≥®**: 19 pages, 8 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RegCacheÔºåÈÄöËøáÂâçÁºÄÂØÑÂ≠òÂô®ÂÆûÁé∞ËßÜËßâÁºñÁ†ÅÂô®ÊøÄÊ¥ªÈáèÂåñÁöÑÊó†ËÆ≠ÁªÉ‰ºòÂåñ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÁºñÁ†ÅÂô®` `ÈáèÂåñ` `ÊøÄÊ¥ªÂÄº` `ÂºÇÂ∏∏ÂÄºÊäëÂà∂` `ÂâçÁºÄÂØÑÂ≠òÂô®`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâÁºñÁ†ÅÂô®ÈáèÂåñÈù¢‰∏¥Â§ßËßÑÊ®°ÊøÄÊ¥ªÂºÇÂ∏∏ÂÄºÁöÑÊåëÊàòÔºåÂç≥‰ΩøÂú®8‰ΩçÁ≤æÂ∫¶‰∏ã‰πüÈöæ‰ª•‰øùËØÅÊÄßËÉΩ„ÄÇ
2. RegCacheÈÄöËøáÂºïÂÖ•ÂâçÁºÄtokenÊù•ÊäëÂà∂ÊøÄÊ¥ªÂºÇÂ∏∏ÂÄºÔºåÊó†ÈúÄËÆ≠ÁªÉÔºåÂèØ‰Ωú‰∏∫Êèí‰ª∂ÈõÜÊàêÂà∞Áé∞ÊúâÈáèÂåñÊµÅÁ®ã‰∏≠„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåRegCacheËÉΩÊúâÊïàÊèêÂçáÈáèÂåñÂêéËßÜËßâÁºñÁ†ÅÂô®ÁöÑÁ≤æÂ∫¶ÔºåÈÄÇÁî®‰∫éÊñáÊú¨ÁõëÁù£ÂíåËá™ÁõëÁù£Ê®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âü∫‰∫éTransformerÁöÑËßÜËßâÁºñÁ†ÅÂô®ÔºåÂ¶ÇCLIPÔºåÂú®Â§öÊ®°ÊÄÅÊô∫ËÉΩ‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÈ©±Âä®ÁùÄ‰ªéËá™‰∏ªÁΩëÁªú‰ª£ÁêÜÂà∞Êú∫Âô®‰∫∫ÊéßÂà∂Á≠âÂ∫îÁî®„ÄÇÁî±‰∫éËøô‰∫õÂ∫îÁî®ÈÄöÂ∏∏ÈúÄË¶ÅÂØπÊµ∑ÈáèËßÜËßâÊï∞ÊçÆËøõË°åÂÆûÊó∂Â§ÑÁêÜÔºåÂõ†Ê≠§Èôç‰ΩéËßÜËßâÁºñÁ†ÅÂô®ÁöÑÊé®ÁêÜÊàêÊú¨Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÈáèÂåñÊèê‰æõ‰∫Ü‰∏ÄÊù°ÂèØË°åÁöÑÈÄîÂæÑÔºå‰ΩÜÁî±‰∫éÂ§ßËßÑÊ®°ÊøÄÊ¥ªÔºàÂç≥ÂºÇÂ∏∏ÂÄºÔºâÔºåÂç≥‰ΩøÂú®8‰ΩçÁ≤æÂ∫¶‰∏ã‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫RegCacheÔºå‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑÁÆóÊ≥ïÔºåÁî®‰∫éÁºìËß£Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÔºåÂπ∂‰Ωú‰∏∫‰∏Ä‰∏™Êèí‰ª∂Ê®°ÂùóÂ∫îÁî®‰∫éÂÖ∂‰ªñÈáèÂåñÊñπÊ≥ï‰πã‰∏ä„ÄÇRegCacheÂ∞ÜÊòì‰∫éÂá∫Áé∞ÂºÇÂ∏∏ÂÄº‰ΩÜËØ≠‰πâ‰∏äÊó†ÊÑè‰πâÁöÑÂâçÁºÄtokenÂºïÂÖ•ÁõÆÊ†áËßÜËßâÁºñÁ†ÅÂô®Ôºå‰ªéËÄåÈò≤Ê≠¢ÂÖ∂‰ªñtokenÂá∫Áé∞ÂºÇÂ∏∏ÂÄº„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàë‰ª¨ËßÇÂØüÂà∞ËßÜËßâÁºñÁ†ÅÂô®‰∏≠ÁöÑÂºÇÂ∏∏ÂÄº‰∏éËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºË°®Áé∞‰∏çÂêåÔºåËøô‰øÉ‰Ωø‰∫Ü‰∏§È°πÊäÄÊúØÂàõÊñ∞Ôºö‰∏≠Èó¥Â±ÇÂâçÁºÄÂíåtokenÂà†Èô§„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂßãÁªàÊèêÈ´òÊñáÊú¨ÁõëÁù£ÂíåËá™ÁõëÁù£ËßÜËßâÁºñÁ†ÅÂô®‰∏≠ÈáèÂåñÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâÁºñÁ†ÅÂô®ÈáèÂåñËøáÁ®ã‰∏≠ÔºåÁî±‰∫éÊøÄÊ¥ªÂÄº‰∏≠Â≠òÂú®Â§ßÈáèÂºÇÂ∏∏ÂÄºÔºàoutliersÔºâËÄåÂØºËá¥ÁöÑÁ≤æÂ∫¶‰∏ãÈôçÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÈáèÂåñÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§ßËßÑÊ®°ËßÜËßâÁºñÁ†ÅÂô®Êó∂ÔºåÈöæ‰ª•ÊúâÊïàÊäëÂà∂Ëøô‰∫õÂºÇÂ∏∏ÂÄºÔºåÂØºËá¥ÈáèÂåñÂêéÁöÑÊ®°ÂûãÊÄßËÉΩÊòæËëóÈôç‰Ωé„ÄÇÂ∞§ÂÖ∂ÊòØÂú®‰ΩéÊØîÁâπÈáèÂåñÁöÑÊÉÖÂÜµ‰∏ãÔºåËøô‰∏™ÈóÆÈ¢òÊõ¥Âä†Á™ÅÂá∫„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂºïÂÖ•‚ÄúÂâçÁºÄÂØÑÂ≠òÂô®‚ÄùÔºàPrefixing RegistersÔºâÔºåÂç≥Âú®ËßÜËßâÁºñÁ†ÅÂô®ÁöÑËæìÂÖ•tokenÂ∫èÂàó‰∏≠Ê∑ªÂä†‰∏Ä‰∫õÁâπÊÆäÁöÑ„ÄÅËØ≠‰πâ‰∏äÊó†ÊÑè‰πâÁöÑtoken„ÄÇËøô‰∫õtokenÁöÑËÆæËÆ°ÁõÆÁöÑÊòØÂê∏ÂºïÂπ∂ÈöîÁ¶ªÊøÄÊ¥ªÂÄº‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÔºå‰ªéËÄåÈò≤Ê≠¢ÂÖ∂‰ªñÂÖ∑ÊúâÂÆûÈôÖËØ≠‰πâ‰ø°ÊÅØÁöÑtokenÂèóÂà∞ÂºÇÂ∏∏ÂÄºÁöÑÂΩ±Âìç„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•ÊúâÊïàÂú∞ÊäëÂà∂ÈáèÂåñËøáÁ®ã‰∏≠ÁöÑÁ≤æÂ∫¶ÊçüÂ§±„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRegCache‰Ωú‰∏∫‰∏Ä‰∏™Êèí‰ª∂Ê®°ÂùóÔºåÂèØ‰ª•Ê∑ªÂä†Âà∞Áé∞ÊúâÁöÑËßÜËßâÁºñÁ†ÅÂô®ÈáèÂåñÊµÅÁ®ã‰∏≠„ÄÇÂÖ∂‰∏ªË¶ÅÊ≠•È™§ÂåÖÊã¨Ôºö1ÔºâÂú®ËæìÂÖ•tokenÂ∫èÂàó‰∏≠Ê∑ªÂä†ÂâçÁºÄtokenÔºõ2ÔºâÂú®ËßÜËßâÁºñÁ†ÅÂô®ÁöÑ‰∏≠Èó¥Â±ÇÔºàËÄåÈùû‰ªÖÂú®ËæìÂÖ•Â±ÇÔºâÊ∑ªÂä†ÂâçÁºÄtokenÔºå‰ª•Êõ¥Â•ΩÂú∞ÊçïËé∑ÂíåÈöîÁ¶ªÂºÇÂ∏∏ÂÄºÔºõ3ÔºâÂØπÊ∑ªÂä†ÁöÑÂâçÁºÄtokenËøõË°åÈÄâÊã©ÊÄßÂà†Èô§Ôºå‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÊÄßËÉΩ„ÄÇÊï¥‰∏™Ê°ÜÊû∂Êó†ÈúÄÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÔºåÂèØ‰ª•Áõ¥Êé•Â∫îÁî®‰∫éÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÁºñÁ†ÅÂô®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1ÔºâÊèêÂá∫‰∫ÜÂâçÁºÄÂØÑÂ≠òÂô®ÁöÑÊ¶ÇÂøµÔºåÈÄöËøáÂºïÂÖ•ËØ≠‰πâ‰∏äÊó†ÊÑè‰πâÁöÑtokenÊù•ÊäëÂà∂ÊøÄÊ¥ªÂºÇÂ∏∏ÂÄºÔºõ2ÔºâËßÇÂØüÂà∞ËßÜËßâÁºñÁ†ÅÂô®‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºË°å‰∏∫‰∏éËØ≠Ë®ÄÊ®°Âûã‰∏çÂêåÔºåÂõ†Ê≠§ÊèêÂá∫‰∫Ü‰∏≠Èó¥Â±ÇÂâçÁºÄÂíåtokenÂà†Èô§Á≠ñÁï•ÔºåÊõ¥ÊúâÊïàÂú∞Â§ÑÁêÜËßÜËßâÁºñÁ†ÅÂô®ÁöÑÈáèÂåñÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖ∑‰ΩìÂÆûÁé∞‰∏äÔºåÂâçÁºÄtokenÁöÑÊï∞ÈáèÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜËøõË°åË∞ÉÊï¥„ÄÇËÆ∫Êñá‰∏≠ËøòÊèêÂà∞ÔºåÂú®‰∏≠Èó¥Â±ÇÊ∑ªÂä†ÂâçÁºÄtokenÂèØ‰ª•Êõ¥Â•ΩÂú∞ÊçïËé∑ÂíåÈöîÁ¶ªÂºÇÂ∏∏ÂÄºÔºåËøô‰∏éÂú®ËæìÂÖ•Â±ÇÊ∑ªÂä†ÂâçÁºÄtokenÁöÑÊïàÊûú‰∏çÂêå„ÄÇÊ≠§Â§ñÔºåtokenÂà†Èô§Á≠ñÁï•‰πüÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÁöÑËÆæËÆ°ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÊ≤°ÊúâÊîπÂèòÔºåRegCache‰Ωú‰∏∫‰∏Ä‰∏™Êèí‰ª∂Ôºå‰∏ªË¶Å‰ΩúÁî®‰∫éËæìÂÖ•tokenÂ∫èÂàóÁöÑÂ§ÑÁêÜ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRegCacheËÉΩÂ§üÊòæËëóÊèêÈ´òÈáèÂåñÂêéËßÜËßâÁºñÁ†ÅÂô®ÁöÑÁ≤æÂ∫¶„ÄÇ‰æãÂ¶ÇÔºåÂú®CLIPÊ®°Âûã‰∏äÔºå‰ΩøÁî®RegCacheÂêéÔºåÈáèÂåñÊ®°ÂûãÁöÑÁ≤æÂ∫¶ÊèêÂçá‰∫ÜÂ§ö‰∏™ÁôæÂàÜÁÇπ„ÄÇÊ≠§Â§ñÔºåRegCacheÂú®ÊñáÊú¨ÁõëÁù£ÂíåËá™ÁõëÁù£ËßÜËßâÁºñÁ†ÅÂô®‰∏äÂùáË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂ÈÄöÁî®ÊÄßÂíåÊúâÊïàÊÄß„ÄÇËØ•ÊñπÊ≥ïÊó†ÈúÄËÆ≠ÁªÉÔºåÊòì‰∫éÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÈáèÂåñÊµÅÁ®ã‰∏≠„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

RegCacheÊäÄÊúØÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßç‰æùËµñËßÜËßâÁºñÁ†ÅÂô®ÁöÑÂ§öÊ®°ÊÄÅÊô∫ËÉΩÂ∫îÁî®‰∏≠Ôºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÂõæÂÉèÊêúÁ¥¢„ÄÅËßÜÈ¢ëÂàÜÊûêÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéËßÜËßâÁºñÁ†ÅÂô®ÁöÑÊé®ÁêÜÊàêÊú¨ÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥ÂÆûÊó∂ÁöÑËßÜËßâ‰ø°ÊÅØÂ§ÑÁêÜÔºå‰ªéËÄåÊèêÂçáÁõ∏ÂÖ≥Â∫îÁî®ÁöÑÊÄßËÉΩÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇËØ•ÊäÄÊúØËøòÊúâÂä©‰∫éÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÈÉ®ÁΩ≤Â§çÊùÇÁöÑËßÜËßâÊ®°Âûã„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\textit{RegCache}$, a training-free algorithm that mitigates outliers in large-scale pretrained vision encoders and serves as a plug-in module that can be applied on top of other quantization methods. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.

