---
layout: default
title: Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation
---

# Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04838" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.04838v1</a>
  <a href="https://arxiv.org/pdf/2510.04838.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04838v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.04838v1', 'Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Muquan Li, Hang Gou, Dongyang Zhang, Shuang Liang, Xiurui Xie, Deqiang Ouyang, Ke Qin

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAT-BPTTï¼Œé€šè¿‡è‡ªåŠ¨å†…å¾ªç¯ä¼˜åŒ–æå‡æ•°æ®é›†è’¸é¦æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `å†…å¾ªç¯ä¼˜åŒ–` `åå‘ä¼ æ’­` `æ¢¯åº¦æˆªæ–­` `è‡ªé€‚åº”å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ•°æ®é›†è’¸é¦æ–¹æ³•ä¾èµ–éšæœºæˆªæ–­ç­–ç•¥ï¼Œå¿½ç•¥äº†ç¥ç»ç½‘ç»œåœ¨ä¸åŒè®­ç»ƒé˜¶æ®µçš„å­¦ä¹ åŠ¨æ€å·®å¼‚ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
2. AT-BPTT æ¡†æ¶é€šè¿‡æ¦‚ç‡æœºåˆ¶é€‰æ‹©æ—¶é—´æ­¥ï¼Œå¹¶æ ¹æ®æ¢¯åº¦å˜åŒ–è‡ªé€‚åº”è°ƒæ•´çª—å£å¤§å°ï¼Œå®ç°åŠ¨æ€æˆªæ–­åå‘ä¼ æ’­ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒAT-BPTT åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹å‡†ç¡®ç‡ï¼Œå¹¶åŠ é€Ÿäº†å†…å¾ªç¯ä¼˜åŒ–ï¼Œé™ä½äº†å†…å­˜æ¶ˆè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†åº”å¯¹é«˜æ•ˆæ·±åº¦å­¦ä¹ æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œæ•°æ®é›†è’¸é¦å·²æˆä¸ºä¸€ç§å…³é”®æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿåœ¨å‹ç¼©è®­ç»ƒæ•°æ®é›†çš„åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†è’¸é¦å†…å¾ªç¯ä¼˜åŒ–æ–¹æ³•é€šå¸¸ä¾èµ–äºéšæœºæˆªæ–­ç­–ç•¥ï¼Œç¼ºä¹çµæ´»æ€§ï¼Œå¹¶ä¸”å¸¸å¸¸äº§ç”Ÿæ¬¡ä¼˜ç»“æœã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ç¥ç»ç½‘ç»œåœ¨ä¸åŒçš„è®­ç»ƒé˜¶æ®µï¼ˆæ—©æœŸã€ä¸­æœŸå’Œæ™šæœŸï¼‰è¡¨ç°å‡ºä¸åŒçš„å­¦ä¹ åŠ¨æ€ï¼Œè¿™ä½¿å¾—éšæœºæˆªæ–­å˜å¾—æ— æ•ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”è‡ªåŠ¨æˆªæ–­åå‘ä¼ æ’­æ—¶é—´ï¼ˆAT-BPTTï¼‰ï¼Œå®ƒå¯ä»¥æ ¹æ®å†…åœ¨çš„æ¢¯åº¦è¡Œä¸ºåŠ¨æ€åœ°è°ƒæ•´æˆªæ–­ä½ç½®å’Œçª—å£å¤§å°ã€‚AT-BPTT å¼•å…¥äº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ä¸€ç§ç”¨äºé˜¶æ®µæ„ŸçŸ¥æ—¶é—´æ­¥é€‰æ‹©çš„æ¦‚ç‡æœºåˆ¶ï¼Œï¼ˆ2ï¼‰ä¸€ç§åŸºäºæ¢¯åº¦å˜åŒ–çš„è‡ªé€‚åº”çª—å£å¤§å°è°ƒæ•´ç­–ç•¥ï¼Œä»¥åŠï¼ˆ3ï¼‰ä¸€ç§ç”¨äºé™ä½è®¡ç®—å¼€é”€çš„ä½ç§© Hessian è¿‘ä¼¼ã€‚åœ¨ CIFAR-10ã€CIFAR-100ã€Tiny-ImageNet å’Œ ImageNet-1K ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAT-BPTT å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡æé«˜äº† 6.16% çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å†…å¾ªç¯ä¼˜åŒ–åŠ é€Ÿäº† 3.9 å€ï¼ŒåŒæ—¶èŠ‚çœäº† 63% çš„å†…å­˜æˆæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ•°æ®é›†è’¸é¦æ—¨åœ¨ç”¨è¿œå°äºåŸå§‹æ•°æ®é›†çš„åˆæˆæ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶è¾¾åˆ°ä¸åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒç›¸ä¼¼çš„æ€§èƒ½ã€‚ç°æœ‰çš„å†…å¾ªç¯ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚éšæœºæˆªæ–­åå‘ä¼ æ’­ï¼ˆBPTTï¼‰ï¼Œåœ¨å¤„ç†ä¸åŒè®­ç»ƒé˜¶æ®µçš„å­¦ä¹ åŠ¨æ€æ—¶ç¼ºä¹é€‚åº”æ€§ï¼Œå¯¼è‡´è’¸é¦å‡ºçš„æ•°æ®é›†è´¨é‡ä¸é«˜ï¼Œæ¨¡å‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸åŒé˜¶æ®µçš„æ¢¯åº¦è¡Œä¸ºï¼ŒåŠ¨æ€åœ°è°ƒæ•´ BPTT çš„æˆªæ–­ä½ç½®å’Œçª—å£å¤§å°ã€‚é€šè¿‡è‡ªé€‚åº”åœ°é€‰æ‹©æ›´é‡è¦çš„æ—¶é—´æ­¥è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œå¹¶è°ƒæ•´çª—å£å¤§å°ä»¥é€‚åº”æ¢¯åº¦å˜åŒ–ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è¿›è¡Œæ•°æ®é›†è’¸é¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAT-BPTT æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š1) **é˜¶æ®µæ„ŸçŸ¥æ—¶é—´æ­¥é€‰æ‹©**ï¼šä½¿ç”¨æ¦‚ç‡æœºåˆ¶ï¼Œæ ¹æ®å½“å‰è®­ç»ƒé˜¶æ®µï¼ˆæ—©æœŸã€ä¸­æœŸã€æ™šæœŸï¼‰çš„é‡è¦æ€§ï¼Œé€‰æ‹©å‚ä¸æ¢¯åº¦æ›´æ–°çš„æ—¶é—´æ­¥ã€‚2) **è‡ªé€‚åº”çª—å£å¤§å°è°ƒæ•´**ï¼šæ ¹æ®æ¢¯åº¦å˜åŒ–åŠ¨æ€è°ƒæ•´ BPTT çš„çª—å£å¤§å°ï¼Œç¡®ä¿é‡è¦æ¢¯åº¦ä¿¡æ¯èƒ½å¤Ÿè¢«æœ‰æ•ˆä¼ æ’­ã€‚3) **ä½ç§© Hessian è¿‘ä¼¼**ï¼šä½¿ç”¨ä½ç§© Hessian è¿‘ä¼¼æ¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜ä¼˜åŒ–æ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šAT-BPTT çš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŠ¨æ€è°ƒæ•´ BPTT æˆªæ–­ä½ç½®å’Œçª—å£å¤§å°çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„éšæœºæˆªæ–­æ–¹æ³•ç›¸æ¯”ï¼ŒAT-BPTT èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç¥ç»ç½‘ç»œåœ¨ä¸åŒè®­ç»ƒé˜¶æ®µçš„å­¦ä¹ åŠ¨æ€ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è¿›è¡Œæ•°æ®é›†è’¸é¦ã€‚æ­¤å¤–ï¼Œä½ç§© Hessian è¿‘ä¼¼çš„ä½¿ç”¨æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šé˜¶æ®µæ„ŸçŸ¥æ—¶é—´æ­¥é€‰æ‹©ä½¿ç”¨ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒæ¥è¡¨ç¤ºæ¯ä¸ªæ—¶é—´æ­¥çš„é‡è¦æ€§ï¼Œè¯¥åˆ†å¸ƒæ ¹æ®è®­ç»ƒé˜¶æ®µè¿›è¡Œè°ƒæ•´ã€‚è‡ªé€‚åº”çª—å£å¤§å°è°ƒæ•´åŸºäºæ¢¯åº¦å˜åŒ–çš„ç»Ÿè®¡é‡ï¼Œä¾‹å¦‚æ–¹å·®æˆ–å‡å€¼ï¼Œæ¥åŠ¨æ€è°ƒæ•´çª—å£å¤§å°ã€‚ä½ç§© Hessian è¿‘ä¼¼ä½¿ç”¨ Lanczos ç®—æ³•æ¥ä¼°è®¡ Hessian çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AT-BPTT åœ¨ CIFAR-10ã€CIFAR-100ã€Tiny-ImageNet å’Œ ImageNet-1K æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”åŸºçº¿æ–¹æ³•æé«˜äº† 6.16%ã€‚æ­¤å¤–ï¼ŒAT-BPTT å°†å†…å¾ªç¯ä¼˜åŒ–åŠ é€Ÿäº† 3.9 å€ï¼ŒåŒæ—¶èŠ‚çœäº† 63% çš„å†…å­˜æˆæœ¬ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢éƒ½å…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºèµ„æºå—é™çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼ç³»ç»Ÿï¼Œé€šè¿‡æ•°æ®é›†è’¸é¦å‡å°‘å­˜å‚¨å’Œè®¡ç®—éœ€æ±‚ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼Œæé«˜å¼€å‘æ•ˆç‡ï¼Œå¹¶ä¸ºå¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒæä¾›äº†ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºè”é‚¦å­¦ä¹ ã€æŒç»­å­¦ä¹ ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The growing demand for efficient deep learning has positioned dataset distillation as a pivotal technique for compressing training dataset while preserving model performance. However, existing inner-loop optimization methods for dataset distillation typically rely on random truncation strategies, which lack flexibility and often yield suboptimal results. In this work, we observe that neural networks exhibit distinct learning dynamics across different training stages-early, middle, and late-making random truncation ineffective. To address this limitation, we propose Automatic Truncated Backpropagation Through Time (AT-BPTT), a novel framework that dynamically adapts both truncation positions and window sizes according to intrinsic gradient behavior. AT-BPTT introduces three key components: (1) a probabilistic mechanism for stage-aware timestep selection, (2) an adaptive window sizing strategy based on gradient variation, and (3) a low-rank Hessian approximation to reduce computational overhead. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art performance, improving accuracy by an average of 6.16% over baseline methods. Moreover, our approach accelerates inner-loop optimization by 3.9x while saving 63% memory cost.

