---
layout: default
title: Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation
---

# Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04838" target="_blank" class="toolbar-btn">arXiv: 2510.04838v1</a>
    <a href="https://arxiv.org/pdf/2510.04838.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04838v1" 
            onclick="toggleFavorite(this, '2510.04838v1', 'Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Muquan Li, Hang Gou, Dongyang Zhang, Shuang Liang, Xiurui Xie, Deqiang Ouyang, Ke Qin

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AT-BPTTÔºåÈÄöËøáËá™Âä®ÂÜÖÂæ™ÁéØ‰ºòÂåñÊèêÂçáÊï∞ÊçÆÈõÜËí∏È¶èÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êï∞ÊçÆÈõÜËí∏È¶è` `ÂÜÖÂæ™ÁéØ‰ºòÂåñ` `ÂèçÂêë‰º†Êí≠` `Ê¢ØÂ∫¶Êà™Êñ≠` `Ëá™ÈÄÇÂ∫îÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊï∞ÊçÆÈõÜËí∏È¶èÊñπÊ≥ï‰æùËµñÈöèÊú∫Êà™Êñ≠Á≠ñÁï•ÔºåÂøΩÁï•‰∫ÜÁ•ûÁªèÁΩëÁªúÂú®‰∏çÂêåËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂ≠¶‰π†Âä®ÊÄÅÂ∑ÆÂºÇÔºåÂØºËá¥ÊÄßËÉΩÂèóÈôê„ÄÇ
2. AT-BPTT Ê°ÜÊû∂ÈÄöËøáÊ¶ÇÁéáÊú∫Âà∂ÈÄâÊã©Êó∂Èó¥Ê≠•ÔºåÂπ∂Ê†πÊçÆÊ¢ØÂ∫¶ÂèòÂåñËá™ÈÄÇÂ∫îË∞ÉÊï¥Á™óÂè£Â§ßÂ∞èÔºåÂÆûÁé∞Âä®ÊÄÅÊà™Êñ≠ÂèçÂêë‰º†Êí≠„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAT-BPTT Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂáÜÁ°ÆÁéáÔºåÂπ∂Âä†ÈÄü‰∫ÜÂÜÖÂæ™ÁéØ‰ºòÂåñÔºåÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÊ∂àËÄó„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏∫‰∫ÜÂ∫îÂØπÈ´òÊïàÊ∑±Â∫¶Â≠¶‰π†Êó•ÁõäÂ¢ûÈïøÁöÑÈúÄÊ±ÇÔºåÊï∞ÊçÆÈõÜËí∏È¶èÂ∑≤Êàê‰∏∫‰∏ÄÁßçÂÖ≥ÈîÆÊäÄÊúØÔºåÂÆÉËÉΩÂ§üÂú®ÂéãÁº©ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÂêåÊó∂‰øùÊåÅÊ®°ÂûãÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÊï∞ÊçÆÈõÜËí∏È¶èÂÜÖÂæ™ÁéØ‰ºòÂåñÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÈöèÊú∫Êà™Êñ≠Á≠ñÁï•ÔºåÁº∫‰πèÁÅµÊ¥ªÊÄßÔºåÂπ∂‰∏îÂ∏∏Â∏∏‰∫ßÁîüÊ¨°‰ºòÁªìÊûú„ÄÇÊú¨ÊñáËßÇÂØüÂà∞Á•ûÁªèÁΩëÁªúÂú®‰∏çÂêåÁöÑËÆ≠ÁªÉÈò∂ÊÆµÔºàÊó©Êúü„ÄÅ‰∏≠ÊúüÂíåÊôöÊúüÔºâË°®Áé∞Âá∫‰∏çÂêåÁöÑÂ≠¶‰π†Âä®ÊÄÅÔºåËøô‰ΩøÂæóÈöèÊú∫Êà™Êñ≠ÂèòÂæóÊó†Êïà„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™Â±ÄÈôêÊÄßÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂‚Äî‚ÄîËá™Âä®Êà™Êñ≠ÂèçÂêë‰º†Êí≠Êó∂Èó¥ÔºàAT-BPTTÔºâÔºåÂÆÉÂèØ‰ª•Ê†πÊçÆÂÜÖÂú®ÁöÑÊ¢ØÂ∫¶Ë°å‰∏∫Âä®ÊÄÅÂú∞Ë∞ÉÊï¥Êà™Êñ≠‰ΩçÁΩÆÂíåÁ™óÂè£Â§ßÂ∞è„ÄÇAT-BPTT ÂºïÂÖ•‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºöÔºà1Ôºâ‰∏ÄÁßçÁî®‰∫éÈò∂ÊÆµÊÑüÁü•Êó∂Èó¥Ê≠•ÈÄâÊã©ÁöÑÊ¶ÇÁéáÊú∫Âà∂ÔºåÔºà2Ôºâ‰∏ÄÁßçÂü∫‰∫éÊ¢ØÂ∫¶ÂèòÂåñÁöÑËá™ÈÄÇÂ∫îÁ™óÂè£Â§ßÂ∞èË∞ÉÊï¥Á≠ñÁï•Ôºå‰ª•ÂèäÔºà3Ôºâ‰∏ÄÁßçÁî®‰∫éÈôç‰ΩéËÆ°ÁÆóÂºÄÈîÄÁöÑ‰ΩéÁß© Hessian Ëøë‰ºº„ÄÇÂú® CIFAR-10„ÄÅCIFAR-100„ÄÅTiny-ImageNet Âíå ImageNet-1K ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåAT-BPTT ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºå‰∏éÂü∫Á∫øÊñπÊ≥ïÁõ∏ÊØîÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü 6.16% ÁöÑÂáÜÁ°ÆÁéá„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ∞ÜÂÜÖÂæ™ÁéØ‰ºòÂåñÂä†ÈÄü‰∫Ü 3.9 ÂÄçÔºåÂêåÊó∂ËäÇÁúÅ‰∫Ü 63% ÁöÑÂÜÖÂ≠òÊàêÊú¨„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊï∞ÊçÆÈõÜËí∏È¶èÊó®Âú®Áî®ËøúÂ∞è‰∫éÂéüÂßãÊï∞ÊçÆÈõÜÁöÑÂêàÊàêÊï∞ÊçÆÈõÜËÆ≠ÁªÉÊ®°ÂûãÔºå‰ΩøÂÖ∂ËææÂà∞‰∏éÂú®ÂéüÂßãÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁõ∏‰ººÁöÑÊÄßËÉΩ„ÄÇÁé∞ÊúâÁöÑÂÜÖÂæ™ÁéØ‰ºòÂåñÊñπÊ≥ïÔºåÂ¶ÇÈöèÊú∫Êà™Êñ≠ÂèçÂêë‰º†Êí≠ÔºàBPTTÔºâÔºåÂú®Â§ÑÁêÜ‰∏çÂêåËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂ≠¶‰π†Âä®ÊÄÅÊó∂Áº∫‰πèÈÄÇÂ∫îÊÄßÔºåÂØºËá¥Ëí∏È¶èÂá∫ÁöÑÊï∞ÊçÆÈõÜË¥®Èáè‰∏çÈ´òÔºåÊ®°ÂûãÊÄßËÉΩÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊ†πÊçÆÁ•ûÁªèÁΩëÁªúÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰∏çÂêåÈò∂ÊÆµÁöÑÊ¢ØÂ∫¶Ë°å‰∏∫ÔºåÂä®ÊÄÅÂú∞Ë∞ÉÊï¥ BPTT ÁöÑÊà™Êñ≠‰ΩçÁΩÆÂíåÁ™óÂè£Â§ßÂ∞è„ÄÇÈÄöËøáËá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©Êõ¥ÈáçË¶ÅÁöÑÊó∂Èó¥Ê≠•ËøõË°åÊ¢ØÂ∫¶Êõ¥Êñ∞ÔºåÂπ∂Ë∞ÉÊï¥Á™óÂè£Â§ßÂ∞è‰ª•ÈÄÇÂ∫îÊ¢ØÂ∫¶ÂèòÂåñÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞ËøõË°åÊï∞ÊçÆÈõÜËí∏È¶è„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAT-BPTT Ê°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÁªÑ‰ª∂Ôºö1) **Èò∂ÊÆµÊÑüÁü•Êó∂Èó¥Ê≠•ÈÄâÊã©**Ôºö‰ΩøÁî®Ê¶ÇÁéáÊú∫Âà∂ÔºåÊ†πÊçÆÂΩìÂâçËÆ≠ÁªÉÈò∂ÊÆµÔºàÊó©Êúü„ÄÅ‰∏≠Êúü„ÄÅÊôöÊúüÔºâÁöÑÈáçË¶ÅÊÄßÔºåÈÄâÊã©ÂèÇ‰∏éÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑÊó∂Èó¥Ê≠•„ÄÇ2) **Ëá™ÈÄÇÂ∫îÁ™óÂè£Â§ßÂ∞èË∞ÉÊï¥**ÔºöÊ†πÊçÆÊ¢ØÂ∫¶ÂèòÂåñÂä®ÊÄÅË∞ÉÊï¥ BPTT ÁöÑÁ™óÂè£Â§ßÂ∞èÔºåÁ°Æ‰øùÈáçË¶ÅÊ¢ØÂ∫¶‰ø°ÊÅØËÉΩÂ§üË¢´ÊúâÊïà‰º†Êí≠„ÄÇ3) **‰ΩéÁß© Hessian Ëøë‰ºº**Ôºö‰ΩøÁî®‰ΩéÁß© Hessian Ëøë‰ººÊù•Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊèêÈ´ò‰ºòÂåñÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAT-BPTT ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âä®ÊÄÅË∞ÉÊï¥ BPTT Êà™Êñ≠‰ΩçÁΩÆÂíåÁ™óÂè£Â§ßÂ∞èÁöÑËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÈöèÊú∫Êà™Êñ≠ÊñπÊ≥ïÁõ∏ÊØîÔºåAT-BPTT ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁ•ûÁªèÁΩëÁªúÂú®‰∏çÂêåËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂ≠¶‰π†Âä®ÊÄÅÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞ËøõË°åÊï∞ÊçÆÈõÜËí∏È¶è„ÄÇÊ≠§Â§ñÔºå‰ΩéÁß© Hessian Ëøë‰ººÁöÑ‰ΩøÁî®ÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂºÄÈîÄ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈò∂ÊÆµÊÑüÁü•Êó∂Èó¥Ê≠•ÈÄâÊã©‰ΩøÁî®‰∏Ä‰∏™Ê¶ÇÁéáÂàÜÂ∏ÉÊù•Ë°®Á§∫ÊØè‰∏™Êó∂Èó¥Ê≠•ÁöÑÈáçË¶ÅÊÄßÔºåËØ•ÂàÜÂ∏ÉÊ†πÊçÆËÆ≠ÁªÉÈò∂ÊÆµËøõË°åË∞ÉÊï¥„ÄÇËá™ÈÄÇÂ∫îÁ™óÂè£Â§ßÂ∞èË∞ÉÊï¥Âü∫‰∫éÊ¢ØÂ∫¶ÂèòÂåñÁöÑÁªüËÆ°ÈáèÔºå‰æãÂ¶ÇÊñπÂ∑ÆÊàñÂùáÂÄºÔºåÊù•Âä®ÊÄÅË∞ÉÊï¥Á™óÂè£Â§ßÂ∞è„ÄÇ‰ΩéÁß© Hessian Ëøë‰ºº‰ΩøÁî® Lanczos ÁÆóÊ≥ïÊù•‰º∞ËÆ° Hessian Áü©ÈòµÁöÑÁâπÂæÅÂÄºÂíåÁâπÂæÅÂêëÈáèÔºå‰ªéËÄåÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

AT-BPTT Âú® CIFAR-10„ÄÅCIFAR-100„ÄÅTiny-ImageNet Âíå ImageNet-1K Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ≥ÂùáÂáÜÁ°ÆÁéáÊØîÂü∫Á∫øÊñπÊ≥ïÊèêÈ´ò‰∫Ü 6.16%„ÄÇÊ≠§Â§ñÔºåAT-BPTT Â∞ÜÂÜÖÂæ™ÁéØ‰ºòÂåñÂä†ÈÄü‰∫Ü 3.9 ÂÄçÔºåÂêåÊó∂ËäÇÁúÅ‰∫Ü 63% ÁöÑÂÜÖÂ≠òÊàêÊú¨ÔºåË°®ÊòéËØ•ÊñπÊ≥ïÂú®ÊïàÁéáÂíåÊÄßËÉΩÊñπÈù¢ÈÉΩÂÖ∑Êúâ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§áÊàñÂµåÂÖ•ÂºèÁ≥ªÁªüÔºåÈÄöËøáÊï∞ÊçÆÈõÜËí∏È¶èÂáèÂ∞ëÂ≠òÂÇ®ÂíåËÆ°ÁÆóÈúÄÊ±Ç„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Âä†ÈÄüÊ®°ÂûãËÆ≠ÁªÉÔºåÊèêÈ´òÂºÄÂèëÊïàÁéáÔºåÂπ∂‰∏∫Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØË°åÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éËÅîÈÇ¶Â≠¶‰π†„ÄÅÊåÅÁª≠Â≠¶‰π†Á≠âÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The growing demand for efficient deep learning has positioned dataset distillation as a pivotal technique for compressing training dataset while preserving model performance. However, existing inner-loop optimization methods for dataset distillation typically rely on random truncation strategies, which lack flexibility and often yield suboptimal results. In this work, we observe that neural networks exhibit distinct learning dynamics across different training stages-early, middle, and late-making random truncation ineffective. To address this limitation, we propose Automatic Truncated Backpropagation Through Time (AT-BPTT), a novel framework that dynamically adapts both truncation positions and window sizes according to intrinsic gradient behavior. AT-BPTT introduces three key components: (1) a probabilistic mechanism for stage-aware timestep selection, (2) an adaptive window sizing strategy based on gradient variation, and (3) a low-rank Hessian approximation to reduce computational overhead. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art performance, improving accuracy by an average of 6.16% over baseline methods. Moreover, our approach accelerates inner-loop optimization by 3.9x while saving 63% memory cost.

