---
layout: default
title: Visual Representations inside the Language Model
---

# Visual Representations inside the Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04819" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.04819v1</a>
  <a href="https://arxiv.org/pdf/2510.04819.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04819v1" onclick="toggleFavorite(this, '2510.04819v1', 'Visual Representations inside the Language Model')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Benlin Liu, Amita Kamath, Madeleine Grunde-McLaughlin, Winson Han, Ranjay Krishna

**ÂàÜÁ±ª**: cs.CV, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-06

**Â§áÊ≥®**: Accepted to COLM 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÂàÜÊûêÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂÜÖÈÉ®ËßÜËßâË°®ÂæÅÔºåÊè≠Á§∫ÂÖ∂ÊÑüÁü•ËÉΩÂäõÁì∂È¢à‰∏éÊîπËøõÊñπÂêë**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ËßÜËßâË°®ÂæÅ` `ÈîÆÂÄºtokens` `ÊÑüÁü•ËÉΩÂäõ` `ÂèØËß£ÈáäÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÊÑüÁü•‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÁé∞ÊúâÁ†îÁ©∂Áº∫‰πèÂØπÂÖ∂ÂÜÖÈÉ®ËßÜËßâË°®ÂæÅÁöÑÊ∑±ÂÖ•ÁêÜËß£„ÄÇ
2. ÈÄöËøáÂàÜÊûêËßÜËßâÈîÆÂÄºtokensÂú®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊµÅÂä®ÔºåÊè≠Á§∫ËßÜËßâ‰ø°ÊÅØÂú®Ê®°ÂûãÂÜÖÈÉ®ÁöÑÂ§ÑÁêÜÊñπÂºèÂíåÊΩúÂú®Áì∂È¢à„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂõæÂÉèÂÄºtokensÂåÖÂê´Ë∂≥Â§üÁöÑÊÑüÁü•‰ø°ÊÅØÔºå‰ΩÜËØ≠Ë®ÄÊ®°ÂûãÂØπËßÜËßâ‰ø°ÊÅØÁöÑÂ§ÑÁêÜÂ≠òÂú®ÊîπËøõÁ©∫Èó¥Ôºå‰æãÂ¶ÇÈÄöËøáÊñáÊú¨ÂâçÁºÄÊéßÂà∂ËßÜËßâ‰ø°ÊÅØ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∞ΩÁÆ°Â∑≤ÊúâÂ§ßÈáèÂ∑•‰ΩúÂàÜÊûêËßÜËßâTransformerÁºñÁ†ÅÂô®ÂíåTransformerÊøÄÊ¥ªÔºåÊàë‰ª¨‰ªçÁÑ∂‰∏çÊ∏ÖÊ•öÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLM)Âú®ÊÑüÁü•ÂØÜÈõÜÂûã‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÁöÑÂéüÂõ†„ÄÇÊú¨ÊñáÈÄöËøáÁ†îÁ©∂‰∏ªÊµÅMLM(LLaVA-OneVision, Qwen2.5-VL, Llama-3-LLaVA-NeXT)Â¶Ç‰ΩïÂ§ÑÁêÜÂÖ∂ËßÜËßâÈîÆÂÄº(key-value)tokensÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËßÜËßí„ÄÇÊàë‰ª¨È¶ñÂÖàÁ†îÁ©∂ËßÜËßâ‰ø°ÊÅØÂú®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊµÅÂä®ÔºåÂèëÁé∞ÂõæÂÉèÂÄºtokensÁºñÁ†Å‰∫ÜË∂≥Â§üÁöÑ‰ø°ÊÅØÊù•Èõ∂Ê†∑Êú¨ÊâßË°åÂ§ö‰∏™ÊÑüÁü•ÂØÜÈõÜÂûã‰ªªÂä°ÔºöÂàÜÂâ≤„ÄÅËØ≠‰πâÂØπÂ∫î„ÄÅÊó∂Èó¥ÂØπÂ∫îÂíåÊåá‰ª£Ë°®ËææÂºèÊ£ÄÊµã„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåËôΩÁÑ∂ËØ≠Ë®ÄÊ®°ÂûãÁ°ÆÂÆûÂ¢ûÂº∫‰∫ÜÊù•Ëá™ËæìÂÖ•ËßÜËßâÁºñÁ†ÅÊäïÂΩ±ÁöÑËßÜËßâ‰ø°ÊÅØÔºàÊàë‰ª¨Êè≠Á§∫Ëøô‰∏éMLMÁöÑÊï¥‰ΩìÊÑüÁü•ËÉΩÂäõÁõ∏ÂÖ≥ÔºâÔºå‰ΩÜÂÆÉÂú®Âá†‰∏™‰ªªÂä°‰∏äÂåÖÂê´ÁöÑËßÜËßâ‰ø°ÊÅØÂ∞ë‰∫éÊú™ÁªèMLMÂæÆË∞ÉÁöÑÁ≠âÊïàËßÜËßâÁºñÁ†ÅÂô®(SigLIP)„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂèëÁé∞ËØ≠Ë®ÄÊ®°ÂûãÂêéÊúüÂ±Ç‰∏≠‰∏éËæìÂÖ•Êó†ÂÖ≥ÁöÑÂõæÂÉèÈîÆtokensÂØπÂ∫îÁöÑËßÜËßâ‰ø°ÊÅØÂåÖÂê´Èôç‰ΩéÊï¥‰ΩìMLMÊÑüÁü•ËÉΩÂäõÁöÑ‰∫∫Â∑•ÁóïËøπ„ÄÇÊé•‰∏ãÊù•ÔºåÊàë‰ª¨ËÆ®ËÆ∫ÊéßÂà∂ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑËßÜËßâ‰ø°ÊÅØÔºåË°®ÊòéÂêëÂõæÂÉèËæìÂÖ•Ê∑ªÂä†ÊñáÊú¨ÂâçÁºÄÂèØ‰ª•ÊèêÈ´òËßÜËßâË°®ÂæÅÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Êè≠Á§∫ÔºåÂ¶ÇÊûúËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊéßÂà∂ÂÖ∂ËßÜËßâ‰ø°ÊÅØÔºåÂÆÉ‰ª¨ÁöÑÊÑüÁü•ËÉΩÂäõÂ∞ÜÊòæËëóÊèêÈ´òÔºõ‰æãÂ¶ÇÔºåÂú®BLINKÂü∫ÂáÜÊµãËØï‰∏≠33.3%ÁöÑËâ∫ÊúØÈ£éÊ†ºÈóÆÈ¢ò‰∏≠ÔºåËØ≠Ë®ÄÊ®°Âûã‰∏≠Â≠òÂú®ÁöÑÊÑüÁü•‰ø°ÊÅØÊ≤°Êúâ‰º†ÈÄíÂà∞ËæìÂá∫ÔºÅÊàë‰ª¨ÁöÑÂèëÁé∞Êè≠Á§∫‰∫ÜÈîÆÂÄºtokensÂú®Â§öÊ®°ÊÄÅÁ≥ªÁªü‰∏≠ÁöÑ‰ΩúÁî®Ôºå‰∏∫MLMÁöÑÊõ¥Ê∑±Â±ÇÊ¨°ÁöÑÊú∫Âà∂ÂèØËß£ÈáäÊÄßÈì∫Âπ≥‰∫ÜÈÅìË∑ØÔºåÂπ∂‰∏∫ËÆ≠ÁªÉÂÖ∂ËßÜËßâÁºñÁ†ÅÂô®ÂíåËØ≠Ë®ÄÊ®°ÂûãÁªÑ‰ª∂ÊèêÂá∫‰∫ÜÊñ∞ÁöÑÊñπÂêë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLM)Âú®Â§ÑÁêÜÊÑüÁü•ÂØÜÈõÜÂûã‰ªªÂä°Êó∂Ë°®Áé∞‰∏ç‰Ω≥Ôºå‰æãÂ¶ÇÂõæÂÉèÂàÜÂâ≤„ÄÅËØ≠‰πâÂØπÂ∫îÁ≠â„ÄÇÁé∞ÊúâÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËßÜËßâÁºñÁ†ÅÂô®ÂíåTransformerÊøÄÊ¥ªÁöÑÂàÜÊûêÔºåÁº∫‰πèÂØπËØ≠Ë®ÄÊ®°ÂûãÂÜÖÈÉ®Â¶Ç‰ΩïÂ§ÑÁêÜËßÜËßâ‰ø°ÊÅØÁöÑÊ∑±ÂÖ•ÁêÜËß£ÔºåÁâπÂà´ÊòØËßÜËßâÈîÆÂÄº(key-value)tokensÁöÑ‰ΩúÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂàÜÊûêMLMÂÜÖÈÉ®ËßÜËßâÈîÆÂÄºtokensÊâÄÂåÖÂê´ÁöÑËßÜËßâ‰ø°ÊÅØÔºåÊù•ÁêÜËß£MLMÂú®ÊÑüÁü•‰ªªÂä°‰∏äÁöÑÁì∂È¢à„ÄÇÈÄöËøáÊØîËæÉMLMÂÜÖÈÉ®ËßÜËßâË°®ÂæÅ‰∏éÂéüÂßãËßÜËßâÁºñÁ†ÅÂô®ÁöÑË°®ÂæÅÔºå‰ª•ÂèäÁ†îÁ©∂‰∏çÂêåÂ±ÇÁ∫ßÁöÑÈîÆÂÄºtokensÔºåÊè≠Á§∫ËßÜËßâ‰ø°ÊÅØÂú®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊµÅÂä®ÂíåÂèòÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊú¨ÊñáÁöÑÁ†îÁ©∂Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö1) ÈÄâÊã©‰∏ªÊµÅÁöÑMLMÊ®°ÂûãÔºåÂ¶ÇLLaVA-OneVision, Qwen2.5-VL, Llama-3-LLaVA-NeXTÔºõ2) ÂàÜÊûêËßÜËßâÈîÆÂÄºtokensÂú®ËØ≠Ë®ÄÊ®°Âûã‰∏çÂêåÂ±ÇÁ∫ßÁöÑÊøÄÊ¥ªÔºõ3) ‰ΩøÁî®Èõ∂Ê†∑Êú¨ÊñπÂºèÂú®ÊÑüÁü•‰ªªÂä°‰∏äËØÑ‰º∞ËßÜËßâË°®ÂæÅÁöÑÊÄßËÉΩÔºåÂåÖÊã¨ÂàÜÂâ≤„ÄÅËØ≠‰πâÂØπÂ∫î„ÄÅÊó∂Èó¥ÂØπÂ∫îÂíåÊåá‰ª£Ë°®ËææÂºèÊ£ÄÊµãÔºõ4) ÊØîËæÉMLMÂÜÖÈÉ®ËßÜËßâË°®ÂæÅ‰∏éÂéüÂßãËßÜËßâÁºñÁ†ÅÂô®(SigLIP)ÁöÑË°®ÂæÅÔºõ5) Á†îÁ©∂ÊñáÊú¨ÂâçÁºÄÂØπËßÜËßâË°®ÂæÅÁöÑÂΩ±Âìç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßíÊù•ÁêÜËß£MLMÁöÑÊÑüÁü•ËÉΩÂäõÔºåÂç≥ÈÄöËøáÂàÜÊûêËØ≠Ë®ÄÊ®°ÂûãÂÜÖÈÉ®ÁöÑËßÜËßâË°®ÂæÅ„ÄÇ‰∏é‰ª•ÂæÄ‰∏ªË¶ÅÂÖ≥Ê≥®ËßÜËßâÁºñÁ†ÅÂô®ÂíåTransformerÊøÄÊ¥ªÁöÑÁ†îÁ©∂‰∏çÂêåÔºåÊú¨ÊñáÊ∑±ÂÖ•Á†îÁ©∂‰∫ÜËßÜËßâÈîÆÂÄºtokensÂú®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑ‰ΩúÁî®ÔºåÊè≠Á§∫‰∫ÜËßÜËßâ‰ø°ÊÅØÂú®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊµÅÂä®ÂíåÂèòÂåñÔºå‰ª•ÂèäÊΩúÂú®ÁöÑÁì∂È¢à„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊú¨ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÈÄâÊã©ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑMLMÊ®°ÂûãÔºõ2) ‰ΩøÁî®Èõ∂Ê†∑Êú¨ÊñπÂºèËØÑ‰º∞ËßÜËßâË°®ÂæÅÁöÑÊÄßËÉΩÔºåÈÅøÂÖç‰∫ÜÂæÆË∞ÉÂ∏¶Êù•ÁöÑÂÅèÂ∑ÆÔºõ3) ÊØîËæÉMLMÂÜÖÈÉ®ËßÜËßâË°®ÂæÅ‰∏éÂéüÂßãËßÜËßâÁºñÁ†ÅÂô®ÁöÑË°®ÂæÅÔºåÂèØ‰ª•Êõ¥Ê∏ÖÊô∞Âú∞‰∫ÜËß£ËØ≠Ë®ÄÊ®°ÂûãÂØπËßÜËßâ‰ø°ÊÅØÁöÑÂΩ±ÂìçÔºõ4) Á†îÁ©∂ÊñáÊú¨ÂâçÁºÄÂØπËßÜËßâË°®ÂæÅÁöÑÂΩ±ÂìçÔºåÊé¢Á¥¢ÊéßÂà∂ËßÜËßâ‰ø°ÊÅØÁöÑÊñπÊ≥ï„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Á†îÁ©∂ÂèëÁé∞ÔºåÂõæÂÉèÂÄºtokensÂåÖÂê´Ë∂≥Â§üÁöÑÊÑüÁü•‰ø°ÊÅØÔºåÂèØ‰ª•Èõ∂Ê†∑Êú¨ÊâßË°åÂàÜÂâ≤„ÄÅËØ≠‰πâÂØπÂ∫îÁ≠â‰ªªÂä°„ÄÇÂêåÊó∂ÔºåËØ≠Ë®ÄÊ®°ÂûãÂØπËßÜËßâ‰ø°ÊÅØÁöÑÂ§ÑÁêÜÂ≠òÂú®ÊîπËøõÁ©∫Èó¥Ôºå‰æãÂ¶ÇÊ∑ªÂä†ÊñáÊú¨ÂâçÁºÄÂèØ‰ª•ÊèêÈ´òËßÜËßâË°®ÂæÅÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇÂú®BLINKÂü∫ÂáÜÊµãËØï‰∏≠Ôºå33.3%ÁöÑËâ∫ÊúØÈ£éÊ†ºÈóÆÈ¢ò‰∏≠ÔºåËØ≠Ë®ÄÊ®°Âûã‰∏≠Â≠òÂú®ÁöÑÊÑüÁü•‰ø°ÊÅØÊ≤°Êúâ‰º†ÈÄíÂà∞ËæìÂá∫ÔºåË°®ÊòéÊ®°ÂûãÂØπËßÜËßâ‰ø°ÊÅØÁöÑÊéßÂà∂ËÉΩÂäõÊúâÂæÖÊèêÈ´ò„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜËßâÊÑüÁü•‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÔºå‰æãÂ¶ÇÂõæÂÉèÁêÜËß£„ÄÅËßÜÈ¢ëÂàÜÊûê„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠â„ÄÇÈÄöËøáÊõ¥Â•ΩÂú∞ÊéßÂà∂ÂíåÂà©Áî®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑËßÜËßâ‰ø°ÊÅØÔºåÂèØ‰ª•ÂºÄÂèëÂá∫Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÂèØÈù†ÁöÑÂ§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÔºåÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅÂåªÁñóËØäÊñ≠Á≠âÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Despite interpretability work analyzing VIT encoders and transformer activations, we don't yet understand why Multimodal Language Models (MLMs) struggle on perception-heavy tasks. We offer an under-studied perspective by examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the flow of visual information through the language model, finding that image value tokens encode sufficient information to perform several perception-heavy tasks zero-shot: segmentation, semantic correspondence, temporal correspondence, and referring expression detection. We find that while the language model does augment the visual information received from the projection of input visual encodings-which we reveal correlates with overall MLM perception capability-it contains less visual information on several tasks than the equivalent visual encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that the visual information corresponding to input-agnostic image key tokens in later layers of language models contains artifacts which reduce perception capability of the overall MLM. Next, we discuss controlling visual information in the language model, showing that adding a text prefix to the image input improves perception capabilities of visual representations. Finally, we reveal that if language models were able to better control their visual information, their perception would significantly improve; e.g., in 33.3% of Art Style questions in the BLINK benchmark, perception information present in the language model is not surfaced to the output! Our findings reveal insights into the role of key-value tokens in multimodal systems, paving the way for deeper mechanistic interpretability of MLMs and suggesting new directions for training their visual encoder and language model components.

