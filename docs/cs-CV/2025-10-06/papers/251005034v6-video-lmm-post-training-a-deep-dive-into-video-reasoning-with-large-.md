---
layout: default
title: Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models
---

# Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.05034" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.05034v6</a>
  <a href="https://arxiv.org/pdf/2510.05034.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05034v6" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.05034v6', 'Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yolo Y. Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Junhua Huang, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-06 (æ›´æ–°: 2025-11-25)

**å¤‡æ³¨**: Version v1.1

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yunlong10/Awesome-Video-LMM-Post-Training)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å…¨é¢å‰–æè§†é¢‘å¤§æ¨¡å‹åè®­ç»ƒæ–¹æ³•ï¼Œæå‡è§†é¢‘æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¤§æ¨¡å‹` `åè®­ç»ƒ` `ç›‘ç£å¾®è°ƒ` `å¼ºåŒ–å­¦ä¹ ` `æµ‹è¯•æ—¶ç¼©æ”¾` `è§†é¢‘ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å¤§æ¨¡å‹åè®­ç»ƒæ–¹æ³•åˆ†æ•£ï¼Œç¼ºä¹ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œé˜»ç¢äº†æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æå‡ã€‚
2. æœ¬æ–‡å…¨é¢è€ƒå¯Ÿäº†è§†é¢‘å¤§æ¨¡å‹çš„åè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶ç¼©æ”¾ä¸‰å¤§æ”¯æŸ±ã€‚
3. é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæ€»ç»“äº†å…³é”®è®¾è®¡åŸåˆ™å’Œè¯„ä¼°åè®®ï¼Œå¹¶æŒ‡å‡ºäº†å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘ç†è§£æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€å…·æŒ‘æˆ˜æ€§çš„å‰æ²¿æ–¹å‘ï¼Œå®ƒè¦æ±‚æ¨¡å‹èƒ½å¤Ÿæ¨ç†å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€é•¿æœŸä¾èµ–å’Œå¤šæ¨¡æ€è¯æ®ã€‚è¿‘å¹´æ¥ï¼Œæ¶Œç°å‡ºçš„è§†é¢‘å¤§æ¨¡å‹ï¼ˆVideo-LMMsï¼‰é€šè¿‡å°†è§†è§‰ç¼–ç å™¨ä¸å¼ºå¤§çš„åŸºäºè§£ç å™¨çš„è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ä»åŸºæœ¬çš„æ„ŸçŸ¥ç³»ç»Ÿè½¬å˜ä¸ºå¤æ‚çš„æ¨ç†å¼•æ“çš„å…³é”®é˜¶æ®µâ€”â€”åè®­ç»ƒï¼Œåœ¨æ–‡çŒ®ä¸­ä»ç„¶åˆ†æ•£ã€‚æœ¬ç»¼è¿°é¦–æ¬¡å…¨é¢è€ƒå¯Ÿäº†Video-LMMsçš„åè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ï¼šå¸¦æœ‰æ€ç»´é“¾çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€åŸºäºå¯éªŒè¯ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥åŠé€šè¿‡å¢å¼ºæ¨ç†è®¡ç®—å®ç°çš„æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼Œé˜æ˜äº†è¿™äº›æŠ€æœ¯çš„ä½œç”¨ã€ç›¸äº’è”ç³»å’Œç‰¹å®šäºè§†é¢‘çš„è°ƒæ•´ï¼Œè§£å†³äº†è¯¸å¦‚æ—¶é—´å®šä½ã€æ—¶ç©º groundingã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®æ•´åˆç­‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬ç»¼åˆäº†å…³é”®çš„è®¾è®¡åŸåˆ™ã€è§è§£å’Œè¯„ä¼°åè®®ï¼ŒåŒæ—¶è¯†åˆ«äº†å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬-æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ•´ç†äº†å¿…è¦çš„åŸºå‡†ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä»¥ä¿ƒè¿›å¯¹åè®­ç»ƒæœ‰æ•ˆæ€§çš„ä¸¥æ ¼è¯„ä¼°ã€‚æœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šäººå‘˜æä¾›ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä»¥æå‡Video-LMMçš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘ç†è§£ä»»åŠ¡éœ€è¦æ¨¡å‹å…·å¤‡å¤æ‚çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œå¤„ç†é•¿æœŸä¾èµ–å…³ç³»å’Œå¤šæ¨¡æ€ä¿¡æ¯ã€‚ç°æœ‰çš„Video-LMMsè™½ç„¶åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åè®­ç»ƒæ–¹æ³•åˆ†æ•£ï¼Œç¼ºä¹ç³»ç»Ÿæ€§çš„ç ”ç©¶ï¼Œéš¾ä»¥å……åˆ†æŒ–æ˜æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚ç°æœ‰æ–¹æ³•åœ¨æ—¶é—´å®šä½ã€æ—¶ç©º groundingã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®æ•´åˆç­‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¯¹Video-LMMsçš„åè®­ç»ƒæ–¹æ³•è¿›è¡Œç³»ç»Ÿæ€§çš„æ¢³ç†å’Œåˆ†æï¼Œå°†å…¶å½’çº³ä¸ºä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ã€‚é€šè¿‡å¯¹è¿™ä¸‰ä¸ªæ”¯æŸ±çš„æ·±å…¥ç ”ç©¶ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šäººå‘˜æä¾›ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä»¥æå‡Video-LMMçš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼Œç”¨äºé˜æ˜SFTã€RLå’ŒTTSè¿™ä¸‰ç§åè®­ç»ƒæŠ€æœ¯åœ¨Video-LMMsä¸­çš„ä½œç”¨ã€ç›¸äº’è”ç³»å’Œç‰¹å®šäºè§†é¢‘çš„è°ƒæ•´ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†æ—¶é—´å®šä½ã€æ—¶ç©º groundingã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®æ•´åˆç­‰å…³é”®æ–¹é¢ã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„åˆ†æï¼Œæ€»ç»“äº†å…³é”®çš„è®¾è®¡åŸåˆ™ã€è§è§£å’Œè¯„ä¼°åè®®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¯¹Video-LMMsåè®­ç»ƒæ–¹æ³•çš„ç³»ç»Ÿæ€§ç ”ç©¶å’Œåˆ†ç±»ã€‚é¦–æ¬¡å…¨é¢è€ƒå¯Ÿäº†SFTã€RLå’ŒTTSè¿™ä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼Œé˜æ˜äº†è¿™äº›æŠ€æœ¯åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„ä½œç”¨å’Œç›¸äº’å…³ç³»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯†åˆ«äº†å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬-æ€§èƒ½ä¼˜åŒ–ç­‰å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚

**å…³é”®è®¾è®¡**ï¼šæœ¬æ–‡å¯¹SFTã€RLå’ŒTTSè¿™ä¸‰ç§åè®­ç»ƒæ–¹æ³•è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶æ€»ç»“äº†å…³é”®çš„è®¾è®¡åŸåˆ™ã€‚ä¾‹å¦‚ï¼Œåœ¨SFTä¸­ï¼Œå¼ºè°ƒä½¿ç”¨æ€ç»´é“¾ï¼ˆchain-of-thoughtï¼‰æ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼›åœ¨RLä¸­ï¼Œå¼ºè°ƒè®¾è®¡å¯éªŒè¯çš„ç›®æ ‡æ¥æŒ‡å¯¼æ¨¡å‹çš„å­¦ä¹ ï¼›åœ¨TTSä¸­ï¼Œå¼ºè°ƒé€šè¿‡å¢å¼ºæ¨ç†è®¡ç®—æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ•´ç†äº†å¿…è¦çš„åŸºå‡†ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä»¥ä¿ƒè¿›å¯¹åè®­ç»ƒæœ‰æ•ˆæ€§çš„ä¸¥æ ¼è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

æœ¬æ–‡å¯¹Video-LMMsçš„åè®­ç»ƒæ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ³•ã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„åˆ†æï¼Œæ€»ç»“äº†å…³é”®çš„è®¾è®¡åŸåˆ™ã€è§è§£å’Œè¯„ä¼°åè®®ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯†åˆ«äº†å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬-æ€§èƒ½ä¼˜åŒ–ç­‰å…³é”®å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘æœç´¢ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†é¢‘å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´ç²¾å‡†çš„äº‹ä»¶æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ã€åœºæ™¯ç†è§£ç­‰åŠŸèƒ½ï¼Œä»è€Œæé«˜ç›¸å…³åº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

