---
layout: default
title: SegMASt3R: Geometry Grounded Segment Matching
---

# SegMASt3R: Geometry Grounded Segment Matching

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.05051" target="_blank" class="toolbar-btn">arXiv: 2510.05051v2</a>
    <a href="https://arxiv.org/pdf/2510.05051.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05051v2" 
            onclick="toggleFavorite(this, '2510.05051v2', 'SegMASt3R: Geometry Grounded Segment Matching')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Rohit Jayanti, Swayam Agrawal, Vansh Garg, Siddharth Tourani, Muhammad Haris Khan, Sourav Garg, Madhava Krishna

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-06 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: Accepted to The 39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025) as a Spotlight (top 3.5%)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SegMASt3Rï¼šåˆ©ç”¨3DåŸºç¡€æ¨¡å‹å®ç°å‡ ä½•æ„ŸçŸ¥çš„å›¾åƒåˆ†å‰²åŒ¹é…**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆ†å‰²åŒ¹é…` `ä¸‰ç»´é‡å»º` `å®½åŸºçº¿` `å‡ ä½•æ„ŸçŸ¥` `æ·±åº¦å­¦ä¹ ` `3DåŸºç¡€æ¨¡å‹` `å›¾åƒåŒ¹é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æç«¯è§†è§’å˜åŒ–ä¸‹éš¾ä»¥å‡†ç¡®åŒ¹é…å›¾åƒåˆ†å‰²ï¼Œå°¤å…¶æ˜¯åœ¨å®½åŸºçº¿åœºæ™¯ä¸­ã€‚
2. åˆ©ç”¨3DåŸºç¡€æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå­¦ä¹ å›¾åƒåˆ†å‰²ä¹‹é—´çš„å‡ ä½•å¯¹åº”å…³ç³»ï¼Œæå‡åŒ¹é…çš„é²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ScanNet++å’ŒReplicaæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨3DåŸºç¡€æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›æ¥è§£å†³å®½åŸºçº¿åˆ†å‰²åŒ¹é…é—®é¢˜çš„æ–¹æ³•ã€‚å®½åŸºçº¿åˆ†å‰²åŒ¹é…æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå®ƒéœ€è¦åœ¨å…·æœ‰æç«¯è§†è§’å˜åŒ–çš„å›¾åƒå¯¹ä¹‹é—´å»ºç«‹è¯­ä¹‰æˆ–å‡ ä½•ä¸€è‡´åŒºåŸŸçš„å¯¹åº”å…³ç³»ã€‚ä¸ä¾§é‡äºå±€éƒ¨ç‰¹å¾çš„å…³é”®ç‚¹åŒ¹é…ä¸åŒï¼Œåˆ†å‰²åŒ¹é…æ•è·çš„æ˜¯ç»“æ„åŒ–åŒºåŸŸï¼Œå› æ­¤å¯¹äºé®æŒ¡ã€å…‰ç…§å˜åŒ–å’Œè§†è§’å˜åŒ–å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚è¯¥æ¶æ„åˆ©ç”¨3DåŸºç¡€æ¨¡å‹çš„å½’çº³åç½®ï¼Œèƒ½å¤ŸåŒ¹é…è§†è§’å˜åŒ–é«˜è¾¾180åº¦çš„å›¾åƒå¯¹ä¸­çš„åˆ†å‰²ã€‚åœ¨ScanNet++å’ŒReplicaæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬SAM2è§†é¢‘ä¼ æ’­å™¨å’Œå±€éƒ¨ç‰¹å¾åŒ¹é…æ–¹æ³•ï¼Œåœ¨AUPRCæŒ‡æ ‡ä¸Šæå‡é«˜è¾¾30%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨3Då®ä¾‹æ˜ å°„å’Œç‰©ä½“ç›¸å¯¹å¯¼èˆªç­‰ç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å®½åŸºçº¿åœºæ™¯ä¸‹çš„å›¾åƒåˆ†å‰²åŒ¹é…é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºå±€éƒ¨ç‰¹å¾çš„æ–¹æ³•ï¼Œåœ¨è§†è§’å˜åŒ–è¾ƒå¤§æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚å…³é”®ç‚¹åŒ¹é…ä¾§é‡äºå±€éƒ¨ç‰¹å¾ï¼Œéš¾ä»¥æ•æ‰å…¨å±€çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¯¹é®æŒ¡å’Œå…‰ç…§å˜åŒ–æ•æ„Ÿã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿåˆ©ç”¨å›¾åƒçš„å‡ ä½•ä¿¡æ¯ï¼Œåœ¨æç«¯è§†è§’å˜åŒ–ä¸‹ä¹Ÿèƒ½é²æ£’åŒ¹é…åˆ†å‰²åŒºåŸŸçš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨3DåŸºç¡€æ¨¡å‹æä¾›çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå°†å›¾åƒåˆ†å‰²åŒ¹é…é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªå‡ ä½•æ¨ç†é—®é¢˜ã€‚é€šè¿‡3DåŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥æå–å›¾åƒä¸­åˆ†å‰²åŒºåŸŸçš„3Då‡ ä½•ä¿¡æ¯ï¼Œä»è€Œå»ºç«‹ä¸åŒè§†è§’ä¸‹åˆ†å‰²åŒºåŸŸä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹è§†è§’å˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæé«˜åŒ¹é…çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åˆ†å‰²æå–ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹ï¼ˆå¦‚SAMï¼‰æå–å›¾åƒä¸­çš„åˆ†å‰²åŒºåŸŸã€‚2) 3Då‡ ä½•ä¿¡æ¯æå–ï¼šåˆ©ç”¨3DåŸºç¡€æ¨¡å‹ï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰æå–æ¯ä¸ªåˆ†å‰²åŒºåŸŸçš„3Då‡ ä½•ä¿¡æ¯ï¼Œä¾‹å¦‚æ·±åº¦ã€æ³•å‘é‡ç­‰ã€‚3) ç‰¹å¾åŒ¹é…ï¼šåŸºäºæå–çš„3Då‡ ä½•ä¿¡æ¯ï¼Œè®¡ç®—åˆ†å‰²åŒºåŸŸä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå¹¶å»ºç«‹å¯¹åº”å…³ç³»ã€‚4) åŒ¹é…ä¼˜åŒ–ï¼šä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼‰å¯¹åˆå§‹åŒ¹é…ç»“æœè¿›è¡Œä¼˜åŒ–ï¼Œæé«˜åŒ¹é…çš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†3DåŸºç¡€æ¨¡å‹å¼•å…¥åˆ°å›¾åƒåˆ†å‰²åŒ¹é…ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„ç©ºé—´ç†è§£èƒ½åŠ›æ¥è§£å†³å®½åŸºçº¿åœºæ™¯ä¸‹çš„åŒ¹é…éš¾é¢˜ã€‚ä¸ä¼ ç»Ÿçš„åŸºäº2Dç‰¹å¾çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å›¾åƒçš„å‡ ä½•ä¿¡æ¯ï¼Œæé«˜åŒ¹é…çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨3DåŸºç¡€æ¨¡å‹æå–åˆ†å‰²åŒºåŸŸçš„å‡ ä½•ä¿¡æ¯ã€‚2) å¦‚ä½•è®¾è®¡åˆé€‚çš„ç›¸ä¼¼åº¦åº¦é‡å‡½æ•°ï¼Œä»¥å‡†ç¡®è¯„ä¼°åˆ†å‰²åŒºåŸŸä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚3) å¦‚ä½•è®¾è®¡ä¼˜åŒ–ç®—æ³•ï¼Œä»¥æé«˜åŒ¹é…çš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨ScanNet++å’ŒReplicaæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AUPRCæŒ‡æ ‡ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬SAM2è§†é¢‘ä¼ æ’­å™¨å’Œå±€éƒ¨ç‰¹å¾åŒ¹é…æ–¹æ³•ï¼Œæå‡é«˜è¾¾30%ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å®½åŸºçº¿åˆ†å‰²åŒ¹é…ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨3Då®ä¾‹æ˜ å°„å’Œç‰©ä½“ç›¸å¯¹å¯¼èˆªç­‰ç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºä¼˜åŠ¿ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¸‰ç»´é‡å»ºã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‰ç»´é‡å»ºä¸­ï¼Œå¯ä»¥åˆ©ç”¨åˆ†å‰²åŒ¹é…æ¥æé«˜é‡å»ºçš„ç²¾åº¦å’Œå®Œæ•´æ€§ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥åˆ©ç”¨åˆ†å‰²åŒ¹é…æ¥å®ç°ç‰©ä½“çº§åˆ«çš„å®šä½å’Œè¯†åˆ«ã€‚åœ¨å¢å¼ºç°å®ä¸­ï¼Œå¯ä»¥åˆ©ç”¨åˆ†å‰²åŒ¹é…æ¥å®ç°è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯çš„ç²¾ç¡®å¯¹é½ã€‚è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Segment matching is an important intermediate task in computer vision that establishes correspondences between semantically or geometrically coherent regions across images. Unlike keypoint matching, which focuses on localized features, segment matching captures structured regions, offering greater robustness to occlusions, lighting variations, and viewpoint changes. In this paper, we leverage the spatial understanding of 3D foundation models to tackle wide-baseline segment matching, a challenging setting involving extreme viewpoint shifts. We propose an architecture that uses the inductive bias of these 3D foundation models to match segments across image pairs with up to 180 degree view-point change rotation. Extensive experiments show that our approach outperforms state-of-the-art methods, including the SAM2 video propagator and local feature matching methods, by up to 30% on the AUPRC metric, on ScanNet++ and Replica datasets. We further demonstrate benefits of the proposed model on relevant downstream tasks, including 3D instance mapping and object-relative navigation. Project Page: https://segmast3r.github.io/

