---
layout: default
title: ActiveMark: on watermarking of visual foundation models via massive activations
---

# ActiveMark: on watermarking of visual foundation models via massive activations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04966" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.04966v1</a>
  <a href="https://arxiv.org/pdf/2510.04966.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04966v1" onclick="toggleFavorite(this, '2510.04966v1', 'ActiveMark: on watermarking of visual foundation models via massive activations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anna Chistyakova, Mikhail Pautov

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºActiveMarkä»¥è§£å†³è§†è§‰åŸºç¡€æ¨¡å‹çš„æ°´å°ä¿æŠ¤é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰åŸºç¡€æ¨¡å‹` `æ°´å°æŠ€æœ¯` `çŸ¥è¯†äº§æƒä¿æŠ¤` `æ‰€æœ‰æƒéªŒè¯` `è®¡ç®—æœºè§†è§‰` `æ¨¡å‹å¾®è°ƒ` `æ•°å­—æ°´å°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹åœ¨çŸ¥è¯†äº§æƒä¿æŠ¤æ–¹é¢å­˜åœ¨æ¼æ´ï¼Œå®¹æ˜“è¢«ä¸æ³•ç”¨æˆ·éæ³•å†åˆ†å‘ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å¾®è°ƒVFMçš„éƒ¨åˆ†å±‚å’Œå°å‹ç½‘ç»œï¼Œå°†æ•°å­—æ°´å°åµŒå…¥æ¨¡å‹å†…éƒ¨è¡¨ç¤ºï¼Œä»¥å®ç°æ‰€æœ‰æƒéªŒè¯ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ°´å°æ¨¡å‹å’Œéæ°´å°æ¨¡å‹çš„è¯¯æ£€æ¦‚ç‡ä¸Šå‡æ˜¾è‘—é™ä½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½å¤Ÿé’ˆå¯¹å¤šç§ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œæ¨¡å‹çš„çŸ¥è¯†äº§æƒä¿æŠ¤é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é˜²æ­¢ä¸æ³•ç”¨æˆ·éæ³•å†åˆ†å‘æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¾®è°ƒVFMçš„éƒ¨åˆ†å±‚ä»¥åŠå°å‹ç¼–ç -è§£ç ç½‘ç»œï¼Œå°†æ•°å­—æ°´å°åµŒå…¥è¾“å…¥å›¾åƒçš„å†…éƒ¨è¡¨ç¤ºçš„æ–¹æ³•ã€‚è¯¥æ°´å°åœ¨ç»è¿‡å¾®è°ƒçš„åŠŸèƒ½æ€§æ¨¡å‹ä¸­ä»ç„¶å¯è¢«æ£€æµ‹ï¼Œä»è€Œæœ‰æ•ˆåŒºåˆ†å—ä¿æŠ¤æ¨¡å‹çš„å¤åˆ¶å“ä¸ç‹¬ç«‹æ¨¡å‹ã€‚ç†è®ºä¸å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ°´å°æ¨¡å‹å’Œéæ°´å°æ¨¡å‹çš„è¯¯æ£€æ¦‚ç‡ä¸Šå‡ä¿æŒè¾ƒä½æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†äº§æƒä¿æŠ¤é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é˜²æ­¢æ¨¡å‹è¢«éæ³•å†åˆ†å‘æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ‰€æœ‰æƒéªŒè¯å·¥å…·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºé€šè¿‡å¾®è°ƒVFMçš„éƒ¨åˆ†å±‚å’Œå°å‹ç¼–ç -è§£ç ç½‘ç»œï¼Œå°†æ•°å­—æ°´å°åµŒå…¥è¾“å…¥å›¾åƒçš„å†…éƒ¨è¡¨ç¤ºï¼Œä»¥ç¡®ä¿æ°´å°åœ¨åŠŸèƒ½æ€§æ¨¡å‹ä¸­ä»å¯æ£€æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹VFMçš„éƒ¨åˆ†å±‚è¿›è¡Œå¾®è°ƒï¼Œä»¥åŠè®¾è®¡ä¸€ä¸ªå°å‹çš„ç¼–ç -è§£ç ç½‘ç»œï¼Œæ°´å°åµŒå…¥è¿‡ç¨‹ä¸æ¨¡å‹è®­ç»ƒç›¸ç»“åˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„åˆ›æ–°ä¹‹å¤„åœ¨äºæ°´å°çš„åµŒå…¥æ–¹å¼ï¼Œä½¿å¾—å³ä½¿åœ¨æ¨¡å‹å¾®è°ƒåï¼Œæ°´å°ä¾ç„¶å¯è¢«æ£€æµ‹ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ‰€æœ‰æƒéªŒè¯èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé€‰æ‹©äº†ç‰¹å®šçš„å±‚è¿›è¡Œå¾®è°ƒï¼Œå¹¶è®¾ç½®äº†é€‚å½“çš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ°´å°çš„åµŒå…¥æ•ˆæœï¼Œç¡®ä¿æ°´å°åœ¨ä¸åŒä»»åŠ¡ä¸‹çš„å¯æ£€æµ‹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ°´å°æ¨¡å‹çš„è¯¯æ£€æ¦‚ç‡ä½äº5%ï¼Œè€Œéæ°´å°æ¨¡å‹çš„è¯¯æ£€æ¦‚ç‡ä¹Ÿä¿æŒåœ¨è¾ƒä½æ°´å¹³ï¼ŒéªŒè¯äº†å…¶åœ¨æ‰€æœ‰æƒéªŒè¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æŠ€æœ¯åœ¨æ°´å°çš„å¯æ£€æµ‹æ€§å’Œæ¨¡å‹å¾®è°ƒåçš„ç¨³å®šæ€§ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨çŸ¥è¯†äº§æƒä¿æŠ¤ã€æ¨¡å‹åˆ†å‘å’Œè®¡ç®—æœºè§†è§‰åº”ç”¨ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚é€šè¿‡æœ‰æ•ˆçš„æ°´å°æŠ€æœ¯ï¼Œæ¨¡å‹å¼€å‘è€…å¯ä»¥æ›´å¥½åœ°ä¿æŠ¤å…¶çŸ¥è¯†äº§æƒï¼Œé˜²æ­¢ä¸æ³•ä½¿ç”¨ï¼Œä¿ƒè¿›æ¨¡å‹çš„åˆæ³•ä½¿ç”¨ä¸åˆ†å‘ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šåœ¨æ›´å¤šçš„è§†è§‰ä»»åŠ¡å’Œæ¨¡å‹ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å¥åº·å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Being trained on large and vast datasets, visual foundation models (VFMs) can be fine-tuned for diverse downstream tasks, achieving remarkable performance and efficiency in various computer vision applications. The high computation cost of data collection and training motivates the owners of some VFMs to distribute them alongside the license to protect their intellectual property rights. However, a dishonest user of the protected model's copy may illegally redistribute it, for example, to make a profit. As a consequence, the development of reliable ownership verification tools is of great importance today, since such methods can be used to differentiate between a redistributed copy of the protected model and an independent model. In this paper, we propose an approach to ownership verification of visual foundation models by fine-tuning a small set of expressive layers of a VFM along with a small encoder-decoder network to embed digital watermarks into an internal representation of a hold-out set of input images. Importantly, the watermarks embedded remain detectable in the functional copies of the protected model, obtained, for example, by fine-tuning the VFM for a particular downstream task. Theoretically and experimentally, we demonstrate that the proposed method yields a low probability of false detection of a non-watermarked model and a low probability of false misdetection of a watermarked model.

