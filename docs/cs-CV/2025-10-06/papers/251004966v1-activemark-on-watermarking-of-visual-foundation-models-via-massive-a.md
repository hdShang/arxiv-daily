---
layout: default
title: ActiveMark: on watermarking of visual foundation models via massive activations
---

# ActiveMark: on watermarking of visual foundation models via massive activations

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04966" target="_blank" class="toolbar-btn">arXiv: 2510.04966v1</a>
    <a href="https://arxiv.org/pdf/2510.04966.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04966v1" 
            onclick="toggleFavorite(this, '2510.04966v1', 'ActiveMark: on watermarking of visual foundation models via massive activations')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Anna Chistyakova, Mikhail Pautov

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ActiveMark‰ª•Ëß£ÂÜ≥ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÊ∞¥Âç∞‰øùÊä§ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÂü∫Á°ÄÊ®°Âûã` `Ê∞¥Âç∞ÊäÄÊúØ` `Áü•ËØÜ‰∫ßÊùÉ‰øùÊä§` `ÊâÄÊúâÊùÉÈ™åËØÅ` `ËÆ°ÁÆóÊú∫ËßÜËßâ` `Ê®°ÂûãÂæÆË∞É` `Êï∞Â≠óÊ∞¥Âç∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâÂü∫Á°ÄÊ®°ÂûãÂú®Áü•ËØÜ‰∫ßÊùÉ‰øùÊä§ÊñπÈù¢Â≠òÂú®ÊºèÊ¥ûÔºåÂÆπÊòìË¢´‰∏çÊ≥ïÁî®Êà∑ÈùûÊ≥ïÂÜçÂàÜÂèë„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøáÂæÆË∞ÉVFMÁöÑÈÉ®ÂàÜÂ±ÇÂíåÂ∞èÂûãÁΩëÁªúÔºåÂ∞ÜÊï∞Â≠óÊ∞¥Âç∞ÂµåÂÖ•Ê®°ÂûãÂÜÖÈÉ®Ë°®Á§∫Ôºå‰ª•ÂÆûÁé∞ÊâÄÊúâÊùÉÈ™åËØÅ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ê∞¥Âç∞Ê®°ÂûãÂíåÈùûÊ∞¥Âç∞Ê®°ÂûãÁöÑËØØÊ£ÄÊ¶ÇÁéá‰∏äÂùáÊòæËëóÈôç‰ΩéÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºàVFMÔºâÂú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåËÉΩÂ§üÈíàÂØπÂ§öÁßç‰∏ãÊ∏∏‰ªªÂä°ËøõË°åÂæÆË∞ÉÔºåÂ±ïÁé∞Âá∫ÂçìË∂äÁöÑÊÄßËÉΩÂíåÊïàÁéá„ÄÇÁÑ∂ËÄåÔºåÊ®°ÂûãÁöÑÁü•ËØÜ‰∫ßÊùÉ‰øùÊä§Èù¢‰∏¥ÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÈò≤Ê≠¢‰∏çÊ≥ïÁî®Êà∑ÈùûÊ≥ïÂÜçÂàÜÂèëÊ®°Âûã„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöËøáÂæÆË∞ÉVFMÁöÑÈÉ®ÂàÜÂ±Ç‰ª•ÂèäÂ∞èÂûãÁºñÁ†Å-Ëß£Á†ÅÁΩëÁªúÔºåÂ∞ÜÊï∞Â≠óÊ∞¥Âç∞ÂµåÂÖ•ËæìÂÖ•ÂõæÂÉèÁöÑÂÜÖÈÉ®Ë°®Á§∫ÁöÑÊñπÊ≥ï„ÄÇËØ•Ê∞¥Âç∞Âú®ÁªèËøáÂæÆË∞ÉÁöÑÂäüËÉΩÊÄßÊ®°Âûã‰∏≠‰ªçÁÑ∂ÂèØË¢´Ê£ÄÊµãÔºå‰ªéËÄåÊúâÊïàÂå∫ÂàÜÂèó‰øùÊä§Ê®°ÂûãÁöÑÂ§çÂà∂ÂìÅ‰∏éÁã¨Á´ãÊ®°Âûã„ÄÇÁêÜËÆ∫‰∏éÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ê∞¥Âç∞Ê®°ÂûãÂíåÈùûÊ∞¥Âç∞Ê®°ÂûãÁöÑËØØÊ£ÄÊ¶ÇÁéá‰∏äÂùá‰øùÊåÅËæÉ‰ΩéÊ∞¥Âπ≥„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÁü•ËØÜ‰∫ßÊùÉ‰øùÊä§ÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Èò≤Ê≠¢Ê®°ÂûãË¢´ÈùûÊ≥ïÂÜçÂàÜÂèëÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÁº∫‰πèÊúâÊïàÁöÑÊâÄÊúâÊùÉÈ™åËØÅÂ∑•ÂÖ∑„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÈÄöËøáÂæÆË∞ÉVFMÁöÑÈÉ®ÂàÜÂ±ÇÂíåÂ∞èÂûãÁºñÁ†Å-Ëß£Á†ÅÁΩëÁªúÔºåÂ∞ÜÊï∞Â≠óÊ∞¥Âç∞ÂµåÂÖ•ËæìÂÖ•ÂõæÂÉèÁöÑÂÜÖÈÉ®Ë°®Á§∫Ôºå‰ª•Á°Æ‰øùÊ∞¥Âç∞Âú®ÂäüËÉΩÊÄßÊ®°Âûã‰∏≠‰ªçÂèØÊ£ÄÊµã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂØπVFMÁöÑÈÉ®ÂàÜÂ±ÇËøõË°åÂæÆË∞ÉÔºå‰ª•ÂèäËÆæËÆ°‰∏Ä‰∏™Â∞èÂûãÁöÑÁºñÁ†Å-Ëß£Á†ÅÁΩëÁªúÔºåÊ∞¥Âç∞ÂµåÂÖ•ËøáÁ®ã‰∏éÊ®°ÂûãËÆ≠ÁªÉÁõ∏ÁªìÂêà„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÊ∞¥Âç∞ÁöÑÂµåÂÖ•ÊñπÂºèÔºå‰ΩøÂæóÂç≥‰ΩøÂú®Ê®°ÂûãÂæÆË∞ÉÂêéÔºåÊ∞¥Âç∞‰æùÁÑ∂ÂèØË¢´Ê£ÄÊµãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊâÄÊúâÊùÉÈ™åËØÅËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈÄâÊã©‰∫ÜÁâπÂÆöÁöÑÂ±ÇËøõË°åÂæÆË∞ÉÔºåÂπ∂ËÆæÁΩÆ‰∫ÜÈÄÇÂΩìÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÊ∞¥Âç∞ÁöÑÂµåÂÖ•ÊïàÊûúÔºåÁ°Æ‰øùÊ∞¥Âç∞Âú®‰∏çÂêå‰ªªÂä°‰∏ãÁöÑÂèØÊ£ÄÊµãÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®Ê∞¥Âç∞Ê®°ÂûãÁöÑËØØÊ£ÄÊ¶ÇÁéá‰Ωé‰∫é5%ÔºåËÄåÈùûÊ∞¥Âç∞Ê®°ÂûãÁöÑËØØÊ£ÄÊ¶ÇÁéá‰πü‰øùÊåÅÂú®ËæÉ‰ΩéÊ∞¥Âπ≥ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÊâÄÊúâÊùÉÈ™åËØÅ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊú¨ÊñáÊèêÂá∫ÁöÑÊäÄÊúØÂú®Ê∞¥Âç∞ÁöÑÂèØÊ£ÄÊµãÊÄßÂíåÊ®°ÂûãÂæÆË∞ÉÂêéÁöÑÁ®≥ÂÆöÊÄß‰∏äÊúâÊòæËëóÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®Áü•ËØÜ‰∫ßÊùÉ‰øùÊä§„ÄÅÊ®°ÂûãÂàÜÂèëÂíåËÆ°ÁÆóÊú∫ËßÜËßâÂ∫îÁî®Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÊΩúÂú®Â∫îÁî®‰ª∑ÂÄº„ÄÇÈÄöËøáÊúâÊïàÁöÑÊ∞¥Âç∞ÊäÄÊúØÔºåÊ®°ÂûãÂºÄÂèëËÄÖÂèØ‰ª•Êõ¥Â•ΩÂú∞‰øùÊä§ÂÖ∂Áü•ËØÜ‰∫ßÊùÉÔºåÈò≤Ê≠¢‰∏çÊ≥ï‰ΩøÁî®Ôºå‰øÉËøõÊ®°ÂûãÁöÑÂêàÊ≥ï‰ΩøÁî®‰∏éÂàÜÂèë„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØËÉΩ‰ºöÂú®Êõ¥Â§öÁöÑËßÜËßâ‰ªªÂä°ÂíåÊ®°Âûã‰∏≠ÂæóÂà∞Â∫îÁî®ÔºåÊé®Âä®ËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÁöÑÂÅ•Â∫∑ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Being trained on large and vast datasets, visual foundation models (VFMs) can be fine-tuned for diverse downstream tasks, achieving remarkable performance and efficiency in various computer vision applications. The high computation cost of data collection and training motivates the owners of some VFMs to distribute them alongside the license to protect their intellectual property rights. However, a dishonest user of the protected model's copy may illegally redistribute it, for example, to make a profit. As a consequence, the development of reliable ownership verification tools is of great importance today, since such methods can be used to differentiate between a redistributed copy of the protected model and an independent model. In this paper, we propose an approach to ownership verification of visual foundation models by fine-tuning a small set of expressive layers of a VFM along with a small encoder-decoder network to embed digital watermarks into an internal representation of a hold-out set of input images. Importantly, the watermarks embedded remain detectable in the functional copies of the protected model, obtained, for example, by fine-tuning the VFM for a particular downstream task. Theoretically and experimentally, we demonstrate that the proposed method yields a low probability of false detection of a non-watermarked model and a low probability of false misdetection of a watermarked model.

