---
layout: default
title: "WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion"
---

# WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.15874" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.15874v1</a>
  <a href="https://arxiv.org/pdf/2511.15874.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15874v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.15874v1', 'WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sajjad Pakdamansavoji, Yintao Ma, Amir Rasouli, Tongtong Cao

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**WALDOï¼šæå‡ºä¸€ç§æ–°é¢–çš„åŸºäºæ¨¡å‹çš„6Dä½å§¿ä¼°è®¡æ–¹æ³•ï¼Œæå‡é®æŒ¡åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `6Dä½å§¿ä¼°è®¡` `é®æŒ¡å¤„ç†` `æ¨¡å‹é©±åŠ¨` `æœºå™¨äºº` `å¢å¼ºç°å®` `åŠ¨æ€é‡‡æ ·` `å¤šå‡è®¾æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ¨¡å‹çš„6Dä½å§¿ä¼°è®¡æ–¹æ³•åœ¨é®æŒ¡åœºæ™¯ä¸‹ï¼Œç”±äºæ—©æœŸé˜¶æ®µçš„æ£€æµ‹å’Œåˆ†å‰²è¯¯å·®ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºWALDOï¼Œé€šè¿‡åŠ¨æ€é‡‡æ ·ã€å¤šå‡è®¾æ¨ç†ã€è¿­ä»£ç»†åŒ–å’Œé®æŒ¡å¢å¼ºç­‰ç­–ç•¥ï¼Œæå‡é®æŒ¡åœºæ™¯ä¸‹çš„ä½å§¿ä¼°è®¡é²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒWALDOåœ¨ICBINå’ŒBOPæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„ç²¾åº¦æå‡ï¼Œå¹¶æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç²¾ç¡®çš„6Dç‰©ä½“ä½å§¿ä¼°è®¡å¯¹äºæœºå™¨äººã€å¢å¼ºç°å®å’Œåœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚å¯¹äºå·²è§è¿‡çš„ç‰©ä½“ï¼Œé€šè¿‡é€å¯¹è±¡å¾®è°ƒé€šå¸¸å¯ä»¥å®ç°é«˜ç²¾åº¦ï¼Œä½†æ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿‡å»çš„æ–¹æ³•é€šå¸¸å‡è®¾åœ¨æµ‹è¯•æ—¶å¯ä»¥è®¿é—®CADæ¨¡å‹ï¼Œå¹¶ä¸”é€šå¸¸éµå¾ªä¸€ä¸ªå¤šé˜¶æ®µæµç¨‹æ¥ä¼°è®¡ä½å§¿ï¼šæ£€æµ‹å’Œåˆ†å‰²ç‰©ä½“ï¼Œæå‡ºåˆå§‹ä½å§¿ï¼Œç„¶åå¯¹å…¶è¿›è¡Œç»†åŒ–ã€‚ç„¶è€Œï¼Œåœ¨é®æŒ¡ä¸‹ï¼Œè¿™ç§æµç¨‹çš„æ—©æœŸé˜¶æ®µå®¹æ˜“å‡ºé”™ï¼Œè¿™äº›é”™è¯¯ä¼šé€šè¿‡é¡ºåºå¤„ç†ä¼ æ’­ï¼Œä»è€Œé™ä½æ€§èƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸ªç¼ºç‚¹ï¼Œæˆ‘ä»¬å¯¹åŸºäºæ¨¡å‹çš„6Dä½å§¿ä¼°è®¡æ–¹æ³•æå‡ºäº†å››ä¸ªæ–°çš„æ‰©å±•ï¼šï¼ˆiï¼‰ä¸€ç§åŠ¨æ€éå‡åŒ€å¯†é›†é‡‡æ ·ç­–ç•¥ï¼Œå°†è®¡ç®—é›†ä¸­åœ¨å¯è§åŒºåŸŸï¼Œå‡å°‘é®æŒ¡å¼•èµ·çš„è¯¯å·®ï¼›ï¼ˆiiï¼‰ä¸€ç§å¤šå‡è®¾æ¨ç†æœºåˆ¶ï¼Œä¿ç•™å‡ ä¸ªç½®ä¿¡åº¦æ’åºçš„ä½å§¿å€™é€‰ï¼Œå‡è½»è„†å¼±çš„å•è·¯å¾„å¤±è´¥ï¼›ï¼ˆiiiï¼‰è¿­ä»£ç»†åŒ–ï¼Œä»¥é€æ­¥æé«˜ä½å§¿ç²¾åº¦ï¼›ï¼ˆivï¼‰ä¸€ç³»åˆ—ä»¥é®æŒ¡ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ•°æ®å¢å¼ºï¼Œä»¥å¢å¼ºé²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŒ‰å¯è§æ€§åŠ æƒçš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºåœ¨é®æŒ¡ä¸‹è¿›è¡Œè¯„ä¼°ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘ç°æœ‰åè®®ä¸­çš„åå·®ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨ICBINä¸Šå®ç°äº†è¶…è¿‡5%çš„ç²¾åº¦æå‡ï¼Œåœ¨BOPæ•°æ®é›†åŸºå‡†ä¸Šå®ç°äº†è¶…è¿‡2%çš„ç²¾åº¦æå‡ï¼ŒåŒæ—¶å®ç°äº†å¤§çº¦3å€çš„æ¨ç†é€Ÿåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨å­˜åœ¨é®æŒ¡çš„æƒ…å†µä¸‹ï¼ŒåŸºäºæ¨¡å‹çš„6Dä½å§¿ä¼°è®¡ç²¾åº¦ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç²¾ç¡®çš„ç‰©ä½“æ£€æµ‹å’Œåˆ†å‰²ï¼Œä½†åœ¨é®æŒ¡ç¯å¢ƒä¸‹ï¼Œè¿™äº›æ­¥éª¤å®¹æ˜“å‡ºé”™ï¼Œå¯¼è‡´åç»­çš„ä½å§¿ä¼°è®¡æ€§èƒ½å—åˆ°ä¸¥é‡å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‡å°‘å¯¹å®Œæ•´ç‰©ä½“ä¿¡æ¯çš„ä¾èµ–ï¼Œæ›´åŠ å…³æ³¨å¯è§åŒºåŸŸçš„ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å¤šå‡è®¾æ¨ç†æ¥åº”å¯¹ä¸ç¡®å®šæ€§ã€‚é€šè¿‡åŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼Œå°†è®¡ç®—èµ„æºé›†ä¸­åœ¨å¯è§åŒºåŸŸï¼Œå‡å°‘é®æŒ¡å¸¦æ¥çš„å¹²æ‰°ã€‚å¤šå‡è®¾æ¨ç†åˆ™ä¿ç•™å¤šä¸ªå¯èƒ½çš„ä½å§¿ä¼°è®¡ï¼Œé¿å…å› æ—©æœŸé”™è¯¯è€Œå¯¼è‡´çš„å•è·¯å¾„å¤±è´¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWALDOçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŠ¨æ€éå‡åŒ€å¯†é›†é‡‡æ ·ï¼šæ ¹æ®å¯è§æ€§å¯¹ç‰©ä½“è¡¨é¢è¿›è¡Œé‡‡æ ·ï¼Œé›†ä¸­è®¡ç®—èµ„æºäºå¯è§åŒºåŸŸã€‚2) å¤šå‡è®¾æ¨ç†ï¼šç”Ÿæˆå¤šä¸ªå€™é€‰ä½å§¿ï¼Œå¹¶æ ¹æ®ç½®ä¿¡åº¦è¿›è¡Œæ’åºã€‚3) è¿­ä»£ç»†åŒ–ï¼šé€æ­¥ä¼˜åŒ–å€™é€‰ä½å§¿ï¼Œæé«˜ç²¾åº¦ã€‚4) é®æŒ¡å¢å¼ºï¼šé€šè¿‡æ¨¡æ‹Ÿå„ç§é®æŒ¡æƒ…å†µæ¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåŠ¨æ€éå‡åŒ€å¯†é›†é‡‡æ ·ç­–ç•¥å’Œå¤šå‡è®¾æ¨ç†æœºåˆ¶ã€‚åŠ¨æ€é‡‡æ ·èƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘é®æŒ¡å¸¦æ¥çš„å¹²æ‰°ï¼Œæé«˜ä½å§¿ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚å¤šå‡è®¾æ¨ç†åˆ™èƒ½å¤Ÿåº”å¯¹ä¸ç¡®å®šæ€§ï¼Œé¿å…å› æ—©æœŸé”™è¯¯è€Œå¯¼è‡´çš„å•è·¯å¾„å¤±è´¥ã€‚æ­¤å¤–ï¼Œæå‡ºçš„é®æŒ¡æ„ŸçŸ¥è¯„ä¼°æŒ‡æ ‡ä¹Ÿæ›´åŠ åˆç†åœ°è¯„ä¼°äº†ç®—æ³•åœ¨é®æŒ¡ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€é‡‡æ ·ç­–ç•¥æ ¹æ®ç‰©ä½“çš„å¯è§æ€§æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œå¯è§æ€§æ¦‚ç‡å¯ä»¥é€šè¿‡æ¸²æŸ“æˆ–å…¶ä»–æ–¹æ³•ä¼°è®¡ã€‚å¤šå‡è®¾æ¨ç†é‡‡ç”¨ç½®ä¿¡åº¦æ’åºï¼Œç½®ä¿¡åº¦å¯ä»¥åŸºäºæ¸²æŸ“ç»“æœä¸è§‚æµ‹å›¾åƒçš„ç›¸ä¼¼åº¦æ¥è®¡ç®—ã€‚è¿­ä»£ç»†åŒ–é‡‡ç”¨ICPæˆ–å…¶ä»–ä¼˜åŒ–ç®—æ³•æ¥é€æ­¥æé«˜ä½å§¿ç²¾åº¦ã€‚é®æŒ¡å¢å¼ºåˆ™é€šè¿‡åœ¨è®­ç»ƒå›¾åƒä¸­éšæœºæ·»åŠ é®æŒ¡ç‰©æ¥å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒWALDOåœ¨ICBINæ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡5%çš„ç²¾åº¦æå‡ï¼Œåœ¨BOPæ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡2%çš„ç²¾åº¦æå‡ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æé«˜äº†çº¦3å€ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒWALDOåœ¨é®æŒ¡åœºæ™¯ä¸‹å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œæ›´é«˜çš„æ•ˆç‡ï¼Œä¼˜äºç°æœ‰çš„åŸºäºæ¨¡å‹çš„6Dä½å§¿ä¼°è®¡æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œã€å¢å¼ºç°å®ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œå‡†ç¡®çš„6Dä½å§¿ä¼°è®¡æ˜¯å®ç°ç‰©ä½“æŠ“å–å’Œæ“ä½œçš„åŸºç¡€ã€‚åœ¨å¢å¼ºç°å®ä¸­ï¼Œå¯ä»¥ç”¨äºå°†è™šæ‹Ÿç‰©ä½“ç²¾ç¡®åœ°å åŠ åˆ°çœŸå®åœºæ™¯ä¸­ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ç”¨äºæ„ŸçŸ¥å‘¨å›´ç¯å¢ƒä¸­çš„ç‰©ä½“ï¼Œå¹¶è¿›è¡Œç²¾ç¡®çš„å®šä½å’Œè·Ÿè¸ªã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.

