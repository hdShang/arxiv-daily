---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-03
---

# cs.CVï¼ˆ2025-05-03ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250501799v1-aquags-fast-underwater-scene-reconstruction-with-sfm-free-gaussian-s.html">AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting</a></td>
  <td>æå‡ºAquaGSä»¥è§£å†³æ°´ä¸‹åœºæ™¯é‡å»ºé€Ÿåº¦æ…¢ä¸ç²¾åº¦ä½çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01799v1" data-paper-url="./papers/250501799v1-aquags-fast-underwater-scene-reconstruction-with-sfm-free-gaussian-s.html" onclick="toggleFavorite(this, '2505.01799v1', 'AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250501928v1-gensync-a-generalized-talking-head-framework-for-audio-driven-multi-.html">GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting</a></td>
  <td>æå‡ºGenSyncæ¡†æ¶ä»¥è§£å†³å¤šèº«ä»½å£å‹åŒæ­¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01928v1" data-paper-url="./papers/250501928v1-gensync-a-generalized-talking-head-framework-for-audio-driven-multi-.html" onclick="toggleFavorite(this, '2505.01928v1', 'GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250501938v1-hybridgs-high-efficiency-gaussian-splatting-data-compression-using-d.html">HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder</a></td>
  <td>æå‡ºHybridGSä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘å‹ç¼©æ•ˆç‡ä½çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01938v1" data-paper-url="./papers/250501938v1-hybridgs-high-efficiency-gaussian-splatting-data-compression-using-d.html" onclick="toggleFavorite(this, '2505.01938v1', 'HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250502867v1-resanything-attribute-prompting-for-arbitrary-referring-segmentation.html">RESAnything: Attribute Prompting for Arbitrary Referring Segmentation</a></td>
  <td>æå‡ºRESAnythingä»¥è§£å†³ä»»æ„æŒ‡ç§°åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02867v1" data-paper-url="./papers/250502867v1-resanything-attribute-prompting-for-arbitrary-referring-segmentation.html" onclick="toggleFavorite(this, '2505.02867v1', 'RESAnything: Attribute Prompting for Arbitrary Referring Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250501711v1-knowledge-augmented-language-models-interpreting-structured-chest-x-.html">Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings</a></td>
  <td>æå‡ºCXR-TextInterä»¥è§£å†³èƒ¸éƒ¨Xå…‰å›¾åƒè§£è¯»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01711v1" data-paper-url="./papers/250501711v1-knowledge-augmented-language-models-interpreting-structured-chest-x-.html" onclick="toggleFavorite(this, '2505.01711v1', 'Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250501680v1-automated-arat-scoring-using-multimodal-video-analysis-multi-view-fu.html">Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study</a></td>
  <td>æå‡ºè‡ªåŠ¨åŒ–ARATè¯„åˆ†ç³»ç»Ÿä»¥è§£å†³ä¸­é£åº·å¤è¯„ä¼°çš„æ—¶é—´å’Œå‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01680v1" data-paper-url="./papers/250501680v1-automated-arat-scoring-using-multimodal-video-analysis-multi-view-fu.html" onclick="toggleFavorite(this, '2505.01680v1', 'Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250501851v1-mitigating-group-level-fairness-disparities-in-federated-visual-lang.html">Mitigating Group-Level Fairness Disparities in Federated Visual Language Models</a></td>
  <td>æå‡ºFVL-FPæ¡†æ¶ä»¥è§£å†³è”é‚¦è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç¾¤ä½“å…¬å¹³æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01851v1" data-paper-url="./papers/250501851v1-mitigating-group-level-fairness-disparities-in-federated-visual-lang.html" onclick="toggleFavorite(this, '2505.01851v1', 'Mitigating Group-Level Fairness Disparities in Federated Visual Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250501790v1-enhancing-the-learning-experience-using-vision-language-models-to-ge.html">Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos</a></td>
  <td>åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆæ•™è‚²è§†é¢‘é—®é¢˜ä»¥æå‡å­¦ä¹ ä½“éªŒ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01790v1" data-paper-url="./papers/250501790v1-enhancing-the-learning-experience-using-vision-language-models-to-ge.html" onclick="toggleFavorite(this, '2505.01790v1', 'Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/250501766v1-multimodal-graph-representation-learning-for-robust-surgical-workflo.html">Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å›¾è¡¨ç¤ºå­¦ä¹ ä»¥è§£å†³æ‰‹æœ¯å·¥ä½œæµç¨‹è¯†åˆ«çš„é²æ£’æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01766v1" data-paper-url="./papers/250501766v1-multimodal-graph-representation-learning-for-robust-surgical-workflo.html" onclick="toggleFavorite(this, '2505.01766v1', 'Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250501729v2-posepilot-steering-camera-pose-for-generative-world-models-with-self.html">PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth</a></td>
  <td>æå‡ºPosePilotä»¥è§£å†³æ‘„åƒå¤´å§¿æ€æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01729v2" data-paper-url="./papers/250501729v2-posepilot-steering-camera-pose-for-generative-world-models-with-self.html" onclick="toggleFavorite(this, '2505.01729v2', 'PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250501713v1-vision-and-intention-boost-large-language-model-in-long-term-action-.html">Vision and Intention Boost Large Language Model in Long-Term Action Anticipation</a></td>
  <td>æå‡ºæ„å›¾æ¡ä»¶è§†è§‰è¯­è¨€æ¨¡å‹ä»¥è§£å†³é•¿æ—¶é—´åŠ¨ä½œé¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Ego4D</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01713v1" data-paper-url="./papers/250501713v1-vision-and-intention-boost-large-language-model-in-long-term-action-.html" onclick="toggleFavorite(this, '2505.01713v1', 'Vision and Intention Boost Large Language Model in Long-Term Action Anticipation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250501838v1-mvhumannet-a-large-scale-dataset-of-multi-view-daily-dressing-human-.html">MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization</a></td>
  <td>æå‡ºMVHumanNet++ä»¥è§£å†³3Däººç±»æ•°å­—åŒ–æ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SMPL</span> <span class="paper-tag">SMPL-X</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01838v1" data-paper-url="./papers/250501838v1-mvhumannet-a-large-scale-dataset-of-multi-view-daily-dressing-human-.html" onclick="toggleFavorite(this, '2505.01838v1', 'MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250501809v1-3dwg-3d-weakly-supervised-visual-grounding-via-category-and-instance.html">3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment</a></td>
  <td>æå‡º3DWGä»¥è§£å†³3Då¼±ç›‘ç£è§†è§‰å®šä½ä¸­çš„ç±»åˆ«ä¸å®ä¾‹å¤æ‚æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01809v1" data-paper-url="./papers/250501809v1-3dwg-3d-weakly-supervised-visual-grounding-via-category-and-instance.html" onclick="toggleFavorite(this, '2505.01809v1', '3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250503829v1-videollm-benchmarks-and-evaluation-a-survey.html">VideoLLM Benchmarks and Evaluation: A Survey</a></td>
  <td>è¯„ä¼°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„åŸºå‡†ä¸æ–¹æ³•è®ºç»¼è¿°</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03829v1" data-paper-url="./papers/250503829v1-videollm-benchmarks-and-evaluation-a-survey.html" onclick="toggleFavorite(this, '2505.03829v1', 'VideoLLM Benchmarks and Evaluation: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250501802v1-efficient-3d-full-body-motion-generation-from-sparse-tracking-inputs.html">Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows</a></td>
  <td>æå‡ºåŸºäºMLPçš„é«˜æ•ˆ3Då…¨èº«åŠ¨ä½œç”Ÿæˆæ–¹æ³•ä»¥è§£å†³ç¨€ç–è¿½è¸ªè¾“å…¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01802v1" data-paper-url="./papers/250501802v1-efficient-3d-full-body-motion-generation-from-sparse-tracking-inputs.html" onclick="toggleFavorite(this, '2505.01802v1', 'Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250503821v1-beyond-recognition-evaluating-visual-perspective-taking-in-vision-la.html">Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models</a></td>
  <td>æå‡ºè§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰è§†è§’ç†è§£è¯„ä¼°æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03821v1" data-paper-url="./papers/250503821v1-beyond-recognition-evaluating-visual-perspective-taking-in-vision-la.html" onclick="toggleFavorite(this, '2505.03821v1', 'Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250501746v1-co3gesture-towards-coherent-concurrent-co-speech-3d-gesture-generati.html">Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</a></td>
  <td>æå‡ºCo$^{3}$Gestureä»¥è§£å†³åŒäººäº’åŠ¨è¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">mutual attention</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01746v1" data-paper-url="./papers/250501746v1-co3gesture-towards-coherent-concurrent-co-speech-3d-gesture-generati.html" onclick="toggleFavorite(this, '2505.01746v1', 'Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)