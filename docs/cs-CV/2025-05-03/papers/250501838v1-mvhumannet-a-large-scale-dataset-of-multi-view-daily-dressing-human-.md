---
layout: default
title: "MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization"
---

# MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01838" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01838v1</a>
  <a href="https://arxiv.org/pdf/2505.01838.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01838v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01838v1', 'MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenghong Li, Hongjie Liao, Yihao Zhi, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Shuguang Cui, Xiaoguang Han

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

**å¤‡æ³¨**: project page: https://kevinlee09.github.io/research/MVHumanNet++/. arXiv admin note: substantial text overlap with arXiv:2312.02963

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://kevinlee09.github.io/research/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMVHumanNet++ä»¥è§£å†³3Däººç±»æ•°å­—åŒ–æ•°æ®ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Däººç±»æ•°å­—åŒ–` `å¤šè§†è§’æ•æ‰` `æ•°æ®é›†æ„å»º` `äººç±»åŠ¨ä½œè¯†åˆ«` `æ—¥å¸¸æœè£…è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dè§†è§‰é¢†åŸŸçš„äººç±»ä¸­å¿ƒä»»åŠ¡ç”±äºç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯¼è‡´ç ”ç©¶è¿›å±•ç¼“æ…¢ã€‚
2. MVHumanNet++æ•°æ®é›†é€šè¿‡å¤šè§†è§’æ•æ‰ç³»ç»Ÿæ”¶é›†äº†4500ä¸ªèº«ä»½çš„ä¸°å¯Œäººç±»æ•°æ®ï¼ŒåŒ…å«å¤šæ ·çš„æ—¥å¸¸æœè£…ã€‚
3. åˆæ­¥å®éªŒè¡¨æ˜ï¼ŒMVHumanNet++åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å–å¾—æˆåŠŸçš„èƒŒæ™¯ä¸‹ï¼Œ3Dè§†è§‰é¢†åŸŸçš„äººç±»ä¸­å¿ƒä»»åŠ¡ç”±äºç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†è€Œè¿›å±•æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºMVHumanNet++æ•°æ®é›†ï¼ŒåŒ…å«4500ä¸ªèº«ä»½çš„å¤šè§†è§’äººç±»åŠ¨ä½œåºåˆ—ã€‚è¯¥æ•°æ®é›†æ”¶å½•äº†9000ç§æ—¥å¸¸æœè£…ã€60000ä¸ªè¿åŠ¨åºåˆ—å’Œ645äº¿å¸§å›¾åƒï¼Œæä¾›ä¸°å¯Œçš„æ³¨é‡Šä¿¡æ¯ï¼ŒåŒ…æ‹¬äººç±»æ©è†œã€ç›¸æœºå‚æ•°ã€2Då’Œ3Då…³é”®ç‚¹ã€SMPL/SMPLXå‚æ•°åŠç›¸åº”çš„æ–‡æœ¬æè¿°ã€‚æ­¤å¤–ï¼ŒMVHumanNet++è¿˜å¢å¼ºäº†æ³•çº¿å›¾å’Œæ·±åº¦å›¾ï¼Œæ˜¾è‘—æ‰©å±•äº†å…¶åœ¨3Däººç±»ç ”ç©¶ä¸­çš„é€‚ç”¨æ€§ã€‚é€šè¿‡ä¸€ç³»åˆ—åˆæ­¥ç ”ç©¶ï¼Œå±•ç¤ºäº†è¯¥æ•°æ®é›†åœ¨å¤šç§2Då’Œ3Dè§†è§‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆåº”ç”¨å’Œæ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰3Dè§†è§‰é¢†åŸŸç¼ºä¹å¤§è§„æ¨¡çš„äººç±»æ•°æ®é›†ï¼Œé™åˆ¶äº†äººç±»ä¸­å¿ƒä»»åŠ¡çš„ç ”ç©¶å’Œåº”ç”¨ã€‚ç°æœ‰æ•°æ®é›†å¤šé›†ä¸­äºç‰©ä½“ï¼Œè€Œäººç±»æ•°æ®çš„ç¨€ç¼ºä½¿å¾—ç›¸å…³ç ”ç©¶è¿›å±•ç¼“æ…¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºMVHumanNet++æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡å¤šè§†è§’äººç±»æ•æ‰ç³»ç»Ÿï¼Œæ”¶é›†å¤šæ ·åŒ–çš„æ—¥å¸¸æœè£…å’Œä¸°å¯Œçš„äººç±»åŠ¨ä½œæ•°æ®ï¼Œä»¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚è¯¥æ•°æ®é›†çš„è®¾è®¡è€ƒè™‘äº†æ•°æ®çš„å¯æ‰©å±•æ€§å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMVHumanNet++æ•°æ®é›†çš„æ„å»ºåŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ•°æ®å¤„ç†å’Œæ³¨é‡Šä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡å¤šè§†è§’æ•æ‰ç³»ç»Ÿè·å–äººç±»åŠ¨ä½œåºåˆ—ï¼Œç„¶åè¿›è¡Œæ•°æ®æ¸…æ´—å’Œå¤„ç†ï¼Œæœ€åæ·»åŠ ä¸°å¯Œçš„æ³¨é‡Šä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šMVHumanNet++æ˜¯å½“å‰æœ€å¤§çš„3Däººç±»æ•°æ®é›†ï¼ŒåŒ…å«645äº¿å¸§å›¾åƒå’Œå¤šç§æ³¨é‡Šï¼Œæ˜¾è‘—æå‡äº†äººç±»ä¸­å¿ƒä»»åŠ¡çš„ç ”ç©¶åŸºç¡€ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼Œå…¶åœ¨èº«ä»½å¤šæ ·æ€§å’Œæ—¥å¸¸æœè£…çš„è¦†ç›–ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†ä¸­åŒ…å«äººç±»æ©è†œã€ç›¸æœºå‚æ•°ã€2Då’Œ3Då…³é”®ç‚¹ã€SMPL/SMPLXå‚æ•°ç­‰å¤šç§æ³¨é‡Šä¿¡æ¯ï¼Œæ­¤å¤–è¿˜æ–°å¢äº†æ³•çº¿å›¾å’Œæ·±åº¦å›¾ï¼Œå¢å¼ºäº†æ•°æ®é›†çš„å®ç”¨æ€§å’Œé€‚ç”¨èŒƒå›´ã€‚å®éªŒä¸­é‡‡ç”¨äº†æ ‡å‡†çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥éªŒè¯æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡åˆæ­¥å®éªŒï¼ŒMVHumanNet++åœ¨å¤šä¸ª2Då’Œ3Dè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨äººç±»åŠ¨ä½œè¯†åˆ«ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡äº†15%ï¼Œå±•ç¤ºäº†è¯¥æ•°æ®é›†åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MVHumanNet++æ•°æ®é›†åœ¨3Däººç±»æ•°å­—åŒ–ã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶ä¸°å¯Œçš„æ³¨é‡Šä¿¡æ¯å’Œå¤šæ ·åŒ–çš„æ•°æ®ä¸ºç ”ç©¶äººå‘˜æä¾›äº†å¼ºå¤§çš„åŸºç¡€ï¼Œæ¨åŠ¨äº†äººç±»åŠ¨ä½œè¯†åˆ«ã€æœè£…è¯†åˆ«ç­‰ç›¸å…³ç ”ç©¶çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½ç©¿æˆ´è®¾å¤‡å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while significant progress has been achieved in object-centric tasks through large-scale datasets like Objaverse and MVImgNet, human-centric tasks have seen limited advancement, largely due to the absence of a comparable large-scale human dataset. To bridge this gap, we present MVHumanNet++, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using multi-view human capture systems, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. Additionally, the proposed MVHumanNet++ dataset is enhanced with newly processed normal maps and depth maps, significantly expanding its applicability and utility for advanced human-centric research. To explore the potential of our proposed MVHumanNet++ dataset in various 2D and 3D visual tasks, we conducted several pilot studies to demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet++. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet++ dataset with annotations will foster further innovations in the domain of 3D human-centric tasks at scale. MVHumanNet++ is publicly available at https://kevinlee09.github.io/research/MVHumanNet++/.

