---
layout: default
title: Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings
---

# Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01711" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01711v1</a>
  <a href="https://arxiv.org/pdf/2505.01711.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01711v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01711v1', 'Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexander Davis, Rafael Souza, Jia-Hao Lim

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCXR-TextInterä»¥è§£å†³èƒ¸éƒ¨Xå…‰å›¾åƒè§£è¯»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `èƒ¸éƒ¨Xå…‰è§£è¯»` `å¤§å‹è¯­è¨€æ¨¡å‹` `åŒ»å­¦çŸ¥è¯†æ¨¡å—` `å¤šæ¨¡æ€æ¨¡å‹` `ä¸´åºŠæ¨ç†` `è‡ªåŠ¨åŒ–åŒ»ç–—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰å›¾åƒè§£è¯»ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ï¼Œå¯¼è‡´è§£è¯»æ•ˆæœæœ‰é™ã€‚
2. CXR-TextInteræ¡†æ¶é€šè¿‡ç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤ºå’ŒåŒ»å­¦çŸ¥è¯†æ¨¡å—ï¼Œé‡æ–°å®šä¹‰äº†èƒ¸éƒ¨Xå…‰å›¾åƒçš„è§£è¯»æ–¹å¼ï¼Œæå‡äº†ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚
3. åœ¨CXR-ClinEvalåŸºå‡†ä¸Šï¼ŒCXR-TextInteråœ¨å¤šä¸ªè§£è¯»ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨è§£è¯»èƒ¸éƒ¨Xå…‰å›¾åƒï¼ˆCXRï¼‰æ˜¯æå‡ä¸´åºŠå·¥ä½œæµç¨‹å’Œæ‚£è€…æŠ¤ç†çš„é‡è¦ä»»åŠ¡ã€‚å°½ç®¡å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¿›å±•ä»¤äººé¼“èˆï¼Œä½†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè§†è§‰ä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†CXR-TextInteræ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤ºæ¥é‡æ–°åˆ©ç”¨å¼ºå¤§çš„æ–‡æœ¬ä¸­å¿ƒLLMsè¿›è¡ŒCXRè§£è¯»ï¼Œå¹¶é›†æˆåŒ»å­¦çŸ¥è¯†æ¨¡å—ä»¥å¢å¼ºä¸´åºŠæ¨ç†ã€‚ä¸ºä¾¿äºè®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†MediInstruct-CXRæ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†CXR-ClinEvalåŸºå‡†è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCXR-TextInteråœ¨ç—…ç†æ£€æµ‹ã€æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œä¸”ç»è¿‡è®¤è¯çš„æ”¾å°„ç§‘åŒ»å¸ˆçš„ç›²è¯„æ˜¾ç¤ºå¯¹å…¶ä¸´åºŠè¾“å‡ºè´¨é‡çš„æ˜¾è‘—åå¥½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³èƒ¸éƒ¨Xå…‰å›¾åƒè§£è¯»ä¸­ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é—®é¢˜ï¼Œå¯¼è‡´è§£è¯»æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCXR-TextInteræ¡†æ¶é€šè¿‡ç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤ºæ¥é‡æ–°åˆ©ç”¨æ–‡æœ¬ä¸­å¿ƒçš„LLMsï¼Œå¹¶é›†æˆåŒ»å­¦çŸ¥è¯†æ¨¡å—ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªå›¾åƒåˆ†æç®¡é“ç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤ºï¼Œéšååˆ©ç”¨LLMè¿›è¡Œè§£è¯»ï¼Œå¹¶é€šè¿‡åŒ»å­¦çŸ¥è¯†æ¨¡å—è¿›è¡Œå¢å¼ºã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†åŒ»å­¦çŸ¥è¯†æ¨¡å—ä¸LLMç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„è§£è¯»æ–¹å¼ï¼Œä¸ä¼ ç»Ÿçš„å¤šæ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ç»“æ„åŒ–ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è§£è¯»è´¨é‡ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”åŒ»å­¦é¢†åŸŸçš„ç‰¹æ®Šéœ€æ±‚ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CXR-TextInteråœ¨CXR-ClinEvalåŸºå‡†ä¸Šå®ç°äº†åœ¨ç—…ç†æ£€æµ‹ã€æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚æ­¤å¤–ï¼Œç»è¿‡è®¤è¯çš„æ”¾å°„ç§‘åŒ»å¸ˆçš„ç›²è¯„æ˜¾ç¤ºå¯¹å…¶è¾“å‡ºè´¨é‡çš„æ˜¾è‘—åå¥½ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—å½±åƒåˆ†æã€ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿå’Œè¿œç¨‹åŒ»ç–—ç­‰ã€‚é€šè¿‡æé«˜èƒ¸éƒ¨Xå…‰å›¾åƒçš„è§£è¯»å‡†ç¡®æ€§ï¼ŒCXR-TextInterèƒ½å¤Ÿæ˜¾è‘—æå‡ä¸´åºŠå·¥ä½œæ•ˆç‡å’Œæ‚£è€…æŠ¤ç†è´¨é‡ï¼Œæœªæ¥å¯èƒ½å¯¹åŒ»ç–—è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automated interpretation of chest X-rays (CXR) is a critical task with the potential to significantly improve clinical workflow and patient care. While recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (LLMs) for this visual task remains an underexplored area. This paper introduces CXR-TextInter, a novel framework that repurposes powerful text-centric LLMs for CXR interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. We augment this LLM-centric approach with an integrated medical knowledge module to enhance clinical reasoning. To facilitate training and evaluation, we developed the MediInstruct-CXR dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the CXR-ClinEval benchmark for comprehensive assessment across various interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that CXR-TextInter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. Ablation studies confirm the critical contribution of the knowledge integration module. Furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by CXR-TextInter. Our work validates an alternative paradigm for medical image AI, showcasing the potential of harnessing advanced LLM capabilities when visual information is effectively structured and domain knowledge is integrated.

