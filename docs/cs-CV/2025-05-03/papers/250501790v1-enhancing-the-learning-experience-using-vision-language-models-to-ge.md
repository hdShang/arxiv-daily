---
layout: default
title: "Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos"
---

# Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01790" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01790v1</a>
  <a href="https://arxiv.org/pdf/2505.01790.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01790v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01790v1', 'Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Markos Stamatakis, Joshua Berger, Christian Wartena, Ralph Ewerth, Anett Hoppe

**åˆ†ç±»**: cs.CV, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

**å¤‡æ³¨**: 12 pages (excluding references), 8 tables, 1 equation

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆæ•™è‚²è§†é¢‘é—®é¢˜ä»¥æå‡å­¦ä¹ ä½“éªŒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•™è‚²è§†é¢‘` `è§†è§‰-è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨é—®é¢˜ç”Ÿæˆ` `çŸ¥è¯†è·å–` `å¤šæ¨¡æ€æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•™è‚²è§†é¢‘åœ¨æå‡ç”¨æˆ·å‚ä¸åº¦å’ŒçŸ¥è¯†ä¿ç•™æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œè‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆå­¦ä¹ å¯¼å‘çš„é—®é¢˜ï¼Œé€šè¿‡å¾®è°ƒå’Œä¸åŒè§†é¢‘æ¨¡æ€çš„åˆ†ææ¥æå‡é—®é¢˜è´¨é‡ã€‚
3. ç ”ç©¶è¡¨æ˜ï¼Œå¾®è°ƒæ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—æé«˜é—®é¢˜çš„ç›¸å…³æ€§å’Œå¯å›ç­”æ€§ï¼ŒåŒæ—¶æŒ‡å‡ºäº†æœªæ¥å¤šæ¨¡æ€æ•°æ®é›†çš„éœ€æ±‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºç½‘ç»œçš„æ•™è‚²è§†é¢‘æä¾›çµæ´»çš„å­¦ä¹ æœºä¼šï¼Œç„¶è€Œæé«˜ç”¨æˆ·å‚ä¸åº¦å’ŒçŸ¥è¯†ä¿ç•™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜å¯ä»¥æ¿€æ´»å­¦ä¹ è€…å¹¶æ”¯æŒå…¶çŸ¥è¯†è·å–ï¼ŒåŒæ—¶å¸®åŠ©æ•™å¸ˆå’Œå­¦ä¹ è€…è¯„ä¼°ç†è§£ç¨‹åº¦ã€‚å°½ç®¡å¤§å‹è¯­è¨€å’Œè§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œä½†å…¶åœ¨æ•™è‚²è§†é¢‘é—®é¢˜ç”Ÿæˆä¸­çš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ç ”ç©¶äº†å½“å‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå­¦ä¹ å¯¼å‘é—®é¢˜æ–¹é¢çš„èƒ½åŠ›ï¼Œè¯„ä¼°äº†æ¨¡å‹çš„æ€§èƒ½ã€å¾®è°ƒå¯¹å†…å®¹ç‰¹å®šé—®é¢˜ç”Ÿæˆçš„å½±å“ã€ä¸åŒè§†é¢‘æ¨¡æ€å¯¹é—®é¢˜è´¨é‡çš„å½±å“ï¼Œä»¥åŠç”Ÿæˆé—®é¢˜çš„ç›¸å…³æ€§ã€å¯å›ç­”æ€§å’Œéš¾åº¦æ°´å¹³ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰è§†è§‰-è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¼ºè°ƒäº†å¾®è°ƒçš„å¿…è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†é—®é¢˜å¤šæ ·æ€§å’Œç›¸å…³æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ•™è‚²è§†é¢‘ä¸­è‡ªåŠ¨ç”Ÿæˆé—®é¢˜çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨é—®é¢˜å¤šæ ·æ€§å’Œç›¸å…³æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆå­¦ä¹ å¯¼å‘çš„é—®é¢˜ï¼Œè®ºæ–‡æ¢è®¨äº†æ¨¡å‹çš„å¾®è°ƒå’Œä¸åŒè§†é¢‘æ¨¡æ€å¯¹é—®é¢˜è´¨é‡çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åˆ†ä¸ºå‡ ä¸ªä¸»è¦æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æ€§èƒ½è¯„ä¼°ã€å¾®è°ƒå®éªŒã€è§†é¢‘æ¨¡æ€åˆ†æå’Œå®šæ€§ç ”ç©¶ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†ç”Ÿæˆé—®é¢˜çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²è§†é¢‘é—®é¢˜ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†å¾®è°ƒçš„é‡è¦æ€§å’Œä¸åŒæ¨¡æ€å¯¹é—®é¢˜ç”Ÿæˆçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œé’ˆå¯¹ä¸åŒè§†é¢‘å†…å®¹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥æé«˜ç”Ÿæˆé—®é¢˜çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨é—®é¢˜ç”Ÿæˆçš„ç›¸å…³æ€§å’Œå¯å›ç­”æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºæœªå¾®è°ƒçš„æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šï¼Œè¡¨æ˜å¾®è°ƒå¯¹æ•™è‚²è§†é¢‘é—®é¢˜ç”Ÿæˆçš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åœ¨çº¿æ•™è‚²å¹³å°ã€å­¦ä¹ ç®¡ç†ç³»ç»Ÿå’Œæ™ºèƒ½æ•™è‚²å·¥å…·ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªåŠ¨ç”Ÿæˆé—®é¢˜æå‡å­¦ä¹ è€…çš„å‚ä¸åº¦å’ŒçŸ¥è¯†æŒæ¡ã€‚æœªæ¥ï¼Œéšç€å¤šæ¨¡æ€æ•°æ®é›†çš„æ„å»ºï¼Œç ”ç©¶æˆæœæœ‰æœ›åœ¨æ•™è‚²æŠ€æœ¯é¢†åŸŸäº§ç”Ÿæ›´å¹¿æ³›çš„å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Web-based educational videos offer flexible learning opportunities and are becoming increasingly popular. However, improving user engagement and knowledge retention remains a challenge. Automatically generated questions can activate learners and support their knowledge acquisition. Further, they can help teachers and learners assess their understanding. While large language and vision-language models have been employed in various tasks, their application to question generation for educational videos remains underexplored. In this paper, we investigate the capabilities of current vision-language models for generating learning-oriented questions for educational video content. We assess (1) out-of-the-box models' performance; (2) fine-tuning effects on content-specific question generation; (3) the impact of different video modalities on question quality; and (4) in a qualitative study, question relevance, answerability, and difficulty levels of generated questions. Our findings delineate the capabilities of current vision-language models, highlighting the need for fine-tuning and addressing challenges in question diversity and relevance. We identify requirements for future multimodal datasets and outline promising research directions.

