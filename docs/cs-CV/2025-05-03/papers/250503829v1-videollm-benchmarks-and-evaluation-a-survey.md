---
layout: default
title: "VideoLLM Benchmarks and Evaluation: A Survey"
---

# VideoLLM Benchmarks and Evaluation: A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03829" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03829v1</a>
  <a href="https://arxiv.org/pdf/2505.03829.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03829v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03829v1', 'VideoLLM Benchmarks and Evaluation: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yogesh Kumar

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

**å¤‡æ³¨**: 12 pages, 2 Tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„åŸºå‡†ä¸æ–¹æ³•è®ºç»¼è¿°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `å¤§è¯­è¨€æ¨¡å‹` `è¯„ä¼°æ–¹æ³•` `å¤šæ¨¡æ€` `æ—¶åºç†è§£` `åŸºå‡†è®¾è®¡` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰è§†é¢‘ç†è§£åŸºå‡†å­˜åœ¨ç‰¹å¾ä¸ä¸€è‡´ã€è¯„ä¼°åè®®ç¼ºä¹æ ‡å‡†åŒ–ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„å…¨é¢è¯„ä¼°ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–å¤šç§è¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨æå‡è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ•ˆæœã€‚
3. é€šè¿‡å¯¹ç°æœ‰æ¨¡å‹åœ¨ä¸åŒåŸºå‡†ä¸Šçš„è¡¨ç°è¿›è¡Œåˆ†æï¼Œè¯†åˆ«å‡ºå…³é”®æŒ‘æˆ˜ï¼Œå¹¶æå‡ºæ”¹è¿›å»ºè®®ï¼Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè§†é¢‘ç†è§£æŠ€æœ¯ä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬ç»¼è¿°å…¨é¢åˆ†æäº†ä¸“ä¸ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰è®¾è®¡æˆ–ä½¿ç”¨çš„åŸºå‡†å’Œè¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬è€ƒå¯Ÿäº†å½“å‰è§†é¢‘ç†è§£åŸºå‡†çš„ç‰¹ç‚¹ã€è¯„ä¼°åè®®åŠå…¶å±€é™æ€§ï¼Œåˆ†æäº†åŒ…æ‹¬é—­é›†ã€å¼€é›†åŠé’ˆå¯¹æ—¶åºå’Œæ—¶ç©ºç†è§£ä»»åŠ¡çš„ä¸“é—¨è¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡å¼ºè°ƒäº†å½“å‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨è¿™äº›åŸºå‡†ä¸Šçš„æ€§èƒ½è¶‹åŠ¿ï¼Œå¹¶è¯†åˆ«äº†ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä»¥å¢å¼ºåŸºå‡†è®¾è®¡ã€è¯„ä¼°æŒ‡æ ‡å’Œåè®®ï¼Œå¼ºè°ƒäº†å¤šæ ·æ€§ã€å¤šæ¨¡æ€å’Œå¯è§£é‡Šæ€§åŸºå‡†çš„å¿…è¦æ€§ã€‚æœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›æœ‰æ•ˆè¯„ä¼°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–ç†è§£ï¼Œå¹¶è¯†åˆ«æ¨åŠ¨è§†é¢‘ç†è§£é¢†åŸŸçš„æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨åŸºå‡†è®¾è®¡å’Œè¯„ä¼°åè®®ä¸Šç¼ºä¹ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§å—åˆ°å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶é€šè¿‡ç³»ç»ŸåŒ–åˆ†æç°æœ‰è§†é¢‘ç†è§£åŸºå‡†ï¼Œæå‡ºæ”¹è¿›å»ºè®®ï¼Œå¼ºè°ƒå¤šæ¨¡æ€å’Œå¯è§£é‡Šæ€§çš„é‡è¦æ€§ï¼Œä»¥ä¾¿ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æ›´æœ‰æ•ˆçš„è¯„ä¼°å·¥å…·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ç°æœ‰åŸºå‡†çš„åˆ†ç±»ã€è¯„ä¼°æ–¹æ³•çš„æ¯”è¾ƒä»¥åŠå¯¹æœªæ¥ç ”ç©¶æ–¹å‘çš„å»ºè®®ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬åŸºå‡†ç‰¹å¾åˆ†æã€è¯„ä¼°åè®®è®¾è®¡å’Œæ€§èƒ½è¶‹åŠ¿åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†å¤šç§è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å¼ºè°ƒäº†å¤šæ ·æ€§å’Œå¯è§£é‡Šæ€§åœ¨åŸºå‡†è®¾è®¡ä¸­çš„é‡è¦æ€§ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æ€§å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡å…³æ³¨äº†é—­é›†ä¸å¼€é›†è¯„ä¼°çš„è®¾è®¡ï¼Œæå‡ºäº†é’ˆå¯¹æ—¶åºå’Œæ—¶ç©ºç†è§£ä»»åŠ¡çš„ä¸“é—¨è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å»ºè®®ä½¿ç”¨å¤šæ¨¡æ€æ•°æ®æ¥æå‡è¯„ä¼°çš„å…¨é¢æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æ–°æå‡ºçš„åŸºå‡†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶åºç†è§£ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°15%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ”¹è¿›çš„è¯„ä¼°æ¡†æ¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹çš„å®é™…èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘å†…å®¹åˆ†æã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨è§†é¢‘æ‘˜è¦ç”Ÿæˆç­‰ã€‚é€šè¿‡æä¾›æ›´æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶ï¼Œç ”ç©¶å°†æ¨åŠ¨è§†é¢‘ç†è§£æŠ€æœ¯çš„è¿›æ­¥ï¼Œä¿ƒè¿›ç›¸å…³é¢†åŸŸçš„åˆ›æ–°ä¸å‘å±•ï¼Œæå‡è§†é¢‘å¤„ç†çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid development of Large Language Models (LLMs) has catalyzed significant advancements in video understanding technologies. This survey provides a comprehensive analysis of benchmarks and evaluation methodologies specifically designed or used for Video Large Language Models (VideoLLMs). We examine the current landscape of video understanding benchmarks, discussing their characteristics, evaluation protocols, and limitations. The paper analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks. We highlight the performance trends of state-of-the-art VideoLLMs across these benchmarks and identify key challenges in current evaluation frameworks. Additionally, we propose future research directions to enhance benchmark design, evaluation metrics, and protocols, including the need for more diverse, multimodal, and interpretability-focused benchmarks. This survey aims to equip researchers with a structured understanding of how to effectively evaluate VideoLLMs and identify promising avenues for advancing the field of video understanding with large language models.

