---
layout: default
title: Vision and Intention Boost Large Language Model in Long-Term Action Anticipation
---

# Vision and Intention Boost Large Language Model in Long-Term Action Anticipation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01713" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01713v1</a>
  <a href="https://arxiv.org/pdf/2505.01713.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01713v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01713v1', 'Vision and Intention Boost Large Language Model in Long-Term Action Anticipation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Congqi Cao, Lanshu Hu, Yating Yu, Yanning Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ„å›¾æ¡ä»¶è§†è§‰è¯­è¨€æ¨¡å‹ä»¥è§£å†³é•¿æ—¶é—´åŠ¨ä½œé¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ—¶é—´åŠ¨ä½œé¢„æµ‹` `æ„å›¾æ¨æ–­` `å¤šæ¨¡æ€èåˆ` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¡Œä¸ºç†è§£` `ä¿¡æ¯é€‰æ‹©ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è§†é¢‘æ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œç¼ºä¹å¯¹è¡Œä¸ºæ„å›¾çš„ç†è§£ï¼Œå¯¼è‡´é•¿æ—¶é—´åŠ¨ä½œé¢„æµ‹çš„å‡†ç¡®æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„æ„å›¾æ¡ä»¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆICVLï¼‰é€šè¿‡æ¨æ–­è¡Œä¸ºæ„å›¾å¹¶ä¸è§†è§‰ç‰¹å¾èåˆï¼Œæå‡äº†åŠ¨ä½œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚
3. åœ¨Ego4Dã€EPIC-Kitchens-55å’ŒEGTEA GAZE+æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒICVLæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿æ—¶é—´åŠ¨ä½œé¢„æµ‹ï¼ˆLTAï¼‰æ—¨åœ¨é¢„æµ‹æœªæ¥çš„åŠ¨ä½œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è§†é¢‘æ•°æ®å­¦ä¹ ï¼Œç¼ºä¹å…ˆéªŒçŸ¥è¯†ã€‚è¿‘æœŸç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†æ–‡æœ¬è¾“å…¥ï¼Œä½†ä¿¡æ¯æŸå¤±ä¸¥é‡ã€‚ä¸ºè§£å†³å•ä¸€æ¨¡æ€æ–¹æ³•çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ„å›¾æ¡ä»¶è§†è§‰è¯­è¨€ï¼ˆICVLï¼‰æ¨¡å‹ï¼Œå……åˆ†åˆ©ç”¨è§†è§‰æ•°æ®çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯å’ŒLLMsçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»è§†é¢‘è¾“å…¥ä¸­æ¨æ–­è¡Œä¸ºæ„å›¾ï¼Œå¹¶å°†æ¨æ–­çš„æ„å›¾ä¸è§†è§‰ç‰¹å¾èåˆï¼Œç”Ÿæˆå¢å¼ºçš„è§†è§‰è¡¨ç¤ºã€‚è¿™äº›å¢å¼ºçš„è§†è§‰è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºä¸€èµ·è¾“å…¥LLMè¿›è¡Œæœªæ¥åŠ¨ä½œé¢„æµ‹ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ç¤ºä¾‹é€‰æ‹©ç­–ç•¥ï¼Œç»¼åˆè€ƒè™‘è§†è§‰å’Œæ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œä¸ºä¸Šä¸‹æ–‡å­¦ä¹ æä¾›æ›´ç›¸å…³çš„ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Ego4Dã€EPIC-Kitchens-55å’ŒEGTEA GAZE+æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿æ—¶é—´åŠ¨ä½œé¢„æµ‹ä¸­çš„ä¿¡æ¯æŸå¤±å’Œå¯¹è¡Œä¸ºæ„å›¾ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–å•ä¸€æ¨¡æ€ï¼Œç¼ºä¹å¯¹è§†é¢‘å†…å®¹çš„æ·±å±‚æ¬¡ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„ICVLæ¨¡å‹é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨æ–­è¡Œä¸ºæ„å›¾ï¼Œå¹¶å°†å…¶ä¸è§†è§‰ç‰¹å¾èåˆï¼Œä»è€Œå¢å¼ºè§†è§‰è¡¨ç¤ºï¼Œæå‡é¢„æµ‹æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”¨äºæ¨æ–­æ„å›¾ï¼Œæ„å›¾ä¸è§†è§‰ç‰¹å¾çš„å¤šæ¨¡æ€èåˆæ¨¡å—ï¼Œä»¥åŠåŸºäºLLMçš„æœªæ¥åŠ¨ä½œé¢„æµ‹æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†æ„å›¾æ¨æ–­ä¸è§†è§‰ç‰¹å¾èåˆï¼Œå½¢æˆæ„å›¾å¢å¼ºçš„è§†è§‰è¡¨ç¤ºï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç¤ºä¾‹é€‰æ‹©ç­–ç•¥ï¼Œä»¥ç¡®ä¿è¾“å…¥çš„ç›¸å…³æ€§å’Œä¿¡æ¯é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Ego4Dã€EPIC-Kitchens-55å’ŒEGTEA GAZE+æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒICVLæ¨¡å‹åœ¨åŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨é•¿æ—¶é—´åŠ¨ä½œé¢„æµ‹é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºæ™ºèƒ½ç›‘æ§ã€æœºå™¨äººå¯¼èˆªå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡å¯¹æœªæ¥åŠ¨ä½œçš„é¢„æµ‹èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„è¿™äº›ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-term action anticipation (LTA) aims to predict future actions over an extended period. Previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. Recent researches leverage large language models (LLMs) by utilizing text-based inputs which suffer severe information loss. To tackle these limitations single-modality methods face, we propose a novel Intention-Conditioned Vision-Language (ICVL) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of LLMs. Considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (VLM) to infer behavioral intentions as comprehensive textual features directly from video inputs. The inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. These enhanced visual representations, along with textual prompts, are fed into LLM for future action anticipation. Furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. Extensive experiments with state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+ datasets fully demonstrate the effectiveness and superiority of the proposed method.

