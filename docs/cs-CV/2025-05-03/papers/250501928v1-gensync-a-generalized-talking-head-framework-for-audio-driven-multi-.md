---
layout: default
title: "GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting"
---

# GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01928" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01928v1</a>
  <a href="https://arxiv.org/pdf/2505.01928.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01928v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01928v1', 'GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anushka Agarwal, Muhammad Yusuf Hassan, Talha Chafekar

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGenSyncæ¡†æ¶ä»¥è§£å†³å¤šèº«ä»½å£å‹åŒæ­¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å£å‹åŒæ­¥` `è§†é¢‘åˆæˆ` `3Dé«˜æ–¯ç‚¹äº‘` `å¤šèº«ä»½å¤„ç†` `è§£è€¦æ¨¡å—` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Då£å‹åŒæ­¥æ–¹æ³•é€šå¸¸éœ€è¦ä¸ºæ¯ä¸ªèº«ä»½å•ç‹¬è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹å’Œæ•ˆç‡ä½ä¸‹ã€‚
2. GenSyncæ¡†æ¶é€šè¿‡å­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„ç½‘ç»œï¼Œç»“åˆè§£è€¦æ¨¡å—ï¼Œæœ‰æ•ˆåˆ†ç¦»èº«ä»½ç‰¹å¾ä¸éŸ³é¢‘ä¿¡æ¯ï¼Œå®ç°å¤šèº«ä»½å£å‹åŒæ­¥è§†é¢‘åˆæˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGenSyncåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ¯”ç°æœ‰æ¨¡å‹å¿«6.8å€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ°´å¹³çš„å£å‹åŒæ­¥ç²¾åº¦å’Œè§†è§‰è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†GenSyncï¼Œä¸€ä¸ªç”¨äºå¤šèº«ä»½å£å‹åŒæ­¥è§†é¢‘åˆæˆçš„æ–°æ¡†æ¶ï¼Œé‡‡ç”¨3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯ã€‚ä¸å¤§å¤šæ•°ç°æœ‰çš„3Dæ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªèº«ä»½è®­ç»ƒæ–°æ¨¡å‹ä¸åŒï¼ŒGenSyncå­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„ç½‘ç»œï¼Œèƒ½å¤Ÿä¸ºå¤šä¸ªè¯´è¯è€…åˆæˆå£å‹åŒæ­¥è§†é¢‘ã€‚é€šè¿‡å¼•å…¥è§£è€¦æ¨¡å—ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†èº«ä»½ç‰¹å¾ä¸éŸ³é¢‘è¡¨ç¤ºåˆ†ç¦»ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¤šèº«ä»½è§†é¢‘åˆæˆã€‚è¯¥è®¾è®¡å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†6.8å€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å£å‹åŒæ­¥ç²¾åº¦å’Œè§†è§‰è´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Då£å‹åŒæ­¥æ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªèº«ä»½å•ç‹¬è®­ç»ƒæ¨¡å‹çš„é—®é¢˜ï¼Œè¿™å¯¼è‡´äº†è®¡ç®—èµ„æºçš„æµªè´¹å’Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGenSyncé€šè¿‡æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„ç½‘ç»œï¼Œç»“åˆè§£è€¦æ¨¡å—ï¼Œå°†èº«ä»½ç‰¹å¾ä¸éŸ³é¢‘è¡¨ç¤ºåˆ†ç¦»ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¤šèº«ä»½è§†é¢‘åˆæˆã€‚è¿™ç§è®¾è®¡ä½¿å¾—åŒä¸€æ¨¡å‹å¯ä»¥å¤„ç†å¤šä¸ªè¯´è¯è€…ï¼Œé¿å…äº†é‡å¤è®­ç»ƒçš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGenSyncçš„æ•´ä½“æ¶æ„åŒ…æ‹¬éŸ³é¢‘è¾“å…¥ã€è§£è€¦æ¨¡å—å’Œè§†é¢‘åˆæˆæ¨¡å—ã€‚éŸ³é¢‘è¾“å…¥ç»è¿‡è§£è€¦æ¨¡å—å¤„ç†åï¼Œç”Ÿæˆä¸èº«ä»½æ— å…³çš„ç‰¹å¾ï¼Œå†ä¸èº«ä»½ç‰¹å¾ç»“åˆï¼Œæœ€ç»ˆåˆæˆå£å‹åŒæ­¥è§†é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†è§£è€¦æ¨¡å—ï¼Œä½¿å¾—èº«ä»½ç‰¹å¾ä¸éŸ³é¢‘ä¿¡æ¯å¯ä»¥ç‹¬ç«‹å¤„ç†ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºä¸å†éœ€è¦ä¸ºæ¯ä¸ªèº«ä»½è®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼ŒGenSyncé‡‡ç”¨äº†å¤šå±‚å·ç§¯ç½‘ç»œï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å£å‹åŒæ­¥ç²¾åº¦å’Œè§†è§‰è´¨é‡çš„æƒé‡ï¼Œä»¥ç¡®ä¿åˆæˆè§†é¢‘çš„é«˜è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGenSyncåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹å¿«6.8å€ï¼ŒåŒæ—¶åœ¨å£å‹åŒæ­¥ç²¾åº¦å’Œè§†è§‰è´¨é‡ä¸Šä¿æŒäº†é«˜æ°´å¹³ã€‚è¿™ä¸€æ˜¾è‘—æå‡ä½¿å¾—å¤šèº«ä»½è§†é¢‘åˆæˆå˜å¾—æ›´åŠ é«˜æ•ˆå’Œå®ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å½±è§†åˆ¶ä½œã€è™šæ‹Ÿç°å®å’Œåœ¨çº¿æ•™è‚²ç­‰åœºæ™¯ã€‚é€šè¿‡é«˜æ•ˆçš„å¤šèº«ä»½å£å‹åŒæ­¥æŠ€æœ¯ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ä¸ºçœŸå®å’Œæ²‰æµ¸çš„äº’åŠ¨ä½“éªŒï¼Œæå‡å†…å®¹åˆ›ä½œçš„æ•ˆç‡å’Œè´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨ç¤¾äº¤åª’ä½“å’Œæ¸¸æˆç­‰é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨ç›¸å…³è¡Œä¸šçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.

