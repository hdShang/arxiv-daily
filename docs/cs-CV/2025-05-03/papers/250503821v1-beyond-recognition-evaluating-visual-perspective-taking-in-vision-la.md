---
layout: default
title: Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models
---

# Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03821" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03821v1</a>
  <a href="https://arxiv.org/pdf/2505.03821.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03821v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03821v1', 'Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gracjan GÃ³ral, Alicja Ziarko, Piotr MiÅ‚oÅ›, MichaÅ‚ Nauman, Maciej WoÅ‚czyk, MichaÅ‚ KosiÅ„ski

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-03

**å¤‡æ³¨**: Dataset: https://huggingface.co/datasets/Gracjan/Isle/viewer/Isle-Brick-V2

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰è§†è§’ç†è§£è¯„ä¼°æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è§†è§‰è§†è§’ç†è§£` `ç©ºé—´æ¨ç†` `è®¤çŸ¥è¯„ä¼°` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†å’Œè§†è§‰è§†è§’ç†è§£æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨æ˜æ˜¾çš„è®¤çŸ¥èƒ½åŠ›å·®è·ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡è®¾è®¡144ä¸ªè§†è§‰ä»»åŠ¡ï¼Œç³»ç»Ÿè¯„ä¼°VLMsåœ¨è§†è§‰è§†è§’ç†è§£ä¸­çš„è¡¨ç°ï¼Œæå‡ºäº†æ–°çš„è¯„ä¼°æ¡†æ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹åœ¨åœºæ™¯ç†è§£ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œè§†è§’ç†è§£ä¸Šæ˜¾è‘—ä¸‹é™ï¼Œéœ€æ”¹è¿›è®­ç»ƒæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰è§†è§’ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œé‡‡ç”¨äº†ä¸€å¥—æ–°é¢–çš„è§†è§‰ä»»åŠ¡ï¼Œçµæ„Ÿæ¥æºäºå·²å»ºç«‹çš„äººç±»æµ‹è¯•ã€‚é€šè¿‡ç²¾å¿ƒæ§åˆ¶çš„åœºæ™¯ï¼Œå°†å•ä¸ªç±»äººå°äººå’Œå•ä¸ªç‰©ä½“é…å¯¹ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°æ”¹å˜ç©ºé—´é…ç½®ï¼Œåˆ›å»ºäº†144ä¸ªç‹¬ç‰¹çš„è§†è§‰ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡é…æœ‰7ä¸ªè¯Šæ–­é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°åœºæ™¯ç†è§£ã€ç©ºé—´æ¨ç†å’Œè§†è§‰è§†è§’ç†è§£ä¸‰ä¸ªå±‚æ¬¡çš„è§†è§‰è®¤çŸ¥ã€‚å¯¹å¤šç§å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡å®ƒä»¬åœ¨åœºæ™¯ç†è§£æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œè§†è§’ç†è§£ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†è¡¨é¢ç‰©ä½“è¯†åˆ«ä¸å¤æ‚è§†è§‰ä»»åŠ¡æ‰€éœ€çš„æ·±å±‚ç©ºé—´å’Œè§†è§’æ¨ç†ä¹‹é—´çš„å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è§†è§’ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨ç©ºé—´æ¨ç†å’Œè§†è§’ç†è§£ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´è®¤çŸ¥èƒ½åŠ›å·®è·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¾è®¡ä¸€å¥—æ–°é¢–çš„è§†è§‰ä»»åŠ¡ï¼Œç»“åˆç©ºé—´é…ç½®çš„å˜åŒ–ï¼Œç³»ç»Ÿè¯„ä¼°VLMsçš„è§†è§‰è§†è§’ç†è§£èƒ½åŠ›ï¼Œå¼ºè°ƒå‡ ä½•è¡¨ç¤ºå’Œå®šåˆ¶è®­ç»ƒçš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡è®¾è®¡ã€æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ä»»åŠ¡è®¾è®¡ä¸­ï¼Œä½¿ç”¨ç±»äººå°äººå’Œç‰©ä½“çš„ç»„åˆï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†ç©ºé—´é…ç½®å˜åŒ–å’Œå¤šå±‚æ¬¡çš„è®¤çŸ¥é—®é¢˜ï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨è§†è§‰è§†è§’ç†è§£è¯„ä¼°ä¸Šçš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»»åŠ¡è®¾è®¡ä¸­ï¼Œè®¾ç½®äº†144ä¸ªç‹¬ç‰¹çš„è§†è§‰ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªä»»åŠ¡è®¾è®¡äº†7ä¸ªè¯Šæ–­é—®é¢˜ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„è§†è§‰è®¤çŸ¥èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡æ¨¡å‹åœ¨åœºæ™¯ç†è§£ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¾—åˆ†æ¥è¿‘æ»¡åˆ†ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œè§†è§‰è§†è§’ç†è§£æ–¹é¢çš„å¾—åˆ†æ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜åœ¨è¿™ä¸¤ä¸ªé¢†åŸŸçš„æ€§èƒ½å·®è·è¾¾åˆ°30%ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨VLMsä¸­æ•´åˆå‡ ä½•è¡¨ç¤ºçš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿæå‡æœºå™¨å¯¹å¤æ‚åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œæ¨åŠ¨å¤šæ¨¡æ€å­¦ä¹ çš„å‘å±•ã€‚æœªæ¥ï¼Œæ”¹è¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹å¯åœ¨æ›´å¹¿æ³›çš„å®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ·±å±‚æ¬¡ç©ºé—´æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.

