---
layout: default
title: "Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps"
---

# Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.05590" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.05590v1</a>
  <a href="https://arxiv.org/pdf/2511.05590.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.05590v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.05590v1', 'Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yoojin Oh, Junhyug Noh

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-05

**å¤‡æ³¨**: Accepted at BMVC 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/finallyupper/beyond-softmax)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒåˆ†æ”¯Sigmoidæ¶æ„ï¼Œè§£å†³CAMä¸­logitåç§»å’Œç¬¦å·åå¡Œé—®é¢˜ï¼Œæå‡å®šä½ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç±»æ¿€æ´»æ˜ å°„` `å¯è§£é‡Šæ€§` `å¼±ç›‘ç£å®šä½` `åŒåˆ†æ”¯ç½‘ç»œ` `Sigmoidåˆ†ç±»å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸCAMæ–¹æ³•ä¾èµ–Softmaxåˆ†ç±»å™¨ï¼Œå­˜åœ¨logitåç§»å’Œç¬¦å·åå¡Œé—®é¢˜ï¼Œå½±å“å®šä½ç²¾åº¦ã€‚
2. æå‡ºåŒåˆ†æ”¯Sigmoidæ¶æ„ï¼Œè§£è€¦åˆ†ç±»å’Œå®šä½ï¼Œåˆ©ç”¨Sigmoidåˆ†æ”¯ç”Ÿæˆæ›´å‡†ç¡®çš„ç±»æ¿€æ´»å›¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæå‡äº†è§£é‡Šä¿çœŸåº¦å’ŒTop-1å®šä½ç²¾åº¦ï¼Œä¸”ä¸æŸå¤±åˆ†ç±»æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç±»æ¿€æ´»æ˜ å°„(CAM)åŠå…¶æ‰©å±•å·²æˆä¸ºå¯è§†åŒ–æ·±åº¦ç½‘ç»œé¢„æµ‹ä¾æ®çš„é‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œç”±äºä¾èµ–æœ€ç»ˆçš„softmaxåˆ†ç±»å™¨ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªæ ¹æœ¬æ€§çš„å¤±çœŸï¼šä»»æ„åç½®é‡è¦æ€§åˆ†æ•°çš„åŠ æ€§logitåç§»ï¼Œä»¥åŠæ··æ·†äº†å…´å¥‹å’ŒæŠ‘åˆ¶ç‰¹å¾çš„ç¬¦å·åå¡Œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„ã€ä¸æ¶æ„æ— å…³çš„åŒåˆ†æ”¯sigmoidå¤´ï¼Œå°†å®šä½ä¸åˆ†ç±»è§£è€¦ã€‚ç»™å®šä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å°†å…¶åˆ†ç±»å¤´å…‹éš†åˆ°ä¸€ä¸ªä»¥per-class sigmoidè¾“å‡ºç»“æŸçš„å¹¶è¡Œåˆ†æ”¯ä¸­ï¼Œå†»ç»“åŸå§‹çš„softmaxå¤´ï¼Œå¹¶ä»…ä½¿ç”¨ç±»åˆ«å¹³è¡¡çš„äºŒå…ƒç›‘ç£å¾®è°ƒsigmoidåˆ†æ”¯ã€‚åœ¨æ¨ç†æ—¶ï¼Œsoftmaxä¿ç•™è¯†åˆ«ç²¾åº¦ï¼Œè€Œç±»è¯æ®å›¾ä»sigmoidåˆ†æ”¯ç”Ÿæˆâ€”â€”ä¿ç•™äº†ç‰¹å¾è´¡çŒ®çš„å¤§å°å’Œç¬¦å·ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸å¤§å¤šæ•°CAMå˜ä½“æ— ç¼é›†æˆï¼Œå¹¶ä¸”å¼€é”€å¯å¿½ç•¥ä¸è®¡ã€‚åœ¨ç»†ç²’åº¦ä»»åŠ¡(CUB-200-2011, Stanford Cars)å’ŒWSOLåŸºå‡†(ImageNet-1K, OpenImages30K)ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè§£é‡Šä¿çœŸåº¦å¾—åˆ°æ”¹å–„ï¼Œå¹¶ä¸”Top-1å®šä½å§‹ç»ˆè·å¾—æå‡â€”â€”è€Œåˆ†ç±»ç²¾åº¦æ²¡æœ‰ä»»ä½•ä¸‹é™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„ç±»æ¿€æ´»æ˜ å°„ï¼ˆCAMï¼‰æ–¹æ³•ä¾èµ–äºSoftmaxåˆ†ç±»å™¨è¿›è¡Œé¢„æµ‹ï¼Œè¿™å¯¼è‡´ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚ä¸€æ˜¯Softmaxçš„logitåç§»ä¼šä»»æ„åç½®é‡è¦æ€§åˆ†æ•°ï¼Œä½¿å¾—ç”Ÿæˆçš„æ¿€æ´»å›¾ä¸å‡†ç¡®ã€‚äºŒæ˜¯Softmaxå°†å…´å¥‹å’ŒæŠ‘åˆ¶ç‰¹å¾æ··æ·†ï¼Œå¯¼è‡´ç¬¦å·åå¡Œï¼Œæ— æ³•åŒºåˆ†æ­£è´Ÿè´¡çŒ®çš„ç‰¹å¾ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†CAMæ–¹æ³•åœ¨å®šä½å’Œè§£é‡Šæ¨¡å‹é¢„æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åˆ†ç±»å’Œå®šä½è§£è€¦ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªå¹¶è¡Œçš„Sigmoidåˆ†æ”¯ï¼Œä¸“é—¨è´Ÿè´£ç”Ÿæˆç±»æ¿€æ´»å›¾ã€‚Softmaxåˆ†æ”¯ä¿æŒåŸæœ‰çš„åˆ†ç±»åŠŸèƒ½ï¼Œè€ŒSigmoidåˆ†æ”¯åˆ™é€šè¿‡äºŒå…ƒäº¤å‰ç†µæŸå¤±è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ æ¯ä¸ªç±»åˆ«çš„ç‹¬ç«‹æ¿€æ´»å›¾ã€‚è¿™æ ·å¯ä»¥é¿å…Softmaxçš„logitåç§»å’Œç¬¦å·åå¡Œé—®é¢˜ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„ç±»æ¿€æ´»å›¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŒåˆ†æ”¯æ¶æ„ã€‚é¦–å…ˆï¼Œç»™å®šä¸€ä¸ªé¢„è®­ç»ƒçš„åˆ†ç±»æ¨¡å‹ï¼Œå…‹éš†å…¶åˆ†ç±»å¤´ï¼Œåˆ›å»ºä¸€ä¸ªå¹¶è¡Œçš„Sigmoidåˆ†æ”¯ã€‚åŸå§‹çš„Softmaxåˆ†æ”¯ä¿æŒå†»ç»“ï¼Œåªè®­ç»ƒSigmoidåˆ†æ”¯ã€‚Sigmoidåˆ†æ”¯çš„è¾“å‡ºæ˜¯æ¯ä¸ªç±»åˆ«çš„ç‹¬ç«‹æ¦‚ç‡ï¼Œä½¿ç”¨ç±»åˆ«å¹³è¡¡çš„äºŒå…ƒäº¤å‰ç†µæŸå¤±è¿›è¡Œè®­ç»ƒã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒSoftmaxåˆ†æ”¯ç”¨äºåˆ†ç±»ï¼Œè€ŒSigmoidåˆ†æ”¯ç”¨äºç”Ÿæˆç±»æ¿€æ´»å›¾ã€‚è¯¥æ–¹æ³•å¯ä»¥ä¸å¤§å¤šæ•°CAMå˜ä½“æ— ç¼é›†æˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºè§£è€¦äº†åˆ†ç±»å’Œå®šä½ã€‚é€šè¿‡å¼•å…¥ç‹¬ç«‹çš„Sigmoidåˆ†æ”¯ï¼Œé¿å…äº†Softmaxçš„å›ºæœ‰ç¼ºé™·å¯¹ç±»æ¿€æ´»å›¾çš„å½±å“ã€‚è¿™ç§è§£è€¦çš„æ€æƒ³å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦è§£é‡Šæ€§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå®šä½ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç±»åˆ«å¹³è¡¡çš„äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒSigmoidåˆ†æ”¯ï¼Œä»¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚2) å†»ç»“Softmaxåˆ†æ”¯ï¼Œåªè®­ç»ƒSigmoidåˆ†æ”¯ï¼Œä»¥ä¿æŒåˆ†ç±»ç²¾åº¦ã€‚3) Sigmoidåˆ†æ”¯çš„è¾“å‡ºæ˜¯æ¯ä¸ªç±»åˆ«çš„ç‹¬ç«‹æ¦‚ç‡ï¼Œè€Œä¸æ˜¯Softmaxçš„æ¦‚ç‡åˆ†å¸ƒã€‚4) è¯¥æ–¹æ³•ä¸å¤§å¤šæ•°CAMå˜ä½“å…¼å®¹ï¼Œå¯ä»¥æ–¹ä¾¿åœ°åº”ç”¨äºä¸åŒçš„æ¨¡å‹å’Œä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CUB-200-2011ã€Stanford Carsã€ImageNet-1Kå’ŒOpenImages30Kç­‰æ•°æ®é›†ä¸Šï¼Œæ˜¾è‘—æå‡äº†è§£é‡Šä¿çœŸåº¦å’ŒTop-1å®šä½ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†åˆ†ç±»ç²¾åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNet-1Kæ•°æ®é›†ä¸Šï¼ŒTop-1å®šä½ç²¾åº¦æå‡äº†å¤šä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³Softmaxå¸¦æ¥çš„é—®é¢˜ï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„ç±»æ¿€æ´»å›¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦æ¨¡å‹å¯è§£é‡Šæ€§çš„é¢†åŸŸï¼Œä¾‹å¦‚åŒ»å­¦å›¾åƒè¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€å®‰å…¨ç›‘æ§ç­‰ã€‚é€šè¿‡æä¾›æ›´å‡†ç¡®çš„ç±»æ¿€æ´»å›¾ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹çš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºå¼±ç›‘ç£ç›®æ ‡å®šä½ï¼Œæé«˜å®šä½ç²¾åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Class Activation Mapping (CAM) and its extensions have become indispensable tools for visualizing the evidence behind deep network predictions. However, by relying on a final softmax classifier, these methods suffer from two fundamental distortions: additive logit shifts that arbitrarily bias importance scores, and sign collapse that conflates excitatory and inhibitory features. We propose a simple, architecture-agnostic dual-branch sigmoid head that decouples localization from classification. Given any pretrained model, we clone its classification head into a parallel branch ending in per-class sigmoid outputs, freeze the original softmax head, and fine-tune only the sigmoid branch with class-balanced binary supervision. At inference, softmax retains recognition accuracy, while class evidence maps are generated from the sigmoid branch -- preserving both magnitude and sign of feature contributions. Our method integrates seamlessly with most CAM variants and incurs negligible overhead. Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity and consistent Top-1 Localization gains -- without any drop in classification accuracy. Code is available at https://github.com/finallyupper/beyond-softmax.

