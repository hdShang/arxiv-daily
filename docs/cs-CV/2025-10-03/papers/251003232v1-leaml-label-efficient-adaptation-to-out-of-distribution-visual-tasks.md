---
layout: default
title: LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models
---

# LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.03232" target="_blank" class="toolbar-btn">arXiv: 2510.03232v1</a>
    <a href="https://arxiv.org/pdf/2510.03232.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03232v1" 
            onclick="toggleFavorite(this, '2510.03232v1', 'LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ci-Siang Lin, Min-Hung Chen, Yu-Yang Sheng, Yu-Chiang Frank Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**LEAMLÔºöÈù¢ÂêëÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÂÆûÁé∞Ê†áÁ≠æÈ´òÊïàÁöÑÈ¢ÜÂüüÂ§ñËßÜËßâ‰ªªÂä°Ëá™ÈÄÇÂ∫î**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `È¢ÜÂüüËá™ÈÄÇÂ∫î` `Ê†áÁ≠æÈ´òÊïàÂ≠¶‰π†` `‰º™Ê†áÁ≠æÁîüÊàê` `ÈÄâÊã©ÊÄßÁ•ûÁªèÂÖÉÊõ¥Êñ∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÁâπÂÆöÈ¢ÜÂüüÂ§ñÊï∞ÊçÆ‰∏äË°®Áé∞‰∏ç‰Ω≥Ôºå‰∏ªË¶ÅÂéüÂõ†ÊòØÁº∫‰πèÊ†áÊ≥®Êï∞ÊçÆÔºåËé∑ÂèñÊàêÊú¨È´òÊòÇ„ÄÇ
2. LEAMLÊ°ÜÊû∂Âà©Áî®Â∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÂíåÂ§ßÈáèÊú™Ê†áÊ≥®Êï∞ÊçÆÔºåÈÄöËøá‰º™Ê†áÁ≠æÁîüÊàêÂíåÈÄâÊã©ÊÄßÁ•ûÁªèÂÖÉÊõ¥Êñ∞ÂÆûÁé∞È´òÊïàËá™ÈÄÇÂ∫î„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåLEAMLÂú®ËÉÉËÇ†ÂÜÖÁ™•ÈïúÂíå‰ΩìËÇ≤VQA‰ªªÂä°‰∏ä‰ºò‰∫éÊ†áÂáÜÂæÆË∞ÉÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLMs)Âú®ÈÄöÁî®ËßÜËßâÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÂåªÂ≠¶ÊàêÂÉèÁ≠â‰∏ì‰∏öÈ¢ÜÂüüÁöÑÂ§ñÂàÜÂ∏É(OOD)‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåËøô‰∫õÈ¢ÜÂüüÁöÑÊ†áÊ≥®Êï∞ÊçÆÊúâÈôê‰∏îÊòÇË¥µ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜLEAMLÔºå‰∏Ä‰∏™Ê†áÁ≠æÈ´òÊïàÁöÑËá™ÈÄÇÂ∫îÊ°ÜÊû∂ÔºåÂÆÉÂà©Áî®Á®ÄÁº∫ÁöÑÊ†áÊ≥®VQAÊ†∑Êú¨ÂíåÂ§ßÈáèÁöÑÊú™Ê†áÊ≥®ÂõæÂÉè„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÁî®Áî±caption distillationÊ≠£ÂàôÂåñÁöÑQAÁîüÊàêÂô®Ôºå‰∏∫Êú™Ê†áÊ≥®Êï∞ÊçÆÁîüÊàêÈ¢ÜÂüüÁõ∏ÂÖ≥ÁöÑ‰º™ÈóÆÁ≠îÂØπ„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÊàë‰ª¨ÈÄâÊã©ÊÄßÂú∞Êõ¥Êñ∞‰∏éÈóÆÁ≠îÊúÄÁõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÔºå‰ΩøQAÁîüÊàêÂô®ËÉΩÂ§üÂú®Ëí∏È¶èËøáÁ®ã‰∏≠ÊúâÊïàÂú∞Ëé∑ÂèñÈ¢ÜÂüüÁâπÂÆöÁü•ËØÜ„ÄÇÂú®ËÉÉËÇ†ÂÜÖÁ™•ÈïúÂíå‰ΩìËÇ≤VQA‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåLEAMLÂú®ÊúÄÂ∞èÁõëÁù£‰∏ãÂßãÁªà‰ºò‰∫éÊ†áÂáÜÂæÆË∞ÉÔºåÁ™ÅÂá∫‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑLEAMLÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLMs)Âú®ÁâπÂÆöÈ¢ÜÂüüÔºåÁâπÂà´ÊòØÊ†áÊ≥®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÈ¢ÜÂüüÔºåËøõË°åËßÜËßâÈóÆÁ≠î(VQA)‰ªªÂä°Êó∂ÔºåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆËøõË°åÂæÆË∞ÉÔºå‰ΩÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåËé∑ÂèñËøô‰∫õÊï∞ÊçÆÁöÑÊàêÊú¨ÂæàÈ´òÔºåÈôêÂà∂‰∫ÜMLLMsÂú®Ëøô‰∫õÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÈáèÁöÑÊú™Ê†áÊ≥®Êï∞ÊçÆÔºåÈÄöËøáÁîüÊàê‰º™Ê†áÁ≠æÁöÑÊñπÂºèÊù•Êâ©ÂÖÖËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂ÈááÁî®ÈÄâÊã©ÊÄßÊõ¥Êñ∞Á≠ñÁï•ÔºåÂè™Êõ¥Êñ∞‰∏éÈóÆÁ≠î‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÔºå‰ªéËÄåÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•Âú®Â∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑÊù°‰ª∂‰∏ãÔºå‰ΩøÊ®°ÂûãÂø´ÈÄüÈÄÇÂ∫îÊñ∞ÁöÑÈ¢ÜÂüü„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLEAMLÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) QAÁîüÊàêÂô®ÔºöÁî®‰∫éÁîüÊàê‰º™ÈóÆÁ≠îÂØπÔºåËæìÂÖ•ÊòØÊú™Ê†áÊ≥®ÂõæÂÉèÔºåËæìÂá∫ÊòØÈóÆÈ¢òÂíåÁ≠îÊ°à„ÄÇ2) Caption DistillationÔºö‰ΩøÁî®ÂõæÂÉèÊèèËø∞Ê®°ÂûãÁîüÊàêÂõæÂÉèÁöÑÊèèËø∞ÔºåÂπ∂‰ª•Ê≠§Êù•Ê≠£ÂàôÂåñQAÁîüÊàêÂô®Ôºå‰øùËØÅÁîüÊàêÁöÑÈóÆÈ¢òÂíåÁ≠îÊ°à‰∏éÂõæÂÉèÂÜÖÂÆπÁõ∏ÂÖ≥„ÄÇ3) ÈÄâÊã©ÊÄßÁ•ûÁªèÂÖÉÊõ¥Êñ∞ÔºöÂè™Êõ¥Êñ∞‰∏éÈóÆÁ≠î‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÔºåÈÅøÂÖçÊó†ÂÖ≥Á•ûÁªèÂÖÉÁöÑÂπ≤Êâ∞ÔºåÊèêÈ´òËÆ≠ÁªÉÊïàÁéá„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÈ¶ñÂÖà‰ΩøÁî®Caption DistillationÊ≠£ÂàôÂåñQAÁîüÊàêÂô®ÔºåÁÑ∂Âêé‰ΩøÁî®QAÁîüÊàêÂô®‰∏∫Êú™Ê†áÊ≥®Êï∞ÊçÆÁîüÊàê‰º™ÈóÆÁ≠îÂØπÔºåÊúÄÂêé‰ΩøÁî®Ê†áÊ≥®Êï∞ÊçÆÂíå‰º™ÈóÆÁ≠îÂØπÂæÆË∞ÉMLLMÔºåÂπ∂ÈááÁî®ÈÄâÊã©ÊÄßÁ•ûÁªèÂÖÉÊõ¥Êñ∞Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÈÄâÊã©ÊÄßÁ•ûÁªèÂÖÉÊõ¥Êñ∞Á≠ñÁï•„ÄÇ‰º†ÁªüÁöÑÂæÆË∞ÉÊñπÊ≥ï‰ºöÊõ¥Êñ∞ÊâÄÊúâÁ•ûÁªèÂÖÉÔºå‰ΩÜÂú®È¢ÜÂüüËá™ÈÄÇÂ∫î‰ªªÂä°‰∏≠ÔºåÂæàÂ§öÁ•ûÁªèÂÖÉ‰∏éÁõÆÊ†á‰ªªÂä°Êó†ÂÖ≥ÔºåÊõ¥Êñ∞Ëøô‰∫õÁ•ûÁªèÂÖÉÂèçËÄå‰ºöÈôç‰ΩéÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇLEAMLÈÄöËøáÈÄâÊã©ÊÄßÂú∞Êõ¥Êñ∞‰∏éÈóÆÁ≠î‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÔºåÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞Âà©Áî®ÊúâÈôêÁöÑÊ†áÊ≥®Êï∞ÊçÆÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Caption Distillation‰∏≠Ôºå‰ΩøÁî®‰∫Ü‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Êù•Ë°°ÈáèQAÁîüÊàêÂô®ÁîüÊàêÁöÑÈóÆÈ¢òÂíåÁ≠îÊ°à‰∏éÂõæÂÉèÊèèËø∞‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÂú®ÈÄâÊã©ÊÄßÁ•ûÁªèÂÖÉÊõ¥Êñ∞‰∏≠Ôºå‰ΩøÁî®‰∫ÜL1Ê≠£ÂàôÂåñÊù•ÈÄâÊã©‰∏éÈóÆÁ≠î‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂØπ‰∫éÊØè‰∏™Á•ûÁªèÂÖÉÔºåËÆ°ÁÆóÂÖ∂Âú®ÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑÊøÄÊ¥ªÁ®ãÂ∫¶ÔºåÂπ∂Ê†πÊçÆÊøÄÊ¥ªÁ®ãÂ∫¶ÁöÑÂ§ßÂ∞èÊù•ÂÜ≥ÂÆöÊòØÂê¶Êõ¥Êñ∞ËØ•Á•ûÁªèÂÖÉ„ÄÇÊ≠§Â§ñÔºåËøò‰ΩøÁî®‰∫ÜdropoutÁ≠âÊ≠£ÂàôÂåñÊäÄÊúØÊù•Èò≤Ê≠¢ËøáÊãüÂêà„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

LEAMLÂú®ËÉÉËÇ†ÂÜÖÁ™•ÈïúVQAÂíå‰ΩìËÇ≤VQA‰∏§‰∏™‰ªªÂä°‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåLEAMLÂú®Â∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëó‰ºò‰∫éÊ†áÂáÜÂæÆË∞ÉÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®ËÉÉËÇ†ÂÜÖÁ™•ÈïúVQA‰ªªÂä°‰∏≠ÔºåLEAMLÂú®Âè™‰ΩøÁî®10%Ê†áÊ≥®Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåËææÂà∞‰∫Ü‰∏é‰ΩøÁî®ÂÖ®ÈÉ®Ê†áÊ≥®Êï∞ÊçÆËøõË°åÊ†áÂáÜÂæÆË∞ÉÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåLEAMLËøòËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Êú™Ê†áÊ≥®Êï∞ÊçÆÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LEAMLÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê„ÄÅÈÅ•ÊÑüÂõæÂÉèËß£ËØë„ÄÅÂ∑•‰∏öË¥®Ê£ÄÁ≠âÈ¢ÜÂüüÔºåËøô‰∫õÈ¢ÜÂüüÈÄöÂ∏∏Áº∫‰πèÂ§ßÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆ„ÄÇÈÄöËøáÂà©Áî®Êú™Ê†áÊ≥®Êï∞ÊçÆÂíåÂ∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÔºåLEAMLÂèØ‰ª•Â∏ÆÂä©MLLMsÂø´ÈÄüÈÄÇÂ∫îËøô‰∫õÈ¢ÜÂüüÁöÑ‰ªªÂä°ÔºåÊèêÈ´òËØäÊñ≠Á≤æÂ∫¶„ÄÅËß£ËØëÊïàÁéáÂíå‰∫ßÂìÅË¥®ÈáèÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÊΩúÂú®ÁöÑÁ§æ‰ºöÁªèÊµéÊïàÁõä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework.

