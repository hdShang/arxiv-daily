---
layout: default
title: Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes
---

# Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08589" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08589v1</a>
  <a href="https://arxiv.org/pdf/2510.08589.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08589v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08589v1', 'Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nirmal Elamon, Rouzbeh Davoudi

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤šæ¨¡æ€LLMé«˜æ•ˆå¾®è°ƒï¼Œè§£å†³ä½æ•°æ®é‡ä¸‹çš„ç›®æ ‡æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç›®æ ‡æ£€æµ‹` `å¾®è°ƒ` `ä½æ•°æ®å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå¦‚CNNï¼Œè™½ç„¶æœ‰æ•ˆï¼Œä½†ç¼ºä¹åŠ¨æ€ä¸Šä¸‹æ–‡æ¨ç†å’Œæ•´ä½“åœºæ™¯ç†è§£èƒ½åŠ›ã€‚
2. åˆ©ç”¨å¤šæ¨¡æ€LLMï¼Œé€šè¿‡è¯­è¨€å¼•å¯¼æç¤ºï¼Œå¹¶è¿›è¡Œå°‘é‡æ•°æ®å¾®è°ƒï¼Œæå‡ç›®æ ‡æ£€æµ‹ç²¾åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåœ¨å°‘äº1000å¼ å›¾åƒçš„æ•°æ®é›†ä¸Šï¼ŒLLMå¾®è°ƒåç²¾åº¦æå‡é«˜è¾¾36%ï¼Œåª²ç¾ç”šè‡³è¶…è¶ŠCNNã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›®æ ‡æ£€æµ‹å’Œç†è§£é¢†åŸŸæ­£å¿«é€Ÿå‘å±•ï¼Œè¿™å¾—ç›Šäºä¼ ç»ŸCNNæ¨¡å‹å’Œæ–°å…´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„è¿›æ­¥ã€‚è™½ç„¶ResNetå’ŒYOLOç­‰CNNåœ¨å›¾åƒä»»åŠ¡ä¸­ä»ç„¶éå¸¸æœ‰æ•ˆï¼Œä½†åŸºäºTransformerçš„LLMå¼•å…¥äº†åŠ¨æ€ä¸Šä¸‹æ–‡æ¨ç†ã€è¯­è¨€å¼•å¯¼æç¤ºå’Œæ•´ä½“åœºæ™¯ç†è§£ç­‰æ–°åŠŸèƒ½ã€‚ç„¶è€Œï¼Œå¼€ç®±å³ç”¨çš„LLMçš„å…¨éƒ¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œé€šå¸¸å¯¼è‡´åœ¨ä¸“é—¨çš„è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ã€‚æœ¬æ–‡å¯¹å¾®è°ƒçš„ä¼ ç»ŸCNNã€é›¶æ ·æœ¬é¢„è®­ç»ƒå¤šæ¨¡æ€LLMå’Œå¾®è°ƒå¤šæ¨¡æ€LLMåœ¨å›¾åƒä¸­äººå·¥æ–‡æœ¬å åŠ æ£€æµ‹è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚æœ¬ç ”ç©¶çš„ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯è¯æ˜äº†LLMå¯ä»¥åœ¨éå¸¸æœ‰é™çš„æ•°æ®ï¼ˆå°‘äº1000å¼ å›¾åƒï¼‰ä¸Šè¿›è¡Œæœ‰æ•ˆå¾®è°ƒï¼Œä»¥å®ç°é«˜è¾¾36%çš„ç²¾åº¦æå‡ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡é€šå¸¸éœ€è¦æ›´å¤šæ•°é‡çº§æ•°æ®çš„åŸºäºCNNçš„åŸºçº¿ã€‚é€šè¿‡æ¢ç´¢å¦‚ä½•è°ƒæ•´è¯­è¨€å¼•å¯¼æ¨¡å‹ä»¥å®ç°ç²¾ç¡®çš„è§†è§‰ç†è§£å’Œæœ€å°‘çš„ç›‘ç£ï¼Œæˆ‘ä»¬çš„å·¥ä½œæœ‰åŠ©äºå¼¥åˆè§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„å·®è·ï¼Œä¸ºé«˜æ•ˆçš„è·¨æ¨¡æ€å­¦ä¹ ç­–ç•¥æä¾›æ–°çš„è§è§£ã€‚è¿™äº›å‘ç°çªå‡ºäº†åŸºäºLLMçš„æ–¹æ³•åœ¨å®é™…ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ•°æ®æ•ˆç‡ï¼Œå¹¶ä¸ºåœ¨ä½èµ„æºè§†è§‰ç¯å¢ƒä¸­åº”ç”¨å¤šæ¨¡æ€Transformeræä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼ã€‚ä¸ºäº†æ”¯æŒè¯¥é¢†åŸŸçš„æŒç»­è¿›æ­¥ï¼Œæˆ‘ä»¬å·²åœ¨GitHubä¸­æä¾›äº†ç”¨äºå¾®è°ƒæ¨¡å‹çš„ä»£ç ï¼Œä»è€Œå¯ä»¥åœ¨ç›¸å…³åº”ç”¨ä¸­è¿›è¡Œæœªæ¥çš„æ”¹è¿›å’Œé‡ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä½æ•°æ®é‡ä¸‹ç›®æ ‡æ£€æµ‹ç²¾åº¦ä¸é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚CNNï¼Œéœ€è¦å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¸”ç¼ºä¹å¯¹ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ã€‚å¤šæ¨¡æ€LLMè™½ç„¶å…·æœ‰æ½œåŠ›ï¼Œä½†ç›´æ¥åº”ç”¨æ•ˆæœä¸ä½³ï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€LLMçš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œç»“åˆå°‘é‡ç›®æ ‡æ£€æµ‹æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæå‡æ¨¡å‹åœ¨ä½æ•°æ®é‡ä¸‹çš„ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚é€šè¿‡è¯­è¨€å¼•å¯¼ï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°ç†è§£å›¾åƒå†…å®¹å’Œç›®æ ‡ä¹‹é—´çš„å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ï¼š1ï¼‰é€‰æ‹©é¢„è®­ç»ƒçš„å¤šæ¨¡æ€LLMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼›2ï¼‰æ„å»ºåŒ…å«å›¾åƒå’Œæ–‡æœ¬æè¿°çš„æ•°æ®é›†ï¼›3ï¼‰ä½¿ç”¨å°‘é‡æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼›4ï¼‰è¯„ä¼°å¾®è°ƒåæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºè¯æ˜äº†å¤šæ¨¡æ€LLMåœ¨å°‘é‡æ•°æ®ä¸‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡ç›®æ ‡æ£€æµ‹ç²¾åº¦ï¼Œå¹¶è¾¾åˆ°æˆ–è¶…è¿‡ä¼ ç»ŸCNNæ–¹æ³•ã€‚è¿™è¡¨æ˜LLMå…·æœ‰å¼ºå¤§çš„è¿ç§»å­¦ä¹ èƒ½åŠ›å’Œæ•°æ®æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒå¤šæ¨¡æ€LLMï¼Œä¾‹å¦‚CLIPæˆ–ç±»ä¼¼æ¨¡å‹ï¼›2ï¼‰è®¾è®¡åˆé€‚çš„æ–‡æœ¬æç¤ºï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ç›®æ ‡åŒºåŸŸï¼›3ï¼‰ä½¿ç”¨åˆé€‚çš„æŸå¤±å‡½æ•°è¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚äº¤å‰ç†µæŸå¤±æˆ–Focal Lossï¼›4ï¼‰æ¢ç´¢ä¸åŒçš„å¾®è°ƒç­–ç•¥ï¼Œä¾‹å¦‚åªå¾®è°ƒéƒ¨åˆ†å‚æ•°æˆ–ä½¿ç”¨Adapteræ¨¡å—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨äººå·¥æ–‡æœ¬å åŠ æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨å°‘äº1000å¼ å›¾åƒçš„æ•°æ®é›†å¯¹å¤šæ¨¡æ€LLMè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥å®ç°é«˜è¾¾36%çš„ç²¾åº¦æå‡ã€‚å¾®è°ƒåçš„LLMæ€§èƒ½ä¸éœ€è¦å¤§é‡æ•°æ®çš„CNNåŸºçº¿æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½ã€‚è¿™çªå‡ºäº†LLMåœ¨ä½æ•°æ®é‡åœºæ™¯ä¸‹çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§ä½èµ„æºè§†è§‰åœºæ™¯ï¼Œå¦‚åŒ»å­¦å›¾åƒåˆ†æã€é¥æ„Ÿå›¾åƒè§£è¯‘ã€å·¥ä¸šç¼ºé™·æ£€æµ‹ç­‰ã€‚é€šè¿‡å°‘é‡æ ‡æ³¨æ•°æ®ï¼Œå³å¯è®­ç»ƒå‡ºé«˜æ€§èƒ½çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œé™ä½äº†æ•°æ®æ ‡æ³¨æˆæœ¬ï¼ŒåŠ é€Ÿäº†æ¨¡å‹éƒ¨ç½²ã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢æ›´é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥å’Œæ›´å¼ºå¤§çš„å¤šæ¨¡æ€LLMï¼Œæ‹“å±•åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The field of object detection and understanding is rapidly evolving, driven by advances in both traditional CNN-based models and emerging multi-modal large language models (LLMs). While CNNs like ResNet and YOLO remain highly effective for image-based tasks, recent transformer-based LLMs introduce new capabilities such as dynamic context reasoning, language-guided prompts, and holistic scene understanding. However, when used out-of-the-box, the full potential of LLMs remains underexploited, often resulting in suboptimal performance on specialized visual tasks. In this work, we conduct a comprehensive comparison of fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and fine-tuned multi-modal LLMs on the challenging task of artificial text overlay detection in images. A key contribution of our study is demonstrating that LLMs can be effectively fine-tuned on very limited data (fewer than 1,000 images) to achieve up to 36% accuracy improvement, matching or surpassing CNN-based baselines that typically require orders of magnitude more data. By exploring how language-guided models can be adapted for precise visual understanding with minimal supervision, our work contributes to the broader effort of bridging vision and language, offering novel insights into efficient cross-modal learning strategies. These findings highlight the adaptability and data efficiency of LLM-based approaches for real-world object detection tasks and provide actionable guidance for applying multi-modal transformers in low-resource visual environments. To support continued progress in this area, we have made the code used to fine-tune the models available in our GitHub, enabling future improvements and reuse in related applications.

