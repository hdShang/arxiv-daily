---
layout: default
title: Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes
---

# Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08589" target="_blank" class="toolbar-btn">arXiv: 2510.08589v1</a>
    <a href="https://arxiv.org/pdf/2510.08589.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08589v1" 
            onclick="toggleFavorite(this, '2510.08589v1', 'Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Nirmal Elamon, Rouzbeh Davoudi

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-03

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Âà©Áî®Â§öÊ®°ÊÄÅLLMÈ´òÊïàÂæÆË∞ÉÔºåËß£ÂÜ≥‰ΩéÊï∞ÊçÆÈáè‰∏ãÁöÑÁõÆÊ†áÊ£ÄÊµãÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÁõÆÊ†áÊ£ÄÊµã` `ÂæÆË∞É` `‰ΩéÊï∞ÊçÆÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïÔºåÂ¶ÇCNNÔºåËôΩÁÑ∂ÊúâÊïàÔºå‰ΩÜÁº∫‰πèÂä®ÊÄÅ‰∏ä‰∏ãÊñáÊé®ÁêÜÂíåÊï¥‰ΩìÂú∫ÊôØÁêÜËß£ËÉΩÂäõ„ÄÇ
2. Âà©Áî®Â§öÊ®°ÊÄÅLLMÔºåÈÄöËøáËØ≠Ë®ÄÂºïÂØºÊèêÁ§∫ÔºåÂπ∂ËøõË°åÂ∞ëÈáèÊï∞ÊçÆÂæÆË∞ÉÔºåÊèêÂçáÁõÆÊ†áÊ£ÄÊµãÁ≤æÂ∫¶„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂú®Â∞ë‰∫é1000Âº†ÂõæÂÉèÁöÑÊï∞ÊçÆÈõÜ‰∏äÔºåLLMÂæÆË∞ÉÂêéÁ≤æÂ∫¶ÊèêÂçáÈ´òËææ36%ÔºåÂ™≤ÁæéÁîöËá≥Ë∂ÖË∂äCNN„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁõÆÊ†áÊ£ÄÊµãÂíåÁêÜËß£È¢ÜÂüüÊ≠£Âø´ÈÄüÂèëÂ±ïÔºåËøôÂæóÁõä‰∫é‰º†ÁªüCNNÊ®°ÂûãÂíåÊñ∞ÂÖ¥Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)ÁöÑËøõÊ≠•„ÄÇËôΩÁÑ∂ResNetÂíåYOLOÁ≠âCNNÂú®ÂõæÂÉè‰ªªÂä°‰∏≠‰ªçÁÑ∂ÈùûÂ∏∏ÊúâÊïàÔºå‰ΩÜÂü∫‰∫éTransformerÁöÑLLMÂºïÂÖ•‰∫ÜÂä®ÊÄÅ‰∏ä‰∏ãÊñáÊé®ÁêÜ„ÄÅËØ≠Ë®ÄÂºïÂØºÊèêÁ§∫ÂíåÊï¥‰ΩìÂú∫ÊôØÁêÜËß£Á≠âÊñ∞ÂäüËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂºÄÁÆ±Âç≥Áî®ÁöÑLLMÁöÑÂÖ®ÈÉ®ÊΩúÂäõÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÂà©Áî®ÔºåÈÄöÂ∏∏ÂØºËá¥Âú®‰∏ìÈó®ÁöÑËßÜËßâ‰ªªÂä°‰∏≠Ë°®Áé∞Ê¨†‰Ω≥„ÄÇÊú¨ÊñáÂØπÂæÆË∞ÉÁöÑ‰º†ÁªüCNN„ÄÅÈõ∂Ê†∑Êú¨È¢ÑËÆ≠ÁªÉÂ§öÊ®°ÊÄÅLLMÂíåÂæÆË∞ÉÂ§öÊ®°ÊÄÅLLMÂú®ÂõæÂÉè‰∏≠‰∫∫Â∑•ÊñáÊú¨Âè†Âä†Ê£ÄÊµãËøô‰∏ÄÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°‰∏äËøõË°å‰∫ÜÂÖ®Èù¢ÊØîËæÉ„ÄÇÊú¨Á†îÁ©∂ÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆË¥°ÁåÆÊòØËØÅÊòé‰∫ÜLLMÂèØ‰ª•Âú®ÈùûÂ∏∏ÊúâÈôêÁöÑÊï∞ÊçÆÔºàÂ∞ë‰∫é1000Âº†ÂõæÂÉèÔºâ‰∏äËøõË°åÊúâÊïàÂæÆË∞ÉÔºå‰ª•ÂÆûÁé∞È´òËææ36%ÁöÑÁ≤æÂ∫¶ÊèêÂçáÔºåËææÂà∞ÊàñË∂ÖËøáÈÄöÂ∏∏ÈúÄË¶ÅÊõ¥Â§öÊï∞ÈáèÁ∫ßÊï∞ÊçÆÁöÑÂü∫‰∫éCNNÁöÑÂü∫Á∫ø„ÄÇÈÄöËøáÊé¢Á¥¢Â¶Ç‰ΩïË∞ÉÊï¥ËØ≠Ë®ÄÂºïÂØºÊ®°Âûã‰ª•ÂÆûÁé∞Á≤æÁ°ÆÁöÑËßÜËßâÁêÜËß£ÂíåÊúÄÂ∞ëÁöÑÁõëÁù£ÔºåÊàë‰ª¨ÁöÑÂ∑•‰ΩúÊúâÂä©‰∫éÂº•ÂêàËßÜËßâÂíåËØ≠Ë®Ä‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰∏∫È´òÊïàÁöÑË∑®Ê®°ÊÄÅÂ≠¶‰π†Á≠ñÁï•Êèê‰æõÊñ∞ÁöÑËßÅËß£„ÄÇËøô‰∫õÂèëÁé∞Á™ÅÂá∫‰∫ÜÂü∫‰∫éLLMÁöÑÊñπÊ≥ïÂú®ÂÆûÈôÖÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÊï∞ÊçÆÊïàÁéáÔºåÂπ∂‰∏∫Âú®‰ΩéËµÑÊ∫êËßÜËßâÁéØÂ¢É‰∏≠Â∫îÁî®Â§öÊ®°ÊÄÅTransformerÊèê‰æõ‰∫ÜÂèØÊìç‰ΩúÁöÑÊåáÂØº„ÄÇ‰∏∫‰∫ÜÊîØÊåÅËØ•È¢ÜÂüüÁöÑÊåÅÁª≠ËøõÊ≠•ÔºåÊàë‰ª¨Â∑≤Âú®GitHub‰∏≠Êèê‰æõ‰∫ÜÁî®‰∫éÂæÆË∞ÉÊ®°ÂûãÁöÑ‰ª£Á†ÅÔºå‰ªéËÄåÂèØ‰ª•Âú®Áõ∏ÂÖ≥Â∫îÁî®‰∏≠ËøõË°åÊú™Êù•ÁöÑÊîπËøõÂíåÈáçÁî®„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰ΩéÊï∞ÊçÆÈáè‰∏ãÁõÆÊ†áÊ£ÄÊµãÁ≤æÂ∫¶‰∏çÈ´òÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇCNNÔºåÈúÄË¶ÅÂ§ßÈáèÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºå‰∏îÁº∫‰πèÂØπ‰∏ä‰∏ãÊñáÂíåËØ≠‰πâ‰ø°ÊÅØÁöÑÊúâÊïàÂà©Áî®„ÄÇÂ§öÊ®°ÊÄÅLLMËôΩÁÑ∂ÂÖ∑ÊúâÊΩúÂäõÔºå‰ΩÜÁõ¥Êé•Â∫îÁî®ÊïàÊûú‰∏ç‰Ω≥ÔºåÈúÄË¶ÅÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°å‰ºòÂåñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§öÊ®°ÊÄÅLLMÁöÑËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõÔºåÁªìÂêàÂ∞ëÈáèÁõÆÊ†áÊ£ÄÊµãÊï∞ÊçÆËøõË°åÂæÆË∞ÉÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÂú®‰ΩéÊï∞ÊçÆÈáè‰∏ãÁöÑÁõÆÊ†áÊ£ÄÊµãÊÄßËÉΩ„ÄÇÈÄöËøáËØ≠Ë®ÄÂºïÂØºÔºå‰ΩøÊ®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£ÂõæÂÉèÂÜÖÂÆπÂíåÁõÆÊ†á‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨Ôºö1ÔºâÈÄâÊã©È¢ÑËÆ≠ÁªÉÁöÑÂ§öÊ®°ÊÄÅLLM‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºõ2ÔºâÊûÑÂª∫ÂåÖÂê´ÂõæÂÉèÂíåÊñáÊú¨ÊèèËø∞ÁöÑÊï∞ÊçÆÈõÜÔºõ3Ôºâ‰ΩøÁî®Â∞ëÈáèÊï∞ÊçÆÂØπLLMËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°Ôºõ4ÔºâËØÑ‰º∞ÂæÆË∞ÉÂêéÊ®°ÂûãÂú®ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éËØÅÊòé‰∫ÜÂ§öÊ®°ÊÄÅLLMÂú®Â∞ëÈáèÊï∞ÊçÆ‰∏ãËøõË°åÂæÆË∞ÉÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáÁõÆÊ†áÊ£ÄÊµãÁ≤æÂ∫¶ÔºåÂπ∂ËææÂà∞ÊàñË∂ÖËøá‰º†ÁªüCNNÊñπÊ≥ï„ÄÇËøôË°®ÊòéLLMÂÖ∑ÊúâÂº∫Â§ßÁöÑËøÅÁßªÂ≠¶‰π†ËÉΩÂäõÂíåÊï∞ÊçÆÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1ÔºâÈÄâÊã©ÂêàÈÄÇÁöÑÈ¢ÑËÆ≠ÁªÉÂ§öÊ®°ÊÄÅLLMÔºå‰æãÂ¶ÇCLIPÊàñÁ±ª‰ººÊ®°ÂûãÔºõ2ÔºâËÆæËÆ°ÂêàÈÄÇÁöÑÊñáÊú¨ÊèêÁ§∫ÔºåÂºïÂØºÊ®°ÂûãÂÖ≥Ê≥®ÁõÆÊ†áÂå∫ÂüüÔºõ3Ôºâ‰ΩøÁî®ÂêàÈÄÇÁöÑÊçüÂ§±ÂáΩÊï∞ËøõË°åÂæÆË∞ÉÔºå‰æãÂ¶Ç‰∫§ÂèâÁÜµÊçüÂ§±ÊàñFocal LossÔºõ4ÔºâÊé¢Á¥¢‰∏çÂêåÁöÑÂæÆË∞ÉÁ≠ñÁï•Ôºå‰æãÂ¶ÇÂè™ÂæÆË∞ÉÈÉ®ÂàÜÂèÇÊï∞Êàñ‰ΩøÁî®AdapterÊ®°Âùó„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®‰∫∫Â∑•ÊñáÊú¨Âè†Âä†Ê£ÄÊµã‰ªªÂä°‰∏≠Ôºå‰ΩøÁî®Â∞ë‰∫é1000Âº†ÂõæÂÉèÁöÑÊï∞ÊçÆÈõÜÂØπÂ§öÊ®°ÊÄÅLLMËøõË°åÂæÆË∞ÉÔºåÂèØ‰ª•ÂÆûÁé∞È´òËææ36%ÁöÑÁ≤æÂ∫¶ÊèêÂçá„ÄÇÂæÆË∞ÉÂêéÁöÑLLMÊÄßËÉΩ‰∏éÈúÄË¶ÅÂ§ßÈáèÊï∞ÊçÆÁöÑCNNÂü∫Á∫øÊ®°ÂûãÁõ∏ÂΩìÁîöËá≥Êõ¥Â•Ω„ÄÇËøôÁ™ÅÂá∫‰∫ÜLLMÂú®‰ΩéÊï∞ÊçÆÈáèÂú∫ÊôØ‰∏ãÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ§öÁßç‰ΩéËµÑÊ∫êËßÜËßâÂú∫ÊôØÔºåÂ¶ÇÂåªÂ≠¶ÂõæÂÉèÂàÜÊûê„ÄÅÈÅ•ÊÑüÂõæÂÉèËß£ËØë„ÄÅÂ∑•‰∏öÁº∫Èô∑Ê£ÄÊµãÁ≠â„ÄÇÈÄöËøáÂ∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÔºåÂç≥ÂèØËÆ≠ÁªÉÂá∫È´òÊÄßËÉΩÁöÑÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÔºåÈôç‰Ωé‰∫ÜÊï∞ÊçÆÊ†áÊ≥®ÊàêÊú¨ÔºåÂä†ÈÄü‰∫ÜÊ®°ÂûãÈÉ®ÁΩ≤„ÄÇÊú™Êù•ÂèØËøõ‰∏ÄÊ≠•Êé¢Á¥¢Êõ¥È´òÊïàÁöÑÂæÆË∞ÉÁ≠ñÁï•ÂíåÊõ¥Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅLLMÔºåÊãìÂ±ïÂ∫îÁî®ËåÉÂõ¥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The field of object detection and understanding is rapidly evolving, driven by advances in both traditional CNN-based models and emerging multi-modal large language models (LLMs). While CNNs like ResNet and YOLO remain highly effective for image-based tasks, recent transformer-based LLMs introduce new capabilities such as dynamic context reasoning, language-guided prompts, and holistic scene understanding. However, when used out-of-the-box, the full potential of LLMs remains underexploited, often resulting in suboptimal performance on specialized visual tasks. In this work, we conduct a comprehensive comparison of fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and fine-tuned multi-modal LLMs on the challenging task of artificial text overlay detection in images. A key contribution of our study is demonstrating that LLMs can be effectively fine-tuned on very limited data (fewer than 1,000 images) to achieve up to 36% accuracy improvement, matching or surpassing CNN-based baselines that typically require orders of magnitude more data. By exploring how language-guided models can be adapted for precise visual understanding with minimal supervision, our work contributes to the broader effort of bridging vision and language, offering novel insights into efficient cross-modal learning strategies. These findings highlight the adaptability and data efficiency of LLM-based approaches for real-world object detection tasks and provide actionable guidance for applying multi-modal transformers in low-resource visual environments. To support continued progress in this area, we have made the code used to fine-tune the models available in our GitHub, enabling future improvements and reuse in related applications.

