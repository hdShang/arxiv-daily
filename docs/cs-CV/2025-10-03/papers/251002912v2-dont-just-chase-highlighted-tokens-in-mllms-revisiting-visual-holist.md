---
layout: default
title: Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention
---

# Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02912" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02912v2</a>
  <a href="https://arxiv.org/pdf/2510.02912.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02912v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02912v2', 'Don\'t Just Chase &quot;Highlighted Tokens&quot; in MLLMs: Revisiting Visual Holistic Context Retention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Zou, Di Lu, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Xu Zheng, Linfeng Zhang, Xuming Hu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-03 (æ›´æ–°: 2025-10-10)

**å¤‡æ³¨**: Accepted by NeurIPS 2025 main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HoloVï¼šä¸€ç§è§†è§‰tokenå‰ªææ¡†æ¶ï¼Œé€šè¿‡å…¨å±€ä¸Šä¸‹æ–‡ä¿ç•™æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰tokenå‰ªæ` `å…¨å±€ä¸Šä¸‹æ–‡` `æ¨¡å‹æ•ˆç‡` `LLaVA1.5`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMçš„tokenå‰ªææ–¹æ³•å€¾å‘äºä¿ç•™è¯­ä¹‰ç›¸ä¼¼çš„tokensï¼Œå¯¼è‡´é«˜å‰ªæç‡ä¸‹æ€§èƒ½ä¸‹é™ã€‚
2. HoloVé€šè¿‡è‡ªé€‚åº”åœ°åœ¨ä¸åŒç©ºé—´åŒºåŸŸåˆ†é…å‰ªæé¢„ç®—ï¼Œç¡®ä¿ä¿ç•™tokensæ•è·å…¨å±€è§†è§‰ä¸Šä¸‹æ–‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHoloVåœ¨å„ç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šä¼˜äºSOTAæ–¹æ³•ï¼Œå®ç°äº†æ›´å¥½çš„æ•ˆç‡-ç²¾åº¦å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ç”±äºä¾èµ–å¤§é‡çš„è§†è§‰tokensè€Œé¢ä¸´å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†tokenå‰ªæï¼Œé€šå¸¸ä½¿ç”¨æ–‡æœ¬-è§†è§‰äº¤å‰æ³¨æ„åŠ›æˆ–[	exttt{CLS}]æ³¨æ„åŠ›æ¥è¯„ä¼°å’Œä¸¢å¼ƒå†—ä½™çš„è§†è§‰tokensã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œè¿™ç§ä»¥æ³¨æ„åŠ›ä¸ºå…ˆçš„å‰ªææ–¹æ³•å­˜åœ¨ä¸€ä¸ªå…³é”®é™åˆ¶ï¼Œå³å®ƒä»¬å€¾å‘äºä¿ç•™è¯­ä¹‰ç›¸ä¼¼çš„tokensï¼Œå¯¼è‡´åœ¨é«˜å‰ªæç‡ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†HoloVï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„ã€å³æ’å³ç”¨çš„è§†è§‰tokenå‰ªææ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆæ¨ç†ã€‚ä¸ä»¥å¾€çš„ä»¥æ³¨æ„åŠ›ä¸ºå…ˆçš„æ–¹æ¡ˆä¸åŒï¼ŒHoloVä»æ•´ä½“çš„è§’åº¦é‡æ–°æ€è€ƒtokenä¿ç•™ã€‚é€šè¿‡è‡ªé€‚åº”åœ°åœ¨ä¸åŒçš„ç©ºé—´è£å‰ªåŒºåŸŸåˆ†é…å‰ªæé¢„ç®—ï¼ŒHoloVç¡®ä¿ä¿ç•™çš„tokensæ•è·å…¨å±€è§†è§‰ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„æ˜¾è‘—ç‰¹å¾ã€‚è¿™ç§ç­–ç•¥æœ€å¤§é™åº¦åœ°å‡å°‘äº†è¡¨å¾å´©æºƒï¼Œå¹¶åœ¨é«˜å¼ºåº¦å‰ªæä¸‹ä¿æŒäº†ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸SOTAæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„HoloVåœ¨å„ç§ä»»åŠ¡ã€MLLMæ¶æ„å’Œå‰ªæç‡ä¸‹éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œé…å¤‡HoloVçš„LLaVA1.5åœ¨å‰ªæ88.9%çš„è§†è§‰tokensåï¼Œä»èƒ½ä¿æŒåŸå§‹æ€§èƒ½çš„95.8%ï¼Œå®ç°äº†å“è¶Šçš„æ•ˆç‡-ç²¾åº¦æƒè¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­è§†è§‰tokenæ•°é‡åºå¤§å¯¼è‡´çš„è®¡ç®—å¼€é”€é—®é¢˜ã€‚ç°æœ‰çš„tokenå‰ªææ–¹æ³•ï¼Œå¦‚åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ–¹æ³•ï¼Œå¾€å¾€ä¼šä¿ç•™è¯­ä¹‰ç›¸ä¼¼çš„tokenï¼Œåœ¨é«˜å‰ªæç‡ä¸‹é€ æˆä¸¥é‡çš„æ€§èƒ½ä¸‹é™ï¼Œæ— æ³•æœ‰æ•ˆæå–å…¨å±€è§†è§‰ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHoloVçš„æ ¸å¿ƒæ€è·¯æ˜¯ä»å…¨å±€è§†è§’å‡ºå‘è¿›è¡Œtokenå‰ªæï¼Œé¿å…åªå…³æ³¨å±€éƒ¨æ˜¾è‘—ç‰¹å¾ã€‚é€šè¿‡åœ¨ä¸åŒçš„ç©ºé—´åŒºåŸŸè‡ªé€‚åº”åœ°åˆ†é…å‰ªæé¢„ç®—ï¼Œç¡®ä¿ä¿ç•™çš„tokenèƒ½å¤Ÿæ•æ‰åˆ°å›¾åƒçš„æ•´ä½“ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå‡å°‘ä¿¡æ¯æŸå¤±ï¼Œç»´æŒæ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHoloVæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„è§†è§‰tokenå‰ªææ¡†æ¶ï¼Œå¯ä»¥åº”ç”¨äºå„ç§MLLMæ¶æ„ã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) å°†è¾“å…¥å›¾åƒåˆ’åˆ†ä¸ºå¤šä¸ªç©ºé—´åŒºåŸŸï¼ˆcropsï¼‰ï¼›2) æ ¹æ®æ¯ä¸ªåŒºåŸŸçš„é‡è¦æ€§è‡ªé€‚åº”åœ°åˆ†é…å‰ªæé¢„ç®—ï¼›3) æ ¹æ®åˆ†é…çš„é¢„ç®—ï¼Œå¯¹æ¯ä¸ªåŒºåŸŸå†…çš„tokenè¿›è¡Œå‰ªæï¼›4) å°†ä¿ç•™çš„tokenè¾“å…¥åˆ°MLLMä¸­è¿›è¡Œåç»­å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šHoloVçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…¨å±€ä¸Šä¸‹æ–‡ä¿ç•™çš„å‰ªæç­–ç•¥ã€‚ä¸ä»¥å¾€åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å±€éƒ¨å‰ªææ–¹æ³•ä¸åŒï¼ŒHoloVè€ƒè™‘äº†å›¾åƒçš„æ•´ä½“ç»“æ„å’Œä¸åŒåŒºåŸŸä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿ç•™ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚è¿™ç§å…¨å±€è§†è§’é¿å…äº†æ¨¡å‹è¿‡åº¦å…³æ³¨å±€éƒ¨æ˜¾è‘—ç‰¹å¾ï¼Œä»è€Œåœ¨é«˜å‰ªæç‡ä¸‹ä¹Ÿèƒ½ä¿æŒè¾ƒå¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šHoloVçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è‡ªé€‚åº”å‰ªæé¢„ç®—åˆ†é…ç­–ç•¥ï¼šæ ¹æ®æ¯ä¸ªç©ºé—´åŒºåŸŸçš„é‡è¦æ€§ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºæ³¨æ„åŠ›å¾—åˆ†æˆ–åŒºåŸŸå†…çš„ä¿¡æ¯ç†µï¼‰åŠ¨æ€è°ƒæ•´å‰ªæé¢„ç®—ã€‚2) ç©ºé—´åŒºåŸŸåˆ’åˆ†ç­–ç•¥ï¼šé‡‡ç”¨ä¸åŒçš„åˆ’åˆ†æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œå‡åŒ€åˆ’åˆ†ã€åŸºäºæ˜¾è‘—æ€§åˆ’åˆ†ï¼‰æ¥é€‚åº”ä¸åŒçš„å›¾åƒå’Œä»»åŠ¡ã€‚3) å‰ªæå‡†åˆ™ï¼šå¯ä»¥ä½¿ç”¨ä¸åŒçš„å‰ªæå‡†åˆ™ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºæ³¨æ„åŠ›å¾—åˆ†ã€tokençš„L1èŒƒæ•°ï¼‰æ¥é€‰æ‹©è¦åˆ é™¤çš„tokenã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHoloVåœ¨å„ç§ä»»åŠ¡ã€MLLMæ¶æ„å’Œå‰ªæç‡ä¸‹éƒ½ä¼˜äºSOTAæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œé…å¤‡HoloVçš„LLaVA1.5åœ¨å‰ªæ88.9%çš„è§†è§‰tokensåï¼Œä»èƒ½ä¿æŒåŸå§‹æ€§èƒ½çš„95.8%ï¼Œå®ç°äº†å“è¶Šçš„æ•ˆç‡-ç²¾åº¦æƒè¡¡ã€‚è¿™è¡¨æ˜HoloVèƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HoloVå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†å›¾åƒè¾“å…¥çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸‹ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—ç­‰ã€‚é€šè¿‡å‡å°‘è§†è§‰tokençš„æ•°é‡ï¼ŒHoloVå¯ä»¥æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨ï¼Œæé«˜æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ï¼Œä»è€Œæ‰©å±•MLLMçš„åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their powerful capabilities, Multimodal Large Language Models (MLLMs) suffer from considerable computational overhead due to their reliance on massive visual tokens. Recent studies have explored token pruning to alleviate this problem, which typically uses text-vision cross-attention or [\texttt{CLS}] attention to assess and discard redundant visual tokens. In this work, we identify a critical limitation of such attention-first pruning approaches, i.e., they tend to preserve semantically similar tokens, resulting in pronounced performance drops under high pruning ratios. To this end, we propose {HoloV}, a simple yet effective, plug-and-play visual token pruning framework for efficient inference. Distinct from previous attention-first schemes, HoloV rethinks token retention from a holistic perspective. By adaptively distributing the pruning budget across different spatial crops, HoloV ensures that the retained tokens capture the global visual context rather than isolated salient features. This strategy minimizes representational collapse and maintains task-relevant information even under aggressive pruning. Experimental results demonstrate that our HoloV achieves superior performance across various tasks, MLLM architectures, and pruning ratios compared to SOTA methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the original performance after pruning 88.9\% of visual tokens, achieving superior efficiency-accuracy trade-offs.

