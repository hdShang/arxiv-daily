---
layout: default
title: Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights
---

# Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02922" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02922v1</a>
  <a href="https://arxiv.org/pdf/2510.02922.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02922v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02922v1', 'Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daphne Tsolissou, Theofanis Ganitidis, Konstantinos Mitsis, Stergios CHristodoulidis, Maria Vakalopoulou, Konstantina Nikita

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€é¢ˆåŠ¨è„‰é£é™©åˆ†å±‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¢ˆåŠ¨è„‰é£é™©åˆ†å±‚` `å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€èåˆ` `è¶…å£°æˆåƒ` `é¢†åŸŸè‡ªé€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é¢ˆåŠ¨è„‰ç²¥æ ·ç¡¬åŒ–ç–¾ç—…çš„å¯é é£é™©è¯„ä¼°é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ•´åˆå¤šç§ä¸´åºŠå’Œå½±åƒä¿¡æ¯ï¼Œå¹¶ä¿è¯å¯¹ä¸´åºŠåŒ»ç”Ÿé€æ˜ä¸”å¯è§£é‡Šã€‚
2. æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰æ•´åˆè¶…å£°å½±åƒå’Œä¸´åºŠæ•°æ®ï¼Œè¿›è¡Œé¢ˆåŠ¨è„‰æ–‘å—è¯„ä¼°å’Œå’ä¸­é£é™©åˆ†å±‚ã€‚
3. é€šè¿‡ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å¯¹LLaVa-NeXT-Vicunaè¿›è¡Œé¢†åŸŸé€‚é…ï¼Œå¹¶æ•´åˆå¤šæ¨¡æ€è¡¨æ ¼æ•°æ®ï¼Œæ˜¾è‘—æå‡äº†å’ä¸­é£é™©åˆ†å±‚çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å¤šæ¨¡æ€é¢ˆåŠ¨è„‰æ–‘å—è¯„ä¼°ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡æ•´åˆè¶…å£°æˆåƒï¼ˆUSIï¼‰ä¸ç»“æ„åŒ–çš„ä¸´åºŠã€äººå£ç»Ÿè®¡å­¦ã€å®éªŒå®¤å’Œè›‹ç™½è´¨ç”Ÿç‰©æ ‡å¿—ç‰©æ•°æ®ã€‚æå‡ºäº†ä¸€ä¸ªé€šè¿‡è®¿è°ˆå¼é—®é¢˜åºåˆ—æ¨¡æ‹ŸçœŸå®è¯Šæ–­åœºæ™¯çš„æ¡†æ¶ï¼Œæ¯”è¾ƒäº†ä¸€ç³»åˆ—å¼€æºLVLMï¼ŒåŒ…æ‹¬é€šç”¨æ¨¡å‹å’ŒåŒ»å­¦è°ƒä¼˜æ¨¡å‹ã€‚é›¶æ ·æœ¬å®éªŒè¡¨æ˜ï¼Œå¹¶éæ‰€æœ‰LVLMéƒ½èƒ½å‡†ç¡®è¯†åˆ«æˆåƒæ–¹å¼å’Œè§£å‰–ç»“æ„ï¼Œä¸”åœ¨å‡†ç¡®çš„é£é™©åˆ†ç±»æ–¹é¢è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å°†LLaVa-NeXT-Vicunaé€‚é…åˆ°è¶…å£°é¢†åŸŸï¼Œä»è€Œæ˜¾è‘—æ”¹å–„äº†å’ä¸­é£é™©åˆ†å±‚ã€‚ä»¥æ–‡æœ¬å½¢å¼æ•´åˆå¤šæ¨¡æ€è¡¨æ ¼æ•°æ®è¿›ä¸€æ­¥æé«˜äº†ç‰¹å¼‚æ€§å’Œå¹³è¡¡å‡†ç¡®æ€§ï¼Œä¸å…ˆå‰åœ¨ç›¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åŸºçº¿ç›¸æ¯”ï¼Œè·å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†LVLMåœ¨åŸºäºè¶…å£°çš„å¿ƒè¡€ç®¡é£é™©é¢„æµ‹ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œçªå‡ºäº†å¤šæ¨¡æ€æ•´åˆã€æ¨¡å‹æ ¡å‡†å’Œé¢†åŸŸè‡ªé€‚åº”å¯¹äºä¸´åºŠè½¬åŒ–çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé¢ˆåŠ¨è„‰ç²¥æ ·ç¡¬åŒ–é£é™©è¯„ä¼°éœ€è¦æ•´åˆè¶…å£°å½±åƒã€ä¸´åºŠæ•°æ®ç­‰å¤šç§æ¨¡æ€ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ä¼ ç»Ÿçš„CNNï¼Œéš¾ä»¥æœ‰æ•ˆèåˆå¤šæ¨¡æ€æ•°æ®ï¼Œä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è™½ç„¶å¼ºå¤§ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒç†è§£å’Œé£é™©é¢„æµ‹æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨LVLMå¼ºå¤§çš„è§†è§‰å’Œè¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå°†è¶…å£°å½±åƒå’Œä¸´åºŠæ•°æ®è½¬åŒ–ä¸ºç»Ÿä¸€çš„æ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶é€šè¿‡é—®ç­”å½¢å¼æ¨¡æ‹Ÿä¸´åºŠè¯Šæ–­æµç¨‹ã€‚é€šè¿‡é¢†åŸŸè‡ªé€‚åº”å’Œå¤šæ¨¡æ€èåˆï¼Œæå‡LVLMåœ¨é¢ˆåŠ¨è„‰é£é™©åˆ†å±‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªåŸºäºLVLMçš„å¤šæ¨¡æ€é¢ˆåŠ¨è„‰é£é™©åˆ†å±‚æ¡†æ¶ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é¢„å¤„ç†ï¼šå¯¹è¶…å£°å½±åƒå’Œä¸´åºŠæ•°æ®è¿›è¡Œæ¸…æ´—å’Œæ ¼å¼åŒ–ã€‚2) æ¨¡å‹é€‰æ‹©ï¼šé€‰æ‹©ä¸€ç³»åˆ—å¼€æºLVLMï¼ŒåŒ…æ‹¬é€šç”¨æ¨¡å‹å’ŒåŒ»å­¦è°ƒä¼˜æ¨¡å‹ã€‚3) é¢†åŸŸè‡ªé€‚åº”ï¼šä½¿ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å°†LLaVa-NeXT-Vicunaé€‚é…åˆ°è¶…å£°é¢†åŸŸã€‚4) å¤šæ¨¡æ€èåˆï¼šå°†ä¸´åºŠæ•°æ®ä»¥æ–‡æœ¬å½¢å¼æ•´åˆåˆ°LVLMçš„è¾“å…¥ä¸­ã€‚5) é£é™©é¢„æµ‹ï¼šé€šè¿‡é—®ç­”å½¢å¼ï¼Œåˆ©ç”¨LVLMè¿›è¡Œå’ä¸­é£é™©åˆ†å±‚ã€‚6) æ€§èƒ½è¯„ä¼°ï¼šä½¿ç”¨å¤šç§æŒ‡æ ‡è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸CNNåŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚

**å…³é”®åˆ›æ–°**ï¼š1) å°†LVLMåº”ç”¨äºé¢ˆåŠ¨è„‰é£é™©åˆ†å±‚ä»»åŠ¡ï¼Œæ¢ç´¢äº†å…¶åœ¨åŒ»å­¦å›¾åƒç†è§£å’Œé£é™©é¢„æµ‹æ–¹é¢çš„æ½œåŠ›ã€‚2) æå‡ºäº†ä¸€ä¸ªåŸºäºé—®ç­”å½¢å¼çš„å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œæ¨¡æ‹Ÿäº†çœŸå®çš„ä¸´åºŠè¯Šæ–­æµç¨‹ã€‚3) ä½¿ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å¯¹LVLMè¿›è¡Œé¢†åŸŸé€‚é…ï¼Œæ˜¾è‘—æå‡äº†å…¶åœ¨è¶…å£°å›¾åƒç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼š1) ä½¿ç”¨LLaVa-NeXT-Vicunaä½œä¸ºåŸºç¡€LVLMï¼Œå¹¶ä½¿ç”¨LoRAè¿›è¡Œé¢†åŸŸé€‚é…ï¼ŒLoRAçš„ç§©ï¼ˆrankï¼‰æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œéœ€è¦æ ¹æ®æ•°æ®é›†å¤§å°å’Œæ¨¡å‹å¤æ‚åº¦è¿›è¡Œè°ƒæ•´ã€‚2) å°†ä¸´åºŠæ•°æ®ä»¥æ–‡æœ¬å½¢å¼æ•´åˆåˆ°LVLMçš„è¾“å…¥ä¸­ï¼Œæ–‡æœ¬çš„æ ¼å¼å’Œå†…å®¹éœ€è¦ carefully è®¾è®¡ï¼Œä»¥ç¡®ä¿LVLMèƒ½å¤Ÿæœ‰æ•ˆåœ°ç†è§£å’Œåˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚3) ä½¿ç”¨é—®ç­”å½¢å¼è¿›è¡Œé£é™©é¢„æµ‹ï¼Œé—®é¢˜çš„è®¾è®¡éœ€è¦æ¶µç›–å…³é”®çš„ä¸´åºŠä¿¡æ¯å’Œå½±åƒç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å°†LLaVa-NeXT-Vicunaé€‚é…åˆ°è¶…å£°é¢†åŸŸï¼Œæ˜¾è‘—æ”¹å–„äº†å’ä¸­é£é™©åˆ†å±‚ã€‚æ•´åˆå¤šæ¨¡æ€è¡¨æ ¼æ•°æ®è¿›ä¸€æ­¥æé«˜äº†ç‰¹å¼‚æ€§å’Œå¹³è¡¡å‡†ç¡®æ€§ï¼Œä¸å…ˆå‰åœ¨ç›¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åŸºçº¿ç›¸æ¯”ï¼Œè·å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†ä¼˜äºCNNåŸºçº¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé¢ˆåŠ¨è„‰ç–¾ç—…çš„æ—©æœŸç­›æŸ¥å’Œé£é™©è¯„ä¼°ï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­å’Œæ²»ç–—å†³ç­–ã€‚é€šè¿‡æ•´åˆå¤šæ¨¡æ€æ•°æ®ï¼Œæé«˜é£é™©é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œé™ä½å’ä¸­é£é™©ã€‚æœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–å¿ƒè¡€ç®¡ç–¾ç—…çš„é£é™©è¯„ä¼°ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reliable risk assessment for carotid atheromatous disease remains a major clinical challenge, as it requires integrating diverse clinical and imaging information in a manner that is transparent and interpretable to clinicians. This study investigates the potential of state-of-the-art and recent large vision-language models (LVLMs) for multimodal carotid plaque assessment by integrating ultrasound imaging (USI) with structured clinical, demographic, laboratory, and protein biomarker data. A framework that simulates realistic diagnostic scenarios through interview-style question sequences is proposed, comparing a range of open-source LVLMs, including both general-purpose and medically tuned models. Zero-shot experiments reveal that even if they are very powerful, not all LVLMs can accurately identify imaging modality and anatomy, while all of them perform poorly in accurate risk classification. To address this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using low-rank adaptation (LoRA), resulting in substantial improvements in stroke risk stratification. The integration of multimodal tabular data in the form of text further enhances specificity and balanced accuracy, yielding competitive performance compared to prior convolutional neural network (CNN) baselines trained on the same dataset. Our findings highlight both the promise and limitations of LVLMs in ultrasound-based cardiovascular risk prediction, underscoring the importance of multimodal integration, model calibration, and domain adaptation for clinical translation.

