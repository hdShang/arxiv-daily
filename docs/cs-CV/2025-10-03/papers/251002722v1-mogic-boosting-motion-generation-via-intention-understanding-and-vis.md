---
layout: default
title: MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context
---

# MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02722" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02722v1</a>
  <a href="https://arxiv.org/pdf/2510.02722.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02722v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02722v1', 'MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junyu Shi, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-03

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/JunyuShi02/MoGIC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MoGICï¼šé€šè¿‡æ„å›¾ç†è§£å’Œè§†è§‰ä¸Šä¸‹æ–‡å¢å¼ºè¿åŠ¨ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿åŠ¨ç”Ÿæˆ` `æ„å›¾ç†è§£` `è§†è§‰ä¸Šä¸‹æ–‡` `å¤šæ¨¡æ€èåˆ` `Transformer` `æ··åˆæ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬é©±åŠ¨çš„è¿åŠ¨ç”Ÿæˆæ–¹æ³•éš¾ä»¥æ•æ‰åŠ¨ä½œçš„å› æœé€»è¾‘å’Œäººç±»æ„å›¾ã€‚
2. MoGICé€šè¿‡è”åˆä¼˜åŒ–å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆå’Œæ„å›¾é¢„æµ‹ï¼Œèå…¥è§†è§‰å…ˆéªŒï¼Œæå‡ç”Ÿæˆæ•ˆæœã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMoGICåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†FIDï¼Œå¹¶åœ¨è¿åŠ¨æè¿°æ–¹é¢è¶…è¶Šäº†LLMã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„æ–‡æœ¬é©±åŠ¨è¿åŠ¨ç”Ÿæˆæ–¹æ³•é€šå¸¸å°†åˆæˆè§†ä¸ºè¯­è¨€å’Œè¿åŠ¨ä¹‹é—´çš„åŒå‘æ˜ å°„ï¼Œä½†åœ¨æ•æ‰åŠ¨ä½œæ‰§è¡Œçš„å› æœé€»è¾‘å’Œé©±åŠ¨è¡Œä¸ºçš„äººç±»æ„å›¾æ–¹é¢ä»ç„¶æœ‰é™ã€‚ç”±äºè¯­è¨€æœ¬èº«æ— æ³•æŒ‡å®šç»†ç²’åº¦çš„æ—¶ç©ºç»†èŠ‚ï¼Œç¼ºä¹è§†è§‰åŸºç¡€è¿›ä¸€æ­¥é™åˆ¶äº†ç²¾åº¦å’Œä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†MoGICï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†æ„å›¾å»ºæ¨¡å’Œè§†è§‰å…ˆéªŒé›†æˆåˆ°å¤šæ¨¡æ€è¿åŠ¨åˆæˆä¸­ã€‚é€šè¿‡è”åˆä¼˜åŒ–å¤šæ¨¡æ€æ¡ä»¶è¿åŠ¨ç”Ÿæˆå’Œæ„å›¾é¢„æµ‹ï¼ŒMoGICæ­ç¤ºäº†æ½œåœ¨çš„äººç±»ç›®æ ‡ï¼Œåˆ©ç”¨è§†è§‰å…ˆéªŒæ¥å¢å¼ºç”Ÿæˆï¼Œå¹¶å±•ç¤ºäº†é€šç”¨çš„å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§å…·æœ‰è‡ªé€‚åº”èŒƒå›´çš„æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å®ç°æ¡ä»¶tokenå’Œè¿åŠ¨å­åºåˆ—ä¹‹é—´çš„æœ‰æ•ˆå±€éƒ¨å¯¹é½ã€‚ä¸ºäº†æ”¯æŒè¿™ç§èŒƒå¼ï¼Œæˆ‘ä»¬ä»21ä¸ªé«˜è´¨é‡çš„è¿åŠ¨æ•°æ®é›†ä¸­æ•´ç†äº†ä¸€ä¸ª440å°æ—¶çš„åŸºå‡†Mo440Hã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåï¼ŒMoGICåœ¨HumanML3Dä¸Šå°†FIDé™ä½äº†38.6ï¼…ï¼Œåœ¨Mo440Hä¸Šé™ä½äº†34.6ï¼…ï¼Œé€šè¿‡è½»é‡çº§çš„æ–‡æœ¬å¤´åœ¨è¿åŠ¨æè¿°æ–¹é¢è¶…è¿‡äº†åŸºäºLLMçš„æ–¹æ³•ï¼Œå¹¶è¿›ä¸€æ­¥å®ç°äº†æ„å›¾é¢„æµ‹å’Œè§†è§‰æ¡ä»¶ç”Ÿæˆï¼Œä»è€Œæ¨è¿›äº†å¯æ§è¿åŠ¨åˆæˆå’Œæ„å›¾ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–‡æœ¬é©±åŠ¨çš„è¿åŠ¨ç”Ÿæˆæ–¹æ³•ä¸»è¦ä¾èµ–äºè¯­è¨€å’Œè¿åŠ¨ä¹‹é—´çš„ç›´æ¥æ˜ å°„ï¼Œå¿½ç•¥äº†äººç±»æ„å›¾åœ¨åŠ¨ä½œæ‰§è¡Œä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶ä¸”ç¼ºä¹è§†è§‰ä¿¡æ¯çš„è¾…åŠ©ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸å¤Ÿç²¾ç¡®å’Œä¸ªæ€§åŒ–ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥æ•æ‰åŠ¨ä½œçš„å› æœå…³ç³»ï¼Œä¹Ÿæ— æ³•æ ¹æ®å…·ä½“çš„è§†è§‰åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoGICçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ„å›¾ç†è§£å’Œè§†è§‰ä¸Šä¸‹æ–‡èå…¥åˆ°è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚é€šè¿‡è”åˆä¼˜åŒ–å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„è¿åŠ¨ç”Ÿæˆå’Œæ„å›¾é¢„æµ‹ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ½œåœ¨çš„äººç±»ç›®æ ‡ï¼Œå¹¶åˆ©ç”¨è§†è§‰å…ˆéªŒä¿¡æ¯æ¥æŒ‡å¯¼è¿åŠ¨çš„ç”Ÿæˆï¼Œä»è€Œæé«˜ç”Ÿæˆç»“æœçš„è´¨é‡å’Œå¯æ§æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoGICæ¡†æ¶åŒ…å«å¤šæ¨¡æ€æ¡ä»¶è¿åŠ¨ç”Ÿæˆæ¨¡å—ã€æ„å›¾é¢„æµ‹æ¨¡å—å’Œè§†è§‰å…ˆéªŒèåˆæ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹æ¥æ”¶æ–‡æœ¬æè¿°å’Œè§†è§‰ä¿¡æ¯ä½œä¸ºè¾“å…¥ã€‚ç„¶åï¼Œå¤šæ¨¡æ€æ¡ä»¶è¿åŠ¨ç”Ÿæˆæ¨¡å—æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆåˆæ­¥çš„è¿åŠ¨åºåˆ—ã€‚åŒæ—¶ï¼Œæ„å›¾é¢„æµ‹æ¨¡å—æ ¹æ®æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯é¢„æµ‹äººç±»çš„æ„å›¾ã€‚æœ€åï¼Œè§†è§‰å…ˆéªŒèåˆæ¨¡å—å°†è§†è§‰ä¿¡æ¯å’Œé¢„æµ‹çš„æ„å›¾èå…¥åˆ°è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆæœ€ç»ˆçš„è¿åŠ¨åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoGICçš„å…³é”®åˆ›æ–°åœ¨äºå°†æ„å›¾ç†è§£å’Œè§†è§‰ä¸Šä¸‹æ–‡èå…¥åˆ°è¿åŠ¨ç”Ÿæˆä¸­ï¼Œå®ç°äº†å¤šæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´æ³¨æ„åŠ›èŒƒå›´ï¼Œä»è€Œæ›´å¥½åœ°å¯¹é½æ¡ä»¶tokenå’Œè¿åŠ¨å­åºåˆ—ã€‚

**å…³é”®è®¾è®¡**ï¼šMoGICä½¿ç”¨äº†Transformeræ¶æ„ä½œä¸ºå…¶æ ¸å¿ƒç½‘ç»œç»“æ„ã€‚æ··åˆæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å±€éƒ¨å’Œå…¨å±€èŒƒå›´å†…å…³æ³¨ä¸åŒçš„æ¡ä»¶tokenã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è¿åŠ¨ç”ŸæˆæŸå¤±ã€æ„å›¾é¢„æµ‹æŸå¤±å’Œå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„å„ä¸ªæ¨¡å—ã€‚Mo440Hæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè¯¥ç ”ç©¶æä¾›äº†é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MoGICåœ¨HumanML3Då’ŒMo440Hæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†38.6%å’Œ34.6%çš„FIDé™ä½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMoGICåœ¨è¿åŠ¨æè¿°ä»»åŠ¡ä¸­è¶…è¶Šäº†åŸºäºLLMçš„æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿå®ç°æ„å›¾é¢„æµ‹å’Œè§†è§‰æ¡ä»¶ç”Ÿæˆï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoGICçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°å’Œåœºæ™¯è§†è§‰ä¿¡æ¯ï¼Œç”Ÿæˆé€¼çœŸçš„äººä½“è¿åŠ¨åŠ¨ç”»ï¼Œå¢å¼ºè™šæ‹Ÿç°å®ä½“éªŒã€‚åœ¨æœºå™¨äººæ§åˆ¶ä¸­ï¼Œå¯ä»¥æ ¹æ®äººç±»çš„æŒ‡ä»¤å’Œç¯å¢ƒæ„ŸçŸ¥ï¼Œç”Ÿæˆåˆç†çš„æœºå™¨äººåŠ¨ä½œï¼Œæé«˜äººæœºäº¤äº’çš„è‡ªç„¶æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing text-driven motion generation methods often treat synthesis as a bidirectional mapping between language and motion, but remain limited in capturing the causal logic of action execution and the human intentions that drive behavior. The absence of visual grounding further restricts precision and personalization, as language alone cannot specify fine-grained spatiotemporal details. We propose MoGIC, a unified framework that integrates intention modeling and visual priors into multimodal motion synthesis. By jointly optimizing multimodal-conditioned motion generation and intention prediction, MoGIC uncovers latent human goals, leverages visual priors to enhance generation, and exhibits versatile multimodal generative capability. We further introduce a mixture-of-attention mechanism with adaptive scope to enable effective local alignment between conditional tokens and motion subsequences. To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21 high-quality motion datasets. Experiments show that after finetuning, MoGIC reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based methods in motion captioning with a lightweight text head, and further enables intention prediction and vision-conditioned generation, advancing controllable motion synthesis and intention understanding. The code is available at https://github.com/JunyuShi02/MoGIC

