---
layout: default
title: MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context
---

# MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.02722" target="_blank" class="toolbar-btn">arXiv: 2510.02722v1</a>
    <a href="https://arxiv.org/pdf/2510.02722.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02722v1" 
            onclick="toggleFavorite(this, '2510.02722v1', 'MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Junyu Shi, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-03

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/JunyuShi02/MoGIC)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**MoGICÔºöÈÄöËøáÊÑèÂõæÁêÜËß£ÂíåËßÜËßâ‰∏ä‰∏ãÊñáÂ¢ûÂº∫ËøêÂä®ÁîüÊàê**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)** **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËøêÂä®ÁîüÊàê` `ÊÑèÂõæÁêÜËß£` `ËßÜËßâ‰∏ä‰∏ãÊñá` `Â§öÊ®°ÊÄÅËûçÂêà` `Transformer` `Ê∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñáÊú¨È©±Âä®ÁöÑËøêÂä®ÁîüÊàêÊñπÊ≥ïÈöæ‰ª•ÊçïÊçâÂä®‰ΩúÁöÑÂõ†ÊûúÈÄªËæëÂíå‰∫∫Á±ªÊÑèÂõæ„ÄÇ
2. MoGICÈÄöËøáËÅîÂêà‰ºòÂåñÂ§öÊ®°ÊÄÅËøêÂä®ÁîüÊàêÂíåÊÑèÂõæÈ¢ÑÊµãÔºåËûçÂÖ•ËßÜËßâÂÖàÈ™åÔºåÊèêÂçáÁîüÊàêÊïàÊûú„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMoGICÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëóÈôç‰Ωé‰∫ÜFIDÔºåÂπ∂Âú®ËøêÂä®ÊèèËø∞ÊñπÈù¢Ë∂ÖË∂ä‰∫ÜLLM„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑÊñáÊú¨È©±Âä®ËøêÂä®ÁîüÊàêÊñπÊ≥ïÈÄöÂ∏∏Â∞ÜÂêàÊàêËßÜ‰∏∫ËØ≠Ë®ÄÂíåËøêÂä®‰πãÈó¥ÁöÑÂèåÂêëÊò†Â∞ÑÔºå‰ΩÜÂú®ÊçïÊçâÂä®‰ΩúÊâßË°åÁöÑÂõ†ÊûúÈÄªËæëÂíåÈ©±Âä®Ë°å‰∏∫ÁöÑ‰∫∫Á±ªÊÑèÂõæÊñπÈù¢‰ªçÁÑ∂ÊúâÈôê„ÄÇÁî±‰∫éËØ≠Ë®ÄÊú¨Ë∫´Êó†Ê≥ïÊåáÂÆöÁªÜÁ≤íÂ∫¶ÁöÑÊó∂Á©∫ÁªÜËäÇÔºåÁº∫‰πèËßÜËßâÂü∫Á°ÄËøõ‰∏ÄÊ≠•ÈôêÂà∂‰∫ÜÁ≤æÂ∫¶Âíå‰∏™ÊÄßÂåñ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜMoGICÔºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÂ∞ÜÊÑèÂõæÂª∫Ê®°ÂíåËßÜËßâÂÖàÈ™åÈõÜÊàêÂà∞Â§öÊ®°ÊÄÅËøêÂä®ÂêàÊàê‰∏≠„ÄÇÈÄöËøáËÅîÂêà‰ºòÂåñÂ§öÊ®°ÊÄÅÊù°‰ª∂ËøêÂä®ÁîüÊàêÂíåÊÑèÂõæÈ¢ÑÊµãÔºåMoGICÊè≠Á§∫‰∫ÜÊΩúÂú®ÁöÑ‰∫∫Á±ªÁõÆÊ†áÔºåÂà©Áî®ËßÜËßâÂÖàÈ™åÊù•Â¢ûÂº∫ÁîüÊàêÔºåÂπ∂Â±ïÁ§∫‰∫ÜÈÄöÁî®ÁöÑÂ§öÊ®°ÊÄÅÁîüÊàêËÉΩÂäõ„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂÖ∑ÊúâËá™ÈÄÇÂ∫îËåÉÂõ¥ÁöÑÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ª•ÂÆûÁé∞Êù°‰ª∂tokenÂíåËøêÂä®Â≠êÂ∫èÂàó‰πãÈó¥ÁöÑÊúâÊïàÂ±ÄÈÉ®ÂØπÈΩê„ÄÇ‰∏∫‰∫ÜÊîØÊåÅËøôÁßçËåÉÂºèÔºåÊàë‰ª¨‰ªé21‰∏™È´òË¥®ÈáèÁöÑËøêÂä®Êï∞ÊçÆÈõÜ‰∏≠Êï¥ÁêÜ‰∫Ü‰∏Ä‰∏™440Â∞èÊó∂ÁöÑÂü∫ÂáÜMo440H„ÄÇÂÆûÈ™åË°®ÊòéÔºåÁªèËøáÂæÆË∞ÉÂêéÔºåMoGICÂú®HumanML3D‰∏äÂ∞ÜFIDÈôç‰Ωé‰∫Ü38.6ÔºÖÔºåÂú®Mo440H‰∏äÈôç‰Ωé‰∫Ü34.6ÔºÖÔºåÈÄöËøáËΩªÈáèÁ∫ßÁöÑÊñáÊú¨Â§¥Âú®ËøêÂä®ÊèèËø∞ÊñπÈù¢Ë∂ÖËøá‰∫ÜÂü∫‰∫éLLMÁöÑÊñπÊ≥ïÔºåÂπ∂Ëøõ‰∏ÄÊ≠•ÂÆûÁé∞‰∫ÜÊÑèÂõæÈ¢ÑÊµãÂíåËßÜËßâÊù°‰ª∂ÁîüÊàêÔºå‰ªéËÄåÊé®Ëøõ‰∫ÜÂèØÊéßËøêÂä®ÂêàÊàêÂíåÊÑèÂõæÁêÜËß£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñáÊú¨È©±Âä®ÁöÑËøêÂä®ÁîüÊàêÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éËØ≠Ë®ÄÂíåËøêÂä®‰πãÈó¥ÁöÑÁõ¥Êé•Êò†Â∞ÑÔºåÂøΩÁï•‰∫Ü‰∫∫Á±ªÊÑèÂõæÂú®Âä®‰ΩúÊâßË°å‰∏≠ÁöÑÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÂπ∂‰∏îÁº∫‰πèËßÜËßâ‰ø°ÊÅØÁöÑËæÖÂä©ÔºåÂØºËá¥ÁîüÊàêÁªìÊûú‰∏çÂ§üÁ≤æÁ°ÆÂíå‰∏™ÊÄßÂåñ„ÄÇËøô‰∫õÊñπÊ≥ïÈöæ‰ª•ÊçïÊçâÂä®‰ΩúÁöÑÂõ†ÊûúÂÖ≥Á≥ªÔºå‰πüÊó†Ê≥ïÊ†πÊçÆÂÖ∑‰ΩìÁöÑËßÜËßâÂú∫ÊôØËøõË°åË∞ÉÊï¥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMoGICÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊÑèÂõæÁêÜËß£ÂíåËßÜËßâ‰∏ä‰∏ãÊñáËûçÂÖ•Âà∞ËøêÂä®ÁîüÊàêËøáÁ®ã‰∏≠„ÄÇÈÄöËøáËÅîÂêà‰ºòÂåñÂ§öÊ®°ÊÄÅÊù°‰ª∂‰∏ãÁöÑËøêÂä®ÁîüÊàêÂíåÊÑèÂõæÈ¢ÑÊµãÔºåÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†Âà∞ÊΩúÂú®ÁöÑ‰∫∫Á±ªÁõÆÊ†áÔºåÂπ∂Âà©Áî®ËßÜËßâÂÖàÈ™å‰ø°ÊÅØÊù•ÊåáÂØºËøêÂä®ÁöÑÁîüÊàêÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁªìÊûúÁöÑË¥®ÈáèÂíåÂèØÊéßÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMoGICÊ°ÜÊû∂ÂåÖÂê´Â§öÊ®°ÊÄÅÊù°‰ª∂ËøêÂä®ÁîüÊàêÊ®°Âùó„ÄÅÊÑèÂõæÈ¢ÑÊµãÊ®°ÂùóÂíåËßÜËßâÂÖàÈ™åËûçÂêàÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÊ®°ÂûãÊé•Êî∂ÊñáÊú¨ÊèèËø∞ÂíåËßÜËßâ‰ø°ÊÅØ‰Ωú‰∏∫ËæìÂÖ•„ÄÇÁÑ∂ÂêéÔºåÂ§öÊ®°ÊÄÅÊù°‰ª∂ËøêÂä®ÁîüÊàêÊ®°ÂùóÊ†πÊçÆÊñáÊú¨ÊèèËø∞ÁîüÊàêÂàùÊ≠•ÁöÑËøêÂä®Â∫èÂàó„ÄÇÂêåÊó∂ÔºåÊÑèÂõæÈ¢ÑÊµãÊ®°ÂùóÊ†πÊçÆÊñáÊú¨ÂíåËßÜËßâ‰ø°ÊÅØÈ¢ÑÊµã‰∫∫Á±ªÁöÑÊÑèÂõæ„ÄÇÊúÄÂêéÔºåËßÜËßâÂÖàÈ™åËûçÂêàÊ®°ÂùóÂ∞ÜËßÜËßâ‰ø°ÊÅØÂíåÈ¢ÑÊµãÁöÑÊÑèÂõæËûçÂÖ•Âà∞ËøêÂä®ÁîüÊàêËøáÁ®ã‰∏≠ÔºåÁîüÊàêÊúÄÁªàÁöÑËøêÂä®Â∫èÂàó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMoGICÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÊÑèÂõæÁêÜËß£ÂíåËßÜËßâ‰∏ä‰∏ãÊñáËûçÂÖ•Âà∞ËøêÂä®ÁîüÊàê‰∏≠ÔºåÂÆûÁé∞‰∫ÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÊúâÊïàËûçÂêà„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥Ê≥®ÊÑèÂäõËåÉÂõ¥Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÂØπÈΩêÊù°‰ª∂tokenÂíåËøêÂä®Â≠êÂ∫èÂàó„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMoGIC‰ΩøÁî®‰∫ÜTransformerÊû∂ÊûÑ‰Ωú‰∏∫ÂÖ∂Ê†∏ÂøÉÁΩëÁªúÁªìÊûÑ„ÄÇÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÂÖÅËÆ∏Ê®°ÂûãÂú®Â±ÄÈÉ®ÂíåÂÖ®Â±ÄËåÉÂõ¥ÂÜÖÂÖ≥Ê≥®‰∏çÂêåÁöÑÊù°‰ª∂token„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ËøêÂä®ÁîüÊàêÊçüÂ§±„ÄÅÊÑèÂõæÈ¢ÑÊµãÊçüÂ§±ÂíåÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÊ®°ÂûãÁöÑÂêÑ‰∏™Ê®°Âùó„ÄÇMo440HÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫‰πü‰∏∫ËØ•Á†îÁ©∂Êèê‰æõ‰∫ÜÈ´òË¥®ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MoGICÂú®HumanML3DÂíåMo440HÊï∞ÊçÆÈõÜ‰∏äÂàÜÂà´ÂÆûÁé∞‰∫Ü38.6%Âíå34.6%ÁöÑFIDÈôç‰ΩéÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåMoGICÂú®ËøêÂä®ÊèèËø∞‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÂü∫‰∫éLLMÁöÑÊñπÊ≥ïÔºåÂπ∂‰∏îËÉΩÂ§üÂÆûÁé∞ÊÑèÂõæÈ¢ÑÊµãÂíåËßÜËßâÊù°‰ª∂ÁîüÊàêÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÁîüÊàêËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MoGICÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèë„ÄÅÊú∫Âô®‰∫∫ÊéßÂà∂Á≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑÊñáÊú¨ÊèèËø∞ÂíåÂú∫ÊôØËßÜËßâ‰ø°ÊÅØÔºåÁîüÊàêÈÄºÁúüÁöÑ‰∫∫‰ΩìËøêÂä®Âä®ÁîªÔºåÂ¢ûÂº∫ËôöÊãüÁé∞ÂÆû‰ΩìÈ™å„ÄÇÂú®Êú∫Âô®‰∫∫ÊéßÂà∂‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆ‰∫∫Á±ªÁöÑÊåá‰ª§ÂíåÁéØÂ¢ÉÊÑüÁü•ÔºåÁîüÊàêÂêàÁêÜÁöÑÊú∫Âô®‰∫∫Âä®‰ΩúÔºåÊèêÈ´ò‰∫∫Êú∫‰∫§‰∫íÁöÑËá™ÁÑ∂ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Existing text-driven motion generation methods often treat synthesis as a bidirectional mapping between language and motion, but remain limited in capturing the causal logic of action execution and the human intentions that drive behavior. The absence of visual grounding further restricts precision and personalization, as language alone cannot specify fine-grained spatiotemporal details. We propose MoGIC, a unified framework that integrates intention modeling and visual priors into multimodal motion synthesis. By jointly optimizing multimodal-conditioned motion generation and intention prediction, MoGIC uncovers latent human goals, leverages visual priors to enhance generation, and exhibits versatile multimodal generative capability. We further introduce a mixture-of-attention mechanism with adaptive scope to enable effective local alignment between conditional tokens and motion subsequences. To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21 high-quality motion datasets. Experiments show that after finetuning, MoGIC reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based methods in motion captioning with a lightweight text head, and further enables intention prediction and vision-conditioned generation, advancing controllable motion synthesis and intention understanding. The code is available at https://github.com/JunyuShi02/MoGIC

