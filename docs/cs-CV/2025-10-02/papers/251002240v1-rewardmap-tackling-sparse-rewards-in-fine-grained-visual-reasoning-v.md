---
layout: default
title: RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning
---

# RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02240" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02240v1</a>
  <a href="https://arxiv.org/pdf/2510.02240.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02240v1" onclick="toggleFavorite(this, '2510.02240v1', 'RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sicheng Feng, Kaiwen Tuo, Song Wang, Lingdong Kong, Jianke Zhu, Huan Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRewardMapï¼Œé€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è§£å†³ç»†ç²’åº¦è§†è§‰æ¨ç†ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»†ç²’åº¦è§†è§‰æ¨ç†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç¨€ç–å¥–åŠ±` `å¤šé˜¶æ®µå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰æ¨ç†ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ï¼Œé¢ä¸´ç¨€ç–å¥–åŠ±å’Œä¼˜åŒ–ä¸ç¨³å®šçš„æŒ‘æˆ˜ã€‚
2. RewardMapé€šè¿‡éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼Œä»ç®€å•æ„ŸçŸ¥åˆ°å¤æ‚æ¨ç†ï¼Œæœ‰æ•ˆè§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜å¹¶æå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRewardMapåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹³å‡æå‡3.47%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)åœ¨ç»†ç²’åº¦è§†è§‰æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ReasonMapæå‡ºçš„åœ¨äº¤é€šåœ°å›¾ç­‰ç»“æ„åŒ–å’Œä¿¡æ¯ä¸°å¯Œçš„ç¯å¢ƒä¸­è¿›è¡Œç©ºé—´æ¨ç†çš„éš¾é¢˜ï¼Œæå‡ºäº†RewardMapæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³æ ‡å‡†å¼ºåŒ–å­¦ä¹ åœ¨æ­¤ç±»ä»»åŠ¡ä¸­é¢ä¸´çš„ç¨€ç–å¥–åŠ±å’Œä¼˜åŒ–ä¸ç¨³å®šçš„é—®é¢˜ã€‚é¦–å…ˆï¼Œæ„å»ºäº†ReasonMap-Plusæ•°æ®é›†ï¼Œé€šè¿‡è§†è§‰é—®ç­”(VQA)ä»»åŠ¡å¼•å…¥å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œå®ç°ç»†ç²’åº¦è§†è§‰ç†è§£æŠ€èƒ½çš„æœ‰æ•ˆå†·å¯åŠ¨è®­ç»ƒã€‚å…¶æ¬¡ï¼Œæå‡ºäº†RewardMapï¼Œä¸€ä¸ªå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜MLLMçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚RewardMapåŒ…å«éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ï¼Œé€šè¿‡ç»†èŠ‚å¥–åŠ±ç›´æ¥è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶æä¾›æ›´ä¸°å¯Œçš„ç›‘ç£ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªå¤šé˜¶æ®µRLæ–¹æ¡ˆï¼Œä»ç®€å•çš„æ„ŸçŸ¥ä»»åŠ¡åˆ°å¤æ‚çš„æ¨ç†ä»»åŠ¡è¿›è¡Œå¼•å¯¼è®­ç»ƒï¼Œæä¾›æ¯”ä¼ ç»Ÿç›‘ç£å¾®è°ƒ(SFT)æ›´æœ‰æ•ˆçš„å†·å¯åŠ¨ç­–ç•¥ã€‚åœ¨ReasonMapå’ŒReasonMap-Plusä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRewardMapçš„æ¯ä¸ªç»„æˆéƒ¨åˆ†éƒ½æœ‰åŠ©äºæ€§èƒ½çš„æŒç»­æå‡ï¼Œè€Œå®ƒä»¬çš„ç»„åˆäº§ç”Ÿäº†æœ€ä½³ç»“æœã€‚æ­¤å¤–ï¼Œä½¿ç”¨RewardMapè®­ç»ƒçš„æ¨¡å‹åœ¨æ¶µç›–ç©ºé—´æ¨ç†ã€ç»†ç²’åº¦è§†è§‰æ¨ç†ä»¥åŠè¶…å‡ºäº¤é€šåœ°å›¾çš„é€šç”¨ä»»åŠ¡çš„6ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æé«˜äº†3.47%ï¼Œçªå‡ºäº†å¢å¼ºçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ï¼Œç”±äºå¥–åŠ±ç¨€ç–å¯¼è‡´å¼ºåŒ–å­¦ä¹ è®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸­éš¾ä»¥è·å¾—æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å­¦ä¹ ã€‚ReasonMapæ•°æ®é›†è¿›ä¸€æ­¥çªå‡ºäº†ç°æœ‰æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¯†é›†å¥–åŠ±å’Œå¤šé˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼Œå…‹æœç¨€ç–å¥–åŠ±å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å¯†é›†å¥–åŠ±é€šè¿‡VQAä»»åŠ¡æä¾›æ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œå¤šé˜¶æ®µå­¦ä¹ åˆ™å°†å¤æ‚çš„æ¨ç†ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªéš¾åº¦é€’å¢çš„é˜¶æ®µï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥å­¦ä¹ å’Œæå‡èƒ½åŠ›ã€‚è¿™æ ·è®¾è®¡èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œå¹¶é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRewardMapæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šéš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆã€‚éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡é€šè¿‡å¼•å…¥ç»†èŠ‚å¥–åŠ±ï¼Œä¸ºæ¨¡å‹æä¾›æ›´ç»†ç²’åº¦çš„åé¦ˆï¼Œä»è€Œç¼“è§£å¥–åŠ±ç¨€ç–çš„é—®é¢˜ã€‚å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆåˆ™å°†è®­ç»ƒè¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µä¾§é‡äºä¸åŒçš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä»ç®€å•çš„æ„ŸçŸ¥åˆ°å¤æ‚çš„æ¨ç†ã€‚æ•´ä½“æµç¨‹æ˜¯ä»ReasonMap-Plusæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ åœ¨ReasonMapæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆç›¸ç»“åˆï¼Œä»¥è§£å†³ç»†ç²’åº¦è§†è§‰æ¨ç†ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å•é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒRewardMapèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œå¹¶å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡èƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œè€Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆåˆ™èƒ½å¤Ÿä½¿æ¨¡å‹é€æ­¥å­¦ä¹ å’Œæå‡èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šéš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ä¸­ï¼Œç»†èŠ‚å¥–åŠ±çš„æƒé‡éœ€è¦ä»”ç»†è°ƒæ•´ï¼Œä»¥å¹³è¡¡æ•´ä½“å¥–åŠ±çš„ç¨€ç–æ€§å’Œç»†èŠ‚å¥–åŠ±çš„ä¸°å¯Œæ€§ã€‚å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆä¸­ï¼Œæ¯ä¸ªé˜¶æ®µçš„ä»»åŠ¡éš¾åº¦éœ€è¦é€æ­¥å¢åŠ ï¼Œä»¥ä¿è¯æ¨¡å‹èƒ½å¤Ÿé€æ­¥å­¦ä¹ å’Œæå‡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦é€‰æ‹©åˆé€‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚PPOæˆ–SACï¼Œå¹¶è°ƒæ•´ç›¸åº”çš„è¶…å‚æ•°ï¼Œä»¥è·å¾—æœ€ä½³çš„è®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RewardMapåœ¨ReasonMapå’ŒReasonMap-Plusæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼ŒRewardMapçš„æ¯ä¸ªç»„æˆéƒ¨åˆ†éƒ½å¯¹æ€§èƒ½æå‡åšå‡ºäº†è´¡çŒ®ï¼Œè€Œå®ƒä»¬çš„ç»„åˆäº§ç”Ÿäº†æœ€ä½³ç»“æœã€‚æ­¤å¤–ï¼Œä½¿ç”¨RewardMapè®­ç»ƒçš„æ¨¡å‹åœ¨æ¶µç›–ç©ºé—´æ¨ç†ã€ç»†ç²’åº¦è§†è§‰æ¨ç†ä»¥åŠè¶…å‡ºäº¤é€šåœ°å›¾çš„é€šç”¨ä»»åŠ¡çš„6ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æé«˜äº†3.47%ï¼Œè¯æ˜äº†å…¶å¢å¼ºçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç»†ç²’åº¦è§†è§‰æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½äº¤é€šã€æœºå™¨äººå¯¼èˆªã€åŒ»å­¦å›¾åƒåˆ†æç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½äº¤é€šé¢†åŸŸï¼Œå¯ä»¥å¸®åŠ©è‡ªåŠ¨é©¾é©¶è½¦è¾†æ›´å¥½åœ°ç†è§£äº¤é€šåœ°å›¾ï¼Œä»è€Œåšå‡ºæ›´å®‰å…¨çš„å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.

