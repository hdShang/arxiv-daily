---
layout: default
title: The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models
---

# The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.02230" target="_blank" class="toolbar-btn">arXiv: 2510.02230v1</a>
    <a href="https://arxiv.org/pdf/2510.02230.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02230v1" 
            onclick="toggleFavorite(this, '2510.02230v1', 'The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan

**ÂàÜÁ±ª**: cs.AI, cs.CL, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02

**Â§áÊ≥®**: 23 pages, 15 figures

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/mail-research/SELF-llm-interference)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Êè≠Á§∫RLVRÁ∫¶ÊùüËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËæπÁïåÁöÑÊÇñËÆ∫ÔºåÂπ∂ÊèêÂá∫Êï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïÊèêÂçáÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜËÉΩÂäõ` `Êï∞ÊçÆÁ≠ñÂ±ï` `Ë¥üÂπ≤Êâ∞` `Ëµ¢ËÄÖÈÄöÂêÉ` `Êé®ÁêÜËæπÁïå` `Êï∞Â≠¶Êé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRLVRÊñπÊ≥ïÂú®ÊèêÂçáLLMÊé®ÁêÜËÉΩÂäõÊó∂Â≠òÂú®Êé®ÁêÜËæπÁïåÊî∂Áº©ÁöÑÊåëÊàòÔºåÂØºËá¥Ê®°ÂûãÊ≥õÂåñËÉΩÂäõÂèóÈôê„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÈÄöËøáÂàÜÊûêRLVRÂ≠¶‰π†Âä®ÊÄÅÔºåÊè≠Á§∫Ë¥üÂπ≤Êâ∞ÂíåËµ¢ËÄÖÈÄöÂêÉÁé∞Ë±°ÔºåÂπ∂ËÆæËÆ°Êï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïËß£ÂÜ≥„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÊâÄÊèêÊï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïËÉΩÂ§üÊúâÊïàÊèêÂçáPass@$k$ÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Ëß£ÂÜ≥Êé®ÁêÜËæπÁïåÊî∂Áº©ÈóÆÈ¢ò‰∏äÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫Ü‰ΩøÁî®ÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†(RLVR)Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)Êé®ÁêÜËÉΩÂäõÊó∂Âá∫Áé∞ÁöÑÊé®ÁêÜËæπÁïåÊî∂Áº©ÈóÆÈ¢ò„ÄÇÈÄöËøáÂàÜÊûêRLVRÁöÑÂ≠¶‰π†Âä®ÊÄÅÔºåÊè≠Á§∫‰∫ÜÂØºËá¥ËØ•ÈóÆÈ¢òÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÁé∞Ë±°„ÄÇÈ¶ñÂÖàÔºåÂèëÁé∞‰∫ÜRLVR‰∏≠ÁöÑË¥üÂπ≤Êâ∞ÔºåÂç≥Â≠¶‰π†Ëß£ÂÜ≥Êüê‰∫õËÆ≠ÁªÉÈóÆÈ¢ò‰ºöÈôç‰ΩéÂÖ∂‰ªñÈóÆÈ¢òÊ≠£Á°ÆËß£ÁöÑÂèØËÉΩÊÄßÔºåÂØºËá¥Pass@$k$ÊÄßËÉΩ‰∏ãÈôç„ÄÇÂÖ∂Ê¨°ÔºåÊè≠Á§∫‰∫ÜËµ¢ËÄÖÈÄöÂêÉÁé∞Ë±°ÔºöRLVR‰∏çÊàêÊØî‰æãÂú∞Âº∫ÂåñÂü∫Á°ÄÊ®°Âûã‰∏ãÈ´òÊ¶ÇÁéáÁöÑÊ≠£Á°ÆËß£ÈóÆÈ¢òÔºåÂêåÊó∂ÊäëÂà∂ÂÖ∂‰ªñÂàùÂßãÊ¶ÇÁéáËæÉ‰ΩéÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂú®Â§ö‰∏™Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äÁöÑÁêÜËÆ∫ÂíåÂÆûËØÅÂàÜÊûêÔºåË°®ÊòéËøôÁßçÊïàÂ∫îÊ∫ê‰∫éÊ†áÂáÜRLÁõÆÊ†á‰∏≠Âõ∫ÊúâÁöÑon-policyÈááÊ†∑ÔºåÂØºËá¥Ê®°ÂûãÊî∂ÊïõÂà∞Áã≠Á™ÑÁöÑËß£ÂÜ≥ÊñπÊ°àÁ≠ñÁï•„ÄÇÂü∫‰∫éËøô‰∫õËßÅËß£ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïÔºåÂ∞ÜRLVRÂ≠¶‰π†ÈõÜ‰∏≠Âú®‰ΩéÊ¶ÇÁéáÈóÆÈ¢ò‰∏äÔºå‰ªéËÄåÊòæËëóÊèêÈ´ò‰∫ÜPass@$k$ÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÊèêÂçáÊñπÊ≥ïÔºåÁâπÂà´ÊòØ‰ΩøÁî®ÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†(RLVR)ÔºåËôΩÁÑ∂Âú®Êüê‰∫õÊñπÈù¢ÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÂ≠òÂú®‰∏Ä‰∏™ÊÇñËÆ∫ÔºöÂÆÉ‰ª¨ÂèØËÉΩ‰ºöÁº©Â∞èÊ®°ÂûãÁöÑÊé®ÁêÜËæπÁïåÔºåÂç≥Ê®°ÂûãÂè™ËÉΩËß£ÂÜ≥ÁâπÂÆöÁ±ªÂûãÁöÑÈóÆÈ¢òÔºåËÄåÂØπÂÖ∂‰ªñÈóÆÈ¢òÁöÑËß£ÂÜ≥ËÉΩÂäõ‰∏ãÈôç„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÂÖÖÂàÜËÄÉËôë‰∏çÂêåÈóÆÈ¢ò‰πãÈó¥ÁöÑÁõ∏‰∫íÂΩ±ÂìçÔºå‰ª•ÂèäÂº∫ÂåñÂ≠¶‰π†ÁöÑÈááÊ†∑ÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊ∑±ÂÖ•ÂàÜÊûêRLVRÁöÑÂ≠¶‰π†Âä®ÊÄÅÔºåÊè≠Á§∫ÂØºËá¥Êé®ÁêÜËæπÁïåÊî∂Áº©ÁöÑÊ†πÊú¨ÂéüÂõ†„ÄÇÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÂíåÂÆûÈ™åÈ™åËØÅÔºåÂèëÁé∞‰∫ÜË¥üÂπ≤Êâ∞ÂíåËµ¢ËÄÖÈÄöÂêÉ‰∏§ÁßçÁé∞Ë±°„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊèêÂá∫‰∏ÄÁßçÊï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïÔºåÊúâÈÄâÊã©ÊÄßÂú∞ÂÖ≥Ê≥®ÈÇ£‰∫õÂú®ÂàùÂßãÈò∂ÊÆµË°®Áé∞‰∏ç‰Ω≥ÁöÑÈóÆÈ¢òÔºå‰ªéËÄåÂπ≥Ë°°Ê®°ÂûãÁöÑÂ≠¶‰π†ËøáÁ®ãÔºåÊâ©Â§ßÊé®ÁêÜËæπÁïå„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºö1) ‰ΩøÁî®RLVRËÆ≠ÁªÉLLMÔºå2) ÂàÜÊûêRLVRÁöÑÂ≠¶‰π†Âä®ÊÄÅÔºåÊè≠Á§∫Ë¥üÂπ≤Êâ∞ÂíåËµ¢ËÄÖÈÄöÂêÉÁé∞Ë±°Ôºå3) ËÆæËÆ°Âπ∂Â∫îÁî®Êï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïÔºåÈáçÊñ∞ÂàÜÈÖçËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊùÉÈáçÔºå‰ΩøÊ®°ÂûãÊõ¥Â§öÂú∞ÂÖ≥Ê≥®‰ΩéÊ¶ÇÁéáÈóÆÈ¢ò„ÄÇÊï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïÁöÑÂÖ∑‰ΩìÊµÅÁ®ãÊòØÔºöÈ¶ñÂÖàËØÑ‰º∞ÊØè‰∏™ÈóÆÈ¢òÂú®Âü∫Á°ÄÊ®°Âûã‰∏ãÁöÑË°®Áé∞Ôºà‰æãÂ¶ÇÔºåPass@$k$ÔºâÔºåÁÑ∂ÂêéÊ†πÊçÆË°®Áé∞ÂØπÈóÆÈ¢òËøõË°åÊéíÂ∫èÔºåÊúÄÂêéÂ¢ûÂä†Ë°®Áé∞ËæÉÂ∑ÆÁöÑÈóÆÈ¢òÂú®ËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊùÉÈáç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) È¶ñÊ¨°Êè≠Á§∫‰∫ÜRLVRÂú®LLMÊé®ÁêÜËÉΩÂäõÊèêÂçá‰∏≠Â≠òÂú®ÁöÑÊé®ÁêÜËæπÁïåÊî∂Áº©ÊÇñËÆ∫„ÄÇ2) Ê∑±ÂÖ•ÂàÜÊûê‰∫ÜÂØºËá¥ËØ•ÊÇñËÆ∫ÁöÑË¥üÂπ≤Êâ∞ÂíåËµ¢ËÄÖÈÄöÂêÉÁé∞Ë±°„ÄÇ3) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÊï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïÔºåËÉΩÂ§üÊòæËëóÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊõ¥Âä†ÂÖ≥Ê≥®ÈóÆÈ¢òÁöÑÂ§öÊ†∑ÊÄßÂíåÊ®°ÂûãÁöÑÂ≠¶‰π†Âπ≥Ë°°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂ¶Ç‰ΩïÈÄâÊã©ÈúÄË¶ÅÈáçÁÇπÂÖ≥Ê≥®ÁöÑ‰ΩéÊ¶ÇÁéáÈóÆÈ¢ò„ÄÇËÆ∫ÊñáÈááÁî®‰∫Ü‰∏ÄÁßçÂü∫‰∫éPass@$k$ÁöÑÁÆÄÂçïÁ≠ñÁï•ÔºöËÆ°ÁÆóÊØè‰∏™ÈóÆÈ¢òÂú®Âü∫Á°ÄÊ®°Âûã‰∏ãÁöÑPass@$k$ÂÄºÔºåÁÑ∂ÂêéÈÄâÊã©Pass@$k$ÂÄºËæÉ‰ΩéÁöÑÈóÆÈ¢òÔºåÂπ∂Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Â¢ûÂä†Ëøô‰∫õÈóÆÈ¢òÁöÑÊùÉÈáç„ÄÇÂÖ∑‰ΩìÁöÑÊùÉÈáçË∞ÉÊï¥Á≠ñÁï•ÂèØ‰ª•Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµËøõË°åË∞ÉÊï¥Ôºå‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®‰∏Ä‰∏™ÁÆÄÂçïÁöÑÁ∫øÊÄßÂáΩÊï∞ÔºåÂ∞ÜPass@$k$ÂÄºÊò†Â∞ÑÂà∞ÊùÉÈáçÂÄº„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊï∞ÊçÆÁ≠ñÂ±ïÁÆóÊ≥ïËÉΩÂ§üÊòæËëóÊèêÂçáLLMÂú®Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äÁöÑPass@$k$ÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õÂü∫ÂáÜ‰∏äÔºåPass@$k$ÂÄºÊèêÂçá‰∫ÜË∂ÖËøá10‰∏™ÁôæÂàÜÁÇπ„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åËøòÈ™åËØÅ‰∫ÜË¥üÂπ≤Êâ∞ÂíåËµ¢ËÄÖÈÄöÂêÉÁé∞Ë±°ÁöÑÂ≠òÂú®Ôºå‰∏∫ÁêÜËß£RLVRÁöÑÂ≠¶‰π†Âä®ÊÄÅÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑ insights„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊèêÂçáÂêÑÁßçLLMÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞Â≠¶„ÄÅÈÄªËæëÁ≠âÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÁöÑÈ¢ÜÂüü„ÄÇÈÄöËøáËß£ÂÜ≥Êé®ÁêÜËæπÁïåÊî∂Áº©ÈóÆÈ¢òÔºåÂèØ‰ª•ÊèêÈ´òLLMÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºå‰æãÂ¶ÇÂú®Êô∫ËÉΩÂÆ¢Êúç„ÄÅËá™Âä®ÁºñÁ®ã„ÄÅÁßëÂ≠¶Á†îÁ©∂Á≠âÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.

