---
layout: default
title: "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning"
---

# Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01681" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01681v1</a>
  <a href="https://arxiv.org/pdf/2510.01681.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01681v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01681v1', 'Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuchen Li, Xuzhao Li, Jiahui Gao, Renjie Pi, Shiyu Hu, Wentao Zhang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: Preprint, Under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºRolloutå¼•å¯¼çš„è‡ªé€‚åº”åƒç´ ç©ºé—´æ¨ç†æ¡†æ¶ï¼Œæå‡VLMåœ¨ç»†ç²’åº¦è§†è§‰ä»»åŠ¡ä¸Šçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `åƒç´ ç©ºé—´æ¨ç†` `è‡ªé€‚åº”æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨å¤„ç†ç»†ç²’åº¦è§†è§‰ä¿¡æ¯æ—¶å­˜åœ¨ä¿¡æ¯æŸå¤±å’Œæ³¨æ„åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
2. æå‡ºä¸€ç§è‡ªé€‚åº”åƒç´ æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡rolloutå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ åŠ¨æ€å†³å®šä½•æ—¶ä½¿ç”¨åƒç´ çº§æ“ä½œã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†ä¸å¿…è¦çš„è§†è§‰æ“ä½œï¼Œæå‡äº†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)åœ¨è®¸å¤šå¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç»å¸¸éš¾ä»¥å¤„ç†éœ€è¦ç²¾ç¡®ç†è§£å’Œå¤„ç†ç»†ç²’åº¦è§†è§‰å…ƒç´ ã€‚è¿™ä¸»è¦æ˜¯ç”±äºå›¾åƒç¼–ç è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¸¢å¤±æˆ–å¯¹å…³é”®åŒºåŸŸçš„å…³æ³¨ä¸è¶³ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°†åƒç´ çº§è§†è§‰ä¿¡æ¯çº³å…¥æ¨ç†è¿‡ç¨‹æ˜¯æœ‰å¸Œæœ›çš„ï¼Œè¿™ä½¿å¾—VLMèƒ½å¤Ÿåœ¨æ€è€ƒè¿‡ç¨‹ä¸­è®¿é—®é«˜åˆ†è¾¨ç‡çš„è§†è§‰ç»†èŠ‚ã€‚ç„¶è€Œï¼Œè¿™ç§åƒç´ çº§ä¿¡æ¯ç»å¸¸è¢«è¿‡åº¦ä½¿ç”¨ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å¹¶åˆ†æ•£å¯¹ä¸ç›¸å…³è§†è§‰ç»†èŠ‚çš„æ³¨æ„åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªè‡ªé€‚åº”åƒç´ æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºè¾“å…¥æŸ¥è¯¢åŠ¨æ€åœ°ç¡®å®šå¿…è¦çš„åƒç´ çº§æ“ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåº”ç”¨æ“ä½œæ„ŸçŸ¥çš„ç›‘ç£å¾®è°ƒï¼Œä»¥å»ºç«‹æ–‡æœ¬æ¨ç†å’Œè§†è§‰æ“ä½œçš„åŸºçº¿èƒ½åŠ›ï¼Œç„¶åè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„rolloutå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¾èµ–äºæ¨¡å‹è‡ªèº«å“åº”çš„åé¦ˆï¼Œè¿™ä½¿å¾—VLMèƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢éš¾åº¦ç¡®å®šä½•æ—¶åº”è°ƒç”¨åƒç´ æ“ä½œã€‚åœ¨å¹¿æ³›çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†ä¸å¿…è¦çš„è§†è§‰æ“ä½œã€‚ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨HR-Bench 4Kä¸Šå®ç°äº†73.4%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒäº†ä»…20.1%çš„å·¥å…·ä½¿ç”¨ç‡ï¼Œä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæé«˜äº†å‡†ç¡®ç‡ï¼ŒåŒæ—¶é™ä½äº†66.5%çš„å·¥å…·ä½¿ç”¨ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦ç²¾ç»†è§†è§‰ç†è§£çš„ä»»åŠ¡æ—¶ï¼Œå¾€å¾€è¿‡åº¦ä¾èµ–åƒç´ çº§ä¿¡æ¯ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œå¹¶å®¹æ˜“è¢«æ— å…³ä¿¡æ¯å¹²æ‰°ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©æ¨¡å‹åœ¨éœ€è¦æ—¶æ‰å…³æ³¨åƒç´ çº§ç»†èŠ‚ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®©æ¨¡å‹å­¦ä¼šæ ¹æ®è¾“å…¥æŸ¥è¯¢çš„éš¾åº¦ï¼Œè‡ªé€‚åº”åœ°å†³å®šæ˜¯å¦éœ€è¦è¿›è¡Œåƒç´ çº§æ“ä½œã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®è‡ªèº«çš„åé¦ˆæ¥å­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨åƒç´ çº§ä¿¡æ¯ï¼Œä»è€Œåœ¨ä¿è¯å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œè¿›è¡Œæ“ä½œæ„ŸçŸ¥çš„ç›‘ç£å¾®è°ƒï¼Œä½¿æ¨¡å‹å…·å¤‡åŸºæœ¬çš„æ–‡æœ¬æ¨ç†å’Œè§†è§‰æ“ä½œèƒ½åŠ›ã€‚ç„¶åï¼Œä½¿ç”¨rolloutå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œè®©æ¨¡å‹å­¦ä¹ æ ¹æ®æŸ¥è¯¢éš¾åº¦åŠ¨æ€å†³å®šæ˜¯å¦è°ƒç”¨åƒç´ æ“ä½œã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–å¥–åŠ±ï¼Œå¥–åŠ±åŸºäºæ¨¡å‹çš„å‡†ç¡®ç‡å’Œå·¥å…·ä½¿ç”¨ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†rolloutå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸æ¨¡å‹æ ¹æ®è‡ªèº«çš„åé¦ˆæ¥å­¦ä¹ ä½•æ—¶ä½¿ç”¨åƒç´ çº§æ“ä½œã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨åƒç´ çº§ä¿¡æ¯ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œå‡†ç¡®ç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ã€‚æœ¬æ–‡çš„å¥–åŠ±å‡½æ•°ç»¼åˆè€ƒè™‘äº†æ¨¡å‹çš„å‡†ç¡®ç‡å’Œå·¥å…·ä½¿ç”¨ç‡ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ä¿è¯å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå°½å¯èƒ½å‡å°‘å·¥å…·çš„ä½¿ç”¨ã€‚Rolloutç­–ç•¥ç”¨äºä¼°è®¡ä¸åŒåŠ¨ä½œçš„é•¿æœŸå›æŠ¥ï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹çš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ¨¡å‹åœ¨HR-Bench 4Kæ•°æ®é›†ä¸Šå–å¾—äº†73.4%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å·¥å…·ä½¿ç”¨ç‡ä»…ä¸º20.1%ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œå‡†ç¡®ç‡å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶å·¥å…·ä½¿ç”¨ç‡é™ä½äº†66.5%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åƒç´ çº§ä¿¡æ¯ï¼Œå¹¶åœ¨ä¿è¯å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦ç»†ç²’åº¦è§†è§‰ç†è§£çš„å„ç§åœºæ™¯ï¼Œä¾‹å¦‚å›¾åƒç¼–è¾‘ã€è§†è§‰é—®ç­”ã€æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡è‡ªé€‚åº”åœ°åˆ©ç”¨åƒç´ çº§ä¿¡æ¯ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„è§†è§‰ä¿¡æ¯ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–éœ€è¦æƒè¡¡è®¡ç®—æˆæœ¬å’Œæ€§èƒ½çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) excel at many multimodal tasks, yet they frequently struggle with tasks requiring precise understanding and handling of fine-grained visual elements. This is mainly due to information loss during image encoding or insufficient attention to critical regions. Recent work has shown promise by incorporating pixel-level visual information into the reasoning process, enabling VLMs to access high-resolution visual details during their thought process. However, this pixel-level information is often overused, leading to inefficiency and distraction from irrelevant visual details. To address these challenges, we propose the first framework for adaptive pixel reasoning that dynamically determines necessary pixel-level operations based on the input query. Specifically, we first apply operation-aware supervised fine-tuning to establish baseline competence in textual reasoning and visual operations, then design a novel rollout-guided reinforcement learning framework relying on feedback of the model's own responses, which enables the VLM to determine when pixel operations should be invoked based on query difficulty. Experiments on extensive multimodal reasoning benchmarks show that our model achieves superior performance while significantly reducing unnecessary visual operations. Impressively, our model achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of only 20.1\%, improving accuracy and simultaneously reducing tool usage by 66.5\% compared to the previous methods.

