---
layout: default
title: Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning
---

# Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01681" target="_blank" class="toolbar-btn">arXiv: 2510.01681v1</a>
    <a href="https://arxiv.org/pdf/2510.01681.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01681v1" 
            onclick="toggleFavorite(this, '2510.01681v1', 'Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xuchen Li, Xuzhao Li, Jiahui Gao, Renjie Pi, Shiyu Hu, Wentao Zhang

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02

**Â§áÊ≥®**: Preprint, Under review

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éRolloutÂºïÂØºÁöÑËá™ÈÄÇÂ∫îÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜÊ°ÜÊû∂ÔºåÊèêÂçáVLMÂú®ÁªÜÁ≤íÂ∫¶ËßÜËßâ‰ªªÂä°‰∏äÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜ` `Ëá™ÈÄÇÂ∫îÊé®ÁêÜ` `Âº∫ÂåñÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLMÂú®Â§ÑÁêÜÁªÜÁ≤íÂ∫¶ËßÜËßâ‰ø°ÊÅØÊó∂Â≠òÂú®‰ø°ÊÅØÊçüÂ§±ÂíåÊ≥®ÊÑèÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÊÄßËÉΩÁì∂È¢à„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçËá™ÈÄÇÂ∫îÂÉèÁ¥†Êé®ÁêÜÊ°ÜÊû∂ÔºåÈÄöËøárolloutÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†Âä®ÊÄÅÂÜ≥ÂÆö‰ΩïÊó∂‰ΩøÁî®ÂÉèÁ¥†Á∫ßÊìç‰Ωú„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®ÊèêÈ´òÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰Ωé‰∫Ü‰∏çÂøÖË¶ÅÁöÑËßÜËßâÊìç‰ΩúÔºåÊèêÂçá‰∫ÜÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)Âú®ËÆ∏Â§öÂ§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁªèÂ∏∏Èöæ‰ª•Â§ÑÁêÜÈúÄË¶ÅÁ≤æÁ°ÆÁêÜËß£ÂíåÂ§ÑÁêÜÁªÜÁ≤íÂ∫¶ËßÜËßâÂÖÉÁ¥†„ÄÇËøô‰∏ªË¶ÅÊòØÁî±‰∫éÂõæÂÉèÁºñÁ†ÅËøáÁ®ã‰∏≠ÁöÑ‰ø°ÊÅØ‰∏¢Â§±ÊàñÂØπÂÖ≥ÈîÆÂå∫ÂüüÁöÑÂÖ≥Ê≥®‰∏çË∂≥„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ÜÂÉèÁ¥†Á∫ßËßÜËßâ‰ø°ÊÅØÁ∫≥ÂÖ•Êé®ÁêÜËøáÁ®ãÊòØÊúâÂ∏åÊúõÁöÑÔºåËøô‰ΩøÂæóVLMËÉΩÂ§üÂú®ÊÄùËÄÉËøáÁ®ã‰∏≠ËÆøÈóÆÈ´òÂàÜËæ®ÁéáÁöÑËßÜËßâÁªÜËäÇ„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÂÉèÁ¥†Á∫ß‰ø°ÊÅØÁªèÂ∏∏Ë¢´ËøáÂ∫¶‰ΩøÁî®ÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÂπ∂ÂàÜÊï£ÂØπ‰∏çÁõ∏ÂÖ≥ËßÜËßâÁªÜËäÇÁöÑÊ≥®ÊÑèÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™Ëá™ÈÄÇÂ∫îÂÉèÁ¥†Êé®ÁêÜÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âü∫‰∫éËæìÂÖ•Êü•ËØ¢Âä®ÊÄÅÂú∞Á°ÆÂÆöÂøÖË¶ÅÁöÑÂÉèÁ¥†Á∫ßÊìç‰Ωú„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨È¶ñÂÖàÂ∫îÁî®Êìç‰ΩúÊÑüÁü•ÁöÑÁõëÁù£ÂæÆË∞ÉÔºå‰ª•Âª∫Á´ãÊñáÊú¨Êé®ÁêÜÂíåËßÜËßâÊìç‰ΩúÁöÑÂü∫Á∫øËÉΩÂäõÔºåÁÑ∂ÂêéËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑrolloutÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂‰æùËµñ‰∫éÊ®°ÂûãËá™Ë∫´ÂìçÂ∫îÁöÑÂèçÈ¶àÔºåËøô‰ΩøÂæóVLMËÉΩÂ§üÊ†πÊçÆÊü•ËØ¢ÈöæÂ∫¶Á°ÆÂÆö‰ΩïÊó∂Â∫îË∞ÉÁî®ÂÉèÁ¥†Êìç‰Ωú„ÄÇÂú®ÂπøÊ≥õÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÂü∫ÂáÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂÆûÁé∞‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÊòæËëóÂáèÂ∞ë‰∫Ü‰∏çÂøÖË¶ÅÁöÑËßÜËßâÊìç‰Ωú„ÄÇ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊòØÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®HR-Bench 4K‰∏äÂÆûÁé∞‰∫Ü73.4%ÁöÑÂáÜÁ°ÆÁéáÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰ªÖ20.1%ÁöÑÂ∑•ÂÖ∑‰ΩøÁî®ÁéáÔºå‰∏é‰πãÂâçÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÁéáÔºåÂêåÊó∂Èôç‰Ωé‰∫Ü66.5%ÁöÑÂ∑•ÂÖ∑‰ΩøÁî®Áéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈúÄË¶ÅÁ≤æÁªÜËßÜËßâÁêÜËß£ÁöÑ‰ªªÂä°Êó∂ÔºåÂæÄÂæÄËøáÂ∫¶‰æùËµñÂÉèÁ¥†Á∫ß‰ø°ÊÅØÔºåÂØºËá¥ËÆ°ÁÆóÊïàÁéá‰Ωé‰∏ãÔºåÂπ∂ÂÆπÊòìË¢´Êó†ÂÖ≥‰ø°ÊÅØÂπ≤Êâ∞„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïËÆ©Ê®°ÂûãÂú®ÈúÄË¶ÅÊó∂ÊâçÂÖ≥Ê≥®ÂÉèÁ¥†Á∫ßÁªÜËäÇÔºåÈÅøÂÖç‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÔºåÊòØÊú¨ÊñáË¶ÅËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËÆ©Ê®°ÂûãÂ≠¶‰ºöÊ†πÊçÆËæìÂÖ•Êü•ËØ¢ÁöÑÈöæÂ∫¶ÔºåËá™ÈÄÇÂ∫îÂú∞ÂÜ≥ÂÆöÊòØÂê¶ÈúÄË¶ÅËøõË°åÂÉèÁ¥†Á∫ßÊìç‰Ωú„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºåÊ®°ÂûãÂèØ‰ª•Ê†πÊçÆËá™Ë∫´ÁöÑÂèçÈ¶àÊù•Â≠¶‰π†‰ΩïÊó∂‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®ÂÉèÁ¥†Á∫ß‰ø°ÊÅØÔºå‰ªéËÄåÂú®‰øùËØÅÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàÔºåËøõË°åÊìç‰ΩúÊÑüÁü•ÁöÑÁõëÁù£ÂæÆË∞ÉÔºå‰ΩøÊ®°ÂûãÂÖ∑Â§áÂü∫Êú¨ÁöÑÊñáÊú¨Êé®ÁêÜÂíåËßÜËßâÊìç‰ΩúËÉΩÂäõ„ÄÇÁÑ∂ÂêéÔºå‰ΩøÁî®rolloutÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåËÆ©Ê®°ÂûãÂ≠¶‰π†Ê†πÊçÆÊü•ËØ¢ÈöæÂ∫¶Âä®ÊÄÅÂÜ≥ÂÆöÊòØÂê¶Ë∞ÉÁî®ÂÉèÁ¥†Êìç‰Ωú„ÄÇÂº∫ÂåñÂ≠¶‰π†ÁöÑÁõÆÊ†áÊòØÊúÄÂ§ßÂåñÂ•ñÂä±ÔºåÂ•ñÂä±Âü∫‰∫éÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéáÂíåÂ∑•ÂÖ∑‰ΩøÁî®Áéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÊèêÂá∫‰∫ÜrolloutÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÂÖÅËÆ∏Ê®°ÂûãÊ†πÊçÆËá™Ë∫´ÁöÑÂèçÈ¶àÊù•Â≠¶‰π†‰ΩïÊó∂‰ΩøÁî®ÂÉèÁ¥†Á∫ßÊìç‰Ωú„ÄÇ‰∏é‰ª•ÂæÄÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÂÉèÁ¥†Á∫ß‰ø°ÊÅØÔºåÈÅøÂÖçËøáÂ∫¶‰ΩøÁî®Ôºå‰ªéËÄåÊèêÈ´òÊïàÁéáÂíåÂáÜÁ°ÆÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÔºåÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÁöÑÂ•ñÂä±ÂáΩÊï∞ÁªºÂêàËÄÉËôë‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéáÂíåÂ∑•ÂÖ∑‰ΩøÁî®ÁéáÔºåÈºìÂä±Ê®°ÂûãÂú®‰øùËØÅÂáÜÁ°ÆÁéáÁöÑÂâçÊèê‰∏ãÔºåÂ∞ΩÂèØËÉΩÂáèÂ∞ëÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®„ÄÇRolloutÁ≠ñÁï•Áî®‰∫é‰º∞ËÆ°‰∏çÂêåÂä®‰ΩúÁöÑÈïøÊúüÂõûÊä•Ôºå‰ªéËÄåÊåáÂØºÊ®°ÂûãÁöÑÂ≠¶‰π†„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Ê®°ÂûãÂú®HR-Bench 4KÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü73.4%ÁöÑÂáÜÁ°ÆÁéáÔºåÂêåÊó∂Â∑•ÂÖ∑‰ΩøÁî®Áéá‰ªÖ‰∏∫20.1%„ÄÇ‰∏é‰πãÂâçÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÂáÜÁ°ÆÁéáÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçáÔºåÂêåÊó∂Â∑•ÂÖ∑‰ΩøÁî®ÁéáÈôç‰Ωé‰∫Ü66.5%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®ÂÉèÁ¥†Á∫ß‰ø°ÊÅØÔºåÂπ∂Âú®‰øùËØÅÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÈúÄË¶ÅÁªÜÁ≤íÂ∫¶ËßÜËßâÁêÜËß£ÁöÑÂêÑÁßçÂú∫ÊôØÔºå‰æãÂ¶ÇÂõæÂÉèÁºñËæë„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠â„ÄÇÈÄöËøáËá™ÈÄÇÂ∫îÂú∞Âà©Áî®ÂÉèÁ¥†Á∫ß‰ø°ÊÅØÔºåÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÂíåÊïàÁéáÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ§ÑÁêÜÂ§çÊùÇÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÇËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êé®ÂπøÂà∞ÂÖ∂‰ªñÈúÄË¶ÅÊùÉË°°ËÆ°ÁÆóÊàêÊú¨ÂíåÊÄßËÉΩÁöÑ‰ªªÂä°‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Models (VLMs) excel at many multimodal tasks, yet they frequently struggle with tasks requiring precise understanding and handling of fine-grained visual elements. This is mainly due to information loss during image encoding or insufficient attention to critical regions. Recent work has shown promise by incorporating pixel-level visual information into the reasoning process, enabling VLMs to access high-resolution visual details during their thought process. However, this pixel-level information is often overused, leading to inefficiency and distraction from irrelevant visual details. To address these challenges, we propose the first framework for adaptive pixel reasoning that dynamically determines necessary pixel-level operations based on the input query. Specifically, we first apply operation-aware supervised fine-tuning to establish baseline competence in textual reasoning and visual operations, then design a novel rollout-guided reinforcement learning framework relying on feedback of the model's own responses, which enables the VLM to determine when pixel operations should be invoked based on query difficulty. Experiments on extensive multimodal reasoning benchmarks show that our model achieves superior performance while significantly reducing unnecessary visual operations. Impressively, our model achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of only 20.1\%, improving accuracy and simultaneously reducing tool usage by 66.5\% compared to the previous methods.

