---
layout: default
title: Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations
---

# Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01576" target="_blank" class="toolbar-btn">arXiv: 2510.01576v1</a>
    <a href="https://arxiv.org/pdf/2510.01576.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01576v1" 
            onclick="toggleFavorite(this, '2510.01576v1', 'Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ricardo Gonzalez Penuela, Felipe Arias-Russi, Victor Capriles

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.HC

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02

**Â§áÊ≥®**: 7 pages, 2 figure, 2 tables, CV4A11y Workshop at ICCV 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Âà©Áî®Áõ≤‰∫∫Âíå‰ΩéËßÜÂäõ‰∫∫Áæ§ËßÜËßâÈóÆÈ¢òÂºïÂØºÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÂÆûÁé∞‰∏ªÂä®ËßÜËßâËß£ËØª**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ËßÜËßâËß£ËØª` `Áõ≤‰∫∫ËæÖÂä©` `‰ΩéËßÜÂäõËæÖÂä©` `‰∏ä‰∏ãÊñáÊÑüÁü•` `ÈóÆÈ¢òÂºïÂØº` `VizWiz-LFÊï∞ÊçÆÈõÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMËßÜËßâËß£ËØªÂ∫îÁî®‰∏∫BLVÁî®Êà∑Êèê‰æõ‰ø°ÊÅØÊó∂ÔºåÁº∫‰πèÈíàÂØπÊÄßÔºå‰∫ßÁîüÂÜó‰Ωô‰ø°ÊÅØÔºåÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫Âà©Áî®ÂéÜÂè≤BLVÁî®Êà∑ÊèêÈóÆÊï∞ÊçÆÔºåÂºïÂØºMLLMÁîüÊàêÊõ¥Á¨¶ÂêàÁî®Êà∑ÈúÄÊ±ÇÁöÑ‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊèèËø∞„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÁîüÊàêÁöÑÊèèËø∞Âú®È¢ÑÊµãÁî®Êà∑ÈúÄÊ±ÇÂíåÁî®Êà∑ÂÅèÂ•ΩÊñπÈù¢Âùá‰ºò‰∫éÊó†‰∏ä‰∏ãÊñáÊèèËø∞„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLM)Âõ†ÂÖ∂ÂáÜÁ°ÆÊÄßÂíåÊèê‰æõ‰∏∞ÂØå„ÄÅÁ±ª‰∫∫Ëß£ËØªÁöÑËÉΩÂäõËÄåË¢´ÈõÜÊàêÂà∞ËßÜËßâËß£ËØªÂ∫îÁî®‰∏≠Ôºå‰ª•ÊîØÊåÅÁõ≤‰∫∫Âíå‰ΩéËßÜÂäõ(BLV)Áî®Êà∑„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÂ∫îÁî®ÈÄöÂ∏∏ÈªòËÆ§Êèê‰æõÂÖ®Èù¢„ÄÅÂÜóÈïøÁöÑÊèèËø∞ÔºåËÄåÂøΩÁï•‰∫Ü‰∏ä‰∏ãÊñá„ÄÇËøôÂØºËá¥‰∫Ü‰ΩéÊïàÁöÑ‰∫§ÊµÅÔºåÂõ†‰∏∫Áî®Êà∑ÂøÖÈ°ªÁ≠õÈÄâ‰∏çÁõ∏ÂÖ≥ÁöÑÁªÜËäÇÔºåËÄå‰∏çÊòØÊé•Êî∂‰ªñ‰ª¨ÂèØËÉΩÂØªÊ±ÇÁöÑÁâπÂÆö‰ø°ÊÅØ„ÄÇ‰∏∫‰∫ÜÊèê‰æõÊõ¥ÂÖ∑‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄßÁöÑ‰ø°ÊÅØÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Á≥ªÁªüÔºåËØ•Á≥ªÁªüÂà©Áî®‰∫ÜÂéÜÂè≤BLVÁî®Êà∑ÁöÑÈóÆÈ¢ò„ÄÇÂΩìÁªôÂÆö‰∏ÄÂº†ÂõæÂÉèÊó∂ÔºåÊàë‰ª¨ÁöÑÁ≥ªÁªü‰ªéVizWiz-LFÊï∞ÊçÆÈõÜ‰∏≠ËØÜÂà´Âá∫Áõ∏‰ººÁöÑËøáÂéªËßÜËßâ‰∏ä‰∏ãÊñáÔºåÂπ∂‰ΩøÁî®Áõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÊù•ÂºïÂØºMLLMÁîüÊàê‰∏éBLVÁî®Êà∑Êõ¥Áõ∏ÂÖ≥ÁöÑÊèèËø∞„ÄÇÂØπ92‰∏™‰∏ä‰∏ãÊñáÊÑüÁü•ÂíåÊó†‰∏ä‰∏ãÊñáÊèèËø∞ÁöÑËØÑ‰º∞ÊòæÁ§∫Ôºå‰∏ä‰∏ãÊñáÊÑüÁü•ÊèèËø∞Âú®76.1%ÁöÑÊÉÖÂÜµ‰∏ã(92‰∏™‰∏≠ÁöÑ70‰∏™)È¢ÑÊµãÂπ∂ÂõûÁ≠î‰∫ÜÁî®Êà∑ÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏îÂú®54.4%ÁöÑÊØîËæÉ‰∏≠(92‰∏™‰∏≠ÁöÑ50‰∏™)Êõ¥ÂèóÊ¨¢Ëøé„ÄÇÊàë‰ª¨ÁöÑËÆ∫ÊñáÂÆ°Êü•ÂíåÊï∞ÊçÆÂàÜÊûêÂèØÂú®GithubÂ≠òÂÇ®Â∫ìhttps://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions ‰∏≠ÂÖ¨ÂºÄËé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÈù¢ÂêëÁõ≤‰∫∫Âíå‰ΩéËßÜÂäõ‰∫∫Áæ§ÁöÑËßÜËßâËß£ËØªÂ∫îÁî®ÔºåÂú®‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåÈÄöÂ∏∏Êèê‰æõÂÜóÈïø‰∏îÂÖ®Èù¢ÁöÑÊèèËø∞ÔºåÂøΩÁï•‰∫ÜÁî®Êà∑ÂÆûÈôÖÁöÑÈúÄÊ±ÇÂíå‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇËøôÂØºËá¥Áî®Êà∑ÈúÄË¶ÅËä±Ë¥πÂ§ßÈáèÊó∂Èó¥Á≠õÈÄâ‰ø°ÊÅØÔºåÊïàÁéá‰Ωé‰∏ã„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÊñπÊ≥ïËÉΩÂ§üËÆ©MLLMÁîüÊàêÊõ¥ÂÖ∑ÈíàÂØπÊÄßÂíå‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄßÁöÑÊèèËø∞Ôºå‰ª•Êª°Ë∂≥BLVÁî®Êà∑ÁöÑÁâπÂÆöÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËØ•ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂéÜÂè≤BLVÁî®Êà∑ÊèêÂá∫ÁöÑÈóÆÈ¢òÊù•ÂºïÂØºMLLMÁîüÊàêÊèèËø∞„ÄÇÈÄöËøáÊ£ÄÁ¥¢‰∏éÂΩìÂâçÂõæÂÉèÁõ∏‰ººÁöÑËßÜËßâ‰∏ä‰∏ãÊñáÔºåÂπ∂ÊèêÂèñÁõ∏ÂÖ≥ÁöÑÁî®Êà∑ÈóÆÈ¢òÔºåÂèØ‰ª•ÊúâÊïàÂú∞ÊåáÂØºMLLMÂÖ≥Ê≥®Áî®Êà∑ÊúÄÂÖ≥ÂøÉÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÁîüÊàêÊõ¥ÁÆÄÊ¥Å„ÄÅÊõ¥Áõ∏ÂÖ≥ÁöÑÊèèËø∞„ÄÇËøôÁßçÊñπÊ≥ïÁöÑÊ†∏ÂøÉÂú®‰∫éÂ∞ÜÁî®Êà∑ÁöÑÊèêÈóÆ‰Ωú‰∏∫‰∏ÄÁßçÂÖàÈ™åÁü•ËØÜÔºåËûçÂÖ•Âà∞MLLMÁöÑËßÜËßâËß£ËØªËøáÁ®ã‰∏≠„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Á≥ªÁªü‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1. ÂõæÂÉèËæìÂÖ•Ê®°ÂùóÔºöÊé•Êî∂ÂæÖËß£ËØªÁöÑÂõæÂÉè„ÄÇ2. ËßÜËßâ‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢Ê®°ÂùóÔºöÂú®VizWiz-LFÊï∞ÊçÆÈõÜ‰∏≠Ê£ÄÁ¥¢‰∏éËæìÂÖ•ÂõæÂÉèÁõ∏‰ººÁöÑËßÜËßâ‰∏ä‰∏ãÊñá„ÄÇ3. ÈóÆÈ¢òÊèêÂèñÊ®°ÂùóÔºö‰ªéÊ£ÄÁ¥¢Âà∞ÁöÑËßÜËßâ‰∏ä‰∏ãÊñá‰∏≠ÊèêÂèñÁõ∏ÂÖ≥ÁöÑÁî®Êà∑ÈóÆÈ¢ò„ÄÇ4. MLLMÂºïÂØºÊ®°ÂùóÔºöÂà©Áî®ÊèêÂèñÂà∞ÁöÑÁî®Êà∑ÈóÆÈ¢òÔºåÂºïÂØºMLLMÁîüÊàêÊèèËø∞„ÄÇ5. ÊèèËø∞ËæìÂá∫Ê®°ÂùóÔºöËæìÂá∫ÊúÄÁªàÁöÑËßÜËßâËß£ËØªÊèèËø∞„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºåÁªôÂÆö‰∏ÄÂº†ÂõæÂÉèÔºåÁ≥ªÁªüÈ¶ñÂÖàÊ£ÄÁ¥¢Áõ∏‰ººÁöÑËßÜËßâ‰∏ä‰∏ãÊñáÔºåÁÑ∂ÂêéÊèêÂèñÁõ∏ÂÖ≥ÈóÆÈ¢òÔºåÊúÄÂêéÂà©Áî®Ëøô‰∫õÈóÆÈ¢òÂºïÂØºMLLMÁîüÊàêÊõ¥Á¨¶ÂêàÁî®Êà∑ÈúÄÊ±ÇÁöÑÊèèËø∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂéÜÂè≤BLVÁî®Êà∑ÁöÑÈóÆÈ¢ò‰Ωú‰∏∫‰∏ÄÁßçÂºïÂØº‰ø°Âè∑ÔºåÁî®‰∫éÊåáÂØºMLLMÁîüÊàêËßÜËßâËß£ËØªÊèèËø∞„ÄÇ‰∏é‰º†ÁªüÁöÑÊó†‰∏ä‰∏ãÊñáÊèèËø∞ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÁî®Êà∑ÁöÑÈúÄÊ±ÇÔºåÁîüÊàêÊõ¥ÂÖ∑ÈíàÂØπÊÄßÂíå‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄßÁöÑÊèèËø∞„ÄÇËøôÁßçÊñπÊ≥ïÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫ÜÁé∞ÊúâMLLMËßÜËßâËß£ËØªÂ∫îÁî®‰∏≠‰ø°ÊÅØÂÜó‰ΩôÁöÑÈóÆÈ¢òÔºåÊèêÈ´ò‰∫ÜÁî®Êà∑Ëé∑Âèñ‰ø°ÊÅØÁöÑÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠Ê≤°ÊúâËØ¶ÁªÜÊèèËø∞ÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞ÊàñÁΩëÁªúÁªìÊûÑÁ≠âÊäÄÊúØÁªÜËäÇ„ÄÇ‰ΩÜÊòØÔºåËßÜËßâ‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢Ê®°ÂùóÁöÑÁõ∏‰ººÂ∫¶Â∫¶ÈáèÊñπÂºè„ÄÅÈóÆÈ¢òÊèêÂèñÊ®°ÂùóÁöÑÁ≠ñÁï•‰ª•ÂèäMLLMÂºïÂØºÊ®°ÂùóÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÔºà‰æãÂ¶ÇÔºåÂ¶Ç‰ΩïÂ∞ÜÈóÆÈ¢òËûçÂÖ•Âà∞MLLMÁöÑËæìÂÖ•ÊàñËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºâÊòØÂΩ±ÂìçÁ≥ªÁªüÊÄßËÉΩÁöÑÂÖ≥ÈîÆËÆæËÆ°Âõ†Á¥†„ÄÇËøô‰∫õÁªÜËäÇÈúÄË¶ÅÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ËøõË°å‰ªîÁªÜÁöÑË∞ÉÊï¥Âíå‰ºòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

‰∫∫Â∑•ËØÑ‰º∞ÁªìÊûúÊòæÁ§∫Ôºå‰∏ä‰∏ãÊñáÊÑüÁü•ÊèèËø∞Âú®76.1%ÁöÑÊÉÖÂÜµ‰∏ãÈ¢ÑÊµãÂπ∂ÂõûÁ≠î‰∫ÜÁî®Êà∑ÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏îÂú®54.4%ÁöÑÊØîËæÉ‰∏≠Êõ¥ÂèóÊ¨¢Ëøé„ÄÇËøôË°®ÊòéËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òËßÜËßâËß£ËØªÁöÑÈíàÂØπÊÄßÂíåÁî®Êà∑Êª°ÊÑèÂ∫¶„ÄÇ‰∏éÊó†‰∏ä‰∏ãÊñáÊèèËø∞Áõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞Êª°Ë∂≥BLVÁî®Êà∑ÁöÑÂÆûÈôÖÈúÄÊ±Ç„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈù¢ÂêëÁõ≤‰∫∫Âíå‰ΩéËßÜÂäõ‰∫∫Áæ§ÁöÑËßÜËßâËæÖÂä©ËÆæÂ§áÂíåÂ∫îÁî®Ôºå‰æãÂ¶ÇÊô∫ËÉΩÁúºÈïú„ÄÅÊâãÊú∫Â∫îÁî®Á≠â„ÄÇÈÄöËøáÊèê‰æõÊõ¥ÂÖ∑ÈíàÂØπÊÄßÂíå‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄßÁöÑËßÜËßâËß£ËØªÔºåÂèØ‰ª•Â∏ÆÂä©BLVÁî®Êà∑Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºåÊèêÈ´òÁîüÊ¥ªË¥®Èáè„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ï‰πüÂèØ‰ª•Êé®ÂπøÂà∞ÂÖ∂‰ªñÈúÄË¶Å‰∏™ÊÄßÂåñËßÜËßâËß£ËØªÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÂÆ¢Êúç„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) have been integrated into visual interpretation applications to support Blind and Low Vision (BLV) users because of their accuracy and ability to provide rich, human-like interpretations. However, these applications often default to comprehensive, lengthy descriptions regardless of context. This leads to inefficient exchanges, as users must go through irrelevant details rather than receiving the specific information they are likely to seek. To deliver more contextually-relevant information, we developed a system that draws on historical BLV users questions. When given an image, our system identifies similar past visual contexts from the VizWiz-LF dataset and uses the associated questions to guide the MLLM generate descriptions more relevant to BLV users. An evaluation with three human labelers who revised 92 context-aware and context-free descriptions showed that context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of comparisons (50 out of 92). Our paper reviews, and data analysis are publicly available in a Github repository at https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

