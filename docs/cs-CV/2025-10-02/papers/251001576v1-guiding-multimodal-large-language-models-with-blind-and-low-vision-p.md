---
layout: default
title: Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations
---

# Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01576" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01576v1</a>
  <a href="https://arxiv.org/pdf/2510.01576.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01576v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01576v1', 'Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ricardo Gonzalez Penuela, Felipe Arias-Russi, Victor Capriles

**åˆ†ç±»**: cs.CV, cs.AI, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: 7 pages, 2 figure, 2 tables, CV4A11y Workshop at ICCV 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨ç›²äººå’Œä½è§†åŠ›äººç¾¤è§†è§‰é—®é¢˜å¼•å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°ä¸»åŠ¨è§†è§‰è§£è¯»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰è§£è¯»` `ç›²äººè¾…åŠ©` `ä½è§†åŠ›è¾…åŠ©` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `é—®é¢˜å¼•å¯¼` `VizWiz-LFæ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMè§†è§‰è§£è¯»åº”ç”¨ä¸ºBLVç”¨æˆ·æä¾›ä¿¡æ¯æ—¶ï¼Œç¼ºä¹é’ˆå¯¹æ€§ï¼Œäº§ç”Ÿå†—ä½™ä¿¡æ¯ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
2. è¯¥è®ºæ–‡æå‡ºåˆ©ç”¨å†å²BLVç”¨æˆ·æé—®æ•°æ®ï¼Œå¼•å¯¼MLLMç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„ä¸Šä¸‹æ–‡ç›¸å…³æè¿°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æè¿°åœ¨é¢„æµ‹ç”¨æˆ·éœ€æ±‚å’Œç”¨æˆ·åå¥½æ–¹é¢å‡ä¼˜äºæ— ä¸Šä¸‹æ–‡æè¿°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)å› å…¶å‡†ç¡®æ€§å’Œæä¾›ä¸°å¯Œã€ç±»äººè§£è¯»çš„èƒ½åŠ›è€Œè¢«é›†æˆåˆ°è§†è§‰è§£è¯»åº”ç”¨ä¸­ï¼Œä»¥æ”¯æŒç›²äººå’Œä½è§†åŠ›(BLV)ç”¨æˆ·ã€‚ç„¶è€Œï¼Œè¿™äº›åº”ç”¨é€šå¸¸é»˜è®¤æä¾›å…¨é¢ã€å†—é•¿çš„æè¿°ï¼Œè€Œå¿½ç•¥äº†ä¸Šä¸‹æ–‡ã€‚è¿™å¯¼è‡´äº†ä½æ•ˆçš„äº¤æµï¼Œå› ä¸ºç”¨æˆ·å¿…é¡»ç­›é€‰ä¸ç›¸å…³çš„ç»†èŠ‚ï¼Œè€Œä¸æ˜¯æ¥æ”¶ä»–ä»¬å¯èƒ½å¯»æ±‚çš„ç‰¹å®šä¿¡æ¯ã€‚ä¸ºäº†æä¾›æ›´å…·ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨äº†å†å²BLVç”¨æˆ·çš„é—®é¢˜ã€‚å½“ç»™å®šä¸€å¼ å›¾åƒæ—¶ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿä»VizWiz-LFæ•°æ®é›†ä¸­è¯†åˆ«å‡ºç›¸ä¼¼çš„è¿‡å»è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¹¶ä½¿ç”¨ç›¸å…³çš„é—®é¢˜æ¥å¼•å¯¼MLLMç”Ÿæˆä¸BLVç”¨æˆ·æ›´ç›¸å…³çš„æè¿°ã€‚å¯¹92ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œæ— ä¸Šä¸‹æ–‡æè¿°çš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°åœ¨76.1%çš„æƒ…å†µä¸‹(92ä¸ªä¸­çš„70ä¸ª)é¢„æµ‹å¹¶å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨54.4%çš„æ¯”è¾ƒä¸­(92ä¸ªä¸­çš„50ä¸ª)æ›´å—æ¬¢è¿ã€‚æˆ‘ä»¬çš„è®ºæ–‡å®¡æŸ¥å’Œæ•°æ®åˆ†æå¯åœ¨Githubå­˜å‚¨åº“https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions ä¸­å…¬å¼€è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é¢å‘ç›²äººå’Œä½è§†åŠ›äººç¾¤çš„è§†è§‰è§£è¯»åº”ç”¨ï¼Œåœ¨ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œé€šå¸¸æä¾›å†—é•¿ä¸”å…¨é¢çš„æè¿°ï¼Œå¿½ç•¥äº†ç”¨æˆ·å®é™…çš„éœ€æ±‚å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™å¯¼è‡´ç”¨æˆ·éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´ç­›é€‰ä¿¡æ¯ï¼Œæ•ˆç‡ä½ä¸‹ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•èƒ½å¤Ÿè®©MLLMç”Ÿæˆæ›´å…·é’ˆå¯¹æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„æè¿°ï¼Œä»¥æ»¡è¶³BLVç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å†å²BLVç”¨æˆ·æå‡ºçš„é—®é¢˜æ¥å¼•å¯¼MLLMç”Ÿæˆæè¿°ã€‚é€šè¿‡æ£€ç´¢ä¸å½“å‰å›¾åƒç›¸ä¼¼çš„è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¹¶æå–ç›¸å…³çš„ç”¨æˆ·é—®é¢˜ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼MLLMå…³æ³¨ç”¨æˆ·æœ€å…³å¿ƒçš„ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´ç®€æ´ã€æ›´ç›¸å…³çš„æè¿°ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå°†ç”¨æˆ·çš„æé—®ä½œä¸ºä¸€ç§å…ˆéªŒçŸ¥è¯†ï¼Œèå…¥åˆ°MLLMçš„è§†è§‰è§£è¯»è¿‡ç¨‹ä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç³»ç»Ÿä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1. å›¾åƒè¾“å…¥æ¨¡å—ï¼šæ¥æ”¶å¾…è§£è¯»çš„å›¾åƒã€‚2. è§†è§‰ä¸Šä¸‹æ–‡æ£€ç´¢æ¨¡å—ï¼šåœ¨VizWiz-LFæ•°æ®é›†ä¸­æ£€ç´¢ä¸è¾“å…¥å›¾åƒç›¸ä¼¼çš„è§†è§‰ä¸Šä¸‹æ–‡ã€‚3. é—®é¢˜æå–æ¨¡å—ï¼šä»æ£€ç´¢åˆ°çš„è§†è§‰ä¸Šä¸‹æ–‡ä¸­æå–ç›¸å…³çš„ç”¨æˆ·é—®é¢˜ã€‚4. MLLMå¼•å¯¼æ¨¡å—ï¼šåˆ©ç”¨æå–åˆ°çš„ç”¨æˆ·é—®é¢˜ï¼Œå¼•å¯¼MLLMç”Ÿæˆæè¿°ã€‚5. æè¿°è¾“å‡ºæ¨¡å—ï¼šè¾“å‡ºæœ€ç»ˆçš„è§†è§‰è§£è¯»æè¿°ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œç»™å®šä¸€å¼ å›¾åƒï¼Œç³»ç»Ÿé¦–å…ˆæ£€ç´¢ç›¸ä¼¼çš„è§†è§‰ä¸Šä¸‹æ–‡ï¼Œç„¶åæå–ç›¸å…³é—®é¢˜ï¼Œæœ€ååˆ©ç”¨è¿™äº›é—®é¢˜å¼•å¯¼MLLMç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„æè¿°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å†å²BLVç”¨æˆ·çš„é—®é¢˜ä½œä¸ºä¸€ç§å¼•å¯¼ä¿¡å·ï¼Œç”¨äºæŒ‡å¯¼MLLMç”Ÿæˆè§†è§‰è§£è¯»æè¿°ã€‚ä¸ä¼ ç»Ÿçš„æ— ä¸Šä¸‹æ–‡æè¿°æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç”¨æˆ·çš„éœ€æ±‚ï¼Œç”Ÿæˆæ›´å…·é’ˆå¯¹æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„æè¿°ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†ç°æœ‰MLLMè§†è§‰è§£è¯»åº”ç”¨ä¸­ä¿¡æ¯å†—ä½™çš„é—®é¢˜ï¼Œæé«˜äº†ç”¨æˆ·è·å–ä¿¡æ¯çš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚ä½†æ˜¯ï¼Œè§†è§‰ä¸Šä¸‹æ–‡æ£€ç´¢æ¨¡å—çš„ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼ã€é—®é¢˜æå–æ¨¡å—çš„ç­–ç•¥ä»¥åŠMLLMå¼•å¯¼æ¨¡å—çš„å…·ä½“å®ç°æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œå¦‚ä½•å°†é—®é¢˜èå…¥åˆ°MLLMçš„è¾“å…¥æˆ–è®­ç»ƒè¿‡ç¨‹ä¸­ï¼‰æ˜¯å½±å“ç³»ç»Ÿæ€§èƒ½çš„å…³é”®è®¾è®¡å› ç´ ã€‚è¿™äº›ç»†èŠ‚éœ€è¦åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œä»”ç»†çš„è°ƒæ•´å’Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

äººå·¥è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°åœ¨76.1%çš„æƒ…å†µä¸‹é¢„æµ‹å¹¶å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨54.4%çš„æ¯”è¾ƒä¸­æ›´å—æ¬¢è¿ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜è§†è§‰è§£è¯»çš„é’ˆå¯¹æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚ä¸æ— ä¸Šä¸‹æ–‡æè¿°ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³BLVç”¨æˆ·çš„å®é™…éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§é¢å‘ç›²äººå’Œä½è§†åŠ›äººç¾¤çš„è§†è§‰è¾…åŠ©è®¾å¤‡å’Œåº”ç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½çœ¼é•œã€æ‰‹æœºåº”ç”¨ç­‰ã€‚é€šè¿‡æä¾›æ›´å…·é’ˆå¯¹æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„è§†è§‰è§£è¯»ï¼Œå¯ä»¥å¸®åŠ©BLVç”¨æˆ·æ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œæé«˜ç”Ÿæ´»è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–éœ€è¦ä¸ªæ€§åŒ–è§†è§‰è§£è¯»çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€æ™ºèƒ½å®¶å±…ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have been integrated into visual interpretation applications to support Blind and Low Vision (BLV) users because of their accuracy and ability to provide rich, human-like interpretations. However, these applications often default to comprehensive, lengthy descriptions regardless of context. This leads to inefficient exchanges, as users must go through irrelevant details rather than receiving the specific information they are likely to seek. To deliver more contextually-relevant information, we developed a system that draws on historical BLV users questions. When given an image, our system identifies similar past visual contexts from the VizWiz-LF dataset and uses the associated questions to guide the MLLM generate descriptions more relevant to BLV users. An evaluation with three human labelers who revised 92 context-aware and context-free descriptions showed that context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of comparisons (50 out of 92). Our paper reviews, and data analysis are publicly available in a Github repository at https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

