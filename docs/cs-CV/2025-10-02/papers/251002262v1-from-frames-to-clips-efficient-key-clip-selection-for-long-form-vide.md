---
layout: default
title: From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding
---

# From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02262" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02262v1</a>
  <a href="https://arxiv.org/pdf/2510.02262.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02262v1" onclick="toggleFavorite(this, '2510.02262v1', 'From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºF2Cï¼šé€šè¿‡é«˜æ•ˆå…³é”®ç‰‡æ®µé€‰æ‹©æå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `å…³é”®ç‰‡æ®µé€‰æ‹©` `æ—¶é—´è¿è´¯æ€§` `è‡ªé€‚åº”åˆ†è¾¨ç‡` `è§†é¢‘å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘ç†è§£æ—¶ï¼Œç”±äºè§†è§‰tokensæ•°é‡åºå¤§ï¼Œå®¹æ˜“è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ï¼Œä¸”é€å¸§é€‰æ‹©å¿½ç•¥äº†è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ä¿¡æ¯ã€‚
2. è®ºæ–‡æå‡ºF2Cæ–¹æ³•ï¼Œå°†å…³é”®å¸§é€‰æ‹©æ‰©å±•åˆ°å…³é”®ç‰‡æ®µé€‰æ‹©ï¼Œä¿ç•™æ—¶é—´è¿è´¯æ€§ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”åˆ†è¾¨ç‡ç­–ç•¥å¹³è¡¡ç©ºé—´åˆ†è¾¨ç‡å’Œç‰‡æ®µé•¿åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒF2Cåœ¨ä¸‰ä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºå‡åŒ€é‡‡æ ·ï¼Œè¯æ˜äº†ä¿ç•™æ—¶é—´è¿è´¯æ€§å¯¹äºæå‡é•¿è§†é¢‘ç†è§£çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(VLMs)åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å…¶å®é™…åº”ç”¨å—åˆ°â€œå¤§æµ·æé’ˆâ€é—®é¢˜çš„é™åˆ¶ï¼šåŸå§‹è§†é¢‘å¸§äº§ç”Ÿçš„å¤§é‡è§†è§‰tokensè€—å°½äº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆé€šè¿‡é€‰æ‹©ç¨€ç–çš„å¸§é›†åˆæ¥å‡å°‘tokenæ•°é‡ï¼Œä½†è¿™ç§é€å¸§é€‰æ‹©ä¼šä¸¢å¼ƒé‡è¦çš„æ—¶é—´åŠ¨æ€ä¿¡æ¯ï¼Œå¯¼è‡´å¯¹è¿åŠ¨å’Œäº‹ä»¶è¿ç»­æ€§çš„æ¬¡ä¼˜æ¨ç†ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†æ—¶é—´ä¿¡æ¯çš„å½±å“ï¼Œå¹¶è¯æ˜äº†å°†é€‰æ‹©ä»å­¤ç«‹çš„å…³é”®å¸§æ‰©å±•åˆ°å…³é”®ç‰‡æ®µï¼ˆå³çŸ­çš„ã€æ—¶é—´ä¸Šè¿è´¯çš„ç‰‡æ®µï¼‰å¯ä»¥æ”¹å–„è§†é¢‘ç†è§£ã€‚ä¸ºäº†åœ¨é€‚åº”ç‰‡æ®µè¾ƒå¤§tokenå ç”¨ç©ºé—´çš„åŒæ—¶ä¿æŒå›ºå®šçš„è®¡ç®—é¢„ç®—ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åˆ†è¾¨ç‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŠ¨æ€åœ°å¹³è¡¡ç©ºé—´åˆ†è¾¨ç‡å’Œç‰‡æ®µé•¿åº¦ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘çš„tokenæ•°é‡æ’å®šã€‚åœ¨ä¸‰ä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å…è®­ç»ƒæ–¹æ³•F2Cåœ¨Video-MMEã€LongVideoBenchå’ŒMLVUåŸºå‡†æµ‹è¯•ä¸Šåˆ†åˆ«ä¼˜äºå‡åŒ€é‡‡æ ·é«˜è¾¾8.1%ã€5.6%å’Œ10.3%ã€‚è¿™äº›ç»“æœçªå‡ºäº†åœ¨å¸§é€‰æ‹©ä¸­ä¿æŒæ—¶é—´è¿è´¯æ€§çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå°†è§†é¢‘LLMæ‰©å±•åˆ°å®é™…è§†é¢‘ç†è§£åº”ç”¨æä¾›äº†ä¸€æ¡å¯è¡Œçš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é•¿è§†é¢‘ç†è§£æ–¹æ³•é¢ä¸´â€œå¤§æµ·æé’ˆâ€é—®é¢˜ï¼Œå³è§†é¢‘å¸§æ•°é‡è¿‡å¤šå¯¼è‡´è§†è§‰tokensè¶…å‡ºVideo LLMçš„ä¸Šä¸‹æ–‡çª—å£ã€‚ç®€å•çš„å…³é”®å¸§é€‰æ‹©æ–¹æ³•å¿½ç•¥äº†è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰è¿åŠ¨å’Œäº‹ä»¶çš„è¿ç»­æ€§ï¼Œå¯¼è‡´ç†è§£èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯å°†å…³é”®å¸§çš„é€‰æ‹©æ‰©å±•åˆ°å…³é”®ç‰‡æ®µçš„é€‰æ‹©ã€‚é€šè¿‡é€‰æ‹©çŸ­çš„ã€æ—¶é—´ä¸Šè¿è´¯çš„è§†é¢‘ç‰‡æ®µï¼Œä¿ç•™è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹è§†é¢‘å†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼Œä¸ºäº†æ§åˆ¶è®¡ç®—é‡ï¼Œé‡‡ç”¨è‡ªé€‚åº”åˆ†è¾¨ç‡ç­–ç•¥ï¼ŒåŠ¨æ€å¹³è¡¡ç©ºé—´åˆ†è¾¨ç‡å’Œç‰‡æ®µé•¿åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šF2Cæ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼šå…³é”®ç‰‡æ®µé€‰æ‹©å’Œè‡ªé€‚åº”åˆ†è¾¨ç‡è°ƒæ•´ã€‚é¦–å…ˆï¼Œé€šè¿‡æŸç§ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œå‡åŒ€é‡‡æ ·æˆ–åŸºäºæ˜¾è‘—æ€§çš„é€‰æ‹©ï¼‰é€‰æ‹©å…³é”®ç‰‡æ®µã€‚ç„¶åï¼Œæ ¹æ®ç‰‡æ®µçš„é•¿åº¦å’Œé¢„è®¾çš„tokenæ•°é‡é¢„ç®—ï¼ŒåŠ¨æ€è°ƒæ•´ç‰‡æ®µçš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œä»¥ç¡®ä¿æ¯ä¸ªè§†é¢‘çš„tokenæ•°é‡ä¿æŒæ’å®šã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰çš„Video LLMã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä»å…³é”®å¸§é€‰æ‹©åˆ°å…³é”®ç‰‡æ®µé€‰æ‹©çš„è½¬å˜ã€‚è¿™ç§è½¬å˜ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯ï¼Œä»è€Œæå‡å¯¹è¿åŠ¨å’Œäº‹ä»¶è¿ç»­æ€§çš„ç†è§£ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”åˆ†è¾¨ç‡ç­–ç•¥èƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—é¢„ç®—çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–æ—¶é—´ä¿¡æ¯çš„åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šè‡ªé€‚åº”åˆ†è¾¨ç‡ç­–ç•¥æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªé€‰å®šçš„è§†é¢‘ç‰‡æ®µï¼Œæ ¹æ®å…¶é•¿åº¦è®¡ç®—æ‰€éœ€çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œä»¥æ»¡è¶³é¢„è®¾çš„tokenæ•°é‡é¢„ç®—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç‰‡æ®µé•¿åº¦è¾ƒé•¿ï¼Œåˆ™é™ä½ç©ºé—´åˆ†è¾¨ç‡ï¼›å¦‚æœç‰‡æ®µé•¿åº¦è¾ƒçŸ­ï¼Œåˆ™æé«˜ç©ºé—´åˆ†è¾¨ç‡ã€‚å…·ä½“çš„ç©ºé—´åˆ†è¾¨ç‡è°ƒæ•´æ–¹æ³•å¯ä»¥é‡‡ç”¨åŒçº¿æ€§æ’å€¼ç­‰å›¾åƒç¼©æ”¾æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œå…³é”®ç‰‡æ®µçš„é€‰æ‹©ç­–ç•¥ä¹Ÿä¼šå½±å“æœ€ç»ˆæ€§èƒ½ï¼Œå¯ä»¥é€‰æ‹©å‡åŒ€é‡‡æ ·ã€åŸºäºæ˜¾è‘—æ€§çš„é€‰æ‹©ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

F2Cæ–¹æ³•åœ¨Video-MMEã€LongVideoBenchå’ŒMLVUä¸‰ä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šåˆ†åˆ«å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸å¯¹äºå‡åŒ€é‡‡æ ·ï¼Œåˆ†åˆ«æå‡äº†8.1%ã€5.6%å’Œ10.3%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡é€‰æ‹©å…³é”®ç‰‡æ®µå¹¶ä¿ç•™æ—¶é—´è¿è´¯æ€§ï¼Œå¯ä»¥æœ‰æ•ˆæå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸”è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œæ˜“äºéƒ¨ç½²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦é•¿è§†é¢‘ç†è§£çš„åœºæ™¯ï¼Œä¾‹å¦‚è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘å†…å®¹åˆ†æã€æ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡æå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´å‡†ç¡®çš„äº‹ä»¶æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«å’Œåœºæ™¯ç†è§£ï¼Œä»è€Œæé«˜ç›¸å…³åº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘é—®ç­”ã€è§†é¢‘æ‘˜è¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video Large Language Models (VLMs) have achieved remarkable results on a variety of vision language tasks, yet their practical use is limited by the "needle in a haystack" problem: the massive number of visual tokens produced from raw video frames exhausts the model's context window. Existing solutions alleviate this issue by selecting a sparse set of frames, thereby reducing token count, but such frame-wise selection discards essential temporal dynamics, leading to suboptimal reasoning about motion and event continuity. In this work we systematically explore the impact of temporal information and demonstrate that extending selection from isolated key frames to key clips, which are short, temporally coherent segments, improves video understanding. To maintain a fixed computational budget while accommodating the larger token footprint of clips, we propose an adaptive resolution strategy that dynamically balances spatial resolution and clip length, ensuring a constant token count per video. Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These results highlight the importance of preserving temporal coherence in frame selection and provide a practical pathway for scaling Video LLMs to real world video understanding applications. Project webpage is available at https://guangyusun.com/f2c .

