---
layout: default
title: Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery
---

# Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01662" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01662v1</a>
  <a href="https://arxiv.org/pdf/2510.01662.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01662v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01662v1', 'Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minh Tran, Maksim Siniukov, Zhangyu Jin, Mohammad Soleymani

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¦»æ•£é¢éƒ¨ç¼–ç (DFE)ï¼Œç”¨äºæ•°æ®é©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…å‘ç°ï¼Œæ›¿ä»£FACSã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `é¢éƒ¨è¡¨æƒ…åˆ†æ` `ç¦»æ•£é¢éƒ¨ç¼–ç ` `æ— ç›‘ç£å­¦ä¹ ` `RVQ-VAE` `3Då½¢å˜æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é¢éƒ¨è¡¨æƒ…åˆ†ææ–¹æ³•å¦‚FACSå­˜åœ¨è¦†ç›–èŒƒå›´æœ‰é™å’Œæ ‡æ³¨æˆæœ¬é«˜çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºç¦»æ•£é¢éƒ¨ç¼–ç (DFE)ï¼Œåˆ©ç”¨RVQ-VAEå­¦ä¹ é¢éƒ¨è¡¨æƒ…çš„ç¦»æ•£tokenè¡¨ç¤ºï¼Œå®ç°æ— ç›‘ç£å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDFEåœ¨å‹åŠ›æ£€æµ‹ã€äººæ ¼é¢„æµ‹å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­ä¼˜äºFACSå’ŒMasked Autoencodersã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢éƒ¨è¡¨æƒ…åˆ†ææ˜¯ç†è§£äººç±»è¡Œä¸ºçš„å…³é”®ï¼Œä½†ç°æœ‰çš„ç¼–ç ç³»ç»Ÿï¼Œå¦‚é¢éƒ¨åŠ¨ä½œç¼–ç ç³»ç»Ÿ(FACS)ï¼Œå—åˆ°è¦†ç›–èŒƒå›´æœ‰é™å’Œäººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„ã€æ•°æ®é©±åŠ¨çš„æ›¿ä»£æ–¹æ¡ˆâ€”â€”ç¦»æ•£é¢éƒ¨ç¼–ç (DFE)ï¼Œå®ƒä»3Dç½‘æ ¼åºåˆ—ä¸­å­¦ä¹ ç´§å‡‘ä¸”å¯è§£é‡Šçš„é¢éƒ¨è¡¨æƒ…å­—å…¸ï¼Œè¯¥å­—å…¸é€šè¿‡æ®‹å·®å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨(RVQ-VAE)å­¦ä¹ å¾—åˆ°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨3Då½¢å˜æ¨¡å‹(3DMM)ä»å›¾åƒä¸­æå–ä¸èº«ä»½æ— å…³çš„è¡¨æƒ…ç‰¹å¾ï¼Œæœ‰æ•ˆåœ°è§£è€¦äº†å¤´éƒ¨å§¿åŠ¿å’Œé¢éƒ¨å‡ ä½•ç­‰å› ç´ ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨RVQ-VAEå¯¹è¿™äº›ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œä»å…±äº«ç æœ¬ä¸­ç”Ÿæˆä¸€ç³»åˆ—ç¦»æ•£tokenï¼Œæ¯ä¸ªtokenæ•è·ä¸€ä¸ªç‰¹å®šçš„ã€å¯é‡ç”¨çš„é¢éƒ¨å˜å½¢æ¨¡å¼ï¼Œè¯¥æ¨¡å¼æœ‰åŠ©äºæ•´ä½“è¡¨æƒ…çš„è¡¨è¾¾ã€‚é€šè¿‡å¤§é‡çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ç¦»æ•£é¢éƒ¨ç¼–ç æ¯”FACSå’Œå…¶ä»–é¢éƒ¨ç¼–ç æ›¿ä»£æ–¹æ¡ˆèƒ½æ•æ‰åˆ°æ›´ç²¾ç¡®çš„é¢éƒ¨è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªé«˜å±‚æ¬¡çš„å¿ƒç†å­¦ä»»åŠ¡ä¸­è¯„ä¼°äº†æˆ‘ä»¬è¡¨ç¤ºçš„æ•ˆç”¨ï¼šå‹åŠ›æ£€æµ‹ã€äººæ ¼é¢„æµ‹å’ŒæŠ‘éƒç—‡æ£€æµ‹ã€‚ä½¿ç”¨å»ºç«‹åœ¨å­¦ä¹ åˆ°çš„tokenä¹‹ä¸Šçš„ç®€å•è¯è¢‹æ¨¡å‹ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå§‹ç»ˆä¼˜äºåŸºäºFACSçš„pipelineä»¥åŠå¼ºå¤§çš„å›¾åƒå’Œè§†é¢‘è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ï¼Œå¦‚æ©ç è‡ªç¼–ç å™¨ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¡¨ç¤ºæ¶µç›–äº†æ›´å¹¿æ³›çš„é¢éƒ¨å±•ç¤ºï¼Œçªå‡ºäº†å…¶ä½œä¸ºFACSåœ¨å¿ƒç†å’Œæƒ…æ„Ÿè®¡ç®—åº”ç”¨ä¸­å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é¢éƒ¨è¡¨æƒ…åˆ†ææ–¹æ³•ï¼Œå¦‚FACSï¼Œä¾èµ–äºäººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”è¦†ç›–èŒƒå›´æœ‰é™ï¼Œéš¾ä»¥æ•æ‰ç»†å¾®çš„é¢éƒ¨å˜åŒ–ã€‚è¿™é™åˆ¶äº†å…¶åœ¨å¿ƒç†å­¦å’Œæƒ…æ„Ÿè®¡ç®—ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè‡ªåŠ¨ä»é¢éƒ¨å›¾åƒåºåˆ—ä¸­å­¦ä¹ åˆ°ä¸€ç»„ç¦»æ•£çš„ã€å¯è§£é‡Šçš„é¢éƒ¨è¡¨æƒ…å•å…ƒï¼ˆtokensï¼‰ã€‚é€šè¿‡å°†å¤æ‚çš„é¢éƒ¨è¡¨æƒ…åˆ†è§£ä¸ºè¿™äº›åŸºæœ¬å•å…ƒçš„ç»„åˆï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è¡¨ç¤ºå’Œåˆ†æé¢éƒ¨è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDFEæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨3DMMæå–èº«ä»½æ— å…³çš„è¡¨æƒ…ç‰¹å¾ï¼Œæ¶ˆé™¤å¤´éƒ¨å§¿åŠ¿å’Œé¢éƒ¨å‡ ä½•çš„å½±å“ï¼›2) ä½¿ç”¨RVQ-VAEå¯¹æå–çš„ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆç¦»æ•£çš„tokenåºåˆ—ï¼›3) ä½¿ç”¨å­¦ä¹ åˆ°çš„tokenæ„å»ºè¯è¢‹æ¨¡å‹ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„åˆ†ç±»æˆ–å›å½’ã€‚

**å…³é”®åˆ›æ–°**ï¼šDFEçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨RVQ-VAEå­¦ä¹ é¢éƒ¨è¡¨æƒ…çš„ç¦»æ•£è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„è¿ç»­è¡¨ç¤ºç›¸æ¯”ï¼Œç¦»æ•£è¡¨ç¤ºæ›´æ˜“äºè§£é‡Šï¼Œå¹¶ä¸”å¯ä»¥æ›´å¥½åœ°æ•æ‰é¢éƒ¨è¡¨æƒ…çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒRVQ-VAEé€šè¿‡æ®‹å·®é‡åŒ–ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç æœ¬ç©ºé—´ï¼Œæé«˜è¡¨ç¤ºçš„ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼š3DMMç”¨äºæå–èº«ä»½æ— å…³çš„è¡¨æƒ…ç‰¹å¾ï¼Œç¡®ä¿æ¨¡å‹å…³æ³¨è¡¨æƒ…æœ¬èº«è€Œéä¸ªä½“å·®å¼‚ã€‚RVQ-VAEçš„ç æœ¬å¤§å°å’Œå±‚æ•°æ˜¯é‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æ„æŸå¤±å’Œé‡åŒ–æŸå¤±ï¼Œç”¨äºä¿è¯é‡æ„è´¨é‡å’Œç æœ¬çš„æœ‰æ•ˆæ€§ã€‚è¯è¢‹æ¨¡å‹ä½¿ç”¨TF-IDFåŠ æƒï¼Œçªå‡ºé‡è¦tokençš„ä½œç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDFEåœ¨å‹åŠ›æ£€æµ‹ã€äººæ ¼é¢„æµ‹å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­ï¼Œå‡ä¼˜äºåŸºäºFACSçš„pipelineä»¥åŠMasked Autoencodersç­‰å…ˆè¿›æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŠ‘éƒç—‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒDFEçš„æ€§èƒ½æå‡è¶…è¿‡5%ã€‚è¿™è¡¨æ˜DFEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰ä¸å¿ƒç†çŠ¶æ€ç›¸å…³çš„é¢éƒ¨è¡Œä¸ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¿ƒç†å­¦ã€æƒ…æ„Ÿè®¡ç®—ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºè‡ªåŠ¨è¯Šæ–­å¿ƒç†ç–¾ç—…ã€è¯„ä¼°ç”¨æˆ·çš„æƒ…ç»ªçŠ¶æ€ã€æ”¹å–„è™šæ‹Ÿè§’è‰²çš„è¡¨æƒ…ç”Ÿæˆç­‰ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–éè¯­è¨€è¡Œä¸ºçš„åˆ†æï¼Œä¾‹å¦‚èº«ä½“å§¿åŠ¿å’Œè¯­éŸ³è¯­è°ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Facial expression analysis is central to understanding human behavior, yet existing coding systems such as the Facial Action Coding System (FACS) are constrained by limited coverage and costly manual annotation. In this work, we introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven alternative of compact and interpretable dictionary of facial expressions from 3D mesh sequences learned through a Residual Vector Quantized Variational Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant expression features from images using a 3D Morphable Model (3DMM), effectively disentangling factors such as head pose and facial geometry. We then encode these features using an RVQ-VAE, producing a sequence of discrete tokens from a shared codebook, where each token captures a specific, reusable facial deformation pattern that contributes to the overall expression. Through extensive experiments, we demonstrate that Discrete Facial Encoding captures more precise facial behaviors than FACS and other facial encoding alternatives. We evaluate the utility of our representation across three high-level psychological tasks: stress detection, personality prediction, and depression detection. Using a simple Bag-of-Words model built on top of the learned tokens, our system consistently outperforms both FACS-based pipelines and strong image and video representation learning models such as Masked Autoencoders. Further analysis reveals that our representation covers a wider variety of facial displays, highlighting its potential as a scalable and effective alternative to FACS for psychological and affective computing applications.

