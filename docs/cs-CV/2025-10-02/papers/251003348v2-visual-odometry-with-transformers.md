---
layout: default
title: Visual Odometry with Transformers
---

# Visual Odometry with Transformers

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.03348" target="_blank" class="toolbar-btn">arXiv: 2510.03348v2</a>
    <a href="https://arxiv.org/pdf/2510.03348.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03348v2" 
            onclick="toggleFavorite(this, '2510.03348v2', 'Visual Odometry with Transformers')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Vlardimir Yugay, Duy-Kien Nguyen, Theo Gevers, Cees G. M. Snoek, Martin R. Oswald

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02 (Êõ¥Êñ∞: 2025-11-19)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éTransformerÁöÑËßÜËßâÈáåÁ®ãËÆ°VoTÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÂçïÁõÆ‰ΩçÂßøÂõûÂΩí„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÈáåÁ®ãËÆ°` `Transformer` `‰ΩçÂßøÂõûÂΩí` `Á´ØÂà∞Á´ØÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÂØºËà™`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüËßÜËßâÈáåÁ®ãËÆ°‰æùËµñÁõ∏Êú∫ÂèÇÊï∞ÂíåÊâãÂ∑•ÁªÑ‰ª∂ÔºåÂ¶ÇÊçÜÁªëË∞ÉÊï¥ÂíåÁâπÂæÅÂåπÈÖçÔºåÈÄüÂ∫¶ÊÖ¢‰∏îÈöæ‰ª•Êâ©Â±ï„ÄÇ
2. VoTÂ∞ÜÂçïÁõÆËßÜËßâÈáåÁ®ãËÆ°Âª∫Ê®°‰∏∫Áõ¥Êé•ÁöÑÁõ∏ÂØπ‰ΩçÂßøÂõûÂΩíÈóÆÈ¢òÔºåÊó†ÈúÄÊâãÂ∑•ÁªÑ‰ª∂ÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÊµÅÁ®ã„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVoTÊØî‰º†ÁªüÊñπÊ≥ïÂø´4ÂÄçÔºå‰∏îÊÄßËÉΩÊõ¥‰ºòÔºåÂêåÊó∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÈ´òÊïàÁöÑÊû∂ÊûÑÔºåÂç≥ËßÜËßâÈáåÁ®ãËÆ°Transformer (VoT)ÔºåÂÆÉÂ∞ÜÂçïÁõÆËßÜËßâÈáåÁ®ãËÆ°Âª∫Ê®°‰∏∫‰∏Ä‰∏™Áõ¥Êé•ÁöÑÁõ∏ÂØπ‰ΩçÂßøÂõûÂΩíÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ï‰ª•Á´ØÂà∞Á´ØÁöÑÊñπÂºèÁÆÄÂåñ‰∫ÜÂçïÁõÆËßÜËßâÈáåÁ®ãËÆ°ÊµÅÁ®ãÔºåÊúâÊïàÂú∞Ê∂àÈô§‰∫ÜËØ∏Â¶ÇÊçÜÁªëË∞ÉÊï¥„ÄÅÁâπÂæÅÂåπÈÖçÊàñÁõ∏Êú∫Ê†°ÂáÜÁ≠âÊâãÂ∑•ÁªÑ‰ª∂ÁöÑÈúÄÊ±Ç„ÄÇÂÆûÈ™åË°®ÊòéÔºåVoTÊØî‰º†ÁªüÊñπÊ≥ïÂø´4ÂÄçÔºåÂêåÊó∂ÂÖ∑ÊúâÁ´û‰∫âÊÄßÊàñÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ‰∏éÊúÄËøëÁöÑ3DÂü∫Á°ÄÊ®°ÂûãÁõ∏ÊØîÔºåVoTËøêË°åÈÄüÂ∫¶Âø´10ÂÄçÔºåÂπ∂‰∏îÂú®Ê®°ÂûãÂ§ßÂ∞èÂíåËÆ≠ÁªÉÊï∞ÊçÆÊñπÈù¢ÈÉΩÂÖ∑ÊúâÂº∫Â§ßÁöÑÊâ©Â±ïÊÄß„ÄÇÊ≠§Â§ñÔºåVoTÂú®‰ΩéÊï∞ÊçÆÁéØÂ¢ÉÂíå‰ª•ÂâçÊú™ËßÅËøáÁöÑÂú∫ÊôØ‰∏≠ÈÉΩÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÁº©Â∞è‰∫ÜÂü∫‰∫é‰ºòÂåñÊñπÊ≥ïÂíåÁ´ØÂà∞Á´ØÊñπÊ≥ï‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**Ôºö‰º†ÁªüËßÜËßâÈáåÁ®ãËÆ°ÊñπÊ≥ï‰æùËµñ‰∫éÊâãÂ∑•ËÆæËÆ°ÁöÑÁâπÂæÅÊèêÂèñ„ÄÅÁâπÂæÅÂåπÈÖç‰ª•ÂèäÂ§çÊùÇÁöÑÊçÜÁªëË∞ÉÊï¥Á≠âÊ≠•È™§ÔºåËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÈÄüÂ∫¶ÊÖ¢ÔºåÂπ∂‰∏îÈöæ‰ª•Âà©Áî®Â§ßËßÑÊ®°Êï∞ÊçÆËøõË°åÂ≠¶‰π†ÔºåÊ≥õÂåñËÉΩÂäõÂèóÈôê„ÄÇÊ≠§Â§ñÔºåËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÁ≤æÁ°ÆÁöÑÁõ∏Êú∫ÂèÇÊï∞ÔºåÂØπÁéØÂ¢ÉÂèòÂåñËæÉ‰∏∫ÊïèÊÑü„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËßÜËßâÈáåÁ®ãËÆ°ÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™Áõ¥Êé•ÁöÑÁõ∏ÂØπ‰ΩçÂßøÂõûÂΩíÈóÆÈ¢òÔºåÂà©Áî®TransformerÂº∫Â§ßÁöÑÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõÔºåÁõ¥Êé•‰ªéÂõæÂÉèÂ∫èÂàó‰∏≠Â≠¶‰π†Áõ∏Êú∫‰ΩçÂßøÁöÑÂèòÂåñ„ÄÇÈÄöËøáÁ´ØÂà∞Á´ØÁöÑËÆ≠ÁªÉÊñπÂºèÔºåÈÅøÂÖç‰∫ÜÊâãÂ∑•ÁâπÂæÅËÆæËÆ°ÂíåÂ§çÊùÇÁöÑ‰ºòÂåñËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÈÄüÂ∫¶ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVoTÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂõæÂÉèÁºñÁ†ÅÂô®„ÄÅTransformerÁºñÁ†ÅÂô®Âíå‰ΩçÂßøÂõûÂΩíÂ§¥„ÄÇÂõæÂÉèÁºñÁ†ÅÂô®Ë¥üË¥£ÊèêÂèñÂõæÂÉèÁâπÂæÅÔºåTransformerÁºñÁ†ÅÂô®ÂØπÂõæÂÉèÁâπÂæÅÂ∫èÂàóËøõË°åÂª∫Ê®°ÔºåÂ≠¶‰π†ÂõæÂÉè‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºå‰ΩçÂßøÂõûÂΩíÂ§¥ÂàôÊ†πÊçÆTransformerÁöÑËæìÂá∫È¢ÑÊµãÁõ∏ÂØπ‰ΩçÂßø„ÄÇÊï¥‰∏™ÊµÅÁ®ã‰ª•Á´ØÂà∞Á´ØÁöÑÊñπÂºèËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVoTÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫é‰ΩøÁî®TransformerÁõ¥Êé•ËøõË°åÁõ∏ÂØπ‰ΩçÂßøÂõûÂΩíÔºåÊëíÂºÉ‰∫Ü‰º†ÁªüËßÜËßâÈáåÁ®ãËÆ°‰∏≠ÁöÑÊâãÂ∑•ÁâπÂæÅÂíåÊçÜÁªëË∞ÉÊï¥Á≠âÊ≠•È™§„ÄÇËøôÁßçÊñπÊ≥ïÁÆÄÂåñ‰∫ÜÊµÅÁ®ãÔºåÊèêÈ´ò‰∫ÜÈÄüÂ∫¶ÔºåÂπ∂‰∏îËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®Â§ßËßÑÊ®°Êï∞ÊçÆËøõË°åÂ≠¶‰π†„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVoT‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑResNet‰Ωú‰∏∫ÂõæÂÉèÁºñÁ†ÅÂô®ÔºåTransformerÁºñÁ†ÅÂô®ÈááÁî®Ê†áÂáÜÁöÑTransformerÁªìÊûÑÔºå‰ΩçÂßøÂõûÂΩíÂ§¥Áî±Âá†‰∏™ÂÖ®ËøûÊé•Â±ÇÁªÑÊàê„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®‰ΩçÂßøËØØÂ∑ÆÁöÑL1ÊçüÂ§±ÂíåËßíÂ∫¶ËØØÂ∑ÆÁöÑL1ÊçüÂ§±ÁöÑÂä†ÊùÉÂíå„ÄÇ‰ΩúËÄÖËøòÊé¢Á¥¢‰∫Ü‰∏çÂêåÁöÑTransformerÂ±ÇÊï∞ÂíåÈöêËóèÂ±ÇÂ§ßÂ∞èÔºå‰ª•ÊâæÂà∞ÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÈÖçÁΩÆ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVoTÊØî‰º†ÁªüÊñπÊ≥ïÂø´4ÂÄçÔºåÂπ∂‰∏îÂú®KITTIÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇ‰∏éÂü∫‰∫é3DÂü∫Á°ÄÊ®°ÂûãÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåVoTËøêË°åÈÄüÂ∫¶Âø´10ÂÄçÔºåÂπ∂‰∏îÂú®‰ΩéÊï∞ÊçÆÂíåÊú™ËßÅËøáÁöÑÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåVoTÊòØ‰∏ÄÁßçÈ´òÊïà‰∏îÈÄöÁî®ÁöÑËßÜËßâÈáåÁ®ãËÆ°ÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VoTÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÂÖ∂Á´ØÂà∞Á´ØÁöÑËÆæËÆ°ÂíåÂø´ÈÄüÁöÑÊé®ÁêÜÈÄüÂ∫¶‰ΩøÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁßªÂä®Âπ≥Âè∞‰∏äÂÖ∑ÊúâÂæàÂ§ßÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÊú™Êù•ÔºåVoTÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Â§öÁõÆËßÜËßâÈáåÁ®ãËÆ°ÂíåËßÜËßâSLAMÁ≠âÊõ¥Â§çÊùÇÁöÑÂú∫ÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Despite the rapid development of large 3D models, classical optimization-based approaches dominate the field of visual odometry (VO). Thus, current approaches to VO heavily rely on camera parameters and many handcrafted components, most of which involve complex bundle adjustment and feature-matching processes. Although disregarded in the literature, we find it problematic in terms of both (1) speed, that performs bundle adjustment requires a significant amount of time, and (2) scalability, as hand-crafted components struggle to learn from large-scale training data. In this work, we introduce a simple yet efficient architecture, Visual Odometry Transformer (VoT), that formulates monocular visual odometry as a direct relative pose regression problem. Our approach streamlines the monocular visual odometry pipeline in an end-to-end manner, effectively eliminating the need for handcrafted components such as bundle adjustment, feature matching, or camera calibration. We show that VoT is up to 4 times faster than traditional approaches, yet with competitive or better performance. Compared to recent 3D foundation models, VoT runs 10 times faster with strong scaling behavior in terms of both model sizes and training data. Moreover, VoT generalizes well in both low-data regimes and previously unseen scenarios, reducing the gap between optimization-based and end-to-end approaches.

