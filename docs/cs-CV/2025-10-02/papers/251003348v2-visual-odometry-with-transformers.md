---
layout: default
title: Visual Odometry with Transformers
---

# Visual Odometry with Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03348" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03348v2</a>
  <a href="https://arxiv.org/pdf/2510.03348.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03348v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03348v2', 'Visual Odometry with Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vlardimir Yugay, Duy-Kien Nguyen, Theo Gevers, Cees G. M. Snoek, Martin R. Oswald

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02 (æ›´æ–°: 2025-11-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºTransformerçš„è§†è§‰é‡Œç¨‹è®¡VoTï¼Œå®ç°ç«¯åˆ°ç«¯å•ç›®ä½å§¿å›å½’ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰é‡Œç¨‹è®¡` `Transformer` `ä½å§¿å›å½’` `ç«¯åˆ°ç«¯å­¦ä¹ ` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿè§†è§‰é‡Œç¨‹è®¡ä¾èµ–ç›¸æœºå‚æ•°å’Œæ‰‹å·¥ç»„ä»¶ï¼Œå¦‚æ†ç»‘è°ƒæ•´å’Œç‰¹å¾åŒ¹é…ï¼Œé€Ÿåº¦æ…¢ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. VoTå°†å•ç›®è§†è§‰é‡Œç¨‹è®¡å»ºæ¨¡ä¸ºç›´æ¥çš„ç›¸å¯¹ä½å§¿å›å½’é—®é¢˜ï¼Œæ— éœ€æ‰‹å·¥ç»„ä»¶ï¼Œå®ç°ç«¯åˆ°ç«¯æµç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVoTæ¯”ä¼ ç»Ÿæ–¹æ³•å¿«4å€ï¼Œä¸”æ€§èƒ½æ›´ä¼˜ï¼ŒåŒæ—¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„æ¶æ„ï¼Œå³è§†è§‰é‡Œç¨‹è®¡Transformer (VoT)ï¼Œå®ƒå°†å•ç›®è§†è§‰é‡Œç¨‹è®¡å»ºæ¨¡ä¸ºä¸€ä¸ªç›´æ¥çš„ç›¸å¯¹ä½å§¿å›å½’é—®é¢˜ã€‚è¯¥æ–¹æ³•ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ç®€åŒ–äº†å•ç›®è§†è§‰é‡Œç¨‹è®¡æµç¨‹ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†è¯¸å¦‚æ†ç»‘è°ƒæ•´ã€ç‰¹å¾åŒ¹é…æˆ–ç›¸æœºæ ¡å‡†ç­‰æ‰‹å·¥ç»„ä»¶çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒVoTæ¯”ä¼ ç»Ÿæ–¹æ³•å¿«4å€ï¼ŒåŒæ—¶å…·æœ‰ç«äº‰æ€§æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚ä¸æœ€è¿‘çš„3DåŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼ŒVoTè¿è¡Œé€Ÿåº¦å¿«10å€ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®æ–¹é¢éƒ½å…·æœ‰å¼ºå¤§çš„æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒVoTåœ¨ä½æ•°æ®ç¯å¢ƒå’Œä»¥å‰æœªè§è¿‡çš„åœºæ™¯ä¸­éƒ½å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œç¼©å°äº†åŸºäºä¼˜åŒ–æ–¹æ³•å’Œç«¯åˆ°ç«¯æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿè§†è§‰é‡Œç¨‹è®¡æ–¹æ³•ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„ç‰¹å¾æå–ã€ç‰¹å¾åŒ¹é…ä»¥åŠå¤æ‚çš„æ†ç»‘è°ƒæ•´ç­‰æ­¥éª¤ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œé€Ÿåº¦æ…¢ï¼Œå¹¶ä¸”éš¾ä»¥åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦ç²¾ç¡®çš„ç›¸æœºå‚æ•°ï¼Œå¯¹ç¯å¢ƒå˜åŒ–è¾ƒä¸ºæ•æ„Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰é‡Œç¨‹è®¡é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªç›´æ¥çš„ç›¸å¯¹ä½å§¿å›å½’é—®é¢˜ï¼Œåˆ©ç”¨Transformerå¼ºå¤§çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œç›´æ¥ä»å›¾åƒåºåˆ—ä¸­å­¦ä¹ ç›¸æœºä½å§¿çš„å˜åŒ–ã€‚é€šè¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹å¼ï¼Œé¿å…äº†æ‰‹å·¥ç‰¹å¾è®¾è®¡å’Œå¤æ‚çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œæé«˜é€Ÿåº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVoTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å›¾åƒç¼–ç å™¨ã€Transformerç¼–ç å™¨å’Œä½å§¿å›å½’å¤´ã€‚å›¾åƒç¼–ç å™¨è´Ÿè´£æå–å›¾åƒç‰¹å¾ï¼ŒTransformerç¼–ç å™¨å¯¹å›¾åƒç‰¹å¾åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œå­¦ä¹ å›¾åƒä¹‹é—´çš„å…³ç³»ï¼Œä½å§¿å›å½’å¤´åˆ™æ ¹æ®Transformerçš„è¾“å‡ºé¢„æµ‹ç›¸å¯¹ä½å§¿ã€‚æ•´ä¸ªæµç¨‹ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šVoTæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä½¿ç”¨Transformerç›´æ¥è¿›è¡Œç›¸å¯¹ä½å§¿å›å½’ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿè§†è§‰é‡Œç¨‹è®¡ä¸­çš„æ‰‹å·¥ç‰¹å¾å’Œæ†ç»‘è°ƒæ•´ç­‰æ­¥éª¤ã€‚è¿™ç§æ–¹æ³•ç®€åŒ–äº†æµç¨‹ï¼Œæé«˜äº†é€Ÿåº¦ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®è¿›è¡Œå­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šVoTä½¿ç”¨é¢„è®­ç»ƒçš„ResNetä½œä¸ºå›¾åƒç¼–ç å™¨ï¼ŒTransformerç¼–ç å™¨é‡‡ç”¨æ ‡å‡†çš„Transformerç»“æ„ï¼Œä½å§¿å›å½’å¤´ç”±å‡ ä¸ªå…¨è¿æ¥å±‚ç»„æˆã€‚æŸå¤±å‡½æ•°é‡‡ç”¨ä½å§¿è¯¯å·®çš„L1æŸå¤±å’Œè§’åº¦è¯¯å·®çš„L1æŸå¤±çš„åŠ æƒå’Œã€‚ä½œè€…è¿˜æ¢ç´¢äº†ä¸åŒçš„Transformerå±‚æ•°å’Œéšè—å±‚å¤§å°ï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹é…ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVoTæ¯”ä¼ ç»Ÿæ–¹æ³•å¿«4å€ï¼Œå¹¶ä¸”åœ¨KITTIæ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä¸åŸºäº3DåŸºç¡€æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼ŒVoTè¿è¡Œé€Ÿåº¦å¿«10å€ï¼Œå¹¶ä¸”åœ¨ä½æ•°æ®å’Œæœªè§è¿‡çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVoTæ˜¯ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„è§†è§‰é‡Œç¨‹è®¡æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VoTå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚å…¶ç«¯åˆ°ç«¯çš„è®¾è®¡å’Œå¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ä½¿å…¶åœ¨èµ„æºå—é™çš„ç§»åŠ¨å¹³å°ä¸Šå…·æœ‰å¾ˆå¤§çš„åº”ç”¨æ½œåŠ›ã€‚æœªæ¥ï¼ŒVoTå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šç›®è§†è§‰é‡Œç¨‹è®¡å’Œè§†è§‰SLAMç­‰æ›´å¤æ‚çš„åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the rapid development of large 3D models, classical optimization-based approaches dominate the field of visual odometry (VO). Thus, current approaches to VO heavily rely on camera parameters and many handcrafted components, most of which involve complex bundle adjustment and feature-matching processes. Although disregarded in the literature, we find it problematic in terms of both (1) speed, that performs bundle adjustment requires a significant amount of time, and (2) scalability, as hand-crafted components struggle to learn from large-scale training data. In this work, we introduce a simple yet efficient architecture, Visual Odometry Transformer (VoT), that formulates monocular visual odometry as a direct relative pose regression problem. Our approach streamlines the monocular visual odometry pipeline in an end-to-end manner, effectively eliminating the need for handcrafted components such as bundle adjustment, feature matching, or camera calibration. We show that VoT is up to 4 times faster than traditional approaches, yet with competitive or better performance. Compared to recent 3D foundation models, VoT runs 10 times faster with strong scaling behavior in terms of both model sizes and training data. Moreover, VoT generalizes well in both low-data regimes and previously unseen scenarios, reducing the gap between optimization-based and end-to-end approaches.

