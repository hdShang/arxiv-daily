---
layout: default
title: VaPR -- Vision-language Preference alignment for Reasoning
---

# VaPR -- Vision-language Preference alignment for Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01700" target="_blank" class="toolbar-btn">arXiv: 2510.01700v1</a>
    <a href="https://arxiv.org/pdf/2510.01700.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01700v1" 
            onclick="toggleFavorite(this, '2510.01700v1', 'VaPR -- Vision-language Preference alignment for Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Rohan Wadhawan, Fabrice Y Harel-Canada, Zi-Yi Dou, Suhaila Shakiah, Robinson Piramuthu, Nanyun Peng

**ÂàÜÁ±ª**: cs.AI, cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02

**ÊúüÂàä**: COLM 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VaPRÔºöÈÄöËøáËßÜËßâ-ËØ≠Ë®ÄÂÅèÂ•ΩÂØπÈΩêÊèêÂçáÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ÂÅèÂ•ΩÂØπÈΩê` `Á°¨Ë¥ü‰æãÊåñÊéò` `Êé®ÁêÜËÉΩÂäõ` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂÅèÂ•ΩÂæÆË∞ÉÊñπÊ≥ïÂøΩÁï•‰∫ÜÂêàÊàêÂÅèÂ•ΩÊ†áÊ≥®‰∏≠Â≠òÂú®ÁöÑÂô™Â£∞ÔºåÂ¶ÇÈ£éÊ†ºÂíåÈïøÂ∫¶ÂÅèÂ∑ÆÔºåÂΩ±Âìç‰∫ÜLVLMÁöÑÊÄßËÉΩ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Âü∫‰∫éLLMÂºïÂØºÁöÑÂìçÂ∫îÁºñËæëÁöÑÁ°¨Ë¥ü‰æãÁîüÊàêÊ°ÜÊû∂ÔºåÁîüÊàêÂÖ∑ÊúâÈíàÂØπÊÄßÈîôËØØ‰ΩÜÈ£éÊ†ºÂíåÈïøÂ∫¶Áõ∏‰ººÁöÑÊãíÁªùÂìçÂ∫î„ÄÇ
3. VaPRÊï∞ÊçÆÈõÜÂíåÂæÆË∞ÉÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÂçá‰∫ÜLVLMÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂπ∂Èôç‰Ωé‰∫ÜÂõûÁ≠î‚ÄúÊòØ‚ÄùÁöÑÂÄæÂêë„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÂÅèÂ•ΩÂØπÈΩêÁöÑÊé®ÁêÜÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMÔºâ‰∏≠Â≠òÂú®ÁöÑÂêàÊàêÂÅèÂ•ΩÊ†áÊ≥®Âô™Â£∞ÈóÆÈ¢òÔºåÁâπÂà´ÊòØÈ£éÊ†ºÂíåÈïøÂ∫¶ÂÅèÂ∑Æ„ÄÇ‰∏∫Ê≠§Ôºå‰ΩúËÄÖËÆæËÆ°‰∫Ü‰∏Ä‰∏™Âü∫‰∫éLLMÂºïÂØºÁöÑÂìçÂ∫îÁºñËæëÁöÑÁ°¨Ë¥ü‰æãÂìçÂ∫îÁîüÊàêÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÁîüÊàêÂÖ∑ÊúâÁõÆÊ†áÊÄßÈîôËØØÁöÑÊãíÁªùÂìçÂ∫îÔºåÂêåÊó∂‰øùÊåÅ‰∏éÊé•ÂèóÂìçÂ∫îÂú®È£éÊ†ºÂíåÈïøÂ∫¶‰∏äÁöÑÁõ∏‰ººÊÄß„ÄÇÂà©Áî®ËØ•Ê°ÜÊû∂ÔºåÊûÑÂª∫‰∫ÜÂåÖÂê´3‰∏á‰∏™È´òË¥®ÈáèÊ†∑Êú¨ÁöÑVaPRÊï∞ÊçÆÈõÜÔºåÂπ∂ÂØπLLaVA-V1.5„ÄÅQwen2VLÂíåQwen2.5VLÔºà2B-13BÂ§ßÂ∞èÔºâ‰∏â‰∏™LVLMÂÆ∂ÊóèËøõË°å‰∫ÜÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVaPRÊ®°ÂûãÂú®ÂçÅ‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ≥ÂùáÂ¢ûÁõäÂàÜÂà´‰∏∫6.5%ÔºàLLaVAÔºâ„ÄÅ4.0%ÔºàQwen2VLÔºâÂíå1.5%ÔºàQwen2.5VLÔºâÔºåÂ∞§ÂÖ∂Âú®Êé®ÁêÜ‰ªªÂä°‰∏äË°®Áé∞Á™ÅÂá∫„ÄÇÊ≠§Â§ñÔºåVaPRËøòÈôç‰Ωé‰∫ÜLVLMÔºàÂ¶ÇLLaVAÔºâÂú®‰∫åÂÖÉÈóÆÈ¢ò‰∏≠ÂõûÁ≠î‚ÄúÊòØ‚ÄùÁöÑÂÄæÂêë„ÄÇËØ•Ê°ÜÊû∂ËøòÂèØÊé®ÂπøÂà∞ÂºÄÊ∫êLLM‰Ωú‰∏∫ÁºñËæëÂô®Ôºå‰ΩøÁî®GPT-4oÂêàÊàêÁöÑÊï∞ÊçÆËÆ≠ÁªÉÁöÑÊ®°ÂûãÊÄßËÉΩÊé•Ëøë‰ΩøÁî®GPT-4oËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇÊï∞ÊçÆ„ÄÅÊ®°ÂûãÂíå‰ª£Á†ÅÂ∑≤ÂÖ¨ÂºÄ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMÔºâÂú®ËøõË°åÂÅèÂ•ΩÂæÆË∞ÉÊó∂Ôºå‰æùËµñ‰∫éAIÁîüÊàêÁöÑÂèçÈ¶à„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÂèçÈ¶à‰∏≠Â≠òÂú®Â§ßÈáèÁöÑÂô™Â£∞ÔºåÁâπÂà´ÊòØÈ£éÊ†ºÂíåÈïøÂ∫¶‰∏äÁöÑÂÅèÂ∑Æ„ÄÇËøô‰∫õÂÅèÂ∑Æ‰ºöÂΩ±ÂìçÊ®°ÂûãÂ≠¶‰π†Âà∞ÁúüÊ≠£ÁöÑ‰∫∫Á±ªÂÅèÂ•ΩÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®Êé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇÁé∞ÊúâÊñπÊ≥ïÊ≤°ÊúâÂÖÖÂàÜËÄÉËôëÂà∞Ëøô‰∫õÂô™Â£∞ÔºåÂØºËá¥Ê®°ÂûãÂÆπÊòìÂèóÂà∞Ëøô‰∫õÂÅèÂ∑ÆÁöÑÂΩ±Âìç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁîüÊàêÈ´òË¥®ÈáèÁöÑÁ°¨Ë¥ü‰æãÊù•ÊèêÈ´òÂÅèÂ•ΩÂæÆË∞ÉÁöÑË¥®Èáè„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáLLMÂºïÂØºÁöÑÂìçÂ∫îÁºñËæëÔºåÁîüÊàê‰∏éÊé•ÂèóÂìçÂ∫îÂú®È£éÊ†ºÂíåÈïøÂ∫¶‰∏äÁõ∏‰ººÔºå‰ΩÜÂåÖÂê´ÁâπÂÆöÈîôËØØÁöÑÊãíÁªùÂìçÂ∫î„ÄÇËøôÊ†∑ÂèØ‰ª•Ëø´‰ΩøÊ®°ÂûãÊõ¥Âä†ÂÖ≥Ê≥®ËØ≠‰πâ‰∏äÁöÑÂ∑ÆÂºÇÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖ‰æùËµñÈ£éÊ†ºÂíåÈïøÂ∫¶Á≠âË°®Èù¢ÁâπÂæÅËøõË°åÂà§Êñ≠„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVaPRÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºö1) Á°¨Ë¥ü‰æãÂìçÂ∫îÁîüÊàêÈò∂ÊÆµÔºö‰ΩøÁî®LLMÔºàÂ¶ÇGPT-4oÔºâ‰Ωú‰∏∫ÁºñËæëÂô®ÔºåÂØπÂéüÂßãÂìçÂ∫îËøõË°åÁºñËæëÔºåÂºïÂÖ•ÁâπÂÆöÁöÑÈîôËØØÔºåÁîüÊàêÊãíÁªùÂìçÂ∫î„ÄÇ2) ÂÅèÂ•ΩÂæÆË∞ÉÈò∂ÊÆµÔºö‰ΩøÁî®ÁîüÊàêÁöÑVaPRÊï∞ÊçÆÈõÜÔºåÈááÁî®Direct Preference Optimization (DPO)Á≠âÂÅèÂ•Ω‰ºòÂåñÁÆóÊ≥ïÔºåÂØπLVLMËøõË°åÂæÆË∞É„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊó®Âú®ÂØπÈΩêÊ®°ÂûãÁöÑÂÅèÂ•Ω‰∏é‰∫∫Á±ªÁöÑÂÅèÂ•ΩÔºåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏Ä‰∏™Á°¨Ë¥ü‰æãÂìçÂ∫îÁîüÊàêÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÁîüÊàêÂÖ∑ÊúâÈíàÂØπÊÄßÈîôËØØÁöÑÊãíÁªùÂìçÂ∫îÔºåÂêåÊó∂‰øùÊåÅ‰∏éÊé•ÂèóÂìçÂ∫îÂú®È£éÊ†ºÂíåÈïøÂ∫¶‰∏äÁöÑÁõ∏‰ººÊÄß„ÄÇËøôÁßçÊñπÊ≥ïÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫ÜÁé∞ÊúâÂÅèÂ•ΩÂæÆË∞ÉÊñπÊ≥ï‰∏≠Â≠òÂú®ÁöÑÂêàÊàêÂÅèÂ•ΩÊ†áÊ≥®Âô™Â£∞ÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ÂÖ∑ÊúâÈÄöÁî®ÊÄßÔºåÂèØ‰ª•Êé®ÂπøÂà∞‰∏çÂêåÁöÑLLM‰Ωú‰∏∫ÁºñËæëÂô®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Á°¨Ë¥ü‰æãÂìçÂ∫îÁîüÊàêÈò∂ÊÆµÔºå‰ΩøÁî®LLM‰Ωú‰∏∫ÁºñËæëÂô®ÔºåÈÄöËøáÁâπÂÆöÁöÑpromptÂºïÂØºLLMÂºïÂÖ•‰∏çÂêåÁ±ªÂûãÁöÑÈîôËØØÔºå‰æãÂ¶Ç‰∫ãÂÆûÈîôËØØ„ÄÅÈÄªËæëÈîôËØØÁ≠â„ÄÇÂêåÊó∂ÔºåÈÄöËøáÊéßÂà∂LLMÁöÑÁîüÊàêËøáÁ®ãÔºå‰øùËØÅÊãíÁªùÂìçÂ∫î‰∏éÊé•ÂèóÂìçÂ∫îÂú®È£éÊ†ºÂíåÈïøÂ∫¶‰∏ä‰øùÊåÅÁõ∏‰ºº„ÄÇÂú®ÂÅèÂ•ΩÂæÆË∞ÉÈò∂ÊÆµÔºåÈááÁî®DPOÁÆóÊ≥ïÔºåÂπ∂Ë∞ÉÊï¥‰∫ÜDPOÁöÑË∂ÖÂèÇÊï∞Ôºå‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VaPRÊ®°ÂûãÂú®ÂçÅ‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåLLaVA„ÄÅQwen2VLÂíåQwen2.5VLÁöÑÂπ≥ÂùáÂ¢ûÁõäÂàÜÂà´‰∏∫6.5%„ÄÅ4.0%Âíå1.5%„ÄÇÂ∞§ÂÖ∂Âú®Êé®ÁêÜ‰ªªÂä°‰∏äË°®Áé∞Á™ÅÂá∫„ÄÇÊ≠§Â§ñÔºåVaPRËøòÈôç‰Ωé‰∫ÜLVLMÔºàÂ¶ÇLLaVAÔºâÂú®‰∫åÂÖÉÈóÆÈ¢ò‰∏≠ÂõûÁ≠î‚ÄúÊòØ‚ÄùÁöÑÂÄæÂêë„ÄÇ‰ΩøÁî®ÂºÄÊ∫êLLM‰Ωú‰∏∫ÁºñËæëÂô®ËÆ≠ÁªÉÁöÑÊ®°ÂûãÊÄßËÉΩÊé•Ëøë‰ΩøÁî®GPT-4oËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºåË°®ÊòéËØ•Ê°ÜÊû∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VaPRÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËßÜËßâÂíåËØ≠Ë®ÄÁêÜËß£ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÈóÆÁ≠î„ÄÅËßÜËßâÊé®ÁêÜ„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠â„ÄÇÈÄöËøáÊèêÈ´òLVLMÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•ÊèêÂçáËøô‰∫õÂ∫îÁî®ÁöÑÁî®Êà∑‰ΩìÈ™åÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂ÊèêÂá∫ÁöÑÁ°¨Ë¥ü‰æãÁîüÊàêÊ°ÜÊû∂‰πüÂèØ‰ª•Êé®ÂπøÂà∞ÂÖ∂‰ªñÊ®°ÊÄÅÔºå‰æãÂ¶ÇÈü≥È¢ëÂíåÊñáÊú¨Ôºå‰ªéËÄåÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Preference finetuning methods like Direct Preference Optimization (DPO) with AI-generated feedback have shown promise in aligning Large Vision-Language Models (LVLMs) with human preferences. However, existing techniques overlook the prevalence of noise in synthetic preference annotations in the form of stylistic and length biases. To this end, we introduce a hard-negative response generation framework based on LLM-guided response editing, that produces rejected responses with targeted errors, maintaining stylistic and length similarity to the accepted ones. Using this framework, we develop the VaPR dataset, comprising 30K high-quality samples, to finetune three LVLM families: LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver significant performance improvements across ten benchmarks, achieving average gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable improvements on reasoning tasks. A scaling analysis shows that performance consistently improves with data size, with LLaVA models benefiting even at smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we show that the framework generalizes to open-source LLMs as editors, with models trained on VaPR-OS achieving ~99% of the performance of models trained on \name, which is synthesized using GPT-4o. Our data, models, and code can be found on the project page https://vap-r.github.io

