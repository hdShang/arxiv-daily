---
layout: default
title: "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification"
---

# microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02270" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.02270v1</a>
  <a href="https://arxiv.org/pdf/2510.02270.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02270v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02270v1', 'microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/sathiiii/microCLIP)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**microCLIPÔºöÈÄöËøáÁ≤óÁªÜÁ≤íÂ∫¶TokenËûçÂêàÂÆûÁé∞Êó†ÁõëÁù£CLIPÂæÆË∞ÉÔºåÊèêÂçáÁªÜÁ≤íÂ∫¶ÂõæÂÉèÂàÜÁ±ªÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁªÜÁ≤íÂ∫¶ÂõæÂÉèÂàÜÁ±ª` `CLIP` `Êó†ÁõëÁù£ÂæÆË∞É` `TokenËûçÂêà` `Ëá™ËÆ≠ÁªÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. CLIPÂú®ÁªÜÁ≤íÂ∫¶ÂõæÂÉèÂàÜÁ±ª‰∏≠‰æùËµñÁ≤óÁ≥ôÁöÑÂÖ®Â±ÄÁâπÂæÅÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊÄßËÉΩÔºåÁé∞ÊúâÊñπÊ≥ïÂøΩÁï•‰∫ÜÁ©∫Èó¥Á≤æÂ∫¶„ÄÇ
2. microCLIPÈÄöËøáÊòæËëóÊÄßÂØºÂêëÁöÑTokenFusionÊ®°ÂùóÔºåËûçÂêàÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÁâπÂæÅÔºåÂπ∂Âà©Áî®Ëá™ËÆ≠ÁªÉÊ°ÜÊû∂‰ºòÂåñCLIPÁöÑËßÜËßâÂíåÊñáÊú¨Ë°®Á§∫„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåmicroCLIPÂú®13‰∏™ÁªÜÁ≤íÂ∫¶Âü∫ÂáÜÊµãËØï‰∏≠Âπ≥ÂùáÁ≤æÂ∫¶ÊèêÂçá2.90%ÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫microCLIPÔºå‰∏Ä‰∏™Ëá™ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁî®‰∫éÂà©Áî®ÁªÜÁ≤íÂ∫¶Á∫øÁ¥¢ËÅîÂêà‰ºòÂåñCLIPÁöÑËßÜËßâÂíåÊñáÊú¨Ë°®Á§∫Ôºå‰ª•ÂÆûÁé∞Âü∫‰∫éCLIPÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)ÁöÑÊó†ÁõëÁù£ÂæÆË∞ÉÔºåÁî®‰∫éÁªÜÁ≤íÂ∫¶ÂõæÂÉèÂàÜÁ±ª„ÄÇËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊòØËΩªÈáèÁ∫ßÁöÑTokenFusionÊ®°Âùó‰∏≠ÁöÑÊòæËëóÊÄßÂØºÂêëÊ≥®ÊÑèÂäõÊ±†Âåñ(SOAP)ÔºåÂÆÉ‰ªépatchÂµåÂÖ•ÊûÑÂª∫‰∏Ä‰∏™ÊòæËëóÊÄßÂºïÂØºÁöÑ[FG]tokenÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÂÖ®Â±Ä[CLS]tokenËûçÂêàÔºå‰ª•ÂÆûÁé∞Á≤óÁªÜÁ≤íÂ∫¶ÁöÑÂØπÈΩê„ÄÇ‰∏∫‰∫ÜÁ®≥ÂÆöÂæÆË∞ÉÔºåÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂèåÂ§¥LLMÊ¥æÁîüÁöÑÂàÜÁ±ªÂô®Ôºö‰∏Ä‰∏™ÂÜªÁªìÁöÑÂàÜÁ±ªÂô®ÔºåÈÄöËøáÂ§öËßÜËßíÂØπÈΩêÔºå‰∏∫‰º™Ê†áÁ≠æÊèê‰æõÁ®≥ÂÆöÁöÑÂü∫‰∫éÊñáÊú¨ÁöÑÂÖàÈ™åÔºõ‰ª•Âèä‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÂàÜÁ±ªÂô®Ôºå‰ªéLLMÊèèËø∞ÂàùÂßãÂåñÔºåÂπ∂‰ΩøÁî®TokenFusionËøõË°åÂæÆË∞É„ÄÇÊ≠§Â§ñÔºåËøòÂºÄÂèë‰∫ÜÂä®ÊÄÅÁü•ËØÜËÅöÂêàÔºåÂÆÉÂá∏ÁªÑÂêàÂõ∫ÂÆöÁöÑLLM/CLIPÂÖàÈ™å‰∏éTokenFusion‰∏çÊñ≠ÊºîÂèòÁöÑlogitsÔºå‰ª•Ëø≠‰ª£Âú∞‰ºòÂåñ‰º™Ê†áÁ≠æ„ÄÇËøô‰∫õÁªÑ‰ª∂ÂÖ±ÂêåÊè≠Á§∫‰∫ÜCLIP‰∏≠ÊΩúÂú®ÁöÑÁªÜÁ≤íÂ∫¶‰ø°Âè∑ÔºåÂú®13‰∏™ÁªÜÁ≤íÂ∫¶Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá2.90%ÁöÑÁ≤æÂ∫¶ÊèêÂçáÔºåÂêåÊó∂Âè™ÈúÄË¶ÅËΩªÈáèÁ∫ßÁöÑÂæÆË∞É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁªÜÁ≤íÂ∫¶ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°ÈúÄË¶ÅÊ®°ÂûãÂØπÂõæÂÉè‰∏≠ÂæÆÂ∞èÁöÑÂ±ÄÈÉ®Á∫øÁ¥¢ÂÖ∑ÊúâÈ´òÂ∫¶ÁöÑÊïèÊÑüÊÄß„ÄÇCLIPËôΩÁÑ∂ÂÖ∑ÊúâÂº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßªËÉΩÂäõÔºå‰ΩÜÂÖ∂‰æùËµñ‰∫éÁ≤óÁ≥ôÁöÑÂÖ®Â±ÄÁâπÂæÅÔºåÂØºËá¥ÂÖ∂Âú®ÁªÜÁ≤íÂ∫¶ÂàÜÁ±ª‰ªªÂä°‰∏äÁöÑË°®Áé∞ÂèóÈôê„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ∞ùËØïÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)ÁöÑÊèèËø∞‰∏éCLIPÁöÑ[CLS]tokenÂØπÈΩêÔºå‰ª•Ê≥®ÂÖ•ÁªÜÁ≤íÂ∫¶Áü•ËØÜÔºå‰ΩÜÂøΩÁï•‰∫ÜÁ©∫Èó¥Á≤æÂ∫¶ÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®ÂõæÂÉèÁöÑÂ±ÄÈÉ®‰ø°ÊÅØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºömicroCLIPÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËûçÂêàCLIPÁöÑÂÖ®Â±ÄÁâπÂæÅÂíå‰ªéÂõæÂÉèÂ±ÄÈÉ®Âå∫ÂüüÊèêÂèñÁöÑÁªÜÁ≤íÂ∫¶ÁâπÂæÅÔºåÊù•ÊèêÂçáÊ®°ÂûãÂØπÁªÜÁ≤íÂ∫¶‰ø°ÊÅØÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ÊòæËëóÊÄßÂØºÂêëÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ªéÂõæÂÉèpatchÂµåÂÖ•‰∏≠ÊèêÂèñÈáçË¶ÅÁöÑÂ±ÄÈÉ®ÁâπÂæÅÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÂÖ®Â±ÄÁâπÂæÅËûçÂêàÔºå‰ªéËÄåÂÆûÁé∞Á≤óÁªÜÁ≤íÂ∫¶ÁöÑÂØπÈΩê„ÄÇÊ≠§Â§ñÔºåÈÄöËøáËá™ËÆ≠ÁªÉÁöÑÊñπÂºèÔºå‰∏çÊñ≠‰ºòÂåñCLIPÁöÑËßÜËßâÂíåÊñáÊú¨Ë°®Á§∫ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºömicroCLIPÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) TokenFusionÊ®°ÂùóÔºöËØ•Ê®°ÂùóÂà©Áî®ÊòæËëóÊÄßÂØºÂêëÊ≥®ÊÑèÂäõÊ±†Âåñ(SOAP)‰ªéÂõæÂÉèpatchÂµåÂÖ•‰∏≠ÊèêÂèñÊòæËëóÊÄßÂå∫ÂüüÁöÑÁâπÂæÅÔºåÂπ∂ÁîüÊàê‰∏Ä‰∏™[FG]tokenÔºåÁÑ∂ÂêéÂ∞Ü[FG]token‰∏éCLIPÁöÑ[CLS]tokenËûçÂêàÔºåÂæóÂà∞ËûçÂêàÂêéÁöÑÁâπÂæÅË°®Á§∫„ÄÇ2) ÂèåÂ§¥LLMÊ¥æÁîüÂàÜÁ±ªÂô®ÔºöËØ•ÂàÜÁ±ªÂô®ÂåÖÂê´‰∏Ä‰∏™ÂÜªÁªìÁöÑÂàÜÁ±ªÂô®Âíå‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÂàÜÁ±ªÂô®„ÄÇÂÜªÁªìÁöÑÂàÜÁ±ªÂô®Âà©Áî®Â§öËßÜËßíÂØπÈΩêÊèê‰æõÁ®≥ÂÆöÁöÑÊñáÊú¨ÂÖàÈ™åÔºåÁî®‰∫é‰º™Ê†áÁ≠æÁîüÊàêÔºõÂèØÂ≠¶‰π†ÁöÑÂàÜÁ±ªÂô®‰ªéLLMÊèèËø∞ÂàùÂßãÂåñÔºåÂπ∂‰ΩøÁî®TokenFusionÁöÑËæìÂá∫ËøõË°åÂæÆË∞É„ÄÇ3) Âä®ÊÄÅÁü•ËØÜËÅöÂêàÔºöËØ•Ê®°ÂùóÂ∞ÜÂõ∫ÂÆöÁöÑLLM/CLIPÂÖàÈ™å‰∏éTokenFusionÁöÑËæìÂá∫ËøõË°åÂá∏ÁªÑÂêàÔºå‰ª•Ëø≠‰ª£Âú∞‰ºòÂåñ‰º™Ê†áÁ≠æ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºömicroCLIPÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö1) ÊèêÂá∫‰∫ÜSaliency-Oriented Attention Pooling (SOAP)Êú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàÂú∞‰ªéÂõæÂÉèpatchÂµåÂÖ•‰∏≠ÊèêÂèñÊòæËëóÊÄßÂå∫ÂüüÁöÑÁâπÂæÅ„ÄÇ2) ËÆæËÆ°‰∫ÜTokenFusionÊ®°ÂùóÔºåÂ∞ÜÂÖ®Â±ÄÁâπÂæÅÂíåÂ±ÄÈÉ®ÁâπÂæÅËøõË°åËûçÂêàÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÂØπÁªÜÁ≤íÂ∫¶‰ø°ÊÅØÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ3) ÂºïÂÖ•‰∫ÜÂèåÂ§¥LLMÊ¥æÁîüÂàÜÁ±ªÂô®ÂíåÂä®ÊÄÅÁü•ËØÜËÅöÂêàÊú∫Âà∂ÔºåËÉΩÂ§üÁ®≥ÂÆöËá™ËÆ≠ÁªÉËøáÁ®ãÔºåÂπ∂ÊèêÂçá‰º™Ê†áÁ≠æÁöÑË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöTokenFusionÊ®°Âùó‰∏≠ÁöÑSOAPÊú∫Âà∂Âà©Áî®Ê≥®ÊÑèÂäõÊùÉÈáçÊù•ÈÄâÊã©ÈáçË¶ÅÁöÑpatchÂµåÂÖ•ÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨Âä†ÊùÉÂπ≥ÂùáÔºå‰ªéËÄåÁîüÊàê[FG]token„ÄÇÂèåÂ§¥LLMÊ¥æÁîüÂàÜÁ±ªÂô®‰∏≠ÁöÑÂÜªÁªìÂàÜÁ±ªÂô®‰ΩøÁî®CLIPÁöÑÊñáÊú¨ÁºñÁ†ÅÂô®ÂØπLLMÁîüÊàêÁöÑÊñáÊú¨ÊèèËø∞ËøõË°åÁºñÁ†ÅÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫ÊñáÊú¨ÂÖàÈ™å„ÄÇÂèØÂ≠¶‰π†ÁöÑÂàÜÁ±ªÂô®‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ËøõË°åËÆ≠ÁªÉÔºåÁõÆÊ†áÊòØÊúÄÂ∞èÂåñÈ¢ÑÊµãÊ†áÁ≠æ‰∏é‰º™Ê†áÁ≠æ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÂä®ÊÄÅÁü•ËØÜËÅöÂêà‰ΩøÁî®‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÂèÇÊï∞Êù•ÊéßÂà∂LLM/CLIPÂÖàÈ™åÂíåTokenFusionËæìÂá∫‰πãÈó¥ÁöÑÊùÉÈáç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

microCLIPÂú®13‰∏™ÁªÜÁ≤íÂ∫¶ÂõæÂÉèÂàÜÁ±ªÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ≥ÂùáÁ≤æÂ∫¶ÊèêÂçá‰∫Ü2.90%„ÄÇ‰æãÂ¶ÇÔºåÂú®CUB-200-2011Êï∞ÊçÆÈõÜ‰∏äÔºåmicroCLIPÁöÑÁ≤æÂ∫¶ËææÂà∞‰∫Ü87.5%ÔºåË∂ÖËøá‰∫ÜÁé∞ÊúâÁöÑÊó†ÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåmicroCLIPËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®ÁªÜÁ≤íÂ∫¶‰ø°ÊÅØÔºåÊèêÂçáÊ®°ÂûãÁöÑÂàÜÁ±ªÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

microCLIPÂú®ÁªÜÁ≤íÂ∫¶ÂõæÂÉèÂàÜÁ±ªÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÈ∏üÁ±ªËØÜÂà´„ÄÅÊ§çÁâ©ÂàÜÁ±ª„ÄÅËΩ¶ÂûãËØÜÂà´Á≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•Â∫îÁî®‰∫éÊô∫ËÉΩÂÆâÈò≤„ÄÅËá™Âä®È©æÈ©∂„ÄÅÁîüÁâ©Â§öÊ†∑ÊÄß‰øùÊä§Á≠âÈ¢ÜÂüüÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄº„ÄÇÊú™Êù•ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Á†îÁ©∂Â¶Ç‰ΩïÂ∞ÜmicroCLIPÂ∫îÁî®‰∫éÂÖ∂‰ªñËßÜËßâ‰ªªÂä°Ôºå‰æãÂ¶ÇÁõÆÊ†áÊ£ÄÊµã„ÄÅÂõæÂÉèÂàÜÂâ≤Á≠âÔºåÂπ∂Êé¢Á¥¢Êõ¥ÊúâÊïàÁöÑÁâπÂæÅËûçÂêàÂíåÁü•ËØÜËøÅÁßªÊñπÊ≥ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training framework that jointly refines CLIP's visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at https://github.com/sathiiii/microCLIP.

