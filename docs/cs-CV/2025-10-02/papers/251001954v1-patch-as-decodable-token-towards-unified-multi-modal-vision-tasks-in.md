---
layout: default
title: Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs
---

# Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01954" target="_blank" class="toolbar-btn">arXiv: 2510.01954v1</a>
    <a href="https://arxiv.org/pdf/2510.01954.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01954v1" 
            onclick="toggleFavorite(this, '2510.01954v1', 'Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yongyi Su, Haojie Zhang, Shijie Li, Nanqing Liu, Jingyi Liao, Junyi Pan, Yuan Liu, Xiaofen Xing, Chong Sun, Chen Li, Nancy F. Chen, Shuicheng Yan, Xulei Yang, Xun Xu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02

**Â§áÊ≥®**: 24 pages, 12 figures and 9 tables

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Gorilla-Lab-SCUT/PaDT)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Patch-as-Decodable-Token (PaDT)ÔºåÂÆûÁé∞MLLM‰∏≠Áªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅËßÜËßâ‰ªªÂä°Â§ÑÁêÜ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËßÜËßâ‰ªªÂä°` `ÁõÆÊ†áÊ£ÄÊµã` `ÂõæÂÉèÂàÜÂâ≤` `ËßÜËßâÂèÇËÄÉ‰ª§Áâå` `PatchÂµåÂÖ•`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMËßÜËßâ‰ªªÂä°‰æùËµñÈó¥Êé•Ë°®Á§∫ÔºåÂ¶ÇÂùêÊ†áÊñáÊú¨ÔºåÈôêÂà∂‰∫ÜÊÄßËÉΩÂíåÂØÜÈõÜÈ¢ÑÊµãËÉΩÂäõ„ÄÇ
2. PaDTÈÄöËøáËßÜËßâÂèÇËÄÉ‰ª§ÁâåÔºàVRTÔºâÁõ¥Êé•ÁîüÊàêÊñáÊú¨ÂíåËßÜËßâËæìÂá∫ÔºåÂÆûÁé∞Áªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÂ§ÑÁêÜ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåPaDTÂú®Â§ö‰∏™ËßÜËßâ‰ªªÂä°‰∏äËææÂà∞SOTAÔºå‰ºò‰∫éÊõ¥Â§ßÁöÑMLLMÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâËøëÂπ¥Êù•ÂèëÂ±ïËøÖÈÄü„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑËßÜËßâ‰ªªÂä°ÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÈó¥Êé•Ë°®Á§∫Ôºå‰æãÂ¶ÇÁîüÊàêÂùêÊ†á‰Ωú‰∏∫ÊñáÊú¨ËøõË°åÊ£ÄÊµãÔºåËøôÈôêÂà∂‰∫ÜÊÄßËÉΩÂπ∂ÈòªÁ¢ç‰∫ÜËØ∏Â¶ÇÂàÜÂâ≤‰πãÁ±ªÁöÑÂØÜÈõÜÈ¢ÑÊµã‰ªªÂä°„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜPatch-as-Decodable TokenÔºàPaDTÔºâÔºåËøôÊòØ‰∏ÄÁßçÁªü‰∏ÄÁöÑËåÉ‰æãÔºå‰ΩøMLLMËÉΩÂ§üÁõ¥Êé•ÁîüÊàêÊñáÊú¨ÂíåÂêÑÁßçËßÜËßâËæìÂá∫„ÄÇPaDTÁöÑÊ†∏ÂøÉÊòØËßÜËßâÂèÇËÄÉ‰ª§ÁâåÔºàVRTÔºâÔºåÂÆÉÊ∫êËá™Êü•ËØ¢ÂõæÂÉèÁöÑËßÜËßâpatchÂµåÂÖ•ÔºåÂπ∂‰∏éLLMÁöÑËæìÂá∫ÊñáÊú¨‰ª§ÁâåÊó†Áºù‰∫§Áªá„ÄÇÁÑ∂ÂêéÔºå‰∏Ä‰∏™ËΩªÈáèÁ∫ßËß£Á†ÅÂô®Â∞ÜLLMÁöÑËæìÂá∫ËΩ¨Êç¢‰∏∫Ê£ÄÊµã„ÄÅÂàÜÂâ≤ÂíågroundingÈ¢ÑÊµã„ÄÇ‰∏éÂÖàÂâçÁöÑÊñπÊ≥ï‰∏çÂêåÔºåPaDTÂú®ÊØèÊ¨°ÂâçÂêë‰º†ÈÄí‰∏≠Áã¨Á´ãÂ§ÑÁêÜVRTÔºåÂπ∂Âä®ÊÄÅÊâ©Â±ïÂµåÂÖ•Ë°®Ôºå‰ªéËÄåÊèêÈ´òÂÆö‰ΩçÂíåÂå∫ÂàÜÁõ∏‰ººÂØπË±°ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÈÄöËøáÈöèÊú∫ÈÄâÊã©VRTËøõË°åÁõëÁù£ÂæÆË∞ÉÔºåÂπ∂ÂºïÂÖ•Âº∫Â§ßÁöÑper-token‰∫§ÂèâÁÜµÊçüÂ§±Ôºå‰∏∫PaDTÈáèË∫´ÂÆöÂà∂‰∫ÜËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÊàë‰ª¨Âú®Âõõ‰∏™ËßÜËßâÊÑüÁü•ÂíåÁêÜËß£‰ªªÂä°‰∏≠ÁöÑÂÆûËØÅÁ†îÁ©∂Ë°®ÊòéÔºåPaDTÂßãÁªàÂ¶Ç‰∏ÄÂú∞ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÁîöËá≥‰∏éÊõ¥Â§ßÁöÑMLLMÊ®°ÂûãÁõ∏ÊØî‰πüÊòØÂ¶ÇÊ≠§„ÄÇ‰ª£Á†ÅÂèØÂú®https://github.com/Gorilla-Lab-SCUT/PaDTËé∑Âæó„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâMLLMÂú®Â§ÑÁêÜËßÜËßâ‰ªªÂä°Êó∂ÔºåÈÄöÂ∏∏Â∞ÜËßÜËßâ‰ø°ÊÅØËΩ¨Êç¢‰∏∫ÊñáÊú¨ÊèèËø∞ÔºàÂ¶ÇÂùêÊ†áÔºâÔºåÂÜçÁî±LLMÂ§ÑÁêÜ„ÄÇËøôÁßçÈó¥Êé•Ë°®Á§∫ÊñπÂºèÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®ÂØÜÈõÜÈ¢ÑÊµã‰ªªÂä°ÔºàÂ¶ÇÂõæÂÉèÂàÜÂâ≤Ôºâ‰∏≠ÁöÑË°®Áé∞ÔºåÂπ∂‰∏îÂèØËÉΩÂØºËá¥ÂÆö‰ΩçÁ≤æÂ∫¶‰∏ãÈôç„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂå∫ÂàÜÁõ∏‰ººÁâ©‰ΩìÔºåÂΩ±ÂìçÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPaDTÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÂõæÂÉèpatchÁõ¥Êé•ÁºñÁ†Å‰∏∫ÂèØËß£Á†ÅÁöÑ‰ª§ÁâåÔºàVisual Reference Tokens, VRTsÔºâÔºå‰∏éLLMÁöÑÊñáÊú¨‰ª§ÁâåÊ∑∑ÂêàÔºå‰ΩøLLMËÉΩÂ§üÁõ¥Êé•Â§ÑÁêÜËßÜËßâ‰ø°ÊÅØÂπ∂ÁîüÊàêËßÜËßâËæìÂá∫„ÄÇÈÄöËøáÂä®ÊÄÅÊâ©Â±ïVRTÂµåÂÖ•Ë°®ÔºåÂ¢ûÂº∫Ê®°ÂûãÂå∫ÂàÜÁõ∏‰ººÂØπË±°ÁöÑËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPaDTÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) Â∞ÜËæìÂÖ•ÂõæÂÉèÂàÜÂâ≤ÊàêpatchÔºåÂπ∂ÊèêÂèñpatchÁöÑËßÜËßâÂµåÂÖ•Ôºõ2) Â∞ÜËøô‰∫õËßÜËßâÂµåÂÖ•‰Ωú‰∏∫VRTsÔºå‰∏éLLMÁöÑÊñáÊú¨ËæìÂÖ•‰∫§ÁªáÂú®‰∏ÄËµ∑Ôºõ3) LLMÂ§ÑÁêÜÊ∑∑ÂêàÁöÑÊñáÊú¨ÂíåËßÜËßâ‰ª§ÁâåÔºåÁîüÊàêËæìÂá∫Ôºõ4) ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑËß£Á†ÅÂô®Â∞ÜLLMÁöÑËæìÂá∫ËΩ¨Êç¢‰∏∫ÂÖ∑‰ΩìÁöÑËßÜËßâÈ¢ÑÊµãÔºåÂ¶ÇÊ£ÄÊµãÊ°Ü„ÄÅÂàÜÂâ≤Êé©Á†ÅÁ≠â„ÄÇÊï¥‰∏™ËøáÁ®ãÊó†ÈúÄÂ∞ÜËßÜËßâ‰ø°ÊÅØËΩ¨Êç¢‰∏∫ÊñáÊú¨ÔºåÂÆûÁé∞‰∫ÜÁ´ØÂà∞Á´ØÁöÑËßÜËßâ‰ªªÂä°Â§ÑÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPaDTÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÂºïÂÖ•‰∫ÜVRTsÔºå‰ΩøLLMËÉΩÂ§üÁõ¥Êé•Â§ÑÁêÜËßÜËßâ‰ø°ÊÅØÔºåÈÅøÂÖç‰∫Ü‰ø°ÊÅØÊçüÂ§±ÂíåÊÄßËÉΩÁì∂È¢àÔºõ2) Âä®ÊÄÅÊâ©Â±ïVRTÂµåÂÖ•Ë°®ÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂå∫ÂàÜÁõ∏‰ººÂØπË±°ÁöÑËÉΩÂäõÔºõ3) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÈíàÂØπPaDTÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂåÖÊã¨ÈöèÊú∫ÈÄâÊã©VRTËøõË°åÁõëÁù£ÂæÆË∞ÉÂíåÂºïÂÖ•per-token‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPaDTÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) VRTÁöÑÁîüÊàêÊñπÂºèÔºö‰ΩøÁî®ËßÜËßâÁºñÁ†ÅÂô®ÔºàÂ¶ÇViTÔºâÊèêÂèñÂõæÂÉèpatchÁöÑÂµåÂÖ•‰Ωú‰∏∫VRTÔºõ2) VRT‰∏éÊñáÊú¨‰ª§ÁâåÁöÑ‰∫§ÁªáÊñπÂºèÔºöÂ∞ÜVRTÊèíÂÖ•Âà∞ÊñáÊú¨Â∫èÂàó‰∏≠Ôºå‰ΩøLLMËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÊñáÊú¨ÂíåËßÜËßâ‰ø°ÊÅØÔºõ3) Ëß£Á†ÅÂô®ÁöÑËÆæËÆ°Ôºö‰ΩøÁî®ËΩªÈáèÁ∫ßÁöÑËß£Á†ÅÂô®Â∞ÜLLMÁöÑËæìÂá∫ËΩ¨Êç¢‰∏∫ÂÖ∑‰ΩìÁöÑËßÜËßâÈ¢ÑÊµãÔºåÂ¶ÇÊ£ÄÊµãÊ°Ü„ÄÅÂàÜÂâ≤Êé©Á†ÅÁ≠âÔºõ4) ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°Ôºö‰ΩøÁî®per-token‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÂØπÊØè‰∏™‰ª§ÁâåÁöÑÈ¢ÑÊµãËøõË°åÁõëÁù£ÔºåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

PaDTÂú®Âõõ‰∏™ËßÜËßâÊÑüÁü•ÂíåÁêÜËß£‰ªªÂä°‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂåÖÊã¨ÁõÆÊ†áÊ£ÄÊµã„ÄÅÂõæÂÉèÂàÜÂâ≤ÂíåËßÜËßâgrounding„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPaDTËÉΩÂ§üË∂ÖË∂äÊõ¥Â§ßÁöÑMLLMÊ®°ÂûãÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∏™ÂõæÂÉèÂàÜÂâ≤‰ªªÂä°‰∏äÔºåPaDTÁöÑÊÄßËÉΩÊØîÁé∞ÊúâÊúÄ‰Ω≥Ê®°ÂûãÊèêÂçá‰∫ÜX%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PaDTÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂåÖÊã¨Êô∫ËÉΩÂÆâÈò≤„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê„ÄÅÊú∫Âô®‰∫∫ËßÜËßâÁ≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÁõÆÊ†áÊ£ÄÊµã„ÄÅÂõæÂÉèÂàÜÂâ≤„ÄÅËßÜËßâÂÖ≥Á≥ªÊé®ÁêÜÁ≠â‰ªªÂä°ÔºåÊèêÂçáÁõ∏ÂÖ≥Â∫îÁî®ÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇÊú™Êù•ÔºåPaDTÊúâÊúõÊàê‰∏∫Â§öÊ®°ÊÄÅËßÜËßâ‰ªªÂä°Â§ÑÁêÜÁöÑÈáçË¶ÅÂü∫Áü≥ÔºåÊé®Âä®‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

