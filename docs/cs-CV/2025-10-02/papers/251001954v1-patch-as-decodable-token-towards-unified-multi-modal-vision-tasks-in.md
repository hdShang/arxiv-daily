---
layout: default
title: "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs"
---

# Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01954" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01954v1</a>
  <a href="https://arxiv.org/pdf/2510.01954.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01954v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01954v1', 'Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongyi Su, Haojie Zhang, Shijie Li, Nanqing Liu, Jingyi Liao, Junyi Pan, Yuan Liu, Xiaofen Xing, Chong Sun, Chen Li, Nancy F. Chen, Shuicheng Yan, Xulei Yang, Xun Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: 24 pages, 12 figures and 9 tables

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Gorilla-Lab-SCUT/PaDT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPatch-as-Decodable-Token (PaDT)ï¼Œå®ç°MLLMä¸­ç»Ÿä¸€çš„å¤šæ¨¡æ€è§†è§‰ä»»åŠ¡å¤„ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰ä»»åŠ¡` `ç›®æ ‡æ£€æµ‹` `å›¾åƒåˆ†å‰²` `è§†è§‰å‚è€ƒä»¤ç‰Œ` `PatchåµŒå…¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMè§†è§‰ä»»åŠ¡ä¾èµ–é—´æ¥è¡¨ç¤ºï¼Œå¦‚åæ ‡æ–‡æœ¬ï¼Œé™åˆ¶äº†æ€§èƒ½å’Œå¯†é›†é¢„æµ‹èƒ½åŠ›ã€‚
2. PaDTé€šè¿‡è§†è§‰å‚è€ƒä»¤ç‰Œï¼ˆVRTï¼‰ç›´æ¥ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰è¾“å‡ºï¼Œå®ç°ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤„ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPaDTåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸Šè¾¾åˆ°SOTAï¼Œä¼˜äºæ›´å¤§çš„MLLMæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿‘å¹´æ¥å‘å±•è¿…é€Ÿã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰ä»»åŠ¡æ–¹æ³•é€šå¸¸ä¾èµ–äºé—´æ¥è¡¨ç¤ºï¼Œä¾‹å¦‚ç”Ÿæˆåæ ‡ä½œä¸ºæ–‡æœ¬è¿›è¡Œæ£€æµ‹ï¼Œè¿™é™åˆ¶äº†æ€§èƒ½å¹¶é˜»ç¢äº†è¯¸å¦‚åˆ†å‰²ä¹‹ç±»çš„å¯†é›†é¢„æµ‹ä»»åŠ¡ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Patch-as-Decodable Tokenï¼ˆPaDTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„èŒƒä¾‹ï¼Œä½¿MLLMèƒ½å¤Ÿç›´æ¥ç”Ÿæˆæ–‡æœ¬å’Œå„ç§è§†è§‰è¾“å‡ºã€‚PaDTçš„æ ¸å¿ƒæ˜¯è§†è§‰å‚è€ƒä»¤ç‰Œï¼ˆVRTï¼‰ï¼Œå®ƒæºè‡ªæŸ¥è¯¢å›¾åƒçš„è§†è§‰patchåµŒå…¥ï¼Œå¹¶ä¸LLMçš„è¾“å‡ºæ–‡æœ¬ä»¤ç‰Œæ— ç¼äº¤ç»‡ã€‚ç„¶åï¼Œä¸€ä¸ªè½»é‡çº§è§£ç å™¨å°†LLMçš„è¾“å‡ºè½¬æ¢ä¸ºæ£€æµ‹ã€åˆ†å‰²å’Œgroundingé¢„æµ‹ã€‚ä¸å…ˆå‰çš„æ–¹æ³•ä¸åŒï¼ŒPaDTåœ¨æ¯æ¬¡å‰å‘ä¼ é€’ä¸­ç‹¬ç«‹å¤„ç†VRTï¼Œå¹¶åŠ¨æ€æ‰©å±•åµŒå…¥è¡¨ï¼Œä»è€Œæé«˜å®šä½å’ŒåŒºåˆ†ç›¸ä¼¼å¯¹è±¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡éšæœºé€‰æ‹©VRTè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¹¶å¼•å…¥å¼ºå¤§çš„per-tokenäº¤å‰ç†µæŸå¤±ï¼Œä¸ºPaDTé‡èº«å®šåˆ¶äº†è®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬åœ¨å››ä¸ªè§†è§‰æ„ŸçŸ¥å’Œç†è§£ä»»åŠ¡ä¸­çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒPaDTå§‹ç»ˆå¦‚ä¸€åœ°å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”šè‡³ä¸æ›´å¤§çš„MLLMæ¨¡å‹ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä»£ç å¯åœ¨https://github.com/Gorilla-Lab-SCUT/PaDTè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMåœ¨å¤„ç†è§†è§‰ä»»åŠ¡æ—¶ï¼Œé€šå¸¸å°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼ˆå¦‚åæ ‡ï¼‰ï¼Œå†ç”±LLMå¤„ç†ã€‚è¿™ç§é—´æ¥è¡¨ç¤ºæ–¹å¼é™åˆ¶äº†æ¨¡å‹åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰ä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´å®šä½ç²¾åº¦ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†ç›¸ä¼¼ç‰©ä½“ï¼Œå½±å“æ•´ä½“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPaDTçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å›¾åƒpatchç›´æ¥ç¼–ç ä¸ºå¯è§£ç çš„ä»¤ç‰Œï¼ˆVisual Reference Tokens, VRTsï¼‰ï¼Œä¸LLMçš„æ–‡æœ¬ä»¤ç‰Œæ··åˆï¼Œä½¿LLMèƒ½å¤Ÿç›´æ¥å¤„ç†è§†è§‰ä¿¡æ¯å¹¶ç”Ÿæˆè§†è§‰è¾“å‡ºã€‚é€šè¿‡åŠ¨æ€æ‰©å±•VRTåµŒå…¥è¡¨ï¼Œå¢å¼ºæ¨¡å‹åŒºåˆ†ç›¸ä¼¼å¯¹è±¡çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPaDTçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) å°†è¾“å…¥å›¾åƒåˆ†å‰²æˆpatchï¼Œå¹¶æå–patchçš„è§†è§‰åµŒå…¥ï¼›2) å°†è¿™äº›è§†è§‰åµŒå…¥ä½œä¸ºVRTsï¼Œä¸LLMçš„æ–‡æœ¬è¾“å…¥äº¤ç»‡åœ¨ä¸€èµ·ï¼›3) LLMå¤„ç†æ··åˆçš„æ–‡æœ¬å’Œè§†è§‰ä»¤ç‰Œï¼Œç”Ÿæˆè¾“å‡ºï¼›4) ä¸€ä¸ªè½»é‡çº§çš„è§£ç å™¨å°†LLMçš„è¾“å‡ºè½¬æ¢ä¸ºå…·ä½“çš„è§†è§‰é¢„æµ‹ï¼Œå¦‚æ£€æµ‹æ¡†ã€åˆ†å‰²æ©ç ç­‰ã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€å°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„è§†è§‰ä»»åŠ¡å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šPaDTçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å¼•å…¥äº†VRTsï¼Œä½¿LLMèƒ½å¤Ÿç›´æ¥å¤„ç†è§†è§‰ä¿¡æ¯ï¼Œé¿å…äº†ä¿¡æ¯æŸå¤±å’Œæ€§èƒ½ç“¶é¢ˆï¼›2) åŠ¨æ€æ‰©å±•VRTåµŒå…¥è¡¨ï¼Œå¢å¼ºäº†æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å¯¹è±¡çš„èƒ½åŠ›ï¼›3) è®¾è®¡äº†ä¸€ç§é’ˆå¯¹PaDTçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬éšæœºé€‰æ‹©VRTè¿›è¡Œç›‘ç£å¾®è°ƒå’Œå¼•å…¥per-tokenäº¤å‰ç†µæŸå¤±ï¼Œæé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šPaDTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) VRTçš„ç”Ÿæˆæ–¹å¼ï¼šä½¿ç”¨è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ViTï¼‰æå–å›¾åƒpatchçš„åµŒå…¥ä½œä¸ºVRTï¼›2) VRTä¸æ–‡æœ¬ä»¤ç‰Œçš„äº¤ç»‡æ–¹å¼ï¼šå°†VRTæ’å…¥åˆ°æ–‡æœ¬åºåˆ—ä¸­ï¼Œä½¿LLMèƒ½å¤ŸåŒæ—¶å¤„ç†æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼›3) è§£ç å™¨çš„è®¾è®¡ï¼šä½¿ç”¨è½»é‡çº§çš„è§£ç å™¨å°†LLMçš„è¾“å‡ºè½¬æ¢ä¸ºå…·ä½“çš„è§†è§‰é¢„æµ‹ï¼Œå¦‚æ£€æµ‹æ¡†ã€åˆ†å‰²æ©ç ç­‰ï¼›4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼šä½¿ç”¨per-tokenäº¤å‰ç†µæŸå¤±ï¼Œå¯¹æ¯ä¸ªä»¤ç‰Œçš„é¢„æµ‹è¿›è¡Œç›‘ç£ï¼Œæé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PaDTåœ¨å››ä¸ªè§†è§‰æ„ŸçŸ¥å’Œç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼ŒåŒ…æ‹¬ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²å’Œè§†è§‰groundingã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPaDTèƒ½å¤Ÿè¶…è¶Šæ›´å¤§çš„MLLMæ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒPaDTçš„æ€§èƒ½æ¯”ç°æœ‰æœ€ä½³æ¨¡å‹æå‡äº†X%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PaDTå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æ™ºèƒ½å®‰é˜²ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåˆ†æã€æœºå™¨äººè§†è§‰ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€è§†è§‰å…³ç³»æ¨ç†ç­‰ä»»åŠ¡ï¼Œæå‡ç›¸å…³åº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼ŒPaDTæœ‰æœ›æˆä¸ºå¤šæ¨¡æ€è§†è§‰ä»»åŠ¡å¤„ç†çš„é‡è¦åŸºçŸ³ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

