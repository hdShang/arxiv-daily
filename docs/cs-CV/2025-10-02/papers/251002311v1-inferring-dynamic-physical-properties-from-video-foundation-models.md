---
layout: default
title: Inferring Dynamic Physical Properties from Video Foundation Models
---

# Inferring Dynamic Physical Properties from Video Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02311" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02311v1</a>
  <a href="https://arxiv.org/pdf/2510.02311.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02311v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02311v1', 'Inferring Dynamic Physical Properties from Video Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanqi Zhan, Xianzheng Ma, Weidi Xie, Andrew Zisserman

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨è§†é¢‘åŸºç¡€æ¨¡å‹æ¨æ–­è§†é¢‘ä¸­çš„åŠ¨æ€ç‰©ç†å±æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `ç‰©ç†å±æ€§æ¨æ–­` `è§†é¢‘åŸºç¡€æ¨¡å‹` `è§†è§‰æç¤ºå­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥ä»…ä»è§†é¢‘ä¸­å‡†ç¡®æ¨æ–­ç‰©ä½“çš„å¼¹æ€§ã€æ¶²ä½“çš„ç²˜åº¦ã€æ‘©æ“¦åŠ›ç­‰åŠ¨æ€ç‰©ç†å±æ€§ã€‚
2. åˆ©ç”¨è§†é¢‘åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰æç¤ºå’Œå¯è®­ç»ƒæç¤ºå‘é‡ï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°ç‰©ç†å±æ€§çš„æ¨æ–­ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè§†é¢‘ç”Ÿæˆå’Œè‡ªç›‘ç£æ¨¡å‹æ€§èƒ½ç›¸è¿‘ï¼Œä½†å‡ä½äºoracleæ–¹æ³•ï¼ŒMLLMæ€§èƒ½æœ‰å¾…æå‡ï¼Œä½†æ½œåŠ›å·¨å¤§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä»è§†é¢‘ä¸­é¢„æµ‹åŠ¨æ€ç‰©ç†å±æ€§çš„ä»»åŠ¡ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬è€ƒè™‘éœ€è¦æ—¶é—´ä¿¡æ¯æ‰èƒ½æ¨æ–­çš„ç‰©ç†å±æ€§ï¼šå¼¹è·³ç‰©ä½“çš„å¼¹æ€§ã€æµåŠ¨æ¶²ä½“çš„ç²˜åº¦ä»¥åŠç‰©ä½“åœ¨è¡¨é¢ä¸Šæ»‘åŠ¨çš„åŠ¨æ‘©æ“¦åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åšå‡ºäº†ä»¥ä¸‹è´¡çŒ®ï¼šï¼ˆiï¼‰æˆ‘ä»¬ä¸ºæ¯ä¸ªç‰©ç†å±æ€§æ”¶é›†äº†ä¸€ä¸ªæ–°çš„è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…æ‹¬åˆæˆè®­ç»ƒå’Œæµ‹è¯•é›†ï¼Œä»¥åŠç”¨äºçœŸå®ä¸–ç•Œè¯„ä¼°çš„çœŸå®æ•°æ®é›†ã€‚ï¼ˆiiï¼‰æˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§ä»è§†é¢‘ä¸­æ¨æ–­ç‰©ç†å±æ€§çš„æ–¹æ³•ï¼šï¼ˆaï¼‰ä¸€ç§oracleæ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»å…¸çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯æä¾›å†…åœ¨åæ˜ è¯¥å±æ€§çš„è§†è§‰çº¿ç´¢ï¼›ï¼ˆbï¼‰ä¸€ç§ç®€å•çš„è¯»å–æœºåˆ¶ï¼Œä½¿ç”¨è§†è§‰æç¤ºå’Œå¯è®­ç»ƒçš„æç¤ºå‘é‡ï¼Œç”¨äºé¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆå’Œè‡ªç›‘ç£æ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›ï¼›ï¼ˆcï¼‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æç¤ºç­–ç•¥ã€‚ï¼ˆiiiï¼‰æˆ‘ä»¬è¡¨æ˜ï¼Œä»¥ç”Ÿæˆæˆ–è‡ªç›‘ç£æ–¹å¼è®­ç»ƒçš„è§†é¢‘åŸºç¡€æ¨¡å‹å®ç°äº†ç›¸ä¼¼çš„æ€§èƒ½ï¼Œå°½ç®¡è½åäºoracleæ–¹æ³•ï¼Œå¹¶ä¸”MLLMç›®å‰ä¸å¦‚å…¶ä»–æ¨¡å‹ï¼Œä½†å¯ä»¥é€šè¿‡é€‚å½“çš„æç¤ºæ¥æé«˜å…¶æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»è§†é¢‘ä¸­æ¨æ–­åŠ¨æ€ç‰©ç†å±æ€§çš„é—®é¢˜ï¼Œä¾‹å¦‚ç‰©ä½“çš„å¼¹æ€§ã€æ¶²ä½“çš„ç²˜åº¦ä»¥åŠç‰©ä½“è¡¨é¢çš„æ‘©æ“¦åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šçš„è§†è§‰çº¿ç´¢æˆ–æ‰‹å·¥è®¾è®¡çš„ç‰¹å¾ï¼Œç¼ºä¹é€šç”¨æ€§å’Œé²æ£’æ€§ï¼Œéš¾ä»¥å¤„ç†å¤æ‚åœºæ™¯å’ŒçœŸå®ä¸–ç•Œçš„è§†é¢‘æ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰æç¤ºå­¦ä¹ çš„æ–¹å¼ï¼Œå°†è§†é¢‘å†…å®¹ä¸ç‰©ç†å±æ€§å»ºç«‹è”ç³»ã€‚é€šè¿‡å¯è®­ç»ƒçš„æç¤ºå‘é‡ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ä¸ç‰¹å®šç‰©ç†å±æ€§ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œå®ç°å±æ€§çš„æ¨æ–­ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ‰‹å·¥è®¾è®¡ç‰¹å¾çš„ç¹çè¿‡ç¨‹ï¼Œå¹¶èƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šï¼ˆ1ï¼‰æ•°æ®æ”¶é›†ï¼šæ„å»ºåŒ…å«åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®çš„è§†é¢‘æ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒçš„ç‰©ç†å±æ€§ã€‚ï¼ˆ2ï¼‰æ¨¡å‹é€‰æ‹©ï¼šé€‰æ‹©é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Generative Pre-trained Transformerï¼‰å’Œè‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚Masked Autoencodersï¼‰ï¼Œä»¥åŠå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚ï¼ˆ3ï¼‰æç¤ºå­¦ä¹ ï¼šè®¾è®¡è§†è§‰æç¤ºå’Œå¯è®­ç»ƒçš„æç¤ºå‘é‡ï¼Œç”¨äºå¼•å¯¼æ¨¡å‹å…³æ³¨ä¸ç‰©ç†å±æ€§ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å°†æç¤ºä¿¡æ¯èå…¥åˆ°è§†é¢‘ç‰¹å¾ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨è§†é¢‘åŸºç¡€æ¨¡å‹è¿›è¡ŒåŠ¨æ€ç‰©ç†å±æ€§çš„æ¨æ–­ï¼Œå¹¶æå‡ºäº†åŸºäºè§†è§‰æç¤ºçš„å­¦ä¹ æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†MLLMåœ¨ç‰©ç†å±æ€§æ¨æ–­ä¸­çš„åº”ç”¨ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰è§†è§‰æç¤ºçš„è®¾è®¡ï¼šé€‰æ‹©ä¸ç‰©ç†å±æ€§ç›¸å…³çš„è§†è§‰çº¿ç´¢ä½œä¸ºæç¤ºï¼Œä¾‹å¦‚å¼¹è·³é«˜åº¦ã€æ¶²ä½“æµåŠ¨é€Ÿåº¦ç­‰ã€‚ï¼ˆ2ï¼‰æç¤ºå‘é‡çš„è®­ç»ƒï¼šä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æˆ–å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–æç¤ºå‘é‡ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹å…³æ³¨ä¸ç‰©ç†å±æ€§ç›¸å…³çš„è§†è§‰ç‰¹å¾ã€‚ï¼ˆ3ï¼‰æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒæ•´ï¼šé’ˆå¯¹ä¸åŒçš„è§†é¢‘åŸºç¡€æ¨¡å‹ï¼Œé€‰æ‹©åˆé€‚çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®ï¼Œä»¥è·å¾—æœ€ä½³çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè§†é¢‘åŸºç¡€æ¨¡å‹åœ¨åŠ¨æ€ç‰©ç†å±æ€§æ¨æ–­ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸€å®šçš„æ½œåŠ›ï¼Œç”Ÿæˆæ¨¡å‹å’Œè‡ªç›‘ç£æ¨¡å‹æ€§èƒ½ç›¸è¿‘ï¼Œä½†å‡ä½äºoracleæ–¹æ³•ã€‚MLLMçš„æ€§èƒ½ç›¸å¯¹è¾ƒå·®ï¼Œä½†é€šè¿‡åˆé€‚çš„æç¤ºç­–ç•¥å¯ä»¥æ˜¾è‘—æé«˜å…¶æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨å¼¹æ€§é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–æç¤ºå‘é‡ï¼ŒMLLMçš„å‡†ç¡®ç‡æé«˜äº†10%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€ç‰©ç†ä»¿çœŸã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®è§†è§‰ä¿¡æ¯åˆ¤æ–­ç‰©ä½“çš„å¼¹æ€§ï¼Œä»è€Œæ›´å¥½åœ°å®ŒæˆæŠ“å–å’Œæ“ä½œä»»åŠ¡ã€‚ç‰©ç†å¼•æ“å¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•æé«˜ä»¿çœŸç²¾åº¦ï¼Œç”Ÿæˆæ›´é€¼çœŸçš„ç‰©ç†æ•ˆæœã€‚æ¸¸æˆå¼€å‘è€…å¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆæ¸¸æˆåœºæ™¯ä¸­çš„ç‰©ç†å±æ€§ï¼Œæé«˜å¼€å‘æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dynamic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, consisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mechanism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strategies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting.

