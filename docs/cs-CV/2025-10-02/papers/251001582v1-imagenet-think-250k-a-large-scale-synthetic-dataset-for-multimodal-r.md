---
layout: default
title: ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models
---

# ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01582" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01582v1</a>
  <a href="https://arxiv.org/pdf/2510.01582.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01582v1" onclick="toggleFavorite(this, '2510.01582v1', 'ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Krishna Teja Chitty-Venkata, Murali Emani

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºImageNet-Think-250Kï¼Œç”¨äºæå‡è§†è§‰è¯­è¨€æ¨¡å‹å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `åˆæˆæ•°æ®é›†` `ImageNet` `æ˜¾å¼æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œç¼ºä¹æ˜¾å¼çš„æ¨ç†è¿‡ç¨‹ã€‚
2. æ„å»ºå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«å›¾åƒã€æ¨ç†æ­¥éª¤å’Œç­”æ¡ˆï¼Œç”¨äºè®­ç»ƒVLMã€‚
3. ä½¿ç”¨å…ˆè¿›VLMç”Ÿæˆæ•°æ®ï¼Œæä¾›ç»“æ„åŒ–æ¨ç†é“¾ï¼Œå¹¶å…¬å¼€æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ImageNet-Thinkï¼Œä¸€ä¸ªå¤šæ¨¡æ€æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨è¾…åŠ©å¼€å‘å…·æœ‰æ˜¾å¼æ¨ç†èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚è¯¥æ•°æ®é›†åŸºäºImageNet21kæ•°æ®é›†ä¸­çš„25ä¸‡å¼ å›¾åƒæ„å»ºï¼Œæä¾›äº†ç»“æ„åŒ–çš„æ€è€ƒtokenå’Œç›¸åº”çš„ç­”æ¡ˆã€‚è¯¥åˆæˆæ•°æ®é›†ç”±ä¸¤ä¸ªå…ˆè¿›çš„VLMç”Ÿæˆï¼šGLM-4.1V-9B-Thinkingå’ŒKimi-VL-A3B-Thinking-2506ã€‚æ¯å¼ å›¾åƒéƒ½é…æœ‰ä¸¤å¯¹æ€è€ƒ-ç­”æ¡ˆåºåˆ—ï¼Œä¸ºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æä¾›äº†èµ„æºã€‚è¯¥æ•°æ®é›†æ•æ‰äº†VLMçš„é€æ­¥æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆçš„æè¿°æ€§ç­”æ¡ˆã€‚ç›®æ ‡æ˜¯ä¿ƒè¿›æ›´å¼ºå¤§çš„VLMçš„å¼€å‘ï¼Œå¹¶ä¿ƒè¿›å¯¹å¤šæ¨¡æ€æ¨ç†æœºåˆ¶çš„æ›´å¹¿æ³›ç†è§£ã€‚æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†å°†å…¬å¼€æä¾›ï¼Œä»¥å¸®åŠ©æ¨ç†/æ€è€ƒå¤šæ¨¡æ€VLMçš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„VLMé€šå¸¸ç¼ºä¹æ˜¾å¼çš„æ¨ç†è¿‡ç¨‹ï¼Œéš¾ä»¥å¤„ç†éœ€è¦é€æ­¥æ€è€ƒå’Œæ¨ç†çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•æ¥å¢å¼ºVLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåƒäººç±»ä¸€æ ·è¿›è¡Œé€æ­¥æ€è€ƒå¹¶ç»™å‡ºåˆç†çš„ç­”æ¡ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å›¾åƒã€ç»“æ„åŒ–çš„æ¨ç†æ­¥éª¤ï¼ˆæ€è€ƒtokenï¼‰ä»¥åŠæœ€ç»ˆç­”æ¡ˆã€‚é€šè¿‡åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒVLMï¼Œå¯ä»¥ä½¿å…¶å­¦ä¹ åˆ°æ˜¾å¼çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæé«˜å…¶å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºäººç±»é€šè¿‡å­¦ä¹ å’Œç»ƒä¹ æ¥æé«˜è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä»ImageNet21kæ•°æ®é›†ä¸­é€‰æ‹©25ä¸‡å¼ å›¾åƒï¼›2) ä½¿ç”¨ä¸¤ä¸ªå…ˆè¿›çš„VLMï¼ˆGLM-4.1V-9B-Thinkingå’ŒKimi-VL-A3B-Thinking-2506ï¼‰ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆä¸¤å¯¹æ€è€ƒ-ç­”æ¡ˆåºåˆ—ï¼›3) å°†å›¾åƒã€æ€è€ƒtokenå’Œç­”æ¡ˆç»„æˆæ•°æ®é›†ImageNet-Think-250Kï¼›4) ä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°VLMã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ã€åŒ…å«ç»“æ„åŒ–æ¨ç†æ­¥éª¤çš„åˆæˆæ•°æ®é›†ã€‚ä¸ä»¥å¾€çš„æ•°æ®é›†ä¸åŒï¼ŒImageNet-Think-250Kä¸ä»…åŒ…å«å›¾åƒå’Œç­”æ¡ˆï¼Œè¿˜åŒ…å«äº†VLMçš„é€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œè¿™ä½¿å¾—VLMèƒ½å¤Ÿå­¦ä¹ åˆ°æ˜¾å¼çš„æ¨ç†è·¯å¾„ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å…ˆè¿›çš„VLMç”Ÿæˆæ•°æ®ä¹Ÿä¿è¯äº†æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ImageNet21kæ•°æ®é›†ä½œä¸ºå›¾åƒæ¥æºï¼Œä¿è¯äº†å›¾åƒçš„å¤šæ ·æ€§å’Œè§„æ¨¡ï¼›2) ä½¿ç”¨GLM-4.1V-9B-Thinkingå’ŒKimi-VL-A3B-Thinking-2506è¿™ä¸¤ä¸ªå…ˆè¿›çš„VLMç”Ÿæˆæ€è€ƒ-ç­”æ¡ˆåºåˆ—ï¼Œä¿è¯äº†æ•°æ®çš„è´¨é‡ï¼›3) ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆä¸¤å¯¹æ€è€ƒ-ç­”æ¡ˆåºåˆ—ï¼Œå¢åŠ äº†æ•°æ®çš„å¤šæ ·æ€§ï¼›4) å…¬å¼€æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€æ¨ç†VLMçš„ç ”ç©¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«25ä¸‡å¼ å›¾åƒçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ªå…ˆè¿›çš„VLMç”Ÿæˆæ€è€ƒ-ç­”æ¡ˆåºåˆ—ã€‚è¯¥æ•°æ®é›†ä¸ºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚è™½ç„¶è®ºæ–‡æ²¡æœ‰æä¾›å…·ä½“çš„æ€§èƒ½æ•°æ®ï¼Œä½†å…¬å¼€çš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†å°†ä¿ƒè¿›ç›¸å…³ç ”ç©¶çš„è¿›å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½é—®ç­”ã€å›¾åƒç†è§£ã€è§†è§‰æ¨ç†ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºå¼€å‘èƒ½å¤Ÿç†è§£å›¾åƒå†…å®¹å¹¶è¿›è¡Œå¤æ‚æ¨ç†çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ–è€…ç”¨äºæé«˜å›¾åƒæœç´¢çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ¨åŠ¨VLMåœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the development of Vision Language Models (VLMs) with explicit reasoning capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset, providing structured thinking tokens and corresponding answers. Our synthetic dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of thinking-answer sequences, creating a resource for training and evaluating multimodal reasoning models. We capture the step-by-step reasoning process of VLMs and the final descriptive answers. Our goal with this dataset is to enable the development of more robust VLMs while contributing to the broader understanding of multimodal reasoning mechanisms. The dataset and evaluation benchmarks will be publicly available to aid research in reasoning/thinking multimodal VLMs.

