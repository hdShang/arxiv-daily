---
layout: default
title: VLA-R1: Enhancing Reasoning in Vision-Language-Action Models
---

# VLA-R1: Enhancing Reasoning in Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01623" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01623v1</a>
  <a href="https://arxiv.org/pdf/2510.01623.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01623v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01623v1', 'VLA-R1: Enhancing Reasoning in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, Zheng Zhu

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/GigaAI-research/VLA-R1) | [PROJECT_PAGE](https://gigaai-research.github.io/VLA-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLA-R1ä»¥è§£å†³è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹æ¨ç†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-è¡ŒåŠ¨` `æ¨ç†å¢å¼º` `å¼ºåŒ–å­¦ä¹ ` `ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–` `å¯éªŒè¯å¥–åŠ±` `å¤šæ¨¡æ€å­¦ä¹ ` `å…·èº«äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ç¼ºä¹é€æ­¥æ¨ç†ï¼Œç›´æ¥è¾“å‡ºæœ€ç»ˆåŠ¨ä½œï¼Œå¿½è§†äº†å¯ç”¨æ€§å’Œå‡ ä½•å…³ç³»ã€‚
2. VLA-R1é€šè¿‡ç»“åˆRLVRå’ŒGRPOï¼Œæå‡ºäº†ä¸€ç§ç³»ç»Ÿä¼˜åŒ–æ¨ç†å’Œæ‰§è¡Œçš„ç­–ç•¥ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªé¢†åŸŸçš„è¯„ä¼°ä¸­ï¼ŒVLA-R1è¡¨ç°å‡ºæ¯”ä¹‹å‰çš„VLAæ–¹æ³•æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹æ—¨åœ¨ç»Ÿä¸€æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’Œè¡ŒåŠ¨ç”Ÿæˆï¼Œå…·æœ‰å¼ºå¤§çš„è·¨ä»»åŠ¡å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹å…·èº«äººå·¥æ™ºèƒ½äº§ç”Ÿå¹¿æ³›å½±å“ã€‚ç„¶è€Œï¼Œç°æœ‰VLAæ¨¡å‹å¾€å¾€ç¼ºä¹æ˜ç¡®çš„é€æ­¥æ¨ç†ï¼Œç›´æ¥è¾“å‡ºæœ€ç»ˆåŠ¨ä½œè€Œä¸è€ƒè™‘å¯ç”¨æ€§çº¦æŸæˆ–å‡ ä½•å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†VLA-R1ï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºæ¨ç†çš„VLAæ¨¡å‹ï¼Œç»“åˆäº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç³»ç»Ÿåœ°ä¼˜åŒ–æ¨ç†å’Œæ‰§è¡Œã€‚é€šè¿‡è®¾è®¡åŸºäºRLVRçš„åè®­ç»ƒç­–ç•¥ï¼Œå¢å¼ºæ¨ç†çš„é²æ£’æ€§å’Œæ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLA-R1åœ¨å¤šä¸ªå¹³å°ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œç°å®ä¸–ç•Œæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯ç°æœ‰VLAæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹é€æ­¥æ¨ç†å’Œå¯¹å¯ç”¨æ€§çº¦æŸçš„è€ƒè™‘ï¼Œå¯¼è‡´æœ€ç»ˆåŠ¨ä½œçš„ç”Ÿæˆè´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLA-R1çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç³»ç»Ÿæ€§åœ°ä¼˜åŒ–æ¨ç†å’Œæ‰§è¡Œè¿‡ç¨‹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ‰§è¡Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLA-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€RLVRåè®­ç»ƒç­–ç•¥ã€GRPOä¼˜åŒ–æ¨¡å—å’Œæœ€ç»ˆçš„åŠ¨ä½œç”Ÿæˆæ¨¡å—ã€‚é€šè¿‡è¿™äº›æ¨¡å—çš„ååŒå·¥ä½œï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å’Œæ‰§è¡Œä¸­è¾¾åˆ°æ›´é«˜çš„è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLA-R1çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†åŸºäºRLVRçš„åè®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨å¯éªŒè¯å¥–åŠ±æ¥å¼ºåŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†çš„é²æ£’æ€§å’Œæ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒRLVRç­–ç•¥å…³æ³¨åŒºåŸŸå¯¹é½ã€è½¨è¿¹ä¸€è‡´æ€§å’Œè¾“å‡ºæ ¼å¼åŒ–ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”ŸæˆåŠ¨ä½œæ—¶èƒ½å¤Ÿè€ƒè™‘åˆ°ç¯å¢ƒçš„å‡ ä½•å…³ç³»å’Œå¯ç”¨æ€§çº¦æŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªé¢†åŸŸçš„è¯„ä¼°ä¸­ï¼ŒVLA-R1åœ¨æ¨ç†å’Œæ‰§è¡Œæ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¹‹å‰çš„VLAæ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒVLA-R1åœ¨çœŸå®æœºå™¨äººå¹³å°ä¸Šçš„ä»»åŠ¡æˆåŠŸç‡æé«˜äº†20%ï¼Œåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›æå‡äº†15%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVLA-R1åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLA-R1æ¨¡å‹åœ¨å…·èº«äººå·¥æ™ºèƒ½é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ã€‚é€šè¿‡å¢å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¤æ‚ç¯å¢ƒä¸­çš„ä»»åŠ¡ï¼Œä»è€Œæé«˜æ‰§è¡Œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼ŒVLA-R1æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿçš„å‘å±•ï¼Œæå‡äººæœºåä½œçš„æ™ºèƒ½æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.

