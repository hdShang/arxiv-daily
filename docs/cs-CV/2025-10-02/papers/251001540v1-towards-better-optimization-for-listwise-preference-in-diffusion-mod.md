---
layout: default
title: Towards Better Optimization For Listwise Preference in Diffusion Models
---

# Towards Better Optimization For Listwise Preference in Diffusion Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01540" target="_blank" class="toolbar-btn">arXiv: 2510.01540v1</a>
    <a href="https://arxiv.org/pdf/2510.01540.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01540v1" 
            onclick="toggleFavorite(this, '2510.01540v1', 'Towards Better Optimization For Listwise Preference in Diffusion Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiamu Bai, Xin Yu, Meilong Xu, Weitao Lu, Xin Pan, Kiwan Maeng, Daniel Kifer, Jian Wang, Yu Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Diffusion-LPOÔºåÁî®‰∫éÊâ©Êï£Ê®°Âûã‰∏≠Âü∫‰∫éÂàóË°®ÂÅèÂ•ΩÁöÑ‰ºòÂåñÔºåÊèêÂçáÂõæÂÉèË¥®ÈáèÂíåÂÅèÂ•ΩÂØπÈΩê„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Ê®°Âûã` `ÂÅèÂ•ΩÂ≠¶‰π†` `ÂàóË°®ÂÅèÂ•Ω‰ºòÂåñ` `Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ` `Plackett-LuceÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊâ©Êï£Ê®°ÂûãÂÅèÂ•ΩÂ≠¶‰π†‰∏ªË¶Å‰æùËµñÊàêÂØπÂÅèÂ•ΩÔºåÂøΩÁï•‰∫Ü‰∫∫Â∑•ÂèçÈ¶à‰∏≠Êõ¥Á≤æÁ°ÆÁöÑÂàóË°®ÊéíÂ∫è‰ø°ÊÅØ„ÄÇ
2. Diffusion-LPOÈÄöËøáËÅöÂêàÁî®Êà∑ÂèçÈ¶à‰∏∫ÊéíÂ∫èÂàóË°®ÔºåÂπ∂Âü∫‰∫éPlackett-LuceÊ®°ÂûãÊâ©Â±ïDPOÁõÆÊ†áÔºåÂÆûÁé∞ÂàóË°®ÂÅèÂ•Ω‰ºòÂåñ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDiffusion-LPOÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê„ÄÅÂõæÂÉèÁºñËæëÂíå‰∏™ÊÄßÂåñÂÅèÂ•ΩÂØπÈΩêÁ≠â‰ªªÂä°‰∏äÔºåÊòæËëó‰ºò‰∫éÊàêÂØπDPOÂü∫Á∫ø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Diffusion-LPOÁöÑÁÆÄÂçïÊúâÊïàÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÊâ©Êï£Ê®°Âûã‰∏≠Âü∫‰∫éÂàóË°®Êï∞ÊçÆÁöÑÂàóË°®ÂÅèÂ•Ω‰ºòÂåñ„ÄÇÂ∞ΩÁÆ°Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ(DPO)Âõ†ÂÖ∂ËÆ°ÁÆóÊïàÁéáÂíåÈÅøÂÖçÊòæÂºèÂ•ñÂä±Âª∫Ê®°ËÄåË¢´ÂπøÊ≥õÈááÁî®Ôºå‰ΩÜÂÖ∂Âú®Êâ©Êï£Ê®°Âûã‰∏≠ÁöÑÂ∫îÁî®‰∏ªË¶Å‰æùËµñ‰∫éÊàêÂØπÂÅèÂ•Ω„ÄÇÂØπÂàóË°®ÂÅèÂ•ΩÁöÑÁ≤æÁ°Æ‰ºòÂåñÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™ÂæóÂà∞Ëß£ÂÜ≥„ÄÇÂÆûÈôÖ‰∏äÔºåÂÖ≥‰∫éÂõæÂÉèÂÅèÂ•ΩÁöÑ‰∫∫Â∑•ÂèçÈ¶àÈÄöÂ∏∏ÂåÖÂê´ÈöêÂºèÊéíÂ∫è‰ø°ÊÅØÔºåËøôÊØîÊàêÂØπÊØîËæÉ‰º†Ëææ‰∫ÜÊõ¥Á≤æÁ°ÆÁöÑ‰∫∫Â∑•ÂÅèÂ•Ω„ÄÇÁªôÂÆö‰∏Ä‰∏™Ê†áÈ¢òÔºåÊàë‰ª¨Â∞ÜÁî®Êà∑ÂèçÈ¶àËÅöÂêàÂà∞‰∏Ä‰∏™ÊéíÂ∫èÁöÑÂõæÂÉèÂàóË°®‰∏≠ÔºåÂπ∂Âú®Plackett-LuceÊ®°Âûã‰∏ãÊé®ÂØºÂá∫DPOÁõÆÊ†áÁöÑÂàóË°®Êâ©Â±ï„ÄÇDiffusion-LPOÈÄöËøáÈºìÂä±ÊØè‰∏™Ê†∑Êú¨‰ºò‰∫éÊâÄÊúâÊéíÂêçËæÉ‰ΩéÁöÑÊõø‰ª£ÊñπÊ°àÊù•Âº∫Âà∂ÊâßË°åÊï¥‰∏™ÊéíÂêçÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÁªèÈ™åËØÅË°®ÊòéÔºåDiffusion-LPOÂú®ÂêÑÁßç‰ªªÂä°‰∏≠ÈÉΩÊúâÊïàÔºåÂåÖÊã¨ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê„ÄÅÂõæÂÉèÁºñËæëÂíå‰∏™ÊÄßÂåñÂÅèÂ•ΩÂØπÈΩê„ÄÇDiffusion-LPOÂú®ËßÜËßâË¥®ÈáèÂíåÂÅèÂ•ΩÂØπÈΩêÊñπÈù¢ÂßãÁªà‰ºò‰∫éÊàêÂØπDPOÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫é‰∫∫Á±ªÂèçÈ¶àÁöÑÊâ©Êï£Ê®°ÂûãËÆ≠ÁªÉÊñπÊ≥ïÔºåÂ¶ÇDPOÔºå‰∏ªË¶Å‰æùËµñ‰∫éÊàêÂØπÂÅèÂ•ΩÊï∞ÊçÆ„ÄÇÁÑ∂ËÄåÔºåÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÁî®Êà∑ÂØπÂõæÂÉèÁöÑÂÅèÂ•ΩÂæÄÂæÄ‰ª•ÊéíÂ∫èÂàóË°®ÁöÑÂΩ¢ÂºèÁªôÂá∫ÔºåÂåÖÂê´Êõ¥‰∏∞ÂØåÁöÑ‰ø°ÊÅØ„ÄÇÁõ¥Êé•‰ΩøÁî®ÊàêÂØπÂÅèÂ•ΩÂøΩÁï•‰∫ÜÂàóË°®‰∏≠ÁöÑÊéíÂ∫èÂÖ≥Á≥ªÔºåÂØºËá¥‰ºòÂåñÊïàÁéáÈôç‰ΩéÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®‰∫∫Á±ªÂèçÈ¶à„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDiffusion-LPOÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁî®Êà∑ÁöÑÂàóË°®ÂÅèÂ•Ω‰ø°ÊÅØÁ∫≥ÂÖ•‰ºòÂåñËøáÁ®ã‰∏≠„ÄÇÂÆÉÈÄöËøáÂ∞ÜÁî®Êà∑ÂèçÈ¶àËÅöÂêà‰∏∫ÊéíÂ∫èÁöÑÂõæÂÉèÂàóË°®ÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äÊâ©Â±ïDPOÁöÑÁõÆÊ†áÂáΩÊï∞Ôºå‰ªéËÄåÂÆûÁé∞ÂØπÂàóË°®ÂÅèÂ•ΩÁöÑÁõ¥Êé•‰ºòÂåñ„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊõ¥ÂÖÖÂàÜÂú∞Âà©Áî®‰∫∫Á±ªÂèçÈ¶à‰∏≠ÁöÑÊéíÂ∫è‰ø°ÊÅØÔºåÊèêÈ´òÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÁîüÊàêË¥®Èáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDiffusion-LPOÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) Êî∂ÈõÜÁî®Êà∑ÂØπÁªôÂÆöÊñáÊú¨ÊèèËø∞ÁîüÊàêÁöÑÂ§ö‰∏™ÂõæÂÉèÁöÑÂÅèÂ•ΩÊéíÂ∫èÔºõ2) Â∞ÜËøô‰∫õÊéíÂ∫è‰ø°ÊÅØËΩ¨Âåñ‰∏∫ÂàóË°®ÂÅèÂ•ΩÊï∞ÊçÆÔºõ3) Âü∫‰∫éPlackett-LuceÊ®°ÂûãÔºåÊé®ÂØºÂá∫DPOÁõÆÊ†áÁöÑÂàóË°®Êâ©Â±ïÔºõ4) ‰ΩøÁî®Êâ©Â±ïÂêéÁöÑDPOÁõÆÊ†áÂáΩÊï∞ËÆ≠ÁªÉÊâ©Êï£Ê®°Âûã„ÄÇËØ•Ê°ÜÊû∂ÂèØ‰ª•Áõ¥Êé•Â∫îÁî®‰∫éÁé∞ÊúâÁöÑÊâ©Êï£Ê®°ÂûãËÆ≠ÁªÉÊµÅÁ®ã‰∏≠ÔºåÊó†ÈúÄ‰øÆÊîπÊ®°ÂûãÁªìÊûÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDiffusion-LPOÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈíàÂØπÂàóË°®ÂÅèÂ•ΩÁöÑDPOÁõÆÊ†áÂáΩÊï∞Êâ©Â±ï„ÄÇ‰∏é‰º†ÁªüÁöÑÊàêÂØπDPOÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÁõ¥Êé•Âà©Áî®ÂàóË°®‰∏≠ÁöÑÊéíÂ∫è‰ø°ÊÅØÔºå‰ªéËÄåÊõ¥Á≤æÁ°ÆÂú∞ÂØπÈΩêÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•Ω„ÄÇÊ≠§Â§ñÔºåDiffusion-LPOËøòÈÄöËøáPlackett-LuceÊ®°ÂûãÂØπÂàóË°®ÂÅèÂ•ΩËøõË°åÂª∫Ê®°Ôºå‰øùËØÅ‰∫Ü‰ºòÂåñËøáÁ®ãÁöÑÂêàÁêÜÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDiffusion-LPOÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Plackett-LuceÊ®°ÂûãÂØπÂàóË°®ÂÅèÂ•ΩËøõË°åÂª∫Ê®°ÔºåËØ•Ê®°ÂûãÂÅáËÆæÊØè‰∏™ÂõæÂÉèË¢´ÈÄâÊã©ÁöÑÊ¶ÇÁéá‰∏éÂÖ∂‚ÄúÂê∏ÂºïÂäõ‚ÄùÊàêÊ≠£ÊØîÔºõ2) Âü∫‰∫éPlackett-LuceÊ®°ÂûãÔºåÊé®ÂØºÂá∫DPOÁõÆÊ†áÁöÑÂàóË°®Êâ©Â±ïÔºåËØ•ÁõÆÊ†áÂáΩÊï∞ÈºìÂä±Ê®°ÂûãÁîüÊàêÊõ¥Á¨¶ÂêàÁî®Êà∑ÂÅèÂ•ΩÁöÑÂõæÂÉèÔºõ3) Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®Ê¢ØÂ∫¶‰∏ãÈôçÁ≠â‰ºòÂåñÁÆóÊ≥ïÔºåÊúÄÂ∞èÂåñÂàóË°®DPOÁõÆÊ†áÂáΩÊï∞Ôºå‰ªéËÄåÊõ¥Êñ∞Êâ©Êï£Ê®°ÂûãÁöÑÂèÇÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDiffusion-LPOÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê„ÄÅÂõæÂÉèÁºñËæëÂíå‰∏™ÊÄßÂåñÂÅèÂ•ΩÂØπÈΩêÁ≠â‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰∏éÊàêÂØπDPOÂü∫Á∫øÁõ∏ÊØîÔºåDiffusion-LPOÂú®ËßÜËßâË¥®ÈáèÂíåÂÅèÂ•ΩÂØπÈΩêÊñπÈù¢ÂùáÊúâÊòéÊòæ‰ºòÂäø„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåDiffusion-LPOËÉΩÂ§üÁîüÊàêÊõ¥Á¨¶ÂêàÁî®Êà∑ÂÅèÂ•ΩÁöÑÂõæÂÉèÔºåÂπ∂‰∏îÂú®ÂõæÂÉèË¥®ÈáèÊñπÈù¢‰πü‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ï„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜDiffusion-LPOÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Diffusion-LPOÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÊèêÂçáÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê„ÄÅÂõæÂÉèÁºñËæëÁ≠â‰ªªÂä°ÁöÑË¥®ÈáèÂíåÁî®Êà∑Êª°ÊÑèÂ∫¶„ÄÇÈÄöËøáÊõ¥Â•ΩÂú∞ÂØπÈΩêÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÔºåÂèØ‰ª•ÁîüÊàêÊõ¥Á¨¶ÂêàÁî®Êà∑ÈúÄÊ±ÇÁöÑÂõæÂÉèÂÜÖÂÆπ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫é‰∏™ÊÄßÂåñÂõæÂÉèÁîüÊàêÔºåÊ†πÊçÆÁî®Êà∑ÁöÑÁâπÂÆöÂÅèÂ•ΩÂÆöÂà∂ÂõæÂÉèÂÜÖÂÆπ„ÄÇÊú™Êù•ÔºåDiffusion-LPOÊúâÊúõÂú®ÂàõÊÑèËÆæËÆ°„ÄÅÂÜÖÂÆπÁîüÊàê„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.

