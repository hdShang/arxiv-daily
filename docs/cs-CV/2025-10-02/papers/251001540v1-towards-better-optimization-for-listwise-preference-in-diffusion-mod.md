---
layout: default
title: Towards Better Optimization For Listwise Preference in Diffusion Models
---

# Towards Better Optimization For Listwise Preference in Diffusion Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01540" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01540v1</a>
  <a href="https://arxiv.org/pdf/2510.01540.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01540v1" onclick="toggleFavorite(this, '2510.01540v1', 'Towards Better Optimization For Listwise Preference in Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiamu Bai, Xin Yu, Meilong Xu, Weitao Lu, Xin Pan, Kiwan Maeng, Daniel Kifer, Jian Wang, Yu Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiffusion-LPOï¼Œç”¨äºæ‰©æ•£æ¨¡å‹ä¸­åŸºäºåˆ—è¡¨åå¥½çš„ä¼˜åŒ–ï¼Œæå‡å›¾åƒè´¨é‡å’Œåå¥½å¯¹é½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `åå¥½å­¦ä¹ ` `åˆ—è¡¨åå¥½ä¼˜åŒ–` `ç›´æ¥åå¥½ä¼˜åŒ–` `Plackett-Luceæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰©æ•£æ¨¡å‹åå¥½å­¦ä¹ ä¸»è¦ä¾èµ–æˆå¯¹åå¥½ï¼Œå¿½ç•¥äº†äººå·¥åé¦ˆä¸­æ›´ç²¾ç¡®çš„åˆ—è¡¨æ’åºä¿¡æ¯ã€‚
2. Diffusion-LPOé€šè¿‡èšåˆç”¨æˆ·åé¦ˆä¸ºæ’åºåˆ—è¡¨ï¼Œå¹¶åŸºäºPlackett-Luceæ¨¡å‹æ‰©å±•DPOç›®æ ‡ï¼Œå®ç°åˆ—è¡¨åå¥½ä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDiffusion-LPOåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œä¸ªæ€§åŒ–åå¥½å¯¹é½ç­‰ä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—ä¼˜äºæˆå¯¹DPOåŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDiffusion-LPOçš„ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºæ‰©æ•£æ¨¡å‹ä¸­åŸºäºåˆ—è¡¨æ•°æ®çš„åˆ—è¡¨åå¥½ä¼˜åŒ–ã€‚å°½ç®¡ç›´æ¥åå¥½ä¼˜åŒ–(DPO)å› å…¶è®¡ç®—æ•ˆç‡å’Œé¿å…æ˜¾å¼å¥–åŠ±å»ºæ¨¡è€Œè¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†å…¶åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ä¸»è¦ä¾èµ–äºæˆå¯¹åå¥½ã€‚å¯¹åˆ—è¡¨åå¥½çš„ç²¾ç¡®ä¼˜åŒ–åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå¾—åˆ°è§£å†³ã€‚å®é™…ä¸Šï¼Œå…³äºå›¾åƒåå¥½çš„äººå·¥åé¦ˆé€šå¸¸åŒ…å«éšå¼æ’åºä¿¡æ¯ï¼Œè¿™æ¯”æˆå¯¹æ¯”è¾ƒä¼ è¾¾äº†æ›´ç²¾ç¡®çš„äººå·¥åå¥½ã€‚ç»™å®šä¸€ä¸ªæ ‡é¢˜ï¼Œæˆ‘ä»¬å°†ç”¨æˆ·åé¦ˆèšåˆåˆ°ä¸€ä¸ªæ’åºçš„å›¾åƒåˆ—è¡¨ä¸­ï¼Œå¹¶åœ¨Plackett-Luceæ¨¡å‹ä¸‹æ¨å¯¼å‡ºDPOç›®æ ‡çš„åˆ—è¡¨æ‰©å±•ã€‚Diffusion-LPOé€šè¿‡é¼“åŠ±æ¯ä¸ªæ ·æœ¬ä¼˜äºæ‰€æœ‰æ’åè¾ƒä½çš„æ›¿ä»£æ–¹æ¡ˆæ¥å¼ºåˆ¶æ‰§è¡Œæ•´ä¸ªæ’åçš„ä¸€è‡´æ€§ã€‚ç»éªŒè¯è¡¨æ˜ï¼ŒDiffusion-LPOåœ¨å„ç§ä»»åŠ¡ä¸­éƒ½æœ‰æ•ˆï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œä¸ªæ€§åŒ–åå¥½å¯¹é½ã€‚Diffusion-LPOåœ¨è§†è§‰è´¨é‡å’Œåå¥½å¯¹é½æ–¹é¢å§‹ç»ˆä¼˜äºæˆå¯¹DPOåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºäººç±»åé¦ˆçš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œå¦‚DPOï¼Œä¸»è¦ä¾èµ–äºæˆå¯¹åå¥½æ•°æ®ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨ä¸­ï¼Œç”¨æˆ·å¯¹å›¾åƒçš„åå¥½å¾€å¾€ä»¥æ’åºåˆ—è¡¨çš„å½¢å¼ç»™å‡ºï¼ŒåŒ…å«æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚ç›´æ¥ä½¿ç”¨æˆå¯¹åå¥½å¿½ç•¥äº†åˆ—è¡¨ä¸­çš„æ’åºå…³ç³»ï¼Œå¯¼è‡´ä¼˜åŒ–æ•ˆç‡é™ä½ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨äººç±»åé¦ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiffusion-LPOçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç”¨æˆ·çš„åˆ—è¡¨åå¥½ä¿¡æ¯çº³å…¥ä¼˜åŒ–è¿‡ç¨‹ä¸­ã€‚å®ƒé€šè¿‡å°†ç”¨æˆ·åé¦ˆèšåˆä¸ºæ’åºçš„å›¾åƒåˆ—è¡¨ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ‰©å±•DPOçš„ç›®æ ‡å‡½æ•°ï¼Œä»è€Œå®ç°å¯¹åˆ—è¡¨åå¥½çš„ç›´æ¥ä¼˜åŒ–ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å……åˆ†åœ°åˆ©ç”¨äººç±»åé¦ˆä¸­çš„æ’åºä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiffusion-LPOçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æ”¶é›†ç”¨æˆ·å¯¹ç»™å®šæ–‡æœ¬æè¿°ç”Ÿæˆçš„å¤šä¸ªå›¾åƒçš„åå¥½æ’åºï¼›2) å°†è¿™äº›æ’åºä¿¡æ¯è½¬åŒ–ä¸ºåˆ—è¡¨åå¥½æ•°æ®ï¼›3) åŸºäºPlackett-Luceæ¨¡å‹ï¼Œæ¨å¯¼å‡ºDPOç›®æ ‡çš„åˆ—è¡¨æ‰©å±•ï¼›4) ä½¿ç”¨æ‰©å±•åçš„DPOç›®æ ‡å‡½æ•°è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¡†æ¶å¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæµç¨‹ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šDiffusion-LPOçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªé’ˆå¯¹åˆ—è¡¨åå¥½çš„DPOç›®æ ‡å‡½æ•°æ‰©å±•ã€‚ä¸ä¼ ç»Ÿçš„æˆå¯¹DPOç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç›´æ¥åˆ©ç”¨åˆ—è¡¨ä¸­çš„æ’åºä¿¡æ¯ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°å¯¹é½æ¨¡å‹ä¸äººç±»åå¥½ã€‚æ­¤å¤–ï¼ŒDiffusion-LPOè¿˜é€šè¿‡Plackett-Luceæ¨¡å‹å¯¹åˆ—è¡¨åå¥½è¿›è¡Œå»ºæ¨¡ï¼Œä¿è¯äº†ä¼˜åŒ–è¿‡ç¨‹çš„åˆç†æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDiffusion-LPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Plackett-Luceæ¨¡å‹å¯¹åˆ—è¡¨åå¥½è¿›è¡Œå»ºæ¨¡ï¼Œè¯¥æ¨¡å‹å‡è®¾æ¯ä¸ªå›¾åƒè¢«é€‰æ‹©çš„æ¦‚ç‡ä¸å…¶â€œå¸å¼•åŠ›â€æˆæ­£æ¯”ï¼›2) åŸºäºPlackett-Luceæ¨¡å‹ï¼Œæ¨å¯¼å‡ºDPOç›®æ ‡çš„åˆ—è¡¨æ‰©å±•ï¼Œè¯¥ç›®æ ‡å‡½æ•°é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·åå¥½çš„å›¾åƒï¼›3) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–ç®—æ³•ï¼Œæœ€å°åŒ–åˆ—è¡¨DPOç›®æ ‡å‡½æ•°ï¼Œä»è€Œæ›´æ–°æ‰©æ•£æ¨¡å‹çš„å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusion-LPOåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œä¸ªæ€§åŒ–åå¥½å¯¹é½ç­‰ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸æˆå¯¹DPOåŸºçº¿ç›¸æ¯”ï¼ŒDiffusion-LPOåœ¨è§†è§‰è´¨é‡å’Œåå¥½å¯¹é½æ–¹é¢å‡æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚å…·ä½“è€Œè¨€ï¼ŒDiffusion-LPOèƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·åå¥½çš„å›¾åƒï¼Œå¹¶ä¸”åœ¨å›¾åƒè´¨é‡æ–¹é¢ä¹Ÿä¼˜äºåŸºçº¿æ–¹æ³•ã€‚è¿™äº›ç»“æœéªŒè¯äº†Diffusion-LPOçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Diffusion-LPOå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ç­‰ä»»åŠ¡çš„è´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚é€šè¿‡æ›´å¥½åœ°å¯¹é½æ¨¡å‹ä¸äººç±»åå¥½ï¼Œå¯ä»¥ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„å›¾åƒå†…å®¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆï¼Œæ ¹æ®ç”¨æˆ·çš„ç‰¹å®šåå¥½å®šåˆ¶å›¾åƒå†…å®¹ã€‚æœªæ¥ï¼ŒDiffusion-LPOæœ‰æœ›åœ¨åˆ›æ„è®¾è®¡ã€å†…å®¹ç”Ÿæˆã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.

