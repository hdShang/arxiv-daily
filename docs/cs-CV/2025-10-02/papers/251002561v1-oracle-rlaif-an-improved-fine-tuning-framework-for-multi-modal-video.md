---
layout: default
title: Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback
---

# Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.02561" target="_blank" class="toolbar-btn">arXiv: 2510.02561v1</a>
    <a href="https://arxiv.org/pdf/2510.02561.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02561v1" 
            onclick="toggleFavorite(this, '2510.02561v1', 'Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Derek Shi, Ruben Glatt, Christine Klymko, Shubham Mohole, Hongjun Choi, Shashank Kushwaha, Sam Sakla, Felipe Leno da Silva

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02

**Â§áÊ≥®**: Proceedings of the 39th Annual Conference on Neural Information Processing Systems, ARLET Workshop (Aligning Reinforcement Learning Experimentalists and Theorists)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Oracle-RLAIFÊ°ÜÊû∂ÔºåÈÄöËøáÊéíÂ∫èÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂ§öÊ®°ÊÄÅËßÜÈ¢ëÊ®°ÂûãÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ËßÜÈ¢ëÁêÜËß£` `Âº∫ÂåñÂ≠¶‰π†` `ÊéíÂ∫èÂ≠¶‰π†` `AIÂèçÈ¶à`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëËØ≠Ë®ÄÊ®°ÂûãÂæÆË∞É‰æùËµñÂ§ßÈáè‰∫∫Â∑•ÂèçÈ¶àÔºåÊàêÊú¨È´òÊòÇ‰∏îÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Oracle-RLAIFÊ°ÜÊû∂‰ΩøÁî®AIÊéíÂ∫èÂô®Êõø‰ª£‰∫∫Â∑•ÂèçÈ¶àÔºåÈôç‰ΩéÊàêÊú¨Âπ∂ÊèêÂçáÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåOracle-RLAIFÂú®ËßÜÈ¢ëÁêÜËß£‰ªªÂä°‰∏ä‰ºò‰∫éÁé∞ÊúâÂæÆË∞ÉÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑÂ§öÊ®°ÊÄÅËßÜÈ¢ëÊ®°ÂûãÂæÆË∞ÉÊ°ÜÊû∂Oracle-RLAIFÔºåËØ•Ê°ÜÊû∂Âà©Áî®ÊéíÂ∫èÂèçÈ¶àÁöÑÂº∫ÂåñÂ≠¶‰π†Êù•Â¢ûÂº∫ÊñáÊú¨ÂíåËßÜËßâÁêÜËß£‰πãÈó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®ÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÂü∫‰∫éÂÅèÂ•ΩÊï∞ÊçÆÁöÑÂº∫ÂåñÂ≠¶‰π†Áõ∏ÁªìÂêàÁöÑÊñπÂºèÔºå‰ΩÜÈöèÁùÄÊ®°ÂûãÂèÇÊï∞ËßÑÊ®°ÁöÑÂ¢ûÂ§ßÔºåÊî∂ÈõÜË∂≥Â§üÁöÑ‰∫∫Â∑•ÂèçÈ¶àÊàêÊú¨‰πüÈöè‰πãÂ¢ûÂä†„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÂæÆË∞ÉÁöÑÊàêÊú¨ÊïàÁõäÔºåÊú¨ÊñáÁî®AIÂèçÈ¶àÔºàRLAIFÔºâÂèñ‰ª£‰∫∫Â∑•ÂÅèÂ•Ω„ÄÇ‰∏é‰æùËµñ‰∫é‰ΩøÁî®ËßÜÈ¢ëÂèô‰∫ãËÆ≠ÁªÉÁöÑ‰∏ìÁî®Â•ñÂä±Ê®°ÂûãÁöÑÁé∞ÊúâRLAIFÊ°ÜÊû∂‰∏çÂêåÔºåOracle-RLAIF‰ΩøÁî®Êõ¥ÈÄöÁî®ÁöÑOracleÊéíÂ∫èÂô®Êù•ÂØπÂÄôÈÄâÊ®°ÂûãÂìçÂ∫îËøõË°åÊéíÂ∫è„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÂºïÂÖ•‰∫ÜÂü∫‰∫éGroup Relative Policy Optimization (GRPO)ÁöÑÊéíÂ∫èÊçüÂ§±ÂáΩÊï∞$GRPO_{rank}$ÔºåËØ•ÂáΩÊï∞Áõ¥Êé•‰ºòÂåñÂÖ∑ÊúâÊéíÂ∫èÊÑüÁü•‰ºòÂäøÁöÑÂ∫èÊï∞ÂèçÈ¶à„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®ÂêÑÁßçËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÔºåOracle-RLAIFÂßãÁªà‰ºò‰∫é‰ΩøÁî®Áé∞ÊúâÂæÆË∞ÉÊñπÊ≥ïÁöÑÈ¢ÜÂÖàVLM„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËßÜÈ¢ëËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ‰æùËµñ‰∫éÂ§ßÈáèÁöÑÊúâÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÁâπÂà´ÊòØ‰ªé‰∫∫Á±ªÂèçÈ¶à‰∏≠ËøõË°åÂº∫ÂåñÂ≠¶‰π†ÔºàRLHFÔºâ„ÄÇÁÑ∂ËÄåÔºåÈöèÁùÄÊ®°ÂûãËßÑÊ®°ÁöÑÊâ©Â§ßÔºåËé∑ÂèñË∂≥Â§üÁöÑ‰∫∫Â∑•ÂèçÈ¶àÂèòÂæóÈùûÂ∏∏ÊòÇË¥µ„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éAIÂèçÈ¶àÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLAIFÔºâÊñπÊ≥ï‰æùËµñ‰∫é‰∏Ä‰∏™‰∏ìÈó®ËÆ≠ÁªÉÁöÑÂ•ñÂä±Ê®°ÂûãÔºåËØ•Ê®°ÂûãÈúÄË¶ÅÂ§ßÈáèÁöÑËßÜÈ¢ëÂèôËø∞Êï∞ÊçÆÔºåÈôêÂà∂‰∫ÜÂÖ∂ÈÄöÁî®ÊÄßÂíåÊàêÊú¨ÊïàÁõä„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰Ωï‰ª•Êõ¥ÁªèÊµéÈ´òÊïàÁöÑÊñπÂºèÂØπÂ§ßÂûãÂ§öÊ®°ÊÄÅËßÜÈ¢ëÊ®°ÂûãËøõË°åÂæÆË∞ÉÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöOracle-RLAIFÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁî®‰∏Ä‰∏™Êõ¥ÈÄöÁî®ÁöÑOracleÊéíÂ∫èÂô®Âèñ‰ª£‰∏ìÈó®ËÆ≠ÁªÉÁöÑÂ•ñÂä±Ê®°Âûã„ÄÇËøô‰∏™OracleÊéíÂ∫èÂô®Áõ¥Êé•ÂØπÂÄôÈÄâÊ®°ÂûãÂìçÂ∫îËøõË°åÊéíÂ∫èÔºåËÄå‰∏çÊòØÂÉèÂ•ñÂä±Ê®°ÂûãÈÇ£Ê†∑ËæìÂá∫Ê†áÈáèÂ•ñÂä±„ÄÇÈÄöËøáÁõ¥Êé•‰ºòÂåñÊéíÂ∫èÁªìÊûúÔºåÂèØ‰ª•ÈÅøÂÖçËÆ≠ÁªÉÂ§çÊùÇÁöÑÂ•ñÂä±Ê®°ÂûãÔºå‰ªéËÄåÈôç‰ΩéÊàêÊú¨Âπ∂ÊèêÈ´òÊïàÁéá„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫‰∫éÊéíÂ∫èÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•Êõ¥Â•ΩÂú∞Âà©Áî®ÊéíÂ∫è‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöOracle-RLAIFÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ‰ΩøÁî®VLMÁîüÊàêÂ§ö‰∏™ÂÄôÈÄâÂìçÂ∫îÔºõ2) ‰ΩøÁî®OracleÊéíÂ∫èÂô®ÂØπËøô‰∫õÂìçÂ∫îËøõË°åÊéíÂ∫èÔºõ3) ‰ΩøÁî®Âü∫‰∫éÊéíÂ∫èÁöÑÊçüÂ§±ÂáΩÊï∞Ôºà$GRPO_{rank}$ÔºâÂØπVLMËøõË°åÂæÆË∞É„ÄÇOracleÊéíÂ∫èÂô®ÂèØ‰ª•ÊòØ‰ªª‰ΩïËÉΩÂ§üÂØπËßÜÈ¢ëÁêÜËß£‰ªªÂä°ËøõË°åÊéíÂ∫èÁöÑÊ®°ÂûãÔºå‰æãÂ¶ÇÈ¢ÑËÆ≠ÁªÉÁöÑVLMÊàñ‰∏ìÈó®ËÆ≠ÁªÉÁöÑÊéíÂ∫èÊ®°Âûã„ÄÇ$GRPO_{rank}$ÊçüÂ§±ÂáΩÊï∞Âü∫‰∫éGroup Relative Policy Optimization (GRPO)ÔºåÂπ∂ÈíàÂØπÊéíÂ∫èÂèçÈ¶àËøõË°å‰∫Ü‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöOracle-RLAIFÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ΩøÁî®OracleÊéíÂ∫èÂô®Êõø‰ª£Â•ñÂä±Ê®°ÂûãÔºå‰ª•ÂèäÂºïÂÖ•‰∫ÜÂü∫‰∫éÊéíÂ∫èÁöÑÊçüÂ§±ÂáΩÊï∞$GRPO_{rank}$„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåOracle-RLAIF‰∏çÈúÄË¶ÅËÆ≠ÁªÉ‰∏ìÈó®ÁöÑÂ•ñÂä±Ê®°ÂûãÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜÊàêÊú¨ÂíåÂ§çÊùÇÊÄß„ÄÇ$GRPO_{rank}$ÊçüÂ§±ÂáΩÊï∞ËÉΩÂ§üÁõ¥Êé•‰ºòÂåñÊéíÂ∫èÂèçÈ¶àÔºåÊõ¥Â•ΩÂú∞Âà©Áî®‰∫ÜÊéíÂ∫è‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂæÆË∞ÉÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö$GRPO_{rank}$ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆ„ÄÇÂÆÉÂü∫‰∫éGRPOÔºå‰ΩÜÈíàÂØπÊéíÂ∫èÂèçÈ¶àËøõË°å‰∫Ü‰øÆÊîπÔºå‰ª•ËÄÉËôëÊéíÂ∫èÁöÑÁõ∏ÂØπÂÖ≥Á≥ª„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉËÆ°ÁÆó‰∫ÜÊØè‰∏™ÂìçÂ∫îÁöÑÊéíÂ∫èÊÑüÁü•‰ºòÂäøÔºåÂπ∂‰ΩøÁî®Ëøô‰∫õ‰ºòÂäøÊù•Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞„ÄÇOracleÊéíÂ∫èÂô®ÁöÑÈÄâÊã©‰πüÂæàÈáçË¶ÅÔºåÈúÄË¶ÅÈÄâÊã©‰∏Ä‰∏™ËÉΩÂ§üÂáÜÁ°ÆÂèçÊò†ËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÂÄôÈÄâÂìçÂ∫îÁöÑÁîüÊàêÁ≠ñÁï•‰πü‰ºöÂΩ±ÂìçÊúÄÁªàÁöÑÂæÆË∞ÉÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOracle-RLAIFÂú®Â§ö‰∏™ËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éÁé∞ÊúâÁöÑÂæÆË∞ÉÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Áü•Ôºå‰ΩÜËÆ∫ÊñáÂº∫Ë∞ÉOracle-RLAIFÂú®ÂêÑÁßçËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÂßãÁªà‰ºò‰∫é‰ΩøÁî®Áé∞ÊúâÂæÆË∞ÉÊñπÊ≥ïÁöÑÈ¢ÜÂÖàVLM„ÄÇËøôË°®ÊòéOracle-RLAIFÊòØ‰∏ÄÁßçÊúâÊïà‰∏îÈ´òÊïàÁöÑËßÜÈ¢ëÊ®°ÂûãÂæÆË∞ÉÊ°ÜÊû∂„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Oracle-RLAIFÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËßÜÈ¢ëÁêÜËß£ÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜÈ¢ëÈóÆÁ≠î„ÄÅËßÜÈ¢ëÂ≠óÂπïÁîüÊàê„ÄÅËßÜÈ¢ëÊ£ÄÁ¥¢Á≠â„ÄÇËØ•Ê°ÜÊû∂ËÉΩÂ§üÈôç‰ΩéÊ®°ÂûãÂæÆË∞ÉÁöÑÊàêÊú¨ÔºåÊèêÈ´òÊïàÁéáÔºå‰øÉËøõÂ§ßÂûãËßÜÈ¢ëËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅ‰ªªÂä°ÂíåÊ®°Âûã„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in large video-language models (VLMs) rely on extensive fine-tuning techniques that strengthen alignment between textual and visual comprehension. Leading pipelines typically pair supervised fine-tuning (SFT) with reinforcement learning from preference data to enhance video comprehension. However, as VLMs scale in parameter size, so does the cost of gathering enough human feedback. To make fine-tuning more cost-effective, recent frameworks explore reinforcement learning with AI feedback (RLAIF), which replace human preference with AI as a judge. Current RLAIF frameworks rely on a specialized reward model trained with video narratives to create calibrated scalar rewards -- an expensive and restrictive pipeline. We propose Oracle-RLAIF, a novel framework that replaces the trained reward model with a more general Oracle ranker which acts as a drop-in model ranking candidate model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce $GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware advantages. Empirically, we demonstrate that Oracle-RLAIF consistently outperforms leading VLMs using existing fine-tuning methods when evaluated across various video comprehension benchmarks. Oracle-RLAIF paves the path to creating flexible and data-efficient frameworks for aligning large multi-modal video models with reinforcement learning from rank rather than score.

