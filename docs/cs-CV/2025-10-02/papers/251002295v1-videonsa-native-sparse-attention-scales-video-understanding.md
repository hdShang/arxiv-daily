---
layout: default
title: "VideoNSA: Native Sparse Attention Scales Video Understanding"
---

# VideoNSA: Native Sparse Attention Scales Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02295" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02295v1</a>
  <a href="https://arxiv.org/pdf/2510.02295.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02295v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02295v1', 'VideoNSA: Native Sparse Attention Scales Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Enxin Song, Wenhao Chai, Shusheng Yang, Ethan Armand, Xiaojun Shan, Haiyang Xu, Jianwen Xie, Zhuowen Tu

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: Project Page: https://enxinsong.com/VideoNSA-web/, Code: https://github.com/Espere-1119-Song/VideoNSA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoNSAï¼Œé€šè¿‡åŸç”Ÿç¨€ç–æ³¨æ„åŠ›æœ‰æ•ˆæ‰©å±•è§†é¢‘ç†è§£æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `ç¨€ç–æ³¨æ„åŠ›` `è§†é¢‘è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æ—¶é—´æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç†è§£æ¨¡å‹å—é™äºä¸Šä¸‹æ–‡é•¿åº¦ï¼Œéš¾ä»¥æ•æ‰å…³é”®å¸§è½¬æ¢å’Œç»´æŒé•¿æ—¶é—´è¿è´¯æ€§ã€‚
2. VideoNSAå°†åŸç”Ÿç¨€ç–æ³¨æ„åŠ›åº”ç”¨äºè§†é¢‘ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒï¼Œæå‡æ¨¡å‹å¤„ç†é•¿è§†é¢‘çš„èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVideoNSAåœ¨é•¿è§†é¢‘ç†è§£ã€æ—¶é—´æ¨ç†å’Œç©ºé—´åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä¸­å—é™äºä¸Šä¸‹æ–‡é•¿åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†VideoNSAï¼Œå°†åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰åº”ç”¨äºè§†é¢‘è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä¸€ä¸ªåŒ…å«216Kè§†é¢‘æŒ‡ä»¤çš„æ•°æ®é›†ä¸Šè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œé€‚é…äº†Qwen2.5-VLæ¨¡å‹ã€‚VideoNSAé‡‡ç”¨äº†ä¸€ç§ç¡¬ä»¶æ„ŸçŸ¥çš„æ··åˆæ³¨æ„åŠ›æ–¹æ³•ï¼Œå¯¹æ–‡æœ¬ä¿ç•™å¯†é›†æ³¨æ„åŠ›ï¼Œè€Œå¯¹è§†é¢‘é‡‡ç”¨NSAã€‚ä¸tokenå‹ç¼©å’Œå…è®­ç»ƒçš„ç¨€ç–åŸºçº¿ç›¸æ¯”ï¼ŒVideoNSAåœ¨é•¿è§†é¢‘ç†è§£ã€æ—¶é—´æ¨ç†å’Œç©ºé—´åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„æ¶ˆèåˆ†ææ­ç¤ºäº†å››ä¸ªå…³é”®å‘ç°ï¼šï¼ˆ1ï¼‰å¯é åœ°æ‰©å±•åˆ°128K tokensï¼›ï¼ˆ2ï¼‰åœ¨å›ºå®šé¢„ç®—ä¸‹ï¼Œå…¨å±€-å±€éƒ¨æ³¨æ„åŠ›çš„æœ€ä½³åˆ†é…ï¼›ï¼ˆ3ï¼‰ä»»åŠ¡ç›¸å…³çš„åˆ†æ”¯ä½¿ç”¨æ¨¡å¼ï¼›ï¼ˆ4ï¼‰å¯å­¦ä¹ çš„ç»„åˆç¨€ç–æ³¨æ„åŠ›æœ‰åŠ©äºè¯±å¯¼åŠ¨æ€æ³¨æ„åŠ›æ±‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†é¢‘ç†è§£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„æ¨¡å‹ï¼Œåœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´è®¡ç®—å¤æ‚åº¦é«˜å’Œå†…å­˜æ¶ˆè€—å¤§çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„å¯†é›†æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—é‡éšåºåˆ—é•¿åº¦å‘ˆå¹³æ–¹å¢é•¿ï¼Œä½¿å¾—æ¨¡å‹éš¾ä»¥æ‰©å±•åˆ°å¤„ç†æ›´é•¿çš„è§†é¢‘åºåˆ—ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ä¹Ÿéš¾ä»¥æ•æ‰è§†é¢‘ä¸­çš„å…³é”®å¸§è½¬æ¢å’Œç»´æŒé•¿æ—¶é—´çš„è¿è´¯æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVideoNSAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰å¼•å…¥åˆ°è§†é¢‘ç†è§£æ¨¡å‹ä¸­ï¼Œåˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è§†é¢‘åºåˆ—ã€‚é€šè¿‡å­¦ä¹ ç¨€ç–æ¨¡å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°å…³æ³¨è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œæé«˜é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„æ··åˆæ³¨æ„åŠ›æ–¹æ³•ï¼Œå¯¹æ–‡æœ¬ä¿ç•™å¯†é›†æ³¨æ„åŠ›ï¼Œå¯¹è§†é¢‘é‡‡ç”¨NSAï¼Œå…¼é¡¾äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVideoNSAåŸºäºQwen2.5-VLæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†é€‚é…ã€‚æ•´ä½“æ¡†æ¶åŒ…æ‹¬è§†é¢‘ç¼–ç å™¨ã€æ–‡æœ¬ç¼–ç å™¨å’Œè·¨æ¨¡æ€äº¤äº’æ¨¡å—ã€‚è§†é¢‘ç¼–ç å™¨è´Ÿè´£å°†è§†é¢‘å¸§è½¬æ¢ä¸ºè§†è§‰ç‰¹å¾ï¼Œæ–‡æœ¬ç¼–ç å™¨è´Ÿè´£å°†æ–‡æœ¬æŒ‡ä»¤è½¬æ¢ä¸ºæ–‡æœ¬ç‰¹å¾ã€‚è·¨æ¨¡æ€äº¤äº’æ¨¡å—åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œèåˆï¼Œä»è€Œå®ç°è§†é¢‘ç†è§£ã€‚æ¨¡å‹é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒçš„æ–¹å¼ï¼Œåœ¨ä¸€ä¸ªåŒ…å«216Kè§†é¢‘æŒ‡ä»¤çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šVideoNSAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰æˆåŠŸåº”ç”¨äºè§†é¢‘ç†è§£ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„å¯†é›†æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒNSAé€šè¿‡å­¦ä¹ ç¨€ç–æ¨¡å¼ï¼Œåªå…³æ³¨è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚æ­¤å¤–ï¼ŒVideoNSAè¿˜é‡‡ç”¨äº†ä¸€ç§ç¡¬ä»¶æ„ŸçŸ¥çš„æ··åˆæ³¨æ„åŠ›æ–¹æ³•ï¼Œå¯¹æ–‡æœ¬ä¿ç•™å¯†é›†æ³¨æ„åŠ›ï¼Œå¯¹è§†é¢‘é‡‡ç”¨NSAï¼Œå…¼é¡¾äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šVideoNSAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼çš„å­¦ä¹ ï¼Œé€šè¿‡å¯å­¦ä¹ çš„å‚æ•°æ¥æ§åˆ¶ç¨€ç–æ¨¡å¼ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ åˆ°æœ€ä¼˜çš„ç¨€ç–æ¨¡å¼ï¼›(2) å…¨å±€-å±€éƒ¨æ³¨æ„åŠ›åˆ†é…ï¼Œé€šè¿‡è°ƒæ•´å…¨å±€æ³¨æ„åŠ›å’Œå±€éƒ¨æ³¨æ„åŠ›çš„æ¯”ä¾‹ï¼Œæ¥å¹³è¡¡æ¨¡å‹çš„å…¨å±€ç†è§£èƒ½åŠ›å’Œå±€éƒ¨ç»†èŠ‚æ•æ‰èƒ½åŠ›ï¼›(3) ç¡¬ä»¶æ„ŸçŸ¥çš„æ··åˆæ³¨æ„åŠ›æ–¹æ³•ï¼Œæ ¹æ®ç¡¬ä»¶çš„ç‰¹æ€§ï¼Œé€‰æ‹©åˆé€‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VideoNSAåœ¨é•¿è§†é¢‘ç†è§£ã€æ—¶é—´æ¨ç†å’Œç©ºé—´åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoNSAèƒ½å¤Ÿå¯é åœ°æ‰©å±•åˆ°128K tokensï¼Œå¹¶ä¸”åœ¨å›ºå®šé¢„ç®—ä¸‹ï¼Œå…¨å±€-å±€éƒ¨æ³¨æ„åŠ›çš„æœ€ä½³åˆ†é…èƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œä»»åŠ¡ç›¸å…³çš„åˆ†æ”¯ä½¿ç”¨æ¨¡å¼å’Œå¯å­¦ä¹ çš„ç»„åˆç¨€ç–æ³¨æ„åŠ›æœ‰åŠ©äºè¯±å¯¼åŠ¨æ€æ³¨æ„åŠ›æ±‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VideoNSAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ã€åœ¨çº¿æ•™è‚²ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºåˆ†æç›‘æ§è§†é¢‘ä¸­çš„å¼‚å¸¸è¡Œä¸ºï¼Œå¸®åŠ©è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿç†è§£å‘¨å›´ç¯å¢ƒï¼Œå®ç°æ™ºèƒ½å®¶å±…è®¾å¤‡çš„æ™ºèƒ½åŒ–æ§åˆ¶ï¼Œä»¥åŠæä¾›æ›´ä¸ªæ€§åŒ–çš„åœ¨çº¿æ•™è‚²æœåŠ¡ã€‚è¯¥ç ”ç©¶çš„æˆæœæœ‰åŠ©äºæ¨åŠ¨è§†é¢‘ç†è§£æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶ä¸ºç›¸å…³åº”ç”¨æä¾›æ›´å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.

