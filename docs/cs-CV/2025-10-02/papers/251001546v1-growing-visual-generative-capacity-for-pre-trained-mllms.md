---
layout: default
title: Growing Visual Generative Capacity for Pre-Trained MLLMs
---

# Growing Visual Generative Capacity for Pre-Trained MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01546" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01546v1</a>
  <a href="https://arxiv.org/pdf/2510.01546.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01546v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01546v1', 'Growing Visual Generative Capacity for Pre-Trained MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanyu Wang, Jiaming Han, Ziyan Yang, Qi Zhao, Shanchuan Lin, Xiangyu Yue, Abhinav Shrivastava, Zhenheng Yang, Hao Chen

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: Project page: https://hywang66.github.io/bridge/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBridgeï¼šä¸€ç§åŸºäºæ··åˆTransformeræ¶æ„çš„çº¯è‡ªå›å½’ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡è§†è§‰ç”Ÿæˆèƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è‡ªå›å½’æ¨¡å‹` `å›¾åƒç”Ÿæˆ` `è§†è§‰ç†è§£` `æ··åˆTransformer` `è¯­ä¹‰åˆ°åƒç´ ` `next-tokené¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨è§†è§‰ç”Ÿæˆæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œæ··åˆæ–¹æ³•æ‰“ç ´è‡ªå›å½’èŒƒå¼ï¼Œçº¯è‡ªå›å½’æ–¹æ³•åˆ™åœ¨è¯­ä¹‰å¯¹é½å’Œåƒç´ ä¿çœŸåº¦é—´æƒè¡¡ã€‚
2. Bridgeé€šè¿‡æ··åˆTransformeræ¶æ„ï¼Œåœ¨çº¯è‡ªå›å½’æ¡†æ¶ä¸‹ï¼Œå¢å¼ºé¢„è®­ç»ƒè§†è§‰ç†è§£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°ç»Ÿä¸€çš„å›¾åƒç†è§£å’Œç”Ÿæˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBridgeåœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”è®­ç»ƒæ‰€éœ€æ•°æ®æ›´å°‘ï¼Œæ—¶é—´æ›´çŸ­ï¼Œæ˜¾è‘—æå‡äº†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)å°†è¯­è¨€æ¨¡å‹çš„æˆåŠŸæ‰©å±•åˆ°è§†è§‰ç†è§£é¢†åŸŸã€‚ç›®å‰çš„ç ”ç©¶è‡´åŠ›äºæ„å»ºç»Ÿä¸€çš„MLLMï¼Œä»¥æ”¯æŒç†è§£å’Œç”Ÿæˆã€‚ç„¶è€Œï¼Œæ„å»ºæ­¤ç±»æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼šæ··åˆæ–¹æ³•å°†è¿ç»­åµŒå…¥ä¸æ‰©æ•£æˆ–åŸºäºæµçš„ç›®æ ‡ç›¸ç»“åˆï¼Œäº§ç”Ÿé«˜è´¨é‡çš„å›¾åƒï¼Œä½†æ‰“ç ´äº†è‡ªå›å½’èŒƒå¼ï¼›è€Œçº¯è‡ªå›å½’æ–¹æ³•ç»Ÿä¸€äº†æ–‡æœ¬å’Œå›¾åƒé¢„æµ‹ï¼Œé€šè¿‡ç¦»æ•£è§†è§‰tokensï¼Œä½†é€šå¸¸é¢ä¸´è¯­ä¹‰å¯¹é½å’Œåƒç´ çº§ä¿çœŸåº¦ä¹‹é—´çš„æƒè¡¡ã€‚æœ¬æ–‡æå‡ºBridgeï¼Œä¸€ç§çº¯è‡ªå›å½’ç»Ÿä¸€MLLMï¼Œé€šè¿‡æ··åˆTransformeræ¶æ„å¢å¼ºé¢„è®­ç»ƒè§†è§‰ç†è§£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»è€Œåœ¨å•ä¸ªnext-tokené¢„æµ‹æ¡†æ¶å†…å®ç°å›¾åƒç†è§£å’Œç”Ÿæˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è§†è§‰ç”Ÿæˆä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰åˆ°åƒç´ çš„ç¦»æ•£è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºå°†ç´§å‡‘çš„è¯­ä¹‰tokensä¸ç»†ç²’åº¦çš„åƒç´ tokensé›†æˆåœ¨ä¸€èµ·ï¼Œä»…ä»¥7.9%çš„åºåˆ—é•¿åº¦å¢åŠ å®ç°äº†å¼ºå¤§çš„è¯­è¨€å¯¹é½å’Œè§†è§‰ç»†èŠ‚çš„ç²¾ç¡®æè¿°ã€‚åœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„ç»Ÿä¸€MLLMç›¸æ¯”ï¼ŒBridgeåœ¨ç†è§£å’Œç”ŸæˆåŸºå‡†ä¸Šéƒ½å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æˆ–æ›´ä¼˜è¶Šçš„ç»“æœï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„è®­ç»ƒæ•°æ®å’Œæ›´çŸ­çš„è®­ç»ƒæ—¶é—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMæ¨¡å‹åœ¨ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æ··åˆæ–¹æ³•è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†ç ´åäº†è‡ªå›å½’ç‰¹æ€§ã€‚çº¯è‡ªå›å½’æ–¹æ³•è™½ç„¶ç»Ÿä¸€äº†æ–‡æœ¬å’Œå›¾åƒé¢„æµ‹ï¼Œä½†åœ¨è¯­ä¹‰å¯¹é½å’Œåƒç´ çº§ä¿çœŸåº¦ä¹‹é—´å­˜åœ¨trade-offã€‚å› æ­¤ï¼Œå¦‚ä½•æ„å»ºä¸€ä¸ªæ—¢èƒ½ç†è§£åˆèƒ½ç”Ÿæˆï¼Œä¸”ä¿æŒè‡ªå›å½’ç‰¹æ€§çš„MLLMæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBridgeçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ··åˆTransformeræ¶æ„ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰ç†è§£æ¨¡å‹ä¸ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œä»è€Œåœ¨çº¯è‡ªå›å½’æ¡†æ¶ä¸‹å®ç°å›¾åƒç†è§£å’Œç”Ÿæˆã€‚é€šè¿‡next-tokené¢„æµ‹çš„æ–¹å¼ï¼Œç»Ÿä¸€å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œé¿å…äº†æ··åˆæ–¹æ³•å¸¦æ¥çš„è‡ªå›å½’é—®é¢˜ï¼ŒåŒæ—¶é€šè¿‡è¯­ä¹‰åˆ°åƒç´ çš„ç¦»æ•£è¡¨ç¤ºï¼Œæå‡ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBridgeçš„æ•´ä½“æ¶æ„æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„è‡ªå›å½’æ¨¡å‹ã€‚å®ƒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) é¢„è®­ç»ƒçš„è§†è§‰ç†è§£æ¨¡å‹ï¼Œç”¨äºæå–å›¾åƒçš„è¯­ä¹‰ç‰¹å¾ï¼›2) æ··åˆTransformeræ¨¡å—ï¼Œç”¨äºèåˆè§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œnext-tokené¢„æµ‹ï¼›3) è¯­ä¹‰åˆ°åƒç´ çš„ç¦»æ•£è¡¨ç¤ºæ¨¡å—ï¼Œç”¨äºå°†è¯­ä¹‰tokensè½¬æ¢ä¸ºç»†ç²’åº¦çš„åƒç´ tokensï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šè¾“å…¥æ–‡æœ¬å’Œå›¾åƒï¼Œè§†è§‰ç†è§£æ¨¡å‹æå–å›¾åƒç‰¹å¾ï¼Œæ··åˆTransformerèåˆç‰¹å¾å¹¶é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œè¯­ä¹‰åˆ°åƒç´ æ¨¡å—å°†è¯­ä¹‰tokenè½¬æ¢ä¸ºåƒç´ tokenï¼Œæœ€ç»ˆç”Ÿæˆå›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šBridgeçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹ä¸¤ç‚¹ï¼š1) æ··åˆTransformeræ¶æ„ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨çº¯è‡ªå›å½’æ¡†æ¶ä¸‹åŒæ—¶è¿›è¡Œç†è§£å’Œç”Ÿæˆï¼›2) è¯­ä¹‰åˆ°åƒç´ çš„ç¦»æ•£è¡¨ç¤ºï¼Œå®ƒé€šè¿‡å°†ç´§å‡‘çš„è¯­ä¹‰tokensä¸ç»†ç²’åº¦çš„åƒç´ tokensç›¸ç»“åˆï¼Œå®ç°äº†å¼ºå¤§çš„è¯­è¨€å¯¹é½å’Œè§†è§‰ç»†èŠ‚çš„ç²¾ç¡®æè¿°ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒBridgeé¿å…äº†æ··åˆæ–¹æ³•å¸¦æ¥çš„è‡ªå›å½’é—®é¢˜ï¼ŒåŒæ—¶æå‡äº†ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šBridgeçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ··åˆTransformeræ¨¡å—çš„å…·ä½“ç»“æ„ï¼Œä¾‹å¦‚Transformerå±‚æ•°ã€æ³¨æ„åŠ›æœºåˆ¶çš„é€‰æ‹©ç­‰ï¼›2) è¯­ä¹‰åˆ°åƒç´ çš„ç¦»æ•£è¡¨ç¤ºçš„å…·ä½“å®ç°æ–¹å¼ï¼Œä¾‹å¦‚è¯­ä¹‰tokenå’Œåƒç´ tokençš„æ•°é‡ã€ç¼–ç æ–¹å¼ç­‰ï¼›3) è®­ç»ƒç›®æ ‡å‡½æ•°çš„è®¾è®¡ï¼Œä¾‹å¦‚å¦‚ä½•å¹³è¡¡ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æŸå¤±ï¼Œå¦‚ä½•ä¼˜åŒ–è¯­ä¹‰åˆ°åƒç´ çš„è½¬æ¢è¿‡ç¨‹ç­‰ã€‚è®ºæ–‡ä¸­å¯èƒ½è¿˜æ¶‰åŠä¸€äº›è¶…å‚æ•°çš„è®¾ç½®ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€batch sizeç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBridgeåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ã€‚ä¸ä¹‹å‰çš„ç»Ÿä¸€MLLMç›¸æ¯”ï¼ŒBridgeåœ¨ç†è§£å’Œç”ŸæˆåŸºå‡†ä¸Šéƒ½å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æˆ–æ›´ä¼˜è¶Šçš„ç»“æœï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„è®­ç»ƒæ•°æ®å’Œæ›´çŸ­çš„è®­ç»ƒæ—¶é—´ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒBridgeç”Ÿæˆçš„å›¾åƒè´¨é‡æ˜æ˜¾ä¼˜äºå…¶ä»–è‡ªå›å½’æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è¯­ä¹‰å¯¹é½æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Bridgeå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å›¾åƒç¼–è¾‘ã€å›¾åƒç”Ÿæˆã€è§†è§‰å¯¹è¯ã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ›´é€¼çœŸã€æ›´å¯æ§çš„è™šæ‹Ÿä¸–ç•Œï¼Œä¹Ÿå¯ä»¥ç”¨äºè¾…åŠ©äººç±»è¿›è¡Œè®¾è®¡å’Œåˆ›ä½œã€‚æ­¤å¤–ï¼ŒBridgeè¿˜å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œæå‡äººæœºäº¤äº’çš„æ•ˆç‡å’Œè´¨é‡ã€‚æœªæ¥ï¼ŒBridgeæœ‰æœ›æˆä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦åŸºçŸ³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) extend the success of language models to visual understanding, and recent efforts have sought to build unified MLLMs that support both understanding and generation. However, constructing such models remains challenging: hybrid approaches combine continuous embeddings with diffusion or flow-based objectives, producing high-quality images but breaking the autoregressive paradigm, while pure autoregressive approaches unify text and image prediction over discrete visual tokens but often face trade-offs between semantic alignment and pixel-level fidelity. In this work, we present Bridge, a pure autoregressive unified MLLM that augments pre-trained visual understanding models with generative ability through a Mixture-of-Transformers architecture, enabling both image understanding and generation within a single next-token prediction framework. To further improve visual generation fidelity, we propose a semantic-to-pixel discrete representation that integrates compact semantic tokens with fine-grained pixel tokens, achieving strong language alignment and precise description of visual details with only a 7.9% increase in sequence length. Extensive experiments across diverse multimodal benchmarks demonstrate that Bridge achieves competitive or superior results in both understanding and generation benchmarks, while requiring less training data and reduced training time compared to prior unified MLLMs.

