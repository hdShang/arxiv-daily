---
layout: default
title: Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions
---

# Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02313" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02313v1</a>
  <a href="https://arxiv.org/pdf/2510.02313.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02313v1" onclick="toggleFavorite(this, '2510.02313v1', 'Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengyu Yang, Yiming Chen, Haozheng Pei, Siddhant Agarwal, Arun Balajee Vasudevan, James Hays

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-02

**å¤‡æ³¨**: ICCV 2025. Project page: https://clink-chop-thud.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºçœŸå®ä¸–ç•Œäº¤äº’å­¦ä¹ ç‰©ä½“å£°éŸ³çš„æ£€æµ‹æ¡†æ¶ï¼Œè§£å†³å£°éŸ³ä¸ç‰©ä½“çš„å…³è”é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‘å£°ç‰©ä½“æ£€æµ‹` `å¤šæ¨¡æ€å­¦ä¹ ` `ç‰©ä½“åˆ†å‰²` `Slot Attention` `å£°éŸ³è¯†åˆ«` `è§†é¢‘ç†è§£` `æœºå™¨äººå¬è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥åŒºåˆ†ç»†å¾®çš„ç‰©ä½“äº¤äº’å£°éŸ³ï¼Œä¾‹å¦‚å‹ºå­æ•²å‡»ä¸åŒæè´¨åœ°é¢äº§ç”Ÿçš„å£°éŸ³å·®å¼‚ã€‚
2. è®ºæ–‡æå‡ºå¤šæ¨¡æ€ç‰©ä½“æ„ŸçŸ¥æ¡†æ¶ï¼Œåˆ©ç”¨ç‰©ä½“åˆ†å‰²æ©ç å’ŒSlot Attentionæœºåˆ¶ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨å…³é”®ç‰©ä½“åŒºåŸŸã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‘å£°ç‰©ä½“æ£€æµ‹ä»»åŠ¡å’Œå¤šæ¨¡æ€åŠ¨ä½œç†è§£ä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼šå‘å£°ç‰©ä½“æ£€æµ‹ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹å°†å£°éŸ³ä¸å…¶ç›´æ¥ç›¸å…³çš„ç‰©ä½“è”ç³»èµ·æ¥çš„èƒ½åŠ›ã€‚å—äººç±»æ„ŸçŸ¥çš„å¯å‘ï¼Œè¯¥å¤šæ¨¡æ€ã€ç‰©ä½“æ„ŸçŸ¥çš„æ¡†æ¶ä»çœŸå®åœºæ™¯çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­å­¦ä¹ ã€‚ä¸ºäº†é¼“åŠ±ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œé¦–å…ˆå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨æµç¨‹æ¥è®¡ç®—æ‰€æ¶‰åŠç‰©ä½“çš„åˆ†å‰²æ©ç ï¼Œä»¥å¼•å¯¼æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´å…³æ³¨äº¤äº’ä¸­æœ€å…·ä¿¡æ¯çš„åŒºåŸŸã€‚ä½¿ç”¨Slot Attentionè§†è§‰ç¼–ç å™¨è¿›ä¸€æ­¥åŠ å¼ºç‰©ä½“å…ˆéªŒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–°ä»»åŠ¡ä»¥åŠç°æœ‰çš„å¤šæ¨¡æ€åŠ¨ä½œç†è§£ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å‘å£°ç‰©ä½“æ£€æµ‹é—®é¢˜ï¼Œå³è¯†åˆ«è§†é¢‘ä¸­å‘å‡ºç‰¹å®šå£°éŸ³çš„ç‰©ä½“ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†çœŸå®ä¸–ç•Œå¤æ‚åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯åŒºåˆ†ç›¸ä¼¼ç‰©ä½“æˆ–ç»†å¾®å£°éŸ³å·®å¼‚æ—¶è¡¨ç°ä¸è¶³ï¼Œç¼ºä¹å¯¹ç‰©ä½“æœ¬èº«ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆè§†è§‰å’Œå¬è§‰ï¼‰ä»¥åŠç‰©ä½“åˆ†å‰²ä¿¡æ¯ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ å£°éŸ³å’Œç‰©ä½“ä¹‹é—´çš„å…³è”ã€‚é€šè¿‡å…³æ³¨è§†é¢‘ä¸­ä¸å£°éŸ³äº§ç”Ÿç›´æ¥ç›¸å…³çš„ç‰©ä½“ï¼Œæé«˜æ¨¡å‹å¯¹å£°éŸ³æ¥æºçš„å®šä½èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è‡ªåŠ¨ç‰©ä½“åˆ†å‰²æµç¨‹ï¼Œç”¨äºç”Ÿæˆè§†é¢‘ä¸­ç‰©ä½“çš„åˆ†å‰²æ©ç ï¼›2) Slot Attentionè§†è§‰ç¼–ç å™¨ï¼Œç”¨äºæå–ç‰©ä½“çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼›3) éŸ³é¢‘ç¼–ç å™¨ï¼Œç”¨äºæå–éŸ³é¢‘ç‰¹å¾ï¼›4) å¤šæ¨¡æ€èåˆæ¨¡å—ï¼Œå°†è§†è§‰å’Œå¬è§‰ç‰¹å¾è¿›è¡Œèåˆï¼›5) å‘å£°ç‰©ä½“æ£€æµ‹æ¨¡å—ï¼Œé¢„æµ‹ä¸å£°éŸ³ç›¸å…³çš„ç‰©ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†å‘å£°ç‰©ä½“æ£€æµ‹è¿™ä¸€æ–°ä»»åŠ¡ï¼›2) åˆ©ç”¨è‡ªåŠ¨ç‰©ä½“åˆ†å‰²æµç¨‹å’ŒSlot Attentionæœºåˆ¶ï¼Œæ˜¾å¼åœ°å¼•å…¥äº†ç‰©ä½“å…ˆéªŒï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨å…³é”®ç‰©ä½“åŒºåŸŸï¼›3) æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€ã€ç‰©ä½“æ„ŸçŸ¥çš„å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å£°éŸ³å’Œç‰©ä½“ä¹‹é—´çš„å…³è”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è§†è§‰ç¼–ç å™¨ä¸­ï¼Œä½¿ç”¨äº†Slot Attentionæœºåˆ¶æ¥æå–ç‰©ä½“çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œæ¯ä¸ªSlotå¯¹åº”ä¸€ä¸ªæ½œåœ¨çš„ç‰©ä½“ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬å‘å£°ç‰©ä½“æ£€æµ‹æŸå¤±å’Œè¾…åŠ©æŸå¤±ï¼Œç”¨äºæé«˜ç‰©ä½“åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨æå‡ºçš„å‘å£°ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰çš„å¤šæ¨¡æ€åŠ¨ä½œç†è§£ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†æœ‰è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¬è§‰æ„ŸçŸ¥ã€æ™ºèƒ½å®¶å±…ã€è§†é¢‘ç›‘æ§ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥é€šè¿‡è¯†åˆ«ç‰©ä½“å‘å‡ºçš„å£°éŸ³æ¥ç†è§£ç¯å¢ƒï¼Œå¹¶ä¸ç‰©ä½“è¿›è¡Œäº¤äº’ã€‚æ™ºèƒ½å®¶å±…ç³»ç»Ÿå¯ä»¥æ ¹æ®å£°éŸ³è¯†åˆ«ç”¨æˆ·è¡Œä¸ºï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–æœåŠ¡ã€‚è§†é¢‘ç›‘æ§ç³»ç»Ÿå¯ä»¥åˆ©ç”¨å£°éŸ³æ¥æ£€æµ‹å¼‚å¸¸äº‹ä»¶ï¼Œä¾‹å¦‚ç»ç’ƒç ´ç¢æˆ–æªå£°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Can a model distinguish between the sound of a spoon hitting a hardwood floor versus a carpeted one? Everyday object interactions produce sounds unique to the objects involved. We introduce the sounding object detection task to evaluate a model's ability to link these sounds to the objects directly involved. Inspired by human perception, our multimodal object-aware framework learns from in-the-wild egocentric videos. To encourage an object-centric approach, we first develop an automatic pipeline to compute segmentation masks of the objects involved to guide the model's focus during training towards the most informative regions of the interaction. A slot attention visual encoder is used to further enforce an object prior. We demonstrate state of the art performance on our new task along with existing multimodal action understanding tasks.

