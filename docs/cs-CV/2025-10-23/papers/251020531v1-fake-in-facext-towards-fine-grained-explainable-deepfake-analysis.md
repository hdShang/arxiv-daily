---
layout: default
title: Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis
---

# Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20531v1</a>
  <a href="https://arxiv.org/pdf/2510.20531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20531v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.20531v1', 'Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lixiong Qin, Yang Zhang, Mei Wang, Jiani Hu, Weihong Deng, Weiran Xu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23

**å¤‡æ³¨**: 25 pages, 9 figures, 17 tables

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/lxq1000/Fake-in-Facext)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFake-in-Facextæ¡†æ¶ï¼Œå®ç°ç»†ç²’åº¦ã€å¯è§£é‡Šçš„DeepFakeäººè„¸åˆ†æã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `DeepFakeåˆ†æ` `å¯è§£é‡Šæ€§AI` `å¤šæ¨¡æ€å­¦ä¹ ` `äººè„¸å›¾åƒå¤„ç†` `ä¼ªé€ ç—•è¿¹å®šä½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯è§£é‡ŠDeepFakeåˆ†ææ–¹æ³•ç¼ºä¹ç»†ç²’åº¦æ„ŸçŸ¥ï¼Œæ•°æ®æ ‡æ³¨ç²—ç³™ï¼Œæ— æ³•æœ‰æ•ˆè¿æ¥æ–‡æœ¬è§£é‡Šå’Œè§†è§‰è¯æ®ã€‚
2. æå‡ºFake-in-Facextæ¡†æ¶ï¼Œé€šè¿‡é¢éƒ¨å›¾åƒæ¦‚å¿µæ ‘(FICT)å®ç°ç»†ç²’åº¦æ•°æ®æ ‡æ³¨ï¼Œå¹¶å¼•å…¥ä¼ªé€ ç—•è¿¹å®šä½è§£é‡Š(AGE)ä»»åŠ¡ã€‚
3. FiFa-MLLMåœ¨AGEä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰åŸºçº¿ï¼Œå¹¶åœ¨XDFAæ•°æ®é›†ä¸Šå–å¾—SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)çš„å‘å±•å¼¥åˆäº†è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¹‹é—´çš„å·®è·ï¼Œä½¿å¾—å¯è§£é‡Šçš„DeepFakeåˆ†æ(XDFA)æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ç¼ºä¹ç»†ç²’åº¦æ„ŸçŸ¥ï¼šæ•°æ®æ ‡æ³¨ä¸­çš„ä¼ªé€ ç—•è¿¹æè¿°ä¸å¯é ä¸”ç²—ç³™ï¼Œæ¨¡å‹æ— æ³•æ”¯æŒæ–‡æœ¬ä¼ªé€ è§£é‡Šä¸ä¼ªé€ ç—•è¿¹è§†è§‰è¯æ®ä¹‹é—´çš„è¿æ¥è¾“å‡ºï¼Œä»¥åŠä»»æ„é¢éƒ¨åŒºåŸŸçš„æŸ¥è¯¢è¾“å…¥ã€‚å› æ­¤ï¼Œå®ƒä»¬çš„å“åº”æ²¡æœ‰å……åˆ†åŸºäºäººè„¸è§†è§‰ä¸Šä¸‹æ–‡(Facext)ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Fake-in-Facext (FiFa)æ¡†æ¶ï¼Œå…¶è´¡çŒ®é›†ä¸­åœ¨æ•°æ®æ ‡æ³¨å’Œæ¨¡å‹æ„å»ºä¸Šã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªé¢éƒ¨å›¾åƒæ¦‚å¿µæ ‘(FICT)å°†é¢éƒ¨å›¾åƒåˆ’åˆ†ä¸ºç»†ç²’åº¦çš„åŒºåŸŸæ¦‚å¿µï¼Œä»è€Œè·å¾—æ›´å¯é çš„ä¼ªé€ è§£é‡Šæ•°æ®æ ‡æ³¨æµç¨‹FiFa-Annotatorã€‚åŸºäºè¿™ç§ä¸“ç”¨æ•°æ®æ ‡æ³¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä¼ªé€ ç—•è¿¹å®šä½è§£é‡Š(AGE)ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡ç”Ÿæˆä¸ç¯¡æ”¹ä¼ªé€ ç—•è¿¹çš„åˆ†å‰²æ©ç äº¤ç»‡çš„æ–‡æœ¬ä¼ªé€ è§£é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡å­¦ä¹ æ¶æ„FiFa-MLLMï¼Œä»¥åŒæ—¶æ”¯æŒä¸°å¯Œçš„å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºï¼Œç”¨äºç»†ç²’åº¦çš„å¯è§£é‡ŠDeepFakeåˆ†æã€‚é€šè¿‡å¤šä¸ªè¾…åŠ©ç›‘ç£ä»»åŠ¡ï¼ŒFiFa-MLLMå¯ä»¥åœ¨AGEä»»åŠ¡ä¸Šä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œå¹¶åœ¨ç°æœ‰çš„XDFAæ•°æ®é›†ä¸Šå®ç°SOTAæ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨https://github.com/lxq1000/Fake-in-Facextå¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¯è§£é‡ŠDeepFakeåˆ†æ(XDFA)æ–¹æ³•åœ¨ç»†ç²’åº¦ä¸Šå­˜åœ¨ä¸è¶³ã€‚å…·ä½“æ¥è¯´ï¼Œæ•°æ®æ ‡æ³¨ä¸­å¯¹ä¼ªé€ ç—•è¿¹çš„æè¿°ä¸å¤Ÿç²¾ç¡®ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•å‡†ç¡®åœ°å°†æ–‡æœ¬è§£é‡Šä¸å¯¹åº”çš„è§†è§‰ä¼ªé€ åŒºåŸŸå…³è”èµ·æ¥ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ”¯æŒå¯¹ä»»æ„é¢éƒ¨åŒºåŸŸè¿›è¡ŒæŸ¥è¯¢ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚è¿™äº›é—®é¢˜å¯¼è‡´æ¨¡å‹è¾“å‡ºçš„è§£é‡Šä¸å¤Ÿå……åˆ†ï¼Œç¼ºä¹äººè„¸è§†è§‰ä¸Šä¸‹æ–‡çš„æ”¯æ’‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFiFaæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯æå‡XDFAçš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºé¢éƒ¨å›¾åƒæ¦‚å¿µæ ‘(FICT)ï¼Œå°†äººè„¸å›¾åƒåˆ†è§£ä¸ºæ›´ç»†è‡´çš„åŒºåŸŸæ¦‚å¿µï¼Œä»è€Œå®ç°æ›´å¯é çš„æ•°æ®æ ‡æ³¨ã€‚åŒæ—¶ï¼Œå¼•å…¥ä¼ªé€ ç—•è¿¹å®šä½è§£é‡Š(AGE)ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹ç”ŸæˆåŒ…å«åˆ†å‰²æ©ç çš„æ–‡æœ¬è§£é‡Šï¼Œä»è€Œå°†æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ç´§å¯†ç»“åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFiFaæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šFiFa-Annotatorå’ŒFiFa-MLLMã€‚FiFa-Annotatoræ˜¯ä¸€ä¸ªæ•°æ®æ ‡æ³¨æµç¨‹ï¼Œåˆ©ç”¨FICTè¿›è¡Œç»†ç²’åº¦æ ‡æ³¨ã€‚FiFa-MLLMæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡å­¦ä¹ æ¶æ„ï¼Œç”¨äºæ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºã€‚è¯¥æ¨¡å‹åŒæ—¶æ‰§è¡ŒAGEä»»åŠ¡ä»¥åŠå¤šä¸ªè¾…åŠ©ç›‘ç£ä»»åŠ¡ï¼Œä»¥æå‡æ€§èƒ½ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆä½¿ç”¨FiFa-Annotatoræ ‡æ³¨æ•°æ®ï¼Œç„¶åä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒFiFa-MLLMï¼Œæœ€åä½¿ç”¨è®­ç»ƒå¥½çš„FiFa-MLLMè¿›è¡Œç»†ç²’åº¦çš„DeepFakeåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼š1) æå‡ºäº†é¢éƒ¨å›¾åƒæ¦‚å¿µæ ‘(FICT)ï¼Œç”¨äºç»†ç²’åº¦çš„äººè„¸åŒºåŸŸåˆ’åˆ†ã€‚2) å¼•å…¥äº†ä¼ªé€ ç—•è¿¹å®šä½è§£é‡Š(AGE)ä»»åŠ¡ï¼Œå°†æ–‡æœ¬è§£é‡Šä¸è§†è§‰åˆ†å‰²æ©ç ç›¸ç»“åˆã€‚3) æ„å»ºäº†ç»Ÿä¸€çš„å¤šä»»åŠ¡å­¦ä¹ æ¶æ„FiFa-MLLMï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFiFaæ¡†æ¶èƒ½å¤Ÿæä¾›æ›´ç²¾ç¡®ã€æ›´å¯è§£é‡Šçš„DeepFakeåˆ†æç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ ‡æ³¨æ–¹é¢ï¼ŒFICTçš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå®ƒå†³å®šäº†æ ‡æ³¨çš„ç»†ç²’åº¦ç¨‹åº¦ã€‚åœ¨æ¨¡å‹æ–¹é¢ï¼ŒFiFa-MLLMé‡‡ç”¨äº†å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å¤šä¸ªè¾…åŠ©ä»»åŠ¡æ¥æå‡AGEä»»åŠ¡çš„æ€§èƒ½ã€‚å…·ä½“çš„è¾…åŠ©ä»»åŠ¡åŒ…æ‹¬ï¼šäººè„¸å±æ€§è¯†åˆ«ã€äººè„¸åŒºåŸŸåˆ†å‰²ç­‰ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿéœ€è¦ä»”ç»†è€ƒè™‘ï¼Œä»¥å¹³è¡¡ä¸åŒä»»åŠ¡ä¹‹é—´çš„æƒé‡ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒFiFa-MLLMå¯èƒ½é‡‡ç”¨äº†Transformeræ¶æ„ï¼Œä»¥æ›´å¥½åœ°å¤„ç†åºåˆ—æ•°æ®å’Œå¤šæ¨¡æ€ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFiFa-MLLMåœ¨AGEä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒFiFa-MLLMåœ¨ç°æœ‰çš„XDFAæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†SOTAæ€§èƒ½ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ•°å­—å–è¯ã€ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€èº«ä»½éªŒè¯ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›ç»†ç²’åº¦ã€å¯è§£é‡Šçš„DeepFakeåˆ†æï¼Œæœ‰åŠ©äºè¯†åˆ«å’Œæ­éœ²è™šå‡ä¿¡æ¯ï¼Œç»´æŠ¤ç½‘ç»œå®‰å…¨å’Œä¿¡æ¯çœŸå®æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯è¿›ä¸€æ­¥åº”ç”¨äºè§†é¢‘DeepFakeæ£€æµ‹ï¼Œä»¥åŠæ›´å¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ç¯¡æ”¹åˆ†æã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The advancement of Multimodal Large Language Models (MLLMs) has bridged the gap between vision and language tasks, enabling the implementation of Explainable DeepFake Analysis (XDFA). However, current methods suffer from a lack of fine-grained awareness: the description of artifacts in data annotation is unreliable and coarse-grained, and the models fail to support the output of connections between textual forgery explanations and the visual evidence of artifacts, as well as the input of queries for arbitrary facial regions. As a result, their responses are not sufficiently grounded in Face Visual Context (Facext). To address this limitation, we propose the Fake-in-Facext (FiFa) framework, with contributions focusing on data annotation and model construction. We first define a Facial Image Concept Tree (FICT) to divide facial images into fine-grained regional concepts, thereby obtaining a more reliable data annotation pipeline, FiFa-Annotator, for forgery explanation. Based on this dedicated data annotation, we introduce a novel Artifact-Grounding Explanation (AGE) task, which generates textual forgery explanations interleaved with segmentation masks of manipulated artifacts. We propose a unified multi-task learning architecture, FiFa-MLLM, to simultaneously support abundant multimodal inputs and outputs for fine-grained Explainable DeepFake Analysis. With multiple auxiliary supervision tasks, FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA performance on existing XDFA datasets. The code and data will be made open-source at https://github.com/lxq1000/Fake-in-Facext.

