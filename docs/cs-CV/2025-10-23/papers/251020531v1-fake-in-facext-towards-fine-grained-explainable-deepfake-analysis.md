---
layout: default
title: Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis
---

# Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.20531" target="_blank" class="toolbar-btn">arXiv: 2510.20531v1</a>
    <a href="https://arxiv.org/pdf/2510.20531.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20531v1" 
            onclick="toggleFavorite(this, '2510.20531v1', 'Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Lixiong Qin, Yang Zhang, Mei Wang, Jiani Hu, Weihong Deng, Weiran Xu

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-23

**Â§áÊ≥®**: 25 pages, 9 figures, 17 tables

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/lxq1000/Fake-in-Facext)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Fake-in-FacextÊ°ÜÊû∂ÔºåÂÆûÁé∞ÁªÜÁ≤íÂ∫¶„ÄÅÂèØËß£ÈáäÁöÑDeepFake‰∫∫ËÑ∏ÂàÜÊûê„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `DeepFakeÂàÜÊûê` `ÂèØËß£ÈáäÊÄßAI` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `‰∫∫ËÑ∏ÂõæÂÉèÂ§ÑÁêÜ` `‰º™ÈÄ†ÁóïËøπÂÆö‰Ωç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂèØËß£ÈáäDeepFakeÂàÜÊûêÊñπÊ≥ïÁº∫‰πèÁªÜÁ≤íÂ∫¶ÊÑüÁü•ÔºåÊï∞ÊçÆÊ†áÊ≥®Á≤óÁ≥ôÔºåÊó†Ê≥ïÊúâÊïàËøûÊé•ÊñáÊú¨Ëß£ÈáäÂíåËßÜËßâËØÅÊçÆ„ÄÇ
2. ÊèêÂá∫Fake-in-FacextÊ°ÜÊû∂ÔºåÈÄöËøáÈù¢ÈÉ®ÂõæÂÉèÊ¶ÇÂøµÊ†ë(FICT)ÂÆûÁé∞ÁªÜÁ≤íÂ∫¶Êï∞ÊçÆÊ†áÊ≥®ÔºåÂπ∂ÂºïÂÖ•‰º™ÈÄ†ÁóïËøπÂÆö‰ΩçËß£Èáä(AGE)‰ªªÂä°„ÄÇ
3. FiFa-MLLMÂú®AGE‰ªªÂä°‰∏äË∂ÖË∂äÁé∞ÊúâÂü∫Á∫øÔºåÂπ∂Âú®XDFAÊï∞ÊçÆÈõÜ‰∏äÂèñÂæóSOTAÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLM)ÁöÑÂèëÂ±ïÂº•Âêà‰∫ÜËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰ΩøÂæóÂèØËß£ÈáäÁöÑDeepFakeÂàÜÊûê(XDFA)Êàê‰∏∫ÂèØËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÊñπÊ≥ïÁº∫‰πèÁªÜÁ≤íÂ∫¶ÊÑüÁü•ÔºöÊï∞ÊçÆÊ†áÊ≥®‰∏≠ÁöÑ‰º™ÈÄ†ÁóïËøπÊèèËø∞‰∏çÂèØÈù†‰∏îÁ≤óÁ≥ôÔºåÊ®°ÂûãÊó†Ê≥ïÊîØÊåÅÊñáÊú¨‰º™ÈÄ†Ëß£Èáä‰∏é‰º™ÈÄ†ÁóïËøπËßÜËßâËØÅÊçÆ‰πãÈó¥ÁöÑËøûÊé•ËæìÂá∫Ôºå‰ª•Âèä‰ªªÊÑèÈù¢ÈÉ®Âå∫ÂüüÁöÑÊü•ËØ¢ËæìÂÖ•„ÄÇÂõ†Ê≠§ÔºåÂÆÉ‰ª¨ÁöÑÂìçÂ∫îÊ≤°ÊúâÂÖÖÂàÜÂü∫‰∫é‰∫∫ËÑ∏ËßÜËßâ‰∏ä‰∏ãÊñá(Facext)„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFake-in-Facext (FiFa)Ê°ÜÊû∂ÔºåÂÖ∂Ë¥°ÁåÆÈõÜ‰∏≠Âú®Êï∞ÊçÆÊ†áÊ≥®ÂíåÊ®°ÂûãÊûÑÂª∫‰∏ä„ÄÇÊàë‰ª¨È¶ñÂÖàÂÆö‰πâ‰∫Ü‰∏Ä‰∏™Èù¢ÈÉ®ÂõæÂÉèÊ¶ÇÂøµÊ†ë(FICT)Â∞ÜÈù¢ÈÉ®ÂõæÂÉèÂàíÂàÜ‰∏∫ÁªÜÁ≤íÂ∫¶ÁöÑÂå∫ÂüüÊ¶ÇÂøµÔºå‰ªéËÄåËé∑ÂæóÊõ¥ÂèØÈù†ÁöÑ‰º™ÈÄ†Ëß£ÈáäÊï∞ÊçÆÊ†áÊ≥®ÊµÅÁ®ãFiFa-Annotator„ÄÇÂü∫‰∫éËøôÁßç‰∏ìÁî®Êï∞ÊçÆÊ†áÊ≥®ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰º™ÈÄ†ÁóïËøπÂÆö‰ΩçËß£Èáä(AGE)‰ªªÂä°ÔºåËØ•‰ªªÂä°ÁîüÊàê‰∏éÁØ°Êîπ‰º™ÈÄ†ÁóïËøπÁöÑÂàÜÂâ≤Êé©Á†Å‰∫§ÁªáÁöÑÊñáÊú¨‰º™ÈÄ†Ëß£Èáä„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†Êû∂ÊûÑFiFa-MLLMÔºå‰ª•ÂêåÊó∂ÊîØÊåÅ‰∏∞ÂØåÁöÑÂ§öÊ®°ÊÄÅËæìÂÖ•ÂíåËæìÂá∫ÔºåÁî®‰∫éÁªÜÁ≤íÂ∫¶ÁöÑÂèØËß£ÈáäDeepFakeÂàÜÊûê„ÄÇÈÄöËøáÂ§ö‰∏™ËæÖÂä©ÁõëÁù£‰ªªÂä°ÔºåFiFa-MLLMÂèØ‰ª•Âú®AGE‰ªªÂä°‰∏ä‰ºò‰∫éÂº∫Â§ßÁöÑÂü∫Á∫øÔºåÂπ∂Âú®Áé∞ÊúâÁöÑXDFAÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞SOTAÊÄßËÉΩ„ÄÇ‰ª£Á†ÅÂíåÊï∞ÊçÆÂ∞ÜÂú®https://github.com/lxq1000/Fake-in-FacextÂºÄÊ∫ê„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂèØËß£ÈáäDeepFakeÂàÜÊûê(XDFA)ÊñπÊ≥ïÂú®ÁªÜÁ≤íÂ∫¶‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊï∞ÊçÆÊ†áÊ≥®‰∏≠ÂØπ‰º™ÈÄ†ÁóïËøπÁöÑÊèèËø∞‰∏çÂ§üÁ≤æÁ°ÆÔºåÂØºËá¥Ê®°ÂûãÊó†Ê≥ïÂáÜÁ°ÆÂú∞Â∞ÜÊñáÊú¨Ëß£Èáä‰∏éÂØπÂ∫îÁöÑËßÜËßâ‰º™ÈÄ†Âå∫ÂüüÂÖ≥ËÅîËµ∑Êù•„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊîØÊåÅÂØπ‰ªªÊÑèÈù¢ÈÉ®Âå∫ÂüüËøõË°åÊü•ËØ¢ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®ËåÉÂõ¥„ÄÇËøô‰∫õÈóÆÈ¢òÂØºËá¥Ê®°ÂûãËæìÂá∫ÁöÑËß£Èáä‰∏çÂ§üÂÖÖÂàÜÔºåÁº∫‰πè‰∫∫ËÑ∏ËßÜËßâ‰∏ä‰∏ãÊñáÁöÑÊîØÊíë„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöFiFaÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊèêÂçáXDFAÁöÑÁªÜÁ≤íÂ∫¶ÊÑüÁü•ËÉΩÂäõ„ÄÇÈÄöËøáÊûÑÂª∫Èù¢ÈÉ®ÂõæÂÉèÊ¶ÇÂøµÊ†ë(FICT)ÔºåÂ∞Ü‰∫∫ËÑ∏ÂõæÂÉèÂàÜËß£‰∏∫Êõ¥ÁªÜËá¥ÁöÑÂå∫ÂüüÊ¶ÇÂøµÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂèØÈù†ÁöÑÊï∞ÊçÆÊ†áÊ≥®„ÄÇÂêåÊó∂ÔºåÂºïÂÖ•‰º™ÈÄ†ÁóïËøπÂÆö‰ΩçËß£Èáä(AGE)‰ªªÂä°ÔºåË¶ÅÊ±ÇÊ®°ÂûãÁîüÊàêÂåÖÂê´ÂàÜÂâ≤Êé©Á†ÅÁöÑÊñáÊú¨Ëß£ÈáäÔºå‰ªéËÄåÂ∞ÜÊñáÊú¨ÂíåËßÜËßâ‰ø°ÊÅØÁ¥ßÂØÜÁªìÂêà„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöFiFaÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºöFiFa-AnnotatorÂíåFiFa-MLLM„ÄÇFiFa-AnnotatorÊòØ‰∏Ä‰∏™Êï∞ÊçÆÊ†áÊ≥®ÊµÅÁ®ãÔºåÂà©Áî®FICTËøõË°åÁªÜÁ≤íÂ∫¶Ê†áÊ≥®„ÄÇFiFa-MLLMÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†Êû∂ÊûÑÔºåÁî®‰∫éÊîØÊåÅÂ§öÊ®°ÊÄÅËæìÂÖ•ÂíåËæìÂá∫„ÄÇËØ•Ê®°ÂûãÂêåÊó∂ÊâßË°åAGE‰ªªÂä°‰ª•ÂèäÂ§ö‰∏™ËæÖÂä©ÁõëÁù£‰ªªÂä°Ôºå‰ª•ÊèêÂçáÊÄßËÉΩ„ÄÇÊï¥‰ΩìÊµÅÁ®ã‰∏∫ÔºöÈ¶ñÂÖà‰ΩøÁî®FiFa-AnnotatorÊ†áÊ≥®Êï∞ÊçÆÔºåÁÑ∂Âêé‰ΩøÁî®Ê†áÊ≥®Êï∞ÊçÆËÆ≠ÁªÉFiFa-MLLMÔºåÊúÄÂêé‰ΩøÁî®ËÆ≠ÁªÉÂ•ΩÁöÑFiFa-MLLMËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑDeepFakeÂàÜÊûê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ª•‰∏ãÂá†ÁÇπÔºö1) ÊèêÂá∫‰∫ÜÈù¢ÈÉ®ÂõæÂÉèÊ¶ÇÂøµÊ†ë(FICT)ÔºåÁî®‰∫éÁªÜÁ≤íÂ∫¶ÁöÑ‰∫∫ËÑ∏Âå∫ÂüüÂàíÂàÜ„ÄÇ2) ÂºïÂÖ•‰∫Ü‰º™ÈÄ†ÁóïËøπÂÆö‰ΩçËß£Èáä(AGE)‰ªªÂä°ÔºåÂ∞ÜÊñáÊú¨Ëß£Èáä‰∏éËßÜËßâÂàÜÂâ≤Êé©Á†ÅÁõ∏ÁªìÂêà„ÄÇ3) ÊûÑÂª∫‰∫ÜÁªü‰∏ÄÁöÑÂ§ö‰ªªÂä°Â≠¶‰π†Êû∂ÊûÑFiFa-MLLMÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂ§öÊ®°ÊÄÅËæìÂÖ•ÂíåËæìÂá∫„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåFiFaÊ°ÜÊû∂ËÉΩÂ§üÊèê‰æõÊõ¥Á≤æÁ°Æ„ÄÅÊõ¥ÂèØËß£ÈáäÁöÑDeepFakeÂàÜÊûêÁªìÊûú„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Êï∞ÊçÆÊ†áÊ≥®ÊñπÈù¢ÔºåFICTÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂÆÉÂÜ≥ÂÆö‰∫ÜÊ†áÊ≥®ÁöÑÁªÜÁ≤íÂ∫¶Á®ãÂ∫¶„ÄÇÂú®Ê®°ÂûãÊñπÈù¢ÔºåFiFa-MLLMÈááÁî®‰∫ÜÂ§ö‰ªªÂä°Â≠¶‰π†Á≠ñÁï•ÔºåÈÄöËøáÂ§ö‰∏™ËæÖÂä©‰ªªÂä°Êù•ÊèêÂçáAGE‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÁöÑËæÖÂä©‰ªªÂä°ÂåÖÊã¨Ôºö‰∫∫ËÑ∏Â±ûÊÄßËØÜÂà´„ÄÅ‰∫∫ËÑ∏Âå∫ÂüüÂàÜÂâ≤Á≠â„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüÈúÄË¶Å‰ªîÁªÜËÄÉËôëÔºå‰ª•Âπ≥Ë°°‰∏çÂêå‰ªªÂä°‰πãÈó¥ÁöÑÊùÉÈáç„ÄÇÁΩëÁªúÁªìÊûÑÊñπÈù¢ÔºåFiFa-MLLMÂèØËÉΩÈááÁî®‰∫ÜTransformerÊû∂ÊûÑÔºå‰ª•Êõ¥Â•ΩÂú∞Â§ÑÁêÜÂ∫èÂàóÊï∞ÊçÆÂíåÂ§öÊ®°ÊÄÅ‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFiFa-MLLMÂú®AGE‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåFiFa-MLLMÂú®Áé∞ÊúâÁöÑXDFAÊï∞ÊçÆÈõÜ‰∏ä‰πüÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÂÖ∂Ê≥õÂåñËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶ÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊï∞Â≠óÂèñËØÅ„ÄÅÁ§æ‰∫§Â™í‰ΩìÂÜÖÂÆπÂÆ°Ê†∏„ÄÅË∫´‰ªΩÈ™åËØÅÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèê‰æõÁªÜÁ≤íÂ∫¶„ÄÅÂèØËß£ÈáäÁöÑDeepFakeÂàÜÊûêÔºåÊúâÂä©‰∫éËØÜÂà´ÂíåÊè≠Èú≤ËôöÂÅá‰ø°ÊÅØÔºåÁª¥Êä§ÁΩëÁªúÂÆâÂÖ®Âíå‰ø°ÊÅØÁúüÂÆûÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØËøõ‰∏ÄÊ≠•Â∫îÁî®‰∫éËßÜÈ¢ëDeepFakeÊ£ÄÊµãÔºå‰ª•ÂèäÊõ¥ÂπøÊ≥õÁöÑÂõæÂÉèÂíåËßÜÈ¢ëÁØ°ÊîπÂàÜÊûê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The advancement of Multimodal Large Language Models (MLLMs) has bridged the gap between vision and language tasks, enabling the implementation of Explainable DeepFake Analysis (XDFA). However, current methods suffer from a lack of fine-grained awareness: the description of artifacts in data annotation is unreliable and coarse-grained, and the models fail to support the output of connections between textual forgery explanations and the visual evidence of artifacts, as well as the input of queries for arbitrary facial regions. As a result, their responses are not sufficiently grounded in Face Visual Context (Facext). To address this limitation, we propose the Fake-in-Facext (FiFa) framework, with contributions focusing on data annotation and model construction. We first define a Facial Image Concept Tree (FICT) to divide facial images into fine-grained regional concepts, thereby obtaining a more reliable data annotation pipeline, FiFa-Annotator, for forgery explanation. Based on this dedicated data annotation, we introduce a novel Artifact-Grounding Explanation (AGE) task, which generates textual forgery explanations interleaved with segmentation masks of manipulated artifacts. We propose a unified multi-task learning architecture, FiFa-MLLM, to simultaneously support abundant multimodal inputs and outputs for fine-grained Explainable DeepFake Analysis. With multiple auxiliary supervision tasks, FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA performance on existing XDFA datasets. The code and data will be made open-source at https://github.com/lxq1000/Fake-in-Facext.

