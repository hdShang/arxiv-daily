---
layout: default
title: Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence
---

# Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.20470" target="_blank" class="toolbar-btn">arXiv: 2510.20470v2</a>
    <a href="https://arxiv.org/pdf/2510.20470.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20470v2" 
            onclick="toggleFavorite(this, '2510.20470v2', 'Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-23 (Êõ¥Êñ∞: 2025-11-20)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ConanÔºöÊèêÂá∫Âü∫‰∫éÂ§öÂ∞∫Â∫¶ËßÜËßâËØÅÊçÆÁöÑÊ∏êËøõÂºèÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜÈ¢ëÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ËßÜËßâËØÅÊçÆ` `Ê∏êËøõÂºèÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜÈ¢ëÊé®ÁêÜ‰∏≠Èù¢‰∏¥ÊåëÊàòÔºåÁé∞ÊúâÊñπÊ≥ïÊàñ‰æùËµñÊó†‰æùÊçÆÁöÑÊñáÊú¨ÈìæÔºåÊàñÈöæ‰ª•Á≤æÁ°ÆÂÆö‰ΩçËßÜËßâËØÅÊçÆ„ÄÇ
2. ConanÊ°ÜÊû∂ÈÄöËøáËØÜÂà´‰∏ä‰∏ãÊñáÂíåËØÅÊçÆÂ∏ßÔºåËøõË°åË∑®Â∏ßÁ∫øÁ¥¢Êé®ÁêÜÔºåÂπ∂Ëá™ÈÄÇÂ∫îÂÜ≥ÂÆöÊé®ÁêÜÊ≠•È™§ÔºåÂÆûÁé∞Âü∫‰∫éËßÜËßâËØÅÊçÆÁöÑÂ§öÊ≠•Êé®ÁêÜ„ÄÇ
3. ConanÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÊ®°ÂûãÔºåÂπ∂Âú®ÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜÈ¢ëÊé®ÁêÜÈúÄË¶ÅÂú®Â∏ß‰πãÈó¥ËøõË°åÂ§öÊ≠•È™§Êé®ÂØºÔºåËøôÂØπÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÊù•ËØ¥‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™‰∏ªË¶ÅÁöÑÊåëÊàò„ÄÇËôΩÁÑ∂Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁöÑÊñπÊ≥ïÂ¢ûÂº∫‰∫ÜÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂÆÉ‰ª¨ÈÄöÂ∏∏‰æùËµñ‰∫éÁ∫ØÊñáÊú¨ÈìæÔºåÂØºËá¥ÁªìËÆ∫Áº∫‰πè‰æùÊçÆÊàñ‰∫ßÁîüÂπªËßâ„ÄÇÁõ∏ÂèçÔºåÂ∏ßÊ£ÄÁ¥¢ÊñπÊ≥ïÂºïÂÖ•‰∫ÜËßÜËßâÂü∫Á°ÄÔºå‰ΩÜ‰ªçÁÑ∂Èöæ‰ª•ÂáÜÁ°ÆÂÆö‰ΩçËØÅÊçÆ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜConanÔºå‰∏Ä‰∏™Âü∫‰∫éËØÅÊçÆÁöÑÂ§öÊ≠•È™§ËßÜÈ¢ëÊé®ÁêÜÊ°ÜÊû∂„ÄÇConanËØÜÂà´‰∏ä‰∏ãÊñáÂíåËØÅÊçÆÂ∏ßÔºåÊé®ÁêÜË∑®Â∏ßÁ∫øÁ¥¢ÔºåÂπ∂Ëá™ÈÄÇÂ∫îÂú∞ÂÜ≥ÂÆö‰ΩïÊó∂ÁªìÊùüÊàñËøõ‰∏ÄÊ≠•Êé¢Á¥¢„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨1ÔºâÊûÑÂª∫‰∫ÜConan-91KÔºå‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑËá™Âä®ÁîüÊàêÁöÑÊé®ÁêÜËΩ®ËøπÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨Â∏ßËØÜÂà´„ÄÅËØÅÊçÆÊé®ÁêÜÂíåÂä®‰ΩúÂÜ≥Á≠ñÔºå‰ª•Âèä2ÔºâËÆæËÆ°‰∫Ü‰∏Ä‰∏™Â§öÈò∂ÊÆµÊ∏êËøõÂºèÂÜ∑ÂêØÂä®Á≠ñÁï•ÔºåÁªìÂêàËØÜÂà´-Êé®ÁêÜ-Ë°åÂä®ÔºàAIRÔºâRLVRËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰ª•ÈÄêÊ≠•ÊøÄÂä±Â§öÊ≠•È™§ËßÜËßâÊé®ÁêÜ„ÄÇÂú®ÂÖ≠‰∏™Â§öÊ≠•È™§Êé®ÁêÜÂü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåConanÁöÑÂáÜÁ°ÆÁéáÂπ≥ÂùáË∂ÖËøáÂü∫Á∫øQwen2.5-VL-7B-Instruct 10%‰ª•‰∏äÔºåÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåConanÊúâÊïàÂú∞Êé®ÂπøÂà∞ÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÂèØÊâ©Â±ïÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜÈ¢ëÊé®ÁêÜ‰ªªÂä°ÈúÄË¶ÅÊ®°ÂûãÂÖ∑Â§áÂú®Â§ö‰∏™Â∏ß‰πãÈó¥ËøõË°åÊé®ÁêÜÁöÑËÉΩÂäõÔºåÁé∞ÊúâÊñπÊ≥ïË¶Å‰πà‰æùËµñ‰∫éÁ∫ØÊñáÊú¨Êé®ÁêÜÈìæÔºåÂÆπÊòì‰∫ßÁîüÂπªËßâÔºõË¶Å‰πà‰æùËµñ‰∫éÂ∏ßÊ£ÄÁ¥¢Ôºå‰ΩÜÈöæ‰ª•ÂáÜÁ°ÆÂÆö‰ΩçÂÖ≥ÈîÆËØÅÊçÆÂ∏ß„ÄÇËøô‰∫õÈóÆÈ¢òÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®Â§çÊùÇËßÜÈ¢ëÂú∫ÊôØ‰∏ãÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöConanÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊ∏êËøõÂºèÂ≠¶‰π†ÔºåÂºïÂØºÊ®°ÂûãÈÄêÊ≠•Â≠¶‰π†ËØÜÂà´ÂÖ≥ÈîÆÂ∏ß„ÄÅËøõË°åËØÅÊçÆÊé®ÁêÜÂíåÂÅöÂá∫Ë°åÂä®ÂÜ≥Á≠ñ„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºåÊ®°ÂûãËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Êé¢Á¥¢ÂíåÂà©Áî®ËßÜËßâËØÅÊçÆÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöConanÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Â∏ßËØÜÂà´Ê®°ÂùóÔºåÁî®‰∫éËØÜÂà´‰∏éÊé®ÁêÜÁõ∏ÂÖ≥ÁöÑ‰∏ä‰∏ãÊñáÂ∏ßÂíåËØÅÊçÆÂ∏ßÔºõ2) ËØÅÊçÆÊé®ÁêÜÊ®°ÂùóÔºåÁî®‰∫éÂü∫‰∫éËØÜÂà´ÁöÑÂ∏ßËøõË°åË∑®Â∏ßÁ∫øÁ¥¢Êé®ÁêÜÔºõ3) Ë°åÂä®ÂÜ≥Á≠ñÊ®°ÂùóÔºåÁî®‰∫éÂÜ≥ÂÆö‰ΩïÊó∂ÁªìÊùüÊé®ÁêÜÊàñËøõ‰∏ÄÊ≠•Êé¢Á¥¢„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÈááÁî®Â§öÈò∂ÊÆµÊ∏êËøõÂºèÂÜ∑ÂêØÂä®Á≠ñÁï•ÔºåÈÄêÊ≠•ËÆ≠ÁªÉÊ®°ÂûãÁöÑÂêÑ‰∏™Ê®°Âùó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöConanÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂AIR (Identification-Reasoning-Action) Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊ°ÜÊû∂ÂíåÂ§öÈò∂ÊÆµÊ∏êËøõÂºèÂÜ∑ÂêØÂä®Á≠ñÁï•„ÄÇAIRÊ°ÜÊû∂Â∞ÜÊé®ÁêÜËøáÁ®ãÂàÜËß£‰∏∫ËØÜÂà´„ÄÅÊé®ÁêÜÂíåË°åÂä®‰∏â‰∏™Ê≠•È™§ÔºåÂπ∂‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†Êù•‰ºòÂåñÊØè‰∏™Ê≠•È™§ÁöÑÁ≠ñÁï•„ÄÇÊ∏êËøõÂºèÂÜ∑ÂêØÂä®Á≠ñÁï•ÂàôÈÄöËøáÈÄêÊ≠•Â¢ûÂä†ËÆ≠ÁªÉÈöæÂ∫¶ÔºåÂ∏ÆÂä©Ê®°ÂûãÊõ¥Â•ΩÂú∞Â≠¶‰π†Â§çÊùÇÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöConan-91KÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫ÊòØÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏ÄÔºåÂÆÉÊèê‰æõ‰∫ÜÂ§ßËßÑÊ®°ÁöÑËá™Âä®ÁîüÊàêÁöÑÊé®ÁêÜËΩ®ËøπÔºåÂåÖÊã¨Â∏ßËØÜÂà´„ÄÅËØÅÊçÆÊé®ÁêÜÂíåË°åÂä®ÂÜ≥Á≠ñ„ÄÇÊ≠§Â§ñÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüËá≥ÂÖ≥ÈáçË¶ÅÔºåÂÆÉÈúÄË¶ÅÂπ≥Ë°°ËØÜÂà´„ÄÅÊé®ÁêÜÂíåË°åÂä®‰∏â‰∏™Ê≠•È™§ÁöÑ‰ºòÂåñÁõÆÊ†á„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠Â∫îËØ•ÊúâÊõ¥ËØ¶ÁªÜÁöÑÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ConanÂú®ÂÖ≠‰∏™Â§öÊ≠•È™§Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂπ≥ÂùáÂáÜÁ°ÆÁéáË∂ÖËøáÂü∫Á∫øÊ®°ÂûãQwen2.5-VL-7B-Instruct 10%‰ª•‰∏äÔºåÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåConanÂú®ÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°‰∏≠‰πüË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§çÊùÇËßÜÈ¢ëÂú∫ÊôØ‰∏ãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ConanÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÁõëÊéß„ÄÅËßÜÈ¢ëÂÜÖÂÆπÁêÜËß£„ÄÅÊô∫ËÉΩÂÆ¢ÊúçÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êô∫ËÉΩÁõëÊéß‰∏≠ÔºåConanÂèØ‰ª•Áî®‰∫éÂàÜÊûêÁõëÊéßËßÜÈ¢ëÔºåËØÜÂà´ÂºÇÂ∏∏Ë°å‰∏∫Âπ∂ËøõË°åÈ¢ÑË≠¶„ÄÇÂú®ËßÜÈ¢ëÂÜÖÂÆπÁêÜËß£‰∏≠ÔºåConanÂèØ‰ª•Áî®‰∫éÁêÜËß£ËßÜÈ¢ëÂÜÖÂÆπÔºåÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØÂπ∂ÁîüÊàêÊëòË¶Å„ÄÇÂú®Êô∫ËÉΩÂÆ¢Êúç‰∏≠ÔºåConanÂèØ‰ª•Áî®‰∫éÂõûÁ≠îÁî®Êà∑ÂÖ≥‰∫éËßÜÈ¢ëÂÜÖÂÆπÁöÑÈóÆÈ¢ò„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding, yet still struggle with inaccurate evidence localization. To address these limitations, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies context and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we 1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that include frame identification, evidence reasoning, and action decision, and 2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to progressively incentivize multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long video understanding tasks, validating its strong scalability and robustness.

