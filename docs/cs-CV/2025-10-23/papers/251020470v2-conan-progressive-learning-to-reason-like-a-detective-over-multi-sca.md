---
layout: default
title: Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence
---

# Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20470" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20470v2</a>
  <a href="https://arxiv.org/pdf/2510.20470.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20470v2" onclick="toggleFavorite(this, '2510.20470v2', 'Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23 (æ›´æ–°: 2025-11-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Conanï¼šæå‡ºåŸºäºå¤šå°ºåº¦è§†è§‰è¯æ®çš„æ¸è¿›å¼å­¦ä¹ æ¡†æ¶ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ¨ç†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯æ®` `æ¸è¿›å¼å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•æˆ–ä¾èµ–æ— ä¾æ®çš„æ–‡æœ¬é“¾ï¼Œæˆ–éš¾ä»¥ç²¾ç¡®å®šä½è§†è§‰è¯æ®ã€‚
2. Conanæ¡†æ¶é€šè¿‡è¯†åˆ«ä¸Šä¸‹æ–‡å’Œè¯æ®å¸§ï¼Œè¿›è¡Œè·¨å¸§çº¿ç´¢æ¨ç†ï¼Œå¹¶è‡ªé€‚åº”å†³å®šæ¨ç†æ­¥éª¤ï¼Œå®ç°åŸºäºè§†è§‰è¯æ®çš„å¤šæ­¥æ¨ç†ã€‚
3. Conanåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘æ¨ç†éœ€è¦åœ¨å¸§ä¹‹é—´è¿›è¡Œå¤šæ­¥éª¤æ¨å¯¼ï¼Œè¿™å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºçº¯æ–‡æœ¬é“¾ï¼Œå¯¼è‡´ç»“è®ºç¼ºä¹ä¾æ®æˆ–äº§ç”Ÿå¹»è§‰ã€‚ç›¸åï¼Œå¸§æ£€ç´¢æ–¹æ³•å¼•å…¥äº†è§†è§‰åŸºç¡€ï¼Œä½†ä»ç„¶éš¾ä»¥å‡†ç¡®å®šä½è¯æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Conanï¼Œä¸€ä¸ªåŸºäºè¯æ®çš„å¤šæ­¥éª¤è§†é¢‘æ¨ç†æ¡†æ¶ã€‚Conanè¯†åˆ«ä¸Šä¸‹æ–‡å’Œè¯æ®å¸§ï¼Œæ¨ç†è·¨å¸§çº¿ç´¢ï¼Œå¹¶è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶ç»“æŸæˆ–è¿›ä¸€æ­¥æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬1ï¼‰æ„å»ºäº†Conan-91Kï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„è‡ªåŠ¨ç”Ÿæˆçš„æ¨ç†è½¨è¿¹æ•°æ®é›†ï¼ŒåŒ…æ‹¬å¸§è¯†åˆ«ã€è¯æ®æ¨ç†å’ŒåŠ¨ä½œå†³ç­–ï¼Œä»¥åŠ2ï¼‰è®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µæ¸è¿›å¼å†·å¯åŠ¨ç­–ç•¥ï¼Œç»“åˆè¯†åˆ«-æ¨ç†-è¡ŒåŠ¨ï¼ˆAIRï¼‰RLVRè®­ç»ƒæ¡†æ¶ï¼Œä»¥é€æ­¥æ¿€åŠ±å¤šæ­¥éª¤è§†è§‰æ¨ç†ã€‚åœ¨å…­ä¸ªå¤šæ­¥éª¤æ¨ç†åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒConançš„å‡†ç¡®ç‡å¹³å‡è¶…è¿‡åŸºçº¿Qwen2.5-VL-7B-Instruct 10%ä»¥ä¸Šï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒConanæœ‰æ•ˆåœ°æ¨å¹¿åˆ°é•¿è§†é¢‘ç†è§£ä»»åŠ¡ï¼ŒéªŒè¯äº†å…¶å¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘æ¨ç†ä»»åŠ¡éœ€è¦æ¨¡å‹å…·å¤‡åœ¨å¤šä¸ªå¸§ä¹‹é—´è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–äºçº¯æ–‡æœ¬æ¨ç†é“¾ï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼›è¦ä¹ˆä¾èµ–äºå¸§æ£€ç´¢ï¼Œä½†éš¾ä»¥å‡†ç¡®å®šä½å…³é”®è¯æ®å¸§ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚è§†é¢‘åœºæ™¯ä¸‹çš„æ¨ç†æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šConançš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¸è¿›å¼å­¦ä¹ ï¼Œå¼•å¯¼æ¨¡å‹é€æ­¥å­¦ä¹ è¯†åˆ«å…³é”®å¸§ã€è¿›è¡Œè¯æ®æ¨ç†å’Œåšå‡ºè¡ŒåŠ¨å†³ç­–ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ¢ç´¢å’Œåˆ©ç”¨è§†è§‰è¯æ®ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šConanæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¸§è¯†åˆ«æ¨¡å—ï¼Œç”¨äºè¯†åˆ«ä¸æ¨ç†ç›¸å…³çš„ä¸Šä¸‹æ–‡å¸§å’Œè¯æ®å¸§ï¼›2) è¯æ®æ¨ç†æ¨¡å—ï¼Œç”¨äºåŸºäºè¯†åˆ«çš„å¸§è¿›è¡Œè·¨å¸§çº¿ç´¢æ¨ç†ï¼›3) è¡ŒåŠ¨å†³ç­–æ¨¡å—ï¼Œç”¨äºå†³å®šä½•æ—¶ç»“æŸæ¨ç†æˆ–è¿›ä¸€æ­¥æ¢ç´¢ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨å¤šé˜¶æ®µæ¸è¿›å¼å†·å¯åŠ¨ç­–ç•¥ï¼Œé€æ­¥è®­ç»ƒæ¨¡å‹çš„å„ä¸ªæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šConançš„å…³é”®åˆ›æ–°åœ¨äºå…¶AIR (Identification-Reasoning-Action) å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶å’Œå¤šé˜¶æ®µæ¸è¿›å¼å†·å¯åŠ¨ç­–ç•¥ã€‚AIRæ¡†æ¶å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºè¯†åˆ«ã€æ¨ç†å’Œè¡ŒåŠ¨ä¸‰ä¸ªæ­¥éª¤ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¯ä¸ªæ­¥éª¤çš„ç­–ç•¥ã€‚æ¸è¿›å¼å†·å¯åŠ¨ç­–ç•¥åˆ™é€šè¿‡é€æ­¥å¢åŠ è®­ç»ƒéš¾åº¦ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šConan-91Kæ•°æ®é›†çš„æ„å»ºæ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ï¼Œå®ƒæä¾›äº†å¤§è§„æ¨¡çš„è‡ªåŠ¨ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ï¼ŒåŒ…æ‹¬å¸§è¯†åˆ«ã€è¯æ®æ¨ç†å’Œè¡ŒåŠ¨å†³ç­–ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œå®ƒéœ€è¦å¹³è¡¡è¯†åˆ«ã€æ¨ç†å’Œè¡ŒåŠ¨ä¸‰ä¸ªæ­¥éª¤çš„ä¼˜åŒ–ç›®æ ‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Conanåœ¨å…­ä¸ªå¤šæ­¥éª¤æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡å‡†ç¡®ç‡è¶…è¿‡åŸºçº¿æ¨¡å‹Qwen2.5-VL-7B-Instruct 10%ä»¥ä¸Šï¼Œå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒConanåœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚è§†é¢‘åœºæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Conanæ¡†æ¶å¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€è§†é¢‘å†…å®¹ç†è§£ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼ŒConanå¯ä»¥ç”¨äºåˆ†æç›‘æ§è§†é¢‘ï¼Œè¯†åˆ«å¼‚å¸¸è¡Œä¸ºå¹¶è¿›è¡Œé¢„è­¦ã€‚åœ¨è§†é¢‘å†…å®¹ç†è§£ä¸­ï¼ŒConanå¯ä»¥ç”¨äºç†è§£è§†é¢‘å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆæ‘˜è¦ã€‚åœ¨æ™ºèƒ½å®¢æœä¸­ï¼ŒConanå¯ä»¥ç”¨äºå›ç­”ç”¨æˆ·å…³äºè§†é¢‘å†…å®¹çš„é—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding, yet still struggle with inaccurate evidence localization. To address these limitations, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies context and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we 1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that include frame identification, evidence reasoning, and action decision, and 2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to progressively incentivize multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long video understanding tasks, validating its strong scalability and robustness.

