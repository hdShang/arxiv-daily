---
layout: default
title: SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding
---

# SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20622" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20622v1</a>
  <a href="https://arxiv.org/pdf/2510.20622.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20622v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.20622v1', 'SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSeViCESæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰-è§†è§‰å…±è¯†æå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `è§†é¢‘å¤§è¯­è¨€æ¨¡å‹` `è¯­ä¹‰è§†è§‰å…±è¯†` `å…³é”®å¸§é€‰æ‹©` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿è§†é¢‘ç†è§£æ–¹æ³•å¿½ç•¥æ—¶é—´ä¾èµ–æˆ–ä¾èµ–å•æ¨¡æ€è¯æ®ï¼Œéš¾ä»¥æä¾›å®Œæ•´ä¸Šä¸‹æ–‡ã€‚
2. SeViCESæ¡†æ¶é€šè¿‡è¯­ä¹‰å’Œè§†è§‰å…±è¯†é€‰æ‹©å…³é”®å¸§ï¼Œå¹¶ç»†åŒ–ç­”æ¡ˆä»¥æé«˜ä¸€è‡´æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSeViCESåœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿è§†é¢‘ç†è§£å› å…¶å¤æ‚ã€å¤šæ ·å’Œæ—¶é—´åˆ†æ•£çš„å†…å®¹è€Œæå…·æŒ‘æˆ˜æ€§ã€‚å°½ç®¡è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(Video-LLMs)å¯ä»¥å¤„ç†é•¿è¾¾æ•°ååˆ†é’Ÿçš„è§†é¢‘ï¼Œä½†å°†å…¶åº”ç”¨äºçœŸæ­£é•¿çš„åºåˆ—åœ¨è®¡ç®—ä¸Šæ˜¯éš¾ä»¥æ‰¿å—çš„ï¼Œå¹¶ä¸”å¸¸å¸¸å¯¼è‡´ä¸é›†ä¸­æˆ–ä¸ä¸€è‡´çš„æ¨ç†ã€‚ä¸€ä¸ªæœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆæ˜¯ä»…é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„å¸§ï¼Œç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥æ—¶é—´ä¾èµ–æ€§æˆ–ä¾èµ–äºå•æ¨¡æ€è¯æ®ï¼Œé™åˆ¶äº†å®ƒä»¬æä¾›å®Œæ•´å’ŒæŸ¥è¯¢ç›¸å…³ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰-è§†è§‰å…±è¯†è¯æ®é€‰æ‹©(SeViCES)æ¡†æ¶ï¼Œç”¨äºæœ‰æ•ˆå’Œå¯é çš„é•¿è§†é¢‘ç†è§£ã€‚SeViCESæ˜¯å…è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚è¯­ä¹‰-è§†è§‰å…±è¯†å¸§é€‰æ‹©(SVCFS)æ¨¡å—é€šè¿‡(1)ä¸€ä¸ªåˆ©ç”¨LLMå¯¹å­—å¹•è¿›è¡Œæ¨ç†çš„æ—¶é—´æ„ŸçŸ¥è¯­ä¹‰åˆ†æ”¯ï¼Œä»¥åŠ(2)ä¸€ä¸ªé€šè¿‡äº’ä¿¡æ¯å°†åµŒå…¥ä¸è¯­ä¹‰åˆ†æ•°å¯¹é½çš„èšç±»å¼•å¯¼è§†è§‰åˆ†æ”¯æ¥é€‰æ‹©å¸§ã€‚ç­”æ¡ˆå…±è¯†ç»†åŒ–(ACR)æ¨¡å—é€šè¿‡èåˆè¯æ®å’Œçº¦æŸç­”æ¡ˆç©ºé—´æ¥è¿›ä¸€æ­¥è§£å†³åŸºäºè¯­ä¹‰å’ŒåŸºäºè§†è§‰çš„é¢„æµ‹ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚åœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSeViCESåœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…±è¯†é©±åŠ¨çš„è¯æ®é€‰æ‹©å¯¹äºVideo-LLMsçš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé•¿è§†é¢‘ç†è§£ä»»åŠ¡é¢ä¸´çš„å…³é”®æŒ‘æˆ˜åœ¨äºå¦‚ä½•ä»å†—é•¿ä¸”ä¿¡æ¯åˆ†æ•£çš„è§†é¢‘ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œä»¥ä¾¿è¿›è¡Œå‡†ç¡®çš„æ¨ç†å’Œç†è§£ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥åº”ç”¨Video-LLMsï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å®¹æ˜“äº§ç”Ÿä¸ä¸€è‡´çš„æ¨ç†ç»“æœã€‚è€ŒåŸºäºå…³é”®å¸§é€‰æ‹©çš„æ–¹æ³•ï¼Œå¾€å¾€å¿½ç•¥äº†è§†é¢‘å¸§ä¹‹é—´çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œæˆ–è€…ä»…ä¾èµ–å•ä¸€æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å®Œæ•´ï¼Œå½±å“æœ€ç»ˆçš„ç†è§£æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSeViCESçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¯­ä¹‰å’Œè§†è§‰ä¿¡æ¯çš„å…±è¯†æ¥é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„è§†é¢‘å¸§ï¼Œå¹¶åˆ©ç”¨è¿™äº›å¸§è¿›è¡Œåç»­çš„æ¨ç†å’Œé¢„æµ‹ã€‚è¿™ç§å…±è¯†æœºåˆ¶æ—¨åœ¨èåˆä¸åŒæ¨¡æ€çš„ä¼˜åŠ¿ï¼Œå¼¥è¡¥å•ä¸€æ¨¡æ€çš„ä¸è¶³ï¼Œä»è€Œæä¾›æ›´å…¨é¢å’Œå¯é çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡åœ¨è¯­ä¹‰å’Œè§†è§‰å±‚é¢è¿›è¡Œä¿¡æ¯å¯¹é½å’Œèåˆï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘å™ªå£°å¹²æ‰°ï¼Œæé«˜å…³é”®å¸§é€‰æ‹©çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSeViCESæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šè¯­ä¹‰-è§†è§‰å…±è¯†å¸§é€‰æ‹©(SVCFS)å’Œç­”æ¡ˆå…±è¯†ç»†åŒ–(ACR)ã€‚SVCFSæ¨¡å—é¦–å…ˆé€šè¿‡æ—¶é—´æ„ŸçŸ¥è¯­ä¹‰åˆ†æ”¯å’Œèšç±»å¼•å¯¼è§†è§‰åˆ†æ”¯åˆ†åˆ«æå–è¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ã€‚è¯­ä¹‰åˆ†æ”¯åˆ©ç”¨LLMå¯¹è§†é¢‘å­—å¹•è¿›è¡Œæ¨ç†ï¼Œæ•æ‰è§†é¢‘å†…å®¹çš„é«˜å±‚è¯­ä¹‰ä¿¡æ¯ã€‚è§†è§‰åˆ†æ”¯åˆ™é€šè¿‡èšç±»æ–¹æ³•å¯¹è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾è¿›è¡Œåˆ†ç»„ï¼Œå¹¶åˆ©ç”¨äº’ä¿¡æ¯å°†è§†è§‰åµŒå…¥ä¸è¯­ä¹‰åˆ†æ•°å¯¹é½ã€‚ACRæ¨¡å—åˆ™è´Ÿè´£èåˆè¯­ä¹‰å’Œè§†è§‰åˆ†æ”¯çš„é¢„æµ‹ç»“æœï¼Œå¹¶é€šè¿‡çº¦æŸç­”æ¡ˆç©ºé—´æ¥è¿›ä¸€æ­¥æé«˜é¢„æµ‹çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSeViCESçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…±è¯†é©±åŠ¨çš„è¯æ®é€‰æ‹©æœºåˆ¶ã€‚ä¸ä»¥å¾€ä¾èµ–å•ä¸€æ¨¡æ€æˆ–ç®€å•èåˆçš„æ–¹æ³•ä¸åŒï¼ŒSeViCESå¼ºè°ƒè¯­ä¹‰å’Œè§†è§‰ä¿¡æ¯ä¹‹é—´çš„ç›¸äº’éªŒè¯å’Œè¡¥å……ã€‚é€šè¿‡SVCFSæ¨¡å—ï¼Œæ¡†æ¶èƒ½å¤Ÿé€‰æ‹©æ—¢å…·æœ‰ä»£è¡¨æ€§åˆä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®å¸§ï¼Œä»è€Œä¸ºåç»­çš„æ¨ç†æä¾›æ›´å¯é çš„ä¾æ®ã€‚ACRæ¨¡å—åˆ™è¿›ä¸€æ­¥åˆ©ç”¨å…±è¯†æœºåˆ¶æ¥æ¶ˆé™¤ä¸åŒæ¨¡æ€ä¹‹é—´çš„é¢„æµ‹å·®å¼‚ï¼Œæé«˜æœ€ç»ˆç»“æœçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SVCFSæ¨¡å—ä¸­ï¼Œæ—¶é—´æ„ŸçŸ¥è¯­ä¹‰åˆ†æ”¯åˆ©ç”¨LLMè¿›è¡Œæ¨ç†ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„LLMæ¨¡å‹å’Œpromptè®¾è®¡ã€‚èšç±»å¼•å¯¼è§†è§‰åˆ†æ”¯åˆ™éœ€è¦é€‰æ‹©åˆé€‚çš„èšç±»ç®—æ³•å’Œç‰¹å¾æå–æ–¹æ³•ï¼Œå¹¶è®¾è®¡äº’ä¿¡æ¯æŸå¤±å‡½æ•°æ¥å¯¹é½è§†è§‰åµŒå…¥å’Œè¯­ä¹‰åˆ†æ•°ã€‚åœ¨ACRæ¨¡å—ä¸­ï¼Œéœ€è¦è®¾è®¡åˆé€‚çš„èåˆç­–ç•¥å’Œç­”æ¡ˆç©ºé—´çº¦æŸæ–¹æ³•ï¼Œä»¥å¹³è¡¡ä¸åŒæ¨¡æ€çš„è´¡çŒ®å¹¶æé«˜é¢„æµ‹çš„ä¸€è‡´æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚éœ€è¦åœ¨å®éªŒä¸­è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SeViCESåœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSeViCESåœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…±è¯†é©±åŠ¨çš„è¯æ®é€‰æ‹©å¯¹äºVideo-LLMsçš„é‡è¦æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿ä¿¡æ¯éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ï¼Œæ­¤å¤„æ— æ³•æä¾›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SeViCESæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºé•¿è§†é¢‘ç†è§£ç›¸å…³çš„é¢†åŸŸï¼Œå¦‚è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€åœ¨çº¿æ•™è‚²ã€ç”µå½±åˆ†æç­‰ã€‚é€šè¿‡æå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œæœ‰æ•ˆæ¨ç†ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¸®åŠ©äººä»¬æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨é•¿è§†é¢‘å†…å®¹ï¼Œä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­å¿«é€Ÿå®šä½å¼‚å¸¸äº‹ä»¶ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ç†è§£å¤æ‚çš„äº¤é€šåœºæ™¯ï¼Œåœ¨åœ¨çº¿æ•™è‚²ä¸­æå–è¯¾ç¨‹é‡ç‚¹ï¼Œåœ¨ç”µå½±åˆ†æä¸­ç†è§£å‰§æƒ…å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long video understanding remains challenging due to its complex, diverse, and temporally scattered content. Although video large language models (Video-LLMs) can process videos lasting tens of minutes, applying them to truly long sequences is computationally prohibitive and often leads to unfocused or inconsistent reasoning. A promising solution is to select only the most informative frames, yet existing approaches typically ignore temporal dependencies or rely on unimodal evidence, limiting their ability to provide complete and query-relevant context. We propose a Semantic-Visual Consensus Evidence Selection (SeViCES) framework for effective and reliable long video understanding. SeViCES is training-free and model-agnostic, and introduces two key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module selects frames through (1) a temporal-aware semantic branch that leverages LLM reasoning over captions, and (2) a cluster-guided visual branch that aligns embeddings with semantic scores via mutual information. The Answer Consensus Refinement (ACR) module further resolves inconsistencies between semantic- and visual-based predictions by fusing evidence and constraining the answer space. Extensive experiments on long video understanding benchmarks show that SeViCES consistently outperforms state-of-the-art methods in both accuracy and robustness, demonstrating the importance of consensus-driven evidence selection for Video-LLMs.

