---
layout: default
title: BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models
---

# BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20095" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20095v2</a>
  <a href="https://arxiv.org/pdf/2510.20095.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20095v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.20095v2', 'BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: Project page: https://imageomics.github.io/biocap/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BioCAPï¼šåˆ©ç”¨åˆæˆå­—å¹•å¢å¼ºç”Ÿç‰©å­¦åŸºç¡€æ¨¡å‹ï¼Œè¶…è¶Šæ ‡ç­¾ç›‘ç£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿç‰©å­¦åŸºç¡€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `åˆæˆå­—å¹•` `ç‰©ç§åˆ†ç±»` `æ–‡æœ¬-å›¾åƒæ£€ç´¢` `å¯¹æ¯”å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç”Ÿç‰©å­¦å¤šæ¨¡æ€æ¨¡å‹ç¼ºä¹å¤§è§„æ¨¡å®ä¾‹ç‰¹å®šæè¿°æ€§å­—å¹•ï¼Œé™åˆ¶äº†å…¶å¯¹ç»†ç²’åº¦ç”Ÿç‰©å­¦ç‰¹å¾çš„ç†è§£ã€‚
2. BioCAPåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆå­—å¹•ï¼Œç»“åˆç»´åŸºç™¾ç§‘ä¿¡æ¯å’Œåˆ†ç±»å•å…ƒå®šåˆ¶æ ¼å¼ï¼Œæå‡å­—å¹•è´¨é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒBioCAPåœ¨ç‰©ç§åˆ†ç±»å’Œæ–‡æœ¬-å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†æè¿°æ€§å­—å¹•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢ç´¢äº†æè¿°æ€§å­—å¹•ä½œä¸ºç”Ÿç‰©å­¦å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„é¢å¤–ç›‘ç£æ¥æºã€‚å›¾åƒå’Œå­—å¹•å¯ä»¥è¢«è§†ä¸ºç‰©ç§æ½œåœ¨å½¢æ€ç©ºé—´ä¸­çš„äº’è¡¥æ ·æœ¬ï¼Œå„è‡ªæ•æ‰ç‰¹å®šçš„ç”Ÿç‰©å­¦ç‰¹å¾ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥å­—å¹•ï¼Œèƒ½å¤Ÿä¿ƒè¿›æ¨¡å‹ä¸å…±äº«æ½œåœ¨ç»“æ„çš„å¯¹é½ï¼Œä»è€Œå¼ºè°ƒæ½œåœ¨çš„è¯Šæ–­ç‰¹å¾ï¼ŒåŒæ—¶æŠ‘åˆ¶è™šå‡ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡è·å–å¿ å®çš„ã€ç‰¹å®šäºå®ä¾‹çš„å­—å¹•æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ä¸è®¸å¤šå…¶ä»–ç§‘å­¦é¢†åŸŸç›¸æ¯”ï¼Œè¿™ä¸€è¦æ±‚é™åˆ¶äº†è‡ªç„¶è¯­è¨€ç›‘ç£åœ¨ç”Ÿç‰©æœ‰æœºä½“å­¦ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆåˆæˆå­—å¹•æ¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œå¹¶ä»¥ç»´åŸºç™¾ç§‘è¡ç”Ÿçš„è§†è§‰ä¿¡æ¯å’Œé’ˆå¯¹ç‰¹å®šåˆ†ç±»å•å…ƒå®šåˆ¶çš„æ ¼å¼ç¤ºä¾‹ä¸ºæŒ‡å¯¼ã€‚è¿™äº›é¢†åŸŸç‰¹å®šçš„ä¸Šä¸‹æ–‡æœ‰åŠ©äºå‡å°‘å¹»è§‰ï¼Œå¹¶äº§ç”Ÿå‡†ç¡®çš„ã€åŸºäºå®ä¾‹çš„æè¿°æ€§å­—å¹•ã€‚åˆ©ç”¨è¿™äº›å­—å¹•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†BioCAPï¼ˆå³å¸¦æœ‰å­—å¹•çš„BioCLIPï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿç‰©å­¦åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ•æ‰ä¸°å¯Œçš„è¯­ä¹‰ï¼Œå¹¶åœ¨ç‰©ç§åˆ†ç±»å’Œæ–‡æœ¬-å›¾åƒæ£€ç´¢æ–¹é¢å–å¾—ä¼˜å¼‚çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¯æ˜äº†æè¿°æ€§å­—å¹•åœ¨è¿æ¥ç”Ÿç‰©å­¦å›¾åƒä¸å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹æ–¹é¢çš„ä»·å€¼ï¼Œè¶…è¶Šäº†æ ‡ç­¾çš„ç›‘ç£ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç”Ÿç‰©å­¦å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸»è¦ä¾èµ–å›¾åƒæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œç¼ºä¹å¯¹å›¾åƒå†…å®¹çš„ç»†ç²’åº¦è¯­ä¹‰ç†è§£ã€‚è·å–å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„ç”Ÿç‰©å­¦å›¾åƒæè¿°æ€§å­—å¹•éå¸¸å›°éš¾ï¼Œé™åˆ¶äº†è‡ªç„¶è¯­è¨€ç›‘ç£çš„åº”ç”¨ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æ•æ‰æ½œåœ¨çš„è¯Šæ–­ç‰¹å¾ï¼Œå®¹æ˜“å—åˆ°è™šå‡ç›¸å…³æ€§çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆåˆæˆå­—å¹•ï¼Œå°†å›¾åƒå’Œå­—å¹•è§†ä¸ºç‰©ç§æ½œåœ¨å½¢æ€ç©ºé—´ä¸­çš„äº’è¡¥æ ·æœ¬ï¼Œé€šè¿‡å­—å¹•æä¾›é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹ç”Ÿç‰©å­¦ç‰¹å¾çš„ç†è§£ã€‚é€šè¿‡é¢†åŸŸçŸ¥è¯†å¼•å¯¼å­—å¹•ç”Ÿæˆï¼Œå‡å°‘å¹»è§‰ï¼Œæé«˜å­—å¹•è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBioCAPåŸºäºBioCLIPæ¡†æ¶ï¼Œä¸»è¦åŒ…æ‹¬å›¾åƒç¼–ç å™¨ã€æ–‡æœ¬ç¼–ç å™¨å’Œå­—å¹•ç”Ÿæˆæ¨¡å—ã€‚å›¾åƒç¼–ç å™¨è´Ÿè´£æå–å›¾åƒç‰¹å¾ï¼Œæ–‡æœ¬ç¼–ç å™¨è´Ÿè´£æå–æ–‡æœ¬ç‰¹å¾ï¼Œå­—å¹•ç”Ÿæˆæ¨¡å—ä½¿ç”¨MLLMç”Ÿæˆä¸å›¾åƒå†…å®¹ç›¸å…³çš„æè¿°æ€§å­—å¹•ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œä½¿å›¾åƒå’Œå­—å¹•åœ¨ç‰¹å¾ç©ºé—´ä¸­å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨MLLMç”Ÿæˆé«˜è´¨é‡çš„ç”Ÿç‰©å­¦å›¾åƒåˆæˆå­—å¹•ï¼Œå¹¶å°†å…¶ä½œä¸ºé¢å¤–çš„ç›‘ç£ä¿¡å·ç”¨äºè®­ç»ƒç”Ÿç‰©å­¦åŸºç¡€æ¨¡å‹ã€‚é€šè¿‡ç»´åŸºç™¾ç§‘çŸ¥è¯†å’Œåˆ†ç±»å•å…ƒå®šåˆ¶æ ¼å¼çš„å¼•å¯¼ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆå­—å¹•çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ï¼Œå…‹æœäº†ç›´æ¥ä½¿ç”¨é€šç”¨MLLMç”Ÿæˆå­—å¹•æ—¶å®¹æ˜“å‡ºç°å¹»è§‰çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šå­—å¹•ç”Ÿæˆæ¨¡å—çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç»´åŸºç™¾ç§‘ä¸­ä¸å›¾åƒç›¸å…³çš„æ–‡æœ¬ä¿¡æ¯ä½œä¸ºMLLMçš„è¾“å…¥ï¼Œä»¥æä¾›é¢†åŸŸçŸ¥è¯†ï¼›2) æ ¹æ®ä¸åŒçš„åˆ†ç±»å•å…ƒå®šåˆ¶å­—å¹•æ ¼å¼ï¼Œä¾‹å¦‚ï¼Œå¯¹äºæ¤ç‰©ï¼Œå¯ä»¥è¦æ±‚å­—å¹•åŒ…å«å¶ç‰‡å½¢çŠ¶ã€èŠ±æœµé¢œè‰²ç­‰ä¿¡æ¯ï¼›3) ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±å›¾åƒå’Œå­—å¹•åœ¨ç‰¹å¾ç©ºé—´ä¸­å¯¹é½ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

BioCAPåœ¨ç‰©ç§åˆ†ç±»å’Œæ–‡æœ¬-å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ç‰©ç§åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒBioCAPçš„å‡†ç¡®ç‡æ¯”åŸºçº¿æ¨¡å‹æé«˜äº†5%ä»¥ä¸Šã€‚åœ¨æ–‡æœ¬-å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒBioCAPçš„å¬å›ç‡æé«˜äº†8%ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨åˆæˆå­—å¹•å¯ä»¥æœ‰æ•ˆåœ°æé«˜ç”Ÿç‰©å­¦åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BioCAPåœ¨ç”Ÿç‰©å¤šæ ·æ€§ç ”ç©¶ã€ç‰©ç§é‰´å®šã€ç”Ÿæ€ç›‘æµ‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å‡†ç¡®åœ°è¯†åˆ«å’Œåˆ†ç±»ç‰©ç§ï¼Œåˆ†æç‰©ç§é—´çš„å…³ç³»ï¼Œå¹¶ç›‘æµ‹ç”Ÿæ€ç³»ç»Ÿçš„å˜åŒ–ã€‚æ­¤å¤–ï¼ŒBioCAPè¿˜å¯ä»¥ç”¨äºå¼€å‘æ™ºèƒ½åŒ–çš„ç”Ÿç‰©å­¦å›¾åƒæœç´¢å’Œæ£€ç´¢ç³»ç»Ÿï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿæ‰¾åˆ°æ‰€éœ€çš„å›¾åƒä¿¡æ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work investigates descriptive captions as an additional source of supervision for biological multimodal foundation models. Images and captions can be viewed as complementary samples from the latent morphospace of a species, each capturing certain biological traits. Incorporating captions during training encourages alignment with this shared latent structure, emphasizing potentially diagnostic characters while suppressing spurious correlations. The main challenge, however, lies in obtaining faithful, instance-specific captions at scale. This requirement has limited the utilization of natural language supervision in organismal biology compared with many other scientific domains. We complement this gap by generating synthetic captions with multimodal large language models (MLLMs), guided by Wikipedia-derived visual information and taxon-tailored format examples. These domain-specific contexts help reduce hallucination and yield accurate, instance-based descriptive captions. Using these captions, we train BioCAP (i.e., BioCLIP with Captions), a biological foundation model that captures rich semantics and achieves strong performance in species classification and text-image retrieval. These results demonstrate the value of descriptive captions beyond labels in bridging biological images with multimodal foundation models.

