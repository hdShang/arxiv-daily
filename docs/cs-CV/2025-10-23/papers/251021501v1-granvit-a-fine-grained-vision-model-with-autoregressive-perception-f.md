---
layout: default
title: GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs
---

# GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21501" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21501v1</a>
  <a href="https://arxiv.org/pdf/2510.21501.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21501v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21501v1', 'GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanghao Zheng, Bowen Shi, Mingxing Xu, Ruoyu Sun, Peisen Zhao, Zhibo Zhang, Wenrui Dai, Junni Zou, Hongkai Xiong, Xiaopeng Zhang, Qi Tian

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23

**å¤‡æ³¨**: 21 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GranViTï¼šé¢å‘MLLMçš„ç»†ç²’åº¦è§†è§‰æ¨¡å‹ï¼Œé€šè¿‡è‡ªå›å½’æ„ŸçŸ¥æå‡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»†ç²’åº¦è§†è§‰æ¨¡å‹` `è§†è§‰Transformer` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è‡ªå›å½’è®­ç»ƒ` `åŒºåŸŸçº§æ ‡æ³¨` `è§†è§‰é—®ç­”` `OCRç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰ç¼–ç å™¨ä¾§é‡äºå…¨å±€å›¾åƒè¡¨ç¤ºï¼Œå¿½ç•¥äº†ç»†ç²’åº¦çš„åŒºåŸŸåˆ†æï¼Œé™åˆ¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
2. GranViTé€šè¿‡åŒºåŸŸçº§åˆ«çš„è‡ªå›å½’è®­ç»ƒï¼Œå°†ç»†ç²’åº¦ç‰¹å¾æå–ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å¯¹é½ï¼Œä»è€Œæå‡è§†è§‰ç¼–ç å™¨çš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. GranViTåœ¨ç»†ç²’åº¦è¯†åˆ«ã€å¤šæ¨¡æ€VQAå’ŒOCRç†è§£æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„å¯è¿ç§»æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºGranViTï¼Œä¸€ç§æ–°å‹Vision Transformerï¼Œå®ƒé›†æˆäº†ç»†ç²’åº¦ç‰¹å¾æå–å’Œä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰å¯¹é½ï¼Œé€šè¿‡åŒºåŸŸçº§åˆ«çš„è‡ªå›å½’è®­ç»ƒå®ç°ã€‚ä¸ºäº†è¿›è¡Œå¤§è§„æ¨¡çš„ç»†ç²’åº¦é¢„è®­ç»ƒï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«200ä¸‡å¼ è‡ªç„¶å›¾åƒå’ŒOCRå›¾åƒçš„æ•°æ®é›†Gran-29Mï¼Œå¹¶é…æœ‰è¶…è¿‡1.8äº¿ä¸ªé«˜è´¨é‡çš„åŒºåŸŸçº§æ ‡æ³¨ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé¢„è®­ç»ƒ-é€‚åº”æ¡†æ¶ï¼Œä»¥åŠä¸€ä¸ªè‡ªè’¸é¦æœºåˆ¶ï¼Œä»¥åœ¨Gran-29Mä¸Šè®­ç»ƒç»†ç²’åº¦çš„GranViTã€‚æˆ‘ä»¬å……åˆ†åˆ©ç”¨Gran-29Mä¸­çš„ç»†ç²’åº¦æ ‡æ³¨ï¼Œé‡‡ç”¨bounding-boxåˆ°captionçš„å›å½’æ¥å¢å¼ºè§†è§‰ç¼–ç å™¨åœ¨é¢„è®­ç»ƒä¸­çš„å±€éƒ¨è§†è§‰è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨captionåˆ°bounding-boxçš„å›å½’æ¥æé«˜LLMåœ¨é€‚åº”è¿‡ç¨‹ä¸­çš„è§†è§‰ç‰¹å¾åˆ©ç”¨ç‡å’Œå®šä½èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç»“åˆè‡ªè’¸é¦æœºåˆ¶ï¼Œå¯¹è§†è§‰ç¼–ç å™¨æ–½åŠ æ˜¾å¼çš„å®šä½çº¦æŸï¼Œä»¥å¢å¼ºå…¶åŒºåŸŸæ¨ç†èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGranViTè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰ç¼–ç å™¨ï¼Œå¹¶åœ¨ä¸åŒçš„LLMä¸Šå®ç°äº†å¼ºå¤§çš„å¯è¿ç§»æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨ç»†ç²’åº¦è¯†åˆ«ã€å¤šæ¨¡æ€VQAå’ŒOCRç†è§£æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰ç¼–ç å™¨åœ¨å¤„ç†è§†è§‰é—®ç­”å’Œæ¨ç†ç­‰è§†è§‰è¯­è¨€ä»»åŠ¡æ—¶ï¼Œç”±äºç¼ºä¹å¯¹å›¾åƒç»†ç²’åº¦åŒºåŸŸçš„åˆ†æèƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºç¼ºä¹è¶³å¤Ÿè§„æ¨¡çš„ç»†ç²’åº¦æ ‡æ³¨æ•°æ®ä»¥åŠç›¸åº”çš„é¢„è®­ç»ƒèŒƒå¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGranViTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„ç»†ç²’åº¦æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶ç»“åˆåŒºåŸŸçº§åˆ«çš„è‡ªå›å½’è®­ç»ƒï¼Œæ¥æå‡è§†è§‰ç¼–ç å™¨å¯¹å›¾åƒç»†ç²’åº¦åŒºåŸŸçš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ã€‚é€šè¿‡é¢„è®­ç»ƒå’Œé€‚åº”é˜¶æ®µçš„bounding-boxå’Œcaptionä¹‹é—´çš„å›å½’ï¼Œå®ç°è§†è§‰ç‰¹å¾çš„ç²¾ç¡®å®šä½å’Œè¯­ä¹‰å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGranViTçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤§è§„æ¨¡ç»†ç²’åº¦æ ‡æ³¨æ•°æ®é›†Gran-29Mçš„æ„å»ºï¼›2) åŸºäºGran-29Mçš„é¢„è®­ç»ƒé˜¶æ®µï¼Œåˆ©ç”¨bounding-boxåˆ°captionçš„å›å½’å¢å¼ºå±€éƒ¨è§†è§‰è¡¨ç¤ºï¼›3) é€‚åº”é˜¶æ®µï¼Œåˆ©ç”¨captionåˆ°bounding-boxçš„å›å½’æå‡LLMçš„è§†è§‰ç‰¹å¾åˆ©ç”¨ç‡å’Œå®šä½èƒ½åŠ›ï¼›4) è‡ªè’¸é¦æœºåˆ¶ï¼Œé€šè¿‡æ˜¾å¼çš„å®šä½çº¦æŸå¢å¼ºåŒºåŸŸæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šGranViTçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ„å»ºäº†å¤§è§„æ¨¡çš„ç»†ç²’åº¦æ ‡æ³¨æ•°æ®é›†Gran-29Mï¼Œä¸ºç»†ç²’åº¦è§†è§‰æ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ•°æ®åŸºç¡€ï¼›2) æå‡ºäº†åŒºåŸŸçº§åˆ«çš„è‡ªå›å½’è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡bounding-boxå’Œcaptionä¹‹é—´çš„å›å½’ï¼Œå®ç°äº†è§†è§‰ç‰¹å¾çš„ç²¾ç¡®å®šä½å’Œè¯­ä¹‰å¯¹é½ï¼›3) å¼•å…¥äº†è‡ªè’¸é¦æœºåˆ¶ï¼Œé€šè¿‡æ˜¾å¼çš„å®šä½çº¦æŸå¢å¼ºäº†æ¨¡å‹çš„åŒºåŸŸæ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒGranViTæ›´åŠ å…³æ³¨å›¾åƒçš„ç»†ç²’åº¦åŒºåŸŸä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæœ‰æ•ˆå¯¹é½ã€‚

**å…³é”®è®¾è®¡**ï¼šGranViTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) Gran-29Mæ•°æ®é›†çš„æ„å»ºï¼ŒåŒ…å«200ä¸‡å¼ å›¾åƒå’Œ1.8äº¿ä¸ªåŒºåŸŸçº§æ ‡æ³¨ï¼›2) é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨bounding-boxåˆ°captionçš„å›å½’æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–è§†è§‰ç¼–ç å™¨çš„å±€éƒ¨è§†è§‰è¡¨ç¤ºï¼›3) é€‚åº”é˜¶æ®µé‡‡ç”¨captionåˆ°bounding-boxçš„å›å½’æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–LLMçš„è§†è§‰ç‰¹å¾åˆ©ç”¨ç‡å’Œå®šä½èƒ½åŠ›ï¼›4) è‡ªè’¸é¦æœºåˆ¶ä¸­ï¼Œé€šè¿‡å¼•å…¥é¢å¤–çš„å®šä½çº¦æŸæŸå¤±å‡½æ•°ï¼Œå¢å¼ºæ¨¡å‹çš„åŒºåŸŸæ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GranViTåœ¨ç»†ç²’åº¦è¯†åˆ«ã€å¤šæ¨¡æ€VQAå’ŒOCRç†è§£ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰ç¼–ç å™¨ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å¤šä¸ªbenchmarkæ•°æ®é›†ä¸Šå®ç°äº†state-of-the-artçš„ç»“æœï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„å¯è¿ç§»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGranViTèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–å›¾åƒçš„ç»†ç²’åº¦ç‰¹å¾ï¼Œå¹¶å°†å…¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæœ‰æ•ˆå¯¹é½ï¼Œä»è€Œæå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GranViTåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºè§†è§‰é—®ç­”ã€å›¾åƒæè¿°ã€ç›®æ ‡æ£€æµ‹ã€OCRè¯†åˆ«ç­‰é¢†åŸŸã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å›¾åƒå†…å®¹ï¼Œä»è€Œæé«˜å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡çš„æ€§èƒ½ã€‚æœªæ¥ï¼ŒGranViTå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­ï¼Œå¹¶ä¸å…¶ä»–æ¨¡æ€çš„ä¿¡æ¯è¿›è¡Œèåˆï¼Œå®ç°æ›´å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision encoders are indispensable for allowing impressive performance of Multi-modal Large Language Models (MLLMs) in vision language tasks such as visual question answering and reasoning. However, existing vision encoders focus on global image representations but overlook fine-grained regional analysis. They are limited in fine grained perception due to the scarcity of fine grained annotated data and the lack of a fine grained pre-training paradigm. In this paper, we propose GranViT, a novel Vision Transformer that integrates fine-grained feature extraction with semantic alignment to Large Language Models (LLMs) via region level autoregressive training. We first construct Gran-29M, a dataset comprising 2million natural and OCR images paired with over 180 million high-quality region-level annotations, to enable large scale fine grained pretraining. Consequently, we develop a pretraining-adaptation framework along with a self distillation mechanism to train fine-grained GranViT on Gran-29M. We sufficiently exploit the fine-grained annotations from Gran-29M to resort to bounding-box-to-caption regression to enhance localized visual representation of the vision encoder in the pretraining and caption-to-bounding-box regression to improve vision feature utilization and localization for LLM in the adaptation. We further incorporate a self distillation mechanism that imposes explicit localization constraints on the vision encoder to strengthen its regional reasoning capability. Extensive experiments show that GranViT surpasses existing vision encoders and attains strong transferability to varying LLMs. Remarkably, it achieves state-of-the-art results on fine-grained recognition, multimodal VQA, and OCR understanding.

