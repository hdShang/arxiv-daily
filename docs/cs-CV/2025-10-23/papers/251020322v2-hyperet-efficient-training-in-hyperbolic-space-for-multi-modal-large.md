---
layout: default
title: HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models
---

# HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20322" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20322v2</a>
  <a href="https://arxiv.org/pdf/2510.20322.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20322v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.20322v2', 'HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zelin Peng, Zhengqin Xu, Qingyang Liu, Xiaokang Yang, Wei Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23 (æ›´æ–°: 2025-10-29)

**å¤‡æ³¨**: Accepted by NeurIPS2025 (Oral)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HyperETï¼šé€šè¿‡åŒæ›²ç©ºé—´é«˜æ•ˆè®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡è·¨æ¨¡æ€å¯¹é½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `åŒæ›²ç©ºé—´` `è·¨æ¨¡æ€å¯¹é½` `é«˜æ•ˆè®­ç»ƒ` `è§†è§‰ç¼–ç å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMè®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œä¸»è¦åŸå› æ˜¯è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€åœ¨å¤šç²’åº¦çº§åˆ«ä¸Šå¯¹é½ä¸è¶³ã€‚
2. HyperETåˆ©ç”¨åŒæ›²ç©ºé—´çš„å±‚æ¬¡ç»“æ„å»ºæ¨¡èƒ½åŠ›ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´åŒæ›²åŠå¾„ï¼Œå®ç°è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€åœ¨ä»»æ„ç²’åº¦ä¸Šçš„å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHyperETåœ¨å¤šä¸ªMLLMåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»¥ä¸åˆ°1%çš„é¢å¤–å‚æ•°ï¼Œæ˜¾è‘—æå‡äº†é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)å·²æˆä¸ºå¯¹é½è§†è§‰å’Œæ–‡æœ¬ç†è§£çš„ä¸€ç§å˜é©æ€§æ–¹æ³•ã€‚å®ƒä»¬é€šå¸¸éœ€è¦æé«˜çš„è®¡ç®—èµ„æº(ä¾‹å¦‚ï¼Œæ•°åƒä¸ªGPU)è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å¤šç²’åº¦çº§åˆ«çš„è·¨æ¨¡æ€å¯¹é½ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ç§ä½æ•ˆç‡çš„ä¸€ä¸ªå…³é”®æ¥æºåœ¨äºå®ƒä»¬å¹¿æ³›é…å¤‡çš„è§†è§‰ç¼–ç å™¨ï¼Œä¾‹å¦‚CLIPå’ŒSAMï¼Œè¿™äº›ç¼–ç å™¨ç¼ºä¹ä¸è¯­è¨€åœ¨å¤šç²’åº¦çº§åˆ«ä¸Šçš„å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åŒæ›²ç©ºé—´ï¼Œå®ƒå›ºæœ‰åœ°å»ºæ¨¡äº†å±‚æ¬¡ç»“æ„ï¼Œå› æ­¤æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„æ¡†æ¶ï¼Œç”¨äºå¼¥åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´åœ¨ä»»æ„ç²’åº¦çº§åˆ«ä¸Šçš„ç²’åº¦å·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºMLLMçš„é«˜æ•ˆè®­ç»ƒèŒƒå¼ï¼Œç§°ä¸ºHyperETï¼Œå®ƒå¯ä»¥é€šè¿‡åŒæ›²ç©ºé—´ä¸­çš„åŠ¨æ€åŒæ›²åŠå¾„è°ƒæ•´æ¥ä¼˜åŒ–è§†è§‰è¡¨ç¤ºï¼Œä½¿å…¶ä¸æ–‡æœ¬å¯¹åº”ç‰©åœ¨ä»»æ„ç²’åº¦çº§åˆ«ä¸Šå¯¹é½ã€‚HyperETé‡‡ç”¨å…·æœ‰MÃ¶biusä¹˜æ³•è¿ç®—çš„å¯å­¦ä¹ çŸ©é˜µï¼Œé€šè¿‡ä¸‰ç§æœ‰æ•ˆçš„é…ç½®å®ç°ï¼šå¯¹è§’ç¼©æ”¾çŸ©é˜µã€å—å¯¹è§’çŸ©é˜µå’Œå¸¦çŠ¶çŸ©é˜µï¼Œæä¾›äº†ä¸€ç§çµæ´»è€Œé«˜æ•ˆçš„å‚æ•°åŒ–ç­–ç•¥ã€‚è·¨å¤šä¸ªMLLMåŸºå‡†çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHyperETå§‹ç»ˆå¦‚ä¸€åœ°æ”¹è¿›äº†ç°æœ‰çš„é¢„è®­ç»ƒå’Œå¾®è°ƒMLLMï¼Œä¸”é™„åŠ å‚æ•°ä¸åˆ°1%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œä¸»è¦ç“¶é¢ˆåœ¨äºè§†è§‰ç¼–ç å™¨ï¼ˆå¦‚CLIPå’ŒSAMï¼‰åœ¨å¤šç²’åº¦çº§åˆ«ä¸Šä¸è¯­è¨€æ¨¡å‹çš„å¯¹é½ä¸è¶³ã€‚è¿™å¯¼è‡´æ¨¡å‹éœ€è¦æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºæ‰èƒ½å­¦ä¹ åˆ°æœ‰æ•ˆçš„è·¨æ¨¡æ€è¡¨ç¤ºã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨è®¡ç®—æ•ˆç‡å’Œè·¨æ¨¡æ€å¯¹é½ç²¾åº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHyperETçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨åŒæ›²ç©ºé—´çš„å›ºæœ‰å±‚æ¬¡ç»“æ„å»ºæ¨¡èƒ½åŠ›ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ˜ å°„åˆ°åŒæ›²ç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡åŠ¨æ€è°ƒæ•´åŒæ›²åŠå¾„æ¥æ§åˆ¶å¯¹é½çš„ç²’åº¦ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åœ¨ä¸åŒçš„æŠ½è±¡å±‚æ¬¡ä¸Šå¯¹é½è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œä»è€Œæé«˜è·¨æ¨¡æ€ç†è§£èƒ½åŠ›ã€‚é€‰æ‹©åŒæ›²ç©ºé—´æ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿè‡ªç„¶åœ°è¡¨ç¤ºå±‚æ¬¡å…³ç³»ï¼Œæ›´é€‚åˆå»ºæ¨¡è§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„å¤æ‚å…³è”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHyperETçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æå–è§†è§‰ç‰¹å¾ï¼›2) å°†è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾æ˜ å°„åˆ°åŒæ›²ç©ºé—´ï¼›3) é€šè¿‡å¯å­¦ä¹ çš„çŸ©é˜µï¼ˆå¯¹è§’ç¼©æ”¾çŸ©é˜µã€å—å¯¹è§’çŸ©é˜µæˆ–å¸¦çŠ¶çŸ©é˜µï¼‰è¿›è¡ŒMÃ¶biusä¹˜æ³•è¿ç®—ï¼Œè°ƒæ•´è§†è§‰ç‰¹å¾åœ¨åŒæ›²ç©ºé—´ä¸­çš„ä½ç½®ï¼›4) ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±æˆ–å…¶ä»–åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„å¯¹é½ï¼›5) é€šè¿‡åŠ¨æ€è°ƒæ•´åŒæ›²åŠå¾„ï¼Œæ§åˆ¶å¯¹é½çš„ç²’åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šHyperETçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨åŒæ›²ç©ºé—´è¿›è¡Œè·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶å¼•å…¥äº†åŠ¨æ€åŒæ›²åŠå¾„è°ƒæ•´æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ç›¸æ¯”ï¼ŒåŒæ›²ç©ºé—´æ›´é€‚åˆå»ºæ¨¡å±‚æ¬¡ç»“æ„æ•°æ®ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰è§†è§‰å’Œæ–‡æœ¬ä¹‹é—´çš„å¤šç²’åº¦å…³ç³»ã€‚åŠ¨æ€åŒæ›²åŠå¾„è°ƒæ•´å…è®¸æ¨¡å‹æ ¹æ®ä¸åŒçš„ä»»åŠ¡å’Œæ•°æ®ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´å¯¹é½çš„ç²’åº¦ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒHyperETé‡‡ç”¨çš„MÃ¶biusä¹˜æ³•è¿ç®—å’Œå¯å­¦ä¹ çŸ©é˜µï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”çµæ´»çš„å‚æ•°åŒ–ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šHyperETçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨MÃ¶biusä¹˜æ³•è¿ç®—å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°åŒæ›²ç©ºé—´ï¼›2) è®¾è®¡äº†ä¸‰ç§ä¸åŒçš„å¯å­¦ä¹ çŸ©é˜µç»“æ„ï¼ˆå¯¹è§’ç¼©æ”¾çŸ©é˜µã€å—å¯¹è§’çŸ©é˜µå’Œå¸¦çŠ¶çŸ©é˜µï¼‰ï¼Œä»¥æ§åˆ¶å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼›3) é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾åœ¨åŒæ›²ç©ºé—´ä¸­å¯¹é½ï¼›4) å¼•å…¥åŠ¨æ€åŒæ›²åŠå¾„è°ƒæ•´æœºåˆ¶ï¼Œæ ¹æ®è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´åŒæ›²åŠå¾„çš„å¤§å°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHyperETåœ¨å¤šä¸ªMLLMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”ä»…éœ€ä¸åˆ°1%çš„é¢å¤–å‚æ•°ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒHyperETçš„æ€§èƒ½æå‡è¶…è¿‡äº†5%ã€‚ä¸ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´æ–¹æ³•ç›¸æ¯”ï¼ŒHyperETèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¯¹é½è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è·¨æ¨¡æ€ç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HyperETå¯åº”ç”¨äºå„ç§å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€è·¨æ¨¡æ€æ£€ç´¢ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œé™ä½è®­ç»ƒæˆæœ¬ã€‚æœªæ¥ï¼ŒHyperETæœ‰æœ›åº”ç”¨äºç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—ç­‰åœºæ™¯ï¼Œä¿ƒè¿›å¤šæ¨¡æ€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. HyperET employs learnable matrices with MÃ¶bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that HyperET consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\% additional parameters.

