---
layout: default
title: "Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection"
---

# Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20214" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20214v1</a>
  <a href="https://arxiv.org/pdf/2510.20214.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20214v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.20214v1', 'Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Talha Ilyas, Duong Nhu, Allison Thomas, Arie Levin, Lim Wei Yap, Shu Gong, David Vera Anaya, Yiwen Jiang, Deval Mehta, Ritesh Warty, Vinayak Smith, Maya Reddy, Euan Wallace, Wenlong Cheng, Zongyuan Ge, Faezeh Marzbanrad

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23

**å¤‡æ³¨**: This is the preprint version of the manuscript submitted to IEEE Journal of Biomedical and Health Informatics (JBHI) for review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCURLæ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œèƒå„¿è¶…å£°è§†é¢‘ä¸­çš„èƒåŠ¨æ£€æµ‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `èƒå„¿è¿åŠ¨æ£€æµ‹` `å¯¹æ¯”å­¦ä¹ ` `è‡ªç›‘ç£å­¦ä¹ ` `è¶…å£°è§†é¢‘` `äº§å‰è¯Šæ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸèƒåŠ¨æ£€æµ‹æ–¹æ³•ä¸»è§‚æ€§å¼ºã€å‡†ç¡®ç‡ä½ï¼Œéš¾ä»¥æœ‰æ•ˆè¯„ä¼°èƒå„¿å¥åº·çŠ¶å†µï¼Œå­˜åœ¨ä¸´åºŠéœ€æ±‚ç¼ºå£ã€‚
2. CURLæ¡†æ¶åˆ©ç”¨ç©ºé—´å’Œæ—¶é—´å¯¹æ¯”å­¦ä¹ ï¼Œä»æ— æ ‡ç­¾è¶…å£°è§†é¢‘ä¸­å­¦ä¹ é²æ£’çš„èƒåŠ¨è¡¨å¾ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCURLåœ¨èƒåŠ¨æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œçµæ•åº¦è¾¾åˆ°78.01%ï¼ŒAUROCè¾¾åˆ°81.60%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡†ç¡®çš„èƒåŠ¨(FM)æ£€æµ‹å¯¹äºè¯„ä¼°äº§å‰å¥åº·è‡³å…³é‡è¦ï¼Œå› ä¸ºå¼‚å¸¸çš„è¿åŠ¨æ¨¡å¼å¯èƒ½è¡¨æ˜æ½œåœ¨çš„å¹¶å‘ç—‡ï¼Œå¦‚èƒç›˜åŠŸèƒ½éšœç¢æˆ–èƒå„¿çª˜è¿«ã€‚ä¼ ç»Ÿçš„èƒåŠ¨æ£€æµ‹æ–¹æ³•ï¼ŒåŒ…æ‹¬å­•å¦‡æ„ŸçŸ¥å’Œå¿ƒç”µç›‘æŠ¤(CTG)ï¼Œå­˜åœ¨ä¸»è§‚æ€§å’Œå‡†ç¡®æ€§æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶â€”â€”å¯¹æ¯”è¶…å£°è§†é¢‘è¡¨å¾å­¦ä¹ (CURL)ï¼Œç”¨äºä»å»¶é•¿çš„èƒå„¿è¶…å£°è§†é¢‘è®°å½•ä¸­è¿›è¡ŒèƒåŠ¨æ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åŒé‡å¯¹æ¯”æŸå¤±ï¼Œç»“åˆäº†ç©ºé—´å’Œæ—¶é—´å¯¹æ¯”å­¦ä¹ ï¼Œä»¥å­¦ä¹ é²æ£’çš„è¿åŠ¨è¡¨å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç‰¹å®šäºä»»åŠ¡çš„é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿åœ¨è‡ªç›‘ç£è®­ç»ƒæœŸé—´æœ‰æ•ˆåˆ†ç¦»è¿åŠ¨å’Œéè¿åŠ¨ç‰‡æ®µï¼ŒåŒæ—¶é€šè¿‡æ¦‚ç‡å¾®è°ƒæ–¹æ³•å®ç°å¯¹ä»»æ„é•¿åº¦è¶…å£°è®°å½•çš„çµæ´»æ¨ç†ã€‚åœ¨åŒ…å«92åå—è¯•è€…ã€æ¯äºº30åˆ†é’Ÿè¶…å£°ä¼šè¯çš„å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒCURLå®ç°äº†78.01%çš„çµæ•åº¦å’Œ81.60%çš„AUROCï¼Œè¯æ˜äº†å…¶åœ¨å¯é å’Œå®¢è§‚çš„èƒåŠ¨åˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚è¿™äº›ç»“æœçªå‡ºäº†è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ åœ¨èƒå„¿è¿åŠ¨åˆ†æä¸­çš„æ½œåŠ›ï¼Œä¸ºæ”¹å–„äº§å‰ç›‘æµ‹å’Œä¸´åºŠå†³ç­–é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³èƒå„¿è¶…å£°è§†é¢‘ä¸­èƒåŠ¨æ£€æµ‹çš„è‡ªåŠ¨åŒ–å’Œå®¢è§‚åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚å­•å¦‡æ„ŸçŸ¥å’Œå¿ƒç”µç›‘æŠ¤ï¼Œä¾èµ–ä¸»è§‚åˆ¤æ–­ï¼Œå‡†ç¡®æ€§ä¸è¶³ï¼Œä¸”æ— æ³•æä¾›è¿ç»­çš„ã€å®šé‡çš„èƒåŠ¨ä¿¡æ¯ã€‚è¿™é™åˆ¶äº†å¯¹èƒå„¿å¥åº·çŠ¶å†µçš„å‡†ç¡®è¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œä»å¤§é‡çš„æ— æ ‡ç­¾èƒå„¿è¶…å£°è§†é¢‘ä¸­å­¦ä¹ èƒåŠ¨çš„é€šç”¨è¡¨å¾ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤ŸåŒºåˆ†åŒ…å«èƒåŠ¨å’Œä¸åŒ…å«èƒåŠ¨çš„è§†é¢‘ç‰‡æ®µï¼Œä»è€Œæå–å‡ºä¸èƒåŠ¨ç›¸å…³çš„å…³é”®ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œé™ä½äº†æˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCURLæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š1) æ•°æ®é¢„å¤„ç†ï¼šå¯¹è¶…å£°è§†é¢‘è¿›è¡Œè£å‰ªå’Œæ ‡å‡†åŒ–ã€‚2) è‡ªç›‘ç£è®­ç»ƒï¼šä½¿ç”¨åŒé‡å¯¹æ¯”æŸå¤±ï¼ˆç©ºé—´å¯¹æ¯”æŸå¤±å’Œæ—¶é—´å¯¹æ¯”æŸå¤±ï¼‰è®­ç»ƒæ¨¡å‹ï¼Œå­¦ä¹ èƒåŠ¨è¡¨å¾ã€‚3) å¾®è°ƒï¼šä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ç‰¹å®šçš„èƒåŠ¨æ£€æµ‹ä»»åŠ¡ã€‚4) æ¨ç†ï¼šä½¿ç”¨æ¦‚ç‡å¾®è°ƒæ–¹æ³•ï¼Œå¯¹ä»»æ„é•¿åº¦çš„è¶…å£°è§†é¢‘è¿›è¡ŒèƒåŠ¨æ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åŒé‡å¯¹æ¯”æŸå¤±ï¼ŒåŒæ—¶è€ƒè™‘äº†ç©ºé—´å’Œæ—¶é—´ä¸Šçš„å¯¹æ¯”ä¿¡æ¯ï¼Œä»è€Œå­¦ä¹ æ›´é²æ£’çš„èƒåŠ¨è¡¨å¾ã€‚2) å¼•å…¥äº†ä»»åŠ¡ç‰¹å®šçš„é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿åœ¨è‡ªç›‘ç£è®­ç»ƒæœŸé—´æœ‰æ•ˆåˆ†ç¦»è¿åŠ¨å’Œéè¿åŠ¨ç‰‡æ®µã€‚3) æå‡ºäº†æ¦‚ç‡å¾®è°ƒæ–¹æ³•ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°å¤„ç†ä»»æ„é•¿åº¦çš„è¶…å£°è§†é¢‘ã€‚

**å…³é”®è®¾è®¡**ï¼šCURLä½¿ç”¨ResNet-18ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œæå–è§†é¢‘å¸§çš„ç‰¹å¾ã€‚ç©ºé—´å¯¹æ¯”æŸå¤±æ—¨åœ¨åŒºåˆ†åŒä¸€è§†é¢‘ä¸­ä¸åŒä½ç½®çš„å¸§ï¼Œè€Œæ—¶é—´å¯¹æ¯”æŸå¤±æ—¨åœ¨åŒºåˆ†åŒä¸€è§†é¢‘ä¸­ä¸åŒæ—¶é—´æ®µçš„å¸§ã€‚ä»»åŠ¡ç‰¹å®šçš„é‡‡æ ·ç­–ç•¥é€šè¿‡é€‰æ‹©åŒ…å«è¿åŠ¨å’Œä¸åŒ…å«è¿åŠ¨çš„è§†é¢‘ç‰‡æ®µæ¥å¹³è¡¡è®­ç»ƒæ•°æ®ã€‚æ¦‚ç‡å¾®è°ƒæ–¹æ³•ä½¿ç”¨sigmoidå‡½æ•°å°†æ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡å€¼ï¼Œä»è€Œå®ç°å¯¹ä»»æ„é•¿åº¦è§†é¢‘çš„èƒåŠ¨æ£€æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CURLåœ¨å†…éƒ¨æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œçµæ•åº¦è¾¾åˆ°78.01%ï¼ŒAUROCè¾¾åˆ°81.60%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒCURLèƒ½å¤Ÿæœ‰æ•ˆåœ°æ£€æµ‹èƒå„¿çš„è¿åŠ¨ï¼Œå¹¶ä¼˜äºä¼ ç»Ÿçš„ä¸»è§‚è¯„ä¼°æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºèƒå„¿å¥åº·ç›‘æµ‹æä¾›äº†ä¸€ç§å®¢è§‚ã€å¯é çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäº§å‰èƒå„¿å¥åº·ç›‘æµ‹ï¼Œä¸ºåŒ»ç”Ÿæä¾›å®¢è§‚ã€å®šé‡çš„èƒåŠ¨ä¿¡æ¯ï¼Œè¾…åŠ©è¯Šæ–­èƒå„¿çª˜è¿«ç­‰é—®é¢˜ã€‚é€šè¿‡è‡ªåŠ¨åŒ–èƒåŠ¨æ£€æµ‹ï¼Œå¯ä»¥å‡è½»åŒ»æŠ¤äººå‘˜çš„å·¥ä½œè´Ÿæ‹…ï¼Œæé«˜è¯Šæ–­æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›é›†æˆåˆ°ä¾¿æºå¼è¶…å£°è®¾å¤‡ä¸­ï¼Œå®ç°å®¶åº­åŒ–çš„èƒå„¿å¥åº·ç›‘æµ‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate fetal movement (FM) detection is essential for assessing prenatal health, as abnormal movement patterns can indicate underlying complications such as placental dysfunction or fetal distress. Traditional methods, including maternal perception and cardiotocography (CTG), suffer from subjectivity and limited accuracy. To address these challenges, we propose Contrastive Ultrasound Video Representation Learning (CURL), a novel self-supervised learning framework for FM detection from extended fetal ultrasound video recordings. Our approach leverages a dual-contrastive loss, incorporating both spatial and temporal contrastive learning, to learn robust motion representations. Additionally, we introduce a task-specific sampling strategy, ensuring the effective separation of movement and non-movement segments during self-supervised training, while enabling flexible inference on arbitrarily long ultrasound recordings through a probabilistic fine-tuning approach. Evaluated on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions, CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its potential for reliable and objective FM analysis. These results highlight the potential of self-supervised contrastive learning for fetal movement analysis, paving the way for improved prenatal monitoring and clinical decision-making.

