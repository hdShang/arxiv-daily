---
layout: default
title: VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models
---

# VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20994" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20994v1</a>
  <a href="https://arxiv.org/pdf/2510.20994.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20994v1" onclick="toggleFavorite(this, '2510.20994v1', 'VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jesimon Barreto, Carlos Caetano, AndrÃ© Araujo, William Robson Schwartz

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23

**å¤‡æ³¨**: Conference on Neural Information Processing Systems (NeurIPS 2025)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jesimonbarreto/VESSA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVESSAï¼šä¸€ç§åŸºäºè§†é¢‘å¯¹è±¡ä¸­å¿ƒçš„è‡ªç›‘ç£è§†è§‰åŸºç¡€æ¨¡å‹é€‚åº”æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰åŸºç¡€æ¨¡å‹` `è‡ªç›‘ç£å­¦ä¹ ` `é¢†åŸŸé€‚åº”` `è§†é¢‘æ•°æ®` `è‡ªè’¸é¦` `å‚æ•°é«˜æ•ˆé€‚åº”` `å¯¹è±¡ä¸­å¿ƒåŒ–` `å¤šè§†è§’å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰åŸºç¡€æ¨¡å‹åœ¨åˆ†å¸ƒåç§»å’Œæ ‡ç­¾ç¨€ç¼ºåœºæ™¯ä¸‹æ€§èƒ½ä¸‹é™ï¼Œæœ‰ç›‘ç£å¾®è°ƒä¸å¯è¡Œï¼Œè€Œé¢å‘è§†è§‰ç¼–ç å™¨çš„è‡ªç›‘ç£å­¦ä¹ é€‚åº”æ–¹æ³•æ•ˆæœä¸ä½³ã€‚
2. VESSAåˆ©ç”¨çŸ­è§†é¢‘ä¸­çš„å¤šè§†è§’å¯¹è±¡ä¿¡æ¯ï¼Œé€šè¿‡è‡ªè’¸é¦å­¦ä¹ ï¼Œä½¿æ¨¡å‹åœ¨æ— éœ€æ ‡æ³¨çš„æƒ…å†µä¸‹é€‚åº”æ–°é¢†åŸŸï¼Œæå‡é²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVESSAåœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸå§‹æ¨¡å‹å’Œå…¶ä»–é€‚åº”æ–¹æ³•ï¼Œæ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è§†è§‰åŸºç¡€æ¨¡å‹çš„è‡ªç›‘ç£å¾®è°ƒæ–°æ–¹æ³•ï¼Œç”¨äºè§£å†³æ¨¡å‹åœ¨åˆ†å¸ƒåç§»å’Œæ ‡ç­¾ç¨€ç¼ºé¢†åŸŸè¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åä¸ºVESSAï¼ˆVideo-based objEct-centric Self-Supervised Adaptationï¼‰ï¼Œåˆ©ç”¨çŸ­çš„å¤šè§†è§’å¯¹è±¡ä¸­å¿ƒè§†é¢‘ï¼Œæ— éœ€ä»»ä½•æ ‡æ³¨å³å¯å°†æ¨¡å‹é€‚åº”åˆ°æ–°é¢†åŸŸã€‚VESSAçš„è®­ç»ƒæŠ€æœ¯åŸºäºè‡ªè’¸é¦èŒƒå¼ï¼Œå…¶ä¸­é¢„æµ‹å¤´çš„ç²¾ç»†è°ƒæ•´å’Œå‚æ•°é«˜æ•ˆé€‚åº”æŠ€æœ¯çš„éƒ¨ç½²è‡³å…³é‡è¦ï¼Œå¦åˆ™æ¨¡å‹å¯èƒ½ä¼šè¿…é€Ÿé—å¿˜å…¶é¢„è®­ç»ƒçŸ¥è¯†å¹¶è¾¾åˆ°é€€åŒ–çŠ¶æ€ã€‚VESSAå—ç›Šäºæ¥è‡ªå¯¹è±¡ä¸­å¿ƒè§†é¢‘ä¸­ä¸åŒå¸§çš„å¤šè§†è§’å¯¹è±¡è§‚æµ‹ï¼Œé«˜æ•ˆåœ°å­¦ä¹ å¯¹å„ç§æ•è·æ¡ä»¶çš„é²æ£’æ€§ï¼Œè€Œæ— éœ€æ ‡æ³¨ã€‚é€šè¿‡åœ¨2ä¸ªæ•°æ®é›†ä¸Šå¯¹3ä¸ªè§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œå…¨é¢å®éªŒï¼ŒVESSAåœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ï¼Œä¼˜äºåŸºç¡€æ¨¡å‹å’Œä»¥å‰çš„é€‚åº”æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰åŸºç¡€æ¨¡å‹åœ¨é¢å¯¹æ•°æ®åˆ†å¸ƒåç§»çš„æ–°é¢†åŸŸæ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¼ ç»Ÿçš„æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•ä¾èµ–äºå¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œä½†åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­ï¼Œè·å–è¿™äº›æ ‡æ³¨æ•°æ®éå¸¸å›°éš¾ç”šè‡³ä¸å¯èƒ½ã€‚ç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè™½ç„¶åœ¨è¯­è¨€æ¨¡å‹é¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†åœ¨è§†è§‰ç¼–ç å™¨æ¨¡å‹ä¸Šçš„æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œå®¹æ˜“å‡ºç°ç¾éš¾æ€§é—å¿˜ç­‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVESSAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†é¢‘ä¸­çš„å¤šè§†è§’ä¿¡æ¯ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œè®©æ¨¡å‹å­¦ä¹ åˆ°å¯¹æ–°é¢†åŸŸæ•°æ®åˆ†å¸ƒçš„é²æ£’æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒVESSAåˆ©ç”¨å¯¹è±¡ä¸­å¿ƒåŒ–çš„çŸ­è§†é¢‘ï¼Œä»ä¸åŒçš„è§†è§’è§‚å¯ŸåŒä¸€ä¸ªå¯¹è±¡ï¼Œä»è€Œè®©æ¨¡å‹å­¦ä¹ åˆ°å¯¹è±¡åœ¨ä¸åŒè§†è§’ä¸‹çš„ä¸å˜æ€§ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦ä»»ä½•äººå·¥æ ‡æ³¨ï¼Œåªéœ€è¦å¤§é‡çš„æ— æ ‡æ³¨è§†é¢‘æ•°æ®å³å¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVESSAçš„æ•´ä½“æ¡†æ¶åŸºäºè‡ªè’¸é¦å­¦ä¹ ã€‚é¦–å…ˆï¼Œå°†è§†é¢‘æ•°æ®è¾“å…¥åˆ°è§†è§‰åŸºç¡€æ¨¡å‹ä¸­ï¼Œå¾—åˆ°ç‰¹å¾è¡¨ç¤ºã€‚ç„¶åï¼Œåˆ©ç”¨è¿™äº›ç‰¹å¾è¡¨ç¤ºä½œä¸ºâ€œæ•™å¸ˆâ€ä¿¡å·ï¼Œè®­ç»ƒä¸€ä¸ªæ–°çš„â€œå­¦ç”Ÿâ€æ¨¡å‹ã€‚å­¦ç”Ÿæ¨¡å‹çš„ç›®æ ‡æ˜¯å°½å¯èƒ½åœ°é€¼è¿‘æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºï¼Œä»è€Œå­¦ä¹ åˆ°æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚ä¸ºäº†é˜²æ­¢å­¦ç”Ÿæ¨¡å‹é—å¿˜é¢„è®­ç»ƒçš„çŸ¥è¯†ï¼ŒVESSAé‡‡ç”¨äº†å‚æ•°é«˜æ•ˆçš„é€‚åº”æŠ€æœ¯ï¼Œåªæ›´æ–°æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šVESSAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åˆ©ç”¨è§†é¢‘ä¸­çš„å¤šè§†è§’ä¿¡æ¯è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒVESSAä¸éœ€è¦äººå·¥è®¾è®¡å¤æ‚çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨è§†é¢‘æ•°æ®ä¸­çš„è‡ªç„¶ç›‘ç£ä¿¡å·ã€‚æ­¤å¤–ï¼ŒVESSAè¿˜é‡‡ç”¨äº†å‚æ•°é«˜æ•ˆçš„é€‚åº”æŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°é˜²æ­¢äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šVESSAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¯¹è±¡ä¸­å¿ƒåŒ–çš„è§†é¢‘æ•°æ®ï¼šç¡®ä¿è§†é¢‘ä¸­çš„å¯¹è±¡å§‹ç»ˆä½äºå›¾åƒä¸­å¿ƒï¼Œä»è€Œæ–¹ä¾¿æ¨¡å‹å­¦ä¹ å¯¹è±¡çš„ä¸å˜æ€§ç‰¹å¾ã€‚2) è‡ªè’¸é¦å­¦ä¹ æ¡†æ¶ï¼šåˆ©ç”¨æ•™å¸ˆæ¨¡å‹æä¾›ç›‘ç£ä¿¡å·ï¼Œå¼•å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ ã€‚3) å‚æ•°é«˜æ•ˆçš„é€‚åº”æŠ€æœ¯ï¼šåªæ›´æ–°æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚4) ç²¾ç»†è°ƒæ•´çš„é¢„æµ‹å¤´ï¼šé’ˆå¯¹ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œéœ€è¦å¯¹é¢„æµ‹å¤´è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VESSAåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå¯¹ä¸‰ä¸ªè§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜VESSAèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨ViT-B/16æ¨¡å‹åœ¨VTABæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ—¶ï¼ŒVESSAçš„æ€§èƒ½æ¯”åŸå§‹æ¨¡å‹æé«˜äº†5%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒVESSAè¿˜ä¼˜äºå…¶ä»–è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VESSAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€åŒ»ç–—å½±åƒåˆ†æç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ•°æ®åˆ†å¸ƒå¾€å¾€ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè€Œä¸”è·å–æ ‡æ³¨æ•°æ®éå¸¸å›°éš¾ã€‚VESSAå¯ä»¥å¸®åŠ©æ¨¡å‹å¿«é€Ÿé€‚åº”æ–°çš„æ•°æ®åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œä»è€Œæå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒVESSAè¿˜å¯ä»¥ç”¨äºæ„å»ºæ›´åŠ é€šç”¨çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å„ç§ä¸åŒçš„è§†è§‰ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models have advanced computer vision by enabling strong performance across diverse tasks through large-scale pretraining and supervised fine-tuning. However, they may underperform in domains with distribution shifts and scarce labels, where supervised fine-tuning may be infeasible. While continued self-supervised learning for model adaptation is common for generative language models, this strategy has not proven effective for vision-centric encoder models. To address this challenge, we introduce a novel formulation of self-supervised fine-tuning for vision foundation models, where the model is adapted to a new domain without requiring annotations, leveraging only short multi-view object-centric videos. Our method is referred to as VESSA: Video-based objEct-centric Self-Supervised Adaptation for visual foundation models. VESSA's training technique is based on a self-distillation paradigm, where it is critical to carefully tune prediction heads and deploy parameter-efficient adaptation techniques - otherwise, the model may quickly forget its pretrained knowledge and reach a degraded state. VESSA benefits significantly from multi-view object observations sourced from different frames in an object-centric video, efficiently learning robustness to varied capture conditions, without the need of annotations. Through comprehensive experiments with 3 vision foundation models on 2 datasets, VESSA demonstrates consistent improvements in downstream classification tasks, compared to the base models and previous adaptation methods. Code is publicly available at https://github.com/jesimonbarreto/VESSA.

