---
layout: default
title: VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models
---

# VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.20994" target="_blank" class="toolbar-btn">arXiv: 2510.20994v1</a>
    <a href="https://arxiv.org/pdf/2510.20994.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20994v1" 
            onclick="toggleFavorite(this, '2510.20994v1', 'VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jesimon Barreto, Carlos Caetano, Andr√© Araujo, William Robson Schwartz

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-23

**Â§áÊ≥®**: Conference on Neural Information Processing Systems (NeurIPS 2025)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/jesimonbarreto/VESSA)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VESSAÔºö‰∏ÄÁßçÂü∫‰∫éËßÜÈ¢ëÂØπË±°‰∏≠ÂøÉÁöÑËá™ÁõëÁù£ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÈÄÇÂ∫îÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÂü∫Á°ÄÊ®°Âûã` `Ëá™ÁõëÁù£Â≠¶‰π†` `È¢ÜÂüüÈÄÇÂ∫î` `ËßÜÈ¢ëÊï∞ÊçÆ` `Ëá™Ëí∏È¶è` `ÂèÇÊï∞È´òÊïàÈÄÇÂ∫î` `ÂØπË±°‰∏≠ÂøÉÂåñ` `Â§öËßÜËßíÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÂú®ÂàÜÂ∏ÉÂÅèÁßªÂíåÊ†áÁ≠æÁ®ÄÁº∫Âú∫ÊôØ‰∏ãÊÄßËÉΩ‰∏ãÈôçÔºåÊúâÁõëÁù£ÂæÆË∞É‰∏çÂèØË°åÔºåËÄåÈù¢ÂêëËßÜËßâÁºñÁ†ÅÂô®ÁöÑËá™ÁõëÁù£Â≠¶‰π†ÈÄÇÂ∫îÊñπÊ≥ïÊïàÊûú‰∏ç‰Ω≥„ÄÇ
2. VESSAÂà©Áî®Áü≠ËßÜÈ¢ë‰∏≠ÁöÑÂ§öËßÜËßíÂØπË±°‰ø°ÊÅØÔºåÈÄöËøáËá™Ëí∏È¶èÂ≠¶‰π†Ôºå‰ΩøÊ®°ÂûãÂú®Êó†ÈúÄÊ†áÊ≥®ÁöÑÊÉÖÂÜµ‰∏ãÈÄÇÂ∫îÊñ∞È¢ÜÂüüÔºåÊèêÂçáÈ≤ÅÊ£íÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVESSAÂú®‰∏ãÊ∏∏ÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåÁõ∏ËæÉ‰∫éÂéüÂßãÊ®°ÂûãÂíåÂÖ∂‰ªñÈÄÇÂ∫îÊñπÊ≥ïÔºåÊÄßËÉΩÂæóÂà∞ÊòæËëóÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈíàÂØπËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑËá™ÁõëÁù£ÂæÆË∞ÉÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éËß£ÂÜ≥Ê®°ÂûãÂú®ÂàÜÂ∏ÉÂÅèÁßªÂíåÊ†áÁ≠æÁ®ÄÁº∫È¢ÜÂüüË°®Áé∞‰∏ç‰Ω≥ÁöÑÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÂêç‰∏∫VESSAÔºàVideo-based objEct-centric Self-Supervised AdaptationÔºâÔºåÂà©Áî®Áü≠ÁöÑÂ§öËßÜËßíÂØπË±°‰∏≠ÂøÉËßÜÈ¢ëÔºåÊó†ÈúÄ‰ªª‰ΩïÊ†áÊ≥®Âç≥ÂèØÂ∞ÜÊ®°ÂûãÈÄÇÂ∫îÂà∞Êñ∞È¢ÜÂüü„ÄÇVESSAÁöÑËÆ≠ÁªÉÊäÄÊúØÂü∫‰∫éËá™Ëí∏È¶èËåÉÂºèÔºåÂÖ∂‰∏≠È¢ÑÊµãÂ§¥ÁöÑÁ≤æÁªÜË∞ÉÊï¥ÂíåÂèÇÊï∞È´òÊïàÈÄÇÂ∫îÊäÄÊúØÁöÑÈÉ®ÁΩ≤Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂê¶ÂàôÊ®°ÂûãÂèØËÉΩ‰ºöËøÖÈÄüÈÅóÂøòÂÖ∂È¢ÑËÆ≠ÁªÉÁü•ËØÜÂπ∂ËææÂà∞ÈÄÄÂåñÁä∂ÊÄÅ„ÄÇVESSAÂèóÁõä‰∫éÊù•Ëá™ÂØπË±°‰∏≠ÂøÉËßÜÈ¢ë‰∏≠‰∏çÂêåÂ∏ßÁöÑÂ§öËßÜËßíÂØπË±°ËßÇÊµãÔºåÈ´òÊïàÂú∞Â≠¶‰π†ÂØπÂêÑÁßçÊçïËé∑Êù°‰ª∂ÁöÑÈ≤ÅÊ£íÊÄßÔºåËÄåÊó†ÈúÄÊ†áÊ≥®„ÄÇÈÄöËøáÂú®2‰∏™Êï∞ÊçÆÈõÜ‰∏äÂØπ3‰∏™ËßÜËßâÂü∫Á°ÄÊ®°ÂûãËøõË°åÂÖ®Èù¢ÂÆûÈ™åÔºåVESSAÂú®‰∏ãÊ∏∏ÂàÜÁ±ª‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫‰∏ÄËá¥ÁöÑÊîπËøõÔºå‰ºò‰∫éÂü∫Á°ÄÊ®°ÂûãÂíå‰ª•ÂâçÁöÑÈÄÇÂ∫îÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜËßâÂü∫Á°ÄÊ®°ÂûãÂú®Èù¢ÂØπÊï∞ÊçÆÂàÜÂ∏ÉÂÅèÁßªÁöÑÊñ∞È¢ÜÂüüÊó∂ÔºåÊÄßËÉΩ‰ºöÊòæËëó‰∏ãÈôç„ÄÇ‰º†ÁªüÁöÑÊúâÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ï‰æùËµñ‰∫éÂ§ßÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆÔºå‰ΩÜÂú®ËÆ∏Â§öÂÆûÈôÖÂú∫ÊôØ‰∏≠ÔºåËé∑ÂèñËøô‰∫õÊ†áÊ≥®Êï∞ÊçÆÈùûÂ∏∏Âõ∞ÈöæÁîöËá≥‰∏çÂèØËÉΩ„ÄÇÁé∞ÊúâÁöÑËá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåËôΩÁÑ∂Âú®ËØ≠Ë®ÄÊ®°ÂûãÈ¢ÜÂüüÂèñÂæó‰∫ÜÊàêÂäüÔºå‰ΩÜÂú®ËßÜËßâÁºñÁ†ÅÂô®Ê®°Âûã‰∏äÁöÑÊïàÊûúÂπ∂‰∏çÁêÜÊÉ≥ÔºåÂÆπÊòìÂá∫Áé∞ÁÅæÈöæÊÄßÈÅóÂøòÁ≠âÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVESSAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜÈ¢ë‰∏≠ÁöÑÂ§öËßÜËßí‰ø°ÊÅØÔºåÈÄöËøáËá™ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÂºèÔºåËÆ©Ê®°ÂûãÂ≠¶‰π†Âà∞ÂØπÊñ∞È¢ÜÂüüÊï∞ÊçÆÂàÜÂ∏ÉÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåVESSAÂà©Áî®ÂØπË±°‰∏≠ÂøÉÂåñÁöÑÁü≠ËßÜÈ¢ëÔºå‰ªé‰∏çÂêåÁöÑËßÜËßíËßÇÂØüÂêå‰∏Ä‰∏™ÂØπË±°Ôºå‰ªéËÄåËÆ©Ê®°ÂûãÂ≠¶‰π†Âà∞ÂØπË±°Âú®‰∏çÂêåËßÜËßí‰∏ãÁöÑ‰∏çÂèòÊÄßÁâπÂæÅ„ÄÇËøôÁßçÊñπÊ≥ï‰∏çÈúÄË¶Å‰ªª‰Ωï‰∫∫Â∑•Ê†áÊ≥®ÔºåÂè™ÈúÄË¶ÅÂ§ßÈáèÁöÑÊó†Ê†áÊ≥®ËßÜÈ¢ëÊï∞ÊçÆÂç≥ÂèØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVESSAÁöÑÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éËá™Ëí∏È¶èÂ≠¶‰π†„ÄÇÈ¶ñÂÖàÔºåÂ∞ÜËßÜÈ¢ëÊï∞ÊçÆËæìÂÖ•Âà∞ËßÜËßâÂü∫Á°ÄÊ®°Âûã‰∏≠ÔºåÂæóÂà∞ÁâπÂæÅË°®Á§∫„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®Ëøô‰∫õÁâπÂæÅË°®Á§∫‰Ωú‰∏∫‚ÄúÊïôÂ∏à‚Äù‰ø°Âè∑ÔºåËÆ≠ÁªÉ‰∏Ä‰∏™Êñ∞ÁöÑ‚ÄúÂ≠¶Áîü‚ÄùÊ®°Âûã„ÄÇÂ≠¶ÁîüÊ®°ÂûãÁöÑÁõÆÊ†áÊòØÂ∞ΩÂèØËÉΩÂú∞ÈÄºËøëÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫Ôºå‰ªéËÄåÂ≠¶‰π†Âà∞ÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜ„ÄÇ‰∏∫‰∫ÜÈò≤Ê≠¢Â≠¶ÁîüÊ®°ÂûãÈÅóÂøòÈ¢ÑËÆ≠ÁªÉÁöÑÁü•ËØÜÔºåVESSAÈááÁî®‰∫ÜÂèÇÊï∞È´òÊïàÁöÑÈÄÇÂ∫îÊäÄÊúØÔºåÂè™Êõ¥Êñ∞Ê®°ÂûãÁöÑÈÉ®ÂàÜÂèÇÊï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVESSAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âà©Áî®ËßÜÈ¢ë‰∏≠ÁöÑÂ§öËßÜËßí‰ø°ÊÅØËøõË°åËá™ÁõëÁù£Â≠¶‰π†„ÄÇ‰∏é‰º†ÁªüÁöÑËá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåVESSA‰∏çÈúÄË¶Å‰∫∫Â∑•ËÆæËÆ°Â§çÊùÇÁöÑÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°ÔºåËÄåÊòØÁõ¥Êé•Âà©Áî®ËßÜÈ¢ëÊï∞ÊçÆ‰∏≠ÁöÑËá™ÁÑ∂ÁõëÁù£‰ø°Âè∑„ÄÇÊ≠§Â§ñÔºåVESSAËøòÈááÁî®‰∫ÜÂèÇÊï∞È´òÊïàÁöÑÈÄÇÂ∫îÊäÄÊúØÔºåÊúâÊïàÂú∞Èò≤Ê≠¢‰∫ÜÁÅæÈöæÊÄßÈÅóÂøòÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVESSAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÂØπË±°‰∏≠ÂøÉÂåñÁöÑËßÜÈ¢ëÊï∞ÊçÆÔºöÁ°Æ‰øùËßÜÈ¢ë‰∏≠ÁöÑÂØπË±°ÂßãÁªà‰Ωç‰∫éÂõæÂÉè‰∏≠ÂøÉÔºå‰ªéËÄåÊñπ‰æøÊ®°ÂûãÂ≠¶‰π†ÂØπË±°ÁöÑ‰∏çÂèòÊÄßÁâπÂæÅ„ÄÇ2) Ëá™Ëí∏È¶èÂ≠¶‰π†Ê°ÜÊû∂ÔºöÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÊèê‰æõÁõëÁù£‰ø°Âè∑ÔºåÂºïÂØºÂ≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†„ÄÇ3) ÂèÇÊï∞È´òÊïàÁöÑÈÄÇÂ∫îÊäÄÊúØÔºöÂè™Êõ¥Êñ∞Ê®°ÂûãÁöÑÈÉ®ÂàÜÂèÇÊï∞ÔºåÈò≤Ê≠¢ÁÅæÈöæÊÄßÈÅóÂøò„ÄÇ4) Á≤æÁªÜË∞ÉÊï¥ÁöÑÈ¢ÑÊµãÂ§¥ÔºöÈíàÂØπ‰∏çÂêåÁöÑ‰∏ãÊ∏∏‰ªªÂä°ÔºåÈúÄË¶ÅÂØπÈ¢ÑÊµãÂ§¥ËøõË°åÁ≤æÁªÜË∞ÉÊï¥Ôºå‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VESSAÂú®‰∏§‰∏™Êï∞ÊçÆÈõÜ‰∏äÂØπ‰∏â‰∏™ËßÜËßâÂü∫Á°ÄÊ®°ÂûãËøõË°å‰∫ÜËØÑ‰º∞ÔºåÁªìÊûúË°®ÊòéVESSAËÉΩÂ§üÊòæËëóÊèêÈ´òÊ®°ÂûãÂú®‰∏ãÊ∏∏ÂàÜÁ±ª‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®‰ΩøÁî®ViT-B/16Ê®°ÂûãÂú®VTABÊï∞ÊçÆÈõÜ‰∏äËøõË°åËØÑ‰º∞Êó∂ÔºåVESSAÁöÑÊÄßËÉΩÊØîÂéüÂßãÊ®°ÂûãÊèêÈ´ò‰∫Ü5%‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºåVESSAËøò‰ºò‰∫éÂÖ∂‰ªñËá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VESSAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂú®Ëá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÂåªÁñóÂΩ±ÂÉèÂàÜÊûêÁ≠âÈ¢ÜÂüü„ÄÇÂú®Ëøô‰∫õÈ¢ÜÂüü‰∏≠ÔºåÊï∞ÊçÆÂàÜÂ∏ÉÂæÄÂæÄ‰ºöÂèëÁîüÂèòÂåñÔºåËÄå‰∏îËé∑ÂèñÊ†áÊ≥®Êï∞ÊçÆÈùûÂ∏∏Âõ∞Èöæ„ÄÇVESSAÂèØ‰ª•Â∏ÆÂä©Ê®°ÂûãÂø´ÈÄüÈÄÇÂ∫îÊñ∞ÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºå‰ªéËÄåÊèêÂçáÁ≥ªÁªüÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåVESSAËøòÂèØ‰ª•Áî®‰∫éÊûÑÂª∫Êõ¥Âä†ÈÄöÁî®ÁöÑËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÈÄÇÂ∫îÂêÑÁßç‰∏çÂêåÁöÑËßÜËßâ‰ªªÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Foundation models have advanced computer vision by enabling strong performance across diverse tasks through large-scale pretraining and supervised fine-tuning. However, they may underperform in domains with distribution shifts and scarce labels, where supervised fine-tuning may be infeasible. While continued self-supervised learning for model adaptation is common for generative language models, this strategy has not proven effective for vision-centric encoder models. To address this challenge, we introduce a novel formulation of self-supervised fine-tuning for vision foundation models, where the model is adapted to a new domain without requiring annotations, leveraging only short multi-view object-centric videos. Our method is referred to as VESSA: Video-based objEct-centric Self-Supervised Adaptation for visual foundation models. VESSA's training technique is based on a self-distillation paradigm, where it is critical to carefully tune prediction heads and deploy parameter-efficient adaptation techniques - otherwise, the model may quickly forget its pretrained knowledge and reach a degraded state. VESSA benefits significantly from multi-view object observations sourced from different frames in an object-centric video, efficiently learning robustness to varied capture conditions, without the need of annotations. Through comprehensive experiments with 3 vision foundation models on 2 datasets, VESSA demonstrates consistent improvements in downstream classification tasks, compared to the base models and previous adaptation methods. Code is publicly available at https://github.com/jesimonbarreto/VESSA.

