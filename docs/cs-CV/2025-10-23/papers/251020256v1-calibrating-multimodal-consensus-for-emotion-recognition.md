---
layout: default
title: Calibrating Multimodal Consensus for Emotion Recognition
---

# Calibrating Multimodal Consensus for Emotion Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20256" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20256v1</a>
  <a href="https://arxiv.org/pdf/2510.20256.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20256v1" onclick="toggleFavorite(this, '2510.20256v1', 'Calibrating Multimodal Consensus for Emotion Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/gw-zhong/CMC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ ¡å‡†å¤šæ¨¡æ€å…±è¯†æ¨¡å‹ä»¥è§£å†³æƒ…æ„Ÿè¯†åˆ«ä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«` `ä¼ªæ ‡ç­¾ç”Ÿæˆ` `æ— å‚æ•°èåˆ` `è¯­ä¹‰ä¸€è‡´æ€§` `æƒ…æ„Ÿåˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•å¿½è§†äº†æ¨¡æ€é—´çš„è¯­ä¹‰ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´æƒ…æ„Ÿçº¿ç´¢å†²çªï¼Œå½±å“è¯†åˆ«å‡†ç¡®æ€§ã€‚
2. æå‡ºçš„æ ¡å‡†å¤šæ¨¡æ€å…±è¯†æ¨¡å‹é€šè¿‡ä¼ªæ ‡ç­¾ç”Ÿæˆå’Œæ— å‚æ•°èåˆæ¨¡å—ï¼Œå‡è½»æ–‡æœ¬æ¨¡æ€çš„ä¸»å¯¼åœ°ä½ï¼Œæå‡èåˆæ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCMCåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨å­˜åœ¨è¯­ä¹‰ä¸ä¸€è‡´çš„åœºæ™¯ä¸­ï¼Œè¯†åˆ«æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å¿½è§†äº†ä¸åŒæ¨¡æ€ä¹‹é—´å¯èƒ½å‡ºç°çš„è¯­ä¹‰ä¸ä¸€è‡´æ€§ï¼Œä¾‹å¦‚æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ä¹‹é—´çš„æƒ…æ„Ÿçº¿ç´¢å†²çªã€‚æ­¤å¤–ï¼Œå½“å‰æ–¹æ³•å¾€å¾€å—åˆ°æ–‡æœ¬æ¨¡æ€çš„ä¸»å¯¼å½±å“ï¼Œå¯¼è‡´è¯†åˆ«å‡†ç¡®æ€§ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ ¡å‡†å¤šæ¨¡æ€å…±è¯†ï¼ˆCMCï¼‰çš„æ¨¡å‹ã€‚CMCå¼•å…¥äº†ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—ï¼ˆPLGMï¼‰ï¼Œä»¥è‡ªç›‘ç£æ–¹å¼ç”Ÿæˆä¼ªå•æ¨¡æ€æ ‡ç­¾ï¼Œä»è€Œå®ç°å•æ¨¡æ€é¢„è®­ç»ƒã€‚æ¥ç€ï¼Œé‡‡ç”¨æ— å‚æ•°èåˆæ¨¡å—ï¼ˆPFMï¼‰å’Œå¤šæ¨¡æ€å…±è¯†è·¯ç”±å™¨ï¼ˆMCRï¼‰è¿›è¡Œå¤šæ¨¡æ€å¾®è°ƒï¼Œå‡è½»æ–‡æœ¬ä¸»å¯¼æ€§å¹¶å¼•å¯¼èåˆè¿‡ç¨‹æœå‘æ›´å¯é çš„å…±è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCMCåœ¨å››ä¸ªæ•°æ®é›†ï¼ˆCH-SIMSã€CH-SIMS v2ã€CMU-MOSIå’ŒCMU-MOSEIï¼‰ä¸Šçš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå¹¶åœ¨CH-SIMSå’ŒCH-SIMS v2ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­æ¨¡æ€é—´è¯­ä¹‰ä¸ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å—åˆ°æ–‡æœ¬æ¨¡æ€çš„ä¸»å¯¼å½±å“ï¼Œå¯¼è‡´æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ ¡å‡†å¤šæ¨¡æ€å…±è¯†æ¨¡å‹ï¼ˆCMCï¼‰é€šè¿‡ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—ï¼ˆPLGMï¼‰å®ç°è‡ªç›‘ç£çš„å•æ¨¡æ€é¢„è®­ç»ƒï¼Œè¿›è€Œé€šè¿‡æ— å‚æ•°èåˆæ¨¡å—ï¼ˆPFMï¼‰å’Œå¤šæ¨¡æ€å…±è¯†è·¯ç”±å™¨ï¼ˆMCRï¼‰è¿›è¡Œæœ‰æ•ˆçš„å¤šæ¨¡æ€å¾®è°ƒï¼Œä»¥å‡è½»æ–‡æœ¬æ¨¡æ€çš„ä¸»å¯¼æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCMCæ¨¡å‹ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—ï¼ˆPLGMï¼‰ç”¨äºç”Ÿæˆä¼ªå•æ¨¡æ€æ ‡ç­¾ï¼Œä¿ƒè¿›å•æ¨¡æ€çš„è‡ªç›‘ç£å­¦ä¹ ï¼›æ— å‚æ•°èåˆæ¨¡å—ï¼ˆPFMï¼‰ç”¨äºèåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼›å¤šæ¨¡æ€å…±è¯†è·¯ç”±å™¨ï¼ˆMCRï¼‰åˆ™è´Ÿè´£å¼•å¯¼èåˆè¿‡ç¨‹ï¼Œç¡®ä¿æœ€ç»ˆçš„æƒ…æ„Ÿè¯†åˆ«ç»“æœæ›´ä¸ºå¯é ã€‚

**å…³é”®åˆ›æ–°**ï¼šCMCçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥ä¼ªæ ‡ç­¾ç”Ÿæˆæœºåˆ¶å’Œæ— å‚æ•°èåˆç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€æ¨¡æ€ä¸»å¯¼æ€§å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æ¨¡æ€é—´çš„è¯­ä¹‰å†²çªã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒPLGMé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ŒPFMåˆ™é‡‡ç”¨æ— å‚æ•°è®¾è®¡ä»¥ç®€åŒ–èåˆè¿‡ç¨‹ï¼ŒMCRé€šè¿‡åŠ¨æ€è°ƒæ•´æ¨¡æ€æƒé‡æ¥ä¼˜åŒ–æœ€ç»ˆçš„æƒ…æ„Ÿè¯†åˆ«ç»“æœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCMCåœ¨CH-SIMSã€CH-SIMS v2ã€CMU-MOSIå’ŒCMU-MOSEIå››ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨CH-SIMSå’ŒCH-SIMS v2æ•°æ®é›†ä¸­ï¼Œé’ˆå¯¹è¯­ä¹‰ä¸ä¸€è‡´åœºæ™¯çš„è¯†åˆ«æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›æ–¹æ³•çš„æ°´å¹³æˆ–æ›´ä¼˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æƒ…æ„Ÿåˆ†æã€ç¤¾äº¤åª’ä½“ç›‘æµ‹ã€å¿ƒç†å¥åº·è¯„ä¼°ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·æƒ…æ„Ÿï¼Œè¿›è€Œä¸ºä¸ªæ€§åŒ–æœåŠ¡å’Œå¹²é¢„æªæ–½æä¾›æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¦‚æƒ…æ„Ÿé©±åŠ¨çš„å¯¹è¯ç³»ç»Ÿå’Œæƒ…æ„Ÿæ™ºèƒ½æœºå™¨äººç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, Multimodal Emotion Recognition (MER) has made substantial progress. Nevertheless, most existing approaches neglect the semantic inconsistencies that may arise across modalities, such as conflicting emotional cues between text and visual inputs. Besides, current methods are often dominated by the text modality due to its strong representational capacity, which can compromise recognition accuracy. To address these challenges, we propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels, enabling unimodal pretraining in a self-supervised fashion. It then employs a Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for multimodal finetuning, thereby mitigating text dominance and guiding the fusion process toward a more reliable consensus. Experimental results demonstrate that CMC achieves performance on par with or superior to state-of-the-art methods across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and exhibits notable advantages in scenarios with semantic inconsistencies on CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible at https://github.com/gw-zhong/CMC.

