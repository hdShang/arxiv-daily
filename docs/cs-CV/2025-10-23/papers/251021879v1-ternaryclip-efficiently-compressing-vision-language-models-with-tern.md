---
layout: default
title: TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge
---

# TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21879" target="_blank" class="toolbar-btn">arXiv: 2510.21879v1</a>
    <a href="https://arxiv.org/pdf/2510.21879.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21879v1" 
            onclick="toggleFavorite(this, '2510.21879v1', 'TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shu-Hao Zhang, Wei-Cheng Tang, Chen Wu, Peng Hu, Nan Li, Liang-Jie Zhang, Qi Zhang, Shao-Qun Zhang

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-23

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**TernaryCLIPÔºöÈÄöËøá‰∏âÂÖÉÊùÉÈáçÂíåÁü•ËØÜËí∏È¶èÈ´òÊïàÂéãÁº©ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `CLIPÊ®°ÂûãÂéãÁº©` `‰∏âÂÖÉÈáèÂåñ` `ÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉ` `Áü•ËØÜËí∏È¶è` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Ê®°ÂûãÂä†ÈÄü` `ËµÑÊ∫êÂèóÈôêËÆæÂ§á`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâCLIPÊ®°ÂûãÂèÇÊï∞ÈáèÂ§ßÔºåËÆ°ÁÆóÊàêÊú¨È´òÔºåÈöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÈÉ®ÁΩ≤„ÄÇ
2. TernaryCLIPÈÄöËøáÂ∞ÜCLIPÊ®°ÂûãÁöÑÊùÉÈáç‰∏âÂÖÉÂåñÔºåÂπ∂ÁªìÂêàÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÂíåÁü•ËØÜËí∏È¶èÔºåÂÆûÁé∞Ê®°ÂûãÂéãÁº©ÂíåÂä†ÈÄü„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåTernaryCLIPÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÂéãÁº©Áéá„ÄÅÊé®ÁêÜÂä†ÈÄüÂíåÂÜÖÂ≠ò‰ºòÂåñ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫TernaryCLIPÔºå‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑËÆ°ÁÆóÊ°ÜÊû∂ÔºåÂ∞ÜCLIPÊ®°ÂûãÁöÑËßÜËßâÂíåÊñáÊú¨ÁºñÁ†ÅÂô®ÁöÑËøûÊé•ÊùÉÈáçËΩ¨Êç¢‰∏∫‰∏âÂÖÉÊ†ºÂºèÔºåËÄåÈùûÂÖ®Á≤æÂ∫¶ÊàñÊµÆÁÇπÊ†ºÂºè„ÄÇTernaryCLIPÁªìÂêà‰∫ÜÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÂíåËí∏È¶èÊ®°ÂùóÔºå‰ª•Èò≤Ê≠¢Á≤æÂ∫¶‰∏ãÈôçÔºåÂπ∂ÂÆûÁé∞‰ΩéÊàêÊú¨ÂíåÈ´òÊïàÁöÑËÆ°ÁÆó„ÄÇÁªºÂêàÂÆûÈ™åË°®ÊòéÔºåTernaryCLIPÂèØ‰ª•ÂÆûÁé∞È´òËææ99%ÁöÑ‰∏âÂÖÉÂåñÊùÉÈáçÔºå‰ΩøÁî®1.58‰ΩçÁöÑË°®Á§∫Ôºå16.98ÂÄçÁöÑÂéãÁº©ÁéáÔºå2.3ÂÄçÁöÑÊé®ÁêÜÂä†ÈÄüÔºå16ÂÄçÁöÑÂ≠òÂÇ®ÂáèÂ∞ëÔºå10ÂÄçÁöÑÂÜÖÂ≠ò‰ºòÂåñÂíå60%ÁöÑÁ®ÄÁñèÊÄßÔºåÂêåÊó∂Âú®41‰∏™Â∏∏Áî®Êï∞ÊçÆÈõÜ‰∏äÁöÑÈõ∂Ê†∑Êú¨ÂõæÂÉèÂàÜÁ±ªÂíåÂõæÂÉè-ÊñáÊú¨Ê£ÄÁ¥¢‰ªªÂä°‰∏≠‰øùÊåÅËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇËøôÈ°πÂ∑•‰ΩúÁ™ÅÂá∫‰∫ÜÂØπÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãËøõË°åÊûÅÁ´ØÈáèÂåñÁöÑÂèØË°åÊÄßÔºåÊîØÊåÅÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äËøõË°åÊúâÊïàÂíåÈ´òÊïàÁöÑÈÉ®ÁΩ≤„ÄÇÊ®°ÂûãÂíå‰ª£Á†ÅÂèØ‰ª•Âú®Hugging FaceÂíåGitHub‰∏äËÆøÈóÆ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥CLIPÊ®°ÂûãËÆ°ÁÆóÂºÄÈîÄÂ§ßÔºåÈöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÈÉ®ÁΩ≤ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®Ê®°ÂûãÂâ™ÊûùÊàñÈáèÂåñÔºå‰ΩÜËøô‰∫õÊñπÊ≥ïÂú®ÊûÅÁ´ØÈáèÂåñÔºàÂ¶Ç‰∫åÂÄºÂåñÊàñ‰∏âÂÖÉÂåñÔºâÊó∂ÔºåÊÄßËÉΩ‰∏ãÈôçÊòéÊòæ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜCLIPÊ®°ÂûãÁöÑÊùÉÈáç‰∏âÂÖÉÂåñÔºåÂç≥ÊùÉÈáçÂèñÂÄº‰∏∫{-1, 0, 1}„ÄÇ‰∏∫‰∫ÜÂº•Ë°•‰∏âÂÖÉÂåñÂ∏¶Êù•ÁöÑÁ≤æÂ∫¶ÊçüÂ§±ÔºåËÆ∫ÊñáÈááÁî®‰∫ÜÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÂíåÁü•ËØÜËí∏È¶èÊäÄÊúØ„ÄÇÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ê®°ÊãüÈáèÂåñÊìç‰ΩúÔºå‰ΩøÊ®°ÂûãÈÄÇÂ∫î‰∏âÂÖÉÊùÉÈáç„ÄÇÁü•ËØÜËí∏È¶èÂàôÂà©Áî®ÂéüÂßãCLIPÊ®°Âûã‰Ωú‰∏∫ÊïôÂ∏àÊ®°ÂûãÔºåÊåáÂØº‰∏âÂÖÉÂåñÂêéÁöÑÂ≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTernaryCLIPÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) ÊùÉÈáç‰∏âÂÖÉÂåñÔºöÂ∞ÜCLIPÊ®°ÂûãÁöÑËßÜËßâÂíåÊñáÊú¨ÁºñÁ†ÅÂô®ÁöÑÊùÉÈáçËΩ¨Êç¢‰∏∫‰∏âÂÖÉÊ†ºÂºè„ÄÇ2) ÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºöÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÊãüÊùÉÈáç‰∏âÂÖÉÂåñÊìç‰ΩúÔºåÂπ∂‰ΩøÁî®ÈáèÂåñÂêéÁöÑÊùÉÈáçËøõË°åÂâçÂêë‰º†Êí≠ÂíåÂèçÂêë‰º†Êí≠„ÄÇ3) Áü•ËØÜËí∏È¶èÔºö‰ΩøÁî®ÂéüÂßãCLIPÊ®°Âûã‰Ωú‰∏∫ÊïôÂ∏àÊ®°ÂûãÔºåÂà©Áî®ÂÖ∂ËæìÂá∫logitsÊåáÂØº‰∏âÂÖÉÂåñÂêéÁöÑÂ≠¶ÁîüÊ®°ÂûãËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞Ü‰∏âÂÖÉÂåñÊùÉÈáç„ÄÅÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÂíåÁü•ËØÜËí∏È¶èÁõ∏ÁªìÂêàÔºåÂÆûÁé∞‰∫ÜCLIPÊ®°ÂûãÁöÑÊûÅÁ´ØÂéãÁº©ÂíåÂä†ÈÄüÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∫åÂÄºÂåñÊàñ‰∏âÂÖÉÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåTernaryCLIPÈÄöËøáÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÂíåÁü•ËØÜËí∏È¶èÔºåÊúâÊïàÂú∞ÁºìËß£‰∫ÜÁ≤æÂ∫¶ÊçüÂ§±„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Straight-Through Estimator (STE)ËøõË°åÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºåËß£ÂÜ≥Ê¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢ò„ÄÇ2) ËÆæËÆ°‰∫ÜÂü∫‰∫élogitsÁöÑÁü•ËØÜËí∏È¶èÊçüÂ§±ÂáΩÊï∞ÔºåÈºìÂä±Â≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫ÂàÜÂ∏É„ÄÇ3) ÂØπËßÜËßâÂíåÊñáÊú¨ÁºñÁ†ÅÂô®ÂàÜÂà´ËøõË°å‰∏âÂÖÉÂåñÔºåÂπ∂ÈíàÂØπ‰∏çÂêåÊ®°ÊÄÅÁöÑÁâπÁÇπËøõË°å‰ºòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTernaryCLIPÂú®41‰∏™Â∏∏Áî®Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÂéãÁº©ÂíåÂä†ÈÄüÊïàÊûú„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåTernaryCLIPÂÆûÁé∞‰∫ÜÈ´òËææ99%ÁöÑ‰∏âÂÖÉÂåñÊùÉÈáçÔºå16.98ÂÄçÁöÑÂéãÁº©ÁéáÔºå2.3ÂÄçÁöÑÊé®ÁêÜÂä†ÈÄüÔºå16ÂÄçÁöÑÂ≠òÂÇ®ÂáèÂ∞ëÔºå10ÂÄçÁöÑÂÜÖÂ≠ò‰ºòÂåñÂíå60%ÁöÑÁ®ÄÁñèÊÄßÔºåÂêåÊó∂Âú®Èõ∂Ê†∑Êú¨ÂõæÂÉèÂàÜÁ±ªÂíåÂõæÂÉè-ÊñáÊú¨Ê£ÄÁ¥¢‰ªªÂä°‰∏≠‰øùÊåÅ‰∫Ü‰∏éÂéüÂßãCLIPÊ®°ÂûãÁõ∏ËøëÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TernaryCLIPÂèØÂ∫îÁî®‰∫éÁßªÂä®ËÆæÂ§á„ÄÅÂµåÂÖ•ÂºèÁ≥ªÁªüÁ≠âËµÑÊ∫êÂèóÈôêÁöÑÂú∫ÊôØÔºåÂÆûÁé∞È´òÊïàÁöÑÂõæÂÉè-ÊñáÊú¨ÁêÜËß£ÂíåÊ£ÄÁ¥¢„ÄÇ‰æãÂ¶ÇÔºåÂú®Êô∫ËÉΩÊâãÊú∫‰∏äËøõË°åÂõæÂÉèÊêúÁ¥¢„ÄÅÂú®Êú∫Âô®‰∫∫‰∏äËøõË°åËßÜËßâÂØºËà™„ÄÅÂú®ÂèØÁ©øÊà¥ËÆæÂ§á‰∏äËøõË°åÂõæÂÉèËØÜÂà´Á≠â„ÄÇËØ•Á†îÁ©∂‰∏∫Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent years have witnessed an increasing interest in image-text contrastive modeling, exemplified by models such as Contrastive Language-Image Pretraining (CLIP). In this paper, we propose the TernaryCLIP, a lightweight computational framework that converts connection weights of both vision and text encoders of CLIP into the ternary format, instead of full-precision or floating ones. TernaryCLIP incorporates quantization-aware training and distillation modules, preventing precision degradation and enabling low-cost and high-efficiency computations. Comprehensive experiments demonstrate that TernaryCLIP can achieve up to 99\% ternarized weights with 1.58-bit representation, 16.98 $\times$ compression ratio, 2.3 $\times$ inference acceleration, 16 $\times$ storage reduction, 10 $\times$ memory optimization, and 60\% sparsity while maintaining promising performance on zero-shot image classification and image-text retrieval tasks across 41 commonly used datasets. Our work highlights the feasibility of extreme quantization for large multimodal models, supporting effective and efficient deployment on resource-constrained devices. The model and code can be accessed from Hugging Face and GitHub.

