---
layout: default
title: "TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge"
---

# TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21879" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21879v1</a>
  <a href="https://arxiv.org/pdf/2510.21879.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21879v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21879v1', 'TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shu-Hao Zhang, Wei-Cheng Tang, Chen Wu, Peng Hu, Nan Li, Liang-Jie Zhang, Qi Zhang, Shao-Qun Zhang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TernaryCLIPï¼šé€šè¿‡ä¸‰å…ƒæƒé‡å’ŒçŸ¥è¯†è’¸é¦é«˜æ•ˆå‹ç¼©è§†è§‰-è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `CLIPæ¨¡å‹å‹ç¼©` `ä¸‰å…ƒé‡åŒ–` `é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ` `çŸ¥è¯†è’¸é¦` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ¨¡å‹åŠ é€Ÿ` `èµ„æºå—é™è®¾å¤‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CLIPæ¨¡å‹å‚æ•°é‡å¤§ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚
2. TernaryCLIPé€šè¿‡å°†CLIPæ¨¡å‹çš„æƒé‡ä¸‰å…ƒåŒ–ï¼Œå¹¶ç»“åˆé‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦ï¼Œå®ç°æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTernaryCLIPåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„å‹ç¼©ç‡ã€æ¨ç†åŠ é€Ÿå’Œå†…å­˜ä¼˜åŒ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºTernaryCLIPï¼Œä¸€ä¸ªè½»é‡çº§çš„è®¡ç®—æ¡†æ¶ï¼Œå°†CLIPæ¨¡å‹çš„è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨çš„è¿æ¥æƒé‡è½¬æ¢ä¸ºä¸‰å…ƒæ ¼å¼ï¼Œè€Œéå…¨ç²¾åº¦æˆ–æµ®ç‚¹æ ¼å¼ã€‚TernaryCLIPç»“åˆäº†é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’Œè’¸é¦æ¨¡å—ï¼Œä»¥é˜²æ­¢ç²¾åº¦ä¸‹é™ï¼Œå¹¶å®ç°ä½æˆæœ¬å’Œé«˜æ•ˆçš„è®¡ç®—ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒTernaryCLIPå¯ä»¥å®ç°é«˜è¾¾99%çš„ä¸‰å…ƒåŒ–æƒé‡ï¼Œä½¿ç”¨1.58ä½çš„è¡¨ç¤ºï¼Œ16.98å€çš„å‹ç¼©ç‡ï¼Œ2.3å€çš„æ¨ç†åŠ é€Ÿï¼Œ16å€çš„å­˜å‚¨å‡å°‘ï¼Œ10å€çš„å†…å­˜ä¼˜åŒ–å’Œ60%çš„ç¨€ç–æ€§ï¼ŒåŒæ—¶åœ¨41ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œå›¾åƒ-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œæç«¯é‡åŒ–çš„å¯è¡Œæ€§ï¼Œæ”¯æŒåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œæœ‰æ•ˆå’Œé«˜æ•ˆçš„éƒ¨ç½²ã€‚æ¨¡å‹å’Œä»£ç å¯ä»¥åœ¨Hugging Faceå’ŒGitHubä¸Šè®¿é—®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³CLIPæ¨¡å‹è®¡ç®—å¼€é”€å¤§ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ¨¡å‹å‰ªææˆ–é‡åŒ–ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æç«¯é‡åŒ–ï¼ˆå¦‚äºŒå€¼åŒ–æˆ–ä¸‰å…ƒåŒ–ï¼‰æ—¶ï¼Œæ€§èƒ½ä¸‹é™æ˜æ˜¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†CLIPæ¨¡å‹çš„æƒé‡ä¸‰å…ƒåŒ–ï¼Œå³æƒé‡å–å€¼ä¸º{-1, 0, 1}ã€‚ä¸ºäº†å¼¥è¡¥ä¸‰å…ƒåŒ–å¸¦æ¥çš„ç²¾åº¦æŸå¤±ï¼Œè®ºæ–‡é‡‡ç”¨äº†é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ã€‚é‡åŒ–æ„ŸçŸ¥è®­ç»ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–æ“ä½œï¼Œä½¿æ¨¡å‹é€‚åº”ä¸‰å…ƒæƒé‡ã€‚çŸ¥è¯†è’¸é¦åˆ™åˆ©ç”¨åŸå§‹CLIPæ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼ŒæŒ‡å¯¼ä¸‰å…ƒåŒ–åçš„å­¦ç”Ÿæ¨¡å‹å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTernaryCLIPçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) æƒé‡ä¸‰å…ƒåŒ–ï¼šå°†CLIPæ¨¡å‹çš„è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨çš„æƒé‡è½¬æ¢ä¸ºä¸‰å…ƒæ ¼å¼ã€‚2) é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡æ‹Ÿæƒé‡ä¸‰å…ƒåŒ–æ“ä½œï¼Œå¹¶ä½¿ç”¨é‡åŒ–åçš„æƒé‡è¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚3) çŸ¥è¯†è’¸é¦ï¼šä½¿ç”¨åŸå§‹CLIPæ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œåˆ©ç”¨å…¶è¾“å‡ºlogitsæŒ‡å¯¼ä¸‰å…ƒåŒ–åçš„å­¦ç”Ÿæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ä¸‰å…ƒåŒ–æƒé‡ã€é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦ç›¸ç»“åˆï¼Œå®ç°äº†CLIPæ¨¡å‹çš„æç«¯å‹ç¼©å’ŒåŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„äºŒå€¼åŒ–æˆ–ä¸‰å…ƒåŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒTernaryCLIPé€šè¿‡é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’ŒçŸ¥è¯†è’¸é¦ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ç²¾åº¦æŸå¤±ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Straight-Through Estimator (STE)è¿›è¡Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚2) è®¾è®¡äº†åŸºäºlogitsçš„çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒã€‚3) å¯¹è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨åˆ†åˆ«è¿›è¡Œä¸‰å…ƒåŒ–ï¼Œå¹¶é’ˆå¯¹ä¸åŒæ¨¡æ€çš„ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTernaryCLIPåœ¨41ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„å‹ç¼©å’ŒåŠ é€Ÿæ•ˆæœã€‚å…·ä½“æ¥è¯´ï¼ŒTernaryCLIPå®ç°äº†é«˜è¾¾99%çš„ä¸‰å…ƒåŒ–æƒé‡ï¼Œ16.98å€çš„å‹ç¼©ç‡ï¼Œ2.3å€çš„æ¨ç†åŠ é€Ÿï¼Œ16å€çš„å­˜å‚¨å‡å°‘ï¼Œ10å€çš„å†…å­˜ä¼˜åŒ–å’Œ60%çš„ç¨€ç–æ€§ï¼ŒåŒæ—¶åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œå›¾åƒ-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­ä¿æŒäº†ä¸åŸå§‹CLIPæ¨¡å‹ç›¸è¿‘çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TernaryCLIPå¯åº”ç”¨äºç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿç­‰èµ„æºå—é™çš„åœºæ™¯ï¼Œå®ç°é«˜æ•ˆçš„å›¾åƒ-æ–‡æœ¬ç†è§£å’Œæ£€ç´¢ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½æ‰‹æœºä¸Šè¿›è¡Œå›¾åƒæœç´¢ã€åœ¨æœºå™¨äººä¸Šè¿›è¡Œè§†è§‰å¯¼èˆªã€åœ¨å¯ç©¿æˆ´è®¾å¤‡ä¸Šè¿›è¡Œå›¾åƒè¯†åˆ«ç­‰ã€‚è¯¥ç ”ç©¶ä¸ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent years have witnessed an increasing interest in image-text contrastive modeling, exemplified by models such as Contrastive Language-Image Pretraining (CLIP). In this paper, we propose the TernaryCLIP, a lightweight computational framework that converts connection weights of both vision and text encoders of CLIP into the ternary format, instead of full-precision or floating ones. TernaryCLIP incorporates quantization-aware training and distillation modules, preventing precision degradation and enabling low-cost and high-efficiency computations. Comprehensive experiments demonstrate that TernaryCLIP can achieve up to 99\% ternarized weights with 1.58-bit representation, 16.98 $\times$ compression ratio, 2.3 $\times$ inference acceleration, 16 $\times$ storage reduction, 10 $\times$ memory optimization, and 60\% sparsity while maintaining promising performance on zero-shot image classification and image-text retrieval tasks across 41 commonly used datasets. Our work highlights the feasibility of extreme quantization for large multimodal models, supporting effective and efficient deployment on resource-constrained devices. The model and code can be accessed from Hugging Face and GitHub.

