---
layout: default
title: TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning
---

# TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20162" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.20162v1</a>
  <a href="https://arxiv.org/pdf/2510.20162.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20162v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.20162v1', 'TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xudong Yan, Songhe Feng

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-23

**Â§áÊ≥®**: Accepted to NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/xud-yan/TOMCAT)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TOMCATÔºåÈÄöËøáÊµãËØïÊó∂Áü•ËØÜÁ¥ØÁßØËß£ÂÜ≥ÁªÑÂêàÈõ∂Ê†∑Êú¨Â≠¶‰π†‰∏≠ÁöÑÂàÜÂ∏ÉÂÅèÁßªÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁªÑÂêàÈõ∂Ê†∑Êú¨Â≠¶‰π†` `Áü•ËØÜÁ¥ØÁßØ` `ÂàÜÂ∏ÉÂÅèÁßª` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂéüÂûãÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁªÑÂêàÈõ∂Ê†∑Êú¨Â≠¶‰π†ÊñπÊ≥ïÂú®ÊµãËØïÊó∂Èù¢‰∏¥Ê†áÁ≠æÁ©∫Èó¥ÂàÜÂ∏ÉÂÅèÁßªÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. ÊèêÂá∫TOMCATÊñπÊ≥ïÔºåÈÄöËøáÁ¥ØÁßØÊñáÊú¨ÂíåËßÜËßâÊ®°ÊÄÅÁöÑÁü•ËØÜÔºåËá™ÈÄÇÂ∫îÊõ¥Êñ∞Â§öÊ®°ÊÄÅÂéüÂûãÔºåÁºìËß£ÂàÜÂ∏ÉÂÅèÁßª„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Âõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁªÑÂêàÈõ∂Ê†∑Êú¨Â≠¶‰π†(CZSL)Êó®Âú®Âü∫‰∫é‰ªéÂ∑≤Áü•ÁªÑÂêà‰∏≠Â≠¶‰π†Âà∞ÁöÑÁü•ËØÜÊù•ËØÜÂà´Êñ∞ÁöÑÂ±ûÊÄß-ÂØπË±°ÁªÑÂêà„ÄÇÁé∞ÊúâÊñπÊ≥ïÁî±‰∫éÊµãËØïÊó∂Ê†áÁ≠æÁ©∫Èó¥ÂàÜÂ∏ÉÂÅèÁßªËÄåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÔºåËøôÁßçÂÅèÁßªÊ∫ê‰∫éÂåÖÂê´Áî±Â±ûÊÄßÂíåÂØπË±°ÈáçÊñ∞ÁªÑÂêàËÄåÊàêÁöÑÊú™Áü•ÁªÑÂêà„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∏ÄÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ï‰ªéÊó†ÁõëÁù£Êï∞ÊçÆ‰∏≠Á¥ØÁßØÊñáÊú¨ÂíåËßÜËßâÊ®°ÊÄÅÁöÑÁªºÂêàÁü•ËØÜÔºå‰ª•Âú®ÊµãËØïÊó∂Êõ¥Êñ∞Â§öÊ®°ÊÄÅÂéüÂûã„ÄÇÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ËÆæËÆ°‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÊõ¥Êñ∞ÊùÉÈáçÊù•ÊéßÂà∂ÂéüÂûãË∞ÉÊï¥ÁöÑÁ®ãÂ∫¶Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®ÊµãËØïÊúüÈó¥ÁÅµÊ¥ªÂú∞ÈÄÇÂ∫îÂàÜÂ∏ÉÂÅèÁßª„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âä®ÊÄÅ‰ºòÂÖàÁ∫ßÈòüÂàóÔºåÁî®‰∫éÂ≠òÂÇ®È´òÁΩÆ‰ø°Â∫¶ÁöÑÂõæÂÉèÔºå‰ª•‰æø‰ªéÂéÜÂè≤ÂõæÂÉè‰∏≠Ëé∑ÂèñËßÜËßâÁü•ËØÜ‰ª•ËøõË°åÊé®ÁêÜ„ÄÇËÄÉËôëÂà∞Â§öÊ®°ÊÄÅÁü•ËØÜÁöÑËØ≠‰πâ‰∏ÄËá¥ÊÄßÔºåÊàë‰ª¨ÈÄöËøáÂ§öÊ®°ÊÄÅÂçèÂêåË°®Á§∫Â≠¶‰π†Êù•ÂØπÈΩêÊñáÊú¨ÂíåËßÜËßâÂéüÂûã„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â∞ÅÈó≠‰∏ñÁïåÂíåÂºÄÊîæ‰∏ñÁïåËÆæÁΩÆ‰∏ãÁöÑÂõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÈÉΩÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁªÑÂêàÈõ∂Ê†∑Êú¨Â≠¶‰π†(CZSL)Êó®Âú®ËØÜÂà´Êú™ËßÅËøáÁöÑÂ±ûÊÄß-ÂØπË±°ÁªÑÂêà„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÔºåÊµãËØïÈò∂ÊÆµ‰ºöÈÅáÂà∞Áî±Â∑≤Áü•Â±ûÊÄßÂíåÂØπË±°ÁªÑÊàêÁöÑÊñ∞ÁªÑÂêàÔºåÂØºËá¥Ê†áÁ≠æÁ©∫Èó¥ÂàÜÂ∏ÉÂèëÁîüÂÅèÁßªÔºåÊ®°ÂûãÊ≥õÂåñËÉΩÂäõ‰∏ãÈôç„ÄÇÊ®°ÂûãÈöæ‰ª•ÈÄÇÂ∫îËøôÁßçÊñ∞ÁöÑÂàÜÂ∏ÉÔºåÂØºËá¥ËØÜÂà´Á≤æÂ∫¶Èôç‰Ωé„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTOMCATÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂú®ÊµãËØïÈò∂ÊÆµÔºåÂà©Áî®Êó†ÁõëÁù£Êï∞ÊçÆÊåÅÁª≠Á¥ØÁßØÁü•ËØÜÔºåÂä®ÊÄÅÊõ¥Êñ∞Ê®°ÂûãÁöÑÂéüÂûãË°®Á§∫„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•ÈÄêÊ≠•ÈÄÇÂ∫îÊñ∞ÁöÑÂàÜÂ∏ÉÔºå‰ªéËÄåÊèêÈ´òÂØπÊú™ËßÅÁªÑÂêàÁöÑËØÜÂà´ËÉΩÂäõ„ÄÇËá™ÈÄÇÂ∫îÊõ¥Êñ∞ÊùÉÈáçÊéßÂà∂Êõ¥Êñ∞ÂπÖÂ∫¶ÔºåÈÅøÂÖçËøáÂ∫¶ÊãüÂêà„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTOMCATÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) **Â§öÊ®°ÊÄÅÂéüÂûãË°®Á§∫**Ôºö‰∏∫ÊØè‰∏™Â±ûÊÄßÂíåÂØπË±°Â≠¶‰π†ÊñáÊú¨ÂíåËßÜËßâÂéüÂûã„ÄÇ2) **ÊµãËØïÊó∂Áü•ËØÜÁ¥ØÁßØ**ÔºöÂà©Áî®Êó†ÁõëÁù£Êï∞ÊçÆÔºåÈÄöËøáÂ§öÊ®°ÊÄÅÂçèÂêåË°®Á§∫Â≠¶‰π†ÔºåÊõ¥Êñ∞ÊñáÊú¨ÂíåËßÜËßâÂéüÂûã„ÄÇ3) **Ëá™ÈÄÇÂ∫îÊõ¥Êñ∞ÊùÉÈáç**ÔºöÊ†πÊçÆÊï∞ÊçÆÁΩÆ‰ø°Â∫¶Âä®ÊÄÅË∞ÉÊï¥ÂéüÂûãÊõ¥Êñ∞ÁöÑÂπÖÂ∫¶„ÄÇ4) **Âä®ÊÄÅ‰ºòÂÖàÁ∫ßÈòüÂàó**ÔºöÂ≠òÂÇ®È´òÁΩÆ‰ø°Â∫¶ÂõæÂÉèÔºåÁî®‰∫é‰ªéÂéÜÂè≤ÂõæÂÉè‰∏≠Ëé∑ÂèñËßÜËßâÁü•ËØÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTOMCATÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊµãËØïÊó∂Áü•ËØÜÁ¥ØÁßØÂíåËá™ÈÄÇÂ∫îÊõ¥Êñ∞ÊùÉÈáç„ÄÇ‰º†ÁªüÁöÑCZSLÊñπÊ≥ïÂú®ËÆ≠ÁªÉÂÆåÊàêÂêéÊ®°ÂûãÂèÇÊï∞Âõ∫ÂÆöÔºåÊó†Ê≥ïÈÄÇÂ∫îÊµãËØïÊó∂ÁöÑÂàÜÂ∏ÉÂèòÂåñ„ÄÇTOMCATÈÄöËøáÂú®ÊµãËØïÊó∂ÊåÅÁª≠Â≠¶‰π†Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂä®ÊÄÅÈÄÇÂ∫îÊñ∞ÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÔºåÊòæËëóÊèêÂçá‰∫ÜÊ≥õÂåñËÉΩÂäõ„ÄÇËá™ÈÄÇÂ∫îÊõ¥Êñ∞ÊùÉÈáçÂàôÂπ≥Ë°°‰∫ÜÊñ∞Áü•ËØÜÁöÑÂ≠¶‰π†ÂíåÂ∑≤ÊúâÁü•ËØÜÁöÑ‰øùÁïô„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËá™ÈÄÇÂ∫îÊõ¥Êñ∞ÊùÉÈáçÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂÆÉÂü∫‰∫éÂõæÂÉèÁöÑÁΩÆ‰ø°Â∫¶Êù•Âä®ÊÄÅË∞ÉÊï¥Êõ¥Êñ∞ÂπÖÂ∫¶„ÄÇÈ´òÁΩÆ‰ø°Â∫¶ÁöÑÂõæÂÉèÂÖ∑ÊúâÊõ¥Â§ßÁöÑÊõ¥Êñ∞ÊùÉÈáçÔºåÊúâÂä©‰∫éÊ®°ÂûãÂø´ÈÄüÈÄÇÂ∫îÊñ∞ÁöÑÂàÜÂ∏É„ÄÇÂ§öÊ®°ÊÄÅÂçèÂêåË°®Á§∫Â≠¶‰π†ÈÄöËøáÂØπÈΩêÊñáÊú¨ÂíåËßÜËßâÂéüÂûãÔºå‰øùËØÅ‰∫ÜÁü•ËØÜÁöÑËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇÂä®ÊÄÅ‰ºòÂÖàÁ∫ßÈòüÂàóÂàôÂÖÅËÆ∏Ê®°ÂûãÂõûÈ°æÂéÜÂè≤ÁªèÈ™åÔºåÈÅøÂÖçÈÅóÂøòÈáçË¶Å‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

TOMCATÂú®Âõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂú®Â∞ÅÈó≠‰∏ñÁïåÂíåÂºÄÊîæ‰∏ñÁïåËÆæÁΩÆ‰∏ãÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊµãËØïÊó∂Áü•ËØÜÁ¥ØÁßØÂíåËá™ÈÄÇÂ∫îÊõ¥Êñ∞ÊùÉÈáçÊòØÊèêÂçáÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂõæÂÉèËØÜÂà´„ÄÅÁõÆÊ†áÊ£ÄÊµã„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÂàÜÂ∏É‰∏çÊñ≠ÂèòÂåñÁöÑÂú∫ÊôØ‰∏ã„ÄÇ‰æãÂ¶ÇÔºåÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåËΩ¶ËæÜÈúÄË¶ÅËØÜÂà´ÂêÑÁßçÊú™ÊõæËßÅËøáÁöÑÁâ©‰ΩìÁªÑÂêàÔºåTOMCATÂèØ‰ª•Â∏ÆÂä©ÊèêÈ´òËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÊ®°ÊÄÅÊï∞ÊçÆÔºåÂ¶ÇËØ≠Èü≥ÂíåÊñáÊú¨ÔºåÂÆûÁé∞Êõ¥ÂπøÊ≥õÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual knowledge from historical images for inference. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. Code will be available at https://github.com/xud-yan/TOMCAT .

