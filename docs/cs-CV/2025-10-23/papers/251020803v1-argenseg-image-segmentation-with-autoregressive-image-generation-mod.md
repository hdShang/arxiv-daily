---
layout: default
title: ARGenSeg: Image Segmentation with Autoregressive Image Generation Model
---

# ARGenSeg: Image Segmentation with Autoregressive Image Generation Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.20803" target="_blank" class="toolbar-btn">arXiv: 2510.20803v1</a>
    <a href="https://arxiv.org/pdf/2510.20803.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20803v1" 
            onclick="toggleFavorite(this, '2510.20803v1', 'ARGenSeg: Image Segmentation with Autoregressive Image Generation Model')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23

**å¤‡æ³¨**: Accepted to NeurIPS 2025, 18 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ARGenSegï¼šæå‡ºåŸºäºè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„å›¾åƒåˆ†å‰²æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒåˆ†å‰²` `è‡ªå›å½’ç”Ÿæˆæ¨¡å‹` `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `VQ-VAE` `åƒç´ çº§ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–ç¦»æ•£è¡¨ç¤ºæˆ–è¯­ä¹‰æç¤ºï¼Œé™åˆ¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ•è·ç»†ç²’åº¦è§†è§‰ç»†èŠ‚çš„èƒ½åŠ›ã€‚
2. ARGenSegåˆ©ç”¨MLLMç”Ÿæˆè§†è§‰tokensï¼Œå¹¶é€šè¿‡VQ-VAEå°†å…¶è½¬æ¢ä¸ºå›¾åƒï¼Œå®ç°åƒç´ çº§çš„åˆ†å‰²ç†è§£ã€‚
3. é€šè¿‡ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥å¹¶è¡Œç”Ÿæˆè§†è§‰tokensï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºè‡ªå›å½’ç”Ÿæˆæ¨¡å‹çš„å›¾åƒåˆ†å‰²èŒƒå¼ï¼ˆARGenSegï¼‰ï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶å†…å®ç°å¤šæ¨¡æ€ç†è§£å’Œåƒç´ çº§æ„ŸçŸ¥ã€‚ç°æœ‰å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­çš„å·¥ä½œé€šå¸¸é‡‡ç”¨è¾¹ç•Œç‚¹è¡¨ç¤ºæˆ–ä¸“ç”¨åˆ†å‰²å¤´ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºç¦»æ•£è¡¨ç¤ºæˆ–è¾“å…¥åˆ°ç‰¹å®šä»»åŠ¡è§£ç å™¨çš„è¯­ä¹‰æç¤ºï¼Œé™åˆ¶äº†MLLMæ•è·ç»†ç²’åº¦è§†è§‰ç»†èŠ‚çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå›¾åƒç”Ÿæˆçš„MLLMåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è‡ªç„¶åœ°ä¸ºç›®æ ‡å¯¹è±¡ç”Ÿæˆå¯†é›†æ©ç ã€‚æˆ‘ä»¬åˆ©ç”¨MLLMè¾“å‡ºè§†è§‰tokensï¼Œå¹¶ä½¿ç”¨é€šç”¨VQ-VAEå°†å…¶è§£tokenåŒ–ä¸ºå›¾åƒï¼Œä½¿åˆ†å‰²å®Œå…¨ä¾èµ–äºMLLMçš„åƒç´ çº§ç†è§£ã€‚ä¸ºäº†å‡å°‘æ¨ç†å»¶è¿Ÿï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥æ¥å¹¶è¡Œç”Ÿæˆæ‰€éœ€çš„è§†è§‰tokensã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šè¶…è¶Šäº†å…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ¨ç†é€Ÿåº¦æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•ï¼Œå¦‚åŸºäºè¾¹ç•Œç‚¹è¡¨ç¤ºæˆ–ä¸“ç”¨åˆ†å‰²å¤´çš„æ–¹æ³•ï¼Œä¾èµ–äºç¦»æ•£è¡¨ç¤ºæˆ–è¯­ä¹‰æç¤ºï¼Œæ— æ³•å……åˆ†åˆ©ç”¨MLLMçš„åƒç´ çº§ç†è§£èƒ½åŠ›ï¼Œé™åˆ¶äº†æ¨¡å‹æ•è·ç»†ç²’åº¦è§†è§‰ç»†èŠ‚çš„èƒ½åŠ›ã€‚è¿™äº›æ–¹æ³•åœ¨åˆ†å‰²ç²¾åº¦å’Œæ•ˆç‡ä¸Šå­˜åœ¨ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šARGenSegçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å›¾åƒåˆ†å‰²é—®é¢˜è½¬åŒ–ä¸ºå›¾åƒç”Ÿæˆé—®é¢˜ã€‚é€šè¿‡è®©MLLMç”Ÿæˆè§†è§‰tokensï¼Œç„¶åä½¿ç”¨VQ-VAEå°†è¿™äº›tokensè§£ç ä¸ºå›¾åƒï¼Œä»è€Œç›´æ¥ç”Ÿæˆå¯†é›†çš„åˆ†å‰²æ©ç ã€‚è¿™ç§æ–¹æ³•å……åˆ†åˆ©ç”¨äº†MLLMçš„ç”Ÿæˆèƒ½åŠ›å’Œåƒç´ çº§ç†è§£èƒ½åŠ›ï¼Œé¿å…äº†å¯¹ç¦»æ•£è¡¨ç¤ºçš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šARGenSegçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼šè´Ÿè´£æ¥æ”¶è¾“å…¥å›¾åƒå’Œæ–‡æœ¬æç¤ºï¼Œå¹¶ç”Ÿæˆè§†è§‰tokensã€‚2) VQ-VAEï¼šç”¨äºå°†MLLMç”Ÿæˆçš„è§†è§‰tokensè§£ç ä¸ºå›¾åƒï¼Œä»è€Œå¾—åˆ°åˆ†å‰²æ©ç ã€‚3) ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥ï¼šç”¨äºå¹¶è¡Œç”Ÿæˆè§†è§‰tokensï¼Œä»¥å‡å°‘æ¨ç†å»¶è¿Ÿã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œè¾“å…¥å›¾åƒå’Œæ–‡æœ¬æç¤ºåˆ°MLLMï¼ŒMLLMè¾“å‡ºè§†è§‰tokensï¼ŒVQ-VAEè§£ç tokensç”Ÿæˆåˆ†å‰²å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šARGenSegçš„å…³é”®åˆ›æ–°åœ¨äºå°†å›¾åƒåˆ†å‰²é—®é¢˜è½¬åŒ–ä¸ºå›¾åƒç”Ÿæˆé—®é¢˜ï¼Œå¹¶åˆ©ç”¨MLLMçš„ç”Ÿæˆèƒ½åŠ›ç›´æ¥ç”Ÿæˆå¯†é›†çš„åˆ†å‰²æ©ç ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒARGenSegé¿å…äº†å¯¹ç¦»æ•£è¡¨ç¤ºçš„ä¾èµ–ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨MLLMçš„åƒç´ çº§ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥çš„å¼•å…¥æ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚

**å…³é”®è®¾è®¡**ï¼šARGenSegçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é€šç”¨çš„VQ-VAEè¿›è¡Œè§†è§‰tokensçš„è§£ç ï¼Œä¿è¯äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚2) é‡‡ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥ï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆè§†è§‰tokensæ¥å‡å°‘æ¨ç†å»¶è¿Ÿã€‚3) æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘åˆ†å‰²ç²¾åº¦å’Œç”Ÿæˆè´¨é‡ï¼Œå¯èƒ½åŒ…æ‹¬äº¤å‰ç†µæŸå¤±å’Œç”Ÿæˆå¯¹æŠ—æŸå¤±ç­‰ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ARGenSegåœ¨å¤šä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚åŒæ—¶ï¼Œé€šè¿‡ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥ï¼Œæ¨ç†é€Ÿåº¦å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ï¼Œä¾‹å¦‚åœ¨æŸä¸ªæ•°æ®é›†ä¸ŠmIOUæå‡äº†X%ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†Yå€ç­‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ARGenSegåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å›¾åƒåˆ†æã€é¥æ„Ÿå›¾åƒå¤„ç†ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥ç”¨äºç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²ç­‰ä»»åŠ¡ï¼Œæé«˜å›¾åƒç†è§£å’Œåˆ†æçš„ç²¾åº¦å’Œæ•ˆç‡ã€‚æœªæ¥ï¼ŒARGenSegå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘åˆ†å‰²ã€ä¸‰ç»´é‡å»ºç­‰é¢†åŸŸï¼Œä¸ºäººå·¥æ™ºèƒ½åº”ç”¨æä¾›æ›´å¼ºå¤§çš„æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.

