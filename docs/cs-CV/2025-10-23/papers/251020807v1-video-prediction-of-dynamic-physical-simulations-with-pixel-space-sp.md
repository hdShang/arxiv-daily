---
layout: default
title: Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers
---

# Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20807" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.20807v1</a>
  <a href="https://arxiv.org/pdf/2510.20807.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20807v1" onclick="toggleFavorite(this, '2510.20807v1', 'Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-23

**Â§áÊ≥®**: 14 pages, 14 figures

**ÊúüÂàä**: IEEE Transactions on Neural Networks and Learning Systems, 36, 19106-19118, 2025

**DOI**: [10.1109/TNNLS.2025.3585949](https://doi.org/10.1109/TNNLS.2025.3585949)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂÉèÁ¥†Á©∫Èó¥Êó∂Á©∫TransformerÁöÑÁâ©ÁêÜÊ®°ÊãüËßÜÈ¢ëÈ¢ÑÊµãÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÈ¢ÑÊµã` `Êó∂Á©∫Transformer` `Áâ©ÁêÜÊ®°Êãü` `Ëá™ÂõûÂΩíÊ®°Âûã` `ÂÉèÁ¥†Á©∫Èó¥` `ÂèØËß£ÈáäÊÄß` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÁîüÊàêÊñπÊ≥ïÂú®Áâ©ÁêÜÊ®°ÊãüÁöÑÊó∂Á©∫Êé®ÁêÜÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÈöæ‰ª•ËøõË°åÈïøÊó∂Èó¥ÁöÑÁ≤æÁ°ÆÈ¢ÑÊµã„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÂÉèÁ¥†Á©∫Èó¥Êó∂Á©∫TransformerÁöÑËá™ÂõûÂΩíËßÜÈ¢ëÈ¢ÑÊµãÊ®°ÂûãÔºåÊó†ÈúÄÂ§çÊùÇÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÊàñÊΩúÂú®ÁâπÂæÅÂ≠¶‰π†„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Áâ©ÁêÜÁ≤æÁ°ÆÈ¢ÑÊµãÁöÑÊó∂Èó¥ËåÉÂõ¥‰∏äÔºåÁõ∏ÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÂçá‰∫ÜÈ´òËææ50%ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁõ∏ÂΩìÁöÑËßÜÈ¢ëË¥®Èáè„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Á†îÁ©∂ÂèóËá™ÂõûÂΩíÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊÄßËÉΩÂíåÂèØÊâ©Â±ïÊÄßÂêØÂèëÔºåÊé¢Á¥¢‰∫Ü‰∏ÄÁßçÂü∫‰∫éTransformerÁöÑËßÜÈ¢ëÈ¢ÑÊµãÊñπÊ≥ïÔºåÂπ∂ÊØîËæÉ‰∫ÜÂêÑÁßçÊó∂Á©∫Ëá™Ê≥®ÊÑèÂäõÂ∏ÉÂ±Ä„ÄÇËØ•ÊñπÊ≥ï‰∏ìÊ≥®‰∫éÁâ©ÁêÜÊ®°ÊãüÁöÑÂõ†ÊûúÂª∫Ê®°ÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜÈ¢ëÁîüÊàêÊñπÊ≥ïÂú®Êó∂Á©∫Êé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÈÄöËøáÁâ©ÁêÜÂØπË±°Ë∑üË∏™ÊåáÊ†áÂíåÁâ©ÁêÜÊ®°ÊãüÊï∞ÊçÆÈõÜ‰∏äÁöÑÊó†ÁõëÁù£ËÆ≠ÁªÉÊù•ÂàÜÁ¶ªÊó∂Á©∫Êé®ÁêÜ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÁ∫ØTransformerÊ®°ÂûãÔºåÁî®‰∫éËá™ÂõûÂΩíËßÜÈ¢ëÈ¢ÑÊµãÔºåÂà©Áî®ËøûÁª≠ÂÉèÁ¥†Á©∫Èó¥Ë°®Á§∫ËøõË°åËßÜÈ¢ëÈ¢ÑÊµã„ÄÇÊó†ÈúÄÂ§çÊùÇÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÊàñÊΩúÂú®ÁâπÂæÅÂ≠¶‰π†ÁªÑ‰ª∂Ôºå‰∏éÁé∞ÊúâÁöÑÊΩúÂú®Á©∫Èó¥ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®‰øùÊåÅÁõ∏ÂΩìÁöÑËßÜÈ¢ëË¥®ÈáèÊåáÊ†áÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÂ∞ÜÁâ©ÁêÜÁ≤æÁ°ÆÈ¢ÑÊµãÁöÑÊó∂Èó¥ËåÉÂõ¥ÊòæËëóÂª∂Èïø‰∫ÜÈ´òËææ50%„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøõË°å‰∫ÜÂèØËß£ÈáäÊÄßÂÆûÈ™åÔºå‰ª•ËØÜÂà´ÁΩëÁªú‰∏≠ÁºñÁ†Å‰∫ÜÂèØÁî®‰∫éÂáÜÁ°Æ‰º∞ËÆ°PDEÊ®°ÊãüÂèÇÊï∞ÁöÑ‰ø°ÊÅØÁöÑÂå∫ÂüüÔºåÂπ∂ÈÄöËøáÊé¢ÊµãÊ®°ÂûãÂèëÁé∞ËøôÂèØ‰ª•Êé®ÂπøÂà∞ÂØπË∂ÖÂá∫ÂàÜÂ∏ÉÁöÑÊ®°ÊãüÂèÇÊï∞ÁöÑ‰º∞ËÆ°„ÄÇËøôÈ°πÂ∑•‰Ωú‰∏∫Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑËßÜÈ¢ëÊó∂Á©∫Âª∫Ê®°Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÁÆÄÂçï„ÄÅÂèÇÊï∞È´òÊïà‰∏îÂèØËß£ÈáäÁöÑÂπ≥Âè∞„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜÈ¢ëÈ¢ÑÊµãÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂä®ÊÄÅÁâ©ÁêÜÊ®°ÊãüÊó∂ÔºåÈöæ‰ª•ËøõË°åÈïøÊó∂Èó¥ÁöÑÁ≤æÁ°ÆÈ¢ÑÊµãÔºåÂπ∂‰∏îÁº∫‰πèÂØπÊó∂Á©∫Êé®ÁêÜÁöÑÊúâÊïàÂª∫Ê®°„ÄÇËÆ∏Â§öÊñπÊ≥ï‰æùËµñ‰∫éÂ§çÊùÇÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÊàñÊΩúÂú®ÁâπÂæÅÂ≠¶‰π†ÔºåÂ¢ûÂä†‰∫ÜÊ®°ÂûãÁöÑÂ§çÊùÇÊÄßÂíåËÆ≠ÁªÉÈöæÂ∫¶„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®TransformerÊ®°ÂûãÂº∫Â§ßÁöÑÊó∂Á©∫Âª∫Ê®°ËÉΩÂäõÔºåÁõ¥Êé•Âú®ÂÉèÁ¥†Á©∫Èó¥ËøõË°åËá™ÂõûÂΩíËßÜÈ¢ëÈ¢ÑÊµã„ÄÇÈÄöËøáÁÆÄÂåñÊ®°ÂûãÁªìÊûÑÔºåÈÅøÂÖçÂ§çÊùÇÁöÑÊΩúÂú®ÁâπÂæÅÂ≠¶‰π†Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÈ¢ÑÊµãÁ≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÈááÁî®‰∏Ä‰∏™Á∫ØTransformerÊ®°ÂûãÔºåÁõ¥Êé•‰ª•ËøûÁª≠ÂÉèÁ¥†Á©∫Èó¥Ë°®Á§∫‰Ωú‰∏∫ËæìÂÖ•ÂíåËæìÂá∫„ÄÇÊ®°ÂûãÈÄöËøáËá™ÂõûÂΩíÁöÑÊñπÂºèÔºåÈÄêÂ∏ßÈ¢ÑÊµãËßÜÈ¢ëÂ∫èÂàó„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨ÔºöËæìÂÖ•ÂéÜÂè≤ËßÜÈ¢ëÂ∏ßÔºåÈÄöËøáTransformerÊ®°ÂûãËøõË°åÊó∂Á©∫ÁâπÂæÅÊèêÂèñÂíåÈ¢ÑÊµãÔºåÁîüÊàê‰∏ã‰∏ÄÂ∏ßÁöÑÂÉèÁ¥†Ë°®Á§∫ÔºåÂπ∂Â∞ÜÈ¢ÑÊµãÁªìÊûú‰Ωú‰∏∫‰∏ã‰∏ÄÊ≠•ÁöÑËæìÂÖ•ÔºåÂæ™ÁéØËøõË°å„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÁõ¥Êé•Âú®ÂÉèÁ¥†Á©∫Èó¥ËøõË°åÊó∂Á©∫Âª∫Ê®°ÔºåÈÅøÂÖç‰∫ÜÂ§çÊùÇÁöÑÊΩúÂú®Á©∫Èó¥Êò†Â∞Ñ„ÄÇËøôÁßçÊñπÊ≥ïÁÆÄÂåñ‰∫ÜÊ®°ÂûãÁªìÊûÑÔºåÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéáÔºåÂπ∂‰∏îËÉΩÂ§üÊõ¥Â•ΩÂú∞‰øùÁïôËßÜÈ¢ë‰∏≠ÁöÑÁªÜËäÇ‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊé¢Á¥¢‰∫Ü‰∏çÂêåÁöÑÊó∂Á©∫Ëá™Ê≥®ÊÑèÂäõÂ∏ÉÂ±ÄÔºå‰ª•‰ºòÂåñÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ®°ÂûãÈááÁî®Ê†áÂáÜÁöÑTransformerÁªìÊûÑÔºåÂåÖÊã¨Ëá™Ê≥®ÊÑèÂäõÂ±ÇÂíåÂâçÈ¶àÁ•ûÁªèÁΩëÁªú„ÄÇÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1Ôºâ‰ΩøÁî®ËøûÁª≠ÂÉèÁ¥†Á©∫Èó¥Ë°®Á§∫‰Ωú‰∏∫ËæìÂÖ•ÂíåËæìÂá∫Ôºõ2ÔºâÈááÁî®Ëá™ÂõûÂΩíÁöÑÈ¢ÑÊµãÊñπÂºèÔºõ3ÔºâÊé¢Á¥¢‰∏çÂêåÁöÑÊó∂Á©∫Ëá™Ê≥®ÊÑèÂäõÂ∏ÉÂ±ÄÔºå‰æãÂ¶ÇÔºåÂ∞ÜÁ©∫Èó¥ÂíåÊó∂Èó¥Ê≥®ÊÑèÂäõÂàÜÂºÄÂ§ÑÁêÜÔºåÊàñËÄÖÈááÁî®ÂÖ®Â±ÄÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÇÊçüÂ§±ÂáΩÊï∞ÈÄöÂ∏∏ÈááÁî®ÂÉèÁ¥†Á∫ßÂà´ÁöÑÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÊàñÁ±ª‰ººÁöÑÂ∫¶Èáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Áâ©ÁêÜÁ≤æÁ°ÆÈ¢ÑÊµãÁöÑÊó∂Èó¥ËåÉÂõ¥‰∏äÔºåÁõ∏ÊØîÁé∞ÊúâÁöÑÊΩúÂú®Á©∫Èó¥ÊñπÊ≥ïÊèêÂçá‰∫ÜÈ´òËææ50%ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁõ∏ÂΩìÁöÑËßÜÈ¢ëË¥®ÈáèÊåáÊ†áÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂèØËß£ÈáäÊÄßÂÆûÈ™åÔºåËÆ∫ÊñáËøòÂèëÁé∞ÁΩëÁªú‰∏≠Â≠òÂú®ÁºñÁ†Å‰∫ÜÂèØÁî®‰∫éÂáÜÁ°Æ‰º∞ËÆ°PDEÊ®°ÊãüÂèÇÊï∞ÁöÑ‰ø°ÊÅØÁöÑÂå∫ÂüüÔºåÂπ∂‰∏îËøôÁßçËÉΩÂäõÂèØ‰ª•Êé®ÂπøÂà∞ÂØπË∂ÖÂá∫ÂàÜÂ∏ÉÁöÑÊ®°ÊãüÂèÇÊï∞ÁöÑ‰º∞ËÆ°„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÁ≤æÁ°ÆËßÜÈ¢ëÈ¢ÑÊµãÁöÑÈ¢ÜÂüüÔºå‰æãÂ¶ÇÔºöËá™Âä®È©æÈ©∂‰∏≠ÁöÑÁéØÂ¢ÉÈ¢ÑÊµã„ÄÅÊú∫Âô®‰∫∫ÂØºËà™‰∏≠ÁöÑÂú∫ÊôØÁêÜËß£„ÄÅ‰ª•ÂèäÁßëÂ≠¶ËÆ°ÁÆó‰∏≠ÁöÑÁâ©ÁêÜÊ®°ÊãüÂèØËßÜÂåñ„ÄÇÈÄöËøáÊèêÈ´òËßÜÈ¢ëÈ¢ÑÊµãÁöÑÁ≤æÂ∫¶ÂíåÊó∂Èó¥ËåÉÂõ¥ÔºåÂèØ‰ª•Â∏ÆÂä©Áõ∏ÂÖ≥Á≥ªÁªüÂÅöÂá∫Êõ¥ÂáÜÁ°ÆÁöÑÂÜ≥Á≠ñÔºåÂπ∂ÊèêÈ´òÂÖ∂È≤ÅÊ£íÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÁöÑÂèØËß£ÈáäÊÄß‰πü‰ΩøÂÖ∂Âú®ÁßëÂ≠¶Á†îÁ©∂‰∏≠ÂÖ∑ÊúâÊΩúÂú®ÁöÑÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.

