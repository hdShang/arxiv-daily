---
layout: default
title: Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning
---

# Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20519" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20519v2</a>
  <a href="https://arxiv.org/pdf/2510.20519.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20519v2" onclick="toggleFavorite(this, '2510.20519v2', 'Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaohan Lan, Fanfan Liu, Haibo Qiu, Siqi Yang, Delian Ruan, Peng Shi, Lin Ma

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23 (æ›´æ–°: 2025-11-25)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MM-Thinking/Metis-HOME)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMetis-HOMEï¼Œé€šè¿‡æ··åˆä¸“å®¶æ¨¡å‹è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ•ˆç‡ä¸æ³›åŒ–éš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `æ··åˆä¸“å®¶æ¨¡å‹` `æ¨¡å‹æ•ˆç‡` `æ¨¡å‹æ³›åŒ–` `è§†è§‰é—®ç­”` `OCR` `Qwen2.5-VL-7B`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†æ—¶ï¼Œå³ä½¿é¢å¯¹ç®€å•é—®é¢˜ä¹Ÿé‡‡ç”¨å¤æ‚è®¡ç®—ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œä¸”ç‰ºç‰²äº†é€šç”¨èƒ½åŠ›ã€‚
2. Metis-HOMEæ„å»ºæ··åˆä¸“å®¶æ¨¡å‹ï¼ŒåŒºåˆ†â€œæ€è€ƒâ€å’Œâ€œéæ€è€ƒâ€åˆ†æ”¯ï¼Œå¹¶ç”¨è½»é‡çº§è·¯ç”±å™¨åŠ¨æ€åˆ†é…æŸ¥è¯¢ï¼Œå®ç°é«˜æ•ˆæ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMetis-HOMEä¸ä»…æå‡äº†å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ”¹å–„äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œå…‹æœäº†æ¨ç†ä¸“ç”¨æ¨¡å‹å¸¸è§çš„æ³›åŒ–èƒ½åŠ›ä¸‹é™é—®é¢˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å—å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›è¿›å±•çš„å¯å‘ï¼Œå¤šæ¨¡æ€æ¨ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œåœ¨å¤æ‚çš„æ•°å­¦é—®é¢˜æ±‚è§£ç­‰ä»»åŠ¡ä¸Šè·å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šå³ä½¿å¯¹äºç®€å•çš„æŸ¥è¯¢ï¼Œå®ƒä»¬ä¹Ÿå€¾å‘äºé‡‡ç”¨è®¡ç®—æˆæœ¬é«˜æ˜‚çš„æ¨ç†ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼›æ­¤å¤–ï¼Œå¯¹ä¸“é—¨æ¨ç†çš„å…³æ³¨å¾€å¾€ä¼šæŸå®³å…¶æ›´å¹¿æ³›ã€æ›´é€šç”¨çš„ç†è§£èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†Metis-HOMEï¼šä¸€ç§æ··åˆä¼˜åŒ–çš„æ··åˆä¸“å®¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ç§æƒè¡¡ã€‚Metis-HOMEé€šè¿‡å°†åŸå§‹å¯†é›†æ¨¡å‹æ„å»ºæˆä¸¤ä¸ªä¸åŒçš„ä¸“å®¶åˆ†æ”¯æ¥å®ç°â€œæ··åˆæ€ç»´â€èŒƒå¼ï¼šä¸€ä¸ªæ€è€ƒåˆ†æ”¯ï¼Œä¸“é—¨ç”¨äºå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ï¼›ä¸€ä¸ªéæ€è€ƒåˆ†æ”¯ï¼Œé’ˆå¯¹è¯¸å¦‚é€šç”¨VQAå’ŒOCRç­‰ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®ç°å¿«é€Ÿã€ç›´æ¥çš„æ¨ç†ã€‚ä¸€ä¸ªè½»é‡çº§çš„ã€å¯è®­ç»ƒçš„è·¯ç”±å™¨åŠ¨æ€åœ°å°†æŸ¥è¯¢åˆ†é…ç»™æœ€åˆé€‚çš„ä¸“å®¶ã€‚æˆ‘ä»¬é€šè¿‡å°†Qwen2.5-VL-7Bé€‚é…æˆMoEæ¶æ„æ¥å®ä¾‹åŒ–Metis-HOMEã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æ˜¾è‘—å¢å¼ºäº†å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œè¿˜æé«˜äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œæ‰­è½¬äº†å…¶ä»–æ¨ç†ä¸“ç”¨æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°çš„æ€§èƒ½ä¸‹é™è¶‹åŠ¿ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæ„å»ºå¼ºå¤§è€Œé€šç”¨çš„MLLMå»ºç«‹äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ™®éå­˜åœ¨çš„æ¨ç†ä¸æ³›åŒ–å›°å¢ƒã€‚ä»£ç å’Œæƒé‡å¯åœ¨https://github.com/MM-Thinking/Metis-HOMEè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰å¤šæ¨¡æ€å¤§å‹æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œå³ä½¿æ˜¯ç®€å•çš„è§†è§‰é—®ç­”æˆ–OCRä»»åŠ¡ï¼Œæ¨¡å‹ä¹Ÿå€¾å‘äºä½¿ç”¨å¤æ‚çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ã€‚åŒæ—¶ï¼Œä¸ºäº†æå‡ç‰¹å®šæ¨ç†ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ¨¡å‹å¾€å¾€ç‰ºç‰²äº†åœ¨å…¶ä»–é€šç”¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œé€ æˆäº†æ¨ç†èƒ½åŠ›å’Œé€šç”¨èƒ½åŠ›ä¹‹é—´çš„trade-offã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMetis-HOMEçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture-of-Experts, MoEï¼‰ï¼Œå°†åŸå§‹çš„å¯†é›†æ¨¡å‹åˆ†è§£ä¸ºä¸¤ä¸ªä¸“é—¨åŒ–çš„åˆ†æ”¯ï¼šä¸€ä¸ªâ€œæ€è€ƒâ€åˆ†æ”¯å’Œä¸€ä¸ªâ€œéæ€è€ƒâ€åˆ†æ”¯ã€‚â€œæ€è€ƒâ€åˆ†æ”¯ä¸“æ³¨äºå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ï¼Œè€Œâ€œéæ€è€ƒâ€åˆ†æ”¯åˆ™é’ˆå¯¹ç®€å•çš„ã€ç›´æ¥çš„æ¨ç†ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®è¾“å…¥çš„ä¸åŒï¼ŒåŠ¨æ€åœ°é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶åˆ†æ”¯è¿›è¡Œå¤„ç†ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMetis-HOMEçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åŸå§‹çš„å¯†é›†æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen2.5-VL-7Bï¼‰ï¼›2) â€œæ€è€ƒâ€ä¸“å®¶åˆ†æ”¯ï¼Œç”¨äºå¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼›3) â€œéæ€è€ƒâ€ä¸“å®¶åˆ†æ”¯ï¼Œç”¨äºå¤„ç†ç®€å•çš„ç›´æ¥æ¨ç†ä»»åŠ¡ï¼›4) ä¸€ä¸ªè½»é‡çº§çš„å¯è®­ç»ƒè·¯ç”±å™¨ï¼Œç”¨äºæ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€åœ°å°†æŸ¥è¯¢åˆ†é…ç»™åˆé€‚çš„ä¸“å®¶åˆ†æ”¯ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè·¯ç”±å™¨å­¦ä¹ å¦‚ä½•æ ¹æ®è¾“å…¥ç‰¹å¾çš„å¤æ‚ç¨‹åº¦ï¼Œå°†æŸ¥è¯¢åˆ†é…ç»™æœ€åˆé€‚çš„ä¸“å®¶åˆ†æ”¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šMetis-HOMEçš„å…³é”®åˆ›æ–°åœ¨äºå…¶â€œæ··åˆæ€ç»´â€çš„èŒƒå¼ï¼Œå³é€šè¿‡æ„å»ºä¸“é—¨åŒ–çš„ä¸“å®¶åˆ†æ”¯ï¼Œå¹¶ä½¿ç”¨è·¯ç”±å™¨åŠ¨æ€åœ°é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡Œå¤„ç†ï¼Œä»è€Œåœ¨æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹ç›¸æ¯”ï¼ŒMetis-HOMEèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„å¤æ‚ç¨‹åº¦è‡ªé€‚åº”åœ°é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼ï¼Œä»è€Œé¿å…äº†ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šMetis-HOMEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä¸“å®¶åˆ†æ”¯çš„ç»“æ„è®¾è®¡ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–ï¼›2) è·¯ç”±å™¨çš„è®¾è®¡ï¼Œéœ€è¦ä¿è¯å…¶è½»é‡çº§å’Œé«˜æ•ˆæ€§ï¼ŒåŒæ—¶èƒ½å¤Ÿå‡†ç¡®åœ°å°†æŸ¥è¯¢åˆ†é…ç»™åˆé€‚çš„ä¸“å®¶åˆ†æ”¯ï¼›3) è®­ç»ƒç­–ç•¥çš„è®¾è®¡ï¼Œéœ€è¦ä¿è¯ä¸“å®¶åˆ†æ”¯å’Œè·¯ç”±å™¨èƒ½å¤ŸååŒå·¥ä½œï¼Œä»è€Œå®ç°æœ€ä½³çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Metis-HOMEåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶é¿å…äº†é€šç”¨èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡å¤æ‚æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œè¿˜æé«˜äº†æ¨¡å‹åœ¨é€šç”¨VQAå’ŒOCRç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨ç†ä¸æ³›åŒ–ä¹‹é—´çš„trade-offã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Metis-HOMEé€‚ç”¨äºå„ç§éœ€è¦å¤šæ¨¡æ€æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚å®ƒå¯ä»¥æé«˜è¿™äº›åº”ç”¨åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå¹¶æå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°æ›´å¤šçš„æ¨¡æ€å’Œä»»åŠ¡ï¼Œæ„å»ºæ›´åŠ é€šç”¨å’Œå¼ºå¤§çš„å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inspired by recent advancements in LLM reasoning, the field of multimodal reasoning has seen remarkable progress, achieving significant performance gains on intricate tasks such as mathematical problem-solving. Despite this progress, current multimodal large reasoning models exhibit two key limitations. They tend to employ computationally expensive reasoning even for simple queries, leading to inefficiency. Furthermore, this focus on specialized reasoning often impairs their broader, more general understanding capabilities. In this paper, we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by structuring the original dense model into two distinct expert branches: a thinking branch tailored for complex, multi-step reasoning, and a non-thinking branch optimized for rapid, direct inference on tasks like general VQA and OCR. A lightweight, trainable router dynamically allocates queries to the most suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into an MoE architecture. Comprehensive evaluations reveal that our approach not only substantially enhances complex reasoning abilities but also improves the model's general capabilities, reversing the degradation trend observed in other reasoning-specialized models. Our work establishes a new paradigm for building powerful and versatile MLLMs, effectively resolving the prevalent reasoning-vs-generalization dilemma. Code and weights are available at https://github.com/MM-Thinking/Metis-HOME.

