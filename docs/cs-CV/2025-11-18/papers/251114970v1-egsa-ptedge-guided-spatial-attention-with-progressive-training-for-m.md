---
layout: default
title: EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects
---

# EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.14970" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.14970v1</a>
  <a href="https://arxiv.org/pdf/2511.14970.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14970v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.14970v1', 'EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gbenga Omotara, Ramy Farag, Seyed Mohamad Ali Tousi, G. N. DeSouza

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEGSA-PTï¼Œé€šè¿‡è¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›å’Œæ¸è¿›å¼è®­ç»ƒæå‡é€æ˜ç‰©ä½“æ·±åº¦ä¼°è®¡ä¸åˆ†å‰²æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `é€æ˜ç‰©ä½“æ„ŸçŸ¥` `æ·±åº¦ä¼°è®¡` `è¯­ä¹‰åˆ†å‰²` `è¾¹ç¼˜å¼•å¯¼` `ç©ºé—´æ³¨æ„åŠ›` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¸è¿›å¼è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é€æ˜ç‰©ä½“æ„ŸçŸ¥æ˜¯è®¡ç®—æœºè§†è§‰éš¾é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¤šä»»åŠ¡å­¦ä¹ æ˜“å—è´Ÿé¢è·¨ä»»åŠ¡å½±å“ã€‚
2. æå‡ºè¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›ï¼ˆEGSAï¼‰æœºåˆ¶ï¼Œèåˆè¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ï¼Œåˆ©ç”¨è¾¹ç¼˜ä¿¡æ¯ç¼“è§£è´Ÿé¢äº¤äº’ï¼Œæå‡é€æ˜ç‰©ä½“æ„ŸçŸ¥ã€‚
3. å¼•å…¥å¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒï¼Œä»RGBè¾¹ç¼˜åˆ°é¢„æµ‹æ·±åº¦è¾¹ç¼˜ï¼Œæ— éœ€ground-truthæ·±åº¦å³å¯å¼•å¯¼å­¦ä¹ ï¼Œå¹¶åœ¨Syn-TODDå’ŒClearPoseä¸ŠéªŒè¯æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€æ˜ç‰©ä½“æ„ŸçŸ¥æ˜¯è®¡ç®—æœºè§†è§‰ç ”ç©¶ä¸­çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºé€æ˜æ€§ä¼šæ··æ·†æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ä»¥æé«˜é²æ£’æ€§ï¼Œä½†è´Ÿé¢çš„è·¨ä»»åŠ¡äº¤äº’é€šå¸¸ä¼šé˜»ç¢æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›ï¼ˆEGSAï¼‰èåˆæœºåˆ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è¾¹ç•Œä¿¡æ¯æ•´åˆåˆ°è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ä¹‹é—´çš„èåˆä¸­æ¥å‡è½»ç ´åæ€§äº¤äº’ã€‚åœ¨Syn-TODDå’ŒClearPoseåŸºå‡†æµ‹è¯•ä¸­ï¼ŒEGSAå§‹ç»ˆä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ˆMODESTï¼‰ï¼Œæé«˜äº†æ·±åº¦ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨é€æ˜åŒºåŸŸçš„æ”¹è¿›æœ€ä¸ºæ˜¾è‘—ã€‚é™¤äº†èåˆè®¾è®¡ä¹‹å¤–ï¼Œæœ¬æ–‡çš„ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯ä¸€ç§å¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œå…¶ä¸­å­¦ä¹ ä»RGBå›¾åƒå¯¼å‡ºçš„è¾¹ç¼˜è¿‡æ¸¡åˆ°ä»é¢„æµ‹æ·±åº¦å›¾åƒå¯¼å‡ºçš„è¾¹ç¼˜ã€‚è¿™ç§æ–¹æ³•å…è®¸ç³»ç»Ÿä»RGBå›¾åƒä¸­åŒ…å«çš„ä¸°å¯Œçº¹ç†ä¸­å¼•å¯¼å­¦ä¹ ï¼Œç„¶ååˆ‡æ¢åˆ°æ·±åº¦å›¾ä¸­æ›´ç›¸å…³çš„å‡ ä½•å†…å®¹ï¼ŒåŒæ—¶æ¶ˆé™¤äº†è®­ç»ƒæ—¶å¯¹ground-truthæ·±åº¦çš„éœ€æ±‚ã€‚æ€»ä¹‹ï¼Œè¿™äº›è´¡çŒ®çªå‡ºäº†è¾¹ç¼˜å¼•å¯¼èåˆä½œä¸ºä¸€ç§èƒ½å¤Ÿæé«˜é€æ˜ç‰©ä½“æ„ŸçŸ¥çš„é²æ£’æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé€æ˜ç‰©ä½“çš„æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„éš¾ç‚¹ã€‚ç”±äºé€æ˜ç‰©ä½“ç¼ºä¹çº¹ç†å’Œé¢œè‰²ä¿¡æ¯ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å‡†ç¡®ä¼°è®¡å…¶æ·±åº¦å’Œè¿›è¡Œåˆ†å‰²ã€‚ç°æœ‰çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•è¯•å›¾åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œä½†å¸¸å¸¸å—åˆ°è´Ÿé¢è·¨ä»»åŠ¡äº¤äº’çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¾¹ç¼˜ä¿¡æ¯æ¥å¼•å¯¼ç‰¹å¾èåˆï¼Œä»è€Œå‡è½»è´Ÿé¢è·¨ä»»åŠ¡äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›ï¼ˆEGSAï¼‰æœºåˆ¶ï¼Œå°†è¾¹ç¼˜ä¿¡æ¯æ•´åˆåˆ°è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾çš„èåˆä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´åŠ å…³æ³¨ç‰©ä½“çš„è¾¹ç•Œï¼Œä»è€Œæé«˜æ·±åº¦ä¼°è®¡å’Œåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œé¿å…äº†å¯¹ground-truthæ·±åº¦çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ä¸¤ä¸ªåˆ†æ”¯ï¼Œä»¥åŠEGSAèåˆæ¨¡å—å’Œæ¸è¿›å¼è®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆï¼ŒRGBå›¾åƒåˆ†åˆ«è¾“å…¥åˆ°æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ç½‘ç»œä¸­ï¼Œæå–å‡ ä½•ç‰¹å¾å’Œè¯­ä¹‰ç‰¹å¾ã€‚ç„¶åï¼ŒEGSAæ¨¡å—åˆ©ç”¨è¾¹ç¼˜ä¿¡æ¯å¯¹è¿™ä¸¤ä¸ªç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°èåˆåçš„ç‰¹å¾ã€‚æœ€åï¼Œåˆ©ç”¨èåˆåçš„ç‰¹å¾è¿›è¡Œæ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ã€‚æ¸è¿›å¼è®­ç»ƒç­–ç•¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨RGBå›¾åƒçš„è¾¹ç¼˜ä¿¡æ¯è¿›è¡Œè®­ç»ƒï¼›ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨é¢„æµ‹æ·±åº¦å›¾åƒçš„è¾¹ç¼˜ä¿¡æ¯è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è¾¹ç¼˜å¼•å¯¼ç©ºé—´æ³¨æ„åŠ›ï¼ˆEGSAï¼‰æœºåˆ¶å’Œå¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒç­–ç•¥ã€‚EGSAæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆè¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¾¹ç¼˜ä¿¡æ¯æ¥å¼•å¯¼ç‰¹å¾èåˆï¼Œä»è€Œæé«˜æ·±åº¦ä¼°è®¡å’Œåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å¤šæ¨¡æ€æ¸è¿›å¼è®­ç»ƒç­–ç•¥èƒ½å¤Ÿé€æ­¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œé¿å…äº†å¯¹ground-truthæ·±åº¦çš„ä¾èµ–ï¼Œå¹¶ä¸”èƒ½å¤Ÿåˆ©ç”¨RGBå›¾åƒå’Œæ·±åº¦å›¾åƒçš„äº’è¡¥ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šEGSAæ¨¡å—ä½¿ç”¨ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®è¾¹ç¼˜ä¿¡æ¯å¯¹ç‰¹å¾å›¾çš„ä¸åŒåŒºåŸŸè¿›è¡ŒåŠ æƒã€‚è¾¹ç¼˜ä¿¡æ¯é€šè¿‡Cannyè¾¹ç¼˜æ£€æµ‹å™¨ä»RGBå›¾åƒæˆ–é¢„æµ‹æ·±åº¦å›¾åƒä¸­æå–ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ·±åº¦ä¼°è®¡æŸå¤±å’Œè¯­ä¹‰åˆ†å‰²æŸå¤±ã€‚æ·±åº¦ä¼°è®¡æŸå¤±é‡‡ç”¨L1æŸå¤±æˆ–Smooth L1æŸå¤±ã€‚è¯­ä¹‰åˆ†å‰²æŸå¤±é‡‡ç”¨äº¤å‰ç†µæŸå¤±ã€‚ç½‘ç»œç»“æ„å¯ä»¥é‡‡ç”¨å„ç§ç°æœ‰çš„æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼Œä¾‹å¦‚ResNetã€UNetç­‰ã€‚æ¸è¿›å¼è®­ç»ƒç­–ç•¥ä¸­ï¼Œä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒè½®æ•°å’Œå­¦ä¹ ç‡éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EGSAåœ¨Syn-TODDå’ŒClearPoseæ•°æ®é›†ä¸Šå‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•MODESTï¼Œå°¤å…¶åœ¨é€æ˜åŒºåŸŸçš„æ·±åº¦ä¼°è®¡ç²¾åº¦æå‡æ˜¾è‘—ã€‚åœ¨ä¿æŒåˆ†å‰²æ€§èƒ½çš„åŒæ—¶ï¼Œæ·±åº¦ä¼°è®¡ç²¾åº¦å¾—åˆ°äº†æœ‰æ•ˆæé«˜ï¼Œè¯æ˜äº†è¾¹ç¼˜å¼•å¯¼èåˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæå‡ºçš„æ¸è¿›å¼è®­ç»ƒç­–ç•¥æ— éœ€ground-truthæ·±åº¦å³å¯å®ç°æœ‰æ•ˆè®­ç»ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæŠ“å–é€æ˜ç‰©ä½“æ—¶ï¼Œå‡†ç¡®çš„æ·±åº¦ä¼°è®¡å’Œåˆ†å‰²æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œè¯†åˆ«é€æ˜ç‰©ä½“ï¼ˆå¦‚ç»ç’ƒã€æ°´é¢ï¼‰æœ‰åŠ©äºæé«˜å®‰å…¨æ€§ã€‚åœ¨å¢å¼ºç°å®ä¸­ï¼Œå¯ä»¥å®ç°æ›´é€¼çœŸçš„é€æ˜ç‰©ä½“æ¸²æŸ“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.

