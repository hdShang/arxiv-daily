---
layout: default
title: Human Action Recognition from Point Clouds over Time
---

# Human Action Recognition from Point Clouds over Time

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.05506" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.05506v3</a>
  <a href="https://arxiv.org/pdf/2510.05506.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05506v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.05506v3', 'Human Action Recognition from Point Clouds over Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: James Dickens

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-07 (æ›´æ–°: 2025-10-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºç‚¹äº‘åºåˆ—çš„äººä½“åŠ¨ä½œè¯†åˆ«æ¡†æ¶ï¼Œç»“åˆç‚¹äº‘å’Œç¨€ç–å·ç§¯ç½‘ç»œã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `äººä½“åŠ¨ä½œè¯†åˆ«` `ç‚¹äº‘å¤„ç†` `ç¨€ç–å·ç§¯ç½‘ç»œ` `3Dè§†é¢‘` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŠ¨ä½œè¯†åˆ«æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨éª¨éª¼åŠ¨ä½œè¯†åˆ«å’Œè§†é¢‘æ–¹æ³•ï¼Œç¼ºä¹å¯¹å¯†é›†3Dç‚¹äº‘æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ã€‚
2. æå‡ºä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ†å‰²ã€è·Ÿè¸ªå’Œèº«ä½“éƒ¨ä½åˆ†å‰²å¤„ç†3Dç‚¹äº‘ï¼Œå¹¶ç»“åˆç‚¹äº‘æŠ€æœ¯å’Œç¨€ç–å·ç§¯ç½‘ç»œã€‚
3. åœ¨NTU RGB-D 120æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¸éª¨éª¼åŠ¨ä½œè¯†åˆ«ç®—æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œé›†æˆä¼ æ„Ÿå™¨å’Œä¼°è®¡æ·±åº¦è¾“å…¥æ—¶ï¼Œå‡†ç¡®ç‡è¾¾åˆ°89.3%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dè§†é¢‘äººä½“åŠ¨ä½œè¯†åˆ«æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¦–å…ˆä»åœºæ™¯èƒŒæ™¯ä¸­åˆ†å‰²äººä½“ç‚¹äº‘ï¼Œç„¶åéšæ—¶é—´è·Ÿè¸ªä¸ªä½“ï¼Œå¹¶æ‰§è¡Œèº«ä½“éƒ¨ä½åˆ†å‰²ã€‚è¯¥æ–¹æ³•æ”¯æŒæ¥è‡ªæ·±åº¦ä¼ æ„Ÿå™¨å’Œå•ç›®æ·±åº¦ä¼°è®¡çš„ç‚¹äº‘ã€‚è¯¥HARæ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°é¢–çš„3DåŠ¨ä½œè¯†åˆ«éª¨å¹²ç½‘ç»œï¼Œå®ƒç»“åˆäº†åŸºäºç‚¹çš„æŠ€æœ¯å’Œåº”ç”¨äºä½“ç´ æ˜ å°„ç‚¹äº‘åºåˆ—çš„ç¨€ç–å·ç§¯ç½‘ç»œã€‚å®éªŒç»“åˆäº†è¾…åŠ©ç‚¹ç‰¹å¾ï¼ŒåŒ…æ‹¬è¡¨é¢æ³•çº¿ã€é¢œè‰²ã€çº¢å¤–å¼ºåº¦å’Œèº«ä½“éƒ¨ä½è§£ææ ‡ç­¾ï¼Œä»¥æé«˜è¯†åˆ«ç²¾åº¦ã€‚åœ¨NTU RGB-D 120æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ç°æœ‰çš„éª¨éª¼åŠ¨ä½œè¯†åˆ«ç®—æ³•å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨é›†æˆè®¾ç½®ä¸­ç»“åˆåŸºäºä¼ æ„Ÿå™¨å’Œä¼°è®¡çš„æ·±åº¦è¾“å…¥ï¼Œå½“è€ƒè™‘ä¸åŒçš„è®­ç»ƒå’Œæµ‹è¯•å¯¹è±¡æ—¶ï¼Œè¯¥æ–¹æ³•å®ç°äº†89.3%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºä»¥å¾€çš„ç‚¹äº‘åŠ¨ä½œè¯†åˆ«æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººä½“åŠ¨ä½œè¯†åˆ«æ–¹æ³•ä¸»è¦ä¾èµ–äºéª¨éª¼æ•°æ®æˆ–RGBè§†é¢‘ï¼Œè€Œå¿½ç•¥äº†æ—¥ç›Šæ™®åŠçš„æ·±åº¦ä¼ æ„Ÿå™¨å’Œæ¿€å…‰é›·è¾¾äº§ç”Ÿçš„å¯†é›†3Dç‚¹äº‘æ•°æ®ã€‚å¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›3Dç‚¹äº‘æ•°æ®è¿›è¡Œäººä½“åŠ¨ä½œè¯†åˆ«æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„ç‚¹äº‘åŠ¨ä½œè¯†åˆ«æ–¹æ³•å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨ç‚¹äº‘çš„æ—¶åºä¿¡æ¯å’Œå‡ ä½•ç‰¹å¾ï¼Œå¯¼è‡´è¯†åˆ«ç²¾åº¦ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç‚¹äº‘æ•°æ®è½¬æ¢ä¸ºä½“ç´ è¡¨ç¤ºï¼Œç„¶ååˆ©ç”¨ç¨€ç–å·ç§¯ç½‘ç»œæå–æ—¶ç©ºç‰¹å¾ã€‚åŒæ—¶ï¼Œç»“åˆç‚¹äº‘çš„å‡ ä½•ç‰¹å¾ï¼ˆå¦‚è¡¨é¢æ³•çº¿ï¼‰å’Œå¤–è§‚ç‰¹å¾ï¼ˆå¦‚é¢œè‰²ã€çº¢å¤–å¼ºåº¦ï¼‰æ¥å¢å¼ºè¯†åˆ«èƒ½åŠ›ã€‚é€šè¿‡é›†æˆæ¥è‡ªä¸åŒæ·±åº¦æºï¼ˆä¼ æ„Ÿå™¨å’Œå•ç›®æ·±åº¦ä¼°è®¡ï¼‰çš„ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æé«˜é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ç‚¹äº‘åˆ†å‰²ï¼šå°†äººä½“ç‚¹äº‘ä»èƒŒæ™¯ä¸­åˆ†å‰²å‡ºæ¥ã€‚2) ä¸ªä½“è·Ÿè¸ªï¼šéšæ—¶é—´è·Ÿè¸ªä¸ªä½“ã€‚3) èº«ä½“éƒ¨ä½åˆ†å‰²ï¼šå°†äººä½“ç‚¹äº‘åˆ†å‰²æˆä¸åŒçš„èº«ä½“éƒ¨ä½ã€‚4) ç‰¹å¾æå–ï¼šæå–ç‚¹äº‘çš„å‡ ä½•ç‰¹å¾ï¼ˆè¡¨é¢æ³•çº¿ï¼‰å’Œå¤–è§‚ç‰¹å¾ï¼ˆé¢œè‰²ã€çº¢å¤–å¼ºåº¦ï¼‰ã€‚5) åŠ¨ä½œè¯†åˆ«ï¼šå°†ç‚¹äº‘åºåˆ—ä½“ç´ åŒ–ï¼Œç„¶åä½¿ç”¨ç¨€ç–å·ç§¯ç½‘ç»œæå–æ—¶ç©ºç‰¹å¾ï¼Œå¹¶è¿›è¡ŒåŠ¨ä½œåˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§æ–°çš„3DåŠ¨ä½œè¯†åˆ«éª¨å¹²ç½‘ç»œï¼Œè¯¥ç½‘ç»œç»“åˆäº†åŸºäºç‚¹çš„æŠ€æœ¯å’Œç¨€ç–å·ç§¯ç½‘ç»œï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ç‚¹äº‘åºåˆ—ã€‚2) åˆ©ç”¨è¾…åŠ©ç‚¹ç‰¹å¾ï¼ˆè¡¨é¢æ³•çº¿ã€é¢œè‰²ã€çº¢å¤–å¼ºåº¦ã€èº«ä½“éƒ¨ä½è§£ææ ‡ç­¾ï¼‰æ¥å¢å¼ºè¯†åˆ«ç²¾åº¦ã€‚3) é›†æˆäº†æ¥è‡ªä¸åŒæ·±åº¦æºçš„ä¿¡æ¯ï¼Œæé«˜äº†é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•ä½¿ç”¨ç¨€ç–å·ç§¯ç½‘ç»œæ¥å¤„ç†ä½“ç´ åŒ–çš„ç‚¹äº‘åºåˆ—ï¼Œä»¥å‡å°‘è®¡ç®—é‡å’Œå†…å­˜æ¶ˆè€—ã€‚ä½“ç´ å¤§å°æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œéœ€è¦æ ¹æ®ç‚¹äº‘çš„å¯†åº¦å’ŒåŠ¨ä½œçš„å°ºåº¦è¿›è¡Œè°ƒæ•´ã€‚æŸå¤±å‡½æ•°é€šå¸¸é‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºè®­ç»ƒåŠ¨ä½œåˆ†ç±»å™¨ã€‚ç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹Ÿä¼šå½±å“æ€§èƒ½ï¼Œéœ€è¦æ ¹æ®æ•°æ®é›†çš„ç‰¹ç‚¹è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨NTU RGB-D 120æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚åœ¨è·¨è¢«è¯•è®¾ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†89.3%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†ä»¥å¾€çš„ç‚¹äº‘åŠ¨ä½œè¯†åˆ«æ–¹æ³•ã€‚é€šè¿‡ç»“åˆä¼ æ„Ÿå™¨æ•°æ®å’Œå•ç›®æ·±åº¦ä¼°è®¡æ•°æ®ï¼Œè¯¥æ–¹æ³•å±•ç°äº†è‰¯å¥½çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¾…åŠ©ç‚¹ç‰¹å¾çš„å¼•å…¥èƒ½å¤Ÿæœ‰æ•ˆæé«˜è¯†åˆ«ç²¾åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€äººæœºäº¤äº’ã€åº·å¤åŒ»ç–—ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•è¯†åˆ«å¼‚å¸¸è¡Œä¸ºï¼›åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥å®ç°åŸºäºæ‰‹åŠ¿æˆ–èº«ä½“åŠ¨ä½œçš„æ§åˆ¶ï¼›åœ¨åº·å¤åŒ»ç–—ä¸­ï¼Œå¯ä»¥è¯„ä¼°æ‚£è€…çš„åº·å¤è¿›åº¦ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent research into human action recognition (HAR) has focused predominantly on skeletal action recognition and video-based methods. With the increasing availability of consumer-grade depth sensors and Lidar instruments, there is a growing opportunity to leverage dense 3D data for action recognition, to develop a third way. This paper presents a novel approach for recognizing actions from 3D videos by introducing a pipeline that segments human point clouds from the background of a scene, tracks individuals over time, and performs body part segmentation. The method supports point clouds from both depth sensors and monocular depth estimation. At the core of the proposed HAR framework is a novel backbone for 3D action recognition, which combines point-based techniques with sparse convolutional networks applied to voxel-mapped point cloud sequences. Experiments incorporate auxiliary point features including surface normals, color, infrared intensity, and body part parsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D 120 dataset demonstrates that the method is competitive with existing skeletal action recognition algorithms. Moreover, combining both sensor-based and estimated depth inputs in an ensemble setup, this approach achieves 89.3% accuracy when different human subjects are considered for training and testing, outperforming previous point cloud action recognition methods.

