---
layout: default
title: Human Action Recognition from Point Clouds over Time
---

# Human Action Recognition from Point Clouds over Time

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.05506" target="_blank" class="toolbar-btn">arXiv: 2510.05506v3</a>
    <a href="https://arxiv.org/pdf/2510.05506.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05506v3" 
            onclick="toggleFavorite(this, '2510.05506v3', 'Human Action Recognition from Point Clouds over Time')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: James Dickens

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-07 (Êõ¥Êñ∞: 2025-10-09)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÁÇπ‰∫ëÂ∫èÂàóÁöÑ‰∫∫‰ΩìÂä®‰ΩúËØÜÂà´Ê°ÜÊû∂ÔºåÁªìÂêàÁÇπ‰∫ëÂíåÁ®ÄÁñèÂç∑ÁßØÁΩëÁªú„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫‰ΩìÂä®‰ΩúËØÜÂà´` `ÁÇπ‰∫ëÂ§ÑÁêÜ` `Á®ÄÁñèÂç∑ÁßØÁΩëÁªú` `3DËßÜÈ¢ë` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂä®‰ΩúËØÜÂà´ÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®È™®È™ºÂä®‰ΩúËØÜÂà´ÂíåËßÜÈ¢ëÊñπÊ≥ïÔºåÁº∫‰πèÂØπÂØÜÈõÜ3DÁÇπ‰∫ëÊï∞ÊçÆÁöÑÊúâÊïàÂà©Áî®„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÂàÜÂâ≤„ÄÅË∑üË∏™ÂíåË∫´‰ΩìÈÉ®‰ΩçÂàÜÂâ≤Â§ÑÁêÜ3DÁÇπ‰∫ëÔºåÂπ∂ÁªìÂêàÁÇπ‰∫ëÊäÄÊúØÂíåÁ®ÄÁñèÂç∑ÁßØÁΩëÁªú„ÄÇ
3. Âú®NTU RGB-D 120Êï∞ÊçÆÈõÜ‰∏äÔºåËØ•ÊñπÊ≥ï‰∏éÈ™®È™ºÂä®‰ΩúËØÜÂà´ÁÆóÊ≥ïÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåÈõÜÊàê‰º†ÊÑüÂô®Âíå‰º∞ËÆ°Ê∑±Â∫¶ËæìÂÖ•Êó∂ÔºåÂáÜÁ°ÆÁéáËææÂà∞89.3%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ3DËßÜÈ¢ë‰∫∫‰ΩìÂä®‰ΩúËØÜÂà´ÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ªéÂú∫ÊôØËÉåÊôØ‰∏≠ÂàÜÂâ≤‰∫∫‰ΩìÁÇπ‰∫ëÔºåÁÑ∂ÂêéÈöèÊó∂Èó¥Ë∑üË∏™‰∏™‰ΩìÔºåÂπ∂ÊâßË°åË∫´‰ΩìÈÉ®‰ΩçÂàÜÂâ≤„ÄÇËØ•ÊñπÊ≥ïÊîØÊåÅÊù•Ëá™Ê∑±Â∫¶‰º†ÊÑüÂô®ÂíåÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÁÇπ‰∫ë„ÄÇËØ•HARÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑ3DÂä®‰ΩúËØÜÂà´È™®Âπ≤ÁΩëÁªúÔºåÂÆÉÁªìÂêà‰∫ÜÂü∫‰∫éÁÇπÁöÑÊäÄÊúØÂíåÂ∫îÁî®‰∫é‰ΩìÁ¥†Êò†Â∞ÑÁÇπ‰∫ëÂ∫èÂàóÁöÑÁ®ÄÁñèÂç∑ÁßØÁΩëÁªú„ÄÇÂÆûÈ™åÁªìÂêà‰∫ÜËæÖÂä©ÁÇπÁâπÂæÅÔºåÂåÖÊã¨Ë°®Èù¢Ê≥ïÁ∫ø„ÄÅÈ¢úËâ≤„ÄÅÁ∫¢Â§ñÂº∫Â∫¶ÂíåË∫´‰ΩìÈÉ®‰ΩçËß£ÊûêÊ†áÁ≠æÔºå‰ª•ÊèêÈ´òËØÜÂà´Á≤æÂ∫¶„ÄÇÂú®NTU RGB-D 120Êï∞ÊçÆÈõÜ‰∏äÁöÑËØÑ‰º∞Ë°®ÊòéÔºåËØ•ÊñπÊ≥ï‰∏éÁé∞ÊúâÁöÑÈ™®È™ºÂä®‰ΩúËØÜÂà´ÁÆóÊ≥ïÂÖ∑ÊúâÁ´û‰∫âÂäõ„ÄÇÊ≠§Â§ñÔºåÂú®ÈõÜÊàêËÆæÁΩÆ‰∏≠ÁªìÂêàÂü∫‰∫é‰º†ÊÑüÂô®Âíå‰º∞ËÆ°ÁöÑÊ∑±Â∫¶ËæìÂÖ•ÔºåÂΩìËÄÉËôë‰∏çÂêåÁöÑËÆ≠ÁªÉÂíåÊµãËØïÂØπË±°Êó∂ÔºåËØ•ÊñπÊ≥ïÂÆûÁé∞‰∫Ü89.3%ÁöÑÂáÜÁ°ÆÁéáÔºå‰ºò‰∫é‰ª•ÂæÄÁöÑÁÇπ‰∫ëÂä®‰ΩúËØÜÂà´ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑ‰∫∫‰ΩìÂä®‰ΩúËØÜÂà´ÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÈ™®È™ºÊï∞ÊçÆÊàñRGBËßÜÈ¢ëÔºåËÄåÂøΩÁï•‰∫ÜÊó•ÁõäÊôÆÂèäÁöÑÊ∑±Â∫¶‰º†ÊÑüÂô®ÂíåÊøÄÂÖâÈõ∑Ëææ‰∫ßÁîüÁöÑÂØÜÈõÜ3DÁÇπ‰∫ëÊï∞ÊçÆ„ÄÇÂ¶Ç‰ΩïÊúâÊïàÂú∞Âà©Áî®Ëøô‰∫õ3DÁÇπ‰∫ëÊï∞ÊçÆËøõË°å‰∫∫‰ΩìÂä®‰ΩúËØÜÂà´ÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇÁé∞ÊúâÁöÑÁÇπ‰∫ëÂä®‰ΩúËØÜÂà´ÊñπÊ≥ïÂèØËÉΩÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®ÁÇπ‰∫ëÁöÑÊó∂Â∫è‰ø°ÊÅØÂíåÂá†‰ΩïÁâπÂæÅÔºåÂØºËá¥ËØÜÂà´Á≤æÂ∫¶‰∏çÈ´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁÇπ‰∫ëÊï∞ÊçÆËΩ¨Êç¢‰∏∫‰ΩìÁ¥†Ë°®Á§∫ÔºåÁÑ∂ÂêéÂà©Áî®Á®ÄÁñèÂç∑ÁßØÁΩëÁªúÊèêÂèñÊó∂Á©∫ÁâπÂæÅ„ÄÇÂêåÊó∂ÔºåÁªìÂêàÁÇπ‰∫ëÁöÑÂá†‰ΩïÁâπÂæÅÔºàÂ¶ÇË°®Èù¢Ê≥ïÁ∫øÔºâÂíåÂ§ñËßÇÁâπÂæÅÔºàÂ¶ÇÈ¢úËâ≤„ÄÅÁ∫¢Â§ñÂº∫Â∫¶ÔºâÊù•Â¢ûÂº∫ËØÜÂà´ËÉΩÂäõ„ÄÇÈÄöËøáÈõÜÊàêÊù•Ëá™‰∏çÂêåÊ∑±Â∫¶Ê∫êÔºà‰º†ÊÑüÂô®ÂíåÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÔºâÁöÑ‰ø°ÊÅØÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´òÈ≤ÅÊ£íÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ÁÇπ‰∫ëÂàÜÂâ≤ÔºöÂ∞Ü‰∫∫‰ΩìÁÇπ‰∫ë‰ªéËÉåÊôØ‰∏≠ÂàÜÂâ≤Âá∫Êù•„ÄÇ2) ‰∏™‰ΩìË∑üË∏™ÔºöÈöèÊó∂Èó¥Ë∑üË∏™‰∏™‰Ωì„ÄÇ3) Ë∫´‰ΩìÈÉ®‰ΩçÂàÜÂâ≤ÔºöÂ∞Ü‰∫∫‰ΩìÁÇπ‰∫ëÂàÜÂâ≤Êàê‰∏çÂêåÁöÑË∫´‰ΩìÈÉ®‰Ωç„ÄÇ4) ÁâπÂæÅÊèêÂèñÔºöÊèêÂèñÁÇπ‰∫ëÁöÑÂá†‰ΩïÁâπÂæÅÔºàË°®Èù¢Ê≥ïÁ∫øÔºâÂíåÂ§ñËßÇÁâπÂæÅÔºàÈ¢úËâ≤„ÄÅÁ∫¢Â§ñÂº∫Â∫¶Ôºâ„ÄÇ5) Âä®‰ΩúËØÜÂà´ÔºöÂ∞ÜÁÇπ‰∫ëÂ∫èÂàó‰ΩìÁ¥†ÂåñÔºåÁÑ∂Âêé‰ΩøÁî®Á®ÄÁñèÂç∑ÁßØÁΩëÁªúÊèêÂèñÊó∂Á©∫ÁâπÂæÅÔºåÂπ∂ËøõË°åÂä®‰ΩúÂàÜÁ±ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂàõÊñ∞ÁÇπÂú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ3DÂä®‰ΩúËØÜÂà´È™®Âπ≤ÁΩëÁªúÔºåËØ•ÁΩëÁªúÁªìÂêà‰∫ÜÂü∫‰∫éÁÇπÁöÑÊäÄÊúØÂíåÁ®ÄÁñèÂç∑ÁßØÁΩëÁªúÔºåËÉΩÂ§üÊúâÊïàÂú∞Â§ÑÁêÜÁÇπ‰∫ëÂ∫èÂàó„ÄÇ2) Âà©Áî®ËæÖÂä©ÁÇπÁâπÂæÅÔºàË°®Èù¢Ê≥ïÁ∫ø„ÄÅÈ¢úËâ≤„ÄÅÁ∫¢Â§ñÂº∫Â∫¶„ÄÅË∫´‰ΩìÈÉ®‰ΩçËß£ÊûêÊ†áÁ≠æÔºâÊù•Â¢ûÂº∫ËØÜÂà´Á≤æÂ∫¶„ÄÇ3) ÈõÜÊàê‰∫ÜÊù•Ëá™‰∏çÂêåÊ∑±Â∫¶Ê∫êÁöÑ‰ø°ÊÅØÔºåÊèêÈ´ò‰∫ÜÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËØ•ÊñπÊ≥ï‰ΩøÁî®Á®ÄÁñèÂç∑ÁßØÁΩëÁªúÊù•Â§ÑÁêÜ‰ΩìÁ¥†ÂåñÁöÑÁÇπ‰∫ëÂ∫èÂàóÔºå‰ª•ÂáèÂ∞ëËÆ°ÁÆóÈáèÂíåÂÜÖÂ≠òÊ∂àËÄó„ÄÇ‰ΩìÁ¥†Â§ßÂ∞èÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÁÇπ‰∫ëÁöÑÂØÜÂ∫¶ÂíåÂä®‰ΩúÁöÑÂ∞∫Â∫¶ËøõË°åË∞ÉÊï¥„ÄÇÊçüÂ§±ÂáΩÊï∞ÈÄöÂ∏∏ÈááÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éËÆ≠ÁªÉÂä®‰ΩúÂàÜÁ±ªÂô®„ÄÇÁΩëÁªúÁªìÊûÑÁöÑÈÄâÊã©‰πü‰ºöÂΩ±ÂìçÊÄßËÉΩÔºåÈúÄË¶ÅÊ†πÊçÆÊï∞ÊçÆÈõÜÁöÑÁâπÁÇπËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú®NTU RGB-D 120Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÊàêÊûú„ÄÇÂú®Ë∑®Ë¢´ËØïËÆæÁΩÆ‰∏ãÔºåËØ•ÊñπÊ≥ïËææÂà∞‰∫Ü89.3%ÁöÑÂáÜÁ°ÆÁéáÔºåË∂ÖËøá‰∫Ü‰ª•ÂæÄÁöÑÁÇπ‰∫ëÂä®‰ΩúËØÜÂà´ÊñπÊ≥ï„ÄÇÈÄöËøáÁªìÂêà‰º†ÊÑüÂô®Êï∞ÊçÆÂíåÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Êï∞ÊçÆÔºåËØ•ÊñπÊ≥ïÂ±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËæÖÂä©ÁÇπÁâπÂæÅÁöÑÂºïÂÖ•ËÉΩÂ§üÊúâÊïàÊèêÈ´òËØÜÂà´Á≤æÂ∫¶„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅÂ∫∑Â§çÂåªÁñóÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êô∫ËÉΩÁõëÊéß‰∏≠ÔºåÂèØ‰ª•Âà©Áî®ËØ•ÊñπÊ≥ïËØÜÂà´ÂºÇÂ∏∏Ë°å‰∏∫ÔºõÂú®‰∫∫Êú∫‰∫§‰∫í‰∏≠ÔºåÂèØ‰ª•ÂÆûÁé∞Âü∫‰∫éÊâãÂäøÊàñË∫´‰ΩìÂä®‰ΩúÁöÑÊéßÂà∂ÔºõÂú®Â∫∑Â§çÂåªÁñó‰∏≠ÔºåÂèØ‰ª•ËØÑ‰º∞ÊÇ£ËÄÖÁöÑÂ∫∑Â§çËøõÂ∫¶„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂú®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent research into human action recognition (HAR) has focused predominantly on skeletal action recognition and video-based methods. With the increasing availability of consumer-grade depth sensors and Lidar instruments, there is a growing opportunity to leverage dense 3D data for action recognition, to develop a third way. This paper presents a novel approach for recognizing actions from 3D videos by introducing a pipeline that segments human point clouds from the background of a scene, tracks individuals over time, and performs body part segmentation. The method supports point clouds from both depth sensors and monocular depth estimation. At the core of the proposed HAR framework is a novel backbone for 3D action recognition, which combines point-based techniques with sparse convolutional networks applied to voxel-mapped point cloud sequences. Experiments incorporate auxiliary point features including surface normals, color, infrared intensity, and body part parsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D 120 dataset demonstrates that the method is competitive with existing skeletal action recognition algorithms. Moreover, combining both sensor-based and estimated depth inputs in an ensemble setup, this approach achieves 89.3% accuracy when different human subjects are considered for training and testing, outperforming previous point cloud action recognition methods.

