---
layout: default
title: "When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance"
---

# When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10466" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10466v1</a>
  <a href="https://arxiv.org/pdf/2510.10466.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10466v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10466v1', 'When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinjin Cao, Zhiyang Chen, Zijun Wang, Liyuan Ma, Weijian Luo, Guojun Qi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè·¨æ¨¡æ€å¼•å¯¼ï¼ˆCMGï¼‰æ–¹æ³•ï¼Œç¼“è§£è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è¯­è¨€åè§å¯¼è‡´çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `è¯­è¨€åè§` `å¹»è§‰é—®é¢˜` `è·¨æ¨¡æ€å¼•å¯¼` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMæ˜“äº§ç”Ÿä¸å›¾åƒæ— å…³ä½†è¯­è¨€æµç•…çš„å¹»è§‰ï¼Œæºäºè¯­è¨€åè§ã€‚
2. æå‡ºè·¨æ¨¡æ€å¼•å¯¼ï¼ˆCMGï¼‰æ–¹æ³•ï¼Œé€šè¿‡é€€åŒ–è§†è§‰-è¯­è¨€æ³¨æ„åŠ›æ¥é™ä½è¯­è¨€åè§ã€‚
3. å®éªŒè¡¨æ˜CMGèƒ½æœ‰æ•ˆæå‡VLMåœ¨å¹»è§‰åŸºå‡†ä¸Šçš„æ€§èƒ½ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨è§†è§‰å’Œè¯­è¨€ä¸Šä¸‹æ–‡çš„å¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLMå¸¸å¸¸é¢ä¸´ä¸¥é‡çš„å¹»è§‰æŒ‘æˆ˜ï¼Œå³VLMå€¾å‘äºç”Ÿæˆåœ¨è¯­è¨€ä¸Šæµç•…ä½†ä¸å…ˆå‰ä¸Šä¸‹æ–‡ä¸­çš„å›¾åƒæ— å…³çš„å“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ†æäº†è¯­è¨€åè§å¦‚ä½•å¯¼è‡´å¹»è§‰ï¼Œå¹¶å¼•å…¥äº†è·¨æ¨¡æ€å¼•å¯¼(CMG)ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨åŸå§‹æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒä¸è§†è§‰-è¯­è¨€æ³¨æ„åŠ›é€€åŒ–åçš„æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚æ¥è§£å†³å¹»è§‰é—®é¢˜ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°å±è”½é€‰å®šçš„Transformerå±‚ä¸­æœ€å…·å½±å“åŠ›çš„å›¾åƒtokençš„æ³¨æ„åŠ›æƒé‡ï¼Œä»¥ç ´åè§†è§‰-è¯­è¨€æ„ŸçŸ¥ï¼Œä½œä¸ºä¸€ç§å…·ä½“çš„é€€åŒ–æ–¹å¼ã€‚è¿™ç§é€€åŒ–è¯±å¯¼çš„è§£ç å¼ºè°ƒäº†å¯¹è§†è§‰ä¸Šä¸‹æ–‡çš„æ„ŸçŸ¥ï¼Œå› æ­¤æ˜¾è‘—é™ä½äº†è¯­è¨€åè§ï¼Œè€Œä¸ä¼šæŸå®³VLMçš„èƒ½åŠ›ã€‚åœ¨å®éªŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ã€‚æ‰€æœ‰ç»“æœéƒ½è¯æ˜äº†CMGçš„ä¼˜è¶Šæ€§ï¼Œæ— éœ€é¢å¤–çš„æ¡ä»¶æˆ–è®­ç»ƒæˆæœ¬ã€‚æˆ‘ä»¬è¿˜å®šé‡åœ°è¡¨æ˜ï¼ŒCMGå¯ä»¥æé«˜ä¸åŒVLMåœ¨ç‰¹å®šäºå¹»è§‰çš„åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆåœ°æ³›åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå³ç”Ÿæˆè¯­è¨€æµç•…ä½†ä¸å›¾åƒå†…å®¹æ— å…³çš„å›å¤ã€‚è¿™ç§å¹»è§‰é—®é¢˜ä¸»è¦æºäºæ¨¡å‹å¯¹è¯­è¨€çš„è¿‡åº¦ä¾èµ–ï¼Œè€Œå¿½ç•¥äº†è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´è¯­è¨€åè§ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆç¼“è§£è¿™ç§è¯­è¨€åè§ï¼Œä»è€Œé™åˆ¶äº†VLMçš„å¯é æ€§å’Œåº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è·¨æ¨¡æ€å¼•å¯¼ï¼ˆCMGï¼‰æ¥é™ä½VLMä¸­çš„è¯­è¨€åè§ã€‚CMGçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œé€šè¿‡å¯¹æ¯”åŸå§‹VLMçš„è¾“å‡ºåˆ†å¸ƒå’Œç»è¿‡è§†è§‰-è¯­è¨€æ³¨æ„åŠ›é€€åŒ–åçš„VLMçš„è¾“å‡ºåˆ†å¸ƒï¼Œæ¥å¼•å¯¼æ¨¡å‹æ›´å¤šåœ°å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•åŸºäºä¸€ä¸ªå‡è®¾ï¼šå¦‚æœæ¨¡å‹è¿‡åº¦ä¾èµ–è¯­è¨€ï¼Œé‚£ä¹ˆåœ¨è§†è§‰ä¿¡æ¯è¢«éƒ¨åˆ†ç§»é™¤åï¼Œå…¶è¾“å‡ºåˆ†å¸ƒä¼šå‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCMGæ˜¯ä¸€ç§è®­ç»ƒè‡ªç”±çš„è§£ç æ–¹æ³•ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ã€‚å…¶ä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š1) ä½¿ç”¨åŸå§‹VLMç”Ÿæˆä¸€ä¸ªè¾“å‡ºåˆ†å¸ƒã€‚2) é€šè¿‡è‡ªé€‚åº”åœ°å±è”½é€‰å®šçš„Transformerå±‚ä¸­æœ€å…·å½±å“åŠ›çš„å›¾åƒtokençš„æ³¨æ„åŠ›æƒé‡ï¼Œæ¥é€€åŒ–è§†è§‰-è¯­è¨€æ³¨æ„åŠ›ã€‚3) ä½¿ç”¨é€€åŒ–åçš„VLMç”Ÿæˆå¦ä¸€ä¸ªè¾“å‡ºåˆ†å¸ƒã€‚4) è®¡ç®—ä¸¤ä¸ªè¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶åˆ©ç”¨è¯¥å·®å¼‚æ¥è°ƒæ•´åŸå§‹VLMçš„è¾“å‡ºï¼Œä»è€Œå¼•å¯¼æ¨¡å‹æ›´å¤šåœ°å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šCMGçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ— éœ€è®­ç»ƒçš„ç‰¹æ€§ï¼Œä»¥åŠé€šè¿‡å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œé€€åŒ–æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ¥å¼•å¯¼æ¨¡å‹å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚ä¸éœ€è¦é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒçš„æ–¹æ³•ä¸åŒï¼ŒCMGå¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰çš„VLMï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„æˆ–å‚æ•°ã€‚æ­¤å¤–ï¼ŒCMGé€šè¿‡è‡ªé€‚åº”åœ°é€‰æ‹©éœ€è¦å±è”½çš„å›¾åƒtokenï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°ç ´åè§†è§‰-è¯­è¨€æ³¨æ„åŠ›ï¼Œä»è€Œæ›´å¥½åœ°é™ä½è¯­è¨€åè§ã€‚

**å…³é”®è®¾è®¡**ï¼šCMGçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è‡ªé€‚åº”åœ°é€‰æ‹©éœ€è¦å±è”½çš„å›¾åƒtokenã€‚å…·ä½“æ¥è¯´ï¼Œé€‰æ‹©åœ¨è§†è§‰-è¯­è¨€æ³¨æ„åŠ›æƒé‡ä¸­å…·æœ‰æœ€é«˜å€¼çš„tokenï¼Œå› ä¸ºè¿™äº›tokenè¢«è®¤ä¸ºæ˜¯å¯¹æ¨¡å‹è¾“å‡ºå½±å“æœ€å¤§çš„è§†è§‰ä¿¡æ¯ã€‚2) é€‰æ‹©åˆé€‚çš„Transformerå±‚è¿›è¡Œæ³¨æ„åŠ›å±è”½ã€‚è®ºæ–‡ä¸­æåˆ°é€‰æ‹©ç‰¹å®šçš„Transformerå±‚å¯ä»¥æ›´æœ‰æ•ˆåœ°ç ´åè§†è§‰-è¯­è¨€æ³¨æ„åŠ›ã€‚3) ä½¿ç”¨åˆé€‚çš„è·ç¦»åº¦é‡æ¥è®¡ç®—åŸå§‹æ¨¡å‹å’Œé€€åŒ–æ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚è®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†KLæ•£åº¦æˆ–å…¶ä»–ç±»ä¼¼çš„è·ç¦»åº¦é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCMGæ–¹æ³•åœ¨å¤šä¸ªå¹»è§‰ç›¸å…³çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†VLMçš„æ€§èƒ½ï¼Œä¸”æ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼ŒCMGèƒ½å¤Ÿæœ‰æ•ˆåœ°é™ä½VLMç”Ÿæˆçš„å›å¤ä¸­ä¸å›¾åƒå†…å®¹æ— å…³çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜å›å¤çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚æ­¤å¤–ï¼ŒCMGè¿˜å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„VLMæ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å¯é è§†è§‰è¯­è¨€ç†è§£çš„åœºæ™¯ï¼Œä¾‹å¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿç­‰ã€‚é€šè¿‡é™ä½è¯­è¨€åè§å’Œå‡å°‘å¹»è§‰ï¼Œå¯ä»¥æé«˜VLMåœ¨è¿™äº›åº”ç”¨ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œåº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡å’Œæ¨¡å‹ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have shown solid ability for multimodal understanding of both visual and language contexts. However, existing VLMs often face severe challenges of hallucinations, meaning that VLMs tend to generate responses that are only fluent in the language but irrelevant to images in previous contexts. To address this issue, we analyze how language bias contributes to hallucinations and then introduce Cross-Modal Guidance(CMG), a training-free decoding method that addresses the hallucinations by leveraging the difference between the output distributions of the original model and the one with degraded visual-language attention. In practice, we adaptively mask the attention weight of the most influential image tokens in selected transformer layers to corrupt the visual-language perception as a concrete type of degradation. Such a degradation-induced decoding emphasizes the perception of visual contexts and therefore significantly reduces language bias without harming the ability of VLMs. In experiment sections, we conduct comprehensive studies. All results demonstrate the superior advantages of CMG with neither additional conditions nor training costs. We also quantitatively show CMG can improve different VLM's performance on hallucination-specific benchmarks and generalize effectively.

