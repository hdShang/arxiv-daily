---
layout: default
title: When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance
---

# When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10466" target="_blank" class="toolbar-btn">arXiv: 2510.10466v1</a>
    <a href="https://arxiv.org/pdf/2510.10466.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10466v1" 
            onclick="toggleFavorite(this, '2510.10466v1', 'When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jinjin Cao, Zhiyang Chen, Zijun Wang, Liyuan Ma, Weijian Luo, Guojun Qi

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-12

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ë∑®Ê®°ÊÄÅÂºïÂØºÔºàCMGÔºâÊñπÊ≥ïÔºåÁºìËß£ËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑËØ≠Ë®ÄÂÅèËßÅÂØºËá¥ÁöÑÂπªËßâÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ËØ≠Ë®ÄÂÅèËßÅ` `ÂπªËßâÈóÆÈ¢ò` `Ë∑®Ê®°ÊÄÅÂºïÂØº` `Ê≥®ÊÑèÂäõÊú∫Âà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLMÊòì‰∫ßÁîü‰∏éÂõæÂÉèÊó†ÂÖ≥‰ΩÜËØ≠Ë®ÄÊµÅÁïÖÁöÑÂπªËßâÔºåÊ∫ê‰∫éËØ≠Ë®ÄÂÅèËßÅ„ÄÇ
2. ÊèêÂá∫Ë∑®Ê®°ÊÄÅÂºïÂØºÔºàCMGÔºâÊñπÊ≥ïÔºåÈÄöËøáÈÄÄÂåñËßÜËßâ-ËØ≠Ë®ÄÊ≥®ÊÑèÂäõÊù•Èôç‰ΩéËØ≠Ë®ÄÂÅèËßÅ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéCMGËÉΩÊúâÊïàÊèêÂçáVLMÂú®ÂπªËßâÂü∫ÂáÜ‰∏äÁöÑÊÄßËÉΩÔºå‰∏îÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)Âú®ËßÜËßâÂíåËØ≠Ë®Ä‰∏ä‰∏ãÊñáÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ÊñπÈù¢Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑVLMÂ∏∏Â∏∏Èù¢‰∏¥‰∏•ÈáçÁöÑÂπªËßâÊåëÊàòÔºåÂç≥VLMÂÄæÂêë‰∫éÁîüÊàêÂú®ËØ≠Ë®Ä‰∏äÊµÅÁïÖ‰ΩÜ‰∏éÂÖàÂâç‰∏ä‰∏ãÊñá‰∏≠ÁöÑÂõæÂÉèÊó†ÂÖ≥ÁöÑÂìçÂ∫î„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜËØ≠Ë®ÄÂÅèËßÅÂ¶Ç‰ΩïÂØºËá¥ÂπªËßâÔºåÂπ∂ÂºïÂÖ•‰∫ÜË∑®Ê®°ÊÄÅÂºïÂØº(CMG)ÔºåËøôÊòØ‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑËß£Á†ÅÊñπÊ≥ïÔºåÈÄöËøáÂà©Áî®ÂéüÂßãÊ®°ÂûãÁöÑËæìÂá∫ÂàÜÂ∏É‰∏éËßÜËßâ-ËØ≠Ë®ÄÊ≥®ÊÑèÂäõÈÄÄÂåñÂêéÁöÑÊ®°ÂûãÁöÑËæìÂá∫ÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊù•Ëß£ÂÜ≥ÂπªËßâÈóÆÈ¢ò„ÄÇÂú®ÂÆûË∑µ‰∏≠ÔºåÊàë‰ª¨Ëá™ÈÄÇÂ∫îÂú∞Â±èËîΩÈÄâÂÆöÁöÑTransformerÂ±Ç‰∏≠ÊúÄÂÖ∑ÂΩ±ÂìçÂäõÁöÑÂõæÂÉètokenÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÔºå‰ª•Á†¥ÂùèËßÜËßâ-ËØ≠Ë®ÄÊÑüÁü•Ôºå‰Ωú‰∏∫‰∏ÄÁßçÂÖ∑‰ΩìÁöÑÈÄÄÂåñÊñπÂºè„ÄÇËøôÁßçÈÄÄÂåñËØ±ÂØºÁöÑËß£Á†ÅÂº∫Ë∞É‰∫ÜÂØπËßÜËßâ‰∏ä‰∏ãÊñáÁöÑÊÑüÁü•ÔºåÂõ†Ê≠§ÊòæËëóÈôç‰Ωé‰∫ÜËØ≠Ë®ÄÂÅèËßÅÔºåËÄå‰∏ç‰ºöÊçüÂÆ≥VLMÁöÑËÉΩÂäõ„ÄÇÂú®ÂÆûÈ™åÈÉ®ÂàÜÔºåÊàë‰ª¨ËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÁ†îÁ©∂„ÄÇÊâÄÊúâÁªìÊûúÈÉΩËØÅÊòé‰∫ÜCMGÁöÑ‰ºòË∂äÊÄßÔºåÊó†ÈúÄÈ¢ùÂ§ñÁöÑÊù°‰ª∂ÊàñËÆ≠ÁªÉÊàêÊú¨„ÄÇÊàë‰ª¨ËøòÂÆöÈáèÂú∞Ë°®ÊòéÔºåCMGÂèØ‰ª•ÊèêÈ´ò‰∏çÂêåVLMÂú®ÁâπÂÆö‰∫éÂπªËßâÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÁöÑÊÄßËÉΩÔºåÂπ∂ÊúâÊïàÂú∞Ê≥õÂåñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂú®Â§öÊ®°ÊÄÅÁêÜËß£ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆπÊòì‰∫ßÁîüÂπªËßâÔºåÂç≥ÁîüÊàêËØ≠Ë®ÄÊµÅÁïÖ‰ΩÜ‰∏éÂõæÂÉèÂÜÖÂÆπÊó†ÂÖ≥ÁöÑÂõûÂ§ç„ÄÇËøôÁßçÂπªËßâÈóÆÈ¢ò‰∏ªË¶ÅÊ∫ê‰∫éÊ®°ÂûãÂØπËØ≠Ë®ÄÁöÑËøáÂ∫¶‰æùËµñÔºåËÄåÂøΩÁï•‰∫ÜËßÜËßâ‰ø°ÊÅØÔºåÂØºËá¥ËØ≠Ë®ÄÂÅèËßÅ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÁºìËß£ËøôÁßçËØ≠Ë®ÄÂÅèËßÅÔºå‰ªéËÄåÈôêÂà∂‰∫ÜVLMÁöÑÂèØÈù†ÊÄßÂíåÂ∫îÁî®ËåÉÂõ¥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•Ë∑®Ê®°ÊÄÅÂºïÂØºÔºàCMGÔºâÊù•Èôç‰ΩéVLM‰∏≠ÁöÑËØ≠Ë®ÄÂÅèËßÅ„ÄÇCMGÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÔºåÈÄöËøáÂØπÊØîÂéüÂßãVLMÁöÑËæìÂá∫ÂàÜÂ∏ÉÂíåÁªèËøáËßÜËßâ-ËØ≠Ë®ÄÊ≥®ÊÑèÂäõÈÄÄÂåñÂêéÁöÑVLMÁöÑËæìÂá∫ÂàÜÂ∏ÉÔºåÊù•ÂºïÂØºÊ®°ÂûãÊõ¥Â§öÂú∞ÂÖ≥Ê≥®ËßÜËßâ‰ø°ÊÅØ„ÄÇËøôÁßçÊñπÊ≥ïÂü∫‰∫é‰∏Ä‰∏™ÂÅáËÆæÔºöÂ¶ÇÊûúÊ®°ÂûãËøáÂ∫¶‰æùËµñËØ≠Ë®ÄÔºåÈÇ£‰πàÂú®ËßÜËßâ‰ø°ÊÅØË¢´ÈÉ®ÂàÜÁßªÈô§ÂêéÔºåÂÖ∂ËæìÂá∫ÂàÜÂ∏É‰ºöÂèëÁîüÊòæËëóÂèòÂåñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCMGÊòØ‰∏ÄÁßçËÆ≠ÁªÉËá™Áî±ÁöÑËß£Á†ÅÊñπÊ≥ïÔºå‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÇÂÖ∂‰∏ªË¶ÅÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) ‰ΩøÁî®ÂéüÂßãVLMÁîüÊàê‰∏Ä‰∏™ËæìÂá∫ÂàÜÂ∏É„ÄÇ2) ÈÄöËøáËá™ÈÄÇÂ∫îÂú∞Â±èËîΩÈÄâÂÆöÁöÑTransformerÂ±Ç‰∏≠ÊúÄÂÖ∑ÂΩ±ÂìçÂäõÁöÑÂõæÂÉètokenÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÔºåÊù•ÈÄÄÂåñËßÜËßâ-ËØ≠Ë®ÄÊ≥®ÊÑèÂäõ„ÄÇ3) ‰ΩøÁî®ÈÄÄÂåñÂêéÁöÑVLMÁîüÊàêÂè¶‰∏Ä‰∏™ËæìÂá∫ÂàÜÂ∏É„ÄÇ4) ËÆ°ÁÆó‰∏§‰∏™ËæìÂá∫ÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂπ∂Âà©Áî®ËØ•Â∑ÆÂºÇÊù•Ë∞ÉÊï¥ÂéüÂßãVLMÁöÑËæìÂá∫Ôºå‰ªéËÄåÂºïÂØºÊ®°ÂûãÊõ¥Â§öÂú∞ÂÖ≥Ê≥®ËßÜËßâ‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCMGÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Êó†ÈúÄËÆ≠ÁªÉÁöÑÁâπÊÄßÔºå‰ª•ÂèäÈÄöËøáÂØπÊØîÂéüÂßãÊ®°ÂûãÂíåÈÄÄÂåñÊ®°ÂûãÁöÑËæìÂá∫ÂàÜÂ∏ÉÊù•ÂºïÂØºÊ®°ÂûãÂÖ≥Ê≥®ËßÜËßâ‰ø°ÊÅØ„ÄÇ‰∏éÈúÄË¶ÅÈ¢ùÂ§ñËÆ≠ÁªÉÊàñÂæÆË∞ÉÁöÑÊñπÊ≥ï‰∏çÂêåÔºåCMGÂèØ‰ª•Áõ¥Êé•Â∫îÁî®‰∫éÁé∞ÊúâÁöÑVLMÔºåËÄåÊó†ÈúÄ‰øÆÊîπÊ®°ÂûãÁªìÊûÑÊàñÂèÇÊï∞„ÄÇÊ≠§Â§ñÔºåCMGÈÄöËøáËá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©ÈúÄË¶ÅÂ±èËîΩÁöÑÂõæÂÉètokenÔºåÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞Á†¥ÂùèËßÜËßâ-ËØ≠Ë®ÄÊ≥®ÊÑèÂäõÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Èôç‰ΩéËØ≠Ë®ÄÂÅèËßÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCMGÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Ëá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©ÈúÄË¶ÅÂ±èËîΩÁöÑÂõæÂÉètoken„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄâÊã©Âú®ËßÜËßâ-ËØ≠Ë®ÄÊ≥®ÊÑèÂäõÊùÉÈáç‰∏≠ÂÖ∑ÊúâÊúÄÈ´òÂÄºÁöÑtokenÔºåÂõ†‰∏∫Ëøô‰∫õtokenË¢´ËÆ§‰∏∫ÊòØÂØπÊ®°ÂûãËæìÂá∫ÂΩ±ÂìçÊúÄÂ§ßÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÇ2) ÈÄâÊã©ÂêàÈÄÇÁöÑTransformerÂ±ÇËøõË°åÊ≥®ÊÑèÂäõÂ±èËîΩ„ÄÇËÆ∫Êñá‰∏≠ÊèêÂà∞ÈÄâÊã©ÁâπÂÆöÁöÑTransformerÂ±ÇÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞Á†¥ÂùèËßÜËßâ-ËØ≠Ë®ÄÊ≥®ÊÑèÂäõ„ÄÇ3) ‰ΩøÁî®ÂêàÈÄÇÁöÑË∑ùÁ¶ªÂ∫¶ÈáèÊù•ËÆ°ÁÆóÂéüÂßãÊ®°ÂûãÂíåÈÄÄÂåñÊ®°ÂûãËæìÂá∫ÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇËÆ∫Êñá‰∏≠ÂèØËÉΩ‰ΩøÁî®‰∫ÜKLÊï£Â∫¶ÊàñÂÖ∂‰ªñÁ±ª‰ººÁöÑË∑ùÁ¶ªÂ∫¶Èáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCMGÊñπÊ≥ïÂú®Â§ö‰∏™ÂπªËßâÁõ∏ÂÖ≥ÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜVLMÁöÑÊÄßËÉΩÔºå‰∏îÊó†ÈúÄ‰ªª‰ΩïÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÊàêÊú¨„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåCMGËÉΩÂ§üÊúâÊïàÂú∞Èôç‰ΩéVLMÁîüÊàêÁöÑÂõûÂ§ç‰∏≠‰∏éÂõæÂÉèÂÜÖÂÆπÊó†ÂÖ≥ÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÂõûÂ§çÁöÑÂáÜÁ°ÆÊÄßÂíåÁõ∏ÂÖ≥ÊÄß„ÄÇÊ≠§Â§ñÔºåCMGËøòÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•Â∫îÁî®‰∫é‰∏çÂêåÁöÑVLMÊ®°Âûã„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂèØÈù†ËßÜËßâËØ≠Ë®ÄÁêÜËß£ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂõæÂÉèÊèèËø∞ÁîüÊàê„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÂ§öÊ®°ÊÄÅÂØπËØùÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéËØ≠Ë®ÄÂÅèËßÅÂíåÂáèÂ∞ëÂπªËßâÔºåÂèØ‰ª•ÊèêÈ´òVLMÂú®Ëøô‰∫õÂ∫îÁî®‰∏≠ÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄßÔºå‰ªéËÄåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÂíåÂ∫îÁî®‰ª∑ÂÄº„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅ‰ªªÂä°ÂíåÊ®°Âûã‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Models (VLMs) have shown solid ability for multimodal understanding of both visual and language contexts. However, existing VLMs often face severe challenges of hallucinations, meaning that VLMs tend to generate responses that are only fluent in the language but irrelevant to images in previous contexts. To address this issue, we analyze how language bias contributes to hallucinations and then introduce Cross-Modal Guidance(CMG), a training-free decoding method that addresses the hallucinations by leveraging the difference between the output distributions of the original model and the one with degraded visual-language attention. In practice, we adaptively mask the attention weight of the most influential image tokens in selected transformer layers to corrupt the visual-language perception as a concrete type of degradation. Such a degradation-induced decoding emphasizes the perception of visual contexts and therefore significantly reduces language bias without harming the ability of VLMs. In experiment sections, we conduct comprehensive studies. All results demonstrate the superior advantages of CMG with neither additional conditions nor training costs. We also quantitatively show CMG can improve different VLM's performance on hallucination-specific benchmarks and generalize effectively.

