---
layout: default
title: Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey
---

# Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10671" target="_blank" class="toolbar-btn">arXiv: 2510.10671v1</a>
    <a href="https://arxiv.org/pdf/2510.10671.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10671v1" 
            onclick="toggleFavorite(this, '2510.10671v1', 'Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Jinxuan Li, Chaolei Tan, Haoxuan Chen, Jianxin Ma, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

**å¤‡æ³¨**: Draft version, work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é¦–ä¸ªåŸºäºå›¾åƒ-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹ çš„ç»¼è¿°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹ ` `å›¾åƒ-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹` `è§†é¢‘ç†è§£` `è§†é¢‘-æ–‡æœ¬å­¦ä¹ ` `è¿ç§»å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†é¢‘-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®å’Œç®—åŠ›ï¼Œæˆæœ¬é«˜æ˜‚ã€‚
2. åˆ©ç”¨å›¾åƒ-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œå¯æœ‰æ•ˆé™ä½è§†é¢‘-æ–‡æœ¬å­¦ä¹ çš„æˆæœ¬ã€‚
3. è¯¥ç»¼è¿°ç³»ç»Ÿæ€§åœ°æ•´ç†äº†å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹ çš„ç­–ç•¥ï¼Œå¹¶åˆ†æäº†å…¶åœ¨ä¸åŒè§†é¢‘ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾åƒ-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹(ILFM)åœ¨å›¾åƒ-æ–‡æœ¬ç†è§£/ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œæä¾›äº†å¯è¿ç§»çš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°å„ç§ä¸‹æ¸¸å›¾åƒä»»åŠ¡ã€‚è§†é¢‘-æ–‡æœ¬ç ”ç©¶çš„è¿›å±•æ¿€å‘äº†äººä»¬å°†åŸºäºå›¾åƒçš„æ¨¡å‹æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸçš„å…´è¶£ã€‚è¿™ç§èŒƒå¼è¢«ç§°ä¸ºå›¾åƒåˆ°è§†é¢‘çš„è¿ç§»å­¦ä¹ ï¼ŒæˆåŠŸåœ°ç¼“è§£äº†ä»å¤´å¼€å§‹è®­ç»ƒè§†é¢‘-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ä»¥è¿›è¡Œè§†é¢‘-æ–‡æœ¬å­¦ä¹ æ‰€éœ€çš„å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚æœ¬ç»¼è¿°é¦–æ¬¡å…¨é¢å›é¡¾äº†è¿™ä¸€æ–°å…´é¢†åŸŸï¼Œé¦–å…ˆæ€»ç»“äº†å¹¿æ³›ä½¿ç”¨çš„ILFMåŠå…¶èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®æ˜¯å¦ä¿ç•™æˆ–ä¿®æ”¹æ¥è‡ªILFMçš„åŸå§‹è¡¨ç¤ºï¼Œå°†ç°æœ‰çš„å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹ ç­–ç•¥ç³»ç»Ÿåœ°åˆ†ä¸ºä¸¤ç±»ï¼šå†»ç»“ç‰¹å¾å’Œä¿®æ”¹ç‰¹å¾ã€‚åŸºäºå›¾åƒåˆ°è§†é¢‘è¿ç§»çš„ä»»åŠ¡ç‰¹å®šæ€§è´¨ï¼Œæœ¬ç»¼è¿°ç³»ç»Ÿåœ°é˜è¿°äº†è¿™äº›ç­–ç•¥ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†å®ƒä»¬åœ¨å„ç§è§†é¢‘-æ–‡æœ¬å­¦ä¹ ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä»ç»†ç²’åº¦ï¼ˆä¾‹å¦‚ï¼Œæ—¶ç©ºè§†é¢‘å®šä½ï¼‰åˆ°ç²—ç²’åº¦ï¼ˆä¾‹å¦‚ï¼Œè§†é¢‘é—®ç­”ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†è¯¦ç»†çš„å®éªŒåˆ†æï¼Œä»¥ç ”ç©¶ä¸åŒçš„å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹ èŒƒå¼åœ¨ä¸€ç³»åˆ—ä¸‹æ¸¸è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ç¡®å®šäº†æ™®éå­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥ç ”ç©¶çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚é€šè¿‡æä¾›å…¨é¢å’Œç»“æ„åŒ–çš„æ¦‚è¿°ï¼Œæœ¬ç»¼è¿°æ—¨åœ¨ä¸ºåŸºäºç°æœ‰ILFMæ¨è¿›è§†é¢‘-æ–‡æœ¬å­¦ä¹ å»ºç«‹ç»“æ„åŒ–çš„è·¯çº¿å›¾ï¼Œå¹¶æ¿€å‘è¿™ä¸ªå¿«é€Ÿå‘å±•é¢†åŸŸä¸­æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘-è¯­è¨€æ¨¡å‹è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®å’Œè®¡ç®—èµ„æºã€‚ç›´æ¥ä»å¤´è®­ç»ƒè§†é¢‘-è¯­è¨€æ¨¡å‹ä¸åˆ‡å®é™…ã€‚å›¾åƒ-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨å›¾åƒé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è¿™äº›æ¨¡å‹æ¥æå‡è§†é¢‘ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½è®­ç»ƒæˆæœ¬ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨å·²æœ‰çš„å›¾åƒ-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹(ILFM)çš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ çš„æ–¹å¼ï¼Œå°†å…¶çŸ¥è¯†è¿ç§»åˆ°è§†é¢‘é¢†åŸŸã€‚æ ¸å¿ƒåœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°å°†å›¾åƒé¢†åŸŸçš„çŸ¥è¯†é€‚é…åˆ°è§†é¢‘é¢†åŸŸï¼Œä»è€Œé¿å…ä»å¤´è®­ç»ƒè§†é¢‘-è¯­è¨€æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç»¼è¿°å°†ç°æœ‰çš„å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹ ç­–ç•¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼šå†»ç»“ç‰¹å¾å’Œä¿®æ”¹ç‰¹å¾ã€‚å†»ç»“ç‰¹å¾æ˜¯æŒ‡ç›´æ¥ä½¿ç”¨ILFMæå–çš„ç‰¹å¾ï¼Œä¸è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼›ä¿®æ”¹ç‰¹å¾æ˜¯æŒ‡å¯¹ILFMæå–çš„ç‰¹å¾è¿›è¡Œè°ƒæ•´æˆ–èåˆï¼Œä»¥é€‚åº”è§†é¢‘æ•°æ®çš„ç‰¹ç‚¹ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé€‰æ‹©åˆé€‚çš„ILFMï¼Œæå–å›¾åƒç‰¹å¾ï¼Œå°†å›¾åƒç‰¹å¾é€‚é…åˆ°è§†é¢‘æ•°æ®ï¼Œåœ¨ä¸‹æ¸¸è§†é¢‘ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒæˆ–è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç»¼è¿°é¦–æ¬¡å¯¹å›¾åƒåˆ°è§†é¢‘çš„è¿ç§»å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„æ€»ç»“å’Œåˆ†ç±»ï¼Œå¹¶ä»å†»ç»“ç‰¹å¾å’Œä¿®æ”¹ç‰¹å¾ä¸¤ä¸ªè§’åº¦å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†åˆ†æã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜å¯¹ä¸åŒè¿ç§»å­¦ä¹ ç­–ç•¥åœ¨ä¸åŒè§†é¢‘ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿›è¡Œäº†å®éªŒåˆ†æï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åœ¨äºå¦‚ä½•å°†å›¾åƒç‰¹å¾ä¸è§†é¢‘çš„æ—¶åºä¿¡æ¯è¿›è¡Œèåˆã€‚å¸¸è§çš„æ–¹æ³•åŒ…æ‹¬ï¼šä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æˆ–Transformeræ¥å»ºæ¨¡è§†é¢‘çš„æ—¶åºå…³ç³»ï¼Œä½¿ç”¨3Då·ç§¯ç¥ç»ç½‘ç»œæ¥æå–è§†é¢‘çš„æ—¶ç©ºç‰¹å¾ï¼Œä»¥åŠä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å…³æ³¨è§†é¢‘ä¸­çš„å…³é”®å¸§æˆ–ç‰‡æ®µã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°å¯¹ä¸åŒçš„å›¾åƒåˆ°è§†é¢‘è¿ç§»å­¦ä¹ èŒƒå¼åœ¨ä¸€ç³»åˆ—ä¸‹æ¸¸è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯¦ç»†çš„å®éªŒåˆ†æï¼Œç»“æœè¡¨æ˜ï¼Œé€šè¿‡åˆé€‚çš„è¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æå‡è§†é¢‘ç†è§£çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­ï¼ŒåŸºäºå›¾åƒ-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„è¿ç§»å­¦ä¹ æ–¹æ³•å¯ä»¥è¾¾åˆ°ä¸ä»å¤´è®­ç»ƒçš„è§†é¢‘-è¯­è¨€æ¨¡å‹ç›¸è¿‘ç”šè‡³æ›´å¥½çš„æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè§†é¢‘ç†è§£ã€è§†é¢‘æ£€ç´¢ã€è§†é¢‘é—®ç­”ã€è§†é¢‘ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨å›¾åƒ-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œå¯ä»¥æœ‰æ•ˆæå‡è§†é¢‘åˆ†æçš„æ€§èƒ½ï¼Œå¹¶é™ä½å¼€å‘æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Image-Language Foundation Models (ILFM) have demonstrated remarkable success in image-text understanding/generation tasks, providing transferable multimodal representations that generalize across diverse downstream image-based tasks. The advancement of video-text research has spurred growing interest in extending image-based models to the video domain. This paradigm, known as image-to-video transfer learning, succeeds in alleviating the substantial data and computational requirements associated with training video-language foundation models from scratch for video-text learning. This survey provides the first comprehensive review of this emerging field, which begins by summarizing the widely used ILFM and their capabilities. We then systematically classify existing image-to-video transfer learning strategies into two categories: frozen features and modified features, depending on whether the original representations from ILFM are preserved or undergo modifications. Building upon the task-specific nature of image-to-video transfer, this survey methodically elaborates these strategies and details their applications across a spectrum of video-text learning tasks, ranging from fine-grained (e.g., spatio-temporal video grounding) to coarse-grained (e.g., video question answering). We further present a detailed experimental analysis to investigate the efficacy of different image-to-video transfer learning paradigms on a range of downstream video understanding tasks. Finally, we identify prevailing challenges and highlight promising directions for future research. By offering a comprehensive and structured overview, this survey aims to establish a structured roadmap for advancing video-text learning based on existing ILFM, and to inspire future research directions in this rapidly evolving domain.

