---
layout: default
title: Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes
---

# Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10406" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10406v1</a>
  <a href="https://arxiv.org/pdf/2510.10406.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10406v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10406v1', 'Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhao-Yang Wang, Jieneng Chen, Jiang Liu, Yuxiang Guo, Rama Chellappa

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Mesh-Gaitï¼šæå‡ºä¸€ç§åŸºäº2Dè½®å»“å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ çš„ç»Ÿä¸€æ­¥æ€è¯†åˆ«æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ­¥æ€è¯†åˆ«` `å¤šæ¨¡æ€å­¦ä¹ ` `3Dé‡å»º` `çƒ­å›¾è¡¨ç¤º` `ç”Ÿç‰©ç‰¹å¾è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ­¥æ€è¯†åˆ«æ–¹æ³•åœ¨è§†è§’å˜åŒ–ã€é®æŒ¡å’Œå™ªå£°ä¸‹è¡¨ç°ä¸ä½³ï¼Œè€Œç›´æ¥ä½¿ç”¨3Dä¿¡æ¯è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚
2. Mesh-Gaité€šè¿‡ä»2Dè½®å»“é‡å»º3Dçƒ­å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œé«˜æ•ˆåœ°èåˆäº†2Då’Œ3Dä¿¡æ¯çš„ä¼˜åŠ¿ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMesh-Gaitåœ¨æ­¥æ€è¯†åˆ«ç²¾åº¦ä¸Šè¾¾åˆ°äº†å½“å‰æœ€ä¼˜æ°´å¹³ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ­¥æ€è¯†åˆ«æ˜¯ä¸€ç§é‡è¦çš„ç”Ÿç‰©ç‰¹å¾æŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨ç‹¬ç‰¹çš„è¡Œèµ°æ¨¡å¼è¿›è¡Œä¸ªä½“è¯†åˆ«ï¼Œé€šå¸¸ä½¿ç”¨è½®å»“æˆ–éª¨éª¼ç­‰2Dè¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨è§†è§’å˜åŒ–ã€é®æŒ¡å’Œå™ªå£°æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç»“åˆ3Dèº«ä½“å½¢çŠ¶ä¿¡æ¯çš„å¤šæ¨¡æ€æ–¹æ³•è™½ç„¶æé«˜äº†é²æ£’æ€§ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ­¥æ€è¯†åˆ«æ¡†æ¶Mesh-Gaitï¼Œè¯¥æ¡†æ¶ç›´æ¥ä»2Dè½®å»“é‡å»º3Dè¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†ä¸¤ç§æ¨¡æ€çš„ä¼˜åŠ¿ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œç›´æ¥ä»3Då…³èŠ‚æˆ–ç½‘æ ¼å­¦ä¹ 3Dç‰¹å¾æ˜¯å¤æ‚ä¸”éš¾ä»¥ä¸åŸºäºè½®å»“çš„æ­¥æ€ç‰¹å¾èåˆçš„ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼ŒMesh-Gaité‡å»º3Dçƒ­å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·3Då‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒç®€å•æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸­é—´3Dçƒ­å›¾åœ¨ç›‘ç£å­¦ä¹ ä¸‹é€æ¸é‡å»ºå¹¶å˜å¾—è¶Šæ¥è¶Šå‡†ç¡®ï¼Œå…¶ä¸­æŸå¤±æ˜¯åœ¨é‡å»ºçš„3Då…³èŠ‚ã€è™šæ‹Ÿæ ‡è®°å’Œ3Dç½‘æ ¼åŠå…¶å¯¹åº”çš„çœŸå®å€¼ä¹‹é—´è®¡ç®—çš„ï¼Œç¡®ä¿ç²¾ç¡®çš„ç©ºé—´å¯¹é½å’Œä¸€è‡´çš„3Dç»“æ„ã€‚Mesh-Gaitä»¥è®¡ç®—é«˜æ•ˆçš„æ–¹å¼ä»è½®å»“å’Œé‡å»ºçš„3Dçƒ­å›¾ä¸­æå–åˆ¤åˆ«æ€§ç‰¹å¾ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·ç©ºé—´å’Œç»“æ„æ­¥æ€ç‰¹å¾ï¼ŒåŒæ—¶é¿å…äº†ç›´æ¥ä»RGBè§†é¢‘è¿›è¡Œ3Dé‡å»ºçš„ç¹é‡å¼€é”€ï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿä¸“æ³¨äºè¿åŠ¨åŠ¨æ€è€Œä¸æ˜¯ä¸ç›¸å…³çš„è§†è§‰ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMesh-Gaitå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚ä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ­¥æ€è¯†åˆ«æ–¹æ³•ä¸»è¦ä¾èµ–äº2Dè½®å»“æˆ–éª¨éª¼ä¿¡æ¯ï¼Œå®¹æ˜“å—åˆ°è§†è§’å˜åŒ–ã€é®æŒ¡å’Œå™ªå£°çš„å½±å“ï¼Œé²æ£’æ€§è¾ƒå·®ã€‚è™½ç„¶åŸºäº3Dä¿¡æ¯çš„æ–¹æ³•å¯ä»¥æé«˜é²æ£’æ€§ï¼Œä½†ç›´æ¥ä»RGBè§†é¢‘è¿›è¡Œ3Dé‡å»ºè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯é²æ£’æ€§çš„å‰æä¸‹ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæ˜¯æ­¥æ€è¯†åˆ«é¢†åŸŸé¢ä¸´çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMesh-Gaitçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡2Dè½®å»“é‡å»º3Dçƒ­å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œä»è€Œå°†2Dè½®å»“ä¿¡æ¯å’Œ3Då‡ ä½•ä¿¡æ¯æœ‰æ•ˆåœ°ç»“åˆèµ·æ¥ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥ä»RGBè§†é¢‘è¿›è¡Œ3Dé‡å»ºçš„å¤æ‚æ€§ï¼ŒåŒæ—¶åˆèƒ½åˆ©ç”¨3Dä¿¡æ¯æé«˜é²æ£’æ€§ã€‚é€šè¿‡ç›‘ç£å­¦ä¹ ï¼Œé€æ­¥ä¼˜åŒ–3Dçƒ­å›¾çš„é‡å»ºç²¾åº¦ï¼Œç¡®ä¿ç©ºé—´å¯¹é½å’Œç»“æ„ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMesh-Gaitæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä»2Dè½®å»“è¾“å…¥å¼€å§‹ï¼›2) é€šè¿‡ç½‘ç»œé‡å»º3Dçƒ­å›¾ï¼›3) ä»2Dè½®å»“å’Œé‡å»ºçš„3Dçƒ­å›¾ä¸­æå–ç‰¹å¾ï¼›4) å°†æå–çš„ç‰¹å¾è¿›è¡Œèåˆï¼›5) ä½¿ç”¨åˆ†ç±»å™¨è¿›è¡Œæ­¥æ€è¯†åˆ«ã€‚è¯¥æ¡†æ¶æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶ä¼˜åŒ–2Då’Œ3Dç‰¹å¾çš„æå–å’Œèåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šMesh-Gaitçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨3Dçƒ­å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œè¿æ¥2Dè½®å»“å’Œ3Då‡ ä½•ä¿¡æ¯ã€‚ä¸ç›´æ¥ä»3Då…³èŠ‚æˆ–ç½‘æ ¼å­¦ä¹ 3Dç‰¹å¾ç›¸æ¯”ï¼Œé‡å»º3Dçƒ­å›¾æ›´åŠ ç®€å•é«˜æ•ˆï¼Œå¹¶ä¸”æ›´å®¹æ˜“ä¸åŸºäºè½®å»“çš„æ­¥æ€ç‰¹å¾èåˆã€‚æ­¤å¤–ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ ï¼Œé€æ­¥ä¼˜åŒ–3Dçƒ­å›¾çš„é‡å»ºç²¾åº¦ï¼Œç¡®ä¿ç©ºé—´å¯¹é½å’Œç»“æ„ä¸€è‡´æ€§ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒæŸå¤±å‡½æ•°æ˜¯åœ¨é‡å»ºçš„3Då…³èŠ‚ã€è™šæ‹Ÿæ ‡è®°å’Œ3Dç½‘æ ¼åŠå…¶å¯¹åº”çš„çœŸå®å€¼ä¹‹é—´è®¡ç®—çš„ï¼Œä»¥ç¡®ä¿ç²¾ç¡®çš„ç©ºé—´å¯¹é½å’Œä¸€è‡´çš„3Dç»“æ„ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†è¯´æ˜ï¼Œéœ€è¦åœ¨ä»£ç å‘å¸ƒåè¿›ä¸€æ­¥åˆ†æã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ï¼Œéœ€è¦å¹³è¡¡2Dè½®å»“ä¿¡æ¯å’Œ3Då‡ ä½•ä¿¡æ¯çš„è´¡çŒ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†Mesh-Gaitçš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜Mesh-Gaitåœ¨æ­¥æ€è¯†åˆ«ç²¾åº¦ä¸Šè¾¾åˆ°äº†å½“å‰æœ€ä¼˜æ°´å¹³ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚è¯¥æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œè¯†åˆ«ç²¾åº¦ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œä½¿å…¶æ›´é€‚åˆå®é™…åº”ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Mesh-Gaitåœ¨å®‰é˜²ç›‘æ§ã€æ™ºèƒ½å®¶å±…ã€åŒ»ç–—å¥åº·ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºåœ¨ç›‘æ§è§†é¢‘ä¸­è¯†åˆ«ç‰¹å®šäººå‘˜ï¼Œåœ¨æ™ºèƒ½å®¶å±…ä¸­æ ¹æ®æ­¥æ€è¯†åˆ«ç”¨æˆ·èº«ä»½ï¼Œåœ¨åŒ»ç–—å¥åº·é¢†åŸŸè¯„ä¼°æ‚£è€…çš„æ­¥æ€å¥åº·çŠ¶å†µã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºï¼Œå®ƒä¸ºæ­¥æ€è¯†åˆ«æä¾›äº†ä¸€ç§æ›´åŠ é²æ£’å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æœ›æ¨åŠ¨æ­¥æ€è¯†åˆ«æŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Gait recognition, a fundamental biometric technology, leverages unique walking patterns for individual identification, typically using 2D representations such as silhouettes or skeletons. However, these methods often struggle with viewpoint variations, occlusions, and noise. Multi-modal approaches that incorporate 3D body shape information offer improved robustness but are computationally expensive, limiting their feasibility for real-time applications. To address these challenges, we introduce Mesh-Gait, a novel end-to-end multi-modal gait recognition framework that directly reconstructs 3D representations from 2D silhouettes, effectively combining the strengths of both modalities. Compared to existing methods, directly learning 3D features from 3D joints or meshes is complex and difficult to fuse with silhouette-based gait features. To overcome this, Mesh-Gait reconstructs 3D heatmaps as an intermediate representation, enabling the model to effectively capture 3D geometric information while maintaining simplicity and computational efficiency. During training, the intermediate 3D heatmaps are gradually reconstructed and become increasingly accurate under supervised learning, where the loss is calculated between the reconstructed 3D joints, virtual markers, and 3D meshes and their corresponding ground truth, ensuring precise spatial alignment and consistent 3D structure. Mesh-Gait extracts discriminative features from both silhouettes and reconstructed 3D heatmaps in a computationally efficient manner. This design enables the model to capture spatial and structural gait characteristics while avoiding the heavy overhead of direct 3D reconstruction from RGB videos, allowing the network to focus on motion dynamics rather than irrelevant visual details. Extensive experiments demonstrate that Mesh-Gait achieves state-of-the-art accuracy. The code will be released upon acceptance of the paper.

