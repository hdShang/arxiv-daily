---
layout: default
title: Towards Self-Refinement of Vision-Language Models with Triangular Consistency
---

# Towards Self-Refinement of Vision-Language Models with Triangular Consistency

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10487" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10487v1</a>
  <a href="https://arxiv.org/pdf/2510.10487.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10487v1" onclick="toggleFavorite(this, '2510.10487v1', 'Towards Self-Refinement of Vision-Language Models with Triangular Consistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunlong Deng, Guangyi Chen, Tianpei Gu, Lingjing Kong, Yan Li, Zeyu Tang, Kun Zhang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dengyl20/SRF-LLaVA-1.5)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸‰è§’ä¸€è‡´æ€§çš„è‡ªç²¾ç‚¼æ¡†æ¶ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `è‡ªç²¾ç‚¼` `ä¸‰è§’ä¸€è‡´æ€§` `æ— ç›‘ç£å­¦ä¹ ` `æŒ‡ä»¤å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ä¸»è¦ä¾èµ–æœ‰ç›‘ç£çš„æŒ‡ä»¤å¾®è°ƒï¼Œæœªå……åˆ†æŒ–æ˜æ— ç›‘ç£æŒ‡ä»¤ä¸‹çš„è‡ªå­¦ä¹ æ½œåŠ›ã€‚
2. æå‡ºåŸºäºä¸‰è§’ä¸€è‡´æ€§çš„è‡ªç²¾ç‚¼æ¡†æ¶ï¼Œé€šè¿‡å›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„çš„ä¸€è‡´æ€§çº¦æŸï¼Œå®ç°æ¨¡å‹è‡ªä¸»å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨LLaVA-1.5åŸºç¡€ä¸Šï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£å³å¯åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)åœ¨æ— ç›‘ç£æŒ‡ä»¤ä¸‹è¿›è¡Œè‡ªç²¾ç‚¼çš„æ½œåŠ›ã€‚é€šè¿‡å›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„ï¼ŒVLMsé›†æˆäº†è§†è§‰çŸ¥è¯†å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„åˆ†æèƒ½åŠ›ã€‚æœ¬æ–‡éªŒè¯äº†VLMså…·æœ‰å†…åœ¨çš„è‡ªç²¾ç‚¼èƒ½åŠ›ï¼Œæ— éœ€å¤–éƒ¨è¾“å…¥å³å¯ç”Ÿæˆé«˜è´¨é‡çš„ç›‘ç£æ•°æ®ï¼Œä»è€Œå®ç°è‡ªä¸»å­¦ä¹ ã€‚ä¸ºäº†æ¿€å‘VLMsçš„è‡ªç²¾ç‚¼èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸‰è§’ä¸€è‡´æ€§åŸåˆ™çš„è‡ªç²¾ç‚¼æ¡†æ¶ï¼šåœ¨å›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰è§’ä¸­ï¼Œä»»ä½•è¢«æ©ç›–çš„å…ƒç´ éƒ½åº”è¢«ä¸€è‡´ä¸”å‡†ç¡®åœ°é‡å»ºã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š(1)é€šè¿‡æ·»åŠ å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒ(å¦‚imageâ†’question-answeræˆ–image-answerâ†’question)æ¥å¯ç”¨VLMsçš„æŒ‡ä»¤ç”Ÿæˆèƒ½åŠ›ã€‚(2)ä»æ— æ ‡ç­¾å›¾åƒç”Ÿæˆå›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œå¹¶ä½¿ç”¨ä¸‰è§’ä¸€è‡´æ€§åŸåˆ™è¿›è¡Œè¿‡æ»¤ã€‚(3)ä½¿ç”¨è¿‡æ»¤åçš„åˆæˆæ•°æ®è¿›ä¸€æ­¥æ›´æ–°æ¨¡å‹ã€‚ä¸ºäº†ç ”ç©¶è¿™ç§è‡ªç²¾ç‚¼èƒ½åŠ›èƒŒåçš„æ½œåœ¨æœºåˆ¶ï¼Œæœ¬æ–‡ä»å› æœè§’åº¦è¿›è¡Œäº†ç†è®ºåˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¹¿æ³›è®¤å¯çš„LLaVA-1.5ä½œä¸ºåŸºçº¿ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•å¤–éƒ¨ç›‘ç£(å¦‚äººå·¥æ ‡æ³¨æˆ–ç¯å¢ƒåé¦ˆ)çš„æƒ…å†µä¸‹ï¼Œè‡ªä¸»åœ°åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°ä¸€è‡´çš„æ”¹è¿›ã€‚æœ¬æ–‡å¯¹VLMsè‡ªç²¾ç‚¼èƒ½åŠ›çš„è§è§£å¯ä»¥å¯å‘æœªæ¥å¯¹VLMså­¦ä¹ æœºåˆ¶çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)ä¸»è¦ä¾èµ–äºäººå·¥æ ‡æ³¨çš„å›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚å¦‚ä½•åˆ©ç”¨VLMsè‡ªèº«çš„èƒ½åŠ›ï¼Œåœ¨æ²¡æœ‰æˆ–å¾ˆå°‘äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œå®ç°æ¨¡å‹çš„è‡ªå­¦ä¹ å’Œæ€§èƒ½æå‡ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä»¥åŠæœªèƒ½å……åˆ†æŒ–æ˜VLMsè‡ªèº«è•´å«çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨VLMsè‡ªèº«çš„èƒ½åŠ›ï¼Œé€šè¿‡ä¸‰è§’ä¸€è‡´æ€§åŸåˆ™ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è‡ªç›‘ç£æ•°æ®ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®æ¥è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œå›¾åƒã€é—®é¢˜å’Œç­”æ¡ˆæ„æˆä¸€ä¸ªä¸‰è§’å…³ç³»ï¼Œåœ¨è¿™ä¸ªå…³ç³»ä¸­ï¼Œä»»ä½•ä¸€ä¸ªå…ƒç´ éƒ½å¯ä»¥ç”±å…¶ä»–ä¸¤ä¸ªå…ƒç´ æ¨å¯¼å‡ºæ¥ã€‚å¦‚æœVLMsèƒ½å¤Ÿåœ¨è¿™ä¸ªä¸‰è§’å…³ç³»ä¸­ä¿æŒä¸€è‡´æ€§ï¼Œé‚£ä¹ˆå°±å¯ä»¥è®¤ä¸ºå®ƒå…·å¤‡äº†è‡ªç²¾ç‚¼çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥è‡ªç²¾ç‚¼æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æŒ‡ä»¤ç”Ÿæˆèƒ½åŠ›å¢å¼ºï¼šé€šè¿‡å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒï¼Œä¾‹å¦‚imageâ†’question-answeræˆ–image-answerâ†’questionï¼Œä½¿VLMså…·å¤‡ç”ŸæˆæŒ‡ä»¤çš„èƒ½åŠ›ã€‚2) æ•°æ®ç”Ÿæˆä¸è¿‡æ»¤ï¼šä»æ— æ ‡ç­¾å›¾åƒç”Ÿæˆå›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œå¹¶ä½¿ç”¨ä¸‰è§’ä¸€è‡´æ€§åŸåˆ™å¯¹ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œä¿ç•™é«˜è´¨é‡çš„æ•°æ®ã€‚3) æ¨¡å‹æ›´æ–°ï¼šä½¿ç”¨è¿‡æ»¤åçš„åˆæˆæ•°æ®è¿›ä¸€æ­¥æ›´æ–°æ¨¡å‹ï¼Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºä¸‰è§’ä¸€è‡´æ€§çš„è‡ªç²¾ç‚¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨VLMsè‡ªèº«çš„èƒ½åŠ›ï¼Œåœ¨æ²¡æœ‰æˆ–å¾ˆå°‘äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è‡ªç›‘ç£æ•°æ®ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®æ¥è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦äººå·¥æ ‡æ³¨æ•°æ®ï¼Œå¯ä»¥å®ç°æ¨¡å‹çš„è‡ªä¸»å­¦ä¹ å’Œæ€§èƒ½æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨äº†ä¸åŒçš„promptæ¥ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆï¼Œä»¥å¢åŠ æ•°æ®çš„å¤šæ ·æ€§ã€‚åœ¨æ•°æ®è¿‡æ»¤é˜¶æ®µï¼Œä½¿ç”¨äº†å¤šç§æŒ‡æ ‡æ¥è¡¡é‡ä¸‰è§’ä¸€è‡´æ€§ï¼Œä¾‹å¦‚é—®é¢˜ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®ç‡ã€ç­”æ¡ˆç”Ÿæˆé—®é¢˜çš„ç›¸å…³æ€§ç­‰ã€‚åœ¨æ¨¡å‹æ›´æ–°é˜¶æ®µï¼Œä½¿ç”¨äº†ä¸åŒçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚äº¤å‰ç†µæŸå¤±å‡½æ•°ã€å¯¹æ¯”æŸå¤±å‡½æ•°ç­‰ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ä¸LLaVA-1.5ä¿æŒä¸€è‡´ï¼Œä»¥ä¿è¯å®éªŒçš„å…¬å¹³æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºä¸‰è§’ä¸€è‡´æ€§çš„è‡ªç²¾ç‚¼æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæå‡LLaVA-1.5çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¾‹å¦‚VQAv2ã€OK-VQAç­‰ï¼Œæ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚è™½ç„¶æå‡å¹…åº¦ç›¸å¯¹ä¿å®ˆï¼Œä½†è¯æ˜äº†VLMså…·å¤‡è‡ªä¸»å­¦ä¹ å’Œæ€§èƒ½æå‡çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è§†è§‰-è¯­è¨€ç†è§£çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€å›¾åƒæœç´¢ã€è§†è§‰é—®ç­”ã€æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡è‡ªç²¾ç‚¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¸æ–­æå‡æ€§èƒ½ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) integrate visual knowledge with the analytical capabilities of Large Language Models (LLMs) through supervised visual instruction tuning, using image-question-answer triplets. However, the potential of VLMs trained without supervised instruction remains largely unexplored. This study validates that VLMs possess inherent self-refinement capabilities, enabling them to generate high-quality supervised data without external inputs and thereby learn autonomously. Specifically, to stimulate the self-refinement ability of VLMs, we propose a self-refinement framework based on a Triangular Consistency principle: within the image-query-answer triangle, any masked elements should be consistently and accurately reconstructed. The framework involves three steps: (1) We enable the instruction generation ability of VLMs by adding multi-task instruction tuning like image$\rightarrow$question-answer or image-answer$\rightarrow$question. (2) We generate image-query-answer triplets from unlabeled images and use the Triangular Consistency principle for filtering. (3) The model is further updated using the filtered synthetic data. To investigate the underlying mechanisms behind this self-refinement capability, we conduct a theoretical analysis from a causal perspective. Using the widely recognized LLaVA-1.5 as our baseline, our experiments reveal that the model can autonomously achieve consistent, though deliberately modest, improvements across multiple benchmarks without any external supervision, such as human annotations or environmental feedback. We expect that the insights of this study on the self-refinement ability of VLMs can inspire future research on the learning mechanism of VLMs. Code is available at https://github.com/dengyl20/SRF-LLaVA-1.5.

