---
layout: default
title: VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning
---

# VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10518" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10518v3</a>
  <a href="https://arxiv.org/pdf/2510.10518.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10518v3" onclick="toggleFavorite(this, '2510.10518v3', 'VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qunzhong Wang, Jie Liu, Jiajun Liang, Yilei Jiang, Yuanxing Zhang, Jinyuan Chen, Yaozhi Zheng, Xintao Wang, Pengfei Wan, Xiangyu Yue, Jiaheng Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12 (æ›´æ–°: 2025-10-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VR-Thinkerï¼šé€šè¿‡å›¾åƒæ¨ç†å¢å¼ºè§†é¢‘å¥–åŠ±æ¨¡å‹ï¼Œæå‡é•¿è§†é¢‘åå¥½åˆ¤æ–­ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¥–åŠ±æ¨¡å‹` `å›¾åƒæ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `é•¿è§†é¢‘ç†è§£` `è§†è§‰è®°å¿†` `æ€ç»´é“¾æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å¥–åŠ±æ¨¡å‹å—é™äºè§†è§‰è¾“å…¥çš„ä¸Šä¸‹æ–‡é¢„ç®—ï¼Œå¯¼è‡´æ— æ³•å¤„ç†é•¿è§†é¢‘ï¼Œä¸”å®¹æ˜“åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå¹»è§‰ã€‚
2. VR-Thinker å¼•å…¥äº†â€œå›¾åƒæ¨ç†â€æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨é€‰æ‹©å’Œæ›´æ–°è§†è§‰è¯æ®ï¼Œä»è€Œåœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ›´å¯é çš„æ¨ç†ã€‚
3. é€šè¿‡å†·å¯åŠ¨ã€æ‹’ç»é‡‡æ ·å¾®è°ƒå’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç­‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒVR-Thinkeråœ¨å¤šä¸ªè§†é¢‘åå¥½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹(RMs)çš„æœ€æ–°è¿›å±•æ˜¾è‘—æ”¹å–„äº†è§†è§‰ç”Ÿæˆæ¨¡å‹çš„åè®­ç»ƒã€‚ç„¶è€Œï¼Œå½“å‰çš„RMsé¢ä¸´å›ºæœ‰çš„å±€é™æ€§ï¼š(1)è§†è§‰è¾“å…¥æ¶ˆè€—å¤§é‡çš„ä¸Šä¸‹æ–‡é¢„ç®—ï¼Œè¿«ä½¿å‡å°‘å¸§æ•°ï¼Œå¯¼è‡´ä¸¢å¤±ç»†ç²’åº¦ç»†èŠ‚ï¼›(2)æ‰€æœ‰è§†è§‰ä¿¡æ¯éƒ½è¢«æ‰“åŒ…åˆ°åˆå§‹æç¤ºä¸­ï¼ŒåŠ å‰§äº†æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰å’Œé—å¿˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoReward Thinker (VR-Thinker)ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å›¾åƒæ¨ç†çš„æ¡†æ¶ï¼Œå®ƒä¸ºRMé…å¤‡äº†è§†è§‰æ¨ç†æ“ä½œ(ä¾‹å¦‚ï¼Œé€‰æ‹©å¸§)å’Œä¸€ä¸ªå¯é…ç½®çš„è§†è§‰è®°å¿†çª—å£ã€‚è¿™ä½¿å¾—RMèƒ½å¤Ÿåœ¨ä¸Šä¸‹æ–‡é™åˆ¶å†…ä¸»åŠ¨è·å–å’Œæ›´æ–°è§†è§‰è¯æ®ï¼Œæé«˜æ¨ç†çš„ä¿çœŸåº¦å’Œå¯é æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¼ºåŒ–å¾®è°ƒç®¡é“æ¿€æ´»è§†è§‰æ¨ç†ï¼š(i)ä½¿ç”¨ç²¾é€‰çš„è§†è§‰æ€ç»´é“¾æ•°æ®è¿›è¡Œå†·å¯åŠ¨ï¼Œä»¥æç‚¼åŸºæœ¬çš„æ¨ç†æŠ€èƒ½å’Œæ“ä½œæ ¼å¼ï¼›(ii)é€‰æ‹©æ¯ä¸ªç»´åº¦å’Œæ€»ä½“åˆ¤æ–­éƒ½æ­£ç¡®çš„æ ·æœ¬ï¼Œç„¶åå¯¹è¿™äº›é«˜è´¨é‡çš„è½¨è¿¹è¿›è¡Œæ‹’ç»é‡‡æ ·å¾®è°ƒï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨ç†ï¼›(iii)åº”ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)æ¥åŠ å¼ºæ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘åå¥½åŸºå‡†æµ‹è¯•ä¸­æä¾›äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒé•¿çš„è§†é¢‘ä¸­ï¼šä¸€ä¸ª7B VR-Thinkeråœ¨VideoGen Rewardä¸Šè¾¾åˆ°80.5%ï¼Œåœ¨GenAI-Benchä¸Šè¾¾åˆ°82.3%ï¼Œåœ¨MJ-Bench-Videoä¸Šè¾¾åˆ°75.6%ã€‚è¿™äº›ç»“æœéªŒè¯äº†é€šè¿‡å›¾åƒè¿›è¡Œå¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡çš„æœ‰æ•ˆæ€§å’Œå‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘å¥–åŠ±æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œç”±äºè§†è§‰ä¿¡æ¯é‡å¤§ï¼Œä¸Šä¸‹æ–‡é¢„ç®—æœ‰é™ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•å……åˆ†åˆ©ç”¨æ‰€æœ‰å¸§çš„ä¿¡æ¯ï¼Œå®¹æ˜“ä¸¢å¤±ç»†èŠ‚ï¼Œå¹¶ä¸”åœ¨æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°å¹»è§‰å’Œé—å¿˜ç°è±¡ã€‚è¿™é™åˆ¶äº†æ¨¡å‹å¯¹è§†é¢‘å†…å®¹è¿›è¡Œå‡†ç¡®å’Œå¯é è¯„ä¼°çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVR-Thinkerçš„æ ¸å¿ƒæ€è·¯æ˜¯èµ‹äºˆå¥–åŠ±æ¨¡å‹â€œæ€è€ƒâ€çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œä¸»åŠ¨é€‰æ‹©å’Œåˆ©ç”¨å…³é”®çš„è§†è§‰ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚é€šè¿‡å¼•å…¥è§†è§‰æ¨ç†æ“ä½œå’Œè§†è§‰è®°å¿†çª—å£ï¼Œæ¨¡å‹å¯ä»¥åœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡é¢„ç®—ä¸‹ï¼ŒåŠ¨æ€åœ°è·å–å’Œæ›´æ–°è§†è§‰è¯æ®ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVR-Thinkerçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰æ¨ç†æ¨¡å—ï¼šè´Ÿè´£æ‰§è¡Œè§†è§‰æ¨ç†æ“ä½œï¼Œä¾‹å¦‚é€‰æ‹©å…³é”®å¸§ã€‚2) è§†è§‰è®°å¿†çª—å£ï¼šç”¨äºå­˜å‚¨å’Œæ›´æ–°è§†è§‰è¯æ®ã€‚3) å¥–åŠ±æ¨¡å‹ï¼šåŸºäºè§†è§‰è¯æ®è¿›è¡Œå¥–åŠ±é¢„æµ‹ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ï¼š(1) å†·å¯åŠ¨ï¼šä½¿ç”¨ç²¾é€‰çš„è§†è§‰æ€ç»´é“¾æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ åŸºæœ¬çš„æ¨ç†æŠ€èƒ½å’Œæ“ä½œæ ¼å¼ã€‚(2) æ‹’ç»é‡‡æ ·å¾®è°ƒï¼šé€‰æ‹©é«˜è´¨é‡çš„æ¨ç†è½¨è¿¹è¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚(3) ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼šä½¿ç”¨GRPOç®—æ³•ä¼˜åŒ–ç­–ç•¥ï¼Œæé«˜æ¨ç†çš„é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šVR-Thinkerçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†â€œé€šè¿‡å›¾åƒæ¨ç†â€çš„å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„å°†æ‰€æœ‰è§†è§‰ä¿¡æ¯æ‰“åŒ…åˆ°åˆå§‹æç¤ºä¸­çš„æ–¹æ³•ä¸åŒï¼ŒVR-Thinkerå…è®¸æ¨¡å‹ä¸»åŠ¨åœ°ä¸è§†è§‰ä¿¡æ¯è¿›è¡Œäº¤äº’ï¼Œé€‰æ‹©å’Œåˆ©ç”¨å…³é”®ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚è¿™ç§æ–¹æ³•æ›´ç¬¦åˆäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVR-Thinkerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) å¯é…ç½®çš„è§†è§‰è®°å¿†çª—å£å¤§å°ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹å¯ä»¥å­˜å‚¨çš„è§†è§‰è¯æ®æ•°é‡ã€‚(2) åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒpipelineï¼ŒåŒ…æ‹¬å†·å¯åŠ¨ã€æ‹’ç»é‡‡æ ·å¾®è°ƒå’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œç”¨äºæœ‰æ•ˆåœ°è®­ç»ƒè§†è§‰æ¨ç†æ¨¡å—ã€‚(3) è§†è§‰æ¨ç†æ“ä½œçš„å…·ä½“å®ç°ï¼Œä¾‹å¦‚ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©å…³é”®å¸§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VR-Thinkeråœ¨å¤šä¸ªè§†é¢‘åå¥½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª7Bå‚æ•°çš„VR-Thinkeræ¨¡å‹åœ¨VideoGen Rewardä¸Šè¾¾åˆ°äº†80.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨GenAI-Benchä¸Šè¾¾åˆ°äº†82.3%çš„å‡†ç¡®ç‡ï¼Œåœ¨MJ-Bench-Videoä¸Šè¾¾åˆ°äº†75.6%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVR-Thinkeråœ¨é•¿è§†é¢‘åå¥½åˆ¤æ–­æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VR-Thinkerå¯åº”ç”¨äºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œä¾‹å¦‚é€šè¿‡å¥–åŠ±æ¨¡å‹å¼•å¯¼ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºè§†é¢‘å†…å®¹ç†è§£ã€è§†é¢‘æ‘˜è¦ã€è§†é¢‘è´¨é‡è¯„ä¼°ç­‰é¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in multimodal reward models (RMs) have substantially improved post-training for visual generative models. However, current RMs face inherent limitations: (1) visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and (2) all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and a configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: (i) Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii) select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longer videos: a 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling.

