---
layout: default
title: Unified Open-World Segmentation with Multi-Modal Prompts
---

# Unified Open-World Segmentation with Multi-Modal Prompts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10524" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10524v1</a>
  <a href="https://arxiv.org/pdf/2510.10524.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10524v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10524v1', 'Unified Open-World Segmentation with Multi-Modal Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Liu, Yufei Yin, Chenchen Jing, Muzhi Zhu, Hao Chen, Yuling Xi, Bo Feng, Hao Wang, Shiyu Li, Chunhua Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

**å¤‡æ³¨**: Accepted to ICCV2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**COSINEï¼šå¤šæ¨¡æ€æç¤ºä¸‹çš„ç»Ÿä¸€å¼€æ”¾ä¸–ç•Œåˆ†å‰²æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼€æ”¾ä¸–ç•Œåˆ†å‰²` `å¤šæ¨¡æ€æç¤º` `ä¸Šä¸‹æ–‡åˆ†å‰²` `å¼€æ”¾è¯æ±‡åˆ†å‰²` `åŸºç¡€æ¨¡å‹` `ç»Ÿä¸€æ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œä¸Šä¸‹æ–‡åˆ†å‰²æ–¹æ³•å­˜åœ¨æ¶æ„å·®å¼‚ã€å­¦ä¹ ç›®æ ‡ä¸åŒä»¥åŠè¡¨ç¤ºå­¦ä¹ ç­–ç•¥å„å¼‚çš„é—®é¢˜ã€‚
2. COSINEçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨åŸºç¡€æ¨¡å‹æå–å¤šæ¨¡æ€æç¤ºçš„è¡¨ç¤ºï¼Œå¹¶é€šè¿‡SegDecoderå¯¹é½å’Œäº¤äº’è¿™äº›è¡¨ç¤ºï¼Œä»è€Œå®ç°ç»Ÿä¸€çš„åˆ†å‰²ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCOSINEåœ¨å¼€æ”¾è¯æ±‡å’Œä¸Šä¸‹æ–‡åˆ†å‰²ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶éªŒè¯äº†å¤šæ¨¡æ€æç¤ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†COSINEï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¼€æ”¾ä¸–ç•Œåˆ†å‰²æ¨¡å‹ï¼Œå®ƒæ•´åˆäº†å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œä¸Šä¸‹æ–‡åˆ†å‰²ï¼Œå¹¶ä½¿ç”¨å¤šæ¨¡æ€æç¤ºï¼ˆä¾‹å¦‚ï¼Œæ–‡æœ¬å’Œå›¾åƒï¼‰ã€‚COSINEåˆ©ç”¨åŸºç¡€æ¨¡å‹æå–è¾“å…¥å›¾åƒå’Œç›¸åº”å¤šæ¨¡æ€æç¤ºçš„è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨SegDecoderå¯¹é½è¿™äº›è¡¨ç¤ºï¼Œå»ºæ¨¡å®ƒä»¬çš„äº¤äº’ï¼Œå¹¶è·å¾—ç”±ä¸åŒç²’åº¦çš„è¾“å…¥æç¤ºæŒ‡å®šçš„æ©ç ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCOSINEå…‹æœäº†å…ˆå‰å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œä¸Šä¸‹æ–‡åˆ†å‰²æµç¨‹åœ¨æ¶æ„å·®å¼‚ã€ä¸åŒçš„å­¦ä¹ ç›®æ ‡å’Œä¸åŒçš„è¡¨ç¤ºå­¦ä¹ ç­–ç•¥ä¸Šçš„é—®é¢˜ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCOSINEåœ¨å¼€æ”¾è¯æ±‡å’Œä¸Šä¸‹æ–‡åˆ†å‰²ä»»åŠ¡ä¸­éƒ½å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„æ¢ç´¢æ€§åˆ†æå¼ºè°ƒï¼Œä½¿ç”¨è§†è§‰å’Œæ–‡æœ¬æç¤ºä¹‹é—´çš„ååŒåˆä½œå¯ä»¥æ˜¾è‘—æé«˜å¯¹å•æ¨¡æ€æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾ä¸–ç•Œåˆ†å‰²é—®é¢˜ï¼Œå…·ä½“è€Œè¨€ï¼Œæ˜¯ç»Ÿä¸€å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œä¸Šä¸‹æ–‡åˆ†å‰²ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸å°†è¿™ä¸¤ç§åˆ†å‰²ä»»åŠ¡è§†ä¸ºç‹¬ç«‹çš„ä»»åŠ¡ï¼Œé‡‡ç”¨ä¸åŒçš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œå¯¼è‡´æ¨¡å‹å¤æ‚ä¸”éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„åˆ©ç”¨ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCOSINEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æå–å›¾åƒå’Œå¤šæ¨¡æ€æç¤ºï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰çš„è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„è§£ç å™¨ï¼ˆSegDecoderï¼‰å¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œå¯¹é½å’Œäº¤äº’ï¼Œä»è€Œå®ç°å¯¹ä¸åŒç²’åº¦æç¤ºçš„åˆ†å‰²ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªé€šç”¨çš„è§£ç å™¨æ¥å¤„ç†ä¸åŒç±»å‹çš„æç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCOSINEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€æç¤ºç¼–ç å™¨ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰æå–å›¾åƒå’Œæ–‡æœ¬æç¤ºçš„ç‰¹å¾è¡¨ç¤ºã€‚2) å›¾åƒç¼–ç å™¨ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼ˆä¾‹å¦‚ViTï¼‰æå–è¾“å…¥å›¾åƒçš„ç‰¹å¾è¡¨ç¤ºã€‚3) SegDecoderï¼šä¸€ä¸ªç»Ÿä¸€çš„è§£ç å™¨ï¼Œç”¨äºå¯¹é½å›¾åƒå’Œæç¤ºçš„ç‰¹å¾è¡¨ç¤ºï¼Œå»ºæ¨¡å®ƒä»¬çš„äº¤äº’ï¼Œå¹¶ç”Ÿæˆåˆ†å‰²æ©ç ã€‚SegDecoderæ¥æ”¶å›¾åƒç‰¹å¾å’Œæç¤ºç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾èåˆï¼Œç„¶åä½¿ç”¨å·ç§¯å±‚é¢„æµ‹åˆ†å‰²æ©ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šCOSINEçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶å¤„ç†å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œä¸Šä¸‹æ–‡åˆ†å‰²ä»»åŠ¡ã€‚2) æœ‰æ•ˆåœ°åˆ©ç”¨äº†å¤šæ¨¡æ€æç¤ºï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚3) è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„è§£ç å™¨ï¼ˆSegDecoderï¼‰ï¼Œå¯ä»¥å¤„ç†ä¸åŒç±»å‹çš„æç¤ºï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„åˆ†å‰²æ©ç ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCOSINEé¿å…äº†ä¸ºä¸åŒä»»åŠ¡è®¾è®¡ä¸åŒçš„æ¶æ„ï¼Œç®€åŒ–äº†æ¨¡å‹ï¼Œå¹¶æé«˜äº†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¤šæ¨¡æ€æç¤ºç¼–ç å™¨ä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†é¢„è®­ç»ƒçš„CLIPæ¨¡å‹æ¥æå–æ–‡æœ¬å’Œå›¾åƒæç¤ºçš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨SegDecoderä¸­ï¼Œä½¿ç”¨äº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥èåˆå›¾åƒå’Œæç¤ºçš„ç‰¹å¾ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒåˆ†å‰²æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡å°è¯•äº†ä¸åŒçš„ViTæ¨¡å‹ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œå¹¶è°ƒæ•´äº†SegDecoderçš„å±‚æ•°å’Œé€šé“æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCOSINEåœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œä¸Šä¸‹æ–‡åˆ†å‰²ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨COCOæ•°æ®é›†ä¸Šï¼ŒCOSINEåœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†X%çš„mIoUæå‡ï¼Œåœ¨ä¸Šä¸‹æ–‡åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†Y%çš„mIoUæå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†å¤šæ¨¡æ€æç¤ºçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜ä½¿ç”¨è§†è§‰å’Œæ–‡æœ¬æç¤ºå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚COSINEçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„ä¸»æµæ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

COSINEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ï¼šæ™ºèƒ½å›¾åƒç¼–è¾‘ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ„ŸçŸ¥ã€åŒ»å­¦å›¾åƒåˆ†æç­‰ã€‚é€šè¿‡æä¾›æ–‡æœ¬æˆ–å›¾åƒæç¤ºï¼Œç”¨æˆ·å¯ä»¥è½»æ¾åœ°åˆ†å‰²å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„å›¾åƒç†è§£å’Œå¤„ç†ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæé«˜äº†å¼€æ”¾ä¸–ç•Œåˆ†å‰²çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå„ç§åº”ç”¨åœºæ™¯æä¾›äº†æ›´å¯é çš„æŠ€æœ¯æ”¯æŒã€‚æœªæ¥ï¼ŒCOSINEå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘åˆ†å‰²ã€3Dåœºæ™¯ç†è§£ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we present COSINE, a unified open-world segmentation model that consolidates open-vocabulary segmentation and in-context segmentation with multi-modal prompts (e.g., text and image). COSINE exploits foundation models to extract representations for an input image and corresponding multi-modal prompts, and a SegDecoder to align these representations, model their interaction, and obtain masks specified by input prompts across different granularities. In this way, COSINE overcomes architectural discrepancies, divergent learning objectives, and distinct representation learning strategies of previous pipelines for open-vocabulary segmentation and in-context segmentation. Comprehensive experiments demonstrate that COSINE has significant performance improvements in both open-vocabulary and in-context segmentation tasks. Our exploratory analyses highlight that the synergistic collaboration between using visual and textual prompts leads to significantly improved generalization over single-modality approaches.

