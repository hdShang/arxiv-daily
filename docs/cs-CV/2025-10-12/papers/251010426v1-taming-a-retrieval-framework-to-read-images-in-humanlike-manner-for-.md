---
layout: default
title: Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs
---

# Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10426" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10426v1</a>
  <a href="https://arxiv.org/pdf/2510.10426.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10426v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10426v1', 'Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Suyang Xi, Chenxi Yang, Hong Ding, Yiqing Ni, Catherine C. Liu, Yunhao Liu, Chengqi Zhang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

**å¤‡æ³¨**: 12 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHuLiRAGæ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»è§†è§‰å¤„ç†æ–¹å¼å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰é—®ç­”` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `è§†è§‰ grounding` `äººç±»è§†è§‰æ¨¡æ‹Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨è§†è§‰é—®ç­”ä¸­æ˜“äº§ç”Ÿå¹»è§‰ï¼ŒåŸå› æ˜¯æ–‡æœ¬æŸ¥è¯¢ä¸è§†è§‰ä¿¡æ¯ç¼ºä¹ç²¾ç¡®é”šå®šï¼Œå¯¼è‡´æ¨ç†ä¸å‡†ç¡®ã€‚
2. HuLiRAGæ¡†æ¶æ¨¡æ‹Ÿäººç±»è§†è§‰å¤„ç†æµç¨‹ï¼Œé€šè¿‡â€œwhat-where-reweightâ€çº§è”å®ç°æ›´ç²¾ç»†çš„è§†è§‰ä¿¡æ¯æ£€ç´¢ä¸å¢å¼ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHuLiRAGèƒ½æœ‰æ•ˆæé«˜groundingä¿çœŸåº¦ï¼Œå‡å°‘å¹»è§‰ï¼Œæå‡å¤šæ¨¡æ€é—®ç­”çš„äº‹å®ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ç»†ç²’åº¦è§†è§‰é—®ç­”ä¸­è¡¨ç°ä¸ä½³ï¼Œå¸¸å¸¸äº§ç”Ÿå…³äºç‰©ä½“èº«ä»½ã€ä½ç½®å’Œå…³ç³»çš„å¹»è§‰ï¼Œè¿™æ˜¯å› ä¸ºæ–‡æœ¬æŸ¥è¯¢æ²¡æœ‰æ˜ç¡®åœ°é”šå®šåˆ°è§†è§‰å‚ç…§ç‰©ä¸Šã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å¯ä»¥ç¼“è§£ä¸€äº›é”™è¯¯ï¼Œä½†å®ƒåœ¨æ£€ç´¢å’Œå¢å¼ºå±‚é¢éƒ½æœªèƒ½ä¸äººç±»çš„å¤„ç†æ–¹å¼å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼ŒRAGåªå…³æ³¨å…¨å±€å±‚é¢çš„å›¾åƒä¿¡æ¯ï¼Œç¼ºä¹å±€éƒ¨ç»†èŠ‚ï¼Œå¹¶ä¸”é™åˆ¶äº†å¯¹ç»†ç²’åº¦äº¤äº’çš„æ¨ç†ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Human-Like Retrieval-Augmented Generation (HuLiRAG)ï¼Œè¯¥æ¡†æ¶å°†å¤šæ¨¡æ€æ¨ç†åˆ†é˜¶æ®µè¿›è¡Œï¼Œå½¢æˆä¸€ä¸ªâ€œwhat--where--reweightâ€çš„çº§è”ã€‚é¦–å…ˆé€šè¿‡å¼€æ”¾è¯æ±‡æ£€æµ‹å°†æŸ¥è¯¢é”šå®šåˆ°å€™é€‰å‚ç…§ç‰©(what)ï¼Œç„¶åä½¿ç”¨SAMè¡ç”Ÿçš„æ©ç åœ¨ç©ºé—´ä¸Šè§£æï¼Œä»¥æ¢å¤ç»†ç²’åº¦ç²¾åº¦(where)ï¼Œæœ€åé€šè¿‡å±€éƒ¨å’Œå…¨å±€å¯¹é½ä¹‹é—´çš„æƒè¡¡æ¥è‡ªé€‚åº”åœ°ç¡®å®šä¼˜å…ˆçº§(reweight)ã€‚æ©ç å¼•å¯¼çš„å¾®è°ƒè¿›ä¸€æ­¥å°†ç©ºé—´è¯æ®æ³¨å…¥åˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå°†groundingä»è¢«åŠ¨åå·®è½¬å˜ä¸ºå¯¹ç­”æ¡ˆå…¬å¼åŒ–çš„æ˜¾å¼çº¦æŸã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç±»ä¼¼äººç±»çš„çº§è”æé«˜äº†groundingçš„ä¿çœŸåº¦å’Œäº‹å®ä¸€è‡´æ€§ï¼ŒåŒæ—¶å‡å°‘äº†å¹»è§‰ï¼Œä»è€Œæ¨åŠ¨å¤šæ¨¡æ€é—®ç­”æœç€å¯ä¿¡çš„æ¨ç†æ–¹å‘å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦è§†è§‰é—®ç­”æ—¶ï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå³ç”Ÿæˆä¸å›¾åƒå†…å®¹ä¸ç¬¦çš„ä¿¡æ¯ã€‚ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼ˆRAGï¼‰è™½ç„¶èƒ½ç¼“è§£éƒ¨åˆ†é—®é¢˜ï¼Œä½†å…¶æ£€ç´¢æ–¹å¼ä¸»è¦å…³æ³¨å…¨å±€å›¾åƒä¿¡æ¯ï¼Œå¿½ç•¥äº†å±€éƒ¨ç»†èŠ‚å’Œç»†ç²’åº¦äº¤äº’ï¼Œå¯¼è‡´æ— æ³•åƒäººç±»ä¸€æ ·è¿›è¡Œç²¾ç¡®çš„è§†è§‰æ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHuLiRAGçš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡æ‹Ÿäººç±»çš„è§†è§‰å¤„ç†æ–¹å¼ï¼Œå°†å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šâ€œwhat-where-reweightâ€ã€‚é¦–å…ˆç¡®å®šå›¾åƒä¸­å¯èƒ½ç›¸å…³çš„ç‰©ä½“ï¼ˆwhatï¼‰ï¼Œç„¶åç²¾ç¡®å®šä½è¿™äº›ç‰©ä½“çš„ä½ç½®å’Œè¾¹ç•Œï¼ˆwhereï¼‰ï¼Œæœ€åæ ¹æ®å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯çš„é‡è¦æ€§è°ƒæ•´æƒé‡ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å›¾åƒå†…å®¹ã€‚è¿™ç§åˆ†é˜¶æ®µå¤„ç†çš„æ–¹å¼æ—¨åœ¨æé«˜æ£€ç´¢çš„ç²¾åº¦å’Œç›¸å…³æ€§ï¼Œå‡å°‘å¹»è§‰çš„äº§ç”Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHuLiRAGæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **å¼€æ”¾è¯æ±‡æ£€æµ‹ (What)**ï¼šä½¿ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨è¯†åˆ«å›¾åƒä¸­çš„å€™é€‰å‚ç…§ç‰©ï¼Œå°†æ–‡æœ¬æŸ¥è¯¢ä¸å›¾åƒä¸­çš„ç‰©ä½“è¿›è¡Œåˆæ­¥å…³è”ã€‚2) **ç©ºé—´è§£æ (Where)**ï¼šåˆ©ç”¨SAMï¼ˆSegment Anything Modelï¼‰ç”Ÿæˆå€™é€‰å‚ç…§ç‰©çš„ç²¾ç¡®æ©ç ï¼Œä»è€Œè·å¾—ç‰©ä½“åœ¨å›¾åƒä¸­çš„ç²¾ç¡®å®šä½ä¿¡æ¯ã€‚3) **é‡åŠ æƒ (Reweight)**ï¼šé€šè¿‡æƒè¡¡å±€éƒ¨å’Œå…¨å±€å¯¹é½ç¨‹åº¦ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´ä¸åŒåŒºåŸŸçš„é‡è¦æ€§ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å›¾åƒå†…å®¹ã€‚4) **æ©ç å¼•å¯¼å¾®è°ƒ**ï¼šåˆ©ç”¨ç”Ÿæˆçš„æ©ç ä¿¡æ¯å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œå°†ç©ºé—´ä¿¡æ¯æ˜¾å¼åœ°èå…¥åˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šHuLiRAGçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¨¡æ‹Ÿäººç±»è§†è§‰å¤„ç†çš„â€œwhat-where-reweightâ€çº§è”æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„RAGæ–¹æ³•ç›¸æ¯”ï¼ŒHuLiRAGæ›´åŠ å…³æ³¨å›¾åƒçš„å±€éƒ¨ç»†èŠ‚å’Œç»†ç²’åº¦äº¤äº’ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å°†æ–‡æœ¬æŸ¥è¯¢ä¸è§†è§‰ä¿¡æ¯è¿›è¡Œå…³è”ã€‚æ­¤å¤–ï¼Œæ©ç å¼•å¯¼çš„å¾®è°ƒå°†ç©ºé—´ä¿¡æ¯ä»è¢«åŠ¨åå·®è½¬å˜ä¸ºå¯¹ç­”æ¡ˆç”Ÿæˆçš„æ˜¾å¼çº¦æŸï¼Œè¿›ä¸€æ­¥æé«˜äº†ç”Ÿæˆç»“æœçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨â€œwhatâ€é˜¶æ®µï¼Œä½¿ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨ï¼ˆå¦‚Grounding DINOï¼‰æ¥è¯†åˆ«å›¾åƒä¸­çš„å€™é€‰å‚ç…§ç‰©ã€‚åœ¨â€œwhereâ€é˜¶æ®µï¼Œåˆ©ç”¨SAMç”Ÿæˆå€™é€‰å‚ç…§ç‰©çš„ç²¾ç¡®æ©ç ã€‚åœ¨â€œreweightâ€é˜¶æ®µï¼Œè®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”æƒé‡è°ƒæ•´æœºåˆ¶ï¼Œæ ¹æ®å±€éƒ¨å’Œå…¨å±€å¯¹é½ç¨‹åº¦æ¥è°ƒæ•´ä¸åŒåŒºåŸŸçš„é‡è¦æ€§ã€‚åœ¨æ©ç å¼•å¯¼å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–MLLMçš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆçš„æ©ç ä¿¡æ¯ä½œä¸ºé¢å¤–çš„è¾“å…¥ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHuLiRAGæ¡†æ¶åœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç»†ç²’åº¦è§†è§‰æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚ä¸ä¼ ç»Ÿçš„RAGæ–¹æ³•ç›¸æ¯”ï¼ŒHuLiRAGèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å‡å°‘å¹»è§‰ï¼Œæé«˜groundingçš„ä¿çœŸåº¦å’Œäº‹å®ä¸€è‡´æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºï¼Œè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HuLiRAGæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦ç²¾ç¡®è§†è§‰ç†è§£çš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ã€æœºå™¨äººå¯¼èˆªç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦å’Œå¯é æ€§ï¼Œä½¿å…¶åœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤„ç†æ›´å¤æ‚çš„è§†è§‰åœºæ™¯å’Œä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£å’Œä¸‰ç»´åœºæ™¯ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) often fail in fine-grained visual question answering, producing hallucinations about object identities, positions, and relations because textual queries are not explicitly anchored to visual referents. Retrieval-augmented generation (RAG) alleviates some errors, but it fails to align with human-like processing at both the retrieval and augmentation levels. Specifically, it focuses only on global-level image information but lacks local detail and limits reasoning about fine-grained interactions. To overcome this limitation, we present Human-Like Retrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal reasoning as a ``what--where--reweight'' cascade. Queries are first anchored to candidate referents via open-vocabulary detection (what), then spatially resolved with SAM-derived masks to recover fine-grained precision (where), and adaptively prioritized through the trade-off between local and global alignment (reweight). Mask-guided fine-tuning further injects spatial evidence into the generation process, transforming grounding from a passive bias into an explicit constraint on answer formulation. Extensive experiments demonstrate that this human-like cascade improves grounding fidelity and factual consistency while reducing hallucinations, advancing multimodal question answering toward trustworthy reasoning.

