---
layout: default
title: DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis
---

# DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10650" target="_blank" class="toolbar-btn">arXiv: 2510.10650v1</a>
    <a href="https://arxiv.org/pdf/2510.10650.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10650v1" 
            onclick="toggleFavorite(this, '2510.10650v1', 'DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Peiyin Chen, Zhuowei Yang, Hui Feng, Sheng Jiang, Rui Yan

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-12

**Â§áÊ≥®**: 5 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**DEMOÔºöËß£ËÄ¶ËøêÂä®ÊΩúÂú®ÊµÅÂåπÈÖçÔºåÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÂèØÊéßÁöÑËØ¥ËØù‰∫∫ÂÉèÂêàÊàê**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `ËØ¥ËØù‰∫∫ÂÉèÁîüÊàê` `Èü≥È¢ëÈ©±Âä®` `ËøêÂä®Ëß£ËÄ¶` `ÊµÅÂåπÈÖç` `Transformer` `ËßÜÈ¢ëÂêàÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈü≥È¢ëÈ©±Âä®ÁöÑËØ¥ËØù‰∫∫ÂÉèÁîüÊàêÊñπÊ≥ïÈöæ‰ª•ÂÆûÁé∞Êó∂Èó¥ËøûË¥ØÂíåÁªÜÁ≤íÂ∫¶ÁöÑËøêÂä®ÊéßÂà∂ÔºåÈôêÂà∂‰∫ÜËßÜÈ¢ëÁöÑÁúüÂÆûÊÑüÂíåÂèØÊéßÊÄß„ÄÇ
2. DEMOÈÄöËøáËøêÂä®Ëá™ÁºñÁ†ÅÂô®ÊûÑÂª∫Ëß£ËÄ¶ÁöÑËøêÂä®ÊΩúÂú®Á©∫Èó¥ÔºåÂπ∂Âà©Áî®ÊúÄ‰ºò‰º†ËæìÁöÑÊµÅÂåπÈÖçÁîüÊàêÂπ≥ÊªëÁöÑËøêÂä®ËΩ®ËøπÔºå‰ªéËÄåÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÊéßÂà∂„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDEMOÂú®ËßÜÈ¢ëÁúüÂÆûÊÑü„ÄÅÂîáÈü≥ÂêåÊ≠•ÂíåËøêÂä®‰øùÁúüÂ∫¶ÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Èü≥È¢ëÈ©±Âä®ÁöÑËØ¥ËØù‰∫∫ÂÉèÁîüÊàêÊäÄÊúØÂú®Êâ©Êï£Ê®°ÂûãÁöÑÂü∫Á°Ä‰∏äÂèñÂæó‰∫ÜÂø´ÈÄüËøõÂ±ïÔºå‰ΩÜÁîüÊàêÂÖ∑ÊúâÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÁªÜÁ≤íÂ∫¶ËøêÂä®ÊéßÂà∂ÁöÑËßÜÈ¢ë‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜDEMOÔºå‰∏Ä‰∏™Âü∫‰∫éÊµÅÂåπÈÖçÁöÑÁîüÊàêÊ°ÜÊû∂ÔºåÁî®‰∫éÈü≥È¢ëÈ©±Âä®ÁöÑËØ¥ËØù‰∫∫ÂÉèËßÜÈ¢ëÂêàÊàêÔºåÂÆÉÂèØ‰ª•ÂØπÂò¥ÂîáËøêÂä®„ÄÅÂ§¥ÈÉ®ÂßøÂäøÂíåÁúºÁùõÊ≥®ËßÜËøõË°åËß£ËÄ¶ÂíåÈ´ò‰øùÁúüÊéßÂà∂„ÄÇÊ†∏ÂøÉË¥°ÁåÆÂú®‰∫é‰∏Ä‰∏™ËøêÂä®Ëá™ÁºñÁ†ÅÂô®ÔºåÂÆÉÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÊΩúÂú®Á©∫Èó¥ÔºåÂÖ∂‰∏≠ËøêÂä®Âõ†Á¥†Ë¢´Áã¨Á´ãË°®Á§∫Âπ∂Ëøë‰ººÊ≠£‰∫§Âåñ„ÄÇÂú®Ëøô‰∏™Ëß£ËÄ¶ÁöÑËøêÂä®Á©∫Èó¥‰∏äÔºåÊàë‰ª¨Â∫îÁî®Âü∫‰∫éÊúÄ‰ºò‰º†ËæìÁöÑÊµÅÂåπÈÖçÔºåÂπ∂‰ΩøÁî®TransformerÈ¢ÑÊµãÂô®Êù•ÁîüÊàê‰ª•Èü≥È¢ë‰∏∫Êù°‰ª∂ÁöÑ„ÄÅÊó∂Èó¥‰∏äÂπ≥ÊªëÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåDEMOÂú®ËßÜÈ¢ëÁúüÂÆûÊÑü„ÄÅÂîáÈü≥ÂêåÊ≠•ÂíåËøêÂä®‰øùÁúüÂ∫¶ÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÂ∞ÜÁªÜÁ≤íÂ∫¶ÁöÑËøêÂä®Ëß£ËÄ¶‰∏éÂü∫‰∫éÊµÅÁöÑÁîüÊàêÊ®°ÂûãÁõ∏ÁªìÂêàÔºå‰∏∫ÂèØÊéßÁöÑËØ¥ËØù‰∫∫ÂÉèËßÜÈ¢ëÂêàÊàêÊèê‰æõ‰∫Ü‰∏ÄÁßçÂº∫Â§ßÁöÑÊñ∞ËåÉÂºè„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÈü≥È¢ëÈ©±Âä®ÁöÑËØ¥ËØù‰∫∫ÂÉèÁîüÊàêÊó®Âú®Ê†πÊçÆÁªôÂÆöÁöÑÈü≥È¢ëÁîüÊàêÈÄºÁúüÁöÑËØ¥ËØù‰∫∫ËßÜÈ¢ë„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÁîüÊàêÊó∂Èó¥ËøûË¥ØÁöÑËßÜÈ¢ëÔºåÁâπÂà´ÊòØÂØπÂò¥ÂîáËøêÂä®„ÄÅÂ§¥ÈÉ®ÂßøÂäøÂíåÁúºÁùõÊ≥®ËßÜÁ≠âÁªÜÁ≤íÂ∫¶ËøêÂä®ËøõË°åÁ≤æÁ°ÆÊéßÂà∂ÊñπÈù¢Â≠òÂú®Âõ∞Èöæ„ÄÇËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Èöæ‰ª•Ëß£ËÄ¶‰∏çÂêåÁöÑËøêÂä®Âõ†Á¥†ÔºåÂØºËá¥ÁîüÊàêÁöÑËßÜÈ¢ëÂú®ËøêÂä®‰∏ä‰∏çËá™ÁÑ∂Êàñ‰∏éÈü≥È¢ë‰∏çÂåπÈÖç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDEMOÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËøêÂä®Âõ†Á¥†Ëß£ËÄ¶Âà∞‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ÔºåÁÑ∂ÂêéÂà©Áî®ÊµÅÂåπÈÖçÁîüÊàêÂπ≥ÊªëÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇÈÄöËøáËß£ËÄ¶ËøêÂä®Âõ†Á¥†ÔºåÂèØ‰ª•Áã¨Á´ãÊéßÂà∂Âò¥ÂîáËøêÂä®„ÄÅÂ§¥ÈÉ®ÂßøÂäøÂíåÁúºÁùõÊ≥®ËßÜÔºå‰ªéËÄåÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑËøêÂä®ÊéßÂà∂„ÄÇÊµÅÂåπÈÖçÊñπÊ≥ïËÉΩÂ§üÁîüÊàêÊó∂Èó¥‰∏äÂπ≥ÊªëÁöÑËøêÂä®ËΩ®ËøπÔºå‰ªéËÄåÊèêÈ´òËßÜÈ¢ëÁöÑËøûË¥ØÊÄßÂíåÁúüÂÆûÊÑü„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDEMOÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöËøêÂä®Ëá™ÁºñÁ†ÅÂô®„ÄÅÊµÅÂåπÈÖçÁîüÊàêÂô®ÂíåÊ∏≤ÊüìÂô®„ÄÇËøêÂä®Ëá™ÁºñÁ†ÅÂô®Áî®‰∫éÂ∞ÜËøêÂä®‰ø°ÊÅØÁºñÁ†ÅÂà∞Ëß£ËÄ¶ÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠„ÄÇÊµÅÂåπÈÖçÁîüÊàêÂô®Âü∫‰∫éÈü≥È¢ë‰ø°ÊÅØÁîüÊàêÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇÊ∏≤ÊüìÂô®Â∞ÜËøêÂä®ËΩ®ËøπËß£Á†Å‰∏∫ÊúÄÁªàÁöÑËØ¥ËØù‰∫∫ËßÜÈ¢ë„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDEMOÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éËøêÂä®Ëß£ËÄ¶ÂíåÊµÅÂåπÈÖçÁöÑÁªìÂêà„ÄÇËøêÂä®Ëß£ËÄ¶ÂÖÅËÆ∏Áã¨Á´ãÊéßÂà∂‰∏çÂêåÁöÑËøêÂä®Âõ†Á¥†ÔºåËÄåÊµÅÂåπÈÖç‰øùËØÅ‰∫ÜÁîüÊàêËøêÂä®ËΩ®ËøπÁöÑÊó∂Èó¥Âπ≥ÊªëÊÄß„ÄÇÊ≠§Â§ñÔºåDEMO‰ΩøÁî®TransformerÈ¢ÑÊµãÂô®Êù•Âª∫Ê®°Èü≥È¢ëÂíåËøêÂä®‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºå‰ªéËÄåÊèêÈ´òÂîáÈü≥ÂêåÊ≠•ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËøêÂä®Ëá™ÁºñÁ†ÅÂô®‰ΩøÁî®ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÁªìÊûÑÔºåÂπ∂ÈááÁî®È¢ùÂ§ñÁöÑÊçüÂ§±ÂáΩÊï∞Êù•ÈºìÂä±ÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑËøêÂä®Âõ†Á¥†Ëß£ËÄ¶„ÄÇÊµÅÂåπÈÖçÁîüÊàêÂô®‰ΩøÁî®ÊúÄ‰ºò‰º†ËæìÁêÜËÆ∫Êù•ÂÆö‰πâÊµÅÔºåÂπ∂‰ΩøÁî®TransformerÁΩëÁªúÊù•È¢ÑÊµãÊµÅÁöÑÂêëÈáèÂú∫„ÄÇÊ∏≤ÊüìÂô®ÂèØ‰ª•‰ΩøÁî®Áé∞ÊúâÁöÑÁ•ûÁªèÊ∏≤ÊüìÊäÄÊúØÔºå‰æãÂ¶ÇNeRFÊàñGAN„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DEMOÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂîáÈü≥ÂêåÊ≠•ÊñπÈù¢ÔºåDEMOÁöÑÊåáÊ†á‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïËá≥Â∞ë10%„ÄÇÂú®ËßÜÈ¢ëÁúüÂÆûÊÑüÊñπÈù¢ÔºåÁî®Êà∑ËØÑ‰ª∑DEMOÁîüÊàêÁöÑËßÜÈ¢ëÊõ¥ÈÄºÁúüÔºåËøêÂä®Êõ¥Ëá™ÁÑ∂„ÄÇËøô‰∫õÁªìÊûúËØÅÊòé‰∫ÜDEMOÂú®ÂèØÊéßËØ¥ËØù‰∫∫ÂÉèÂêàÊàêÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DEMOÊäÄÊúØÂèØÂπøÊ≥õÂ∫îÁî®‰∫éËôöÊãüÂΩ¢Ë±°ÂÆöÂà∂„ÄÅÊï∞Â≠óÂÜÖÂÆπÂàõ‰Ωú„ÄÅÂú®Á∫øÊïôËÇ≤„ÄÅËßÜÈ¢ë‰ºöËÆÆÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÁ≤æÁ°ÆÊéßÂà∂ËØ¥ËØù‰∫∫ÂÉèÁöÑÁªÜÂæÆË°®ÊÉÖÂíåÂä®‰ΩúÔºåÂèØ‰ª•ÊèêÂçáÁî®Êà∑‰ΩìÈ™åÔºåÂàõÈÄ†Êõ¥ÂÖ∑Ë°®Áé∞ÂäõÂíå‰∏™ÊÄßÂåñÁöÑÊï∞Â≠óÂÜÖÂÆπ„ÄÇËØ•ÊäÄÊúØËøòÊúâÊΩúÂäõÂ∫îÁî®‰∫é‰∫∫Êú∫‰∫§‰∫íÔºå‰ΩøËôöÊãüÂä©ÊâãÊõ¥ÂÖ∑‰∫∫ÊÉÖÂë≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Audio-driven talking-head generation has advanced rapidly with diffusion-based generative models, yet producing temporally coherent videos with fine-grained motion control remains challenging. We propose DEMO, a flow-matching generative framework for audio-driven talking-portrait video synthesis that delivers disentangled, high-fidelity control of lip motion, head pose, and eye gaze. The core contribution is a motion auto-encoder that builds a structured latent space in which motion factors are independently represented and approximately orthogonalized. On this disentangled motion space, we apply optimal-transport-based flow matching with a transformer predictor to generate temporally smooth motion trajectories conditioned on audio. Extensive experiments across multiple benchmarks show that DEMO outperforms prior methods in video realism, lip-audio synchronization, and motion fidelity. These results demonstrate that combining fine-grained motion disentanglement with flow-based generative modeling provides a powerful new paradigm for controllable talking-head video synthesis.

