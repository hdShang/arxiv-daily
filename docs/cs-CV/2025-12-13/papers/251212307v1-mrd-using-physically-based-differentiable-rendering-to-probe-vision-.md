---
layout: default
title: MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding
---

# MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.12307" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.12307v1</a>
  <a href="https://arxiv.org/pdf/2512.12307.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.12307v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.12307v1', 'MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Benjamin Beilharz, Thomas S. A. Wallis

**åˆ†ç±»**: cs.CV, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-13

**å¤‡æ³¨**: 18 pages, 6 figures. Supplementary material and code will be provided at the end of January

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMRDï¼Œåˆ©ç”¨å¯å¾®æ¸²æŸ“æ¢ç©¶è§†è§‰æ¨¡å‹å¯¹3Dåœºæ™¯çš„ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¯å¾®æ¸²æŸ“` `æ¨¡å‹å¯è§£é‡Šæ€§` `3Dåœºæ™¯ç†è§£` `è§†è§‰æ¨¡å‹` `ç‰©ç†æ¸²æŸ“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰æ¨¡å‹éš¾ä»¥è§£é‡Šå…¶å¯¹3Dåœºæ™¯çš„éšå¼ç†è§£ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ¢ç©¶æ–¹æ³•ã€‚
2. MRDåˆ©ç”¨å¯å¾®æ¸²æŸ“ï¼Œå¯»æ‰¾ç‰©ç†ä¸Šä¸åŒä½†æ¨¡å‹æ¿€æ´»ç›¸åŒçš„3Dåœºæ™¯å‚æ•°ï¼Œä»è€Œæ¢ç©¶æ¨¡å‹å¯¹åœºæ™¯å±æ€§çš„æ•æ„Ÿæ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMRDèƒ½æœ‰æ•ˆé‡å»ºåœºæ™¯å‚æ•°ï¼Œæ­ç¤ºæ¨¡å‹å¯¹å½¢çŠ¶å’Œæè´¨ç­‰å±æ€§çš„å…³æ³¨ç‚¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è®¸å¤šè§†è§‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç†è§£å’Œè§£é‡Šè¿™äº›æ¨¡å‹çš„è¡¨å¾å’Œå†³ç­–ä»ç„¶å¾ˆå›°éš¾ã€‚è™½ç„¶è§†è§‰æ¨¡å‹é€šå¸¸åœ¨2Dè¾“å…¥ä¸Šè®­ç»ƒï¼Œä½†äººä»¬å¸¸å¸¸å‡è®¾å®ƒä»¬å‘å±•äº†å¯¹åº•å±‚3Dåœºæ™¯çš„éšå¼è¡¨å¾ï¼ˆä¾‹å¦‚ï¼Œå¯¹éƒ¨åˆ†é®æŒ¡çš„å®¹å¿åº¦ï¼Œæˆ–æ¨ç†ç›¸å¯¹æ·±åº¦çš„èƒ½åŠ›ï¼‰ã€‚æœ¬æ–‡ä»‹ç»MRDï¼ˆmetamers rendered differentiablyï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºäºç‰©ç†çš„å¯å¾®æ¸²æŸ“æ¥æ¢ç©¶è§†è§‰æ¨¡å‹å¯¹ç”Ÿæˆå¼3Dåœºæ™¯å±æ€§çš„éšå¼ç†è§£ï¼Œé€šè¿‡å¯»æ‰¾åœ¨ç‰©ç†ä¸Šä¸åŒä½†äº§ç”Ÿç›¸åŒæ¨¡å‹æ¿€æ´»çš„3Dåœºæ™¯å‚æ•°ï¼ˆå³æ¨¡å‹åŒåº¦å¼‚æ„ä½“ï¼‰ã€‚ä¸ä¹‹å‰åŸºäºåƒç´ çš„è¯„ä¼°æ¨¡å‹è¡¨å¾çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›é‡å»ºç»“æœå§‹ç»ˆåŸºäºç‰©ç†åœºæ™¯æè¿°ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥æ¢ç©¶æ¨¡å‹å¯¹ç‰©ä½“å½¢çŠ¶çš„æ•æ„Ÿæ€§ï¼ŒåŒæ—¶ä¿æŒæè´¨å’Œå…‰ç…§ä¸å˜ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªæ¨¡å‹æ¢å¤å‡ ä½•å½¢çŠ¶ï¼ˆå½¢çŠ¶ï¼‰å’ŒåŒå‘åå°„åˆ†å¸ƒå‡½æ•°ï¼ˆæè´¨ï¼‰ç­‰åœºæ™¯å‚æ•°çš„èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œç›®æ ‡åœºæ™¯å’Œä¼˜åŒ–åœºæ™¯ä¹‹é—´çš„æ¨¡å‹æ¿€æ´»å…·æœ‰é«˜åº¦ç›¸ä¼¼æ€§ï¼Œä½†è§†è§‰ç»“æœå„ä¸ç›¸åŒã€‚å®šæ€§åœ°ï¼Œè¿™äº›é‡å»ºæœ‰åŠ©äºç ”ç©¶æ¨¡å‹æ•æ„Ÿæˆ–ä¸å˜çš„ç‰©ç†åœºæ™¯å±æ€§ã€‚MRDæœ‰æœ›é€šè¿‡åˆ†æç‰©ç†åœºæ™¯å‚æ•°å¦‚ä½•é©±åŠ¨æ¨¡å‹å“åº”çš„å˜åŒ–ï¼Œä»è€Œä¿ƒè¿›æˆ‘ä»¬å¯¹è®¡ç®—æœºå’Œäººç±»è§†è§‰çš„ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ·±åº¦å­¦ä¹ è§†è§‰æ¨¡å‹åœ¨2Då›¾åƒä¸Šè®­ç»ƒï¼Œè™½ç„¶è¢«è®¤ä¸ºå­¦ä¹ äº†å¯¹3Dåœºæ™¯çš„éšå¼ç†è§£ï¼Œä½†ç¼ºä¹æœ‰æ•ˆçš„æ–¹æ³•æ¥æ¢ç©¶å’Œè§£é‡Šè¿™ç§ç†è§£ã€‚ä¹‹å‰çš„åƒç´ çº§æ–¹æ³•æ— æ³•ä¿è¯é‡å»ºç»“æœçš„ç‰©ç†åˆç†æ€§ï¼Œéš¾ä»¥æ§åˆ¶åœºæ™¯å±æ€§ï¼Œä¾‹å¦‚ç‹¬ç«‹åœ°æ”¹å˜å½¢çŠ¶æˆ–æè´¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMRDçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¯å¾®æ¸²æŸ“æŠ€æœ¯ï¼Œå°†è§†è§‰æ¨¡å‹çš„è¾“å‡ºä¸3Dåœºæ™¯å‚æ•°è”ç³»èµ·æ¥ã€‚é€šè¿‡ä¼˜åŒ–3Dåœºæ™¯å‚æ•°ï¼Œä½¿å¾—æ¸²æŸ“å‡ºçš„å›¾åƒåœ¨è§†è§‰æ¨¡å‹ä¸­äº§ç”Ÿä¸ç›®æ ‡å›¾åƒç›¸ä¼¼çš„æ¿€æ´»ï¼Œä»è€Œåæ¨å‡ºæ¨¡å‹æ‰€â€œçœ‹åˆ°â€çš„3Dåœºæ™¯ã€‚è¿™ç§æ–¹æ³•ä¿è¯äº†é‡å»ºç»“æœçš„ç‰©ç†ä¸€è‡´æ€§ï¼Œå¹¶å…è®¸ç ”ç©¶è€…æ§åˆ¶å’Œæ“çºµåœºæ™¯å±æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMRDçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) é€‰æ‹©ä¸€ä¸ªç›®æ ‡å›¾åƒï¼Œè¾“å…¥åˆ°é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ä¸­ï¼Œæå–ç‰¹å®šå±‚çš„æ¿€æ´»ä½œä¸ºç›®æ ‡æ¿€æ´»ã€‚2) åˆå§‹åŒ–ä¸€ä¸ª3Dåœºæ™¯ï¼ŒåŒ…å«å‡ ä½•å½¢çŠ¶ã€æè´¨å’Œå…‰ç…§ç­‰å‚æ•°ã€‚3) ä½¿ç”¨å¯å¾®æ¸²æŸ“å™¨å°†3Dåœºæ™¯æ¸²æŸ“æˆ2Då›¾åƒã€‚4) å°†æ¸²æŸ“çš„å›¾åƒè¾“å…¥åˆ°ç›¸åŒçš„è§†è§‰æ¨¡å‹ä¸­ï¼Œæå–å¯¹åº”å±‚çš„æ¿€æ´»ã€‚5) è®¡ç®—æ¸²æŸ“å›¾åƒçš„æ¿€æ´»ä¸ç›®æ ‡æ¿€æ´»ä¹‹é—´çš„æŸå¤±å‡½æ•°ã€‚6) ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–ç®—æ³•ï¼Œè°ƒæ•´3Dåœºæ™¯å‚æ•°ï¼Œæœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚7) é‡å¤æ­¥éª¤3-6ï¼Œç›´åˆ°æŸå¤±å‡½æ•°æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMRDçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¯å¾®æ¸²æŸ“æŠ€æœ¯ä¸è§†è§‰æ¨¡å‹çš„è¡¨å¾å­¦ä¹ è”ç³»èµ·æ¥ï¼Œæä¾›äº†ä¸€ç§åŸºäºç‰©ç†çš„ã€å¯è§£é‡Šçš„æ¨¡å‹æ¢ç©¶æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºåƒç´ çš„ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒMRDçš„é‡å»ºç»“æœå…·æœ‰ç‰©ç†åˆç†æ€§ï¼Œå¹¶ä¸”å¯ä»¥æ§åˆ¶åœºæ™¯å±æ€§ï¼Œä¾‹å¦‚ç‹¬ç«‹åœ°æ”¹å˜å½¢çŠ¶æˆ–æè´¨ã€‚

**å…³é”®è®¾è®¡**ï¼šMRDçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨åŸºäºç‰©ç†çš„æ¸²æŸ“å™¨ï¼Œä¿è¯æ¸²æŸ“ç»“æœçš„çœŸå®æ„Ÿå’Œç‰©ç†ä¸€è‡´æ€§ã€‚2) é€‰æ‹©åˆé€‚çš„è§†è§‰æ¨¡å‹å±‚ï¼Œä»¥æå–å…·æœ‰ä»£è¡¨æ€§çš„æ¿€æ´»ã€‚3) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚L2æŸå¤±æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»¥è¡¡é‡æ¿€æ´»ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚4) é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç®—æ³•ï¼Œä¾‹å¦‚Adamæˆ–LBFGSï¼Œä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–3Dåœºæ™¯å‚æ•°ã€‚5) å¯¹3Dåœºæ™¯å‚æ•°è¿›è¡Œåˆç†çš„åˆå§‹åŒ–å’Œçº¦æŸï¼Œä»¥é¿å…ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å¥‡å¼‚å€¼å’Œä¸åˆç†çš„åœºæ™¯é…ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMRDèƒ½å¤Ÿæœ‰æ•ˆåœ°é‡å»º3Dåœºæ™¯å‚æ•°ï¼Œä½¿å¾—é‡å»ºåœºæ™¯åœ¨è§†è§‰æ¨¡å‹ä¸­äº§ç”Ÿä¸ç›®æ ‡åœºæ™¯ç›¸ä¼¼çš„æ¿€æ´»ã€‚è™½ç„¶è§†è§‰æ•ˆæœä¸Šå¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œä½†æ¨¡å‹æ¿€æ´»çš„ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè¡¨æ˜æ¨¡å‹å¯¹æŸäº›ç‰©ç†å±æ€§å…·æœ‰ä¸å˜æ€§ã€‚é€šè¿‡åˆ†æé‡å»ºç»“æœï¼Œå¯ä»¥æ­ç¤ºæ¨¡å‹å¯¹å½¢çŠ¶ã€æè´¨å’Œå…‰ç…§ç­‰å±æ€§çš„æ•æ„Ÿç¨‹åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MRDå¯ç”¨äºåˆ†æå’Œç†è§£è®¡ç®—æœºè§†è§‰æ¨¡å‹çš„å†…éƒ¨è¡¨å¾ï¼Œæ­ç¤ºæ¨¡å‹å¯¹ä¸åŒåœºæ™¯å±æ€§çš„æ•æ„Ÿæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯åº”ç”¨äºå¯¹æŠ—æ ·æœ¬ç”Ÿæˆï¼Œé€šè¿‡æ“çºµ3Dåœºæ™¯å‚æ•°ç”Ÿæˆéš¾ä»¥è¢«æ¨¡å‹è¯†åˆ«çš„å›¾åƒã€‚æœªæ¥ï¼ŒMRDæœ‰æœ›ä¿ƒè¿›è®¡ç®—æœºè§†è§‰å’Œäººç±»è§†è§‰çš„äº¤å‰ç ”ç©¶ï¼Œå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£äººç±»è§†è§‰æ„ŸçŸ¥æœºåˆ¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

