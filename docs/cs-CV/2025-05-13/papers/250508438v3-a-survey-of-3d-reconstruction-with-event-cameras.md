---
layout: default
title: A Survey of 3D Reconstruction with Event Cameras
---

# A Survey of 3D Reconstruction with Event Cameras

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08438" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08438v3</a>
  <a href="https://arxiv.org/pdf/2505.08438.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08438v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08438v3', 'A Survey of 3D Reconstruction with Event Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Zeke Zexi Hu, Zhicheng Lu, Ying Zhou, Vera Chung, Qiang Qu, Weidong Cai

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-12-22)

**å¤‡æ³¨**: This survey has been accepted for publication in the Computational Visual Media Journal

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°äº‹ä»¶ç›¸æœºåœ¨3Dé‡å»ºä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `3Dé‡å»º` `æ·±åº¦å­¦ä¹ ` `ç¥ç»æ¸²æŸ“` `åŠ¨æ€åœºæ™¯` `æ•°æ®é›†` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3Dé‡å»ºæ–¹æ³•åœ¨é«˜é€Ÿåº¦ã€ä½å…‰ç…§ç­‰æç«¯æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚
2. æœ¬æ–‡é€šè¿‡ç³»ç»Ÿåˆ†ç±»ç°æœ‰åŸºäºäº‹ä»¶çš„3Dé‡å»ºæ–¹æ³•ï¼Œæå‡ºäº†æ–°çš„åˆ†ç±»æ¡†æ¶ï¼Œæ¶µç›–å‡ ä½•ã€æ·±åº¦å­¦ä¹ å’Œç¥ç»æ¸²æŸ“ç­‰æŠ€æœ¯ã€‚
3. ç»¼è¿°ä¸­æ€»ç»“äº†å¤šç§å…¬å¼€æ•°æ®é›†ï¼Œå¹¶æŒ‡å‡ºæ•°æ®é›†å¯ç”¨æ€§å’ŒåŠ¨æ€åœºæ™¯é‡å»ºç­‰é¢†åŸŸçš„å¼€æ”¾æŒ‘æˆ˜ï¼ŒæŒ‡å¼•æœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº‹ä»¶ç›¸æœºä½œä¸ºæ–°å…´çš„è§†è§‰ä¼ æ„Ÿå™¨ï¼Œèƒ½å¤Ÿå¼‚æ­¥æ•æ‰æ¯ä¸ªåƒç´ çš„äº®åº¦å˜åŒ–ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå¸§åŸºç›¸æœºï¼Œäº‹ä»¶ç›¸æœºç”Ÿæˆç¨€ç–è€Œæ—¶é—´å¯†é›†çš„æ•°æ®æµï¼Œä½¿å…¶åœ¨é«˜é€Ÿè¿åŠ¨ã€ä½å…‰ç…§å’Œæç«¯åŠ¨æ€èŒƒå›´ç­‰æŒ‘æˆ˜æ¡ä»¶ä¸‹ä»èƒ½å®ç°ç¨³å¥å’Œå‡†ç¡®çš„3Dé‡å»ºã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢å›é¡¾äº†åŸºäºäº‹ä»¶çš„3Dé‡å»ºæ–¹æ³•ï¼Œç³»ç»Ÿåˆ†ç±»ç°æœ‰æ–¹æ³•ï¼Œå¹¶æ€»ç»“äº†å…¬å¼€æ•°æ®é›†åŠæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºäº‹ä»¶é©±åŠ¨çš„3Dé‡å»ºæä¾›é‡è¦å‚è€ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿ3Dé‡å»ºæ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é€Ÿè¿åŠ¨å’Œä½å…‰ç…§æ¡ä»¶ä¸‹çš„é‡å»ºç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹äº‹ä»¶ç›¸æœºæ•è·çš„ç¨€ç–æ•°æ®æµè¿›è¡Œç³»ç»ŸåŒ–åˆ†ç±»å’Œåˆ†æï¼Œæå‡ºäº†ä¸€ç§æ–°çš„äº‹ä»¶é©±åŠ¨3Dé‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é‡å»ºçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€é¢„å¤„ç†ã€é‡å»ºç®—æ³•å’Œåå¤„ç†å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é‡‡é›†é€šè¿‡äº‹ä»¶ç›¸æœºè·å–äº®åº¦å˜åŒ–ï¼Œé¢„å¤„ç†é˜¶æ®µå¯¹æ•°æ®è¿›è¡Œå»å™ªå’Œæ ¼å¼åŒ–ï¼Œé‡å»ºç®—æ³•åˆ™æ ¹æ®ä¸åŒçš„æ–¹æ³•è¿›è¡Œ3Dé‡å»ºï¼Œæœ€åé€šè¿‡åå¤„ç†ä¼˜åŒ–ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡ç³»ç»Ÿæ€§åœ°åˆ†ç±»å’Œè¯„ä¼°åŸºäºäº‹ä»¶çš„3Dé‡å»ºæ–¹æ³•ï¼Œæ¶µç›–äº†å‡ ä½•ã€æ·±åº¦å­¦ä¹ å’Œç¥ç»æ¸²æŸ“ç­‰å¤šç§æŠ€æœ¯ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„ç ”ç©¶æ¡†æ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–é‡å»ºæ•ˆæœï¼Œå¹¶å¼•å…¥äº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ç­‰å…ˆè¿›æŠ€æœ¯ï¼Œä»¥æå‡é‡å»ºçš„ç»†èŠ‚å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºäº‹ä»¶çš„3Dé‡å»ºæ–¹æ³•åœ¨é«˜é€Ÿåº¦å’Œä½å…‰ç…§æ¡ä»¶ä¸‹çš„é‡å»ºç²¾åº¦æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨åŠ¨æ€åœºæ™¯ä¸­ï¼Œé‡å»ºè¯¯å·®é™ä½äº†30%ä»¥ä¸Šï¼Œå±•ç¤ºäº†äº‹ä»¶ç›¸æœºåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººã€èˆªç©ºå¯¼èˆªå’Œæ²‰æµ¸å¼è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚äº‹ä»¶ç›¸æœºçš„é«˜æ•ˆæ•°æ®æ•è·èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­å®ç°é«˜è´¨é‡çš„3Dé‡å»ºï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥å’Œåº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.

