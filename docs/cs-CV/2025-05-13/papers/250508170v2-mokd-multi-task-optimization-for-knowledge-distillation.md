---
layout: default
title: MoKD: Multi-Task Optimization for Knowledge Distillation
---

# MoKD: Multi-Task Optimization for Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08170" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08170v2</a>
  <a href="https://arxiv.org/pdf/2505.08170.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08170v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08170v2', 'MoKD: Multi-Task Optimization for Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeeshan Hayder, Ali Cheraghian, Lars Petersson, Mehrtash Harandi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-08-02)

**å¤‡æ³¨**: Major changes to the paper and the authorship

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoKDä»¥è§£å†³çŸ¥è¯†è’¸é¦ä¸­çš„æ¢¯åº¦å†²çªä¸ä¸»å¯¼é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¤šä»»åŠ¡ä¼˜åŒ–` `æ¢¯åº¦å†²çª` `æ¨¡å‹å‹ç¼©` `è®¡ç®—æœºè§†è§‰` `é«˜æ•ˆå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨æ•™å¸ˆæŒ‡å¯¼ä¸ä»»åŠ¡ç›®æ ‡ä¹‹é—´çš„å­¦ä¹ å¹³è¡¡ä»¥åŠçŸ¥è¯†è¡¨ç¤ºå·®å¼‚å¤„ç†ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºMoKDï¼Œé€šè¿‡å°†çŸ¥è¯†è’¸é¦è§†ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œè§£å†³æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜ï¼Œå®ç°æ›´å¥½çš„ç›®æ ‡å¹³è¡¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMoKDåœ¨å›¾åƒåˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç´§å‡‘æ¨¡å‹å¯ä»¥é€šè¿‡çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æœ‰æ•ˆè®­ç»ƒï¼Œè¯¥æŠ€æœ¯å°†çŸ¥è¯†ä»å¤§å‹é«˜æ€§èƒ½æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ã€‚çŸ¥è¯†è’¸é¦é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯å¹³è¡¡æ•™å¸ˆæŒ‡å¯¼ä¸ä»»åŠ¡ç›®æ ‡çš„å­¦ä¹ ï¼ŒäºŒæ˜¯å¤„ç†æ•™å¸ˆä¸å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çŸ¥è¯†è¡¨ç¤ºçš„å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å¤šä»»åŠ¡ä¼˜åŒ–çŸ¥è¯†è’¸é¦ï¼ˆMoKDï¼‰ï¼Œè¯¥æ–¹æ³•å°†KDé‡æ–°æ„å»ºä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œè§£å†³äº†æ¢¯åº¦å†²çªå’Œæ¢¯åº¦ä¸»å¯¼é—®é¢˜ã€‚MoKDè¿˜å¼•å…¥äº†å­ç©ºé—´å­¦ä¹ æ¡†æ¶ï¼Œå°†ç‰¹å¾è¡¨ç¤ºæŠ•å½±åˆ°é«˜ç»´ç©ºé—´ï¼Œä»è€Œæ”¹å–„çŸ¥è¯†è½¬ç§»ã€‚é€šè¿‡åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„å›¾åƒåˆ†ç±»å’ŒCOCOæ•°æ®é›†ä¸Šçš„ç›®æ ‡æ£€æµ‹è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼ŒMoKDæ˜¾ç¤ºå‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œä¸”åœ¨æ•ˆç‡ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­æ•™å¸ˆæ¨¡å‹ä¸å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»ä¸å¹³è¡¡é—®é¢˜ï¼Œå°¤å…¶æ˜¯æ¢¯åº¦å†²çªå’Œä¸»å¯¼ç°è±¡ï¼Œè¿™äº›é—®é¢˜ä¼šå½±å“æ¨¡å‹çš„å­¦ä¹ æ•ˆæœå’Œæœ€ç»ˆæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoKDé€šè¿‡å°†çŸ¥è¯†è’¸é¦é‡æ–°æ„å»ºä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨å¹³è¡¡ä»»åŠ¡ç›®æ ‡ä¸æ•™å¸ˆæŒ‡å¯¼ä¹‹é—´çš„å­¦ä¹ ï¼Œé¿å…æ¢¯åº¦å†²çªå’Œä¸»å¯¼ç°è±¡ï¼Œä»è€Œæé«˜çŸ¥è¯†è½¬ç§»çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoKDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯å¤šç›®æ ‡ä¼˜åŒ–æ¨¡å—ï¼Œå¤„ç†ä»»åŠ¡ç‰¹å®šå’Œè’¸é¦æ¢¯åº¦çš„å¹³è¡¡ï¼›äºŒæ˜¯å­ç©ºé—´å­¦ä¹ æ¡†æ¶ï¼Œå°†ç‰¹å¾è¡¨ç¤ºæ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œä»¥å¢å¼ºçŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoKDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†çŸ¥è¯†è’¸é¦è§†ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å­˜åœ¨çš„æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜ï¼Œä»è€Œå®ç°äº†æ›´é«˜æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMoKDé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä¸åŒç›®æ ‡çš„æ¢¯åº¦ï¼Œå¹¶é€šè¿‡é«˜ç»´ç‰¹å¾ç©ºé—´çš„æŠ•å½±æ¥ä¼˜åŒ–çŸ¥è¯†è¡¨ç¤ºï¼Œç¡®ä¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¸æ”¶æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ImageNet-1Kæ•°æ®é›†å’ŒCOCOæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoKDåœ¨å›¾åƒåˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œå…·ä½“æ•°æ®æœªæä¾›ï¼Œä½†æ•´ä½“æ•ˆç‡å’Œæ•ˆæœå‡ä¼˜äºä»å¤´è®­ç»ƒçš„æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ¨¡å‹éƒ¨ç½²çš„åœºæ™¯ä¸­ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ã€‚MoKDçš„é«˜æ•ˆçŸ¥è¯†è½¬ç§»èƒ½åŠ›å°†æ¨åŠ¨ç´§å‡‘æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„æ€§èƒ½ä¸å“åº”é€Ÿåº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Compact models can be effectively trained through Knowledge Distillation (KD), a technique that transfers knowledge from larger, high-performing teacher models. Two key challenges in Knowledge Distillation (KD) are: 1) balancing learning from the teacher's guidance and the task objective, and 2) handling the disparity in knowledge representation between teacher and student models. To address these, we propose Multi-Task Optimization for Knowledge Distillation (MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where task-specific and distillation gradients are misaligned, and b) Gradient Dominance, where one objective's gradient dominates, causing imbalance. MoKD reformulates KD as a multi-objective optimization problem, enabling better balance between objectives. Additionally, it introduces a subspace learning framework to project feature representations into a high-dimensional space, improving knowledge transfer. Our MoKD is demonstrated to outperform existing methods through extensive experiments on image classification using the ImageNet-1K dataset and object detection using the COCO dataset, achieving state-of-the-art performance with greater efficiency. To the best of our knowledge, MoKD models also achieve state-of-the-art performance compared to models trained from scratch.

