---
layout: default
title: Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training
---

# Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08971" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08971v1</a>
  <a href="https://arxiv.org/pdf/2505.08971.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08971v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08971v1', 'Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yangyi Chen, Hao Peng, Tong Zhang, Heng Ji

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: The code will be available at https://github.com/Yangyi-Chen/PRIOR

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPRIORä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å™ªå£°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹` `é‡è¦æ€§é‡‡æ ·` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–` `å›¾åƒæè¿°ç”Ÿæˆ` `å¹»è§‰é—®é¢˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®¹æ˜“å—åˆ°ä¸è§†è§‰å†…å®¹æ— å…³çš„å™ªå£°å½±å“ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºçš„PRIORæ–¹æ³•é€šè¿‡å¯¹å›¾åƒç›¸å…³æ ‡è®°è¿›è¡Œå·®å¼‚åŠ æƒï¼Œä¼˜åŒ–äº†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„æŸå¤±å‡½æ•°ï¼Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPRIORåœ¨å¤šä¸ªè§†è§‰è¯­è¨€åŸºå‡†ä¸Šç›¸è¾ƒäºNTPåˆ†åˆ«æå‡äº†19%å’Œ8%çš„æ€§èƒ½ï¼Œä¸”å…·æœ‰æ›´å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ ‡å‡†çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é¢„è®­ç»ƒä¸­ï¼Œæ¨¡å‹é€šå¸¸é€šè¿‡ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼ˆNTPï¼‰æœ€å¤§åŒ–å›¾åƒæ¡ä»¶ä¸‹çš„å­—å¹•è”åˆæ¦‚ç‡ã€‚ç„¶è€Œï¼Œç”±äºåªæœ‰ä¸€å°éƒ¨åˆ†å­—å¹•æ ‡è®°ä¸è§†è§‰å†…å®¹ç›´æ¥ç›¸å…³ï¼Œè¿™ç§ç®€å•çš„NTPæ— æ„ä¸­ä½¿æ¨¡å‹é€‚åº”å™ªå£°ï¼Œå¢åŠ äº†å¹»è§‰çš„é£é™©ã€‚æˆ‘ä»¬æå‡ºäº†PRIORï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨NTPæŸå¤±ä¸­å¯¹å›¾åƒç›¸å…³æ ‡è®°è¿›è¡Œå·®å¼‚åŠ æƒæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚PRIORå¼•å…¥äº†ä¸€ä¸ªå‚è€ƒæ¨¡å‹â€”â€”ä¸€ä¸ªä»…åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºæ ¹æ®å…¶åœ¨æ²¡æœ‰å›¾åƒè¾“å…¥çš„æƒ…å†µä¸‹çš„æ¦‚ç‡æ¥åŠ æƒæ¯ä¸ªæ ‡è®°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç›´æ¥ä¸è§†è§‰è¾“å…¥ç›¸å…³çš„æ ‡è®°åœ¨æ²¡æœ‰å›¾åƒçš„æƒ…å†µä¸‹æ›´éš¾é¢„æµ‹ï¼Œå› æ­¤ä»æ–‡æœ¬å‚è€ƒLLMè·å¾—çš„æ¦‚ç‡è¾ƒä½ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„è®¾ç½®ä¸­å®ç°äº†PRIORï¼Œå¹¶åœ¨å¤šä¸ªè§†è§‰è¯­è¨€åŸºå‡†ä¸Šè§‚å¯Ÿåˆ°ç›¸è¾ƒäºNTPåˆ†åˆ«æœ‰19%å’Œ8%çš„å¹³å‡ç›¸å¯¹æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒä¸­å®¹æ˜“å—åˆ°ä¸è§†è§‰å†…å®¹æ— å…³çš„å™ªå£°å½±å“ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ç”Ÿæˆå­—å¹•æ—¶å‡ºç°å¹»è§‰ç°è±¡ã€‚ç°æœ‰çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼ˆNTPï¼‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåŒºåˆ†ä¸å›¾åƒå†…å®¹ç›¸å…³çš„æ ‡è®°å’Œæ— å…³æ ‡è®°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPRIORæ–¹æ³•çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–‡æœ¬å‚è€ƒæ¨¡å‹ï¼Œå¯¹æ¯ä¸ªæ ‡è®°è¿›è¡Œå·®å¼‚åŠ æƒï¼Œä»è€Œåœ¨æŸå¤±å‡½æ•°ä¸­ä¼˜å…ˆè€ƒè™‘ä¸å›¾åƒç›¸å…³çš„æ ‡è®°ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ å…³æ³¨é‡è¦çš„è§†è§‰ä¿¡æ¯ï¼Œå‡å°‘äº†å™ªå£°çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPRIORçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€ä¸ªæ˜¯å›¾åƒè¾“å…¥çš„è§†è§‰ç¼–ç å™¨ï¼Œå¦ä¸€ä¸ªæ˜¯æ–‡æœ¬å‚è€ƒæ¨¡å‹ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ ¹æ®æ–‡æœ¬å‚è€ƒæ¨¡å‹çš„è¾“å‡ºå¯¹æ¯ä¸ªæ ‡è®°çš„æŸå¤±è¿›è¡Œè°ƒæ•´ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šPRIORçš„æœ€é‡è¦æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡é‡è¦æ€§é‡‡æ ·æ¡†æ¶å¼•å…¥äº†æ ‡è®°çš„å·®å¼‚åŠ æƒæœºåˆ¶ã€‚è¿™ä¸€æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¯†åˆ«ä¸è§†è§‰å†…å®¹ç›¸å…³çš„æ ‡è®°ï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå­—å¹•çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­ï¼ŒPRIORå¼•å…¥äº†ä¸€ä¸ªæ ‡è®°ç‰¹å®šçš„é‡åŠ æƒé¡¹ï¼ŒåŸºäºæ–‡æœ¬å‚è€ƒæ¨¡å‹çš„æ¦‚ç‡å¯¹æ¯ä¸ªæ ‡è®°è¿›è¡ŒåŠ æƒã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨ä¸åŒè®¾ç½®ä¸‹çš„å®ç°ä¹Ÿå±•ç¤ºäº†å…¶çµæ´»æ€§ï¼ŒåŒ…æ‹¬æœ‰æ— è§†è§‰ç¼–ç å™¨çš„æƒ…å†µã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPRIORåœ¨å¤šä¸ªè§†è§‰è¯­è¨€åŸºå‡†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿçš„NTPæ–¹æ³•åˆ†åˆ«å®ç°äº†19%å’Œ8%çš„å¹³å‡ç›¸å¯¹æå‡ã€‚æ­¤å¤–ï¼ŒPRIORåœ¨æ‰©å±•æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾ç¤ºå‡ºåœ¨è®¡ç®—å’Œæ•°æ®å¢åŠ æ—¶æ›´å¤§çš„æ€§èƒ½æå‡æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”å’Œå¤šæ¨¡æ€å†…å®¹æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ï¼ŒPRIORèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´ä¸ºå¯é çš„ç»“æœï¼Œæ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰æŠ€æœ¯çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination. We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework. PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training. Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM. During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders. We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data.

