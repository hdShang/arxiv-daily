---
layout: default
title: Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World
---

# Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08607" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08607v1</a>
  <a href="https://arxiv.org/pdf/2505.08607.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08607v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08607v1', 'Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuran Wang, Yingping Liang, Ying Fu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBooSTerä»¥è§£å†³çœŸå®ä¸–ç•Œä¸­é›¶-shotç«‹ä½“åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç«‹ä½“åŒ¹é…` `æ·±åº¦ä¼°è®¡` `çŸ¥è¯†è½¬ç§»` `è§†è§‰åŸºç¡€æ¨¡å‹` `æ•°æ®ç”Ÿæˆ` `ä¼ªæ ‡ç­¾` `åŠ¨æ€æŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç«‹ä½“åŒ¹é…æ–¹æ³•ä¾èµ–äºå¯†é›†çš„åƒç´ çº§æ ‡ç­¾ï¼Œè·å–è¿™äº›æ ‡ç­¾åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸­éå¸¸å›°éš¾ï¼Œå¯¼è‡´æ•°æ®ç¨€ç¼ºå’Œé¢†åŸŸå·®è·é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„BooSTeræ¡†æ¶ç»“åˆäº†å•ç›®æ·±åº¦ä¼°è®¡å’Œæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»å•è§†å›¾å›¾åƒç”Ÿæˆå¯†é›†çš„ç«‹ä½“åŒ¹é…æ•°æ®ï¼Œå¹¶åˆ©ç”¨ä¼ªæ ‡ç­¾å¢å¼ºç›‘ç£ã€‚
3. åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBooSTeråœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç«‹ä½“åŒ¹é…æ–¹æ³•ä¾èµ–äºå¯†é›†çš„åƒç´ çº§çœŸå®æ ‡ç­¾ï¼Œè¿™åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸­è·å–éå¸¸å›°éš¾ã€‚ç¼ºä¹æ ‡æ³¨æ•°æ®ä»¥åŠåˆæˆä¸çœŸå®å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ä¹Ÿå¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶BooSTerï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹å’Œå¤§è§„æ¨¡æ··åˆå›¾åƒæºï¼ŒåŒ…æ‹¬åˆæˆå›¾åƒã€çœŸå®å›¾åƒå’Œå•è§†å›¾å›¾åƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ•°æ®ç”Ÿæˆç­–ç•¥ï¼Œç»“åˆå•ç›®æ·±åº¦ä¼°è®¡å’Œæ‰©æ•£æ¨¡å‹ï¼Œä»å•è§†å›¾å›¾åƒç”Ÿæˆå¯†é›†çš„ç«‹ä½“åŒ¹é…æ•°æ®ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸­ç¨€ç–æ ‡ç­¾çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä»å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ä¸­è½¬ç§»çŸ¥è¯†ï¼Œä½¿ç”¨ä¼ªå•ç›®æ·±åº¦æ ‡ç­¾å’ŒåŠ¨æ€å°ºåº¦åŠä½ç§»ä¸å˜æŸå¤±è¿›è¡Œé¢å¤–ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºç¼–ç å™¨ï¼Œä»¥æå–ç¨³å¥ä¸”å¯è¿ç§»çš„ç‰¹å¾ï¼Œä»è€Œæå‡å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®æœ‰é™å’Œé¢†åŸŸè½¬ç§»çš„åœºæ™¯ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç«‹ä½“åŒ¹é…ä¸­å¯¹å¯†é›†åƒç´ çº§çœŸå®æ ‡ç­¾çš„ä¾èµ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šé¢ä¸´æ ‡æ³¨ç¨€ç¼ºå’Œé¢†åŸŸå·®è·çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºBooSTeræ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå•ç›®æ·±åº¦ä¼°è®¡å’Œæ‰©æ•£æ¨¡å‹ï¼Œä»å•è§†å›¾å›¾åƒç”Ÿæˆå¯†é›†ç«‹ä½“åŒ¹é…æ•°æ®ï¼ŒåŒæ—¶åˆ©ç”¨ä¼ªå•ç›®æ·±åº¦æ ‡ç­¾è¿›è¡ŒçŸ¥è¯†è½¬ç§»ï¼Œå¢å¼ºç›‘ç£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBooSTeræ¡†æ¶ä¸»è¦åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—ã€çŸ¥è¯†è½¬ç§»æ¨¡å—å’Œç‰¹å¾æå–æ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—è´Ÿè´£ä»å•è§†å›¾å›¾åƒç”Ÿæˆç«‹ä½“åŒ¹é…æ•°æ®ï¼ŒçŸ¥è¯†è½¬ç§»æ¨¡å—åˆ©ç”¨ä¼ªæ ‡ç­¾è¿›è¡Œé¢å¤–ç›‘ç£ï¼Œç‰¹å¾æå–æ¨¡å—åˆ™ä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æå–ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å•ç›®æ·±åº¦ä¼°è®¡ä¸æ‰©æ•£æ¨¡å‹çš„ç»“åˆï¼ŒæˆåŠŸç”Ÿæˆå¯†é›†çš„ç«‹ä½“åŒ¹é…æ•°æ®ï¼Œå¹¶é€šè¿‡åŠ¨æ€æŸå¤±å‡½æ•°æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨åŠ¨æ€å°ºåº¦å’Œä½ç§»ä¸å˜æŸå¤±ï¼Œä»¥é€‚åº”ä¸åŒåœºæ™¯ä¸‹çš„ç¨€ç–æ ‡ç­¾é—®é¢˜ï¼›ç½‘ç»œç»“æ„ä¸Šï¼Œä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºç¼–ç å™¨ï¼Œç¡®ä¿æå–çš„ç‰¹å¾å…·æœ‰ç¨³å¥æ€§å’Œå¯è¿ç§»æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBooSTeråœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†ç«‹ä½“åŒ¹é…çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®æ€§æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ä¸»æµæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ã€è™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿåœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹æå‡ç«‹ä½“åŒ¹é…çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼ŒBooSTeræ¡†æ¶æœ‰æœ›æ¨åŠ¨æ›´å¤šå®é™…åº”ç”¨çš„å‘å±•ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„è§†è§‰ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Stereo matching methods rely on dense pixel-wise ground truth labels, which are laborious to obtain, especially for real-world datasets. The scarcity of labeled data and domain gaps between synthetic and real-world images also pose notable challenges. In this paper, we propose a novel framework, \textbf{BooSTer}, that leverages both vision foundation models and large-scale mixed image sources, including synthetic, real, and single-view images. First, to fully unleash the potential of large-scale single-view images, we design a data generation strategy combining monocular depth estimation and diffusion models to generate dense stereo matching data from single-view images. Second, to tackle sparse labels in real-world datasets, we transfer knowledge from monocular depth estimation models, using pseudo-mono depth labels and a dynamic scale- and shift-invariant loss for additional supervision. Furthermore, we incorporate vision foundation model as an encoder to extract robust and transferable features, boosting accuracy and generalization. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, achieving significant improvements in accuracy over existing methods, particularly in scenarios with limited labeled data and domain shifts.

