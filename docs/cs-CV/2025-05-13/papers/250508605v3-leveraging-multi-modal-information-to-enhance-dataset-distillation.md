---
layout: default
title: Leveraging Multi-Modal Information to Enhance Dataset Distillation
---

# Leveraging Multi-Modal Information to Enhance Dataset Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08605" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08605v3</a>
  <a href="https://arxiv.org/pdf/2505.08605.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08605v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08605v3', 'Leveraging Multi-Modal Information to Enhance Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhe Li, Hadrien Reynaud, Bernhard Kainz

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-12-08)

**å¤‡æ³¨**: Accepted at BMVC Workshop (Privacy, Fairness, Accountability and Transparency in Computer Vision)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ•°æ®è’¸é¦æ¡†æ¶ä»¥æå‡æ•°æ®é›†è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `å¤šæ¨¡æ€ä¿¡æ¯` `éšç§ä¿æŠ¤` `å¯¹è±¡ä¸­å¿ƒå­¦ä¹ ` `åˆæˆæ•°æ®ç”Ÿæˆ` `è®¡ç®—æœºè§†è§‰` `æ ‡é¢˜åŒ¹é…` `ç‰¹å¾å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°æ®é›†è’¸é¦æ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰ä¿¡æ¯çš„ä¼˜åŒ–ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½æå‡æœ‰é™ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡å¼•å…¥æ–‡æœ¬ä¿¡æ¯å’Œå¯¹è±¡ä¸­å¿ƒæ©è†œï¼Œå¢å¼ºäº†æ•°æ®é›†è’¸é¦çš„æ•ˆæœï¼Œä¿ƒè¿›äº†åˆæˆæ•°æ®çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒåŒæ—¶æœ‰æ•ˆä¿æŠ¤äº†éšç§ï¼Œå‡å°‘äº†å¯¹çœŸå®æ•°æ®çš„ä¾èµ–ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®é›†è’¸é¦æ—¨åœ¨åˆ›å»ºä¸€ä¸ªå°å‹ä¸”é«˜åº¦ä»£è¡¨æ€§çš„åˆæˆæ•°æ®é›†ï¼Œä»¥ä¿ç•™å¤§å‹çœŸå®æ•°æ®é›†çš„å…³é”®ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºä¼˜åŒ–è§†è§‰è¡¨ç¤ºï¼Œå¿½è§†äº†å¤šæ¨¡æ€ä¿¡æ¯çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ•°æ®è’¸é¦æ¡†æ¶ï¼Œç»“åˆäº†ä¸¤é¡¹å…³é”®å¢å¼ºï¼šåŸºäºæ ‡é¢˜çš„ç›‘ç£å’Œå¯¹è±¡ä¸­å¿ƒæ©è†œã€‚é€šè¿‡å¼•å…¥æ ‡é¢˜è¿æ¥å’Œæ ‡é¢˜åŒ¹é…ç­–ç•¥ï¼Œåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯ä¿ƒè¿›çœŸå®ä¸åˆæˆæ•°æ®çš„è¯­ä¹‰å¯¹é½ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨åˆ†å‰²æ©è†œéš”ç¦»ç›®æ ‡å¯¹è±¡ï¼Œå¹¶å¼•å…¥äº†æ©è†œç‰¹å¾å¯¹é½å’Œæ©è†œæ¢¯åº¦åŒ¹é…ä¸¤ç§æ–°æŸå¤±ï¼Œæ—¨åœ¨ä¿ƒè¿›å¯¹è±¡ä¸­å¿ƒå­¦ä¹ ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜ä¸‹æ¸¸æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†å¯¹çœŸå®æ•°æ®çš„æš´éœ²ï¼Œä»è€Œå¢å¼ºäº†éšç§ä¿æŠ¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†è’¸é¦æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„é—®é¢˜ï¼Œå¯¼è‡´åˆæˆæ•°æ®çš„è¡¨ç°ä¸ä½³å’Œéšç§ä¿æŠ¤ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ–‡æœ¬ä¿¡æ¯å’Œå¯¹è±¡ä¸­å¿ƒæ©è†œï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°çš„æ•°æ®è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨æå‡åˆæˆæ•°æ®çš„ä»£è¡¨æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŸºäºæ ‡é¢˜çš„ç›‘ç£æ¨¡å—å’Œå¯¹è±¡ä¸­å¿ƒæ©è†œæ¨¡å—ã€‚å‰è€…é€šè¿‡æ ‡é¢˜è¿æ¥å’Œæ ‡é¢˜åŒ¹é…ç­–ç•¥æ¥å¢å¼ºè§†è§‰ç‰¹å¾ï¼Œåè€…é€šè¿‡åˆ†å‰²æ©è†œæ¥éš”ç¦»ç›®æ ‡å¯¹è±¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºç»“åˆäº†æ–‡æœ¬ä¿¡æ¯ä¸è§†è§‰ç‰¹å¾çš„èåˆï¼Œé‡‡ç”¨äº†æ©è†œç‰¹å¾å¯¹é½å’Œæ©è†œæ¢¯åº¦åŒ¹é…æŸå¤±ï¼Œä¿ƒè¿›äº†å¯¹è±¡ä¸­å¿ƒå­¦ä¹ ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†åˆæˆæ•°æ®çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œæœ¬æ–‡å¼•å…¥äº†åŸºäºæ ‡é¢˜çš„æŸå¤±æ¥ä¿ƒè¿›çœŸå®ä¸åˆæˆæ•°æ®çš„è¯­ä¹‰å¯¹é½ï¼ŒåŒæ—¶é‡‡ç”¨äº†æ©è†œç‰¹å¾å¯¹é½å’Œæ©è†œæ¢¯åº¦åŒ¹é…æŸå¤±ï¼Œä»¥å¢å¼ºå¯¹è±¡ä¸­å¿ƒçš„å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚å°¤å…¶åœ¨éšç§ä¿æŠ¤æ–¹é¢ï¼Œå‡å°‘äº†å¯¹çœŸå®æ•°æ®çš„æš´éœ²ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ä¸­çš„æ•°æ®éšç§ä¿æŠ¤ã€åˆæˆæ•°æ®ç”Ÿæˆä»¥åŠé«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚é€šè¿‡å‡å°‘å¯¹çœŸå®æ•°æ®çš„ä¾èµ–ï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œæå‡æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dataset distillation aims to create a small and highly representative synthetic dataset that preserves the essential information of a larger real dataset. Beyond reducing storage and computational costs, related approaches offer a promising avenue for privacy preservation in computer vision by eliminating the need to store or share sensitive real-world images. Existing methods focus solely on optimizing visual representations, overlooking the potential of multi-modal information. In this work, we propose a multi-modal dataset distillation framework that incorporates two key enhancements: caption-guided supervision and object-centric masking. To leverage textual information, we introduce two strategies: caption concatenation, which fuses caption embeddings with visual features during classification, and caption matching, which enforces semantic alignment between real and synthetic data through a caption-based loss. To improve data utility and reduce unnecessary background noise, we employ segmentation masks to isolate target objects and introduce two novel losses: masked feature alignment and masked gradient matching, both aimed at promoting object-centric learning. Extensive evaluations demonstrate that our approach improves downstream performance while promoting privacy protection by minimizing exposure to real data.

