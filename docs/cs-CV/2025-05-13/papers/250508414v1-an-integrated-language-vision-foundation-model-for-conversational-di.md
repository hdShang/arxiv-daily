---
layout: default
title: An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care
---

# An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08414" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08414v1</a>
  <a href="https://arxiv.org/pdf/2505.08414.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08414v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08414v1', 'An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhi Da Soh, Yang Bai, Kai Yu, Yang Zhou, Xiaofeng Lei, Sahil Thakur, Zann Lee, Lee Ching Linette Phang, Qingsheng Peng, Can Can Xue, Rachel Shujuan Chong, Quan V. Hoang, Lavanya Raghavan, Yih Chung Tham, Charumathi Sabanayagam, Wei-Chi Wu, Ming-Chih Ho, Jiangnan He, Preeti Gupta, Ecosse Lamoureux, Seang Mei Saw, Vinay Nangia, Songhomitra Panda-Jonas, Jie Xu, Ya Xing Wang, Xinxing Xu, Jost B. Jonas, Tien Yin Wong, Rick Siow Mong Goh, Yong Liu, Ching-Yu Cheng

**åˆ†ç±»**: eess.IV, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMeta-EyeFMä»¥è§£å†³åˆçº§çœ¼ç§‘è¯Šæ–­ä¸­çš„å¤šä»»åŠ¡æ•´åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çœ¼ç§‘ç–¾ç—…è¯Šæ–­` `å¤šåŠŸèƒ½æ¨¡å‹` `è§†è§‰åŸºç¡€æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `è·¯ç”±æœºåˆ¶` `ä½ç§©é€‚åº”` `å†³ç­–æ”¯æŒå·¥å…·`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å¤§å¤šä¸“æ³¨äºå•ä¸€ä»»åŠ¡ï¼Œç¼ºä¹æ•´åˆå¤šç§åŠŸèƒ½çš„èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨çœ¼ç§‘è¯Šæ–­ä¸­æ•ˆç‡ä½ä¸‹ã€‚
2. Meta-EyeFMé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ï¼Œåˆ©ç”¨è·¯ç”±æœºåˆ¶å®ç°å¤šä»»åŠ¡å¤„ç†ï¼Œæå‡äº†çœ¼ç§‘ç–¾ç—…è¯„ä¼°çš„å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMeta-EyeFMåœ¨çœ¼ç—…æ£€æµ‹å’Œä¸¥é‡ç¨‹åº¦åŒºåˆ†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºç°æœ‰æ¨¡å‹ï¼Œä¸”ä¸ä¸“ä¸šçœ¼ç§‘åŒ»ç”Ÿçš„è¯Šæ–­æ°´å¹³ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å¤§å¤šæ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ï¼Œç¼ºä¹ç”¨æˆ·å‹å¥½çš„æ“ä½œç•Œé¢ã€‚æˆ‘ä»¬æå‡ºäº†Meta-EyeFMï¼Œè¿™æ˜¯ä¸€ç§å¤šåŠŸèƒ½åŸºç¡€æ¨¡å‹ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ç»“åˆç”¨äºçœ¼ç§‘ç–¾ç—…è¯„ä¼°ã€‚Meta-EyeFMåˆ©ç”¨è·¯ç”±æœºåˆ¶ï¼Œæ ¹æ®æ–‡æœ¬æŸ¥è¯¢å®ç°å‡†ç¡®çš„ä»»åŠ¡ç‰¹å®šåˆ†æã€‚é€šè¿‡ä½ç§©é€‚åº”ï¼Œæˆ‘ä»¬å¯¹VFMè¿›è¡Œäº†å¾®è°ƒï¼Œä»¥æ£€æµ‹çœ¼éƒ¨å’Œç³»ç»Ÿæ€§ç–¾ç—…ã€åŒºåˆ†çœ¼ç—…ä¸¥é‡ç¨‹åº¦ä»¥åŠè¯†åˆ«å¸¸è§çœ¼éƒ¨ç—‡çŠ¶ã€‚è¯¥æ¨¡å‹åœ¨å°†çœ¼åº•å›¾åƒè·¯ç”±åˆ°é€‚å½“çš„VFMæ—¶è¾¾åˆ°äº†100%çš„å‡†ç¡®ç‡ï¼Œåœ¨ç–¾ç—…æ£€æµ‹ã€ä¸¥é‡ç¨‹åº¦åŒºåˆ†å’Œç—‡çŠ¶è¯†åˆ«æ–¹é¢åˆ†åˆ«è¾¾åˆ°äº†â‰¥82.2%ã€â‰¥89%å’Œâ‰¥76%çš„å‡†ç¡®ç‡ã€‚Meta-EyeFMåœ¨æ£€æµ‹å„ç§çœ¼ç—…æ—¶æ¯”Gemini-1.5-flashå’ŒChatGPT-4o LMMsçš„å‡†ç¡®ç‡é«˜å‡º11%è‡³43%ï¼Œå¹¶ä¸”ä¸çœ¼ç§‘åŒ»ç”Ÿçš„è¡¨ç°ç›¸å½“ã€‚è¯¥ç³»ç»Ÿæå‡äº†å¯ç”¨æ€§å’Œè¯Šæ–­æ€§èƒ½ï¼Œæ˜¯åˆçº§çœ¼ç§‘çš„é‡è¦å†³ç­–æ”¯æŒå·¥å…·æˆ–åœ¨çº¿LLMç”¨äºçœ¼åº•è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å½“å‰çœ¼ç§‘ç–¾ç—…è¯Šæ–­ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å•ä¸€ä»»åŠ¡é™åˆ¶åŠç”¨æˆ·æ“ä½œä¸ä¾¿çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹æ•´åˆå¤šç§åŠŸèƒ½çš„èƒ½åŠ›ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œå‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMeta-EyeFMçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰åŸºç¡€æ¨¡å‹ç›¸ç»“åˆï¼Œé€šè¿‡è·¯ç”±æœºåˆ¶å®ç°ä»»åŠ¡ç‰¹å®šåˆ†æï¼Œä»è€Œæå‡çœ¼ç§‘ç–¾ç—…è¯„ä¼°çš„å‡†ç¡®æ€§å’Œç”¨æˆ·å‹å¥½æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šä¸ªè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨è·¯ç”±æœºåˆ¶æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æŸ¥è¯¢å°†è¾“å…¥çš„çœ¼åº•å›¾åƒå‡†ç¡®è·¯ç”±åˆ°ç›¸åº”çš„è§†è§‰æ¨¡å‹è¿›è¡Œåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šMeta-EyeFMçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šåŠŸèƒ½æ•´åˆèƒ½åŠ›å’Œé«˜æ•ˆçš„è·¯ç”±æœºåˆ¶ï¼Œä½¿å…¶åœ¨å¤„ç†å¤æ‚çš„çœ¼ç§‘è¯Šæ–­ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•ä¸€ä»»åŠ¡æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæ¨¡å‹é‡‡ç”¨ä½ç§©é€‚åº”æ–¹æ³•å¯¹è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç–¾ç—…æ£€æµ‹ã€ä¸¥é‡ç¨‹åº¦åŒºåˆ†å’Œç—‡çŠ¶è¯†åˆ«çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMeta-EyeFMåœ¨å°†çœ¼åº•å›¾åƒè·¯ç”±åˆ°é€‚å½“çš„è§†è§‰æ¨¡å‹æ—¶è¾¾åˆ°äº†100%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨ç–¾ç—…æ£€æµ‹ã€ä¸¥é‡ç¨‹åº¦åŒºåˆ†å’Œç—‡çŠ¶è¯†åˆ«æ–¹é¢åˆ†åˆ«å–å¾—äº†â‰¥82.2%ã€â‰¥89%å’Œâ‰¥76%çš„å‡†ç¡®ç‡ã€‚ä¸Gemini-1.5-flashå’ŒChatGPT-4o LMMsç›¸æ¯”ï¼ŒMeta-EyeFMçš„å‡†ç¡®ç‡æé«˜äº†11%è‡³43%ï¼Œè¡¨ç°å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Meta-EyeFMåœ¨åˆçº§çœ¼ç§‘è¯Šæ–­ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿä½œä¸ºå†³ç­–æ”¯æŒå·¥å…·å¸®åŠ©åŒ»ç”Ÿè¿›è¡Œæ›´å‡†ç¡®çš„ç–¾ç—…è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿä¹Ÿå¯ä»¥ä½œä¸ºåœ¨çº¿å¹³å°ï¼Œä¸ºæ‚£è€…æä¾›ä¾¿æ·çš„çœ¼åº•å›¾åƒåˆ†ææœåŠ¡ï¼Œæå‡åŒ»ç–—æœåŠ¡çš„å¯åŠæ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current deep learning models are mostly task specific and lack a user-friendly interface to operate. We present Meta-EyeFM, a multi-function foundation model that integrates a large language model (LLM) with vision foundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a routing mechanism to enable accurate task-specific analysis based on text queries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and systemic diseases, differentiate ocular disease severity, and identify common ocular signs. The model achieved 100% accuracy in routing fundus images to appropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection, $\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification. Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o LMMs in detecting various eye diseases and comparable to an ophthalmologist. This system offers enhanced usability and diagnostic performance, making it a valuable decision support tool for primary eye care or an online LLM for fundus evaluation.

