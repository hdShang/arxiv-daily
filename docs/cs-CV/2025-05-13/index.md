---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-13
---

# cs.CVï¼ˆ2025-05-13ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250508644v2-dlo-splatting-tracking-deformable-linear-objects-using-3d-gaussian-s.html">DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting</a></td>
  <td>æå‡ºDLO-Splattingä»¥è§£å†³å¯å˜å½¢çº¿æ€§ç‰©ä½“è·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08644v2" data-paper-url="./papers/250508644v2-dlo-splatting-tracking-deformable-linear-objects-using-3d-gaussian-s.html" onclick="toggleFavorite(this, '2505.08644v2', 'DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250508196v1-adc-gs-anchor-driven-deformable-and-compressed-gaussian-splatting-fo.html">ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction</a></td>
  <td>æå‡ºADC-GSä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„å†—ä½™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08196v1" data-paper-url="./papers/250508196v1-adc-gs-anchor-driven-deformable-and-compressed-gaussian-splatting-fo.html" onclick="toggleFavorite(this, '2505.08196v1', 'ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250508438v3-a-survey-of-3d-reconstruction-with-event-cameras.html">A Survey of 3D Reconstruction with Event Cameras</a></td>
  <td>ç»¼è¿°äº‹ä»¶ç›¸æœºåœ¨3Dé‡å»ºä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08438v3" data-paper-url="./papers/250508438v3-a-survey-of-3d-reconstruction-with-event-cameras.html" onclick="toggleFavorite(this, '2505.08438v3', 'A Survey of 3D Reconstruction with Event Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250508178v1-monocular-depth-guided-occlusion-aware-disparity-refinement-via-semi.html">Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images</a></td>
  <td>æå‡ºæ·±åº¦å¼•å¯¼çš„é®æŒ¡æ„ŸçŸ¥è§†å·®ç²¾ç‚¼ç½‘ç»œä»¥è§£å†³è…¹è…”é•œå›¾åƒä¸­çš„è§†å·®ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">monocular depth</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08178v1" data-paper-url="./papers/250508178v1-monocular-depth-guided-occlusion-aware-disparity-refinement-via-semi.html" onclick="toggleFavorite(this, '2505.08178v1', 'Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250508607v1-boosting-zero-shot-stereo-matching-using-large-scale-mixed-images-so.html">Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World</a></td>
  <td>æå‡ºBooSTerä»¥è§£å†³çœŸå®ä¸–ç•Œä¸­é›¶-shotç«‹ä½“åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08607v1" data-paper-url="./papers/250508607v1-boosting-zero-shot-stereo-matching-using-large-scale-mixed-images-so.html" onclick="toggleFavorite(this, '2505.08607v1', 'Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250508235v1-eventdiff-a-unified-and-efficient-diffusion-model-framework-for-even.html">EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation</a></td>
  <td>æå‡ºEventDiffä»¥è§£å†³äº‹ä»¶é©±åŠ¨è§†é¢‘å¸§æ’å€¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08235v1" data-paper-url="./papers/250508235v1-eventdiff-a-unified-and-efficient-diffusion-model-framework-for-even.html" onclick="toggleFavorite(this, '2505.08235v1', 'EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250508191v1-spnerf-memory-efficient-sparse-volumetric-neural-rendering-accelerat.html">SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices</a></td>
  <td>æå‡ºSpNeRFä»¥è§£å†³è¾¹ç¼˜è®¾å¤‡ä¸Šç¨€ç–ä½“ç§¯ç¥ç»æ¸²æŸ“çš„å†…å­˜æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08191v1" data-paper-url="./papers/250508191v1-spnerf-memory-efficient-sparse-volumetric-neural-rendering-accelerat.html" onclick="toggleFavorite(this, '2505.08191v1', 'SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250508561v2-reinforcement-learning-meets-masked-video-modeling-trajectory-guided.html">Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection</a></td>
  <td>æå‡ºè½¨è¿¹æ„ŸçŸ¥è‡ªé€‚åº”æ ‡è®°é€‰æ‹©ä»¥è§£å†³è§†é¢‘å»ºæ¨¡ä¸­çš„æ©è”½ç­–ç•¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span> <span class="paper-tag">masked autoencoder</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08561v2" data-paper-url="./papers/250508561v2-reinforcement-learning-meets-masked-video-modeling-trajectory-guided.html" onclick="toggleFavorite(this, '2505.08561v2', 'Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250508552v1-dfa-con-a-contrastive-learning-approach-for-detecting-copyright-infr.html">DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art</a></td>
  <td>æå‡ºDFA-CONä»¥è§£å†³æ·±åº¦ä¼ªé€ è‰ºæœ¯ä½œå“çš„ç‰ˆæƒä¾µçŠ¯æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08552v1" data-paper-url="./papers/250508552v1-dfa-con-a-contrastive-learning-approach-for-detecting-copyright-infr.html" onclick="toggleFavorite(this, '2505.08552v1', 'DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250508837v1-adaptive-security-policy-management-in-cloud-environments-using-rein.html">Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€å®‰å…¨ç­–ç•¥ç®¡ç†æ¡†æ¶ä»¥åº”å¯¹äº‘ç¯å¢ƒå®‰å…¨æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08837v1" data-paper-url="./papers/250508837v1-adaptive-security-policy-management-in-cloud-environments-using-rein.html" onclick="toggleFavorite(this, '2505.08837v1', 'Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250508617v2-openthinkimg-learning-to-think-with-images-via-visual-tool-reinforce.html">OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning</a></td>
  <td>æå‡ºOpenThinkIMGä»¥è§£å†³è§†è§‰å·¥å…·å¢å¼ºå­¦ä¹ çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08617v2" data-paper-url="./papers/250508617v2-openthinkimg-learning-to-think-with-images-via-visual-tool-reinforce.html" onclick="toggleFavorite(this, '2505.08617v2', 'OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250508605v3-leveraging-multi-modal-information-to-enhance-dataset-distillation.html">Leveraging Multi-Modal Information to Enhance Dataset Distillation</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ•°æ®è’¸é¦æ¡†æ¶ä»¥æå‡æ•°æ®é›†è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08605v3" data-paper-url="./papers/250508605v3-leveraging-multi-modal-information-to-enhance-dataset-distillation.html" onclick="toggleFavorite(this, '2505.08605v3', 'Leveraging Multi-Modal Information to Enhance Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250508170v2-mokd-multi-task-optimization-for-knowledge-distillation.html">MoKD: Multi-Task Optimization for Knowledge Distillation</a></td>
  <td>æå‡ºMoKDä»¥è§£å†³çŸ¥è¯†è’¸é¦ä¸­çš„æ¢¯åº¦å†²çªä¸ä¸»å¯¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08170v2" data-paper-url="./papers/250508170v2-mokd-multi-task-optimization-for-knowledge-distillation.html" onclick="toggleFavorite(this, '2505.08170v2', 'MoKD: Multi-Task Optimization for Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250508414v1-an-integrated-language-vision-foundation-model-for-conversational-di.html">An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care</a></td>
  <td>æå‡ºMeta-EyeFMä»¥è§£å†³åˆçº§çœ¼ç§‘è¯Šæ–­ä¸­çš„å¤šä»»åŠ¡æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08414v1" data-paper-url="./papers/250508414v1-an-integrated-language-vision-foundation-model-for-conversational-di.html" onclick="toggleFavorite(this, '2505.08414v1', 'An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250508854v1-generative-ai-for-autonomous-driving-frontiers-and-opportunities.html">Generative AI for Autonomous Driving: Frontiers and Opportunities</a></td>
  <td>ç»¼è¿°ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08854v1" data-paper-url="./papers/250508854v1-generative-ai-for-autonomous-driving-frontiers-and-opportunities.html" onclick="toggleFavorite(this, '2505.08854v1', 'Generative AI for Autonomous Driving: Frontiers and Opportunities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250509018v2-multimodal-fusion-of-glucose-monitoring-and-food-imagery-for-caloric.html">Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ä»¥æå‡å¡è·¯é‡Œä¼°ç®—ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.09018v2" data-paper-url="./papers/250509018v2-multimodal-fusion-of-glucose-monitoring-and-food-imagery-for-caloric.html" onclick="toggleFavorite(this, '2505.09018v2', 'Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250508971v1-prioritizing-image-related-tokens-enhances-vision-language-pre-train.html">Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training</a></td>
  <td>æå‡ºPRIORä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å™ªå£°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08971v1" data-paper-url="./papers/250508971v1-prioritizing-image-related-tokens-enhances-vision-language-pre-train.html" onclick="toggleFavorite(this, '2505.08971v1', 'Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250508747v1-advancing-food-nutrition-estimation-via-visual-ingredient-feature-fu.html">Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion</a></td>
  <td>æå‡ºè§†è§‰-æˆåˆ†ç‰¹å¾èåˆæ–¹æ³•ä»¥æå‡é£Ÿå“è¥å…»ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08747v1" data-paper-url="./papers/250508747v1-advancing-food-nutrition-estimation-via-visual-ingredient-feature-fu.html" onclick="toggleFavorite(this, '2505.08747v1', 'Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250508281v1-ultra-lowrate-image-compression-with-semantic-residual-coding-and-co.html">Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion</a></td>
  <td>æå‡ºResULICä»¥è§£å†³ç°æœ‰å›¾åƒå‹ç¼©æ•ˆç‡ä½çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08281v1" data-paper-url="./papers/250508281v1-ultra-lowrate-image-compression-with-semantic-residual-coding-and-co.html" onclick="toggleFavorite(this, '2505.08281v1', 'Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250508437v2-tt-df-a-large-scale-diffusion-based-dataset-and-benchmark-for-human-.html">TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection</a></td>
  <td>æå‡ºTT-DFæ•°æ®é›†ä»¥è§£å†³äººä½“ä¼ªé€ æ£€æµ‹çš„ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">optical flow</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08437v2" data-paper-url="./papers/250508437v2-tt-df-a-large-scale-diffusion-based-dataset-and-benchmark-for-human-.html" onclick="toggleFavorite(this, '2505.08437v2', 'TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250508234v1-removing-watermarks-with-partial-regeneration-using-semantic-informa.html">Removing Watermarks with Partial Regeneration using Semantic Information</a></td>
  <td>æå‡ºSemanticRegenä»¥è§£å†³æ°´å°é˜²æŠ¤è„†å¼±æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08234v1" data-paper-url="./papers/250508234v1-removing-watermarks-with-partial-regeneration-using-semantic-informa.html" onclick="toggleFavorite(this, '2505.08234v1', 'Removing Watermarks with Partial Regeneration using Semantic Information')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250508723v1-timo-spatiotemporal-foundation-model-for-satellite-image-time-series.html">TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series</a></td>
  <td>æå‡ºTiMoä»¥è§£å†³å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—åˆ†æä¸­çš„å¤šå°ºåº¦æ—¶ç©ºå…³ç³»æ•æ‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08723v1" data-paper-url="./papers/250508723v1-timo-spatiotemporal-foundation-model-for-satellite-image-time-series.html" onclick="toggleFavorite(this, '2505.08723v1', 'TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)