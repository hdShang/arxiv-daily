---
layout: default
title: Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors
---

# Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16485" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16485v1</a>
  <a href="https://arxiv.org/pdf/2512.16485.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16485v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16485v1', 'Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kejun Liu, Yuanyuan Liu, Lin Wei, Chang Tang, Yibing Zhan, Zijing Chen, Zhe Chen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Accepted by TMM

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/kejun1/EMER)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEMERæ•°æ®é›†å’ŒEMERTæ¨¡å‹ï¼Œåˆ©ç”¨çœ¼éƒ¨è¡Œä¸ºå¼¥åˆé¢éƒ¨è¡¨æƒ…è¯†åˆ«å’Œæƒ…æ„Ÿè¯†åˆ«ä¹‹é—´çš„å·®è·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿè¯†åˆ«` `å¤šæ¨¡æ€èåˆ` `çœ¼éƒ¨è¡Œä¸º` `é¢éƒ¨è¡¨æƒ…` `Transformer` `å¯¹æŠ—å­¦ä¹ ` `æ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æƒ…æ„Ÿè¯†åˆ«ä¸»è¦ä¾èµ–é¢éƒ¨è¡¨æƒ…ï¼Œä½†é¢éƒ¨è¡¨æƒ…å¸¸ä½œä¸ºç¤¾äº¤å·¥å…·ï¼Œæ— æ³•çœŸå®åæ˜ å†…å¿ƒæƒ…æ„Ÿã€‚
2. è®ºæ–‡æå‡ºå°†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºæƒ…æ„Ÿçº¿ç´¢ï¼Œæ„å»ºEMERæ•°æ®é›†ï¼Œå¹¶è®¾è®¡EMERTæ¨¡å‹å¼¥åˆæƒ…æ„Ÿå·®è·ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEMERTæ¨¡å‹åœ¨EMERæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å¤šæ¨¡æ€æ–¹æ³•ï¼ŒéªŒè¯äº†çœ¼éƒ¨è¡Œä¸ºçš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æƒ…æ„Ÿè¯†åˆ«(ER)æ˜¯ä»æ„ŸçŸ¥æ•°æ®ä¸­åˆ†æå’Œè¯†åˆ«äººç±»æƒ…æ„Ÿçš„è¿‡ç¨‹ã€‚ç›®å‰ï¼Œè¯¥é¢†åŸŸä¸¥é‡ä¾èµ–äºé¢éƒ¨è¡¨æƒ…è¯†åˆ«(FER)ï¼Œå› ä¸ºè§†è§‰é€šé“ä¼ é€’ä¸°å¯Œçš„æƒ…æ„Ÿçº¿ç´¢ã€‚ç„¶è€Œï¼Œé¢éƒ¨è¡¨æƒ…é€šå¸¸è¢«ç”¨ä½œç¤¾äº¤å·¥å…·ï¼Œè€Œä¸æ˜¯çœŸå®å†…å¿ƒæƒ…æ„Ÿçš„ä½“ç°ã€‚ä¸ºäº†ç†è§£å’Œå¼¥åˆFERå’ŒERä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥çœ¼éƒ¨è¡Œä¸ºä½œä¸ºä¸€ä¸ªé‡è¦çš„æƒ…æ„Ÿçº¿ç´¢ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªçœ¼éƒ¨è¡Œä¸ºè¾…åŠ©çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«(EMER)æ•°æ®é›†ã€‚ä¸ºäº†æ”¶é›†å…·æœ‰çœŸå®æƒ…æ„Ÿçš„æ•°æ®ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆºæ¿€ææ–™è¿›è¡Œè‡ªå‘æƒ…æ„Ÿè¯±å¯¼ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œéä¾µå…¥æ€§çš„çœ¼éƒ¨è¡Œä¸ºæ•°æ®ï¼Œå¦‚çœ¼åŠ¨åºåˆ—å’Œçœ¼éƒ¨æ³¨è§†å›¾ï¼Œä¸é¢éƒ¨è¡¨æƒ…è§†é¢‘ä¸€èµ·è¢«æ•è·ã€‚ä¸ºäº†æ›´å¥½åœ°è¯´æ˜ERå’ŒFERä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬åˆ†åˆ«å¯¹å¤šæ¨¡æ€ERå’ŒFERè¿›è¡Œå¤šè§†è§’æƒ…æ„Ÿæ ‡æ³¨ã€‚æ­¤å¤–ï¼ŒåŸºäºæ–°çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„çœ¼éƒ¨è¡Œä¸ºè¾…åŠ©çš„MER Transformer (EMERT)ï¼Œé€šè¿‡å¼¥åˆæƒ…æ„Ÿå·®è·æ¥å¢å¼ºERã€‚EMERTåˆ©ç”¨æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦å’Œä¸€ä¸ªå¤šä»»åŠ¡Transformeræ¥å»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºï¼Œä½œä¸ºé¢éƒ¨è¡¨æƒ…çš„æœ‰åŠ›è¡¥å……ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬ä¸ºEMERæ•°æ®é›†å¼•å…¥äº†ä¸ƒä¸ªå¤šæ¨¡æ€åŸºå‡†åè®®ï¼Œç”¨äºå„ç§ç»¼åˆè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒEMERTä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œæ­ç¤ºäº†å»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºå¯¹äºé²æ£’ERçš„é‡è¦æ€§ã€‚æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å¯¹çœ¼éƒ¨è¡Œä¸ºåœ¨ERä¸­çš„é‡è¦æ€§è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œä»è€Œæ¨è¿›äº†è§£å†³FERå’ŒERä¹‹é—´å·®è·çš„ç ”ç©¶ï¼Œä»¥è·å¾—æ›´å¼ºå¤§çš„ERæ€§èƒ½ã€‚æˆ‘ä»¬çš„EMERæ•°æ®é›†å’Œè®­ç»ƒå¥½çš„EMERTæ¨¡å‹å°†åœ¨https://github.com/kejun1/EMERä¸Šå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•è¿‡åº¦ä¾èµ–é¢éƒ¨è¡¨æƒ…ï¼Œå¿½ç•¥äº†é¢éƒ¨è¡¨æƒ…å¯èƒ½å­˜åœ¨çš„ä¼ªè£…æ€§ï¼Œå¯¼è‡´æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å—åˆ°å½±å“ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´é²æ£’çš„æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœé¢éƒ¨è¡¨æƒ…çš„å±€é™æ€§ï¼Œæ›´å‡†ç¡®åœ°æ•æ‰çœŸå®çš„æƒ…æ„ŸçŠ¶æ€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºæƒ…æ„Ÿè¯†åˆ«çš„é‡è¦è¡¥å……ä¿¡æ¯ã€‚çœ¼éƒ¨è¡Œä¸ºï¼Œå¦‚çœ¼åŠ¨åºåˆ—å’Œçœ¼éƒ¨æ³¨è§†å›¾ï¼Œèƒ½å¤Ÿåæ˜ ä¸ªä½“çš„æƒ…ç»ªçŠ¶æ€ï¼Œå¹¶ä¸”ç›¸å¯¹äºé¢éƒ¨è¡¨æƒ…æ›´éš¾ä¼ªè£…ã€‚é€šè¿‡èåˆçœ¼éƒ¨è¡Œä¸ºå’Œé¢éƒ¨è¡¨æƒ…ä¿¡æ¯ï¼Œå¯ä»¥æé«˜æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„EMERTæ¨¡å‹ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1)æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ï¼Œç”¨äºåˆ†ç¦»é¢éƒ¨è¡¨æƒ…å’Œçœ¼éƒ¨è¡Œä¸ºä¸­çš„æƒ…æ„Ÿç›¸å…³å’Œæƒ…æ„Ÿæ— å…³ç‰¹å¾ï¼›2)å¤šä»»åŠ¡Transformeræ¨¡å—ï¼Œç”¨äºèåˆè§£è€¦åçš„é¢éƒ¨è¡¨æƒ…å’Œçœ¼éƒ¨è¡Œä¸ºç‰¹å¾ï¼Œå¹¶åŒæ—¶é¢„æµ‹æƒ…æ„Ÿæ ‡ç­¾å’Œçœ¼éƒ¨è¡Œä¸ºæ ‡ç­¾ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šè¾“å…¥é¢éƒ¨è¡¨æƒ…è§†é¢‘å’Œçœ¼éƒ¨è¡Œä¸ºæ•°æ®ï¼Œç»è¿‡ç‰¹å¾æå–å’Œè§£è€¦åï¼Œè¾“å…¥åˆ°å¤šä»»åŠ¡Transformerä¸­è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1)æå‡ºäº†å°†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºæƒ…æ„Ÿè¯†åˆ«çš„é‡è¦çº¿ç´¢ï¼Œå¹¶æ„å»ºäº†ç›¸åº”çš„å¤šæ¨¡æ€æ•°æ®é›†EMERï¼›2)è®¾è®¡äº†æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ†ç¦»ä¸åŒæ¨¡æ€ä¸­çš„æƒ…æ„Ÿç›¸å…³å’Œæƒ…æ„Ÿæ— å…³ç‰¹å¾ï¼›3)æå‡ºäº†å¤šä»»åŠ¡Transformeræ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å­¦ä¹ æƒ…æ„Ÿå’Œçœ¼éƒ¨è¡Œä¸ºçš„è¡¨ç¤ºã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEMERTæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨çœ¼éƒ¨è¡Œä¸ºä¿¡æ¯ï¼Œæé«˜æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†æ¢¯åº¦åè½¬å±‚(Gradient Reversal Layer)æ¥å®ç°å¯¹æŠ—è®­ç»ƒï¼Œä»è€Œåˆ†ç¦»æƒ…æ„Ÿç›¸å…³å’Œæƒ…æ„Ÿæ— å…³ç‰¹å¾ã€‚åœ¨å¤šä»»åŠ¡Transformeræ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶(Cross-Attention)æ¥èåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æƒ…æ„Ÿåˆ†ç±»æŸå¤±å’Œçœ¼éƒ¨è¡Œä¸ºé¢„æµ‹æŸå¤±ï¼Œé€šè¿‡è”åˆä¼˜åŒ–è¿™ä¸¤ä¸ªæŸå¤±å‡½æ•°æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16485v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEMERTæ¨¡å‹åœ¨EMERæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒEMERTæ¨¡å‹åœ¨ä¸ƒä¸ªå¤šæ¨¡æ€åŸºå‡†åè®®ä¸Šéƒ½å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¯æ˜äº†çœ¼éƒ¨è¡Œä¸ºåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠEMERTæ¨¡å‹æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯çš„èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäººæœºäº¤äº’ã€å¿ƒç†å¥åº·è¯„ä¼°ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡ç»“åˆé¢éƒ¨è¡¨æƒ…å’Œçœ¼éƒ¨è¡Œä¸ºè¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ï¼Œå¯ä»¥æé«˜äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œå‡†ç¡®æ€§ï¼Œå¸®åŠ©å¿ƒç†åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æ‚£è€…çš„æƒ…ç»ªçŠ¶æ€ï¼Œå¹¶ä½¿æ™ºèƒ½å®¢æœèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æƒ…æ„Ÿéœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

