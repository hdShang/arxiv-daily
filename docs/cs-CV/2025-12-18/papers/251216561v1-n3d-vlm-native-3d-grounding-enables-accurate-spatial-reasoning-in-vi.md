---
layout: default
title: N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models
---

# N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16561" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16561v1</a>
  <a href="https://arxiv.org/pdf/2512.16561.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16561v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16561v1', 'N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxin Wang, Lei Ke, Boqiang Zhang, Tianyuan Qu, Hanxun Yu, Zhenpeng Huang, Meng Yu, Dan Xu, Dong Yu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Project Page: https://n3d-vlm.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**N3D-VLMï¼šåŸç”Ÿ3Dæ„ŸçŸ¥èµ‹èƒ½è§†è§‰è¯­è¨€æ¨¡å‹ç²¾ç¡®ç©ºé—´æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `3Dæ„ŸçŸ¥` `ç©ºé—´æ¨ç†` `3D grounding` `æ·±åº¦ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ç¼ºä¹å¯¹3Dåœºæ™¯çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œéš¾ä»¥ç†è§£ç©ºé—´å…³ç³»å’Œæ·±åº¦ä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. N3D-VLMé€šè¿‡é›†æˆåŸç”Ÿ3Dç‰©ä½“æ„ŸçŸ¥å’Œ3Dæ„ŸçŸ¥è§†è§‰æ¨ç†ï¼Œå®ç°ç²¾ç¡®çš„3Då®šä½å’Œå¯è§£é‡Šçš„ç©ºé—´ç†è§£ã€‚
3. è¯¥æ–¹æ³•åœ¨3D groundingå’Œç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šå‡å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡3Dæ ‡æ³¨æ•°æ®é›†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„å¤šæ¨¡æ€æ¨¡å‹è™½ç„¶å¯ä»¥åŸºäº2Då›¾åƒå›ç­”é—®é¢˜ï¼Œä½†ç¼ºä¹å›ºæœ‰çš„3Dç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶ç†è§£3Dåœºæ™¯ä¸­çš„ç©ºé—´å…³ç³»å’Œæ·±åº¦çº¿ç´¢çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†N3D-VLMï¼Œä¸€ç§æ–°é¢–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒå°†åŸç”Ÿ3Dç‰©ä½“æ„ŸçŸ¥ä¸3Dæ„ŸçŸ¥è§†è§‰æ¨ç†æ— ç¼é›†æˆï¼Œä»è€Œå®ç°ç²¾ç¡®çš„3D groundingå’Œå¯è§£é‡Šçš„ç©ºé—´ç†è§£ã€‚ä¸ç›´æ¥ä»RGB/RGB-Dè¾“å…¥é¢„æµ‹ç­”æ¡ˆçš„ä¼ ç»Ÿç«¯åˆ°ç«¯æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èµ‹äºˆæ¨¡å‹åŸç”Ÿçš„3Dç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤ŸåŸºäºæ–‡æœ¬æè¿°ç›´æ¥åœ¨3Dç©ºé—´ä¸­å®šä½ç‰©ä½“ã€‚åœ¨ç²¾ç¡®çš„3Dç‰©ä½“å®šä½çš„åŸºç¡€ä¸Šï¼Œè¯¥æ¨¡å‹è¿›ä¸€æ­¥åœ¨3Dä¸­æ‰§è¡Œæ˜¾å¼æ¨ç†ï¼Œä»è€Œå®ç°æ›´å¯è§£é‡Šå’Œç»“æ„åŒ–çš„ç©ºé—´ç†è§£ã€‚ä¸ºäº†æ”¯æŒè¿™äº›èƒ½åŠ›çš„ç¨³å¥è®­ç»ƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œè¯¥æµç¨‹åˆ©ç”¨æ·±åº¦ä¼°è®¡å°†å¤§è§„æ¨¡2Dæ ‡æ³¨æå‡åˆ°3Dç©ºé—´ï¼Œæ˜¾è‘—å¢åŠ äº†3Dç‰©ä½“groundingæ•°æ®çš„å¤šæ ·æ€§å’Œè¦†ç›–èŒƒå›´ï¼Œæ¯”ç°æœ‰çš„æœ€å¤§å•å›¾åƒ3Dæ£€æµ‹æ•°æ®é›†å¤§å…­å€ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æµç¨‹ç”Ÿæˆé’ˆå¯¹3Dä¸­æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„ç©ºé—´é—®ç­”æ•°æ®é›†ï¼Œä¿ƒè¿›3Dç‰©ä½“å®šä½å’Œ3Dç©ºé—´æ¨ç†çš„è”åˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€æ¡†æ¶ä¸ä»…åœ¨3D groundingä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„3Dç©ºé—´æ¨ç†æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ä¸»è¦åŸºäº2Då›¾åƒè¿›è¡Œæ¨ç†ï¼Œç¼ºä¹å¯¹3Dåœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œæ— æ³•å‡†ç¡®æ„ŸçŸ¥ç‰©ä½“é—´çš„ç©ºé—´å…³ç³»å’Œæ·±åº¦ä¿¡æ¯ã€‚è¿™é™åˆ¶äº†æ¨¡å‹åœ¨éœ€è¦ç©ºé—´æ¨ç†çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¾‹å¦‚å›ç­”å…³äºç‰©ä½“ä½ç½®ã€è·ç¦»ç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥ä»RGBæˆ–RGB-Då›¾åƒé¢„æµ‹ç­”æ¡ˆï¼Œç¼ºä¹å¯¹3Dåœºæ™¯çš„æ˜¾å¼å»ºæ¨¡ï¼Œå¯¼è‡´ç»“æœéš¾ä»¥è§£é‡Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šN3D-VLMçš„æ ¸å¿ƒæ€è·¯æ˜¯èµ‹äºˆæ¨¡å‹åŸç”Ÿçš„3Dç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿç›´æ¥åœ¨3Dç©ºé—´ä¸­å®šä½ç‰©ä½“ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œç©ºé—´æ¨ç†ã€‚é€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡3Dåœºæ™¯ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£ç‰©ä½“é—´çš„ç©ºé—´å…³ç³»ï¼Œä»è€Œæé«˜ç©ºé—´æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥ä»2Då›¾åƒé¢„æµ‹ç­”æ¡ˆçš„å±€é™æ€§ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œå…ˆç†è§£åœºæ™¯çš„3Dç»“æ„ï¼Œå†è¿›è¡Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šN3D-VLMçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) 3Dç‰©ä½“æ„ŸçŸ¥æ¨¡å—ï¼šè¯¥æ¨¡å—è´Ÿè´£åŸºäºæ–‡æœ¬æè¿°åœ¨3Dç©ºé—´ä¸­å®šä½ç‰©ä½“ã€‚2) 3Dç©ºé—´æ¨ç†æ¨¡å—ï¼šè¯¥æ¨¡å—åŸºäº3Dç‰©ä½“å®šä½çš„ç»“æœï¼Œè¿›è¡Œç©ºé—´å…³ç³»çš„æ¨ç†ï¼Œä¾‹å¦‚åˆ¤æ–­ç‰©ä½“ä¹‹é—´çš„è·ç¦»ã€æ–¹ä½ç­‰ã€‚3) æ•°æ®æ„å»ºæµç¨‹ï¼šä¸ºäº†æ”¯æŒæ¨¡å‹çš„è®­ç»ƒï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œè¯¥æµç¨‹åˆ©ç”¨æ·±åº¦ä¼°è®¡å°†å¤§è§„æ¨¡2Dæ ‡æ³¨æå‡åˆ°3Dç©ºé—´ï¼Œç”Ÿæˆå¤§è§„æ¨¡çš„3Dç‰©ä½“groundingå’Œç©ºé—´é—®ç­”æ•°æ®é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šN3D-VLMæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†åŸç”Ÿ3Dç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›é›†æˆåˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒN3D-VLMä¸æ˜¯ç›´æ¥ä»2Då›¾åƒé¢„æµ‹ç­”æ¡ˆï¼Œè€Œæ˜¯å…ˆåœ¨3Dç©ºé—´ä¸­å®šä½ç‰©ä½“ï¼Œå†è¿›è¡Œç©ºé—´æ¨ç†ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åœºæ™¯çš„3Dç»“æ„ï¼Œä»è€Œæé«˜ç©ºé—´æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºçš„æ•°æ®æ„å»ºæµç¨‹ä¹Ÿä¸º3Dè§†è§‰è¯­è¨€æ¨¡å‹çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®æ„å»ºæµç¨‹åˆ©ç”¨æ·±åº¦ä¼°è®¡å°†2Dæ ‡æ³¨æå‡åˆ°3Dç©ºé—´ï¼Œå…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…å«3Dç‰©ä½“å®šä½çš„æŸå¤±å’Œç©ºé—´æ¨ç†çš„æŸå¤±ï¼Œå…·ä½“å½¢å¼æœªçŸ¥ã€‚ç½‘ç»œç»“æ„å¯èƒ½åŒ…å«ç”¨äº3Dç‰©ä½“å®šä½çš„æ¨¡å—å’Œç”¨äºç©ºé—´æ¨ç†çš„æ¨¡å—ï¼Œå…·ä½“ç»“æ„æœªçŸ¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16561v1/x3.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16561v1/x4.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16561v1/x5.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

N3D-VLMåœ¨3D groundingä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå…·ä½“æ•°å€¼æœªçŸ¥ã€‚åœ¨3Dç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šï¼ŒN3D-VLMä¹Ÿæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªæ¯”ç°æœ‰æœ€å¤§å•å›¾åƒ3Dæ£€æµ‹æ•°æ®é›†å¤§å…­å€ä»¥ä¸Šçš„å¤§è§„æ¨¡3Dæ ‡æ³¨æ•°æ®é›†ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

N3D-VLMåœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨N3D-VLMç†è§£äººç±»çš„æŒ‡ä»¤ï¼Œä¾‹å¦‚â€œæŠŠæ¡Œå­ä¸Šçš„æ¯å­æ‹¿åˆ°æ²™å‘æ—è¾¹â€ï¼Œä»è€Œå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒN3D-VLMå¯ä»¥å¸®åŠ©è½¦è¾†ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä¾‹å¦‚è¯†åˆ«è¡Œäººã€è½¦è¾†å’Œäº¤é€šæ ‡å¿—ï¼Œå¹¶è¿›è¡Œç›¸åº”çš„å†³ç­–ã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼ŒN3D-VLMå¯ä»¥å¢å¼ºç”¨æˆ·çš„æ²‰æµ¸æ„Ÿï¼Œä¾‹å¦‚è®©ç”¨æˆ·ä¸è™šæ‹Ÿç¯å¢ƒä¸­çš„ç‰©ä½“è¿›è¡Œäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.

