---
layout: default
title: Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation
---

# Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16893" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16893v1</a>
  <a href="https://arxiv.org/pdf/2512.16893.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16893v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16893v1', 'Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaiwen Jiang, Xueting Li, Seonwook Park, Ravi Ramamoorthi, Shalini De Mello, Koki Nagano

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Project website is https://research.nvidia.com/labs/amri/projects/instant4d

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäº3Dæ„ŸçŸ¥è¡¨è¾¾è’¸é¦çš„å¿«é€Ÿé«˜è¡¨ç°åŠ›é«˜æ–¯å¤´éƒ¨Avataræ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `äººè„¸åŠ¨ç”»` `3Däººè„¸` `é«˜æ–¯æº…å°„` `çŸ¥è¯†è’¸é¦` `æ‰©æ•£æ¨¡å‹` `å®æ—¶æ¸²æŸ“` `æ•°å­—å­ªç”Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. 2DäººåƒåŠ¨ç”»åœ¨è´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œä½†é€šå¸¸ç‰ºç‰²äº†3Dä¸€è‡´æ€§å’Œé€Ÿåº¦ï¼Œé™åˆ¶äº†å…¶åœ¨æ•°å­—å­ªç”Ÿç­‰åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. è¯¥æ–¹æ³•é€šè¿‡å°†2Dæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†æç‚¼åˆ°å‰é¦ˆç¼–ç å™¨ä¸­ï¼Œå®ç°å¿«é€Ÿç”Ÿæˆ3Dä¸€è‡´ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„å¯åŠ¨ç”»äººè„¸ã€‚
3. è¯¥æ–¹æ³•é‡‡ç”¨è½»é‡çº§å±€éƒ¨èåˆç­–ç•¥ï¼Œåœ¨ä¿è¯åŠ¨ç”»è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†107.31 FPSçš„å¿«é€ŸåŠ¨ç”»å’Œå§¿æ€æ§åˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç»“åˆåŸºäºæ‰©æ•£æ¨¡å‹çš„2DäººåƒåŠ¨ç”»å’ŒåŸºäºæ˜¾å¼3Dè¡¨ç¤ºï¼ˆå¦‚ç¥ç»è¾å°„åœºæˆ–é«˜æ–¯æº…å°„ï¼‰çš„3Däººè„¸åŠ¨ç”»çš„ä¼˜ç‚¹ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†çŸ¥è¯†ä»2Dæ‰©æ•£æ¨¡å‹æç‚¼åˆ°ä¸€ä¸ªå‰é¦ˆç¼–ç å™¨ä¸­ï¼Œå®ç°ä»å•å¼ å›¾åƒåˆ°3Dä¸€è‡´ã€å¿«é€Ÿä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„å¯åŠ¨ç”»è¡¨ç¤ºçš„å³æ—¶è½¬æ¢ã€‚åŠ¨ç”»è¡¨ç¤ºä¸äººè„¸çš„3Dè¡¨ç¤ºè§£è€¦ï¼Œå¹¶ä»æ•°æ®ä¸­éšå¼åœ°å­¦ä¹ è¿åŠ¨ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹é¢„å®šä¹‰å‚æ•°æ¨¡å‹çš„ä¾èµ–ã€‚é‡‡ç”¨è½»é‡çº§çš„å±€éƒ¨èåˆç­–ç•¥ï¼Œä»¥å®ç°é«˜åŠ¨ç”»è¡¨ç°åŠ›ï¼Œé¿å…äº†ä»¥å¾€è®¡ç®—å¯†é›†å‹çš„å…¨å±€èåˆæœºåˆ¶ã€‚è¯¥æ–¹æ³•åœ¨åŠ¨ç”»å’Œå§¿æ€æ§åˆ¶æ–¹é¢ä»¥107.31 FPSçš„é€Ÿåº¦è¿è¡Œï¼ŒåŒæ—¶å®ç°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„åŠ¨ç”»è´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰2DäººåƒåŠ¨ç”»æ–¹æ³•è™½ç„¶è´¨é‡é«˜ï¼Œä½†ç¼ºä¹3Dä¸€è‡´æ€§ä¸”é€Ÿåº¦æ…¢ï¼Œéš¾ä»¥åº”ç”¨äºå®æ—¶åœºæ™¯ã€‚è€ŒåŸºäº3Dè¡¨ç¤ºçš„äººè„¸åŠ¨ç”»æ–¹æ³•è™½ç„¶ä¿è¯äº†3Dä¸€è‡´æ€§å’Œé€Ÿåº¦ï¼Œä½†åŠ¨ç”»ç»†èŠ‚è¡¨ç°åŠ›ä¸è¶³ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ—¢èƒ½ä¿è¯3Dä¸€è‡´æ€§å’Œé€Ÿåº¦ï¼Œåˆèƒ½å®ç°é«˜è¡¨ç°åŠ›åŠ¨ç”»çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯å°†é«˜è´¨é‡çš„2Dæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†è’¸é¦åˆ°åŸºäº3Dè¡¨ç¤ºçš„å‰é¦ˆç½‘ç»œä¸­ï¼Œä»è€Œç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆå…·æœ‰ä¸°å¯Œè¡¨æƒ…ç»†èŠ‚ä¸”3Dä¸€è‡´çš„äººè„¸åŠ¨ç”»ã€‚åŒæ—¶ï¼Œå°†åŠ¨ç”»è¡¨ç¤ºä¸3Dç»“æ„è§£è€¦ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§çš„å±€éƒ¨èåˆç­–ç•¥ï¼Œè¿›ä¸€æ­¥æå‡äº†æ•ˆç‡å’Œè¡¨ç°åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä¸€ä¸ªå‰é¦ˆç¼–ç å™¨ï¼Œç”¨äºå°†å•å¼ è¾“å…¥å›¾åƒè½¬æ¢ä¸º3Däººè„¸è¡¨ç¤ºå’ŒåŠ¨ç”»å‚æ•°ã€‚è¯¥3Däººè„¸è¡¨ç¤ºåŸºäºé«˜æ–¯æº…å°„ï¼Œä¿è¯äº†æ¸²æŸ“é€Ÿåº¦å’Œè´¨é‡ã€‚åŠ¨ç”»å‚æ•°åˆ™ç”¨äºæ§åˆ¶äººè„¸çš„è¡¨æƒ…å’Œå§¿æ€ã€‚ä¸ºäº†èåˆ3Dç»“æ„å’ŒåŠ¨ç”»ä¿¡æ¯ï¼Œé‡‡ç”¨äº†è½»é‡çº§çš„å±€éƒ¨èåˆæ¨¡å—ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡è’¸é¦å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨2Dæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åŠ¨ç”»åºåˆ—ä½œä¸ºç›‘ç£ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) é‡‡ç”¨è’¸é¦å­¦ä¹ çš„æ–¹å¼ï¼Œå°†2Dæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°3Däººè„¸åŠ¨ç”»ä¸­ï¼›2) å°†åŠ¨ç”»è¡¨ç¤ºä¸3Dç»“æ„è§£è€¦ï¼Œä»è€Œå¯ä»¥ç‹¬ç«‹æ§åˆ¶è¡¨æƒ…å’Œå§¿æ€ï¼›3) é‡‡ç”¨è½»é‡çº§çš„å±€éƒ¨èåˆç­–ç•¥ï¼Œé¿å…äº†è®¡ç®—å¯†é›†å‹çš„å…¨å±€èåˆï¼Œæé«˜äº†æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼Œç¼–ç å™¨é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œç”¨äºæå–å›¾åƒç‰¹å¾ã€‚é«˜æ–¯æº…å°„è¡¨ç¤ºé‡‡ç”¨æ ‡å‡†çš„3Dé«˜æ–¯å‚æ•°åŒ–ã€‚å±€éƒ¨èåˆæ¨¡å—é‡‡ç”¨å¤šä¸ªå·ç§¯å±‚å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºèåˆ3Dç»“æ„å’ŒåŠ¨ç”»ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±ã€3Dä¸€è‡´æ€§æŸå¤±å’ŒåŠ¨ç”»æŸå¤±ã€‚é‡å»ºæŸå¤±ç”¨äºä¿è¯ç”Ÿæˆçš„äººè„¸å›¾åƒä¸è¾“å…¥å›¾åƒç›¸ä¼¼ã€‚3Dä¸€è‡´æ€§æŸå¤±ç”¨äºä¿è¯ç”Ÿæˆçš„äººè„¸åœ¨ä¸åŒè§†è§’ä¸‹çš„ä¸€è‡´æ€§ã€‚åŠ¨ç”»æŸå¤±ç”¨äºä¿è¯ç”Ÿæˆçš„åŠ¨ç”»ä¸2Dæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åŠ¨ç”»ç›¸ä¼¼ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16893v1/fig/expressiveness_vs_consistency_colored.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16893v1/fig/pipeline-2.jpg" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16893v1/fig/residual_features.jpg" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨åŠ¨ç”»å’Œå§¿æ€æ§åˆ¶æ–¹é¢å®ç°äº†107.31 FPSçš„é€Ÿåº¦ï¼ŒåŒæ—¶è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„åŠ¨ç”»è´¨é‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¡¨æƒ…ç»†èŠ‚å’Œ3Dä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºå…¶ä»–åŸºäº3Dè¡¨ç¤ºçš„äººè„¸åŠ¨ç”»æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ•°å­—å­ªç”Ÿã€è¿œç¨‹å‘ˆç°ã€è™šæ‹Ÿä¼šè®®ã€æ¸¸æˆè§’è‰²ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆé€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„3Däººè„¸Avatarï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ æ²‰æµ¸å¼çš„äº¤äº’ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºäººè„¸è¡¨æƒ…è¯†åˆ«ã€äººè„¸åŠ¨ç”»ç¼–è¾‘ç­‰ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d

