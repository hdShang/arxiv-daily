---
layout: default
title: MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning
---

# MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16909" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16909v1</a>
  <a href="https://arxiv.org/pdf/2512.16909.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16909v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16909v1', 'MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: 25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMomaGraphï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ä¸ºå…·èº«ä»»åŠ¡è§„åˆ’æ„å»ºçŠ¶æ€æ„ŸçŸ¥çš„ç»Ÿä¸€åœºæ™¯å›¾ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `åœºæ™¯å›¾` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ä»»åŠ¡è§„åˆ’` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åœºæ™¯å›¾è¡¨ç¤ºæ–¹æ³•åœ¨å¤„ç†å…·èº«ä»»åŠ¡æ—¶ï¼Œç¼ºä¹å¯¹ç©ºé—´-åŠŸèƒ½å…³ç³»çš„ç»Ÿä¸€å»ºæ¨¡ï¼Œä¸”å¿½ç•¥äº†å¯¹è±¡çŠ¶æ€å’Œä»»åŠ¡ç›¸å…³ä¿¡æ¯ã€‚
2. MomaGraphé€šè¿‡æ•´åˆç©ºé—´-åŠŸèƒ½å…³ç³»å’Œéƒ¨ä»¶çº§åˆ«çš„äº¤äº’å…ƒç´ ï¼Œæ„å»ºäº†çŠ¶æ€æ„ŸçŸ¥çš„ç»Ÿä¸€åœºæ™¯å›¾ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ”¯æŒå…·èº«æ™ºèƒ½ä½“çš„ä»»åŠ¡è§„åˆ’ã€‚
3. MomaGraph-R1æ¨¡å‹åœ¨MomaGraph-Benchä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººå®éªŒä¸­å±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMomaGraphï¼Œä¸€ç§ç”¨äºå…·èº«æ™ºèƒ½ä½“çš„ç»Ÿä¸€åœºæ™¯è¡¨ç¤ºï¼Œå®ƒé›†æˆäº†ç©ºé—´-åŠŸèƒ½å…³ç³»å’Œéƒ¨ä»¶çº§åˆ«çš„äº¤äº’å…ƒç´ ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åœºæ™¯å›¾è¡¨ç¤ºæ–¹æ³•ä¸­ç©ºé—´å’ŒåŠŸèƒ½å…³ç³»åˆ†ç¦»ã€åœºæ™¯é™æ€åŒ–ä»¥åŠå¿½ç•¥ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è´¡çŒ®äº†MomaGraph-Scenesï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„ã€å¸¦æœ‰ä¸°å¯Œæ ‡æ³¨çš„ã€ä»»åŠ¡é©±åŠ¨çš„å®¶åº­ç¯å¢ƒåœºæ™¯å›¾æ•°æ®é›†ï¼Œä»¥åŠMomaGraph-Benchï¼Œä¸€ä¸ªåŒ…å«ä»é«˜å±‚è§„åˆ’åˆ°ç»†ç²’åº¦åœºæ™¯ç†è§£çš„å…­ç§æ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°å¥—ä»¶ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡è¿›ä¸€æ­¥å¼€å‘äº†MomaGraph-R1ï¼Œä¸€ä¸ªåœ¨MomaGraph-Scenesä¸Šé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„7Bè§†è§‰-è¯­è¨€æ¨¡å‹ã€‚MomaGraph-R1èƒ½å¤Ÿé¢„æµ‹é¢å‘ä»»åŠ¡çš„åœºæ™¯å›¾ï¼Œå¹¶ä½œä¸ºGraph-then-Planæ¡†æ¶ä¸‹çš„é›¶æ ·æœ¬ä»»åŠ¡è§„åˆ’å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¼€æºæ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†71.6%çš„å‡†ç¡®ç‡ï¼ˆæ¯”æœ€ä½³åŸºçº¿é«˜å‡º11.4%ï¼‰ï¼ŒåŒæ—¶èƒ½å¤Ÿæ³›åŒ–åˆ°å…¬å…±åŸºå‡†æµ‹è¯•ï¼Œå¹¶æœ‰æ•ˆåœ°è¿ç§»åˆ°çœŸå®æœºå™¨äººå®éªŒä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åœºæ™¯å›¾æ–¹æ³•é€šå¸¸å°†ç©ºé—´å’ŒåŠŸèƒ½å…³ç³»åˆ†ç¦»å¤„ç†ï¼Œå°†åœºæ™¯è§†ä¸ºé™æ€å¿«ç…§ï¼Œå¿½ç•¥äº†å¯¹è±¡çš„çŠ¶æ€å˜åŒ–å’Œä¸å½“å‰ä»»åŠ¡æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚è¿™å¯¼è‡´å…·èº«æ™ºèƒ½ä½“éš¾ä»¥æœ‰æ•ˆåœ°è¿›è¡Œä»»åŠ¡è§„åˆ’ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å®¶åº­ç¯å¢ƒä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMomaGraphçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„ã€çŠ¶æ€æ„ŸçŸ¥çš„åœºæ™¯å›¾è¡¨ç¤ºï¼Œå®ƒèƒ½å¤ŸåŒæ—¶æ•æ‰å¯¹è±¡çš„ä½ç½®ã€åŠŸèƒ½ä»¥åŠå¯äº¤äº’çš„éƒ¨ä»¶ã€‚é€šè¿‡æ•´åˆç©ºé—´-åŠŸèƒ½å…³ç³»ï¼Œå¹¶åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹é¢„æµ‹åœºæ™¯å›¾ï¼ŒMomaGraphèƒ½å¤Ÿä¸ºå…·èº«æ™ºèƒ½ä½“æä¾›æ›´å…¨é¢çš„åœºæ™¯ç†è§£ï¼Œä»è€Œæ”¯æŒæ›´æœ‰æ•ˆçš„ä»»åŠ¡è§„åˆ’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMomaGraphçš„æ•´ä½“æ¡†æ¶åŒ…å«æ•°æ®æ”¶é›†ä¸æ ‡æ³¨ã€æ¨¡å‹è®­ç»ƒå’Œä»»åŠ¡è§„åˆ’ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ„å»ºMomaGraph-Scenesæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„åœºæ™¯å›¾æ ‡æ³¨ï¼ŒåŒ…æ‹¬å¯¹è±¡ã€å…³ç³»å’ŒçŠ¶æ€ä¿¡æ¯ã€‚ç„¶åï¼Œåˆ©ç”¨è¯¥æ•°æ®é›†è®­ç»ƒMomaGraph-R1è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è§†è§‰è¾“å…¥é¢„æµ‹é¢å‘ä»»åŠ¡çš„åœºæ™¯å›¾ã€‚æœ€åï¼Œå°†é¢„æµ‹çš„åœºæ™¯å›¾ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨Graph-then-Planæ¡†æ¶è¿›è¡Œä»»åŠ¡è§„åˆ’ã€‚

**å…³é”®åˆ›æ–°**ï¼šMomaGraphçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„åœºæ™¯å›¾è¡¨ç¤ºï¼Œå®ƒèƒ½å¤ŸåŒæ—¶æ•æ‰ç©ºé—´-åŠŸèƒ½å…³ç³»å’Œå¯¹è±¡çŠ¶æ€ã€‚æ­¤å¤–ï¼ŒMomaGraph-R1æ¨¡å‹åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œåœºæ™¯å›¾é¢„æµ‹ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMomaGraphèƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£åœºæ™¯ï¼Œå¹¶æ›´å¥½åœ°æ”¯æŒå…·èº«æ™ºèƒ½ä½“çš„ä»»åŠ¡è§„åˆ’ã€‚

**å…³é”®è®¾è®¡**ï¼šMomaGraph-R1æ¨¡å‹æ˜¯ä¸€ä¸ª7Bå‚æ•°çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¥å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºåœºæ™¯å›¾çš„è¡¨ç¤ºã€‚æ¨¡å‹é‡‡ç”¨Transformeræ¶æ„ï¼Œå¹¶ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¼˜åŒ–å…¶åœ¨ä»»åŠ¡è§„åˆ’æ–¹é¢çš„æ€§èƒ½ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°åŒ…æ‹¬åœºæ™¯å›¾é¢„æµ‹æŸå¤±å’Œä»»åŠ¡è§„åˆ’å¥–åŠ±ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16909v1/Figures/Teaser.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16909v1/Figures/Failure.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16909v1/x1.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

MomaGraph-R1åœ¨MomaGraph-BenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†71.6%çš„å‡†ç¡®ç‡ï¼Œæ¯”æœ€ä½³åŸºçº¿é«˜å‡º11.4%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜èƒ½å¤Ÿæ³›åŒ–åˆ°å…¬å…±åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººå®éªŒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMomaGraphæ˜¯ä¸€ç§æœ‰æ•ˆçš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å…·èº«æ™ºèƒ½ä½“çš„ä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MomaGraphåœ¨å®¶åº­æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œæ‰§è¡Œæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©å“æ•´ç†ã€æ¸…æ´å’Œçƒ¹é¥ªã€‚æ­¤å¤–ï¼ŒMomaGraphè¿˜å¯ä»¥ç”¨äºæ„å»ºæ›´æ™ºèƒ½çš„è™šæ‹ŸåŠ©æ‰‹ï¼Œä¸ºç”¨æˆ·æä¾›æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

