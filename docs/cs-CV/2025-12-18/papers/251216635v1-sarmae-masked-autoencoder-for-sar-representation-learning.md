---
layout: default
title: SARMAE: Masked Autoencoder for SAR Representation Learning
---

# SARMAE: Masked Autoencoder for SAR Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16635" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16635v1</a>
  <a href="https://arxiv.org/pdf/2512.16635.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16635v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16635v1', 'SARMAE: Masked Autoencoder for SAR Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Danxu Liu, Di Wang, Hebaixu Wang, Haoyang Chen, Wentao Jiang, Yilin Cheng, Haonan Guo, Wei Cui, Jing Zhang

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Code and models will be available at https://github.com/MiliLab/SARMAE

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MiliLab/SARMAE)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSARMAEä»¥è§£å†³SARå›¾åƒè¡¨ç¤ºå­¦ä¹ ä¸­çš„å™ªå£°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åˆæˆå­”å¾„é›·è¾¾` `è‡ªç›‘ç£å­¦ä¹ ` `æ•£æ–‘å™ªå£°` `è¡¨ç¤ºå­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰SARå›¾åƒæ·±åº¦å­¦ä¹ æ–¹æ³•å—åˆ°æ•°æ®ç¨€ç¼ºå’Œæ•£æ–‘å™ªå£°çš„å½±å“ï¼Œé™åˆ¶äº†å…¶åœ¨ç»†ç²’åº¦è¯­ä¹‰è¡¨ç¤ºå­¦ä¹ ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºSARMAEï¼Œé€šè¿‡æ„å»ºSAR-1Mæ•°æ®é›†å’Œå¼•å…¥æ•£æ–‘æ„ŸçŸ¥è¡¨ç¤ºå¢å¼ºï¼ˆSAREï¼‰æ¥å®ç°å™ªå£°æ„ŸçŸ¥çš„è‡ªç›‘ç£å­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSARMAEåœ¨å¤šä¸ªSARæ•°æ®é›†ä¸Šå®ç°äº†åˆ†ç±»ã€æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å›¾åƒåœ¨å…¨å¤©å€™ã€æ˜¼å¤œé¥æ„Ÿåº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SARå¯¼å‘æ·±åº¦å­¦ä¹ å—åˆ°æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ï¼ŒåŒæ—¶SARå›¾åƒä¸­çš„ç‰©ç†æ•£æ–‘å™ªå£°è¿›ä¸€æ­¥é˜»ç¢äº†ç»†ç²’åº¦è¯­ä¹‰è¡¨ç¤ºå­¦ä¹ ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†SARMAEï¼Œä¸€ç§å™ªå£°æ„ŸçŸ¥çš„è‡ªç›‘ç£SARè¡¨ç¤ºå­¦ä¹ çš„æ©ç è‡ªç¼–ç å™¨ã€‚æˆ‘ä»¬æ„å»ºäº†SAR-1Mï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç™¾ä¸‡è§„æ¨¡çš„SARæ•°æ®é›†ï¼Œå¹¶é…æœ‰é¢å¤–çš„å…‰å­¦å›¾åƒï¼Œä»¥æ”¯æŒå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ•£æ–‘æ„ŸçŸ¥è¡¨ç¤ºå¢å¼ºï¼ˆSAREï¼‰ï¼Œå°†SARç‰¹æœ‰çš„æ•£æ–‘å™ªå£°æ³¨å…¥æ©ç è‡ªç¼–ç å™¨ï¼Œä»¥ä¿ƒè¿›å™ªå£°æ„ŸçŸ¥å’Œé²æ£’çš„è¡¨ç¤ºå­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰é”šå®šè¡¨ç¤ºçº¦æŸï¼ˆSARCï¼‰ï¼Œåˆ©ç”¨é…å¯¹çš„å…‰å­¦å…ˆéªŒå¯¹é½SARç‰¹å¾ï¼Œç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSARMAEåœ¨åˆ†ç±»ã€æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³SARå›¾åƒè¡¨ç¤ºå­¦ä¹ ä¸­çš„æ•°æ®ç¨€ç¼ºå’Œæ•£æ–‘å™ªå£°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†SARå›¾åƒæ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆåº”å¯¹å™ªå£°å¯¹è¡¨ç¤ºå­¦ä¹ çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSARMAEé€šè¿‡æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„SARæ•°æ®é›†ï¼Œå¹¶åœ¨æ©ç è‡ªç¼–ç å™¨ä¸­å¼•å…¥æ•£æ–‘å™ªå£°ï¼Œæ¥å®ç°å™ªå£°æ„ŸçŸ¥çš„è‡ªç›‘ç£å­¦ä¹ ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¢å¼ºæ¨¡å‹å¯¹å™ªå£°çš„é²æ£’æ€§ï¼Œæé«˜è¡¨ç¤ºå­¦ä¹ çš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSARMAEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ•£æ–‘å™ªå£°æ³¨å…¥ã€æ©ç è‡ªç¼–ç å™¨è®­ç»ƒå’Œè¯­ä¹‰é”šå®šçº¦æŸç­‰ä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œåˆ©ç”¨SAR-1Mæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡SAREå’ŒSARCæ¨¡å—è¿›è¡Œç‰¹å¾å¢å¼ºå’Œå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ•£æ–‘æ„ŸçŸ¥è¡¨ç¤ºå¢å¼ºï¼ˆSAREï¼‰å’Œè¯­ä¹‰é”šå®šè¡¨ç¤ºçº¦æŸï¼ˆSARCï¼‰ï¼Œè¿™ä¸¤ä¸ªæ¨¡å—ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†SARç‰¹æœ‰çš„å™ªå£°ï¼Œå¹¶ç¡®ä¿ç‰¹å¾çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å™ªå£°æ„ŸçŸ¥å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”SARå›¾åƒçš„ç‰¹æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16635v1/images/radar.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16635v1/images/dataset.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16635v1/images/model.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªSARæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSARMAEåœ¨åˆ†ç±»ã€æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†10%ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡æé«˜äº†15%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†›äº‹ä¾¦å¯Ÿã€ç¾å®³ç›‘æµ‹ã€ç¯å¢ƒç›‘æµ‹ç­‰éœ€è¦é«˜ç²¾åº¦SARå›¾åƒåˆ†æçš„åœºæ™¯ã€‚é€šè¿‡æé«˜SARå›¾åƒçš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ï¼ŒSARMAEèƒ½å¤Ÿä¸ºç›¸å…³é¢†åŸŸæä¾›æ›´ä¸ºå‡†ç¡®çš„å†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.

