---
layout: default
title: Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs
---

# Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16584" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16584v1</a>
  <a href="https://arxiv.org/pdf/2512.16584.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16584v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16584v1', 'Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, Ruixuan Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: 14 pages, 11 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TungChintao/SkiLa)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSketch-in-Latents (SkiLa)ï¼Œå®ç°MLLMä¸­ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†ä¸è§†è§‰æƒ³è±¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æƒ³è±¡` `ç»Ÿä¸€æ¨ç†` `æ½œåœ¨ç©ºé—´` `è‡ªå›å½’ç”Ÿæˆ` `è§†è§‰è¯­ä¹‰é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨è§†è§‰æƒ³è±¡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•åƒäººç±»ä¸€æ ·çµæ´»è¿›è¡Œè§†è§‰-æ–‡æœ¬äº¤äº’ã€‚
2. SkiLaé€šè¿‡ç”Ÿæˆè¿ç»­çš„æ½œåœ¨è‰å›¾tokenï¼Œå°†è§†è§‰ä¿¡æ¯æ— ç¼èå…¥MLLMçš„æ¨ç†è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSkiLaåœ¨è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„å¤šæ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æ“…é•¿é€šè¿‡æ–‡æœ¬æ¨ç†è¿›è¡Œè§†è§‰ç†è§£ä»»åŠ¡ï¼Œä½†åœ¨éœ€è¦è§†è§‰æƒ³è±¡çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸é‡‡ç”¨é¢„å®šä¹‰å¤–éƒ¨å·¥å…·åŒ…æˆ–åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå›¾åƒçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œäººç±»å¯ä»¥åœ¨æ²¡æœ‰é¢„å®šä¹‰å·¥å…·åŒ…çš„æƒ…å†µä¸‹è¿›è¡Œçµæ´»çš„è§†è§‰-æ–‡æœ¬æƒ³è±¡å’Œäº¤äº’ï¼Œä¸€ä¸ªé‡è¦åŸå› æ˜¯äººç±»åœ¨å¤§è„‘å†…éƒ¨çš„ç»Ÿä¸€ç©ºé—´ä¸­æ„å»ºè§†è§‰-æ–‡æœ¬æ€è€ƒè¿‡ç¨‹ã€‚å—æ­¤å¯å‘ï¼Œé‰´äºå½“å‰çš„MLLMå·²ç»å°†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ç¼–ç åœ¨ç›¸åŒçš„ç‰¹å¾ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè§†è§‰tokenå¯ä»¥æ— ç¼åœ°æ’å…¥åˆ°æ–‡æœ¬tokenæ‰€æºå¸¦çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„è§†è§‰æƒ³è±¡è¿‡ç¨‹éƒ½å¯ä»¥ç”±æ½œåœ¨ç‰¹å¾ç¼–ç ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºSketch-in-Latents (SkiLa)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†çš„æ–°èŒƒå¼ï¼Œå®ƒæ‰©å±•äº†MLLMçš„è‡ªå›å½’èƒ½åŠ›ï¼Œä»¥åŸç”Ÿç”Ÿæˆè¿ç»­çš„è§†è§‰åµŒå…¥ï¼Œç§°ä¸ºæ½œåœ¨è‰å›¾tokenï¼Œä½œä¸ºè§†è§‰æ€è€ƒã€‚åœ¨å¤šæ­¥æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åŠ¨æ€åœ°åœ¨ç”¨äºç”Ÿæˆæ–‡æœ¬æ€è€ƒtokençš„æ–‡æœ¬æ€è€ƒæ¨¡å¼å’Œç”¨äºç”Ÿæˆæ½œåœ¨è‰å›¾tokençš„è§†è§‰è‰å›¾æ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚æå‡ºäº†ä¸€ç§æ½œåœ¨çš„è§†è§‰è¯­ä¹‰é‡å»ºæœºåˆ¶ï¼Œä»¥ç¡®ä¿è¿™äº›æ½œåœ¨çš„è‰å›¾tokenåœ¨è¯­ä¹‰ä¸Šæ˜¯æ¥åœ°çš„ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒSkiLaåœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒæ—¶å¯¹å„ç§é€šç”¨å¤šæ¨¡æ€åŸºå‡†è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMåœ¨å¤„ç†éœ€è¦è§†è§‰æƒ³è±¡çš„ä»»åŠ¡æ—¶ï¼Œä¾èµ–äºå¤–éƒ¨å·¥å…·æˆ–ç”Ÿæˆå›¾åƒï¼Œè¿™é™åˆ¶äº†å…¶çµæ´»æ€§å’Œæ•ˆç‡ã€‚å®ƒä»¬æ— æ³•åƒäººç±»ä¸€æ ·ï¼Œåœ¨ç»Ÿä¸€çš„æ€ç»´ç©ºé—´ä¸­è¿›è¡Œè§†è§‰å’Œæ–‡æœ¬çš„æ— ç¼äº¤äº’ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºç¼ºä¹ä¸€ç§å†…åœ¨çš„ã€ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSkiLaçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰ä¿¡æ¯è¡¨ç¤ºä¸ºè¿ç»­çš„æ½œåœ¨è‰å›¾tokenï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°MLLMçš„è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ–‡æœ¬æ€è€ƒå’Œè§†è§‰è‰å›¾ä¹‹é—´åŠ¨æ€åˆ‡æ¢ï¼Œå®ç°è§†è§‰å’Œæ–‡æœ¬çš„ç»Ÿä¸€æ¨ç†ã€‚è¿™ç§è®¾è®¡æ¨¡ä»¿äº†äººç±»å¤§è„‘ä¸­è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯åœ¨ç»Ÿä¸€ç©ºé—´ä¸­äº¤äº’çš„æ–¹å¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSkiLaçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ–‡æœ¬ç¼–ç å™¨ï¼šå°†æ–‡æœ¬è¾“å…¥ç¼–ç ä¸ºæ–‡æœ¬tokenåºåˆ—ã€‚2) è§†è§‰ç¼–ç å™¨ï¼šå°†è§†è§‰è¾“å…¥ç¼–ç ä¸ºè§†è§‰ç‰¹å¾ã€‚3) æ½œåœ¨è‰å›¾ç”Ÿæˆå™¨ï¼šåŸºäºæ–‡æœ¬tokenå’Œè§†è§‰ç‰¹å¾ï¼Œç”Ÿæˆæ½œåœ¨è‰å›¾tokenåºåˆ—ã€‚4) è‡ªå›å½’è§£ç å™¨ï¼šäº¤æ›¿ç”Ÿæˆæ–‡æœ¬tokenå’Œæ½œåœ¨è‰å›¾tokenï¼Œè¿›è¡Œå¤šæ­¥æ¨ç†ã€‚5) è§†è§‰è¯­ä¹‰é‡å»ºæ¨¡å—ï¼šç”¨äºç¡®ä¿æ½œåœ¨è‰å›¾tokençš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSkiLaæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå®ƒå°†è§†è§‰æƒ³è±¡è¿‡ç¨‹è¡¨ç¤ºä¸ºè¿ç»­çš„æ½œåœ¨åµŒå…¥ï¼Œå¹¶å°†å…¶èå…¥åˆ°MLLMçš„è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ä¾èµ–äºç¦»æ•£çš„å¤–éƒ¨å·¥å…·æˆ–ç”Ÿæˆå›¾åƒçš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚SkiLaå®ç°äº†è§†è§‰å’Œæ–‡æœ¬çš„ç»Ÿä¸€è¡¨ç¤ºå’Œæ¨ç†ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šSkiLaçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ½œåœ¨è‰å›¾tokençš„è¡¨ç¤ºæ–¹å¼ï¼šä½¿ç”¨è¿ç»­çš„å‘é‡è¡¨ç¤ºè§†è§‰ä¿¡æ¯ï¼Œå…è®¸æ¨¡å‹è¿›è¡Œç»†ç²’åº¦çš„è§†è§‰æ¨ç†ã€‚2) è§†è§‰è¯­ä¹‰é‡å»ºæŸå¤±ï¼šç”¨äºçº¦æŸæ½œåœ¨è‰å›¾tokençš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œç¡®ä¿å…¶èƒ½å¤Ÿå‡†ç¡®åœ°è¡¨è¾¾è§†è§‰ä¿¡æ¯ã€‚3) æ–‡æœ¬æ€è€ƒå’Œè§†è§‰è‰å›¾æ¨¡å¼çš„åŠ¨æ€åˆ‡æ¢æœºåˆ¶ï¼šå…è®¸æ¨¡å‹æ ¹æ®ä»»åŠ¡éœ€æ±‚çµæ´»åœ°è°ƒæ•´æ¨ç†è¿‡ç¨‹ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/method.jpg" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/hyper.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16584v1/img/case_geo.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

SkiLaåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šï¼ŒSkiLaçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€ä½³æ¨¡å‹æé«˜äº†5%ã€‚åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šï¼ŒSkiLaèƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸã€æ›´ç¬¦åˆç”¨æˆ·æ„å›¾çš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSkiLaå…·æœ‰å¼ºå¤§çš„è§†è§‰æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SkiLaå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒç¼–è¾‘ã€æœºå™¨äººå¯¼èˆªå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚æœªæ¥ï¼ŒSkiLaæœ‰æœ›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.

