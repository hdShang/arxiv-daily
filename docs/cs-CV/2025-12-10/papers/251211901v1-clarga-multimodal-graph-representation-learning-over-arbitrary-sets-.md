---
layout: default
title: CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities
---

# CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.11901" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.11901v1</a>
  <a href="https://arxiv.org/pdf/2512.11901.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11901v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.11901v1', 'CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Santosh Patapati

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-10

**å¤‡æ³¨**: WACV; Supplementary material is available on CVF proceedings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CLARGAï¼šæå‡ºä¸€ç§é€šç”¨çš„å¤šæ¨¡æ€å›¾è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºä»»æ„æ¨¡æ€ç»„åˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `å›¾ç¥ç»ç½‘ç»œ` `æ³¨æ„åŠ›æœºåˆ¶` `è¡¨ç¤ºå­¦ä¹ ` `è·¨æ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶ï¼Œéš¾ä»¥é€‚åº”ä¸åŒæ¨¡æ€ç»„åˆï¼Œä¸”è®¡ç®—å¤æ‚åº¦é«˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. CLARGAé€šè¿‡æ„å»ºæ³¨æ„åŠ›åŠ æƒå›¾ï¼Œå­¦ä¹ æ¨¡æ€é—´çš„ç›¸äº’å½±å“ï¼Œå¹¶åˆ©ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼Œå®ç°é«˜æ•ˆèåˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCLARGAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¯¹ç¼ºå¤±æ¨¡æ€å’Œå™ªå£°è¾“å…¥å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šç”¨çš„å¤šæ¨¡æ€èåˆæ¶æ„CLARGAï¼Œç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œå®ƒå¯ä»¥åœ¨ä¸æ”¹å˜åº•å±‚æ¡†æ¶çš„æƒ…å†µä¸‹å¤„ç†ä»»æ„æ•°é‡å’Œç±»å‹çš„æ¨¡æ€ã€‚ç»™å®šä¸€ä¸ªæœ‰ç›‘ç£æ•°æ®é›†ï¼ŒCLARGAå¯ä»¥åº”ç”¨äºå‡ ä¹ä»»ä½•æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œä»¥èåˆä¸åŒçš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œä¾›ä¸‹æ¸¸å±‚å¤„ç†ã€‚CLARGAé€šè¿‡æ„å»ºä¸€ä¸ªæ³¨æ„åŠ›åŠ æƒå›¾æ¥å­¦ä¹ æ¨¡æ€ä¹‹é—´å¦‚ä½•ç›¸äº’å½±å“ï¼Œå¹¶åœ¨è¯¥å›¾ä¸Šä½¿ç”¨å¤šå¤´å›¾æ³¨æ„åŠ›ç½‘ç»œä¼ é€’æ¶ˆæ¯ï¼Œä»è€Œå®ç°æ ·æœ¬çº§åˆ«çš„æ¨¡æ€èåˆã€‚è¿™ç§è®¾è®¡ä¸ä»…ä½¿CLARGAå…·æœ‰é«˜åº¦çš„é€‚åº”æ€§ï¼Œå› ä¸ºå®ƒä¸ºä¸åŒçš„æ ·æœ¬æ„å»ºç‹¬ç‰¹çš„å›¾ï¼Œè€Œä¸”éšç€æ¨¡æ€æ•°é‡çš„å¢é•¿ï¼Œå®ƒè¿˜èƒ½å®ç°äºšäºŒæ¬¡å¤æ‚åº¦çš„æœ‰æ•ˆèåˆã€‚é€šè¿‡å¯å­¦ä¹ çš„æ©ç ï¼Œå®ƒè¿˜å¯ä»¥é€‚åº”ç¼ºå¤±çš„æ¨¡æ€è¾“å…¥ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ··åˆç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œè¯¥ç›®æ ‡å°†æœ‰ç›‘ç£ä»»åŠ¡æŸå¤±ä¸å¯¹æ¯”InfoNCEæŸå¤±ç›¸ç»“åˆï¼Œä»è€Œæé«˜è·¨æ¨¡æ€ä¸€è‡´æ€§å’Œå¯¹å™ªå£°è¾“å…¥çš„é²æ£’æ€§ã€‚æˆ‘ä»¬åœ¨æ¶µç›–é‡‘èã€äººæœºäº¤äº’ã€é€šç”¨å¤šåª’ä½“åˆ†ç±»å’Œæƒ…æ„Ÿè®¡ç®—çš„7ä¸ªæ•°æ®é›†ä¸Šçš„å„ç§å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­è¯æ˜äº†CLARGAçš„æœ‰æ•ˆæ€§ã€‚å®ƒå§‹ç»ˆä¼˜äºåŸºçº¿æ¨¡å‹ã€æœ€å…ˆè¿›çš„æ¨¡å‹å’Œæ¶ˆèå®éªŒã€‚é¢å¤–çš„å®éªŒä¹Ÿè¯æ˜äº†å®ƒå¯¹ç¼ºå¤±è¾“å…¥çš„é²æ£’æ€§ä»¥åŠåœ¨å°ä¼—ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²çš„èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒCLARGAå¯ä»¥å¾ˆå®¹æ˜“åœ°æ’å…¥åˆ°æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ï¼Œä»¥æœ‰æ•ˆåœ°å­¦ä¹ å„ç§ä»»åŠ¡çš„è¡¨ç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€èåˆæ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šæ¨¡æ€ç»„åˆè¿›è¡Œè®¾è®¡ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œéšç€æ¨¡æ€æ•°é‡çš„å¢åŠ ï¼Œèåˆè¿‡ç¨‹çš„è®¡ç®—å¤æ‚åº¦ä¹Ÿä¼šæ˜¾è‘—å¢åŠ ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨ã€‚å¦‚ä½•è®¾è®¡ä¸€ç§é€šç”¨çš„ã€é«˜æ•ˆçš„å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCLARGAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å›¾ç»“æ„æ¥å»ºæ¨¡ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªæ ·æœ¬ï¼ŒCLARGAæ„å»ºä¸€ä¸ªä»¥æ¨¡æ€ç‰¹å¾ä¸ºèŠ‚ç‚¹çš„å›¾ï¼Œå¹¶ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ èŠ‚ç‚¹ä¹‹é—´çš„è¾¹æƒé‡ï¼Œä»è€Œè¡¨ç¤ºæ¨¡æ€ä¹‹é—´çš„ç›¸äº’å½±å“ã€‚ç„¶åï¼Œåˆ©ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œåœ¨è¯¥å›¾ä¸Šè¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼Œå®ç°æ¨¡æ€ä¿¡æ¯çš„èåˆã€‚è¿™ç§åŸºäºå›¾çš„èåˆæ–¹å¼å…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯ä»¥é€‚åº”ä»»æ„æ•°é‡å’Œç±»å‹çš„æ¨¡æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLARGAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šç”¨äºæå–æ¯ä¸ªæ¨¡æ€çš„ç‰¹å¾è¡¨ç¤ºï¼›2) å›¾æ„å»ºæ¨¡å—ï¼šåŸºäºæ¨¡æ€ç‰¹å¾æ„å»ºæ³¨æ„åŠ›åŠ æƒå›¾ï¼›3) å›¾æ³¨æ„åŠ›ç½‘ç»œæ¨¡å—ï¼šåœ¨è¯¥å›¾ä¸Šè¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼Œèåˆæ¨¡æ€ä¿¡æ¯ï¼›4) é¢„æµ‹æ¨¡å—ï¼šåŸºäºèåˆåçš„ç‰¹å¾è¿›è¡Œé¢„æµ‹ã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLARGAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶åŸºäºå›¾çš„æ¨¡æ€èåˆæ–¹å¼ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè¿æ¥æˆ–æ³¨æ„åŠ›æœºåˆ¶çš„èåˆæ–¹æ³•ç›¸æ¯”ï¼ŒCLARGAèƒ½å¤Ÿæ›´çµæ´»åœ°å»ºæ¨¡æ¨¡æ€ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨æ¨¡æ€ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒCLARGAè¿˜å¼•å…¥äº†å¯å­¦ä¹ çš„æ©ç æœºåˆ¶ï¼Œä»¥é€‚åº”ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCLARGAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¤šå¤´å›¾æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼Œä»¥æ•æ‰ä¸åŒæ–¹é¢çš„æ¨¡æ€å…³ç³»ï¼›2) é‡‡ç”¨InfoNCEæŸå¤±æ¥æé«˜è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼›3) ä½¿ç”¨å¯å­¦ä¹ çš„æ©ç æ¥å¤„ç†ç¼ºå¤±æ¨¡æ€ï¼›4) æ··åˆäº†æœ‰ç›‘ç£ä»»åŠ¡æŸå¤±å’Œå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CLARGAåœ¨7ä¸ªä¸åŒçš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†é‡‘èã€äººæœºäº¤äº’ã€é€šç”¨å¤šåª’ä½“åˆ†ç±»å’Œæƒ…æ„Ÿè®¡ç®—ç­‰å¤šä¸ªé¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLARGAåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹å’Œæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒCLARGAçš„æ€§èƒ½æå‡è¶…è¿‡äº†5%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¯æ˜äº†CLARGAå¯¹ç¼ºå¤±æ¨¡æ€å’Œå™ªå£°è¾“å…¥çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CLARGAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚é‡‘èé¢†åŸŸçš„é£é™©é¢„æµ‹ã€äººæœºäº¤äº’é¢†åŸŸçš„æƒ…æ„Ÿè¯†åˆ«ã€å¤šåª’ä½“å†…å®¹ç†è§£ç­‰ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæä¾›äº†ä¸€ç§é€šç”¨çš„å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œå¯ä»¥æ–¹ä¾¿åœ°åº”ç”¨äºå„ç§æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å°†CLARGAåº”ç”¨äºæ›´å¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶æ¢ç´¢æ›´æœ‰æ•ˆçš„å›¾ç»“æ„å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

