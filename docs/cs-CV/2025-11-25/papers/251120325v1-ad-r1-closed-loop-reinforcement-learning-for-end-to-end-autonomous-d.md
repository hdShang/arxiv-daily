---
layout: default
title: AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models
---

# AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20325" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20325v1</a>
  <a href="https://arxiv.org/pdf/2511.20325.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20325v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.20325v1', 'AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyi Yan, Tao Tang, Xingtai Gui, Yongkang Li, Jiasen Zhesng, Weiyao Huang, Lingdong Kong, Wencheng Han, Xia Zhou, Xueyang Zhang, Yifei Zhan, Kun Zhan, Cheng-zhong Xu, Jianbing Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AD-R1ï¼šåŸºäºå…¬æ­£ä¸–ç•Œæ¨¡å‹çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶é—­ç¯å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `é£é™©é¢„æµ‹` `åäº‹å®åˆæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”±äºä¸–ç•Œæ¨¡å‹ä¸­å›ºæœ‰çš„ä¹è§‚åå·®ï¼Œéš¾ä»¥ä¿è¯å®‰å…¨æ€§å’Œå¤„ç†é•¿å°¾äº‹ä»¶ã€‚
2. æå‡ºä¸€ç§åŸºäºå…¬æ­£ä¸–ç•Œæ¨¡å‹çš„åè®­ç»ƒç­–ç•¥æ”¹è¿›æ¡†æ¶ï¼Œé€šè¿‡åäº‹å®åˆæˆæ•°æ®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹å±é™©ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹å¤±è´¥æ–¹é¢ä¼˜äºåŸºçº¿ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å®‰å…¨è¿è§„è¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹æœ‰æœ›ç›´æ¥ä»ä¼ æ„Ÿå™¨æ•°æ®ä¸­å­¦ä¹ å¤æ‚è¡Œä¸ºï¼Œä½†é¢ä¸´å®‰å…¨æ€§å’Œå¤„ç†é•¿å°¾äº‹ä»¶çš„å…³é”®æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ (RL)ä¸ºå…‹æœè¿™äº›é™åˆ¶æä¾›äº†ä¸€æ¡æœ‰å¸Œæœ›çš„é€”å¾„ï¼Œä½†å…¶åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æˆåŠŸä»ç„¶éš¾ä»¥æ‰æ‘¸ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªé˜»ç¢è¿™ä¸€è¿›å±•çš„æ ¹æœ¬ç¼ºé™·ï¼šç”¨äºRLçš„ä¸–ç•Œæ¨¡å‹ä¸­å­˜åœ¨æ ¹æ·±è’‚å›ºçš„ä¹è§‚åå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå›´ç»•å…¬æ­£ä¸–ç•Œæ¨¡å‹æ„å»ºçš„åè®­ç»ƒç­–ç•¥æ”¹è¿›æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯æ•™ä¼šè¿™ä¸ªæ¨¡å‹è¯šå®åœ°é¢å¯¹å±é™©ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°é¢–çš„æ•°æ®åˆæˆç®¡é“â€”â€”åäº‹å®åˆæˆæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥ç®¡é“ç³»ç»Ÿåœ°ç”Ÿæˆäº†ä¸°å¯Œçš„ã€çœ‹ä¼¼åˆç†çš„ç¢°æ’å’Œè¶Šé‡äº‹ä»¶è¯¾ç¨‹ã€‚è¿™ä½¿å¾—æ¨¡å‹ä»è¢«åŠ¨çš„åœºæ™¯è¡¥å…¨å™¨è½¬å˜ä¸ºçœŸå®çš„é¢„æµ‹å™¨ï¼Œä¿æŒäº†è¡ŒåŠ¨å’Œç»“æœä¹‹é—´çš„å› æœè”ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™ä¸ªå…¬æ­£çš„ä¸–ç•Œæ¨¡å‹é›†æˆåˆ°æˆ‘ä»¬çš„é—­ç¯RLæ¡†æ¶ä¸­ï¼Œå®ƒåœ¨å…¶ä¸­å……å½“å†…éƒ¨è¯„è®ºå‘˜ã€‚åœ¨æ”¹è¿›è¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“æŸ¥è¯¢è¯„è®ºå‘˜ä»¥â€œæ¢¦æƒ³â€å€™é€‰è¡ŒåŠ¨çš„ç»“æœã€‚é€šè¿‡åŒ…æ‹¬æ–°çš„é£é™©é¢„æµ‹åŸºå‡†åœ¨å†…çš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨é¢„æµ‹å¤±è´¥æ–¹é¢æ˜æ˜¾ä¼˜äºåŸºçº¿ã€‚å› æ­¤ï¼Œå½“ç”¨ä½œè¯„è®ºå‘˜æ—¶ï¼Œå®ƒèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿä¸­çš„å®‰å…¨è¿è§„è¡Œä¸ºï¼Œè¯æ˜äº†æ•™ä¼šæ¨¡å‹æ¢¦æƒ³å±é™©æ˜¯æ„å»ºçœŸæ­£å®‰å…¨å’Œæ™ºèƒ½çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“çš„å…³é”®ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ–¹æ³•ï¼Œå…¶ä¸–ç•Œæ¨¡å‹å¾€å¾€å­˜åœ¨ä¹è§‚åå·®ï¼Œå³å€¾å‘äºä½ä¼°å±é™©æƒ…å†µå‘ç”Ÿçš„æ¦‚ç‡ï¼Œå¯¼è‡´æ™ºèƒ½ä½“åœ¨è®­ç»ƒå’Œå®é™…åº”ç”¨ä¸­åšå‡ºä¸å®‰å…¨çš„å†³ç­–ã€‚å°¤å…¶æ˜¯åœ¨å¤„ç†ç½•è§ä½†å±é™©çš„é•¿å°¾äº‹ä»¶æ—¶ï¼Œè¿™ç§åå·®ä¼šæ›´åŠ æ˜æ˜¾ã€‚å› æ­¤ï¼Œå¦‚ä½•æ„å»ºä¸€ä¸ªèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹å±é™©æƒ…å†µçš„ä¸–ç•Œæ¨¡å‹ï¼Œæ˜¯æå‡è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§çš„å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒä¸€ä¸ªâ€œå…¬æ­£â€çš„ä¸–ç•Œæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå¦‚å®åœ°é¢„æµ‹å±é™©æƒ…å†µï¼Œé¿å…ä¹è§‚åå·®ã€‚é€šè¿‡è®©æ¨¡å‹â€œæ¢¦æƒ³â€å„ç§å¯èƒ½å‘ç”Ÿçš„å±é™©åœºæ™¯ï¼Œå¹¶å­¦ä¹ è¿™äº›åœºæ™¯çš„åæœï¼Œä»è€Œæé«˜å…¶å¯¹é£é™©çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºè®©æ¨¡å‹è¿›è¡Œâ€œå‹åŠ›æµ‹è¯•â€ï¼Œä½¿å…¶åœ¨å„ç§æç«¯æƒ…å†µä¸‹éƒ½èƒ½ä¿æŒæ¸…é†’çš„å¤´è„‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAD-R1æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡åäº‹å®åˆæˆæ•°æ®è®­ç»ƒä¸€ä¸ªå…¬æ­£çš„ä¸–ç•Œæ¨¡å‹ã€‚ç„¶åï¼Œå°†è¯¥ä¸–ç•Œæ¨¡å‹é›†æˆåˆ°é—­ç¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œä½œä¸ºå†…éƒ¨è¯„è®ºå‘˜ï¼Œç”¨äºè¯„ä¼°å€™é€‰åŠ¨ä½œçš„å®‰å…¨æ€§ã€‚åœ¨ç­–ç•¥æ”¹è¿›é˜¶æ®µï¼Œæ™ºèƒ½ä½“ä¼šæŸ¥è¯¢ä¸–ç•Œæ¨¡å‹ï¼Œé¢„æµ‹ä¸åŒåŠ¨ä½œå¯èƒ½å¯¼è‡´çš„åæœï¼Œå¹¶é€‰æ‹©æœ€å®‰å…¨çš„åŠ¨ä½œæ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åäº‹å®åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºè®­ç»ƒå…¬æ­£çš„ä¸–ç•Œæ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–çœŸå®æ•°æ®çš„æ–¹æ³•ä¸åŒï¼Œåäº‹å®åˆæˆèƒ½å¤Ÿç³»ç»Ÿåœ°ç”Ÿæˆå„ç§å¯èƒ½å‘ç”Ÿçš„å±é™©åœºæ™¯ï¼ŒåŒ…æ‹¬ç¢°æ’å’Œè¶Šé‡äº‹ä»¶ï¼Œä»è€Œæœ‰æ•ˆåœ°å…‹æœäº†é•¿å°¾äº‹ä»¶æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå°†ä¸–ç•Œæ¨¡å‹ä½œä¸ºå†…éƒ¨è¯„è®ºå‘˜ï¼Œä¹Ÿä¸ºå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å®‰å…¨æ€§è¯„ä¼°æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåäº‹å®åˆæˆæ•°æ®ç”Ÿæˆç®¡é“æ˜¯è¯¥æ–¹æ³•çš„æ ¸å¿ƒã€‚å®ƒé€šè¿‡å¯¹çœŸå®åœºæ™¯è¿›è¡Œä¿®æ”¹ï¼Œä¾‹å¦‚æ”¹å˜è½¦è¾†çš„è¿åŠ¨è½¨è¿¹ã€å¢åŠ éšœç¢ç‰©ç­‰ï¼Œæ¥ç”Ÿæˆå„ç§å±é™©åœºæ™¯ã€‚ä¸ºäº†ä¿è¯åˆæˆæ•°æ®çš„åˆç†æ€§ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç³»åˆ—çº¦æŸæ¡ä»¶ï¼Œä¾‹å¦‚ä¿æŒåœºæ™¯çš„ç‰©ç†ä¸€è‡´æ€§ã€‚åœ¨è®­ç»ƒä¸–ç•Œæ¨¡å‹æ—¶ï¼Œé‡‡ç”¨äº†å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹åŒºåˆ†ä¸åŒçš„å±é™©åœºæ™¯ï¼Œå¹¶å‡†ç¡®é¢„æµ‹å…¶åæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ¨¡å‹åœ¨æ–°çš„é£é™©é¢„æµ‹åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨é¢„æµ‹å¤±è´¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œä½¿ç”¨è¯¥æ¨¡å‹ä½œä¸ºè¯„è®ºå‘˜çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œå…¶å®‰å…¨è¿è§„è¡Œä¸ºæ˜¾è‘—å‡å°‘ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§æå‡ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£é™©åœºæ™¯ä¸‹çš„å†³ç­–ã€‚é€šè¿‡æ›´å‡†ç¡®çš„é£é™©é¢„æµ‹ï¼Œè‡ªåŠ¨é©¾é©¶è½¦è¾†èƒ½å¤Ÿæ›´å¥½åœ°é¿å…äº‹æ•…ï¼Œæé«˜è¡Œé©¶å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚æ— äººæœºã€æœåŠ¡æœºå™¨äººç­‰ï¼Œç”¨äºæé«˜å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.

