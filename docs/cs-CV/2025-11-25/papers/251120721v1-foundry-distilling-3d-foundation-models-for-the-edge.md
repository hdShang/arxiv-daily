---
layout: default
title: Foundry: Distilling 3D Foundation Models for the Edge
---

# Foundry: Distilling 3D Foundation Models for the Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20721" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20721v1</a>
  <a href="https://arxiv.org/pdf/2511.20721.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20721v1" onclick="toggleFavorite(this, '2511.20721v1', 'Foundry: Distilling 3D Foundation Models for the Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guillaume Letellier, Siddharth Srivastava, FrÃ©dÃ©ric Jurie, Gaurav Sharma

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Foundryï¼šè¾¹ç¼˜è®¾å¤‡3DåŸºç¡€æ¨¡å‹è’¸é¦ï¼Œä¿æŒé€šç”¨æ€§çš„åŒæ—¶å®ç°é«˜æ•ˆå‹ç¼©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dç‚¹äº‘` `åŸºç¡€æ¨¡å‹` `çŸ¥è¯†è’¸é¦` `æ¨¡å‹å‹ç¼©` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºç¡€æ¨¡å‹ä½“ç§¯åºå¤§ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œè€Œä¼ ç»ŸçŸ¥è¯†è’¸é¦ä¼šç‰ºç‰²æ¨¡å‹çš„é€šç”¨æ€§ã€‚
2. è®ºæ–‡æå‡ºåŸºç¡€æ¨¡å‹è’¸é¦(FMD)èŒƒå¼ï¼Œé€šè¿‡è®­ç»ƒå­¦ç”Ÿæ¨¡å‹å­¦ä¹ å‹ç¼©çš„SuperTokenæ¥é‡å»ºæ•™å¸ˆæ¨¡å‹çš„tokençº§è¡¨ç¤ºï¼Œä¿ç•™é€šç”¨æ€§ã€‚
3. Foundryæ˜¯FMDåœ¨3Dç‚¹äº‘ä¸Šçš„é¦–æ¬¡å®ç°ï¼Œåœ¨ä¿æŒæ€§èƒ½æ¥è¿‘å®Œæ•´åŸºç¡€æ¨¡å‹çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†tokenæ•°é‡å’ŒFLOPsã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé€šè¿‡è‡ªç›‘ç£å­¦ä¹ (SSL)é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹å·²æˆä¸ºå¼ºå¤§çš„é€šç”¨ç‰¹å¾æå–å™¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬å·¨å¤§çš„å°ºå¯¸å’Œè®¡ç®—æˆæœ¬ä½¿å…¶éš¾ä»¥éƒ¨ç½²åœ¨æœºå™¨äººå’ŒAR/VRå¤´æ˜¾ç­‰è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚ç°æœ‰çš„å‹ç¼©æŠ€æœ¯ï¼Œå¦‚æ ‡å‡†çŸ¥è¯†è’¸é¦ï¼Œè™½ç„¶å¯ä»¥åˆ›å»ºé«˜æ•ˆçš„â€œä¸“å®¶â€æ¨¡å‹ï¼Œä½†ç‰ºç‰²äº†åŸºç¡€æ¨¡å‹è‡³å…³é‡è¦çš„ã€ä¸‹æ¸¸ä»»åŠ¡æ— å…³çš„é€šç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†åŸºç¡€æ¨¡å‹è’¸é¦(FMD)è¿™ä¸€æ–°èŒƒå¼ï¼Œç”¨äºå°†å¤§å‹SSLæ¨¡å‹å‹ç¼©æˆç´§å‡‘ã€é«˜æ•ˆä¸”å¿ å®çš„ä»£ç†ï¼ŒåŒæ—¶ä¿ç•™å…¶é€šç”¨è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†Foundryï¼Œè¿™æ˜¯FMDåœ¨3Dç‚¹äº‘ä¸Šçš„é¦–æ¬¡å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•Foundryè®­ç»ƒä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹æ¥å­¦ä¹ ä¸€ç»„å‹ç¼©çš„SuperTokenï¼Œè¿™äº›SuperTokené‡å»ºæ•™å¸ˆæ¨¡å‹çš„tokençº§è¡¨ç¤ºï¼Œä»è€Œæ•è·å…¶æ½œåœ¨ç©ºé—´çš„ç´§å‡‘åŸºã€‚å•ä¸ªè’¸é¦æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ˆåˆ†ç±»ã€éƒ¨ä»¶åˆ†å‰²å’Œå°‘æ ·æœ¬åœºæ™¯ï¼‰ä¸­ä¿æŒå¼ºå¤§çš„å¯è¿ç§»æ€§ï¼Œæ¥è¿‘å®Œæ•´åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨æ˜æ˜¾æ›´å°‘çš„tokenå’ŒFLOPsï¼Œä½¿æ­¤ç±»æ¨¡å‹æ›´é€‚åˆåœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šéƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹3DåŸºç¡€æ¨¡å‹è™½ç„¶å…·æœ‰å¼ºå¤§çš„é€šç”¨ç‰¹å¾æå–èƒ½åŠ›ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å’Œè®¡ç®—å¤æ‚åº¦é™åˆ¶äº†å®ƒä»¬åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•è™½ç„¶å¯ä»¥å‹ç¼©æ¨¡å‹ï¼Œä½†å¾€å¾€ä¼šç‰ºç‰²æ¨¡å‹çš„é€šç”¨æ€§ï¼Œä½¿å…¶æˆä¸ºç‰¹å®šä»»åŠ¡çš„â€œä¸“å®¶â€æ¨¡å‹ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŸºç¡€æ¨¡å‹è’¸é¦(FMD)æ¥å‹ç¼©å¤§å‹3DåŸºç¡€æ¨¡å‹ï¼Œä½¿å…¶åœ¨ä¿æŒé€šç”¨æ€§çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚FMDçš„å…³é”®åœ¨äºå­¦ä¹ ä¸€ç»„å‹ç¼©çš„SuperTokenï¼Œè¿™äº›SuperTokenèƒ½å¤Ÿæœ‰æ•ˆåœ°é‡å»ºæ•™å¸ˆæ¨¡å‹çš„tokençº§è¡¨ç¤ºï¼Œä»è€Œæ•è·å…¶æ½œåœ¨ç©ºé—´çš„ç´§å‡‘åŸºã€‚è¿™æ ·ï¼Œå­¦ç”Ÿæ¨¡å‹å°±å¯ä»¥é€šè¿‡æ›´å°‘çš„å‚æ•°å’Œè®¡ç®—é‡æ¥è·å¾—ä¸æ•™å¸ˆæ¨¡å‹ç›¸ä¼¼çš„è¡¨ç¤ºèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFoundryçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„3DåŸºç¡€æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼›2) è®¾è®¡ä¸€ä¸ªæ›´å°çš„å­¦ç”Ÿæ¨¡å‹ï¼›3) å¼•å…¥SuperTokençš„æ¦‚å¿µï¼Œå­¦ç”Ÿæ¨¡å‹å­¦ä¹ ç”Ÿæˆè¿™äº›SuperTokenï¼›4) ä½¿ç”¨é‡å»ºæŸå¤±å‡½æ•°ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆçš„SuperTokenèƒ½å¤Ÿå°½å¯èƒ½åœ°é‡å»ºæ•™å¸ˆæ¨¡å‹çš„tokençº§è¡¨ç¤ºï¼›5) åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¯¹è’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºç¡€æ¨¡å‹è’¸é¦(FMD)è¿™ä¸€æ–°çš„è’¸é¦èŒƒå¼ï¼Œä»¥åŠSuperTokençš„æ¦‚å¿µã€‚ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸åŒï¼ŒFMDæ—¨åœ¨ä¿ç•™åŸºç¡€æ¨¡å‹çš„é€šç”¨æ€§ï¼Œè€Œä¸æ˜¯ä»…ä»…é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚SuperTokençš„è®¾è®¡ä½¿å¾—å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿä»¥æ›´ç´§å‡‘çš„æ–¹å¼å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å‹ç¼©ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸Šï¼Œè®ºæ–‡å¯èƒ½æ¶‰åŠä»¥ä¸‹å…³é”®è®¾è®¡ï¼š1) SuperTokençš„æ•°é‡å’Œç»´åº¦ï¼›2) å­¦ç”Ÿæ¨¡å‹çš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚Transformerçš„å±‚æ•°å’Œéšè—å±‚å¤§å°ï¼›3) é‡å»ºæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œä¾‹å¦‚å‡æ–¹è¯¯å·®æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼›4) è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¶…å‚æ•°è®¾ç½®ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€batch sizeå’Œepochæ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Foundryåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€éƒ¨ä»¶åˆ†å‰²å’Œå°‘æ ·æœ¬å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹åœ¨ä¿æŒæ€§èƒ½æ¥è¿‘å®Œæ•´åŸºç¡€æ¨¡å‹çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†tokenæ•°é‡å’ŒFLOPsã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼Œå­¦ç”Ÿæ¨¡å‹ä»…ä½¿ç”¨æ•™å¸ˆæ¨¡å‹10%çš„tokenï¼Œå°±èƒ½è¾¾åˆ°90%ä»¥ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººã€AR/VRã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡å°†å¤§å‹3DåŸºç¡€æ¨¡å‹å‹ç¼©åˆ°è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„3Dæ„ŸçŸ¥å’Œç†è§£ï¼Œä¾‹å¦‚åœ¨æœºå™¨äººå¯¼èˆªã€ç‰©ä½“è¯†åˆ«ã€åœºæ™¯é‡å»ºç­‰æ–¹é¢ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºå¼€å‘æ›´è½»é‡çº§çš„3Dæ¨¡å‹ï¼Œé™ä½å­˜å‚¨å’Œä¼ è¾“æˆæœ¬ï¼Œä¿ƒè¿›3DæŠ€æœ¯çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.

