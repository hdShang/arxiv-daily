---
layout: default
title: "Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention"
---

# Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20032" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20032v1</a>
  <a href="https://arxiv.org/pdf/2511.20032.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20032v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.20032v1', 'Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng, Zhixing Tan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

**å¤‡æ³¨**: Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶ï¼ˆVGAï¼‰ï¼Œç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æ³¨æ„åŠ›` `å¹»è§‰ç¼“è§£` `è§†è§‰å¼•å¯¼` `å›¾åƒæè¿°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§†è§‰æ³¨æ„åŠ›å®šä½èƒ½åŠ›æœ‰é™ï¼Œå¯¼è‡´äº§ç”Ÿå¹»è§‰ï¼Œå½±å“äº†æ¨¡å‹æ€§èƒ½ã€‚
2. æå‡ºè§†è§‰å¼•å¯¼æ³¨æ„åŠ›ï¼ˆVGAï¼‰æœºåˆ¶ï¼Œé€šè¿‡æ„å»ºç²¾ç¡®çš„è§†è§‰åŸºç¡€ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ç›¸å…³è§†è§‰åŒºåŸŸï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVGAåœ¨å¤šä¸ªMLLMå’Œå¹»è§‰åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å»å¹»è§‰æ€§èƒ½ï¼Œä¸”å»¶è¿Ÿå¼€é”€æä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰æ³¨æ„åŠ›æ˜¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è§£é‡Šè§†è§‰ä¿¡æ¯çš„ä¸»è¦æœºåˆ¶ï¼Œä½†å…¶æœ‰é™çš„å®šä½èƒ½åŠ›å¸¸å¸¸å¯¼è‡´å¹»è§‰ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå°½ç®¡MLLMå¯ä»¥å‡†ç¡®åœ°ä»è§†è§‰tokenä¸­æå–è§†è§‰è¯­ä¹‰ï¼Œä½†åœ¨åç»­æ¨ç†è¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨è¿™ä¸€ä¼˜åŠ¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…è®­ç»ƒçš„æ–¹æ³•ï¼Œå³è§†è§‰å¼•å¯¼æ³¨æ„åŠ›ï¼ˆVGAï¼‰ã€‚VGAé¦–å…ˆé€šè¿‡åˆ©ç”¨è§†è§‰tokençš„è¯­ä¹‰å†…å®¹æ„å»ºç²¾ç¡®çš„è§†è§‰åŸºç¡€ï¼Œç„¶åä½¿ç”¨è¿™ä¸ªåŸºç¡€æ¥å¼•å¯¼æ¨¡å‹å…³æ³¨ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­ï¼ŒVGAé€šè¿‡æŠ‘åˆ¶å·²ç»æè¿°è¿‡çš„åŒºåŸŸï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€åœ°ç»†åŒ–è¿™ç§å¼•å¯¼ã€‚åœ¨VGAä¸­ï¼Œæ¯ä¸ªtokenä»…ç»å†ä¸€æ¬¡å‰å‘ä¼ é€’ï¼Œå¼•å…¥çš„å»¶è¿Ÿå¼€é”€å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œä»…ä¸º4.36%ã€‚æ­¤å¤–ï¼ŒVGAå®Œå…¨å…¼å®¹é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°ï¼Œå¦‚FlashAttentionã€‚åœ¨å„ç§MLLMå’Œå¤šä¸ªå¹»è§‰åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVGAå®ç°äº†æœ€å…ˆè¿›çš„å»å¹»è§‰æ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¯å®ï¼Œæ˜¾å¼çš„è§†è§‰å¼•å¯¼åœ¨å¢å¼ºMLLMçš„è§†è§‰ç†è§£èƒ½åŠ›æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†è§†è§‰ä¿¡æ¯æ—¶ï¼Œè™½ç„¶èƒ½å¤Ÿæå–è§†è§‰è¯­ä¹‰ï¼Œä½†ç”±äºè§†è§‰æ³¨æ„åŠ›æœºåˆ¶çš„å®šä½èƒ½åŠ›ä¸è¶³ï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå³ç”Ÿæˆä¸å›¾åƒå†…å®¹ä¸ç¬¦çš„ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è§†è§‰tokençš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•å‡†ç¡®å…³æ³¨å›¾åƒä¸­çš„å…³é”®åŒºåŸŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰tokençš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ„å»ºç²¾ç¡®çš„è§†è§‰åŸºç¡€ï¼ˆVisual Groundingï¼‰ï¼Œç„¶ååˆ©ç”¨è¿™ä¸ªè§†è§‰åŸºç¡€æ¥å¼•å¯¼æ¨¡å‹çš„æ³¨æ„åŠ›ï¼Œä½¿å…¶æ›´åŠ å…³æ³¨å›¾åƒä¸­ç›¸å…³çš„åŒºåŸŸã€‚é€šè¿‡è¿™ç§æ˜¾å¼çš„è§†è§‰å¼•å¯¼ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹äº§ç”Ÿå¹»è§‰çš„å¯èƒ½æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVGAæ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šè§†è§‰åŸºç¡€æ„å»ºå’Œæ³¨æ„åŠ›å¼•å¯¼ã€‚é¦–å…ˆï¼Œåˆ©ç”¨è§†è§‰tokençš„è¯­ä¹‰å†…å®¹æ„å»ºè§†è§‰åŸºç¡€ï¼Œç¡®å®šå›¾åƒä¸­å„ä¸ªåŒºåŸŸçš„é‡è¦æ€§ã€‚ç„¶åï¼Œåœ¨æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œåˆ©ç”¨æ„å»ºçš„è§†è§‰åŸºç¡€æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨é‡è¦çš„è§†è§‰åŒºåŸŸã€‚åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­ï¼ŒVGAè¿˜å¼•å…¥äº†åŠ¨æ€ç»†åŒ–æœºåˆ¶ï¼ŒæŠ‘åˆ¶å·²ç»æè¿°è¿‡çš„åŒºåŸŸï¼Œé¿å…é‡å¤æè¿°ã€‚

**å…³é”®åˆ›æ–°**ï¼šVGAçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§å…è®­ç»ƒçš„è§†è§‰å¼•å¯¼æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰çš„MLLMï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒVGAé€šè¿‡æ˜¾å¼åœ°åˆ©ç”¨è§†è§‰tokençš„è¯­ä¹‰ä¿¡æ¯æ¥æ„å»ºè§†è§‰åŸºç¡€ï¼Œä»è€Œæ›´å‡†ç¡®åœ°å¼•å¯¼æ¨¡å‹çš„æ³¨æ„åŠ›ã€‚VGAä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå®ƒä¸æ˜¯é€šè¿‡éšå¼çš„æ–¹å¼å­¦ä¹ è§†è§‰æ³¨æ„åŠ›ï¼Œè€Œæ˜¯é€šè¿‡æ˜¾å¼çš„æ–¹å¼å¼•å¯¼æ¨¡å‹å…³æ³¨ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚

**å…³é”®è®¾è®¡**ï¼šVGAæ–¹æ³•ä¸­ï¼Œè§†è§‰åŸºç¡€çš„æ„å»ºæ–¹å¼æ˜¯å…³é”®ã€‚è®ºæ–‡ä¸­å…·ä½“å¦‚ä½•åˆ©ç”¨è§†è§‰tokençš„è¯­ä¹‰ä¿¡æ¯æ„å»ºè§†è§‰åŸºç¡€ï¼Œä»¥åŠå¦‚ä½•å°†è§†è§‰åŸºç¡€èå…¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œè¿™äº›ç»†èŠ‚å†³å®šäº†VGAçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€ç»†åŒ–æœºåˆ¶çš„è®¾è®¡ä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®ç‚¹ï¼Œéœ€è¦æœ‰æ•ˆåœ°æŠ‘åˆ¶å·²ç»æè¿°è¿‡çš„åŒºåŸŸï¼ŒåŒæ—¶é¿å…è¿‡åº¦æŠ‘åˆ¶å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚è®ºæ–‡ä¸­å…³äºè¿™äº›å…³é”®è®¾è®¡çš„å…·ä½“å®ç°ç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œè§†è§‰åŸºç¡€çš„è®¡ç®—æ–¹å¼ã€æ³¨æ„åŠ›æƒé‡çš„è°ƒæ•´ç­–ç•¥ã€åŠ¨æ€ç»†åŒ–æœºåˆ¶çš„å®ç°æ–¹å¼ï¼‰éœ€è¦è¿›ä¸€æ­¥æŸ¥é˜…åŸæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVGAåœ¨å¤šä¸ªMLLMå’Œå¹»è§‰åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å»å¹»è§‰æ€§èƒ½ã€‚VGAå¼•å…¥çš„å»¶è¿Ÿå¼€é”€æä½ï¼Œä»…ä¸º4.36%ï¼Œå¹¶ä¸”å®Œå…¨å…¼å®¹é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°ï¼Œå¦‚FlashAttentionã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVGAæ˜¯ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„å»å¹»è§‰æ–¹æ³•ï¼Œå…·æœ‰å¾ˆå¼ºçš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚é€šè¿‡å‡å°‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ï¼Œå¯ä»¥æé«˜è¿™äº›åº”ç”¨çš„å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å‡†ç¡®åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œåšå‡ºæ›´å®‰å…¨çš„å†³ç­–ã€‚åœ¨åŒ»ç–—è¯Šæ–­ä¸­ï¼Œå¯ä»¥è¾…åŠ©åŒ»ç”Ÿåˆ†æåŒ»å­¦å½±åƒï¼Œæé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual attention serves as the primary mechanism through which MLLMs interpret visual information; however, its limited localization capability often leads to hallucinations. We observe that although MLLMs can accurately extract visual semantics from visual tokens, they fail to fully leverage this advantage during subsequent inference. To address this limitation, we propose Vision-Guided Attention (VGA), a training-free method that first constructs precise visual grounding by exploiting the semantic content of visual tokens, and then uses this grounding to guide the model's focus toward relevant visual regions. In image captioning, VGA further refines this guidance dynamically during generation by suppressing regions that have already been described. In VGA, each token undergoes only a single forward pass, introducing a negligible latency overhead of just 4.36\%. In addition, VGA is fully compatible with efficient attention implementations such as FlashAttention. Extensive experiments across diverse MLLMs and multiple hallucination benchmarks demonstrate that VGA achieves state-of-the-art dehallucination performance. Further analysis confirms that explicit visual guidance plays a crucial role in enhancing the visual understanding capabilities of MLLMs.

