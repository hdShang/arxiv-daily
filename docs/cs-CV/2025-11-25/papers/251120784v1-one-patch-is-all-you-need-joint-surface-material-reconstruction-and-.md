---
layout: default
title: One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues
---

# One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20784" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20784v1</a>
  <a href="https://arxiv.org/pdf/2511.20784.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20784v1" onclick="toggleFavorite(this, '2511.20784v1', 'One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sindhuja Penchala, Gavin Money, Gabriel Marques, Samuel Wood, Jessica Kirschman, Travis Atkison, Shahram Rahimi, Noorbakhsh Amiri Golilarz

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

**å¤‡æ³¨**: 9 pages,3 figures, 5 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SMARCï¼šä»…éœ€å›¾åƒ10%åŒºåŸŸï¼Œå³å¯å®ç°è¡¨é¢æè´¨é‡å»ºä¸åˆ†ç±»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¡¨é¢æè´¨é‡å»º` `æè´¨åˆ†ç±»` `éƒ¨åˆ†å·ç§¯` `U-Net` `ç¨€ç–è§†è§‰ä¿¡æ¯` `æœºå™¨äºº` `å›¾åƒä¿®å¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æè´¨ç†è§£æ–¹æ³•ä¾èµ–ç¨ å¯†è§‚æµ‹ï¼Œé™åˆ¶äº†å…¶åœ¨å—é™æˆ–éƒ¨åˆ†è§†å›¾ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. SMARCæ¨¡å‹ä»…éœ€å›¾åƒçš„10%åŒºåŸŸï¼Œå³å¯åŒæ—¶å®Œæˆè¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ä»»åŠ¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSMARCåœ¨è¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSMARCçš„ç»Ÿä¸€æ¨¡å‹ï¼Œç”¨äºä»æå°‘çš„è§†è§‰è¾“å…¥ä¸­è¿›è¡Œè¡¨é¢æè´¨é‡å»ºå’Œåˆ†ç±»ã€‚é’ˆå¯¹æœºå™¨äººã€ä»¿çœŸå’Œæè´¨æ„ŸçŸ¥ç­‰åº”ç”¨ä¸­ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºç¨ å¯†æˆ–å®Œæ•´åœºæ™¯è§‚æµ‹çš„å±€é™æ€§ï¼ŒSMARCä»…éœ€å›¾åƒä¸­ä¸€ä¸ªè¿ç»­çš„10%åŒºåŸŸï¼Œå³å¯è¯†åˆ«å¹¶é‡å»ºå®Œæ•´çš„RGBè¡¨é¢ï¼ŒåŒæ—¶å¯¹æè´¨ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚è¯¥æ¶æ„ç»“åˆäº†éƒ¨åˆ†å·ç§¯U-Netå’Œä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå®ç°äº†åœ¨æç«¯è§‚æµ‹ç¨€ç–æƒ…å†µä¸‹çš„ç©ºé—´ä¿®å¤å’Œè¯­ä¹‰ç†è§£ã€‚åœ¨Touch and Goæ•°æ®é›†ä¸Šï¼ŒSMARCä¸åŒ…æ‹¬å·ç§¯è‡ªç¼–ç å™¨ã€Vision Transformer (ViT)ã€Masked Autoencoder (MAE)ã€Swin Transformerå’ŒDETRåœ¨å†…çš„äº”ç§æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå–å¾—äº†state-of-the-artçš„ç»“æœï¼ŒPSNRè¾¾åˆ°17.55 dBï¼Œæè´¨åˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°85.10%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œéƒ¨åˆ†å·ç§¯åœ¨ç¼ºå¤±æ•°æ®ä¸‹çš„ç©ºé—´æ¨ç†æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶ä¸ºæç®€è§†è§‰è¡¨é¢ç†è§£å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è¡¨é¢æè´¨ç†è§£æ–¹æ³•é€šå¸¸éœ€è¦å®Œæ•´çš„å›¾åƒæˆ–è€…ç¨ å¯†çš„è§‚æµ‹æ•°æ®ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œæˆ–è€…èµ„æºå—é™çš„åµŒå…¥å¼ç³»ç»Ÿä¸­ï¼Œéš¾ä»¥æ»¡è¶³ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æåº¦ç¨€ç–çš„è§†è§‰ä¿¡æ¯ä¸‹ï¼Œå‡†ç¡®åœ°é‡å»ºè¡¨é¢æè´¨å¹¶è¿›è¡Œåˆ†ç±»ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•æœ‰æ•ˆåˆ©ç”¨å±€éƒ¨ä¿¡æ¯è¿›è¡Œå…¨å±€æ¨ç†ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSMARCçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éƒ¨åˆ†å·ç§¯ï¼ˆPartial Convolutionï¼‰æ¥å¤„ç†ç¼ºå¤±æ•°æ®ï¼Œå¹¶ç»“åˆU-Netçš„ç»“æ„è¿›è¡Œç©ºé—´ä¿¡æ¯çš„ä¼ é€’å’Œé‡å»ºã€‚é€šè¿‡éƒ¨åˆ†å·ç§¯ï¼Œæ¨¡å‹å¯ä»¥åªå…³æ³¨å·²çŸ¥çš„åƒç´ ä¿¡æ¯ï¼Œé¿å…ç¼ºå¤±åƒç´ å¸¦æ¥çš„å¹²æ‰°ã€‚åŒæ—¶ï¼ŒU-Netçš„è·³è·ƒè¿æ¥å¯ä»¥æœ‰æ•ˆåœ°å°†ä½å±‚ç‰¹å¾ä¼ é€’åˆ°é«˜å±‚ï¼Œä»è€Œæ›´å¥½åœ°é‡å»ºå›¾åƒã€‚æ­¤å¤–ï¼Œå¢åŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥åŒæ—¶è¿›è¡Œè¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSMARCçš„æ•´ä½“æ¶æ„æ˜¯ä¸€ä¸ªåŸºäºU-Netçš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå…¶ä¸­ç¼–ç å™¨å’Œè§£ç å™¨éƒ½ä½¿ç”¨äº†éƒ¨åˆ†å·ç§¯å±‚ã€‚åœ¨ç¼–ç å™¨çš„æœ€åï¼Œè¿æ¥ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œç”¨äºé¢„æµ‹æè´¨ç±»åˆ«ã€‚æ•´ä¸ªæµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œè¾“å…¥ä¸€ä¸ªåŒ…å«10%åŒºåŸŸçš„å›¾åƒpatchï¼›ç„¶åï¼Œé€šè¿‡ç¼–ç å™¨æå–ç‰¹å¾ï¼›æ¥ç€ï¼Œé€šè¿‡è§£ç å™¨é‡å»ºå®Œæ•´çš„RGBå›¾åƒï¼›åŒæ—¶ï¼Œé€šè¿‡åˆ†ç±»å¤´é¢„æµ‹æè´¨ç±»åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šSMARCæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†éƒ¨åˆ†å·ç§¯åº”ç”¨äºè¡¨é¢æè´¨é‡å»ºå’Œåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¹¶ç»“åˆU-Netçš„ç»“æ„ï¼Œå®ç°äº†åœ¨æåº¦ç¨€ç–è§‚æµ‹ä¸‹çš„é«˜æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSMARCèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å±€éƒ¨ä¿¡æ¯è¿›è¡Œå…¨å±€æ¨ç†ï¼Œä»è€Œåœ¨ç¼ºå¤±å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½å¤Ÿå‡†ç¡®åœ°é‡å»ºè¡¨é¢æè´¨å¹¶è¿›è¡Œåˆ†ç±»ã€‚

**å…³é”®è®¾è®¡**ï¼šSMARCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨éƒ¨åˆ†å·ç§¯å±‚æ¥å¤„ç†ç¼ºå¤±æ•°æ®ï¼Œé¿å…æ— æ•ˆåƒç´ çš„å¹²æ‰°ï¼›2) ä½¿ç”¨U-Netç»“æ„è¿›è¡Œç©ºé—´ä¿¡æ¯çš„ä¼ é€’å’Œé‡å»ºï¼›3) å¢åŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥åŒæ—¶è¿›è¡Œè¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ï¼›4) æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±ï¼ˆä¾‹å¦‚L1æŸå¤±æˆ–PSNRï¼‰å’Œåˆ†ç±»æŸå¤±ï¼ˆä¾‹å¦‚äº¤å‰ç†µæŸå¤±ï¼‰ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å®é™…æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SMARCåœ¨Touch and Goæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„ç»“æœï¼ŒPSNRè¾¾åˆ°17.55 dBï¼Œæè´¨åˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°85.10%ã€‚ç›¸æ¯”äºå·ç§¯è‡ªç¼–ç å™¨ã€Vision Transformer (ViT)ã€Masked Autoencoder (MAE)ã€Swin Transformerå’ŒDETRç­‰åŸºçº¿æ¨¡å‹ï¼ŒSMARCåœ¨è¡¨é¢é‡å»ºå’Œæè´¨åˆ†ç±»ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¯æ˜äº†éƒ¨åˆ†å·ç§¯åœ¨å¤„ç†ç¼ºå¤±æ•°æ®æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SMARCåœ¨æœºå™¨äººæ“ä½œã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€æè´¨è¯†åˆ«ã€å·¥ä¸šæ£€æµ‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨SMARCä»å°‘é‡è§†è§‰ä¿¡æ¯ä¸­ç†è§£ç‰©ä½“è¡¨é¢æè´¨ï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡ŒæŠ“å–å’Œæ“ä½œã€‚åœ¨è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ä¸­ï¼ŒSMARCå¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„è¡¨é¢çº¹ç†ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒSMARCè¿˜å¯ä»¥åº”ç”¨äºå·¥ä¸šæ£€æµ‹ä¸­ï¼Œç”¨äºè¯†åˆ«äº§å“è¡¨é¢çš„ç¼ºé™·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding material surfaces from sparse visual cues is critical for applications in robotics, simulation, and material perception. However, most existing methods rely on dense or full-scene observations, limiting their effectiveness in constrained or partial view environment. To address this challenge, we introduce SMARC, a unified model for Surface MAterial Reconstruction and Classification from minimal visual input. By giving only a single 10% contiguous patch of the image, SMARC recognizes and reconstructs the full RGB surface while simultaneously classifying the material category. Our architecture combines a Partial Convolutional U-Net with a classification head, enabling both spatial inpainting and semantic understanding under extreme observation sparsity. We compared SMARC against five models including convolutional autoencoders [17], Vision Transformer (ViT) [13], Masked Autoencoder (MAE) [5], Swin Transformer [9], and DETR [2] using Touch and Go dataset [16] of real-world surface textures. SMARC achieves state-of-the-art results with a PSNR of 17.55 dB and a material classification accuracy of 85.10%. Our findings highlight the advantages of partial convolution in spatial reasoning under missing data and establish a strong foundation for minimal-vision surface understanding.

