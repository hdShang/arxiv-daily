---
layout: default
title: 3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding
---

# 3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20646" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20646v1</a>
  <a href="https://arxiv.org/pdf/2511.20646.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20646v1" onclick="toggleFavorite(this, '2511.20646v1', '3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoye Wang, Chen Tang, Xiangyu Yue, Wei-Hong Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

**å¤‡æ³¨**: 3D-aware Multi-task Learning, Cross-view Correlations, Code will be available at https://github.com/WeiHongLee/CrossView3DMTL

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè·¨è§†è§’ç›¸å…³æ€§çš„3Dæ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ ï¼Œç”¨äºå¯†é›†åœºæ™¯ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `å¤šä»»åŠ¡å­¦ä¹ ` `3Dæ„ŸçŸ¥` `è·¨è§†è§’ç›¸å…³æ€§` `ä»£ä»·ä½“` `åœºæ™¯ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MTLæ–¹æ³•åœ¨2Då›¾åƒç©ºé—´æ•è·è·¨ä»»åŠ¡å…³ç³»ï¼Œç¼ºä¹3Dæ„ŸçŸ¥ï¼Œé™åˆ¶äº†åœºæ™¯ç†è§£èƒ½åŠ›ã€‚
2. æå‡ºè·¨è§†è§’æ¨¡å—ï¼ˆCvMï¼‰ï¼Œé€šè¿‡ä»£ä»·ä½“æ•´åˆè·¨è§†è§’ä¿¡æ¯ï¼Œæ³¨å…¥å‡ ä½•ä¸€è‡´æ€§ï¼Œå¢å¼º3Dæ„ŸçŸ¥ã€‚
3. CvMæ¨¡å—æ˜“äºé›†æˆï¼Œåœ¨NYUv2å’ŒPASCAL-Contextæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæå‡äº†MTLæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨è§£å†³è®­ç»ƒå•ä¸ªç½‘ç»œä»¥è”åˆæ‰§è¡Œå¤šä¸ªå¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼‰çš„å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åœ¨2Då›¾åƒç©ºé—´ä¸­æ•è·è·¨ä»»åŠ¡å…³ç³»ï¼Œé€šå¸¸å¯¼è‡´ç¼ºä¹3Dæ„ŸçŸ¥çš„éç»“æ„åŒ–ç‰¹å¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œ3Dæ„ŸçŸ¥å¯¹äºå»ºæ¨¡å¯¹å…¨é¢åœºæ™¯ç†è§£è‡³å…³é‡è¦çš„è·¨ä»»åŠ¡ç›¸å…³æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡æ•´åˆè·¨è§†è§’çš„å…³è”ï¼ˆå³ä»£ä»·ä½“ï¼‰ä½œä¸ºMTLç½‘ç»œä¸­çš„å‡ ä½•ä¸€è‡´æ€§æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„è·¨è§†è§’æ¨¡å—ï¼ˆCvMï¼‰ï¼Œè¯¥æ¨¡å—åœ¨ä»»åŠ¡ä¹‹é—´å…±äº«ï¼Œä»¥äº¤æ¢è·¨è§†è§’çš„ä¿¡æ¯å¹¶æ•è·è·¨è§†è§’çš„ç›¸å…³æ€§ï¼Œå¹¶ä¸æ¥è‡ªMTLç¼–ç å™¨çš„ç‰¹å¾é›†æˆï¼Œç”¨äºå¤šä»»åŠ¡é¢„æµ‹ã€‚è¯¥æ¨¡å—ä¸æ¶æ„æ— å…³ï¼Œå¯ä»¥åº”ç”¨äºå•è§†å›¾å’Œå¤šè§†å›¾æ•°æ®ã€‚åœ¨NYUv2å’ŒPASCAL-Contextä¸Šçš„å¤§é‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†å‡ ä½•ä¸€è‡´æ€§æ³¨å…¥åˆ°ç°æœ‰çš„MTLæ–¹æ³•ä¸­ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•åœ¨å¯†é›†åœºæ™¯ç†è§£ä»»åŠ¡ä¸­ï¼Œä¸»è¦ä¾èµ–äº2Då›¾åƒç©ºé—´ä¸­çš„ç‰¹å¾å…³è”ï¼Œå¿½ç•¥äº†åœºæ™¯çš„3Då‡ ä½•ä¿¡æ¯ã€‚è¿™å¯¼è‡´ç½‘ç»œå­¦ä¹ åˆ°çš„ç‰¹å¾ç¼ºä¹3Dæ„ŸçŸ¥èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶å¯¹åœºæ™¯çš„å…¨é¢ç†è§£ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ä¸­ï¼Œ3Dä¿¡æ¯è‡³å…³é‡è¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†3Då‡ ä½•ä¿¡æ¯æ˜¾å¼åœ°å¼•å…¥åˆ°å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡æ„å»ºè·¨è§†è§’çš„ä»£ä»·ä½“ï¼ˆcost volumeï¼‰æ¥æ•æ‰ä¸åŒè§†è§’ä¹‹é—´çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶å°†è¿™ç§å‡ ä½•ä¸€è‡´æ€§ä½œä¸ºä¸€ç§å…ˆéªŒçŸ¥è¯†æ³¨å…¥åˆ°ç½‘ç»œä¸­ï¼Œä»è€Œå¢å¼ºç½‘ç»œå¯¹3Dåœºæ™¯çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ ç¼–ç å™¨å’Œä¸€ä¸ªè·¨è§†è§’æ¨¡å—ï¼ˆCvMï¼‰ã€‚ç¼–ç å™¨è´Ÿè´£æå–å›¾åƒç‰¹å¾ï¼ŒCvMæ¨¡å—åˆ™è´Ÿè´£åœ¨ä¸åŒè§†è§’ä¹‹é—´äº¤æ¢ä¿¡æ¯ï¼Œæ„å»ºä»£ä»·ä½“ï¼Œå¹¶æå–è·¨è§†è§’çš„ç›¸å…³æ€§ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾éšåä¸ç¼–ç å™¨çš„ç‰¹å¾èåˆï¼Œç”¨äºå¤šä»»åŠ¡é¢„æµ‹ã€‚è¯¥æ¡†æ¶æ˜¯æ¶æ„æ— å…³çš„ï¼Œå¯ä»¥ä¸ç°æœ‰çš„å¤šä»»åŠ¡å­¦ä¹ ç½‘ç»œç»“åˆä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†è·¨è§†è§’æ¨¡å—ï¼ˆCvMï¼‰ï¼Œé€šè¿‡ä»£ä»·ä½“æ˜¾å¼åœ°å»ºæ¨¡äº†è·¨è§†è§’çš„å‡ ä½•ä¸€è‡´æ€§ã€‚è¿™ä¸ä»¥å¾€ä¸»è¦å…³æ³¨2Då›¾åƒç©ºé—´ç‰¹å¾å…³è”çš„æ–¹æ³•ä¸åŒï¼ŒCvMæ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†3Då‡ ä½•ä¿¡æ¯èå…¥åˆ°å¤šä»»åŠ¡å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œä»è€Œæå‡äº†ç½‘ç»œçš„3Dæ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šCvMæ¨¡å—çš„è®¾è®¡æ˜¯è½»é‡çº§çš„ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰çš„å¤šä»»åŠ¡å­¦ä¹ ç½‘ç»œä¸­ã€‚å…·ä½“å®ç°ç»†èŠ‚åŒ…æ‹¬ï¼šå¦‚ä½•æ„å»ºä»£ä»·ä½“ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨å“ªäº›ç‰¹å¾è¿›è¡ŒåŒ¹é…ï¼‰ï¼Œå¦‚ä½•æå–è·¨è§†è§’ç›¸å…³æ€§ç‰¹å¾ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼‰ï¼Œä»¥åŠå¦‚ä½•å°†è¿™äº›ç‰¹å¾ä¸ç¼–ç å™¨çš„ç‰¹å¾è¿›è¡Œèåˆï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿéœ€è¦è€ƒè™‘å¦‚ä½•å¹³è¡¡ä¸åŒä»»åŠ¡ä¹‹é—´çš„å­¦ä¹ ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨å‡ ä½•ä¸€è‡´æ€§ä¿¡æ¯æ¥çº¦æŸç½‘ç»œçš„å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨NYUv2å’ŒPASCAL-Contextæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡å¤šä»»åŠ¡å­¦ä¹ çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨NYUv2æ•°æ®é›†ä¸Šï¼Œåˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½æå‡äº†X%ï¼Œæ·±åº¦ä¼°è®¡ä»»åŠ¡çš„æ€§èƒ½æå‡äº†Y%ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡åœºæ™¯ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç¯å¢ƒæ„ŸçŸ¥ç²¾åº¦ï¼Œå¢å¼ºæœºå™¨äººå¯¹å¤æ‚ç¯å¢ƒçš„é€‚åº”æ€§ï¼Œå¹¶ä¸ºARåº”ç”¨æä¾›æ›´é€¼çœŸçš„3Dåœºæ™¯é‡å»ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.

