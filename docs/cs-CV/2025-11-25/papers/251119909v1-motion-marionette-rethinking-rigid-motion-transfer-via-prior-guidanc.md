---
layout: default
title: Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance
---

# Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.19909" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.19909v1</a>
  <a href="https://arxiv.org/pdf/2511.19909.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19909v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.19909v1', 'Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoxuan Wang, Jiachen Tao, Junyi Wu, Gaowen Liu, Ramana Rao Kompella, Yan Yan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMotion Marionetteä»¥è§£å†³åˆšæ€§è¿åŠ¨è½¬ç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `åˆšæ€§è¿åŠ¨è½¬ç§»` `æ—¶ç©ºå…ˆéªŒ` `è§†é¢‘ç”Ÿæˆ` `å¯æ§ç”Ÿæˆ` `è§†è§‰ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤–éƒ¨å‡ ä½•æˆ–ç”Ÿæˆå…ˆéªŒï¼Œå¯¼è‡´åœ¨å¯æ³›åŒ–æ€§ä¸æ—¶é—´ä¸€è‡´æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡å†…éƒ¨æ—¶ç©ºå…ˆéªŒæ¥æŒ‡å¯¼è¿åŠ¨è½¬ç§»ï¼Œé¿å…äº†å¤–éƒ¨çº¦æŸçš„å½±å“ï¼Œæå‡äº†çµæ´»æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMotion Marionetteåœ¨å¤šç§å¯¹è±¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆçš„è§†é¢‘åœ¨æ—¶é—´ä¸Šä¿æŒä¸€è‡´ï¼Œå¹¶èƒ½è¿›è¡Œå¯æ§ç”Ÿæˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†Motion Marionetteï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶-shotæ¡†æ¶ï¼Œæ—¨åœ¨å°†å•ç›®æºè§†é¢‘ä¸­çš„åˆšæ€§è¿åŠ¨è½¬ç§»åˆ°å•è§†å›¾ç›®æ ‡å›¾åƒä¸­ã€‚ä»¥å¾€çš„ç ”ç©¶é€šå¸¸ä¾èµ–å‡ ä½•ã€ç”Ÿæˆæˆ–ä»¿çœŸå…ˆéªŒæ¥æŒ‡å¯¼è½¬ç§»è¿‡ç¨‹ï¼Œä½†è¿™äº›å¤–éƒ¨å…ˆéªŒå¼•å…¥äº†è¾…åŠ©çº¦æŸï¼Œå¯¼è‡´äº†å¯æ³›åŒ–æ€§ä¸æ—¶é—´ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å†…éƒ¨å…ˆéªŒæ¥æŒ‡å¯¼è¿åŠ¨è½¬ç§»è¿‡ç¨‹ï¼Œè¯¥å…ˆéªŒä¸“é—¨æ•æ‰æºè§†é¢‘ä¸ä»»ä½•è½¬ç§»ç›®æ ‡è§†é¢‘ä¹‹é—´çš„æ—¶ç©ºå˜æ¢ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æºè§†é¢‘å’Œç›®æ ‡å›¾åƒæå‡åˆ°ç»Ÿä¸€çš„3Dè¡¨ç¤ºç©ºé—´ï¼Œä»æºè§†é¢‘ä¸­æå–è¿åŠ¨è½¨è¿¹ä»¥æ„å»ºç‹¬ç«‹äºç‰©ä½“å‡ ä½•å’Œè¯­ä¹‰çš„æ—¶ç©ºå…ˆéªŒï¼Œç¼–ç ç›¸å¯¹çš„ç©ºé—´å˜åŒ–ã€‚è¯¥å…ˆéªŒä¸ç›®æ ‡å¯¹è±¡ç»“åˆï¼Œåˆæˆå¯æ§çš„é€Ÿåº¦åœºï¼Œå¹¶é€šè¿‡åŸºäºä½ç½®çš„åŠ¨åŠ›å­¦è¿›è¡Œç²¾ç»†åŒ–ï¼Œä»¥å‡å°‘ä¼ªå½±å¹¶å¢å¼ºè§†è§‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMotion Marionetteèƒ½å¤Ÿåœ¨ä¸åŒå¯¹è±¡é—´æ³›åŒ–ï¼Œç”Ÿæˆä¸æºè¿åŠ¨é«˜åº¦ä¸€è‡´çš„æ—¶é—´ä¸€è‡´æ€§è§†é¢‘ï¼Œå¹¶æ”¯æŒå¯æ§çš„è§†é¢‘ç”Ÿæˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åˆšæ€§è¿åŠ¨è½¬ç§»çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨å…ˆéªŒï¼Œå¯¼è‡´å¯æ³›åŒ–æ€§ä¸æ—¶é—´ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé€šè¿‡å†…éƒ¨å…ˆéªŒæ¥æŒ‡å¯¼è¿åŠ¨è½¬ç§»ï¼Œè¯¥å…ˆéªŒæ•æ‰æºè§†é¢‘ä¸ç›®æ ‡è§†é¢‘ä¹‹é—´çš„æ—¶ç©ºå˜æ¢ï¼Œé¿å…äº†å¤–éƒ¨çº¦æŸçš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å°†æºè§†é¢‘å’Œç›®æ ‡å›¾åƒæå‡åˆ°ç»Ÿä¸€çš„3Dè¡¨ç¤ºç©ºé—´ï¼Œæå–è¿åŠ¨è½¨è¿¹æ„å»ºæ—¶ç©ºå…ˆéªŒï¼Œå¹¶ä¸ç›®æ ‡å¯¹è±¡ç»“åˆç”Ÿæˆé€Ÿåº¦åœºï¼Œæœ€åé€šè¿‡åŸºäºä½ç½®çš„åŠ¨åŠ›å­¦è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å†…éƒ¨æ—¶ç©ºå…ˆéªŒï¼Œç‹¬ç«‹äºç‰©ä½“å‡ ä½•å’Œè¯­ä¹‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼–ç ç›¸å¯¹ç©ºé—´å˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†è¿åŠ¨è½¬ç§»çš„çµæ´»æ€§ä¸ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæ„å»ºçš„æ—¶ç©ºå…ˆéªŒé€šè¿‡è¿åŠ¨è½¨è¿¹æå–ï¼Œé€Ÿåº¦åœºçš„åˆæˆä¸ç²¾ç»†åŒ–å¤„ç†é‡‡ç”¨åŸºäºä½ç½®çš„åŠ¨åŠ›å­¦æ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆè§†é¢‘çš„è§†è§‰ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMotion Marionetteåœ¨å¤šç§å¯¹è±¡ä¸Šå‡èƒ½æœ‰æ•ˆæ³›åŒ–ï¼Œç”Ÿæˆçš„è§†é¢‘åœ¨æ—¶é—´ä¸€è‡´æ€§ä¸Šä¸æºè¿åŠ¨é«˜åº¦å¯¹é½ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆè§†é¢‘çš„è´¨é‡æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®å°šæœªæä¾›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŠ¨ç”»åˆ¶ä½œã€è™šæ‹Ÿç°å®å’Œæ¸¸æˆå¼€å‘ç­‰ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›é«˜æ•ˆçš„è¿åŠ¨è½¬ç§»è§£å†³æ–¹æ¡ˆï¼Œæå‡å†…å®¹ç”Ÿæˆçš„çµæ´»æ€§å’Œè´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“å®æ—¶è§†é¢‘ç¼–è¾‘å’Œäº¤äº’å¼åª’ä½“çš„ç”Ÿæˆæ–¹å¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Motion Marionette, a zero-shot framework for rigid motion transfer from monocular source videos to single-view target images. Previous works typically employ geometric, generative, or simulation priors to guide the transfer process, but these external priors introduce auxiliary constraints that lead to trade-offs between generalizability and temporal consistency. To address these limitations, we propose guiding the motion transfer process through an internal prior that exclusively captures the spatial-temporal transformations and is shared between the source video and any transferred target video. Specifically, we first lift both the source video and the target image into a unified 3D representation space. Motion trajectories are then extracted from the source video to construct a spatial-temporal (SpaT) prior that is independent of object geometry and semantics, encoding relative spatial variations over time. This prior is further integrated with the target object to synthesize a controllable velocity field, which is subsequently refined using Position-Based Dynamics to mitigate artifacts and enhance visual coherence. The resulting velocity field can be flexibly employed for efficient video production. Empirical results demonstrate that Motion Marionette generalizes across diverse objects, produces temporally consistent videos that align well with the source motion, and supports controllable video generation.

