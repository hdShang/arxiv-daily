---
layout: default
title: "VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction"
---

# VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.19971" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.19971v1</a>
  <a href="https://arxiv.org/pdf/2511.19971.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19971v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.19971v1', 'VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Hu, Chong Cheng, Sicheng Yu, Xiaoyang Guo, Hao Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VGGT4Dï¼šæŒ–æ˜è§†è§‰å‡ ä½•Transformerä¸­çš„è¿åŠ¨çº¿ç´¢ï¼Œç”¨äº4Dåœºæ™¯é‡å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `4Dåœºæ™¯é‡å»º` `åŠ¨æ€åœºæ™¯ç†è§£` `è§†è§‰å‡ ä½•Transformer` `è¿åŠ¨çº¿ç´¢æŒ–æ˜` `å…è®­ç»ƒå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŠ¨æ€åœºæ™¯é‡å»ºçš„å…³é”®æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆåˆ†ç¦»åŠ¨æ€ç‰©ä½“å’Œé™æ€èƒŒæ™¯ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨ä¿¡æ¯æˆ–éœ€è¦å¤§é‡ä¼˜åŒ–ã€‚
2. VGGT4Dé€šè¿‡æŒ–æ˜VGGTä¸­éšå«çš„åŠ¨æ€çº¿ç´¢ï¼Œåˆ©ç”¨Gramç›¸ä¼¼æ€§å’ŒæŠ•å½±æ¢¯åº¦ç»†åŒ–ï¼Œå®ç°åŠ¨æ€ç‰©ä½“çš„ç²¾ç¡®åˆ†å‰²ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVGGT4Dåœ¨åŠ¨æ€ç‰©ä½“åˆ†å‰²ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œå¯†é›†é‡å»ºæ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ”¯æŒé•¿åºåˆ—æ¨ç†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŠ¨æ€4Dåœºæ™¯é‡å»ºæå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦ç¨³å¥åœ°å°†åŠ¨æ€ç‰©ä½“ä»é™æ€èƒŒæ™¯ä¸­åˆ†ç¦»å‡ºæ¥ã€‚è™½ç„¶åƒVGGTè¿™æ ·çš„3DåŸºç¡€æ¨¡å‹æä¾›äº†ç²¾ç¡®çš„3Då‡ ä½•ä¿¡æ¯ï¼Œä½†å½“ç§»åŠ¨ç‰©ä½“å æ®ä¸»å¯¼åœ°ä½æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ç°æœ‰çš„4Dæ–¹æ³•é€šå¸¸ä¾èµ–äºå¤–éƒ¨å…ˆéªŒã€ç¹é‡çš„åä¼˜åŒ–ï¼Œæˆ–è€…éœ€è¦åœ¨4Dæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æœ¬æ–‡æå‡ºäº†VGGT4Dï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œæ‰©å±•äº†3DåŸºç¡€æ¨¡å‹VGGTï¼Œç”¨äºç¨³å¥çš„4Dåœºæ™¯é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸€ä¸ªå…³é”®å‘ç°ï¼šVGGTçš„å…¨å±€æ³¨æ„åŠ›å±‚å·²ç»éšå¼åœ°ç¼–ç äº†ä¸°å¯Œçš„ã€é€å±‚çš„åŠ¨æ€çº¿ç´¢ã€‚ä¸ºäº†è·å¾—è§£è€¦é™æ€å’ŒåŠ¨æ€å…ƒç´ çš„æ©ç ï¼Œæˆ‘ä»¬é€šè¿‡Gramç›¸ä¼¼æ€§æŒ–æ˜å’Œæ”¾å¤§å…¨å±€åŠ¨æ€çº¿ç´¢ï¼Œå¹¶åœ¨æ—¶é—´çª—å£å†…èšåˆå®ƒä»¬ã€‚ä¸ºäº†è¿›ä¸€æ­¥é”åŒ–æ©ç è¾¹ç•Œï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”±æŠ•å½±æ¢¯åº¦é©±åŠ¨çš„ç»†åŒ–ç­–ç•¥ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ç²¾ç¡®çš„æ©ç é›†æˆåˆ°VGGTçš„æ—©æœŸæ¨ç†é˜¶æ®µï¼Œæœ‰æ•ˆåœ°å‡è½»äº†è¿åŠ¨å¯¹å§¿æ€ä¼°è®¡å’Œå‡ ä½•é‡å»ºçš„å¹²æ‰°ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŠ¨æ€å¯¹è±¡åˆ†å‰²ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œå¯†é›†é‡å»ºæ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚å®ƒè¿˜æ”¯æŒå¯¹è¶…è¿‡500å¸§çš„åºåˆ—è¿›è¡Œå•æ¬¡æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šåŠ¨æ€4Dåœºæ™¯é‡å»ºæ—¨åœ¨ä»è§†é¢‘åºåˆ—ä¸­æ¢å¤åœºæ™¯çš„3Då‡ ä½•ç»“æ„ä»¥åŠéšæ—¶é—´å˜åŒ–çš„åŠ¨æ€ç‰©ä½“ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†åŒ…å«å¤§é‡è¿åŠ¨ç‰©ä½“çš„åœºæ™¯æ—¶ï¼Œå¾€å¾€ç”±äºåŠ¨æ€ç‰©ä½“ä¸é™æ€èƒŒæ™¯çš„æ··æ·†è€Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºé¢å¤–çš„å…ˆéªŒçŸ¥è¯†æˆ–éœ€è¦è€—æ—¶çš„åå¤„ç†ä¼˜åŒ–ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVGGT4Dçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨3DåŸºç¡€æ¨¡å‹VGGTä¸­å·²ç»å­˜åœ¨çš„ã€ä½†æœªè¢«å……åˆ†åˆ©ç”¨çš„åŠ¨æ€çº¿ç´¢ã€‚ä½œè€…å‘ç°ï¼ŒVGGTçš„å…¨å±€æ³¨æ„åŠ›å±‚åœ¨ä¸åŒå±‚çº§ä¸Šéšå¼åœ°ç¼–ç äº†åœºæ™¯ä¸­çš„è¿åŠ¨ä¿¡æ¯ã€‚é€šè¿‡æœ‰æ•ˆåœ°æŒ–æ˜å’Œæ”¾å¤§è¿™äº›åŠ¨æ€çº¿ç´¢ï¼Œå¯ä»¥å®ç°åŠ¨æ€ç‰©ä½“å’Œé™æ€èƒŒæ™¯çš„è§£è€¦ï¼Œä»è€Œæé«˜4Dåœºæ™¯é‡å»ºçš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVGGT4Dæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) åŠ¨æ€çº¿ç´¢æŒ–æ˜ä¸èšåˆï¼šåˆ©ç”¨GramçŸ©é˜µè®¡ç®—VGGTå„å±‚ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæå–åŠ¨æ€çº¿ç´¢ï¼Œå¹¶åœ¨æ—¶é—´çª—å£å†…è¿›è¡Œèšåˆï¼Œç”Ÿæˆåˆå§‹çš„åŠ¨æ€ç‰©ä½“æ©ç ã€‚2) æ©ç ç»†åŒ–ï¼šé€šè¿‡æŠ•å½±æ¢¯åº¦é©±åŠ¨çš„ç»†åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–æ©ç è¾¹ç•Œï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚3) é›†æˆåˆ°VGGTæ¨ç†ï¼šå°†ç»†åŒ–åçš„æ©ç é›†æˆåˆ°VGGTçš„æ—©æœŸæ¨ç†é˜¶æ®µï¼ŒæŠ‘åˆ¶è¿åŠ¨å¹²æ‰°ï¼Œæå‡å§¿æ€ä¼°è®¡å’Œå‡ ä½•é‡å»ºçš„å‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šVGGT4Dçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ— éœ€è®­ç»ƒçš„ç‰¹æ€§ï¼Œä»¥åŠå¯¹ç°æœ‰3DåŸºç¡€æ¨¡å‹VGGTçš„æœ‰æ•ˆåˆ©ç”¨ã€‚ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®æˆ–ä¾èµ–å¤–éƒ¨å…ˆéªŒçš„æ–¹æ³•ä¸åŒï¼ŒVGGT4Dç›´æ¥ä»VGGTçš„å†…éƒ¨è¡¨ç¤ºä¸­æŒ–æ˜åŠ¨æ€çº¿ç´¢ï¼Œå®ç°äº†å¯¹åŠ¨æ€åœºæ™¯çš„é²æ£’é‡å»ºã€‚æ­¤å¤–ï¼ŒæŠ•å½±æ¢¯åº¦é©±åŠ¨çš„æ©ç ç»†åŒ–ç­–ç•¥è¿›ä¸€æ­¥æå‡äº†åˆ†å‰²ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŠ¨æ€çº¿ç´¢æŒ–æ˜é˜¶æ®µï¼Œä½¿ç”¨GramçŸ©é˜µè®¡ç®—ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œèƒ½å¤Ÿæ•æ‰ä¸åŒå±‚çº§ä¸Šçš„è¿åŠ¨ä¿¡æ¯ã€‚æ—¶é—´çª—å£çš„é•¿åº¦éœ€è¦æ ¹æ®åœºæ™¯çš„è¿åŠ¨é€Ÿåº¦è¿›è¡Œè°ƒæ•´ã€‚æŠ•å½±æ¢¯åº¦ç»†åŒ–ç­–ç•¥åˆ©ç”¨å›¾åƒæ¢¯åº¦ä¿¡æ¯æ¥ä¼˜åŒ–æ©ç è¾¹ç•Œï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦å¹³è¡¡åˆ†å‰²ç²¾åº¦å’Œè¾¹ç•Œé”åŒ–ã€‚å°†æ©ç é›†æˆåˆ°VGGTæ—©æœŸæ¨ç†é˜¶æ®µï¼Œå¯ä»¥æœ‰æ•ˆæŠ‘åˆ¶è¿åŠ¨å¯¹åç»­å¤„ç†çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VGGT4Dåœ¨å…­ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨åŠ¨æ€ç‰©ä½“åˆ†å‰²ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œå¯†é›†é‡å»ºæ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰çš„3DåŸºç¡€æ¨¡å‹VGGTã€‚æ­¤å¤–ï¼ŒVGGT4Dæ”¯æŒå¯¹è¶…è¿‡500å¸§çš„é•¿åºåˆ—è¿›è¡Œå•æ¬¡æ¨ç†ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VGGT4Dåœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£åŠ¨æ€ç¯å¢ƒï¼Œä»è€Œåšå‡ºæ›´å®‰å…¨ã€æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå‡†ç¡®çš„4Dåœºæ™¯é‡å»ºå¯ä»¥æé«˜è½¦è¾†å¯¹å‘¨å›´ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¢å¼ºé©¾é©¶å®‰å…¨æ€§ã€‚åœ¨å¢å¼ºç°å®ä¸­ï¼ŒVGGT4Då¯ä»¥å®ç°æ›´é€¼çœŸçš„è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯çš„äº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.

