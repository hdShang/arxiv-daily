---
layout: default
title: DinoLizer: Learning from the Best for Generative Inpainting Localization
---

# DinoLizer: Learning from the Best for Generative Inpainting Localization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20722" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20722v1</a>
  <a href="https://arxiv.org/pdf/2511.20722.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20722v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.20722v1', 'DinoLizer: Learning from the Best for Generative Inpainting Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minh Thong Doi, Jan Butora, Vincent Itier, JÃ©rÃ©mie Boulanger, Patrick Bas

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DinoLizerï¼šåˆ©ç”¨DINOv2å­¦ä¹ ç”Ÿæˆå¼å›¾åƒä¿®å¤ç¯¡æ”¹åŒºåŸŸçš„å®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å›¾åƒç¯¡æ”¹æ£€æµ‹` `ç”Ÿæˆå¼å›¾åƒä¿®å¤` `DINOv2` `Vision Transformer` `è‡ªç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¯¡æ”¹æ£€æµ‹æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå®šä½ç”Ÿæˆå¼å›¾åƒä¿®å¤ä¸­çš„ç¯¡æ”¹åŒºåŸŸï¼Œå°¤å…¶æ˜¯åœ¨è¯­ä¹‰ä¸€è‡´æ€§è¾ƒå¼ºçš„æƒ…å†µä¸‹ã€‚
2. DinoLizeråˆ©ç”¨é¢„è®­ç»ƒçš„DINOv2æ¨¡å‹ï¼Œé€šè¿‡å¾®è°ƒçº¿æ€§åˆ†ç±»å¤´ï¼Œä¸“æ³¨äºæ£€æµ‹è¯­ä¹‰å±‚é¢çš„ç¯¡æ”¹ï¼Œå¿½ç•¥éè¯­ä¹‰ç¼–è¾‘ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDinoLizeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨åå¤„ç†æ“ä½œåï¼ŒIoUæå‡é«˜è¾¾12%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºDinoLizerï¼Œä¸€ä¸ªåŸºäºDINOv2çš„æ¨¡å‹ï¼Œç”¨äºå®šä½ç”Ÿæˆå¼å›¾åƒä¿®å¤ä¸­è¢«ç¯¡æ”¹çš„åŒºåŸŸã€‚è¯¥æ–¹æ³•åŸºäºä¸€ä¸ªé¢„è®­ç»ƒçš„DINOv2æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨B-Freeæ•°æ®é›†ä¸Šè®­ç»ƒä»¥æ£€æµ‹åˆæˆå›¾åƒã€‚æˆ‘ä»¬åœ¨Vision Transformerçš„patchåµŒå…¥ä¹‹ä¸Šæ·»åŠ ä¸€ä¸ªçº¿æ€§åˆ†ç±»å¤´ï¼Œä»¥$14	imes 14$çš„patchåˆ†è¾¨ç‡é¢„æµ‹ç¯¡æ”¹ã€‚è¯¥åˆ†ç±»å¤´è¢«è®­ç»ƒä¸ºå…³æ³¨è¯­ä¹‰ä¸Šè¢«æ”¹å˜çš„åŒºåŸŸï¼Œå°†éè¯­ä¹‰ç¼–è¾‘è§†ä¸ºåŸå§‹å†…å®¹çš„ä¸€éƒ¨åˆ†ã€‚ç”±äºViTåªæ¥å—å›ºå®šå¤§å°çš„è¾“å…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥æ¥èšåˆæ›´å¤§å›¾åƒä¸Šçš„é¢„æµ‹ï¼›å¯¹ç”Ÿæˆçš„heatmapè¿›è¡Œåå¤„ç†ï¼Œä»¥ç»†åŒ–ä¼°è®¡çš„äºŒå€¼ç¯¡æ”¹maskã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDinoLizeråœ¨ä»ä¸åŒç”Ÿæˆæ¨¡å‹å¯¼å‡ºçš„å„ç§ä¿®å¤æ•°æ®é›†ä¸Šï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„å±€éƒ¨ç¯¡æ”¹æ£€æµ‹å™¨ã€‚å®ƒå¯¹å¸¸è§çš„åå¤„ç†æ“ä½œï¼ˆå¦‚è°ƒæ•´å¤§å°ã€æ·»åŠ å™ªå£°å’ŒJPEGï¼ˆåŒé‡ï¼‰å‹ç¼©ï¼‰ä¿æŒé²æ£’æ€§ã€‚å¹³å‡è€Œè¨€ï¼ŒDinoLizerå®ç°äº†æ¯”æ¬¡ä¼˜æ¨¡å‹é«˜12%çš„äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ï¼Œåå¤„ç†åå¢ç›Šæ›´å¤§ã€‚æˆ‘ä»¬ä½¿ç”¨ç°æˆçš„DINOv2è¿›è¡Œçš„å®éªŒè¯æ˜äº†Vision Transformeråœ¨è¯¥ä»»åŠ¡ä¸­çš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ã€‚æœ€åï¼Œå°†DINOv2åŠå…¶åç»§è€…DINOv3åœ¨deepfakeå®šä½ä¸­è¿›è¡Œæ¯”è¾ƒçš„å¹¿æ³›æ¶ˆèç ”ç©¶è¯å®äº†DinoLizerçš„ä¼˜è¶Šæ€§ã€‚ä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç”Ÿæˆå¼å›¾åƒä¿®å¤åï¼Œå¦‚ä½•ç²¾ç¡®å®šä½è¢«ç¯¡æ”¹åŒºåŸŸçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹è¯­ä¹‰ä¸€è‡´æ€§è¾ƒå¼ºçš„ä¿®å¤å›¾åƒæ—¶ï¼Œéš¾ä»¥åŒºåˆ†çœŸå®å†…å®¹å’Œç”Ÿæˆå†…å®¹ï¼Œå¯¼è‡´å®šä½ç²¾åº¦ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„DINOv2æ¨¡å‹å¼ºå¤§çš„è§†è§‰è¡¨å¾èƒ½åŠ›ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œä¸“æ³¨äºæ£€æµ‹å›¾åƒä¸­è¯­ä¹‰å±‚é¢çš„å˜åŒ–ã€‚é€šè¿‡å¿½ç•¥éè¯­ä¹‰ç¼–è¾‘ï¼Œæ¨¡å‹å¯ä»¥æ›´å‡†ç¡®åœ°è¯†åˆ«å‡ºè¢«ç¯¡æ”¹çš„åŒºåŸŸã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDinoLizerçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨åœ¨B-Freeæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„DINOv2æ¨¡å‹æå–å›¾åƒç‰¹å¾ï¼›2) åœ¨Vision Transformerçš„patchåµŒå…¥ä¹‹ä¸Šæ·»åŠ ä¸€ä¸ªçº¿æ€§åˆ†ç±»å¤´ï¼›3) ä½¿ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥å¤„ç†å¤§å°ºå¯¸å›¾åƒï¼Œç”Ÿæˆheatmapï¼›4) å¯¹heatmapè¿›è¡Œåå¤„ç†ï¼Œå¾—åˆ°æœ€ç»ˆçš„äºŒå€¼ç¯¡æ”¹maskã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨DINOv2çš„è‡ªç›‘ç£å­¦ä¹ èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°å›¾åƒçš„æ·±å±‚è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡å¾®è°ƒçº¿æ€§åˆ†ç±»å¤´ï¼Œæ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºæ£€æµ‹è¯­ä¹‰å±‚é¢çš„ç¯¡æ”¹ï¼Œä»è€Œæé«˜å®šä½ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæ»‘åŠ¨çª—å£ç­–ç•¥å’Œåå¤„ç†æ­¥éª¤ä¹Ÿè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šDinoLizerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨DINOv2ä½œä¸ºç‰¹å¾æå–å™¨ï¼›2) åœ¨ViTçš„patchåµŒå…¥ä¸Šæ·»åŠ çº¿æ€§åˆ†ç±»å¤´ï¼Œä»¥$14	imes 14$çš„patchåˆ†è¾¨ç‡è¿›è¡Œé¢„æµ‹ï¼›3) ä½¿ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥å¤„ç†å¤§å°ºå¯¸å›¾åƒï¼›4) é€šè¿‡åå¤„ç†æ­¥éª¤ï¼ˆæœªçŸ¥å…·ä½“ç»†èŠ‚ï¼‰ç»†åŒ–ç¯¡æ”¹maskã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DinoLizeråœ¨å¤šä¸ªç”Ÿæˆå¼å›¾åƒä¿®å¤æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹³å‡IoUæ¯”æ¬¡ä¼˜æ¨¡å‹é«˜12%ï¼Œå¹¶ä¸”åœ¨ç»è¿‡å¸¸è§çš„åå¤„ç†æ“ä½œï¼ˆå¦‚ç¼©æ”¾ã€å™ªå£°æ·»åŠ ã€JPEGå‹ç¼©ï¼‰åï¼Œä»ç„¶ä¿æŒäº†è¾ƒå¥½çš„é²æ£’æ€§ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒDINOv2ä¼˜äºå…¶åç»§è€…DINOv3åœ¨deepfakeå®šä½ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DinoLizerå¯åº”ç”¨äºæ•°å­—å–è¯ã€å›¾åƒç‰ˆæƒä¿æŠ¤ã€æ–°é—»çœŸå®æ€§éªŒè¯ç­‰é¢†åŸŸã€‚é€šè¿‡è‡ªåŠ¨æ£€æµ‹å›¾åƒä¸­çš„ç¯¡æ”¹åŒºåŸŸï¼Œå¯ä»¥å¸®åŠ©è¯†åˆ«ä¼ªé€ å›¾åƒï¼Œç»´æŠ¤ç½‘ç»œä¿¡æ¯å®‰å…¨ï¼Œå¹¶ä¸ºå¸æ³•é‰´å®šæä¾›æŠ€æœ¯æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›é›†æˆåˆ°å›¾åƒç¼–è¾‘è½¯ä»¶ä¸­ï¼Œè¾…åŠ©ç”¨æˆ·è¯†åˆ«æ½œåœ¨çš„ç¯¡æ”¹é£é™©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.

