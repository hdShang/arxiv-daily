---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-30
---

# cs.CVï¼ˆ2025-05-30ï¼‰

ğŸ“Š å…± **56** ç¯‡è®ºæ–‡
 | ğŸ”— **16** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (18 ğŸ”—7)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (14 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (6 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (4)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (18 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250524476v1-period-llm-extending-the-periodic-capability-of-multimodal-large-lan.html">Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model</a></td>
  <td>æå‡ºPeriod-LLMä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å‘¨æœŸæ€§ä»»åŠ¡ä¸­çš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24476v1" data-paper-url="./papers/250524476v1-period-llm-extending-the-periodic-capability-of-multimodal-large-lan.html" onclick="toggleFavorite(this, '2505.24476v1', 'Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250524541v1-mixpert-mitigating-multimodal-learning-conflicts-with-efficient-mixt.html">Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts</a></td>
  <td>æå‡ºMixpertä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ å†²çªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24541v1" data-paper-url="./papers/250524541v1-mixpert-mitigating-multimodal-learning-conflicts-with-efficient-mixt.html" onclick="toggleFavorite(this, '2505.24541v1', 'Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250524329v2-distime-distribution-based-time-representation-for-video-large-langu.html">DisTime: Distribution-based Time Representation for Video Large Language Models</a></td>
  <td>æå‡ºDisTimeä»¥è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´è¡¨ç¤ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">TAMP</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24329v2" data-paper-url="./papers/250524329v2-distime-distribution-based-time-representation-for-video-large-langu.html" onclick="toggleFavorite(this, '2505.24329v2', 'DisTime: Distribution-based Time Representation for Video Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250524225v1-reasoning-can-hurt-the-inductive-abilities-of-large-language-models.html">Reasoning Can Hurt the Inductive Abilities of Large Language Models</a></td>
  <td>æå‡ºç»“æ„åŒ–å¹²é¢„ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„å½’çº³æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24225v1" data-paper-url="./papers/250524225v1-reasoning-can-hurt-the-inductive-abilities-of-large-language-models.html" onclick="toggleFavorite(this, '2505.24225v1', 'Reasoning Can Hurt the Inductive Abilities of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250524876v1-agent-x-evaluating-deep-multimodal-reasoning-in-vision-centric-agent.html">Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks</a></td>
  <td>æå‡ºAgent-Xä»¥è§£å†³å¤šæ­¥è§†è§‰æ¨ç†ä»»åŠ¡è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24876v1" data-paper-url="./papers/250524876v1-agent-x-evaluating-deep-multimodal-reasoning-in-vision-centric-agent.html" onclick="toggleFavorite(this, '2505.24876v1', 'Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250524528v2-geospatial-foundation-models-to-enable-progress-on-sustainable-devel.html">Geospatial Foundation Models to Enable Progress on Sustainable Development Goals</a></td>
  <td>æå‡ºSustainFMæ¡†æ¶ä»¥æ¨åŠ¨å¯æŒç»­å‘å±•ç›®æ ‡çš„å®ç°</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24528v2" data-paper-url="./papers/250524528v2-geospatial-foundation-models-to-enable-progress-on-sustainable-devel.html" onclick="toggleFavorite(this, '2505.24528v2', 'Geospatial Foundation Models to Enable Progress on Sustainable Development Goals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250524372v2-beyond-quantity-distribution-aware-labeling-for-visual-grounding.html">Beyond Quantity: Distribution-Aware Labeling for Visual Grounding</a></td>
  <td>æå‡ºDALæ¡†æ¶ä»¥è§£å†³è§†è§‰å®šä½ä¸­çš„æ ‡ç­¾åˆ†å¸ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24372v2" data-paper-url="./papers/250524372v2-beyond-quantity-distribution-aware-labeling-for-visual-grounding.html" onclick="toggleFavorite(this, '2505.24372v2', 'Beyond Quantity: Distribution-Aware Labeling for Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250524232v1-from-hallucinations-to-jailbreaks-rethinking-the-vulnerability-of-la.html">From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models</a></td>
  <td>æå‡ºç»Ÿä¸€æ¡†æ¶ä»¥è§£å†³å¤§å‹åŸºç¡€æ¨¡å‹çš„å¹»è§‰ä¸è¶Šç‹±æ”»å‡»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24232v1" data-paper-url="./papers/250524232v1-from-hallucinations-to-jailbreaks-rethinking-the-vulnerability-of-la.html" onclick="toggleFavorite(this, '2505.24232v1', 'From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250524182v1-seeing-is-not-reasoning-mvpbench-for-graph-based-evaluation-of-multi.html">Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT</a></td>
  <td>æå‡ºMVPBenchä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç‰©ç†æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24182v1" data-paper-url="./papers/250524182v1-seeing-is-not-reasoning-mvpbench-for-graph-based-evaluation-of-multi.html" onclick="toggleFavorite(this, '2505.24182v1', 'Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250524141v1-the-butterfly-effect-in-pathology-exploring-security-in-pathology-fo.html">The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models</a></td>
  <td>æå‡ºå±€éƒ¨æ‰°åŠ¨ä¸å…¨çƒå½±å“åŸåˆ™ä»¥æå‡ç—…ç†æ¨¡å‹å®‰å…¨æ€§</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24141v1" data-paper-url="./papers/250524141v1-the-butterfly-effect-in-pathology-exploring-security-in-pathology-fo.html" onclick="toggleFavorite(this, '2505.24141v1', 'The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250524120v2-csvqa-a-chinese-multimodal-benchmark-for-evaluating-stem-reasoning-c.html">CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs</a></td>
  <td>æå‡ºCSVQAä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç§‘å­¦æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24120v2" data-paper-url="./papers/250524120v2-csvqa-a-chinese-multimodal-benchmark-for-evaluating-stem-reasoning-c.html" onclick="toggleFavorite(this, '2505.24120v2', 'CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250524108v2-federated-foundation-model-for-gi-endoscopy-images.html">Federated Foundation Model for GI Endoscopy Images</a></td>
  <td>æå‡ºè”é‚¦åŸºç¡€æ¨¡å‹ä»¥è§£å†³èƒƒè‚ å†…é•œå›¾åƒæ•°æ®éšç§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24108v2" data-paper-url="./papers/250524108v2-federated-foundation-model-for-gi-endoscopy-images.html" onclick="toggleFavorite(this, '2505.24108v2', 'Federated Foundation Model for GI Endoscopy Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250524869v1-silvr-a-simple-language-based-video-reasoning-framework.html">SiLVR: A Simple Language-based Video Reasoning Framework</a></td>
  <td>æå‡ºSiLVRæ¡†æ¶ä»¥è§£å†³å¤æ‚è§†é¢‘è¯­è¨€ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24869v1" data-paper-url="./papers/250524869v1-silvr-a-simple-language-based-video-reasoning-framework.html" onclick="toggleFavorite(this, '2505.24869v1', 'SiLVR: A Simple Language-based Video Reasoning Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250524441v1-sorce-small-object-retrieval-in-complex-environments.html">SORCE: Small Object Retrieval in Complex Environments</a></td>
  <td>æå‡ºSORCEä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸­å°ç‰©ä½“æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24441v1" data-paper-url="./papers/250524441v1-sorce-small-object-retrieval-in-complex-environments.html" onclick="toggleFavorite(this, '2505.24441v1', 'SORCE: Small Object Retrieval in Complex Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250524158v1-threading-keyframe-with-narratives-mllms-as-strong-long-video-compre.html">Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders</a></td>
  <td>æå‡ºNar-KFCæ¨¡å—ä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„å…³é”®å¸§é€‰æ‹©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24158v1" data-paper-url="./papers/250524158v1-threading-keyframe-with-narratives-mllms-as-strong-long-video-compre.html" onclick="toggleFavorite(this, '2505.24158v1', 'Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250600129v2-geo-sign-hyperbolic-contrastive-regularisation-for-geometrically-awa.html">Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation</a></td>
  <td>æå‡ºGeo-Signä»¥æå‡æ‰‹è¯­ç¿»è¯‘ä¸­çš„å‡ ä½•è¡¨ç¤ºèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00129v2" data-paper-url="./papers/250600129v2-geo-sign-hyperbolic-contrastive-regularisation-for-geometrically-awa.html" onclick="toggleFavorite(this, '2506.00129v2', 'Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250524862v4-vistorybench-comprehensive-benchmark-suite-for-story-visualization.html">ViStoryBench: Comprehensive Benchmark Suite for Story Visualization</a></td>
  <td>æå‡ºViStoryBenchä»¥è§£å†³æ•…äº‹å¯è§†åŒ–è¯„ä¼°ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24862v4" data-paper-url="./papers/250524862v4-vistorybench-comprehensive-benchmark-suite-for-story-visualization.html" onclick="toggleFavorite(this, '2505.24862v4', 'ViStoryBench: Comprehensive Benchmark Suite for Story Visualization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250524693v1-conformal-prediction-for-zero-shot-models.html">Conformal Prediction for Zero-Shot Models</a></td>
  <td>æå‡ºConf-OTä»¥è§£å†³é›¶æ ·æœ¬æ¨¡å‹çš„ä¸ç¡®å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24693v1" data-paper-url="./papers/250524693v1-conformal-prediction-for-zero-shot-models.html" onclick="toggleFavorite(this, '2505.24693v1', 'Conformal Prediction for Zero-Shot Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250524871v2-modomodo-multi-domain-data-mixtures-for-multimodal-llm-reinforcement.html">MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning</a></td>
  <td>æå‡ºå¤šåŸŸæ•°æ®æ··åˆç­–ç•¥ä»¥æå‡å¤šæ¨¡æ€LLMçš„å¼ºåŒ–å­¦ä¹ èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24871v2" data-paper-url="./papers/250524871v2-modomodo-multi-domain-data-mixtures-for-multimodal-llm-reinforcement.html" onclick="toggleFavorite(this, '2505.24871v2', 'MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250524249v1-harnessing-foundation-models-for-robust-and-generalizable-6-dof-bron.html">Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization</a></td>
  <td>æå‡ºPANSv2ä»¥è§£å†³æ”¯æ°”ç®¡é•œå®šä½çš„é²æ£’æ€§ä¸æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24249v1" data-paper-url="./papers/250524249v1-harnessing-foundation-models-for-robust-and-generalizable-6-dof-bron.html" onclick="toggleFavorite(this, '2505.24249v1', 'Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250524718v3-reinforcing-video-reasoning-with-focused-thinking.html">Reinforcing Video Reasoning with Focused Thinking</a></td>
  <td>æå‡ºTW-GRPOä»¥è§£å†³è§†é¢‘æ¨ç†ä¸­çš„æ— æ•ˆé“¾æ¡å’Œå¥–åŠ±ç¨€ç–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24718v3" data-paper-url="./papers/250524718v3-reinforcing-video-reasoning-with-focused-thinking.html" onclick="toggleFavorite(this, '2505.24718v3', 'Reinforcing Video Reasoning with Focused Thinking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250524838v2-videocad-a-dataset-and-model-for-learning-long-horizon-3d-cad-ui-int.html">VideoCAD: A Dataset and Model for Learning Long-Horizon 3D CAD UI Interactions from Video</a></td>
  <td>æå‡ºVideoCADä»¥è§£å†³å¤æ‚3D CADç•Œé¢äº¤äº’å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">behavior cloning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24838v2" data-paper-url="./papers/250524838v2-videocad-a-dataset-and-model-for-learning-long-horizon-3d-cad-ui-int.html" onclick="toggleFavorite(this, '2505.24838v2', 'VideoCAD: A Dataset and Model for Learning Long-Horizon 3D CAD UI Interactions from Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250524481v1-acm-unet-adaptive-integration-of-cnns-and-mamba-for-efficient-medica.html">ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation</a></td>
  <td>æå‡ºACM-UNetä»¥è§£å†³åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­çš„ç»“æ„ä¸åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24481v1" data-paper-url="./papers/250524481v1-acm-unet-adaptive-integration-of-cnns-and-mamba-for-efficient-medica.html" onclick="toggleFavorite(this, '2505.24481v1', 'ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250524245v1-ltm3d-bridging-token-spaces-for-conditional-3d-generation-with-auto-.html">LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework</a></td>
  <td>æå‡ºLTM3Dä»¥è§£å†³æ¡ä»¶3Dç”Ÿæˆä¸­çš„ä¾èµ–å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span> <span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24245v1" data-paper-url="./papers/250524245v1-ltm3d-bridging-token-spaces-for-conditional-3d-generation-with-auto-.html" onclick="toggleFavorite(this, '2505.24245v1', 'LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250524134v1-a-mathematical-perspective-on-contrastive-learning.html">A Mathematical Perspective On Contrastive Learning</a></td>
  <td>æå‡ºä¸€ç§æ•°å­¦è§†è§’çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€æ•°æ®å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24134v1" data-paper-url="./papers/250524134v1-a-mathematical-perspective-on-contrastive-learning.html" onclick="toggleFavorite(this, '2505.24134v1', 'A Mathematical Perspective On Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250524361v1-revisiting-cross-modal-knowledge-distillation-a-disentanglement-appr.html">Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation</a></td>
  <td>æå‡ºCroDiNo-KDä»¥è§£å†³RGBDè¯­ä¹‰åˆ†å‰²ä¸­çš„çŸ¥è¯†è’¸é¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24361v1" data-paper-url="./papers/250524361v1-revisiting-cross-modal-knowledge-distillation-a-disentanglement-appr.html" onclick="toggleFavorite(this, '2505.24361v1', 'Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250524310v1-progressive-class-level-distillation.html">Progressive Class-level Distillation</a></td>
  <td>æå‡ºæ¸è¿›å¼ç±»çº§è’¸é¦ä»¥è§£å†³çŸ¥è¯†è’¸é¦ä¸­çš„ä½æ¦‚ç‡ç±»ä¿¡æ¯ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">teacher-student</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24310v1" data-paper-url="./papers/250524310v1-progressive-class-level-distillation.html" onclick="toggleFavorite(this, '2505.24310v1', 'Progressive Class-level Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250524641v1-a-cross-branch-fusion-based-contrastive-learning-framework-for-point.html">A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-supervised Learning</a></td>
  <td>æå‡ºPoCCAæ¡†æ¶ä»¥æå‡ç‚¹äº‘è‡ªç›‘ç£å­¦ä¹ æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24641v1" data-paper-url="./papers/250524641v1-a-cross-branch-fusion-based-contrastive-learning-framework-for-point.html" onclick="toggleFavorite(this, '2505.24641v1', 'A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-supervised Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250600101v2-egoviscvpr-what-changed-and-what-could-have-changed-state-change-cou.html">EgoVIS@CVPR: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning</a></td>
  <td>æå‡ºçŠ¶æ€å˜åŒ–åäº‹å®ä»¥æå‡ç¨‹åºæ„è¯†è§†é¢‘è¡¨ç¤ºå­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00101v2" data-paper-url="./papers/250600101v2-egoviscvpr-what-changed-and-what-could-have-changed-state-change-cou.html" onclick="toggleFavorite(this, '2506.00101v2', 'EgoVIS@CVPR: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250524499v1-reason-svg-hybrid-reward-rl-for-aha-moments-in-vector-graphics-gener.html">Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation</a></td>
  <td>æå‡ºReason-SVGä»¥è§£å†³SVGç”Ÿæˆä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24499v1" data-paper-url="./papers/250524499v1-reason-svg-hybrid-reward-rl-for-aha-moments-in-vector-graphics-gener.html" onclick="toggleFavorite(this, '2505.24499v1', 'Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250524210v2-stork-faster-diffusion-and-flow-matching-sampling-by-resolving-both-.html">STORK: Faster Diffusion And Flow Matching Sampling By Resolving Both Stiffness And Structure-Dependence</a></td>
  <td>æå‡ºSTORKä»¥è§£å†³æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æ¨¡å‹çš„é‡‡æ ·æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24210v2" data-paper-url="./papers/250524210v2-stork-faster-diffusion-and-flow-matching-sampling-by-resolving-both-.html" onclick="toggleFavorite(this, '2505.24210v2', 'STORK: Faster Diffusion And Flow Matching Sampling By Resolving Both Stiffness And Structure-Dependence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250605375v1-state-estimation-and-control-of-dynamic-systems-from-high-dimensiona.html">State Estimation and Control of Dynamic Systems from High-Dimensional Image Data</a></td>
  <td>æå‡ºä¸€ç§æ–°å‹ç¥ç»æ¶æ„ä»¥è§£å†³åŠ¨æ€ç³»ç»ŸçŠ¶æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.05375v1" data-paper-url="./papers/250605375v1-state-estimation-and-control-of-dynamic-systems-from-high-dimensiona.html" onclick="toggleFavorite(this, '2506.05375v1', 'State Estimation and Control of Dynamic Systems from High-Dimensional Image Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>33</td>
  <td><a href="./papers/250524746v1-tackling-view-dependent-semantics-in-3d-language-gaussian-splatting.html">Tackling View-Dependent Semantics in 3D Language Gaussian Splatting</a></td>
  <td>æå‡ºLaGaä»¥è§£å†³3Dåœºæ™¯ä¸­çš„è§†è§’ä¾èµ–è¯­ä¹‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24746v1" data-paper-url="./papers/250524746v1-tackling-view-dependent-semantics-in-3d-language-gaussian-splatting.html" onclick="toggleFavorite(this, '2505.24746v1', 'Tackling View-Dependent Semantics in 3D Language Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250524315v1-interactanything-zero-shot-human-object-interaction-synthesis-via-ll.html">InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing</a></td>
  <td>æå‡ºInteractAnythingä»¥è§£å†³é›¶æ ·æœ¬äººæœºäº¤äº’åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">affordance</span> <span class="paper-tag">human-object interaction</span> <span class="paper-tag">HOI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24315v1" data-paper-url="./papers/250524315v1-interactanything-zero-shot-human-object-interaction-synthesis-via-ll.html" onclick="toggleFavorite(this, '2505.24315v1', 'InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250524103v1-weakly-supervised-affordance-grounding-guided-by-part-level-semantic.html">Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors</a></td>
  <td>æå‡ºå¼±ç›‘ç£çš„å¯ä¾›æ€§å®šä½æ–¹æ³•ä»¥è§£å†³æ ‡ç­¾ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">affordance</span> <span class="paper-tag">human-object interaction</span> <span class="paper-tag">egocentric</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24103v1" data-paper-url="./papers/250524103v1-weakly-supervised-affordance-grounding-guided-by-part-level-semantic.html" onclick="toggleFavorite(this, '2505.24103v1', 'Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250524517v1-un2clip-improving-clips-visual-detail-capturing-ability-via-invertin.html">un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP</a></td>
  <td>æå‡ºun$^2$CLIPä»¥æå‡CLIPåœ¨è§†è§‰ç»†èŠ‚æ•æ‰èƒ½åŠ›çš„è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24517v1" data-paper-url="./papers/250524517v1-un2clip-improving-clips-visual-detail-capturing-ability-via-invertin.html" onclick="toggleFavorite(this, '2505.24517v1', 'un$^2$CLIP: Improving CLIP&#39;s Visual Detail Capturing Ability via Inverting unCLIP')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250600280v1-3d-gaussian-splat-vulnerabilities.html">3D Gaussian Splat Vulnerabilities</a></td>
  <td>æå‡ºCLOAKä¸DAGGERä»¥æ­ç¤º3Dé«˜æ–¯ç‚¹äº‘çš„å®‰å…¨æ¼æ´</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00280v1" data-paper-url="./papers/250600280v1-3d-gaussian-splat-vulnerabilities.html" onclick="toggleFavorite(this, '2506.00280v1', '3D Gaussian Splat Vulnerabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250524625v3-learning-from-videos-for-3d-world-enhancing-mllms-with-3d-vision-geo.html">Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors</a></td>
  <td>æå‡ºVG LLMä»¥è§£å†³è§†é¢‘ç›´æ¥ç†è§£3Dåœºæ™¯çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24625v3" data-paper-url="./papers/250524625v3-learning-from-videos-for-3d-world-enhancing-mllms-with-3d-vision-geo.html" onclick="toggleFavorite(this, '2505.24625v3', 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250524669v1-6d-pose-estimation-on-point-cloud-data-through-prior-knowledge-integ.html">6D Pose Estimation on Point Cloud Data through Prior Knowledge Integration: A Case Study in Autonomous Disassembly</a></td>
  <td>æå‡ºåŸºäºå…ˆéªŒçŸ¥è¯†çš„6Då§¿æ€ä¼°è®¡æ–¹æ³•ä»¥è§£å†³è‡ªåŠ¨æ‹†å¸é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">6D pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24669v1" data-paper-url="./papers/250524669v1-6d-pose-estimation-on-point-cloud-data-through-prior-knowledge-integ.html" onclick="toggleFavorite(this, '2505.24669v1', '6D Pose Estimation on Point Cloud Data through Prior Knowledge Integration: A Case Study in Autonomous Disassembly')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250524877v1-adahuman-animatable-detailed-3d-human-generation-with-compositional-.html">AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</a></td>
  <td>æå‡ºAdaHumanä»¥è§£å†³é«˜è´¨é‡3Däººç±»å¤´åƒç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24877v1" data-paper-url="./papers/250524877v1-adahuman-animatable-detailed-3d-human-generation-with-compositional-.html" onclick="toggleFavorite(this, '2505.24877v1', 'AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>41</td>
  <td><a href="./papers/250524257v1-out-of-sight-not-out-of-context-egocentric-spatial-reasoning-in-vlms.html">Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames</a></td>
  <td>æå‡ºDisjoint-3DQAåŸºå‡†ä»¥è§£å†³é•¿æ—¶é—´ç©ºé—´æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24257v1" data-paper-url="./papers/250524257v1-out-of-sight-not-out-of-context-egocentric-spatial-reasoning-in-vlms.html" onclick="toggleFavorite(this, '2505.24257v1', 'Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250524389v2-leadership-assessment-in-pediatric-intensive-care-unit-team-training.html">Leadership Assessment in Pediatric Intensive Care Unit Team Training</a></td>
  <td>æå‡ºè‡ªåŠ¨åŒ–åˆ†ææ¡†æ¶ä»¥è¯„ä¼°PICUå›¢é˜Ÿçš„é¢†å¯¼èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24389v2" data-paper-url="./papers/250524389v2-leadership-assessment-in-pediatric-intensive-care-unit-team-training.html" onclick="toggleFavorite(this, '2505.24389v2', 'Leadership Assessment in Pediatric Intensive Care Unit Team Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250524690v1-learning-reusable-concepts-across-different-egocentric-video-underst.html">Learning reusable concepts across different egocentric video understanding tasks</a></td>
  <td>æå‡ºHier-EgoPackæ¡†æ¶ä»¥è§£å†³è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„æ¦‚å¿µé‡ç”¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24690v1" data-paper-url="./papers/250524690v1-learning-reusable-concepts-across-different-egocentric-video-underst.html" onclick="toggleFavorite(this, '2505.24690v1', 'Learning reusable concepts across different egocentric video understanding tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/250524404v1-pcie-interaction-solution-for-ego4d-social-interaction-challenge.html">PCIE_Interaction Solution for Ego4D Social Interaction Challenge</a></td>
  <td>æå‡ºPCIE_Interactionè§£å†³æ–¹æ¡ˆä»¥åº”å¯¹Ego4Dç¤¾äº¤äº’åŠ¨æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">Ego4D</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24404v1" data-paper-url="./papers/250524404v1-pcie-interaction-solution-for-ego4d-social-interaction-challenge.html" onclick="toggleFavorite(this, '2505.24404v1', 'PCIE_Interaction Solution for Ego4D Social Interaction Challenge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250524848v2-reading-recognition-in-the-wild.html">Reading Recognition in the Wild</a></td>
  <td>æå‡ºé˜…è¯»è¯†åˆ«ä»»åŠ¡ä»¥è§£å†³æ™ºèƒ½çœ¼é•œä¸­çš„ç”¨æˆ·äº¤äº’è®°å½•é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24848v2" data-paper-url="./papers/250524848v2-reading-recognition-in-the-wild.html" onclick="toggleFavorite(this, '2505.24848v2', 'Reading Recognition in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/250524411v1-pcie-pose-solution-for-egoexo4d-pose-and-proficiency-estimation-chal.html">PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge</a></td>
  <td>æå‡ºHP-ViT+è§£å†³RGBè§†é¢‘ä¸­çš„æ‰‹éƒ¨å§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24411v1" data-paper-url="./papers/250524411v1-pcie-pose-solution-for-egoexo4d-pose-and-proficiency-estimation-chal.html" onclick="toggleFavorite(this, '2505.24411v1', 'PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>47</td>
  <td><a href="./papers/250600123v1-visual-embodied-brain-let-multimodal-large-language-models-see-think.html">Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces</a></td>
  <td>æå‡ºVeBrainæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00123v1" data-paper-url="./papers/250600123v1-visual-embodied-brain-let-multimodal-large-language-models-see-think.html" onclick="toggleFavorite(this, '2506.00123v1', 'Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/250524156v1-towards-a-generalizable-bimanual-foundation-policy-via-flow-based-vi.html">Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction</a></td>
  <td>æå‡ºåŸºäºæµçš„è§†é¢‘é¢„æµ‹æ–¹æ³•ä»¥è§£å†³åŒæ‰‹æ“ä½œç­–ç•¥æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">bi-manual</span> <span class="paper-tag">dual-arm</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24156v1" data-paper-url="./papers/250524156v1-towards-a-generalizable-bimanual-foundation-policy-via-flow-based-vi.html" onclick="toggleFavorite(this, '2505.24156v1', 'Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>49</td>
  <td><a href="./papers/250524139v2-s4-driver-scalable-self-supervised-driving-multimodal-large-language.html">S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation</a></td>
  <td>æå‡ºS4-Driverä»¥è§£å†³è‡ªç›‘ç£é©¾é©¶è§„åˆ’ä¸­çš„è¾“å…¥è¡¨ç¤ºä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24139v2" data-paper-url="./papers/250524139v2-s4-driver-scalable-self-supervised-driving-multimodal-large-language.html" onclick="toggleFavorite(this, '2505.24139v2', 'S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/250524214v1-benchmarking-foundation-models-for-zero-shot-biometric-tasks.html">Benchmarking Foundation Models for Zero-Shot Biometric Tasks</a></td>
  <td>æå‡ºåŸºäºåŸºç¡€æ¨¡å‹çš„é›¶-shotç”Ÿç‰©è¯†åˆ«ä»»åŠ¡åŸºå‡†è¯„ä¼°</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24214v1" data-paper-url="./papers/250524214v1-benchmarking-foundation-models-for-zero-shot-biometric-tasks.html" onclick="toggleFavorite(this, '2505.24214v1', 'Benchmarking Foundation Models for Zero-Shot Biometric Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>51</td>
  <td><a href="./papers/250524238v2-mirage-assessing-hallucination-in-multimodal-reasoning-chains-of-mll.html">MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM</a></td>
  <td>æå‡ºMIRAGEåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24238v2" data-paper-url="./papers/250524238v2-mirage-assessing-hallucination-in-multimodal-reasoning-chains-of-mll.html" onclick="toggleFavorite(this, '2505.24238v2', 'MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>52</td>
  <td><a href="./papers/250524787v1-draw-all-your-imagine-a-holistic-benchmark-and-agent-framework-for-c.html">Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation</a></td>
  <td>æå‡ºLongBench-T2IåŸºå‡†ä»¥è§£å†³å¤æ‚æŒ‡ä»¤å›¾åƒç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24787v1" data-paper-url="./papers/250524787v1-draw-all-your-imagine-a-holistic-benchmark-and-agent-framework-for-c.html" onclick="toggleFavorite(this, '2505.24787v1', 'Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>53</td>
  <td><a href="./papers/250524401v1-s3ce-net-spike-guided-spatiotemporal-semantic-coupling-and-expansion.html">S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification</a></td>
  <td>æå‡ºS3CE-Netä»¥è§£å†³é•¿åºåˆ—äº‹ä»¶é‡è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24401v1" data-paper-url="./papers/250524401v1-s3ce-net-spike-guided-spatiotemporal-semantic-coupling-and-expansion.html" onclick="toggleFavorite(this, '2505.24401v1', 'S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>54</td>
  <td><a href="./papers/250524375v1-spatiotemporal-analysis-of-forest-machine-operations-using-3d-video-.html">Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ä»¥åˆ†ç±»æ£®æ—æœºæ¢°æ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24375v1" data-paper-url="./papers/250524375v1-spatiotemporal-analysis-of-forest-machine-operations-using-3d-video-.html" onclick="toggleFavorite(this, '2505.24375v1', 'Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>55</td>
  <td><a href="./papers/250600227v2-ctrl-crash-controllable-diffusion-for-realistic-car-crashes.html">Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes</a></td>
  <td>æå‡ºCtrl-Crashä»¥è§£å†³çœŸå®æ±½è½¦ç¢°æ’æ¨¡æ‹Ÿé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00227v2" data-paper-url="./papers/250600227v2-ctrl-crash-controllable-diffusion-for-realistic-car-crashes.html" onclick="toggleFavorite(this, '2506.00227v2', 'Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>56</td>
  <td><a href="./papers/250524873v1-minimax-remover-taming-bad-noise-helps-video-object-removal.html">MiniMax-Remover: Taming Bad Noise Helps Video Object Removal</a></td>
  <td>æå‡ºMiniMax-Removerä»¥è§£å†³è§†é¢‘å¯¹è±¡ç§»é™¤ä¸­çš„å™ªå£°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24873v1" data-paper-url="./papers/250524873v1-minimax-remover-taming-bad-noise-helps-video-object-removal.html" onclick="toggleFavorite(this, '2505.24873v1', 'MiniMax-Remover: Taming Bad Noise Helps Video Object Removal')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)