---
layout: default
title: Reasoning Can Hurt the Inductive Abilities of Large Language Models
---

# Reasoning Can Hurt the Inductive Abilities of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24225" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24225v1</a>
  <a href="https://arxiv.org/pdf/2505.24225.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24225v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24225v1', 'Reasoning Can Hurt the Inductive Abilities of Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: 26 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»“æ„åŒ–å¹²é¢„ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„å½’çº³æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å½’çº³æ¨ç†` `é“¾å¼æ€ç»´` `ç»“æ„åŒ–å¹²é¢„` `æ¨ç†å¤±è´¥æ¨¡å¼` `æ€§èƒ½æå‡` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å½’çº³æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç¨€ç–ç¤ºä¾‹æ—¶ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡ç»“æ„åŒ–å¹²é¢„æ¥ä¼˜åŒ–é“¾å¼æ€ç»´ç”Ÿæˆï¼Œä»¥åº”å¯¹æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯æ”¾å¤§é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ç»“æ„åŒ–å¹²é¢„åï¼Œæ¨¡å‹çš„å½’çº³å‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å½’çº³æ¨ç†èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚é€šå¸¸è®¤ä¸ºï¼Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥å¢å¼ºè¿™ç§æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡åˆ›å»ºå››ä¸ªå—æ§çš„åŸºäºæ¸¸æˆçš„ä»»åŠ¡ï¼ˆå›½é™…è±¡æ£‹ã€å¾·å·æ‰‘å…‹ã€éª°å­æ¸¸æˆå’ŒäºŒåä¸€ç‚¹ï¼‰ï¼Œæ¢è®¨äº†è¿™ä¸€å‡è®¾ã€‚ç ”ç©¶å‘ç°ï¼ŒCoTæ¨ç†å¯èƒ½ä¼šé™ä½å½’çº³æ€§èƒ½ï¼ŒLRMså¾€å¾€è¡¨ç°ä¸å¦‚å…¶éæ¨ç†å¯¹æ‰‹ã€‚ä¸ºäº†è§£é‡Šè¿™ä¸€ç°è±¡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œæ­ç¤ºäº†æ¨ç†æ­¥éª¤å¦‚ä½•é€šè¿‡ä¸‰ç§å¤±è´¥æ¨¡å¼æ”¾å¤§é”™è¯¯ã€‚åŸºäºç†è®ºå’Œå®è¯åˆ†æï¼Œæœ¬æ–‡å¼•å…¥äº†ç»“æ„åŒ–å¹²é¢„ï¼Œä¾æ®è¯†åˆ«çš„å¤±è´¥ç±»å‹è°ƒæ•´CoTç”Ÿæˆï¼Œä»è€Œåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜å½’çº³å‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ‰æ•ˆçš„CoTæ¨ç†ä¸ä»…ä¾èµ–äºæ­¥éª¤æ•°é‡ï¼Œè¿˜éœ€ç¡®ä¿æ­¥éª¤ç»“æ„åˆç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å½’çº³æ¨ç†ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯é“¾å¼æ€ç»´æç¤ºå¯èƒ½å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå¯¼è‡´å½’çº³èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è¯†åˆ«æ¨ç†è¿‡ç¨‹ä¸­çš„å¤±è´¥æ¨¡å¼ï¼Œè®¾è®¡ç»“æ„åŒ–å¹²é¢„æ¥ä¼˜åŒ–é“¾å¼æ€ç»´ç”Ÿæˆã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å‡å°‘æ¨ç†æ­¥éª¤ä¸­çš„é”™è¯¯ï¼Œä»è€Œæå‡æ¨¡å‹çš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šä»»åŠ¡è®¾è®¡ã€å¤±è´¥æ¨¡å¼è¯†åˆ«ã€ç»“æ„åŒ–å¹²é¢„ç”Ÿæˆå’Œæ€§èƒ½è¯„ä¼°ã€‚é€šè¿‡è¿™äº›æ¨¡å—ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„æ¸¸æˆä»»åŠ¡ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸‰ç§æ¨ç†å¤±è´¥æ¨¡å¼ï¼Œå¹¶åŸºäºè¿™äº›æ¨¡å¼è®¾è®¡äº†ç›¸åº”çš„ç»“æ„åŒ–å¹²é¢„ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€åªå…³æ³¨æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œè€Œå¿½è§†äº†æ­¥éª¤çš„ç»“æ„æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæœ¬æ–‡é€šè¿‡å®éªŒç¡®å®šäº†æœ€ä¼˜çš„å¹²é¢„ç­–ç•¥ï¼Œå¹¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¯¹é”™è¯¯ç±»å‹çš„æƒ©ç½šæœºåˆ¶ï¼Œä»¥å¼•å¯¼æ¨¡å‹æ›´å¥½åœ°è¿›è¡Œæ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ç»“æ„åŒ–å¹²é¢„åï¼Œæ¨¡å‹åœ¨å½’çº³æ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§æé«˜äº†çº¦15%ï¼Œç›¸æ¯”äºæœªé‡‡ç”¨å¹²é¢„çš„å¯¹ç…§ç»„ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ¨ç†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€æ•™è‚²æŠ€æœ¯å’Œæ¸¸æˆAIç­‰ã€‚é€šè¿‡æå‡å¤§è¯­è¨€æ¨¡å‹çš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å¾—æ›´åŠ æ™ºèƒ½å’Œé«˜æ•ˆï¼Œè¿›è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›ä½¿ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts.
>   To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.

