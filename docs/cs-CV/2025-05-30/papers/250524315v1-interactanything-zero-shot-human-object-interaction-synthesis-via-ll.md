---
layout: default
title: InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing
---

# InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24315" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24315v1</a>
  <a href="https://arxiv.org/pdf/2505.24315.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24315v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24315v1', 'InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinlu Zhang, Yixin Chen, Zan Wang, Jie Yang, Yizhou Wang, Siyuan Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: CVPR 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInteractAnythingä»¥è§£å†³é›¶æ ·æœ¬äººæœºäº¤äº’åˆæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `3Dç”Ÿæˆ` `é›¶æ ·æœ¬å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å›¾åƒè§£æ` `ä¼˜åŒ–ç®—æ³•` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä»æ–‡æœ¬ç”Ÿæˆæ–°çš„äººæœºäº¤äº’æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¼€æ”¾é›†å¯¹è±¡æ—¶ï¼Œé¢ä¸´äººæœºå…³ç³»æ¨ç†å’Œå§¿æ€åˆæˆçš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬3D HOIç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨æ–­äººæœºå…³ç³»ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„2Då›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¯¹è±¡è§£æã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº¤äº’ç»†è‡´ç¨‹åº¦å’Œå¼€æ”¾é›†3Då¯¹è±¡å¤„ç†èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œ3Däººç±»æ„ŸçŸ¥ç”Ÿæˆå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ä»æ–‡æœ¬ç”Ÿæˆæ–°çš„äººæœºäº¤äº’ï¼ˆHOIï¼‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºå¼€æ”¾é›†å¯¹è±¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬3D HOIç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œæ— éœ€åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè®­ç»ƒã€‚é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨æ–­äººæœºå…³ç³»ï¼Œåˆå§‹åŒ–å¯¹è±¡å±æ€§å¹¶æŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„2Då›¾åƒæ‰©æ•£æ¨¡å‹è§£ææœªè§å¯¹è±¡å¹¶æå–æ¥è§¦ç‚¹ï¼Œé¿å…äº†ç°æœ‰3Dèµ„äº§çŸ¥è¯†çš„é™åˆ¶ã€‚æœ€ç»ˆï¼Œé€šè¿‡ç»†è‡´çš„ä¼˜åŒ–ç”Ÿæˆç²¾ç»†ã€è‡ªç„¶çš„äº¤äº’ï¼Œç¡®ä¿3Då¯¹è±¡ä¸æ¶‰åŠèº«ä½“éƒ¨ä½ï¼ˆå¦‚æ‰‹ï¼‰çš„çœŸå®æ¥è§¦ã€‚å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äº¤äº’çš„ç»†è‡´ç¨‹åº¦å’Œå¤„ç†å¼€æ”¾é›†3Då¯¹è±¡çš„èƒ½åŠ›ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»æ–‡æœ¬ç”Ÿæˆæ–°çš„äººæœºäº¤äº’ï¼ˆHOIï¼‰æ—¶çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¼€æ”¾é›†å¯¹è±¡çš„å¤„ç†ã€‚ç°æœ‰æ–¹æ³•åœ¨ç²¾ç¡®çš„äººæœºå…³ç³»æ¨ç†ã€ç‰©ä½“å¯ç”¨æ€§è§£æå’Œå§¿æ€åˆæˆä¸Šå­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨æ–­äººæœºå…³ç³»ï¼Œåˆå§‹åŒ–å¯¹è±¡å±æ€§å¹¶æŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ï¼Œé¿å…äº†å¯¹ç‰¹å®šæ•°æ®é›†çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ©ç”¨LLMsæ¨æ–­äººæœºå…³ç³»ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„2Då›¾åƒæ‰©æ•£æ¨¡å‹è§£ææœªè§å¯¹è±¡ï¼›æœ€åï¼Œé€šè¿‡å¤šè§†è§’æ ·æœ¬ç”Ÿæˆåˆå§‹äººç±»å§¿æ€ï¼Œå¹¶è¿›è¡Œç»†è‡´ä¼˜åŒ–ä»¥ç”Ÿæˆè‡ªç„¶äº¤äº’ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆLLMsçš„åé¦ˆä¸ç‰©ä½“å¯ç”¨æ€§è§£æï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç‰¹å®šæ•°æ®é›†çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡çš„3D HOIï¼Œæ˜¾è‘—æå‡äº†äº¤äº’çš„ç»†è‡´ç¨‹åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œé‡‡ç”¨äº†å¤šè§†è§’æ ·æœ¬ç”Ÿæˆç­–ç•¥ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ç¡®ä¿3Då¯¹è±¡ä¸èº«ä½“éƒ¨ä½çš„çœŸå®æ¥è§¦ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥äº†äººç±»åé¦ˆä»¥æå‡ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨äº¤äº’ç»†è‡´ç¨‹åº¦ä¸Šç›¸æ¯”äºç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨å¤„ç†å¼€æ”¾é›†3Då¯¹è±¡æ—¶ï¼Œç”Ÿæˆçš„äº¤äº’è´¨é‡æ›´é«˜ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œäº¤äº’çš„è‡ªç„¶æ€§å’ŒçœŸå®æ„Ÿå‡æœ‰æ˜æ˜¾æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘å’Œäººæœºäº¤äº’è®¾è®¡ç­‰ã€‚é€šè¿‡å®ç°é«˜è´¨é‡çš„3Däººæœºäº¤äº’åˆæˆï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåº”ç”¨åˆ›æ–°ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆå’Œæ™ºèƒ½æœºå™¨äººé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in 3D human-aware generation have made significant progress. However, existing methods still struggle with generating novel Human Object Interaction (HOI) from text, particularly for open-set objects. We identify three main challenges of this task: precise human-object relation reasoning, affordance parsing for any object, and detailed human interaction pose synthesis aligning description and object geometry. In this work, we propose a novel zero-shot 3D HOI generation framework without training on specific datasets, leveraging the knowledge from large-scale pre-trained models. Specifically, the human-object relations are inferred from large language models (LLMs) to initialize object properties and guide the optimization process. Then we utilize a pre-trained 2D image diffusion model to parse unseen objects and extract contact points, avoiding the limitations imposed by existing 3D asset knowledge. The initial human pose is generated by sampling multiple hypotheses through multi-view SDS based on the input text and object geometry. Finally, we introduce a detailed optimization to generate fine-grained, precise, and natural interaction, enforcing realistic 3D contact between the 3D object and the involved body parts, including hands in grasping. This is achieved by distilling human-level feedback from LLMs to capture detailed human-object relations from the text instruction. Extensive experiments validate the effectiveness of our approach compared to prior works, particularly in terms of the fine-grained nature of interactions and the ability to handle open-set 3D objects.

