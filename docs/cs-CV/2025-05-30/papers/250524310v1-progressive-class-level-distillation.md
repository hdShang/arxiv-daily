---
layout: default
title: Progressive Class-level Distillation
---

# Progressive Class-level Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24310" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24310v1</a>
  <a href="https://arxiv.org/pdf/2505.24310.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24310v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24310v1', 'Progressive Class-level Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiayan Li, Jun Li, Zhourui Zhang, Jianhua Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¸è¿›å¼ç±»çº§è’¸é¦ä»¥è§£å†³çŸ¥è¯†è’¸é¦ä¸­çš„ä½æ¦‚ç‡ç±»ä¿¡æ¯ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `ç±»çº§è’¸é¦` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰` `æ¨¡å‹å‹ç¼©` `ç›®æ ‡æ£€æµ‹` `åˆ†ç±»ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨å¤„ç†é«˜ç½®ä¿¡åº¦ç±»æ—¶ï¼Œå¾€å¾€å¿½è§†ä½æ¦‚ç‡ç±»çš„ä¿¡æ¯ï¼Œå¯¼è‡´çŸ¥è¯†è½¬ç§»ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¸è¿›å¼ç±»çº§è’¸é¦æ–¹æ³•é€šè¿‡é˜¶æ®µæ€§è’¸é¦å®ç°é€æ­¥çš„çŸ¥è¯†è½¬ç§»ï¼Œç¡®ä¿ä½æ¦‚ç‡ç±»çš„ä¿¡æ¯å¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚
3. åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„æ‰©å±•å®éªŒè¡¨æ˜ï¼ŒPCDåœ¨åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨çŸ¥è¯†è’¸é¦ä¸­ï¼Œæ—¥å¿—è’¸é¦æ—¨åœ¨é€šè¿‡å‡†ç¡®çš„æ•™å¸ˆ-å­¦ç”Ÿå¯¹é½ï¼Œå°†å¼ºå¤§çš„æ•™å¸ˆç½‘ç»œçš„ç±»çº§çŸ¥è¯†è½¬ç§»åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€å¿½è§†ä½æ¦‚ç‡ç±»çš„ä¿¡æ¯ï¼Œå¯¼è‡´çŸ¥è¯†è½¬ç§»ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ¸è¿›å¼ç±»çº§è’¸é¦æ–¹æ³•ï¼ˆPCDï¼‰ï¼Œé€šè¿‡é˜¶æ®µæ€§è’¸é¦å®ç°é€æ­¥çŸ¥è¯†è½¬ç§»ã€‚å…·ä½“è€Œè¨€ï¼ŒPCDé€šè¿‡å¯¹æ•™å¸ˆ-å­¦ç”Ÿæ—¥å¿—å·®å¼‚è¿›è¡Œæ’åºï¼Œç¡®å®šè’¸é¦ä¼˜å…ˆçº§ï¼Œå¹¶å°†æ•´ä¸ªè¿‡ç¨‹åˆ†ä¸ºå¤šä¸ªé˜¶æ®µï¼Œé‡‡ç”¨åŒå‘é˜¶æ®µè’¸é¦ï¼Œå…è®¸åœ¨ä¸åŒé˜¶æ®µå†…è¿›è¡Œå……åˆ†çš„æ—¥å¿—å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCDåœ¨åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨å¤„ç†ä½æ¦‚ç‡ç±»æ—¶çš„ä¿¡æ¯è½¬ç§»ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾§é‡äºé«˜ç½®ä¿¡åº¦ç±»ï¼Œå¯¼è‡´ä½æ¦‚ç‡ç±»çš„çŸ¥è¯†è¢«å¿½è§†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ¸è¿›å¼ç±»çº§è’¸é¦æ–¹æ³•é€šè¿‡é˜¶æ®µæ€§è’¸é¦ï¼Œé€æ­¥è½¬ç§»çŸ¥è¯†ï¼Œç¡®ä¿ä½æ¦‚ç‡ç±»çš„ä¿¡æ¯å¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ—¥å¿—å·®å¼‚çš„æ’åºï¼Œç¡®å®šè’¸é¦çš„ä¼˜å…ˆçº§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPCDçš„æ•´ä½“æ¶æ„åˆ†ä¸ºå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µè¿›è¡ŒåŒå‘è’¸é¦ï¼Œç»“åˆç»†ç²’åº¦åˆ°ç²—ç²’åº¦çš„æ¸è¿›å­¦ä¹ å’Œåå‘ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„ç²¾ç»†åŒ–ã€‚æ¯ä¸ªé˜¶æ®µå†…ï¼Œé’ˆå¯¹ä¸åŒç±»ç»„è¿›è¡Œå……åˆ†çš„æ—¥å¿—å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé˜¶æ®µæ€§è’¸é¦çš„è®¾è®¡ï¼Œä½¿å¾—çŸ¥è¯†è½¬ç§»æ›´åŠ é«˜æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨ä½æ¦‚ç‡ç±»çš„å¤„ç†ä¸Šï¼Œä¸ç°æœ‰æ–¹æ³•çš„å…¨ç±»è’¸é¦å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒPCDé€šè¿‡å¯¹æ•™å¸ˆ-å­¦ç”Ÿæ—¥å¿—å·®å¼‚çš„æ’åºæ¥ç¡®å®šè’¸é¦ä¼˜å…ˆçº§ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šé‡‡ç”¨äº†é€‚åº”æ€§è°ƒæ•´ï¼Œä»¥ç¡®ä¿ä¸åŒé˜¶æ®µçš„è’¸é¦æ•ˆæœæœ€ä½³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPCDåœ¨å¤šä¸ªå…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œåˆ†ç±»ä»»åŠ¡çš„å‡†ç¡®ç‡æé«˜äº†X%ï¼Œæ£€æµ‹ä»»åŠ¡çš„mAPæå‡äº†Y%ã€‚è¿™äº›ç»“æœè¡¨æ˜PCDåœ¨çŸ¥è¯†è’¸é¦é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤„ç†ä¸å¹³è¡¡ç±»åˆ«åˆ†å¸ƒçš„åœºæ™¯ä¸­ã€‚é€šè¿‡æœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ï¼ŒPCDå¯ä»¥æå‡å°å‹æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In knowledge distillation (KD), logit distillation (LD) aims to transfer class-level knowledge from a more powerful teacher network to a small student model via accurate teacher-student alignment at the logits level. Since high-confidence object classes usually dominate the distillation process, low-probability classes which also contain discriminating information are downplayed in conventional methods, leading to insufficient knowledge transfer. To address this issue, we propose a simple yet effective LD method termed Progressive Class-level Distillation (PCD). In contrast to existing methods which perform all-class ensemble distillation, our PCD approach performs stage-wise distillation for step-by-step knowledge transfer. More specifically, we perform ranking on teacher-student logits difference for identifying distillation priority from scratch, and subsequently divide the entire LD process into multiple stages. Next, bidirectional stage-wise distillation incorporating fine-to-coarse progressive learning and reverse coarse-to-fine refinement is conducted, allowing comprehensive knowledge transfer via sufficient logits alignment within separate class groups in different distillation stages. Extension experiments on public benchmarking datasets demonstrate the superiority of our method compared to state-of-the-arts for both classification and detection tasks.

