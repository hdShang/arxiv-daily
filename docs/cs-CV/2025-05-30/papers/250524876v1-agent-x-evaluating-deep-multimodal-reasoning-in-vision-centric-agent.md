---
layout: default
title: "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks"
---

# Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24876" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24876v1</a>
  <a href="https://arxiv.org/pdf/2505.24876.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24876v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24876v1', 'Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair Nawaz, Jean Lahoud, Hisham Cholakkal, Mubarak Shah, Philip Torr, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mbzuai-oryx/Agent-X)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAgent-Xä»¥è§£å†³å¤šæ­¥è§†è§‰æ¨ç†ä»»åŠ¡è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰ç†è§£` `æ·±åº¦å­¦ä¹ ` `ä»£ç†ä»»åŠ¡` `æ¨ç†è¯„ä¼°` `å·¥å…·ä½¿ç”¨` `çœŸå®åœºæ™¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–åˆæˆçš„å•è½®æŸ¥è¯¢ï¼Œç¼ºä¹å¯¹å¤šæ­¥æ¨ç†å’ŒçœŸå®åœºæ™¯çš„è€ƒé‡ã€‚
2. è®ºæ–‡æå‡ºAgent-XåŸºå‡†ï¼ŒåŒ…å«828ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¼ºè°ƒåœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œå¤šæ­¥æ¨ç†çš„èƒ½åŠ›è¯„ä¼°ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€ä½³æ¨¡å‹åœ¨å¤šæ­¥è§†è§‰ä»»åŠ¡ä¸­çš„æˆåŠŸç‡ä½äº50%ï¼Œæ­ç¤ºäº†æ¨ç†èƒ½åŠ›çš„ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦æ¨ç†åœ¨è§£å†³å¤æ‚ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é¡ºåºå’Œå¤šæ¨¡æ€ç†è§£çš„è§†è§‰ä¸­å¿ƒåœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†é€šå¸¸ä»…è¯„ä¼°å…·æœ‰å®Œå…¨åˆæˆçš„å•è½®æŸ¥è¯¢çš„ä»£ç†ï¼Œè§†è§‰æ¨¡æ€æœ‰é™ï¼Œä¸”ç¼ºä¹è¯„ä¼°å¤šæ­¥éª¤æ¨ç†è´¨é‡çš„æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Agent-Xï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰ä¸­å¿ƒä»£ç†åœ¨çœŸå®å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„å¤šæ­¥å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ã€‚Agent-XåŒ…å«828ä¸ªå…·æœ‰çœŸå®è§†è§‰ä¸Šä¸‹æ–‡çš„ä»£ç†ä»»åŠ¡ï¼Œæ¶µç›–ä¸€èˆ¬è§†è§‰æ¨ç†ã€ç½‘é¡µæµè§ˆã€å®‰å…¨ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€ä½“è‚²å’Œæ•°å­¦æ¨ç†ç­‰å…­å¤§ç¯å¢ƒã€‚æˆ‘ä»¬çš„åŸºå‡†è¦æ±‚ä»£ç†åœ¨è¿™äº›å¤šæ ·åŒ–çš„ç¯å¢ƒä¸­ç»“åˆå·¥å…·ä½¿ç”¨ä¸æ˜ç¡®çš„é€æ­¥å†³ç­–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»†ç²’åº¦çš„é€æ­¥è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œä»¥åŠå·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå¦‚GPTã€Geminiå’ŒQwenç³»åˆ—ï¼Œåœ¨è§£å†³å¤šæ­¥è§†è§‰ä»»åŠ¡æ—¶ä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼Œå®Œæ•´é“¾æˆåŠŸç‡ä¸è¶³50%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰LMMæ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„å…³é”®ç“¶é¢ˆï¼Œå¹¶æŒ‡æ˜äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨ç†åŸºå‡†åœ¨çœŸå®åœºæ™¯ä¸­è¯„ä¼°å¤šæ­¥æ¨ç†èƒ½åŠ›çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨å•è½®æŸ¥è¯¢ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°å¤æ‚ä»»åŠ¡çš„æ¨ç†è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºAgent-XåŸºå‡†ï¼Œé€šè¿‡è®¾è®¡å¤šæ ·åŒ–çš„çœŸå®è§†è§‰ä»»åŠ¡ï¼Œè¦æ±‚ä»£ç†åœ¨å¤šæ­¥æ¨ç†ä¸­ç»“åˆå·¥å…·ä½¿ç”¨ä¸é€æ­¥å†³ç­–ï¼Œæå‡æ¨ç†èƒ½åŠ›çš„è¯„ä¼°æ ‡å‡†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAgent-Xçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡ç”Ÿæˆæ¨¡å—ã€è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ã€‚ä»»åŠ¡ç”Ÿæˆæ¨¡å—åˆ›å»ºå¤šç§çœŸå®åœºæ™¯ä»»åŠ¡ï¼Œè¯„ä¼°æ¡†æ¶åˆ™ç”¨äºåˆ†ææ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†ç»†ç²’åº¦çš„é€æ­¥è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°åˆ†ææ¯ä¸€æ­¥æ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„æ¨ç†è´¨é‡è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œä»»åŠ¡æ¶µç›–å¤šç§è§†è§‰æ¨¡æ€ï¼Œå¦‚å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬ï¼Œè¯„ä¼°è¿‡ç¨‹ä¸­é‡‡ç”¨äº†æ˜ç¡®çš„å·¥å…·ä½¿ç”¨æ ‡å‡†ï¼Œç¡®ä¿æ¯ä¸ªæ¨ç†æ­¥éª¤çš„é€»è¾‘æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤šæ­¥è§†è§‰ä»»åŠ¡ä¸­çš„å®Œæ•´é“¾æˆåŠŸç‡ä¸è¶³50%ï¼Œå¦‚GPTã€Geminiå’ŒQwenç³»åˆ—æ¨¡å‹å‡æœªèƒ½æœ‰æ•ˆè§£å†³è¿™äº›ä»»åŠ¡ã€‚è¿™ä¸€ç»“æœæ­ç¤ºäº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„æ˜¾è‘—ä¸è¶³ï¼ŒæŒ‡å‘äº†æœªæ¥ç ”ç©¶çš„å…³é”®æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå’Œå®‰å…¨ç›‘æ§ç­‰ï¼Œèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œæ¨ç†æ°´å¹³ã€‚æœªæ¥ï¼ŒAgent-XåŸºå‡†å°†æ¨åŠ¨å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„ç ”ç©¶ä¸å‘å±•ï¼Œä¿ƒè¿›æ›´æ™ºèƒ½çš„è§†è§‰ä¸­å¿ƒä»£ç†çš„å‡ºç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep reasoning is fundamental for solving complex tasks, especially in vision-centric scenarios that demand sequential, multimodal understanding. However, existing benchmarks typically evaluate agents with fully synthetic, single-turn queries, limited visual modalities, and lack a framework to assess reasoning quality over multiple steps as required in real-world settings. To address this, we introduce Agent-X, a large-scale benchmark for evaluating vision-centric agents multi-step and deep reasoning capabilities in real-world, multimodal settings. Agent- X features 828 agentic tasks with authentic visual contexts, including images, multi-image comparisons, videos, and instructional text. These tasks span six major agentic environments: general visual reasoning, web browsing, security and surveillance, autonomous driving, sports, and math reasoning. Our benchmark requires agents to integrate tool use with explicit, stepwise decision-making in these diverse settings. In addition, we propose a fine-grained, step-level evaluation framework that assesses the correctness and logical coherence of each reasoning step and the effectiveness of tool usage throughout the task. Our results reveal that even the best-performing models, including GPT, Gemini, and Qwen families, struggle to solve multi-step vision tasks, achieving less than 50% full-chain success. These findings highlight key bottlenecks in current LMM reasoning and tool-use capabilities and identify future research directions in vision-centric agentic reasoning models. Our data and code are publicly available at https://github.com/mbzuai-oryx/Agent-X

