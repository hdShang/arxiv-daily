---
layout: default
title: Reading Recognition in the Wild
---

# Reading Recognition in the Wild

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24848" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24848v2</a>
  <a href="https://arxiv.org/pdf/2505.24848.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24848v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24848v2', 'Reading Recognition in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Charig Yang, Samiul Alam, Shakhrul Iman Siam, Michael J. Proulx, Lambert Mathias, Kiran Somasundaram, Luis Pesqueira, James Fort, Sheroze Sheriffdeen, Omkar Parkhi, Carl Ren, Mi Zhang, Yuning Chai, Richard Newcombe, Hyo Jin Kim

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-05)

**å¤‡æ³¨**: Project Page: https://www.projectaria.com/datasets/reading-in-the-wild/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé˜…è¯»è¯†åˆ«ä»»åŠ¡ä»¥è§£å†³æ™ºèƒ½çœ¼é•œä¸­çš„ç”¨æˆ·äº¤äº’è®°å½•é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é˜…è¯»è¯†åˆ«` `å¤šæ¨¡æ€å­¦ä¹ ` `æ™ºèƒ½çœ¼é•œ` `å˜æ¢å™¨æ¨¡å‹` `æ•°æ®é›†æ„å»º` `äººæœºäº¤äº’` `çœ¼åŠ¨è¿½è¸ª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€å’ŒçœŸå®åœºæ™¯ä¸­éš¾ä»¥å‡†ç¡®è¯†åˆ«ç”¨æˆ·çš„é˜…è¯»è¡Œä¸ºï¼Œç¼ºä¹å¤§è§„æ¨¡æ•°æ®æ”¯æŒã€‚
2. æœ¬æ–‡æå‡ºäº†é˜…è¯»è¯†åˆ«ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†åŒ…å«å¤šæ¨¡æ€ä¿¡æ¯çš„â€œé‡å¤–é˜…è¯»â€æ•°æ®é›†ï¼Œåˆ©ç”¨å˜æ¢å™¨æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨é˜…è¯»è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¸åŒæ¨¡æ€çš„ç»“åˆæ˜¾è‘—æå‡äº†è¯†åˆ«å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å®ç°å§‹ç»ˆåœ¨çº¿çš„æ™ºèƒ½çœ¼é•œä¸­çš„è‡ªæˆ‘ä¸­å¿ƒä¸Šä¸‹æ–‡äººå·¥æ™ºèƒ½ï¼Œè®°å½•ç”¨æˆ·ä¸ä¸–ç•Œçš„äº¤äº’ï¼ˆåŒ…æ‹¬é˜…è¯»ï¼‰è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ–°çš„ä»»åŠ¡â€”â€”é˜…è¯»è¯†åˆ«ï¼Œä»¥ç¡®å®šç”¨æˆ·ä½•æ—¶åœ¨é˜…è¯»ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†å¤§è§„æ¨¡å¤šæ¨¡æ€çš„â€œé‡å¤–é˜…è¯»â€æ•°æ®é›†ï¼ŒåŒ…å«100å°æ—¶çš„é˜…è¯»å’Œéé˜…è¯»è§†é¢‘ï¼Œæ¶µç›–å¤šæ ·ä¸”çœŸå®çš„åœºæ™¯ã€‚æˆ‘ä»¬è¯†åˆ«äº†ä¸‰ç§æ¨¡æ€ï¼ˆè‡ªæˆ‘ä¸­å¿ƒRGBã€çœ¼åŠ¨ã€å¤´éƒ¨å§¿æ€ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§çµæ´»çš„å˜æ¢å™¨æ¨¡å‹ï¼Œèƒ½å¤Ÿå•ç‹¬æˆ–ç»“åˆä½¿ç”¨è¿™äº›æ¨¡æ€æ¥å®Œæˆä»»åŠ¡ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™äº›æ¨¡æ€ä¸ä»»åŠ¡çš„ç›¸å…³æ€§å’Œäº’è¡¥æ€§ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåœ°ç¼–ç æ¯ç§æ¨¡æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†è¯¥æ•°æ®é›†åœ¨åˆ†ç±»é˜…è¯»ç±»å‹æ–¹é¢çš„å®ç”¨æ€§ï¼Œå°†å½“å‰åœ¨å—é™ç¯å¢ƒä¸‹è¿›è¡Œçš„é˜…è¯»ç†è§£ç ”ç©¶æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡ã€å¤šæ ·æ€§å’Œç°å®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨åŠ¨æ€å’ŒçœŸå®åœºæ™¯ä¸­è¯†åˆ«ç”¨æˆ·é˜…è¯»è¡Œä¸ºçš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå—é™ç¯å¢ƒï¼Œç¼ºä¹å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„æ•°æ®æ”¯æŒï¼Œå¯¼è‡´è¯†åˆ«å‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„é˜…è¯»è¯†åˆ«ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚é€šè¿‡ç»“åˆè‡ªæˆ‘ä¸­å¿ƒRGBå›¾åƒã€çœ¼åŠ¨å’Œå¤´éƒ¨å§¿æ€ç­‰ä¿¡æ¯ï¼Œåˆ©ç”¨å˜æ¢å™¨æ¨¡å‹æ¥æé«˜è¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡æ€ç‰¹å¾æå–å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä»è§†é¢‘ä¸­æå–è‡ªæˆ‘ä¸­å¿ƒRGBå›¾åƒã€çœ¼åŠ¨è½¨è¿¹å’Œå¤´éƒ¨å§¿æ€ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾è¾“å…¥åˆ°å˜æ¢å™¨æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé¦–æ¬¡å¼•å…¥å¤§è§„æ¨¡çš„â€œé‡å¤–é˜…è¯»â€æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯çš„å˜æ¢å™¨æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†é˜…è¯»è¯†åˆ«çš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œè®¾ç½®äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥å¹³è¡¡å„æ¨¡æ€çš„è´¡çŒ®ï¼ŒåŒæ—¶ä¼˜åŒ–äº†å˜æ¢å™¨çš„å±‚æ•°å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„å˜æ¢å™¨æ¨¡å‹åœ¨é˜…è¯»è¯†åˆ«ä»»åŠ¡ä¸Šè¾¾åˆ°äº†85%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†15%ã€‚ä¸åŒæ¨¡æ€çš„ç»“åˆæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€ä¿¡æ¯åœ¨å¤æ‚åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½çœ¼é•œã€å¢å¼ºç°å®å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡å‡†ç¡®è¯†åˆ«ç”¨æˆ·çš„é˜…è¯»è¡Œä¸ºï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå¢å¼ºçš„äº¤äº’ä½“éªŒï¼Œæ¨åŠ¨æ™ºèƒ½è®¾å¤‡çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨æ•™è‚²ã€åŒ»ç–—å’Œè¾…åŠ©æŠ€æœ¯ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To enable egocentric contextual AI in always-on smart glasses, it is crucial to be able to keep a record of the user's interactions with the world, including during reading. In this paper, we introduce a new task of reading recognition to determine when the user is reading. We first introduce the first-of-its-kind large-scale multimodal Reading in the Wild dataset, containing 100 hours of reading and non-reading videos in diverse and realistic scenarios. We then identify three modalities (egocentric RGB, eye gaze, head pose) that can be used to solve the task, and present a flexible transformer model that performs the task using these modalities, either individually or combined. We show that these modalities are relevant and complementary to the task, and investigate how to efficiently and effectively encode each modality. Additionally, we show the usefulness of this dataset towards classifying types of reading, extending current reading understanding studies conducted in constrained settings to larger scale, diversity and realism.

