---
layout: default
title: "Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces"
---

# Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00123" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00123v1</a>
  <a href="https://arxiv.org/pdf/2506.00123.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00123v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00123v1', 'Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, Shenglong Ye, Lewei Lu, Jingbo Wang, Wenhai Wang, Jifeng Dai, Yu Qiao, Rongrong Ji, Xizhou Zhu

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVeBrainæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„æ•´åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æœºå™¨äººæ§åˆ¶` `è§†è§‰ç©ºé—´æ¨ç†` `æ•°æ®é›†æ„å»º` `æ™ºèƒ½å†³ç­–` `äººæœºäº¤äº’` `é€‚åº”æ€§å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥å°†å¤šæ¨¡æ€ç†è§£ã€è§†è§‰ç©ºé—´æ¨ç†å’Œç‰©ç†äº¤äº’èƒ½åŠ›ç»Ÿä¸€ï¼Œé™åˆ¶äº†å…¶åœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„VeBrainæ¡†æ¶é€šè¿‡å°†æœºå™¨äººæ§åˆ¶è½¬åŒ–ä¸ºæ–‡æœ¬ä»»åŠ¡ï¼Œç»Ÿä¸€äº†ä¸åŒä»»åŠ¡çš„ç›®æ ‡å’Œæ˜ å°„ç©ºé—´ï¼Œæå‡äº†å¤šæ¨¡æ€èƒ½åŠ›çš„æ•´åˆã€‚
3. åœ¨13ä¸ªå¤šæ¨¡æ€åŸºå‡†å’Œ5ä¸ªç©ºé—´æ™ºèƒ½åŸºå‡†ä¸Šï¼ŒVeBrainç›¸è¾ƒäºç°æœ‰æ¨¡å‹Qwen2.5-VLè¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨å››è¶³æœºå™¨äººä»»åŠ¡ä¸­æå‡å¹…åº¦è¾¾50%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ˜¾è‘—è¿›å±•å¼•èµ·äº†å°†å…¶æ‰©å±•åˆ°ç‰©ç†å®ä½“ï¼ˆå¦‚å››è¶³æœºå™¨äººï¼‰çš„å…³æ³¨ã€‚è¿™é€šå¸¸è¦æ±‚MLLMsä¸ä»…å…·å¤‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œè¿˜éœ€æ•´åˆè§†è§‰ç©ºé—´æ¨ç†å’Œç‰©ç†äº¤äº’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å› å…¶åŸºæœ¬å·®å¼‚è€Œéš¾ä»¥ç»Ÿä¸€è¿™äº›èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†è§†è§‰å…·èº«å¤§è„‘ï¼ˆVeBrainï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç°å®ä¸–ç•Œæ„ŸçŸ¥ã€æ¨ç†å’Œæ§åˆ¶çš„ç»Ÿä¸€æ¡†æ¶ã€‚VeBrainå°†æœºå™¨äººæ§åˆ¶é‡æ–°è¡¨è¿°ä¸ºåŸºäºæ–‡æœ¬çš„MLLMä»»åŠ¡ï¼Œä»è€Œç»Ÿä¸€äº†ä¸åŒä»»åŠ¡çš„ç›®æ ‡å’Œæ˜ å°„ç©ºé—´ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æœºå™¨äººé€‚é…å™¨ï¼Œå°†MLLMçš„æ–‡æœ¬æ§åˆ¶ä¿¡å·è½¬æ¢ä¸ºçœŸå®æœºå™¨äººçš„è¿åŠ¨ç­–ç•¥ã€‚é€šè¿‡VeBrain-600kæ•°æ®é›†çš„æ„å»ºï¼Œå±•ç¤ºäº†VeBrainåœ¨å¤šæ¨¡æ€åŸºå‡†å’Œç©ºé—´æ™ºèƒ½åŸºå‡†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„èƒ½åŠ›æ•´åˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å› èƒ½åŠ›å·®å¼‚è€Œéš¾ä»¥ç»Ÿä¸€ï¼Œé™åˆ¶äº†å…¶åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVeBrainæ¡†æ¶é€šè¿‡å°†æœºå™¨äººæ§åˆ¶ä»»åŠ¡é‡æ–°è¡¨è¿°ä¸ºæ–‡æœ¬ä»»åŠ¡ï¼Œåˆ©ç”¨MLLMçš„å¼ºå¤§è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œç»Ÿä¸€äº†å¤šç§ä»»åŠ¡çš„ç›®æ ‡å’Œæ˜ å°„ç©ºé—´ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ§åˆ¶å’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVeBrainçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ„ŸçŸ¥æ¨¡å—ã€æ¨ç†æ¨¡å—å’Œæ§åˆ¶æ¨¡å—ã€‚æ„ŸçŸ¥æ¨¡å—è´Ÿè´£è·å–ç¯å¢ƒä¿¡æ¯ï¼Œæ¨ç†æ¨¡å—è¿›è¡Œå†³ç­–åˆ†æï¼Œæ§åˆ¶æ¨¡å—åˆ™å°†å†³ç­–è½¬åŒ–ä¸ºå…·ä½“çš„æœºå™¨äººåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šVeBrainçš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æœºå™¨äººé€‚é…å™¨ï¼Œå°†æ–‡æœ¬æ§åˆ¶ä¿¡å·æœ‰æ•ˆè½¬æ¢ä¸ºæœºå™¨äººçš„è¿åŠ¨ç­–ç•¥ï¼Œè§£å†³äº†å¤šæ¨¡æ€ä»»åŠ¡ä¹‹é—´çš„æ•´åˆé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®å¤„ç†ä¸Šï¼ŒVeBrain-600kæ•°æ®é›†ç»è¿‡æ•°ç™¾å°æ—¶çš„æ”¶é›†å’Œæ³¨é‡Šï¼Œé‡‡ç”¨å¤šæ¨¡æ€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•ï¼Œå°†ä¸åŒèƒ½åŠ›æ•´åˆä¸ºå•ä¸€å¯¹è¯ï¼Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒVeBrainåœ¨13ä¸ªå¤šæ¨¡æ€åŸºå‡†å’Œ5ä¸ªç©ºé—´æ™ºèƒ½åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºQwen2.5-VLåœ¨MMVetåŸºå‡†ä¸Šæå‡äº†5.6%ï¼Œåœ¨å››è¶³æœºå™¨äººä»»åŠ¡ä¸­å®ç°äº†50%çš„å¹³å‡æå‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VeBrainæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–æ§åˆ¶å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„å¤šæ¨¡æ€ç†è§£å’Œæ§åˆ¶èƒ½åŠ›å°†æ¨åŠ¨æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–å’Œæ“ä½œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities, but also integrate visual-spatial reasoning and physical interaction capabilities. Nevertheless,existing methods struggle to unify these capabilities due to their fundamental differences.In this paper, we present the Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning, and control in real world. VeBrain reformulates robotic control into common text-based MLLM tasks in the 2D visual space, thus unifying the objectives and mapping spaces of different tasks. Then, a novel robotic adapter is proposed to convert textual control signals from MLLMs to motion policies of real robots. From the data perspective, we further introduce VeBrain-600k, a high-quality instruction dataset encompassing various capabilities of VeBrain. In VeBrain-600k, we take hundreds of hours to collect, curate and annotate the data, and adopt multimodal chain-of-thought(CoT) to mix the different capabilities into a single conversation. Extensive experiments on 13 multimodal benchmarks and 5 spatial intelligence benchmarks demonstrate the superior performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to legged robots and robotic arms, VeBrain shows strong adaptability, flexibility, and compositional capabilities compared to existing methods. For example, compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by +5.6%, but also excels in legged robot tasks with +50% average gains.

