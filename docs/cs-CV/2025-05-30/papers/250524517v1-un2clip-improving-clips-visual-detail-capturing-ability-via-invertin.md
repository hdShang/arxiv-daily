---
layout: default
title: "un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP"
---

# un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24517" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.24517v1</a>
  <a href="https://arxiv.org/pdf/2505.24517.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24517v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24517v1', 'un$^2$CLIP: Improving CLIP\'s Visual Detail Capturing Ability via Inverting unCLIP')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yinqi Li, Jiahe Zhao, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-30

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/LiYinqi/un2CLIP)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫un$^2$CLIP‰ª•ÊèêÂçáCLIPÂú®ËßÜËßâÁªÜËäÇÊçïÊçâËÉΩÂäõÁöÑË°®Áé∞**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `CLIPÊ®°Âûã` `ËßÜËßâÁªÜËäÇÊçïÊçâ` `ÁîüÊàêÊ®°Âûã` `Â§öÊ®°ÊÄÅ‰ªªÂä°` `ÂõæÂÉèÁîüÊàê` `Ê∑±Â∫¶Â≠¶‰π†` `ËÆ°ÁÆóÊú∫ËßÜËßâ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑCLIPÊ®°ÂûãÂú®ÊçïÊçâÂõæÂÉèÁªÜËäÇÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂Âú®ÂØÜÈõÜÈ¢ÑÊµãÂíåÂ§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ
2. Êú¨Á†îÁ©∂ÊèêÂá∫un$^2$CLIPÔºåÈÄöËøáÂèçËΩ¨unCLIPÊ®°ÂûãÔºåÊèêÂçáCLIPÁöÑËßÜËßâÁªÜËäÇÊçïÊçâËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅ‰∏éÊñáÊú¨ÁºñÁ†ÅÂô®ÁöÑÂØπÈΩê„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºåun$^2$CLIPÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåË∂ÖË∂ä‰∫ÜÂéüÂßãCLIPÂèäÂÖ∂ÊîπËøõÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂØπÊØîËØ≠Ë®Ä-ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºàCLIPÔºâÂ∑≤Êàê‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºåÂπ∂ÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçËßÜËßâÂíåÂ§öÊ®°ÊÄÅ‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåËøëÊúüÁ†îÁ©∂Ë°®ÊòéÔºåCLIPÂú®Âå∫ÂàÜÂõæÂÉèÁªÜËäÇÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂Âú®ÂØÜÈõÜÈ¢ÑÊµãÂíå‰ª•ËßÜËßâ‰∏∫‰∏≠ÂøÉÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇÂõ†Ê≠§ÔºåÊú¨Á†îÁ©∂Êó®Âú®ÊîπËøõÁé∞ÊúâÁöÑCLIPÊ®°ÂûãÔºå‰ª•Â∞ΩÂèØËÉΩÊçïÊçâÂõæÂÉè‰∏≠ÁöÑËßÜËßâÁªÜËäÇ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁâπÂÆöÁ±ªÂûãÁöÑÁîüÊàêÊ®°ÂûãunCLIP‰∏∫ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÊèê‰æõ‰∫ÜÂêàÈÄÇÁöÑÊ°ÜÊû∂„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåunCLIPËÆ≠ÁªÉ‰∏Ä‰∏™Êù°‰ª∂‰∫éCLIPÂõæÂÉèÂµåÂÖ•ÁöÑÂõæÂÉèÁîüÊàêÂô®ÔºåÂèçËΩ¨‰∫ÜCLIPÂõæÂÉèÁºñÁ†ÅÂô®„ÄÇ‰∏éCLIPÁ≠âÂà§Âà´Ê®°ÂûãÁõ∏ÊØîÔºåÁîüÊàêÊ®°ÂûãÂú®ÊçïÊçâÂõæÂÉèÁªÜËäÇÊñπÈù¢Ë°®Áé∞Êõ¥‰Ω≥„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑun$^2$CLIPÈÄöËøáÂèçËΩ¨unCLIPÔºåÊèêÂçá‰∫ÜCLIPÊ®°ÂûãÁöÑËßÜËßâÁªÜËäÇÊçïÊçâËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅ‰∏éÂéüÂßãÊñáÊú¨ÁºñÁ†ÅÂô®ÁöÑÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåun$^2$CLIPÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÂéüÂßãCLIPÂèäÂÖ∂ÊîπËøõÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥CLIPÊ®°ÂûãÂú®ÂõæÂÉèÁªÜËäÇÊçïÊçâÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂØÜÈõÜÈ¢ÑÊµãÂíåÂ§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞‰∏ç‰Ω≥„ÄÇÁé∞ÊúâÁöÑCLIPÊ®°Âûã‰∏ªË¶Å‰æùËµñ‰∫éÂà§Âà´Â≠¶‰π†ÔºåÂØºËá¥ÂÖ∂Âú®ÁªÜËäÇÂå∫ÂàÜ‰∏äÂ≠òÂú®Â±ÄÈôêÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑun$^2$CLIPÈÄöËøáÂèçËΩ¨unCLIPÊ®°ÂûãÔºåÂà©Áî®ÁîüÊàêÊ®°ÂûãÁöÑ‰ºòÂäøÊù•ÊçïÊçâÊõ¥Â§öÁöÑËßÜËßâÁªÜËäÇ„ÄÇÁîüÊàêÊ®°ÂûãÂú®Â≠¶‰π†Êï∞ÊçÆÂàÜÂ∏ÉÊñπÈù¢Ë°®Áé∞Êõ¥‰Ω≥ÔºåÂõ†Ê≠§ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÂõæÂÉèÁöÑÁªÜËäÇ‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**Ôºöun$^2$CLIPÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™ÂèçËΩ¨ÁöÑunCLIPÊ®°ÂûãÔºåËØ•Ê®°ÂûãËÆ≠ÁªÉ‰∏Ä‰∏™Êù°‰ª∂‰∫éCLIPÂõæÂÉèÂµåÂÖ•ÁöÑÂõæÂÉèÁîüÊàêÂô®„ÄÇËØ•Ê°ÜÊû∂Á°Æ‰øùÁîüÊàêÁöÑÂõæÂÉèËÉΩÂ§ü‰∏éÂéüÂßãÁöÑÊñáÊú¨ÁºñÁ†ÅÂô®‰øùÊåÅ‰∏ÄËá¥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**Ôºöun$^2$CLIPÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÁîüÊàêÊ®°ÂûãÁöÑËÉΩÂäõÂºïÂÖ•Âà∞CLIP‰∏≠ÔºåÊèêÂçá‰∫ÜÂÖ∂Âú®ÁªÜËäÇÊçïÊçâ‰∏äÁöÑË°®Áé∞„ÄÇËøôÁßçÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂà§Âà´Ê®°ÂûãÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â≠¶‰π†ÂõæÂÉèÁöÑÁªÜËäÇÁâπÂæÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠Ôºåun$^2$CLIPÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•Á°Æ‰øùÁîüÊàêÂõæÂÉèÁöÑË¥®ÈáèÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äËøõË°å‰∫Ü‰ºòÂåñÔºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÂíåÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåun$^2$CLIPÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÂéüÂßãCLIPÊ®°ÂûãÂíå‰πãÂâçÁöÑÊîπËøõÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®MMVP-VLMÂü∫ÂáÜÊµãËØïÂíåÂºÄÊîæËØçÊ±áÂàÜÂâ≤‰ªªÂä°‰∏≠Ôºåun$^2$CLIPÁöÑÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞X%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÂæÖË°•ÂÖÖÔºâÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®ËßÜËßâÁªÜËäÇÊçïÊçâÊñπÈù¢ÁöÑÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠ÁöÑÂõæÂÉèÁêÜËß£„ÄÅÂõæÂÉèÁîüÊàê‰ª•ÂèäÂ§öÊ®°ÊÄÅ‰ªªÂä°Â§ÑÁêÜÁ≠â„ÄÇÈÄöËøáÊèêÂçáCLIPÊ®°ÂûãÁöÑÁªÜËäÇÊçïÊçâËÉΩÂäõÔºåun$^2$CLIPÂèØ‰ª•Âú®ÂõæÂÉèÂàÜÂâ≤„ÄÅÂõæÂÉèÊ£ÄÁ¥¢ÂíåÂ§öÊ®°ÊÄÅ‰∫§‰∫íÁ≠âÂÆûÈôÖÂ∫îÁî®‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un$^2$CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.

