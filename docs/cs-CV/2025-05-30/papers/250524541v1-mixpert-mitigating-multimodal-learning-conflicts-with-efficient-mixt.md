---
layout: default
title: "Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts"
---

# Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24541" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24541v1</a>
  <a href="https://arxiv.org/pdf/2505.24541.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24541v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24541v1', 'Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin He, Xumeng Han, Longhui Wei, Lingxi Xie, Qi Tian

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMixpertä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ å†²çªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰ç¼–ç å™¨` `åŠ¨æ€è·¯ç”±` `ä¸“å®¶ç³»ç»Ÿ` `ä»»åŠ¡ç‰¹å®šå¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–å•ä¸€è§†è§‰ç¼–ç å™¨å¤„ç†å¤šæ ·ä»»åŠ¡ï¼Œå¯¼è‡´æ€§èƒ½å†²çªå’Œä¼˜åŒ–å›°éš¾ã€‚
2. Mixperté€šè¿‡å¤šä¸“å®¶æ¶æ„å’ŒåŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œä¼˜åŒ–ä»»åŠ¡ç‰¹å®šçš„è§†è§‰ä¿¡æ¯å¤„ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMixpertåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¸”è®¡ç®—æ•ˆç‡é«˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰éœ€è¦å¯¹å¤æ‚å›¾åƒä¿¡æ¯è¿›è¡Œç»†è‡´çš„è§£è¯»ï¼Œé€šå¸¸ä¾èµ–è§†è§‰ç¼–ç å™¨æ¥æ„ŸçŸ¥å„ç§è§†è§‰åœºæ™¯ã€‚ç„¶è€Œï¼Œå•ä¸€è§†è§‰ç¼–ç å™¨åœ¨å¤„ç†å¤šæ ·ä»»åŠ¡é¢†åŸŸæ—¶é¢ä¸´å›°éš¾ï¼Œå®¹æ˜“å¯¼è‡´å†²çªã€‚è¿‘æœŸç ”ç©¶é€šè¿‡ç›´æ¥æ•´åˆå¤šä¸ªé¢†åŸŸç‰¹å®šçš„è§†è§‰ç¼–ç å™¨æ¥å¢å¼ºæ•°æ®æ„ŸçŸ¥ï¼Œä½†è¿™ç§ç»“æ„å¢åŠ äº†å¤æ‚æ€§å¹¶é™åˆ¶äº†è”åˆä¼˜åŒ–çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†Mixpertï¼Œä¸€ç§é«˜æ•ˆçš„è§†è§‰ä¸“å®¶æ··åˆæ¶æ„ï¼Œç»§æ‰¿äº†å•ä¸€è§†è§‰ç¼–ç å™¨çš„è”åˆå­¦ä¹ ä¼˜åŠ¿ï¼ŒåŒæ—¶é‡æ„ä¸ºå¤šä¸“å®¶èŒƒå¼ï¼Œä»¥ä¾¿åœ¨ä¸åŒè§†è§‰ä»»åŠ¡ä¸Šè¿›è¡Œä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œå°†è¾“å…¥å›¾åƒåˆ†é…ç»™æœ€åˆé€‚çš„è§†è§‰ä¸“å®¶ã€‚Mixpertæœ‰æ•ˆç¼“è§£äº†å•ä¸€è§†è§‰ç¼–ç å™¨åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­é‡åˆ°çš„é¢†åŸŸå†²çªï¼Œä¸”é¢å¤–è®¡ç®—æˆæœ¬æå°ï¼Œä½¿å…¶æ¯”å¤šä¸ªç¼–ç å™¨æ›´é«˜æ•ˆã€‚æ­¤å¤–ï¼ŒMixpertèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ä»»ä½•MLLMä¸­ï¼Œå®éªŒç»“æœæ˜¾ç¤ºåœ¨å„ç§ä»»åŠ¡ä¸Šå‡æœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­å•ä¸€è§†è§‰ç¼–ç å™¨åœ¨å¤„ç†å¤šæ ·ä»»åŠ¡æ—¶çš„å†²çªé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ä¸åŒé¢†åŸŸæ—¶ï¼Œéš¾ä»¥å®ç°æœ‰æ•ˆçš„è”åˆä¼˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMixpertçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¤šä¸“å®¶æ¶æ„ï¼Œç»“åˆåŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œä½¿å¾—æ¯ä¸ªè§†è§‰ä¸“å®¶èƒ½å¤Ÿä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œä»è€Œæé«˜ä»»åŠ¡ç‰¹å®šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMixpertçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªè§†è§‰ä¸“å®¶ï¼Œæ¯ä¸ªä¸“å®¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚è¾“å…¥å›¾åƒé€šè¿‡åŠ¨æ€è·¯ç”±æœºåˆ¶åˆ†é…ç»™æœ€åˆé€‚çš„ä¸“å®¶ï¼Œç¡®ä¿ä¿¡æ¯å¤„ç†çš„é«˜æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMixpertçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šä¸“å®¶æ¶æ„ä¸åŠ¨æ€è·¯ç”±æœºåˆ¶çš„ç»“åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£å•ä¸€ç¼–ç å™¨çš„é¢†åŸŸå†²çªé—®é¢˜ï¼Œä¸”åœ¨è®¡ç®—æˆæœ¬ä¸Šä¼˜äºä¼ ç»Ÿçš„å¤šç¼–ç å™¨æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒMixperté‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ä¸“å®¶çš„ä»»åŠ¡é€‚åº”æ€§ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå®ç°äº†ä¸“å®¶é—´çš„é«˜æ•ˆä¿¡æ¯å…±äº«ï¼Œç¡®ä¿äº†æ•´ä½“æ€§èƒ½çš„æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMixpertåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Mixpertçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è§†é¢‘åˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚å…¶é«˜æ•ˆçš„å¤šä¸“å®¶æ¶æ„èƒ½å¤Ÿä¸ºå¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡æä¾›æ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) require a nuanced interpretation of complex image information, typically leveraging a vision encoder to perceive various visual scenarios. However, relying solely on a single vision encoder to handle diverse task domains proves difficult and inevitably leads to conflicts. Recent work enhances data perception by directly integrating multiple domain-specific vision encoders, yet this structure adds complexity and limits the potential for joint optimization. In this paper, we introduce Mixpert, an efficient mixture-of-vision-experts architecture that inherits the joint learning advantages from a single vision encoder while being restructured into a multi-expert paradigm for task-specific fine-tuning across different visual tasks. Additionally, we design a dynamic routing mechanism that allocates input images to the most suitable visual expert. Mixpert effectively alleviates domain conflicts encountered by a single vision encoder in multi-task learning with minimal additional computational cost, making it more efficient than multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM, with experimental results demonstrating substantial performance gains across various tasks.

