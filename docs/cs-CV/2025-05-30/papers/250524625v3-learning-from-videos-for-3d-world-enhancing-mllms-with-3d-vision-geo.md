---
layout: default
title: Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors
---

# Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24625" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24625v3</a>
  <a href="https://arxiv.org/pdf/2505.24625.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24625v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24625v3', 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-10-22)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVG LLMä»¥è§£å†³è§†é¢‘ç›´æ¥ç†è§£3Dåœºæ™¯çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `3Dåœºæ™¯` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å‡ ä½•å…ˆéªŒ` `ç©ºé—´æ¨ç†` `è§†è§‰ç‰¹å¾æå–` `è‡ªåŠ¨é©¾é©¶` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„3Dæ•°æ®è¾“å…¥ï¼Œé™åˆ¶äº†ä»è§†é¢‘ç›´æ¥ç†è§£3Dåœºæ™¯çš„èƒ½åŠ›ã€‚
2. æˆ‘ä»¬æå‡ºçš„VG LLMé€šè¿‡3Dè§†è§‰å‡ ä½•ç¼–ç å™¨ä»è§†é¢‘åºåˆ—ä¸­æå–3Då…ˆéªŒä¿¡æ¯ï¼Œç›´æ¥ä¸è§†è§‰æ ‡è®°ç»“åˆè¾“å…¥MLLMã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVG LLMåœ¨å¤šé¡¹3Dåœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨VSI-Benchè¯„ä¼°ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£3Dåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯é€šè¿‡è§†é¢‘è¿›è¡Œç†è§£ã€‚ä»¥å¾€çš„æ–¹æ³•ä¾èµ–äºå…¨é¢çš„3Dæ•°æ®è¾“å…¥ï¼Œå¦‚ç‚¹äº‘æˆ–é‡å»ºçš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºè§†é¢‘-3Då‡ ä½•å¤§è¯­è¨€æ¨¡å‹ï¼ˆVG LLMï¼‰ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä»è§†é¢‘æ•°æ®ä¸­æå–3Då…ˆéªŒä¿¡æ¯ï¼Œè€Œæ— éœ€é¢å¤–çš„3Dè¾“å…¥ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯æˆ‘ä»¬çš„4Bæ¨¡å‹åœ¨VSI-Benchè¯„ä¼°ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒ…æ‹¬Gemini-1.5-Proã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£3Dåœºæ™¯æ—¶å¯¹3Dæ•°æ®è¾“å…¥çš„ä¾èµ–é—®é¢˜ã€‚ä»¥å¾€æ–¹æ³•éœ€è¦å¤æ‚çš„3Dæ•°æ®ï¼Œå¦‚ç‚¹äº‘æˆ–BEVï¼Œé™åˆ¶äº†å…¶åº”ç”¨çµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„VG LLMé€šè¿‡ç›´æ¥ä»è§†é¢‘æ•°æ®ä¸­æå–3Då‡ ä½•å…ˆéªŒä¿¡æ¯ï¼Œé¿å…äº†å¯¹é¢å¤–3Dè¾“å…¥çš„éœ€æ±‚ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç†è§£å’Œæ¨ç†3Dç©ºé—´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVG LLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè§†é¢‘åºåˆ—è¾“å…¥ã€3Dè§†è§‰å‡ ä½•ç¼–ç å™¨å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚è§†é¢‘åºåˆ—é¦–å…ˆè¢«å¤„ç†ä»¥æå–è§†è§‰ç‰¹å¾ï¼Œéšåé€šè¿‡å‡ ä½•ç¼–ç å™¨æå–3Dä¿¡æ¯ï¼Œæœ€åä¸è§†è§‰æ ‡è®°ç»“åˆè¾“å…¥åˆ°MLLMä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šVG LLMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿä»è§†é¢‘ä¸­ç›´æ¥å­¦ä¹ 3Dåœºæ™¯ç†è§£ï¼Œè€Œä¸ä¾èµ–äºä¼ ç»Ÿçš„3Dæ•°æ®è¾“å…¥ã€‚è¿™ä¸€æ–¹æ³•åœ¨æœ¬è´¨ä¸Šæ”¹å˜äº†3Dç†è§£çš„æ–¹å¼ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤„ç†è§†é¢‘æ—¶æ›´å…·çµæ´»æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–3Då‡ ä½•ä¿¡æ¯çš„æå–ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥ç¡®ä¿è§†è§‰ç‰¹å¾ä¸3Dä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒVG LLMçš„4Bæ¨¡å‹åœ¨VSI-Benchè¯„ä¼°ä¸­è¡¨ç°ä¼˜äºGemini-1.5-Proï¼Œæ˜¾ç¤ºå‡ºåœ¨3Dåœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šçš„æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶åœ¨ä¸ä¾èµ–æ˜¾å¼3Dæ•°æ®è¾“å…¥çš„æƒ…å†µä¸‹ä»èƒ½å–å¾—ç«äº‰æ€§ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿå¯¹å¤æ‚3Dç¯å¢ƒçš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡ç›´æ¥ä»è§†é¢‘ä¸­å­¦ä¹ 3Dä¿¡æ¯ï¼Œæœªæ¥çš„åº”ç”¨å°†æ›´åŠ çµæ´»å’Œé«˜æ•ˆï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method called the Video-3D Geometry Large Language Model (VG LLM). Our approach utilizes a 3D visual geometry encoder to extract 3D prior information from video sequences. This information is then integrated with visual tokens and input into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.

