---
layout: default
title: Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames
---

# Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24257" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24257v1</a>
  <a href="https://arxiv.org/pdf/2505.24257.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24257v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24257v1', 'Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sahithya Ravi, Gabriel Sarch, Vibhav Vineet, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDisjoint-3DQAåŸºå‡†ä»¥è§£å†³é•¿æ—¶é—´ç©ºé—´æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ—¶é—´ç©ºé—´æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `ç”Ÿæˆå¼é—®ç­”` `3Dåœºæ™¯è¡¨ç¤º` `å…·èº«AI`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶é—´è·¨åº¦çš„ç©ºé—´æ¨ç†æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç‰©ä½“ä¸åŒæ—¶å¸§ä¸å¯è§çš„æƒ…å†µä¸‹ã€‚
2. è®ºæ–‡æå‡ºDisjoint-3DQAåŸºå‡†ï¼Œé€šè¿‡ç”Ÿæˆå¼é—®ç­”çš„æ–¹å¼è¯„ä¼°VLMsåœ¨ä¸åŒå¸§ä¸­å¯¹ç‰©ä½“å…³ç³»çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVLMsçš„è¡¨ç°è½åäºäººç±»28%ï¼Œå¹¶ä¸”æä¾›3Dåæ ‡èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½20%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­ï¼Œå…·èº«AIåŠ©æ‰‹å¦‚ä½•æ•´åˆæ—¶é—´ä¸Šçš„ç©ºé—´çº¿ç´¢ï¼Œä»¥ç¡®å®šç‰©ä½“Aä¸ç‰©ä½“Bä¹‹é—´çš„ç›¸å¯¹ä½ç½®ã€‚æˆ‘ä»¬å¼•å…¥äº†Disjoint-3DQAï¼Œä¸€ä¸ªç”Ÿæˆå¼é—®ç­”åŸºå‡†ï¼Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¸åŒå¸§ä¸­å¯¹ä¸å¯è§ç‰©ä½“å¯¹çš„æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸ƒç§æœ€å…ˆè¿›çš„VLMsåœ¨æ€§èƒ½ä¸Šè½åäºäººç±»28%ï¼Œä¸”éšç€æ—¶é—´é—´éš”çš„å¢åŠ ï¼Œå‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚æä¾›è½¨è¿¹æˆ–é¸Ÿç°å›¾å¯¹æ¨¡å‹çš„æå‡æœ‰é™ï¼Œè€Œæä¾›çœŸå®çš„3Dåæ ‡åˆ™èƒ½æ˜¾è‘—æé«˜20%çš„æ€§èƒ½ã€‚è¿™ä¸€å‘ç°çªæ˜¾äº†å¤šå¸§VLMsåœ¨æ„å»ºå’Œç»´æŠ¤3Dåœºæ™¯è¡¨ç¤ºæ–¹é¢çš„æ ¸å¿ƒç“¶é¢ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å…·èº«AIåŠ©æ‰‹åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­å¦‚ä½•æœ‰æ•ˆæ•´åˆæ—¶é—´ä¸Šçš„ç©ºé—´çº¿ç´¢çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸å¯è§ç‰©ä½“å¯¹æ—¶ï¼Œå‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯æ—¶é—´é—´éš”è¾ƒå¤§æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥Disjoint-3DQAåŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°VLMsåœ¨ä¸åŒå¸§ä¸­å¯¹ç‰©ä½“å…³ç³»çš„æ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨æ¨åŠ¨VLMsåœ¨é•¿æ—¶é—´è·¨åº¦ç©ºé—´æ¨ç†æ–¹é¢çš„ç ”ç©¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€é—®é¢˜ç”Ÿæˆå’Œæ¨¡å‹è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†åŒ…å«ä¸åŒå¸§ä¸­ä¸å¯è§ç‰©ä½“å¯¹çš„é—®é¢˜ï¼Œæ¨¡å‹é€šè¿‡ç”Ÿæˆå¼é—®ç­”æ–¹å¼è¿›è¡Œæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†Disjoint-3DQAåŸºå‡†ï¼Œæ˜ç¡®äº†å¤šå¸§VLMsåœ¨ç©ºé—´æ¨ç†ä¸­çš„ä¸è¶³ï¼Œå¹¶æä¾›äº†å¯é‡åŒ–çš„è¯„ä¼°æ ‡å‡†ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥åŸºå‡†æ›´å…³æ³¨æ—¶é—´è·¨åº¦å¯¹æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œæ¨¡å‹çš„è¾“å…¥åŒ…æ‹¬ç‰©ä½“çš„è½¨è¿¹ã€é¸Ÿç°å›¾å’ŒçœŸå®çš„3Dåæ ‡ã€‚ç ”ç©¶å‘ç°ï¼Œæä¾›çœŸå®çš„3Dåæ ‡èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œè€Œå…¶ä»–è¾“å…¥æ–¹å¼çš„æå‡æ•ˆæœæœ‰é™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ƒç§æœ€å…ˆè¿›çš„VLMsåœ¨Disjoint-3DQAåŸºå‡†ä¸Šçš„è¡¨ç°è½åäºäººç±»28%ã€‚éšç€æ—¶é—´é—´éš”çš„å¢åŠ ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡ä»60%ä¸‹é™è‡³30%ã€‚æä¾›çœŸå®çš„3Dåæ ‡èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½20%ï¼Œè€Œè½¨è¿¹å’Œé¸Ÿç°å›¾çš„æå‡æ•ˆæœåˆ™ç›¸å¯¹æœ‰é™ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡æå‡VLMsåœ¨é•¿æ—¶é—´ç©ºé—´æ¨ç†ä¸­çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿä½¿å…·èº«AIåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°ç†è§£å’Œäº’åŠ¨ï¼Œè¿›è€Œæé«˜ç”¨æˆ·ä½“éªŒå’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œè¯¥ç ”ç©¶å¯èƒ½æ¨åŠ¨æ›´å¤šè·¨é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> An embodied AI assistant operating on egocentric video must integrate spatial cues across time - for instance, determining where an object A, glimpsed a few moments ago lies relative to an object B encountered later. We introduce Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs by posing questions about object pairs that are not co-visible in the same frame. We evaluated seven state-of-the-art VLMs and found that models lag behind human performance by 28%, with steeper declines in accuracy (60% to 30 %) as the temporal gap widens. Our analysis further reveals that providing trajectories or bird's-eye-view projections to VLMs results in only marginal improvements, whereas providing oracle 3D coordinates leads to a substantial 20% performance increase. This highlights a core bottleneck of multi-frame VLMs in constructing and maintaining 3D scene representations over time from visual signals. Disjoint-3DQA therefore sets a clear, measurable challenge for long-horizon spatial reasoning and aims to catalyze future research at the intersection of vision, language, and embodied AI.

