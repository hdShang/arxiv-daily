---
layout: default
title: Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction
---

# Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24156" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.24156v1</a>
  <a href="https://arxiv.org/pdf/2505.24156.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24156v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24156v1', 'Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Chenyou Fan, Fangzheng Yan, Chenjia Bai, Jiepeng Wang, Chi Zhang, Zhen Wang, Xuelong Li

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÊµÅÁöÑËßÜÈ¢ëÈ¢ÑÊµãÊñπÊ≥ï‰ª•Ëß£ÂÜ≥ÂèåÊâãÊìç‰ΩúÁ≠ñÁï•Ê≥õÂåñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂèåÊâãÊìç‰Ωú` `ËßÜÈ¢ëÈ¢ÑÊµã` `ÂÖâÊµÅÊ®°Âûã` `Á≠ñÁï•Â≠¶‰π†` `Êú∫Âô®‰∫∫ÊäÄÊúØ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÂèåÊâãÊìç‰ΩúÁ≠ñÁï•Â≠¶‰π†‰∏≠Èù¢‰∏¥Áü•ËØÜËøÅÁßªÂõ∞ÈöæÔºåÂ∞§ÂÖ∂ÊòØ‰ªéÂçïËáÇÊï∞ÊçÆÈõÜÂà∞ÂèåÊâãÊìç‰ΩúÁöÑÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøáÂæÆË∞ÉÊñáÊú¨Âà∞ËßÜÈ¢ëÊ®°ÂûãÔºåÁªìÂêàÂÖâÊµÅÈ¢ÑÊµãÂíåËßÜÈ¢ëÁîüÊàêÔºåÊù•ÂÆûÁé∞ÂèåÊâãÊìç‰ΩúÁ≠ñÁï•ÁöÑÂ≠¶‰π†„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®ÁúüÂÆûÂèåËáÇÊú∫Âô®‰∫∫‰∏äÊî∂ÈõÜÁöÑÊï∞ÊçÆ‰∏≠ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂØπÊú∫Âô®‰∫∫Êï∞ÊçÆÁöÑÈúÄÊ±ÇÔºåÂπ∂ÊèêÂçá‰∫ÜÁ≠ñÁï•ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â≠¶‰π†ÂèØÊ≥õÂåñÁöÑÂèåÊâãÊìç‰ΩúÁ≠ñÁï•ÂØπÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìËÄåË®ÄÊûÅÂÖ∑ÊåëÊàòÔºå‰∏ªË¶ÅÁî±‰∫éÂä®‰ΩúÁ©∫Èó¥Â∫ûÂ§ßÂèäÂçèË∞ÉËáÇÈÉ®ËøêÂä®ÁöÑÈúÄÊ±Ç„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÊù•Ëé∑ÂèñÂèåÊâãÁ≠ñÁï•Ôºå‰ΩÜ‰ªéÂçïËáÇÊï∞ÊçÆÈõÜÊàñÈ¢ÑËÆ≠ÁªÉVLAÊ®°ÂûãËøÅÁßªÁü•ËØÜÊó∂Â∏∏Êó†Ê≥ïÊúâÊïàÊ≥õÂåñÔºå‰∏ªË¶ÅÂéüÂõ†Âú®‰∫éÂèåÊâãÊï∞ÊçÆÁ®ÄÁº∫ÂèäÂçïËáÇ‰∏éÂèåÊâãÊìç‰Ωú‰πãÈó¥ÁöÑÊ†πÊú¨Â∑ÆÂºÇ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåÊâãÂü∫Á°ÄÁ≠ñÁï•ÔºåÈÄöËøáÂæÆË∞ÉÈ¢ÜÂÖàÁöÑÊñáÊú¨Âà∞ËßÜÈ¢ëÊ®°ÂûãÊù•È¢ÑÊµãÊú∫Âô®‰∫∫ËΩ®ËøπÔºåÂπ∂ËÆ≠ÁªÉËΩªÈáèÁ∫ßÊâ©Êï£Á≠ñÁï•ÁîüÊàêÂä®‰Ωú„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™‰∏§Èò∂ÊÆµÁöÑËåÉÂºèÔºåÂæÆË∞ÉÁã¨Á´ãÁöÑÊñáÊú¨Âà∞ÂÖâÊµÅÂíåÂÖâÊµÅÂà∞ËßÜÈ¢ëÊ®°ÂûãÔºåÂà©Áî®ÂÖâÊµÅ‰Ωú‰∏∫‰∏≠Èó¥ÂèòÈáèÔºåÊèê‰æõÂõæÂÉèÈó¥ÁªÜÂæÆËøêÂä®ÁöÑÁÆÄÊ¥ÅË°®Á§∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®‰ªøÁúüÂíåÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠ÂùáÂ±ïÁé∞‰∫ÜÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂèåÊâãÊìç‰ΩúÁ≠ñÁï•ÁöÑÊ≥õÂåñÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Áü•ËØÜËøÅÁßªÊó∂Èù¢‰∏¥ÂçïËáÇ‰∏éÂèåÊâãÊìç‰Ωú‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂØºËá¥Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÈÄöËøáÂæÆË∞ÉÊñáÊú¨Âà∞ËßÜÈ¢ëÊ®°ÂûãÔºåÂà©Áî®ÂÖâÊµÅ‰Ωú‰∏∫‰∏≠Èó¥ÂèòÈáèÔºåÊù•ÂÖ∑‰ΩìÂåñËØ≠Ë®ÄÊåá‰ª§ÁöÑÊÑèÂõæÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÂèåÊâãÊìç‰ΩúÁ≠ñÁï•Â≠¶‰π†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàÊòØÊñáÊú¨Âà∞ÂÖâÊµÅÊ®°ÂûãÁöÑÂæÆË∞ÉÔºåÂÖ∂Ê¨°ÊòØÂÖâÊµÅÂà∞ËßÜÈ¢ëÊ®°ÂûãÁöÑËÆ≠ÁªÉÔºåÂΩ¢Êàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÈ¢ÑÊµãÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂºïÂÖ•ÂÖâÊµÅ‰Ωú‰∏∫‰∏≠Èó¥Ë°®Á§∫ÔºåËß£ÂÜ≥‰∫ÜÂçïÈò∂ÊÆµÊñáÊú¨Âà∞ËßÜÈ¢ëÈ¢ÑÊµã‰∏≠ÁöÑËØ≠Ë®ÄÊ®°Á≥äÊÄßÈóÆÈ¢òÔºåÂêåÊó∂ÊòæËëóÂáèÂ∞ë‰∫ÜÂØπ‰ΩéÁ∫ßÂä®‰ΩúÁöÑÁõ¥Êé•‰ΩøÁî®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®ËΩªÈáèÁ∫ßÁöÑÊâ©Êï£Á≠ñÁï•ÁîüÊàêÂä®‰ΩúÔºå‰ºòÂåñ‰∫ÜÊçüÂ§±ÂáΩÊï∞‰ª•ÈÄÇÂ∫îÂèåÊâãÊìç‰ΩúÁöÑÁâπÊÄßÔºåÂπ∂Á°Æ‰øù‰∫ÜÊ®°ÂûãÁöÑÈ´òÊïàÊÄß‰∏éÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÊñπÊ≥ïÂú®ÂèåËáÇÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÁ≠ñÁï•ÁöÑÊúâÊïàÊÄßÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÊú∫Âô®‰∫∫Êï∞ÊçÆÈúÄÊ±ÇÂáèÂ∞ë‰∫Ü50%‰ª•‰∏äÔºåÂêåÊó∂Âú®‰ªøÁúüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÂùáË°®Áé∞Âá∫‰ºòË∂äÁöÑÊìç‰ΩúËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨‰∫∫Êú∫Âçè‰Ωú„ÄÅËá™Âä®ÂåñÂà∂ÈÄ†ÂíåÊúçÂä°Êú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊèêÂçáÂèåÊâãÊìç‰ΩúÁ≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõÔºåËÉΩÂ§ü‰ΩøÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠Êõ¥Â•ΩÂú∞ÊâßË°å‰ªªÂä°ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Learning a generalizable bimanual manipulation policy is extremely challenging for embodied agents due to the large action space and the need for coordinated arm movements. Existing approaches rely on Vision-Language-Action (VLA) models to acquire bimanual policies. However, transferring knowledge from single-arm datasets or pre-trained VLA models often fails to generalize effectively, primarily due to the scarcity of bimanual data and the fundamental differences between single-arm and bimanual manipulation. In this paper, we propose a novel bimanual foundation policy by fine-tuning the leading text-to-video models to predict robot trajectories and training a lightweight diffusion policy for action generation. Given the lack of embodied knowledge in text-to-video models, we introduce a two-stage paradigm that fine-tunes independent text-to-flow and flow-to-video models derived from a pre-trained text-to-video model. Specifically, optical flow serves as an intermediate variable, providing a concise representation of subtle movements between images. The text-to-flow model predicts optical flow to concretize the intent of language instructions, and the flow-to-video model leverages this flow for fine-grained video prediction. Our method mitigates the ambiguity of language in single-stage text-to-video prediction and significantly reduces the robot-data requirement by avoiding direct use of low-level actions. In experiments, we collect high-quality manipulation data for real dual-arm robot, and the results of simulation and real-world experiments demonstrate the effectiveness of our method.

