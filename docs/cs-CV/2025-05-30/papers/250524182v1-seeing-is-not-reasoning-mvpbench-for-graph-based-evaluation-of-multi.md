---
layout: default
title: Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT
---

# Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24182" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.24182v1</a>
  <a href="https://arxiv.org/pdf/2505.24182.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24182v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24182v1', 'Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhuobai Dong, Junchao Yi, Ziyuan Zheng, Haochen Han, Xiangxi Zheng, Alex Jinpeng Wang, Fangming Liu, Linjie Li

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-30

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MVPBench‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâÁâ©ÁêÜÊé®ÁêÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Áâ©ÁêÜÊé®ÁêÜ` `ÂõæÂü∫ËØÑ‰º∞` `Êé®ÁêÜÈìæ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜËßâÁâ©ÁêÜÊé®ÁêÜÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥ÔºåÊó†Ê≥ïÊúâÊïàÁêÜËß£Áâ©ÁêÜÊ≥ïÂàôÂíåÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇ
2. Êú¨ÊñáÊèêÂá∫MVPBenchÂü∫ÂáÜÔºåÈÄöËøáËßÜËßâÊé®ÁêÜÈìæËØÑ‰º∞Ê®°ÂûãÁöÑËßÜËßâÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõÔºåÂº∫Ë∞ÉËøûË¥ØÁöÑÈÄêÊ≠•Êé®ÁêÜËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®ËßÜËßâÊé®ÁêÜÂáÜÁ°ÆÊÄß‰∏äË°®Áé∞‰∏ç‰Ω≥Ôºå‰∏îÂêéËÆ≠ÁªÉÂØπÈΩêÊñπÊ≥ïÂèØËÉΩ‰ºöÊçüÂÆ≥Á©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁêÜËß£Áâ©ÁêÜ‰∏ñÁïåÁöÑËßÑÂæãÔºåÂ¶ÇËøêÂä®Ê≥ïÂàô„ÄÅÁ©∫Èó¥ÂÖ≥Á≥ªÂíåÂõ†ÊûúÂÖ≥Á≥ªÔºåÂØπÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÊûÑÊàê‰∫ÜÂü∫Êú¨ÊåëÊàò„ÄÇÂ∞ΩÁÆ°OpenAIÁöÑo3ÂíåGPT-4oÂú®ÊÑüÁü•ÂíåÊé®ÁêÜËÉΩÂäõ‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÁ†îÁ©∂Ë°®ÊòéËøô‰∫õÊ®°ÂûãÂú®ËßÜËßâÁâ©ÁêÜÊé®ÁêÜÊñπÈù¢Â≠òÂú®‰∏•Èáç‰∏çË∂≥ÔºåÊó†Ê≥ïÊéåÊè°Âü∫Êú¨ÁöÑÁâ©ÁêÜÊ≥ïÂàôÂíåÂ§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÁ©∫Èó¥‰∫§‰∫í„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜMVPBenchÔºå‰∏Ä‰∏™Êó®Âú®ÈÄöËøáËßÜËßâÊé®ÁêÜÈìæÔºàCoTÔºâ‰∏•Ê†ºËØÑ‰º∞ËßÜËßâÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõÁöÑÂü∫ÂáÜÔºåË¶ÅÊ±ÇÊ®°Âûã‰∏ç‰ªÖÊèê‰æõÊ≠£Á°ÆÁ≠îÊ°àÔºåËøòÈúÄÂ±ïÁ§∫Âü∫‰∫éËßÜËßâÁ∫øÁ¥¢ÁöÑËøûË¥ØÊé®ÁêÜË∑ØÂæÑ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑMLLMsÂú®Áâ©ÁêÜÈ¢ÜÂüüÁöÑËßÜËßâÊé®ÁêÜÂáÜÁ°ÆÊÄßÂíåÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÂ∫¶‰πüË°®Áé∞‰∏ç‰Ω≥„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜËßâÁâ©ÁêÜÊé®ÁêÜ‰∏≠ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÂØπÁâ©ÁêÜÊ≥ïÂàôÂíåÁ©∫Èó¥ÂÖ≥Á≥ªÁöÑÁêÜËß£„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñÊñáÊú¨ÂÖàÈ™åÔºåÂØºËá¥Ê®°ÂûãÂú®ËßÜËßâÁêÜËß£‰∏äË°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫MVPBenchÂü∫ÂáÜÔºåÂº∫Ë∞ÉÈÄöËøáËßÜËßâÊé®ÁêÜÈìæËøõË°åËØÑ‰º∞ÔºåË¶ÅÊ±ÇÊ®°Âûã‰∏ç‰ªÖÁªôÂá∫Ê≠£Á°ÆÁ≠îÊ°àÔºåËøòÈúÄÊèê‰æõÂü∫‰∫éËßÜËßâÁ∫øÁ¥¢ÁöÑËøûË¥ØÊé®ÁêÜË∑ØÂæÑ„ÄÇËøôÁßçËÆæËÆ°Ê®°Êãü‰∫Ü‰∫∫Á±ªÂú®Áé∞ÂÆûÁâ©ÁêÜËøáÁ®ã‰∏≠ÁöÑÊé®ÁêÜÊñπÂºè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMVPBenchÂåÖÂê´Â§ö‰∏™Ê®°ÂùóÔºåÈ¶ñÂÖàÊòØÂ§öÂõæÂÉèËæìÂÖ•ÁöÑÁ§∫‰æãÔºåÂÖ∂Ê¨°ÊòØË¶ÅÊ±ÇÊ®°ÂûãÊèê‰æõÊúÄÁªàÁ≠îÊ°àÂíåÊé®ÁêÜË∑ØÂæÑÔºåÊúÄÂêéÈÄöËøáÂõæÂü∫CoT‰∏ÄËá¥ÊÄßÂ∫¶ÈáèËØÑ‰º∞Êé®ÁêÜÁöÑÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂºïÂÖ•ÂõæÂü∫CoT‰∏ÄËá¥ÊÄßÂ∫¶ÈáèÔºåÈ™åËØÅÊ®°ÂûãÊé®ÁêÜË∑ØÂæÑÊòØÂê¶Á¨¶ÂêàÊúâÊïàÁöÑÁâ©ÁêÜÈÄªËæë„ÄÇËøô‰∏ÄÂàõÊñ∞‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÂº∫Ë∞ÉËßÜËßâÁêÜËß£ËÄåÈùûÊñáÊú¨ÂÖàÈ™å„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÁâπÂà´ÂÖ≥Ê≥®ÂáèÂ∞ëÊñáÊú¨ÂÖàÈ™åÁöÑÂΩ±ÂìçÔºåÈºìÂä±Ê®°Âûã‰æùËµñËßÜËßâ‰ø°ÊÅØËøõË°åÊé®ÁêÜ„ÄÇÂêåÊó∂ÔºåÂÆûÈ™å‰∏≠‰ΩøÁî®‰∫ÜÂ§öÊ†∑ÂåñÁöÑÁ§∫‰æãÔºå‰ª•Á°Æ‰øùËØÑ‰º∞ÁöÑÁªÜËá¥ÊÄßÂíåÂÖ®Èù¢ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜËßâÁâ©ÁêÜÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß‰∏ä‰ªçÁÑ∂ËæÉ‰ΩéÔºå‰∏îÂõæÂÉè‰∏éÊñáÊú¨ÁöÑÂØπÈΩêÂ∫¶‰∏çË∂≥„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂêéËÆ≠ÁªÉÂØπÈΩêÊñπÊ≥ïÂú®Á©∫Èó¥Êé®ÁêÜ‰∏äÂèçËÄåË°®Áé∞‰∏ç‰Ω≥ÔºåÊèêÁ§∫Êàë‰ª¨ÈúÄË¶ÅÈáçÊñ∞ÂÆ°ËßÜÂΩìÂâçÁöÑÂæÆË∞ÉÂÆûË∑µ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ËßÜËßâ„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÁõëÊéßÁ≠âÔºåËÉΩÂ§üÂ∏ÆÂä©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊé®ÁêÜÁâ©ÁêÜ‰∏ñÁïå‰∏≠ÁöÑÂ§çÊùÇÂú∫ÊôØ„ÄÇÊú™Êù•ÔºåMVPBenchÂèØËÉΩÊàê‰∏∫ËØÑ‰º∞ËßÜËßâÊé®ÁêÜËÉΩÂäõÁöÑÊ†áÂáÜÂ∑•ÂÖ∑ÔºåÊé®Âä®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Understanding the physical world - governed by laws of motion, spatial relations, and causality - poses a fundamental challenge for multimodal large language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o demonstrate impressive perceptual and reasoning capabilities, our investigation reveals these models struggle profoundly with visual physical reasoning, failing to grasp basic physical laws, spatial interactions, and causal effects in complex scenes. More importantly, they often fail to follow coherent reasoning chains grounded in visual evidence, especially when multiple steps are needed to arrive at the correct answer. To rigorously evaluate this capability, we introduce MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual chain-of-thought (CoT). Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. This setup mirrors how humans reason through real-world physical processes over time. To ensure fine-grained evaluation, we introduce a graph-based CoT consistency metric that verifies whether the reasoning path of model adheres to valid physical logic. Additionally, we minimize shortcut exploitation from text priors, encouraging models to rely on visual understanding. Experimental results reveal a concerning trend: even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. Surprisingly, RL-based post-training alignment - commonly believed to improve visual reasoning performance - often harms spatial reasoning, suggesting a need to rethink current fine-tuning practices.

