---
layout: default
title: MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM
---

# MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24238" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24238v2</a>
  <a href="https://arxiv.org/pdf/2505.24238.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24238v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24238v2', 'MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-02)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMIRAGEåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¹»è§‰è¯„ä¼°` `æ¨ç†èƒ½åŠ›` `è¯¾ç¨‹å¼ºåŒ–å¾®è°ƒ` `åä½œæç¤ºæ¨ç†` `æ¨¡å‹è®­ç»ƒ` `è¯„ä¼°æŒ‡æ ‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåŒºåˆ†å¤šæ¨¡æ€å¹»è§‰çš„æ¥æºï¼Œé™åˆ¶äº†å¯¹MLLMæ¨ç†å¤±è´¥çš„è¯Šæ–­ã€‚
2. æå‡ºMIRAGEåŸºå‡†ï¼Œé€šè¿‡æ„å»ºç‰¹å®šé—®é¢˜æ¥éš”ç¦»æ¨ç†å¹»è§‰ï¼Œå¹¶å¼•å…¥å¤šå±‚æ¬¡è¯„ä¼°æŒ‡æ ‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨é™ä½é€»è¾‘å¹»è§‰æ–¹é¢æ˜¾è‘—ä¼˜äºåŸå§‹æ¨¡å‹ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„å¹»è§‰ç°è±¡é™åˆ¶äº†å…¶æ­£ç¡®æ€§ï¼Œç°æœ‰åŸºå‡†æœªèƒ½æœ‰æ•ˆåŒºåˆ†æ„ŸçŸ¥å¼•èµ·çš„å¹»è§‰ä¸æ¨ç†å¼•èµ·çš„å¹»è§‰ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MIRAGEåŸºå‡†ï¼Œé€šè¿‡æ„å»ºè¾“å…¥å›¾åƒè¢«æ­£ç¡®æ„ŸçŸ¥ä½†æ¨ç†é”™è¯¯çš„é—®é¢˜ï¼Œæ¥éš”ç¦»æ¨ç†å¹»è§‰ã€‚MIRAGEå¼•å…¥äº†å¤šå±‚æ¬¡è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€äº‹å®æ€§å’Œå¹»è§‰è¯„åˆ†ã€‚åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹è§„æ¨¡ã€æ•°æ®è§„æ¨¡å’Œè®­ç»ƒé˜¶æ®µæ˜¾è‘—å½±å“å¹»è§‰çš„ç¨‹åº¦ï¼Œä¸”å½“å‰MLLMsåœ¨ç©ºé—´å¹»è§‰æ–¹é¢çš„æ”¹è¿›æœ‰é™ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ç»“åˆè¯¾ç¨‹å¼ºåŒ–å¾®è°ƒå’Œåä½œæç¤ºæ¨ç†çš„æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†é€»è¾‘å¹»è§‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ç°è±¡ï¼Œå°¤å…¶æ˜¯æ¨ç†å¼•èµ·çš„å¹»è§‰ã€‚ç°æœ‰åŸºå‡†æ— æ³•æœ‰æ•ˆåŒºåˆ†æ„ŸçŸ¥ä¸æ¨ç†å¼•èµ·çš„å¹»è§‰ï¼Œå¯¼è‡´å¯¹æ¨¡å‹æ€§èƒ½çš„è¯„ä¼°ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºç‰¹å®šé—®é¢˜ï¼Œç¡®ä¿è¾“å…¥å›¾åƒè¢«æ­£ç¡®æ„ŸçŸ¥ä½†æ¨ç†ä»ç„¶å‡ºé”™ï¼Œä»è€Œéš”ç¦»æ¨ç†å¹»è§‰ã€‚å¼•å…¥å¤šå±‚æ¬¡è¯„ä¼°æŒ‡æ ‡ï¼Œå¸®åŠ©é‡åŒ–å¹»è§‰ç°è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬MIRAGEåŸºå‡†çš„æ„å»ºã€è¯„ä¼°æŒ‡æ ‡çš„è®¾è®¡ï¼Œä»¥åŠç»“åˆè¯¾ç¨‹å¼ºåŒ–å¾®è°ƒå’Œåä½œæç¤ºæ¨ç†çš„è®­ç»ƒæ–¹æ³•ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†MIRAGEåŸºå‡†å’Œå¤šå±‚æ¬¡è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œé‡åŒ–ä¸åŒç±»å‹çš„å¹»è§‰ç°è±¡ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ç»†è‡´çš„åˆ†æå·¥å…·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨è¯¾ç¨‹å¼ºåŒ–å¾®è°ƒç­–ç•¥ï¼Œé€æ­¥é™ä½å­¦ä¹ éš¾åº¦ï¼Œå¹¶ç»“åˆåä½œæç¤ºæ¨ç†ä»¥ç®€åŒ–æ¨ç†è¿‡ç¨‹ã€‚å…·ä½“å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒè®ºæ–‡çš„å…·ä½“å†…å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨MIRAGEåŸºå‡†ä¸Šæ˜¾è‘—é™ä½äº†é€»è¾‘å¹»è§‰ï¼Œç›¸è¾ƒäºåŸå§‹æ¨¡å‹ï¼Œé€»è¾‘å¹»è§‰å‡å°‘å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€å‚è€ƒè®ºæ–‡ï¼‰ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†çš„æ”¹è¿›æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€ä»¥åŠå›¾åƒç†è§£ç­‰ã€‚é€šè¿‡æ”¹å–„æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æå‡äººæœºäº¤äº’çš„å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal hallucination in multimodal large language models (MLLMs) restricts the correctness of MLLMs. However, multimodal hallucinations are multi-sourced and arise from diverse causes. Existing benchmarks fail to adequately distinguish between perception-induced hallucinations and reasoning-induced hallucinations. This failure constitutes a significant issue and hinders the diagnosis of multimodal reasoning failures within MLLMs. To address this, we propose the {\dataset} benchmark, which isolates reasoning hallucinations by constructing questions where input images are correctly perceived by MLLMs yet reasoning errors persist. {\dataset} introduces multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination score for hallucination quantification. Our analysis reveals that (1) the model scale, data scale, and training stages significantly affect the degree of logical, fabrication, and factual hallucinations; (2) current MLLMs show no effective improvement on spatial hallucinations caused by misinterpreted spatial relationships, indicating their limited visual reasoning capabilities; and (3) question types correlate with distinct hallucination patterns, highlighting targeted challenges and potential mitigation strategies. To address these challenges, we propose {\method}, a method that combines curriculum reinforcement fine-tuning to encourage models to generate logic-consistent reasoning chains by stepwise reducing learning difficulty, and collaborative hint inference to reduce reasoning complexity. {\method} establishes a baseline on {\dataset}, and reduces the logical hallucinations in original base models.

