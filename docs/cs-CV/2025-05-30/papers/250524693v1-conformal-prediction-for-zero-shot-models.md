---
layout: default
title: Conformal Prediction for Zero-Shot Models
---

# Conformal Prediction for Zero-Shot Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24693" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24693v1</a>
  <a href="https://arxiv.org/pdf/2505.24693.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24693v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24693v1', 'Conformal Prediction for Zero-Shot Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Julio Silva-RodrÃ­guez, Ismail Ben Ayed, Jose Dolz

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: CVPR 2025. Code: https://github.com/jusiro/CLIP-Conformal

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºConf-OTä»¥è§£å†³é›¶æ ·æœ¬æ¨¡å‹çš„ä¸ç¡®å®šæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é›¶æ ·æœ¬å­¦ä¹ ` `ä¿å½¢é¢„æµ‹` `æœ€ä¼˜ä¼ è¾“` `è§†è§‰-è¯­è¨€æ¨¡å‹` `åŸŸé€‚åº”` `ä¸ç¡®å®šæ€§è¯„ä¼°` `è®¡ç®—æœºè§†è§‰` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é›¶æ ·æœ¬æ¨¡å‹æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆè§£å†³å…¶ä¸ç¡®å®šæ€§å’Œå¯é æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨åŸŸæ¼‚ç§»çš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†Conf-OTï¼Œé€šè¿‡åœ¨æ ¡å‡†å’ŒæŸ¥è¯¢é›†ä¸Šè¿›è¡Œä¼ å¯¼å­¦ä¹ ï¼Œè§£å†³äº†é¢„è®­ç»ƒä¸é€‚åº”ä¹‹é—´çš„åŸŸå·®è·ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒConf-OTåœ¨15ä¸ªæ•°æ®é›†ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œé›†æ•ˆç‡æå‡é«˜è¾¾20%ï¼Œä¸”é€Ÿåº¦å¿«15å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶å¯é æ€§å’Œä¸ç¡®å®šæ€§ä»ç„¶è¢«å¿½è§†ã€‚æœ¬æ–‡æ¢è®¨äº†CLIPæ¨¡å‹åœ¨åˆ†è£‚ä¿å½¢é¢„æµ‹èŒƒå¼ä¸‹çš„èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•åŸºäºå°è§„æ¨¡æ ‡è®°çš„æ ¡å‡†é›†ä¸ºé»‘ç®±æ¨¡å‹æä¾›ç†è®ºä¿è¯ã€‚ä¸ç°æœ‰æ–‡çŒ®ä¸­çš„ä¿å½¢é¢„æµ‹å™¨ä¸åŒï¼ŒåŸºç¡€æ¨¡å‹åœ¨ä¸€ä¸ªä¸å¯è®¿é—®çš„æºåŸŸä¸Šè¿›è¡Œä¸€æ¬¡æ€§é¢„è®­ç»ƒï¼Œè¿™ç§åŸŸæ¼‚ç§»å¯¹ä¿å½¢é›†çš„æ•ˆç‡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†Conf-OTï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ ¡å‡†å’ŒæŸ¥è¯¢é›†ä¸Šè¿›è¡Œä¼ å¯¼å­¦ä¹ çš„è®¾ç½®ï¼Œè§£å†³äº†æœ€ä¼˜ä¼ è¾“é—®é¢˜ï¼Œå¼¥åˆäº†é¢„è®­ç»ƒä¸é€‚åº”ä¹‹é—´çš„åŸŸå·®è·ï¼ŒåŒæ—¶ä¿æŒè¦†ç›–ä¿è¯ã€‚æˆ‘ä»¬åœ¨15ä¸ªæ•°æ®é›†å’Œä¸‰ç§éä¿å½¢è¯„åˆ†ä¸Šå…¨é¢æ¢ç´¢äº†è¿™ä¸€ç­–ç•¥ï¼ŒConf-OTåœ¨é›†æ•ˆç‡ä¸Šæä¾›äº†é«˜è¾¾20%çš„ç›¸å¯¹æå‡ï¼ŒåŒæ—¶æ¯”æµè¡Œçš„ä¼ å¯¼æ–¹æ³•å¿«15å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é›¶æ ·æœ¬æ¨¡å‹åœ¨åŸŸæ¼‚ç§»æƒ…å†µä¸‹çš„ä¸ç¡®å®šæ€§å’Œå¯é æ€§é—®é¢˜ã€‚ç°æœ‰çš„ä¿å½¢é¢„æµ‹æ–¹æ³•åœ¨å¤„ç†é¢„è®­ç»ƒæ¨¡å‹æ—¶æ•ˆç‡ä½ä¸‹ï¼Œä¸”æœªèƒ½å……åˆ†è€ƒè™‘æºåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„Conf-OTæ–¹æ³•é€šè¿‡æœ€ä¼˜ä¼ è¾“æŠ€æœ¯åœ¨æ ¡å‡†é›†å’ŒæŸ¥è¯¢é›†ä¹‹é—´è¿›è¡Œä¼ å¯¼å­¦ä¹ ï¼Œæ—¨åœ¨å¼¥åˆé¢„è®­ç»ƒä¸é€‚åº”é˜¶æ®µçš„åŸŸå·®è·ï¼Œä»è€Œæé«˜ä¿å½¢é›†çš„æ•ˆç‡å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æœ€ä¼˜ä¼ è¾“è®¡ç®—å’Œä¿å½¢é›†ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ ¡å‡†é›†å’ŒæŸ¥è¯¢é›†çš„ç»“åˆï¼Œè¿›è¡Œæ•°æ®çš„æ•´åˆä¸å¤„ç†ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨æœ€ä¼˜ä¼ è¾“ç®—æ³•è®¡ç®—æºåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„æ˜ å°„ï¼›æœ€åï¼Œç”Ÿæˆé«˜æ•ˆçš„ä¿å½¢é›†ä»¥è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šConf-OTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åœ¨ä¸éœ€è¦é¢å¤–æ•°æ®åˆ’åˆ†çš„æƒ…å†µä¸‹ï¼Œä»èƒ½ä¿æŒè¦†ç›–ä¿è¯ï¼Œå¹¶æœ‰æ•ˆè§£å†³äº†åŸŸæ¼‚ç§»å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ä¿å½¢é¢„æµ‹å™¨ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒConf-OTé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æœ€ä¼˜ä¼ è¾“è¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒæ•°æ®é›†çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒConf-OTåœ¨15ä¸ªä¸åŒæ•°æ®é›†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œé›†æ•ˆç‡æå‡é«˜è¾¾20%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„è®¡ç®—é€Ÿåº¦æ¯”æµè¡Œçš„ä¼ å¯¼æ–¹æ³•å¿«15å€ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ä»¥åŠè·¨æ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æé«˜é›¶æ ·æœ¬æ¨¡å‹çš„å¯é æ€§å’Œä¸ç¡®å®šæ€§è¯„ä¼°èƒ½åŠ›ï¼ŒConf-OTå¯åœ¨åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ç­‰é«˜é£é™©é¢†åŸŸä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å®‰å…¨çš„AIç³»ç»Ÿçš„å¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models pre-trained at large scale have shown unprecedented adaptability and generalization to downstream tasks. Although its discriminative potential has been widely explored, its reliability and uncertainty are still overlooked. In this work, we investigate the capabilities of CLIP models under the split conformal prediction paradigm, which provides theoretical guarantees to black-box models based on a small, labeled calibration set. In contrast to the main body of literature on conformal predictors in vision classifiers, foundation models exhibit a particular characteristic: they are pre-trained on a one-time basis on an inaccessible source domain, different from the transferred task. This domain drift negatively affects the efficiency of the conformal sets and poses additional challenges. To alleviate this issue, we propose Conf-OT, a transfer learning setting that operates transductive over the combined calibration and query sets. Solving an optimal transport problem, the proposed method bridges the domain gap between pre-training and adaptation without requiring additional data splits but still maintaining coverage guarantees. We comprehensively explore this conformal prediction strategy on a broad span of 15 datasets and three non-conformity scores. Conf-OT provides consistent relative improvements of up to 20% on set efficiency while being 15 times faster than popular transductive approaches.

