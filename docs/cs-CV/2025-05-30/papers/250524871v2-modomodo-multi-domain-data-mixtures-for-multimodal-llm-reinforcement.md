---
layout: default
title: "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning"
---

# MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24871" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24871v2</a>
  <a href="https://arxiv.org/pdf/2505.24871.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24871v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24871v2', 'MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-05)

**å¤‡æ³¨**: Project Webpage: https://modomodo-rl.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šåŸŸæ•°æ®æ··åˆç­–ç•¥ä»¥æå‡å¤šæ¨¡æ€LLMçš„å¼ºåŒ–å­¦ä¹ èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®æ··åˆ` `å¯éªŒè¯å¥–åŠ±` `æ¨¡å‹è®­ç»ƒ` `è§†è§‰-è¯­è¨€ä»»åŠ¡` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€LLMsçš„è®­ç»ƒä¸­é¢ä¸´æ•°æ®é›†é—´ç›®æ ‡å†²çªçš„é—®é¢˜ï¼Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€RLVRæ¡†æ¶ï¼Œç»“åˆå¤šæ•°æ®é›†åè®­ç»ƒå’Œæ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æœ€ä½³æ•°æ®æ··åˆç­–ç•¥åï¼Œæ¨¡å‹åœ¨åˆ†å¸ƒå¤–åŸºå‡†ä¸Šçš„å‡†ç¡®ç‡å¹³å‡æå‡5.24%ï¼Œç›¸è¾ƒäºå‡åŒ€æ•°æ®æ··åˆæå‡20.74%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æœ€è¿‘æˆä¸ºåè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§èŒƒå¼ï¼Œåœ¨å…·æœ‰ç»“æ„åŒ–ã€å¯éªŒè¯ç­”æ¡ˆçš„ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°†RLVRåº”ç”¨äºå¤šæ¨¡æ€LLMsï¼ˆMLLMsï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè§†è§‰-è¯­è¨€ä»»åŠ¡çš„å¼‚è´¨æ€§è¦æ±‚ç»†è‡´çš„è§†è§‰ã€é€»è¾‘å’Œç©ºé—´èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„åè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«ä¸¥æ ¼çš„æ•°æ®æ··åˆé—®é¢˜å®šä¹‰å’ŒåŸºå‡†å®ç°ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡æ€RLVRæ¡†æ¶ï¼Œé€šè¿‡ç­–åˆ’åŒ…å«ä¸åŒå¯éªŒè¯è§†è§‰-è¯­è¨€é—®é¢˜çš„æ•°æ®é›†ï¼Œæ”¯æŒå¤šåŸŸåœ¨çº¿RLå­¦ä¹ ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ•°æ®æ··åˆç­–ç•¥ï¼Œé€šè¿‡é¢„æµ‹RLå¾®è°ƒç»“æœæ¥ä¼˜åŒ–æœ€ä½³æ··åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆæ··åˆé¢„æµ‹ç­–ç•¥çš„å¤šåŸŸRLVRè®­ç»ƒæ˜¾è‘—æå‡äº†MLLMçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æœ€ä½³æ··åˆåœ¨åˆ†å¸ƒå¤–åŸºå‡†ä¸Šçš„å‡†ç¡®ç‡æå‡äº†5.24%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€LLMsåœ¨ä½¿ç”¨RLVRè¿›è¡Œåè®­ç»ƒæ—¶ï¼Œç”±äºæ•°æ®é›†é—´ç›®æ ‡å†²çªè€Œå¯¼è‡´çš„æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–æ•°æ®é›†çš„å¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„åè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆå¤šæ•°æ®é›†çš„RLVRè®­ç»ƒå’Œæ•°æ®æ··åˆç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–æ•°æ®æ··åˆæ¥æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†ç­–åˆ’ã€RLVRè®­ç»ƒæ¨¡å—å’Œæ•°æ®æ··åˆç­–ç•¥æ¨¡å—ã€‚é¦–å…ˆç­–åˆ’åŒ…å«å¤šç§å¯éªŒè¯è§†è§‰-è¯­è¨€é—®é¢˜çš„æ•°æ®é›†ï¼Œç„¶åè¿›è¡Œå¤šåŸŸåœ¨çº¿RLå­¦ä¹ ï¼Œæœ€åé€šè¿‡æ··åˆç­–ç•¥ä¼˜åŒ–è®­ç»ƒæ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ•°æ®æ··åˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿé¢„æµ‹RLå¾®è°ƒç»“æœå¹¶ä¼˜åŒ–æœ€ä½³æ•°æ®æ··åˆï¼Œä»è€Œæœ‰æ•ˆæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç­–ç•¥æ›´å…·é’ˆå¯¹æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç‡å’Œå¤šæ ·åŒ–çš„å¥–åŠ±æœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šç»“åˆäº†å¯éªŒè¯å¥–åŠ±å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œä»¥é€‚åº”å¤šåŸŸæ•°æ®çš„ç‰¹ç‚¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æœ€ä½³æ•°æ®æ··åˆç­–ç•¥çš„æ¨¡å‹åœ¨åˆ†å¸ƒå¤–åŸºå‡†ä¸Šçš„å‡†ç¡®ç‡å¹³å‡æå‡5.24%ï¼Œç›¸æ¯”äºå‡åŒ€æ•°æ®æ··åˆçš„æ¨¡å‹æå‡äº†20.74%ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†å¤šåŸŸRLVRè®­ç»ƒä¸æ··åˆé¢„æµ‹ç­–ç•¥ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€å›¾åƒæè¿°ç”Ÿæˆã€è·¨æ¨¡æ€æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline.

