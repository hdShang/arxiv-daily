---
layout: default
title: "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs"
---

# CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24120" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24120v2</a>
  <a href="https://arxiv.org/pdf/2505.24120.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24120v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24120v2', 'CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ai Jian, Weijie Qiu, Xiaokun Wang, Peiyu Wang, Yunzhuo Hao, Jiangbo Pei, Yichen Wei, Yi Peng, Xuchen Song

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-17)

**å¤‡æ³¨**: 36 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/Skywork)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCSVQAä»¥è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç§‘å­¦æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç§‘å­¦æ¨ç†` `å¤šæ¨¡æ€åŸºå‡†` `é¢†åŸŸçŸ¥è¯†` `è§†è§‰é—®ç­”` `STEMæ•™è‚²` `æ¨ç†èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€åŸºå‡†ä¸»è¦é›†ä¸­äºé€šç”¨å›¾åƒç†è§£å’Œæ–‡æœ¬æ¨ç†ï¼Œç¼ºä¹ç§‘å­¦é¢†åŸŸçš„çœŸå®èƒŒæ™¯è¯„ä¼°ã€‚
2. æœ¬æ–‡æå‡ºCSVQAåŸºå‡†ï¼Œä¸“æ³¨äºé€šè¿‡é¢†åŸŸåŸºç¡€çš„è§†è§‰é—®ç­”è¯„ä¼°ç§‘å­¦æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«1,378ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹ã€‚
3. å¯¹15ä¸ªVLMsçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæœ€é«˜æ¨¡å‹ä»…æœ‰49.6%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜ç§‘å­¦æ¨ç†èƒ½åŠ›äºŸå¾…æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ç§‘å­¦æ¨ç†èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚ç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†ä¸»è¦è¯„ä¼°é€šç”¨å›¾åƒç†è§£æˆ–æ–‡æœ¬é©±åŠ¨çš„æ¨ç†ï¼Œç¼ºä¹çœŸå®çš„ç§‘å­¦èƒŒæ™¯ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†ä¸è§†è§‰è¯æ®åˆ†æã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†CSVQAï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºé€šè¿‡é¢†åŸŸåŸºç¡€çš„è§†è§‰é—®ç­”è¯„ä¼°ç§‘å­¦æ¨ç†çš„è¯Šæ–­æ€§å¤šæ¨¡æ€åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«1,378å¯¹ç²¾å¿ƒæ„å»ºçš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–å¤šä¸ªSTEMå­¦ç§‘ï¼Œè¦æ±‚é¢†åŸŸçŸ¥è¯†ã€è§†è§‰è¯æ®æ•´åˆå’Œé«˜é˜¶æ¨ç†ã€‚ä¸ä»¥å¾€çš„å¤šæ¨¡æ€åŸºå‡†ç›¸æ¯”ï¼ŒCSVQAæ›´å¼ºè°ƒçœŸå®çš„ç§‘å­¦å†…å®¹å’Œå¤æ‚æ¨ç†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸¥æ ¼çš„è¯„ä¼°åè®®ï¼Œä»¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°æ¨¡å‹é¢„æµ‹æ˜¯å¦åŸºäºæœ‰æ•ˆçš„ä¸­é—´æ¨ç†æ­¥éª¤ã€‚å¯¹15ä¸ªVLMsçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ï¼Œå³ä½¿æ˜¯æ’åæœ€é«˜çš„ä¸“æœ‰æ¨¡å‹ä¹Ÿä»…è¾¾åˆ°49.6%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€å®è¯ç»“æœå¼ºè°ƒäº†æå‡VLMsç§‘å­¦æ¨ç†èƒ½åŠ›çš„è¿«åˆ‡éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€åŸºå‡†åœ¨ç§‘å­¦æ¨ç†è¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹çœŸå®ç§‘å­¦èƒŒæ™¯å’Œé¢†åŸŸçŸ¥è¯†çš„æ•´åˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCSVQAåŸºå‡†é€šè¿‡æ„å»ºé¢†åŸŸåŸºç¡€çš„è§†è§‰é—®ç­”ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åœ¨å›ç­”é—®é¢˜æ—¶æ•´åˆè§†è§‰è¯æ®ä¸é¢†åŸŸçŸ¥è¯†ï¼Œä»è€Œæ›´å¥½åœ°è¯„ä¼°å…¶ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥åŸºå‡†åŒ…å«1,378ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œè¦†ç›–å¤šä¸ªSTEMå­¦ç§‘ã€‚è¯„ä¼°æµç¨‹åŒ…æ‹¬é—®é¢˜ç”Ÿæˆã€ç­”æ¡ˆéªŒè¯å’Œæ¨ç†æ­¥éª¤çš„å®¡æŸ¥ï¼Œç¡®ä¿æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹æ˜¯åˆç†çš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šCSVQAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä¸“æ³¨äºç§‘å­¦æ¨ç†çš„å¤šæ¨¡æ€è¯„ä¼°ï¼Œå¼ºè°ƒçœŸå®ä¸–ç•Œçš„ç§‘å­¦å†…å®¹å’Œå¤æ‚æ¨ç†ï¼Œä¸ç°æœ‰åŸºå‡†ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œé—®é¢˜å’Œç­”æ¡ˆå¯¹çš„æ„å»ºéµå¾ªä¸¥æ ¼çš„é¢†åŸŸçŸ¥è¯†æ ‡å‡†ï¼Œè¯„ä¼°åè®®è¦æ±‚æ¨¡å‹æä¾›æœ‰æ•ˆçš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œç¡®ä¿é¢„æµ‹çš„åˆç†æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¯¹15ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ä¸­ï¼Œæœ€é«˜æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º49.6%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚è¿™ä¸€ç»“æœå¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œè¡¨æ˜éœ€è¦è¿›ä¸€æ­¥æå‡å…¶èƒ½åŠ›ä»¥æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CSVQAåŸºå‡†çš„æå‡ºä¸ºç§‘å­¦æ¨ç†èƒ½åŠ›çš„è¯„ä¼°æä¾›äº†æ–°çš„å·¥å…·ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å®ƒå¯ä»¥ç”¨äºæ•™è‚²é¢†åŸŸï¼Œå¸®åŠ©å­¦ç”Ÿå’Œç ”ç©¶äººå‘˜ç†è§£ç§‘å­¦æ¨ç†è¿‡ç¨‹ï¼Œä¹Ÿå¯ä¸ºVLMsçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘æä¾›æŒ‡å¯¼ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯èƒ½ä¼šå½±å“ç§‘å­¦æ•™è‚²å’Œç ”ç©¶æ–¹æ³•çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remain inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering. Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning. We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6% accuracy. This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA

