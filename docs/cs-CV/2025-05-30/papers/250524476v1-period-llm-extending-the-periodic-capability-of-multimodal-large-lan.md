---
layout: default
title: "Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model"
---

# Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24476" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24476v1</a>
  <a href="https://arxiv.org/pdf/2505.24476.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24476v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24476v1', 'Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuting Zhang, Hao Lu, Qingyong Hu, Yin Wang, Kaishen Yuan, Xin Liu, Kaishun Wu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: Accepted by CVPR 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/keke-nice/Period-LLM)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPeriod-LLMä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å‘¨æœŸæ€§ä»»åŠ¡ä¸­çš„ä¸è¶³**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‘¨æœŸæ€§ä»»åŠ¡` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ—¶é—´å»ºæ¨¡` `æ¨ç†èƒ½åŠ›` `ä¼˜åŒ–ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å‘¨æœŸæ€§ä»»åŠ¡æ—¶å­˜åœ¨æ—¶é—´å»ºæ¨¡ä¸è¶³å’ŒçŸ­æœŸä¸é•¿æœŸå‘¨æœŸå†²çªçš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†Period-LLMï¼Œé€šè¿‡â€œä»æ˜“åˆ°éš¾çš„æ³›åŒ–â€ç­–ç•¥å’Œâ€œæŠµæŠ—é€»è¾‘é—å¿˜â€çš„ä¼˜åŒ–ç­–ç•¥æ¥å¢å¼ºæ¨¡å‹çš„å‘¨æœŸæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPeriod-LLMåœ¨å‘¨æœŸæ€§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‘¨æœŸæ€§æˆ–å‡†å‘¨æœŸæ€§ç°è±¡æ­ç¤ºäº†å¤šç§è‡ªç„¶è¿‡ç¨‹ä¸­çš„å†…åœ¨ç‰¹å¾ï¼Œå¦‚å¤©æ°”æ¨¡å¼ã€è¿åŠ¨è¡Œä¸ºã€äº¤é€šæµå’Œç”Ÿç‰©ä¿¡å·ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•æ‰å’Œç†è§£è¿™äº›å¤æ‚ç°è±¡æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å½“å‰çš„MLLMsåœ¨å¤„ç†å‘¨æœŸæ€§ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ä½“ç°åœ¨ç¼ºä¹æ—¶é—´å»ºæ¨¡å’ŒçŸ­æœŸä¸é•¿æœŸå‘¨æœŸä¹‹é—´çš„å†²çªã€‚æœ¬æ–‡æå‡ºäº†Period-LLMï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å‘¨æœŸæ€§ä»»åŠ¡æ€§èƒ½çš„æ¨¡å‹ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¤šéš¾åº¦åŸºå‡†ä»¥è¯„ä¼°å¤§æ¨¡å‹çš„è·¨æ¨¡æ€å‘¨æœŸèƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨â€œä»æ˜“åˆ°éš¾çš„æ³›åŒ–â€èŒƒå¼ï¼Œç¡®ä¿æ¨¡å‹é€æ­¥å»ºç«‹ç¨³å¥çš„å‘¨æœŸæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæå‡ºäº†â€œæŠµæŠ—é€»è¾‘é—å¿˜â€çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥ä¿æŒè¯­ä¹‰å¯¹é½è¿‡ç¨‹ä¸­çš„å‘¨æœŸæ¨ç†èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPeriod-LLMåœ¨å‘¨æœŸæ€§ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„MLLMsã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å‘¨æœŸæ€§ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹æœ‰æ•ˆçš„æ—¶é—´å»ºæ¨¡èƒ½åŠ›ä»¥åŠçŸ­æœŸä¸é•¿æœŸå‘¨æœŸä¹‹é—´çš„å†²çªé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºPeriod-LLMï¼Œé€šè¿‡â€œä»æ˜“åˆ°éš¾çš„æ³›åŒ–â€ç­–ç•¥é€æ­¥æå‡æ¨¡å‹çš„å‘¨æœŸæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å¼•å…¥â€œæŠµæŠ—é€»è¾‘é—å¿˜â€çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥ç¡®ä¿åœ¨è¯­ä¹‰å¯¹é½è¿‡ç¨‹ä¸­ä¿æŒå‘¨æœŸæ€§æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPeriod-LLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯ç®€å•çš„æ–‡æœ¬ä»»åŠ¡ï¼Œç„¶åé€æ­¥è¿‡æ¸¡åˆ°å¤æ‚çš„è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒéš¾åº¦ä¸‹çš„å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºâ€œä»æ˜“åˆ°éš¾çš„æ³›åŒ–â€èŒƒå¼å’Œâ€œæŠµæŠ—é€»è¾‘é—å¿˜â€çš„ä¼˜åŒ–ç­–ç•¥ï¼Œè¿™äº›è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å‘¨æœŸæ€§ä»»åŠ¡ï¼Œè€Œä¸æ˜¯ä»…ä¾èµ–äºä¼ ç»Ÿçš„æ—¶é—´åºåˆ—åˆ†ææ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡çŸ­æœŸå’Œé•¿æœŸå‘¨æœŸçš„å­¦ä¹ ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æ”¯æŒå¤šæ¨¡æ€è¾“å…¥çš„æœ‰æ•ˆå¤„ç†ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œå±‚æ¬¡ç»“æ„åœ¨å®éªŒä¸­ç»è¿‡è°ƒä¼˜ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPeriod-LLMåœ¨å‘¨æœŸæ€§ä»»åŠ¡ä¸Šç›¸è¾ƒäºç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†15%-20%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPeriod-LLMåœ¨å¤„ç†å¤æ‚çš„å‘¨æœŸæ€§ç°è±¡æ—¶å…·æœ‰æ›´å¼ºçš„èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ°”è±¡é¢„æµ‹ã€äº¤é€šæµé‡åˆ†æã€ç”Ÿç‰©ä¿¡å·å¤„ç†ç­‰å¤šä¸ªéœ€è¦å‘¨æœŸæ€§åˆ†æçš„åœºæ™¯ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å‘¨æœŸæ€§ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒPeriod-LLMèƒ½å¤Ÿä¸ºç›¸å…³é¢†åŸŸæä¾›æ›´ç²¾å‡†çš„é¢„æµ‹å’Œåˆ†æå·¥å…·ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Periodic or quasi-periodic phenomena reveal intrinsic characteristics in various natural processes, such as weather patterns, movement behaviors, traffic flows, and biological signals. Given that these phenomena span multiple modalities, the capabilities of Multimodal Large Language Models (MLLMs) offer promising potential to effectively capture and understand their complex nature. However, current MLLMs struggle with periodic tasks due to limitations in: 1) lack of temporal modelling and 2) conflict between short and long periods. This paper introduces Period-LLM, a multimodal large language model designed to enhance the performance of periodic tasks across various modalities, and constructs a benchmark of various difficulty for evaluating the cross-modal periodic capabilities of large models. Specially, We adopt an "Easy to Hard Generalization" paradigm, starting with relatively simple text-based tasks and progressing to more complex visual and multimodal tasks, ensuring that the model gradually builds robust periodic reasoning capabilities. Additionally, we propose a "Resisting Logical Oblivion" optimization strategy to maintain periodic reasoning abilities during semantic alignment. Extensive experiments demonstrate the superiority of the proposed Period-LLM over existing MLLMs in periodic tasks. The code is available at https://github.com/keke-nice/Period-LLM.

