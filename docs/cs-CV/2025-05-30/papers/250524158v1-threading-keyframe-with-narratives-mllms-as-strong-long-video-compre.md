---
layout: default
title: Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders
---

# Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24158" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24158v1</a>
  <a href="https://arxiv.org/pdf/2505.24158.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24158v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24158v1', 'Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bo Fang, Wenhao Wu, Qiangqiang Wu, Yuxin Song, Antoni B. Chan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNar-KFCæ¨¡å—ä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„å…³é”®å¸§é€‰æ‹©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å…³é”®å¸§é€‰æ‹©` `æ–‡æœ¬å™è¿°ç”Ÿæˆ` `è§†é¢‘æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é•¿è§†é¢‘ç†è§£ä¸­é¢ä¸´è§†é¢‘å¸§æ•°é‡åºå¤§ä¸è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™çš„çŸ›ç›¾ï¼Œå¯¼è‡´ä¿¡æ¯é€‰æ‹©ä¸å½“ã€‚
2. æœ¬æ–‡æå‡ºNar-KFCæ¨¡å—ï¼Œé€šè¿‡ä¼˜åŒ–å…³é”®å¸§é€‰æ‹©ä¸æ’å…¥æ–‡æœ¬å™è¿°ï¼Œæå‡é•¿è§†é¢‘çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNar-KFCåœ¨å¤šä¸ªé•¿è§†é¢‘åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†MLLMçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œé•¿è§†é¢‘ç†è§£ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯è§†é¢‘å¸§æ•°é‡åºå¤§ä¸è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™ä¹‹é—´çš„çŸ›ç›¾ã€‚ä¼ ç»Ÿçš„å‡åŒ€é‡‡æ ·æ–¹æ³•å¸¸å¯¼è‡´é€‰æ‹©æ— å…³å†…å®¹ï¼Œè€Œåœ¨æ•°åƒå¸§ä¸Šè¿›è¡Œåè®­ç»ƒåˆ™å¸¦æ¥å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNar-KFCçš„å…³é”®å¸§ä¸å™è¿°äº¤ç»‡æ¨¡å—ï¼Œæ—¨åœ¨æœ‰æ•ˆä¸”é«˜æ•ˆåœ°è¿›è¡Œé•¿è§†é¢‘æ„ŸçŸ¥ã€‚Nar-KFCåŒ…æ‹¬ä¸¤ä¸ªåä½œæ­¥éª¤ï¼šé¦–å…ˆï¼Œå°†å…³é”®å¸§é€‰æ‹©è¿‡ç¨‹å½¢å¼åŒ–ä¸ºæ•´æ•°äºŒæ¬¡è§„åˆ’é—®é¢˜ï¼Œä¼˜åŒ–æŸ¥è¯¢ç›¸å…³æ€§å’Œå¸§å¤šæ ·æ€§ï¼›å…¶æ¬¡ï¼Œé€šè¿‡æ’å…¥ä»éå…³é”®å¸§ç”Ÿæˆçš„æ–‡æœ¬å™è¿°ï¼Œç¼“è§£ç¨€ç–å…³é”®å¸§é‡‡æ ·é€ æˆçš„æ—¶é—´ä¸è¿ç»­æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNar-KFCæ˜¾è‘—æå‡äº†å¤šç§é•¿è§†é¢‘åŸºå‡†ä¸Šçš„MLLMæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿è§†é¢‘ç†è§£ä¸­å…³é”®å¸§é€‰æ‹©çš„æ•ˆç‡ä¸æ•ˆæœé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å› è§†é¢‘å¸§æ•°é‡åºå¤§è€Œå¯¼è‡´ä¿¡æ¯é€‰æ‹©ä¸å½“ï¼Œå½±å“ç†è§£æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Nar-KFCæ¨¡å—é€šè¿‡ä¼˜åŒ–å…³é”®å¸§é€‰æ‹©ä¸æ’å…¥æ–‡æœ¬å™è¿°ï¼Œå½¢æˆè¿è´¯çš„æ—¶é—´åºåˆ—è¡¨ç¤ºï¼Œä»è€Œæå‡é•¿è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNar-KFCæ¨¡å—ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œä½¿ç”¨æ•´æ•°äºŒæ¬¡è§„åˆ’ä¼˜åŒ–å…³é”®å¸§çš„é€‰æ‹©ï¼›å…¶æ¬¡ï¼Œé€šè¿‡ç°æˆçš„å›¾åƒæè¿°ç”Ÿæˆå™¨ç”Ÿæˆæ–‡æœ¬å™è¿°ï¼Œå¹¶å°†å…¶æ’å…¥å…³é”®å¸§ä¹‹é—´ï¼Œå½¢æˆå®Œæ•´çš„æ—¶é—´åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šNar-KFCçš„åˆ›æ–°åœ¨äºå°†å…³é”®å¸§é€‰æ‹©ä¸æ–‡æœ¬å™è¿°ç”Ÿæˆç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ—¶é—´å’Œå†…å®¹æ„ŸçŸ¥çš„å‹ç¼©ç­–ç•¥ï¼Œæ˜¾è‘—æ”¹å–„äº†ä¼ ç»Ÿæ–¹æ³•çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®å¸§é€‰æ‹©ä¸­ï¼Œé‡‡ç”¨å®šåˆ¶çš„è´ªå©ªæœç´¢ç­–ç•¥ä»¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼›åœ¨æ–‡æœ¬å™è¿°ç”Ÿæˆä¸­ï¼Œç¡®ä¿å™è¿°ä¸å…³é”®å¸§çš„æ—¶é—´é¡ºåºä¸€è‡´ï¼Œä»¥ä¿æŒä¿¡æ¯çš„è¿è´¯æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNar-KFCåœ¨å¤šä¸ªé•¿è§†é¢‘åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†MLLMçš„æ€§èƒ½ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„ç†è§£èƒ½åŠ›å’Œä¿¡æ¯é€‰æ‹©æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨è§†é¢‘æ‘˜è¦ç”Ÿæˆã€æ™ºèƒ½è§†é¢‘æ£€ç´¢ç­‰ã€‚é€šè¿‡æå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼ŒNar-KFCæ¨¡å—èƒ½å¤Ÿä¸ºå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿæä¾›æ›´å‡†ç¡®çš„ä¸Šä¸‹æ–‡ç†è§£ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Employing Multimodal Large Language Models (MLLMs) for long video understanding remains a challenging problem due to the dilemma between the substantial number of video frames (i.e., visual tokens) versus the limited context length of language models. Traditional uniform sampling often leads to selection of irrelevant content, while post-training MLLMs on thousands of frames imposes a substantial computational burden. In this paper, we propose threading keyframes with narratives (Nar-KFC), a plug-and-play module to facilitate effective and efficient long video perception. Nar-KFC generally involves two collaborative steps. First, we formulate the keyframe selection process as an integer quadratic programming problem, jointly optimizing query-relevance and frame-diversity. To avoid its computational complexity, a customized greedy search strategy is designed as an efficient alternative. Second, to mitigate the temporal discontinuity caused by sparse keyframe sampling, we further introduce interleaved textual narratives generated from non-keyframes using off-the-shelf captioners. These narratives are inserted between keyframes based on their true temporal order, forming a coherent and compact representation. Nar-KFC thus serves as a temporal- and content-aware compression strategy that complements visual and textual modalities. Experimental results on multiple long-video benchmarks demonstrate that Nar-KFC significantly improves the performance of popular MLLMs. Code will be made publicly available.

