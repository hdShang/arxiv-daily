---
layout: default
title: "From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models"
---

# From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24232" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24232v1</a>
  <a href="https://arxiv.org/pdf/2505.24232.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24232v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24232v1', 'From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haibo Jin, Peiyan Zhang, Peiran Wang, Man Luo, Haohan Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¡†æ¶ä»¥è§£å†³å¤§å‹åŸºç¡€æ¨¡å‹çš„å¹»è§‰ä¸è¶Šç‹±æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹åŸºç¡€æ¨¡å‹` `å¹»è§‰` `è¶Šç‹±æ”»å‡»` `ä¼˜åŒ–æ–¹æ³•` `å®‰å…¨æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤§å‹åŸºç¡€æ¨¡å‹åœ¨å¹»è§‰å’Œè¶Šç‹±æ”»å‡»æ–¹é¢å­˜åœ¨è„†å¼±æ€§ï¼Œç°æœ‰é˜²å¾¡æ–¹æ³•å¾€å¾€åªé’ˆå¯¹å…¶ä¸­ä¸€ç§ï¼Œå¿½è§†äº†å®ƒä»¬ä¹‹é—´çš„å…³è”ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼Œå°†è¶Šç‹±æ”»å‡»è§†ä¸ºä»¤ç‰Œçº§ä¼˜åŒ–ï¼Œå¹»è§‰è§†ä¸ºæ³¨æ„åŠ›çº§ä¼˜åŒ–ï¼Œæ­ç¤ºäº†å®ƒä»¬ä¹‹é—´çš„æ·±å±‚è”ç³»ã€‚
3. é€šè¿‡åœ¨å¤šä¸ªæ¨¡å‹ä¸Šçš„å®è¯ç ”ç©¶ï¼ŒéªŒè¯äº†ä¼˜åŒ–è¶‹åŠ¿å’Œæ¢¯åº¦ä¸€è‡´æ€§ï¼Œè¡¨æ˜é’ˆå¯¹ä¸€ç§è„†å¼±æ€§çš„ç¼“è§£æªæ–½ä¹Ÿèƒ½æœ‰æ•ˆæ”¹å–„å¦ä¸€ç§è„†å¼±æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆLFMï¼‰å®¹æ˜“å—åˆ°ä¸¤ç§ä¸åŒçš„è„†å¼±æ€§å½±å“ï¼šå¹»è§‰å’Œè¶Šç‹±æ”»å‡»ã€‚å°½ç®¡é€šå¸¸å•ç‹¬ç ”ç©¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é’ˆå¯¹ä¸€ç§è„†å¼±æ€§çš„é˜²å¾¡å¾€å¾€ä¼šå½±å“å¦ä¸€ç§ï¼Œæš—ç¤ºå®ƒä»¬ä¹‹é—´å­˜åœ¨æ›´æ·±å±‚çš„è”ç³»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼Œå°†è¶Šç‹±æ”»å‡»å»ºæ¨¡ä¸ºä»¤ç‰Œçº§ä¼˜åŒ–ï¼Œè€Œå°†å¹»è§‰å»ºæ¨¡ä¸ºæ³¨æ„åŠ›çº§ä¼˜åŒ–ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªå…³é”®å‘½é¢˜ï¼š1ï¼‰ç›¸ä¼¼æŸå¤±æ”¶æ•›æ€§â€”â€”å½“ä¼˜åŒ–ç‰¹å®šç›®æ ‡è¾“å‡ºæ—¶ï¼Œä¸¤ç§è„†å¼±æ€§çš„æŸå¤±å‡½æ•°æ”¶æ•›æ–¹å¼ç›¸ä¼¼ï¼›2ï¼‰æ³¨æ„åŠ›é‡åˆ†é…ä¸­çš„æ¢¯åº¦ä¸€è‡´æ€§â€”â€”ä¸¤è€…éƒ½è¡¨ç°å‡ºç”±å…±äº«æ³¨æ„åŠ›åŠ¨æ€é©±åŠ¨çš„ä¸€è‡´æ¢¯åº¦è¡Œä¸ºã€‚é€šè¿‡åœ¨LLaVA-1.5å’ŒMiniGPT-4ä¸Šçš„å®è¯éªŒè¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¼˜åŒ–è¶‹åŠ¿çš„ä¸€è‡´æ€§å’Œæ¢¯åº¦çš„å¯¹é½ã€‚åˆ©ç”¨è¿™ä¸€è”ç³»ï¼Œæˆ‘ä»¬è¯æ˜äº†å¹»è§‰çš„ç¼“è§£æŠ€æœ¯å¯ä»¥é™ä½è¶Šç‹±æˆåŠŸç‡ï¼Œåä¹‹äº¦ç„¶ã€‚æˆ‘ä»¬çš„å‘ç°æ­ç¤ºäº†LFMçš„å…±åŒå¤±æ•ˆæ¨¡å¼ï¼Œå¹¶å»ºè®®ç¨³å¥æ€§ç­–ç•¥åº”å…±åŒè§£å†³è¿™ä¸¤ç§è„†å¼±æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹åŸºç¡€æ¨¡å‹åœ¨å¹»è§‰å’Œè¶Šç‹±æ”»å‡»æ–¹é¢çš„è„†å¼±æ€§ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨å•ä¸€é—®é¢˜ï¼Œç¼ºä¹å¯¹ä¸¤è€…å…³ç³»çš„æ·±å…¥ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å»ºç«‹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼Œå°†è¶Šç‹±æ”»å‡»å’Œå¹»è§‰åˆ†åˆ«å»ºæ¨¡ä¸ºä»¤ç‰Œçº§å’Œæ³¨æ„åŠ›çº§çš„ä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œæ­ç¤ºå®ƒä»¬ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯é’ˆå¯¹è¶Šç‹±æ”»å‡»çš„ä»¤ç‰Œçº§ä¼˜åŒ–ï¼ŒäºŒæ˜¯é’ˆå¯¹å¹»è§‰çš„æ³¨æ„åŠ›çº§ä¼˜åŒ–ï¼ŒäºŒè€…é€šè¿‡å…±äº«çš„æ³¨æ„åŠ›åŠ¨æ€ç›¸äº’å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºäº†ç›¸ä¼¼æŸå¤±æ”¶æ•›æ€§å’Œæ³¨æ„åŠ›é‡åˆ†é…ä¸­çš„æ¢¯åº¦ä¸€è‡´æ€§ä¸¤ä¸ªå‘½é¢˜ï¼Œæ­ç¤ºäº†ä¸¤ç§è„†å¼±æ€§ä¹‹é—´çš„å…±æ€§ï¼Œæ¨åŠ¨äº†å¯¹LFMçš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ä½¿ç”¨äº†LLaVA-1.5å’ŒMiniGPT-4æ¨¡å‹ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ï¼Œä»¥éªŒè¯ç†è®ºæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚é€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œè§‚å¯Ÿåˆ°ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹å¹»è§‰çš„ç¼“è§£æŠ€æœ¯èƒ½å¤Ÿæœ‰æ•ˆé™ä½è¶Šç‹±æ”»å‡»çš„æˆåŠŸç‡ï¼Œåä¹‹äº¦ç„¶ã€‚åœ¨LLaVA-1.5å’ŒMiniGPT-4æ¨¡å‹ä¸Šï¼Œä¼˜åŒ–è¶‹åŠ¿å’Œæ¢¯åº¦ä¸€è‡´æ€§å¾—åˆ°äº†éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æ•°æ®æœªå…¬å¼€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€å¯¹è¯ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ï¼Œèƒ½å¤Ÿæé«˜å¤§å‹åŸºç¡€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚é€šè¿‡å…±åŒè§£å†³å¹»è§‰å’Œè¶Šç‹±æ”»å‡»é—®é¢˜ï¼Œæœªæ¥çš„æ¨¡å‹å°†æ›´å…·é²æ£’æ€§ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„åº”ç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large foundation models (LFMs) are susceptible to two distinct vulnerabilities: hallucinations and jailbreak attacks. While typically studied in isolation, we observe that defenses targeting one often affect the other, hinting at a deeper connection.
>   We propose a unified theoretical framework that models jailbreaks as token-level optimization and hallucinations as attention-level optimization. Within this framework, we establish two key propositions: (1) \textit{Similar Loss Convergence} - the loss functions for both vulnerabilities converge similarly when optimizing for target-specific outputs; and (2) \textit{Gradient Consistency in Attention Redistribution} - both exhibit consistent gradient behavior driven by shared attention dynamics.
>   We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4, showing consistent optimization trends and aligned gradients. Leveraging this connection, we demonstrate that mitigation techniques for hallucinations can reduce jailbreak success rates, and vice versa. Our findings reveal a shared failure mode in LFMs and suggest that robustness strategies should jointly address both vulnerabilities.

