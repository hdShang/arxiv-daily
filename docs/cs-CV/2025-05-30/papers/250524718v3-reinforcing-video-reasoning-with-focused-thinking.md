---
layout: default
title: Reinforcing Video Reasoning with Focused Thinking
---

# Reinforcing Video Reasoning with Focused Thinking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24718" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24718v3</a>
  <a href="https://arxiv.org/pdf/2505.24718.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24718v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24718v3', 'Reinforcing Video Reasoning with Focused Thinking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang, Tat-Seng Chua

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-08)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/longmalongma/TW-GRPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTW-GRPOä»¥è§£å†³è§†é¢‘æ¨ç†ä¸­çš„æ— æ•ˆé“¾æ¡å’Œå¥–åŠ±ç¨€ç–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€æ¨¡å‹` `ä¿¡æ¯åŠ æƒ` `å¥–åŠ±æœºåˆ¶` `æ•°æ®å¢å¼º` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†é¢‘æ¨ç†ä¸­äº§ç”Ÿå†—é•¿ä¸”æ— æ•ˆçš„æ¨ç†é“¾æ¡ï¼Œéš¾ä»¥èšç„¦äºé‡è¦çš„æ—¶ç©ºä¿¡æ¯ã€‚
2. æå‡ºTW-GRPOæ¡†æ¶ï¼Œé€šè¿‡ä»¤ç‰ŒåŠ æƒæœºåˆ¶å’Œå¤šé€‰é¢˜è®­ç»ƒï¼Œæå‡æ¨ç†çš„èšç„¦æ€§å’Œå¥–åŠ±çš„ç»†ç²’åº¦ã€‚
3. TW-GRPOåœ¨CLEVRERå’ŒMMVUåŸºå‡†ä¸Šåˆ†åˆ«å–å¾—äº†50.4%å’Œ65.8%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šä¸€æ˜¯æ¨ç†é“¾æ¡å¾€å¾€å†—é•¿ä¸”ç¼ºä¹èšç„¦ï¼Œé®è”½äº†é‡è¦çš„æ—¶ç©ºçº¿ç´¢ï¼›äºŒæ˜¯äºŒå…ƒå¥–åŠ±æœºåˆ¶æœªèƒ½è€ƒè™‘éƒ¨åˆ†æ­£ç¡®ç­”æ¡ˆï¼Œå¯¼è‡´å¥–åŠ±æ–¹å·®é«˜ã€å­¦ä¹ æ•ˆç‡ä½ã€‚æœ¬æ–‡æå‡ºäº†TW-GRPOï¼Œä¸€ä¸ªé€šè¿‡èšç„¦æ€ç»´å’Œå¯†é›†å¥–åŠ±ç²’åº¦å¢å¼ºè§†è§‰æ¨ç†çš„æ–°æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªä»¤ç‰ŒåŠ æƒæœºåˆ¶ï¼Œä¼˜å…ˆè€ƒè™‘ä¿¡æ¯å¯†åº¦é«˜çš„ä»¤ç‰Œï¼ŒæŠ‘åˆ¶å†—ä½™ä»¤ç‰Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»å•é€‰é¢˜è½¬å˜ä¸ºå¤šé€‰é¢˜ï¼Œåˆ©ç”¨è½¯å¥–åŠ±å®ç°æ›´ç»†ç²’åº¦çš„æ¢¯åº¦ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTW-GRPOåœ¨å¤šä¸ªè§†é¢‘æ¨ç†å’Œé€šç”¨ç†è§£åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘æ¨ç†æ–¹æ³•ä¸­æ¨ç†é“¾æ¡å†—é•¿ä¸”å¥–åŠ±æœºåˆ¶ç¨€ç–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆèšç„¦äºé‡è¦çš„æ—¶ç©ºçº¿ç´¢ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTW-GRPOæ¡†æ¶é€šè¿‡å¼•å…¥ä»¤ç‰ŒåŠ æƒæœºåˆ¶ï¼Œä¼˜å…ˆè€ƒè™‘ä¿¡æ¯å¯†åº¦é«˜çš„ä»¤ç‰Œï¼Œä»è€ŒæŠ‘åˆ¶å†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚åŒæ—¶ï¼Œå°†è®­ç»ƒä»»åŠ¡ä»å•é€‰é¢˜è½¬å˜ä¸ºå¤šé€‰é¢˜ï¼Œä»¥å®ç°æ›´ç»†è‡´çš„å¥–åŠ±åé¦ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTW-GRPOçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¤ç‰ŒåŠ æƒæ¨¡å—å’Œå¤šé€‰é¢˜è®­ç»ƒæ¨¡å—ã€‚ä»¤ç‰ŒåŠ æƒæ¨¡å—é€šè¿‡è®¡ç®—ä¿¡æ¯ç†µæ¥è¯„ä¼°ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œè€Œå¤šé€‰é¢˜è®­ç»ƒæ¨¡å—åˆ™é€šè¿‡è½¯å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä»¤ç‰ŒåŠ æƒæœºåˆ¶å’Œå¤šé€‰é¢˜è®­ç»ƒçš„ç»“åˆï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èšç„¦äºå…³é”®ä¿¡æ¯ï¼Œå¹¶åœ¨å¥–åŠ±åé¦ˆä¸Šå®ç°æ›´ç»†è‡´çš„åŒºåˆ†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†åŸºäºä¿¡æ¯ç†µçš„ä»¤ç‰ŒåŠ æƒç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå¼•å…¥äº†è½¯å¥–åŠ±æœºåˆ¶ï¼Œä»¥æ”¯æŒéƒ¨åˆ†æ­£ç¡®ç­”æ¡ˆçš„è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TW-GRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨CLEVRERä¸Šè¾¾åˆ°äº†50.4%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºVideo-R1æé«˜äº†18.8%ï¼›åœ¨MMVUä¸Šåˆ™è¾¾åˆ°äº†65.8%çš„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç†è§£ã€æ™ºèƒ½ç›‘æ§å’Œè‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡è§†é¢‘æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒTW-GRPOèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿæä¾›æ›´å¼ºçš„æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in reinforcement learning, particularly through Group Relative Policy Optimization (GRPO), have significantly improved multimodal large language models for complex reasoning tasks. However, two critical limitations persist: 1) they often produce unfocused, verbose reasoning chains that obscure salient spatiotemporal cues and 2) binary rewarding fails to account for partially correct answers, resulting in high reward variance and inefficient learning. In this paper, we propose TW-GRPO, a novel framework that enhances visual reasoning with focused thinking and dense reward granularity. Specifically, we employs a token weighting mechanism that prioritizes tokens with high informational density (estimated by intra-group information entropy), suppressing redundant tokens like generic reasoning prefixes. Furthermore, we reformulate RL training by shifting from single-choice to multi-choice QA tasks, where soft rewards enable finer-grained gradient estimation by distinguishing partial correctness. Additionally, we propose question-answer inversion, a data augmentation strategy to generate diverse multi-choice samples from existing benchmarks. Experiments demonstrate state-of-the-art performance on several video reasoning and general understanding benchmarks. Notably, TW-GRPO achieves 50.4\% accuracy on CLEVRER (18.8\% improvement over Video-R1) and 65.8\% on MMVU. Our codes are available at \href{https://github.com/longmalongma/TW-GRPO}.

