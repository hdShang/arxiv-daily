---
layout: default
title: Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors
---

# Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24103" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24103v1</a>
  <a href="https://arxiv.org/pdf/2505.24103.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24103v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24103v1', 'Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peiran Xu, Yadong Mu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: ICLR 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/woyut/WSAG-PLSP)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼±ç›‘ç£çš„å¯ä¾›æ€§å®šä½æ–¹æ³•ä»¥è§£å†³æ ‡ç­¾ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼±ç›‘ç£å­¦ä¹ ` `å¯ä¾›æ€§å®šä½` `ä¼ªæ ‡ç­¾` `éƒ¨ä»¶åˆ†å‰²` `ç‰¹å¾å¯¹é½` `äººæœºäº¤äº’` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¤šä¾èµ–ç±»æ¿€æ´»å›¾ï¼Œé€‚åˆè¯­ä¹‰åˆ†å‰²ä½†éš¾ä»¥å‡†ç¡®å®šä½åŠ¨ä½œå’ŒåŠŸèƒ½ï¼Œé¢ä¸´æ ‡ç­¾ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚
2. æœ¬ç ”ç©¶æå‡ºåŸºäºä¼ªæ ‡ç­¾çš„ç›‘ç£è®­ç»ƒæµç¨‹ï¼Œç»“åˆéƒ¨ä»¶åˆ†å‰²æ¨¡å‹ä¸å¯ä¾›æ€§æ˜ å°„ï¼Œæå‡å¯ä¾›æ€§å®šä½æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ä¸åˆ›æ–°æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶èšç„¦äºå¼±ç›‘ç£å¯ä¾›æ€§å®šä½ä»»åŠ¡ï¼Œæ—¨åœ¨åˆ©ç”¨äººæœºäº¤äº’å›¾åƒå’Œè‡ªæˆ‘ä¸­å¿ƒç‰©ä½“å›¾åƒè¯†åˆ«ç‰©ä½“çš„å¯ä¾›æ€§åŒºåŸŸï¼Œè€Œæ— éœ€å¯†é›†æ ‡ç­¾ã€‚ä»¥å¾€æ–¹æ³•ä¸»è¦åŸºäºç±»æ¿€æ´»å›¾ï¼Œé€‚ç”¨äºè¯­ä¹‰åˆ†å‰²ï¼Œä½†ä¸é€‚åˆå®šä½åŠ¨ä½œå’ŒåŠŸèƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºä¼ªæ ‡ç­¾çš„ç›‘ç£è®­ç»ƒæµç¨‹ï¼Œä¼ªæ ‡ç­¾ç”±ç°æˆçš„éƒ¨ä»¶åˆ†å‰²æ¨¡å‹ç”Ÿæˆï¼Œå¹¶é€šè¿‡å¯ä¾›æ€§ä¸éƒ¨ä»¶åç§°çš„æ˜ å°„è¿›è¡ŒæŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªå…³é”®å¢å¼ºæŠ€æœ¯ï¼šæ ‡ç­¾ç²¾ç‚¼é˜¶æ®µã€ç»†ç²’åº¦ç‰¹å¾å¯¹é½è¿‡ç¨‹å’Œè½»é‡æ¨ç†æ¨¡å—ã€‚è¿™äº›æŠ€æœ¯åˆ©ç”¨ç°æˆåŸºç¡€æ¨¡å‹ä¸­é™æ€ç‰©ä½“çš„è¯­ä¹‰çŸ¥è¯†ï¼Œæå‡å¯ä¾›æ€§å­¦ä¹ ï¼ŒæˆåŠŸå¼¥åˆç‰©ä½“ä¸åŠ¨ä½œä¹‹é—´çš„å·®è·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¼±ç›‘ç£å¯ä¾›æ€§å®šä½é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å®šä½åŠ¨ä½œå’ŒåŠŸèƒ½æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”ç¼ºä¹å¯†é›†æ ‡ç­¾æ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ©ç”¨ç°æˆçš„éƒ¨ä»¶åˆ†å‰²æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶ç»“åˆå¯ä¾›æ€§ä¸éƒ¨ä»¶åç§°çš„æ˜ å°„ï¼Œæ„å»ºäº†ä¸€ç§æ–°çš„ç›‘ç£è®­ç»ƒæµç¨‹ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å¯ä¾›æ€§åŒºåŸŸçš„è¯†åˆ«èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¼ªæ ‡ç­¾ç”Ÿæˆã€æ ‡ç­¾ç²¾ç‚¼ã€ç»†ç²’åº¦ç‰¹å¾å¯¹é½å’Œè½»é‡æ¨ç†æ¨¡å—ã€‚ä¼ªæ ‡ç­¾ç”Ÿæˆé˜¶æ®µåˆ©ç”¨éƒ¨ä»¶åˆ†å‰²æ¨¡å‹ï¼Œåç»­é€šè¿‡ç²¾ç‚¼å’Œå¯¹é½æå‡æ ‡ç­¾è´¨é‡å’Œç‰¹å¾ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥äº†æ ‡ç­¾ç²¾ç‚¼å’Œç»†ç²’åº¦ç‰¹å¾å¯¹é½ç­‰æ–°æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯ä¾›æ€§å­¦ä¹ èƒ½åŠ›ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿæ–¹æ³•çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ä¼ªæ ‡ç­¾çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡è½»é‡æ¨ç†æ¨¡å—å‡å°‘è®¡ç®—å¤æ‚åº¦ï¼Œç¡®ä¿æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†XX%ï¼Œæœ‰æ•ˆéªŒè¯äº†æ¨¡å‹çš„åˆ›æ–°æ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€æ™ºèƒ½å®¶å±…å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜å¯ä¾›æ€§å®šä½çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿå¢å¼ºæœºå™¨äººä¸ç¯å¢ƒçš„äº’åŠ¨èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„æ™ºèƒ½åŒ–å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we focus on the task of weakly supervised affordance grounding, where a model is trained to identify affordance regions on objects using human-object interaction images and egocentric object images without dense labels. Previous works are mostly built upon class activation maps, which are effective for semantic segmentation but may not be suitable for locating actions and functions. Leveraging recent advanced foundation models, we develop a supervised training pipeline based on pseudo labels. The pseudo labels are generated from an off-the-shelf part segmentation model, guided by a mapping from affordance to part names. Furthermore, we introduce three key enhancements to the baseline model: a label refining stage, a fine-grained feature alignment process, and a lightweight reasoning module. These techniques harness the semantic knowledge of static objects embedded in off-the-shelf foundation models to improve affordance learning, effectively bridging the gap between objects and actions. Extensive experiments demonstrate that the performance of the proposed model has achieved a breakthrough improvement over existing methods. Our codes are available at https://github.com/woyut/WSAG-PLSP .

