---
layout: default
title: Image Quality Assessment for Embodied AI
---

# Image Quality Assessment for Embodied AI

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.16815" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.16815v2</a>
  <a href="https://arxiv.org/pdf/2505.16815.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.16815v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.16815v2', 'Image Quality Assessment for Embodied AI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunyi Li, Jiaohao Xiao, Jianbo Zhang, Farong Wen, Zicheng Zhang, Yuan Tian, Xiangyang Zhu, Xiaohong Liu, Zhengxue Cheng, Weisi Lin, Guangtao Zhai

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-22 (æ›´æ–°: 2025-10-14)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/lcysyzxdxc/EmbodiedIQA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•ä»¥è§£å†³æœºå™¨äººæ„ŸçŸ¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒè´¨é‡è¯„ä¼°` `å…·èº«äººå·¥æ™ºèƒ½` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ•°æ®åº“æ„å»º` `æœºå™¨äººæ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•æ— æ³•æœ‰æ•ˆè¯„ä¼°å›¾åƒåœ¨å…·èº«AIä»»åŠ¡ä¸­çš„å¯ç”¨æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„IQAæ–¹æ³•ï¼Œæ„å»ºäº†æ„ŸçŸ¥-è®¤çŸ¥-å†³ç­–-æ‰§è¡Œçš„ç®¡é“ï¼Œå¹¶å»ºç«‹äº†Embodied-IQAæ•°æ®åº“ä»¥æ”¯æŒè¯„ä¼°ã€‚
3. é€šè¿‡å¯¹ä¸»æµIQAæ–¹æ³•çš„éªŒè¯ï¼Œå‘ç°ç°æœ‰æ–¹æ³•åœ¨å…·èº«AIåœºæ™¯ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œå¼ºè°ƒäº†å¼€å‘æ–°æŒ‡æ ‡çš„å¿…è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå…·èº«äººå·¥æ™ºèƒ½ï¼ˆEmbodied AIï¼‰è¿…é€Ÿå‘å±•ï¼Œä½†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨å—åˆ°å„ç§å¤±çœŸå› ç´ çš„é™åˆ¶ã€‚ä¼ ç»Ÿçš„å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ–¹æ³•ä¸»è¦ç”¨äºé¢„æµ‹äººç±»å¯¹å¤±çœŸå›¾åƒçš„åå¥½ï¼Œç„¶è€Œç¼ºä¹é’ˆå¯¹å…·èº«ä»»åŠ¡çš„å›¾åƒå¯ç”¨æ€§è¯„ä¼°æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºäº†å…·èº«AIçš„å›¾åƒè´¨é‡è¯„ä¼°ä¸»é¢˜ï¼Œæ„å»ºäº†æ„ŸçŸ¥-è®¤çŸ¥-å†³ç­–-æ‰§è¡Œçš„ç®¡é“ï¼Œå¹¶å®šä¹‰äº†å…¨é¢çš„ä¸»è§‚è¯„åˆ†æ”¶é›†è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œå»ºç«‹äº†åŒ…å«è¶…è¿‡36,000å¯¹å‚è€ƒ/å¤±çœŸå›¾åƒçš„Embodied-IQAæ•°æ®åº“ï¼Œå¹¶æä¾›äº†è¶…è¿‡500ä¸‡æ¡ç”±è§†è§‰è¯­è¨€æ¨¡å‹å’Œç°å®ä¸–ç•Œæœºå™¨äººç”Ÿæˆçš„ç»†ç²’åº¦æ³¨é‡Šã€‚é€šè¿‡å¯¹ä¸»æµIQAæ–¹æ³•åœ¨Embodied-IQAä¸Šçš„è®­ç»ƒå’ŒéªŒè¯ï¼Œå±•ç¤ºäº†å¼€å‘æ›´å‡†ç¡®çš„å…·èº«AIè´¨é‡æŒ‡æ ‡çš„å¿…è¦æ€§ã€‚å¸Œæœ›é€šè¿‡è¯„ä¼°ï¼Œä¿ƒè¿›å…·èº«AIåœ¨å¤æ‚å¤±çœŸç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•æ— æ³•æœ‰æ•ˆè¯„ä¼°å›¾åƒåœ¨å…·èº«AIä»»åŠ¡ä¸­çš„å¯ç”¨æ€§çš„é—®é¢˜ã€‚ä¼ ç»ŸIQAæ–¹æ³•ä¸»è¦å…³æ³¨äººç±»åå¥½ï¼Œè€Œå¿½è§†äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºMertonianç³»ç»Ÿå’Œå…ƒè®¤çŸ¥ç†è®ºçš„è¯„ä¼°æ¡†æ¶ï¼Œæ„å»ºäº†æ„ŸçŸ¥-è®¤çŸ¥-å†³ç­–-æ‰§è¡Œçš„ç®¡é“ï¼Œä»¥å…¨é¢è¯„ä¼°å›¾åƒçš„ä¸»è§‚è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ„ŸçŸ¥é˜¶æ®µï¼ˆå›¾åƒè·å–ï¼‰ã€è®¤çŸ¥é˜¶æ®µï¼ˆå›¾åƒåˆ†æï¼‰ã€å†³ç­–é˜¶æ®µï¼ˆè´¨é‡è¯„ä¼°ï¼‰å’Œæ‰§è¡Œé˜¶æ®µï¼ˆä»»åŠ¡æ‰§è¡Œï¼‰ï¼Œå¹¶é€šè¿‡Embodied-IQAæ•°æ®åº“æä¾›ä¸°å¯Œçš„å‚è€ƒæ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå»ºç«‹äº†Embodied-IQAæ•°æ®åº“ï¼ŒåŒ…å«è¶…è¿‡36,000å¯¹å›¾åƒåŠ500ä¸‡æ¡æ³¨é‡Šï¼Œå¡«è¡¥äº†å…·èº«AIé¢†åŸŸå¯¹å›¾åƒè´¨é‡è¯„ä¼°çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ³¨é‡Šï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶å®šä¹‰äº†å…¨é¢çš„ä¸»è§‚è¯„åˆ†æ”¶é›†è¿‡ç¨‹ï¼Œä»¥æå‡è¯„ä¼°çš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸»æµIQAæ–¹æ³•åœ¨Embodied-IQAæ•°æ®åº“ä¸Šçš„è¡¨ç°ä¸è¶³ï¼Œå¼ºè°ƒäº†å¼€å‘æ–°è´¨é‡æŒ‡æ ‡çš„å¿…è¦æ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒæŸäº›æ–¹æ³•çš„è¯„ä¼°å‡†ç¡®ç‡æå‡äº†20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºæ–°æ–¹æ³•åœ¨å…·èº«AIåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰ã€‚é€šè¿‡æä¾›å‡†ç¡®çš„å›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼Œå¯ä»¥æ˜¾è‘—æå‡å…·èº«AIåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œæ‰§è¡Œæ•ˆç‡ï¼Œæ¨åŠ¨å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied AI has developed rapidly in recent years, but it is still mainly deployed in laboratories, with various distortions in the Real-world limiting its application. Traditionally, Image Quality Assessment (IQA) methods are applied to predict human preferences for distorted images; however, there is no IQA method to assess the usability of an image in embodied tasks, namely, the perceptual quality for robots. To provide accurate and reliable quality indicators for future embodied scenarios, we first propose the topic: IQA for Embodied AI. Specifically, we (1) based on the Mertonian system and meta-cognitive theory, constructed a perception-cognition-decision-execution pipeline and defined a comprehensive subjective score collection process; (2) established the Embodied-IQA database, containing over 36k reference/distorted image pairs, with more than 5m fine-grained annotations provided by Vision Language Models/Vision Language Action-models/Real-world robots; (3) trained and validated the performance of mainstream IQA methods on Embodied-IQA, demonstrating the need to develop more accurate quality indicators for Embodied AI. We sincerely hope that through evaluation, we can promote the application of Embodied AI under complex distortions in the Real-world. Project page: https://github.com/lcysyzxdxc/EmbodiedIQA

