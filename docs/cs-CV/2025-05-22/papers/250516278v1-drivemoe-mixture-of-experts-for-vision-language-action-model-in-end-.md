---
layout: default
title: DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving
---

# DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.16278" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.16278v1</a>
  <a href="https://arxiv.org/pdf/2505.16278.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.16278v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.16278v1', 'DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenjie Yang, Yilin Chai, Xiaosong Jia, Qifeng Li, Yuqian Shao, Xuekai Zhu, Haisheng Su, Junchi Yan

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-22

**å¤‡æ³¨**: Project Page: https://thinklab-sjtu.github.io/DriveMoE/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDriveMoEä»¥è§£å†³ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­çš„å¤æ‚åœºæ™¯å¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `æ··åˆä¸“å®¶` `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `åŠ¨æ€è·¯ç”±` `è¡Œä¸ºä¸“é—¨åŒ–` `å¤æ‚åœºæ™¯å¤„ç†` `æ™ºèƒ½äº¤é€š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ–¹æ³•åœ¨å¤„ç†å¤æ‚é©¾é©¶åœºæ™¯æ—¶å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åº”å¯¹ç¨€æœ‰å’Œæ¿€è¿›çš„é©¾é©¶è¡Œä¸ºæ—¶ã€‚
2. è®ºæ–‡æå‡ºçš„DriveMoEæ¡†æ¶é€šè¿‡å¼•å…¥è§†è§‰å’ŒåŠ¨ä½œçš„æ··åˆä¸“å®¶æœºåˆ¶ï¼ŒåŠ¨æ€é€‰æ‹©ç›¸å…³ä¿¡æ¯ä»¥æé«˜å†³ç­–æ•ˆç‡ã€‚
3. åœ¨Bench2Driveè¯„ä¼°å®éªŒä¸­ï¼ŒDriveMoEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ ·åŒ–åœºæ™¯å¤„ç†ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ï¼ˆE2E-ADï¼‰éœ€è¦æœ‰æ•ˆå¤„ç†å¤šè§†è§’ä¼ æ„Ÿå™¨æ•°æ®ï¼Œå¹¶ç¨³å¥åº”å¯¹å¤šæ ·ä¸”å¤æ‚çš„é©¾é©¶åœºæ™¯ï¼Œå°¤å…¶æ˜¯ç¨€æœ‰çš„æ¿€è¿›è½¬å¼¯ç­‰åŠ¨ä½œã€‚æœ¬æ–‡æå‡ºDriveMoEï¼Œä¸€ä¸ªåŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„E2E-ADæ¡†æ¶ï¼ŒåŒ…å«åœºæ™¯ä¸“ç”¨çš„è§†è§‰MoEå’ŒæŠ€èƒ½ä¸“ç”¨çš„åŠ¨ä½œMoEã€‚DriveMoEåœ¨æˆ‘ä»¬çš„$Ï€_0$è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰åŸºçº¿Drive-$Ï€_0$åŸºç¡€ä¸Šæ„å»ºï¼Œé€šè¿‡è®­ç»ƒè·¯ç”±å™¨åŠ¨æ€é€‰æ‹©ä¸é©¾é©¶ä¸Šä¸‹æ–‡ç›¸å…³çš„æ‘„åƒå¤´ï¼Œæ¨¡æ‹Ÿäººç±»é©¾é©¶è®¤çŸ¥ã€‚é€šè¿‡æ˜ç¡®çš„è¡Œä¸ºä¸“é—¨åŒ–ï¼ŒDriveMoEèƒ½å¤Ÿå¤„ç†å¤šæ ·åœºæ™¯ï¼Œè€Œä¸å—ç°æœ‰æ¨¡å‹çš„æ¨¡å¼å¹³å‡å½±å“ã€‚åœ¨Bench2Driveé—­ç¯è¯„ä¼°å®éªŒä¸­ï¼ŒDriveMoEå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è§†è§‰å’ŒåŠ¨ä½œMoEç»“åˆåœ¨è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­å¯¹å¤æ‚å¤šæ ·é©¾é©¶åœºæ™¯çš„å¤„ç†é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ç¨€æœ‰é©¾é©¶è¡Œä¸ºæ—¶å®¹æ˜“å‡ºç°æ¨¡å¼å¹³å‡ç°è±¡ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDriveMoEé€šè¿‡å¼•å…¥æ··åˆä¸“å®¶æ¶æ„ï¼Œåˆ†åˆ«é’ˆå¯¹è§†è§‰å’ŒåŠ¨ä½œè¿›è¡Œä¸“é—¨åŒ–å¤„ç†ï¼ŒåŠ¨æ€é€‰æ‹©ä¸å½“å‰é©¾é©¶æƒ…å¢ƒç›¸å…³çš„ä¸“å®¶æ¨¡å—ï¼Œä»¥æ¨¡æ‹Ÿäººç±»é©¾é©¶è€…çš„é€‰æ‹©æ€§æ³¨æ„æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDriveMoEæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåœºæ™¯ä¸“ç”¨çš„è§†è§‰MoEå’ŒæŠ€èƒ½ä¸“ç”¨çš„åŠ¨ä½œMoEã€‚è§†è§‰MoEé€šè¿‡è·¯ç”±å™¨é€‰æ‹©åˆé€‚çš„æ‘„åƒå¤´è¾“å…¥ï¼Œè€ŒåŠ¨ä½œMoEåˆ™é€šè¿‡å¦ä¸€ä¸ªè·¯ç”±å™¨æ¿€æ´»ä¸åŒçš„ä¸“å®¶æ¨¡å—ä»¥åº”å¯¹ç‰¹å®šçš„é©¾é©¶è¡Œä¸ºã€‚

**å…³é”®åˆ›æ–°**ï¼šDriveMoEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡æ˜ç¡®çš„è¡Œä¸ºä¸“é—¨åŒ–ï¼Œé¿å…äº†ä¼ ç»Ÿæ¨¡å‹çš„æ¨¡å¼å¹³å‡é—®é¢˜ï¼Œä»è€Œæå‡äº†åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å†³ç­–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œè·¯ç”±å™¨çš„è®­ç»ƒç­–ç•¥å’Œä¸“å®¶æ¨¡å—çš„é€‰æ‹©æœºåˆ¶æ˜¯å…³é”®ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å®æ—¶é©¾é©¶ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´å…¶å†³ç­–è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Bench2Driveé—­ç¯è¯„ä¼°å®éªŒä¸­ï¼ŒDriveMoEå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹Drive-$Ï€_0$ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæŠ«éœ²ï¼Œä½†å®éªŒç»“æœè¡¨æ˜å…¶åœ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸­çš„å¤„ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DriveMoEçš„ç ”ç©¶æˆæœåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯æ‰©å±•è‡³å…¶ä»–éœ€è¦å®æ—¶å†³ç­–çš„é¢†åŸŸï¼Œå¦‚æœºå™¨äººå¯¼èˆªå’Œæ™ºèƒ½äº¤é€šç®¡ç†ï¼Œæ¨åŠ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> End-to-end autonomous driving (E2E-AD) demands effective processing of multi-view sensory data and robust handling of diverse and complex driving scenarios, particularly rare maneuvers such as aggressive turns. Recent success of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs) demonstrates that specialization of parameters enables strong scalability. In this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is built upon our $Ï€_0$ Vision-Language-Action (VLA) baseline (originally from the embodied AI field), called Drive-$Ï€_0$. Specifically, we add Vision MoE to Drive-$Ï€_0$ by training a router to select relevant cameras according to the driving context dynamically. This design mirrors human driving cognition, where drivers selectively attend to crucial visual cues rather than exhaustively processing all visual information. In addition, we add Action MoE by training another router to activate specialized expert modules for different driving behaviors. Through explicit behavioral specialization, DriveMoE is able to handle diverse scenarios without suffering from modes averaging like existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness of combining vision and action MoE in autonomous driving tasks. We will release our code and models of DriveMoE and Drive-$Ï€_0$.

