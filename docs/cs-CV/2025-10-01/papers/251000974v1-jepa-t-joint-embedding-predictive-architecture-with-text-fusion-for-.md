---
layout: default
title: "JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation"
---

# JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00974" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00974v1</a>
  <a href="https://arxiv.org/pdf/2510.00974.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00974v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00974v1', 'JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siheng Wan, Zhengtao Yao, Zhengdao Li, Junhao Dong, Yanshu Li, Yikai Li, Linshan Li, Haoyan Xu, Yijiang Li, Zhikang Dong, Huacan Wang, Jifeng Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/justin-herry/JEPA-T.git)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºJEPA-Tï¼Œé€šè¿‡æ–‡æœ¬èåˆçš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„æå‡å›¾åƒç”Ÿæˆæ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `è”åˆåµŒå…¥` `Transformer` `äº¤å‰æ³¨æ„åŠ›` `æ¡ä»¶å»å™ª` `å¤šæ¨¡æ€èåˆ` `è‡ªç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆæ–‡æœ¬å’Œè§†è§‰tokenï¼Œé™åˆ¶äº†ç”Ÿæˆæ•ˆæœã€‚
2. JEPA-Tæå‡ºè”åˆåµŒå…¥é¢„æµ‹Transformerï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›å’Œç›®æ ‡çº§åˆ«å¯¹é½ï¼Œå¢å¼ºæ–‡æœ¬å’Œå›¾åƒçš„èåˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒJEPA-Tåœ¨æ•°æ®æ•ˆç‡å’Œå¼€æ”¾è¯æ±‡æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆè¶Šæ¥è¶Šä¾èµ–äºä½¿ç”¨è‡ªç›‘ç£è®­ç»ƒçš„ä»¥tokenä¸ºä¸­å¿ƒçš„æ¶æ„ï¼Œç„¶è€Œï¼Œæœ‰æ•ˆåœ°èåˆæ–‡æœ¬å’Œè§†è§‰tokenä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†JEPA-Tï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œå®ƒå°†å›¾åƒå’Œæ ‡é¢˜ç¼–ç ä¸ºç¦»æ•£çš„è§†è§‰å’Œæ–‡æœ¬tokenï¼Œå¹¶ç”±è”åˆåµŒå…¥é¢„æµ‹Transformerå¤„ç†ã€‚ä¸ºäº†å¢å¼ºèåˆï¼Œæˆ‘ä»¬åœ¨ç‰¹å¾é¢„æµ‹å™¨ä¹‹ååŠ å…¥äº†äº¤å‰æ³¨æ„åŠ›ï¼Œç”¨äºæ¡ä»¶å»å™ªï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ— å…³çš„éª¨å¹²ç½‘ç»œã€‚æ­¤å¤–ï¼Œåœ¨æµåŒ¹é…æŸå¤±ä¹‹å‰æ³¨å…¥åŸå§‹æ–‡æœ¬åµŒå…¥ï¼Œä»¥æé«˜è®­ç»ƒæœŸé—´çš„å¯¹é½ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒåŒä¸€ç½‘ç»œé€šè¿‡è¿­ä»£åœ°å¯¹ä»¥æ–‡æœ¬ä¸ºæ¡ä»¶çš„è§†è§‰tokenè¿›è¡Œå»å™ªï¼Œæ¥æ‰§è¡Œç±»æ¡ä»¶å’Œè‡ªç”±æ–‡æœ¬å›¾åƒç”Ÿæˆã€‚åœ¨ImageNet-1Kä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒJEPA-Tå®ç°äº†å¼ºå¤§çš„æ•°æ®æ•ˆç‡ã€å¼€æ”¾è¯æ±‡æ³›åŒ–ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºéèåˆå’Œæ™šæœŸèåˆåŸºçº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œæ™šæœŸæ¶æ„èåˆä¸ç›®æ ‡çº§åˆ«çš„å¯¹é½ç›¸ç»“åˆï¼Œåœ¨åŸºäºtokençš„T2Iä¸­ï¼Œåœ¨æ¡ä»¶å¼ºåº¦å’Œéª¨å¹²é€šç”¨æ€§ä¹‹é—´æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºtokençš„æ¶æ„ï¼Œåœ¨å¦‚ä½•æœ‰æ•ˆåœ°å°†æ–‡æœ¬ä¿¡æ¯èå…¥åˆ°è§†è§‰tokençš„å¤„ç†æµç¨‹ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚ç®€å•çš„æ‹¼æ¥æˆ–æ™šæœŸèåˆç­–ç•¥å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨æ–‡æœ¬ä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æè¿°ä¸ä¸€è‡´æˆ–ç»†èŠ‚ç¼ºå¤±ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¡ä»¶å¼ºåº¦å’Œéª¨å¹²é€šç”¨æ€§ä¹‹é—´éš¾ä»¥å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šJEPA-Tçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼Œå°†å›¾åƒå’Œæ–‡æœ¬ç¼–ç ä¸ºç¦»æ•£çš„tokenï¼Œå¹¶åœ¨Transformerä¸­è¿›è¡Œè”åˆå¤„ç†ã€‚é€šè¿‡åœ¨ç‰¹å¾é¢„æµ‹å™¨åå¼•å…¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°æ–‡æœ¬ä¿¡æ¯å¯¹è§†è§‰tokençš„æ¡ä»¶å»å™ªï¼Œä»è€Œå¢å¼ºæ–‡æœ¬å’Œå›¾åƒçš„èåˆã€‚æ­¤å¤–ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡åœ¨æµåŒ¹é…æŸå¤±ä¹‹å‰æ³¨å…¥åŸå§‹æ–‡æœ¬åµŒå…¥ï¼Œè¿›ä¸€æ­¥æé«˜æ–‡æœ¬å’Œå›¾åƒçš„å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šJEPA-Tçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼šå°†å›¾åƒå’Œæ–‡æœ¬åˆ†åˆ«ç¼–ç ä¸ºç¦»æ•£çš„è§†è§‰å’Œæ–‡æœ¬tokenã€‚2) è”åˆåµŒå…¥é¢„æµ‹Transformerï¼šå¤„ç†è§†è§‰å’Œæ–‡æœ¬tokenï¼Œè¿›è¡Œç‰¹å¾é¢„æµ‹å’Œèåˆã€‚3) äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼šåœ¨ç‰¹å¾é¢„æµ‹å™¨ä¹‹åï¼Œåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯å¯¹è§†è§‰tokenè¿›è¡Œæ¡ä»¶å»å™ªã€‚4) æµåŒ¹é…æŸå¤±ï¼šç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨æŸå¤±è®¡ç®—å‰æ³¨å…¥åŸå§‹æ–‡æœ¬åµŒå…¥ä»¥æé«˜å¯¹é½ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡è¿­ä»£å»å™ªè§†è§‰tokenç”Ÿæˆå›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šJEPA-Tçš„å…³é”®åˆ›æ–°åœ¨äºå…¶èåˆç­–ç•¥ï¼Œå³æ™šæœŸæ¶æ„èåˆä¸ç›®æ ‡çº§åˆ«å¯¹é½ç›¸ç»“åˆã€‚è¿™ç§ç­–ç•¥åœ¨ä¿æŒéª¨å¹²ç½‘ç»œé€šç”¨æ€§çš„åŒæ—¶ï¼Œå¢å¼ºäº†æ–‡æœ¬ä¿¡æ¯çš„æ¡ä»¶ä½œç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œäº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„å¼•å…¥å’ŒåŸå§‹æ–‡æœ¬åµŒå…¥çš„æ³¨å…¥ï¼Œä½¿å¾—æ–‡æœ¬ä¿¡æ¯èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šJEPA-Tçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç¦»æ•£çš„è§†è§‰å’Œæ–‡æœ¬tokenè¡¨ç¤ºå›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ã€‚2) åœ¨ç‰¹å¾é¢„æµ‹å™¨åæ·»åŠ äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°æ¡ä»¶å»å™ªã€‚3) åœ¨æµåŒ¹é…æŸå¤±ä¹‹å‰æ³¨å…¥åŸå§‹æ–‡æœ¬åµŒå…¥ï¼Œä»¥æé«˜è®­ç»ƒæœŸé—´çš„å¯¹é½ã€‚4) ä½¿ç”¨Transformerä½œä¸ºæ ¸å¿ƒå¤„ç†æ¨¡å—ï¼Œè¿›è¡Œç‰¹å¾é¢„æµ‹å’Œèåˆã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€è¦å‚è€ƒè®ºæ–‡å…¨æ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

JEPA-Tåœ¨ImageNet-1Kæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒJEPA-Tå®ç°äº†å¼ºå¤§çš„æ•°æ®æ•ˆç‡å’Œå¼€æ”¾è¯æ±‡æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºéèåˆå’Œæ™šæœŸèåˆåŸºçº¿æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡å…¨æ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

JEPA-Tåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è‰ºæœ¯åˆ›ä½œã€å›¾åƒç¼–è¾‘ã€è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ç­‰ã€‚è¯¥ç ”ç©¶å¯ä»¥ç”¨äºç”Ÿæˆé«˜è´¨é‡ã€ä¸æ–‡æœ¬æè¿°é«˜åº¦ä¸€è‡´çš„å›¾åƒï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œåˆ›ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘ç”Ÿæˆã€3Dæ¨¡å‹ç”Ÿæˆç­‰é¢†åŸŸï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern Text-to-Image (T2I) generation increasingly relies on token-centric architectures that are trained with self-supervision, yet effectively fusing text with visual tokens remains a challenge. We propose \textbf{JEPA-T}, a unified multimodal framework that encodes images and captions into discrete visual and textual tokens, processed by a joint-embedding predictive Transformer. To enhance fusion, we incorporate cross-attention after the feature predictor for conditional denoising while maintaining a task-agnostic backbone. Additionally, raw texts embeddings are injected prior to the flow matching loss to improve alignment during training. During inference, the same network performs both class-conditional and free-text image generation by iteratively denoising visual tokens conditioned on text. Evaluations on ImageNet-1K demonstrate that JEPA-T achieves strong data efficiency, open-vocabulary generalization, and consistently outperforms non-fusion and late-fusion baselines. Our approach shows that late architectural fusion combined with objective-level alignment offers an effective balance between conditioning strength and backbone generality in token-based T2I.The code is now available: https://github.com/justin-herry/JEPA-T.git

