---
layout: default
title: VIRTUE: Visual-Interactive Text-Image Universal Embedder
---

# VIRTUE: Visual-Interactive Text-Image Universal Embedder

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.00523" target="_blank" class="toolbar-btn">arXiv: 2510.00523v1</a>
    <a href="https://arxiv.org/pdf/2510.00523.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00523v1" 
            onclick="toggleFavorite(this, '2510.00523v1', 'VIRTUE: Visual-Interactive Text-Image Universal Embedder')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Wei-Yao Wang, Kazuya Tateishi, Qiyu Wu, Shusuke Takahashi, Yuki Mitsufuji

**ÂàÜÁ±ª**: cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-01

**Â§áÊ≥®**: 25 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VIRTUEÔºö‰∏ÄÁßçËßÜËßâ‰∫§‰∫íÂºèÊñáÊú¨-ÂõæÂÉèÈÄöÁî®ÂµåÂÖ•Ê®°ÂûãÔºåÊèêÂçáÂ§öÊ®°ÊÄÅË°®ÂæÅËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ‰∫§‰∫í` `ÊñáÊú¨ÂõæÂÉèÂµåÂÖ•` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂàÜÂâ≤Ê®°Âûã` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Ë°®ÂæÅÂ≠¶‰π†` `‰∫∫Êú∫‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂµåÂÖ•Ê®°ÂûãÁº∫‰πèËßÜËßâ‰∫§‰∫íËÉΩÂäõÔºåÊó†Ê≥ïÊ†πÊçÆÁî®Êà∑ÊåáÂÆöÁöÑÂõæÂÉèÂå∫ÂüüËøõË°åÁ≤æÁ°ÆÂµåÂÖ•ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∫∫Êú∫‰∫§‰∫íÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. VIRTUEÈÄöËøáÈõÜÊàêÂàÜÂâ≤Ê®°ÂûãÂíåËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÂµåÂÖ•Âô®ËÉΩÂ§üÂ§ÑÁêÜËßÜËßâÊèêÁ§∫Ôºå‰ªéËÄåÁ≤æÁ°ÆÂÆö‰ΩçÂõæÂÉè‰∏≠ÁöÑÁâπÂÆöÂå∫ÂüüÔºåÊèêÂçáË°®ÂæÅÂ≠¶‰π†ËÉΩÂäõ„ÄÇ
3. Âú®ÂåÖÂê´100‰∏áÊ†∑Êú¨ÁöÑSCaRÂü∫ÂáÜÊµãËØï‰∏≠ÔºåVIRTUEÂú®Â§ö‰∏™‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåËØÅÊòé‰∫ÜÂÖ∂ËßÜËßâ‰∫§‰∫íËÉΩÂäõÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅË°®ÂæÅÂ≠¶‰π†Ê®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÈõÜÊàêËøõ‰∏ÄÊ≠•‰ΩøÂµåÂÖ•Ê®°ÂûãÂÖ∑Â§á‰∫ÜÊåá‰ª§Ë∑üÈöèËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂµåÂÖ•Ê®°ÂûãÁº∫‰πèËßÜËßâ‰∫§‰∫íËÉΩÂäõÔºåÊó†Ê≥ïÊåáÂÆöÁî®Êà∑ÊÑüÂÖ¥Ë∂£ÁöÑÂå∫ÂüüÔºà‰æãÂ¶ÇÔºåÁÇπ„ÄÅËæπÁïåÊ°Ü„ÄÅÊé©Á†ÅÔºâÔºåËÄåËøôÁßçËÉΩÂäõÂ∑≤ÁªèÂú®ÁîüÊàêÊ®°Âûã‰∏≠ÂæóÂà∞Êé¢Á¥¢Ôºå‰ª•Êâ©Â±ïÂÖ∂‰∫∫Êú∫‰∫§‰∫íÈÄÇÁî®ÊÄß„ÄÇ‰∏∫ÂµåÂÖ•Ê®°ÂûãÈÖçÂ§áËßÜËßâ‰∫§‰∫íËÉΩÂäõ‰∏ç‰ªÖÂèØ‰ª•Ëß£ÈîÅÊñ∞ÁöÑÂ∫îÁî®ÔºåÂÆûÁé∞Áî®Êà∑ÊÑèÂõæÁöÑÂ±ÄÈÉ®ÂåñÂÆö‰ΩçÔºåËøô‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™Ë¢´Êé¢Á¥¢ÁöÑÈ¢ÜÂüüÔºåËøòÂèØ‰ª•‰ΩøÊ®°ÂûãÂ≠¶‰π†ÂõæÂÉè‰∏≠ÁöÑÂÆû‰ΩìÁ∫ßÂà´‰ø°ÊÅØÔºå‰ª•Ë°•ÂÖÖÂÖ∂Áî®‰∫é‰º†ÁªüÂµåÂÖ•‰ªªÂä°ÁöÑÂÖ®Â±ÄË°®ÂæÅ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâ‰∫§‰∫íÂºèÊñáÊú¨-ÂõæÂÉèÈÄöÁî®ÂµåÂÖ•Âô®ÔºàVIRTUEÔºâÔºåÂÆÉÂ∞ÜÂàÜÂâ≤Ê®°ÂûãÂíåËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõÊâ©Â±ïÂà∞Ë°®ÂæÅÂ≠¶‰π†È¢ÜÂüü„ÄÇÂú®VIRTUE‰∏≠ÔºåÂàÜÂâ≤Ê®°ÂûãÂèØ‰ª•Â§ÑÁêÜËßÜËßâÊèêÁ§∫ÔºåÁ≤æÁ°ÆÂÆö‰ΩçÂõæÂÉè‰∏≠ÁöÑÁâπÂÆöÂå∫ÂüüÔºå‰ªéËÄå‰ΩøÂµåÂÖ•Âô®ËÉΩÂ§üÊõ¥Á≤æÁ°ÆÂú∞Â§ÑÁêÜÂ§çÊùÇÂíåÊ®°Á≥äÁöÑÂú∫ÊôØ„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞VIRTUEÁöÑËßÜËßâ‰∫§‰∫íËÉΩÂäõÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂàÜÂâ≤ÂíåÂú∫ÊôØÂ≠óÂπïÊ£ÄÁ¥¢ÔºàSCaRÔºâÂü∫ÂáÜÔºåÂåÖÂê´100‰∏á‰∏™Ê†∑Êú¨ÔºåÊó®Âú®ÈÄöËøáËÅîÂêàËÄÉËôëÂÖ∑ÊúâÁâπÂÆöÂØπË±°ÂíåÂõæÂÉèÂú∫ÊôØÁöÑÂÆû‰ΩìÊù•Ê£ÄÁ¥¢ÊñáÊú¨Â≠óÂπï„ÄÇVIRTUEÂú®36‰∏™ÈÄöÁî®MMEB‰ªªÂä°Ôºà3.1%-8.5%ÔºâÂíå5‰∏™ËßÜËßâ‰∫§‰∫íÂºèSCaR‰ªªÂä°Ôºà15.2%-20.3%Ôºâ‰∏≠ÂßãÁªàÂ¶Ç‰∏ÄÂú∞ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂπ∂ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊîπËøõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñáÊú¨-ÂõæÂÉèÂµåÂÖ•Ê®°Âûã‰∏ªË¶ÅÂÖ≥Ê≥®ÂÖ®Â±ÄÂõæÂÉèÁâπÂæÅÔºåÁº∫‰πèÂØπÁî®Êà∑ÊåáÂÆöÂõæÂÉèÂå∫ÂüüÁöÑ‰∫§‰∫íËÉΩÂäõ„ÄÇËøôÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®ÈúÄË¶ÅÁ≤æÁªÜÂåñÁêÜËß£ÂíåÂÆö‰ΩçÁî®Êà∑ÊÑèÂõæÁöÑÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®Ôºå‰æãÂ¶ÇÔºåÊ†πÊçÆÁî®Êà∑ÁÇπÂáªÁöÑÁâ©‰ΩìÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÊú¨ÊèèËø∞„ÄÇÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂà©Áî®ËßÜËßâÊèêÁ§∫‰ø°ÊÅØÔºåÂØºËá¥Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãË°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVIRTUEÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂàÜÂâ≤Ê®°Âûã‰∏éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁõ∏ÁªìÂêàÔºåÂà©Áî®ÂàÜÂâ≤Ê®°ÂûãÂ§ÑÁêÜËßÜËßâÊèêÁ§∫Ôºå‰ªéËÄåÁ≤æÁ°ÆÂÆö‰ΩçÂõæÂÉè‰∏≠ÁöÑÁâπÂÆöÂå∫Âüü„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞Êõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÂÆû‰ΩìÁ∫ßÂà´‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂ËûçÂÖ•Âà∞ÂÖ®Â±ÄË°®ÂæÅ‰∏≠Ôºå‰ªéËÄåÊèêÂçáÂµåÂÖ•ÁöÑÂáÜÁ°ÆÊÄßÂíå‰∫§‰∫íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVIRTUEÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöËßÜËßâÊèêÁ§∫Â§ÑÁêÜÊ®°Âùó„ÄÅÂõæÂÉèÁºñÁ†ÅÊ®°ÂùóÂíåÊñáÊú¨ÁºñÁ†ÅÊ®°Âùó„ÄÇËßÜËßâÊèêÁ§∫Â§ÑÁêÜÊ®°ÂùóÂà©Áî®ÂàÜÂâ≤Ê®°ÂûãÂ§ÑÁêÜÁî®Êà∑Êèê‰æõÁöÑËßÜËßâÊèêÁ§∫ÔºàÂ¶ÇÁÇπ„ÄÅËæπÁïåÊ°Ü„ÄÅÊé©Á†ÅÔºâÔºåÊèêÂèñÊÑüÂÖ¥Ë∂£Âå∫ÂüüÁöÑÁâπÂæÅ„ÄÇÂõæÂÉèÁºñÁ†ÅÊ®°ÂùóË¥üË¥£ÊèêÂèñÂÖ®Â±ÄÂõæÂÉèÁâπÂæÅ„ÄÇÊñáÊú¨ÁºñÁ†ÅÊ®°ÂùóË¥üË¥£ÊèêÂèñÊñáÊú¨ÊèèËø∞ÁöÑÁâπÂæÅ„ÄÇÊúÄÁªàÔºåÊ®°ÂûãÂ∞ÜËßÜËßâÊèêÁ§∫ÁâπÂæÅ„ÄÅÂÖ®Â±ÄÂõæÂÉèÁâπÂæÅÂíåÊñáÊú¨ÁâπÂæÅËûçÂêàÔºåÁîüÊàêÊúÄÁªàÁöÑÂµåÂÖ•ÂêëÈáè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVIRTUEÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂàÜÂâ≤Ê®°ÂûãÂºïÂÖ•Âà∞ÊñáÊú¨-ÂõæÂÉèÂµåÂÖ•Ê°ÜÊû∂‰∏≠Ôºå‰ΩøÂÖ∂ÂÖ∑Â§á‰∫ÜËßÜËßâ‰∫§‰∫íËÉΩÂäõ„ÄÇËøô‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊ†πÊçÆÁî®Êà∑ÊåáÂÆöÁöÑÂõæÂÉèÂå∫ÂüüËøõË°åÁ≤æÁ°ÆÂµåÂÖ•Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£Áî®Êà∑ÊÑèÂõæ„ÄÇÊ≠§Â§ñÔºåVIRTUEËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂàÜÂâ≤ÂíåÂú∫ÊôØÂ≠óÂπïÊ£ÄÁ¥¢ÔºàSCaRÔºâÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÁöÑËßÜËßâ‰∫§‰∫íËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVIRTUE‰ΩøÁî®‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Âü∫Á°ÄÊû∂ÊûÑÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äËøõË°å‰∫ÜÂæÆË∞É„ÄÇÂàÜÂâ≤Ê®°ÂûãÈááÁî®‰∫ÜMask2Former„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂØπÊØîÊçüÂ§±Âíå‰∫§ÂèâÁÜµÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÂµåÂÖ•ÂêëÈáèÁöÑÁõ∏‰ººÊÄßÂíåÂàÜÁ±ªÊÄßËÉΩ„ÄÇËßÜËßâÊèêÁ§∫ÁöÑÁºñÁ†ÅÊñπÂºèÈááÁî®‰∫ÜRoI Align„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VIRTUEÂú®36‰∏™ÈÄöÁî®MMEB‰ªªÂä°‰∏äÂèñÂæó‰∫Ü3.1%-8.5%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂú®5‰∏™ËßÜËßâ‰∫§‰∫íÂºèSCaR‰ªªÂä°‰∏äÂèñÂæó‰∫Ü15.2%-20.3%ÁöÑÊòæËëóÊèêÂçá„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåVIRTUEÁöÑËßÜËßâ‰∫§‰∫íËÉΩÂäõËÉΩÂ§üÊúâÊïàÊèêÂçáÂ§öÊ®°ÊÄÅË°®ÂæÅÂ≠¶‰π†ÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÁ≤æÁªÜÂåñÁêÜËß£ÂíåÂÆö‰ΩçÁî®Êà∑ÊÑèÂõæÁöÑÂú∫ÊôØ‰∏ã„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VIRTUEÂèØÂ∫îÁî®‰∫éÂõæÂÉèÊ£ÄÁ¥¢„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅ‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ÁîµÂïÜÂú∫ÊôØ‰∏≠ÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøáÁÇπÂáªÂïÜÂìÅÂõæÁâá‰∏≠ÁöÑÁâπÂÆöÂå∫ÂüüÊù•ÊêúÁ¥¢Áõ∏‰ººÂïÜÂìÅÔºõÂú®Êô∫ËÉΩÂÆ¢ÊúçÂú∫ÊôØ‰∏≠ÔºåÂèØ‰ª•ÈÄöËøáËßÜËßâÊèêÁ§∫ÂºïÂØºÊ®°ÂûãÁêÜËß£Áî®Êà∑ÊÑèÂõæÔºå‰ªéËÄåÊèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÁ≠îÊ°à„ÄÇËØ•Á†îÁ©∂ÊúâÊúõÊé®Âä®Â§öÊ®°ÊÄÅ‰∫§‰∫íÂºè‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.

