---
layout: default
title: Feature Identification for Hierarchical Contrastive Learning
---

# Feature Identification for Hierarchical Contrastive Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.00837" target="_blank" class="toolbar-btn">arXiv: 2510.00837v1</a>
    <a href="https://arxiv.org/pdf/2510.00837.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00837v1" 
            onclick="toggleFavorite(this, '2510.00837v1', 'Feature Identification for Hierarchical Contrastive Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Julius Ott, Nastassia Vysotskaya, Huawei Sun, Lorenzo Servadei, Robert Wille

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-01

**Â§áÊ≥®**: Submitted to ICASSP 2026

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏§ÁßçÂ±ÇÁ∫ßÂØπÊØîÂ≠¶‰π†ÊñπÊ≥ïÔºåÂà©Áî®Â±ÇÁ∫ßÂÖ≥Á≥ªÊèêÂçáÁªÜÁ≤íÂ∫¶ÂàÜÁ±ªÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â±ÇÁ∫ßÂàÜÁ±ª` `ÂØπÊØîÂ≠¶‰π†` `È´òÊñØÊ∑∑ÂêàÊ®°Âûã` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÁªÜÁ≤íÂ∫¶ÂàÜÁ±ª` `ËÆ°ÁÆóÊú∫ËßÜËßâ` `Ë°®Á§∫Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÂàÜÁ±ªÊñπÊ≥ïÂøΩÁï•‰∫ÜÂ±ÇÁ∫ßÁªìÊûÑ‰∏≠Âõ∫ÊúâÁöÑÁ±ªÈó¥ÂÖ≥Á≥ªÔºå‰∏¢Â§±‰∫ÜÈáçË¶ÅÁöÑÁõëÁù£‰ø°Âè∑„ÄÇ
2. ÊèêÂá∫G-HMLCÂíåA-HMLC‰∏§ÁßçÊñπÊ≥ïÔºåÂàÜÂà´Âà©Áî®È´òÊñØÊ∑∑ÂêàÊ®°ÂûãÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ÊçïËé∑Â±ÇÁ∫ßÁâπÂæÅ„ÄÇ
3. Âú®CIFAR100ÂíåModelNet40Êï∞ÊçÆÈõÜ‰∏äÔºåÁ∫øÊÄßËØÑ‰º∞ÂáÜÁ°ÆÁéáË∂ÖË∂äÁé∞ÊúâÊñπÊ≥ï2‰∏™ÁôæÂàÜÁÇπ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏§ÁßçÊñ∞È¢ñÁöÑÂ±ÇÁ∫ßÂØπÊØîÂ≠¶‰π†ÔºàHMLCÔºâÊñπÊ≥ïÔºåÁî®‰∫éËß£ÂÜ≥Â±ÇÁ∫ßÂàÜÁ±ªÈóÆÈ¢ò„ÄÇÁ¨¨‰∏ÄÁßçÊñπÊ≥ïÔºàG-HMLCÔºâÂà©Áî®È´òÊñØÊ∑∑ÂêàÊ®°ÂûãÔºåÁ¨¨‰∫åÁßçÊñπÊ≥ïÔºàA-HMLCÔºâ‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•ÊçïËé∑Â±ÇÁ∫ßÁâπÂÆöÁöÑÁâπÂæÅÔºåÊ®°‰ªø‰∫∫Á±ªÂ§ÑÁêÜÊñπÂºè„ÄÇËØ•ÊñπÊ≥ïÊòæÂºèÂú∞Âª∫Ê®°‰∫Ü‰∏çÂêåÂ±ÇÁ∫ßÈó¥ÁöÑÁ±ªÈó¥ÂÖ≥Á≥ª‰ª•ÂèäÈ´òÂ±ÇÁ∫ßÁöÑ‰∏çÂπ≥Ë°°Á±ªÂàÜÂ∏ÉÔºå‰ªéËÄåÂÆûÁé∞Ë∑®ÊâÄÊúâÂ±ÇÁ∫ßÁöÑÁªÜÁ≤íÂ∫¶ËÅöÁ±ª„ÄÇÂú®CIFAR100ÂíåModelNet40Êï∞ÊçÆÈõÜ‰∏äÁöÑÁ∫øÊÄßËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂú®ÂáÜÁ°ÆÁéáÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÂ±ÇÁ∫ßÂØπÊØîÂ≠¶‰π†ÊñπÊ≥ï2‰∏™ÁôæÂàÜÁÇπ„ÄÇÂÆöÈáèÂíåÂÆöÊÄßÁªìÊûúÂùáÊîØÊåÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÂèäÂÖ∂‰ªñÈ¢ÜÂüüÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ±ÇÁ∫ßÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåÁé∞ÊúâÊñπÊ≥ïÂøΩÁï•‰∫Ü‰∏çÂêåÂ±ÇÁ∫ßÁ±ªÂà´‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Â±ÇÁ∫ßÁªìÊûÑÊèê‰æõÁöÑÁõëÁù£‰ø°ÊÅØ„ÄÇËøôÂØºËá¥Ê®°ÂûãÊó†Ê≥ïÊúâÊïàÂå∫ÂàÜÁªÜÁ≤íÂ∫¶Á±ªÂà´ÔºåÂ∞§ÂÖ∂ÊòØÂú®È´òÂ±ÇÁ∫ßÁ±ªÂà´ÂàÜÂ∏É‰∏çÂπ≥Ë°°ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊÄßËÉΩ‰ºöÂèóÂà∞ÊòæËëóÂΩ±Âìç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊòæÂºèÂú∞Âª∫Ê®°Â±ÇÁ∫ßÁªìÊûÑ‰∏≠ÁöÑÁ±ªÈó¥ÂÖ≥Á≥ª„ÄÇÈÄöËøáÂú®È´òÂ±ÇÁ∫ßÂºïÂÖ•È´òÊñØÊ∑∑ÂêàÊ®°ÂûãÊàñÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÂ±ÇÁ∫ßÁâπÂÆöÁöÑÁâπÂæÅÔºåÂπ∂Â≠¶‰π†Âà∞Êõ¥ÂÖ∑Âå∫ÂàÜÊÄßÁöÑË°®Á§∫„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®Ê®°‰ªø‰∫∫Á±ªÂú®Â§ÑÁêÜÂ±ÇÁ∫ß‰ø°ÊÅØÊó∂ÁöÑËÆ§Áü•ÊñπÂºèÔºå‰ªéËÄåÊèêÂçáÂàÜÁ±ªÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´ÁâπÂæÅÊèêÂèñ„ÄÅÂ±ÇÁ∫ßÂÖ≥Á≥ªÂª∫Ê®°ÂíåÂØπÊØîÂ≠¶‰π†‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÈ¶ñÂÖàÔºå‰ΩøÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÊèêÂèñÂõæÂÉèÊàñ3DÊ®°ÂûãÁöÑÁâπÂæÅ„ÄÇÁÑ∂ÂêéÔºåG-HMLCÊñπÊ≥ï‰ΩøÁî®È´òÊñØÊ∑∑ÂêàÊ®°ÂûãÂØπÈ´òÂ±ÇÁ∫ßÁ±ªÂà´ËøõË°åÂª∫Ê®°ÔºåA-HMLCÊñπÊ≥ïÂàô‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•ÂÖ≥Ê≥®‰∏çÂêåÂ±ÇÁ∫ßÁöÑÁâπÂæÅ„ÄÇÊúÄÂêéÔºåÈÄöËøáÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÂáΩÊï∞Ôºå‰øÉ‰ΩøÊ®°ÂûãÂ≠¶‰π†Âà∞ËÉΩÂ§üÂå∫ÂàÜ‰∏çÂêåÂ±ÇÁ∫ßÁ±ªÂà´ÁöÑË°®Á§∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÂØπÊØîÂ≠¶‰π†‰∏éÂ±ÇÁ∫ßÁªìÊûÑÁõ∏ÁªìÂêàÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏§Áßç‰∏çÂêåÁöÑÂ±ÇÁ∫ßÂÖ≥Á≥ªÂª∫Ê®°ÊñπÊ≥ïÔºàG-HMLCÂíåA-HMLCÔºâ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊú¨ÊñáÁöÑÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Â±ÇÁ∫ßÁªìÊûÑÊèê‰æõÁöÑÁõëÁù£‰ø°ÊÅØÔºå‰ªéËÄåÊèêÂçáÁªÜÁ≤íÂ∫¶ÂàÜÁ±ªÁöÑÊÄßËÉΩ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®ÊúÄÂêé‰∏ÄÂ±ÇÁöÑÂàÜÁ±ªÔºåËÄåÂøΩÁï•‰∫ÜÂ±ÇÁ∫ßÁªìÊûÑ‰∏≠Ëï¥Âê´ÁöÑ‰∏∞ÂØå‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöG-HMLCÊñπÊ≥ïÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÈ´òÊñØÊ∑∑ÂêàÊ®°ÂûãÁöÑÂèÇÊï∞ËÆæÁΩÆÔºå‰æãÂ¶ÇÈ´òÊñØÂàÜÈáèÁöÑÊï∞ÈáèÂíåÊñπÂ∑Æ„ÄÇA-HMLCÊñπÊ≥ïÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÁªìÊûÑÂíåËÆ≠ÁªÉÊñπÂºèÔºå‰æãÂ¶ÇÊ≥®ÊÑèÂäõÂ§¥ÁöÑÊï∞ÈáèÂíåÊ≥®ÊÑèÂäõÊùÉÈáçÁöÑËÆ°ÁÆóÊñπÊ≥ï„ÄÇÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÂáΩÊï∞ÈÄöÂ∏∏ÈááÁî®InfoNCEÊçüÂ§±ÔºåÈúÄË¶Å‰ªîÁªÜË∞ÉÊï¥Ê∏©Â∫¶ÂèÇÊï∞‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•ÁöÑÈÄâÊã©‰πü‰ºöÂΩ±ÂìçÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®CIFAR100ÂíåModelNet40Êï∞ÊçÆÈõÜ‰∏äÔºåÊú¨ÊñáÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Á∫øÊÄßËØÑ‰º∞‰∏≠ÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂ±ÇÁ∫ßÂØπÊØîÂ≠¶‰π†ÊñπÊ≥ï2‰∏™ÁôæÂàÜÁÇπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Â±ÇÁ∫ßÁªìÊûÑ‰ø°ÊÅØÔºåÊèêÂçáÁªÜÁ≤íÂ∫¶ÂàÜÁ±ªÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊ∂àËûçÂÆûÈ™åÈ™åËØÅ‰∫ÜÈ´òÊñØÊ∑∑ÂêàÊ®°ÂûãÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂Âú®Â±ÇÁ∫ßÂÖ≥Á≥ªÂª∫Ê®°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éËÆ°ÁÆóÊú∫ËßÜËßâÂíåÊú∫Âô®‰∫∫È¢ÜÂüüÁöÑÂ±ÇÁ∫ßÂàÜÁ±ª‰ªªÂä°Ôºå‰æãÂ¶ÇÂõæÂÉèÂàÜÁ±ª„ÄÅÁâ©‰ΩìËØÜÂà´„ÄÅÂú∫ÊôØÁêÜËß£Á≠â„ÄÇÂú®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüüÔºåÂèØÁî®‰∫éÁñæÁóÖËØäÊñ≠ÂíåÂü∫Âõ†ÂàÜÁ±ª„ÄÇÂú®Â∑•‰∏öÁïåÔºåÂèØÁî®‰∫é‰∫ßÂìÅÂàÜÁ±ªÂíåË¥®ÈáèÊ£ÄÊµã„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÂçáÁªÜÁ≤íÂ∫¶ÂàÜÁ±ªÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÊΩúÂú®ÁöÑÂïÜ‰∏öÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Hierarchical classification is a crucial task in many applications, where objects are organized into multiple levels of categories. However, conventional classification approaches often neglect inherent inter-class relationships at different hierarchy levels, thus missing important supervisory signals. Thus, we propose two novel hierarchical contrastive learning (HMLC) methods. The first, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an attention mechanism to capture hierarchy-specific features (A-HMLC), imitating human processing. Our approach explicitly models inter-class relationships and imbalanced class distribution at higher hierarchy levels, enabling fine-grained clustering across all hierarchy levels. On the competitive CIFAR100 and ModelNet40 datasets, our method achieves state-of-the-art performance in linear evaluation, outperforming existing hierarchical contrastive learning methods by 2 percentage points in terms of accuracy. The effectiveness of our approach is backed by both quantitative and qualitative results, highlighting its potential for applications in computer vision and beyond.

