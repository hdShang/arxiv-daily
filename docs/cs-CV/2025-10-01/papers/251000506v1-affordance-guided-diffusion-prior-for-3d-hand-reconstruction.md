---
layout: default
title: Affordance-Guided Diffusion Prior for 3D Hand Reconstruction
---

# Affordance-Guided Diffusion Prior for 3D Hand Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00506" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00506v1</a>
  <a href="https://arxiv.org/pdf/2510.00506.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00506v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00506v1', 'Affordance-Guided Diffusion Prior for 3D Hand Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Naru Suzuki, Takehiko Ohkawa, Tatsuro Banno, Jihyun Lee, Ryosuke Furuta, Yoichi Sato

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¯ä¾›æ€§çš„æ‰©æ•£å…ˆéªŒï¼Œç”¨äºè§£å†³3Dæ‰‹éƒ¨é‡å»ºä¸­ä¸¥é‡é®æŒ¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `3Dæ‰‹éƒ¨é‡å»º` `å¯ä¾›æ€§` `æ‰©æ•£æ¨¡å‹` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ‰‹éƒ¨å§¿æ€ä¼°è®¡` `é®æŒ¡å¤„ç†` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dæ‰‹éƒ¨å§¿æ€é‡å»ºæ–¹æ³•åœ¨ä¸¥é‡é®æŒ¡æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºç¼ºä¹å¯¹åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºå¯ä¾›æ€§çš„æ‰©æ•£å…ˆéªŒï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æ¨æ–­æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’çš„å¯ä¾›æ€§æè¿°ï¼ŒæŒ‡å¯¼æ‰‹éƒ¨å§¿æ€é‡å»ºã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨HOGraspNetæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ‰‹éƒ¨å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œä¼˜äºç°æœ‰å›å½’å’Œæ‰©æ•£æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆå¼å…ˆéªŒï¼Œç”¨äºåœ¨æ‰‹éƒ¨å¤§é¢ç§¯è¢«è‡ªèº«æˆ–ç‰©ä½“é®æŒ¡çš„æƒ…å†µä¸‹ï¼Œä¼˜åŒ–3Dæ‰‹éƒ¨å§¿æ€é‡å»ºã€‚è¯¥æ–¹æ³•å—åˆ°äººç±»åˆ©ç”¨ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼ˆå¦‚å¯ä¾›æ€§ï¼‰æ¥è§£å†³æ­§ä¹‰çš„å¯å‘ï¼Œå…¶ä¸­ç‰©ä½“çš„å½¢çŠ¶å’ŒåŠŸèƒ½æš—ç¤ºäº†å…¸å‹çš„æŠ“æ¡æ–¹å¼ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œå­¦ä¹ åœ¨å¯ä¾›æ€§æè¿°æ¡ä»¶ä¸‹åˆç†çš„æ‰‹éƒ¨å§¿æ€åˆ†å¸ƒï¼Œè¿™äº›æè¿°æ˜¯ä»å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­æ¨æ–­å‡ºæ¥çš„ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿå°†é®æŒ¡åŒºåŸŸä¼˜åŒ–ä¸ºæ›´å‡†ç¡®å’ŒåŠŸèƒ½è¿è´¯çš„æ‰‹éƒ¨å§¿æ€ã€‚åœ¨å…·æœ‰ä¸¥é‡é®æŒ¡çš„3Dæ‰‹éƒ¨å¯ä¾›æ€§æ•°æ®é›†HOGraspNetä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€è¿‘çš„å›å½’æ–¹æ³•å’Œç¼ºä¹ä¸Šä¸‹æ–‡æ¨ç†çš„åŸºäºæ‰©æ•£çš„ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„å¯ä¾›æ€§å¼•å¯¼ä¼˜åŒ–æ˜¾è‘—æé«˜äº†æ‰‹éƒ¨å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³3Dæ‰‹éƒ¨é‡å»ºä¸­ï¼Œç”±äºæ‰‹éƒ¨è‡ªèº«æˆ–ä¸ç‰©ä½“çš„äº¤äº’é€ æˆçš„ä¸¥é‡é®æŒ¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»é®æŒ¡æ—¶ï¼Œå¾€å¾€éš¾ä»¥å‡†ç¡®æ¨æ–­è¢«é®æŒ¡éƒ¨åˆ†çš„å§¿æ€ï¼Œå¯¼è‡´é‡å»ºç»“æœä¸å‡†ç¡®ï¼Œç¼ºä¹åŠŸèƒ½åˆç†æ€§ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ç‰©ä½“æä¾›çš„å¯ä¾›æ€§ä¿¡æ¯ï¼Œå³ç‰©ä½“å¦‚ä½•è¢«æŠ“æ¡å’Œä½¿ç”¨çš„çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç‰©ä½“æä¾›çš„å¯ä¾›æ€§ä¿¡æ¯ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼ŒæŒ‡å¯¼æ‰‹éƒ¨å§¿æ€çš„é‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»å›¾åƒä¸­æå–æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’çš„å¯ä¾›æ€§æè¿°ï¼Œç„¶ååˆ©ç”¨è¿™äº›æè¿°ä½œä¸ºæ¡ä»¶ï¼Œè®­ç»ƒä¸€ä¸ªåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œå­¦ä¹ åœ¨ç»™å®šå¯ä¾›æ€§æè¿°ä¸‹åˆç†çš„æ‰‹éƒ¨å§¿æ€åˆ†å¸ƒã€‚è¿™æ ·ï¼Œå³ä½¿æ‰‹éƒ¨è¢«é®æŒ¡ï¼Œä¹Ÿå¯ä»¥æ ¹æ®ç‰©ä½“æä¾›çš„å¯ä¾›æ€§ä¿¡æ¯ï¼Œç”Ÿæˆåˆç†çš„æ‰‹éƒ¨å§¿æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) è¾“å…¥åŒ…å«æ‰‹éƒ¨å’Œç‰©ä½“çš„å›¾åƒï¼›2) ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æå–å›¾åƒä¸­æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’çš„å¯ä¾›æ€§æè¿°ï¼›3) å°†å¯ä¾›æ€§æè¿°ä½œä¸ºæ¡ä»¶ï¼Œè¾“å…¥åˆ°åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ä¸­ï¼›4) ç”Ÿæˆæ¨¡å‹æ ¹æ®å¯ä¾›æ€§æè¿°ï¼Œé€æ­¥ä¼˜åŒ–æ‰‹éƒ¨å§¿æ€ï¼Œæœ€ç»ˆå¾—åˆ°é‡å»ºç»“æœã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒå­¦ä¹ äº†åœ¨ç»™å®šå¯ä¾›æ€§æè¿°ä¸‹åˆç†çš„æ‰‹éƒ¨å§¿æ€åˆ†å¸ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¯ä¾›æ€§ä¿¡æ¯èå…¥åˆ°æ‰‹éƒ¨å§¿æ€é‡å»ºçš„è¿‡ç¨‹ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸ä»…è€ƒè™‘äº†æ‰‹éƒ¨çš„è§†è§‰ä¿¡æ¯ï¼Œè¿˜åˆ©ç”¨äº†ç‰©ä½“æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ¨æ–­è¢«é®æŒ¡éƒ¨åˆ†çš„å§¿æ€ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´åŠ å¤šæ ·å’Œåˆç†çš„æ‰‹éƒ¨å§¿æ€ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æå–å¯ä¾›æ€§æè¿°ï¼›2) è®¾è®¡åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œå­¦ä¹ åœ¨ç»™å®šå¯ä¾›æ€§æè¿°ä¸‹åˆç†çš„æ‰‹éƒ¨å§¿æ€åˆ†å¸ƒï¼›3) ä½¿ç”¨HOGraspNetæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§é‡å…·æœ‰ä¸¥é‡é®æŒ¡çš„æ‰‹éƒ¨-ç‰©ä½“äº¤äº’å›¾åƒã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨HOGraspNetæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ‰‹éƒ¨å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰çš„å›å½’æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨MPJPEï¼ˆMean Per Joint Position Errorï¼‰æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ä¸ç¼ºä¹ä¸Šä¸‹æ–‡æ¨ç†çš„åŸºäºæ‰©æ•£çš„ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¹Ÿå–å¾—äº†æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œè¯æ˜äº†å¯ä¾›æ€§ä¿¡æ¯å¯¹äºæ‰‹éƒ¨å§¿æ€é‡å»ºçš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäººæœºäº¤äº’ã€è™šæ‹Ÿç°å®ã€æœºå™¨äººæ“ä½œç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•ç†è§£äººç±»çš„æŠ“æ¡æ„å›¾ï¼Œä»è€Œæ›´å®‰å…¨æœ‰æ•ˆåœ°ä¸äººç±»åä½œã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆæ›´é€¼çœŸçš„æ‰‹éƒ¨å§¿æ€ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å…¶ä»–äººä½“å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å…¨èº«å§¿æ€ä¼°è®¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> How can we reconstruct 3D hand poses when large portions of the hand are heavily occluded by itself or by objects? Humans often resolve such ambiguities by leveraging contextual knowledge -- such as affordances, where an object's shape and function suggest how the object is typically grasped. Inspired by this observation, we propose a generative prior for hand pose refinement guided by affordance-aware textual descriptions of hand-object interactions (HOI). Our method employs a diffusion-based generative model that learns the distribution of plausible hand poses conditioned on affordance descriptions, which are inferred from a large vision-language model (VLM). This enables the refinement of occluded regions into more accurate and functionally coherent hand poses. Extensive experiments on HOGraspNet, a 3D hand-affordance dataset with severe occlusions, demonstrate that our affordance-guided refinement significantly improves hand pose estimation over both recent regression methods and diffusion-based refinement lacking contextual reasoning.

