---
layout: default
title: Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs
---

# Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00705" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00705v1</a>
  <a href="https://arxiv.org/pdf/2510.00705.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00705v1" onclick="toggleFavorite(this, '2510.00705v1', 'Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sanghwan Kim, Rui Xiao, Stephan Alaniz, Yongqin Xian, Zeynep Akata

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å…è®­ç»ƒçš„MLLMä¸ç¡®å®šæ€§å¼•å¯¼æ¡†æ¶ï¼Œç”¨äºå¤æ‚è§†è§‰ä»»åŠ¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ä¸ç¡®å®šæ€§å¼•å¯¼` `å…è®­ç»ƒå­¦ä¹ ` `è§†è§‰æœç´¢` `é•¿è§†é¢‘ç†è§£` `æ—¶é—´å®šä½` `ç»†ç²’åº¦æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨ç»†ç²’åº¦è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”ä¾èµ–äºè€—æ—¶è€—åŠ›çš„ä»»åŠ¡ç‰¹å®šå¾®è°ƒã€‚
2. åˆ©ç”¨MLLMçš„å†…åœ¨ä¸ç¡®å®šæ€§ï¼Œè®¾è®¡ä¸€ç§å…è®­ç»ƒæ¡†æ¶ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨æœ€ç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚
3. åœ¨è§†è§‰æœç´¢ã€é•¿è§†é¢‘ç†è§£å’Œæ—¶é—´å®šä½ç­‰ä»»åŠ¡ä¸Šï¼Œå–å¾—äº†ä¸å¾®è°ƒæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨å¤„ç†ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œä¾‹å¦‚è¯†åˆ«é«˜åˆ†è¾¨ç‡å›¾åƒä¸­çš„å°ç‰©ä½“æˆ–å¯»æ‰¾é•¿è§†é¢‘ä¸­çš„å…³é”®æ—¶åˆ»ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„ã€ç‰¹å®šäºä»»åŠ¡çš„å¾®è°ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›å¹¶å¢åŠ äº†æ¨¡å‹å¤æ‚åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ã€å…è®­ç»ƒçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨MLLMçš„å†…åœ¨ä¸ç¡®å®šæ€§ä½œä¸ºä¸»åŠ¨å¼•å¯¼ä¿¡å·ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“æ¨¡å‹æ¥æ”¶åˆ°ç›¸å…³çš„è§†è§‰ä¿¡æ¯æ—¶ï¼Œå…¶è¾“å‡ºç†µä¼šé™ä½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„æœºåˆ¶ï¼Œé€šè¿‡å“åº”ä¸ç¡®å®šæ€§æ¥å¯¹å€™é€‰è§†è§‰è¾“å…¥è¿›è¡Œè¯„åˆ†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»åœ°å…³æ³¨æœ€æ˜¾è‘—çš„æ•°æ®ã€‚æˆ‘ä»¬å°†è¿™ä¸€ç®€å•çš„åŸåˆ™åº”ç”¨äºä¸‰ä¸ªå¤æ‚çš„è§†è§‰ä»»åŠ¡ï¼šè§†è§‰æœç´¢ã€é•¿è§†é¢‘ç†è§£å’Œæ—¶é—´å®šä½ï¼Œä½¿ç°æˆçš„MLLMèƒ½å¤Ÿè¾¾åˆ°ä¸ä¸“é—¨çš„ã€å¾®è°ƒæ–¹æ³•ç›¸åª²ç¾çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œéªŒè¯äº†åˆ©ç”¨å†…åœ¨ä¸ç¡®å®šæ€§æ˜¯å¢å¼ºç»†ç²’åº¦å¤šæ¨¡æ€æ€§èƒ½çš„å¼ºå¤§ä¸”é€šç”¨çš„ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šMLLMåœ¨å¤„ç†éœ€è¦ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥çš„å¤æ‚ä»»åŠ¡æ—¶ï¼Œä¾‹å¦‚åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­å®šä½å°ç‰©ä½“æˆ–åœ¨é•¿è§†é¢‘ä¸­å¯»æ‰¾å…³é”®å¸§ï¼Œè¡¨ç°ä¸ä½³ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆé€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œè¿™ä¸ä»…å¢åŠ äº†è®¡ç®—æˆæœ¬ï¼Œä¹Ÿé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡MLLMåœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼ŒåŒæ—¶é¿å…ç¹ççš„å¾®è°ƒè¿‡ç¨‹ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨MLLMè‡ªèº«çš„ä¸ç¡®å®šæ€§ä½œä¸ºä¸€ç§å¼•å¯¼ä¿¡å·ã€‚ä½œè€…è§‚å¯Ÿåˆ°ï¼Œå½“MLLMæ¥æ”¶åˆ°ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡æ¯æ—¶ï¼Œå…¶è¾“å‡ºç»“æœçš„ä¸ç¡®å®šæ€§ï¼ˆä¾‹å¦‚ï¼Œè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒçš„ç†µï¼‰ä¼šé™ä½ã€‚å› æ­¤ï¼Œå¯ä»¥é€šè¿‡è¯„ä¼°ä¸åŒè§†è§‰è¾“å…¥æ‰€å¼•èµ·çš„è¾“å‡ºä¸ç¡®å®šæ€§å˜åŒ–ï¼Œæ¥åˆ¤æ–­å“ªäº›è¾“å…¥åŒ…å«æ›´å¤šæœ‰ç”¨çš„ä¿¡æ¯ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹å…³æ³¨è¿™äº›è¾“å…¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªä¸ç¡®å®šæ€§è¯„åˆ†æ¨¡å—ï¼Œç”¨äºè¯„ä¼°ä¸åŒå€™é€‰è§†è§‰è¾“å…¥çš„è´¨é‡ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š1) ç»™å®šä¸€ä¸ªä»»åŠ¡å’Œä¸€ç»„å€™é€‰è§†è§‰è¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒçš„ä¸åŒåŒºåŸŸæˆ–è§†é¢‘çš„ä¸åŒç‰‡æ®µï¼‰ï¼›2) å°†æ¯ä¸ªå€™é€‰è¾“å…¥ä¸ä»»åŠ¡æè¿°ä¸€èµ·è¾“å…¥åˆ°MLLMä¸­ï¼Œå¾—åˆ°ç›¸åº”çš„è¾“å‡ºï¼›3) è®¡ç®—æ¯ä¸ªè¾“å‡ºçš„ä¸ç¡®å®šæ€§å¾—åˆ†ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡è®¡ç®—è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒçš„ç†µï¼‰ï¼›4) æ ¹æ®ä¸ç¡®å®šæ€§å¾—åˆ†å¯¹å€™é€‰è¾“å…¥è¿›è¡Œæ’åºï¼Œé€‰æ‹©å¾—åˆ†æœ€ä½ï¼ˆå³æœ€ç¡®å®šï¼‰çš„è¾“å…¥ï¼›5) å°†é€‰å®šçš„è¾“å…¥ç”¨äºåç»­çš„ä»»åŠ¡å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€å…³é”®çš„åˆ›æ–°åœ¨äºï¼Œå®ƒæ— éœ€ä»»ä½•è®­ç»ƒæˆ–å¾®è°ƒï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨äº†MLLMå›ºæœ‰çš„ä¸ç¡®å®šæ€§ä¿¡æ¯ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯ä»¥åº”ç”¨äºå„ç§ä¸åŒçš„è§†è§‰ä»»åŠ¡å’ŒMLLMæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå³å¦‚ä½•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ä¸ç¡®å®šæ€§æ¥æå‡å…¶æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°è®¡ç®—è¾“å‡ºçš„ä¸ç¡®å®šæ€§ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒçš„ç†µä½œä¸ºä¸ç¡®å®šæ€§çš„åº¦é‡ï¼Œä½†ä¹Ÿå¯ä»¥å°è¯•å…¶ä»–åº¦é‡æ–¹å¼ï¼Œä¾‹å¦‚æ–¹å·®æˆ–äº’ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„å€™é€‰è§†è§‰è¾“å…¥ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„è®¾è®¡è€ƒè™‘å› ç´ ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰æœç´¢ä»»åŠ¡ä¸­ï¼Œå¯ä»¥é€‰æ‹©å›¾åƒçš„ä¸åŒåŒºåŸŸä½œä¸ºå€™é€‰è¾“å…¥ï¼›åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼Œå¯ä»¥é€‰æ‹©è§†é¢‘çš„ä¸åŒç‰‡æ®µä½œä¸ºå€™é€‰è¾“å…¥ã€‚å€™é€‰è¾“å…¥çš„é€‰æ‹©ç­–ç•¥ä¼šç›´æ¥å½±å“æœ€ç»ˆçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰æœç´¢ã€é•¿è§†é¢‘ç†è§£å’Œæ—¶é—´å®šä½ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç”šè‡³å¯ä»¥ä¸ç»è¿‡ä¸“é—¨å¾®è°ƒçš„æ¨¡å‹ç›¸åª²ç¾ã€‚ä¾‹å¦‚ï¼Œåœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¸è¿›è¡Œä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°±è¾¾åˆ°äº†ä¸å¾®è°ƒæ¨¡å‹ç›¸å½“çš„å‡†ç¡®ç‡ã€‚è¿™å……åˆ†éªŒè¯äº†åˆ©ç”¨MLLMå†…åœ¨ä¸ç¡®å®šæ€§è¿›è¡Œå¼•å¯¼çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥çš„é¢†åŸŸï¼Œå¦‚æ™ºèƒ½ç›‘æ§ï¼ˆå¿«é€Ÿå®šä½å¼‚å¸¸äº‹ä»¶ï¼‰ã€è‡ªåŠ¨é©¾é©¶ï¼ˆè¯†åˆ«äº¤é€šæ ‡å¿—å’Œè¡Œäººï¼‰ã€åŒ»ç–—å½±åƒåˆ†æï¼ˆæ£€æµ‹ç—…ç¶åŒºåŸŸï¼‰ç­‰ã€‚é€šè¿‡æå‡MLLMåœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¯ä»¥é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŠ é€Ÿç›¸å…³æŠ€æœ¯çš„è½åœ°åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) often struggle with fine-grained perception, such as identifying small objects in high-resolution images or finding key moments in long videos. Existing works typically rely on complicated, task-specific fine-tuning, which limits their generalizability and increases model complexity. In this work, we propose an effective, training-free framework that uses an MLLM's intrinsic uncertainty as a proactive guidance signal. Our core insight is that a model's output entropy decreases when presented with relevant visual information. We introduce a unified mechanism that scores candidate visual inputs by response uncertainty, enabling the model to autonomously focus on the most salient data. We apply this simple principle to three complex visual tasks: Visual Search, Long Video Understanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieve performance competitive with specialized, fine-tuned methods. Our work validates that harnessing intrinsic uncertainty is a powerful, general strategy for enhancing fine-grained multimodal performance.

