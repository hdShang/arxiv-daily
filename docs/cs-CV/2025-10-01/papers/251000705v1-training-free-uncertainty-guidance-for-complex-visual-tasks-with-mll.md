---
layout: default
title: Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs
---

# Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.00705" target="_blank" class="toolbar-btn">arXiv: 2510.00705v1</a>
    <a href="https://arxiv.org/pdf/2510.00705.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00705v1" 
            onclick="toggleFavorite(this, '2510.00705v1', 'Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Sanghwan Kim, Rui Xiao, Stephan Alaniz, Yongqin Xian, Zeynep Akata

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-01

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÂÖçËÆ≠ÁªÉÁöÑMLLM‰∏çÁ°ÆÂÆöÊÄßÂºïÂØºÊ°ÜÊû∂ÔºåÁî®‰∫éÂ§çÊùÇËßÜËßâ‰ªªÂä°„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `‰∏çÁ°ÆÂÆöÊÄßÂºïÂØº` `ÂÖçËÆ≠ÁªÉÂ≠¶‰π†` `ËßÜËßâÊêúÁ¥¢` `ÈïøËßÜÈ¢ëÁêÜËß£` `Êó∂Èó¥ÂÆö‰Ωç` `ÁªÜÁ≤íÂ∫¶ÊÑüÁü•`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMÂú®ÁªÜÁ≤íÂ∫¶ËßÜËßâ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥Ôºå‰∏î‰æùËµñ‰∫éËÄóÊó∂ËÄóÂäõÁöÑ‰ªªÂä°ÁâπÂÆöÂæÆË∞É„ÄÇ
2. Âà©Áî®MLLMÁöÑÂÜÖÂú®‰∏çÁ°ÆÂÆöÊÄßÔºåËÆæËÆ°‰∏ÄÁßçÂÖçËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂºïÂØºÊ®°ÂûãÂÖ≥Ê≥®ÊúÄÁõ∏ÂÖ≥ÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÇ
3. Âú®ËßÜËßâÊêúÁ¥¢„ÄÅÈïøËßÜÈ¢ëÁêÜËß£ÂíåÊó∂Èó¥ÂÆö‰ΩçÁ≠â‰ªªÂä°‰∏äÔºåÂèñÂæó‰∫Ü‰∏éÂæÆË∞ÉÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLM)Âú®Â§ÑÁêÜÁªÜÁ≤íÂ∫¶ÊÑüÁü•‰ªªÂä°Êó∂Â∏∏Â∏∏ÈÅáÂà∞Âõ∞ÈöæÔºå‰æãÂ¶ÇËØÜÂà´È´òÂàÜËæ®ÁéáÂõæÂÉè‰∏≠ÁöÑÂ∞èÁâ©‰ΩìÊàñÂØªÊâæÈïøËßÜÈ¢ë‰∏≠ÁöÑÂÖ≥ÈîÆÊó∂Âàª„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂ§çÊùÇÁöÑ„ÄÅÁâπÂÆö‰∫é‰ªªÂä°ÁöÑÂæÆË∞ÉÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÊ≥õÂåñËÉΩÂäõÂπ∂Â¢ûÂä†‰∫ÜÊ®°ÂûãÂ§çÊùÇÂ∫¶„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑ„ÄÅÂÖçËÆ≠ÁªÉÁöÑÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âà©Áî®MLLMÁöÑÂÜÖÂú®‰∏çÁ°ÆÂÆöÊÄß‰Ωú‰∏∫‰∏ªÂä®ÂºïÂØº‰ø°Âè∑„ÄÇÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÔºåÂΩìÊ®°ÂûãÊé•Êî∂Âà∞Áõ∏ÂÖ≥ÁöÑËßÜËßâ‰ø°ÊÅØÊó∂ÔºåÂÖ∂ËæìÂá∫ÁÜµ‰ºöÈôç‰Ωé„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÊú∫Âà∂ÔºåÈÄöËøáÂìçÂ∫î‰∏çÁ°ÆÂÆöÊÄßÊù•ÂØπÂÄôÈÄâËßÜËßâËæìÂÖ•ËøõË°åËØÑÂàÜÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üËá™‰∏ªÂú∞ÂÖ≥Ê≥®ÊúÄÊòæËëóÁöÑÊï∞ÊçÆ„ÄÇÊàë‰ª¨Â∞ÜËøô‰∏ÄÁÆÄÂçïÁöÑÂéüÂàôÂ∫îÁî®‰∫é‰∏â‰∏™Â§çÊùÇÁöÑËßÜËßâ‰ªªÂä°ÔºöËßÜËßâÊêúÁ¥¢„ÄÅÈïøËßÜÈ¢ëÁêÜËß£ÂíåÊó∂Èó¥ÂÆö‰ΩçÔºå‰ΩøÁé∞ÊàêÁöÑMLLMËÉΩÂ§üËææÂà∞‰∏é‰∏ìÈó®ÁöÑ„ÄÅÂæÆË∞ÉÊñπÊ≥ïÁõ∏Â™≤ÁæéÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰ΩúÈ™åËØÅ‰∫ÜÂà©Áî®ÂÜÖÂú®‰∏çÁ°ÆÂÆöÊÄßÊòØÂ¢ûÂº∫ÁªÜÁ≤íÂ∫¶Â§öÊ®°ÊÄÅÊÄßËÉΩÁöÑÂº∫Â§ß‰∏îÈÄöÁî®ÁöÑÁ≠ñÁï•„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöMLLMÂú®Â§ÑÁêÜÈúÄË¶ÅÁªÜÁ≤íÂ∫¶ËßÜËßâÊÑüÁü•ÁöÑÂ§çÊùÇ‰ªªÂä°Êó∂Ôºå‰æãÂ¶ÇÂú®È´òÂàÜËæ®ÁéáÂõæÂÉè‰∏≠ÂÆö‰ΩçÂ∞èÁâ©‰ΩìÊàñÂú®ÈïøËßÜÈ¢ë‰∏≠ÂØªÊâæÂÖ≥ÈîÆÂ∏ßÔºåË°®Áé∞‰∏ç‰Ω≥„ÄÇÁé∞ÊúâÁöÑËß£ÂÜ≥ÊñπÊ°àÈÄöÂ∏∏ÈúÄË¶ÅÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åÂæÆË∞ÉÔºåËøô‰∏ç‰ªÖÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨Ôºå‰πüÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊèêÂçáMLLMÂú®Â§çÊùÇËßÜËßâ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÈÅøÂÖçÁπÅÁêêÁöÑÂæÆË∞ÉËøáÁ®ãÔºåÊòØ‰∏Ä‰∏™‰∫üÂæÖËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®MLLMËá™Ë∫´ÁöÑ‰∏çÁ°ÆÂÆöÊÄß‰Ωú‰∏∫‰∏ÄÁßçÂºïÂØº‰ø°Âè∑„ÄÇ‰ΩúËÄÖËßÇÂØüÂà∞ÔºåÂΩìMLLMÊé•Êî∂Âà∞‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜËßâ‰ø°ÊÅØÊó∂ÔºåÂÖ∂ËæìÂá∫ÁªìÊûúÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºà‰æãÂ¶ÇÔºåËæìÂá∫Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÁÜµÔºâ‰ºöÈôç‰Ωé„ÄÇÂõ†Ê≠§ÔºåÂèØ‰ª•ÈÄöËøáËØÑ‰º∞‰∏çÂêåËßÜËßâËæìÂÖ•ÊâÄÂºïËµ∑ÁöÑËæìÂá∫‰∏çÁ°ÆÂÆöÊÄßÂèòÂåñÔºåÊù•Âà§Êñ≠Âì™‰∫õËæìÂÖ•ÂåÖÂê´Êõ¥Â§öÊúâÁî®ÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÂºïÂØºÊ®°ÂûãÂÖ≥Ê≥®Ëøô‰∫õËæìÂÖ•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™‰∏çÁ°ÆÂÆöÊÄßËØÑÂàÜÊ®°ÂùóÔºåÁî®‰∫éËØÑ‰º∞‰∏çÂêåÂÄôÈÄâËßÜËßâËæìÂÖ•ÁöÑË¥®Èáè„ÄÇÂÖ∑‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) ÁªôÂÆö‰∏Ä‰∏™‰ªªÂä°Âíå‰∏ÄÁªÑÂÄôÈÄâËßÜËßâËæìÂÖ•Ôºà‰æãÂ¶ÇÔºåÂõæÂÉèÁöÑ‰∏çÂêåÂå∫ÂüüÊàñËßÜÈ¢ëÁöÑ‰∏çÂêåÁâáÊÆµÔºâÔºõ2) Â∞ÜÊØè‰∏™ÂÄôÈÄâËæìÂÖ•‰∏é‰ªªÂä°ÊèèËø∞‰∏ÄËµ∑ËæìÂÖ•Âà∞MLLM‰∏≠ÔºåÂæóÂà∞Áõ∏Â∫îÁöÑËæìÂá∫Ôºõ3) ËÆ°ÁÆóÊØè‰∏™ËæìÂá∫ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂæóÂàÜÔºà‰æãÂ¶ÇÔºåÈÄöËøáËÆ°ÁÆóËæìÂá∫Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÁÜµÔºâÔºõ4) Ê†πÊçÆ‰∏çÁ°ÆÂÆöÊÄßÂæóÂàÜÂØπÂÄôÈÄâËæìÂÖ•ËøõË°åÊéíÂ∫èÔºåÈÄâÊã©ÂæóÂàÜÊúÄ‰ΩéÔºàÂç≥ÊúÄÁ°ÆÂÆöÔºâÁöÑËæìÂÖ•Ôºõ5) Â∞ÜÈÄâÂÆöÁöÑËæìÂÖ•Áî®‰∫éÂêéÁª≠ÁöÑ‰ªªÂä°Â§ÑÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÂÖ≥ÈîÆÁöÑÂàõÊñ∞Âú®‰∫éÔºåÂÆÉÊó†ÈúÄ‰ªª‰ΩïËÆ≠ÁªÉÊàñÂæÆË∞ÉÔºåËÄåÊòØÁõ¥Êé•Âà©Áî®‰∫ÜMLLMÂõ∫ÊúâÁöÑ‰∏çÁ°ÆÂÆöÊÄß‰ø°ÊÅØ„ÄÇËøô‰ΩøÂæóËØ•ÊñπÊ≥ïÂÖ∑ÊúâÂæàÂº∫ÁöÑÈÄöÁî®ÊÄßÂíåÂèØÊâ©Â±ïÊÄßÔºåÂèØ‰ª•Â∫îÁî®‰∫éÂêÑÁßç‰∏çÂêåÁöÑËßÜËßâ‰ªªÂä°ÂíåMLLMÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßíÔºåÂç≥Â¶Ç‰ΩïÂà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÊù•ÊèêÂçáÂÖ∂ÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂ¶Ç‰ΩïÊúâÊïàÂú∞ËÆ°ÁÆóËæìÂá∫ÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇËÆ∫Êñá‰∏≠‰ΩøÁî®‰∫ÜËæìÂá∫Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÁÜµ‰Ωú‰∏∫‰∏çÁ°ÆÂÆöÊÄßÁöÑÂ∫¶ÈáèÔºå‰ΩÜ‰πüÂèØ‰ª•Â∞ùËØïÂÖ∂‰ªñÂ∫¶ÈáèÊñπÂºèÔºå‰æãÂ¶ÇÊñπÂ∑ÆÊàñ‰∫í‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåÂ¶Ç‰ΩïÈÄâÊã©ÂêàÈÄÇÁöÑÂÄôÈÄâËßÜËßâËæìÂÖ•‰πüÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑËÆæËÆ°ËÄÉËôëÂõ†Á¥†„ÄÇ‰æãÂ¶ÇÔºåÂú®ËßÜËßâÊêúÁ¥¢‰ªªÂä°‰∏≠ÔºåÂèØ‰ª•ÈÄâÊã©ÂõæÂÉèÁöÑ‰∏çÂêåÂå∫Âüü‰Ωú‰∏∫ÂÄôÈÄâËæìÂÖ•ÔºõÂú®ÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°‰∏≠ÔºåÂèØ‰ª•ÈÄâÊã©ËßÜÈ¢ëÁöÑ‰∏çÂêåÁâáÊÆµ‰Ωú‰∏∫ÂÄôÈÄâËæìÂÖ•„ÄÇÂÄôÈÄâËæìÂÖ•ÁöÑÈÄâÊã©Á≠ñÁï•‰ºöÁõ¥Êé•ÂΩ±ÂìçÊúÄÁªàÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ËßÜËßâÊêúÁ¥¢„ÄÅÈïøËßÜÈ¢ëÁêÜËß£ÂíåÊó∂Èó¥ÂÆö‰ΩçÁ≠â‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁîöËá≥ÂèØ‰ª•‰∏éÁªèËøá‰∏ìÈó®ÂæÆË∞ÉÁöÑÊ®°ÂûãÁõ∏Â™≤Áæé„ÄÇ‰æãÂ¶ÇÔºåÂú®ÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°‰∏≠ÔºåËØ•ÊñπÊ≥ïÂú®‰∏çËøõË°å‰ªª‰ΩïËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞±ËææÂà∞‰∫Ü‰∏éÂæÆË∞ÉÊ®°ÂûãÁõ∏ÂΩìÁöÑÂáÜÁ°ÆÁéá„ÄÇËøôÂÖÖÂàÜÈ™åËØÅ‰∫ÜÂà©Áî®MLLMÂÜÖÂú®‰∏çÁ°ÆÂÆöÊÄßËøõË°åÂºïÂØºÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÈúÄË¶ÅÁªÜÁ≤íÂ∫¶ËßÜËßâÊÑüÁü•ÁöÑÈ¢ÜÂüüÔºåÂ¶ÇÊô∫ËÉΩÁõëÊéßÔºàÂø´ÈÄüÂÆö‰ΩçÂºÇÂ∏∏‰∫ã‰ª∂Ôºâ„ÄÅËá™Âä®È©æÈ©∂ÔºàËØÜÂà´‰∫§ÈÄöÊ†áÂøóÂíåË°å‰∫∫Ôºâ„ÄÅÂåªÁñóÂΩ±ÂÉèÂàÜÊûêÔºàÊ£ÄÊµãÁóÖÁÅ∂Âå∫ÂüüÔºâÁ≠â„ÄÇÈÄöËøáÊèêÂçáMLLMÂú®Â§çÊùÇËßÜËßâ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºåÂèØ‰ª•Èôç‰ΩéÂØπ‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñÔºåÂä†ÈÄüÁõ∏ÂÖ≥ÊäÄÊúØÁöÑËêΩÂú∞Â∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal Large Language Models (MLLMs) often struggle with fine-grained perception, such as identifying small objects in high-resolution images or finding key moments in long videos. Existing works typically rely on complicated, task-specific fine-tuning, which limits their generalizability and increases model complexity. In this work, we propose an effective, training-free framework that uses an MLLM's intrinsic uncertainty as a proactive guidance signal. Our core insight is that a model's output entropy decreases when presented with relevant visual information. We introduce a unified mechanism that scores candidate visual inputs by response uncertainty, enabling the model to autonomously focus on the most salient data. We apply this simple principle to three complex visual tasks: Visual Search, Long Video Understanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieve performance competitive with specialized, fine-tuned methods. Our work validates that harnessing intrinsic uncertainty is a powerful, general strategy for enhancing fine-grained multimodal performance.

