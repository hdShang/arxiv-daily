---
layout: default
title: Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning
---

# Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00570" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00570v1</a>
  <a href="https://arxiv.org/pdf/2510.00570.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00570v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00570v1', 'Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minghao Yang, Ren Togo, Guang Li, Takahiro Ogawa, Miki Haseyama

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLoRAçš„è‡ªé€‚åº”å…±äº«ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œæå‡å¤šä»»åŠ¡å­¦ä¹ æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å¤šä»»åŠ¡å­¦ä¹ ` `æ··åˆä¸“å®¶æ¨¡å‹` `ä½ç§©é€‚åº”` `çŸ¥è¯†å…±äº«` `è‡ªé€‚åº”å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MoE-MTLæ–¹æ³•ä¾èµ–å•ä»»åŠ¡é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œï¼ŒSTLåˆ°MTLè¿‡æ¸¡æ—¶å­˜åœ¨å†—ä½™é€‚åº”å’ŒçŸ¥è¯†å…±äº«æ•ˆç‡ä½çš„é—®é¢˜ã€‚
2. æå‡ºè‡ªé€‚åº”å…±äº«ä¸“å®¶(ASE)æ¡†æ¶ï¼Œç»“åˆLoRAå’ŒMoEï¼Œé€šè¿‡å…±äº«ä¸“å®¶å’Œç»†ç²’åº¦ä¸“å®¶è®¾è®¡ï¼Œä¿ƒè¿›çŸ¥è¯†å…±äº«å’Œä¸“å®¶åä½œã€‚
3. åœ¨PASCAL-Contextæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒASEåœ¨å¤šç§é…ç½®ä¸‹å‡èƒ½æå‡æ€§èƒ½ï¼ŒéªŒè¯äº†ç»†ç²’åº¦ä¸“å®¶è®¾è®¡åœ¨MTLä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶æ¨¡å‹(MoE)å·²æˆä¸ºå¤šä»»åŠ¡å­¦ä¹ (MTL)çš„å¼ºå¤§æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MoE-MTLæ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä»»åŠ¡é¢„è®­ç»ƒçš„éª¨å¹²ç½‘ç»œï¼Œå¹¶ä¸”åœ¨ä»å•ä»»åŠ¡å­¦ä¹ (STL)è¿‡æ¸¡åˆ°å¤šä»»åŠ¡å­¦ä¹ (MTL)æœŸé—´å­˜åœ¨å†—ä½™é€‚åº”å’Œä½æ•ˆçš„çŸ¥è¯†å…±äº«ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä½ç§©é€‚åº”(LoRA)çš„MoEä¸­çš„è‡ªé€‚åº”å…±äº«ä¸“å®¶(ASE)ï¼Œå…¶ä¸­å…±äº«ä¸“å®¶è¢«åˆ†é…ç”±è·¯ç”±å™¨è®¡ç®—çš„é—¨æ§æƒé‡ï¼Œè¿™äº›æƒé‡ä¸ç¨€ç–ä¸“å®¶è”åˆå½’ä¸€åŒ–ã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºSTLåˆ°MTLçš„è¿‡æ¸¡ï¼Œå¢å¼ºäº†ä¸“å®¶çš„ä¸“ä¸šåŒ–å’Œåä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¢åŠ LoRAä¸“å®¶çš„æ•°é‡å¹¶æŒ‰æ¯”ä¾‹é™ä½å…¶ç§©æ¥æ•´åˆç»†ç²’åº¦ä¸“å®¶ï¼Œä»è€Œåœ¨å¯æ¯”çš„å‚æ•°é¢„ç®—ä¸‹å®ç°æ›´æœ‰æ•ˆçš„çŸ¥è¯†å…±äº«ã€‚åœ¨ç»Ÿä¸€çš„è®­ç»ƒè®¾ç½®ä¸‹ï¼Œåœ¨PASCAL-ContextåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒASEåœ¨ä¸åŒçš„é…ç½®ä¸­å§‹ç»ˆæé«˜æ€§èƒ½ï¼Œå¹¶éªŒè¯äº†ç»†ç²’åº¦è®¾è®¡å¯¹äºMTLçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šä»»åŠ¡å­¦ä¹ æ··åˆä¸“å®¶æ¨¡å‹(MoE-MTL)æ–¹æ³•é€šå¸¸åŸºäºå•ä»»åŠ¡é¢„è®­ç»ƒçš„éª¨å¹²ç½‘ç»œï¼Œè¿™å¯¼è‡´ä»å•ä»»åŠ¡å­¦ä¹ (STL)åˆ°å¤šä»»åŠ¡å­¦ä¹ (MTL)çš„è¿‡æ¸¡è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹éœ€è¦è¿›è¡Œå¤§é‡çš„å†—ä½™å‚æ•°è°ƒæ•´ï¼Œå¹¶ä¸”çŸ¥è¯†å…±äº«æ•ˆç‡ä½ä¸‹ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨ä¸åŒä»»åŠ¡ä¹‹é—´çš„å…³è”æ€§ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§è‡ªé€‚åº”å…±äº«ä¸“å®¶(ASE)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºä½ç§©é€‚åº”(LoRA)çš„MoEç»“æ„ã€‚é€šè¿‡å¼•å…¥å…±äº«ä¸“å®¶ï¼Œå¹¶ä½¿ç”¨è·¯ç”±å™¨è®¡ç®—çš„é—¨æ§æƒé‡è¿›è¡Œè‡ªé€‚åº”åˆ†é…ï¼Œä¿ƒè¿›ä¸åŒä»»åŠ¡ä¹‹é—´çš„çŸ¥è¯†å…±äº«å’Œä¸“å®¶åä½œã€‚åŒæ—¶ï¼Œé€šè¿‡å¢åŠ LoRAä¸“å®¶çš„æ•°é‡å¹¶é™ä½å…¶ç§©ï¼Œå®ç°ç»†ç²’åº¦çš„ä¸“å®¶è®¾è®¡ï¼Œè¿›ä¸€æ­¥æå‡çŸ¥è¯†å…±äº«çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å•ä»»åŠ¡é¢„è®­ç»ƒçš„éª¨å¹²ç½‘ç»œï¼›2) åŸºäºLoRAçš„ä¸“å®¶å±‚ï¼ŒåŒ…å«å…±äº«ä¸“å®¶å’Œç¨€ç–ä¸“å®¶ï¼›3) è·¯ç”±å™¨ï¼Œç”¨äºè®¡ç®—æ¯ä¸ªä»»åŠ¡å¯¹ä¸åŒä¸“å®¶çš„é—¨æ§æƒé‡ï¼›4) è”åˆå½’ä¸€åŒ–å±‚ï¼Œç”¨äºå¯¹å…±äº«ä¸“å®¶å’Œç¨€ç–ä¸“å®¶çš„æƒé‡è¿›è¡Œå½’ä¸€åŒ–ï¼›5) ä»»åŠ¡ç‰¹å®šçš„è¾“å‡ºå±‚ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬STLé˜¶æ®µå’ŒMTLé˜¶æ®µï¼Œåœ¨MTLé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´ä¸“å®¶æƒé‡ï¼Œå®ç°çŸ¥è¯†å…±äº«å’Œä¸“å®¶åä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé€‚åº”å…±äº«ä¸“å®¶(ASE)çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶ä¸LoRAå’ŒMoEç›¸ç»“åˆã€‚ä¸ä¼ ç»Ÿçš„MoE-MTLæ–¹æ³•ç›¸æ¯”ï¼ŒASEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸åŒä»»åŠ¡ä¹‹é—´çš„å…³è”æ€§ï¼Œå‡å°‘å†—ä½™å‚æ•°è°ƒæ•´ï¼Œå¹¶æå‡çŸ¥è¯†å…±äº«çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç»†ç²’åº¦ä¸“å®¶è®¾è®¡ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨å¯æ¯”çš„å‚æ•°é¢„ç®—ä¸‹ï¼Œæ‹¥æœ‰æ›´å¤šçš„ä¸“å®¶ï¼Œä»è€Œå®ç°æ›´ç²¾ç»†çš„çŸ¥è¯†è¡¨ç¤ºå’Œå…±äº«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LoRAä¸“å®¶å±‚ä¸­ï¼Œä¸“å®¶æ•°é‡å’ŒLoRAç§©æ˜¯ä¸¤ä¸ªå…³é”®çš„å‚æ•°ã€‚è®ºæ–‡é€šè¿‡å®éªŒåˆ†æäº†ä¸åŒä¸“å®¶æ•°é‡å’ŒLoRAç§©å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ¯”ä¾‹ç¼©æ”¾ç­–ç•¥ï¼Œå³å¢åŠ ä¸“å®¶æ•°é‡çš„åŒæ—¶ï¼ŒæŒ‰æ¯”ä¾‹é™ä½LoRAç§©ï¼Œä»¥ä¿æŒå‚æ•°é¢„ç®—ä¸å˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªè”åˆå½’ä¸€åŒ–å±‚ï¼Œç”¨äºå¯¹å…±äº«ä¸“å®¶å’Œç¨€ç–ä¸“å®¶çš„æƒé‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥ç¡®ä¿ä¸åŒä¸“å®¶ä¹‹é—´çš„æƒé‡åˆ†å¸ƒå¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨PASCAL-Contextæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒASEåœ¨å¤šç§é…ç½®ä¸‹å‡ä¼˜äºç°æœ‰çš„MoE-MTLæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šé…ç½®ä¸‹ï¼ŒASEç›¸æ¯”äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨å¹³å‡IoUæŒ‡æ ‡ä¸Šæå‡äº†2-3ä¸ªç™¾åˆ†ç‚¹ã€‚å®éªŒè¿˜éªŒè¯äº†ç»†ç²’åº¦ä¸“å®¶è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œå³åœ¨ç›¸åŒå‚æ•°é¢„ç®—ä¸‹ï¼Œå¢åŠ ä¸“å®¶æ•°é‡å¹¶é™ä½LoRAç§©å¯ä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ä¸­çš„æ„ŸçŸ¥ä»»åŠ¡ï¼ˆç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ï¼‰ã€åŒ»ç–—å›¾åƒåˆ†æï¼ˆç–¾ç—…è¯Šæ–­ã€å™¨å®˜åˆ†å‰²ç­‰ï¼‰ä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤šè¯­è¨€ç¿»è¯‘ç­‰ã€‚é€šè¿‡æå‡å¤šä»»åŠ¡å­¦ä¹ çš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œå¯ä»¥é™ä½æ¨¡å‹å¼€å‘æˆæœ¬ï¼Œæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ½œåœ¨çš„å•†ä¸šå‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task learning (MTL). However, existing MoE-MTL methods often rely on single-task pretrained backbones and suffer from redundant adaptation and inefficient knowledge sharing during the transition from single-task to multi-task learning (STL to MTL). To address these limitations, we propose adaptive shared experts (ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are assigned router-computed gating weights jointly normalized with sparse experts. This design facilitates STL to MTL transition, enhances expert specialization, and cooperation. Furthermore, we incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank, enabling more effective knowledge sharing under a comparable parameter budget. Extensive experiments on the PASCAL-Context benchmark, under unified training settings, demonstrate that ASE consistently improves performance across diverse configurations and validates the effectiveness of fine-grained designs for MTL.

