---
layout: default
title: Graph Integrated Multimodal Concept Bottleneck Model
---

# Graph Integrated Multimodal Concept Bottleneck Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00701" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00701v1</a>
  <a href="https://arxiv.org/pdf/2510.00701.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00701v1" onclick="toggleFavorite(this, '2510.00701v1', 'Graph Integrated Multimodal Concept Bottleneck Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiakai Lin, Jinchang Zhang, Guoyu Lu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoE-SGTï¼Œé€šè¿‡å›¾Transformerå’Œæ··åˆä¸“å®¶æ¨¡å‹å¢å¼ºå¤šæ¨¡æ€æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼Œæå‡å¤æ‚æ¦‚å¿µæ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¦‚å¿µç“¶é¢ˆæ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `å›¾Transformer` `æ··åˆä¸“å®¶æ¨¡å‹` `å¯è§£é‡Šæ€§` `çŸ¥è¯†å›¾è°±` `è§†è§‰é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šå¸¸æ˜¯å•æ¨¡æ€çš„ï¼Œå¿½ç•¥äº†æ¦‚å¿µä¹‹é—´çš„ç»“æ„åŒ–å…³ç³»ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚
2. MoE-SGTé€šè¿‡å¼•å…¥å›¾Transformerå’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—ï¼Œæ˜¾å¼å»ºæ¨¡æ¦‚å¿µé—´çš„ç»“æ„åŒ–å…³ç³»ï¼Œå¹¶åŠ¨æ€åˆ†é…æ¨ç†ä»»åŠ¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMoE-SGTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–æ¦‚å¿µç“¶é¢ˆç½‘ç»œï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚æ¦‚å¿µæ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†æ»¡è¶³æ·±åº¦å­¦ä¹ ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é£é™©é¢†åŸŸä¸­å¯¹å¯è§£é‡Šæ€§çš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡å°†äººç±»å¯ç†è§£çš„æ¦‚å¿µæ’å…¥åˆ°é¢„æµ‹æµç¨‹ä¸­æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯å•æ¨¡æ€çš„ï¼Œå¹¶ä¸”å¿½ç•¥äº†ç»“æ„åŒ–çš„æ¦‚å¿µå…³ç³»ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†MoE-SGTï¼Œä¸€ä¸ªæ¨ç†é©±åŠ¨çš„æ¡†æ¶ï¼Œå®ƒä½¿ç”¨ç»“æ„æ³¨å…¥å›¾Transformerå’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—æ¥å¢å¼ºCBMsã€‚æˆ‘ä»¬æ„å»ºäº†å¤šæ¨¡æ€è¾“å…¥çš„ç­”æ¡ˆ-æ¦‚å¿µå’Œç­”æ¡ˆ-é—®é¢˜å›¾ï¼Œä»¥æ˜¾å¼åœ°å»ºæ¨¡æ¦‚å¿µä¹‹é—´çš„ç»“æ„åŒ–å…³ç³»ã€‚éšåï¼Œæˆ‘ä»¬é›†æˆå›¾Transformeræ¥æ•è·å¤šå±‚æ¬¡çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œè§£å†³äº†ä¼ ç»Ÿæ¦‚å¿µç“¶é¢ˆæ¨¡å‹åœ¨å»ºæ¨¡æ¦‚å¿µäº¤äº’æ–¹é¢çš„å±€é™æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶åœ¨é€‚åº”å¤æ‚çš„æ¦‚å¿µæ¨¡å¼æ—¶é‡åˆ°ç“¶é¢ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—æ›¿æ¢äº†å‰é¦ˆå±‚ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å­¦ä¹ ä¸åŒçš„æ¦‚å¿µå…³ç³»æ–¹é¢å…·æœ‰æ›´å¤§çš„èƒ½åŠ›ï¼ŒåŒæ—¶å°†æ¨ç†ä»»åŠ¡åŠ¨æ€åœ°åˆ†é…ç»™ä¸åŒçš„å­ä¸“å®¶ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å¤æ‚æ¦‚å¿µæ¨ç†çš„é€‚åº”æ€§ã€‚é€šè¿‡å¯¹æ¦‚å¿µä¹‹é—´çš„ç»“æ„åŒ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼ŒMoE-SGTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ¯”å…¶ä»–æ¦‚å¿µç“¶é¢ˆç½‘ç»œæ›´é«˜çš„å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶ï¼Œæ— æ³•æœ‰æ•ˆå»ºæ¨¡æ¦‚å¿µä¹‹é—´ç»“æ„åŒ–å…³ç³»çš„é—®é¢˜ã€‚ä¼ ç»ŸCBMsé€šå¸¸æ˜¯å•æ¨¡æ€çš„ï¼Œå¿½ç•¥äº†æ¦‚å¿µé—´çš„ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºå›¾ç»“æ„æ¥æ˜¾å¼åœ°å»ºæ¨¡æ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åˆ©ç”¨å›¾Transformeræ¥æ•è·å¤šå±‚æ¬¡çš„ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼Œå¼•å…¥æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—æ¥å¢å¼ºæ¨¡å‹å¯¹å¤æ‚æ¦‚å¿µæ¨¡å¼çš„é€‚åº”æ€§ï¼Œé€šè¿‡åŠ¨æ€åˆ†é…æ¨ç†ä»»åŠ¡ç»™ä¸åŒçš„å­ä¸“å®¶ï¼Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoE-SGTæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å¤šæ¨¡æ€è¾“å…¥å¤„ç†æ¨¡å—ï¼Œç”¨äºæå–ç­”æ¡ˆã€æ¦‚å¿µå’Œé—®é¢˜ç­‰ä¿¡æ¯ï¼›2) å›¾æ„å»ºæ¨¡å—ï¼Œç”¨äºæ„å»ºç­”æ¡ˆ-æ¦‚å¿µå›¾å’Œç­”æ¡ˆ-é—®é¢˜å›¾ï¼Œæ˜¾å¼å»ºæ¨¡æ¦‚å¿µé—´çš„å…³ç³»ï¼›3) å›¾Transformeræ¨¡å—ï¼Œç”¨äºæ•è·å›¾ç»“æ„ä¸­çš„å¤šå±‚æ¬¡ä¾èµ–å…³ç³»ï¼›4) æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—ï¼Œç”¨äºåŠ¨æ€åˆ†é…æ¨ç†ä»»åŠ¡ç»™ä¸åŒçš„å­ä¸“å®¶ï¼›5) é¢„æµ‹æ¨¡å—ï¼Œç”¨äºæ ¹æ®å­¦ä¹ åˆ°çš„æ¦‚å¿µè¡¨ç¤ºè¿›è¡Œæœ€ç»ˆé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§åŸºäºå›¾ç»“æ„çš„CBMså¢å¼ºæ¡†æ¶ï¼Œèƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡æ¦‚å¿µä¹‹é—´çš„ç»“æ„åŒ–å…³ç³»ï¼›2) å¼•å…¥äº†å›¾Transformeræ¥æ•è·å¤šå±‚æ¬¡çš„ä¾èµ–å…³ç³»ï¼Œå…‹æœäº†ä¼ ç»ŸCBMsåœ¨æ¦‚å¿µäº¤äº’å»ºæ¨¡æ–¹é¢çš„å±€é™æ€§ï¼›3) ä½¿ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—æ¥å¢å¼ºæ¨¡å‹å¯¹å¤æ‚æ¦‚å¿µæ¨¡å¼çš„é€‚åº”æ€§ï¼Œé€šè¿‡åŠ¨æ€åˆ†é…æ¨ç†ä»»åŠ¡æé«˜æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å›¾æ„å»ºæ¨¡å—ä¸­ï¼Œè®ºæ–‡è®¾è®¡äº†ç­”æ¡ˆ-æ¦‚å¿µå›¾å’Œç­”æ¡ˆ-é—®é¢˜å›¾ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºæ¦‚å¿µæˆ–é—®é¢˜ï¼Œè¾¹è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚å›¾Transformeré‡‡ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¥æ•è·èŠ‚ç‚¹ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—åŒ…å«å¤šä¸ªå­ä¸“å®¶ç½‘ç»œï¼Œæ¯ä¸ªå­ä¸“å®¶è´Ÿè´£å¤„ç†ç‰¹å®šçš„æ¦‚å¿µæ¨¡å¼ã€‚é€šè¿‡é—¨æ§ç½‘ç»œåŠ¨æ€åœ°é€‰æ‹©åˆé€‚çš„å­ä¸“å®¶è¿›è¡Œæ¨ç†ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é¢„æµ‹æŸå¤±å’Œä¸“å®¶é€‰æ‹©çš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥ä¿è¯ä¸“å®¶é€‰æ‹©çš„åˆç†æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MoE-SGTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºå…¶ä»–æ¦‚å¿µç“¶é¢ˆç½‘ç»œçš„æ€§èƒ½ã€‚é€šè¿‡å»ºæ¨¡æ¦‚å¿µé—´çš„ç»“æ„åŒ–å…³ç³»å’Œåˆ©ç”¨åŠ¨æ€ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚æ¦‚å¿µæ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†å±•ç¤ºï¼ŒéªŒè¯äº†MoE-SGTçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦é«˜å¯è§£é‡Šæ€§çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€åŒ»å­¦è¯Šæ–­å’Œæ™ºèƒ½æ•™è‚²ç­‰é¢†åŸŸã€‚é€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡æ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ï¼Œä»è€Œåœ¨å…³é”®å†³ç­–åœºæ™¯ä¸­æä¾›æ›´å¯ä¿¡çš„ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„çŸ¥è¯†å›¾è°±æ¨ç†å’Œå¤šæ¨¡æ€èåˆä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With growing demand for interpretability in deep learning, especially in high stakes domains, Concept Bottleneck Models (CBMs) address this by inserting human understandable concepts into the prediction pipeline, but they are generally single modal and ignore structured concept relationships. To overcome these limitations, we present MoE-SGT, a reasoning driven framework that augments CBMs with a structure injecting Graph Transformer and a Mixture of Experts (MoE) module. We construct answer-concept and answer-question graphs for multimodal inputs to explicitly model the structured relationships among concepts. Subsequently, we integrate Graph Transformer to capture multi level dependencies, addressing the limitations of traditional Concept Bottleneck Models in modeling concept interactions. However, it still encounters bottlenecks in adapting to complex concept patterns. Therefore, we replace the feed forward layers with a Mixture of Experts (MoE) module, enabling the model to have greater capacity in learning diverse concept relationships while dynamically allocating reasoning tasks to different sub experts, thereby significantly enhancing the model's adaptability to complex concept reasoning. MoE-SGT achieves higher accuracy than other concept bottleneck networks on multiple datasets by modeling structured relationships among concepts and utilizing a dynamic expert selection mechanism.

