---
layout: default
title: Can World Models Benefit VLMs for World Dynamics?
---

# Can World Models Benefit VLMs for World Dynamics?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00855" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00855v1</a>
  <a href="https://arxiv.org/pdf/2510.00855.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00855v1" onclick="toggleFavorite(this, '2510.00855v1', 'Can World Models Benefit VLMs for World Dynamics?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kevin Zhang, Kuangzhi Ge, Xiaowei Chi, Renrui Zhang, Shaojun Shi, Zhen Dong, Sirui Han, Shanghang Zhang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: Project page: https://dyva-worldlm.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWorldLMï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹å…ˆéªŒå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸–ç•ŒåŠ¨æ€ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸–ç•Œæ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `è§†é¢‘æ‰©æ•£æ¨¡å‹` `ç”Ÿæˆå¼ç¼–ç å™¨` `ç©ºé—´æ¨ç†` `åŠ¨æ€ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `è¿åŠ¨ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠåŠ¨æ€å˜åŒ–å’Œç©ºé—´æ¨ç†çš„ä»»åŠ¡æ—¶å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹å¯¹ä¸–ç•ŒåŠ¨æ€çš„æœ‰æ•ˆå»ºæ¨¡ã€‚
2. è®ºæ–‡æå‡ºWorldLMï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆå¼ç¼–ç å™¨ï¼Œæå–åŒ…å«è¿åŠ¨ä¿¡æ¯çš„è§†è§‰åµŒå…¥ï¼Œä»è€Œå¼•å…¥ä¸–ç•Œæ¨¡å‹å…ˆéªŒã€‚
3. å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„Dynamic Vision Aligner (DyVA) åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå¹¶æå‡äº†å•å›¾åƒæ¨¡å‹çš„å¤šå¸§æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ¢è®¨äº†ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelsï¼‰èƒ½å¦æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹ä¸–ç•ŒåŠ¨æ€çš„ç†è§£èƒ½åŠ›ã€‚ä¸–ç•Œæ¨¡å‹é€šè¿‡äº’è”ç½‘è§„æ¨¡çš„è§†é¢‘æ•°æ®è®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆè¿è´¯ä¸”åˆç†çš„ç»“æ„ã€è¿åŠ¨å’Œç‰©ç†åŠ¨æ€ï¼Œè¢«è®¤ä¸ºæ˜¯å¼ºå¤§çš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨ã€‚æœ¬æ–‡å°†è§†é¢‘æ‰©æ•£æ¨¡å‹é‡æ–°ç”¨ä½œç”Ÿæˆå¼ç¼–ç å™¨ï¼Œæ‰§è¡Œå•æ­¥å»å™ªï¼Œå¹¶å°†å¾—åˆ°çš„æ½œåœ¨å˜é‡ä½œä¸ºè§†è§‰åµŒå…¥ã€‚é€šè¿‡å¯¹è¿™ç±»æ¨¡å‹ï¼ˆç§°ä¸ºWorldLMsï¼‰çš„å®è¯ç ”ç©¶ï¼Œå‘ç°ç”Ÿæˆå¼ç¼–ç å™¨èƒ½å¤Ÿæ•è·å¯¹ä¸‹æ¸¸ç†è§£æœ‰ç”¨çš„æ½œåœ¨å˜é‡ï¼Œè¿™äº›å˜é‡ä¸ä¼ ç»Ÿç¼–ç å™¨æœ‰æ‰€ä¸åŒã€‚æœ€ä½³æ¨¡å‹Dynamic Vision Aligner (DyVA) æ˜¾è‘—å¢å¼ºäº†ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä½¿å•å›¾åƒæ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œå¤šå¸§æ¨ç†ã€‚åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ï¼ŒDyVAè¶…è¶Šäº†å¼€æºå’Œä¸“æœ‰åŸºçº¿ï¼Œå®ç°äº†æœ€å…ˆè¿›æˆ–å¯æ¯”çš„æ€§èƒ½ã€‚è¿™äº›æå‡å½’å› äºWorldLMä»è§†é¢‘é¢„è®­ç»ƒä¸­ç»§æ‰¿çš„è¿åŠ¨ä¸€è‡´æ€§å†…åŒ–ã€‚æœ€åï¼Œç³»ç»Ÿåœ°æ¢ç´¢äº†å¹¿æ³›çš„æ¨¡å‹è®¾è®¡ï¼Œä¸ºæœªæ¥çš„å·¥ä½œæŒ‡æ˜äº†æ–¹å‘ã€‚å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½ä¸ºåˆ©ç”¨ä¸–ç•Œæ¨¡å‹å…ˆéªŒçš„æ–°å‹VLMé“ºå¹³é“è·¯ï¼Œå¹¶æœç€é€šç”¨è§†è§‰å­¦ä¹ å™¨çš„æ–¹å‘å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£ä¸–ç•ŒåŠ¨æ€å’Œè¿›è¡Œç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¼ ç»Ÿçš„è§†è§‰ç¼–ç å™¨éš¾ä»¥æ•æ‰è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯å’Œè¿åŠ¨è§„å¾‹ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†éœ€è¦ç†è§£ç‰©ä½“è¿åŠ¨ã€äº¤äº’å’Œå˜åŒ–çš„åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†ä¸–ç•ŒåŠ¨æ€çŸ¥è¯†èå…¥è§†è§‰è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„ä¸–ç•Œæ¨¡å‹ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå°†è§†é¢‘æ•°æ®ä¸­å­¦ä¹ åˆ°çš„è¿åŠ¨è§„å¾‹å’Œç‰©ç†çŸ¥è¯†è¿ç§»åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œè®ºæ–‡ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆå¼ç¼–ç å™¨ï¼Œé€šè¿‡å•æ­¥å»å™ªè¿‡ç¨‹æå–è§†è§‰ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åŒ…å«äº†ä¸°å¯Œçš„è¿åŠ¨ä¿¡æ¯å’Œä¸–ç•ŒåŠ¨æ€å…ˆéªŒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆå¼ç¼–ç å™¨ï¼›2) å¯¹è¾“å…¥å›¾åƒæˆ–è§†é¢‘å¸§è¿›è¡Œå•æ­¥å»å™ªï¼Œå¾—åˆ°è§†è§‰åµŒå…¥ï¼›3) å°†è§†è§‰åµŒå…¥è¾“å…¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒå’Œæ¨ç†ã€‚æå‡ºçš„Dynamic Vision Aligner (DyVA) æ˜¯ä¸€ä¸ªå…·ä½“çš„WorldLMå®ç°ï¼Œå®ƒåˆ©ç”¨Transformerç»“æ„å¯¹è§†è§‰åµŒå…¥è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å…¶ä¸æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†ä¸–ç•Œæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼‰å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†å…¶ç”¨ä½œç”Ÿæˆå¼ç¼–ç å™¨ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†è§†é¢‘æ•°æ®ä¸­å­¦ä¹ åˆ°çš„è¿åŠ¨è§„å¾‹å’Œç‰©ç†çŸ¥è¯†è¿ç§»åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹ä¸–ç•ŒåŠ¨æ€çš„ç†è§£èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„è§†è§‰ç¼–ç å™¨ç›¸æ¯”ï¼Œç”Ÿæˆå¼ç¼–ç å™¨èƒ½å¤Ÿæ•è·æ›´ä¸°å¯Œçš„è¿åŠ¨ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿ç¼–ç å™¨å…·æœ‰å¼ºå¤§çš„ä¸–ç•ŒåŠ¨æ€å»ºæ¨¡èƒ½åŠ›ï¼›2) ä½¿ç”¨å•æ­¥å»å™ªè¿‡ç¨‹ï¼Œä»¥é«˜æ•ˆåœ°æå–è§†è§‰åµŒå…¥ï¼›3) è®¾è®¡Dynamic Vision Aligner (DyVA) æ¨¡å‹ï¼Œåˆ©ç”¨Transformerç»“æ„å¯¹è§†è§‰åµŒå…¥è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å…¶ä¸æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ã€‚è®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„æ¨¡å‹è®¾è®¡ï¼Œä¾‹å¦‚ä¸åŒçš„å»å™ªæ­¥æ•°ã€ä¸åŒçš„Transformerç»“æ„ç­‰ï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹é…ç½®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„Dynamic Vision Aligner (DyVA) åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šï¼ŒDyVAçš„æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†10%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹å…ˆéªŒå¯ä»¥æœ‰æ•ˆåœ°æå‡è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ä¸–ç•ŒåŠ¨æ€çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç†è§£ã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ä¸–ç•ŒåŠ¨æ€çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°æ„ŸçŸ¥ã€æ¨ç†å’Œå†³ç­–ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½åŒ–çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Trained on internet-scale video data, generative world models are increasingly recognized as powerful world simulators that can generate consistent and plausible dynamics over structure, motion, and physics. This raises a natural question: with the advent of strong video foundational models, might they supplant conventional vision encoder paradigms for general-purpose multimodal understanding? While recent studies have begun to explore the potential of world models on common vision tasks, these explorations typically lack a systematic investigation of generic, multimodal tasks. In this work, we strive to investigate the capabilities when world model priors are transferred into Vision-Language Models: we re-purpose a video diffusion model as a generative encoder to perform a single denoising step and treat the resulting latents as a set of visual embedding. We empirically investigate this class of models, which we refer to as World-Language Models (WorldLMs), and we find that generative encoders can capture latents useful for downstream understanding that show distinctions from conventional encoders. Naming our best-performing variant Dynamic Vision Aligner (DyVA), we further discover that this method significantly enhances spatial reasoning abilities and enables single-image models to perform multi-frame reasoning. Through the curation of a suite of visual reasoning tasks, we find DyVA to surpass both open-source and proprietary baselines, achieving state-of-the-art or comparable performance. We attribute these gains to WorldLM's inherited motion-consistency internalization from video pre-training. Finally, we systematically explore extensive model designs to highlight promising directions for future work. We hope our study can pave the way for a new family of VLMs that leverage priors from world models and are on a promising path towards generalist vision learners.

