---
layout: default
title: IMAGEdit: Let Any Subject Transform
---

# IMAGEdit: Let Any Subject Transform

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01186" target="_blank" class="toolbar-btn">arXiv: 2510.01186v1</a>
    <a href="https://arxiv.org/pdf/2510.01186.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01186v1" 
            onclick="toggleFavorite(this, '2510.01186v1', 'IMAGEdit: Let Any Subject Transform')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Fei Shen, Weihao Xu, Rui Yan, Dong Zhang, Xiangbo Shu, Jinhui Tang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-01

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/XWH-A/IMAGEdit)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**IMAGEditÔºöÊèêÂá∫‰∏ÄÁßçÂÖçËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂÆûÁé∞‰ªªÊÑèÊï∞ÈáèËßÜÈ¢ë‰∏ª‰ΩìÁöÑÂ§ñËßÇÂèòÊç¢„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÁºñËæë` `Â§ö‰∏ª‰ΩìÁºñËæë` `ÂÖçËÆ≠ÁªÉ` `Â§öÊ®°ÊÄÅÂØπÈΩê` `Êé©Á†ÅÈ©±Âä®ÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÁºñËæëÊñπÊ≥ïÈöæ‰ª•Â§ÑÁêÜÂ§ö‰∏ª‰ΩìÂú∫ÊôØÔºå‰∏îÂÆπÊòìÂá∫Áé∞Êé©Á†ÅËæπÁïåÊ®°Á≥äÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÂ∫îÁî®ËåÉÂõ¥„ÄÇ
2. IMAGEditÂà©Áî®Â§ßÂûãÊ®°ÂûãÁîüÊàêÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂíåÊé©Á†ÅÂ∫èÂàóÔºåÂπ∂ÁªìÂêàÊé©Á†ÅÈ©±Âä®ÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÂÆûÁé∞‰∏ª‰ΩìÂ§ñËßÇÂèòÊç¢„ÄÇ
3. Âú®MSVBenchÂü∫ÂáÜÊµãËØï‰∏≠ÔºåIMAGEditË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§ö‰∏ª‰ΩìËßÜÈ¢ëÁºñËæëÊñπÈù¢ÁöÑ‰ºòË∂äÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫IMAGEditÔºå‰∏Ä‰∏™ÂÖçËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁî®‰∫éÁºñËæë‰ªªÊÑèÊï∞ÈáèÁöÑËßÜÈ¢ë‰∏ª‰ΩìÔºåÂú®‰øùÁïôÈùûÁõÆÊ†áÂå∫ÂüüÁöÑÂêåÊó∂ÔºåÊìçÁ∫µÂ§ö‰∏™ÊåáÂÆö‰∏ª‰ΩìÁöÑÂ§ñËßÇÔºåÊó†ÈúÄÂæÆË∞ÉÊàñÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÈÄöËøáÊèêÁ§∫ÂºïÂØºÁöÑÂ§öÊ®°ÊÄÅÂØπÈΩêÊ®°ÂùóÂíåÂü∫‰∫éÂÖàÈ™åÁöÑÊé©Á†ÅÈáçÂÆöÂêëÊ®°ÂùóÔºåÊèê‰æõÈ≤ÅÊ£íÁöÑÂ§öÊ®°ÊÄÅÊù°‰ª∂ÂíåÁ≤æÁ°ÆÁöÑÊé©Á†ÅÂ∫èÂàóÊù•ÂÆûÁé∞Ëøô‰∏ÄÁÇπ„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨Âà©Áî®Â§ßÂûãÊ®°ÂûãÁöÑÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõÔºå‰∏∫ÂêÑÁßçÁ±ªÂûãÁöÑÂ§ö‰∏™‰∏ª‰ΩìÁîüÊàêÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂíåÊé©Á†ÅËøêÂä®Â∫èÂàó„ÄÇÁÑ∂ÂêéÔºåÂ∞ÜËé∑ÂæóÁöÑÂÖàÈ™åÊé©Á†ÅÂ∫èÂàóËæìÂÖ•Âà∞È¢ÑËÆ≠ÁªÉÁöÑÊé©Á†ÅÈ©±Âä®ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠Ôºå‰ª•ÂêàÊàêÁºñËæëÂêéÁöÑËßÜÈ¢ë„ÄÇÂá≠ÂÄüÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåIMAGEditÂº•Ë°•‰∫ÜÊèêÁ§∫Á´ØÂ§öÊ®°ÊÄÅÊù°‰ª∂ÁöÑ‰∏çË∂≥ÔºåÂÖãÊúç‰∫Ü‰ªªÊÑèÊï∞Èáè‰∏ª‰ΩìÁöÑËßÜÈ¢ë‰∏≠Êé©Á†ÅËæπÁïåÁöÑÁ∫†Áº†Ôºå‰ªéËÄåÊòæËëóÊâ©Â±ï‰∫ÜËßÜÈ¢ëÁºñËæëÁöÑÈÄÇÁî®ÊÄß„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåIMAGEdit‰∏é‰ªª‰ΩïÊé©Á†ÅÈ©±Âä®ÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂÖºÂÆπÔºå‰ªéËÄåÊòæËëóÊèêÈ´ò‰∫ÜÊï¥‰ΩìÊÄßËÉΩ„ÄÇÂú®Êàë‰ª¨Êñ∞ÊûÑÂª∫ÁöÑÂ§ö‰∏ª‰ΩìÂü∫ÂáÜMSVBench‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åÈ™åËØÅ‰∫ÜIMAGEditÂßãÁªà‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ‰ª£Á†Å„ÄÅÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜÂèØÂú®https://github.com/XWH-A/IMAGEditÂÖ¨ÂºÄËé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜÈ¢ëÁºñËæëÊñπÊ≥ïÂú®Â§ÑÁêÜÂåÖÂê´Â§ö‰∏™‰∏ª‰ΩìÁöÑËßÜÈ¢ëÊó∂ÔºåÈù¢‰∏¥ÁùÄ‰∏ª‰ΩìÈó¥Êé©Á†ÅËæπÁïåÂÆπÊòìÊ∑∑Ê∑Ü„ÄÅÊèêÁ§∫‰ø°ÊÅØ‰∏çË∂≥Á≠âÈóÆÈ¢òÔºåÂØºËá¥ÁºñËæëÊïàÊûú‰∏ç‰Ω≥ÔºåÊ≥õÂåñËÉΩÂäõÂèóÈôê„ÄÇÊ≠§Â§ñÔºåËÆ∏Â§öÊñπÊ≥ïÈúÄË¶ÅÈíàÂØπÁâπÂÆöÂú∫ÊôØËøõË°åÂæÆË∞ÉÊàñÈáçÊñ∞ËÆ≠ÁªÉÔºåÂ¢ûÂä†‰∫Ü‰ΩøÁî®ÊàêÊú¨„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöIMAGEditÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãÊ®°ÂûãÂº∫Â§ßÁöÑÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõÔºå‰∏∫ËßÜÈ¢ë‰∏≠ÁöÑÂ§ö‰∏™‰∏ª‰ΩìÁîüÊàêÈ≤ÅÊ£íÁöÑÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂíåÁ≤æÁ°ÆÁöÑÊé©Á†ÅÂ∫èÂàó„ÄÇÁÑ∂ÂêéÔºåÂ∞ÜËøô‰∫õ‰ø°ÊÅØ‰Ωú‰∏∫Êù°‰ª∂ËæìÂÖ•Âà∞È¢ÑËÆ≠ÁªÉÁöÑÊé©Á†ÅÈ©±Âä®ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠Ôºå‰ªéËÄåÂÆûÁé∞ÂØπÊåáÂÆö‰∏ª‰ΩìÁöÑÂ§ñËßÇÂèòÊç¢ÔºåÂêåÊó∂‰øùÊåÅÈùûÁõÆÊ†áÂå∫Âüü‰∏çÂèò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöIMAGEditÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê®°ÂùóÔºöÊèêÁ§∫ÂºïÂØºÁöÑÂ§öÊ®°ÊÄÅÂØπÈΩêÊ®°ÂùóÂíåÂü∫‰∫éÂÖàÈ™åÁöÑÊé©Á†ÅÈáçÂÆöÂêëÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÂà©Áî®Â§ßÂûãÊ®°ÂûãÔºàÂ¶ÇCLIPÔºâÊèêÂèñËßÜÈ¢ë‰∏≠ÂêÑ‰∏™‰∏ª‰ΩìÁöÑÂ§öÊ®°ÊÄÅÁâπÂæÅÔºåÂπ∂Ê†πÊçÆÁî®Êà∑Êèê‰æõÁöÑÊñáÊú¨ÊèêÁ§∫ËøõË°åÂØπÈΩê„ÄÇÁÑ∂ÂêéÔºåÂü∫‰∫éËøô‰∫õÂØπÈΩêÂêéÁöÑÁâπÂæÅÔºåÁîüÊàêÊØè‰∏™‰∏ª‰ΩìÁöÑÊé©Á†ÅÂ∫èÂàó„ÄÇÊúÄÂêéÔºåÂ∞ÜÊé©Á†ÅÂ∫èÂàóÂíåÂ§öÊ®°ÊÄÅÁâπÂæÅËæìÂÖ•Âà∞È¢ÑËÆ≠ÁªÉÁöÑÊé©Á†ÅÈ©±Âä®ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠ÔºåÁîüÊàêÁºñËæëÂêéÁöÑËßÜÈ¢ë„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöIMAGEditÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÂÖçËÆ≠ÁªÉÁöÑËÆæËÆ°ÂíåÂØπÂ§ö‰∏ª‰ΩìÂú∫ÊôØÁöÑËâØÂ•ΩÊîØÊåÅ„ÄÇÈÄöËøáÂà©Áî®Â§ßÂûãÊ®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜÂíåÁîüÊàêËÉΩÂäõÔºåIMAGEditÈÅøÂÖç‰∫ÜÈíàÂØπÁâπÂÆöÂú∫ÊôØËøõË°åÂæÆË∞ÉÊàñÈáçÊñ∞ËÆ≠ÁªÉÁöÑÈúÄÊ±ÇÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂ§öÊ®°ÊÄÅÂØπÈΩêÂíåÊé©Á†ÅÈáçÂÆöÂêëÊ®°ÂùóÔºåIMAGEditËÉΩÂ§üÊúâÊïàÂú∞Â§ÑÁêÜÂ§ö‰∏ª‰ΩìËßÜÈ¢ë‰∏≠Êé©Á†ÅËæπÁïåÁöÑÊ∑∑Ê∑ÜÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöIMAGEditÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®CLIPÁ≠âÂ§ßÂûãÊ®°ÂûãÊèêÂèñÂ§öÊ®°ÊÄÅÁâπÂæÅÔºå‰ª•Êèê‰æõÊõ¥‰∏∞ÂØåÁöÑËØ≠‰πâ‰ø°ÊÅØÔºõ2) ËÆæËÆ°ÊèêÁ§∫ÂºïÂØºÁöÑÂ§öÊ®°ÊÄÅÂØπÈΩêÊ®°ÂùóÔºåÂ∞ÜÁî®Êà∑Êèê‰æõÁöÑÊñáÊú¨ÊèêÁ§∫‰∏éËßÜÈ¢ëÂÜÖÂÆπËøõË°åÂØπÈΩêÔºõ3) ËÆæËÆ°Âü∫‰∫éÂÖàÈ™åÁöÑÊé©Á†ÅÈáçÂÆöÂêëÊ®°ÂùóÔºåÁîüÊàêÁ≤æÁ°ÆÁöÑÊé©Á†ÅÂ∫èÂàóÔºõ4) Âà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊé©Á†ÅÈ©±Âä®ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºå‰ª•ÊèêÈ´òÁîüÊàêËßÜÈ¢ëÁöÑË¥®ÈáèÂíå‰∏ÄËá¥ÊÄß„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÊú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

IMAGEditÂú®Ëá™Âª∫ÁöÑÂ§ö‰∏ª‰ΩìËßÜÈ¢ëÁºñËæëÂü∫ÂáÜMSVBench‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåIMAGEditÂú®Â§ö‰∏ª‰ΩìËßÜÈ¢ëÁºñËæë‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÊú™Âú®ÊëòË¶Å‰∏≠ÁªôÂá∫ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ‰ΩÜÊëòË¶ÅÂº∫Ë∞É‰∫ÜIMAGEditÁöÑ‰∏ÄËá¥ÊÄßË∂ÖË∂ä„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

IMAGEditÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÁîµÂΩ±ÁâπÊïàÂà∂‰Ωú„ÄÅÂπøÂëäÂàõÊÑèËÆæËÆ°„ÄÅËôöÊãüÂΩ¢Ë±°ÂÆöÂà∂„ÄÅÊïôËÇ≤ÂÜÖÂÆπÁîüÊàêÁ≠â„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Áî®Êà∑ËΩªÊùæÂú∞ÂØπËßÜÈ¢ë‰∏≠ÁöÑ‰∫∫Áâ©ÊàñÁâ©‰ΩìËøõË°åÂ§ñËßÇÂèòÊç¢ÔºåÂàõÈÄ†Âá∫ÂêÑÁßçÊúâË∂£ÂíåÂØåÊúâÂàõÊÑèÁöÑËßÜÈ¢ëÂÜÖÂÆπ„ÄÇÊ≠§Â§ñÔºåIMAGEditÁöÑÂÖçËÆ≠ÁªÉÁâπÊÄß‰πüÈôç‰Ωé‰∫Ü‰ΩøÁî®Èó®ÊßõÔºå‰ΩøÂæóÊõ¥Â§öÁî®Êà∑ËÉΩÂ§üÂèÇ‰∏éÂà∞ËßÜÈ¢ëÁºñËæëÂàõ‰Ωú‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In this paper, we present IMAGEdit, a training-free framework for any number of video subject editing that manipulates the appearances of multiple designated subjects while preserving non-target regions, without finetuning or retraining. We achieve this by providing robust multimodal conditioning and precise mask sequences through a prompt-guided multimodal alignment module and a prior-based mask retargeting module. We first leverage large models' understanding and generation capabilities to produce multimodal information and mask motion sequences for multiple subjects across various types. Then, the obtained prior mask sequences are fed into a pretrained mask-driven video generation model to synthesize the edited video. With strong generalization capability, IMAGEdit remedies insufficient prompt-side multimodal conditioning and overcomes mask boundary entanglement in videos with any number of subjects, thereby significantly expanding the applicability of video editing. More importantly, IMAGEdit is compatible with any mask-driven video generation model, significantly improving overall performance. Extensive experiments on our newly constructed multi-subject benchmark MSVBench verify that IMAGEdit consistently surpasses state-of-the-art methods. Code, models, and datasets are publicly available at https://github.com/XWH-A/IMAGEdit.

