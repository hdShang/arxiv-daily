---
layout: default
title: IMAGEdit: Let Any Subject Transform
---

# IMAGEdit: Let Any Subject Transform

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01186" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01186v1</a>
  <a href="https://arxiv.org/pdf/2510.01186.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01186v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01186v1', 'IMAGEdit: Let Any Subject Transform')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fei Shen, Weihao Xu, Rui Yan, Dong Zhang, Xiangbo Shu, Jinhui Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/XWH-A/IMAGEdit)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**IMAGEditï¼šæå‡ºä¸€ç§å…è®­ç»ƒæ¡†æ¶ï¼Œå®ç°ä»»æ„æ•°é‡è§†é¢‘ä¸»ä½“çš„å¤–è§‚å˜æ¢ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç¼–è¾‘` `å¤šä¸»ä½“ç¼–è¾‘` `å…è®­ç»ƒ` `å¤šæ¨¡æ€å¯¹é½` `æ©ç é©±åŠ¨ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç¼–è¾‘æ–¹æ³•éš¾ä»¥å¤„ç†å¤šä¸»ä½“åœºæ™¯ï¼Œä¸”å®¹æ˜“å‡ºç°æ©ç è¾¹ç•Œæ¨¡ç³Šé—®é¢˜ï¼Œé™åˆ¶äº†åº”ç”¨èŒƒå›´ã€‚
2. IMAGEditåˆ©ç”¨å¤§å‹æ¨¡å‹ç”Ÿæˆå¤šæ¨¡æ€ä¿¡æ¯å’Œæ©ç åºåˆ—ï¼Œå¹¶ç»“åˆæ©ç é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°ä¸»ä½“å¤–è§‚å˜æ¢ã€‚
3. åœ¨MSVBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒIMAGEditè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šä¸»ä½“è§†é¢‘ç¼–è¾‘æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºIMAGEditï¼Œä¸€ä¸ªå…è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºç¼–è¾‘ä»»æ„æ•°é‡çš„è§†é¢‘ä¸»ä½“ï¼Œåœ¨ä¿ç•™éç›®æ ‡åŒºåŸŸçš„åŒæ—¶ï¼Œæ“çºµå¤šä¸ªæŒ‡å®šä¸»ä½“çš„å¤–è§‚ï¼Œæ— éœ€å¾®è°ƒæˆ–é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€å¯¹é½æ¨¡å—å’ŒåŸºäºå…ˆéªŒçš„æ©ç é‡å®šå‘æ¨¡å—ï¼Œæä¾›é²æ£’çš„å¤šæ¨¡æ€æ¡ä»¶å’Œç²¾ç¡®çš„æ©ç åºåˆ—æ¥å®ç°è¿™ä¸€ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹æ¨¡å‹çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºå„ç§ç±»å‹çš„å¤šä¸ªä¸»ä½“ç”Ÿæˆå¤šæ¨¡æ€ä¿¡æ¯å’Œæ©ç è¿åŠ¨åºåˆ—ã€‚ç„¶åï¼Œå°†è·å¾—çš„å…ˆéªŒæ©ç åºåˆ—è¾“å…¥åˆ°é¢„è®­ç»ƒçš„æ©ç é©±åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»¥åˆæˆç¼–è¾‘åçš„è§†é¢‘ã€‚å‡­å€Ÿå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒIMAGEditå¼¥è¡¥äº†æç¤ºç«¯å¤šæ¨¡æ€æ¡ä»¶çš„ä¸è¶³ï¼Œå…‹æœäº†ä»»æ„æ•°é‡ä¸»ä½“çš„è§†é¢‘ä¸­æ©ç è¾¹ç•Œçš„çº ç¼ ï¼Œä»è€Œæ˜¾è‘—æ‰©å±•äº†è§†é¢‘ç¼–è¾‘çš„é€‚ç”¨æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒIMAGEditä¸ä»»ä½•æ©ç é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å…¼å®¹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ•´ä½“æ€§èƒ½ã€‚åœ¨æˆ‘ä»¬æ–°æ„å»ºçš„å¤šä¸»ä½“åŸºå‡†MSVBenchä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†IMAGEditå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å¯åœ¨https://github.com/XWH-A/IMAGEditå…¬å¼€è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç¼–è¾‘æ–¹æ³•åœ¨å¤„ç†åŒ…å«å¤šä¸ªä¸»ä½“çš„è§†é¢‘æ—¶ï¼Œé¢ä¸´ç€ä¸»ä½“é—´æ©ç è¾¹ç•Œå®¹æ˜“æ··æ·†ã€æç¤ºä¿¡æ¯ä¸è¶³ç­‰é—®é¢˜ï¼Œå¯¼è‡´ç¼–è¾‘æ•ˆæœä¸ä½³ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•éœ€è¦é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œå¾®è°ƒæˆ–é‡æ–°è®­ç»ƒï¼Œå¢åŠ äº†ä½¿ç”¨æˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIMAGEditçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹æ¨¡å‹å¼ºå¤§çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºè§†é¢‘ä¸­çš„å¤šä¸ªä¸»ä½“ç”Ÿæˆé²æ£’çš„å¤šæ¨¡æ€ä¿¡æ¯å’Œç²¾ç¡®çš„æ©ç åºåˆ—ã€‚ç„¶åï¼Œå°†è¿™äº›ä¿¡æ¯ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°é¢„è®­ç»ƒçš„æ©ç é©±åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»è€Œå®ç°å¯¹æŒ‡å®šä¸»ä½“çš„å¤–è§‚å˜æ¢ï¼ŒåŒæ—¶ä¿æŒéç›®æ ‡åŒºåŸŸä¸å˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIMAGEditæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šæç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€å¯¹é½æ¨¡å—å’ŒåŸºäºå…ˆéªŒçš„æ©ç é‡å®šå‘æ¨¡å—ã€‚é¦–å…ˆï¼Œåˆ©ç”¨å¤§å‹æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æå–è§†é¢‘ä¸­å„ä¸ªä¸»ä½“çš„å¤šæ¨¡æ€ç‰¹å¾ï¼Œå¹¶æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬æç¤ºè¿›è¡Œå¯¹é½ã€‚ç„¶åï¼ŒåŸºäºè¿™äº›å¯¹é½åçš„ç‰¹å¾ï¼Œç”Ÿæˆæ¯ä¸ªä¸»ä½“çš„æ©ç åºåˆ—ã€‚æœ€åï¼Œå°†æ©ç åºåˆ—å’Œå¤šæ¨¡æ€ç‰¹å¾è¾“å…¥åˆ°é¢„è®­ç»ƒçš„æ©ç é©±åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œç”Ÿæˆç¼–è¾‘åçš„è§†é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šIMAGEditçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…è®­ç»ƒçš„è®¾è®¡å’Œå¯¹å¤šä¸»ä½“åœºæ™¯çš„è‰¯å¥½æ”¯æŒã€‚é€šè¿‡åˆ©ç”¨å¤§å‹æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒIMAGEdité¿å…äº†é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œå¾®è°ƒæˆ–é‡æ–°è®­ç»ƒçš„éœ€æ±‚ï¼Œä»è€Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤šæ¨¡æ€å¯¹é½å’Œæ©ç é‡å®šå‘æ¨¡å—ï¼ŒIMAGEditèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šä¸»ä½“è§†é¢‘ä¸­æ©ç è¾¹ç•Œçš„æ··æ·†é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šIMAGEditçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨CLIPç­‰å¤§å‹æ¨¡å‹æå–å¤šæ¨¡æ€ç‰¹å¾ï¼Œä»¥æä¾›æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼›2) è®¾è®¡æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€å¯¹é½æ¨¡å—ï¼Œå°†ç”¨æˆ·æä¾›çš„æ–‡æœ¬æç¤ºä¸è§†é¢‘å†…å®¹è¿›è¡Œå¯¹é½ï¼›3) è®¾è®¡åŸºäºå…ˆéªŒçš„æ©ç é‡å®šå‘æ¨¡å—ï¼Œç”Ÿæˆç²¾ç¡®çš„æ©ç åºåˆ—ï¼›4) åˆ©ç”¨é¢„è®­ç»ƒçš„æ©ç é©±åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»¥æé«˜ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

IMAGEditåœ¨è‡ªå»ºçš„å¤šä¸»ä½“è§†é¢‘ç¼–è¾‘åŸºå‡†MSVBenchä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒIMAGEditåœ¨å¤šä¸»ä½“è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†æ‘˜è¦å¼ºè°ƒäº†IMAGEditçš„ä¸€è‡´æ€§è¶…è¶Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IMAGEditå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ç”µå½±ç‰¹æ•ˆåˆ¶ä½œã€å¹¿å‘Šåˆ›æ„è®¾è®¡ã€è™šæ‹Ÿå½¢è±¡å®šåˆ¶ã€æ•™è‚²å†…å®¹ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·è½»æ¾åœ°å¯¹è§†é¢‘ä¸­çš„äººç‰©æˆ–ç‰©ä½“è¿›è¡Œå¤–è§‚å˜æ¢ï¼Œåˆ›é€ å‡ºå„ç§æœ‰è¶£å’Œå¯Œæœ‰åˆ›æ„çš„è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼ŒIMAGEditçš„å…è®­ç»ƒç‰¹æ€§ä¹Ÿé™ä½äº†ä½¿ç”¨é—¨æ§›ï¼Œä½¿å¾—æ›´å¤šç”¨æˆ·èƒ½å¤Ÿå‚ä¸åˆ°è§†é¢‘ç¼–è¾‘åˆ›ä½œä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present IMAGEdit, a training-free framework for any number of video subject editing that manipulates the appearances of multiple designated subjects while preserving non-target regions, without finetuning or retraining. We achieve this by providing robust multimodal conditioning and precise mask sequences through a prompt-guided multimodal alignment module and a prior-based mask retargeting module. We first leverage large models' understanding and generation capabilities to produce multimodal information and mask motion sequences for multiple subjects across various types. Then, the obtained prior mask sequences are fed into a pretrained mask-driven video generation model to synthesize the edited video. With strong generalization capability, IMAGEdit remedies insufficient prompt-side multimodal conditioning and overcomes mask boundary entanglement in videos with any number of subjects, thereby significantly expanding the applicability of video editing. More importantly, IMAGEdit is compatible with any mask-driven video generation model, significantly improving overall performance. Extensive experiments on our newly constructed multi-subject benchmark MSVBench verify that IMAGEdit consistently surpasses state-of-the-art methods. Code, models, and datasets are publicly available at https://github.com/XWH-A/IMAGEdit.

