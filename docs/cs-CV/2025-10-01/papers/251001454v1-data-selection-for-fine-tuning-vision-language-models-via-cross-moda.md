---
layout: default
title: Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories
---

# Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01454" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01454v1</a>
  <a href="https://arxiv.org/pdf/2510.01454.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01454v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01454v1', 'Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nilay Naharas, Dang Nguyen, Nesihan Bulut, Mohammadhossein Bateni, Vahab Mirrokni, Baharan Mirzasoleiman

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: 30 pages, 10 figures, 5 tables, link: https://bigml-cs-ucla.github.io/XMAS-project-page/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://bigml-cs-ucla.github.io/XMAS-project-page/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºXMASæ–¹æ³•ï¼Œé€šè¿‡è·¨æ¨¡æ€å¯¹é½è½¨è¿¹è¿›è¡Œè§†è§‰è¯­è¨€æ¨¡å‹é«˜æ•ˆæ•°æ®é€‰æ‹©ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æ•°æ®é€‰æ‹©` `æŒ‡ä»¤å¾®è°ƒ` `è·¨æ¨¡æ€å¯¹é½` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LVLMæ•°æ®é€‰æ‹©æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆå»é™¤è®­ç»ƒæ•°æ®ä¸­çš„å†—ä½™ï¼Œç”šè‡³ä¸å¦‚éšæœºé€‰æ‹©ã€‚
2. XMASæ–¹æ³•é€šè¿‡åˆ†æè·¨æ¨¡æ€æ³¨æ„åŠ›çŸ©é˜µçš„è½¨è¿¹ï¼Œè¯†åˆ«å¹¶å»é™¤å†—ä½™æ ·æœ¬ï¼Œä¿ç•™ä¿¡æ¯é‡å¤§çš„æ ·æœ¬ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒXMASèƒ½æ˜¾è‘—å‡å°‘è®­ç»ƒæ•°æ®é‡ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰æŒ‡ä»¤å¾®è°ƒä¸­çš„æ•°æ®å†—ä½™é—®é¢˜ã€‚ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•åœ¨ä¸åŒå­é›†å¤§å°ä¸‹å‡æ— æ³•è¶…è¶Šéšæœºé€‰æ‹©ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸåˆ™æ€§çš„LVLMæŒ‡ä»¤å¾®è°ƒæ•°æ®é«˜æ•ˆé€‰æ‹©æ–¹æ³•XMASã€‚è¯¥æ–¹æ³•è¯æ˜ï¼ŒæŒ‡ä»¤å¾®è°ƒæœŸé—´å…·æœ‰ç›¸ä¼¼è·¨æ¨¡æ€æ³¨æ„åŠ›çŸ©é˜µçš„æ ·æœ¬å…·æœ‰ç›¸ä¼¼çš„æ¢¯åº¦ï¼Œä»è€Œä»¥ç±»ä¼¼çš„æ–¹å¼å½±å“æ¨¡å‹å‚æ•°ã€‚XMASé€šè¿‡å¯¹å°å‹ä»£ç†LVLMå¾®è°ƒè·å¾—çš„æ³¨æ„åŠ›çŸ©é˜µçš„é¡¶éƒ¨å¥‡å¼‚å€¼è½¨è¿¹è¿›è¡Œèšç±»ï¼Œç„¶åä»ä¸­é‡‡æ ·å¹³è¡¡å­é›†ï¼Œæœ‰æ•ˆå»é™¤å¤§è§„æ¨¡LVLMè®­ç»ƒæ•°æ®ä¸­çš„å†—ä½™ã€‚å®éªŒè¡¨æ˜ï¼ŒXMASå¯ä»¥åœ¨å®Œå…¨ä¿æŒLLaVA-1.5-7Båœ¨10ä¸ªä¸‹æ¸¸åŸºå‡†ä¸Šçš„æ€§èƒ½çš„åŒæ—¶ï¼Œä¸¢å¼ƒLLaVA-665kæ•°æ®é›†çš„50%å’ŒVision-Flanæ•°æ®é›†çš„85%ï¼Œå¹¶å°†å…¶è®­ç»ƒé€Ÿåº¦æé«˜1.2å€ã€‚ä¸LLaVA-665kçš„æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼Œæ•°æ®ç¼©å‡å¹…åº¦æé«˜äº†30%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­æ•°æ®å†—ä½™çš„é—®é¢˜ã€‚ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œå¦‚åŸºäºæ¢¯åº¦çš„æ–¹æ³•æˆ–åŸºäºä¿¡æ¯è®ºçš„æ–¹æ³•ï¼Œåœ¨LVLMä¸Šè¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆå»é™¤å†—ä½™æ•°æ®ï¼Œç”šè‡³ä¸å¦‚éšæœºé€‰æ‹©ã€‚è¿™å¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œæµªè´¹è®¡ç®—èµ„æºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼Œå…·æœ‰ç›¸ä¼¼è·¨æ¨¡æ€æ³¨æ„åŠ›çŸ©é˜µçš„æ ·æœ¬ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹æ¨¡å‹å‚æ•°çš„å½±å“ç›¸ä¼¼ï¼Œå› æ­¤å¯ä»¥è¢«è®¤ä¸ºæ˜¯å†—ä½™çš„ã€‚é€šè¿‡è¯†åˆ«å’Œå»é™¤è¿™äº›å†—ä½™æ ·æœ¬ï¼Œå¯ä»¥å‡å°‘è®­ç»ƒæ•°æ®é‡ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šXMASæ–¹æ³•çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1. ä½¿ç”¨å°å‹ä»£ç†LVLMè¿›è¡Œåˆæ­¥å¾®è°ƒã€‚2. æå–è®­ç»ƒæ ·æœ¬åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„è·¨æ¨¡æ€æ³¨æ„åŠ›çŸ©é˜µã€‚3. å¯¹æ¯ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›çŸ©é˜µè®¡ç®—é¡¶éƒ¨å¥‡å¼‚å€¼ï¼Œå¹¶è®°å½•å…¶éšè®­ç»ƒæ­¥æ•°å˜åŒ–çš„è½¨è¿¹ã€‚4. åŸºäºå¥‡å¼‚å€¼è½¨è¿¹å¯¹æ ·æœ¬è¿›è¡Œèšç±»ã€‚5. ä»æ¯ä¸ªèšç±»ä¸­é‡‡æ ·ä»£è¡¨æ€§æ ·æœ¬ï¼Œç»„æˆæœ€ç»ˆçš„è®­ç»ƒå­é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šXMASçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›çŸ©é˜µçš„è½¨è¿¹æ¥è¡¡é‡æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä¸ç›´æ¥æ¯”è¾ƒæ³¨æ„åŠ›çŸ©é˜µæˆ–æ¢¯åº¦ç›¸æ¯”ï¼Œæ³¨æ„åŠ›çŸ©é˜µè½¨è¿¹èƒ½å¤Ÿæ•æ‰æ ·æœ¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ï¼Œæ›´å‡†ç¡®åœ°åæ˜ æ ·æœ¬å¯¹æ¨¡å‹å‚æ•°çš„å½±å“ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼Œå¯ä»¥æå–æ³¨æ„åŠ›çŸ©é˜µçš„ä¸»è¦ç‰¹å¾ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šXMASçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1. ä½¿ç”¨å°å‹ä»£ç†LVLMåŠ é€Ÿæ³¨æ„åŠ›çŸ©é˜µçš„æå–ã€‚2. é€‰æ‹©é¡¶éƒ¨å¥‡å¼‚å€¼ä½œä¸ºæ³¨æ„åŠ›çŸ©é˜µçš„ä»£è¡¨æ€§ç‰¹å¾ã€‚3. ä½¿ç”¨K-meansç­‰èšç±»ç®—æ³•å¯¹å¥‡å¼‚å€¼è½¨è¿¹è¿›è¡Œèšç±»ã€‚4. åœ¨æ¯ä¸ªèšç±»ä¸­ï¼Œæ ¹æ®æ ·æœ¬ä¸èšç±»ä¸­å¿ƒçš„è·ç¦»ï¼Œé€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬ã€‚è®ºæ–‡è¿˜é‡‡ç”¨äº†å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿æ¯ä¸ªèšç±»éƒ½æœ‰æ ·æœ¬è¢«é€‰ä¸­ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒXMASæ–¹æ³•åœ¨LLaVA-665kæ•°æ®é›†ä¸Šå¯ä»¥ä¸¢å¼ƒ50%çš„æ•°æ®ï¼Œåœ¨Vision-Flanæ•°æ®é›†ä¸Šå¯ä»¥ä¸¢å¼ƒ85%çš„æ•°æ®ï¼ŒåŒæ—¶å®Œå…¨ä¿æŒLLaVA-1.5-7Bæ¨¡å‹åœ¨10ä¸ªä¸‹æ¸¸åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒXMASæ–¹æ³•è¿˜å¯ä»¥å°†è®­ç»ƒé€Ÿåº¦æé«˜1.2å€ï¼Œå¹¶ä¸”æ¯”LLaVA-665kçš„æœ€ä½³åŸºçº¿çš„æ•°æ®ç¼©å‡å¹…åº¦æé«˜äº†30%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

XMASæ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒåœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡åºå¤§ã€è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹è¿­ä»£ï¼Œå¹¶æå‡æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ„å»ºæ›´é«˜æ•ˆçš„æ•°æ®é›†ï¼Œä¿ƒè¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Data-efficient learning aims to eliminate redundancy in large training datasets by training models on smaller subsets of the most informative examples. While data selection has been extensively explored for vision models and large language models (LLMs), it remains underexplored for Large Vision-Language Models (LVLMs). Notably, none of existing methods can outperform random selection at different subset sizes. In this work, we propose the first principled method for data-efficient instruction tuning of LVLMs. We prove that examples with similar cross-modal attention matrices during instruction tuning have similar gradients. Thus, they influence model parameters in a similar manner and convey the same information to the model during training. Building on this insight, we propose XMAS, which clusters examples based on the trajectories of the top singular values of their attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a balanced subset from these clusters, XMAS effectively removes redundancy in large-scale LVLM training data. Extensive experiments show that XMAS can discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and speeding up its training by 1.2x. This is 30% more data reduction compared to the best baseline for LLaVA-665k. The project's website can be found at https://bigml-cs-ucla.github.io/XMAS-project-page/.

