---
layout: default
title: Efficient Multi-modal Large Language Models via Progressive Consistency Distillation
---

# Efficient Multi-modal Large Language Models via Progressive Consistency Distillation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.00515" target="_blank" class="toolbar-btn">arXiv: 2510.00515v1</a>
    <a href="https://arxiv.org/pdf/2510.00515.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00515v1" 
            onclick="toggleFavorite(this, '2510.00515v1', 'Efficient Multi-modal Large Language Models via Progressive Consistency Distillation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zichen Wen, Shaobo Wang, Yufa Zhou, Junyuan Zhang, Qintong Zhang, Yifeng Gao, Zhaorun Chen, Bin Wang, Weijia Li, Conghui He, Linfeng Zhang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-01

**Â§áÊ≥®**: Accepted by NeurIPS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EPICÊ°ÜÊû∂ÔºåÈÄöËøáÊ∏êËøõ‰∏ÄËá¥ÊÄßËí∏È¶èÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁöÑÊïàÁéá**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßÊ®°Âûã` `Ê®°ÂûãÂéãÁº©` `Áü•ËØÜËí∏È¶è` `Ê∏êËøõÂ≠¶‰π†` `‰∏ÄËá¥ÊÄßÂ≠¶‰π†` `ËßÜËßâÈóÆÁ≠î` `ËÆ°ÁÆóÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÊïàÁéáÂèóËßÜËßâtokensËÆ°ÁÆóÈáèÈôêÂà∂ÔºåÁé∞ÊúâÂéãÁº©ÊñπÊ≥ïÂøΩÁï•‰∫ÜÂéãÁº©Â∏¶Êù•ÁöÑÂ≠¶‰π†ÈöæÂ∫¶Â¢ûÂä†„ÄÇ
2. EPICÊ°ÜÊû∂ÈÄöËøáÊ∏êËøõ‰∏ÄËá¥ÊÄßËí∏È¶èÔºåÂàÜËß£ÁâπÂæÅÁ©∫Èó¥Êâ∞Âä®ÔºåÈôç‰ΩéËÆ≠ÁªÉÈöæÂ∫¶ÔºåÊèêÂçáÊ®°ÂûãÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåEPICÊ°ÜÊû∂Âú®ÊúâÊïàÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõÊñπÈù¢ÂùáË°®Áé∞Âá∫Ëâ≤„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÊ®°Âûã(MLLM)‰∏≠ÔºåËßÜËßâtokensÊ∂àËÄó‰∫ÜÂ§ßÈáèÁöÑËÆ°ÁÆóËµÑÊ∫êÔºå‰∏•ÈáçÂΩ±Âìç‰∫ÜÊïàÁéá„ÄÇÁõÆÂâçÁöÑÂ∑•‰ΩúËØïÂõæÈÄöËøáÂú®ËÆ≠ÁªÉÊúüÈó¥ÂéãÁº©ËßÜËßâtokensÊù•ÊèêÈ´òÊïàÁéáÔºå‰ΩÜËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏ÂøΩÁï•‰∫ÜÁî±Ê≠§Â∏¶Êù•ÁöÑÂ≠¶‰π†ÈöæÂ∫¶Â¢ûÂä†ÔºåÂõ†‰∏∫Ê®°ÂûãÁöÑÂèÇÊï∞Á©∫Èó¥Èöæ‰ª•Âø´ÈÄüÈÄÇÂ∫îtokenÂéãÁº©ÂºïËµ∑ÁöÑÁâπÂæÅÁ©∫Èó¥‰∏≠ÁöÑÂ∑®Â§ßÊâ∞Âä®„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÈÄöËøáÊ∏êËøõ‰∏ÄËá¥ÊÄßËí∏È¶è(EPIC)Êù•ÂºÄÂèëÈ´òÊïàÁöÑMLLMÔºåËøôÊòØ‰∏Ä‰∏™Ê∏êËøõÂºèÂ≠¶‰π†Ê°ÜÊû∂„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÂ∞ÜtokenÂéãÁº©ÂºïÂÖ•ÁöÑÁâπÂæÅÁ©∫Èó¥Êâ∞Âä®ÂàÜËß£‰∏∫tokenÁª¥Â∫¶ÂíåÂ±ÇÁª¥Â∫¶ÔºåÂàÜÂà´ÂºïÂÖ•token‰∏ÄËá¥ÊÄßËí∏È¶èÂíåÂ±Ç‰∏ÄËá¥ÊÄßËí∏È¶èÔºåÊó®Âú®ÈÄöËøáÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÁöÑÊåáÂØºÂπ∂ÈÅµÂæ™Ê∏êËøõÂºèÂ≠¶‰π†ËΩ®ËøπÊù•Èôç‰ΩéËÆ≠ÁªÉÈöæÂ∫¶„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åËØÅÊòé‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑÊ°ÜÊû∂ÂÖ∑ÊúâÂçìË∂äÁöÑÊúâÊïàÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºàMLLMÔºâÂú®Â§ÑÁêÜËßÜËßâ‰ø°ÊÅØÊó∂ÔºåÈúÄË¶ÅÊ∂àËÄóÂ§ßÈáèÁöÑËÆ°ÁÆóËµÑÊ∫êÔºåËøô‰∏ªË¶ÅÊòØÁî±‰∫éËßÜËßâtokensÁöÑÊï∞ÈáèÂ∫ûÂ§ß„ÄÇÁé∞ÊúâÁöÑÂéãÁº©ËßÜËßâtokensÁöÑÊñπÊ≥ïÔºåËôΩÁÑ∂ËÉΩÂ§üÂáèÂ∞ëËÆ°ÁÆóÈáèÔºå‰ΩÜÊòØ‰ºöÂºïÂÖ•ËæÉÂ§ßÁöÑÁâπÂæÅÁ©∫Èó¥Êâ∞Âä®Ôºå‰ΩøÂæóÊ®°ÂûãÁöÑËÆ≠ÁªÉÂèòÂæóÊõ¥Âä†Âõ∞ÈöæÔºåÂèÇÊï∞Á©∫Èó¥Èöæ‰ª•ÈÄÇÂ∫îËøôÁßçÁ™ÅÂèò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊ∏êËøõÂºèÂ≠¶‰π†ÁöÑÊñπÂºèÔºåÈÄêÊ≠•Âú∞ÂéãÁº©ËßÜËßâtokensÔºå‰ªéËÄåÈôç‰ΩéËÆ≠ÁªÉÁöÑÈöæÂ∫¶„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ∞ÜÁâπÂæÅÁ©∫Èó¥Êâ∞Âä®ÂàÜËß£‰∏∫tokenÁª¥Â∫¶ÂíåÂ±ÇÁª¥Â∫¶ÔºåÂπ∂ÂàÜÂà´ËøõË°å‰∏ÄËá¥ÊÄßËí∏È¶èÔºå‰ΩøÂæóÂ≠¶ÁîüÊ®°ÂûãËÉΩÂ§üÈÄêÊ≠•Âú∞Â≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂéãÁº©ÂêéÁöÑÁâπÂæÅÁ©∫Èó¥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEPICÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÊïôÂ∏àÊ®°ÂûãËÆ≠ÁªÉÈò∂ÊÆµÂíåÂ≠¶ÁîüÊ®°ÂûãËÆ≠ÁªÉÈò∂ÊÆµ„ÄÇÂú®ÊïôÂ∏àÊ®°ÂûãËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ΩøÁî®ÂéüÂßãÁöÑ„ÄÅÊú™ÂéãÁº©ÁöÑËßÜËßâtokensËÆ≠ÁªÉ‰∏Ä‰∏™È´òÊÄßËÉΩÁöÑMLLM„ÄÇÂú®Â≠¶ÁîüÊ®°ÂûãËÆ≠ÁªÉÈò∂ÊÆµÔºåÈ¶ñÂÖàÂØπËßÜËßâtokensËøõË°åÂéãÁº©ÔºåÁÑ∂Âêé‰ΩøÁî®ÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫‰Ωú‰∏∫ÊåáÂØºÔºåÈÄöËøátoken‰∏ÄËá¥ÊÄßËí∏È¶èÂíåÂ±Ç‰∏ÄËá¥ÊÄßËí∏È¶èÊù•ËÆ≠ÁªÉÂ≠¶ÁîüÊ®°Âûã„ÄÇtoken‰∏ÄËá¥ÊÄßËí∏È¶èÊó®Âú®‰øùËØÅÂ≠¶ÁîüÊ®°ÂûãÂú®tokenÁª¥Â∫¶‰∏ä‰∏éÊïôÂ∏àÊ®°Âûã‰øùÊåÅ‰∏ÄËá¥ÔºåËÄåÂ±Ç‰∏ÄËá¥ÊÄßËí∏È¶èÊó®Âú®‰øùËØÅÂ≠¶ÁîüÊ®°ÂûãÂú®Â±ÇÁª¥Â∫¶‰∏ä‰∏éÊïôÂ∏àÊ®°Âûã‰øùÊåÅ‰∏ÄËá¥„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÊ∏êËøõ‰∏ÄËá¥ÊÄßËí∏È¶èÁöÑÊÄùÊÉ≥ÔºåÂ∞ÜÁâπÂæÅÁ©∫Èó¥Êâ∞Âä®ÂàÜËß£‰∏∫tokenÁª¥Â∫¶ÂíåÂ±ÇÁª¥Â∫¶ÔºåÂπ∂ÂàÜÂà´ËøõË°å‰∏ÄËá¥ÊÄßËí∏È¶è„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Èôç‰ΩéËÆ≠ÁªÉÈöæÂ∫¶Ôºå‰ΩøÂæóÂ≠¶ÁîüÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Â≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®token‰∏ÄËá¥ÊÄßËí∏È¶è‰∏≠Ôºå‰ΩøÁî®KLÊï£Â∫¶Êù•Ë°°ÈáèÂ≠¶ÁîüÊ®°ÂûãÂíåÊïôÂ∏àÊ®°ÂûãÂú®tokenÁª¥Â∫¶‰∏äÁöÑËæìÂá∫ÂàÜÂ∏ÉÁöÑÂ∑ÆÂºÇ„ÄÇÂú®Â±Ç‰∏ÄËá¥ÊÄßËí∏È¶è‰∏≠Ôºå‰ΩøÁî®MSEÊçüÂ§±Êù•Ë°°ÈáèÂ≠¶ÁîüÊ®°ÂûãÂíåÊïôÂ∏àÊ®°ÂûãÂú®ÊØè‰∏ÄÂ±ÇÁöÑÁâπÂæÅË°®Á§∫ÁöÑÂ∑ÆÂºÇ„ÄÇÊ≠§Â§ñÔºåËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™Ê∏êËøõÂºèÁöÑÂ≠¶‰π†Á≠ñÁï•ÔºåÈÄêÊ≠•Âú∞Â¢ûÂä†tokenÂéãÁº©ÁöÑÊØî‰æãÔºå‰ªéËÄå‰ΩøÂæóÂ≠¶ÁîüÊ®°ÂûãËÉΩÂ§üÈÄêÊ≠•Âú∞ÈÄÇÂ∫îÂéãÁº©ÂêéÁöÑÁâπÂæÅÁ©∫Èó¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEPICÊ°ÜÊû∂Âú®Â§ö‰∏™Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏äÔºåEPICÊ°ÜÊû∂Âú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÂ∞ÜËÆ°ÁÆóÈáèÈôç‰Ωé30%„ÄÇÊ≠§Â§ñÔºåEPICÊ°ÜÊû∂ËøòË°®Áé∞Âá∫‰∫ÜËâØÂ•ΩÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåËÉΩÂ§üÂú®‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíåÊ®°ÂûãÊû∂ÊûÑ‰∏äÁ®≥ÂÆöÂú∞Â∑•‰Ωú„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈ´òÊïàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂ§ÑÁêÜÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á‰∏äÁöÑËßÜËßâÈóÆÁ≠î„ÄÅËá™Âä®È©æÈ©∂‰∏≠ÁöÑÂú∫ÊôØÁêÜËß£„ÄÅ‰ª•ÂèäÊú∫Âô®‰∫∫ÂØºËà™Á≠â„ÄÇÈÄöËøáÈôç‰ΩéÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁöÑËÆ°ÁÆóÊàêÊú¨ÔºåÂèØ‰ª•‰ΩøÂÖ∂Êõ¥ÂÆπÊòìÈÉ®ÁΩ≤Âú®ËµÑÊ∫êÂèóÈôêÁöÑÂπ≥Âè∞‰∏äÔºå‰ªéËÄåÊé®Âä®‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁöÑÊôÆÂèä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Visual tokens consume substantial computational resources in multi-modal large models (MLLMs), significantly compromising their efficiency. Recent works have attempted to improve efficiency by compressing visual tokens during training, either through modifications to model components or by introducing additional parameters. However, they often overlook the increased learning difficulty caused by such compression, as the model's parameter space struggles to quickly adapt to the substantial perturbations in the feature space induced by token compression. In this work, we propose to develop Efficient MLLMs via Progressive Consistency Distillation (EPIC), a progressive learning framework. Specifically, by decomposing the feature space perturbations introduced by token compression along the token-wise and layer-wise dimensions, we introduce token consistency distillation and layer consistency distillation, respectively, aiming to reduce the training difficulty by leveraging guidance from a teacher model and following a progressive learning trajectory. Extensive experiments demonstrate the superior effectiveness, robustness, and generalization capabilities of our proposed framework.

