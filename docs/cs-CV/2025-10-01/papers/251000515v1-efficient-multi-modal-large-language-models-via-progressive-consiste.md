---
layout: default
title: Efficient Multi-modal Large Language Models via Progressive Consistency Distillation
---

# Efficient Multi-modal Large Language Models via Progressive Consistency Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00515" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00515v1</a>
  <a href="https://arxiv.org/pdf/2510.00515.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00515v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00515v1', 'Efficient Multi-modal Large Language Models via Progressive Consistency Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zichen Wen, Shaobo Wang, Yufa Zhou, Junyuan Zhang, Qintong Zhang, Yifeng Gao, Zhaorun Chen, Bin Wang, Weijia Li, Conghui He, Linfeng Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: Accepted by NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEPICæ¡†æ¶ï¼Œé€šè¿‡æ¸è¿›ä¸€è‡´æ€§è’¸é¦æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§æ¨¡å‹` `æ¨¡å‹å‹ç¼©` `çŸ¥è¯†è’¸é¦` `æ¸è¿›å­¦ä¹ ` `ä¸€è‡´æ€§å­¦ä¹ ` `è§†è§‰é—®ç­”` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§æ¨¡å‹æ•ˆç‡å—è§†è§‰tokensè®¡ç®—é‡é™åˆ¶ï¼Œç°æœ‰å‹ç¼©æ–¹æ³•å¿½ç•¥äº†å‹ç¼©å¸¦æ¥çš„å­¦ä¹ éš¾åº¦å¢åŠ ã€‚
2. EPICæ¡†æ¶é€šè¿‡æ¸è¿›ä¸€è‡´æ€§è’¸é¦ï¼Œåˆ†è§£ç‰¹å¾ç©ºé—´æ‰°åŠ¨ï¼Œé™ä½è®­ç»ƒéš¾åº¦ï¼Œæå‡æ¨¡å‹æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEPICæ¡†æ¶åœ¨æœ‰æ•ˆæ€§ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§æ¨¡å‹(MLLM)ä¸­ï¼Œè§†è§‰tokensæ¶ˆè€—äº†å¤§é‡çš„è®¡ç®—èµ„æºï¼Œä¸¥é‡å½±å“äº†æ•ˆç‡ã€‚ç›®å‰çš„å·¥ä½œè¯•å›¾é€šè¿‡åœ¨è®­ç»ƒæœŸé—´å‹ç¼©è§†è§‰tokensæ¥æé«˜æ•ˆç‡ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸å¿½ç•¥äº†ç”±æ­¤å¸¦æ¥çš„å­¦ä¹ éš¾åº¦å¢åŠ ï¼Œå› ä¸ºæ¨¡å‹çš„å‚æ•°ç©ºé—´éš¾ä»¥å¿«é€Ÿé€‚åº”tokenå‹ç¼©å¼•èµ·çš„ç‰¹å¾ç©ºé—´ä¸­çš„å·¨å¤§æ‰°åŠ¨ã€‚æœ¬æ–‡æå‡ºäº†é€šè¿‡æ¸è¿›ä¸€è‡´æ€§è’¸é¦(EPIC)æ¥å¼€å‘é«˜æ•ˆçš„MLLMï¼Œè¿™æ˜¯ä¸€ä¸ªæ¸è¿›å¼å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å°†tokenå‹ç¼©å¼•å…¥çš„ç‰¹å¾ç©ºé—´æ‰°åŠ¨åˆ†è§£ä¸ºtokenç»´åº¦å’Œå±‚ç»´åº¦ï¼Œåˆ†åˆ«å¼•å…¥tokenä¸€è‡´æ€§è’¸é¦å’Œå±‚ä¸€è‡´æ€§è’¸é¦ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼å¹¶éµå¾ªæ¸è¿›å¼å­¦ä¹ è½¨è¿¹æ¥é™ä½è®­ç»ƒéš¾åº¦ã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¡†æ¶å…·æœ‰å“è¶Šçš„æœ‰æ•ˆæ€§ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†è§†è§‰ä¿¡æ¯æ—¶ï¼Œéœ€è¦æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè§†è§‰tokensçš„æ•°é‡åºå¤§ã€‚ç°æœ‰çš„å‹ç¼©è§†è§‰tokensçš„æ–¹æ³•ï¼Œè™½ç„¶èƒ½å¤Ÿå‡å°‘è®¡ç®—é‡ï¼Œä½†æ˜¯ä¼šå¼•å…¥è¾ƒå¤§çš„ç‰¹å¾ç©ºé—´æ‰°åŠ¨ï¼Œä½¿å¾—æ¨¡å‹çš„è®­ç»ƒå˜å¾—æ›´åŠ å›°éš¾ï¼Œå‚æ•°ç©ºé—´éš¾ä»¥é€‚åº”è¿™ç§çªå˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¸è¿›å¼å­¦ä¹ çš„æ–¹å¼ï¼Œé€æ­¥åœ°å‹ç¼©è§†è§‰tokensï¼Œä»è€Œé™ä½è®­ç»ƒçš„éš¾åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå°†ç‰¹å¾ç©ºé—´æ‰°åŠ¨åˆ†è§£ä¸ºtokenç»´åº¦å’Œå±‚ç»´åº¦ï¼Œå¹¶åˆ†åˆ«è¿›è¡Œä¸€è‡´æ€§è’¸é¦ï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿé€æ­¥åœ°å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”å‹ç¼©åçš„ç‰¹å¾ç©ºé—´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEPICæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šæ•™å¸ˆæ¨¡å‹è®­ç»ƒé˜¶æ®µå’Œå­¦ç”Ÿæ¨¡å‹è®­ç»ƒé˜¶æ®µã€‚åœ¨æ•™å¸ˆæ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨åŸå§‹çš„ã€æœªå‹ç¼©çš„è§†è§‰tokensè®­ç»ƒä¸€ä¸ªé«˜æ€§èƒ½çš„MLLMã€‚åœ¨å­¦ç”Ÿæ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œé¦–å…ˆå¯¹è§†è§‰tokensè¿›è¡Œå‹ç¼©ï¼Œç„¶åä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºä½œä¸ºæŒ‡å¯¼ï¼Œé€šè¿‡tokenä¸€è‡´æ€§è’¸é¦å’Œå±‚ä¸€è‡´æ€§è’¸é¦æ¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚tokenä¸€è‡´æ€§è’¸é¦æ—¨åœ¨ä¿è¯å­¦ç”Ÿæ¨¡å‹åœ¨tokenç»´åº¦ä¸Šä¸æ•™å¸ˆæ¨¡å‹ä¿æŒä¸€è‡´ï¼Œè€Œå±‚ä¸€è‡´æ€§è’¸é¦æ—¨åœ¨ä¿è¯å­¦ç”Ÿæ¨¡å‹åœ¨å±‚ç»´åº¦ä¸Šä¸æ•™å¸ˆæ¨¡å‹ä¿æŒä¸€è‡´ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ¸è¿›ä¸€è‡´æ€§è’¸é¦çš„æ€æƒ³ï¼Œå°†ç‰¹å¾ç©ºé—´æ‰°åŠ¨åˆ†è§£ä¸ºtokenç»´åº¦å’Œå±‚ç»´åº¦ï¼Œå¹¶åˆ†åˆ«è¿›è¡Œä¸€è‡´æ€§è’¸é¦ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°é™ä½è®­ç»ƒéš¾åº¦ï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨tokenä¸€è‡´æ€§è’¸é¦ä¸­ï¼Œä½¿ç”¨KLæ•£åº¦æ¥è¡¡é‡å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹åœ¨tokenç»´åº¦ä¸Šçš„è¾“å‡ºåˆ†å¸ƒçš„å·®å¼‚ã€‚åœ¨å±‚ä¸€è‡´æ€§è’¸é¦ä¸­ï¼Œä½¿ç”¨MSEæŸå¤±æ¥è¡¡é‡å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹åœ¨æ¯ä¸€å±‚çš„ç‰¹å¾è¡¨ç¤ºçš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªæ¸è¿›å¼çš„å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥åœ°å¢åŠ tokenå‹ç¼©çš„æ¯”ä¾‹ï¼Œä»è€Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿé€æ­¥åœ°é€‚åº”å‹ç¼©åçš„ç‰¹å¾ç©ºé—´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEPICæ¡†æ¶åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šï¼ŒEPICæ¡†æ¶åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå°†è®¡ç®—é‡é™ä½30%ã€‚æ­¤å¤–ï¼ŒEPICæ¡†æ¶è¿˜è¡¨ç°å‡ºäº†è‰¯å¥½çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„æ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šç¨³å®šåœ°å·¥ä½œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆå¤šæ¨¡æ€ä¿¡æ¯å¤„ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„è§†è§‰é—®ç­”ã€è‡ªåŠ¨é©¾é©¶ä¸­çš„åœºæ™¯ç†è§£ã€ä»¥åŠæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡é™ä½å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è®¡ç®—æˆæœ¬ï¼Œå¯ä»¥ä½¿å…¶æ›´å®¹æ˜“éƒ¨ç½²åœ¨èµ„æºå—é™çš„å¹³å°ä¸Šï¼Œä»è€Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual tokens consume substantial computational resources in multi-modal large models (MLLMs), significantly compromising their efficiency. Recent works have attempted to improve efficiency by compressing visual tokens during training, either through modifications to model components or by introducing additional parameters. However, they often overlook the increased learning difficulty caused by such compression, as the model's parameter space struggles to quickly adapt to the substantial perturbations in the feature space induced by token compression. In this work, we propose to develop Efficient MLLMs via Progressive Consistency Distillation (EPIC), a progressive learning framework. Specifically, by decomposing the feature space perturbations introduced by token compression along the token-wise and layer-wise dimensions, we introduce token consistency distillation and layer consistency distillation, respectively, aiming to reduce the training difficulty by leveraging guidance from a teacher model and following a progressive learning trajectory. Extensive experiments demonstrate the superior effectiveness, robustness, and generalization capabilities of our proposed framework.

