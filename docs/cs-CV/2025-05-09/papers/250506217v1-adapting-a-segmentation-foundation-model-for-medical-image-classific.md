---
layout: default
title: Adapting a Segmentation Foundation Model for Medical Image Classification
---

# Adapting a Segmentation Foundation Model for Medical Image Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06217" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06217v1</a>
  <a href="https://arxiv.org/pdf/2505.06217.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06217v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06217v1', 'Adapting a Segmentation Foundation Model for Medical Image Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pengfei Gu, Haoteng Tang, Islam A. Ebeid, Jose A. Nunez, Fabian Vazquez, Diego Adame, Marcus Zhan, Huimin Li, Bin Fu, Danny Z. Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥é€‚åº”SAMæ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†ç±»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»å­¦å›¾åƒåˆ†ç±»` `åŸºç¡€æ¨¡å‹` `å›¾åƒåˆ†å‰²` `ç©ºé—´æ³¨æ„åŠ›` `æ·±åº¦å­¦ä¹ ` `ç‰¹å¾æå–` `SLCAæœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒ»å­¦å›¾åƒåˆ†ç±»æ–¹æ³•åœ¨é€‚åº”åŸºç¡€æ¨¡å‹æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å›¾åƒåˆ†å‰²ç‰¹å¾ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡å†»ç»“SAMçš„å›¾åƒç¼–ç å™¨å¹¶å¼•å…¥SLCAæœºåˆ¶ï¼Œå¢å¼ºæ¨¡å‹å¯¹ç©ºé—´ç‰¹å¾çš„å…³æ³¨ã€‚
3. åœ¨ä¸‰ä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ•°æ®æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒåŸºç¡€æ¨¡å‹å¦‚Segment Anything Modelï¼ˆSAMï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒåˆ†å‰²æ–¹é¢ï¼Œå±•ç°äº†å¼ºå¤§çš„é›¶-shotåˆ†å‰²èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹æœ‰æ•ˆé€‚åº”äºåŒ»å­¦å›¾åƒåˆ†ç±»ä»ç„¶æ˜¯ä¸€ä¸ªè¾ƒå°‘æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†SAMç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨SAMçš„å›¾åƒç¼–ç å™¨ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œæ•æ‰ä¼ è¾¾é‡è¦ç©ºé—´å’Œä¸Šä¸‹æ–‡ç»†èŠ‚çš„åˆ†å‰²ç‰¹å¾ï¼ŒåŒæ—¶å†»ç»“å…¶æƒé‡ä»¥é¿å…è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸å¿…è¦å¼€é”€ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç©ºé—´å±€éƒ¨é€šé“æ³¨æ„åŠ›ï¼ˆSLCAï¼‰æœºåˆ¶ï¼Œç”¨äºè®¡ç®—ç‰¹å¾å›¾çš„ç©ºé—´å±€éƒ¨æ³¨æ„åŠ›æƒé‡ã€‚é€šè¿‡SLCAå¤„ç†çš„ç‰¹å¾è¢«æ•´åˆåˆ°æ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡å‹ä¸­ï¼Œä»¥å¢å¼ºå…¶å¯¹å›¾åƒä¸­ç©ºé—´ç›¸å…³æˆ–æœ‰æ„ä¹‰åŒºåŸŸçš„å…³æ³¨ï¼Œä»è€Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œæ•°æ®æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆå°†åŸºç¡€æ¨¡å‹SAMé€‚åº”äºåŒ»å­¦å›¾åƒåˆ†ç±»çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨å›¾åƒåˆ†å‰²ç‰¹å¾æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´åˆ†ç±»æ€§èƒ½ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨SAMçš„å›¾åƒç¼–ç å™¨ä½œä¸ºç‰¹å¾æå–å™¨ï¼ŒåŒæ—¶å¼•å…¥SLCAæœºåˆ¶æ¥è®¡ç®—ç©ºé—´å±€éƒ¨æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œæå‡åˆ†ç±»æ¨¡å‹å¯¹é‡è¦åŒºåŸŸçš„å…³æ³¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯ç‰¹å¾æå–æ¨¡å—ï¼Œåˆ©ç”¨SAMçš„å›¾åƒç¼–ç å™¨æå–ç‰¹å¾ï¼›å…¶æ¬¡æ˜¯SLCAæ¨¡å—ï¼Œé€šè¿‡è®¡ç®—æ³¨æ„åŠ›æƒé‡æ¥å¢å¼ºç‰¹å¾è¡¨ç¤ºï¼Œæœ€ç»ˆå°†å…¶æ•´åˆåˆ°åˆ†ç±»æ¨¡å‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥SLCAæœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿè®¡ç®—ç©ºé—´å±€éƒ¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œä¸ä¼ ç»Ÿçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼Œæ›´åŠ å…³æ³¨å›¾åƒä¸­çš„é‡è¦åŒºåŸŸã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSAMçš„æƒé‡è¢«å†»ç»“ä»¥å‡å°‘è®­ç»ƒå¼€é”€ï¼ŒSLCAçš„å‚æ•°è®¾ç½®ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿æ³¨æ„åŠ›æƒé‡çš„æœ‰æ•ˆè®¡ç®—ï¼ŒæŸå¤±å‡½æ•°åˆ™é‡‡ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ï¼Œä»¥é€‚åº”åˆ†ç±»ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œåˆ†ç±»å‡†ç¡®ç‡æé«˜äº†çº¦10%-15%ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ•°æ®æ•ˆç‡å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦å½±åƒåˆ†æã€ç–¾ç—…è¯Šæ–­è¾…åŠ©ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¿›è¡Œå›¾åƒåˆ†ç±»å’Œç—…ç¶è¯†åˆ«ï¼Œæå‡ä¸´åºŠå†³ç­–çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨å¹¿è‡³å…¶ä»–åŒ»å­¦å›¾åƒå¤„ç†ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨æ™ºèƒ½åŒ»ç–—çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in foundation models, such as the Segment Anything Model (SAM), have shown strong performance in various vision tasks, particularly image segmentation, due to their impressive zero-shot segmentation capabilities. However, effectively adapting such models for medical image classification is still a less explored topic. In this paper, we introduce a new framework to adapt SAM for medical image classification. First, we utilize the SAM image encoder as a feature extractor to capture segmentation-based features that convey important spatial and contextual details of the image, while freezing its weights to avoid unnecessary overhead during training. Next, we propose a novel Spatially Localized Channel Attention (SLCA) mechanism to compute spatially localized attention weights for the feature maps. The features extracted from SAM's image encoder are processed through SLCA to compute attention weights, which are then integrated into deep learning classification models to enhance their focus on spatially relevant or meaningful regions of the image, thus improving classification performance. Experimental results on three public medical image classification datasets demonstrate the effectiveness and data-efficiency of our approach.

