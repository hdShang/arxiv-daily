---
layout: default
title: VIN-NBV: A View Introspection Network for Next-Best-View Selection
---

# VIN-NBV: A View Introspection Network for Next-Best-View Selection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06219" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06219v3</a>
  <a href="https://arxiv.org/pdf/2505.06219.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06219v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06219v3', 'VIN-NBV: A View Introspection Network for Next-Best-View Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Noah Frahm, Dongxu Zhao, Andrea Dunn Beltran, Ron Alterovitz, Jan-Michael Frahm, Junier Oliva, Roni Sengupta

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09 (æ›´æ–°: 2025-08-25)

**å¤‡æ³¨**: 9 pages, 9 figures, 2 tables. Reformat into two column. Additional experiments and results

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVIN-NBVä»¥è§£å†³å¤æ‚åœºæ™¯ä¸‹çš„ä¸‹ä¸€æœ€ä½³è§†è§’é€‰æ‹©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸‹ä¸€æœ€ä½³è§†è§’` `3Dé‡å»º` `è§†è§’å†…çœç½‘ç»œ` `æ·±åº¦å­¦ä¹ ` `æœºå™¨äººè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„NBVç®—æ³•å¾€å¾€ä¾èµ–äºè¦†ç›–ç‡æœ€å¤§åŒ–ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸­é‡å»ºè´¨é‡ä¸ä½³ï¼Œå°¤å…¶æ˜¯å­˜åœ¨é®æŒ¡å’Œç»†èŠ‚æ—¶ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è·å–ç­–ç•¥ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„è§†è§’å†…çœç½‘ç»œï¼ˆVINï¼‰ï¼Œç›´æ¥ä¼˜åŒ–é‡å»ºè´¨é‡è€Œéè¦†ç›–ç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RRIæ ‡å‡†çš„VIN-NBVåœ¨é‡å»ºè´¨é‡ä¸Šæ¯”ä¼ ç»Ÿæ–¹æ³•æå‡çº¦30%ï¼Œå¹¶ä¸”åœ¨ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ¯”è¾ƒä¸­è¡¨ç°æ›´ä¼˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸‹ä¸€æœ€ä½³è§†è§’ï¼ˆNBVï¼‰ç®—æ³•æ—¨åœ¨ä»¥æœ€å°èµ„æºæœ€å¤§åŒ–3Dåœºæ™¯è·å–è´¨é‡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–è¦†ç›–ç‡æœ€å¤§åŒ–ä½œä¸ºé‡å»ºè´¨é‡çš„ä»£ç†ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸­ï¼Œè¿™ç§æ–¹æ³•å¾€å¾€ä¸è¶³ä»¥ä¿è¯é‡å»ºè´¨é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è·å–ç­–ç•¥ï¼Œç›´æ¥ä¼˜åŒ–é‡å»ºè´¨é‡ï¼Œè€Œä¸ä»…ä»…æ˜¯è¦†ç›–ç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†è§†è§’å†…çœç½‘ç»œï¼ˆVINï¼‰ï¼Œè¯¥è½»é‡çº§ç¥ç»ç½‘ç»œèƒ½å¤Ÿé¢„æµ‹æ½œåœ¨ä¸‹ä¸€ä¸ªè§†ç‚¹çš„ç›¸å¯¹é‡å»ºæ”¹è¿›ï¼ˆRRIï¼‰ï¼Œæ— éœ€è¿›è¡Œæ–°è·å–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„VIN-NBVæ–¹æ³•åœ¨é‡å»ºè´¨é‡ä¸Šç›¸è¾ƒäºåŸºäºè¦†ç›–ç‡çš„æ ‡å‡†æå‡äº†çº¦30%ã€‚æ­¤å¤–ï¼ŒVIN-NBVè¿˜è¶…è¶Šäº†æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•Scan-RLå’ŒGenNBVï¼Œæå‡å¹…åº¦çº¦ä¸º40%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤æ‚3Dåœºæ™¯ä¸­ä¸‹ä¸€æœ€ä½³è§†è§’é€‰æ‹©çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºè¦†ç›–ç‡æœ€å¤§åŒ–ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†é®æŒ¡å’Œç»†èŠ‚ä¸°å¯Œçš„åœºæ™¯ï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è·å–ç­–ç•¥ï¼Œç›´æ¥ä¼˜åŒ–é‡å»ºè´¨é‡ã€‚é€šè¿‡è®­ç»ƒè§†è§’å†…çœç½‘ç»œï¼ˆVINï¼‰ï¼Œé¢„æµ‹æ½œåœ¨è§†ç‚¹çš„ç›¸å¯¹é‡å»ºæ”¹è¿›ï¼ˆRRIï¼‰ï¼Œä»è€Œé¿å…äº†æ–°è·å–çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§’å†…çœç½‘ç»œï¼ˆVINï¼‰å’ŒåŸºäºè´ªå©ªç­–ç•¥çš„åºåˆ—é‡‡æ ·NBVç­–ç•¥ã€‚VINè´Ÿè´£é¢„æµ‹RRIï¼Œè€ŒNBVç­–ç•¥åˆ™æ ¹æ®è¿™äº›é¢„æµ‹é€‰æ‹©æœ€ä½³è§†è§’ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†RRIä½œä¸ºä¼˜åŒ–æ ‡å‡†ï¼Œç›´æ¥é’ˆå¯¹é‡å»ºè´¨é‡è¿›è¡Œä¼˜åŒ–ï¼Œè€Œéä¾èµ–äºè¦†ç›–ç‡ï¼Œè¿™ä¸€è®¾è®¡ä½¿å¾—æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°æ›´ä½³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼ŒVINæ˜¯ä¸€ä¸ªè½»é‡çº§ç¥ç»ç½‘ç»œï¼Œè®¾è®¡ä¸Šæ³¨é‡é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨æœ€å°åŒ–é¢„æµ‹è¯¯å·®ï¼Œä»¥æé«˜RRIçš„é¢„æµ‹ç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVIN-NBVåœ¨é‡å»ºè´¨é‡ä¸Šæ¯”åŸºäºè¦†ç›–ç‡çš„æ ‡å‡†æå‡çº¦30%ï¼Œå¹¶ä¸”åœ¨ä¸Scan-RLå’ŒGenNBVç­‰æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ¯”è¾ƒä¸­ï¼ŒVIN-NBVçš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°çº¦40%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡ä¼˜åŒ–3Dåœºæ™¯çš„è·å–è´¨é‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¿™äº›é¢†åŸŸçš„ç³»ç»Ÿæ€§èƒ½å’Œç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Next Best View (NBV) algorithms aim to maximize 3D scene acquisition quality using minimal resources, e.g. number of acquisitions, time taken, or distance traversed. Prior methods often rely on coverage maximization as a proxy for reconstruction quality, but for complex scenes with occlusions and finer details, this is not always sufficient and leads to poor reconstructions. Our key insight is to train an acquisition policy that directly optimizes for reconstruction quality rather than just coverage. To achieve this, we introduce the View Introspection Network (VIN): a lightweight neural network that predicts the Relative Reconstruction Improvement (RRI) of a potential next viewpoint without making any new acquisitions. We use this network to power a simple, yet effective, sequential samplingbased greedy NBV policy. Our approach, VIN-NBV, generalizes to unseen object categories, operates without prior scene knowledge, is adaptable to resource constraints, and can handle occlusions. We show that our RRI fitness criterion leads to a ~30% gain in reconstruction quality over a coverage-based criterion using the same greedy strategy. Furthermore, VIN-NBV also outperforms deep reinforcement learning methods, Scan-RL and GenNBV, by ~40%.

