---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-19
---

# cs.CVï¼ˆ2025-05-19ï¼‰

ğŸ“Š å…± **45** ç¯‡è®ºæ–‡
 | ğŸ”— **12** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (16 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (11 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (16 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250513436v1-kintwin-imitation-learning-with-torque-and-muscle-driven-biomechanic.html">KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture</a></td>
  <td>æå‡ºKinTwinä»¥è§£å†³è¿åŠ¨åˆ†æä¸­çš„é€†åŠ¨åŠ›å­¦è®¡ç®—é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">markerless motion capture</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13436v1" data-paper-url="./papers/250513436v1-kintwin-imitation-learning-with-torque-and-muscle-driven-biomechanic.html" onclick="toggleFavorite(this, '2505.13436v1', 'KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250513261v2-unlocking-the-potential-of-difficulty-prior-in-rl-based-multimodal-r.html">Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning</a></td>
  <td>é€šè¿‡å›°éš¾å…ˆéªŒå»ºæ¨¡æå‡å¤šæ¨¡æ€æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13261v2" data-paper-url="./papers/250513261v2-unlocking-the-potential-of-difficulty-prior-in-rl-based-multimodal-r.html" onclick="toggleFavorite(this, '2505.13261v2', 'Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250512685v1-mamba-adaptor-state-space-model-adaptor-for-visual-recognition.html">Mamba-Adaptor: State Space Model Adaptor for Visual Recognition</a></td>
  <td>æå‡ºMamba-Adaptorä»¥è§£å†³è§†è§‰è¯†åˆ«ä¸­çš„é•¿ç¨‹é—å¿˜å’Œç©ºé—´å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12685v1" data-paper-url="./papers/250512685v1-mamba-adaptor-state-space-model-adaptor-for-visual-recognition.html" onclick="toggleFavorite(this, '2505.12685v1', 'Mamba-Adaptor: State Space Model Adaptor for Visual Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250513426v1-g1-bootstrapping-perception-and-reasoning-abilities-of-vision-langua.html">G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning</a></td>
  <td>æå‡ºVLM-Gymä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆä¸­çš„å†³ç­–èƒ½åŠ›ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13426v1" data-paper-url="./papers/250513426v1-g1-bootstrapping-perception-and-reasoning-abilities-of-vision-langua.html" onclick="toggleFavorite(this, '2505.13426v1', 'G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250512656v2-spklip-aligning-spike-video-streams-with-natural-language.html">SPKLIP: Aligning Spike Video Streams with Natural Language</a></td>
  <td>æå‡ºSPKLIPä»¥è§£å†³Spikeè§†é¢‘ä¸è‡ªç„¶è¯­è¨€å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">VLA</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12656v2" data-paper-url="./papers/250512656v2-spklip-aligning-spike-video-streams-with-natural-language.html" onclick="toggleFavorite(this, '2505.12656v2', 'SPKLIP: Aligning Spike Video Streams with Natural Language')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250512650v1-automat-enabling-automated-crystal-structure-reconstruction-from-mic.html">AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use</a></td>
  <td>æå‡ºAutoMatä»¥è§£å†³æ˜¾å¾®é•œå›¾åƒè½¬åŒ–ä¸ºæ™¶ä½“ç»“æ„çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12650v1" data-paper-url="./papers/250512650v1-automat-enabling-automated-crystal-structure-reconstruction-from-mic.html" onclick="toggleFavorite(this, '2505.12650v1', 'AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250512620v6-busterx-mllm-powered-ai-generated-video-forgery-detection-and-explan.html">BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation</a></td>
  <td>æå‡ºBusterXæ¡†æ¶ä»¥è§£å†³AIç”Ÿæˆè§†é¢‘ä¼ªé€ æ£€æµ‹ä¸è§£é‡Šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12620v6" data-paper-url="./papers/250512620v6-busterx-mllm-powered-ai-generated-video-forgery-detection-and-explan.html" onclick="toggleFavorite(this, '2505.12620v6', 'BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250512674v1-few-step-diffusion-via-score-identity-distillation.html">Few-Step Diffusion via Score identity Distillation</a></td>
  <td>æå‡ºScore identity Distillationä»¥è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">classifier-free guidance</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12674v1" data-paper-url="./papers/250512674v1-few-step-diffusion-via-score-identity-distillation.html" onclick="toggleFavorite(this, '2505.12674v1', 'Few-Step Diffusion via Score identity Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250513777v1-sat2sound-a-unified-framework-for-zero-shot-soundscape-mapping.html">Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping</a></td>
  <td>æå‡ºSat2Soundæ¡†æ¶ä»¥è§£å†³å£°éŸ³æ™¯è§‚æ˜ å°„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13777v1" data-paper-url="./papers/250513777v1-sat2sound-a-unified-framework-for-zero-shot-soundscape-mapping.html" onclick="toggleFavorite(this, '2505.13777v1', 'Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250512667v2-safe-sora-safe-text-to-video-generation-via-graphical-watermarking.html">Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking</a></td>
  <td>æå‡ºSafe-Soraä»¥è§£å†³AIç”Ÿæˆè§†é¢‘ç‰ˆæƒä¿æŠ¤é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12667v2" data-paper-url="./papers/250512667v2-safe-sora-safe-text-to-video-generation-via-graphical-watermarking.html" onclick="toggleFavorite(this, '2505.12667v2', 'Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250513300v3-dd-ranking-rethinking-the-evaluation-of-dataset-distillation.html">DD-Ranking: Rethinking the Evaluation of Dataset Distillation</a></td>
  <td>æå‡ºDD-Rankingä»¥è§£å†³æ•°æ®é›†è’¸é¦è¯„ä¼°ä¸å‡†ç¡®çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13300v3" data-paper-url="./papers/250513300v3-dd-ranking-rethinking-the-evaluation-of-dataset-distillation.html" onclick="toggleFavorite(this, '2505.13300v3', 'DD-Ranking: Rethinking the Evaluation of Dataset Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250512861v2-rmmss-towards-advanced-robust-multi-modal-semantic-segmentation-with.html">RMMSS: Towards Advanced Robust Multi-Modal Semantic Segmentation with Hybrid Prototype Distillation and Feature Selection</a></td>
  <td>æå‡ºRMMSSä»¥è§£å†³å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²ä¸­çš„é²æ£’æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12861v2" data-paper-url="./papers/250512861v2-rmmss-towards-advanced-robust-multi-modal-semantic-segmentation-with.html" onclick="toggleFavorite(this, '2505.12861v2', 'RMMSS: Towards Advanced Robust Multi-Modal Semantic Segmentation with Hybrid Prototype Distillation and Feature Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250512580v2-coarse-attribute-prediction-with-task-agnostic-distillation-for-real.html">Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID</a></td>
  <td>æå‡ºRLQæ¡†æ¶ä»¥è§£å†³ä½è´¨é‡å›¾åƒä¸‹çš„æœè£…å˜åŒ–é‡è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12580v2" data-paper-url="./papers/250512580v2-coarse-attribute-prediction-with-task-agnostic-distillation-for-real.html" onclick="toggleFavorite(this, '2505.12580v2', 'Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250513344v2-ropecraft-training-free-motion-transfer-with-trajectory-guided-rope-.html">RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers</a></td>
  <td>æå‡ºRoPECraftä»¥è§£å†³è§†é¢‘è¿åŠ¨è½¬ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13344v2" data-paper-url="./papers/250513344v2-ropecraft-training-free-motion-transfer-with-trajectory-guided-rope-.html" onclick="toggleFavorite(this, '2505.13344v2', 'RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250513091v1-touch2shape-touch-conditioned-3d-diffusion-for-shape-exploration-and.html">Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction</a></td>
  <td>æå‡ºTouch2Shapeä»¥è§£å†³3Då½¢çŠ¶é‡å»ºä¸­çš„å±€éƒ¨ç»†èŠ‚æ•æ‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">reward design</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13091v1" data-paper-url="./papers/250513091v1-touch2shape-touch-conditioned-3d-diffusion-for-shape-exploration-and.html" onclick="toggleFavorite(this, '2505.13091v1', 'Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250512903v1-towards-low-latency-event-stream-based-visual-object-tracking-a-slow.html">Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach</a></td>
  <td>æå‡ºSlow-Fastè·Ÿè¸ªæ–¹æ³•ä»¥è§£å†³ä½å»¶è¿Ÿè§†è§‰ç›®æ ‡è·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12903v1" data-paper-url="./papers/250512903v1-towards-low-latency-event-stream-based-visual-object-tracking-a-slow.html" onclick="toggleFavorite(this, '2505.12903v1', 'Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250513419v1-feallm-advancing-facial-emotion-analysis-in-multimodal-large-languag.html">FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning</a></td>
  <td>æå‡ºFEALLMä»¥è§£å†³é¢éƒ¨æƒ…æ„Ÿåˆ†æä¸­çš„å¤šæ¨¡æ€æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13419v1" data-paper-url="./papers/250513419v1-feallm-advancing-facial-emotion-analysis-in-multimodal-large-languag.html" onclick="toggleFavorite(this, '2505.13419v1', 'FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250512890v2-specialized-foundation-models-for-intelligent-operating-rooms.html">Specialized Foundation Models for Intelligent Operating Rooms</a></td>
  <td>æå‡ºORQAæ¨¡å‹ä»¥è§£å†³æ‰‹æœ¯å®¤æ™ºèƒ½åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12890v2" data-paper-url="./papers/250512890v2-specialized-foundation-models-for-intelligent-operating-rooms.html" onclick="toggleFavorite(this, '2505.12890v2', 'Specialized Foundation Models for Intelligent Operating Rooms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250513212v3-semantic-change-detection-of-roads-and-bridges-a-fine-grained-datase.html">Semantic Change Detection of Roads and Bridges: A Fine-grained Dataset and Multimodal Frequency-driven Detector</a></td>
  <td>æå‡ºå¤šæ¨¡æ€é¢‘ç‡é©±åŠ¨æ£€æµ‹å™¨ä»¥è§£å†³é“è·¯ä¸æ¡¥æ¢è¯­ä¹‰å˜åŒ–æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13212v3" data-paper-url="./papers/250513212v3-semantic-change-detection-of-roads-and-bridges-a-fine-grained-datase.html" onclick="toggleFavorite(this, '2505.13212v3', 'Semantic Change Detection of Roads and Bridges: A Fine-grained Dataset and Multimodal Frequency-driven Detector')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250512766v1-reasoning-ocr-can-large-multimodal-models-solve-complex-logical-reas.html">Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?</a></td>
  <td>æå‡ºReasoning-OCRä»¥è§£å†³å¤æ‚é€»è¾‘æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12766v1" data-paper-url="./papers/250512766v1-reasoning-ocr-can-large-multimodal-models-solve-complex-logical-reas.html" onclick="toggleFavorite(this, '2505.12766v1', 'Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250512728v2-flash-latent-aware-semi-autoregressive-speculative-decoding-for-mult.html">FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks</a></td>
  <td>æå‡ºFLASHä»¥è§£å†³å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è§£ç é€Ÿåº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12728v2" data-paper-url="./papers/250512728v2-flash-latent-aware-semi-autoregressive-speculative-decoding-for-mult.html" onclick="toggleFavorite(this, '2505.12728v2', 'FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250512715v1-vlc-fusion-vision-language-conditioned-sensor-fusion-for-robust-obje.html">VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection</a></td>
  <td>æå‡ºVLC Fusionä»¥è§£å†³å¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆä¸­çš„ç¯å¢ƒé€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">language conditioned</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12715v1" data-paper-url="./papers/250512715v1-vlc-fusion-vision-language-conditioned-sensor-fusion-for-robust-obje.html" onclick="toggleFavorite(this, '2505.12715v1', 'VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250512711v2-any-to-any-learning-in-computational-pathology-via-triplet-multimoda.html">Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining</a></td>
  <td>æå‡ºALTERæ¡†æ¶ä»¥è§£å†³è®¡ç®—ç—…ç†ä¸­çš„å¤šæ¨¡æ€èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12711v2" data-paper-url="./papers/250512711v2-any-to-any-learning-in-computational-pathology-via-triplet-multimoda.html" onclick="toggleFavorite(this, '2505.12711v2', 'Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250512826v1-mitigating-hallucination-in-videollms-via-temporal-aware-activation-.html">Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering</a></td>
  <td>æå‡ºæ—¶é—´æ„ŸçŸ¥æ¿€æ´»å·¥ç¨‹ä»¥ç¼“è§£è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12826v1" data-paper-url="./papers/250512826v1-mitigating-hallucination-in-videollms-via-temporal-aware-activation-.html" onclick="toggleFavorite(this, '2505.12826v1', 'Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250513281v1-computer-vision-models-show-human-like-sensitivity-to-geometric-and-.html">Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts</a></td>
  <td>åˆ©ç”¨è®¡ç®—æœºè§†è§‰æ¨¡å‹æ¢è®¨äººç±»å¯¹å‡ ä½•ä¸æ‹“æ‰‘æ¦‚å¿µçš„æ•æ„Ÿæ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13281v1" data-paper-url="./papers/250513281v1-computer-vision-models-show-human-like-sensitivity-to-geometric-and-.html" onclick="toggleFavorite(this, '2505.13281v1', 'Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250513233v1-from-local-details-to-global-context-advancing-vision-language-model.html">From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection</a></td>
  <td>æå‡ºæ³¨æ„åŠ›å¼•å¯¼é€‰æ‹©æ–¹æ³•ä»¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13233v1" data-paper-url="./papers/250513233v1-from-local-details-to-global-context-advancing-vision-language-model.html" onclick="toggleFavorite(this, '2505.13233v1', 'From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250513099v2-industrial-synthetic-segment-pre-training.html">Industrial Synthetic Segment Pre-training</a></td>
  <td>æå‡ºå·¥ä¸šåˆæˆåˆ†å‰²é¢„è®­ç»ƒæ•°æ®é›†ä»¥è§£å†³å›¾åƒæ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13099v2" data-paper-url="./papers/250513099v2-industrial-synthetic-segment-pre-training.html" onclick="toggleFavorite(this, '2505.13099v2', 'Industrial Synthetic Segment Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250512632v1-scalable-video-to-dataset-generation-for-cross-platform-mobile-agent.html">Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents</a></td>
  <td>æå‡ºMONDAYæ•°æ®é›†ä»¥è§£å†³è·¨å¹³å°ç§»åŠ¨æ“ä½œç³»ç»Ÿå¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12632v1" data-paper-url="./papers/250512632v1-scalable-video-to-dataset-generation-for-cross-platform-mobile-agent.html" onclick="toggleFavorite(this, '2505.12632v1', 'Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250512605v1-temporal-oriented-recipe-for-transferring-large-vision-language-mode.html">Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding</a></td>
  <td>æå‡ºæ—¶é—´å¯¼å‘é…æ–¹ä»¥æå‡è§†é¢‘ç†è§£ä¸­çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12605v1" data-paper-url="./papers/250512605v1-temporal-oriented-recipe-for-transferring-large-vision-language-mode.html" onclick="toggleFavorite(this, '2505.12605v1', 'Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/250513215v1-hybrid-3d-4d-gaussian-splatting-for-fast-dynamic-scene-representatio.html">Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation</a></td>
  <td>æå‡ºæ··åˆ3D-4Dé«˜æ–¯ç‚¹äº‘ä»¥è§£å†³åŠ¨æ€åœºæ™¯è¡¨ç¤ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13215v1" data-paper-url="./papers/250513215v1-hybrid-3d-4d-gaussian-splatting-for-fast-dynamic-scene-representatio.html" onclick="toggleFavorite(this, '2505.13215v1', 'Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250513061v4-3d-visual-illusion-depth-estimation.html">3D Visual Illusion Depth Estimation</a></td>
  <td>æå‡º3Dè§†è§‰å¹»è§‰æ·±åº¦ä¼°è®¡æ¡†æ¶ä»¥æå‡æ·±åº¦ä¼°è®¡ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13061v4" data-paper-url="./papers/250513061v4-3d-visual-illusion-depth-estimation.html" onclick="toggleFavorite(this, '2505.13061v4', '3D Visual Illusion Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250513309v1-estonefish-scenes-a-synthetically-generated-dataset-for-underwater-e.html">eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks</a></td>
  <td>æå‡ºeStonefish-scenesä»¥è§£å†³æ°´ä¸‹äº‹ä»¶é©±åŠ¨å…‰æµé¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual odometry</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13309v1" data-paper-url="./papers/250513309v1-estonefish-scenes-a-synthetically-generated-dataset-for-underwater-e.html" onclick="toggleFavorite(this, '2505.13309v1', 'eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250513633v1-ipensinteractive-unsupervised-framework-for-rapid-plant-phenotyping-.html">IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion</a></td>
  <td>æå‡ºIPENSä»¥è§£å†³æ¤ç‰©è¡¨å‹æå–ä¸­çš„æ— ç›‘ç£å¤šç›®æ ‡åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13633v1" data-paper-url="./papers/250513633v1-ipensinteractive-unsupervised-framework-for-rapid-plant-phenotyping-.html" onclick="toggleFavorite(this, '2505.13633v1', 'IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250512693v1-tacocctarget-adaptive-cross-modal-fusion-with-volume-rendering-for-3.html">TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy</a></td>
  <td>æå‡ºTACOccä»¥è§£å†³å¤šæ¨¡æ€3Då ç”¨é¢„æµ‹ä¸­çš„èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12693v1" data-paper-url="./papers/250512693v1-tacocctarget-adaptive-cross-modal-fusion-with-volume-rendering-for-3.html" onclick="toggleFavorite(this, '2505.12693v1', 'TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250512660v1-predicting-reaction-time-to-comprehend-scenes-with-foveated-scene-un.html">Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps</a></td>
  <td>æå‡ºF-SUMæ¨¡å‹ä»¥è§£å†³åœºæ™¯ç†è§£ååº”æ—¶é—´é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12660v1" data-paper-url="./papers/250512660v1-predicting-reaction-time-to-comprehend-scenes-with-foveated-scene-un.html" onclick="toggleFavorite(this, '2505.12660v1', 'Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250513440v1-recollection-from-pensieve-novel-view-synthesis-via-learning-from-un.html">Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos</a></td>
  <td>æå‡ºä¸€ç§æ–°æ–¹æ³•ä»¥è§£å†³æ— æ ‡å®šè§†é¢‘çš„è§†å›¾åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13440v1" data-paper-url="./papers/250513440v1-recollection-from-pensieve-novel-view-synthesis-via-learning-from-un.html" onclick="toggleFavorite(this, '2505.13440v1', 'Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250513279v2-event-driven-dynamic-scene-depth-completion.html">Event-Driven Dynamic Scene Depth Completion</a></td>
  <td>æå‡ºEventDCä»¥è§£å†³åŠ¨æ€åœºæ™¯æ·±åº¦è¡¥å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13279v2" data-paper-url="./papers/250513279v2-event-driven-dynamic-scene-depth-completion.html" onclick="toggleFavorite(this, '2505.13279v2', 'Event-Driven Dynamic Scene Depth Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250513174v1-flowcut-unsupervised-video-instance-segmentation-via-temporal-mask-m.html">FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching</a></td>
  <td>æå‡ºFlowCutä»¥è§£å†³æ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13174v1" data-paper-url="./papers/250513174v1-flowcut-unsupervised-video-instance-segmentation-via-temporal-mask-m.html" onclick="toggleFavorite(this, '2505.13174v1', 'FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250513123v1-just-dance-with-Ï€-a-poly-modal-inductor-for-weakly-supervised-video-.html">Just Dance with $Ï€$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection</a></td>
  <td>æå‡ºPI-VADä»¥è§£å†³å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹ä¸­çš„æ¨¡æ€ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13123v1" data-paper-url="./papers/250513123v1-just-dance-with-Ï€-a-poly-modal-inductor-for-weakly-supervised-video-.html" onclick="toggleFavorite(this, '2505.13123v1', 'Just Dance with $Ï€$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250512714v1-ia-mvs-instance-focused-adaptive-depth-sampling-for-multi-view-stere.html">IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo</a></td>
  <td>æå‡ºIA-MVSä»¥è§£å†³å¤šè§†è§’ç«‹ä½“è§†è§‰ä¸­çš„æ·±åº¦ä¼°è®¡ç²¾åº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12714v1" data-paper-url="./papers/250512714v1-ia-mvs-instance-focused-adaptive-depth-sampling-for-multi-view-stere.html" onclick="toggleFavorite(this, '2505.12714v1', 'IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>41</td>
  <td><a href="./papers/250513250v1-joint-depth-and-reflectivity-estimation-using-single-photon-lidar.html">Joint Depth and Reflectivity Estimation using Single-Photon LiDAR</a></td>
  <td>æå‡ºè”åˆæ·±åº¦ä¸åå°„ç‡ä¼°è®¡æ–¹æ³•ä»¥è§£å†³åŠ¨æ€åœºæ™¯ä¸­çš„é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">PULSE</span> <span class="paper-tag">TAMP</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13250v1" data-paper-url="./papers/250513250v1-joint-depth-and-reflectivity-estimation-using-single-photon-lidar.html" onclick="toggleFavorite(this, '2505.13250v1', 'Joint Depth and Reflectivity Estimation using Single-Photon LiDAR')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250512702v2-long-rvos-a-comprehensive-benchmark-for-long-term-referring-video-ob.html">Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation</a></td>
  <td>æå‡ºLong-RVOSä»¥è§£å†³é•¿è§†é¢‘ç‰©ä½“åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12702v2" data-paper-url="./papers/250512702v2-long-rvos-a-comprehensive-benchmark-for-long-term-referring-video-ob.html" onclick="toggleFavorite(this, '2505.12702v2', 'Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>43</td>
  <td><a href="./papers/250512911v1-hiero-understanding-the-hierarchy-of-human-behavior-enhances-reasoni.html">HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos</a></td>
  <td>æå‡ºHiEROä»¥å¢å¼ºå¯¹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span> <span class="paper-tag">Ego4D</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12911v1" data-paper-url="./papers/250512911v1-hiero-understanding-the-hierarchy-of-human-behavior-enhances-reasoni.html" onclick="toggleFavorite(this, '2505.12911v1', 'HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>44</td>
  <td><a href="./papers/250513731v3-georanker-distance-aware-ranking-for-worldwide-image-geolocalization.html">GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization</a></td>
  <td>æå‡ºGeoRankerä»¥è§£å†³å…¨çƒå›¾åƒåœ°ç†å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13731v3" data-paper-url="./papers/250513731v3-georanker-distance-aware-ranking-for-worldwide-image-geolocalization.html" onclick="toggleFavorite(this, '2505.13731v3', 'GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>45</td>
  <td><a href="./papers/250513437v1-finephys-fine-grained-human-action-generation-by-explicitly-incorpor.html">FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</a></td>
  <td>æå‡ºFinePhysä»¥è§£å†³ç»†ç²’åº¦äººç±»åŠ¨ä½œç”Ÿæˆä¸­çš„ç‰©ç†ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13437v1" data-paper-url="./papers/250513437v1-finephys-fine-grained-human-action-generation-by-explicitly-incorpor.html" onclick="toggleFavorite(this, '2505.13437v1', 'FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)