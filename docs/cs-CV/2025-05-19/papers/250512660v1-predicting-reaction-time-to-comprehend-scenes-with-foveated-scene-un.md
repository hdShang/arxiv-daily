---
layout: default
title: Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps
---

# Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12660" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12660v1</a>
  <a href="https://arxiv.org/pdf/2505.12660.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12660v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12660v1', 'Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziqi Wen, Jonathan Skaza, Shravan Murlidaran, William Y. Wang, Miguel P. Eckstein

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºF-SUMæ¨¡å‹ä»¥è§£å†³åœºæ™¯ç†è§£ååº”æ—¶é—´é¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `åœºæ™¯ç†è§£` `ååº”æ—¶é—´é¢„æµ‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ³¨è§†ç‰¹æ€§` `å›¾åƒå¯è®¡ç®—æ¨¡å‹` `äººæœºäº¤äº’` `è§†è§‰æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡å‹åœ¨é¢„æµ‹äººç±»åœºæ™¯ç†è§£ååº”æ—¶é—´æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶ç¼ºä¹å›¾åƒå¯è®¡ç®—çš„é¢„æµ‹å·¥å…·ã€‚
2. æœ¬æ–‡æå‡ºçš„F-SUMæ¨¡å‹é€šè¿‡ç»“åˆæ³¨è§†ç‰¹æ€§ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”Ÿæˆç©ºé—´åˆ†å¸ƒçš„åœºæ™¯ç†è§£å›¾ï¼Œæä¾›äº†æ–°çš„é¢„æµ‹æ–¹å¼ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒF-SUMè¯„åˆ†ä¸äººç±»ååº”æ—¶é—´å’Œæè¿°å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ï¼Œä¼˜äºä¼ ç»Ÿå›¾åƒæŒ‡æ ‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å·²æœ‰æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹äººç±»åœ¨ç›®æ ‡æœç´¢å’Œè§†è§‰è¾¨åˆ«ç­‰ä»»åŠ¡ä¸­çš„ååº”æ—¶é—´ï¼Œä½†é’ˆå¯¹åœºæ™¯ç†è§£æ—¶é—´çš„å›¾åƒå¯è®¡ç®—é¢„æµ‹å™¨çš„å¼€å‘ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾åƒå¯è®¡ç®—æ¨¡å‹ï¼Œç»“åˆäº†äººç±»è§†è§‰ç³»ç»Ÿçš„æ³¨è§†ç‰¹æ€§ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç”Ÿæˆäº†åŸºäºæ³¨è§†ä½ç½®çš„åœºæ™¯ç†è§£ç©ºé—´åˆ†å¸ƒå›¾ï¼ˆFoveated Scene Understanding Map, F-SUMï¼‰ã€‚è¯¥æ¨¡å‹çš„F-SUMè¯„åˆ†ä¸äººç±»ååº”æ—¶é—´å’Œæè¿°å‡†ç¡®æ€§æ˜¾è‘—ç›¸å…³ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿå›¾åƒæŒ‡æ ‡çš„æ•ˆæœï¼Œå±•ç¤ºäº†æ³¨è§†è§†è§‰å¤„ç†åœ¨ç†è§£éš¾åº¦ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆé¢„æµ‹äººç±»åœ¨åœºæ™¯ç†è§£ä¸­çš„ååº”æ—¶é—´è¿™ä¸€å…·ä½“é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™ä¸€é¢†åŸŸç¼ºä¹å›¾åƒå¯è®¡ç®—çš„é¢„æµ‹å·¥å…·ï¼Œå¯¼è‡´é¢„æµ‹ç²¾åº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†äººç±»è§†è§‰ç³»ç»Ÿçš„æ³¨è§†ç‰¹æ€§ä¸è§†è§‰è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œç”ŸæˆåŸºäºæ³¨è§†ä½ç½®çš„åœºæ™¯ç†è§£å›¾ï¼Œä»è€Œæ›´å¥½åœ°åæ˜ äººç±»çš„ç†è§£è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆåœºæ™¯æè¿°ã€ç»“åˆæ³¨è§†ç‰¹æ€§ç”ŸæˆF-SUMå›¾ã€è®¡ç®—F-SUMè¯„åˆ†ç­‰ä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†Foveated Scene Understanding Mapï¼ˆF-SUMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ³¨è§†ä½ç½®çš„ç©ºé—´ç†è§£å›¾ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ äººç±»çš„ç†è§£è¿‡ç¨‹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬æ³¨è§†ä½ç½®çš„é€‰æ‹©ã€è§†è§‰è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæŸå¤±å‡½æ•°ï¼Œä»¥åŠF-SUMè¯„åˆ†çš„è®¡ç®—æ–¹å¼ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åœºæ™¯ç†è§£çš„åŠ¨æ€ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒF-SUMè¯„åˆ†ä¸äººç±»ååº”æ—¶é—´çš„ç›¸å…³æ€§è¾¾åˆ°0.47ï¼Œä¸æ‰€éœ€çš„çœ¼åŠ¨æ¬¡æ•°ç›¸å…³æ€§ä¸º0.51ï¼Œä¸”åœ¨æ—¶é—´é™åˆ¶ä¸‹çš„æè¿°å‡†ç¡®æ€§ç›¸å…³æ€§ä¸º-0.56ã€‚è¿™äº›ç»“æœæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿå›¾åƒæŒ‡æ ‡ï¼Œå¦‚æ‚ä¹±åº¦å’Œè§†è§‰å¤æ‚æ€§ï¼Œå±•ç¤ºäº†F-SUMæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„è§†è§‰æ³¨æ„åŠ›å’Œååº”æ—¶é—´ï¼Œä»è€Œä¼˜åŒ–ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨æ™ºèƒ½è§†è§‰ç³»ç»Ÿä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæå‡å…¶ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although models exist that predict human response times (RTs) in tasks such as target search and visual discrimination, the development of image-computable predictors for scene understanding time remains an open challenge. Recent advances in vision-language models (VLMs), which can generate scene descriptions for arbitrary images, combined with the availability of quantitative metrics for comparing linguistic descriptions, offer a new opportunity to model human scene understanding. We hypothesize that the primary bottleneck in human scene understanding and the driving source of variability in response times across scenes is the interaction between the foveated nature of the human visual system and the spatial distribution of task-relevant visual information within an image. Based on this assumption, we propose a novel image-computable model that integrates foveated vision with VLMs to produce a spatially resolved map of scene understanding as a function of fixation location (Foveated Scene Understanding Map, or F-SUM), along with an aggregate F-SUM score. This metric correlates with average (N=17) human RTs (r=0.47) and number of saccades (r=0.51) required to comprehend a scene (across 277 scenes). The F-SUM score also correlates with average (N=16) human description accuracy (r=-0.56) in time-limited presentations. These correlations significantly exceed those of standard image-based metrics such as clutter, visual complexity, and scene ambiguity based on language entropy. Together, our work introduces a new image-computable metric for predicting human response times in scene understanding and demonstrates the importance of foveated visual processing in shaping comprehension difficulty.

