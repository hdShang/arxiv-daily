---
layout: default
title: "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach"
---

# Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12903" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12903v1</a>
  <a href="https://arxiv.org/pdf/2505.12903.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12903v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12903v1', 'Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shiao Wang, Xiao Wang, Liye Jin, Bo Jiang, Lin Zhu, Lan Chen, Yonghong Tian, Bin Luo

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Event-AHU/SlowFast_Event_Track)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSlow-Fastè·Ÿè¸ªæ–¹æ³•ä»¥è§£å†³ä½å»¶è¿Ÿè§†è§‰ç›®æ ‡è·Ÿè¸ªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰ç›®æ ‡è·Ÿè¸ª` `äº‹ä»¶æ‘„åƒå¤´` `ä½å»¶è¿Ÿ` `çŸ¥è¯†è’¸é¦` `å›¾åŸºè¡¨ç¤ºå­¦ä¹ ` `FlashAttention` `æ…¢-å¿«è·Ÿè¸ª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰ç›®æ ‡è·Ÿè¸ªæ–¹æ³•åœ¨ä½å»¶è¿Ÿæ€§èƒ½å’Œèµ„æºå—é™ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶åº”ç”¨éœ€æ±‚ã€‚
2. æå‡ºçš„SFTrackæ–¹æ³•ç»“åˆé«˜ç²¾åº¦æ…¢è·Ÿè¸ªå™¨ä¸é«˜æ•ˆå¿«é€Ÿè·Ÿè¸ªå™¨ï¼Œçµæ´»é€‚åº”ä¸åŒçš„è®¡ç®—èµ„æºå’Œå»¶è¿Ÿè¦æ±‚ã€‚
3. åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒSFTrackåœ¨è·Ÿè¸ªç²¾åº¦å’Œé€Ÿåº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œé€‚ç”¨äºå¤šç§å®é™…åœºæ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è·Ÿè¸ªç®—æ³•é€šå¸¸ä¾èµ–ä½å¸§ç‡RGBæ‘„åƒå¤´å’Œè®¡ç®—å¯†é›†å‹æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„æ¥å®ç°æœ‰æ•ˆè·Ÿè¸ªã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäºå¸§çš„æ–¹æ³•åœ¨ä½å»¶è¿Ÿæ€§èƒ½ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºç”Ÿç‰©å¯å‘çš„äº‹ä»¶æ‘„åƒå¤´åœ¨è§†è§‰ç›®æ ‡è·Ÿè¸ªä¸­å±•ç°å‡ºä½å»¶è¿Ÿåº”ç”¨çš„ä¼˜åŠ¿ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„Slow-Fastè·Ÿè¸ªèŒƒå¼ï¼ˆSFTrackï¼‰ï¼Œçµæ´»é€‚åº”ä¸åŒçš„æ“ä½œéœ€æ±‚ï¼Œæ”¯æŒé«˜ç²¾åº¦æ…¢è·Ÿè¸ªå™¨å’Œé«˜æ•ˆå¿«é€Ÿè·Ÿè¸ªå™¨ã€‚é€šè¿‡å›¾åŸºè¡¨ç¤ºå­¦ä¹ å’ŒFlashAttentionè§†è§‰éª¨å¹²ç½‘ç»œçš„ç»“åˆï¼Œå¿«é€Ÿè·Ÿè¸ªå™¨å®ç°äº†ä½å»¶è¿Ÿï¼Œå¹¶é€šè¿‡çŸ¥è¯†è’¸é¦ç­–ç•¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¸åŒçœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºäºå¸§çš„è§†è§‰ç›®æ ‡è·Ÿè¸ªæ–¹æ³•åœ¨ä½å»¶è¿Ÿå’Œèµ„æºå—é™ç¯å¢ƒä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶åº”ç”¨ä¸­çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„Slow-Fastè·Ÿè¸ªèŒƒå¼é€šè¿‡ç»“åˆé«˜ç²¾åº¦å’Œé«˜æ•ˆçš„è·Ÿè¸ªå™¨ï¼Œçµæ´»é€‚åº”ä¸åŒçš„æ“ä½œéœ€æ±‚ï¼Œæ—¨åœ¨å®ç°ä½å»¶è¿Ÿå’Œé«˜ç²¾åº¦çš„ç›®æ ‡è·Ÿè¸ªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶é¦–å…ˆä»é«˜æ—¶é—´åˆ†è¾¨ç‡çš„äº‹ä»¶æµä¸­è¿›è¡Œå›¾åŸºè¡¨ç¤ºå­¦ä¹ ï¼Œç„¶åå°†å­¦ä¹ åˆ°çš„å›¾ç»“æ„ä¿¡æ¯æ•´åˆåˆ°ä¸¤ä¸ªFlashAttentionåŸºç¡€è§†è§‰ç½‘ç»œä¸­ï¼Œåˆ†åˆ«ç”Ÿæˆæ…¢è·Ÿè¸ªå™¨å’Œå¿«è·Ÿè¸ªå™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å›¾åŸºè¡¨ç¤ºå­¦ä¹ ä¸FlashAttentionç½‘ç»œç»“åˆï¼Œå½¢æˆäº†é«˜æ•ˆçš„å¿«é€Ÿè·Ÿè¸ªå™¨ï¼Œå¹¶é€šè¿‡çŸ¥è¯†è’¸é¦ç­–ç•¥æå‡å…¶æ€§èƒ½ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„è®¾è®¡ç†å¿µæœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šå¿«é€Ÿè·Ÿè¸ªå™¨é‡‡ç”¨è½»é‡çº§ç½‘ç»œè®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­ç”Ÿæˆå¤šä¸ªè¾¹ç•Œæ¡†è¾“å‡ºï¼Œæ­¤å¤–ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’ŒçŸ¥è¯†è’¸é¦è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨FE240ã€COESOTå’ŒEventVOTç­‰å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šï¼ŒSFTrackåœ¨è·Ÿè¸ªç²¾åº¦å’Œé€Ÿåº¦ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå¿«é€Ÿè·Ÿè¸ªå™¨åœ¨å¤šä¸ªåœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—çš„ä½å»¶è¿Ÿï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€æ— äººé©¾é©¶ã€å¢å¼ºç°å®ç­‰éœ€è¦å®æ—¶ç›®æ ‡è·Ÿè¸ªçš„åœºæ™¯ã€‚é€šè¿‡æé«˜è·Ÿè¸ªçš„ç²¾åº¦å’Œé€Ÿåº¦ï¼ŒSFTrackèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æœ‰æ•ˆåº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing tracking algorithms typically rely on low-frame-rate RGB cameras coupled with computationally intensive deep neural network architectures to achieve effective tracking. However, such frame-based methods inherently face challenges in achieving low-latency performance and often fail in resource-constrained environments. Visual object tracking using bio-inspired event cameras has emerged as a promising research direction in recent years, offering distinct advantages for low-latency applications. In this paper, we propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different operational requirements, termed SFTrack. The proposed framework supports two complementary modes, i.e., a high-precision slow tracker for scenarios with sufficient computational resources, and an efficient fast tracker tailored for latency-aware, resource-constrained environments. Specifically, our framework first performs graph-based representation learning from high-temporal-resolution event streams, and then integrates the learned graph-structured information into two FlashAttention-based vision backbones, yielding the slow and fast trackers, respectively. The fast tracker achieves low latency through a lightweight network design and by producing multiple bounding box outputs in a single forward pass. Finally, we seamlessly combine both trackers via supervised fine-tuning and further enhance the fast tracker's performance through a knowledge distillation strategy. Extensive experiments on public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the effectiveness and efficiency of our proposed method across different real-world scenarios. The source code has been released on https://github.com/Event-AHU/SlowFast_Event_Track.

