---
layout: default
title: Few-Step Diffusion via Score identity Distillation
---

# Few-Step Diffusion via Score identity Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12674" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12674v1</a>
  <a href="https://arxiv.org/pdf/2505.12674.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12674v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12674v1', 'Few-Step Diffusion via Score identity Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingyuan Zhou, Yi Gu, Zhendong Wang

**åˆ†ç±»**: cs.CV, cs.LG, stat.ML

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mingyuanzhou/SiD-LSG)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºScore identity Distillationä»¥è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `æ‰©æ•£è’¸é¦` `å›¾åƒç”Ÿæˆ` `æ— ç›‘ç£å­¦ä¹ ` `å¯¹æŠ—æŸå¤±` `å¤šæ ·æ€§ä¸å¯¹é½æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ‰©æ•£è’¸é¦æ–¹æ³•åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­ä¾èµ–çœŸå®æˆ–åˆæˆå›¾åƒï¼Œä¸”å­˜åœ¨å¯¹é½ä¸å¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®æ— å…³çš„ä¸€æ­¥è’¸é¦æ¡†æ¶Score identity Distillationï¼ˆSiDï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–å°‘æ­¥ç”Ÿæˆè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSiDåœ¨SD1.5å’ŒSDXLä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”åœ¨ç¼ºä¹çœŸå®å›¾åƒçš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£è’¸é¦å·²æˆä¸ºåŠ é€Ÿæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆç­–ç•¥ï¼Œé€šè¿‡å°†é¢„è®­ç»ƒçš„è¯„åˆ†ç½‘ç»œè’¸é¦ä¸ºå•æ­¥æˆ–å°‘æ­¥ç”Ÿæˆå™¨ã€‚ç°æœ‰æ–¹æ³•åœ¨è’¸é¦é«˜åˆ†è¾¨ç‡T2Iæ‰©æ•£æ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¾èµ–çœŸå®æˆ–æ•™å¸ˆåˆæˆå›¾åƒï¼Œå¹¶ä¸”ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ä¼šå¯¼è‡´æ–‡æœ¬å›¾åƒå¯¹é½ä¸ç”Ÿæˆå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®æ— å…³çš„ä¸€æ­¥è’¸é¦æ¡†æ¶Score identity Distillationï¼ˆSiDï¼‰ï¼Œé€šè¿‡ç†è®ºåˆ†æè¯æ˜å°†æ‰€æœ‰ç”Ÿæˆæ­¥éª¤çš„è¾“å‡ºå‡åŒ€æ··åˆä¸æ•°æ®åˆ†å¸ƒåŒ¹é…çš„æœ‰æ•ˆæ€§ï¼Œé¿å…äº†ç‰¹å®šæ­¥éª¤ç½‘ç»œçš„éœ€æ±‚ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰ç®¡é“ä¸­ï¼Œåœ¨1024x1024åˆ†è¾¨ç‡ä¸‹å®ç°äº†SDXLçš„æœ€å…ˆè¿›æ€§èƒ½ã€‚ä¸ºç¼“è§£çœŸå®æ–‡æœ¬å›¾åƒå¯¹çš„å¯¹é½å¤šæ ·æ€§æƒè¡¡ï¼Œæœ¬æ–‡å¼•å…¥äº†åŸºäºæ‰©æ•£GANçš„å¯¹æŠ—æŸå¤±ï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ–°çš„å¼•å¯¼ç­–ç•¥ï¼šZero-CFGå’ŒAnti-CFGï¼Œçµæ´»çš„è®¾ç½®æé«˜äº†å¤šæ ·æ€§è€Œä¸ç‰ºç‰²å¯¹é½æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œåœ¨ä¸€æ­¥å’Œå°‘æ­¥ç”Ÿæˆè®¾ç½®ä¸‹å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯¹ç¼ºä¹çœŸå®å›¾åƒå…·æœ‰é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ‰©æ•£è’¸é¦æ–¹æ³•åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­ä¾èµ–çœŸå®æˆ–åˆæˆå›¾åƒï¼Œä¸”ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰å¯¼è‡´æ–‡æœ¬å›¾åƒå¯¹é½ä¸ç”Ÿæˆå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œé™åˆ¶äº†ç”Ÿæˆæ•ˆæœçš„æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºScore identity Distillationï¼ˆSiDï¼‰ï¼Œé€šè¿‡ç†è®ºåˆ†æè¯æ˜å°†æ‰€æœ‰ç”Ÿæˆæ­¥éª¤çš„è¾“å‡ºå‡åŒ€æ··åˆä¸æ•°æ®åˆ†å¸ƒåŒ¹é…çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œé¿å…äº†ç‰¹å®šæ­¥éª¤ç½‘ç»œçš„éœ€æ±‚ï¼Œä¼˜åŒ–äº†å°‘æ­¥ç”Ÿæˆè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSiDæ¡†æ¶åŒ…æ‹¬æ•°æ®æ— å…³çš„ä¸€æ­¥è’¸é¦è¿‡ç¨‹ï¼Œç»“åˆäº†åŸºäºæ‰©æ•£GANçš„å¯¹æŠ—æŸå¤±å’Œä¸¤ç§æ–°çš„å¼•å¯¼ç­–ç•¥Zero-CFGä¸Anti-CFGï¼Œèƒ½å¤Ÿçµæ´»è°ƒæ•´ç”Ÿæˆå¤šæ ·æ€§ä¸å¯¹é½æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ— éœ€çœŸå®å›¾åƒçš„è’¸é¦æ–¹æ³•ï¼Œå¹¶é€šè¿‡å¯¹æŠ—æŸå¤±å’Œæ–°å¼•å¯¼ç­–ç•¥æœ‰æ•ˆæ”¹å–„äº†ç”Ÿæˆçš„å¤šæ ·æ€§ä¸å¯¹é½æ€§ï¼Œçªç ´äº†ç°æœ‰æ–¹æ³•çš„å±€é™ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†åŸºäºæ‰©æ•£GANçš„å¯¹æŠ—æŸå¤±ï¼›Zero-CFGç­–ç•¥ç¦ç”¨æ•™å¸ˆç½‘ç»œä¸­çš„CFGï¼Œè€ŒAnti-CFGåˆ™åœ¨å‡è¯„åˆ†ç½‘ç»œä¸­åº”ç”¨è´ŸCFGï¼Œè¿™äº›è®¾è®¡æé«˜äº†ç”Ÿæˆå¤šæ ·æ€§è€Œä¸å½±å“å¯¹é½æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSiDåœ¨SD1.5å’ŒSDXLä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨1024x1024åˆ†è¾¨ç‡ä¸‹ï¼Œç”Ÿæˆè´¨é‡æ˜¾è‘—æå‡ï¼Œä¸”åœ¨ä¸€è‡³å°‘æ­¥ç”Ÿæˆè®¾ç½®ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç”Ÿæˆã€è‰ºæœ¯åˆ›ä½œã€è™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿä¸ºç”Ÿæˆæ¨¡å‹æä¾›æ›´é«˜æ•ˆçš„è®­ç»ƒæ–¹å¼ï¼Œæå‡ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Diffusion distillation has emerged as a promising strategy for accelerating text-to-image (T2I) diffusion models by distilling a pretrained score network into a one- or few-step generator. While existing methods have made notable progress, they often rely on real or teacher-synthesized images to perform well when distilling high-resolution T2I diffusion models such as Stable Diffusion XL (SDXL), and their use of classifier-free guidance (CFG) introduces a persistent trade-off between text-image alignment and generation diversity. We address these challenges by optimizing Score identity Distillation (SiD) -- a data-free, one-step distillation framework -- for few-step generation. Backed by theoretical analysis that justifies matching a uniform mixture of outputs from all generation steps to the data distribution, our few-step distillation algorithm avoids step-specific networks and integrates seamlessly into existing pipelines, achieving state-of-the-art performance on SDXL at 1024x1024 resolution. To mitigate the alignment-diversity trade-off when real text-image pairs are available, we introduce a Diffusion GAN-based adversarial loss applied to the uniform mixture and propose two new guidance strategies: Zero-CFG, which disables CFG in the teacher and removes text conditioning in the fake score network, and Anti-CFG, which applies negative CFG in the fake score network. This flexible setup improves diversity without sacrificing alignment. Comprehensive experiments on SD1.5 and SDXL demonstrate state-of-the-art performance in both one-step and few-step generation settings, along with robustness to the absence of real images. Our efficient PyTorch implementation, along with the resulting one- and few-step distilled generators, will be released publicly as a separate branch at https://github.com/mingyuanzhou/SiD-LSG.

