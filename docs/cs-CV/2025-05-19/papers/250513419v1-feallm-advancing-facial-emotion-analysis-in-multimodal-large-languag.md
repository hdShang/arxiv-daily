---
layout: default
title: "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning"
---

# FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13419" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.13419v1</a>
  <a href="https://arxiv.org/pdf/2505.13419.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13419v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13419v1', 'FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhuozhao Hu, Kaishen Yuan, Xin Liu, Zitong Yu, Yuan Zong, Jingang Shi, Huanjing Yue, Jingyu Yang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-19

**Â§áÊ≥®**: 10 pages, 7 figures

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/953206211/FEALLM)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫FEALLM‰ª•Ëß£ÂÜ≥Èù¢ÈÉ®ÊÉÖÊÑüÂàÜÊûê‰∏≠ÁöÑÂ§öÊ®°ÊÄÅÊåëÊàò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èù¢ÈÉ®ÊÉÖÊÑüÂàÜÊûê` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ÊÉÖÊÑüËÆ°ÁÆó` `Âä®‰ΩúÂçïÂÖÉ` `Âõ†ÊûúÊé®ÁêÜ` `Ê∑±Â∫¶Â≠¶‰π†` `ÁâπÂæÅÊèêÂèñ` `Êô∫ËÉΩ‰∫∫Êú∫‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈù¢ÈÉ®ÊÉÖÊÑüÂàÜÊûêÊñπÊ≥ïÂú®ÂèØËß£ÈáäÊÄßÂíåÊé®ÁêÜËÉΩÂäõÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÈöæ‰ª•Â§ÑÁêÜÂ§çÊùÇÁöÑÈù¢ÈÉ®Ë°®ÊÉÖ‰∏éÂä®‰ΩúÂçïÂÖÉ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑFEAÊåá‰ª§Êï∞ÊçÆÈõÜÔºåÂª∫Á´ã‰∫ÜÈù¢ÈÉ®Ë°®ÊÉÖ‰∏éÂä®‰ΩúÂçïÂÖÉ‰πãÈó¥ÁöÑÂõ†ÊûúÂÖ≥Á≥ªÔºåÂπ∂ÊûÑÂª∫‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜFEABench„ÄÇ
3. FEALLMÊ®°ÂûãÂú®FEABench‰∏äË°®Áé∞‰ºòÂºÇÔºåÂπ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äËøõË°åÈõ∂-shotËØÑ‰º∞ÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Èù¢ÈÉ®ÊÉÖÊÑüÂàÜÊûêÔºàFEAÔºâÂú®ËßÜËßâÊÉÖÊÑüËÆ°ÁÆó‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÊó®Âú®Ê†πÊçÆÈù¢ÈÉ®Êï∞ÊçÆÊé®Êñ≠‰∏™‰ΩìÁöÑÊÉÖÊÑüÁä∂ÊÄÅ„ÄÇÈù¢ÈÉ®Ë°®ÊÉÖÔºàFEsÔºâÁî±Èù¢ÈÉ®ËÇåËÇâÁöÑÂçèË∞ÉËøêÂä®‰∫ßÁîüÔºåÂèØ‰ª•ÂàÜËß£‰∏∫ÁâπÂÆöÁöÑÂä®‰ΩúÂçïÂÖÉÔºàAUsÔºâÔºåÊèê‰æõËØ¶ÁªÜÁöÑÊÉÖÊÑüÊ¥ûÂØü„ÄÇÁÑ∂ËÄåÔºå‰º†ÁªüÊñπÊ≥ïÂú®ÂèØËß£ÈáäÊÄß„ÄÅÊ≥õÂåñËÉΩÂäõÂíåÊé®ÁêÜËÉΩÂäõÊñπÈù¢Â≠òÂú®Â±ÄÈôê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑFEAÊåá‰ª§Êï∞ÊçÆÈõÜÔºåÊèê‰æõÂáÜÁ°ÆÁöÑFEÂíåAUÊèèËø∞ÔºåÂπ∂Âª∫Á´ãÂÆÉ‰ª¨‰πãÈó¥ÁöÑÂõ†ÊûúÊé®ÁêÜÂÖ≥Á≥ªÔºåÂêåÊó∂ÊûÑÂª∫‰∫ÜÊñ∞ÁöÑÂü∫ÂáÜFEABench„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFEALLMÔºå‰∏ÄÁßçÊñ∞ÂûãÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊû∂ÊûÑÔºåÊó®Âú®ÊçïÊçâÊõ¥ËØ¶ÁªÜÁöÑÈù¢ÈÉ®‰ø°ÊÅØÔºåÂ¢ûÂº∫ÂÖ∂Âú®FEA‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®FEABench‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂ÈÄöËøáÈõ∂-shotËØÑ‰º∞Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂ±ïÁ§∫‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊ≥õÂåñËÉΩÂäõÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®FEA‰ªªÂä°‰∏≠ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÈù¢ÈÉ®ÊÉÖÊÑüÂàÜÊûêÊñπÊ≥ïÂú®ÂèØËß£ÈáäÊÄß„ÄÅÊ≥õÂåñËÉΩÂäõÂíåÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Èù¢ÈÉ®Ë°®ÊÉÖ‰∏éÂä®‰ΩúÂçïÂÖÉ‰πãÈó¥Â§çÊùÇÂÖ≥Á≥ªÁöÑÊçïÊçâ‰∏äÂ≠òÂú®ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™Êñ∞ÁöÑFEAÊåá‰ª§Êï∞ÊçÆÈõÜÔºåÊèê‰æõÂáÜÁ°ÆÁöÑÈù¢ÈÉ®Ë°®ÊÉÖÂíåÂä®‰ΩúÂçïÂÖÉÊèèËø∞ÔºåÂπ∂Âª∫Á´ãÂÆÉ‰ª¨‰πãÈó¥ÁöÑÂõ†ÊûúÊé®ÁêÜÂÖ≥Á≥ªÔºå‰ªéËÄåÂ¢ûÂº∫Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊÉÖÊÑüÂàÜÊûêÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöFEALLMÊ®°ÂûãÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊ®°Âùó„ÄÅÁâπÂæÅÊèêÂèñÊ®°ÂùóÂíåÊé®ÁêÜÊ®°Âùó„ÄÇÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊ®°ÂùóË¥üË¥£Â§ÑÁêÜËæìÂÖ•ÁöÑÈù¢ÈÉ®ÂõæÂÉèÔºåÁâπÂæÅÊèêÂèñÊ®°ÂùóÂà©Áî®Ê∑±Â∫¶Â≠¶‰π†ÊäÄÊúØÊèêÂèñÈù¢ÈÉ®ÁâπÂæÅÔºåÊé®ÁêÜÊ®°ÂùóÂàôÂü∫‰∫éÊèêÂèñÁöÑÁâπÂæÅËøõË°åÊÉÖÊÑüÁä∂ÊÄÅÁöÑÊé®Êñ≠„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜFEAÊåá‰ª§Êï∞ÊçÆÈõÜÂíåFEABenchÂü∫ÂáÜÔºåÂâçËÄÖÊèê‰æõ‰∫ÜÊõ¥‰∏∫ÂáÜÁ°ÆÁöÑÊÉÖÊÑüÊèèËø∞ÔºåÂêéËÄÖ‰∏∫Ê®°ÂûãËØÑ‰º∞Êèê‰æõ‰∫ÜÊ†áÂáÜÂåñÁöÑÊµãËØïÁéØÂ¢É„ÄÇËøô‰∫õÂàõÊñ∞‰ΩøÂæóFEALLMËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÈù¢ÈÉ®Ë°®ÊÉÖ‰∏éÂä®‰ΩúÂçïÂÖÉ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂ§öÂ±ÇÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâËøõË°åÁâπÂæÅÊèêÂèñÔºåÂπ∂ÂºïÂÖ•‰∫ÜËá™Ê≥®ÊÑèÂäõÊú∫Âà∂‰ª•Â¢ûÂº∫Ê®°ÂûãÂØπÈáçË¶ÅÁâπÂæÅÁöÑÂÖ≥Ê≥®„ÄÇÊ≠§Â§ñÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÁªìÂêà‰∫ÜÂàÜÁ±ªÊçüÂ§±ÂíåÂõûÂΩíÊçüÂ§±Ôºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

FEALLMÂú®FEABenchÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜÔºàÂ¶ÇRAF-DB„ÄÅAffectNet„ÄÅBP4DÂíåDISFAÔºâ‰∏äÁöÑÈõ∂-shotËØÑ‰º∞ËÉΩÂäõÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊÉÖÊÑüÂàÜÊûêÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Èù¢ÈÉ®ÊÉÖÊÑüÂàÜÊûê‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊÉÖÊÑüËÆ°ÁÆó„ÄÅÊô∫ËÉΩ‰∫∫Êú∫‰∫§‰∫í„ÄÅÂøÉÁêÜÂÅ•Â∫∑ÁõëÊµãÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÈù¢ÈÉ®ÊÉÖÊÑüÂàÜÊûêÁöÑÂáÜÁ°ÆÊÄßÔºåFEALLMËÉΩÂ§üÂú®Á§æ‰∫§Êú∫Âô®‰∫∫„ÄÅËôöÊãüÂä©ÊâãÂíåÊÉÖÊÑüËØÜÂà´Á≥ªÁªü‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®ÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑËøõÊ≠•‰∏éÂ∫îÁî®„ÄÇÊú™Êù•ÔºåËØ•Ê®°ÂûãÊúâÊúõÂú®Êõ¥ÂπøÊ≥õÁöÑÊÉÖÊÑüÁêÜËß£Âíå‰∫∫Êú∫‰∫§‰∫íÂú∫ÊôØ‰∏≠ÂæóÂà∞Â∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Facial Emotion Analysis (FEA) plays a crucial role in visual affective computing, aiming to infer a person's emotional state based on facial data. Scientifically, facial expressions (FEs) result from the coordinated movement of facial muscles, which can be decomposed into specific action units (AUs) that provide detailed emotional insights. However, traditional methods often struggle with limited interpretability, constrained generalization and reasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have shown exceptional performance in various visual tasks, while they still face significant challenges in FEA due to the lack of specialized datasets and their inability to capture the intricate relationships between FEs and AUs. To address these issues, we introduce a novel FEA Instruction Dataset that provides accurate and aligned FE and AU descriptions and establishes causal reasoning relationships between them, followed by constructing a new benchmark, FEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to capture more detailed facial information, enhancing its capability in FEA tasks. Our model demonstrates strong performance on FEABench and impressive generalization capability through zero-shot evaluation on various datasets, including RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and effectiveness in FEA tasks. The dataset and code will be available at https://github.com/953206211/FEALLM.

