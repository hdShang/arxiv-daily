---
layout: default
title: "FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks"
---

# FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12728" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.12728v2</a>
  <a href="https://arxiv.org/pdf/2505.12728.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12728v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12728v2', 'FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zihua Wang, Ruibo Li, Haozhe Du, Joey Tianyi Zhou, Yu Zhang, Xu Yang

**ÂàÜÁ±ª**: cs.CV, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-19 (Êõ¥Êñ∞: 2025-05-25)

**Â§áÊ≥®**: This preprint is under review

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/ZihuaEvan/FlashSD/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫FLASH‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÁöÑËß£Á†ÅÈÄüÂ∫¶ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊ®°Âûã` `ÊäïÊú∫Ëß£Á†Å` `ÊΩúÂú®ÊÑüÁü•` `ÂçäËá™ÂõûÂΩí` `ËßÜËßâËæìÂÖ•` `Ëß£Á†ÅÈÄüÂ∫¶` `ËßÜÈ¢ëÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊäïÊú∫Ëß£Á†ÅÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÊ®°Âûã‰∏≠Êú™ËÉΩÂÖÖÂàÜÂà©Áî®ËßÜËßâËæìÂÖ•ÁöÑÁâπÊÄßÔºåÂØºËá¥Ëß£Á†ÅÈÄüÂ∫¶ÊÖ¢„ÄÇ
2. Êú¨ÊñáÊèêÂá∫FLASHÊ°ÜÊû∂ÔºåÈÄöËøáÊΩúÂú®ÊÑüÁü•Ê†áËÆ∞ÂéãÁº©ÂíåÂçäËá™ÂõûÂΩíËß£Á†ÅÁ≠ñÁï•Ôºå‰ºòÂåñÂ§öÊ®°ÊÄÅÊï∞ÊçÆÁöÑËß£Á†ÅËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåFLASHÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜÈ´òËææ2.68ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Êé®ÁêÜËÉΩÂäõ‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËß£Á†ÅÈÄüÂ∫¶ËæÉÊÖ¢ÁöÑÈóÆÈ¢ò‰æùÁÑ∂Â≠òÂú®„ÄÇÂ∞§ÂÖ∂Âú®LMMs‰∏≠ÔºåËßÜËßâËæìÂÖ•ÈÄöÂ∏∏ÂåÖÂê´Êõ¥Â§öÁöÑÊ†áËÆ∞‰∏î‰ø°ÊÅØÂØÜÂ∫¶ËæÉ‰Ωé„ÄÇÁé∞ÊúâÁöÑÊäïÊú∫Ëß£Á†ÅÊñπÊ≥ïËôΩÁÑ∂ËÉΩÂä†ÈÄüLLMÊé®ÁêÜÔºå‰ΩÜÂú®LMMs‰∏≠Êú™ËÉΩÂÖÖÂàÜËÄÉËôëËßÜËßâËæìÂÖ•ÁöÑÁã¨ÁâπÁâπÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫FLASHÔºàÂø´ÈÄüÊΩúÂú®ÊÑüÁü•ÂçäËá™ÂõûÂΩíÂêØÂèëÂºèÔºâÔºå‰∏ì‰∏∫LMMsËÆæËÆ°ÁöÑÊäïÊú∫Ëß£Á†ÅÊ°ÜÊû∂ÔºåÈÄöËøáËΩªÈáèÁ∫ßÁöÑÊΩúÂú®ÊÑüÁü•Ê†áËÆ∞ÂéãÁº©Êú∫Âà∂ÂíåÂçäËá™ÂõûÂΩíËß£Á†ÅÁ≠ñÁï•ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËß£Á†ÅÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅÈ´òÊé•ÂèóÁéá„ÄÇÂÆûÈ™åË°®ÊòéÔºåFLASHÂú®ËßÜÈ¢ëÂ≠óÂπïÁîüÊàêÂíåËßÜËßâÊåá‰ª§Ë∞É‰ºò‰ªªÂä°‰∏≠ÔºåÂàÜÂà´ÂÆûÁé∞‰∫Ü2.68ÂÄçÂíå2.55ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Ëß£Á†ÅÈÄüÂ∫¶‰∏äÁöÑÁì∂È¢àÔºåÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÊñáÊú¨Ê®°ÂûãÔºåÊú™ËÉΩÂÖÖÂàÜËÄÉËôëËßÜËßâËæìÂÖ•ÁöÑÁâπÁÇπÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöFLASHÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•ÊΩúÂú®ÊÑüÁü•Ê†áËÆ∞ÂéãÁº©Êú∫Âà∂ÔºåÂáèÂ∞ëËßÜËßâÊ†áËÆ∞ÁöÑÂÜó‰ΩôÔºåÂêåÊó∂ÈááÁî®ÂçäËá™ÂõûÂΩíËß£Á†ÅÁ≠ñÁï•ÔºåÂú®‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠‰∏≠ÁîüÊàêÂ§ö‰∏™Ê†áËÆ∞Ôºå‰ªéËÄåÂä†ÈÄüËß£Á†ÅËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöFLASHÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊΩúÂú®ÊÑüÁü•Ê†áËÆ∞ÂéãÁº©Ê®°ÂùóÂíåÂçäËá™ÂõûÂΩíËß£Á†ÅÊ®°Âùó„ÄÇÂâçËÄÖË¥üË¥£‰ºòÂåñËßÜËßâËæìÂÖ•ÁöÑÊ†áËÆ∞Ë°®Á§∫ÔºåÂêéËÄÖÂàôÈÄöËøáÁîüÊàêÂ§ö‰∏™ÂÄôÈÄâÊ†áËÆ∞Êù•ÊèêÈ´òËß£Á†ÅÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöFLASHÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂÖ∂‰∏ì‰∏∫Â§öÊ®°ÊÄÅ‰ªªÂä°ËÆæËÆ°ÁöÑÊäïÊú∫Ëß£Á†ÅÁ≠ñÁï•ÔºåÂÖÖÂàÜÂà©Áî®‰∫ÜËßÜËßâÊï∞ÊçÆÁöÑÁâπÊÄßÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåÊòæËëóÊèêÂçá‰∫ÜËß£Á†ÅÈÄüÂ∫¶ÂíåÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜËΩªÈáèÁ∫ßÁöÑÊΩúÂú®ÊÑüÁü•Ê†áËÆ∞ÂéãÁº©ÁÆóÊ≥ïÔºå‰ª•Èôç‰ΩéËßÜËßâÊ†áËÆ∞ÁöÑÂÜó‰ΩôÔºåÂêåÊó∂Âú®ÂçäËá™ÂõûÂΩíËß£Á†Å‰∏≠ÔºåËÆæÁΩÆ‰∫ÜÂêàÁêÜÁöÑÁîüÊàêÁ≠ñÁï•Ôºå‰ª•Á°Æ‰øùÁîüÊàêÊ†áËÆ∞ÁöÑË¥®ÈáèÂíåÈÄüÂ∫¶„ÄÇÈÄöËøáËøô‰∫õËÆæËÆ°ÔºåFLASHÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑËß£Á†ÅËøáÁ®ã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFLASHÂú®ËßÜÈ¢ëÂ≠óÂπïÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫Ü2.68ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçáÔºåÂú®ËßÜËßâÊåá‰ª§Ë∞É‰ºò‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫Ü2.55ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçáÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊäïÊú∫Ëß£Á†ÅÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄßÂíå‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜÈ¢ëÁêÜËß£„ÄÅÂõæÂÉèÊèèËø∞ÁîüÊàêÂíåËßÜËßâÊåá‰ª§ÊâßË°åÁ≠âÂ§öÊ®°ÊÄÅ‰ªªÂä°„ÄÇÈÄöËøáÊèêÈ´òËß£Á†ÅÈÄüÂ∫¶ÔºåFLASHËÉΩÂ§üÂú®ÂÆûÊó∂Á≥ªÁªü‰∏≠ÂèëÊå•ÈáçË¶Å‰ΩúÁî®ÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÂíåÁ≥ªÁªüÊïàÁéáÔºåÊú™Êù•ÂèØËÉΩÂú®Êô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüü‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language and multimodal models (LLMs and LMMs) exhibit strong inference capabilities but are often limited by slow decoding speeds. This challenge is especially acute in LMMs, where visual inputs typically comprise more tokens with lower information density than text -- an issue exacerbated by recent trends toward finer-grained visual tokenizations to boost performance. Speculative decoding has been effective in accelerating LLM inference by using a smaller draft model to generate candidate tokens, which are then selectively verified by the target model, improving speed without sacrificing output quality. While this strategy has been extended to LMMs, existing methods largely overlook the unique properties of visual inputs and depend solely on text-based draft models. In this work, we propose \textbf{FLASH} (Fast Latent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework designed specifically for LMMs, which leverages two key properties of multimodal data to design the draft model. First, to address redundancy in visual tokens, we propose a lightweight latent-aware token compression mechanism. Second, recognizing that visual objects often co-occur within a scene, we employ a semi-autoregressive decoding strategy to generate multiple tokens per forward pass. These innovations accelerate draft decoding while maintaining high acceptance rates, resulting in faster overall inference. Experiments show that FLASH significantly outperforms prior speculative decoding approaches in both unimodal and multimodal settings, achieving up to \textbf{2.68$\times$} speed-up on video captioning and \textbf{2.55$\times$} on visual instruction tuning tasks compared to the original LMM. Our code is available \href{https://github.com/ZihuaEvan/FlashSD/}{[here]}.

