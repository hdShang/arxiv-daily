---
layout: default
title: Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding
---

# Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12605" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12605v1</a>
  <a href="https://arxiv.org/pdf/2505.12605.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12605v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12605v1', 'Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thong Nguyen, Zhiyuan Hu, Xu Lin, Cong-Duy Nguyen, See-Kiong Ng, Luu Anh Tuan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**å¤‡æ³¨**: In Progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶é—´å¯¼å‘é…æ–¹ä»¥æå‡è§†é¢‘ç†è§£ä¸­çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹` `æ—¶é—´ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¨¡å‹ä¼˜åŒ–` `è®­ç»ƒæ–¹æ¡ˆ` `æ¥å£è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä¸­ä¾èµ–éšå«çš„æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œæœªèƒ½å……åˆ†æŒ–æ˜æ—¶é—´ç†è§£çš„å…³é”®å› ç´ ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ—¶é—´å¯¼å‘çš„é…æ–¹ï¼ŒåŒ…å«æ—¶é—´å¯¼å‘çš„è®­ç»ƒæ–¹æ¡ˆå’Œå‡çº§çš„æ¥å£ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºè¯¥é…æ–¹å¼€å‘çš„æ¨¡å‹åœ¨æ ‡å‡†è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„LVLMsï¼Œæå‡æ•ˆæœæ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼Œè¿™äº›æ¨¡å‹ä¸»è¦ä¾èµ–å…¶éšå«çš„æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œæœªèƒ½æ·±å…¥è§£æå½±å“æ—¶é—´ç†è§£èƒ½åŠ›çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æœ¬æ–‡é€šè¿‡å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œæ­ç¤ºäº†å½±å“LVLMsæ—¶é—´ç†è§£çš„å…³é”®å› ç´ ï¼Œå°¤å…¶æ˜¯è§†è§‰ç¼–ç å™¨ä¸å¤§è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ä¸­é—´æ¥å£ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†ä¸€ç§æ—¶é—´å¯¼å‘çš„è®­ç»ƒæ–¹æ¡ˆå’Œå‡çº§æ¥å£ï¼Œæœ€ç»ˆå¼€å‘çš„æ¨¡å‹åœ¨æ ‡å‡†è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†ä»¥å¾€LVLMsçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä¸­çš„æ—¶é—´ç†è§£èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æ˜ç¡®å½±å“æ—¶é—´ç†è§£çš„å…³é”®å› ç´ ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®è¯ç ”ç©¶æ­ç¤ºå½±å“æ—¶é—´ç†è§£çš„å…³é”®ç»„ä»¶ï¼Œç‰¹åˆ«æ˜¯è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹ä¹‹é—´çš„æ¥å£ï¼Œè¿›è€Œæå‡ºæ—¶é—´å¯¼å‘çš„è®­ç»ƒæ–¹æ¡ˆå’Œæ¥å£å‡çº§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰ç¼–ç å™¨ã€è¯­è¨€æ¨¡å‹å’Œä¸­é—´æ¥å£ã€‚é€šè¿‡ä¼˜åŒ–ä¸­é—´æ¥å£å’Œè®­ç»ƒæ–¹æ¡ˆï¼Œæå‡æ¨¡å‹å¯¹æ—¶é—´ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†æ—¶é—´å¯¼å‘çš„è®­ç»ƒæ–¹æ¡ˆå’Œå‡çº§æ¥å£ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è®¾è®¡æ€è·¯æœ‰æœ¬è´¨åŒºåˆ«ï¼Œå¼ºè°ƒäº†æ—¶é—´ç†è§£åœ¨è§†é¢‘ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„è®¾è®¡å‡å›´ç»•æ—¶é—´ç†è§£èƒ½åŠ›å±•å¼€ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ—¶é—´ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºæå‡ºçš„æ—¶é—´å¯¼å‘é…æ–¹å¼€å‘çš„æ¨¡å‹åœ¨æ ‡å‡†è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è§†é¢‘åˆ†æã€è‡ªåŠ¨è§†é¢‘æ‘˜è¦ç”Ÿæˆå’Œå¤šåª’ä½“æ£€ç´¢ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒæ™ºèƒ½ç›‘æ§ã€å†…å®¹æ¨èå’Œäººæœºäº¤äº’ç­‰å®é™…åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å•†ä¸šä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent years have witnessed outstanding advances of large vision-language models (LVLMs). In order to tackle video understanding, most of them depend upon their implicit temporal understanding capacity. As such, they have not deciphered important components that contribute to temporal understanding ability, which might limit the potential of these LVLMs for video understanding. In this work, we conduct a thorough empirical study to demystify crucial components that influence the temporal understanding of LVLMs. Our empirical study reveals that significant impacts are centered around the intermediate interface between the visual encoder and the large language model. Building on these insights, we propose a temporal-oriented recipe that encompasses temporal-oriented training schemes and an upscaled interface. Our final model developed using our recipe significantly enhances previous LVLMs on standard video understanding tasks.

