---
layout: default
title: Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning
---

# Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13261" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13261v2</a>
  <a href="https://arxiv.org/pdf/2505.13261.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13261v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13261v2', 'Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingrui Chen, Haogeng Liu, Hao Liang, Huaibo Huang, Wentao Zhang, Ran He

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-12-14)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡å›°éš¾å…ˆéªŒå»ºæ¨¡æå‡å¤šæ¨¡æ€æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›°éš¾å…ˆéªŒ` `å¤šæ¨¡æ€æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®æ•´ç†` `ä¼˜åŠ¿å·®å¼‚åŒ–` `åæ€æ€§éªŒè¯` `æ•°å­¦æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­æœªèƒ½æœ‰æ•ˆåˆ©ç”¨é—®é¢˜çš„å›°éš¾å…ˆéªŒä¿¡æ¯ï¼Œå¯¼è‡´å­¦ä¹ ä¿¡å·ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ç¦»çº¿æ•°æ®æ•´ç†å’Œåœ¨çº¿ä¼˜åŠ¿å·®å¼‚åŒ–ç­‰æ–¹æ³•ï¼Œæ˜¾å¼å»ºæ¨¡é—®é¢˜çš„å›°éš¾å…ˆéªŒä¿¡æ¯ï¼Œä»¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æå‡ï¼Œä»…éœ€å°‘é‡è®­ç»ƒæ•°æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ˜¾å¼å»ºæ¨¡é—®é¢˜çš„å›°éš¾å…ˆéªŒä¿¡æ¯ï¼Œæå‡åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ¨¡æ€æ¨ç†å¾®è°ƒæ•ˆæœã€‚æˆ‘ä»¬ä»ä¸‰ä¸ªæ–¹é¢è¿›è¡Œæ¢ç´¢ï¼šé¦–å…ˆï¼Œé€šè¿‡ç¦»çº¿æ•°æ®æ•´ç†ï¼Œåˆ†æä¸¤ä¸ªæ•°æ®é›†çš„Uå‹å›°éš¾åˆ†å¸ƒï¼Œè¿‡æ»¤å‡ºæ—¢ä¸ç®€å•ä¹Ÿä¸æéš¾çš„æç¤ºï¼Œä»¥æä¾›æœ‰æ„ä¹‰çš„æ¢¯åº¦å¹¶è¿›è¡Œåç»­çš„ä¸¤é˜¶æ®µè®­ç»ƒã€‚å…¶æ¬¡ï¼Œå®æ–½åœ¨çº¿ä¼˜åŠ¿å·®å¼‚åŒ–ï¼Œè®¡ç®—ç»„åˆ«çš„ç»éªŒå‡†ç¡®åº¦ä½œä¸ºå›°éš¾çš„ä»£ç†ï¼Œé€‚åº”æ€§åœ°é‡æ–°åŠ æƒä¼˜åŠ¿ä¼°è®¡ï¼Œä¸ºæ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜æä¾›æ›´å¼ºçš„å­¦ä¹ ä¿¡å·ã€‚æœ€åï¼Œåœ¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒä¸­å¼•å…¥å›°éš¾æç¤ºï¼Œé¼“åŠ±æ¨¡å‹æ ¡å‡†æ¨ç†æ·±åº¦å¹¶è¿›è¡Œåæ€æ€§éªŒè¯æ£€æŸ¥ã€‚æˆ‘ä»¬çš„ç»¼åˆæ–¹æ³•åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¡¨ç°æ˜¾è‘—ï¼Œä»…éœ€2K+0.6Kçš„ä¸¤é˜¶æ®µè®­ç»ƒæ•°æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨é—®é¢˜å›°éš¾å…ˆéªŒä¿¡æ¯çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ ·æœ¬æ—¶çš„å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ˜¾å¼å»ºæ¨¡é—®é¢˜çš„å›°éš¾å…ˆéªŒä¿¡æ¯ï¼Œé‡‡ç”¨ç¦»çº¿æ•°æ®æ•´ç†å’Œåœ¨çº¿ä¼˜åŠ¿å·®å¼‚åŒ–çš„æ–¹æ³•ï¼Œå¢å¼ºæ¨¡å‹å¯¹å›°éš¾é—®é¢˜çš„å­¦ä¹ ä¿¡å·ï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç¦»çº¿æ•°æ®æ•´ç†ã€åœ¨çº¿ä¼˜åŠ¿å·®å¼‚åŒ–å’Œå›°éš¾æç¤ºå¼•å…¥ã€‚é¦–å…ˆï¼Œåˆ†ææ•°æ®é›†çš„å›°éš¾åˆ†å¸ƒï¼Œè¿‡æ»¤å‡ºåˆé€‚çš„è®­ç»ƒæ ·æœ¬ï¼›å…¶æ¬¡ï¼Œè®¡ç®—ç»„åˆ«çš„ç»éªŒå‡†ç¡®åº¦ï¼Œé€‚åº”æ€§åœ°è°ƒæ•´å­¦ä¹ ä¿¡å·ï¼›æœ€åï¼Œåœ¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒä¸­å¼•å…¥å›°éš¾æç¤ºä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†æ·±åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé€šè¿‡å›°éš¾å…ˆéªŒä¿¡æ¯çš„æ˜¾å¼å»ºæ¨¡ï¼Œæä¾›äº†æ›´å¼ºçš„å­¦ä¹ ä¿¡å·ï¼Œæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å›°éš¾æ ·æœ¬æ—¶è¡¨ç°å‡ºæ›´é«˜çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¤šè½®é‡‡æ ·å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å›°éš¾æ ·æœ¬çš„åŠ æƒï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™å¼•å…¥äº†å›°éš¾æç¤ºæ¨¡å—ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„åæ€æ€§éªŒè¯èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•åï¼Œæ¨¡å‹åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æ ·æœ¬æ—¶ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦15%ã€‚ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨2K+0.6Kçš„è®­ç»ƒæ•°æ®æƒ…å†µä¸‹ï¼Œå±•ç°å‡ºæ›´å¼ºçš„å­¦ä¹ èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿå’Œæ™ºèƒ½é—®ç­”ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå¤æ‚é—®é¢˜çš„è§£å†³ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ·±åº¦ç†è§£å’Œæ¨ç†çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we investigate how explicitly modeling problem's difficulty prior information shapes the effectiveness of reinforcement learning based fine-tuning for multimodal reasoning. Our exploration mainly comprises of following three perspective: First, through offline data curation, we analyze the U-shaped difficulty distribution of two given datasets using the base model by multi-round sampling, and then filter out prompts that are either too simple or extremely difficult to provide meaningful gradients and perform subsequent two-stage training. Second, we implement an online advantage differentiation, computing group-wise empirical accuracy as a difficulty proxy to adaptively reweight advantages estimation, providing stronger learning signals for more challenging problems. Finally, we introduce difficulty hints as explicit prompts for more complex samples in the second training stage, encouraging the model to calibrate its reasoning depth and perform reflective validation checks. Our comprehensive approach demonstrates significant performances across various multi-modal mathematical reasoning benchmarks with only 2K+0.6K two-stage training data.

