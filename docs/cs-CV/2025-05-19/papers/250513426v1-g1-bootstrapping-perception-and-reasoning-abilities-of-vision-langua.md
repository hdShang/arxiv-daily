---
layout: default
title: G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning
---

# G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13426" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13426v1</a>
  <a href="https://arxiv.org/pdf/2505.13426.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13426v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13426v1', 'G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liang Chen, Hongcheng Gao, Tianyu Liu, Zhiqi Huang, Flood Sung, Xinyu Zhou, Yuxin Wu, Baobao Chang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**å¤‡æ³¨**: 21 pages, 14 figures, code released at https://github.com/chenllliang/G1

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/chenllliang/G1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLM-Gymä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆä¸­çš„å†³ç­–èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¸¸æˆæ™ºèƒ½ä½“` `æ„ŸçŸ¥æ¨ç†` `è‡ªæˆ‘æ¼”åŒ–è®­ç»ƒ` `VLM-Gym`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç®€å•æ¸¸æˆä¸­çš„è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å…¶ä½œä¸ºè‡ªä¸»ä»£ç†çš„æ½œåŠ›ã€‚
2. æå‡ºVLM-Gymç¯å¢ƒï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒG0å’ŒG1æ¨¡å‹ï¼Œå¢å¼ºæ¨¡å‹çš„æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ã€‚
3. G1æ¨¡å‹åœ¨æ‰€æœ‰æ¸¸æˆä¸­å‡è¶…è¶Šæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„é¢†å…ˆæ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨äº’åŠ¨æ€§å¼ºã€è§†è§‰ä¸°å¯Œçš„ç¯å¢ƒï¼ˆå¦‚æ¸¸æˆï¼‰ä¸­è¿›è¡Œæœ‰æ•ˆå†³ç­–æ—¶å´é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€â€œçŸ¥è¡Œä¸ä¸€â€çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VLM-Gymï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ŒåŒ…å«å¤šæ ·åŒ–çš„è§†è§‰æ¸¸æˆï¼Œå…·æœ‰ç»Ÿä¸€æ¥å£å’Œå¯è°ƒèŠ‚çš„éš¾åº¦ï¼Œæ—¨åœ¨å®ç°å¯æ‰©å±•çš„å¤šæ¸¸æˆå¹¶è¡Œè®­ç»ƒã€‚é€šè¿‡VLM-Gymï¼Œæˆ‘ä»¬åˆ©ç”¨çº¯å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è‡ªæˆ‘æ¼”åŒ–è®­ç»ƒG0æ¨¡å‹ï¼Œå±•ç°å‡ºæ–°å…´çš„æ„ŸçŸ¥å’Œæ¨ç†æ¨¡å¼ã€‚ä¸ºè¿›ä¸€æ­¥åº”å¯¹æ¸¸æˆå¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†G1æ¨¡å‹ï¼ŒG1åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¹‹å‰å¼•å…¥äº†æ„ŸçŸ¥å¢å¼ºçš„å†·å¯åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒG1æ¨¡å‹åœ¨æ‰€æœ‰æ¸¸æˆä¸­å‡ä¼˜äºå…¶æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶è¶…è¶Šäº†é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹å¦‚Claude-3.7-Sonnet-Thinkingã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚äº’åŠ¨ç¯å¢ƒï¼ˆå¦‚æ¸¸æˆï¼‰ä¸­çš„å†³ç­–èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç®€å•æ¸¸æˆä¸­çš„è¡¨ç°ä¸ç†æƒ³ï¼Œå¯¼è‡´å…¶ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„æ½œåŠ›å—åˆ°é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥VLM-Gymç¯å¢ƒï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¤šæ¸¸æˆå¹¶è¡Œè®­ç»ƒï¼Œæå‡æ¨¡å‹çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚G1æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒå‰è¿›è¡Œæ„ŸçŸ¥å¢å¼ºçš„å†·å¯åŠ¨ï¼Œä»¥åº”å¯¹æ¸¸æˆå¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬VLM-Gymç¯å¢ƒã€G0æ¨¡å‹çš„è‡ªæˆ‘æ¼”åŒ–è®­ç»ƒå’ŒG1æ¨¡å‹çš„å†·å¯åŠ¨ä¸å¾®è°ƒé˜¶æ®µã€‚VLM-Gymæä¾›äº†ç»Ÿä¸€æ¥å£å’Œå¯è°ƒèŠ‚çš„éš¾åº¦ï¼Œæ”¯æŒå¤šæ ·åŒ–çš„è§†è§‰æ¸¸æˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ„ŸçŸ¥å¢å¼ºçš„å†·å¯åŠ¨æœºåˆ¶ï¼Œä½¿å¾—G1æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤šæ ·åŒ–çš„æ¸¸æˆç¯å¢ƒã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å•ä¸€è®­ç»ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„ç›¸äº’ä¿ƒè¿›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç­–ç•¥åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†è°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒG1æ¨¡å‹åœ¨æ‰€æœ‰æµ‹è¯•æ¸¸æˆä¸­å‡è¶…è¶Šäº†å…¶æ•™å¸ˆæ¨¡å‹ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šä¼˜äºClaude-3.7-Sonnet-Thinkingç­‰é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æå‡å¹…åº¦ï¼Œè¯æ˜äº†æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›çš„ç›¸äº’ä¿ƒè¿›æ•ˆåº”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆæ™ºèƒ½ä½“ã€è™šæ‹ŸåŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å†³ç­–èƒ½åŠ›ï¼Œæœªæ¥å¯ä»¥åœ¨æ›´å¤æ‚çš„äº¤äº’ç¯å¢ƒä¸­å®ç°æ›´æ™ºèƒ½çš„è‡ªä¸»ä»£ç†ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing'' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents.

