---
layout: default
title: Industrial Synthetic Segment Pre-training
---

# Industrial Synthetic Segment Pre-training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13099" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13099v2</a>
  <a href="https://arxiv.org/pdf/2505.13099.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13099v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13099v2', 'Industrial Synthetic Segment Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shinichi Mae, Ryousuke Yamada, Hirokatsu Kataoka

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19 (æ›´æ–°: 2025-05-20)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå·¥ä¸šåˆæˆåˆ†å‰²é¢„è®­ç»ƒæ•°æ®é›†ä»¥è§£å†³å›¾åƒæ•°æ®ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®ä¾‹åˆ†å‰²` `åˆæˆæ•°æ®é›†` `å·¥ä¸šåº”ç”¨` `è§†è§‰æ¨¡å‹` `æ•°æ®æ•ˆç‡` `ç›‘ç£å­¦ä¹ ` `é¢†åŸŸé€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å·¥ä¸šåº”ç”¨ä¸­é¢ä¸´æ³•å¾‹é™åˆ¶å’Œé¢†åŸŸå·®è·ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„InsCoreæ•°æ®é›†é€šè¿‡åˆæˆç”Ÿæˆå·¥ä¸šç‰¹å¾çš„å®ä¾‹åˆ†å‰²å›¾åƒï¼Œæ— éœ€çœŸå®å›¾åƒå’Œäººå·¥æ ‡æ³¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨InsCoreé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å®ä¾‹åˆ†å‰²é¢†åŸŸï¼ŒåŸºäºçœŸå®å›¾åƒæ•°æ®é›†çš„é¢„è®­ç»ƒå·²è¢«å¹¿æ³›è¯æ˜æœ‰æ•ˆã€‚ç„¶è€Œï¼Œå·¥ä¸šåº”ç”¨é¢ä¸´æ³•å¾‹å’Œä¼¦ç†é™åˆ¶ï¼Œä»¥åŠç½‘ç»œå›¾åƒä¸å·¥ä¸šå›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å®ä¾‹æ ¸å¿ƒåˆ†å‰²æ•°æ®é›†ï¼ˆInsCoreï¼‰ï¼Œè¯¥æ•°æ®é›†åŸºäºå…¬å¼é©±åŠ¨çš„ç›‘ç£å­¦ä¹ ç”Ÿæˆå®Œå…¨æ ‡æ³¨çš„åˆæˆå®ä¾‹åˆ†å‰²å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨InsCoreé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨äº”ä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåœ¨COCOå’ŒImageNet-21kä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œä»¥åŠå¾®è°ƒåçš„SAMï¼Œå¹³å‡æå‡6.2ä¸ªç™¾åˆ†ç‚¹ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æ•°æ®æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å·¥ä¸šåº”ç”¨ä¸­å®ä¾‹åˆ†å‰²æ¨¡å‹å¯¹çœŸå®å›¾åƒå’Œäººå·¥æ ‡æ³¨çš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ³•å¾‹å’Œé¢†åŸŸé€‚åº”æ€§ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå®ä¾‹æ ¸å¿ƒåˆ†å‰²æ•°æ®é›†ï¼ˆInsCoreï¼‰ï¼Œé€šè¿‡å…¬å¼é©±åŠ¨çš„ç›‘ç£å­¦ä¹ ç”Ÿæˆåˆæˆå›¾åƒï¼Œåæ˜ å·¥ä¸šæ•°æ®çš„ç‰¹å¾ï¼Œé¿å…äº†å¯¹çœŸå®å›¾åƒçš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInsCoreæ•°æ®é›†çš„ç”Ÿæˆæµç¨‹åŒ…æ‹¬æ•°æ®åˆæˆã€æ ‡æ³¨ç”Ÿæˆå’Œç‰¹å¾åæ˜ ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒå…·æœ‰å¤æ‚é®æŒ¡ã€å¯†é›†å±‚æ¬¡æ©è†œå’Œå¤šæ ·çš„éåˆšæ€§å½¢çŠ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šInsCoreçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å®Œå…¨åˆæˆçš„ç‰¹æ€§ï¼Œé¿å…äº†æ³•å¾‹å’Œä¼¦ç†é—®é¢˜ï¼ŒåŒæ—¶åœ¨å·¥ä¸šæ•°æ®ç‰¹å¾çš„åæ˜ ä¸Šä¼˜äºä¼ ç»Ÿçš„çœŸå®å›¾åƒæ•°æ®é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®åˆæˆè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šæ³¨é‡å¯¹å·¥ä¸šç‰¹å¾çš„æ•æ‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨InsCoreé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨äº”ä¸ªå·¥ä¸šæ•°æ®é›†ä¸Šçš„å®ä¾‹åˆ†å‰²æ€§èƒ½å¹³å‡æå‡6.2ä¸ªç™¾åˆ†ç‚¹ï¼Œè¶…è¶Šäº†åœ¨COCOå’ŒImageNet-21kä¸Šè®­ç»ƒçš„æ¨¡å‹ä»¥åŠå¾®è°ƒåçš„SAMï¼Œå±•ç°å‡ºæé«˜çš„æ•°æ®æ•ˆç‡ï¼Œä»…ä½¿ç”¨10ä¸‡å¼ åˆæˆå›¾åƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åˆ¶é€ ä¸šã€è‡ªåŠ¨åŒ–æ£€æµ‹å’Œæœºå™¨äººè§†è§‰ç­‰ï¼Œèƒ½å¤Ÿä¸ºå·¥ä¸šç•Œæä¾›ä¸€ç§æ— éœ€çœŸå®å›¾åƒå’Œäººå·¥æ ‡æ³¨çš„é«˜æ•ˆå®ä¾‹åˆ†å‰²è§£å†³æ–¹æ¡ˆã€‚æœªæ¥ï¼ŒInsCoreæœ‰æœ›æ¨åŠ¨å·¥ä¸šè§†è§‰æ¨¡å‹çš„æ™®åŠå’Œåº”ç”¨ï¼Œé™ä½æ•°æ®è·å–æˆæœ¬ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pre-training on real-image datasets has been widely proven effective for improving instance segmentation. However, industrial applications face two key challenges: (1) legal and ethical restrictions, such as ImageNet's prohibition of commercial use, and (2) limited transferability due to the domain gap between web images and industrial imagery. Even recent vision foundation models, including the segment anything model (SAM), show notable performance degradation in industrial settings. These challenges raise critical questions: Can we build a vision foundation model for industrial applications without relying on real images or manual annotations? And can such models outperform even fine-tuned SAM on industrial datasets? To address these questions, we propose the Instance Core Segmentation Dataset (InsCore), a synthetic pre-training dataset based on formula-driven supervised learning (FDSL). InsCore generates fully annotated instance segmentation images that reflect key characteristics of industrial data, including complex occlusions, dense hierarchical masks, and diverse non-rigid shapes, distinct from typical web imagery. Unlike previous methods, InsCore requires neither real images nor human annotations. Experiments on five industrial datasets show that models pre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as well as fine-tuned SAM, achieving an average improvement of 6.2 points in instance segmentation performance. This result is achieved using only 100k synthetic images, more than 100 times fewer than the 11 million images in SAM's SA-1B dataset, demonstrating the data efficiency of our approach. These findings position InsCore as a practical and license-free vision foundation model for industrial applications.

