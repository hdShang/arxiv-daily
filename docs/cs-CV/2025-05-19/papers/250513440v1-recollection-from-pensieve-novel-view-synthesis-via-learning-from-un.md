---
layout: default
title: Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos
---

# Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13440" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13440v1</a>
  <a href="https://arxiv.org/pdf/2505.13440.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13440v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13440v1', 'Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruoyu Wang, Yi Ma, Shenghua Gao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-19

**å¤‡æ³¨**: 13 pages, 4 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Dwawayu/Pensieve)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ–°æ–¹æ³•ä»¥è§£å†³æ— æ ‡å®šè§†é¢‘çš„è§†å›¾åˆæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æ— æ ‡å®šè§†é¢‘` `è§†å›¾åˆæˆ` `è‡ªç›‘ç£å­¦ä¹ ` `3Dé‡å»º` `é«˜æ–¯åŸè¯­` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†å›¾åˆæˆæ¨¡å‹å¤§å¤šä¾èµ–äºæ ‡å®šç›¸æœºæˆ–å‡ ä½•å…ˆéªŒï¼Œé™åˆ¶äº†å…¶åœ¨æ— æ ‡å®šæ•°æ®ä¸Šçš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µéšå¼é‡å»ºåœºæ™¯ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡æ˜¾å¼3Dé«˜æ–¯åŸè¯­å‡å°‘æ½œåœ¨è¡¨ç¤ºä¸çœŸå®ä¸–ç•Œçš„å·®è·ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨è§†å›¾åˆæˆå’Œç›¸æœºå§¿æ€ä¼°è®¡ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å‡ ä¹æ‰€æœ‰æœ€å…ˆè¿›çš„è§†å›¾åˆæˆå’Œé‡å»ºæ¨¡å‹éƒ½ä¾èµ–äºæ ‡å®šç›¸æœºæˆ–é¢å¤–çš„å‡ ä½•å…ˆéªŒè¿›è¡Œè®­ç»ƒã€‚è¿™äº›å‰ææ¡ä»¶æ˜¾è‘—é™åˆ¶äº†å®ƒä»¬åœ¨å¤§è§„æ¨¡æ— æ ‡å®šæ•°æ®ä¸Šçš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜å¹¶é‡Šæ”¾å¯¹å¤§è§„æ¨¡æ— æ ‡å®šè§†é¢‘çš„è‡ªç›‘ç£è®­ç»ƒæ½œåŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µç­–ç•¥ï¼Œä»…ä½¿ç”¨åŸå§‹è§†é¢‘å¸§æˆ–å¤šè§†å›¾å›¾åƒè¿›è¡Œè§†å›¾åˆæˆæ¨¡å‹çš„è®­ç»ƒï¼Œè€Œæ— éœ€æä¾›ç›¸æœºå‚æ•°æˆ–å…¶ä»–å…ˆéªŒã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­éšå¼é‡å»ºåœºæ™¯ï¼Œé¢„æµ‹æ¯å¸§çš„æ½œåœ¨ç›¸æœºå’Œåœºæ™¯ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œå¹¶å°†è§†å›¾åˆæˆæ¨¡å‹ä½œä¸ºæ˜¾å¼æ¸²æŸ“çš„ä»£ç†ã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡æ˜¾å¼é¢„æµ‹3Dé«˜æ–¯åŸè¯­æ¥å‡å°‘æ½œåœ¨è¡¨ç¤ºä¸çœŸå®3Dä¸–ç•Œä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨è§†å›¾åˆæˆå’Œç›¸æœºå§¿æ€ä¼°è®¡æ–¹é¢ä¼˜äºä¾èµ–æ ‡å®šã€å§¿æ€æˆ–æ·±åº¦ä¿¡æ¯çš„å…¶ä»–æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰è§†å›¾åˆæˆæ¨¡å‹å¯¹æ ‡å®šç›¸æœºå’Œå‡ ä½•å…ˆéªŒçš„ä¾èµ–é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æ— æ ‡å®šè§†é¢‘æ•°æ®ä¸Šçš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡éšå¼å­¦ä¹ é‡å»ºåœºæ™¯ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡æ˜¾å¼3Dé«˜æ–¯åŸè¯­æ¥å‡å°‘æ½œåœ¨è¡¨ç¤ºä¸çœŸå®3Dä¸–ç•Œä¹‹é—´çš„å·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µå­¦ä¹ æ½œåœ¨ç›¸æœºå’Œåœºæ™¯ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡æ˜¾å¼é«˜æ–¯åŸè¯­å’ŒæŸå¤±å‡½æ•°æ¥å¯¹é½æ½œåœ¨è¡¨ç¤ºä¸3Då‡ ä½•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ— é¡»æ ‡å®šæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥å®ç°äº†é«˜è´¨é‡çš„è§†å›¾åˆæˆï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è‡ªç›‘ç£å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé‡‡ç”¨æ½œåœ¨ç©ºé—´éšå¼é‡å»ºï¼Œç¬¬äºŒé˜¶æ®µå¼•å…¥é«˜æ–¯ç‚¹äº‘å’Œæ·±åº¦æŠ•å½±æŸå¤±ï¼Œç¡®ä¿å­¦ä¹ åˆ°çš„è¡¨ç¤ºä¸ç‰©ç†3Då‡ ä½•ä¸€è‡´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨è§†å›¾åˆæˆè´¨é‡å’Œç›¸æœºå§¿æ€ä¼°è®¡ä¸Šæ˜¾è‘—ä¼˜äºä¾èµ–æ ‡å®šçš„åŸºçº¿æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨æ— æ ‡å®šè§†é¢‘å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œè®¡ç®—æœºå›¾å½¢å­¦ç­‰ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡å®šç›¸æœºçš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡çš„è§†å›¾åˆæˆï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Currently almost all state-of-the-art novel view synthesis and reconstruction models rely on calibrated cameras or additional geometric priors for training. These prerequisites significantly limit their applicability to massive uncalibrated data. To alleviate this requirement and unlock the potential for self-supervised training on large-scale uncalibrated videos, we propose a novel two-stage strategy to train a view synthesis model from only raw video frames or multi-view images, without providing camera parameters or other priors. In the first stage, we learn to reconstruct the scene implicitly in a latent space without relying on any explicit 3D representation. Specifically, we predict per-frame latent camera and scene context features, and employ a view synthesis model as a proxy for explicit rendering. This pretraining stage substantially reduces the optimization complexity and encourages the network to learn the underlying 3D consistency in a self-supervised manner. The learned latent camera and implicit scene representation have a large gap compared with the real 3D world. To reduce this gap, we introduce the second stage training by explicitly predicting 3D Gaussian primitives. We additionally apply explicit Gaussian Splatting rendering loss and depth projection loss to align the learned latent representations with physically grounded 3D geometry. In this way, Stage 1 provides a strong initialization and Stage 2 enforces 3D consistency - the two stages are complementary and mutually beneficial. Extensive experiments demonstrate the effectiveness of our approach, achieving high-quality novel view synthesis and accurate camera pose estimation, compared to methods that employ supervision with calibration, pose, or depth information. The code is available at https://github.com/Dwawayu/Pensieve.

