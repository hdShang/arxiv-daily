---
layout: default
title: "Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence"
---

# Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.04619" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.04619v1</a>
  <a href="https://arxiv.org/pdf/2512.04619.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04619v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.04619v1', 'Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyu Yuan, Yuanbo Yang, Lin-Zhuo Chen, Yao Yao, Zhuzhong Qian

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHeFTï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£å…ˆéªŒå®ç°é²æ£’çš„é›¶æ ·æœ¬ç‚¹è·Ÿè¸ª**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `é›¶æ ·æœ¬å­¦ä¹ ` `ç‚¹è·Ÿè¸ª` `è§†é¢‘æ‰©æ•£æ¨¡å‹` `è§†è§‰å…ˆéªŒ` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é›¶æ ·æœ¬ç‚¹è·Ÿè¸ªä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹å¯¹è§†é¢‘æ—¶ç©ºä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ã€‚
2. HeFTåˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è§†è§‰å…ˆéªŒï¼Œé€šè¿‡åˆ†æVDiTçš„å†…éƒ¨è¡¨ç¤ºï¼Œå®ç°æ›´é²æ£’çš„è·Ÿè¸ªã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHeFTåœ¨TAP-VidåŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬è·Ÿè¸ªæ€§èƒ½ï¼Œæ¥è¿‘ç›‘ç£æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHeFTï¼ˆHead-Frequency Trackerï¼‰çš„é›¶æ ·æœ¬ç‚¹è·Ÿè¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº†é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è§†è§‰å…ˆéªŒã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è§†é¢‘æ‰©æ•£Transformerï¼ˆVDiTï¼‰å¦‚ä½•ç¼–ç æ—¶ç©ºä¿¡æ¯ï¼Œæˆ‘ä»¬åˆ†æäº†å…¶å†…éƒ¨è¡¨ç¤ºã€‚åˆ†æè¡¨æ˜ï¼Œæ³¨æ„åŠ›å¤´ä½œä¸ºæœ€å°åŠŸèƒ½å•å…ƒï¼Œåœ¨åŒ¹é…ã€è¯­ä¹‰ç†è§£å’Œä½ç½®ç¼–ç æ–¹é¢å…·æœ‰ä¸åŒçš„ä¸“ä¸šåŒ–åˆ†å·¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°VDiTç‰¹å¾ä¸­çš„ä½é¢‘åˆ†é‡å¯¹äºå»ºç«‹å¯¹åº”å…³ç³»è‡³å…³é‡è¦ï¼Œè€Œé«˜é¢‘åˆ†é‡å¾€å¾€ä¼šå¼•å…¥å™ªå£°ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤´å’Œé¢‘ç‡æ„ŸçŸ¥çš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è”åˆé€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„æ³¨æ„åŠ›å¤´å’Œä½é¢‘åˆ†é‡ï¼Œä»¥æé«˜è·Ÿè¸ªæ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å•æ­¥å»å™ªæå–åˆ¤åˆ«æ€§ç‰¹å¾ï¼Œåº”ç”¨ç‰¹å¾é€‰æ‹©ï¼Œå¹¶é‡‡ç”¨å…·æœ‰å‰åä¸€è‡´æ€§æ£€æŸ¥çš„è½¯argmaxå®šä½è¿›è¡Œå¯¹åº”å…³ç³»ä¼°è®¡ã€‚åœ¨TAP-VidåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHeFTå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬è·Ÿè¸ªæ€§èƒ½ï¼Œæ¥è¿‘äºç›‘ç£æ–¹æ³•çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ¶ˆé™¤äº†å¯¹å¸¦æ³¨é‡Šè®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„å·¥ä½œè¿›ä¸€æ­¥å¼ºè°ƒäº†è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§åŸºç¡€æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œä¸ºç»Ÿä¸€çš„è§†è§‰åŸºç¡€æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é›¶æ ·æœ¬ç‚¹è·Ÿè¸ªé—®é¢˜ï¼Œå³åœ¨æ²¡æœ‰æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•å‡†ç¡®åœ°è·Ÿè¸ªè§†é¢‘ä¸­çš„ç‰¹å®šç‚¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„ç‰¹å¾æˆ–åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œä¸”éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è§†é¢‘çš„æ—¶ç©ºä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚VDiTï¼‰ä¸­è•´å«çš„ä¸°å¯Œè§†è§‰å…ˆéªŒçŸ¥è¯†ã€‚é€šè¿‡åˆ†æVDiTçš„å†…éƒ¨è¡¨ç¤ºï¼Œå‘ç°ä¸åŒæ³¨æ„åŠ›å¤´å’Œé¢‘ç‡åˆ†é‡åœ¨æ—¶ç©ºä¿¡æ¯ç¼–ç ä¸­çš„ä½œç”¨ï¼Œå¹¶é€‰æ‹©æœ€é€‚åˆè·Ÿè¸ªçš„ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHeFTæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) ç‰¹å¾æå–ï¼šé€šè¿‡å•æ­¥å»å™ªè¿‡ç¨‹ä»VDiTä¸­æå–ç‰¹å¾ã€‚2) ç‰¹å¾é€‰æ‹©ï¼šé‡‡ç”¨å¤´å’Œé¢‘ç‡æ„ŸçŸ¥çš„ç­–ç•¥ï¼Œé€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„æ³¨æ„åŠ›å¤´å’Œä½é¢‘åˆ†é‡ã€‚3) å¯¹åº”å…³ç³»ä¼°è®¡ï¼šä½¿ç”¨è½¯argmaxå®šä½å’Œå‰åä¸€è‡´æ€§æ£€æŸ¥æ¥ä¼°è®¡å¯¹åº”å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å¤´å’Œé¢‘ç‡æ„ŸçŸ¥çš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚é€šè¿‡åˆ†æVDiTçš„å†…éƒ¨è¡¨ç¤ºï¼Œå‘ç°ä¸åŒæ³¨æ„åŠ›å¤´å’Œé¢‘ç‡åˆ†é‡åœ¨æ—¶ç©ºä¿¡æ¯ç¼–ç ä¸­çš„ä½œç”¨ï¼Œå¹¶é€‰æ‹©æœ€é€‚åˆè·Ÿè¸ªçš„ç‰¹å¾ã€‚è¿™ç§ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°å»é™¤å™ªå£°ï¼Œæé«˜è·Ÿè¸ªçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å•æ­¥å»å™ªæå–ç‰¹å¾ï¼Œå‡å°‘è®¡ç®—é‡ã€‚2) è®¾è®¡å¤´å’Œé¢‘ç‡æ„ŸçŸ¥çš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œé€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„ç‰¹å¾ã€‚3) é‡‡ç”¨è½¯argmaxå®šä½å’Œå‰åä¸€è‡´æ€§æ£€æŸ¥ï¼Œæé«˜å¯¹åº”å…³ç³»ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HeFTåœ¨TAP-VidåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬è·Ÿè¸ªæ€§èƒ½ï¼Œå¹¶ä¸”æ€§èƒ½æ¥è¿‘æœ‰ç›‘ç£æ–¹æ³•ã€‚è¿™è¡¨æ˜äº†é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸï¼Œå®ç°å¯¹è§†é¢‘ä¸­ç‰¹å®šç›®æ ‡çš„ç²¾ç¡®è·Ÿè¸ªã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è§†è§‰å…ˆéªŒï¼Œå¯ä»¥é™ä½å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæé«˜è·Ÿè¸ªç³»ç»Ÿçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ›´å¹¿æ³›çš„è§†è§‰ä»»åŠ¡æä¾›åŸºç¡€æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.

