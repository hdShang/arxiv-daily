---
layout: default
title: "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging"
---

# LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.04939" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.04939v1</a>
  <a href="https://arxiv.org/pdf/2512.04939.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04939v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.04939v1', 'LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhijian Shu, Cheng Lin, Tao Xie, Wei Yin, Ben Li, Zhiyuan Pu, Weize Li, Yao Yao, Xun Cao, Xiaoyang Guo, Xiao-Xiao Long

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://garlicba.github.io/LiteVGGT/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LiteVGGTï¼šé€šè¿‡å‡ ä½•æ„ŸçŸ¥ç¼“å­˜Tokenåˆå¹¶åŠ é€ŸVGGTï¼Œå®ç°å¤§è§„æ¨¡åœºæ™¯é«˜æ•ˆ3Dé‡å»ºã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dé‡å»º` `è§†è§‰å‡ ä½•Transformer` `æ¨¡å‹åŠ é€Ÿ` `Tokenåˆå¹¶` `å‡ ä½•æ„ŸçŸ¥` `ç¼“å­˜æœºåˆ¶` `å¤§è§„æ¨¡åœºæ™¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VGGTç­‰3Dè§†è§‰æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶è®¡ç®—å’Œå†…å­˜å¼€é”€å¤§ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. LiteVGGTé€šè¿‡å‡ ä½•æ„ŸçŸ¥ç¼“å­˜tokenåˆå¹¶ï¼Œåˆ©ç”¨å±€éƒ¨tokençš„å‡ ä½•ç›¸å…³æ€§å’Œå±‚é—´tokenç›¸ä¼¼æ€§ï¼Œå‡å°‘è®¡ç®—å†—ä½™ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLiteVGGTå®ç°äº†é«˜è¾¾10å€çš„åŠ é€Ÿå’Œæ˜¾è‘—çš„å†…å­˜å‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†VGGTçš„æ ¸å¿ƒæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å‡ ä½•åŸºç¡€Transformer (VGGT) ç­‰3Dè§†è§‰åŸºç¡€æ¨¡å‹åœ¨å‡ ä½•æ„ŸçŸ¥æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹äºé•¿åºåˆ—è€Œè¨€ï¼Œå…¶è®¡ç®—è€—æ—¶å’Œå†…å­˜å ç”¨è¾ƒé«˜ï¼Œé™åˆ¶äº†å…¶åœ¨æ•°ç™¾å¼ å›¾åƒä»¥ä¸Šçš„å¤§è§„æ¨¡åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LiteVGGTï¼Œå®ç°äº†é«˜è¾¾10å€çš„åŠ é€Ÿå’Œæ˜¾è‘—çš„å†…å­˜å‡å°‘ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†åŒ…å«1000å¼ å›¾åƒçš„åœºæ™¯ã€‚æˆ‘ä»¬ä¸º3Dé‡å»ºæ¨å¯¼å‡ºäº†ä¸¤ä¸ªå…³é”®è§è§£ï¼š(1) æ¥è‡ªå±€éƒ¨å›¾åƒåŒºåŸŸçš„tokenså…·æœ‰å›ºæœ‰çš„å‡ ä½•ç›¸å…³æ€§ï¼Œå¯¼è‡´é«˜åº¦ç›¸ä¼¼æ€§å’Œè®¡ç®—å†—ä½™ï¼›(2) ç›¸é‚»ç½‘ç»œå±‚ä¹‹é—´çš„tokenç›¸ä¼¼æ€§ä¿æŒç¨³å®šï¼Œå…è®¸é‡å¤ä½¿ç”¨åˆå¹¶å†³ç­–ã€‚åœ¨è¿™äº›è§è§£çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥ï¼Œç§°ä¸ºå‡ ä½•æ„ŸçŸ¥ç¼“å­˜tokenåˆå¹¶ã€‚æˆ‘ä»¬åˆ†ææ¯ä¸ªtokençš„å‡ ä½•é‡è¦æ€§ï¼Œä¼˜åŒ–anchor tokençš„é€‰æ‹©ï¼Œä»¥æ›´å¥½åœ°ä¿ç•™ç”¨äºé‡å»ºçš„å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜åœ¨å„å±‚ä¹‹é—´ç¼“å­˜å’Œé‡ç”¨åˆå¹¶ç´¢å¼•ï¼Œä»è€Œåœ¨æœ€å°åŒ–ç²¾åº¦å½±å“çš„åŒæ—¶æ˜¾è‘—é™ä½å»¶è¿Ÿã€‚è¯¥ç­–ç•¥ä¿ç•™äº†VGGTçš„æ ¸å¿ƒæ€§èƒ½ï¼Œä»è€Œå¯ä»¥è¿›è¡Œé«˜æ•ˆçš„å¾®è°ƒå’ŒFP8é‡åŒ–ä»¥è·å¾—è¿›ä¸€æ­¥çš„æ”¶ç›Šã€‚å¤§é‡çš„å®éªŒéªŒè¯äº†LiteVGGTçš„æœ‰æ•ˆæ€§ã€å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVGGTç­‰æ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯ï¼ˆä¾‹å¦‚åŒ…å«å¤§é‡å›¾åƒçš„3Dé‡å»ºä»»åŠ¡ï¼‰æ—¶ï¼Œè®¡ç®—é‡å’Œå†…å­˜å ç”¨è¿‡é«˜ï¼Œéš¾ä»¥åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå¯¹æ‰€æœ‰tokensè¿›è¡ŒåŒç­‰å¤„ç†ï¼Œå¿½ç•¥äº†å±€éƒ¨åŒºåŸŸtokensçš„å‡ ä½•ç›¸å…³æ€§å’Œå±‚é—´tokenç›¸ä¼¼æ€§ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å›¾åƒå±€éƒ¨åŒºåŸŸtokensçš„å‡ ä½•ç›¸å…³æ€§å’Œç›¸é‚»ç½‘ç»œå±‚ä¹‹é—´tokenç›¸ä¼¼æ€§çš„ç¨³å®šæ€§ï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥çš„ç¼“å­˜tokenåˆå¹¶ç­–ç•¥ï¼Œå‡å°‘è®¡ç®—å†—ä½™ã€‚å…·ä½“æ¥è¯´ï¼Œé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„anchor tokensï¼Œå¹¶ç¼“å­˜åˆå¹¶ç´¢å¼•ï¼Œä»è€ŒåŠ é€Ÿè®¡ç®—è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLiteVGGTçš„æ•´ä½“æ¡†æ¶åŸºäºVGGTï¼Œä¸»è¦æ”¹è¿›åœ¨äºtokenåˆå¹¶ç­–ç•¥ã€‚é¦–å…ˆï¼Œåˆ†ææ¯ä¸ªtokençš„å‡ ä½•é‡è¦æ€§ï¼Œé€‰æ‹©åˆé€‚çš„anchor tokensã€‚ç„¶åï¼Œåœ¨ç½‘ç»œå±‚ä¹‹é—´ç¼“å­˜å’Œé‡ç”¨åˆå¹¶ç´¢å¼•ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚è¯¥æ¡†æ¶åŒ…å«å‡ ä½•é‡è¦æ€§åˆ†ææ¨¡å—ã€anchor tokené€‰æ‹©æ¨¡å—å’Œç¼“å­˜åˆå¹¶ç´¢å¼•æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯å‡ ä½•æ„ŸçŸ¥ç¼“å­˜tokenåˆå¹¶ç­–ç•¥ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒLiteVGGTä¸æ˜¯å¯¹æ‰€æœ‰tokensè¿›è¡ŒåŒç­‰å¤„ç†ï¼Œè€Œæ˜¯æ ¹æ®å‡ ä½•é‡è¦æ€§é€‰æ‹©anchor tokensï¼Œå¹¶åˆ©ç”¨å±‚é—´tokenç›¸ä¼¼æ€§çš„ç¨³å®šæ€§ï¼Œç¼“å­˜å’Œé‡ç”¨åˆå¹¶ç´¢å¼•ã€‚è¿™ç§ç­–ç•¥åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—é‡å’Œå†…å­˜å ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šå‡ ä½•é‡è¦æ€§åˆ†æå¯èƒ½æ¶‰åŠè®¡ç®—æ¯ä¸ªtokençš„æ¢¯åº¦æˆ–æ³¨æ„åŠ›æƒé‡ï¼Œé€‰æ‹©æ¢¯åº¦æˆ–æƒé‡è¾ƒé«˜çš„tokenä½œä¸ºanchor tokensã€‚ç¼“å­˜åˆå¹¶ç´¢å¼•çš„è®¾è®¡éœ€è¦è€ƒè™‘ç¼“å­˜å¤§å°å’ŒæŸ¥æ‰¾æ•ˆç‡ï¼Œå¯ä»¥ä½¿ç”¨å“ˆå¸Œè¡¨ç­‰æ•°æ®ç»“æ„ã€‚æŸå¤±å‡½æ•°ä¸VGGTä¿æŒä¸€è‡´ï¼Œç½‘ç»œç»“æ„ä¹ŸåŸºäºVGGTè¿›è¡Œå¾®è°ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLiteVGGTåœ¨ä¿æŒVGGTæ ¸å¿ƒæ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾10å€çš„åŠ é€Ÿå’Œæ˜¾è‘—çš„å†…å­˜å‡å°‘ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†åŒ…å«1000å¼ å›¾åƒçš„åœºæ™¯ã€‚é€šè¿‡é«˜æ•ˆå¾®è°ƒå’ŒFP8é‡åŒ–ï¼ŒLiteVGGTå¯ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†LiteVGGTçš„æœ‰æ•ˆæ€§ã€å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LiteVGGTå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡åœºæ™¯çš„3Dé‡å»ºã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨ï¼ŒLiteVGGTä½¿å¾—åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œå¤§è§„æ¨¡3Dåœºæ™¯ç†è§£æˆä¸ºå¯èƒ½ï¼ŒåŠ é€Ÿäº†ç›¸å…³æŠ€æœ¯çš„è½åœ°å’Œæ™®åŠï¼Œå¹¶ä¸ºæœªæ¥çš„ä¸‰ç»´è§†è§‰åº”ç”¨æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/

