---
layout: default
title: FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization
---

# FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.04952" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.04952v2</a>
  <a href="https://arxiv.org/pdf/2512.04952.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04952v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.04952v2', 'FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yicheng Liu, Shiduo Zhang, Zibin Dong, Baijun Ye, Tianyuan Yuan, Xiaopeng Yu, Linqi Yin, Chenhao Lu, Junhao Shi, Luca Jiang-Tao Yu, Liangtao Zheng, Tao Jiang, Jingjing Gong, Xipeng Qiu, Hang Zhao

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04 (æ›´æ–°: 2025-12-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FASTerï¼šé€šè¿‡ç¥ç»åŠ¨ä½œæ ‡è®°åŒ–å®ç°é«˜æ•ˆçš„è‡ªå›å½’è§†è§‰-è¯­è¨€-åŠ¨ä½œå»ºæ¨¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `è‡ªå›å½’æ¨¡å‹` `åŠ¨ä½œæ ‡è®°åŒ–` `å‘é‡é‡åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªå›å½’VLAæ¨¡å‹åœ¨åŠ¨ä½œæ ‡è®°åŒ–è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åœ¨é‡å»ºè´¨é‡å’Œæ¨ç†æ•ˆç‡ä¹‹é—´åšå‡ºæƒè¡¡ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. FASTeræ¡†æ¶é€šè¿‡å¯å­¦ä¹ çš„æ ‡è®°å™¨å’Œè‡ªå›å½’ç­–ç•¥çš„é›†æˆï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯æ³›åŒ–çš„æœºå™¨äººå­¦ä¹ ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFASTeråœ¨é‡å»ºè´¨é‡ã€ä»¤ç‰Œåˆ©ç”¨ç‡ã€æ³›åŒ–èƒ½åŠ›ä»¥åŠæ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½æ–¹é¢å‡ä¼˜äºç°æœ‰VLAæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªå›å½’è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹æœ€è¿‘åœ¨æœºå™¨äººæ“ä½œæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶æ ¸å¿ƒçš„åŠ¨ä½œæ ‡è®°åŒ–è¿‡ç¨‹é€šå¸¸éœ€è¦åœ¨é‡å»ºä¿çœŸåº¦å’Œæ¨ç†æ•ˆç‡ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†FASTerï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆä¸”å¯æ³›åŒ–çš„æœºå™¨äººå­¦ä¹ ï¼Œå®ƒé›†æˆäº†å¯å­¦ä¹ çš„æ ‡è®°å™¨å’ŒåŸºäºå®ƒçš„è‡ªå›å½’ç­–ç•¥ã€‚FASTerVQå°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒï¼Œæ•è·å…¨å±€æ—¶ç©ºä¾èµ–å…³ç³»ï¼ŒåŒæ—¶ä¿æŒé«˜å‹ç¼©ç‡ã€‚FASTerVLAåœ¨æ­¤æ ‡è®°å™¨çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å—çŠ¶è‡ªå›å½’è§£ç å’Œè½»é‡çº§åŠ¨ä½œä¸“å®¶ï¼Œå®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´é«˜çš„ä»»åŠ¡æ€§èƒ½ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFASTerVQæä¾›äº†å“è¶Šçš„é‡å»ºè´¨é‡ã€é«˜ä»¤ç‰Œåˆ©ç”¨ç‡ä»¥åŠå¼ºå¤§çš„è·¨ä»»åŠ¡å’Œè·¨ç¯å¢ƒæ³›åŒ–èƒ½åŠ›ï¼Œè€ŒFASTerVLAè¿›ä¸€æ­¥æé«˜äº†æ•´ä½“èƒ½åŠ›ï¼Œåœ¨æ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½æ–¹é¢å‡è¶…è¿‡äº†å…ˆå‰çš„æœ€å…ˆè¿›çš„VLAæ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è‡ªå›å½’è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ ¸å¿ƒçš„åŠ¨ä½œæ ‡è®°åŒ–è¿‡ç¨‹é¢ä¸´ç€é‡å»ºä¿çœŸåº¦å’Œæ¨ç†æ•ˆç‡ä¹‹é—´çš„å›ºæœ‰çŸ›ç›¾ã€‚é«˜ä¿çœŸåº¦çš„æ ‡è®°åŒ–æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´å¤§é‡çš„åŠ¨ä½œtokenï¼Œä»è€Œé™ä½æ¨ç†é€Ÿåº¦ã€‚åä¹‹ï¼Œä¸ºäº†æé«˜æ•ˆç‡è€Œç‰ºç‰²é‡å»ºè´¨é‡åˆ™ä¼šå½±å“ä»»åŠ¡æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§æ—¢èƒ½ä¿è¯åŠ¨ä½œé‡å»ºè´¨é‡ï¼Œåˆèƒ½å®ç°é«˜æ•ˆæ¨ç†çš„VLAæ¨¡å‹æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFASTerçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„åŠ¨ä½œæ ‡è®°å™¨ï¼ˆFASTerVQï¼‰ï¼Œå°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒï¼Œä»è€Œæ•è·å…¨å±€æ—¶ç©ºä¾èµ–å…³ç³»å¹¶å®ç°é«˜å‹ç¼©ç‡ã€‚ç„¶åï¼ŒåŸºäºæ­¤æ ‡è®°å™¨æ„å»ºä¸€ä¸ªè‡ªå›å½’ç­–ç•¥ï¼ˆFASTerVLAï¼‰ï¼Œé€šè¿‡å—çŠ¶è‡ªå›å½’è§£ç å’Œè½»é‡çº§åŠ¨ä½œä¸“å®¶ï¼Œè¿›ä¸€æ­¥æé«˜æ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨è§£è€¦åŠ¨ä½œè¡¨ç¤ºå­¦ä¹ å’Œç­–ç•¥å­¦ä¹ ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆå’Œå¯æ³›åŒ–çš„æœºå™¨äººå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFASTeræ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šFASTerVQå’ŒFASTerVLAã€‚FASTerVQæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡é‡åŒ–å™¨ï¼Œè´Ÿè´£å°†è¿ç»­çš„åŠ¨ä½œç©ºé—´ç¦»æ•£åŒ–ä¸ºç¦»æ•£çš„åŠ¨ä½œtokenã€‚å®ƒå°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œæå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡å‘é‡é‡åŒ–å±‚å°†ç‰¹å¾æ˜ å°„åˆ°ç¦»æ•£çš„tokenç©ºé—´ã€‚FASTerVLAåˆ™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„è‡ªå›å½’æ¨¡å‹ï¼Œå®ƒä»¥FASTerVQç”Ÿæˆçš„åŠ¨ä½œtokenåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹æœªæ¥çš„åŠ¨ä½œã€‚å®ƒé‡‡ç”¨å—çŠ¶è‡ªå›å½’è§£ç ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„åŠ¨ä½œä¸“å®¶ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šFASTerçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æå‡ºçš„ç¥ç»åŠ¨ä½œæ ‡è®°åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·å…¨å±€æ—¶ç©ºä¾èµ–å…³ç³»ï¼Œå¹¶å®ç°é«˜å‹ç¼©ç‡ã€‚ä¸ä¼ ç»Ÿçš„å‘é‡é‡åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒFASTerVQèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™åŠ¨ä½œçš„æ—¶ç©ºä¿¡æ¯ï¼Œä»è€Œæé«˜é‡å»ºè´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒFASTerVLAé‡‡ç”¨å—çŠ¶è‡ªå›å½’è§£ç å’Œè½»é‡çº§åŠ¨ä½œä¸“å®¶ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šFASTerVQä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå‘é‡é‡åŒ–å±‚é‡‡ç”¨Gumbel-SoftmaxæŠ€å·§è¿›è¡Œè®­ç»ƒã€‚FASTerVLAä½¿ç”¨Transformerä½œä¸ºè‡ªå›å½’æ¨¡å‹ï¼Œå—å¤§å°è®¾ç½®ä¸ºå›ºå®šå€¼ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±å’Œé‡åŒ–æŸå¤±ï¼Œç”¨äºä¼˜åŒ–FASTerVQå’ŒFASTerVLAã€‚åŠ¨ä½œä¸“å®¶æ˜¯ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œï¼Œç”¨äºé¢„æµ‹åŠ¨ä½œçš„å‡å€¼å’Œæ–¹å·®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFASTerVQåœ¨åŠ¨ä½œé‡å»ºè´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å…·æœ‰æ›´é«˜çš„ä»¤ç‰Œåˆ©ç”¨ç‡å’Œæ›´å¼ºçš„è·¨ä»»åŠ¡å’Œè·¨ç¯å¢ƒæ³›åŒ–èƒ½åŠ›ã€‚FASTerVLAåœ¨æ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½æ–¹é¢å‡è¶…è¿‡äº†å…ˆå‰çš„æœ€å…ˆè¿›çš„VLAæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒFASTerVLAçš„æ¨ç†é€Ÿåº¦æé«˜äº†2å€ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†10%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FASTeræ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œå¦‚ç‰©ä½“æŠ“å–ã€è£…é…ã€å¯¼èˆªç­‰ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæé«˜æœºå™¨äººæ“ä½œçš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦é«˜æ•ˆåŠ¨ä½œè¡¨ç¤ºå­¦ä¹ çš„é¢†åŸŸï¼Œå¦‚æ¸¸æˆAIã€è™šæ‹Ÿç°å®ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.

