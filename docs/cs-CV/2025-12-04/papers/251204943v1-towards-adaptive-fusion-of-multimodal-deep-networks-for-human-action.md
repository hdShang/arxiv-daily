---
layout: default
title: Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition
---

# Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition

**arXiv**: [2512.04943v1](https://arxiv.org/abs/2512.04943) | [PDF](https://arxiv.org/pdf/2512.04943.pdf)

**ä½œè€…**: Novanto Yudistira

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽé—¨æŽ§æœºåˆ¶çš„å¤šæ¨¡æ€è‡ªé€‚åº”èžåˆç½‘ç»œï¼Œæå‡äººç±»è¡Œä¸ºè¯†åˆ«ç²¾åº¦**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `äººç±»è¡Œä¸ºè¯†åˆ«` `å¤šæ¨¡æ€èžåˆ` `é—¨æŽ§æœºåˆ¶` `æ·±åº¦å­¦ä¹ ` `è‡ªé€‚åº”åŠ æƒ` `è§†é¢‘åˆ†æž` `äººæœºäº¤äº’`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå•æ¨¡æ€è¡Œä¸ºè¯†åˆ«æ–¹æ³•å­˜åœ¨ä¿¡æ¯ä¸è¶³çš„å±€é™æ€§ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚åœºæ™¯ã€‚
2. æå‡ºåŸºäºŽé—¨æŽ§æœºåˆ¶çš„å¤šæ¨¡æ€è‡ªé€‚åº”èžåˆæ–¹æ³•ï¼Œé€‰æ‹©æ€§æ•´åˆä¸åŒæ¨¡æ€ä¿¡æ¯ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨äººç±»è¡Œä¸ºè¯†åˆ«ã€æš´åŠ›è¡Œä¸ºæ£€æµ‹ç­‰ä»»åŠ¡ä¸Šç²¾åº¦æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„äººç±»è¡Œä¸ºè¯†åˆ«æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦ç¥žç»ç½‘ç»œæŠ€æœ¯å’Œè·¨å¤šç§æ¨¡æ€ï¼ˆåŒ…æ‹¬RGBã€å…‰æµã€éŸ³é¢‘å’Œæ·±åº¦ä¿¡æ¯ï¼‰çš„è‡ªé€‚åº”èžåˆç­–ç•¥ã€‚é€šè¿‡é‡‡ç”¨é—¨æŽ§æœºåˆ¶è¿›è¡Œå¤šæ¨¡æ€èžåˆï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿå•æ¨¡æ€è¯†åˆ«æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æŽ¢ç´¢å„ç§åº”ç”¨çš„æ–°å¯èƒ½æ€§ã€‚é€šè¿‡å¯¹é—¨æŽ§æœºåˆ¶å’ŒåŸºäºŽè‡ªé€‚åº”åŠ æƒçš„èžåˆæž¶æž„çš„å…¨é¢ç ”ç©¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé€‰æ‹©æ€§åœ°æ•´åˆæ¥è‡ªå„ç§æ¨¡æ€çš„ç›¸å…³ä¿¡æ¯ï¼Œä»Žè€Œæé«˜åŠ¨ä½œè¯†åˆ«ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æˆ‘ä»¬ä»”ç»†ç ”ç©¶äº†å„ç§é—¨æŽ§èžåˆç­–ç•¥ï¼Œä»¥ç¡®å®šç”¨äºŽå¤šæ¨¡æ€åŠ¨ä½œè¯†åˆ«çš„æœ€æœ‰æ•ˆæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ä¼˜äºŽä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•çš„ä¼˜åŠ¿ã€‚é—¨æŽ§æœºåˆ¶æœ‰åŠ©äºŽæå–å…³é”®ç‰¹å¾ï¼Œä»Žè€Œå®žçŽ°æ›´å…¨é¢çš„åŠ¨ä½œè¡¨ç¤ºï¼Œå¹¶æ˜¾ç€æé«˜è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå¯¹äººç±»åŠ¨ä½œè¯†åˆ«ã€æš´åŠ›è¡Œä¸ºæ£€æµ‹å’Œå¤šé¡¹è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡çš„è¯„ä¼°è¡¨æ˜Žï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†å¯å–œçš„è¿›å±•ã€‚è¿™é¡¹ç ”ç©¶çš„æ„ä¹‰åœ¨äºŽå®ƒæœ‰å¯èƒ½å½»åº•æ”¹å˜å„ä¸ªé¢†åŸŸçš„åŠ¨ä½œè¯†åˆ«ç³»ç»Ÿã€‚å¤šæ¨¡æ€ä¿¡æ¯çš„èžåˆæœ‰æœ›åœ¨ç›‘æŽ§å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå®žçŽ°å¤æ‚çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Žä¸»åŠ¨è¾…åŠ©ç”Ÿæ´»ç›¸å…³çš„çŽ¯å¢ƒä¸­ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„äººç±»è¡Œä¸ºè¯†åˆ«æ–¹æ³•é€šå¸¸ä¾èµ–äºŽå•ä¸€æ¨¡æ€çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ä»…ä½¿ç”¨RGBå›¾åƒæˆ–å…‰æµã€‚è¿™ç§å•æ¨¡æ€æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹å®¹æ˜“å—åˆ°å…‰ç…§å˜åŒ–ã€é®æŒ¡ç­‰å› ç´ çš„å½±å“ï¼Œå¯¼è‡´è¯†åˆ«ç²¾åº¦ä¸‹é™ã€‚å¤šæ¨¡æ€èžåˆæ˜¯æå‡æ€§èƒ½çš„æœ‰æ•ˆé€”å¾„ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°èžåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œé¿å…å™ªå£°æ¨¡æ€çš„å¹²æ‰°ï¼Œæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é—¨æŽ§æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°å­¦ä¹ ä¸åŒæ¨¡æ€çš„é‡è¦æ€§ï¼Œå¹¶é€‰æ‹©æ€§åœ°èžåˆè¿™äº›æ¨¡æ€çš„ä¿¡æ¯ã€‚é—¨æŽ§æœºåˆ¶å¯ä»¥æ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„æƒé‡ï¼Œä»Žè€Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ›´åŠ å…³æ³¨é‡è¦çš„æ¨¡æ€ï¼ŒæŠ‘åˆ¶å™ªå£°æ¨¡æ€çš„å½±å“ã€‚è¿™ç§è‡ªé€‚åº”èžåˆç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ¨¡åž‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•çš„æŠ€æœ¯æ¡†æž¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨æ·±åº¦ç¥žç»ç½‘ç»œï¼ˆå¦‚CNNã€RNNï¼‰ä»Žæ¯ä¸ªæ¨¡æ€ï¼ˆRGBã€å…‰æµã€éŸ³é¢‘ã€æ·±åº¦ï¼‰ä¸­æå–ç‰¹å¾ã€‚2) é—¨æŽ§æ¨¡å—ï¼šä¸ºæ¯ä¸ªæ¨¡æ€è®¾ç½®ä¸€ä¸ªé—¨æŽ§å•å…ƒï¼Œç”¨äºŽå­¦ä¹ è¯¥æ¨¡æ€çš„é‡è¦æ€§æƒé‡ã€‚é—¨æŽ§å•å…ƒçš„è¾“å…¥æ˜¯è¯¥æ¨¡æ€çš„ç‰¹å¾ï¼Œè¾“å‡ºæ˜¯0åˆ°1ä¹‹é—´çš„æƒé‡å€¼ã€‚3) èžåˆæ¨¡å—ï¼šå°†å„ä¸ªæ¨¡æ€çš„ç‰¹å¾æŒ‰ç…§é—¨æŽ§å•å…ƒçš„æƒé‡è¿›è¡ŒåŠ æƒèžåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚4) åˆ†ç±»æ¨¡å—ï¼šä½¿ç”¨åˆ†ç±»å™¨ï¼ˆå¦‚Softmaxï¼‰å¯¹èžåˆåŽçš„ç‰¹å¾è¿›è¡Œåˆ†ç±»ï¼Œå¾—åˆ°æœ€ç»ˆçš„åŠ¨ä½œè¯†åˆ«ç»“æžœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå¼•å…¥äº†é—¨æŽ§æœºåˆ¶æ¥å®žçŽ°å¤šæ¨¡æ€ä¿¡æ¯çš„è‡ªé€‚åº”èžåˆã€‚ä¸Žä¼ ç»Ÿçš„å›ºå®šæƒé‡èžåˆæ–¹æ³•ç›¸æ¯”ï¼Œé—¨æŽ§æœºåˆ¶å¯ä»¥æ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹ç‚¹åŠ¨æ€åœ°è°ƒæ•´æ¯ä¸ªæ¨¡æ€çš„æƒé‡ï¼Œä»Žè€Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ›´åŠ å…³æ³¨é‡è¦çš„æ¨¡æ€ï¼ŒæŠ‘åˆ¶å™ªå£°æ¨¡æ€çš„å½±å“ã€‚è¿™ç§è‡ªé€‚åº”èžåˆç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ¨¡åž‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šé—¨æŽ§å•å…ƒé€šå¸¸é‡‡ç”¨Sigmoidå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œå°†è¾“å‡ºå€¼é™åˆ¶åœ¨0åˆ°1ä¹‹é—´ï¼Œè¡¨ç¤ºè¯¥æ¨¡æ€çš„é‡è¦æ€§æƒé‡ã€‚æŸå¤±å‡½æ•°é€šå¸¸é‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºŽè¡¡é‡æ¨¡åž‹é¢„æµ‹ç»“æžœä¸ŽçœŸå®žæ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚ç½‘ç»œç»“æž„å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œæ¥æå–æ›´å¤æ‚çš„ç‰¹å¾ï¼Œæˆ–è€…ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è¿›ä¸€æ­¥æé«˜æ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥è®ºæ–‡åœ¨äººç±»åŠ¨ä½œè¯†åˆ«ã€æš´åŠ›è¡Œä¸ºæ£€æµ‹å’Œå¤šé¡¹è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“çš„æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œè¡¨æ˜Žè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†å¯å–œçš„è¿›å±•ã€‚å®žéªŒç»“æžœè¯æ˜Žäº†è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€åŠ¨ä½œè¯†åˆ«æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽè§†é¢‘ç›‘æŽ§ã€äººæœºäº¤äº’ã€æ™ºèƒ½å®¶å±…ã€åŒ»ç–—å¥åº·ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æŽ§ä¸­ï¼Œå¯ä»¥ç”¨äºŽè‡ªåŠ¨æ£€æµ‹æš´åŠ›è¡Œä¸ºæˆ–å¼‚å¸¸äº‹ä»¶ï¼›åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥ç”¨äºŽè¯†åˆ«ç”¨æˆ·çš„åŠ¨ä½œæŒ‡ä»¤ï¼Œå®žçŽ°æ›´åŠ è‡ªç„¶çš„äººæœºäº¤äº’ï¼›åœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œå¯ä»¥ç”¨äºŽç›‘æµ‹è€å¹´äººçš„æ´»åŠ¨çŠ¶æ€ï¼Œæä¾›ä¸»åŠ¨è¾…åŠ©ç”Ÿæ´»æœåŠ¡ï¼›åœ¨åŒ»ç–—å¥åº·ä¸­ï¼Œå¯ä»¥ç”¨äºŽè¯„ä¼°æ‚£è€…çš„åº·å¤æƒ…å†µï¼Œæä¾›ä¸ªæ€§åŒ–çš„åº·å¤æ–¹æ¡ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.

