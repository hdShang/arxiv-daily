---
layout: default
title: 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer
---

# 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer

**arXiv**: [2512.05060v1](https://arxiv.org/abs/2512.05060) | [PDF](https://arxiv.org/pdf/2512.05060.pdf)

**ä½œè€…**: Xianfeng Wu, Yajing Bai, Minghan Li, Xianzu Wu, Xueqi Zhao, Zhongyuan Lai, Wenyu Liu, Xinggang Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04

**å¤‡æ³¨**: Code: https://github.com/hustvl/4DLangVGGT, Webpage: https://hustvl.github.io/4DLangVGGT

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hustvl/4DLangVGGT)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º4DLangVGGTï¼Œç”¨äºŽé«˜æ•ˆä¸”å¯æ³›åŒ–çš„4Dè¯­è¨€-è§†è§‰å‡ ä½•å¯¹é½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `4Dåœºæ™¯ç†è§£` `è¯­è¨€å¯¹é½` `è§†è§‰å‡ ä½•` `Transformer` `åŠ¨æ€åœºæ™¯` `ç¥žç»è¾å°„åœº` `å¼€æ”¾è¯æ±‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰4Dè¯­ä¹‰åœºæž„å»ºæ–¹æ³•ä¾èµ–é€åœºæ™¯ä¼˜åŒ–ï¼Œæ³›åŒ–æ€§å·®ï¼Œéš¾ä»¥æ‰©å±•åˆ°çœŸå®žåœºæ™¯ã€‚
2. æå‡º4DLangVGGTï¼Œé€šè¿‡Transformerè”åˆå­¦ä¹ å‡ ä½•æ„ŸçŸ¥å’Œè¯­è¨€å¯¹é½ï¼Œæ— éœ€é€åœºæ™¯ä¼˜åŒ–ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œ4DLangVGGTåœ¨HyperNeRFå’ŒNeu3Dæ•°æ®é›†ä¸Šå‡è¾¾åˆ°SOTAæ€§èƒ½ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æž„å»º4Dè¯­è¨€åœºå¯¹äºŽå…·èº«æ™ºèƒ½ã€å¢žå¼º/è™šæ‹ŸçŽ°å®žå’Œ4Dåœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œå®ƒæä¾›äº†åŠ¨æ€çŽ¯å¢ƒçš„ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶æ”¯æŒå¤æ‚åœºæ™¯ä¸­çš„å¼€æ”¾è¯æ±‡æŸ¥è¯¢ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰çš„4Dè¯­ä¹‰åœºæž„å»ºæ–¹æ³•ä¸»è¦ä¾èµ–äºŽç‰¹å®šåœºæ™¯çš„é«˜æ–¯æº…å°„ï¼Œè¿™éœ€è¦é€åœºæ™¯ä¼˜åŒ–ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œå¹¶ä¸”éš¾ä»¥æ‰©å±•åˆ°å®žé™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†4DLangVGGTï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºŽTransformerçš„å‰é¦ˆç»Ÿä¸€æ¡†æž¶ï¼Œç”¨äºŽ4Dè¯­è¨€å¯¹é½ï¼Œå®ƒåœ¨å•ä¸ªæž¶æž„ä¸­è”åˆé›†æˆäº†å‡ ä½•æ„ŸçŸ¥å’Œè¯­è¨€å¯¹é½ã€‚4DLangVGGTæœ‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š4Dè§†è§‰å‡ ä½•Transformerï¼ŒStreamVGGTï¼Œå®ƒæ•èŽ·åŠ¨æ€åœºæ™¯çš„æ—¶ç©ºå‡ ä½•è¡¨ç¤ºï¼›ä»¥åŠè¯­ä¹‰æ¡¥æŽ¥è§£ç å™¨ï¼ˆSBDï¼‰ï¼Œå®ƒå°†å‡ ä½•æ„ŸçŸ¥ç‰¹å¾æŠ•å½±åˆ°è¯­è¨€å¯¹é½çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œä»Žè€Œå¢žå¼ºè¯­ä¹‰å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¿æŒç»“æž„ä¿çœŸåº¦ã€‚ä¸Žä¾èµ–äºŽæ˜‚è´µçš„é€åœºæ™¯ä¼˜åŒ–æ–¹æ³•ä¸åŒï¼Œ4DLangVGGTå¯ä»¥åœ¨å¤šä¸ªåŠ¨æ€åœºæ™¯ä¸­è”åˆè®­ç»ƒï¼Œå¹¶åœ¨æŽ¨ç†æœŸé—´ç›´æŽ¥åº”ç”¨ï¼Œä»Žè€Œå®žçŽ°éƒ¨ç½²æ•ˆçŽ‡å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—æé«˜äº†å¤§è§„æ¨¡éƒ¨ç½²çš„å®žç”¨æ€§ï¼Œå¹¶ä¸ºå¼€æ”¾è¯æ±‡4Dåœºæ™¯ç†è§£å»ºç«‹äº†ä¸€ç§æ–°èŒƒå¼ã€‚åœ¨HyperNeRFå’ŒNeu3Dæ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…èƒ½æœ‰æ•ˆåœ°æ³›åŒ–ï¼Œè€Œä¸”è¿˜èƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨é€åœºæ™¯è®­ç»ƒä¸‹å®žçŽ°äº†é«˜è¾¾2%çš„å¢žç›Šï¼Œåœ¨å¤šåœºæ™¯è®­ç»ƒä¸‹å®žçŽ°äº†1%çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨https://github.com/hustvl/4DLangVGGTå‘å¸ƒã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰4Dåœºæ™¯ç†è§£æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºŽç¥žç»è¾å°„åœºçš„æ–¹æ³•ï¼Œé€šå¸¸éœ€è¦é’ˆå¯¹æ¯ä¸ªç‰¹å®šåœºæ™¯è¿›è¡Œä¼˜åŒ–ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥åº”ç”¨äºŽå¤§è§„æ¨¡åŠ¨æ€åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å¼€æ”¾è¯æ±‡çš„è¯­è¨€æŸ¥è¯¢æ—¶ï¼Œè¯­ä¹‰ç†è§£èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼š4DLangVGGTçš„æ ¸å¿ƒåœ¨äºŽæž„å»ºä¸€ä¸ªå¯æ³›åŒ–çš„ã€ç«¯åˆ°ç«¯çš„4Dè¯­è¨€-è§†è§‰å‡ ä½•å¯¹é½æ¡†æž¶ã€‚é€šè¿‡Transformeræž¶æž„ï¼Œå°†åŠ¨æ€åœºæ™¯çš„å‡ ä½•ä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯è¿›è¡Œè”åˆç¼–ç å’Œè§£ç ï¼Œä»Žè€Œå®žçŽ°é«˜æ•ˆçš„è¯­ä¹‰ç†è§£å’Œåœºæ™¯é‡å»ºï¼Œé¿å…äº†é€åœºæ™¯ä¼˜åŒ–å¸¦æ¥çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼š4DLangVGGTä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šStreamVGGTï¼ˆ4Dè§†è§‰å‡ ä½•Transformerï¼‰å’ŒSemantic Bridging Decoder (SBD)ã€‚StreamVGGTè´Ÿè´£æ•èŽ·åŠ¨æ€åœºæ™¯çš„æ—¶ç©ºå‡ ä½•è¡¨ç¤ºï¼Œå°†4Dåœºæ™¯ä¿¡æ¯ç¼–ç æˆå‡ ä½•ç‰¹å¾ã€‚SBDåˆ™å°†è¿™äº›å‡ ä½•ç‰¹å¾æŠ•å½±åˆ°è¯­è¨€å¯¹é½çš„è¯­ä¹‰ç©ºé—´ï¼Œä»Žè€Œå®žçŽ°å‡ ä½•ä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯çš„èžåˆã€‚æ•´ä¸ªæ¡†æž¶é€šè¿‡è”åˆè®­ç»ƒï¼Œå®žçŽ°ç«¯åˆ°ç«¯çš„4Dè¯­è¨€å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼š4DLangVGGTçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶ç»Ÿä¸€çš„Transformeræž¶æž„ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å‡ ä½•ä¿¡æ¯å’Œè¯­è¨€ä¿¡æ¯ï¼Œå¹¶å®žçŽ°è·¨åœºæ™¯çš„æ³›åŒ–ã€‚ä¸Žä»¥å¾€ä¾èµ–äºŽç‰¹å®šåœºæ™¯ä¼˜åŒ–çš„æ–¹æ³•ä¸åŒï¼Œ4DLangVGGTå¯ä»¥åœ¨å¤šä¸ªåœºæ™¯ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä»Žè€Œå­¦ä¹ åˆ°æ›´é€šç”¨çš„åœºæ™¯è¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒSBDæ¨¡å—çš„è®¾è®¡æœ‰æ•ˆåœ°å°†å‡ ä½•ç‰¹å¾æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´ï¼Œå¢žå¼ºäº†è¯­ä¹‰å¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šStreamVGGTé‡‡ç”¨Transformerç»“æž„ï¼Œè¾“å…¥æ˜¯4Dç‚¹äº‘æ•°æ®ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ æ—¶ç©ºå‡ ä½•ç‰¹å¾ã€‚SBDä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†StreamVGGTè¾“å‡ºçš„å‡ ä½•ç‰¹å¾ä¸Žè¯­è¨€åµŒå…¥è¿›è¡Œå¯¹é½ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬å‡ ä½•é‡å»ºæŸå¤±å’Œè¯­è¨€å¯¹é½æŸå¤±ï¼Œç”¨äºŽä¼˜åŒ–æ•´ä¸ªç½‘ç»œã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

4DLangVGGTåœ¨HyperNeRFå’ŒNeu3Dæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚åœ¨per-sceneè®­ç»ƒä¸‹ï¼Œæ€§èƒ½æå‡é«˜è¾¾2%ï¼›åœ¨multi-sceneè®­ç»ƒä¸‹ï¼Œæ€§èƒ½æå‡é«˜è¾¾1%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼Œ4DLangVGGTå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œé«˜æ•ˆçš„åœºæ™¯ç†è§£èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

4DLangVGGTåœ¨å…·èº«æ™ºèƒ½ã€å¢žå¼º/è™šæ‹ŸçŽ°å®žã€æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºŽæž„å»ºåŠ¨æ€çŽ¯å¢ƒçš„è¯­ä¹‰åœ°å›¾ï¼Œæ”¯æŒæœºå™¨äººè¿›è¡Œå¤æ‚çš„åœºæ™¯ç†è§£å’Œäº¤äº’ï¼Œå¹¶ä¸ºAR/VRåº”ç”¨æä¾›æ›´é€¼çœŸçš„æ²‰æµ¸å¼ä½“éªŒã€‚è¯¥ç ”ç©¶ä¸ºå¼€æ”¾è¯æ±‡4Dåœºæ™¯ç†è§£å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT

