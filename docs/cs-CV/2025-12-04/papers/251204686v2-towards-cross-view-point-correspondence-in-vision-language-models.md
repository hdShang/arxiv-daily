---
layout: default
title: Towards Cross-View Point Correspondence in Vision-Language Models
---

# Towards Cross-View Point Correspondence in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.04686" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.04686v2</a>
  <a href="https://arxiv.org/pdf/2512.04686.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04686v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.04686v2', 'Towards Cross-View Point Correspondence in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yipu Wang, Yuheng Ji, Yuyang Liu, Enshen Zhou, Ziqiang Yang, Yuxuan Tian, Ziheng Qin, Yue Liu, Huajie Tan, Cheng Chi, Zhiyuan Ma, Daniel Dajun Zeng, Xiaolong Zheng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04 (æ›´æ–°: 2025-12-07)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/WangYipu2002/CrossPoint)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCrossPoint-Benchå’ŒCroPondæ¨¡å‹ï¼Œè§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è·¨è§†è§’ç‚¹å¯¹åº”éš¾é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è·¨è§†è§’å¯¹åº”` `è§†è§‰è¯­è¨€æ¨¡å‹` `ç‚¹å¯¹åº”` `åŸºå‡†æµ‹è¯•` `æ•°æ®é›†` `å…·èº«æ™ºèƒ½` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨è§†è§’ç‚¹å¯¹åº”æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾ç¡®ç‚¹çº§å¯¹åº”ä¸Šï¼Œé™åˆ¶äº†å…¶åœ¨å…·èº«æ™ºèƒ½ä¸­çš„åº”ç”¨ã€‚
2. æå‡ºCrossPoint-BenchåŸºå‡†æµ‹è¯•å’ŒCrossPoint-378Kæ•°æ®é›†ï¼Œå¹¶è®¾è®¡CroPondæ¨¡å‹ï¼Œä»¥æå‡æ¨¡å‹åœ¨è·¨è§†è§’ç‚¹å¯¹åº”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCroPondæ¨¡å‹åœ¨CrossPoint-Benchä¸Šè¶…è¶Šäº†Gemini-2.5-Proï¼Œå‡†ç¡®ç‡æå‡äº†39.7%ï¼Œæ˜¾è‘—æé«˜äº†è·¨è§†è§’ç‚¹å¯¹åº”çš„ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è·¨è§†è§’å¯¹åº”æ˜¯ç©ºé—´ç†è§£å’Œå…·èº«æ™ºèƒ½çš„ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è¿™æ–¹é¢ä»æœ‰ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å®ç°ç²¾ç¡®çš„ç‚¹çº§å¯¹åº”æ–¹é¢ï¼Œè¿™å¯¹äºç²¾ç¡®çš„äº¤äº’è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨è§†è§’ç‚¹å¯¹åº”(CVPC)ä»»åŠ¡å’ŒCrossPoint-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„åŸºå‡†ï¼Œå…¶åˆ†å±‚è®¾è®¡çµæ„Ÿæ¥æºäºäººç±»â€œæ„ŸçŸ¥â€ã€â€œæ¨ç†â€å’Œâ€œå¯¹åº”â€çš„è®¤çŸ¥è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹(ä¾‹å¦‚ï¼ŒGemini-2.5-Pro)ä»ç„¶è¿œè¿œè½åäºäººç±»ï¼Œæ€»ä½“å‡†ç¡®ç‡å·®è·è¶…è¿‡54.65%ï¼Œè¿™æš´éœ²äº†ä»ç²—ç²’åº¦åˆ¤æ–­åˆ°ç»†ç²’åº¦åæ ‡é¢„æµ‹çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†CrossPoint-378Kæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«900ä¸ªåœºæ™¯ä¸­378Kä¸ªé—®ç­”å¯¹ï¼Œé‡ç‚¹å…³æ³¨å¯æ“ä½œçš„åŒºåŸŸï¼Œæ›´å¥½åœ°åæ˜ äº†ç°å®ä¸–ç•Œçš„æ“ä½œå’Œäº¤äº’åœºæ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨CrossPoint-378Kæ•°æ®é›†ä¸Šè®­ç»ƒçš„CroPondã€‚æˆ‘ä»¬çš„CroPondåœ¨CrossPoint-Benchä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¶…è¿‡Gemini-2.5-Pro 39.7%ï¼Œè¿™ä¸ºæ¨è¿›æœªæ¥è·¨è§†è§’å¯¹åº”å·¥ä½œå¥ å®šäº†åŸºç¡€ã€‚è¯¥åŸºå‡†ã€æ•°æ®é›†å’Œæ¨¡å‹å·²åœ¨https://github.com/WangYipu2002/CrossPointå…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨è§†è§’åœºæ™¯ä¸‹ï¼Œéš¾ä»¥å»ºç«‹ç²¾ç¡®ç‚¹å¯¹åº”å…³ç³»çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªèƒ½è¿›è¡Œç²—ç²’åº¦çš„åˆ¤æ–­ï¼Œæ— æ³•å‡†ç¡®é¢„æµ‹ç›®æ ‡ç‚¹åœ¨ä¸åŒè§†è§’ä¸‹çš„åæ ‡ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦ç²¾ç»†æ“ä½œçš„å…·èº«æ™ºèƒ½ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•CrossPoint-Benchå’Œä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†CrossPoint-378Kï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹CroPondã€‚é€šè¿‡æ›´ç»†ç²’åº¦çš„æ•°æ®å’Œæ›´æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼Œæå‡æ¨¡å‹åœ¨è·¨è§†è§’ç‚¹å¯¹åº”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) CrossPoint-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è·¨è§†è§’ç‚¹å¯¹åº”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼›2) CrossPoint-378Kæ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡ä¸åŒè§†è§’ä¸‹çš„é—®ç­”å¯¹ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ï¼›3) CroPondæ¨¡å‹ï¼ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œé€šè¿‡åœ¨CrossPoint-378Kæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæå‡è·¨è§†è§’ç‚¹å¯¹åº”çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†CrossPoint-BenchåŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†æµ‹è¯•æ›´å…·æŒ‘æˆ˜æ€§ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹åœ¨è·¨è§†è§’ç‚¹å¯¹åº”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼›2) æ„å»ºäº†CrossPoint-378Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§é‡é«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„è®­ç»ƒæ•ˆæœï¼›3) æå‡ºäº†CroPondæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨CrossPoint-Benchä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šCrossPoint-BenchåŸºå‡†æµ‹è¯•é‡‡ç”¨åˆ†å±‚è®¾è®¡ï¼Œæ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼ŒåŒ…å«â€œæ„ŸçŸ¥â€ã€â€œæ¨ç†â€å’Œâ€œå¯¹åº”â€ä¸‰ä¸ªé˜¶æ®µã€‚CrossPoint-378Kæ•°æ®é›†é‡ç‚¹å…³æ³¨å¯æ“ä½œçš„åŒºåŸŸï¼Œæ›´å¥½åœ°åæ˜ äº†ç°å®ä¸–ç•Œçš„æ“ä½œå’Œäº¤äº’åœºæ™¯ã€‚CroPondæ¨¡å‹çš„å…·ä½“ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CroPondæ¨¡å‹åœ¨CrossPoint-BenchåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå‡†ç¡®ç‡è¶…è¿‡äº†Gemini-2.5-Pro 39.7%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ„å»ºæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•å’Œæ›´å¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œé’ˆå¯¹æ€§çš„æ¨¡å‹è®­ç»ƒï¼Œå¯ä»¥æœ‰æ•ˆæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨è§†è§’ç‚¹å¯¹åº”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€ç‰©ä½“æŠ“å–ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨è§†è§’ç‚¹å¯¹åº”æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶è¿›è¡Œæ›´ç²¾ç¡®çš„æ“ä½œã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·çš„æŒ‡ä»¤ï¼Œåœ¨ä¸åŒè§†è§’ä¸‹å‡†ç¡®æ‰¾åˆ°ç›®æ ‡ç‰©ä½“å¹¶è¿›è¡ŒæŠ“å–ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of "perceive", "reason", and "correspond". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.

