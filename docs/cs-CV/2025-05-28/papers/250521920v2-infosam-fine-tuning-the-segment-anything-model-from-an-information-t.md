---
layout: default
title: InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective
---

# InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21920" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21920v2</a>
  <a href="https://arxiv.org/pdf/2505.21920.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21920v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21920v2', 'InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuanhong Zhang, Muyao Yuan, Weizhan Zhang, Tieliang Gong, Wen Wen, Jiangyong Ying, Weijie Shi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28 (æ›´æ–°: 2025-06-03)

**å¤‡æ³¨**: Accepted by ICML 2025 (spotlight)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInfoSAMä»¥æå‡SAMåœ¨ä¸“ä¸šé¢†åŸŸçš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `ä¿¡æ¯è®º` `çŸ¥è¯†è’¸é¦` `é¢†åŸŸä¸å˜å…³ç³»` `è§†è§‰åŸºç¡€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­ç¼–ç çš„é¢†åŸŸä¸å˜å…³ç³»ï¼Œå¯¼è‡´SAMåœ¨ä¸“ä¸šé¢†åŸŸçš„è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºInfoSAMï¼Œé€šè¿‡ä¿¡æ¯è®ºçš„æ–¹æ³•å¢å¼ºSAMçš„å¾®è°ƒèƒ½åŠ›ï¼Œé‡ç‚¹åœ¨äºè’¸é¦å’Œä¿ç•™é¢„è®­ç»ƒçš„åˆ†å‰²çŸ¥è¯†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒInfoSAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†SAMæ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸“ä¸šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Segment Anything Model (SAM) æ˜¯ä¸€ç§è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œè™½ç„¶åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„é›¶-shot èƒ½åŠ›ï¼Œä½†åœ¨ä¸“ä¸šé¢†åŸŸå´é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¿¡æ¯è®ºè§†è§’çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•InfoSAMï¼Œé€šè¿‡è’¸é¦å’Œä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†å‰²çŸ¥è¯†ï¼Œå¢å¼ºSAMçš„å¾®è°ƒèƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†çŸ¥è¯†è½¬ç§»è¿‡ç¨‹å½¢å¼åŒ–ä¸ºä¸¤ä¸ªåŸºäºäº’ä¿¡æ¯çš„æ–°ç›®æ ‡ï¼šä¸€æ˜¯å‹ç¼©ä»é¢„è®­ç»ƒSAMä¸­æå–çš„é¢†åŸŸä¸å˜å…³ç³»ï¼ŒäºŒæ˜¯æœ€å¤§åŒ–æ•™å¸ˆæ¨¡å‹ï¼ˆé¢„è®­ç»ƒSAMï¼‰ä¸å­¦ç”Ÿæ¨¡å‹ï¼ˆå¾®è°ƒæ¨¡å‹ï¼‰ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒInfoSAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆæå‡äº†SAMç³»åˆ—æ¨¡å‹åœ¨å®é™…ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå±•ç°äº†å…¶åœ¨å¤„ç†ä¸“ä¸šåœºæ™¯ä¸­çš„é€‚åº”æ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨SAMé¢„è®­ç»ƒæ¨¡å‹ä¸­çš„é¢†åŸŸä¸å˜å…³ç³»çš„é—®é¢˜ã€‚è¿™å¯¼è‡´SAMåœ¨ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨æ•ˆæœä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šInfoSAMé€šè¿‡ä¿¡æ¯è®ºçš„è§†è§’ï¼Œæå‡ºäº†ä¸¤ä¸ªåŸºäºäº’ä¿¡æ¯çš„ç›®æ ‡ï¼Œä»¥å¢å¼ºçŸ¥è¯†è½¬ç§»è¿‡ç¨‹ï¼Œç¡®ä¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¿ç•™é‡è¦çš„åˆ†å‰²çŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInfoSAMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯é¢†åŸŸä¸å˜å…³ç³»çš„å‹ç¼©ï¼ŒäºŒæ˜¯æ•™å¸ˆæ¨¡å‹ä¸å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„äº’ä¿¡æ¯æœ€å¤§åŒ–ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å—çš„ååŒä½œç”¨ï¼Œæå‡äº†å¾®è°ƒæ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šInfoSAMçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†äº’ä¿¡æ¯çš„æ¦‚å¿µæ¥æŒ‡å¯¼çŸ¥è¯†è½¬ç§»è¿‡ç¨‹ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåè€…å¾€å¾€å¿½è§†äº†é¢†åŸŸä¸å˜å…³ç³»çš„åˆ©ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒInfoSAMé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å®ç°äº’ä¿¡æ¯çš„æœ€å¤§åŒ–ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„ç½‘ç»œç»“æ„æ¥ç¡®ä¿çŸ¥è¯†çš„æœ‰æ•ˆè’¸é¦å’Œä¿ç•™ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInfoSAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šæå‡äº†5%-15%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸“ä¸šåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ã€å·¥ä¸šæ£€æµ‹ç­‰ä¸“ä¸šé¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœªæ¥ï¼ŒInfoSAMæœ‰æœ›æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½åŒ–åº”ç”¨ï¼Œæå‡è‡ªåŠ¨åŒ–æ°´å¹³å’Œå†³ç­–æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Segment Anything Model (SAM), a vision foundation model, exhibits impressive zero-shot capabilities in general tasks but struggles in specialized domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to unleash the potential of SAM in novel scenarios. However, existing PEFT methods for SAM neglect the domain-invariant relations encoded in the pre-trained model. To bridge this gap, we propose InfoSAM, an information-theoretic approach that enhances SAM fine-tuning by distilling and preserving its pre-trained segmentation knowledge. Specifically, we formulate the knowledge transfer process as two novel mutual information-based objectives: (i) to compress the domain-invariant relation extracted from pre-trained SAM, excluding pseudo-invariant information as possible, and (ii) to maximize mutual information between the relational knowledge learned by the teacher (pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM establishes a robust distillation framework for PEFT of SAM. Extensive experiments across diverse benchmarks validate InfoSAM's effectiveness in improving SAM family's performance on real-world tasks, demonstrating its adaptability and superiority in handling specialized scenarios.

