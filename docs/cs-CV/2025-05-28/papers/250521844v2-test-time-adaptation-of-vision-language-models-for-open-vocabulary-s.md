---
layout: default
title: Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation
---

# Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21844" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21844v2</a>
  <a href="https://arxiv.org/pdf/2505.21844.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21844v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21844v2', 'Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-28 (æ›´æ–°: 2025-11-09)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/dosowiechi/MLMP)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šçº§å¤šæç¤ºç†µæœ€å°åŒ–æ–¹æ³•ä»¥è§£å†³å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡` `è¯­ä¹‰åˆ†å‰²` `æµ‹è¯•æ—¶é€‚åº”` `è§†è§‰è¯­è¨€æ¨¡å‹` `ç†µæœ€å°åŒ–` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œå¿½è§†äº†å¯†é›†é¢„æµ‹ä»»åŠ¡å¦‚å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºçš„å¤šçº§å¤šæç¤ºç†µæœ€å°åŒ–æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆä¸­é—´å±‚ç‰¹å¾å’Œå¤šç§æ–‡æœ¬æç¤ºæ¨¡æ¿ï¼Œé’ˆå¯¹åˆ†å‰²ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨87ä¸ªä¸åŒæµ‹è¯•åœºæ™¯ä¸­ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„TTAåˆ†ç±»åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰åœ¨å›¾åƒåˆ†ç±»çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ç­‰å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œè¯¥é—®é¢˜å‡ ä¹è¢«å¿½è§†ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„TTAæ–¹æ³•ï¼Œæ—¨åœ¨åœ¨æµ‹è¯•æ—¶é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ†å‰²ã€‚ä¸å›¾åƒåˆ†ç±»çš„TTAæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„å¤šçº§å¤šæç¤ºï¼ˆMLMPï¼‰ç†µæœ€å°åŒ–æ–¹æ³•æ•´åˆäº†æ¥è‡ªä¸­é—´è§†è§‰ç¼–ç å™¨å±‚çš„ç‰¹å¾ï¼Œå¹¶åœ¨å…¨å±€CLSæ ‡è®°å’Œå±€éƒ¨åƒç´ çº§åˆ«ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºæ¨¡æ¿ã€‚è¯¥æ–¹æ³•å¯ä½œä¸ºä»»ä½•åˆ†å‰²ç½‘ç»œçš„æ’ä»¶ä½¿ç”¨ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ ‡ç­¾ï¼Œä¸”å³ä½¿åœ¨å•ä¸ªæµ‹è¯•æ ·æœ¬ä¸‹ä¹Ÿèƒ½ä¿æŒæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„OVSS TTAåŸºå‡†å¥—ä»¶ï¼ŒåŒ…å«ä¸¥æ ¼çš„è¯„ä¼°åè®®ã€ä¹ä¸ªåˆ†å‰²æ•°æ®é›†ã€15ç§å¸¸è§åˆæˆå¹²æ‰°ä»¥åŠé¢å¤–çš„çœŸå®å’Œæ¸²æŸ“åŸŸè½¬ç§»ï¼Œå…±è®¡87ç§ä¸åŒçš„æµ‹è¯•åœºæ™¯ï¼Œä¸ºæœªæ¥çš„å¼€æ”¾è¯æ±‡åˆ†å‰²TTAç ”ç©¶å»ºç«‹äº†æ ‡å‡†åŒ–çš„æµ‹è¯•å¹³å°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²ä»»åŠ¡ä¸­ç›¸è¾ƒäºç›´æ¥é‡‡ç”¨TTAåˆ†ç±»åŸºçº¿æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­æµ‹è¯•æ—¶é€‚åº”çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ç¼ºä¹æœ‰æ•ˆçš„é€‚åº”ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„å¤šçº§å¤šæç¤ºç†µæœ€å°åŒ–æ–¹æ³•é€šè¿‡ç»“åˆä¸­é—´è§†è§‰ç¼–ç å™¨å±‚çš„ç‰¹å¾å’Œå¤šç§æ–‡æœ¬æç¤ºæ¨¡æ¿ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆï¼Œåˆ©ç”¨è§†è§‰ç¼–ç å™¨æå–ç‰¹å¾ï¼›å…¶æ¬¡ï¼Œåº”ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºæ¨¡æ¿è¿›è¡Œç†µæœ€å°åŒ–ï¼›æœ€åï¼Œé€šè¿‡å…¨å±€å’Œå±€éƒ¨å±‚æ¬¡çš„ç‰¹å¾æ•´åˆå®ç°é€‚åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¤šçº§ç‰¹å¾å’Œå¤šç§æç¤ºæ¨¡æ¿ç»“åˆï¼Œçªç ´äº†ä¼ ç»ŸTTAæ–¹æ³•åœ¨å›¾åƒåˆ†ç±»ä¸­çš„å±€é™æ€§ï¼Œé€‚åº”äºåˆ†å‰²ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¤šç§æ–‡æœ¬æç¤ºæ¨¡æ¿ï¼Œå¹¶è®¾è®¡äº†é’ˆå¯¹åˆ†å‰²ä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼Œç¡®ä¿æ¨¡å‹åœ¨å•ä¸ªæµ‹è¯•æ ·æœ¬ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆå·¥ä½œã€‚å®éªŒä¸­ä½¿ç”¨çš„ç½‘ç»œç»“æ„èƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„åˆ†å‰²ç½‘ç»œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨87ä¸ªä¸åŒçš„æµ‹è¯•åœºæ™¯ä¸­ï¼Œæœ¬æ–‡æå‡ºçš„åˆ†å‰²ä¸“ç”¨æ–¹æ³•ç›¸è¾ƒäºç›´æ¥é‡‡ç”¨çš„TTAåˆ†ç±»åŸºçº¿ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªæä¾›ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåˆ†æå’Œæœºå™¨äººè§†è§‰ç­‰ï¼Œéœ€è¦åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œå®æ—¶è¯­ä¹‰åˆ†å‰²çš„åœºæ™¯ã€‚å…¶å®é™…ä»·å€¼åœ¨äºæå‡æ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸‹çš„é€‚åº”èƒ½åŠ›ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, test-time adaptation has attracted wide interest in the context of vision-language models for image classification. However, to the best of our knowledge, the problem is completely overlooked in dense prediction tasks such as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a novel TTA method tailored to adapting VLMs for segmentation during test time. Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt (MLMP) entropy minimization integrates features from intermediate vision-encoder layers and is performed with different text-prompt templates at both the global CLS token and local pixel-wise levels. Our approach could be used as plug-and-play for any segmentation network, does not require additional training data or labels, and remains effective even with a single test sample. Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which integrates a rigorous evaluation protocol, nine segmentation datasets, 15 common synthetic corruptions, and additional real and rendered domain shifts, \textbf{with a total of 87 distinct test scenarios}, establishing a standardized and comprehensive testbed for future TTA research in open-vocabulary segmentation. Our experiments on this suite demonstrate that our segmentation-tailored method consistently delivers significant gains over direct adoption of TTA classification baselines. Code and data are available at https://github.com/dosowiechi/MLMP.

