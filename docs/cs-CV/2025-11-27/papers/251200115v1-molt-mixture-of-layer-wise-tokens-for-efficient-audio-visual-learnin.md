---
layout: default
title: MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning
---

# MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00115" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00115v1</a>
  <a href="https://arxiv.org/pdf/2512.00115.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00115v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.00115v1', 'MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kyeongha Rho, Hyeongkeun Lee, Jae Won Cho, Joon Son Chung

**åˆ†ç±»**: cs.SD, cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-11-27

**å¤‡æ³¨**: 10 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoLTï¼Œé€šè¿‡æ··åˆå±‚çº§Tokenå®ç°é«˜æ•ˆçš„éŸ³è§†é¢‘å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `éŸ³è§†é¢‘å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `Transformer` `è‡ªé€‚åº”å­¦ä¹ ` `å±‚çº§Token`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³è§†é¢‘å­¦ä¹ æ–¹æ³•åœ¨Transformerçš„æ¯ä¸€å±‚è¿›è¡Œä¸²è¡Œè‡ªé€‚åº”ï¼Œè®¡ç®—é‡å¤§ï¼Œæ•ˆç‡ä½ã€‚
2. MoLTé€šè¿‡å¹¶è¡Œçš„è½»é‡çº§æ–¹æ¡ˆï¼Œä»…ä»Transformerçš„åæœŸå±‚æå–å’Œèåˆå±‚çº§Tokenï¼Œå®ç°é«˜æ•ˆè‡ªé€‚åº”ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMoLTåœ¨éŸ³è§†é¢‘é—®ç­”ã€åˆ†å‰²å’Œäº‹ä»¶å®šä½ç­‰ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒå‚æ•°å’Œå†…å­˜æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ··åˆå±‚çº§Tokenï¼ˆMoLTï¼‰çš„å‚æ•°å’Œå†…å­˜é«˜æ•ˆçš„éŸ³è§†é¢‘å­¦ä¹ è‡ªé€‚åº”æ¡†æ¶ã€‚MoLTçš„æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨å¹¶è¡Œçš„è½»é‡çº§æ–¹æ¡ˆå–ä»£ä¼ ç»ŸTransformerä¸­è®¡ç®—é‡å¤§çš„ä¸²è¡Œè‡ªé€‚åº”ï¼Œè¯¥æ–¹æ¡ˆä»…ä»åæœŸçš„Transformerå±‚æå–å’Œèåˆå±‚çº§Tokenã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤ç§ç±»å‹çš„é€‚é…å™¨ï¼Œå°†æ¨¡æ€ç‰¹å®šä¿¡æ¯å’Œè·¨æ¨¡æ€äº¤äº’æç‚¼æˆç´§å‡‘çš„æ½œåœ¨Tokenã€‚ç„¶åï¼ŒTokenèåˆæ¨¡å—é€šè¿‡è€ƒè™‘å®ƒä»¬çš„ç›¸å¯¹é‡è¦æ€§æ¥åŠ¨æ€åœ°èåˆè¿™äº›å±‚çº§Tokenã€‚ä¸ºäº†é˜²æ­¢æ½œåœ¨Tokençš„å†—ä½™ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´å¯¹æ½œåœ¨Tokenåº”ç”¨æ­£äº¤æ­£åˆ™åŒ–ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒTransformerä¸­è‡ªé€‚åº”ä½ç½®çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬ä»…ä»Transformerçš„åæœŸå±‚æå–æ½œåœ¨Tokenã€‚è¿™ç§ç­–ç•¥æ€§çš„è‡ªé€‚åº”æ–¹æ³•é¿å…äº†æ¥è‡ªæ˜“å¤±çš„æ—©æœŸå±‚ç‰¹å¾çš„è¯¯å·®ä¼ æ’­ï¼Œä»è€Œåœ¨ä¿æŒå‚æ•°å’Œå†…å­˜æ•ˆç‡çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–äº†è‡ªé€‚åº”æ€§èƒ½ã€‚é€šè¿‡å¤§é‡çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†MoLTåœ¨å„ç§éŸ³è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬éŸ³è§†é¢‘é—®ç­”ã€éŸ³è§†é¢‘åˆ†å‰²å’ŒéŸ³è§†é¢‘äº‹ä»¶å®šä½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰éŸ³è§†é¢‘å­¦ä¹ æ–¹æ³•é€šå¸¸åœ¨Transformerçš„æ¯ä¸€å±‚è¿›è¡Œä¸²è¡Œè‡ªé€‚åº”ï¼Œå¯¼è‡´è®¡ç®—é‡å¤§ã€å‚æ•°æ•ˆç‡ä½ï¼Œéš¾ä»¥éƒ¨ç½²åˆ°èµ„æºå—é™çš„è®¾å¤‡ä¸Šã€‚æ­¤å¤–ï¼Œæ—©æœŸå±‚çš„ç‰¹å¾å¯èƒ½ä¸ç¨³å®šï¼Œå®¹æ˜“å¯¼è‡´è¯¯å·®ä¼ æ’­ï¼Œå½±å“æœ€ç»ˆæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoLTçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨ä¸€ç§å¹¶è¡Œçš„ã€è½»é‡çº§çš„è‡ªé€‚åº”æ–¹æ¡ˆï¼Œä»…ä»Transformerçš„åæœŸå±‚æå–å’Œèåˆå±‚çº§Tokenã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥é¿å…å¯¹æ¯ä¸€å±‚éƒ½è¿›è¡Œå¤æ‚çš„è®¡ç®—ï¼ŒåŒæ—¶åˆ©ç”¨åæœŸå±‚æ›´ç¨³å®šçš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoLTçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ¨¡æ€ç‰¹å®šé€‚é…å™¨ï¼šç”¨äºæå–éŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€çš„ç‰¹å®šä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç´§å‡‘çš„æ½œåœ¨Tokenã€‚2) è·¨æ¨¡æ€äº¤äº’é€‚é…å™¨ï¼šç”¨äºæ•æ‰éŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€ä¹‹é—´çš„äº¤äº’ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ½œåœ¨Tokenã€‚3) Tokenèåˆæ¨¡å—ï¼šç”¨äºåŠ¨æ€åœ°èåˆæ¥è‡ªä¸åŒå±‚çš„æ½œåœ¨Tokenï¼Œè¯¥æ¨¡å—ä¼šæ ¹æ®Tokençš„ç›¸å¯¹é‡è¦æ€§è¿›è¡ŒåŠ æƒèåˆã€‚4) æ­£äº¤æ­£åˆ™åŒ–ï¼šç”¨äºé˜²æ­¢æ½œåœ¨Tokençš„å†—ä½™ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoLTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ··åˆå±‚çº§Tokençš„æå–å’Œèåˆæœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„ä¸²è¡Œè‡ªé€‚åº”æ–¹æ³•ä¸åŒï¼ŒMoLTé‡‡ç”¨å¹¶è¡Œçš„è½»é‡çº§æ–¹æ¡ˆï¼Œä»…ä»Transformerçš„åæœŸå±‚æå–Tokenï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚æ­¤å¤–ï¼ŒTokenèåˆæ¨¡å—èƒ½å¤ŸåŠ¨æ€åœ°è°ƒæ•´ä¸åŒå±‚Tokençš„æƒé‡ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨ä¸åŒå±‚çš„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šMoLTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‚é…å™¨çš„ç±»å‹å’Œæ•°é‡ï¼šè®ºæ–‡é‡‡ç”¨äº†ä¸¤ç§ç±»å‹çš„é€‚é…å™¨ï¼Œåˆ†åˆ«ç”¨äºæå–æ¨¡æ€ç‰¹å®šä¿¡æ¯å’Œè·¨æ¨¡æ€äº¤äº’ä¿¡æ¯ã€‚2) Tokenèåˆæ¨¡å—çš„æƒé‡è®¡ç®—æ–¹å¼ï¼šè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶æ¥è®¡ç®—Tokençš„æƒé‡ã€‚3) æ­£äº¤æ­£åˆ™åŒ–çš„å¼ºåº¦ï¼šè®ºæ–‡é€šè¿‡å®éªŒç¡®å®šäº†æ­£äº¤æ­£åˆ™åŒ–çš„æœ€ä½³å¼ºåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoLTåœ¨éŸ³è§†é¢‘é—®ç­”ã€éŸ³è§†é¢‘åˆ†å‰²å’ŒéŸ³è§†é¢‘äº‹ä»¶å®šä½ç­‰ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨Audio-Visual Question Answeringä»»åŠ¡ä¸Šï¼ŒMoLTçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å‚æ•°é‡æ›´å°‘ã€‚è¿™äº›ç»“æœè¯æ˜äº†MoLTçš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoLTå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è§†é¢‘ä¼šè®®ä¸­çš„è¯­éŸ³å¢å¼ºã€æ™ºèƒ½ç›‘æ§ä¸­çš„äº‹ä»¶æ£€æµ‹ã€ä»¥åŠè‡ªåŠ¨é©¾é©¶ä¸­çš„ç¯å¢ƒæ„ŸçŸ¥ã€‚è¯¥æ–¹æ³•å¯ä»¥éƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œå®ç°é«˜æ•ˆçš„éŸ³è§†é¢‘å¤„ç†ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå•†ä¸šæ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we propose Mixture of Layer-Wise Tokens (MoLT), a parameter- and memory-efficient adaptation framework for audio-visual learning. The key idea of MoLT is to replace conventional, computationally heavy sequential adaptation at every transformer layer with a parallel, lightweight scheme that extracts and fuses layer-wise tokens only from the late layers. We adopt two types of adapters to distill modality-specific information and cross-modal interaction into compact latent tokens in a layer-wise manner. A token fusion module then dynamically fuses these layer-wise tokens by taking into account their relative significance. To prevent the redundancy of latent tokens, we apply an orthogonality regularization between latent tokens during training. Through the systematic analysis of the position of adaptation in the pre-trained transformers, we extract latent tokens only from the late layers of the transformers. This strategic adaptation approach avoids error propagation from the volatile early-layer features, thereby maximizing the adaptation performance while maintaining parameter and memory efficiency. Through extensive experiments, we demonstrate that MoLT outperforms existing methods on diverse audio-visual benchmarks, including Audio-Visual Question Answering, Audio-Visual Segmentation, and Audio-Visual Event Localization.

