---
layout: default
title: Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization
---

# Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.22586" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.22586v1</a>
  <a href="https://arxiv.org/pdf/2511.22586.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.22586v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.22586v1', 'Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Du, Kun Zhou, Yingqian Min, Yue Ling, Wayne Xin Zhao, Youbin Wu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è§†è§‰æ¨ç†æ³›åŒ–ä¸­ï¼Œç®€æ´çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ä¼˜äºå†—é•¿çš„CoTã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰æ¨ç†` `æ€ç»´é“¾` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ³›åŒ–èƒ½åŠ›` `è¿·å®«æ±‚è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ä¾èµ–å†—é•¿çš„æ€ç»´é“¾è¿›è¡Œè§†è§‰æ¨ç†ï¼Œä½†å…¶å¿…è¦æ€§å’Œæ³›åŒ–èƒ½åŠ›æœ‰å¾…è€ƒå¯Ÿã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡å¯¹æ¯”ä¸åŒCoTæ ¼å¼ï¼ˆè¯­è¨€ã€æ¥åœ°ã€è§†è§‰ï¼‰åœ¨è¿·å®«æ±‚è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¢ç©¶CoTè®¾è®¡å¯¹æ³›åŒ–èƒ½åŠ›çš„å½±å“ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç®€æ´çš„ã€ä»…åŒ…å«å¿…è¦æ¥åœ°æ­¥éª¤çš„CoTï¼Œåœ¨ä¸åŒè¿·å®«å°ºå¯¸ä¸Šè¡¨ç°å‡ºæœ€ä½³çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†ä¸åŒçš„æ€ç»´é“¾ï¼ˆCoTï¼‰è®¾è®¡å¦‚ä½•å½±å“è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­å¯æ³›åŒ–çš„è§†è§‰æ¨ç†èƒ½åŠ›çš„è·å–ã€‚å°½ç®¡CoTæ•°æ®ï¼Œç‰¹åˆ«æ˜¯é•¿CoTæˆ–è§†è§‰CoTï¼ˆå¦‚â€œthink with imageâ€ï¼‰å·²è¢«å¹¿æ³›ç”¨äºç›‘ç£ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œä½†å°šä¸æ¸…æ¥šä¸ºä»€ä¹ˆç‰¹å®šçš„CoTè®¾è®¡æœ‰æ•ˆï¼Œä»¥åŠå“ªäº›è®¾è®¡çœŸæ­£æ”¯æŒå¯æ³›åŒ–çš„æ¨ç†ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¸€ä¸ªå—æ§çš„è¿·å®«æ±‚è§£åŸºå‡†ï¼Œå…¶ä¸­æ¨ç†è§„åˆ™å®Œå…¨æ˜¯è§†è§‰çš„ï¼Œéš¾åº¦å¯ä»¥é€šè¿‡ç½‘æ ¼å¤§å°è¿›è¡Œè°ƒæ•´ï¼Œå¹¶ä¸”æ‰€æœ‰ä¸­é—´æ­¥éª¤éƒ½å¯ä»¥è‡ªåŠ¨ç”Ÿæˆã€‚ä½¿ç”¨Qwen2.5-VL-7Båœ¨æ ‡å‡†çš„SFT-then-RLæµç¨‹ä¸‹ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§å…·æœ‰ä»£è¡¨æ€§çš„CoTæ ¼å¼ï¼šè¯­è¨€CoTã€å¸¦æœ‰ç©ºé—´åæ ‡è½¨è¿¹çš„æ¥åœ°CoTå’Œå¸¦æœ‰å›¾åƒæ“ä½œçš„è§†è§‰CoTã€‚å®éªŒè¡¨æ˜ï¼Œè§†è§‰å’Œè¾ƒé•¿çš„CoTä¸»è¦åŠ é€Ÿæ”¶æ•›ï¼Œä½†ä¸ä¼šæé«˜æœ€ç»ˆæ€§èƒ½ä¸Šé™ï¼›åŒ…å«åŸºæœ¬æ¥åœ°æ­¥éª¤çš„ç®€æ´CoTä¼˜äºè¾ƒé•¿çš„è½¨è¿¹ï¼›å¹¶ä¸”ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»…ä¿ç•™æœ€å°æ¥åœ°ç»“æœçš„CoTåœ¨ä¸åŒçš„è¿·å®«å°ºå¯¸ä¸Šæ³›åŒ–æ•ˆæœæœ€å¥½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨å…¶ä»–ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸ŠéªŒè¯äº†è¿™äº›è§è§£ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ä¸€ç§â€œçŸ­å³æ˜¯é•¿â€çš„æ•ˆåº”ï¼Œå¹¶ä¸ºæ„å»ºæ›´å…·æ³›åŒ–æ€§çš„è§†è§‰æ¨ç†SFTæ•°æ®é›†æä¾›äº†å®è·µæŒ‡å¯¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ï¼Œé€šå¸¸é‡‡ç”¨å†—é•¿çš„æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥æœŸè·å¾—æ›´å¥½çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›å†—é•¿çš„CoTæ˜¯å¦çœŸæ­£å¿…è¦ï¼Œä»¥åŠå“ªç§CoTè®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒæ³›åŒ–èƒ½åŠ›ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ä¸åŒCoTè®¾è®¡è¿›è¡Œç³»ç»Ÿæ€§çš„è¯„ä¼°ï¼Œéš¾ä»¥æŒ‡å¯¼æ›´æœ‰æ•ˆçš„è§†è§‰æ¨ç†æ¨¡å‹è®­ç»ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¯¹æ¯”ä¸åŒç±»å‹çš„CoTæ•°æ®åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¥æ­ç¤ºCoTè®¾è®¡å¯¹æ³›åŒ–èƒ½åŠ›çš„å½±å“ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªå—æ§çš„è¿·å®«æ±‚è§£ç¯å¢ƒï¼Œå¹¶æ¯”è¾ƒäº†è¯­è¨€CoTã€æ¥åœ°CoTï¼ˆåŒ…å«ç©ºé—´åæ ‡è½¨è¿¹ï¼‰å’Œè§†è§‰CoTï¼ˆåŒ…å«å›¾åƒæ“ä½œï¼‰ä¸‰ç§CoTæ ¼å¼ã€‚é€šè¿‡åˆ†æä¸åŒCoTæ ¼å¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä»è€Œæ‰¾åˆ°æ›´æœ‰æ•ˆçš„CoTè®¾è®¡æ–¹æ¡ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨æ ‡å‡†çš„SFT-then-RLï¼ˆSupervised Fine-Tuning followed by Reinforcement Learningï¼‰æµç¨‹ã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸åŒç±»å‹çš„CoTæ•°æ®å¯¹Qwen2.5-VL-7Bæ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ç„¶åï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿·å®«æ±‚è§£ç¯å¢ƒä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®è§†è§‰è¾“å…¥ï¼Œé€æ­¥æ¨ç†å‡ºåˆ°è¾¾ç›®æ ‡ä½ç½®çš„è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå‘ç°â€œçŸ­å³æ˜¯é•¿â€çš„æ•ˆåº”ï¼Œå³ç®€æ´çš„ã€ä»…åŒ…å«å¿…è¦æ¥åœ°æ­¥éª¤çš„CoTï¼Œåœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸ä»¥å¾€è®¤ä¸ºæ›´é•¿ã€æ›´å¤æ‚çš„CoTèƒ½å¤Ÿæå‡æ¨ç†èƒ½åŠ›çš„è§‚ç‚¹ä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è®¾è®¡äº†ä¸€ä¸ªå¯æ§çš„è¿·å®«æ±‚è§£ç¯å¢ƒï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆä¸åŒéš¾åº¦çš„è¿·å®«å’Œå¯¹åº”çš„CoTæ•°æ®ï¼›2) æ¯”è¾ƒäº†ä¸‰ç§å…·æœ‰ä»£è¡¨æ€§çš„CoTæ ¼å¼ï¼šè¯­è¨€CoTã€æ¥åœ°CoTå’Œè§†è§‰CoTï¼›3) ä½¿ç”¨Qwen2.5-VL-7Bæ¨¡å‹å’Œæ ‡å‡†çš„SFT-then-RLæµç¨‹è¿›è¡Œå®éªŒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¿·å®«æ±‚è§£ä»»åŠ¡ä¸­ï¼Œç®€æ´çš„æ¥åœ°CoTä¼˜äºå†—é•¿çš„è§†è§‰CoTå’Œè¯­è¨€CoTã€‚å…·ä½“æ¥è¯´ï¼Œä»…ä¿ç•™æœ€å°æ¥åœ°ç»“æœçš„CoTåœ¨ä¸åŒè¿·å®«å°ºå¯¸ä¸Šè¡¨ç°å‡ºæœ€ä½³çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†ä»¥å¾€è®¤ä¸ºæ›´é•¿CoTæ›´æœ‰æ•ˆçš„è§‚ç‚¹ï¼Œå¹¶ä¸ºæ„å»ºæ›´å…·æ³›åŒ–æ€§çš„è§†è§‰æ¨ç†SFTæ•°æ®é›†æä¾›äº†å®è·µæŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€å›¾åƒç†è§£ã€è§†è§‰é—®ç­”ç­‰ã€‚é€šè¿‡é‡‡ç”¨æ›´ç®€æ´æœ‰æ•ˆçš„CoTæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥é™ä½æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´é€šç”¨çš„è§†è§‰æ¨ç†ç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as "think with image", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a "short is long" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.

