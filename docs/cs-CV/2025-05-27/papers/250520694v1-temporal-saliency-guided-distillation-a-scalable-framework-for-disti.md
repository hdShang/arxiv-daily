---
layout: default
title: Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets
---

# Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20694" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20694v1</a>
  <a href="https://arxiv.org/pdf/2505.20694.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20694v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20694v1', 'Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xulin Gu, Xinhao Zhong, Zhixing Wei, Yimin Zhou, Shuoyang Sun, Bin Chen, Hongpeng Wang, Yuan Luo

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶é—´æ˜¾è‘—æ€§å¼•å¯¼è’¸é¦æ¡†æ¶ä»¥è§£å†³è§†é¢‘æ•°æ®é›†å‹ç¼©é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘è’¸é¦` `æ•°æ®é›†å‹ç¼©` `æ—¶é—´æ˜¾è‘—æ€§` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘è’¸é¦æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”éš¾ä»¥ä¿ç•™æ—¶é—´åŠ¨æ€ï¼Œç®€å•æ‰©å±•å›¾åƒæ–¹æ³•å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. æå‡ºä¸€ç§å•çº§è§†é¢‘æ•°æ®é›†è’¸é¦æ¡†æ¶ï¼Œç›´æ¥ä¼˜åŒ–åˆæˆè§†é¢‘ï¼Œåˆ©ç”¨æ—¶é—´æ˜¾è‘—æ€§å¼•å¯¼è¿‡æ»¤æœºåˆ¶ã€‚
3. åœ¨æ ‡å‡†è§†é¢‘åŸºå‡†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæå‡äº†è’¸é¦è§†é¢‘çš„è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®é›†è’¸é¦ï¼ˆDDï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ•°æ®é›†å‹ç¼©èŒƒå¼ï¼Œèƒ½å¤Ÿåˆæˆç´§å‡‘çš„æ›¿ä»£æ•°æ®é›†ï¼Œä»¥è¿‘ä¼¼å¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒæ•ˆç”¨ã€‚å°½ç®¡åœ¨å›¾åƒæ•°æ®é›†è’¸é¦æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†DDæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè§†é¢‘æ•°æ®çš„é«˜ç»´æ€§å’Œæ—¶é—´å¤æ‚æ€§ã€‚ç°æœ‰çš„è§†é¢‘è’¸é¦æ–¹æ³•å¾€å¾€é¢ä¸´è¿‡é«˜çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”éš¾ä»¥ä¿ç•™æ—¶é—´åŠ¨æ€ï¼Œå› ä¸ºç®€å•æ‰©å±•åŸºäºå›¾åƒçš„æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å•çº§è§†é¢‘æ•°æ®é›†è’¸é¦æ¡†æ¶ï¼Œç›´æ¥é’ˆå¯¹é¢„è®­ç»ƒæ¨¡å‹ä¼˜åŒ–åˆæˆè§†é¢‘ã€‚ä¸ºäº†è§£å†³æ—¶é—´å†—ä½™å¹¶å¢å¼ºè¿åŠ¨ä¿ç•™ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ—¶é—´æ˜¾è‘—æ€§å¼•å¯¼çš„è¿‡æ»¤æœºåˆ¶ï¼Œåˆ©ç”¨å¸§é—´å·®å¼‚æ¥æŒ‡å¯¼è’¸é¦è¿‡ç¨‹ï¼Œé¼“åŠ±ä¿ç•™ä¿¡æ¯ä¸°å¯Œçš„æ—¶é—´çº¿ç´¢ï¼ŒåŒæ—¶æŠ‘åˆ¶å¸§çº§å†—ä½™ã€‚å¤§é‡åœ¨æ ‡å‡†è§†é¢‘åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¼¥åˆäº†çœŸå®è§†é¢‘æ•°æ®ä¸è’¸é¦è§†é¢‘æ•°æ®ä¹‹é—´çš„å·®è·ï¼Œä¸ºè§†é¢‘æ•°æ®é›†å‹ç¼©æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘æ•°æ®é›†è’¸é¦ä¸­çš„é«˜ç»´æ€§å’Œæ—¶é—´å¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬å’Œæ—¶é—´åŠ¨æ€ä¿ç•™æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¯¼è‡´è’¸é¦æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¡†æ¶é€šè¿‡ç›´æ¥ä¼˜åŒ–åˆæˆè§†é¢‘æ¥æå‡è’¸é¦æ•ˆæœï¼Œå¹¶å¼•å…¥æ—¶é—´æ˜¾è‘—æ€§å¼•å¯¼çš„è¿‡æ»¤æœºåˆ¶ï¼Œä»¥ä¿ç•™é‡è¦çš„æ—¶é—´ä¿¡æ¯ï¼ŒæŠ‘åˆ¶å†—ä½™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥ã€æ—¶é—´æ˜¾è‘—æ€§è®¡ç®—ã€åˆæˆè§†é¢‘ç”Ÿæˆå’Œä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆè®¡ç®—å¸§é—´å·®å¼‚ï¼Œç„¶åæ ¹æ®æ˜¾è‘—æ€§æŒ‡å¯¼åˆæˆè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥æ—¶é—´æ˜¾è‘—æ€§å¼•å¯¼çš„è¿‡æ»¤æœºåˆ¶æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿ç•™è§†é¢‘ä¸­çš„é‡è¦åŠ¨æ€ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ—¶é—´ä¿¡æ¯ä¿ç•™ä¸å†—ä½™æŠ‘åˆ¶ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿åˆæˆè§†é¢‘çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†è§†é¢‘åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œè’¸é¦è§†é¢‘çš„è´¨é‡æå‡æ˜¾è‘—ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæ˜ç¡®æä¾›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘åˆ†æç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©è§†é¢‘æ•°æ®é›†ï¼Œé™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„è®­ç»ƒæ•ˆæœã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›æ¨åŠ¨è§†é¢‘ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dataset distillation (DD) has emerged as a powerful paradigm for dataset compression, enabling the synthesis of compact surrogate datasets that approximate the training utility of large-scale ones. While significant progress has been achieved in distilling image datasets, extending DD to the video domain remains challenging due to the high dimensionality and temporal complexity inherent in video data. Existing video distillation (VD) methods often suffer from excessive computational costs and struggle to preserve temporal dynamics, as naÃ¯ve extensions of image-based approaches typically lead to degraded performance. In this paper, we propose a novel uni-level video dataset distillation framework that directly optimizes synthetic videos with respect to a pre-trained model. To address temporal redundancy and enhance motion preservation, we introduce a temporal saliency-guided filtering mechanism that leverages inter-frame differences to guide the distillation process, encouraging the retention of informative temporal cues while suppressing frame-level redundancy. Extensive experiments on standard video benchmarks demonstrate that our method achieves state-of-the-art performance, bridging the gap between real and distilled video data and offering a scalable solution for video dataset compression.

