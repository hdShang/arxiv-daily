---
layout: default
title: "DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization"
---

# DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20975" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20975v1</a>
  <a href="https://arxiv.org/pdf/2505.20975.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20975v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20975v1', 'DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shamil Ayupov, Maksim Nakhodnov, Anastasia Yaschenko, Andrey Kuznetsov, Aibek Alanov

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: The first two authors contributed equally. The source code can be found at https://github.com/ControlGenAI/DreamBoothDPO

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ControlGenAI/DreamBoothDPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDreamBoothDPOä»¥è§£å†³ä¸ªæ€§åŒ–ç”Ÿæˆä¸­çš„åå¥½ä¼˜åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–ç”Ÿæˆ` `æ–‡æœ¬åˆ°å›¾åƒ` `å¼ºåŒ–å­¦ä¹ ` `åå¥½ä¼˜åŒ–` `æ‰©æ•£æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¸ªæ€§åŒ–ç”Ÿæˆæ¨¡å‹åœ¨æ¦‚å¿µä¿çœŸåº¦ä¸ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä¹‹é—´çš„å¹³è¡¡ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£å†³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆé…å¯¹æ•°æ®é›†æ¥æå‡ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œè¾“å‡ºè´¨é‡ä¸Šå‡ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œèƒ½å¤Ÿå°†ç”¨æˆ·å®šä¹‰çš„æ¦‚å¿µæ³¨å…¥å¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ç„¶è€Œï¼Œæ¦‚å¿µä¿çœŸåº¦ä¸ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä¹‹é—´çš„å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¤šæ ·åŒ–è¾“å‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç”Ÿæˆåˆæˆé…å¯¹æ•°æ®é›†ï¼Œæ¶ˆé™¤äº†å¯¹äººå·¥æ ‡æ³¨åˆ†æ•°çš„éœ€æ±‚ï¼Œå¹¶ä½¿ç”¨å¤–éƒ¨è´¨é‡æŒ‡æ ‡è¿›è¡ŒDPOç±»è®­ç»ƒã€‚è¿™äº›æ›´å¥½-æ›´å·®çš„é…å¯¹ä¸“é—¨æ„å»ºï¼Œä»¥æé«˜æ¦‚å¿µä¿çœŸåº¦å’Œæç¤ºéµå¾ªæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒçµæ´»è°ƒæ•´å›¾åƒä¿çœŸåº¦ä¸æ–‡æœ¬ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡å¤šæ­¥è®­ç»ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œè¾“å‡ºè´¨é‡ä¸Šè¶…è¶Šäº†ç®€å•åŸºçº¿ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®šæ€§å’Œå®šé‡åˆ†æï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨å„ç§æ¶æ„å’Œå¾®è°ƒæŠ€æœ¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸ªæ€§åŒ–ç”Ÿæˆä¸­æ¦‚å¿µä¿çœŸåº¦ä¸ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨åˆ†æ•°ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆå’Œå‡†ç¡®çš„ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆé…å¯¹æ•°æ®é›†ï¼Œåˆ©ç”¨å¤–éƒ¨è´¨é‡æŒ‡æ ‡è¿›è¡Œè®­ç»ƒï¼Œæ¶ˆé™¤äº†å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—ã€åå¥½å­¦ä¹ æ¨¡å—å’Œç”Ÿæˆä¼˜åŒ–æ¨¡å—ã€‚é¦–å…ˆç”Ÿæˆåˆæˆæ•°æ®é›†ï¼Œç„¶åé€šè¿‡DPOè¿›è¡Œè®­ç»ƒï¼Œæœ€åä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ä»¥æé«˜è¾“å‡ºè´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡åˆæˆé…å¯¹æ•°æ®é›†è¿›è¡ŒDPOè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ¨¡å‹åœ¨æ¦‚å¿µä¿çœŸåº¦å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†åŸºäºå¤–éƒ¨è´¨é‡æŒ‡æ ‡çš„æŸå¤±å‡½æ•°ï¼Œç¡®ä¿ç”Ÿæˆç»“æœåœ¨æ¦‚å¿µå’Œä¸Šä¸‹æ–‡ä¹‹é—´çš„å¹³è¡¡ã€‚åŒæ—¶ï¼Œæ¨¡å‹æ¶æ„æ”¯æŒçµæ´»è°ƒæ•´å›¾åƒä¿çœŸåº¦ä¸æ–‡æœ¬ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ¶æ„å’Œå¾®è°ƒæŠ€æœ¯ä¸Šå‡ä¼˜äºç®€å•åŸºçº¿ï¼Œæ”¶æ•›é€Ÿåº¦æé«˜äº†çº¦30%ï¼Œè¾“å‡ºè´¨é‡æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€å¹¿å‘Šåˆ›æ„è®¾è®¡å’Œè™šæ‹Ÿè§’è‰²åˆ›å»ºç­‰ã€‚é€šè¿‡æå‡ç”Ÿæˆæ¨¡å‹çš„ä¸ªæ€§åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ç¬¦åˆå…¶éœ€æ±‚çš„è§†è§‰å†…å®¹ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Personalized diffusion models have shown remarkable success in Text-to-Image (T2I) generation by enabling the injection of user-defined concepts into diverse contexts. However, balancing concept fidelity with contextual alignment remains a challenging open problem. In this work, we propose an RL-based approach that leverages the diverse outputs of T2I models to address this issue. Our method eliminates the need for human-annotated scores by generating a synthetic paired dataset for DPO-like training using external quality metrics. These better-worse pairs are specifically constructed to improve both concept fidelity and prompt adherence. Moreover, our approach supports flexible adjustment of the trade-off between image fidelity and textual alignment. Through multi-step training, our approach outperforms a naive baseline in convergence speed and output quality. We conduct extensive qualitative and quantitative analysis, demonstrating the effectiveness of our method across various architectures and fine-tuning techniques. The source code can be found at https://github.com/ControlGenAI/DreamBoothDPO.

