---
layout: default
title: ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models
---

# ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21500" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21500v2</a>
  <a href="https://arxiv.org/pdf/2505.21500.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21500v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21500v2', 'ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: Project: https://zju-real.github.io/ViewSpatial-Page/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViewSpatial-Benchä»¥è§£å†³å¤šè§†è§’ç©ºé—´å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `å¤šè§†è§’è¯„ä¼°` `3Dæ³¨é‡Š` `æœºå™¨äººå¯¼èˆª` `å¢å¼ºç°å®` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨è§†è§’ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä»äººç±»è§†è§’è¿›è¡Œæ¨ç†æ—¶è¡¨ç°ä¸ä½³ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šè®ºæ–‡æå‡ºäº†ViewSpatial-BenchåŸºå‡†ï¼Œä¸“æ³¨äºå¤šè§†è§’ç©ºé—´å®šä½è¯„ä¼°ï¼Œå¹¶å¼•å…¥è‡ªåŠ¨åŒ–3Dæ³¨é‡Šç®¡é“ä»¥ç”Ÿæˆæ–¹å‘æ ‡ç­¾ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šé€šè¿‡åœ¨å¤šè§†è§’ç©ºé—´æ•°æ®é›†ä¸Šå¾®è°ƒVLMsï¼Œæ•´ä½“æ€§èƒ½æå‡è¾¾46.24%ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£å’Œæ¨ç†è§†è§‰å†…å®¹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è·¨è§†è§’ç†è§£å’Œç©ºé—´æ¨ç†çš„ä»»åŠ¡ä¸­ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°å½“å‰VLMsä¸»è¦åœ¨è‡ªæˆ‘ä¸­å¿ƒç©ºé—´æ¨ç†ï¼ˆä»ç›¸æœºè§†è§’ï¼‰æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦é‡‡ç”¨å…¶ä»–å®ä½“ç©ºé—´å‚è€ƒæ¡†æ¶æ—¶ï¼Œéš¾ä»¥æ¨å¹¿åˆ°å¤–éƒ¨è§†è§’ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ViewSpatial-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå¤šè§†è§’ç©ºé—´å®šä½è¯†åˆ«è¯„ä¼°è®¾è®¡çš„ç»¼åˆåŸºå‡†ï¼Œæ¶µç›–äº”ç§ä¸åŒä»»åŠ¡ç±»å‹ï¼Œå¹¶æ”¯æŒä¸€ä¸ªè‡ªåŠ¨åŒ–çš„3Dæ³¨é‡Šç®¡é“ä»¥ç”Ÿæˆç²¾ç¡®çš„æ–¹å‘æ ‡ç­¾ã€‚å¯¹å¤šç§VLMsåœ¨ViewSpatial-Benchä¸Šçš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ç›¸æœºè§†è§’ä»»åŠ¡ä¸Šè¡¨ç°åˆç†ï¼Œä½†åœ¨ä»äººç±»è§†è§’æ¨ç†æ—¶å‡†ç¡®æ€§é™ä½ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„å¤šè§†è§’ç©ºé—´æ•°æ®é›†ä¸Šå¾®è°ƒVLMsï¼Œæˆ‘ä»¬åœ¨å„ä»»åŠ¡ä¸Šå®ç°äº†46.24%çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œçªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè§†è§’ç©ºé—´å®šä½ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä»äººç±»è§†è§’è¿›è¡Œæ¨ç†æ—¶çš„å‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è‡ªæˆ‘ä¸­å¿ƒçš„ç©ºé—´æ¨ç†ï¼Œå¯¼è‡´åœ¨éœ€è¦å¤–éƒ¨è§†è§’æ—¶æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºViewSpatial-BenchåŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤šè§†è§’ç©ºé—´å®šä½èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„3Dæ³¨é‡Šç®¡é“ï¼Œç”Ÿæˆç²¾ç¡®çš„æ–¹å‘æ ‡ç­¾ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œæ¨ç†ç©ºé—´å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä¸ªæ¡†æ¶åŒ…æ‹¬æ•°æ®æ”¶é›†ã€3Dæ³¨é‡Šç”Ÿæˆã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é˜¶æ®µæ¶µç›–å¤šç§è§†è§’çš„å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œæ³¨é‡Šç”Ÿæˆé˜¶æ®µåˆ™åˆ©ç”¨è‡ªåŠ¨åŒ–å·¥å…·ç”Ÿæˆæ–¹å‘æ ‡ç­¾ã€‚æ¨¡å‹è®­ç»ƒé˜¶æ®µé€šè¿‡å¾®è°ƒç°æœ‰VLMsï¼Œæœ€ååœ¨è¯„ä¼°é˜¶æ®µå¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œå…¨é¢æµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¤šè§†è§’ç©ºé—´å®šä½çš„åŸºå‡†å’Œç›¸åº”çš„3Dæ³¨é‡Šç®¡é“ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„ç©ºé—´å‚è€ƒæ¡†æ¶ä¸‹è¿›è¡Œæœ‰æ•ˆæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶è°ƒæ•´äº†ç½‘ç»œç»“æ„ä»¥é€‚åº”å¤šè§†è§’æ•°æ®çš„ç‰¹æ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹åœ¨å¤„ç†ä¸åŒè§†è§’æ—¶çš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å¯¹å¤šç§è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨ç›¸æœºè§†è§’ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä»äººç±»è§†è§’æ¨ç†æ—¶å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„å¤šè§†è§’ç©ºé—´æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæ•´ä½“æ€§èƒ½æå‡è¾¾46.24%ï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰åœºæ™¯ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå‡†ç¡®çš„ç©ºé—´å®šä½å’Œç†è§£æ˜¯å®ç°é«˜æ•ˆäº¤äº’å’Œæ“ä½œçš„å…³é”®ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒViewSpatial-Benchå¯èƒ½æˆä¸ºè¯„ä¼°å’Œæå‡å¤šæ¨¡æ€AIç³»ç»Ÿç©ºé—´æ™ºèƒ½çš„é‡è¦å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.

