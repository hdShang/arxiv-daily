---
layout: default
title: GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution
---

# GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21375" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21375v2</a>
  <a href="https://arxiv.org/pdf/2505.21375.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21375v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21375v2', 'GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fengxiang Wang, Mingshuo Chen, Yueying Li, Di Wang, Haotian Wang, Zonghao Guo, Zefan Wang, Boqi Shan, Long Lan, Yulin Wang, Hongzhen Wang, Wenjing Yang, Bo Du, Jing Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-11-04)

**å¤‡æ³¨**: NeurlPS 2025 Spotlight

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGeoLLaVA-8Kä»¥è§£å†³è¶…é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒå¤„ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¶…é«˜åˆ†è¾¨ç‡` `é¥æ„Ÿå›¾åƒ` `å¤šæ¨¡æ€æ¨¡å‹` `æ•°æ®é›†æ„å»º` `tokené€‰æ‹©` `èƒŒæ™¯ä¿®å‰ª` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨å¤„ç†è¶…é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒæ—¶é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œtokençˆ†ç‚¸çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†SuperRS-VQAå’ŒHighRS-VQAæ•°æ®é›†ï¼Œå¹¶é€šè¿‡èƒŒæ™¯tokenä¿®å‰ªå’Œé”šå®štokené€‰æ‹©æ¥å‡å°‘å†…å­˜å ç”¨ã€‚
3. GeoLLaVA-8Kåœ¨XLRS-Benchä¸Šè®¾å®šäº†æ–°çš„æ€§èƒ½åŸºå‡†ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¶…é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒä¸ºåœ°çƒè§‚æµ‹æä¾›äº†å®è´µæ•°æ®ï¼Œä½†ç°æœ‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹é¢ä¸´ä¸¤ä¸ªä¸»è¦ç“¶é¢ˆï¼šä¸€æ˜¯è¶…é«˜åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼ŒäºŒæ˜¯ç”±äºå›¾åƒå°ºå¯¸å¤§å¯¼è‡´çš„tokençˆ†ç‚¸ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†SuperRS-VQAå’ŒHighRS-VQAè¿™ä¸¤ä¸ªæœ€é«˜åˆ†è¾¨ç‡çš„è§†è§‰-è¯­è¨€æ•°æ®é›†ï¼Œæ¶µç›–22ä¸ªçœŸå®ä¸–ç•Œå¯¹è¯ä»»åŠ¡ã€‚ä¸ºç¼“è§£tokençˆ†ç‚¸ï¼Œç ”ç©¶å‘ç°é¥æ„Ÿå›¾åƒä¸­å…³é”®ä¿¡æ¯é›†ä¸­åœ¨å°‘é‡å¯¹è±¡ä¸­å¿ƒtokenä¸­ï¼Œå»é™¤èƒŒæ™¯tokenï¼ˆå¦‚æµ·æ´‹æˆ–æ£®æ—ï¼‰åè€Œèƒ½æé«˜æ€§èƒ½ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†èƒŒæ™¯tokenä¿®å‰ªå’Œé”šå®štokené€‰æ‹©ä¸¤ç§ç­–ç•¥ï¼Œæœ€ç»ˆæ¨å‡ºGeoLLaVA-8Kï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†é«˜è¾¾8KÃ—8Kåˆ†è¾¨ç‡è¾“å…¥çš„é¥æ„Ÿä¸“ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¶…é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒå¤„ç†ä¸­çš„æ•°æ®ç¨€ç¼ºå’Œtokençˆ†ç‚¸é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§å°ºå¯¸å›¾åƒæ—¶ï¼Œå¾€å¾€é¢ä¸´å†…å­˜ä¸è¶³å’Œæ€§èƒ½ä¸‹é™çš„å›°å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥SuperRS-VQAå’ŒHighRS-VQAæ•°æ®é›†ï¼Œæä¾›é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼ŒåŒæ—¶é‡‡ç”¨èƒŒæ™¯tokenä¿®å‰ªå’Œé”šå®štokené€‰æ‹©ç­–ç•¥ï¼Œå‡å°‘å†—ä½™ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGeoLLaVA-8KåŸºäºLLaVAæ¡†æ¶æ„å»ºï¼Œæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€tokené€‰æ‹©ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ç­‰ä¸»è¦æ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µé‡ç‚¹åœ¨äºç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„è§†è§‰-è¯­è¨€å¯¹ï¼Œtokené€‰æ‹©é˜¶æ®µåˆ™é€šè¿‡åˆ†æå›¾åƒå†…å®¹æ¥ä¼˜åŒ–è¾“å…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†èƒŒæ™¯tokenä¿®å‰ªå’Œé”šå®štokené€‰æ‹©ä¸¤ç§ç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è§†è§‰å’Œè¯­è¨€çš„å¯¹é½ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”é«˜åˆ†è¾¨ç‡è¾“å…¥çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GeoLLaVA-8Kåœ¨XLRS-Benchä¸Šè®¾å®šäº†æ–°çš„æ€§èƒ½åŸºå‡†ï¼Œå±•ç¤ºäº†åœ¨å¤„ç†8Kåˆ†è¾¨ç‡è¾“å…¥æ—¶ï¼Œç›¸è¾ƒäºç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¯å¢ƒç›‘æµ‹ã€åŸå¸‚è§„åˆ’ã€å†œä¸šç®¡ç†ç­‰ï¼Œèƒ½å¤Ÿä¸ºé¥æ„Ÿæ•°æ®çš„åˆ†æå’Œå†³ç­–æä¾›æ›´é«˜æ•ˆçš„å·¥å…·ã€‚æœªæ¥ï¼ŒGeoLLaVA-8Kæœ‰æœ›æ¨åŠ¨é¥æ„ŸæŠ€æœ¯åœ¨å„è¡Œå„ä¸šçš„å¹¿æ³›åº”ç”¨ï¼Œæå‡æ•°æ®å¤„ç†çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce SuperRS-VQA (avg. 8,376$\times$8,376) and HighRS-VQA (avg. 2,000$\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: Background Token Pruning and Anchored Token Selection, to reduce the memory footprint while preserving key semantics.Integrating these techniques, we introduce GeoLLaVA-8K, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench.

