---
layout: default
title: Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs
---

# Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20638" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20638v1</a>
  <a href="https://arxiv.org/pdf/2505.20638.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20638v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20638v1', 'Music\'s Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhao You, Xingjian Diao, Chunhui Zhang, Keyi Kong, Weiyi Wu, Zhongyu Ouyang, Chiyu Ma, Tingxuan Wu, Noah Wei, Zong Ke, Ming Cheng, Soroush Vosoughi, Jiang Gui

**åˆ†ç±»**: cs.SD, cs.CV, cs.MM, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xid32/Survey4MusicAVQA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸“é—¨åŒ–æ–¹æ³•ä»¥è§£å†³éŸ³ä¹éŸ³è§†é¢‘é—®ç­”çš„å¤æ‚æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³ä¹éŸ³è§†é¢‘é—®ç­”` `å¤šæ¨¡æ€å­¦ä¹ ` `é¢†åŸŸç‰¹å®šå»ºæ¨¡` `æ—¶ç©ºè®¾è®¡` `éŸ³é¢‘ç‰¹å¾æå–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éŸ³ä¹éŸ³è§†é¢‘é—®ç­”æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å…¶å¤æ‚çš„éŸ³è§†é¢‘å†…å®¹å’Œæ—¶é—´åŠ¨æ€ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸“é—¨åŒ–çš„è¾“å…¥å¤„ç†å’Œæ¶æ„è®¾è®¡ï¼Œå¼ºè°ƒäº†éŸ³ä¹ç‰¹å®šå»ºæ¨¡ç­–ç•¥çš„é‡è¦æ€§ï¼Œä»¥åº”å¯¹Music AVQAçš„ç‹¬ç‰¹éœ€æ±‚ã€‚
3. é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œç ”ç©¶å±•ç¤ºäº†æœ‰æ•ˆè®¾è®¡æ¨¡å¼ä¸å¼ºæ€§èƒ½ä¹‹é—´çš„å…³è”ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å…·ä½“æ–¹å‘å’ŒåŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è¿‘æœŸçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†éŸ³ä¹ç­‰ä¸“ä¸šé¢†åŸŸéœ€è¦é‡èº«å®šåˆ¶çš„æ–¹æ³•ã€‚éŸ³ä¹éŸ³è§†é¢‘é—®ç­”ï¼ˆMusic AVQAï¼‰ç‰¹åˆ«å¼ºè°ƒäº†è¿™ä¸€ç‚¹ï¼Œé¢ä¸´ç€è¿ç»­ã€å¯†é›†çš„éŸ³è§†é¢‘å†…å®¹ã€å¤æ‚çš„æ—¶é—´åŠ¨æ€ä»¥åŠå¯¹é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„è¿«åˆ‡éœ€æ±‚ã€‚é€šè¿‡å¯¹Music AVQAæ•°æ®é›†å’Œæ–¹æ³•çš„ç³»ç»Ÿåˆ†æï¼Œæœ¬æ–‡æŒ‡å‡ºä¸“é—¨çš„è¾“å…¥å¤„ç†ã€åŒ…å«ä¸“ç”¨æ—¶ç©ºè®¾è®¡çš„æ¶æ„ä»¥åŠéŸ³ä¹ç‰¹å®šçš„å»ºæ¨¡ç­–ç•¥å¯¹äºè¯¥é¢†åŸŸçš„æˆåŠŸè‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¼ºè°ƒäº†ä¸å¼ºæ€§èƒ½ç›¸å…³çš„æœ‰æ•ˆè®¾è®¡æ¨¡å¼ï¼Œæå‡ºäº†ç»“åˆéŸ³ä¹å…ˆéªŒçš„å…·ä½“æœªæ¥æ–¹å‘ï¼Œå¹¶æ—¨åœ¨ä¸ºæ¨è¿›å¤šæ¨¡æ€éŸ³ä¹ç†è§£å¥ å®šåšå®åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³ä¹éŸ³è§†é¢‘é—®ç­”ï¼ˆMusic AVQAï¼‰ä¸­çš„å¤æ‚æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿ç»­ã€å¯†é›†çš„éŸ³è§†é¢‘å†…å®¹å’Œæ—¶é—´åŠ¨æ€æ—¶å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨é¢†åŸŸç‰¹å®šçŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸“é—¨åŒ–çš„è¾“å…¥å¤„ç†å’Œæ¶æ„è®¾è®¡ï¼Œç»“åˆéŸ³ä¹ç‰¹å®šçš„å»ºæ¨¡ç­–ç•¥ï¼Œä»¥æé«˜å¯¹éŸ³ä¹å†…å®¹çš„ç†è§£å’Œé—®ç­”èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰éŸ³è§†é¢‘ä¹‹é—´çš„å…³ç³»å’Œæ—¶é—´å˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µé’ˆå¯¹éŸ³ä¹å†…å®¹è¿›è¡Œç‰¹å®šçš„è¾“å…¥å¤„ç†ï¼Œç‰¹å¾æå–æ¨¡å—åˆ™ä½¿ç”¨ä¸“é—¨çš„æ—¶ç©ºè®¾è®¡æ¥æ•æ‰éŸ³è§†é¢‘ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†ä¸“é—¨çš„æ—¶ç©ºè®¾è®¡å’ŒéŸ³ä¹ç‰¹å®šçš„å»ºæ¨¡ç­–ç•¥ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„é€šç”¨æ€§è®¾è®¡å½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†éŸ³ä¹çš„å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é’ˆå¯¹éŸ³ä¹ç‰¹å¾çš„ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼Œä»¥æ›´å¥½åœ°æ•æ‰éŸ³è§†é¢‘çš„æ—¶åºç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ä¸“é—¨åŒ–æ–¹æ³•çš„æ¨¡å‹åœ¨Music AVQAä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå‡†ç¡®ç‡æé«˜äº†15%ï¼Œåœ¨å¤æ‚éŸ³è§†é¢‘å†…å®¹çš„ç†è§£ä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬éŸ³ä¹æ¨èç³»ç»Ÿã€éŸ³ä¹å†…å®¹æ£€ç´¢ä»¥åŠéŸ³ä¹æ•™è‚²ç­‰ã€‚é€šè¿‡æå‡å¯¹éŸ³ä¹éŸ³è§†é¢‘å†…å®¹çš„ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ç²¾å‡†çš„æœåŠ¡å’Œä½“éªŒï¼Œæœªæ¥å¯èƒ½åœ¨å¨±ä¹å’Œæ•™è‚²è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While recent Multimodal Large Language Models exhibit impressive capabilities for general multimodal tasks, specialized domains like music necessitate tailored approaches. Music Audio-Visual Question Answering (Music AVQA) particularly underscores this, presenting unique challenges with its continuous, densely layered audio-visual content, intricate temporal dynamics, and the critical need for domain-specific knowledge. Through a systematic analysis of Music AVQA datasets and methods, this position paper identifies that specialized input processing, architectures incorporating dedicated spatial-temporal designs, and music-specific modeling strategies are critical for success in this domain. Our study provides valuable insights for researchers by highlighting effective design patterns empirically linked to strong performance, proposing concrete future directions for incorporating musical priors, and aiming to establish a robust foundation for advancing multimodal musical understanding. This work is intended to inspire broader attention and further research, supported by a continuously updated anonymous GitHub repository of relevant papers: https://github.com/xid32/Survey4MusicAVQA.

