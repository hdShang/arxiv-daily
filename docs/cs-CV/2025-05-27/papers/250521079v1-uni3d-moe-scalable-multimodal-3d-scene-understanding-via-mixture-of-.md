---
layout: default
title: Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts
---

# Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21079" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21079v1</a>
  <a href="https://arxiv.org/pdf/2505.21079.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21079v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21079v1', 'Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yue Zhang, Yingzhao Jian, Hehe Fan, Yi Yang, Roger Zimmermann

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUni3D-MoEä»¥è§£å†³å¤šæ¨¡æ€3Dåœºæ™¯ç†è§£ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `3Dåœºæ™¯ç†è§£` `ä¸“å®¶æ··åˆ` `æ·±åº¦å­¦ä¹ ` `åŠ¨æ€è·¯ç”±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€3Dåœºæ™¯ç†è§£æ–¹æ³•é€šå¸¸åªåˆ©ç”¨å•ä¸€æ¨¡æ€ï¼Œå¯¼è‡´åœºæ™¯è¡¨ç¤ºä¸å®Œæ•´ï¼Œå½±å“ç†è§£å‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„Uni3D-MoEé€šè¿‡ç¨€ç–ä¸“å®¶æ··åˆæœºåˆ¶ï¼ŒåŠ¨æ€é€‰æ‹©é€‚åˆçš„ä¸“å®¶å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œæå‡äº†3Dåœºæ™¯ç†è§£çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚
3. åœ¨æ ‡å‡†åŸºå‡†å’Œä¸“ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUni3D-MoEåœ¨å¤šæ¨¡æ€èåˆå’Œä»»åŠ¡é€‚åº”æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºåœ¨å…¨é¢3Dåœºæ™¯ç†è§£æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä»…ä½¿ç”¨å•ä¸€æˆ–æœ‰é™çš„3Dæ¨¡æ€ï¼Œå¯¼è‡´3Dåœºæ™¯è¡¨ç¤ºä¸å®Œæ•´ï¼Œè§£é‡Šå‡†ç¡®æ€§é™ä½ã€‚æ­¤å¤–ï¼Œä¸åŒç±»å‹çš„æŸ¥è¯¢æœ¬è´¨ä¸Šä¾èµ–äºä¸åŒçš„æ¨¡æ€ï¼Œç»Ÿä¸€å¤„ç†æ‰€æœ‰æ¨¡æ€ä»¤ç‰Œå¯èƒ½æ— æ³•æœ‰æ•ˆæ•æ‰æŸ¥è¯¢ç‰¹å®šçš„ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Uni3D-MoEï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¨€ç–ä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„3D MLLMï¼Œæ—¨åœ¨å®ç°è‡ªé€‚åº”çš„3Då¤šæ¨¡æ€èåˆã€‚Uni3D-MoEé›†æˆäº†å¤šç§3Dæ¨¡æ€ï¼ŒåŒ…æ‹¬å¤šè§†è§’RGBå’Œæ·±åº¦å›¾åƒã€é¸Ÿç°å›¾ï¼ˆBEVï¼‰åœ°å›¾ã€ç‚¹äº‘å’Œä½“ç´ è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¡†æ¶æ ¸å¿ƒé‡‡ç”¨å¯å­¦ä¹ çš„è·¯ç”±æœºåˆ¶ï¼Œåœ¨ç¨€ç–MoEåŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­åŠ¨æ€é€‰æ‹©é€‚å½“çš„ä¸“å®¶ï¼Œç¡®ä¿æ¯ä¸ªä¸“å®¶æ ¹æ®å­¦ä¹ åˆ°çš„æ¨¡æ€åå¥½å¤„ç†å¤šæ¨¡æ€ä»¤ç‰Œï¼Œä»è€Œä¿ƒè¿›çµæ´»çš„åä½œä»¥æ»¡è¶³å¤šæ ·åŒ–çš„ä»»åŠ¡éœ€æ±‚ã€‚å¯¹æ ‡å‡†3Dåœºæ™¯ç†è§£åŸºå‡†å’Œä¸“ç”¨æ•°æ®é›†çš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†Uni3D-MoEçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€3Dåœºæ™¯ç†è§£æ–¹æ³•ä¸­æ¨¡æ€åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´çš„åœºæ™¯è¡¨ç¤ºä¸å®Œæ•´å’Œç†è§£å‡†ç¡®æ€§é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUni3D-MoEé€šè¿‡ç¨€ç–ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹æ ¹æ®è¾“å…¥ä»¤ç‰Œçš„ç‰¹æ€§åŠ¨æ€é€‰æ‹©åˆé€‚çš„ä¸“å®¶è¿›è¡Œå¤„ç†ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„å¤šæ¨¡æ€èåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯è¾“å…¥çš„å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚RGBå›¾åƒã€æ·±åº¦å›¾ã€BEVåœ°å›¾ç­‰ï¼‰ï¼Œç„¶åé€šè¿‡å¯å­¦ä¹ çš„è·¯ç”±æœºåˆ¶é€‰æ‹©ä¸“å®¶ï¼Œæœ€åå°†å¤„ç†ç»“æœè¿›è¡Œèåˆä»¥è¾“å‡ºæœ€ç»ˆç†è§£ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šUni3D-MoEçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŠ¨æ€é€‰æ‹©ä¸“å®¶çš„èƒ½åŠ›ï¼Œä½¿å¾—æ¯ä¸ªä¸“å®¶èƒ½å¤Ÿä¸“æ³¨äºç‰¹å®šæ¨¡æ€çš„å¤„ç†ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ç»Ÿä¸€å¤„ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®¾è®¡ä¸­é‡‡ç”¨äº†å¯å­¦ä¹ çš„è·¯ç”±æœºåˆ¶ï¼Œç¡®ä¿ä¸“å®¶é€‰æ‹©çš„çµæ´»æ€§ï¼›åŒæ—¶ï¼Œé’ˆå¯¹ä¸åŒæ¨¡æ€çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ ‡å‡†3Dåœºæ™¯ç†è§£åŸºå‡†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUni3D-MoEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€èåˆå’Œä»»åŠ¡é€‚åº”æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œè™šæ‹Ÿç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´å‡†ç¡®çš„ç¯å¢ƒç†è§£å’Œå†³ç­–æ”¯æŒã€‚æœªæ¥ï¼ŒUni3D-MoEæœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€AIç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ï¼Œæå‡å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œæ™ºèƒ½æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in multimodal large language models (MLLMs) have demonstrated considerable potential for comprehensive 3D scene understanding. However, existing approaches typically utilize only one or a limited subset of 3D modalities, resulting in incomplete representations of 3D scenes and reduced interpretive accuracy. Furthermore, different types of queries inherently depend on distinct modalities, indicating that uniform processing of all modality tokens may fail to effectively capture query-specific context. To address these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts (MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion. Specifically, Uni3D-MoE integrates a comprehensive set of 3D modalities, including multi-view RGB and depth images, bird's-eye-view (BEV) maps, point clouds, and voxel representations. At its core, our framework employs a learnable routing mechanism within the sparse MoE-based large language model, dynamically selecting appropriate experts at the token level. Each expert specializes in processing multimodal tokens based on learned modality preferences, thus facilitating flexible collaboration tailored to diverse task-specific requirements. Extensive evaluations on standard 3D scene understanding benchmarks and specialized datasets demonstrate the efficacy of Uni3D-MoE.

