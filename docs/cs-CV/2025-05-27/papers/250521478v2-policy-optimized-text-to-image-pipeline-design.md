---
layout: default
title: Policy Optimized Text-to-Image Pipeline Design
---

# Policy Optimized Text-to-Image Pipeline Design

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21478" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21478v2</a>
  <a href="https://arxiv.org/pdf/2505.21478.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21478v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21478v2', 'Policy Optimized Text-to-Image Pipeline Design')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-11-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç®¡é“è®¾è®¡ä»¥è§£å†³æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±æ¨¡å‹` `å›¾åƒè´¨é‡ä¼˜åŒ–` `å¤šç»„ä»¶ç®¡é“`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•åœ¨è®¾è®¡å¤æ‚ç®¡é“æ—¶é¢ä¸´é«˜è®¡ç®—éœ€æ±‚å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹æ¥é¢„æµ‹å›¾åƒè´¨é‡ï¼Œé¿å…äº†é«˜æˆæœ¬çš„å›¾åƒç”Ÿæˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·åŒ–å·¥ä½œæµå’Œæå‡å›¾åƒè´¨é‡æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå·²ä»å•ä¸€æ¨¡å‹å‘å±•ä¸ºå¤æ‚çš„å¤šç»„ä»¶ç®¡é“ï¼Œè¿™äº›ç®¡é“ç»“åˆäº†å¾®è°ƒç”Ÿæˆå™¨ã€é€‚é…å™¨ã€æ”¾å¤§æ¨¡å—å’Œç¼–è¾‘æ­¥éª¤ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ã€‚ç„¶è€Œï¼Œè®¾è®¡è¿™äº›ç®¡é“éœ€è¦å¤§é‡ä¸“ä¸šçŸ¥è¯†ã€‚è¿‘æœŸç ”ç©¶å°è¯•é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œä½†é¢ä¸´ç”Ÿæˆå›¾åƒæ—¶è®¡ç®—éœ€æ±‚é«˜å’Œæ³›åŒ–èƒ½åŠ›å·®çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œé¦–å…ˆè®­ç»ƒä¸€ç»„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥ä»æç¤º-å·¥ä½œæµç»„åˆä¸­é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ï¼Œä»è€Œæ¶ˆé™¤è®­ç»ƒæœŸé—´ç”Ÿæˆå›¾åƒçš„é«˜æˆæœ¬ã€‚æ¥ç€ï¼Œå®æ–½ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä¼˜åŒ–å·¥ä½œæµç©ºé—´ä¸­çš„é«˜æ€§èƒ½åŒºåŸŸã€‚æœ€åï¼Œç»“åˆæ— åˆ†ç±»å™¨å¼•å¯¼çš„å¢å¼ºæŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æå‡è¾“å‡ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸåˆ›å»ºå¤šæ ·åŒ–çš„æ–°å·¥ä½œæµï¼Œå¹¶åœ¨å›¾åƒè´¨é‡ä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç®¡é“è®¾è®¡ä¸­çš„æ•ˆç‡ä½ä¸‹å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦ç”Ÿæˆå¤§é‡å›¾åƒä»¥ä¼˜åŒ–å·¥ä½œæµï¼Œå¯¼è‡´è®¡ç®—èµ„æºæ¶ˆè€—é«˜ä¸”éš¾ä»¥æ¨å¹¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹ç›´æ¥é¢„æµ‹å›¾åƒè´¨é‡ï¼Œä»è€Œæ¶ˆé™¤è®­ç»ƒæœŸé—´çš„å›¾åƒç”Ÿæˆéœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œå·¥ä½œæµè¯æ±‡çš„åˆæ­¥è®­ç»ƒï¼Œç„¶åé€šè¿‡GRPOä¼˜åŒ–å¼•å¯¼æ¨¡å‹å‘é«˜æ€§èƒ½åŒºåŸŸç§»åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¥–åŠ±æ¨¡å‹æ¥é¢„æµ‹å›¾åƒè´¨é‡ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­é«˜æ˜‚çš„å›¾åƒç”Ÿæˆæˆæœ¬ï¼ŒåŒæ—¶ç»“åˆæ— åˆ†ç±»å™¨å¼•å¯¼æŠ€æœ¯è¿›ä¸€æ­¥æå‡äº†è¾“å‡ºè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿å¥–åŠ±æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œæ¶æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç”Ÿæˆæ–°å·¥ä½œæµçš„å¤šæ ·æ€§å’Œå›¾åƒè´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‰ºæœ¯åˆ›ä½œã€å¹¿å‘Šè®¾è®¡ã€æ¸¸æˆå¼€å‘ç­‰å¤šä¸ªéœ€è¦é«˜è´¨é‡å›¾åƒç”Ÿæˆçš„è¡Œä¸šã€‚é€šè¿‡ä¼˜åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç®¡é“ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ç”Ÿäº§æ•ˆç‡å’Œå›¾åƒè´¨é‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´å¤šåˆ›æ„äº§ä¸šçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines.

