---
layout: default
title: Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation
---

# Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20897" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20897v2</a>
  <a href="https://arxiv.org/pdf/2505.20897.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20897v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20897v2', 'Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pingrui Zhang, Yifei Su, Pengyuan Wu, Dong An, Li Zhang, Zhigang Wang, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-06-22)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/zhangpingrui/Adaptive-Text-Dreamer)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”æ–‡æœ¬æ¢¦å¢ƒç”Ÿæˆå™¨ä»¥è§£å†³è§†è§‰ä¸è¯­è¨€å¯¼èˆªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰ä¸è¯­è¨€å¯¼èˆª` `è‡ªé€‚åº”æ–‡æœ¬ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `é€»è¾‘æ¨ç†` `åœºæ™¯æƒ³è±¡` `æ·±åº¦å­¦ä¹ ` `æ™ºèƒ½å¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰ä¸è¯­è¨€å¯¼èˆªæ–¹æ³•ä¾èµ–äºè§†è§‰åˆæˆï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œå†—ä½™ç»†èŠ‚ï¼Œéš¾ä»¥æœ‰æ•ˆå¯¹é½æ„ŸçŸ¥ä¸è¯­è¨€ã€‚
2. æœ¬æ–‡æå‡ºçš„è‡ªé€‚åº”æ–‡æœ¬æ¢¦å¢ƒç”Ÿæˆå™¨ï¼ˆATDï¼‰é€šè¿‡è¯­è¨€å½¢å¼è‡ªé€‚åº”æƒ³è±¡ç¯å¢ƒè¯­ä¹‰ï¼Œç»“åˆé€»è¾‘æ¨ç†ä¸æƒ³è±¡é¢„æµ‹ã€‚
3. åœ¨R2RåŸºå‡†ä¸Šï¼ŒATDå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”å‚æ•°é‡æ›´å°‘ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰è¦æ±‚æ™ºèƒ½ä½“åœ¨éƒ¨åˆ†å¯è§‚æµ‹çš„ç¯å¢ƒä¸­æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œå¯¼èˆªï¼Œè¿™ä½¿å¾—æ„ŸçŸ¥ä¸è¯­è¨€çš„å¯¹é½å˜å¾—å›°éš¾ã€‚è¿‘æœŸçš„æ–¹æ³•é€šè¿‡æƒ³è±¡æœªæ¥åœºæ™¯æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ä¾èµ–äºåŸºäºè§†è§‰çš„åˆæˆï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ä¸”ç»†èŠ‚å†—ä½™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ–‡æœ¬æ¢¦å¢ƒç”Ÿæˆå™¨ï¼ˆATDï¼‰ï¼Œé€šè¿‡è¯­è¨€å½¢å¼è‡ªé€‚åº”åœ°æƒ³è±¡å…³é”®ç¯å¢ƒè¯­ä¹‰ï¼Œä»è€Œå®ç°æ›´å¯é å’Œé«˜æ•ˆçš„ç­–ç•¥ã€‚ATDé‡‡ç”¨äººç±»å·¦è„‘-å³è„‘æ¶æ„ï¼Œå·¦è„‘ä¸“æ³¨äºé€»è¾‘æ•´åˆï¼Œå³è„‘è´Ÿè´£æœªæ¥åœºæ™¯çš„æƒ³è±¡é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨R2RåŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒATDä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰ä¸è¯­è¨€å¯¼èˆªä¸­æ„ŸçŸ¥ä¸è¯­è¨€å¯¹é½çš„å›°éš¾ï¼Œç°æœ‰æ–¹æ³•åœ¨æƒ³è±¡æœªæ¥åœºæ™¯æ—¶å­˜åœ¨é«˜è®¡ç®—æˆæœ¬å’Œç»†èŠ‚å†—ä½™çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè‡ªé€‚åº”æ–‡æœ¬æ¢¦å¢ƒç”Ÿæˆå™¨ï¼ˆATDï¼‰ï¼Œé€šè¿‡è¯­è¨€å½¢å¼è‡ªé€‚åº”åœ°æƒ³è±¡å…³é”®ç¯å¢ƒè¯­ä¹‰ï¼Œé‡‡ç”¨å·¦è„‘-å³è„‘æ¶æ„ï¼Œå·¦è„‘è´Ÿè´£é€»è¾‘æ•´åˆï¼Œå³è„‘è¿›è¡Œæœªæ¥åœºæ™¯çš„æƒ³è±¡é¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šATDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦åˆ†æ”¯ï¼šå·¦è„‘åˆ†æ”¯ä¸“æ³¨äºé€»è¾‘æ¨ç†ï¼Œå³è„‘åˆ†æ”¯è¿›è¡Œåœºæ™¯æƒ³è±¡ã€‚é€šè¿‡å¯¹Q-formerè¿›è¡Œå¾®è°ƒï¼Œæ¿€æ´»é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œå®ç°é€»è¾‘æ¨ç†å’Œæƒ³è±¡çš„åŠ¨æ€æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šATDçš„åˆ›æ–°åœ¨äºå…¶åŒåˆ†æ”¯è‡ªæˆ‘å¼•å¯¼çš„æƒ³è±¡ç­–ç•¥ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸å¯¼èˆªæ¨¡å‹çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå½¢æˆäº†æœ‰æ•ˆçš„äº¤äº’æœºåˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒATDé€šè¿‡å¾®è°ƒQ-formeræ¥å®ç°é«˜æ•ˆçš„çŸ¥è¯†æ¿€æ´»ï¼Œå¹¶å¼•å…¥äº¤äº’æœºåˆ¶ä»¥è§„èŒƒåŒ–æƒ³è±¡è¾“å‡ºï¼Œç¡®ä¿å…¶ä¸å¯¼èˆªä¸“å®¶æ¨¡å—çš„æœ‰æ•ˆç»“åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨R2RåŸºå‡†ä¸Šï¼ŒATDå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‚æ•°é‡æ˜¾è‘—ä½äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨è§†è§‰ä¸è¯­è¨€å¯¼èˆªä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ï¼Œå…·ä½“æ€§èƒ½æ•°æ®æœªæä¾›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººå¯¼èˆªã€è™šæ‹ŸåŠ©æ‰‹ä»¥åŠå¢å¼ºç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªèƒ½åŠ›ã€‚æœªæ¥ï¼ŒATDçš„è®¾è®¡ç†å¿µå¯æ‰©å±•è‡³å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.

