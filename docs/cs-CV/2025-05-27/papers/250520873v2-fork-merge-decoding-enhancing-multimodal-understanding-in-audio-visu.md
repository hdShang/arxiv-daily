---
layout: default
title: "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models"
---

# Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20873" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20873v2</a>
  <a href="https://arxiv.org/pdf/2505.20873.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20873v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20873v2', 'Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chaeyoung Jung, Youngjoon Jang, Jongmin Choi, Joon Son Chung

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFork-Mergeè§£ç ä»¥è§£å†³éŸ³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ¨¡æ€åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³è§†é¢‘ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `è§£ç ç­–ç•¥` `æ¨¡æ€åå·®` `æ¨ç†ä¼˜åŒ–` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„éŸ³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨è§£ç è¿‡ç¨‹ä¸­é€šå¸¸ä¼šå¼•å…¥æ¨¡æ€åå·®ï¼Œå¯¼è‡´æ¨¡å‹å¯¹æŸä¸€æ¨¡æ€çš„è¿‡åº¦ä¾èµ–ã€‚
2. è®ºæ–‡æå‡ºçš„Fork-Mergeè§£ç ï¼ˆFMDï¼‰ç­–ç•¥é€šè¿‡åœ¨æ¨ç†æ—¶åˆ†å‰å¤„ç†éŸ³é¢‘å’Œè§†é¢‘è¾“å…¥ï¼Œéšåå†åˆå¹¶ç»“æœï¼Œæ¥å¹³è¡¡æ¨¡æ€è´¡çŒ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFMDåœ¨éŸ³é¢‘ã€è§†é¢‘å’ŒéŸ³è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡è§£å†³æ¨¡æ€åå·®æ¥å¢å¼ºéŸ³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆAV-LLMsï¼‰ä¸­çš„å¹³è¡¡å¤šæ¨¡æ€ç†è§£ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚ç›®å‰çš„AV-LLMsé€šå¸¸åœ¨è§£ç å™¨ä¸­è”åˆå¤„ç†éŸ³é¢‘å’Œè§†é¢‘ç‰¹å¾ï¼Œè¿™ç§ç­–ç•¥è™½ç„¶ä¿ƒè¿›äº†ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£ï¼Œä½†å¯èƒ½å¯¼è‡´æ¨¡æ€åå·®ï¼Œä½¿æ¨¡å‹è¿‡åº¦ä¾èµ–æŸä¸€æ¨¡æ€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Fork-Mergeè§£ç ï¼ˆFMDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ¨ç†æ—¶ç­–ç•¥ï¼Œè¦æ±‚ä¸è¿›è¡Œé¢å¤–è®­ç»ƒæˆ–æ¶æ„ä¿®æ”¹ã€‚FMDé¦–å…ˆé€šè¿‡æ—©æœŸè§£ç å™¨å±‚å¤„ç†éŸ³é¢‘å’Œè§†é¢‘è¾“å…¥ï¼ˆåˆ†å‰ï¼‰ï¼Œç„¶ååœ¨å‰©ä½™å±‚ä¸­åˆå¹¶ç»“æœçš„éšè—çŠ¶æ€ä»¥è¿›è¡Œè”åˆæ¨ç†ã€‚è¿™ç§åˆ†ç¦»ä½¿æ¯ç§æ¨¡æ€åœ¨æ—©æœŸé˜¶æ®µå¾—åˆ°å¼ºè°ƒï¼ŒåŒæ—¶åœ¨æ•´åˆè¿‡ç¨‹ä¸­é¼“åŠ±å¹³è¡¡è´¡çŒ®ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä»£è¡¨æ€§çš„AV-LLMsä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºåœ¨éŸ³é¢‘ã€è§†é¢‘å’ŒéŸ³è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¸€è‡´çš„æå‡ï¼Œçªæ˜¾äº†æ¨ç†æ—¶å¹²é¢„åœ¨ç¨³å¥é«˜æ•ˆçš„å¤šæ¨¡æ€ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶è§£å†³çš„æ˜¯éŸ³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨è§£ç è¿‡ç¨‹ä¸­å‡ºç°çš„æ¨¡æ€åå·®é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†éŸ³é¢‘å’Œè§†é¢‘ç‰¹å¾è”åˆå¤„ç†ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹å¯¹æŸä¸€æ¨¡æ€çš„è¿‡åº¦ä¾èµ–ï¼Œä»è€Œå½±å“å¤šæ¨¡æ€ç†è§£çš„å¹³è¡¡æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡Fork-Mergeè§£ç ï¼ˆFMDï¼‰ç­–ç•¥ï¼Œåœ¨æ¨ç†é˜¶æ®µåˆ†å¼€å¤„ç†éŸ³é¢‘å’Œè§†é¢‘è¾“å…¥ï¼Œåˆ†åˆ«è¿›è¡Œæ¨¡æ€ç‰¹å®šçš„æ¨ç†ï¼Œç„¶åå†åˆå¹¶ç»“æœã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¼ºè°ƒæ¯ç§æ¨¡æ€çš„è´¡çŒ®ï¼ŒåŒæ—¶åœ¨æ•´åˆæ—¶ä¿æŒå¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFMDçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯â€œåˆ†å‰â€ï¼Œåœ¨æ—©æœŸè§£ç å™¨å±‚ä¸­åˆ†åˆ«å¤„ç†éŸ³é¢‘å’Œè§†é¢‘è¾“å…¥ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯â€œåˆå¹¶â€ï¼Œåœ¨åç»­å±‚ä¸­å°†ä¸¤ç§æ¨¡æ€çš„éšè—çŠ¶æ€è¿›è¡Œæ•´åˆï¼Œä»¥å®ç°è”åˆæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šFMDçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ¨ç†æ—¶çš„å¹²é¢„ç­–ç•¥ï¼Œé¿å…äº†å¯¹æ¨¡å‹è¿›è¡Œé¢å¤–è®­ç»ƒæˆ–æ¶æ„ä¿®æ”¹ã€‚è¿™ç§æ–¹æ³•ä¸ç°æœ‰çš„è”åˆå¤„ç†ç­–ç•¥æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ¨¡æ€åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨FMDä¸­ï¼ŒéŸ³é¢‘å’Œè§†é¢‘è¾“å…¥çš„å¤„ç†é€šè¿‡ä¸åŒçš„è§£ç å™¨å±‚è¿›è¡Œï¼Œç¡®ä¿åœ¨æ—©æœŸé˜¶æ®µå„è‡ªçš„ç‰¹å¾å¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå¯èƒ½éœ€è¦å‚è€ƒå®Œæ•´è®ºæ–‡ä»¥è·å–æ›´å¤šæŠ€æœ¯ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFMDåœ¨éŸ³é¢‘ã€è§†é¢‘å’ŒéŸ³è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†10%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆã€è§†é¢‘ç†è§£å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡éŸ³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒFMDå¯ä»¥åœ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å­—å¹•ç”Ÿæˆå’Œè§†é¢‘åˆ†æç­‰å®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The goal of this work is to enhance balanced multimodal understanding in audio-visual large language models (AV-LLMs) by addressing modality bias without additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (fork), and then merges the resulting hidden states for joint reasoning in the remaining layers (merge). This separation allows each modality to be emphasized in the early stages while encouraging balanced contributions during integration. We validate our method on three representative AV-LLMs-VideoLLaMA2, video-SALMONN, and Qwen2.5-Omni-using three benchmark datasets. Experimental results show consistent gains in audio, video, and audio-visual reasoning tasks, highlighting the effectiveness of inference-time interventions for robust and efficient multimodal understanding.

