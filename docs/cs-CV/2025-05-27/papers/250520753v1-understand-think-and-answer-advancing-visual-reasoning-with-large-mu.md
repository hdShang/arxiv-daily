---
layout: default
title: "Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models"
---

# Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20753" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20753v1</a>
  <a href="https://arxiv.org/pdf/2505.20753.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20753v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20753v1', 'Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yufei Zhan, Hongyin Zhao, Yousong Zhu, Shurong Zheng, Fan Yang, Ming Tang, Jinqiao Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Tech report

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jefferyZhan/Griffon/tree)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€è§†è§‰æ¨ç†æœºåˆ¶ä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„å¤åˆæ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `è§†è§‰æ¨ç†` `ç»„åˆæ¨ç†` `ç±»äººæ€ç»´` `è‡ªåŠ¨ç†è§£` `æ·±åº¦å­¦ä¹ ` `è§†è§‰ç†è§£` `æ™ºèƒ½é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç»„åˆæ¨ç†ä»»åŠ¡æ—¶ï¼Œå¾€å¾€ç¼ºä¹ä»»åŠ¡ç‰¹å®šçš„é«˜çº§èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶é€šç”¨è§†è§‰æ¨¡å‹çš„å‘å±•ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†è§‰æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡å¼•å…¥ç±»äººç†è§£-æ€è€ƒ-å›ç­”çš„è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­å®Œæˆå¤æ‚æ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGriffon-Råœ¨å¤æ‚è§†è§‰æ¨ç†åŸºå‡†ï¼ˆå¦‚VSRå’ŒCLEVRï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å¤šæ¨¡æ€èƒ½åŠ›ä¸Šæå‡äº†MMBenchå’ŒScienceQAç­‰åŸºå‡†çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚çš„ç»„åˆæ¨ç†ä»»åŠ¡ä¸­ä»æ˜¾ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†è§‰æ¨ç†æœºåˆ¶ï¼Œä½¿LMMsèƒ½å¤Ÿé€šè¿‡å…¶å†…åœ¨èƒ½åŠ›ï¼ˆå¦‚å®šä½å’Œè§†è§‰ç†è§£ï¼‰è§£å†³å¤æ‚çš„ç»„åˆé—®é¢˜ã€‚ä¸ä»¥å¾€çš„å¿«æ·å­¦ä¹ æœºåˆ¶ä¸åŒï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†ç±»äººç†è§£-æ€è€ƒ-å›ç­”çš„è¿‡ç¨‹ï¼Œå…è®¸æ¨¡å‹åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­å®Œæˆæ‰€æœ‰æ­¥éª¤ï¼Œæ— éœ€å¤šæ¬¡æ¨ç†æˆ–å¤–éƒ¨å·¥å…·ã€‚è¿™ä¸€è®¾è®¡å¼¥åˆäº†åŸºç¡€è§†è§‰èƒ½åŠ›ä¸ä¸€èˆ¬é—®ç­”ä¹‹é—´çš„å·®è·ï¼Œé¼“åŠ±LMMsä¸ºå¤æ‚è§†è§‰æ¨ç†ç”Ÿæˆå¯ä¿¡ä¸”å¯è¿½æº¯çš„å“åº”ã€‚æˆ‘ä»¬è¿˜æ•´ç†äº†334Kä¸ªè§†è§‰æŒ‡ä»¤æ ·æœ¬ï¼Œæ¶µç›–ä¸€èˆ¬åœºæ™¯å’Œæ–‡æœ¬ä¸°å¯Œåœºæ™¯ï¼Œå¹¶æ¶‰åŠå¤šç§åŸºç¡€è§†è§‰èƒ½åŠ›ã€‚ç»è¿‡è®­ç»ƒçš„æ¨¡å‹Griffon-Rå…·å¤‡ç«¯åˆ°ç«¯çš„è‡ªåŠ¨ç†è§£ã€è‡ªæˆ‘æ€è€ƒå’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚ç»„åˆæ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤šæ¬¡æ¨ç†æˆ–å¤–éƒ¨å·¥å…·ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œå‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§ç±»äººç†è§£-æ€è€ƒ-å›ç­”çš„è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­å®Œæˆæ‰€æœ‰æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç†è§£æ¨¡å—è´Ÿè´£è§†è§‰ä¿¡æ¯çš„è§£æï¼Œæ€è€ƒæ¨¡å—è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œå›ç­”æ¨¡å—ç”Ÿæˆæœ€ç»ˆçš„å“åº”ã€‚è¿™ä¸€æµç¨‹ç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆæµåŠ¨å’Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†ç±»äººæ€ç»´è¿‡ç¨‹ï¼Œæ¨¡å‹ä¸å†ä¾èµ–äºä¼ ç»Ÿçš„å¿«æ·å­¦ä¹ æœºåˆ¶ï¼Œè€Œæ˜¯é€šè¿‡å†…åœ¨èƒ½åŠ›å®Œæˆå¤æ‚æ¨ç†ï¼Œæå‡äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ç½‘ç»œç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œç¡®ä¿å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Griffon-Råœ¨å¤æ‚è§†è§‰æ¨ç†åŸºå‡†VSRå’ŒCLEVRä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†å‡†ç¡®æ€§ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ã€‚æ­¤å¤–ï¼Œåœ¨å¤šæ¨¡æ€èƒ½åŠ›è¯„ä¼°ä¸­ï¼ŒGriffon-Råœ¨MMBenchå’ŒScienceQAç­‰åŸºå‡†ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶è§†è§‰ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„å†³ç­–æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g. grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability of end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advancing performance on complex visual reasoning benchmarks including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA. Data, models, and codes will be release at https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.

