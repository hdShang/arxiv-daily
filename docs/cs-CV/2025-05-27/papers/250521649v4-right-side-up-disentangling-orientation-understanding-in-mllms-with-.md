---
layout: default
title: Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks
---

# Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21649" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21649v4</a>
  <a href="https://arxiv.org/pdf/2505.21649.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21649v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21649v4', 'Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Keanu Nichols, Nazia Tasnim, Yuting Yan, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, Bryan A. Plummer

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-06-04)

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/appledora)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDORIåŸºå‡†ä»¥è§£å†³å¤šæ¨¡æ€ç³»ç»Ÿçš„ç‰©ä½“æ–¹å‘ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç‰©ä½“æ–¹å‘ç†è§£` `å¤šæ¨¡æ€ç³»ç»Ÿ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `DORIåŸºå‡†` `æœºå™¨äººæ“ä½œ` `å¢å¼ºç°å®` `ç©ºé—´è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€åŸºå‡†æœªèƒ½æœ‰æ•ˆéš”ç¦»ç‰©ä½“æ–¹å‘ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å‡†ç¡®ã€‚
2. è®ºæ–‡æå‡ºDORIåŸºå‡†ï¼Œä¸“æ³¨äºç‰©ä½“æ–¹å‘æ„ŸçŸ¥çš„å››ä¸ªç»´åº¦ï¼Œæä¾›ç³»ç»ŸåŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨æ–¹å‘ç†è§£ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç‰©ä½“æ–¹å‘ç†è§£æ˜¯è§†è§‰æ„ŸçŸ¥ä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ï¼Œå¯¹äºæœºå™¨äººæ“ä½œå’Œå¢å¼ºç°å®ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„è§†è§‰-è¯­è¨€åŸºå‡†æœªèƒ½å•ç‹¬è¯„ä¼°è¿™ä¸€èƒ½åŠ›ï¼Œå¾€å¾€å°†å…¶ä¸ä½ç½®å…³ç³»å’Œä¸€èˆ¬åœºæ™¯ç†è§£æ··æ·†ã€‚æœ¬æ–‡æå‡ºDORIï¼ˆDiscriminative Orientation Reasoning Intelligenceï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç¡®ç«‹äº†ç‰©ä½“æ–¹å‘æ„ŸçŸ¥ä½œä¸ºä¸»è¦è¯„ä¼°ç›®æ ‡ã€‚DORIè¯„ä¼°æ–¹å‘ç†è§£çš„å››ä¸ªç»´åº¦ï¼šæ­£é¢å¯¹é½ã€æ—‹è½¬å˜æ¢ã€ç›¸å¯¹æ–¹å‘å…³ç³»å’Œè§„èŒƒæ–¹å‘ç†è§£ã€‚é€šè¿‡ä»11ä¸ªæ•°æ®é›†ä¸­ç²¾å¿ƒç­–åˆ’çš„ä»»åŠ¡ï¼Œæ¶µç›–67ä¸ªç‰©ä½“ç±»åˆ«ï¼ŒDORIæä¾›äº†å¤šæ¨¡æ€ç³»ç»Ÿå¦‚ä½•ç†è§£ç‰©ä½“æ–¹å‘çš„æ·±å…¥è§è§£ã€‚å¯¹15ä¸ªæœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºå‡ºå…³é”®çš„å±€é™æ€§ï¼Œæœ€ä½³æ¨¡å‹åœ¨ç²—ç•¥ä»»åŠ¡ä¸Šä»…è¾¾åˆ°54.2%çš„å‡†ç¡®ç‡ï¼Œåœ¨ç»†ç²’åº¦æ–¹å‘åˆ¤æ–­ä¸Šä»…ä¸º33.0%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç‰©ä½“æ–¹å‘ç†è§£çš„è¯„ä¼°é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å•ç‹¬è¯„ä¼°è¿™ä¸€èƒ½åŠ›ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDORIåŸºå‡†é€šè¿‡èšç„¦äºç‰©ä½“æ–¹å‘æ„ŸçŸ¥çš„å››ä¸ªç»´åº¦ï¼Œæä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå¸®åŠ©ç ”ç©¶è€…ç†è§£å¤šæ¨¡æ€ç³»ç»Ÿåœ¨æ–¹å‘ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDORIåŸºå‡†åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šæ­£é¢å¯¹é½ã€æ—‹è½¬å˜æ¢ã€ç›¸å¯¹æ–¹å‘å…³ç³»å’Œè§„èŒƒæ–¹å‘ç†è§£ã€‚æ¯ä¸ªæ¨¡å—é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–å¤šä¸ªæ•°æ®é›†å’Œç‰©ä½“ç±»åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šDORIæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹å¤šæ¨¡æ€ç³»ç»Ÿæ–¹å‘æ„è¯†çš„è¯Šæ–­æ¡†æ¶ï¼Œå¼ºè°ƒäº†æ–¹å‘ç†è§£çš„ç‹¬ç«‹æ€§å’Œé‡è¦æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ç»†è‡´çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»»åŠ¡è®¾è®¡ä¸­ï¼ŒDORIä½¿ç”¨äº†æ¥è‡ª11ä¸ªæ•°æ®é›†çš„ä»»åŠ¡ï¼Œæ¶µç›–67ä¸ªç‰©ä½“ç±»åˆ«ï¼Œé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿äº†è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ15ä¸ªæœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç²—ç•¥ä»»åŠ¡ä¸Šä»…è¾¾åˆ°54.2%çš„å‡†ç¡®ç‡ï¼Œè€Œåœ¨ç»†ç²’åº¦æ–¹å‘åˆ¤æ–­ä¸Šä»…ä¸º33.0%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†æ–¹å‘ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å‚è€ƒæ¡†æ¶è½¬æ¢æˆ–å¤åˆæ—‹è½¬çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€3Dåœºæ™¯é‡å»ºå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æ”¹è¿›ç‰©ä½“æ–¹å‘ç†è§£ï¼ŒDORIåŸºå‡†å¯ä»¥å¸®åŠ©æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œå¢å¼ºç°å®åº”ç”¨çš„äº¤äº’ä½“éªŒï¼Œå¹¶æ¨åŠ¨å¤šæ¨¡æ€ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Object orientation understanding represents a fundamental challenge in visual perception critical for applications like robotic manipulation and augmented reality. Current vision-language benchmarks fail to isolate this capability, often conflating it with positional relationships and general scene understanding. We introduce DORI (Discriminative Orientation Reasoning Intelligence), a comprehensive benchmark establishing object orientation perception as a primary evaluation target. DORI assesses four dimensions of orientation comprehension: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation understanding. Through carefully curated tasks from 11 datasets spanning 67 object categories across synthetic and real-world scenarios, DORI provides insights on how multi-modal systems understand object orientations. Our evaluation of 15 state-of-the-art vision-language models reveals critical limitations: even the best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular orientation judgments, with performance deteriorating for tasks requiring reference frame shifts or compound rotations. These findings demonstrate the need for dedicated orientation representation mechanisms, as models show systematic inability to perform precise angular estimations, track orientation changes across viewpoints, and understand compound rotations - suggesting limitations in their internal 3D spatial representations. As the first diagnostic framework specifically designed for orientation awareness in multimodal systems, DORI offers implications for improving robotic control, 3D scene reconstruction, and human-AI interaction in physical environments. DORI data: https://huggingface.co/datasets/appledora/DORI-Benchmark

