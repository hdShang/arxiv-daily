---
layout: default
title: Exploring Timeline Control for Facial Motion Generation
---

# Exploring Timeline Control for Facial Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20861" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20861v1</a>
  <a href="https://arxiv.org/pdf/2505.20861.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20861v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20861v1', 'Exploring Timeline Control for Facial Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifeng Ma, Jinwei Qi, Chaonan Ji, Peng Zhang, Bang Zhang, Zhidong Deng, Liefeng Bo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Accepted by CVPR 2025, Project Page: https://humanaigc.github.io/facial-motion-timeline-control/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶é—´çº¿æ§åˆ¶ä¿¡å·ä»¥æå‡é¢éƒ¨åŠ¨ä½œç”Ÿæˆç²¾åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `é¢éƒ¨åŠ¨ä½œç”Ÿæˆ` `æ—¶é—´çº¿æ§åˆ¶` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ‰©æ•£æ¨¡å‹` `äººæœºäº¤äº’` `è™šæ‹Ÿç°å®` `åŠ¨ç”»åˆ¶ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é¢éƒ¨åŠ¨ä½œç”Ÿæˆä¸­ç¼ºä¹ç²¾ç»†çš„æ—¶é—´æ§åˆ¶ï¼Œå¯¼è‡´ç”Ÿæˆçš„åŠ¨ä½œä¸å¤Ÿè‡ªç„¶å’Œå‡†ç¡®ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ—¶é—´çº¿æ§åˆ¶æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡å¤šè½¨æ—¶é—´çº¿ç²¾ç¡®æŒ‡å®šé¢éƒ¨åŠ¨ä½œçš„æ—¶æœºã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨é¢éƒ¨åŠ¨ä½œé—´éš”æ³¨é‡Šå’Œç”Ÿæˆè‡ªç„¶åŠ¨ä½œæ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®æ€§ä»¤äººæ»¡æ„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é¢éƒ¨åŠ¨ä½œç”Ÿæˆæ§åˆ¶ä¿¡å·ï¼šæ—¶é—´çº¿æ§åˆ¶ã€‚ä¸éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡å·ç›¸æ¯”ï¼Œæ—¶é—´çº¿æä¾›äº†æ›´ç»†ç²’åº¦çš„æ§åˆ¶ï¼Œç”¨æˆ·å¯ä»¥æŒ‡å®šå¤šè½¨æ—¶é—´çº¿ï¼Œç²¾ç¡®æ§åˆ¶æ¯ä¸ªåŠ¨ä½œçš„æ—¶æœºã€‚æˆ‘ä»¬é¦–å…ˆåœ¨è‡ªç„¶é¢éƒ¨åŠ¨ä½œåºåˆ—ä¸­å¯¹é¢éƒ¨åŠ¨ä½œçš„æ—¶é—´é—´éš”è¿›è¡Œé€å¸§æ³¨é‡Šï¼Œåˆ©ç”¨Toeplitzé€†åæ–¹å·®èšç±»æ–¹æ³•å‡å°‘äººå·¥åŠ³åŠ¨ã€‚åŸºäºè¿™äº›æ³¨é‡Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶ä¸”ä¸è¾“å…¥æ—¶é—´çº¿å‡†ç¡®å¯¹é½çš„é¢éƒ¨åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»¥ä»¤äººæ»¡æ„çš„å‡†ç¡®æ€§æ³¨é‡Šé¢éƒ¨åŠ¨ä½œé—´éš”ï¼Œå¹¶ç”Ÿæˆä¸æ—¶é—´çº¿å‡†ç¡®å¯¹é½çš„è‡ªç„¶é¢éƒ¨åŠ¨ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é¢éƒ¨åŠ¨ä½œç”Ÿæˆæ–¹æ³•åœ¨æ—¶é—´æ§åˆ¶ä¸Šçš„ä¸è¶³ï¼Œå¯¼è‡´ç”ŸæˆåŠ¨ä½œçš„è‡ªç„¶æ€§å’Œå‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ—¶é—´çº¿æ§åˆ¶ä¿¡å·ï¼Œç”¨æˆ·å¯ä»¥ç²¾ç¡®æŒ‡å®šé¢éƒ¨åŠ¨ä½œçš„æ—¶é—´å®‰æ’ï¼Œä»è€Œå®ç°æ›´è‡ªç„¶çš„åŠ¨ä½œç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬æ—¶é—´é—´éš”æ³¨é‡Šã€åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹å’Œæ–‡æœ¬å¼•å¯¼çš„æ—¶é—´çº¿ç”Ÿæˆã€‚é¦–å…ˆå¯¹é¢éƒ¨åŠ¨ä½œè¿›è¡Œé€å¸§æ³¨é‡Šï¼Œç„¶ååˆ©ç”¨ç”Ÿæˆæ¨¡å‹ç”Ÿæˆä¸æ—¶é—´çº¿å¯¹é½çš„åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†æ—¶é—´çº¿æ§åˆ¶æœºåˆ¶ï¼Œä½¿å¾—é¢éƒ¨åŠ¨ä½œç”Ÿæˆå¯ä»¥åœ¨æ—¶é—´ä¸Šè¿›è¡Œç²¾ç»†åŒ–æ§åˆ¶ï¼Œè¿™ä¸ä¼ ç»Ÿçš„éŸ³é¢‘æˆ–æ–‡æœ¬ä¿¡å·ç”Ÿæˆæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨Toeplitzé€†åæ–¹å·®èšç±»æ–¹æ³•è¿›è¡Œæ—¶é—´é—´éš”æ³¨é‡Šï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼ŒåŒæ—¶åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹ç¡®ä¿ç”ŸæˆåŠ¨ä½œçš„è‡ªç„¶æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨é¢éƒ¨åŠ¨ä½œé—´éš”æ³¨é‡Šçš„å‡†ç¡®æ€§ä¸Šè¾¾åˆ°äº†ä»¤äººæ»¡æ„çš„æ°´å¹³ï¼Œå¹¶ä¸”ç”Ÿæˆçš„é¢éƒ¨åŠ¨ä½œä¸æ—¶é—´çº¿çš„å¯¹é½åº¦æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•çš„æ˜æ˜¾ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨è™šæ‹Ÿç°å®ã€åŠ¨ç”»åˆ¶ä½œå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›ç²¾ç¡®çš„é¢éƒ¨åŠ¨ä½œç”Ÿæˆï¼Œèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒå’Œäº¤äº’çš„çœŸå®æ„Ÿï¼Œæœªæ¥å¯èƒ½åœ¨ç¤¾äº¤æœºå™¨äººå’Œæ™ºèƒ½åŠ©æ‰‹ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces a new control signal for facial motion generation: timeline control. Compared to audio and text signals, timelines provide more fine-grained control, such as generating specific facial motions with precise timing. Users can specify a multi-track timeline of facial actions arranged in temporal intervals, allowing precise control over the timing of each action. To model the timeline control capability, We first annotate the time intervals of facial actions in natural facial motion sequences at a frame-level granularity. This process is facilitated by Toeplitz Inverse Covariance-based Clustering to minimize human labor. Based on the annotations, we propose a diffusion-based generation model capable of generating facial motions that are natural and accurately aligned with input timelines. Our method supports text-guided motion generation by using ChatGPT to convert text into timelines. Experimental results show that our method can annotate facial action intervals with satisfactory accuracy, and produces natural facial motions accurately aligned with timelines.

