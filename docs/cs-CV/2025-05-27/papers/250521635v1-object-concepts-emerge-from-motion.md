---
layout: default
title: Object Concepts Emerge from Motion
---

# Object Concepts Emerge from Motion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21635" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21635v1</a>
  <a href="https://arxiv.org/pdf/2505.21635.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21635v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21635v1', 'Object Concepts Emerge from Motion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoqian Liang, Xiaohui Wang, Zhichao Li, Ya Yang, Naiyan Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ— ç›‘ç£æ¡†æ¶ä»¥ä»è¿åŠ¨ä¸­å­¦ä¹ ç‰©ä½“æ¦‚å¿µ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— ç›‘ç£å­¦ä¹ ` `ç‰©ä½“è¯†åˆ«` `è¿åŠ¨åˆ†æ` `è§†è§‰ç¼–ç ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç‰©ä½“æ¦‚å¿µå­¦ä¹ ä¸­ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡è§†é¢‘æ•°æ®ä¸Šçš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡è¿åŠ¨è¾¹ç•Œç”Ÿæˆä¼ªå®ä¾‹ç›‘ç£ï¼Œåˆ©ç”¨æ— ç›‘ç£å­¦ä¹ æ¡†æ¶å®ç°ç‰©ä½“ä¸­å¿ƒè§†è§‰è¡¨å¾çš„å­¦ä¹ ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿç›‘ç£å’Œè‡ªç›‘ç£æ–¹æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç‰©ä½“æ¦‚å¿µåœ¨äººçš„è§†è§‰è®¤çŸ¥ä¸­èµ·ç€åŸºç¡€æ€§ä½œç”¨ï¼Œä¿ƒè¿›å¯¹ç‰©ç†ä¸–ç•Œçš„æ„ŸçŸ¥ã€è®°å¿†å’Œäº’åŠ¨ã€‚å—å‘å±•ç¥ç»ç§‘å­¦ç ”ç©¶å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿç‰©å¯å‘çš„æ¡†æ¶ï¼Œé€šè¿‡è§‚å¯Ÿè¿åŠ¨ä»¥æ— ç›‘ç£æ–¹å¼å­¦ä¹ ç‰©ä½“ä¸­å¿ƒçš„è§†è§‰è¡¨å¾ã€‚æˆ‘ä»¬å‘ç°è¿åŠ¨è¾¹ç•Œæ˜¯ç‰©ä½“çº§åˆ†ç»„çš„å¼ºä¿¡å·ï¼Œå¯ä»¥ä»åŸå§‹è§†é¢‘ä¸­æ¨å¯¼å‡ºä¼ªå®ä¾‹ç›‘ç£ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä½¿ç”¨ç°æˆçš„å…‰æµå’Œèšç±»ç®—æ³•ç”ŸæˆåŸºäºè¿åŠ¨çš„å®ä¾‹æ©ç ï¼Œå¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒè§†è§‰ç¼–ç å™¨ã€‚è¯¥æ¡†æ¶å®Œå…¨æ— æ ‡ç­¾ï¼Œä¸ä¾èµ–äºç›¸æœºæ ‡å®šï¼Œé€‚ç”¨äºå¤§è§„æ¨¡éç»“æ„åŒ–è§†é¢‘æ•°æ®ã€‚æˆ‘ä»¬åœ¨å•ç›®æ·±åº¦ä¼°è®¡ã€3Dç‰©ä½“æ£€æµ‹å’Œå ç”¨é¢„æµ‹ç­‰ä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„ç›‘ç£å’Œè‡ªç›‘ç£åŸºçº¿ï¼Œå¹¶åœ¨æœªè§åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç‰©ä½“æ¦‚å¿µå­¦ä¹ ä¸­å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡éç»“æ„åŒ–è§†é¢‘æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è¿åŠ¨è¾¹ç•Œä½œä¸ºç‰©ä½“çº§åˆ†ç»„çš„ä¿¡å·ï¼Œç”Ÿæˆä¼ªå®ä¾‹ç›‘ç£ï¼Œä»è€Œå®ç°æ— ç›‘ç£çš„ç‰©ä½“ä¸­å¿ƒè§†è§‰è¡¨å¾å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬è¿åŠ¨æ©ç ç”Ÿæˆæ¨¡å—å’Œè§†è§‰ç¼–ç å™¨è®­ç»ƒæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å…‰æµå’Œèšç±»ç®—æ³•ç”Ÿæˆè¿åŠ¨æ©ç ï¼Œç„¶ååˆ©ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒè§†è§‰ç¼–ç å™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†è¿åŠ¨è¾¹ç•Œä½œä¸ºä¿¡å·æ¥è¿›è¡Œç‰©ä½“åˆ†ç»„ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–æ ‡ç­¾çš„å­¦ä¹ æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šä½¿ç”¨ç°æˆçš„å…‰æµç®—æ³•è¿›è¡Œè¿åŠ¨æ£€æµ‹ï¼Œèšç±»ç®—æ³•ç”¨äºç”Ÿæˆå®ä¾‹æ©ç ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°ä»¥å¢å¼ºæ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å°†åœ¨ä»£ç å‘å¸ƒæ—¶æä¾›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å•ç›®æ·±åº¦ä¼°è®¡ã€3Dç‰©ä½“æ£€æµ‹å’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„ç›‘ç£å’Œè‡ªç›‘ç£åŸºçº¿ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰å’Œè§†é¢‘ç›‘æ§ç­‰ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„ç‰©ä½“è¯†åˆ«å’Œç†è§£ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ— ç›‘ç£å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œé™ä½å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Object concepts play a foundational role in human visual cognition, enabling perception, memory, and interaction in the physical world. Inspired by findings in developmental neuroscience - where infants are shown to acquire object understanding through observation of motion - we propose a biologically inspired framework for learning object-centric visual representations in an unsupervised manner. Our key insight is that motion boundary serves as a strong signal for object-level grouping, which can be used to derive pseudo instance supervision from raw videos. Concretely, we generate motion-based instance masks using off-the-shelf optical flow and clustering algorithms, and use them to train visual encoders via contrastive learning. Our framework is fully label-free and does not rely on camera calibration, making it scalable to large-scale unstructured video data. We evaluate our approach on three downstream tasks spanning both low-level (monocular depth estimation) and high-level (3D object detection and occupancy prediction) vision. Our models outperform previous supervised and self-supervised baselines and demonstrate strong generalization to unseen scenes. These results suggest that motion-induced object representations offer a compelling alternative to existing vision foundation models, capturing a crucial but overlooked level of abstraction: the visual instance. The corresponding code will be released upon paper acceptance.

