---
layout: default
title: Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration
---

# Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21472" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21472v2</a>
  <a href="https://arxiv.org/pdf/2505.21472.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21472v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21472v2', 'Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mehrdad Fazli, Bowen Wei, Ahmet Sari, Ziwei Zhu

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-08-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAACæ¡†æ¶ä»¥è§£å†³å¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¹»è§‰ç°è±¡` `æ³¨æ„åŠ›æœºåˆ¶` `å¤šæ¨¡æ€å­¦ä¹ ` `ç”Ÿæˆæ¨¡å‹` `ä¿¡å¿ƒæ„ŸçŸ¥` `é•¿æ–‡æœ¬ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®¹æ˜“å‡ºç°å¹»è§‰ç°è±¡ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ä¸è¾“å…¥å›¾åƒä¸ä¸€è‡´ã€‚
2. æœ¬æ–‡æå‡ºçš„ä¿¡å¿ƒæ„ŸçŸ¥æ³¨æ„åŠ›æ ¡å‡†æ¡†æ¶é€šè¿‡è§†è§‰æ ‡è®°æ ¡å‡†å’Œé€‚åº”æ€§æ³¨æ„åŠ›é‡æ–°ç¼©æ”¾ï¼Œè§£å†³äº†æ³¨æ„åŠ›åˆ†é…ä¸å‡çš„é—®é¢˜ã€‚
3. åœ¨CHAIRã€AMBERå’ŒPOPEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAACæ˜¾è‘—æå‡äº†ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å‡ºç°å¹»è§‰ç°è±¡ï¼Œå³è‡ªä¿¡åœ°æè¿°å›¾åƒä¸­ä¸å­˜åœ¨çš„å¯¹è±¡æˆ–å±æ€§ã€‚ç›®å‰çš„æ— è®­ç»ƒå¹²é¢„æªæ–½åœ¨å¼€æ”¾å¼å’Œé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­éš¾ä»¥ä¿æŒå‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¿¡å¿ƒæ„ŸçŸ¥æ³¨æ„åŠ›æ ¡å‡†ï¼ˆCAACï¼‰æ¡†æ¶ï¼Œé’ˆå¯¹ç©ºé—´æ„ŸçŸ¥åå·®å’Œæ¨¡æ€åå·®è¿›è¡Œè°ƒæ•´ã€‚CAACé‡‡ç”¨ä¸¤æ­¥æ³•ï¼šè§†è§‰æ ‡è®°æ ¡å‡†ï¼ˆVTCï¼‰å¹³è¡¡è§†è§‰æ ‡è®°é—´çš„æ³¨æ„åŠ›åˆ†é…ï¼Œé€‚åº”æ€§æ³¨æ„åŠ›é‡æ–°ç¼©æ”¾ï¼ˆAARï¼‰åˆ™æ ¹æ®æ¨¡å‹çš„ä¿¡å¿ƒå¼ºåŒ–è§†è§‰å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAACåœ¨CHAIRã€AMBERå’ŒPOPEåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­æœ‰æ•ˆå‡å°‘äº†å¹»è§‰ç°è±¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰ç°è±¡ï¼Œç°æœ‰æ–¹æ³•åœ¨å¼€æ”¾å¼å’Œé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­éš¾ä»¥ä¿æŒå‡†ç¡®æ€§ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ä¸å›¾åƒä¸ç¬¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¿¡å¿ƒæ„ŸçŸ¥æ³¨æ„åŠ›æ ¡å‡†ï¼ˆCAACï¼‰æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰æ ‡è®°æ ¡å‡†ï¼ˆVTCï¼‰å’Œé€‚åº”æ€§æ³¨æ„åŠ›é‡æ–°ç¼©æ”¾ï¼ˆAARï¼‰æ¥å¹³è¡¡è§†è§‰æ ‡è®°é—´çš„æ³¨æ„åŠ›åˆ†é…ï¼Œå¢å¼ºè§†è§‰å¯¹é½ï¼Œç¡®ä¿ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAACæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œè§†è§‰æ ‡è®°æ ¡å‡†ï¼ˆVTCï¼‰ç”¨äºè°ƒæ•´è§†è§‰æ ‡è®°é—´çš„æ³¨æ„åŠ›åˆ†é…ï¼›å…¶æ¬¡ï¼Œé€‚åº”æ€§æ³¨æ„åŠ›é‡æ–°ç¼©æ”¾ï¼ˆAARï¼‰æ ¹æ®æ¨¡å‹çš„ä¿¡å¿ƒåŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›ï¼Œç¡®ä¿è§†è§‰ä¿¡æ¯çš„ä¼˜å…ˆçº§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCAACçš„åˆ›æ–°åœ¨äºå…¶ä¿¡å¿ƒé©±åŠ¨çš„è°ƒæ•´æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç©ºé—´æ„ŸçŸ¥åå·®å’Œæ¨¡æ€åå·®ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´ä¸ºç²¾ç¡®çš„è§†è§‰å¯¹é½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒCAACé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ³¨æ„åŠ›åˆ†é…ï¼Œå¹¶é€šè¿‡åŠ¨æ€è°ƒæ•´å‚æ•°æ¥é€‚åº”ä¸åŒçš„ç”Ÿæˆåœºæ™¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒé«˜æ•ˆçš„è§†è§‰å¯¹é½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCAACåœ¨CHAIRã€AMBERå’ŒPOPEåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå°¤å…¶åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¹»è§‰ç°è±¡å‡å°‘äº†æ˜¾è‘—ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”å’Œå¤šæ¨¡æ€å†…å®¹åˆ›ä½œç­‰ã€‚é€šè¿‡å‡å°‘å¹»è§‰ç°è±¡ï¼ŒCAACæ¡†æ¶èƒ½å¤Ÿæå‡ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€AIç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models (LVLMs) achieve impressive performance on multimodal tasks but often suffer from hallucination, and confidently describe objects or attributes not present in the image. Current training-free interventions struggle to maintain accuracy in open-ended and long-form generation scenarios. We introduce the Confidence-Aware Attention Calibration (CAAC) framework to address this challenge by targeting two key biases: spatial perception bias, which distributes attention disproportionately across image tokens, and modality bias, which shifts focus from visual to textual inputs over time. CAAC employs a two-step approach: Visual-Token Calibration (VTC) to balance attention across visual tokens, and Adaptive Attention Re-Scaling (AAR) to reinforce visual grounding guided by the model's confidence. This confidence-driven adjustment ensures consistent visual alignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks demonstrate that CAAC outperforms baselines, particularly in long-form generations, effectively reducing hallucination.

