---
layout: default
title: MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios
---

# MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21333" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21333v2</a>
  <a href="https://arxiv.org/pdf/2505.21333.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21333v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21333v2', 'MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yushuo Guan, Zhang Zhang, Liang Wang, Haoxuan Li, Zhouchen Lin, Yuanxing Zhang, Pengfei Wan, Haotian Wang, Wenjing Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-25)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMME-VideoOCRä»¥è§£å†³è§†é¢‘åœºæ™¯ä¸‹OCRæ•ˆæœä¸è¶³çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘OCR` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ—¶ç©ºæ¨ç†` `åŠ¨æ€è§†é¢‘ç†è§£` `åŸºå‡†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘OCRä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åŠ¨æ€è§†é¢‘åœºæ™¯æ—¶ï¼Œé¢ä¸´è¿åŠ¨æ¨¡ç³Šå’Œæ—¶é—´å˜åŒ–ç­‰æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºMME-VideoOCRåŸºå‡†ï¼Œæ¶µç›–å¤šç§è§†é¢‘OCRä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä¸­çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨å•å¸§æ–‡æœ¬è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æ•´ä½“è§†é¢‘ç†è§£çš„ä»»åŠ¡ä¸Šèƒ½åŠ›æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶ç©ºæ¨ç†æ–¹é¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é™æ€å›¾åƒçš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºè¿åŠ¨æ¨¡ç³Šã€æ—¶é—´å˜åŒ–å’Œè§†é¢‘å†…å®¹å›ºæœ‰çš„è§†è§‰æ•ˆæœï¼Œå…¶åœ¨è§†é¢‘OCRä¸­çš„æœ‰æ•ˆæ€§æ˜¾è‘—é™ä½ã€‚ä¸ºæä¾›æ›´æ¸…æ™°çš„è®­ç»ƒæŒ‡å¯¼ï¼Œæœ¬æ–‡å¼•å…¥äº†MME-VideoOCRåŸºå‡†ï¼Œæ¶µç›–äº†å¹¿æ³›çš„è§†é¢‘OCRåº”ç”¨åœºæ™¯ã€‚è¯¥åŸºå‡†åŒ…å«10ä¸ªä»»åŠ¡ç±»åˆ«ã€25ä¸ªç‹¬ç«‹ä»»åŠ¡å’Œ44ç§ä¸åŒåœºæ™¯ï¼Œä»»åŠ¡ä¸ä»…é™äºæ–‡æœ¬è¯†åˆ«ï¼Œè¿˜åŒ…æ‹¬å¯¹è§†é¢‘ä¸­æ–‡æœ¬å†…å®¹çš„æ·±å…¥ç†è§£å’Œæ¨ç†ã€‚åŸºå‡†ç”±1464ä¸ªä¸åŒåˆ†è¾¨ç‡ã€é•¿å®½æ¯”å’Œæ—¶é•¿çš„è§†é¢‘ä»¥åŠ2000ä¸ªç²¾å¿ƒç­–åˆ’çš„æ‰‹åŠ¨æ ‡æ³¨é—®ç­”å¯¹ç»„æˆã€‚å¯¹18ä¸ªæœ€å…ˆè¿›çš„MLLMsè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆGemini-2.5 Proï¼‰å‡†ç¡®ç‡ä¹Ÿä»…ä¸º73.7%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘OCRä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€åœºæ™¯ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è¿åŠ¨æ¨¡ç³Šå’Œæ—¶é—´å˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥MME-VideoOCRåŸºå‡†ï¼Œæä¾›å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œåœºæ™¯ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹åœ¨è§†é¢‘ç†è§£å’Œæ–‡æœ¬æ¨ç†æ–¹é¢çš„èƒ½åŠ›æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMME-VideoOCRåŸºå‡†åŒ…æ‹¬10ä¸ªä»»åŠ¡ç±»åˆ«å’Œ25ä¸ªç‹¬ç«‹ä»»åŠ¡ï¼Œæ¶µç›–äº†ä»æ–‡æœ¬è¯†åˆ«åˆ°æ·±å±‚ç†è§£çš„å¤šä¸ªé˜¶æ®µï¼Œæ•´ä½“æ¶æ„è®¾è®¡æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè§†é¢‘åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼ŒåŒ…å«å¤šç§è§†é¢‘OCRä»»åŠ¡å’Œä¸°å¯Œçš„åœºæ™¯è®¾ç½®ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶åœ¨åŠ¨æ€è§†é¢‘OCRè¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåŸºå‡†åŒ…å«1464ä¸ªè§†é¢‘æ ·æœ¬å’Œ2000ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„é—®ç­”å¯¹ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†åšå®åŸºç¡€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹Gemini-2.5 Proåœ¨MME-VideoOCRåŸºå‡†ä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º73.7%ï¼Œè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ—¶ç©ºæ¨ç†å’Œè·¨å¸§ä¿¡æ¯æ•´åˆçš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨å­—å¹•ç”Ÿæˆã€æ•™è‚²è§†é¢‘åˆ†æç­‰ï¼Œèƒ½å¤Ÿä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„å®é™…åº”ç”¨æä¾›æŒ‡å¯¼å’Œæ”¯æŒã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒMME-VideoOCRåŸºå‡†å°†æ¨åŠ¨è§†é¢‘OCRæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œæå‡å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.

