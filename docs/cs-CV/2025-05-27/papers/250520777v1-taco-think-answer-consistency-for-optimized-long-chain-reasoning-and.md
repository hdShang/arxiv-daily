---
layout: default
title: "TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs"
---

# TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20777" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.20777v1</a>
  <a href="https://arxiv.org/pdf/2505.20777.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20777v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20777v1', 'TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhehan Kan, Yanlin Liu, Kun Yin, Xinghua Jiang, Xin Li, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun, Qingmin Liao, Wenming Yang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TACO‰ª•Ëß£ÂÜ≥ÈïøÈìæÊé®ÁêÜ‰∏≠ÁöÑ‰∏ÄËá¥ÊÄß‰∏éÂ≠¶‰π†ÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÈìæÊé®ÁêÜ` `ËßÜËßâÊé®ÁêÜ` `Âº∫ÂåñÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Êï∞ÊçÆÂ≠¶‰π†ÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠Â≠òÂú®Êé®ÁêÜ‰∏éÁ≠îÊ°à‰∏ç‰∏ÄËá¥„ÄÅÊ®°Âûã‰∏çÁ®≥ÂÆöÂèä‰ΩéÊïàÂ≠¶‰π†Á≠âÈóÆÈ¢ò„ÄÇ
2. TACOÈÄöËøáÂºïÂÖ•ÊÄùËÄÉ-Á≠îÊ°à‰∏ÄËá¥ÊÄßÂíåÂõûÊªöÈáçÈááÊ†∑Á≠ñÁï•ÔºåÁ°Æ‰øùÊé®ÁêÜËøáÁ®ãÁöÑÁ®≥ÂÆöÊÄßÂíåÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄß„ÄÇ
3. Âú®RECÂíåVQA‰ªªÂä°‰∏äÔºåÂæÆË∞ÉLVLMsÂêéÊÄßËÉΩÊòæËëóÊèêÂçáÔºåÈ™åËØÅ‰∫ÜTACOÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

DeepSeek R1Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§çÊùÇÊé®ÁêÜÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅËÆæÁΩÆ‰∏≠Â§çÂà∂R1ÁöÑÊé®ÁêÜËÉΩÂäõÊó∂Èù¢‰∏¥ËØ∏Â§öÊåëÊàòÔºåÂåÖÊã¨Êé®ÁêÜ‰∏éÊúÄÁªàÁ≠îÊ°à‰πãÈó¥ÁöÑ‰∏ç‰∏ÄËá¥ÊÄß„ÄÅÈïøÈìæÊé¢Á¥¢‰∏≠ÁöÑÊ®°Âûã‰∏çÁ®≥ÂÆöÊÄßÂíåÂ¥©Ê∫ÉÔºå‰ª•ÂèäÊï∞ÊçÆÂ≠¶‰π†ÊïàÁéá‰Ωé‰∏ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜTACOÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜËßâÊé®ÁêÜÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï„ÄÇTACOÂºïÂÖ•‰∫ÜÊÄùËÄÉ-Á≠îÊ°à‰∏ÄËá¥ÊÄßÔºåÁ°Æ‰øùÁ≠îÊ°à‰∏éÊ∑±ÊÄùÁÜüËôëÁöÑÊé®ÁêÜÁ¥ßÂØÜÁªìÂêà„ÄÇÊ≠§Â§ñÔºåÈááÁî®ÂõûÊªöÈáçÈááÊ†∑Á≠ñÁï•‰ª•Á®≥ÂÆöÈïøÈìæÊé¢Á¥¢ÔºåÂπ∂ÂºïÂÖ•Ëá™ÈÄÇÂ∫îÂ≠¶‰π†ËÆ°Âàí‰ª•‰ºòÂåñÊï∞ÊçÆÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂæÆË∞ÉLVLMsÂú®RECÂíåVQA‰ªªÂä°‰∏äÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈïøÈìæÊé®ÁêÜ‰∏≠ÁöÑ‰∏ÄËá¥ÊÄßÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Êé®ÁêÜ‰∏éÊúÄÁªàÁ≠îÊ°à‰πãÈó¥Â≠òÂú®ÊòæËëó‰∏ç‰∏ÄËá¥ÔºåÂØºËá¥Ê®°Âûã‰∏çÁ®≥ÂÆöÂíåÂ≠¶‰π†ÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTACOÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂºïÂÖ•ÊÄùËÄÉ-Á≠îÊ°à‰∏ÄËá¥ÊÄßÔºåÁ°Æ‰øùÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÁ≠îÊ°à‰∏éÊÄùËÄÉËøáÁ®ãÁ¥ßÂØÜÁªìÂêàÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÁ®≥ÂÆöÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTACOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊÄùËÄÉ-Á≠îÊ°à‰∏ÄËá¥ÊÄßÊ®°Âùó„ÄÅÂõûÊªöÈáçÈááÊ†∑Á≠ñÁï•Ê®°ÂùóÂíåËá™ÈÄÇÂ∫îÂ≠¶‰π†ËÆ°ÂàíÊ®°Âùó„ÄÇÊÄùËÄÉ-Á≠îÊ°à‰∏ÄËá¥ÊÄßÊ®°ÂùóÁ°Æ‰øùÊé®ÁêÜ‰∏éÁ≠îÊ°àÁöÑ‰∏ÄËá¥ÊÄßÔºåÂõûÊªöÈáçÈááÊ†∑Á≠ñÁï•Ê®°ÂùóÁî®‰∫éÁ®≥ÂÆöÈïøÈìæÊé¢Á¥¢ÔºåËá™ÈÄÇÂ∫îÂ≠¶‰π†ËÆ°ÂàíÊ®°ÂùóÂàô‰ºòÂåñÊï∞ÊçÆÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTACOÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÊÄùËÄÉ-Á≠îÊ°à‰∏ÄËá¥ÊÄßÂíåÂõûÊªöÈáçÈááÊ†∑Á≠ñÁï•ÁöÑÁªìÂêàÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÊõ¥Â•ΩÂú∞Ëß£ÂÜ≥‰∫ÜÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂíåÁ≠îÊ°à‰∏ç‰∏ÄËá¥ÁöÑÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöTACOÈááÁî®Ëá™ÈÄÇÂ∫îÂ≠¶‰π†ËÆ°ÂàíÔºåÈáçÁÇπÂÖ≥Ê≥®‰∏≠Á≠âÈöæÂ∫¶Ê†∑Êú¨ÁöÑÂ≠¶‰π†Ôºå‰ª•ÊèêÈ´òÊï∞ÊçÆÊïàÁéá„ÄÇÊ≠§Â§ñÔºåËÆæËÆ°‰∫ÜÊµãËØïÊó∂ÂàÜËæ®ÁéáÁº©ÊîæÊñπÊ°àÔºå‰ª•Â∫îÂØπÊé®ÁêÜËøáÁ®ã‰∏≠Âõ†ÂàÜËæ®ÁéáÂèòÂåñÂØºËá¥ÁöÑÊÄßËÉΩ‰∏ãÈôçÔºåÂêåÊó∂Âπ≥Ë°°ËÆ°ÁÆóÂºÄÈîÄ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®RECÂíåVQA‰ªªÂä°‰∏äÔºåTACOÈÄöËøáÂæÆË∞ÉLVLMsÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂÖ∑‰ΩìË°®Áé∞‰∏∫Âú®Ê†áÂáÜÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÊú™Áü•ÔºâÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TACOÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ËßÜËßâÊé®ÁêÜ„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÂ§öÊ®°ÊÄÅÂ≠¶‰π†Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÂÖ∂ÂàõÊñ∞ÁöÑÊñπÊ≥ïÂèØ‰ª•ÊèêÂçáÊô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®ÈóÆÁ≠îÁ≥ªÁªüÂíåÂõæÂÉèÁêÜËß£Á≠âÊäÄÊúØÁöÑÊÄßËÉΩÔºåÊé®Âä®Áõ∏ÂÖ≥È¢ÜÂüüÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> DeepSeek R1 has significantly advanced complex reasoning for large language models (LLMs). While recent methods have attempted to replicate R1's reasoning capabilities in multimodal settings, they face limitations, including inconsistencies between reasoning and final answers, model instability and crashes during long-chain exploration, and low data learning efficiency. To address these challenges, we propose TACO, a novel reinforcement learning algorithm for visual reasoning. Building on Generalized Reinforcement Policy Optimization (GRPO), TACO introduces Think-Answer Consistency, which tightly couples reasoning with answer consistency to ensure answers are grounded in thoughtful reasoning. We also introduce the Rollback Resample Strategy, which adaptively removes problematic samples and reintroduces them to the sampler, enabling stable long-chain exploration and future learning opportunities. Additionally, TACO employs an adaptive learning schedule that focuses on moderate difficulty samples to optimize data efficiency. Furthermore, we propose the Test-Time-Resolution-Scaling scheme to address performance degradation due to varying resolutions during reasoning while balancing computational overhead. Extensive experiments on in-distribution and out-of-distribution benchmarks for REC and VQA tasks show that fine-tuning LVLMs leads to significant performance improvements.

