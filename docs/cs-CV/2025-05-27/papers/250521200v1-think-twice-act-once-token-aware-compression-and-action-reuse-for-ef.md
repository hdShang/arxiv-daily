---
layout: default
title: "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models"
---

# Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21200" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21200v1</a>
  <a href="https://arxiv.org/pdf/2505.21200.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21200v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21200v1', 'Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xudong Tan, Yaoxin Yang, Peng Ye, Jialin Zheng, Bizhe Bai, Xinyi Wang, Jia Hao, Tao Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlashVLAä»¥è§£å†³VLAæ¨¡å‹æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `åŠ¨ä½œé‡ç”¨` `æ¨ç†æ•ˆç‡` `è¾¹ç¼˜è®¡ç®—` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶å’Œè¾¹ç¼˜åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚
2. æœ¬æ–‡æå‡ºFlashVLAæ¡†æ¶ï¼Œé€šè¿‡åŠ¨ä½œé‡ç”¨å’Œè§†è§‰tokené€‰æ‹©ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜VLAæ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFlashVLAåœ¨ä¿æŒä»»åŠ¡æˆåŠŸç‡çš„åŒæ—¶ï¼ŒFLOPså‡å°‘55.7%ï¼Œå»¶è¿Ÿé™ä½36.0%ï¼Œå±•ç°äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä½œä¸ºä¸€ç§å¼ºå¤§çš„é€šç”¨æœºå™¨äººæ§åˆ¶èŒƒå¼ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œæ“ä½œã€‚ç„¶è€Œï¼Œç”±äºå¤§è§„æ¨¡çš„tokenè®¡ç®—å’Œè‡ªå›å½’è§£ç ï¼Œæ¨ç†æˆæœ¬é«˜ï¼Œç»™å®æ—¶éƒ¨ç½²å’Œè¾¹ç¼˜åº”ç”¨å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºFlashVLAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”å³æ’å³ç”¨çš„åŠ é€Ÿæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨VLAæ¨¡å‹ä¸­å®ç°åŠ¨ä½œé‡ç”¨ã€‚FlashVLAé€šè¿‡tokenæ„ŸçŸ¥çš„åŠ¨ä½œé‡ç”¨æœºåˆ¶å’Œä¿¡æ¯å¼•å¯¼çš„è§†è§‰tokené€‰æ‹©ç­–ç•¥ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashVLAåœ¨LIBEROåŸºå‡†ä¸Šå°†FLOPså‡å°‘äº†55.7%ï¼Œå»¶è¿Ÿé™ä½äº†36.0%ï¼Œä»»åŠ¡æˆåŠŸç‡ä»…ä¸‹é™0.7%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºæ¶æ„ä¼˜åŒ–ï¼Œæœªèƒ½æœ‰æ•ˆåº”å¯¹è¿ç»­åŠ¨ä½œæ­¥éª¤çš„é«˜ç›¸ä¼¼æ€§å’Œè§†è§‰tokençš„å†—ä½™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFlashVLAé€šè¿‡è¯†åˆ«VLAæ¨¡å‹ä¸­çš„å†—ä½™æ€§ï¼Œæå‡ºäº†ä¸€ç§tokenæ„ŸçŸ¥çš„åŠ¨ä½œé‡ç”¨æœºåˆ¶ï¼Œé¿å…åœ¨ç¨³å®šåŠ¨ä½œæ­¥éª¤ä¸­è¿›è¡Œå†—ä½™è§£ç ï¼ŒåŒæ—¶é‡‡ç”¨ä¿¡æ¯å¼•å¯¼çš„è§†è§‰tokené€‰æ‹©ç­–ç•¥ï¼Œå‰”é™¤ä½è´¡çŒ®çš„tokenï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFlashVLAæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŠ¨ä½œé‡ç”¨æ¨¡å—å’Œè§†è§‰tokené€‰æ‹©æ¨¡å—ã€‚åŠ¨ä½œé‡ç”¨æ¨¡å—è´Ÿè´£è¯†åˆ«å’Œé‡ç”¨ç›¸ä¼¼çš„åŠ¨ä½œæ­¥éª¤ï¼Œè€Œè§†è§‰tokené€‰æ‹©æ¨¡å—åˆ™é€šè¿‡ä¿¡æ¯å¼•å¯¼ç­–ç•¥é€‰æ‹©å¯¹ä»»åŠ¡è´¡çŒ®è¾ƒå¤§çš„tokenã€‚

**å…³é”®åˆ›æ–°**ï¼šFlashVLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è®­ç»ƒ-freeçš„è®¾è®¡ï¼Œå…è®¸åœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°åŠ é€Ÿã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å…³æ³¨äºåŠ¨ä½œæ­¥éª¤å’Œtokençš„å†—ä½™æ€§ï¼Œè€Œéå•çº¯çš„æ¶æ„ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨FlashVLAä¸­ï¼ŒåŠ¨ä½œé‡ç”¨æœºåˆ¶é€šè¿‡åˆ†æè¿ç»­åŠ¨ä½œçš„ç›¸ä¼¼æ€§æ¥å®ç°ï¼Œè€Œè§†è§‰tokené€‰æ‹©åˆ™åŸºäºä¿¡æ¯å¢ç›Šè¿›è¡Œtokençš„ç­›é€‰ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€å‚è€ƒåŸæ–‡è·å–æ›´å¤šæŠ€æœ¯ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashVLAåœ¨LIBEROåŸºå‡†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒFLOPså‡å°‘äº†55.7%ï¼Œå»¶è¿Ÿé™ä½äº†36.0%ï¼Œè€Œä»»åŠ¡æˆåŠŸç‡ä»…ä¸‹é™0.7%ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†FlashVLAåœ¨å®ç°è½»é‡çº§ã€ä½å»¶è¿ŸVLAæ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FlashVLAçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å®æ—¶å“åº”çš„æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ä½¿å¾—VLAæ¨¡å‹èƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œï¼Œä»è€Œæ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸æœºå™¨äººæŠ€æœ¯çš„ç»“åˆï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼ŒFlashVLAå¯èƒ½ä¼šåœ¨æ›´å¤šå¤æ‚ä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have emerged as a powerful paradigm for general-purpose robot control through natural language instructions. However, their high inference cost-stemming from large-scale token computation and autoregressive decoding-poses significant challenges for real-time deployment and edge applications. While prior work has primarily focused on architectural optimization, we take a different perspective by identifying a dual form of redundancy in VLA models: (i) high similarity across consecutive action steps, and (ii) substantial redundancy in visual tokens. Motivated by these observations, we propose FlashVLA, the first training-free and plug-and-play acceleration framework that enables action reuse in VLA models. FlashVLA improves inference efficiency through a token-aware action reuse mechanism that avoids redundant decoding across stable action steps, and an information-guided visual token selection strategy that prunes low-contribution tokens. Extensive experiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7% and latency by 36.0%, with only a 0.7% drop in task success rate. These results demonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency VLA inference without retraining.

