---
layout: default
title: "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning"
---

# MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21771" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21771v1</a>
  <a href="https://arxiv.org/pdf/2505.21771.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21771v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21771v1', 'MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prasham Yatinkumar Titiya, Jainil Trivedi, Chitta Baral, Vivek Gupta

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMTBENCHä»¥è§£å†³å¤æ‚å¤šæ¨¡æ€è¡¨æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¡¨æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `æ•°æ®åˆ†æ` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€è¡¨æ¨ç†æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è§†è§‰æ¨ç†å’Œå¤šæ­¥æ¨ç†çš„é—®é¢˜ä¸Šã€‚
2. æœ¬æ–‡æå‡ºMMTBENCHåŸºå‡†ï¼ŒåŒ…å«500ä¸ªå¤šæ¨¡æ€è¡¨æ ¼åŠ4021ä¸ªé—®ç­”å¯¹ï¼Œä»¥è¯„ä¼°å’Œæ¨åŠ¨å¤šæ¨¡æ€è¡¨æ¨ç†çš„ç ”ç©¶ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤šæ¨¡æ€è¡¨æ¨ç†ä¸Šå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰æ¨ç†å’Œå¤šæ­¥æ¨ç†æ–¹é¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€è¡¨æ ¼ç»“åˆäº†åŠç»“æ„åŒ–æ•°æ®ä¸è§†è§‰å…ƒç´ ï¼Œå¦‚å›¾è¡¨å’Œåœ°å›¾ï¼Œå¹¿æ³›å­˜åœ¨äºç°å®ä¸–ç•Œä¸­ï¼Œä½†å¯¹ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒVLMsåœ¨æ–‡æœ¬å’Œå›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚çš„ç°å®ä¸–ç•Œå¤šæ¨¡æ€è¡¨æ¨ç†ä¸Šçš„è¡¨ç°ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†MMTBENCHï¼ˆå¤šæ¨¡æ€è¡¨åŸºå‡†ï¼‰ï¼Œè¯¥åŸºå‡†åŒ…å«500ä¸ªæ¥è‡ªå¤šæ ·åŒ–ç°å®ä¸–ç•Œæ¥æºçš„å¤šæ¨¡æ€è¡¨æ ¼ï¼Œå…±æœ‰4021ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–å››ç§é—®é¢˜ç±»å‹å’Œäº”ç§æ¨ç†ç±»å‹ã€‚å¯¹ç°æœ‰æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨éœ€è¦è§†è§‰æ¨ç†å’Œå¤šæ­¥æ¨ç†çš„é—®é¢˜ä¸Šå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†æ”¹è¿›æ¶æ„çš„è¿«åˆ‡éœ€æ±‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ¨¡æ€è¡¨æ¨ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰æ¨ç†å’Œå¤šæ­¥æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMMTBENCHåŸºå‡†ï¼Œé€šè¿‡æä¾›çœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€è¡¨æ ¼å’Œå¤šæ ·åŒ–çš„é—®ç­”å¯¹ï¼Œæ¥è¯„ä¼°å’Œæ¨åŠ¨å¤šæ¨¡æ€è¡¨æ¨ç†çš„ç ”ç©¶ï¼Œä¿ƒè¿›æ¨¡å‹åœ¨æ­¤é¢†åŸŸçš„æ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMTBENCHåŸºå‡†åŒ…æ‹¬500ä¸ªå¤šæ¨¡æ€è¡¨æ ¼ï¼Œæ¶µç›–å››ç§é—®é¢˜ç±»å‹ï¼ˆæ˜¾å¼ã€éšå¼ã€ç­”æ¡ˆæåŠå’ŒåŸºäºè§†è§‰çš„é—®é¢˜ï¼‰å’Œäº”ç§æ¨ç†ç±»å‹ï¼ˆæ•°å­¦ã€æå€¼è¯†åˆ«ã€äº‹å®éªŒè¯ã€åŸºäºè§†è§‰çš„æ¨ç†åŠå…¶ä»–ï¼‰ï¼Œä¸ºæ¨¡å‹è¯„ä¼°æä¾›å…¨é¢çš„æµ‹è¯•åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šMMTBENCHçš„åˆ›æ–°åœ¨äºå…¶ç»¼åˆæ€§å’Œå¤šæ ·æ€§ï¼Œæä¾›äº†ä¸€ä¸ªé«˜è´¨é‡çš„èµ„æºï¼Œåæ˜ ç°å®ä¸–ç•Œä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡MMTBENCHæ—¶ï¼Œè€ƒè™‘äº†å¤šç§è¡¨æ ¼ç±»å‹ï¼ˆå•/å¤šå®ä½“ã€å¸¦å®ä½“çš„åœ°å›¾å’Œå›¾è¡¨ã€å•/å¤šå›¾è¡¨ã€åœ°å›¾å’Œå¯è§†åŒ–ï¼‰ï¼Œç¡®ä¿äº†åŸºå‡†çš„å…¨é¢æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚æ¯ä¸ªé—®é¢˜ç±»å‹å’Œæ¨ç†ç±»å‹çš„è®¾è®¡éƒ½æ—¨åœ¨è€ƒå¯Ÿæ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨MMTBENCHåŸºå‡†ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰æ¨ç†å’Œå¤šæ­¥æ¨ç†æ–¹é¢ï¼Œå¼ºè°ƒäº†å¯¹æ–°æ¶æ„çš„éœ€æ±‚ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®åˆ†æã€å•†ä¸šæ™ºèƒ½ã€æ•™è‚²å’ŒåŒ»ç–—ç­‰å¤šä¸ªé¢†åŸŸï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å¤æ‚çš„å¤šæ¨¡æ€æ•°æ®ã€‚æœªæ¥ï¼ŒMMTBENCHæœ‰æœ›æˆä¸ºå¤šæ¨¡æ€è¡¨æ¨ç†ç ”ç©¶çš„æ ‡å‡†åŸºå‡†ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal tables those that integrate semi structured data with visual elements such as charts and maps are ubiquitous across real world domains, yet they pose a formidable challenge to current vision language models (VLMs). While Large Language models (LLMs) and VLMs have demonstrated strong capabilities in text and image understanding, their performance on complex, real world multimodal table reasoning remains unexplored. To bridge this gap, we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of 500 real world multimodal tables drawn from diverse real world sources, with a total of 4021 question answer pairs. MMTBENCH questions cover four question types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning types (Mathematical, Extrema Identification, Fact Verification, Vision Based, and Others), and eight table types (Single/Multiple Entity, Maps and Charts with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive evaluation of state of the art models on all types reveals substantial performance gaps, particularly on questions requiring visual-based reasoning and multi-step inference. These findings show the urgent need for improved architectures that more tightly integrate vision and language processing. By providing a challenging, high-quality resource that mirrors the complexity of real-world tasks, MMTBENCH underscores its value as a resource for future research on multimodal tables.

