---
layout: default
title: EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models
---

# EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21567" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21567v2</a>
  <a href="https://arxiv.org/pdf/2505.21567.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21567v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21567v2', 'EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Feng Jiang, Zihao Zheng, Xiuping Cui, Maoliang Li, JIayu Chen, Xiang Chen

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27 (Êõ¥Êñ∞: 2025-07-31)

**Â§áÊ≥®**: There is an error in this paper, and as the author, I request retraction

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EaqVLA‰ª•Ëß£ÂÜ≥VLAÊ®°ÂûãÈáèÂåñÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `ÈáèÂåñÊäÄÊúØ` `ÁºñÁ†ÅÂØπÈΩê` `Ê∑∑ÂêàÁ≤æÂ∫¶` `ÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑVLAÊ®°ÂûãÂú®ËÆ°ÁÆóÂíåÂ≠òÂÇ®‰∏äÊàêÊú¨È´òÊòÇÔºå‰∫üÈúÄ‰ºòÂåñ‰ª•ÊèêÈ´òÊïàÁéá„ÄÇ
2. Êú¨ÊñáÊèêÂá∫EaqVLAÊ°ÜÊû∂ÔºåÈÄöËøáÁºñÁ†ÅÂØπÈΩêÈáèÂåñÊù•Ëß£ÂÜ≥VLAÊ®°Âûã‰∏≠ÁöÑÈáèÂåñÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåEaqVLAÂú®ÈáèÂåñÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈáèÂåñÊçüÂ§±ÊúÄÂ∞è‰∏îÂä†ÈÄüÊïàÊûúÊòæËëó„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèëÂ±ïÔºåÁ´ØÂà∞Á´ØÊéßÂà∂Á≠ñÁï•Â¶ÇËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂ∑≤Êàê‰∏∫‰∏ªÊµÅ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâVLAÊ®°ÂûãÈù¢‰∏¥È´òÊòÇÁöÑËÆ°ÁÆóÂíåÂ≠òÂÇ®ÊàêÊú¨ÔºåÈúÄË¶Å‰ºòÂåñ„ÄÇÈáèÂåñË¢´ËÆ§‰∏∫ÊòØÈôç‰ΩéÂÜÖÂ≠òÊàêÊú¨ÂíåÂä†ÈÄüËÆ°ÁÆóÁöÑÊúâÊïàÊñπÊ≥ïÔºå‰ΩÜÁé∞ÊúâÈáèÂåñÊñπÊ≥ïÂú®VLAÊ®°Âûã‰∏≠Â∫îÁî®ÂèóÈôê‰∫é‰ª§ÁâåÂØπÈΩêÈóÆÈ¢ò„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫EaqVLAÁöÑ‰ºòÂåñÊ°ÜÊû∂ÔºåÈááÁî®ÁºñÁ†ÅÂØπÈΩêÈáèÂåñ„ÄÇÈÄöËøáÂÆåÊï¥ÁöÑÂàÜÊûêÊñπÊ≥ïËØÜÂà´‰∏çÂêåÁ≤íÂ∫¶ÁöÑÈîô‰ΩçÔºåÂü∫‰∫éÂàÜÊûêÁªìÊûúÊèêÂá∫‰∫ÜËÄÉËôëÁºñÁ†ÅÂØπÈΩêÁöÑÊ∑∑ÂêàÁ≤æÂ∫¶ÈáèÂåñ„ÄÇÂÆûÈ™åË°®ÊòéÔºåEaqVLAÂú®Á´ØÂà∞Á´ØÂä®‰ΩúÊéßÂà∂‰∏≠ÂÆûÁé∞‰∫ÜÊõ¥Â•ΩÁöÑÈáèÂåñÊÄßËÉΩÔºåÈáèÂåñÊçüÂ§±ÊúÄÂ∞èÔºåÂπ∂‰∏îÂä†ÈÄüÊïàÊûúÊòæËëó„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥VLAÊ®°ÂûãÂú®ÈáèÂåñËøáÁ®ã‰∏≠Âõ†‰ª§ÁâåÂØπÈΩêÈóÆÈ¢òÂØºËá¥ÁöÑÊïàÁéá‰Ωé‰∏ã„ÄÇÁé∞ÊúâÈáèÂåñÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂ∫îÁî®‰∫éËøô‰∫õÊ®°ÂûãÔºåÈÄ†ÊàêËÆ°ÁÆóÂíåÂ≠òÂÇ®ÊàêÊú¨È´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫EaqVLAÊ°ÜÊû∂ÔºåÈÄöËøáÁºñÁ†ÅÂØπÈΩêÈáèÂåñÊäÄÊúØÔºåËØÜÂà´Âπ∂Ëß£ÂÜ≥VLAÊ®°Âûã‰∏≠ÁöÑÈîô‰ΩçÈóÆÈ¢òÔºå‰ª•ÊèêÈ´òÈáèÂåñÊïàÊûúÂíåËÆ°ÁÆóÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEaqVLAÊ°ÜÊû∂ÂåÖÊã¨ÂàÜÊûêÊ®°ÂùóÂíåÈáèÂåñÊ®°Âùó„ÄÇÂàÜÊûêÊ®°ÂùóË¥üË¥£ËØÜÂà´‰∏çÂêåÁ≤íÂ∫¶ÁöÑÈîô‰ΩçÔºåÈáèÂåñÊ®°ÂùóÂàôÂü∫‰∫éÂàÜÊûêÁªìÊûúÂÆûÊñΩÊ∑∑ÂêàÁ≤æÂ∫¶ÈáèÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEaqVLAÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂºïÂÖ•ÁºñÁ†ÅÂØπÈΩêÁöÑÊ¶ÇÂøµÔºåÊòæËëóÊîπÂñÑ‰∫ÜÈáèÂåñËøáÁ®ã‰∏≠ÁöÑÂØπÈΩêÈóÆÈ¢òÔºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îVLAÊ®°ÂûãÁöÑÁâπÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÊ∑∑ÂêàÁ≤æÂ∫¶ÈáèÂåñÁ≠ñÁï•ÔºåÁªìÂêà‰∫Ü‰∏çÂêåÂ±ÇÊ¨°ÁöÑÈáèÂåñÁ≤æÂ∫¶Ôºå‰ª•ÊúÄÂ∞èÂåñÈáèÂåñÊçüÂ§±ÔºåÂêåÊó∂‰ºòÂåñ‰∫ÜÁΩëÁªúÁªìÊûÑ‰ª•ÈÄÇÂ∫îÊñ∞ÁöÑÈáèÂåñÊñπÊ°à„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Âú®ÂÆûÈ™å‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÈ™åËØÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEaqVLAÂú®ÈáèÂåñÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈáèÂåñÊçüÂ§±ÊúÄÂ∞èÔºå‰∏îÂú®Á´ØÂà∞Á´ØÂä®‰ΩúÊéßÂà∂‰∏≠ÂÆûÁé∞‰∫ÜxxxÂÄçÁöÑÂä†ÈÄüÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EaqVLAÊ°ÜÊû∂Âú®ÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÊô∫ËÉΩÂä©ÊâãÂíåËá™Âä®È©æÈ©∂Á≠âÂú∫ÊôØ‰∏≠„ÄÇÈÄöËøáÈôç‰ΩéËÆ°ÁÆóÂíåÂ≠òÂÇ®ÊàêÊú¨ÔºåËØ•ÊñπÊ≥ïËÉΩÂ§ü‰ΩøÂæóVLAÊ®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠Êõ¥È´òÊïàÂú∞ËøêË°åÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂÆûÈôÖÂ∫îÁî®ÂíåÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> With the development of Embodied Artificial intelligence, the end-to-end control policy such as Vision-Language-Action (VLA) model has become the mainstream. Existing VLA models faces expensive computing/storage cost, which need to be optimized. Quantization is considered as the most effective method which can not only reduce the memory cost but also achieve computation acceleration. However, we find the token alignment of VLA models hinders the application of existing quantization methods. To address this, we proposed an optimized framework called EaqVLA, which apply encoding-aligned quantization to VLA models. Specifically, we propose an complete analysis method to find the misalignment in various granularity. Based on the analysis results, we propose a mixed precision quantization with the awareness of encoding alignment. Experiments shows that the porposed EaqVLA achieves better quantization performance (with the minimal quantization loss for end-to-end action control and xxx times acceleration) than existing quantization methods.

