---
layout: default
title: "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding"
---

# PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20759" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20759v3</a>
  <a href="https://arxiv.org/pdf/2505.20759.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20759v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20759v3', 'PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ansel Blume, Jeonghwan Kim, Hyeonjeong Ha, Elen Chatikyan, Xiaomeng Jin, Khanh Duy Nguyen, Nanyun Peng, Kai-Wei Chang, Derek Hoiem, Heng Ji

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-10-26)

**å¤‡æ³¨**: NeurIPS 2025 Spotlight; project page: https://wjdghks950.github.io/partonomy.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPARTONOMYä»¥è§£å†³å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„éƒ¨ä»¶è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `éƒ¨ä»¶è¯†åˆ«` `è§†è§‰ç†è§£` `ç»†ç²’åº¦æ¨ç†` `PLUMæ¨¡å‹` `æ•°æ®é›†æ„å»º` `åé¦ˆæœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹åœ¨éƒ¨ä»¶è¯†åˆ«å’Œå®šä½æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆè¿›è¡Œç»†ç²’åº¦æ¨ç†ã€‚
2. æœ¬æ–‡æå‡ºPARTONOMYåŸºå‡†ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«ä¸°å¯Œéƒ¨ä»¶å’Œç‰©ä½“æ ‡ç­¾çš„æ•°æ®é›†ï¼Œå¹¶æå‡ºPLUMæ¨¡å‹ä»¥æ”¹è¿›éƒ¨ä»¶å®šä½èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPLUMåœ¨æ¨ç†åˆ†å‰²ã€è§†è§‰é—®ç­”å’Œè§†è§‰å¹»è§‰åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åˆ†å‰²LMMï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œä¸­çš„ç‰©ä½“ç”±ç‹¬ç‰¹çš„ã€ç‰¹å®šäºç‰©ä½“çš„éƒ¨ä»¶ç»„æˆã€‚è¯†åˆ«è¿™äº›éƒ¨ä»¶å¯¹äºè¿›è¡Œç»†ç²’åº¦çš„ç»„åˆæ¨ç†è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è¿™ä¸€çœ‹ä¼¼ç®€å•çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†PARTONOMYï¼Œä¸€ä¸ªæ—¨åœ¨è¿›è¡Œåƒç´ çº§éƒ¨ä»¶å®šä½çš„LMMåŸºå‡†ã€‚PARTONOMYåŸºäºç°æœ‰çš„éƒ¨ä»¶æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªå·±ä¸¥æ ¼æ³¨é‡Šçš„å›¾åƒé›†æ„å»ºï¼Œæ¶µç›–862ä¸ªéƒ¨ä»¶æ ‡ç­¾å’Œ534ä¸ªç‰©ä½“æ ‡ç­¾ã€‚ä¸ç°æœ‰æ•°æ®é›†ä¸åŒï¼ŒPARTONOMYä½¿ç”¨ä¸“ä¸šæ¦‚å¿µï¼Œå¹¶æŒ‘æˆ˜æ¨¡å‹æ¯”è¾ƒç‰©ä½“çš„éƒ¨ä»¶ã€è€ƒè™‘éƒ¨ä»¶ä¸æ•´ä½“çš„å…³ç³»ï¼Œå¹¶ç”¨è§†è§‰åˆ†å‰²æ¥è¯æ˜æ–‡æœ¬é¢„æµ‹çš„åˆç†æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„LMMsåœ¨éƒ¨ä»¶å®šä½èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PLUMï¼Œä¸€ä¸ªæ–°çš„åˆ†å‰²LMMï¼Œé‡‡ç”¨è·¨åº¦æ ‡è®°è€Œéåˆ†å‰²æ ‡è®°ï¼Œå¹¶åœ¨åé¦ˆå¾ªç¯ä¸­ä¾èµ–äºå…ˆå‰çš„é¢„æµ‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹åœ¨éƒ¨ä»¶è¯†åˆ«å’Œå®šä½æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç»†ç²’åº¦æ¨ç†æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨éƒ¨ä»¶ä¸æ•´ä½“å…³ç³»çš„ç†è§£ä¸Šå­˜åœ¨ç¼ºé™·ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†PARTONOMYåŸºå‡†ï¼Œåˆ©ç”¨ä¸°å¯Œçš„éƒ¨ä»¶å’Œç‰©ä½“æ ‡ç­¾æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†PLUMæ¨¡å‹ï¼Œé€šè¿‡è·¨åº¦æ ‡è®°æ›¿ä»£åˆ†å‰²æ ‡è®°ï¼Œå¢å¼ºæ¨¡å‹çš„éƒ¨ä»¶å®šä½èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPLUMæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥å›¾åƒçš„ç‰¹å¾æå–ã€éƒ¨ä»¶å®šä½çš„è·¨åº¦æ ‡è®°å¤„ç†ï¼Œä»¥åŠåŸºäºå…ˆå‰é¢„æµ‹çš„åé¦ˆå¾ªç¯æœºåˆ¶ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿåˆ©ç”¨å†å²ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šPLUMçš„ä¸»è¦åˆ›æ–°åœ¨äºé‡‡ç”¨è·¨åº¦æ ‡è®°è€Œéä¼ ç»Ÿçš„åˆ†å‰²æ ‡è®°ï¼Œé¿å…äº†åœ¨é¢„è®­ç»ƒé˜¶æ®µæœªè§è¿‡çš„æ ‡è®°å¼•èµ·çš„åˆ†å¸ƒåç§»ï¼ŒåŒæ—¶é€šè¿‡åé¦ˆæœºåˆ¶æå‡äº†æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šPLUMæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†æ–°çš„æŸå¤±å‡½æ•°ï¼Œå¼ºè°ƒéƒ¨ä»¶ä¸æ•´ä½“çš„å…³ç³»ï¼Œå¹¶é€šè¿‡å¤šå±‚æ¬¡çš„ç‰¹å¾èåˆæ¥å¢å¼ºæ¨¡å‹çš„è¡¨ç°ï¼Œç¡®ä¿åœ¨ç»†ç²’åº¦æ¨ç†ä»»åŠ¡ä¸­å…·å¤‡æ›´å¼ºçš„èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢„è®­ç»ƒçš„PLUMåœ¨æ¨ç†åˆ†å‰²ã€è§†è§‰é—®ç­”å’Œè§†è§‰å¹»è§‰åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åˆ†å‰²LMMï¼Œå°¤å…¶åœ¨æ¨ç†åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒPLUMçš„è¡¨ç°æå‡å¹…åº¦è¾¾åˆ°äº†æ˜¾è‘—çš„æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶åœ¨éƒ¨ä»¶å®šä½èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚åœºæ™¯ä¸­çš„ç‰©ä½“å’Œéƒ¨ä»¶å…³ç³»ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹æ€§èƒ½çš„æå‡ï¼Œå¯èƒ½ä¼šæ¨åŠ¨æ›´å¹¿æ³›çš„è§†è§‰ç†è§£ä»»åŠ¡çš„è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 part labels and 534 object labels for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY uses specialized concepts (e.g., agricultural airplane), and challenges models to compare objects' parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that uses span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM outperforms existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.

