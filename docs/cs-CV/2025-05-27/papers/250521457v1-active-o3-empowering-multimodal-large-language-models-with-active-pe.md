---
layout: default
title: Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO
---

# Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21457" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21457v1</a>
  <a href="https://arxiv.org/pdf/2505.21457.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21457v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21457v1', 'Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, Chunhua Shen

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27

**Â§áÊ≥®**: Project Page: https://aim-uofa.github.io/ACTIVE-o3

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ACTIVE-O3‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏ªÂä®ÊÑüÁü•ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰∏ªÂä®ÊÑüÁü•` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÂØºËà™` `Ëá™Âä®È©æÈ©∂` `ÈÅ•ÊÑüÂõæÂÉèÂàÜÊûê` `‰ø°ÊÅØËé∑Âèñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®‰∏ªÂä®ÊÑüÁü•ËÉΩÂäõ‰∏äÊé¢Á¥¢‰∏çË∂≥ÔºåÂØºËá¥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®‰ªªÂä°Áõ∏ÂÖ≥‰ø°ÊÅØËé∑Âèñ‰∏äÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. ÊèêÂá∫ACTIVE-O3ÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂GRPOÔºåËµã‰∫àÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏ªÂä®ÊÑüÁü•ËÉΩÂäõÔºåÊèêÂçá‰ø°ÊÅØËé∑ÂèñÊïàÁéá„ÄÇ
3. ACTIVE-O3Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®Èõ∂-shotÊé®ÁêÜËÉΩÂäõ‰∏äÔºåÊú™‰æùËµñ‰ªª‰ΩïÊòæÂºèÊé®ÁêÜÊï∞ÊçÆ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏ªÂä®ËßÜËßâÔºå‰πüÁß∞‰∏∫‰∏ªÂä®ÊÑüÁü•ÔºåÊåáÁöÑÊòØ‰∏ªÂä®ÈÄâÊã©ËßÇÂØü‰ΩçÁΩÆÂíåÊñπÂºè‰ª•Ëé∑Âèñ‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØ„ÄÇÂÆÉÊòØ‰∫∫Á±ªÂíåÂÖàËøõÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÈ´òÊïàÊÑüÁü•‰∏éÂÜ≥Á≠ñÁöÑÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜ„ÄÇËøëÂπ¥Êù•ÔºåÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ‰Ωú‰∏∫Êú∫Âô®‰∫∫Á≥ªÁªü‰∏≠ÁöÑ‰∏≠Â§ÆËßÑÂàíÂíåÂÜ≥Á≠ñÊ®°ÂùóÂèóÂà∞ÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÁÑ∂ËÄåÔºåÂ∞ΩÁÆ°‰∏ªÂä®ÊÑüÁü•Âú®ÂÖ∑Ë∫´Êô∫ËÉΩ‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÂÖ≥‰∫éÂ¶Ç‰Ωï‰∏∫MLLMsËµã‰∫àÊàñÂ≠¶‰π†‰∏ªÂä®ÊÑüÁü•ËÉΩÂäõÁöÑÊé¢Á¥¢‰ªçÁÑ∂ËæÉÂ∞ë„ÄÇÊú¨ÊñáÈ¶ñÂÖàÁ≥ªÁªüÂÆö‰πâ‰∫ÜÂü∫‰∫éMLLMÁöÑ‰∏ªÂä®ÊÑüÁü•‰ªªÂä°ÔºåÂπ∂ÊåáÂá∫ÊúÄËøëÊèêÂá∫ÁöÑGPT-o3Ê®°ÂûãÁöÑÊîæÂ§ßÊêúÁ¥¢Á≠ñÁï•ÂèØ‰ª•ËßÜ‰∏∫‰∏ªÂä®ÊÑüÁü•ÁöÑÁâπ‰æãÔºå‰ΩÜ‰ªçÂ≠òÂú®ÊêúÁ¥¢ÊïàÁéá‰ΩéÂíåÂå∫ÂüüÈÄâÊã©‰∏çÂáÜÁ°ÆÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜACTIVE-O3Ôºå‰∏Ä‰∏™Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊó®Âú®‰∏∫MLLMsËµã‰∫à‰∏ªÂä®ÊÑüÁü•ËÉΩÂäõ„ÄÇÊàë‰ª¨ËøòÂª∫Á´ã‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÂ•ó‰ª∂Ôºå‰ª•ËØÑ‰º∞ACTIVE-O3Âú®‰∏ÄËà¨ÂºÄÊîæ‰∏ñÁïå‰ªªÂä°ÂíåÁâπÂÆöÈ¢ÜÂüüÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏ªÂä®ÊÑüÁü•ËÉΩÂäõ‰∏äÁöÑ‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇGPT-o3Âú®ÊêúÁ¥¢ÊïàÁéáÂíåÂå∫ÂüüÈÄâÊã©‰∏äÂ≠òÂú®ÁóõÁÇπ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöACTIVE-O3ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂GRPOÔºåËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºå‰ΩøÂæóMLLMsËÉΩÂ§ü‰∏ªÂä®ÈÄâÊã©ËßÇÂØüÂå∫ÂüüÔºå‰ªéËÄåÊèêÈ´ò‰ø°ÊÅØËé∑ÂèñÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöACTIVE-O3ÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰ªªÂä°ÂÆö‰πâ„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåËØÑ‰º∞‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÂÆö‰πâ‰∏ªÂä®ÊÑüÁü•‰ªªÂä°ÔºåÁÑ∂ÂêéÈÄöËøáGRPOËøõË°åÊ®°ÂûãËÆ≠ÁªÉÔºåÊúÄÂêéÂú®Â§ö‰∏™Âü∫ÂáÜ‰∏äËøõË°åËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöACTIVE-O3ÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊòæËëóÊèêÂçá‰∫ÜMLLMsÂú®‰∏ªÂä®ÊÑüÁü•‰ªªÂä°‰∏≠ÁöÑË°®Áé∞Ôºå‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÈÄâÊã©ËßÇÂØüÂå∫Âüü„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåACTIVE-O3ÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÂå∫ÂüüÈÄâÊã©ÁöÑÂáÜÁ°ÆÊÄßÔºåÂπ∂ËÆæËÆ°‰∫ÜÈÄÇÂ∫îÊÄßÁΩëÁªúÁªìÊûÑ‰ª•ÊîØÊåÅÂ§öÊ®°ÊÄÅËæìÂÖ•ÁöÑÂ§ÑÁêÜ„ÄÇÈÄöËøáËøô‰∫õËÆæËÆ°ÔºåÊ®°ÂûãÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞ÂæóÂà∞‰∫ÜÊèêÂçá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ACTIVE-O3Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®V*Âü∫ÂáÜ‰∏äÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈõ∂-shotÊé®ÁêÜËÉΩÂäõÔºåÊú™‰æùËµñ‰ªª‰ΩïÊòæÂºèÊé®ÁêÜÊï∞ÊçÆ„ÄÇ‰∏éÂü∫Á∫øÊ®°ÂûãÁõ∏ÊØîÔºåACTIVE-O3Âú®Â∞èÁâ©‰ΩìÊ£ÄÊµãÂíåÂØÜÈõÜÁâ©‰ΩìÂÆö‰Ωç‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÈÅ•ÊÑüÂõæÂÉèÂàÜÊûêÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏ªÂä®ÊÑüÁü•ËÉΩÂäõÔºåËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠Êõ¥ÊúâÊïàÂú∞ËøõË°å‰ø°ÊÅØËé∑ÂèñÂíåÂÜ≥Á≠ñÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.

