---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-27
---

# cs.CVï¼ˆ2025-05-27ï¼‰

ğŸ“Š å…± **65** ç¯‡è®ºæ–‡
 | ğŸ”— **24** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (24 ğŸ”—8)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (18 ğŸ”—6)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (24 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250521375v2-geollava-8k-scaling-remote-sensing-multimodal-large-language-models-.html">GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution</a></td>
  <td>æå‡ºGeoLLaVA-8Kä»¥è§£å†³è¶…é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒå¤„ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21375v2" data-paper-url="./papers/250521375v2-geollava-8k-scaling-remote-sensing-multimodal-large-language-models-.html" onclick="toggleFavorite(this, '2505.21375v2', 'GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250521076v2-dynamicvl-benchmarking-multimodal-large-language-models-for-dynamic-.html">DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding</a></td>
  <td>æå‡ºDVL-Suiteä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŸå¸‚åŠ¨æ€ç†è§£ä¸­çš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21076v2" data-paper-url="./papers/250521076v2-dynamicvl-benchmarking-multimodal-large-language-models-for-dynamic-.html" onclick="toggleFavorite(this, '2505.21076v2', 'DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250520873v2-fork-merge-decoding-enhancing-multimodal-understanding-in-audio-visu.html">Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models</a></td>
  <td>æå‡ºFork-Mergeè§£ç ä»¥è§£å†³éŸ³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ¨¡æ€åå·®é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20873v2" data-paper-url="./papers/250520873v2-fork-merge-decoding-enhancing-multimodal-understanding-in-audio-visu.html" onclick="toggleFavorite(this, '2505.20873v2', 'Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250521771v1-mmtbench-a-unified-benchmark-for-complex-multimodal-table-reasoning.html">MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning</a></td>
  <td>æå‡ºMMTBENCHä»¥è§£å†³å¤æ‚å¤šæ¨¡æ€è¡¨æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21771v1" data-paper-url="./papers/250521771v1-mmtbench-a-unified-benchmark-for-complex-multimodal-table-reasoning.html" onclick="toggleFavorite(this, '2505.21771v1', 'MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250521333v2-mme-videoocr-evaluating-ocr-based-capabilities-of-multimodal-llms-in.html">MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios</a></td>
  <td>æå‡ºMME-VideoOCRä»¥è§£å†³è§†é¢‘åœºæ™¯ä¸‹OCRæ•ˆæœä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21333v2" data-paper-url="./papers/250521333v2-mme-videoocr-evaluating-ocr-based-capabilities-of-multimodal-llms-in.html" onclick="toggleFavorite(this, '2505.21333v2', 'MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250521200v1-think-twice-act-once-token-aware-compression-and-action-reuse-for-ef.html">Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models</a></td>
  <td>æå‡ºFlashVLAä»¥è§£å†³VLAæ¨¡å‹æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21200v1" data-paper-url="./papers/250521200v1-think-twice-act-once-token-aware-compression-and-action-reuse-for-ef.html" onclick="toggleFavorite(this, '2505.21200v1', 'Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250520862v2-avcd-mitigating-hallucinations-in-audio-visual-large-language-models.html">AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding</a></td>
  <td>æå‡ºAVCDä»¥è§£å†³éŸ³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20862v2" data-paper-url="./papers/250520862v2-avcd-mitigating-hallucinations-in-audio-visual-large-language-models.html" onclick="toggleFavorite(this, '2505.20862v2', 'AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250521567v2-eaqvla-encoding-aligned-quantization-for-vision-language-action-mode.html">EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models</a></td>
  <td>æå‡ºEaqVLAä»¥è§£å†³VLAæ¨¡å‹é‡åŒ–æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21567v2" data-paper-url="./papers/250521567v2-eaqvla-encoding-aligned-quantization-for-vision-language-action-mode.html" onclick="toggleFavorite(this, '2505.21567v2', 'EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250520638v1-musics-multimodal-complexity-in-avqa-why-we-need-more-than-general-m.html">Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs</a></td>
  <td>æå‡ºä¸“é—¨åŒ–æ–¹æ³•ä»¥è§£å†³éŸ³ä¹éŸ³è§†é¢‘é—®ç­”çš„å¤æ‚æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20638v1" data-paper-url="./papers/250520638v1-musics-multimodal-complexity-in-avqa-why-we-need-more-than-general-m.html" onclick="toggleFavorite(this, '2505.20638v1', 'Music&#39;s Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250602012v1-leveraging-large-language-models-in-visual-speech-recognition-model-.html">Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing</a></td>
  <td>æå‡ºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æå‡è§†è§‰è¯­éŸ³è¯†åˆ«æ€§èƒ½çš„æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02012v1" data-paper-url="./papers/250602012v1-leveraging-large-language-models-in-visual-speech-recognition-model-.html" onclick="toggleFavorite(this, '2506.02012v1', 'Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250521497v2-paper2poster-towards-multimodal-poster-automation-from-scientific-pa.html">Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers</a></td>
  <td>æå‡ºPaper2Posterä»¥è§£å†³å­¦æœ¯æµ·æŠ¥è‡ªåŠ¨ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21497v2" data-paper-url="./papers/250521497v2-paper2poster-towards-multimodal-poster-automation-from-scientific-pa.html" onclick="toggleFavorite(this, '2505.21497v2', 'Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250521334v3-holitom-holistic-token-merging-for-fast-video-large-language-models.html">HoliTom: Holistic Token Merging for Fast Video Large Language Models</a></td>
  <td>æå‡ºHoliTomä»¥è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„è®¡ç®—æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21334v3" data-paper-url="./papers/250521334v3-holitom-holistic-token-merging-for-fast-video-large-language-models.html" onclick="toggleFavorite(this, '2505.21334v3', 'HoliTom: Holistic Token Merging for Fast Video Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250520759v3-partonomy-large-multimodal-models-with-part-level-visual-understandi.html">PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding</a></td>
  <td>æå‡ºPARTONOMYä»¥è§£å†³å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„éƒ¨ä»¶è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20759v3" data-paper-url="./papers/250520759v3-partonomy-large-multimodal-models-with-part-level-visual-understandi.html" onclick="toggleFavorite(this, '2505.20759v3', 'PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250520753v1-understand-think-and-answer-advancing-visual-reasoning-with-large-mu.html">Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models</a></td>
  <td>æå‡ºç»Ÿä¸€è§†è§‰æ¨ç†æœºåˆ¶ä»¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„å¤åˆæ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20753v1" data-paper-url="./papers/250520753v1-understand-think-and-answer-advancing-visual-reasoning-with-large-mu.html" onclick="toggleFavorite(this, '2505.20753v1', 'Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250521698v2-medbridge-bridging-foundation-vision-language-models-to-medical-imag.html">MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis in Chest X-Ray</a></td>
  <td>æå‡ºMedBridgeä»¥è§£å†³åŒ»å­¦å½±åƒè¯Šæ–­ä¸­çš„é¢†åŸŸé€‚åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21698v2" data-paper-url="./papers/250521698v2-medbridge-bridging-foundation-vision-language-models-to-medical-imag.html" onclick="toggleFavorite(this, '2505.21698v2', 'MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis in Chest X-Ray')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250521653v3-think-before-you-diffuse-infusing-physical-rules-into-video-diffusio.html">Think Before You Diffuse: Infusing Physical Rules into Video Diffusion</a></td>
  <td>æå‡ºDiffPhyæ¡†æ¶ä»¥è§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ç†å‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21653v3" data-paper-url="./papers/250521653v3-think-before-you-diffuse-infusing-physical-rules-into-video-diffusio.html" onclick="toggleFavorite(this, '2505.21653v3', 'Think Before You Diffuse: Infusing Physical Rules into Video Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250521494v1-adversarial-attacks-against-closed-source-mllms-via-feature-optimal-.html">Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment</a></td>
  <td>æå‡ºFOA-Attackä»¥è§£å†³é—­æºMLLMsçš„å¯¹æŠ—æ”»å‡»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21494v1" data-paper-url="./papers/250521494v1-adversarial-attacks-against-closed-source-mllms-via-feature-optimal-.html" onclick="toggleFavorite(this, '2505.21494v1', 'Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250521472v2-mitigating-hallucination-in-large-vision-language-models-via-adaptiv.html">Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration</a></td>
  <td>æå‡ºCAACæ¡†æ¶ä»¥è§£å†³å¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21472v2" data-paper-url="./papers/250521472v2-mitigating-hallucination-in-large-vision-language-models-via-adaptiv.html" onclick="toggleFavorite(this, '2505.21472v2', 'Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250602011v2-oasis-online-sample-selection-for-continual-visual-instruction-tunin.html">OASIS: Online Sample Selection for Continual Visual Instruction Tuning</a></td>
  <td>æå‡ºOASISä»¥è§£å†³æŒç»­è§†è§‰æŒ‡ä»¤è°ƒä¼˜ä¸­çš„æ ·æœ¬é€‰æ‹©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.02011v2" data-paper-url="./papers/250602011v2-oasis-online-sample-selection-for-continual-visual-instruction-tunin.html" onclick="toggleFavorite(this, '2506.02011v2', 'OASIS: Online Sample Selection for Continual Visual Instruction Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250521420v1-mentor3ad-feature-reconstruction-based-3d-anomaly-detection-via-mult.html">Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning</a></td>
  <td>æå‡ºMentor3ADä»¥è§£å†³3Då¼‚å¸¸æ£€æµ‹ä¸­çš„ç‰¹å¾é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21420v1" data-paper-url="./papers/250521420v1-mentor3ad-feature-reconstruction-based-3d-anomaly-detection-via-mult.html" onclick="toggleFavorite(this, '2505.21420v1', 'Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250521374v1-video-holmes-can-mllm-think-like-holmes-for-complex-video-reasoning.html">Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?</a></td>
  <td>æå‡ºVideo-HolmesåŸºå‡†ä»¥è§£å†³å¤æ‚è§†é¢‘æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21374v1" data-paper-url="./papers/250521374v1-video-holmes-can-mllm-think-like-holmes-for-complex-video-reasoning.html" onclick="toggleFavorite(this, '2505.21374v1', 'Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250521062v1-inverse-virtual-try-on-generating-multi-category-product-style-image.html">Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals</a></td>
  <td>æå‡ºTEMU-VTOFFä»¥è§£å†³è™šæ‹Ÿè¯•ç©¿é€†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21062v1" data-paper-url="./papers/250521062v1-inverse-virtual-try-on-generating-multi-category-product-style-image.html" onclick="toggleFavorite(this, '2505.21062v1', 'Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250521050v2-advancing-high-fidelity-3d-and-texture-generation-with-25d-latents.html">Advancing high-fidelity 3D and Texture Generation with 2.5D latents</a></td>
  <td>æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥è§£å†³3Då‡ ä½•ä¸çº¹ç†ç”Ÿæˆä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21050v2" data-paper-url="./papers/250521050v2-advancing-high-fidelity-3d-and-texture-generation-with-25d-latents.html" onclick="toggleFavorite(this, '2505.21050v2', 'Advancing high-fidelity 3D and Texture Generation with 2.5D latents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250520629v2-unified-text-image-to-video-generation-a-training-free-approach-to-f.html">Unified Text-Image-to-Video Generation: A Training-Free Approach to Flexible Visual Conditioning</a></td>
  <td>æå‡ºFlexTI2Vä»¥è§£å†³è®­ç»ƒæˆæœ¬é«˜å’Œæ¡ä»¶è®¾ç½®æœ‰é™çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20629v2" data-paper-url="./papers/250520629v2-unified-text-image-to-video-generation-a-training-free-approach-to-f.html" onclick="toggleFavorite(this, '2505.20629v2', 'Unified Text-Image-to-Video Generation: A Training-Free Approach to Flexible Visual Conditioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (18 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250521457v1-active-o3-empowering-multimodal-large-language-models-with-active-pe.html">Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO</a></td>
  <td>æå‡ºACTIVE-O3ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸»åŠ¨æ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21457v1" data-paper-url="./papers/250521457v1-active-o3-empowering-multimodal-large-language-models-with-active-pe.html" onclick="toggleFavorite(this, '2505.21457v1', 'Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250520611v2-mamba-driven-topology-fusion-for-monocular-3d-human-pose-estimation.html">Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation</a></td>
  <td>æå‡ºMambaé©±åŠ¨çš„æ‹“æ‰‘èåˆæ¡†æ¶ä»¥è§£å†³å•ç›®3Däººä½“å§¿æ€ä¼°è®¡ä¸­çš„è®¡ç®—æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20611v2" data-paper-url="./papers/250520611v2-mamba-driven-topology-fusion-for-monocular-3d-human-pose-estimation.html" onclick="toggleFavorite(this, '2505.20611v2', 'Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250520897v2-cross-from-left-to-right-brain-adaptive-text-dreamer-for-vision-and-.html">Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation</a></td>
  <td>æå‡ºè‡ªé€‚åº”æ–‡æœ¬æ¢¦å¢ƒç”Ÿæˆå™¨ä»¥è§£å†³è§†è§‰ä¸è¯­è¨€å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dreamer</span> <span class="paper-tag">VLN</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20897v2" data-paper-url="./papers/250520897v2-cross-from-left-to-right-brain-adaptive-text-dreamer-for-vision-and-.html" onclick="toggleFavorite(this, '2505.20897v2', 'Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250521635v1-object-concepts-emerge-from-motion.html">Object Concepts Emerge from Motion</a></td>
  <td>æå‡ºä¸€ç§æ— ç›‘ç£æ¡†æ¶ä»¥ä»è¿åŠ¨ä¸­å­¦ä¹ ç‰©ä½“æ¦‚å¿µ</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21635v1" data-paper-url="./papers/250521635v1-object-concepts-emerge-from-motion.html" onclick="toggleFavorite(this, '2505.21635v1', 'Object Concepts Emerge from Motion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250520777v1-taco-think-answer-consistency-for-optimized-long-chain-reasoning-and.html">TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs</a></td>
  <td>æå‡ºTACOä»¥è§£å†³é•¿é“¾æ¨ç†ä¸­çš„ä¸€è‡´æ€§ä¸å­¦ä¹ æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20777v1" data-paper-url="./papers/250520777v1-taco-think-answer-consistency-for-optimized-long-chain-reasoning-and.html" onclick="toggleFavorite(this, '2505.20777v1', 'TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250521381v6-zigzagpointmamba-spatial-semantic-mamba-for-point-cloud-understandin.html">ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding</a></td>
  <td>æå‡ºZigzagPointMambaä»¥è§£å†³ç‚¹äº‘ç†è§£ä¸­çš„ç©ºé—´è¿ç»­æ€§å’Œè¯­ä¹‰å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">SSM</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21381v6" data-paper-url="./papers/250521381v6-zigzagpointmamba-spatial-semantic-mamba-for-point-cloud-understandin.html" onclick="toggleFavorite(this, '2505.21381v6', 'ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250520715v1-museg-reinforcing-video-temporal-understanding-via-timestamp-aware-m.html">MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding</a></td>
  <td>æå‡ºMUSEGä»¥è§£å†³è§†é¢‘æ—¶é—´ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20715v1" data-paper-url="./papers/250520715v1-museg-reinforcing-video-temporal-understanding-via-timestamp-aware-m.html" onclick="toggleFavorite(this, '2505.20715v1', 'MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250521478v2-policy-optimized-text-to-image-pipeline-design.html">Policy Optimized Text-to-Image Pipeline Design</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç®¡é“è®¾è®¡ä»¥è§£å†³æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">classifier-free guidance</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21478v2" data-paper-url="./papers/250521478v2-policy-optimized-text-to-image-pipeline-design.html" onclick="toggleFavorite(this, '2505.21478v2', 'Policy Optimized Text-to-Image Pipeline Design')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250520617v3-occle-label-efficient-3d-semantic-occupancy-prediction.html">OccLE: Label-Efficient 3D Semantic Occupancy Prediction</a></td>
  <td>æå‡ºOccLEä»¥è§£å†³3Dè¯­ä¹‰å ç”¨é¢„æµ‹ä¸­çš„æ ‡æ³¨æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20617v3" data-paper-url="./papers/250520617v3-occle-label-efficient-3d-semantic-occupancy-prediction.html" onclick="toggleFavorite(this, '2505.20617v3', 'OccLE: Label-Efficient 3D Semantic Occupancy Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250521448v2-omnisync-towards-universal-lip-synchronization-via-diffusion-transfo.html">OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers</a></td>
  <td>æå‡ºOmniSyncä»¥è§£å†³å¤šæ ·åŒ–åœºæ™¯ä¸‹çš„å”‡åŠ¨åŒæ­¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">classifier-free guidance</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21448v2" data-paper-url="./papers/250521448v2-omnisync-towards-universal-lip-synchronization-via-diffusion-transfo.html" onclick="toggleFavorite(this, '2505.21448v2', 'OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250520975v1-dreamboothdpo-improving-personalized-generation-using-direct-prefere.html">DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization</a></td>
  <td>æå‡ºDreamBoothDPOä»¥è§£å†³ä¸ªæ€§åŒ–ç”Ÿæˆä¸­çš„åå¥½ä¼˜åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20975v1" data-paper-url="./papers/250520975v1-dreamboothdpo-improving-personalized-generation-using-direct-prefere.html" onclick="toggleFavorite(this, '2505.20975v1', 'DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250520941v1-pma-towards-parameter-efficient-point-cloud-understanding-via-point-.html">PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter</a></td>
  <td>æå‡ºPMAä»¥è§£å†³ç‚¹äº‘ç†è§£ä¸­çš„ä¿¡æ¯åˆ©ç”¨ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20941v1" data-paper-url="./papers/250520941v1-pma-towards-parameter-efficient-point-cloud-understanding-via-point-.html" onclick="toggleFavorite(this, '2505.20941v1', 'PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250520793v2-rendering-aware-reinforcement-learning-for-vector-graphics-generatio.html">Rendering-Aware Reinforcement Learning for Vector Graphics Generation</a></td>
  <td>æå‡ºRLRFä»¥è§£å†³SVGç”Ÿæˆä¸­çš„æ¸²æŸ“åé¦ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20793v2" data-paper-url="./papers/250520793v2-rendering-aware-reinforcement-learning-for-vector-graphics-generatio.html" onclick="toggleFavorite(this, '2505.20793v2', 'Rendering-Aware Reinforcement Learning for Vector Graphics Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250520710v1-hierarchical-instruction-aware-embodied-visual-tracking.html">Hierarchical Instruction-aware Embodied Visual Tracking</a></td>
  <td>æå‡ºHIEVTä»¥è§£å†³ç”¨æˆ·ä¸­å¿ƒçš„è§†è§‰è·Ÿè¸ªæŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20710v1" data-paper-url="./papers/250520710v1-hierarchical-instruction-aware-embodied-visual-tracking.html" onclick="toggleFavorite(this, '2505.20710v1', 'Hierarchical Instruction-aware Embodied Visual Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250520694v1-temporal-saliency-guided-distillation-a-scalable-framework-for-disti.html">Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets</a></td>
  <td>æå‡ºæ—¶é—´æ˜¾è‘—æ€§å¼•å¯¼è’¸é¦æ¡†æ¶ä»¥è§£å†³è§†é¢‘æ•°æ®é›†å‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20694v1" data-paper-url="./papers/250520694v1-temporal-saliency-guided-distillation-a-scalable-framework-for-disti.html" onclick="toggleFavorite(this, '2505.20694v1', 'Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250520676v1-supervised-contrastive-learning-for-ordinal-engagement-measurement.html">Supervised Contrastive Learning for Ordinal Engagement Measurement</a></td>
  <td>æå‡ºç›‘ç£å¯¹æ¯”å­¦ä¹ ä»¥è§£å†³å­¦ç”Ÿå‚ä¸åº¦æµ‹é‡ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20676v1" data-paper-url="./papers/250520676v1-supervised-contrastive-learning-for-ordinal-engagement-measurement.html" onclick="toggleFavorite(this, '2505.20676v1', 'Supervised Contrastive Learning for Ordinal Engagement Measurement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250521325v3-magictryon-harnessing-diffusion-transformer-for-garment-preserving-v.html">MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on</a></td>
  <td>æå‡ºMagicTryOnä»¥è§£å†³è§†é¢‘è™šæ‹Ÿè¯•ç©¿ä¸­çš„æœè£…ä¿ç•™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21325v3" data-paper-url="./papers/250521325v3-magictryon-harnessing-diffusion-transformer-for-garment-preserving-v.html" onclick="toggleFavorite(this, '2505.21325v3', 'MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250521061v1-lpoi-listwise-preference-optimization-for-vision-language-models.html">LPOI: Listwise Preference Optimization for Vision Language Models</a></td>
  <td>æå‡ºLPOIä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">RLHF</span> <span class="paper-tag">DPO</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21061v1" data-paper-url="./papers/250521061v1-lpoi-listwise-preference-optimization-for-vision-language-models.html" onclick="toggleFavorite(this, '2505.21061v1', 'LPOI: Listwise Preference Optimization for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>43</td>
  <td><a href="./papers/250520729v1-intern-gs-vision-model-guided-sparse-view-3d-gaussian-splatting.html">Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting</a></td>
  <td>æå‡ºIntern-GSä»¥è§£å†³ç¨€ç–è§†å›¾ä¸‰ç»´é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20729v1" data-paper-url="./papers/250520729v1-intern-gs-vision-model-guided-sparse-view-3d-gaussian-splatting.html" onclick="toggleFavorite(this, '2505.20729v1', 'Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/250521079v1-uni3d-moe-scalable-multimodal-3d-scene-understanding-via-mixture-of-.html">Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts</a></td>
  <td>æå‡ºUni3D-MoEä»¥è§£å†³å¤šæ¨¡æ€3Dåœºæ™¯ç†è§£ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21079v1" data-paper-url="./papers/250521079v1-uni3d-moe-scalable-multimodal-3d-scene-understanding-via-mixture-of-.html" onclick="toggleFavorite(this, '2505.21079v1', 'Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250521502v2-generalizable-and-relightable-gaussian-splatting-for-human-novel-vie.html">Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis</a></td>
  <td>æå‡ºGRGSæ¡†æ¶ä»¥è§£å†³é«˜ä¿çœŸäººä½“æ–°è§†è§’åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21502v2" data-paper-url="./papers/250521502v2-generalizable-and-relightable-gaussian-splatting-for-human-novel-vie.html" onclick="toggleFavorite(this, '2505.21502v2', 'Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/250521238v2-3d-uir-3d-gaussian-for-underwater-3d-scene-reconstruction-via-physic.html">3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics Based Appearance-Medium Decoupling</a></td>
  <td>æå‡ºåŸºäºç‰©ç†çš„3Dé«˜æ–¯æ¨¡å‹ä»¥è§£å†³æ°´ä¸‹åœºæ™¯é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21238v2" data-paper-url="./papers/250521238v2-3d-uir-3d-gaussian-for-underwater-3d-scene-reconstruction-via-physic.html" onclick="toggleFavorite(this, '2505.21238v2', '3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics Based Appearance-Medium Decoupling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>47</td>
  <td><a href="./papers/250521377v1-empowering-vector-graphics-with-consistently-arbitrary-viewing-and-v.html">Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility</a></td>
  <td>æå‡ºDream3DVGä»¥è§£å†³æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ç”Ÿæˆä¸­çš„è§†è§’ä¸é®æŒ¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21377v1" data-paper-url="./papers/250521377v1-empowering-vector-graphics-with-consistently-arbitrary-viewing-and-v.html" onclick="toggleFavorite(this, '2505.21377v1', 'Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/250521231v3-occlusion-boundary-and-depth-mutual-enhancement-via-multi-task-learn.html">Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning</a></td>
  <td>æå‡ºMoDOTæ¡†æ¶ä»¥è§£å†³å•ç›®æ·±åº¦ä¼°è®¡ä¸é®æŒ¡è¾¹ç•Œä¼°è®¡çš„äº’è¡¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">geometric consistency</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21231v3" data-paper-url="./papers/250521231v3-occlusion-boundary-and-depth-mutual-enhancement-via-multi-task-learn.html" onclick="toggleFavorite(this, '2505.21231v3', 'Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>49</td>
  <td><a href="./papers/250521780v4-compositional-scene-understanding-through-inverse-generative-modelin.html">Compositional Scene Understanding through Inverse Generative Modeling</a></td>
  <td>é€šè¿‡é€†ç”Ÿæˆå»ºæ¨¡æå‡ºç»„åˆåœºæ™¯ç†è§£æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21780v4" data-paper-url="./papers/250521780v4-compositional-scene-understanding-through-inverse-generative-modelin.html" onclick="toggleFavorite(this, '2505.21780v4', 'Compositional Scene Understanding through Inverse Generative Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/250521258v1-plenodium-underwater-3d-scene-reconstruction-with-plenoptic-medium-r.html">Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation</a></td>
  <td>æå‡ºPlenodiumä»¥è§£å†³æ°´ä¸‹3Dåœºæ™¯é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21258v1" data-paper-url="./papers/250521258v1-plenodium-underwater-3d-scene-reconstruction-with-plenoptic-medium-r.html" onclick="toggleFavorite(this, '2505.21258v1', 'Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>51</td>
  <td><a href="./papers/250521049v1-robust-video-based-pothole-detection-and-area-estimation-for-intelli.html">Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing</a></td>
  <td>æå‡ºåŸºäºè§†é¢‘çš„å¼ºå¥å‘æ´æ£€æµ‹ä¸é¢ç§¯ä¼°è®¡æ–¹æ³•ä»¥æå‡æ™ºèƒ½è½¦è¾†å®‰å…¨æ€§</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">Depth Anything</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21049v1" data-paper-url="./papers/250521049v1-robust-video-based-pothole-detection-and-area-estimation-for-intelli.html" onclick="toggleFavorite(this, '2505.21049v1', 'Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>52</td>
  <td><a href="./papers/250520610v1-omniindoor3d-comprehensive-indoor-3d-reconstruction.html">OmniIndoor3D: Comprehensive Indoor 3D Reconstruction</a></td>
  <td>æå‡ºOmniIndoor3Dä»¥è§£å†³å®¤å†…3Dé‡å»ºç²¾åº¦ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">scene understanding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20610v1" data-paper-url="./papers/250520610v1-omniindoor3d-comprehensive-indoor-3d-reconstruction.html" onclick="toggleFavorite(this, '2505.20610v1', 'OmniIndoor3D: Comprehensive Indoor 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>53</td>
  <td><a href="./papers/250520904v2-htmnet-a-hybrid-network-with-transformer-mamba-bottleneck-multimodal.html">HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion</a></td>
  <td>æå‡ºHTMNetä»¥è§£å†³é€æ˜å’Œåå°„ç‰©ä½“æ·±åº¦è¡¥å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20904v2" data-paper-url="./papers/250520904v2-htmnet-a-hybrid-network-with-transformer-mamba-bottleneck-multimodal.html" onclick="toggleFavorite(this, '2505.20904v2', 'HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>54</td>
  <td><a href="./papers/250521649v4-right-side-up-disentangling-orientation-understanding-in-mllms-with-.html">Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks</a></td>
  <td>æå‡ºDORIåŸºå‡†ä»¥è§£å†³å¤šæ¨¡æ€ç³»ç»Ÿçš„ç‰©ä½“æ–¹å‘ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">scene reconstruction</span> <span class="paper-tag">scene understanding</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21649v4" data-paper-url="./papers/250521649v4-right-side-up-disentangling-orientation-understanding-in-mllms-with-.html" onclick="toggleFavorite(this, '2505.21649v4', 'Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>55</td>
  <td><a href="./papers/250521144v2-fastface-tuning-identity-preservation-in-distilled-diffusion-via-gui.html">FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention</a></td>
  <td>æå‡ºFastFaceæ¡†æ¶ä»¥è§£å†³æ‰©æ•£æ¨¡å‹ä¸­èº«ä»½ä¿ç•™é€‚é…å™¨çš„è®­ç»ƒæ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">distillation</span> <span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21144v2" data-paper-url="./papers/250521144v2-fastface-tuning-identity-preservation-in-distilled-diffusion-via-gui.html" onclick="toggleFavorite(this, '2505.21144v2', 'FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>56</td>
  <td><a href="./papers/250520981v2-refav-towards-planning-centric-scenario-mining.html">RefAV: Towards Planning-Centric Scenario Mining</a></td>
  <td>æå‡ºRefAVä»¥è§£å†³è‡ªåŠ¨é©¾é©¶åœºæ™¯æŒ–æ˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20981v2" data-paper-url="./papers/250520981v2-refav-towards-planning-centric-scenario-mining.html" onclick="toggleFavorite(this, '2505.20981v2', 'RefAV: Towards Planning-Centric Scenario Mining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>57</td>
  <td><a href="./papers/250520914v1-geometry-editable-and-appearance-preserving-object-compositon.html">Geometry-Editable and Appearance-Preserving Object Compositon</a></td>
  <td>æå‡ºDGADæ¨¡å‹ä»¥è§£å†³å¯¹è±¡åˆæˆä¸­çš„å‡ ä½•ç¼–è¾‘ä¸å¤–è§‚ä¿ç•™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20914v1" data-paper-url="./papers/250520914v1-geometry-editable-and-appearance-preserving-object-compositon.html" onclick="toggleFavorite(this, '2505.20914v1', 'Geometry-Editable and Appearance-Preserving Object Compositon')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>58</td>
  <td><a href="./papers/250521500v2-viewspatial-bench-evaluating-multi-perspective-spatial-localization-.html">ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models</a></td>
  <td>æå‡ºViewSpatial-Benchä»¥è§£å†³å¤šè§†è§’ç©ºé—´å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">spatial relationship</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21500v2" data-paper-url="./papers/250521500v2-viewspatial-bench-evaluating-multi-perspective-spatial-localization-.html" onclick="toggleFavorite(this, '2505.21500v2', 'ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>59</td>
  <td><a href="./papers/250520644v1-hcqa-15-ego4d-egoschema-challenge-2025.html">HCQA-1.5 @ Ego4D EgoSchema Challenge 2025</a></td>
  <td>æå‡ºHCQAæ¡†æ¶æ‰©å±•ä»¥æå‡è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘é—®ç­”çš„å¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">Ego4D</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20644v1" data-paper-url="./papers/250520644v1-hcqa-15-ego4d-egoschema-challenge-2025.html" onclick="toggleFavorite(this, '2505.20644v1', 'HCQA-1.5 @ Ego4D EgoSchema Challenge 2025')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>60</td>
  <td><a href="./papers/250521795v2-sansa-unleashing-the-hidden-semantics-in-sam2-for-few-shot-segmentat.html">SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</a></td>
  <td>æå‡ºSANSAä»¥è§£å†³å°‘æ ·æœ¬åˆ†å‰²ä¸­çš„è¯­ä¹‰ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21795v2" data-paper-url="./papers/250521795v2-sansa-unleashing-the-hidden-semantics-in-sam2-for-few-shot-segmentat.html" onclick="toggleFavorite(this, '2505.21795v2', 'SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>61</td>
  <td><a href="./papers/250520861v1-exploring-timeline-control-for-facial-motion-generation.html">Exploring Timeline Control for Facial Motion Generation</a></td>
  <td>æå‡ºæ—¶é—´çº¿æ§åˆ¶ä¿¡å·ä»¥æå‡é¢éƒ¨åŠ¨ä½œç”Ÿæˆç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20861v1" data-paper-url="./papers/250520861v1-exploring-timeline-control-for-facial-motion-generation.html" onclick="toggleFavorite(this, '2505.20861v1', 'Exploring Timeline Control for Facial Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>62</td>
  <td><a href="./papers/250521179v3-normalized-attention-guidance-universal-negative-guidance-for-diffus.html">Normalized Attention Guidance: Universal Negative Guidance for Diffusion Models</a></td>
  <td>æå‡ºå½’ä¸€åŒ–æ³¨æ„åŠ›å¼•å¯¼ä»¥è§£å†³æ‰©æ•£æ¨¡å‹ä¸­çš„è´Ÿå¼•å¯¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21179v3" data-paper-url="./papers/250521179v3-normalized-attention-guidance-universal-negative-guidance-for-diffus.html" onclick="toggleFavorite(this, '2505.21179v3', 'Normalized Attention Guidance: Universal Negative Guidance for Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>63</td>
  <td><a href="./papers/250521724v2-omniresponse-online-multimodal-conversational-response-generation-in.html">OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions</a></td>
  <td>æå‡ºOmniResponseä»¥è§£å†³å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dyadic interaction</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21724v2" data-paper-url="./papers/250521724v2-omniresponse-online-multimodal-conversational-response-generation-in.html" onclick="toggleFavorite(this, '2505.21724v2', 'OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>64</td>
  <td><a href="./papers/250521357v2-agrifm-a-multi-source-temporal-remote-sensing-foundation-model-for-c.html">AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping</a></td>
  <td>æå‡ºAgriFMä»¥è§£å†³å†œä¸šä½œç‰©æ˜ å°„ä¸­çš„æ—¶ç©ºç‰¹å¾æå–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21357v2" data-paper-url="./papers/250521357v2-agrifm-a-multi-source-temporal-remote-sensing-foundation-model-for-c.html" onclick="toggleFavorite(this, '2505.21357v2', 'AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>65</td>
  <td><a href="./papers/250520858v1-proba-probabilistic-bundle-adjustment-with-the-bhattacharyya-coeffic.html">ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient</a></td>
  <td>æå‡ºProBAä»¥è§£å†³ä¼ ç»ŸæŸè°ƒæ•´æ–¹æ³•çš„åˆå§‹åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20858v1" data-paper-url="./papers/250520858v1-proba-probabilistic-bundle-adjustment-with-the-bhattacharyya-coeffic.html" onclick="toggleFavorite(this, '2505.20858v1', 'ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)