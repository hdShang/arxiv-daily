---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-02
---

# cs.CVï¼ˆ2025-05-02ï¼‰

ğŸ“Š å…± **15** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250501571v6-painformer-a-vision-foundation-model-for-automatic-pain-assessment.html">PainFormer: a Vision Foundation Model for Automatic Pain Assessment</a></td>
  <td>æå‡ºPainFormerä»¥è§£å†³è‡ªåŠ¨ç–¼ç—›è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01571v6" data-paper-url="./papers/250501571v6-painformer-a-vision-foundation-model-for-automatic-pain-assessment.html" onclick="toggleFavorite(this, '2505.01571v6', 'PainFormer: a Vision Foundation Model for Automatic Pain Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250501064v1-efficient-vocabulary-free-fine-grained-visual-recognition-in-the-age.html">Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs</a></td>
  <td>æå‡ºNeaRæ–¹æ³•ä»¥è§£å†³æ— è¯æ±‡ç»†ç²’åº¦è§†è§‰è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01064v1" data-paper-url="./papers/250501064v1-efficient-vocabulary-free-fine-grained-visual-recognition-in-the-age.html" onclick="toggleFavorite(this, '2505.01064v1', 'Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250501615v1-multimodal-and-multiview-deep-fusion-for-autonomous-marine-navigatio.html">Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation</a></td>
  <td>æå‡ºè·¨æ³¨æ„åŠ›å˜æ¢å™¨æ–¹æ³•ä»¥è§£å†³è‡ªä¸»æµ·æ´‹å¯¼èˆªä¸­çš„å¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01615v1" data-paper-url="./papers/250501615v1-multimodal-and-multiview-deep-fusion-for-autonomous-marine-navigatio.html" onclick="toggleFavorite(this, '2505.01615v1', 'Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250501578v1-grounding-task-assistance-with-multimodal-cues-from-a-single-demonst.html">Grounding Task Assistance with Multimodal Cues from a Single Demonstration</a></td>
  <td>æå‡ºMICAæ¡†æ¶ä»¥è§£å†³ä»»åŠ¡è¾…åŠ©ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ç¼ºå¤±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01578v1" data-paper-url="./papers/250501578v1-grounding-task-assistance-with-multimodal-cues-from-a-single-demonst.html" onclick="toggleFavorite(this, '2505.01578v1', 'Grounding Task Assistance with Multimodal Cues from a Single Demonstration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250501390v1-multimodal-doctor-in-the-loop-a-clinically-guided-explainable-framew.html">Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer</a></td>
  <td>æå‡ºå¤šæ¨¡æ€åŒ»ç”Ÿå‚ä¸æ¡†æ¶ä»¥é¢„æµ‹éå°ç»†èƒè‚ºç™Œçš„ç—…ç†ååº”</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01390v1" data-paper-url="./papers/250501390v1-multimodal-doctor-in-the-loop-a-clinically-guided-explainable-framew.html" onclick="toggleFavorite(this, '2505.01390v1', 'Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250501239v1-can-foundation-models-really-segment-tumors-a-benchmarking-odyssey-i.html">Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging</a></td>
  <td>åŸºäºåŸºç¡€æ¨¡å‹çš„è‚ºè‚¿ç˜¤åˆ†å‰²æ–¹æ³•æ˜¾è‘—æå‡å‡†ç¡®æ€§ä¸æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01239v1" data-paper-url="./papers/250501239v1-can-foundation-models-really-segment-tumors-a-benchmarking-odyssey-i.html" onclick="toggleFavorite(this, '2505.01239v1', 'Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250501091v1-any-to-any-vision-language-model-for-multimodal-x-ray-imaging-and-ra.html">Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation</a></td>
  <td>æå‡ºå¤šæ¨¡æ€Xå…‰å½±åƒä¸æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ä»¥è§£å†³åŒ»ç–—æ•°æ®ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01091v1" data-paper-url="./papers/250501091v1-any-to-any-vision-language-model-for-multimodal-x-ray-imaging-and-ra.html" onclick="toggleFavorite(this, '2505.01091v1', 'Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250501558v1-a-sensor-agnostic-domain-generalization-framework-for-leveraging-geo.html">A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning</a></td>
  <td>æå‡ºä¸€ç§ä¼ æ„Ÿå™¨æ— å…³çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ä»¥æå‡é¥æ„Ÿè¯­ä¹‰åˆ†å‰²æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01558v1" data-paper-url="./papers/250501558v1-a-sensor-agnostic-domain-generalization-framework-for-leveraging-geo.html" onclick="toggleFavorite(this, '2505.01558v1', 'A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250501263v2-flowdubber-movie-dubbing-with-llm-based-semantic-aware-learning-and-.html">FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing</a></td>
  <td>æå‡ºFlowDubberä»¥è§£å†³ç”µå½±é…éŸ³ä¸­çš„éŸ³é¢‘è´¨é‡ä¸å£å‹åŒæ­¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01263v2" data-paper-url="./papers/250501263v2-flowdubber-movie-dubbing-with-llm-based-semantic-aware-learning-and-.html" onclick="toggleFavorite(this, '2505.01263v2', 'FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250501237v2-cav-mae-sync-improving-contrastive-audio-visual-mask-autoencoders-vi.html">CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment</a></td>
  <td>æå‡ºCAV-MAE Syncä»¥è§£å†³éŸ³è§†é¢‘æ¨¡æ€å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01237v2" data-paper-url="./papers/250501237v2-cav-mae-sync-improving-contrastive-audio-visual-mask-autoencoders-vi.html" onclick="toggleFavorite(this, '2505.01237v2', 'CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250501235v1-compensating-spatiotemporally-inconsistent-observations-for-online-d.html">Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting</a></td>
  <td>æå‡ºä¸€ç§æ–¹æ³•ä»¥è§£å†³åœ¨çº¿åŠ¨æ€3Dé‡å»ºä¸­çš„æ—¶ç©ºä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01235v1" data-paper-url="./papers/250501235v1-compensating-spatiotemporally-inconsistent-observations-for-online-d.html" onclick="toggleFavorite(this, '2505.01235v1', 'Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250501548v2-learning-flow-guided-registration-for-rgb-event-semantic-segmentatio.html">Learning Flow-Guided Registration for RGB-Event Semantic Segmentation</a></td>
  <td>æå‡ºBRENetä»¥è§£å†³RGB-Eventè¯­ä¹‰åˆ†å‰²ä¸­çš„é…å‡†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01548v2" data-paper-url="./papers/250501548v2-learning-flow-guided-registration-for-rgb-event-semantic-segmentatio.html" onclick="toggleFavorite(this, '2505.01548v2', 'Learning Flow-Guided Registration for RGB-Event Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250501182v2-tstmotion-training-free-scene-aware-text-to-motion-generation.html">TSTMotion: Training-free Scene-aware Text-to-motion Generation</a></td>
  <td>æå‡ºTSTMotionä»¥è§£å†³åœºæ™¯æ„ŸçŸ¥æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">text-driven motion</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01182v2" data-paper-url="./papers/250501182v2-tstmotion-training-free-scene-aware-text-to-motion-generation.html" onclick="toggleFavorite(this, '2505.01182v2', 'TSTMotion: Training-free Scene-aware Text-to-motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250501322v4-freeinsert-disentangled-text-guided-object-insertion-in-3d-gaussian-.html">FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors</a></td>
  <td>æå‡ºFreeInsertä»¥è§£å†³æ— ç©ºé—´å…ˆéªŒçš„3Dåœºæ™¯å¯¹è±¡æ’å…¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01322v4" data-paper-url="./papers/250501322v4-freeinsert-disentangled-text-guided-object-insertion-in-3d-gaussian-.html" onclick="toggleFavorite(this, '2505.01322v4', 'FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250501406v2-vidstamp-a-temporally-aware-watermark-for-ownership-and-integrity-in.html">VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</a></td>
  <td>æå‡ºVidStampä»¥è§£å†³è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„æ°´å°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01406v2" data-paper-url="./papers/250501406v2-vidstamp-a-temporally-aware-watermark-for-ownership-and-integrity-in.html" onclick="toggleFavorite(this, '2505.01406v2', 'VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)