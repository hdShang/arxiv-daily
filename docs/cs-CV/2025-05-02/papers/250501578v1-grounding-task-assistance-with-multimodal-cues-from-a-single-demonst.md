---
layout: default
title: Grounding Task Assistance with Multimodal Cues from a Single Demonstration
---

# Grounding Task Assistance with Multimodal Cues from a Single Demonstration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01578" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01578v1</a>
  <a href="https://arxiv.org/pdf/2505.01578.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01578v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01578v1', 'Grounding Task Assistance with Multimodal Cues from a Single Demonstration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gabriel Sarch, Balasaravanan Thoravi Kumaravel, Sahithya Ravi, Vibhav Vineet, Andrew D. Wilson

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMICAæ¡†æ¶ä»¥è§£å†³ä»»åŠ¡è¾…åŠ©ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ç¼ºå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `ä»»åŠ¡è¾…åŠ©` `è§†è§‰è¯­è¨€æ¨¡å‹` `çœ¼åŠ¨è¿½è¸ª` `è¯­éŸ³è¯†åˆ«` `ä¸Šä¸‹æ–‡ç†è§£` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RGBè§†é¢‘ç¤ºèŒƒæ— æ³•æœ‰æ•ˆæ•æ‰ç»†ç²’åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé™åˆ¶äº†ä»»åŠ¡è¾…åŠ©çš„æ•ˆæœã€‚
2. MICAæ¡†æ¶é€šè¿‡æ•´åˆçœ¼åŠ¨å’Œè¯­éŸ³çº¿ç´¢ï¼Œåˆ†å‰²ç¤ºèŒƒä¸ºå­ä»»åŠ¡ï¼Œæå–å…³é”®å¸§å’Œå­—å¹•ï¼Œå¢å¼ºä¸Šä¸‹æ–‡ç†è§£ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€çº¿ç´¢åœ¨ä»»åŠ¡å¤åˆ¶ä¸­çš„å“åº”è´¨é‡æ˜¾è‘—æå‡ï¼Œçœ¼åŠ¨çº¿ç´¢çš„è¡¨ç°æ¥è¿‘è¯­éŸ³çº¿ç´¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»çš„ç¤ºèŒƒé€šå¸¸æ˜¯å­¦ä¹ ç›¸åŒä»»åŠ¡çš„å…³é”®å‚è€ƒã€‚ç„¶è€Œï¼ŒRGBè§†é¢‘ä½œä¸ºä¸»è¦çš„ç¤ºèŒƒåª’ä»‹ï¼Œå¸¸å¸¸æ— æ³•æ•æ‰åˆ°ç»†ç²’åº¦çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œå¦‚æ„å›¾ã€å®‰å…¨å…³é”®çš„ç¯å¢ƒå› ç´ å’Œäººç±»è¡Œä¸ºä¸­çš„å¾®å¦™åå¥½ã€‚è¿™ä¸€æ„ŸçŸ¥å·®è·é™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹åŠ¨ä½œå‘ç”ŸåŸå› çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MICAï¼ˆå¤šæ¨¡æ€äº’åŠ¨ä¸Šä¸‹æ–‡è¾…åŠ©ï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆçœ¼åŠ¨å’Œè¯­éŸ³çº¿ç´¢æ¥æ”¹å–„ä»»åŠ¡è¾…åŠ©çš„å¯¹è¯ä»£ç†ã€‚MICAå°†ç¤ºèŒƒåˆ†å‰²ä¸ºæœ‰æ„ä¹‰çš„å­ä»»åŠ¡ï¼Œå¹¶æå–å…³é”®å¸§å’Œæ•æ‰ç»†ç²’åº¦æ„å›¾çš„å­—å¹•ï¼Œä»è€Œå¢å¼ºè§†è§‰é—®ç­”çš„ä¸Šä¸‹æ–‡åŸºç¡€ã€‚å®éªŒè¯æ˜ï¼Œå¤šæ¨¡æ€çº¿ç´¢æ˜¾è‘—æé«˜äº†å“åº”è´¨é‡ï¼Œçœ¼åŠ¨çº¿ç´¢å•ç‹¬å°±è¾¾åˆ°äº†93%çš„è¯­éŸ³æ€§èƒ½ï¼ŒäºŒè€…ç»“åˆåˆ™å–å¾—äº†æœ€é«˜å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰ä»»åŠ¡è¾…åŠ©ç³»ç»Ÿä¸­å› ç¼ºä¹ç»†ç²’åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯è€Œå¯¼è‡´çš„æ€§èƒ½ä¸è¶³é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨RGBè§†é¢‘ä½œä¸ºç¤ºèŒƒåª’ä»‹æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºMICAæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆçœ¼åŠ¨å’Œè¯­éŸ³çº¿ç´¢ï¼Œå¢å¼ºå¯¹ä»»åŠ¡ç¤ºèŒƒçš„ç†è§£ï¼Œè¿›è€Œæé«˜å¯¹è¯ä»£ç†çš„ä»»åŠ¡è¾…åŠ©èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMICAæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç¤ºèŒƒåˆ†å‰²æ¨¡å—ã€å…³é”®å¸§æå–æ¨¡å—å’Œä¸Šä¸‹æ–‡ç†è§£æ¨¡å—ã€‚ç¤ºèŒƒåˆ†å‰²æ¨¡å—å°†ä»»åŠ¡ç¤ºèŒƒåˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œå…³é”®å¸§æå–æ¨¡å—ä»ä¸­æå–é‡è¦å¸§å’Œå­—å¹•ï¼Œè€Œä¸Šä¸‹æ–‡ç†è§£æ¨¡å—åˆ™ç»“åˆå¤šæ¨¡æ€çº¿ç´¢è¿›è¡Œåˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šMICAçš„åˆ›æ–°åœ¨äºé¦–æ¬¡å°†çœ¼åŠ¨å’Œè¯­éŸ³çº¿ç´¢ç»“åˆç”¨äºä»»åŠ¡è¾…åŠ©ï¼Œæ˜¾è‘—æå‡äº†å¯¹ä»»åŠ¡æ„å›¾å’Œç”¨æˆ·ç‰¹å®šéœ€æ±‚çš„ç†è§£èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºå¸§çš„æ£€ç´¢æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMICAé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¤šæ¨¡æ€çº¿ç´¢çš„èåˆæ•ˆæœï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºå¯¹é‡è¦ä¿¡æ¯çš„å…³æ³¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¤šæ¨¡æ€çº¿ç´¢çš„å“åº”è´¨é‡æ˜¾è‘—é«˜äºä¼ ç»Ÿçš„åŸºäºå¸§çš„æ£€ç´¢æ–¹æ³•ã€‚çœ¼åŠ¨çº¿ç´¢å•ç‹¬è¾¾åˆ°äº†93%çš„è¯­éŸ³æ€§èƒ½ï¼Œè€ŒäºŒè€…ç»“åˆçš„å‡†ç¡®ç‡æ›´æ˜¯æœ€é«˜ï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€ä¿¡å·åœ¨å®é™…ä»»åŠ¡è¾…åŠ©ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²åŸ¹è®­ã€æœºå™¨äººä»»åŠ¡æ‰§è¡Œå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æä¾›æ›´ä¸ºç²¾å‡†çš„ä»»åŠ¡è¾…åŠ©ï¼ŒMICAæ¡†æ¶èƒ½å¤Ÿæå‡ç”¨æˆ·å­¦ä¹ æ•ˆç‡å’Œä»»åŠ¡æ‰§è¡Œçš„å®‰å…¨æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance.

