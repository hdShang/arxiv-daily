---
layout: default
title: CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment
---

# CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01237v2</a>
  <a href="https://arxiv.org/pdf/2505.01237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01237v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01237v2', 'CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Edson Araujo, Andrew Rouditchenko, Yuan Gong, Saurabhchand Bhati, Samuel Thomas, Brian Kingsbury, Leonid Karlinsky, Rogerio Feris, James R. Glass, Hilde Kuehne

**åˆ†ç±»**: cs.MM, cs.CV, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02 (æ›´æ–°: 2025-05-21)

**å¤‡æ³¨**: To be published at CVPR 2025, code available at https://github.com/edsonroteia/cav-mae-sync

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCAV-MAE Syncä»¥è§£å†³éŸ³è§†é¢‘æ¨¡æ€å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `éŸ³è§†é¢‘å­¦ä¹ ` `æ¨¡æ€å¯¹é½` `è‡ªç›‘ç£å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³è§†é¢‘å­¦ä¹ æ–¹æ³•ä¾èµ–å…¨å±€éŸ³é¢‘è¡¨ç¤ºï¼Œæœªèƒ½æœ‰æ•ˆæ•æ‰ç»†ç²’åº¦çš„æ—¶é—´å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. CAV-MAE Syncé€šè¿‡å°†éŸ³é¢‘è§†ä¸ºä¸è§†é¢‘å¸§å¯¹é½çš„æ—¶é—´åºåˆ—ï¼Œåˆ†ç¦»å¯¹æ¯”ä¸é‡å»ºç›®æ ‡ï¼Œæå‡äº†æ¨¡æ€é—´çš„å¯¹é½æ•ˆæœã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬æ£€ç´¢ã€åˆ†ç±»å’Œå®šä½ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¤æ‚æ¨¡å‹çš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼ŒéŸ³è§†é¢‘å­¦ä¹ çš„è¿›å±•åœ¨è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­å±•ç°å‡ºè‰¯å¥½æ•ˆæœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºå…¨å±€éŸ³é¢‘è¡¨ç¤ºï¼Œæœªèƒ½æ•æ‰ä¸è§†è§‰å¸§çš„ç»†ç²’åº¦æ—¶é—´å¯¹åº”å…³ç³»ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨è”åˆå­¦ä¹ é‡å»ºä¸è·¨æ¨¡æ€å¯¹é½æ—¶å¸¸é¢ä¸´ä¼˜åŒ–ç›®æ ‡å†²çªã€‚æœ¬æ–‡æå‡ºCAV-MAE Syncï¼Œä½œä¸ºåŸå§‹CAV-MAEæ¡†æ¶çš„æœ‰æ•ˆæ‰©å±•ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸‰å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡å°†éŸ³é¢‘è§†ä¸ºä¸è§†é¢‘å¸§å¯¹é½çš„æ—¶é—´åºåˆ—ï¼Œåˆ†ç¦»å¯¹æ¯”ä¸é‡å»ºç›®æ ‡ï¼Œå¹¶å¼•å…¥å¯å­¦ä¹ çš„æ³¨å†Œæ ‡è®°æ¥æ”¹å–„ç©ºé—´å®šä½ã€‚æˆ‘ä»¬åœ¨AudioSetã€VGG Soundå’ŒADE20K Soundæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬æ£€ç´¢ã€åˆ†ç±»å’Œå®šä½ä»»åŠ¡ä¸Šçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¶…è¶Šäº†æ›´å¤æ‚çš„æ¶æ„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³è§†é¢‘æ¨¡æ€å¯¹é½ä¸­çš„ç»†ç²’åº¦æ—¶é—´å¯¹åº”é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å…¨å±€éŸ³é¢‘è¡¨ç¤ºï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ä¸è§†è§‰å¸§çš„ç»†èŠ‚å…³ç³»ï¼ŒåŒæ—¶åœ¨è”åˆå­¦ä¹ é‡å»ºä¸å¯¹é½æ—¶é¢ä¸´ä¼˜åŒ–ç›®æ ‡å†²çªã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCAV-MAE Syncé€šè¿‡å°†éŸ³é¢‘è§†ä¸ºä¸è§†é¢‘å¸§å¯¹é½çš„æ—¶é—´åºåˆ—ï¼Œè§£å†³äº†æ¨¡æ€é—´çš„ç²’åº¦ä¸åŒ¹é…é—®é¢˜ã€‚é€šè¿‡åˆ†ç¦»å¯¹æ¯”å’Œé‡å»ºç›®æ ‡ï¼Œé¿å…äº†ä¼˜åŒ–ç›®æ ‡çš„å†²çªï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šéŸ³é¢‘æ—¶é—´åºåˆ—å¤„ç†æ¨¡å—ã€å¯¹æ¯”å­¦ä¹ æ¨¡å—å’Œé‡å»ºæ¨¡å—ã€‚éŸ³é¢‘æ¨¡å—è´Ÿè´£å°†éŸ³é¢‘ä¿¡å·è½¬åŒ–ä¸ºä¸è§†é¢‘å¸§å¯¹é½çš„è¡¨ç¤ºï¼Œå¯¹æ¯”å­¦ä¹ æ¨¡å—ç”¨äºä¼˜åŒ–æ¨¡æ€é—´çš„å¯¹é½ï¼Œè€Œé‡å»ºæ¨¡å—åˆ™ä¸“æ³¨äºé‡å»ºè¾“å…¥ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å¯å­¦ä¹ çš„æ³¨å†Œæ ‡è®°ï¼Œå‡å°‘äº†è¯­ä¹‰è´Ÿæ‹…ï¼Œä½¿å¾—è¡¥ä¸æ ‡è®°çš„è¡¨ç¤ºæ›´åŠ æœ‰æ•ˆã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨ç©ºé—´å®šä½ä¸Šè¡¨ç°æ›´ä½³ï¼Œæ˜¾è‘—æå‡äº†å¯¹é½æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæ¨¡å‹ä½¿ç”¨äº†ä¸“é—¨çš„å…¨å±€æ ‡è®°æ¥åˆ†ç¦»å¯¹æ¯”å’Œé‡å»ºç›®æ ‡ï¼ŒæŸå¤±å‡½æ•°åˆ™ç»“åˆäº†å¯¹æ¯”æŸå¤±ä¸é‡å»ºæŸå¤±ï¼Œä»¥ç¡®ä¿ä¸¤è€…çš„ä¼˜åŒ–ä¸å†²çªã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†æ·±åº¦å­¦ä¹ æ¡†æ¶ä»¥æ”¯æŒå¤æ‚çš„éŸ³è§†é¢‘ç‰¹å¾æå–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCAV-MAE Syncåœ¨é›¶æ ·æœ¬æ£€ç´¢ã€åˆ†ç±»å’Œå®šä½ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æ›´å¤æ‚çš„æ¨¡å‹æ¶æ„ã€‚ä¾‹å¦‚ï¼Œåœ¨AudioSetæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨éŸ³è§†é¢‘å­¦ä¹ é¢†åŸŸçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€å†…å®¹æ£€ç´¢ã€è§†é¢‘ç†è§£ã€è‡ªåŠ¨å­—å¹•ç”Ÿæˆç­‰ã€‚é€šè¿‡æå‡éŸ³è§†é¢‘æ¨¡æ€çš„å¯¹é½æ•ˆæœï¼ŒCAV-MAE Syncå¯ä»¥åœ¨å¤šç§å®é™…åœºæ™¯ä¸­æé«˜ä¿¡æ¯æå–çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in audio-visual learning have shown promising results in learning representations across modalities. However, most approaches rely on global audio representations that fail to capture fine-grained temporal correspondences with visual frames. Additionally, existing methods often struggle with conflicting optimization objectives when trying to jointly learn reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync as a simple yet effective extension of the original CAV-MAE framework for self-supervised audio-visual learning. We address three key challenges: First, we tackle the granularity mismatch between modalities by treating audio as a temporal sequence aligned with video frames, rather than using global representations. Second, we resolve conflicting optimization goals by separating contrastive and reconstruction objectives through dedicated global tokens. Third, we improve spatial localization by introducing learnable register tokens that reduce semantic load on patch tokens. We evaluate the proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on zero-shot retrieval, classification and localization tasks demonstrating state-of-the-art performance and outperforming more complex architectures.

