---
layout: default
title: "TSTMotion: Training-free Scene-aware Text-to-motion Generation"
---

# TSTMotion: Training-free Scene-aware Text-to-motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01182" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.01182v2</a>
  <a href="https://arxiv.org/pdf/2505.01182.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01182v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01182v2', 'TSTMotion: Training-free Scene-aware Text-to-motion Generation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Ziyan Guo, Haoxuan Qu, Hossein Rahmani, Dewen Soh, Ping Hu, Qiuhong Ke, Jun Liu

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-02 (Êõ¥Êñ∞: 2025-05-05)

**Â§áÊ≥®**: Accepted by ICME2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TSTMotion‰ª•Ëß£ÂÜ≥Âú∫ÊôØÊÑüÁü•ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)** **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàê` `Âú∫ÊôØÊÑüÁü•` `Êó†ËÆ≠ÁªÉÊ°ÜÊû∂` `3DÂú∫ÊôØ` `Âä®‰ΩúÊåáÂØº` `ËôöÊãüÁé∞ÂÆû` `‰∫∫Êú∫‰∫§‰∫í`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂú∫ÊôØÊÑüÁü•ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊñπÊ≥ï‰æùËµñ‰∫éÂ§ßÈáèÁúüÂÆûÁöÑÂä®‰ΩúÂ∫èÂàóÔºåÊàêÊú¨È´ò‰∏îÈöæ‰ª•Ëé∑Âèñ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊ°ÜÊû∂TSTMotionÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÁ©∫ÁôΩËÉåÊôØÂä®‰ΩúÁîüÊàêÂô®ÔºåËµã‰∫àÂÖ∂Âú∫ÊôØÊÑüÁü•ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTSTMotionÂú®ÁîüÊàêÂú∫ÊôØÊÑüÁü•ÁöÑÊñáÊú¨È©±Âä®Âä®‰ΩúÂ∫èÂàóÊñπÈù¢ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊïàÊûúÂíåÂπøÊ≥õÁöÑÈÄÇÁî®ÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊúÄËøëÂºïËµ∑‰∫ÜÂπøÊ≥õÁöÑÁ†îÁ©∂ÂÖ¥Ë∂£Ôºå‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÁîüÊàêÁ©∫ÁôΩËÉåÊôØ‰∏ãÁöÑ‰∫∫Á±ªÂä®‰ΩúÂ∫èÂàó„ÄÇÁÑ∂ËÄåÔºå‰∫∫Á±ªÂä®‰ΩúÈÄöÂ∏∏ÂèëÁîüÂú®Â§öÊ†∑ÁöÑ3DÂú∫ÊôØ‰∏≠ÔºåËøô‰øÉ‰ΩøÂØπÂú∫ÊôØÊÑüÁü•ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊñπÊ≥ïÁöÑÊé¢Á¥¢„ÄÇÁé∞ÊúâÁöÑÂú∫ÊôØÊÑüÁü•ÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫éÂ§ßËßÑÊ®°ÁöÑÁúüÂÆûÂä®‰ΩúÂ∫èÂàóÔºåËøôÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Èù¢‰∏¥È´òÊòÇÁöÑÊàêÊú¨„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÊåëÊàòÔºåÊú¨ÊñáÈ¶ñÊ¨°ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÂú∫ÊôØÊÑüÁü•ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊ°ÜÊû∂TSTMotionÔºåËÉΩÂ§üÊúâÊïàËµã‰∫àÈ¢ÑËÆ≠ÁªÉÁöÑÁ©∫ÁôΩËÉåÊôØÂä®‰ΩúÁîüÊàêÂô®Âú∫ÊôØÊÑüÁü•ËÉΩÂäõ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂü∫‰∫éÁªôÂÆöÁöÑ3DÂú∫ÊôØÂíåÊñáÊú¨ÊèèËø∞ÔºåÊàë‰ª¨ÁªìÂêàÂü∫Á°ÄÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÅÈ¢ÑÊµãÂíåÈ™åËØÅÂú∫ÊôØÊÑüÁü•ÁöÑÂä®‰ΩúÊåáÂØº„ÄÇÁÑ∂ÂêéÔºåÂ∞ÜÂä®‰ΩúÊåáÂØºÊï¥ÂêàÂà∞Á©∫ÁôΩËÉåÊôØÂä®‰ΩúÁîüÊàêÂô®‰∏≠ÔºåÁªèËøá‰∏§È°π‰øÆÊîπÔºåÁîüÊàêÂú∫ÊôØÊÑüÁü•ÁöÑÊñáÊú¨È©±Âä®Âä®‰ΩúÂ∫èÂàó„ÄÇÂ§ßÈáèÂÆûÈ™åË°®Êòé‰∫ÜÊàë‰ª¨ÊèêÂá∫Ê°ÜÊû∂ÁöÑÊúâÊïàÊÄßÂíåÈÄöÁî®ÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂú∫ÊôØÊÑüÁü•ÊñáÊú¨Âà∞Âä®‰ΩúÁîüÊàêÊñπÊ≥ïÂØπÂ§ßËßÑÊ®°ÁúüÂÆûÂä®‰ΩúÂ∫èÂàóÁöÑ‰æùËµñÈóÆÈ¢òÔºåËøô‰ΩøÂæóÂÆûÈôÖÂ∫îÁî®Èù¢‰∏¥È´òÊàêÊú¨ÂíåÈöæÂ∫¶„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊèêÂá∫‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊ°ÜÊû∂TSTMotionÔºåÈÄöËøáÁªìÂêàÂü∫Á°ÄÊ®°ÂûãÔºåÂà©Áî®Â∑≤ÊúâÁöÑÁ©∫ÁôΩËÉåÊôØÂä®‰ΩúÁîüÊàêÂô®Êù•ÂÆûÁé∞Âú∫ÊôØÊÑüÁü•ËÉΩÂäõÁöÑÂ¢ûÂº∫„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÔºåÂü∫‰∫éÁªôÂÆöÁöÑ3DÂú∫ÊôØÂíåÊñáÊú¨ÊèèËø∞ËøõË°åÊé®ÁêÜÔºõÂÖ∂Ê¨°ÔºåÁîüÊàêÂú∫ÊôØÊÑüÁü•ÁöÑÂä®‰ΩúÊåáÂØºÔºõÊúÄÂêéÔºåÂ∞ÜÂä®‰ΩúÊåáÂØºÊï¥ÂêàÂà∞Á©∫ÁôΩËÉåÊôØÂä®‰ΩúÁîüÊàêÂô®‰∏≠ÔºåÁîüÊàêÊúÄÁªàÁöÑÂä®‰ΩúÂ∫èÂàó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫Êó†ËÆ≠ÁªÉÁöÑÂú∫ÊôØÊÑüÁü•ÁîüÊàêÊ°ÜÊû∂ÔºåÈÅøÂÖç‰∫ÜÂØπÁúüÂÆûÂä®‰ΩúÂ∫èÂàóÁöÑ‰æùËµñÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÂΩ¢Êàê‰∫ÜÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨ÂØπÂü∫Á°ÄÊ®°ÂûãÁöÑÈÄâÊã©„ÄÅÂä®‰ΩúÊåáÂØºÁöÑÁîüÊàêÁ≠ñÁï•Ôºå‰ª•ÂèäÂú®Á©∫ÁôΩËÉåÊôØÁîüÊàêÂô®‰∏≠ËøõË°åÁöÑ‰∏§È°πÈáçË¶Å‰øÆÊîπÔºå‰ª•Á°Æ‰øùÁîüÊàêÁöÑÂä®‰ΩúÂ∫èÂàóËÉΩÂ§üÊúâÊïàÂèçÊò†Âú∫ÊôØ‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåTSTMotionÂú®ÁîüÊàêÂú∫ÊôØÊÑüÁü•ÁöÑÊñáÊú¨È©±Âä®Âä®‰ΩúÂ∫èÂàóÊñπÈù¢ÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÂú®ÂáÜÁ°ÆÊÄßÂíåËá™ÁÑ∂ÊÄß‰∏äÊúâÊòæËëóÊèêÂçáÔºåÂÖ∑‰ΩìÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÈÄöÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèëÂíåÂä®ÁîªÂà∂‰ΩúÁ≠âÔºåËÉΩÂ§ü‰∏∫Ëøô‰∫õÈ¢ÜÂüüÊèê‰æõÊõ¥Âä†Ëá™ÁÑ∂ÂíåÁúüÂÆûÁöÑ‰∫∫Áâ©Âä®‰ΩúÁîüÊàê„ÄÇÊú™Êù•ÔºåÈöèÁùÄÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÔºåTSTMotionÂèØËÉΩ‰ºöÂú®‰∫∫Êú∫‰∫§‰∫íÂíåÊú∫Âô®‰∫∫ÊéßÂà∂Á≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \textbf{T}raining-free \textbf{S}cene-aware \textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \href{https://tstmotion.github.io/}{Project Page}.

