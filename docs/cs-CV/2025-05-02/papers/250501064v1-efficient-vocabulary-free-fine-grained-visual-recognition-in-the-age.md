---
layout: default
title: Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs
---

# Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01064" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01064v1</a>
  <a href="https://arxiv.org/pdf/2505.01064.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01064v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01064v1', 'Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

**å¤‡æ³¨**: preprint; earlier version accepted at NeurIPS 2024 Workshop on Adaptive Foundation Models

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNeaRæ–¹æ³•ä»¥è§£å†³æ— è¯æ±‡ç»†ç²’åº¦è§†è§‰è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»†ç²’åº¦è§†è§‰è¯†åˆ«` `æ— è¯æ±‡å­¦ä¹ ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¼±ç›‘ç£å­¦ä¹ ` `CLIPæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç»†ç²’åº¦è§†è§‰è¯†åˆ«é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç¼ºä¹è¶³å¤Ÿçš„æ ‡æ³¨æ•°æ®ï¼Œå°¤å…¶åœ¨åŒ»ç–—æˆåƒç­‰é¢†åŸŸã€‚
2. æå‡ºçš„NeaRæ–¹æ³•é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ ‡ç­¾ï¼Œæ„å»ºå¼±ç›‘ç£æ•°æ®é›†ä»¥è§£å†³æ— è¯æ±‡FGVRé—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒNeaRåœ¨VF-FGVRä»»åŠ¡ä¸­å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¯†åˆ«æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»†ç²’åº¦è§†è§‰è¯†åˆ«ï¼ˆFGVRï¼‰æ¶‰åŠåŒºåˆ†è§†è§‰ä¸Šç›¸ä¼¼çš„ç±»åˆ«ï¼Œè¿™ä¸€ä»»åŠ¡å› ç±»é—´å¾®å°å·®å¼‚å’Œç¼ºä¹å¤§è§„æ¨¡ä¸“å®¶æ ‡æ³¨æ•°æ®é›†è€Œå˜å¾—æå…·æŒ‘æˆ˜æ€§ã€‚åœ¨åŒ»ç–—æˆåƒç­‰é¢†åŸŸï¼Œç”±äºéšç§é—®é¢˜å’Œé«˜æ˜‚çš„æ ‡æ³¨æˆæœ¬ï¼Œéš¾ä»¥è·å¾—ç»è¿‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ã€‚åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒFGVRæ¨¡å‹æ— æ³•ä¾èµ–é¢„å®šä¹‰çš„è®­ç»ƒæ ‡ç­¾ï¼Œå› æ­¤å…¶è¾“å‡ºç©ºé—´æ˜¯æ— çº¦æŸçš„ã€‚æˆ‘ä»¬å°†è¿™ä¸€ä»»åŠ¡ç§°ä¸ºæ— è¯æ±‡FGVRï¼ˆVF-FGVRï¼‰ï¼Œæ¨¡å‹å¿…é¡»åœ¨æ²¡æœ‰å…ˆå‰æ ‡ç­¾ä¿¡æ¯çš„æƒ…å†µä¸‹è¿›è¡Œé¢„æµ‹ã€‚å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨VF-FGVRä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¯¹æ¯ä¸ªæµ‹è¯•è¾“å…¥æŸ¥è¯¢è¿™äº›æ¨¡å‹åœ¨æˆæœ¬å’Œæ¨ç†æ—¶é—´ä¸Šéƒ½ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æœ€è¿‘é‚»æ ‡ç­¾ç»†åŒ–ï¼ˆNeaRï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡MLLMç”Ÿæˆçš„æ ‡ç­¾å¾®è°ƒä¸‹æ¸¸CLIPæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåˆ©ç”¨MLLMsç”Ÿæˆæ ‡ç­¾ï¼Œä»å°è§„æ¨¡æœªæ ‡æ³¨è®­ç»ƒé›†ä¸­æ„å»ºå¼±ç›‘ç£æ•°æ®é›†ï¼ŒNeaRæ—¨åœ¨å¤„ç†MLLMç”Ÿæˆæ ‡ç­¾ä¸­çš„å™ªå£°å’Œéšæœºæ€§ï¼Œå¹¶ä¸ºé«˜æ•ˆçš„VF-FGVRå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æ— è¯æ±‡ç»†ç²’åº¦è§†è§‰è¯†åˆ«ï¼ˆVF-FGVRï¼‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹éš¾ä»¥è¿›è¡Œæœ‰æ•ˆçš„åˆ†ç±»ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—æˆåƒç­‰é¢†åŸŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„NeaRæ–¹æ³•é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆæ ‡ç­¾ï¼Œä»è€Œæ„å»ºä¸€ä¸ªå¼±ç›‘ç£çš„æ•°æ®é›†ï¼Œè¿›è€Œå¾®è°ƒä¸‹æ¸¸çš„CLIPæ¨¡å‹ï¼Œä»¥åº”å¯¹æ— çº¦æŸçš„è¾“å‡ºç©ºé—´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨MLLMç”Ÿæˆæ ‡ç­¾ï¼›å…¶æ¬¡ï¼Œæ„å»ºå¼±ç›‘ç£æ•°æ®é›†ï¼›æœ€åï¼Œå¾®è°ƒCLIPæ¨¡å‹ä»¥æé«˜è¯†åˆ«ç²¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šNeaRæ–¹æ³•çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†MLLMç”Ÿæˆæ ‡ç­¾ä¸­çš„å™ªå£°å’Œéšæœºæ€§ï¼Œå»ºç«‹äº†ä¸€ç§æ–°çš„é«˜æ•ˆVF-FGVRåŸºå‡†ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒNeaRé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”æ— è¯æ±‡æ ‡ç­¾çš„ç”Ÿæˆå’Œå¤„ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNeaRæ–¹æ³•åœ¨VF-FGVRä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†è¯†åˆ«å‡†ç¡®ç‡ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æ–°çš„æœ€ä½³è¡¨ç°ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†æ—¶é—´å’Œæˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—æˆåƒã€è‡ªåŠ¨é©¾é©¶ã€å®‰é˜²ç›‘æ§ç­‰éœ€è¦é«˜ç²¾åº¦åˆ†ç±»çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆçš„æ— è¯æ±‡ç»†ç²’åº¦è§†è§‰è¯†åˆ«ï¼Œèƒ½å¤Ÿåœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.

