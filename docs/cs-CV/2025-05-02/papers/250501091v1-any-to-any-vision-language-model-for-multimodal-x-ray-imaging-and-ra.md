---
layout: default
title: Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation
---

# Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01091" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01091v1</a>
  <a href="https://arxiv.org/pdf/2505.01091.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01091v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01091v1', 'Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Daniele Molino, Francesco di Feola, Linlin Shen, Paolo Soda, Valerio Guarrasi

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

**å¤‡æ³¨**: arXiv admin note: substantial text overlap with arXiv:2501.04614

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€Xå…‰å½±åƒä¸æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ä»¥è§£å†³åŒ»ç–—æ•°æ®ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç”Ÿæˆ` `åŒ»ç–—å½±åƒ` `Xå…‰å½±åƒ` `ä¸´åºŠæŠ¥å‘Š` `ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ` `åŒ»å­¦ç ”ç©¶` `æ•°æ®ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨é¢ä¸´æ•°æ®å¤æ‚æ€§å’Œä¸´åºŠå‡†ç¡®æ€§è¦æ±‚çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸“æ³¨äºç”Ÿæˆå¤šè§†è§’èƒ¸éƒ¨Xå…‰å½±åƒåŠå…¶ä¸´åºŠæŠ¥å‘Šï¼Œæ»¡è¶³åŒ»ç–—ç‰¹å®šéœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¡†æ¶åœ¨ç”Ÿæˆæ•°æ®è´¨é‡ä¸Šè¶…è¶Šäº†åŸºçº¿ï¼Œä¸”åœ¨ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆæ¨¡å‹åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¼•å‘äº†é©å‘½ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€åº”ç”¨ä¸­ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹é€‚åº”äºåŒ»ç–—é¢†åŸŸé¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå› åŒ»ç–—æ•°æ®å¤æ‚ä¸”å¯¹ä¸´åºŠå‡†ç¡®æ€§è¦æ±‚ä¸¥æ ¼ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“ä¸ºå¤šæ¨¡æ€åŒ»ç–—æ•°æ®ç”Ÿæˆè®¾è®¡çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šè§†è§’èƒ¸éƒ¨Xå…‰å½±åƒåŠå…¶ç›¸å…³ä¸´åºŠæŠ¥å‘Šï¼Œå¼¥åˆäº†é€šç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ä¸åŒ»ç–—ä¿å¥ç‰¹æ®Šéœ€æ±‚ä¹‹é—´çš„å·®è·ã€‚åˆ©ç”¨MIMIC-CXRæ•°æ®é›†ï¼Œæ‰€ææ¡†æ¶åœ¨ç”Ÿæˆé«˜ä¿çœŸå½±åƒå’Œè¯­ä¹‰ä¸€è‡´æŠ¥å‘Šæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚å®šé‡è¯„ä¼°æ˜¾ç¤ºåœ¨FIDå’ŒBLEUåˆ†æ•°ä¸Šå–å¾—æ˜¾è‘—ç»“æœï¼Œä¸”åœ¨ä¸‹æ¸¸ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸çœŸå®æ•°æ®ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨åŒ»å­¦ç ”ç©¶å’Œè¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŒ»ç–—é¢†åŸŸä¸­å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆçš„å¤æ‚æ€§å’Œå‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ»¡è¶³åŒ»ç–—æ•°æ®çš„ç‰¹æ®Šéœ€æ±‚ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„ä¸´åºŠé€‚ç”¨æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¡†æ¶é€šè¿‡ç»“åˆå¤šè§†è§’Xå…‰å½±åƒä¸ä¸´åºŠæŠ¥å‘Šç”Ÿæˆï¼Œæ—¨åœ¨æå‡ç”Ÿæˆæ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚é€šè¿‡ä¸“é—¨è®¾è®¡çš„æ¨¡å‹æ¶æ„ï¼Œç¡®ä¿ç”Ÿæˆæ•°æ®çš„é«˜ä¿çœŸæ€§ä¸è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œç”Ÿæˆé˜¶æ®µã€‚é¦–å…ˆï¼Œåˆ©ç”¨MIMIC-CXRæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ¥ç€é€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”Ÿæˆå½±åƒï¼Œå¹¶ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ç”Ÿæˆç›¸åº”çš„ä¸´åºŠæŠ¥å‘Šã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å¤šæ¨¡æ€ç”Ÿæˆä¸åŒ»ç–—ç‰¹å®šéœ€æ±‚ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹æ¶æ„ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ•°æ®çš„è´¨é‡å’Œä¸´åºŠç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å½±åƒä¸æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å¤šè§†è§’ä¿¡æ¯å¤„ç†æ¨¡å—ï¼Œä»¥æé«˜ç”Ÿæˆå½±åƒçš„å¤šæ ·æ€§å’ŒçœŸå®æ„Ÿã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨FIDå’ŒBLEUåˆ†æ•°ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç”Ÿæˆçš„å½±åƒä¸çœŸå®æ•°æ®åœ¨ä¸‹æ¸¸ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨åŒ»ç–—æ•°æ®ç”Ÿæˆä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»å­¦å½±åƒåˆ†æã€ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿä»¥åŠåŒ»å­¦æ•™è‚²ç­‰ã€‚é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„åŒ»ç–—æ•°æ®ï¼Œèƒ½å¤Ÿä¸ºåŒ»ç”Ÿæä¾›æ›´å¥½çš„è¾…åŠ©å·¥å…·ï¼Œæå‡è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨åŒ»ç–—äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generative models have revolutionized Artificial Intelligence (AI), particularly in multimodal applications. However, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent need for clinical accuracy. In this work, we introduce a framework specifically designed for multimodal medical data generation. By enabling the generation of multi-view chest X-rays and their associated clinical report, it bridges the gap between general-purpose vision-language models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR dataset, the proposed framework shows superior performance in generating high-fidelity images and semantically coherent reports. Our quantitative evaluation reveals significant results in terms of FID and BLEU scores, showcasing the quality of the generated data. Notably, our framework achieves comparable or even superior performance compared to real data on downstream disease classification tasks, underlining its potential as a tool for medical research and diagnostics. This study highlights the importance of domain-specific adaptations in enhancing the relevance and utility of generative models for clinical applications, paving the way for future advancements in synthetic multimodal medical data generation.

