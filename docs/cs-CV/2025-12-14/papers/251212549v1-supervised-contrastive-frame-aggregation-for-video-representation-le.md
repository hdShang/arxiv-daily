---
layout: default
title: Supervised Contrastive Frame Aggregation for Video Representation Learning
---

# Supervised Contrastive Frame Aggregation for Video Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.12549" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.12549v1</a>
  <a href="https://arxiv.org/pdf/2512.12549.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.12549v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.12549v1', 'Supervised Contrastive Frame Aggregation for Video Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaif Chowdhury, Mushfika Rahman, Greg Hamerly

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-14

**å¤‡æ³¨**: 12 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›‘ç£å¯¹æ¯”å¸§èšåˆæ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆè§†é¢‘è¡¨å¾å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘è¡¨å¾å­¦ä¹ ` `ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `å¸§èšåˆ` `æ—¶é—´ä¸Šä¸‹æ–‡` `å·ç§¯ç¥ç»ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘è¡¨å¾å­¦ä¹ æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æ—¶åºå…¨å±€ä¿¡æ¯ã€‚
2. æå‡ºä¸€ç§è§†é¢‘å¸§èšåˆç­–ç•¥ï¼Œå°†å¤šå¸§å›¾åƒç»„åˆæˆå•å¼ å›¾åƒï¼Œåˆ©ç”¨é¢„è®­ç»ƒCNNæå–ç‰¹å¾ã€‚
3. è®¾è®¡ç›‘ç£å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œé€šè¿‡ä¸åŒæ—¶é—´é‡‡æ ·æ„å»ºæ­£æ ·æœ¬ï¼Œæå‡åˆ†ç±»ç²¾åº¦å¹¶å‡å°‘è¿‡æ‹Ÿåˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè§†é¢‘è¡¨å¾å­¦ä¹ çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº†æ—¶é—´ä¸Šçš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§†é¢‘åˆ°å›¾åƒçš„èšåˆç­–ç•¥ï¼Œå°†æ¯ä¸ªè§†é¢‘çš„å¤šä¸ªå¸§åœ¨ç©ºé—´ä¸Šæ’åˆ—æˆå•ä¸ªè¾“å…¥å›¾åƒã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œéª¨å¹²ç½‘ç»œï¼ˆå¦‚ResNet50ï¼‰ï¼Œå¹¶é¿å…äº†å¤æ‚è§†é¢‘Transformeræ¨¡å‹å¸¦æ¥çš„è®¡ç®—å¼€é”€ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œç›´æ¥æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„æˆå¯¹æŠ•å½±ã€‚æ­£æ ·æœ¬å¯¹è¢«å®šä¹‰ä¸ºæ¥è‡ªå…±äº«ç›¸åŒæ ‡ç­¾çš„è§†é¢‘çš„æŠ•å½±ï¼Œè€Œæ‰€æœ‰å…¶ä»–æŠ•å½±éƒ½è¢«è§†ä¸ºè´Ÿæ ·æœ¬ã€‚é€šè¿‡ä»åŒä¸€åº•å±‚è§†é¢‘è¿›è¡Œä¸åŒçš„æ—¶é—´å¸§é‡‡æ ·ï¼Œåˆ›å»ºåŒä¸€è§†é¢‘çš„å¤šä¸ªè‡ªç„¶è§†å›¾ã€‚è¿™äº›å¸§çº§åˆ«çš„å˜åŒ–äº§ç”Ÿå…·æœ‰å…¨å±€ä¸Šä¸‹æ–‡çš„å¤šæ ·åŒ–æ­£æ ·æœ¬ï¼Œå¹¶å‡å°‘è¿‡æ‹Ÿåˆï¼Œè€Œä¸æ˜¯ä¾èµ–äºæ•°æ®å¢å¼ºã€‚åœ¨Penn Actionå’ŒHMDB51æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆ†ç±»ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶éœ€è¦çš„è®¡ç®—èµ„æºæ›´å°‘ã€‚æ‰€æå‡ºçš„ç›‘ç£å¯¹æ¯”å¸§èšåˆæ–¹æ³•åœ¨ç›‘ç£å’Œè‡ªç›‘ç£è®¾ç½®ä¸­éƒ½èƒ½å­¦ä¹ æœ‰æ•ˆçš„è§†é¢‘è¡¨å¾ï¼Œå¹¶æ”¯æŒåŸºäºè§†é¢‘çš„ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»å’Œå­—å¹•ç”Ÿæˆã€‚è¯¥æ–¹æ³•åœ¨Penn Actionä¸Šå®ç°äº†76%çš„åˆ†ç±»ç²¾åº¦ï¼Œè€ŒViViTçš„ç²¾åº¦ä¸º43%ï¼Œåœ¨HMDB51ä¸Šå®ç°äº†48%çš„ç²¾åº¦ï¼Œè€ŒViViTçš„ç²¾åº¦ä¸º37%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†é¢‘è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„æ¨¡å‹ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”åœ¨æ•æ‰è§†é¢‘ä¸­çš„å…¨å±€æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¿‡åº¦ä¾èµ–æ•°æ®å¢å¼ºæ¥ç”Ÿæˆæ­£æ ·æœ¬å¯èƒ½å¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘å¸§èšåˆä¸ºå•ä¸ªå›¾åƒï¼Œä»è€Œèƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„CNNæ¨¡å‹æå–ç‰¹å¾ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚åŒæ—¶ï¼Œé€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œå°†åŒä¸€è§†é¢‘çš„ä¸åŒæ—¶é—´é‡‡æ ·ä½œä¸ºæ­£æ ·æœ¬ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ å…·æœ‰å…¨å±€æ—¶é—´ä¸Šä¸‹æ–‡çš„è§†é¢‘è¡¨å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è§†é¢‘å¸§èšåˆï¼šå°†è§†é¢‘ä¸­çš„å¤šä¸ªå¸§æŒ‰ç…§ä¸€å®šçš„è§„åˆ™æ’åˆ—æˆä¸€å¼ å›¾åƒã€‚2) ç‰¹å¾æå–ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„CNNï¼ˆå¦‚ResNet50ï¼‰æå–èšåˆå›¾åƒçš„ç‰¹å¾ã€‚3) æŠ•å½±ï¼šå°†æå–çš„ç‰¹å¾æŠ•å½±åˆ°ä½ç»´ç©ºé—´ã€‚4) å¯¹æ¯”å­¦ä¹ ï¼šä½¿ç”¨ç›‘ç£å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œå°†åŒä¸€è§†é¢‘çš„ä¸åŒæ—¶é—´é‡‡æ ·ä½œä¸ºæ­£æ ·æœ¬ï¼Œä¸åŒè§†é¢‘çš„é‡‡æ ·ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œè®­ç»ƒæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†è§†é¢‘å¸§èšåˆç­–ç•¥ï¼Œæœ‰æ•ˆåˆ©ç”¨äº†é¢„è®­ç»ƒçš„CNNæ¨¡å‹ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚2) ä½¿ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œé€šè¿‡ä¸åŒçš„æ—¶é—´é‡‡æ ·æ„å»ºæ­£æ ·æœ¬ï¼Œé¿å…äº†è¿‡åº¦ä¾èµ–æ•°æ®å¢å¼ºï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚3) å°†æ—¶é—´å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯èå…¥åˆ°å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­ï¼Œæå‡äº†è§†é¢‘è¡¨å¾çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¸§èšåˆç­–ç•¥ï¼šé€‰æ‹©åˆé€‚çš„å¸§æ•°å’Œæ’åˆ—æ–¹å¼ï¼Œä»¥ä¿ç•™å°½å¯èƒ½å¤šçš„æ—¶é—´ä¿¡æ¯ã€‚2) å¯¹æ¯”æŸå¤±å‡½æ•°ï¼šä½¿ç”¨ç›‘ç£å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ åŒºåˆ†ä¸åŒç±»åˆ«çš„è§†é¢‘ï¼Œå¹¶ä½¿åŒä¸€è§†é¢‘çš„ä¸åŒæ—¶é—´é‡‡æ ·å°½å¯èƒ½æ¥è¿‘ã€‚3) æ—¶é—´é‡‡æ ·ç­–ç•¥ï¼šé‡‡ç”¨ä¸åŒçš„æ—¶é—´é‡‡æ ·æ–¹å¼ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æ­£æ ·æœ¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨Penn Actionæ•°æ®é›†ä¸Šå–å¾—äº†76%çš„åˆ†ç±»ç²¾åº¦ï¼Œç›¸æ¯”ViViTçš„43%æœ‰æ˜¾è‘—æå‡ã€‚åœ¨HMDB51æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†48%çš„åˆ†ç±»ç²¾åº¦ï¼Œè€ŒViViTçš„ç²¾åº¦ä¸º37%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶éœ€è¦çš„è®¡ç®—èµ„æºæ›´å°‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘åˆ†ç±»ã€è§†é¢‘æ£€ç´¢ã€è§†é¢‘å­—å¹•ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ é«˜æ•ˆçš„è§†é¢‘è¡¨å¾ï¼Œå¯ä»¥æå‡è¿™äº›ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘å†…å®¹åˆ†æç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

