---
layout: default
title: Supervised Contrastive Frame Aggregation for Video Representation Learning
---

# Supervised Contrastive Frame Aggregation for Video Representation Learning

**arXiv**: [2512.12549v1](https://arxiv.org/abs/2512.12549) | [PDF](https://arxiv.org/pdf/2512.12549.pdf)

**ä½œè€…**: Shaif Chowdhury, Mushfika Rahman, Greg Hamerly

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-14

**å¤‡æ³¨**: 12 pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç›‘ç£å¯¹æ¯”å¸§èšåˆæ–¹æ³•ï¼Œç”¨äºŽé«˜æ•ˆè§†é¢‘è¡¨å¾å­¦ä¹ ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘è¡¨å¾å­¦ä¹ ` `ç›‘ç£å¯¹æ¯”å­¦ä¹ ` `å¸§èšåˆ` `æ—¶é—´ä¸Šä¸‹æ–‡` `å·ç§¯ç¥žç»ç½‘ç»œ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†é¢‘è¡¨å¾å­¦ä¹ æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æ—¶åºå…¨å±€ä¿¡æ¯ã€‚
2. æå‡ºä¸€ç§è§†é¢‘å¸§èšåˆç­–ç•¥ï¼Œå°†å¤šå¸§å›¾åƒç»„åˆæˆå•å¼ å›¾åƒï¼Œåˆ©ç”¨é¢„è®­ç»ƒCNNæå–ç‰¹å¾ã€‚
3. è®¾è®¡ç›‘ç£å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œé€šè¿‡ä¸åŒæ—¶é—´é‡‡æ ·æž„å»ºæ­£æ ·æœ¬ï¼Œæå‡åˆ†ç±»ç²¾åº¦å¹¶å‡å°‘è¿‡æ‹Ÿåˆã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºŽè§†é¢‘è¡¨å¾å­¦ä¹ çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æž¶ï¼Œè¯¥æ¡†æž¶åˆ©ç”¨äº†æ—¶é—´ä¸Šçš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§†é¢‘åˆ°å›¾åƒçš„èšåˆç­–ç•¥ï¼Œå°†æ¯ä¸ªè§†é¢‘çš„å¤šä¸ªå¸§åœ¨ç©ºé—´ä¸ŠæŽ’åˆ—æˆå•ä¸ªè¾“å…¥å›¾åƒã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥žç»ç½‘ç»œéª¨å¹²ç½‘ç»œï¼ˆå¦‚ResNet50ï¼‰ï¼Œå¹¶é¿å…äº†å¤æ‚è§†é¢‘Transformeræ¨¡åž‹å¸¦æ¥çš„è®¡ç®—å¼€é”€ã€‚ç„¶åŽï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œç›´æŽ¥æ¯”è¾ƒæ¨¡åž‹ç”Ÿæˆçš„æˆå¯¹æŠ•å½±ã€‚æ­£æ ·æœ¬å¯¹è¢«å®šä¹‰ä¸ºæ¥è‡ªå…±äº«ç›¸åŒæ ‡ç­¾çš„è§†é¢‘çš„æŠ•å½±ï¼Œè€Œæ‰€æœ‰å…¶ä»–æŠ•å½±éƒ½è¢«è§†ä¸ºè´Ÿæ ·æœ¬ã€‚é€šè¿‡ä»ŽåŒä¸€åº•å±‚è§†é¢‘è¿›è¡Œä¸åŒçš„æ—¶é—´å¸§é‡‡æ ·ï¼Œåˆ›å»ºåŒä¸€è§†é¢‘çš„å¤šä¸ªè‡ªç„¶è§†å›¾ã€‚è¿™äº›å¸§çº§åˆ«çš„å˜åŒ–äº§ç”Ÿå…·æœ‰å…¨å±€ä¸Šä¸‹æ–‡çš„å¤šæ ·åŒ–æ­£æ ·æœ¬ï¼Œå¹¶å‡å°‘è¿‡æ‹Ÿåˆï¼Œè€Œä¸æ˜¯ä¾èµ–äºŽæ•°æ®å¢žå¼ºã€‚åœ¨Penn Actionå’ŒHMDB51æ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆ†ç±»ç²¾åº¦æ–¹é¢ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶éœ€è¦çš„è®¡ç®—èµ„æºæ›´å°‘ã€‚æ‰€æå‡ºçš„ç›‘ç£å¯¹æ¯”å¸§èšåˆæ–¹æ³•åœ¨ç›‘ç£å’Œè‡ªç›‘ç£è®¾ç½®ä¸­éƒ½èƒ½å­¦ä¹ æœ‰æ•ˆçš„è§†é¢‘è¡¨å¾ï¼Œå¹¶æ”¯æŒåŸºäºŽè§†é¢‘çš„ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»å’Œå­—å¹•ç”Ÿæˆã€‚è¯¥æ–¹æ³•åœ¨Penn Actionä¸Šå®žçŽ°äº†76%çš„åˆ†ç±»ç²¾åº¦ï¼Œè€ŒViViTçš„ç²¾åº¦ä¸º43%ï¼Œåœ¨HMDB51ä¸Šå®žçŽ°äº†48%çš„ç²¾åº¦ï¼Œè€ŒViViTçš„ç²¾åº¦ä¸º37%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„è§†é¢‘è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºŽTransformerçš„æ¨¡åž‹ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”åœ¨æ•æ‰è§†é¢‘ä¸­çš„å…¨å±€æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¿‡åº¦ä¾èµ–æ•°æ®å¢žå¼ºæ¥ç”Ÿæˆæ­£æ ·æœ¬å¯èƒ½å¯¼è‡´æ¨¡åž‹æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘å¸§èšåˆä¸ºå•ä¸ªå›¾åƒï¼Œä»Žè€Œèƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„CNNæ¨¡åž‹æå–ç‰¹å¾ï¼Œé™ä½Žè®¡ç®—æˆæœ¬ã€‚åŒæ—¶ï¼Œé€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œå°†åŒä¸€è§†é¢‘çš„ä¸åŒæ—¶é—´é‡‡æ ·ä½œä¸ºæ­£æ ·æœ¬ï¼Œé¼“åŠ±æ¨¡åž‹å­¦ä¹ å…·æœ‰å…¨å±€æ—¶é—´ä¸Šä¸‹æ–‡çš„è§†é¢‘è¡¨å¾ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è§†é¢‘å¸§èšåˆï¼šå°†è§†é¢‘ä¸­çš„å¤šä¸ªå¸§æŒ‰ç…§ä¸€å®šçš„è§„åˆ™æŽ’åˆ—æˆä¸€å¼ å›¾åƒã€‚2) ç‰¹å¾æå–ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„CNNï¼ˆå¦‚ResNet50ï¼‰æå–èšåˆå›¾åƒçš„ç‰¹å¾ã€‚3) æŠ•å½±ï¼šå°†æå–çš„ç‰¹å¾æŠ•å½±åˆ°ä½Žç»´ç©ºé—´ã€‚4) å¯¹æ¯”å­¦ä¹ ï¼šä½¿ç”¨ç›‘ç£å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œå°†åŒä¸€è§†é¢‘çš„ä¸åŒæ—¶é—´é‡‡æ ·ä½œä¸ºæ­£æ ·æœ¬ï¼Œä¸åŒè§†é¢‘çš„é‡‡æ ·ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œè®­ç»ƒæ¨¡åž‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†è§†é¢‘å¸§èšåˆç­–ç•¥ï¼Œæœ‰æ•ˆåˆ©ç”¨äº†é¢„è®­ç»ƒçš„CNNæ¨¡åž‹ï¼Œé™ä½Žäº†è®¡ç®—æˆæœ¬ã€‚2) ä½¿ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œé€šè¿‡ä¸åŒçš„æ—¶é—´é‡‡æ ·æž„å»ºæ­£æ ·æœ¬ï¼Œé¿å…äº†è¿‡åº¦ä¾èµ–æ•°æ®å¢žå¼ºï¼Œæå‡äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚3) å°†æ—¶é—´å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯èžå…¥åˆ°å¯¹æ¯”å­¦ä¹ æ¡†æž¶ä¸­ï¼Œæå‡äº†è§†é¢‘è¡¨å¾çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¸§èšåˆç­–ç•¥ï¼šé€‰æ‹©åˆé€‚çš„å¸§æ•°å’ŒæŽ’åˆ—æ–¹å¼ï¼Œä»¥ä¿ç•™å°½å¯èƒ½å¤šçš„æ—¶é—´ä¿¡æ¯ã€‚2) å¯¹æ¯”æŸå¤±å‡½æ•°ï¼šä½¿ç”¨ç›‘ç£å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡åž‹å­¦ä¹ åŒºåˆ†ä¸åŒç±»åˆ«çš„è§†é¢‘ï¼Œå¹¶ä½¿åŒä¸€è§†é¢‘çš„ä¸åŒæ—¶é—´é‡‡æ ·å°½å¯èƒ½æŽ¥è¿‘ã€‚3) æ—¶é—´é‡‡æ ·ç­–ç•¥ï¼šé‡‡ç”¨ä¸åŒçš„æ—¶é—´é‡‡æ ·æ–¹å¼ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æ­£æ ·æœ¬ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨Penn Actionæ•°æ®é›†ä¸Šå–å¾—äº†76%çš„åˆ†ç±»ç²¾åº¦ï¼Œç›¸æ¯”ViViTçš„43%æœ‰æ˜¾è‘—æå‡ã€‚åœ¨HMDB51æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†48%çš„åˆ†ç±»ç²¾åº¦ï¼Œè€ŒViViTçš„ç²¾åº¦ä¸º37%ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»ç²¾åº¦æ–¹é¢ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶éœ€è¦çš„è®¡ç®—èµ„æºæ›´å°‘ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè§†é¢‘åˆ†ç±»ã€è§†é¢‘æ£€ç´¢ã€è§†é¢‘å­—å¹•ç”Ÿæˆç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ é«˜æ•ˆçš„è§†é¢‘è¡¨å¾ï¼Œå¯ä»¥æå‡è¿™äº›ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶é™ä½Žè®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨æ™ºèƒ½ç›‘æŽ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘å†…å®¹åˆ†æžç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

