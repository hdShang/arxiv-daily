---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-05
---

# cs.CVï¼ˆ2025-12-05ï¼‰

ğŸ“Š å…± **21** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (13 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251205710v1-manifold-aware-point-cloud-completion-via-geodesic-attentive-hierarc.html">Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning</a></td>
  <td>æå‡ºåŸºäºæµå½¢æ„ŸçŸ¥çš„ç‚¹äº‘è¡¥å…¨æ¡†æ¶ï¼Œé€šè¿‡æµ‹åœ°çº¿æ³¨æ„åŠ›æœºåˆ¶æå‡å‡ ä½•ä¸€è‡´æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05710v1" onclick="toggleFavorite(this, '2512.05710v1', 'Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251205529v1-see-in-depth-training-free-surgical-scene-segmentation-with-monocula.html">See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors</a></td>
  <td>æå‡ºåŸºäºå•ç›®æ·±åº¦å…ˆéªŒçš„æ— è®­ç»ƒæ‰‹æœ¯åœºæ™¯åˆ†å‰²æ–¹æ³•DepSeg</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05529v1" onclick="toggleFavorite(this, '2512.05529v1', 'See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251205783v1-curvature-regularized-variational-autoencoder-for-3d-scene-reconstru.html">Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth</a></td>
  <td>æå‡ºæ›²ç‡æ­£åˆ™åŒ–VAEï¼Œç”¨äºä»ç¨€ç–æ·±åº¦æ•°æ®é‡å»º3Dåœºæ™¯</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05783v1" onclick="toggleFavorite(this, '2512.05783v1', 'Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251205759v1-label-efficient-point-cloud-segmentation-with-active-learning.html">Label-Efficient Point Cloud Segmentation with Active Learning</a></td>
  <td>æå‡ºåŸºäº2Dç½‘æ ¼åˆ’åˆ†å’Œç½‘ç»œé›†æˆçš„ç‚¹äº‘ä¸»åŠ¨å­¦ä¹ åˆ†å‰²æ–¹æ³•ï¼Œæå‡æ ‡æ³¨æ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05759v1" onclick="toggleFavorite(this, '2512.05759v1', 'Label-Efficient Point Cloud Segmentation with Active Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251205446v1-ted-4dgs-temporally-activated-and-embedding-based-deformation-for-4d.html">TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</a></td>
  <td>æå‡ºTED-4DGSï¼Œç”¨äºåŠ¨æ€3Dé«˜æ–¯æº…å°„å‹ç¼©ï¼Œå®ç°ç‡å¤±çœŸä¼˜åŒ–ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05446v1" onclick="toggleFavorite(this, '2512.05446v1', 'TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251205412v1-yolo-and-sgbm-integration-for-autonomous-tree-branch-detection-and-d.html">YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications</a></td>
  <td>æå‡ºYOLOä¸SGBMèåˆæ¡†æ¶ï¼Œç”¨äºè¾å°„æ¾ä¿®å‰ªä¸­æ ‘æçš„è‡ªä¸»æ£€æµ‹ä¸æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05412v1" onclick="toggleFavorite(this, '2512.05412v1', 'YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251205354v1-splatpainter-interactive-authoring-of-3d-gaussians-from-2d-edits-via.html">SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training</a></td>
  <td>æå‡ºSplatPainterä»¥è§£å†³3Dé«˜æ–¯æ¨¡å‹äº¤äº’ç¼–è¾‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05354v1" onclick="toggleFavorite(this, '2512.05354v1', 'SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251206179v1-physics-grounded-attached-shadow-detection-using-approximate-3d-geom.html">Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction</a></td>
  <td>æå‡ºåŸºäºè¿‘ä¼¼3Då‡ ä½•å’Œå…‰ç…§æ–¹å‘çš„ç‰©ç†çº¦æŸé˜´å½±æ£€æµ‹æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.06179v1" onclick="toggleFavorite(this, '2512.06179v1', 'Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251206158v1-tracking-guided-4d-generation-foundation-tracker-motion-priors-for-3.html">Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation</a></td>
  <td>æå‡ºTrack4DGenï¼Œåˆ©ç”¨è·Ÿè¸ªå¼•å¯¼çš„è¿åŠ¨å…ˆéªŒå®ç°é«˜è´¨é‡3Dæ¨¡å‹åŠ¨ç”»ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.06158v1" onclick="toggleFavorite(this, '2512.06158v1', 'Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251206080v1-shoot-bounce-3d-single-shot-occlusion-aware-3d-from-lidar-by-decompo.html">Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light</a></td>
  <td>Shoot-Bounce-3Dï¼šåˆ©ç”¨å•å…‰å­æ¿€å…‰é›·è¾¾å’ŒåŒæ¬¡åå°„å…‰è¿›è¡Œé®æŒ¡æ„ŸçŸ¥çš„ä¸‰ç»´é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.06080v1" onclick="toggleFavorite(this, '2512.06080v1', 'Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251206065v1-egoedit-dataset-real-time-streaming-model-and-benchmark-for-egocentr.html">EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</a></td>
  <td>EgoEditï¼šç”¨äºç¬¬ä¸€äººç§°è§†é¢‘ç¼–è¾‘çš„æ•°æ®é›†ã€å®æ—¶æ¨¡å‹ä¸è¯„æµ‹åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">ego-motion</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.06065v1" onclick="toggleFavorite(this, '2512.06065v1', 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251205941v1-zoom-in-click-out-unlocking-and-evaluating-the-potential-of-zooming-.html">Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</a></td>
  <td>æå‡ºZoomClickï¼Œåˆ©ç”¨ç¼©æ”¾å…ˆéªŒæå‡GUIç•Œé¢å…ƒç´ å®šä½æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05941v1" onclick="toggleFavorite(this, '2512.05941v1', 'Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251205610v1-normalview-sensor-agnostic-tree-species-classification-from-backpack.html">NormalView: sensor-agnostic tree species classification from backpack and aerial lidar data using geometric projections</a></td>
  <td>NormalViewï¼šä¸€ç§åŸºäºå‡ ä½•æŠ•å½±çš„ä¼ æ„Ÿå™¨æ— å…³æ ‘ç§åˆ†ç±»æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05610v1" onclick="toggleFavorite(this, '2512.05610v1', 'NormalView: sensor-agnostic tree species classification from backpack and aerial lidar data using geometric projections')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/251206058v1-representation-learning-for-point-cloud-understanding.html">Representation Learning for Point Cloud Understanding</a></td>
  <td>æå‡ºä¸€ç§èåˆ2Dé¢„è®­ç»ƒæ¨¡å‹çš„3Dç‚¹äº‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæå‡ç‚¹äº‘ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">point cloud</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.06058v1" onclick="toggleFavorite(this, '2512.06058v1', 'Representation Learning for Point Cloud Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251205927v1-world-models-that-know-when-they-dont-know-controllable-video-genera.html">World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</a></td>
  <td>æå‡ºC3æ–¹æ³•ï¼Œä¸ºå¯æ§è§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›æ ¡å‡†çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç¼“è§£å¹»è§‰é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05927v1" onclick="toggleFavorite(this, '2512.05927v1', 'World Models That Know When They Don&#39;t Know: Controllable Video Generation with Calibrated Uncertainty')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251205809v1-probing-the-effectiveness-of-world-models-for-spatial-reasoning-thro.html">Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling</a></td>
  <td>æå‡ºViSAæ¡†æ¶ï¼Œé€šè¿‡ç©ºé—´æ–­è¨€æ”¹è¿›ä¸–ç•Œæ¨¡å‹åœ¨ç©ºé—´æ¨ç†ä¸­çš„æµ‹è¯•æ—¶ç¼©æ”¾æ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">world model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05809v1" onclick="toggleFavorite(this, '2512.05809v1', 'Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251208980v2-training-multi-image-vision-agents-via-end2end-reinforcement-learnin.html">Training Multi-Image Vision Agents via End2End Reinforcement Learning</a></td>
  <td>æå‡ºIMAgentï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šå›¾è§†è§‰Agentï¼Œè§£å†³å¤æ‚å¤šå›¾QAä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.08980v2" onclick="toggleFavorite(this, '2512.08980v2', 'Training Multi-Image Vision Agents via End2End Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/251205905v1-scail-towards-studio-grade-character-animation-via-in-context-learni.html">SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</a></td>
  <td>SCAILï¼šé€šè¿‡3Dä¸€è‡´å§¿æ€è¡¨ç¤ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°å·¥ä½œå®¤çº§è§’è‰²åŠ¨ç”»</td>
  <td class="tags-cell"><span class="paper-tag">character animation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05905v1" onclick="toggleFavorite(this, '2512.05905v1', 'SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251205597v1-fast-scenescript-accurate-and-efficient-structured-language-model-vi.html">Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction</a></td>
  <td>Fast SceneScriptï¼šé€šè¿‡å¤šTokené¢„æµ‹å®ç°é«˜æ•ˆç²¾ç¡®çš„ç»“æ„åŒ–è¯­è¨€æ¨¡å‹ï¼Œç”¨äº3Dåœºæ™¯å¸ƒå±€ä¼°è®¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">ASE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05597v1" onclick="toggleFavorite(this, '2512.05597v1', 'Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/251211865v1-explainable-adversarial-robust-vision-language-action-model-for-robo.html">Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation</a></td>
  <td>æå‡ºå¯è§£é‡Šçš„å¯¹æŠ—é²æ£’è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œç”¨äºæå‡æœºå™¨äººæ“ä½œåœ¨æ™ºèƒ½å†œä¸šä¸­çš„é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.11865v1" onclick="toggleFavorite(this, '2512.11865v1', 'Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251205663v1-lead-m3d-leveraging-asymmetric-distillation-for-real-time-monocular-.html">LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</a></td>
  <td>LeAD-M3Dï¼šåˆ©ç”¨éå¯¹ç§°è’¸é¦å®ç°å®æ—¶å•ç›®3Dç›®æ ‡æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">running</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.05663v1" onclick="toggleFavorite(this, '2512.05663v1', 'LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)