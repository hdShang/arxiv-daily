---
layout: default
title: Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light
---

# Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.06080" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.06080v1</a>
  <a href="https://arxiv.org/pdf/2512.06080.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.06080v1" onclick="toggleFavorite(this, '2512.06080v1', 'Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tzofi Klinghoffer, Siddharth Somasundaram, Xiaoyu Xiang, Yuchen Fan, Christian Richardt, Akshat Dave, Ramesh Raskar, Rakesh Ranjan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-05

**å¤‡æ³¨**: SIGGRAPH Asia 2025. Project page: https://shoot-bounce-3d.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Shoot-Bounce-3Dï¼šåˆ©ç”¨å•å…‰å­æ¿€å…‰é›·è¾¾å’ŒåŒæ¬¡åå°„å…‰è¿›è¡Œé®æŒ¡æ„ŸçŸ¥çš„ä¸‰ç»´é‡å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å•å…‰å­æ¿€å…‰é›·è¾¾` `ä¸‰ç»´é‡å»º` `é®æŒ¡æ„ŸçŸ¥` `åŒæ¬¡åå°„å…‰` `å…‰ä¼ è¾“åæ¼”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å•å…‰å­æ¿€å…‰é›·è¾¾åœ¨å¤æ‚åœºæ™¯ä¸‹çš„ä¸‰ç»´é‡å»ºï¼Œå°¤å…¶æ˜¯åœ¨é®æŒ¡å’Œé•œé¢åå°„å­˜åœ¨æ—¶ï¼Œé¢ä¸´å…‰è·¯å¤æ‚éš¾ä»¥è§£æçš„é—®é¢˜ã€‚
2. æå‡ºShoot-Bounce-3Dï¼Œåˆ©ç”¨å•å…‰å­æ¿€å…‰é›·è¾¾è·å–çš„åŒæ¬¡åå°„å…‰ä¿¡æ¯ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨æ–¹æ³•å­¦ä¹ å…‰ä¼ è¾“å…ˆéªŒï¼Œåˆ†è§£å¤šé‡åå°„å…‰ã€‚
3. é€šè¿‡å¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®é›†è®­ç»ƒï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ¨æ–­é®æŒ¡å’Œé•œé¢åœºæ™¯ä¸­çš„ä¸‰ç»´å‡ ä½•å½¢çŠ¶ï¼Œå®ç°å•æ¬¡æµ‹é‡é‡å»ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å•å…‰å­æ¿€å…‰é›·è¾¾è¿›è¡Œä¸‰ç»´åœºæ™¯é‡å»ºçš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨é®æŒ¡åŒºåŸŸå’Œé•œé¢åå°„ææ–™çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¿€å…‰é›·è¾¾å‘å°„åˆ°åœºæ™¯ä¸­å¹¶ç›´æ¥åå°„å›ä¼ æ„Ÿå™¨çš„å…‰æ¥ä¼°è®¡æ·±åº¦ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åˆ©ç”¨åœ¨åœºæ™¯ä¸­å¤šæ¬¡åå°„ååˆ°è¾¾ä¼ æ„Ÿå™¨çš„å¤šé‡åå°„å…‰ã€‚è¿™ç§å¤šé‡åå°„å…‰åŒ…å«å¯ç”¨äºæ¢å¤å¯†é›†æ·±åº¦ã€é®æŒ¡å‡ ä½•å½¢çŠ¶å’Œææ–™å±æ€§çš„é¢å¤–ä¿¡æ¯ã€‚ä¸ä»¥å¾€å•å…‰å­æ¿€å…‰é›·è¾¾é€ç‚¹æ‰«æçš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡å…³æ³¨æ›´å…·æŒ‘æˆ˜æ€§çš„åŒæ—¶ç…§å°„å¤šä¸ªåœºæ™¯ç‚¹çš„åœºæ™¯ã€‚ç”±äºå¤šè·¯å¤ç”¨ç…§æ˜ã€åŒæ¬¡åå°„å…‰ã€é˜´å½±å’Œé•œé¢åå°„çš„ç»¼åˆå½±å“ï¼Œå…‰ä¼ è¾“çš„å¤æ‚æ€§éš¾ä»¥è¿›è¡Œè§£æåæ¼”ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥åæ¼”å•å…‰å­æ¿€å…‰é›·è¾¾ä¸­çš„å…‰ä¼ è¾“ã€‚ä¸ºäº†å®ç°è¿™ç§æ–¹æ³•ï¼Œæœ¬æ–‡åˆ›å»ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å®¤å†…åœºæ™¯æ¿€å…‰é›·è¾¾ç¬æ€æ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆçº¦10ä¸‡ä¸ªï¼‰ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†å­¦ä¹ å¤æ‚å…‰ä¼ è¾“çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå°†æµ‹é‡çš„åŒæ¬¡åå°„å…‰åˆ†è§£ä¸ºæ¥è‡ªæ¯ä¸ªæ¿€å…‰ç‚¹çš„ç»„æˆéƒ¨åˆ†ã€‚æœ€åï¼Œå®éªŒè¯æ˜äº†å¦‚ä½•åˆ©ç”¨è¿™ç§åˆ†è§£çš„å…‰æ¥æ¨æ–­å…·æœ‰é®æŒ¡å’Œé•œå­çš„åœºæ™¯ä¸­çš„ä¸‰ç»´å‡ ä½•å½¢çŠ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å•æ¬¡æµ‹é‡ä¸‹å•å…‰å­æ¿€å…‰é›·è¾¾ä¸‰ç»´é‡å»ºä¸­ï¼Œç”±äºé®æŒ¡å’Œé•œé¢åå°„å¯¼è‡´å…‰è·¯å¤æ‚ï¼Œéš¾ä»¥å‡†ç¡®é‡å»ºåœºæ™¯å‡ ä½•çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–é€ç‚¹æ‰«æï¼Œæ•ˆç‡è¾ƒä½ï¼Œä¸”éš¾ä»¥å¤„ç†å¤æ‚å…‰è·¯å¸¦æ¥çš„å¹²æ‰°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å•å…‰å­æ¿€å…‰é›·è¾¾èƒ½å¤Ÿæ•è·çš„åŒæ¬¡åå°„å…‰ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åŒ…å«äº†åœºæ™¯ä¸­è¢«é®æŒ¡åŒºåŸŸå’Œé•œé¢åå°„çš„ä¿¡æ¯ã€‚é€šè¿‡å­¦ä¹ å…‰ä¼ è¾“çš„å…ˆéªŒçŸ¥è¯†ï¼Œå°†å¤æ‚çš„åŒæ¬¡åå°„å…‰åˆ†è§£ä¸ºå„ä¸ªæ¿€å…‰ç‚¹è´¡çŒ®çš„ç»„æˆéƒ¨åˆ†ï¼Œä»è€Œæ¨æ–­å‡ºåœºæ™¯çš„å®Œæ•´å‡ ä½•ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) å¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®é›†ç”Ÿæˆï¼šåˆ›å»ºåŒ…å«å„ç§å®¤å†…åœºæ™¯å’Œå…‰ç…§æ¡ä»¶çš„æ¿€å…‰é›·è¾¾ç¬æ€æ•°æ®ã€‚2) å…‰ä¼ è¾“å…ˆéªŒå­¦ä¹ ï¼šåˆ©ç”¨ç”Ÿæˆçš„æ•°æ®é›†è®­ç»ƒç¥ç»ç½‘ç»œï¼Œå­¦ä¹ å¤æ‚å…‰ä¼ è¾“çš„æ¨¡å¼å’Œè§„å¾‹ã€‚3) åŒæ¬¡åå°„å…‰åˆ†è§£ï¼šåˆ©ç”¨è®­ç»ƒå¥½çš„ç½‘ç»œï¼Œå°†æµ‹é‡çš„åŒæ¬¡åå°„å…‰åˆ†è§£ä¸ºå„ä¸ªæ¿€å…‰ç‚¹å¯¹åº”çš„è´¡çŒ®ã€‚4) ä¸‰ç»´å‡ ä½•æ¨æ–­ï¼šåˆ©ç”¨åˆ†è§£åçš„å…‰ä¿¡æ¯ï¼Œæ¨æ–­åœºæ™¯çš„ä¸‰ç»´å‡ ä½•å½¢çŠ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥åæ¼”å•å…‰å­æ¿€å…‰é›·è¾¾ä¸­çš„å…‰ä¼ è¾“ã€‚ä¸ä¼ ç»Ÿçš„è§£ææ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„å…‰è·¯å’Œéçº¿æ€§æ•ˆåº”ï¼Œä»è€Œæ›´å‡†ç¡®åœ°é‡å»ºåœºæ™¯å‡ ä½•ã€‚æ­¤å¤–ï¼Œå¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè¯¥æ–¹æ³•çš„è®­ç»ƒå’ŒéªŒè¯æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æ„å»ºäº†å¤§è§„æ¨¡çš„æ¨¡æ‹Ÿæ•°æ®é›†ï¼ŒåŒ…å«çº¦10ä¸‡ä¸ªæ¿€å…‰é›·è¾¾ç¬æ€æ•°æ®ï¼Œæ¶µç›–äº†å„ç§å®¤å†…åœºæ™¯å’Œå…‰ç…§æ¡ä»¶ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œå…·ä½“ç»†èŠ‚æœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿå°†å¤æ‚åŒæ¬¡åå°„å…‰åˆ†è§£ä¸ºå„ä¸ªæ¿€å…‰ç‚¹è´¡çŒ®çš„æ¨¡å‹ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼ç½‘ç»œå­¦ä¹ å…‰ä¼ è¾“çš„å…ˆéªŒçŸ¥è¯†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨é®æŒ¡å’Œé•œé¢åå°„åœºæ™¯ä¸‹çš„ä¸‰ç»´é‡å»ºèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†è§£åŒæ¬¡åå°„å…‰ï¼Œå¹¶å‡†ç¡®åœ°æ¨æ–­å‡ºè¢«é®æŒ¡åŒºåŸŸå’Œé•œé¢çš„å‡ ä½•å½¢çŠ¶ã€‚è™½ç„¶è®ºæ–‡ä¸­æ²¡æœ‰ç»™å‡ºå…·ä½“çš„æ€§èƒ½æŒ‡æ ‡ï¼Œä½†å®éªŒç»“æœç›´è§‚åœ°å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€ä¸‰ç»´åœºæ™¯é‡å»ºã€å®¤å†…åœ°å›¾æ„å»ºç­‰é¢†åŸŸã€‚å°¤å…¶æ˜¯åœ¨å…‰çº¿æ¡ä»¶å¤æ‚ã€å­˜åœ¨é®æŒ¡å’Œé•œé¢åå°„çš„ç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæä¾›æ›´å‡†ç¡®å’Œå®Œæ•´çš„ä¸‰ç»´ä¿¡æ¯ï¼Œæé«˜ç³»ç»Ÿçš„æ„ŸçŸ¥èƒ½åŠ›å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ‹“å±•åˆ°æ–‡ç‰©ä¿æŠ¤ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

