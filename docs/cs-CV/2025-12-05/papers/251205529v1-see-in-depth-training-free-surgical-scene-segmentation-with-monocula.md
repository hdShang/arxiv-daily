---
layout: default
title: See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors
---

# See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.05529" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.05529v1</a>
  <a href="https://arxiv.org/pdf/2512.05529.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.05529v1" onclick="toggleFavorite(this, '2512.05529v1', 'See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kunyi Yang, Qingyu Wang, Cheng Yuan, Yutong Ban

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-05

**å¤‡æ³¨**: The first two authors contributed equally

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå•ç›®æ·±åº¦å…ˆéªŒçš„æ— è®­ç»ƒæ‰‹æœ¯åœºæ™¯åˆ†å‰²æ–¹æ³•DepSeg**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ‰‹æœ¯åœºæ™¯åˆ†å‰²` `å•ç›®æ·±åº¦ä¼°è®¡` `æ— è®­ç»ƒå­¦ä¹ ` `è§†è§‰åŸºç¡€æ¨¡å‹` `æ¨¡æ¿åŒ¹é…` `è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯` `è…¹è…”é•œæ‰‹æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è…¹è…”é•œæ‰‹æœ¯åœºæ™¯åˆ†å‰²æ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. DepSegåˆ©ç”¨å•ç›®æ·±åº¦ä¼°è®¡ä½œä¸ºå‡ ä½•å…ˆéªŒï¼Œç»“åˆé¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼Œå®ç°æ— è®­ç»ƒçš„åœºæ™¯åˆ†å‰²ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDepSegåœ¨åˆ†å‰²ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºç›´æ¥ä½¿ç”¨SAM2çš„æ–¹æ³•ï¼Œä¸”å¯¹æ¨¡æ¿æ•°é‡ä¸æ•æ„Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è…¹è…”é•œåœºæ™¯çš„åƒç´ çº§åˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯è‡³å…³é‡è¦ï¼Œä½†ç”±äºå¯†é›†æ ‡æ³¨çš„é«˜æˆæœ¬è€Œéš¾ä»¥æ‰©å±•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦å¼•å¯¼çš„æ‰‹æœ¯åœºæ™¯åˆ†å‰²æ¡†æ¶(DepSeg)ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å•ç›®æ·±åº¦ä½œä¸ºå‡ ä½•å…ˆéªŒï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæ— éœ€è®­ç»ƒã€‚DepSegé¦–å…ˆä½¿ç”¨é¢„è®­ç»ƒçš„å•ç›®æ·±åº¦ä¼°è®¡ç½‘ç»œä¼°è®¡ç›¸å¯¹æ·±åº¦å›¾ï¼Œå¹¶æå‡ºæ·±åº¦å¼•å¯¼çš„ç‚¹æç¤ºï¼ŒSAM2å°†å…¶è½¬æ¢ä¸ºç±»åˆ«æ— å…³çš„æ©ç ã€‚ç„¶åï¼Œæ¯ä¸ªæ©ç ç”±ä¸€ä¸ªæ± åŒ–çš„é¢„è®­ç»ƒè§†è§‰ç‰¹å¾æè¿°ï¼Œå¹¶é€šè¿‡æ¨¡æ¿åŒ¹é…é’ˆå¯¹ä»å¸¦æ³¨é‡Šçš„å¸§æ„å»ºçš„æ¨¡æ¿åº“è¿›è¡Œåˆ†ç±»ã€‚åœ¨CholecSeg8kæ•°æ®é›†ä¸Šï¼ŒDepSegä¼˜äºç›´æ¥çš„SAM2è‡ªåŠ¨åˆ†å‰²åŸºçº¿ï¼ˆ35.9% vs. 14.7% mIoUï¼‰ï¼Œå³ä½¿ä»…ä½¿ç”¨10-20%çš„å¯¹è±¡æ¨¡æ¿ä¹Ÿèƒ½ä¿æŒæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å¼•å¯¼çš„æç¤ºå’ŒåŸºäºæ¨¡æ¿çš„åˆ†ç±»æä¾›äº†ä¸€ç§æ³¨é‡Šé«˜æ•ˆçš„åˆ†å‰²æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è…¹è…”é•œæ‰‹æœ¯åœºæ™¯ä¸­ï¼Œç”±äºç¼ºä¹å¤§é‡æ ‡æ³¨æ•°æ®è€Œå¯¼è‡´çš„åƒç´ çº§åˆ†å‰²éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡çš„åƒç´ çº§æ ‡æ³¨ï¼Œè¿™åœ¨åŒ»ç–—é¢†åŸŸéå¸¸è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ— éœ€è®­ç»ƒæˆ–ä»…éœ€å°‘é‡æ ‡æ³¨æ•°æ®å°±èƒ½å®ç°ç²¾ç¡®åˆ†å‰²çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å•ç›®æ·±åº¦ä¼°è®¡ä½œä¸ºå‡ ä½•å…ˆéªŒä¿¡æ¯ï¼Œå¼•å¯¼åˆ†å‰²è¿‡ç¨‹ã€‚é€šè¿‡é¢„è®­ç»ƒçš„å•ç›®æ·±åº¦ä¼°è®¡ç½‘ç»œè·å–åœºæ™¯çš„æ·±åº¦ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç‚¹æç¤ºï¼Œä»è€Œå¼•å¯¼SAM2ç”Ÿæˆç±»åˆ«æ— å…³çš„æ©ç ã€‚ç„¶åï¼Œåˆ©ç”¨æ¨¡æ¿åŒ¹é…çš„æ–¹å¼ï¼Œå°†è¿™äº›æ©ç ä¸å°‘é‡æ ‡æ³¨æ ·æœ¬è¿›è¡ŒåŒ¹é…ï¼Œå®ç°æœ€ç»ˆçš„åˆ†å‰²ã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨æ·±åº¦ä¿¡æ¯å‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDepSegæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **å•ç›®æ·±åº¦ä¼°è®¡**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„å•ç›®æ·±åº¦ä¼°è®¡ç½‘ç»œä¼°è®¡è¾“å…¥å›¾åƒçš„ç›¸å¯¹æ·±åº¦å›¾ã€‚2) **æ·±åº¦å¼•å¯¼çš„ç‚¹æç¤º**ï¼šæ ¹æ®æ·±åº¦å›¾ç”Ÿæˆç‚¹æç¤ºï¼Œç”¨äºå¼•å¯¼SAM2ç”Ÿæˆç±»åˆ«æ— å…³çš„æ©ç ã€‚3) **æ©ç ç”Ÿæˆ**ï¼šä½¿ç”¨SAM2å°†ç‚¹æç¤ºè½¬æ¢ä¸ºç±»åˆ«æ— å…³çš„æ©ç ã€‚4) **ç‰¹å¾æå–**ï¼šå¯¹æ¯ä¸ªæ©ç åŒºåŸŸæå–é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„ç‰¹å¾ã€‚5) **æ¨¡æ¿åŒ¹é…**ï¼šå°†æå–çš„ç‰¹å¾ä¸ä»å°‘é‡æ ‡æ³¨æ ·æœ¬æ„å»ºçš„æ¨¡æ¿åº“è¿›è¡ŒåŒ¹é…ï¼Œä»è€Œç¡®å®šæ©ç çš„ç±»åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å•ç›®æ·±åº¦ä¼°è®¡ä¸é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°äº†ä¸€ç§æ— éœ€è®­ç»ƒæˆ–ä»…éœ€å°‘é‡æ ‡æ³¨æ•°æ®çš„è…¹è…”é•œæ‰‹æœ¯åœºæ™¯åˆ†å‰²æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œæé«˜äº†å¯æ‰©å±•æ€§ã€‚ä¸ç›´æ¥ä½¿ç”¨SAM2ç­‰é€šç”¨åˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦ä¿¡æ¯ä½œä¸ºå…ˆéªŒï¼Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ·±åº¦å¼•å¯¼çš„ç‚¹æç¤ºæ–¹é¢ï¼Œè®ºæ–‡æ ¹æ®æ·±åº¦å›¾çš„åˆ†å¸ƒé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„ç‚¹ä½œä¸ºæç¤ºã€‚åœ¨ç‰¹å¾æå–æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„æ± åŒ–ç‰¹å¾æ¥æè¿°æ©ç åŒºåŸŸï¼Œä»¥æé«˜ç‰¹å¾çš„é²æ£’æ€§ã€‚åœ¨æ¨¡æ¿åŒ¹é…æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºåŒ¹é…åº¦é‡ï¼Œå¹¶è®¾ç½®é˜ˆå€¼æ¥è¿‡æ»¤ä¸åŒ¹é…çš„æ©ç ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DepSegåœ¨CholecSeg8kæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒmIoUè¾¾åˆ°35.9%ï¼Œè¿œé«˜äºç›´æ¥ä½¿ç”¨SAM2çš„14.7%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå³ä½¿ä»…ä½¿ç”¨10-20%çš„å¯¹è±¡æ¨¡æ¿ï¼ŒDepSegä¹Ÿèƒ½ä¿æŒæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„æ ‡æ³¨æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ç³»ç»Ÿï¼Œä¾‹å¦‚æœºå™¨äººè¾…åŠ©æ‰‹æœ¯ã€‚é€šè¿‡å®æ—¶åˆ†å‰²æ‰‹æœ¯åœºæ™¯ï¼Œå¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å¥½åœ°ç†è§£æ‰‹æœ¯è¿‡ç¨‹ï¼Œæé«˜æ‰‹æœ¯ç²¾åº¦å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€æ‰‹æœ¯æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.

