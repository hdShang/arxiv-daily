---
layout: default
title: Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation
---

# Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.11865" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.11865v1</a>
  <a href="https://arxiv.org/pdf/2512.11865.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11865v1" onclick="toggleFavorite(this, '2512.11865v1', 'Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ju-Young Kim, Ji-Hong Park, Myeongjun Kim, Gun-Woo Kim

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-05

**å¤‡æ³¨**: Accepted to MobieSec 2025 (poster session)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯è§£é‡Šçš„å¯¹æŠ—é²æ£’è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œç”¨äºæå‡æœºå™¨äººæ“ä½œåœ¨æ™ºèƒ½å†œä¸šä¸­çš„é²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å¯¹æŠ—é²æ£’æ€§` `å¯è§£é‡Šæ€§` `æ™ºèƒ½å†œä¸š` `å…‰åº¦æ‰°åŠ¨` `è‡ªç„¶è¯­è¨€è§£é‡Š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ™ºèƒ½å†œä¸šç³»ç»Ÿä¸­ï¼ŒåŸºäºRGBç›¸æœºçš„è§†è§‰æ„ŸçŸ¥å’Œæœºå™¨äººæ“ä½œæ˜“å—å…‰ç…§ç­‰æ‰°åŠ¨å½±å“ï¼Œå¯¼è‡´å¯¹æŠ—æ”»å‡»ä¸‹çš„ç³»ç»Ÿå¤±æ•ˆã€‚
2. è®ºæ–‡æå‡ºåŸºäºOpenVLA-OFTæ¡†æ¶çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé›†æˆEvidence-3æ¨¡å—æ£€æµ‹æ‰°åŠ¨å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—é™ä½äº†åŠ¨ä½œé¢„æµ‹çš„L1æŸå¤±ï¼Œæå‡äº†åœ¨å¯¹æŠ—ç¯å¢ƒä¸‹çš„åŠ¨ä½œé¢„æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„å¯¹æŠ—é²æ£’è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºOpenVLA-OFTæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½å†œä¸šä¸­ä¾èµ–RGBç›¸æœºæ„ŸçŸ¥å’Œæœºå™¨äººæ“ä½œçš„ç³»ç»Ÿæ˜“å—å…‰åº¦æ‰°åŠ¨ï¼ˆå¦‚è‰²è°ƒã€å…‰ç…§å’Œå™ªå£°å˜åŒ–ï¼‰å½±å“çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é›†æˆäº†ä¸€ä¸ªEvidence-3æ¨¡å—ï¼Œç”¨äºæ£€æµ‹å…‰åº¦æ‰°åŠ¨ï¼Œå¹¶ç”Ÿæˆå…³äºå…¶åŸå› å’Œå½±å“çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å½“å‰åŠ¨ä½œL1æŸå¤±ä¸Šé™ä½äº†21.7%ï¼Œåœ¨åç»­åŠ¨ä½œL1æŸå¤±ä¸Šé™ä½äº†18.4%ï¼Œè¯æ˜äº†å…¶åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹å…·æœ‰æ›´é«˜çš„åŠ¨ä½œé¢„æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ™ºèƒ½å†œä¸šç³»ç»Ÿä¸­ï¼Œæœºå™¨äººæ“ä½œä¾èµ–RGBç›¸æœºè¿›è¡Œè§†è§‰æ„ŸçŸ¥ï¼Œä½†RGBç›¸æœºå®¹æ˜“å—åˆ°å…‰ç…§ã€è‰²è°ƒã€å™ªå£°ç­‰å…‰åº¦æ‰°åŠ¨çš„å½±å“ã€‚è¿™äº›æ‰°åŠ¨ä¼šå¯¼è‡´ç³»ç»Ÿåœ¨å¯¹æŠ—æ”»å‡»ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œç”šè‡³å®Œå…¨å¤±æ•ˆã€‚å› æ­¤ï¼Œå¦‚ä½•æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å…‰åº¦æ‰°åŠ¨ä¸‹çš„é²æ£’æ€§ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ‰°åŠ¨åŸå› å’Œå½±å“çš„è§£é‡Šèƒ½åŠ›ï¼Œéš¾ä»¥è¿›è¡Œé’ˆå¯¹æ€§çš„é˜²å¾¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¯è§£é‡Šçš„å¯¹æŠ—é²æ£’è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡é›†æˆEvidence-3æ¨¡å—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ£€æµ‹å…‰åº¦æ‰°åŠ¨ï¼Œå¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§ã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹ä¸ä»…èƒ½å¤Ÿé¢„æµ‹åŠ¨ä½œï¼Œè¿˜èƒ½ç†è§£å¹¶è§£é‡Šå…¶é¢„æµ‹çš„åŸå› ï¼Œä»è€Œæ›´å®¹æ˜“è¿›è¡Œè°ƒè¯•å’Œæ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹åŸºäºOpenVLA-OFTæ¡†æ¶æ„å»ºï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š1) è§†è§‰æ„ŸçŸ¥æ¨¡å—ï¼šè´Ÿè´£ä»RGBå›¾åƒä¸­æå–è§†è§‰ç‰¹å¾ã€‚2) è¯­è¨€ç†è§£æ¨¡å—ï¼šè´Ÿè´£ç†è§£è¾“å…¥çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚3) åŠ¨ä½œé¢„æµ‹æ¨¡å—ï¼šè´Ÿè´£æ ¹æ®è§†è§‰ç‰¹å¾å’Œè¯­è¨€æŒ‡ä»¤é¢„æµ‹æœºå™¨äººçš„åŠ¨ä½œã€‚4) Evidence-3æ¨¡å—ï¼šè¿™æ˜¯è®ºæ–‡çš„å…³é”®åˆ›æ–°ï¼Œè´Ÿè´£æ£€æµ‹å…‰åº¦æ‰°åŠ¨ï¼Œå¹¶ç”Ÿæˆå…³äºå…¶åŸå› å’Œå½±å“çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šè¾“å…¥RGBå›¾åƒå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè§†è§‰æ„ŸçŸ¥æ¨¡å—å’Œè¯­è¨€ç†è§£æ¨¡å—åˆ†åˆ«æå–è§†è§‰ç‰¹å¾å’Œè¯­è¨€ç‰¹å¾ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾è¾“å…¥åˆ°åŠ¨ä½œé¢„æµ‹æ¨¡å—å’ŒEvidence-3æ¨¡å—ä¸­ï¼ŒåŠ¨ä½œé¢„æµ‹æ¨¡å—é¢„æµ‹æœºå™¨äººçš„åŠ¨ä½œï¼ŒEvidence-3æ¨¡å—æ£€æµ‹å…‰åº¦æ‰°åŠ¨å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºé›†æˆäº†Evidence-3æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿæ£€æµ‹å…‰åº¦æ‰°åŠ¨ï¼Œå¹¶ç”Ÿæˆå…³äºå…¶åŸå› å’Œå½±å“çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹çš„é²æ£’æ€§ï¼Œè¿˜èƒ½å¤Ÿæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿç†è§£æ¨¡å‹é¢„æµ‹çš„åŸå› ï¼Œä»è€Œæ›´å®¹æ˜“è¿›è¡Œè°ƒè¯•å’Œæ”¹è¿›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³äºEvidence-3æ¨¡å—çš„å…·ä½“è®¾è®¡ç»†èŠ‚æœªçŸ¥ï¼Œæ‘˜è¦ä¸­æ²¡æœ‰è¯¦ç»†è¯´æ˜ã€‚ä½†æ˜¯å¯ä»¥æ¨æµ‹ï¼Œè¯¥æ¨¡å—å¯èƒ½ä½¿ç”¨äº†æŸç§å½¢å¼çš„æ³¨æ„åŠ›æœºåˆ¶æˆ–å› æœæ¨ç†æ¨¡å‹ï¼Œä»¥ä¾¿èƒ½å¤Ÿè¯†åˆ«å…‰åº¦æ‰°åŠ¨ï¼Œå¹¶ç”Ÿæˆå…³äºå…¶åŸå› å’Œå½±å“çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨äº†Current Action L1 losså’ŒNext Actions L1 lossæ¥è¯„ä¼°åŠ¨ä½œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å½“å‰åŠ¨ä½œL1æŸå¤±ä¸Šé™ä½äº†21.7%ï¼Œåœ¨åç»­åŠ¨ä½œL1æŸå¤±ä¸Šé™ä½äº†18.4%ã€‚è¿™è¡¨æ˜è¯¥æ¨¡å‹åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹å…·æœ‰æ›´é«˜çš„åŠ¨ä½œé¢„æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å…‰åº¦æ‰°åŠ¨å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ“ä½œæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å†œä¸šã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸã€‚åœ¨æ™ºèƒ½å†œä¸šä¸­ï¼Œå¯ä»¥æé«˜æœºå™¨äººæ“ä½œåœ¨å¤æ‚å…‰ç…§æ¡ä»¶ä¸‹çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥æé«˜è½¦è¾†åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥æé«˜ç›‘æ§ç³»ç»Ÿåœ¨å…‰ç…§å˜åŒ–ä¸‹çš„å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå¹¶æé«˜äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

