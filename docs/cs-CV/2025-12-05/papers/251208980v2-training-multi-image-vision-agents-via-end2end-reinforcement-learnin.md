---
layout: default
title: Training Multi-Image Vision Agents via End2End Reinforcement Learning
---

# Training Multi-Image Vision Agents via End2End Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.08980" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.08980v2</a>
  <a href="https://arxiv.org/pdf/2512.08980.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.08980v2" onclick="toggleFavorite(this, '2512.08980v2', 'Training Multi-Image Vision Agents via End2End Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengqi Dong, Chuhuai Yue, Hang He, Rongge Mao, Fenghe Tang, S Kevin Zhou, Zekun Xu, Xiaohan Wang, Jiajun Chai, Wei Lin, Guojun Yin

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-05 (æ›´æ–°: 2025-12-16)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIMAgentï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šå›¾è§†è§‰Agentï¼Œè§£å†³å¤æ‚å¤šå›¾QAä»»åŠ¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šå›¾QA` `è§†è§‰Agent` `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·ä½¿ç”¨` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºVLMçš„Agentåœ¨å·¥å…·ä½¿ç”¨æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¤§å¤šä»…é™äºå•å¼ å›¾åƒè¾“å…¥ï¼Œéš¾ä»¥åº”å¯¹çœŸå®ä¸–ç•Œçš„å¤šå›¾QAä»»åŠ¡ã€‚
2. IMAgenté€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¹¶å¼•å…¥å¤šAgentç³»ç»Ÿç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ï¼Œä»è€Œæå‡VLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒIMAgentåœ¨å¤šå›¾QAä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨å•å›¾åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºIMAgentï¼Œä¸€ä¸ªå¼€æºçš„è§†è§‰Agentï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä¸“é—¨ç”¨äºå¤„ç†å¤æ‚çš„å¤šå›¾ä»»åŠ¡ã€‚åˆ©ç”¨å¤šAgentç³»ç»Ÿï¼Œç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§å’Œè§†è§‰ä¸°å¯Œæ€§çš„å¤šå›¾QAå¯¹ï¼Œå……åˆ†æ¿€æ´»åŸºç¡€VLMçš„å·¥å…·ä½¿ç”¨æ½œåŠ›ã€‚é€šè¿‡äººå·¥éªŒè¯ï¼Œæ„å»ºäº†åŒ…å«1ä¸‡ä¸ªæ ·æœ¬çš„MIFG-QAæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚é’ˆå¯¹VLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å¿½ç•¥è§†è§‰è¾“å…¥çš„é—®é¢˜ï¼Œå¼€å‘äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨é‡æ–°åˆ†é…å¯¹å›¾åƒå†…å®¹çš„æ³¨æ„åŠ›ã€‚å—ç›Šäºç²¾å¿ƒè®¾è®¡çš„åŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥ï¼ŒIMAgenté€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°äº†ç¨³å®šçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œæ— éœ€æ˜‚è´µçš„ç›‘ç£å¾®è°ƒæ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIMAgentåœ¨ç°æœ‰å•å›¾åŸºå‡†ä¸Šä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶åœ¨æå‡ºçš„å¤šå›¾æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåˆ†æä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚ä»£ç å’Œæ•°æ®å³å°†å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„Agentåœ¨å¤„ç†å¤šå›¾QAä»»åŠ¡æ—¶å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦åŸå› æ˜¯å®ƒä»¬é€šå¸¸åªæ¥å—å•å¼ å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨éœ€è¦ç»¼åˆå¤šå¼ å›¾åƒä¿¡æ¯æ‰èƒ½å®Œæˆçš„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç°æœ‰çš„å¼€æºæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨VLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›æ¥è§£å†³å¤æ‚çš„å¤šå›¾æ¨ç†é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šå¼ å›¾åƒä¿¡æ¯çš„è§†è§‰Agentã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªå¤šAgentç³»ç»Ÿæ¥ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ï¼Œä»è€Œè®­ç»ƒVLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³VLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å¿½ç•¥è§†è§‰è¾“å…¥çš„é—®é¢˜ï¼Œå¼•å…¥äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œä¿ƒä½¿æ¨¡å‹æ›´åŠ å…³æ³¨å›¾åƒå†…å®¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIMAgentçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šAgentæ•°æ®ç”Ÿæˆå™¨ï¼šè´Ÿè´£ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ï¼Œç”¨äºè®­ç»ƒAgentã€‚2) åŸºäºVLMçš„Agentï¼šä½œä¸ºæ ¸å¿ƒæ¨ç†æ¨¡å—ï¼Œè´Ÿè´£æ¥æ”¶å›¾åƒå’Œé—®é¢˜ï¼Œå¹¶è¾“å‡ºç­”æ¡ˆã€‚3) è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼šç”¨äºåœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡æ–°åˆ†é…å¯¹å›¾åƒå†…å®¹çš„æ³¨æ„åŠ›ã€‚4) å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°å’ŒåŠ¨ä½œè½¨è¿¹æ©ç ç­–ç•¥ï¼Œè®­ç»ƒAgentçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šAgentç³»ç»Ÿçš„å¤šå›¾QAæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„è®­ç»ƒæ•°æ®ã€‚2) è®¾è®¡äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œè§£å†³äº†VLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å¿½ç•¥è§†è§‰è¾“å…¥çš„é—®é¢˜ã€‚3) æå‡ºäº†ä¸€ä¸ªåŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥ï¼Œä½¿å¾—Agentèƒ½å¤Ÿé€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°ç¨³å®šçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œæ— éœ€ä¾èµ–æ˜‚è´µçš„ç›‘ç£å¾®è°ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®ç”Ÿæˆæ–¹é¢ï¼Œè®¾è®¡äº†ä¸åŒçš„Agentè§’è‰²ï¼Œåˆ†åˆ«è´Ÿè´£ç”Ÿæˆé—®é¢˜ã€é€‰æ‹©å›¾åƒå’Œæä¾›ç­”æ¡ˆï¼Œä»è€Œä¿è¯æ•°æ®çš„å¤šæ ·æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹é¢ï¼Œä½¿ç”¨äº†ç¨€ç–å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±Agenté‡‡å–æ­£ç¡®çš„åŠ¨ä½œåºåˆ—ã€‚åŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥é€šè¿‡é™åˆ¶Agentåœ¨ä¸åŒé˜¶æ®µå¯ä»¥é‡‡å–çš„åŠ¨ä½œï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

IMAgentåœ¨æå‡ºçš„å¤šå›¾æ•°æ®é›†MIFG-QAä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å•å›¾Agentã€‚åŒæ—¶ï¼ŒIMAgentåœ¨ç°æœ‰çš„å•å›¾åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç«äº‰åŠ›ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ¶ˆèå®éªŒï¼ŒéªŒè¯äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ä»¥åŠåŠ¨ä½œè½¨è¿¹æ©ç ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†å±•ç¤ºï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IMAgentå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€åŒ»å­¦å›¾åƒè¯Šæ–­ã€é¥æ„Ÿå›¾åƒåˆ†æç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·ä»å¤šå¼ å›¾åƒä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œæ·±å…¥çš„æ¨ç†å’Œå†³ç­–ã€‚æœªæ¥ï¼ŒIMAgentå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€æœºå™¨äººå¯¼èˆªç­‰ï¼Œä¸ºäººå·¥æ™ºèƒ½åº”ç”¨å¸¦æ¥æ›´å¼ºå¤§çš„èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.

