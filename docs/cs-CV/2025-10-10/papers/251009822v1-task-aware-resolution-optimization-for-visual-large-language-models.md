---
layout: default
title: Task-Aware Resolution Optimization for Visual Large Language Models
---

# Task-Aware Resolution Optimization for Visual Large Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.09822" target="_blank" class="toolbar-btn">arXiv: 2510.09822v1</a>
    <a href="https://arxiv.org/pdf/2510.09822.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09822v1" 
            onclick="toggleFavorite(this, '2510.09822v1', 'Task-Aware Resolution Optimization for Visual Large Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Weiqing Luo, Zhen Tan, Yifan Li, Xinyu Zhao, Kwonjoon Lee, Behzad Dariush, Tianlong Chen

**ÂàÜÁ±ª**: cs.CV, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10

**Â§áÊ≥®**: Accepted as a main conference paper at EMNLP 2025. 9 pages (main content), 7 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰ªªÂä°ÊÑüÁü•ÂàÜËæ®Áéá‰ºòÂåñÊñπÊ≥ïÔºåÊèêÂçáËßÜËßâÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ÂàÜËæ®Áéá‰ºòÂåñ` `‰ªªÂä°ÊÑüÁü•` `ÂèÇÊï∞È´òÊïàÂæÆË∞É` `ÂõæÂÉèÂ§çÊùÇÂ∫¶` `Ê®°Âûã‰∏çÁ°ÆÂÆöÊÄß` `ËßÜËßâÈóÆÁ≠î` `ÂõæÂÉèÊèèËø∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLLM‰∏∫ÊâÄÊúâ‰ªªÂä°ÈááÁî®Âõ∫ÂÆöÂàÜËæ®ÁéáÔºåÂøΩÁï•‰∫Ü‰∏çÂêå‰ªªÂä°ÂØπÂõæÂÉèÁªÜËäÇÁöÑÈúÄÊ±ÇÂ∑ÆÂºÇÔºåÂØºËá¥ÊÄßËÉΩÁì∂È¢à„ÄÇ
2. ÊèêÂá∫‰ªªÂä°ÊÑüÁü•ÁöÑÂàÜËæ®Áéá‰ºòÂåñÊñπÊ≥ïÔºåÈÄöËøáÂàÜÊûêÂõæÂÉèÂ§çÊùÇÂ∫¶ÂíåÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄßÔºåÁ°ÆÂÆöÊØè‰∏™‰ªªÂä°ÁöÑÊúÄ‰Ω≥ÂàÜËæ®Áéá„ÄÇ
3. ËÆæËÆ°ÂèÇÊï∞È´òÊïàÁöÑÂæÆË∞ÉÁ≠ñÁï•Ôºå‰ΩøÈ¢ÑËÆ≠ÁªÉVLLMÈÄÇÂ∫îÊñ∞ÁöÑÂàÜËæ®ÁéáÔºåÂπ∂Âú®Â§öÁßçËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°‰∏äÈ™åËØÅ‰∫ÜÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑËßÜËßâÂ§ßËØ≠Ë®ÄÊ®°Âûã(VLLMs)ÔºåÂ¶ÇLLaVAÔºåÈÄöÂ∏∏‰∏∫‰∏ãÊ∏∏‰ªªÂä°È¢ÑËÆæÂõ∫ÂÆöÁöÑÂàÜËæ®ÁéáÔºåÂØºËá¥ÊÄßËÉΩÊ¨†‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨È¶ñÂÖàÂØπ‰∏çÂêåËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°ÁöÑÂàÜËæ®ÁéáÂÅèÂ•ΩËøõË°å‰∫ÜÂÖ®Èù¢ËÄåÂºÄÂàõÊÄßÁöÑÁ†îÁ©∂ÔºåÊè≠Á§∫‰∫ÜÂàÜËæ®ÁéáÂÅèÂ•Ω‰∏éÂõæÂÉèÂ§çÊùÇÊÄß‰ª•ÂèäVLLMÂú®‰∏çÂêåÂõæÂÉèËæìÂÖ•ÂàÜËæ®Áéá‰∏ãÁöÑ‰∏çÁ°ÆÂÆöÊÄßÊñπÂ∑Æ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁªèÈ™åÂÖ¨ÂºèÔºåÁªìÂêàËøô‰∏§‰∏™Âõ†Á¥†Êù•Á°ÆÂÆöÁªôÂÆöËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°ÁöÑÊúÄ‰Ω≥ÂàÜËæ®Áéá„ÄÇÂÖ∂Ê¨°ÔºåÂü∫‰∫é‰∏•Ê†ºÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèÇÊï∞È´òÊïàÂæÆË∞ÉÊäÄÊúØÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉVLLMÁöÑËßÜËßâËæìÂÖ•ÂàÜËæ®ÁéáÊâ©Â±ïÂà∞Á°ÆÂÆöÁöÑÊúÄ‰Ω≥ÂàÜËæ®Áéá„ÄÇÂú®ÂêÑÁßçËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàVLLMsÔºâÈÄöÂ∏∏ÈááÁî®Âõ∫ÂÆöÁöÑËæìÂÖ•ÂàÜËæ®ÁéáÔºåÊó†Ê≥ïÈÄÇÂ∫î‰∏çÂêåËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°ÂØπÂõæÂÉèÁªÜËäÇÁöÑ‰∏çÂêåÈúÄÊ±Ç„ÄÇ‰æãÂ¶ÇÔºå‰∏Ä‰∫õ‰ªªÂä°ÂèØËÉΩÈúÄË¶ÅÈ´òÂàÜËæ®Áéá‰ª•ÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑ‰ø°ÊÅØÔºåËÄåÂè¶‰∏Ä‰∫õ‰ªªÂä°ÂàôÂèØËÉΩÂú®È´òÂàÜËæ®Áéá‰∏ãÂºïÂÖ•‰∏çÂøÖË¶ÅÁöÑÂô™Â£∞„ÄÇËøôÁßçÂõ∫ÂÆöÂàÜËæ®ÁéáÁöÑÁ≠ñÁï•ÈôêÂà∂‰∫ÜVLLMsÂú®ÂêÑÁßçÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊ†πÊçÆ‰ªªÂä°ÁöÑÁâπÊÄßÂä®ÊÄÅË∞ÉÊï¥ËæìÂÖ•ÂàÜËæ®Áéá„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÂàÜÊûêÂõæÂÉèÁöÑÂ§çÊùÇÂ∫¶ÂíåVLLMÂú®‰∏çÂêåÂàÜËæ®Áéá‰∏ãÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÊù•Á°ÆÂÆöÊØè‰∏™‰ªªÂä°ÁöÑÊúÄ‰Ω≥ÂàÜËæ®Áéá„ÄÇÂõæÂÉèÂ§çÊùÇÂ∫¶ÂèçÊò†‰∫Ü‰ªªÂä°ÂØπÁªÜËäÇÁöÑÈúÄÊ±ÇÁ®ãÂ∫¶ÔºåËÄå‰∏çÁ°ÆÂÆöÊÄßÂàôÂèçÊò†‰∫ÜÊ®°ÂûãÂú®ÁâπÂÆöÂàÜËæ®Áéá‰∏ãÁöÑÁΩÆ‰ø°Â∫¶„ÄÇÂ∞Ü‰∏§ËÄÖÁªìÂêàÔºåÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∏™Âπ≥Ë°°ÁÇπÔºå‰ΩøÂæóÊ®°ÂûãÊó¢ËÉΩÊçïÊçâÂà∞Ë∂≥Â§üÁöÑÁªÜËäÇÔºåÂèà‰∏ç‰ºöÂèóÂà∞ËøáÂ§öÂô™Â£∞ÁöÑÂπ≤Êâ∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµ„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÊòØÂàÜËæ®ÁéáÂÅèÂ•ΩÂàÜÊûêÔºåÈÄöËøáÂÆûÈ™åÁ°ÆÂÆö‰∏çÂêåËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°ÁöÑÊúÄ‰Ω≥ÂàÜËæ®ÁéáËåÉÂõ¥„ÄÇËøô‰∏ÄÈò∂ÊÆµÁöÑÂÖ≥ÈîÆÊòØËÆæËÆ°ÂêàÈÄÇÁöÑÊåáÊ†áÊù•Ë°°ÈáèÂõæÂÉèÂ§çÊùÇÂ∫¶ÂíåÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄß„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÊòØÂèÇÊï∞È´òÊïàÁöÑÂæÆË∞ÉÔºåÂà©Áî®Â∞ëÈáèÂèÇÊï∞Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑVLLMÈÄÇÂ∫îÂà∞Êñ∞ÁöÑÂàÜËæ®Áéá„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÈ¶ñÂÖàÔºåÂØπÁªôÂÆöÁöÑËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°ÔºåÂà©Áî®ÁªèÈ™åÂÖ¨ÂºèËÆ°ÁÆóÂá∫ÊúÄ‰Ω≥ÂàÜËæ®ÁéáÔºõÁÑ∂ÂêéÔºå‰ΩøÁî®ÂèÇÊï∞È´òÊïàÁöÑÂæÆË∞ÉÊäÄÊúØÔºåË∞ÉÊï¥VLLMÁöÑËßÜËßâËæìÂÖ•Â±ÇÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂ§ÑÁêÜÊñ∞ÁöÑÂàÜËæ®ÁéáÔºõÊúÄÂêéÔºåÂú®ÁõÆÊ†á‰ªªÂä°‰∏äËøõË°åÂæÆË∞ÉÔºå‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÂçáÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÊèêÂá∫‰∫Ü‰ªªÂä°ÊÑüÁü•ÁöÑÂàÜËæ®Áéá‰ºòÂåñÁ≠ñÁï•„ÄÇ‰∏é‰ª•ÂæÄÁöÑÂõ∫ÂÆöÂàÜËæ®ÁéáÊñπÊ≥ï‰∏çÂêåÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊ†πÊçÆ‰ªªÂä°ÁöÑÁâπÊÄßÂä®ÊÄÅË∞ÉÊï¥ËæìÂÖ•ÂàÜËæ®ÁéáÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏çÂêå‰ªªÂä°ÁöÑÈúÄÊ±Ç„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫ÁöÑÁªèÈ™åÂÖ¨ÂºèÂíåÂèÇÊï∞È´òÊïàÂæÆË∞ÉÊäÄÊúØ‰πü‰∏∫ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÊèê‰æõ‰∫ÜÊúâÊïàÁöÑÊâãÊÆµ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÁªèÈ™åÂÖ¨ÂºèÊòØËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÔºåÂÆÉÂ∞ÜÂõæÂÉèÂ§çÊùÇÂ∫¶ÂíåÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄßÁªìÂêàËµ∑Êù•ÔºåÁî®‰∫éÁ°ÆÂÆöÊúÄ‰Ω≥ÂàÜËæ®Áéá„ÄÇÂõæÂÉèÂ§çÊùÇÂ∫¶ÂèØ‰ª•‰ΩøÁî®ÂõæÂÉèÁöÑÊ¢ØÂ∫¶ÂπÖÂ∫¶ÊàñÁÜµÊù•Ë°°Èáè„ÄÇÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄßÂèØ‰ª•‰ΩøÁî®Ê®°ÂûãËæìÂá∫ÁöÑÊñπÂ∑ÆÊàñÁÜµÊù•Ë°°Èáè„ÄÇÂèÇÊï∞È´òÊïàÂæÆË∞ÉÊäÄÊúØÈááÁî®‰∫Ü‰∏ÄÁßçÁ±ª‰ºº‰∫éAdapterÁöÑÁªìÊûÑÔºåÂè™Ë∞ÉÊï¥Â∞ëÈáèÂèÇÊï∞Ôºå‰ªéËÄåÈÅøÂÖç‰∫ÜÂØπÊï¥‰∏™Ê®°ÂûãËøõË°åÂæÆË∞ÉÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÂíåËøáÊãüÂêàÁöÑÈ£éÈô©„ÄÇÊçüÂ§±ÂáΩÊï∞ÈÄöÂ∏∏ÈááÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÊàñÂØπÊØîÊçüÂ§±Ôºå‰ª•ÈºìÂä±Ê®°ÂûãÂ≠¶‰π†Âà∞Êõ¥Â•ΩÁöÑËßÜËßâ-ËØ≠Ë®ÄË°®Á§∫„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™ËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏äÔºåËØ•ÊñπÊ≥ïÁõ∏ÊØî‰∫éÂü∫Á∫øÊñπÊ≥ïÊèêÂçá‰∫Ü5%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÂú®ÂõæÂÉèÊèèËø∞‰ªªÂä°‰∏äÔºåËØ•ÊñπÊ≥ïÁîüÊàêÁöÑÊèèËø∞Êõ¥Âä†ÂáÜÁ°ÆÂíåËØ¶ÁªÜ„ÄÇÊ≠§Â§ñÔºåÂèÇÊï∞È´òÊïàÂæÆË∞ÉÊäÄÊúØ‰πüÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨Ôºå‰ΩøÂæóËØ•ÊñπÊ≥ïÂèØ‰ª•Âú®ËµÑÊ∫êÊúâÈôêÁöÑËÆæÂ§á‰∏äÈÉ®ÁΩ≤„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°Ôºå‰æãÂ¶ÇÂõæÂÉèÊèèËø∞„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÁõÆÊ†áÊ£ÄÊµãÂíåÂõæÂÉèÂàÜÂâ≤Á≠â„ÄÇÈÄöËøáÈíàÂØπ‰∏çÂêå‰ªªÂä°‰ºòÂåñËæìÂÖ•ÂàÜËæ®ÁéáÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáVLLMÁöÑÊÄßËÉΩÔºå‰ΩøÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Êõ¥Âä†ÊúâÊïà„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÔºåÊèêÈ´òÊú∫Âô®ÂØπÁéØÂ¢ÉÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Real-world vision-language applications demand varying levels of perceptual granularity. However, most existing visual large language models (VLLMs), such as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to subpar performance. To address this problem, we first conduct a comprehensive and pioneering investigation into the resolution preferences of different vision-language tasks, revealing a correlation between resolution preferences with image complexity, and uncertainty variance of the VLLM at different image input resolutions. Building on this insight, we propose an empirical formula to determine the optimal resolution for a given vision-language task, combining these two factors. Second, based on rigorous experiments, we propose a novel parameter-efficient fine-tuning technique to extend the visual input resolution of pre-trained VLLMs to the identified optimal resolution. Extensive experiments on various vision-language tasks validate the effectiveness of our method.

