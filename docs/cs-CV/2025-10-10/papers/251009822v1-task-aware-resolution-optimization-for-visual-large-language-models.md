---
layout: default
title: Task-Aware Resolution Optimization for Visual Large Language Models
---

# Task-Aware Resolution Optimization for Visual Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09822" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09822v1</a>
  <a href="https://arxiv.org/pdf/2510.09822.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09822v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09822v1', 'Task-Aware Resolution Optimization for Visual Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weiqing Luo, Zhen Tan, Yifan Li, Xinyu Zhao, Kwonjoon Lee, Behzad Dariush, Tianlong Chen

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

**å¤‡æ³¨**: Accepted as a main conference paper at EMNLP 2025. 9 pages (main content), 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä»»åŠ¡æ„ŸçŸ¥åˆ†è¾¨ç‡ä¼˜åŒ–æ–¹æ³•ï¼Œæå‡è§†è§‰å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å¤§è¯­è¨€æ¨¡å‹` `åˆ†è¾¨ç‡ä¼˜åŒ–` `ä»»åŠ¡æ„ŸçŸ¥` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `å›¾åƒå¤æ‚åº¦` `æ¨¡å‹ä¸ç¡®å®šæ€§` `è§†è§‰é—®ç­”` `å›¾åƒæè¿°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLLMä¸ºæ‰€æœ‰ä»»åŠ¡é‡‡ç”¨å›ºå®šåˆ†è¾¨ç‡ï¼Œå¿½ç•¥äº†ä¸åŒä»»åŠ¡å¯¹å›¾åƒç»†èŠ‚çš„éœ€æ±‚å·®å¼‚ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
2. æå‡ºä»»åŠ¡æ„ŸçŸ¥çš„åˆ†è¾¨ç‡ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡åˆ†æå›¾åƒå¤æ‚åº¦å’Œæ¨¡å‹ä¸ç¡®å®šæ€§ï¼Œç¡®å®šæ¯ä¸ªä»»åŠ¡çš„æœ€ä½³åˆ†è¾¨ç‡ã€‚
3. è®¾è®¡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œä½¿é¢„è®­ç»ƒVLLMé€‚åº”æ–°çš„åˆ†è¾¨ç‡ï¼Œå¹¶åœ¨å¤šç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸ŠéªŒè¯äº†æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†è§‰å¤§è¯­è¨€æ¨¡å‹(VLLMs)ï¼Œå¦‚LLaVAï¼Œé€šå¸¸ä¸ºä¸‹æ¸¸ä»»åŠ¡é¢„è®¾å›ºå®šçš„åˆ†è¾¨ç‡ï¼Œå¯¼è‡´æ€§èƒ½æ¬ ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹ä¸åŒè§†è§‰-è¯­è¨€ä»»åŠ¡çš„åˆ†è¾¨ç‡åå¥½è¿›è¡Œäº†å…¨é¢è€Œå¼€åˆ›æ€§çš„ç ”ç©¶ï¼Œæ­ç¤ºäº†åˆ†è¾¨ç‡åå¥½ä¸å›¾åƒå¤æ‚æ€§ä»¥åŠVLLMåœ¨ä¸åŒå›¾åƒè¾“å…¥åˆ†è¾¨ç‡ä¸‹çš„ä¸ç¡®å®šæ€§æ–¹å·®ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»éªŒå…¬å¼ï¼Œç»“åˆè¿™ä¸¤ä¸ªå› ç´ æ¥ç¡®å®šç»™å®šè§†è§‰-è¯­è¨€ä»»åŠ¡çš„æœ€ä½³åˆ†è¾¨ç‡ã€‚å…¶æ¬¡ï¼ŒåŸºäºä¸¥æ ¼çš„å®éªŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œå°†é¢„è®­ç»ƒVLLMçš„è§†è§‰è¾“å…¥åˆ†è¾¨ç‡æ‰©å±•åˆ°ç¡®å®šçš„æœ€ä½³åˆ†è¾¨ç‡ã€‚åœ¨å„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰é€šå¸¸é‡‡ç”¨å›ºå®šçš„è¾“å…¥åˆ†è¾¨ç‡ï¼Œæ— æ³•é€‚åº”ä¸åŒè§†è§‰-è¯­è¨€ä»»åŠ¡å¯¹å›¾åƒç»†èŠ‚çš„ä¸åŒéœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œä¸€äº›ä»»åŠ¡å¯èƒ½éœ€è¦é«˜åˆ†è¾¨ç‡ä»¥æ•æ‰ç»†ç²’åº¦çš„ä¿¡æ¯ï¼Œè€Œå¦ä¸€äº›ä»»åŠ¡åˆ™å¯èƒ½åœ¨é«˜åˆ†è¾¨ç‡ä¸‹å¼•å…¥ä¸å¿…è¦çš„å™ªå£°ã€‚è¿™ç§å›ºå®šåˆ†è¾¨ç‡çš„ç­–ç•¥é™åˆ¶äº†VLLMsåœ¨å„ç§å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ ¹æ®ä»»åŠ¡çš„ç‰¹æ€§åŠ¨æ€è°ƒæ•´è¾“å…¥åˆ†è¾¨ç‡ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åˆ†æå›¾åƒçš„å¤æ‚åº¦å’ŒVLLMåœ¨ä¸åŒåˆ†è¾¨ç‡ä¸‹çš„ä¸ç¡®å®šæ€§ï¼Œæ¥ç¡®å®šæ¯ä¸ªä»»åŠ¡çš„æœ€ä½³åˆ†è¾¨ç‡ã€‚å›¾åƒå¤æ‚åº¦åæ˜ äº†ä»»åŠ¡å¯¹ç»†èŠ‚çš„éœ€æ±‚ç¨‹åº¦ï¼Œè€Œä¸ç¡®å®šæ€§åˆ™åæ˜ äº†æ¨¡å‹åœ¨ç‰¹å®šåˆ†è¾¨ç‡ä¸‹çš„ç½®ä¿¡åº¦ã€‚å°†ä¸¤è€…ç»“åˆï¼Œå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡ç‚¹ï¼Œä½¿å¾—æ¨¡å‹æ—¢èƒ½æ•æ‰åˆ°è¶³å¤Ÿçš„ç»†èŠ‚ï¼Œåˆä¸ä¼šå—åˆ°è¿‡å¤šå™ªå£°çš„å¹²æ‰°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯åˆ†è¾¨ç‡åå¥½åˆ†æï¼Œé€šè¿‡å®éªŒç¡®å®šä¸åŒè§†è§‰-è¯­è¨€ä»»åŠ¡çš„æœ€ä½³åˆ†è¾¨ç‡èŒƒå›´ã€‚è¿™ä¸€é˜¶æ®µçš„å…³é”®æ˜¯è®¾è®¡åˆé€‚çš„æŒ‡æ ‡æ¥è¡¡é‡å›¾åƒå¤æ‚åº¦å’Œæ¨¡å‹ä¸ç¡®å®šæ€§ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼Œåˆ©ç”¨å°‘é‡å‚æ•°å°†é¢„è®­ç»ƒçš„VLLMé€‚åº”åˆ°æ–°çš„åˆ†è¾¨ç‡ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œå¯¹ç»™å®šçš„è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œåˆ©ç”¨ç»éªŒå…¬å¼è®¡ç®—å‡ºæœ€ä½³åˆ†è¾¨ç‡ï¼›ç„¶åï¼Œä½¿ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œè°ƒæ•´VLLMçš„è§†è§‰è¾“å…¥å±‚ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ–°çš„åˆ†è¾¨ç‡ï¼›æœ€åï¼Œåœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä»»åŠ¡æ„ŸçŸ¥çš„åˆ†è¾¨ç‡ä¼˜åŒ–ç­–ç•¥ã€‚ä¸ä»¥å¾€çš„å›ºå®šåˆ†è¾¨ç‡æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„ç‰¹æ€§åŠ¨æ€è°ƒæ•´è¾“å…¥åˆ†è¾¨ç‡ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæå‡ºçš„ç»éªŒå…¬å¼å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ä¹Ÿä¸ºå®ç°è¿™ä¸€ç›®æ ‡æä¾›äº†æœ‰æ•ˆçš„æ‰‹æ®µã€‚

**å…³é”®è®¾è®¡**ï¼šç»éªŒå…¬å¼æ˜¯è¯¥æ–¹æ³•çš„æ ¸å¿ƒï¼Œå®ƒå°†å›¾åƒå¤æ‚åº¦å’Œæ¨¡å‹ä¸ç¡®å®šæ€§ç»“åˆèµ·æ¥ï¼Œç”¨äºç¡®å®šæœ€ä½³åˆ†è¾¨ç‡ã€‚å›¾åƒå¤æ‚åº¦å¯ä»¥ä½¿ç”¨å›¾åƒçš„æ¢¯åº¦å¹…åº¦æˆ–ç†µæ¥è¡¡é‡ã€‚æ¨¡å‹ä¸ç¡®å®šæ€§å¯ä»¥ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„æ–¹å·®æˆ–ç†µæ¥è¡¡é‡ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯é‡‡ç”¨äº†ä¸€ç§ç±»ä¼¼äºAdapterçš„ç»“æ„ï¼Œåªè°ƒæ•´å°‘é‡å‚æ•°ï¼Œä»è€Œé¿å…äº†å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†è®¡ç®—æˆæœ¬å’Œè¿‡æ‹Ÿåˆçš„é£é™©ã€‚æŸå¤±å‡½æ•°é€šå¸¸é‡‡ç”¨äº¤å‰ç†µæŸå¤±æˆ–å¯¹æ¯”æŸå¤±ï¼Œä»¥é¼“åŠ±æ¨¡å‹å­¦ä¹ åˆ°æ›´å¥½çš„è§†è§‰-è¯­è¨€è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºåŸºçº¿æ–¹æ³•æå‡äº†5%çš„å‡†ç¡®ç‡ã€‚åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æè¿°æ›´åŠ å‡†ç¡®å’Œè¯¦ç»†ã€‚æ­¤å¤–ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ä¹Ÿæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—è¯¥æ–¹æ³•å¯ä»¥åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ç­‰ã€‚é€šè¿‡é’ˆå¯¹ä¸åŒä»»åŠ¡ä¼˜åŒ–è¾“å…¥åˆ†è¾¨ç‡ï¼Œå¯ä»¥æ˜¾è‘—æå‡VLLMçš„æ€§èƒ½ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œæé«˜æœºå™¨å¯¹ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world vision-language applications demand varying levels of perceptual granularity. However, most existing visual large language models (VLLMs), such as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to subpar performance. To address this problem, we first conduct a comprehensive and pioneering investigation into the resolution preferences of different vision-language tasks, revealing a correlation between resolution preferences with image complexity, and uncertainty variance of the VLLM at different image input resolutions. Building on this insight, we propose an empirical formula to determine the optimal resolution for a given vision-language task, combining these two factors. Second, based on rigorous experiments, we propose a novel parameter-efficient fine-tuning technique to extend the visual input resolution of pre-trained VLLMs to the identified optimal resolution. Extensive experiments on various vision-language tasks validate the effectiveness of our method.

