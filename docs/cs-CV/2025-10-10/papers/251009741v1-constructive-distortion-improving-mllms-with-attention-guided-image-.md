---
layout: default
title: Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping
---

# Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.09741" target="_blank" class="toolbar-btn">arXiv: 2510.09741v1</a>
    <a href="https://arxiv.org/pdf/2510.09741.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09741v1" 
            onclick="toggleFavorite(this, '2510.09741v1', 'Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Dwip Dalal, Gautam Vashishtha, Utkarsh Mishra, Jeonghwan Kim, Madhav Kanda, Hyeonjeong Ha, Svetlana Lazebnik, Heng Ji, Unnat Jain

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫AttWarpÔºåÂà©Áî®Ê≥®ÊÑèÂäõÂºïÂØºÂõæÂÉèÊâ≠Êõ≤ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÂõæÂÉèÊâ≠Êõ≤` `ËßÜËßâÈóÆÁ≠î` `ÁªÜÁ≤íÂ∫¶ÊÑüÁü•`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÂõæÂÉèÊó∂ÔºåÈöæ‰ª•ÊçïÊçâÁªÜÂæÆ‰πãÂ§ÑÂíåÁ©∫Èó¥ÂÖ≥Á≥ªÔºåÂΩ±ÂìçÊÑüÁü•ËÉΩÂäõ„ÄÇ
2. AttWarp Âà©Áî® MLLM ÁöÑË∑®Ê®°ÊÄÅÊ≥®ÊÑèÂäõÔºåÂØπÂõæÂÉèËøõË°åÊâ≠Êõ≤ÔºåÂ∞ÜÊõ¥Â§öÂàÜËæ®ÁéáÂàÜÈÖçÁªôÈáçË¶ÅÂå∫Âüü„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåAttWarp Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØïÂíå MLLM ‰∏äÂùáËÉΩÊèêÂçáÂáÜÁ°ÆÁéáÔºåÂ¢ûÂº∫Êé®ÁêÜËÉΩÂäõÔºåÂáèÂ∞ëÂπªËßâ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLMs)Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠Â∏∏Â∏∏ÂøΩÁï•ÁªÜËäÇÂíåÁ©∫Èó¥ÂÖ≥Á≥ªÔºåÂØºËá¥ÁªÜÁ≤íÂ∫¶ÊÑüÁü• grounding Âá∫Áé∞ÈîôËØØ„ÄÇÊàë‰ª¨ÊèêÂá∫AttWarpÔºå‰∏ÄÁßçËΩªÈáèÁ∫ßÊñπÊ≥ïÔºåÂÆÉÂú®‰øùÁïôÂÖ®Â±Ä‰∏ä‰∏ãÊñáÁöÑÂêåÊó∂Ôºå‰∏∫Êü•ËØ¢Áõ∏ÂÖ≥ÁöÑÂÜÖÂÆπÂàÜÈÖçÊõ¥Â§öÂàÜËæ®ÁéáÔºåÂπ∂ÂéãÁº©‰ø°ÊÅØÈáèËæÉÂ∞ëÁöÑÂå∫Âüü„ÄÇÂú®ÊµãËØïÊó∂ÔºåËØ•ÊñπÊ≥ïÂà©Áî® MLLM ÁöÑË∑®Ê®°ÊÄÅÊ≥®ÊÑèÂäõÂØπËæìÂÖ•ÂõæÂÉèËøõË°åÁ∫øÊÄßÊâ≠Êõ≤ÔºåÂ∞ÜÁ©∫Èó¥ÂàÜËæ®ÁéáÈáçÊñ∞ÂàÜÈÖçÂà∞Ê®°ÂûãËÆ§‰∏∫ÈáçË¶ÅÁöÑÂå∫ÂüüÔºåËÄåÊó†ÈúÄÊõ¥ÊîπÊ®°ÂûãÊùÉÈáçÊàñÊû∂ÊûÑ„ÄÇËøôÁßçÊ≥®ÊÑèÂäõÂºïÂØºÁöÑÊâ≠Êõ≤‰øùÁïô‰∫ÜÊâÄÊúâÂéüÂßãÂõæÂÉè‰ø°ÊÅØÔºå‰ΩÜ‰ª•ÈùûÂùáÂåÄÁöÑÊñπÂºèÈáçÊñ∞ÂàÜÈÖçÔºåÂõ†Ê≠§Áõ∏ÂêåÁöÑÊ®°ÂûãÂèØ‰ª•Êõ¥ÂÆπÊòìÂú∞ËØªÂèñÂ∞èÁâ©‰ΩìÂíåÂæÆÂ¶ôÂÖ≥Á≥ªÔºåÂêåÊó∂ÂÖ®Â±ÄÂ∏ÉÂ±Ä‰øùÊåÅ‰∏çÂèò„ÄÇÂú®‰∫î‰∏™Âü∫ÂáÜÊµãËØïÔºàTextVQA„ÄÅGQA„ÄÅDocVQA„ÄÅPOPE„ÄÅMMMUÔºâÂíåÂõõ‰∏™ MLLMÔºàLLaVA„ÄÅQwen-VL„ÄÅInternVL Âíå InstructBLIPÔºâ‰∏äÔºåAttWarp ÂßãÁªàÊèêÈ´òÂáÜÁ°ÆÊÄßÔºåÂä†Âº∫ÁªÑÂêàÊé®ÁêÜÔºåÂπ∂ÂáèÂ∞ëÂπªËßâÔºå‰ºò‰∫éÂõõÁßçÂú®ÊµãËØïÊó∂Êìç‰ΩúÂéüÂßãÂõæÂÉèÁöÑÁ´û‰∫âÂü∫Á∫ø„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÊ≥®ÊÑèÂäõÂºïÂØºÁöÑÊâ≠Êõ≤‰ºòÂÖàËÄÉËôë‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÔºåÂêåÊó∂‰øùÁïô‰∏ä‰∏ãÊñáÔºåÂπ∂‰∏îÁõ∏ÂêåÁöÑ MLLM Âú®Ëé∑ÂæóËøôÁßçÊâ≠Êõ≤ÁöÑËæìÂÖ•Êó∂Ë°®Áé∞Êõ¥Â•Ω„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÂõæÂÉèÊó∂ÔºåÂ∏∏Â∏∏Âõ†‰∏∫ÂøΩÁï•ÂõæÂÉè‰∏≠ÁöÑÂ∞èÁªÜËäÇÂíåÁ©∫Èó¥ÂÖ≥Á≥ªÔºåÂØºËá¥Âú®ËßÜËßâÈóÆÁ≠îÁ≠â‰ªªÂä°‰∏≠Âá∫Áé∞ÈîôËØØ„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïË¶Å‰πà‰æùËµñ‰∫éÊõ¥Â§ßÁöÑÊ®°ÂûãÔºåË¶Å‰πàÈúÄË¶ÅÂØπÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏îÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAttWarp ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî® MLLM Ëá™Ë∫´ÂØπÂõæÂÉè‰∏çÂêåÂå∫ÂüüÁöÑÂÖ≥Ê≥®Á®ãÂ∫¶ÔºàÂç≥Ê≥®ÊÑèÂäõÔºâÔºåÂä®ÊÄÅÂú∞Ë∞ÉÊï¥ÂõæÂÉèÁöÑÂàÜËæ®ÁéáÂàÜÂ∏É„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ∞ÜÊ®°ÂûãËÆ§‰∏∫ÈáçË¶ÅÁöÑÂå∫ÂüüÊîæÂ§ßÔºå‰∏çÈáçË¶ÅÁöÑÂå∫ÂüüÁº©Â∞èÔºå‰ªéËÄåËÆ©Ê®°ÂûãÊõ¥ÂÆπÊòìÊçïÊçâÂà∞ÂÖ≥ÈîÆ‰ø°ÊÅØÔºåËÄåÊó†ÈúÄÊîπÂèòÊ®°ÂûãÊú¨Ë∫´ÁöÑÁªìÊûÑÂíåÂèÇÊï∞„ÄÇËøôÊ†∑ËÆæËÆ°ÁöÑÁõÆÁöÑÊòØÂú®‰∏çÂ¢ûÂä†ËÆ°ÁÆóË¥üÊãÖÁöÑÂâçÊèê‰∏ãÔºåÊèêÂçáÊ®°ÂûãÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAttWarp ÁöÑÊï¥‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1. ËæìÂÖ•ÂõæÂÉèÂíåÈóÆÈ¢òÂà∞ MLLMÔºõ2. ‰ªé MLLM ‰∏≠ÊèêÂèñË∑®Ê®°ÊÄÅÊ≥®ÊÑèÂäõÂõæÔºõ3. Âü∫‰∫éÊ≥®ÊÑèÂäõÂõæËÆ°ÁÆóÊâ≠Êõ≤ÂèòÊç¢Ôºõ4. ÂØπËæìÂÖ•ÂõæÂÉèËøõË°åÊâ≠Êõ≤ÔºåÁîüÊàêÊâ≠Êõ≤ÂêéÁöÑÂõæÂÉèÔºõ5. Â∞ÜÊâ≠Êõ≤ÂêéÁöÑÂõæÂÉèËæìÂÖ•Âà∞ MLLM ‰∏≠ËøõË°åÊé®ÁêÜ„ÄÇÊï¥‰∏™ËøáÁ®ãÊòØ‰∏Ä‰∏™ÂâçÂêëËøáÁ®ãÔºå‰∏çÈúÄË¶ÅËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAttWarp ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî® MLLM Ëá™Ë∫´ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•ÊåáÂØºÂõæÂÉèÁöÑÊâ≠Êõ≤ÂèòÊç¢„ÄÇ‰∏é‰º†ÁªüÁöÑÂõæÂÉèÂ§ÑÁêÜÊñπÊ≥ï‰∏çÂêåÔºåAttWarp ‰∏çÊòØÁõ≤ÁõÆÂú∞ÂØπÂõæÂÉèËøõË°åÂ§ÑÁêÜÔºåËÄåÊòØÊ†πÊçÆÊ®°ÂûãÁöÑÈúÄÊ±ÇÔºåÊúâÈíàÂØπÊÄßÂú∞Ë∞ÉÊï¥ÂõæÂÉèÁöÑÂàÜËæ®ÁéáÂàÜÂ∏É„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®Ê®°ÂûãÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöAttWarp ‰ΩøÁî®Á∫øÊÄßÊâ≠Êõ≤Ôºàrectilinear warpingÔºâÊù•ÂÆûÁé∞ÂõæÂÉèÁöÑÂèòÂΩ¢„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ∞ÜÂõæÂÉèÂàíÂàÜ‰∏∫ÁΩëÊ†ºÔºåÁÑ∂ÂêéÊ†πÊçÆÊ≥®ÊÑèÂäõÂõæËÆ°ÁÆóÊØè‰∏™ÁΩëÊ†ºÁöÑÂèòÊç¢Áü©Èòµ„ÄÇ‰∏∫‰∫Ü‰øùËØÅÊâ≠Êõ≤ÂêéÁöÑÂõæÂÉè‰ªçÁÑ∂ÂÖ∑ÊúâÂÖ®Â±Ä‰∏ÄËá¥ÊÄßÔºåAttWarp ‰ΩøÁî®‰∫Ü‰∏ÄÁßçÂπ≥ÊªëÁöÑÊèíÂÄºÊñπÊ≥ïÊù•ËÆ°ÁÆóÂèòÊç¢Áü©Èòµ„ÄÇÊ≠§Â§ñÔºåAttWarp ËøòÂèØ‰ª•ÈÄöËøáË∞ÉÊï¥Ê≥®ÊÑèÂäõÂõæÁöÑÈòàÂÄºÊù•ÊéßÂà∂Êâ≠Êõ≤ÁöÑÁ®ãÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

AttWarp Âú®‰∫î‰∏™Âü∫ÂáÜÊµãËØïÔºàTextVQA„ÄÅGQA„ÄÅDocVQA„ÄÅPOPE„ÄÅMMMUÔºâÂíåÂõõ‰∏™ MLLMÔºàLLaVA„ÄÅQwen-VL„ÄÅInternVL Âíå InstructBLIPÔºâ‰∏äÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú® TextVQA ‰∏äÔºåAttWarp Â∞Ü LLaVA ÁöÑÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü X%„ÄÇ‰∏éÂõõÁßçÁ´û‰∫âÂü∫Á∫øÁõ∏ÊØîÔºåAttWarp Âú®ÊâÄÊúâÊµãËØï‰∏≠ÈÉΩË°®Áé∞Âá∫Êõ¥‰ºòÁöÑÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AttWarp ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÁªÜÁ≤íÂ∫¶ËßÜËßâÊÑüÁü•ÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜËßâÈóÆÁ≠î„ÄÅÊñáÊ°£ÁêÜËß£„ÄÅÂõæÂÉèÊèèËø∞Á≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÂçá MLLM Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÁêÜËß£ËÉΩÂäõÔºåÂáèÂ∞ëÈîôËØØÂíåÂπªËßâÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÊú™Êù•ÔºåÂèØ‰ª•Â∞Ü AttWarp Â∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÔºåÊèêÂçáÊú∫Âô®‰∫∫ÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) often miss small details and spatial relations in cluttered scenes, leading to errors in fine-grained perceptual grounding. We introduce AttWarp, a lightweight method that allocates more resolution to query-relevant content while compressing less informative areas, all while preserving global context. At test time, the approach uses an MLLM's cross-modal attention to perform rectilinear warping of the input image, reallocating spatial resolution toward regions the model deems important, without changing model weights or architecture. This attention-guided warping preserves all original image information but redistributes it non-uniformly, so small objects and subtle relationships become easier for the same model to read while the global layout remains intact. Across five benchmarks (TextVQA, GQA, DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and InstructBLIP), AttWarp consistently improves accuracy, strengthens compositional reasoning, and reduces hallucinations, outperforming four competitive baselines that manipulate raw images at test time. Together, these results show that attention-guided warping prioritizes information relevant to the query while preserving context, and that the same MLLMs perform better when given such warped inputs.

