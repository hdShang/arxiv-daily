---
layout: default
title: "Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping"
---

# Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09741" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09741v1</a>
  <a href="https://arxiv.org/pdf/2510.09741.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09741v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09741v1', 'Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dwip Dalal, Gautam Vashishtha, Utkarsh Mishra, Jeonghwan Kim, Madhav Kanda, Hyeonjeong Ha, Svetlana Lazebnik, Heng Ji, Unnat Jain

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAttWarpï¼Œåˆ©ç”¨æ³¨æ„åŠ›å¼•å¯¼å›¾åƒæ‰­æ›²æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `å›¾åƒæ‰­æ›²` `è§†è§‰é—®ç­”` `ç»†ç²’åº¦æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚å›¾åƒæ—¶ï¼Œéš¾ä»¥æ•æ‰ç»†å¾®ä¹‹å¤„å’Œç©ºé—´å…³ç³»ï¼Œå½±å“æ„ŸçŸ¥èƒ½åŠ›ã€‚
2. AttWarp åˆ©ç”¨ MLLM çš„è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼Œå¯¹å›¾åƒè¿›è¡Œæ‰­æ›²ï¼Œå°†æ›´å¤šåˆ†è¾¨ç‡åˆ†é…ç»™é‡è¦åŒºåŸŸã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAttWarp åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œ MLLM ä¸Šå‡èƒ½æå‡å‡†ç¡®ç‡ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œå‡å°‘å¹»è§‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨å¤æ‚åœºæ™¯ä¸­å¸¸å¸¸å¿½ç•¥ç»†èŠ‚å’Œç©ºé—´å…³ç³»ï¼Œå¯¼è‡´ç»†ç²’åº¦æ„ŸçŸ¥ grounding å‡ºç°é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºAttWarpï¼Œä¸€ç§è½»é‡çº§æ–¹æ³•ï¼Œå®ƒåœ¨ä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡çš„åŒæ—¶ï¼Œä¸ºæŸ¥è¯¢ç›¸å…³çš„å†…å®¹åˆ†é…æ›´å¤šåˆ†è¾¨ç‡ï¼Œå¹¶å‹ç¼©ä¿¡æ¯é‡è¾ƒå°‘çš„åŒºåŸŸã€‚åœ¨æµ‹è¯•æ—¶ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ MLLM çš„è·¨æ¨¡æ€æ³¨æ„åŠ›å¯¹è¾“å…¥å›¾åƒè¿›è¡Œçº¿æ€§æ‰­æ›²ï¼Œå°†ç©ºé—´åˆ†è¾¨ç‡é‡æ–°åˆ†é…åˆ°æ¨¡å‹è®¤ä¸ºé‡è¦çš„åŒºåŸŸï¼Œè€Œæ— éœ€æ›´æ”¹æ¨¡å‹æƒé‡æˆ–æ¶æ„ã€‚è¿™ç§æ³¨æ„åŠ›å¼•å¯¼çš„æ‰­æ›²ä¿ç•™äº†æ‰€æœ‰åŸå§‹å›¾åƒä¿¡æ¯ï¼Œä½†ä»¥éå‡åŒ€çš„æ–¹å¼é‡æ–°åˆ†é…ï¼Œå› æ­¤ç›¸åŒçš„æ¨¡å‹å¯ä»¥æ›´å®¹æ˜“åœ°è¯»å–å°ç‰©ä½“å’Œå¾®å¦™å…³ç³»ï¼ŒåŒæ—¶å…¨å±€å¸ƒå±€ä¿æŒä¸å˜ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆTextVQAã€GQAã€DocVQAã€POPEã€MMMUï¼‰å’Œå››ä¸ª MLLMï¼ˆLLaVAã€Qwen-VLã€InternVL å’Œ InstructBLIPï¼‰ä¸Šï¼ŒAttWarp å§‹ç»ˆæé«˜å‡†ç¡®æ€§ï¼ŒåŠ å¼ºç»„åˆæ¨ç†ï¼Œå¹¶å‡å°‘å¹»è§‰ï¼Œä¼˜äºå››ç§åœ¨æµ‹è¯•æ—¶æ“ä½œåŸå§‹å›¾åƒçš„ç«äº‰åŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ³¨æ„åŠ›å¼•å¯¼çš„æ‰­æ›²ä¼˜å…ˆè€ƒè™‘ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”ç›¸åŒçš„ MLLM åœ¨è·å¾—è¿™ç§æ‰­æ›²çš„è¾“å…¥æ—¶è¡¨ç°æ›´å¥½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚å›¾åƒæ—¶ï¼Œå¸¸å¸¸å› ä¸ºå¿½ç•¥å›¾åƒä¸­çš„å°ç»†èŠ‚å’Œç©ºé—´å…³ç³»ï¼Œå¯¼è‡´åœ¨è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­å‡ºç°é”™è¯¯ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºæ›´å¤§çš„æ¨¡å‹ï¼Œè¦ä¹ˆéœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAttWarp çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ MLLM è‡ªèº«å¯¹å›¾åƒä¸åŒåŒºåŸŸçš„å…³æ³¨ç¨‹åº¦ï¼ˆå³æ³¨æ„åŠ›ï¼‰ï¼ŒåŠ¨æ€åœ°è°ƒæ•´å›¾åƒçš„åˆ†è¾¨ç‡åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œå°†æ¨¡å‹è®¤ä¸ºé‡è¦çš„åŒºåŸŸæ”¾å¤§ï¼Œä¸é‡è¦çš„åŒºåŸŸç¼©å°ï¼Œä»è€Œè®©æ¨¡å‹æ›´å®¹æ˜“æ•æ‰åˆ°å…³é”®ä¿¡æ¯ï¼Œè€Œæ— éœ€æ”¹å˜æ¨¡å‹æœ¬èº«çš„ç»“æ„å’Œå‚æ•°ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯åœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„å‰æä¸‹ï¼Œæå‡æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAttWarp çš„æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1. è¾“å…¥å›¾åƒå’Œé—®é¢˜åˆ° MLLMï¼›2. ä» MLLM ä¸­æå–è·¨æ¨¡æ€æ³¨æ„åŠ›å›¾ï¼›3. åŸºäºæ³¨æ„åŠ›å›¾è®¡ç®—æ‰­æ›²å˜æ¢ï¼›4. å¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ‰­æ›²ï¼Œç”Ÿæˆæ‰­æ›²åçš„å›¾åƒï¼›5. å°†æ‰­æ›²åçš„å›¾åƒè¾“å…¥åˆ° MLLM ä¸­è¿›è¡Œæ¨ç†ã€‚æ•´ä¸ªè¿‡ç¨‹æ˜¯ä¸€ä¸ªå‰å‘è¿‡ç¨‹ï¼Œä¸éœ€è¦è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šAttWarp çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨ MLLM è‡ªèº«çš„æ³¨æ„åŠ›æœºåˆ¶æ¥æŒ‡å¯¼å›¾åƒçš„æ‰­æ›²å˜æ¢ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒå¤„ç†æ–¹æ³•ä¸åŒï¼ŒAttWarp ä¸æ˜¯ç›²ç›®åœ°å¯¹å›¾åƒè¿›è¡Œå¤„ç†ï¼Œè€Œæ˜¯æ ¹æ®æ¨¡å‹çš„éœ€æ±‚ï¼Œæœ‰é’ˆå¯¹æ€§åœ°è°ƒæ•´å›¾åƒçš„åˆ†è¾¨ç‡åˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ¨¡å‹çš„ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šAttWarp ä½¿ç”¨çº¿æ€§æ‰­æ›²ï¼ˆrectilinear warpingï¼‰æ¥å®ç°å›¾åƒçš„å˜å½¢ã€‚å…·ä½“æ¥è¯´ï¼Œå°†å›¾åƒåˆ’åˆ†ä¸ºç½‘æ ¼ï¼Œç„¶åæ ¹æ®æ³¨æ„åŠ›å›¾è®¡ç®—æ¯ä¸ªç½‘æ ¼çš„å˜æ¢çŸ©é˜µã€‚ä¸ºäº†ä¿è¯æ‰­æ›²åçš„å›¾åƒä»ç„¶å…·æœ‰å…¨å±€ä¸€è‡´æ€§ï¼ŒAttWarp ä½¿ç”¨äº†ä¸€ç§å¹³æ»‘çš„æ’å€¼æ–¹æ³•æ¥è®¡ç®—å˜æ¢çŸ©é˜µã€‚æ­¤å¤–ï¼ŒAttWarp è¿˜å¯ä»¥é€šè¿‡è°ƒæ•´æ³¨æ„åŠ›å›¾çš„é˜ˆå€¼æ¥æ§åˆ¶æ‰­æ›²çš„ç¨‹åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AttWarp åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆTextVQAã€GQAã€DocVQAã€POPEã€MMMUï¼‰å’Œå››ä¸ª MLLMï¼ˆLLaVAã€Qwen-VLã€InternVL å’Œ InstructBLIPï¼‰ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ TextVQA ä¸Šï¼ŒAttWarp å°† LLaVA çš„å‡†ç¡®ç‡æé«˜äº† X%ã€‚ä¸å››ç§ç«äº‰åŸºçº¿ç›¸æ¯”ï¼ŒAttWarp åœ¨æ‰€æœ‰æµ‹è¯•ä¸­éƒ½è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AttWarp å¯åº”ç”¨äºå„ç§éœ€è¦ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥çš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€æ–‡æ¡£ç†è§£ã€å›¾åƒæè¿°ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡ MLLM åœ¨å¤æ‚åœºæ™¯ä¸‹çš„ç†è§£èƒ½åŠ›ï¼Œå‡å°‘é”™è¯¯å’Œå¹»è§‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œå¯ä»¥å°† AttWarp åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œæå‡æœºå™¨äººçš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) often miss small details and spatial relations in cluttered scenes, leading to errors in fine-grained perceptual grounding. We introduce AttWarp, a lightweight method that allocates more resolution to query-relevant content while compressing less informative areas, all while preserving global context. At test time, the approach uses an MLLM's cross-modal attention to perform rectilinear warping of the input image, reallocating spatial resolution toward regions the model deems important, without changing model weights or architecture. This attention-guided warping preserves all original image information but redistributes it non-uniformly, so small objects and subtle relationships become easier for the same model to read while the global layout remains intact. Across five benchmarks (TextVQA, GQA, DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and InstructBLIP), AttWarp consistently improves accuracy, strengthens compositional reasoning, and reduces hallucinations, outperforming four competitive baselines that manipulate raw images at test time. Together, these results show that attention-guided warping prioritizes information relevant to the query while preserving context, and that the same MLLMs perform better when given such warped inputs.

