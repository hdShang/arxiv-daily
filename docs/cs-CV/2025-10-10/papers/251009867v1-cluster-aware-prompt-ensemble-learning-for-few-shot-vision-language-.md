---
layout: default
title: Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation
---

# Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.09867" target="_blank" class="toolbar-btn">arXiv: 2510.09867v1</a>
    <a href="https://arxiv.org/pdf/2510.09867.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09867v1" 
            onclick="toggleFavorite(this, '2510.09867v1', 'Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhi Chen, Xin Yu, Xiaohui Tao, Yan Li, Zi Huang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10

**Â§áÊ≥®**: Accepted to the journal Pattern Recognition in 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ËÅöÁ±ªÊÑüÁü•ÁöÑÊèêÁ§∫ÈõÜÊàêÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊèêÂçáÂ∞ëÊ†∑Êú¨ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÄÇÂ∫îÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Â∞ëÊ†∑Êú¨Â≠¶‰π†` `ÊèêÁ§∫Â≠¶‰π†` `ËÅöÁ±ªÂàÜÊûê` `logitsÈõÜÊàê` `Ê≠£ÂàôÂåñ` `Ëá™ÈÄÇÂ∫îÂä†ÊùÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÊèêÁ§∫ÈõÜÊàêÈÄöËøáÂπ≥ÂùáÊñáÊú¨ÁâπÂæÅÔºåÊòìÂØºËá¥Á±ª‰∏≠ÂøÉÂÅèÁßªÔºåÂΩ±ÂìçÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇ
2. CAPELÊ°ÜÊû∂ÈÄöËøáÂú®logitsÁ©∫Èó¥ÈõÜÊàêÊèêÁ§∫ÔºåÂπ∂ÂºïÂÖ•ËÅöÁ±ª‰øùÊåÅÊ≠£ÂàôÂåñÔºåÁª¥ÊåÅÁ∞áÁöÑÂå∫ÂàÜÊÄß„ÄÇ
3. Ëá™ÈÄÇÂ∫îÊèêÁ§∫Âä†ÊùÉÂä®ÊÄÅË∞ÉÊï¥ÊùÉÈáçÔºåÊèêÂçáÊ®°ÂûãÂú®Â§çÊùÇÊï∞ÊçÆÈõÜ‰∏äÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

CLIPÁ≠âËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)ÈÄöËøáÂú®Â§ßÈáèÂõæÂÉè-ÊñáÊú¨ÂØπ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂÆûÁé∞‰∫ÜË∑®ÂêÑÁßç‰ªªÂä°ÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßª„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÂèóÁõä‰∫é‰ΩøÁî®‰∏ä‰∏ãÊñáÊèêÁ§∫ÁöÑÈõÜÊàêÊù•Ë°®Á§∫‰∏Ä‰∏™Á±ªÂà´„ÄÇÂ∞ΩÁÆ°‰º†ÁªüÁöÑÊèêÁ§∫ÈõÜÊàêÊñπÊ≥ïÔºàÂç≥Âπ≥Âùá‰∏ä‰∏ãÊñáÊèêÁ§∫ÁöÑÊñáÊú¨ÁâπÂæÅÔºâÊúâÊïàÔºå‰ΩÜÂ∏∏Â∏∏‰∫ßÁîüÊ¨°‰ºòÁªìÊûúÔºåÂõ†‰∏∫ÁâπÂæÅÂπ≥Âùá‰ºöÂ∞ÜÁ±ª‰∏≠ÂøÉ‰ªéÁúüÂÆûÁöÑÁ±ªÂàÜÂ∏É‰∏≠ÁßªÂºÄ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËÅöÁ±ªÊÑüÁü•ÁöÑÊèêÁ§∫ÈõÜÊàêÂ≠¶‰π†(CAPEL)Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂‰øùÁïô‰∫Ü‰∏ä‰∏ãÊñáÊèêÁ§∫ÁöÑËÅöÁ±ªÊÄßË¥®„ÄÇCAPELÂ∞ÜÂõæÂÉèÂàÜÁ±ªÂà∞Â§ö‰∏™Á±ªÁ∞á‰∏≠ÁöÑ‰∏Ä‰∏™ÔºåÊØè‰∏™Á±ªÁ∞áÁî±‰∏Ä‰∏™‰∏çÂêåÁöÑÊèêÁ§∫Ë°®Á§∫„ÄÇÊàë‰ª¨‰∏çÂú®ÁâπÂæÅÁ©∫Èó¥‰∏≠ÈõÜÊàêÊèêÁ§∫ÔºåËÄåÊòØÂú®ÂàÜÁ±ªlogitsÁ©∫Èó¥‰∏≠ÊâßË°åÈõÜÊàêÔºåËøô‰∏éËßÜËßâÁâπÂæÅÂàÜÂ∏ÉÊõ¥Â•ΩÂú∞ÂØπÈΩê„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•‰ºòÂåñÊèêÁ§∫ÂæÆË∞ÉÔºåÂêåÊó∂‰øùÊåÅÁâπÂÆö‰∫éÁ∞áÁöÑÂå∫ÂàÜËÉΩÂäõÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Á∞á‰øùÊåÅÊ≠£ÂàôÂåñÈ°π„ÄÇËøôÁ°Æ‰øù‰∫ÜÊèêÁ§∫‰øùÊåÅ‰∏çÂêåÔºåÂπ∂‰∏ìÈó®Áî®‰∫é‰∏çÂêåÁöÑÁ∞áÔºåÈò≤Ê≠¢ÂùçÁº©Êàê‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊñπÂêë„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÈõÜÊàê‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÊèêÁ§∫Âä†ÊùÉÊäÄÊúØÔºå‰ª•Âä®ÊÄÅË∞ÉÊï¥ÊúâÁº∫Èô∑ÊàñÊ®°Á≥äÊèêÁ§∫ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÔºåÁ°Æ‰øùÂú®‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíå‰ªªÂä°‰∏≠ÂÆûÁé∞Á®≥ÂÅ•ÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÂ¶ÇCLIPÔºåÂú®Â∞ëÊ†∑Êú¨Â≠¶‰π†‰∏≠‰æùËµñ‰∫éÊèêÁ§∫Â∑•Á®ã„ÄÇ‰º†ÁªüÁöÑÊèêÁ§∫ÈõÜÊàêÊñπÊ≥ïÔºå‰æãÂ¶ÇÁÆÄÂçïÂú∞Âπ≥Âùá‰∏çÂêåÊèêÁ§∫ÁöÑÊñáÊú¨ÁâπÂæÅÔºå‰ºö‰ΩøÂæóÁ±ªÂà´ÁöÑË°®Á§∫ÂÅèÁ¶ªÁúüÂÆûÂàÜÂ∏ÉÔºåÂØºËá¥ÂàÜÁ±ªÊÄßËÉΩ‰∏ãÈôç„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Á±ªÂà´ÂÜÖÈÉ®Â≠òÂú®Â§ö‰∏™Â≠êÁ∞áÁöÑÊÉÖÂÜµ‰∏ãÔºåËøôÁßçÂπ≥ÂùáÊìç‰Ωú‰ºöÊ®°Á≥ä‰∏çÂêåÂ≠êÁ∞áÁöÑÁâπÂæÅÔºå‰ªéËÄåÂΩ±ÂìçÊ®°ÂûãÁöÑÂà§Âà´ËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCAPELÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØ‰øùÁïôÊèêÁ§∫ÁöÑËÅöÁ±ªÁªìÊûÑÔºåÈÅøÂÖçÁõ¥Êé•Âú®ÁâπÂæÅÁ©∫Èó¥ËøõË°åÂπ≥Âùá„ÄÇÂÆÉÂ∞ÜÊØè‰∏™Á±ªÂà´ËßÜ‰∏∫Â§ö‰∏™Â≠êÁ∞áÁöÑÈõÜÂêàÔºåÊØè‰∏™Â≠êÁ∞áÂØπÂ∫î‰∏Ä‰∏™ÁâπÂÆöÁöÑÊèêÁ§∫„ÄÇÈÄöËøáÂú®logitsÁ©∫Èó¥ËøõË°åÈõÜÊàêÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îËßÜËßâÁâπÂæÅÁöÑÂàÜÂ∏ÉÔºå‰ªéËÄåÊèêÈ´òÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•ËÅöÁ±ª‰øùÊåÅÊ≠£ÂàôÂåñÈ°πÔºåÈò≤Ê≠¢ÊèêÁ§∫ÂùçÁº©Âà∞Âêå‰∏ÄÊñπÂêëÔºå‰øùËØÅÊØè‰∏™ÊèêÁ§∫ÁöÑÁã¨ÁâπÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCAPELÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™ÂÖ≥ÈîÆÊ®°ÂùóÔºö1) ËÅöÁ±ªÊÑüÁü•ÁöÑÊèêÁ§∫ÈÄâÊã©ÔºöÂ∞ÜÊØè‰∏™Á±ªÂà´ÂàÜËß£‰∏∫Â§ö‰∏™Â≠êÁ∞áÔºåÊØè‰∏™Â≠êÁ∞áÂØπÂ∫î‰∏Ä‰∏™ÊèêÁ§∫„ÄÇ2) LogitsÁ©∫Èó¥ÈõÜÊàêÔºöÂ∞ÜÊØè‰∏™ÊèêÁ§∫ÁöÑlogitsËæìÂá∫ËøõË°åÂä†ÊùÉÂπ≥ÂùáÔºåÂæóÂà∞ÊúÄÁªàÁöÑÂàÜÁ±ªÁªìÊûú„ÄÇ3) ËÅöÁ±ª‰øùÊåÅÊ≠£ÂàôÂåñÔºöÈÄöËøáÊ≠£ÂàôÂåñÈ°πÁ∫¶ÊùüÊèêÁ§∫ÁöÑÊõ¥Êñ∞Ôºå‰øùËØÅÊèêÁ§∫‰πãÈó¥ÁöÑÂå∫ÂàÜÊÄß„ÄÇ4) Ëá™ÈÄÇÂ∫îÊèêÁ§∫Âä†ÊùÉÔºöÊ†πÊçÆÊèêÁ§∫ÁöÑË¥®ÈáèÂä®ÊÄÅË∞ÉÊï¥ÊùÉÈáç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCAPELÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜËÅöÁ±ªÊÑüÁü•ÁöÑÊèêÁ§∫ÈõÜÊàêÊñπÊ≥ïÔºåÊõ¥Â•ΩÂú∞‰øùÁïô‰∫ÜÁ±ªÂà´ÁöÑÂÜÖÈÉ®ÁªìÊûÑ„ÄÇ2) Âú®logitsÁ©∫Èó¥ËøõË°åÈõÜÊàêÔºåÈÅøÂÖç‰∫ÜÁâπÂæÅÂπ≥ÂùáÂ∏¶Êù•ÁöÑ‰ø°ÊÅØÊçüÂ§±„ÄÇ3) ÂºïÂÖ•‰∫ÜËÅöÁ±ª‰øùÊåÅÊ≠£ÂàôÂåñÈ°πÔºåÈò≤Ê≠¢ÊèêÁ§∫ÂùçÁº©Ôºå‰øùËØÅ‰∫ÜÊèêÁ§∫ÁöÑÂ§öÊ†∑ÊÄß„ÄÇ4) ÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îÊèêÁ§∫Âä†ÊùÉÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÂØπÂô™Â£∞ÊèêÁ§∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÅöÁ±ª‰øùÊåÅÊ≠£ÂàôÂåñÈ°πÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆ„ÄÇÂÆÉÈÄöËøáÊúÄÂ∞èÂåñ‰∏çÂêåÁ∞áÁöÑÊèêÁ§∫‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ÔºåÊù•‰øùËØÅÊèêÁ§∫ÁöÑÂå∫ÂàÜÊÄß„ÄÇËá™ÈÄÇÂ∫îÊèêÁ§∫Âä†ÊùÉÊ®°Âùó‰ΩøÁî®‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÊùÉÈáçÂêëÈáèÊù•Âä®ÊÄÅË∞ÉÊï¥ÊØè‰∏™ÊèêÁ§∫ÁöÑÊùÉÈáçÔºåÊùÉÈáçÁöÑÂ§ßÂ∞èÂèñÂÜ≥‰∫éÊèêÁ§∫ÁöÑË¥®Èáè„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨‰∫§ÂèâÁÜµÊçüÂ§±„ÄÅËÅöÁ±ª‰øùÊåÅÊ≠£ÂàôÂåñÊçüÂ§±ÂíåËá™ÈÄÇÂ∫îÊèêÁ§∫Âä†ÊùÉÊçüÂ§±„ÄÇÂÖ∑‰ΩìÁöÑÊùÉÈáçÂèÇÊï∞ÈúÄË¶ÅÊ†πÊçÆÂÆûÈ™åËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCAPELÊ°ÜÊû∂Âú®Â§ö‰∏™Â∞ëÊ†∑Êú¨Â≠¶‰π†Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ImageNetÊï∞ÊçÆÈõÜ‰∏äÔºåCAPELÁõ∏ÊØî‰∫é‰º†ÁªüÁöÑÊèêÁ§∫ÈõÜÊàêÊñπÊ≥ïÔºåTop-1ÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü5%‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºåCAPELÂú®Â§ÑÁêÜÂô™Â£∞Êï∞ÊçÆÂíåÈïøÂ∞æÂàÜÂ∏ÉÊï∞ÊçÆÊó∂ÔºåË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂõæÂÉèÂàÜÁ±ª„ÄÅÁõÆÊ†áÊ£ÄÊµã„ÄÅÂõæÂÉèÊ£ÄÁ¥¢Á≠âËßÜËßâ‰ªªÂä°ÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÊ†áÊ≥®Á®ÄÁº∫ÁöÑÂú∫ÊôØ‰∏ãÔºåËÉΩÂ§üÊúâÊïàÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈÄÇÂ∫îÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅÂ≠¶‰π†‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇËßÜÈ¢ëÁêÜËß£„ÄÅÊñáÊú¨ÁîüÊàêÁ≠âÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across various tasks by pre-training on numerous image-text pairs. These models often benefit from using an ensemble of context prompts to represent a class. Despite being effective, conventional prompt ensembling that averages textual features of context prompts often yields suboptimal results. This is because feature averaging shifts the class centroids away from the true class distribution. To address this issue, we propose the Cluster-Aware Prompt Ensemble Learning (CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL classifies images into one of several class clusters, each represented by a distinct prompt. Instead of ensembling prompts in the feature space, we perform ensembling in the classification logits space, aligning better with the visual feature distribution. To further optimize prompt fine-tuning while maintaining cluster-specific discriminative power, we introduce a cluster-preserving regularization term. This ensures that prompts remain distinct and specialized for different clusters, preventing collapse into a uniform direction. Additionally, we integrate an adaptive prompt weighting technique to dynamically adjust the attention weights for flawed or ambiguous prompts, ensuring robust performance across diverse datasets and tasks.

