---
layout: default
title: Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation
---

# Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09867" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09867v1</a>
  <a href="https://arxiv.org/pdf/2510.09867.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09867v1" onclick="toggleFavorite(this, '2510.09867v1', 'Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhi Chen, Xin Yu, Xiaohui Tao, Yan Li, Zi Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

**å¤‡æ³¨**: Accepted to the journal Pattern Recognition in 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºèšç±»æ„ŸçŸ¥çš„æç¤ºé›†æˆå­¦ä¹ æ¡†æ¶ï¼Œæå‡å°‘æ ·æœ¬è§†è§‰-è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å°‘æ ·æœ¬å­¦ä¹ ` `æç¤ºå­¦ä¹ ` `èšç±»åˆ†æ` `logitsé›†æˆ` `æ­£åˆ™åŒ–` `è‡ªé€‚åº”åŠ æƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæç¤ºé›†æˆé€šè¿‡å¹³å‡æ–‡æœ¬ç‰¹å¾ï¼Œæ˜“å¯¼è‡´ç±»ä¸­å¿ƒåç§»ï¼Œå½±å“åˆ†ç±»ç²¾åº¦ã€‚
2. CAPELæ¡†æ¶é€šè¿‡åœ¨logitsç©ºé—´é›†æˆæç¤ºï¼Œå¹¶å¼•å…¥èšç±»ä¿æŒæ­£åˆ™åŒ–ï¼Œç»´æŒç°‡çš„åŒºåˆ†æ€§ã€‚
3. è‡ªé€‚åº”æç¤ºåŠ æƒåŠ¨æ€è°ƒæ•´æƒé‡ï¼Œæå‡æ¨¡å‹åœ¨å¤æ‚æ•°æ®é›†ä¸Šçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

CLIPç­‰è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)é€šè¿‡åœ¨å¤§é‡å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°äº†è·¨å„ç§ä»»åŠ¡çš„é›¶æ ·æœ¬è¿ç§»ã€‚è¿™äº›æ¨¡å‹é€šå¸¸å—ç›Šäºä½¿ç”¨ä¸Šä¸‹æ–‡æç¤ºçš„é›†æˆæ¥è¡¨ç¤ºä¸€ä¸ªç±»åˆ«ã€‚å°½ç®¡ä¼ ç»Ÿçš„æç¤ºé›†æˆæ–¹æ³•ï¼ˆå³å¹³å‡ä¸Šä¸‹æ–‡æç¤ºçš„æ–‡æœ¬ç‰¹å¾ï¼‰æœ‰æ•ˆï¼Œä½†å¸¸å¸¸äº§ç”Ÿæ¬¡ä¼˜ç»“æœï¼Œå› ä¸ºç‰¹å¾å¹³å‡ä¼šå°†ç±»ä¸­å¿ƒä»çœŸå®çš„ç±»åˆ†å¸ƒä¸­ç§»å¼€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†èšç±»æ„ŸçŸ¥çš„æç¤ºé›†æˆå­¦ä¹ (CAPEL)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¿ç•™äº†ä¸Šä¸‹æ–‡æç¤ºçš„èšç±»æ€§è´¨ã€‚CAPELå°†å›¾åƒåˆ†ç±»åˆ°å¤šä¸ªç±»ç°‡ä¸­çš„ä¸€ä¸ªï¼Œæ¯ä¸ªç±»ç°‡ç”±ä¸€ä¸ªä¸åŒçš„æç¤ºè¡¨ç¤ºã€‚æˆ‘ä»¬ä¸åœ¨ç‰¹å¾ç©ºé—´ä¸­é›†æˆæç¤ºï¼Œè€Œæ˜¯åœ¨åˆ†ç±»logitsç©ºé—´ä¸­æ‰§è¡Œé›†æˆï¼Œè¿™ä¸è§†è§‰ç‰¹å¾åˆ†å¸ƒæ›´å¥½åœ°å¯¹é½ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æç¤ºå¾®è°ƒï¼ŒåŒæ—¶ä¿æŒç‰¹å®šäºç°‡çš„åŒºåˆ†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç°‡ä¿æŒæ­£åˆ™åŒ–é¡¹ã€‚è¿™ç¡®ä¿äº†æç¤ºä¿æŒä¸åŒï¼Œå¹¶ä¸“é—¨ç”¨äºä¸åŒçš„ç°‡ï¼Œé˜²æ­¢åç¼©æˆä¸€ä¸ªç»Ÿä¸€çš„æ–¹å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†ä¸€ç§è‡ªé€‚åº”æç¤ºåŠ æƒæŠ€æœ¯ï¼Œä»¥åŠ¨æ€è°ƒæ•´æœ‰ç¼ºé™·æˆ–æ¨¡ç³Šæç¤ºçš„æ³¨æ„åŠ›æƒé‡ï¼Œç¡®ä¿åœ¨ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡ä¸­å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¦‚CLIPï¼Œåœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­ä¾èµ–äºæç¤ºå·¥ç¨‹ã€‚ä¼ ç»Ÿçš„æç¤ºé›†æˆæ–¹æ³•ï¼Œä¾‹å¦‚ç®€å•åœ°å¹³å‡ä¸åŒæç¤ºçš„æ–‡æœ¬ç‰¹å¾ï¼Œä¼šä½¿å¾—ç±»åˆ«çš„è¡¨ç¤ºåç¦»çœŸå®åˆ†å¸ƒï¼Œå¯¼è‡´åˆ†ç±»æ€§èƒ½ä¸‹é™ã€‚å°¤å…¶æ˜¯åœ¨ç±»åˆ«å†…éƒ¨å­˜åœ¨å¤šä¸ªå­ç°‡çš„æƒ…å†µä¸‹ï¼Œè¿™ç§å¹³å‡æ“ä½œä¼šæ¨¡ç³Šä¸åŒå­ç°‡çš„ç‰¹å¾ï¼Œä»è€Œå½±å“æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCAPELçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¿ç•™æç¤ºçš„èšç±»ç»“æ„ï¼Œé¿å…ç›´æ¥åœ¨ç‰¹å¾ç©ºé—´è¿›è¡Œå¹³å‡ã€‚å®ƒå°†æ¯ä¸ªç±»åˆ«è§†ä¸ºå¤šä¸ªå­ç°‡çš„é›†åˆï¼Œæ¯ä¸ªå­ç°‡å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„æç¤ºã€‚é€šè¿‡åœ¨logitsç©ºé—´è¿›è¡Œé›†æˆï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”è§†è§‰ç‰¹å¾çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜åˆ†ç±»ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå¼•å…¥èšç±»ä¿æŒæ­£åˆ™åŒ–é¡¹ï¼Œé˜²æ­¢æç¤ºåç¼©åˆ°åŒä¸€æ–¹å‘ï¼Œä¿è¯æ¯ä¸ªæç¤ºçš„ç‹¬ç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCAPELæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼š1) èšç±»æ„ŸçŸ¥çš„æç¤ºé€‰æ‹©ï¼šå°†æ¯ä¸ªç±»åˆ«åˆ†è§£ä¸ºå¤šä¸ªå­ç°‡ï¼Œæ¯ä¸ªå­ç°‡å¯¹åº”ä¸€ä¸ªæç¤ºã€‚2) Logitsç©ºé—´é›†æˆï¼šå°†æ¯ä¸ªæç¤ºçš„logitsè¾“å‡ºè¿›è¡ŒåŠ æƒå¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„åˆ†ç±»ç»“æœã€‚3) èšç±»ä¿æŒæ­£åˆ™åŒ–ï¼šé€šè¿‡æ­£åˆ™åŒ–é¡¹çº¦æŸæç¤ºçš„æ›´æ–°ï¼Œä¿è¯æç¤ºä¹‹é—´çš„åŒºåˆ†æ€§ã€‚4) è‡ªé€‚åº”æç¤ºåŠ æƒï¼šæ ¹æ®æç¤ºçš„è´¨é‡åŠ¨æ€è°ƒæ•´æƒé‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCAPELçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†èšç±»æ„ŸçŸ¥çš„æç¤ºé›†æˆæ–¹æ³•ï¼Œæ›´å¥½åœ°ä¿ç•™äº†ç±»åˆ«çš„å†…éƒ¨ç»“æ„ã€‚2) åœ¨logitsç©ºé—´è¿›è¡Œé›†æˆï¼Œé¿å…äº†ç‰¹å¾å¹³å‡å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚3) å¼•å…¥äº†èšç±»ä¿æŒæ­£åˆ™åŒ–é¡¹ï¼Œé˜²æ­¢æç¤ºåç¼©ï¼Œä¿è¯äº†æç¤ºçš„å¤šæ ·æ€§ã€‚4) æå‡ºäº†è‡ªé€‚åº”æç¤ºåŠ æƒæ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹å¯¹å™ªå£°æç¤ºçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šèšç±»ä¿æŒæ­£åˆ™åŒ–é¡¹çš„è®¾è®¡æ˜¯å…³é”®ã€‚å®ƒé€šè¿‡æœ€å°åŒ–ä¸åŒç°‡çš„æç¤ºä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œæ¥ä¿è¯æç¤ºçš„åŒºåˆ†æ€§ã€‚è‡ªé€‚åº”æç¤ºåŠ æƒæ¨¡å—ä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„æƒé‡å‘é‡æ¥åŠ¨æ€è°ƒæ•´æ¯ä¸ªæç¤ºçš„æƒé‡ï¼Œæƒé‡çš„å¤§å°å–å†³äºæç¤ºçš„è´¨é‡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬äº¤å‰ç†µæŸå¤±ã€èšç±»ä¿æŒæ­£åˆ™åŒ–æŸå¤±å’Œè‡ªé€‚åº”æç¤ºåŠ æƒæŸå¤±ã€‚å…·ä½“çš„æƒé‡å‚æ•°éœ€è¦æ ¹æ®å®éªŒè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCAPELæ¡†æ¶åœ¨å¤šä¸ªå°‘æ ·æœ¬å­¦ä¹ æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼ŒCAPELç›¸æ¯”äºä¼ ç»Ÿçš„æç¤ºé›†æˆæ–¹æ³•ï¼ŒTop-1å‡†ç¡®ç‡æå‡äº†5%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒCAPELåœ¨å¤„ç†å™ªå£°æ•°æ®å’Œé•¿å°¾åˆ†å¸ƒæ•°æ®æ—¶ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒæ£€ç´¢ç­‰è§†è§‰ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æ ‡æ³¨ç¨€ç¼ºçš„åœºæ™¯ä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€æ–‡æœ¬ç”Ÿæˆç­‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across various tasks by pre-training on numerous image-text pairs. These models often benefit from using an ensemble of context prompts to represent a class. Despite being effective, conventional prompt ensembling that averages textual features of context prompts often yields suboptimal results. This is because feature averaging shifts the class centroids away from the true class distribution. To address this issue, we propose the Cluster-Aware Prompt Ensemble Learning (CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL classifies images into one of several class clusters, each represented by a distinct prompt. Instead of ensembling prompts in the feature space, we perform ensembling in the classification logits space, aligning better with the visual feature distribution. To further optimize prompt fine-tuning while maintaining cluster-specific discriminative power, we introduce a cluster-preserving regularization term. This ensures that prompts remain distinct and specialized for different clusters, preventing collapse into a uniform direction. Additionally, we integrate an adaptive prompt weighting technique to dynamically adjust the attention weights for flawed or ambiguous prompts, ensuring robust performance across diverse datasets and tasks.

