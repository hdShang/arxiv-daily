---
layout: default
title: "Vision Language Models: A Survey of 26K Papers"
---

# Vision Language Models: A Survey of 26K Papers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09586" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09586v1</a>
  <a href="https://arxiv.org/pdf/2510.09586.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09586v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09586v1', 'Vision Language Models: A Survey of 26K Papers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fengming Lin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

**å¤‡æ³¨**: VLM/LLM Learning Notes

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶è¶‹åŠ¿åˆ†æï¼šåŸºäº2.6ä¸‡ç¯‡è®ºæ–‡çš„ç»¼åˆè°ƒç ”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç ”ç©¶è¶‹åŠ¿åˆ†æ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `è®¡ç®—æœºè§†è§‰` `å¤§è§„æ¨¡æ•°æ®åˆ†æ` `æŒ‡ä»¤è°ƒä¼˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶ç¼ºä¹å¤§è§„æ¨¡ã€ç³»ç»Ÿæ€§çš„è¶‹åŠ¿åˆ†æï¼Œéš¾ä»¥æŠŠæ¡é¢†åŸŸå‘å±•æ–¹å‘ã€‚
2. æœ¬æ–‡æ„å»ºäº†åŒ…å«2.6ä¸‡ç¯‡è®ºæ–‡çš„çŸ¥è¯†å›¾è°±ï¼Œå¹¶è®¾è®¡äº†è‡ªåŠ¨åŒ–çš„ä¸»é¢˜æ ‡ç­¾åˆ†é…å’Œè¶‹åŠ¿æŒ–æ˜æ–¹æ³•ã€‚
3. é€šè¿‡å¯¹ä¸‰å¤§é¡¶ä¼šè®ºæ–‡çš„åˆ†æï¼Œæ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹é¢†åŸŸåœ¨å¤šæ¨¡æ€èåˆã€ç”Ÿæˆæ–¹æ³•å’Œ3D/è§†é¢‘ç†è§£ç­‰æ–¹é¢çš„å…³é”®è¶‹åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¯¹CVPRã€ICLRå’ŒNeurIPSä¸‰å¤§é¡¶ä¼š2023-2025å¹´é—´æ”¶å½•çš„26104ç¯‡è®ºæ–‡è¿›è¡Œäº†é€æ˜ä¸”å¯å¤ç°çš„ç ”ç©¶è¶‹åŠ¿æµ‹é‡ã€‚é€šè¿‡è§„èŒƒåŒ–æ ‡é¢˜å’Œæ‘˜è¦ï¼Œå¹¶ä½¿ç”¨æ‰‹å·¥æ„å»ºçš„è¯å…¸è¿›è¡ŒçŸ­è¯­ä¿æŠ¤å’ŒåŒ¹é…ï¼Œä¸ºè®ºæ–‡åˆ†é…å¤šè¾¾35ä¸ªä¸»é¢˜æ ‡ç­¾ï¼Œå¹¶æŒ–æ˜å…³äºä»»åŠ¡ã€æ¶æ„ã€è®­ç»ƒæœºåˆ¶ã€ç›®æ ‡ã€æ•°æ®é›†å’Œå…±ç°æ¨¡æ€çš„ç»†ç²’åº¦çº¿ç´¢ã€‚åˆ†æé‡åŒ–äº†ä¸‰ä¸ªå®è§‚è½¬å˜ï¼š(1)å¤šæ¨¡æ€è§†è§‰-è¯­è¨€-LLMå·¥ä½œçš„æ€¥å‰§å¢åŠ ï¼Œå°†ç»å…¸æ„ŸçŸ¥é‡æ„ä¸ºæŒ‡ä»¤è·Ÿéšå’Œå¤šæ­¥éª¤æ¨ç†ï¼›(2)ç”Ÿæˆæ–¹æ³•çš„ç¨³æ­¥æ‰©å±•ï¼Œæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶å›´ç»•å¯æ§æ€§ã€è’¸é¦å’Œé€Ÿåº¦è¿›è¡Œæ•´åˆï¼›(3)3Då’Œè§†é¢‘æ´»åŠ¨çš„æŒç»­å‘å±•ï¼Œç»„åˆä»NeRFsè½¬å‘Gaussian splattingï¼Œå¹¶è¶Šæ¥è¶Šå¼ºè°ƒä»¥äººå’Œæ™ºèƒ½ä½“ä¸ºä¸­å¿ƒçš„ç†è§£ã€‚åœ¨VLMä¸­ï¼Œè¯¸å¦‚Prompting/Adapters/LoRAä¹‹ç±»çš„å‚æ•°é«˜æ•ˆé€‚é…å’Œè½»é‡çº§è§†è§‰-è¯­è¨€æ¡¥æ¥å æ®ä¸»å¯¼åœ°ä½ï¼›è®­ç»ƒå®è·µä»å¤´å¼€å§‹æ„å»ºç¼–ç å™¨è½¬å˜ä¸ºæŒ‡ä»¤è°ƒä¼˜å’Œå¾®è°ƒå¼ºå¤§çš„éª¨å¹²ç½‘ç»œï¼›å¯¹æ¯”ç›®æ ‡ç›¸å¯¹äºäº¤å‰ç†µ/æ’åºå’Œè’¸é¦æœ‰æ‰€å‡å°‘ã€‚è·¨ä¼šè®®æ¯”è¾ƒè¡¨æ˜ï¼ŒCVPRå…·æœ‰æ›´å¼ºçš„3Dè¶³è¿¹ï¼Œè€ŒICLRå…·æœ‰æœ€é«˜çš„VLMä»½é¢ï¼Œè€Œæ•ˆç‡æˆ–é²æ£’æ€§ç­‰å¯é æ€§ä¸»é¢˜åˆ™åœ¨å„ä¸ªé¢†åŸŸæ‰©æ•£ã€‚æˆ‘ä»¬å‘å¸ƒè¯å…¸å’Œæ–¹æ³•è®ºï¼Œä»¥å®ç°å®¡è®¡å’Œæ‰©å±•ã€‚å±€é™æ€§åŒ…æ‹¬è¯å…¸å¬å›ç‡å’Œä»…æ‘˜è¦èŒƒå›´ï¼Œä½†çºµå‘ä¿¡å·åœ¨å„ä¸ªä¼šè®®å’Œå¹´ä»½ä¸­æ˜¯ä¸€è‡´çš„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç ”ç©¶å‘å±•è¿…é€Ÿï¼Œä½†ç¼ºä¹å¯¹å¤§è§„æ¨¡è®ºæ–‡æ•°æ®çš„ç³»ç»Ÿæ€§åˆ†æï¼Œéš¾ä»¥å‡†ç¡®æŠŠæ¡é¢†åŸŸå‘å±•è¶‹åŠ¿å’Œçƒ­ç‚¹æ–¹å‘ã€‚ç°æœ‰çš„åˆ†ææ–¹æ³•å¾€å¾€ä¾èµ–äººå·¥æ ‡æ³¨æˆ–å°è§„æ¨¡æ•°æ®é›†ï¼Œéš¾ä»¥è¦†ç›–VLMé¢†åŸŸçš„å…¨è²Œï¼Œå¹¶ä¸”ç¼ºä¹å¯å¤ç°æ€§å’Œé€æ˜æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„VLMè®ºæ–‡çŸ¥è¯†å›¾è°±ï¼Œå¹¶è®¾è®¡ä¸€å¥—è‡ªåŠ¨åŒ–çš„åˆ†ææµç¨‹ï¼Œä»è€Œå®ç°å¯¹VLMé¢†åŸŸç ”ç©¶è¶‹åŠ¿çš„é‡åŒ–åˆ†æã€‚é€šè¿‡å¯¹è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œå¤„ç†ï¼Œæå–å…³é”®ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨æ‰‹å·¥æ„å»ºçš„è¯å…¸è¿›è¡ŒåŒ¹é…ï¼Œä»è€Œä¸ºè®ºæ–‡åˆ†é…ä¸»é¢˜æ ‡ç­¾ï¼Œå¹¶æŒ–æ˜ç»†ç²’åº¦çš„ç ”ç©¶çº¿ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) æ•°æ®æ”¶é›†ï¼šæ”¶é›†CVPRã€ICLRå’ŒNeurIPSä¸‰å¤§é¡¶ä¼š2023-2025å¹´é—´æ”¶å½•çš„26104ç¯‡è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ã€‚2) æ•°æ®é¢„å¤„ç†ï¼šå¯¹æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼ŒåŒ…æ‹¬å»é™¤åœç”¨è¯ã€è¯å¹²æå–ç­‰ã€‚3) è¯å…¸æ„å»ºï¼šæ‰‹å·¥æ„å»ºä¸€ä¸ªåŒ…å«VLMé¢†åŸŸç›¸å…³æœ¯è¯­çš„è¯å…¸ï¼Œç”¨äºä¸»é¢˜æ ‡ç­¾åˆ†é…å’Œç ”ç©¶çº¿ç´¢æŒ–æ˜ã€‚4) ä¸»é¢˜æ ‡ç­¾åˆ†é…ï¼šä½¿ç”¨è¯å…¸å¯¹è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡ŒåŒ¹é…ï¼Œä¸ºè®ºæ–‡åˆ†é…ä¸»é¢˜æ ‡ç­¾ã€‚5) è¶‹åŠ¿åˆ†æï¼šå¯¹ä¸»é¢˜æ ‡ç­¾çš„åˆ†å¸ƒå’Œå˜åŒ–è¶‹åŠ¿è¿›è¡Œåˆ†æï¼Œä»è€Œæ­ç¤ºVLMé¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹å’Œå‘å±•æ–¹å‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„VLMè®ºæ–‡çŸ¥è¯†å›¾è°±ï¼Œå¹¶è®¾è®¡äº†ä¸€å¥—è‡ªåŠ¨åŒ–çš„åˆ†ææµç¨‹ï¼Œä»è€Œå®ç°äº†å¯¹VLMé¢†åŸŸç ”ç©¶è¶‹åŠ¿çš„é‡åŒ–åˆ†æã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•å…·æœ‰æ›´é«˜çš„å¯å¤ç°æ€§å’Œé€æ˜æ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¦†ç›–æ›´å¹¿æ³›çš„ç ”ç©¶é¢†åŸŸã€‚

**å…³é”®è®¾è®¡**ï¼šæœ¬æ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ‰‹å·¥æ„å»ºçš„VLMé¢†åŸŸè¯å…¸ï¼Œè¯¥è¯å…¸åŒ…å«äº†VLMé¢†åŸŸç›¸å…³çš„æœ¯è¯­ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°ä¸ºè®ºæ–‡åˆ†é…ä¸»é¢˜æ ‡ç­¾ã€‚2) è‡ªåŠ¨åŒ–çš„åˆ†ææµç¨‹ï¼Œè¯¥æµç¨‹èƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†å¤§è§„æ¨¡çš„è®ºæ–‡æ•°æ®ï¼Œå¹¶æŒ–æ˜ç»†ç²’åº¦çš„ç ”ç©¶çº¿ç´¢ã€‚3) çºµå‘åˆ†ææ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ­ç¤ºVLMé¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿å’Œå‘å±•æ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶æ­ç¤ºäº†VLMé¢†åŸŸçš„ä¸‰å¤§å®è§‚è¶‹åŠ¿ï¼šå¤šæ¨¡æ€è§†è§‰-è¯­è¨€-LLMå·¥ä½œçš„æ€¥å‰§å¢åŠ ï¼›ç”Ÿæˆæ–¹æ³•çš„ç¨³æ­¥æ‰©å±•ï¼›3Då’Œè§†é¢‘æ´»åŠ¨çš„æŒç»­å‘å±•ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå‚æ•°é«˜æ•ˆé€‚é…å’Œè½»é‡çº§è§†è§‰-è¯­è¨€æ¡¥æ¥åœ¨VLMä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œè®­ç»ƒå®è·µä»å¤´å¼€å§‹æ„å»ºç¼–ç å™¨è½¬å˜ä¸ºæŒ‡ä»¤è°ƒä¼˜å’Œå¾®è°ƒå¼ºå¤§çš„éª¨å¹²ç½‘ç»œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬ï¼šä¸ºç ”ç©¶äººå‘˜æä¾›VLMé¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿åˆ†æï¼Œå¸®åŠ©ä»–ä»¬å¿«é€Ÿäº†è§£é¢†åŸŸåŠ¨æ€ï¼›ä¸ºæ”¿ç­–åˆ¶å®šè€…æä¾›å†³ç­–æ”¯æŒï¼ŒæŒ‡å¯¼äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•æ–¹å‘ï¼›ä¸ºä¼ä¸šæä¾›æŠ€æœ¯æƒ…æŠ¥ï¼Œå¸®åŠ©ä»–ä»¬æŠŠæ¡å¸‚åœºæœºé‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶çš„æ–¹æ³•è®ºå’Œå·¥å…·å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿åˆ†æä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities. The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding. Within VLMs, parameter-efficient adaptation like prompting/adapters/LoRA and lightweight vision-language bridges dominate; training practice shifts from building encoders from scratch to instruction tuning and finetuning strong backbones; contrastive objectives recede relative to cross-entropy/ranking and distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and ICLR the highest VLM share, while reliability themes such as efficiency or robustness diffuse across areas. We release the lexicon and methodology to enable auditing and extension. Limitations include lexicon recall and abstract-only scope, but the longitudinal signals are consistent across venues and years.

