---
layout: default
title: Vision Language Models: A Survey of 26K Papers
---

# Vision Language Models: A Survey of 26K Papers

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.09586" target="_blank" class="toolbar-btn">arXiv: 2510.09586v1</a>
    <a href="https://arxiv.org/pdf/2510.09586.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09586v1" 
            onclick="toggleFavorite(this, '2510.09586v1', 'Vision Language Models: A Survey of 26K Papers')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Fengming Lin

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10

**Â§áÊ≥®**: VLM/LLM Learning Notes

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Â§ßËßÑÊ®°ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁ†îÁ©∂Ë∂ãÂäøÂàÜÊûêÔºöÂü∫‰∫é2.6‰∏áÁØáËÆ∫ÊñáÁöÑÁªºÂêàË∞ÉÁ†î**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Á†îÁ©∂Ë∂ãÂäøÂàÜÊûê` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê∑±Â∫¶Â≠¶‰π†` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ` `ËÆ°ÁÆóÊú∫ËßÜËßâ` `Â§ßËßÑÊ®°Êï∞ÊçÆÂàÜÊûê` `Êåá‰ª§Ë∞É‰ºò`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁ†îÁ©∂Áº∫‰πèÂ§ßËßÑÊ®°„ÄÅÁ≥ªÁªüÊÄßÁöÑË∂ãÂäøÂàÜÊûêÔºåÈöæ‰ª•ÊääÊè°È¢ÜÂüüÂèëÂ±ïÊñπÂêë„ÄÇ
2. Êú¨ÊñáÊûÑÂª∫‰∫ÜÂåÖÂê´2.6‰∏áÁØáËÆ∫ÊñáÁöÑÁü•ËØÜÂõæË∞±ÔºåÂπ∂ËÆæËÆ°‰∫ÜËá™Âä®ÂåñÁöÑ‰∏ªÈ¢òÊ†áÁ≠æÂàÜÈÖçÂíåË∂ãÂäøÊåñÊéòÊñπÊ≥ï„ÄÇ
3. ÈÄöËøáÂØπ‰∏âÂ§ßÈ°∂‰ºöËÆ∫ÊñáÁöÑÂàÜÊûêÔºåÊè≠Á§∫‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÈ¢ÜÂüüÂú®Â§öÊ®°ÊÄÅËûçÂêà„ÄÅÁîüÊàêÊñπÊ≥ïÂíå3D/ËßÜÈ¢ëÁêÜËß£Á≠âÊñπÈù¢ÁöÑÂÖ≥ÈîÆË∂ãÂäø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÂØπCVPR„ÄÅICLRÂíåNeurIPS‰∏âÂ§ßÈ°∂‰ºö2023-2025Âπ¥Èó¥Êî∂ÂΩïÁöÑ26104ÁØáËÆ∫ÊñáËøõË°å‰∫ÜÈÄèÊòé‰∏îÂèØÂ§çÁé∞ÁöÑÁ†îÁ©∂Ë∂ãÂäøÊµãÈáè„ÄÇÈÄöËøáËßÑËåÉÂåñÊ†áÈ¢òÂíåÊëòË¶ÅÔºåÂπ∂‰ΩøÁî®ÊâãÂ∑•ÊûÑÂª∫ÁöÑËØçÂÖ∏ËøõË°åÁü≠ËØ≠‰øùÊä§ÂíåÂåπÈÖçÔºå‰∏∫ËÆ∫ÊñáÂàÜÈÖçÂ§öËææ35‰∏™‰∏ªÈ¢òÊ†áÁ≠æÔºåÂπ∂ÊåñÊéòÂÖ≥‰∫é‰ªªÂä°„ÄÅÊû∂ÊûÑ„ÄÅËÆ≠ÁªÉÊú∫Âà∂„ÄÅÁõÆÊ†á„ÄÅÊï∞ÊçÆÈõÜÂíåÂÖ±Áé∞Ê®°ÊÄÅÁöÑÁªÜÁ≤íÂ∫¶Á∫øÁ¥¢„ÄÇÂàÜÊûêÈáèÂåñ‰∫Ü‰∏â‰∏™ÂÆèËßÇËΩ¨ÂèòÔºö(1)Â§öÊ®°ÊÄÅËßÜËßâ-ËØ≠Ë®Ä-LLMÂ∑•‰ΩúÁöÑÊÄ•ÂâßÂ¢ûÂä†ÔºåÂ∞ÜÁªèÂÖ∏ÊÑüÁü•ÈáçÊûÑ‰∏∫Êåá‰ª§Ë∑üÈöèÂíåÂ§öÊ≠•È™§Êé®ÁêÜÔºõ(2)ÁîüÊàêÊñπÊ≥ïÁöÑÁ®≥Ê≠•Êâ©Â±ïÔºåÊâ©Êï£Ê®°ÂûãÁöÑÁ†îÁ©∂Âõ¥ÁªïÂèØÊéßÊÄß„ÄÅËí∏È¶èÂíåÈÄüÂ∫¶ËøõË°åÊï¥ÂêàÔºõ(3)3DÂíåËßÜÈ¢ëÊ¥ªÂä®ÁöÑÊåÅÁª≠ÂèëÂ±ïÔºåÁªÑÂêà‰ªéNeRFsËΩ¨ÂêëGaussian splattingÔºåÂπ∂Ë∂äÊù•Ë∂äÂº∫Ë∞É‰ª•‰∫∫ÂíåÊô∫ËÉΩ‰Ωì‰∏∫‰∏≠ÂøÉÁöÑÁêÜËß£„ÄÇÂú®VLM‰∏≠ÔºåËØ∏Â¶ÇPrompting/Adapters/LoRA‰πãÁ±ªÁöÑÂèÇÊï∞È´òÊïàÈÄÇÈÖçÂíåËΩªÈáèÁ∫ßËßÜËßâ-ËØ≠Ë®ÄÊ°•Êé•Âç†ÊçÆ‰∏ªÂØºÂú∞‰ΩçÔºõËÆ≠ÁªÉÂÆûË∑µ‰ªéÂ§¥ÂºÄÂßãÊûÑÂª∫ÁºñÁ†ÅÂô®ËΩ¨Âèò‰∏∫Êåá‰ª§Ë∞É‰ºòÂíåÂæÆË∞ÉÂº∫Â§ßÁöÑÈ™®Âπ≤ÁΩëÁªúÔºõÂØπÊØîÁõÆÊ†áÁõ∏ÂØπ‰∫é‰∫§ÂèâÁÜµ/ÊéíÂ∫èÂíåËí∏È¶èÊúâÊâÄÂáèÂ∞ë„ÄÇË∑®‰ºöËÆÆÊØîËæÉË°®ÊòéÔºåCVPRÂÖ∑ÊúâÊõ¥Âº∫ÁöÑ3DË∂≥ËøπÔºåËÄåICLRÂÖ∑ÊúâÊúÄÈ´òÁöÑVLM‰ªΩÈ¢ùÔºåËÄåÊïàÁéáÊàñÈ≤ÅÊ£íÊÄßÁ≠âÂèØÈù†ÊÄß‰∏ªÈ¢òÂàôÂú®ÂêÑ‰∏™È¢ÜÂüüÊâ©Êï£„ÄÇÊàë‰ª¨ÂèëÂ∏ÉËØçÂÖ∏ÂíåÊñπÊ≥ïËÆ∫Ôºå‰ª•ÂÆûÁé∞ÂÆ°ËÆ°ÂíåÊâ©Â±ï„ÄÇÂ±ÄÈôêÊÄßÂåÖÊã¨ËØçÂÖ∏Âè¨ÂõûÁéáÂíå‰ªÖÊëòË¶ÅËåÉÂõ¥Ôºå‰ΩÜÁ∫µÂêë‰ø°Âè∑Âú®ÂêÑ‰∏™‰ºöËÆÆÂíåÂπ¥‰ªΩ‰∏≠ÊòØ‰∏ÄËá¥ÁöÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁ†îÁ©∂ÂèëÂ±ïËøÖÈÄüÔºå‰ΩÜÁº∫‰πèÂØπÂ§ßËßÑÊ®°ËÆ∫ÊñáÊï∞ÊçÆÁöÑÁ≥ªÁªüÊÄßÂàÜÊûêÔºåÈöæ‰ª•ÂáÜÁ°ÆÊääÊè°È¢ÜÂüüÂèëÂ±ïË∂ãÂäøÂíåÁÉ≠ÁÇπÊñπÂêë„ÄÇÁé∞ÊúâÁöÑÂàÜÊûêÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫∫Â∑•Ê†áÊ≥®ÊàñÂ∞èËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÈöæ‰ª•Ë¶ÜÁõñVLMÈ¢ÜÂüüÁöÑÂÖ®Ë≤åÔºåÂπ∂‰∏îÁº∫‰πèÂèØÂ§çÁé∞ÊÄßÂíåÈÄèÊòéÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑVLMËÆ∫ÊñáÁü•ËØÜÂõæË∞±ÔºåÂπ∂ËÆæËÆ°‰∏ÄÂ•óËá™Âä®ÂåñÁöÑÂàÜÊûêÊµÅÁ®ãÔºå‰ªéËÄåÂÆûÁé∞ÂØπVLMÈ¢ÜÂüüÁ†îÁ©∂Ë∂ãÂäøÁöÑÈáèÂåñÂàÜÊûê„ÄÇÈÄöËøáÂØπËÆ∫ÊñáÊ†áÈ¢òÂíåÊëòË¶ÅËøõË°åÂ§ÑÁêÜÔºåÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂπ∂‰ΩøÁî®ÊâãÂ∑•ÊûÑÂª∫ÁöÑËØçÂÖ∏ËøõË°åÂåπÈÖçÔºå‰ªéËÄå‰∏∫ËÆ∫ÊñáÂàÜÈÖç‰∏ªÈ¢òÊ†áÁ≠æÔºåÂπ∂ÊåñÊéòÁªÜÁ≤íÂ∫¶ÁöÑÁ†îÁ©∂Á∫øÁ¥¢„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊú¨ÊñáÁöÑÊäÄÊúØÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö1) Êï∞ÊçÆÊî∂ÈõÜÔºöÊî∂ÈõÜCVPR„ÄÅICLRÂíåNeurIPS‰∏âÂ§ßÈ°∂‰ºö2023-2025Âπ¥Èó¥Êî∂ÂΩïÁöÑ26104ÁØáËÆ∫ÊñáÁöÑÊ†áÈ¢òÂíåÊëòË¶Å„ÄÇ2) Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÔºöÂØπÊ†áÈ¢òÂíåÊëòË¶ÅËøõË°åËßÑËåÉÂåñÂ§ÑÁêÜÔºåÂåÖÊã¨ÂéªÈô§ÂÅúÁî®ËØç„ÄÅËØçÂπ≤ÊèêÂèñÁ≠â„ÄÇ3) ËØçÂÖ∏ÊûÑÂª∫ÔºöÊâãÂ∑•ÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´VLMÈ¢ÜÂüüÁõ∏ÂÖ≥ÊúØËØ≠ÁöÑËØçÂÖ∏ÔºåÁî®‰∫é‰∏ªÈ¢òÊ†áÁ≠æÂàÜÈÖçÂíåÁ†îÁ©∂Á∫øÁ¥¢ÊåñÊéò„ÄÇ4) ‰∏ªÈ¢òÊ†áÁ≠æÂàÜÈÖçÔºö‰ΩøÁî®ËØçÂÖ∏ÂØπËÆ∫ÊñáÊ†áÈ¢òÂíåÊëòË¶ÅËøõË°åÂåπÈÖçÔºå‰∏∫ËÆ∫ÊñáÂàÜÈÖç‰∏ªÈ¢òÊ†áÁ≠æ„ÄÇ5) Ë∂ãÂäøÂàÜÊûêÔºöÂØπ‰∏ªÈ¢òÊ†áÁ≠æÁöÑÂàÜÂ∏ÉÂíåÂèòÂåñË∂ãÂäøËøõË°åÂàÜÊûêÔºå‰ªéËÄåÊè≠Á§∫VLMÈ¢ÜÂüüÁöÑÁ†îÁ©∂ÁÉ≠ÁÇπÂíåÂèëÂ±ïÊñπÂêë„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑVLMËÆ∫ÊñáÁü•ËØÜÂõæË∞±ÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÂ•óËá™Âä®ÂåñÁöÑÂàÜÊûêÊµÅÁ®ãÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂØπVLMÈ¢ÜÂüüÁ†îÁ©∂Ë∂ãÂäøÁöÑÈáèÂåñÂàÜÊûê„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊú¨ÊñáÁöÑÊñπÊ≥ïÂÖ∑ÊúâÊõ¥È´òÁöÑÂèØÂ§çÁé∞ÊÄßÂíåÈÄèÊòéÊÄßÔºåÂπ∂‰∏îËÉΩÂ§üË¶ÜÁõñÊõ¥ÂπøÊ≥õÁöÑÁ†îÁ©∂È¢ÜÂüü„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊú¨ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÊâãÂ∑•ÊûÑÂª∫ÁöÑVLMÈ¢ÜÂüüËØçÂÖ∏ÔºåËØ•ËØçÂÖ∏ÂåÖÂê´‰∫ÜVLMÈ¢ÜÂüüÁõ∏ÂÖ≥ÁöÑÊúØËØ≠ÔºåËÉΩÂ§üÂáÜÁ°ÆÂú∞‰∏∫ËÆ∫ÊñáÂàÜÈÖç‰∏ªÈ¢òÊ†áÁ≠æ„ÄÇ2) Ëá™Âä®ÂåñÁöÑÂàÜÊûêÊµÅÁ®ãÔºåËØ•ÊµÅÁ®ãËÉΩÂ§üÈ´òÊïàÂú∞Â§ÑÁêÜÂ§ßËßÑÊ®°ÁöÑËÆ∫ÊñáÊï∞ÊçÆÔºåÂπ∂ÊåñÊéòÁªÜÁ≤íÂ∫¶ÁöÑÁ†îÁ©∂Á∫øÁ¥¢„ÄÇ3) Á∫µÂêëÂàÜÊûêÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊè≠Á§∫VLMÈ¢ÜÂüüÁöÑÁ†îÁ©∂Ë∂ãÂäøÂíåÂèëÂ±ïÊñπÂêë„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Á†îÁ©∂Êè≠Á§∫‰∫ÜVLMÈ¢ÜÂüüÁöÑ‰∏âÂ§ßÂÆèËßÇË∂ãÂäøÔºöÂ§öÊ®°ÊÄÅËßÜËßâ-ËØ≠Ë®Ä-LLMÂ∑•‰ΩúÁöÑÊÄ•ÂâßÂ¢ûÂä†ÔºõÁîüÊàêÊñπÊ≥ïÁöÑÁ®≥Ê≠•Êâ©Â±ïÔºõ3DÂíåËßÜÈ¢ëÊ¥ªÂä®ÁöÑÊåÅÁª≠ÂèëÂ±ï„ÄÇÁ†îÁ©∂ËøòÂèëÁé∞ÔºåÂèÇÊï∞È´òÊïàÈÄÇÈÖçÂíåËΩªÈáèÁ∫ßËßÜËßâ-ËØ≠Ë®ÄÊ°•Êé•Âú®VLM‰∏≠Âç†ÊçÆ‰∏ªÂØºÂú∞‰ΩçÔºåËÆ≠ÁªÉÂÆûË∑µ‰ªéÂ§¥ÂºÄÂßãÊûÑÂª∫ÁºñÁ†ÅÂô®ËΩ¨Âèò‰∏∫Êåá‰ª§Ë∞É‰ºòÂíåÂæÆË∞ÉÂº∫Â§ßÁöÑÈ™®Âπ≤ÁΩëÁªú„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ§ö‰∏™È¢ÜÂüüÔºåÂåÖÊã¨Ôºö‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõVLMÈ¢ÜÂüüÁöÑÁ†îÁ©∂Ë∂ãÂäøÂàÜÊûêÔºåÂ∏ÆÂä©‰ªñ‰ª¨Âø´ÈÄü‰∫ÜËß£È¢ÜÂüüÂä®ÊÄÅÔºõ‰∏∫ÊîøÁ≠ñÂà∂ÂÆöËÄÖÊèê‰æõÂÜ≥Á≠ñÊîØÊåÅÔºåÊåáÂØº‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÁöÑÂèëÂ±ïÊñπÂêëÔºõ‰∏∫‰ºÅ‰∏öÊèê‰æõÊäÄÊúØÊÉÖÊä•ÔºåÂ∏ÆÂä©‰ªñ‰ª¨ÊääÊè°Â∏ÇÂú∫Êú∫ÈÅá„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂ÁöÑÊñπÊ≥ïËÆ∫ÂíåÂ∑•ÂÖ∑ÂèØ‰ª•Êé®ÂπøÂà∞ÂÖ∂‰ªñ‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÁöÑÁ†îÁ©∂Ë∂ãÂäøÂàÜÊûê‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities. The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding. Within VLMs, parameter-efficient adaptation like prompting/adapters/LoRA and lightweight vision-language bridges dominate; training practice shifts from building encoders from scratch to instruction tuning and finetuning strong backbones; contrastive objectives recede relative to cross-entropy/ranking and distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and ICLR the highest VLM share, while reliability themes such as efficiency or robustness diffuse across areas. We release the lexicon and methodology to enable auditing and extension. Limitations include lexicon recall and abstract-only scope, but the longitudinal signals are consistent across venues and years.

