---
layout: default
title: Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation
---

# Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09224" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09224v2</a>
  <a href="https://arxiv.org/pdf/2510.09224.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09224v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09224v2', 'Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wangyu Wu, Xuhang Chen, Zhenhong Chen, Jing-En Jiang, Kim-Fung Tsang, Xiaowei Huang, Fei Ma, Jimin Xiao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10 (æ›´æ–°: 2025-10-20)

**å¤‡æ³¨**: Accepted in IEEE Transactions on Consumer Electronics 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTEMA-LLMï¼Œåˆ©ç”¨LLMå¢å¼ºçš„å¤šæ³¨æ„åŠ›æœºåˆ¶è§£å†³è·¨åŸŸåºåˆ—æ¨èé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨åŸŸåºåˆ—æ¨è` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ³¨æ„åŠ›æœºåˆ¶` `è¯­ä¹‰æ ‡ç­¾ç”Ÿæˆ` `ç”¨æˆ·åå¥½å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è·¨åŸŸåºåˆ—æ¨èæ–¹æ³•éš¾ä»¥åŒæ—¶æ•æ‰é¢†åŸŸç‰¹å®šå’Œè·¨é¢†åŸŸçš„å¤æ‚ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼Œå¯¼è‡´æ¨èç²¾åº¦ä¸è¶³ã€‚
2. TEMA-LLMåˆ©ç”¨LLMç”Ÿæˆå¯Œå«è¯­ä¹‰ä¿¡æ¯çš„æ ‡ç­¾ï¼Œå¹¶é€šè¿‡å¤šæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡ç”¨æˆ·åœ¨ä¸åŒé¢†åŸŸé—´çš„åå¥½è½¬ç§»ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTEMA-LLMåœ¨å¤šä¸ªç”µå•†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†LLMè¯­ä¹‰å¢å¼ºå’Œå¤šæ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è·¨åŸŸåºåˆ—æ¨è(CDSR)åœ¨ç°ä»£æ¶ˆè´¹ç”µå­å’Œç”µå­å•†åŠ¡å¹³å°ä¸­è‡³å…³é‡è¦ï¼Œç”¨æˆ·ä¸ä¹¦ç±ã€ç”µå½±å’Œåœ¨çº¿é›¶å”®äº§å“ç­‰å„ç§æœåŠ¡è¿›è¡Œäº¤äº’ã€‚è¿™äº›ç³»ç»Ÿå¿…é¡»å‡†ç¡®åœ°æ•æ‰é¢†åŸŸç‰¹å®šå’Œè·¨é¢†åŸŸçš„è¡Œä¸ºæ¨¡å¼ï¼Œä»¥æä¾›ä¸ªæ€§åŒ–å’Œæ— ç¼çš„æ¶ˆè´¹è€…ä½“éªŒã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TEMA-LLMï¼ˆåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ ‡ç­¾å¢å¼ºå¤šæ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç”¨ä¸”æœ‰æ•ˆçš„æ¡†æ¶ï¼Œé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ç”¨äºè¯­ä¹‰æ ‡ç­¾ç”Ÿæˆå’Œå¢å¼ºã€‚å…·ä½“æ¥è¯´ï¼ŒTEMA-LLMé‡‡ç”¨LLMæ¥åˆ†é…é¢†åŸŸæ„ŸçŸ¥çš„æç¤ºï¼Œå¹¶ä»é¡¹ç›®æ ‡é¢˜å’Œæè¿°ä¸­ç”Ÿæˆæè¿°æ€§æ ‡ç­¾ã€‚å°†ç”Ÿæˆçš„æ ‡ç­¾åµŒå…¥ä¸é¡¹ç›®æ ‡è¯†ç¬¦ä»¥åŠæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾èåˆï¼Œä»¥æ„å»ºå¢å¼ºçš„é¡¹ç›®è¡¨ç¤ºã€‚ç„¶åå¼•å…¥æ ‡ç­¾å¢å¼ºçš„å¤šæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥è”åˆå»ºæ¨¡ç”¨æˆ·åœ¨åŸŸå†…å’Œè·¨åŸŸçš„åå¥½ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæ•è·å¤æ‚ä¸”ä¸æ–­å˜åŒ–çš„æ¶ˆè´¹è€…å…´è¶£ã€‚åœ¨å››ä¸ªå¤§å‹ç”µå­å•†åŠ¡æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTEMA-LLMå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œçªå‡ºäº†åŸºäºLLMçš„è¯­ä¹‰æ ‡ç­¾å’Œå¤šæ³¨æ„åŠ›é›†æˆå¯¹äºé¢å‘æ¶ˆè´¹è€…çš„æ¨èç³»ç»Ÿçš„ç›Šå¤„ã€‚æ‰€æå‡ºçš„æ–¹æ³•çªå‡ºäº†LLMåœ¨æ¨è¿›æ¶ˆè´¹ç”µå­é¢†åŸŸæ™ºèƒ½ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æœåŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè·¨åŸŸåºåˆ—æ¨èæ—¨åœ¨åˆ©ç”¨ç”¨æˆ·åœ¨å¤šä¸ªé¢†åŸŸçš„å†å²è¡Œä¸ºæ•°æ®ï¼Œé¢„æµ‹ç”¨æˆ·åœ¨ç›®æ ‡é¢†åŸŸçš„ä¸‹ä¸€ä¸ªäº¤äº’é¡¹ç›®ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºéš¾ä»¥å……åˆ†åˆ©ç”¨é¡¹ç›®ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥åŠéš¾ä»¥æœ‰æ•ˆå»ºæ¨¡ç”¨æˆ·åœ¨ä¸åŒé¢†åŸŸä¹‹é—´çš„åå¥½è½¬ç§»æ¨¡å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTEMA-LLMçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»é¡¹ç›®æ ‡é¢˜å’Œæè¿°ä¸­æå–è¯­ä¹‰æ ‡ç­¾ï¼Œä»è€Œä¸°å¯Œé¡¹ç›®è¡¨ç¤ºã€‚ç„¶åï¼Œé€šè¿‡ä¸€ä¸ªæ–°é¢–çš„æ ‡ç­¾å¢å¼ºå¤šæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶å»ºæ¨¡ç”¨æˆ·åœ¨å•ä¸ªé¢†åŸŸå†…çš„åå¥½å’Œè·¨é¢†åŸŸä¹‹é—´çš„åå¥½è½¬ç§»ã€‚è¿™æ ·å¯ä»¥æ›´å…¨é¢åœ°ç†è§£ç”¨æˆ·çš„å…´è¶£ï¼Œå¹¶æé«˜æ¨èçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTEMA-LLMçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) LLMæ ‡ç­¾ç”Ÿæˆæ¨¡å—ï¼šä½¿ç”¨LLMä¸ºæ¯ä¸ªé¡¹ç›®ç”Ÿæˆä¸€ç»„æè¿°æ€§æ ‡ç­¾ã€‚2) é¡¹ç›®è¡¨ç¤ºå¢å¼ºæ¨¡å—ï¼šå°†æ ‡ç­¾åµŒå…¥ä¸é¡¹ç›®æ ‡è¯†ç¬¦ã€æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾èåˆï¼Œå¾—åˆ°å¢å¼ºçš„é¡¹ç›®è¡¨ç¤ºã€‚3) æ ‡ç­¾å¢å¼ºå¤šæ³¨æ„åŠ›æ¨¡å—ï¼šä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«è®¡ç®—ç”¨æˆ·åœ¨æ¯ä¸ªé¢†åŸŸå†…çš„åå¥½å’Œè·¨é¢†åŸŸçš„åå¥½è½¬ç§»ã€‚4) é¢„æµ‹æ¨¡å—ï¼šæ ¹æ®å­¦ä¹ åˆ°çš„ç”¨æˆ·åå¥½ï¼Œé¢„æµ‹ç”¨æˆ·åœ¨ç›®æ ‡é¢†åŸŸæœ€å¯èƒ½äº¤äº’çš„é¡¹ç›®ã€‚

**å…³é”®åˆ›æ–°**ï¼šTEMA-LLMçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) åˆ©ç”¨LLMè¿›è¡Œè¯­ä¹‰æ ‡ç­¾ç”Ÿæˆï¼Œä»è€Œæœ‰æ•ˆåˆ©ç”¨äº†é¡¹ç›®æ ‡é¢˜å’Œæè¿°ä¸­çš„ä¿¡æ¯ã€‚2) æå‡ºäº†æ ‡ç­¾å¢å¼ºå¤šæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥åŒæ—¶å»ºæ¨¡ç”¨æˆ·åœ¨å•ä¸ªé¢†åŸŸå†…çš„åå¥½å’Œè·¨é¢†åŸŸçš„åå¥½è½¬ç§»ï¼Œä»è€Œæ›´å…¨é¢åœ°ç†è§£ç”¨æˆ·çš„å…´è¶£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨LLMæ ‡ç­¾ç”Ÿæˆæ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†é¢†åŸŸæ„ŸçŸ¥çš„æç¤ºå·¥ç¨‹ï¼Œä»¥æé«˜æ ‡ç­¾çš„è´¨é‡ã€‚åœ¨æ ‡ç­¾å¢å¼ºå¤šæ³¨æ„åŠ›æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¯ä¸ªå¤´å…³æ³¨ä¸åŒçš„ç‰¹å¾å­ç©ºé—´ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç”¨æˆ·ç‚¹å‡»é¡¹ç›®çš„æ¦‚ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TEMA-LLMåœ¨å››ä¸ªå¤§å‹ç”µå•†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒTEMA-LLMå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸæ•°æ®é›†ä¸Šï¼ŒTEMA-LLMçš„HR@10æŒ‡æ ‡æå‡äº†5%ä»¥ä¸Šï¼ŒNDCG@10æŒ‡æ ‡æå‡äº†3%ä»¥ä¸Šï¼Œæ˜¾è‘—æé«˜äº†æ¨èçš„å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TEMA-LLMå¯åº”ç”¨äºå„ç§è·¨åŸŸæ¨èåœºæ™¯ï¼Œä¾‹å¦‚ç”µå•†å¹³å°ã€åœ¨çº¿æ•™è‚²å¹³å°å’Œå†…å®¹æ¨èå¹³å°ã€‚é€šè¿‡æ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·åœ¨ä¸åŒé¢†åŸŸçš„å…´è¶£ï¼Œå¯ä»¥æä¾›æ›´ä¸ªæ€§åŒ–çš„æ¨èæœåŠ¡ï¼Œæé«˜ç”¨æˆ·æ»¡æ„åº¦å’Œå¹³å°æ”¶ç›Šã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¤šæ¨¡æ€çš„æ•°æ®ï¼Œä¾‹å¦‚éŸ³é¢‘å’Œè§†é¢‘ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨èçš„å‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cross-Domain Sequential Recommendation (CDSR) plays a crucial role in modern consumer electronics and e-commerce platforms, where users interact with diverse services such as books, movies, and online retail products. These systems must accurately capture both domain-specific and cross-domain behavioral patterns to provide personalized and seamless consumer experiences. To address this challenge, we propose \textbf{TEMA-LLM} (\textit{Tag-Enriched Multi-Attention with Large Language Models}), a practical and effective framework that integrates \textit{Large Language Models (LLMs)} for semantic tag generation and enrichment. Specifically, TEMA-LLM employs LLMs to assign domain-aware prompts and generate descriptive tags from item titles and descriptions. The resulting tag embeddings are fused with item identifiers as well as textual and visual features to construct enhanced item representations. A \textit{Tag-Enriched Multi-Attention} mechanism is then introduced to jointly model user preferences within and across domains, enabling the system to capture complex and evolving consumer interests. Extensive experiments on four large-scale e-commerce datasets demonstrate that TEMA-LLM consistently outperforms state-of-the-art baselines, underscoring the benefits of LLM-based semantic tagging and multi-attention integration for consumer-facing recommendation systems. The proposed approach highlights the potential of LLMs to advance intelligent, user-centric services in the field of consumer electronics.

