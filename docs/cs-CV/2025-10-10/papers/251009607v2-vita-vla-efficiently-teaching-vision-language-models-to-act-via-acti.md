---
layout: default
title: VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation
---

# VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09607" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.09607v2</a>
  <a href="https://arxiv.org/pdf/2510.09607.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09607v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09607v2', 'VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shaoqi Dong, Chaoyou Fu, Haihan Gao, Yi-Fan Zhang, Chi Yan, Chu Wu, Xiaoyu Liu, Yunhang Shen, Jing Huo, Deqiang Jiang, Haoyu Cao, Yang Gao, Xing Sun, Ran He, Caifeng Shan

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10 (Êõ¥Êñ∞: 2025-10-17)

**Â§áÊ≥®**: Homepage: https://ltbai.github.io/VITA-VLA/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VITA-VLAÔºåÈÄöËøáÂä®‰Ωú‰∏ìÂÆ∂Ëí∏È¶èÈ´òÊïàËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã‰ª•ÊâßË°åÊú∫Âô®‰∫∫Âä®‰Ωú**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Áü•ËØÜËí∏È¶è` `Êú∫Âô®‰∫∫Êìç‰Ωú` `È¢ÑËÆ≠ÁªÉÊ®°Âûã` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãËÆ≠ÁªÉÊàêÊú¨È´òÊòÇÔºåÈöæ‰ª•ÂÖÖÂàÜÂà©Áî®È¢ÑËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)ÁöÑÂº∫Â§ßÊÑüÁü•ËÉΩÂäõ„ÄÇ
2. VITA-VLAÈÄöËøáÁü•ËØÜËí∏È¶èÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑÂ∞èÂûãÂä®‰ΩúÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞VLMÔºå‰ΩøÂÖ∂ÂÖ∑Â§áÂä®‰ΩúÊâßË°åËÉΩÂäõÔºåÈÅøÂÖç‰ªéÂ§¥ËÆ≠ÁªÉ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVITA-VLAÂú®Â§ö‰∏™Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Èôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÔºåÊèêÂçá‰∫ÜÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËí∏È¶èÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËøÅÁßªÈ¢ÑËÆ≠ÁªÉÁöÑÂ∞èÂûãÂä®‰ΩúÊ®°ÂûãÁöÑÁü•ËØÜÔºå‰ΩøËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)ÂÖ∑Â§áÂä®‰ΩúÊâßË°åËÉΩÂäõ„ÄÇËØ•Êû∂ÊûÑ‰øùÁïô‰∫ÜÂéüÂßãVLMÁöÑÁªìÊûÑÔºå‰ªÖÊ∑ªÂä†‰∫Ü‰∏Ä‰∏™Âä®‰ΩútokenÂíå‰∏Ä‰∏™Áä∂ÊÄÅÁºñÁ†ÅÂô®Êù•Êï¥ÂêàÁâ©ÁêÜËæìÂÖ•„ÄÇÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºöÈ¶ñÂÖàÔºåÈÄöËøáÂ∞ÜVLMÈöêËóèÁä∂ÊÄÅÊò†Â∞ÑÂà∞Â∞èÂûãÂä®‰ΩúÊ®°ÂûãÁöÑÂä®‰ΩúÁ©∫Èó¥ËøõË°åËΩªÈáèÁ∫ßÂØπÈΩêÔºå‰ªéËÄåÊúâÊïàÈáçÁî®ÂÖ∂È¢ÑËÆ≠ÁªÉÁöÑÂä®‰ΩúËß£Á†ÅÂô®ÔºåÈÅøÂÖçÊòÇË¥µÁöÑÈ¢ÑËÆ≠ÁªÉ„ÄÇÂÖ∂Ê¨°ÔºåÈÄâÊã©ÊÄßÂú∞ÂæÆË∞ÉËØ≠Ë®ÄÊ®°Âûã„ÄÅÁä∂ÊÄÅÁºñÁ†ÅÂô®ÂíåÂä®‰ΩúÊ®°ÂùóÔºå‰ΩøÁ≥ªÁªüËÉΩÂ§üÊï¥ÂêàÂ§öÊ®°ÊÄÅËæìÂÖ•Âπ∂ÁîüÊàêÁ≤æÁ°ÆÁöÑÂä®‰Ωú„ÄÇÂä®‰Ωútoken‰∏∫VLMÊèê‰æõ‰∫Ü‰∏Ä‰∏™Áõ¥Êé•È¢ÑÊµãÊú™Êù•Âä®‰ΩúÁöÑÂè•ÊüÑÔºåËÄåÁä∂ÊÄÅÁºñÁ†ÅÂô®ÂÖÅËÆ∏Ê®°ÂûãÊï¥ÂêàËßÜËßâ‰πãÂ§ñÁöÑÊú∫Âô®‰∫∫Âä®ÂäõÂ≠¶‰ø°ÊÅØ„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®LIBEROÂíåLIBERO-LONG‰∏äÂàÜÂà´ÂèñÂæó‰∫Ü97.3%Âíå93.5%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºåÂπ∂Âú®ÁúüÂÆû‰∏ñÁïåÁöÑÊìç‰Ωú‰ªªÂä°‰∏≠‰ºò‰∫éÊïôÂ∏àÊ®°ÂûãÔºåËØÅÊòé‰∫ÜÂä®‰ΩúËí∏È¶èËÉΩÂ§üÊúâÊïàÁîüÊàêÁ≤æÁ°ÆÂä®‰ΩúÔºåÂêåÊó∂ÊòæËëóÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâVLAÊ®°ÂûãÈÄöÂ∏∏ÈúÄË¶Å‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏îÈöæ‰ª•ÂÖÖÂàÜÂà©Áî®È¢ÑËÆ≠ÁªÉVLMsÂº∫Â§ßÁöÑËßÜËßâÂíåËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõ„ÄÇËøô‰∫õÊ®°ÂûãÂú®Ê≥õÂåñÊÄßÂíåÊïàÁéáÊñπÈù¢Â≠òÂú®Áì∂È¢àÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇÁöÑÊìç‰Ωú‰ªªÂä°‰∏≠„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVITA-VLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁü•ËØÜËí∏È¶èÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑÂ∞èÂûãÂä®‰ΩúÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â§ßÂûãVLM‰∏≠Ôºå‰ªéËÄå‰ΩøVLMÂÖ∑Â§áÂä®‰ΩúÊâßË°åËÉΩÂäõ„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫Ü‰ªéÂ§¥ËÆ≠ÁªÉÂ§ßÂûãVLAÊ®°ÂûãÁöÑÈúÄË¶ÅÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÂπ∂ËÉΩÊúâÊïàÂà©Áî®VLMsÁöÑÈ¢ÑËÆ≠ÁªÉÁü•ËØÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVITA-VLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÁöÑVLM„ÄÅ‰∏Ä‰∏™Âä®‰Ωútoken„ÄÅ‰∏Ä‰∏™Áä∂ÊÄÅÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÁöÑÂ∞èÂûãÂä®‰ΩúÊ®°ÂûãÔºà‰Ωú‰∏∫ÊïôÂ∏àÊ®°ÂûãÔºâ„ÄÇËÆ≠ÁªÉËøáÁ®ãÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºö1) ËΩªÈáèÁ∫ßÂØπÈΩêÔºöÂ∞ÜVLMÁöÑÈöêËóèÁä∂ÊÄÅÊò†Â∞ÑÂà∞Â∞èÂûãÂä®‰ΩúÊ®°ÂûãÁöÑÂä®‰ΩúÁ©∫Èó¥Ôºå‰ª•‰æøÈáçÁî®ÂÖ∂È¢ÑËÆ≠ÁªÉÁöÑÂä®‰ΩúËß£Á†ÅÂô®„ÄÇ2) ÈÄâÊã©ÊÄßÂæÆË∞ÉÔºöÂæÆË∞ÉËØ≠Ë®ÄÊ®°Âûã„ÄÅÁä∂ÊÄÅÁºñÁ†ÅÂô®ÂíåÂä®‰ΩúÊ®°ÂùóÔºå‰ª•Êï¥ÂêàÂ§öÊ®°ÊÄÅËæìÂÖ•Âπ∂ÁîüÊàêÁ≤æÁ°ÆÁöÑÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVITA-VLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âü∫‰∫éËí∏È¶èÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÂä®‰ΩúÁü•ËØÜ‰ªéÂ∞èÂûãÂä®‰ΩúÊ®°ÂûãËøÅÁßªÂà∞Â§ßÂûãVLM„ÄÇÈÄöËøáÂºïÂÖ•Âä®‰ΩútokenÂíåÁä∂ÊÄÅÁºñÁ†ÅÂô®ÔºåVITA-VLAËÉΩÂ§üÊõ¥Â•ΩÂú∞Êï¥ÂêàËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÊú∫Âô®‰∫∫Áä∂ÊÄÅ‰ø°ÊÅØÔºå‰ªéËÄåÁîüÊàêÊõ¥Á≤æÁ°ÆÁöÑÂä®‰Ωú„ÄÇ‰∏é‰ªéÂ§¥ËÆ≠ÁªÉÁõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂä®‰ΩútokenË¢´Ê∑ªÂä†Âà∞VLM‰∏≠Ôºå‰Ωú‰∏∫È¢ÑÊµãÊú™Êù•Âä®‰ΩúÁöÑÁõ¥Êé•Âè•ÊüÑ„ÄÇÁä∂ÊÄÅÁºñÁ†ÅÂô®Áî®‰∫éÁºñÁ†ÅÊú∫Âô®‰∫∫Áä∂ÊÄÅ‰ø°ÊÅØÔºå‰æãÂ¶ÇÂÖ≥ËäÇËßíÂ∫¶ÂíåÈÄüÂ∫¶ÔºåËøô‰∫õ‰ø°ÊÅØÂèØËÉΩÊó†Ê≥ï‰ªÖ‰ªéËßÜËßâËæìÂÖ•‰∏≠Ëé∑Âæó„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨Âä®‰ΩúÈ¢ÑÊµãÊçüÂ§±ÂíåÁä∂ÊÄÅÈ¢ÑÊµãÊçüÂ§±ÔºåÁî®‰∫éÊåáÂØºÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÇÈÄâÊã©ÊÄßÂæÆË∞ÉÁ≠ñÁï•Áî®‰∫éÈÅøÂÖçËøáÂ∫¶ÊãüÂêàÔºåÂπ∂‰øùÊåÅVLMÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VITA-VLAÂú®LIBEROÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü97.3%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºåÁõ∏ÊØî‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá‰∫Ü11.8%„ÄÇÂú®Êõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑLIBERO-LONGÊï∞ÊçÆÈõÜ‰∏äÔºåÊàêÂäüÁéáËææÂà∞‰∫Ü93.5%ÔºåÊèêÂçáÂπÖÂ∫¶È´òËææ24.5%„ÄÇÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÂÆûÈ™å‰∏≠ÔºåVITA-VLAÁöÑÊàêÂäüÁéá‰∏∫82.0%ÔºåÊØîÊïôÂ∏àÊ®°ÂûãÊèêÈ´ò‰∫Ü17%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåVITA-VLAËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÂä®‰ΩúÁü•ËØÜËøÅÁßªÂà∞VLMÔºåÂπ∂ÊòæËëóÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊìç‰ΩúÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VITA-VLAÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÁâ©‰ΩìÊäìÂèñ„ÄÅË£ÖÈÖç„ÄÅÂØºËà™Á≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÈôç‰ΩéÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊàêÊú¨ÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÔºåÂπ∂‰øÉËøõÊú∫Âô®‰∫∫Âú®Â∑•‰∏öËá™Âä®Âåñ„ÄÅÂåªÁñó‰øùÂÅ•„ÄÅÂÆ∂Â∫≠ÊúçÂä°Á≠âÈ¢ÜÂüüÁöÑÂπøÊ≥õÂ∫îÁî®„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑÊú∫Âô®‰∫∫‰ªªÂä°Ôºå‰æãÂ¶Ç‰∫∫Êú∫Âçè‰ΩúÂíåËá™‰∏ªÂÜ≥Á≠ñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs). By integrating action modules into these pretrained models, VLA methods exhibit improved generalization. However, training them from scratch is costly. In this work, we propose a simple yet effective distillation-based framework that equips VLMs with action-execution capability by transferring knowledge from pretrained small action models. Our architecture retains the original VLM structure, adding only an action token and a state encoder to incorporate physical inputs. To distill action knowledge, we adopt a two-stage training strategy. First, we perform lightweight alignment by mapping VLM hidden states into the action space of the small action model, enabling effective reuse of its pretrained action decoder and avoiding expensive pretraining. Second, we selectively fine-tune the language model, state encoder, and action modules, enabling the system to integrate multimodal inputs with precise action generation. Specifically, the action token provides the VLM with a direct handle for predicting future actions, while the state encoder allows the model to incorporate robot dynamics not captured by vision alone. This design yields substantial efficiency gains over training large VLA models from scratch. Compared with previous state-of-the-art methods, our method achieves 97.3% average success rate on LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In real-world experiments across five manipulation tasks, our method consistently outperforms the teacher model, achieving 82.0% success rate (17% improvement), which demonstrate that action distillation effectively enables VLMs to generate precise actions while substantially reducing training costs.

