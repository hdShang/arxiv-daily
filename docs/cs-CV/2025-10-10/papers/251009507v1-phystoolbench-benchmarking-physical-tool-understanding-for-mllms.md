---
layout: default
title: PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs
---

# PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.09507" target="_blank" class="toolbar-btn">arXiv: 2510.09507v1</a>
    <a href="https://arxiv.org/pdf/2510.09507.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09507v1" 
            onclick="toggleFavorite(this, '2510.09507v1', 'PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zixin Zhang, Kanghao Chen, Xingwang Lin, Lutao Jiang, Xu Zheng, Yuanhuiyi Lyu, Litao Guo, Yinchuan Li, Ying-Cong Chen

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**PhysToolBenchÔºöÈ¶ñ‰∏™Èù¢ÂêëMLLMÁöÑÁâ©ÁêÜÂ∑•ÂÖ∑ÁêÜËß£ËÉΩÂäõËØÑÊµãÂü∫ÂáÜ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Áâ©ÁêÜÂ∑•ÂÖ∑ÁêÜËß£` `ËßÜËßâÈóÆÁ≠î` `ÂÖ∑Ë∫´Êô∫ËÉΩ` `ËØÑÊµãÂü∫ÂáÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂÖ∑Ë∫´Êô∫ËÉΩÂíåËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã‰æùËµñMLLMËøõË°åÈ´òÂ±ÇËßÑÂàíÔºå‰ΩÜMLLMÂØπÁâ©ÁêÜÂ∑•ÂÖ∑ÁöÑÁêÜËß£Á®ãÂ∫¶Â∞ö‰∏çÊòéÁ°Æ„ÄÇ
2. PhysToolBenchÈÄöËøáVQAÊï∞ÊçÆÈõÜÔºå‰ªéÂ∑•ÂÖ∑ËØÜÂà´„ÄÅÁêÜËß£ÂíåÂàõÈÄ†‰∏â‰∏™Â±ÇÈù¢ËØÑ‰º∞MLLMÂØπÁâ©ÁêÜÂ∑•ÂÖ∑ÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁé∞ÊúâMLLMÂú®Â∑•ÂÖ∑ÁêÜËß£ÊñπÈù¢Â≠òÂú®ÊòéÊòæ‰∏çË∂≥ÔºåËÆ∫ÊñáÊèê‰æõ‰∫ÜÊ∑±ÂÖ•ÂàÜÊûêÂíåÂàùÊ≠•Ëß£ÂÜ≥ÊñπÊ°à„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜPhysToolBenchÔºåËøôÊòØÈ¶ñ‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÂØπÁâ©ÁêÜÂ∑•ÂÖ∑ÁêÜËß£ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇËØ•Âü∫ÂáÜÊòØ‰∏Ä‰∏™ËßÜËßâÈóÆÁ≠îÔºàVQAÔºâÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Ë∂ÖËøá1000‰∏™ÂõæÂÉè-ÊñáÊú¨ÂØπÔºåÊó®Âú®ËØÑ‰º∞Ê®°ÂûãÂú®‰∏â‰∏™ÈöæÂ∫¶Á∫ßÂà´‰∏äÁöÑËÉΩÂäõÔºöÂ∑•ÂÖ∑ËØÜÂà´ÔºàËØÜÂà´Â∑•ÂÖ∑ÁöÑ‰∏ªË¶ÅÂäüËÉΩÔºâ„ÄÅÂ∑•ÂÖ∑ÁêÜËß£ÔºàÁêÜËß£Â∑•ÂÖ∑ÁöÑÊìç‰ΩúÂéüÁêÜÔºâÂíåÂ∑•ÂÖ∑ÂàõÈÄ†ÔºàÂú®Ê≤°Êúâ‰º†ÁªüÂ∑•ÂÖ∑ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂà©Áî®Âë®Âõ¥Áâ©‰ΩìÂà∂ÈÄ†Êñ∞Â∑•ÂÖ∑Ôºâ„ÄÇÂØπ32‰∏™MLLMÔºàÂåÖÊã¨‰∏ìÊúâÊ®°Âûã„ÄÅÂºÄÊ∫êÊ®°Âûã„ÄÅ‰∏ìÁî®ÂÖ∑Ë∫´Ê®°ÂûãÂíåVLA‰∏≠ÁöÑÈ™®Âπ≤ÁΩëÁªúÔºâÁöÑÂÖ®Èù¢ËØÑ‰º∞Ë°®ÊòéÔºåÊ®°ÂûãÂú®Â∑•ÂÖ∑ÁêÜËß£ÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÊèê‰æõ‰∫ÜÊ∑±ÂÖ•ÁöÑÂàÜÊûêÂπ∂ÊèêÂá∫‰∫ÜÂàùÊ≠•ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ‰ª£Á†ÅÂíåÊï∞ÊçÆÈõÜÂ∑≤ÂÖ¨ÂºÄ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàMLLMÔºâÂú®ÂÖ∑Ë∫´Êô∫ËÉΩÂíåËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâ‰ªªÂä°‰∏≠Ë¢´ÂπøÊ≥õÂ∫îÁî®Ôºå‰ΩÜÂÆÉ‰ª¨ÂØπÁâ©ÁêÜÂ∑•ÂÖ∑ÁöÑÁúüÊ≠£ÁêÜËß£Á®ãÂ∫¶‰ªçÁÑ∂Êú™Áü•„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÁº∫‰πè‰∏Ä‰∏™‰∏ìÈó®ÁöÑÂü∫ÂáÜÊù•Á≥ªÁªüÂú∞ËØÑ‰º∞MLLMÂØπÂ∑•ÂÖ∑ÁöÑËÆ§Áü•ËÉΩÂäõÔºåËøôÈòªÁ¢ç‰∫ÜËØ•È¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPhysToolBenchÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´ÂõæÂÉèÂíåÊñáÊú¨ÁöÑËßÜËßâÈóÆÁ≠îÔºàVQAÔºâÊï∞ÊçÆÈõÜÔºåÊù•Á≥ªÁªüÂú∞ËØÑ‰º∞MLLMÂú®Â∑•ÂÖ∑ËØÜÂà´„ÄÅÂ∑•ÂÖ∑ÁêÜËß£ÂíåÂ∑•ÂÖ∑ÂàõÈÄ†‰∏â‰∏™‰∏çÂêåÈöæÂ∫¶Á∫ßÂà´‰∏äÁöÑËÉΩÂäõ„ÄÇËøôÁßçÂàÜÂ±ÇËØÑ‰º∞ÁöÑÊñπÂºèËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞Êè≠Á§∫Ê®°ÂûãÂú®Áâ©ÁêÜÂ∑•ÂÖ∑ËÆ§Áü•ÊñπÈù¢ÁöÑ‰ºòÂäøÂíå‰∏çË∂≥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPhysToolBenchÊï∞ÊçÆÈõÜÂåÖÂê´Ë∂ÖËøá1000‰∏™ÂõæÂÉè-ÊñáÊú¨ÂØπÔºåÊØè‰∏™Ê†∑Êú¨ÈÉΩÂõ¥Áªï‰∏Ä‰∏™ÁâπÂÆöÁöÑÁâ©ÁêÜÂ∑•ÂÖ∑Â±ïÂºÄ„ÄÇÊï∞ÊçÆÈõÜË¢´ÂàíÂàÜ‰∏∫‰∏â‰∏™ÈöæÂ∫¶Á∫ßÂà´Ôºö
1. **Â∑•ÂÖ∑ËØÜÂà´**ÔºöË¶ÅÊ±ÇÊ®°ÂûãËØÜÂà´Â∑•ÂÖ∑ÁöÑÂü∫Êú¨ÂäüËÉΩ„ÄÇ
2. **Â∑•ÂÖ∑ÁêÜËß£**ÔºöË¶ÅÊ±ÇÊ®°ÂûãÁêÜËß£Â∑•ÂÖ∑ÁöÑÂ∑•‰ΩúÂéüÁêÜ„ÄÇ
3. **Â∑•ÂÖ∑ÂàõÈÄ†**ÔºöË¶ÅÊ±ÇÊ®°ÂûãÂú®Ê≤°ÊúâÁé∞ÊúâÂ∑•ÂÖ∑ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂà©Áî®Âë®Âõ¥ÁöÑÁâ©‰ΩìÂàõÈÄ†Êñ∞ÁöÑÂ∑•ÂÖ∑„ÄÇ

ÂØπ‰∫éÊØè‰∏™ÂõæÂÉè-ÊñáÊú¨ÂØπÔºåÊ®°ÂûãÈúÄË¶ÅÂõûÁ≠î‰∏éÂ∑•ÂÖ∑Áõ∏ÂÖ≥ÁöÑÂêÑÁßçÈóÆÈ¢òÔºå‰ªéËÄåËØÑ‰º∞ÂÖ∂ÂØπÂ∑•ÂÖ∑ÁöÑÁêÜËß£Á®ãÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPhysToolBenchÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÆÉÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπMLLMÁâ©ÁêÜÂ∑•ÂÖ∑ÁêÜËß£ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÂÆÉ‰∏ç‰ªÖÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê†áÂáÜÂåñÁöÑËØÑ‰º∞Âπ≥Âè∞ÔºåËøòÈÄöËøáÂàÜÂ±ÇÈöæÂ∫¶ËÆæËÆ°ÔºåÊõ¥ÂÖ®Èù¢Âú∞ËÄÉÂØü‰∫ÜÊ®°ÂûãÂú®‰∏çÂêåËÆ§Áü•Â±ÇÈù¢ÁöÑË°®Áé∞„ÄÇÊ≠§Â§ñÔºåËØ•Âü∫ÂáÜËøò‰øÉËøõ‰∫ÜÂØπÁé∞ÊúâMLLMÂú®Â∑•ÂÖ∑ÁêÜËß£ÊñπÈù¢‰∏çË∂≥‰πãÂ§ÑÁöÑÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫ËøáÁ®ã‰∏≠ÔºåÂõæÂÉèÁöÑÈÄâÊã©ÂíåÊñáÊú¨ÈóÆÈ¢òÁöÑËÆæËÆ°ÈÉΩÁªèËøáÁ≤æÂøÉËÄÉËôëÔºå‰ª•Á°Æ‰øùËÉΩÂ§üÊúâÊïàÂú∞ËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêåÈöæÂ∫¶Á∫ßÂà´‰∏äÁöÑËÉΩÂäõ„ÄÇ‰æãÂ¶ÇÔºåÂú®Â∑•ÂÖ∑ÂàõÈÄ†ÊñπÈù¢ÔºåÊï∞ÊçÆÈõÜ‰ºöÊèê‰æõ‰∏Ä‰∫õÂë®Âõ¥Áâ©‰ΩìÁöÑÂõæÂÉèÔºåÂπ∂Ë¶ÅÊ±ÇÊ®°ÂûãÊèèËø∞Â¶Ç‰ΩïÂà©Áî®Ëøô‰∫õÁâ©‰ΩìÊù•Âà∂ÈÄ†‰∏Ä‰∏™ËÉΩÂ§üÂÆåÊàêÁâπÂÆö‰ªªÂä°ÁöÑÂ∑•ÂÖ∑„ÄÇÊ≠§Â§ñÔºåÊï∞ÊçÆÈõÜËøòÂåÖÂê´‰∫ÜÂ§öÁßçÁ±ªÂûãÁöÑÂ∑•ÂÖ∑Ôºå‰ª•Â¢ûÂä†ËØÑ‰º∞ÁöÑÂÖ®Èù¢ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂØπ32‰∏™MLLMÁöÑËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÁé∞ÊúâÊ®°ÂûãÂú®Â∑•ÂÖ∑ÁêÜËß£ÊñπÈù¢Â≠òÂú®ÊòæËëó‰∏çË∂≥„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊ®°ÂûãÂú®Â∑•ÂÖ∑ËØÜÂà´ÊñπÈù¢Ë°®Áé∞Áõ∏ÂØπËæÉÂ•ΩÔºå‰ΩÜÂú®Â∑•ÂÖ∑ÁêÜËß£ÂíåÂ∑•ÂÖ∑ÂàõÈÄ†ÊñπÈù¢Ë°®Áé∞ËæÉÂ∑Æ„ÄÇËøôË°®ÊòéÔºåÂ∞ΩÁÆ°MLLMÂú®ËßÜËßâÂíåËØ≠Ë®ÄÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÆÉ‰ª¨ÂØπÁâ©ÁêÜ‰∏ñÁïåÁöÑÁêÜËß£‰ªçÁÑ∂ÊúâÈôê„ÄÇËØ•Á†îÁ©∂‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêëÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÂêØÁ§∫„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PhysToolBenchÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫„ÄÅÊô∫ËÉΩÂà∂ÈÄ†„ÄÅËæÖÂä©ËÆæËÆ°Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáMLLMÂØπÁâ©ÁêÜÂ∑•ÂÖ∑ÁöÑÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫Êõ¥Êô∫ËÉΩÂú∞Êìç‰ΩúÂ∑•ÂÖ∑ÔºåÊèêÈ´òÁîü‰∫ßÊïàÁéáÔºõÂú®Êô∫ËÉΩÂà∂ÈÄ†‰∏≠ÔºåÂèØ‰ª•Â∏ÆÂä©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£Â∑•Ëâ∫ÊµÅÁ®ãÔºå‰ºòÂåñÁîü‰∫ßÊñπÊ°àÔºõÂú®ËæÖÂä©ËÆæËÆ°‰∏≠ÔºåÂèØ‰ª•‰∏∫ËÆæËÆ°Â∏àÊèê‰æõÊõ¥Êô∫ËÉΩÁöÑÂ∑•ÂÖ∑ÈÄâÊã©Âíå‰ΩøÁî®Âª∫ËÆÆ„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÊúâÊúõÊé®Âä®ÂÖ∑Ë∫´Êô∫ËÉΩÂíå‰∫∫Êú∫Âçè‰ΩúÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as a Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: (1) Tool Recognition: Requiring the recognition of a tool's primary function. (2) Tool Understanding: Testing the ability to grasp the underlying principles of a tool's operation. (3) Tool Creation: Challenging the model to fashion a new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source, specialized embodied, and backbones in VLAs-reveals a significant deficiency in tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available.

