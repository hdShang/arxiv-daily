---
layout: default
title: Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation
---

# Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09320" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09320v1</a>
  <a href="https://arxiv.org/pdf/2510.09320.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09320v1" onclick="toggleFavorite(this, '2510.09320v1', 'Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenyao Zhang, Hongsi Liu, Bohan Li, Jiawei He, Zekun Qi, Yunnan Wang, Shengyang Zhao, Xinqiang Yu, Wenjun Zeng, Xin Jin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

**å¤‡æ³¨**: ICCV 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Zhangwenyao1/Hybrid-depth)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHybrid-depthæ¡†æ¶ï¼Œåˆ©ç”¨ç²—ç»†ç²’åº¦ç‰¹å¾èåˆå’Œè¯­è¨€å¼•å¯¼æå‡è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªç›‘ç£æ·±åº¦ä¼°è®¡` `å•ç›®æ·±åº¦ä¼°è®¡` `å¤šç²’åº¦ç‰¹å¾èåˆ` `å¯¹æ¯”å­¦ä¹ ` `è¯­è¨€å¼•å¯¼` `è§†è§‰Transformer` `BEVæ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨æå–è¶³å¤Ÿçš„è¯­ä¹‰å’Œç©ºé—´çŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
2. Hybrid-depthæ¡†æ¶é€šè¿‡å¯¹æ¯”è¯­è¨€å¼•å¯¼ï¼ŒèåˆCLIPçš„å…¨å±€è¯­ä¹‰å’ŒDINOçš„å±€éƒ¨ç©ºé—´ç»†èŠ‚ï¼Œå®ç°å¤šç²’åº¦ç‰¹å¾èšåˆã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨KITTIæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½æå‡ä¸‹æ¸¸BEVæ„ŸçŸ¥ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHybrid-depthçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆåŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPå’ŒDINOï¼‰æå–çš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶è·å–å……åˆ†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå…‹æœå½“å‰è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰æ–¹æ³•åœ¨è¯­ä¹‰-ç©ºé—´çŸ¥è¯†æå–æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§ç”±ç²—åˆ°ç²¾çš„æ¸è¿›å¼å­¦ä¹ æ¡†æ¶ï¼šé¦–å…ˆï¼Œåœ¨å¯¹æ¯”è¯­è¨€å¼•å¯¼ä¸‹ï¼Œèšåˆæ¥è‡ªCLIPï¼ˆå…¨å±€è¯­ä¹‰ï¼‰å’ŒDINOï¼ˆå±€éƒ¨ç©ºé—´ç»†èŠ‚ï¼‰çš„å¤šç²’åº¦ç‰¹å¾ã€‚è®¾è®¡äº†ä¸€ä¸ªæ¯”è¾ƒè¿œè¿‘å›¾åƒå—çš„ä»£ç†ä»»åŠ¡ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºå¼ºåˆ¶è¿›è¡Œæ·±åº¦æ„ŸçŸ¥çš„ç‰¹å¾å¯¹é½ï¼›å…¶æ¬¡ï¼Œåœ¨ç²—ç•¥ç‰¹å¾çš„åŸºç¡€ä¸Šï¼Œæ•´åˆç›¸æœºä½å§¿ä¿¡æ¯å’Œåƒç´ çº§è¯­è¨€å¯¹é½æ¥ç»†åŒ–æ·±åº¦é¢„æµ‹ã€‚è¯¥æ¨¡å—å¯ä»¥ä½œä¸ºå³æ’å³ç”¨çš„æ·±åº¦ç¼–ç å™¨æ— ç¼é›†æˆåˆ°ç°æœ‰çš„è‡ªç›‘ç£MDEæµç¨‹ï¼ˆå¦‚Monodepth2ã€ManyDepthï¼‰ä¸­ï¼Œä»è€Œå¢å¼ºè¿ç»­æ·±åº¦ä¼°è®¡ã€‚é€šè¿‡è¯­è¨€å¼•å¯¼èšåˆCLIPçš„è¯­ä¹‰ä¸Šä¸‹æ–‡å’ŒDINOçš„ç©ºé—´ç»†èŠ‚ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†ç‰¹å¾ç²’åº¦ä¸åŒ¹é…çš„é—®é¢˜ã€‚åœ¨KITTIåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºSOTAæ–¹æ³•ï¼Œå¹¶ä¸”ç¡®å®æœ‰åˆ©äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚BEVæ„ŸçŸ¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•éš¾ä»¥å……åˆ†æå–å›¾åƒä¸­çš„è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯ï¼Œå¯¼è‡´æ·±åº¦ä¼°è®¡ç²¾åº¦å—é™ã€‚å°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶ï¼Œç¼ºä¹è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç»†ç²’åº¦ç‰¹å¾ï¼Œä½¿å¾—æ·±åº¦é¢„æµ‹å®¹æ˜“å‡ºç°è¯¯å·®ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€å°ºåº¦çš„ç‰¹å¾ï¼Œæ— æ³•æœ‰æ•ˆèåˆå…¨å±€è¯­ä¹‰ä¿¡æ¯å’Œå±€éƒ¨ç©ºé—´ç»†èŠ‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHybrid-depthçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„CLIPå’ŒDINOæ¨¡å‹ï¼Œåˆ†åˆ«æå–å›¾åƒçš„å…¨å±€è¯­ä¹‰ç‰¹å¾å’Œå±€éƒ¨ç©ºé—´ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¯¹æ¯”è¯­è¨€å¼•å¯¼çš„æ–¹å¼ï¼Œå°†è¿™äº›å¤šç²’åº¦ç‰¹å¾è¿›è¡Œæœ‰æ•ˆèåˆã€‚é€šè¿‡å¼•å…¥è¯­è¨€ä¿¡æ¯ä½œä¸ºæ¡¥æ¢ï¼Œä¿ƒä½¿ä¸åŒæ¨¡æ€çš„ç‰¹å¾å¯¹é½ï¼Œä»è€Œæå‡æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHybrid-depthæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) å¤šç²’åº¦ç‰¹å¾èšåˆï¼šåˆ©ç”¨CLIPæå–å…¨å±€è¯­ä¹‰ç‰¹å¾ï¼ŒDINOæå–å±€éƒ¨ç©ºé—´ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œè¯­è¨€å¼•å¯¼ï¼Œå°†è¿™äº›ç‰¹å¾è¿›è¡Œèåˆã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ä¸ªä»£ç†ä»»åŠ¡ï¼Œæ¯”è¾ƒå›¾åƒä¸­è¿œè¿‘å›¾åƒå—çš„æ·±åº¦å·®å¼‚ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼ç‰¹å¾å¯¹é½ã€‚2) æ·±åº¦ç»†åŒ–ï¼šåœ¨ç²—ç•¥çš„æ·±åº¦é¢„æµ‹åŸºç¡€ä¸Šï¼Œæ•´åˆç›¸æœºä½å§¿ä¿¡æ¯å’Œåƒç´ çº§è¯­è¨€å¯¹é½ï¼Œè¿›ä¸€æ­¥ç»†åŒ–æ·±åº¦é¢„æµ‹ç»“æœã€‚è¯¥æ¨¡å—å¯ä»¥ä½œä¸ºå³æ’å³ç”¨çš„æ·±åº¦ç¼–ç å™¨é›†æˆåˆ°ç°æœ‰çš„è‡ªç›‘ç£æ·±åº¦ä¼°è®¡æµç¨‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†æ··åˆç²’åº¦çš„ç‰¹å¾èšåˆæ–¹å¼ï¼Œé€šè¿‡å¯¹æ¯”è¯­è¨€å¼•å¯¼ï¼Œæœ‰æ•ˆåœ°èåˆäº†CLIPçš„å…¨å±€è¯­ä¹‰ä¿¡æ¯å’ŒDINOçš„å±€éƒ¨ç©ºé—´ç»†èŠ‚ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç‰¹å¾ç²’åº¦ä¸åŒ¹é…çš„é—®é¢˜ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å›¾åƒçš„è¯­ä¹‰å’Œç©ºé—´ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¤šç²’åº¦ç‰¹å¾èšåˆé˜¶æ®µï¼Œä½¿ç”¨äº†å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œä¿ƒä½¿æ¨¡å‹å­¦ä¹ åˆ°æ·±åº¦æ„ŸçŸ¥çš„ç‰¹å¾è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ ï¼Œé€‰æ‹©å…¶å‘¨å›´çš„è¿‘é‚»åƒç´ å’Œè¿œè·ç¦»åƒç´ ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬æç¤ºæ¥æè¿°è¿™äº›åƒç´ ä¹‹é—´çš„æ·±åº¦å…³ç³»ã€‚é€šè¿‡æœ€å°åŒ–å¯¹æ¯”æŸå¤±ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†è¿œè¿‘åƒç´ ï¼Œä»è€Œæå‡æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œåœ¨æ·±åº¦ç»†åŒ–é˜¶æ®µï¼Œä½¿ç”¨äº†åƒç´ çº§çš„è¯­è¨€å¯¹é½æŸå¤±ï¼Œè¿›ä¸€æ­¥æå‡äº†æ·±åº¦é¢„æµ‹çš„ç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨KITTIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHybrid-depthæ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºSOTAæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»å¯¹ç›¸å¯¹è¯¯å·®ï¼ˆAbs Relï¼‰æŒ‡æ ‡ä¸Šï¼Œç›¸æ¯”äºç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œæ€§èƒ½æå‡è¶…è¿‡5%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿæœ‰æ•ˆæå‡ä¸‹æ¸¸BEVæ„ŸçŸ¥ä»»åŠ¡çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚ç²¾ç¡®çš„å•ç›®æ·±åº¦ä¼°è®¡æ˜¯è¿™äº›åº”ç”¨åœºæ™¯ä¸­çš„å…³é”®æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´å®‰å…¨ã€æ›´æ™ºèƒ½çš„å†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºä¸‰ç»´é‡å»ºã€åœºæ™¯ç†è§£ç­‰ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current self-supervised monocular depth estimation (MDE) approaches encounter performance limitations due to insufficient semantic-spatial knowledge extraction. To address this challenge, we propose Hybrid-depth, a novel framework that systematically integrates foundation models (e.g., CLIP and DINO) to extract visual priors and acquire sufficient contextual information for MDE. Our approach introduces a coarse-to-fine progressive learning framework: 1) Firstly, we aggregate multi-grained features from CLIP (global semantics) and DINO (local spatial details) under contrastive language guidance. A proxy task comparing close-distant image patches is designed to enforce depth-aware feature alignment using text prompts; 2) Next, building on the coarse features, we integrate camera pose information and pixel-wise language alignment to refine depth predictions. This module seamlessly integrates with existing self-supervised MDE pipelines (e.g., Monodepth2, ManyDepth) as a plug-and-play depth encoder, enhancing continuous depth estimation. By aggregating CLIP's semantic context and DINO's spatial details through language guidance, our method effectively addresses feature granularity mismatches. Extensive experiments on the KITTI benchmark demonstrate that our method significantly outperforms SOTA methods across all metrics, which also indeed benefits downstream tasks like BEV perception. Code is available at https://github.com/Zhangwenyao1/Hybrid-depth.

