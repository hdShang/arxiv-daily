---
layout: default
title: Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning
---

# Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08938" target="_blank" class="toolbar-btn">arXiv: 2510.08938v1</a>
    <a href="https://arxiv.org/pdf/2510.08938.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08938v1" 
            onclick="toggleFavorite(this, '2510.08938v1', 'Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhen Yang, Yansong Ma, Lei Chen

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèåÂ±ÇÂÖÉÁ≠ñÁï•ÊéßÂà∂‰ª•Ëß£ÂÜ≥Âä®ÊÄÅ‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `ËØÅÊçÆÊ∑±Â∫¶Â≠¶‰π†` `‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜ` `ÂÖÉÂ≠¶‰π†` `Âä®ÊÄÅÊï∞ÊçÆÂàÜÂ∏É` `KLÊï£Â∫¶` `DirichletÂÖàÈ™å` `È´òÈ£éÈô©ÂÜ≥Á≠ñ` `Ê®°ÂûãÂèØÈù†ÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËØÅÊçÆÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ï‰æùËµñÈùôÊÄÅË∂ÖÂèÇÊï∞ÔºåÊó†Ê≥ïÈÄÇÂ∫îÂä®ÊÄÅÊï∞ÊçÆÂàÜÂ∏ÉÔºåÂØºËá¥‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜÊïàÊûúÂ∑Æ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑÂÖÉÁ≠ñÁï•ÊéßÂà∂Âô®ÔºàMPCÔºâÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥KLÊï£Â∫¶Á≥ªÊï∞ÂíåDirichletÂÖàÈ™åÂº∫Â∫¶ÔºåÊèêÂçá‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMPCÂú®Â§ö‰∏™‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÔºåÊîπÂñÑ‰∫Ü‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜÊïàÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰º†ÁªüÁöÑËØÅÊçÆÊ∑±Â∫¶Â≠¶‰π†ÔºàEDLÔºâÊñπÊ≥ï‰æùËµñ‰∫éÈùôÊÄÅË∂ÖÂèÇÊï∞ËøõË°å‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Âú®Âä®ÊÄÅÊï∞ÊçÆÂàÜÂ∏É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÔºåÂØºËá¥Âú®È´òÈ£éÈô©ÂÜ≥Á≠ñ‰ªªÂä°‰∏≠ÁöÑÊ†°ÂáÜÂíåÊ≥õÂåñÊÄßËÉΩËæÉÂ∑Æ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈôêÂà∂ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂÖÉÁ≠ñÁï•ÊéßÂà∂Âô®ÔºàMPCÔºâÔºåËøôÊòØ‰∏Ä‰∏™Âä®ÊÄÅÂÖÉÂ≠¶‰π†Ê°ÜÊû∂ÔºåËÉΩÂ§üË∞ÉÊï¥KLÊï£Â∫¶Á≥ªÊï∞ÂíåDirichletÂÖàÈ™åÂº∫Â∫¶Ôºå‰ª•ÂÆûÁé∞ÊúÄ‰Ω≥ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°„ÄÇMPCÈááÁî®ÂèåÂ±Ç‰ºòÂåñÊñπÊ≥ïÔºöÂú®ÂÜÖÂ±ÇÔºåÈÄöËøáÂä®ÊÄÅÈÖçÁΩÆÁöÑÊçüÂ§±ÂáΩÊï∞Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞ÔºõÂú®Â§ñÂ±ÇÔºåÁ≠ñÁï•ÁΩëÁªúÂü∫‰∫éÂ§öÁõÆÊ†áÂ•ñÂä±‰ºòÂåñKLÊï£Â∫¶Á≥ªÊï∞ÂíåÁ±ªÁâπÂÆöÁöÑDirichletÂÖàÈ™åÂº∫Â∫¶„ÄÇ‰∏é‰ª•ÂæÄÂõ∫ÂÆöÂÖàÈ™åÁöÑÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÂèØÂ≠¶‰π†DirichletÂÖàÈ™åËÉΩÂ§üÁÅµÊ¥ªÈÄÇÂ∫îÁ±ªÂàÜÂ∏ÉÂíåËÆ≠ÁªÉÂä®ÊÄÅ„ÄÇÂ§ßÈáèÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMPCÊòæËëóÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÈ¢ÑÊµãÁöÑÂèØÈù†ÊÄßÂíåÊ†°ÂáÜÊÄßÔºåÊèêÈ´ò‰∫Ü‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜ„ÄÅÈ¢ÑÊµãÂáÜÁ°ÆÊÄß‰ª•ÂèäÂú®Âü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÊ†∑Êú¨ÊãíÁªùÂêéÁöÑÊÄßËÉΩ‰øùÊåÅ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüËØÅÊçÆÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÂú®Âä®ÊÄÅÊï∞ÊçÆÂàÜÂ∏É‰∏ãÁöÑ‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñÈùôÊÄÅË∂ÖÂèÇÊï∞ÔºåÂØºËá¥Âú®È´òÈ£éÈô©ÂÜ≥Á≠ñ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÂÖÉÁ≠ñÁï•ÊéßÂà∂Âô®ÔºàMPCÔºâÔºåÈÄöËøáÂèåÂ±Ç‰ºòÂåñÊ°ÜÊû∂Âä®ÊÄÅË∞ÉÊï¥Ê®°ÂûãÁöÑKLÊï£Â∫¶Á≥ªÊï∞ÂíåDirichletÂÖàÈ™åÂº∫Â∫¶Ôºå‰ª•ÂÆûÁé∞Êõ¥ÁÅµÊ¥ªÁöÑÈÄÇÂ∫îÊÄßÂíåÊõ¥Â•ΩÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂª∫Ê®°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMPCÈááÁî®ÂèåÂ±Ç‰ºòÂåñÁªìÊûÑÔºåÂÜÖÂ±ÇÈÄöËøáÂä®ÊÄÅÊçüÂ§±ÂáΩÊï∞Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞ÔºåÂ§ñÂ±ÇÈÄöËøáÁ≠ñÁï•ÁΩëÁªú‰ºòÂåñKLÊï£Â∫¶Á≥ªÊï∞ÂíåDirichletÂÖàÈ™åÂº∫Â∫¶ÔºåÂπ≥Ë°°È¢ÑÊµãÂáÜÁ°ÆÊÄß‰∏é‰∏çÁ°ÆÂÆöÊÄßË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMPCÁöÑÂèØÂ≠¶‰π†DirichletÂÖàÈ™åÊòØÂÖ∂‰∏ªË¶ÅÂàõÊñ∞ÁÇπÔºå‰∏é‰º†ÁªüÂõ∫ÂÆöÂÖàÈ™åÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊ†πÊçÆÁ±ªÂàÜÂ∏ÉÂíåËÆ≠ÁªÉÂä®ÊÄÅÁÅµÊ¥ªË∞ÉÊï¥„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÔºåÂÜÖÂ±ÇÊçüÂ§±ÂáΩÊï∞Ê†πÊçÆÂΩìÂâçËÆ≠ÁªÉÁä∂ÊÄÅÂä®ÊÄÅÈÖçÁΩÆÔºåÂ§ñÂ±ÇÁ≠ñÁï•ÁΩëÁªúÂàôÂü∫‰∫éÂ§öÁõÆÊ†áÂ•ñÂä±ËøõË°å‰ºòÂåñÔºåÁ°Æ‰øùÊ®°ÂûãÂú®‰∏çÂêå‰ªªÂä°‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÊÄßËÉΩÊèêÂçá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMPCÂú®Â§ö‰∏™Âü∫ÂáÜ‰ªªÂä°‰∏äÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÈ¢ÑÊµãÂáÜÁ°ÆÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õ‰ªªÂä°‰∏≠ÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåMPCÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÊèêÈ´ò‰∫ÜÁ∫¶15%Ôºå‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜÊïàÊûúÊèêÂçá‰∫Ü20%‰ª•‰∏äÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Âä®ÊÄÅÊï∞ÊçÆÁéØÂ¢É‰∏≠ÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂåªÁñóËØäÊñ≠„ÄÅËá™Âä®È©æÈ©∂„ÄÅÈáëËûçÈ£éÈô©ËØÑ‰º∞Á≠âÈ´òÈ£éÈô©ÂÜ≥Á≠ñÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÈ´ò‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜËÉΩÂäõÔºåMPCËÉΩÂ§üÂ∏ÆÂä©ÂÜ≥Á≠ñËÄÖÊõ¥Â•ΩÂú∞ÁêÜËß£Ê®°ÂûãÈ¢ÑÊµãÁöÑÂèØÈù†ÊÄßÔºå‰ªéËÄåÂÅöÂá∫Êõ¥‰∏∫ÂáÜÁ°ÆÁöÑÂÜ≥Á≠ñÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Traditional Evidence Deep Learning (EDL) methods rely on static hyperparameter for uncertainty calibration, limiting their adaptability in dynamic data distributions, which results in poor calibration and generalization in high-risk decision-making tasks. To address this limitation, we propose the Meta-Policy Controller (MPC), a dynamic meta-learning framework that adjusts the KL divergence coefficient and Dirichlet prior strengths for optimal uncertainty modeling. Specifically, MPC employs a bi-level optimization approach: in the inner loop, model parameters are updated through a dynamically configured loss function that adapts to the current training state; in the outer loop, a policy network optimizes the KL divergence coefficient and class-specific Dirichlet prior strengths based on multi-objective rewards balancing prediction accuracy and uncertainty quality. Unlike previous methods with fixed priors, our learnable Dirichlet prior enables flexible adaptation to class distributions and training dynamics. Extensive experimental results show that MPC significantly enhances the reliability and calibration of model predictions across various tasks, improving uncertainty calibration, prediction accuracy, and performance retention after confidence-based sample rejection.

