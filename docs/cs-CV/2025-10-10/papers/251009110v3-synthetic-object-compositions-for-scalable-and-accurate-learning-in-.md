---
layout: default
title: Synthetic Object Compositions for Scalable and Accurate Learning in Detection, Segmentation, and Grounding
---

# Synthetic Object Compositions for Scalable and Accurate Learning in Detection, Segmentation, and Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09110" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09110v3</a>
  <a href="https://arxiv.org/pdf/2510.09110.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09110v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09110v3', 'Synthetic Object Compositions for Scalable and Accurate Learning in Detection, Segmentation, and Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weikai Huang, Jieyu Zhang, Taoyang Jia, Chenhao Zheng, Ziqi Gao, Jae Sung Park, Winson Han, Ranjay Krishna

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10 (æ›´æ–°: 2025-11-21)

**å¤‡æ³¨**: Project website: https://github.com/weikaih04/Synthetic-Detection-Segmentation-Grounding-Data

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSOCï¼šä¸€ç§å¯æ‰©å±•ã€ç²¾ç¡®çš„åˆæˆå¯¹è±¡ç»„åˆæ–¹æ³•ï¼Œç”¨äºæå‡æ£€æµ‹ã€åˆ†å‰²å’Œå®šä½ä»»åŠ¡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆæˆæ•°æ®ç”Ÿæˆ` `ç›®æ ‡æ£€æµ‹` `å®ä¾‹åˆ†å‰²` `è§†è§‰å®šä½` `æ•°æ®å¢å¼º` `3Då‡ ä½•å¸ƒå±€` `ç”Ÿæˆå¼å’Œè°åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰ä»»åŠ¡ä¾èµ–çš„å¤§è§„æ¨¡æ•°æ®é›†å­˜åœ¨æ„å»ºæˆæœ¬é«˜ã€è¦†ç›–åå·®å’Œéš¾ä»¥æ‰©å±•ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. SOCé€šè¿‡å¯¹è±¡ä¸­å¿ƒç»„åˆç­–ç•¥ï¼Œç»“åˆ3Då‡ ä½•å¸ƒå±€å’Œç›¸æœºé…ç½®å¢å¼ºï¼Œä»¥åŠç”Ÿæˆå¼å’Œè°åŒ–ç­‰æŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„åˆæˆæ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä»…ç”¨å°‘é‡SOCåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºåœ¨æ›´å¤§è§„æ¨¡çš„çœŸå®æˆ–åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶åœ¨ä½æ•°æ®åœºæ™¯ä¸‹è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰åˆ†ç»„ï¼ˆé€šè¿‡å®ä¾‹åˆ†å‰²ã€è§†è§‰å®šä½å’Œç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡å®ç°ï¼‰æ”¯æŒä»æœºå™¨äººæ„ŸçŸ¥åˆ°ç…§ç‰‡ç¼–è¾‘ç­‰åº”ç”¨ã€‚è¿™äº›è®¡ç®—æœºè§†è§‰ä¸­çš„åŸºæœ¬é—®é¢˜ä¾èµ–äºå¤§è§„æ¨¡ã€ç²¾å¿ƒæ ‡æ³¨çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›æ•°æ®é›†æ„å»ºæˆæœ¬é«˜æ˜‚ã€è¦†ç›–èŒƒå›´å­˜åœ¨åå·®ä¸”éš¾ä»¥æ‰©å±•ã€‚åˆæˆæ•°æ®é›†æä¾›äº†ä¸€ç§æœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é¢ä¸´çµæ´»æ€§ã€å‡†ç¡®æ€§å’Œç»„åˆå¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåˆæˆå¯¹è±¡ç»„åˆï¼ˆSOCï¼‰çš„ç²¾ç¡®ä¸”å¯æ‰©å±•çš„æ•°æ®åˆæˆæµç¨‹ï¼Œè¯¥æµç¨‹é€šè¿‡ä¸€ç§æ–°é¢–çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç»„åˆç­–ç•¥å®ç°ã€‚å®ƒä½¿ç”¨3Då‡ ä½•å¸ƒå±€å¢å¼ºå’Œç›¸æœºé…ç½®å¢å¼ºï¼Œç»“åˆç”Ÿæˆå¼å’Œè°åŒ–å’Œæ©ç é¢ç§¯åŠ æƒæ··åˆï¼Œå°†é«˜è´¨é‡çš„åˆæˆå¯¹è±¡åˆ†å‰²ç»„åˆæˆæ–°çš„å›¾åƒï¼Œä»è€Œäº§ç”Ÿå‡†ç¡®ä¸”å¤šæ ·åŒ–çš„æ©ç ã€æ¡†å’ŒæŒ‡ä»£è¡¨è¾¾å¼ã€‚ä»…ä½¿ç”¨10ä¸‡å¼ åˆæˆå›¾åƒè®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºåœ¨æ›´å¤§çš„çœŸå®æ•°æ®é›†ï¼ˆGRIT 20Mã€V3Det 200Kï¼‰å’Œåˆæˆæµç¨‹ï¼ˆCopy-Pasteã€X-Pasteã€SynGroundã€SegGenï¼‰ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæå‡å¹…åº¦ä¸º+24-36%ï¼Œåœ¨LVISä¸Šè¾¾åˆ°+10.9 APï¼Œåœ¨gRefCOCOä¸Šè¾¾åˆ°+8.4 NAccã€‚é™¤äº†é€šç”¨çš„å¼€æ”¾è¯æ±‡è®¾ç½®å¤–ï¼ŒSOCè¿˜æ”¯æŒé’ˆå¯¹ä¸åŒç”¨ä¾‹çš„å¯æ§æ•°æ®é›†æ„å»ºï¼Œå¹¶æé«˜äº†åœ¨ä½æ•°æ®å’Œå°é—­è¯æ±‡åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨åˆæˆå¯¹è±¡åˆ†å‰²å¢å¼ºLVISå’ŒCOCOï¼Œå¯ä»¥åœ¨ä¸åŒçš„çœŸå®æ•°æ®è§„æ¨¡ä¸‹å®ç°å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ææœ‰é™çš„çœŸå®æ•°æ®æ¡ä»¶ä¸‹äº§ç”Ÿæ›´å¤§çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬åœ¨1% COCOæ•°æ®è®¾ç½®ä¸‹+6.59 APã€‚æ­¤å¤–ï¼Œè¿™ç§å¯æ§æ€§ä½¿å¾—èƒ½å¤Ÿé’ˆå¯¹ç±»å†…æŒ‡ä»£è¿›è¡Œç›®æ ‡æ•°æ®ç”Ÿæˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éœ€è¦ç»†ç²’åº¦å±æ€§åŒºåˆ†çš„è¯Šæ–­æ€§å®šä½ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰ä»»åŠ¡ä¸­å¯¹å¤§è§„æ¨¡ã€é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨çœŸå®æ•°æ®æˆ–ç®€å•çš„åˆæˆæ•°æ®å¢å¼ºï¼Œå­˜åœ¨æ ‡æ³¨æˆæœ¬é«˜ã€æ•°æ®åå·®å¤§ã€åˆæˆæ•°æ®è´¨é‡ä½ç­‰ç—›ç‚¹ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹ï¼Œé€šè¿‡å°†é«˜è´¨é‡çš„åˆæˆå¯¹è±¡åˆ†å‰²ç»„åˆæˆæ–°çš„å›¾åƒï¼Œå¹¶åˆ©ç”¨3Då‡ ä½•å¸ƒå±€å’Œç›¸æœºé…ç½®å¢å¼ºæ•°æ®çš„å¤šæ ·æ€§ï¼ŒåŒæ—¶é‡‡ç”¨ç”Ÿæˆå¼å’Œè°åŒ–æŠ€æœ¯æé«˜åˆæˆæ•°æ®çš„çœŸå®æ„Ÿã€‚è¿™æ ·æ—¢èƒ½é™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ï¼Œåˆèƒ½ä¿è¯æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSOCçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **å¯¹è±¡åˆ†å‰²åº“æ„å»º**ï¼šæ”¶é›†æˆ–ç”Ÿæˆé«˜è´¨é‡çš„3Då¯¹è±¡æ¨¡å‹å’Œå¯¹åº”çš„åˆ†å‰²æ©ç ã€‚2) **åœºæ™¯å¸ƒå±€**ï¼šåˆ©ç”¨3Då‡ ä½•å¸ƒå±€ç®—æ³•ï¼Œå°†å¯¹è±¡æ”¾ç½®åœ¨è™šæ‹Ÿåœºæ™¯ä¸­ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ç‰©ä½“æ’åˆ—æ–¹å¼ã€‚3) **ç›¸æœºé…ç½®**ï¼šéšæœºè°ƒæ•´è™šæ‹Ÿç›¸æœºçš„å‚æ•°ï¼Œå¦‚ä½ç½®ã€è§’åº¦å’Œç„¦è·ï¼Œä»¥ç”Ÿæˆä¸åŒçš„è§†è§’ã€‚4) **å›¾åƒåˆæˆ**ï¼šå°†å¯¹è±¡åˆ†å‰²å’Œåœºæ™¯å¸ƒå±€ä¿¡æ¯æ¸²æŸ“æˆå›¾åƒï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆå¼å’Œè°åŒ–æŠ€æœ¯ï¼Œä½¿åˆæˆå¯¹è±¡ä¸èƒŒæ™¯æ›´åŠ èåˆã€‚5) **æ•°æ®æ ‡æ³¨**ï¼šè‡ªåŠ¨ç”Ÿæˆç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè§†è§‰å®šä½æ‰€éœ€çš„æ ‡æ³¨ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šSOCæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç»„åˆç­–ç•¥å’Œ3Då‡ ä½•å¸ƒå±€å¢å¼ºã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒçº§åˆæˆæ–¹æ³•ä¸åŒï¼ŒSOCç›´æ¥æ“ä½œå¯¹è±¡åˆ†å‰²ï¼Œå¯ä»¥æ›´ç²¾ç¡®åœ°æ§åˆ¶åˆæˆæ•°æ®çš„å±æ€§å’Œå…³ç³»ã€‚3Då‡ ä½•å¸ƒå±€å¢å¼ºåˆ™å¯ä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ç‰©ä½“æ’åˆ—æ–¹å¼ï¼Œæé«˜åˆæˆæ•°æ®çš„çœŸå®æ„Ÿå’Œå¤šæ ·æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šSOCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é«˜è´¨é‡çš„3Då¯¹è±¡æ¨¡å‹å’Œåˆ†å‰²æ©ç ï¼Œä¿è¯åˆæˆæ•°æ®çš„å‡†ç¡®æ€§ã€‚2) é‡‡ç”¨æ©ç é¢ç§¯åŠ æƒæ··åˆï¼Œä½¿åˆæˆå¯¹è±¡ä¸èƒŒæ™¯æ›´åŠ è‡ªç„¶åœ°èåˆã€‚3) åˆ©ç”¨ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰è¿›è¡Œå›¾åƒå’Œè°åŒ–ï¼Œæé«˜åˆæˆæ•°æ®çš„çœŸå®æ„Ÿã€‚4) è®¾è®¡äº†é’ˆå¯¹ç±»å†…æŒ‡ä»£çš„è¯Šæ–­æ€§å®šä½ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹å¯¹ç»†ç²’åº¦å±æ€§çš„åŒºåˆ†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨10ä¸‡å¼ SOCåˆæˆå›¾åƒè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨LVISæ•°æ®é›†ä¸Šè¾¾åˆ°äº†+10.9 APçš„æå‡ï¼Œåœ¨gRefCOCOæ•°æ®é›†ä¸Šè¾¾åˆ°äº†+8.4 NAccçš„æå‡ï¼Œæ˜¾è‘—ä¼˜äºåœ¨æ›´å¤§è§„æ¨¡çš„çœŸå®æ•°æ®é›†ï¼ˆGRIT 20Mã€V3Det 200Kï¼‰å’Œåˆæˆæµç¨‹ï¼ˆCopy-Pasteã€X-Pasteã€SynGroundã€SegGenï¼‰ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚åœ¨1% COCOæ•°æ®è®¾ç½®ä¸‹ï¼Œä½¿ç”¨SOCè¿›è¡Œæ•°æ®å¢å¼ºï¼Œå¯ä»¥è·å¾—+6.59 APçš„æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SOCå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæœºå™¨äººæ„ŸçŸ¥ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®‰é˜²ã€å›¾åƒç¼–è¾‘ç­‰é¢†åŸŸã€‚é€šè¿‡ç”Ÿæˆå¤§é‡é«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œå¯ä»¥é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŠ é€Ÿç›¸å…³ç®—æ³•çš„å¼€å‘å’Œéƒ¨ç½²ã€‚æ­¤å¤–ï¼ŒSOCçš„å¯æ§æ€§ä½¿å¾—èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šåœºæ™¯å’Œä»»åŠ¡ç”Ÿæˆå®šåˆ¶åŒ–çš„æ•°æ®é›†ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual grouping -- operationalized through tasks such as instance segmentation, visual grounding, and object detection -- enables applications ranging from robotic perception to photo editing. These fundamental problems in computer vision are powered by large-scale, painstakingly annotated datasets. Despite their impact, these datasets are costly to build, biased in coverage, and difficult to scale. Synthetic datasets offer a promising alternative but struggle with flexibility, accuracy, and compositional diversity.
>   We introduce Synthetic Object Compositions (SOC), an accurate and scalable data synthesis pipeline via a novel object-centric composition strategy. It composes high-quality synthetic object segments into new images using 3D geometric layout augmentation and camera configuration augmentation with generative harmonization and mask-area-weighted blending, yielding accurate and diverse masks, boxes, and referring expressions.
>   Models trained on just 100K of our synthetic images outperform those trained on larger real datasets (GRIT 20M, V3Det 200K) and synthetic pipelines (Copy-Paste, X-Paste, SynGround, SegGen) by +24-36% -- achieving +10.9 AP on LVIS and +8.4 NAcc on gRefCOCO. Beyond the general open-vocabulary setup, SOC also enables controllable dataset construction for different use cases and boosts performance in both low-data and closed-vocabulary scenarios.
>   Augmenting LVIS and COCO with synthetic object segments delivers strong performance across different real-data scales and yields even greater improvements under extremely limited real-data conditions, including +6.59 AP on a 1% COCO data setup. Furthermore, this controllability enables targeted data generation for intra-class referring, a diagnostic grounding task we propose that requires fine-grained attribute discrimination.

