---
layout: default
title: Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption
---

# Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09182" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09182v1</a>
  <a href="https://arxiv.org/pdf/2510.09182.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09182v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09182v1', 'Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Johann-Friedrich Feiden, Tim KÃ¼chler, Denis Zavadski, Bogdan Savchynskyy, Carsten Rother

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºoVDAï¼Œé€šè¿‡ç¼“å­˜å’Œæ©ç æŠ€æœ¯å®ç°ä½å†…å­˜ã€åœ¨çº¿è§†é¢‘æ·±åº¦ä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åœ¨çº¿è§†é¢‘æ·±åº¦ä¼°è®¡` `ä½å†…å­˜å ç”¨` `è¾¹ç¼˜è®¡ç®—` `æ—¶é—´ä¸€è‡´æ€§` `ç¼“å­˜æœºåˆ¶` `å¸§æ©ç ` `å•ç›®æ·±åº¦ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VDAæ–¹æ³•ä¾èµ–æ‰¹å¤„ç†ï¼Œæ— æ³•æ»¡è¶³åœ¨çº¿è§†é¢‘æ·±åº¦ä¼°è®¡çš„å®æ—¶æ€§éœ€æ±‚ï¼Œé™åˆ¶äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚
2. oVDAå€Ÿé‰´LLMçš„æ€è·¯ï¼Œé€šè¿‡ç¼“å­˜æ¨ç†è¿‡ç¨‹ä¸­çš„æ½œåœ¨ç‰¹å¾å’Œè®­ç»ƒæ—¶çš„å¸§æ©ç ï¼Œé™ä½äº†å†…å­˜å ç”¨ï¼Œå®ç°äº†åœ¨çº¿å¤„ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒoVDAåœ¨ç²¾åº¦å’ŒVRAMä½¿ç”¨ä¸Šä¼˜äºå…¶ä»–åœ¨çº¿æ–¹æ³•ï¼Œå¹¶åœ¨NVIDIA A100å’ŒJetsonè®¾å¤‡ä¸Šå®ç°äº†è¾ƒé«˜çš„å¸§ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å•ç›®è§†é¢‘æ·±åº¦ä¼°è®¡å·²æˆä¸ºè®¸å¤šå®é™…è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æœ€è¿‘ï¼ŒVideo Depth Anything (VDA) åœ¨é•¿è§†é¢‘åºåˆ—ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä¾èµ–äºæ‰¹å¤„ç†ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨åœ¨çº¿ç¯å¢ƒä¸­çš„ä½¿ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…‹æœäº†è¿™ä¸ªé™åˆ¶ï¼Œå¹¶å¼•å…¥äº†åœ¨çº¿VDA (oVDA)ã€‚å…³é”®åˆ›æ–°æ˜¯é‡‡ç”¨æ¥è‡ªå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„æŠ€æœ¯ï¼Œå³åœ¨æ¨ç†æœŸé—´ç¼“å­˜æ½œåœ¨ç‰¹å¾å¹¶åœ¨è®­ç»ƒæ—¶æ©ç›–å¸§ã€‚æˆ‘ä»¬çš„oVDAæ–¹æ³•åœ¨å‡†ç¡®æ€§å’ŒVRAMä½¿ç”¨æ–¹é¢éƒ½ä¼˜äºæ‰€æœ‰ç«äº‰çš„åœ¨çº¿è§†é¢‘æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚ä½VRAMä½¿ç”¨å¯¹äºåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å°¤å…¶é‡è¦ã€‚æˆ‘ä»¬è¯æ˜äº†oVDAåœ¨NVIDIA A100ä¸Šä»¥42 FPSè¿è¡Œï¼Œåœ¨NVIDIA Jetsonè¾¹ç¼˜è®¾å¤‡ä¸Šä»¥20 FPSè¿è¡Œã€‚æˆ‘ä»¬å°†å‘å¸ƒä»£ç å’Œç¼–è¯‘è„šæœ¬ï¼Œä½¿oVDAæ˜“äºéƒ¨ç½²åœ¨ä½åŠŸè€—ç¡¬ä»¶ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨çº¿è§†é¢‘æ·±åº¦ä¼°è®¡é—®é¢˜ï¼Œå³åœ¨è§†é¢‘æµå®æ—¶è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿå‡†ç¡®åœ°ä¼°è®¡æ¯ä¸€å¸§çš„æ·±åº¦ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯VDAï¼Œè™½ç„¶ç²¾åº¦é«˜ï¼Œä½†ä¾èµ–äºæ‰¹å¤„ç†ï¼Œéœ€è¦ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªè§†é¢‘åºåˆ—ï¼Œæ— æ³•æ»¡è¶³åœ¨çº¿åœºæ™¯çš„ä½å»¶è¿Ÿéœ€æ±‚ï¼Œä¸”å†…å­˜å ç”¨é«˜ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ç¼“å­˜æœºåˆ¶ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼“å­˜å…ˆå‰å¸§çš„æ½œåœ¨ç‰¹å¾ï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œä»è€Œé™ä½å†…å­˜å ç”¨å¹¶æé«˜å¤„ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥å¸§æ©ç ï¼Œå¢å¼ºæ¨¡å‹å¯¹é®æŒ¡å’Œè¿åŠ¨æ¨¡ç³Šçš„é²æ£’æ€§ï¼Œæé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šoVDAçš„æ•´ä½“æ¡†æ¶åŸºäºç°æœ‰çš„VDAæ¨¡å‹ï¼Œä¸»è¦åŒ…æ‹¬ç‰¹å¾æå–ã€ç‰¹å¾èåˆå’Œæ·±åº¦é¢„æµ‹ä¸‰ä¸ªæ¨¡å—ã€‚å…³é”®æ”¹è¿›åœ¨äºç‰¹å¾èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—å¼•å…¥äº†ç¼“å­˜æœºåˆ¶ï¼Œå°†å…ˆå‰å¸§çš„æ½œåœ¨ç‰¹å¾å­˜å‚¨åœ¨ç¼“å­˜ä¸­ï¼Œå¹¶åœ¨å½“å‰å¸§çš„ç‰¹å¾èåˆè¿‡ç¨‹ä¸­åˆ©ç”¨è¿™äº›ç¼“å­˜ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè®­ç»ƒè¿‡ç¨‹ä¹Ÿè¿›è¡Œäº†ä¿®æ”¹ï¼Œå¼•å…¥äº†å¸§æ©ç ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†LLMä¸­çš„ç¼“å­˜æœºåˆ¶å¼•å…¥åˆ°è§†é¢‘æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­ã€‚ä¸ä¼ ç»Ÿçš„æ‰¹å¤„ç†æ–¹æ³•ç›¸æ¯”ï¼ŒoVDAåªéœ€è¦å­˜å‚¨å°‘é‡å…ˆå‰å¸§çš„æ½œåœ¨ç‰¹å¾ï¼Œå¤§å¤§é™ä½äº†å†…å­˜å ç”¨ã€‚ä¸ç›´æ¥çš„åœ¨çº¿æ–¹æ³•ç›¸æ¯”ï¼ŒoVDAé€šè¿‡ç¼“å­˜ç‰¹å¾å®ç°äº†æ›´å¥½çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæé«˜äº†æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç¼“å­˜å¤§å°æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œå†³å®šäº†å†…å­˜å ç”¨å’Œæ—¶é—´ä¸€è‡´æ€§ä¹‹é—´çš„æƒè¡¡ã€‚è®ºæ–‡å¯èƒ½é‡‡ç”¨äº†æ»‘åŠ¨çª—å£çš„æ–¹å¼ç®¡ç†ç¼“å­˜ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„ç‰¹å¾æ›´æ–°ç­–ç•¥ã€‚å¸§æ©ç ç­–ç•¥å¯èƒ½åŒ…æ‹¬éšæœºæ©ç›–éƒ¨åˆ†å¸§æˆ–åŒºåŸŸï¼Œä»¥æ¨¡æ‹Ÿé®æŒ¡å’Œè¿åŠ¨æ¨¡ç³Šã€‚æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬æ·±åº¦é¢„æµ‹æŸå¤±ã€æ—¶é—´ä¸€è‡´æ€§æŸå¤±ç­‰ï¼Œä»¥ä¿è¯æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œæ—¶é—´ç¨³å®šæ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å’Œå‚æ•°è®¾ç½®éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

oVDAåœ¨å‡†ç¡®æ€§å’ŒVRAMä½¿ç”¨æ–¹é¢å‡ä¼˜äºç°æœ‰åœ¨çº¿è§†é¢‘æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒoVDAåœ¨NVIDIA A100 GPUä¸Šå®ç°äº†42 FPSçš„å¸§ç‡ï¼Œåœ¨NVIDIA Jetsonè¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†20 FPSçš„å¸§ç‡ï¼Œè¯æ˜äº†å…¶åœ¨ä½åŠŸè€—ç¡¬ä»¶ä¸Šçš„éƒ¨ç½²æ½œåŠ›ã€‚å…·ä½“çš„ç²¾åº¦æå‡å¹…åº¦éœ€è¦å‚è€ƒè®ºæ–‡ä¸­çš„å®éªŒæ•°æ®ï¼Œä¾‹å¦‚ä¸å…¶ä»–åœ¨çº¿æ–¹æ³•çš„æ·±åº¦ä¼°è®¡è¯¯å·®æŒ‡æ ‡å¯¹æ¯”ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

oVDAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨oVDAå®æ—¶æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒçš„æ·±åº¦ä¿¡æ¯ï¼Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£å‘¨å›´ç¯å¢ƒï¼Œè§„åˆ’æœ€ä½³è·¯å¾„ã€‚åœ¨AR/VRä¸­ï¼Œå¯ä»¥æä¾›æ›´é€¼çœŸçš„æ²‰æµ¸å¼ä½“éªŒã€‚ç”±äºå…¶ä½å†…å­˜å ç”¨å’Œé«˜æ•ˆç‡ï¼ŒoVDAç‰¹åˆ«é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œä¸ºç§»åŠ¨åº”ç”¨å’Œç‰©è”ç½‘è®¾å¤‡æä¾›å¼ºå¤§çš„æ·±åº¦æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Depth estimation from monocular video has become a key component of many real-world computer vision systems. Recently, Video Depth Anything (VDA) has demonstrated strong performance on long video sequences. However, it relies on batch-processing which prohibits its use in an online setting. In this work, we overcome this limitation and introduce online VDA (oVDA). The key innovation is to employ techniques from Large Language Models (LLMs), namely, caching latent features during inference and masking frames at training. Our oVDA method outperforms all competing online video depth estimation methods in both accuracy and VRAM usage. Low VRAM usage is particularly important for deployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an NVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release both, code and compilation scripts, making oVDA easy to deploy on low-power hardware.

