---
layout: default
title: Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models
---

# Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.09358" target="_blank" class="toolbar-btn">arXiv: 2510.09358v1</a>
    <a href="https://arxiv.org/pdf/2510.09358.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09358v1" 
            onclick="toggleFavorite(this, '2510.09358v1', 'Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Qihang Ma, Shengyu Li, Jie Tang, Dingkang Yang, Shaodong Chen, Yingyi Zhang, Chao Feng, Jiao Ran

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10

**Â§áÊ≥®**: EMNLP2025. Code is avaible at https://github.com/bytedance/DynamicCoT

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/bytedance/DynamicCoT)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âä®ÊÄÅÈìæÂºèÊÄùËÄÉÊñπÊ≥ïÔºåÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÂÖ≥ÈîÆÁü≠ËØ≠È¢ÑÊµã‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂÖ≥ÈîÆÁü≠ËØ≠È¢ÑÊµã` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `ÈìæÂºèÊÄùËÄÉ` `Âä®ÊÄÅÈìæÂºèÊÄùËÄÉ` `Ê®°ÂûãÂæÆË∞É` `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `ÁõëÁù£Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMMKPÊñπÊ≥ïÂú®Â§ÑÁêÜÊï∞ÊçÆÁº∫Â§±ÂíåÊú™ËßÅÂú∫ÊôØÊó∂Â≠òÂú®‰∏çË∂≥Ôºå‰∏îÁé∞ÊúâbenchmarkÂ≠òÂú®ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÈáçÂè†ÈóÆÈ¢òÔºåÂØºËá¥Ê®°ÂûãËÉΩÂäõË¢´È´ò‰º∞„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Âà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)Ëß£ÂÜ≥MMKP‰ªªÂä°ÔºåÂπ∂ÂºïÂÖ•Âä®ÊÄÅÈìæÂºèÊÄùËÄÉ(Dynamic CoT)Á≠ñÁï•ÔºåÊèêÂçáVLMÁöÑÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑDynamic CoTÁ≠ñÁï•Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊúâÊïàÊèêÂçá‰∫ÜMMKPÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂÖ≥ÈîÆÁü≠ËØ≠È¢ÑÊµã(MMKP)Êó®Âú®ÈÄöËøáÊï¥ÂêàÂ§öÁßçÊ®°ÊÄÅÁöÑËæìÂÖ•‰ø°ÊÅØÔºåÁîüÊàê‰∏ÄÁªÑÁªìËÆ∫ÊÄßÁöÑÁü≠ËØ≠Ôºå‰ªéËÄåË∂ÖË∂ä‰ªÖ‰æùËµñÊñáÊú¨ÁöÑÊñπÊ≥ï„ÄÇ‰º†ÁªüÁöÑÂ§öÊ®°ÊÄÅÊñπÊ≥ïÂú®Â§ÑÁêÜÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÁº∫Â§±ÂíåÊú™ËßÅÂú∫ÊôØÊó∂Â≠òÂú®ÊòæËëóÂ±ÄÈôêÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂèëÁé∞Áé∞ÊúâÂü∫ÂáÜÊµãËØïÁî±‰∫éËÆ≠ÁªÉÊµãËØï‰∏≠Â≠òÂú®ÊòæËëóÈáçÂè†ÔºåÈ´ò‰º∞‰∫ÜÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫Âà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)Êù•Ëß£ÂÜ≥MMKP‰ªªÂä°„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨‰ΩøÁî®‰∏§ÁßçÂπøÊ≥õ‰ΩøÁî®ÁöÑÁ≠ñÁï•Ôºå‰æãÂ¶ÇÈõ∂Ê†∑Êú¨ÂíåÁõëÁù£ÂæÆË∞É(SFT)Êù•ËØÑ‰º∞VLMÁöÑ‰∏ãÈôêÊÄßËÉΩ„ÄÇÊé•‰∏ãÊù•Ôºå‰∏∫‰∫ÜÊèêÈ´òVLMÁöÑÂ§çÊùÇÊé®ÁêÜËÉΩÂäõÔºåÊàë‰ª¨ÈááÁî®Fine-tune-CoTÔºåÂÆÉÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÁîüÊàêÁöÑÈ´òË¥®ÈáèCoTÊé®ÁêÜÊï∞ÊçÆÊù•ÂæÆË∞ÉËæÉÂ∞èÁöÑÊ®°Âûã„ÄÇÊúÄÂêéÔºå‰∏∫‰∫ÜËß£ÂÜ≥‚ÄúËøáÂ∫¶ÊÄùËÄÉ‚ÄùÁé∞Ë±°ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂä®ÊÄÅCoTÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•Âú®ËÆ≠ÁªÉÊúüÈó¥Ëá™ÈÄÇÂ∫îÂú∞Ê≥®ÂÖ•CoTÊï∞ÊçÆÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®Êé®ÁêÜÈò∂ÊÆµÁÅµÊ¥ªÂú∞Âà©Áî®ÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Âú®ÂêÑÁßçÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜÊâÄÊèêÂá∫ÁöÑÁ≠ñÁï•ÔºåÂÆûÈ™åÁªìÊûúËØÅÊòé‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ‰ª£Á†ÅÂèØÂú®https://github.com/bytedance/DynamicCoTËé∑Âæó„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂÖ≥ÈîÆÁü≠ËØ≠È¢ÑÊµãÔºàMMKPÔºâ‰ªªÂä°‰∏≠ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÊï∞ÊçÆÁº∫Â§±„ÄÅÊú™ËßÅÂú∫ÊôØ‰ª•ÂèäËÆ≠ÁªÉÊµãËØïÊï∞ÊçÆÈáçÂè†Á≠âÈóÆÈ¢ò‰∏äÁöÑ‰∏çË∂≥„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÂÖÖÂàÜÂà©Áî®Â§öÊ®°ÊÄÅ‰ø°ÊÅØËøõË°åÊúâÊïàÊé®ÁêÜÔºåÂØºËá¥È¢ÑÊµãÊÄßËÉΩÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõÔºåÂπ∂ÁªìÂêàÈìæÂºèÊÄùËÄÉÔºàCoTÔºâÊñπÊ≥ïÊù•Â¢ûÂº∫Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÈÅøÂÖçCoTÂ∏¶Êù•ÁöÑ‚ÄúËøáÂ∫¶ÊÄùËÄÉ‚ÄùÈóÆÈ¢òÔºåÊèêÂá∫‰∫ÜÂä®ÊÄÅCoTÁ≠ñÁï•Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Âà©Áî®Êé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ‰ΩøÁî®Èõ∂Ê†∑Êú¨ÂíåÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊñπÊ≥ïËØÑ‰º∞VLMÂú®MMKP‰ªªÂä°‰∏äÁöÑ‰∏ãÈôêÊÄßËÉΩ„ÄÇ2) ÈááÁî®Fine-tune-CoTÊñπÊ≥ïÔºåÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÁîüÊàêÁöÑÈ´òË¥®ÈáèCoTÊï∞ÊçÆÂæÆË∞ÉVLMÔºåÊèêÂçáÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇ3) ÂºïÂÖ•Âä®ÊÄÅCoTÁ≠ñÁï•ÔºåÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ëá™ÈÄÇÂ∫îÂú∞Ê≥®ÂÖ•CoTÊï∞ÊçÆÔºå‰ΩøÊ®°ÂûãÂú®Êé®ÁêÜÈò∂ÊÆµËÉΩÂ§üÁÅµÊ¥ªÂú∞Âà©Áî®Êé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂä®ÊÄÅCoTÁ≠ñÁï•„ÄÇ‰∏é‰º†ÁªüÁöÑCoTÊñπÊ≥ï‰∏çÂêåÔºåÂä®ÊÄÅCoTËÉΩÂ§üÊ†πÊçÆÊ®°ÂûãÁöÑËÆ≠ÁªÉÁä∂ÊÄÅÂíåËæìÂÖ•Êï∞ÊçÆÁöÑÁâπÁÇπÔºåËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥CoTÊï∞ÊçÆÁöÑ‰ΩøÁî®ÈáèÔºå‰ªéËÄåÈÅøÂÖç‚ÄúËøáÂ∫¶ÊÄùËÄÉ‚ÄùÈóÆÈ¢òÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂä®ÊÄÅCoTÁ≠ñÁï•ÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂ¶Ç‰ΩïÁ°ÆÂÆö‰ΩïÊó∂‰ª•ÂèäÂ¶Ç‰ΩïÊ≥®ÂÖ•CoTÊï∞ÊçÆ„ÄÇÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÊú™Áü•Ôºå‰ΩÜÊé®ÊµãÂèØËÉΩÊ∂âÂèäÂà∞ÁõëÊéßÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÊçüÂ§±ÂáΩÊï∞ÂèòÂåñ„ÄÅÊé®ÁêÜÁΩÆ‰ø°Â∫¶Á≠âÊåáÊ†áÔºåÂπ∂Ê†πÊçÆËøô‰∫õÊåáÊ†áÂä®ÊÄÅË∞ÉÊï¥CoTÊï∞ÊçÆÁöÑ‰ΩøÁî®ÊØî‰æã„ÄÇÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÁªÜËäÇÊú™Âú®ÊëòË¶Å‰∏≠ÊèêÂèäÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜÊâÄÊèêÂá∫ÁöÑÂä®ÊÄÅCoTÁ≠ñÁï•Âú®Â§ö‰∏™MMKPÊï∞ÊçÆÈõÜ‰∏äÁöÑÊúâÊïàÊÄß„ÄÇËôΩÁÑ∂ÊëòË¶Å‰∏≠Ê≤°ÊúâÁªôÂá∫ÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Ôºå‰ΩÜÂº∫Ë∞É‰∫ÜÂÆûÈ™åÁªìÊûúËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑ‰ºòË∂äÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÈúÄË¶ÅÂú®ËÆ∫ÊñáÊ≠£Êñá‰∏≠Êü•Êâæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂõæÂÉè/ËßÜÈ¢ëÂÜÖÂÆπÁêÜËß£„ÄÅÊô∫ËÉΩÈóÆÁ≠î„ÄÅÂïÜÂìÅÊé®ËçêÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÁªìÂêàËßÜËßâ‰ø°ÊÅØÂíåÊñáÊú¨‰ø°ÊÅØÔºåÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞ÊèêÂèñÂÖ≥ÈîÆÁü≠ËØ≠Ôºå‰ªéËÄåÊèêÂçáÁõ∏ÂÖ≥Â∫îÁî®ÁöÑÊÄßËÉΩÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇ‰æãÂ¶ÇÔºåÂú®ÁîµÂïÜÈ¢ÜÂüüÔºåÂèØ‰ª•Ê†πÊçÆÂïÜÂìÅÂõæÁâáÂíåÊèèËø∞Ëá™Âä®ÁîüÊàêÂÖ≥ÈîÆÊ†áÁ≠æÔºåÊñπ‰æøÁî®Êà∑ÊêúÁ¥¢ÂíåÊµèËßà„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only methods by incorporating multiple modalities of input information to produce a set of conclusive phrases. Traditional multi-modal approaches have been proven to have significant limitations in handling the challenging absence and unseen scenarios. Additionally, we identify shortcomings in existing benchmarks that overestimate model capability due to significant overlap in training tests. In this work, we propose leveraging vision-language models (VLMs) for the MMKP task. Firstly, we use two widely-used strategies, e.g., zero-shot and supervised fine-tuning (SFT) to assess the lower bound performance of VLMs. Next, to improve the complex reasoning capabilities of VLMs, we adopt Fine-tune-CoT, which leverages high-quality CoT reasoning data generated by a teacher model to finetune smaller models. Finally, to address the "overthinking" phenomenon, we propose a dynamic CoT strategy which adaptively injects CoT data during training, allowing the model to flexibly leverage its reasoning capabilities during the inference stage. We evaluate the proposed strategies on various datasets and the experimental results demonstrate the effectiveness of the proposed approaches. The code is available at https://github.com/bytedance/DynamicCoT.

