---
layout: default
title: Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models
---

# Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09358v1</a>
  <a href="https://arxiv.org/pdf/2510.09358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09358v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09358v1', 'Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qihang Ma, Shengyu Li, Jie Tang, Dingkang Yang, Shaodong Chen, Yingyi Zhang, Chao Feng, Jiao Ran

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

**å¤‡æ³¨**: EMNLP2025. Code is avaible at https://github.com/bytedance/DynamicCoT

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/bytedance/DynamicCoT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨æ€é“¾å¼æ€è€ƒæ–¹æ³•ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€å…³é”®çŸ­è¯­é¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å…³é”®çŸ­è¯­é¢„æµ‹` `è§†è§‰-è¯­è¨€æ¨¡å‹` `é“¾å¼æ€è€ƒ` `åŠ¨æ€é“¾å¼æ€è€ƒ` `æ¨¡å‹å¾®è°ƒ` `é›¶æ ·æœ¬å­¦ä¹ ` `ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MMKPæ–¹æ³•åœ¨å¤„ç†æ•°æ®ç¼ºå¤±å’Œæœªè§åœºæ™¯æ—¶å­˜åœ¨ä¸è¶³ï¼Œä¸”ç°æœ‰benchmarkå­˜åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†é‡å é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹èƒ½åŠ›è¢«é«˜ä¼°ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)è§£å†³MMKPä»»åŠ¡ï¼Œå¹¶å¼•å…¥åŠ¨æ€é“¾å¼æ€è€ƒ(Dynamic CoT)ç­–ç•¥ï¼Œæå‡VLMçš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Dynamic CoTç­–ç•¥åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæœ‰æ•ˆæå‡äº†MMKPçš„æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å…³é”®çŸ­è¯­é¢„æµ‹(MMKP)æ—¨åœ¨é€šè¿‡æ•´åˆå¤šç§æ¨¡æ€çš„è¾“å…¥ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ç»„ç»“è®ºæ€§çš„çŸ­è¯­ï¼Œä»è€Œè¶…è¶Šä»…ä¾èµ–æ–‡æœ¬çš„æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å¤šæ¨¡æ€æ–¹æ³•åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¼ºå¤±å’Œæœªè§åœºæ™¯æ—¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰åŸºå‡†æµ‹è¯•ç”±äºè®­ç»ƒæµ‹è¯•ä¸­å­˜åœ¨æ˜¾è‘—é‡å ï¼Œé«˜ä¼°äº†æ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)æ¥è§£å†³MMKPä»»åŠ¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ç§å¹¿æ³›ä½¿ç”¨çš„ç­–ç•¥ï¼Œä¾‹å¦‚é›¶æ ·æœ¬å’Œç›‘ç£å¾®è°ƒ(SFT)æ¥è¯„ä¼°VLMçš„ä¸‹é™æ€§èƒ½ã€‚æ¥ä¸‹æ¥ï¼Œä¸ºäº†æé«˜VLMçš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨Fine-tune-CoTï¼Œå®ƒåˆ©ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„é«˜è´¨é‡CoTæ¨ç†æ•°æ®æ¥å¾®è°ƒè¾ƒå°çš„æ¨¡å‹ã€‚æœ€åï¼Œä¸ºäº†è§£å†³â€œè¿‡åº¦æ€è€ƒâ€ç°è±¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€CoTç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨è®­ç»ƒæœŸé—´è‡ªé€‚åº”åœ°æ³¨å…¥CoTæ•°æ®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†é˜¶æ®µçµæ´»åœ°åˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å„ç§æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„ç­–ç•¥ï¼Œå®éªŒç»“æœè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨https://github.com/bytedance/DynamicCoTè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å…³é”®çŸ­è¯­é¢„æµ‹ï¼ˆMMKPï¼‰ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ•°æ®ç¼ºå¤±ã€æœªè§åœºæ™¯ä»¥åŠè®­ç»ƒæµ‹è¯•æ•°æ®é‡å ç­‰é—®é¢˜ä¸Šçš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œæœ‰æ•ˆæ¨ç†ï¼Œå¯¼è‡´é¢„æµ‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œå¹¶ç»“åˆé“¾å¼æ€è€ƒï¼ˆCoTï¼‰æ–¹æ³•æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†é¿å…CoTå¸¦æ¥çš„â€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜ï¼Œæå‡ºäº†åŠ¨æ€CoTç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨é›¶æ ·æœ¬å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•è¯„ä¼°VLMåœ¨MMKPä»»åŠ¡ä¸Šçš„ä¸‹é™æ€§èƒ½ã€‚2) é‡‡ç”¨Fine-tune-CoTæ–¹æ³•ï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„é«˜è´¨é‡CoTæ•°æ®å¾®è°ƒVLMï¼Œæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚3) å¼•å…¥åŠ¨æ€CoTç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°æ³¨å…¥CoTæ•°æ®ï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µèƒ½å¤Ÿçµæ´»åœ°åˆ©ç”¨æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŠ¨æ€CoTç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„CoTæ–¹æ³•ä¸åŒï¼ŒåŠ¨æ€CoTèƒ½å¤Ÿæ ¹æ®æ¨¡å‹çš„è®­ç»ƒçŠ¶æ€å’Œè¾“å…¥æ•°æ®çš„ç‰¹ç‚¹ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´CoTæ•°æ®çš„ä½¿ç”¨é‡ï¼Œä»è€Œé¿å…â€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€CoTç­–ç•¥çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•ç¡®å®šä½•æ—¶ä»¥åŠå¦‚ä½•æ³¨å…¥CoTæ•°æ®ã€‚å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œä½†æ¨æµ‹å¯èƒ½æ¶‰åŠåˆ°ç›‘æ§æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°å˜åŒ–ã€æ¨ç†ç½®ä¿¡åº¦ç­‰æŒ‡æ ‡ï¼Œå¹¶æ ¹æ®è¿™äº›æŒ‡æ ‡åŠ¨æ€è°ƒæ•´CoTæ•°æ®çš„ä½¿ç”¨æ¯”ä¾‹ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­æåŠï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„åŠ¨æ€CoTç­–ç•¥åœ¨å¤šä¸ªMMKPæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è™½ç„¶æ‘˜è¦ä¸­æ²¡æœ‰ç»™å‡ºå…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦ï¼Œä½†å¼ºè°ƒäº†å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®éœ€è¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå›¾åƒ/è§†é¢‘å†…å®¹ç†è§£ã€æ™ºèƒ½é—®ç­”ã€å•†å“æ¨èç­‰é¢†åŸŸã€‚é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°æå–å…³é”®çŸ­è¯­ï¼Œä»è€Œæå‡ç›¸å…³åº”ç”¨çš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚ä¾‹å¦‚ï¼Œåœ¨ç”µå•†é¢†åŸŸï¼Œå¯ä»¥æ ¹æ®å•†å“å›¾ç‰‡å’Œæè¿°è‡ªåŠ¨ç”Ÿæˆå…³é”®æ ‡ç­¾ï¼Œæ–¹ä¾¿ç”¨æˆ·æœç´¢å’Œæµè§ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only methods by incorporating multiple modalities of input information to produce a set of conclusive phrases. Traditional multi-modal approaches have been proven to have significant limitations in handling the challenging absence and unseen scenarios. Additionally, we identify shortcomings in existing benchmarks that overestimate model capability due to significant overlap in training tests. In this work, we propose leveraging vision-language models (VLMs) for the MMKP task. Firstly, we use two widely-used strategies, e.g., zero-shot and supervised fine-tuning (SFT) to assess the lower bound performance of VLMs. Next, to improve the complex reasoning capabilities of VLMs, we adopt Fine-tune-CoT, which leverages high-quality CoT reasoning data generated by a teacher model to finetune smaller models. Finally, to address the "overthinking" phenomenon, we propose a dynamic CoT strategy which adaptively injects CoT data during training, allowing the model to flexibly leverage its reasoning capabilities during the inference stage. We evaluate the proposed strategies on various datasets and the experimental results demonstrate the effectiveness of the proposed approaches. The code is available at https://github.com/bytedance/DynamicCoT.

