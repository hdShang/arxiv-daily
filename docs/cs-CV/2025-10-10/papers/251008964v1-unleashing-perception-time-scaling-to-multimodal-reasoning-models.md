---
layout: default
title: Unleashing Perception-Time Scaling to Multimodal Reasoning Models
---

# Unleashing Perception-Time Scaling to Multimodal Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08964" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08964v1</a>
  <a href="https://arxiv.org/pdf/2510.08964.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08964v1" onclick="toggleFavorite(this, '2510.08964v1', 'Unleashing Perception-Time Scaling to Multimodal Reasoning Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifan Li, Zhenghao Chen, Ziheng Wu, Kun Zhou, Ruipu Luo, Can Zhang, Zhentao He, Yufei Zhan, Wayne Xin Zhao, Minghui Qiu

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ„ŸçŸ¥æ—¶é—´å°ºåº¦è°ƒæ•´(PTS)ï¼Œæå‡å¤šæ¨¡æ€æ¨ç†æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰æ„ŸçŸ¥` `æ„ŸçŸ¥æ—¶é—´å°ºåº¦è°ƒæ•´` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LVLMsåœ¨è§†è§‰æ„ŸçŸ¥ä¸Šé‡‡ç”¨å¿«é€Ÿæ„ŸçŸ¥èŒƒå¼ï¼Œç¼ºä¹å¯¹åº•å±‚æ„ŸçŸ¥è¿‡ç¨‹çš„å»ºæ¨¡ï¼Œå¯¼è‡´ä¼°è®¡ç²¾åº¦ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºæ„ŸçŸ¥æ—¶é—´å°ºåº¦è°ƒæ•´(PTS)èŒƒå¼ï¼Œé€šè¿‡tokenä¸°å¯Œçš„æ„ŸçŸ¥å’Œé—®é¢˜åˆ†è§£ï¼Œä½¿æ„ŸçŸ¥ä¸æ¨ç†æ—¶å°ºåº¦è°ƒæ•´å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPTSæ˜¾è‘—æé«˜äº†æ„ŸçŸ¥ç²¾åº¦ï¼Œåœ¨DisTANCEä¸Šä»8.0%æå‡åˆ°64.7%ï¼Œå¹¶æ³›åŒ–åˆ°åŸŸå¤–ä»»åŠ¡å’ŒçœŸå®ä¸–ç•Œæ„ŸçŸ¥ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ¨ç†æ—¶å°ºåº¦è°ƒæ•´æŠ€æœ¯æ˜¾è‘—æå‡äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)çš„æ¨ç†èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œç±»ä¼¼ç­–ç•¥ä¹Ÿè¢«åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ï¼Œä½†å…¶å¯¹è§†è§‰æ„ŸçŸ¥çš„å½±å“å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä»¥æ„ŸçŸ¥ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•DisTANCEï¼Œç”¨äºè¯„ä¼°è§†è§‰ä¼°è®¡ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLVLMsçš„ä¼°è®¡ç²¾åº¦æœ‰é™ï¼Œä¸”æ¨ç†æ—¶å°ºåº¦è°ƒæ•´å¸¦æ¥çš„å¢ç›Šç”šå¾®ã€‚ä½œè€…è®¤ä¸ºè¿™æ˜¯ç”±äºå½“å‰LVLMsçš„å¿«é€Ÿæ„ŸçŸ¥èŒƒå¼ï¼Œå³å°†è§†è§‰ç†è§£è§†ä¸ºä¸€æ¬¡æ€§è¾“å‡ºï¼Œè€Œæ²¡æœ‰å¯¹åº•å±‚æ„ŸçŸ¥è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼â€”â€”æ„ŸçŸ¥æ—¶é—´å°ºåº¦è°ƒæ•´(PTS)ï¼Œé¼“åŠ±tokenä¸°å¯Œçš„æ„ŸçŸ¥ï¼Œå¹¶å°†å¤æ‚çš„æ„ŸçŸ¥é—®é¢˜åˆ†è§£ä¸ºä¸­é—´å¯å¤„ç†çš„å­é—®é¢˜ï¼Œä»è€Œä½¿æ„ŸçŸ¥èƒ½å¤Ÿä¸æ¨ç†æ—¶å°ºåº¦è°ƒæ•´å¯¹é½å¹¶ä»ä¸­å—ç›Šã€‚ç»“åˆå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼ŒPTSæ˜¾è‘—æé«˜äº†æ„ŸçŸ¥ç²¾åº¦ï¼Œåœ¨DisTANCEä¸Šçš„é«˜ç²¾åº¦æ€§èƒ½ä»8.0%æé«˜åˆ°64.7%ï¼Œå¹¶å¾ˆå¥½åœ°æ³›åŒ–åˆ°åŸŸå¤–ä»»åŠ¡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿PTSæ•°æ®æ˜¯çº¯ç²¹åˆæˆçš„ï¼Œå°†å®ƒä»¬ä¸æ•°å­¦æ¨ç†æ•°æ®ç›¸ç»“åˆï¼Œä¹Ÿèƒ½åœ¨æ¨ç†å’ŒçœŸå®ä¸–ç•Œæ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸­è·å¾—ä¸€è‡´çš„æ”¶ç›Šã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒPTSå¼•å…¥äº†æ›´å¤šä¸æ„ŸçŸ¥ç›¸å…³çš„tokenï¼Œå¹¶å¢åŠ äº†æ¨¡å‹å¯¹å›¾åƒtokençš„å…³æ³¨ã€‚ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­ç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰LVLMsé€šå¸¸é‡‡ç”¨â€œå¿«é€Ÿæ„ŸçŸ¥â€èŒƒå¼ï¼Œå³ç›´æ¥å°†å›¾åƒä½œä¸ºè¾“å…¥ï¼Œä¸€æ¬¡æ€§è¾“å‡ºç»“æœï¼Œç¼ºä¹å¯¹åº•å±‚æ„ŸçŸ¥è¿‡ç¨‹çš„å»ºæ¨¡å’Œè¿­ä»£ä¼˜åŒ–ã€‚è¿™ç§æ–¹å¼é™åˆ¶äº†æ¨¡å‹åˆ©ç”¨æ¨ç†æ—¶å°ºåº¦è°ƒæ•´æŠ€æœ¯æå‡æ„ŸçŸ¥èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥â€œæ„ŸçŸ¥æ—¶é—´å°ºåº¦è°ƒæ•´â€(Perception-Time Scaling, PTS) èŒƒå¼ã€‚PTSé¼“åŠ±æ¨¡å‹è¿›è¡Œtokenä¸°å¯Œçš„æ„ŸçŸ¥ï¼Œå°†å¤æ‚çš„æ„ŸçŸ¥é—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—ä¸­é—´å¯å¤„ç†çš„å­é—®é¢˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥é€æ­¥ç»†åŒ–å¯¹å›¾åƒçš„ç†è§£ï¼Œå¹¶æ›´å¥½åœ°åˆ©ç”¨æ¨ç†æ—¶å°ºåº¦è°ƒæ•´æŠ€æœ¯æ¥æå‡æ„ŸçŸ¥ç²¾åº¦ã€‚PTSçš„æ ¸å¿ƒåœ¨äºå°†æ„ŸçŸ¥è¿‡ç¨‹ä»â€œä¸€æ­¥åˆ°ä½â€è½¬å˜ä¸ºä¸€ä¸ªè¿­ä»£ä¼˜åŒ–çš„è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPTSçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **é—®é¢˜åˆ†è§£**ï¼šå°†å¤æ‚çš„è§†è§‰æ„ŸçŸ¥ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•çš„å­é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯¹äºç›®æ ‡å®šä½ä»»åŠ¡ï¼Œå¯ä»¥åˆ†è§£ä¸ºç›®æ ‡æ£€æµ‹ã€åŒºåŸŸåˆ†å‰²ã€åæ ‡ä¼°è®¡ç­‰å­é—®é¢˜ã€‚2) **Tokenä¸°å¯Œçš„æ„ŸçŸ¥**ï¼šé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´å¤šçš„ä¸æ„ŸçŸ¥ç›¸å…³çš„tokenï¼Œä¾‹å¦‚ç›®æ ‡æè¿°ã€å±æ€§æè¿°ã€å…³ç³»æè¿°ç­‰ã€‚è¿™äº›tokenå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å›¾åƒå†…å®¹ã€‚3) **è¿­ä»£ä¼˜åŒ–**ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œè®­ç»ƒæ¨¡å‹é€æ­¥ä¼˜åŒ–æ„ŸçŸ¥è¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°è§£å†³å­é—®é¢˜ï¼Œå¹¶æœ€ç»ˆå®Œæˆæ•´ä¸ªæ„ŸçŸ¥ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†æ„ŸçŸ¥æ—¶é—´å°ºåº¦è°ƒæ•´(PTS)èŒƒå¼ã€‚ä¸ç°æœ‰çš„å¿«é€Ÿæ„ŸçŸ¥èŒƒå¼ç›¸æ¯”ï¼ŒPTSæ›´åŠ æ³¨é‡å¯¹åº•å±‚æ„ŸçŸ¥è¿‡ç¨‹çš„å»ºæ¨¡å’Œè¿­ä»£ä¼˜åŒ–ã€‚é€šè¿‡é—®é¢˜åˆ†è§£å’Œtokenä¸°å¯Œçš„æ„ŸçŸ¥ï¼ŒPTSä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æ¨ç†æ—¶å°ºåº¦è°ƒæ•´æŠ€æœ¯æ¥æå‡æ„ŸçŸ¥ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªä»¥æ„ŸçŸ¥ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•DisTANCEï¼Œç”¨äºè¯„ä¼°LVLMsåœ¨è§†è§‰ä¼°è®¡ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸Šï¼Œè®ºæ–‡ä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ æŠ€æœ¯æ¥è®­ç»ƒPTSæ¨¡å‹ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦èƒ½å¤Ÿå¼•å¯¼æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æ„ŸçŸ¥tokenï¼Œå¹¶é€æ­¥è§£å†³å­é—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–PTSæ¨¡å‹çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨Transformeræ¨¡å‹æ¥å¤„ç†å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å…³æ³¨å›¾åƒä¸­çš„å…³é”®åŒºåŸŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPTSæ˜¾è‘—æé«˜äº†æ„ŸçŸ¥ç²¾åº¦ï¼Œåœ¨DisTANCEåŸºå‡†æµ‹è¯•ä¸­ï¼Œé«˜ç²¾åº¦æ€§èƒ½ä»8.0%æå‡åˆ°64.7%ã€‚æ­¤å¤–ï¼ŒPTSè¿˜è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”ç”¨äºåŸŸå¤–ä»»åŠ¡å’ŒçœŸå®ä¸–ç•Œæ„ŸçŸ¥ä»»åŠ¡ã€‚æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿ä½¿ç”¨çº¯åˆæˆæ•°æ®è®­ç»ƒPTSæ¨¡å‹ï¼Œä¹Ÿèƒ½åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è·å¾—ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰æ„ŸçŸ¥ç²¾åº¦ï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œå†³ç­–èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤šæ¨¡æ€å’Œæ›´å¤æ‚çš„æ„ŸçŸ¥ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in inference-time scaling, particularly those leveraging reinforcement learning with verifiable rewards, have substantially enhanced the reasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by this success, similar strategies have been applied to multimodal reasoning, yet their impact on visual perception remains unclear. To investigate this gap, we introduce DisTANCE, a perception-centric benchmark for visual estimation tasks. Evaluation results show that LVLMs exhibit limited estimation precision, and inference-time scaling offers only marginal gains. We attribute this to the fast perception paradigm of current LVLMs, where visual understanding is treated as a one-shot output without modeling the underlying perceptual process. To address this, we propose Perception-Time Scaling (PTS), a novel paradigm that encourages token-rich perception and decomposes complex perception problems into intermediate tractable sub-problems, thereby enabling perception to align with and benefit from inference-time scaling. Combined with reinforcement learning techniques, PTS significantly improves perception accuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%, and generalizes well to out-of-domain tasks. Surprisingly, even though PTS data are purely synthetic, combining them with math reasoning data yields consistent gains in both reasoning and real-world perception benchmarks. Further analysis reveals that PTS introduces more perception-related tokens and increases the model's attention to image tokens. Our code and data will be publicly released.

