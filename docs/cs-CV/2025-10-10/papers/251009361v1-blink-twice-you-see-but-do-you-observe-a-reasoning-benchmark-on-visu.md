---
layout: default
title: "BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception"
---

# BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09361" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09361v1</a>
  <a href="https://arxiv.org/pdf/2510.09361.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09361v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09361v1', 'BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junyan Ye, Dongzhi Jiang, Jun He, Baichuan Zhou, Zilong Huang, Zhiyuan Yan, Hongsheng Li, Conghui He, Weijia Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

**å¤‡æ³¨**: Accepted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Track on Datasets and Benchmarks

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/PicoTrex/BLINK-Twice)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BLINK-Twiceï¼šæå‡ºè§†è§‰æ„ŸçŸ¥æ¨ç†åŸºå‡†ï¼Œå¼ºè°ƒç»†ç²’åº¦è§‚å¯Ÿä¸åˆ†æï¼ŒæŒ‘æˆ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æ„ŸçŸ¥` `æ¨ç†åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†åŸºå‡†ä¾§é‡äºè¯­è¨€æ¨ç†ï¼Œå¿½ç•¥äº†è§†è§‰æ„ŸçŸ¥çš„ç»†ç²’åº¦æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç†è§£æ·±åº¦ã€‚
2. BLINK-TwiceåŸºå‡†é€šè¿‡è§†è§‰æŒ‘æˆ˜ã€å¯¹æŠ—å›¾åƒå’Œæ¨ç†é“¾ï¼Œè¿«ä½¿æ¨¡å‹ä»è§†è§‰å†…å®¹æ¨ç†ï¼Œå¹¶æä¾›ç»†ç²’åº¦çš„æ¨ç†è¿‡ç¨‹è¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰MLLMåœ¨BLINK-Twiceä¸Šè¡¨ç°ä¸ä½³ï¼Œé‡å¤è§‚å¯Ÿå›¾åƒå’Œä¸»åŠ¨è§†è§‰äº¤äº’èƒ½æå‡æ€§èƒ½ï¼Œæ­ç¤ºäº†è§†è§‰æ¨ç†çš„æ–°æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨ç†åŸºå‡†ä¸»è¦è¯„ä¼°åŸºäºè¯­è¨€çš„æ¨ç†ï¼Œé€šå¸¸å°†è§†è§‰è¾“å…¥è§†ä¸ºå¯æ›¿æ¢çš„ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†BLINK-Twiceï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨ç†åŸºå‡†ï¼Œå®ƒåŸºäºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ„ŸçŸ¥ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»»åŠ¡ä¸éœ€è¦ä¾èµ–å¤–éƒ¨çŸ¥è¯†ï¼Œè€Œæ˜¯è¦æ±‚æ¨¡å‹ä»…ä»è§†è§‰å†…å®¹è¿›è¡Œæ¨ç†ï¼Œä»è€Œå°†é‡ç‚¹ä»åŸºäºè¯­è¨€çš„æ¨ç†è½¬ç§»åˆ°åŸºäºå›¾åƒçš„æ¨ç†ã€‚ä¸ä¹‹å‰çš„æ„ŸçŸ¥åŸºå‡†ç›¸æ¯”ï¼Œå®ƒè¶…è¶Šäº†æµ…å±‚æ„ŸçŸ¥ï¼ˆâ€œçœ‹â€ï¼‰ï¼Œéœ€è¦ç»†ç²’åº¦çš„è§‚å¯Ÿå’Œåˆ†ææ¨ç†ï¼ˆâ€œè§‚å¯Ÿâ€ï¼‰ã€‚BLINK-Twiceé›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸ƒç§ç”¨äºæµ‹è¯•è§†è§‰æ¨ç†çš„è§†è§‰æŒ‘æˆ˜ç±»å‹ï¼Œå¼ºåˆ¶ä¾èµ–è§†è§‰å†…å®¹çš„è‡ªç„¶å¯¹æŠ—å›¾åƒå¯¹ï¼Œä»¥åŠç”¨äºå¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ï¼ˆè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰çš„å¸¦æ³¨é‡Šçš„æ¨ç†é“¾ã€‚æˆ‘ä»¬è¯„ä¼°äº†20ä¸ªé¢†å…ˆçš„MLLMï¼ŒåŒ…æ‹¬12ä¸ªåŸºç¡€æ¨¡å‹å’Œ8ä¸ªæ¨ç†å¢å¼ºæ¨¡å‹ã€‚BLINK-Twiceå¯¹å½“å‰æ¨¡å‹æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶è¯­è¨€ç©ºé—´ä¸­ç°æœ‰çš„æ¨ç†ç­–ç•¥ï¼ˆå¦‚æ€ç»´é“¾æˆ–è‡ªæˆ‘æ‰¹è¯„ï¼‰å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šå¯¼è‡´ä¸ç¨³å®šå’Œå†—ä½™çš„æ¨ç†ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé‡å¤çš„å›¾åƒè§‚å¯Ÿå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸»åŠ¨çš„è§†è§‰äº¤äº’ï¼ˆå¦‚o3æ¨¡å‹æ‰€å±•ç¤ºçš„ï¼‰çªå‡ºäº†å¯¹è§†è§‰æ¨ç†æ–°èŒƒå¼çš„éœ€æ±‚ã€‚è¯¥æ•°æ®é›†å¯åœ¨https://github.com/PicoTrex/BLINK-Twiceå…¬å¼€è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§éƒ¨åˆ†å¤šæ¨¡æ€å­¦ä¹ benchmarkä¸»è¦å…³æ³¨è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œå°†è§†è§‰ä¿¡æ¯ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼Œç¼ºä¹å¯¹æ¨¡å‹è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„æ·±å…¥è¯„ä¼°ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åŒºåˆ†æ¨¡å‹æ˜¯çœŸæ­£ç†è§£äº†å›¾åƒå†…å®¹ï¼Œè¿˜æ˜¯ä»…ä»…ä¾èµ–äºè¯­è¨€çŸ¥è¯†è¿›è¡Œæ¨æ–­ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªæ›´ä¾§é‡äºè§†è§‰çš„æ¨ç†åŸºå‡†ï¼Œæ¥è¯„ä¼°æ¨¡å‹ä»è§†è§‰ä¿¡æ¯ä¸­è¿›è¡Œç»†ç²’åº¦è§‚å¯Ÿå’Œåˆ†ææ¨ç†çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBLINK-Twiceçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨ç†åŸºå‡†ï¼Œé€šè¿‡è®¾è®¡å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰ä»»åŠ¡ã€å¯¹æŠ—æ€§å›¾åƒå¯¹å’Œæ¨ç†é“¾ï¼Œè¿«ä½¿æ¨¡å‹ä¾èµ–è§†è§‰å†…å®¹è¿›è¡Œæ¨ç†ï¼Œå¹¶æä¾›ç»†ç²’åº¦çš„æ¨ç†è¿‡ç¨‹è¯„ä¼°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBLINK-TwiceåŸºå‡†åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š
1. **è§†è§‰æŒ‘æˆ˜ç±»å‹**ï¼šåŒ…å«ä¸ƒç§ä¸åŒçš„è§†è§‰æŒ‘æˆ˜ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚
2. **è‡ªç„¶å¯¹æŠ—å›¾åƒå¯¹**ï¼šè¿™äº›å›¾åƒå¯¹æ—¨åœ¨å¼ºåˆ¶æ¨¡å‹ä¾èµ–è§†è§‰å†…å®¹ï¼Œè€Œä¸æ˜¯å¤–éƒ¨çŸ¥è¯†ã€‚
3. **å¸¦æ³¨é‡Šçš„æ¨ç†é“¾**ï¼šæä¾›ç»†ç²’åº¦çš„æ¨ç†è¿‡ç¨‹è¯„ä¼°ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šBLINK-Twiceçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨ç†è®¾è®¡ï¼Œå®ƒå¼ºè°ƒç»†ç²’åº¦çš„è§‚å¯Ÿå’Œåˆ†ææ¨ç†ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–è¯­è¨€ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†è¿˜å¼•å…¥äº†è‡ªç„¶å¯¹æŠ—å›¾åƒå¯¹å’Œå¸¦æ³¨é‡Šçš„æ¨ç†é“¾ï¼Œä»¥æä¾›æ›´å…¨é¢å’Œç»†è‡´çš„è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†æ„å»ºæ–¹é¢ï¼Œä½œè€…ç²¾å¿ƒè®¾è®¡äº†ä¸ƒç§è§†è§‰æŒ‘æˆ˜ç±»å‹ï¼ŒåŒ…æ‹¬è®¡æ•°ã€æ¯”è¾ƒã€ç©ºé—´å…³ç³»æ¨ç†ç­‰ã€‚å¯¹æŠ—å›¾åƒå¯¹çš„è®¾è®¡æ—¨åœ¨é€šè¿‡ç»†å¾®çš„è§†è§‰å·®å¼‚æ¥è¿·æƒ‘æ¨¡å‹ï¼Œè¿«ä½¿å…¶è¿›è¡Œæ›´æ·±å…¥çš„è§†è§‰åˆ†æã€‚æ¨ç†é“¾çš„æ ‡æ³¨åˆ™æä¾›äº†æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ä¸­é—´æ­¥éª¤ï¼Œæ–¹ä¾¿è¿›è¡Œæ›´ç»†ç²’åº¦çš„è¯„ä¼°å’Œåˆ†æã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºè¢«è¯„ä¼°çš„MLLMæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å¯¹20ä¸ªé¢†å…ˆMLLMçš„è¯„ä¼°è¡¨æ˜ï¼ŒBLINK-Twiceå¯¹ç°æœ‰æ¨¡å‹æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æ€ç»´é“¾ç­‰è¯­è¨€æ¨ç†ç­–ç•¥å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†æ•ˆæœä¸ç¨³å®šä¸”å­˜åœ¨å†—ä½™ã€‚é‡å¤å›¾åƒè§‚å¯Ÿèƒ½æå‡æ¨¡å‹æ€§èƒ½ï¼Œä¸»åŠ¨è§†è§‰äº¤äº’ï¼ˆå¦‚o3æ¨¡å‹ï¼‰è¡¨æ˜è§†è§‰æ¨ç†éœ€è¦æ–°èŒƒå¼ã€‚è¿™äº›ç»“æœçªå‡ºäº†å½“å‰æ¨¡å‹åœ¨è§†è§‰æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†å¯ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BLINK-TwiceåŸºå‡†çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥æå‡è¿™äº›åº”ç”¨åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œå¯é æ€§ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå¹¶ä¿ƒè¿›è§†è§‰æ¨ç†é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, Multimodal Large Language Models (MLLMs) have made rapid progress, particularly in enhancing their reasoning capabilities. However, existing reasoning benchmarks still primarily assess language-based reasoning, often treating visual input as replaceable context. To address this gap, we introduce BLINK-Twice, a vision-centric reasoning benchmark grounded in challenging perceptual tasks. Instead of relying on external knowledge, our tasks require models to reason from visual content alone, shifting the focus from language-based to image-grounded reasoning. Compared to prior perception benchmarks, it moves beyond shallow perception ("see") and requires fine-grained observation and analytical reasoning ("observe"). BLINK-Twice integrates three core components: seven types of visual challenges for testing visual reasoning, natural adversarial image pairs that enforce reliance on visual content, and annotated reasoning chains for fine-grained evaluation of the reasoning process rather than final answers alone. We evaluate 20 leading MLLMs, including 12 foundation models and 8 reasoning-enhanced models. BLINK-Twice poses a significant challenge to current models. While existing reasoning strategies in the language space-such as chain-of-thought or self-criticism can improve performance, they often result in unstable and redundant reasoning. We observe that repeated image observation improves performance across models, and active visual interaction, as demonstrated by models like o3, highlights the need for a new paradigm for vision reasoning. The dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice

