---
layout: default
title: HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images
---

# HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08978" target="_blank" class="toolbar-btn">arXiv: 2510.08978v1</a>
    <a href="https://arxiv.org/pdf/2510.08978.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08978v1" 
            onclick="toggleFavorite(this, '2510.08978v1', 'HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Zichuan Wang, Bo Peng, Songlin Yang, Zhenchen Tang, Jing Dong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHandEvalï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆå›¾åƒä¸­æ‰‹éƒ¨è´¨é‡ï¼Œæå‡AIGCåº”ç”¨æ•ˆæœã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰‹éƒ¨è´¨é‡è¯„ä¼°` `ç”Ÿæˆå›¾åƒ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `AIGCæ£€æµ‹` `æ–‡å›¾ç”Ÿæˆ` `æ‰‹éƒ¨å…³é”®ç‚¹` `HandPairæ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡å›¾ç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚å±€éƒ¨åŒºåŸŸï¼ˆç‰¹åˆ«æ˜¯æ‰‹éƒ¨ï¼‰çš„ç»†èŠ‚ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ‰‹éƒ¨ç»“æ„æ‰­æ›²ã€çº¹ç†ä¸çœŸå®ã€‚
2. è®ºæ–‡æå‡ºHandEvalæ¨¡å‹ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œæ‰‹éƒ¨å…³é”®ç‚¹å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°å¯¹æ‰‹éƒ¨è´¨é‡çš„ç²¾å‡†è¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHandEvalä¸äººç±»åˆ¤æ–­æ›´ä¸€è‡´ï¼Œé›†æˆåˆ°å›¾åƒç”Ÿæˆå’ŒAIGCæ£€æµ‹æµç¨‹åï¼Œèƒ½æ˜¾è‘—æé«˜æ‰‹éƒ¨çœŸå®æ„Ÿå’Œæ£€æµ‹ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†é¦–ä¸ªé’ˆå¯¹ç”Ÿæˆå›¾åƒä¸­æ‰‹éƒ¨åŒºåŸŸè´¨é‡è¯„ä¼°çš„ä»»åŠ¡ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¸°å¯Œçš„ä¸‹æ¸¸åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†HandPairæ•°æ®é›†ï¼ŒåŒ…å«48kä¸ªç”±é«˜è´¨é‡å’Œä½è´¨é‡æ‰‹éƒ¨å›¾åƒå¯¹ç»„æˆçš„æ•°æ®ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨å³å¯å®ç°ä½æˆæœ¬ã€é«˜æ•ˆçš„ç›‘ç£å­¦ä¹ ã€‚åŸºäºæ­¤ï¼Œå¼€å‘äº†HandEvalï¼Œä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„æ‰‹éƒ¨è´¨é‡è¯„ä¼°æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¼ºå¤§çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå¹¶ç»“åˆæ‰‹éƒ¨å…³é”®ç‚¹çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œè·å¾—å¯¹æ‰‹éƒ¨è´¨é‡çš„å¼ºå¤§æ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æ„å»ºäº†ä¸€ä¸ªäººå·¥æ ‡æ³¨çš„æµ‹è¯•é›†ï¼ŒåŒ…å«æ¥è‡ªå„ç§æœ€å…ˆè¿›ï¼ˆSOTAï¼‰T2Iæ¨¡å‹çš„æ‰‹éƒ¨å›¾åƒï¼Œä»¥éªŒè¯å…¶è´¨é‡è¯„ä¼°èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒHandEvalæ¯”ç°æœ‰çš„SOTAæ–¹æ³•æ›´ç¬¦åˆäººç±»åˆ¤æ–­ã€‚HandEvalè¢«é›†æˆåˆ°å›¾åƒç”Ÿæˆå’ŒAIGCæ£€æµ‹æµç¨‹ä¸­ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ‰‹éƒ¨çš„çœŸå®æ„Ÿå’Œæ£€æµ‹ç²¾åº¦ï¼Œè¯å®äº†å…¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æ™®éæœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ•°æ®é›†å°†ä¼šå¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰æ–‡å›¾ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆæ‰‹éƒ¨ç­‰å¤æ‚å±€éƒ¨åŒºåŸŸæ—¶ï¼Œç»†èŠ‚è´¨é‡è¾ƒå·®ï¼Œå®¹æ˜“å‡ºç°ç»“æ„æ‰­æ›²å’Œçº¹ç†ä¸çœŸå®çš„é—®é¢˜ã€‚ç°æœ‰çš„è´¨é‡è¯„ä¼°æ–¹æ³•é€šå¸¸å…³æ³¨æ•´ä½“å›¾åƒè´¨é‡ï¼Œå¿½ç•¥äº†å¯¹æ‰‹éƒ¨ç­‰ç‰¹å®šåŒºåŸŸçš„è¯„ä¼°ï¼Œç¼ºä¹é’ˆå¯¹æ€§ï¼Œæ— æ³•æœ‰æ•ˆæŒ‡å¯¼ç”Ÿæˆæ¨¡å‹çš„ä¼˜åŒ–å’ŒAIGCæ£€æµ‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç”Ÿæˆæ‰‹éƒ¨åŒºåŸŸçš„è´¨é‡è¯„ä¼°æ¨¡å‹HandEvalã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›å’Œæ‰‹éƒ¨å…³é”®ç‚¹çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åˆ¤æ–­ç”Ÿæˆæ‰‹éƒ¨çš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHandEvalçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1ï¼‰HandPairæ•°æ®é›†çš„æ„å»ºï¼Œç”¨äºè®­ç»ƒè´¨é‡è¯„ä¼°æ¨¡å‹ï¼›2ï¼‰HandEvalæ¨¡å‹çš„æ„å»ºï¼Œåˆ©ç”¨MLLMå’Œæ‰‹éƒ¨å…³é”®ç‚¹ä¿¡æ¯è¿›è¡Œè´¨é‡è¯„ä¼°ï¼›3ï¼‰äººå·¥æ ‡æ³¨çš„æµ‹è¯•é›†çš„æ„å»ºï¼Œç”¨äºéªŒè¯HandEvalçš„æ€§èƒ½ï¼›4ï¼‰å°†HandEvalé›†æˆåˆ°å›¾åƒç”Ÿæˆå’ŒAIGCæ£€æµ‹æµç¨‹ä¸­ï¼ŒéªŒè¯å…¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šHandEvalçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1ï¼‰é¦–æ¬¡æå‡ºäº†é’ˆå¯¹ç”Ÿæˆæ‰‹éƒ¨åŒºåŸŸçš„è´¨é‡è¯„ä¼°ä»»åŠ¡ï¼›2ï¼‰æ„å»ºäº†HandPairæ•°æ®é›†ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨å³å¯å®ç°ä½æˆæœ¬ã€é«˜æ•ˆçš„ç›‘ç£å­¦ä¹ ï¼›3ï¼‰å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œæ‰‹éƒ¨å…³é”®ç‚¹ä¿¡æ¯ç›¸ç»“åˆï¼Œæé«˜äº†æ‰‹éƒ¨è´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šHandEvalçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰HandPairæ•°æ®é›†çš„æ„å»ºæ–¹å¼ï¼Œé€šè¿‡é«˜è´¨é‡å’Œä½è´¨é‡æ‰‹éƒ¨å›¾åƒå¯¹æ¥æä¾›ç›‘ç£ä¿¡å·ï¼›2ï¼‰MLLMçš„é€‰æ‹©å’Œä½¿ç”¨ï¼Œå¦‚ä½•åˆ©ç”¨MLLMçš„è§†è§‰ç†è§£èƒ½åŠ›æ¥æå–æ‰‹éƒ¨ç‰¹å¾ï¼›3ï¼‰æ‰‹éƒ¨å…³é”®ç‚¹ä¿¡æ¯çš„èåˆæ–¹å¼ï¼Œå¦‚ä½•å°†å…³é”®ç‚¹ä¿¡æ¯èå…¥åˆ°è´¨é‡è¯„ä¼°è¿‡ç¨‹ä¸­ï¼›4ï¼‰æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œå¦‚ä½•è®­ç»ƒHandEvalæ¨¡å‹ä½¿å…¶èƒ½å¤Ÿå‡†ç¡®åœ°åˆ¤æ–­æ‰‹éƒ¨è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHandEvalåœ¨æ‰‹éƒ¨è´¨é‡è¯„ä¼°æ–¹é¢ä¸äººç±»åˆ¤æ–­çš„å¯¹é½ç¨‹åº¦ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ã€‚å°†HandEvalé›†æˆåˆ°å›¾åƒç”Ÿæˆæµç¨‹ä¸­ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆæ‰‹éƒ¨çš„çœŸå®æ„Ÿã€‚åœ¨AIGCæ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒHandEvalçš„åŠ å…¥ä¹Ÿæ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œä¸äººç±»åˆ¤æ–­çš„å¯¹é½ç¨‹åº¦æå‡ç™¾åˆ†æ¯”ï¼ŒAIGCæ£€æµ‹ç²¾åº¦æå‡ç™¾åˆ†æ¯”ï¼‰éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºAIGCé¢†åŸŸï¼Œä¾‹å¦‚ï¼šæå‡æ–‡å›¾ç”Ÿæˆæ¨¡å‹ç”Ÿæˆæ‰‹éƒ¨çš„çœŸå®æ„Ÿï¼Œä¼˜åŒ–äººåƒç”Ÿæˆè´¨é‡ï¼›æé«˜AIGCæ£€æµ‹çš„å‡†ç¡®ç‡ï¼Œæœ‰æ•ˆè¯†åˆ«AIç”Ÿæˆå†…å®¹ï¼›è¾…åŠ©æ‰‹éƒ¨ç›¸å…³çš„åŒ»ç–—è¯Šæ–­ï¼Œä¾‹å¦‚è¯„ä¼°æ‰‹éƒ¨ç•¸å½¢ç¨‹åº¦ï¼›ä»¥åŠåœ¨è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®åº”ç”¨ä¸­ï¼Œæå‡æ‰‹éƒ¨äº¤äº’çš„çœŸå®æ„Ÿå’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although recent text-to-image (T2I) models have significantly improved the overall visual quality of generated images, they still struggle in the generation of accurate details in complex local regions, especially human hands. Generated hands often exhibit structural distortions and unrealistic textures, which can be very noticeable even when the rest of the body is well-generated. However, the quality assessment of hand regions remains largely neglected, limiting downstream task performance like human-centric generation quality optimization and AIGC detection. To address this, we propose the first quality assessment task targeting generated hand regions and showcase its abundant downstream applications. We first introduce the HandPair dataset for training hand quality assessment models. It consists of 48k images formed by high- and low-quality hand pairs, enabling low-cost, efficient supervision without manual annotation. Based on it, we develop HandEval, a carefully designed hand-specific quality assessment model. It leverages the powerful visual understanding capability of Multimodal Large Language Model (MLLM) and incorporates prior knowledge of hand keypoints, gaining strong perception of hand quality. We further construct a human-annotated test set with hand images from various state-of-the-art (SOTA) T2I models to validate its quality evaluation capability. Results show that HandEval aligns better with human judgments than existing SOTA methods. Furthermore, we integrate HandEval into image generation and AIGC detection pipelines, prominently enhancing generated hand realism and detection accuracy, respectively, confirming its universal effectiveness in downstream applications. Code and dataset will be available.

