---
layout: default
title: Video Reasoning without Training
---

# Video Reasoning without Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17045" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17045v1</a>
  <a href="https://arxiv.org/pdf/2510.17045.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17045v1" onclick="toggleFavorite(this, '2510.17045v1', 'Video Reasoning without Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Deepak Sridhar, Kartikeya Bhardwaj, Jeya Pradha Jeyaraj, Nuno Vasconcelos, Ankita Nayak, Harris Teague

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºV-Reasonï¼Œæ— éœ€è®­ç»ƒå³å¯æå‡å¤§æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ¨ç†` `å¤§æ¨¡å‹` `æ— è®­ç»ƒ` `ç†µä¼˜åŒ–` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ¨ç†LMMsä¾èµ–å¼ºåŒ–å­¦ä¹ å’Œæ€ç»´é“¾ï¼Œè®¡ç®—å¼€é”€å¤§ï¼Œä¸”æ¨ç†è¿‡ç¨‹æ§åˆ¶æœºåˆ¶æœ‰é™ã€‚
2. V-Reasoné€šè¿‡ç†µä¿¡å·æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ï¼Œä¼˜åŒ–æ¨¡å‹çš„æ¢ç´¢å’Œåˆ©ç”¨è¡Œä¸ºï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒV-Reasonåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨ç†ç²¾åº¦ï¼Œå¹¶å¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘æ¨ç†æ–¹æ³•V-Reasonï¼Œæ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†é¢‘æ¨ç†ä¸­ä¾èµ–æ˜‚è´µçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå†—é•¿çš„æ€ç»´é“¾ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­è®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚é€šè¿‡å°†æ¨¡å‹è¾“å‡ºçš„ç†µä½œä¸ºä¿¡å·ï¼Œå‘ç°é«˜è´¨é‡çš„æ¨¡å‹ä¼šç»å†ä¸€ç³»åˆ—å¾®æ¢ç´¢å’Œå¾®åˆ©ç”¨ï¼Œä»è€Œä¿æŒæ¨ç†è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œæ›´å‡†ç¡®çš„æ¨¡å‹åœ¨â€œæ€è€ƒâ€è¿‡ç¨‹ç»“æŸåï¼Œé€šè¿‡æœ€ç»ˆçš„åˆ©ç”¨é˜¶æ®µæ˜¾è‘—é™ä½ç†µï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ”¶æ•›æ€§ã€‚V-Reasonåˆ©ç”¨è¿™äº›ç†è®ºåŸºç¡€ï¼Œé€šè¿‡åŸºäºç†µçš„ç›®æ ‡å‡½æ•°ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­è°ƒæ•´LMMçš„å€¼ç¼“å­˜ï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„å¾®æ¢ç´¢å’Œåˆ©ç”¨è¡Œä¸ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œåœ¨ä¸è¿›è¡Œä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†ä¸RLè®­ç»ƒæ¨¡å‹çš„å·®è·ç¼©å°åˆ°å¹³å‡ç²¾åº¦0.6%ä»¥å†…ï¼ŒåŒæ—¶è¾“å‡ºtokenå‡å°‘äº†58.6%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è§†é¢‘æ¨ç†æ–¹æ³•ï¼Œé€šå¸¸éœ€è¦å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆ–å¤æ‚çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰æŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯¼è‡´äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œå¹¶ä¸”æ¨ç†è¿‡ç¨‹çš„æ§åˆ¶æœºåˆ¶ä¸å¤Ÿçµæ´»ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡å‹è¾“å‡ºçš„ç†µä½œä¸ºä¿¡å·ï¼Œæ¥æŒ‡å¯¼æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ¢ç´¢å’Œåˆ©ç”¨è¡Œä¸ºã€‚ä½œè€…è§‚å¯Ÿåˆ°ï¼Œé«˜è´¨é‡çš„æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šç»å†ä¸€ä¸ªå¾®æ¢ç´¢å’Œå¾®åˆ©ç”¨çš„äº¤æ›¿è¿‡ç¨‹ï¼Œå¹¶ä¸”æœ€ç»ˆä¼šæ”¶æ•›åˆ°ä¸€ä¸ªç¡®å®šçš„ç­”æ¡ˆã€‚é€šè¿‡ä¼˜åŒ–è¿™ä¸ªè¿‡ç¨‹ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ¨ç†ç²¾åº¦å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šV-Reasonæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **ç‰¹å¾æå–**ï¼šä½¿ç”¨LMMæå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ã€‚2) **å€¼ç¼“å­˜åˆå§‹åŒ–**ï¼šåˆå§‹åŒ–LMMçš„å€¼ç¼“å­˜ã€‚3) **æ§åˆ¶å™¨è®­ç»ƒ**ï¼šä½¿ç”¨åŸºäºç†µçš„ç›®æ ‡å‡½æ•°ï¼Œè®­ç»ƒä¸€ä¸ªå°çš„å¯è®­ç»ƒæ§åˆ¶å™¨ï¼Œç”¨äºè°ƒæ•´å€¼ç¼“å­˜ã€‚4) **æ¨ç†**ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„æ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´å€¼ç¼“å­˜ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹çš„æ¢ç´¢å’Œåˆ©ç”¨è¡Œä¸ºã€‚

**å…³é”®åˆ›æ–°**ï¼šV-Reasonçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†ä¼˜åŒ–æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒV-Reasonä¸éœ€è¦ä»»ä½•è®­ç»ƒæ•°æ®æˆ–å¥–åŠ±å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼˜åŒ–æ¨¡å‹çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼ŒV-Reasonè¿˜æå‡ºäº†ä¸€ç§åŸºäºç†µçš„ç›®æ ‡å‡½æ•°ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼æ¨¡å‹çš„æ¢ç´¢å’Œåˆ©ç”¨è¡Œä¸ºã€‚

**å…³é”®è®¾è®¡**ï¼šV-Reasonçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **ç†µç›®æ ‡å‡½æ•°**ï¼šä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„ç†µä½œä¸ºç›®æ ‡å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æ¢ç´¢é˜¶æ®µä¿æŒä¸€å®šçš„éšæœºæ€§ï¼Œåœ¨åˆ©ç”¨é˜¶æ®µå¿«é€Ÿæ”¶æ•›ã€‚2) **å¯è®­ç»ƒæ§åˆ¶å™¨**ï¼šä½¿ç”¨ä¸€ä¸ªå°çš„å¯è®­ç»ƒæ§åˆ¶å™¨æ¥è°ƒæ•´å€¼ç¼“å­˜ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹è¡Œä¸ºçš„ç²¾ç»†æ§åˆ¶ã€‚3) **ä¼˜åŒ–ç®—æ³•**ï¼šä½¿ç”¨Adamä¼˜åŒ–å™¨æ¥è®­ç»ƒæ§åˆ¶å™¨ï¼Œå¹¶è®¾ç½®åˆé€‚çš„å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

V-Reasonåœ¨å¤šä¸ªè§†é¢‘æ¨ç†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸è¿›è¡Œä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒV-Reasonå°†ä¸RLè®­ç»ƒæ¨¡å‹çš„å¹³å‡ç²¾åº¦å·®è·ç¼©å°åˆ°0.6%ä»¥å†…ã€‚åŒæ—¶ï¼ŒV-Reasonè¿˜å¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè¾“å‡ºtokenå‡å°‘äº†58.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒV-Reasonæ˜¯ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„è§†é¢‘æ¨ç†æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

V-Reasonå¯åº”ç”¨äºå„ç§éœ€è¦è§†é¢‘ç†è§£å’Œæ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘æœç´¢å’Œæ¨èç­‰ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒçš„ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿéƒ¨ç½²åˆ°æ–°çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œå¹¶é™ä½äº†æ¨¡å‹è®­ç»ƒå’Œç»´æŠ¤çš„æˆæœ¬ã€‚æœªæ¥ï¼ŒV-Reasonå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€çš„æ•°æ®ï¼Œä¾‹å¦‚éŸ³é¢‘å’Œæ–‡æœ¬ï¼Œä»è€Œå®ç°æ›´å…¨é¢çš„å¤šæ¨¡æ€æ¨ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video reasoning using Large Multimodal Models (LMMs) relies on costly reinforcement learning (RL) and verbose chain-of-thought, resulting in substantial computational overhead during both training and inference. Moreover, the mechanisms that control the thinking process in these reasoning models are very limited. In this paper, using entropy of the model's output as a signal, we discover that the high-quality models go through a series of micro-explorations and micro-exploitations which keep the reasoning process grounded (i.e., avoid excessive randomness while the model is exploring or thinking through an answer). We further observe that once this "thinking" process is over, more accurate models demonstrate a better convergence by reducing the entropy significantly via a final exploitation phase (i.e., a more certain convergence towards a solution trajectory). We then use these novel, theoretically-grounded insights to tune the model's behavior directly at inference, without using any RL or supervised fine-tuning. Specifically, during inference, our proposed approach called V-Reason (Video-Reason) adapts the value cache of the LMM via a few optimization steps on a small, trainable controller using an entropy-based objective, i.e., no supervision from any dataset or RL is necessary. This tuning improves the model's micro-exploration and exploitation behavior during inference. Our experiments show that our proposed method achieves significant improvements over the base instruction-tuned models across several video reasoning datasets, narrowing the gap with RL-trained models to within 0.6% average accuracy without any training, while offering massive efficiency benefits: output tokens are reduced by 58.6% compared to the RL model.

