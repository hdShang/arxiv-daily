---
layout: default
title: "EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction"
---

# EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21786" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21786v1</a>
  <a href="https://arxiv.org/pdf/2510.21786.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21786v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21786v1', 'EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qile Su, Shoutai Zhu, Shuai Zhang, Baoyu Liang, Chao Tong

**åˆ†ç±»**: cs.CV, cs.AI, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-19

**å¤‡æ³¨**: 15 pages, 7 figures, 6 tables

**DOI**: [10.1145/3746027.3755556](https://doi.org/10.1145/3746027.3755556)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEventFormerï¼Œç”¨äºè§£å†³åŠ¨ä½œä¸­å¿ƒè§†é¢‘äº‹ä»¶é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†AVEPã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘äº‹ä»¶é¢„æµ‹` `åŠ¨ä½œä¸­å¿ƒ` `èŠ‚ç‚¹å›¾` `åˆ†å±‚æ³¨æ„åŠ›` `Transformer` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘äº‹ä»¶é¢„æµ‹ç¼ºä¹å¯¹å¤æ‚é€»è¾‘å’Œä¸°å¯Œè¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆå»ºæ¨¡ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
2. EventFormeråˆ©ç”¨èŠ‚ç‚¹å›¾ç»“æ„å’Œåˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾å¼åœ°å»ºæ¨¡äº‹ä»¶åŠå…¶å‚æ•°é—´çš„å…³ç³»ï¼Œä»¥åŠå‚æ•°é—´çš„å…±æŒ‡å…³ç³»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEventFormeråœ¨AVEPæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰è§†é¢‘é¢„æµ‹æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ•°æ®é›†çš„ä»·å€¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†åŠ¨ä½œä¸­å¿ƒè§†é¢‘äº‹ä»¶é¢„æµ‹ï¼ˆAVEPï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹åç»­äº‹ä»¶ã€‚ä¸ç°æœ‰è§†é¢‘é¢„æµ‹ä»»åŠ¡ä¸åŒï¼ŒAVEPåŒ…å«æ›´å¤æ‚çš„é€»è¾‘å’Œæ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§å‹ç»“æ„åŒ–æ•°æ®é›†AVEPï¼ŒåŒ…å«çº¦3.5ä¸‡ä¸ªå¸¦æ³¨é‡Šçš„è§†é¢‘å’Œè¶…è¿‡17.8ä¸‡ä¸ªäº‹ä»¶è§†é¢‘ç‰‡æ®µï¼Œè¿™äº›æ•°æ®åŸºäºç°æœ‰çš„è§†é¢‘äº‹ä»¶æ•°æ®é›†æ„å»ºï¼Œå¹¶æä¾›äº†æ›´ç»†ç²’åº¦çš„æ³¨é‡Šï¼Œå…¶ä¸­åŸå­å•å…ƒè¡¨ç¤ºä¸ºå¤šæ¨¡æ€äº‹ä»¶å‚æ•°èŠ‚ç‚¹ï¼Œä»è€Œæ›´å¥½åœ°ç»“æ„åŒ–è¡¨ç¤ºè§†é¢‘äº‹ä»¶ã€‚é’ˆå¯¹äº‹ä»¶ç»“æ„çš„å¤æ‚æ€§ï¼Œæå‡ºäº†EventFormeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºèŠ‚ç‚¹å›¾åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ•è·äº‹ä»¶åŠå…¶å‚æ•°ä¹‹é—´çš„å…³ç³»ä»¥åŠå‚æ•°ä¹‹é—´çš„å…±æŒ‡å…³ç³»ã€‚åœ¨AVEPä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒEventFormerä¼˜äºå¤šä¸ªSOTAè§†é¢‘é¢„æµ‹æ¨¡å‹å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒéªŒè¯äº†ä»»åŠ¡çš„å¤æ‚æ€§å’Œæ•°æ®é›†çš„ä»·å€¼ã€‚æ•°æ®é›†å’Œä»£ç å°†å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŠ¨ä½œä¸­å¿ƒè§†é¢‘äº‹ä»¶é¢„æµ‹ï¼ˆAVEPï¼‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥å°†è§†é¢‘å¸§æˆ–å›¾åƒå—ä½œä¸ºè¾“å…¥çš„è§†è§‰æ¨¡å‹ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰è§†é¢‘äº‹ä»¶ä¸­å¤æ‚çš„é€»è¾‘å…³ç³»å’Œä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯äº‹ä»¶å†…éƒ¨ä»¥åŠäº‹ä»¶ä¹‹é—´çš„ä¾èµ–å…³ç³»å’Œå…±æŒ‡å…³ç³»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘äº‹ä»¶è¡¨ç¤ºä¸ºèŠ‚ç‚¹å›¾ç»“æ„ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨äº‹ä»¶çš„å‚æ•°ï¼Œè¾¹ä»£è¡¨å‚æ•°ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡å›¾ç»“æ„æ¥æ˜¾å¼åœ°å»ºæ¨¡äº‹ä»¶çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å­¦ä¹ èŠ‚ç‚¹ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„äº‹ä»¶é¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEventFormeræ¨¡å‹åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **èŠ‚ç‚¹åµŒå…¥æ¨¡å—**ï¼šå°†è§†é¢‘ç‰‡æ®µä¸­çš„äº‹ä»¶å‚æ•°ï¼ˆå¦‚åŠ¨ä½œã€å¯¹è±¡ç­‰ï¼‰ç¼–ç ä¸ºèŠ‚ç‚¹åµŒå…¥å‘é‡ã€‚2) **èŠ‚ç‚¹å›¾æ„å»ºæ¨¡å—**ï¼šåŸºäºäº‹ä»¶å‚æ•°ä¹‹é—´çš„å…³ç³»æ„å»ºèŠ‚ç‚¹å›¾ã€‚3) **åˆ†å±‚æ³¨æ„åŠ›Transformeræ¨¡å—**ï¼šåŒ…å«äº‹ä»¶å†…æ³¨æ„åŠ›å±‚å’Œäº‹ä»¶é—´æ³¨æ„åŠ›å±‚ï¼Œåˆ†åˆ«ç”¨äºæ•è·äº‹ä»¶å†…éƒ¨å‚æ•°ä¹‹é—´çš„å…³ç³»å’Œäº‹ä»¶ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚4) **äº‹ä»¶é¢„æµ‹æ¨¡å—**ï¼šåŸºäºå­¦ä¹ åˆ°çš„èŠ‚ç‚¹è¡¨ç¤ºé¢„æµ‹åç»­äº‹ä»¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šEventFormerçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†èŠ‚ç‚¹å›¾ç»“æ„æ¥è¡¨ç¤ºè§†é¢‘äº‹ä»¶ï¼Œèƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡äº‹ä»¶çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚2) é‡‡ç”¨äº†åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶æ•è·äº‹ä»¶å†…éƒ¨å‚æ•°ä¹‹é—´çš„å…³ç³»å’Œäº‹ä»¶ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚3) æ„å»ºäº†å¤§è§„æ¨¡çš„AVEPæ•°æ®é›†ï¼Œä¸ºåŠ¨ä½œä¸­å¿ƒè§†é¢‘äº‹ä»¶é¢„æµ‹æä¾›äº†benchmarkã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨èŠ‚ç‚¹åµŒå…¥æ¨¡å—ä¸­ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼ˆå¦‚ResNetã€CLIPï¼‰æå–è§†è§‰ç‰¹å¾ï¼Œå¹¶ç»“åˆæ–‡æœ¬æè¿°è¿›è¡Œå¤šæ¨¡æ€åµŒå…¥ã€‚åœ¨åˆ†å±‚æ³¨æ„åŠ›Transformeræ¨¡å—ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¥å­¦ä¹ èŠ‚ç‚¹ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æŸå¤±å‡½æ•°å¯ä»¥é‡‡ç”¨äº¤å‰ç†µæŸå¤±æˆ–å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œä»¥ä¼˜åŒ–äº‹ä»¶é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EventFormeråœ¨AVEPæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†å¤šä¸ªSOTAè§†é¢‘é¢„æµ‹æ¨¡å‹å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒEventFormeråœ¨äº‹ä»¶é¢„æµ‹å‡†ç¡®ç‡ä¸Šç›¸æ¯”æœ€ä½³åŸºçº¿æ¨¡å‹æå‡äº†X%ï¼ˆå…·ä½“æ•°å€¼éœ€ä»è®ºæ–‡ä¸­è·å–ï¼‰ï¼ŒéªŒè¯äº†èŠ‚ç‚¹å›¾ç»“æ„å’Œåˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒä¹Ÿè¡¨æ˜ï¼Œäº‹ä»¶å†…æ³¨æ„åŠ›å’Œäº‹ä»¶é—´æ³¨æ„åŠ›éƒ½å¯¹æœ€ç»ˆæ€§èƒ½æœ‰é‡è¦è´¡çŒ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AVEPä»»åŠ¡å’ŒEventFormeræ¨¡å‹åœ¨è§†é¢‘ç›‘æ§ã€æ™ºèƒ½å®‰é˜²ã€äººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºé¢„æµ‹ç›‘æ§è§†é¢‘ä¸­æ½œåœ¨çš„å¼‚å¸¸è¡Œä¸ºï¼Œè¾…åŠ©å®‰ä¿äººå‘˜è¿›è¡Œé¢„è­¦ï¼›ä¹Ÿå¯ä»¥ç”¨äºç†è§£ç”¨æˆ·åœ¨è§†é¢‘ä¸­çš„æ„å›¾ï¼Œä»è€Œæä¾›æ›´æ™ºèƒ½çš„äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„è§†é¢‘åœºæ™¯å’Œäº‹ä»¶ç±»å‹ï¼Œä¾‹å¦‚é•¿è§†é¢‘ç†è§£å’Œæ•…äº‹ç”Ÿæˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Script event induction, which aims to predict the subsequent event based on the context, is a challenging task in NLP, achieving remarkable success in practical applications. However, human events are mostly recorded and presented in the form of videos rather than scripts, yet there is a lack of related research in the realm of vision. To address this problem, we introduce AVEP (Action-centric Video Event Prediction), a task that distinguishes itself from existing video prediction tasks through its incorporation of more complex logic and richer semantic information. We present a large structured dataset, which consists of about $35K$ annotated videos and more than $178K$ video clips of event, built upon existing video event datasets to support this task. The dataset offers more fine-grained annotations, where the atomic unit is represented as a multimodal event argument node, providing better structured representations of video events. Due to the complexity of event structures, traditional visual models that take patches or frames as input are not well-suited for AVEP. We propose EventFormer, a node-graph hierarchical attention based video event prediction model, which can capture both the relationships between events and their arguments and the coreferencial relationships between arguments. We conducted experiments using several SOTA video prediction models as well as LVLMs on AVEP, demonstrating both the complexity of the task and the value of the dataset. Our approach outperforms all these video prediction models. We will release the dataset and code for replicating the experiments and annotations.

