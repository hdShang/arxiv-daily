---
layout: default
title: EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction
---

# EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21786" target="_blank" class="toolbar-btn">arXiv: 2510.21786v1</a>
    <a href="https://arxiv.org/pdf/2510.21786.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21786v1" 
            onclick="toggleFavorite(this, '2510.21786v1', 'EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Qile Su, Shoutai Zhu, Shuai Zhang, Baoyu Liang, Chao Tong

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-19

**Â§áÊ≥®**: 15 pages, 7 figures, 6 tables

**DOI**: [10.1145/3746027.3755556](https://doi.org/10.1145/3746027.3755556)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EventFormerÔºåÁî®‰∫éËß£ÂÜ≥Âä®‰Ωú‰∏≠ÂøÉËßÜÈ¢ë‰∫ã‰ª∂È¢ÑÊµã‰ªªÂä°ÔºåÂπ∂ÊûÑÂª∫Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜAVEP„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ë‰∫ã‰ª∂È¢ÑÊµã` `Âä®‰Ωú‰∏≠ÂøÉ` `ËäÇÁÇπÂõæ` `ÂàÜÂ±ÇÊ≥®ÊÑèÂäõ` `Transformer` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ËßÜÈ¢ëÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ë‰∫ã‰ª∂È¢ÑÊµãÁº∫‰πèÂØπÂ§çÊùÇÈÄªËæëÂíå‰∏∞ÂØåËØ≠‰πâ‰ø°ÊÅØÁöÑÊúâÊïàÂª∫Ê®°ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ
2. EventFormerÂà©Áî®ËäÇÁÇπÂõæÁªìÊûÑÂíåÂàÜÂ±ÇÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊòæÂºèÂú∞Âª∫Ê®°‰∫ã‰ª∂ÂèäÂÖ∂ÂèÇÊï∞Èó¥ÁöÑÂÖ≥Á≥ªÔºå‰ª•ÂèäÂèÇÊï∞Èó¥ÁöÑÂÖ±ÊåáÂÖ≥Á≥ª„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåEventFormerÂú®AVEPÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâËßÜÈ¢ëÈ¢ÑÊµãÊ®°ÂûãÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÊï∞ÊçÆÈõÜÁöÑ‰ª∑ÂÄº„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜÂä®‰Ωú‰∏≠ÂøÉËßÜÈ¢ë‰∫ã‰ª∂È¢ÑÊµãÔºàAVEPÔºâ‰ªªÂä°ÔºåÊó®Âú®Ê†πÊçÆ‰∏ä‰∏ãÊñáÈ¢ÑÊµãÂêéÁª≠‰∫ã‰ª∂„ÄÇ‰∏éÁé∞ÊúâËßÜÈ¢ëÈ¢ÑÊµã‰ªªÂä°‰∏çÂêåÔºåAVEPÂåÖÂê´Êõ¥Â§çÊùÇÁöÑÈÄªËæëÂíåÊõ¥‰∏∞ÂØåÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇ‰∏∫Ê≠§ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßÂûãÁªìÊûÑÂåñÊï∞ÊçÆÈõÜAVEPÔºåÂåÖÂê´Á∫¶3.5‰∏á‰∏™Â∏¶Ê≥®ÈáäÁöÑËßÜÈ¢ëÂíåË∂ÖËøá17.8‰∏á‰∏™‰∫ã‰ª∂ËßÜÈ¢ëÁâáÊÆµÔºåËøô‰∫õÊï∞ÊçÆÂü∫‰∫éÁé∞ÊúâÁöÑËßÜÈ¢ë‰∫ã‰ª∂Êï∞ÊçÆÈõÜÊûÑÂª∫ÔºåÂπ∂Êèê‰æõ‰∫ÜÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÊ≥®ÈáäÔºåÂÖ∂‰∏≠ÂéüÂ≠êÂçïÂÖÉË°®Á§∫‰∏∫Â§öÊ®°ÊÄÅ‰∫ã‰ª∂ÂèÇÊï∞ËäÇÁÇπÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁªìÊûÑÂåñË°®Á§∫ËßÜÈ¢ë‰∫ã‰ª∂„ÄÇÈíàÂØπ‰∫ã‰ª∂ÁªìÊûÑÁöÑÂ§çÊùÇÊÄßÔºåÊèêÂá∫‰∫ÜEventFormerÊ®°ÂûãÔºåËØ•Ê®°ÂûãÂü∫‰∫éËäÇÁÇπÂõæÂàÜÂ±ÇÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÊçïËé∑‰∫ã‰ª∂ÂèäÂÖ∂ÂèÇÊï∞‰πãÈó¥ÁöÑÂÖ≥Á≥ª‰ª•ÂèäÂèÇÊï∞‰πãÈó¥ÁöÑÂÖ±ÊåáÂÖ≥Á≥ª„ÄÇÂú®AVEP‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåEventFormer‰ºò‰∫éÂ§ö‰∏™SOTAËßÜÈ¢ëÈ¢ÑÊµãÊ®°ÂûãÂíåÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåÈ™åËØÅ‰∫Ü‰ªªÂä°ÁöÑÂ§çÊùÇÊÄßÂíåÊï∞ÊçÆÈõÜÁöÑ‰ª∑ÂÄº„ÄÇÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÂ∞ÜÂºÄÊ∫ê„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âä®‰Ωú‰∏≠ÂøÉËßÜÈ¢ë‰∫ã‰ª∂È¢ÑÊµãÔºàAVEPÔºâÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÁõ¥Êé•Â∞ÜËßÜÈ¢ëÂ∏ßÊàñÂõæÂÉèÂùó‰Ωú‰∏∫ËæìÂÖ•ÁöÑËßÜËßâÊ®°ÂûãÔºåÈöæ‰ª•ÊúâÊïàÊçïÊçâËßÜÈ¢ë‰∫ã‰ª∂‰∏≠Â§çÊùÇÁöÑÈÄªËæëÂÖ≥Á≥ªÂíå‰∏∞ÂØåÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÁâπÂà´ÊòØ‰∫ã‰ª∂ÂÜÖÈÉ®‰ª•Âèä‰∫ã‰ª∂‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÂíåÂÖ±ÊåáÂÖ≥Á≥ª„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËßÜÈ¢ë‰∫ã‰ª∂Ë°®Á§∫‰∏∫ËäÇÁÇπÂõæÁªìÊûÑÔºåÂÖ∂‰∏≠ËäÇÁÇπ‰ª£Ë°®‰∫ã‰ª∂ÁöÑÂèÇÊï∞ÔºåËæπ‰ª£Ë°®ÂèÇÊï∞‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÈÄöËøáÂõæÁªìÊûÑÊù•ÊòæÂºèÂú∞Âª∫Ê®°‰∫ã‰ª∂ÁöÑÁªìÊûÑÂåñ‰ø°ÊÅØÔºåÂπ∂Âà©Áî®Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â≠¶‰π†ËäÇÁÇπ‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑ‰∫ã‰ª∂È¢ÑÊµã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEventFormerÊ®°ÂûãÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) **ËäÇÁÇπÂµåÂÖ•Ê®°Âùó**ÔºöÂ∞ÜËßÜÈ¢ëÁâáÊÆµ‰∏≠ÁöÑ‰∫ã‰ª∂ÂèÇÊï∞ÔºàÂ¶ÇÂä®‰Ωú„ÄÅÂØπË±°Á≠âÔºâÁºñÁ†Å‰∏∫ËäÇÁÇπÂµåÂÖ•ÂêëÈáè„ÄÇ2) **ËäÇÁÇπÂõæÊûÑÂª∫Ê®°Âùó**ÔºöÂü∫‰∫é‰∫ã‰ª∂ÂèÇÊï∞‰πãÈó¥ÁöÑÂÖ≥Á≥ªÊûÑÂª∫ËäÇÁÇπÂõæ„ÄÇ3) **ÂàÜÂ±ÇÊ≥®ÊÑèÂäõTransformerÊ®°Âùó**ÔºöÂåÖÂê´‰∫ã‰ª∂ÂÜÖÊ≥®ÊÑèÂäõÂ±ÇÂíå‰∫ã‰ª∂Èó¥Ê≥®ÊÑèÂäõÂ±ÇÔºåÂàÜÂà´Áî®‰∫éÊçïËé∑‰∫ã‰ª∂ÂÜÖÈÉ®ÂèÇÊï∞‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂíå‰∫ã‰ª∂‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ4) **‰∫ã‰ª∂È¢ÑÊµãÊ®°Âùó**ÔºöÂü∫‰∫éÂ≠¶‰π†Âà∞ÁöÑËäÇÁÇπË°®Á§∫È¢ÑÊµãÂêéÁª≠‰∫ã‰ª∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEventFormerÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜËäÇÁÇπÂõæÁªìÊûÑÊù•Ë°®Á§∫ËßÜÈ¢ë‰∫ã‰ª∂ÔºåËÉΩÂ§üÊòæÂºèÂú∞Âª∫Ê®°‰∫ã‰ª∂ÁöÑÁªìÊûÑÂåñ‰ø°ÊÅØ„ÄÇ2) ÈááÁî®‰∫ÜÂàÜÂ±ÇÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÂêåÊó∂ÊçïËé∑‰∫ã‰ª∂ÂÜÖÈÉ®ÂèÇÊï∞‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂíå‰∫ã‰ª∂‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ3) ÊûÑÂª∫‰∫ÜÂ§ßËßÑÊ®°ÁöÑAVEPÊï∞ÊçÆÈõÜÔºå‰∏∫Âä®‰Ωú‰∏≠ÂøÉËßÜÈ¢ë‰∫ã‰ª∂È¢ÑÊµãÊèê‰æõ‰∫Übenchmark„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËäÇÁÇπÂµåÂÖ•Ê®°Âùó‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÊ®°ÂûãÔºàÂ¶ÇResNet„ÄÅCLIPÔºâÊèêÂèñËßÜËßâÁâπÂæÅÔºåÂπ∂ÁªìÂêàÊñáÊú¨ÊèèËø∞ËøõË°åÂ§öÊ®°ÊÄÅÂµåÂÖ•„ÄÇÂú®ÂàÜÂ±ÇÊ≥®ÊÑèÂäõTransformerÊ®°Âùó‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â≠¶‰π†ËäÇÁÇπ‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇÊçüÂ§±ÂáΩÊï∞ÂèØ‰ª•ÈááÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÊàñÂØπÊØîÂ≠¶‰π†ÊçüÂ§±Ôºå‰ª•‰ºòÂåñ‰∫ã‰ª∂È¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EventFormerÂú®AVEPÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåË∂ÖË∂ä‰∫ÜÂ§ö‰∏™SOTAËßÜÈ¢ëÈ¢ÑÊµãÊ®°ÂûãÂíåÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåEventFormerÂú®‰∫ã‰ª∂È¢ÑÊµãÂáÜÁ°ÆÁéá‰∏äÁõ∏ÊØîÊúÄ‰Ω≥Âü∫Á∫øÊ®°ÂûãÊèêÂçá‰∫ÜX%ÔºàÂÖ∑‰ΩìÊï∞ÂÄºÈúÄ‰ªéËÆ∫Êñá‰∏≠Ëé∑ÂèñÔºâÔºåÈ™åËØÅ‰∫ÜËäÇÁÇπÂõæÁªìÊûÑÂíåÂàÜÂ±ÇÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåÊ∂àËûçÂÆûÈ™å‰πüË°®ÊòéÔºå‰∫ã‰ª∂ÂÜÖÊ≥®ÊÑèÂäõÂíå‰∫ã‰ª∂Èó¥Ê≥®ÊÑèÂäõÈÉΩÂØπÊúÄÁªàÊÄßËÉΩÊúâÈáçË¶ÅË¥°ÁåÆ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AVEP‰ªªÂä°ÂíåEventFormerÊ®°ÂûãÂú®ËßÜÈ¢ëÁõëÊéß„ÄÅÊô∫ËÉΩÂÆâÈò≤„ÄÅ‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Áî®‰∫éÈ¢ÑÊµãÁõëÊéßËßÜÈ¢ë‰∏≠ÊΩúÂú®ÁöÑÂºÇÂ∏∏Ë°å‰∏∫ÔºåËæÖÂä©ÂÆâ‰øù‰∫∫ÂëòËøõË°åÈ¢ÑË≠¶Ôºõ‰πüÂèØ‰ª•Áî®‰∫éÁêÜËß£Áî®Êà∑Âú®ËßÜÈ¢ë‰∏≠ÁöÑÊÑèÂõæÔºå‰ªéËÄåÊèê‰æõÊõ¥Êô∫ËÉΩÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÂèØ‰ª•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑËßÜÈ¢ëÂú∫ÊôØÂíå‰∫ã‰ª∂Á±ªÂûãÔºå‰æãÂ¶ÇÈïøËßÜÈ¢ëÁêÜËß£ÂíåÊïÖ‰∫ãÁîüÊàê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Script event induction, which aims to predict the subsequent event based on the context, is a challenging task in NLP, achieving remarkable success in practical applications. However, human events are mostly recorded and presented in the form of videos rather than scripts, yet there is a lack of related research in the realm of vision. To address this problem, we introduce AVEP (Action-centric Video Event Prediction), a task that distinguishes itself from existing video prediction tasks through its incorporation of more complex logic and richer semantic information. We present a large structured dataset, which consists of about $35K$ annotated videos and more than $178K$ video clips of event, built upon existing video event datasets to support this task. The dataset offers more fine-grained annotations, where the atomic unit is represented as a multimodal event argument node, providing better structured representations of video events. Due to the complexity of event structures, traditional visual models that take patches or frames as input are not well-suited for AVEP. We propose EventFormer, a node-graph hierarchical attention based video event prediction model, which can capture both the relationships between events and their arguments and the coreferencial relationships between arguments. We conducted experiments using several SOTA video prediction models as well as LVLMs on AVEP, demonstrating both the complexity of the task and the value of the dataset. Our approach outperforms all these video prediction models. We will release the dataset and code for replicating the experiments and annotations.

