---
layout: default
title: Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding
---

# Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17034" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17034v1</a>
  <a href="https://arxiv.org/pdf/2510.17034.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17034v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17034v1', 'Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yutong Zhong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºW2R2æ¡†æ¶ï¼Œè§£å†³è§†é¢‘LLMä¸­3D groundingçš„2Dè¯­ä¹‰åè§é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3D Grounding` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€èåˆ` `è¡¨ç¤ºå­¦ä¹ ` `å‡ ä½•æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨3D groundingä¸­è¿‡åº¦ä¾èµ–2Då›¾åƒç‰¹å¾ï¼Œå¿½ç•¥3Då‡ ä½•ä¿¡æ¯ï¼Œå¯¼è‡´â€œ2Dè¯­ä¹‰åè§â€å’Œå®šä½ç²¾åº¦ä¸‹é™ã€‚
2. W2R2æ¡†æ¶é€šè¿‡è§£è€¦2Då’Œ3Dç‰¹å¾è¡¨ç¤ºï¼Œåˆ†åˆ«ä½œä¸ºè¯­ä¹‰ä¿¡æ ‡å’Œç©ºé—´é”šç‚¹ï¼Œä»è€ŒæŠ‘åˆ¶2Dè¯­ä¹‰åè§ã€‚
3. åœ¨ScanReferå’ŒScanQAæ•°æ®é›†ä¸Šï¼ŒW2R2æ˜¾è‘—æå‡äº†3D groundingçš„å®šä½ç²¾åº¦å’Œé²æ£’æ€§ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€3D groundingåœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­å¤‡å—å…³æ³¨ï¼Œæ—¨åœ¨æå‡å¤æ‚ç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„â€œ2Dè¯­ä¹‰åè§â€ï¼Œè¿‡åº¦ä¾èµ–2Då›¾åƒç‰¹å¾è¿›è¡Œç²—ç•¥å®šä½ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½ç•¥äº†3Då‡ ä½•è¾“å…¥ï¼Œå¯¼è‡´èåˆæ€§èƒ½æ¬ ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºWhat-Where Representation Re-Forming (W2R2) çš„æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è¡¨ç¤ºå­¦ä¹ å’Œæœ‰é’ˆå¯¹æ€§çš„æ·å¾„æŠ‘åˆ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»æ ¹æœ¬ä¸Šé‡å¡‘äº†æ¨¡å‹çš„å†…éƒ¨ç©ºé—´ï¼Œå°†2Dç‰¹å¾æŒ‡å®šä¸ºâ€œWhatâ€è¯†åˆ«çš„è¯­ä¹‰ä¿¡æ ‡ï¼Œå°†3Dç‰¹å¾æŒ‡å®šä¸ºâ€œWhereâ€å®šä½çš„ç©ºé—´é”šç‚¹ï¼Œä»è€Œåœ¨ä¸ä¿®æ”¹æ¨ç†æ¶æ„çš„æƒ…å†µä¸‹å®ç°ç²¾ç¡®çš„3D groundingã€‚å…³é”®ç»„ä»¶åŒ…æ‹¬ä¸€ä¸ªåŒç›®æ ‡æŸå¤±å‡½æ•°ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªå¯¹é½æŸå¤±ï¼Œè¯¥æŸå¤±ä½¿ç”¨é€‚åº”çš„äº¤å‰ç†µæ¥ç›‘ç£èåˆé¢„æµ‹ï¼Œä»¥å®ç°å¤šæ¨¡æ€ååŒï¼›ä»¥åŠä¸€ä¸ªä¼ªæ ‡ç­¾æŸå¤±ï¼Œè¯¥æŸå¤±é€šè¿‡åŸºäºè¾¹é™…çš„æœºåˆ¶æƒ©ç½šè¿‡äºæœ‰æ•ˆçš„2Dä¸»å¯¼ä¼ªè¾“å‡ºã€‚åœ¨ScanReferå’ŒScanQAä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†W2R2çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å®šä½ç²¾åº¦å’Œé²æ£’æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ‚ä¹±çš„æˆ·å¤–åœºæ™¯ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è¿›è¡Œ3D groundingæ—¶ï¼Œè¿‡åº¦ä¾èµ–2Då›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†3Då‡ ä½•ä¿¡æ¯æä¾›çš„ç©ºé—´çº¿ç´¢ã€‚è¿™ç§â€œ2Dè¯­ä¹‰åè§â€å¯¼è‡´æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹å®šä½ç²¾åº¦ä¸‹é™ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„ä¼˜åŠ¿ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆ2Dè¯­ä¹‰å’Œ3Då‡ ä½•ä¿¡æ¯ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„3D groundingæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šW2R2çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è§£è€¦2Då’Œ3Dç‰¹å¾çš„è¡¨ç¤ºï¼Œå¼ºåˆ¶æ¨¡å‹å­¦ä¹ å°†2Dç‰¹å¾ä½œä¸ºâ€œWhatâ€çš„è¯­ä¹‰ä¿¡æ¯æ¥æºï¼Œå°†3Dç‰¹å¾ä½œä¸ºâ€œWhereâ€çš„ç©ºé—´ä¿¡æ¯æ¥æºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨3Då‡ ä½•ä¿¡æ¯è¿›è¡Œç²¾ç¡®å®šä½ï¼Œä»è€Œå…‹æœ2Dè¯­ä¹‰åè§ã€‚è¿™ç§è§£è€¦æ˜¯é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥æ¥å®ç°çš„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šW2R2æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæ¨¡å‹æ¥æ”¶è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œæå–2Då›¾åƒç‰¹å¾å’Œ3Då‡ ä½•ç‰¹å¾ã€‚ç„¶åï¼Œé€šè¿‡ç‰¹å¾èåˆæ¨¡å—å°†å¤šæ¨¡æ€ç‰¹å¾è¿›è¡Œèåˆã€‚æ¥ä¸‹æ¥ï¼Œæ¨¡å‹åŸºäºèåˆåçš„ç‰¹å¾è¿›è¡Œ3D groundingé¢„æµ‹ã€‚æœ€åï¼Œé€šè¿‡åŒç›®æ ‡æŸå¤±å‡½æ•°å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬å¯¹é½æŸå¤±å’Œä¼ªæ ‡ç­¾æŸå¤±ã€‚æ•´ä¸ªæ¡†æ¶åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œä¼˜åŒ–ï¼Œæ¨ç†é˜¶æ®µä¿æŒä¸å˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šW2R2æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶è§£è€¦è¡¨ç¤ºå­¦ä¹ å’Œæœ‰é’ˆå¯¹æ€§çš„æ·å¾„æŠ‘åˆ¶ç­–ç•¥ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒW2R2ä¸æ˜¯ç®€å•åœ°å°†2Då’Œ3Dç‰¹å¾è¿›è¡Œèåˆï¼Œè€Œæ˜¯é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥ï¼Œå¼ºåˆ¶æ¨¡å‹å­¦ä¹ å°†2Då’Œ3Dç‰¹å¾åˆ†åˆ«ä½œä¸ºè¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯çš„æ¥æºã€‚è¿™ç§è§£è€¦è¡¨ç¤ºå­¦ä¹ å¯ä»¥æœ‰æ•ˆæŠ‘åˆ¶2Dè¯­ä¹‰åè§ï¼Œæé«˜3D groundingçš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šW2R2çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¯¹é½æŸå¤±ï¼šä½¿ç”¨é€‚åº”çš„äº¤å‰ç†µæŸå¤±æ¥ç›‘ç£èåˆé¢„æµ‹ï¼Œé¼“åŠ±å¤šæ¨¡æ€ç‰¹å¾çš„ååŒä½œç”¨ã€‚2) ä¼ªæ ‡ç­¾æŸå¤±ï¼šé€šè¿‡åŸºäºè¾¹é™…çš„æœºåˆ¶æƒ©ç½šè¿‡äºæœ‰æ•ˆçš„2Dä¸»å¯¼ä¼ªè¾“å‡ºï¼Œä»è€ŒæŠ‘åˆ¶2Dè¯­ä¹‰åè§ã€‚3) åŒç›®æ ‡æŸå¤±å‡½æ•°ï¼šå°†å¯¹é½æŸå¤±å’Œä¼ªæ ‡ç­¾æŸå¤±ç»“åˆèµ·æ¥ï¼Œå…±åŒä¼˜åŒ–æ¨¡å‹ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°æƒé‡å’Œè¾¹é™…å‚æ•°éœ€è¦æ ¹æ®å®éªŒè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒW2R2åœ¨ScanReferå’ŒScanQAæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ScanReferæ•°æ®é›†ä¸Šï¼ŒW2R2çš„å®šä½ç²¾åº¦æé«˜äº†X%ï¼Œåœ¨ScanQAæ•°æ®é›†ä¸Šï¼ŒW2R2çš„é²æ£’æ€§æé«˜äº†Y%ã€‚å°¤å…¶æ˜¯åœ¨æ‚ä¹±çš„æˆ·å¤–åœºæ™¯ä¸­ï¼ŒW2R2çš„æ€§èƒ½æå‡æ›´ä¸ºæ˜æ˜¾ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæŠ‘åˆ¶2Dè¯­ä¹‰åè§çš„èƒ½åŠ›ã€‚å…·ä½“æ•°å€¼Xå’ŒYéœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡3D groundingçš„ç²¾åº¦å’Œé²æ£’æ€§ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°ç†è§£å’Œå®šä½ç‰©ä½“ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„äº¤äº’å’Œå†³ç­–ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸï¼Œæå‡äººæœºåä½œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal 3D grounding has garnered considerable interest in Vision-Language Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex environments. However, these models suffer from a severe "2D semantic bias" that arises from over-reliance on 2D image features for coarse localization, largely disregarding 3D geometric inputs and resulting in suboptimal fusion performance. In this paper, we propose a novel training framework called What-Where Representation Re-Forming (W2R2) to tackle this issue via disentangled representation learning and targeted shortcut suppression. Our approach fundamentally reshapes the model's internal space by designating 2D features as semantic beacons for "What" identification and 3D features as spatial anchors for "Where" localization, enabling precise 3D grounding without modifying inference architecture. Key components include a dual-objective loss function with an Alignment Loss that supervises fused predictions using adapted cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes overly effective 2D-dominant pseudo-outputs via a margin-based mechanism. Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of W2R2, with significant gains in localization accuracy and robustness, particularly in cluttered outdoor scenes.

