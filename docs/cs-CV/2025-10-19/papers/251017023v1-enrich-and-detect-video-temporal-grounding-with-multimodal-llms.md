---
layout: default
title: Enrich and Detect: Video Temporal Grounding with Multimodal LLMs
---

# Enrich and Detect: Video Temporal Grounding with Multimodal LLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.17023" target="_blank" class="toolbar-btn">arXiv: 2510.17023v1</a>
    <a href="https://arxiv.org/pdf/2510.17023.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17023v1" 
            onclick="toggleFavorite(this, '2510.17023v1', 'Enrich and Detect: Video Temporal Grounding with Multimodal LLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shraman Pramanick, Effrosyni Mavroudi, Yale Song, Rama Chellappa, Lorenzo Torresani, Triantafyllos Afouras

**ÂàÜÁ±ª**: cs.CV, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-19

**Â§áÊ≥®**: ICCV 2025 (Highlights)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ED-VTGÔºåÂà©Áî®Â§öÊ®°ÊÄÅLLMËøõË°åÁªÜÁ≤íÂ∫¶ËßÜÈ¢ëÊó∂Â∫èÂÆö‰Ωç**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÊó∂Â∫èÂÆö‰Ωç` `Â§öÊ®°ÊÄÅLLM` `Êü•ËØ¢ÂØåÈõÜ` `Â§öÁ§∫‰æãÂ≠¶‰π†` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂà©Áî®ÊñáÊú¨‰ø°ÊÅØÔºå‰∏îÊòìÂèóÂô™Â£∞ÂíåÂπªËßâÂΩ±Âìç„ÄÇ
2. ED-VTGÈÄöËøáÂ§öÊ®°ÊÄÅLLMÂ¢ûÂº∫Êü•ËØ¢‰ø°ÊÅØÔºåÂπ∂‰ΩøÁî®ËΩªÈáèÁ∫ßËß£Á†ÅÂô®ËøõË°åÁ≤æÁ°ÆÂÆö‰Ωç„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåED-VTGÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞SOTAÔºåÂπ∂Âú®Èõ∂Ê†∑Êú¨Â≠¶‰π†‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ED-VTGÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÁªÜÁ≤íÂ∫¶ËßÜÈ¢ëÊó∂Â∫èÂÆö‰Ωç„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Â§öÊ®°ÊÄÅLLMÁöÑËÉΩÂäõÔºåËÅîÂêàÂ§ÑÁêÜÊñáÊú¨ÂíåËßÜÈ¢ëÔºåÈÄöËøá‰∏Ä‰∏™‰∏§Èò∂ÊÆµËøáÁ®ãÊúâÊïàÂú∞Âú®ËßÜÈ¢ë‰∏≠ÂÆö‰ΩçËá™ÁÑ∂ËØ≠Ë®ÄÊü•ËØ¢„ÄÇÈ¶ñÂÖàÔºåËØ≠Ë®ÄÊü•ËØ¢Ë¢´ËΩ¨Êç¢‰∏∫ÂØåÂê´‰ø°ÊÅØÁöÑÂè•Â≠êÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∫ÜÁº∫Â§±ÁöÑÁªÜËäÇÂíåÁ∫øÁ¥¢Ôºå‰ª•ËæÖÂä©ÂÆö‰Ωç„ÄÇÂÖ∂Ê¨°Ôºå‰ΩøÁî®‰∏Ä‰∏™ËΩªÈáèÁ∫ßËß£Á†ÅÂô®ÂØπËøô‰∫õÂØåÂê´‰ø°ÊÅØÁöÑÊü•ËØ¢ËøõË°åÂÆö‰ΩçÔºåËØ•Ëß£Á†ÅÂô®‰∏ìÈó®Áî®‰∫éÈ¢ÑÊµãÂü∫‰∫éÂØåÂê´‰ø°ÊÅØÊü•ËØ¢ÁöÑ‰∏ä‰∏ãÊñáË°®Á§∫ÁöÑÁ≤æÁ°ÆËæπÁïå„ÄÇ‰∏∫‰∫ÜÂáèËΩªÂô™Â£∞Âπ∂ÂáèÂ∞ëÂπªËßâÁöÑÂΩ±ÂìçÔºåËØ•Ê®°ÂûãÈááÁî®Â§öÁ§∫‰æãÂ≠¶‰π†ÁõÆÊ†áËøõË°åËÆ≠ÁªÉÔºåËØ•ÁõÆÊ†áÂä®ÊÄÅÂú∞‰∏∫ÊØè‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÈÄâÊã©Êü•ËØ¢ÁöÑÊúÄ‰Ω≥ÁâàÊú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êó∂Â∫èËßÜÈ¢ëÂÆö‰ΩçÂíåÊÆµËêΩÂÆö‰ΩçÁöÑÂêÑÁßçÂü∫ÂáÜÊµãËØï‰∏≠ÈÉΩÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊòæËëó‰ºò‰∫éÊâÄÊúâÂÖàÂâçÊèêÂá∫ÁöÑÂü∫‰∫éLLMÁöÑÊó∂Â∫èÂÆö‰ΩçÊñπÊ≥ïÔºåÂπ∂‰∏î‰ºò‰∫éÊàñÂèØ‰∏é‰∏ìÁî®Ê®°ÂûãÁõ∏Â™≤ÁæéÔºåÂêåÊó∂Âú®Èõ∂Ê†∑Êú¨ËØÑ‰º∞Âú∫ÊôØ‰∏≠‰øùÊåÅ‰∫ÜÊòéÊòæÁöÑ‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÊó®Âú®Ê†πÊçÆÁªôÂÆöÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÊü•ËØ¢ÔºåÂú®ËßÜÈ¢ë‰∏≠ÊâæÂà∞ÂØπÂ∫îÁöÑËµ∑ÂßãÂíåÁªìÊùüÊó∂Èó¥ÁÇπ„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÊü•ËØ¢„ÄÅÂà©Áî®‰∏ä‰∏ãÊñá‰ø°ÊÅØ‰ª•ÂèäÂáèËΩªÂô™Â£∞ÂíåÂπªËßâÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåÂ¶Ç‰ΩïÊúâÊïàËûçÂêàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂπ∂ÈÅøÂÖçÁîüÊàê‰∏çÂáÜÁ°ÆÁöÑÂÆö‰ΩçÁªìÊûúÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöED-VTGÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈ¶ñÂÖàÂà©Áî®Â§öÊ®°ÊÄÅLLMÂØπÂéüÂßãÊü•ËØ¢ËøõË°å‚ÄúÂØåÈõÜ‚ÄùÔºåÂç≥Ë°•ÂÖÖÁº∫Â§±ÁöÑÁªÜËäÇÂíåÁ∫øÁ¥¢Ôºå‰ΩøÂÖ∂Êõ¥Êòì‰∫éÂÆö‰Ωç„ÄÇÁÑ∂ÂêéÔºå‰ΩøÁî®‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑËß£Á†ÅÂô®ÔºåÂü∫‰∫éÂØåÈõÜÂêéÁöÑÊü•ËØ¢Ë°®Á§∫ÔºåÈ¢ÑÊµãËßÜÈ¢ëÁâáÊÆµÁöÑËµ∑ÂßãÂíåÁªìÊùüÊó∂Èó¥„ÄÇËøôÁßç‰∏§Èò∂ÊÆµÁöÑÊñπÊ≥ïÊó®Âú®Ëß£ËÄ¶Êü•ËØ¢ÁêÜËß£ÂíåÊó∂Â∫èÂÆö‰ΩçÔºå‰ªéËÄåÊèêÈ´òÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöED-VTGÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÊü•ËØ¢ÂØåÈõÜÂíåÊó∂Â∫èÂÆö‰Ωç„ÄÇÂú®Êü•ËØ¢ÂØåÈõÜÈò∂ÊÆµÔºå‰ΩøÁî®Â§öÊ®°ÊÄÅLLMÔºà‰æãÂ¶ÇÔºåLLaVAÔºâËÅîÂêàÂ§ÑÁêÜÊñáÊú¨Êü•ËØ¢ÂíåËßÜÈ¢ëÂ∏ßÔºåÁîüÊàêÂåÖÂê´Êõ¥Â§ö‰∏ä‰∏ãÊñá‰ø°ÊÅØÁöÑÂ¢ûÂº∫Êü•ËØ¢„ÄÇÂú®Êó∂Â∫èÂÆö‰ΩçÈò∂ÊÆµÔºå‰ΩøÁî®‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑËß£Á†ÅÂô®Ôºà‰æãÂ¶ÇÔºå‰∏Ä‰∏™ÁÆÄÂçïÁöÑÁ∫øÊÄßÂ±ÇÊàñTransformerËß£Á†ÅÂô®ÔºâÔºåÂü∫‰∫éÂ¢ûÂº∫Êü•ËØ¢ÁöÑË°®Á§∫ÔºåÈ¢ÑÊµãËßÜÈ¢ëÁâáÊÆµÁöÑËµ∑ÂßãÂíåÁªìÊùüÊó∂Èó¥„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÈááÁî®Á´ØÂà∞Á´ØÁöÑÊñπÂºèËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöED-VTGÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî®Â§öÊ®°ÊÄÅLLMËøõË°åÊü•ËØ¢ÂØåÈõÜÔºå‰ªéËÄåÊúâÊïàÂú∞Âà©Áî®‰∫ÜËßÜÈ¢ëÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂπ∂ÁºìËß£‰∫ÜÂéüÂßãÊü•ËØ¢ÁöÑÊ≠ß‰πâÊÄß„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®Â§öÁ§∫‰æãÂ≠¶‰π†ÁõÆÊ†áÔºåÂä®ÊÄÅÈÄâÊã©ÊØè‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊúÄ‰Ω≥Êü•ËØ¢ÁâàÊú¨Ôºå‰ªéËÄåÂáèËΩª‰∫ÜÂô™Â£∞ÂíåÂπªËßâÁöÑÂΩ±Âìç„ÄÇ‰∏éÁõ¥Êé•‰ΩøÁî®LLMËøõË°åÊó∂Â∫èÂÆö‰ΩçÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåED-VTGÈÄöËøáËß£ËÄ¶Êü•ËØ¢ÁêÜËß£ÂíåÊó∂Â∫èÂÆö‰ΩçÔºåÊèêÈ´ò‰∫ÜÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Êü•ËØ¢ÂØåÈõÜÈò∂ÊÆµÔºåËæìÂÖ•ÂåÖÊã¨ÂéüÂßãÊñáÊú¨Êü•ËØ¢ÂíåËßÜÈ¢ëÂ∏ßÔºà‰æãÂ¶ÇÔºåÂÖ≥ÈîÆÂ∏ßÊàñÂùáÂåÄÈááÊ†∑ÁöÑÂ∏ßÔºâ„ÄÇÂ§öÊ®°ÊÄÅLLMÁöÑËæìÂá∫ÊòØÂ¢ûÂº∫ÂêéÁöÑÊñáÊú¨Êü•ËØ¢„ÄÇÂú®Êó∂Â∫èÂÆö‰ΩçÈò∂ÊÆµÔºåËß£Á†ÅÂô®ÁöÑËæìÂÖ•ÊòØÂ¢ûÂº∫Êü•ËØ¢ÁöÑË°®Á§∫Ôºà‰æãÂ¶ÇÔºåÈÄöËøáTransformerÁºñÁ†ÅÂô®ÊèêÂèñÁöÑÁâπÂæÅÔºâÔºåËæìÂá∫ÊòØËßÜÈ¢ëÁâáÊÆµÁöÑËµ∑ÂßãÂíåÁªìÊùüÊó∂Èó¥„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®Â§öÁ§∫‰æãÂ≠¶‰π†ÁõÆÊ†áÔºåÂç≥ÂØπÂ§ö‰∏™Â¢ûÂº∫Êü•ËØ¢ËøõË°åÈ¢ÑÊµãÔºåÂπ∂ÈÄâÊã©ÊçüÂ§±ÊúÄÂ∞èÁöÑÊü•ËØ¢ËøõË°åÂèçÂêë‰º†Êí≠„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂåÖÊã¨LLMÁöÑÈÄâÊã©„ÄÅËß£Á†ÅÂô®ÁöÑÁªìÊûÑ„ÄÅ‰ª•ÂèäÂ§öÁ§∫‰æãÂ≠¶‰π†ÁöÑÈááÊ†∑Á≠ñÁï•Á≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ED-VTGÂú®Â§ö‰∏™ËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÂü∫‰∫éLLMÁöÑÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õÊï∞ÊçÆÈõÜ‰∏äÔºåED-VTGÁöÑÊÄßËÉΩÊèêÂçáË∂ÖËøá‰∫Ü10%„ÄÇÊ≠§Â§ñÔºåED-VTGÂú®Èõ∂Ê†∑Êú¨ËØÑ‰º∞‰∏≠‰πüË°®Áé∞Âá∫Ëâ≤ÔºåË°®ÊòéÂÖ∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåED-VTGËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåÂπ∂ÂáèËΩªÂô™Â£∞ÂíåÂπªËßâÁöÑÂΩ±Âìç„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËßÜÈ¢ëÊ£ÄÁ¥¢„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅËßÜÈ¢ëÁºñËæëÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Âø´ÈÄüÂÆö‰ΩçËßÜÈ¢ë‰∏≠ÁöÑÁâπÂÆö‰∫ã‰ª∂ÔºåÁõëÊéßÁ≥ªÁªüÂèØ‰ª•Ëá™Âä®Ê£ÄÊµãÂºÇÂ∏∏Ë°å‰∏∫ÔºåËßÜÈ¢ëÁºñËæë‰∫∫ÂëòÂèØ‰ª•Ê†πÊçÆÊñáÊú¨ÊèèËø∞Á≤æÁ°ÆÂâ™ËæëËßÜÈ¢ëÁâáÊÆµ„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫∫Êú∫‰∫§‰∫íÁöÑÊïàÁéáÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios.

