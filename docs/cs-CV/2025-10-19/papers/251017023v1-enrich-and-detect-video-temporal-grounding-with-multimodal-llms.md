---
layout: default
title: "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs"
---

# Enrich and Detect: Video Temporal Grounding with Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17023" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17023v1</a>
  <a href="https://arxiv.org/pdf/2510.17023.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17023v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17023v1', 'Enrich and Detect: Video Temporal Grounding with Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shraman Pramanick, Effrosyni Mavroudi, Yale Song, Rama Chellappa, Lorenzo Torresani, Triantafyllos Afouras

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-19

**å¤‡æ³¨**: ICCV 2025 (Highlights)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºED-VTGï¼Œåˆ©ç”¨å¤šæ¨¡æ€LLMè¿›è¡Œç»†ç²’åº¦è§†é¢‘æ—¶åºå®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€LLM` `æŸ¥è¯¢å¯Œé›†` `å¤šç¤ºä¾‹å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ—¶åºå®šä½æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯ï¼Œä¸”æ˜“å—å™ªå£°å’Œå¹»è§‰å½±å“ã€‚
2. ED-VTGé€šè¿‡å¤šæ¨¡æ€LLMå¢å¼ºæŸ¥è¯¢ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§è§£ç å™¨è¿›è¡Œç²¾ç¡®å®šä½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒED-VTGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAï¼Œå¹¶åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºED-VTGçš„æ–¹æ³•ï¼Œç”¨äºåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç»†ç²’åº¦è§†é¢‘æ—¶åºå®šä½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€LLMçš„èƒ½åŠ›ï¼Œè”åˆå¤„ç†æ–‡æœ¬å’Œè§†é¢‘ï¼Œé€šè¿‡ä¸€ä¸ªä¸¤é˜¶æ®µè¿‡ç¨‹æœ‰æ•ˆåœ°åœ¨è§†é¢‘ä¸­å®šä½è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚é¦–å…ˆï¼Œè¯­è¨€æŸ¥è¯¢è¢«è½¬æ¢ä¸ºå¯Œå«ä¿¡æ¯çš„å¥å­ï¼Œå…¶ä¸­åŒ…å«äº†ç¼ºå¤±çš„ç»†èŠ‚å’Œçº¿ç´¢ï¼Œä»¥è¾…åŠ©å®šä½ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨ä¸€ä¸ªè½»é‡çº§è§£ç å™¨å¯¹è¿™äº›å¯Œå«ä¿¡æ¯çš„æŸ¥è¯¢è¿›è¡Œå®šä½ï¼Œè¯¥è§£ç å™¨ä¸“é—¨ç”¨äºé¢„æµ‹åŸºäºå¯Œå«ä¿¡æ¯æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„ç²¾ç¡®è¾¹ç•Œã€‚ä¸ºäº†å‡è½»å™ªå£°å¹¶å‡å°‘å¹»è§‰çš„å½±å“ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å¤šç¤ºä¾‹å­¦ä¹ ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œè¯¥ç›®æ ‡åŠ¨æ€åœ°ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬é€‰æ‹©æŸ¥è¯¢çš„æœ€ä½³ç‰ˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ—¶åºè§†é¢‘å®šä½å’Œæ®µè½å®šä½çš„å„ç§åŸºå‡†æµ‹è¯•ä¸­éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºæ‰€æœ‰å…ˆå‰æå‡ºçš„åŸºäºLLMçš„æ—¶åºå®šä½æ–¹æ³•ï¼Œå¹¶ä¸”ä¼˜äºæˆ–å¯ä¸ä¸“ç”¨æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶åœ¨é›¶æ ·æœ¬è¯„ä¼°åœºæ™¯ä¸­ä¿æŒäº†æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘æ—¶åºå®šä½æ—¨åœ¨æ ¹æ®ç»™å®šçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œåœ¨è§†é¢‘ä¸­æ‰¾åˆ°å¯¹åº”çš„èµ·å§‹å’Œç»“æŸæ—¶é—´ç‚¹ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢ã€åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥åŠå‡è½»å™ªå£°å’Œå¹»è§‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¦‚ä½•æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯å¹¶é¿å…ç”Ÿæˆä¸å‡†ç¡®çš„å®šä½ç»“æœæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šED-VTGçš„æ ¸å¿ƒæ€è·¯æ˜¯é¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€LLMå¯¹åŸå§‹æŸ¥è¯¢è¿›è¡Œâ€œå¯Œé›†â€ï¼Œå³è¡¥å……ç¼ºå¤±çš„ç»†èŠ‚å’Œçº¿ç´¢ï¼Œä½¿å…¶æ›´æ˜“äºå®šä½ã€‚ç„¶åï¼Œä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„è§£ç å™¨ï¼ŒåŸºäºå¯Œé›†åçš„æŸ¥è¯¢è¡¨ç¤ºï¼Œé¢„æµ‹è§†é¢‘ç‰‡æ®µçš„èµ·å§‹å’Œç»“æŸæ—¶é—´ã€‚è¿™ç§ä¸¤é˜¶æ®µçš„æ–¹æ³•æ—¨åœ¨è§£è€¦æŸ¥è¯¢ç†è§£å’Œæ—¶åºå®šä½ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šED-VTGåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæŸ¥è¯¢å¯Œé›†å’Œæ—¶åºå®šä½ã€‚åœ¨æŸ¥è¯¢å¯Œé›†é˜¶æ®µï¼Œä½¿ç”¨å¤šæ¨¡æ€LLMï¼ˆä¾‹å¦‚ï¼ŒLLaVAï¼‰è”åˆå¤„ç†æ–‡æœ¬æŸ¥è¯¢å’Œè§†é¢‘å¸§ï¼Œç”ŸæˆåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¢å¼ºæŸ¥è¯¢ã€‚åœ¨æ—¶åºå®šä½é˜¶æ®µï¼Œä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„è§£ç å™¨ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚æˆ–Transformerè§£ç å™¨ï¼‰ï¼ŒåŸºäºå¢å¼ºæŸ¥è¯¢çš„è¡¨ç¤ºï¼Œé¢„æµ‹è§†é¢‘ç‰‡æ®µçš„èµ·å§‹å’Œç»“æŸæ—¶é—´ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šED-VTGçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨å¤šæ¨¡æ€LLMè¿›è¡ŒæŸ¥è¯¢å¯Œé›†ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†è§†é¢‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç¼“è§£äº†åŸå§‹æŸ¥è¯¢çš„æ­§ä¹‰æ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¤šç¤ºä¾‹å­¦ä¹ ç›®æ ‡ï¼ŒåŠ¨æ€é€‰æ‹©æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æœ€ä½³æŸ¥è¯¢ç‰ˆæœ¬ï¼Œä»è€Œå‡è½»äº†å™ªå£°å’Œå¹»è§‰çš„å½±å“ã€‚ä¸ç›´æ¥ä½¿ç”¨LLMè¿›è¡Œæ—¶åºå®šä½çš„æ–¹æ³•ç›¸æ¯”ï¼ŒED-VTGé€šè¿‡è§£è€¦æŸ¥è¯¢ç†è§£å’Œæ—¶åºå®šä½ï¼Œæé«˜äº†æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŸ¥è¯¢å¯Œé›†é˜¶æ®µï¼Œè¾“å…¥åŒ…æ‹¬åŸå§‹æ–‡æœ¬æŸ¥è¯¢å’Œè§†é¢‘å¸§ï¼ˆä¾‹å¦‚ï¼Œå…³é”®å¸§æˆ–å‡åŒ€é‡‡æ ·çš„å¸§ï¼‰ã€‚å¤šæ¨¡æ€LLMçš„è¾“å‡ºæ˜¯å¢å¼ºåçš„æ–‡æœ¬æŸ¥è¯¢ã€‚åœ¨æ—¶åºå®šä½é˜¶æ®µï¼Œè§£ç å™¨çš„è¾“å…¥æ˜¯å¢å¼ºæŸ¥è¯¢çš„è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡Transformerç¼–ç å™¨æå–çš„ç‰¹å¾ï¼‰ï¼Œè¾“å‡ºæ˜¯è§†é¢‘ç‰‡æ®µçš„èµ·å§‹å’Œç»“æŸæ—¶é—´ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨å¤šç¤ºä¾‹å­¦ä¹ ç›®æ ‡ï¼Œå³å¯¹å¤šä¸ªå¢å¼ºæŸ¥è¯¢è¿›è¡Œé¢„æµ‹ï¼Œå¹¶é€‰æ‹©æŸå¤±æœ€å°çš„æŸ¥è¯¢è¿›è¡Œåå‘ä¼ æ’­ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®åŒ…æ‹¬LLMçš„é€‰æ‹©ã€è§£ç å™¨çš„ç»“æ„ã€ä»¥åŠå¤šç¤ºä¾‹å­¦ä¹ çš„é‡‡æ ·ç­–ç•¥ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ED-VTGåœ¨å¤šä¸ªè§†é¢‘æ—¶åºå®šä½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºLLMçš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒED-VTGçš„æ€§èƒ½æå‡è¶…è¿‡äº†10%ã€‚æ­¤å¤–ï¼ŒED-VTGåœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒED-VTGèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶å‡è½»å™ªå£°å’Œå¹»è§‰çš„å½±å“ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘æ£€ç´¢ã€æ™ºèƒ½ç›‘æ§ã€è§†é¢‘ç¼–è¾‘ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å¿«é€Ÿå®šä½è§†é¢‘ä¸­çš„ç‰¹å®šäº‹ä»¶ï¼Œç›‘æ§ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸è¡Œä¸ºï¼Œè§†é¢‘ç¼–è¾‘äººå‘˜å¯ä»¥æ ¹æ®æ–‡æœ¬æè¿°ç²¾ç¡®å‰ªè¾‘è§†é¢‘ç‰‡æ®µã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æå‡äººæœºäº¤äº’çš„æ•ˆç‡å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios.

