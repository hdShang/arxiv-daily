---
layout: default
title: Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback
---

# Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16888" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16888v3</a>
  <a href="https://arxiv.org/pdf/2510.16888.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16888v3" onclick="toggleFavorite(this, '2510.16888v3', 'Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Feize Wu, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, Shaodong Wang, Xinhua Cheng, Li Yuan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-19 (æ›´æ–°: 2025-11-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Uniworld-V2ï¼šåˆ©ç”¨æ‰©æ•£è´Ÿæ„ŸçŸ¥å¾®è°ƒå’ŒMLLMéšå¼åé¦ˆå¢å¼ºå›¾åƒç¼–è¾‘èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒç¼–è¾‘` `æ‰©æ•£æ¨¡å‹` `ç­–ç•¥ä¼˜åŒ–` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¥–åŠ±æ¨¡å‹` `æµåŒ¹é…` `è´Ÿæ„ŸçŸ¥å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŒ‡ä»¤é©±åŠ¨å›¾åƒç¼–è¾‘æ¨¡å‹æ˜“äºè¿‡æ‹Ÿåˆæ ‡æ³¨æ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ï¼Œéš¾ä»¥æ¢ç´¢è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„ç¼–è¾‘ã€‚
2. æå‡ºEdit-R1æ¡†æ¶ï¼Œæ ¸å¿ƒæ˜¯DiffusionNFTå’ŒMLLMéšå¼åé¦ˆï¼Œå‰è€…æå‡è®­ç»ƒæ•ˆç‡ï¼Œåè€…æä¾›ç»†ç²’åº¦å¥–åŠ±ä¿¡å·ã€‚
3. UniWorld-V2åœ¨ImgEditå’ŒGEdit-Benchä¸Šè¾¾åˆ°SOTAï¼Œä¸”æ¡†æ¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯æå‡å¤šç§åŸºç¡€æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºEdit-R1ï¼Œä¸€ç§åŸºäºç­–ç•¥ä¼˜åŒ–çš„æŒ‡ä»¤é©±åŠ¨å›¾åƒç¼–è¾‘åè®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£è´Ÿæ„ŸçŸ¥å¾®è°ƒ(DiffusionNFT)ï¼Œè¿™æ˜¯ä¸€ç§ä¸æµåŒ¹é…å‰å‘è¿‡ç¨‹ä¸€è‡´çš„æ— ä¼¼ç„¶ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨æ›´é«˜é˜¶çš„é‡‡æ ·å™¨å’Œæ›´é«˜æ•ˆçš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œç”±äºç¼–è¾‘æŒ‡ä»¤å’Œä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œç¼ºä¹é€šç”¨çš„å¥–åŠ±æ¨¡å‹ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæœ¬æ–‡é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)ä½œä¸ºç»Ÿä¸€çš„ã€å…è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨å…¶è¾“å‡ºlogitsæä¾›ç»†ç²’åº¦çš„åé¦ˆã€‚æ­¤å¤–ï¼Œç²¾å¿ƒè®¾è®¡äº†ä¸€ç§ä½æ–¹å·®çš„ç¾¤ä½“è¿‡æ»¤æœºåˆ¶ï¼Œä»¥å‡å°‘MLLMè¯„åˆ†å™ªå£°å¹¶ç¨³å®šä¼˜åŒ–ã€‚ä½¿ç”¨è¯¥æ¡†æ¶è®­ç»ƒçš„UniWorld-V2åœ¨ImgEditå’ŒGEdit-BenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåˆ†åˆ«è·å¾—äº†4.49å’Œ7.83åˆ†ã€‚é‡è¦çš„æ˜¯ï¼Œè¯¥æ¡†æ¶æ˜¯æ¨¡å‹æ— å…³çš„ï¼Œå½“åº”ç”¨äºQwen-Image-Editå’ŒFLUX-Kontextç­‰ä¸åŒçš„åŸºç¡€æ¨¡å‹æ—¶ï¼Œå¯æä¾›æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç¼–è¾‘ä»»åŠ¡æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æŒ‡ä»¤ä¿®æ”¹å›¾åƒã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒï¼Œä½†å®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•å¤„ç†æœªè§è¿‡çš„ç¼–è¾‘æŒ‡ä»¤æˆ–å›¾åƒã€‚æ­¤å¤–ï¼Œç¼ºä¹æœ‰æ•ˆçš„å¥–åŠ±æœºåˆ¶æ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤æ‚çš„ç¼–è¾‘ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¥–åŠ±ä¿¡å·å¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´å¥½çš„ç¼–è¾‘ç­–ç•¥ã€‚å…·ä½“è€Œè¨€ï¼Œé‡‡ç”¨DiffusionNFTæ–¹æ³•è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ–¹æ³•ä¸æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸€è‡´ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚åŒæ—¶ï¼Œåˆ©ç”¨MLLMä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æä¾›ç»†ç²’åº¦çš„åé¦ˆä¿¡å·ï¼Œä»è€Œå…‹æœäº†ç¼ºä¹é€šç”¨å¥–åŠ±æ¨¡å‹çš„éš¾é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEdit-R1æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼›2) ä½¿ç”¨DiffusionNFTæ–¹æ³•å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä¼˜åŒ–ç¼–è¾‘ç­–ç•¥ï¼›3) ä½¿ç”¨MLLMä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œè¯„ä¼°ç¼–è¾‘ç»“æœçš„è´¨é‡ï¼Œå¹¶æä¾›åé¦ˆä¿¡å·ï¼›4) ä½¿ç”¨ä½æ–¹å·®çš„ç¾¤ä½“è¿‡æ»¤æœºåˆ¶ï¼Œå‡å°‘MLLMè¯„åˆ†å™ªå£°ï¼Œç¨³å®šä¼˜åŒ–è¿‡ç¨‹ã€‚æ•´ä¸ªæ¡†æ¶æ˜¯ä¸€ä¸ªåè®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„åŸºç¡€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†DiffusionNFTæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸€è‡´ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼›2) åˆ©ç”¨MLLMä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æä¾›ç»†ç²’åº¦çš„åé¦ˆä¿¡å·ï¼›3) è®¾è®¡äº†ä½æ–¹å·®çš„ç¾¤ä½“è¿‡æ»¤æœºåˆ¶ï¼Œå‡å°‘MLLMè¯„åˆ†å™ªå£°ï¼Œç¨³å®šä¼˜åŒ–è¿‡ç¨‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç¼–è¾‘è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šDiffusionNFTæ–¹æ³•åŸºäºæµåŒ¹é…ç†è®ºï¼Œé€šè¿‡ä¼˜åŒ–ä¸€ä¸ªç­–ç•¥ç½‘ç»œæ¥åŒ¹é…æ‰©æ•£æ¨¡å‹çš„å™ªå£°åˆ†å¸ƒã€‚MLLMå¥–åŠ±æ¨¡å‹ä½¿ç”¨å…¶è¾“å‡ºlogitsä½œä¸ºåé¦ˆä¿¡å·ï¼Œlogitså¯ä»¥æä¾›æ›´ç»†ç²’åº¦çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥åŒºåˆ†ä¸åŒçš„ç¼–è¾‘å±æ€§ã€‚ç¾¤ä½“è¿‡æ»¤æœºåˆ¶é€šè¿‡å¯¹å¤šä¸ªç¼–è¾‘ç»“æœè¿›è¡Œè¯„åˆ†ï¼Œå¹¶å–å¹³å‡å€¼æ¥å‡å°‘MLLMè¯„åˆ†å™ªå£°ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UniWorld-V2åœ¨ImgEditå’ŒGEdit-BenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†state-of-the-artçš„ç»“æœï¼Œåˆ†åˆ«è·å¾—äº†4.49å’Œ7.83åˆ†ã€‚è¯¥æ¡†æ¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå½“åº”ç”¨äºQwen-Image-Editå’ŒFLUX-Kontextç­‰ä¸åŒçš„åŸºç¡€æ¨¡å‹æ—¶ï¼Œå¯æä¾›æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç¼–è¾‘è´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå›¾åƒç¼–è¾‘ã€å†…å®¹åˆ›ä½œã€è‰ºæœ¯è®¾è®¡ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æ–‡æœ¬æŒ‡ä»¤å¿«é€Ÿä¿®æ”¹å›¾åƒï¼Œç”Ÿæˆä¸ªæ€§åŒ–çš„å†…å®¹ã€‚è¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºè‡ªåŠ¨åŒ–å›¾åƒå¤„ç†æµç¨‹ï¼Œæé«˜æ•ˆç‡å’Œè´¨é‡ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å›¾åƒç¼–è¾‘æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´ä¾¿æ·çš„å›¾åƒå¤„ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Instruction-based image editing has achieved remarkable progress; however, models solely trained via supervised fine-tuning often overfit to annotated patterns, hindering their ability to explore and generalize beyond training distributions. To this end, we introduce Edit-R1, a novel post-training framework for instruction-based image editing based on policy optimization. Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a likelihood-free policy optimization method consistent with the flow matching forward process, thereby enabling the use of higher-order samplers and more efficient training. Another key challenge here is the absence of a universal reward model, resulting from the diverse nature of editing instructions and tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM) as a unified, training-free reward model, leveraging its output logits to provide fine-grained feedback. Furthermore, we carefully design a low-variance group filtering mechanism to reduce MLLM scoring noise and stabilize optimization. \texttt{UniWorld-V2}, trained with this framework, achieves \textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks, scoring 4.49 and 7.83, respectively. Crucially, our framework is model-agnostic, delivering substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its wide applicability. Code and models are publicly available to support further research.

