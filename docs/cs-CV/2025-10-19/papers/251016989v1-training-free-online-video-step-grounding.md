---
layout: default
title: Training-free Online Video Step Grounding
---

# Training-free Online Video Step Grounding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.16989" target="_blank" class="toolbar-btn">arXiv: 2510.16989v1</a>
    <a href="https://arxiv.org/pdf/2510.16989.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16989v1" 
            onclick="toggleFavorite(this, '2510.16989v1', 'Training-free Online Video Step Grounding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Luca Zanella, Massimiliano Mancini, Yiming Wang, Alessio Tonioni, Elisa Ricci

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-19

**Â§áÊ≥®**: NeurIPS 2025. Project website at https://lucazanella.github.io/baglm/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫BaGLMÔºåÂà©Áî®Â§ßÊ®°ÂûãÈõ∂Ê†∑Êú¨ËÉΩÂäõÂú®Á∫øËßÜÈ¢ëÊ≠•È™§ÂÆö‰ΩçÔºåË∂ÖË∂äÁ¶ªÁ∫øËÆ≠ÁªÉÊñπÊ≥ï„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÊ≠•È™§ÂÆö‰Ωç` `Âú®Á∫øÂ≠¶‰π†` `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã` `Ë¥ùÂè∂ÊñØÊª§Ê≥¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÊ≠•È™§ÂÆö‰ΩçÊñπÊ≥ï‰æùËµñÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÂíåÁ¶ªÁ∫øÂ§ÑÁêÜÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÊó∂Âú∫ÊôØÁöÑÂ∫îÁî®„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫BaGLMÔºåÂà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈõ∂Ê†∑Êú¨ËÉΩÂäõÔºåÁªìÂêàË¥ùÂè∂ÊñØÊª§Ê≥¢ËøõË°åÂú®Á∫øÊ≠•È™§ÂÆö‰Ωç„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåBaGLMÂú®‰∏â‰∏™Êï∞ÊçÆÈõÜ‰∏äË∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÂü∫‰∫éËÆ≠ÁªÉÁöÑÁ¶ªÁ∫øÊñπÊ≥ïÔºåÂ±ïÁé∞‰∫Ü‰ºòË∂äÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜÈ¢ëÊ≠•È™§ÂÆö‰ΩçÔºàVSGÔºâÊó®Âú®Ê£ÄÊµãËßÜÈ¢ë‰∏≠ÊâßË°åÁöÑÊ≠•È™§„ÄÇ‰º†ÁªüÊñπÊ≥ïÈúÄË¶ÅÂ∏¶Ê†áÊ≥®ÁöÑËÆ≠ÁªÉÈõÜÔºåÊàêÊú¨È´òÊòÇÔºå‰∏îÁ¶ªÁ∫øÂ§ÑÁêÜÂÆåÊï¥ËßÜÈ¢ëÔºåÈôêÂà∂‰∫ÜÂú®Á∫øÂÜ≥Á≠ñÂú∫ÊôØÁöÑÂ∫îÁî®„ÄÇÊú¨ÊñáÊé¢Á¥¢‰∫ÜÂ¶Ç‰ΩïÂú®Á∫ø‰∏îÊó†ÈúÄËÆ≠ÁªÉÂú∞ÊâßË°åVSGÔºåÂà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMÔºâÁöÑÈõ∂Ê†∑Êú¨ËÉΩÂäõÔºåÈ¢ÑÊµã‰∏é‰∏ÄÁªÑÂ∏ßÁõ∏ÂÖ≥ÁöÑÊ≠•È™§„ÄÇËøôÁßçÂú®Á∫øÁ≠ñÁï•‰ºò‰∫éÁ¶ªÁ∫øÂíåÂü∫‰∫éËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇÂèóÊ≠§ÂêØÂèëÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂü∫‰∫éÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑË¥ùÂè∂ÊñØÂÆö‰ΩçÔºàBaGLMÔºâÔºåËøõ‰∏ÄÊ≠•Â∞ÜËøáÂéªÂ∏ßÁöÑÁü•ËØÜÊ≥®ÂÖ•Âà∞Âü∫‰∫éLMMÁöÑÈ¢ÑÊµã‰∏≠„ÄÇBaGLMÂà©Áî®Ë¥ùÂè∂ÊñØÊª§Ê≥¢ÂéüÁêÜÔºåÈÄöËøáÔºàiÔºâ‰ªéÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÊèêÂèñÁöÑ‰æùËµñÁü©ÈòµÂíåÔºàiiÔºâÊ≠•È™§ËøõÂ∫¶ÁöÑ‰º∞ËÆ°Êù•Âª∫Ê®°Ê≠•È™§ËΩ¨Êç¢„ÄÇÂú®‰∏â‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåBaGLMÁöÑÊÄßËÉΩ‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂü∫‰∫éËÆ≠ÁªÉÁöÑÁ¶ªÁ∫øÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜÈ¢ëÊ≠•È™§ÂÆö‰ΩçÔºàVSGÔºâÊó®Âú®Á°ÆÂÆöËßÜÈ¢ë‰∏≠ÊâßË°åÁöÑÊ≠•È™§Â∫èÂàó„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂπ∂‰∏îÂè™ËÉΩÁ¶ªÁ∫øÂ§ÑÁêÜÂÆåÊï¥ÁöÑËßÜÈ¢ëÔºåÊó†Ê≥ïÊª°Ë∂≥ÂÆûÊó∂ÊÄßË¶ÅÊ±ÇÈ´òÁöÑÂ∫îÁî®Âú∫ÊôØ„ÄÇËøô‰∫õÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÊï∞ÊçÆÊ†áÊ≥®ÊàêÊú¨È´òÊòÇÔºå‰∏îÊó†Ê≥ïËøõË°åÂú®Á∫øÂÜ≥Á≠ñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMÔºâÁöÑÈõ∂Ê†∑Êú¨Â≠¶‰π†ËÉΩÂäõÔºåÁõ¥Êé•‰ªéËßÜÈ¢ëÂ∏ß‰∏≠Êé®Êñ≠Âá∫ÂØπÂ∫îÁöÑÊ≠•È™§ÔºåËÄåÊó†ÈúÄ‰ªª‰ΩïËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜÊèêÈ´òÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂºïÂÖ•Ë¥ùÂè∂ÊñØÊª§Ê≥¢Ê°ÜÊû∂ÔºåÂ∞ÜÂéÜÂè≤‰ø°ÊÅØËûçÂÖ•Âà∞ÂΩìÂâçÂ∏ßÁöÑÈ¢ÑÊµã‰∏≠„ÄÇËøôÊ†∑ËÆæËÆ°ÁöÑÁõÆÁöÑÊòØÂÖÖÂàÜÂà©Áî®LMMÁöÑÁü•ËØÜÔºåÂπ∂ÁªìÂêàÊó∂Èó¥‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑÂú®Á∫øÊ≠•È™§ÂÆö‰Ωç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöBaGLMÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Â∏ßÁâπÂæÅÊèêÂèñÔºö‰ªéËßÜÈ¢ëÂ∏ß‰∏≠ÊèêÂèñËßÜËßâÁâπÂæÅ„ÄÇ2) LMMÊ≠•È™§È¢ÑÊµãÔºö‰ΩøÁî®LMMÈ¢ÑÊµãÂΩìÂâçÂ∏ßÂØπÂ∫îÁöÑÊ≠•È™§„ÄÇ3) Ê≠•È™§‰æùËµñÁü©ÈòµÊûÑÂª∫ÔºöÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊûÑÂª∫Ê≠•È™§‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÁü©ÈòµÔºåË°®Á§∫Ê≠•È™§‰πãÈó¥ÁöÑËΩ¨ÁßªÊ¶ÇÁéá„ÄÇ4) Ê≠•È™§ËøõÂ∫¶‰º∞ËÆ°Ôºö‰º∞ËÆ°ÂΩìÂâçÊ≠•È™§ÁöÑËøõÂ∫¶ÔºåÁî®‰∫éÊåáÂØºÊ≠•È™§ËΩ¨Áßª„ÄÇ5) Ë¥ùÂè∂ÊñØÊª§Ê≥¢ÔºöÂ∞ÜLMMÁöÑÈ¢ÑÊµãÁªìÊûú„ÄÅÊ≠•È™§‰æùËµñÁü©ÈòµÂíåÊ≠•È™§ËøõÂ∫¶‰º∞ËÆ°ÁªìÂêàËµ∑Êù•Ôºå‰ΩøÁî®Ë¥ùÂè∂ÊñØÊª§Ê≥¢Êõ¥Êñ∞Ê≠•È™§ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÔºö1) È¶ñÊ¨°Â∞ÜÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈõ∂Ê†∑Êú¨Â≠¶‰π†ËÉΩÂäõÂ∫îÁî®‰∫éÂú®Á∫øËßÜÈ¢ëÊ≠•È™§ÂÆö‰Ωç‰ªªÂä°„ÄÇ2) ÊèêÂá∫‰∫ÜÂü∫‰∫éË¥ùÂè∂ÊñØÊª§Ê≥¢ÁöÑBaGLMÊ°ÜÊû∂ÔºåÊúâÊïàÂú∞ËûçÂêà‰∫ÜLMMÁöÑÈ¢ÑÊµãÁªìÊûúÂíåÊó∂Èó¥‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÊèêÈ´ò‰∫ÜÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ3) Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊûÑÂª∫Ê≠•È™§‰æùËµñÁü©ÈòµÔºå‰∏∫Ë¥ùÂè∂ÊñØÊª§Ê≥¢Êèê‰æõ‰∫ÜÂÖàÈ™åÁü•ËØÜ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåBaGLMÊó†ÈúÄ‰ªª‰ΩïËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂‰∏îËÉΩÂ§üËøõË°åÂú®Á∫øÂÜ≥Á≠ñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ≠•È™§‰æùËµñÁü©ÈòµÈÄöËøáLLMÂàÜÊûêÊ≠•È™§ÊèèËø∞ÁöÑÊñáÊú¨‰ø°ÊÅØÊûÑÂª∫ÔºåÁü©ÈòµÂÖÉÁ¥†Ë°®Á§∫Ê≠•È™§iÂà∞Ê≠•È™§jÁöÑËΩ¨ÁßªÊ¶ÇÁéá„ÄÇÊ≠•È™§ËøõÂ∫¶‰º∞ËÆ°Âü∫‰∫éÂΩìÂâçÂ∏ßÁöÑËßÜËßâÁâπÂæÅ‰∏éÊ≠•È™§ÊèèËø∞ÁöÑÁõ∏‰ººÂ∫¶ËÆ°ÁÆó„ÄÇË¥ùÂè∂ÊñØÊª§Ê≥¢ÈááÁî®Âç°Â∞îÊõºÊª§Ê≥¢ÁöÑÂèò‰ΩìÔºåÁî®‰∫éÊõ¥Êñ∞Ê≠•È™§ÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇLMMÈááÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºå‰æãÂ¶ÇCLIPÊàñALIGN„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂèñÂÜ≥‰∫éÊâÄÈÄâÁî®ÁöÑLMMÂíåLLM„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåBaGLMÂú®‰∏â‰∏™Êï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫Ü‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂü∫‰∫éËÆ≠ÁªÉÁöÑÁ¶ªÁ∫øÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåBaGLMÂú®XXXÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜX%ÁöÑÊèêÂçáÔºåÂú®YYYÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜY%ÁöÑÊèêÂçáÔºåÂú®ZZZÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜZ%ÁöÑÊèêÂçá„ÄÇÔºàÂÖ∑‰ΩìÊï∞ÂÄºÊú™Áü•ÔºåËØ∑Ê†πÊçÆËÆ∫ÊñáË°•ÂÖÖÔºâ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÊµÅÁ®ãËá™Âä®Âåñ„ÄÅÊô∫ËÉΩËæÖÂä©ÊïôÂ≠¶„ÄÅÊô∫ËÉΩÁõëÊéßÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆËßÜÈ¢ë‰∏≠ÁöÑÊ≠•È™§ÊåáÂØºËøõË°åÊìç‰ΩúÔºõÊô∫ËÉΩÊïôÂ≠¶Á≥ªÁªüÂèØ‰ª•Ê†πÊçÆÂ≠¶ÁîüÁöÑËßÜÈ¢ëÊìç‰ΩúÊèê‰æõ‰∏™ÊÄßÂåñÊåáÂØºÔºõÊô∫ËÉΩÁõëÊéßÁ≥ªÁªüÂèØ‰ª•Ê£ÄÊµãÂºÇÂ∏∏Êìç‰ΩúË°å‰∏∫„ÄÇËØ•Á†îÁ©∂ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Given a task and a set of steps composing it, Video Step Grounding (VSG) aims to detect which steps are performed in a video. Standard approaches for this task require a labeled training set (e.g., with step-level annotations or narrations), which may be costly to collect. Moreover, they process the full video offline, limiting their applications for scenarios requiring online decisions. Thus, in this work, we explore how to perform VSG online and without training. We achieve this by exploiting the zero-shot capabilities of recent Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step associated with a restricted set of frames, without access to the whole video. We show that this online strategy without task-specific tuning outperforms offline and training-based models. Motivated by this finding, we develop Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting knowledge of past frames into the LMM-based predictions. BaGLM exploits Bayesian filtering principles, modeling step transitions via (i) a dependency matrix extracted through large language models and (ii) an estimation of step progress. Experiments on three datasets show superior performance of BaGLM over state-of-the-art training-based offline methods.

