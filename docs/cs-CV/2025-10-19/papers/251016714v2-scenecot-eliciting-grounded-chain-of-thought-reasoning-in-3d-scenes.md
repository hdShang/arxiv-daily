---
layout: default
title: SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes
---

# SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16714" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16714v2</a>
  <a href="https://arxiv.org/pdf/2510.16714.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16714v2" onclick="toggleFavorite(this, '2510.16714v2', 'SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiongkun Linghu, Jiangyong Huang, Ziyu Zhu, Baoxiong Jia, Siyuan Huang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-19 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: Project page: https://scenecot.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SceneCOTï¼šæå‡º3Dåœºæ™¯ä¸­åŸºäºå¸¸è¯†é“¾çš„æ¨ç†æ¡†æ¶ï¼Œæå‡å…·èº«é—®ç­”æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `å¸¸è¯†é“¾æ¨ç†` `å…·èº«é—®ç­”` `å¤šæ¨¡æ€èåˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨äºº` `è§†è§‰æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3D LLMåœ¨å…·èº«é—®ç­”æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹å¯¹åœºæ™¯-å¯¹è±¡é—´ç±»äººæ¨ç†æœºåˆ¶çš„æœ‰æ•ˆåˆ©ç”¨ã€‚
2. è®ºæ–‡æå‡ºSCENECOTæ¡†æ¶ï¼Œé€šè¿‡å¸¸è¯†é“¾æ¨ç†å°†å¤æ‚ä»»åŠ¡åˆ†è§£ï¼Œå¹¶ç»“åˆå¤šæ¨¡æ€ä¸“å®¶æ¨¡å—æä¾›è§†è§‰çº¿ç´¢ã€‚
3. æ„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†SCENECOT-185Kï¼Œå®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨3Dåœºæ™¯æ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæå‡äº†å…·èº«é—®ç­”çš„ä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„3Då¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®ç°å…·èº«é—®ç­”æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå¯¹ç±»äººåœºæ™¯-å¯¹è±¡å…·èº«æ¨ç†æœºåˆ¶çš„æ¢ç´¢ä¸è¶³ã€‚æœ¬æ–‡é€šè¿‡æå‡ºä¸€ç§æ–°é¢–çš„æ¡†æ¶æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§3Dåœºæ™¯ä¸­åŸºäºå¸¸è¯†é“¾ï¼ˆChain-of-Thought, CoTï¼‰çš„å…·èº«æ¨ç†æ–¹æ³•ï¼ˆSCENECOTï¼‰ï¼Œå°†å¤æ‚çš„æ¨ç†ä»»åŠ¡åˆ†è§£ä¸ºæ›´ç®€å•ã€æ›´æ˜“äºç®¡ç†çš„é—®é¢˜ï¼Œå¹¶åŸºäºå¤šæ¨¡æ€ä¸“å®¶æ¨¡å—æ„å»ºç›¸åº”çš„è§†è§‰çº¿ç´¢ã€‚ä¸ºäº†æ”¯æŒè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†SCENECOT-185Kï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å…·èº«CoTæ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«185Kä¸ªé«˜è´¨é‡å®ä¾‹ã€‚åœ¨å„ç§å¤æ‚çš„3Dåœºæ™¯æ¨ç†åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–°æ¡†æ¶å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰é«˜åº¦çš„å…·èº«-é—®ç­”ä¸€è‡´æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯CoTæ¨ç†é¦–æ¬¡æˆåŠŸåº”ç”¨äº3Dåœºæ™¯ç†è§£ï¼Œå®ç°äº†é€æ­¥çš„ç±»äººæ¨ç†ï¼Œå¹¶æ˜¾ç¤ºå‡ºæ‰©å±•åˆ°æ›´å¹¿æ³›çš„3Dåœºæ™¯ç†è§£åœºæ™¯çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰3Dåœºæ™¯ç†è§£ä¸­çš„é—®ç­”ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯å…·èº«é—®ç­”ï¼Œé¢ä¸´ç€ç¼ºä¹æœ‰æ•ˆæ¨ç†æœºåˆ¶çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„3Då¤§å‹è¯­è¨€æ¨¡å‹éš¾ä»¥åƒäººç±»ä¸€æ ·è¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»è€Œå¯¼è‡´é—®ç­”ç»“æœä¸åœºæ™¯çš„å…³è”æ€§è¾ƒå¼±ï¼Œå³â€œä¸å…·èº«â€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥å¸¸è¯†é“¾ï¼ˆChain-of-Thought, CoTï¼‰æ¨ç†åˆ°3Dåœºæ™¯ç†è§£ä¸­ã€‚é€šè¿‡å°†å¤æ‚çš„æ¨ç†ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´å°çš„ã€å¯ç®¡ç†çš„æ­¥éª¤ï¼Œå¹¶ä¸ºæ¯ä¸ªæ­¥éª¤æä¾›ç›¸åº”çš„è§†è§‰çº¿ç´¢ï¼Œæ¨¡å‹å¯ä»¥é€æ­¥æ¨ç†å¹¶ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å…·èº«çš„ç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»åœ¨ç†è§£åœºæ™¯å’Œå›ç­”é—®é¢˜æ—¶çš„æ€è€ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSCENECOTæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **é—®é¢˜è§£ææ¨¡å—**ï¼šå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—å­é—®é¢˜ã€‚2) **è§†è§‰çº¿ç´¢ç”Ÿæˆæ¨¡å—**ï¼šåˆ©ç”¨å¤šæ¨¡æ€ä¸“å®¶æ¨¡å—ï¼ˆä¾‹å¦‚ï¼Œç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ï¼‰ä¸ºæ¯ä¸ªå­é—®é¢˜ç”Ÿæˆç›¸åº”çš„è§†è§‰çº¿ç´¢ã€‚3) **å¸¸è¯†é“¾æ¨ç†æ¨¡å—**ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆè§†è§‰çº¿ç´¢è¿›è¡Œé€æ­¥æ¨ç†ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚4) **å…·èº«ä¸€è‡´æ€§è¯„ä¼°æ¨¡å—**ï¼šè¯„ä¼°ç­”æ¡ˆä¸åœºæ™¯çš„å…³è”æ€§ï¼Œç¡®ä¿ç­”æ¡ˆçš„å…·èº«æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¸¸è¯†é“¾æ¨ç†æˆåŠŸåº”ç”¨äº3Dåœºæ™¯ç†è§£ã€‚ä¸ä»¥å¾€æ–¹æ³•ç›´æ¥å°†3Dåœºæ™¯ä½œä¸ºè¾“å…¥å¹¶ç”Ÿæˆç­”æ¡ˆä¸åŒï¼ŒSCENECOTé€šè¿‡åˆ†è§£æ¨ç†è¿‡ç¨‹å¹¶ç»“åˆè§†è§‰çº¿ç´¢ï¼Œå®ç°äº†æ›´ç»†ç²’åº¦çš„æ¨ç†å’Œæ›´å¼ºçš„å…·èº«æ€§ã€‚æ­¤å¤–ï¼ŒSCENECOT-185Kæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šSCENECOTæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šæ¨¡æ€ä¸“å®¶æ¨¡å—çš„é€‰æ‹©å’Œé›†æˆï¼Œç¡®ä¿èƒ½å¤Ÿæä¾›å‡†ç¡®çš„è§†è§‰çº¿ç´¢ã€‚2) å¸¸è¯†é“¾æ¨ç†æ¨¡å—ä¸­LLMçš„é€‰æ‹©å’Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è§†è§‰çº¿ç´¢è¿›è¡Œæ¨ç†ã€‚3) å…·èº«ä¸€è‡´æ€§è¯„ä¼°æ¨¡å—çš„è®¾è®¡ï¼Œç”¨äºè¡¡é‡ç­”æ¡ˆä¸åœºæ™¯çš„å…³è”æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSCENECOTæ¡†æ¶åœ¨å¤šä¸ª3Dåœºæ™¯æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å…·èº«é—®ç­”ä»»åŠ¡ä¸Šï¼ŒSCENECOTæ¡†æ¶çš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶å®ç°äº†æ›´é«˜çš„å…·èº«-é—®ç­”ä¸€è‡´æ€§ã€‚SCENECOT-185Kæ•°æ®é›†çš„å‘å¸ƒä¹Ÿä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å®è´µèµ„æºï¼Œä¿ƒè¿›äº†ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚ï¼ˆå…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªçŸ¥ï¼‰

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡3Dåœºæ™¯ç†è§£çš„å‡†ç¡®æ€§å’Œå…·èº«æ€§ï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶åšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¶å±…ä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·çš„æŒ‡ä»¤ï¼Œç»“åˆå¯¹åœºæ™¯çš„ç†è§£ï¼Œå®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚å¯»æ‰¾ç‰¹å®šç‰©å“ã€æ•´ç†æˆ¿é—´ç­‰ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨3Dåœºæ™¯ç†è§£å’Œå…·èº«æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing research on 3D Large Language Models (LLMs) still struggles to achieve grounded question-answering, primarily due to the under-exploration of the mechanism of human-like scene-object grounded reasoning. This paper bridges the gap by presenting a novel framework. We first introduce a grounded Chain-of-Thought reasoning method in 3D scenes (SCENECOT), decoupling a complex reasoning task into simpler and manageable problems, and building corresponding visual clues based on multimodal expert modules. To enable such a method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning dataset, consisting of 185K high-quality instances. Extensive experiments across various complex 3D scene reasoning benchmarks demonstrate that our new framework achieves strong performance with high grounding-QA coherence. To the best of our knowledge, this is the first successful application of CoT reasoning to 3D scene understanding, enabling step-by-step human-like reasoning and showing potential for extension to broader 3D scene understanding scenarios.

