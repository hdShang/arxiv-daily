---
layout: default
title: Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution
---

# Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution

**arXiv**: [2512.06642v1](https://arxiv.org/abs/2512.06642) | [PDF](https://arxiv.org/pdf/2512.06642.pdf)

**ä½œè€…**: Achmad Ardani Prasha, Clavino Ourizqi Rachmadi, Muhamad Fauzan Ibnu Syahlan, Naufal Rahfi Anugerah, Nanda Garin Raditya, Putri Amelia, Sabrina Laila Mutiara, Hilman Syachr Ramadhan

**åˆ†ç±»**: cs.CV, astro-ph.CO, astro-ph.IM, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-07

**å¤‡æ³¨**: 21 pages, 7 figures, 3 table

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽæŽ©ç è‡ªç¼–ç å™¨çš„å¼ºå¼•åŠ›é€é•œå›¾åƒé¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºŽæš—ç‰©è´¨æ¨¡åž‹åˆ†ç±»å’Œè¶…åˆ†è¾¨çŽ‡é‡å»ºã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºå¼•åŠ›é€é•œ` `æŽ©ç è‡ªç¼–ç å™¨` `é¢„è®­ç»ƒ` `æš—ç‰©è´¨æ¨¡åž‹åˆ†ç±»` `è¶…åˆ†è¾¨çŽ‡` `Vision Transformer` `å›¾åƒé‡å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åˆ†æžä½Žåˆ†è¾¨çŽ‡ã€é«˜å™ªå£°çš„å¼ºå¼•åŠ›é€é•œå›¾åƒä»¥æ­ç¤ºæš—ç‰©è´¨å­ç»“æž„çš„å½±å“æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚
2. åˆ©ç”¨æŽ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰åœ¨æ¨¡æ‹Ÿçš„å¼ºå¼•åŠ›é€é•œå›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ å¯æ³›åŒ–çš„å›¾åƒè¡¨ç¤ºã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æš—ç‰©è´¨æ¨¡åž‹åˆ†ç±»å’Œè¶…åˆ†è¾¨çŽ‡é‡å»ºä»»åŠ¡ä¸Šå‡ä¼˜äºŽä»Žå¤´è®­ç»ƒçš„æ¨¡åž‹ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºå¼•åŠ›é€é•œå¯ä»¥æ­ç¤ºæ˜Ÿç³»ä¸­æš—ç‰©è´¨å­ç»“æž„çš„å½±å“ï¼Œä½†ä»Žå™ªå£°è¾ƒå¤§çš„ä½Žåˆ†è¾¨çŽ‡å›¾åƒä¸­åˆ†æžè¿™äº›å½±å“æžå…·æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºŽæŽ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨DeepLense ML4SCIåŸºå‡†æµ‹è¯•ä¸­æ¨¡æ‹Ÿçš„å¼ºå¼•åŠ›é€é•œå›¾åƒä¸Šè¿›è¡Œï¼Œä»¥å­¦ä¹ å¯æ³›åŒ–çš„è¡¨ç¤ºï¼Œç”¨äºŽä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼šï¼ˆiï¼‰å¯¹æ½œåœ¨çš„æš—ç‰©è´¨æ¨¡åž‹ï¼ˆå†·æš—ç‰©è´¨ã€ç±»è½´å­æˆ–æ— å­ç»“æž„ï¼‰è¿›è¡Œåˆ†ç±»ï¼›ï¼ˆiiï¼‰é€šè¿‡è¶…åˆ†è¾¨çŽ‡å¢žå¼ºä½Žåˆ†è¾¨çŽ‡é€é•œå›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨æŽ©ç å›¾åƒå»ºæ¨¡ç›®æ ‡é¢„è®­ç»ƒVision Transformerç¼–ç å™¨ï¼Œç„¶åŽé’ˆå¯¹æ¯ä¸ªä»»åŠ¡åˆ†åˆ«å¾®è°ƒç¼–ç å™¨ã€‚ç»“æžœè¡¨æ˜Žï¼ŒMAEé¢„è®­ç»ƒä¸Žé€‚å½“çš„æŽ©ç æ¯”ä¾‹è°ƒæ•´ç›¸ç»“åˆï¼Œäº§ç”Ÿäº†ä¸€ä¸ªå…±äº«ç¼–ç å™¨ï¼Œå…¶æ€§èƒ½ä¸Žä»Žå¤´å¼€å§‹è®­ç»ƒçš„ViTç›¸åŒ¹é…æˆ–è¶…è¿‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨90%çš„æŽ©ç æ¯”ä¾‹ä¸‹ï¼Œå¾®è°ƒåŽçš„åˆ†ç±»å™¨å®žçŽ°äº†0.968çš„å®å¹³å‡AUCå’Œ88.65%çš„å‡†ç¡®çŽ‡ï¼Œè€Œä»Žå¤´å¼€å§‹è®­ç»ƒçš„åŸºçº¿åˆ†åˆ«ä¸º0.957å’Œ82.46%ã€‚å¯¹äºŽè¶…åˆ†è¾¨çŽ‡ï¼ˆ16x16åˆ°64x64ï¼‰ï¼ŒMAEé¢„è®­ç»ƒæ¨¡åž‹é‡å»ºçš„å›¾åƒçš„PSNRçº¦ä¸º33 dBï¼ŒSSIMä¸º0.961ï¼Œç•¥ä¼˜äºŽä»Žå¤´å¼€å§‹è®­ç»ƒã€‚æˆ‘ä»¬å¯¹MAEæŽ©ç æ¯”ä¾‹è¿›è¡Œäº†æ¶ˆèžç ”ç©¶ï¼Œæ­ç¤ºäº†ä¸€ä¸ªä¸€è‡´çš„æƒè¡¡ï¼šè¾ƒé«˜çš„æŽ©ç æ¯”ä¾‹æé«˜äº†åˆ†ç±»æ€§èƒ½ï¼Œä½†ç•¥å¾®é™ä½Žäº†é‡å»ºä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œåœ¨å¯Œå«ç‰©ç†ä¿¡æ¯çš„æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡ŒMAEé¢„è®­ç»ƒï¼Œä¸ºå¤šä¸ªå¼ºå¼•åŠ›é€é•œåˆ†æžä»»åŠ¡æä¾›äº†ä¸€ä¸ªçµæ´»ã€å¯é‡ç”¨çš„ç¼–ç å™¨ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»Žä½Žåˆ†è¾¨çŽ‡ã€é«˜å™ªå£°çš„å¼ºå¼•åŠ›é€é•œå›¾åƒä¸­å‡†ç¡®åˆ†ç±»æš—ç‰©è´¨æ¨¡åž‹ï¼ˆå†·æš—ç‰©è´¨ã€ç±»è½´å­æˆ–æ— å­ç»“æž„ï¼‰å¹¶è¿›è¡Œè¶…åˆ†è¾¨çŽ‡é‡å»ºçš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»å›¾åƒæ—¶ï¼Œç”±äºŽå›¾åƒè´¨é‡å·®ï¼Œç‰¹å¾æå–å›°éš¾ï¼Œå¯¼è‡´åˆ†ç±»ç²¾åº¦å’Œé‡å»ºè´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æŽ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ å›¾åƒçš„é€šç”¨è¡¨ç¤ºã€‚é€šè¿‡åœ¨å¤§é‡æ¨¡æ‹Ÿçš„å¼ºå¼•åŠ›é€é•œå›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ•æ‰åˆ°å›¾åƒä¸­çš„å…³é”®ç‰¹å¾ï¼Œä»Žè€Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æŽ©ç å›¾åƒå»ºæ¨¡è¿«ä½¿æ¨¡åž‹ç†è§£å›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå³ä½¿éƒ¨åˆ†å›¾åƒè¢«é®ç›–ä¹Ÿèƒ½è¿›è¡Œé‡å»ºï¼Œä»Žè€Œå¢žå¼ºæ¨¡åž‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨æ¨¡æ‹Ÿçš„å¼ºå¼•åŠ›é€é•œå›¾åƒæ•°æ®é›†è¿›è¡ŒMAEé¢„è®­ç»ƒï¼Œè®­ç»ƒä¸€ä¸ªVision Transformer (ViT) ç¼–ç å™¨ã€‚2) å°†é¢„è®­ç»ƒçš„ViTç¼–ç å™¨åº”ç”¨äºŽä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼šæš—ç‰©è´¨æ¨¡åž‹åˆ†ç±»å’Œè¶…åˆ†è¾¨çŽ‡é‡å»ºã€‚3) åˆ†åˆ«é’ˆå¯¹æ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å¯¹ç¼–ç å™¨è¿›è¡Œå¾®è°ƒã€‚å¯¹äºŽåˆ†ç±»ä»»åŠ¡ï¼Œåœ¨ç¼–ç å™¨åŽæ·»åŠ åˆ†ç±»å¤´ï¼›å¯¹äºŽè¶…åˆ†è¾¨çŽ‡ä»»åŠ¡ï¼Œä½¿ç”¨è§£ç å™¨å°†ç¼–ç å™¨çš„è¾“å‡ºæ˜ å°„åˆ°é«˜åˆ†è¾¨çŽ‡å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†MAEé¢„è®­ç»ƒæ–¹æ³•åº”ç”¨äºŽå¼ºå¼•åŠ›é€é•œå›¾åƒåˆ†æžã€‚ä¸Žä¼ ç»Ÿçš„ä»Žå¤´å¼€å§‹è®­ç»ƒç›¸æ¯”ï¼ŒMAEé¢„è®­ç»ƒèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…·æ³›åŒ–èƒ½åŠ›çš„å›¾åƒè¡¨ç¤ºï¼Œä»Žè€Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç ”ç©¶äº†æŽ©ç æ¯”ä¾‹å¯¹é¢„è®­ç»ƒæ•ˆæžœçš„å½±å“ï¼Œå‘çŽ°é€‚å½“çš„æŽ©ç æ¯”ä¾‹å¯ä»¥æé«˜åˆ†ç±»æ€§èƒ½ï¼Œä½†å¯èƒ½ä¼šç•¥å¾®é™ä½Žé‡å»ºä¿çœŸåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†Vision Transformer (ViT) ä½œä¸ºç¼–ç å™¨ï¼Œå¹¶é‡‡ç”¨äº†æŽ©ç å›¾åƒå»ºæ¨¡ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ã€‚å…³é”®å‚æ•°åŒ…æ‹¬æŽ©ç æ¯”ä¾‹ï¼ˆmask ratioï¼‰ï¼Œå®žéªŒè¡¨æ˜Ž90%çš„æŽ©ç æ¯”ä¾‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨çŽ°æœ€ä½³ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨åƒç´ çº§åˆ«çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºé‡å»ºæŸå¤±ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œåˆ†ç±»ä»»åŠ¡ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œè¶…åˆ†è¾¨çŽ‡ä»»åŠ¡ä½¿ç”¨PSNRå’ŒSSIMä½œä¸ºè¯„ä»·æŒ‡æ ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œåœ¨90%çš„æŽ©ç æ¯”ä¾‹ä¸‹ï¼ŒMAEé¢„è®­ç»ƒçš„åˆ†ç±»å™¨åœ¨æš—ç‰©è´¨æ¨¡åž‹åˆ†ç±»ä»»åŠ¡ä¸­å®žçŽ°äº†0.968çš„å®å¹³å‡AUCå’Œ88.65%çš„å‡†ç¡®çŽ‡ï¼Œæ˜¾è‘—ä¼˜äºŽä»Žå¤´å¼€å§‹è®­ç»ƒçš„åŸºçº¿ï¼ˆAUC 0.957ï¼Œå‡†ç¡®çŽ‡ 82.46%ï¼‰ã€‚å¯¹äºŽè¶…åˆ†è¾¨çŽ‡ä»»åŠ¡ï¼ŒMAEé¢„è®­ç»ƒæ¨¡åž‹é‡å»ºçš„å›¾åƒçš„PSNRçº¦ä¸º33 dBï¼ŒSSIMä¸º0.961ï¼Œç•¥ä¼˜äºŽä»Žå¤´å¼€å§‹è®­ç»ƒçš„æ¨¡åž‹ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå¤©æ–‡å›¾åƒå¤„ç†ã€æš—ç‰©è´¨ç ”ç©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜å¼ºå¼•åŠ›é€é•œå›¾åƒçš„åˆ†æžç²¾åº¦ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ç ”ç©¶æš—ç‰©è´¨çš„æ€§è´¨å’Œåˆ†å¸ƒï¼Œä»Žè€ŒåŠ æ·±æˆ‘ä»¬å¯¹å®‡å®™ç»“æž„çš„ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æŽ¨å¹¿åˆ°å…¶ä»–ä½Žåˆ†è¾¨çŽ‡ã€é«˜å™ªå£°çš„å›¾åƒå¤„ç†ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚åŒ»å­¦å›¾åƒåˆ†æžã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

