---
layout: default
title: Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution
---

# Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.06642" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.06642v1</a>
  <a href="https://arxiv.org/pdf/2512.06642.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.06642v1" onclick="toggleFavorite(this, '2512.06642v1', 'Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Achmad Ardani Prasha, Clavino Ourizqi Rachmadi, Muhamad Fauzan Ibnu Syahlan, Naufal Rahfi Anugerah, Nanda Garin Raditya, Putri Amelia, Sabrina Laila Mutiara, Hilman Syachr Ramadhan

**åˆ†ç±»**: cs.CV, astro-ph.CO, astro-ph.IM, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-07

**å¤‡æ³¨**: 21 pages, 7 figures, 3 table

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ©ç è‡ªç¼–ç å™¨çš„å¼ºå¼•åŠ›é€é•œå›¾åƒé¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæš—ç‰©è´¨æ¨¡å‹åˆ†ç±»å’Œè¶…åˆ†è¾¨ç‡é‡å»ºã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºå¼•åŠ›é€é•œ` `æ©ç è‡ªç¼–ç å™¨` `é¢„è®­ç»ƒ` `æš—ç‰©è´¨æ¨¡å‹åˆ†ç±»` `è¶…åˆ†è¾¨ç‡` `Vision Transformer` `å›¾åƒé‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åˆ†æä½åˆ†è¾¨ç‡ã€é«˜å™ªå£°çš„å¼ºå¼•åŠ›é€é•œå›¾åƒä»¥æ­ç¤ºæš—ç‰©è´¨å­ç»“æ„çš„å½±å“æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚
2. åˆ©ç”¨æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰åœ¨æ¨¡æ‹Ÿçš„å¼ºå¼•åŠ›é€é•œå›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ å¯æ³›åŒ–çš„å›¾åƒè¡¨ç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æš—ç‰©è´¨æ¨¡å‹åˆ†ç±»å’Œè¶…åˆ†è¾¨ç‡é‡å»ºä»»åŠ¡ä¸Šå‡ä¼˜äºä»å¤´è®­ç»ƒçš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºå¼•åŠ›é€é•œå¯ä»¥æ­ç¤ºæ˜Ÿç³»ä¸­æš—ç‰©è´¨å­ç»“æ„çš„å½±å“ï¼Œä½†ä»å™ªå£°è¾ƒå¤§çš„ä½åˆ†è¾¨ç‡å›¾åƒä¸­åˆ†æè¿™äº›å½±å“æå…·æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨DeepLense ML4SCIåŸºå‡†æµ‹è¯•ä¸­æ¨¡æ‹Ÿçš„å¼ºå¼•åŠ›é€é•œå›¾åƒä¸Šè¿›è¡Œï¼Œä»¥å­¦ä¹ å¯æ³›åŒ–çš„è¡¨ç¤ºï¼Œç”¨äºä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼šï¼ˆiï¼‰å¯¹æ½œåœ¨çš„æš—ç‰©è´¨æ¨¡å‹ï¼ˆå†·æš—ç‰©è´¨ã€ç±»è½´å­æˆ–æ— å­ç»“æ„ï¼‰è¿›è¡Œåˆ†ç±»ï¼›ï¼ˆiiï¼‰é€šè¿‡è¶…åˆ†è¾¨ç‡å¢å¼ºä½åˆ†è¾¨ç‡é€é•œå›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨æ©ç å›¾åƒå»ºæ¨¡ç›®æ ‡é¢„è®­ç»ƒVision Transformerç¼–ç å™¨ï¼Œç„¶åé’ˆå¯¹æ¯ä¸ªä»»åŠ¡åˆ†åˆ«å¾®è°ƒç¼–ç å™¨ã€‚ç»“æœè¡¨æ˜ï¼ŒMAEé¢„è®­ç»ƒä¸é€‚å½“çš„æ©ç æ¯”ä¾‹è°ƒæ•´ç›¸ç»“åˆï¼Œäº§ç”Ÿäº†ä¸€ä¸ªå…±äº«ç¼–ç å™¨ï¼Œå…¶æ€§èƒ½ä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„ViTç›¸åŒ¹é…æˆ–è¶…è¿‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨90%çš„æ©ç æ¯”ä¾‹ä¸‹ï¼Œå¾®è°ƒåçš„åˆ†ç±»å™¨å®ç°äº†0.968çš„å®å¹³å‡AUCå’Œ88.65%çš„å‡†ç¡®ç‡ï¼Œè€Œä»å¤´å¼€å§‹è®­ç»ƒçš„åŸºçº¿åˆ†åˆ«ä¸º0.957å’Œ82.46%ã€‚å¯¹äºè¶…åˆ†è¾¨ç‡ï¼ˆ16x16åˆ°64x64ï¼‰ï¼ŒMAEé¢„è®­ç»ƒæ¨¡å‹é‡å»ºçš„å›¾åƒçš„PSNRçº¦ä¸º33 dBï¼ŒSSIMä¸º0.961ï¼Œç•¥ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒã€‚æˆ‘ä»¬å¯¹MAEæ©ç æ¯”ä¾‹è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œæ­ç¤ºäº†ä¸€ä¸ªä¸€è‡´çš„æƒè¡¡ï¼šè¾ƒé«˜çš„æ©ç æ¯”ä¾‹æé«˜äº†åˆ†ç±»æ€§èƒ½ï¼Œä½†ç•¥å¾®é™ä½äº†é‡å»ºä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å¯Œå«ç‰©ç†ä¿¡æ¯çš„æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡ŒMAEé¢„è®­ç»ƒï¼Œä¸ºå¤šä¸ªå¼ºå¼•åŠ›é€é•œåˆ†æä»»åŠ¡æä¾›äº†ä¸€ä¸ªçµæ´»ã€å¯é‡ç”¨çš„ç¼–ç å™¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»ä½åˆ†è¾¨ç‡ã€é«˜å™ªå£°çš„å¼ºå¼•åŠ›é€é•œå›¾åƒä¸­å‡†ç¡®åˆ†ç±»æš—ç‰©è´¨æ¨¡å‹ï¼ˆå†·æš—ç‰©è´¨ã€ç±»è½´å­æˆ–æ— å­ç»“æ„ï¼‰å¹¶è¿›è¡Œè¶…åˆ†è¾¨ç‡é‡å»ºçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»å›¾åƒæ—¶ï¼Œç”±äºå›¾åƒè´¨é‡å·®ï¼Œç‰¹å¾æå–å›°éš¾ï¼Œå¯¼è‡´åˆ†ç±»ç²¾åº¦å’Œé‡å»ºè´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ å›¾åƒçš„é€šç”¨è¡¨ç¤ºã€‚é€šè¿‡åœ¨å¤§é‡æ¨¡æ‹Ÿçš„å¼ºå¼•åŠ›é€é•œå›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°å›¾åƒä¸­çš„å…³é”®ç‰¹å¾ï¼Œä»è€Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ©ç å›¾åƒå»ºæ¨¡è¿«ä½¿æ¨¡å‹ç†è§£å›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå³ä½¿éƒ¨åˆ†å›¾åƒè¢«é®ç›–ä¹Ÿèƒ½è¿›è¡Œé‡å»ºï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨æ¨¡æ‹Ÿçš„å¼ºå¼•åŠ›é€é•œå›¾åƒæ•°æ®é›†è¿›è¡ŒMAEé¢„è®­ç»ƒï¼Œè®­ç»ƒä¸€ä¸ªVision Transformer (ViT) ç¼–ç å™¨ã€‚2) å°†é¢„è®­ç»ƒçš„ViTç¼–ç å™¨åº”ç”¨äºä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼šæš—ç‰©è´¨æ¨¡å‹åˆ†ç±»å’Œè¶…åˆ†è¾¨ç‡é‡å»ºã€‚3) åˆ†åˆ«é’ˆå¯¹æ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å¯¹ç¼–ç å™¨è¿›è¡Œå¾®è°ƒã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œåœ¨ç¼–ç å™¨åæ·»åŠ åˆ†ç±»å¤´ï¼›å¯¹äºè¶…åˆ†è¾¨ç‡ä»»åŠ¡ï¼Œä½¿ç”¨è§£ç å™¨å°†ç¼–ç å™¨çš„è¾“å‡ºæ˜ å°„åˆ°é«˜åˆ†è¾¨ç‡å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†MAEé¢„è®­ç»ƒæ–¹æ³•åº”ç”¨äºå¼ºå¼•åŠ›é€é•œå›¾åƒåˆ†æã€‚ä¸ä¼ ç»Ÿçš„ä»å¤´å¼€å§‹è®­ç»ƒç›¸æ¯”ï¼ŒMAEé¢„è®­ç»ƒèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…·æ³›åŒ–èƒ½åŠ›çš„å›¾åƒè¡¨ç¤ºï¼Œä»è€Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç ”ç©¶äº†æ©ç æ¯”ä¾‹å¯¹é¢„è®­ç»ƒæ•ˆæœçš„å½±å“ï¼Œå‘ç°é€‚å½“çš„æ©ç æ¯”ä¾‹å¯ä»¥æé«˜åˆ†ç±»æ€§èƒ½ï¼Œä½†å¯èƒ½ä¼šç•¥å¾®é™ä½é‡å»ºä¿çœŸåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†Vision Transformer (ViT) ä½œä¸ºç¼–ç å™¨ï¼Œå¹¶é‡‡ç”¨äº†æ©ç å›¾åƒå»ºæ¨¡ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ã€‚å…³é”®å‚æ•°åŒ…æ‹¬æ©ç æ¯”ä¾‹ï¼ˆmask ratioï¼‰ï¼Œå®éªŒè¡¨æ˜90%çš„æ©ç æ¯”ä¾‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨åƒç´ çº§åˆ«çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºé‡å»ºæŸå¤±ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œåˆ†ç±»ä»»åŠ¡ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä½¿ç”¨PSNRå’ŒSSIMä½œä¸ºè¯„ä»·æŒ‡æ ‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨90%çš„æ©ç æ¯”ä¾‹ä¸‹ï¼ŒMAEé¢„è®­ç»ƒçš„åˆ†ç±»å™¨åœ¨æš—ç‰©è´¨æ¨¡å‹åˆ†ç±»ä»»åŠ¡ä¸­å®ç°äº†0.968çš„å®å¹³å‡AUCå’Œ88.65%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒçš„åŸºçº¿ï¼ˆAUC 0.957ï¼Œå‡†ç¡®ç‡ 82.46%ï¼‰ã€‚å¯¹äºè¶…åˆ†è¾¨ç‡ä»»åŠ¡ï¼ŒMAEé¢„è®­ç»ƒæ¨¡å‹é‡å»ºçš„å›¾åƒçš„PSNRçº¦ä¸º33 dBï¼ŒSSIMä¸º0.961ï¼Œç•¥ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤©æ–‡å›¾åƒå¤„ç†ã€æš—ç‰©è´¨ç ”ç©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜å¼ºå¼•åŠ›é€é•œå›¾åƒçš„åˆ†æç²¾åº¦ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ç ”ç©¶æš—ç‰©è´¨çš„æ€§è´¨å’Œåˆ†å¸ƒï¼Œä»è€ŒåŠ æ·±æˆ‘ä»¬å¯¹å®‡å®™ç»“æ„çš„ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ä½åˆ†è¾¨ç‡ã€é«˜å™ªå£°çš„å›¾åƒå¤„ç†ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚åŒ»å­¦å›¾åƒåˆ†æã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

