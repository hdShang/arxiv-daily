---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-21
---

# cs.CVï¼ˆ2025-12-21ï¼‰

ğŸ“Š å…± **31** ç¯‡è®ºæ–‡
 | ğŸ”— **6** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251217495v1-groundingme-exposing-the-visual-grounding-gap-in-mllms-through-multi.html">GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</a></td>
  <td>GroundingMEï¼šå¤šç»´åº¦è¯„æµ‹æ­ç¤ºMLLMåœ¨è§†è§‰å®šä½èƒ½åŠ›ä¸Šçš„å·®è·</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17495v1" data-paper-url="./papers/251217495v1-groundingme-exposing-the-visual-grounding-gap-in-mllms-through-multi.html" onclick="toggleFavorite(this, '2512.17495v1', 'GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251217902v1-adversarial-robustness-of-vision-in-open-foundation-models.html">Adversarial Robustness of Vision in Open Foundation Models</a></td>
  <td>ç ”ç©¶æ­ç¤ºå¼€æ”¾è§†è§‰åŸºç¡€æ¨¡å‹åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„è„†å¼±æ€§ï¼Œå¹¶å‘ç°é²æ£’æ€§ä¸åŸºå‡†æ€§èƒ½ä¸ç›´æ¥ç›¸å…³ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17902v1" data-paper-url="./papers/251217902v1-adversarial-robustness-of-vision-in-open-foundation-models.html" onclick="toggleFavorite(this, '2512.17902v1', 'Adversarial Robustness of Vision in Open Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251217621v1-pathflip-fine-grained-language-image-pretraining-for-versatile-compu.html">PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology</a></td>
  <td>PathFLIPï¼šç”¨äºå¤šåŠŸèƒ½è®¡ç®—ç—…ç†å­¦çš„ç»†ç²’åº¦è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17621v1" data-paper-url="./papers/251217621v1-pathflip-fine-grained-language-image-pretraining-for-versatile-compu.html" onclick="toggleFavorite(this, '2512.17621v1', 'PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251217514v1-foundation-model-priors-enhance-object-focus-in-feature-space-for-so.html">Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection</a></td>
  <td>FALCON-SFODï¼šåˆ©ç”¨å…ˆéªŒçŸ¥è¯†å¢å¼ºæºåŸŸæ— å…³ç›®æ ‡æ£€æµ‹ä¸­çš„ç›®æ ‡èšç„¦</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17514v1" data-paper-url="./papers/251217514v1-foundation-model-priors-enhance-object-focus-in-feature-space-for-so.html" onclick="toggleFavorite(this, '2512.17514v1', 'Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251217450v1-multiaqua-a-multimodal-maritime-dataset-and-robust-training-strategi.html">MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</a></td>
  <td>æå‡ºMULTIAQUAå¤šæ¨¡æ€æ°´é¢æ•°æ®é›†ï¼Œå¹¶è®¾è®¡é²æ£’è®­ç»ƒç­–ç•¥æå‡æ°´é¢è¯­ä¹‰åˆ†å‰²æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17450v1" data-paper-url="./papers/251217450v1-multiaqua-a-multimodal-maritime-dataset-and-robust-training-strategi.html" onclick="toggleFavorite(this, '2512.17450v1', 'MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251217601v1-headhunt-vad-hunting-robust-anomaly-sensitive-heads-in-mllm-for-tuni.html">HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection</a></td>
  <td>HeadHunt-VADï¼šåœ¨MLLMä¸­å¯»æ‰¾å¼‚å¸¸æ•æ„Ÿå¤´ï¼Œå®ç°å…è°ƒä¼˜è§†é¢‘å¼‚å¸¸æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17601v1" data-paper-url="./papers/251217601v1-headhunt-vad-hunting-robust-anomaly-sensitive-heads-in-mllm-for-tuni.html" onclick="toggleFavorite(this, '2512.17601v1', 'HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251217532v1-robust-r1-degradation-aware-reasoning-for-robust-visual-understandin.html">Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</a></td>
  <td>æå‡ºRobust-R1æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡è§†è§‰é€€åŒ–æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17532v1" data-paper-url="./papers/251217532v1-robust-r1-degradation-aware-reasoning-for-robust-visual-understandin.html" onclick="toggleFavorite(this, '2512.17532v1', 'Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251217319v1-a-benchmark-for-ultra-high-resolution-remote-sensing-mllms.html">A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs</a></td>
  <td>æå‡ºRSHR-Benchä»¥è§£å†³é¥æ„Ÿè¶…é«˜åˆ†è¾¨ç‡è§†è§‰ç†è§£è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17319v1" data-paper-url="./papers/251217319v1-a-benchmark-for-ultra-high-resolution-remote-sensing-mllms.html" onclick="toggleFavorite(this, '2512.17319v1', 'A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251217306v1-deep-but-reliable-advancing-multi-turn-reasoning-for-thinking-with-i.html">Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images</a></td>
  <td>æå‡ºDRIMæ¨¡å‹ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ¨ç†ä¸­çš„å¤šè½®è‡ªåæ€èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17306v1" data-paper-url="./papers/251217306v1-deep-but-reliable-advancing-multi-turn-reasoning-for-thinking-with-i.html" onclick="toggleFavorite(this, '2512.17306v1', 'Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251217891v1-keypoint-counting-classifiers-turning-vision-transformers-into-self-.html">Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</a></td>
  <td>æå‡ºæ— éœ€è®­ç»ƒçš„Keypoint Counting Classifiersï¼Œå°†ViTè½¬åŒ–ä¸ºè‡ªè§£é‡Šæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17891v1" data-paper-url="./papers/251217891v1-keypoint-counting-classifiers-turning-vision-transformers-into-self-.html" onclick="toggleFavorite(this, '2512.17891v1', 'Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251217313v1-auxiliary-descriptive-knowledge-for-few-shot-adaptation-of-vision-la.html">Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model</a></td>
  <td>æå‡ºè¾…åŠ©æè¿°çŸ¥è¯†ADKï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬è¿ç§»å­¦ä¹ ä¸­çš„æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17313v1" data-paper-url="./papers/251217313v1-auxiliary-descriptive-knowledge-for-few-shot-adaptation-of-vision-la.html" onclick="toggleFavorite(this, '2512.17313v1', 'Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251217178v1-abe-clip-training-free-attribute-binding-enhancement-for-composition.html">ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching</a></td>
  <td>ABE-CLIPï¼šå…è®­ç»ƒçš„å±æ€§ç»‘å®šå¢å¼ºæ–¹æ³•ï¼Œæå‡ç»„åˆå›¾åƒ-æ–‡æœ¬åŒ¹é…æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17178v1" data-paper-url="./papers/251217178v1-abe-clip-training-free-attribute-binding-enhancement-for-composition.html" onclick="toggleFavorite(this, '2512.17178v1', 'ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/251217640v1-generative-human-object-interaction-detection-via-differentiable-cog.html">Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs</a></td>
  <td>æå‡ºGRASP-HOæ¡†æ¶ï¼Œé€šè¿‡å¯å¾®åˆ†è®¤çŸ¥å¼•å¯¼å¤šæ¨¡æ€LLMå®ç°ç”Ÿæˆå¼äºº-ç‰©äº¤äº’æ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17640v1" data-paper-url="./papers/251217640v1-generative-human-object-interaction-detection-via-differentiable-cog.html" onclick="toggleFavorite(this, '2512.17640v1', 'Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251217817v1-chorus-multi-teacher-pretraining-for-holistic-3d-gaussian-scene-enco.html">Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</a></td>
  <td>Chorusï¼šå¤šæ•™å¸ˆé¢„è®­ç»ƒç”¨äºæ•´ä½“3Dé«˜æ–¯åœºæ™¯ç¼–ç </td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17817v1" data-paper-url="./papers/251217817v1-chorus-multi-teacher-pretraining-for-holistic-3d-gaussian-scene-enco.html" onclick="toggleFavorite(this, '2512.17817v1', 'Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251217547v1-g3splat-geometrically-consistent-generalizable-gaussian-splatting.html">G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</a></td>
  <td>G3Splatï¼šé€šè¿‡å‡ ä½•ä¸€è‡´æ€§å…ˆéªŒå®ç°å¯æ³›åŒ–çš„é«˜æ–¯æº…å°„</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17547v1" data-paper-url="./papers/251217547v1-g3splat-geometrically-consistent-generalizable-gaussian-splatting.html" onclick="toggleFavorite(this, '2512.17547v1', 'G3Splat: Geometrically Consistent Generalizable Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251217784v1-long-range-depth-estimation-using-learning-based-hybrid-distortion-m.html">Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras</a></td>
  <td>æå‡ºåŸºäºå­¦ä¹ çš„æ··åˆç•¸å˜æ¨¡å‹ï¼Œç”¨äºCCTVç›¸æœºé•¿è·ç¦»æ·±åº¦ä¼°è®¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17784v1" data-paper-url="./papers/251217784v1-long-range-depth-estimation-using-learning-based-hybrid-distortion-m.html" onclick="toggleFavorite(this, '2512.17784v1', 'Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251217459v1-3d-re-gen-3d-reconstruction-of-indoor-scenes-with-a-generative-frame.html">3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework</a></td>
  <td>3D-RE-GENï¼šæå‡ºä¸€ç§ç”Ÿæˆå¼æ¡†æ¶ï¼Œç”¨äºå®¤å†…åœºæ™¯çš„å•å›¾ä¸‰ç»´é‡å»ºï¼Œæ»¡è¶³è‰ºæœ¯å®¶å¯¹å¯ç¼–è¾‘ç½‘æ ¼çš„éœ€æ±‚ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17459v1" data-paper-url="./papers/251217459v1-3d-re-gen-3d-reconstruction-of-indoor-scenes-with-a-generative-frame.html" onclick="toggleFavorite(this, '2512.17459v1', '3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251217724v1-saved-a-first-person-social-media-video-dataset-for-adas-equipped-ve.html">SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses</a></td>
  <td>SAVeDï¼šç”¨äºADASè½¦è¾†è¿‘å¤±å’Œç¢°æ’äº‹ä»¶åˆ†æçš„ç¬¬ä¸€äººç§°ç¤¾äº¤åª’ä½“è§†é¢‘æ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17724v1" data-paper-url="./papers/251217724v1-saved-a-first-person-social-media-video-dataset-for-adas-equipped-ve.html" onclick="toggleFavorite(this, '2512.17724v1', 'SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251217504v1-insertanywhere-bridging-4d-scene-geometry-and-diffusion-models-for-r.html">InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</a></td>
  <td>InsertAnywhereï¼šèåˆ4Dåœºæ™¯å‡ ä½•ä¸æ‰©æ•£æ¨¡å‹ï¼Œå®ç°é€¼çœŸçš„è§†é¢‘å¯¹è±¡æ’å…¥</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17504v1" data-paper-url="./papers/251217504v1-insertanywhere-bridging-4d-scene-geometry-and-diffusion-models-for-r.html" onclick="toggleFavorite(this, '2512.17504v1', 'InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251217331v1-synergywarpnet-attention-guided-cooperative-warping-for-neural-portr.html">SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation</a></td>
  <td>SynergyWarpNetï¼šç”¨äºç¥ç»è‚–åƒåŠ¨ç”»çš„æ³¨æ„åŠ›å¼•å¯¼ååŒæ‰­æ›²ç½‘ç»œ</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17331v1" data-paper-url="./papers/251217331v1-synergywarpnet-attention-guided-cooperative-warping-for-neural-portr.html" onclick="toggleFavorite(this, '2512.17331v1', 'SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251217227v1-learning-when-to-look-a-disentangled-curriculum-for-strategic-percep.html">Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning</a></td>
  <td>æå‡ºè§£è€¦è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œè§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­è§†è§‰ä¿¡æ¯é—å¿˜é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17227v1" data-paper-url="./papers/251217227v1-learning-when-to-look-a-disentangled-curriculum-for-strategic-percep.html" onclick="toggleFavorite(this, '2512.17227v1', 'Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251217908v1-re-depth-anything-test-time-depth-refinement-via-self-supervised-re-.html">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</a></td>
  <td>Re-Depth Anythingï¼šåˆ©ç”¨è‡ªç›‘ç£é‡ç…§æ˜è¿›è¡Œæµ‹è¯•æ—¶æ·±åº¦ä¼˜åŒ–</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17908v1" data-paper-url="./papers/251217908v1-re-depth-anything-test-time-depth-refinement-via-self-supervised-re-.html" onclick="toggleFavorite(this, '2512.17908v1', 'Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251217541v1-fleg-feed-forward-language-embedded-gaussian-splatting-from-any-view.html">FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views</a></td>
  <td>FLEGï¼šæå‡ºä¸€ç§ä»ä»»æ„è§†è§’è¿›è¡Œå‰é¦ˆè¯­è¨€åµŒå…¥é«˜æ–¯æº…å°„çš„æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17541v1" data-paper-url="./papers/251217541v1-fleg-feed-forward-language-embedded-gaussian-splatting-from-any-view.html" onclick="toggleFavorite(this, '2512.17541v1', 'FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251217436v1-xiaomi-mimo-vl-miloco-technical-report.html">Xiaomi MiMo-VL-Miloco Technical Report</a></td>
  <td>æå‡ºMiMo-VL-Milocoä»¥è§£å†³æ™ºèƒ½å®¶å±…åœºæ™¯ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17436v1" data-paper-url="./papers/251217436v1-xiaomi-mimo-vl-miloco-technical-report.html" onclick="toggleFavorite(this, '2512.17436v1', 'Xiaomi MiMo-VL-Miloco Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251217152v1-physfire-wm-a-physics-informed-world-model-for-emulating-fire-spread.html">PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</a></td>
  <td>æå‡ºPhysFire-WMï¼Œåˆ©ç”¨ç‰©ç†ä¿¡æ¯ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿç«ç¾è”“å»¶åŠ¨æ€</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17152v1" data-paper-url="./papers/251217152v1-physfire-wm-a-physics-informed-world-model-for-emulating-fire-spread.html" onclick="toggleFavorite(this, '2512.17152v1', 'PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/251217303v1-emag-self-rectifying-diffusion-sampling-with-exponential-moving-aver.html">EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance</a></td>
  <td>æå‡ºEMAGï¼šä¸€ç§åŸºäºæŒ‡æ•°ç§»åŠ¨å¹³å‡æŒ‡å¯¼çš„è‡ªæ ¡æ­£æ‰©æ•£é‡‡æ ·æ–¹æ³•ï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17303v1" data-paper-url="./papers/251217303v1-emag-self-rectifying-diffusion-sampling-with-exponential-moving-aver.html" onclick="toggleFavorite(this, '2512.17303v1', 'EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/251217253v1-mitty-diffusion-based-human-to-robot-video-generation.html">Mitty: Diffusion-based Human-to-Robot Video Generation</a></td>
  <td>Mittyï¼šæå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„Human2Robotè§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œå®ç°ç«¯åˆ°ç«¯æœºå™¨äººå­¦ä¹ ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">human-to-robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17253v1" data-paper-url="./papers/251217253v1-mitty-diffusion-based-human-to-robot-video-generation.html" onclick="toggleFavorite(this, '2512.17253v1', 'Mitty: Diffusion-based Human-to-Robot Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251217545v1-clothhmr-3d-mesh-recovery-of-humans-in-diverse-clothing-from-single-.html">ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image</a></td>
  <td>æå‡ºClothHMRä»¥è§£å†³å¤šæ ·æœè£…ä¸‹3Däººç±»ç½‘æ ¼æ¢å¤é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">human mesh recovery</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17545v1" data-paper-url="./papers/251217545v1-clothhmr-3d-mesh-recovery-of-humans-in-diverse-clothing-from-single-.html" onclick="toggleFavorite(this, '2512.17545v1', 'ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/251217907v1-dexterous-world-models.html">Dexterous World Models</a></td>
  <td>æå‡ºçµå·§ä¸–ç•Œæ¨¡å‹DWMï¼Œå®ç°åŸºäºè§†é¢‘æ‰©æ•£çš„äº¤äº’å¼æ•°å­—å­ªç”Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span> <span class="paper-tag">manipulation</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17907v1" data-paper-url="./papers/251217907v1-dexterous-world-models.html" onclick="toggleFavorite(this, '2512.17907v1', 'Dexterous World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/251217224v1-any-optical-model-a-universal-foundation-model-for-optical-remote-se.html">Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing</a></td>
  <td>æå‡ºAny-Optical-Modelï¼Œè§£å†³é¥æ„Ÿé¢†åŸŸè·¨ä¼ æ„Ÿå™¨ã€åˆ†è¾¨ç‡å’Œç¼ºå¤±æ³¢æ®µçš„é€šç”¨æ€§éš¾é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17224v1" data-paper-url="./papers/251217224v1-any-optical-model-a-universal-foundation-model-for-optical-remote-se.html" onclick="toggleFavorite(this, '2512.17224v1', 'Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/251217897v1-radargen-automotive-radar-point-cloud-generation-from-cameras.html">RadarGen: Automotive Radar Point Cloud Generation from Cameras</a></td>
  <td>RadarGenï¼šæå‡ºä¸€ç§åŸºäºå›¾åƒçš„æ±½è½¦é›·è¾¾ç‚¹äº‘ç”Ÿæˆæ‰©æ•£æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.17897v1" data-paper-url="./papers/251217897v1-radargen-automotive-radar-point-cloud-generation-from-cameras.html" onclick="toggleFavorite(this, '2512.17897v1', 'RadarGen: Automotive Radar Point Cloud Generation from Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)