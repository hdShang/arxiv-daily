---
layout: default
title: Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model
---

# Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17313" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17313v1</a>
  <a href="https://arxiv.org/pdf/2512.17313.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17313v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17313v1', 'Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: SuBeen Lee, GilHan Park, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¾…åŠ©æè¿°çŸ¥è¯†ADKï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬è¿ç§»å­¦ä¹ ä¸­çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å°‘æ ·æœ¬å­¦ä¹ ` `è¿ç§»å­¦ä¹ ` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `è¾…åŠ©çŸ¥è¯†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMçš„å°‘æ ·æœ¬è¿ç§»å­¦ä¹ æ–¹æ³•ä¾èµ–æ‰‹å·¥æç¤ºï¼Œéš¾ä»¥å……åˆ†ç†è§£ç±»åˆ«è¯­ä¹‰ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. æå‡ºè¾…åŠ©æè¿°çŸ¥è¯†(ADK)æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸°å¯Œçš„ç±»åˆ«æè¿°ï¼Œå¢å¼ºæ–‡æœ¬è¡¨ç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒADKèƒ½æ˜¾è‘—æå‡ç°æœ‰PEFTæ–¹æ³•çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåœºæ™¯ä¸­è¾¾åˆ°æ–°çš„state-of-the-artã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå½“æ•°æ®åˆ†å¸ƒä¸é¢„è®­ç»ƒæ•°æ®å­˜åœ¨å·®å¼‚æ—¶ï¼Œè¡¨ç°å¾€å¾€ä¸ä½³ã€‚å°‘æ ·æœ¬è¿ç§»å­¦ä¹ (FSA-VLM)å·²æˆä¸ºä¸€ä¸ªå…³é”®è§£å†³æ–¹æ¡ˆï¼Œé€šå¸¸ä½¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ¥ä»¥æœ€å°‘çš„æ•°æ®è°ƒæ•´æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›PEFTæ–¹æ³•å—åˆ°å…¶å¯¹å›ºå®šã€æ‰‹å·¥åˆ¶ä½œæç¤ºçš„ä¾èµ–çš„é™åˆ¶ï¼Œè¿™äº›æç¤ºé€šå¸¸ä¸è¶³ä»¥ç†è§£ç±»åˆ«çš„è¯­ä¹‰ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶æå‡ºäº†åˆ©ç”¨å›¾åƒè¯±å¯¼æç¤ºæ¥ä¸ºåˆ†ç±»æä¾›é¢å¤–çš„çº¿ç´¢ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†æ—¶å¼•å…¥äº†è¿‡é«˜çš„è®¡ç®—å¼€é”€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¾…åŠ©æè¿°çŸ¥è¯†(ADK)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä¸°å¯Œæ–‡æœ¬è¡¨ç¤ºï¼Œè€Œä¸ä¼šå½±å“æ•ˆç‡ã€‚ADKé¦–å…ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç¦»çº¿ç”Ÿæˆæ¯ä¸ªç±»åˆ«çš„ä¸°å¯Œæè¿°æ€§æç¤ºé›†ã€‚ç„¶åä»¥ä¸¤ç§æ–¹å¼éƒ¨ç½²è¿™äº›é¢„å…ˆè®¡ç®—çš„ç‰¹å¾ï¼š(1)ä½œä¸ºç»„åˆçŸ¥è¯†ï¼Œä¸€ç§å¹³å‡è¡¨ç¤ºï¼Œæä¾›ä¸°å¯Œçš„è¯­ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨ç±»åæ¨¡ç³Šæˆ–VLMä¸ç†Ÿæ‚‰æ—¶ï¼›(2)ä½œä¸ºå®ä¾‹ç‰¹å®šçŸ¥è¯†ï¼Œå…¶ä¸­è½»é‡çº§ã€éå‚æ•°æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€åœ°é€‰æ‹©ç»™å®šå›¾åƒæœ€ç›¸å…³çš„æè¿°ã€‚è¿™ç§æ–¹æ³•æä¾›äº†æ‰‹å·¥åˆ¶ä½œæç¤ºä¹‹å¤–çš„ä¸¤ç§é¢å¤–ç±»å‹çš„çŸ¥è¯†ï¼Œä»è€Œæœ‰åŠ©äºè·¨å„ç§é¢†åŸŸçš„ç±»åˆ«åŒºåˆ†ã€‚æ­¤å¤–ï¼ŒADKå……å½“æ— å‚æ•°ã€å³æ’å³ç”¨çš„ç»„ä»¶ï¼Œå¯å¢å¼ºç°æœ‰çš„PEFTæ–¹æ³•ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒADKå§‹ç»ˆæé«˜å¤šä¸ªPEFTåŸºçº¿çš„æ€§èƒ½ï¼Œåœ¨å„ç§åœºæ™¯ä¸­è®¾ç½®äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬è¿ç§»å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æ–‡æœ¬æç¤ºï¼Œè¿™äº›æç¤ºå¾€å¾€æ— æ³•å……åˆ†è¡¨è¾¾ç±»åˆ«çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹åˆ†å¸ƒåç§»æ—¶æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œä¸€äº›åˆ©ç”¨å›¾åƒç”Ÿæˆæç¤ºçš„æ–¹æ³•è™½ç„¶å¯ä»¥æä¾›é¢å¤–ä¿¡æ¯ï¼Œä½†è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œä¸é€‚ç”¨äºå®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆä¸°å¯Œçš„ç±»åˆ«æè¿°ï¼Œä½œä¸ºè¾…åŠ©çŸ¥è¯†æ¥å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬è¡¨ç¤ºã€‚é€šè¿‡é¢„å…ˆè®¡ç®—å¹¶å­˜å‚¨è¿™äº›æè¿°ï¼Œå¯ä»¥åœ¨æ¨ç†é˜¶æ®µé«˜æ•ˆåœ°åˆ©ç”¨è¿™äº›çŸ¥è¯†ï¼Œè€Œæ— éœ€å¼•å…¥é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šADKæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šç¦»çº¿æè¿°ç”Ÿæˆå’Œåœ¨çº¿çŸ¥è¯†èåˆã€‚é¦–å…ˆï¼Œåˆ©ç”¨LLMä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆå¤šä¸ªæè¿°æ€§æç¤ºã€‚ç„¶åï¼Œè¿™äº›é¢„è®¡ç®—çš„æè¿°è¢«ç”¨äºä¸¤ç§æ–¹å¼ï¼šç»„åˆçŸ¥è¯†å’Œå®ä¾‹ç‰¹å®šçŸ¥è¯†ã€‚ç»„åˆçŸ¥è¯†æ˜¯å¯¹æ‰€æœ‰æè¿°è¿›è¡Œå¹³å‡ï¼Œæä¾›ç±»åˆ«çš„æ•´ä½“è¯­ä¹‰ä¿¡æ¯ã€‚å®ä¾‹ç‰¹å®šçŸ¥è¯†åˆ™ä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„éå‚æ•°æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®è¾“å…¥å›¾åƒåŠ¨æ€åœ°é€‰æ‹©æœ€ç›¸å…³çš„æè¿°ã€‚

**å…³é”®åˆ›æ–°**ï¼šADKçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„çŸ¥è¯†èåˆæ–¹å¼ã€‚é€šè¿‡é¢„å…ˆè®¡ç®—æè¿°å¹¶ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€é€‰æ‹©ç›¸å…³æè¿°ï¼ŒADKèƒ½å¤Ÿåœ¨ä¸å¼•å…¥é¢å¤–è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒADKä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥æ–¹ä¾¿åœ°é›†æˆåˆ°ç°æœ‰çš„PEFTæ–¹æ³•ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šADKçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨LLMç”Ÿæˆå¤šæ ·åŒ–çš„ç±»åˆ«æè¿°ï¼›2) ä½¿ç”¨å¹³å‡æ± åŒ–ç”Ÿæˆç»„åˆçŸ¥è¯†ï¼Œæä¾›ç±»åˆ«çš„æ•´ä½“è¯­ä¹‰ï¼›3) ä½¿ç”¨éå‚æ•°æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®å›¾åƒç‰¹å¾åŠ¨æ€é€‰æ‹©å®ä¾‹ç›¸å…³çš„æè¿°ã€‚å…·ä½“æ¥è¯´ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„æƒé‡æ˜¯åŸºäºå›¾åƒç‰¹å¾å’Œæè¿°ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦è®¡ç®—çš„ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17313v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17313v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17313v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒADKèƒ½å¤Ÿæ˜¾è‘—æå‡ç°æœ‰PEFTæ–¹æ³•åœ¨å°‘æ ·æœ¬è¿ç§»å­¦ä¹ ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒADKå°†CoOpã€Tip-Adapterç­‰åŸºçº¿çš„æ€§èƒ½æå‡äº†å¤šä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åœ¨å¤šä¸ªåœºæ™¯ä¸­å–å¾—äº†state-of-the-artçš„ç»“æœã€‚è¿™è¯æ˜äº†ADKèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è¾…åŠ©çŸ¥è¯†æ¥å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬è¡¨ç¤ºï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒæ£€ç´¢ç­‰è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºæˆ–ç±»åˆ«è¯­ä¹‰å¤æ‚çš„åœºæ™¯ä¸‹ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œå¯ä»¥åˆ©ç”¨ADKæ¥è¾…åŠ©åŒ»ç”Ÿè¯Šæ–­ç½•è§ç–¾ç—…ï¼Œæˆ–åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œæå‡æ¨¡å‹å¯¹å¤æ‚äº¤é€šåœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œæœ‰æœ›æ¨åŠ¨è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the impressive zero-shot capabilities of Vision-Language Models (VLMs), they often struggle in downstream tasks with distribution shifts from the pre-training data. Few-Shot Adaptation (FSA-VLM) has emerged as a key solution, typically using Parameter-Efficient Fine-Tuning (PEFT) to adapt models with minimal data. However, these PEFT methods are constrained by their reliance on fixed, handcrafted prompts, which are often insufficient to understand the semantics of classes. While some studies have proposed leveraging image-induced prompts to provide additional clues for classification, they introduce prohibitive computational overhead at inference. Therefore, we introduce Auxiliary Descriptive Knowledge (ADK), a novel framework that efficiently enriches text representations without compromising efficiency. ADK first leverages a Large Language Model to generate a rich set of descriptive prompts for each class offline. These pre-computed features are then deployed in two ways: (1) as Compositional Knowledge, an averaged representation that provides rich semantics, especially beneficial when class names are ambiguous or unfamiliar to the VLM; and (2) as Instance-Specific Knowledge, where a lightweight, non-parametric attention mechanism dynamically selects the most relevant descriptions for a given image. This approach provides two additional types of knowledge alongside the handcrafted prompt, thereby facilitating category distinction across various domains. Also, ADK acts as a parameter-free, plug-and-play component that enhances existing PEFT methods. Extensive experiments demonstrate that ADK consistently boosts the performance of multiple PEFT baselines, setting a new state-of-the-art across various scenarios.

