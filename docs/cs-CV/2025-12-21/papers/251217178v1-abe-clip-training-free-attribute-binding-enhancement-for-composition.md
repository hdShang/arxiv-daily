---
layout: default
title: "ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching"
---

# ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17178" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17178v1</a>
  <a href="https://arxiv.org/pdf/2512.17178.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17178v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17178v1', 'ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen

**åˆ†ç±»**: cs.CV, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

**å¤‡æ³¨**: 10 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ABE-CLIPï¼šå…è®­ç»ƒçš„å±æ€§ç»‘å®šå¢å¼ºæ–¹æ³•ï¼Œæå‡ç»„åˆå›¾åƒ-æ–‡æœ¬åŒ¹é…æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»„åˆå›¾åƒ-æ–‡æœ¬åŒ¹é…` `å±æ€§ç»‘å®š` `CLIP` `å…è®­ç»ƒ` `è¯­ä¹‰ç»†åŒ–` `å±€éƒ¨å¯¹é½` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CLIPåœ¨ç»„åˆå›¾åƒ-æ–‡æœ¬åŒ¹é…ä¸­ï¼Œéš¾ä»¥å‡†ç¡®å…³è”å¯¹è±¡åŠå…¶å±æ€§ï¼Œå› å…¶å…¨å±€è¡¨ç¤ºå¿½ç•¥äº†ç»†ç²’åº¦è¯­ä¹‰ã€‚
2. ABE-CLIPé€šè¿‡è¯­ä¹‰ç»†åŒ–æœºåˆ¶å’Œå±€éƒ¨token-patchå¯¹é½ç­–ç•¥ï¼Œå¢å¼ºCLIPæ¨¡å‹ä¸­çš„å±æ€§-å¯¹è±¡ç»‘å®šã€‚
3. å®éªŒè¡¨æ˜ï¼ŒABE-CLIPæ˜¾è‘—æå‡äº†å±æ€§-å¯¹è±¡ç»‘å®šæ€§èƒ½ï¼Œä¼˜äºéœ€è¦é¢å¤–è®­ç»ƒçš„æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å„ç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒåœ¨ç»„åˆå›¾åƒ-æ–‡æœ¬åŒ¹é…æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®åœ°å°†å¯¹è±¡ä¸å…¶å¯¹åº”çš„å±æ€§ç›¸å…³è”æ—¶ï¼Œå› ä¸ºå®ƒå›ºæœ‰çš„å…¨å±€è¡¨ç¤ºé€šå¸¸å¿½ç•¥äº†å±æ€§ç»‘å®šçš„ç»†ç²’åº¦è¯­ä¹‰ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–å¤§é‡çš„å›°éš¾è´Ÿæ ·æœ¬é‡‡æ ·ï¼Œä½†å®ƒä»¬ç»å¸¸è¡¨ç°å‡ºå¯¹æ–°ç»„åˆæ¦‚å¿µçš„æœ‰é™æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”æœªèƒ½ä»æ ¹æœ¬ä¸Šè§£å†³å…¨å±€è¡¨ç¤ºçš„ç¼ºç‚¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…è®­ç»ƒå±æ€§ç»‘å®šå¢å¼ºæ–¹æ³•ABE-CLIPï¼Œæ—¨åœ¨åŠ å¼ºç±»CLIPæ¨¡å‹ä¸­çš„å±æ€§-å¯¹è±¡ç»‘å®šã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨è¯­ä¹‰ç»†åŒ–æœºåˆ¶æ¥ç»†åŒ–æ–‡æœ¬ä¸­å¯¹è±¡å’Œå±æ€§çŸ­è¯­çš„tokenåµŒå…¥ï¼Œä»è€Œå‡è½»å±æ€§æ··æ·†å¹¶æé«˜è¯­ä¹‰ç²¾åº¦ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§å±€éƒ¨token-patchå¯¹é½ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è®¡ç®—ç»†åŒ–çš„æ–‡æœ¬tokenä¸å…¶æœ€ç›¸å…³çš„å›¾åƒpatchä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚é€šè¿‡èšåˆå±€éƒ¨ç›¸ä¼¼åº¦å¾—åˆ†ï¼ŒABE-CLIPè®¡ç®—æœ€ç»ˆçš„å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒABE-CLIPæ˜¾è‘—æé«˜äº†å±æ€§-å¯¹è±¡ç»‘å®šæ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†éœ€è¦å¤§é‡è®­ç»ƒçš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šCLIPåœ¨ç»„åˆå›¾åƒ-æ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å±æ€§ç»‘å®šæ–¹é¢ã€‚å…¶å…¨å±€è¡¨ç¤ºæ–¹æ³•éš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®åœ°å°†å›¾åƒä¸­çš„å¯¹è±¡ä¸å…¶å¯¹åº”çš„æ–‡æœ¬å±æ€§å…³è”èµ·æ¥ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–å¤æ‚çš„è´Ÿæ ·æœ¬æŒ–æ˜ï¼Œä½†æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œä¸”æœªèƒ½ä»æ ¹æœ¬ä¸Šè§£å†³å…¨å±€è¡¨ç¤ºçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šABE-CLIPçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢å¼ºæ–‡æœ¬tokençš„è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶å»ºç«‹æ–‡æœ¬tokenä¸å›¾åƒpatchä¹‹é—´çš„å±€éƒ¨å¯¹åº”å…³ç³»ï¼Œä»è€Œæå‡å±æ€§-å¯¹è±¡ç»‘å®šçš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œç›´æ¥ä½œç”¨äºé¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œæ—¨åœ¨å¼¥è¡¥CLIPåœ¨ç»†ç²’åº¦è¯­ä¹‰ç†è§£æ–¹é¢çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šABE-CLIPä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šè¯­ä¹‰ç»†åŒ–æœºåˆ¶ï¼ˆSemantic Refinement Mechanismï¼‰å’Œå±€éƒ¨token-patchå¯¹é½ç­–ç•¥ï¼ˆLocal Token-Patch Alignmentï¼‰ã€‚é¦–å…ˆï¼Œè¯­ä¹‰ç»†åŒ–æœºåˆ¶ç”¨äºå¢å¼ºæ–‡æœ¬ä¸­å¯¹è±¡å’Œå±æ€§çŸ­è¯­çš„tokenåµŒå…¥ï¼Œå‡å°‘å±æ€§æ··æ·†ã€‚ç„¶åï¼Œå±€éƒ¨token-patchå¯¹é½ç­–ç•¥è®¡ç®—ç»†åŒ–åçš„æ–‡æœ¬tokenä¸å›¾åƒpatchä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå¹¶èšåˆè¿™äº›å±€éƒ¨ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œå¾—åˆ°æœ€ç»ˆçš„å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šABE-CLIPçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…è®­ç»ƒçš„å±æ€§ç»‘å®šå¢å¼ºæ–¹æ³•ã€‚ä¸éœ€è¦é¢å¤–è®­ç»ƒæˆ–å¤æ‚è´Ÿæ ·æœ¬æŒ–æ˜çš„æ–¹æ³•ä¸åŒï¼ŒABE-CLIPç›´æ¥ä½œç”¨äºé¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œé€šè¿‡è¯­ä¹‰ç»†åŒ–å’Œå±€éƒ¨å¯¹é½æ¥æå‡æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•æ›´é«˜æ•ˆï¼Œä¸”å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯­ä¹‰ç»†åŒ–æœºåˆ¶çš„å…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯æé«˜å¯¹è±¡å’Œå±æ€§tokenåµŒå…¥çš„è¯­ä¹‰ç²¾åº¦ã€‚å±€éƒ¨token-patchå¯¹é½ç­–ç•¥çš„å…³é”®åœ¨äºå¦‚ä½•é€‰æ‹©ä¸æ–‡æœ¬tokenæœ€ç›¸å…³çš„å›¾åƒpatchï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°èšåˆå±€éƒ¨ç›¸ä¼¼åº¦å¾—åˆ†ã€‚è®ºæ–‡ä¸­å¯èƒ½ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•æ¥å®ç°token-patchçš„å¯¹é½ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ­¤å¤„æ— æ³•å¾—çŸ¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17178v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17178v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17178v1/images/yellow_cylinder_and_red_cub.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

ABE-CLIPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å±æ€§-å¯¹è±¡ç»‘å®šæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œå³å¯è¶…è¶Šéœ€è¦å¤§é‡è®­ç»ƒçš„åŸºçº¿æ–¹æ³•ï¼Œæ˜¾ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ABE-CLIPå¯åº”ç”¨äºå›¾åƒæ£€ç´¢ã€è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç†è§£å›¾åƒä¸­å¯¹è±¡åŠå…¶å±æ€§çš„ä»»åŠ¡ä¸­ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”µå•†é¢†åŸŸï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥çš„å±æ€§æè¿°ï¼ˆå¦‚â€œçº¢è‰²çš„è¿è¡£è£™â€ï¼‰æ£€ç´¢åˆ°ç¬¦åˆæ¡ä»¶çš„å•†å“ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡å¤šæ¨¡æ€ç†è§£çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.

