---
layout: default
title: Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images
---

# Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17306" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17306v1</a>
  <a href="https://arxiv.org/pdf/2512.17306.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17306v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17306v1', 'Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenhao Yang, Yu Xia, Jinlong Huang, Shiyin Lu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Yuanyu Wan, Lijun Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRIMæ¨¡å‹ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ¨ç†ä¸­çš„å¤šè½®è‡ªåæ€èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤šè½®æ¨ç†` `è‡ªåæ€` `å¼ºåŒ–å­¦ä¹ ` `æ€ç»´é“¾` `å›¾åƒç†è§£` `å†—ä½™æƒ©ç½š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­ï¼Œéš¾ä»¥åæ€å’Œçº æ­£é”™è¯¯çš„æ¨ç†è½¨è¿¹ã€‚
2. DRIMæ¨¡å‹é€šè¿‡æ•°æ®æ„å»ºã€å†·å¯åŠ¨SFTå’Œå¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªé˜¶æ®µï¼Œå®ç°æ·±åº¦ä¸”å¯é çš„å¤šè½®æ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDRIMæ¨¡å‹åœ¨è§†è§‰ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºDRIMæ¨¡å‹ï¼Œæ—¨åœ¨æå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨å›¾åƒæ¨ç†ä¸­è¿›è¡Œå¤šè½®è‡ªåæ€çš„èƒ½åŠ›ã€‚ç°æœ‰æ¨¡å‹åœ¨è¿›è¡ŒåŸºäºå›¾åƒçš„æ€ç»´é“¾(CoT)æ¨ç†æ—¶ï¼Œéš¾ä»¥åæ€å’Œçº æ­£é”™è¯¯çš„æ¨ç†è½¨è¿¹ã€‚DRIMæ¨¡å‹é€šè¿‡ä¸‰é˜¶æ®µæµç¨‹è§£å†³æ­¤é—®é¢˜ï¼šæ•°æ®æ„å»ºã€å†·å¯åŠ¨SFTå’Œå¼ºåŒ–å­¦ä¹ ã€‚é¦–å…ˆï¼ŒåŸºäºé«˜åˆ†è¾¨ç‡å›¾åƒæ•°æ®é›†ï¼Œæ„å»ºé«˜éš¾åº¦ä¸”å¯éªŒè¯çš„è§†è§‰é—®ç­”å¯¹ï¼Œæ¯ä¸ªä»»åŠ¡éœ€è¦å¤šè½®å·¥å…·è°ƒç”¨æ‰èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆã€‚åœ¨SFTé˜¶æ®µï¼Œæ”¶é›†å·¥å…·è½¨è¿¹ä½œä¸ºå†·å¯åŠ¨æ•°æ®ï¼Œå¼•å¯¼å¤šè½®æ¨ç†æ¨¡å¼ã€‚åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œå¼•å…¥å†—ä½™æƒ©ç½šç­–ç•¥ä¼˜åŒ–ï¼Œæ¿€åŠ±æ¨¡å‹å‘å±•è‡ªåæ€æ¨ç†æ¨¡å¼ï¼Œå¯¹äº§ç”Ÿé”™è¯¯ç­”æ¡ˆä¸”ç¼ºä¹å……åˆ†å¤šå°ºåº¦æ¢ç´¢çš„æ¨ç†è½¨è¿¹è¿›è¡Œæƒ©ç½šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRIMåœ¨è§†è§‰ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿›è¡Œå¤šè½®å›¾åƒæ¨ç†æ—¶ï¼Œç¼ºä¹è‡ªåæ€å’Œçº é”™èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸€æ—¦å‡ºç°é”™è¯¯ï¼Œå¾ˆéš¾å›æº¯å¹¶ä¿®æ­£ï¼Œå¯¼è‡´æœ€ç»ˆç»“æœé”™è¯¯ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºæ¨¡å‹ç¼ºä¹å¯¹è‡ªèº«æ¨ç†è¿‡ç¨‹çš„è¯„ä¼°å’Œæ”¹è¿›æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®©æ¨¡å‹å…·å¤‡â€œæ·±åº¦â€å’Œâ€œå¯é æ€§â€çš„å¤šè½®æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è‡ªåæ€æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¯„ä¼°è‡ªèº«çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨å‘ç°é”™è¯¯æ—¶è¿›è¡Œçº æ­£ã€‚è¿™ç§è‡ªåæ€èƒ½åŠ›é€šè¿‡å†—ä½™æƒ©ç½šç­–ç•¥ä¼˜åŒ–æ¥å®ç°ï¼Œé¼“åŠ±æ¨¡å‹è¿›è¡Œå¤šå°ºåº¦çš„æ¢ç´¢ï¼Œå¹¶å¯¹ä¸å……åˆ†çš„æ¢ç´¢è¿›è¡Œæƒ©ç½šã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDRIMæ¨¡å‹åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ•°æ®æ„å»ºï¼šæ„å»ºé«˜éš¾åº¦ã€å¯éªŒè¯çš„è§†è§‰é—®ç­”å¯¹ï¼Œéœ€è¦å¤šè½®å·¥å…·è°ƒç”¨æ‰èƒ½è§£å†³ã€‚2) å†·å¯åŠ¨SFTï¼šåˆ©ç”¨æ”¶é›†åˆ°çš„å·¥å…·è½¨è¿¹æ•°æ®ï¼Œå¯¹æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ å¤šè½®æ¨ç†æ¨¡å¼ã€‚3) å¼ºåŒ–å­¦ä¹ ï¼šå¼•å…¥å†—ä½™æƒ©ç½šç­–ç•¥ä¼˜åŒ–ï¼Œè®­ç»ƒæ¨¡å‹è¿›è¡Œè‡ªåæ€æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šDRIMçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†å†—ä½™æƒ©ç½šç­–ç•¥ä¼˜åŒ–ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æ¿€åŠ±æ¨¡å‹å‘å±•è‡ªåæ€æ¨ç†æ¨¡å¼ã€‚é€šè¿‡å¯¹æ¨ç†è½¨è¿¹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶å¯¹é‚£äº›äº§ç”Ÿé”™è¯¯ç­”æ¡ˆä¸”ç¼ºä¹å……åˆ†å¤šå°ºåº¦æ¢ç´¢çš„è½¨è¿¹è¿›è¡Œæƒ©ç½šï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¼šè¯†åˆ«å’Œçº æ­£è‡ªèº«çš„é”™è¯¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ„å»ºé˜¶æ®µï¼Œè®ºæ–‡è®¾è®¡äº†é«˜éš¾åº¦ã€å¯éªŒè¯çš„è§†è§‰é—®ç­”å¯¹ï¼Œç¡®ä¿æ¨¡å‹éœ€è¦è¿›è¡Œå¤šè½®æ¨ç†æ‰èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆã€‚åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œå†—ä½™æƒ©ç½šç­–ç•¥ä¼˜åŒ–çš„å…·ä½“å®ç°æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œå¦‚ä½•å®šä¹‰â€œå¤šå°ºåº¦æ¢ç´¢â€å’Œâ€œæƒ©ç½šâ€ï¼‰ä»¥åŠç›¸å…³çš„è¶…å‚æ•°è®¾ç½®æ˜¯å…³é”®çš„æŠ€æœ¯ç»†èŠ‚ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦å¹³è¡¡å¥–åŠ±å’Œæƒ©ç½šï¼Œä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ åˆ°æœ‰æ•ˆçš„è‡ªåæ€æ¨ç†ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DRIMæ¨¡å‹åœ¨è§†è§‰ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚DRIMæ¨¡å‹é€šè¿‡å¼•å…¥è‡ªåæ€æœºåˆ¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å›¾åƒå†…å®¹ï¼Œå¹¶è¿›è¡Œæ›´å‡†ç¡®çš„æ¨ç†ï¼Œä»è€Œåœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DRIMæ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ¨¡å‹éœ€è¦å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œå¯é æ€§ï¼Œæ‰èƒ½åšå‡ºå‡†ç¡®çš„å†³ç­–ã€‚DRIMæ¨¡å‹é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šè½®è‡ªåæ€èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨è¿™äº›é¢†åŸŸçš„åº”ç”¨æ•ˆæœï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.

