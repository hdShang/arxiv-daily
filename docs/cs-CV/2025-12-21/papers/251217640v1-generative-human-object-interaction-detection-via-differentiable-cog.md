---
layout: default
title: Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs
---

# Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17640" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17640v1</a>
  <a href="https://arxiv.org/pdf/2512.17640.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17640v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17640v1', 'Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaolin Cai, Huiyu Duan, Zitong Xu, Fan Li, Zhi Liu, Jing Liu, Wei Shen, Xiongkuo Min, Guangtao Zhai

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRASP-HOæ¡†æ¶ï¼Œé€šè¿‡å¯å¾®åˆ†è®¤çŸ¥å¼•å¯¼å¤šæ¨¡æ€LLMå®ç°ç”Ÿæˆå¼äºº-ç‰©äº¤äº’æ£€æµ‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äºº-ç‰©äº¤äº’æ£€æµ‹` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç”Ÿæˆå¼æ¨¡å‹` `å¼€æ”¾è¯æ±‡` `è®¤çŸ¥å¼•å¯¼` `é›¶æ ·æœ¬å­¦ä¹ ` `æ··åˆæŒ‡å¯¼ç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰HOIæ£€æµ‹æ–¹æ³•å—é™äºé¢„å®šä¹‰çš„åŠ¨è¯é›†åˆï¼Œéš¾ä»¥å¤„ç†çœŸå®åœºæ™¯ä¸­å¤æ‚å¤šå˜çš„äº¤äº’ã€‚
2. GRASP-HOæ¡†æ¶å°†HOIæ£€æµ‹é‡æ„ä¸ºå¼€æ”¾è¯æ±‡ç”Ÿæˆé—®é¢˜ï¼Œåˆ©ç”¨MLLMçš„çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚
3. é€šè¿‡å¯å­¦ä¹ çš„è®¤çŸ¥å¼•å¯¼æ¨¡å—å’Œæ··åˆæŒ‡å¯¼ç­–ç•¥ï¼ŒGRASP-HOåœ¨å°é—­é›†å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äºº-ç‰©äº¤äº’(HOI)æ£€æµ‹æ—¨åœ¨å®šä½äºº-ç‰©å¯¹ä»¥åŠå®ƒä»¬ä¹‹é—´çš„äº¤äº’ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åŸºäºå°é—­ä¸–ç•Œå‡è®¾ï¼Œå°†è¯¥ä»»åŠ¡è§†ä¸ºå¯¹é¢„å®šä¹‰åŠ¨è¯é›†åˆçš„åˆ†ç±»é—®é¢˜ï¼Œéš¾ä»¥æ³›åŒ–åˆ°çœŸå®åœºæ™¯ä¸­æœªè§æˆ–æ¨¡ç³Šçš„é•¿å°¾äº¤äº’ã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æ‹¥æœ‰å¼€æ”¾è¯æ±‡ç†è§£æ‰€éœ€çš„ä¸°å¯Œä¸–ç•ŒçŸ¥è¯†ï¼Œä½†ç”±äºå¾®è°ƒæˆæœ¬è¿‡é«˜ï¼Œå®ƒä»¬ä¸ç°æœ‰çš„HOIæ£€æµ‹å™¨è„±èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†GRASP-HOï¼Œä¸€ç§æ–°é¢–çš„ç”Ÿæˆå¼æ¨ç†å’Œå¯æ§æ„ŸçŸ¥æ¡†æ¶ï¼Œå°†HOIæ£€æµ‹ä»å°é—­é›†åˆ†ç±»ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå¼€æ”¾è¯æ±‡ç”Ÿæˆé—®é¢˜ã€‚ä¸ºäº†è¿æ¥è§†è§‰å’Œè®¤çŸ¥ï¼Œæˆ‘ä»¬é¦–å…ˆæå–æ··åˆäº¤äº’è¡¨ç¤ºï¼Œç„¶åè®¾è®¡ä¸€ä¸ªè½»é‡çº§çš„å¯å­¦ä¹ è®¤çŸ¥å¼•å¯¼æ¨¡å—(CSC)ï¼Œå°†ç»†ç²’åº¦çš„è§†è§‰è¯æ®æ³¨å…¥åˆ°å†»ç»“çš„MLLMä¸­ä»¥è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ã€‚ä¸ºäº†è§£å†³åŸºäºåˆ†ç±»çš„HOIæ•°æ®é›†å’Œå¼€æ”¾è¯æ±‡ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„ç›‘ç£ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ··åˆæŒ‡å¯¼ç­–ç•¥ï¼Œå°†è¯­è¨€å»ºæ¨¡æŸå¤±å’Œè¾…åŠ©åˆ†ç±»æŸå¤±ç›¸ç»“åˆï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²ç”Ÿæˆçµæ´»æ€§çš„æƒ…å†µä¸‹å®ç°åˆ¤åˆ«æ€§ groundingã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°é—­é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå®ç°äº†ä¸€ç§ç»Ÿä¸€çš„èŒƒä¾‹ï¼Œæ— ç¼åœ°æ¡¥æ¥äº†åˆ¤åˆ«æ€§æ„ŸçŸ¥å’Œç”Ÿæˆå¼æ¨ç†ï¼Œç”¨äºå¼€æ”¾ä¸–ç•ŒHOIæ£€æµ‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰HOIæ£€æµ‹æ–¹æ³•ä¸»è¦åŸºäºå°é—­ä¸–ç•Œå‡è®¾ï¼Œä¾èµ–äºé¢„å®šä¹‰çš„åŠ¨è¯ç±»åˆ«è¿›è¡Œåˆ†ç±»ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†çœŸå®åœºæ™¯ä¸­é•¿å°¾åˆ†å¸ƒçš„ã€æœªçŸ¥çš„æˆ–è¯­ä¹‰æ¨¡ç³Šçš„äº¤äº’è¡Œä¸ºã€‚æ­¤å¤–ï¼Œç›´æ¥å¾®è°ƒå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡ŒHOIæ£€æµ‹è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œéš¾ä»¥å®ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGRASP-HOçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†HOIæ£€æµ‹ä»»åŠ¡ä»ä¼ ç»Ÿçš„å°é—­é›†åˆ†ç±»é—®é¢˜è½¬åŒ–ä¸ºå¼€æ”¾è¯æ±‡çš„ç”Ÿæˆé—®é¢˜ï¼Œå……åˆ†åˆ©ç”¨MLLMå¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å°†è§†è§‰ä¿¡æ¯æœ‰æ•ˆåœ°æ³¨å…¥åˆ°MLLMä¸­ï¼Œå¼•å¯¼å…¶ç”Ÿæˆæè¿°äºº-ç‰©äº¤äº’çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œä»è€Œå®ç°å¯¹æœªçŸ¥äº¤äº’çš„è¯†åˆ«ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRASP-HOæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ¨¡å—ï¼š1) æ··åˆäº¤äº’è¡¨ç¤ºæå–æ¨¡å—ï¼Œç”¨äºæå–äººã€ç‰©åŠå…¶äº¤äº’çš„è§†è§‰ç‰¹å¾ï¼›2) å¯å­¦ä¹ è®¤çŸ¥å¼•å¯¼æ¨¡å—(CSC)ï¼Œç”¨äºå°†æå–çš„è§†è§‰ç‰¹å¾æ³¨å…¥åˆ°å†»ç»“çš„MLLMä¸­ï¼Œå¼•å¯¼MLLMè¿›è¡Œæ¨ç†ï¼›3) æ··åˆæŒ‡å¯¼ç­–ç•¥ï¼Œç»“åˆè¯­è¨€å»ºæ¨¡æŸå¤±å’Œè¾…åŠ©åˆ†ç±»æŸå¤±ï¼Œä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šGRASP-HOçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å°†HOIæ£€æµ‹ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå¼€æ”¾è¯æ±‡ç”Ÿæˆé—®é¢˜ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„ç±»åˆ«é™åˆ¶ï¼›2) æå‡ºäº†è½»é‡çº§çš„å¯å­¦ä¹ è®¤çŸ¥å¼•å¯¼æ¨¡å—(CSC)ï¼Œå®ç°äº†è§†è§‰ä¿¡æ¯åˆ°MLLMçš„æœ‰æ•ˆä¼ é€’ï¼Œé¿å…äº†å¯¹MLLMè¿›è¡Œæ˜‚è´µçš„å¾®è°ƒï¼›3) è®¾è®¡äº†æ··åˆæŒ‡å¯¼ç­–ç•¥ï¼Œè§£å†³äº†åˆ†ç±»æ•°æ®é›†ä¸ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„ç›‘ç£ä¸åŒ¹é…é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè®¤çŸ¥å¼•å¯¼æ¨¡å—ï¼ˆCSCï¼‰é‡‡ç”¨è½»é‡çº§ç½‘ç»œç»“æ„ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å°†è§†è§‰ç‰¹å¾èå…¥MLLMçš„è¾“å…¥ä¸­ã€‚æ··åˆæŒ‡å¯¼ç­–ç•¥ç»“åˆäº†è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆè¡¡é‡ç”Ÿæˆæ–‡æœ¬çš„æµç•…æ€§å’Œå‡†ç¡®æ€§ï¼‰å’Œè¾…åŠ©åˆ†ç±»æŸå¤±ï¼ˆåˆ©ç”¨ç°æœ‰HOIæ•°æ®é›†çš„ç±»åˆ«ä¿¡æ¯ï¼‰ï¼Œä»¥æå‡æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚å…·ä½“æŸå¤±å‡½æ•°çš„æƒé‡æ¯”ä¾‹éœ€è¦æ ¹æ®å®éªŒè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17640v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17640v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17640v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGRASP-HOåœ¨å°é—­é›†HOIæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œå¹¶åœ¨é›¶æ ·æœ¬HOIæ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒGRASP-HOåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚å…·ä½“æå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GRASP-HOæ¡†æ¶åœ¨æ™ºèƒ½ç›‘æ§ã€äººæœºäº¤äº’ã€æœºå™¨äººè§†è§‰ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºè¯†åˆ«ç›‘æ§è§†é¢‘ä¸­çš„å¼‚å¸¸è¡Œä¸ºï¼Œç†è§£äººä¸æœºå™¨äººçš„äº¤äº’æ„å›¾ï¼Œä»¥åŠå¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å’Œæ“ä½œå‘¨å›´ç¯å¢ƒä¸­çš„ç‰©ä½“ã€‚è¯¥ç ”ç©¶ä¸ºå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹çš„äºº-ç‰©äº¤äº’ç†è§£æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.

