---
layout: default
title: ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image
---

# ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17545" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17545v1</a>
  <a href="https://arxiv.org/pdf/2512.17545.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17545v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17545v1', 'ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

**å¤‡æ³¨**: 15 pages,16 figures

**DOI**: [10.1145/3731715.3733288](https://doi.org/10.1145/3731715.3733288)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/starVisionTeam/ClothHMR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºClothHMRä»¥è§£å†³å¤šæ ·æœè£…ä¸‹3Däººç±»ç½‘æ ¼æ¢å¤é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `3Dç½‘æ ¼æ¢å¤` `äººç±»è§†è§‰æ¨¡å‹` `æœè£…è£å‰ª` `å§¿æ€ä¼°è®¡` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ ` `åœ¨çº¿è´­ç‰©`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Däººç±»ç½‘æ ¼æ¢å¤æ–¹æ³•ä¸»è¦é’ˆå¯¹ç´§èº«è¡£ç‰©ï¼Œé¢å¯¹å¤šæ ·æœè£…æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯å®½æ¾è¡£ç‰©çš„èº«ä½“å½¢çŠ¶å’Œå§¿æ€ä¼°è®¡ã€‚
2. æœ¬æ–‡æå‡ºClothHMRï¼Œé€šè¿‡æœè£…è£å‰ªæ¨¡å—å’ŒåŸºäºäººç±»è§†è§‰æ¨¡å‹çš„ç½‘æ ¼æ¢å¤æ¨¡å—ï¼Œæé«˜å¤šæ ·æœè£…ä¸‹çš„3Dç½‘æ ¼æ¢å¤ç²¾åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒClothHMRåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨çœŸå®åœºæ™¯å›¾åƒä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€3Dæ•°æ®è¿…é€Ÿæˆä¸ºé‡è¦çš„å¤šåª’ä½“ä¿¡æ¯å½¢å¼ï¼Œ3Däººç±»ç½‘æ ¼æ¢å¤æŠ€æœ¯ä¹Ÿéšä¹‹å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç´§èº«è¡£ç‰©çš„å¤„ç†ä¸Šï¼Œå¯¹äºå¤šæ ·æœè£…ï¼Œå°¤å…¶æ˜¯å®½æ¾è¡£ç‰©ä¸‹çš„èº«ä½“å½¢çŠ¶å’Œå§¿æ€ä¼°è®¡æ•ˆæœè¾ƒå·®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ClothHMRï¼Œé€šè¿‡æœè£…è£å‰ªå’ŒåŸºäºäººç±»è§†è§‰æ¨¡å‹çš„ç½‘æ ¼æ¢å¤ï¼Œå‡†ç¡®æ¢å¤ç©¿ç€å¤šæ ·æœè£…çš„äººç±»3Dç½‘æ ¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClothHMRåœ¨åŸºå‡†æ•°æ®é›†å’ŒçœŸå®åœºæ™¯å›¾åƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒåŸºäºClothHMRå¼€å‘çš„åœ¨çº¿æ—¶å°šè´­ç‰©åº”ç”¨å±•ç¤ºäº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šæ ·æœè£…ä¸‹è¿›è¡Œ3Däººç±»ç½‘æ ¼æ¢å¤çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å®½æ¾è¡£ç‰©ä¸‹çš„èº«ä½“å½¢çŠ¶å’Œå§¿æ€ä¼°è®¡æ•ˆæœè¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è£å‰ªæœè£…ä»¥æ›´å¥½åœ°é€‚åº”äººä½“è½®å»“ï¼Œå¹¶åˆ©ç”¨å¤§å‹åŸºç¡€æ¨¡å‹ä¸­çš„äººç±»è§†è§‰ä¿¡æ¯æ¥å¢å¼ºä¼°è®¡çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šClothHMRä¸»è¦ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šæœè£…è£å‰ªï¼ˆCTï¼‰æ¨¡å—å’ŒåŸºäºFHVMçš„ç½‘æ ¼æ¢å¤ï¼ˆMRï¼‰æ¨¡å—ã€‚CTæ¨¡å—é€šè¿‡èº«ä½“è¯­ä¹‰ä¼°è®¡å’Œè¾¹ç¼˜é¢„æµ‹æ¥è£å‰ªæœè£…ï¼ŒMRæ¨¡å—åˆ™é€šè¿‡æŒç»­å¯¹é½3Dç½‘æ ¼çš„ä¸­é—´è¡¨ç¤ºä¸FHVMæ¨æ–­ç»“æœæ¥ä¼˜åŒ–åˆå§‹å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç»“åˆäº†æœè£…è£å‰ªä¸FHVMçš„ç½‘æ ¼æ¢å¤ï¼Œæ˜¾è‘—æé«˜äº†åœ¨å¤šæ ·æœè£…ä¸‹çš„3Dç½‘æ ¼æ¢å¤ç²¾åº¦ï¼Œè¿™æ˜¯ç°æœ‰æ–¹æ³•æ‰€æœªèƒ½å®ç°çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šCTæ¨¡å—é‡‡ç”¨äº†èº«ä½“è¯­ä¹‰ä¼°è®¡å’Œè¾¹ç¼˜é¢„æµ‹æŠ€æœ¯ï¼Œç¡®ä¿æœè£…ä¸èº«ä½“è½®å»“çš„è´´åˆï¼›MRæ¨¡å—åˆ™é€šè¿‡å¯¹é½ä¸­é—´è¡¨ç¤ºæ¥ä¼˜åŒ–ç½‘æ ¼å‚æ•°ï¼Œæå‡äº†æ¢å¤çš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17545v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17545v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17545v1/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒClothHMRåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æå‡äº†çº¦15%-20%çš„å‡†ç¡®ç‡ï¼Œå°¤å…¶åœ¨çœŸå®åœºæ™¯å›¾åƒä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ClothHMRçš„ç ”ç©¶æˆæœåœ¨æ—¶å°šè¡Œä¸šå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿä¸ºåœ¨çº¿è´­ç‰©ã€è™šæ‹Ÿè¯•è¡£ã€æ¸¸æˆè§’è‰²å»ºæ¨¡ç­‰åœºæ™¯æä¾›ç²¾å‡†çš„3Däººç±»æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯çš„å®é™…ä»·å€¼åœ¨äºæå‡ç”¨æˆ·ä½“éªŒï¼Œå¸®åŠ©æ¶ˆè´¹è€…æ›´å¥½åœ°é€‰æ‹©åˆé€‚çš„æœè£…ã€‚æœªæ¥ï¼ŒClothHMRæœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¦‚è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \url{https://github.com/starVisionTeam/ClothHMR}.

