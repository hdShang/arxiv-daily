---
layout: default
title: A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs
---

# A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17319" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17319v1</a>
  <a href="https://arxiv.org/pdf/2512.17319.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17319v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17319v1', 'A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao

**åˆ†ç±»**: cs.CV, cs.AI, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Yunkaidang/RSHR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRSHR-Benchä»¥è§£å†³é¥æ„Ÿè¶…é«˜åˆ†è¾¨ç‡è§†è§‰ç†è§£è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¥æ„Ÿå›¾åƒ` `è¶…é«˜åˆ†è¾¨ç‡` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰ç†è§£` `æ¨ç†ä»»åŠ¡` `åŸºå‡†è¯„ä¼°` `å¯¹æŠ—è¿‡æ»¤` `äººç±»éªŒè¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é¥æ„ŸåŸºå‡†å¤§å¤šä¾èµ–ä½åˆ†è¾¨ç‡å›¾åƒï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å®é™…è§†è§‰ç†è§£èƒ½åŠ›ä¸åŒ¹é…ã€‚
2. è®ºæ–‡æå‡ºRSHR-BenchåŸºå‡†ï¼ŒåŒ…å«è¶…é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶è®¾è®¡å¤šç§ä»»åŠ¡ä»¥å…¨é¢è¯„ä¼°è§†è§‰ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶…é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼ŒéªŒè¯äº†æ–°åŸºå‡†çš„å¿…è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç°æœ‰é¥æ„ŸåŸºå‡†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºå‡†ä¾èµ–äºä½åˆ†è¾¨ç‡å›¾åƒï¼Œè€Œä¸€äº›é«˜åˆ†è¾¨ç‡åŸºå‡†åœ¨æ¨ç†ä»»åŠ¡è®¾è®¡ä¸Šå­˜åœ¨ç¼ºé™·ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»…ä½¿ç”¨æ–‡æœ¬çš„LLMsåœ¨é¥æ„Ÿæ¨ç†ä»»åŠ¡ä¸­å¯ä»¥ä¸å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹ç«äº‰ï¼Œæ­ç¤ºäº†å½“å‰åŸºå‡†ä¸è§†è§‰ç†è§£è¯„ä¼°ä¹‹é—´çš„å…³é”®ä¸åŒ¹é…ã€‚ä¸ºå®ç°çœŸå®è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†RSHR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè¶…é«˜åˆ†è¾¨ç‡çš„é¥æ„Ÿè§†è§‰ç†è§£å’Œæ¨ç†åŸºå‡†ï¼ŒåŒ…å«5,329å¹…é•¿è¾¹è‡³å°‘ä¸º4,000åƒç´ çš„å…¨åœºæ™¯å›¾åƒï¼Œè®¾è®¡äº†å¤šç§ä»»åŠ¡ç±»å‹ä»¥æ”¯æŒå¤šè½®å’Œå¤šå›¾å¯¹è¯ã€‚é€šè¿‡ä¸¥æ ¼çš„äººç±»éªŒè¯ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤šä¸ªä»»åŠ¡ï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºåœ¨è¶…é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­å­˜åœ¨æŒç»­çš„æ€§èƒ½å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰é¥æ„ŸåŸºå‡†åœ¨è¶…é«˜åˆ†è¾¨ç‡å›¾åƒè¯„ä¼°ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ä½åˆ†è¾¨ç‡å›¾åƒå¯¼è‡´çš„è¯„ä¼°ä¸å‡†ç¡®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥RSHR-BenchåŸºå‡†ï¼Œæä¾›é«˜è¾¾3äº¿åƒç´ çš„å›¾åƒï¼Œè®¾è®¡å¤šæ ·åŒ–çš„ä»»åŠ¡ä»¥çœŸå®è¯„ä¼°é¥æ„Ÿè§†è§‰ç†è§£èƒ½åŠ›ï¼Œå‡å°‘å¯¹è¯­è¨€å…ˆéªŒçš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRSHR-BenchåŒ…å«äº”åƒå¤šå¹…è¶…é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œè®¾è®¡äº†å››ç±»ä»»åŠ¡ï¼šå¤šé¡¹é€‰æ‹©VQAã€å¼€æ”¾å¼VQAã€å›¾åƒæè¿°å’Œå•å›¾è¯„ä¼°ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è¶…é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤šæ ·åŒ–çš„ä»»åŠ¡è®¾è®¡ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨å¯¹æŠ—è¿‡æ»¤ä¸å¼ºå¤§çš„LLMsç»“åˆï¼Œç»è¿‡ä¸¥æ ¼çš„äººç±»éªŒè¯ï¼Œç¡®ä¿ä»»åŠ¡çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œæ„å»ºäº†3,864ä¸ªVQAä»»åŠ¡å’Œ3,913ä¸ªå›¾åƒæè¿°ä»»åŠ¡ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17319v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17319v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17319v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶…é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­è¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨RSHR-Benchä¸Šï¼ŒVQAä»»åŠ¡çš„å‡†ç¡®ç‡æå‡äº†çº¦15%ï¼ŒéªŒè¯äº†æ–°åŸºå‡†çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é¥æ„Ÿå›¾åƒåˆ†æã€ç¯å¢ƒç›‘æµ‹ã€åŸå¸‚è§„åˆ’ç­‰ã€‚é€šè¿‡æä¾›æ›´å‡†ç¡®çš„è§†è§‰ç†è§£è¯„ä¼°ï¼ŒRSHR-Benchèƒ½å¤Ÿä¿ƒè¿›ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ï¼Œæ¨åŠ¨å¤šæ¨¡æ€å­¦ä¹ çš„å‘å±•ï¼Œæå‡é¥æ„Ÿæ•°æ®çš„åˆ©ç”¨æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR

