---
layout: default
title: Recurrent Video Masked Autoencoders
---

# Recurrent Video Masked Autoencoders

**arXiv**: [2512.13684v1](https://arxiv.org/abs/2512.13684) | [PDF](https://arxiv.org/pdf/2512.13684.pdf)

**ä½œè€…**: Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira, Andrew Zisserman

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRVMï¼šä¸€ç§åŸºäºŽTransformerå¾ªçŽ¯ç¥žç»ç½‘ç»œçš„è§†é¢‘æŽ©ç è‡ªç¼–ç å™¨ï¼Œç”¨äºŽé«˜æ•ˆè§†é¢‘è¡¨å¾å­¦ä¹ ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘è¡¨å¾å­¦ä¹ ` `å¾ªçŽ¯ç¥žç»ç½‘ç»œ` `Transformer` `æŽ©ç è‡ªç¼–ç å™¨` `æ—¶ç©ºå»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†é¢‘æ¨¡åž‹åœ¨æ—¶ç©ºå»ºæ¨¡å’Œå‚æ•°æ•ˆçŽ‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶åºè§†é¢‘ç†è§£ä¸­ã€‚
2. RVMåˆ©ç”¨å¾ªçŽ¯Transformerèšåˆå›¾åƒç‰¹å¾ï¼Œé€šè¿‡æŽ©ç è‡ªç¼–ç å™¨å­¦ä¹ è§†é¢‘çš„æ—¶ç©ºç»“æž„ï¼Œå®žçŽ°é«˜æ•ˆçš„è§†é¢‘è¡¨å¾ã€‚
3. RVMåœ¨åŠ¨ä½œè¯†åˆ«ã€ç›®æ ‡è·Ÿè¸ªç­‰ä»»åŠ¡ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œå‚æ•°æ•ˆçŽ‡æ˜¾è‘—æå‡ï¼Œå¹¶èƒ½ç¨³å®šä¼ æ’­é•¿æ—¶åºç‰¹å¾ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¾ªçŽ¯è§†é¢‘æŽ©ç è‡ªç¼–ç å™¨(RVM)ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è§†é¢‘è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œå®ƒä½¿ç”¨åŸºäºŽTransformerçš„å¾ªçŽ¯ç¥žç»ç½‘ç»œæ¥èšåˆå¯†é›†å›¾åƒç‰¹å¾éšæ—¶é—´çš„å˜åŒ–ï¼Œä»Žè€Œæœ‰æ•ˆåœ°æ•èŽ·è‡ªç„¶è§†é¢‘æ•°æ®çš„æ—¶ç©ºç»“æž„ã€‚RVMé€šè¿‡éžå¯¹ç§°æŽ©ç é¢„æµ‹ä»»åŠ¡è¿›è¡Œå­¦ä¹ ï¼Œä»…éœ€è¦æ ‡å‡†çš„åƒç´ é‡å»ºç›®æ ‡ã€‚è¿™ç§è®¾è®¡äº§ç”Ÿäº†ä¸€ä¸ªé«˜æ•ˆçš„â€œé€šç”¨â€ç¼–ç å™¨ï¼šRVMåœ¨è§†é¢‘çº§åˆ«çš„ä»»åŠ¡ï¼ˆå¦‚åŠ¨ä½œè¯†åˆ«å’Œç‚¹/å¯¹è±¡è·Ÿè¸ªï¼‰ä¸Šå®žçŽ°äº†ä¸Žæœ€å…ˆè¿›çš„è§†é¢‘æ¨¡åž‹ï¼ˆä¾‹å¦‚VideoMAEï¼ŒV-JEPAï¼‰ç›¸åª²ç¾Žçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æµ‹è¯•å‡ ä½•å’Œå¯†é›†ç©ºé—´ç†è§£çš„ä»»åŠ¡ä¸Šï¼Œä¹Ÿä¼˜äºŽå›¾åƒæ¨¡åž‹ï¼ˆä¾‹å¦‚DINOv2ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRVMåœ¨å°åž‹æ¨¡åž‹æœºåˆ¶ä¸­å®žçŽ°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œè€Œæ— éœ€çŸ¥è¯†è’¸é¦ï¼Œä¸Žç«žäº‰çš„è§†é¢‘æŽ©ç è‡ªåŠ¨ç¼–ç å™¨ç›¸æ¯”ï¼Œå‚æ•°æ•ˆçŽ‡æé«˜äº†30å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜Žäº†RVMçš„å¾ªçŽ¯ç‰¹æ€§å…è®¸åœ¨è¾ƒé•¿çš„æ—¶é—´èŒƒå›´å†…è¿›è¡Œç¨³å®šçš„ç‰¹å¾ä¼ æ’­ï¼Œä¸”è®¡ç®—æˆæœ¬å‘ˆçº¿æ€§å¢žé•¿ï¼Œå…‹æœäº†æ ‡å‡†åŸºäºŽæ—¶ç©ºæ³¨æ„åŠ›çš„æž¶æž„çš„ä¸€äº›å±€é™æ€§ã€‚æœ€åŽï¼Œæˆ‘ä»¬ä½¿ç”¨å®šæ€§å¯è§†åŒ–æ¥çªå‡ºæ˜¾ç¤ºRVMå­¦ä¹ äº†ä¸°å¯Œçš„åœºæ™¯è¯­ä¹‰ã€ç»“æž„å’Œè¿åŠ¨è¡¨ç¤ºã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†é¢‘æ¨¡åž‹ï¼Œå¦‚VideoMAEå’ŒV-JEPAï¼Œåœ¨è®¡ç®—æˆæœ¬å’Œå‚æ•°æ•ˆçŽ‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é•¿æ—¶ç¨‹è§†é¢‘æ—¶ï¼ŒåŸºäºŽæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡åž‹è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚æ­¤å¤–ï¼Œå¦‚ä½•å­¦ä¹ åˆ°æ—¢èƒ½ç”¨äºŽè§†é¢‘ç†è§£ï¼Œåˆèƒ½ç”¨äºŽå›¾åƒç†è§£çš„é€šç”¨è¡¨å¾ä¹Ÿæ˜¯ä¸€ä¸ªé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRVMçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¾ªçŽ¯ç¥žç»ç½‘ç»œ(RNN)æ¥èšåˆè§†é¢‘å¸§çš„ç‰¹å¾ï¼Œä»Žè€Œæœ‰æ•ˆåœ°æ•èŽ·è§†é¢‘çš„æ—¶ç©ºç»“æž„ã€‚é€šè¿‡ç»“åˆTransformerçš„å¼ºå¤§è¡¨å¾èƒ½åŠ›å’ŒRNNçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼ŒRVMèƒ½å¤Ÿåœ¨é•¿æ—¶ç¨‹è§†é¢‘ä¸­è¿›è¡Œæœ‰æ•ˆçš„ç‰¹å¾ä¼ æ’­ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½Žçš„è®¡ç®—å¤æ‚åº¦ã€‚æŽ©ç è‡ªç¼–ç å™¨(MAE)çš„éžå¯¹ç§°ç»“æž„ç”¨äºŽé«˜æ•ˆçš„é¢„è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šRVMçš„æ•´ä½“æž¶æž„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å›¾åƒç‰¹å¾æå–å™¨ï¼šç”¨äºŽæå–è§†é¢‘å¸§çš„å¯†é›†å›¾åƒç‰¹å¾ã€‚å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ¨¡åž‹ï¼Œå¦‚DINOv2ã€‚2) å¾ªçŽ¯Transformerç¼–ç å™¨ï¼šè¯¥æ¨¡å—æ˜¯RVMçš„æ ¸å¿ƒï¼Œå®ƒä½¿ç”¨å¾ªçŽ¯ç¥žç»ç½‘ç»œæ¥èšåˆå›¾åƒç‰¹å¾éšæ—¶é—´çš„å˜åŒ–ã€‚Transformerç”¨äºŽå¢žå¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚3) æŽ©ç ç­–ç•¥ï¼šé‡‡ç”¨éžå¯¹ç§°æŽ©ç ç­–ç•¥ï¼Œå³ç¼–ç å™¨åªå¤„ç†æœªè¢«æŽ©ç çš„å¸§ï¼Œè€Œè§£ç å™¨åˆ™éœ€è¦é‡å»ºæ‰€æœ‰å¸§ã€‚4) é‡å»ºæŸå¤±ï¼šä½¿ç”¨åƒç´ é‡å»ºæŸå¤±ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œé¼“åŠ±æ¨¡åž‹å­¦ä¹ è§†é¢‘çš„æ—¶ç©ºç»“æž„ã€‚

**å…³é”®åˆ›æ–°**ï¼šRVMçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†å¾ªçŽ¯ç¥žç»ç½‘ç»œä¸ŽTransformerç›¸ç»“åˆï¼Œç”¨äºŽè§†é¢‘è¡¨å¾å­¦ä¹ ã€‚è¿™ç§ç»“åˆå…‹æœäº†ä¼ ç»ŸåŸºäºŽæ³¨æ„åŠ›æœºåˆ¶çš„è§†é¢‘æ¨¡åž‹çš„è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼ŒåŒæ—¶å®žçŽ°äº†é•¿æ—¶ç¨‹è§†é¢‘çš„æœ‰æ•ˆå»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒRVMçš„éžå¯¹ç§°æŽ©ç ç­–ç•¥å’Œåƒç´ é‡å»ºç›®æ ‡ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ åˆ°é€šç”¨çš„è§†é¢‘è¡¨å¾ï¼Œæ—¢èƒ½ç”¨äºŽè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œåˆèƒ½ç”¨äºŽå›¾åƒç†è§£ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šRVMçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¾ªçŽ¯Transformerçš„ç»“æž„ï¼šå…·ä½“RNNå•å…ƒçš„é€‰æ‹©ï¼ˆå¦‚GRUæˆ–LSTMï¼‰ä»¥åŠTransformerçš„å±‚æ•°å’Œå¤´æ•°ã€‚2) æŽ©ç æ¯”ä¾‹ï¼šæŽ§åˆ¶éœ€è¦æŽ©ç çš„å¸§çš„æ¯”ä¾‹ï¼Œé€šå¸¸è®¾ç½®ä¸ºè¾ƒé«˜çš„å€¼ï¼ˆå¦‚70%-90%ï¼‰ä»¥æé«˜å­¦ä¹ æ•ˆçŽ‡ã€‚3) æŸå¤±å‡½æ•°ï¼šåƒç´ é‡å»ºæŸå¤±çš„å…·ä½“å½¢å¼ï¼Œå¦‚L1æˆ–L2æŸå¤±ã€‚4) è®­ç»ƒç­–ç•¥ï¼šåŒ…æ‹¬å­¦ä¹ çŽ‡ã€batch sizeå’Œä¼˜åŒ–å™¨ç­‰å‚æ•°çš„è®¾ç½®ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

RVMåœ¨åŠ¨ä½œè¯†åˆ«å’Œç›®æ ‡è·Ÿè¸ªç­‰è§†é¢‘ä»»åŠ¡ä¸Šå–å¾—äº†ä¸Žæœ€å…ˆè¿›æ¨¡åž‹ï¼ˆå¦‚VideoMAEå’ŒV-JEPAï¼‰ç›¸åª²ç¾Žçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å‡ ä½•å’Œå¯†é›†ç©ºé—´ç†è§£çš„å›¾åƒä»»åŠ¡ä¸Šä¼˜äºŽDINOv2ã€‚RVMåœ¨å°æ¨¡åž‹æœºåˆ¶ä¸‹è¡¨çŽ°å‡ºè‰²ï¼Œæ— éœ€çŸ¥è¯†è’¸é¦ï¼Œå‚æ•°æ•ˆçŽ‡æ¯”å…¶ä»–è§†é¢‘æŽ©ç è‡ªç¼–ç å™¨é«˜30å€ã€‚RVMèƒ½å¤Ÿç¨³å®šåœ°ä¼ æ’­é•¿æ—¶ç¨‹ç‰¹å¾ï¼Œè®¡ç®—æˆæœ¬å‘ˆçº¿æ€§å¢žé•¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

RVMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è§†é¢‘ç›‘æŽ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è§†é¢‘ç¼–è¾‘å’Œå†…å®¹åˆ†æžç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„è§†é¢‘è¡¨å¾å­¦ä¹ èƒ½åŠ›å¯ä»¥ç”¨äºŽæå‡è¿™äº›åº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æŽ§ä¸­è¿›è¡Œå¼‚å¸¸è¡Œä¸ºæ£€æµ‹ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­è¿›è¡Œåœºæ™¯ç†è§£å’Œé¢„æµ‹ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­è¿›è¡Œè§†è§‰å®šä½å’Œè·¯å¾„è§„åˆ’ã€‚RVMçš„é€šç”¨æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è§†è§‰ä»»åŠ¡ï¼Œå…·æœ‰å¾ˆé«˜çš„å®žé™…åº”ç”¨ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

