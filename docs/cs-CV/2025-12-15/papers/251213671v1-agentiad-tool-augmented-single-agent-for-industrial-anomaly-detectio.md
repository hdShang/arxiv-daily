---
layout: default
title: AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection
---

# AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13671" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13671v1</a>
  <a href="https://arxiv.org/pdf/2512.13671.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13671v1" onclick="toggleFavorite(this, '2512.13671v1', 'AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junwen Miao, Penghui Du, Yi Liu, Yu Wang, Yan Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AgentIADï¼šå·¥å…·å¢å¼ºçš„å•æ™ºèƒ½ä½“å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å·¥ä¸šå¼‚å¸¸æ£€æµ‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ™ºèƒ½ä½“` `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·å¢å¼º` `å¤šé˜¶æ®µæ£€æŸ¥` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸­ï¼Œæ­£å¸¸æ ·æœ¬å°‘ä¸”ç¼ºé™·ç»†å¾®ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯†åˆ«ã€‚
2. AgentIADåˆ©ç”¨å·¥å…·å¢å¼ºçš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡å¤šé˜¶æ®µæ£€æŸ¥å’Œæ¯”è¾ƒæ­£å¸¸æ ·æœ¬æ¥æ£€æµ‹å¼‚å¸¸ã€‚
3. AgentIADåœ¨MMADæ•°æ®é›†ä¸Šå–å¾—äº†97.62%çš„åˆ†ç±»ç²¾åº¦ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å·¥ä¸šå¼‚å¸¸æ£€æµ‹(IAD)é¢ä¸´æ­£å¸¸æ ·æœ¬ç¨€ç¼ºå’Œç¼ºé™·ç»†å¾®å±€éƒ¨çš„æŒ‘æˆ˜ã€‚å•æ¬¡è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)å¸¸å¿½ç•¥å°å¼‚å¸¸ï¼Œç¼ºä¹ä¸æ ‡å‡†æ­£å¸¸æ¨¡å¼æ¯”è¾ƒçš„æœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºAgentIADï¼Œä¸€ä¸ªå·¥å…·é©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ç°å¤šé˜¶æ®µè§†è§‰æ£€æŸ¥ã€‚æ™ºèƒ½ä½“é…å¤‡æ„ŸçŸ¥ç¼©æ”¾å™¨(PZ)è¿›è¡Œå±€éƒ¨ç»†ç²’åº¦åˆ†æï¼Œä»¥åŠæ¯”è¾ƒæ£€ç´¢å™¨(CR)åœ¨è¯æ®æ¨¡ç³Šæ—¶æŸ¥è¯¢æ­£å¸¸æ ·æœ¬ã€‚ä¸ºè®­ç»ƒæ£€æŸ¥è¡Œä¸ºï¼Œæˆ‘ä»¬ä»MMADæ•°æ®é›†æ„å»ºç»“æ„åŒ–çš„æ„ŸçŸ¥å’Œæ¯”è¾ƒè½¨è¿¹ï¼Œå¹¶åˆ†ä¸¤é˜¶æ®µè®­ç»ƒï¼šç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚åŒé‡å¥–åŠ±è®¾è®¡é©±åŠ¨æ­¤è¿‡ç¨‹ï¼šæ„ŸçŸ¥å¥–åŠ±ç›‘ç£åˆ†ç±»ç²¾åº¦ã€ç©ºé—´å¯¹é½å’Œç±»å‹æ­£ç¡®æ€§ï¼Œè¡Œä¸ºå¥–åŠ±é¼“åŠ±é«˜æ•ˆå·¥å…·ä½¿ç”¨ã€‚è¿™äº›ç»„ä»¶å…±åŒä½¿æ¨¡å‹é€šè¿‡é€æ­¥è§‚å¯Ÿã€ç¼©æ”¾å’ŒéªŒè¯æ¥æ”¹è¿›åˆ¤æ–­ã€‚AgentIADåœ¨MMADä¸Šè¾¾åˆ°97.62%çš„åˆ†ç±»ç²¾åº¦ï¼Œè¶…è¶Šäº†å…ˆå‰çš„åŸºäºMLLMçš„æ–¹æ³•ï¼Œå¹¶äº§ç”Ÿé€æ˜ä¸”å¯è§£é‡Šçš„æ£€æŸ¥è½¨è¿¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå·¥ä¸šå¼‚å¸¸æ£€æµ‹ä»»åŠ¡æ—¨åœ¨è¯†åˆ«ç”Ÿäº§çº¿ä¸Šçš„ç¼ºé™·äº§å“ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å•æ¬¡è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)ï¼Œåœ¨å¤„ç†ç»†å¾®ã€å±€éƒ¨å¼‚å¸¸æ—¶è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹ä¸æ­£å¸¸æ ·æœ¬è¿›è¡Œæ˜¾å¼æ¯”è¾ƒçš„æœºåˆ¶ï¼Œå®¹æ˜“å¿½ç•¥å…³é”®ç¼ºé™·ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ­£å¸¸æ ·æœ¬ä¿¡æ¯ï¼Œå¹¶è®¾è®¡èƒ½å¤Ÿèšç„¦ç»†å¾®å¼‚å¸¸çš„æ£€æµ‹æµç¨‹ï¼Œæ˜¯æœ¬è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªå·¥å…·å¢å¼ºçš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡å¤šé˜¶æ®µçš„è§†è§‰æ£€æŸ¥æµç¨‹æ¥æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„æ£€æµ‹è¿‡ç¨‹ã€‚æ™ºèƒ½ä½“å¯ä»¥åˆ©ç”¨â€œæ„ŸçŸ¥ç¼©æ”¾å™¨â€èšç„¦å±€éƒ¨åŒºåŸŸï¼Œå¹¶åˆ©ç”¨â€œæ¯”è¾ƒæ£€ç´¢å™¨â€æŸ¥è¯¢æ­£å¸¸æ ·æœ¬è¿›è¡Œå¯¹æ¯”ï¼Œä»è€Œæ›´å‡†ç¡®åœ°åˆ¤æ–­æ˜¯å¦å­˜åœ¨å¼‚å¸¸ã€‚è¿™ç§è®¾è®¡å€Ÿé‰´äº†äººç±»ä¸“å®¶é€æ­¥è§‚å¯Ÿã€æ”¾å¤§ç»†èŠ‚ã€å¯¹æ¯”å‚è€ƒçš„æ£€æµ‹ä¹ æƒ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAgentIADæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) **Perceptive Zoomer (PZ)**ï¼šç”¨äºå¯¹å›¾åƒçš„å±€éƒ¨åŒºåŸŸè¿›è¡Œç»†ç²’åº¦åˆ†æã€‚2) **Comparative Retriever (CR)**ï¼šç”¨äºä»æ­£å¸¸æ ·æœ¬åº“ä¸­æ£€ç´¢ç›¸ä¼¼çš„æ ·æœ¬è¿›è¡Œæ¯”è¾ƒã€‚3) **Agent**ï¼šè´Ÿè´£æ§åˆ¶PZå’ŒCRçš„ä½¿ç”¨ï¼Œå¹¶æ ¹æ®è§‚å¯Ÿç»“æœåšå‡ºåˆ¤æ–­ã€‚è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨ç›‘ç£å­¦ä¹ å¯¹æ™ºèƒ½ä½“è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶åˆæ­¥å…·å¤‡æ„ŸçŸ¥å’Œæ¯”è¾ƒèƒ½åŠ›ï¼›ç„¶åï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„è¡Œä¸ºç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·¥å…·è¿›è¡Œæ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†å·¥å…·å¢å¼ºçš„æ™ºèƒ½ä½“å¼•å…¥å·¥ä¸šå¼‚å¸¸æ£€æµ‹é¢†åŸŸã€‚ä¸ä¼ ç»Ÿçš„å•æ¬¡VLMæ–¹æ³•ç›¸æ¯”ï¼ŒAgentIADèƒ½å¤Ÿé€šè¿‡å¤šé˜¶æ®µçš„æ£€æŸ¥æµç¨‹ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å±€éƒ¨ä¿¡æ¯å’Œæ­£å¸¸æ ·æœ¬ä¿¡æ¯ï¼Œä»è€Œæé«˜æ£€æµ‹ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒAgentIADçš„æ£€æŸ¥è¿‡ç¨‹æ˜¯é€æ˜ä¸”å¯è§£é‡Šçš„ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´è¯¦ç»†çš„å¼‚å¸¸è¯Šæ–­ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªåŒé‡å¥–åŠ±æœºåˆ¶ï¼š1) **æ„ŸçŸ¥å¥–åŠ±**ï¼šç”¨äºç›‘ç£åˆ†ç±»ç²¾åº¦ã€ç©ºé—´å¯¹é½å’Œç±»å‹æ­£ç¡®æ€§ï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«å¼‚å¸¸ç±»å‹å’Œä½ç½®ã€‚2) **è¡Œä¸ºå¥–åŠ±**ï¼šç”¨äºé¼“åŠ±æ™ºèƒ½ä½“é«˜æ•ˆåœ°ä½¿ç”¨å·¥å…·ï¼Œä¾‹å¦‚ï¼Œå‡å°‘ä¸å¿…è¦çš„ç¼©æ”¾æˆ–æ£€ç´¢æ“ä½œã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ„å»ºäº†ç»“æ„åŒ–çš„æ„ŸçŸ¥å’Œæ¯”è¾ƒè½¨è¿¹ï¼Œç”¨äºæŒ‡å¯¼æ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ã€‚å…·ä½“è€Œè¨€ï¼Œè¿™äº›è½¨è¿¹æ¨¡æ‹Ÿäº†äººç±»ä¸“å®¶åœ¨æ£€æµ‹è¿‡ç¨‹ä¸­çš„æ€è€ƒè·¯å¾„å’Œæ“ä½œæ­¥éª¤ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AgentIADåœ¨MMADæ•°æ®é›†ä¸Šå–å¾—äº†97.62%çš„åˆ†ç±»ç²¾åº¦ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…ˆå‰çš„åŸºäºMLLMçš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„state-of-the-artæ°´å¹³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å·¥å…·å¢å¼ºå’Œå¤šé˜¶æ®µæ£€æŸ¥ï¼ŒAgentIADèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ£€æµ‹ç»†å¾®ã€å±€éƒ¨çš„å·¥ä¸šå¼‚å¸¸ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„æ£€æŸ¥è½¨è¿¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AgentIADå¯åº”ç”¨äºå„ç§å·¥ä¸šç”Ÿäº§çº¿çš„è´¨é‡æ£€æµ‹ç¯èŠ‚ï¼Œä¾‹å¦‚ç”µå­å…ƒä»¶ã€æ±½è½¦é›¶éƒ¨ä»¶ã€çººç»‡å“ç­‰äº§å“çš„ç¼ºé™·æ£€æµ‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æ£€æµ‹ç²¾åº¦ï¼Œé™ä½æ¼æ£€ç‡ï¼Œä»è€Œæå‡äº§å“è´¨é‡å’Œç”Ÿäº§æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒAgentIADçš„é€æ˜æ£€æŸ¥è¿‡ç¨‹æœ‰åŠ©äºåˆ†æç¼ºé™·åŸå› ï¼Œä¸ºæ”¹è¿›ç”Ÿäº§å·¥è‰ºæä¾›å‚è€ƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–éœ€è¦ç²¾ç»†è§†è§‰æ£€æŸ¥çš„é¢†åŸŸï¼Œå¦‚åŒ»ç–—å½±åƒåˆ†æã€é¥æ„Ÿå›¾åƒè§£è¯‘ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

