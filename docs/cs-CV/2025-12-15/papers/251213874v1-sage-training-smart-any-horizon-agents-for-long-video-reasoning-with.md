---
layout: default
title: SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning
---

# SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13874" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13874v1</a>
  <a href="https://arxiv.org/pdf/2512.13874.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13874v1" onclick="toggleFavorite(this, '2512.13874v1', 'SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jitesh Jain, Jialuo Li, Zixian Ma, Jieyu Zhang, Chris Dongjoo Kim, Sangho Lee, Rohun Tripathi, Tanmay Gupta, Christopher Clark, Humphrey Shi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

**å¤‡æ³¨**: Project Page: https://praeclarumjj3.github.io/sage/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAGEï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä»»æ„æ—¶åŸŸAgentï¼Œç”¨äºé•¿è§†é¢‘æ¨ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿è§†é¢‘æ¨ç†` `ä»»æ„æ—¶åŸŸæ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æ™ºèƒ½Agent` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ¨ç†æ¨¡å‹é€šå¸¸ä»¥å•è½®æ–¹å¼å¤„ç†å¤§é‡å¸§ï¼Œç±»ä¼¼äºè§‚çœ‹å®Œæ•´é•¿è§†é¢‘ï¼Œæ¶ˆè€—å¤§é‡èµ„æºï¼Œç¼ºä¹çµæ´»æ€§ã€‚
2. SAGEç³»ç»Ÿé€šè¿‡å¤šè½®æ¨ç†å¤„ç†é•¿è§†é¢‘ï¼Œå¹¶èƒ½ä»¥å•è½®æ–¹å¼å¤„ç†ç®€å•é—®é¢˜ï¼Œæ¨¡ä»¿äººç±»çš„è§‚çœ‹ä¹ æƒ¯ï¼Œæå‡æ•ˆç‡ã€‚
3. é€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼ŒSAGEåœ¨é•¿è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è§†é¢‘ä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºSAGEï¼Œä¸€ä¸ªæ™ºèƒ½Agentç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿåƒäººç±»ä¸€æ ·è¿›è¡Œä»»æ„æ—¶åŸŸçš„æ¨ç†ï¼Œå³æ ¹æ®ä»»åŠ¡éœ€è¦ï¼Œå†³å®šæ˜¯å¿«é€Ÿæµè§ˆé•¿è§†é¢‘è¿˜æ˜¯å®Œæ•´è§‚çœ‹çŸ­è§†é¢‘ã€‚ä¸ºäº†è®­ç»ƒSAGEçš„æ ¸å¿ƒæ¨¡å—SAGE-MMï¼Œæˆ‘ä»¬åˆ©ç”¨Gemini-2.5-Flashæå‡ºäº†ä¸€ä¸ªç®€æ˜“çš„åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ–¹æ³•ï¼Œè¿™å¯¹äºåœ¨SAGE-MMä¸­åŸ¹å…»ä»»æ„æ—¶åŸŸæ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä¸ºäº†è¯„ä¼°çœŸå®å¨±ä¹åœºæ™¯ä¸‹è§†é¢‘æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æ„å»ºäº†SAGE-Benchï¼Œå…¶å¹³å‡è§†é¢‘æ—¶é•¿è¶…è¿‡700ç§’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿã€æ•°æ®å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œåœ¨å¼€æ”¾å¼è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†é«˜è¾¾6.1%çš„æ˜¾è‘—æå‡ï¼Œåœ¨è¶…è¿‡10åˆ†é’Ÿçš„è§†é¢‘ä¸Šå–å¾—äº†8.2%çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘æ¨ç†æ¨¡å‹é€šå¸¸éœ€è¦ä¸€æ¬¡æ€§å¤„ç†å¤§é‡è§†é¢‘å¸§ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”ç¼ºä¹åƒäººç±»ä¸€æ ·çš„çµæ´»æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•æ ¹æ®è§†é¢‘å†…å®¹å’Œä»»åŠ¡éœ€æ±‚è°ƒæ•´è§‚çœ‹ç­–ç•¥ã€‚å®ƒä»¬æ— æ³•åœ¨éœ€è¦æ—¶å¿«é€Ÿæµè§ˆé•¿è§†é¢‘ï¼Œæˆ–è€…åœ¨å¿…è¦æ—¶å®Œæ•´è§‚çœ‹çŸ­è§†é¢‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSAGEçš„æ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒä¸€ä¸ªæ™ºèƒ½Agentï¼Œä½¿å…¶èƒ½å¤Ÿåƒäººç±»ä¸€æ ·è¿›è¡Œä»»æ„æ—¶åŸŸçš„æ¨ç†ã€‚è¯¥Agentå¯ä»¥å†³å®šæ˜¯è¿­ä»£åœ°æµè§ˆé•¿è§†é¢‘ï¼Œè¿˜æ˜¯å®Œæ•´åœ°è§‚çœ‹çŸ­è§†é¢‘ï¼Œä»è€Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚è¿™ç§è®¾è®¡æ¨¡ä»¿äº†äººç±»åœ¨å¤„ç†è§†é¢‘æ—¶çš„è‡ªç„¶è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSAGEç³»ç»ŸåŒ…å«ä¸€ä¸ªæ ¸å¿ƒæ¨¡å—SAGE-MMï¼Œå®ƒè´Ÿè´£æ ¹æ®å½“å‰çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥çš„åŠ¨ä½œï¼Œä¾‹å¦‚è§‚çœ‹ä¸€éƒ¨åˆ†è§†é¢‘ã€å›ç­”é—®é¢˜ç­‰ã€‚æ•´ä¸ªæµç¨‹æ˜¯å¤šè½®äº¤äº’å¼çš„ï¼ŒAgentæ ¹æ®æ¯ä¸€è½®çš„è§‚å¯Ÿå’Œå¥–åŠ±ï¼Œä¸æ–­ä¼˜åŒ–å…¶æ¨ç†ç­–ç•¥ã€‚ç³»ç»Ÿä½¿ç”¨Gemini-2.5-Flashç”Ÿæˆåˆæˆæ•°æ®ï¼Œç”¨äºé¢„è®­ç»ƒSAGE-MMã€‚ä¹‹åï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ å¯¹SAGE-MMè¿›è¡Œåè®­ç»ƒï¼Œä»¥æå‡å…¶ä»»æ„æ—¶åŸŸæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šSAGEçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä»»æ„æ—¶åŸŸæ¨ç†èƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ–¹æ³•ã€‚ä¼ ç»Ÿçš„è§†é¢‘æ¨ç†æ¨¡å‹é€šå¸¸æ˜¯å•è½®çš„ï¼Œè€ŒSAGEèƒ½å¤Ÿè¿›è¡Œå¤šè½®äº¤äº’å¼æ¨ç†ï¼Œæ›´åŠ çµæ´»é«˜æ•ˆã€‚å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡Agentçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„è§†é¢‘å’Œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šSAGE-MMçš„è®­ç»ƒåŒ…æ‹¬é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ä¸¤ä¸ªé˜¶æ®µã€‚é¢„è®­ç»ƒä½¿ç”¨åˆæˆæ•°æ®ï¼Œç›®æ ‡æ˜¯è®©Agentåˆæ­¥å…·å¤‡è§†é¢‘ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ é˜¶æ®µåˆ™ä½¿ç”¨å¥–åŠ±å‡½æ•°æ¥å¼•å¯¼Agentå­¦ä¹ æœ€ä½³çš„è§‚çœ‹ç­–ç•¥ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼Œä½†æ­¤å¤„æœªæä¾›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SAGEåœ¨å¼€æ”¾å¼è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œé«˜è¾¾6.1%ã€‚å°¤å…¶æ˜¯åœ¨è¶…è¿‡10åˆ†é’Ÿçš„é•¿è§†é¢‘ä¸Šï¼ŒSAGEçš„æ€§èƒ½æå‡è¾¾åˆ°äº†8.2%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSAGEçš„ä»»æ„æ—¶åŸŸæ¨ç†èƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡é•¿è§†é¢‘æ¨ç†çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SAGEå¯åº”ç”¨äºæ™ºèƒ½è§†é¢‘ç›‘æ§ã€æ™ºèƒ½æ•™è‚²ã€å¨±ä¹è§†é¢‘åˆ†æç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­ï¼ŒSAGEå¯ä»¥å¿«é€Ÿå®šä½å¼‚å¸¸äº‹ä»¶ï¼›åœ¨æ™ºèƒ½æ•™è‚²ä¸­ï¼ŒSAGEå¯ä»¥æ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ è¿›åº¦å’Œç†è§£ç¨‹åº¦ï¼Œæ™ºèƒ½æ¨èå­¦ä¹ å†…å®¹ï¼›åœ¨å¨±ä¹è§†é¢‘åˆ†æä¸­ï¼ŒSAGEå¯ä»¥å¸®åŠ©ç”¨æˆ·å¿«é€Ÿæ‰¾åˆ°æ„Ÿå…´è¶£çš„ç‰‡æ®µã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.

