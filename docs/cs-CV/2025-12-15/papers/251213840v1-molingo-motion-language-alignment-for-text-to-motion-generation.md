---
layout: default
title: MoLingo: Motion-Language Alignment for Text-to-Motion Generation
---

# MoLingo: Motion-Language Alignment for Text-to-Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13840" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13840v1</a>
  <a href="https://arxiv.org/pdf/2512.13840.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13840v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.13840v1', 'MoLingo: Motion-Language Alignment for Text-to-Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yannan He, Garvita Tiwari, Xiaohan Zhang, Pankaj Bora, Tolga Birdal, Jan Eric Lenssen, Gerard Pons-Moll

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

**å¤‡æ³¨**: Project page: https://hynann.github.io/molingo/MoLingo.html

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MoLingoï¼šé€šè¿‡è¿åŠ¨-è¯­è¨€å¯¹é½å®ç°æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆï¼Œè¾¾åˆ°æ–°çš„SOTAã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆ` `è¿åŠ¨ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `è¯­ä¹‰å¯¹é½` `äº¤å‰æ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ–¹æ³•åœ¨è¯­ä¹‰å¯¹é½çš„æ½œåœ¨ç©ºé—´æ„å»ºå’Œæ–‡æœ¬æ¡ä»¶æ³¨å…¥æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå½±å“äº†ç”ŸæˆåŠ¨ä½œçš„çœŸå®æ€§å’Œæ–‡æœ¬ä¸€è‡´æ€§ã€‚
2. MoLingoé€šè¿‡è®­ç»ƒè¯­ä¹‰å¯¹é½çš„è¿åŠ¨ç¼–ç å™¨ï¼Œå¹¶ç»“åˆå¤štokenäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†æ½œåœ¨ç©ºé—´çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›å’Œæ–‡æœ¬æ¡ä»¶çš„æœ‰æ•ˆæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMoLingoåœ¨äººç±»è¿åŠ¨ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨æ ‡å‡†æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶ä¸­è¾¾åˆ°äº†æ–°çš„SOTAã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†MoLingoï¼Œä¸€ä¸ªæ–‡æœ¬åˆ°åŠ¨ä½œï¼ˆT2Mï¼‰æ¨¡å‹ï¼Œå®ƒé€šè¿‡åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­å»å™ªæ¥ç”Ÿæˆé€¼çœŸã€æ ©æ ©å¦‚ç”Ÿçš„äººç±»è¿åŠ¨ã€‚æœ€è¿‘çš„å·¥ä½œåœ¨æ•´ä¸ªæ½œåœ¨ç©ºé—´ä¸Šä¸€æ¬¡æ€§åœ°æˆ–é€šè¿‡å¤šä¸ªæ½œåœ¨å˜é‡è‡ªå›å½’åœ°æ‰§è¡Œæ½œåœ¨ç©ºé—´æ‰©æ•£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•ä½¿è¿ç»­è¿åŠ¨æ½œåœ¨å˜é‡ä¸Šçš„æ‰©æ•£æ•ˆæœæœ€ä½³ã€‚æˆ‘ä»¬ä¸“æ³¨äºä¸¤ä¸ªé—®é¢˜ï¼šï¼ˆ1ï¼‰å¦‚ä½•æ„å»ºè¯­ä¹‰å¯¹é½çš„æ½œåœ¨ç©ºé—´ï¼Œä½¿æ‰©æ•£æ›´æœ‰æ•ˆï¼›ï¼ˆ2ï¼‰å¦‚ä½•æœ€å¥½åœ°æ³¨å…¥æ–‡æœ¬æ¡ä»¶ï¼Œä½¿è¿åŠ¨ç´§å¯†åœ°éµå¾ªæè¿°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯­ä¹‰å¯¹é½çš„è¿åŠ¨ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ä½¿ç”¨å¸§çº§åˆ«çš„æ–‡æœ¬æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¾¿å…·æœ‰ç›¸ä¼¼æ–‡æœ¬å«ä¹‰çš„æ½œåœ¨å˜é‡ä¿æŒæ¥è¿‘ï¼Œè¿™ä½¿å¾—æ½œåœ¨ç©ºé—´æ›´é€‚åˆæ‰©æ•£ã€‚æˆ‘ä»¬è¿˜å°†å•tokenæ¡ä»¶ä¸å¤štokenäº¤å‰æ³¨æ„åŠ›æ–¹æ¡ˆè¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°äº¤å‰æ³¨æ„åŠ›æä¾›äº†æ›´å¥½çš„è¿åŠ¨çœŸå®æ„Ÿå’Œæ–‡æœ¬-è¿åŠ¨å¯¹é½ã€‚å‡­å€Ÿè¯­ä¹‰å¯¹é½çš„æ½œåœ¨å˜é‡ã€è‡ªå›å½’ç”Ÿæˆå’Œäº¤å‰æ³¨æ„åŠ›æ–‡æœ¬æ¡ä»¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶ä¸­ï¼Œåœ¨äººç±»è¿åŠ¨ç”Ÿæˆæ–¹é¢æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ï¼Œä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œä¸‹æ¸¸ä½¿ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆï¼ˆT2Mï¼‰æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°ç”Ÿæˆå¯¹åº”çš„äººä½“è¿åŠ¨åºåˆ—ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆé€¼çœŸä¸”ä¸æ–‡æœ¬æè¿°é«˜åº¦ä¸€è‡´çš„è¿åŠ¨æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºå¦‚ä½•æ„å»ºä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è¿åŠ¨è¯­ä¹‰çš„æ½œåœ¨ç©ºé—´ï¼Œä»¥åŠå¦‚ä½•å°†æ–‡æœ¬ä¿¡æ¯æœ‰æ•ˆåœ°èå…¥åˆ°è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoLingoçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªè¯­ä¹‰å¯¹é½çš„è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œå¹¶é‡‡ç”¨å¤štokenäº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºæ–‡æœ¬æ¡ä»¶çš„ä½œç”¨ã€‚é€šè¿‡è¯­ä¹‰å¯¹é½ï¼Œä½¿å¾—æ½œåœ¨ç©ºé—´ä¸­çš„ç‚¹èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ è¿åŠ¨çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜ç”Ÿæˆè¿åŠ¨çš„è´¨é‡ã€‚äº¤å‰æ³¨æ„åŠ›æœºåˆ¶åˆ™èƒ½å¤Ÿæ›´ç²¾ç»†åœ°æ•æ‰æ–‡æœ¬æè¿°ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†å…¶èå…¥åˆ°è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoLingoçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¿åŠ¨ç¼–ç å™¨ï¼šå°†è¿åŠ¨åºåˆ—ç¼–ç åˆ°æ½œåœ¨ç©ºé—´ä¸­ã€‚2) æ–‡æœ¬ç¼–ç å™¨ï¼šå°†æ–‡æœ¬æè¿°ç¼–ç ä¸ºæ–‡æœ¬ç‰¹å¾ã€‚3) æ‰©æ•£æ¨¡å‹ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå»å™ªæ‰©æ•£ï¼Œç”Ÿæˆæ–°çš„è¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚4) è¿åŠ¨è§£ç å™¨ï¼šå°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºè¿åŠ¨åºåˆ—ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¸§çº§åˆ«çš„æ–‡æœ¬æ ‡ç­¾æ¥è®­ç»ƒè¿åŠ¨ç¼–ç å™¨ï¼Œä»¥å®ç°è¯­ä¹‰å¯¹é½ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å°†æ–‡æœ¬ç‰¹å¾èå…¥åˆ°æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoLingoçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†è¯­ä¹‰å¯¹é½çš„è¿åŠ¨ç¼–ç å™¨ï¼Œé€šè¿‡å¸§çº§åˆ«çš„æ–‡æœ¬æ ‡ç­¾è®­ç»ƒï¼Œä½¿å¾—æ½œåœ¨ç©ºé—´èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ è¿åŠ¨çš„è¯­ä¹‰ä¿¡æ¯ã€‚2) é‡‡ç”¨äº†å¤štokenäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ›´ç²¾ç»†åœ°æ•æ‰æ–‡æœ¬æè¿°ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†å…¶èå…¥åˆ°è¿åŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯­ä¹‰å¯¹é½çš„è¿åŠ¨ç¼–ç å™¨ä¸­ï¼Œä½¿ç”¨äº†å¯¹æ¯”å­¦ä¹ æŸå¤±æ¥æ‹‰è¿‘å…·æœ‰ç›¸ä¼¼æ–‡æœ¬å«ä¹‰çš„è¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚åœ¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½¿ç”¨äº†å¤šä¸ªæ³¨æ„åŠ›å¤´æ¥æ•æ‰æ–‡æœ¬æè¿°ä¸­çš„ä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚æ‰©æ•£æ¨¡å‹é‡‡ç”¨äº†æ ‡å‡†çš„æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œå¹¶ä½¿ç”¨U-Netä½œä¸ºå»å™ªç½‘ç»œã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MoLingoåœ¨HumanML3Då’ŒKIT-MLæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨HumanML3Dæ•°æ®é›†ä¸Šï¼ŒMoLingoåœ¨FIDæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ç”¨æˆ·ç ”ç©¶ä¸­è·å¾—äº†æ›´é«˜çš„ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoLingoèƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸã€æ›´ç¬¦åˆæ–‡æœ¬æè¿°çš„è¿åŠ¨åºåˆ—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoLingoåœ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„äººä½“è¿åŠ¨ï¼Œä»è€Œå¢å¼ºè™šæ‹Ÿè§’è‰²çš„è¡¨ç°åŠ›ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒMoLingoè¿˜å¯ä»¥ç”¨äºè¿åŠ¨åˆ†æã€åº·å¤è®­ç»ƒç­‰é¢†åŸŸï¼Œé€šè¿‡åˆ†æäººä½“è¿åŠ¨æ•°æ®ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.

