---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-10
---

# cs.CVï¼ˆ2025-05-10ï¼‰

ğŸ“Š å…± **11** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250506663v1-metor-a-unified-framework-for-mutual-enhancement-of-objects-and-rela.html">METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection</a></td>
  <td>æå‡ºMETORæ¡†æ¶ä»¥è§£å†³å¼€æ”¾è¯æ±‡è§†é¢‘è§†è§‰å…³ç³»æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06663v1" data-paper-url="./papers/250506663v1-metor-a-unified-framework-for-mutual-enhancement-of-objects-and-rela.html" onclick="toggleFavorite(this, '2505.06663v1', 'METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250506524v1-causal-prompt-calibration-guided-segment-anything-model-for-open-voc.html">Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation</a></td>
  <td>æå‡ºå› æœæç¤ºæ ¡å‡†æ–¹æ³•ä»¥è§£å†³å¼€æ”¾è¯æ±‡å¤šå®ä½“åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06524v1" data-paper-url="./papers/250506524v1-causal-prompt-calibration-guided-segment-anything-model-for-open-voc.html" onclick="toggleFavorite(this, '2505.06524v1', 'Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250506517v1-edge-enabled-vio-with-long-tracked-features-for-high-accuracy-low-al.html">Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation</a></td>
  <td>æå‡ºé•¿è·Ÿè¸ªç‰¹å¾çš„è¾¹ç¼˜å¯ç”¨VIOä»¥è§£å†³ä½ç©ºIoTå¯¼èˆªä¸­çš„å®šä½æ¼‚ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VIO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06517v1" data-paper-url="./papers/250506517v1-edge-enabled-vio-with-long-tracked-features-for-high-accuracy-low-al.html" onclick="toggleFavorite(this, '2505.06517v1', 'Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250506573v1-electricsight-3d-hazard-monitoring-for-power-lines-using-low-cost-se.html">ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors</a></td>
  <td>æå‡ºElectricSightä»¥è§£å†³ç”µåŠ›çº¿è·¯3Då±é™©ç›‘æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06573v1" data-paper-url="./papers/250506573v1-electricsight-3d-hazard-monitoring-for-power-lines-using-low-cost-se.html" onclick="toggleFavorite(this, '2505.06573v1', 'ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250506592v1-batch-augmentation-with-unimodal-fine-tuning-for-multimodal-learning.html">Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning</a></td>
  <td>æå‡ºæ‰¹é‡å¢å¼ºä¸å•æ¨¡æ€å¾®è°ƒä»¥æ£€æµ‹èƒå„¿å™¨å®˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06592v1" data-paper-url="./papers/250506592v1-batch-augmentation-with-unimodal-fine-tuning-for-multimodal-learning.html" onclick="toggleFavorite(this, '2505.06592v1', 'Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250506536v1-tacfn-transformer-based-adaptive-cross-modal-fusion-network-for-mult.html">TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition</a></td>
  <td>æå‡ºTACFNä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„ç‰¹å¾å†—ä½™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06536v1" data-paper-url="./papers/250506536v1-tacfn-transformer-based-adaptive-cross-modal-fusion-network-for-mult.html" onclick="toggleFavorite(this, '2505.06536v1', 'TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250506527v1-improving-generalization-of-medical-image-registration-foundation-mo.html">Improving Generalization of Medical Image Registration Foundation Model</a></td>
  <td>æå‡ºSharpness-Aware Minimizationä»¥å¢å¼ºåŒ»å­¦å›¾åƒé…å‡†åŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06527v1" data-paper-url="./papers/250506527v1-improving-generalization-of-medical-image-registration-foundation-mo.html" onclick="toggleFavorite(this, '2505.06527v1', 'Improving Generalization of Medical Image Registration Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250506575v1-grace-estimating-geometry-level-3d-human-scene-contact-from-2d-image.html">GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</a></td>
  <td>æå‡ºGRACEä»¥è§£å†³3Däººç±»-åœºæ™¯æ¥è§¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SMPL</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06575v1" data-paper-url="./papers/250506575v1-grace-estimating-geometry-level-3d-human-scene-contact-from-2d-image.html" onclick="toggleFavorite(this, '2505.06575v1', 'GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/250506647v2-dataset-distillation-with-probabilistic-latent-features.html">Dataset Distillation with Probabilistic Latent Features</a></td>
  <td>æå‡ºåŸºäºæ¦‚ç‡æ½œåœ¨ç‰¹å¾çš„æ•°æ®é›†è’¸é¦æ–¹æ³•ä»¥é™ä½è®¡ç®—æˆæœ¬</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06647v2" data-paper-url="./papers/250506647v2-dataset-distillation-with-probabilistic-latent-features.html" onclick="toggleFavorite(this, '2505.06647v2', 'Dataset Distillation with Probabilistic Latent Features')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250506543v1-hdglyph-a-hierarchical-disentangled-glyph-based-framework-for-long-t.html">HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models</a></td>
  <td>æå‡ºHDGlyphæ¡†æ¶ä»¥è§£å†³é•¿å°¾æ–‡æœ¬æ¸²æŸ“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06543v1" data-paper-url="./papers/250506543v1-hdglyph-a-hierarchical-disentangled-glyph-based-framework-for-long-t.html" onclick="toggleFavorite(this, '2505.06543v1', 'HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250506537v1-profashion-prototype-guided-fashion-video-generation-with-multiple-r.html">ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images</a></td>
  <td>æå‡ºProFashionä»¥è§£å†³æ—¶å°šè§†é¢‘ç”Ÿæˆä¸­çš„è§†è§’ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06537v1" data-paper-url="./papers/250506537v1-profashion-prototype-guided-fashion-video-generation-with-multiple-r.html" onclick="toggleFavorite(this, '2505.06537v1', 'ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)