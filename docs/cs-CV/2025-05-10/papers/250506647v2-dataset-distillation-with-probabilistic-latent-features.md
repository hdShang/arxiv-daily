---
layout: default
title: Dataset Distillation with Probabilistic Latent Features
---

# Dataset Distillation with Probabilistic Latent Features

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06647" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06647v2</a>
  <a href="https://arxiv.org/pdf/2505.06647.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06647v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06647v2', 'Dataset Distillation with Probabilistic Latent Features')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhe Li, Sarah Cechnicka, Cheng Ouyang, Katharina Breininger, Peter SchÃ¼ffler, Bernhard Kainz

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-10 (æ›´æ–°: 2025-05-17)

**å¤‡æ³¨**: 23 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¦‚ç‡æ½œåœ¨ç‰¹å¾çš„æ•°æ®é›†è’¸é¦æ–¹æ³•ä»¥é™ä½è®¡ç®—æˆæœ¬**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ•°æ®é›†è’¸é¦` `æ·±åº¦å­¦ä¹ ` `åˆæˆæ•°æ®` `æ¦‚ç‡æ¨¡å‹` `è®¡ç®—æœºè§†è§‰` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°æ®é›†è’¸é¦æ–¹æ³•é€šå¸¸ä¾èµ–äºå°†æ•°æ®ä»åƒç´ ç©ºé—´æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰æ•°æ®çš„ç©ºé—´ç»“æ„ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡æ½œåœ¨ç‰¹å¾çš„éšæœºæ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡æ½œåœ¨ç‰¹å¾çš„è”åˆåˆ†å¸ƒæ¥ç”Ÿæˆåˆæˆæ•°æ®ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯åï¼Œæå‡ºçš„æ–¹æ³•åœ¨å¤šç§éª¨å¹²ç½‘ç»œä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¤æ‚æ€§å¢åŠ å’Œè®­ç»ƒæ•°æ®é‡çš„æ‰©å¤§ï¼Œé™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬å˜å¾—æ„ˆå‘é‡è¦ã€‚æ•°æ®é›†è’¸é¦é€šè¿‡åˆæˆä¸€ç»„ç´§å‡‘çš„åˆæˆæ•°æ®ï¼Œæœ‰æ•ˆæ›¿ä»£åŸå§‹æ•°æ®é›†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå°†æ•°æ®ä»åƒç´ ç©ºé—´æ˜ å°„åˆ°ç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œè€Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éšæœºæ–¹æ³•ï¼Œå»ºæ¨¡æ½œåœ¨ç‰¹å¾çš„è”åˆåˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç©ºé—´ç»“æ„ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆæ ·æœ¬ï¼Œä»è€Œæœ‰åˆ©äºæ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”±è½»é‡ç½‘ç»œå‚æ•°åŒ–çš„ä½ç§©å¤šå…ƒæ­£æ€åˆ†å¸ƒï¼Œä¿æŒäº†ä½è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶å…¼å®¹å¤šç§ç”¨äºæ•°æ®é›†è’¸é¦çš„åŒ¹é…ç½‘ç»œã€‚ç»è¿‡è’¸é¦åï¼Œé€šè¿‡å°†å­¦ä¹ åˆ°çš„æ½œåœ¨ç‰¹å¾è¾“å…¥é¢„è®­ç»ƒç”Ÿæˆå™¨ç”Ÿæˆåˆæˆå›¾åƒï¼Œå¹¶ç”¨äºè®­ç»ƒåˆ†ç±»æ¨¡å‹ï¼Œæœ€ç»ˆåœ¨çœŸå®æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼ŒåŒ…æ‹¬ImageNetå­é›†ã€CIFAR-10å’ŒMedMNISTç»„ç»‡ç—…ç†æ•°æ®é›†ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨å¤šç§éª¨å¹²ç½‘ç»œä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è·¨æ¶æ„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†è’¸é¦æ–¹æ³•åœ¨æ•æ‰æ•°æ®ç©ºé—´ç»“æ„æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆåˆæˆæ•°æ®æ—¶çš„å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„éšæœºæ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡æ½œåœ¨ç‰¹å¾çš„è”åˆåˆ†å¸ƒæ¥ç”Ÿæˆåˆæˆæ•°æ®ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®çš„ç©ºé—´ç»“æ„ï¼Œè¿›è€Œæé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨è½»é‡ç½‘ç»œå‚æ•°åŒ–çš„ä½ç§©å¤šå…ƒæ­£æ€åˆ†å¸ƒæ¥å­¦ä¹ æ½œåœ¨ç‰¹å¾ï¼›å…¶æ¬¡ï¼Œå°†å­¦ä¹ åˆ°çš„æ½œåœ¨ç‰¹å¾è¾“å…¥åˆ°é¢„è®­ç»ƒç”Ÿæˆå™¨ä¸­ç”Ÿæˆåˆæˆå›¾åƒï¼›æœ€åï¼Œåˆ©ç”¨è¿™äº›åˆæˆå›¾åƒè®­ç»ƒåˆ†ç±»æ¨¡å‹å¹¶è¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸€ç§æ–°çš„æ¦‚ç‡æ¨¡å‹æ¥æè¿°æ½œåœ¨ç‰¹å¾çš„è”åˆåˆ†å¸ƒï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„åƒç´ ç©ºé—´æ˜ å°„å½¢æˆäº†æœ¬è´¨åŒºåˆ«ï¼Œä½¿å¾—ç”Ÿæˆçš„åˆæˆæ ·æœ¬æ›´åŠ å¤šæ ·åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è½»é‡çº§ç½‘ç»œä»¥ä¿æŒä½è®¡ç®—å¤æ‚åº¦ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šåˆ™è€ƒè™‘äº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œç¡®ä¿ç”Ÿæˆçš„åˆæˆæ•°æ®èƒ½å¤Ÿæœ‰æ•ˆæ›¿ä»£åŸå§‹æ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨å¤šç§éª¨å¹²ç½‘ç»œä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ImageNetå­é›†å’ŒCIFAR-10ä¸Šï¼Œæ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†ç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€åŒ»ç–—å½±åƒåˆ†æå’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½æ•°æ®å­˜å‚¨å’Œå¤„ç†æˆæœ¬ï¼ŒåŒæ—¶æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å¤§è§„æ¨¡æ•°æ®é›†çš„å¤„ç†å’Œæ¨¡å‹è®­ç»ƒä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As deep learning models grow in complexity and the volume of training data increases, reducing storage and computational costs becomes increasingly important. Dataset distillation addresses this challenge by synthesizing a compact set of synthetic data that can effectively replace the original dataset in downstream classification tasks. While existing methods typically rely on mapping data from pixel space to the latent space of a generative model, we propose a novel stochastic approach that models the joint distribution of latent features. This allows our method to better capture spatial structures and produce diverse synthetic samples, which benefits model training. Specifically, we introduce a low-rank multivariate normal distribution parameterized by a lightweight network. This design maintains low computational complexity and is compatible with various matching networks used in dataset distillation. After distillation, synthetic images are generated by feeding the learned latent features into a pretrained generator. These synthetic images are then used to train classification models, and performance is evaluated on real test set. We validate our method on several benchmarks, including ImageNet subsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach achieves state-of-the-art cross architecture performance across a range of backbone architectures, demonstrating its generality and effectiveness.

