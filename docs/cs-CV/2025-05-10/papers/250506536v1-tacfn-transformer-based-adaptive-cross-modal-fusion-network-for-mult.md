---
layout: default
title: "TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition"
---

# TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06536" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06536v1</a>
  <a href="https://arxiv.org/pdf/2505.06536.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06536v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06536v1', 'TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Feng Liu, Ziwang Fu, Yunlong Wang, Qijian Zheng

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-10

**å¤‡æ³¨**: arXiv admin note: text overlap with arXiv:2111.02172

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/shuzihuaiyu/TACFN)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTACFNä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„ç‰¹å¾å†—ä½™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«` `è·¨æ¨¡æ€èåˆ` `è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©` `Transformer` `æƒ…æ„Ÿè®¡ç®—` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æ–¹æ³•åœ¨ç‰¹å¾å†—ä½™å’Œäº’è¡¥ç‰¹å¾æ•æ‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå½±å“äº†æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚
2. è®ºæ–‡æå‡ºçš„TACFNé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå†…éƒ¨ç‰¹å¾é€‰æ‹©ï¼Œä¼˜åŒ–æ¨¡æ€é—´çš„ç‰¹å¾äº¤äº’ï¼Œæå‡èåˆæ•ˆæœã€‚
3. åœ¨RAVDESSå’ŒIEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTACFNæ˜¾è‘—æé«˜äº†æƒ…æ„Ÿè¯†åˆ«çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­çš„èåˆæŠ€æœ¯è‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºè·¨æ¨¡æ€æ³¨æ„åŠ›çš„èåˆæ–¹æ³•è¡¨ç°å‡ºé«˜æ€§èƒ½å’Œå¼ºé²æ£’æ€§ã€‚ç„¶è€Œï¼Œè·¨æ¨¡æ€æ³¨æ„åŠ›å­˜åœ¨å†—ä½™ç‰¹å¾ï¼Œä¸”æœªèƒ½æœ‰æ•ˆæ•æ‰äº’è¡¥ç‰¹å¾ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨è·¨æ¨¡æ€äº¤äº’ä¸­ï¼Œå¹¶ä¸éœ€è¦ä½¿ç”¨ä¸€ä¸ªæ¨¡æ€çš„å…¨éƒ¨ä¿¡æ¯æ¥å¢å¼ºå¦ä¸€ä¸ªæ¨¡æ€ï¼Œèƒ½å¤Ÿå¢å¼ºæ¨¡æ€çš„ç‰¹å¾å¯èƒ½ä»…åŒ…å«å…¶ä¸€éƒ¨åˆ†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†åˆ›æ–°çš„åŸºäºTransformerçš„è‡ªé€‚åº”è·¨æ¨¡æ€èåˆç½‘ç»œï¼ˆTACFNï¼‰ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ä½¿ä¸€ä¸ªæ¨¡æ€è¿›è¡Œå†…éƒ¨ç‰¹å¾é€‰æ‹©ï¼Œä»è€Œä½¿æ‰€é€‰ç‰¹å¾èƒ½å¤Ÿè‡ªé€‚åº”ã€é«˜æ•ˆåœ°ä¸å¦ä¸€ä¸ªæ¨¡æ€äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTACFNç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¹¶è¾¾åˆ°äº†å½“å‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„ç‰¹å¾å†—ä½™å’Œäº’è¡¥ç‰¹å¾æ•æ‰ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„è·¨æ¨¡æ€æ³¨æ„åŠ›æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ¨¡æ€é—´çš„ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå†…éƒ¨ç‰¹å¾é€‰æ‹©ï¼Œä½¿å¾—æ¨¡æ€é—´çš„ç‰¹å¾äº¤äº’æ›´åŠ é«˜æ•ˆå’Œè‡ªé€‚åº”ï¼Œä»è€Œå‡å°‘å†—ä½™ä¿¡æ¯çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTACFNçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç‰¹å¾é€‰æ‹©æ¨¡å—å’Œç‰¹å¾å¢å¼ºæ¨¡å—ã€‚ç‰¹å¾é€‰æ‹©æ¨¡å—åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©é‡è¦ç‰¹å¾ï¼Œç‰¹å¾å¢å¼ºæ¨¡å—é€šè¿‡åŠ æƒèåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©æœºåˆ¶ï¼Œä½¿å¾—æ¨¡æ€é—´çš„äº¤äº’æ›´åŠ ç²¾å‡†ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„å†—ä½™ç‰¹å¾é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨Transformeræ¶æ„è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œèåˆï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºé€‚åº”å¤šæ¨¡æ€ç‰¹å¾çš„ç‰¹æ€§ï¼Œç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒå’Œæ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTACFNåœ¨RAVDESSå’ŒIEMOCAPæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æƒ…æ„Ÿè¯†åˆ«çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼Œè¾¾åˆ°äº†å½“å‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€ç¤¾äº¤åª’ä½“ç›‘æµ‹ã€æ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ï¼ŒTACFNå¯ä»¥ä¸ºæƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’æä¾›æ›´ä¸ºç²¾å‡†çš„æ”¯æŒï¼Œæœªæ¥å¯èƒ½åœ¨å¿ƒç†å¥åº·ç›‘æµ‹å’Œæƒ…æ„Ÿæ™ºèƒ½è®¾å¤‡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods have demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and does not capture complementary features well. We find that it is not necessary to use the entire information of one modality to reinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To this end, we design an innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN). Specifically, for the redundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the selected features can adaptively and efficiently interact with another modality. To better capture the complementary information between the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement of the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal representations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a significant performance improvement compared to other methods and reaches the state-of-the-art. All code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.

