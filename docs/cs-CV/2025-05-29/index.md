---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-29
---

# cs.CVï¼ˆ2025-05-29ï¼‰

ğŸ“Š å…± **46** ç¯‡è®ºæ–‡
 | ğŸ”— **19** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (25 ğŸ”—10)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ğŸ”—3)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (25 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250523757v1-impromptu-vla-open-weights-and-open-data-for-driving-vision-language.html">Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models</a></td>
  <td>æå‡ºImpromptu VLAä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23757v1" data-paper-url="./papers/250523757v1-impromptu-vla-open-weights-and-open-data-for-driving-vision-language.html" onclick="toggleFavorite(this, '2505.23757v1', 'Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250523766v1-argus-vision-centric-reasoning-with-grounded-chain-of-thought.html">Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought</a></td>
  <td>æå‡ºArgusä»¥è§£å†³è§†è§‰æ¨ç†ä¸­çš„æ³¨æ„åŠ›ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23766v1" data-paper-url="./papers/250523766v1-argus-vision-centric-reasoning-with-grounded-chain-of-thought.html" onclick="toggleFavorite(this, '2505.23766v1', 'Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250524007v2-preemptive-hallucination-reduction-an-input-level-approach-for-multi.html">Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model</a></td>
  <td>æå‡ºé¢„é˜²æ€§å¹»è§‰å‡å°‘æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24007v2" data-paper-url="./papers/250524007v2-preemptive-hallucination-reduction-an-input-level-approach-for-multi.html" onclick="toggleFavorite(this, '2505.24007v2', 'Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250523661v3-openuni-a-simple-baseline-for-unified-multimodal-understanding-and-g.html">OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation</a></td>
  <td>æå‡ºOpenUniä»¥å®ç°å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23661v3" data-paper-url="./papers/250523661v3-openuni-a-simple-baseline-for-unified-multimodal-understanding-and-g.html" onclick="toggleFavorite(this, '2505.23661v3', 'OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250524026v1-maskadapt-unsupervised-geometry-aware-domain-adaptation-using-multim.html">MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking</a></td>
  <td>æå‡ºMaskAdaptä»¥è§£å†³å†œä¸šé¢†åŸŸæ— ç›‘ç£åŸŸé€‚åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24026v1" data-paper-url="./papers/250524026v1-maskadapt-unsupervised-geometry-aware-domain-adaptation-using-multim.html" onclick="toggleFavorite(this, '2505.24026v1', 'MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250523747v1-spatial-mllm-boosting-mllm-capabilities-in-visual-based-spatial-inte.html">Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence</a></td>
  <td>æå‡ºSpatial-MLLMä»¥è§£å†³è§†è§‰åŸºç¡€ç©ºé—´æ™ºèƒ½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23747v1" data-paper-url="./papers/250523747v1-spatial-mllm-boosting-mllm-capabilities-in-visual-based-spatial-inte.html" onclick="toggleFavorite(this, '2505.23747v1', 'Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250523726v1-fmg-det-foundation-model-guided-robust-object-detection.html">FMG-Det: Foundation Model Guided Robust Object Detection</a></td>
  <td>æå‡ºFMG-Detä»¥è§£å†³å™ªå£°æ ‡æ³¨ä¸‹çš„ç‰©ä½“æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23726v1" data-paper-url="./papers/250523726v1-fmg-det-foundation-model-guided-robust-object-detection.html" onclick="toggleFavorite(this, '2505.23726v1', 'FMG-Det: Foundation Model Guided Robust Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250523693v1-vf-eval-evaluating-multimodal-llms-for-generating-feedback-on-aigc-v.html">VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos</a></td>
  <td>æå‡ºVF-Evalä»¥è¯„ä¼°å¤šæ¨¡æ€LLMåœ¨AIGCè§†é¢‘åé¦ˆç”Ÿæˆä¸­çš„èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23693v1" data-paper-url="./papers/250523693v1-vf-eval-evaluating-multimodal-llms-for-generating-feedback-on-aigc-v.html" onclick="toggleFavorite(this, '2505.23693v1', 'VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250523601v2-endobench-a-comprehensive-evaluation-of-multi-modal-large-language-m.html">EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis</a></td>
  <td>æå‡ºEndoBenchä»¥è§£å†³å†…çª¥é•œåˆ†æå¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23601v2" data-paper-url="./papers/250523601v2-endobench-a-comprehensive-evaluation-of-multi-modal-large-language-m.html" onclick="toggleFavorite(this, '2505.23601v2', 'EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250523522v2-omniearth-bench-towards-holistic-evaluation-of-earths-six-spheres-an.html">OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data</a></td>
  <td>æå‡ºOmniEarth-Benchä»¥è§£å†³åœ°çƒå…­å¤§åœˆå±‚åŠå…¶äº¤äº’çš„è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23522v2" data-paper-url="./papers/250523522v2-omniearth-bench-towards-holistic-evaluation-of-earths-six-spheres-an.html" onclick="toggleFavorite(this, '2505.23522v2', 'OmniEarth-Bench: Towards Holistic Evaluation of Earth&#39;s Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250523504v1-vau-r1-advancing-video-anomaly-understanding-via-reinforcement-fine-.html">VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning</a></td>
  <td>æå‡ºVAU-R1ä»¥è§£å†³è§†é¢‘å¼‚å¸¸ç†è§£ä¸­çš„æ¨ç†èƒ½åŠ›ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23504v1" data-paper-url="./papers/250523504v1-vau-r1-advancing-video-anomaly-understanding-via-reinforcement-fine-.html" onclick="toggleFavorite(this, '2505.23504v1', 'VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250523365v1-mcfnet-a-multimodal-collaborative-fusion-network-for-fine-grained-se.html">MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification</a></td>
  <td>æå‡ºMCFNetä»¥è§£å†³å¤šæ¨¡æ€ä¿¡æ¯èåˆä¸­çš„ç»†ç²’åº¦è¯­ä¹‰åˆ†ç±»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23365v1" data-paper-url="./papers/250523365v1-mcfnet-a-multimodal-collaborative-fusion-network-for-fine-grained-se.html" onclick="toggleFavorite(this, '2505.23365v1', 'MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250523359v1-videoreasonbench-can-mllms-perform-vision-centric-complex-video-reas.html">VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?</a></td>
  <td>æå‡ºVideoReasonBenchä»¥è§£å†³è§†é¢‘ç†è§£ä¸­çš„å¤æ‚æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23359v1" data-paper-url="./papers/250523359v1-videoreasonbench-can-mllms-perform-vision-centric-complex-video-reas.html" onclick="toggleFavorite(this, '2505.23359v1', 'VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250523752v2-thinkgeo-evaluating-tool-augmented-agents-for-remote-sensing-tasks.html">ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks</a></td>
  <td>æå‡ºThinkGeoä»¥è¯„ä¼°å·¥å…·å¢å¼ºä»£ç†åœ¨é¥æ„Ÿä»»åŠ¡ä¸­çš„è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23752v2" data-paper-url="./papers/250523752v2-thinkgeo-evaluating-tool-augmented-agents-for-remote-sensing-tasks.html" onclick="toggleFavorite(this, '2505.23752v2', 'ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250523543v1-position-paper-metadata-enrichment-model-integrating-neural-networks.html">Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications</a></td>
  <td>æå‡ºå…ƒæ•°æ®å¢å¼ºæ¨¡å‹ä»¥è§£å†³æ–‡åŒ–é—äº§æ•°å­—åŒ–ä¸­çš„å…ƒæ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">TAMP</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23543v1" data-paper-url="./papers/250523543v1-position-paper-metadata-enrichment-model-integrating-neural-networks.html" onclick="toggleFavorite(this, '2505.23543v1', 'Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250523449v3-cmie-combining-mllm-insights-with-external-evidence-for-explainable-.html">CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection</a></td>
  <td>æå‡ºCMIEæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è™šå‡ä¿¡æ¯æ£€æµ‹ä¸­çš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23449v3" data-paper-url="./papers/250523449v3-cmie-combining-mllm-insights-with-external-evidence-for-explainable-.html" onclick="toggleFavorite(this, '2505.23449v3', 'CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250603179v1-vid-sme-membership-inference-attacks-against-large-video-understandi.html">Vid-SME: Membership Inference Attacks against Large Video Understanding Models</a></td>
  <td>æå‡ºVid-SMEä»¥è§£å†³è§†é¢‘ç†è§£æ¨¡å‹çš„æˆå‘˜æ¨æ–­æ”»å‡»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03179v1" data-paper-url="./papers/250603179v1-vid-sme-membership-inference-attacks-against-large-video-understandi.html" onclick="toggleFavorite(this, '2506.03179v1', 'Vid-SME: Membership Inference Attacks against Large Video Understanding Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250524002v1-dgiqa-depth-guided-feature-attention-and-refinement-for-generalizabl.html">DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment</a></td>
  <td>æå‡ºDGIQAä»¥è§£å†³æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„æ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24002v1" data-paper-url="./papers/250524002v1-dgiqa-depth-guided-feature-attention-and-refinement-for-generalizabl.html" onclick="toggleFavorite(this, '2505.24002v1', 'DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250523977v1-visualsphinx-large-scale-synthetic-vision-logic-puzzles-for-rl.html">VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL</a></td>
  <td>æå‡ºVisualSphinxä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23977v1" data-paper-url="./papers/250523977v1-visualsphinx-large-scale-synthetic-vision-logic-puzzles-for-rl.html" onclick="toggleFavorite(this, '2505.23977v1', 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250523922v1-scalelong-a-multi-timescale-benchmark-for-long-video-understanding.html">ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding</a></td>
  <td>æå‡ºScaleLongåŸºå‡†ä»¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„å¤šæ—¶é—´å°ºåº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23922v1" data-paper-url="./papers/250523922v1-scalelong-a-multi-timescale-benchmark-for-long-video-understanding.html" onclick="toggleFavorite(this, '2505.23922v1', 'ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250523660v1-d-ar-diffusion-via-autoregressive-models.html">D-AR: Diffusion via Autoregressive Models</a></td>
  <td>æå‡ºD-ARä»¥é‡æ„å›¾åƒæ‰©æ•£è¿‡ç¨‹ä¸ºè‡ªå›å½’æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23660v1" data-paper-url="./papers/250523660v1-d-ar-diffusion-via-autoregressive-models.html" onclick="toggleFavorite(this, '2505.23660v1', 'D-AR: Diffusion via Autoregressive Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250523625v1-zerosep-separate-anything-in-audio-with-zero-training.html">ZeroSep: Separate Anything in Audio with Zero Training</a></td>
  <td>æå‡ºZeroSepä»¥å®ç°éŸ³é¢‘æºçš„é›¶è®­ç»ƒåˆ†ç¦»</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23625v1" data-paper-url="./papers/250523625v1-zerosep-separate-anything-in-audio-with-zero-training.html" onclick="toggleFavorite(this, '2505.23625v1', 'ZeroSep: Separate Anything in Audio with Zero Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250523566v4-uni-mumer-unified-multi-task-fine-tuning-of-vision-language-model-fo.html">Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition</a></td>
  <td>æå‡ºUni-MuMERä»¥è§£å†³æ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23566v4" data-paper-url="./papers/250523566v4-uni-mumer-unified-multi-task-fine-tuning-of-vision-language-model-fo.html" onclick="toggleFavorite(this, '2505.23566v4', 'Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250603182v1-terraincognita-a-dynamic-benchmark-for-species-discovery-using-front.html">TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models</a></td>
  <td>æå‡ºTerraIncognitaä»¥è§£å†³æ˜†è™«ç‰©ç§å‘ç°çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03182v1" data-paper-url="./papers/250603182v1-terraincognita-a-dynamic-benchmark-for-species-discovery-using-front.html" onclick="toggleFavorite(this, '2506.03182v1', 'TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250523484v1-vcapsbench-a-large-scale-fine-grained-benchmark-for-video-caption-qu.html">VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation</a></td>
  <td>æå‡ºVCapsBenchä»¥è§£å†³è§†é¢‘å­—å¹•è´¨é‡è¯„ä¼°ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23484v1" data-paper-url="./papers/250523484v1-vcapsbench-a-large-scale-fine-grained-benchmark-for-video-caption-qu.html" onclick="toggleFavorite(this, '2505.23484v1', 'VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/250524025v2-dino-r1-incentivizing-reasoning-capability-in-vision-foundation-mode.html">DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models</a></td>
  <td>æå‡ºDINO-R1ä»¥å¢å¼ºè§†è§‰åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24025v2" data-paper-url="./papers/250524025v2-dino-r1-incentivizing-reasoning-capability-in-vision-foundation-mode.html" onclick="toggleFavorite(this, '2505.24025v2', 'DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250523380v1-unirl-self-improving-unified-multimodal-models-via-supervised-and-re.html">UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning</a></td>
  <td>æå‡ºUniRLä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹åè®­ç»ƒæ•°æ®ä¾èµ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23380v1" data-paper-url="./papers/250523380v1-unirl-self-improving-unified-multimodal-models-via-supervised-and-re.html" onclick="toggleFavorite(this, '2505.23380v1', 'UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250523656v1-videorepa-learning-physics-for-video-generation-through-relational-a.html">VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models</a></td>
  <td>æå‡ºVideoREPAä»¥è§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ç†ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">physically plausible</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23656v1" data-paper-url="./papers/250523656v1-videorepa-learning-physics-for-video-generation-through-relational-a.html" onclick="toggleFavorite(this, '2505.23656v1', 'VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250523590v3-jigsaw-r1-a-study-of-rule-based-visual-reinforcement-learning-with-j.html">Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles</a></td>
  <td>æå‡ºåŸºäºè§„åˆ™çš„è§†è§‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23590v3" data-paper-url="./papers/250523590v3-jigsaw-r1-a-study-of-rule-based-visual-reinforcement-learning-with-j.html" onclick="toggleFavorite(this, '2505.23590v3', 'Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250523434v1-urbancraft-urban-view-extrapolation-via-hierarchical-sem-geometric-p.html">UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors</a></td>
  <td>æå‡ºUrbanCraftä»¥è§£å†³åŸå¸‚åœºæ™¯å¤–æ¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">scene reconstruction</span> <span class="paper-tag">occupancy grid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23434v1" data-paper-url="./papers/250523434v1-urbancraft-urban-view-extrapolation-via-hierarchical-sem-geometric-p.html" onclick="toggleFavorite(this, '2505.23434v1', 'UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250523727v1-pixelthink-towards-efficient-chain-of-pixel-reasoning.html">PixelThink: Towards Efficient Chain-of-Pixel Reasoning</a></td>
  <td>æå‡ºPixelThinkä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23727v1" data-paper-url="./papers/250523727v1-pixelthink-towards-efficient-chain-of-pixel-reasoning.html" onclick="toggleFavorite(this, '2505.23727v1', 'PixelThink: Towards Efficient Chain-of-Pixel Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250523883v2-bioclip-2-emergent-properties-from-scaling-hierarchical-contrastive-.html">BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning</a></td>
  <td>æå‡ºBioCLIP 2ä»¥è§£å†³ç”Ÿç‰©è§†è§‰æ¨¡å‹çš„èƒ½åŠ›æå‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23883v2" data-paper-url="./papers/250523883v2-bioclip-2-emergent-properties-from-scaling-hierarchical-contrastive-.html" onclick="toggleFavorite(this, '2505.23883v2', 'BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250523525v4-hallo4-high-fidelity-dynamic-portrait-animation-via-direct-preferenc.html">Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization</a></td>
  <td>æå‡ºäººç±»åå¥½å¯¹é½çš„æ‰©æ•£æ¡†æ¶ä»¥è§£å†³åŠ¨æ€è‚–åƒåŠ¨ç”»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">direct preference optimization</span> <span class="paper-tag">spatiotemporal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23525v4" data-paper-url="./papers/250523525v4-hallo4-high-fidelity-dynamic-portrait-animation-via-direct-preferenc.html" onclick="toggleFavorite(this, '2505.23525v4', 'Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250523678v2-grounded-reinforcement-learning-for-visual-reasoning.html">Grounded Reinforcement Learning for Visual Reasoning</a></td>
  <td>æå‡ºViGoRLä»¥è§£å†³è§†è§‰æ¨ç†ä¸­çš„ç©ºé—´å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23678v2" data-paper-url="./papers/250523678v2-grounded-reinforcement-learning-for-visual-reasoning.html" onclick="toggleFavorite(this, '2505.23678v2', 'Grounded Reinforcement Learning for Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250523346v1-beyond-optimal-transport-model-aligned-coupling-for-flow-matching.html">Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching</a></td>
  <td>æå‡ºæ¨¡å‹å¯¹é½è€¦åˆæ–¹æ³•ä»¥è§£å†³æµåŒ¹é…ä¸­çš„è·¯å¾„äº¤å‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23346v1" data-paper-url="./papers/250523346v1-beyond-optimal-transport-model-aligned-coupling-for-flow-matching.html" onclick="toggleFavorite(this, '2505.23346v1', 'Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/250523400v1-bridging-geometric-and-semantic-foundation-models-for-generalized-mo.html">Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation</a></td>
  <td>æå‡ºBriGeSä»¥è§£å†³å•ç›®æ·±åº¦ä¼°è®¡ä¸­çš„å‡ ä½•ä¸è¯­ä¹‰èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23400v1" data-paper-url="./papers/250523400v1-bridging-geometric-and-semantic-foundation-models-for-generalized-mo.html" onclick="toggleFavorite(this, '2505.23400v1', 'Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250523716v2-anysplat-feed-forward-3d-gaussian-splatting-from-unconstrained-views.html">AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</a></td>
  <td>æå‡ºAnySplatä»¥è§£å†³æ— æ ‡å®šè§†å›¾ä¸‹çš„æ–°è§†å›¾åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23716v2" data-paper-url="./papers/250523716v2-anysplat-feed-forward-3d-gaussian-splatting-from-unconstrained-views.html" onclick="toggleFavorite(this, '2505.23716v2', 'AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250523734v4-zpressor-bottleneck-aware-compression-for-scalable-feed-forward-3dgs.html">ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS</a></td>
  <td>æå‡ºZPressorä»¥è§£å†³3Dé«˜æ–¯ç‚¹äº‘æ¨¡å‹çš„å¯æ‰©å±•æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23734v4" data-paper-url="./papers/250523734v4-zpressor-bottleneck-aware-compression-for-scalable-feed-forward-3dgs.html" onclick="toggleFavorite(this, '2505.23734v4', 'ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/250523764v2-mmsi-bench-a-benchmark-for-multi-image-spatial-intelligence.html">MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</a></td>
  <td>æå‡ºMMSI-Benchä»¥è§£å†³å¤šå›¾åƒç©ºé—´æ™ºèƒ½è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23764v2" data-paper-url="./papers/250523764v2-mmsi-bench-a-benchmark-for-multi-image-spatial-intelligence.html" onclick="toggleFavorite(this, '2505.23764v2', 'MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250523769v2-textregion-text-aligned-region-tokens-from-frozen-image-text-models.html">TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models</a></td>
  <td>æå‡ºTextRegionä»¥è§£å†³å›¾åƒæ–‡æœ¬æ¨¡å‹åœ¨ç»†èŠ‚ç†è§£ä¸Šçš„ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23769v2" data-paper-url="./papers/250523769v2-textregion-text-aligned-region-tokens-from-frozen-image-text-models.html" onclick="toggleFavorite(this, '2505.23769v2', 'TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250523704v1-cldtracker-a-comprehensive-language-description-for-visual-tracking.html">CLDTracker: A Comprehensive Language Description for Visual Tracking</a></td>
  <td>æå‡ºCLDTrackerä»¥è§£å†³è§†è§‰è·Ÿè¸ªä¸­çš„è¯­è¨€æè¿°ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23704v1" data-paper-url="./papers/250523704v1-cldtracker-a-comprehensive-language-description-for-visual-tracking.html" onclick="toggleFavorite(this, '2505.23704v1', 'CLDTracker: A Comprehensive Language Description for Visual Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/250523481v2-physicsnerf-physics-guided-3d-reconstruction-from-sparse-views.html">PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views</a></td>
  <td>æå‡ºPhysicsNeRFä»¥è§£å†³ç¨€ç–è§†å›¾ä¸‹çš„3Dé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23481v2" data-paper-url="./papers/250523481v2-physicsnerf-physics-guided-3d-reconstruction-from-sparse-views.html" onclick="toggleFavorite(this, '2505.23481v2', 'PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>43</td>
  <td><a href="./papers/250523465v1-semantics-aware-human-motion-generation-from-audio-instructions.html">Semantics-Aware Human Motion Generation from Audio Instructions</a></td>
  <td>æå‡ºåŸºäºéŸ³é¢‘æŒ‡ä»¤çš„äººä½“åŠ¨ä½œç”Ÿæˆæ¡†æ¶ä»¥è§£å†³è¯­ä¹‰åŒ¹é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23465v1" data-paper-url="./papers/250523465v1-semantics-aware-human-motion-generation-from-audio-instructions.html" onclick="toggleFavorite(this, '2505.23465v1', 'Semantics-Aware Human Motion Generation from Audio Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>44</td>
  <td><a href="./papers/250523745v2-to-trust-or-not-to-trust-your-vision-language-models-prediction.html">To Trust Or Not To Trust Your Vision-Language Model's Prediction</a></td>
  <td>æå‡ºTrustVLMä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹é¢„æµ‹å¯ä¿¡åº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">IMoS</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23745v2" data-paper-url="./papers/250523745v2-to-trust-or-not-to-trust-your-vision-language-models-prediction.html" onclick="toggleFavorite(this, '2505.23745v2', 'To Trust Or Not To Trust Your Vision-Language Model&#39;s Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>45</td>
  <td><a href="./papers/250523586v1-weakly-supervised-localization-of-manipulated-image-regions-using-mu.html">Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features</a></td>
  <td>æå‡ºå¼±ç›‘ç£æ–¹æ³•ä»¥è§£å†³å›¾åƒç¯¡æ”¹åŒºåŸŸå®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23586v1" data-paper-url="./papers/250523586v1-weakly-supervised-localization-of-manipulated-image-regions-using-mu.html" onclick="toggleFavorite(this, '2505.23586v1', 'Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>46</td>
  <td><a href="./papers/250523439v1-viton-drr-details-retention-virtual-try-on-via-non-rigid-registratio.html">VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration</a></td>
  <td>æå‡ºVITON-DRRä»¥è§£å†³è™šæ‹Ÿè¯•è¡£ä¸­ç»†èŠ‚ä¿ç•™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23439v1" data-paper-url="./papers/250523439v1-viton-drr-details-retention-virtual-try-on-via-non-rigid-registratio.html" onclick="toggleFavorite(this, '2505.23439v1', 'VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)