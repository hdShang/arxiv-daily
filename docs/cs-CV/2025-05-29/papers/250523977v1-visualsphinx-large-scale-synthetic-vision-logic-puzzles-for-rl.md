---
layout: default
title: "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL"
---

# VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23977" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23977v1</a>
  <a href="https://arxiv.org/pdf/2505.23977.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23977v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23977v1', 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yichen Feng, Zhangchen Xu, Fengqing Jiang, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: Project page at https://visualsphinx.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisualSphinxä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `é€»è¾‘æ¨ç†` `åˆæˆæ•°æ®` `å¤šæ¨¡æ€å­¦ä¹ ` `å›¾åƒåˆæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­ç¼ºä¹å¤§è§„æ¨¡ã€ç»“æ„è‰¯å¥½çš„è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½æå‡ã€‚
2. æœ¬æ–‡æå‡ºVisualSphinxï¼Œé€šè¿‡åˆæˆè§†è§‰é€»è¾‘æ¨ç†æ•°æ®é›†ï¼Œè§£å†³äº†å›¾åƒåˆæˆä¸ç­”æ¡ˆå¯¹æ¥çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨VisualSphinxè®­ç»ƒçš„æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—æå‡ï¼Œé€»è¾‘ä¸€è‡´æ€§å’Œå¯è¯»æ€§å¢å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†å’Œé€»è¾‘å†³ç­–æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ï¼Œå°¤å…¶åœ¨å›¾è¡¨ç†è§£å’Œç©ºé—´é—®é¢˜è§£å†³ä¸­ã€‚ç„¶è€Œï¼Œå½“å‰çš„VLMæ¨ç†ç¼ºä¹å¤§è§„æ¨¡ä¸”ç»“æ„è‰¯å¥½çš„è®­ç»ƒæ•°æ®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†VisualSphinxï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡åˆæˆè§†è§‰é€»è¾‘æ¨ç†è®­ç»ƒæ•°æ®é›†ã€‚ä¸ºäº†è§£å†³å›¾åƒåˆæˆä¸ç­”æ¡ˆå¯¹æ¥çš„æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§è§„åˆ™åˆ°å›¾åƒçš„åˆæˆç®¡é“ï¼Œä»ç§å­é—®é¢˜ä¸­æå–å¹¶æ‰©å±•æ‹¼å›¾è§„åˆ™ï¼Œå¹¶ç”Ÿæˆç”¨äºæ‹¼å›¾æ ·æœ¬ç»„è£…çš„å›¾åƒåˆæˆä»£ç ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨VisualSphinxè®­ç»ƒçš„VLMåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„é€»è¾‘ä¸€è‡´æ€§å’Œå¯è¯»æ€§ï¼Œæ¨ç†èƒ½åŠ›çš„æå‡ä¹Ÿæœ‰åŠ©äºä»£æ•°ã€ç®—æœ¯å’Œå‡ ä½•æ¨ç†ç­‰å…¶ä»–ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå°è§„æ¨¡æ•°æ®é›†ï¼Œå¯¼è‡´æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§è§„åˆ™åˆ°å›¾åƒçš„åˆæˆç®¡é“ï¼Œé€šè¿‡ä»ç§å­é—®é¢˜ä¸­æå–å’Œæ‰©å±•æ‹¼å›¾è§„åˆ™ï¼Œç”Ÿæˆåˆæˆå›¾åƒã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æä¾›ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ï¼Œä»¥æå‡æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—å’Œæ¨¡å‹è®­ç»ƒæ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—è´Ÿè´£ä»ç§å­é—®é¢˜æå–è§„åˆ™å¹¶ç”Ÿæˆåˆæˆå›¾åƒï¼Œæ¨¡å‹è®­ç»ƒæ¨¡å—åˆ™ä½¿ç”¨ç”Ÿæˆçš„æ•°æ®è¿›è¡ŒVLMçš„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šVisualSphinxçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶åˆæˆæ•°æ®ç”Ÿæˆçš„è§„åˆ™åˆ°å›¾åƒç®¡é“ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–æ‰‹å·¥æ ‡æ³¨æ•°æ®çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å›¾åƒåˆæˆçš„è´¨é‡ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„ä»¥é€‚åº”åˆæˆä»»åŠ¡çš„éœ€æ±‚ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸é€»è¾‘è§„åˆ™çš„é«˜åº¦ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨VisualSphinxè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æå‡äº†15%ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„é€»è¾‘ä¸€è‡´æ€§å’Œå¯è¯»æ€§ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ¸¸æˆè®¾è®¡å’Œæœºå™¨äººç­‰ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›é«˜è´¨é‡çš„é€»è¾‘æ¨ç†è®­ç»ƒæ•°æ®ï¼Œæå‡ç›¸å…³ç³»ç»Ÿçš„æ™ºèƒ½æ°´å¹³ã€‚æœªæ¥ï¼ŒVisualSphinxå¯èƒ½æ¨åŠ¨æ›´å¤šå¤æ‚æ¨ç†ä»»åŠ¡çš„ç ”ç©¶ä¸åº”ç”¨ï¼Œä¿ƒè¿›å¤šæ¨¡æ€å­¦ä¹ çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.

