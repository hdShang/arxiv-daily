---
layout: default
title: VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning
---

# VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23504" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23504v1</a>
  <a href="https://arxiv.org/pdf/2505.23504.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23504v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23504v1', 'VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liyun Zhu, Qixiang Chen, Xi Shen, Xiaodong Cun

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/GVCLab/VAU-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVAU-R1ä»¥è§£å†³è§†é¢‘å¼‚å¸¸ç†è§£ä¸­çš„æ¨ç†èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¼‚å¸¸ç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å¾®è°ƒ` `é“¾å¼æ€ç»´åŸºå‡†` `å¯è§£é‡Šæ€§` `æ—¶ç©ºæ„ŸçŸ¥` `å¼‚å¸¸æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å¼‚å¸¸ç†è§£æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æ•æ‰å¼‚å¸¸äº‹ä»¶çš„å› æœå…³ç³»å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
2. VAU-R1æ¡†æ¶é€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å¾®è°ƒæŠ€æœ¯ï¼Œæå‡è§†é¢‘å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVAU-R1åœ¨é—®ç­”å‡†ç¡®æ€§ã€æ—¶é—´å®šä½å’Œæ¨ç†ä¸€è‡´æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å¼‚å¸¸ç†è§£ï¼ˆVAUï¼‰åœ¨æ™ºèƒ½åŸå¸‚ã€å®‰å…¨ç›‘æ§å’Œç¾å®³é¢„è­¦ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œä½†ç”±äºå¯¹ç»†ç²’åº¦æ—¶ç©ºæ„ŸçŸ¥å’Œæ¨¡ç³Šæƒ…å†µä¸‹çš„ç¨³å¥æ¨ç†çš„éœ€æ±‚ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡å¼‚å¸¸æ£€æµ‹å·²æœ‰è¿›å±•ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥æ•æ‰å¼‚å¸¸äº‹ä»¶çš„å› æœå’Œä¸Šä¸‹æ–‡æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VAU-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ•°æ®é«˜æ•ˆæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å¢å¼ºå¼‚å¸¸æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†VAU-Benchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è§†é¢‘å¼‚å¸¸æ¨ç†çš„é“¾å¼æ€ç»´åŸºå‡†ï¼ŒåŒ…å«å¤šé¡¹é€‰æ‹©é—®ç­”ã€è¯¦ç»†æ¨ç†ã€æ—¶é—´æ³¨é‡Šå’Œæè¿°æ€§æ ‡é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒVAU-R1åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­æ˜¾è‘—æé«˜äº†é—®ç­”å‡†ç¡®æ€§ã€æ—¶é—´å®šä½å’Œæ¨ç†ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†é¢‘å¼‚å¸¸ç†è§£ä¸­çš„æ¨ç†èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•æ‰å¼‚å¸¸äº‹ä»¶çš„å› æœå’Œä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°åŸºå‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVAU-R1æ¡†æ¶é€šè¿‡ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å¾®è°ƒï¼Œæå‡äº†å¯¹è§†é¢‘å¼‚å¸¸çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚æ­¤è®¾è®¡æ—¨åœ¨é€šè¿‡æ•°æ®é«˜æ•ˆçš„æ–¹å¼å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVAU-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç„¶åé€šè¿‡å¼ºåŒ–å¾®è°ƒä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œæœ€åè¿›è¡Œå¼‚å¸¸äº‹ä»¶çš„æ¨ç†å’Œç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šVAU-R1çš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†å¼ºåŒ–å¾®è°ƒæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚è§†é¢‘åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œæ›´åŠ æ³¨é‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€èåˆæŠ€æœ¯å¢å¼ºæ¨¡å‹å¯¹ä¸åŒç±»å‹æ•°æ®çš„å¤„ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVAU-R1åœ¨é—®ç­”å‡†ç¡®æ€§ä¸Šæé«˜äº†XX%ï¼Œåœ¨æ—¶é—´å®šä½å’Œæ¨ç†ä¸€è‡´æ€§æ–¹é¢ä¹Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒVAU-R1åœ¨å¤šé¡¹é€‰æ‹©é—®ç­”ä»»åŠ¡ä¸­è¾¾åˆ°äº†XX%çš„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†é¢‘å¼‚å¸¸ç†è§£ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŸå¸‚ç›‘æ§ã€å®‰å…¨é˜²èŒƒã€äº¤é€šç®¡ç†ç­‰ã€‚é€šè¿‡æå‡è§†é¢‘å¼‚å¸¸ç†è§£çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼ŒVAU-R1èƒ½å¤Ÿä¸ºå®æ—¶ç›‘æ§ç³»ç»Ÿæä¾›æ›´ä¸ºå¯é çš„æ”¯æŒï¼Œå¸®åŠ©å¿«é€Ÿè¯†åˆ«å’Œå“åº”å¼‚å¸¸äº‹ä»¶ï¼Œè¿›è€Œæé«˜å…¬å…±å®‰å…¨å’Œåº”æ€¥å“åº”èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.

