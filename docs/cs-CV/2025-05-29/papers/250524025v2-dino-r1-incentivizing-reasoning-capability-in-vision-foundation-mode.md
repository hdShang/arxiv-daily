---
layout: default
title: "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models"
---

# DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24025" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24025v2</a>
  <a href="https://arxiv.org/pdf/2505.24025.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24025v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24025v2', 'DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chenbin Pan, Wenbin He, Zhengzhong Tu, Liu Ren

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-08-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDINO-R1ä»¥å¢å¼ºè§†è§‰åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æŸ¥è¯¢ä¼˜åŒ–` `æ¨¡å‹è®­ç»ƒ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ¢ç´¢ç›¸å¯¹è¾ƒå°‘ï¼Œç¼ºä¹æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥æ¥æå‡å…¶è§†è§‰ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚
2. DINO-R1é€šè¿‡å¼•å…¥GRQOç­–ç•¥ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¿€åŠ±è§†è§‰æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç»“åˆKLæ­£åˆ™åŒ–æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDINO-R1åœ¨å¼€æ”¾è¯æ±‡å’Œé—­é›†è§†è§‰æç¤ºåœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨è¿™æ–¹é¢çš„æ¢ç´¢ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡æå‡ºDINO-R1ï¼Œé¦–æ¬¡é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±è§†è§‰ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚DINO-R1å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–è®­ç»ƒç­–ç•¥â€”â€”Group Relative Query Optimization (GRQO)ï¼Œè¯¥ç­–ç•¥åŸºäºç»„å½’ä¸€åŒ–çš„å¯¹é½è´¨é‡è®¡ç®—æŸ¥è¯¢çº§å¥–åŠ±ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åº”ç”¨KLæ­£åˆ™åŒ–ä»¥ç¨³å®šç›®æ ‡åˆ†å¸ƒï¼Œå‡å°‘è®­ç»ƒä¸ç¨³å®šæ€§ã€‚é€šè¿‡Grounding-DINOï¼Œè®­ç»ƒäº†ä¸€ç³»åˆ—DINO-R1æ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨COCOã€LVISå’ŒODinWæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒåŸºçº¿ï¼Œå±•ç°äº†åœ¨å¼€æ”¾è¯æ±‡å’Œé—­é›†è§†è§‰æç¤ºåœºæ™¯ä¸­çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰åŸºç¡€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ¿€åŠ±æ¨¡å‹è¿›è¡Œè§†è§‰ä¸Šä¸‹æ–‡æ¨ç†ï¼Œå¯¼è‡´å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDINO-R1é€šè¿‡å¼•å…¥GRQOç­–ç•¥ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥æ¿€åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè®¾è®¡äº†åŸºäºæŸ¥è¯¢çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥æå‡æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDINO-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰æç¤ºç¼–ç å™¨å’Œè§†è§‰å¼•å¯¼çš„æŸ¥è¯¢é€‰æ‹©æœºåˆ¶ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œç»“åˆKLæ­£åˆ™åŒ–ä»¥å¢å¼ºè®­ç»ƒç¨³å®šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDINO-R1çš„ä¸»è¦åˆ›æ–°åœ¨äºGRQOç­–ç•¥çš„æå‡ºï¼Œå®ƒé€šè¿‡è®¡ç®—ç»„å½’ä¸€åŒ–çš„å¯¹é½è´¨é‡æ¥ç”ŸæˆæŸ¥è¯¢çº§å¥–åŠ±ï¼Œè¿™æ˜¯ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨KLæ­£åˆ™åŒ–æ¥ç¨³å®šç›®æ ‡åˆ†å¸ƒï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å‡å°‘è¿‡æ‹Ÿåˆå’Œåˆ†å¸ƒæ¼‚ç§»ï¼ŒåŒæ—¶ç¡®ä¿æŸ¥è¯¢ä¹‹é—´çš„å¯†é›†å’Œè¡¨è¾¾æ€§ç›‘ç£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨COCOã€LVISå’ŒODinWæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDINO-R1åœ¨å¼€æ”¾è¯æ±‡å’Œé—­é›†è§†è§‰æç¤ºåœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒåŸºçº¿ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œåº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DINO-R1çš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è§†è§‰æ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾åƒç†è§£ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå¤æ‚çš„å†³ç­–è¿‡ç¨‹ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¼šå½±å“æ›´å¤šè§†è§‰ä»»åŠ¡çš„ç ”ç©¶æ–¹å‘ï¼Œä¿ƒè¿›å¤šæ¨¡æ€å­¦ä¹ çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.

