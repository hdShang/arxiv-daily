---
layout: default
title: "EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis"
---

# EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23601" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.23601v2</a>
  <a href="https://arxiv.org/pdf/2505.23601.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23601v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23601v2', 'EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shengyuan Liu, Boyun Zheng, Wenting Chen, Zhihao Peng, Zhenfei Yin, Jing Shao, Jiancong Hu, Yixuan Yuan

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-29 (Êõ¥Êñ∞: 2025-09-24)

**Â§áÊ≥®**: 40 pages, 22 figures; Accepted by NeurIPS 2025 Dataset and Benchmark Track

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EndoBench‰ª•Ëß£ÂÜ≥ÂÜÖÁ™•ÈïúÂàÜÊûêÂ§öÊ®°ÊÄÅÊ®°ÂûãËØÑ‰º∞‰∏çË∂≥ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂÜÖÁ™•ÈïúÂàÜÊûê` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Âü∫ÂáÜÊµãËØï` `‰∏¥Â∫ä‰ªªÂä°` `ËßÜËßâÊèêÁ§∫` `Ê®°ÂûãËØÑ‰º∞` `ÂåªÂ≠¶‰∫∫Â∑•Êô∫ËÉΩ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂÜÖÁ™•ÈïúÂàÜÊûêÂü∫ÂáÜÊµãËØï‰ªÖË¶ÜÁõñÁâπÂÆöÂú∫ÊôØÂíåÂ∞ëÈáè‰ªªÂä°ÔºåÊó†Ê≥ïÂèçÊò†ÁúüÂÆû‰∏¥Â∫äÈúÄÊ±Ç„ÄÇ
2. EndoBenchÊòØ‰∏Ä‰∏™ÁªºÂêàÊÄßÂü∫ÂáÜÔºåÊó®Âú®ÂÖ®Èù¢ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÜÖÁ™•ÈïúÂÆûË∑µ‰∏≠ÁöÑË°®Áé∞„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰∏ìÊúâÊ®°ÂûãÊï¥‰ΩìË°®Áé∞‰ºò‰∫éÂºÄÊ∫êÂíåÂåªÂ≠¶‰∏ìÁî®Ê®°ÂûãÔºå‰ΩÜ‰ªçËêΩÂêé‰∫é‰∫∫Á±ª‰∏ìÂÆ∂„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂÜÖÁ™•ÈïúÁ®ãÂ∫èÂú®ÂÜÖÈÉ®ÁñæÁóÖÁöÑËØäÊñ≠ÂíåÊ≤ªÁñó‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåËÄåÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂÜÖÁ™•ÈïúÂàÜÊûê‰∏≠Â∫îÁî®Êó•ÁõäÂπøÊ≥õ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÂü∫ÂáÜÊµãËØïÈÄöÂ∏∏Â±ÄÈôê‰∫éÁâπÂÆöÁöÑÂÜÖÁ™•ÈïúÂú∫ÊôØÂíåÂ∞ëÈáè‰∏¥Â∫ä‰ªªÂä°ÔºåÊó†Ê≥ïÊçïÊçâÂÜÖÁ™•ÈïúÂú∫ÊôØÁöÑÁúüÂÆûÂ§öÊ†∑ÊÄßÂèä‰∏¥Â∫äÂ∑•‰ΩúÊµÅÁ®ãÊâÄÈúÄÁöÑÂÖ®Êñπ‰ΩçÊäÄËÉΩ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜEndoBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®ËÆæËÆ°Áî®‰∫éËØÑ‰º∞MLLMsÂú®ÂÜÖÁ™•ÈïúÂÆûË∑µ‰∏≠Â§öÁª¥ËÉΩÂäõÁöÑÁªºÂêàÂü∫ÂáÜ„ÄÇEndoBenchÊ∂µÁõñ4Áßç‰∏çÂêåÁöÑÂÜÖÁ™•ÈïúÂú∫ÊôØ„ÄÅ12‰∏™‰∏ì‰∏ö‰∏¥Â∫ä‰ªªÂä°Âèä12‰∏™Ê¨°‰ªªÂä°ÔºåÂπ∂Êèê‰æõ5ÁßçËßÜËßâÊèêÁ§∫Á≤íÂ∫¶ÔºåÁîüÊàê6832ÂØπÁªèËøá‰∏•Ê†ºÈ™åËØÅÁöÑVQAÂØπ„ÄÇÊàë‰ª¨ÁöÑÂ§öÁª¥ËØÑ‰º∞Ê°ÜÊû∂ÂèçÊò†‰∫Ü‰∏¥Â∫äÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÖ®Èù¢ËØÑ‰º∞MLLMsÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÊÑüÁü•ÂíåËØäÊñ≠ËÉΩÂäõ„ÄÇÊàë‰ª¨ÂØπ23‰∏™ÊúÄÂÖàËøõÁöÑÊ®°ÂûãËøõË°å‰∫ÜÂü∫ÂáÜÊµãËØïÔºåÂπ∂Â∞Ü‰∫∫Á±ª‰∏¥Â∫äÂåªÁîüÁöÑË°®Áé∞‰Ωú‰∏∫ÂèÇËÄÉÊ†áÂáÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂΩìÂâçÂÜÖÁ™•ÈïúÂàÜÊûêÁöÑÂü∫ÂáÜÊµãËØïÁº∫‰πèÂÖ®Èù¢ÊÄßÔºåÊó†Ê≥ïÊ∂µÁõñÂ§öÊ†∑ÂåñÁöÑ‰∏¥Â∫äÂú∫ÊôØÂíå‰ªªÂä°ÔºåÂØºËá¥Ê®°ÂûãËØÑ‰º∞‰∏çÂ§üÂáÜÁ°Æ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEndoBenchÈÄöËøáËÆæËÆ°‰∏Ä‰∏™Â§öÁª¥Â∫¶ÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÊ∂µÁõñ‰∏çÂêåÁöÑÂÜÖÁ™•ÈïúÂú∫ÊôØÂíå‰ªªÂä°ÔºåÊó®Âú®ÂÖ®Èù¢ËØÑ‰º∞MLLMsÁöÑËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÊã¨4ÁßçÂÜÖÁ™•ÈïúÂú∫ÊôØ„ÄÅ12‰∏™‰∏ªË¶Å‰∏¥Â∫ä‰ªªÂä°ÂèäÂÖ∂Ê¨°‰ªªÂä°ÔºåÁªìÂêà5ÁßçËßÜËßâÊèêÁ§∫Á≤íÂ∫¶ÔºåÂΩ¢Êàê6832ÂØπVQAÂØπÔºåÊ®°ÊãüÁúüÂÆûÁöÑ‰∏¥Â∫äÂ∑•‰ΩúÊµÅÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEndoBenchÊòØÈ¶ñ‰∏™ÈíàÂØπÂÜÖÁ™•ÈïúÂàÜÊûêÁöÑÁªºÂêàÊÄßÂü∫ÂáÜÔºåËÉΩÂ§üÂÖ®Èù¢ÂèçÊò†Ê®°ÂûãÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞ÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâËØÑ‰º∞ÁöÑÁ©∫ÁôΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËØÑ‰º∞ËøáÁ®ã‰∏≠ÔºåÈááÁî®‰∫Ü‰∏•Ê†ºÁöÑÈ™åËØÅÊú∫Âà∂ÔºåÁ°Æ‰øùÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÂíå‰ª£Ë°®ÊÄßÔºåÂêåÊó∂ÂºïÂÖ•‰∫Ü‰∫∫Á±ª‰∏ìÂÆ∂Ë°®Áé∞‰Ωú‰∏∫ÂèÇËÄÉÊ†áÂáÜ„ÄÇÂÆûÈ™å‰∏≠ËøòËÄÉÂØü‰∫ÜÊèêÁ§∫Ê†ºÂºèÂíå‰ªªÂä°Â§çÊùÇÊÄßÂØπÊ®°ÂûãË°®Áé∞ÁöÑÂΩ±Âìç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏ìÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êï¥‰ΩìË°®Áé∞‰∏ä‰ºò‰∫éÂºÄÊ∫êÂíåÂåªÂ≠¶‰∏ìÁî®Ê®°ÂûãÔºå‰ΩÜ‰ªçÊú™ËææÂà∞‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑÊ∞¥Âπ≥„ÄÇÂåªÂ≠¶È¢ÜÂüüÁöÑÁõëÁù£ÂæÆË∞ÉÊòæËëóÊèêÈ´ò‰∫Ü‰ªªÂä°ÁâπÂÆöÁöÑÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂Ê®°ÂûãË°®Áé∞ÂØπÊèêÁ§∫Ê†ºÂºèÂíå‰ªªÂä°Â§çÊùÇÊÄßÊïèÊÑü„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EndoBenchÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂåªÁñóÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÜÖÁ™•ÈïúÂàÜÊûê„ÄÅÁñæÁóÖËØäÊñ≠ÂíåÊ≤ªÁñóÂÜ≥Á≠ñÊîØÊåÅÁ≠âÊñπÈù¢„ÄÇÈÄöËøáÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËØÑ‰º∞Ê†áÂáÜÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êõ¥Êô∫ËÉΩÁöÑÂåªÁñóËæÖÂä©Á≥ªÁªüÁöÑÂèëÂ±ïÔºåÊîπÂñÑ‰∏¥Â∫äÂ∑•‰ΩúÊïàÁéáÂíåÊÇ£ËÄÖÊä§ÁêÜË¥®Èáè„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Endoscopic procedures are essential for diagnosing and treating internal diseases, and multi-modal large language models (MLLMs) are increasingly applied to assist in endoscopy analysis. However, current benchmarks are limited, as they typically cover specific endoscopic scenarios and a small set of clinical tasks, failing to capture the real-world diversity of endoscopic scenarios and the full range of skills needed in clinical workflows. To address these issues, we introduce EndoBench, the first comprehensive benchmark specifically designed to assess MLLMs across the full spectrum of endoscopic practice with multi-dimensional capacities. EndoBench encompasses 4 distinct endoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks, and 5 levels of visual prompting granularities, resulting in 6,832 rigorously validated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation framework mirrors the clinical workflow--spanning anatomical recognition, lesion analysis, spatial localization, and surgical operations--to holistically gauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios. We benchmark 23 state-of-the-art models, including general-purpose, medical-specialized, and proprietary MLLMs, and establish human clinician performance as a reference standard. Our extensive experiments reveal: (1) proprietary MLLMs outperform open-source and medical-specialized models overall, but still trail human experts; (2) medical-domain supervised fine-tuning substantially boosts task-specific accuracy; and (3) model performance remains sensitive to prompt format and clinical task complexity. EndoBench establishes a new standard for evaluating and advancing MLLMs in endoscopy, highlighting both progress and persistent gaps between current models and expert clinical reasoning. We publicly release our benchmark and code.

