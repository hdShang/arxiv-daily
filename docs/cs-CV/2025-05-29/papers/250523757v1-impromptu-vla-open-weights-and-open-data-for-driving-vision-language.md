---
layout: default
title: "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models"
---

# Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23757" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23757v1</a>
  <a href="https://arxiv.org/pdf/2505.23757.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23757v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23757v1', 'Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu, Zeda Wang, Wenyi Li, Leichen Wang, Xingtao Hu, Hao Sun, Hang Zhao, Hao Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: Project page: https://github.com/ahydchh/Impromptu-VLA

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ahydchh/Impromptu-VLA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºImpromptu VLAä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `è‡ªåŠ¨é©¾é©¶` `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®é›†æ„å»º` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶çš„éç»“æ„åŒ–åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹é’ˆå¯¹æ€§çš„åŸºå‡†æµ‹è¯•å’Œæ•°æ®é›†ã€‚
2. è®ºæ–‡æå‡ºäº†Impromptu VLAæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡80,000ä¸ªè§†é¢‘ç‰‡æ®µï¼Œå¹¶æä¾›ä¸°å¯Œçš„é—®ç­”æ³¨é‡Šä»¥æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨NeuroNCAPè¯„åˆ†å’ŒnuScenesè½¨è¿¹é¢„æµ‹ä¸­æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†æ¥è¿‘æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨éç»“æ„åŒ–çš„è¾¹ç¼˜æ¡ˆä¾‹åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºç¼ºä¹é’ˆå¯¹æ€§çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Impromptu VLAã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè´¡çŒ®æ˜¯Impromptu VLAæ•°æ®é›†ï¼šè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡80,000ä¸ªç²¾å¿ƒç­–åˆ’çš„è§†é¢‘ç‰‡æ®µï¼Œæºè‡ª8ä¸ªå¼€æºå¤§è§„æ¨¡æ•°æ®é›†ä¸­çš„200ä¸‡æºç‰‡æ®µã€‚è¯¥æ•°æ®é›†åŸºäºæˆ‘ä»¬æ–°é¢–çš„å››ç±»æŒ‘æˆ˜æ€§éç»“æ„åŒ–åˆ†ç±»æ³•ï¼Œå¹¶æä¾›ä¸°å¯Œçš„è§„åˆ’å¯¼å‘é—®ç­”æ³¨é‡Šå’ŒåŠ¨ä½œè½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬æ•°æ®é›†è®­ç»ƒçš„VLAåœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæ”¹å–„äº†é—­ç¯NeuroNCAPè¯„åˆ†å’Œç¢°æ’ç‡ï¼Œå¹¶åœ¨å¼€æ”¾ç¯nuScenesè½¨è¿¹é¢„æµ‹ä¸­è¾¾åˆ°äº†æ¥è¿‘æœ€å…ˆè¿›çš„L2å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é—®ç­”å¥—ä»¶ä½œä¸ºæœ‰æ•ˆçš„è¯Šæ–­å·¥å…·ï¼Œæ­ç¤ºäº†VLMåœ¨æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’æ–¹é¢çš„æ˜æ˜¾æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨https://github.com/ahydchh/Impromptu-VLAè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å¯¹éç»“æ„åŒ–åœºæ™¯çš„é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹é’ˆå¯¹æ€§åŸºå‡†æµ‹è¯•çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„æ€§èƒ½å—åˆ°é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºImpromptu VLAæ•°æ®é›†ï¼Œæä¾›ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®å’Œé—®ç­”æ³¨é‡Šï¼Œå¢å¼ºæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å­¦ä¹ èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ•°æ®æ ‡æ³¨ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é‡‡é›†é˜¶æ®µä»å¤šä¸ªå¼€æºæ•°æ®é›†ä¸­æå–è§†é¢‘ç‰‡æ®µï¼Œæ ‡æ³¨é˜¶æ®µåˆ™ä¸ºæ¯ä¸ªç‰‡æ®µæä¾›é—®ç­”å’ŒåŠ¨ä½œè½¨è¿¹ä¿¡æ¯ã€‚æ¨¡å‹è®­ç»ƒé˜¶æ®µä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæœ€åé€šè¿‡æ ‡å‡†åŸºå‡†è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šImpromptu VLAæ•°æ®é›†çš„æ„å»ºæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯å…¶é’ˆå¯¹æ€§å¼ºçš„éç»“æ„åŒ–åœºæ™¯åˆ†ç±»å’Œä¸°å¯Œçš„é—®ç­”æ³¨é‡Šï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šæ¨¡æ€å­¦ä¹ æ•ˆæœï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥æ›´å¥½åœ°èåˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Impromptu VLAæ•°æ®é›†è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨é—­ç¯NeuroNCAPè¯„åˆ†ä¸Šæ˜¾è‘—æé«˜ï¼Œç¢°æ’ç‡é™ä½ï¼Œä¸”åœ¨å¼€æ”¾ç¯nuScenesè½¨è¿¹é¢„æµ‹ä¸­è¾¾åˆ°äº†æ¥è¿‘æœ€å…ˆè¿›çš„L2å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½äº¤é€šæŠ€æœ¯çš„å‘å±•ã€‚æœªæ¥ï¼ŒImpromptu VLAæ•°æ®é›†ä¹Ÿå¯èƒ½ä¸ºå…¶ä»–å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡æä¾›å‚è€ƒå’Œå€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA.

