---
layout: default
title: Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought
---

# Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23766" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23766v1</a>
  <a href="https://arxiv.org/pdf/2505.23766.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23766v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23766v1', 'Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, Zhiding Yu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: CVPR 2025. Project Page: https://yunzeman.github.io/argus/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://yunzeman.github.io/argus/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºArgusä»¥è§£å†³è§†è§‰æ¨ç†ä¸­çš„æ³¨æ„åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æ¨ç†` `æ³¨æ„åŠ›æœºåˆ¶` `å¯¹è±¡ä¸­å¿ƒåŸºç¡€` `è§†è§‰é“¾å¼æ€ç»´` `æ™ºèƒ½è§†è§‰åŠ©æ‰‹` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹ç²¾ç¡®çš„è§†è§‰å…³æ³¨ï¼Œå¯¼è‡´æ¨ç†å‡†ç¡®æ€§ä¸è¶³ã€‚
2. Argusé€šè¿‡å¼•å…¥å¯¹è±¡ä¸­å¿ƒçš„è§†è§‰æ³¨æ„åŠ›åŸºç¡€æœºåˆ¶ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€æ¨ç†ä¸­çš„è§†è§‰å…³æ³¨èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒArgusåœ¨å¤šæ¨¡æ€æ¨ç†å’ŒæŒ‡ä»£å¯¹è±¡åŸºç¡€ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å±•ç°äº†æ˜¾è‘—èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®è§†è§‰å…³æ³¨çš„è§†è§‰ä¸­å¿ƒåœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºArgusï¼Œé€šè¿‡æ–°çš„è§†è§‰æ³¨æ„åŠ›åŸºç¡€æœºåˆ¶æ¥è§£å†³è¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„åŸºç¡€ä½œä¸ºè§†è§‰é“¾å¼æ€ç»´ä¿¡å·ï¼Œä»è€Œåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å®ç°æ›´æœ‰æ•ˆçš„ç›®æ ‡æ¡ä»¶è§†è§‰æ³¨æ„åŠ›ã€‚å¯¹å¤šé¡¹åŸºå‡†çš„è¯„ä¼°è¡¨æ˜ï¼ŒArgusåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡å’ŒæŒ‡ä»£å¯¹è±¡åŸºç¡€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ·±å…¥åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†Argusçš„å„ç§è®¾è®¡é€‰æ‹©ï¼Œå¹¶æ­ç¤ºäº†æ˜¾å¼è¯­è¨€å¼•å¯¼çš„è§†è§‰å…´è¶£åŒºåŸŸå‚ä¸åœ¨MLLMsä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†ä»è§†è§‰ä¸­å¿ƒè§†è§’æ¨è¿›å¤šæ¨¡æ€æ™ºèƒ½çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­å¯¹è§†è§‰å…³æ³¨çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨éœ€è¦ç²¾ç¡®è§†è§‰ä¿¡æ¯æ—¶å¸¸å¸¸æ— æ³•æä¾›è¶³å¤Ÿçš„æ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šArgusçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨å¯¹è±¡ä¸­å¿ƒçš„è§†è§‰æ³¨æ„åŠ›åŸºç¡€æœºåˆ¶ï¼Œå°†è§†è§‰é“¾å¼æ€ç»´ä¿¡å·ä¸ç›®æ ‡æ¡ä»¶è§†è§‰æ³¨æ„åŠ›ç›¸ç»“åˆï¼Œä»è€Œæå‡æ¨ç†çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šArgusçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯è§†è§‰è¾“å…¥çš„å¤„ç†æ¨¡å—ï¼Œç„¶åæ˜¯å¯¹è±¡è¯†åˆ«å’Œæ³¨æ„åŠ›åˆ†é…æ¨¡å—ï¼Œæœ€åæ˜¯å¤šæ¨¡æ€æ¨ç†æ¨¡å—ï¼Œç¡®ä¿è§†è§‰ä¿¡æ¯ä¸è¯­è¨€ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šArgusçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†æ˜¾å¼çš„è¯­è¨€å¼•å¯¼æœºåˆ¶ï¼Œä½¿å¾—è§†è§‰åŒºåŸŸçš„é€‰æ‹©æ›´åŠ ç²¾å‡†ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒArgusé‡‡ç”¨äº†ä¼˜åŒ–çš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡è§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„æƒé‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å¤šå±‚æ¬¡çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ä¸åŒè§†è§‰åŒºåŸŸçš„å…³æ³¨èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒArgusåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†çº¦15%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨æŒ‡ä»£å¯¹è±¡åŸºç¡€ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†å…¶è®¾è®¡çš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è§†è§‰åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼ŒArgusèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å‡†ç¡®åœ°ç†è§£å’Œå“åº”ç”¨æˆ·éœ€æ±‚ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language tasks, yet they often struggle with vision-centric scenarios where precise visual focus is needed for accurate reasoning. In this paper, we introduce Argus to address these limitations with a new visual attention grounding mechanism. Our approach employs object-centric grounding as visual chain-of-thought signals, enabling more effective goal-conditioned visual attention during multimodal reasoning tasks. Evaluations on diverse benchmarks demonstrate that Argus excels in both multimodal reasoning tasks and referring object grounding tasks. Extensive analysis further validates various design choices of Argus, and reveals the effectiveness of explicit language-guided visual region-of-interest engagement in MLLMs, highlighting the importance of advancing multimodal intelligence from a visual-centric perspective. Project page: https://yunzeman.github.io/argus/

