---
layout: default
title: "MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking"
---

# MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24026" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24026v1</a>
  <a href="https://arxiv.org/pdf/2505.24026.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24026v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24026v1', 'MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Numair Nadeem, Muhammad Hamza Asad, Saeed Anwar, Abdul Bais

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: 11 pages, 5 figures, presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025. Reviewer comments available upon request

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMaskAdaptä»¥è§£å†³å†œä¸šé¢†åŸŸæ— ç›‘ç£åŸŸé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— ç›‘ç£åŸŸé€‚åº”` `è¯­ä¹‰åˆ†å‰²` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦å­¦ä¹ ` `å†œä¸šå›¾åƒå¤„ç†` `å‡ ä½•æ„ŸçŸ¥æ©è†œ` `ç‰¹å¾èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•åœ¨å¤„ç†å†œä¸šå›¾åƒæ—¶ï¼Œå®¹æ˜“å—åˆ°é®æŒ¡å’Œè§†è§‰æ··åˆçš„å½±å“ï¼Œå¯¼è‡´åˆ†ç±»é”™è¯¯ã€‚
2. æœ¬æ–‡æå‡ºMaskAdaptï¼Œé€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå°†RGBå›¾åƒä¸æ·±åº¦æ•°æ®ç»“åˆï¼Œåˆ©ç”¨æ·±åº¦æ¢¯åº¦æå‡åˆ†å‰²ç²¾åº¦ã€‚
3. åœ¨çœŸå®å†œä¸šæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMaskAdaptåœ¨åˆ†å‰²å‡å€¼äº¤å¹¶æ¯”ï¼ˆmIOUï¼‰ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä½œç‰©å’Œæ‚è‰çš„è¯­ä¹‰åˆ†å‰²å¯¹ç²¾å‡†å†œä¸šç®¡ç†è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºåŠ³åŠ¨å¯†é›†å‹çš„åƒç´ çº§æ ‡æ³¨ã€‚æ¨¡å‹åœ¨ä¸åŒç”°åœ°ï¼ˆæºåŸŸä¸ç›®æ ‡åŸŸï¼‰é—´çš„æ³›åŒ–èƒ½åŠ›å—é™äºåŸŸåç§»ï¼Œå¦‚å…‰ç…§ã€ç›¸æœºè®¾ç½®ã€åœŸå£¤æˆåˆ†å’Œä½œç‰©ç”Ÿé•¿é˜¶æ®µçš„å˜åŒ–ã€‚æ— ç›‘ç£åŸŸé€‚åº”ï¼ˆUDAï¼‰æ–¹æ³•è™½ç„¶å¯ä»¥åœ¨æ²¡æœ‰ç›®æ ‡åŸŸæ ‡ç­¾çš„æƒ…å†µä¸‹è¿›è¡Œé€‚åº”ï¼Œä½†åœ¨å¤„ç†é®æŒ¡å’Œä½œç‰©ä¸æ‚è‰ä¹‹é—´çš„è§†è§‰æ··åˆæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MaskAdaptï¼Œé€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå°†RGBå›¾åƒä¸æ·±åº¦æ•°æ®ç‰¹å¾ç»“åˆï¼Œæå‡åˆ†å‰²ç²¾åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—æ·±åº¦å›¾çš„æ·±åº¦æ¢¯åº¦ï¼Œæ•æ‰ç©ºé—´è¿‡æ¸¡ï¼Œå¸®åŠ©è§£å†³çº¹ç†æ¨¡ç³Šé—®é¢˜ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶ç»†åŒ–RGBç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œå®ç°æ›´æ¸…æ™°çš„è¾¹ç•Œåˆ’åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskAdaptåœ¨çœŸå®å†œä¸šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›UDAæ–¹æ³•ï¼Œæå‡äº†ä¸åŒç”°åœ°æ¡ä»¶ä¸‹çš„åˆ†å‰²å‡å€¼äº¤å¹¶æ¯”ï¼ˆmIOUï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å†œä¸šé¢†åŸŸä¸­ä½œç‰©ä¸æ‚è‰çš„è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œç°æœ‰æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•åœ¨åŸŸåç§»æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶åœ¨é®æŒ¡å’Œè§†è§‰æ··åˆçš„æƒ…å†µä¸‹æ˜“å¯¼è‡´é”™è¯¯åˆ†ç±»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMaskAdapté€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå°†RGBå›¾åƒä¸æ·±åº¦æ•°æ®ç›¸ç»“åˆï¼Œåˆ©ç”¨æ·±åº¦æ¢¯åº¦æ•æ‰ç©ºé—´è¿‡æ¸¡ï¼Œå¢å¼ºæ¨¡å‹å¯¹çº¹ç†æ¨¡ç³Šçš„è¯†åˆ«èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬RGBå›¾åƒè¾“å…¥ã€æ·±åº¦æ•°æ®å¤„ç†ã€æ·±åº¦æ¢¯åº¦è®¡ç®—ã€ç‰¹å¾èåˆå’Œäº¤å‰æ³¨æ„æœºåˆ¶ï¼Œæœ€ç»ˆè¾“å‡ºç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šMaskAdaptçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥å‡ ä½•æ„ŸçŸ¥çš„æ©è†œç­–ç•¥ï¼Œé€šè¿‡æ°´å¹³ã€å‚ç›´å’Œéšæœºæ©è†œè®­ç»ƒï¼Œä¿ƒä½¿æ¨¡å‹å…³æ³¨æ›´å¹¿æ³›çš„ç©ºé—´ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜è§†è§‰è¯†åˆ«çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†äº¤å‰æ³¨æ„æœºåˆ¶æ¥ç»†åŒ–RGBç‰¹å¾è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ·±åº¦æ¢¯åº¦æ¥å¢å¼ºç‰¹å¾çš„ç©ºé—´ä¿¡æ¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸‹çš„é€‚åº”æ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾ç½®ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥æé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨çœŸå®å†œä¸šæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMaskAdaptåœ¨åˆ†å‰²å‡å€¼äº¤å¹¶æ¯”ï¼ˆmIOUï¼‰ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç²¾å‡†å†œä¸šã€å†œä½œç‰©ç›‘æµ‹å’Œæ™ºèƒ½å†œä¸šæœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜ä½œç‰©ä¸æ‚è‰çš„åˆ†å‰²ç²¾åº¦ï¼ŒMaskAdaptå¯ä»¥å¸®åŠ©å†œæ°‘æ›´æœ‰æ•ˆåœ°ç®¡ç†å†œç”°ï¼Œå‡å°‘åŒ–è‚¥å’Œå†œè¯çš„ä½¿ç”¨ï¼Œä»è€Œå®ç°å¯æŒç»­å†œä¸šå‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸçš„å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Semantic segmentation of crops and weeds is crucial for site-specific farm management; however, most existing methods depend on labor intensive pixel-level annotations. A further challenge arises when models trained on one field (source domain) fail to generalize to new fields (target domain) due to domain shifts, such as variations in lighting, camera setups, soil composition, and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by enabling adaptation without target-domain labels, but current UDA methods struggle with occlusions and visual blending between crops and weeds, leading to misclassifications in real-world conditions. To overcome these limitations, we introduce MaskAdapt, a novel approach that enhances segmentation accuracy through multimodal contextual learning by integrating RGB images with features derived from depth data. By computing depth gradients from depth maps, our method captures spatial transitions that help resolve texture ambiguities. These gradients, through a cross-attention mechanism, refines RGB feature representations, resulting in sharper boundary delineation. In addition, we propose a geometry-aware masking strategy that applies horizontal, vertical, and stochastic masks during training. This encourages the model to focus on the broader spatial context for robust visual recognition. Evaluations on real agricultural datasets demonstrate that MaskAdapt consistently outperforms existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation mean Intersection over Union (mIOU) across diverse field conditions.

