---
layout: default
title: MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence
---

# MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23764" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23764v2</a>
  <a href="https://arxiv.org/pdf/2505.23764.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23764v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23764v2', 'MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-09-25)

**å¤‡æ³¨**: 34 pages. A comprehensive, fully human-curated, multi-image-based spatial intelligence benchmark with reasoning annotation for MLLMs. Project page: https://runsenxu.com/projects/MMSI_Bench

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMSI-Benchä»¥è§£å†³å¤šå›¾åƒç©ºé—´æ™ºèƒ½è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šå›¾åƒæ¨ç†` `ç©ºé—´æ™ºèƒ½` `è§†è§‰é—®ç­”` `åŸºå‡†æµ‹è¯•` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä»…å…³æ³¨å•å›¾åƒå…³ç³»ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. MMSI-Benché€šè¿‡è®¾è®¡1000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œç»“åˆ120,000å¼ å›¾åƒï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢è¯„ä¼°å¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„åŸºå‡†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å¼ºçš„å¼€æºæ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º30%ï¼Œè€Œäººç±»çš„å‡†ç¡®ç‡é«˜è¾¾97%ï¼Œè¡¨æ˜è¯¥é¢†åŸŸä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç©ºé—´æ™ºèƒ½å¯¹äºåœ¨å¤æ‚ç‰©ç†ä¸–ç•Œä¸­è¿ä½œçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†ä»…æ¢è®¨å•å›¾åƒå…³ç³»ï¼Œæ— æ³•è¯„ä¼°å®é™…åº”ç”¨æ‰€éœ€çš„å¤šå›¾åƒç©ºé—´æ¨ç†ã€‚æœ¬æ–‡ä»‹ç»äº†MMSI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¤šå›¾åƒç©ºé—´æ™ºèƒ½çš„è§†è§‰é—®ç­”åŸºå‡†ã€‚å…­ä½3Dè§†è§‰ç ”ç©¶è€…èŠ±è´¹è¶…è¿‡300å°æ—¶ç²¾å¿ƒè®¾è®¡äº†1000ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼ŒåŸºäºè¶…è¿‡120,000å¼ å›¾åƒï¼Œå¹¶é…æœ‰ç²¾å¿ƒè®¾è®¡çš„å¹²æ‰°é¡¹å’Œé€æ­¥æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¯„ä¼°äº†34ä¸ªå¼€æºå’Œä¸“æœ‰çš„MLLMsï¼Œç»“æœæ˜¾ç¤ºï¼Œæœ€å¼ºçš„å¼€æºæ¨¡å‹ä»…è¾¾åˆ°çº¦30%çš„å‡†ç¡®ç‡ï¼Œè€ŒOpenAIçš„o3æ¨ç†æ¨¡å‹è¾¾åˆ°40%ï¼Œè€Œäººç±»å¾—åˆ†ä¸º97%ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†MMSI-Benchçš„æŒ‘æˆ˜æ€§åŠæœªæ¥ç ”ç©¶çš„å¹¿é˜”ç©ºé—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æ— æ³•è¯„ä¼°å¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºå•å›¾åƒå…³ç³»ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMMSI-Benché€šè¿‡è®¾è®¡å¤šé¡¹é€‰æ‹©é¢˜å’Œé€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å¤šå›¾åƒåœºæ™¯ä¸­çš„ç©ºé—´æ™ºèƒ½èƒ½åŠ›ï¼Œæä¾›æ›´å…·æŒ‘æˆ˜æ€§çš„æµ‹è¯•ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢˜ç›®è®¾è®¡ã€å›¾åƒé€‰æ‹©ã€å¹²æ‰°é¡¹è®¾è®¡å’Œæ¨ç†è¿‡ç¨‹çš„æ ‡æ³¨ã€‚ç ”ç©¶è€…ä»¬ä»å¤§é‡å›¾åƒä¸­æå–ä¿¡æ¯ï¼Œå¹¶æ„å»ºå¤šæ ·åŒ–çš„æµ‹è¯•é¢˜ç›®ã€‚

**å…³é”®åˆ›æ–°**ï¼šMMSI-Benchçš„åˆ›æ–°åœ¨äºå…¶ä¸“æ³¨äºå¤šå›¾åƒçš„ç©ºé—´æ¨ç†ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†çš„ç©ºç™½ï¼Œæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œç ”ç©¶è€…ä»¬æ³¨é‡é¢˜ç›®çš„æ¸…æ™°æ€§å’Œå¹²æ‰°é¡¹çš„åˆç†æ€§ï¼Œç¡®ä¿æ¯ä¸ªé—®é¢˜éƒ½èƒ½æœ‰æ•ˆè¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€å¼ºçš„å¼€æºæ¨¡å‹åœ¨MMSI-Benchä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º30%ï¼Œè€ŒOpenAIçš„o3æ¨ç†æ¨¡å‹è¾¾åˆ°40%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»çš„å‡†ç¡®ç‡é«˜è¾¾97%ï¼Œæ˜¾ç¤ºå‡ºå½“å‰æ¨¡å‹åœ¨å¤šå›¾åƒç©ºé—´æ¨ç†æ–¹é¢çš„æ˜¾è‘—ä¸è¶³ï¼Œä¸”æœªæ¥ç ”ç©¶æœ‰å¹¿é˜”çš„æå‡ç©ºé—´ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰ã€‚é€šè¿‡æå‡å¤šå›¾åƒç©ºé—´æ™ºèƒ½ï¼Œèƒ½å¤Ÿä½¿ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œäº’åŠ¨å¤æ‚çš„ç‰©ç†ç¯å¢ƒï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench .

