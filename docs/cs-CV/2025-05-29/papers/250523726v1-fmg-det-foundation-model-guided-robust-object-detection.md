---
layout: default
title: FMG-Det: Foundation Model Guided Robust Object Detection
---

# FMG-Det: Foundation Model Guided Robust Object Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23726" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23726v1</a>
  <a href="https://arxiv.org/pdf/2505.23726.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23726v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23726v1', 'FMG-Det: Foundation Model Guided Robust Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, Yijing Watkins

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: 10 pages, ICIP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFMG-Detä»¥è§£å†³å™ªå£°æ ‡æ³¨ä¸‹çš„ç‰©ä½“æ£€æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç‰©ä½“æ£€æµ‹` `å™ªå£°æ ‡æ³¨` `å¤šå®ä¾‹å­¦ä¹ ` `åŸºç¡€æ¨¡å‹` `å°‘æ ·æœ¬å­¦ä¹ ` `æ•°æ®é¢„å¤„ç†` `é²æ£’æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç‰©ä½“æ£€æµ‹æ–¹æ³•åœ¨å¤„ç†å™ªå£°æ ‡æ³¨æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ï¼Œå°‘é‡é”™è¯¯æ ‡æ³¨ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºFMG-Detï¼Œé€šè¿‡ç»“åˆå¤šå®ä¾‹å­¦ä¹ ä¸åŸºç¡€æ¨¡å‹çš„é¢„å¤„ç†ç®¡é“ï¼Œæ¥ä¿®æ­£è®­ç»ƒå‰çš„æ ‡ç­¾ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFMG-Detåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ–¹æ³•æ›´ä¸ºé«˜æ•ˆç®€æ´ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ”¶é›†é«˜è´¨é‡çš„ç‰©ä½“æ£€æµ‹æ•°æ®é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨è¾¹ç•Œæ—¶çš„ä¸»è§‚æ€§ä½¿å¾—ä¸€è‡´æ€§å’ŒéªŒè¯å˜å¾—å›°éš¾ã€‚éƒ¨åˆ†å¯è§æˆ–æ¨¡ç³Šçš„ç‰©ä½“è¾¹ç•Œè¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸€é—®é¢˜ã€‚åœ¨å™ªå£°æ ‡æ³¨ä¸‹è®­ç»ƒä¼šæ˜¾è‘—é™ä½æ£€æµ‹å™¨æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­ã€‚æœ¬æ–‡æå‡ºFMG-Detï¼Œä¸€ç§ç®€å•é«˜æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ä¸å¼ºå¤§çš„åŸºç¡€æ¨¡å‹é¢„å¤„ç†ç®¡é“ï¼Œæ¥ä¿®æ­£æ ‡ç­¾ï¼Œä»è€Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé€‚ç”¨äºæ ‡å‡†å’Œå°‘æ ·æœ¬åœºæ™¯ï¼Œä¸”æ¯”å…¶ä»–æ–¹æ³•æ›´ä¸ºç®€æ´é«˜æ•ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­å› æ ‡æ³¨å™ªå£°å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸ä¸€è‡´æˆ–æ¨¡ç³Šçš„æ ‡æ³¨æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæå‡æ£€æµ‹å™¨çš„é²æ£’æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFMG-Detçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¡†æ¶ä¸åŸºç¡€æ¨¡å‹çš„é¢„å¤„ç†ç®¡é“ç›¸ç»“åˆï¼Œä¿®æ­£è®­ç»ƒæ•°æ®ä¸­çš„æ ‡ç­¾ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å™ªå£°æ ‡æ³¨ä¸‹çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„å¼ºå¤§ç‰¹æ€§æ¥å¢å¼ºæ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFMG-Detçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†æ¨¡å—å’Œæ£€æµ‹å™¨å¤´ã€‚é¢„å¤„ç†æ¨¡å—åˆ©ç”¨åŸºç¡€æ¨¡å‹å¯¹æ ‡ç­¾è¿›è¡Œä¿®æ­£ï¼Œéšåå°†ä¿®æ­£åçš„æ•°æ®è¾“å…¥åˆ°æ£€æµ‹å™¨ä¸­è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šFMG-Detçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå°†å¤šå®ä¾‹å­¦ä¹ ä¸åŸºç¡€æ¨¡å‹ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æ•°æ®ä¿®æ­£æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€æ ‡æ³¨ä¿®æ­£æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å™ªå£°æ ‡æ³¨é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒFMG-Deté‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¤šå®ä¾‹å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å¯¹æ£€æµ‹å™¨å¤´è¿›è¡Œäº†è½»å¾®ä¿®æ”¹ï¼Œä»¥é€‚åº”ä¿®æ­£åçš„æ•°æ®è¾“å…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFMG-Detåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†çº¦15%çš„æ£€æµ‹ç²¾åº¦ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤„ç†å™ªå£°æ ‡æ³¨æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FMG-Detçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜è´¨é‡æ ‡æ³¨çš„ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€å®‰é˜²ç›‘æ§å’ŒåŒ»ç–—å½±åƒåˆ†æç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æ¨¡å‹å¯¹å™ªå£°æ ‡æ³¨çš„é²æ£’æ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡æ£€æµ‹ç²¾åº¦ï¼Œé™ä½äººå·¥æ ‡æ³¨æˆæœ¬ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Collecting high quality data for object detection tasks is challenging due to the inherent subjectivity in labeling the boundaries of an object. This makes it difficult to not only collect consistent annotations across a dataset but also to validate them, as no two annotators are likely to label the same object using the exact same coordinates. These challenges are further compounded when object boundaries are partially visible or blurred, which can be the case in many domains. Training on noisy annotations significantly degrades detector performance, rendering them unusable, particularly in few-shot settings, where just a few corrupted annotations can impact model performance. In this work, we propose FMG-Det, a simple, efficient methodology for training models with noisy annotations. More specifically, we propose combining a multiple instance learning (MIL) framework with a pre-processing pipeline that leverages powerful foundation models to correct labels prior to training. This pre-processing pipeline, along with slight modifications to the detector head, results in state-of-the-art performance across a number of datasets, for both standard and few-shot scenarios, while being much simpler and more efficient than other approaches.

