---
layout: default
title: Semantics-Aware Human Motion Generation from Audio Instructions
---

# Semantics-Aware Human Motion Generation from Audio Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23465" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23465v1</a>
  <a href="https://arxiv.org/pdf/2505.23465.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23465v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23465v1', 'Semantics-Aware Human Motion Generation from Audio Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zi-An Wang, Shihao Zou, Shiyao Yu, Mingyuan Zhang, Chao Dong

**åˆ†ç±»**: cs.SD, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**æœŸåˆŠ**: Graphical Models,Volume 139,2025,101268,ISSN 1524-0703,

**DOI**: [10.1016/j.gmod.2025.101268](https://doi.org/10.1016/j.gmod.2025.101268)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºéŸ³é¢‘æŒ‡ä»¤çš„äººä½“åŠ¨ä½œç”Ÿæˆæ¡†æ¶ä»¥è§£å†³è¯­ä¹‰åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `éŸ³é¢‘æŒ‡ä»¤` `åŠ¨ä½œç”Ÿæˆ` `è¯­ä¹‰åŒ¹é…` `æ©è”½ç”Ÿæˆå˜æ¢å™¨` `è®°å¿†æ£€ç´¢æ³¨æ„åŠ›` `äººæœºäº¤äº’` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äºä¸éŸ³ä¹æˆ–è¯­éŸ³èŠ‚å¥çš„åŒ¹é…ï¼Œå¯¼è‡´éŸ³é¢‘è¯­ä¹‰ä¸ç”ŸæˆåŠ¨ä½œä¹‹é—´çš„è”ç³»è¾ƒå¼±ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ©è”½ç”Ÿæˆå˜æ¢å™¨çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç»“åˆè®°å¿†æ£€ç´¢æ³¨æ„åŠ›æ¨¡å—ä»¥å¤„ç†å¤æ‚éŸ³é¢‘è¾“å…¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨éŸ³é¢‘æŒ‡ä»¤çš„è¯­ä¹‰ä¼ è¾¾ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†æ›´è‡ªç„¶çš„ç”¨æˆ·äº¤äº’ä½“éªŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€äº¤äº’æŠ€æœ¯çš„è¿›æ­¥ï¼ŒéŸ³é¢‘ä¿¡å·åœ¨è¯­ä¹‰ç¼–ç ä¸­çš„é‡è¦æ€§æ—¥ç›Šçªå‡ºã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œå³åˆ©ç”¨éŸ³é¢‘ä¿¡å·ä½œä¸ºæ¡ä»¶è¾“å…¥ç”Ÿæˆä¸éŸ³é¢‘è¯­ä¹‰ç›¸ç¬¦çš„åŠ¨ä½œã€‚ä¸åŸºäºæ–‡æœ¬çš„äº¤äº’ä¸åŒï¼ŒéŸ³é¢‘æä¾›äº†ä¸€ç§æ›´è‡ªç„¶ç›´è§‚çš„æ²Ÿé€šæ–¹å¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾§é‡äºä¸éŸ³ä¹æˆ–è¯­éŸ³èŠ‚å¥åŒ¹é…åŠ¨ä½œï¼Œå¯¼è‡´éŸ³é¢‘è¯­ä¹‰ä¸ç”ŸæˆåŠ¨ä½œä¹‹é—´çš„è”ç³»è¾ƒå¼±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œé‡‡ç”¨æ©è”½ç”Ÿæˆå˜æ¢å™¨ï¼Œå¹¶é€šè¿‡è®°å¿†æ£€ç´¢æ³¨æ„åŠ›æ¨¡å—æ¥å¤„ç†ç¨€ç–å’Œå†—é•¿çš„éŸ³é¢‘è¾“å…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†æè¿°è½¬æ¢ä¸ºå¯¹è¯é£æ ¼å¹¶ç”Ÿæˆä¸åŒè¯´è¯è€…èº«ä»½çš„éŸ³é¢‘æ¥ä¸°å¯Œç°æœ‰æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œè¯æ˜éŸ³é¢‘æŒ‡ä»¤èƒ½å¤Ÿä¼ è¾¾ä¸æ–‡æœ¬ç›¸ä¼¼çš„è¯­ä¹‰ï¼ŒåŒæ—¶æä¾›æ›´å®ç”¨å’Œç”¨æˆ·å‹å¥½çš„äº¤äº’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³é¢‘ä¿¡å·ä¸ç”ŸæˆåŠ¨ä½œä¹‹é—´çš„è¯­ä¹‰åŒ¹é…é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†éŸ³é¢‘è¯­ä¹‰çš„æ·±å±‚æ¬¡è”ç³»ï¼Œå¯¼è‡´ç”Ÿæˆçš„åŠ¨ä½œç¼ºä¹è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ©è”½ç”Ÿæˆå˜æ¢å™¨æ¥ç”Ÿæˆä¸éŸ³é¢‘è¯­ä¹‰ç›¸ç¬¦çš„åŠ¨ä½œï¼Œå¹¶é€šè¿‡è®°å¿†æ£€ç´¢æ³¨æ„åŠ›æ¨¡å—æ¥å¢å¼ºå¯¹ç¨€ç–å’Œå†—é•¿éŸ³é¢‘çš„å¤„ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬éŸ³é¢‘è¾“å…¥å¤„ç†ã€ç‰¹å¾æå–ã€åŠ¨ä½œç”Ÿæˆå’Œåå¤„ç†å››ä¸ªä¸»è¦æ¨¡å—ã€‚éŸ³é¢‘ä¿¡å·é¦–å…ˆç»è¿‡ç‰¹å¾æå–æ¨¡å—ï¼Œç„¶åè¾“å…¥åˆ°ç”Ÿæˆå˜æ¢å™¨ä¸­ï¼Œæœ€åç”Ÿæˆç›¸åº”çš„åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è®°å¿†æ£€ç´¢æ³¨æ„åŠ›æ¨¡å—ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿æ—¶é—´çš„éŸ³é¢‘è¾“å…¥ï¼Œå¹¶å¢å¼ºäº†ç”ŸæˆåŠ¨ä½œçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ ¹æœ¬åŒºåˆ«åœ¨äºå…¶å¯¹éŸ³é¢‘è¯­ä¹‰çš„æ·±åº¦ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–éŸ³é¢‘ä¸åŠ¨ä½œä¹‹é—´çš„è¯­ä¹‰åŒ¹é…ï¼Œå¹¶å¯¹å˜æ¢å™¨çš„å±‚æ•°å’Œéšè—å•å…ƒè¿›è¡Œäº†ç²¾ç»†è°ƒèŠ‚ï¼Œä»¥æé«˜æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¡†æ¶åœ¨éŸ³é¢‘æŒ‡ä»¤çš„è¯­ä¹‰ä¼ è¾¾ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œç”ŸæˆåŠ¨ä½œçš„è¯­ä¹‰ä¸€è‡´æ€§æé«˜äº†çº¦30%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨å¤„ç†é•¿éŸ³é¢‘è¾“å…¥æ—¶çš„æ•ˆç‡ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡å®ç°éŸ³é¢‘æŒ‡ä»¤ä¸åŠ¨ä½œç”Ÿæˆçš„æ— ç¼å¯¹æ¥ï¼Œå¯ä»¥æå¤§æå‡ç”¨æˆ·ä½“éªŒï¼Œä½¿å¾—äº¤äº’æ›´åŠ è‡ªç„¶å’Œç›´è§‚ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ•™è‚²ã€å¨±ä¹å’ŒåŒ»ç–—ç­‰å¤šä¸ªè¡Œä¸šä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in interactive technologies have highlighted the prominence of audio signals for semantic encoding. This paper explores a new task, where audio signals are used as conditioning inputs to generate motions that align with the semantics of the audio. Unlike text-based interactions, audio provides a more natural and intuitive communication method. However, existing methods typically focus on matching motions with music or speech rhythms, which often results in a weak connection between the semantics of the audio and generated motions. We propose an end-to-end framework using a masked generative transformer, enhanced by a memory-retrieval attention module to handle sparse and lengthy audio inputs. Additionally, we enrich existing datasets by converting descriptions into conversational style and generating corresponding audio with varied speaker identities. Experiments demonstrate the effectiveness and efficiency of the proposed framework, demonstrating that audio instructions can convey semantics similar to text while providing more practical and user-friendly interactions.

