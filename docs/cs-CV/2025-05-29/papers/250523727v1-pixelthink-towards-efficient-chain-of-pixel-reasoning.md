---
layout: default
title: "PixelThink: Towards Efficient Chain-of-Pixel Reasoning"
---

# PixelThink: Towards Efficient Chain-of-Pixel Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23727" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23727v1</a>
  <a href="https://arxiv.org/pdf/2505.23727.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23727v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23727v1', 'PixelThink: Towards Efficient Chain-of-Pixel Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Song Wang, Gongfan Fang, Lingdong Kong, Xiangtai Li, Jianyun Xu, Sheng Yang, Qiang Li, Jianke Zhu, Xinchao Wang

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: Project Page: https://PixelThink.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPixelThinkä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨ç†åˆ†å‰²` `å¤šæ¨¡æ€ç†è§£` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹ä¸ç¡®å®šæ€§` `ä»»åŠ¡éš¾åº¦è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†åˆ†å‰²æ–¹æ³•åœ¨å¤„ç†åˆ†å¸ƒå¤–åœºæ™¯æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚
2. æå‡ºPixelThinkï¼Œé€šè¿‡ç»“åˆä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹ä¸ç¡®å®šæ€§æ¥è°ƒèŠ‚æ¨ç†ç”Ÿæˆï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPixelThinkåœ¨æ¨ç†æ•ˆç‡å’Œåˆ†å‰²æ€§èƒ½ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„æ¨ç†åˆ†å‰²æ–¹æ³•é€šå¸¸é€šè¿‡å›¾åƒ-æ–‡æœ¬å¯¹åŠç›¸åº”çš„æ©ç æ ‡ç­¾æ¥å¾®è°ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä½†åœ¨æ— æ˜ç¡®æ¨ç†è¿‡ç¨‹çš„æƒ…å†µä¸‹å¯¹åˆ†å¸ƒå¤–åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å°½ç®¡è¿‘æœŸçš„ç ”ç©¶é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä½†å¸¸å¸¸å¯¼è‡´è¿‡åº¦æ¨ç†ï¼Œç”Ÿæˆå†—é•¿çš„æ¨ç†é“¾ï¼Œå¢åŠ è®¡ç®—æˆæœ¬å¹¶é™åˆ¶æ¨ç†è´¨é‡çš„æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PixelThinkï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ¡ˆï¼Œç»“åˆå¤–éƒ¨ä¼°è®¡çš„ä»»åŠ¡éš¾åº¦å’Œå†…éƒ¨æµ‹é‡çš„æ¨¡å‹ä¸ç¡®å®šæ€§ï¼Œä»¥è°ƒèŠ‚å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¨ç†ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡å’Œæ•´ä½“åˆ†å‰²æ€§èƒ½ä¸Šå‡æœ‰æ‰€æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ¨ç†åˆ†å‰²æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶ï¼Œå¾€å¾€ç¼ºä¹æœ‰æ•ˆçš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä¸”å¸¸å¸¸ç”Ÿæˆå†—é•¿çš„æ¨ç†é“¾ï¼Œå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„PixelThinkæ–¹æ¡ˆï¼Œé€šè¿‡ç»“åˆå¤–éƒ¨ä»»åŠ¡éš¾åº¦ä¼°è®¡å’Œå†…éƒ¨æ¨¡å‹ä¸ç¡®å®šæ€§æµ‹é‡ï¼Œæ¥è°ƒèŠ‚æ¨ç†ç”Ÿæˆçš„é•¿åº¦ï¼Œä»¥é€‚åº”åœºæ™¯å¤æ‚æ€§å’Œé¢„æµ‹ä¿¡å¿ƒï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPixelThinkçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»»åŠ¡éš¾åº¦è¯„ä¼°æ¨¡å—ã€æ¨¡å‹ä¸ç¡®å®šæ€§æµ‹é‡æ¨¡å—å’Œæ¨ç†ç”Ÿæˆæ¨¡å—ã€‚é¦–å…ˆï¼Œè¯„ä¼°ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œç„¶åæ ¹æ®æ¨¡å‹çš„ä¿¡å¿ƒè°ƒæ•´æ¨ç†é•¿åº¦ï¼Œæœ€åç”Ÿæˆç›¸åº”çš„æ¨ç†é“¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†å¤–éƒ¨ä»»åŠ¡éš¾åº¦ä¸å†…éƒ¨æ¨¡å‹ä¸ç¡®å®šæ€§ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æ¨ç†ç”Ÿæˆè°ƒèŠ‚æœºåˆ¶ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å†—é•¿æ¨ç†é“¾çš„ç”Ÿæˆï¼Œæé«˜æ¨ç†çš„è´¨é‡å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡æ¨ç†é•¿åº¦å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å¤æ‚åœºæ™¯çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPixelThinkåœ¨æ¨ç†æ•ˆç‡ä¸Šæå‡äº†çº¦30%ï¼ŒåŒæ—¶åœ¨åˆ†å‰²æ€§èƒ½ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æé«˜äº†5%ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æå’Œæ™ºèƒ½ç›‘æ§ç­‰ï¼Œèƒ½å¤Ÿåœ¨è¿™äº›å¤æ‚åœºæ™¯ä¸­æé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing reasoning segmentation approaches typically fine-tune multimodal large language models (MLLMs) using image-text pairs and corresponding mask labels. However, they exhibit limited generalization to out-of-distribution scenarios without an explicit reasoning process. Although recent efforts leverage reinforcement learning through group-relative policy optimization (GRPO) to enhance reasoning ability, they often suffer from overthinking - producing uniformly verbose reasoning chains irrespective of task complexity. This results in elevated computational costs and limited control over reasoning quality. To address this problem, we propose PixelThink, a simple yet effective scheme that integrates externally estimated task difficulty and internally measured model uncertainty to regulate reasoning generation within a reinforcement learning paradigm. The model learns to compress reasoning length in accordance with scene complexity and predictive confidence. To support comprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark with annotated reasoning references and difficulty scores, along with a suite of metrics designed to assess segmentation accuracy, reasoning quality, and efficiency jointly. Experimental results demonstrate that the proposed approach improves both reasoning efficiency and overall segmentation performance. Our work contributes novel perspectives towards efficient and interpretable multimodal understanding. The code and model will be publicly available.

