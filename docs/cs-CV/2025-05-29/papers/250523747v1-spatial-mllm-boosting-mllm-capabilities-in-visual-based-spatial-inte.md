---
layout: default
title: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence
---

# Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23747" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23747v1</a>
  <a href="https://arxiv.org/pdf/2505.23747.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23747v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23747v1', 'Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: 21 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://diankun-wu.github.io/Spatial-MLLM/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpatial-MLLMä»¥è§£å†³è§†è§‰åŸºç¡€ç©ºé—´æ™ºèƒ½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç©ºé—´æ™ºèƒ½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æ¨ç†` `åŒç¼–ç å™¨æ¶æ„` `ç©ºé—´æ„ŸçŸ¥é‡‡æ ·` `äºŒç»´è¾“å…¥` `è§†è§‰å‡ ä½•æ¨¡å‹` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸‰ç»´å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¾èµ–é¢å¤–çš„ä¸‰ç»´æˆ–2.5Dæ•°æ®ï¼Œé™åˆ¶äº†å…¶åœ¨ä»…æœ‰äºŒç»´è¾“å…¥åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. æå‡ºSpatial-MLLMæ¡†æ¶ï¼Œé€šè¿‡åŒç¼–ç å™¨æ¶æ„å’Œç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œä»çº¯äºŒç»´è§‚å¯Ÿä¸­è¿›è¡Œè§†è§‰ç©ºé—´æ¨ç†ã€‚
3. åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒSpatial-MLLMåœ¨è§†è§‰åŸºç¡€ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•æ˜¾è‘—æå‡äº†å…¶åœ¨äºŒç»´è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œæå‡å…¶ç©ºé—´æ™ºèƒ½ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¸‰ç»´MLLMsé€šå¸¸ä¾èµ–é¢å¤–çš„ä¸‰ç»´æˆ–2.5Dæ•°æ®æ¥èå…¥ç©ºé—´æ„è¯†ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä»…æœ‰äºŒç»´è¾“å…¥ï¼ˆå¦‚å›¾åƒæˆ–è§†é¢‘ï¼‰çš„åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†Spatial-MLLMï¼Œä¸€ä¸ªåŸºäºçº¯äºŒç»´è§‚å¯Ÿçš„è§†è§‰ç©ºé—´æ¨ç†æ–°æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„è§†é¢‘MLLMsä¾èµ–äºä¼˜åŒ–è¯­ä¹‰ç†è§£çš„CLIPåŸºç¡€è§†è§‰ç¼–ç å™¨ä¸åŒï¼Œæˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯é‡Šæ”¾å‰é¦ˆè§†è§‰å‡ ä½•åŸºç¡€æ¨¡å‹çš„å¼ºç»“æ„å…ˆéªŒã€‚æˆ‘ä»¬æå‡ºäº†åŒç¼–ç å™¨æ¶æ„ï¼šä¸€ä¸ªé¢„è®­ç»ƒçš„äºŒç»´è§†è§‰ç¼–ç å™¨æå–è¯­ä¹‰ç‰¹å¾ï¼Œå¦ä¸€ä¸ªä»è§†è§‰å‡ ä½•æ¨¡å‹çš„ä¸»å¹²åˆå§‹åŒ–çš„ç©ºé—´ç¼–ç å™¨æå–ä¸‰ç»´ç»“æ„ç‰¹å¾ã€‚è¿æ¥å™¨å°†è¿™ä¸¤ç§ç‰¹å¾æ•´åˆä¸ºç»Ÿä¸€çš„è§†è§‰æ ‡è®°ï¼Œä»¥å¢å¼ºç©ºé—´ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨ç†æ—¶æå‡ºäº†ä¸€ç§ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œé€‰æ‹©è§†é¢‘åºåˆ—ä¸­ç©ºé—´ä¿¡æ¯ä¸°å¯Œçš„å¸§ï¼Œç¡®ä¿å³ä½¿åœ¨æœ‰é™çš„æ ‡è®°é•¿åº¦ä¸‹ï¼Œæ¨¡å‹ä¹Ÿèƒ½å…³æ³¨å¯¹ç©ºé—´æ¨ç†è‡³å…³é‡è¦çš„å¸§ã€‚é€šè¿‡æ„å»ºSpatial-MLLM-120kæ•°æ®é›†å¹¶åœ¨å…¶ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒå’ŒGRPOè®­ç»ƒï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSpatial-MLLMåœ¨å„ç§è§†è§‰åŸºç¡€ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä¸‰ç»´å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä»…æœ‰äºŒç»´è¾“å…¥ï¼ˆå¦‚å›¾åƒæˆ–è§†é¢‘ï¼‰åœºæ™¯ä¸­ç¼ºä¹ç©ºé—´æ™ºèƒ½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–é¢å¤–çš„ä¸‰ç»´æ•°æ®ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆäºŒç»´è§†è§‰ç‰¹å¾å’Œä¸‰ç»´ç»“æ„ç‰¹å¾ï¼Œæå‡æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚é€šè¿‡é‡Šæ”¾è§†è§‰å‡ ä½•æ¨¡å‹çš„ç»“æ„å…ˆéªŒï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ä¸‰ç»´æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„ç©ºé—´æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpatial-MLLMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€ä¸ªé¢„è®­ç»ƒçš„äºŒç»´è§†è§‰ç¼–ç å™¨ç”¨äºæå–è¯­ä¹‰ç‰¹å¾ï¼Œå¦ä¸€ä¸ªç©ºé—´ç¼–ç å™¨ä»è§†è§‰å‡ ä½•æ¨¡å‹çš„ä¸»å¹²åˆå§‹åŒ–ï¼Œç”¨äºæå–ä¸‰ç»´ç»“æ„ç‰¹å¾ã€‚è¿æ¥å™¨å°†è¿™ä¸¤ç§ç‰¹å¾æ•´åˆä¸ºç»Ÿä¸€çš„è§†è§‰æ ‡è®°ã€‚æ­¤å¤–ï¼Œæ¨ç†é˜¶æ®µé‡‡ç”¨ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œé€‰æ‹©é‡è¦å¸§è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåŒç¼–ç å™¨æ¶æ„çš„è®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä»…æœ‰äºŒç»´è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå……åˆ†åˆ©ç”¨ç©ºé—´ç»“æ„ä¿¡æ¯ï¼Œä»è€Œæå‡ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºä¸‰ç»´æ•°æ®çš„åšæ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œç©ºé—´ç¼–ç å™¨ï¼ŒæŸå¤±å‡½æ•°é€šè¿‡ç›‘ç£å¾®è°ƒå’ŒGRPOè¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ç¡®ä¿æ¨¡å‹åœ¨æœ‰é™çš„æ ‡è®°é•¿åº¦ä¸‹ï¼Œå…³æ³¨å¯¹ç©ºé—´æ¨ç†è‡³å…³é‡è¦çš„å¸§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatial-MLLMåœ¨è§†è§‰åŸºç¡€ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†è¶…è¿‡10%çš„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘åˆ†æç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿåœ¨ä»…æœ‰äºŒç»´è¾“å…¥çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„ç©ºé—´æ¨ç†ã€‚å…¶å®é™…ä»·å€¼åœ¨äºæå‡æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½å¯¹æ™ºèƒ½ç³»ç»Ÿçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.

