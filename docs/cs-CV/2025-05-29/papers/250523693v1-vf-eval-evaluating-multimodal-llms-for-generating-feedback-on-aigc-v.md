---
layout: default
title: "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos"
---

# VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23693" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23693v1</a>
  <a href="https://arxiv.org/pdf/2505.23693.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23693v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23693v1', 'VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: ACL 2025 Main

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVF-Evalä»¥è¯„ä¼°å¤šæ¨¡æ€LLMåœ¨AIGCè§†é¢‘åé¦ˆç”Ÿæˆä¸­çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `åˆæˆè§†é¢‘` `AIç”Ÿæˆå†…å®¹` `è§†é¢‘ç”Ÿæˆ` `è¯„ä¼°åŸºå‡†` `è¿è´¯æ€§éªŒè¯` `é”™è¯¯æ£€æµ‹` `æ¨ç†è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è‡ªç„¶è§†é¢‘çš„è¯„ä¼°ï¼Œç¼ºä¹å¯¹åˆæˆè§†é¢‘ï¼ˆå¦‚AIGCï¼‰çš„æ·±å…¥ç ”ç©¶ï¼Œå¯¼è‡´MLLMsåœ¨æ­¤é¢†åŸŸçš„èƒ½åŠ›æœªè¢«å……åˆ†æ¢ç´¢ã€‚
2. è®ºæ–‡æå‡ºVF-EvalåŸºå‡†ï¼Œé€šè¿‡å››ä¸ªä»»åŠ¡å…¨é¢è¯„ä¼°MLLMsåœ¨AIGCè§†é¢‘ä¸Šçš„è¡¨ç°ï¼Œå¡«è¡¥äº†è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡GPT-4.1è¡¨ç°æœ€ä½³ï¼Œä½†åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ä»éš¾ä»¥å®ç°ä¸€è‡´çš„ä¼˜å¼‚è¡¨ç°ï¼Œå¼ºè°ƒäº†åŸºå‡†çš„æŒ‘æˆ˜æ€§å’Œé‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘é—®ç­”é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è‡ªç„¶è§†é¢‘ä¸Šï¼Œå¿½è§†äº†åˆæˆè§†é¢‘ï¼ˆå¦‚AIç”Ÿæˆå†…å®¹ï¼ŒAIGCï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°åŸºå‡†VF-Evalï¼ŒåŒ…å«å››ä¸ªä»»åŠ¡ï¼šè¿è´¯æ€§éªŒè¯ã€é”™è¯¯æ„è¯†ã€é”™è¯¯ç±»å‹æ£€æµ‹å’Œæ¨ç†è¯„ä¼°ï¼Œä»¥å…¨é¢è¯„ä¼°MLLMsåœ¨AIGCè§†é¢‘ä¸Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹13ä¸ªå‰æ²¿çš„MLLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹GPT-4.1åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šä¹Ÿéš¾ä»¥ä¿æŒä¸€è‡´çš„è‰¯å¥½è¡¨ç°ï¼Œçªæ˜¾äº†åŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒRePromptæ¢è®¨äº†VF-Evalåœ¨æ”¹å–„è§†é¢‘ç”Ÿæˆä¸­çš„å®é™…åº”ç”¨ï¼Œè¡¨æ˜æ›´ç´§å¯†åœ°ä¸äººç±»åé¦ˆå¯¹é½çš„MLLMså¯ä»¥æå‡è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•å¯¹åˆæˆè§†é¢‘ï¼ˆAIGCï¼‰èƒ½åŠ›çš„å¿½è§†ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°MLLMsæ—¶ä¸»è¦é›†ä¸­äºè‡ªç„¶è§†é¢‘ï¼Œå¯¼è‡´å¯¹åˆæˆè§†é¢‘çš„ç†è§£å’Œåé¦ˆç”Ÿæˆèƒ½åŠ›è¯„ä¼°ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºVF-EvalåŸºå‡†ï¼Œé€šè¿‡è®¾è®¡è¿è´¯æ€§éªŒè¯ã€é”™è¯¯æ„è¯†ã€é”™è¯¯ç±»å‹æ£€æµ‹å’Œæ¨ç†è¯„ä¼°å››ä¸ªä»»åŠ¡ï¼Œå…¨é¢è¯„ä¼°MLLMsåœ¨AIGCè§†é¢‘ä¸Šçš„èƒ½åŠ›ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVF-EvalåŸºå‡†çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¿è´¯æ€§éªŒè¯ï¼Œè¯„ä¼°è§†é¢‘å†…å®¹çš„é€»è¾‘è¿è´¯æ€§ï¼›2) é”™è¯¯æ„è¯†ï¼Œæ£€æµ‹ç”Ÿæˆè§†é¢‘ä¸­çš„é”™è¯¯ï¼›3) é”™è¯¯ç±»å‹æ£€æµ‹ï¼Œè¯†åˆ«é”™è¯¯çš„å…·ä½“ç±»å‹ï¼›4) æ¨ç†è¯„ä¼°ï¼Œè€ƒå¯Ÿæ¨¡å‹å¯¹è§†é¢‘å†…å®¹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šVF-Evalçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶é’ˆå¯¹AIGCè§†é¢‘çš„ä¸“é—¨è®¾è®¡ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒºåˆ«äºä»¥å¾€ä»…é’ˆå¯¹è‡ªç„¶è§†é¢‘çš„è¯„ä¼°æ–¹æ³•ï¼Œå¡«è¡¥äº†ç ”ç©¶ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿å¯¹å„ä¸ªä»»åŠ¡çš„å‡†ç¡®è¯„ä¼°ï¼Œæ¨¡å‹é€‰æ‹©æ¶µç›–äº†13ä¸ªå‰æ²¿çš„MLLMsï¼Œç¡®ä¿äº†ç»“æœçš„å¹¿æ³›æ€§å’Œä»£è¡¨æ€§ã€‚å®éªŒè®¾è®¡ä¸­è¿˜è€ƒè™‘äº†ä¸äººç±»åé¦ˆçš„å¯¹é½ï¼Œä»¥æå‡è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡GPT-4.1åœ¨è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†åœ¨è¿è´¯æ€§éªŒè¯ã€é”™è¯¯æ„è¯†ç­‰ä»»åŠ¡ä¸Šä»æœªèƒ½è¾¾åˆ°ä¸€è‡´çš„ä¼˜å¼‚è¡¨ç°ï¼Œçªæ˜¾äº†VF-EvalåŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚è¯¥åŸºå‡†çš„è®¾è®¡ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VF-EvalåŸºå‡†çš„æå‡ºä¸ºAIGCè§†é¢‘ç”Ÿæˆå’Œè¯„ä¼°æä¾›äº†æ–°çš„å·¥å…·ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¥½åœ°ç†è§£å’Œä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†æœ‰æœ›æ¨åŠ¨AIGCé¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæå‡ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.

