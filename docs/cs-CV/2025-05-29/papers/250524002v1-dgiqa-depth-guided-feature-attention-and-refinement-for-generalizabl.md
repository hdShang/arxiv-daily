---
layout: default
title: DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment
---

# DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24002" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24002v1</a>
  <a href="https://arxiv.org/pdf/2505.24002.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24002v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24002v1', 'DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vaishnav Ramesh, Junliang Liu, Haining Wang, Md Jahidul Islam

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: 18 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDGIQAä»¥è§£å†³æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„æ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°` `æ·±åº¦å¼•å¯¼æœºåˆ¶` `ç‰¹å¾å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `Transformer-CNNæ¡¥æ¥` `è‡ªç„¶å¤±çœŸè¯„ä¼°` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•åœ¨å¤„ç†æœªè§è‡ªç„¶å¤±çœŸæ—¶ç¼ºä¹æœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„Depth-CARæœºåˆ¶é€šè¿‡å¼•å…¥åœºæ™¯æ·±åº¦ä¿¡æ¯å’Œç©ºé—´ç‰¹å¾ï¼Œå¢å¼ºäº†ç‰¹å¾å­¦ä¹ çš„è¾¨åˆ«åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDGIQAåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è‡ªç„¶å›¾åƒå¤±çœŸè¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNR-IQAï¼‰ä¸­ï¼Œä»äººç±»ä¸»è§‚æ„ŸçŸ¥å­¦ä¹ çš„ä¸€ä¸ªé•¿æœŸæŒ‘æˆ˜æ˜¯ç¼ºä¹å¯¹æœªè§è‡ªç„¶å¤±çœŸçš„å®¢è§‚æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é›†æˆäº†ä¸€ç§æ–°é¢–çš„æ·±åº¦å¼•å¯¼äº¤å‰æ³¨æ„åŠ›å’Œç²¾ç‚¼æœºåˆ¶ï¼ˆDepth-CARï¼‰ï¼Œå°†åœºæ™¯æ·±åº¦å’Œç©ºé—´ç‰¹å¾æç‚¼ä¸ºç»“æ„æ„ŸçŸ¥è¡¨ç¤ºï¼Œä»è€Œæ”¹å–„NR-IQAã€‚è¿™ä¸€æ–¹æ³•å¼•å…¥äº†ç‰©ä½“æ˜¾è‘—æ€§å’Œåœºæ™¯ç›¸å¯¹å¯¹æ¯”åº¦çš„çŸ¥è¯†ï¼Œä»¥å®ç°æ›´å…·è¾¨åˆ«åŠ›çš„ç‰¹å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†TCBï¼ˆTransformer-CNNæ¡¥æ¥ï¼‰æ¦‚å¿µï¼Œå°†æ¥è‡ªTransformerä¸»å¹²çš„é«˜å±‚å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–ä¸é€šè¿‡ä¸€ç»„å±‚æ¬¡åŒ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ•è·çš„å±€éƒ¨ç©ºé—´ç‰¹å¾èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGIQAæ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®åŸºå‡†æ•°æ®é›†ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è·¨æ•°æ®é›†è¯„ä¼°å’Œè‡ªç„¶å›¾åƒå¤±çœŸï¼ˆå¦‚ä½å…‰ç…§ã€é›¾éœ¾å’Œé•œå¤´å…‰æ™•ï¼‰æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ä¸­å¯¹æœªè§è‡ªç„¶å¤±çœŸçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†å„ç§è‡ªç„¶å¤±çœŸï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡æ·±åº¦å¼•å¯¼äº¤å‰æ³¨æ„åŠ›å’Œç²¾ç‚¼æœºåˆ¶ï¼ˆDepth-CARï¼‰æ¥æå–åœºæ™¯æ·±åº¦å’Œç©ºé—´ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆç»“æ„æ„ŸçŸ¥çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å›¾åƒä¸­çš„ç‰©ä½“æ˜¾è‘—æ€§å’Œç›¸å¯¹å¯¹æ¯”åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDGIQAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬Depth-CARå’ŒTCBæ¨¡å—ã€‚Depth-CARè´Ÿè´£æå–å’Œç²¾ç‚¼ç‰¹å¾ï¼Œè€ŒTCBåˆ™å°†Transformerçš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸CNNçš„å±€éƒ¨ç‰¹å¾è¿›è¡Œèåˆï¼Œå½¢æˆå¤šæ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºDepth-CARå’ŒTCBçš„ç»“åˆï¼Œå‰è€…é€šè¿‡æ·±åº¦ä¿¡æ¯æå‡ç‰¹å¾å­¦ä¹ çš„è¾¨åˆ«åŠ›ï¼Œåè€…åˆ™æœ‰æ•ˆèåˆäº†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€‚è¿™ä¸€ç»„åˆæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸åŒå¤±çœŸåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„CNNç»“æ„ä»¥æ•æ‰å±€éƒ¨ç‰¹å¾ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†é’ˆå¯¹ç‰¹å¾é€‰æ‹©çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DGIQAæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­è¡¨ç°çªå‡ºã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒDGIQAåœ¨ä½å…‰ç…§ã€é›¾éœ¾å’Œé•œå¤´å…‰æ™•ç­‰è‡ªç„¶å¤±çœŸåœºæ™¯ä¸‹çš„è¯„ä¼°å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒå¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œè‡ªåŠ¨åŒ–è´¨é‡è¯„ä¼°ç­‰ã€‚DGIQAæ¨¡å‹èƒ½å¤Ÿåœ¨å„ç§è‡ªç„¶å¤±çœŸæ¡ä»¶ä¸‹æä¾›æ›´å‡†ç¡®çš„å›¾åƒè´¨é‡è¯„ä¼°ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒå¢å¼ºã€è§†é¢‘ç›‘æ§å’ŒåŒ»ç–—å½±åƒåˆ†æç­‰é¢†åŸŸã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å›¾åƒå¤„ç†ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A long-held challenge in no-reference image quality assessment (NR-IQA) learning from human subjective perception is the lack of objective generalization to unseen natural distortions. To address this, we integrate a novel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which distills scene depth and spatial features into a structure-aware representation for improved NR-IQA. This brings in the knowledge of object saliency and relative contrast of the scene for more discriminative feature learning. Additionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse high-level global contextual dependencies from a transformer backbone with local spatial features captured by a set of hierarchical CNN (convolutional neural network) layers. We implement TCB and Depth-CAR as multimodal attention-based projection functions to select the most informative features, which also improve training time and inference efficiency. Experimental results demonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA) performance on both synthetic and authentic benchmark datasets. More importantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well as in assessing natural image distortions such as low-light effects, hazy conditions, and lens flares.

