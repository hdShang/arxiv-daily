---
layout: default
title: "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?"
---

# VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23359" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.23359v1</a>
  <a href="https://arxiv.org/pdf/2505.23359.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23359v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23359v1', 'VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yuanxin Liu, Kun Ouyang, Haoning Wu, Yi Liu, Lin Sui, Xinhao Li, Yan Zhong, Y. Charles, Xinyu Zhou, Xu Sun

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-29

**Â§áÊ≥®**: Project Page: https://llyx97.github.io/video_reason_bench/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VideoReasonBench‰ª•Ëß£ÂÜ≥ËßÜÈ¢ëÁêÜËß£‰∏≠ÁöÑÂ§çÊùÇÊé®ÁêÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÁêÜËß£` `Â§çÊùÇÊé®ÁêÜ` `ÈïøÈìæÊé®ÁêÜ` `Â§öÊ®°ÊÄÅLLMs` `Âü∫ÂáÜÊµãËØï` `ËßÜËßâÂÜÖÂÆπ` `Êé®ÁêÜËÉΩÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÁº∫‰πèË∂≥Â§üÁöÑÊé®ÁêÜÊ∑±Â∫¶ÔºåÊó†Ê≥ïÂÖÖÂàÜÂ±ïÁ§∫ÈïøÈìæÊé®ÁêÜÁöÑ‰ºòÂäø„ÄÇ
2. ÊèêÂá∫VideoReasonBenchÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫éËßÜËßâÂÜÖÂÆπÁöÑÂ§çÊùÇËßÜÈ¢ëÊé®ÁêÜÔºåËÆæËÆ°‰∫ÜÂ§öÂ±ÇÊ¨°ÁöÑÈóÆÈ¢ò‰ª•ËØÑ‰º∞Êé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÈÄöËøáÂØπ18‰∏™Â§öÊ®°ÊÄÅLLMsÁöÑËØÑ‰º∞ÔºåÂèëÁé∞Â§ßÂ§öÊï∞Ê®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåGemini-2.5-ProÁöÑË°®Áé∞ÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÊúüÁ†îÁ©∂Ë°®ÊòéÔºåÈïøÈìæÊé®ÁêÜÔºàCoTÔºâËÉΩÂ§üÊòæËëóÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇÁÑ∂ËÄåÔºåÂú®ËßÜÈ¢ëÁêÜËß£È¢ÜÂüüÔºåËøô‰∏Ä‰ºòÂäøÂ∞öÊú™ÂæóÂà∞È™åËØÅÔºåÂõ†‰∏∫Áé∞ÊúâÂü∫ÂáÜÁº∫‰πèË∂≥Â§üÁöÑÊé®ÁêÜÊ∑±Â∫¶„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜVideoReasonBenchÔºå‰∏Ä‰∏™Êó®Âú®ËØÑ‰º∞‰ª•ËßÜËßâ‰∏∫‰∏≠ÂøÉÁöÑÂ§çÊùÇËßÜÈ¢ëÊé®ÁêÜÁöÑÂü∫ÂáÜ„ÄÇÊØè‰∏™ËßÜÈ¢ëÂ±ïÁ§∫‰∫Ü‰∏ÄÁ≥ªÂàóÁªÜËá¥ÁöÑÊìç‰ΩúÔºåÈóÆÈ¢òÂàôËØÑ‰º∞‰∫Ü‰∏â‰∏™ÈÄêÊ≠•ÊèêÂçáÁöÑËßÜÈ¢ëÊé®ÁêÜÊäÄËÉΩ„ÄÇÈÄöËøáÂØπ18‰∏™ÊúÄÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅLLMsÁöÑËØÑ‰º∞ÔºåÂèëÁé∞Â§ßÂ§öÊï∞Âú®Â§çÊùÇËßÜÈ¢ëÊé®ÁêÜ‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåËÄåÊÄùÁª¥Â¢ûÂº∫ÁöÑGemini-2.5-ProÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÊ®°Âûã„ÄÇÁ†îÁ©∂ËøòÊè≠Á§∫‰∫ÜÂú®ÊµãËØïÊó∂Êâ©Â±ïÊÄùÁª¥È¢ÑÁÆóÂØπÊèêÂçáÊÄßËÉΩÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÂú®Êé®ÁêÜÊ∑±Â∫¶‰∏äÁöÑ‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇËßÜÈ¢ëÊé®ÁêÜ‰ªªÂä°‰∏≠ÔºåÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñÁü•ËØÜËÄåÈùûËßÜËßâÂÜÖÂÆπ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫VideoReasonBenchÂü∫ÂáÜÔºåÈÄöËøáËÆæËÆ°ÂåÖÂê´ÁªÜËá¥Êìç‰ΩúÁöÑËßÜÈ¢ëÔºåÁ°Æ‰øùËßÜËßâ‰∏∞ÂØåÊÄßÂíåÈ´òÊé®ÁêÜÂ§çÊùÇÊÄßÔºå‰ª•ËØÑ‰º∞Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVideoReasonBenchÂåÖÂê´Â§ö‰∏™ËßÜÈ¢ëÔºåÊØè‰∏™ËßÜÈ¢ëÂ±ïÁ§∫‰∏ÄÁ≥ªÂàóÊìç‰ΩúÔºåÈóÆÈ¢òÂàÜ‰∏∫‰∏â‰∏™Â±ÇÊ¨°ÔºöÂõûÂøÜËßÇÂØüÂà∞ÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÅÊé®Êñ≠ÊΩúÂú®Áä∂ÊÄÅÂÜÖÂÆπ„ÄÅÈ¢ÑÊµãË∂ÖÂá∫ËßÜÈ¢ëÁöÑ‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫Ü‰ª•ËßÜËßâ‰∏∫‰∏≠ÂøÉÁöÑÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°ÔºåÂº∫Ë∞É‰∫ÜÈïøÈìæÊé®ÁêÜÂú®ËßÜÈ¢ëÁêÜËß£‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºå‰∏éÁé∞ÊúâÂü∫ÂáÜÁöÑÁü•ËØÜÈ©±Âä®ÊñπÊ≥ïÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåËßÜÈ¢ëÂÜÖÂÆπÁöÑÈÄâÊã©ÂíåÈóÆÈ¢òÁöÑÂ±ÇÊ¨°ÂåñËÆæÁΩÆÊòØÂÖ≥ÈîÆÔºåÁ°Æ‰øùÊ®°ÂûãÈúÄË¶ÅËøõË°åÈÄêÊ≠•Êé®ÁêÜ‰ª•ÂæóÂá∫Ê≠£Á°ÆÁ≠îÊ°à„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå18‰∏™Â§öÊ®°ÊÄÅLLMsÂú®VideoReasonBench‰∏äÁöÑË°®Áé∞ÊôÆÈÅçËæÉÂ∑ÆÔºå‰æãÂ¶ÇGPT-4o‰ªÖËææÂà∞6.9%ÁöÑÂáÜÁ°ÆÁéáÔºåËÄåÊÄùÁª¥Â¢ûÂº∫ÁöÑGemini-2.5-ProÂàô‰ª•56.0%ÁöÑÂáÜÁ°ÆÁéáÊòæËëóÈ¢ÜÂÖàÔºåÂ±ïÁ§∫‰∫ÜÈïøÈìæÊé®ÁêÜÂú®Â§çÊùÇËßÜÈ¢ëÊé®ÁêÜ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ËßÜÈ¢ëÂàÜÊûê„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂Á≠âÔºåËÉΩÂ§üÂ∏ÆÂä©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊé®ÁêÜËßÜÈ¢ëÂÜÖÂÆπÔºåÊèêÂçá‰∫∫Êú∫‰∫§‰∫íÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇÊú™Êù•ÔºåËØ•Âü∫ÂáÜÂèØËÉΩÊé®Âä®ËßÜÈ¢ëÁêÜËß£È¢ÜÂüüÁöÑËøõ‰∏ÄÊ≠•Á†îÁ©∂ÂíåÊäÄÊúØËøõÊ≠•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent studies have shown that long chain-of-thought (CoT) reasoning can significantly enhance the performance of large language models (LLMs) on complex tasks. However, this benefit is yet to be demonstrated in the domain of video understanding, since most existing benchmarks lack the reasoning depth required to demonstrate the advantages of extended CoT chains. While recent efforts have proposed benchmarks aimed at video reasoning, the tasks are often knowledge-driven and do not rely heavily on visual content. To bridge this gap, we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric, complex video reasoning. To ensure visual richness and high reasoning complexity, each video in VideoReasonBench depicts a sequence of fine-grained operations on a latent state that is only visible in part of the video. The questions evaluate three escalating levels of video reasoning skills: recalling observed visual information, inferring the content of latent states, and predicting information beyond the video. Under such task setting, models have to precisely recall multiple operations in the video, and perform step-by-step reasoning to get correct final answers for these questions. Using VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal LLMs (MLLMs), finding that most perform poorly on complex video reasoning, e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our investigations on "test-time scaling" further reveal that extended thinking budget, while offering none or minimal benefits on existing video benchmarks, is essential for improving the performance on VideoReasonBench.

