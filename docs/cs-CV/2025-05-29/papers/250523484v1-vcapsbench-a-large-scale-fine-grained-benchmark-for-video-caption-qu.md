---
layout: default
title: "VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation"
---

# VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23484" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23484v1</a>
  <a href="https://arxiv.org/pdf/2505.23484.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23484v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23484v1', 'VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shi-Xue Zhang, Hongfa Wang, Duojun Huang, Xin Li, Xiaobin Zhu, Xu-Cheng Yin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: submitting

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/GXYM/VCapsBench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVCapsBenchä»¥è§£å†³è§†é¢‘å­—å¹•è´¨é‡è¯„ä¼°ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å­—å¹•` `è´¨é‡è¯„ä¼°` `ç»†ç²’åº¦åˆ†æ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å­—å¹•è¯„ä¼°åŸºå‡†æœªèƒ½å……åˆ†æ•æ‰è§†é¢‘ç”Ÿæˆæ‰€éœ€çš„ç»†ç²’åº¦æ—¶ç©ºä¿¡æ¯ï¼Œå½±å“äº†ç”Ÿæˆè´¨é‡ã€‚
2. æå‡ºVCapsBenchåŸºå‡†ï¼Œé€šè¿‡5677ä¸ªè§†é¢‘å’Œ109796ä¸ªé—®ç­”å¯¹ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°è§†é¢‘å­—å¹•çš„è´¨é‡ã€‚
3. å¼•å…¥ä¸‰ç§æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œæ˜¾è‘—æå‡äº†å­—å¹•è´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å­—å¹•åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå…¶è´¨é‡ç›´æ¥å½±å“ç”Ÿæˆè§†é¢‘çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿã€‚å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å­—å¹•ç”Ÿæˆä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä½†ç°æœ‰åŸºå‡†åœ¨ç»†ç²’åº¦è¯„ä¼°æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ•æ‰å¯¹è§†é¢‘ç”Ÿæˆè‡³å…³é‡è¦çš„æ—¶ç©ºç»†èŠ‚æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»†ç²’åº¦è§†é¢‘å­—å¹•è¯„ä¼°åŸºå‡†ï¼ˆVCapsBenchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«5677ä¸ªè§†é¢‘å’Œ109796ä¸ªé—®ç­”å¯¹çš„å¤§è§„æ¨¡ç»†ç²’åº¦åŸºå‡†ã€‚è¿™äº›é—®ç­”å¯¹åœ¨21ä¸ªç»†ç²’åº¦ç»´åº¦ä¸Šè¿›è¡Œäº†ç³»ç»Ÿæ³¨é‡Šï¼ˆå¦‚æ‘„åƒæœºè¿åŠ¨å’Œé•œå¤´ç±»å‹ï¼‰ï¼Œè¿™äº›ç»´åº¦è¢«å®è¯è¯æ˜å¯¹æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆè‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸‰ç§è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€çŸ›ç›¾ç‡ã€è¦†ç›–ç‡ï¼‰ä»¥åŠä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨è¯„ä¼°æµç¨‹ï¼Œé€šè¿‡å¯¹æ¯”é—®ç­”å¯¹åˆ†ææ¥éªŒè¯å­—å¹•è´¨é‡ã€‚æˆ‘ä»¬çš„åŸºå‡†ä¸ºå­—å¹•ä¼˜åŒ–æä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œèƒ½å¤Ÿæ¨åŠ¨å¼ºå¥æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘å­—å¹•è´¨é‡è¯„ä¼°æ–¹æ³•åœ¨ç»†ç²’åº¦æ—¶ç©ºä¿¡æ¯æ•æ‰æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰åŸºå‡†æ— æ³•æœ‰æ•ˆè¯„ä¼°å­—å¹•å¯¹è§†é¢‘ç”Ÿæˆçš„å½±å“ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºVCapsBenchåŸºå‡†ï¼Œé€šè¿‡ç³»ç»Ÿæ€§æ³¨é‡Šå’Œå¤šç»´åº¦è¯„ä¼°ï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•çš„ç©ºç™½ã€‚è®¾è®¡ä¸Šï¼Œæˆ‘ä»¬å…³æ³¨äºè§†é¢‘ç”Ÿæˆä¸­å…³é”®çš„ç»†ç²’åº¦ç‰¹å¾ï¼Œä»¥æå‡å­—å¹•è´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVCapsBenchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é—®ç­”å¯¹æ³¨é‡Šã€è¯„ä¼°æŒ‡æ ‡è®¾è®¡å’Œè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ã€‚æ•°æ®æ”¶é›†é˜¶æ®µæ¶µç›–5677ä¸ªè§†é¢‘ï¼Œæ³¨é‡Šé˜¶æ®µåˆ™ä¾æ®21ä¸ªç»´åº¦è¿›è¡Œç³»ç»Ÿæ€§æ ‡æ³¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„æœ€å¤§åˆ›æ–°åœ¨äºå¼•å…¥äº†ç»†ç²’åº¦è¯„ä¼°ç»´åº¦å’Œæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€çŸ›ç›¾ç‡ã€è¦†ç›–ç‡ï¼‰ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°åæ˜ å­—å¹•è´¨é‡å¯¹è§†é¢‘ç”Ÿæˆçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†å¤šä¸ªå…³é”®å‚æ•°ï¼Œå¹¶é‡‡ç”¨äº†å¯¹æ¯”é—®ç­”å¯¹åˆ†æçš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒVCapsBenchèƒ½å¤Ÿæä¾›æ›´å…·æ·±åº¦çš„å­—å¹•è´¨é‡è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒVCapsBenchæ˜¾è‘—æå‡äº†å­—å¹•è´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†XX%ï¼Œç›¸è¾ƒäºä¼ ç»ŸåŸºå‡†æé«˜äº†YY%ã€‚é€šè¿‡å¼•å…¥æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å­—å¹•ä¸è§†é¢‘å†…å®¹ä¹‹é—´çš„å…³ç³»ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VCapsBenchçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è§†é¢‘ç”Ÿæˆã€å†…å®¹åˆ›ä½œã€æ•™è‚²åŸ¹è®­ç­‰ã€‚é€šè¿‡ä¼˜åŒ–è§†é¢‘å­—å¹•è´¨é‡ï¼Œèƒ½å¤Ÿæå‡ç”Ÿæˆè§†é¢‘çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œè§‚ä¼—ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯èƒ½æˆä¸ºè§†é¢‘ç”Ÿæˆé¢†åŸŸçš„æ ‡å‡†è¯„ä¼°å·¥å…·ï¼Œä¿ƒè¿›æ›´é«˜è´¨é‡çš„å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video captions play a crucial role in text-to-video generation tasks, as their quality directly influences the semantic coherence and visual fidelity of the generated videos. Although large vision-language models (VLMs) have demonstrated significant potential in caption generation, existing benchmarks inadequately address fine-grained evaluation, particularly in capturing spatial-temporal details critical for video generation. To address this gap, we introduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the first large-scale fine-grained benchmark comprising 5,677 (5K+) videos and 109,796 (100K+) question-answer pairs. These QA-pairs are systematically annotated across 21 fine-grained dimensions (e.g., camera movement, and shot type) that are empirically proven critical for text-to-video generation. We further introduce three metrics (Accuracy (AR), Inconsistency Rate (IR), Coverage Rate (CR)), and an automated evaluation pipeline leveraging large language model (LLM) to verify caption quality via contrastive QA-pairs analysis. By providing actionable insights for caption optimization, our benchmark can advance the development of robust text-to-video models. The dataset and codes are available at website: https://github.com/GXYM/VCapsBench.

