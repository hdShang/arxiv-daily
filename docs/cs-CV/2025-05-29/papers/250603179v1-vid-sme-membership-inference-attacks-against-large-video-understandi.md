---
layout: default
title: Vid-SME: Membership Inference Attacks against Large Video Understanding Models
---

# Vid-SME: Membership Inference Attacks against Large Video Understanding Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03179" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03179v1</a>
  <a href="https://arxiv.org/pdf/2506.03179.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03179v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03179v1', 'Vid-SME: Membership Inference Attacks against Large Video Understanding Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Li, Runpeng Yu, Xinchao Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVid-SMEä»¥è§£å†³è§†é¢‘ç†è§£æ¨¡å‹çš„æˆå‘˜æ¨æ–­æ”»å‡»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `æˆå‘˜æ¨æ–­æ”»å‡»` `Sharma-Mittalç†µ` `å¤šæ¨¡æ€æ¨¡å‹` `æ•°æ®éšç§` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æˆå‘˜æ¨æ–­æ”»å‡»æ–¹æ³•åœ¨è§†é¢‘é¢†åŸŸçš„æ‰©å±•æ€§å·®ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰è§†é¢‘å¸§çš„æ—¶é—´å˜åŒ–ï¼Œå¯¼è‡´ä½çœŸé˜³æ€§ç‡ã€‚
2. æœ¬æ–‡æå‡ºVid-SMEï¼Œåˆ©ç”¨æ¨¡å‹è¾“å‡ºçš„ç½®ä¿¡åº¦å’Œè‡ªé€‚åº”å‚æ•°åŒ–è®¡ç®—Sharma-Mittalç†µï¼Œä»¥ç”Ÿæˆè§†é¢‘çš„æˆå‘˜è¯„åˆ†ã€‚
3. åœ¨å¤šç§è‡ªè®­ç»ƒå’Œå¼€æºè§†é¢‘ç†è§£æ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒVid-SMEæ˜¾è‘—æå‡äº†æˆå‘˜æ¨æ–­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¿æ³›åº”ç”¨äºè§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¼•å‘äº†ä¸¥é‡çš„æ•°æ®éšç§é—®é¢˜ï¼Œå°¤å…¶æ˜¯è®­ç»ƒæ•°æ®é›†ä¸­å¯èƒ½åŒ…å«æ•æ„Ÿè§†é¢‘å†…å®¹ã€‚ç°æœ‰çš„æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAsï¼‰æ–¹æ³•åœ¨è§†é¢‘é¢†åŸŸçš„é€‚ç”¨æ€§ä¸è¶³ï¼Œä¸»è¦ç”±äºæœªèƒ½æœ‰æ•ˆæ•æ‰è§†é¢‘å¸§çš„æ—¶é—´å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Vid-SMEï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è§†é¢‘æ•°æ®çš„æˆå‘˜æ¨æ–­æ–¹æ³•ï¼Œåˆ©ç”¨æ¨¡å‹è¾“å‡ºçš„ç½®ä¿¡åº¦å’Œè‡ªé€‚åº”å‚æ•°åŒ–è®¡ç®—Sharma-Mittalç†µï¼ˆSMEï¼‰ï¼Œé€šè¿‡è‡ªç„¶è§†é¢‘å¸§ä¸æ—¶é—´åè½¬è§†é¢‘å¸§ä¹‹é—´çš„SMEå·®å¼‚ï¼Œç”Ÿæˆç¨³å¥çš„æˆå‘˜è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVid-SMEåœ¨å¤šç§è‡ªè®­ç»ƒå’Œå¼€æºè§†é¢‘ç†è§£æ¨¡å‹ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æˆå‘˜æ¨æ–­æ”»å‡»æ–¹æ³•åœ¨è§†é¢‘ç†è§£æ¨¡å‹ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å…¶åœ¨å¤„ç†è§†é¢‘å¸§æ—¶é—´å˜åŒ–æ—¶çš„ä½æ•ˆæ€§å’Œæ‰©å±•æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVid-SMEé€šè¿‡ç»“åˆæ¨¡å‹è¾“å‡ºçš„ç½®ä¿¡åº¦å’Œè‡ªé€‚åº”å‚æ•°åŒ–ï¼Œè®¡ç®—è§†é¢‘è¾“å…¥çš„Sharma-Mittalç†µï¼Œåˆ©ç”¨è‡ªç„¶è§†é¢‘å¸§ä¸æ—¶é—´åè½¬å¸§ä¹‹é—´çš„ç†µå·®å¼‚æ¥ç”Ÿæˆæˆå‘˜è¯„åˆ†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVid-SMEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è¾“å‡ºç½®ä¿¡åº¦è®¡ç®—ã€Sharma-Mittalç†µè®¡ç®—å’Œæˆå‘˜è¯„åˆ†ç”Ÿæˆå››ä¸ªä¸»è¦æ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šVid-SMEæ˜¯é¦–ä¸ªä¸“ä¸ºè§†é¢‘æ•°æ®è®¾è®¡çš„æˆå‘˜æ¨æ–­æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è§†é¢‘å¸§çš„æ—¶é—´å˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†æˆå‘˜æ¨æ–­çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒVid-SMEé‡‡ç”¨è‡ªé€‚åº”å‚æ•°åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–äº†ç†µè®¡ç®—è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†ä¸åŒå¸§æ•°å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒVid-SMEåœ¨ä½å‡é˜³æ€§ç‡ä¸‹å®ç°äº†æ›´é«˜çš„çœŸé˜³æ€§ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVid-SMEåœ¨å¤šä¸ªè‡ªè®­ç»ƒå’Œå¼€æºè§†é¢‘ç†è§£æ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†æˆå‘˜æ¨æ–­çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä½å‡é˜³æ€§ç‡ä¸‹å®ç°äº†æ›´é«˜çš„çœŸé˜³æ€§ç‡ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Vid-SMEçš„ç ”ç©¶æˆæœåœ¨è§†é¢‘ç†è§£é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠæ•æ„Ÿè§†é¢‘å†…å®¹çš„åœºæ™¯ä¸­ï¼Œå¦‚ä¸ªäººéšç§ä¿æŠ¤å’Œç›‘æ§è§†é¢‘åˆ†æã€‚é€šè¿‡æé«˜å¯¹è®­ç»ƒæ•°æ®çš„æˆå‘˜æ¨æ–­èƒ½åŠ›ï¼ŒVid-SMEèƒ½å¤Ÿå¸®åŠ©å¼€å‘æ›´å®‰å…¨çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå‡å°‘æ•°æ®æ³„éœ²é£é™©ï¼Œæ¨åŠ¨éšç§ä¿æŠ¤æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) demonstrate remarkable capabilities in handling complex multimodal tasks and are increasingly adopted in video understanding applications. However, their rapid advancement raises serious data privacy concerns, particularly given the potential inclusion of sensitive video content, such as personal recordings and surveillance footage, in their training datasets. Determining improperly used videos during training remains a critical and unresolved challenge. Despite considerable progress on membership inference attacks (MIAs) for text and image data in MLLMs, existing methods fail to generalize effectively to the video domain. These methods suffer from poor scalability as more frames are sampled and generally achieve negligible true positive rates at low false positive rates (TPR@Low FPR), mainly due to their failure to capture the inherent temporal variations of video frames and to account for model behavior differences as the number of frames varies. To address these challenges, we introduce Vid-SME, the first membership inference method tailored for video data used in video understanding LLMs (VULLMs). Vid-SME leverages the confidence of model output and integrates adaptive parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By leveraging the SME difference between natural and temporally-reversed video frames, Vid-SME derives robust membership scores to determine whether a given video is part of the model's training set. Experiments on various self-trained and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.

