---
layout: default
title: BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning
---

# BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23883" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23883v2</a>
  <a href="https://arxiv.org/pdf/2505.23883.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23883v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23883v2', 'BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jianyang Gu, Samuel Stevens, Elizabeth G Campolongo, Matthew J Thompson, Net Zhang, Jiaman Wu, Andrei Kopanev, Zheda Mai, Alexander E. White, James Balhoff, Wasila Dahdul, Daniel Rubenstein, Hilmar Lapp, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-10-23)

**å¤‡æ³¨**: NeurIPS 2025 Spotlight; Project page: https://imageomics.github.io/bioclip-2/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBioCLIP 2ä»¥è§£å†³ç”Ÿç‰©è§†è§‰æ¨¡å‹çš„èƒ½åŠ›æå‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”Ÿç‰©è§†è§‰` `å¯¹æ¯”å­¦ä¹ ` `å±‚æ¬¡ç›‘ç£` `åµŒå…¥ç©ºé—´` `ç”Ÿæ€å­¦` `ç‰©ç§åˆ†ç±»` `å¤§è§„æ¨¡è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç”Ÿç‰©è§†è§‰æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¯¹ç‰©ç§é—´å’Œç‰©ç§å†…å˜å¼‚çš„æœ‰æ•ˆæ•æ‰ã€‚
2. æœ¬æ–‡æå‡ºBioCLIP 2ï¼Œé€šè¿‡å¤§è§„æ¨¡å¯¹æ¯”å­¦ä¹ å’Œå±‚æ¬¡ç›‘ç£ï¼Œæå‡ç”Ÿç‰©è§†è§‰æ¨¡å‹çš„è¡¨ç°å’Œèƒ½åŠ›ã€‚
3. BioCLIP 2åœ¨å¤šé¡¹ç”Ÿç‰©è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨æ –æ¯åœ°åˆ†ç±»å’Œç‰¹å¾é¢„æµ‹ä¸Šï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸‹å±•ç°å‡ºæ˜¾è‘—çš„çªç°è¡Œä¸ºï¼Œè¶…è¶Šäº†åˆå§‹è®­ç»ƒç›®æ ‡ã€‚æœ¬æ–‡é€šè¿‡å¤§è§„æ¨¡å¯¹æ¯”è§†è§‰-è¯­è¨€è®­ç»ƒï¼Œå‘ç°ç”Ÿç‰©è§†è§‰æ¨¡å‹ä¸­çš„çªç°è¡Œä¸ºã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†TreeOfLife-200Mæ•°æ®é›†ï¼ŒåŒ…å«2.14äº¿å¼ ç”Ÿç‰©å›¾åƒï¼Œéšååœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒBioCLIP 2ä»¥åŒºåˆ†ä¸åŒç‰©ç§ã€‚å°½ç®¡è®­ç»ƒç›®æ ‡è¾ƒçª„ï¼ŒBioCLIP 2åœ¨æ –æ¯åœ°åˆ†ç±»å’Œç‰¹å¾é¢„æµ‹ç­‰ç”Ÿç‰©è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨BioCLIP 2çš„åµŒå…¥ç©ºé—´ä¸­è¯†åˆ«å‡ºçªç°å±æ€§ï¼Œç‰©ç§é—´çš„åµŒå…¥åˆ†å¸ƒä¸åŠŸèƒ½å’Œç”Ÿæ€æ„ä¹‰ç´§å¯†å¯¹é½ï¼Œè€Œç‰©ç§å†…çš„å˜å¼‚åˆ™åœ¨æ­£äº¤å­ç©ºé—´ä¸­å¾—ä»¥æ›´å¥½åœ°ä¿ç•™å’Œåˆ†ç¦»ã€‚æˆ‘ä»¬æä¾›äº†å½¢å¼åŒ–è¯æ˜å’Œåˆ†æï¼Œè§£é‡Šäº†å±‚æ¬¡ç›‘ç£å’Œå¯¹æ¯”ç›®æ ‡å¦‚ä½•ä¿ƒè¿›è¿™äº›çªç°å±æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿç‰©è§†è§‰æ¨¡å‹åœ¨ç‰©ç§åŒºåˆ†å’Œå˜å¼‚æ•æ‰æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†ç‰©ç§é—´å’Œç‰©ç§å†…çš„å¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„TreeOfLife-200Mæ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒBioCLIP 2ï¼Œåˆ©ç”¨å±‚æ¬¡ç›‘ç£å’Œå¯¹æ¯”å­¦ä¹ çš„ç»“åˆï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œè¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’ŒåµŒå…¥ç©ºé—´åˆ†æä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æä¾›äº†ä¸°å¯Œçš„ç”Ÿç‰©å›¾åƒï¼Œæ¨¡å‹è®­ç»ƒé‡‡ç”¨å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæœ€åé€šè¿‡åˆ†æåµŒå…¥ç©ºé—´æ¥è¯†åˆ«çªç°å±æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šBioCLIP 2çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨å±‚æ¬¡ç›‘ç£ä¸‹æœ‰æ•ˆæ•æ‰ç‰©ç§é—´å’Œç‰©ç§å†…çš„å˜å¼‚ï¼Œå½¢æˆç”Ÿç‰©å­¦ä¸Šæœ‰æ„ä¹‰çš„åµŒå…¥ç©ºé—´ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€ä»»åŠ¡ç›®æ ‡å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¢å¼ºå¯¹æ¯”å­¦ä¹ æ•ˆæœï¼ŒåŒæ—¶è®¾è®¡äº†å¤šå±‚æ¬¡çš„ç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†ä¸åŒå±‚æ¬¡çš„ç‰¹å¾å’Œå˜å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

BioCLIP 2åœ¨æ –æ¯åœ°åˆ†ç±»å’Œç‰¹å¾é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„å‡†ç¡®æ€§ï¼Œå…·ä½“æ€§èƒ½æ•°æ®è¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨ç”Ÿç‰©è§†è§‰ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹ã€ç”Ÿæ€ç³»ç»Ÿç®¡ç†å’Œç‰©ç§ä¿æŠ¤ç­‰ã€‚é€šè¿‡æå‡ç”Ÿç‰©è§†è§‰æ¨¡å‹çš„èƒ½åŠ›ï¼ŒBioCLIP 2å¯ä¸ºç”Ÿç‰©å­¦ç ”ç©¶æä¾›æ›´ä¸ºç²¾å‡†çš„å·¥å…·ï¼Œæ¨åŠ¨ç”Ÿæ€å­¦å’Œç”Ÿç‰©ä¿¡æ¯å­¦çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models trained at scale exhibit remarkable emergent behaviors, learning new capabilities beyond their initial training objectives. We find such emergent behaviors in biological vision models via large-scale contrastive vision-language training. To achieve this, we first curate TreeOfLife-200M, comprising 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We then train BioCLIP 2 on TreeOfLife-200M to distinguish different species. Despite the narrow training objective, BioCLIP 2 yields extraordinary accuracy when applied to various biological visual tasks such as habitat classification and trait prediction. We identify emergent properties in the learned embedding space of BioCLIP 2. At the inter-species level, the embedding distribution of different species aligns closely with functional and ecological meanings (e.g., beak sizes and habitats). At the intra-species level, instead of being diminished, the intra-species variations (e.g., life stages and sexes) are preserved and better separated in subspaces orthogonal to inter-species distinctions. We provide formal proof and analyses to explain why hierarchical supervision and contrastive objectives encourage these emergent properties. Crucially, our results reveal that these properties become increasingly significant with larger-scale training data, leading to a biologically meaningful embedding space.

