---
layout: default
title: "UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning"
---

# UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23380" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23380v1</a>
  <a href="https://arxiv.org/pdf/2505.23380.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23380v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23380v1', 'UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weijia Mao, Zhenheng Yang, Mike Zheng Shou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/showlab/UniRL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniRLä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹åè®­ç»ƒæ•°æ®ä¾èµ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `è‡ªæˆ‘ç”Ÿæˆ` `åè®­ç»ƒ` `ç›‘ç£å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `ç”Ÿæˆä¸ç†è§£` `æ•°æ®ä¾èµ–` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µä¾èµ–å¤§é‡å¤–éƒ¨æ•°æ®ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œä¸”åè®­ç»ƒæ–¹æ³•é€šå¸¸å—é™äºç‰¹å®šä»»åŠ¡ã€‚
2. UniRLé€šè¿‡è‡ªæˆ‘ç”Ÿæˆå›¾åƒä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ•°æ®çš„ä¾èµ–ï¼Œå¹¶å®ç°ç”Ÿæˆä¸ç†è§£ä»»åŠ¡çš„ç›¸äº’ä¿ƒè¿›ã€‚
3. åœ¨Show-oå’ŒJanusæ¨¡å‹ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒUniRLåˆ†åˆ«å–å¾—äº†0.77å’Œ0.65çš„GenEvalå¾—åˆ†ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¦‚Show-oå’ŒJanusåœ¨ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶åœ¨é¢„è®­ç»ƒé˜¶æ®µéœ€è¦å¤§é‡è®¡ç®—ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åè®­ç»ƒæ–¹æ³•å¾€å¾€ä¾èµ–å¤–éƒ¨æ•°æ®æˆ–ä»…é™äºç‰¹å®šä»»åŠ¡çš„å®šåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†UniRLï¼Œä¸€ç§è‡ªæˆ‘æ”¹è¿›çš„åè®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ¯æ¬¡è¿­ä»£ä¸­ç”Ÿæˆå›¾åƒå¹¶å°†å…¶ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨å›¾åƒæ•°æ®ã€‚è¯¥æ–¹æ³•ä½¿å¾—ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ç›¸äº’ä¿ƒè¿›ï¼Œç”Ÿæˆçš„å›¾åƒç”¨äºç†è§£ï¼Œç†è§£ç»“æœåˆ™ç”¨äºç›‘ç£ç”Ÿæˆã€‚æˆ‘ä»¬æ¢ç´¢äº†ç›‘ç£å¾®è°ƒ(SFT)å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)æ¥ä¼˜åŒ–æ¨¡å‹ã€‚UniRLå…·æœ‰ä¸‰ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šä¸éœ€è¦å¤–éƒ¨å›¾åƒæ•°æ®ã€æé«˜äº†ä»»åŠ¡æ€§èƒ½å¹¶å‡å°‘ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„ä¸å¹³è¡¡ã€ä»…éœ€åœ¨åè®­ç»ƒé˜¶æ®µå¢åŠ å°‘é‡è®­ç»ƒæ­¥éª¤ã€‚æˆ‘ä»¬åœ¨Show-oå’ŒJanusä¸Šè¯„ä¼°UniRLï¼Œåˆ†åˆ«å–å¾—äº†0.77å’Œ0.65çš„GenEvalå¾—åˆ†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨åè®­ç»ƒé˜¶æ®µå¯¹å¤–éƒ¨æ•°æ®çš„ä¾èµ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨çµæ´»æ€§å’Œé€šç”¨æ€§ä¸Šå­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniRLçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æ¨¡å‹è‡ªæˆ‘ç”Ÿæˆå›¾åƒï¼Œåˆ©ç”¨ç”Ÿæˆçš„å›¾åƒä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œé¿å…äº†å¯¹å¤–éƒ¨å›¾åƒæ•°æ®çš„éœ€æ±‚ï¼ŒåŒæ—¶ä¿ƒè¿›ç”Ÿæˆä¸ç†è§£ä»»åŠ¡çš„ç›¸äº’æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ç”Ÿæˆæ¨¡å—å’Œç†è§£æ¨¡å—ï¼Œç”Ÿæˆæ¨¡å—è´Ÿè´£ä»æç¤ºç”Ÿæˆå›¾åƒï¼Œç†è§£æ¨¡å—åˆ™åˆ©ç”¨ç”Ÿæˆçš„å›¾åƒè¿›è¡Œç†è§£ä»»åŠ¡ï¼ŒäºŒè€…é€šè¿‡åé¦ˆæœºåˆ¶ç›¸äº’ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniRLçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è‡ªæˆ‘ç”Ÿæˆæ•°æ®çš„èƒ½åŠ›ï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­è‡ªæˆ‘æ”¹è¿›ï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤–éƒ¨æ•°æ®çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç›‘ç£å¾®è°ƒ(SFT)å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ä½œä¸ºä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆä¸ç†è§£ä»»åŠ¡çš„å¹³è¡¡ï¼Œå¹¶é€šè¿‡å°‘é‡é¢å¤–è®­ç»ƒæ­¥éª¤å®ç°é«˜æ•ˆçš„åè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒUniRLåœ¨Show-oå’ŒJanusæ¨¡å‹ä¸Šåˆ†åˆ«å–å¾—äº†0.77å’Œ0.65çš„GenEvalå¾—åˆ†ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒUniRLä¸ä»…æé«˜äº†å•ä¸€ä»»åŠ¡çš„è¡¨ç°ï¼Œè¿˜æœ‰æ•ˆåœ°å‡å°‘äº†ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„æ€§èƒ½ä¸å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniRLçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆã€æ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡å‡å°‘å¯¹å¤–éƒ¨æ•°æ®çš„ä¾èµ–ï¼ŒUniRLèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL.

