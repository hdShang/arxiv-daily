---
layout: default
title: Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model
---

# Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24007" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24007v2</a>
  <a href="https://arxiv.org/pdf/2505.24007.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24007v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24007v2', 'Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nokimul Hasan Arif, Shadman Rabby, Md Hefzul Hossain Papon, Sabbir Ahmed

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-06-27)

**å¤‡æ³¨**: Submitted for review in NCAA Springer, 21 pages, 4 figures, 4 Tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé¢„é˜²æ€§å¹»è§‰å‡å°‘æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹` `è§†è§‰å¹»è§‰` `è¾“å…¥é¢„å¤„ç†` `è‡ªé€‚åº”è¿‡æ»¤` `æ¨¡å‹å¯é æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºäº‹åä¿®æ­£ï¼Œç¼ºä¹æœ‰æ•ˆçš„è¾“å…¥é˜¶æ®µé¢„å¤„ç†æŠ€æœ¯æ¥è§£å†³å¹»è§‰é—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºäº†ä¸€ç§é›†æˆé¢„å¤„ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®é—®é¢˜ç±»å‹è‡ªé€‚åº”é€‰æ‹©ä¸åŒçš„è¾“å…¥è¿‡æ»¤æ–¹æ³•ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šåœ¨`HaloQuest`æ•°æ®é›†ä¸Šå®ç°äº†44.3%çš„å¹»è§‰ç‡é™ä½ï¼Œæ˜¾è‘—æå‡äº†LLMçš„å“åº”å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œè§†è§‰å¹»è§‰æ˜¯æŒ‡æ¨¡å‹ç”Ÿæˆçš„å“åº”ä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´ï¼Œè¿™åœ¨éœ€è¦ç²¾ç¡®å’Œå¯é è¾“å‡ºçš„åœºæ™¯ä¸­æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨äº‹åä¿®æ­£æˆ–æ¨¡å‹ç‰¹å®šçš„å¾®è°ƒç­–ç•¥ä¸Šï¼Œè€Œå¯¹è¾“å…¥é˜¶æ®µçš„é¢„å¤„ç†æŠ€æœ¯æ¢ç´¢æœ‰é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºé›†æˆçš„é¢„å¤„ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®é—®é¢˜ç±»å‹è‡ªé€‚åº”é€‰æ‹©æœ€åˆé€‚çš„è¿‡æ»¤æ–¹æ³•ï¼Œä»è€Œå‡å°‘å¹»è§‰ï¼Œè€Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒæµç¨‹è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚åœ¨`HaloQuest`æ•°æ®é›†ä¸Šè¯„ä¼°åï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†44.3%çš„å¹»è§‰ç‡é™ä½ï¼Œè¡¨æ˜æ™ºèƒ½è¾“å…¥æ¡ä»¶è®¾ç½®èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºLLMå“åº”çš„äº‹å®åŸºç¡€ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†è‡ªé€‚åº”é¢„å¤„ç†æŠ€æœ¯åœ¨å‡è½»å¹»è§‰æ–¹é¢çš„é‡è¦æ€§ï¼Œä¸ºæ›´å¯é çš„å¤šæ¨¡æ€ç³»ç»Ÿåº”å¯¹ç°å®ä¸–ç•ŒæŒ‘æˆ˜é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è§†è§‰å¹»è§‰çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹åæœŸçš„ä¿®æ­£ï¼Œæœªèƒ½æœ‰æ•ˆå¤„ç†è¾“å…¥é˜¶æ®µçš„å¹»è§‰ç°è±¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºé›†æˆçš„é¢„å¤„ç†æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”é€‰æ‹©æœ€åˆé€‚çš„è¾“å…¥è¿‡æ»¤æ–¹æ³•ï¼ˆå¦‚å™ªå£°å‡å°‘ã€è¾¹ç¼˜å¢å¼ºæˆ–åŸå§‹è¾“å…¥ï¼‰ï¼Œä»¥å‡å°‘å¹»è§‰ç°è±¡ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨åœ¨ä¸ä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒæµç¨‹çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„è¾“å‡ºå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¾“å…¥è¿‡æ»¤æ¨¡å—ã€é—®é¢˜ç±»å‹è¯†åˆ«æ¨¡å—å’Œå¹»è§‰æ£€æµ‹æ¨¡å—ã€‚è¾“å…¥è¿‡æ»¤æ¨¡å—æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ç›¸åº”çš„è¿‡æ»¤æ–¹æ³•ï¼Œé—®é¢˜ç±»å‹è¯†åˆ«æ¨¡å—åˆ†æè¾“å…¥é—®é¢˜ä»¥ç¡®å®šæœ€ä½³ç­–ç•¥ï¼Œå¹»è§‰æ£€æµ‹æ¨¡å—åˆ™è¯„ä¼°è¾“å‡ºçš„å¯é æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„è¾“å…¥é¢„å¤„ç†ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®å…·ä½“é—®é¢˜åŠ¨æ€è°ƒæ•´è¾“å…¥ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘å¹»è§‰ã€‚è¿™ä¸ä¼ ç»Ÿçš„åæœŸä¿®æ­£æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬è¿‡æ»¤æ–¹æ³•çš„é€‰æ‹©æ ‡å‡†ã€è¾“å…¥é—®é¢˜çš„åˆ†ç±»ç®—æ³•ï¼Œä»¥åŠå¹»è§‰æ£€æµ‹çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚è‡ªç„¶è¯­è¨€æ¨ç†å¾—åˆ†ï¼‰ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨`HaloQuest`æ•°æ®é›†ä¸Šå®ç°äº†44.3%çš„å¹»è§‰ç‡é™ä½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚è¿™ä¸€æå‡è¯æ˜äº†è¾“å…¥çº§é¢„å¤„ç†åœ¨å¢å¼ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¾“å‡ºå¯é æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨å†…å®¹ç”Ÿæˆç­‰ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¯é æ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨åŒ»ç–—ã€æ•™è‚²å’Œè‡ªåŠ¨é©¾é©¶ç­‰éœ€è¦é«˜ç²¾åº¦è¾“å‡ºçš„é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual hallucinations in Large Language Models (LLMs), where the model generates responses that are inconsistent with the visual input, pose a significant challenge to their reliability, particularly in contexts where precise and trustworthy outputs are critical. Current research largely emphasizes post-hoc correction or model-specific fine-tuning strategies, with limited exploration of preprocessing techniques to address hallucination issues at the input stage. This study presents a novel ensemble-based preprocessing framework that adaptively selects the most appropriate filtering approach -- noise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the type of question posed, resulting into reduced hallucination without requiring any modifications to the underlying model architecture or training pipeline. Evaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal reasoning on visually complex inputs, our method achieves a 44.3% reduction in hallucination rates, as measured by Natural Language Inference (NLI) scores using SelfCheckGPT. This demonstrates that intelligent input conditioning alone can significantly enhance factual grounding in LLM responses. The findings highlight the importance of adaptive preprocessing techniques in mitigating hallucinations, paving the way for more reliable multimodal systems capable of addressing real-world challenges.

