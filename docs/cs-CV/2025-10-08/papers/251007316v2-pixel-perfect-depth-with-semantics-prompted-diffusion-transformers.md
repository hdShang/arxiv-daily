---
layout: default
title: Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers
---

# Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07316" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07316v2</a>
  <a href="https://arxiv.org/pdf/2510.07316.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07316v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.07316v2', 'Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08 (æ›´æ–°: 2025-10-29)

**å¤‡æ³¨**: NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè¯­ä¹‰æç¤ºæ‰©æ•£Transformerçš„åƒç´ çº§å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡ç‚¹äº‘ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `æ‰©æ•£æ¨¡å‹` `Transformer` `è¯­ä¹‰æç¤º` `ç‚¹äº‘ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç”Ÿæˆå¼æ·±åº¦ä¼°è®¡æ¨¡å‹ä¾èµ–VAEå‹ç¼©æ·±åº¦å›¾ï¼Œå¯¼è‡´è¾¹ç¼˜å‡ºç°æ‚¬æµ®åƒç´ ç­‰ä¼ªå½±ã€‚
2. è®ºæ–‡æå‡ºåœ¨åƒç´ ç©ºé—´ç›´æ¥è¿›è¡Œæ‰©æ•£ç”Ÿæˆï¼Œé¿å…VAEå¼•å…¥çš„ä¼ªå½±ï¼Œå¹¶è®¾è®¡è¯­ä¹‰æç¤ºæ‰©æ•£Transformerã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶åœ¨è¾¹ç¼˜æ„ŸçŸ¥ç‚¹äº‘è¯„ä¼°ä¸­ä¼˜åŠ¿æ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPixel-Perfect Depthçš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºåƒç´ ç©ºé—´æ‰©æ•£ç”Ÿæˆï¼Œèƒ½å¤Ÿä»ä¼°è®¡çš„æ·±åº¦å›¾ä¸­ç”Ÿæˆé«˜è´¨é‡ã€æ— æ‚¬æµ®åƒç´ çš„ç‚¹äº‘ã€‚ç°æœ‰çš„ç”Ÿæˆå¼æ·±åº¦ä¼°è®¡æ¨¡å‹é€šå¸¸å¾®è°ƒStable Diffusionï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬éœ€è¦ä½¿ç”¨VAEå°†æ·±åº¦å›¾å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œè¿™ä¸å¯é¿å…åœ°åœ¨è¾¹ç¼˜å’Œç»†èŠ‚å¤„å¼•å…¥â€œæ‚¬æµ®åƒç´ â€ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ç›´æ¥åœ¨åƒç´ ç©ºé—´ä¸­æ‰§è¡Œæ‰©æ•£ç”Ÿæˆæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œé¿å…äº†VAEå¼•å…¥çš„ä¼ªå½±ã€‚ä¸ºäº†å…‹æœä¸åƒç´ ç©ºé—´ç”Ÿæˆç›¸å…³çš„é«˜å¤æ‚åº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤é¡¹æ–°è®¾è®¡ï¼š1) è¯­ä¹‰æç¤ºæ‰©æ•£Transformer (SP-DiT)ï¼Œå®ƒå°†æ¥è‡ªè§†è§‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰è¡¨ç¤ºèå…¥DiTä¸­ï¼Œä»¥æç¤ºæ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œåœ¨å¢å¼ºç²¾ç»†è§†è§‰ç»†èŠ‚çš„åŒæ—¶ä¿æŒå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ï¼›2) çº§è”DiTè®¾è®¡ï¼Œå®ƒé€æ­¥å¢åŠ tokençš„æ•°é‡ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ‰€æœ‰å·²å‘è¡¨çš„ç”Ÿæˆæ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨è¾¹ç¼˜æ„ŸçŸ¥ç‚¹äº‘è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºæ‰€æœ‰å…¶ä»–æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„ç”Ÿæˆå¼å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹é€šå¸¸ä¾èµ–äºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†æ·±åº¦å›¾å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œç„¶åå†è¿›è¡Œæ‰©æ•£ç”Ÿæˆã€‚è¿™ç§æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†VAEçš„å‹ç¼©è¿‡ç¨‹ä¼šåœ¨æ·±åº¦å›¾çš„è¾¹ç¼˜å’Œç»†èŠ‚å¤„å¼•å…¥ä¼ªå½±ï¼Œè¡¨ç°ä¸ºâ€œæ‚¬æµ®åƒç´ â€ï¼Œå½±å“äº†ç‚¹äº‘çš„è´¨é‡ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸ä¾èµ–VAEçš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æ·±åº¦å›¾å’Œç‚¹äº‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨åƒç´ ç©ºé—´ç›´æ¥è¿›è¡Œæ‰©æ•£ç”Ÿæˆï¼Œé¿å…VAEå¼•å…¥çš„ä¼ªå½±ã€‚ä¸ºäº†å…‹æœåƒç´ ç©ºé—´ç”Ÿæˆå¸¦æ¥çš„é«˜è®¡ç®—å¤æ‚åº¦ï¼Œè®ºæ–‡æå‡ºäº†è¯­ä¹‰æç¤ºæ‰©æ•£Transformerï¼ˆSP-DiTï¼‰å’Œçº§è”DiTè®¾è®¡ã€‚SP-DiTåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰ä¿¡æ¯æ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œåœ¨ä¿æŒå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå¢å¼ºç²¾ç»†çš„è§†è§‰ç»†èŠ‚ã€‚çº§è”DiTè®¾è®¡åˆ™é€šè¿‡é€æ­¥å¢åŠ tokençš„æ•°é‡ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡å’Œç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è¯­ä¹‰ç¼–ç å™¨ï¼šåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æå–è¾“å…¥å›¾åƒçš„è¯­ä¹‰ç‰¹å¾ã€‚2) è¯­ä¹‰æç¤ºæ‰©æ•£Transformer (SP-DiT)ï¼šå°†è¯­ä¹‰ç‰¹å¾èå…¥åˆ°DiTä¸­ï¼Œå¼•å¯¼åƒç´ ç©ºé—´çš„æ‰©æ•£è¿‡ç¨‹ã€‚3) çº§è”DiTï¼šé€šè¿‡é€æ­¥å¢åŠ tokenæ•°é‡ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡å’Œç²¾åº¦ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆè¾“å…¥å•å¼ å›¾åƒï¼Œé€šè¿‡è¯­ä¹‰ç¼–ç å™¨æå–è¯­ä¹‰ç‰¹å¾ï¼Œç„¶åå°†è¯­ä¹‰ç‰¹å¾ä½œä¸ºæç¤ºè¾“å…¥åˆ°SP-DiTä¸­ï¼ŒSP-DiTåœ¨åƒç´ ç©ºé—´è¿›è¡Œæ‰©æ•£ç”Ÿæˆï¼Œå¹¶é€šè¿‡çº§è”DiTé€æ­¥æé«˜ç”Ÿæˆè´¨é‡ï¼Œæœ€ç»ˆè¾“å‡ºé«˜è´¨é‡çš„æ·±åº¦å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºï¼š1) åœ¨åƒç´ ç©ºé—´ç›´æ¥è¿›è¡Œæ‰©æ•£ç”Ÿæˆï¼Œé¿å…äº†VAEå¼•å…¥çš„ä¼ªå½±ï¼›2) æå‡ºäº†è¯­ä¹‰æç¤ºæ‰©æ•£Transformer (SP-DiT)ï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰ä¿¡æ¯æ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œè¯¥æ–¹æ³•ä¸å†ä¾èµ–VAEè¿›è¡Œæ·±åº¦å›¾çš„å‹ç¼©å’Œè§£å‹ç¼©ï¼Œè€Œæ˜¯ç›´æ¥åœ¨åƒç´ ç©ºé—´è¿›è¡Œç”Ÿæˆï¼Œä»è€Œé¿å…äº†ä¿¡æ¯æŸå¤±å’Œä¼ªå½±çš„äº§ç”Ÿã€‚

**å…³é”®è®¾è®¡**ï¼šSP-DiTçš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•å°†è¯­ä¹‰ä¿¡æ¯æœ‰æ•ˆåœ°èå…¥åˆ°DiTä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡å°†è§†è§‰åŸºç¡€æ¨¡å‹æå–çš„è¯­ä¹‰ç‰¹å¾ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°DiTçš„Transformerå—ä¸­ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å°†è¯­ä¹‰ä¿¡æ¯ä¸åƒç´ ç‰¹å¾è¿›è¡Œèåˆã€‚çº§è”DiTçš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•é€æ­¥å¢åŠ tokençš„æ•°é‡ï¼Œä»¥æé«˜ç”Ÿæˆæ•ˆç‡å’Œç²¾åº¦ã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§é‡‘å­—å¡”å¼çš„ç»“æ„ï¼Œä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡é€æ­¥å¢åŠ tokenæ•°é‡ï¼Œä»è€Œåœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ¨¡å‹åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ‰€æœ‰å·²å‘è¡¨çš„ç”Ÿæˆæ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨è¾¹ç¼˜æ„ŸçŸ¥ç‚¹äº‘è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºæ‰€æœ‰å…¶ä»–æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆçš„ç‚¹äº‘åœ¨è¾¹ç¼˜åŒºåŸŸçš„ç²¾åº¦å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œæœ‰æ•ˆå‡å°‘äº†æ‚¬æµ®åƒç´ çš„æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ·±åº¦ä¼°è®¡çš„ç²¾åº¦å’Œç‚¹äº‘è´¨é‡æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€ä¸‰ç»´é‡å»ºã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é«˜è´¨é‡çš„æ·±åº¦ä¼°è®¡å¯¹äºè¿™äº›åº”ç”¨è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®æ„ŸçŸ¥ç¯å¢ƒå‡ ä½•ç»“æ„çš„åœºæ™¯ä¸‹ã€‚è¯¥æ¨¡å‹ç”Ÿæˆçš„æ— æ‚¬æµ®åƒç´ ç‚¹äº‘å¯ä»¥æé«˜ä¸‰ç»´é‡å»ºçš„ç²¾åº¦ï¼Œä»è€Œæå‡ç›¸å…³åº”ç”¨çš„æ€§èƒ½å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ¨åŠ¨è¿™äº›é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.

