---
layout: default
title: MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis
---

# MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07190" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07190v1</a>
  <a href="https://arxiv.org/pdf/2510.07190.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07190v1" onclick="toggleFavorite(this, '2510.07190v1', 'MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yihao Zhi, Chenghong Li, Hongjie Liao, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Xiaodong Cun, Wensen Feng, Xiaoguang Han

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

**å¤‡æ³¨**: Accepted by SIGGRAPH Asia 2025 conference track

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MV-Performerï¼šæå‡ºä¸€ç§ç”¨äºç”Ÿæˆé€¼çœŸåŒæ­¥å¤šè§†è§’è¡¨æ¼”è€…è§†é¢‘çš„æ‰©æ•£æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¤šè§†è§’è§†é¢‘ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `æ–°è§†è§’åˆæˆ` `äººä½“å»ºæ¨¡` `è§†é¢‘åŒæ­¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•éš¾ä»¥ç”Ÿæˆ360åº¦è§†è§’å˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ï¼Œå­˜åœ¨è§†è§’åˆ‡æ¢æ—¶çš„åŒæ­¥æ€§é—®é¢˜ã€‚
2. MV-Performeråˆ©ç”¨å¤šè§†è§’æ•°æ®é›†MVHumanNetï¼Œå¹¶å¼•å…¥ç›¸æœºç›¸å…³çš„æ³•çº¿è´´å›¾ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œç¼“è§£è§†è§’æ¨¡ç³Šæ€§ã€‚
3. é€šè¿‡å¤šè§†è§’æ‰©æ•£æ¨¡å‹èåˆå‚è€ƒè§†é¢‘ã€éƒ¨åˆ†æ¸²æŸ“å’Œä¸åŒè§†ç‚¹ä¿¡æ¯ï¼Œå¹¶è®¾è®¡é²æ£’çš„æ¨ç†è¿‡ç¨‹ï¼Œæå‡ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’ŒåŒæ­¥æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMV-Performerï¼Œä¸€ä¸ªç”¨äºä»å•ç›®å…¨èº«æ•æ‰ç”ŸæˆåŒæ­¥æ–°è§†è§’è§†é¢‘çš„åˆ›æ–°æ¡†æ¶ï¼Œä¸“æ³¨äºä»¥äººä¸ºä¸­å¿ƒçš„å­é¢†åŸŸã€‚ä¸ºäº†å®ç°360åº¦åˆæˆï¼Œå……åˆ†åˆ©ç”¨MVHumanNetæ•°æ®é›†å¹¶ç»“åˆä¿¡æ¯ä¸°å¯Œçš„æ¡ä»¶ä¿¡å·ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ä»å®šå‘éƒ¨åˆ†ç‚¹äº‘æ¸²æŸ“çš„ç›¸æœºç›¸å…³æ³•çº¿è´´å›¾ï¼Œæœ‰æ•ˆç¼“è§£å¯è§å’Œä¸å¯è§è§‚æµ‹ä¹‹é—´çš„æ¨¡ç³Šæ€§ã€‚ä¸ºäº†ä¿æŒç”Ÿæˆè§†é¢‘çš„åŒæ­¥æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„å¤šè§†è§’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèåˆäº†æ¥è‡ªå‚è€ƒè§†é¢‘ã€éƒ¨åˆ†æ¸²æŸ“å’Œä¸åŒè§†ç‚¹çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªé²æ£’çš„æ¨ç†è¿‡ç¨‹ï¼Œç”¨äºå¤„ç†çœŸå®åœºæ™¯ä¸­çš„è§†é¢‘ï¼Œå¤§å¤§å‡è½»äº†ç”±ä¸å®Œç¾çš„å•ç›®æ·±åº¦ä¼°è®¡å¼•èµ·çš„ä¼ªå½±ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†MV-Performerçš„å…ˆè¿›æ€§å’Œé²æ£’æ€§ï¼Œä¸ºä»¥äººä¸ºä¸­å¿ƒçš„4Dæ–°è§†è§’åˆæˆå»ºç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ­£é¢è§†è§’çš„ç›¸æœºè½¨è¿¹æ§åˆ¶ï¼Œéš¾ä»¥ç”Ÿæˆ360åº¦è§†è§’å˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­ï¼Œå•ç›®æ·±åº¦ä¼°è®¡çš„ä¸å‡†ç¡®æ€§ä¼šå¯¼è‡´ä¼ªå½±ï¼Œä¸”ä¸åŒè§†è§’è§†é¢‘çš„åŒæ­¥æ€§éš¾ä»¥ä¿è¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç»“åˆå¤šè§†è§’æ•°æ®é›†çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å¼•å…¥ç›¸æœºç›¸å…³çš„æ³•çº¿è´´å›¾ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œæ¥ç¼“è§£è§†è§’æ¨¡ç³Šæ€§ï¼Œå¹¶è®¾è®¡å¤šè§†è§’æ‰©æ•£æ¨¡å‹æ¥ä¿è¯ç”Ÿæˆè§†é¢‘çš„åŒæ­¥æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMV-Performeræ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä½¿ç”¨å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ä»è¾“å…¥è§†é¢‘ä¸­æå–æ·±åº¦ä¿¡æ¯ï¼›2) ä»æ·±åº¦ä¿¡æ¯æ¸²æŸ“ç›¸æœºç›¸å…³çš„æ³•çº¿è´´å›¾ï¼›3) å°†å‚è€ƒè§†é¢‘ã€æ³•çº¿è´´å›¾å’Œç›®æ ‡è§†è§’ä¿¡æ¯è¾“å…¥åˆ°å¤šè§†è§’æ‰©æ•£æ¨¡å‹ä¸­ï¼›4) æ‰©æ•£æ¨¡å‹ç”Ÿæˆç›®æ ‡è§†è§’çš„è§†é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼š1) æå‡ºä½¿ç”¨ç›¸æœºç›¸å…³çš„æ³•çº¿è´´å›¾ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œæœ‰æ•ˆç¼“è§£äº†å¯è§å’Œä¸å¯è§è§‚æµ‹ä¹‹é—´çš„æ¨¡ç³Šæ€§ï¼Œæå‡äº†360åº¦è§†è§’åˆæˆçš„è´¨é‡ï¼›2) è®¾è®¡äº†å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡èåˆæ¥è‡ªå‚è€ƒè§†é¢‘ã€éƒ¨åˆ†æ¸²æŸ“å’Œä¸åŒè§†ç‚¹çš„ä¿¡æ¯ï¼Œä¿è¯äº†ç”Ÿæˆè§†é¢‘çš„åŒæ­¥æ€§ï¼›3) æå‡ºäº†ä¸€ä¸ªé²æ£’çš„æ¨ç†è¿‡ç¨‹ï¼Œç”¨äºå¤„ç†çœŸå®åœºæ™¯ä¸­çš„è§†é¢‘ï¼Œå‡è½»äº†ç”±ä¸å®Œç¾çš„å•ç›®æ·±åº¦ä¼°è®¡å¼•èµ·çš„ä¼ªå½±ã€‚

**å…³é”®è®¾è®¡**ï¼šå¤šè§†è§’æ‰©æ•£æ¨¡å‹é‡‡ç”¨U-Netç»“æ„ï¼Œè¾“å…¥åŒ…æ‹¬å‚è€ƒè§†é¢‘å¸§ã€æ³•çº¿è´´å›¾å’Œç›®æ ‡è§†è§’ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬L1æŸå¤±å’Œæ„ŸçŸ¥æŸå¤±ï¼Œç”¨äºæå‡ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨è¿­ä»£ç»†åŒ–çš„æ–¹å¼ï¼Œé€æ­¥ç”Ÿæˆç›®æ ‡è§†è§’çš„è§†é¢‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMV-Performeråœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†state-of-the-artçš„æ•ˆæœã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMV-Performerç”Ÿæˆçš„è§†é¢‘åœ¨è§†è§’ä¸€è‡´æ€§ã€äººç‰©é€¼çœŸåº¦å’Œè§†é¢‘åŒæ­¥æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚æ¶ˆèå®éªŒéªŒè¯äº†ç›¸æœºç›¸å…³æ³•çº¿è´´å›¾å’Œå¤šè§†è§’æ‰©æ•£æ¨¡å‹è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MV-Performeråœ¨è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€æ¸¸æˆå¼€å‘å’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„360åº¦äººç‰©è¡¨æ¼”è§†é¢‘ï¼Œä¸ºç”¨æˆ·æä¾›æ²‰æµ¸å¼çš„ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºåˆ›å»ºè™šæ‹ŸåŒ–èº«ã€è¿›è¡ŒåŠ¨ä½œæ•æ‰å’Œè¿›è¡Œè§†é¢‘ç¼–è¾‘ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent breakthroughs in video generation, powered by large-scale datasets and diffusion techniques, have shown that video diffusion models can function as implicit 4D novel view synthesizers. Nevertheless, current methods primarily concentrate on redirecting camera trajectory within the front view while struggling to generate 360-degree viewpoint changes. In this paper, we focus on human-centric subdomain and present MV-Performer, an innovative framework for creating synchronized novel view videos from monocular full-body captures. To achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset and incorporate an informative condition signal. Specifically, we use the camera-dependent normal maps rendered from oriented partial point clouds, which effectively alleviate the ambiguity between seen and unseen observations. To maintain synchronization in the generated videos, we propose a multi-view human-centric video diffusion model that fuses information from the reference video, partial rendering, and different viewpoints. Additionally, we provide a robust inference procedure for in-the-wild video cases, which greatly mitigates the artifacts induced by imperfect monocular depth estimation. Extensive experiments on three datasets demonstrate our MV-Performer's state-of-the-art effectiveness and robustness, setting a strong model for human-centric 4D novel view synthesis.

