---
layout: default
title: MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis
---

# MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.07190" target="_blank" class="toolbar-btn">arXiv: 2510.07190v1</a>
    <a href="https://arxiv.org/pdf/2510.07190.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07190v1" 
            onclick="toggleFavorite(this, '2510.07190v1', 'MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yihao Zhi, Chenghong Li, Hongjie Liao, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Xiaodong Cun, Wensen Feng, Xiaoguang Han

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-08

**Â§áÊ≥®**: Accepted by SIGGRAPH Asia 2025 conference track

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**MV-PerformerÔºöÊèêÂá∫‰∏ÄÁßçÁî®‰∫éÁîüÊàêÈÄºÁúüÂêåÊ≠•Â§öËßÜËßíË°®ÊºîËÄÖËßÜÈ¢ëÁöÑÊâ©Êï£Ê®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `Â§öËßÜËßíËßÜÈ¢ëÁîüÊàê` `Êâ©Êï£Ê®°Âûã` `Êñ∞ËßÜËßíÂêàÊàê` `‰∫∫‰ΩìÂª∫Ê®°` `ËßÜÈ¢ëÂêåÊ≠•`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÁîüÊàêÊñπÊ≥ïÈöæ‰ª•ÁîüÊàê360Â∫¶ËßÜËßíÂèòÂåñÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑÂú∫ÊôØ‰∏≠ÔºåÂ≠òÂú®ËßÜËßíÂàáÊç¢Êó∂ÁöÑÂêåÊ≠•ÊÄßÈóÆÈ¢ò„ÄÇ
2. MV-PerformerÂà©Áî®Â§öËßÜËßíÊï∞ÊçÆÈõÜMVHumanNetÔºåÂπ∂ÂºïÂÖ•Áõ∏Êú∫Áõ∏ÂÖ≥ÁöÑÊ≥ïÁ∫øË¥¥Âõæ‰Ωú‰∏∫Êù°‰ª∂‰ø°Âè∑ÔºåÁºìËß£ËßÜËßíÊ®°Á≥äÊÄß„ÄÇ
3. ÈÄöËøáÂ§öËßÜËßíÊâ©Êï£Ê®°ÂûãËûçÂêàÂèÇËÄÉËßÜÈ¢ë„ÄÅÈÉ®ÂàÜÊ∏≤ÊüìÂíå‰∏çÂêåËßÜÁÇπ‰ø°ÊÅØÔºåÂπ∂ËÆæËÆ°È≤ÅÊ£íÁöÑÊé®ÁêÜËøáÁ®ãÔºåÊèêÂçáÁîüÊàêËßÜÈ¢ëÁöÑË¥®ÈáèÂíåÂêåÊ≠•ÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫MV-PerformerÔºå‰∏Ä‰∏™Áî®‰∫é‰ªéÂçïÁõÆÂÖ®Ë∫´ÊçïÊçâÁîüÊàêÂêåÊ≠•Êñ∞ËßÜËßíËßÜÈ¢ëÁöÑÂàõÊñ∞Ê°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫é‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑÂ≠êÈ¢ÜÂüü„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞360Â∫¶ÂêàÊàêÔºåÂÖÖÂàÜÂà©Áî®MVHumanNetÊï∞ÊçÆÈõÜÂπ∂ÁªìÂêà‰ø°ÊÅØ‰∏∞ÂØåÁöÑÊù°‰ª∂‰ø°Âè∑„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩøÁî®‰ªéÂÆöÂêëÈÉ®ÂàÜÁÇπ‰∫ëÊ∏≤ÊüìÁöÑÁõ∏Êú∫Áõ∏ÂÖ≥Ê≥ïÁ∫øË¥¥ÂõæÔºåÊúâÊïàÁºìËß£ÂèØËßÅÂíå‰∏çÂèØËßÅËßÇÊµã‰πãÈó¥ÁöÑÊ®°Á≥äÊÄß„ÄÇ‰∏∫‰∫Ü‰øùÊåÅÁîüÊàêËßÜÈ¢ëÁöÑÂêåÊ≠•ÊÄßÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑÂ§öËßÜËßíËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºåËûçÂêà‰∫ÜÊù•Ëá™ÂèÇËÄÉËßÜÈ¢ë„ÄÅÈÉ®ÂàÜÊ∏≤ÊüìÂíå‰∏çÂêåËßÜÁÇπÁöÑ‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™È≤ÅÊ£íÁöÑÊé®ÁêÜËøáÁ®ãÔºåÁî®‰∫éÂ§ÑÁêÜÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑËßÜÈ¢ëÔºåÂ§ßÂ§ßÂáèËΩª‰∫ÜÁî±‰∏çÂÆåÁæéÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÂºïËµ∑ÁöÑ‰º™ÂΩ±„ÄÇÂú®‰∏â‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åËØÅÊòé‰∫ÜMV-PerformerÁöÑÂÖàËøõÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºå‰∏∫‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑ4DÊñ∞ËßÜËßíÂêàÊàêÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Âº∫Â§ßÁöÑÊ®°Âûã„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜÈ¢ëÁîüÊàêÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê≠£Èù¢ËßÜËßíÁöÑÁõ∏Êú∫ËΩ®ËøπÊéßÂà∂ÔºåÈöæ‰ª•ÁîüÊàê360Â∫¶ËßÜËßíÂèòÂåñÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑÂú∫ÊôØ‰∏≠ÔºåÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑ‰∏çÂáÜÁ°ÆÊÄß‰ºöÂØºËá¥‰º™ÂΩ±Ôºå‰∏î‰∏çÂêåËßÜËßíËßÜÈ¢ëÁöÑÂêåÊ≠•ÊÄßÈöæ‰ª•‰øùËØÅ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂà©Áî®Êâ©Êï£Ê®°ÂûãÂº∫Â§ßÁöÑÁîüÊàêËÉΩÂäõÔºåÁªìÂêàÂ§öËßÜËßíÊï∞ÊçÆÈõÜÁöÑ‰ºòÂäøÔºåÈÄöËøáÂºïÂÖ•Áõ∏Êú∫Áõ∏ÂÖ≥ÁöÑÊ≥ïÁ∫øË¥¥Âõæ‰Ωú‰∏∫Êù°‰ª∂‰ø°Âè∑ÔºåÊù•ÁºìËß£ËßÜËßíÊ®°Á≥äÊÄßÔºåÂπ∂ËÆæËÆ°Â§öËßÜËßíÊâ©Êï£Ê®°ÂûãÊù•‰øùËØÅÁîüÊàêËßÜÈ¢ëÁöÑÂêåÊ≠•ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMV-PerformerÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ‰ΩøÁî®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ï‰ªéËæìÂÖ•ËßÜÈ¢ë‰∏≠ÊèêÂèñÊ∑±Â∫¶‰ø°ÊÅØÔºõ2) ‰ªéÊ∑±Â∫¶‰ø°ÊÅØÊ∏≤ÊüìÁõ∏Êú∫Áõ∏ÂÖ≥ÁöÑÊ≥ïÁ∫øË¥¥ÂõæÔºõ3) Â∞ÜÂèÇËÄÉËßÜÈ¢ë„ÄÅÊ≥ïÁ∫øË¥¥ÂõæÂíåÁõÆÊ†áËßÜËßí‰ø°ÊÅØËæìÂÖ•Âà∞Â§öËßÜËßíÊâ©Êï£Ê®°Âûã‰∏≠Ôºõ4) Êâ©Êï£Ê®°ÂûãÁîüÊàêÁõÆÊ†áËßÜËßíÁöÑËßÜÈ¢ë„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**Ôºö1) ÊèêÂá∫‰ΩøÁî®Áõ∏Êú∫Áõ∏ÂÖ≥ÁöÑÊ≥ïÁ∫øË¥¥Âõæ‰Ωú‰∏∫Êù°‰ª∂‰ø°Âè∑ÔºåÊúâÊïàÁºìËß£‰∫ÜÂèØËßÅÂíå‰∏çÂèØËßÅËßÇÊµã‰πãÈó¥ÁöÑÊ®°Á≥äÊÄßÔºåÊèêÂçá‰∫Ü360Â∫¶ËßÜËßíÂêàÊàêÁöÑË¥®ÈáèÔºõ2) ËÆæËÆ°‰∫ÜÂ§öËßÜËßíÊâ©Êï£Ê®°ÂûãÔºåÈÄöËøáËûçÂêàÊù•Ëá™ÂèÇËÄÉËßÜÈ¢ë„ÄÅÈÉ®ÂàÜÊ∏≤ÊüìÂíå‰∏çÂêåËßÜÁÇπÁöÑ‰ø°ÊÅØÔºå‰øùËØÅ‰∫ÜÁîüÊàêËßÜÈ¢ëÁöÑÂêåÊ≠•ÊÄßÔºõ3) ÊèêÂá∫‰∫Ü‰∏Ä‰∏™È≤ÅÊ£íÁöÑÊé®ÁêÜËøáÁ®ãÔºåÁî®‰∫éÂ§ÑÁêÜÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑËßÜÈ¢ëÔºåÂáèËΩª‰∫ÜÁî±‰∏çÂÆåÁæéÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÂºïËµ∑ÁöÑ‰º™ÂΩ±„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ§öËßÜËßíÊâ©Êï£Ê®°ÂûãÈááÁî®U-NetÁªìÊûÑÔºåËæìÂÖ•ÂåÖÊã¨ÂèÇËÄÉËßÜÈ¢ëÂ∏ß„ÄÅÊ≥ïÁ∫øË¥¥ÂõæÂíåÁõÆÊ†áËßÜËßí‰ø°ÊÅØ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨L1ÊçüÂ§±ÂíåÊÑüÁü•ÊçüÂ§±ÔºåÁî®‰∫éÊèêÂçáÁîüÊàêËßÜÈ¢ëÁöÑË¥®Èáè„ÄÇÊé®ÁêÜËøáÁ®ã‰∏≠ÔºåÈááÁî®Ëø≠‰ª£ÁªÜÂåñÁöÑÊñπÂºèÔºåÈÄêÊ≠•ÁîüÊàêÁõÆÊ†áËßÜËßíÁöÑËßÜÈ¢ë„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMV-PerformerÂú®‰∏â‰∏™Êï∞ÊçÆÈõÜ‰∏äÈÉΩÂèñÂæó‰∫Üstate-of-the-artÁöÑÊïàÊûú„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåMV-PerformerÁîüÊàêÁöÑËßÜÈ¢ëÂú®ËßÜËßí‰∏ÄËá¥ÊÄß„ÄÅ‰∫∫Áâ©ÈÄºÁúüÂ∫¶ÂíåËßÜÈ¢ëÂêåÊ≠•ÊÄßÊñπÈù¢ÈÉΩÊúâÊòæËëóÊèêÂçá„ÄÇÊ∂àËûçÂÆûÈ™åÈ™åËØÅ‰∫ÜÁõ∏Êú∫Áõ∏ÂÖ≥Ê≥ïÁ∫øË¥¥ÂõæÂíåÂ§öËßÜËßíÊâ©Êï£Ê®°ÂûãËÆæËÆ°ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MV-PerformerÂú®ËôöÊãüÁé∞ÂÆû„ÄÅÂ¢ûÂº∫Áé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèëÂíåÁîµÂΩ±Âà∂‰ΩúÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÁîüÊàêÈÄºÁúüÁöÑ360Â∫¶‰∫∫Áâ©Ë°®ÊºîËßÜÈ¢ëÔºå‰∏∫Áî®Êà∑Êèê‰æõÊ≤âÊµ∏ÂºèÁöÑ‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Áî®‰∫éÂàõÂª∫ËôöÊãüÂåñË∫´„ÄÅËøõË°åÂä®‰ΩúÊçïÊçâÂíåËøõË°åËßÜÈ¢ëÁºñËæëÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent breakthroughs in video generation, powered by large-scale datasets and diffusion techniques, have shown that video diffusion models can function as implicit 4D novel view synthesizers. Nevertheless, current methods primarily concentrate on redirecting camera trajectory within the front view while struggling to generate 360-degree viewpoint changes. In this paper, we focus on human-centric subdomain and present MV-Performer, an innovative framework for creating synchronized novel view videos from monocular full-body captures. To achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset and incorporate an informative condition signal. Specifically, we use the camera-dependent normal maps rendered from oriented partial point clouds, which effectively alleviate the ambiguity between seen and unseen observations. To maintain synchronization in the generated videos, we propose a multi-view human-centric video diffusion model that fuses information from the reference video, partial rendering, and different viewpoints. Additionally, we provide a robust inference procedure for in-the-wild video cases, which greatly mitigates the artifacts induced by imperfect monocular depth estimation. Extensive experiments on three datasets demonstrate our MV-Performer's state-of-the-art effectiveness and robustness, setting a strong model for human-centric 4D novel view synthesis.

