---
layout: default
title: Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking
---

# Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06820" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.06820v1</a>
  <a href="https://arxiv.org/pdf/2510.06820.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06820v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.06820v1', 'Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mitchell Keren Taraday, Shahaf Wagner, Chaim Baskin

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

**å¤‡æ³¨**: preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEDJEï¼šä¸€ç§é«˜æ•ˆåˆ¤åˆ«å¼è”åˆç¼–ç å™¨ï¼Œç”¨äºå¤§è§„æ¨¡è§†è§‰-è¯­è¨€é‡æ’åºã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ£€ç´¢` `è”åˆç¼–ç å™¨` `é‡æ’åº` `å¤šæ¨¡æ€å­¦ä¹ ` `é«˜æ•ˆæ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ£€ç´¢ä¾èµ–åµŒå…¥æ¨¡å‹ï¼Œä½†è”åˆç¼–ç é‡æ’åºå™¨å› è§†è§‰ç‰¹å¾æå–è®¡ç®—é‡å¤§è€Œå—é™ã€‚
2. EDJEé€šè¿‡ç¦»çº¿é¢„è®¡ç®—å’Œå‹ç¼©è§†è§‰tokensï¼Œæ˜¾è‘—é™ä½åœ¨çº¿æ¨ç†çš„è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚
3. EDJEåœ¨ä¿æŒæ£€ç´¢æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†é«˜ååé‡ï¼Œå¹¶åœ¨Flickrå’ŒCOCOæ•°æ®é›†ä¸Šå–å¾—äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„ç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ£€ç´¢ä»ç„¶ä¾èµ–äºåŸºäºåµŒå…¥çš„æ¨¡å‹ï¼Œå¦‚CLIPï¼Œä»¥ä¾¿å¯¹é¢„å…ˆè®¡ç®—çš„å›¾åƒåµŒå…¥è¿›è¡Œå¿«é€Ÿå‘é‡æœç´¢ã€‚ç„¶è€Œï¼Œä¸æ–‡æœ¬æ£€ç´¢ä¸­è”åˆç¼–ç é‡æ’åºå™¨å·²æˆä¸ºæ ‡å‡†ä¸åŒï¼Œç±»ä¼¼çš„è§†è§‰-è¯­è¨€é‡æ’åºå™¨åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç¼ºå¤±çš„ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¯¸å¦‚BLIPç­‰å…·æœ‰å¼€åˆ›æ€§çš„è”åˆç¼–ç å™¨å—åˆ°æ˜‚è´µçš„è§†è§‰ç‰¹å¾æå–é˜¶æ®µçš„ä¸¥é‡ç“¶é¢ˆï¼Œä»è€Œé˜»ç¢äº†å¤§è§„æ¨¡çš„å®é™…éƒ¨ç½²ã€‚å—æ­¤ç“¶é¢ˆçš„é©±åŠ¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†EDJEï¼Œä¸€ç§é«˜æ•ˆåˆ¤åˆ«å¼è”åˆç¼–ç å™¨ï¼Œå®ƒå¯ä»¥ç¦»çº¿é¢„è®¡ç®—è§†è§‰tokensï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„åŸºäºæ³¨æ„åŠ›çš„é€‚é…å™¨å‹ç¼©å®ƒä»¬ï¼Œå› æ­¤åœ¨çº¿æ¨ç†ä»…åœ¨å°‘é‡è§†è§‰tokenså’Œæ–‡æœ¬ä¸Šè¿è¡Œç´§å‡‘çš„è”åˆç¼–ç å™¨ã€‚EDJEåœ¨æ˜¾è‘—é™ä½å­˜å‚¨å’Œåœ¨çº¿è®¡ç®—çš„åŒæ—¶ï¼Œä¿æŒäº†å¼ºå¤§çš„æ£€ç´¢æ€§èƒ½ï¼Œä»è€Œå®ç°äº†é«˜ååé‡çš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒEDJEå¤„ç†5ä¸‡ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹/ç§’ï¼ŒåŒæ—¶æ¯ä¸ªå›¾åƒéœ€è¦49kBçš„ç£ç›˜å­˜å‚¨ç©ºé—´ï¼Œä¸Flickrï¼ˆé›¶æ ·æœ¬ï¼‰å’ŒCOCOï¼ˆå¾®è°ƒï¼‰æ£€ç´¢æ–¹é¢çš„ç°æœ‰æŠ€æœ¯ç›¸åŒ¹é…ã€‚è¯¥å®ç°å’Œæ£€æŸ¥ç‚¹å°†å¾ˆå¿«å…¬å¼€æä¾›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ£€ç´¢ä¸­ï¼Œè”åˆç¼–ç é‡æ’åºå™¨å› è§†è§‰ç‰¹å¾æå–è®¡ç®—é‡å¤§è€Œéš¾ä»¥å®é™…éƒ¨ç½²çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚BLIPï¼Œè™½ç„¶æ€§èƒ½ä¼˜å¼‚ï¼Œä½†å…¶è§†è§‰ç‰¹å¾æå–é˜¶æ®µçš„è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œå¯¼è‡´æ— æ³•æ»¡è¶³å¤§è§„æ¨¡æ£€ç´¢å¯¹é«˜ååé‡å’Œä½å»¶è¿Ÿçš„è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é¢„è®¡ç®—å¹¶å‹ç¼©è§†è§‰ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—é™ä½åœ¨çº¿æ¨ç†é˜¶æ®µçš„è®¡ç®—è´Ÿæ‹…ã€‚å…·ä½“æ¥è¯´ï¼ŒEDJEé¦–å…ˆç¦»çº¿æå–å›¾åƒçš„è§†è§‰tokensï¼Œç„¶åä½¿ç”¨è½»é‡çº§çš„æ³¨æ„åŠ›æœºåˆ¶å¯¹è¿™äº›tokensè¿›è¡Œå‹ç¼©ï¼Œå¾—åˆ°æ›´ç´§å‡‘çš„è§†è§‰è¡¨ç¤ºã€‚åœ¨çº¿æ¨ç†æ—¶ï¼ŒEDJEä»…éœ€å¤„ç†å‹ç¼©åçš„è§†è§‰tokenså’Œæ–‡æœ¬ï¼Œä»è€Œå¤§å¤§æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEDJEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ç¦»çº¿è§†è§‰ç‰¹å¾æå–ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼ˆå¦‚ViTï¼‰æå–å›¾åƒçš„è§†è§‰tokensã€‚2) è§†è§‰tokenså‹ç¼©ï¼šä½¿ç”¨è½»é‡çº§çš„åŸºäºæ³¨æ„åŠ›çš„é€‚é…å™¨å¯¹è§†è§‰tokensè¿›è¡Œå‹ç¼©ï¼Œå¾—åˆ°ç´§å‡‘çš„è§†è§‰è¡¨ç¤ºã€‚3) è”åˆç¼–ç ï¼šå°†å‹ç¼©åçš„è§†è§‰è¡¨ç¤ºå’Œæ–‡æœ¬è¾“å…¥åˆ°è”åˆç¼–ç å™¨ä¸­ï¼Œå¾—åˆ°å›¾åƒ-æ–‡æœ¬å¯¹çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚4) é‡æ’åºï¼šæ ¹æ®ç›¸ä¼¼åº¦å¾—åˆ†å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åºã€‚

**å…³é”®åˆ›æ–°**ï¼šEDJEçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„è§†è§‰ç‰¹å¾å‹ç¼©æ–¹æ³•ã€‚é€šè¿‡ç¦»çº¿é¢„è®¡ç®—å’Œè½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶ï¼ŒEDJEèƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒè¾ƒå¥½çš„æ£€ç´¢æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEDJEé¿å…äº†åœ¨çº¿è¿›è¡Œæ˜‚è´µçš„è§†è§‰ç‰¹å¾æå–ï¼Œä»è€Œå®ç°äº†é«˜ååé‡çš„æ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šEDJEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„ViTæ¨¡å‹æå–è§†è§‰tokensï¼Œä¿è¯äº†è§†è§‰ç‰¹å¾çš„è´¨é‡ã€‚2) ä½¿ç”¨è½»é‡çº§çš„Transformerç»“æ„ä½œä¸ºæ³¨æ„åŠ›é€‚é…å™¨ï¼Œé™ä½äº†å‹ç¼©è¿‡ç¨‹çš„è®¡ç®—æˆæœ¬ã€‚3) é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ç›®æ ‡å‡½æ•°è®­ç»ƒè”åˆç¼–ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ å›¾åƒ-æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦å…³ç³»ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å°†åœ¨è®ºæ–‡ä¸­è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EDJEåœ¨Flickrå’ŒCOCOæ•°æ®é›†ä¸Šå–å¾—äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„æ£€ç´¢æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼ŒEDJEèƒ½å¤Ÿä»¥5ä¸‡ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹/ç§’çš„é€Ÿåº¦è¿›è¡Œæ¨ç†ï¼Œå¹¶ä¸”æ¯ä¸ªå›¾åƒä»…éœ€49kBçš„ç£ç›˜å­˜å‚¨ç©ºé—´ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒEDJEæ˜¯ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§†è§‰-è¯­è¨€é‡æ’åºæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EDJEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€è§†è§‰é—®ç­”ã€å›¾åƒå­—å¹•ç”Ÿæˆç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿéƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚æœªæ¥ï¼ŒEDJEå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£å’Œè¯­éŸ³è¯†åˆ«ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal retrieval still leans on embedding-based models like CLIP for fast vector search over pre-computed image embeddings. Yet, unlike text retrieval, where joint-encoder rerankers are standard, comparable vision--language rerankers are largely absent. We find that seminal joint encoders such as BLIP are severely bottlenecked by an expensive visual feature-extraction stage, preventing practical deployment at scale. Motivated by this bottleneck, we introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes vision tokens offline and compresses them via a lightweight attention-based adapter, so online inference runs only a compact joint encoder over a small set of visual tokens plus the text. EDJE preserves strong retrieval performance while drastically reducing storage and online compute, enabling high-throughput inference. Specifically, EDJE processes 50k image--text pairs/second while requiring 49kB of disk storage per image, matching prior art on Flickr (zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints will be made publicly available shortly.

