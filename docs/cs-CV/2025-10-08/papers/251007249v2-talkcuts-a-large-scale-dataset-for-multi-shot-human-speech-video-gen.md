---
layout: default
title: TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation
---

# TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07249" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07249v2</a>
  <a href="https://arxiv.org/pdf/2510.07249.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07249v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.07249v2', 'TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaben Chen, Zixin Wang, Ailing Zeng, Yang Fu, Xueyang Yu, Siyuan Cen, Julian Tanke, Yihang Chen, Koichi Saito, Yuki Mitsufuji, Chuang Gan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08 (æ›´æ–°: 2025-10-13)

**å¤‡æ³¨**: Project page: https://talkcuts.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTalkCutså¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºå¤šé•œå¤´äººå£°è§†é¢‘ç”Ÿæˆç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šé•œå¤´è§†é¢‘ç”Ÿæˆ` `äººå£°è§†é¢‘ç”Ÿæˆ` `å¤§è§„æ¨¡æ•°æ®é›†` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯­è¨€æ¨¡å‹å¼•å¯¼` `è§†é¢‘åˆæˆ` `å§¿åŠ¿å¼•å¯¼` `éŸ³é¢‘é©±åŠ¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ•°æ®é›†ç¼ºä¹å¤šé•œå¤´å’Œå¤šæ ·åŒ–è§†è§’ï¼Œé™åˆ¶äº†å¤šé•œå¤´äººå£°è§†é¢‘ç”Ÿæˆçš„ç ”ç©¶ã€‚
2. æå‡ºTalkCutsæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡ã€å¤šé•œå¤´ã€å¤šè§†è§’çš„äººå£°è§†é¢‘ï¼Œå¹¶æä¾›è¯¦ç»†æ ‡æ³¨ã€‚
3. æ„å»ºLLMå¼•å¯¼çš„å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶Oratorï¼Œå®éªŒè¯æ˜åœ¨TalkCutsä¸Šè®­ç»ƒèƒ½æ˜¾è‘—æå‡è§†é¢‘è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†TalkCutsï¼Œä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›å¤šé•œå¤´äººå£°è§†é¢‘ç”Ÿæˆçš„ç ”ç©¶ã€‚ä¸ç°æœ‰ä¸“æ³¨äºå•é•œå¤´ã€é™æ€è§†è§’çš„æ•°æ®é›†ä¸åŒï¼ŒTalkCutsæä¾›è¶…è¿‡500å°æ—¶çš„é«˜è´¨é‡äººå£°è§†é¢‘ï¼ŒåŒ…å«16.4ä¸‡ä¸ªç‰‡æ®µï¼Œå…·æœ‰å¤šæ ·åŒ–çš„é•œå¤´ï¼ŒåŒ…æ‹¬ç‰¹å†™ã€åŠèº«å’Œå…¨èº«è§†å›¾ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬è¯¦ç»†çš„æ–‡æœ¬æè¿°ã€2Då…³é”®ç‚¹å’Œ3D SMPL-X è¿åŠ¨æ ‡æ³¨ï¼Œè¦†ç›–è¶…è¿‡1ä¸‡ä¸ªèº«ä»½ï¼Œæ”¯æŒå¤šæ¨¡æ€å­¦ä¹ å’Œè¯„ä¼°ã€‚ä½œä¸ºå±•ç¤ºæ•°æ®é›†ä»·å€¼çš„åˆæ­¥å°è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†Oratorï¼Œä¸€ä¸ªç”±LLMå¼•å¯¼çš„å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶ä½œä¸ºç®€å•åŸºçº¿ï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹å……å½“å¤šæ–¹é¢çš„å¯¼æ¼”ï¼Œåè°ƒç›¸æœºè¿‡æ¸¡ã€è¯´è¯è€…æ‰‹åŠ¿å’Œå£°éŸ³è°ƒåˆ¶ç­‰è¯¦ç»†è§„èŒƒã€‚è¯¥æ¶æ„é€šè¿‡æˆ‘ä»¬é›†æˆçš„å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆæ¨¡å—ï¼Œèƒ½å¤Ÿåˆæˆè¿è´¯çš„é•¿è§†é¢‘ã€‚åœ¨å§¿åŠ¿å¼•å¯¼å’ŒéŸ³é¢‘é©±åŠ¨çš„è®¾ç½®ä¸‹è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨TalkCutsä¸Šè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆçš„å¤šé•œå¤´è¯­éŸ³è§†é¢‘çš„ç”µå½±è¿è´¯æ€§å’Œè§†è§‰å¸å¼•åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡TalkCutsä¸ºå¯æ§çš„å¤šé•œå¤´è¯­éŸ³è§†é¢‘ç”Ÿæˆå’Œæ›´å¹¿æ³›çš„å¤šæ¨¡æ€å­¦ä¹ çš„æœªæ¥å·¥ä½œæä¾›äº†åšå®çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººå£°è§†é¢‘ç”Ÿæˆæ•°æ®é›†é€šå¸¸é›†ä¸­äºå•é•œå¤´å’Œé™æ€è§†è§’ï¼Œç¼ºä¹å¯¹å¤šé•œå¤´åˆ‡æ¢å’ŒåŠ¨æ€åœºæ™¯çš„å»ºæ¨¡èƒ½åŠ›ã€‚è¿™é™åˆ¶äº†ç”Ÿæˆæ›´å…·ç”µå½±æ„Ÿå’ŒçœŸå®æ„Ÿçš„äººå£°è§†é¢‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šé•œå¤´ã€å¤šè§†è§’çš„äººå£°è§†é¢‘æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå¯¼æ¼”ï¼ŒæŒ‡å¯¼å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶ï¼Œä»è€Œå®ç°å¯¹ç›¸æœºè¿‡æ¸¡ã€è¯´è¯è€…æ‰‹åŠ¿å’Œå£°éŸ³è°ƒåˆ¶çš„ç²¾ç»†æ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶OratoråŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) LLMå¯¼æ¼”æ¨¡å—ï¼šåˆ©ç”¨LLMç”Ÿæˆç›¸æœºè¿‡æ¸¡ã€æ‰‹åŠ¿å’Œå£°éŸ³è°ƒåˆ¶çš„æŒ‡ä»¤ã€‚2) å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆæ¨¡å—ï¼šæ ¹æ®LLMçš„æŒ‡ä»¤ï¼Œç»“åˆå§¿åŠ¿æˆ–éŸ³é¢‘ä¿¡æ¯ï¼Œç”Ÿæˆç›¸åº”çš„è§†é¢‘ç‰‡æ®µã€‚3) é›†æˆæ¨¡å—ï¼šå°†ç”Ÿæˆçš„è§†é¢‘ç‰‡æ®µæ‹¼æ¥æˆè¿è´¯çš„é•¿è§†é¢‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ„å»ºäº†å¤§è§„æ¨¡å¤šé•œå¤´äººå£°è§†é¢‘æ•°æ®é›†TalkCutsï¼Œä¸ºå¤šé•œå¤´è§†é¢‘ç”Ÿæˆæä¾›äº†æ•°æ®åŸºç¡€ã€‚2) æå‡ºäº†LLMå¼•å¯¼çš„å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶Oratorï¼Œåˆ©ç”¨LLMçš„å¼ºå¤§èƒ½åŠ›æ¥æ§åˆ¶è§†é¢‘çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå®ç°äº†æ›´ç²¾ç»†çš„æ§åˆ¶å’Œæ›´é«˜çš„ç”Ÿæˆè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šOratoræ¡†æ¶ä¸­ï¼ŒLLMè¢«è®¾è®¡ä¸ºâ€œå¯¼æ¼”â€ï¼Œè´Ÿè´£ç”Ÿæˆè¯¦ç»†çš„æŒ‡ä»¤ï¼ŒåŒ…æ‹¬ç›¸æœºè§’åº¦ã€äººç‰©å§¿åŠ¿ã€è¯­éŸ³è¯­è°ƒç­‰ã€‚è¿™äº›æŒ‡ä»¤è¢«ä¼ é€’ç»™å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®æŒ‡ä»¤ç”Ÿæˆç›¸åº”çš„è§†é¢‘ç‰‡æ®µã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨ä¿è¯ç”Ÿæˆè§†é¢‘çš„è¿è´¯æ€§å’ŒçœŸå®æ„Ÿï¼Œå…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨TalkCutsæ•°æ®é›†ä¸Šè®­ç»ƒçš„Oratoræ¡†æ¶ï¼Œåœ¨å§¿åŠ¿å¼•å¯¼å’ŒéŸ³é¢‘é©±åŠ¨çš„è®¾ç½®ä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ç”Ÿæˆçš„å¤šé•œå¤´è¯­éŸ³è§†é¢‘çš„ç”µå½±è¿è´¯æ€§å’Œè§†è§‰å¸å¼•åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†ç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ŒOratoråœ¨ä¸»è§‚è¯„ä»·ä¸Šè¡¨ç°æ›´ä½³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè™šæ‹Ÿä¸»æ’­ã€è¿œç¨‹ä¼šè®®ã€ç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚é€šè¿‡TalkCutsæ•°æ®é›†å’ŒOratoræ¡†æ¶ï¼Œå¯ä»¥ç”Ÿæˆæ›´å…·è¡¨ç°åŠ›å’ŒçœŸå®æ„Ÿçš„äººå£°è§†é¢‘ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œæ²Ÿé€šæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºä¸ªæ€§åŒ–è§†é¢‘ç”Ÿæˆã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we present TalkCuts, a large-scale dataset designed to facilitate the study of multi-shot human speech video generation. Unlike existing datasets that focus on single-shot, static viewpoints, TalkCuts offers 164k clips totaling over 500 hours of high-quality human speech videos with diverse camera shots, including close-up, half-body, and full-body views. The dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X motion annotations, covering over 10k identities, enabling multimodal learning and evaluation. As a first attempt to showcase the value of the dataset, we present Orator, an LLM-guided multi-modal generation framework as a simple baseline, where the language model functions as a multi-faceted director, orchestrating detailed specifications for camera transitions, speaker gesticulations, and vocal modulation. This architecture enables the synthesis of coherent long-form videos through our integrated multi-modal video generation module. Extensive experiments in both pose-guided and audio-driven settings show that training on TalkCuts significantly enhances the cinematographic coherence and visual appeal of generated multi-shot speech videos. We believe TalkCuts provides a strong foundation for future work in controllable, multi-shot speech video generation and broader multimodal learning.

