---
layout: default
title: Look before Transcription: End-to-End SlideASR with Visually-Anchored Policy Optimization
---

# Look before Transcription: End-to-End SlideASR with Visually-Anchored Policy Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08618" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08618v1</a>
  <a href="https://arxiv.org/pdf/2510.08618.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08618v1" onclick="toggleFavorite(this, '2510.08618v1', 'Look before Transcription: End-to-End SlideASR with Visually-Anchored Policy Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rui Hu, Delai Qiu, Yining Wang, Shengping Liu, Jitao Sang

**åˆ†ç±»**: eess.AS, cs.CV, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVAPOï¼Œé€šè¿‡è§†è§‰é”šå®šçš„ç­–ç•¥ä¼˜åŒ–ï¼Œæå‡SlideASRä¸­é¢†åŸŸæœ¯è¯­çš„è¯†åˆ«ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨è¯­éŸ³è¯†åˆ«` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰é”šå®š` `ç­–ç•¥ä¼˜åŒ–` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ASRç³»ç»Ÿåœ¨å¤„ç†å­¦æœ¯è®²åº§ç­‰ä¸“ä¸šåœºæ™¯ä¸­çš„é¢†åŸŸæœ¯è¯­æ—¶ï¼Œå‡†ç¡®ç‡è¾ƒä½ï¼Œä¼ ç»Ÿæ–¹æ³•å¤æ‚ä¸”æ•ˆæœä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºè§†è§‰é”šå®šçš„ç­–ç•¥ä¼˜åŒ–ï¼ˆVAPOï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹éµå¾ªâ€œå…ˆçœ‹åè½¬å½•â€çš„æµç¨‹ï¼Œæå‡è¯†åˆ«ç²¾åº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVAPOæ˜¾è‘—æé«˜äº†é¢†åŸŸç‰¹å®šæœ¯è¯­çš„è¯†åˆ«ç‡ï¼Œå¹¶åœ¨SlideASR-Benchæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­è¯†åˆ«æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å­¦æœ¯è®²åº§ç­‰ä¸“ä¸šåœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å®šä¹‰äº†SlideASRä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡åˆ©ç”¨æ¼”ç¤ºå¹»ç¯ç‰‡ä¸­çš„ä¸°å¯Œè§†è§‰ä¿¡æ¯æ¥æé«˜è½¬å½•å‡†ç¡®ç‡ã€‚ç°æœ‰çš„æµæ°´çº¿æ–¹æ³•é€šå¸¸å¤æ‚ä¸”æ•ˆæœä¸ä½³ã€‚å°½ç®¡å…¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆOLLMï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œä½†å®ƒä»¬åœ¨å®è·µä¸­ç»å¸¸é€€åŒ–ä¸ºç®€å•çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ç³»ç»Ÿã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åè®­ç»ƒæ–¹æ³•â€”â€”è§†è§‰é”šå®šçš„ç­–ç•¥ä¼˜åŒ–ï¼ˆVAPOï¼‰ï¼Œæ—¨åœ¨æ§åˆ¶æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚å€Ÿé‰´æ€ç»´é“¾æ¨ç†èŒƒå¼ï¼ŒVAPOä½¿ç”¨<think><answer>æ ¼å¼å¼ºåˆ¶æ‰§è¡Œç»“æ„åŒ–çš„â€œå…ˆçœ‹åè½¬å½•â€è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹é¦–å…ˆåœ¨thinkæ­¥éª¤ä¸­å¯¹å¹»ç¯ç‰‡å†…å®¹æ‰§è¡ŒOCRï¼Œç„¶ååœ¨answeræ­¥éª¤ä¸­é€šè¿‡å‚è€ƒæ­¤è¯†åˆ«çš„è§†è§‰ä¿¡æ¯æ¥ç”Ÿæˆè½¬å½•ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ­¤æ¨ç†è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨å››ä¸ªä¸åŒçš„å¥–åŠ±æ¥é’ˆå¯¹æ ¼å¼åˆè§„æ€§ã€OCRå‡†ç¡®æ€§ã€ASRè´¨é‡å’Œè§†è§‰é”šå®šä¸€è‡´æ€§ã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æ„å»ºäº†SlideASR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å®ä½“ä¸°å¯Œçš„åŸºå‡†ï¼ŒåŒ…å«ç”¨äºè®­ç»ƒå’Œæµ‹è¯•çš„åˆæˆæ•°æ®é›†ï¼Œä»¥åŠç”¨äºè¯„ä¼°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒVAPOæ˜¾ç€æé«˜äº†é¢†åŸŸç‰¹å®šæœ¯è¯­çš„è¯†åˆ«ç‡ï¼Œä¸ºSlideASRå»ºç«‹äº†ä¸€ä¸ªæœ‰æ•ˆçš„ç«¯åˆ°ç«¯èŒƒä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³SlideASRä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬æµæ°´çº¿æ–¹æ³•å’Œç›´æ¥ä½¿ç”¨OLLMï¼‰æ— æ³•æœ‰æ•ˆåˆ©ç”¨å¹»ç¯ç‰‡è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´é¢†åŸŸæœ¯è¯­è¯†åˆ«ç²¾åº¦ä½çš„é—®é¢˜ã€‚ç°æœ‰æµæ°´çº¿æ–¹æ³•å¤æ‚ï¼Œè€ŒOLLMå®¹æ˜“é€€åŒ–ä¸ºç®€å•çš„OCRç³»ç»Ÿï¼Œå¿½ç•¥äº†è¯­éŸ³ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå¼•å¯¼æ¨¡å‹æ¨¡ä»¿äººç±»â€œå…ˆçœ‹å¹»ç¯ç‰‡ï¼Œå†è¿›è¡Œè½¬å½•â€çš„è®¤çŸ¥è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥<think><answer>çš„ç»“æ„åŒ–æ¨ç†æ ¼å¼ï¼Œå¼ºåˆ¶æ¨¡å‹å…ˆè¿›è¡ŒOCRï¼Œå†ç»“åˆOCRç»“æœè¿›è¡Œè¯­éŸ³è½¬å½•ï¼Œä»è€Œå®ç°è§†è§‰ä¿¡æ¯å’Œè¯­éŸ³ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œé‡‡ç”¨åè®­ç»ƒçš„æ–¹å¼è¿›è¡Œä¼˜åŒ–ã€‚ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) è¾“å…¥å¹»ç¯ç‰‡å›¾åƒå’Œè¯­éŸ³ï¼›2) æ¨¡å‹ç”Ÿæˆ<think>æ­¥éª¤ï¼Œå¯¹å¹»ç¯ç‰‡å†…å®¹è¿›è¡ŒOCRï¼›3) æ¨¡å‹ç”Ÿæˆ<answer>æ­¥éª¤ï¼Œç»“åˆOCRç»“æœå’Œè¯­éŸ³ä¿¡æ¯è¿›è¡Œè½¬å½•ï¼›4) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œæ ¹æ®å››ä¸ªå¥–åŠ±å‡½æ•°ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºVAPOç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾å¼åœ°å¼•å¯¼æ¨¡å‹è¿›è¡Œè§†è§‰é”šå®šçš„æ¨ç†ã€‚ä¸ç›´æ¥è®­ç»ƒOLLMç›¸æ¯”ï¼ŒVAPOèƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œé¿å…æ¨¡å‹é€€åŒ–ä¸ºç®€å•çš„OCRç³»ç»Ÿã€‚

**å…³é”®è®¾è®¡**ï¼šVAPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) <think><answer>çš„ç»“æ„åŒ–æ¨ç†æ ¼å¼ï¼Œå¼ºåˆ¶æ¨¡å‹è¿›è¡Œè§†è§‰æ¨ç†ï¼›2) å››ä¸ªå¥–åŠ±å‡½æ•°ï¼šæ ¼å¼åˆè§„æ€§å¥–åŠ±ã€OCRå‡†ç¡®æ€§å¥–åŠ±ã€ASRè´¨é‡å¥–åŠ±å’Œè§†è§‰é”šå®šä¸€è‡´æ€§å¥–åŠ±ã€‚è¿™äº›å¥–åŠ±å‡½æ•°å…±åŒå¼•å¯¼æ¨¡å‹å­¦ä¹ æœ‰æ•ˆçš„è§†è§‰é”šå®šç­–ç•¥ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVAPOåœ¨SlideASRä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨SlideASR-Benchæ•°æ®é›†ä¸Šï¼ŒVAPOèƒ½å¤Ÿæœ‰æ•ˆæé«˜é¢†åŸŸç‰¹å®šæœ¯è¯­çš„è¯†åˆ«ç²¾åº¦ï¼Œä¼˜äºç°æœ‰çš„æµæ°´çº¿æ–¹æ³•å’Œç›´æ¥ä½¿ç”¨OLLMçš„æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåœ¨çº¿æ•™è‚²ã€ä¼šè®®è®°å½•ã€å­¦æœ¯è®²åº§ç­‰åœºæ™¯ï¼Œæé«˜è¯­éŸ³è½¬å½•çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚é€šè¿‡åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œå¯ä»¥æ›´å¥½åœ°è¯†åˆ«é¢†åŸŸæœ¯è¯­å’Œä¸“ä¸šçŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®ã€æ›´å¯é çš„è¯­éŸ³è½¬å½•æœåŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†é¢‘å­—å¹•ç”Ÿæˆç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automatic speech recognition (ASR) systems often struggle with domain-specific terminology, especially in specialized settings such as academic lectures. To address this, we define the SlideASR task, which leverages the rich visual information from presentation slides to improve transcription accuracy. Existing pipeline methods for this task tend to be complex and underperform. Although omni-modal large language models (OLLMs) provide a promising end-to-end framework, they frequently fail in practice by degenerating into simple optical character recognition (OCR) systems. To overcome this, we propose Visually-Anchored Policy Optimization (VAPO), a novel post-training method designed to control the model's reasoning process. Drawing on the Chain-of-Thought reasoning paradigm, VAPO enforces a structured "Look before Transcription" procedure using a <think><answer> format. Specifically, the model first performs OCR on the slide content within the think step, then generates the transcription by referencing this recognized visual information in the answer step. This reasoning process is optimized via reinforcement learning with four distinct rewards targeting format compliance, OCR accuracy, ASR quality, and visual anchoring consistency. To support further research, we construct SlideASR-Bench, a new entity-rich benchmark consisting of a synthetic dataset for training and testing, and a challenging real-world set for evaluation. Extensive experiments demonstrate that VAPO significantly improves recognition of domain-specific terms, establishing an effective end-to-end paradigm for SlideASR.

