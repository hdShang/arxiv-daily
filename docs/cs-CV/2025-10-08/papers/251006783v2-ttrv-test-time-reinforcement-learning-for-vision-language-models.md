---
layout: default
title: "TTRV: Test-Time Reinforcement Learning for Vision Language Models"
---

# TTRV: Test-Time Reinforcement Learning for Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06783" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.06783v2</a>
  <a href="https://arxiv.org/pdf/2510.06783.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06783v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.06783v2', 'TTRV: Test-Time Reinforcement Learning for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Akshit Singh, Shyam Marjit, Wei Lin, Paul Gavrikov, Serena Yeung-Levy, Hilde Kuehne, Rogerio Feris, Sivan Doveh, James Glass, M. Jehanzeb Mirza

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08 (æ›´æ–°: 2025-12-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTTRVï¼šä¸€ç§ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `æ— ç›‘ç£å­¦ä¹ ` `æ¨¡å‹è‡ªé€‚åº”` `Group Relative Policy Optimization`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–æ ‡æ³¨æ•°æ®å’Œä¸“ç”¨è®­ç»ƒé›†ï¼Œä¸äººç±»ç›´æ¥ä»ç¯å¢ƒä¸­å­¦ä¹ çš„æ–¹å¼ä¸åŒï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. TTRVé€šè¿‡åœ¨æµ‹è¯•æ—¶åŠ¨æ€è°ƒæ•´æ¨¡å‹ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹çš„è¾“å‡ºé¢‘ç‡è®¾è®¡å¥–åŠ±ï¼Œå¹¶é¼“åŠ±æ¨¡å‹è¾“å‡ºå¤šæ ·æ€§ï¼Œä»è€Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTTRVåœ¨å›¾åƒè¯†åˆ«å’ŒVQAä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œç”šè‡³è¶…è¶Šäº†GPT-4oç­‰å¼ºå¤§çš„ä¸“æœ‰æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºTTRVï¼Œä¸€ç§åœ¨æ¨ç†æ—¶åŠ¨æ€è°ƒæ•´è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºå…¶è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œæ— éœ€ä»»ä½•æ ‡æ³¨æ•°æ®ã€‚TTRVé€šè¿‡æ”¹è¿›Group Relative Policy Optimization (GRPO)æ¡†æ¶å®ç°ï¼ŒåŸºäºåŸºç¡€æ¨¡å‹è¾“å‡ºçš„é¢‘ç‡è®¾è®¡å¥–åŠ±ï¼Œå¹¶åœ¨æ¯ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šå¤šæ¬¡æ¨ç†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹ä»¥è·å¾—ä½ç†µçš„è¾“å‡ºç»éªŒåˆ†å¸ƒï¼Œæ¥æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§ã€‚åœ¨å¯¹è±¡è¯†åˆ«å’Œè§†è§‰é—®ç­”(VQA)ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œåˆ†åˆ«é«˜è¾¾52.4%å’Œ29.8%ï¼Œåœ¨16ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡æå‡åˆ†åˆ«ä¸º24.6%å’Œ10.0%ã€‚åœ¨å›¾åƒè¯†åˆ«æ–¹é¢ï¼Œåº”ç”¨äºInternVL 8Bçš„TTRVè¶…è¿‡äº†GPT-4oï¼Œåœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†2.3%ã€‚åœ¨VQAä»»åŠ¡ä¸­ä¹Ÿä¿æŒäº†ç«äº‰åŠ›ï¼Œè¯æ˜äº†æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ å¯ä»¥åŒ¹æ•Œç”šè‡³è¶…è¶Šæœ€å¼ºçš„ä¸“æœ‰æ¨¡å‹ã€‚å³ä½¿åœ¨æ•°æ®æåº¦å—é™çš„æƒ…å†µä¸‹ï¼Œå³ä»…åœ¨å•ä¸ªéšæœºé€‰æ‹©çš„æœªæ ‡è®°æµ‹è¯•æ ·æœ¬ä¸Šè¿›è¡Œè°ƒæ•´ï¼ŒTTRVåœ¨è¯†åˆ«ä»»åŠ¡ä¸­ä»ç„¶å¯ä»¥äº§ç”Ÿé«˜è¾¾5.5%çš„æ˜¾è‘—æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¾€å¾€éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œè€Œå¾®è°ƒé€šå¸¸ä¾èµ–å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨é¢å¯¹æ–°çš„ã€æœªçŸ¥çš„æµ‹è¯•æ•°æ®æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸ä¾èµ–æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„æ³›åŒ–èƒ½åŠ›æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTTRVçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æµ‹è¯•æ—¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ åŠ¨æ€è°ƒæ•´æ¨¡å‹çš„è¡Œä¸ºï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”å½“å‰çš„æµ‹è¯•æ ·æœ¬ã€‚é€šè¿‡è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹æ¢ç´¢ä¸åŒçš„è¾“å‡ºï¼Œå¹¶é€‰æ‹©æœ€ä¼˜çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•æ— éœ€æ ‡æ³¨æ•°æ®ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­æŒç»­æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTTRVåŸºäºGroup Relative Policy Optimization (GRPO)æ¡†æ¶ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ¨¡å‹è¿›è¡Œå¤šæ¬¡æ¨ç†ï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰è¾“å‡ºï¼›2) æ ¹æ®åŸºç¡€æ¨¡å‹çš„è¾“å‡ºé¢‘ç‡å’Œè¾“å‡ºåˆ†å¸ƒçš„ç†µï¼Œè®¡ç®—æ¯ä¸ªå€™é€‰è¾“å‡ºçš„å¥–åŠ±ï¼›3) ä½¿ç”¨GRPOç®—æ³•æ›´æ–°æ¨¡å‹çš„ç­–ç•¥ï¼Œä½¿å…¶å€¾å‘äºäº§ç”Ÿæ›´é«˜å¥–åŠ±çš„è¾“å‡ºã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨æµ‹è¯•æ—¶å¾ªç¯è¿›è¡Œï¼Œç›´åˆ°æ¨¡å‹æ”¶æ•›æˆ–è¾¾åˆ°é¢„å®šçš„è¿­ä»£æ¬¡æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šTTRVçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºæ¨¡å‹è‡ªèº«è¾“å‡ºé¢‘ç‡å’Œè¾“å‡ºåˆ†å¸ƒç†µçš„å¥–åŠ±å‡½æ•°ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤–éƒ¨çš„å¥–åŠ±ä¿¡å·ï¼Œè€ŒTTRVåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è¾“å‡ºä½œä¸ºå¥–åŠ±çš„æ¥æºï¼Œå®ç°äº†æ— ç›‘ç£çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥è¾“å‡ºåˆ†å¸ƒç†µçš„å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ›´å¤šæ ·åŒ–çš„è¾“å‡ºï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ˜¯TTRVçš„å…³é”®ã€‚å…·ä½“æ¥è¯´ï¼Œå¥–åŠ±å‡½æ•°åŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†åŸºäºåŸºç¡€æ¨¡å‹è¾“å‡ºçš„é¢‘ç‡ï¼Œé¼“åŠ±æ¨¡å‹äº§ç”Ÿæ›´å¸¸è§çš„è¾“å‡ºï¼›å¦ä¸€éƒ¨åˆ†åŸºäºè¾“å‡ºåˆ†å¸ƒçš„ç†µï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ›´å¤šæ ·åŒ–çš„è¾“å‡ºã€‚è¿™ä¸¤ä¸ªéƒ¨åˆ†é€šè¿‡åŠ æƒæ±‚å’Œçš„æ–¹å¼ç»“åˆåœ¨ä¸€èµ·ï¼Œæƒé‡ç³»æ•°éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚æ­¤å¤–ï¼ŒGRPOç®—æ³•çš„å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ä¹Ÿæ˜¯é‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å®éªŒç»“æœè¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TTRVåœ¨å¤šä¸ªè§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒTTRVåº”ç”¨äºInternVL 8Bæ¨¡å‹ï¼Œåœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡è¶…è¶ŠGPT-4o 2.3%ã€‚åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼ŒTTRVåœ¨16ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡æå‡ä¸º10.0%ï¼Œæœ€é«˜æå‡è¾¾åˆ°29.8%ã€‚å³ä½¿åœ¨æ•°æ®æåº¦å—é™çš„æƒ…å†µä¸‹ï¼ŒTTRVä»ç„¶å¯ä»¥äº§ç”Ÿæ˜¾è‘—çš„æ”¹è¿›ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„è‡ªé€‚åº”èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TTRVå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒè¯†åˆ«ã€è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç­‰ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºæ•°æ®æ ‡æ³¨æˆæœ¬é«˜æ˜‚æˆ–éš¾ä»¥è·å–çš„åœºæ™¯ã€‚é€šè¿‡åœ¨æµ‹è¯•æ—¶åŠ¨æ€è°ƒæ•´æ¨¡å‹ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”å®é™…åº”ç”¨ç¯å¢ƒã€‚æœªæ¥ï¼Œå¯ä»¥å°†TTRVä¸å…¶ä»–è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets. Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.

