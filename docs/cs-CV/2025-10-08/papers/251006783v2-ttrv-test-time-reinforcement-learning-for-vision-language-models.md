---
layout: default
title: TTRV: Test-Time Reinforcement Learning for Vision Language Models
---

# TTRV: Test-Time Reinforcement Learning for Vision Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.06783" target="_blank" class="toolbar-btn">arXiv: 2510.06783v2</a>
    <a href="https://arxiv.org/pdf/2510.06783.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06783v2" 
            onclick="toggleFavorite(this, '2510.06783v2', 'TTRV: Test-Time Reinforcement Learning for Vision Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Akshit Singh, Shyam Marjit, Wei Lin, Paul Gavrikov, Serena Yeung-Levy, Hilde Kuehne, Rogerio Feris, Sivan Doveh, James Glass, M. Jehanzeb Mirza

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-08 (Êõ¥Êñ∞: 2025-12-04)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TTRVÔºö‰∏ÄÁßçÁî®‰∫éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊµãËØïÊó∂Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÊó†ÈúÄÊ†áÊ≥®Êï∞ÊçÆ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÊµãËØïÊó∂Âº∫ÂåñÂ≠¶‰π†` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Êó†ÁõëÁù£Â≠¶‰π†` `Ê®°ÂûãËá™ÈÄÇÂ∫î` `Group Relative Policy Optimization`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰æùËµñÊ†áÊ≥®Êï∞ÊçÆÂíå‰∏ìÁî®ËÆ≠ÁªÉÈõÜÔºå‰∏é‰∫∫Á±ªÁõ¥Êé•‰ªéÁéØÂ¢É‰∏≠Â≠¶‰π†ÁöÑÊñπÂºè‰∏çÂêåÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. TTRVÈÄöËøáÂú®ÊµãËØïÊó∂Âä®ÊÄÅË∞ÉÊï¥Ê®°ÂûãÔºåÂà©Áî®Âü∫Á°ÄÊ®°ÂûãÁöÑËæìÂá∫È¢ëÁéáËÆæËÆ°Â•ñÂä±ÔºåÂπ∂ÈºìÂä±Ê®°ÂûãËæìÂá∫Â§öÊ†∑ÊÄßÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåTTRVÂú®ÂõæÂÉèËØÜÂà´ÂíåVQA‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜGPT-4oÁ≠âÂº∫Â§ßÁöÑ‰∏ìÊúâÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫TTRVÔºå‰∏ÄÁßçÂú®Êé®ÁêÜÊó∂Âä®ÊÄÅË∞ÉÊï¥ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñπÊ≥ïÔºå‰ª•Â¢ûÂº∫ÂÖ∂ËßÜËßâËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõÔºåÊó†ÈúÄ‰ªª‰ΩïÊ†áÊ≥®Êï∞ÊçÆ„ÄÇTTRVÈÄöËøáÊîπËøõGroup Relative Policy Optimization (GRPO)Ê°ÜÊû∂ÂÆûÁé∞ÔºåÂü∫‰∫éÂü∫Á°ÄÊ®°ÂûãËæìÂá∫ÁöÑÈ¢ëÁéáËÆæËÆ°Â•ñÂä±ÔºåÂπ∂Âú®ÊØè‰∏™ÊµãËØïÊ†∑Êú¨‰∏äÂ§öÊ¨°Êé®ÁêÜ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂ•ñÂä±Ê®°Âûã‰ª•Ëé∑Âæó‰ΩéÁÜµÁöÑËæìÂá∫ÁªèÈ™åÂàÜÂ∏ÉÔºåÊù•ÊéßÂà∂Ê®°ÂûãËæìÂá∫ÁöÑÂ§öÊ†∑ÊÄß„ÄÇÂú®ÂØπË±°ËØÜÂà´ÂíåËßÜËßâÈóÆÁ≠î(VQA)‰ªªÂä°‰∏≠ÔºåËØ•ÊñπÊ≥ïÂùáÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåÂàÜÂà´È´òËææ52.4%Âíå29.8%ÔºåÂú®16‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂπ≥ÂùáÊèêÂçáÂàÜÂà´‰∏∫24.6%Âíå10.0%„ÄÇÂú®ÂõæÂÉèËØÜÂà´ÊñπÈù¢ÔºåÂ∫îÁî®‰∫éInternVL 8BÁöÑTTRVË∂ÖËøá‰∫ÜGPT-4oÔºåÂú®8‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Âπ≥ÂùáÊèêÂçá‰∫Ü2.3%„ÄÇÂú®VQA‰ªªÂä°‰∏≠‰πü‰øùÊåÅ‰∫ÜÁ´û‰∫âÂäõÔºåËØÅÊòé‰∫ÜÊµãËØïÊó∂Âº∫ÂåñÂ≠¶‰π†ÂèØ‰ª•ÂåπÊïåÁîöËá≥Ë∂ÖË∂äÊúÄÂº∫ÁöÑ‰∏ìÊúâÊ®°Âûã„ÄÇÂç≥‰ΩøÂú®Êï∞ÊçÆÊûÅÂ∫¶ÂèóÈôêÁöÑÊÉÖÂÜµ‰∏ãÔºåÂç≥‰ªÖÂú®Âçï‰∏™ÈöèÊú∫ÈÄâÊã©ÁöÑÊú™Ê†áËÆ∞ÊµãËØïÊ†∑Êú¨‰∏äËøõË°åË∞ÉÊï¥ÔºåTTRVÂú®ËØÜÂà´‰ªªÂä°‰∏≠‰ªçÁÑ∂ÂèØ‰ª•‰∫ßÁîüÈ´òËææ5.5%ÁöÑÊòæËëóÊîπËøõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÂæÄÂæÄÈúÄË¶ÅÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åÂæÆË∞ÉÔºåËÄåÂæÆË∞ÉÈÄöÂ∏∏‰æùËµñÂ§ßÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆÔºåÊàêÊú¨È´òÊòÇ„ÄÇÊ≠§Â§ñÔºåÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂú®Èù¢ÂØπÊñ∞ÁöÑ„ÄÅÊú™Áü•ÁöÑÊµãËØïÊï∞ÊçÆÊó∂ÔºåÊÄßËÉΩÂèØËÉΩ‰ºö‰∏ãÈôç„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®‰∏ç‰æùËµñÊ†áÊ≥®Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèêÂçáÊ®°ÂûãÂú®ÊµãËØïÊó∂ÁöÑÊ≥õÂåñËÉΩÂäõÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTTRVÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂú®ÊµãËØïÊó∂Âà©Áî®Âº∫ÂåñÂ≠¶‰π†Âä®ÊÄÅË∞ÉÊï¥Ê®°ÂûãÁöÑË°å‰∏∫Ôºå‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂΩìÂâçÁöÑÊµãËØïÊ†∑Êú¨„ÄÇÈÄöËøáËÆæËÆ°ÂêàÈÄÇÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÂºïÂØºÊ®°ÂûãÊé¢Á¥¢‰∏çÂêåÁöÑËæìÂá∫ÔºåÂπ∂ÈÄâÊã©ÊúÄ‰ºòÁöÑÁ≠ñÁï•„ÄÇËøôÁßçÊñπÊ≥ïÊó†ÈúÄÊ†áÊ≥®Êï∞ÊçÆÔºåÂèØ‰ª•Âú®Êé®ÁêÜËøáÁ®ã‰∏≠ÊåÅÁª≠ÊîπËøõÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTTRVÂü∫‰∫éGroup Relative Policy Optimization (GRPO)Ê°ÜÊû∂„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) ÂØπÊØè‰∏™ÊµãËØïÊ†∑Êú¨ÔºåÊ®°ÂûãËøõË°åÂ§öÊ¨°Êé®ÁêÜÔºåÁîüÊàêÂ§ö‰∏™ÂÄôÈÄâËæìÂá∫Ôºõ2) Ê†πÊçÆÂü∫Á°ÄÊ®°ÂûãÁöÑËæìÂá∫È¢ëÁéáÂíåËæìÂá∫ÂàÜÂ∏ÉÁöÑÁÜµÔºåËÆ°ÁÆóÊØè‰∏™ÂÄôÈÄâËæìÂá∫ÁöÑÂ•ñÂä±Ôºõ3) ‰ΩøÁî®GRPOÁÆóÊ≥ïÊõ¥Êñ∞Ê®°ÂûãÁöÑÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ÂÄæÂêë‰∫é‰∫ßÁîüÊõ¥È´òÂ•ñÂä±ÁöÑËæìÂá∫„ÄÇËøô‰∏™ËøáÁ®ãÂú®ÊµãËØïÊó∂Âæ™ÁéØËøõË°åÔºåÁõ¥Âà∞Ê®°ÂûãÊî∂ÊïõÊàñËææÂà∞È¢ÑÂÆöÁöÑËø≠‰ª£Ê¨°Êï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTTRVÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂü∫‰∫éÊ®°ÂûãËá™Ë∫´ËæìÂá∫È¢ëÁéáÂíåËæìÂá∫ÂàÜÂ∏ÉÁÜµÁöÑÂ•ñÂä±ÂáΩÊï∞„ÄÇ‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ñÈÉ®ÁöÑÂ•ñÂä±‰ø°Âè∑ÔºåËÄåTTRVÂà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑËæìÂá∫‰Ωú‰∏∫Â•ñÂä±ÁöÑÊù•Ê∫êÔºåÂÆûÁé∞‰∫ÜÊó†ÁõëÁù£ÁöÑÊµãËØïÊó∂Ëá™ÈÄÇÂ∫î„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂºïÂÖ•ËæìÂá∫ÂàÜÂ∏ÉÁÜµÁöÑÂ•ñÂä±ÔºåÈºìÂä±Ê®°ÂûãÊé¢Á¥¢Êõ¥Â§öÊ†∑ÂåñÁöÑËæìÂá∫ÔºåÈÅøÂÖçÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÊòØTTRVÁöÑÂÖ≥ÈîÆ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ•ñÂä±ÂáΩÊï∞ÂåÖÂê´‰∏§ÈÉ®ÂàÜÔºö‰∏ÄÈÉ®ÂàÜÂü∫‰∫éÂü∫Á°ÄÊ®°ÂûãËæìÂá∫ÁöÑÈ¢ëÁéáÔºåÈºìÂä±Ê®°Âûã‰∫ßÁîüÊõ¥Â∏∏ËßÅÁöÑËæìÂá∫ÔºõÂè¶‰∏ÄÈÉ®ÂàÜÂü∫‰∫éËæìÂá∫ÂàÜÂ∏ÉÁöÑÁÜµÔºåÈºìÂä±Ê®°ÂûãÊé¢Á¥¢Êõ¥Â§öÊ†∑ÂåñÁöÑËæìÂá∫„ÄÇËøô‰∏§‰∏™ÈÉ®ÂàÜÈÄöËøáÂä†ÊùÉÊ±ÇÂíåÁöÑÊñπÂºèÁªìÂêàÂú®‰∏ÄËµ∑ÔºåÊùÉÈáçÁ≥ªÊï∞ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇÊ≠§Â§ñÔºåGRPOÁÆóÊ≥ïÁöÑÂ≠¶‰π†ÁéáÂíåËø≠‰ª£Ê¨°Êï∞‰πüÊòØÈáçË¶ÅÁöÑË∂ÖÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÂÆûÈ™åÁªìÊûúËøõË°å‰ºòÂåñ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

TTRVÂú®Â§ö‰∏™ËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®ÂõæÂÉèËØÜÂà´‰ªªÂä°‰∏≠ÔºåTTRVÂ∫îÁî®‰∫éInternVL 8BÊ®°ÂûãÔºåÂú®8‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Âπ≥ÂùáË∂ÖË∂äGPT-4o 2.3%„ÄÇÂú®ËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏≠ÔºåTTRVÂú®16‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂπ≥ÂùáÊèêÂçá‰∏∫10.0%ÔºåÊúÄÈ´òÊèêÂçáËææÂà∞29.8%„ÄÇÂç≥‰ΩøÂú®Êï∞ÊçÆÊûÅÂ∫¶ÂèóÈôêÁöÑÊÉÖÂÜµ‰∏ãÔºåTTRV‰ªçÁÑ∂ÂèØ‰ª•‰∫ßÁîüÊòæËëóÁöÑÊîπËøõÔºåËØÅÊòé‰∫ÜÂÖ∂Âº∫Â§ßÁöÑËá™ÈÄÇÂ∫îËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TTRVÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØ‰ª•Â∫îÁî®‰∫éÂêÑÁßçËßÜËßâËØ≠Ë®Ä‰ªªÂä°Ôºå‰æãÂ¶ÇÂõæÂÉèËØÜÂà´„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞Á≠â„ÄÇËØ•ÊñπÊ≥ïÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÊï∞ÊçÆÊ†áÊ≥®ÊàêÊú¨È´òÊòÇÊàñÈöæ‰ª•Ëé∑ÂèñÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÂú®ÊµãËØïÊó∂Âä®ÊÄÅË∞ÉÊï¥Ê®°ÂûãÔºåÂèØ‰ª•ÊòæËëóÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºå‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂÆûÈôÖÂ∫îÁî®ÁéØÂ¢É„ÄÇÊú™Êù•ÔºåÂèØ‰ª•Â∞ÜTTRV‰∏éÂÖ∂‰ªñËá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÁªìÂêàÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets. Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.

