---
layout: default
title: No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts
---

# No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06988" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.06988v1</a>
  <a href="https://arxiv.org/pdf/2510.06988.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06988v1" onclick="toggleFavorite(this, '2510.06988v1', 'No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Girolamo Macaluso, Lorenzo Mandelli, Mirko Bicchierai, Stefano Berretti, Andrew D. Bagdanov

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-08

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂêéËÆ≠ÁªÉËøêÂä®Êâ©Êï£Ê®°ÂûãÔºå‰ªÖÁî®ÊñáÊú¨ÊèêÁ§∫Âç≥ÂèØÂÆûÁé∞Âä®‰ΩúËøÅÁßª„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)** **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `ËøêÂä®ÁîüÊàê` `Êâ©Êï£Ê®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ÊñáÊú¨ÊèêÁ§∫` `Âä®‰ΩúËøÅÁßª`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËøêÂä®ÁîüÊàêÊ®°ÂûãÈúÄË¶ÅÂ§ßÈáèÂä®‰ΩúÊçïÊçâÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÊàñÂæÆË∞ÉÔºåÈöæ‰ª•ÈÄÇÂ∫îÊñ∞Âä®‰ΩúÊàñÈ£éÊ†º„ÄÇ
2. Âà©Áî®Âº∫ÂåñÂ≠¶‰π†Ôºå‰ªÖÈÄöËøáÊñáÊú¨ÊèêÁ§∫ÂæÆË∞ÉÈ¢ÑËÆ≠ÁªÉÁöÑËøêÂä®Êâ©Êï£Ê®°ÂûãÔºåÊó†ÈúÄÈ¢ùÂ§ñÁöÑÂä®‰ΩúÊçïÊçâÊï∞ÊçÆ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÊèêÈ´òÁîüÊàêËøêÂä®Ë¥®ÈáèÂíåÂ§öÊ†∑ÊÄßÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜÂéüÂßãÂàÜÂ∏É‰∏äÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂêéËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁî®‰∫éÂæÆË∞ÉÈ¢ÑËÆ≠ÁªÉÁöÑËøêÂä®Êâ©Êï£Ê®°ÂûãÔºå‰ªÖ‰ΩøÁî®ÊñáÊú¨ÊèêÁ§∫ÔºåÊó†ÈúÄ‰ªª‰ΩïËøêÂä®ground truth„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨-ËøêÂä®Ê£ÄÁ¥¢ÁΩëÁªú‰Ωú‰∏∫Â•ñÂä±‰ø°Âè∑ÔºåÂπ∂‰ΩøÁî®Denoising Diffusion Policy Optimization‰ºòÂåñÊâ©Êï£Á≠ñÁï•Ôºå‰ªéËÄåÊúâÊïàÂú∞Â∞ÜÊ®°ÂûãÁöÑÁîüÊàêÂàÜÂ∏ÉËΩ¨ÁßªÂà∞ÁõÆÊ†áÂüüÔºåËÄåÊó†ÈúÄÈÖçÂØπÁöÑËøêÂä®Êï∞ÊçÆ„ÄÇÊàë‰ª¨Âú®HumanML3DÂíåKIT-MLÊï∞ÊçÆÈõÜ‰∏äÔºåÈíàÂØπË∑®Êï∞ÊçÆÈõÜÈÄÇÂ∫îÂíåÁïô‰∏ÄÊ≥ïËøêÂä®ÂÆûÈ™åÔºåÂú®ÊΩúÂú®Á©∫Èó¥ÂíåËÅîÂêàÁ©∫Èó¥Êâ©Êï£Êû∂ÊûÑ‰∏äËØÑ‰º∞‰∫ÜËØ•ÊñπÊ≥ï„ÄÇÂÆöÈáèÊåáÊ†áÂíåÁî®Êà∑Á†îÁ©∂ÁöÑÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂßãÁªàÊèêÈ´ò‰∫ÜÁîüÊàêËøêÂä®ÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂéüÂßãÂàÜÂ∏É‰∏äÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÊòØ‰∏ÄÁßçÁÅµÊ¥ª„ÄÅÊï∞ÊçÆÈ´òÊïà‰∏î‰øùÊä§ÈöêÁßÅÁöÑËøêÂä®ÈÄÇÂ∫îËß£ÂÜ≥ÊñπÊ°à„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËøêÂä®ÁîüÊàêÊ®°ÂûãÂú®ÈÄÇÂ∫îÊñ∞ÁöÑÂä®‰ΩúÊàñÈ£éÊ†ºÊó∂ÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÂä®‰ΩúÊçïÊçâÊï∞ÊçÆËøõË°åÈáçÊñ∞ËÆ≠ÁªÉÊàñÂæÆË∞ÉÔºåËøô‰ΩøÂæóÊ®°ÂûãÈöæ‰ª•Êâ©Â±ïÂà∞Êñ∞ÁöÑÈ¢ÜÂüüÔºåÂπ∂‰∏îÊàêÊú¨È´òÊòÇ„ÄÇÊ≠§Â§ñÔºåËé∑ÂèñÈ´òË¥®ÈáèÁöÑÂä®‰ΩúÊçïÊçâÊï∞ÊçÆÊú¨Ë∫´‰πüÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÔºåÂú®‰∏çÈúÄË¶Å‰ªª‰ΩïËøêÂä®ground truthÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªÖÈÄöËøáÊñáÊú¨ÊèêÁ§∫Êù•ÂæÆË∞ÉÈ¢ÑËÆ≠ÁªÉÁöÑËøêÂä®Êâ©Êï£Ê®°Âûã„ÄÇÈÄöËøáÂ∞ÜÊñáÊú¨-ËøêÂä®Ê£ÄÁ¥¢ÁΩëÁªú‰Ωú‰∏∫Â•ñÂä±‰ø°Âè∑ÔºåÂºïÂØºÊ®°ÂûãÁîüÊàê‰∏éÊñáÊú¨ÊèèËø∞Êõ¥ÂåπÈÖçÁöÑËøêÂä®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) È¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨-ËøêÂä®Êâ©Êï£Ê®°ÂûãÔºåÁî®‰∫éÁîüÊàêÂàùÂßãËøêÂä®Ôºõ2) È¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨-ËøêÂä®Ê£ÄÁ¥¢ÁΩëÁªúÔºåÁî®‰∫éËØÑ‰º∞ÁîüÊàêËøêÂä®‰∏éÊñáÊú¨ÊèêÁ§∫ÁöÑÂåπÈÖçÁ®ãÂ∫¶ÔºåÂπ∂Êèê‰æõÂ•ñÂä±‰ø°Âè∑Ôºõ3) Âü∫‰∫éDenoising Diffusion Policy Optimization (DDPO) ÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÁî®‰∫é‰ºòÂåñÊâ©Êï£Ê®°ÂûãÁöÑÁîüÊàêÁ≠ñÁï•„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÁªôÂÆöÊñáÊú¨ÊèêÁ§∫ÔºåÊâ©Êï£Ê®°ÂûãÁîüÊàêËøêÂä®ÔºåÊ£ÄÁ¥¢ÁΩëÁªúËØÑ‰º∞ËøêÂä®‰∏éÊñáÊú¨ÁöÑÂåπÈÖçÂ∫¶ÔºåDDPOÊ†πÊçÆÂ•ñÂä±‰ø°Âè∑Êõ¥Êñ∞Êâ©Êï£Ê®°ÂûãÂèÇÊï∞ÔºåËø≠‰ª£‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÔºåÂÆÉÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÆåÂÖ®Âü∫‰∫éÊñáÊú¨ÊèêÁ§∫ÁöÑËøêÂä®Ê®°ÂûãÂæÆË∞ÉÊñπÊ≥ïÔºåÊó†ÈúÄ‰ªª‰ΩïËøêÂä®Êï∞ÊçÆ„ÄÇËøô‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•ËΩªÊùæÂú∞ÈÄÇÂ∫îÊñ∞ÁöÑÂä®‰ΩúÊàñÈ£éÊ†ºÔºåÂπ∂‰∏î‰øùÊä§‰∫ÜËøêÂä®Êï∞ÊçÆÁöÑÈöêÁßÅ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊõ¥Âä†ÁÅµÊ¥ª„ÄÅÊï∞ÊçÆÈ´òÊïà‰∏îÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËØ•ÊñπÊ≥ï‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨-ËøêÂä®Ê£ÄÁ¥¢ÁΩëÁªú‰Ωú‰∏∫Â•ñÂä±ÂáΩÊï∞ÔºåÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Áõ¥Êé•ÂΩ±Âìç‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇDDPOÁÆóÊ≥ïÁöÑÈÄâÊã©‰πüÂæàÂÖ≥ÈîÆÔºåÂÆÉËÉΩÂ§üÊúâÊïàÂú∞‰ºòÂåñÊâ©Êï£Ê®°ÂûãÁöÑÁîüÊàêÁ≠ñÁï•„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂåÖÊã¨Â≠¶‰π†Áéá„ÄÅÂ•ñÂä±ÂáΩÊï∞ÁöÑÊùÉÈáç„ÄÅ‰ª•ÂèäDDPOÁÆóÊ≥ïÁöÑË∂ÖÂèÇÊï∞Á≠â„ÄÇÊ≠§Â§ñÔºåÊâ©Êï£Ê®°ÂûãÁöÑÊû∂ÊûÑÔºàlatent-spaceÊàñjoint-spaceÔºâ‰πü‰ºöÂΩ±ÂìçÊúÄÁªàÁöÑÁîüÊàêÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®HumanML3DÂíåKIT-MLÊï∞ÊçÆÈõÜ‰∏äÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêËøêÂä®ÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇÁî®Êà∑Á†îÁ©∂Ë°®ÊòéÔºåÁîüÊàêÁöÑËøêÂä®Âú®ÁúüÂÆûÊÄßÂíå‰∏éÊñáÊú¨ÊèèËø∞ÁöÑÂåπÈÖçÂ∫¶ÊñπÈù¢Âùá‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂú®‰øùÊåÅÂéüÂßãÂàÜÂ∏ÉÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊàêÂäüÂú∞Â∞ÜÊ®°ÂûãÁöÑÁîüÊàêÂàÜÂ∏ÉËΩ¨ÁßªÂà∞ÁõÆÊ†áÂüü„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèë„ÄÅÂä®ÁîªÂà∂‰ΩúÁ≠âÈ¢ÜÂüüÔºåËÉΩÂ§üÊ†πÊçÆÊñáÊú¨ÊèèËø∞Ëá™Âä®ÁîüÊàêÈ´òË¥®ÈáèÁöÑ‰∫∫‰ΩìËøêÂä®Âä®Áîª„ÄÇ‰æãÂ¶ÇÔºåÊ∏∏ÊàèÂºÄÂèëËÄÖÂèØ‰ª•ÈÄöËøáËæìÂÖ•ÁÆÄÂçïÁöÑÊñáÊú¨Êåá‰ª§ÔºåÂø´ÈÄüÁîüÊàêËßíËâ≤ÊâÄÈúÄÁöÑÂêÑÁßçÂä®‰ΩúÔºå‰ªéËÄåÊèêÈ´òÂºÄÂèëÊïàÁéá„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÁîüÊàê‰∏™ÊÄßÂåñÁöÑËøêÂä®ËÆ≠ÁªÉÊñπÊ°àÔºåÊ†πÊçÆÁî®Êà∑ÁöÑÊñáÊú¨ÊèèËø∞ÁîüÊàêÂÆöÂà∂ÂåñÁöÑËøêÂä®Â∫èÂàó„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model's generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.

