---
layout: default
title: Temporal Prompting Matters: Rethinking Referring Video Object Segmentation
---

# Temporal Prompting Matters: Rethinking Referring Video Object Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07319" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07319v1</a>
  <a href="https://arxiv.org/pdf/2510.07319.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07319v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.07319v1', 'Temporal Prompting Matters: Rethinking Referring Video Object Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ci-Siang Lin, Min-Hung Chen, I-Jieh Liu, Chien-Yi Wang, Sifei Liu, Yu-Chiang Frank Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTenetæ¡†æ¶ï¼Œåˆ©ç”¨æ—¶åºPrompté«˜æ•ˆè§£å†³Referring Video Object Segmentationé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Referring Video Object Segmentation` `æ—¶åºPrompt` `Promptå­¦ä¹ ` `è§†é¢‘ç†è§£` `ç›®æ ‡åˆ†å‰²`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RVOSæ–¹æ³•ä¾èµ–ç«¯åˆ°ç«¯è®­ç»ƒå’Œå¯†é›†æ ‡æ³¨ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. Tenetæ¡†æ¶å°†RVOSåˆ†è§£ä¸ºæŒ‡ä»£ã€è§†é¢‘å’Œåˆ†å‰²å› ç´ ï¼Œåˆ©ç”¨æ—¶åºPromptè§£å†³æŒ‡ä»£å’Œè§†é¢‘é—®é¢˜ã€‚
3. é€šè¿‡Prompt Preference Learningè¯„ä¼°Promptè´¨é‡ï¼ŒæŒ‡å¯¼åŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆçš„RVOSã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é‡æ–°æ€è€ƒäº†Referring Video Object Segmentation (RVOS)ä»»åŠ¡ï¼Œæ—¨åœ¨æ¢ç©¶è¯¥ä»»åŠ¡çš„å…³é”®è¦ç´ ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œå¹¶ä¾èµ–å¯†é›†çš„maskæ ‡æ³¨ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”æ‰©å±•æ€§å·®ã€‚æœ¬æ–‡å°†RVOSä»»åŠ¡åˆ†è§£ä¸ºæŒ‡ä»£(referring)ã€è§†é¢‘(video)å’Œåˆ†å‰²(segmentation)ä¸‰ä¸ªå› ç´ ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºTemporal Prompt Generation and Selection (Tenet)çš„æ¡†æ¶ï¼Œä»¥è§£å†³æŒ‡ä»£å’Œè§†é¢‘å› ç´ ï¼Œè€Œå°†åˆ†å‰²é—®é¢˜äº¤ç»™åŸºç¡€åˆ†å‰²æ¨¡å‹ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å°†åŸºäºå›¾åƒçš„åŸºç¡€åˆ†å‰²æ¨¡å‹åº”ç”¨äºRVOSï¼Œæœ¬æ–‡åˆ©ç”¨ç°æˆçš„ç›®æ ‡æ£€æµ‹å™¨å’Œè·Ÿè¸ªå™¨ç”Ÿæˆä¸æŒ‡ä»£è¯­å¥ç›¸å…³è”çš„æ—¶åºPromptã€‚ä¸ºäº†è§£å†³é«˜è´¨é‡æ—¶åºPromptéš¾ä»¥é€šè¿‡ç½®ä¿¡åº¦åˆ†æ•°è¯†åˆ«çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Prompt Preference Learningæ¥è¯„ä¼°ç”Ÿæˆçš„æ—¶åºPromptçš„è´¨é‡ã€‚é€šè¿‡ä½¿ç”¨è¿™äº›Promptæ¥æŒ‡å¯¼åŸºäºå›¾åƒçš„åŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œå¯ä»¥ä¸ºè¢«æŒ‡å¯¹è±¡ç”Ÿæˆé«˜è´¨é‡çš„maskï¼Œä»è€Œå®ç°æ¨¡å‹å¯¹RVOSçš„æœ‰æ•ˆé€‚åº”ã€‚åœ¨RVOSåŸºå‡†ä¸Šçš„å®éªŒè¯æ˜äº†Tenetæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šReferring Video Object Segmentation (RVOS)æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°ï¼Œåœ¨è§†é¢‘ä¸­åˆ†å‰²å‡ºç›®æ ‡å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒï¼Œéœ€è¦å¤§é‡çš„maskæ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæ¶ˆè€—å·¨å¤§ï¼Œå¹¶ä¸”æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ï¼Œéš¾ä»¥é€‚åº”æ–°çš„åœºæ™¯å’Œå¯¹è±¡ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶åºä¿¡æ¯ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†RVOSä»»åŠ¡è§£è€¦ä¸ºä¸‰ä¸ªå…³é”®å› ç´ ï¼šæŒ‡ä»£ç†è§£ã€è§†é¢‘æ—¶åºå»ºæ¨¡å’Œå¯¹è±¡åˆ†å‰²ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒåˆ†å‰²æ¨¡å‹ä½œä¸ºåŸºç¡€åˆ†å‰²èƒ½åŠ›ï¼Œé‡ç‚¹è§£å†³æŒ‡ä»£ç†è§£å’Œè§†é¢‘æ—¶åºå»ºæ¨¡é—®é¢˜ã€‚é€šè¿‡ç”Ÿæˆå’Œé€‰æ‹©é«˜è´¨é‡çš„æ—¶åºPromptï¼Œå¼•å¯¼åŸºç¡€åˆ†å‰²æ¨¡å‹å®Œæˆæœ€ç»ˆçš„åˆ†å‰²ä»»åŠ¡ï¼Œä»è€Œé¿å…äº†ç«¯åˆ°ç«¯è®­ç»ƒçš„éœ€è¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTenetæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) **æ—¶åºPromptç”Ÿæˆæ¨¡å—**ï¼šåˆ©ç”¨ç°æˆçš„ç›®æ ‡æ£€æµ‹å™¨å’Œè·Ÿè¸ªå™¨ï¼Œç»“åˆæŒ‡ä»£è¯­å¥ï¼Œåœ¨è§†é¢‘å¸§ä¸­ç”Ÿæˆå€™é€‰çš„Promptã€‚2) **Prompt Preference Learningæ¨¡å—**ï¼šè®¾è®¡ä¸€ä¸ªå­¦ä¹ æœºåˆ¶ï¼Œç”¨äºè¯„ä¼°å’Œé€‰æ‹©é«˜è´¨é‡çš„Promptã€‚è¯¥æ¨¡å—æ—¨åœ¨è§£å†³ä»…ä¾é æ£€æµ‹å™¨å’Œè·Ÿè¸ªå™¨çš„ç½®ä¿¡åº¦åˆ†æ•°éš¾ä»¥åŒºåˆ†é«˜è´¨é‡Promptçš„é—®é¢˜ã€‚3) **åˆ†å‰²æ¨¡å—**ï¼šä½¿ç”¨é€‰å®šçš„Promptæ¥æŒ‡å¯¼é¢„è®­ç»ƒçš„å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†Temporal Prompt Generation and Selection (Tenet)æ¡†æ¶ï¼Œå°†RVOSä»»åŠ¡åˆ†è§£ä¸ºæŒ‡ä»£ã€è§†é¢‘å’Œåˆ†å‰²ä¸‰ä¸ªå› ç´ ï¼Œå¹¶åˆ©ç”¨æ—¶åºPromptæ¥æ¡¥æ¥æŒ‡ä»£ç†è§£å’Œè§†é¢‘æ—¶åºå»ºæ¨¡ã€‚Prompt Preference Learningæ¨¡å—æ˜¯å¦ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°å’Œé€‰æ‹©é«˜è´¨é‡çš„Promptï¼Œä»è€Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šPrompt Preference Learningæ¨¡å—çš„å…·ä½“è®¾è®¡ç»†èŠ‚æœªçŸ¥ï¼Œè®ºæ–‡ä¸­å¯èƒ½æ¶‰åŠç‰¹å®šçš„æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„æ¥å­¦ä¹ Promptçš„è´¨é‡è¯„ä¼°ã€‚æ—¶åºPromptçš„å…·ä½“å½¢å¼ï¼ˆä¾‹å¦‚ï¼Œbounding box, maskç­‰ï¼‰ä»¥åŠå¦‚ä½•å°†å…¶èå…¥åˆ°åŸºç¡€åˆ†å‰²æ¨¡å‹ä¸­ä¹Ÿå¯èƒ½æ˜¯å…³é”®çš„è®¾è®¡ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨ç›®æ ‡æ£€æµ‹å™¨å’Œè·Ÿè¸ªå™¨çš„è¾“å‡ºï¼Œç”Ÿæˆä¸æŒ‡ä»£è¯­å¥ç›¸å…³çš„Promptï¼Œä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨RVOSåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯æ˜äº†Tenetæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®ã€å¯¹æ¯”åŸºçº¿ä»¥åŠæå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œé¿å…äº†ç«¯åˆ°ç«¯è®­ç»ƒï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚Prompt Preference Learningæ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°é€‰æ‹©é«˜è´¨é‡çš„Promptï¼Œä»è€Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç¼–è¾‘ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­ï¼Œå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å¿«é€Ÿå®šä½å’Œåˆ†å‰²ç›®æ ‡å¯¹è±¡ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥æ ¹æ®è¯­éŸ³æŒ‡ä»¤è¯†åˆ«å’Œè·Ÿè¸ªç‰¹å®šè½¦è¾†æˆ–è¡Œäººï¼›åœ¨è§†é¢‘ç¼–è¾‘ä¸­ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å¯¹è§†é¢‘ä¸­çš„ç‰¹å®šå¯¹è±¡è¿›è¡Œç¼–è¾‘å’Œå¤„ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.

