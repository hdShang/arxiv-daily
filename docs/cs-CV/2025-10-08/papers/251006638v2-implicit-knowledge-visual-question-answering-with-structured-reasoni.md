---
layout: default
title: Implicit-Knowledge Visual Question Answering with Structured Reasoning Traces
---

# Implicit-Knowledge Visual Question Answering with Structured Reasoning Traces

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06638" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.06638v2</a>
  <a href="https://arxiv.org/pdf/2510.06638.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06638v2" onclick="toggleFavorite(this, '2510.06638v2', 'Implicit-Knowledge Visual Question Answering with Structured Reasoning Traces')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhihao Wen, Wenkang Wei, Yuan Fang, Xingtong Yu, Hui Zhang, Weicheng Zhu, Xin Zhang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08 (æ›´æ–°: 2025-11-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMODELNAMEæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†è½¨è¿¹æå‡éšå¼çŸ¥è¯†è§†è§‰é—®ç­”æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰é—®ç­”` `éšå¼çŸ¥è¯†` `å¤šæ¨¡æ€å­¦ä¹ ` `ç»“æ„åŒ–æ¨ç†` `è‡ªè’¸é¦` `å¤§è¯­è¨€æ¨¡å‹` `çŸ¥è¯†å›¾è°±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰IK-KVQAæ–¹æ³•ä¾èµ–ç­”æ¡ˆç›‘ç£ï¼Œæ¨ç†è¿‡ç¨‹éšå¼ï¼Œç¼ºä¹æ˜ç¡®çš„æ¨ç†ä¾æ®ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. MODELNAMEæ¡†æ¶é€šè¿‡å¼•å…¥åŒè·¯å¾„ç»“æ„åŒ–æ¨ç†è½¨è¿¹ï¼Œæ˜¾å¼å¼•å¯¼æ¨¡å‹å…³æ³¨ç›¸å…³å®ä½“å’Œå±æ€§ï¼Œå¢å¼ºå½’çº³åç½®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMODELNAMEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ¨ç†é€æ˜åº¦ï¼Œæ— éœ€å¤–éƒ¨çŸ¥è¯†åº“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”(KVQA)è¦æ±‚æ¨¡å‹èƒ½å¤Ÿå®šä½å›¾åƒä¸­çš„å®ä½“å¹¶åŸºäºäº‹å®çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚æœ€è¿‘çš„ç ”ç©¶å¼•å…¥äº†å…¶éšå¼çŸ¥è¯†å˜ä½“IK-KVQAï¼Œå…¶ä¸­å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æ˜¯å”¯ä¸€çš„çŸ¥è¯†æ¥æºï¼Œæ— éœ€å¤–éƒ¨æ£€ç´¢å³å¯ç”Ÿæˆç­”æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„IK-KVQAæ–¹æ³•é€šå¸¸ä»…ä½¿ç”¨ç­”æ¡ˆè¿›è¡Œç›‘ç£è®­ç»ƒï¼šæ¨ç†ä»ç„¶æ˜¯éšå¼çš„ï¼Œç†ç”±é€šå¸¸æ˜¯è–„å¼±æˆ–ä¸ä¸€è‡´çš„ï¼Œå¹¶ä¸”åœ¨æ ‡å‡†ç›‘ç£å¾®è°ƒ(SFT)åçš„æ³›åŒ–èƒ½åŠ›å¯èƒ½å¾ˆå·®ã€‚æˆ‘ä»¬æå‡ºäº†MODELNAMEï¼Œä¸€ä¸ªä¸ºIK-KVQAé…å¤‡åŒè·¯å¾„ç»“æ„åŒ–æ¨ç†è½¨è¿¹ï¼ˆæ–‡æœ¬å’Œè§†è§‰ä¸Šçš„ç¬¦å·å…³ç³»è·¯å¾„ä»¥åŠåŸºäºè·¯å¾„çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼‰çš„æ¡†æ¶ï¼Œä»¥æä¾›æ¯”é€šç”¨ç­”æ¡ˆç›‘ç£æ›´å¼ºçš„å½’çº³åç½®ã€‚è¿™äº›è½¨è¿¹å……å½“äº†æ¨¡æ€æ„ŸçŸ¥çš„æ”¯æ¶ï¼Œå¼•å¯¼æ¨¡å‹æ‰¾åˆ°ç›¸å…³çš„å®ä½“å’Œå±æ€§ï¼Œæä¾›æ¯”é€šç”¨æ€ç»´é“¾ç›‘ç£æ›´å¤šçš„ç»“æ„ï¼ŒåŒæ—¶åˆä¸å°†æ¨ç†é™åˆ¶åœ¨ä»»ä½•å•ä¸€çš„å›ºå®šè·¯å¾„ä¸Šã€‚ä½¿ç”¨å•ä¸ªå¼€æºMLLMï¼ŒMODELNAMEæ„å»ºå¹¶é€‰æ‹©è½¨è¿¹æ¥æ„å»ºç¦»çº¿è½¨è¿¹å¢å¼ºæ•°æ®é›†ï¼Œç„¶åæ‰§è¡Œç»“æ„æ„ŸçŸ¥è‡ªè’¸é¦ï¼›ä¸ä½¿ç”¨å¤–éƒ¨æ£€ç´¢å™¨ã€éªŒè¯å™¨æˆ–ç²¾å¿ƒè®¾è®¡çš„çŸ¥è¯†åº“ï¼Œå¹¶ä¸”æ¨ç†æ˜¯å•ä¸ªè‡ªå›å½’è¿‡ç¨‹ã€‚åœ¨å„ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMODELNAMEå§‹ç»ˆæé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§å’Œä¸­é—´æ¨ç†çš„é€æ˜åº¦ï¼Œåœ¨OK-VQAä¸Šå®ç°äº†é«˜è¾¾11.3%çš„ç­”æ¡ˆå‡†ç¡®ç‡æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³éšå¼çŸ¥è¯†è§†è§‰é—®ç­”(IK-KVQA)ä¸­ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ä»…é€šè¿‡ç­”æ¡ˆç›‘ç£è®­ç»ƒï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹ä¸é€æ˜ã€ç†ç”±ä¸å……åˆ†ã€æ³›åŒ–èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„æœ‰æ•ˆå¼•å¯¼ï¼Œéš¾ä»¥ä¿è¯ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç»“æ„åŒ–çš„æ¨ç†è½¨è¿¹æ¥æ˜¾å¼åœ°å¼•å¯¼MLLMè¿›è¡Œæ¨ç†ã€‚è¿™äº›è½¨è¿¹åŒ…å«æ–‡æœ¬å’Œè§†è§‰ä¸Šçš„ç¬¦å·å…³ç³»è·¯å¾„ï¼Œä»¥åŠåŸºäºè·¯å¾„çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œä»è€Œä¸ºæ¨¡å‹æä¾›æ›´å¼ºçš„å½’çº³åç½®ï¼Œä½¿å…¶èƒ½å¤Ÿå…³æ³¨å›¾åƒä¸­çš„ç›¸å…³å®ä½“å’Œå±æ€§ï¼Œå¹¶è¿›è¡Œæ›´å¯é çš„æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMODELNAMEæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š1) **è½¨è¿¹æ„å»ºä¸é€‰æ‹©**ï¼šåˆ©ç”¨å¼€æºMLLMæ„å»ºå€™é€‰æ¨ç†è½¨è¿¹ï¼Œå¹¶æ ¹æ®ä¸€å®šçš„æ ‡å‡†é€‰æ‹©é«˜è´¨é‡çš„è½¨è¿¹ã€‚è¿™äº›è½¨è¿¹åŒ…æ‹¬ç¬¦å·å…³ç³»è·¯å¾„å’Œè‡ªç„¶è¯­è¨€è§£é‡Šã€‚2) **ç¦»çº¿æ•°æ®é›†æ„å»º**ï¼šä½¿ç”¨é€‰æ‹©çš„è½¨è¿¹å¢å¼ºåŸå§‹æ•°æ®é›†ï¼Œæ„å»ºä¸€ä¸ªè½¨è¿¹å¢å¼ºçš„ç¦»çº¿æ•°æ®é›†ã€‚3) **ç»“æ„æ„ŸçŸ¥è‡ªè’¸é¦**ï¼šä½¿ç”¨è½¨è¿¹å¢å¼ºçš„æ•°æ®é›†å¯¹MLLMè¿›è¡Œè‡ªè’¸é¦è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ç»“æ„åŒ–çš„æ¨ç†æ¨¡å¼ã€‚4) **æ¨ç†**ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡å•ä¸ªè‡ªå›å½’è¿‡ç¨‹ç”Ÿæˆç­”æ¡ˆï¼Œæ— éœ€å¤–éƒ¨æ£€ç´¢å™¨æˆ–çŸ¥è¯†åº“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†åŒè·¯å¾„ç»“æ„åŒ–æ¨ç†è½¨è¿¹ï¼Œå°†ç¬¦å·æ¨ç†å’Œè‡ªç„¶è¯­è¨€è§£é‡Šç›¸ç»“åˆï¼Œä¸ºMLLMæä¾›äº†æ›´å¼ºçš„æ¨ç†æŒ‡å¯¼ã€‚ä¸ä¼ ç»Ÿçš„æ€ç»´é“¾(Chain-of-Thought)æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æä¾›äº†æ›´å¤šçš„ç»“æ„åŒ–ä¿¡æ¯ï¼ŒåŒæ—¶åˆä¸é™åˆ¶æ¨ç†è·¯å¾„çš„çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å®Œå…¨ä¾èµ–äºå•ä¸ªå¼€æºMLLMï¼Œæ— éœ€å¤–éƒ¨çŸ¥è¯†åº“æˆ–æ£€ç´¢å™¨ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **è½¨è¿¹æ„å»ºæ–¹æ³•**ï¼šå…·ä½“å¦‚ä½•åˆ©ç”¨MLLMç”Ÿæˆç¬¦å·å…³ç³»è·¯å¾„å’Œè‡ªç„¶è¯­è¨€è§£é‡Šã€‚2) **è½¨è¿¹é€‰æ‹©æ ‡å‡†**ï¼šå¦‚ä½•è¯„ä¼°å’Œé€‰æ‹©é«˜è´¨é‡çš„æ¨ç†è½¨è¿¹ã€‚3) **è‡ªè’¸é¦è®­ç»ƒç­–ç•¥**ï¼šå¦‚ä½•åˆ©ç”¨è½¨è¿¹å¢å¼ºçš„æ•°æ®é›†æœ‰æ•ˆåœ°è®­ç»ƒMLLMï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°ç»“æ„åŒ–çš„æ¨ç†æ¨¡å¼ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MODELNAMEåœ¨å¤šä¸ªKVQAåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨OK-VQAæ•°æ®é›†ä¸Šï¼ŒMODELNAMEçš„ç­”æ¡ˆå‡†ç¡®ç‡æ¯”æœ€å¼ºçš„åŸºçº¿æ–¹æ³•æé«˜äº†11.3%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMODELNAMEä¸ä»…æé«˜äº†ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œè¿˜å¢å¼ºäº†ä¸­é—´æ¨ç†è¿‡ç¨‹çš„é€æ˜åº¦ï¼Œä½¿å…¶æ›´æ˜“äºç†è§£å’Œè°ƒè¯•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¢æœã€è§†è§‰è¾…åŠ©ã€æ•™è‚²ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¢æœä¸­ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®ç”¨æˆ·æå‡ºçš„è§†è§‰é—®é¢˜ï¼Œç»“åˆå›¾åƒä¿¡æ¯å’Œéšå¼çŸ¥è¯†è¿›è¡Œæ¨ç†ï¼Œç»™å‡ºå‡†ç¡®ä¸”å¯è§£é‡Šçš„ç­”æ¡ˆã€‚åœ¨è§†è§‰è¾…åŠ©é¢†åŸŸï¼Œæ¨¡å‹å¯ä»¥å¸®åŠ©è§†éšœäººå£«ç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶å›ç­”ä»–ä»¬æå‡ºçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ„å»ºæ›´æ™ºèƒ½çš„æ•™è‚²ç³»ç»Ÿï¼Œå¸®åŠ©å­¦ç”Ÿç†è§£å¤æ‚çš„æ¦‚å¿µã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge-based Visual Question Answering (KVQA) requires models to ground entities in images and reason over factual knowledge. Recent work has introduced its implicit-knowledge variant, IK-KVQA, where a multimodal large language model (MLLM) is the sole knowledge source and answers are produced without external retrieval. Existing IK-KVQA approaches, however, are typically trained with answer-only supervision: reasoning remains implicit, justifications are often weak or inconsistent, and generalization after standard supervised fine-tuning (SFT) can be brittle. We propose MODELNAME, a framework that equips IK-KVQA with dual-path structured reasoning traces (symbolic relation paths over text and vision together with path-grounded natural-language explanations) to provide a stronger inductive bias than generic answer-only supervision. These traces act as modality-aware scaffolds that guide the model toward relevant entities and attributes, offering more structure than generic chain-of-thought supervision while not constraining reasoning to any single fixed path. Using a single open-source MLLM, MODELNAME constructs and selects traces to build an offline trace-enriched dataset and then performs structure-aware self-distillation; no external retrievers, verifiers, or curated knowledge bases are used, and inference is a single autoregressive pass. Across benchmarks, MODELNAME consistently improves both answer accuracy and the transparency of intermediate reasoning, achieving up to 11.3% higher answer accuracy on OK-VQA over the strongest baseline.

