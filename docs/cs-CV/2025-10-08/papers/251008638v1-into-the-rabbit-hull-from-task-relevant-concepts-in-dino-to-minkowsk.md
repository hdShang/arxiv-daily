---
layout: default
title: "Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry"
---

# Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08638" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08638v1</a>
  <a href="https://arxiv.org/pdf/2510.08638.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08638v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08638v1', 'Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thomas Fel, Binxu Wang, Michael A. Lepori, Matthew Kowal, Andrew Lee, Randall Balestriero, Sonia Joseph, Ekdeep S. Lubana, Talia Konkle, Demba Ba, Martin Wattenberg

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡SAEåˆ†æDINOv2ï¼Œæ­ç¤ºå…¶è¡¨å¾çš„åŠŸèƒ½ä¸“ä¸šåŒ–å’ŒMinkowskiå‡ ä½•ç‰¹æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `DINOv2` `å¯è§£é‡Šæ€§` `ç¨€ç–è‡ªç¼–ç å™¨` `Minkowskiå‡ ä½•` `è§†è§‰Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. DINOv2åœ¨ç›®æ ‡è¯†åˆ«ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å†…éƒ¨è¡¨å¾æœºåˆ¶å°šä¸æ˜ç¡®ï¼Œé¢ä¸´å¯è§£é‡Šæ€§æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºMinkowskiè¡¨å¾å‡è®¾ï¼ˆMRHï¼‰ï¼Œè®¤ä¸ºDINOv2çš„è¡¨å¾æ˜¯åŸå‹æ¦‚å¿µçš„å‡¸ç»„åˆï¼Œå¹¶ç”¨SAEè¿›è¡ŒéªŒè¯ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDINOv2åœ¨ä¸åŒä»»åŠ¡ä¸­å±•ç°åŠŸèƒ½ä¸“ä¸šåŒ–ï¼Œä¸”å…¶è¡¨å¾å…·æœ‰éçº¿æ€§ç¨€ç–æ€§å’ŒMinkowskiå‡ ä½•ç‰¹æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨æ¢ç©¶DINOv2çš„æ„ŸçŸ¥æœºåˆ¶ã€‚ç ”ç©¶åŸºäºçº¿æ€§è¡¨å¾å‡è®¾ï¼ˆLRHï¼‰ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ„å»ºäº†ä¸€ä¸ªåŒ…å«32000ä¸ªå•å…ƒçš„å­—å…¸ï¼Œä½œä¸ºå¯è§£é‡Šæ€§åˆ†æçš„åŸºç¡€ã€‚ç ”ç©¶åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šé¦–å…ˆï¼Œåˆ†æä¸åŒä¸‹æ¸¸ä»»åŠ¡å¦‚ä½•åˆ©ç”¨å­—å…¸ä¸­çš„æ¦‚å¿µï¼Œæ­ç¤ºäº†åŠŸèƒ½ä¸“ä¸šåŒ–ç°è±¡ï¼šåˆ†ç±»ä»»åŠ¡åˆ©ç”¨â€œElsewhereâ€æ¦‚å¿µå®ç°å­¦ä¹ åˆ°çš„å¦å®šï¼›åˆ†å‰²ä»»åŠ¡ä¾èµ–äºå½¢æˆè¿è´¯å­ç©ºé—´çš„è¾¹ç•Œæ£€æµ‹å™¨ï¼›æ·±åº¦ä¼°è®¡ä»»åŠ¡åˆ©ç”¨ä¸è§†è§‰ç¥ç»ç§‘å­¦åŸç†ç›¸ç¬¦çš„å•ç›®æ·±åº¦çº¿ç´¢ã€‚å…¶æ¬¡ï¼Œåˆ†æSAEå­¦ä¹ åˆ°çš„æ¦‚å¿µçš„å‡ ä½•å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œå‘ç°è¡¨å¾æ˜¯éƒ¨åˆ†ç¨ å¯†çš„ï¼Œå­—å…¸å‘æ›´å¤§çš„è¿è´¯æ€§æ¼”è¿›ï¼Œåç¦»äº†æœ€å¤§æ­£äº¤ç†æƒ³ã€‚å›¾åƒå†…çš„tokenså æ®ä½ç»´å±€éƒ¨è¿æ¥é›†åˆï¼Œä¸”åœ¨ç§»é™¤ä½ç½®ä¿¡æ¯åä»ç„¶å­˜åœ¨ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„è§‚ç‚¹ï¼štokensç”±åŸå‹ï¼ˆä¾‹å¦‚ï¼ŒåŠ¨ç‰©ä¸­çš„å…”å­ï¼Œé¢œè‰²ä¸­çš„æ£•è‰²ï¼Œçº¹ç†ä¸­çš„è“¬æ¾ï¼‰çš„å‡¸æ··åˆå½¢æˆã€‚è¿™ç§ç»“æ„åŸºäºGardenforsçš„æ¦‚å¿µç©ºé—´ï¼Œå¹¶ä¸æ¨¡å‹çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ç›¸ç¬¦ï¼Œä»è€Œå®šä¹‰äº†ç”±åŸå‹ç•Œå®šçš„åŒºåŸŸã€‚æå‡ºäº†Minkowskiè¡¨å¾å‡è®¾ï¼ˆMRHï¼‰ï¼Œå¹¶æ£€éªŒäº†å…¶ç»éªŒç‰¹å¾åŠå…¶å¯¹è§£é‡Šè§†è§‰Transformerè¡¨å¾çš„æ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šDINOv2ç­‰è§†è§‰Transformeræ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å†…éƒ¨è¡¨å¾æœºåˆ¶ï¼Œå³æ¨¡å‹ç©¶ç«Ÿâ€œçœ‹åˆ°â€äº†ä»€ä¹ˆï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé»‘ç›’ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£é‡Šè¿™äº›æ¨¡å‹çš„è¡¨å¾ï¼Œç¼ºä¹å¯¹æ¨¡å‹å†…éƒ¨æ¦‚å¿µå’Œå‡ ä½•ç»“æ„çš„ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å­¦ä¹ DINOv2çš„ä¸­é—´å±‚è¡¨å¾ï¼Œæ„å»ºä¸€ä¸ªå¯è§£é‡Šçš„å­—å…¸ï¼Œç„¶ååˆ†æè¿™ä¸ªå­—å…¸ä¸­çš„æ¦‚å¿µåœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ¿€æ´»æ¨¡å¼ï¼Œä»¥åŠè¿™äº›æ¦‚å¿µçš„å‡ ä½•å’Œç»Ÿè®¡ç‰¹æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ­ç¤ºDINOv2å†…éƒ¨è¡¨å¾çš„åŠŸèƒ½ä¸“ä¸šåŒ–å’ŒMinkowskiå‡ ä½•ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1. ä½¿ç”¨DINOv2æå–å›¾åƒç‰¹å¾ã€‚2. ä½¿ç”¨SAEå¯¹DINOv2çš„ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°ä¸€ä¸ªåŒ…å«32000ä¸ªå•å…ƒçš„å­—å…¸ã€‚3. åˆ†æå­—å…¸ä¸­çš„æ¦‚å¿µåœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ï¼ˆåˆ†ç±»ã€åˆ†å‰²ã€æ·±åº¦ä¼°è®¡ï¼‰ä¸­çš„æ¿€æ´»æ¨¡å¼ã€‚4. åˆ†æå­—å…¸ä¸­æ¦‚å¿µçš„å‡ ä½•å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œä¾‹å¦‚ç¨€ç–æ€§ã€è¿è´¯æ€§ã€æ­£äº¤æ€§ç­‰ã€‚5. æå‡ºMinkowskiè¡¨å¾å‡è®¾ï¼ˆMRHï¼‰ï¼Œå¹¶éªŒè¯å…¶ç»éªŒç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†Minkowskiè¡¨å¾å‡è®¾ï¼ˆMRHï¼‰ï¼Œè®¤ä¸ºDINOv2çš„è¡¨å¾æ˜¯ç”±åŸå‹æ¦‚å¿µçš„å‡¸ç»„åˆå½¢æˆçš„ï¼Œè¿™æ˜¯ä¸€ç§å¯¹è§†è§‰Transformerè¡¨å¾çš„æ–°é¢–è§£é‡Šã€‚ä¸ä¼ ç»Ÿçš„çº¿æ€§ç¨€ç–è¡¨å¾å‡è®¾ä¸åŒï¼ŒMRHè€ƒè™‘äº†æ¦‚å¿µä¹‹é—´çš„éçº¿æ€§å…³ç³»å’Œå‡ ä½•ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šSAEçš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–é‡æ„è¯¯å·®ï¼ŒåŒæ—¶é¼“åŠ±ç¨€ç–æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨äº†L1æ­£åˆ™åŒ–æ¥çº¦æŸSAEçš„æ¿€æ´»ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†å­—å…¸ä¸­æ¦‚å¿µçš„è¿è´¯æ€§ï¼Œé€šè¿‡è®¡ç®—æ¦‚å¿µä¹‹é—´çš„ç›¸å…³æ€§æ¥è¡¡é‡ã€‚åœ¨éªŒè¯MRHæ—¶ï¼Œè®ºæ–‡åˆ†æäº†DINOv2çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‘ç°å…¶äº§ç”Ÿçš„è¾“å‡ºå¯ä»¥è§£é‡Šä¸ºåŸå‹æ¦‚å¿µçš„å‡¸ç»„åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDINOv2åœ¨ä¸åŒä»»åŠ¡ä¸­å±•ç°å‡ºåŠŸèƒ½ä¸“ä¸šåŒ–ï¼Œä¾‹å¦‚ï¼Œåˆ†ç±»ä»»åŠ¡åˆ©ç”¨â€œElsewhereâ€æ¦‚å¿µå®ç°å­¦ä¹ åˆ°çš„å¦å®šï¼Œåˆ†å‰²ä»»åŠ¡ä¾èµ–äºè¾¹ç•Œæ£€æµ‹å™¨ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°DINOv2çš„è¡¨å¾å…·æœ‰éçº¿æ€§ç¨€ç–æ€§å’ŒMinkowskiå‡ ä½•ç‰¹æ€§ï¼Œæ”¯æŒäº†Minkowskiè¡¨å¾å‡è®¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡è§†è§‰Transformeræ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡ç†è§£æ¨¡å‹å†…éƒ¨çš„æ¦‚å¿µï¼Œå¯ä»¥æ›´å¥½åœ°è¿›è¡Œæ¨¡å‹è°ƒè¯•å’Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›å¯¹è§†è§‰è®¤çŸ¥çš„ç†è§£ï¼Œä¸ºå¼€å‘æ›´æ™ºèƒ½çš„è§†è§‰ç³»ç»Ÿæä¾›ç†è®ºåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet the nature of what it perceives remains unknown. As a working baseline, we adopt the Linear Representation Hypothesis (LRH) and operationalize it using SAEs, producing a 32,000-unit dictionary that serves as the interpretability backbone of our study, which unfolds in three parts.
>   In the first part, we analyze how different downstream tasks recruit concepts from our learned dictionary, revealing functional specialization: classification exploits "Elsewhere" concepts that fire everywhere except on target objects, implementing learned negations; segmentation relies on boundary detectors forming coherent subspaces; depth estimation draws on three distinct monocular depth cues matching visual neuroscience principles.
>   Following these functional results, we analyze the geometry and statistics of the concepts learned by the SAE. We found that representations are partly dense rather than strictly sparse. The dictionary evolves toward greater coherence and departs from maximally orthogonal ideals (Grassmannian frames). Within an image, tokens occupy a low dimensional, locally connected set persisting after removing position. These signs suggest representations are organized beyond linear sparsity alone.
>   Synthesizing these observations, we propose a refined view: tokens are formed by combining convex mixtures of archetypes (e.g., a rabbit among animals, brown among colors, fluffy among textures). This structure is grounded in Gardenfors' conceptual spaces and in the model's mechanism as multi-head attention produces sums of convex mixtures, defining regions bounded by archetypes. We introduce the Minkowski Representation Hypothesis (MRH) and examine its empirical signatures and implications for interpreting vision-transformer representations.

