---
layout: default
title: GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping
---

# GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22319" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22319v2</a>
  <a href="https://arxiv.org/pdf/2510.22319.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22319v2" onclick="toggleFavorite(this, '2510.22319v2', 'GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jing Wang, Jiajun Liang, Jie Liu, Henglin Liu, Gongye Liu, Jun Zheng, Wanyuan Pang, Ao Ma, Zhenyu Xie, Xintao Wang, Meng Wang, Pengfei Wan, Xiaodan Liang

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-25 (æ›´æ–°: 2025-10-30)

**å¤‡æ³¨**: Project Page: https://jingw193.github.io/GRPO-Guard/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GRPO-Guardï¼šé€šè¿‡è°ƒèŠ‚è£å‰ªç¼“è§£Flow Matchingä¸­çš„éšå¼è¿‡åº¦ä¼˜åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Flow Matching` `å¼ºåŒ–å­¦ä¹ ` `è¿‡åº¦ä¼˜åŒ–` `æ‰©æ•£æ¨¡å‹` `ç”Ÿæˆæ¨¡å‹` `é‡è¦æ€§é‡‡æ ·` `æ¢¯åº¦è£å‰ª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºGRPOçš„Flow Matchingæ–¹æ³•å­˜åœ¨éšå¼è¿‡åº¦ä¼˜åŒ–é—®é¢˜ï¼Œå¯¼è‡´å›¾åƒè´¨é‡å’Œæ–‡æœ¬å¯¹é½ç­‰å…³é”®æŒ‡æ ‡ä¸‹é™ã€‚
2. GRPO-Guardé€šè¿‡æ¯”ç‡å½’ä¸€åŒ–å’Œæ¢¯åº¦é‡åŠ æƒï¼Œå®ç°è°ƒèŠ‚è£å‰ªæœºåˆ¶ï¼Œç¨³å®šä¼˜åŒ–è¿‡ç¨‹ï¼Œç¼“è§£è¿‡åº¦ä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGRPO-Guardåœ¨å¤šä¸ªæ‰©æ•£æ¨¡å‹å’Œä»£ç†ä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—å‡å°‘è¿‡åº¦ä¼˜åŒ–ï¼Œå¹¶ä¿æŒæˆ–æå‡ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ€è¿‘ï¼ŒåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–flow-matchingæ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹ä¸ç‰¹å®šä»»åŠ¡å¥–åŠ±çš„å¯¹é½ç¨‹åº¦ã€‚åœ¨è¿™äº›æ¡†æ¶ä¸­ï¼Œç­–ç•¥æ›´æ–°ä¾èµ–äºé‡è¦æ€§æ¯”ç‡è£å‰ªæ¥çº¦æŸè¿‡åº¦è‡ªä¿¡çš„æ­£æ¢¯åº¦å’Œè´Ÿæ¢¯åº¦ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é‡è¦æ€§æ¯”ç‡åˆ†å¸ƒçš„ç³»ç»Ÿæ€§åç§»â€”â€”å…¶å‡å€¼ä½äº1ï¼Œå¹¶ä¸”å…¶æ–¹å·®åœ¨ä¸åŒæ—¶é—´æ­¥é•¿ä¸Šå·®å¼‚å¾ˆå¤§ã€‚è¿™ç§å·¦ç§»ä¸”ä¸ä¸€è‡´çš„åˆ†å¸ƒé˜»æ­¢äº†æ­£ä¼˜åŠ¿æ ·æœ¬è¿›å…¥è£å‰ªåŒºåŸŸï¼Œå¯¼è‡´è¯¥æœºåˆ¶æ— æ³•çº¦æŸè¿‡åº¦è‡ªä¿¡çš„æ­£å‘æ›´æ–°ã€‚å› æ­¤ï¼Œç­–ç•¥æ¨¡å‹ä¸å¯é¿å…åœ°è¿›å…¥éšå¼è¿‡åº¦ä¼˜åŒ–é˜¶æ®µâ€”â€”è™½ç„¶ä»£ç†å¥–åŠ±æŒç»­å¢åŠ ï¼Œä½†å›¾åƒè´¨é‡å’Œæ–‡æœ¬æç¤ºå¯¹é½ç­‰å…³é”®æŒ‡æ ‡æ€¥å‰§ä¸‹é™ï¼Œæœ€ç»ˆä½¿å¾—å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨å®é™…åº”ç”¨ä¸­å˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GRPO-Guardï¼Œè¿™æ˜¯å¯¹ç°æœ‰GRPOæ¡†æ¶çš„ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„å¢å¼ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æ¯”ç‡å½’ä¸€åŒ–ï¼Œå®ƒæ¢å¤äº†å¹³è¡¡ä¸”æ­¥é•¿ä¸€è‡´çš„é‡è¦æ€§æ¯”ç‡ï¼Œç¡®ä¿PPOè£å‰ªèƒ½å¤Ÿæ­£ç¡®åœ°çº¦æŸå»å™ªæ—¶é—´æ­¥é•¿ä¸Šçš„æœ‰å®³æ›´æ–°ã€‚æ­¤å¤–ï¼Œæ¢¯åº¦é‡åŠ æƒç­–ç•¥å‡è¡¡äº†å™ªå£°æ¡ä»¶ä¸‹çš„ç­–ç•¥æ¢¯åº¦ï¼Œé˜²æ­¢æ¥è‡ªç‰¹å®šæ—¶é—´æ­¥é•¿åŒºåŸŸçš„è¿‡åº¦æ›´æ–°ã€‚æ€»ä¹‹ï¼Œè¿™äº›è®¾è®¡å……å½“äº†ä¸€ç§è°ƒèŠ‚è£å‰ªæœºåˆ¶ï¼Œç¨³å®šäº†ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶åœ¨ä¸ä¾èµ–äºç¹é‡çš„KLæ­£åˆ™åŒ–çš„æƒ…å†µä¸‹ï¼Œå¤§å¤§ç¼“è§£äº†éšå¼è¿‡åº¦ä¼˜åŒ–ã€‚åœ¨å¤šä¸ªæ‰©æ•£éª¨å¹²ç½‘ç»œï¼ˆä¾‹å¦‚ï¼ŒSD3.5Mï¼ŒFlux.1-devï¼‰å’Œå„ç§ä»£ç†ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGRPO-Guardæ˜¾è‘—å‡å°‘äº†è¿‡åº¦ä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸºäºGRPOçš„Flow Matchingæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„éšå¼è¿‡åº¦ä¼˜åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–é‡è¦æ€§æ¯”ç‡è£å‰ªæ¥çº¦æŸæ¢¯åº¦æ›´æ–°ï¼Œä½†å®é™…ä¸­é‡è¦æ€§æ¯”ç‡åˆ†å¸ƒå­˜åœ¨åç§»å’Œä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´è£å‰ªæœºåˆ¶å¤±æ•ˆï¼Œæ¨¡å‹åœ¨ä»£ç†å¥–åŠ±æå‡çš„åŒæ—¶ï¼Œå›¾åƒè´¨é‡ç­‰å…³é”®æŒ‡æ ‡ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è°ƒèŠ‚è£å‰ªæœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°çº¦æŸæœ‰å®³çš„æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œé¿å…éšå¼è¿‡åº¦ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ¢å¤å¹³è¡¡ä¸”æ­¥é•¿ä¸€è‡´çš„é‡è¦æ€§æ¯”ç‡ï¼Œå¹¶å‡è¡¡å™ªå£°æ¡ä»¶ä¸‹çš„ç­–ç•¥æ¢¯åº¦ï¼Œæ¥ç¨³å®šä¼˜åŒ–è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRPO-Guardæ˜¯å¯¹ç°æœ‰GRPOæ¡†æ¶çš„å¢å¼ºï¼Œä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šæ¯”ç‡å½’ä¸€åŒ–å’Œæ¢¯åº¦é‡åŠ æƒã€‚æ¯”ç‡å½’ä¸€åŒ–ç”¨äºæ¢å¤é‡è¦æ€§æ¯”ç‡çš„å¹³è¡¡å’Œä¸€è‡´æ€§ï¼Œæ¢¯åº¦é‡åŠ æƒç”¨äºå‡è¡¡ä¸åŒå™ªå£°æ¡ä»¶ä¸‹çš„ç­–ç•¥æ¢¯åº¦ã€‚è¿™ä¸¤ä¸ªæ¨¡å—å…±åŒä½œç”¨ï¼Œå½¢æˆä¸€ä¸ªè°ƒèŠ‚è£å‰ªæœºåˆ¶ã€‚æ•´ä½“æµç¨‹æ˜¯åœ¨GRPOæ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œåœ¨è®¡ç®—é‡è¦æ€§æ¯”ç‡å’Œæ›´æ–°ç­–ç•¥æ—¶ï¼ŒåŠ å…¥è¿™ä¸¤ä¸ªæ¨¡å—è¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªè°ƒèŠ‚è£å‰ªæœºåˆ¶ï¼Œé€šè¿‡æ¯”ç‡å½’ä¸€åŒ–å’Œæ¢¯åº¦é‡åŠ æƒï¼Œè§£å†³äº†ç°æœ‰GRPOæ–¹æ³•ä¸­é‡è¦æ€§æ¯”ç‡åˆ†å¸ƒåç§»å’Œä¸ä¸€è‡´çš„é—®é¢˜ï¼Œä»è€Œæœ‰æ•ˆåœ°çº¦æŸäº†æœ‰å®³çš„æ¢¯åº¦æ›´æ–°ï¼Œç¼“è§£äº†éšå¼è¿‡åº¦ä¼˜åŒ–ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGRPO-Guardä¸éœ€è¦ä¾èµ–ç¹é‡çš„KLæ­£åˆ™åŒ–ï¼Œå®ç°æ›´ç¨³å®šçš„ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¯”ç‡å½’ä¸€åŒ–é€šè¿‡å¯¹é‡è¦æ€§æ¯”ç‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶å‡å€¼æ¥è¿‘1ï¼Œæ–¹å·®åœ¨ä¸åŒæ—¶é—´æ­¥é•¿ä¸Šä¿æŒä¸€è‡´ã€‚æ¢¯åº¦é‡åŠ æƒé€šè¿‡å¯¹ä¸åŒå™ªå£°æ¡ä»¶ä¸‹çš„ç­–ç•¥æ¢¯åº¦è¿›è¡ŒåŠ æƒï¼Œä½¿å¾—æ¯ä¸ªå™ªå£°æ¡ä»¶å¯¹ç­–ç•¥æ›´æ–°çš„è´¡çŒ®æ›´åŠ å‡è¡¡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä¾‹å¦‚ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„æ ‡å‡†åŒ–æ–¹æ³•å’ŒåŠ æƒç³»æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPO-Guardåœ¨SD3.5Må’ŒFlux.1-devç­‰å¤šä¸ªæ‰©æ•£æ¨¡å‹ä¸Šï¼Œä»¥åŠå›¾åƒç”Ÿæˆå’Œæ–‡æœ¬ç”Ÿæˆç­‰å¤šä¸ªä»£ç†ä»»åŠ¡ä¸Šï¼Œéƒ½èƒ½å¤Ÿæ˜¾è‘—å‡å°‘è¿‡åº¦ä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æé«˜ç”Ÿæˆè´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼ŒGRPO-Guardåœ¨æŸäº›ä»»åŠ¡ä¸Šèƒ½å¤Ÿå°†å›¾åƒè´¨é‡æŒ‡æ ‡æå‡è¶…è¿‡10%ï¼ŒåŒæ—¶ä¿æŒæ–‡æœ¬å¯¹é½åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GRPO-Guardå¯åº”ç”¨äºå„ç§åŸºäºFlow Matchingçš„ç”Ÿæˆæ¨¡å‹è®­ç»ƒï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜ç”Ÿæˆæ¨¡å‹çš„ç¨³å®šæ€§å’Œç”Ÿæˆè´¨é‡ï¼Œé¿å…è¿‡åº¦ä¼˜åŒ–å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å…¶åœ¨å…¶ä»–å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.

