---
layout: default
title: EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using Self-supervised Foundation Model
---

# EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using Self-supervised Foundation Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.22359" target="_blank" class="toolbar-btn">arXiv: 2510.22359v1</a>
    <a href="https://arxiv.org/pdf/2510.22359.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22359v1" 
            onclick="toggleFavorite(this, '2510.22359v1', 'EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using Self-supervised Foundation Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Changhao Zhang, Matthew J. Clarkson, Mobarak I. Hoque

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-25

**Â§áÊ≥®**: 11 pages

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/MOYF-beta/EndoSfM3D)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**EndoSfM3DÔºöÂà©Áî®Ëá™ÁõëÁù£Âü∫Á°ÄÊ®°ÂûãÂ≠¶‰π†ÂÜÖÁ™•ÈïúÊâãÊúØÂú∫ÊôØÁöÑ3DÈáçÂª∫**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂÜÖÁ™•ÈïúÊâãÊúØ` `3DÈáçÂª∫` `Ëá™ÁõëÁù£Â≠¶‰π†` `ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°` `Âõ∫ÊúâÂèÇÊï∞‰º∞ËÆ°` `Ê∑±Â∫¶Â≠¶‰π†` `ÂßøÊÄÅ‰º∞ËÆ°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂÜÖÁ™•Èïú3DÈáçÂª∫ÊñπÊ≥ïÈöæ‰ª•ÂáÜÁ°Æ‰º∞ËÆ°ÂÜÖÁ™•ÈïúÁöÑÂõ∫ÊúâÂèÇÊï∞ÔºåÈôêÂà∂‰∫ÜÈáçÂª∫ÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫EndoSfM3DÔºåÈÄöËøáËá™ÁõëÁù£Â≠¶‰π†ËÅîÂêàÈ¢ÑÊµãÊ∑±Â∫¶„ÄÅÂßøÊÄÅÂíåÂÜÖÁ™•ÈïúÂõ∫ÊúâÂèÇÊï∞ÔºåÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑ3DÈáçÂª∫„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâËá™ÁõëÁù£ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Âíå3DÈáçÂª∫ÊñπÊ≥ïÔºåÊÄßËÉΩÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂÜÖÁ™•ÈïúÊâãÊúØÂú∫ÊôØÁöÑ3DÈáçÂª∫Âú®Â¢ûÂº∫Âú∫ÊôØÊÑüÁü•„ÄÅÂÆûÁé∞ARÂèØËßÜÂåñ‰ª•ÂèäÊîØÊåÅÂõæÂÉèÂºïÂØºÊâãÊúØ‰∏≠‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÂÜ≥Á≠ñÂà∂ÂÆöÊñπÈù¢Ëµ∑ÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇËØ•ËøáÁ®ã‰∏≠ÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆ‰ΩÜÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊ≠•È™§ÊòØÂáÜÁ°Æ‰º∞ËÆ°ÂÜÖÁ™•ÈïúÁöÑÂõ∫ÊúâÂèÇÊï∞„ÄÇÂú®ÂÆûÈôÖÊâãÊúØÁéØÂ¢É‰∏≠ÔºåÂõ∫ÊúâÂèÇÊï∞ÁöÑÊ†áÂÆöÂèóÂà∞Êó†ËèåÁ∫¶Êùü‰ª•Âèä‰ΩøÁî®ÂÖ∑ÊúâËøûÁª≠ÂèòÁÑ¶ÂíåÊúõËøúÈïúÊóãËΩ¨ÁöÑ‰∏ìÁî®ÂÜÖÁ™•ÈïúÁöÑÈôêÂà∂„ÄÇÂ§ßÂ§öÊï∞Áé∞ÊúâÁöÑÂÜÖÁ™•Èïú3DÈáçÂª∫ÊñπÊ≥ï‰∏ç‰º∞ËÆ°Âõ∫ÊúâÂèÇÊï∞ÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®ÂáÜÁ°ÆÂíåÂèØÈù†ÈáçÂª∫ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÈÄöËøáË∞ÉÊï¥Depth Anything V2 (DA2)Ê®°Âûã‰ª•ËøõË°åËÅîÂêàÊ∑±Â∫¶„ÄÅÂßøÊÄÅÂíåÂõ∫ÊúâÂèÇÊï∞È¢ÑÊµãÔºåÂ∞ÜÂõ∫ÊúâÂèÇÊï∞‰º∞ËÆ°ÈõÜÊàêÂà∞Ëá™ÁõëÁù£ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê°ÜÊû∂‰∏≠„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂßøÊÄÅÁΩëÁªúÂíå‰∏Ä‰∏™ÊùÉÈáçÂàÜËß£‰ΩéÁß©ÈÄÇÂ∫î(DoRA)Á≠ñÁï•ÔºåÁî®‰∫éDA2ÁöÑÊúâÊïàÂæÆË∞É„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®SCAREDÂíåC3VDÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÈ™åËØÅÔºåËØÅÊòé‰∫Ü‰∏éÊúÄËøëÊúÄÂÖàËøõÁöÑËá™ÁõëÁù£ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Âíå3DÈáçÂª∫ÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑Êúâ‰ºòË∂äÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂÜÖÁ™•ÈïúÊâãÊúØÂú∫ÊôØÁöÑ3DÈáçÂª∫ÈúÄË¶ÅÂáÜÁ°ÆÁöÑÂÜÖÁ™•ÈïúÂõ∫ÊúâÂèÇÊï∞Ôºå‰ΩÜÂÆûÈôÖÊâãÊúØ‰∏≠ÔºåÂÜÖÁ™•ÈïúÁöÑÊó†ËèåË¶ÅÊ±ÇÂíåÁâπÊÆäËÆæËÆ°ÔºàËøûÁª≠ÂèòÁÑ¶„ÄÅÊúõËøúÈïúÊóãËΩ¨Ôºâ‰ΩøÂæó‰º†ÁªüÊ†áÂÆöÊñπÊ≥ïÈöæ‰ª•Â∫îÁî®„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÂøΩÁï•ÊàñÁÆÄÂåñÂõ∫ÊúâÂèÇÊï∞‰º∞ËÆ°ÔºåÂØºËá¥ÈáçÂª∫Á≤æÂ∫¶ÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÂú®‰∫éÂ∞ÜÂÜÖÁ™•ÈïúÂõ∫ÊúâÂèÇÊï∞‰º∞ËÆ°ËûçÂÖ•Âà∞Ëá™ÁõëÁù£ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê°ÜÊû∂‰∏≠ÔºåÈÄöËøáÊ∑±Â∫¶„ÄÅÂßøÊÄÅÂíåÂõ∫ÊúâÂèÇÊï∞ÁöÑËÅîÂêàÈ¢ÑÊµãÔºåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑ3DÈáçÂª∫„ÄÇÂà©Áî®Ëá™ÁõëÁù£Â≠¶‰π†ÈÅøÂÖç‰∫ÜÂØπÁúüÂÆûÊ∑±Â∫¶‰ø°ÊÅØÁöÑ‰æùËµñÔºåÈÄÇÁî®‰∫éÂÜÖÁ™•ÈïúÊâãÊúØÂú∫ÊôØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEndoSfM3DÂü∫‰∫éDepth Anything V2 (DA2)Ê®°ÂûãÔºåÂπ∂ÂØπÂÖ∂ËøõË°åÊîπËøõ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Ôºö1) ‰ΩøÁî®DA2Ê®°Âûã‰Ωú‰∏∫backboneÊèêÂèñÂõæÂÉèÁâπÂæÅÔºõ2) Âà©Áî®ÊîπËøõÁöÑÂßøÊÄÅÁΩëÁªúÈ¢ÑÊµãÁõ∏Êú∫ÂßøÊÄÅÔºõ3) ËÅîÂêàÈ¢ÑÊµãÊ∑±Â∫¶ÂõæÂíåÂÜÖÁ™•ÈïúÂõ∫ÊúâÂèÇÊï∞Ôºõ4) ‰ΩøÁî®Ëá™ÁõëÁù£ÊçüÂ§±ÂáΩÊï∞ËøõË°åËÆ≠ÁªÉÔºå‰ºòÂåñÊ∑±Â∫¶„ÄÅÂßøÊÄÅÂíåÂõ∫ÊúâÂèÇÊï∞ÁöÑÈ¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**Ôºö‰∏ªË¶ÅÂàõÊñ∞ÁÇπÂú®‰∫éÔºö1) Â∞ÜÂÜÖÁ™•ÈïúÂõ∫ÊúâÂèÇÊï∞‰º∞ËÆ°ÈõÜÊàêÂà∞Ëá™ÁõëÁù£ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê°ÜÊû∂‰∏≠Ôºõ2) ÂºïÂÖ•Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂßøÊÄÅÁΩëÁªúÔºåÊèêÂçáÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÔºõ3) ÈááÁî®ÊùÉÈáçÂàÜËß£‰ΩéÁß©ÈÄÇÂ∫î(DoRA)Á≠ñÁï•ÔºåÈ´òÊïàÂú∞ÂæÆË∞ÉDA2Ê®°ÂûãÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö1) ÂßøÊÄÅÁΩëÁªúÔºöÈááÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂ¢ûÂº∫ÂØπÂÖ≥ÈîÆÁâπÂæÅÁöÑÂÖ≥Ê≥®ÔºåÊèêÈ´òÂßøÊÄÅ‰º∞ËÆ°Á≤æÂ∫¶„ÄÇ2) DoRAÔºöÈÄöËøáÂàÜËß£ÊùÉÈáçÁü©ÈòµÔºåÂè™ËÆ≠ÁªÉÂ∞ëÈáèÂèÇÊï∞ÔºåÈôç‰Ωé‰∫ÜÂæÆË∞ÉDA2Ê®°ÂûãÁöÑËÆ°ÁÆóÈáè„ÄÇ3) Ëá™ÁõëÁù£ÊçüÂ§±ÂáΩÊï∞ÔºöÁªìÂêàÂÖâÂ∫¶‰∏ÄËá¥ÊÄßÊçüÂ§±„ÄÅÊ∑±Â∫¶‰∏ÄËá¥ÊÄßÊçüÂ§±Á≠âÔºåÁ∫¶ÊùüÊ∑±Â∫¶„ÄÅÂßøÊÄÅÂíåÂõ∫ÊúâÂèÇÊï∞ÁöÑÈ¢ÑÊµã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EndoSfM3DÂú®SCAREDÂíåC3VDÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÈ™åËØÅÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ëá™ÁõëÁù£ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Âíå3DÈáçÂª∫‰ªªÂä°‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÂú®ËÆ∫Êñá‰∏≠ÁªôÂá∫ÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÂú®ÂÜÖÁ™•ÈïúÊâãÊúØÂú∫ÊôØ3DÈáçÂª∫ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ¢ûÂº∫ÂÜÖÁ™•ÈïúÊâãÊúØÁöÑÂú∫ÊôØÊÑüÁü•ÔºåÂÆûÁé∞ARÂèØËßÜÂåñÔºåËæÖÂä©ÂåªÁîüËøõË°åÊõ¥Á≤æÁ°ÆÁöÑÊâãÊúØÊìç‰Ωú„ÄÇÈÄöËøáÊèê‰æõ‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑ3D‰ø°ÊÅØÔºåÂ∏ÆÂä©ÂåªÁîüÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÂÜ≥Á≠ñÔºåÊèêÈ´òÊâãÊúØÊàêÂäüÁéáÔºåÂπ∂ÂèØÁî®‰∫éÊúØÂâçËßÑÂàíÂíåÊúØÂêéËØÑ‰º∞„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÈõÜÊàêÂà∞Êô∫ËÉΩÊâãÊúØÊú∫Âô®‰∫∫Á≥ªÁªü‰∏≠ÔºåÂÆûÁé∞Êõ¥È´òÁ∫ßÁöÑËá™Âä®ÂåñÊâãÊúØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> 3D reconstruction of endoscopic surgery scenes plays a vital role in enhancing scene perception, enabling AR visualization, and supporting context-aware decision-making in image-guided surgery. A critical yet challenging step in this process is the accurate estimation of the endoscope's intrinsic parameters. In real surgical settings, intrinsic calibration is hindered by sterility constraints and the use of specialized endoscopes with continuous zoom and telescope rotation. Most existing methods for endoscopic 3D reconstruction do not estimate intrinsic parameters, limiting their effectiveness for accurate and reliable reconstruction. In this paper, we integrate intrinsic parameter estimation into a self-supervised monocular depth estimation framework by adapting the Depth Anything V2 (DA2) model for joint depth, pose, and intrinsics prediction. We introduce an attention-based pose network and a Weight-Decomposed Low-Rank Adaptation (DoRA) strategy for efficient fine-tuning of DA2. Our method is validated on the SCARED and C3VD public datasets, demonstrating superior performance compared to recent state-of-the-art approaches in self-supervised monocular depth estimation and 3D reconstruction. Code and model weights can be found in project repository: https://github.com/MOYF-beta/EndoSfM3D.

