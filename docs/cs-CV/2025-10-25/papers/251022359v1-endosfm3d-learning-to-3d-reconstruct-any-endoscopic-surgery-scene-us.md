---
layout: default
title: "EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using Self-supervised Foundation Model"
---

# EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using Self-supervised Foundation Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22359" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22359v1</a>
  <a href="https://arxiv.org/pdf/2510.22359.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22359v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22359v1', 'EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using Self-supervised Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changhao Zhang, Matthew J. Clarkson, Mobarak I. Hoque

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-25

**å¤‡æ³¨**: 11 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MOYF-beta/EndoSfM3D)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EndoSfM3Dï¼šåˆ©ç”¨è‡ªç›‘ç£åŸºç¡€æ¨¡å‹å­¦ä¹ å†…çª¥é•œæ‰‹æœ¯åœºæ™¯çš„3Dé‡å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å†…çª¥é•œæ‰‹æœ¯` `3Dé‡å»º` `è‡ªç›‘ç£å­¦ä¹ ` `å•ç›®æ·±åº¦ä¼°è®¡` `å›ºæœ‰å‚æ•°ä¼°è®¡` `æ·±åº¦å­¦ä¹ ` `å§¿æ€ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å†…çª¥é•œ3Dé‡å»ºæ–¹æ³•éš¾ä»¥å‡†ç¡®ä¼°è®¡å†…çª¥é•œçš„å›ºæœ‰å‚æ•°ï¼Œé™åˆ¶äº†é‡å»ºçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚
2. è®ºæ–‡æå‡ºEndoSfM3Dï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ è”åˆé¢„æµ‹æ·±åº¦ã€å§¿æ€å’Œå†…çª¥é•œå›ºæœ‰å‚æ•°ï¼Œå®ç°æ›´ç²¾ç¡®çš„3Dé‡å»ºã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¬å…±æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡å’Œ3Dé‡å»ºæ–¹æ³•ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å†…çª¥é•œæ‰‹æœ¯åœºæ™¯çš„3Dé‡å»ºåœ¨å¢å¼ºåœºæ™¯æ„ŸçŸ¥ã€å®ç°ARå¯è§†åŒ–ä»¥åŠæ”¯æŒå›¾åƒå¼•å¯¼æ‰‹æœ¯ä¸­ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å†³ç­–åˆ¶å®šæ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è¯¥è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªå…³é”®ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ­¥éª¤æ˜¯å‡†ç¡®ä¼°è®¡å†…çª¥é•œçš„å›ºæœ‰å‚æ•°ã€‚åœ¨å®é™…æ‰‹æœ¯ç¯å¢ƒä¸­ï¼Œå›ºæœ‰å‚æ•°çš„æ ‡å®šå—åˆ°æ— èŒçº¦æŸä»¥åŠä½¿ç”¨å…·æœ‰è¿ç»­å˜ç„¦å’Œæœ›è¿œé•œæ—‹è½¬çš„ä¸“ç”¨å†…çª¥é•œçš„é™åˆ¶ã€‚å¤§å¤šæ•°ç°æœ‰çš„å†…çª¥é•œ3Dé‡å»ºæ–¹æ³•ä¸ä¼°è®¡å›ºæœ‰å‚æ•°ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å‡†ç¡®å’Œå¯é é‡å»ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´Depth Anything V2 (DA2)æ¨¡å‹ä»¥è¿›è¡Œè”åˆæ·±åº¦ã€å§¿æ€å’Œå›ºæœ‰å‚æ•°é¢„æµ‹ï¼Œå°†å›ºæœ‰å‚æ•°ä¼°è®¡é›†æˆåˆ°è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ä¸­ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›çš„å§¿æ€ç½‘ç»œå’Œä¸€ä¸ªæƒé‡åˆ†è§£ä½ç§©é€‚åº”(DoRA)ç­–ç•¥ï¼Œç”¨äºDA2çš„æœ‰æ•ˆå¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨SCAREDå’ŒC3VDå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜äº†ä¸æœ€è¿‘æœ€å…ˆè¿›çš„è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡å’Œ3Dé‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå†…çª¥é•œæ‰‹æœ¯åœºæ™¯çš„3Dé‡å»ºéœ€è¦å‡†ç¡®çš„å†…çª¥é•œå›ºæœ‰å‚æ•°ï¼Œä½†å®é™…æ‰‹æœ¯ä¸­ï¼Œå†…çª¥é•œçš„æ— èŒè¦æ±‚å’Œç‰¹æ®Šè®¾è®¡ï¼ˆè¿ç»­å˜ç„¦ã€æœ›è¿œé•œæ—‹è½¬ï¼‰ä½¿å¾—ä¼ ç»Ÿæ ‡å®šæ–¹æ³•éš¾ä»¥åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥æˆ–ç®€åŒ–å›ºæœ‰å‚æ•°ä¼°è®¡ï¼Œå¯¼è‡´é‡å»ºç²¾åº¦å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒåœ¨äºå°†å†…çª¥é•œå›ºæœ‰å‚æ•°ä¼°è®¡èå…¥åˆ°è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ä¸­ï¼Œé€šè¿‡æ·±åº¦ã€å§¿æ€å’Œå›ºæœ‰å‚æ•°çš„è”åˆé¢„æµ‹ï¼Œå®ç°æ›´å‡†ç¡®çš„3Dé‡å»ºã€‚åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ é¿å…äº†å¯¹çœŸå®æ·±åº¦ä¿¡æ¯çš„ä¾èµ–ï¼Œé€‚ç”¨äºå†…çª¥é•œæ‰‹æœ¯åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEndoSfM3DåŸºäºDepth Anything V2 (DA2)æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ”¹è¿›ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨DA2æ¨¡å‹ä½œä¸ºbackboneæå–å›¾åƒç‰¹å¾ï¼›2) åˆ©ç”¨æ”¹è¿›çš„å§¿æ€ç½‘ç»œé¢„æµ‹ç›¸æœºå§¿æ€ï¼›3) è”åˆé¢„æµ‹æ·±åº¦å›¾å’Œå†…çª¥é•œå›ºæœ‰å‚æ•°ï¼›4) ä½¿ç”¨è‡ªç›‘ç£æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–æ·±åº¦ã€å§¿æ€å’Œå›ºæœ‰å‚æ•°çš„é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šä¸»è¦åˆ›æ–°ç‚¹åœ¨äºï¼š1) å°†å†…çª¥é•œå›ºæœ‰å‚æ•°ä¼°è®¡é›†æˆåˆ°è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ä¸­ï¼›2) å¼•å…¥åŸºäºæ³¨æ„åŠ›çš„å§¿æ€ç½‘ç»œï¼Œæå‡å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ï¼›3) é‡‡ç”¨æƒé‡åˆ†è§£ä½ç§©é€‚åº”(DoRA)ç­–ç•¥ï¼Œé«˜æ•ˆåœ°å¾®è°ƒDA2æ¨¡å‹ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼š1) å§¿æ€ç½‘ç»œï¼šé‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºå¯¹å…³é”®ç‰¹å¾çš„å…³æ³¨ï¼Œæé«˜å§¿æ€ä¼°è®¡ç²¾åº¦ã€‚2) DoRAï¼šé€šè¿‡åˆ†è§£æƒé‡çŸ©é˜µï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°ï¼Œé™ä½äº†å¾®è°ƒDA2æ¨¡å‹çš„è®¡ç®—é‡ã€‚3) è‡ªç›‘ç£æŸå¤±å‡½æ•°ï¼šç»“åˆå…‰åº¦ä¸€è‡´æ€§æŸå¤±ã€æ·±åº¦ä¸€è‡´æ€§æŸå¤±ç­‰ï¼Œçº¦æŸæ·±åº¦ã€å§¿æ€å’Œå›ºæœ‰å‚æ•°çš„é¢„æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EndoSfM3Dåœ¨SCAREDå’ŒC3VDå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡å’Œ3Dé‡å»ºä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å†…çª¥é•œæ‰‹æœ¯åœºæ™¯3Dé‡å»ºæ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¢å¼ºå†…çª¥é•œæ‰‹æœ¯çš„åœºæ™¯æ„ŸçŸ¥ï¼Œå®ç°ARå¯è§†åŒ–ï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œæ›´ç²¾ç¡®çš„æ‰‹æœ¯æ“ä½œã€‚é€šè¿‡æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„3Dä¿¡æ¯ï¼Œå¸®åŠ©åŒ»ç”Ÿåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ï¼Œæé«˜æ‰‹æœ¯æˆåŠŸç‡ï¼Œå¹¶å¯ç”¨äºæœ¯å‰è§„åˆ’å’Œæœ¯åè¯„ä¼°ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›é›†æˆåˆ°æ™ºèƒ½æ‰‹æœ¯æœºå™¨äººç³»ç»Ÿä¸­ï¼Œå®ç°æ›´é«˜çº§çš„è‡ªåŠ¨åŒ–æ‰‹æœ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D reconstruction of endoscopic surgery scenes plays a vital role in enhancing scene perception, enabling AR visualization, and supporting context-aware decision-making in image-guided surgery. A critical yet challenging step in this process is the accurate estimation of the endoscope's intrinsic parameters. In real surgical settings, intrinsic calibration is hindered by sterility constraints and the use of specialized endoscopes with continuous zoom and telescope rotation. Most existing methods for endoscopic 3D reconstruction do not estimate intrinsic parameters, limiting their effectiveness for accurate and reliable reconstruction. In this paper, we integrate intrinsic parameter estimation into a self-supervised monocular depth estimation framework by adapting the Depth Anything V2 (DA2) model for joint depth, pose, and intrinsics prediction. We introduce an attention-based pose network and a Weight-Decomposed Low-Rank Adaptation (DoRA) strategy for efficient fine-tuning of DA2. Our method is validated on the SCARED and C3VD public datasets, demonstrating superior performance compared to recent state-of-the-art approaches in self-supervised monocular depth estimation and 3D reconstruction. Code and model weights can be found in project repository: https://github.com/MOYF-beta/EndoSfM3D.

