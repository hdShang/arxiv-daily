---
layout: default
title: MOGRAS: Human Motion with Grasping in 3D Scenes
---

# MOGRAS: Human Motion with Grasping in 3D Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22199" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22199v1</a>
  <a href="https://arxiv.org/pdf/2510.22199.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22199v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22199v1', 'MOGRAS: Human Motion with Grasping in 3D Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kunal Bhosikar, Siddharth Katageri, Vivek Madhavaram, Kai Han, Charu Sharma

**åˆ†ç±»**: cs.CV, cs.GR, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-25

**å¤‡æ³¨**: British Machine Vision Conference Workshop - From Scene Understanding to Human Modeling

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MOGRASï¼šæå‡ºå¤§è§„æ¨¡3Dåœºæ™¯ä¸­äººä½“æŠ“å–äº¤äº’è¿åŠ¨æ•°æ®é›†ä¸åŸºå‡†æ–¹æ³•ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `äººä½“è¿åŠ¨ç”Ÿæˆ` `ç‰©ä½“æŠ“å–` `3Dåœºæ™¯` `æ•°æ®é›†` `äººæœºäº¤äº’` `æœºå™¨äºº` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…¨èº«è¿åŠ¨ç”Ÿæˆæ–¹æ³•éš¾ä»¥å…¼é¡¾3Dåœºæ™¯æ„ŸçŸ¥å’Œç²¾ç»†çš„ç‰©ä½“æŠ“å–åŠ¨ä½œï¼Œå¯¼è‡´äº¤äº’ä¸è‡ªç„¶ã€‚
2. MOGRASæ•°æ®é›†æä¾›å¤§è§„æ¨¡çš„3Dåœºæ™¯ä¸­äººä½“æŠ“å–äº¤äº’æ•°æ®ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°ç›¸å…³ç®—æ³•ã€‚
3. è®ºæ–‡æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„é€‚é…æ–¹æ³•ï¼Œä½¿ç°æœ‰æŠ“å–ç®—æ³•èƒ½å¤Ÿåœ¨3Dåœºæ™¯ä¸­æ›´å¥½åœ°å·¥ä½œï¼Œæå‡äº¤äº’çœŸå®æ„Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”Ÿæˆä¸ç‰©ä½“äº¤äº’çš„é€¼çœŸå…¨èº«è¿åŠ¨å¯¹äºæœºå™¨äººã€è™šæ‹Ÿç°å®å’Œäººæœºäº¤äº’è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶èƒ½åœ¨3Dåœºæ™¯ä¸­ç”Ÿæˆå…¨èº«è¿åŠ¨ï¼Œä½†ç¼ºä¹ç‰©ä½“æŠ“å–ç­‰ç²¾ç»†ä»»åŠ¡æ‰€éœ€çš„ä¿çœŸåº¦ã€‚å¦ä¸€æ–¹é¢ï¼Œç”Ÿæˆç²¾ç¡®æŠ“å–è¿åŠ¨çš„æ–¹æ³•é€šå¸¸å¿½ç•¥å‘¨å›´çš„3Dåœºæ™¯ã€‚ä¸ºäº†è§£å†³åœ¨3Dåœºæ™¯ä¸­ç”Ÿæˆç¬¦åˆç‰©ç†è§„å¾‹çš„å…¨èº«æŠ“å–è¿åŠ¨è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MOGRASï¼ˆ3Dåœºæ™¯ä¸­äººä½“è¿åŠ¨ä¸æŠ“å–ï¼‰å¤§å‹æ•°æ®é›†ã€‚MOGRASæä¾›äº†åœ¨ä¸°å¯Œæ ‡æ³¨çš„3Då®¤å†…åœºæ™¯ä¸­è¿›è¡ŒæŠ“å–å‰çš„å…¨èº«è¡Œèµ°è¿åŠ¨å’Œæœ€ç»ˆæŠ“å–å§¿åŠ¿ã€‚æˆ‘ä»¬åˆ©ç”¨MOGRASæ¥è¯„ä¼°ç°æœ‰å…¨èº«æŠ“å–æ–¹æ³•ï¼Œå¹¶å±•ç¤ºå®ƒä»¬åœ¨åœºæ™¯æ„ŸçŸ¥ç”Ÿæˆæ–¹é¢çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥è°ƒæ•´ç°æœ‰æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨3Dåœºæ™¯ä¸­æ— ç¼å·¥ä½œã€‚é€šè¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼ºè°ƒäº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ‰€å–å¾—çš„æ˜¾è‘—æ”¹è¿›ï¼Œä¸ºæ›´é€¼çœŸçš„äººæœºäº¤äº’é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆä¸3Dåœºæ™¯äº¤äº’çš„å…¨èº«æŠ“å–è¿åŠ¨æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸€æ–¹é¢ï¼Œå…¨èº«è¿åŠ¨ç”Ÿæˆæ–¹æ³•é€šå¸¸éš¾ä»¥ä¿è¯æŠ“å–åŠ¨ä½œçš„ç²¾ç¡®æ€§å’ŒçœŸå®æ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸“æ³¨äºæŠ“å–åŠ¨ä½œç”Ÿæˆçš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†å‘¨å›´çš„3Dåœºæ™¯ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¿åŠ¨ä¸åœºæ™¯ä¸åè°ƒã€‚å› æ­¤ï¼Œå¦‚ä½•ç”Ÿæˆåœ¨3Dåœºæ™¯ä¸­ç¬¦åˆç‰©ç†è§„å¾‹çš„å…¨èº«æŠ“å–è¿åŠ¨æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„ã€åŒ…å«ä¸°å¯Œ3Dåœºæ™¯ä¿¡æ¯å’Œäººä½“æŠ“å–äº¤äº’è¿åŠ¨çš„æ•°æ®é›†MOGRASã€‚é€šè¿‡è¿™ä¸ªæ•°æ®é›†ï¼Œå¯ä»¥è®­ç»ƒå’Œè¯„ä¼°èƒ½å¤Ÿæ„ŸçŸ¥åœºæ™¯å¹¶ç”Ÿæˆè‡ªç„¶æŠ“å–è¿åŠ¨çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„é€‚é…æ–¹æ³•ï¼Œä½¿å¾—ç°æœ‰çš„æŠ“å–è¿åŠ¨ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°èå…¥3Dåœºæ™¯ä¸­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMOGRASæ•°æ®é›†åŒ…å«ä¸¤éƒ¨åˆ†ï¼šæŠ“å–å‰çš„å…¨èº«è¡Œèµ°è¿åŠ¨å’Œæœ€ç»ˆçš„æŠ“å–å§¿åŠ¿ã€‚è®ºæ–‡åˆ©ç”¨è¯¥æ•°æ®é›†å¯¹ç°æœ‰æ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æå‡ºä¸€ç§é€‚é…æ–¹æ³•ã€‚è¯¥é€‚é…æ–¹æ³•å¯èƒ½åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼šé¦–å…ˆï¼Œåˆ©ç”¨åœºæ™¯ä¿¡æ¯å¯¹æŠ“å–è¿åŠ¨è¿›è¡Œçº¦æŸï¼Œä¾‹å¦‚é¿å…ç©¿é€ç­‰ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¾®è°ƒæˆ–è¿ç§»å­¦ä¹ çš„æ–¹å¼ï¼Œä½¿ç°æœ‰æ¨¡å‹é€‚åº”æ–°çš„æ•°æ®é›†å’Œåœºæ™¯ã€‚æœ€åï¼Œå¯¹ç”Ÿæˆçš„è¿åŠ¨è¿›è¡Œåå¤„ç†ï¼Œä»¥ä¿è¯å…¶å¹³æ»‘æ€§å’ŒçœŸå®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†MOGRASæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¡«è¡¥äº†ç°æœ‰ç ”ç©¶åœ¨3Dåœºæ™¯ä¸­äººä½“æŠ“å–äº¤äº’è¿åŠ¨æ•°æ®æ–¹é¢çš„ç©ºç™½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºçš„é€‚é…æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ç°æœ‰æŠ“å–è¿åŠ¨ç”Ÿæˆæ¨¡å‹åº”ç”¨äº3Dåœºæ™¯ä¸­ï¼Œæé«˜äº†ç”Ÿæˆè¿åŠ¨çš„çœŸå®æ€§å’Œè‡ªç„¶æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹å¯èƒ½åŒ…å«ä»¥ä¸‹è®¾è®¡ï¼šæ•°æ®é›†çš„æ ‡æ³¨æ–¹å¼ï¼ŒåŒ…æ‹¬äººä½“å§¿æ€ã€ç‰©ä½“ä½å§¿ã€åœºæ™¯è¯­ä¹‰ä¿¡æ¯ç­‰ï¼›é€‚é…æ–¹æ³•çš„å…·ä½“å®ç°ï¼Œä¾‹å¦‚æŸå¤±å‡½æ•°çš„è®¾è®¡ã€ç½‘ç»œç»“æ„çš„è°ƒæ•´ç­‰ï¼›åå¤„ç†ç®—æ³•çš„è®¾è®¡ï¼Œä¾‹å¦‚è¿åŠ¨å¹³æ»‘ã€ç¢°æ’æ£€æµ‹ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†MOGRASæ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†æå‡ºçš„é€‚é…æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜ç°æœ‰æŠ“å–è¿åŠ¨ç”Ÿæˆæ¨¡å‹åœ¨3Dåœºæ™¯ä¸­çš„æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å®éªŒç»“æœçš„æ˜¾è‘—æ”¹è¿›ï¼Œè¡¨æ˜è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººã€è™šæ‹Ÿç°å®å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡ŒæŠ“å–ä»»åŠ¡ã€‚åœ¨è™šæ‹Ÿç°å®é¢†åŸŸï¼Œå¯ä»¥ç”Ÿæˆæ›´é€¼çœŸçš„äººä½“è¿åŠ¨ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚åœ¨äººæœºäº¤äº’é¢†åŸŸï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’æ–¹å¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generating realistic full-body motion interacting with objects is critical for applications in robotics, virtual reality, and human-computer interaction. While existing methods can generate full-body motion within 3D scenes, they often lack the fidelity for fine-grained tasks like object grasping. Conversely, methods that generate precise grasping motions typically ignore the surrounding 3D scene. This gap, generating full-body grasping motions that are physically plausible within a 3D scene, remains a significant challenge. To address this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), a large-scale dataset that bridges this gap. MOGRAS provides pre-grasping full-body walking motions and final grasping poses within richly annotated 3D indoor scenes. We leverage MOGRAS to benchmark existing full-body grasping methods and demonstrate their limitations in scene-aware generation. Furthermore, we propose a simple yet effective method to adapt existing approaches to work seamlessly within 3D scenes. Through extensive quantitative and qualitative experiments, we validate the effectiveness of our dataset and highlight the significant improvements our proposed method achieves, paving the way for more realistic human-scene interactions.

