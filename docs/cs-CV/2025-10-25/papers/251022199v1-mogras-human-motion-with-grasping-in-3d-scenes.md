---
layout: default
title: MOGRAS: Human Motion with Grasping in 3D Scenes
---

# MOGRAS: Human Motion with Grasping in 3D Scenes

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.22199" target="_blank" class="toolbar-btn">arXiv: 2510.22199v1</a>
    <a href="https://arxiv.org/pdf/2510.22199.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22199v1" 
            onclick="toggleFavorite(this, '2510.22199v1', 'MOGRAS: Human Motion with Grasping in 3D Scenes')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kunal Bhosikar, Siddharth Katageri, Vivek Madhavaram, Kai Han, Charu Sharma

**ÂàÜÁ±ª**: cs.CV, cs.GR, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-25

**Â§áÊ≥®**: British Machine Vision Conference Workshop - From Scene Understanding to Human Modeling

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**MOGRASÔºöÊèêÂá∫Â§ßËßÑÊ®°3DÂú∫ÊôØ‰∏≠‰∫∫‰ΩìÊäìÂèñ‰∫§‰∫íËøêÂä®Êï∞ÊçÆÈõÜ‰∏éÂü∫ÂáÜÊñπÊ≥ï„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)** **ÊîØÊü±‰∫îÔºö‰∫§‰∫í‰∏éÂèçÂ∫î (Interaction & Reaction)**

**ÂÖ≥ÈîÆËØç**: `‰∫∫‰ΩìËøêÂä®ÁîüÊàê` `Áâ©‰ΩìÊäìÂèñ` `3DÂú∫ÊôØ` `Êï∞ÊçÆÈõÜ` `‰∫∫Êú∫‰∫§‰∫í` `Êú∫Âô®‰∫∫` `ËôöÊãüÁé∞ÂÆû`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂÖ®Ë∫´ËøêÂä®ÁîüÊàêÊñπÊ≥ïÈöæ‰ª•ÂÖºÈ°æ3DÂú∫ÊôØÊÑüÁü•ÂíåÁ≤æÁªÜÁöÑÁâ©‰ΩìÊäìÂèñÂä®‰ΩúÔºåÂØºËá¥‰∫§‰∫í‰∏çËá™ÁÑ∂„ÄÇ
2. MOGRASÊï∞ÊçÆÈõÜÊèê‰æõÂ§ßËßÑÊ®°ÁöÑ3DÂú∫ÊôØ‰∏≠‰∫∫‰ΩìÊäìÂèñ‰∫§‰∫íÊï∞ÊçÆÔºåÁî®‰∫éËÆ≠ÁªÉÂíåËØÑ‰º∞Áõ∏ÂÖ≥ÁÆóÊ≥ï„ÄÇ
3. ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÈÄÇÈÖçÊñπÊ≥ïÔºå‰ΩøÁé∞ÊúâÊäìÂèñÁÆóÊ≥ïËÉΩÂ§üÂú®3DÂú∫ÊôØ‰∏≠Êõ¥Â•ΩÂú∞Â∑•‰ΩúÔºåÊèêÂçá‰∫§‰∫íÁúüÂÆûÊÑü„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁîüÊàê‰∏éÁâ©‰Ωì‰∫§‰∫íÁöÑÈÄºÁúüÂÖ®Ë∫´ËøêÂä®ÂØπ‰∫éÊú∫Âô®‰∫∫„ÄÅËôöÊãüÁé∞ÂÆûÂíå‰∫∫Êú∫‰∫§‰∫íËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁé∞ÊúâÊñπÊ≥ïËôΩÁÑ∂ËÉΩÂú®3DÂú∫ÊôØ‰∏≠ÁîüÊàêÂÖ®Ë∫´ËøêÂä®Ôºå‰ΩÜÁº∫‰πèÁâ©‰ΩìÊäìÂèñÁ≠âÁ≤æÁªÜ‰ªªÂä°ÊâÄÈúÄÁöÑ‰øùÁúüÂ∫¶„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÁîüÊàêÁ≤æÁ°ÆÊäìÂèñËøêÂä®ÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÂøΩÁï•Âë®Âõ¥ÁöÑ3DÂú∫ÊôØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Âú®3DÂú∫ÊôØ‰∏≠ÁîüÊàêÁ¨¶ÂêàÁâ©ÁêÜËßÑÂæãÁöÑÂÖ®Ë∫´ÊäìÂèñËøêÂä®Ëøô‰∏ÄÈöæÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜMOGRASÔºà3DÂú∫ÊôØ‰∏≠‰∫∫‰ΩìËøêÂä®‰∏éÊäìÂèñÔºâÂ§ßÂûãÊï∞ÊçÆÈõÜ„ÄÇMOGRASÊèê‰æõ‰∫ÜÂú®‰∏∞ÂØåÊ†áÊ≥®ÁöÑ3DÂÆ§ÂÜÖÂú∫ÊôØ‰∏≠ËøõË°åÊäìÂèñÂâçÁöÑÂÖ®Ë∫´Ë°åËµ∞ËøêÂä®ÂíåÊúÄÁªàÊäìÂèñÂßøÂäø„ÄÇÊàë‰ª¨Âà©Áî®MOGRASÊù•ËØÑ‰º∞Áé∞ÊúâÂÖ®Ë∫´ÊäìÂèñÊñπÊ≥ïÔºåÂπ∂Â±ïÁ§∫ÂÆÉ‰ª¨Âú®Âú∫ÊôØÊÑüÁü•ÁîüÊàêÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ïÊù•Ë∞ÉÊï¥Áé∞ÊúâÊñπÊ≥ïÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®3DÂú∫ÊôØ‰∏≠Êó†ÁºùÂ∑•‰Ωú„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÂÆöÈáèÂíåÂÆöÊÄßÂÆûÈ™åÔºåÊàë‰ª¨È™åËØÅ‰∫ÜÊï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄßÔºåÂπ∂Âº∫Ë∞É‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÊâÄÂèñÂæóÁöÑÊòæËëóÊîπËøõÔºå‰∏∫Êõ¥ÈÄºÁúüÁöÑ‰∫∫Êú∫‰∫§‰∫íÈì∫Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÂú®ÁîüÊàê‰∏é3DÂú∫ÊôØ‰∫§‰∫íÁöÑÂÖ®Ë∫´ÊäìÂèñËøêÂä®Êó∂Èù¢‰∏¥ÊåëÊàò„ÄÇ‰∏ÄÊñπÈù¢ÔºåÂÖ®Ë∫´ËøêÂä®ÁîüÊàêÊñπÊ≥ïÈÄöÂ∏∏Èöæ‰ª•‰øùËØÅÊäìÂèñÂä®‰ΩúÁöÑÁ≤æÁ°ÆÊÄßÂíåÁúüÂÆûÊÄß„ÄÇÂè¶‰∏ÄÊñπÈù¢Ôºå‰∏ìÊ≥®‰∫éÊäìÂèñÂä®‰ΩúÁîüÊàêÁöÑÊñπÊ≥ïÂæÄÂæÄÂøΩÁï•‰∫ÜÂë®Âõ¥ÁöÑ3DÂú∫ÊôØÔºåÂØºËá¥ÁîüÊàêÁöÑËøêÂä®‰∏éÂú∫ÊôØ‰∏çÂçèË∞É„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÁîüÊàêÂú®3DÂú∫ÊôØ‰∏≠Á¨¶ÂêàÁâ©ÁêÜËßÑÂæãÁöÑÂÖ®Ë∫´ÊäìÂèñËøêÂä®ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑ„ÄÅÂåÖÂê´‰∏∞ÂØå3DÂú∫ÊôØ‰ø°ÊÅØÂíå‰∫∫‰ΩìÊäìÂèñ‰∫§‰∫íËøêÂä®ÁöÑÊï∞ÊçÆÈõÜMOGRAS„ÄÇÈÄöËøáËøô‰∏™Êï∞ÊçÆÈõÜÔºåÂèØ‰ª•ËÆ≠ÁªÉÂíåËØÑ‰º∞ËÉΩÂ§üÊÑüÁü•Âú∫ÊôØÂπ∂ÁîüÊàêËá™ÁÑ∂ÊäìÂèñËøêÂä®ÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÁöÑÈÄÇÈÖçÊñπÊ≥ïÔºå‰ΩøÂæóÁé∞ÊúâÁöÑÊäìÂèñËøêÂä®ÁîüÊàêÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ËûçÂÖ•3DÂú∫ÊôØ‰∏≠„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMOGRASÊï∞ÊçÆÈõÜÂåÖÂê´‰∏§ÈÉ®ÂàÜÔºöÊäìÂèñÂâçÁöÑÂÖ®Ë∫´Ë°åËµ∞ËøêÂä®ÂíåÊúÄÁªàÁöÑÊäìÂèñÂßøÂäø„ÄÇËÆ∫ÊñáÂà©Áî®ËØ•Êï∞ÊçÆÈõÜÂØπÁé∞ÊúâÊñπÊ≥ïËøõË°åÂü∫ÂáÜÊµãËØïÔºåÂπ∂ÊèêÂá∫‰∏ÄÁßçÈÄÇÈÖçÊñπÊ≥ï„ÄÇËØ•ÈÄÇÈÖçÊñπÊ≥ïÂèØËÉΩÂåÖÂê´‰ª•‰∏ãÊ≠•È™§ÔºöÈ¶ñÂÖàÔºåÂà©Áî®Âú∫ÊôØ‰ø°ÊÅØÂØπÊäìÂèñËøêÂä®ËøõË°åÁ∫¶ÊùüÔºå‰æãÂ¶ÇÈÅøÂÖçÁ©øÈÄèÁ≠â„ÄÇÂÖ∂Ê¨°ÔºåÈÄöËøáÂæÆË∞ÉÊàñËøÅÁßªÂ≠¶‰π†ÁöÑÊñπÂºèÔºå‰ΩøÁé∞ÊúâÊ®°ÂûãÈÄÇÂ∫îÊñ∞ÁöÑÊï∞ÊçÆÈõÜÂíåÂú∫ÊôØ„ÄÇÊúÄÂêéÔºåÂØπÁîüÊàêÁöÑËøêÂä®ËøõË°åÂêéÂ§ÑÁêÜÔºå‰ª•‰øùËØÅÂÖ∂Âπ≥ÊªëÊÄßÂíåÁúüÂÆûÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫ÜMOGRASÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜÂ°´Ë°•‰∫ÜÁé∞ÊúâÁ†îÁ©∂Âú®3DÂú∫ÊôØ‰∏≠‰∫∫‰ΩìÊäìÂèñ‰∫§‰∫íËøêÂä®Êï∞ÊçÆÊñπÈù¢ÁöÑÁ©∫ÁôΩ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáÊèêÂá∫ÁöÑÈÄÇÈÖçÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÁé∞ÊúâÊäìÂèñËøêÂä®ÁîüÊàêÊ®°ÂûãÂ∫îÁî®‰∫é3DÂú∫ÊôØ‰∏≠ÔºåÊèêÈ´ò‰∫ÜÁîüÊàêËøêÂä®ÁöÑÁúüÂÆûÊÄßÂíåËá™ÁÑ∂ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ∑‰ΩìÁöÑÊäÄÊúØÁªÜËäÇÊú™Áü•Ôºå‰ΩÜÂèØ‰ª•Êé®ÊµãÂèØËÉΩÂåÖÂê´‰ª•‰∏ãËÆæËÆ°ÔºöÊï∞ÊçÆÈõÜÁöÑÊ†áÊ≥®ÊñπÂºèÔºåÂåÖÊã¨‰∫∫‰ΩìÂßøÊÄÅ„ÄÅÁâ©‰Ωì‰ΩçÂßø„ÄÅÂú∫ÊôØËØ≠‰πâ‰ø°ÊÅØÁ≠âÔºõÈÄÇÈÖçÊñπÊ≥ïÁöÑÂÖ∑‰ΩìÂÆûÁé∞Ôºå‰æãÂ¶ÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°„ÄÅÁΩëÁªúÁªìÊûÑÁöÑË∞ÉÊï¥Á≠âÔºõÂêéÂ§ÑÁêÜÁÆóÊ≥ïÁöÑËÆæËÆ°Ôºå‰æãÂ¶ÇËøêÂä®Âπ≥Êªë„ÄÅÁ¢∞ÊíûÊ£ÄÊµãÁ≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜMOGRASÊï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄßÔºåÂπ∂ËØÅÊòé‰∫ÜÊèêÂá∫ÁöÑÈÄÇÈÖçÊñπÊ≥ïËÉΩÂ§üÊòæËëóÊèêÈ´òÁé∞ÊúâÊäìÂèñËøêÂä®ÁîüÊàêÊ®°ÂûãÂú®3DÂú∫ÊôØ‰∏≠ÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Êú™Áü•Ôºå‰ΩÜËÆ∫ÊñáÂº∫Ë∞É‰∫ÜÂÆûÈ™åÁªìÊûúÁöÑÊòæËëóÊîπËøõÔºåË°®ÊòéËØ•Á†îÁ©∂ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÊÑè‰πâ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫„ÄÅËôöÊãüÁé∞ÂÆûÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫È¢ÜÂüüÔºåÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°åÊäìÂèñ‰ªªÂä°„ÄÇÂú®ËôöÊãüÁé∞ÂÆûÈ¢ÜÂüüÔºåÂèØ‰ª•ÁîüÊàêÊõ¥ÈÄºÁúüÁöÑ‰∫∫‰ΩìËøêÂä®ÔºåÊèêÈ´òÁî®Êà∑‰ΩìÈ™å„ÄÇÂú®‰∫∫Êú∫‰∫§‰∫íÈ¢ÜÂüüÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Ëá™ÁÑ∂ÁöÑ‰∫∫Êú∫‰∫§‰∫íÊñπÂºè„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Generating realistic full-body motion interacting with objects is critical for applications in robotics, virtual reality, and human-computer interaction. While existing methods can generate full-body motion within 3D scenes, they often lack the fidelity for fine-grained tasks like object grasping. Conversely, methods that generate precise grasping motions typically ignore the surrounding 3D scene. This gap, generating full-body grasping motions that are physically plausible within a 3D scene, remains a significant challenge. To address this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), a large-scale dataset that bridges this gap. MOGRAS provides pre-grasping full-body walking motions and final grasping poses within richly annotated 3D indoor scenes. We leverage MOGRAS to benchmark existing full-body grasping methods and demonstrate their limitations in scene-aware generation. Furthermore, we propose a simple yet effective method to adapt existing approaches to work seamlessly within 3D scenes. Through extensive quantitative and qualitative experiments, we validate the effectiveness of our dataset and highlight the significant improvements our proposed method achieves, paving the way for more realistic human-scene interactions.

