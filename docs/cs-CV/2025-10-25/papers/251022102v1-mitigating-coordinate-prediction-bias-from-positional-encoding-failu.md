---
layout: default
title: Mitigating Coordinate Prediction Bias from Positional Encoding Failures
---

# Mitigating Coordinate Prediction Bias from Positional Encoding Failures

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22102" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22102v1</a>
  <a href="https://arxiv.org/pdf/2510.22102.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22102v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22102v1', 'Mitigating Coordinate Prediction Bias from Positional Encoding Failures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingjian Tao, Yiwei Wang, Yujun Cai, Yihong Luo, Jing Tang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹MLLMåæ ‡é¢„æµ‹åå·®ï¼Œæå‡ºVision-PE Shuffle Guidanceæ–¹æ³•æå‡å®šä½ç²¾åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `åæ ‡é¢„æµ‹` `ä½ç½®ç¼–ç ` `åå·®æ ¡æ­£` `è§†è§‰è¯­è¨€ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MLLMåœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†é«˜åˆ†è¾¨ç‡è¾“å…¥å¯¼è‡´ä½ç½®ç¼–ç å‡å¼±ï¼Œåæ ‡é¢„æµ‹ç²¾åº¦ä¸‹é™ã€‚
2. æå‡ºVision-PE Shuffle Guidance (VPSG)ï¼Œé€šè¿‡æ‰°åŠ¨ä½ç½®ç¼–ç æ¥ä¼°è®¡åå·®ï¼Œå¹¶è¿›è¡Œæ ¡æ­£ã€‚
3. åœ¨ScreenSpot-Proæ•°æ®é›†ä¸Šï¼ŒVPSGæ˜¾è‘—æå‡äº†åæ ‡é¢„æµ‹çš„å‡†ç¡®æ€§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼ˆå¦‚VQAå’Œæ–‡æ¡£ç†è§£ï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç²¾ç¡®çš„åæ ‡é¢„æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é«˜åˆ†è¾¨ç‡è¾“å…¥ä¼šäº§ç”Ÿé•¿tokenåºåˆ—ï¼Œå‰Šå¼±ä½ç½®ç¼–ç ï¼Œå¹¶åœ¨åæ ‡è¾“å‡ºä¸­å¼•å…¥æ–¹å‘åå·®ï¼Œä»è€ŒåŠ å‰§äº†è¿™ä¸€éš¾é¢˜ã€‚æœ¬æ–‡é€šè¿‡åˆ†æMLLMåœ¨è§†è§‰ä½ç½®ç¼–ç (VPE)è¢«æ•…æ„æ‰°ä¹±ï¼ˆé€šè¿‡æ´—ç‰Œï¼‰æ—¶çš„è¡Œä¸ºæ¥ç ”ç©¶è¿™ç§ç°è±¡ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ‰°åŠ¨ä¼šè¯±å¯¼å¯é¢„æµ‹çš„ã€ééšæœºçš„åæ ‡åå·®ï¼Œè€Œä¸æ˜¯éšæœºè¯¯å·®ï¼Œè¿™è¡¨æ˜å½“ç©ºé—´å®šä½ä¿¡å·é€€åŒ–æ—¶ï¼Œæ¨¡å‹ä¾èµ–äºå†…éƒ¨ä½ç½®å…ˆéªŒã€‚å…³é”®çš„æ˜¯ï¼Œåœ¨è‡ªç„¶é«˜åˆ†è¾¨ç‡æ•°æ®é›†ä¸­è§‚å¯Ÿåˆ°ç±»ä¼¼çš„æ–¹å‘è¯¯å·®æ¨¡å¼ï¼Œè¡¨æ˜ä½ç½®ç¼–ç å¤±è´¥æ˜¯å¤§è§„æ¨¡ç²¾ç¡®åæ ‡é¢„æµ‹çš„å…³é”®ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æµ‹è¯•æ—¶æ–¹æ³•Vision-PE Shuffle Guidance (VPSG)ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¿™äº›åå·®çš„æ–¹å‘æ€§è¿›è¡Œæ ¡æ­£ã€‚VPSGè¿è¡Œå¸¦æœ‰æ´—ç‰ŒVPEçš„è¾…åŠ©è§£ç ä»¥éš”ç¦»ä½ç½®éæ¡ä»¶å€¾å‘ï¼Œç„¶åå°†å…¶ç”¨ä½œè´Ÿè¯æ®æ¥æŒ‡å¯¼æ•°å­—é¢„æµ‹ï¼ŒåŒæ—¶é€šè¿‡è½»é‡çº§æœ‰é™çŠ¶æ€æœºä¿æŒåæ ‡æ ¼å¼ã€‚åœ¨ScreenSpot-Proä¸Šçš„å®éªŒè¯æ˜äº†å¯é çš„æ”¹è¿›ï¼Œçªå‡ºäº†ä½ç½®ç¼–ç é²æ£’æ€§æ˜¯MLLMä¸­ç©ºé—´æ¨ç†çš„å…³é”®å› ç´ ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ï¼Œç”±äºè§†è§‰ä½ç½®ç¼–ç (VPE)çš„tokenåºåˆ—è¿‡é•¿ï¼Œå¯¼è‡´ä½ç½®ä¿¡æ¯ä¸¢å¤±æˆ–å‡å¼±ï¼Œä»è€Œå¼•èµ·åæ ‡é¢„æµ‹çš„åå·®ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸‹ï¼Œæ— æ³•å‡†ç¡®åœ°è¿›è¡Œç©ºé—´å®šä½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®å®šä½åæ ‡çš„ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨VPEæ‰°åŠ¨ï¼ˆæ´—ç‰Œï¼‰æ¥æ­ç¤ºæ¨¡å‹å›ºæœ‰çš„ä½ç½®åå·®ã€‚é€šè¿‡è§‚å¯Ÿåœ¨VPEè¢«æ‰“ä¹±çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹é¢„æµ‹åæ ‡çš„å€¾å‘æ€§ï¼Œå¯ä»¥æ¨æ–­å‡ºæ¨¡å‹åœ¨ç¼ºä¹å‡†ç¡®ä½ç½®ä¿¡æ¯æ—¶æ‰€ä¾èµ–çš„å†…éƒ¨ä½ç½®å…ˆéªŒã€‚ç„¶åï¼Œåˆ©ç”¨è¿™äº›åå·®ä¿¡æ¯ä½œä¸ºè´Ÿè¯æ®ï¼Œæ¥æŒ‡å¯¼æ¨¡å‹çš„åæ ‡é¢„æµ‹ï¼Œä»è€Œå‡å°‘åå·®çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVPSG (Vision-PE Shuffle Guidance) æ˜¯ä¸€ç§æµ‹è¯•æ—¶æ–¹æ³•ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚å…¶ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨åŸå§‹VPEè¿›è¡Œä¸€æ¬¡åæ ‡é¢„æµ‹ï¼›2) å¯¹VPEè¿›è¡Œæ´—ç‰Œï¼Œè¿›è¡Œè¾…åŠ©è§£ç ï¼Œå¾—åˆ°ä½ç½®éæ¡ä»¶å€¾å‘ï¼›3) å°†æ´—ç‰ŒVPEçš„é¢„æµ‹ç»“æœä½œä¸ºè´Ÿè¯æ®ï¼ŒæŒ‡å¯¼åŸå§‹VPEçš„é¢„æµ‹ç»“æœï¼Œä»è€Œæ ¡æ­£åæ ‡åå·®ï¼›4) ä½¿ç”¨è½»é‡çº§çš„æœ‰é™çŠ¶æ€æœº(FSM)æ¥ä¿è¯è¾“å‡ºçš„åæ ‡æ ¼å¼æ­£ç¡®ã€‚

**å…³é”®åˆ›æ–°**ï¼šVPSGçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨VPEæ´—ç‰Œæ¥ä¼°è®¡å’Œæ ¡æ­£åæ ‡é¢„æµ‹ä¸­çš„åå·®ã€‚ä¸ä¼ ç»Ÿçš„å¢åŠ æ•°æ®æˆ–ä¿®æ”¹æ¨¡å‹ç»“æ„çš„æ–¹æ³•ä¸åŒï¼ŒVPSGæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æµ‹è¯•æ—¶æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºç°æœ‰çš„MLLMã€‚é€šè¿‡åˆ†æVPEæ‰°åŠ¨åçš„é¢„æµ‹ç»“æœï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ­ç¤ºæ¨¡å‹çš„ä½ç½®åå·®ï¼Œå¹¶åˆ©ç”¨è¿™äº›åå·®ä¿¡æ¯æ¥æé«˜åæ ‡é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVPSGçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) VPEæ´—ç‰Œç­–ç•¥ï¼Œç”¨äºç”Ÿæˆä½ç½®æ‰°åŠ¨ï¼›2) è¾…åŠ©è§£ç è¿‡ç¨‹ï¼Œç”¨äºä¼°è®¡ä½ç½®éæ¡ä»¶å€¾å‘ï¼›3) åå·®æ ¡æ­£æœºåˆ¶ï¼Œåˆ©ç”¨æ´—ç‰ŒVPEçš„é¢„æµ‹ç»“æœä½œä¸ºè´Ÿè¯æ®ï¼ŒæŒ‡å¯¼åŸå§‹VPEçš„é¢„æµ‹ç»“æœï¼›4) æœ‰é™çŠ¶æ€æœº(FSM)ï¼Œç”¨äºä¿è¯è¾“å‡ºçš„åæ ‡æ ¼å¼æ­£ç¡®ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°æ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œä½†å¼ºè°ƒäº†FSMçš„è½»é‡çº§è®¾è®¡ï¼Œä»¥é¿å…å¼•å…¥é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVPSGåœ¨ScreenSpot-Proæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚é€šè¿‡åˆ©ç”¨æ´—ç‰ŒVPEè¿›è¡Œåå·®æ ¡æ­£ï¼ŒVPSGèƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘åæ ‡é¢„æµ‹çš„è¯¯å·®ï¼Œæé«˜äº†å®šä½çš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœ‰æ‰€å±•ç¤ºï¼Œè¯æ˜äº†VPSGåœ¨è§£å†³MLLMåæ ‡é¢„æµ‹åå·®é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç²¾ç¡®åæ ‡é¢„æµ‹çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œå¦‚æ–‡æ¡£ç†è§£ã€å±å¹•å†…å®¹å®šä½ã€ç›®æ ‡æ£€æµ‹å’Œè§†è§‰é—®ç­”ã€‚é€šè¿‡æé«˜MLLMçš„åæ ‡é¢„æµ‹ç²¾åº¦ï¼Œå¯ä»¥æå‡è¿™äº›åº”ç”¨çš„ç”¨æˆ·ä½“éªŒå’Œæ€§èƒ½ï¼Œä¾‹å¦‚åœ¨æ–‡æ¡£ä¸­å‡†ç¡®å®šä½å…³é”®ä¿¡æ¯ï¼Œæˆ–åœ¨å±å¹•ä¸Šç²¾ç¡®å®šä½ç”¨æˆ·æ„Ÿå…´è¶£çš„å…ƒç´ ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œå¹¶ä¿ƒè¿›æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) excel at vision-language tasks such as VQA and document understanding, yet precise coordinate prediction remains challenging. High-resolution inputs exacerbate this difficulty by producing long token sequences that weaken positional encodings and introduce directional biases in coordinate outputs. We investigate this phenomenon by analyzing how MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed through shuffling. Our analysis reveals that such perturbations induce predictable, non-random coordinate biases rather than random errors, suggesting that models rely on internal positional priors when spatial grounding signals are degraded. Crucially, we observe similar directional error patterns in natural high-resolution datasets, indicating that positional encoding failures are a key bottleneck for accurate coordinate prediction at scale. To address this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free test-time method that leverages the directional nature of these biases for correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate position-unconditioned tendencies, then uses this as negative evidence to guide digit prediction while preserving coordinate format through a lightweight finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable improvements, highlighting positional encoding robustness as a critical factor for spatial reasoning in MLLMs.

