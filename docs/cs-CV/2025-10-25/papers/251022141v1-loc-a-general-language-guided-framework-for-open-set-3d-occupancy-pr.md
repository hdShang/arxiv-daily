---
layout: default
title: LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction
---

# LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22141" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22141v1</a>
  <a href="https://arxiv.org/pdf/2510.22141.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22141v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22141v1', 'LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuhang Gao, Xiang Xiang, Sheng Zhong, Guoyou Wang

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG, cs.RO, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LOCï¼šä¸€ç§é€šç”¨çš„è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼Œç”¨äºå¼€æ”¾é›†3D occupancyé¢„æµ‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3D occupancyé¢„æµ‹` `å¼€æ”¾é›†è¯†åˆ«` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¯¹æ¯”å­¦ä¹ ` `LiDARæ•°æ®èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dåœºæ™¯ç†è§£æ–¹æ³•å—é™äº3Dæ•°æ®é›†è§„æ¨¡ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¼€æ”¾é›†èƒ½åŠ›ã€‚
2. LOCæ¡†æ¶é€šè¿‡èåˆå¤šå¸§LiDARæ•°æ®ã€æ³Šæ¾é‡å»ºå’ŒKNNè¯­ä¹‰åˆ†é…ï¼Œæ„å»ºå…¨é¢çš„ä½“ç´ è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å¯†é›†å¯¹æ¯”å­¦ä¹ ç¼“è§£ç‰¹å¾åŒè´¨åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLOCåœ¨nuScenesæ•°æ®é›†ä¸Šå®ç°äº†é«˜ç²¾åº¦çš„å·²çŸ¥ç±»åˆ«é¢„æµ‹ï¼Œå¹¶èƒ½æœ‰æ•ˆåŒºåˆ†æœªçŸ¥ç±»åˆ«ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæ•°æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)åœ¨å¼€æ”¾é›†æŒ‘æˆ˜ä¸­è¡¨ç°å‡ºæ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œ3Dæ•°æ®é›†çš„æœ‰é™å¯ç”¨æ€§é˜»ç¢äº†å®ƒä»¬åœ¨3Dåœºæ™¯ç†è§£ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†LOCï¼Œä¸€ä¸ªé€šç”¨çš„è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼Œå¯ä»¥é€‚åº”å„ç§occupancyç½‘ç»œï¼Œæ”¯æŒç›‘ç£å’Œè‡ªç›‘ç£å­¦ä¹ èŒƒå¼ã€‚å¯¹äºè‡ªç›‘ç£ä»»åŠ¡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§èåˆå¤šå¸§LiDARç‚¹ç”¨äºåŠ¨æ€/é™æ€åœºæ™¯çš„ç­–ç•¥ï¼Œä½¿ç”¨æ³Šæ¾é‡å»ºæ¥å¡«å……ç©ºéš™ï¼Œå¹¶é€šè¿‡Kè¿‘é‚»(KNN)ä¸ºä½“ç´ åˆ†é…è¯­ä¹‰ï¼Œä»¥è·å¾—å…¨é¢çš„ä½“ç´ è¡¨ç¤ºã€‚ä¸ºäº†ç¼“è§£ç›´æ¥é«˜ç»´ç‰¹å¾è’¸é¦å¼•èµ·çš„ç‰¹å¾è¿‡åº¦åŒè´¨åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯†é›†å¯¹æ¯”å­¦ä¹ (DCL)ã€‚DCLåˆ©ç”¨å¯†é›†ä½“ç´ è¯­ä¹‰ä¿¡æ¯å’Œé¢„å®šä¹‰çš„æ–‡æœ¬æç¤ºã€‚è¿™æœ‰æ•ˆåœ°å¢å¼ºäº†å¼€æ”¾é›†è¯†åˆ«ï¼Œè€Œæ— éœ€å¯†é›†çš„åƒç´ çº§ç›‘ç£ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„æ¡†æ¶è¿˜å¯ä»¥åˆ©ç”¨ç°æœ‰çš„ground truthæ¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹åµŒå…¥åœ¨CLIPç‰¹å¾ç©ºé—´ä¸­çš„å¯†é›†ä½“ç´ ç‰¹å¾ï¼Œæ•´åˆæ–‡æœ¬å’Œå›¾åƒåƒç´ ä¿¡æ¯ï¼Œå¹¶åŸºäºæ–‡æœ¬å’Œè¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œåˆ†ç±»ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ï¼Œå®ç°äº†å·²çŸ¥ç±»çš„é«˜ç²¾åº¦é¢„æµ‹ï¼Œå¹¶åŒºåˆ†äº†æœªçŸ¥ç±»ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾é›†3D occupancyé¢„æµ‹é—®é¢˜ï¼Œå³åœ¨è®­ç»ƒæ•°æ®ä¸åŒ…å«æ‰€æœ‰ç±»åˆ«çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®é¢„æµ‹3Dåœºæ™¯ä¸­æ¯ä¸ªä½“ç´ çš„ç±»åˆ«ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡çš„3Dæ ‡æ³¨æ•°æ®ï¼Œå¹¶ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥è¯†åˆ«æœªè§è¿‡çš„ç±»åˆ«ã€‚è§†è§‰-è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨å¼€æ”¾é›†è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹æœ‰æ•ˆçš„3Dåœºæ™¯ç†è§£æ–¹æ³•æ¥åˆ©ç”¨å…¶èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¯­è¨€çš„è¯­ä¹‰ä¿¡æ¯æ¥å¼•å¯¼3D occupancyé¢„æµ‹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¼€æ”¾é›†è¯†åˆ«èƒ½åŠ›ã€‚é€šè¿‡å°†3Dä½“ç´ ç‰¹å¾åµŒå…¥åˆ°CLIPçš„ç‰¹å¾ç©ºé—´ä¸­ï¼Œæ¨¡å‹å¯ä»¥åˆ©ç”¨æ–‡æœ¬æè¿°æ¥è¯†åˆ«å’ŒåŒºåˆ†ä¸åŒçš„ç±»åˆ«ï¼ŒåŒ…æ‹¬æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°çš„ç±»åˆ«ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†å¯†é›†å¯¹æ¯”å­¦ä¹ æ¥ç¼“è§£ç‰¹å¾åŒè´¨åŒ–é—®é¢˜ï¼Œä»è€Œæé«˜æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLOCæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šå¸§LiDARæ•°æ®èåˆå’Œæ³Šæ¾é‡å»ºï¼Œç”¨äºç”Ÿæˆå®Œæ•´çš„3Dä½“ç´ è¡¨ç¤ºï¼›2) KNNè¯­ä¹‰åˆ†é…ï¼Œç”¨äºä¸ºæ¯ä¸ªä½“ç´ åˆ†é…è¯­ä¹‰æ ‡ç­¾ï¼›3) ç‰¹å¾ç¼–ç å™¨ï¼Œç”¨äºå°†ä½“ç´ è¡¨ç¤ºç¼–ç ä¸ºCLIPç‰¹å¾ï¼›4) å¯†é›†å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼Œç”¨äºæé«˜ç‰¹å¾çš„åˆ¤åˆ«èƒ½åŠ›ï¼›5) åˆ†ç±»å™¨ï¼ŒåŸºäºæ–‡æœ¬å’Œè¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œä½“ç´ åˆ†ç±»ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œå¯¹å¤šå¸§LiDARæ•°æ®è¿›è¡Œèåˆå’Œé‡å»ºï¼Œç„¶åä¸ºæ¯ä¸ªä½“ç´ åˆ†é…è¯­ä¹‰æ ‡ç­¾ã€‚æ¥ç€ï¼Œä½¿ç”¨ç‰¹å¾ç¼–ç å™¨å°†ä½“ç´ è¡¨ç¤ºç¼–ç ä¸ºCLIPç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å¯†é›†å¯¹æ¯”å­¦ä¹ æ¨¡å—æé«˜ç‰¹å¾çš„åˆ¤åˆ«èƒ½åŠ›ã€‚æœ€åï¼Œä½¿ç”¨åˆ†ç±»å™¨åŸºäºæ–‡æœ¬å’Œè¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œä½“ç´ åˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼Œå¯ä»¥é€‚åº”å„ç§occupancyç½‘ç»œï¼›2) æå‡ºäº†å¯†é›†å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ç‰¹å¾åŒè´¨åŒ–é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„å¼€æ”¾é›†è¯†åˆ«èƒ½åŠ›ï¼›3) åˆ©ç”¨å¤šå¸§LiDARæ•°æ®èåˆå’Œæ³Šæ¾é‡å»ºï¼Œç”Ÿæˆäº†æ›´å®Œæ•´çš„3Dä½“ç´ è¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è‡ªç›‘ç£å­¦ä¹ ä¸­ï¼Œä½¿ç”¨å¤šå¸§LiDARç‚¹äº‘èåˆï¼Œå¹¶ä½¿ç”¨æ³Šæ¾é‡å»ºå¡«å……ç©ºæ´ã€‚KNNç®—æ³•ç”¨äºä½“ç´ è¯­ä¹‰æ ‡æ³¨ã€‚å¯†é›†å¯¹æ¯”å­¦ä¹ (DCL)æŸå¤±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ï¼Œå®ƒåˆ©ç”¨å¯†é›†ä½“ç´ è¯­ä¹‰ä¿¡æ¯å’Œé¢„å®šä¹‰çš„æ–‡æœ¬æç¤ºï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºç‰¹å¾çš„åˆ¤åˆ«èƒ½åŠ›ã€‚æ¨¡å‹é¢„æµ‹çš„ä½“ç´ ç‰¹å¾åµŒå…¥åˆ°CLIPç‰¹å¾ç©ºé—´ä¸­ï¼Œä»¥ä¾¿åˆ©ç”¨æ–‡æœ¬ä¿¡æ¯è¿›è¡Œåˆ†ç±»ã€‚åˆ†ç±»å™¨åŸºäºæ–‡æœ¬å’Œè¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œåˆ†ç±»ï¼Œä¾‹å¦‚è®¡ç®—ä½“ç´ ç‰¹å¾ä¸æ–‡æœ¬æè¿°ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LOCåœ¨nuScenesæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•å®ç°äº†å·²çŸ¥ç±»çš„é«˜ç²¾åº¦é¢„æµ‹ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æœªçŸ¥ç±»ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLOCåœ¨å¼€æ”¾é›†3D occupancyé¢„æµ‹æ–¹é¢å…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€åœºæ™¯ç†è§£ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨è¯­è¨€ä¿¡æ¯ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹æœªçŸ¥çš„ç‰©ä½“å’Œåœºæ™¯æ—¶ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•åˆ©ç”¨æ›´ä¸°å¯Œçš„è¯­è¨€ä¿¡æ¯ï¼Œä¾‹å¦‚åœºæ™¯æè¿°å’ŒæŒ‡ä»¤ï¼Œæ¥æé«˜æ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have shown significant progress in open-set challenges. However, the limited availability of 3D datasets hinders their effective application in 3D scene understanding. We propose LOC, a general language-guided framework adaptable to various occupancy networks, supporting both supervised and self-supervised learning paradigms. For self-supervised tasks, we employ a strategy that fuses multi-frame LiDAR points for dynamic/static scenes, using Poisson reconstruction to fill voids, and assigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain comprehensive voxel representations. To mitigate feature over-homogenization caused by direct high-dimensional feature distillation, we introduce Densely Contrastive Learning (DCL). DCL leverages dense voxel semantic information and predefined textual prompts. This efficiently enhances open-set recognition without dense pixel-level supervision, and our framework can also leverage existing ground truth to further improve performance. Our model predicts dense voxel features embedded in the CLIP feature space, integrating textual and image pixel information, and classifies based on text and semantic similarity. Experiments on the nuScenes dataset demonstrate the method's superior performance, achieving high-precision predictions for known classes and distinguishing unknown classes without additional training data.

