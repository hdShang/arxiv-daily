---
layout: default
title: Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents
---

# Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.22443" target="_blank" class="toolbar-btn">arXiv: 2510.22443v1</a>
    <a href="https://arxiv.org/pdf/2510.22443.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22443v1" 
            onclick="toggleFavorite(this, '2510.22443v1', 'Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Vijay Veerabadran, Fanyi Xiao, Nitin Kamra, Pedro Matias, Joy Chen, Caley Drooff, Brett D Roads, Riley Williams, Ethan Henderson, Xuanyi Zhao, Kevin Carlberg, Joseph Tighe, Karl Ridgeway

**ÂàÜÁ±ª**: cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-25

**Â§áÊ≥®**: Accepted as a spotlight paper at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**WAGIBenchÔºöÁî®‰∫éËæÖÂä©ÂèØÁ©øÊà¥‰ª£ÁêÜÁöÑËá™‰∏≠ÂøÉÂ§öÊ®°ÊÄÅÁõÆÊ†áÊé®Êñ≠Âü∫ÂáÜ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ëá™‰∏≠ÂøÉËßÜËßâ` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÁõÆÊ†áÊé®Êñ≠` `ÂèØÁ©øÊà¥ËÆæÂ§á` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËæÖÂä©ÂèØÁ©øÊà¥‰ª£ÁêÜÈúÄË¶ÅÁî®Êà∑‰∫§‰∫íÊù•ÊòéÁ°ÆÁõÆÊ†áÔºåÈôç‰Ωé‰∫ÜÁî®Êà∑‰ΩìÈ™åÔºåËÄåËá™Âä®ÁõÆÊ†áÊé®Êñ≠ÂèØ‰ª•Ëß£ÂÜ≥Ê≠§ÈóÆÈ¢ò„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫WAGIBenchÂü∫ÂáÜÔºåÂà©Áî®Â§öÊ®°ÊÄÅÊï∞ÊçÆÔºàËßÜËßâ„ÄÅÈü≥È¢ëÁ≠âÔºâËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÂÆûÁé∞Áî®Êà∑ÊÑèÂõæÁöÑËá™Âä®Êé®Êñ≠„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÁé∞ÊúâVLMÊ®°ÂûãÂú®WAGIBench‰∏äË°®Áé∞‰∏é‰∫∫Á±ªÂ≠òÂú®Â∑ÆË∑ùÔºå‰ΩÜÈÄöËøáÊ®°ÊÄÅÂàÜÊûêÔºåÂèØ‰ª•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÂÖ≥Ê≥®ËæÖÂä©ÂèØÁ©øÊà¥‰ª£ÁêÜ‰∏≠ÁöÑÁõÆÊ†áÊé®Êñ≠ÈóÆÈ¢òÔºåÊó®Âú®ÈÄöËøáÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáËßÇÂØüËá™Âä®Êé®Êñ≠Áî®Êà∑ÁöÑÁõÆÊ†áÔºå‰ªéËÄåÂáèÂ∞ëÁî®Êà∑‰∏é‰ª£ÁêÜ‰∫§‰∫íÊâÄÈúÄÁöÑÂ∑•‰ΩúÈáè„ÄÇ‰∏∫Ê≠§Ôºå‰ΩúËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫WAGIBenchÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Ê≠§‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ348ÂêçÂèÇ‰∏éËÄÖÁöÑ3477‰∏™ËÆ∞ÂΩïÔºåÂÖ±ËÆ°29Â∞èÊó∂ÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÔºåÂåÖÊã¨ËßÜËßâ„ÄÅÈü≥È¢ë„ÄÅÊï∞Â≠óÂíåÁ∫µÂêë‰∏ä‰∏ãÊñáËßÇÂØü‰ª•ÂèäÂØπÂ∫îÁöÑground-truthÁõÆÊ†á„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∫∫Á±ªÂú®Ê≠§‰ªªÂä°‰∏äÁöÑË°®Áé∞‰ºò‰∫éÊ®°ÂûãÔºåÂ§öÈ°πÈÄâÊã©ÂáÜÁ°ÆÁéáÂàÜÂà´‰∏∫93%ÂíåÊúÄ‰Ω≥VLMÁöÑ84%„ÄÇÁîüÊàêÂºèÂü∫ÂáÜÊµãËØïË°®ÊòéÔºåÊõ¥Â§ßÁöÑÊ®°ÂûãË°®Áé∞Êõ¥Â•ΩÔºå‰ΩÜË∑ùÁ¶ªÂÆûÈôÖÂ∫îÁî®‰ªçÊúâÂ∑ÆË∑ùÔºå‰ªÖÂú®55%ÁöÑÊÉÖÂÜµ‰∏ã‰∫ßÁîüÁõ∏ÂÖ≥ÁõÆÊ†á„ÄÇÊ®°ÊÄÅÊ∂àËûçÂÆûÈ™åË°®ÊòéÔºåÊ®°ÂûãÂèóÁõä‰∫éÁõ∏ÂÖ≥Ê®°ÊÄÅÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØÔºåËÄå‰∏çÁõ∏ÂÖ≥Ê®°ÊÄÅÂØπÊÄßËÉΩÁöÑÂΩ±ÂìçÂæàÂ∞è„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËæÖÂä©ÂèØÁ©øÊà¥ËÆæÂ§á‰∏≠ÔºåÂ¶Ç‰Ωï‰ªéÁî®Êà∑ÁöÑÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºàËßÜËßâ„ÄÅÂê¨Ëßâ„ÄÅÊï∞Â≠ó‰∫§‰∫íÁ≠âÔºâ‰∏≠ÂáÜÁ°ÆÊé®Êñ≠Âá∫Áî®Êà∑ÂΩìÂâçÁõÆÊ†áÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÁî®Êà∑‰∏ªÂä®ËæìÂÖ•ÊàñÈÄâÊã©ÁõÆÊ†áÔºåÊïàÁéáËæÉ‰ΩéÔºå‰∏î‰∏çÂ§üËá™ÁÑ∂„ÄÇÂõ†Ê≠§ÔºåËá™Âä®ÂåñÁöÑÁõÆÊ†áÊé®Êñ≠ÊòØÊèêÂçáÁî®Êà∑‰ΩìÈ™åÁöÑÂÖ≥ÈîÆ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÔºåÂ∞ÜÂ§öÊ®°ÊÄÅÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÁºñÁ†Å‰∏∫Áªü‰∏ÄÁöÑËØ≠‰πâË°®Á§∫ÔºåÁÑ∂ÂêéÂü∫‰∫éÊ≠§Ë°®Á§∫Êé®Êñ≠Áî®Êà∑ÁöÑÁõÆÊ†á„ÄÇÈÄöËøáÊûÑÂª∫Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜWAGIBenchÔºåÂπ∂Âú®Ê≠§Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÂíåËØÑ‰º∞VLMÔºå‰ªéËÄåÊé®Âä®ËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂ËøõÂ±ï„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´Êï∞ÊçÆÈááÈõÜ„ÄÅÊ®°ÂûãËÆ≠ÁªÉÂíåËØÑ‰º∞‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÊï∞ÊçÆÈááÈõÜÈò∂ÊÆµÔºåÈÄöËøáÂèØÁ©øÊà¥ËÆæÂ§áËÆ∞ÂΩïÁî®Êà∑ÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÔºåÂπ∂Ê†áÊ≥®ÂØπÂ∫îÁöÑÁõÆÊ†á„ÄÇÊ®°ÂûãËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ΩøÁî®VLMÊ®°ÂûãÂ≠¶‰π†Â§öÊ®°ÊÄÅÊï∞ÊçÆ‰∏éÁõÆÊ†á‰πãÈó¥ÁöÑÊò†Â∞ÑÂÖ≥Á≥ª„ÄÇËØÑ‰º∞Èò∂ÊÆµÔºå‰ΩøÁî®WAGIBenchÊï∞ÊçÆÈõÜËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩÔºåÂπ∂‰∏é‰∫∫Á±ªË°®Áé∞ËøõË°åÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫ÜWAGIBenchÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜÊòØÈ¶ñ‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Ëá™‰∏≠ÂøÉÂ§öÊ®°ÊÄÅÁõÆÊ†áÊé®Êñ≠‰ªªÂä°ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂØπÂ§öÁßçVLMÊ®°ÂûãËøõË°å‰∫ÜÂü∫ÂáÜÊµãËØïÔºåÂπ∂ÂàÜÊûê‰∫Ü‰∏çÂêåÊ®°ÊÄÅ‰ø°ÊÅØÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöWAGIBenchÊï∞ÊçÆÈõÜÂåÖÂê´ËßÜËßâ„ÄÅÈü≥È¢ë„ÄÅÊï∞Â≠ó‰∫§‰∫íÂíåÁ∫µÂêë‰∏ä‰∏ãÊñáÁ≠âÂ§öÁßçÊ®°ÊÄÅÁöÑ‰ø°ÊÅØ„ÄÇÂú®Ê®°ÂûãËÆ≠ÁªÉÊñπÈù¢ÔºåËÆ∫ÊñáÈááÁî®‰∫ÜÂ§öÁßçVLMÊ®°ÂûãÔºåÂπ∂ÈíàÂØπÁõÆÊ†áÊé®Êñ≠‰ªªÂä°ËøõË°å‰∫ÜÂæÆË∞É„ÄÇÂú®ËØÑ‰º∞ÊñπÈù¢ÔºåËÆ∫ÊñáÈááÁî®‰∫ÜÂ§öÈ°πÈÄâÊã©ÂáÜÁ°ÆÁéáÂíåÁîüÊàêÂºèËØÑ‰º∞ÊåáÊ†áÔºå‰ª•ÂÖ®Èù¢ËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∫∫Á±ªÂú®WAGIBenchÊï∞ÊçÆÈõÜ‰∏äÁöÑÂ§öÈ°πÈÄâÊã©ÂáÜÁ°ÆÁéá‰∏∫93%ÔºåËÄåÊúÄ‰Ω≥VLMÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéá‰∏∫84%ÔºåË°®ÊòéÊ®°ÂûãÊÄßËÉΩ‰ªçÊúâÊèêÂçáÁ©∫Èó¥„ÄÇÁîüÊàêÂºèÂü∫ÂáÜÊµãËØïÊòæÁ§∫ÔºåÊõ¥Â§ßÁöÑÊ®°ÂûãË°®Áé∞Êõ¥Â•ΩÔºå‰ΩÜ‰ªÖÂú®55%ÁöÑÊÉÖÂÜµ‰∏ã‰∫ßÁîüÁõ∏ÂÖ≥ÁõÆÊ†á„ÄÇÊ®°ÊÄÅÊ∂àËûçÂÆûÈ™åË°®ÊòéÔºåÊ®°ÂûãÂèóÁõä‰∫éÁõ∏ÂÖ≥Ê®°ÊÄÅÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØÔºåËÄå‰∏çÁõ∏ÂÖ≥Ê®°ÊÄÅÂØπÊÄßËÉΩÁöÑÂΩ±ÂìçÂæàÂ∞è„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÁúºÈïú„ÄÅÊô∫ËÉΩÊâãË°®Á≠âÂèØÁ©øÊà¥ËÆæÂ§áÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™ÁÑ∂ÁöÑËæÖÂä©ÂäüËÉΩ„ÄÇ‰æãÂ¶ÇÔºåËÆæÂ§áÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑË°å‰∏∫ÂíåÁéØÂ¢ÉËá™Âä®Êé®Êñ≠Áî®Êà∑ÁöÑÈúÄÊ±ÇÔºåÂπ∂Êèê‰æõÁõ∏Â∫îÁöÑÂ∏ÆÂä©ÔºåÂ¶ÇÂØºËà™„ÄÅ‰ø°ÊÅØÊü•ËØ¢„ÄÅËÆæÂ§áÊéßÂà∂Á≠â„ÄÇËøôÂ∞ÜÊûÅÂ§ßÂú∞ÊèêÂçáÁî®Êà∑‰ΩìÈ™åÔºåÂπ∂‰∏∫ÊÆãÁñæ‰∫∫Â£´Êèê‰æõÊõ¥‰æøÊç∑ÁöÑÁîüÊ¥ªËæÖÂä©„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> There has been a surge of interest in assistive wearable agents: agents embodied in wearable form factors (e.g., smart glasses) who take assistive actions toward a user's goal/query (e.g. "Where did I leave my keys?"). In this work, we consider the important complementary problem of inferring that goal from multi-modal contextual observations. Solving this "goal inference" problem holds the promise of eliminating the effort needed to interact with such an agent. This work focuses on creating WAGIBench, a strong benchmark to measure progress in solving this problem using vision-language models (VLMs). Given the limited prior work in this area, we collected a novel dataset comprising 29 hours of multimodal data from 348 participants across 3,477 recordings, featuring ground-truth goals alongside accompanying visual, audio, digital, and longitudinal contextual observations. We validate that human performance exceeds model performance, achieving 93% multiple-choice accuracy compared with 84% for the best-performing VLM. Generative benchmark results that evaluate several families of modern vision-language models show that larger models perform significantly better on the task, yet remain far from practical usefulness, as they produce relevant goals only 55% of the time. Through a modality ablation, we show that models benefit from extra information in relevant modalities with minimal performance degradation from irrelevant modalities.

