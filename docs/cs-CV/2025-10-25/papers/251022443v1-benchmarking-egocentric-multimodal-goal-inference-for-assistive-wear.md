---
layout: default
title: Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents
---

# Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22443" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22443v1</a>
  <a href="https://arxiv.org/pdf/2510.22443.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22443v1" onclick="toggleFavorite(this, '2510.22443v1', 'Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vijay Veerabadran, Fanyi Xiao, Nitin Kamra, Pedro Matias, Joy Chen, Caley Drooff, Brett D Roads, Riley Williams, Ethan Henderson, Xuanyi Zhao, Kevin Carlberg, Joseph Tighe, Karl Ridgeway

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-25

**å¤‡æ³¨**: Accepted as a spotlight paper at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**WAGIBenchï¼šç”¨äºè¾…åŠ©å¯ç©¿æˆ´ä»£ç†çš„è‡ªä¸­å¿ƒå¤šæ¨¡æ€ç›®æ ‡æ¨æ–­åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªä¸­å¿ƒè§†è§‰` `å¤šæ¨¡æ€å­¦ä¹ ` `ç›®æ ‡æ¨æ–­` `å¯ç©¿æˆ´è®¾å¤‡` `è§†è§‰-è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¾…åŠ©å¯ç©¿æˆ´ä»£ç†éœ€è¦ç”¨æˆ·äº¤äº’æ¥æ˜ç¡®ç›®æ ‡ï¼Œé™ä½äº†ç”¨æˆ·ä½“éªŒï¼Œè€Œè‡ªåŠ¨ç›®æ ‡æ¨æ–­å¯ä»¥è§£å†³æ­¤é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºWAGIBenchåŸºå‡†ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ï¼ˆè§†è§‰ã€éŸ³é¢‘ç­‰ï¼‰è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå®ç°ç”¨æˆ·æ„å›¾çš„è‡ªåŠ¨æ¨æ–­ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰VLMæ¨¡å‹åœ¨WAGIBenchä¸Šè¡¨ç°ä¸äººç±»å­˜åœ¨å·®è·ï¼Œä½†é€šè¿‡æ¨¡æ€åˆ†æï¼Œå¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å…³æ³¨è¾…åŠ©å¯ç©¿æˆ´ä»£ç†ä¸­çš„ç›®æ ‡æ¨æ–­é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è§‚å¯Ÿè‡ªåŠ¨æ¨æ–­ç”¨æˆ·çš„ç›®æ ‡ï¼Œä»è€Œå‡å°‘ç”¨æˆ·ä¸ä»£ç†äº¤äº’æ‰€éœ€çš„å·¥ä½œé‡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåä¸ºWAGIBenchçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ­¤ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥åŸºå‡†åŒ…å«ä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œæ¶µç›–348åå‚ä¸è€…çš„3477ä¸ªè®°å½•ï¼Œå…±è®¡29å°æ—¶çš„å¤šæ¨¡æ€æ•°æ®ï¼ŒåŒ…æ‹¬è§†è§‰ã€éŸ³é¢‘ã€æ•°å­—å’Œçºµå‘ä¸Šä¸‹æ–‡è§‚å¯Ÿä»¥åŠå¯¹åº”çš„ground-truthç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæ¨¡å‹ï¼Œå¤šé¡¹é€‰æ‹©å‡†ç¡®ç‡åˆ†åˆ«ä¸º93%å’Œæœ€ä½³VLMçš„84%ã€‚ç”Ÿæˆå¼åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œæ›´å¤§çš„æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä½†è·ç¦»å®é™…åº”ç”¨ä»æœ‰å·®è·ï¼Œä»…åœ¨55%çš„æƒ…å†µä¸‹äº§ç”Ÿç›¸å…³ç›®æ ‡ã€‚æ¨¡æ€æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ¨¡å‹å—ç›Šäºç›¸å…³æ¨¡æ€çš„é¢å¤–ä¿¡æ¯ï¼Œè€Œä¸ç›¸å…³æ¨¡æ€å¯¹æ€§èƒ½çš„å½±å“å¾ˆå°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¾…åŠ©å¯ç©¿æˆ´è®¾å¤‡ä¸­ï¼Œå¦‚ä½•ä»ç”¨æˆ·çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆè§†è§‰ã€å¬è§‰ã€æ•°å­—äº¤äº’ç­‰ï¼‰ä¸­å‡†ç¡®æ¨æ–­å‡ºç”¨æˆ·å½“å‰ç›®æ ‡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ç”¨æˆ·ä¸»åŠ¨è¾“å…¥æˆ–é€‰æ‹©ç›®æ ‡ï¼Œæ•ˆç‡è¾ƒä½ï¼Œä¸”ä¸å¤Ÿè‡ªç„¶ã€‚å› æ­¤ï¼Œè‡ªåŠ¨åŒ–çš„ç›®æ ‡æ¨æ–­æ˜¯æå‡ç”¨æˆ·ä½“éªŒçš„å…³é”®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå°†å¤šæ¨¡æ€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç¼–ç ä¸ºç»Ÿä¸€çš„è¯­ä¹‰è¡¨ç¤ºï¼Œç„¶ååŸºäºæ­¤è¡¨ç¤ºæ¨æ–­ç”¨æˆ·çš„ç›®æ ‡ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†WAGIBenchï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°VLMï¼Œä»è€Œæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«æ•°æ®é‡‡é›†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é‡‡é›†é˜¶æ®µï¼Œé€šè¿‡å¯ç©¿æˆ´è®¾å¤‡è®°å½•ç”¨æˆ·çš„å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶æ ‡æ³¨å¯¹åº”çš„ç›®æ ‡ã€‚æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨VLMæ¨¡å‹å­¦ä¹ å¤šæ¨¡æ€æ•°æ®ä¸ç›®æ ‡ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚è¯„ä¼°é˜¶æ®µï¼Œä½¿ç”¨WAGIBenchæ•°æ®é›†è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸äººç±»è¡¨ç°è¿›è¡Œå¯¹æ¯”ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†WAGIBenchæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è‡ªä¸­å¿ƒå¤šæ¨¡æ€ç›®æ ‡æ¨æ–­ä»»åŠ¡çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹å¤šç§VLMæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶åˆ†æäº†ä¸åŒæ¨¡æ€ä¿¡æ¯å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šWAGIBenchæ•°æ®é›†åŒ…å«è§†è§‰ã€éŸ³é¢‘ã€æ•°å­—äº¤äº’å’Œçºµå‘ä¸Šä¸‹æ–‡ç­‰å¤šç§æ¨¡æ€çš„ä¿¡æ¯ã€‚åœ¨æ¨¡å‹è®­ç»ƒæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†å¤šç§VLMæ¨¡å‹ï¼Œå¹¶é’ˆå¯¹ç›®æ ‡æ¨æ–­ä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚åœ¨è¯„ä¼°æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†å¤šé¡¹é€‰æ‹©å‡†ç¡®ç‡å’Œç”Ÿæˆå¼è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»åœ¨WAGIBenchæ•°æ®é›†ä¸Šçš„å¤šé¡¹é€‰æ‹©å‡†ç¡®ç‡ä¸º93%ï¼Œè€Œæœ€ä½³VLMæ¨¡å‹çš„å‡†ç¡®ç‡ä¸º84%ï¼Œè¡¨æ˜æ¨¡å‹æ€§èƒ½ä»æœ‰æå‡ç©ºé—´ã€‚ç”Ÿæˆå¼åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œæ›´å¤§çš„æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä½†ä»…åœ¨55%çš„æƒ…å†µä¸‹äº§ç”Ÿç›¸å…³ç›®æ ‡ã€‚æ¨¡æ€æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ¨¡å‹å—ç›Šäºç›¸å…³æ¨¡æ€çš„é¢å¤–ä¿¡æ¯ï¼Œè€Œä¸ç›¸å…³æ¨¡æ€å¯¹æ€§èƒ½çš„å½±å“å¾ˆå°ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½çœ¼é•œã€æ™ºèƒ½æ‰‹è¡¨ç­‰å¯ç©¿æˆ´è®¾å¤‡ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„è¾…åŠ©åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œè®¾å¤‡å¯ä»¥æ ¹æ®ç”¨æˆ·çš„è¡Œä¸ºå’Œç¯å¢ƒè‡ªåŠ¨æ¨æ–­ç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶æä¾›ç›¸åº”çš„å¸®åŠ©ï¼Œå¦‚å¯¼èˆªã€ä¿¡æ¯æŸ¥è¯¢ã€è®¾å¤‡æ§åˆ¶ç­‰ã€‚è¿™å°†æå¤§åœ°æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶ä¸ºæ®‹ç–¾äººå£«æä¾›æ›´ä¾¿æ·çš„ç”Ÿæ´»è¾…åŠ©ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> There has been a surge of interest in assistive wearable agents: agents embodied in wearable form factors (e.g., smart glasses) who take assistive actions toward a user's goal/query (e.g. "Where did I leave my keys?"). In this work, we consider the important complementary problem of inferring that goal from multi-modal contextual observations. Solving this "goal inference" problem holds the promise of eliminating the effort needed to interact with such an agent. This work focuses on creating WAGIBench, a strong benchmark to measure progress in solving this problem using vision-language models (VLMs). Given the limited prior work in this area, we collected a novel dataset comprising 29 hours of multimodal data from 348 participants across 3,477 recordings, featuring ground-truth goals alongside accompanying visual, audio, digital, and longitudinal contextual observations. We validate that human performance exceeds model performance, achieving 93% multiple-choice accuracy compared with 84% for the best-performing VLM. Generative benchmark results that evaluate several families of modern vision-language models show that larger models perform significantly better on the task, yet remain far from practical usefulness, as they produce relevant goals only 55% of the time. Through a modality ablation, we show that models benefit from extra information in relevant modalities with minimal performance degradation from irrelevant modalities.

