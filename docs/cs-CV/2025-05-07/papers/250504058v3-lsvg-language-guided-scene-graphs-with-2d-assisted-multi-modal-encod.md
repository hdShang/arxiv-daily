---
layout: default
title: LSVG: Language-Guided Scene Graphs with 2D-Assisted Multi-Modal Encoding for 3D Visual Grounding
---

# LSVG: Language-Guided Scene Graphs with 2D-Assisted Multi-Modal Encoding for 3D Visual Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04058" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04058v3</a>
  <a href="https://arxiv.org/pdf/2505.04058.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04058v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04058v3', 'LSVG: Language-Guided Scene Graphs with 2D-Assisted Multi-Modal Encoding for 3D Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Feng Xiao, Hongbin Xu, Guocan Zhao, Wenxiong Kang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-07 (æ›´æ–°: 2025-08-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­è¨€å¼•å¯¼çš„åœºæ™¯å›¾ä»¥è§£å†³3Dè§†è§‰å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `å¤šæ¨¡æ€èåˆ` `è¯­è¨€å¼•å¯¼` `åœºæ™¯å›¾` `å…³ç³»æ„ŸçŸ¥` `å›¾æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†3Dè§†è§‰å®šä½æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆå»ºæ¨¡è¢«æŒ‡å¯¹è±¡ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸­éš¾ä»¥åŒºåˆ†ç›¸ä¼¼ç›®æ ‡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dè§†è§‰å®šä½æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºè¯­è¨€å¼•å¯¼çš„åœºæ™¯å›¾æ¥å¢å¼ºå…³ç³»æ„ŸçŸ¥ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¤„ç†ç›¸ä¼¼å¹²æ‰°ç‰©ä½“æ—¶ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Dè§†è§‰å®šä½æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€åœ¨3Dåœºæ™¯ä¸­å®šä½ç‰¹å®šç›®æ ‡ã€‚ç”±äº3Då’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„æ˜¾è‘—å·®è·ï¼ŒåŒºåˆ†å¤šä¸ªç›¸ä¼¼å¯¹è±¡æˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ç›®æ ‡ä¸­å¿ƒå­¦ä¹ æœºåˆ¶å®ç°è·¨æ¨¡æ€ç†è§£ï¼Œä½†å¿½è§†äº†è¢«æŒ‡å¯¹è±¡çš„å»ºæ¨¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dè§†è§‰å®šä½æ¡†æ¶ï¼Œæ„å»ºè¯­è¨€å¼•å¯¼çš„åœºæ™¯å›¾å¹¶å®ç°è¢«æŒ‡å¯¹è±¡çš„åŒºåˆ†ï¼Œä»¥æå‡å…³ç³»æ„ŸçŸ¥ã€‚è¯¥æ¡†æ¶ç»“åˆåŒåˆ†æ”¯è§†è§‰ç¼–ç å™¨ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„2Dè¯­ä¹‰å¢å¼ºå’Œç›‘ç£å¤šæ¨¡æ€3Dç¼–ç ï¼Œå¹¶é‡‡ç”¨å›¾æ³¨æ„åŠ›æœºåˆ¶ä¿ƒè¿›è·¨æ¨¡æ€äº¤äº’ä¸­çš„å…³ç³»å¯¼å‘ä¿¡æ¯èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤„ç†å¤šä¸ªç›¸ä¼¼å¹²æ‰°ç‰©ä½“çš„æŒ‘æˆ˜æ—¶ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³3Dè§†è§‰å®šä½ä¸­çš„ç›®æ ‡åŒºåˆ†é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡è¢«æŒ‡å¯¹è±¡ï¼Œå¯¼è‡´å®šä½ç²¾åº¦ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºè¯­è¨€å¼•å¯¼çš„åœºæ™¯å›¾æ¥å®ç°è¢«æŒ‡å¯¹è±¡çš„åŒºåˆ†ï¼Œå¢å¼ºå…³ç³»æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæé«˜3Dè§†è§‰å®šä½çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŒåˆ†æ”¯è§†è§‰ç¼–ç å™¨å’Œå›¾æ³¨æ„åŠ›æœºåˆ¶ã€‚åŒåˆ†æ”¯è§†è§‰ç¼–ç å™¨åˆ©ç”¨é¢„è®­ç»ƒçš„2Dè¯­ä¹‰ä¿¡æ¯æ¥å¢å¼º3Dç¼–ç ï¼Œè€Œå›¾æ³¨æ„åŠ›æœºåˆ¶åˆ™ä¿ƒè¿›è·¨æ¨¡æ€ä¿¡æ¯çš„èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡è¯­è¨€å¼•å¯¼çš„åœºæ™¯å›¾å®ç°è¢«æŒ‡å¯¹è±¡çš„åŒºåˆ†ï¼Œè¿™ä¸€è®¾è®¡ä¸ç°æœ‰æ–¹æ³•çš„ç›®æ ‡ä¸­å¿ƒå­¦ä¹ æœºåˆ¶å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæ˜¾è‘—æå‡äº†å…³ç³»æ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡ä»¥å®ç°2Då’Œ3Dä¿¡æ¯çš„æœ‰æ•ˆèåˆï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥å…³ç³»å¯¼å‘çš„æŸå¤±é¡¹ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå°¤å…¶åœ¨å¤„ç†å¤šä¸ªç›¸ä¼¼å¹²æ‰°ç‰©ä½“æ—¶ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°äº†XX%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æ™ºèƒ½æœºå™¨äººã€å¢å¼ºç°å®å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜3Dè§†è§‰å®šä½çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥æ˜¾è‘—æå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç†è§£å’Œäº¤äº’èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥å’Œåº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D visual grounding aims to localize the unique target described by natural languages in 3D scenes. The significant gap between 3D and language modalities makes it a notable challenge to distinguish multiple similar objects through the described spatial relationships. Current methods attempt to achieve cross-modal understanding in complex scenes via a target-centered learning mechanism, ignoring the modeling of referred objects. We propose a novel 3D visual grounding framework that constructs language-guided scene graphs with referred object discrimination to improve relational perception. The framework incorporates a dual-branch visual encoder that leverages pre-trained 2D semantics to enhance and supervise the multi-modal 3D encoding. Furthermore, we employ graph attention to promote relationship-oriented information fusion in cross-modal interaction. The learned object representations and scene graph structure enable effective alignment between 3D visual content and textual descriptions. Experimental results on popular benchmarks demonstrate our superior performance compared to state-of-the-art methods, especially in handling the challenges of multiple similar distractors.

