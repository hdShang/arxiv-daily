---
layout: default
title: "One2Any: One-Reference 6D Pose Estimation for Any Object"
---

# One2Any: One-Reference 6D Pose Estimation for Any Object

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04109" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04109v1</a>
  <a href="https://arxiv.org/pdf/2505.04109.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04109v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04109v1', 'One2Any: One-Reference 6D Pose Estimation for Any Object')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengya Liu, Siyuan Li, Ajad Chhatkuli, Prune Truong, Luc Van Gool, Federico Tombari

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-07

**å¤‡æ³¨**: accepted by CVPR 2025

**æœŸåˆŠ**: CVPR 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOne2Anyä»¥è§£å†³6Dç‰©ä½“å§¿æ€ä¼°è®¡çš„æ¨¡å‹ä¾èµ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `6Då§¿æ€ä¼°è®¡` `ç‰©ä½“è¯†åˆ«` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰` `æœºå™¨äººæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„6Dç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•ä¾èµ–äºå®Œæ•´çš„3Dæ¨¡å‹å’Œå¤šè§†è§’å›¾åƒï¼Œé™åˆ¶äº†å…¶åœ¨æ–°ç‰©ä½“ä¸Šçš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„One2Anyæ–¹æ³•é€šè¿‡å•ä¸ªRGB-Då›¾åƒè¿›è¡Œå§¿æ€ä¼°è®¡ï¼Œé¿å…äº†å¯¹3Dæ¨¡å‹å’Œå¤šè§†è§’æ•°æ®çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—æ•ˆç‡æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

6Dç‰©ä½“å§¿æ€ä¼°è®¡åœ¨è®¸å¤šåº”ç”¨ä¸­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦ç”±äºå¯¹å®Œæ•´3Dæ¨¡å‹ã€å¤šè§†è§’å›¾åƒçš„ä¾èµ–ï¼Œä»¥åŠè®­ç»ƒé™åˆ¶äºç‰¹å®šç‰©ä½“ç±»åˆ«ã€‚è¿™äº›è¦æ±‚ä½¿å¾—å¯¹æ–°é¢–ç‰©ä½“çš„æ³›åŒ–å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•One2Anyï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨å•ä¸ªå‚è€ƒ-æŸ¥è¯¢RGB-Då›¾åƒæ¥ä¼°è®¡ç›¸å¯¹6è‡ªç”±åº¦ç‰©ä½“å§¿æ€ï¼Œè€Œæ— éœ€å…ˆéªŒçš„3Dæ¨¡å‹ã€å¤šè§†è§’æ•°æ®æˆ–ç±»åˆ«çº¦æŸã€‚æˆ‘ä»¬å°†ç‰©ä½“å§¿æ€ä¼°è®¡è§†ä¸ºç¼–ç -è§£ç è¿‡ç¨‹ï¼Œé¦–å…ˆä»å•ä¸ªå‚è€ƒè§†å›¾ä¸­è·å–å…¨é¢çš„å‚è€ƒç‰©ä½“å§¿æ€åµŒå…¥ï¼ˆROPEï¼‰ï¼Œè¯¥åµŒå…¥ç¼–ç äº†ç‰©ä½“çš„å½¢çŠ¶ã€æ–¹å‘å’Œçº¹ç†ã€‚åˆ©ç”¨è¿™ä¸€åµŒå…¥ï¼ŒåŸºäºU-Netçš„å§¿æ€è§£ç æ¨¡å—ä¸ºæ–°è§†å›¾ç”Ÿæˆå‚è€ƒç‰©ä½“åæ ‡ï¼ˆROCï¼‰ï¼Œä»è€Œå®ç°å¿«é€Ÿè€Œå‡†ç¡®çš„å§¿æ€ä¼°è®¡ã€‚è¯¥ç®€å•çš„ç¼–ç -è§£ç æ¡†æ¶ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»æ„æˆå¯¹å§¿æ€æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç¤ºäº†è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¯¹æ–°é¢–ç‰©ä½“å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œç”šè‡³ä¸éœ€è¦å¤šè§†è§’æˆ–CADè¾“å…¥çš„æ–¹æ³•ç›¸åª²ç¾ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬å¤§å¹…é™ä½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³6Dç‰©ä½“å§¿æ€ä¼°è®¡ä¸­å¯¹å®Œæ•´3Dæ¨¡å‹å’Œå¤šè§†è§’å›¾åƒçš„ä¾èµ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ–°ç‰©ä½“æ—¶é¢ä¸´æ³›åŒ–å›°éš¾ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOne2Anyæ–¹æ³•é€šè¿‡å•ä¸ªå‚è€ƒRGB-Då›¾åƒè¿›è¡Œå§¿æ€ä¼°è®¡ï¼Œå°†ç‰©ä½“å§¿æ€ä¼°è®¡è§†ä¸ºç¼–ç -è§£ç è¿‡ç¨‹ã€‚é€šè¿‡æå–å‚è€ƒç‰©ä½“å§¿æ€åµŒå…¥ï¼ˆROPEï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰3Dæ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„å§¿æ€æ¨æ–­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å‚è€ƒç‰©ä½“å§¿æ€åµŒå…¥æ¨¡å—ï¼Œé€šè¿‡å•ä¸ªè§†å›¾æå–ç‰©ä½“çš„å½¢çŠ¶ã€æ–¹å‘å’Œçº¹ç†ä¿¡æ¯ï¼›å…¶æ¬¡æ˜¯åŸºäºU-Netçš„å§¿æ€è§£ç æ¨¡å—ï¼Œåˆ©ç”¨ROPEä¸ºæ–°è§†å›¾ç”Ÿæˆå‚è€ƒç‰©ä½“åæ ‡ï¼ˆROCï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šOne2Anyçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿåœ¨æ²¡æœ‰3Dæ¨¡å‹å’Œå¤šè§†è§’æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»…ä¾èµ–å•ä¸ªå›¾åƒè¿›è¡Œé«˜æ•ˆçš„6Då§¿æ€ä¼°è®¡ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†U-Netç»“æ„ä½œä¸ºè§£ç æ¨¡å—ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºé€‚åº”æˆå¯¹å§¿æ€æ•°æ®çš„è®­ç»ƒï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOne2Anyåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨å¤„ç†æ–°é¢–ç‰©ä½“æ—¶è¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—èµ„æºæ¶ˆè€—ä¸Šå¤§å¹…é™ä½ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€å¢å¼ºç°å®å’Œè‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿåœ¨ç¼ºä¹3Dæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„ç‰©ä½“è¯†åˆ«ä¸å®šä½ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸçš„æ™ºèƒ½åŒ–è¿›ç¨‹ï¼Œæå‡ç‰©ä½“äº¤äº’çš„æ™ºèƒ½æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories. These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available. To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints. We treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object shape, orientation, and texture from a single reference view. Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation. This simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute.

