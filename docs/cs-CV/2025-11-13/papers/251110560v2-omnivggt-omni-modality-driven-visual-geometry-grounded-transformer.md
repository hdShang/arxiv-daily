---
layout: default
title: "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer"
---

# OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.10560" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.10560v2</a>
  <a href="https://arxiv.org/pdf/2511.10560.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10560v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.10560v2', 'OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haosong Peng, Hao Li, Yalun Dai, Yushi Lan, Yihang Luo, Tianyu Qi, Zhengshen Zhang, Yufeng Zhan, Junfei Zhang, Wenchao Xu, Ziwei Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-13 (æ›´æ–°: 2025-11-14)

**å¤‡æ³¨**: Project Page: https://livioni.github.io/OmniVGGT-official/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniVGGTï¼šå¤šæ¨¡æ€é©±åŠ¨çš„è§†è§‰å‡ ä½•å¯¹é½Transformerï¼Œæå‡3Dè§†è§‰ä»»åŠ¡æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `å‡ ä½•ä¿¡æ¯` `3Dè§†è§‰` `Transformer` `æ·±åº¦ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3DåŸºç¡€æ¨¡å‹ä¸»è¦ä¾èµ–RGBè¾“å…¥ï¼Œå¿½ç•¥äº†æ˜“äºè·å–çš„å‡ ä½•ä¿¡æ¯ï¼ˆå¦‚ç›¸æœºå‚æ•°ã€ä½å§¿ã€æ·±åº¦å›¾ï¼‰ï¼Œé™åˆ¶äº†æ€§èƒ½ã€‚
2. OmniVGGTé€šè¿‡GeoAdapterå°†å‡ ä½•ä¿¡æ¯ç¼–ç åˆ°ç©ºé—´åŸºç¡€æ¨¡å‹ä¸­ï¼Œå¹¶é‡‡ç”¨éšæœºå¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œæå‡æ¨¡å‹å¯¹ä¸åŒæ¨¡æ€è¾“å…¥çš„é€‚åº”æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOmniVGGTåœ¨å¤šä¸ª3Dè§†è§‰ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­æœ‰æ•ˆæå‡äº†æœºå™¨äººä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºOmniVGGTï¼Œä¸€ä¸ªèƒ½æœ‰æ•ˆåˆ©ç”¨ä»»æ„æ•°é‡è¾…åŠ©å‡ ä½•æ¨¡æ€ï¼ˆå¦‚ç›¸æœºå†…å‚ã€ä½å§¿å’Œæ·±åº¦å›¾ï¼‰è¿›è¡Œè®­ç»ƒå’Œæ¨ç†çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥GeoAdapterï¼Œå°†æ·±åº¦å’Œç›¸æœºå†…å¤–å‚ç¼–ç åˆ°ç©ºé—´åŸºç¡€æ¨¡å‹ä¸­ã€‚GeoAdapteré‡‡ç”¨é›¶åˆå§‹åŒ–çš„å·ç§¯ï¼Œé€æ­¥æ³¨å…¥å‡ ä½•ä¿¡æ¯ï¼Œä¸ç ´ååŸºç¡€æ¨¡å‹çš„è¡¨å¾ç©ºé—´ï¼Œä¿è¯äº†ç¨³å®šçš„ä¼˜åŒ–å’Œå¯å¿½ç•¥çš„å¼€é”€ï¼Œä½¿æ¨ç†é€Ÿåº¦ä¸VGGTç›¸å½“ã€‚æ­¤å¤–ï¼Œæå‡ºäº†éšæœºå¤šæ¨¡æ€èåˆæ–¹æ¡ˆï¼Œåœ¨è®­ç»ƒæœŸé—´éšæœºé‡‡æ ·æ¨¡æ€å­é›†ï¼Œä»è€Œåœ¨æµ‹è¯•æœŸé—´å¯ä»¥ä½¿ç”¨ä»»æ„æ•°é‡çš„æ¨¡æ€è¾“å…¥ï¼Œå¹¶ä¿ƒè¿›å­¦ä¹ é²æ£’çš„ç©ºé—´è¡¨ç¤ºï¼Œé¿å…è¿‡æ‹Ÿåˆè¾…åŠ©ä¿¡æ¯ã€‚åœ¨å•ç›®/å¤šè§†è§’æ·±åº¦ä¼°è®¡ã€å¤šè§†è§’ç«‹ä½“å’Œç›¸æœºä½å§¿ä¼°è®¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOmniVGGTä¼˜äºä»¥å¾€ä½¿ç”¨è¾…åŠ©è¾“å…¥çš„æ–¹æ³•ï¼Œå³ä½¿ä»…ä½¿ç”¨RGBè¾“å…¥ä¹Ÿèƒ½è¾¾åˆ°SOTAæ°´å¹³ã€‚OmniVGGTä¹Ÿè¢«é›†æˆåˆ°è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¸­ï¼Œåœ¨ä¸»æµåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºäºç‚¹äº‘çš„baselineï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨è¾…åŠ©è¾“å…¥åœ¨æœºå™¨äººä»»åŠ¡ä¸Šå–å¾—æŒç»­æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰é€šç”¨3DåŸºç¡€æ¨¡å‹ä¸»è¦ä¾èµ–RGBå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¿½ç•¥äº†ç›¸æœºå†…å‚ã€å¤–å‚å’Œæ·±åº¦å›¾ç­‰å‡ ä½•ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥æ˜¾è‘—æå‡3Dè§†è§‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆè¿™äº›å¼‚æ„çš„å‡ ä½•ä¿¡æ¯ï¼Œå¹¶ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆè¾…åŠ©ä¿¡æ¯ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmniVGGTçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„GeoAdapteræ¨¡å—ï¼Œå°†å‡ ä½•ä¿¡æ¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„è§†è§‰Transformeræ¶æ„ä¸­ã€‚GeoAdapteré‡‡ç”¨é›¶åˆå§‹åŒ–çš„å·ç§¯å±‚ï¼Œé€æ­¥å°†å‡ ä½•ä¿¡æ¯æ³¨å…¥åˆ°ç‰¹å¾è¡¨ç¤ºä¸­ï¼Œé¿å…ç ´åé¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ã€‚åŒæ—¶ï¼Œé‡‡ç”¨éšæœºå¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºé€‰æ‹©ä¸åŒçš„æ¨¡æ€ç»„åˆï¼Œå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniVGGTçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰Transformerï¼šä½œä¸ºåŸºç¡€çš„ç©ºé—´ç‰¹å¾æå–å™¨ï¼Œä¾‹å¦‚VGGTã€‚2) GeoAdapterï¼šå°†æ·±åº¦å›¾å’Œç›¸æœºå†…å¤–å‚ç­‰å‡ ä½•ä¿¡æ¯ç¼–ç æˆç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ³¨å…¥åˆ°è§†è§‰Transformerä¸­ã€‚3) å¤šæ¨¡æ€èåˆæ¨¡å—ï¼šå°†è§†è§‰ç‰¹å¾å’Œå‡ ä½•ç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚4) ä»»åŠ¡ç‰¹å®šæ¨¡å—ï¼šåŸºäºèåˆåçš„ç‰¹å¾è¡¨ç¤ºï¼Œå®Œæˆç‰¹å®šçš„3Dè§†è§‰ä»»åŠ¡ï¼Œå¦‚æ·±åº¦ä¼°è®¡ã€ä½å§¿ä¼°è®¡ç­‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniVGGTçš„å…³é”®åˆ›æ–°åœ¨äºGeoAdapterå’Œéšæœºå¤šæ¨¡æ€èåˆç­–ç•¥ã€‚GeoAdapteré€šè¿‡é›¶åˆå§‹åŒ–å·ç§¯å±‚ï¼Œå®ç°äº†å‡ ä½•ä¿¡æ¯çš„æ— ç¼é›†æˆï¼Œé¿å…äº†ç ´åé¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ã€‚éšæœºå¤šæ¨¡æ€èåˆç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ¨¡æ€ç»„åˆã€‚

**å…³é”®è®¾è®¡**ï¼šGeoAdapteré‡‡ç”¨å¤šä¸ªå·ç§¯å±‚é€æ­¥æå–å‡ ä½•ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨æ®‹å·®è¿æ¥å°†å‡ ä½•ç‰¹å¾æ³¨å…¥åˆ°è§†è§‰ç‰¹å¾ä¸­ã€‚é›¶åˆå§‹åŒ–ä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚éšæœºå¤šæ¨¡æ€èåˆç­–ç•¥é€šè¿‡éšæœºmaskæ‰ä¸åŒçš„æ¨¡æ€ï¼Œæ¨¡æ‹Ÿäº†ä¸åŒçš„è¾“å…¥æƒ…å†µã€‚æŸå¤±å‡½æ•°æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè®¾è®¡ï¼Œä¾‹å¦‚æ·±åº¦ä¼°è®¡ä»»åŠ¡ä½¿ç”¨L1æŸå¤±æˆ–HuberæŸå¤±ï¼Œä½å§¿ä¼°è®¡ä»»åŠ¡ä½¿ç”¨é‡æŠ•å½±è¯¯å·®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OmniVGGTåœ¨å•ç›®/å¤šè§†è§’æ·±åº¦ä¼°è®¡ã€å¤šè§†è§’ç«‹ä½“å’Œç›¸æœºä½å§¿ä¼°è®¡ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤šè§†è§’æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­ï¼ŒOmniVGGTç›¸æ¯”äºä»…ä½¿ç”¨RGBå›¾åƒçš„æ–¹æ³•ï¼Œæ€§èƒ½æå‡äº†10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒOmniVGGTåœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¯æ˜äº†å…¶åœ¨æœºå™¨äººä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniVGGTå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€ä¸‰ç»´é‡å»ºã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨å‡ ä½•ä¿¡æ¯ï¼Œå¯ä»¥æå‡è¿™äº›åº”ç”¨åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤šæ¨¡æ€çš„èåˆï¼Œä¾‹å¦‚æ¿€å…‰é›·è¾¾ã€IMUç­‰ï¼Œä»è€Œå®ç°æ›´å…¨é¢çš„ç¯å¢ƒæ„ŸçŸ¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.

