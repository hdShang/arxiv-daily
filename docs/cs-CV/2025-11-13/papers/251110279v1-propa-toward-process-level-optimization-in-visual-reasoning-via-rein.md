---
layout: default
title: "PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning"
---

# PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.10279" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.10279v1</a>
  <a href="https://arxiv.org/pdf/2511.10279.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10279v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.10279v1', 'PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yanbei Jiang, Chao Lei, Yihao Ding, Krista Ehinger, Jey Han Lau

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-13

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/YanbeiJiang/PROPA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPROPAæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è§†è§‰æ¨ç†ä¸­çš„è¿‡ç¨‹çº§ä¾èµ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `è¿‡ç¨‹çº§ä¼˜åŒ–` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä¸­æ˜“å‡ºé”™ï¼ŒåŸå› æ˜¯å¤šæ­¥ä¾èµ–å¯¼è‡´æ—©æœŸé”™è¯¯ç´¯ç§¯ã€‚
2. PROPAåˆ©ç”¨MCTSä¸GRPOç»“åˆï¼Œç”Ÿæˆå¯†é›†è¿‡ç¨‹å¥–åŠ±ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯ä¼˜åŒ–ä¸­é—´æ­¥éª¤ã€‚
3. PROPAé€šè¿‡äº¤é”™GRPOå’ŒSFTæ›´æ–°ï¼Œå¹¶è®­ç»ƒPRMï¼Œæ˜¾è‘—æå‡äº†è§†è§‰æ¨ç†çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨å¤æ‚è§†è§‰æ¨ç†ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¤šæ­¥ä¾èµ–å…³ç³»å¯¼è‡´æ—©æœŸé”™è¯¯åœ¨æ¨ç†é“¾ä¸­ç´¯ç§¯ã€‚ç°æœ‰çš„åè®­ç»ƒæ–¹æ³•å­˜åœ¨å±€é™ï¼šç›‘ç£å¾®è°ƒ(SFT)ä¾èµ–äºæ˜‚è´µçš„æ­¥éª¤çº§æ ‡æ³¨ï¼Œè€ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)æ–¹æ³•ï¼Œå¦‚GRPOï¼Œä»…æä¾›ç¨€ç–çš„ç»“æœçº§åé¦ˆï¼Œé˜»ç¢äº†ç¨³å®šä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†PROPAï¼ˆå…·æœ‰äº¤é”™ç­–ç•¥å¯¹é½çš„è¿‡ç¨‹çº§æ¨ç†ä¼˜åŒ–ï¼‰ï¼Œè¯¥æ¡†æ¶é›†æˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)ä¸GRPOï¼Œä»¥ç”Ÿæˆå¯†é›†çš„ã€è¿‡ç¨‹çº§çš„å¥–åŠ±ï¼Œå¹¶åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¯ä¸ªä¸­é—´æ­¥éª¤çš„æ¨ç†ã€‚ä¸ºäº†å…‹æœå†·å¯åŠ¨é—®é¢˜ï¼ŒPROPAå°†GRPOæ›´æ–°ä¸SFTäº¤é”™è¿›è¡Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æˆåŠŸå’Œå¤±è´¥çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ ã€‚è¿›ä¸€æ­¥è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹(PRM)ä»¥æŒ‡å¯¼æ¨ç†æ—¶çš„æœç´¢ï¼Œä½¿æµ‹è¯•æ—¶æœç´¢ä¸è®­ç»ƒä¿¡å·å¯¹é½ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•å’Œå››ä¸ªVLMéª¨å¹²ç½‘ç»œä¸Šï¼ŒPROPAå§‹ç»ˆä¼˜äºåŸºäºSFTå’ŒRLVRçš„åŸºçº¿ï¼Œåœ¨é¢†åŸŸå†…ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾17.0%çš„å¢ç›Šï¼Œåœ¨é¢†åŸŸå¤–ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾21.0%çš„å¢ç›Šï¼Œä¸ºè§†è§‰æ¨ç†ä»»åŠ¡å»ºç«‹äº†å¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ç”±äºå¤šæ­¥ä¾èµ–å…³ç³»å¯¼è‡´çš„æ—©æœŸé”™è¯¯ç´¯ç§¯é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒ(SFT)ï¼Œéœ€è¦æ˜‚è´µçš„æ­¥éª¤çº§æ ‡æ³¨ï¼Œè€ŒåŸºäºç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)æ–¹æ³•ï¼Œå¦‚GRPOï¼Œæä¾›çš„åé¦ˆè¿‡äºç¨€ç–ï¼Œéš¾ä»¥è¿›è¡Œæœ‰æ•ˆçš„è¿‡ç¨‹çº§ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPROPAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œå¯¹è§†è§‰æ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥è¿›è¡Œä¼˜åŒ–ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚å®ƒåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)æ¥æ¢ç´¢ä¸åŒçš„æ¨ç†è·¯å¾„ï¼Œå¹¶ç»“åˆGRPOæ¥å­¦ä¹ ç­–ç•¥ã€‚é€šè¿‡MCTSç”Ÿæˆå¯†é›†çš„ã€è¿‡ç¨‹çº§çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•ä¸­å¥–åŠ±ç¨€ç–çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPROPAæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†è§‰è¯­è¨€æ¨¡å‹(VLM)ï¼šä½œä¸ºæ¨ç†çš„ä¸»ä½“ï¼›2) è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)ï¼šç”¨äºæ¢ç´¢ä¸åŒçš„æ¨ç†è·¯å¾„ï¼Œå¹¶ç”Ÿæˆè¿‡ç¨‹çº§çš„å¥–åŠ±ä¿¡å·ï¼›3) GRPOï¼šç”¨äºæ›´æ–°VLMçš„ç­–ç•¥ï¼›4) è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(PRM)ï¼šç”¨äºæŒ‡å¯¼æ¨ç†æ—¶çš„æœç´¢ï¼Œä½¿æµ‹è¯•æ—¶æœç´¢ä¸è®­ç»ƒä¿¡å·å¯¹é½ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆä½¿ç”¨SFTè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åäº¤é”™è¿›è¡ŒGRPOæ›´æ–°å’ŒSFTæ›´æ–°ï¼ŒåŒæ—¶è®­ç»ƒPRMã€‚åœ¨æ¨ç†æ—¶ï¼Œä½¿ç”¨PRMæŒ‡å¯¼MCTSæœç´¢ï¼Œå¹¶é€‰æ‹©æœ€ä¼˜çš„æ¨ç†è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šPROPAçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†è¿‡ç¨‹çº§æ¨ç†ä¼˜åŒ–çš„æ¦‚å¿µï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹æ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥è¿›è¡Œä¼˜åŒ–ï¼›2) å°†MCTSä¸GRPOç»“åˆï¼Œç”Ÿæˆå¯†é›†çš„ã€è¿‡ç¨‹çº§çš„å¥–åŠ±ä¿¡å·ï¼Œå…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•ä¸­å¥–åŠ±ç¨€ç–çš„é—®é¢˜ï¼›3) æå‡ºäº†äº¤é”™æ›´æ–°ç­–ç•¥ï¼Œå°†GRPOæ›´æ–°ä¸SFTæ›´æ–°äº¤é”™è¿›è¡Œï¼Œå…‹æœäº†å†·å¯åŠ¨é—®é¢˜ï¼›4) è®­ç»ƒPRMæ¥æŒ‡å¯¼æ¨ç†æ—¶çš„æœç´¢ï¼Œä½¿æµ‹è¯•æ—¶æœç´¢ä¸è®­ç»ƒä¿¡å·å¯¹é½ã€‚

**å…³é”®è®¾è®¡**ï¼šPROPAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼šå¥–åŠ±å‡½æ•°ç»¼åˆè€ƒè™‘äº†æ¨ç†çš„æ­£ç¡®æ€§å’Œæ•ˆç‡ï¼Œé¼“åŠ±æ¨¡å‹é€‰æ‹©æ›´çŸ­ã€æ›´æ­£ç¡®çš„æ¨ç†è·¯å¾„ï¼›2) MCTSçš„æœç´¢ç­–ç•¥ï¼šMCTSä½¿ç”¨UCTç®—æ³•è¿›è¡Œæœç´¢ï¼Œå¹³è¡¡äº†æ¢ç´¢å’Œåˆ©ç”¨ï¼›3) PRMçš„ç½‘ç»œç»“æ„ï¼šPRMæ˜¯ä¸€ä¸ªTransformeræ¨¡å‹ï¼Œè¾“å…¥æ˜¯æ¨ç†æ­¥éª¤çš„ä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºæ˜¯è¯¥æ­¥éª¤çš„å¥–åŠ±é¢„æµ‹ï¼›4) äº¤é”™æ›´æ–°çš„æ¯”ä¾‹ï¼šè®ºæ–‡ä¸­å®éªŒäº†ä¸åŒçš„GRPOæ›´æ–°å’ŒSFTæ›´æ–°çš„æ¯”ä¾‹ï¼Œæœ€ç»ˆé€‰æ‹©äº†ä¸€ä¸ªæœ€ä¼˜çš„æ¯”ä¾‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PROPAåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•å’Œå››ä¸ªVLMéª¨å¹²ç½‘ç»œä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜PROPAå§‹ç»ˆä¼˜äºåŸºäºSFTå’ŒRLVRçš„åŸºçº¿ã€‚åœ¨é¢†åŸŸå†…ä»»åŠ¡ä¸Šï¼ŒPROPAå®ç°äº†é«˜è¾¾17.0%çš„å¢ç›Šï¼Œåœ¨é¢†åŸŸå¤–ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾21.0%çš„å¢ç›Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPROPAå…·æœ‰å¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³è§†è§‰æ¨ç†ä¸­çš„è¿‡ç¨‹çº§ä¾èµ–é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PROPAæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦å¤æ‚è§†è§‰æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆã€æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥æé«˜è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®é™…ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.

