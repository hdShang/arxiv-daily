---
layout: default
title: Depth Anything 3: Recovering the Visual Space from Any Views
---

# Depth Anything 3: Recovering the Visual Space from Any Views

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.10647" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.10647v1</a>
  <a href="https://arxiv.org/pdf/2511.10647.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10647v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.10647v1', 'Depth Anything 3: Recovering the Visual Space from Any Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-13

**å¤‡æ³¨**: https://depth-anything-3.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Depth Anything 3ï¼šä»ä»»æ„è§†è§’æ¢å¤ç©ºé—´å‡ ä½•ä¿¡æ¯ï¼Œæ— éœ€æ¶æ„ç‰¹åŒ–ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ·±åº¦ä¼°è®¡` `è§†è§‰å‡ ä½•` `Transformer` `è‡ªç›‘ç£å­¦ä¹ ` `å¤šè§†è§’é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä»å¤šè§†è§’å›¾åƒä¸­æ¢å¤å‡ ä½•ä¿¡æ¯æ—¶ï¼Œé€šå¸¸éœ€è¦å¤æ‚çš„ç½‘ç»œç»“æ„å’Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œå¢åŠ äº†æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒéš¾åº¦ã€‚
2. DA3çš„æ ¸å¿ƒæ€æƒ³æ˜¯é‡‡ç”¨æç®€å»ºæ¨¡æ–¹å¼ï¼Œä½¿ç”¨å•ä¸€çš„Transformeréª¨å¹²ç½‘ç»œå’Œæ·±åº¦å°„çº¿é¢„æµ‹ç›®æ ‡ï¼Œé¿å…äº†æ¶æ„ç‰¹åŒ–å’Œå¤æ‚çš„å¤šä»»åŠ¡å­¦ä¹ ã€‚
3. DA3åœ¨æ–°çš„è§†è§‰å‡ ä½•åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨ç›¸æœºä½å§¿å’Œå‡ ä½•ç²¾åº¦æ–¹é¢å‡è¶…è¶Šäº†ç°æœ‰SOTAæ–¹æ³•ï¼Œå¹¶åœ¨å•ç›®æ·±åº¦ä¼°è®¡ä¸Šä¼˜äºDA2ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†Depth Anything 3 (DA3)ï¼Œä¸€ä¸ªèƒ½å¤Ÿä»ä»»æ„æ•°é‡çš„è§†è§‰è¾“å…¥ä¸­é¢„æµ‹ç©ºé—´ä¸€è‡´å‡ ä½•ä¿¡æ¯çš„æ¨¡å‹ï¼Œæ— è®ºæ˜¯å¦å·²çŸ¥ç›¸æœºä½å§¿ã€‚DA3è¿½æ±‚æœ€å°åŒ–å»ºæ¨¡ï¼Œå¹¶æå‡ºäº†ä¸¤ä¸ªå…³é”®è§è§£ï¼šä¸€ä¸ªç®€å•çš„Transformerï¼ˆä¾‹å¦‚ï¼Œvanilla DINOç¼–ç å™¨ï¼‰è¶³ä»¥ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œæ— éœ€æ¶æ„ç‰¹åŒ–ï¼›ä»¥åŠå•ä¸€çš„æ·±åº¦å°„çº¿é¢„æµ‹ç›®æ ‡æ¶ˆé™¤äº†å¤æ‚çš„å¤šä»»åŠ¡å­¦ä¹ çš„éœ€è¦ã€‚é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè®­ç»ƒèŒƒå¼ï¼Œè¯¥æ¨¡å‹å®ç°äº†ä¸Depth Anything 2 (DA2)ç›¸å½“çš„ç»†èŠ‚å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„è§†è§‰å‡ ä½•åŸºå‡†ï¼Œæ¶µç›–ç›¸æœºä½å§¿ä¼°è®¡ã€ä»»æ„è§†è§’å‡ ä½•å’Œè§†è§‰æ¸²æŸ“ã€‚åœ¨è¯¥åŸºå‡†ä¸Šï¼ŒDA3åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°äº†æ–°çš„state-of-the-artï¼Œåœ¨ç›¸æœºä½å§¿ç²¾åº¦æ–¹é¢å¹³å‡è¶…è¿‡å…ˆå‰çš„SOTA VGGT 44.3%ï¼Œåœ¨å‡ ä½•ç²¾åº¦æ–¹é¢è¶…è¿‡25.1%ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å•ç›®æ·±åº¦ä¼°è®¡æ–¹é¢ä¼˜äºDA2ã€‚æ‰€æœ‰æ¨¡å‹éƒ½ä»…åœ¨å…¬å…±å­¦æœ¯æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»ä»»æ„æ•°é‡çš„è§†è§‰è¾“å…¥ä¸­æ¢å¤ç©ºé—´ä¸€è‡´çš„å‡ ä½•ä¿¡æ¯çš„é—®é¢˜ï¼Œæ— è®ºæ˜¯å¦å·²çŸ¥ç›¸æœºä½å§¿ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„ç½‘ç»œæ¶æ„å’Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œè¿™å¢åŠ äº†æ¨¡å‹çš„å¤æ‚æ€§ï¼Œå¹¶å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡çš„æ¶æ„å¯èƒ½é™åˆ¶äº†æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDA3çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨æç®€å»ºæ¨¡æ–¹å¼ï¼Œå³ä½¿ç”¨ä¸€ä¸ªç®€å•çš„Transformeréª¨å¹²ç½‘ç»œï¼ˆå¦‚vanilla DINOç¼–ç å™¨ï¼‰å’Œå•ä¸€çš„æ·±åº¦å°„çº¿é¢„æµ‹ç›®æ ‡ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å‡å°‘æ¨¡å‹çš„å¤æ‚æ€§ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡é¿å…æ¶æ„ç‰¹åŒ–å’Œå¤æ‚çš„å¤šä»»åŠ¡å­¦ä¹ ï¼ŒDA3èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ è§†è§‰ç©ºé—´ä¸­çš„å‡ ä½•ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDA3çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªTransformerç¼–ç å™¨å’Œä¸€ä¸ªæ·±åº¦é¢„æµ‹æ¨¡å—ã€‚Transformerç¼–ç å™¨è´Ÿè´£æå–è¾“å…¥å›¾åƒçš„ç‰¹å¾ï¼Œæ·±åº¦é¢„æµ‹æ¨¡å—åˆ™æ ¹æ®è¿™äº›ç‰¹å¾é¢„æµ‹æ¯ä¸ªåƒç´ çš„æ·±åº¦å€¼ã€‚æ¨¡å‹é‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿè®­ç»ƒèŒƒå¼ï¼Œå…¶ä¸­æ•™å¸ˆæ¨¡å‹æä¾›é«˜è´¨é‡çš„æ·±åº¦ä¿¡æ¯ï¼Œå­¦ç”Ÿæ¨¡å‹åˆ™å­¦ä¹ æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šDA3æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶æç®€å»ºæ¨¡æ–¹å¼ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDA3é¿å…äº†å¤æ‚çš„ç½‘ç»œæ¶æ„å’Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œè€Œæ˜¯é‡‡ç”¨äº†ä¸€ä¸ªç®€å•çš„Transformeréª¨å¹²ç½‘ç»œå’Œå•ä¸€çš„æ·±åº¦å°„çº¿é¢„æµ‹ç›®æ ‡ã€‚è¿™ç§è®¾è®¡ä¸ä»…é™ä½äº†æ¨¡å‹çš„å¤æ‚æ€§ï¼Œè¿˜æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šDA3çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨vanilla DINOç¼–ç å™¨ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œæ— éœ€ä»»ä½•ä¿®æ”¹ï¼›2) é‡‡ç”¨æ·±åº¦å°„çº¿é¢„æµ‹ä½œä¸ºå•ä¸€çš„å­¦ä¹ ç›®æ ‡ï¼Œé¿å…äº†å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„ä»»åŠ¡å†²çªï¼›3) ä½¿ç”¨æ•™å¸ˆ-å­¦ç”Ÿè®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹æä¾›é«˜è´¨é‡çš„æ·±åº¦ä¿¡æ¯ï¼ŒæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DA3åœ¨æ–°çš„è§†è§‰å‡ ä½•åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨ç›¸æœºä½å§¿ç²¾åº¦æ–¹é¢å¹³å‡è¶…è¿‡å…ˆå‰çš„SOTA VGGT 44.3%ï¼Œåœ¨å‡ ä½•ç²¾åº¦æ–¹é¢è¶…è¿‡25.1%ã€‚æ­¤å¤–ï¼ŒDA3åœ¨å•ç›®æ·±åº¦ä¼°è®¡æ–¹é¢ä¹Ÿä¼˜äºDA2ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ‰€æœ‰æ¨¡å‹éƒ½ä»…åœ¨å…¬å…±å­¦æœ¯æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DA3å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºæ„å»ºä¸‰ç»´åœºæ™¯åœ°å›¾ã€è¿›è¡Œç‰©ä½“è¯†åˆ«å’Œè·Ÿè¸ªã€ä»¥åŠå®ç°é€¼çœŸçš„è§†è§‰æ¸²æŸ“ã€‚é€šè¿‡ä»ä»»æ„è§†è§’æ¢å¤ç©ºé—´å‡ ä½•ä¿¡æ¯ï¼ŒDA3å¯ä»¥ä¸ºè¿™äº›åº”ç”¨æä¾›æ›´å‡†ç¡®ã€æ›´å¯é çš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

