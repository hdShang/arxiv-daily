---
layout: default
title: ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts
---

# ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26186" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26186v1</a>
  <a href="https://arxiv.org/pdf/2510.26186.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26186v1" onclick="toggleFavorite(this, '2510.26186v1', 'ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinho Choi, Hyesu Lim, Steffen Schneider, Jaegul Choo

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30

**å¤‡æ³¨**: Published in the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ConceptScopeï¼šé€šè¿‡è§£è€¦è§†è§‰æ¦‚å¿µè¡¨å¾æ¥é‡åŒ–å’Œè¯†åˆ«æ•°æ®é›†åå·®ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ•°æ®é›†åå·®` `è§†è§‰æ¦‚å¿µ` `ç¨€ç–è‡ªç¼–ç å™¨` `å¯è§£é‡Šæ€§` `æ¨¡å‹è¯Šæ–­`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹åœ¨æ²¡æœ‰ç»†ç²’åº¦æ ‡æ³¨çš„æƒ…å†µä¸‹æœ‰æ•ˆè¯†åˆ«æ•°æ®é›†åå·®çš„èƒ½åŠ›ï¼Œé™åˆ¶äº†æ¨¡å‹é²æ£’æ€§å’Œå…¬å¹³æ€§ã€‚
2. ConceptScopeåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„è¡¨å¾ï¼Œé€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨è‡ªåŠ¨å‘ç°å’Œé‡åŒ–äººç±»å¯è§£é‡Šçš„è§†è§‰æ¦‚å¿µã€‚
3. å®éªŒè¡¨æ˜ConceptScopeèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å·²çŸ¥å’ŒæœªçŸ¥çš„åå·®ï¼Œå¹¶æä¾›ä¸è¯­ä¹‰ç›¸å…³çš„ç©ºé—´å½’å› ï¼Œæå‡æ•°æ®é›†å®¡è®¡èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ•°æ®é›†åå·®åœ¨æœºå™¨å­¦ä¹ æ•°æ®é›†ä¸­æ™®éå­˜åœ¨ï¼Œè¡¨ç°ä¸ºæ•°æ®ç‚¹åå‘æŸäº›æ¦‚å¿µã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰æ˜‚è´µä¸”ç»†ç²’åº¦çš„å±æ€§æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œç³»ç»Ÿåœ°è¯†åˆ«è¿™äº›åå·®å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ConceptScopeï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨åœ¨è§†è§‰åŸºç¡€æ¨¡å‹è¡¨å¾ä¸Šè®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨æ¥å‘ç°å’Œé‡åŒ–äººç±»å¯è§£é‡Šçš„æ¦‚å¿µï¼Œä»è€Œåˆ†æè§†è§‰æ•°æ®é›†ã€‚ConceptScopeæ ¹æ®æ¦‚å¿µçš„è¯­ä¹‰ç›¸å…³æ€§å’Œä¸ç±»æ ‡ç­¾çš„ç»Ÿè®¡ç›¸å…³æ€§å°†æ¦‚å¿µåˆ†ç±»ä¸ºç›®æ ‡ã€ä¸Šä¸‹æ–‡å’Œåå·®ç±»å‹ï¼Œä»è€Œå®ç°ç±»çº§åˆ«çš„æ•°æ®é›†ç‰¹å¾æè¿°ã€åå·®è¯†åˆ«ä»¥åŠé€šè¿‡åŸºäºæ¦‚å¿µçš„å­åˆ†ç»„è¿›è¡Œé²æ£’æ€§è¯„ä¼°ã€‚é€šè¿‡ä¸å¸¦æ ‡æ³¨æ•°æ®é›†çš„æ¯”è¾ƒï¼ŒéªŒè¯äº†ConceptScopeèƒ½å¤Ÿæ•è·å¹¿æ³›çš„è§†è§‰æ¦‚å¿µï¼ŒåŒ…æ‹¬å¯¹è±¡ã€çº¹ç†ã€èƒŒæ™¯ã€é¢éƒ¨å±æ€§ã€æƒ…ç»ªå’ŒåŠ¨ä½œã€‚æ­¤å¤–ï¼Œæ¦‚å¿µæ¿€æ´»äº§ç”Ÿä¸è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å›¾åƒåŒºåŸŸå¯¹é½çš„ç©ºé—´å½’å› ã€‚ConceptScopeå¯é åœ°æ£€æµ‹å·²çŸ¥çš„åå·®ï¼ˆä¾‹å¦‚ï¼ŒWaterbirdsä¸­çš„èƒŒæ™¯åå·®ï¼‰å¹¶æ­ç¤ºå…ˆå‰æœªæ ‡æ³¨çš„åå·®ï¼ˆä¾‹å¦‚ï¼ŒImageNetä¸­å…±åŒå‡ºç°çš„å¯¹è±¡ï¼‰ï¼Œä»è€Œä¸ºæ•°æ®é›†å®¡è®¡å’Œæ¨¡å‹è¯Šæ–­æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å·¥å…·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ æ•°æ®é›†ä¸­æ™®éå­˜åœ¨çš„æ•°æ®é›†åå·®é—®é¢˜ï¼Œå³æ•°æ®ç‚¹åœ¨æŸäº›æ¦‚å¿µä¸Šå­˜åœ¨å€¾æ–œã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µä¸”ç»†ç²’åº¦çš„å±æ€§æ ‡æ³¨æ‰èƒ½è¯†åˆ«è¿™äº›åå·®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§ã€‚å› æ­¤ï¼Œå¦‚ä½•è‡ªåŠ¨ä¸”é«˜æ•ˆåœ°è¯†åˆ«æ•°æ®é›†ä¸­çš„åå·®æˆä¸ºä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šConceptScopeçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹å­¦ä¹ åˆ°çš„é€šç”¨è¡¨å¾ï¼Œå¹¶é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ä»ä¸­æå–äººç±»å¯è§£é‡Šçš„è§†è§‰æ¦‚å¿µã€‚é€šè¿‡åˆ†æè¿™äº›æ¦‚å¿µä¸ç±»æ ‡ç­¾ä¹‹é—´çš„å…³ç³»ï¼Œå¯ä»¥å°†æ¦‚å¿µåˆ†ç±»ä¸ºç›®æ ‡æ¦‚å¿µã€ä¸Šä¸‹æ–‡æ¦‚å¿µå’Œåå·®æ¦‚å¿µã€‚è¿™ç§æ–¹æ³•æ— éœ€äººå·¥æ ‡æ³¨ï¼Œå³å¯è‡ªåŠ¨å‘ç°å’Œé‡åŒ–æ•°æ®é›†ä¸­çš„åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šConceptScopeæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æå–å›¾åƒçš„è§†è§‰è¡¨å¾ï¼›2) åœ¨è¿™äº›è¡¨å¾ä¸Šè®­ç»ƒç¨€ç–è‡ªç¼–ç å™¨ï¼Œä»¥å­¦ä¹ ä¸€ç»„ç¨€ç–çš„ã€äººç±»å¯è§£é‡Šçš„è§†è§‰æ¦‚å¿µï¼›3) æ ¹æ®æ¦‚å¿µä¸ç±»æ ‡ç­¾ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§å’Œç»Ÿè®¡ç›¸å…³æ€§ï¼Œå°†æ¦‚å¿µåˆ†ç±»ä¸ºç›®æ ‡æ¦‚å¿µã€ä¸Šä¸‹æ–‡æ¦‚å¿µå’Œåå·®æ¦‚å¿µï¼›4) åˆ©ç”¨è¿™äº›æ¦‚å¿µè¿›è¡Œæ•°æ®é›†ç‰¹å¾æè¿°ã€åå·®è¯†åˆ«å’ŒåŸºäºæ¦‚å¿µçš„å­åˆ†ç»„ï¼Œä»è€Œè¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šConceptScopeçš„å…³é”®åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿè‡ªåŠ¨å‘ç°å’Œé‡åŒ–äººç±»å¯è§£é‡Šçš„è§†è§‰æ¦‚å¿µï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚é€šè¿‡å°†ç¨€ç–è‡ªç¼–ç å™¨ä¸è§†è§‰åŸºç¡€æ¨¡å‹ç›¸ç»“åˆï¼ŒConceptScopeèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–å›¾åƒä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯è§£é‡Šçš„æ¦‚å¿µè¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒConceptScopeè¿˜æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ç›¸å…³æ€§å’Œç»Ÿè®¡ç›¸å…³æ€§çš„æ¦‚å¿µåˆ†ç±»æ–¹æ³•ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«æ•°æ®é›†ä¸­çš„åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ConceptScopeä¸­ï¼Œç¨€ç–è‡ªç¼–ç å™¨çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–é‡æ„è¯¯å·®ï¼ŒåŒæ—¶é¼“åŠ±ç¼–ç çš„ç¨€ç–æ€§ã€‚ç¨€ç–æ€§çº¦æŸå¯ä»¥é€šè¿‡L1æ­£åˆ™åŒ–æ¥å®ç°ã€‚æ¦‚å¿µä¸ç±»æ ‡ç­¾ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§å¯ä»¥é€šè¿‡è®¡ç®—å®ƒä»¬åœ¨é¢„è®­ç»ƒçš„è¯å‘é‡ç©ºé—´ä¸­çš„è·ç¦»æ¥è¡¡é‡ã€‚ç»Ÿè®¡ç›¸å…³æ€§å¯ä»¥é€šè¿‡è®¡ç®—æ¦‚å¿µæ¿€æ´»ä¸ç±»æ ‡ç­¾ä¹‹é—´çš„ç›¸å…³ç³»æ•°æ¥è¡¡é‡ã€‚è¿™äº›å‚æ•°çš„è®¾è®¡æ—¨åœ¨ç¡®ä¿æå–çš„æ¦‚å¿µå…·æœ‰å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ æ•°æ®é›†ä¸­çš„åå·®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ConceptScopeåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬Waterbirdså’ŒImageNetã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConceptScopeèƒ½å¤Ÿå¯é åœ°æ£€æµ‹å·²çŸ¥çš„åå·®ï¼ˆå¦‚Waterbirdsä¸­çš„èƒŒæ™¯åå·®ï¼‰ï¼Œå¹¶æ­ç¤ºå…ˆå‰æœªæ ‡æ³¨çš„åå·®ï¼ˆå¦‚ImageNetä¸­å…±åŒå‡ºç°çš„å¯¹è±¡ï¼‰ã€‚æ­¤å¤–ï¼Œæ¦‚å¿µæ¿€æ´»äº§ç”Ÿçš„ç©ºé—´å½’å› ä¸è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å›¾åƒåŒºåŸŸå¯¹é½ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†ConceptScopeçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ConceptScopeå¯åº”ç”¨äºæ•°æ®é›†å®¡è®¡ã€æ¨¡å‹è¯Šæ–­å’Œé²æ£’æ€§è¯„ä¼°ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆè¯†åˆ«æ•°æ®é›†ä¸­çš„åå·®ï¼Œä»è€Œæ”¹è¿›æ•°æ®æ”¶é›†å’Œæ¨¡å‹è®­ç»ƒç­–ç•¥ï¼Œæé«˜æ¨¡å‹çš„å…¬å¹³æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒConceptScopeè¿˜å¯ä»¥ç”¨äºæ„å»ºæ›´å¯é å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dataset bias, where data points are skewed to certain concepts, is ubiquitous in machine learning datasets. Yet, systematically identifying these biases is challenging without costly, fine-grained attribute annotations. We present ConceptScope, a scalable and automated framework for analyzing visual datasets by discovering and quantifying human-interpretable concepts using Sparse Autoencoders trained on representations from vision foundation models. ConceptScope categorizes concepts into target, context, and bias types based on their semantic relevance and statistical correlation to class labels, enabling class-level dataset characterization, bias identification, and robustness evaluation through concept-based subgrouping. We validate that ConceptScope captures a wide range of visual concepts, including objects, textures, backgrounds, facial attributes, emotions, and actions, through comparisons with annotated datasets. Furthermore, we show that concept activations produce spatial attributions that align with semantically meaningful image regions. ConceptScope reliably detects known biases (e.g., background bias in Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects in ImageNet), offering a practical tool for dataset auditing and model diagnostics.

