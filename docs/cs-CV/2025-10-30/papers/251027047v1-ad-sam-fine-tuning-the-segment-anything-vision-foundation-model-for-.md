---
layout: default
title: AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception
---

# AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.27047" target="_blank" class="toolbar-btn">arXiv: 2510.27047v1</a>
    <a href="https://arxiv.org/pdf/2510.27047.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27047v1" 
            onclick="toggleFavorite(this, '2510.27047v1', 'AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Mario Camarena, Het Patel, Fatemeh Nazari, Evangelos Papalexakis, Mohamadhossein Noruzoliaee, Jia Chen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-30

**Â§áÊ≥®**: Submitted to IEEE Transactions on Intelligent Transportation Systems (IEEE T-ITS)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**AD-SAMÔºöÂæÆË∞ÉSAMËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºåÁî®‰∫éËá™Âä®È©æÈ©∂ÊÑüÁü•**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ëá™Âä®È©æÈ©∂` `ËØ≠‰πâÂàÜÂâ≤` `ËßÜËßâÂü∫Á°ÄÊ®°Âûã` `ÂèØÂèòÂΩ¢Âç∑ÁßØ` `ÂèåÁºñÁ†ÅÂô®`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÂÖºÈ°æËá™Âä®È©æÈ©∂Âú∫ÊôØ‰∏≠ÂÖ®Â±ÄËØ≠‰πâ‰ø°ÊÅØÂíåÂ±ÄÈÉ®Á©∫Èó¥ÁªÜËäÇÔºåÂØºËá¥ÂàÜÂâ≤Á≤æÂ∫¶‰∏çË∂≥„ÄÇ
2. AD-SAMÈÄöËøáÂèåÁºñÁ†ÅÂô®ËûçÂêàÂÖ®Â±ÄËØ≠‰πâÂíåÂ±ÄÈÉ®Á©∫Èó¥‰ø°ÊÅØÔºåÂπ∂Âà©Áî®ÂèØÂèòÂΩ¢Ê®°ÂùóÂØπÈΩêÂºÇÊûÑÁâπÂæÅÔºåÊèêÂçáÂàÜÂâ≤ÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåAD-SAMÂú®CityscapesÂíåBDD100KÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éSAM„ÄÅG-SAMÂíåDeepLabV3Ôºå‰∏îÂÖ∑ÊúâÊõ¥Âø´ÁöÑÂ≠¶‰π†ÈÄüÂ∫¶„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜËá™Âä®È©æÈ©∂ÂàÜÂâ≤‰∏ÄÂàáÊ®°ÂûãÔºàAD-SAMÔºâÔºåËøôÊòØ‰∏Ä‰∏™‰∏∫Ëá™Âä®È©æÈ©∂ÔºàADÔºâ‰∏≠ÁöÑËØ≠‰πâÂàÜÂâ≤ËÄåÂæÆË∞ÉÁöÑËßÜËßâÂü∫Á°ÄÊ®°Âûã„ÄÇAD-SAMÈÄöËøá‰∏Ä‰∏™ÂèåÁºñÁ†ÅÂô®ÂíåÂèØÂèòÂΩ¢Ëß£Á†ÅÂô®Êâ©Â±ï‰∫ÜÂàÜÂâ≤‰∏ÄÂàáÊ®°ÂûãÔºàSAMÔºâÔºåËØ•Ëß£Á†ÅÂô®‰∏ì‰∏∫ÈÅìË∑ØÂú∫ÊôØÁöÑÁ©∫Èó¥ÂíåÂá†‰ΩïÂ§çÊùÇÊÄßËÄåËÆæËÆ°„ÄÇÂèåÁºñÁ†ÅÂô®ÈÄöËøáÁªìÂêàÊù•Ëá™SAMÁöÑÈ¢ÑËÆ≠ÁªÉVision TransformerÔºàViT-HÔºâÁöÑÂÖ®Â±ÄËØ≠‰πâ‰∏ä‰∏ãÊñáÂíåÊù•Ëá™ÂèØËÆ≠ÁªÉÂç∑ÁßØÊ∑±Â∫¶Â≠¶‰π†È™®Âπ≤ÁΩëÁªúÔºàÂç≥ResNet-50ÔºâÁöÑÂ±ÄÈÉ®Á©∫Èó¥ÁªÜËäÇÊù•ÁîüÊàêÂ§öÂ∞∫Â∫¶ËûçÂêàË°®Á§∫„ÄÇ‰∏Ä‰∏™ÂèØÂèòÂΩ¢ËûçÂêàÊ®°ÂùóÂØπÈΩêË∑®Â∞∫Â∫¶ÂíåÂØπË±°Âá†‰ΩïÂΩ¢Áä∂ÁöÑÂºÇÊûÑÁâπÂæÅ„ÄÇËß£Á†ÅÂô®‰ΩøÁî®ÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõÊâßË°åÊ∏êËøõÂºèÂ§öÈò∂ÊÆµÁªÜÂåñ„ÄÇËÆ≠ÁªÉÁî±Ê∑∑ÂêàÊçüÂ§±ÊåáÂØºÔºåËØ•Ê∑∑ÂêàÊçüÂ§±ÈõÜÊàê‰∫ÜFocal„ÄÅDice„ÄÅLovasz-SoftmaxÂíåSurfaceÊçüÂ§±Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜËØ≠‰πâÁ±ªÂπ≥Ë°°„ÄÅËæπÁïåÁ≤æÂ∫¶Âíå‰ºòÂåñÁ®≥ÂÆöÊÄß„ÄÇÂú®CityscapesÂíåBerkeley DeepDrive 100KÔºàBDD100KÔºâÂü∫ÂáÜÊµãËØï‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåAD-SAMÂú®ÂàÜÂâ≤Á≤æÂ∫¶ÊñπÈù¢Ë∂ÖË∂ä‰∫ÜSAM„ÄÅÂπø‰πâSAMÔºàG-SAMÔºâÂíå‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†Âü∫Á∫øÔºàDeepLabV3Ôºâ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËá™Âä®È©æÈ©∂Âú∫ÊôØ‰∏ãÁöÑËØ≠‰πâÂàÜÂâ≤‰ªªÂä°ÔºåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇSAMÁ≠âÔºåÂú®Â§ÑÁêÜÈÅìË∑ØÂú∫ÊôØÂ§çÊùÇÁöÑÁ©∫Èó¥Âá†‰ΩïÁªìÊûÑÂíåÂ§öÂ∞∫Â∫¶ÁõÆÊ†áÊó∂ÔºåÁ≤æÂ∫¶‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÖºÈ°æÂÖ®Â±ÄËØ≠‰πâ‰ø°ÊÅØÂíåÂ±ÄÈÉ®Á©∫Èó¥ÁªÜËäÇÊñπÈù¢Â≠òÂú®ÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Âπ≥Ë°°ÂàÜÂâ≤Á≤æÂ∫¶„ÄÅÊ≥õÂåñËÉΩÂäõÂíåËÆ°ÁÆóÊïàÁéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAD-SAMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂèåÁºñÁ†ÅÂô®ÁªìÊûÑÔºåÂ∞ÜSAMÈ¢ÑËÆ≠ÁªÉÁöÑViT-HÊèê‰æõÁöÑÂÖ®Â±ÄËØ≠‰πâ‰ø°ÊÅØ‰∏éResNet-50ÊèêÂèñÁöÑÂ±ÄÈÉ®Á©∫Èó¥ÁªÜËäÇËøõË°åÊúâÊïàËûçÂêà„ÄÇÈÄöËøáÂèØÂèòÂΩ¢ËûçÂêàÊ®°ÂùóÂíåÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËá™ÈÄÇÂ∫îÂú∞ÂØπÈΩêÂíåÁªÜÂåñ‰∏çÂêåÂ∞∫Â∫¶ÂíåÂá†‰ΩïÂΩ¢Áä∂ÁöÑÁâπÂæÅÔºå‰ªéËÄåÊèêÂçáÂàÜÂâ≤Á≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAD-SAMÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Ôºö1) ÂèåÁºñÁ†ÅÂô®ÔºöViT-HÊèêÂèñÂÖ®Â±ÄËØ≠‰πâÁâπÂæÅÔºåResNet-50ÊèêÂèñÂ±ÄÈÉ®Á©∫Èó¥ÁâπÂæÅÔºõ2) ÂèØÂèòÂΩ¢ËûçÂêàÊ®°ÂùóÔºöÂØπÈΩêÂíåËûçÂêàÊù•Ëá™‰∏§‰∏™ÁºñÁ†ÅÂô®ÁöÑÂºÇÊûÑÁâπÂæÅÔºõ3) ÂèØÂèòÂΩ¢Ëß£Á†ÅÂô®ÔºöÈÄöËøáÂ§öÈò∂ÊÆµÁªÜÂåñÔºåÈÄêÊ≠•ÊèêÂçáÂàÜÂâ≤Á≤æÂ∫¶„ÄÇËÆ≠ÁªÉËøáÁ®ã‰ΩøÁî®Ê∑∑ÂêàÊçüÂ§±ÂáΩÊï∞ÔºåÂåÖÊã¨Focal Loss„ÄÅDice Loss„ÄÅLovasz-Softmax LossÂíåSurface Loss„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAD-SAMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÂèåÁºñÁ†ÅÂô®ÁªìÊûÑÔºåÊúâÊïàËûçÂêàÂÖ®Â±ÄËØ≠‰πâÂíåÂ±ÄÈÉ®Á©∫Èó¥‰ø°ÊÅØÔºõ2) ÂèØÂèòÂΩ¢ËûçÂêàÊ®°ÂùóÂíåÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËá™ÈÄÇÂ∫îÂú∞Â§ÑÁêÜÈÅìË∑ØÂú∫ÊôØ‰∏≠Â§çÊùÇÁöÑÁõÆÊ†áÂá†‰ΩïÂΩ¢Áä∂ÂíåÂ∞∫Â∫¶ÂèòÂåñÔºõ3) Ê∑∑ÂêàÊçüÂ§±ÂáΩÊï∞ÔºåÂπ≥Ë°°ËØ≠‰πâÁ±ª„ÄÅËæπÁïåÁ≤æÂ∫¶Âíå‰ºòÂåñÁ®≥ÂÆöÊÄß„ÄÇ‰∏éSAMÁõ∏ÊØîÔºåAD-SAMÊõ¥‰∏ìÊ≥®‰∫éËá™Âä®È©æÈ©∂Âú∫ÊôØÔºåÈÄöËøáÈíàÂØπÊÄßÁöÑÊû∂ÊûÑÂíå‰ºòÂåñÔºåÊòæËëóÊèêÂçá‰∫ÜÂàÜÂâ≤ÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂèåÁºñÁ†ÅÂô®‰∏≠ÔºåViT-H‰ΩøÁî®SAMÁöÑÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºåResNet-50ËøõË°åÁ´ØÂà∞Á´ØËÆ≠ÁªÉ„ÄÇÂèØÂèòÂΩ¢ËûçÂêàÊ®°ÂùóÈááÁî®ÂèØÂèòÂΩ¢Âç∑ÁßØÔºåËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ÊÑüÂèóÈáé„ÄÇËß£Á†ÅÂô®‰ΩøÁî®Â§öÂ±ÇÂèØÂèòÂΩ¢Ê≥®ÊÑèÂäõÔºåÈÄêÊ≠•ÁªÜÂåñÂàÜÂâ≤ÁªìÊûú„ÄÇÊ∑∑ÂêàÊçüÂ§±ÂáΩÊï∞‰∏≠ÔºåÂêÑÁßçÊçüÂ§±ÁöÑÊùÉÈáçÈúÄË¶ÅÊ†πÊçÆÊï∞ÊçÆÈõÜÂíå‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇËÆ≠ÁªÉÈááÁî®AdamW‰ºòÂåñÂô®ÔºåÂ≠¶‰π†ÁéáÊ†πÊçÆÁªèÈ™åËÆæÁΩÆ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

AD-SAMÂú®CityscapesÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü68.1%ÁöÑmIoUÔºåÂú®BDD100KÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü59.5%ÁöÑmIoUÔºåÂàÜÂà´ÊØîSAM„ÄÅG-SAMÂíåDeepLabV3È´òÂá∫È´òËææ+22.9%Âíå+19.2%„ÄÇAD-SAMÂÖ∑ÊúâÊõ¥Âº∫ÁöÑË∑®ÂüüÊ≥õÂåñËÉΩÂäõÔºå‰øùÁïôÂàÜÊï∞‰∏∫0.87ÔºàSAM‰∏∫0.76ÔºâÔºåÂπ∂‰∏îÂ≠¶‰π†ÈÄüÂ∫¶Êõ¥Âø´Ôºå‰ªÖÈúÄ30-40‰∏™epochÂç≥ÂèØÊî∂Êïõ„ÄÇÂú®‰ªÖ‰ΩøÁî®1000‰∏™Ê†∑Êú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªçËÉΩ‰øùÊåÅ0.607ÁöÑmIoUÔºåË°®ÊòéÂÖ∂ÂÖ∑ÊúâÂæàÈ´òÁöÑÊï∞ÊçÆÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AD-SAMÂèØÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂Ê±ΩËΩ¶ÁöÑÁéØÂ¢ÉÊÑüÁü•Á≥ªÁªüÔºå‰∏∫ËΩ¶ËæÜÊèê‰æõÁ≤æÁ°ÆÁöÑËØ≠‰πâÂàÜÂâ≤ÁªìÊûúÔºå‰ªéËÄåÊèêÈ´òËΩ¶ËæÜÂØπÈÅìË∑Ø„ÄÅ‰∫§ÈÄöÂèÇ‰∏éËÄÖÂíåÈöúÁ¢çÁâ©ÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇËøôÊúâÂä©‰∫éÊèêÂçáËá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÂÆâÂÖ®ÊÄß„ÄÅÂèØÈù†ÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÔºåÂπ∂ÂèØÊâ©Â±ïÂà∞ÂÖ∂‰ªñÈúÄË¶ÅÈ´òÁ≤æÂ∫¶ËØ≠‰πâÂàÜÂâ≤ÁöÑÈ¢ÜÂüüÔºåÂ¶ÇÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÊô∫ËÉΩ‰∫§ÈÄöÁÆ°ÁêÜÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper presents the Autonomous Driving Segment Anything Model (AD-SAM), a fine-tuned vision foundation model for semantic segmentation in autonomous driving (AD). AD-SAM extends the Segment Anything Model (SAM) with a dual-encoder and deformable decoder tailored to spatial and geometric complexity of road scenes. The dual-encoder produces multi-scale fused representations by combining global semantic context from SAM's pretrained Vision Transformer (ViT-H) with local spatial detail from a trainable convolutional deep learning backbone (i.e., ResNet-50). A deformable fusion module aligns heterogeneous features across scales and object geometries. The decoder performs progressive multi-stage refinement using deformable attention. Training is guided by a hybrid loss that integrates Focal, Dice, Lovasz-Softmax, and Surface losses, improving semantic class balance, boundary precision, and optimization stability. Experiments on the Cityscapes and Berkeley DeepDrive 100K (BDD100K) benchmarks show that AD-SAM surpasses SAM, Generalized SAM (G-SAM), and a deep learning baseline (DeepLabV3) in segmentation accuracy. It achieves 68.1 mean Intersection over Union (mIoU) on Cityscapes and 59.5 mIoU on BDD100K, outperforming SAM, G-SAM, and DeepLabV3 by margins of up to +22.9 and +19.2 mIoU in structured and diverse road scenes, respectively. AD-SAM demonstrates strong cross-domain generalization with a 0.87 retention score (vs. 0.76 for SAM), and faster, more stable learning dynamics, converging within 30-40 epochs, enjoying double the learning speed of benchmark models. It maintains 0.607 mIoU with only 1000 samples, suggesting data efficiency critical for reducing annotation costs. These results confirm that targeted architectural and optimization enhancements to foundation models enable reliable and scalable AD perception.

