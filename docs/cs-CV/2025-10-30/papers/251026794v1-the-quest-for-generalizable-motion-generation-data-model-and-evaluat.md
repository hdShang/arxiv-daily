---
layout: default
title: The Quest for Generalizable Motion Generation: Data, Model, and Evaluation
---

# The Quest for Generalizable Motion Generation: Data, Model, and Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26794" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26794v1</a>
  <a href="https://arxiv.org/pdf/2510.26794.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26794v1" onclick="toggleFavorite(this, '2510.26794v1', 'The Quest for Generalizable Motion Generation: Data, Model, and Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViMoGenæ¡†æ¶ï¼Œé€šè¿‡è¿ç§»è§†é¢‘ç”ŸæˆçŸ¥è¯†ï¼Œæå‡3Däººä½“åŠ¨ä½œç”Ÿæˆæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Däººä½“åŠ¨ä½œç”Ÿæˆ` `è§†é¢‘ç”Ÿæˆ` `çŸ¥è¯†è¿ç§»` `æ‰©æ•£æ¨¡å‹` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Däººä½“åŠ¨ä½œç”Ÿæˆæ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ç“¶é¢ˆï¼Œéš¾ä»¥é€‚åº”å¤æ‚å¤šæ ·çš„åœºæ™¯ã€‚
2. ViMoGenæ¡†æ¶é€šè¿‡æ•°æ®å¢å¼ºã€æ¨¡å‹è®¾è®¡å’Œè¯„ä¼°ä½“ç³»ä¸‰ä¸ªæ–¹é¢ï¼Œå°†è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„çŸ¥è¯†è¿ç§»åˆ°åŠ¨ä½œç”Ÿæˆé¢†åŸŸã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒViMoGenåœ¨åŠ¨ä½œè´¨é‡ã€æç¤ºä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹ç°æœ‰3Däººä½“åŠ¨ä½œç”Ÿæˆ(MoGen)æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»¼åˆæ¡†æ¶ï¼Œç³»ç»Ÿåœ°å°†è§†é¢‘ç”Ÿæˆ(ViGen)é¢†åŸŸçš„çŸ¥è¯†è¿ç§»åˆ°MoGené¢†åŸŸï¼Œæ¶µç›–æ•°æ®ã€å»ºæ¨¡å’Œè¯„ä¼°ä¸‰ä¸ªå…³é”®æ–¹é¢ã€‚é¦–å…ˆï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ViMoGen-228Kï¼ŒåŒ…å«22.8ä¸‡ä¸ªé«˜è´¨é‡åŠ¨ä½œæ ·æœ¬ï¼Œé›†æˆäº†é«˜ä¿çœŸå…‰å­¦MoCapæ•°æ®ã€æ¥è‡ªç½‘ç»œè§†é¢‘çš„è¯­ä¹‰æ ‡æ³¨åŠ¨ä½œä»¥åŠViGenæ¨¡å‹åˆæˆçš„æ ·æœ¬ï¼Œæ˜¾è‘—æ‰©å±•äº†è¯­ä¹‰å¤šæ ·æ€§ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†ViMoGenï¼Œä¸€ä¸ªåŸºäºæµåŒ¹é…çš„æ‰©æ•£Transformerï¼Œé€šè¿‡é—¨æ§å¤šæ¨¡æ€æ¡ä»¶ä½œç”¨ç»Ÿä¸€äº†MoCapæ•°æ®å’ŒViGenæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œè¿›ä¸€æ­¥å¼€å‘äº†ViMoGen-lightï¼Œä¸€ä¸ªç²¾ç®€ç‰ˆæœ¬ï¼Œæ¶ˆé™¤äº†å¯¹è§†é¢‘ç”Ÿæˆçš„ä¾èµ–ï¼ŒåŒæ—¶ä¿ç•™äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæå‡ºäº†MBenchï¼Œä¸€ä¸ªåˆ†å±‚åŸºå‡†ï¼Œç”¨äºå¯¹è¿åŠ¨è´¨é‡ã€æç¤ºä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç ã€æ•°æ®å’ŒåŸºå‡†å°†å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰3Däººä½“åŠ¨ä½œç”Ÿæˆæ¨¡å‹åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥ç”Ÿæˆè‡ªç„¶ã€ç¬¦åˆè¯­ä¹‰çš„åŠ¨ä½œã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæœ‰é™çš„MoCapæ•°æ®ï¼Œç¼ºä¹å¯¹å¤æ‚åœºæ™¯å’Œè¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆå»ºæ¨¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šå€Ÿé‰´è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„æˆåŠŸç»éªŒï¼Œå°†è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†è¿ç§»åˆ°åŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸­ã€‚é€šè¿‡èåˆMoCapæ•°æ®å’Œè§†é¢‘æ•°æ®ï¼Œå¢å¼ºæ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šViMoGenæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šå¤§è§„æ¨¡æ•°æ®é›†ViMoGen-228Kã€åŸºäºæµåŒ¹é…çš„æ‰©æ•£Transformeræ¨¡å‹ViMoGenå’Œåˆ†å±‚è¯„ä¼°åŸºå‡†MBenchã€‚ViMoGen-228Kæ•°æ®é›†èåˆäº†MoCapæ•°æ®ã€è§†é¢‘æ•°æ®å’Œåˆæˆæ•°æ®ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›ä¸°å¯Œçš„æ ·æœ¬ã€‚ViMoGenæ¨¡å‹é€šè¿‡é—¨æ§å¤šæ¨¡æ€æ¡ä»¶ä½œç”¨ï¼Œå°†æ–‡æœ¬ã€è§†é¢‘å’ŒMoCapæ•°æ®èåˆåœ¨ä¸€èµ·ï¼Œç”Ÿæˆé«˜è´¨é‡çš„åŠ¨ä½œåºåˆ—ã€‚MBenchåŸºå‡†ç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„çŸ¥è¯†è¿ç§»åˆ°åŠ¨ä½œç”Ÿæˆé¢†åŸŸï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„æ¡†æ¶å’Œæ¨¡å‹ã€‚é€šè¿‡èåˆå¤šæ¨¡æ€æ•°æ®ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªåˆ†å±‚è¯„ä¼°åŸºå‡†ï¼Œç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šViMoGenæ¨¡å‹é‡‡ç”¨åŸºäºæµåŒ¹é…çš„æ‰©æ•£Transformeræ¶æ„ï¼Œé€šè¿‡é—¨æ§æœºåˆ¶èåˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æµåŒ¹é…æŸå¤±ã€è¿åŠ¨å­¦æŸå¤±å’Œå¯¹æŠ—æŸå¤±ç­‰ã€‚ViMoGen-lightæ¨¡å‹é€šè¿‡çŸ¥è¯†è’¸é¦ï¼Œå°†ViMoGenæ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°æ›´è½»é‡çº§çš„æ¨¡å‹ä¸­ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ViMoGenåœ¨MBenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨åŠ¨ä½œè´¨é‡æ–¹é¢ï¼ŒViMoGenä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨æç¤ºä¿çœŸåº¦æ–¹é¢ï¼ŒViMoGenèƒ½å¤Ÿæ›´å¥½åœ°ç”Ÿæˆç¬¦åˆæ–‡æœ¬æè¿°çš„åŠ¨ä½œã€‚åœ¨æ³›åŒ–èƒ½åŠ›æ–¹é¢ï¼ŒViMoGenåœ¨æœªè§è¿‡çš„åœºæ™¯å’ŒåŠ¨ä½œä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚äººå·¥è¯„ä¼°ä¹Ÿè¡¨æ˜ï¼ŒViMoGenç”Ÿæˆçš„åŠ¨ä½œæ›´åŠ è‡ªç„¶å’Œé€¼çœŸã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œã€æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°æˆ–è§†é¢‘è¾“å…¥ï¼Œç”Ÿæˆé€¼çœŸçš„äººä½“åŠ¨ä½œï¼Œä»è€Œå¢å¼ºç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºè®­ç»ƒæœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çš„æ„å›¾å¹¶æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.

