---
layout: default
title: Semantic Frame Aggregation-based Transformer for Live Video Comment Generation
---

# Semantic Frame Aggregation-based Transformer for Live Video Comment Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26978" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26978v1</a>
  <a href="https://arxiv.org/pdf/2510.26978.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26978v1" onclick="toggleFavorite(this, '2510.26978v1', 'Semantic Frame Aggregation-based Transformer for Live Video Comment Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anam Fatima, Yi Yu, Janak Kapuriya, Julien Lalanne, Jainendra Shukla

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè¯­ä¹‰å¸§èšåˆTransformerçš„SFATæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆç›´æ’­è§†é¢‘è¯„è®ºã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›´æ’­è§†é¢‘è¯„è®ºç”Ÿæˆ` `è¯­ä¹‰å¸§èšåˆ` `Transformer` `å¤šæ¨¡æ€å­¦ä¹ ` `CLIP` `è·¨æ³¨æ„åŠ›æœºåˆ¶` `è§†é¢‘ç†è§£` `è‡ªç„¶è¯­è¨€ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆç›´æ’­è§†é¢‘è¯„è®ºæ—¶ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘è§†é¢‘å¸§ä¸è§‚ä¼—äº’åŠ¨ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¯„è®ºä¸ä¸Šä¸‹æ–‡å…³è”æ€§è¾ƒå¼±ã€‚
2. è®ºæ–‡æå‡ºSFATæ¨¡å‹ï¼Œåˆ©ç”¨CLIPçš„å¤šæ¨¡æ€çŸ¥è¯†ï¼Œé€šè¿‡è¯­ä¹‰å¸§èšåˆæœºåˆ¶ï¼Œå¯¹è§†é¢‘å¸§è¿›è¡ŒåŠ æƒï¼Œä»è€Œçªå‡ºå…³é”®å¸§ï¼Œæå‡è¯„è®ºçš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚
3. è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„è‹±è¯­ç›´æ’­è§†é¢‘è¯„è®ºæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†SFATæ¨¡å‹åœ¨ç”Ÿæˆè¯„è®ºæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç”Ÿæˆç›´æ’­è§†é¢‘è¯„è®ºçš„åŸºäºè¯­ä¹‰å¸§èšåˆTransformerï¼ˆSFATï¼‰æ¨¡å‹ã€‚ç›´æ’­è¯„è®ºåœ¨Twitchç­‰å¹³å°ä¸Šçš„è§†é¢‘æµä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œé€šè¿‡åŠ¨æ€äº¤äº’å¢å¼ºäº†è§‚ä¼—çš„å‚ä¸åº¦ã€‚ç„¶è€Œï¼Œè‡ªåŠ¨ç”Ÿæˆç¬¦åˆè¯­å¢ƒçš„è¯„è®ºä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è§†é¢‘æµåŒ…å«å¤§é‡æ•°æ®å’Œæ— å…³å†…å®¹ã€‚ç°æœ‰æ–¹æ³•å€¾å‘äºå¿½ç•¥ä¸€ä¸ªé‡è¦æ–¹é¢ï¼Œå³ä¼˜å…ˆè€ƒè™‘ä¸æ­£åœ¨è¿›è¡Œçš„è§‚ä¼—äº’åŠ¨æœ€ç›¸å…³çš„è§†é¢‘å¸§ã€‚è¿™ç§ä¼˜å…ˆçº§å¯¹äºç”Ÿæˆç¬¦åˆè¯­å¢ƒçš„è¯„è®ºè‡³å…³é‡è¦ã€‚SFATæ¨¡å‹åˆ©ç”¨CLIPçš„è§†è§‰-æ–‡æœ¬å¤šæ¨¡æ€çŸ¥è¯†æ¥ç”Ÿæˆè¯„è®ºï¼Œå¹¶æ ¹æ®è§†é¢‘å¸§ä¸æ­£åœ¨è¿›è¡Œçš„è§‚ä¼—å¯¹è¯çš„è¯­ä¹‰ç›¸å…³æ€§ä¸ºå…¶åˆ†é…æƒé‡ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æœ‰æ•ˆçš„å¸§åŠ æƒæ±‚å’ŒæŠ€æœ¯æ¥å¼ºè°ƒä¿¡æ¯ä¸°å¯Œçš„å¸§ï¼ŒåŒæ—¶å‡å°‘å¯¹ä¸ç›¸å…³å¸§çš„å…³æ³¨ã€‚æœ€åï¼Œå¸¦æœ‰è·¨æ³¨æ„åŠ›æœºåˆ¶çš„è¯„è®ºè§£ç å™¨ç¡®ä¿ç”Ÿæˆçš„è¯„è®ºåæ˜ äº†æ¥è‡ªèŠå¤©å’Œè§†é¢‘çš„ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ç°æœ‰æ•°æ®é›†çš„å±€é™æ€§ï¼ˆä¸»è¦é›†ä¸­äºä¸­æ–‡å†…å®¹å’Œæœ‰é™çš„è§†é¢‘ç±»åˆ«ï¼‰ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€è‹±è¯­è§†é¢‘è¯„è®ºæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä»Twitchæå–ï¼Œæ¶µç›–11ä¸ªè§†é¢‘ç±»åˆ«ï¼Œæ€»è®¡438å°æ—¶å’Œ320ä¸‡æ¡è¯„è®ºã€‚é€šè¿‡å°†æˆ‘ä»¬çš„SFATæ¨¡å‹ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¯æ˜äº†å…¶åœ¨ä»ç›´æ’­è§†é¢‘å’Œæ­£åœ¨è¿›è¡Œçš„å¯¹è¯ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆè¯„è®ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç›´æ’­è§†é¢‘è¯„è®ºè‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•æœ‰æ•ˆåŒºåˆ†è§†é¢‘å¸§çš„é‡è¦æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¯„è®ºä¸ä¸Šä¸‹æ–‡å…³è”æ€§è¾ƒå·®ï¼Œæ— æ³•å‡†ç¡®åæ˜ è§‚ä¼—çš„å…³æ³¨ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†é¢‘å¸§ä¸è§‚ä¼—å¯¹è¯ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§ï¼Œå¯¹è§†é¢‘å¸§è¿›è¡ŒåŠ æƒï¼Œä»è€Œçªå‡ºä¸è§‚ä¼—äº’åŠ¨æœ€ç›¸å…³çš„å¸§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´åŠ å…³æ³¨å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆæ›´ç¬¦åˆè¯­å¢ƒçš„è¯„è®ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSFATæ¨¡å‹ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†é¢‘å¸§ç¼–ç å™¨ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹æå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ã€‚2) è¯­ä¹‰å¸§èšåˆæ¨¡å—ï¼šè®¡ç®—æ¯ä¸ªè§†é¢‘å¸§ä¸è§‚ä¼—å¯¹è¯çš„è¯­ä¹‰ç›¸å…³æ€§ï¼Œå¹¶æ®æ­¤å¯¹è§†é¢‘å¸§è¿›è¡ŒåŠ æƒã€‚3) è¯„è®ºè§£ç å™¨ï¼šåˆ©ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œèåˆè§†é¢‘å’Œå¯¹è¯ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆçš„è¯„è®ºã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è¯­ä¹‰å¸§èšåˆæœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæ ¹æ®è§†é¢‘å¸§ä¸è§‚ä¼—å¯¹è¯çš„è¯­ä¹‰ç›¸å…³æ€§ï¼ŒåŠ¨æ€åœ°è°ƒæ•´è§†é¢‘å¸§çš„æƒé‡ï¼Œä»è€Œä½¿æ¨¡å‹æ›´åŠ å…³æ³¨å…³é”®ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡ç”Ÿæˆè¯„è®ºçš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯­ä¹‰å¸§èšåˆæ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•è®¡ç®—è§†é¢‘å¸§ä¸è§‚ä¼—å¯¹è¯çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„è®¡ç®—æ–¹æ³•ï¼Œå°†è§†é¢‘å¸§å’Œè§‚ä¼—å¯¹è¯åˆ†åˆ«ç¼–ç ä¸ºå‘é‡ï¼Œç„¶åè®¡ç®—å®ƒä»¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä½œä¸ºè§†é¢‘å¸§çš„æƒé‡ã€‚è¯„è®ºè§£ç å™¨é‡‡ç”¨Transformerç»“æ„ï¼Œå¹¶å¼•å…¥äº†è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°èåˆè§†é¢‘å’Œå¯¹è¯ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«438å°æ—¶è§†é¢‘å’Œ320ä¸‡æ¡è¯„è®ºçš„å¤§è§„æ¨¡è‹±è¯­ç›´æ’­è§†é¢‘è¯„è®ºæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSFATæ¨¡å‹åœ¨ç”Ÿæˆè¯„è®ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆè¯­å¢ƒã€æ›´å…·ç›¸å…³æ€§çš„è¯„è®ºã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œä½†æ­¤å¤„æœªæä¾›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§ç›´æ’­å¹³å°ï¼Œä¾‹å¦‚æ¸¸æˆç›´æ’­ã€ä½“è‚²èµ›äº‹ç›´æ’­ã€æ–°é—»ç›´æ’­ç­‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆä¸è§†é¢‘å†…å®¹å’Œè§‚ä¼—äº’åŠ¨ç›¸å…³çš„è¯„è®ºï¼Œæå‡ç”¨æˆ·å‚ä¸åº¦å’Œè§‚çœ‹ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºè§†é¢‘æ‘˜è¦ç”Ÿæˆã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Live commenting on video streams has surged in popularity on platforms like Twitch, enhancing viewer engagement through dynamic interactions. However, automatically generating contextually appropriate comments remains a challenging and exciting task. Video streams can contain a vast amount of data and extraneous content. Existing approaches tend to overlook an important aspect of prioritizing video frames that are most relevant to ongoing viewer interactions. This prioritization is crucial for producing contextually appropriate comments. To address this gap, we introduce a novel Semantic Frame Aggregation-based Transformer (SFAT) model for live video comment generation. This method not only leverages CLIP's visual-text multimodal knowledge to generate comments but also assigns weights to video frames based on their semantic relevance to ongoing viewer conversation. It employs an efficient weighted sum of frames technique to emphasize informative frames while focusing less on irrelevant ones. Finally, our comment decoder with a cross-attention mechanism that attends to each modality ensures that the generated comment reflects contextual cues from both chats and video. Furthermore, to address the limitations of existing datasets, which predominantly focus on Chinese-language content with limited video categories, we have constructed a large scale, diverse, multimodal English video comments dataset. Extracted from Twitch, this dataset covers 11 video categories, totaling 438 hours and 3.2 million comments. We demonstrate the effectiveness of our SFAT model by comparing it to existing methods for generating comments from live video and ongoing dialogue contexts.

