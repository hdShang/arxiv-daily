---
layout: default
title: Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models
---

# Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26241" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26241v2</a>
  <a href="https://arxiv.org/pdf/2510.26241.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26241v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.26241v2', 'Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30 (æ›´æ–°: 2025-11-05)

**å¤‡æ³¨**: 10 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAoT-PsyPhyBENCHåŸºå‡†ï¼Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹è§†é¢‘æ—¶é—´æ–¹å‘çš„ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ—¶é—´æ¨ç†` `æ—¶é—´æ–¹å‘` `å¿ƒç†ç‰©ç†å­¦` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç†è§£è§†é¢‘æ—¶é—´ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹æœ‰æ•ˆè¯„ä¼°æ–¹æ³•ã€‚
2. æå‡ºAoT-PsyPhyBENCHåŸºå‡†ï¼Œé€šè¿‡åˆ¤æ–­è§†é¢‘æ­£åæ’­æ”¾æ¥è¯„ä¼°æ¨¡å‹çš„æ—¶é—´æ–¹å‘ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰VLMsåœ¨æ—¶é—´æ–¹å‘åˆ¤æ–­ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿œä½äºäººç±»æ°´å¹³ï¼Œå‡¸æ˜¾äº†æ¨¡å‹åœ¨æ—¶é—´æ¨ç†æ–¹é¢çš„å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¯¹è§†é¢‘ä¸­æ—¶é—´ä¿¡æ¯çš„æŒæ¡ä»ç„¶è–„å¼±ï¼Œå¹¶ä¸”ç¼ºä¹å……åˆ†çš„è¯„ä¼°ã€‚æœ¬æ–‡é€šè¿‡ä¸€ä¸ªçœ‹ä¼¼ç®€å•ä½†å…·æœ‰å¯å‘æ€§çš„æŒ‘æˆ˜æ¥æ¢ç©¶è¿™ä¸€å·®è·ï¼šåˆ¤æ–­æ—¶é—´ç®­å¤´(AoT)ï¼Œå³åˆ¤æ–­ä¸€ä¸ªçŸ­è§†é¢‘ç‰‡æ®µæ˜¯æ­£å‘æ’­æ”¾è¿˜æ˜¯åå‘æ’­æ”¾ã€‚æˆ‘ä»¬å¼•å…¥äº†AoT-PsyPhyBENCHï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡å¿ƒç†ç‰©ç†å­¦éªŒè¯çš„åŸºå‡†ï¼Œç”¨äºæµ‹è¯•VLMsæ˜¯å¦èƒ½å¤Ÿä½¿ç”¨ä¸äººç±»ç›¸åŒçš„åˆºæ¿€å’Œè¡Œä¸ºåŸºçº¿æ¥æ¨æ–­è‡ªç„¶è§†é¢‘ä¸­çš„æ—¶é—´æ–¹å‘ã€‚å¯¹å¼€æ”¾æƒé‡å’Œä¸“æœ‰ã€æ¨ç†å’Œéæ¨ç†VLMsçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹çš„æ€§èƒ½æ¥è¿‘éšæœºæ°´å¹³ï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹ä¹Ÿè¿œè¿œè½åäºäººç±»åœ¨ç‰©ç†ä¸Šä¸å¯é€†è½¬çš„è¿‡ç¨‹ï¼ˆä¾‹å¦‚ï¼Œè‡ªç”±è½ä½“ã€æ‰©æ•£/çˆ†ç‚¸ï¼‰å’Œå› æœæ‰‹åŠ¨æ“ä½œï¼ˆé™¤æ³•/åŠ æ³•ï¼‰ä¸Šçš„å‡†ç¡®æ€§ï¼Œè€Œäººç±»å‡ ä¹å¯ä»¥ç«‹å³è¯†åˆ«è¿™äº›è¿‡ç¨‹ã€‚è¿™äº›ç»“æœçªå‡ºäº†å½“å‰å¤šæ¨¡æ€ç³»ç»Ÿä¸­çš„ä¸€ä¸ªæ ¹æœ¬å·®è·ï¼šè™½ç„¶å®ƒä»¬æ•è·äº†ä¸°å¯Œçš„è§†è§‰-è¯­ä¹‰ç›¸å…³æ€§ï¼Œä½†å®ƒä»¬ç¼ºä¹æ—¶é—´è¿ç»­æ€§å’Œå› æœç†è§£æ‰€éœ€çš„å½’çº³åç½®ã€‚æˆ‘ä»¬å‘å¸ƒäº†AoT-PsyPhyBENCHçš„ä»£ç å’Œæ•°æ®ï¼Œä»¥é¼“åŠ±VLMsåœ¨ç‰©ç†å’Œæ—¶é—´æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—è¿›ä¸€æ­¥è¿›å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç†è§£è§†é¢‘æ—¶é—´ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¯¹æ—¶é—´æ–¹å‘ï¼ˆæ—¶é—´ç®­å¤´ï¼‰çš„ç†è§£ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¯„ä¼°VLMsçš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œç¼ºä¹ä¸€ä¸ªæ ‡å‡†åŒ–çš„ã€ä¸äººç±»è®¤çŸ¥å¯¹é½çš„è¯„ä¼°åŸºå‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´å¿ƒç†ç‰©ç†å­¦å®éªŒæ–¹æ³•ï¼Œè®¾è®¡ä¸€ä¸ªåˆ¤æ–­è§†é¢‘æ­£åæ’­æ”¾çš„ç®€å•ä»»åŠ¡ï¼Œä»¥æ­¤æ¥è¯„ä¼°VLMså¯¹æ—¶é—´æ–¹å‘çš„ç†è§£èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»å¯¹æ—¶é—´æµé€çš„ç›´è§‚æ„ŸçŸ¥ï¼Œèƒ½å¤Ÿæ›´ç›´æ¥åœ°åæ˜ æ¨¡å‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAoT-PsyPhyBENCHåŸºå‡†åŒ…å«ä¸€ç³»åˆ—çŸ­è§†é¢‘ç‰‡æ®µï¼Œæ¶µç›–ç‰©ç†ä¸Šä¸å¯é€†è½¬çš„è¿‡ç¨‹ï¼ˆå¦‚è‡ªç”±è½ä½“ã€çˆ†ç‚¸ï¼‰å’Œå› æœæ‰‹åŠ¨æ“ä½œï¼ˆå¦‚åŠ æ³•ã€é™¤æ³•ï¼‰ã€‚VLMséœ€è¦åˆ¤æ–­æ¯ä¸ªè§†é¢‘ç‰‡æ®µæ˜¯æ­£å‘æ’­æ”¾è¿˜æ˜¯åå‘æ’­æ”¾ã€‚åŸºå‡†è¿˜æä¾›äº†äººç±»è¡Œä¸ºåŸºçº¿æ•°æ®ï¼Œç”¨äºå¯¹æ¯”æ¨¡å‹æ€§èƒ½ã€‚è¯„ä¼°æµç¨‹åŒ…æ‹¬ï¼š1) å°†è§†é¢‘è¾“å…¥VLMsï¼›2) æ¨¡å‹è¾“å‡ºæ—¶é—´æ–¹å‘çš„åˆ¤æ–­ç»“æœï¼›3) å°†æ¨¡å‹ç»“æœä¸äººç±»åŸºçº¿è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªåŸºäºå¿ƒç†ç‰©ç†å­¦çš„è¯„ä¼°åŸºå‡†AoT-PsyPhyBENCHï¼Œè¯¥åŸºå‡†ä¸äººç±»è®¤çŸ¥å¯¹é½ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯„ä¼°VLMsçš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼›2) ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†å¤šç§ä¸»æµVLMsåœ¨æ—¶é—´æ–¹å‘åˆ¤æ–­ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨æ—¶é—´æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼›3) å…¬å¼€äº†åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°ä»£ç ï¼Œä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAoT-PsyPhyBENCHæ›´ä¾§é‡äºè¯„ä¼°æ¨¡å‹å¯¹æ—¶é—´æµé€çš„ç›´è§‚ç†è§£ï¼Œè€Œéä»…ä»…æ˜¯è¯†åˆ«è§†é¢‘ä¸­çš„åŠ¨ä½œæˆ–äº‹ä»¶ã€‚

**å…³é”®è®¾è®¡**ï¼šAoT-PsyPhyBENCHä¸­çš„è§†é¢‘ç‰‡æ®µç»è¿‡ç²¾å¿ƒæŒ‘é€‰ï¼Œç¡®ä¿åŒ…å«æ¸…æ™°çš„æ—¶é—´æ–¹å‘çº¿ç´¢ã€‚è§†é¢‘æ—¶é•¿è¾ƒçŸ­ï¼Œé¿å…æ¨¡å‹è¿‡åº¦ä¾èµ–åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åŸºå‡†è¿˜è€ƒè™‘äº†ä¸åŒç±»å‹çš„è§†é¢‘å†…å®¹ï¼ŒåŒ…æ‹¬ç‰©ç†è¿‡ç¨‹å’Œäººç±»æ´»åŠ¨ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°æŒ‡æ ‡ä¸»è¦é‡‡ç”¨å‡†ç¡®ç‡ï¼Œå³æ¨¡å‹æ­£ç¡®åˆ¤æ–­è§†é¢‘æ’­æ”¾æ–¹å‘çš„æ¯”ä¾‹ã€‚è®ºæ–‡æ²¡æœ‰ç‰¹åˆ«æåŠæŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„çš„è®¾è®¡ï¼Œå› ä¸ºé‡ç‚¹åœ¨äºè¯„ä¼°ç°æœ‰æ¨¡å‹ï¼Œè€Œéæå‡ºæ–°çš„æ¨¡å‹æ¶æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°VLMsåœ¨AoT-PsyPhyBENCHä¸Šçš„è¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ï¼Œè¿œä½äºäººç±»çš„å‡†ç¡®ç‡ã€‚å³ä½¿æ˜¯æ€§èƒ½æœ€å¥½çš„æ¨¡å‹ï¼Œåœ¨ç‰©ç†ä¸Šä¸å¯é€†è½¬çš„è¿‡ç¨‹å’Œå› æœæ‰‹åŠ¨æ“ä½œä¸Šçš„å‡†ç¡®ç‡ä¹Ÿæ˜¾è‘—ä½äºäººç±»ã€‚ä¾‹å¦‚ï¼Œäººç±»åœ¨åˆ¤æ–­è‡ªç”±è½ä½“è§†é¢‘çš„æ—¶é—´æ–¹å‘æ—¶å‡ ä¹è¾¾åˆ°100%çš„å‡†ç¡®ç‡ï¼Œè€ŒVLMsçš„å‡†ç¡®ç‡åˆ™è¿œä½äºæ­¤ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç°æœ‰VLMsåœ¨æ—¶é—´æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡è§†é¢‘å†…å®¹ç†è§£ã€è§†é¢‘ç¼–è¾‘ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­ï¼Œæ¨¡å‹å¯ä»¥åˆ¤æ–­å¼‚å¸¸äº‹ä»¶çš„å‘ç”Ÿæ–¹å‘ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¿›è¡Œé¢„è­¦ã€‚åœ¨æœºå™¨äººé¢†åŸŸï¼Œæ¨¡å‹å¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£ç¯å¢ƒå˜åŒ–çš„æ—¶é—´é¡ºåºï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡Œå†³ç­–å’Œè§„åˆ’ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨VLMsåœ¨æ—¶é—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern vision-language models (VLMs) excel at many multimodal tasks, yet their grasp of temporal information in video remains weak and, crucially, under-evaluated. We probe this gap with a deceptively simple but revealing challenge: judging the arrow of time (AoT)-whether a short clip is played forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated benchmark that tests whether VLMs can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans. Our comprehensive evaluation of open-weight and proprietary, reasoning and non-reasoning VLMs reveals that most models perform near chance, and even the best lag far behind human accuracy on physically irreversible processes (e.g., free fall, diffusion/explosion) and causal manual actions (division/addition) that humans recognize almost instantly. These results highlight a fundamental gap in current multimodal systems: while they capture rich visual-semantic correlations, they lack the inductive biases required for temporal continuity and causal understanding. We release the code and data for AoT-PsyPhyBENCH to encourage further progress in the physical and temporal reasoning capabilities of VLMs.

