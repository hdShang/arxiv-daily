---
layout: default
title: ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning
---

# ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.27492" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.27492v2</a>
  <a href="https://arxiv.org/pdf/2510.27492.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27492v2" onclick="toggleFavorite(this, '2510.27492v2', 'ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, Yu Cheng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30 (æ›´æ–°: 2025-11-04)

**å¤‡æ³¨**: project page: https://thinkmorph.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ThinkMorphï¼šé€šè¿‡å¤šæ¨¡æ€äº¤é”™CoTæ¨ç†æ¶Œç°è§†è§‰æ“ä½œèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `æ€ç»´é“¾` `è§†è§‰æ“ä½œ` `äº¤é”™æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€æ¨ç†éœ€è¦è¯­è¨€å’Œè§†è§‰çš„ååŒï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ä¸¤ç§æ¨¡æ€çš„äº’è¡¥æ€§ã€‚
2. ThinkMorphé€šè¿‡ç”Ÿæˆäº¤é”™çš„æ–‡æœ¬-å›¾åƒæ¨ç†æ­¥éª¤ï¼Œæ˜¾å¼åœ°æ“ä½œè§†è§‰å†…å®¹ï¼Œä¿ƒè¿›æ¨¡æ€é—´çš„æœ‰æ•ˆäº’åŠ¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒThinkMorphåœ¨è§†è§‰ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶å±•ç°å‡ºè§†è§‰æ“ä½œç­‰æ¶Œç°èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ¨ç†éœ€è¦åœ¨è¯­è¨€å’Œè§†è§‰ä¹‹é—´è¿›è¡Œè¿­ä»£åè°ƒï¼Œä½†ä½•ä¸ºæœ‰æ„ä¹‰çš„äº¤é”™å¼æ€ç»´é“¾å°šä¸æ˜ç¡®ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæ–‡æœ¬å’Œå›¾åƒåº”ä½œä¸ºäº’è¡¥è€ŒéåŒæ„çš„æ¨¡æ€ï¼Œä»¥ç›¸äº’ä¿ƒè¿›æ¨ç†ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ThinkMorphï¼Œä¸€ä¸ªåœ¨çº¦24Ké«˜è´¨é‡äº¤é”™æ¨ç†è½¨è¿¹ä¸Šå¾®è°ƒçš„ç»Ÿä¸€æ¨¡å‹ï¼Œæ¶µç›–äº†ä¸åŒè§†è§‰å‚ä¸åº¦çš„ä»»åŠ¡ã€‚ThinkMorphå­¦ä¹ ç”Ÿæˆæ¸è¿›å¼çš„æ–‡æœ¬-å›¾åƒæ¨ç†æ­¥éª¤ï¼Œå…·ä½“åœ°æ“ä½œè§†è§‰å†…å®¹ï¼ŒåŒæ—¶ä¿æŒè¿è´¯çš„è¯­è¨€é€»è¾‘ã€‚å®ƒåœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ï¼ˆå¹³å‡è¶…è¿‡åŸºçº¿æ¨¡å‹34.7%ï¼‰ï¼Œå¹¶æ³›åŒ–åˆ°é¢†åŸŸå¤–ä»»åŠ¡ï¼Œè¾¾åˆ°ç”šè‡³è¶…è¿‡äº†æ›´å¤§è§„æ¨¡çš„ä¸“æœ‰VLMã€‚æ­¤å¤–ï¼ŒThinkMorphè¿˜è¡¨ç°å‡ºæ¶Œç°çš„å¤šæ¨¡æ€æ™ºèƒ½ï¼ŒåŒ…æ‹¬æœªè§è¿‡çš„è§†è§‰æ“ä½œæŠ€èƒ½ã€æ¨ç†æ¨¡å¼ä¹‹é—´çš„è‡ªé€‚åº”åˆ‡æ¢ï¼Œä»¥åŠé€šè¿‡å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ€ç»´å®ç°çš„æ›´å¥½çš„æµ‹è¯•æ—¶ç¼©æ”¾ã€‚è¿™äº›å‘ç°ä¸ºè¡¨å¾å¤šæ¨¡æ€æ¨ç†ç»Ÿä¸€æ¨¡å‹çš„æ¶Œç°èƒ½åŠ›æä¾›äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹é€šå¸¸éš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨è¯­è¨€å’Œè§†è§‰ä¿¡æ¯ä¹‹é—´çš„äº’è¡¥æ€§ï¼Œå¾€å¾€å°†ä¸¤ç§æ¨¡æ€è§†ä¸ºåŒæ„çš„ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹ç¼ºä¹æ·±åº¦å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æ„å»ºæœ‰æ•ˆçš„äº¤é”™å¼æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒæ¨¡æ€ä¹‹é—´è‡ªé€‚åº”åœ°åˆ‡æ¢ï¼Œå¹¶é€æ­¥æ¨è¿›æ¨ç†è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šThinkMorphçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ–‡æœ¬å’Œå›¾åƒè§†ä¸ºäº’è¡¥çš„æ¨¡æ€ï¼Œé€šè¿‡ç”Ÿæˆäº¤é”™çš„æ–‡æœ¬-å›¾åƒæ¨ç†æ­¥éª¤ï¼Œæ˜¾å¼åœ°æ“ä½œè§†è§‰å†…å®¹ï¼Œä»è€Œä¿ƒè¿›æ¨¡æ€é—´çš„æœ‰æ•ˆäº’åŠ¨ã€‚æ¨¡å‹å­¦ä¹ ç”Ÿæˆæ¸è¿›å¼çš„æ¨ç†æ­¥éª¤ï¼Œæ¯ä¸€æ­¥éƒ½åŒ…å«æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ï¼Œæ–‡æœ¬è´Ÿè´£é€»è¾‘æ¨ç†ï¼Œå›¾åƒè´Ÿè´£è§†è§‰æ“ä½œï¼Œä¸¤è€…ç›¸äº’ä¿ƒè¿›ï¼Œå…±åŒå®Œæˆæ¨ç†ä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šThinkMorphæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼ŒåŸºäºé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªå¤šæ¨¡æ€ç¼–ç å™¨å’Œä¸€ä¸ªå¤šæ¨¡æ€è§£ç å™¨ã€‚ç¼–ç å™¨è´Ÿè´£å°†æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ç¼–ç æˆç»Ÿä¸€çš„è¡¨ç¤ºï¼Œè§£ç å™¨è´Ÿè´£ç”Ÿæˆäº¤é”™çš„æ–‡æœ¬-å›¾åƒæ¨ç†æ­¥éª¤ã€‚è®­ç»ƒæ•°æ®åŒ…å«å¤§é‡çš„äº¤é”™æ¨ç†è½¨è¿¹ï¼Œæ¶µç›–äº†ä¸åŒè§†è§‰å‚ä¸åº¦çš„ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šThinkMorphçš„å…³é”®åˆ›æ–°åœ¨äºå…¶äº¤é”™å¼çš„æ¨ç†æ–¹å¼ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒä¹‹é—´è‡ªç”±åˆ‡æ¢ï¼Œå¹¶æ˜¾å¼åœ°æ“ä½œè§†è§‰å†…å®¹ã€‚è¿™ç§æ–¹å¼èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ä¸¤ç§æ¨¡æ€çš„äº’è¡¥æ€§ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒThinkMorphè¿˜å±•ç°å‡ºæ¶Œç°çš„å¤šæ¨¡æ€æ™ºèƒ½ï¼ŒåŒ…æ‹¬æœªè§è¿‡çš„è§†è§‰æ“ä½œæŠ€èƒ½å’Œæ¨ç†æ¨¡å¼ä¹‹é—´çš„è‡ªé€‚åº”åˆ‡æ¢ã€‚

**å…³é”®è®¾è®¡**ï¼šThinkMorphä½¿ç”¨äº†ä¸€ç§ç‰¹æ®Šçš„è®­ç»ƒç­–ç•¥ï¼Œç§°ä¸ºâ€œè§†è§‰æ“ä½œæŒ‡å¯¼â€ï¼ˆVisual Manipulation Guidanceï¼‰ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆèƒ½å¤Ÿæœ‰æ•ˆæ“ä½œè§†è§‰å†…å®¹çš„æ¨ç†æ­¥éª¤ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜ä½¿ç”¨äº†ä¸€ç§è‡ªé€‚åº”çš„æ¨ç†æ¨¡å¼åˆ‡æ¢æœºåˆ¶ï¼Œæ ¹æ®å½“å‰çš„çŠ¶æ€åŠ¨æ€åœ°é€‰æ‹©ä½¿ç”¨æ–‡æœ¬æ¨ç†æˆ–å›¾åƒæ¨ç†ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è¯­è¨€å»ºæ¨¡æŸå¤±å’Œè§†è§‰æ“ä½œæŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œè§†è§‰æ“ä½œèƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ThinkMorphåœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹³å‡è¶…è¿‡åŸºçº¿æ¨¡å‹34.7%ã€‚åœ¨é¢†åŸŸå¤–ä»»åŠ¡ä¸­ï¼ŒThinkMorphçš„æ€§èƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡äº†æ›´å¤§è§„æ¨¡çš„ä¸“æœ‰VLMã€‚æ­¤å¤–ï¼ŒThinkMorphè¿˜å±•ç°å‡ºæ¶Œç°çš„å¤šæ¨¡æ€æ™ºèƒ½ï¼ŒåŒ…æ‹¬æœªè§è¿‡çš„è§†è§‰æ“ä½œæŠ€èƒ½å’Œæ¨ç†æ¨¡å¼ä¹‹é—´çš„è‡ªé€‚åº”åˆ‡æ¢ï¼Œè¡¨æ˜å…¶å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ThinkMorphçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å¤šæ¨¡æ€æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å›¾åƒç¼–è¾‘å’Œè§†è§‰é—®ç­”ç­‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œæ“ä½œè§†è§‰ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„ä»»åŠ¡æ‰§è¡Œã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¹¶ä¸ºæ„å»ºæ›´å¼ºå¤§çš„é€šç”¨äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal reasoning requires iterative coordination between language and vision, yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and image thoughts should function as complementary rather than isomorphic modalities that mutually advance reasoning. Guided by this principle, we build ThinkMorph, a unified model fine-tuned on approximately 24K high-quality interleaved reasoning traces spanning tasks with varying visual engagement. ThinkMorph learns to generate progressive text-image reasoning steps that concretely manipulate visual content while maintaining coherent verbal logic. It delivers large gains on vision-centric benchmarks (averaging 34.7 percent over the base model) and generalizes to out-of-domain tasks, matching or surpassing larger and proprietary VLMs. Beyond performance, ThinkMorph exhibits emergent multimodal intelligence, including unseen visual manipulation skills, adaptive switching between reasoning modes, and better test-time scaling through diversified multimodal thoughts. These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning.

