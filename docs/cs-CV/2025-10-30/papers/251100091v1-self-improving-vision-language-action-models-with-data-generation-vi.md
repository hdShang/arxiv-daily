---
layout: default
title: Self-Improving Vision-Language-Action Models with Data Generation via Residual RL
---

# Self-Improving Vision-Language-Action Models with Data Generation via Residual RL

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.00091" target="_blank" class="toolbar-btn">arXiv: 2511.00091v1</a>
    <a href="https://arxiv.org/pdf/2511.00091.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00091v1" 
            onclick="toggleFavorite(this, '2511.00091v1', 'Self-Improving Vision-Language-Action Models with Data Generation via Residual RL')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi "Jim" Fan, Guanya Shi, Yuke Zhu

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-30

**Â§áÊ≥®**: 26 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PLDÊ°ÜÊû∂ÔºåÈÄöËøáÊÆãÂ∑ÆÂº∫ÂåñÂ≠¶‰π†ÂíåÊï∞ÊçÆÁîüÊàêËá™ÊèêÂçáËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `ÊÆãÂ∑ÆÂº∫ÂåñÂ≠¶‰π†` `Êï∞ÊçÆÁîüÊàê` `Ëá™ÊèêÂçáÂ≠¶‰π†` `Êú∫Âô®‰∫∫Êìç‰Ωú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°Âûã‰æùËµñ‰∫éÊòÇË¥µÁöÑ‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆËøõË°åÁõëÁù£ÂæÆË∞ÉÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. PLDÊ°ÜÊû∂ÈÄöËøáÊÆãÂ∑ÆÂº∫ÂåñÂ≠¶‰π†Êé¢Á¥¢Ê®°ÂûãÂ§±Ë¥•Âå∫ÂüüÔºåÂπ∂Âà©Áî®ÂàÜÂ∏ÉÊÑüÁü•ÁöÑÊï∞ÊçÆÊî∂ÈõÜÊñπÊ≥ïÁîüÊàêÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåPLDÂú®Â§ö‰∏™Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏äÊòæËëóÊèêÂçá‰∫ÜVLAÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂåÖÊã¨Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁõëÁù£ÂæÆË∞É(SFT)Â∑≤Êàê‰∏∫Â§ßÂûãËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÁöÑ‰∫ãÂÆûÊ†áÂáÜÂêéËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ΩÜÂÖ∂ÂØπÊòÇË¥µÁöÑ‰∫∫Â∑•ÊºîÁ§∫ÁöÑ‰æùËµñÈôêÂà∂‰∫ÜÂèØÊâ©Â±ïÊÄßÂíåÊ≥õÂåñÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜProbe, Learn, Distill (PLD)Ôºå‰∏Ä‰∏™‰∏âÈò∂ÊÆµÂç≥ÊèíÂç≥Áî®Ê°ÜÊû∂ÔºåÈÄöËøáÊÆãÂ∑ÆÂº∫ÂåñÂ≠¶‰π†(RL)ÂíåÂàÜÂ∏ÉÊÑüÁü•Êï∞ÊçÆÊî∂ÈõÜÊù•ÊîπËøõVLAÊ®°Âûã„ÄÇÂú®Á¨¨‰∏ÄÈò∂ÊÆµÔºåÊàë‰ª¨ËÆ≠ÁªÉËΩªÈáèÁ∫ßÊÆãÂ∑ÆactorÊù•Êé¢ÊµãVLAÈÄöÁî®Ê®°ÂûãÁöÑÂ§±Ë¥•Âå∫Âüü„ÄÇÂú®Á¨¨‰∫åÈò∂ÊÆµÔºåÊàë‰ª¨‰ΩøÁî®Ê∑∑ÂêàrolloutÊñπÊ°àÔºåËØ•ÊñπÊ°àÂ∞ÜÊî∂ÈõÜÂà∞ÁöÑËΩ®Ëøπ‰∏éÈÄöÁî®Ê®°ÂûãÁöÑÈÉ®ÁΩ≤ÂàÜÂ∏ÉÂØπÈΩêÔºåÂêåÊó∂ÊçïËé∑ÊÅ¢Â§çË°å‰∏∫„ÄÇÂú®Á¨¨‰∏âÈò∂ÊÆµÔºåÊàë‰ª¨‰ΩøÁî®Ê†áÂáÜSFTÂ∞ÜÁ≤æÂøÉËÆæËÆ°ÁöÑËΩ®ËøπÊèêÁÇºÂõûÈÄöÁî®Ê®°Âûã„ÄÇPLDÂú®LIBERO‰∏äÂÆûÁé∞‰∫ÜÊé•ËøëÈ•±ÂíåÁöÑ99%ÁöÑ‰ªªÂä°ÊàêÂäüÁéáÔºåÂú®SimplerEnv‰∏äËé∑Âæó‰∫ÜË∂ÖËøá50%ÁöÑÊî∂ÁõäÔºåÂπ∂Âú®ÁúüÂÆû‰∏ñÁïåÁöÑFrankaÂíåYAMÊú∫Ê¢∞ËáÇÊìç‰Ωú‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü100%ÁöÑÊàêÂäüÁéá„ÄÇÊ∂àËûçÂÆûÈ™åË°®ÊòéÔºåÊÆãÂ∑ÆÊé¢ÊµãÂíåÂàÜÂ∏ÉÊÑüÁü•ÂõûÊîæÊòØÊî∂ÈõÜÈÉ®ÁΩ≤ÂØπÈΩêÊï∞ÊçÆÁöÑÂÖ≥ÈîÆÔºåËøô‰∫õÊï∞ÊçÆÂèØ‰ª•ÊîπËøõÂ∑≤ËßÅÂíåÊú™ËßÅ‰ªªÂä°Ôºå‰ªéËÄå‰∏∫Ëá™ÊèêÂçáVLAÊ®°ÂûãÊèê‰æõÂèØÊâ©Â±ïÁöÑË∑ØÂæÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°Âûã‰æùËµñÂ§ßÈáè‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆËøõË°åÁõëÁù£ÂæÆË∞ÉÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÊï∞ÊçÆËé∑ÂèñÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊÆãÂ∑ÆÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËá™Âä®Êé¢Á¥¢VLAÊ®°ÂûãÁöÑÂ§±Ë¥•Âå∫ÂüüÔºåÂπ∂ÁîüÊàêÈ´òË¥®ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ªéËÄåÂÆûÁé∞Ê®°ÂûãÁöÑËá™ÊèêÂçá„ÄÇÈÄöËøáÂ≠¶‰π†ÊÆãÂ∑ÆÁ≠ñÁï•ÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Á∫†Ê≠£Ëá™Ë∫´ÁöÑÈîôËØØÔºåÂπ∂ÊèêÂçáÂú®Êú™ËßÅ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPLDÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöProbeÔºàÊé¢ÊµãÔºâ„ÄÅLearnÔºàÂ≠¶‰π†ÔºâÂíåDistillÔºàÊèêÁÇºÔºâ„ÄÇ
1. **ProbeÈò∂ÊÆµ**ÔºöËÆ≠ÁªÉËΩªÈáèÁ∫ßÁöÑÊÆãÂ∑ÆactorÔºåÁî®‰∫éÊé¢ÊµãVLAÈÄöÁî®Ê®°ÂûãÂú®ÊâßË°å‰ªªÂä°Êó∂ÁöÑÂ§±Ë¥•Âå∫Âüü„ÄÇ
2. **LearnÈò∂ÊÆµ**Ôºö‰ΩøÁî®Ê∑∑ÂêàrolloutÁ≠ñÁï•ÔºåÊî∂ÈõÜ‰∏éÈÄöÁî®Ê®°ÂûãÈÉ®ÁΩ≤ÂàÜÂ∏ÉÂØπÈΩêÁöÑËΩ®ËøπÔºåÂêåÊó∂ÊçïÊçâÊ®°Âûã‰ªéÂ§±Ë¥•‰∏≠ÊÅ¢Â§çÁöÑË°å‰∏∫„ÄÇ
3. **DistillÈò∂ÊÆµ**ÔºöÂà©Áî®Êî∂ÈõÜÂà∞ÁöÑÈ´òË¥®ÈáèËΩ®ËøπÔºåÈÄöËøáÊ†áÂáÜÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊñπÊ≥ïÔºåÂ∞ÜÁü•ËØÜÊèêÁÇºÂõûÈÄöÁî®Ê®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî®ÊÆãÂ∑ÆÂº∫ÂåñÂ≠¶‰π†Êù•ÊåáÂØºÊï∞ÊçÆÁîüÊàêËøáÁ®ãÔºåÂπ∂ÈááÁî®ÂàÜÂ∏ÉÊÑüÁü•ÁöÑÂõûÊîæÁ≠ñÁï•ÔºåÁ°Æ‰øùÁîüÊàêÁöÑÊï∞ÊçÆ‰∏éÊ®°ÂûãÁöÑÂÆûÈôÖÈÉ®ÁΩ≤ÁéØÂ¢ÉÁõ∏Á¨¶„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂçáÊ®°ÂûãÂú®ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÔºåÂπ∂Èôç‰ΩéÂØπ‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö
* **ÊÆãÂ∑ÆActor**ÔºöËΩªÈáèÁ∫ßÁöÑÁ•ûÁªèÁΩëÁªúÔºåÂ≠¶‰π†Âú®VLAÊ®°ÂûãÁöÑÂü∫Á°Ä‰∏äËøõË°åÂä®‰ΩúË∞ÉÊï¥Ôºå‰ª•Á∫†Ê≠£ÈîôËØØ„ÄÇ
* **Ê∑∑ÂêàRolloutÁ≠ñÁï•**ÔºöÁªìÂêàVLAÊ®°ÂûãÁöÑÁ≠ñÁï•ÂíåÊÆãÂ∑ÆActorÁöÑÁ≠ñÁï•ÔºåÂπ≥Ë°°Êé¢Á¥¢ÂíåÂà©Áî®ÔºåÁ°Æ‰øùÊï∞ÊçÆË¥®Èáè„ÄÇ
* **ÂàÜÂ∏ÉÊÑüÁü•ÂõûÊîæ**ÔºöÊ†πÊçÆVLAÊ®°ÂûãÁöÑÈÉ®ÁΩ≤ÂàÜÂ∏ÉÈÄâÊã©ËÆ≠ÁªÉÊï∞ÊçÆÔºåÈÅøÂÖçÂºïÂÖ•ÂÅèÂ∑Æ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

PLDÊ°ÜÊû∂Âú®LIBEROÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊé•ËøëÈ•±ÂíåÁöÑ99%‰ªªÂä°ÊàêÂäüÁéáÔºåÂú®SimplerEnvÊï∞ÊçÆÈõÜ‰∏äËé∑Âæó‰∫ÜË∂ÖËøá50%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂Âú®ÁúüÂÆû‰∏ñÁïåÁöÑFrankaÂíåYAMÊú∫Ê¢∞ËáÇÊìç‰Ωú‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü100%ÁöÑÊàêÂäüÁéá„ÄÇÊ∂àËûçÂÆûÈ™åËØÅÊòéÔºåÊÆãÂ∑ÆÊé¢ÊµãÂíåÂàÜÂ∏ÉÊÑüÁü•ÂõûÊîæÊòØÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáËá™ÊèêÂçáVLAÊ®°ÂûãÔºåÂèØ‰ª•Èôç‰ΩéÂØπ‰∫∫Â∑•Âπ≤È¢ÑÁöÑÈúÄÊ±ÇÔºåÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥È´òÊïàÁöÑËá™Âä®ÂåñËß£ÂÜ≥ÊñπÊ°à„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÁöÑÊôÆÂèäÂíåÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist's deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.

