---
layout: default
title: Self-Improving Vision-Language-Action Models with Data Generation via Residual RL
---

# Self-Improving Vision-Language-Action Models with Data Generation via Residual RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.00091" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.00091v1</a>
  <a href="https://arxiv.org/pdf/2511.00091.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00091v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.00091v1', 'Self-Improving Vision-Language-Action Models with Data Generation via Residual RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi "Jim" Fan, Guanya Shi, Yuke Zhu

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30

**å¤‡æ³¨**: 26 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPLDæ¡†æ¶ï¼Œé€šè¿‡æ®‹å·®å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®ç”Ÿæˆè‡ªæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æ®‹å·®å¼ºåŒ–å­¦ä¹ ` `æ•°æ®ç”Ÿæˆ` `è‡ªæå‡å­¦ä¹ ` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ä¾èµ–äºæ˜‚è´µçš„äººå·¥æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. PLDæ¡†æ¶é€šè¿‡æ®‹å·®å¼ºåŒ–å­¦ä¹ æ¢ç´¢æ¨¡å‹å¤±è´¥åŒºåŸŸï¼Œå¹¶åˆ©ç”¨åˆ†å¸ƒæ„ŸçŸ¥çš„æ•°æ®æ”¶é›†æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPLDåœ¨å¤šä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†VLAæ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç›‘ç£å¾®è°ƒ(SFT)å·²æˆä¸ºå¤§å‹è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹çš„äº‹å®æ ‡å‡†åè®­ç»ƒç­–ç•¥ï¼Œä½†å…¶å¯¹æ˜‚è´µçš„äººå·¥æ¼”ç¤ºçš„ä¾èµ–é™åˆ¶äº†å¯æ‰©å±•æ€§å’Œæ³›åŒ–æ€§ã€‚æˆ‘ä»¬æå‡ºäº†Probe, Learn, Distill (PLD)ï¼Œä¸€ä¸ªä¸‰é˜¶æ®µå³æ’å³ç”¨æ¡†æ¶ï¼Œé€šè¿‡æ®‹å·®å¼ºåŒ–å­¦ä¹ (RL)å’Œåˆ†å¸ƒæ„ŸçŸ¥æ•°æ®æ”¶é›†æ¥æ”¹è¿›VLAæ¨¡å‹ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è®­ç»ƒè½»é‡çº§æ®‹å·®actoræ¥æ¢æµ‹VLAé€šç”¨æ¨¡å‹çš„å¤±è´¥åŒºåŸŸã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨æ··åˆrolloutæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå°†æ”¶é›†åˆ°çš„è½¨è¿¹ä¸é€šç”¨æ¨¡å‹çš„éƒ¨ç½²åˆ†å¸ƒå¯¹é½ï¼ŒåŒæ—¶æ•è·æ¢å¤è¡Œä¸ºã€‚åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†SFTå°†ç²¾å¿ƒè®¾è®¡çš„è½¨è¿¹æç‚¼å›é€šç”¨æ¨¡å‹ã€‚PLDåœ¨LIBEROä¸Šå®ç°äº†æ¥è¿‘é¥±å’Œçš„99%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œåœ¨SimplerEnvä¸Šè·å¾—äº†è¶…è¿‡50%çš„æ”¶ç›Šï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œçš„Frankaå’ŒYAMæœºæ¢°è‡‚æ“ä½œä»»åŠ¡ä¸Šå®ç°äº†100%çš„æˆåŠŸç‡ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ®‹å·®æ¢æµ‹å’Œåˆ†å¸ƒæ„ŸçŸ¥å›æ”¾æ˜¯æ”¶é›†éƒ¨ç½²å¯¹é½æ•°æ®çš„å…³é”®ï¼Œè¿™äº›æ•°æ®å¯ä»¥æ”¹è¿›å·²è§å’Œæœªè§ä»»åŠ¡ï¼Œä»è€Œä¸ºè‡ªæå‡VLAæ¨¡å‹æä¾›å¯æ‰©å±•çš„è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ•°æ®è·å–æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ®‹å·®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªåŠ¨æ¢ç´¢VLAæ¨¡å‹çš„å¤±è´¥åŒºåŸŸï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œå®ç°æ¨¡å‹çš„è‡ªæå‡ã€‚é€šè¿‡å­¦ä¹ æ®‹å·®ç­–ç•¥ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°çº æ­£è‡ªèº«çš„é”™è¯¯ï¼Œå¹¶æå‡åœ¨æœªè§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPLDæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šProbeï¼ˆæ¢æµ‹ï¼‰ã€Learnï¼ˆå­¦ä¹ ï¼‰å’ŒDistillï¼ˆæç‚¼ï¼‰ã€‚
1. **Probeé˜¶æ®µ**ï¼šè®­ç»ƒè½»é‡çº§çš„æ®‹å·®actorï¼Œç”¨äºæ¢æµ‹VLAé€šç”¨æ¨¡å‹åœ¨æ‰§è¡Œä»»åŠ¡æ—¶çš„å¤±è´¥åŒºåŸŸã€‚
2. **Learné˜¶æ®µ**ï¼šä½¿ç”¨æ··åˆrolloutç­–ç•¥ï¼Œæ”¶é›†ä¸é€šç”¨æ¨¡å‹éƒ¨ç½²åˆ†å¸ƒå¯¹é½çš„è½¨è¿¹ï¼ŒåŒæ—¶æ•æ‰æ¨¡å‹ä»å¤±è´¥ä¸­æ¢å¤çš„è¡Œä¸ºã€‚
3. **Distillé˜¶æ®µ**ï¼šåˆ©ç”¨æ”¶é›†åˆ°çš„é«˜è´¨é‡è½¨è¿¹ï¼Œé€šè¿‡æ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ï¼Œå°†çŸ¥è¯†æç‚¼å›é€šç”¨æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨æ®‹å·®å¼ºåŒ–å­¦ä¹ æ¥æŒ‡å¯¼æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨åˆ†å¸ƒæ„ŸçŸ¥çš„å›æ”¾ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„æ•°æ®ä¸æ¨¡å‹çš„å®é™…éƒ¨ç½²ç¯å¢ƒç›¸ç¬¦ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼š
* **æ®‹å·®Actor**ï¼šè½»é‡çº§çš„ç¥ç»ç½‘ç»œï¼Œå­¦ä¹ åœ¨VLAæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡ŒåŠ¨ä½œè°ƒæ•´ï¼Œä»¥çº æ­£é”™è¯¯ã€‚
* **æ··åˆRolloutç­–ç•¥**ï¼šç»“åˆVLAæ¨¡å‹çš„ç­–ç•¥å’Œæ®‹å·®Actorçš„ç­–ç•¥ï¼Œå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚
* **åˆ†å¸ƒæ„ŸçŸ¥å›æ”¾**ï¼šæ ¹æ®VLAæ¨¡å‹çš„éƒ¨ç½²åˆ†å¸ƒé€‰æ‹©è®­ç»ƒæ•°æ®ï¼Œé¿å…å¼•å…¥åå·®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PLDæ¡†æ¶åœ¨LIBEROæ•°æ®é›†ä¸Šå®ç°äº†æ¥è¿‘é¥±å’Œçš„99%ä»»åŠ¡æˆåŠŸç‡ï¼Œåœ¨SimplerEnvæ•°æ®é›†ä¸Šè·å¾—äº†è¶…è¿‡50%çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œçš„Frankaå’ŒYAMæœºæ¢°è‡‚æ“ä½œä»»åŠ¡ä¸Šå®ç°äº†100%çš„æˆåŠŸç‡ã€‚æ¶ˆèå®éªŒè¯æ˜ï¼Œæ®‹å·®æ¢æµ‹å’Œåˆ†å¸ƒæ„ŸçŸ¥å›æ”¾æ˜¯æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸã€‚é€šè¿‡è‡ªæå‡VLAæ¨¡å‹ï¼Œå¯ä»¥é™ä½å¯¹äººå·¥å¹²é¢„çš„éœ€æ±‚ï¼Œæé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist's deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.

