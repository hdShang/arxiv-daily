---
layout: default
title: Language-Guided Graph Representation Learning for Video Summarization
---

# Language-Guided Graph Representation Learning for Video Summarization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.10953" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.10953v1</a>
  <a href="https://arxiv.org/pdf/2511.10953.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.10953v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.10953v1', 'Language-Guided Graph Representation Learning for Video Summarization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenrui Li, Wei Han, Hengyu Man, Wangmeng Zuo, Xiaopeng Fan, Yonghong Tian

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-14

**å¤‡æ³¨**: Accepted by IEEE TPAMI

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/liwrui/LGRLN)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯­è¨€å¼•å¯¼çš„å›¾è¡¨ç¤ºå­¦ä¹ ç½‘ç»œLGRLNï¼Œç”¨äºè§£å†³è§†é¢‘æ‘˜è¦ä¸­å…¨å±€ä¾èµ–å»ºæ¨¡å’Œå¤šæ¨¡æ€å®šåˆ¶é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘æ‘˜è¦` `å›¾è¡¨ç¤ºå­¦ä¹ ` `è¯­è¨€å¼•å¯¼` `è·¨æ¨¡æ€åµŒå…¥` `å›¾å·ç§¯ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ‘˜è¦æ–¹æ³•éš¾ä»¥æ•æ‰è§†é¢‘å†…å®¹çš„å…¨å±€ä¾èµ–å…³ç³»ï¼Œå¹¶ä¸”éš¾ä»¥é€‚åº”å¤šæ¨¡æ€ç”¨æˆ·å®šåˆ¶éœ€æ±‚ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. LGRLNé€šè¿‡æ„å»ºè§†é¢‘å›¾æ¥ä¿ç•™æ—¶é—´é¡ºåºå’Œä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨è¯­è¨€å¼•å¯¼çš„è·¨æ¨¡æ€åµŒå…¥ç”Ÿæˆç‰¹å®šæ–‡æœ¬æè¿°çš„è§†é¢‘æ‘˜è¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLGRLNåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´å’Œæ¨¡å‹å‚æ•°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ç¤¾äº¤åª’ä½“ä¸Šè§†é¢‘å†…å®¹çš„å¿«é€Ÿå¢é•¿ï¼Œè§†é¢‘æ‘˜è¦å·²æˆä¸ºå¤šåª’ä½“å¤„ç†ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨æ•è·è§†é¢‘å†…å®¹ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»å’Œé€‚åº”å¤šæ¨¡æ€ç”¨æˆ·å®šåˆ¶æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè§†é¢‘å¸§ä¹‹é—´çš„æ—¶é—´é‚»è¿‘æ€§å¹¶ä¸æ€»æ˜¯å¯¹åº”äºè¯­ä¹‰é‚»è¿‘æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè§†é¢‘æ‘˜è¦çš„æ–°å‹è¯­è¨€å¼•å¯¼å›¾è¡¨ç¤ºå­¦ä¹ ç½‘ç»œï¼ˆLGRLNï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè§†é¢‘å›¾ç”Ÿæˆå™¨ï¼Œå®ƒå°†è§†é¢‘å¸§è½¬æ¢ä¸ºç»“æ„åŒ–å›¾ï¼Œä»¥ä¿ç•™æ—¶é—´é¡ºåºå’Œä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚é€šè¿‡æ„å»ºå‰å‘ã€åå‘å’Œæ— å‘å›¾ï¼Œè§†é¢‘å›¾ç”Ÿæˆå™¨æœ‰æ•ˆåœ°ä¿ç•™äº†è§†é¢‘å†…å®¹çš„é¡ºåºæ€§å’Œä¸Šä¸‹æ–‡å…³ç³»ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰åŒé˜ˆå€¼å›¾å·ç§¯æœºåˆ¶çš„å†…å›¾å…³ç³»æ¨ç†æ¨¡å—ï¼Œè¯¥æ¨¡å—åŒºåˆ†èŠ‚ç‚¹ä¹‹é—´è¯­ä¹‰ç›¸å…³çš„å¸§å’Œä¸ç›¸å…³çš„å¸§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„è¯­è¨€å¼•å¯¼è·¨æ¨¡æ€åµŒå…¥æ¨¡å—ç”Ÿæˆå…·æœ‰ç‰¹å®šæ–‡æœ¬æè¿°çš„è§†é¢‘æ‘˜è¦ã€‚æˆ‘ä»¬å°†æ‘˜è¦ç”Ÿæˆè¾“å‡ºå»ºæ¨¡ä¸ºä¼¯åŠªåˆ©åˆ†å¸ƒçš„æ··åˆï¼Œå¹¶ä½¿ç”¨EMç®—æ³•æ±‚è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„LGRLNåˆ†åˆ«å‡å°‘äº†87.8%çš„æ¨ç†æ—¶é—´å’Œ91.7%çš„æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨https://github.com/liwrui/LGRLNè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘æ‘˜è¦æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰è§†é¢‘å¸§ä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»ï¼Œå¹¶ä¸”éš¾ä»¥æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°è¿›è¡Œå®šåˆ¶åŒ–æ‘˜è¦ç”Ÿæˆã€‚æ—¶é—´ä¸Šçš„ç›¸é‚»å¸§å¹¶ä¸ä¸€å®šåœ¨è¯­ä¹‰ä¸Šç›¸å…³ï¼Œè¿™ç»™æ‘˜è¦æå–å¸¦æ¥äº†æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘å¸§è¡¨ç¤ºä¸ºå›¾ç»“æ„ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå­¦ä¹ è§†é¢‘å¸§ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¼•å…¥è¯­è¨€ä¿¡æ¯æŒ‡å¯¼æ‘˜è¦ç”Ÿæˆã€‚é€šè¿‡å›¾ç»“æ„ï¼Œå¯ä»¥æ›´å¥½åœ°æ•æ‰è§†é¢‘çš„å…¨å±€ä¾èµ–å…³ç³»å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¯­è¨€å¼•å¯¼æ¨¡å—åˆ™å®ç°äº†æ ¹æ®ç”¨æˆ·éœ€æ±‚å®šåˆ¶æ‘˜è¦çš„åŠŸèƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLGRLNåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘å›¾ç”Ÿæˆå™¨ï¼šå°†è§†é¢‘å¸§è½¬æ¢ä¸ºç»“æ„åŒ–å›¾ï¼ŒåŒ…æ‹¬å‰å‘ã€åå‘å’Œæ— å‘å›¾ï¼Œä»¥ä¿ç•™æ—¶é—´é¡ºåºå’Œä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚2) å†…å›¾å…³ç³»æ¨ç†æ¨¡å—ï¼šåˆ©ç”¨åŒé˜ˆå€¼å›¾å·ç§¯æœºåˆ¶åŒºåˆ†è¯­ä¹‰ç›¸å…³çš„å¸§å’Œä¸ç›¸å…³çš„å¸§ã€‚3) è¯­è¨€å¼•å¯¼è·¨æ¨¡æ€åµŒå…¥æ¨¡å—ï¼šç”Ÿæˆå…·æœ‰ç‰¹å®šæ–‡æœ¬æè¿°çš„è§†é¢‘æ‘˜è¦ã€‚4) æ‘˜è¦ç”Ÿæˆæ¨¡å—ï¼šå°†æ‘˜è¦ç”Ÿæˆå»ºæ¨¡ä¸ºä¼¯åŠªåˆ©åˆ†å¸ƒçš„æ··åˆï¼Œå¹¶ä½¿ç”¨EMç®—æ³•æ±‚è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è§†é¢‘å¸§ä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚2) è®¾è®¡äº†ä¸€ç§è¯­è¨€å¼•å¯¼çš„è·¨æ¨¡æ€åµŒå…¥æ¨¡å—ï¼Œå®ç°äº†æ ¹æ®ç”¨æˆ·æ–‡æœ¬æè¿°å®šåˆ¶è§†é¢‘æ‘˜è¦çš„åŠŸèƒ½ã€‚3) æå‡ºäº†åŒé˜ˆå€¼å›¾å·ç§¯æœºåˆ¶ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«è¯­ä¹‰ç›¸å…³çš„å¸§ã€‚

**å…³é”®è®¾è®¡**ï¼šè§†é¢‘å›¾ç”Ÿæˆå™¨æ„å»ºäº†å‰å‘ã€åå‘å’Œæ— å‘å›¾ï¼Œä»¥å…¨é¢æ•æ‰è§†é¢‘å¸§ä¹‹é—´çš„å…³ç³»ã€‚å†…å›¾å…³ç³»æ¨ç†æ¨¡å—ä½¿ç”¨åŒé˜ˆå€¼æ¥åŒºåˆ†è¯­ä¹‰ç›¸å…³å’Œä¸ç›¸å…³çš„å¸§ï¼Œæé«˜äº†å›¾å·ç§¯çš„æœ‰æ•ˆæ€§ã€‚æ‘˜è¦ç”Ÿæˆæ¨¡å—ä½¿ç”¨EMç®—æ³•ä¼˜åŒ–ä¼¯åŠªåˆ©åˆ†å¸ƒçš„æ··åˆæ¨¡å‹ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„æ‘˜è¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLGRLNåœ¨å¤šä¸ªè§†é¢‘æ‘˜è¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒLGRLNæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´å’Œæ¨¡å‹å‚æ•°ï¼Œåˆ†åˆ«é™ä½äº†87.8%å’Œ91.7%ï¼Œè¿™ä½¿å¾—è¯¥æ–¹æ³•æ›´æ˜“äºéƒ¨ç½²å’Œåº”ç”¨ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å¼€æºï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œå¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç›‘æ§ã€æ–°é—»æ‘˜è¦ã€ç¤¾äº¤åª’ä½“è§†é¢‘æ¨èç­‰é¢†åŸŸã€‚é€šè¿‡è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ‘˜è¦ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£è§†é¢‘å†…å®¹ï¼Œæé«˜ä¿¡æ¯è·å–æ•ˆç‡ã€‚è¯­è¨€å¼•å¯¼çš„æ‘˜è¦ç”ŸæˆåŠŸèƒ½ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªèº«éœ€æ±‚å®šåˆ¶æ‘˜è¦ï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ™ºèƒ½è§†é¢‘åˆ†æå’Œä¸ªæ€§åŒ–æ¨èç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid growth of video content on social media, video summarization has become a crucial task in multimedia processing. However, existing methods face challenges in capturing global dependencies in video content and accommodating multimodal user customization. Moreover, temporal proximity between video frames does not always correspond to semantic proximity. To tackle these challenges, we propose a novel Language-guided Graph Representation Learning Network (LGRLN) for video summarization. Specifically, we introduce a video graph generator that converts video frames into a structured graph to preserve temporal order and contextual dependencies. By constructing forward, backward and undirected graphs, the video graph generator effectively preserves the sequentiality and contextual relationships of video content. We designed an intra-graph relational reasoning module with a dual-threshold graph convolution mechanism, which distinguishes semantically relevant frames from irrelevant ones between nodes. Additionally, our proposed language-guided cross-modal embedding module generates video summaries with specific textual descriptions. We model the summary generation output as a mixture of Bernoulli distribution and solve it with the EM algorithm. Experimental results show that our method outperforms existing approaches across multiple benchmarks. Moreover, we proposed LGRLN reduces inference time and model parameters by 87.8% and 91.7%, respectively. Our codes and pre-trained models are available at https://github.com/liwrui/LGRLN.

