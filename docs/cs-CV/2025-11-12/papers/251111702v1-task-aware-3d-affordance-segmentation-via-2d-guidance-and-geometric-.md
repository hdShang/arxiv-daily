---
layout: default
title: Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement
---

# Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.11702" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.11702v1</a>
  <a href="https://arxiv.org/pdf/2511.11702.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.11702v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.11702v1', 'Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lian He, Meng Liu, Qilang Ye, Yu Zhou, Xiang Deng, Gangyi Ding

**åˆ†ç±»**: cs.CV, cs.AI, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTASAæ¡†æ¶ï¼Œèåˆ2Då¼•å¯¼ä¸å‡ ä½•ä¼˜åŒ–ï¼Œå®ç°ä»»åŠ¡æ„ŸçŸ¥çš„3Då¯äº¤äº’åŒºåŸŸåˆ†å‰²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `å¯äº¤äº’åŒºåŸŸåˆ†å‰²` `å…·èº«æ™ºèƒ½` `å‡ ä½•æ¨ç†` `è‡ªç„¶è¯­è¨€æŒ‡ä»¤`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨3Då¯äº¤äº’åŒºåŸŸåˆ†å‰²ä¸­å¿½ç•¥äº†å‡ ä½•ä¿¡æ¯ï¼Œä¸”è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…·èº«æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. TASAæ¡†æ¶èåˆ2Dè¯­ä¹‰çº¿ç´¢å’Œ3Då‡ ä½•æ¨ç†ï¼Œä»¥ç²—åˆ°ç²¾çš„æ–¹å¼åˆ†å‰²ï¼Œæå‡äº†åˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTASAåœ¨SceneFun3Dæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„3Dåœºæ™¯çº§å¯äº¤äº’åŒºåŸŸåˆ†å‰²ï¼ˆTASAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç†è§£3Dåœºæ™¯ä¸­çš„å¯äº¤äº’åŒºåŸŸï¼Œä»è€Œä½¿å…·èº«æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ„ä¹‰çš„äº¤äº’ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¯¹è±¡çº§åˆ«çš„å¯äº¤äº’åŒºåŸŸåˆ†å‰²ï¼Œæˆ–è€…ç®€å•åœ°å°†2Dé¢„æµ‹æå‡åˆ°3Dï¼Œå¿½ç•¥äº†ç‚¹äº‘ä¸­ä¸°å¯Œçš„å‡ ä½•ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼ŒTASAè”åˆåˆ©ç”¨2Dè¯­ä¹‰çº¿ç´¢å’Œ3Då‡ ä½•æ¨ç†ï¼Œä»¥ç²—åˆ°ç²¾çš„æ–¹å¼è¿›è¡Œåˆ†å‰²ã€‚TASAåŒ…å«ä¸€ä¸ªä»»åŠ¡æ„ŸçŸ¥çš„2Då¯äº¤äº’åŒºåŸŸæ£€æµ‹æ¨¡å—ï¼Œä»è¯­è¨€å’Œè§†è§‰è¾“å…¥ä¸­è¯†åˆ«å¯æ“ä½œçš„ç‚¹ï¼Œä»è€Œå¼•å¯¼ä»»åŠ¡ç›¸å…³è§†ç‚¹çš„é€‰æ‹©ï¼Œæé«˜æ£€æµ‹æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ª3Då¯äº¤äº’åŒºåŸŸç»†åŒ–æ¨¡å—ï¼Œå°†2Dè¯­ä¹‰å…ˆéªŒä¸å±€éƒ¨3Då‡ ä½•ä¿¡æ¯ç›¸ç»“åˆï¼Œç”Ÿæˆç²¾ç¡®ä¸”ç©ºé—´ä¸€è‡´çš„3Då¯äº¤äº’åŒºåŸŸæ©ç ã€‚åœ¨SceneFun3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTASAåœ¨åœºæ™¯çº§å¯äº¤äº’åŒºåŸŸåˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨3Dåœºæ™¯çº§å¯äº¤äº’åŒºåŸŸåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ã€‚ä¸€æ˜¯å¿½ç•¥äº†ç‚¹äº‘ä¸­ä¸°å¯Œçš„å‡ ä½•ç»“æ„ä¿¡æ¯ï¼Œå¯¼è‡´åˆ†å‰²ç²¾åº¦ä¸é«˜ã€‚äºŒæ˜¯ç›´æ¥å°†2Dé¢„æµ‹æå‡åˆ°3Dï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åº”ç”¨äºå¤§è§„æ¨¡åœºæ™¯ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæ˜¯è¯¥ä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTASAçš„æ ¸å¿ƒæ€è·¯æ˜¯è”åˆåˆ©ç”¨2Dè¯­ä¹‰çº¿ç´¢å’Œ3Då‡ ä½•æ¨ç†ï¼Œä»¥ç²—åˆ°ç²¾çš„æ–¹å¼è¿›è¡Œå¯äº¤äº’åŒºåŸŸåˆ†å‰²ã€‚é¦–å…ˆï¼Œåˆ©ç”¨ä»»åŠ¡æ„ŸçŸ¥çš„2Dæ£€æµ‹æ¨¡å—å¿«é€Ÿå®šä½æ½œåœ¨çš„å¯äº¤äº’åŒºåŸŸï¼Œå‡å°‘åç»­3Då¤„ç†çš„èŒƒå›´ã€‚ç„¶åï¼Œåˆ©ç”¨3Dç»†åŒ–æ¨¡å—ï¼Œç»“åˆ2Dè¯­ä¹‰å…ˆéªŒå’Œå±€éƒ¨3Då‡ ä½•ä¿¡æ¯ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚è¿™ç§è®¾è®¡æ—¢è€ƒè™‘äº†è¯­ä¹‰ä¿¡æ¯ï¼Œåˆå……åˆ†åˆ©ç”¨äº†å‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTASAæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šä»»åŠ¡æ„ŸçŸ¥çš„2Då¯äº¤äº’åŒºåŸŸæ£€æµ‹æ¨¡å—å’Œ3Då¯äº¤äº’åŒºåŸŸç»†åŒ–æ¨¡å—ã€‚é¦–å…ˆï¼Œ2Dæ£€æµ‹æ¨¡å—æ¥æ”¶è¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥ï¼Œé¢„æµ‹2Då›¾åƒä¸­çš„å¯äº¤äº’åŒºåŸŸã€‚ç„¶åï¼Œæ ¹æ®2Dé¢„æµ‹ç»“æœé€‰æ‹©ä»»åŠ¡ç›¸å…³çš„è§†ç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œ3Dç»†åŒ–æ¨¡å—å°†2Dè¯­ä¹‰å…ˆéªŒæŠ•å½±åˆ°3Dç‚¹äº‘ï¼Œå¹¶ç»“åˆå±€éƒ¨3Då‡ ä½•ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆçš„3Då¯äº¤äº’åŒºåŸŸåˆ†å‰²ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šTASAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä»»åŠ¡æ„ŸçŸ¥çš„2Dæ£€æµ‹æ¨¡å—å’Œ3Dç»†åŒ–æ¨¡å—çš„ç»“åˆã€‚ä»»åŠ¡æ„ŸçŸ¥çš„2Dæ£€æµ‹æ¨¡å—èƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤å¿«é€Ÿå®šä½æ½œåœ¨çš„å¯äº¤äº’åŒºåŸŸï¼Œå‡å°‘äº†è®¡ç®—é‡ã€‚3Dç»†åŒ–æ¨¡å—åˆ™èƒ½å¤Ÿæœ‰æ•ˆèåˆ2Dè¯­ä¹‰å…ˆéªŒå’Œ3Då‡ ä½•ä¿¡æ¯ï¼Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTASAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶é™ä½è®¡ç®—å¤æ‚åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨2Dæ£€æµ‹æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†Transformerç»“æ„æ¥å¤„ç†è¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ä»»åŠ¡éœ€æ±‚ã€‚åœ¨3Dç»†åŒ–æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†PointNet++ç»“æ„æ¥æå–ç‚¹äº‘çš„å±€éƒ¨å‡ ä½•ç‰¹å¾ï¼Œå¹¶ç»“åˆ2Dè¯­ä¹‰å…ˆéªŒè¿›è¡Œåˆ†å‰²ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åˆ†å‰²ç»“æœã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTASAåœ¨SceneFun3Dæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨åœºæ™¯çº§å¯äº¤äº’åŒºåŸŸåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒTASAçš„mIoUæŒ‡æ ‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°æ˜¾è‘—æ°´å¹³ã€‚æ­¤å¤–ï¼ŒTASAåœ¨æ•ˆç‡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè®¡ç®—æˆæœ¬æ˜æ˜¾ä½äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ“ä½œã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¯†åˆ«åœºæ™¯ä¸­çš„å¯äº¤äº’åŒºåŸŸï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„æ“ä½œã€‚åœ¨è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸è™šæ‹Ÿç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œä»è€Œè·å¾—æ›´æ²‰æµ¸å¼çš„ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¸®åŠ©è½¦è¾†ç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶åšå‡ºç›¸åº”çš„å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

