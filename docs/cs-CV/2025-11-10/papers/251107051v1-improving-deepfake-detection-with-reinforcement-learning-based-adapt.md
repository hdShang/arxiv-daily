---
layout: default
title: Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation
---

# Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.07051" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.07051v1</a>
  <a href="https://arxiv.org/pdf/2511.07051.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07051v1" onclick="toggleFavorite(this, '2511.07051v1', 'Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuxuan Zhou, Tao Yu, Wen Huang, Yuheng Zhang, Tao Dai, Shu-Tao Xia

**åˆ†ç±»**: cs.CV, cs.CR

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æ•°æ®å¢å¼ºæ–¹æ³•CRDAï¼Œæå‡Deepfakeæ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Deepfakeæ£€æµ‹` `æ•°æ®å¢å¼º` `å¼ºåŒ–å­¦ä¹ ` `å› æœæ¨æ–­` `æ³›åŒ–èƒ½åŠ›` `å¯¹æŠ—æ ·æœ¬` `è‡ªé€‚åº”å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Deepfakeæ£€æµ‹å™¨ä¾èµ–å›ºå®šæ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹çœŸå®ä¸–ç•Œä¸­ä¸æ–­æ¼”å˜çš„ä¼ªé€ æŠ€æœ¯å¤æ‚æ€§ã€‚
2. CRDAæ¡†æ¶ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå› æœæ¨æ–­ï¼ŒåŠ¨æ€é€‰æ‹©å¢å¼ºåŠ¨ä½œå¹¶ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œå¼•å¯¼æ£€æµ‹å™¨å­¦ä¹ å¤šé¢†åŸŸä¼ªé€ ç‰¹å¾ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCRDAæ˜¾è‘—æå‡äº†æ£€æµ‹å™¨åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºå½“å‰æœ€ä¼˜æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›å¯¹äºå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚é€šè¿‡åˆæˆä¼ªé€ äººè„¸ç”Ÿæˆæ•°æ®å¢å¼ºå¯ä»¥æœ‰æ•ˆæå‡æ³›åŒ–èƒ½åŠ›ï¼Œä½†å½“å‰æœ€ä¼˜æ–¹æ³•ä¾èµ–äºå›ºå®šçš„ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå•ä¸€é™æ€å¢å¼ºæ˜¯å¦è¶³å¤Ÿï¼Ÿæˆ–è€…ä¼ªé€ ç‰¹å¾çš„å¤šæ ·æ€§æ˜¯å¦éœ€è¦åŠ¨æ€æ–¹æ³•ï¼Ÿç°æœ‰æ–¹æ³•å¿½ç•¥äº†çœŸå®ä¼ªé€ æŠ€æœ¯çš„å¤æ‚æ€§æ¼”å˜ï¼ˆä¾‹å¦‚ï¼Œé¢éƒ¨æ‰­æ›²ã€è¡¨æƒ…æ“çºµï¼‰ï¼Œè€Œå›ºå®šç­–ç•¥æ— æ³•å®Œå…¨æ¨¡æ‹Ÿè¿™äº›å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CRDAï¼ˆCurriculum Reinforcement-Learning Data Augmentationï¼‰ï¼Œä¸€ä¸ªå¼•å¯¼æ£€æµ‹å™¨é€æ­¥æŒæ¡ä»ç®€å•åˆ°å¤æ‚çš„å¤šé¢†åŸŸä¼ªé€ ç‰¹å¾çš„æ–°æ¡†æ¶ã€‚CRDAé€šè¿‡å¯é…ç½®çš„ä¼ªé€ æ“ä½œæ± åˆæˆå¢å¼ºæ ·æœ¬ï¼Œå¹¶åŠ¨æ€ç”Ÿæˆé’ˆå¯¹æ£€æµ‹å™¨å½“å‰å­¦ä¹ çŠ¶æ€çš„å¯¹æŠ—æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯æ•´åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå› æœæ¨æ–­ã€‚RLæ™ºèƒ½ä½“åŸºäºæ£€æµ‹å™¨æ€§èƒ½åŠ¨æ€é€‰æ‹©å¢å¼ºåŠ¨ä½œï¼Œä»¥æœ‰æ•ˆæ¢ç´¢å¹¿é˜”çš„å¢å¼ºç©ºé—´ï¼Œé€‚åº”æ—¥ç›Šå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¼ªé€ ã€‚åŒæ—¶ï¼Œæ™ºèƒ½ä½“å¼•å…¥åŠ¨ä½œç©ºé—´å˜åŒ–ä»¥ç”Ÿæˆå¼‚æ„ä¼ªé€ æ¨¡å¼ï¼Œå¹¶ç”±å› æœæ¨æ–­å¼•å¯¼ä»¥å‡è½»è™šå‡ç›¸å…³æ€§ï¼ŒæŠ‘åˆ¶ä¸ä»»åŠ¡æ— å…³çš„åå·®ï¼Œå¹¶ä¸“æ³¨äºå› æœä¸å˜ç‰¹å¾ã€‚è¿™ç§é›†æˆé€šè¿‡å°†åˆæˆå¢å¼ºæ¨¡å¼ä¸æ¨¡å‹å­¦ä¹ çš„è¡¨ç¤ºè§£è€¦ï¼Œç¡®ä¿äº†é²æ£’çš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºå¤šä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„SOTAæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Deepfakeæ£€æµ‹å™¨åœ¨é¢å¯¹çœŸå®åœºæ™¯æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚ä¸»è¦åŸå› æ˜¯ç°æœ‰æ•°æ®å¢å¼ºæ–¹æ³•é‡‡ç”¨å›ºå®šçš„ç­–ç•¥ï¼Œæ— æ³•æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­ä¼ªé€ æŠ€æœ¯çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œä¾‹å¦‚é¢éƒ¨æ‰­æ›²ã€è¡¨æƒ…æ“çºµç­‰ã€‚è¿™äº›å›ºå®šç­–ç•¥å®¹æ˜“å¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°ä¸ä»»åŠ¡æ— å…³çš„åå·®ï¼Œä»è€Œå½±å“å…¶åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åŠ¨æ€åœ°é€‰æ‹©æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¹¶ç»“åˆå› æœæ¨æ–­æ¥å‡å°‘æ¨¡å‹å­¦ä¹ åˆ°çš„è™šå‡ç›¸å…³æ€§ã€‚é€šè¿‡RLï¼Œå¯ä»¥æ ¹æ®æ£€æµ‹å™¨çš„å­¦ä¹ çŠ¶æ€ï¼Œè‡ªé€‚åº”åœ°ç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§çš„å¯¹æŠ—æ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚å› æœæ¨æ–­åˆ™ç”¨äºå¼•å¯¼æ¨¡å‹å…³æ³¨å› æœä¸å˜ç‰¹å¾ï¼ŒæŠ‘åˆ¶ä¸ä»»åŠ¡æ— å…³çš„åå·®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCRDAæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šä¼ªé€ æ“ä½œæ± ã€å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å’ŒDeepfakeæ£€æµ‹å™¨ã€‚ä¼ªé€ æ“ä½œæ± åŒ…å«å¤šç§ä¼ªé€ æŠ€æœ¯ï¼Œå¦‚é¢éƒ¨äº¤æ¢ã€è¡¨æƒ…æ“çºµç­‰ã€‚å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ ¹æ®æ£€æµ‹å™¨çš„æ€§èƒ½ï¼Œä»ä¼ªé€ æ“ä½œæ± ä¸­é€‰æ‹©åˆé€‚çš„å¢å¼ºåŠ¨ä½œï¼Œç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚Deepfakeæ£€æµ‹å™¨åˆ™åˆ©ç”¨è¿™äº›å¢å¼ºåçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚æ•´ä¸ªè¿‡ç¨‹æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£çš„è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“ä¸æ–­è°ƒæ•´å¢å¼ºç­–ç•¥ï¼Œæ£€æµ‹å™¨ä¸æ–­æé«˜æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šCRDAçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ å’Œå› æœæ¨æ–­ç»“åˆèµ·æ¥ï¼Œç”¨äºè‡ªé€‚åº”åœ°ç”Ÿæˆæ•°æ®å¢å¼ºæ ·æœ¬ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šæ•°æ®å¢å¼ºæ–¹æ³•ç›¸æ¯”ï¼ŒCRDAèƒ½å¤Ÿæ ¹æ®æ£€æµ‹å™¨çš„å­¦ä¹ çŠ¶æ€åŠ¨æ€åœ°è°ƒæ•´å¢å¼ºç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­ä¼ªé€ æŠ€æœ¯çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œå› æœæ¨æ–­çš„å¼•å…¥å¯ä»¥å‡å°‘æ¨¡å‹å­¦ä¹ åˆ°çš„è™šå‡ç›¸å…³æ€§ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCRDAä½¿ç”¨ä¸€ä¸ªåŸºäºç­–ç•¥æ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥è®­ç»ƒæ™ºèƒ½ä½“ã€‚æ™ºèƒ½ä½“çš„çŠ¶æ€æ˜¯æ£€æµ‹å™¨çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŠ¨ä½œæ˜¯ä»ä¼ªé€ æ“ä½œæ± ä¸­é€‰æ‹©çš„å¢å¼ºåŠ¨ä½œã€‚å¥–åŠ±å‡½æ•°æ ¹æ®æ£€æµ‹å™¨åœ¨å¢å¼ºæ•°æ®ä¸Šçš„æ€§èƒ½æ¥è®¾è®¡ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚å› æœæ¨æ–­é€šè¿‡å¼•å…¥å¹²é¢„å˜é‡æ¥å®ç°ï¼Œç”¨äºè¯†åˆ«å’Œæ¶ˆé™¤ä¸ä»»åŠ¡æ— å…³çš„åå·®ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ ¹æ®ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCRDAåœ¨å¤šä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†Deepfakeæ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨FaceForensics++æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨Celeb-DFæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æå‡äº†X%ï¼Œä¼˜äºå½“å‰æœ€ä¼˜æ–¹æ³•ã€‚æ¶ˆèå®éªŒéªŒè¯äº†å¼ºåŒ–å­¦ä¹ å’Œå› æœæ¨æ–­åœ¨æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸­çš„ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡Deepfakeæ£€æµ‹ç³»ç»Ÿåœ¨ç¤¾äº¤åª’ä½“ã€åœ¨çº¿è§†é¢‘å¹³å°ã€å®‰å…¨ç›‘æ§ç­‰é¢†åŸŸçš„æ€§èƒ½ï¼Œæœ‰æ•ˆè¯†åˆ«å’Œé˜²èŒƒæ¶æ„ä¼ªé€ å†…å®¹ï¼Œç»´æŠ¤ç½‘ç»œå®‰å…¨å’Œç¤¾ä¼šç¨³å®šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¯æ¨å¹¿åˆ°å…¶ä»–å¯¹æŠ—æ ·æœ¬ç”Ÿæˆå’Œæ¨¡å‹é²æ£’æ€§æå‡çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The generalization capability of deepfake detectors is critical for real-world use. Data augmentation via synthetic fake face generation effectively enhances generalization, yet current SoTA methods rely on fixed strategies-raising a key question: Is a single static augmentation sufficient, or does the diversity of forgery features demand dynamic approaches? We argue existing methods overlook the evolving complexity of real-world forgeries (e.g., facial warping, expression manipulation), which fixed policies cannot fully simulate. To address this, we propose CRDA (Curriculum Reinforcement-Learning Data Augmentation), a novel framework guiding detectors to progressively master multi-domain forgery features from simple to complex. CRDA synthesizes augmented samples via a configurable pool of forgery operations and dynamically generates adversarial samples tailored to the detector's current learning state. Central to our approach is integrating reinforcement learning (RL) and causal inference. An RL agent dynamically selects augmentation actions based on detector performance to efficiently explore the vast augmentation space, adapting to increasingly challenging forgeries. Simultaneously, the agent introduces action space variations to generate heterogeneous forgery patterns, guided by causal inference to mitigate spurious correlations-suppressing task-irrelevant biases and focusing on causally invariant features. This integration ensures robust generalization by decoupling synthetic augmentation patterns from the model's learned representations. Extensive experiments show our method significantly improves detector generalizability, outperforming SOTA methods across multiple cross-domain datasets.

