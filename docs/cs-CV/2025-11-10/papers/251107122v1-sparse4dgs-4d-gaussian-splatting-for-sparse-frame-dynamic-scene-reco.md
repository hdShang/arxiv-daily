---
layout: default
title: Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction
---

# Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.07122" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.07122v1</a>
  <a href="https://arxiv.org/pdf/2511.07122.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07122v1" onclick="toggleFavorite(this, '2511.07122v1', 'Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changyue Shi, Chuxiao Yang, Xinyuan Hu, Minghao Chen, Wenwen Pan, Yan Yang, Jiajun Ding, Zhou Yu, Jun Yu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

**å¤‡æ³¨**: AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Sparse4DGSï¼šæå‡ºçº¹ç†æ„ŸçŸ¥æ­£åˆ™åŒ–ä¸ä¼˜åŒ–ï¼Œè§£å†³ç¨€ç–å¸§åŠ¨æ€åœºæ™¯çš„4Dé«˜æ–¯é‡å»ºé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `åŠ¨æ€åœºæ™¯é‡å»º` `4Dé«˜æ–¯æº…å°„` `ç¨€ç–å¸§` `çº¹ç†æ„ŸçŸ¥` `å½¢å˜æ­£åˆ™åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŠ¨æ€é«˜æ–¯æº…å°„æ–¹æ³•ä¾èµ–å¯†é›†å¸§è§†é¢‘åºåˆ—ï¼Œåœ¨ç¨€ç–å¸§åœºæ™¯ä¸‹é‡å»ºæ•ˆæœä¸ä½³ï¼Œå°¤å…¶åœ¨çº¹ç†ä¸°å¯ŒåŒºåŸŸã€‚
2. Sparse4DGSé€šè¿‡çº¹ç†æ„ŸçŸ¥å½¢å˜æ­£åˆ™åŒ–å’Œçº¹ç†æ„ŸçŸ¥è§„èŒƒä¼˜åŒ–ï¼Œæå‡çº¹ç†ä¸°å¯ŒåŒºåŸŸçš„é‡å»ºè´¨é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSparse4DGSåœ¨ç¨€ç–å¸§è¾“å…¥ä¸‹ï¼Œä¼˜äºç°æœ‰åŠ¨æ€æˆ–å°‘æ ·æœ¬NeRFæ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—SOTAç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºSparse4DGSï¼Œä¸€ç§ç”¨äºç¨€ç–å¸§åŠ¨æ€åœºæ™¯é‡å»ºçš„é¦–åˆ›æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç¨€ç–å¸§æ¡ä»¶ä¸‹ï¼ŒåŠ¨æ€é‡å»ºæ–¹æ³•åœ¨è§„èŒƒç©ºé—´å’Œå½¢å˜ç©ºé—´å‡è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨çº¹ç†ä¸°å¯Œçš„åŒºåŸŸã€‚Sparse4DGSé€šè¿‡å…³æ³¨çº¹ç†ä¸°å¯Œçš„åŒºåŸŸæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚é’ˆå¯¹å½¢å˜ç½‘ç»œï¼Œæå‡ºäº†çº¹ç†æ„ŸçŸ¥å½¢å˜æ­£åˆ™åŒ–ï¼Œå¼•å…¥åŸºäºçº¹ç†çš„æ·±åº¦å¯¹é½æŸå¤±æ¥çº¦æŸé«˜æ–¯å½¢å˜ã€‚é’ˆå¯¹è§„èŒƒé«˜æ–¯åœºï¼Œå¼•å…¥äº†çº¹ç†æ„ŸçŸ¥è§„èŒƒä¼˜åŒ–ï¼Œå°†åŸºäºçº¹ç†çš„å™ªå£°èå…¥è§„èŒƒé«˜æ–¯çš„æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ä»¥ç¨€ç–å¸§ä½œä¸ºè¾“å…¥æ—¶ï¼Œè¯¥æ–¹æ³•åœ¨NeRF-Syntheticã€HyperNeRFã€NeRF-DSä»¥åŠiPhone-4Dæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„åŠ¨æ€æˆ–å°‘æ ·æœ¬æŠ€æœ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŠ¨æ€åœºæ™¯é‡å»ºæ–¹æ³•ä¾èµ–äºå¯†é›†çš„è§†é¢‘å¸§åºåˆ—ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç”±äºè®¾å¤‡é™åˆ¶ç­‰åŸå› ï¼Œå¾€å¾€åªèƒ½è·å–ç¨€ç–çš„å¸§ã€‚ç›´æ¥å°†ç°æœ‰æ–¹æ³•åº”ç”¨äºç¨€ç–å¸§ä¼šå¯¼è‡´é‡å»ºè´¨é‡æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨çº¹ç†ä¸°å¯Œçš„åŒºåŸŸï¼Œå› ä¸ºç¼ºä¹è¶³å¤Ÿçš„çº¦æŸä¿¡æ¯æ¥å‡†ç¡®ä¼°è®¡å½¢å˜å’Œè§„èŒƒç©ºé—´ä¸­çš„é«˜æ–¯å‚æ•°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSparse4DGSçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çº¹ç†ä¿¡æ¯æ¥æŒ‡å¯¼å½¢å˜å’Œè§„èŒƒç©ºé—´çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚é€šè¿‡å…³æ³¨çº¹ç†ä¸°å¯Œçš„åŒºåŸŸï¼Œå¹¶å¼•å…¥çº¹ç†æ„ŸçŸ¥çš„æ­£åˆ™åŒ–å’Œä¼˜åŒ–ç­–ç•¥ï¼Œæ¥å¼¥è¡¥ç¨€ç–å¸§å¸¦æ¥çš„ä¿¡æ¯ç¼ºå¤±ï¼Œä»è€Œæå‡é‡å»ºè´¨é‡ã€‚è¯¥æ–¹æ³•å‡è®¾çº¹ç†ä¸°å¯Œçš„åŒºåŸŸåŒ…å«æ›´å¤šçš„å‡ ä½•ä¿¡æ¯ï¼Œå› æ­¤åº”è¯¥æ›´åŠ å…³æ³¨è¿™äº›åŒºåŸŸçš„ä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSparse4DGSåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå½¢å˜ç½‘ç»œå’Œè§„èŒƒé«˜æ–¯åœºã€‚å½¢å˜ç½‘ç»œè´Ÿè´£å°†è§„èŒƒç©ºé—´ä¸­çš„é«˜æ–¯æ˜ å°„åˆ°è§‚å¯Ÿç©ºé—´ï¼Œè§„èŒƒé«˜æ–¯åœºåˆ™è´Ÿè´£è¡¨ç¤ºåœºæ™¯çš„é™æ€å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨ç¨€ç–å¸§ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å½¢å˜ç½‘ç»œé¢„æµ‹æ¯ä¸ªé«˜æ–¯çš„å½¢å˜ï¼›2ï¼‰åœ¨è§‚å¯Ÿç©ºé—´ä¸­æ¸²æŸ“å›¾åƒï¼Œå¹¶è®¡ç®—æ¸²æŸ“æŸå¤±ï¼›3ï¼‰ä½¿ç”¨çº¹ç†æ„ŸçŸ¥å½¢å˜æ­£åˆ™åŒ–æ¥çº¦æŸå½¢å˜ç½‘ç»œçš„å­¦ä¹ ï¼›4ï¼‰ä½¿ç”¨çº¹ç†æ„ŸçŸ¥è§„èŒƒä¼˜åŒ–æ¥æ›´æ–°è§„èŒƒé«˜æ–¯åœºçš„å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šSparse4DGSçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†çº¹ç†æ„ŸçŸ¥å½¢å˜æ­£åˆ™åŒ–å’Œçº¹ç†æ„ŸçŸ¥è§„èŒƒä¼˜åŒ–ã€‚çº¹ç†æ„ŸçŸ¥å½¢å˜æ­£åˆ™åŒ–é€šè¿‡å¼•å…¥åŸºäºçº¹ç†çš„æ·±åº¦å¯¹é½æŸå¤±ï¼Œæ¥çº¦æŸé«˜æ–¯å½¢å˜ï¼Œä»è€Œé¿å…è¿‡æ‹Ÿåˆã€‚çº¹ç†æ„ŸçŸ¥è§„èŒƒä¼˜åŒ–é€šè¿‡å°†åŸºäºçº¹ç†çš„å™ªå£°èå…¥è§„èŒƒé«˜æ–¯çš„æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­ï¼Œæ¥æå‡è§„èŒƒé«˜æ–¯åœºçš„é²æ£’æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSparse4DGSèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨çº¹ç†ä¿¡æ¯æ¥å¼¥è¡¥ç¨€ç–å¸§å¸¦æ¥çš„ä¿¡æ¯ç¼ºå¤±ã€‚

**å…³é”®è®¾è®¡**ï¼šçº¹ç†æ„ŸçŸ¥å½¢å˜æ­£åˆ™åŒ–ä¸­çš„æ·±åº¦å¯¹é½æŸå¤±åŸºäºçº¹ç†æ¢¯åº¦è®¡ç®—ï¼Œé¼“åŠ±å½¢å˜åçš„é«˜æ–¯æ·±åº¦ä¸ç›¸é‚»åƒç´ çš„æ·±åº¦ä¿æŒä¸€è‡´ã€‚çº¹ç†æ„ŸçŸ¥è§„èŒƒä¼˜åŒ–ä¸­ï¼Œçº¹ç†å™ªå£°çš„å¼ºåº¦ä¸çº¹ç†æ¢¯åº¦æˆæ­£æ¯”ï¼Œä½¿å¾—çº¹ç†ä¸°å¯Œçš„åŒºåŸŸèƒ½å¤Ÿè·å¾—æ›´å¤§çš„ä¼˜åŒ–åŠ›åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†è‡ªé€‚åº”å­¦ä¹ ç‡ç­–ç•¥ï¼Œæ ¹æ®çº¹ç†ä¿¡æ¯åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Sparse4DGSåœ¨NeRF-Syntheticã€HyperNeRFã€NeRF-DSå’ŒiPhone-4Dæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparse4DGSåœ¨ç¨€ç–å¸§æ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŠ¨æ€NeRFæ–¹æ³•å’Œå°‘æ ·æœ¬NeRFæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨iPhone-4Dæ•°æ®é›†ä¸Šï¼ŒSparse4DGSçš„PSNRæŒ‡æ ‡æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†2-3dBã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Sparse4DGSåœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥åˆ©ç”¨ç¨€ç–çš„ä¼ æ„Ÿå™¨æ•°æ®é‡å»ºåŠ¨æ€åœºæ™¯ï¼Œä»è€Œé™ä½å¯¹ç¡¬ä»¶è®¾å¤‡çš„è¦æ±‚ï¼Œå¹¶æé«˜ç³»ç»Ÿçš„é²æ£’æ€§å’Œå®æ—¶æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥ä½¿ç”¨Sparse4DGSæ¥é‡å»ºåŠ¨æ€ç¯å¢ƒï¼Œä»è€Œå¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒå¹¶åšå‡ºå†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.

