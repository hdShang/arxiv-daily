---
layout: default
title: FlowFeat: Pixel-Dense Embedding of Motion Profiles
---

# FlowFeat: Pixel-Dense Embedding of Motion Profiles

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.07696" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.07696v1</a>
  <a href="https://arxiv.org/pdf/2511.07696.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07696v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.07696v1', 'FlowFeat: Pixel-Dense Embedding of Motion Profiles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nikita Araslanov, Anna Sonnweber, Daniel Cremers

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

**å¤‡æ³¨**: Project website: https://tum-vision.github.io/flowfeat

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFlowFeatï¼Œé€šè¿‡è¿åŠ¨è½®å»“åµŒå…¥å®ç°åƒç´ çº§å¯†é›†å›¾åƒè¡¨å¾ï¼Œæå‡å¤šç§è§†è§‰ä»»åŠ¡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¯†é›†é¢„æµ‹` `å›¾åƒè¡¨å¾` `è¿åŠ¨è½®å»“` `è‡ªç›‘ç£å­¦ä¹ ` `å…‰æµä¼°è®¡` `è§†é¢‘åˆ†å‰²` `æ·±åº¦ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Transformerç­‰ç½‘ç»œäº§ç”Ÿä½åˆ†è¾¨ç‡ç‰¹å¾å›¾ï¼Œä¸é€‚ç”¨äºå¯†é›†é¢„æµ‹ä»»åŠ¡ï¼Œé™åˆ¶äº†è®¡ç®—æœºè§†è§‰åº”ç”¨ã€‚
2. FlowFeaté€šè¿‡è’¸é¦æŠ€æœ¯åµŒå…¥è¿åŠ¨è½®å»“åˆ†å¸ƒï¼Œåˆ©ç”¨å…‰æµç½‘ç»œå’Œè§†é¢‘æ•°æ®è¿›è¡Œè‡ªç›‘ç£è®­ç»ƒï¼Œç»Ÿè®¡è¿‘ä¼¼ apparent motionã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFlowFeatæ˜¾è‘—æå‡äº†å¤šç§ç¼–ç å™¨åœ¨è§†é¢‘åˆ†å‰²ã€æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä¸”è®¡ç®—æˆæœ¬ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜åˆ†è¾¨ç‡ã€å¤šä»»åŠ¡çš„ç‰¹å¾è¡¨ç¤ºæ–¹æ³•FlowFeatã€‚FlowFeatçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°é¢–çš„è’¸é¦æŠ€æœ¯ï¼Œå®ƒåµŒå…¥äº† plausible çš„ apparent motions åˆ†å¸ƒï¼Œå³è¿åŠ¨è½®å»“ã€‚é€šè¿‡åˆ©ç”¨å…‰æµç½‘ç»œå’Œå¤šæ ·åŒ–çš„è§†é¢‘æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæœ‰æ•ˆçš„è‡ªç›‘ç£è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ç»Ÿè®¡è¿‘ä¼¼ apparent motionã€‚å‡­å€Ÿå…¶å“è¶Šçš„ç©ºé—´ç»†èŠ‚æ°´å¹³ï¼ŒFlowFeat ç¼–ç äº†å¼•äººæ³¨ç›®çš„å‡ ä½•å’Œè¯­ä¹‰çº¿ç´¢ï¼ŒåŒæ—¶è¡¨ç°å‡ºé«˜åº¦çš„æ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFlowFeat æ˜¾è‘—å¢å¼ºäº†äº”ç§æœ€å…ˆè¿›ç¼–ç å™¨å’Œæ›¿ä»£ä¸Šé‡‡æ ·ç­–ç•¥åœ¨ä¸‰ä¸ªå¯†é›†ä»»åŠ¡ï¼ˆè§†é¢‘å¯¹è±¡åˆ†å‰²ã€å•ç›®æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ä¸­çš„è¡¨å¾èƒ½åŠ›ã€‚è®­ç»ƒ FlowFeat çš„è®¡ç®—æˆæœ¬ä½å»‰ï¼Œå¹¶ä¸”å¯¹ä¸å‡†ç¡®çš„å…‰æµä¼°è®¡å…·æœ‰é²æ£’æ€§ï¼Œå³ä½¿åœ¨ä½¿ç”¨æ— ç›‘ç£å…‰æµç½‘ç»œæ—¶ä»ç„¶éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬çš„å·¥ä½œæœç€å¯é ä¸”é€šç”¨çš„å¯†é›†å›¾åƒè¡¨ç¤ºè¿ˆå‡ºäº†ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºTransformerçš„å›¾åƒè¡¨å¾æ–¹æ³•é€šå¸¸ç”Ÿæˆä½åˆ†è¾¨ç‡çš„ç‰¹å¾å›¾ï¼Œè¿™å¯¹äºéœ€è¦åƒç´ çº§åˆ«ç²¾ç»†ä¿¡æ¯çš„å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡ç­‰ï¼‰æ¥è¯´æ˜¯ä¸å¤Ÿçš„ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥æ•æ‰å›¾åƒä¸­çš„ç»†èŠ‚ä¿¡æ¯å’Œç²¾ç¡®çš„å‡ ä½•ç»“æ„ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFlowFeatçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å­¦ä¹ å’ŒåµŒå…¥å›¾åƒä¸­åƒç´ çº§åˆ«çš„è¿åŠ¨ä¿¡æ¯ï¼ˆå³è¿åŠ¨è½®å»“ï¼‰ï¼Œæ¥å¢å¼ºå›¾åƒè¡¨å¾çš„ä¸°å¯Œæ€§å’Œç©ºé—´ç»†èŠ‚ã€‚é€šè¿‡å°†è¿åŠ¨ä¿¡æ¯ä½œä¸ºä¸€ç§é¢å¤–çš„ç‰¹å¾åµŒå…¥åˆ°å›¾åƒè¡¨å¾ä¸­ï¼ŒFlowFeatèƒ½å¤Ÿæä¾›æ›´ç²¾ç¡®çš„å‡ ä½•å’Œè¯­ä¹‰çº¿ç´¢ï¼Œä»è€Œæå‡å¯†é›†é¢„æµ‹ä»»åŠ¡çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº†å…‰æµä¼°è®¡çš„æ€æƒ³ï¼Œä½†ä¸æ˜¯ç›´æ¥ä½¿ç”¨å…‰æµï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªè¿åŠ¨åˆ†å¸ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFlowFeatçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨å…‰æµç½‘ç»œä¼°è®¡è§†é¢‘å¸§ä¹‹é—´çš„å…‰æµï¼›2) åŸºäºä¼°è®¡çš„å…‰æµï¼Œæ„å»ºè¿åŠ¨è½®å»“çš„åˆ†å¸ƒï¼›3) ä½¿ç”¨è’¸é¦æŠ€æœ¯å°†è¿åŠ¨è½®å»“çš„åˆ†å¸ƒåµŒå…¥åˆ°å›¾åƒç‰¹å¾ä¸­ï¼Œç”ŸæˆFlowFeatç‰¹å¾ï¼›4) å°†FlowFeatç‰¹å¾ä¸ç°æœ‰çš„å›¾åƒç¼–ç å™¨ï¼ˆå¦‚Transformerï¼‰çš„è¾“å‡ºè¿›è¡Œèåˆï¼Œå¾—åˆ°å¢å¼ºçš„å›¾åƒè¡¨å¾ï¼›5) ä½¿ç”¨å¢å¼ºçš„å›¾åƒè¡¨å¾è¿›è¡Œä¸‹æ¸¸çš„å¯†é›†é¢„æµ‹ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šFlowFeatçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è¿åŠ¨è½®å»“åµŒå…¥çš„è’¸é¦æŠ€æœ¯ã€‚ä¸ç›´æ¥ä½¿ç”¨å…‰æµä½œä¸ºç‰¹å¾ä¸åŒï¼ŒFlowFeatå­¦ä¹ ä¸€ä¸ªè¿åŠ¨åˆ†å¸ƒï¼Œè¿™ä½¿å¾—å®ƒå¯¹å…‰æµä¼°è®¡çš„è¯¯å·®æ›´åŠ é²æ£’ã€‚æ­¤å¤–ï¼ŒFlowFeaté‡‡ç”¨è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨çš„è¿åŠ¨æ•°æ®ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚é€šè¿‡å°†è¿åŠ¨ä¿¡æ¯åµŒå…¥åˆ°å›¾åƒç‰¹å¾ä¸­ï¼ŒFlowFeatèƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæå‡å¯†é›†é¢„æµ‹ä»»åŠ¡çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šFlowFeatä½¿ç”¨å…‰æµç½‘ç»œï¼ˆå¯ä»¥æ˜¯ç›‘ç£æˆ–æ— ç›‘ç£çš„ï¼‰æ¥ä¼°è®¡è§†é¢‘å¸§ä¹‹é—´çš„å…‰æµã€‚è¿åŠ¨è½®å»“çš„åˆ†å¸ƒå¯ä»¥é€šè¿‡å¯¹å…‰æµè¿›è¡Œç»Ÿè®¡åˆ†æå¾—åˆ°ã€‚è’¸é¦è¿‡ç¨‹ä½¿ç”¨ä¸€ä¸ªå°çš„ç¥ç»ç½‘ç»œæ¥å­¦ä¹ å¦‚ä½•å°†è¿åŠ¨è½®å»“åµŒå…¥åˆ°å›¾åƒç‰¹å¾ä¸­ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ä¸€ä¸ªé‡æ„æŸå¤±å’Œä¸€ä¸ªå¯¹æ¯”æŸå¤±ï¼Œç”¨äºä¿è¯åµŒå…¥çš„è¿åŠ¨ä¿¡æ¯èƒ½å¤Ÿå‡†ç¡®åœ°é‡æ„å…‰æµï¼Œå¹¶ä¸”èƒ½å¤ŸåŒºåˆ†ä¸åŒçš„è¿åŠ¨æ¨¡å¼ã€‚å…·ä½“å‚æ•°è®¾ç½®å–å†³äºæ‰€ä½¿ç”¨çš„å…‰æµç½‘ç»œå’Œå›¾åƒç¼–ç å™¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowFeatèƒ½å¤Ÿæ˜¾è‘—æå‡ç°æœ‰å›¾åƒç¼–ç å™¨åœ¨è§†é¢‘å¯¹è±¡åˆ†å‰²ã€å•ç›®æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘å¯¹è±¡åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒFlowFeatèƒ½å¤Ÿå°†æ€§èƒ½æå‡5%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒFlowFeatå¯¹å…‰æµä¼°è®¡çš„è¯¯å·®å…·æœ‰é²æ£’æ€§ï¼Œå³ä½¿ä½¿ç”¨æ— ç›‘ç£å…‰æµç½‘ç»œä¹Ÿèƒ½å–å¾—è‰¯å¥½çš„æ•ˆæœã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFlowFeatæ˜¯ä¸€ç§æœ‰æ•ˆä¸”é€šç”¨çš„å›¾åƒè¡¨å¾æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FlowFeatåœ¨è§†é¢‘å¯¹è±¡åˆ†å‰²ã€å•ç›®æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€è§†é¢‘ç›‘æ§ç­‰é¢†åŸŸï¼Œæé«˜è¿™äº›åº”ç”¨å¯¹ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›å’Œç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒFlowFeatçš„è‡ªç›‘ç£è®­ç»ƒæ–¹å¼ä½¿å…¶æ˜“äºæ‰©å±•åˆ°æ–°çš„æ•°æ®é›†å’Œä»»åŠ¡ä¸­ï¼Œå…·æœ‰å¾ˆé«˜çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dense and versatile image representations underpin the success of virtually all computer vision applications. However, state-of-the-art networks, such as transformers, produce low-resolution feature grids, which are suboptimal for dense prediction tasks. To address this limitation, we present FlowFeat, a high-resolution and multi-task feature representation. The key ingredient behind FlowFeat is a novel distillation technique that embeds a distribution of plausible apparent motions, or motion profiles. By leveraging optical flow networks and diverse video data, we develop an effective self-supervised training framework that statistically approximates the apparent motion. With its remarkable level of spatial detail, FlowFeat encodes a compelling degree of geometric and semantic cues while exhibiting high temporal consistency. Empirically, FlowFeat significantly enhances the representational power of five state-of-the-art encoders and alternative upsampling strategies across three dense tasks: video object segmentation, monocular depth estimation and semantic segmentation. Training FlowFeat is computationally inexpensive and robust to inaccurate flow estimation, remaining highly effective even when using unsupervised flow networks. Our work takes a step forward towards reliable and versatile dense image representations.

