---
layout: default
title: Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning
---

# Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.06958" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.06958v2</a>
  <a href="https://arxiv.org/pdf/2511.06958.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.06958v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.06958v2', 'Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Raneen Younis, Louay Hamdi, Lukas Chavez, Zahra Ahmadi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10 (æ›´æ–°: 2025-11-19)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**WISE-MAEï¼šä¸€ç§åŸºäºå°æ³¢å˜æ¢çš„åŒé˜¶æ®µæ©ç è‡ªç¼–ç å™¨ï¼Œç”¨äºç—…ç†å›¾åƒè¡¨å¾å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç—…ç†å›¾åƒåˆ†æ` `è‡ªç›‘ç£å­¦ä¹ ` `æ©ç è‡ªç¼–ç å™¨` `å°æ³¢å˜æ¢` `è¡¨å¾å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸMAEåœ¨ç—…ç†å›¾åƒåˆ†æä¸­éšæœºé‡‡æ ·patchï¼Œå¿½ç•¥äº†ç»„ç»‡ç»“æ„çš„ç”Ÿç‰©å­¦æ„ä¹‰ï¼Œå¯¼è‡´æ¨¡å‹å­¦ä¹ æ•ˆç‡é™ä½ã€‚
2. WISE-MAEé€šè¿‡å°æ³¢å˜æ¢ç­›é€‰ä¿¡æ¯ä¸°å¯Œçš„patchï¼Œæ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹ï¼Œæå‡æ¨¡å‹å¯¹å…³é”®ç»„ç»‡ç»“æ„çš„å…³æ³¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒWISE-MAEåœ¨å¤šä¸ªç™Œç—‡æ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨å¾è´¨é‡å’Œä¸‹æ¸¸åˆ†ç±»æ€§èƒ½ï¼Œä¸”ä¿æŒäº†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…¨åˆ‡ç‰‡å›¾åƒåœ¨æ•°å­—ç—…ç†å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶æç«¯å°ºå¯¸å’Œç¨€ç¼ºçš„æ ‡æ³¨ä½¿å¾—è‡ªç›‘ç£å­¦ä¹ æˆä¸ºå¿…è¦ã€‚å¸¦æœ‰Vision Transformeréª¨å¹²ç½‘ç»œçš„æ©ç è‡ªç¼–ç å™¨(MAE)æœ€è¿‘åœ¨ç»„ç»‡ç—…ç†å­¦è¡¨å¾å­¦ä¹ ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„MAEé¢„è®­ç»ƒæœŸé—´çš„éšæœºpatché‡‡æ ·é€šå¸¸åŒ…æ‹¬ä¸ç›¸å…³æˆ–å™ªå£°åŒºåŸŸï¼Œé™åˆ¶äº†æ¨¡å‹æ•è·æœ‰æ„ä¹‰çš„ç»„ç»‡æ¨¡å¼çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„ã€é¢†åŸŸè‡ªé€‚åº”çš„æ¡†æ¶ï¼Œé€šè¿‡å°æ³¢ä¿¡æ¯patché€‰æ‹©ç­–ç•¥ï¼Œå°†ç»“æ„å’Œç”Ÿç‰©å­¦ç›¸å…³æ€§å¼•å…¥åˆ°åŸºäºMAEçš„å­¦ä¹ ä¸­ã€‚WISE-MAEåº”ç”¨ä¸€ä¸ªä¸¤æ­¥çš„ç”±ç²—åˆ°ç²¾çš„è¿‡ç¨‹ï¼šåœ¨ä½æ”¾å¤§å€ç‡ä¸‹è¿›è¡ŒåŸºäºå°æ³¢çš„ç­›é€‰ä»¥å®šä½ç»“æ„ä¸°å¯Œçš„åŒºåŸŸï¼Œç„¶åè¿›è¡Œé«˜åˆ†è¾¨ç‡æå–ä»¥è¿›è¡Œè¯¦ç»†å»ºæ¨¡ã€‚è¿™ç§æ–¹æ³•æ¨¡ä»¿äº†ç—…ç†å­¦å®¶çš„è¯Šæ–­å·¥ä½œæµç¨‹ï¼Œå¹¶æé«˜äº†å­¦ä¹ è¡¨å¾çš„è´¨é‡ã€‚åœ¨åŒ…æ‹¬è‚ºã€è‚¾å’Œç»“ç›´è‚ ç»„ç»‡åœ¨å†…çš„å¤šä¸ªç™Œç—‡æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒWISE-MAEåœ¨å¼±ç›‘ç£ä¸‹å®ç°äº†æœ‰ç«äº‰åŠ›çš„è¡¨å¾è´¨é‡å’Œä¸‹æ¸¸åˆ†ç±»æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå…¨åˆ‡ç‰‡ç—…ç†å›¾åƒå°ºå¯¸å·¨å¤§ï¼Œä¸”æ ‡æ³¨ç¨€ç¼ºï¼Œä¾èµ–äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚ç°æœ‰çš„åŸºäºMAEçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µé€šå¸¸é‡‡ç”¨éšæœºpatché‡‡æ ·ç­–ç•¥ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°å¤§é‡ä¸ç›¸å…³çš„å™ªå£°åŒºåŸŸï¼Œé™ä½äº†å­¦ä¹ æ•ˆç‡å’Œè¡¨å¾è´¨é‡ã€‚å› æ­¤ï¼Œå¦‚ä½•é€‰æ‹©ä¿¡æ¯é‡å¤§çš„patchè¿›è¡Œé¢„è®­ç»ƒï¼Œæ˜¯æå‡ç—…ç†å›¾åƒè¡¨å¾å­¦ä¹ çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šWISE-MAEçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å°æ³¢å˜æ¢å¯¹ä½åˆ†è¾¨ç‡å›¾åƒè¿›è¡Œåˆ†æï¼Œç­›é€‰å‡ºåŒ…å«ä¸°å¯Œç»„ç»‡ç»“æ„çš„åŒºåŸŸï¼Œç„¶ååœ¨è¿™äº›åŒºåŸŸæå–é«˜åˆ†è¾¨ç‡patchè¿›è¡ŒMAEé¢„è®­ç»ƒã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†ç—…ç†åŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹ï¼Œå³å…ˆåœ¨ä½å€é•œä¸‹è§‚å¯Ÿæ•´ä½“ç»“æ„ï¼Œå†åœ¨é«˜å€é•œä¸‹è§‚å¯Ÿç»†èŠ‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWISE-MAEåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) åŸºäºå°æ³¢å˜æ¢çš„patchç­›é€‰é˜¶æ®µï¼šé¦–å…ˆå°†å…¨åˆ‡ç‰‡å›¾åƒç¼©æ”¾åˆ°ä½åˆ†è¾¨ç‡ï¼Œç„¶åä½¿ç”¨å°æ³¢å˜æ¢æå–å›¾åƒçš„é¢‘ç‡ä¿¡æ¯ï¼Œæ ¹æ®é¢‘ç‡ä¿¡æ¯ç­›é€‰å‡ºåŒ…å«ä¸°å¯Œç»„ç»‡ç»“æ„çš„åŒºåŸŸã€‚2) åŸºäºMAEçš„é¢„è®­ç»ƒé˜¶æ®µï¼šåœ¨ç­›é€‰å‡ºçš„åŒºåŸŸæå–é«˜åˆ†è¾¨ç‡patchï¼Œç„¶åä½¿ç”¨MAEè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒã€‚MAEé‡‡ç”¨Vision Transformerä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œé€šè¿‡maskéƒ¨åˆ†patchå¹¶é¢„æµ‹è¢«maskçš„patchæ¥å­¦ä¹ å›¾åƒè¡¨å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šWISE-MAEçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºå°æ³¢å˜æ¢çš„patchç­›é€‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°é€‰æ‹©åŒ…å«ä¸°å¯Œç»„ç»‡ç»“æ„çš„patchï¼Œä»è€Œæé«˜MAEçš„é¢„è®­ç»ƒæ•ˆç‡å’Œè¡¨å¾è´¨é‡ã€‚ä¸ä¼ ç»Ÿçš„éšæœºpatché‡‡æ ·ç›¸æ¯”ï¼ŒWISE-MAEèƒ½å¤Ÿæ›´åŠ å…³æ³¨ç—…ç†å›¾åƒä¸­çš„å…³é”®åŒºåŸŸï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å…·ç”Ÿç‰©å­¦æ„ä¹‰çš„è¡¨å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å°æ³¢å˜æ¢æ–¹é¢ï¼Œè®ºæ–‡é€‰æ‹©äº†åˆé€‚çš„æ¯å°æ³¢å’Œå°æ³¢åˆ†è§£å±‚æ•°ï¼Œä»¥æå–ä¸åŒå°ºåº¦çš„é¢‘ç‡ä¿¡æ¯ã€‚åœ¨MAEçš„é¢„è®­ç»ƒæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†æ ‡å‡†çš„MAEè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬maskæ¯”ä¾‹ã€æŸå¤±å‡½æ•°ç­‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹ç­›é€‰å‡ºçš„patchæ•°é‡è¿›è¡Œäº†æ§åˆ¶ï¼Œä»¥ä¿è¯é¢„è®­ç»ƒçš„æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

WISE-MAEåœ¨è‚ºç™Œã€è‚¾ç™Œå’Œç»“ç›´è‚ ç™Œç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒWISE-MAEåœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æ•°æ®é›†ä¸Šè¶…è¿‡äº†å…¶ä»–è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨è‚ºç™Œæ•°æ®é›†ä¸Šï¼ŒWISE-MAEçš„åˆ†ç±»å‡†ç¡®ç‡ç›¸æ¯”äºéšæœºpatché‡‡æ ·çš„MAEæé«˜äº†2-3ä¸ªç™¾åˆ†ç‚¹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

WISE-MAEå¯åº”ç”¨äºå¤šç§ç—…ç†å›¾åƒåˆ†æä»»åŠ¡ï¼Œä¾‹å¦‚ç™Œç—‡è¯Šæ–­ã€é¢„åé¢„æµ‹ã€åˆ†å­äºšå‹åˆ†ç±»ç­‰ã€‚é€šè¿‡å­¦ä¹ é«˜è´¨é‡çš„ç—…ç†å›¾åƒè¡¨å¾ï¼Œå¯ä»¥æé«˜è¿™äº›ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè¾…åŠ©ç—…ç†åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­å’Œæ²»ç–—å†³ç­–ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸï¼Œä¾‹å¦‚æ”¾å°„å½±åƒå­¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Whole-slide images are central to digital pathology, yet their extreme size and scarce annotations make self-supervised learning essential. Masked Autoencoders (MAEs) with Vision Transformer backbones have recently shown strong potential for histopathology representation learning. However, conventional random patch sampling during MAE pretraining often includes irrelevant or noisy regions, limiting the model's ability to capture meaningful tissue patterns. In this paper, we present a lightweight and domain-adapted framework that brings structure and biological relevance into MAE-based learning through a wavelet-informed patch selection strategy. WISE-MAE applies a two-step coarse-to-fine process: wavelet-based screening at low magnification to locate structurally rich regions, followed by high-resolution extraction for detailed modeling. This approach mirrors the diagnostic workflow of pathologists and improves the quality of learned representations. Evaluations across multiple cancer datasets, including lung, renal, and colorectal tissues, show that WISE-MAE achieves competitive representation quality and downstream classification performance while maintaining efficiency under weak supervision.

