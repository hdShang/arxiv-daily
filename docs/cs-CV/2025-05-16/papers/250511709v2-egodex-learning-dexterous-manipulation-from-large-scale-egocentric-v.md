---
layout: default
title: EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video
---

# EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11709" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11709v2</a>
  <a href="https://arxiv.org/pdf/2505.11709.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11709v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11709v2', 'EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ryan Hoque, Peide Huang, David J. Yoon, Mouli Sivapurapu, Jian Zhang

**åˆ†ç±»**: cs.CV, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-16 (æ›´æ–°: 2025-08-20)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/apple/ml-egodex)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEgoDexä»¥è§£å†³çµå·§æ“ä½œæ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çµå·§æ“ä½œ` `æ¨¡ä»¿å­¦ä¹ ` `ç¬¬ä¸€äººç§°è§†é¢‘` `æ•°æ®é›†æ„å»º` `æ‰‹éƒ¨è¿½è¸ª` `æœºå™¨äººæŠ€æœ¯` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çµå·§æ“ä½œæ•°æ®é›†ç¨€ç¼ºï¼Œå°¤å…¶ç¼ºä¹å¤§è§„æ¨¡çš„å¸¦æœ‰æ‰‹éƒ¨å§¿æ€æ ‡æ³¨çš„æ•°æ®ï¼Œé™åˆ¶äº†æ¨¡ä»¿å­¦ä¹ çš„è¿›å±•ã€‚
2. è®ºæ–‡æå‡ºäº†EgoDexæ•°æ®é›†ï¼Œåˆ©ç”¨Apple Vision Proæ”¶é›†829å°æ—¶çš„ç¬¬ä¸€äººç§°è§†é¢‘å’Œ3Dæ‰‹éƒ¨è¿½è¸ªæ•°æ®ï¼Œè¦†ç›–å¤šç§æ—¥å¸¸æ“ä½œä»»åŠ¡ã€‚
3. é€šè¿‡åœ¨EgoDexä¸Šè®­ç»ƒæ¨¡ä»¿å­¦ä¹ ç­–ç•¥ï¼Œç ”ç©¶è€…å¼•å…¥äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ¨åŠ¨äº†æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ åœ¨æ“ä½œä»»åŠ¡ä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨çµå·§æ“ä½œé¢†åŸŸã€‚ç°æœ‰çš„å¤§è§„æ¨¡æ•°æ®é›†å¦‚Ego4Dç¼ºä¹æ‰‹éƒ¨å§¿æ€æ ‡æ³¨ä¸”ä¸ä¸“æ³¨äºç‰©ä½“æ“ä½œã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä½¿ç”¨Apple Vision Proæ”¶é›†äº†EgoDexï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„çµå·§äººç±»æ“ä½œæ•°æ®é›†ï¼ŒåŒ…å«829å°æ—¶çš„ç¬¬ä¸€äººç§°è§†é¢‘å’Œå®æ—¶3Dæ‰‹æŒ‡è¿½è¸ªæ•°æ®ã€‚è¯¥æ•°æ®é›†æ¶µç›–194ç§æ—¥å¸¸æ¡Œé¢ä»»åŠ¡ï¼Œç ”ç©¶è€…è¿˜åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒå’Œè¯„ä¼°äº†æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹çš„æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ï¼Œæå‡ºäº†è¯„ä¼°è¿›å±•çš„æŒ‡æ ‡å’ŒåŸºå‡†ã€‚é€šè¿‡å‘å¸ƒè¿™ä¸€å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨äººã€è®¡ç®—æœºè§†è§‰å’ŒåŸºç¡€æ¨¡å‹çš„å‰æ²¿å‘å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³çµå·§æ“ä½œé¢†åŸŸæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚Ego4Dç¼ºä¹æ‰‹éƒ¨å§¿æ€æ ‡æ³¨ï¼Œä¸”ä¸ä¸“æ³¨äºç‰©ä½“æ“ä½œï¼Œé™åˆ¶äº†æ¨¡ä»¿å­¦ä¹ çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Apple Vision Proæ”¶é›†å¤§è§„æ¨¡çš„ç¬¬ä¸€äººç§°è§†é¢‘æ•°æ®ï¼Œå¹¶åŒæ—¶è·å–3Dæ‰‹éƒ¨è¿½è¸ªæ•°æ®ï¼Œä»¥æ„å»ºä¸€ä¸ªä¸°å¯Œçš„çµå·§æ“ä½œæ•°æ®é›†EgoDexã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ•°æ®çš„æ”¶é›†æ›´åŠ ç²¾å‡†å’Œé«˜æ•ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEgoDexæ•°æ®é›†çš„æ„å»ºåŒ…æ‹¬å¤šä¸ªæ ¡å‡†æ‘„åƒå¤´çš„ä½¿ç”¨å’Œè®¾å¤‡å†…SLAMæŠ€æœ¯ï¼Œä»¥ç²¾ç¡®è¿½è¸ªæ¯ä¸ªæ‰‹æŒ‡å…³èŠ‚çš„å§¿æ€ã€‚æ•°æ®é›†æ¶µç›–194ç§ä¸åŒçš„æ¡Œé¢ä»»åŠ¡ï¼Œç¡®ä¿äº†å¤šæ ·æ€§å’Œå®ç”¨æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šEgoDexæ˜¯å½“å‰æœ€å¤§çš„çµå·§æ“ä½œæ•°æ®é›†ï¼Œç»“åˆäº†å®æ—¶æ‰‹éƒ¨è¿½è¸ªå’Œä¸°å¯Œçš„æ“ä½œåœºæ™¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡ä»¿å­¦ä¹ çš„ç ”ç©¶åŸºç¡€ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEgoDexæä¾›äº†æ›´ä¸ºå…¨é¢å’Œç²¾ç¡®çš„æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ‘„åƒå¤´ç³»ç»Ÿå’ŒSLAMæŠ€æœ¯ï¼Œç¡®ä¿äº†æ•°æ®çš„é«˜è´¨é‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜è®¾è®¡äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†ï¼Œä»¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°æ¨¡ä»¿å­¦ä¹ ç­–ç•¥çš„æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨EgoDexæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œæ¨¡ä»¿å­¦ä¹ ç­–ç•¥åœ¨æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡äº†ç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EgoDexæ•°æ®é›†çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æœºå™¨äººæ“ä½œã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ä¸­çš„äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›ä¸°å¯Œçš„çµå·§æ“ä½œæ•°æ®ï¼ŒEgoDexèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶è€…å’Œå¼€å‘è€…æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning for manipulation has a well-known data scarcity problem. Unlike natural language and 2D computer vision, there is no Internet-scale corpus of data for dexterous manipulation. One appealing option is egocentric human video, a passively scalable data source. However, existing large-scale datasets such as Ego4D do not have native hand pose annotations and do not focus on object manipulation. To this end, we use Apple Vision Pro to collect EgoDex: the largest and most diverse dataset of dexterous human manipulation to date. EgoDex has 829 hours of egocentric video with paired 3D hand and finger tracking data collected at the time of recording, where multiple calibrated cameras and on-device SLAM can be used to precisely track the pose of every joint of each hand. The dataset covers a wide range of diverse manipulation behaviors with everyday household objects in 194 different tabletop tasks ranging from tying shoelaces to folding laundry. Furthermore, we train and systematically evaluate imitation learning policies for hand trajectory prediction on the dataset, introducing metrics and benchmarks for measuring progress in this increasingly important area. By releasing this large-scale dataset, we hope to push the frontier of robotics, computer vision, and foundation models. EgoDex is publicly available for download at https://github.com/apple/ml-egodex.

