---
layout: default
title: Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation
---

# Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11383" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11383v1</a>
  <a href="https://arxiv.org/pdf/2505.11383.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11383v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11383v1', 'Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihan Wang, Seungjun Lee, Gim Hee Lee

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDynam3Dä»¥è§£å†³3Då¯¼èˆªä¸­çš„åŠ¨æ€ç¯å¢ƒé€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰ä¸è¯­è¨€å¯¼èˆª` `åŠ¨æ€ç¯å¢ƒé€‚åº”` `3Dè¡¨ç¤º` `é•¿æ—¶é—´è®°å¿†` `æœºå™¨äººå¯¼èˆª` `å¤šæ¨¡æ€å­¦ä¹ ` `ç©ºé—´è¯­ä¹‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘è¯­è¨€å¤§æ¨¡å‹åœ¨3Då¯¼èˆªä¸­é¢ä¸´3Då‡ ä½•ç†è§£ä¸è¶³å’ŒåŠ¨æ€ç¯å¢ƒé€‚åº”å·®ç­‰æŒ‘æˆ˜ã€‚
2. Dynam3Dé€šè¿‡åŠ¨æ€åˆ†å±‚3Dè¡¨ç¤ºï¼Œç»“åˆè¯­è¨€å¯¹é½çš„ç‰¹å¾ï¼Œæå‡äº†3Då‡ ä½•å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚
3. åœ¨R2R-CEã€REVERIE-CEå’ŒNavRAG-CEç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDynam3Då®ç°äº†æ–°çš„æ€§èƒ½è®°å½•ï¼ŒéªŒè¯äº†å…¶å®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ˜¯ä¸€ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼Œæ¶‰åŠæ™ºèƒ½ä½“æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨3Dç¯å¢ƒä¸­å¯¼èˆªã€‚å°½ç®¡è§†é¢‘è¯­è¨€å¤§æ¨¡å‹ï¼ˆVideo-VLMsï¼‰åœ¨VLNä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»é¢ä¸´3Då‡ ä½•ç†è§£ä¸è¶³ã€ç¯å¢ƒè®°å¿†æœ‰é™åŠåŠ¨æ€ç¯å¢ƒé€‚åº”å·®ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºDynam3Dï¼Œä¸€ä¸ªåŠ¨æ€åˆ†å±‚3Dè¡¨ç¤ºæ¨¡å‹ï¼Œé€šè¿‡è¯­è¨€å¯¹é½çš„å±‚æ¬¡åŒ–3Dè¡¨ç¤ºæ¥è®­ç»ƒ3D-VLMè¿›è¡Œå¯¼èˆªåŠ¨ä½œé¢„æµ‹ã€‚Dynam3Dèƒ½å¤Ÿåœ¨çº¿ç¼–ç å’Œå®šä½3Då®ä¾‹ï¼Œå¹¶åœ¨å˜åŒ–çš„ç¯å¢ƒä¸­åŠ¨æ€æ›´æ–°ï¼Œä»è€Œå®ç°å¤§è§„æ¨¡æ¢ç´¢å’Œé•¿æœŸè®°å¿†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynam3Dåœ¨å¤šä¸ªVLNåŸºå‡†æµ‹è¯•ä¸­è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨å®é™…éƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘è¯­è¨€å¤§æ¨¡å‹åœ¨3Då¯¼èˆªä»»åŠ¡ä¸­å¯¹3Då‡ ä½•å’Œç©ºé—´è¯­ä¹‰ç†è§£ä¸è¶³ã€ç¯å¢ƒè®°å¿†èƒ½åŠ›æœ‰é™åŠåŠ¨æ€ç¯å¢ƒé€‚åº”æ€§å·®çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDynam3Dé€šè¿‡æ„å»ºåŠ¨æ€åˆ†å±‚3Dè¡¨ç¤ºï¼Œåˆ©ç”¨è¯­è¨€å¯¹é½çš„ç‰¹å¾æ¥å¢å¼º3D-VLMçš„å¯¼èˆªåŠ¨ä½œé¢„æµ‹èƒ½åŠ›ï¼Œæ—¨åœ¨å®ç°æ›´å¥½çš„ç¯å¢ƒç†è§£å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDynam3Dçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆï¼Œè¾“å…¥RGB-Då›¾åƒï¼Œæ¥ç€å°†2D CLIPç‰¹å¾æŠ•å½±åˆ°3Dç©ºé—´ï¼Œæ„å»ºå¤šå±‚æ¬¡çš„3Dè¡¥ä¸-å®ä¾‹-åŒºåŸŸè¡¨ç¤ºï¼Œæœ€åé€šè¿‡åŠ¨æ€æ›´æ–°ç­–ç•¥å®ç°åœ¨çº¿ç¼–ç å’Œå®šä½ã€‚

**å…³é”®åˆ›æ–°**ï¼šDynam3Dçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŠ¨æ€åˆ†å±‚è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å˜åŒ–ç¯å¢ƒä¸­å®æ—¶æ›´æ–°3Då®ä¾‹ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†å¯¹åŠ¨æ€ç¯å¢ƒçš„é€‚åº”èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒDynam3Dé‡‡ç”¨äº†å±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤ºå’ŒåŠ¨æ€æ›´æ–°ç­–ç•¥ï¼Œç¡®ä¿äº†å¯¹å¤§è§„æ¨¡ç¯å¢ƒçš„æ¢ç´¢å’Œé•¿æœŸè®°å¿†èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”å¯¼èˆªä»»åŠ¡çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªè§†è§‰ä¸è¯­è¨€å¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDynam3Då®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå…·ä½“åœ¨R2R-CEã€REVERIE-CEå’ŒNavRAG-CEæµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Dynam3Dçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰ã€‚é€šè¿‡æå‡æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿä¸ºå®é™…åº”ç”¨æä¾›æ›´é«˜çš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„éƒ¨ç½²ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment.

