---
layout: default
title: Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs
---

# Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04142" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.04142v1</a>
  <a href="https://arxiv.org/pdf/2510.04142.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04142v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.04142v1', 'Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyu Yang, Jie Lu, En Yu

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¦‚å¿µå¯¹é½çš„è‡ªä¸»è’¸é¦æ–¹æ³•ï¼Œè§£å†³å¤šæ¼‚ç§»MLLMçš„çŸ¥è¯†è’¸é¦é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¦‚å¿µæ¼‚ç§»` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªä¸»å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMçŸ¥è¯†è’¸é¦æ–¹æ³•å¿½ç•¥äº†å¤šæ•™å¸ˆæ¨¡å‹æ¨ç†è½¨è¿¹çš„æ¦‚å¿µæ¼‚ç§»é—®é¢˜ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. æå‡ºâ€œå­¦ä¹ ã€æ¯”è¾ƒã€æ‰¹åˆ¤â€èŒƒå¼ï¼Œé€šè¿‡è‡ªä¸»åå¥½ä¼˜åŒ–ï¼ˆAPOï¼‰å®ç°æ¦‚å¿µå¯¹é½ï¼Œè§£å†³æ•™å¸ˆæ¨¡å‹æ¼‚ç§»é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸€è‡´æ€§ã€é²æ£’æ€§å’Œæ³›åŒ–æ€§æ–¹é¢ä¼˜äºç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†CXR-MAXã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ—¨åœ¨è§£å†³ä»å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è’¸é¦æ—¶çš„ä¸€ä¸ªå…³é”®ä½†æœªè¢«å……åˆ†æ¢ç´¢çš„æŒ‘æˆ˜ï¼šå¤šä¸ªæ¼‚ç§»æ•™å¸ˆæ¨¡å‹äº§ç”Ÿçš„æ¨ç†è½¨è¿¹è¡¨ç°å‡ºæ¦‚å¿µæ¼‚ç§»ï¼Œå…¶æ¨ç†åˆ†å¸ƒä¸å¯é¢„æµ‹åœ°æ¼”å˜ï¼Œå¹¶å°†åå·®ä¼ é€’ç»™å­¦ç”Ÿæ¨¡å‹ï¼Œæœ€ç»ˆæŸå®³å…¶æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç‡å…ˆå»ºç«‹äº†æ¦‚å¿µæ¼‚ç§»å’ŒçŸ¥è¯†è’¸é¦ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œå°†æ¥è‡ªå¤šä¸ªMLLMæ•™å¸ˆçš„éå¹³ç¨³æ¨ç†åŠ¨æ€è§†ä¸ºå¤šæµæ¨ç†è½¨è¿¹çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹ã€‚åœ¨æ¦‚å¿µæ¼‚ç§»çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œå­¦ä¹ ã€æ¯”è¾ƒã€æ‰¹åˆ¤â€èŒƒå¼ï¼Œæœ€ç»ˆå®ç°äº†è‡ªä¸»åå¥½ä¼˜åŒ–ï¼ˆAPOï¼‰ã€‚åœ¨æ•™å¸ˆçš„ç§¯ææŒ‡å¯¼ä¸‹ï¼Œå­¦ç”Ÿæ¨¡å‹é¦–å…ˆé€šè¿‡æ¯”è¾ƒå¤šä¸ªæ•™å¸ˆæ¥å­¦ä¹ å’Œè‡ªæˆ‘æç‚¼é¦–é€‰çš„æ€è€ƒæ–¹å¼ã€‚ç„¶åï¼Œå®ƒå¯¹æ•™å¸ˆçš„æ¼‚ç§»æ¨ç†è¿›è¡Œæ‰¹åˆ¤æ€§åæ€ï¼Œé€šè¿‡APOæ‰§è¡Œæ¦‚å¿µå¯¹é½ï¼Œæœ€ç»ˆäº§ç”Ÿä¸€ä¸ªé²æ£’ã€ä¸€è‡´å’Œå¯æ³›åŒ–çš„æ¨¡å‹ã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬åœ¨çŸ¥è¯†è’¸é¦ä¸­ä¸€è‡´æ€§ã€é²æ£’æ€§å’Œæ³›åŒ–æ€§çš„ä¼˜è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†CXR-MAXï¼ˆå¤šæ•™å¸ˆå¯¹é½Xå°„çº¿ï¼‰ï¼Œå…¶ä¸­åŒ…å«170,982ä¸ªä»åŸºäºMIMIC-CXRçš„å…¬å¼€MLLMä¸­æå–çš„è’¸é¦æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»å¤šä¸ªæ¼‚ç§»çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡ŒçŸ¥è¯†è’¸é¦æ—¶ï¼Œç”±äºæ•™å¸ˆæ¨¡å‹æ¨ç†è½¨è¿¹çš„æ¦‚å¿µæ¼‚ç§»è€Œå¯¼è‡´çš„å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æ•™å¸ˆæ¨¡å‹æ˜¯é™æ€çš„ï¼Œå¿½ç•¥äº†æ•™å¸ˆæ¨¡å‹æ¨ç†åˆ†å¸ƒéšæ—¶é—´å˜åŒ–å¸¦æ¥çš„åå·®ï¼Œè¿™ä¼šä¼ é€’ç»™å­¦ç”Ÿæ¨¡å‹ï¼Œå½±å“å…¶æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ¦‚å¿µæ¼‚ç§»ä¸çŸ¥è¯†è’¸é¦è”ç³»èµ·æ¥ï¼Œå°†å¤šæ•™å¸ˆMLLMçš„éå¹³ç¨³æ¨ç†åŠ¨æ€å»ºæ¨¡ä¸ºå¤šæµæ¨ç†è½¨è¿¹çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹é—®é¢˜ã€‚é€šè¿‡ä¸»åŠ¨å­¦ä¹ ã€æ¯”è¾ƒå’Œæ‰¹åˆ¤æ•™å¸ˆæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå­¦ç”Ÿæ¨¡å‹å¯ä»¥è¯†åˆ«å¹¶çº æ­£æ•™å¸ˆæ¨¡å‹ä¸­çš„æ¦‚å¿µæ¼‚ç§»ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´é²æ£’å’Œä¸€è‡´çš„çŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šå­¦ä¹ ï¼ˆLearnï¼‰ã€æ¯”è¾ƒï¼ˆCompareï¼‰å’Œæ‰¹åˆ¤ï¼ˆCritiqueï¼‰ã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡å‹æ¨¡ä»¿å¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„æ¨ç†è½¨è¿¹ã€‚åœ¨æ¯”è¾ƒé˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡å‹æ¯”è¾ƒä¸åŒæ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºï¼Œè¯†åˆ«æ½œåœ¨çš„æ¦‚å¿µæ¼‚ç§»ã€‚åœ¨æ‰¹åˆ¤é˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡è‡ªä¸»åå¥½ä¼˜åŒ–ï¼ˆAPOï¼‰å¯¹é½æ•™å¸ˆæ¨¡å‹çš„æ¦‚å¿µï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å¯é çš„çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†æ¦‚å¿µæ¼‚ç§»å¼•å…¥çŸ¥è¯†è’¸é¦ï¼Œå¹¶æå‡ºäº†â€œå­¦ä¹ ã€æ¯”è¾ƒã€æ‰¹åˆ¤â€èŒƒå¼å’Œè‡ªä¸»åå¥½ä¼˜åŒ–ï¼ˆAPOï¼‰æ–¹æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«å’Œçº æ­£æ•™å¸ˆæ¨¡å‹ä¸­çš„æ¦‚å¿µæ¼‚ç§»ï¼Œä»è€Œæé«˜å­¦ç”Ÿæ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè‡ªä¸»åå¥½ä¼˜åŒ–ï¼ˆAPOï¼‰æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚APOé€šè¿‡å¥–åŠ±å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´çš„æ¨ç†è½¨è¿¹ï¼Œå¹¶æƒ©ç½šä¸ä¸€è‡´çš„è½¨è¿¹ï¼Œä»è€Œå®ç°æ¦‚å¿µå¯¹é½ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡æœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯æœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„æ¨ç†å·®å¼‚ï¼ŒåŒæ—¶é¼“åŠ±å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆæ›´ä¸€è‡´å’Œå¯é çš„æ¨ç†è½¨è¿¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸€è‡´æ€§ã€é²æ£’æ€§å’Œæ³›åŒ–æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†åœ¨çŸ¥è¯†è’¸é¦ä¸­ä¸€è‡´æ€§ã€é²æ£’æ€§å’Œæ³›åŒ–æ€§çš„æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†CXR-MAXï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä»å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦çš„åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æå’Œæ™ºèƒ½å®¢æœç­‰ã€‚é€šè¿‡è§£å†³æ¦‚å¿µæ¼‚ç§»é—®é¢˜ï¼Œå¯ä»¥æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ„å»ºæ›´é²æ£’å’Œå¯ä¿¡èµ–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper identifies a critical yet underexplored challenge in distilling from multimodal large language models (MLLMs): the reasoning trajectories generated by multiple drifting teachers exhibit concept drift, whereby their reasoning distributions evolve unpredictably and transmit biases to the student model, ultimately compromising its performance. To tackle this issue, we pioneer a theoretical connection between concept drift and knowledge distillation, casting the non-stationary reasoning dynamics from multiple MLLM teachers as next-token prediction of multi-stream reasoning trajectories.Guided by concept drift, we introduce the "learn, compare, critique" paradigm, culminating in autonomous preference optimization (APO). Under the active guidance of the teachers, the student model first learns and self-distils preferred thinking by comparing multiple teachers. It then engages in critical reflection over the drifting inference from teachers, performing concept alignment through APO, ultimately yielding a robust, consistent, and generalizable model.Extensive experiments demonstrate our superior performance of consistency, robustness and generalization within knowledge distillation. Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public at: https://anonymous.4open.science/r/Autonomous-Distillation/.

