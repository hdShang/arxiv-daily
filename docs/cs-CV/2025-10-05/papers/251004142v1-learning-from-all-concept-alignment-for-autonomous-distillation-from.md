---
layout: default
title: Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs
---

# Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04142" target="_blank" class="toolbar-btn">arXiv: 2510.04142v1</a>
    <a href="https://arxiv.org/pdf/2510.04142.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04142v1" 
            onclick="toggleFavorite(this, '2510.04142v1', 'Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiaoyu Yang, Jie Lu, En Yu

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ê¶ÇÂøµÂØπÈΩêÁöÑËá™‰∏ªËí∏È¶èÊñπÊ≥ïÔºåËß£ÂÜ≥Â§öÊºÇÁßªMLLMÁöÑÁü•ËØÜËí∏È¶èÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê¶ÇÂøµÊºÇÁßª` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Ëá™‰∏ªÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMÁü•ËØÜËí∏È¶èÊñπÊ≥ïÂøΩÁï•‰∫ÜÂ§öÊïôÂ∏àÊ®°ÂûãÊé®ÁêÜËΩ®ËøπÁöÑÊ¶ÇÂøµÊºÇÁßªÈóÆÈ¢òÔºåÂØºËá¥Â≠¶ÁîüÊ®°ÂûãÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. ÊèêÂá∫‚ÄúÂ≠¶‰π†„ÄÅÊØîËæÉ„ÄÅÊâπÂà§‚ÄùËåÉÂºèÔºåÈÄöËøáËá™‰∏ªÂÅèÂ•Ω‰ºòÂåñÔºàAPOÔºâÂÆûÁé∞Ê¶ÇÂøµÂØπÈΩêÔºåËß£ÂÜ≥ÊïôÂ∏àÊ®°ÂûãÊºÇÁßªÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰∏ÄËá¥ÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁü•ËØÜËí∏È¶èÊñπÊ≥ïÔºåÂπ∂ÊûÑÂª∫‰∫ÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜCXR-MAX„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊó®Âú®Ëß£ÂÜ≥‰ªéÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâËí∏È¶èÊó∂ÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆ‰ΩÜÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÊåëÊàòÔºöÂ§ö‰∏™ÊºÇÁßªÊïôÂ∏àÊ®°Âûã‰∫ßÁîüÁöÑÊé®ÁêÜËΩ®ËøπË°®Áé∞Âá∫Ê¶ÇÂøµÊºÇÁßªÔºåÂÖ∂Êé®ÁêÜÂàÜÂ∏É‰∏çÂèØÈ¢ÑÊµãÂú∞ÊºîÂèòÔºåÂπ∂Â∞ÜÂÅèÂ∑Æ‰º†ÈÄíÁªôÂ≠¶ÁîüÊ®°ÂûãÔºåÊúÄÁªàÊçüÂÆ≥ÂÖ∂ÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÁéáÂÖàÂª∫Á´ã‰∫ÜÊ¶ÇÂøµÊºÇÁßªÂíåÁü•ËØÜËí∏È¶è‰πãÈó¥ÁöÑÁêÜËÆ∫ËÅîÁ≥ªÔºåÂ∞ÜÊù•Ëá™Â§ö‰∏™MLLMÊïôÂ∏àÁöÑÈùûÂπ≥Á®≥Êé®ÁêÜÂä®ÊÄÅËßÜ‰∏∫Â§öÊµÅÊé®ÁêÜËΩ®ËøπÁöÑ‰∏ã‰∏Ä‰∏™tokenÈ¢ÑÊµã„ÄÇÂú®Ê¶ÇÂøµÊºÇÁßªÁöÑÊåáÂØº‰∏ãÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‚ÄúÂ≠¶‰π†„ÄÅÊØîËæÉ„ÄÅÊâπÂà§‚ÄùËåÉÂºèÔºåÊúÄÁªàÂÆûÁé∞‰∫ÜËá™‰∏ªÂÅèÂ•Ω‰ºòÂåñÔºàAPOÔºâ„ÄÇÂú®ÊïôÂ∏àÁöÑÁßØÊûÅÊåáÂØº‰∏ãÔºåÂ≠¶ÁîüÊ®°ÂûãÈ¶ñÂÖàÈÄöËøáÊØîËæÉÂ§ö‰∏™ÊïôÂ∏àÊù•Â≠¶‰π†ÂíåËá™ÊàëÊèêÁÇºÈ¶ñÈÄâÁöÑÊÄùËÄÉÊñπÂºè„ÄÇÁÑ∂ÂêéÔºåÂÆÉÂØπÊïôÂ∏àÁöÑÊºÇÁßªÊé®ÁêÜËøõË°åÊâπÂà§ÊÄßÂèçÊÄùÔºåÈÄöËøáAPOÊâßË°åÊ¶ÇÂøµÂØπÈΩêÔºåÊúÄÁªà‰∫ßÁîü‰∏Ä‰∏™È≤ÅÊ£í„ÄÅ‰∏ÄËá¥ÂíåÂèØÊ≥õÂåñÁöÑÊ®°Âûã„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åËØÅÊòé‰∫ÜÊàë‰ª¨Âú®Áü•ËØÜËí∏È¶è‰∏≠‰∏ÄËá¥ÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñÊÄßÁöÑ‰ºòË∂äÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòË¥°ÁåÆ‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜCXR-MAXÔºàÂ§öÊïôÂ∏àÂØπÈΩêXÂ∞ÑÁ∫øÔºâÔºåÂÖ∂‰∏≠ÂåÖÂê´170,982‰∏™‰ªéÂü∫‰∫éMIMIC-CXRÁöÑÂÖ¨ÂºÄMLLM‰∏≠ÊèêÂèñÁöÑËí∏È¶èÊé®ÁêÜËΩ®Ëøπ„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂíåÊï∞ÊçÆÂ∑≤ÂÖ¨ÂºÄ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰ªéÂ§ö‰∏™ÊºÇÁßªÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâËøõË°åÁü•ËØÜËí∏È¶èÊó∂ÔºåÁî±‰∫éÊïôÂ∏àÊ®°ÂûãÊé®ÁêÜËΩ®ËøπÁöÑÊ¶ÇÂøµÊºÇÁßªËÄåÂØºËá¥ÁöÑÂ≠¶ÁîüÊ®°ÂûãÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÂÅáËÆæÊïôÂ∏àÊ®°ÂûãÊòØÈùôÊÄÅÁöÑÔºåÂøΩÁï•‰∫ÜÊïôÂ∏àÊ®°ÂûãÊé®ÁêÜÂàÜÂ∏ÉÈöèÊó∂Èó¥ÂèòÂåñÂ∏¶Êù•ÁöÑÂÅèÂ∑ÆÔºåËøô‰ºö‰º†ÈÄíÁªôÂ≠¶ÁîüÊ®°ÂûãÔºåÂΩ±ÂìçÂÖ∂Ê≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊ¶ÇÂøµÊºÇÁßª‰∏éÁü•ËØÜËí∏È¶èËÅîÁ≥ªËµ∑Êù•ÔºåÂ∞ÜÂ§öÊïôÂ∏àMLLMÁöÑÈùûÂπ≥Á®≥Êé®ÁêÜÂä®ÊÄÅÂª∫Ê®°‰∏∫Â§öÊµÅÊé®ÁêÜËΩ®ËøπÁöÑ‰∏ã‰∏Ä‰∏™tokenÈ¢ÑÊµãÈóÆÈ¢ò„ÄÇÈÄöËøá‰∏ªÂä®Â≠¶‰π†„ÄÅÊØîËæÉÂíåÊâπÂà§ÊïôÂ∏àÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ãÔºåÂ≠¶ÁîüÊ®°ÂûãÂèØ‰ª•ËØÜÂà´Âπ∂Á∫†Ê≠£ÊïôÂ∏àÊ®°Âûã‰∏≠ÁöÑÊ¶ÇÂøµÊºÇÁßªÔºå‰ªéËÄåÂ≠¶‰π†Âà∞Êõ¥È≤ÅÊ£íÂíå‰∏ÄËá¥ÁöÑÁü•ËØÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÂ≠¶‰π†ÔºàLearnÔºâ„ÄÅÊØîËæÉÔºàCompareÔºâÂíåÊâπÂà§ÔºàCritiqueÔºâ„ÄÇÂú®Â≠¶‰π†Èò∂ÊÆµÔºåÂ≠¶ÁîüÊ®°ÂûãÊ®°‰ªøÂ§ö‰∏™ÊïôÂ∏àÊ®°ÂûãÁöÑÊé®ÁêÜËΩ®Ëøπ„ÄÇÂú®ÊØîËæÉÈò∂ÊÆµÔºåÂ≠¶ÁîüÊ®°ÂûãÊØîËæÉ‰∏çÂêåÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫ÔºåËØÜÂà´ÊΩúÂú®ÁöÑÊ¶ÇÂøµÊºÇÁßª„ÄÇÂú®ÊâπÂà§Èò∂ÊÆµÔºåÂ≠¶ÁîüÊ®°ÂûãÈÄöËøáËá™‰∏ªÂÅèÂ•Ω‰ºòÂåñÔºàAPOÔºâÂØπÈΩêÊïôÂ∏àÊ®°ÂûãÁöÑÊ¶ÇÂøµÔºå‰ªéËÄåÂ≠¶‰π†Âà∞Êõ¥ÂèØÈù†ÁöÑÁü•ËØÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÊ¶ÇÂøµÊºÇÁßªÂºïÂÖ•Áü•ËØÜËí∏È¶èÔºåÂπ∂ÊèêÂá∫‰∫Ü‚ÄúÂ≠¶‰π†„ÄÅÊØîËæÉ„ÄÅÊâπÂà§‚ÄùËåÉÂºèÂíåËá™‰∏ªÂÅèÂ•Ω‰ºòÂåñÔºàAPOÔºâÊñπÊ≥ï„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåËØ•ÊñπÊ≥ïËÉΩÂ§ü‰∏ªÂä®ËØÜÂà´ÂíåÁ∫†Ê≠£ÊïôÂ∏àÊ®°Âûã‰∏≠ÁöÑÊ¶ÇÂøµÊºÇÁßªÔºå‰ªéËÄåÊèêÈ´òÂ≠¶ÁîüÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËá™‰∏ªÂÅèÂ•Ω‰ºòÂåñÔºàAPOÔºâÊòØÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏Ä„ÄÇAPOÈÄöËøáÂ•ñÂä±Â≠¶ÁîüÊ®°Âûã‰∏éÊïôÂ∏àÊ®°Âûã‰∏ÄËá¥ÁöÑÊé®ÁêÜËΩ®ËøπÔºåÂπ∂ÊÉ©ÁΩö‰∏ç‰∏ÄËá¥ÁöÑËΩ®ËøπÔºå‰ªéËÄåÂÆûÁé∞Ê¶ÇÂøµÂØπÈΩê„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Êú™Áü•Ôºå‰ΩÜÂÖ∂ÁõÆÊ†áÊòØÊúÄÂ∞èÂåñÂ≠¶ÁîüÊ®°Âûã‰∏éÊïôÂ∏àÊ®°Âûã‰πãÈó¥ÁöÑÊé®ÁêÜÂ∑ÆÂºÇÔºåÂêåÊó∂ÈºìÂä±Â≠¶ÁîüÊ®°ÂûãÁîüÊàêÊõ¥‰∏ÄËá¥ÂíåÂèØÈù†ÁöÑÊé®ÁêÜËΩ®Ëøπ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰∏ÄËá¥ÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Áü•Ôºå‰ΩÜËÆ∫ÊñáÂº∫Ë∞É‰∫ÜÂú®Áü•ËØÜËí∏È¶è‰∏≠‰∏ÄËá¥ÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñÊÄßÁöÑÊòæËëóÊèêÂçá„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòË¥°ÁåÆ‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜCXR-MAXÔºå‰∏∫ËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÂÆùË¥µËµÑÊ∫ê„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶Å‰ªéÂ§ö‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÁü•ËØÜËí∏È¶èÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂåªÁñóËØäÊñ≠„ÄÅÈáëËûçÂàÜÊûêÂíåÊô∫ËÉΩÂÆ¢ÊúçÁ≠â„ÄÇÈÄöËøáËß£ÂÜ≥Ê¶ÇÂøµÊºÇÁßªÈóÆÈ¢òÔºåÂèØ‰ª•ÊèêÈ´òÂ≠¶ÁîüÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÂèØÈù†ÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÂíåÂä®ÊÄÅÁöÑÁéØÂ¢É„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÊûÑÂª∫Êõ¥È≤ÅÊ£íÂíåÂèØ‰ø°ËµñÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper identifies a critical yet underexplored challenge in distilling from multimodal large language models (MLLMs): the reasoning trajectories generated by multiple drifting teachers exhibit concept drift, whereby their reasoning distributions evolve unpredictably and transmit biases to the student model, ultimately compromising its performance. To tackle this issue, we pioneer a theoretical connection between concept drift and knowledge distillation, casting the non-stationary reasoning dynamics from multiple MLLM teachers as next-token prediction of multi-stream reasoning trajectories.Guided by concept drift, we introduce the "learn, compare, critique" paradigm, culminating in autonomous preference optimization (APO). Under the active guidance of the teachers, the student model first learns and self-distils preferred thinking by comparing multiple teachers. It then engages in critical reflection over the drifting inference from teachers, performing concept alignment through APO, ultimately yielding a robust, consistent, and generalizable model.Extensive experiments demonstrate our superior performance of consistency, robustness and generalization within knowledge distillation. Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public at: https://anonymous.4open.science/r/Autonomous-Distillation/.

