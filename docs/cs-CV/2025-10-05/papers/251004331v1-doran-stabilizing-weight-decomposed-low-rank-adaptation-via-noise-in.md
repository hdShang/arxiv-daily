---
layout: default
title: DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks
---

# DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04331" target="_blank" class="toolbar-btn">arXiv: 2510.04331v1</a>
    <a href="https://arxiv.org/pdf/2510.04331.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04331v1" 
            onclick="toggleFavorite(this, '2510.04331v1', 'DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-05

**Â§áÊ≥®**: Nghiem T. Diep, Hien Dang, and Tuan Truong contributed equally to this work

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**DoRANÔºöÈÄöËøáÂô™Â£∞Ê≥®ÂÖ•ÂíåËæÖÂä©ÁΩëÁªúÁ®≥ÂÆöÊùÉÈáçÂàÜËß£‰ΩéÁß©ÈÄÇÂ∫î**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂèÇÊï∞È´òÊïàÂæÆË∞É` `‰ΩéÁß©ÈÄÇÂ∫î` `ÊùÉÈáçÂàÜËß£` `Âô™Â£∞Ê≥®ÂÖ•` `ËæÖÂä©ÁΩëÁªú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLoRAÊñπÊ≥ïÂú®ÂæÆË∞ÉÂ§ßÂûãÊ®°ÂûãÊó∂Â≠òÂú®ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÂíåÊ†∑Êú¨ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇ
2. DoRANÈÄöËøáÂô™Â£∞Ê≥®ÂÖ•ÂíåËæÖÂä©ÁΩëÁªúÔºåËá™ÈÄÇÂ∫îÂú∞Ê≠£ÂàôÂåñÊùÉÈáçÂàÜËß£ËøáÁ®ãÔºåÂπ∂ÂÆûÁé∞Ë∑®Â±ÇÂèÇÊï∞ËÄ¶Âêà„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDoRANÂú®ËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éLoRA„ÄÅDoRAÁ≠âÂü∫Á∫øÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåÊ†∑Êú¨ÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂèÇÊï∞È´òÊïàÂæÆË∞É(PEFT)ÊñπÊ≥ïÂ∑≤Êàê‰∏∫Ë∞ÉÊï¥Â§ßËßÑÊ®°Ê®°ÂûãÁöÑÊ†áÂáÜËåÉÂºè„ÄÇÂú®Ëøô‰∫õÊäÄÊúØ‰∏≠ÔºåÊùÉÈáçÂàÜËß£‰ΩéÁß©ÈÄÇÂ∫î(DoRA)ÈÄöËøáÂ∞ÜÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÊòæÂºèÂàÜËß£‰∏∫ÂπÖÂÄºÂíåÊñπÂêëÂàÜÈáèÔºåÂ∑≤Ë¢´ËØÅÊòéÂèØ‰ª•ÊèêÈ´òÂéüÂßã‰ΩéÁß©ÈÄÇÂ∫î(LoRA)ÊñπÊ≥ïÁöÑÂ≠¶‰π†ËÉΩÂäõÂíåËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫DoRANÔºå‰∏ÄÁßçDoRAÁöÑÊñ∞Âèò‰ΩìÔºåÊó®Âú®Ëøõ‰∏ÄÊ≠•Á®≥ÂÆöËÆ≠ÁªÉÂπ∂ÊèêÈ´òDoRAÁöÑÊ†∑Êú¨ÊïàÁéá„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨‰∏§‰∏™ÂÖ≥ÈîÆÈò∂ÊÆµÔºö(i)Â∞ÜÂô™Â£∞Ê≥®ÂÖ•Âà∞DoRAÊùÉÈáçÂàÜËß£ÁöÑÂàÜÊØç‰∏≠Ôºå‰Ωú‰∏∫Ëá™ÈÄÇÂ∫îÊ≠£ÂàôÂåñÂô®‰ª•ÂáèËΩª‰∏çÁ®≥ÂÆöÊÄßÔºõ(ii)Áî®Âä®ÊÄÅÁîüÊàê‰ΩéÁß©Áü©ÈòµÁöÑËæÖÂä©ÁΩëÁªúÊõøÊç¢ÈùôÊÄÅ‰ΩéÁß©Áü©ÈòµÔºå‰ªéËÄåÂÆûÁé∞Ë∑®Â±ÇÁöÑÂèÇÊï∞ËÄ¶ÂêàÔºåÂπ∂Âú®ÁêÜËÆ∫ÂíåÂÆûË∑µ‰∏≠‰∫ßÁîüÊõ¥Â•ΩÁöÑÊ†∑Êú¨ÊïàÁéá„ÄÇÂú®ËßÜËßâÂíåËØ≠Ë®ÄÂü∫ÂáÜ‰∏äÁöÑÁªºÂêàÂÆûÈ™åË°®ÊòéÔºåDoRANÂßãÁªà‰ºò‰∫éLoRA„ÄÅDoRAÂíåÂÖ∂‰ªñPEFTÂü∫Á∫ø„ÄÇËøô‰∫õÁªìÊûúÂº∫Ë∞É‰∫ÜÈÄöËøáÂü∫‰∫éÂô™Â£∞ÁöÑÊ≠£ÂàôÂåñËøõË°åÁ®≥ÂÆö‰∏éÂü∫‰∫éÁΩëÁªúÁöÑÂèÇÊï∞ÁîüÊàêÁõ∏ÁªìÂêàÁöÑÊúâÊïàÊÄßÔºå‰∏∫Âü∫Á°ÄÊ®°ÂûãÁöÑÁ®≥ÂÅ•ÂíåÈ´òÊïàÂæÆË∞ÉÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂ∏åÊúõÁöÑÊñπÂêë„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöDoRANÊó®Âú®Ëß£ÂÜ≥ÊùÉÈáçÂàÜËß£‰ΩéÁß©ÈÄÇÂ∫îÔºàDoRAÔºâÂú®ÂæÆË∞ÉÂ§ßÂûãÊ®°ÂûãÊó∂‰ªçÁÑ∂Â≠òÂú®ÁöÑËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÂíåÊ†∑Êú¨ÊïàÁéá‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑDoRAÊñπÊ≥ïËôΩÁÑ∂ÈÄöËøáÂàÜËß£ÊùÉÈáç‰∏∫ÂπÖÂÄºÂíåÊñπÂêëÂàÜÈáèÊù•ÊîπÂñÑLoRAÔºå‰ΩÜ‰ªçÁÑ∂ÂèØËÉΩÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Âá∫Áé∞Ê¢ØÂ∫¶ÁàÜÁÇ∏ÊàñÊ∂àÂ§±Á≠âÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÈáèËæÉÂ∞ëÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDoRANÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‰∏§‰∏™ÂÖ≥ÈîÆÊú∫Âà∂Êù•Á®≥ÂÆöËÆ≠ÁªÉÂπ∂ÊèêÈ´òÊ†∑Êú¨ÊïàÁéáÔºö‰∏ÄÊòØÈÄöËøáÂú®DoRAÁöÑÊùÉÈáçÂàÜËß£ËøáÁ®ã‰∏≠Ê≥®ÂÖ•Âô™Â£∞ÔºåÂÆûÁé∞Ëá™ÈÄÇÂ∫îÊ≠£ÂàôÂåñÔºå‰ªéËÄåÊäëÂà∂ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÔºõ‰∫åÊòØ‰ΩøÁî®ËæÖÂä©ÁΩëÁªúÂä®ÊÄÅÁîüÊàê‰ΩéÁß©Áü©ÈòµÔºåÂèñ‰ª£ÈùôÊÄÅÁöÑ‰ΩéÁß©Áü©ÈòµÔºå‰ªéËÄåÂÆûÁé∞Ë∑®Â±ÇÁöÑÂèÇÊï∞ËÄ¶ÂêàÔºåÊèêÈ´òÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõÂíåÊ≥õÂåñÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDoRANÁöÑÊï¥‰ΩìÊ°ÜÊû∂Âü∫‰∫éDoRAÔºå‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÊ®°ÂùóÔºöÂô™Â£∞Ê≥®ÂÖ•Ê®°ÂùóÂíåËæÖÂä©ÁΩëÁªúÊ®°Âùó„ÄÇÂô™Â£∞Ê≥®ÂÖ•Ê®°ÂùóÂú®DoRAÁöÑÊùÉÈáçÂàÜËß£ÂÖ¨ÂºèÁöÑÂàÜÊØç‰∏≠Âä†ÂÖ•Âô™Â£∞ÔºåËµ∑Âà∞Ê≠£ÂàôÂåñ‰ΩúÁî®„ÄÇËæÖÂä©ÁΩëÁªúÊ®°ÂùóÂàôÁî±Â§ö‰∏™Â∞èÂûãÁ•ûÁªèÁΩëÁªúÁªÑÊàêÔºåÁî®‰∫éÂä®ÊÄÅÁîüÊàê‰ΩéÁß©Áü©Èòµ„ÄÇËÆ≠ÁªÉÊó∂ÔºåÂéüÂßãÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊùÉÈáç‰øùÊåÅ‰∏çÂèòÔºåÂè™ËÆ≠ÁªÉÂô™Â£∞Ê≥®ÂÖ•ÁöÑÂèÇÊï∞ÂíåËæÖÂä©ÁΩëÁªúÁöÑÂèÇÊï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDoRANÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂô™Â£∞Ê≥®ÂÖ•ÂíåËæÖÂä©ÁΩëÁªúÁõ∏ÁªìÂêàÔºåÂÖ±Âêå‰ΩúÁî®‰∫éDoRAÁöÑÊùÉÈáçÂàÜËß£ËøáÁ®ã„ÄÇÂô™Â£∞Ê≥®ÂÖ•Êèê‰æõ‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÁöÑÊ≠£ÂàôÂåñÊñπÂºèÔºåËÉΩÂ§üÊúâÊïàÂú∞ÊäëÂà∂ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄß„ÄÇËæÖÂä©ÁΩëÁªúÂàôÈÄöËøáÂä®ÊÄÅÁîüÊàê‰ΩéÁß©Áü©ÈòµÔºåÂÆûÁé∞‰∫ÜË∑®Â±ÇÁöÑÂèÇÊï∞ËÄ¶ÂêàÔºåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõÂíåÊ†∑Êú¨ÊïàÁéá„ÄÇËøôÁßçÁªìÂêàÊñπÂºè‰ΩøÂæóDoRANËÉΩÂ§üÊõ¥Á®≥ÂÆö„ÄÅÊõ¥È´òÊïàÂú∞ÂæÆË∞ÉÂ§ßÂûãÊ®°Âûã„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Âô™Â£∞Ê≥®ÂÖ•Ê®°Âùó‰∏≠ÔºåÂô™Â£∞ÁöÑÊñπÂ∑ÆÊòØ‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÂèÇÊï∞ÔºåÂèØ‰ª•Ê†πÊçÆËÆ≠ÁªÉÊï∞ÊçÆÁöÑÁâπÁÇπËøõË°åËá™ÈÄÇÂ∫îË∞ÉÊï¥„ÄÇÂú®ËæÖÂä©ÁΩëÁªúÊ®°Âùó‰∏≠ÔºåÈááÁî®‰∫ÜÂ∞èÂûãÂ§öÂ±ÇÊÑüÁü•Êú∫ÔºàMLPÔºâ‰Ωú‰∏∫ÁîüÊàêÂô®ÔºåÂÖ∂ËæìÂÖ•ÂèØ‰ª•ÊòØÂ±ÇÁ¥¢ÂºïÊàñÂÖ∂‰ªñ‰∏éÂ±ÇÁõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÔºåËæìÂá∫ÂàôÊòØÂä®ÊÄÅÁîüÊàêÁöÑ‰ΩéÁß©Áü©Èòµ„ÄÇÊçüÂ§±ÂáΩÊï∞Èô§‰∫ÜÂåÖÊã¨Â∏∏ËßÑÁöÑÂæÆË∞ÉÊçüÂ§±Â§ñÔºåËøòÂèØ‰ª•Âä†ÂÖ•Ê≠£ÂàôÂåñÈ°πÔºå‰ª•Á∫¶ÊùüËæÖÂä©ÁΩëÁªúÁöÑËæìÂá∫„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DoRANÂú®Â§ö‰∏™ËßÜËßâÂíåËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏äÔºåDoRANÁõ∏ÊØî‰∫éDoRAÂíåLoRAÔºåÂú®Áõ∏ÂêåÂèÇÊï∞Èáè‰∏ãÔºåTop-1ÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü0.5%-1.0%„ÄÇÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏äÔºåDoRANÂú®GLUEÂü∫ÂáÜÊµãËØï‰∏≠‰πüÂèñÂæó‰∫ÜÁ±ª‰ººÁöÑÊèêÂçá„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåDoRANËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÂèÇÊï∞ÔºåÂπ∂ÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DoRANÈÄÇÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈ´òÊïàÂæÆË∞ÉÂ§ßÂûãÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÁöÑÊñáÊú¨ÂàÜÁ±ª„ÄÅÊú∫Âô®ÁøªËØëÔºå‰ª•ÂèäËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠ÁöÑÂõæÂÉèËØÜÂà´„ÄÅÁõÆÊ†áÊ£ÄÊµãÁ≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•Èôç‰ΩéÂæÆË∞ÉÊàêÊú¨ÔºåÊèêÈ´òÊ®°ÂûãÂú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÈÉ®ÁΩ≤ËÉΩÂäõÔºåÂπ∂Âä†ÈÄüÊñ∞‰ªªÂä°ÁöÑÈÄÇÂ∫îËøáÁ®ã„ÄÇÊú™Êù•ÔºåDoRANÊúâÊúõÂ∫îÁî®‰∫éÊõ¥Â§öÈ¢ÜÂüüÔºåÂ¶ÇËØ≠Èü≥ËØÜÂà´„ÄÅÊé®ËçêÁ≥ªÁªüÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.

