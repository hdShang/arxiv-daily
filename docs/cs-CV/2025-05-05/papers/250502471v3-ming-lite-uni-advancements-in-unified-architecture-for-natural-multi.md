---
layout: default
title: "Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction"
---

# Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02471" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02471v3</a>
  <a href="https://arxiv.org/pdf/2505.02471.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02471v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02471v3', 'Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Inclusion AI, Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-06-13)

**å¤‡æ³¨**: https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMing-Lite-Uniä»¥è§£å†³å¤šæ¨¡æ€äº¤äº’ç»Ÿä¸€æ¶æ„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€äº¤äº’` `è§†è§‰ç”Ÿæˆ` `è‡ªå›å½’æ¨¡å‹` `æ–‡æœ¬åˆ°å›¾åƒ` `å›¾åƒç¼–è¾‘` `å¼€æºæ¡†æ¶` `äººå·¥æ™ºèƒ½` `AGI`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰ä¸è¯­è¨€çš„ç»Ÿä¸€å¤„ç†ä¸Šå­˜åœ¨å±€é™ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„äº¤äº’ã€‚
2. Ming-Lite-Unié€šè¿‡è®¾è®¡ç»Ÿä¸€çš„è§†è§‰ç”Ÿæˆå™¨å’Œå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œæä¾›äº†æ›´é«˜æ•ˆçš„å¤šæ¨¡æ€äº¤äº’è§£å†³æ–¹æ¡ˆã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMing-Lite-Uniåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæå‡äº†äº¤äº’çš„æµç•…æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬ä»‹ç»äº†Ming-Lite-Uniï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œå…·æœ‰æ–°è®¾è®¡çš„ç»Ÿä¸€è§†è§‰ç”Ÿæˆå™¨å’ŒåŸç”Ÿå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€è§†è§‰ä¸è¯­è¨€ã€‚è¯¥é¡¹ç›®æä¾›äº†é›†æˆçš„MetaQuerieså’ŒM2-omniæ¡†æ¶çš„å¼€æºå®ç°ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°é¢–çš„å¤šå°ºåº¦å¯å­¦ä¹ æ ‡è®°å’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ã€‚é€šè¿‡åˆ©ç”¨å›ºå®šçš„MLLMå’Œå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼ŒMing-Lite-Uniä½¿åŸç”Ÿå¤šæ¨¡æ€ARæ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œæ‰©å±•äº†å…¶èƒ½åŠ›ï¼Œè¶…è¶Šäº†çº¯è§†è§‰ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜Ming-Lite-Uniçš„å¼ºå¤§æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å…¶äº¤äº’è¿‡ç¨‹çš„æµç•…æ€§ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡å‡å·²å¼€æºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºçš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰ä¸è¯­è¨€ç»Ÿä¸€å¤„ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨äº¤äº’æ•ˆç‡å’Œç”Ÿæˆè´¨é‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è®¾è®¡ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰ç”Ÿæˆå™¨å’Œå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼ŒMing-Lite-Unièƒ½å¤Ÿæœ‰æ•ˆæ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œä»è€Œæå‡å¤šæ¨¡æ€äº¤äº’çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMing-Lite-Uniçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå›ºå®šçš„å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰å’Œä¸€ä¸ªå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬å¤šå°ºåº¦å¯å­¦ä¹ æ ‡è®°å’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å¤šå°ºåº¦å¯å­¦ä¹ æ ‡è®°å’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ï¼Œè¿™äº›è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡æ—¶è¡¨ç°æ›´ä¸ºä¼˜è¶Šï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¤šæ¨¡æ€ç”Ÿæˆæ•ˆæœï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ç²¾ç»†è°ƒæ•´ï¼Œä»¥ç¡®ä¿ä¸åŒæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMing-Lite-Uniåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€äº¤äº’ä¸­çš„æµç•…æ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Ming-Lite-Uniçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹åˆ›ä½œã€æ•™è‚²åŸ¹è®­ç­‰ã€‚å…¶å¼ºå¤§çš„å¤šæ¨¡æ€äº¤äº’èƒ½åŠ›èƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ï¼Œæœªæ¥å¯èƒ½åœ¨é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.

