---
layout: default
title: "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities"
---

# Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02567" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02567v5</a>
  <a href="https://arxiv.org/pdf/2505.02567.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02567v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02567v5', 'Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-08-17)

**å¤‡æ³¨**: In this version, we incorporate new papers, datasets, and benchmarks. This work is still in progress; Github project: https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆæ¨¡å‹ä»¥è§£å†³ç‹¬ç«‹æ¼”åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `å›¾åƒç”Ÿæˆ` `ç»Ÿä¸€æ¨¡å‹` `è‡ªå›å½’æœºåˆ¶` `æ‰©æ•£æ¨¡å‹` `è·¨æ¨¡æ€æ³¨æ„åŠ›` `æ•°æ®é›†` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€ç†è§£å’Œå›¾åƒç”Ÿæˆæ¨¡å‹ç‹¬ç«‹æ¼”åŒ–ï¼Œå¯¼è‡´æ¶æ„å·®å¼‚å’Œæ•´åˆå›°éš¾ã€‚
2. æå‡ºç»Ÿä¸€æ¡†æ¶ï¼Œæ•´åˆè‡ªå›å½’å’Œæ‰©æ•£æœºåˆ¶ï¼Œæ¨åŠ¨å¤šæ¨¡æ€ä»»åŠ¡çš„ååŒå‘å±•ã€‚
3. é€šè¿‡å¯¹ç°æœ‰æ¨¡å‹çš„åˆ†ç±»å’Œåˆ†æï¼Œæä¾›äº†ç»“æ„è®¾è®¡çš„åˆ›æ–°å’Œæ•°æ®é›†èµ„æºï¼Œä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€ç†è§£æ¨¡å‹å’Œå›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ä¸ªé¢†åŸŸçš„ç‹¬ç«‹æ¼”åŒ–å¯¼è‡´äº†ä¸åŒçš„æ¶æ„èŒƒå¼ï¼šè‡ªå›å½’æ¶æ„ä¸»å¯¼å¤šæ¨¡æ€ç†è§£ï¼Œè€Œæ‰©æ•£æ¨¡å‹æˆä¸ºå›¾åƒç”Ÿæˆçš„åŸºçŸ³ã€‚è¿‘æœŸï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å¼€å§‹å…³æ³¨å¼€å‘ç»Ÿä¸€æ¡†æ¶ä»¥æ•´åˆè¿™äº›ä»»åŠ¡ã€‚GPT-4oçš„æ–°èƒ½åŠ›å±•ç¤ºäº†è¿™ä¸€è¶‹åŠ¿çš„æ½œåŠ›ï¼Œä½†ä¸¤è€…ä¹‹é—´çš„æ¶æ„å·®å¼‚å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ã€‚æœ¬æ–‡æä¾›äº†å¯¹å½“å‰ç»Ÿä¸€åŠªåŠ›çš„å…¨é¢è°ƒæŸ¥ï¼Œä»‹ç»äº†å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„åŸºç¡€æ¦‚å¿µå’Œæœ€æ–°è¿›å±•ï¼Œå›é¡¾äº†ç°æœ‰ç»Ÿä¸€æ¨¡å‹ï¼Œå¹¶åˆ†æäº†ç›¸å…³å·¥ä½œçš„ç»“æ„è®¾è®¡å’Œåˆ›æ–°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ±‡ç¼–äº†é’ˆå¯¹ç»Ÿä¸€æ¨¡å‹çš„æ•°æ®é›†å’ŒåŸºå‡†ï¼Œè®¨è®ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ¿€åŠ±è¿›ä¸€æ­¥ç ”ç©¶å¹¶ä¸ºç¤¾åŒºæä¾›æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€ç†è§£ä¸å›¾åƒç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„ç‹¬ç«‹æ¼”åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ¶æ„ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¼è‡´æ•´åˆå›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆè‡ªå›å½’å’Œæ‰©æ•£æœºåˆ¶ï¼Œä»¥å®ç°ä»»åŠ¡çš„ååŒå¤„ç†å’Œæ€§èƒ½æå‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå¤šæ¨¡æ€ç†è§£æ¨¡å—ã€å›¾åƒç”Ÿæˆæ¨¡å—å’Œç»Ÿä¸€æ¥å£ï¼Œç¡®ä¿ä¿¡æ¯åœ¨ä¸åŒæ¨¡æ€é—´çš„æœ‰æ•ˆä¼ é€’ä¸å¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥æ··åˆæ¶æ„ï¼Œèåˆè‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œçªç ´äº†ä¼ ç»Ÿæ¨¡å‹çš„å±€é™ï¼Œæä¾›äº†æ›´çµæ´»çš„ç”Ÿæˆèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†æ–°çš„tokenizationç­–ç•¥å’Œè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¼˜åŒ–äº†æ•°æ®å¤„ç†æµç¨‹å’ŒæŸå¤±å‡½æ•°è®¾ç½®ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç»Ÿä¸€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨ç”Ÿæˆè´¨é‡å’Œç†è§£å‡†ç¡®æ€§ä¸Šï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€å†…å®¹åˆ›ä½œã€è™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€äº¤äº’ä¸­æä¾›æ›´è‡ªç„¶çš„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œå¯èƒ½ä¼šåœ¨æ•™è‚²ã€å¨±ä¹å’ŒåŒ»ç–—ç­‰å¤šä¸ªè¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).

