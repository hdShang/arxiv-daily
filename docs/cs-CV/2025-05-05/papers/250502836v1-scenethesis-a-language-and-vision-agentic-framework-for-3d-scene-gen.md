---
layout: default
title: "Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation"
---

# Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02836" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02836v1</a>
  <a href="https://arxiv.org/pdf/2505.02836.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02836v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02836v1', 'Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu Zeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera, Zhaoshuo Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºScenethesisä»¥è§£å†³3Dåœºæ™¯ç”Ÿæˆä¸­çš„ç©ºé—´ç°å®æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dåœºæ™¯ç”Ÿæˆ` `è¯­è¨€ä¸è§†è§‰` `ç©ºé—´ç°å®æ€§` `æ— è®­ç»ƒæ¡†æ¶` `è™šæ‹Ÿç°å®` `å…·èº«AI` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–å°è§„æ¨¡æ•°æ®é›†ï¼Œå¯¼è‡´ç”Ÿæˆçš„3Dåœºæ™¯ç¼ºä¹å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚
2. Scenethesisé€šè¿‡ç»“åˆLLMè¿›è¡Œåœºæ™¯è§„åˆ’å’Œè§†è§‰æ¨¡å—è¿›è¡Œå¸ƒå±€ä¼˜åŒ–ï¼Œæä¾›ç©ºé—´æŒ‡å¯¼ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒScenethesisç”Ÿæˆçš„åœºæ™¯åœ¨å¤šæ ·æ€§å’Œç‰©ç†åˆç†æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»æ–‡æœ¬åˆæˆäº¤äº’å¼3Dåœºæ™¯å¯¹äºæ¸¸æˆã€è™šæ‹Ÿç°å®å’Œå…·èº«AIè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä¾èµ–å°è§„æ¨¡å®¤å†…æ•°æ®é›†ï¼Œé™åˆ¶äº†åœºæ™¯çš„å¤šæ ·æ€§å’Œå¸ƒå±€å¤æ‚æ€§ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿåˆ©ç”¨å¤šæ ·çš„æ–‡æœ¬é¢†åŸŸçŸ¥è¯†ï¼Œä½†åœ¨ç©ºé—´ç°å®æ€§æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¸¸å¸¸å¯¼è‡´ä¸è‡ªç„¶çš„ç‰©ä½“æ”¾ç½®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºScenethesisï¼Œä¸€ä¸ªæ— è®­ç»ƒçš„ä»£ç†æ¡†æ¶ï¼Œç»“åˆäº†åŸºäºLLMçš„åœºæ™¯è§„åˆ’ä¸è§†è§‰å¼•å¯¼çš„å¸ƒå±€ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒScenethesisèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ã€çœŸå®ä¸”ç‰©ç†ä¸Šåˆç†çš„3Däº¤äº’åœºæ™¯ï¼Œå…·æœ‰é‡è¦çš„è™šæ‹Ÿå†…å®¹åˆ›ä½œå’Œå…·èº«AIç ”ç©¶ä»·å€¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»æ–‡æœ¬ç”Ÿæˆ3Dåœºæ™¯æ—¶çš„ç©ºé—´ç°å®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å°è§„æ¨¡æ•°æ®é›†ï¼Œå¯¼è‡´ç”Ÿæˆçš„åœºæ™¯ç¼ºä¹å¤šæ ·æ€§å’Œå¸ƒå±€å¤æ‚æ€§ï¼ŒåŒæ—¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“æ”¾ç½®ä¸Šå¸¸å¸¸ä¸ç¬¦åˆå¸¸è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šScenethesisçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰æ„ŸçŸ¥æä¾›ç©ºé—´æŒ‡å¯¼ï¼Œå¼¥è¡¥LLMåœ¨ç©ºé—´å¸ƒå±€ä¸Šçš„ä¸è¶³ã€‚é€šè¿‡å°†è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å—ç»“åˆï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´è‡ªç„¶çš„3Dåœºæ™¯å¸ƒå±€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šScenethesisçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å››ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼ŒLLMæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆç²—ç•¥å¸ƒå±€ï¼›å…¶æ¬¡ï¼Œè§†è§‰æ¨¡å—é€šè¿‡å›¾åƒå¼•å¯¼å’Œæå–åœºæ™¯ç»“æ„æ¥ä¼˜åŒ–å¸ƒå±€ï¼›æ¥ç€ï¼Œä¼˜åŒ–æ¨¡å—è¿­ä»£åœ°ç¡®ä¿ç‰©ä½“å§¿æ€å¯¹é½å’Œç‰©ç†åˆç†æ€§ï¼›æœ€åï¼Œè¯„ä¼°æ¨¡å—éªŒè¯ç©ºé—´ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šScenethesisçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ— è®­ç»ƒçš„ä»£ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç»“åˆè¯­è¨€å’Œè§†è§‰ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆç‰©ç†è§„å¾‹çš„3Dåœºæ™¯ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿä¾èµ–äºè®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒScenethesisé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ç¡®ä¿ç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»åˆç†ï¼ŒåŒæ—¶åœ¨è§†è§‰æ¨¡å—ä¸­å¼•å…¥äº†å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œä»¥å¢å¼ºåœºæ™¯çš„çœŸå®æ„Ÿã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒScenethesisç”Ÿæˆçš„3Dåœºæ™¯åœ¨å¤šæ ·æ€§å’Œç‰©ç†åˆç†æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨ç”ŸæˆçœŸå®åœºæ™¯æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Scenethesisåœ¨è™šæ‹Ÿå†…å®¹åˆ›ä½œã€æ¨¡æ‹Ÿç¯å¢ƒå’Œå…·èº«AIç ”ç©¶ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„3Dåœºæ™¯ç”Ÿæˆèƒ½åŠ›ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ”¯æŒæ¸¸æˆå¼€å‘ã€è™šæ‹Ÿç°å®ä½“éªŒä»¥åŠæœºå™¨äººä¸ç¯å¢ƒçš„äº¤äº’ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research.

