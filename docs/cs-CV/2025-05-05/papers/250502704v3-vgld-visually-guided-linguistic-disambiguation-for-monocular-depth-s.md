---
layout: default
title: VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery
---

# VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02704" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.02704v3</a>
  <a href="https://arxiv.org/pdf/2505.02704.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02704v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02704v3', 'VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Bojin Wu, Jing Chen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-05 (Êõ¥Êñ∞: 2025-07-13)

**Â§áÊ≥®**: 19 pages, conference

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VGLD‰ª•Ëß£ÂÜ≥ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°‰∏≠ÁöÑËØ≠Ë®ÄÊ≠ß‰πâÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°` `ËØ≠Ë®ÄÊ∂àÊ≠ß` `ËßÜËßâËØ≠‰πâ` `Ê∑±Â∫¶Â≠¶‰π†` `Â§öÊ®°ÊÄÅËûçÂêà`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ïÂú®Áº∫‰πèÁªùÂØπÂ∞∫Â∫¶ÁöÑÊÉÖÂÜµ‰∏ãÔºåÈöæ‰ª•Êª°Ë∂≥ÂÆûÈôÖÂ∫îÁî®ÈúÄÊ±Ç„ÄÇ
2. VGLDÊ°ÜÊû∂ÈÄöËøáËßÜËßâÂºïÂØºÁöÑËØ≠Ë®ÄÊ∂àÊ≠ßÔºåÁªìÂêàÂõæÂÉèÂíåÊñáÊú¨‰ø°ÊÅØÔºåËß£ÂÜ≥‰∫ÜËØ≠Ë®ÄÊ≠ß‰πâÈóÆÈ¢ò„ÄÇ
3. Âú®NYUv2ÂíåKITTIÂü∫ÂáÜÊµãËØï‰∏≠ÔºåVGLDÊòæËëóÊèêÈ´ò‰∫ÜÂ∞∫Â∫¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÁ®≥ÂÆöÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÂèØÂàÜ‰∏∫Áõ∏ÂØπÊ∑±Â∫¶‰º∞ËÆ°ÂíåÂ∫¶ÈáèÊ∑±Â∫¶‰º∞ËÆ°‰∏§Á±ª„ÄÇÁõ∏ÂØπÊñπÊ≥ïÁÅµÊ¥ª‰∏îÊï∞ÊçÆÈ´òÊïàÔºå‰ΩÜÁº∫‰πèÁªùÂØπÂ∞∫Â∫¶ÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåVGLDÔºàËßÜËßâÂºïÂØºËØ≠Ë®ÄÊ∂àÊ≠ßÔºâÊ°ÜÊû∂ÈÄöËøáÁªìÂêàÈ´òÂ±ÇÊ¨°ËßÜËßâËØ≠‰πâÊù•Ëß£ÂÜ≥ÊñáÊú¨ËæìÂÖ•‰∏≠ÁöÑÊ≠ß‰πâ„ÄÇVGLDÈÄöËøáËÅîÂêàÁºñÁ†ÅÂõæÂÉèÂíåÊñáÊú¨ÔºåÈ¢ÑÊµã‰∏ÄÁªÑÂÖ®Â±ÄÁ∫øÊÄßÂèòÊç¢ÂèÇÊï∞ÔºåÂ∞ÜÁõ∏ÂØπÊ∑±Â∫¶Âõæ‰∏éÂ∫¶ÈáèÂ∞∫Â∫¶ÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVGLDÊòæËëóÂáèËΩª‰∫ÜÂõ†ËØ≠Ë®Ä‰∏ç‰∏ÄËá¥ÊàñÊ®°Á≥äÂØºËá¥ÁöÑÂ∞∫Â∫¶‰º∞ËÆ°ÂÅèÂ∑ÆÔºåÂÆûÁé∞‰∫ÜÁ®≥ÂÅ•‰∏îÂáÜÁ°ÆÁöÑÂ∫¶ÈáèÈ¢ÑÊµã„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°‰∏≠ÁöÑÂ∞∫Â∫¶ÊÅ¢Â§çÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Êó∂ÂÆπÊòìÂèóÂà∞Ê≠ß‰πâÁöÑÂΩ±ÂìçÔºåÂØºËá¥Ê∑±Â∫¶‰º∞ËÆ°‰∏çÂáÜÁ°Æ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVGLDÈÄöËøáÂºïÂÖ•ËßÜËßâËØ≠‰πâ‰ø°ÊÅØÊù•Ê∂àÈô§ÊñáÊú¨ÊèèËø∞‰∏≠ÁöÑÊ≠ß‰πâÔºåÂà©Áî®ÂõæÂÉèÂíåÊñáÊú¨ÁöÑËÅîÂêàÁºñÁ†ÅÊù•Â¢ûÂº∫Â∞∫Â∫¶ÊÅ¢Â§çÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVGLDÊ°ÜÊû∂ÂåÖÊã¨ÂõæÂÉèÁâπÂæÅÊèêÂèñ„ÄÅÊñáÊú¨ÁâπÂæÅÁºñÁ†ÅÂíåÂÖ®Â±ÄÁ∫øÊÄßÂèòÊç¢ÂèÇÊï∞È¢ÑÊµã‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÊèêÂèñÂõæÂÉèÁöÑÈ´òÂ±ÇÁâπÂæÅÔºåÁÑ∂ÂêéÂØπÊñáÊú¨ËøõË°åÁºñÁ†ÅÔºåÊúÄÂêéÈÄöËøáËÅîÂêà‰ø°ÊÅØÈ¢ÑÊµãÂ∞∫Â∫¶ÂØπÈΩêÂèÇÊï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVGLDÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éËßÜËßâÂºïÂØºÁöÑËØ≠Ë®ÄÊ∂àÊ≠ßÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ§ÑÁêÜ‰∏çÂêåÁöÑËØ≠Ë®ÄÊèèËø∞Ôºå‰ªéËÄåÊèêÈ´òÂ∞∫Â∫¶‰º∞ËÆ°ÁöÑÁ®≥ÂÆöÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÂ§öÂ±ÇÂç∑ÁßØÁΩëÁªúËøõË°åÂõæÂÉèÁâπÂæÅÊèêÂèñÔºåÊñáÊú¨ÁºñÁ†Å‰ΩøÁî®‰∫ÜTransformerÁªìÊûÑÔºåÊçüÂ§±ÂáΩÊï∞ÂàôÁªìÂêà‰∫ÜÁõ∏ÂØπÊ∑±Â∫¶ÂíåÂ∫¶ÈáèÊ∑±Â∫¶ÁöÑÂØπÊØîÊçüÂ§±Ôºå‰ª•‰ºòÂåñÂ∞∫Â∫¶ÂØπÈΩêÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®ÂÆûÈ™å‰∏≠ÔºåVGLDÂú®NYUv2ÂíåKITTIÊï∞ÊçÆÈõÜ‰∏äÊòæËëóÈôç‰Ωé‰∫ÜÂ∞∫Â∫¶‰º∞ËÆ°ÁöÑÂÅèÂ∑ÆÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®Â§ÑÁêÜËØ≠Ë®ÄÊ≠ß‰πâÊñπÈù¢ÁöÑÂº∫Â§ßËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÂíåËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÔºåVGLDËÉΩÂ§ü‰∏∫Ëøô‰∫õÂ∫îÁî®Êèê‰æõÊõ¥ÂèØÈù†ÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÔºåËøõËÄåÊèêÂçáÁ≥ªÁªüÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Monocular depth estimation can be broadly categorized into two directions: relative depth estimation, which predicts normalized or inverse depth without absolute scale, and metric depth estimation, which aims to recover depth with real-world scale. While relative methods are flexible and data-efficient, their lack of metric scale limits their utility in downstream tasks. A promising solution is to infer absolute scale from textual descriptions. However, such language-based recovery is highly sensitive to natural language ambiguity, as the same image may be described differently across perspectives and styles. To address this, we introduce VGLD (Visually-Guided Linguistic Disambiguation), a framework that incorporates high-level visual semantics to resolve ambiguity in textual inputs. By jointly encoding both image and text, VGLD predicts a set of global linear transformation parameters that align relative depth maps with metric scale. This visually grounded disambiguation improves the stability and accuracy of scale estimation. We evaluate VGLD on representative models, including MiDaS and DepthAnything, using standard indoor (NYUv2) and outdoor (KITTI) benchmarks. Results show that VGLD significantly mitigates scale estimation bias caused by inconsistent or ambiguous language, achieving robust and accurate metric predictions. Moreover, when trained on multiple datasets, VGLD functions as a universal and lightweight alignment module, maintaining strong performance even in zero-shot settings. Code will be released upon acceptance.

