---
layout: default
title: Using Knowledge Graphs to harvest datasets for efficient CLIP model training
---

# Using Knowledge Graphs to harvest datasets for efficient CLIP model training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02746" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02746v3</a>
  <a href="https://arxiv.org/pdf/2505.02746.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02746v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02746v3', 'Using Knowledge Graphs to harvest datasets for efficient CLIP model training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Simon Ging, Sebastian Walter, Jelena BratuliÄ‡, Johannes Dienert, Hannah Bast, Thomas Brox

**åˆ†ç±»**: cs.CV, cs.CL, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-09-30)

**å¤‡æ³¨**: Accepted for oral presentation at GCPR 2025 (German Conference on Pattern Recognition). This is the version submitted to the conference, not the official conference proceedings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨çŸ¥è¯†å›¾è°±é«˜æ•ˆæ”¶é›†æ•°æ®é›†ä»¥è®­ç»ƒCLIPæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†å›¾è°±` `CLIPæ¨¡å‹` `æ•°æ®é›†æ”¶é›†` `ç”Ÿç‰©ä½“è¯†åˆ«` `æ¨¡å‹è®­ç»ƒ` `æ™ºèƒ½æœç´¢ç­–ç•¥` `EntityNet`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„CLIPæ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®ï¼Œå¯¼è‡´é¢†åŸŸç‰¹å®šæ¨¡å‹å¼€å‘å—é™ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ä¸­ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ç»“åˆçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½ç½‘ç»œæœç´¢ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ç”¨1000ä¸‡å¼ å›¾åƒå³å¯è®­ç»ƒå‡ºé«˜è´¨é‡çš„ç”Ÿç‰©ä½“CLIPæ¨¡å‹ï¼Œå¹¶å¼•å…¥EntityNetæ•°æ®é›†åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜è´¨é‡CLIPæ¨¡å‹çš„è®­ç»ƒé€šå¸¸éœ€è¦åºå¤§çš„æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†é¢†åŸŸç‰¹å®šæ¨¡å‹çš„å¼€å‘ï¼Œå°¤å…¶æ˜¯åœ¨ä¸€äº›å¤§å‹CLIPæ¨¡å‹è¦†ç›–ä¸ä½³çš„é¢†åŸŸï¼Œå¹¶ä¸”å¢åŠ äº†è®­ç»ƒæˆæœ¬ã€‚æœ¬æ–‡å±•ç¤ºäº†é€šè¿‡æ™ºèƒ½ç½‘ç»œæœç´¢ç­–ç•¥ç»“åˆçŸ¥è¯†å›¾è°±ï¼Œå¯ä»¥ç”¨æ˜¾è‘—æ›´å°‘çš„æ•°æ®ä»é›¶å¼€å§‹è®­ç»ƒå‡ºå¼ºå¤§çš„CLIPæ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä»…ç”¨1000ä¸‡å¼ å›¾åƒå³å¯æ„å»ºå‡ºé’ˆå¯¹ç”Ÿç‰©ä½“çš„ä¸“å®¶åŸºç¡€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†EntityNetæ•°æ®é›†ï¼ŒåŒ…å«3300ä¸‡å¼ å›¾åƒå’Œ4600ä¸‡æ¡æ–‡æœ¬æè¿°ï¼Œæ˜¾è‘—ç¼©çŸ­äº†é€šç”¨CLIPæ¨¡å‹çš„è®­ç»ƒæ—¶é—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é«˜è´¨é‡CLIPæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡åºå¤§è¿™ä¸€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç‰¹å®šé¢†åŸŸçš„åº”ç”¨å—é™ï¼Œä¸”è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»“åˆçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½ç½‘ç»œæœç´¢ç­–ç•¥ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®é‡æ˜¾è‘—å‡å°‘çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶è®­ç»ƒå‡ºæœ‰æ•ˆçš„CLIPæ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€çŸ¥è¯†å›¾è°±æ„å»ºã€æ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡çŸ¥è¯†å›¾è°±ä¼˜åŒ–ç½‘ç»œæœç´¢ï¼Œæ”¶é›†ç›¸å…³æ•°æ®ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨æ”¶é›†çš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥çŸ¥è¯†å›¾è°±æ¥ä¼˜åŒ–æ•°æ®æ”¶é›†è¿‡ç¨‹ï¼Œä½¿å¾—è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡å¤§å¹…å‡å°‘ï¼Œä¸”èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ”¶é›†é˜¶æ®µï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æœç´¢ç­–ç•¥ä»¥ç¡®ä¿æ•°æ®çš„å¤šæ ·æ€§å’Œç›¸å…³æ€§ï¼›åœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œä½¿ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ©ç”¨ä»…1000ä¸‡å¼ å›¾åƒæˆåŠŸè®­ç»ƒå‡ºé’ˆå¯¹ç”Ÿç‰©ä½“çš„CLIPæ¨¡å‹ï¼Œæ€§èƒ½ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼ŒEntityNetæ•°æ®é›†çš„å¼•å…¥ä½¿å¾—é€šç”¨CLIPæ¨¡å‹çš„è®­ç»ƒæ—¶é—´æ˜¾è‘—ç¼©çŸ­ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”Ÿç‰©è¯†åˆ«ã€åŒ»å­¦å½±åƒåˆ†æå’Œç¯å¢ƒç›‘æµ‹ç­‰ã€‚é€šè¿‡å‡å°‘è®­ç»ƒæ•°æ®çš„éœ€æ±‚ï¼Œç ”ç©¶å¯ä»¥é™ä½æ¨¡å‹å¼€å‘çš„æˆæœ¬å’Œæ—¶é—´ï¼Œä¿ƒè¿›ç§‘å­¦ç ”ç©¶å’ŒæŠ€æœ¯åº”ç”¨çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´å¤šé¢†åŸŸç‰¹å®šæ¨¡å‹çš„å¿«é€Ÿå¼€å‘ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.

