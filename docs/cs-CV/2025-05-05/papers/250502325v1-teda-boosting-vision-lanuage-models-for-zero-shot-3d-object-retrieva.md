---
layout: default
title: TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment
---

# TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02325" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02325v1</a>
  <a href="https://arxiv.org/pdf/2505.02325.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02325v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02325v1', 'TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhichuan Wang, Yang Zhou, Jinhai Xiang, Yulong Wang, Xinwei He

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: Accepted by ICMR 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wangzhichuan123/TeDA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTeDAä»¥è§£å†³æœªçŸ¥ç±»åˆ«3Dç‰©ä½“æ£€ç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dç‰©ä½“æ£€ç´¢` `è§†è§‰-è¯­è¨€æ¨¡å‹` `é›¶-shotå­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `è‡ªå¢å¼ºç­–ç•¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨3Dç‰©ä½“æ£€ç´¢ä¸­é¢ä¸´è®­ç»ƒæ•°æ®ä¸è¶³å’Œç±»åˆ«æœªçŸ¥çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºçš„TeDAæ¡†æ¶é€šè¿‡å°†3Dç‰©ä½“æŠ•å½±ä¸ºå¤šè§†å›¾å›¾åƒï¼Œå¹¶åˆ©ç”¨CLIPè¿›è¡Œç‰¹å¾æå–ï¼Œè§£å†³äº†2Dä¸3Dåˆ†å¸ƒå·®å¼‚çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTeDAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨æœªçŸ¥ç±»åˆ«æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­¦ä¹ å…·æœ‰åŒºåˆ†æ€§çš„3Dè¡¨ç¤ºä»¥é€‚åº”æœªçŸ¥æµ‹è¯•ç±»åˆ«æ˜¯è®¸å¤šç°å®ä¸–ç•Œ3Dåº”ç”¨çš„æ–°å…´éœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•å› ç¼ºä¹å¹¿æ³›æ¦‚å¿µçš„3Dè®­ç»ƒæ•°æ®è€Œéš¾ä»¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚é¢„è®­ç»ƒçš„å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨é›¶-shotæ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æå–é€‚åˆçš„3Dè¡¨ç¤ºæ—¶å—åˆ°2Dè®­ç»ƒä¸3Dæµ‹è¯•åˆ†å¸ƒä¹‹é—´å·¨å¤§å·®è·çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†æµ‹è¯•æ—¶åˆ†å¸ƒå¯¹é½ï¼ˆTeDAï¼‰æ¡†æ¶ï¼Œé¦–æ¬¡ç ”ç©¶äº†è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨3Dç‰¹å¾å­¦ä¹ ä¸­çš„æµ‹è¯•æ—¶é€‚åº”ã€‚TeDAå°†3Dç‰©ä½“æŠ•å½±ä¸ºå¤šè§†å›¾å›¾åƒï¼Œåˆ©ç”¨CLIPæå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡è‡ªå¢å¼ºæ–¹å¼å¯¹è‡ªä¿¡çš„æŸ¥è¯¢-ç›®æ ‡æ ·æœ¬å¯¹è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»è€Œä¼˜åŒ–3DæŸ¥è¯¢åµŒå…¥ã€‚æ­¤å¤–ï¼ŒTeDAè¿˜æ•´åˆäº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆInternVLï¼‰ç”Ÿæˆçš„æ–‡æœ¬æè¿°ï¼Œä»¥å¢å¼ºå¯¹3Dç‰©ä½“çš„ç†è§£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTeDAåœ¨å››ä¸ªå¼€æ”¾é›†3Dç‰©ä½“æ£€ç´¢åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨æœªçŸ¥ç±»åˆ«ä¸‹è¿›è¡Œ3Dç‰©ä½“æ£€ç´¢çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å› ç¼ºä¹è¶³å¤Ÿçš„3Dè®­ç»ƒæ•°æ®å’Œ2Dä¸3Dåˆ†å¸ƒä¹‹é—´çš„å·®è·ï¼Œéš¾ä»¥å®ç°è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTeDAæ¡†æ¶é€šè¿‡åœ¨æµ‹è¯•æ—¶å¯¹é¢„è®­ç»ƒçš„2Dè§†è§‰-è¯­è¨€æ¨¡å‹CLIPè¿›è¡Œé€‚åº”ï¼Œåˆ©ç”¨å¤šè§†å›¾å›¾åƒæå–3Dç‰¹å¾ï¼Œå¹¶é€šè¿‡è‡ªå¢å¼ºç­–ç•¥ä¼˜åŒ–æŸ¥è¯¢åµŒå…¥ï¼Œä»¥æé«˜æ£€ç´¢æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTeDAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œå°†3Dç‰©ä½“æŠ•å½±ä¸ºå¤šè§†å›¾å›¾åƒï¼›å…¶æ¬¡ï¼Œä½¿ç”¨CLIPæå–è¿™äº›å›¾åƒçš„ç‰¹å¾ï¼›æœ€åï¼Œé€šè¿‡è‡ªä¿¡çš„æŸ¥è¯¢-ç›®æ ‡æ ·æœ¬å¯¹è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»¥æå‡3DæŸ¥è¯¢çš„åµŒå…¥è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šTeDAæ˜¯é¦–ä¸ªç ”ç©¶è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨3Dç‰¹å¾å­¦ä¹ ä¸­çš„æµ‹è¯•æ—¶é€‚åº”çš„å·¥ä½œï¼Œåˆ›æ–°æ€§åœ°å°†2Dä¸3Dç‰¹å¾æå–ç»“åˆï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•çš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒTeDAé‡‡ç”¨äº†è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨è‡ªä¿¡çš„æ ·æœ¬å¯¹è¿›è¡Œç‰¹å¾å¢å¼ºï¼ŒåŒæ—¶æ•´åˆäº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æè¿°ï¼Œä»¥ä¸°å¯Œ3Dç‰©ä½“çš„ç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å››ä¸ªå¼€æ”¾é›†3Dç‰©ä½“æ£€ç´¢åŸºå‡†ä¸Šï¼ŒTeDAçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ŒéªŒè¯äº†å…¶åœ¨æœªçŸ¥ç±»åˆ«æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œåœ¨Objaverse-LVISæ•°æ®é›†ä¸Šä½¿ç”¨æ·±åº¦å›¾çš„å®éªŒè¿›ä¸€æ­¥æ”¯æŒäº†TeDAçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæŠ“å–ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æœªçŸ¥ç±»åˆ«3Dç‰©ä½“çš„æ£€ç´¢èƒ½åŠ›ï¼ŒTeDAèƒ½å¤Ÿä¿ƒè¿›æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–ä¸æ“ä½œï¼Œæå‡å…¶é€‚åº”æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning discriminative 3D representations that generalize well to unknown testing categories is an emerging requirement for many real-world 3D applications. Existing well-established methods often struggle to attain this goal due to insufficient 3D training data from broader concepts. Meanwhile, pre-trained large vision-language models (e.g., CLIP) have shown remarkable zero-shot generalization capabilities. Yet, they are limited in extracting suitable 3D representations due to substantial gaps between their 2D training and 3D testing distributions. To address these challenges, we propose Testing-time Distribution Alignment (TeDA), a novel framework that adapts a pretrained 2D vision-language model CLIP for unknown 3D object retrieval at test time. To our knowledge, it is the first work that studies the test-time adaptation of a vision-language model for 3D feature learning. TeDA projects 3D objects into multi-view images, extracts features using CLIP, and refines 3D query embeddings with an iterative optimization strategy by confident query-target sample pairs in a self-boosting manner. Additionally, TeDA integrates textual descriptions generated by a multimodal language model (InternVL) to enhance 3D object understanding, leveraging CLIP's aligned feature space to fuse visual and textual cues. Extensive experiments on four open-set 3D object retrieval benchmarks demonstrate that TeDA greatly outperforms state-of-the-art methods, even those requiring extensive training. We also experimented with depth maps on Objaverse-LVIS, further validating its effectiveness. Code is available at https://github.com/wangzhichuan123/TeDA.

