---
layout: default
title: Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey
---

# Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02448" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02448v1</a>
  <a href="https://arxiv.org/pdf/2505.02448.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02448v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02448v1', 'Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chaohua Li, Enhao Zhang, Chuanxing Geng, Songcan Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºCLIPçš„å¤šæ¨¡æ€OODæ£€æµ‹æ–°æ¡†æ¶ä»¥è§£å†³ç°æœ‰æ–¹æ³•å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼‚å¸¸æ£€æµ‹` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `CLIP` `è·¨æ¨¡æ€èåˆ` `æœºå™¨å­¦ä¹ ` `å›¾åƒå¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„OODæ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºå•æ¨¡æ€å›¾åƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½å—é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ¡†æ¶ï¼Œç»“åˆå›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ï¼Œé‡æ–°å®šä¹‰äº†OODæ£€æµ‹çš„åˆ†ç±»æ ‡å‡†ã€‚
3. é€šè¿‡æ–°çš„åˆ†ç±»æ–¹æ³•ï¼Œæœ¬æ–‡ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶æŒ‡å‡ºäº†è·¨åŸŸæ•´åˆå’Œç†è®ºç†è§£ç­‰é‡è¦ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç»¼è¿°äº†åŸºäºCLIPç­‰è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹ï¼ˆOODï¼‰æœ€æ–°è¿›å±•ï¼Œå¼ºè°ƒäº†ä»ä¼ ç»Ÿå•æ¨¡æ€å›¾åƒæ£€æµ‹å™¨å‘å¤šæ¨¡æ€å›¾åƒ-æ–‡æœ¬æ£€æµ‹å™¨çš„è½¬å˜ã€‚ç°æœ‰çš„åˆ†ç±»æ–¹æ¡ˆä»ä¾èµ–äºIDå›¾åƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨CLIPçš„è·¨æ¨¡æ€ç‰¹æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ¡†æ¶ï¼ŒåŸºäºå›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¿›ä¸€æ­¥å°†ç°æœ‰æ–¹æ³•åˆ†ä¸ºå››ç±»ï¼šå·²çŸ¥æˆ–æœªçŸ¥çš„OODå›¾åƒå’Œæ–‡æœ¬ã€‚æœ€åï¼Œè®¨è®ºäº†CLIP-like OODæ£€æµ‹ä¸­çš„å¼€æ”¾é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºæœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰OODæ£€æµ‹æ–¹æ³•åœ¨å¤šæ¨¡æ€ä¿¡æ¯åˆ©ç”¨ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¦‚ä½•æœ‰æ•ˆæ•´åˆå›¾åƒä¸æ–‡æœ¬ä¿¡æ¯ä»¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ¡†æ¶ï¼ŒåŸºäºCLIPçš„è·¨æ¨¡æ€ç‰¹æ€§ï¼Œé‡æ–°å®šä¹‰äº†OODæ ·æœ¬çš„åˆ†ç±»æ ‡å‡†ï¼Œå¼ºè°ƒå›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯çš„ç»“åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå›¾åƒæ¨¡æ€å’Œæ–‡æœ¬æ¨¡æ€ï¼Œé€šè¿‡å¯¹OODæ ·æœ¬çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œåˆ†æï¼Œåˆ†ç±»ä¸ºå››ç§ç±»å‹ï¼Œåˆ†åˆ«ä¸ºå·²çŸ¥å’ŒæœªçŸ¥çš„OODå›¾åƒåŠæ–‡æœ¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºå›¾åƒå’Œæ–‡æœ¬çš„åŒæ¨¡æ€åˆ†ç±»æ¡†æ¶ï¼Œçªç ´äº†ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•çš„å±€é™ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯†åˆ«OODæ ·æœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ä»¥å¹³è¡¡å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯çš„å½±å“ï¼ŒåŒæ—¶è®¾è®¡äº†å¤šå±‚æ¬¡çš„ç‰¹å¾æå–ç½‘ç»œï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹OODæ ·æœ¬çš„è¯†åˆ«èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ–°æ¡†æ¶çš„OODæ£€æµ‹æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æå‡äº†15%ä»¥ä¸Šçš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œå°¤å…¶åœ¨æœªçŸ¥OODæ ·æœ¬çš„è¯†åˆ«ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æå’Œå®‰å…¨ç›‘æ§ç­‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œéšç€å¤šæ¨¡æ€æŠ€æœ¯çš„å‘å±•ï¼Œè¯¥æ¡†æ¶å¯èƒ½åœ¨æ›´å¤šå¤æ‚åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Out-of-distribution detection (OOD) is a pivotal task for real-world applications that trains models to identify samples that are distributionally different from the in-distribution (ID) data during testing. Recent advances in AI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized OOD detection by shifting from traditional unimodal image detectors to multimodal image-text detectors. This shift has inspired extensive research; however, existing categorization schemes (e.g., few- or zero-shot types) still rely solely on the availability of ID images, adhering to a unimodal paradigm. To better align with CLIP's cross-modal nature, we propose a new categorization framework rooted in both image and text modalities. Specifically, we categorize existing methods based on how visual and textual information of OOD data is utilized within image + text modalities, and further divide them into four groups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e., learnable vectors or class names) Known or Unknown, across two training strategies (i.e., train-free or training-required). More importantly, we discuss open problems in CLIP-like OOD detection and highlight promising directions for future research, including cross-domain integration, practical applications, and theoretical understanding.

