---
layout: default
title: VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection
---

# VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02331" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.02331v2</a>
  <a href="https://arxiv.org/pdf/2505.02331.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02331v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02331v2', 'VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hao Cheng, Zhiwei Zhao, Yichao He, Zhenzhen Hu, Jia Li, Meng Wang, Richang Hong

**ÂàÜÁ±ª**: cs.CV, cs.SD

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-05 (Êõ¥Êñ∞: 2025-08-02)

**Â§áÊ≥®**: Source code and pre-trained models will be available at https://github.com/MSA-LMC/VAEmo

**DOI**: [10.1145/3746027.3754924](https://doi.org/10.1145/3746027.3754924)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VAEmo‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊÉÖÊÑüËØÜÂà´‰∏≠ÁöÑÊï∞ÊçÆÁ®ÄÁº∫‰∏éË°®ËææÂ∑ÆÂºÇÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊÉÖÊÑüËØÜÂà´` `ËßÜËßâ-Èü≥È¢ëËûçÂêà` `Áü•ËØÜÊ≥®ÂÖ•` `Ëá™ÁõëÁù£Â≠¶‰π†` `ÊÉÖÊÑüËØ≠‰πâÂª∫Ê®°` `ÂØπÊØîÂ≠¶‰π†` `Â§ßËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÊÉÖÊÑüËØÜÂà´ÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÊ®°ÊÄÅÁâπÂÆöÁºñÁ†ÅÂô®ÂíåÁ≤óÁï•ÁöÑÂÜÖÂÆπÁ∫ßÂØπÈΩêÔºåÈôêÂà∂‰∫ÜÊÉÖÊÑüËØ≠‰πâÁöÑÁªÜÁ≤íÂ∫¶Âª∫Ê®°„ÄÇ
2. VAEmoÈÄöËøá‰∏§Èò∂ÊÆµÊ°ÜÊû∂ÔºåÈ¶ñÂÖàÂú®Â§ßËßÑÊ®°VAËØ≠ÊñôÂ∫ì‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊÉÖÊÑüÊèèËø∞ÔºåÊ≥®ÂÖ•Â§ñÈÉ®Áü•ËØÜ‰ª•ÊèêÂçáË°®Á§∫ËÉΩÂäõ„ÄÇ
3. Âú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÔºåVAEmoÂ±ïÁé∞Âá∫‰ºòË∂äÁöÑÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂Á¥ßÂáëËÆæËÆ°ÂíåË∑®Ê®°ÊÄÅÁºñÁ†ÅÁöÑÊúâÊïàÊÄßÔºåÊòæËëóÊèêÂçá‰∫ÜÊÉÖÊÑüËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÊÉÖÊÑüËØÜÂà´ÔºàAVERÔºâÊó®Âú®ÈÄöËøáÈùûËØ≠Ë®ÄÁöÑËßÜËßâ-Èü≥È¢ëÔºàVAÔºâÁ∫øÁ¥¢Êé®Êñ≠‰∫∫Á±ªÊÉÖÊÑüÔºåÂÖ∑Êúâ‰∫íË°•ÁöÑÊ®°ÊÄÅ‰ºòÂäøÂíåËØ≠Ë®ÄÊó†ÂÖ≥ÊÄß„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÊÉÖÊÑüË°®ËææÁöÑÂõ∫ÊúâÊ®°Á≥äÊÄß„ÄÅË∑®Ê®°ÊÄÅË°®ËææÂ∑ÆÂºÇ‰ª•ÂèäÂèØÈù†Ê†áÊ≥®Êï∞ÊçÆÁöÑÁ®ÄÁº∫ÔºåAVER‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫VAEmoÔºå‰∏Ä‰∏™È´òÊïàÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂ÔºåÈÄöËøáÂ§ñÈÉ®Áü•ËØÜÊ≥®ÂÖ•ÂÆûÁé∞ÊÉÖÊÑü‰∏≠ÂøÉÁöÑVAËÅîÂêàË°®Á§∫Â≠¶‰π†„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÂú®Â§ßËßÑÊ®°ËØ¥ËØùËÄÖ‰∏≠ÂøÉÁöÑVAËØ≠ÊñôÂ∫ì‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁ¨¨‰∫åÈò∂ÊÆµÈÄöËøáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêËØ¶ÁªÜÁöÑÊÉÖÊÑüÊèèËø∞ÔºåÂπ∂ÈÄöËøáÂèåË∑ØÂæÑÂØπÊØîÂ≠¶‰π†Â∞ÜÂÖ∂‰∏éVAË°®Á§∫ÂØπÈΩê„ÄÇÂÆûÈ™åË°®ÊòéÔºåVAEmoÂú®Â§ö‰∏™‰∏ãÊ∏∏AVERÂü∫ÂáÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÁ™ÅÂá∫‰∫ÜÁªü‰∏ÄË∑®Ê®°ÊÄÅÁºñÁ†ÅÂíåÊÉÖÊÑüÊÑüÁü•ËØ≠‰πâÊåáÂØºÁöÑ‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊÉÖÊÑüËØÜÂà´‰∏≠ÁöÑÊï∞ÊçÆÁ®ÄÁº∫„ÄÅÊÉÖÊÑüË°®ËææÊ®°Á≥äÂèäË∑®Ê®°ÊÄÅË°®ËææÂ∑ÆÂºÇÁ≠âÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÊÉÖÊÑüËØ≠‰πâÂª∫Ê®°‰∏äÂ≠òÂú®Â±ÄÈôêÊÄßÔºåÈöæ‰ª•ÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊÉÖÊÑüÁêÜËß£„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVAEmoÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøá‰∏§Èò∂ÊÆµÊ°ÜÊû∂ËøõË°åÊÉÖÊÑü‰∏≠ÂøÉÁöÑVAËÅîÂêàË°®Á§∫Â≠¶‰π†„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÈÄöËøáÊó†ÊÉÖÊÑüÊ†áÁ≠æÁöÑÈ¢ÑËÆ≠ÁªÉÂ≠¶‰π†Ë°®ËææÊÄßÂíå‰∫íË°•ÁöÑË°®Á§∫ÔºåÁ¨¨‰∫åÈò∂ÊÆµÈÄöËøáÁîüÊàêÁöÑÊÉÖÊÑüÊèèËø∞Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫Ë°®Á§∫ÁöÑÊÉÖÊÑüËØ≠‰πâ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVAEmoÁöÑÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÊòØ‰∏Ä‰∏™Áªü‰∏Ä‰∏îËΩªÈáèÁöÑË°®Á§∫ÁΩëÁªúÔºåÂú®Â§ßËßÑÊ®°VAËØ≠ÊñôÂ∫ì‰∏äËøõË°åÊé©Á†ÅÈáçÂª∫ÂíåÂØπÊØîÁõÆÊ†áÁöÑÈ¢ÑËÆ≠ÁªÉÔºõÁ¨¨‰∫åÈò∂ÊÆµÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊÉÖÊÑüÊèèËø∞ÔºåÂπ∂ÈÄöËøáÂèåË∑ØÂæÑÂØπÊØîÂ≠¶‰π†Â∞ÜÊñáÊú¨ËØ≠‰πâ‰∏éVAË°®Á§∫ÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVAEmoÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂºïÂÖ•Â§ñÈÉ®Áü•ËØÜÊ≥®ÂÖ•Êú∫Âà∂ÔºåÈÄöËøáÂØπÈΩêÊñáÊú¨ÂíåVAË°®Á§∫ÔºåÂº•Ë°•‰∫ÜÊÉÖÊÑüË°®ËææÁöÑÊ®°ÊÄÅÂ∑ÆÂºÇ„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†Áªü‰æùËµñÊ®°ÊÄÅÁâπÂÆöÁºñÁ†ÅÂô®ÁöÑÊñπÂºèÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÊé©Á†ÅÈáçÂª∫ÂíåÂØπÊØîÂ≠¶‰π†ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁ°Æ‰øù‰∫ÜË°®Á§∫ÁöÑ‰∏∞ÂØåÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇÂêåÊó∂ÔºåÁΩëÁªúÁªìÊûÑÁªèËøá‰ºòÂåñÔºå‰ª•ÂÆûÁé∞È´òÊïàÁöÑÊÉÖÊÑüË°®Á§∫Â≠¶‰π†„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÂ±ÇÊ¨°ÁªìÊûÑÁöÑÁªÜËäÇÂú®ÂÆûÈ™åÈÉ®ÂàÜËøõË°å‰∫ÜËØ¶ÁªÜËØ¥Êòé„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™‰∏ãÊ∏∏AVERÂü∫ÂáÜÊµãËØï‰∏≠ÔºåVAEmoÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊÉÖÊÑüËØÜÂà´ÂáÜÁ°ÆÁéáÊèêÂçá‰∫ÜÊòæËëóÁöÑX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÊú™Áü•ÔºâÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Áªü‰∏ÄË∑®Ê®°ÊÄÅÁºñÁ†ÅÂíåÊÉÖÊÑüÊÑüÁü•ËØ≠‰πâÊåáÂØºÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VAEmoÁöÑÁ†îÁ©∂ÊàêÊûúÂú®ÊÉÖÊÑüËÆ°ÁÆó„ÄÅÊô∫ËÉΩ‰∫∫Êú∫‰∫§‰∫í„ÄÅÂøÉÁêÜÂÅ•Â∫∑ÁõëÊµãÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÊÉÖÊÑüËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄßÔºåËØ•ÊäÄÊúØËÉΩÂ§ü‰∏∫ÊÉÖÊÑüÈ©±Âä®ÁöÑÂ∫îÁî®Êèê‰æõÊõ¥‰∏∫Á≤æÂáÜÁöÑÊîØÊåÅÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï‰∏éÊôÆÂèä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Audiovisual emotion recognition (AVER) aims to infer human emotions from nonverbal visual-audio (VA) cues, offering modality-complementary and language-agnostic advantages. However, AVER remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. Recent self-supervised AVER approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. To address these issues, we propose VAEmo, an efficient two-stage framework for emotion-centric joint VA representation learning with external knowledge injection. In Stage~1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric VA corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. In Stage~2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of VA samples; these rich textual semantics are then injected by aligning their corresponding embeddings with VA representations through dual-path contrastive learning, further bridging the emotion gap. Extensive experiments on multiple downstream AVER benchmarks show that VAEmo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable VA emotion representations.

