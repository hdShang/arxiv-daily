---
layout: default
title: "No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves"
---

# No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02831" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02831v4</a>
  <a href="https://arxiv.org/pdf/2505.02831.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02831v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02831v4', 'No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-05-17)

**å¤‡æ³¨**: Self-Representation Alignment for Diffusion Transformers. Code: https://github.com/vvvvvjdy/SRA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæˆ‘è¡¨å¾å¯¹é½æ–¹æ³•ä»¥ä¼˜åŒ–æ‰©æ•£å˜æ¢å™¨çš„ç”Ÿæˆè´¨é‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ‰©æ•£å˜æ¢å™¨` `è‡ªæˆ‘è’¸é¦` `è¡¨å¾å­¦ä¹ ` `ç”Ÿæˆæ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚çš„å¤–éƒ¨è¡¨å¾è®­ç»ƒæ¡†æ¶æˆ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œé™åˆ¶äº†æ‰©æ•£å˜æ¢å™¨çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚
2. æœ¬æ–‡æå‡ºè‡ªæˆ‘è¡¨å¾å¯¹é½ï¼ˆSRAï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘è’¸é¦åœ¨ç”Ÿæˆè®­ç»ƒä¸­å®ç°è¡¨å¾æŒ‡å¯¼ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSRAåœ¨å¤šç§æ‰©æ•£å˜æ¢å™¨ä¸Šå‡æ˜¾è‘—æå‡æ€§èƒ½ï¼Œè¶…è¶Šä¼ ç»Ÿæ–¹æ³•å¹¶ä¸ä¾èµ–å¤–éƒ¨å…ˆéªŒçš„æ–¹æ³•ç›¸å½“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå­¦ä¹ æœ‰æ„ä¹‰çš„å†…éƒ¨è¡¨å¾å¯ä»¥åŠ é€Ÿç”Ÿæˆè®­ç»ƒå¹¶æå‡æ‰©æ•£å˜æ¢å™¨çš„ç”Ÿæˆè´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éœ€å¼•å…¥å¤æ‚çš„å¤–éƒ¨è¡¨å¾è®­ç»ƒæ¡†æ¶æˆ–ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è¡¨å¾åŸºç¡€æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºè‡ªæˆ‘è¡¨å¾å¯¹é½ï¼ˆSRAï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨çš„ç‹¬ç‰¹åˆ¤åˆ«è¿‡ç¨‹ï¼Œåœ¨ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹ä¸­è‡ªæˆ‘è’¸é¦è·å–è¡¨å¾æŒ‡å¯¼ã€‚SRAé€šè¿‡å°†æ—©æœŸå±‚è¾“å‡ºçš„é«˜å™ªå£°æ½œåœ¨è¡¨å¾ä¸åæœŸå±‚çš„ä½å™ªå£°è¡¨å¾å¯¹é½ï¼Œé€æ­¥å¢å¼ºæ•´ä½“è¡¨å¾å­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSRAåœ¨DiTså’ŒSiTsä¸Šå‡æ˜¾è‘—æå‡æ€§èƒ½ï¼Œè¶…è¶Šä¾èµ–å¤æ‚å¤–éƒ¨æ¡†æ¶çš„æ–¹æ³•ï¼Œå¹¶ä¸ä¾èµ–å¼ºå¤§å¤–éƒ¨è¡¨å¾å…ˆéªŒçš„æ–¹æ³•è¡¨ç°ç›¸å½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æ‰©æ•£å˜æ¢å™¨åœ¨ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦ä¾èµ–å¤–éƒ¨å¤æ‚çš„è¡¨å¾è®­ç»ƒæ¡†æ¶æˆ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œçµæ´»æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„è‡ªæˆ‘è¡¨å¾å¯¹é½ï¼ˆSRAï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨çš„åˆ¤åˆ«ç‰¹æ€§ï¼Œé€šè¿‡è‡ªæˆ‘è’¸é¦çš„æ–¹å¼åœ¨ç”Ÿæˆè®­ç»ƒä¸­å®ç°è¡¨å¾æŒ‡å¯¼ï¼Œé¿å…äº†å¤–éƒ¨ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSRAæ–¹æ³•çš„æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆåœ¨ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè·å–æ—©æœŸå±‚çš„é«˜å™ªå£°æ½œåœ¨è¡¨å¾ï¼›ç„¶åå°†å…¶ä¸åæœŸå±‚çš„ä½å™ªå£°è¡¨å¾è¿›è¡Œå¯¹é½ï¼Œä»¥å¢å¼ºè¡¨å¾å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šSRAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è‡ªæˆ‘è’¸é¦æœºåˆ¶ï¼Œä½¿å¾—æ‰©æ•£å˜æ¢å™¨èƒ½å¤Ÿåœ¨æ²¡æœ‰å¤–éƒ¨è¡¨å¾ç»„ä»¶çš„æƒ…å†µä¸‹ï¼Œç‹¬ç«‹æä¾›è¡¨å¾æŒ‡å¯¼ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„ä¾èµ–å…³ç³»å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SRAä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬å¯¹æ½œåœ¨è¡¨å¾çš„å™ªå£°æ°´å¹³è¿›è¡Œæ§åˆ¶ï¼Œä»¥åŠåœ¨å¯¹é½è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°ï¼Œè¿™äº›è®¾è®¡ç¡®ä¿äº†è¡¨å¾å­¦ä¹ çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSRAæ–¹æ³•åœ¨DiTså’ŒSiTsä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¾èµ–å¤æ‚å¤–éƒ¨æ¡†æ¶çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šä¸ä¾èµ–å¼ºå¤§å¤–éƒ¨è¡¨å¾å…ˆéªŒçš„æ–¹æ³•ç›¸å½“ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç”Ÿæˆã€è§†é¢‘ç”Ÿæˆå’Œå…¶ä»–éœ€è¦é«˜è´¨é‡ç”Ÿæˆæ¨¡å‹çš„ä»»åŠ¡ã€‚é€šè¿‡ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼ŒSRAæ–¹æ³•èƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçš„ç”Ÿæˆæ¨¡å‹è®­ç»ƒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance the generation quality of diffusion transformers. However, existing approaches necessitate to either introduce an external and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation Alignment (SRA), a simple yet straightforward method that obtains representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in the earlier layer with higher noise to that in the later layer with lower noise to progressively enhance the overall representation learning during only the generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that are heavily dependent on powerful external representation priors.

