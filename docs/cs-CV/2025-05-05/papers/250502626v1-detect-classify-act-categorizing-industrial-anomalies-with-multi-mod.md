---
layout: default
title: Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models
---

# Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02626" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02626v1</a>
  <a href="https://arxiv.org/pdf/2505.02626.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02626v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02626v1', 'Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sassan Mokhtar, Arian Mousakhan, Silvio Galesso, Jawad Tayyub, Thomas Brox

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: Accepted as a spotlight presentation paper at the VAND Workshop, CVPR 2025. 10 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVELMä»¥è§£å†³å·¥ä¸šå¼‚å¸¸åˆ†ç±»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å·¥ä¸šå¼‚å¸¸æ£€æµ‹` `å¼‚å¸¸åˆ†ç±»` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ— ç›‘ç£å­¦ä¹ ` `æ•°æ®é›†æ„å»º` `è§†è§‰æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼‚å¸¸åˆ†ç±»æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´æ•°æ®é›†ç¼ºä¹ç²¾ç¡®æ³¨é‡Šçš„é—®é¢˜ï¼Œå¯¼è‡´åˆ†ç±»æ•ˆæœä¸ç†æƒ³ã€‚
2. æœ¬æ–‡æå‡ºçš„VELMæ–¹æ³•ç»“åˆæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¿«é€Ÿæœ‰æ•ˆåœ°è¿›è¡Œå¼‚å¸¸åˆ†ç±»ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVELMåœ¨MVTec-ADå’ŒMVTec-ACæ•°æ®é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†5%å’Œæ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè§†è§‰å·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„è¿›å±•åœ¨è¯†åˆ«å’Œåˆ†å‰²å¼‚å¸¸åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¼‚å¸¸åˆ†ç±»çš„ç ”ç©¶ä»ç„¶ç›¸å¯¹ä¸è¶³ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼‚å¸¸åˆ†ç±»ç®¡é“VELMã€‚è¯¥æ–¹æ³•é¦–å…ˆåº”ç”¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æŠ€æœ¯è¯„ä¼°è§‚å¯Ÿç»“æœçš„æ­£å¸¸æ€§ï¼Œè‹¥æ£€æµ‹åˆ°å¼‚å¸¸ï¼ŒLLMå°†å¯¹å…¶ç±»å‹è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ•°æ®é›†ä¸­ç¼ºä¹ç²¾ç¡®å¼‚å¸¸ç±»æ³¨é‡Šçš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†MVTec-ACå’ŒVisA-ACæ•°æ®é›†ï¼Œæä¾›å‡†ç¡®çš„å¼‚å¸¸ç±»æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVELMåœ¨MVTec-ADå’ŒMVTec-ACæ•°æ®é›†ä¸Šçš„å¼‚å¸¸åˆ†ç±»å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†80.4%å’Œ84%ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨ç†è§£å’Œåˆ†ç±»å¼‚å¸¸æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å·¥ä¸šå¼‚å¸¸åˆ†ç±»ä¸­çš„å…³é”®é—®é¢˜ï¼Œå³ç°æœ‰æ–¹æ³•åœ¨å¼‚å¸¸ç±»å‹åŒºåˆ†ä¸Šå­˜åœ¨ä¸è¶³ï¼Œä¸”ç¼ºä¹ç²¾ç¡®çš„å¼‚å¸¸ç±»æ³¨é‡Šæ•°æ®é›†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„VELMæ–¹æ³•é€šè¿‡ç»“åˆæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé¦–å…ˆè¯„ä¼°è§‚å¯Ÿç»“æœçš„æ­£å¸¸æ€§ï¼Œè‹¥å‘ç°å¼‚å¸¸ï¼Œåˆ™è¿›è¡Œç±»å‹åˆ†ç±»ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜åˆ†ç±»çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVELMçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç¬¬ä¸€æ¨¡å—ä¸ºæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œè´Ÿè´£åˆ¤æ–­è§‚å¯Ÿç»“æœæ˜¯å¦æ­£å¸¸ï¼›ç¬¬äºŒæ¨¡å—ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè´Ÿè´£å¯¹æ£€æµ‹åˆ°çš„å¼‚å¸¸è¿›è¡Œåˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†MVTec-ACå’ŒVisA-ACæ•°æ®é›†ï¼Œæä¾›äº†å‡†ç¡®çš„å¼‚å¸¸ç±»æ ‡ç­¾ï¼Œä»è€Œä½¿å¾—å¼‚å¸¸åˆ†ç±»çš„è¯„ä¼°æ›´åŠ ä¸¥æ ¼å’Œæœ‰æ•ˆã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVELMåœ¨åˆ†ç±»å‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†ç±»æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”å¤šæ¨¡æ€æ•°æ®çš„å¤„ç†éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒVELMåœ¨MVTec-ADæ•°æ®é›†ä¸Šçš„å¼‚å¸¸åˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°äº†80.4%ï¼Œç›¸æ¯”ä¹‹å‰çš„åŸºçº¿æé«˜äº†5%ï¼›åœ¨MVTec-ACæ•°æ®é›†ä¸Šï¼Œå‡†ç¡®ç‡æ›´æ˜¯è¾¾åˆ°äº†84%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVELMåœ¨å¼‚å¸¸ç†è§£å’Œåˆ†ç±»æ–¹é¢çš„æœ‰æ•ˆæ€§æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šç”Ÿäº§ç›‘æ§ã€è´¨é‡æ£€æµ‹å’Œè®¾å¤‡ç»´æŠ¤ç­‰ã€‚é€šè¿‡æé«˜å¼‚å¸¸åˆ†ç±»çš„å‡†ç¡®æ€§ï¼ŒVELMèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šæ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œå¤„ç†ç”Ÿäº§è¿‡ç¨‹ä¸­çš„å¼‚å¸¸æƒ…å†µï¼Œä»è€Œé™ä½æŸå¤±å’Œæé«˜ç”Ÿäº§æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å…¶ä»–é¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹ä¸åˆ†ç±»ä»»åŠ¡ä¸­å¾—åˆ°æ¨å¹¿ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in visual industrial anomaly detection have demonstrated exceptional performance in identifying and segmenting anomalous regions while maintaining fast inference speeds. However, anomaly classification-distinguishing different types of anomalies-remains largely unexplored despite its critical importance in real-world inspection tasks. To address this gap, we propose VELM, a novel LLM-based pipeline for anomaly classification. Given the critical importance of inference speed, we first apply an unsupervised anomaly detection method as a vision expert to assess the normality of an observation. If an anomaly is detected, the LLM then classifies its type. A key challenge in developing and evaluating anomaly classification models is the lack of precise annotations of anomaly classes in existing datasets. To address this limitation, we introduce MVTec-AC and VisA-AC, refined versions of the widely used MVTec-AD and VisA datasets, which include accurate anomaly class labels for rigorous evaluation. Our approach achieves a state-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD, exceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the effectiveness of VELM in understanding and categorizing anomalies. We hope our methodology and benchmark inspire further research in anomaly classification, helping bridge the gap between detection and comprehensive anomaly characterization.

