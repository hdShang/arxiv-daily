---
layout: default
title: R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning
---

# R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02835" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02835v2</a>
  <a href="https://arxiv.org/pdf/2505.02835.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02835v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02835v2', 'R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, Haojie Ding, Jiankang Chen, Fan Yang, Zhang Zhang, Tingting Gao, Liang Wang

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05 (æ›´æ–°: 2025-05-09)

**å¤‡æ³¨**: Home page: https://github.com/yfzhang114/r1_reward

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR1-Rewardä»¥è§£å†³å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡ä¸ç¨³å®šé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `é•¿æœŸæ¨ç†` `StableReinforceç®—æ³•` `æ¨¡å‹è®­ç»ƒ` `æ€§èƒ½æå‡` `æ•°æ®æ”¶é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•åœ¨åº”ç”¨å¼ºåŒ–å­¦ä¹ æ—¶ï¼Œå¸¸å¸¸é¢ä¸´è®­ç»ƒä¸ç¨³å®šæˆ–å´©æºƒçš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶é•¿æœŸæ¨ç†èƒ½åŠ›çš„å‘æŒ¥ã€‚
2. æœ¬æ–‡æå‡ºå°†å¥–åŠ±å»ºæ¨¡é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶å¼•å…¥StableReinforceç®—æ³•ä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚
3. R1-Rewardæ¨¡å‹åœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ä¼˜åŒ–å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ä¸­çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ€§èƒ½ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚å°½ç®¡è¿‘æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ•°æ®çš„æ”¹è¿›ä¸Šï¼Œä½†å¯¹é•¿æœŸæ¨ç†èƒ½åŠ›åœ¨å¥–åŠ±å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§åŠå…¶æ¿€æ´»æ–¹å¼çš„æ¢ç´¢ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æ”¹å–„å¥–åŠ±å»ºæ¨¡ï¼Œæå‡ºäº†StableReinforceç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–è®­ç»ƒæŸå¤±ã€ä¼˜åŠ¿ä¼°è®¡ç­–ç•¥å’Œå¥–åŠ±è®¾è®¡ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬æ”¶é›†äº†20ä¸‡æ¡æ¥è‡ªå¤šæ ·æ•°æ®é›†çš„åå¥½æ•°æ®ï¼Œä½¿ç”¨StableReinforceç®—æ³•è®­ç»ƒçš„R1-Rewardåœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œåœ¨VL Reward-Benchä¸Šæå‡äº†8.4%ï¼Œåœ¨Multimodal Reward Benchä¸Šæå‡äº†14.3%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±å»ºæ¨¡æ–¹æ³•åœ¨åº”ç”¨å¼ºåŒ–å­¦ä¹ æ—¶çš„ä¸ç¨³å®šæ€§å’Œå´©æºƒé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æœŸæ¨ç†èƒ½åŠ›çš„æ¿€æ´»æ–¹é¢å­˜åœ¨çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†å¥–åŠ±å»ºæ¨¡é—®é¢˜é‡æ–°å®šä¹‰ä¸ºåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œæå‡ºStableReinforceç®—æ³•ï¼Œä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°ã€ä¼˜åŠ¿ä¼°è®¡å’Œå¥–åŠ±è®¾è®¡ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¥–åŠ±å»ºæ¨¡å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ”¶é›†æ¥è‡ªå¤šæ ·æ•°æ®é›†çš„åå¥½æ•°æ®ï¼Œç„¶ååˆ©ç”¨StableReinforceç®—æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šStableReinforceç®—æ³•çš„æå‡ºæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒé€šè¿‡å¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ”¹è¿›ï¼Œå…‹æœäº†è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œä½¿å¾—å¥–åŠ±å»ºæ¨¡æ›´åŠ é«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨StableReinforceç®—æ³•ä¸­ï¼Œä¼˜åŒ–äº†è®­ç»ƒæŸå¤±çš„è®¡ç®—æ–¹å¼ï¼Œæ”¹è¿›äº†ä¼˜åŠ¿ä¼°è®¡ç­–ç•¥ï¼Œå¹¶é‡æ–°è®¾è®¡äº†å¥–åŠ±æœºåˆ¶ï¼Œè¿™äº›è®¾è®¡ç»†èŠ‚å…±åŒä¿ƒè¿›äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½æå‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒR1-Rewardæ¨¡å‹åœ¨VL Reward-Benchä¸Šå®ç°äº†8.4%çš„æ€§èƒ½æå‡ï¼Œåœ¨Multimodal Reward Benchä¸Šå®ç°äº†14.3%çš„æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒStableReinforceç®—æ³•æœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é•¿æœŸæ¨ç†å’Œå¤æ‚å†³ç­–çš„ä»»åŠ¡ä¸­ï¼Œå¦‚æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆå’Œå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€‚æœªæ¥ï¼ŒR1-Rewardæ¨¡å‹å¯èƒ½ä¼šåœ¨å®é™…åº”ç”¨ä¸­æå‡å¤šæ¨¡æ€ç³»ç»Ÿçš„æ™ºèƒ½æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.

