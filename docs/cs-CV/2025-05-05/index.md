---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-05
---

# cs.CVï¼ˆ2025-05-05ï¼‰

ğŸ“Š å…± **25** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ğŸ”—3)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (13 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250502677v2-multimodal-deep-learning-for-stroke-prediction-and-detection-using-r.html">Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ–¹æ³•ä»¥æ”¹å–„ä¸­é£é¢„æµ‹ä¸æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02677v2" data-paper-url="./papers/250502677v2-multimodal-deep-learning-for-stroke-prediction-and-detection-using-r.html" onclick="toggleFavorite(this, '2505.02677v2', 'Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250502830v1-aor-anatomical-ontology-guided-reasoning-for-medical-large-multimoda.html">AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation</a></td>
  <td>æå‡ºè§£å‰–æœ¬ä½“å¼•å¯¼æ¨ç†ä»¥æå‡èƒ¸éƒ¨Xå…‰è§£è¯»èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02830v1" data-paper-url="./papers/250502830v1-aor-anatomical-ontology-guided-reasoning-for-medical-large-multimoda.html" onclick="toggleFavorite(this, '2505.02830v1', 'AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250503846v2-game-learning-multimodal-interactions-via-graph-structures-for-perso.html">GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation</a></td>
  <td>æå‡ºGAMEä»¥è§£å†³çŸ­è§†é¢‘ä¸­ä¸ªæ€§ç‰¹å¾ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03846v2" data-paper-url="./papers/250503846v2-game-learning-multimodal-interactions-via-graph-structures-for-perso.html" onclick="toggleFavorite(this, '2505.03846v2', 'GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250502628v1-deepsparse-a-foundation-model-for-sparse-view-cbct-reconstruction.html">DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction</a></td>
  <td>æå‡ºDeepSparseä»¥è§£å†³ç¨€ç–è§†å›¾CBCTé‡å»ºä¸­çš„é«˜è¾å°„å’Œè®¡ç®—æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02628v1" data-paper-url="./papers/250502628v1-deepsparse-a-foundation-model-for-sparse-view-cbct-reconstruction.html" onclick="toggleFavorite(this, '2505.02628v1', 'DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250502626v1-detect-classify-act-categorizing-industrial-anomalies-with-multi-mod.html">Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models</a></td>
  <td>æå‡ºVELMä»¥è§£å†³å·¥ä¸šå¼‚å¸¸åˆ†ç±»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02626v1" data-paper-url="./papers/250502626v1-detect-classify-act-categorizing-industrial-anomalies-with-multi-mod.html" onclick="toggleFavorite(this, '2505.02626v1', 'Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250502567v5-unified-multimodal-understanding-and-generation-models-advances-chal.html">Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities</a></td>
  <td>æå‡ºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆæ¨¡å‹ä»¥è§£å†³ç‹¬ç«‹æ¼”åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02567v5" data-paper-url="./papers/250502567v5-unified-multimodal-understanding-and-generation-models-advances-chal.html" onclick="toggleFavorite(this, '2505.02567v5', 'Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250502471v3-ming-lite-uni-advancements-in-unified-architecture-for-natural-multi.html">Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</a></td>
  <td>æå‡ºMing-Lite-Uniä»¥è§£å†³å¤šæ¨¡æ€äº¤äº’ç»Ÿä¸€æ¶æ„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02471v3" data-paper-url="./papers/250502471v3-ming-lite-uni-advancements-in-unified-architecture-for-natural-multi.html" onclick="toggleFavorite(this, '2505.02471v3', 'Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250502467v1-timing-is-everything-finding-the-optimal-fusion-points-in-multimodal.html">Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging</a></td>
  <td>æå‡ºåºåˆ—å‰å‘æœç´¢ç®—æ³•ä»¥ä¼˜åŒ–å¤šæ¨¡æ€åŒ»å­¦å½±åƒèåˆæ—¶æœº</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02467v1" data-paper-url="./papers/250502467v1-timing-is-everything-finding-the-optimal-fusion-points-in-multimodal.html" onclick="toggleFavorite(this, '2505.02467v1', 'Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250502393v2-uncertainty-weighted-image-event-multimodal-fusion-for-video-anomaly.html">Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</a></td>
  <td>æå‡ºå›¾åƒ-äº‹ä»¶èåˆæ–¹æ³•ä»¥è§£å†³è§†é¢‘å¼‚å¸¸æ£€æµ‹ä¸­çš„æ—¶åºä¿¡æ¯ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02393v2" data-paper-url="./papers/250502393v2-uncertainty-weighted-image-event-multimodal-fusion-for-video-anomaly.html" onclick="toggleFavorite(this, '2505.02393v2', 'Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250502746v3-using-knowledge-graphs-to-harvest-datasets-for-efficient-clip-model-.html">Using Knowledge Graphs to harvest datasets for efficient CLIP model training</a></td>
  <td>åˆ©ç”¨çŸ¥è¯†å›¾è°±é«˜æ•ˆæ”¶é›†æ•°æ®é›†ä»¥è®­ç»ƒCLIPæ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02746v3" data-paper-url="./papers/250502746v3-using-knowledge-graphs-to-harvest-datasets-for-efficient-clip-model-.html" onclick="toggleFavorite(this, '2505.02746v3', 'Using Knowledge Graphs to harvest datasets for efficient CLIP model training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250502586v3-rgbx-diffusiondet-a-framework-for-multi-modal-rgb-x-object-detection.html">RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet</a></td>
  <td>æå‡ºRGBX-DiffusionDetä»¥è§£å†³å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02586v3" data-paper-url="./papers/250502586v3-rgbx-diffusiondet-a-framework-for-multi-modal-rgb-x-object-detection.html" onclick="toggleFavorite(this, '2505.02586v3', 'RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250502448v1-recent-advances-in-out-of-distribution-detection-with-clip-like-mode.html">Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey</a></td>
  <td>æå‡ºåŸºäºCLIPçš„å¤šæ¨¡æ€OODæ£€æµ‹æ–°æ¡†æ¶ä»¥è§£å†³ç°æœ‰æ–¹æ³•å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02448v1" data-paper-url="./papers/250502448v1-recent-advances-in-out-of-distribution-detection-with-clip-like-mode.html" onclick="toggleFavorite(this, '2505.02448v1', 'Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250502325v1-teda-boosting-vision-lanuage-models-for-zero-shot-3d-object-retrieva.html">TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment</a></td>
  <td>æå‡ºTeDAä»¥è§£å†³æœªçŸ¥ç±»åˆ«3Dç‰©ä½“æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02325v1" data-paper-url="./papers/250502325v1-teda-boosting-vision-lanuage-models-for-zero-shot-3d-object-retrieva.html" onclick="toggleFavorite(this, '2505.02325v1', 'TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250502835v2-r1-reward-training-multimodal-reward-model-through-stable-reinforcem.html">R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning</a></td>
  <td>æå‡ºR1-Rewardä»¥è§£å†³å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡ä¸ç¨³å®šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">reward design</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02835v2" data-paper-url="./papers/250502835v2-r1-reward-training-multimodal-reward-model-through-stable-reinforcem.html" onclick="toggleFavorite(this, '2505.02835v2', 'R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250502331v2-vaemo-efficient-representation-learning-for-visual-audio-emotion-wit.html">VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection</a></td>
  <td>æå‡ºVAEmoä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æ•°æ®ç¨€ç¼ºä¸è¡¨è¾¾å·®å¼‚é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02331v2" data-paper-url="./papers/250502331v2-vaemo-efficient-representation-learning-for-visual-audio-emotion-wit.html" onclick="toggleFavorite(this, '2505.02331v2', 'VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250502527v1-text-to-image-generation-and-editing-a-survey.html">Text to Image Generation and Editing: A Survey</a></td>
  <td>å…¨é¢ç»¼è¿°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸ç¼–è¾‘æŠ€æœ¯</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">classifier-free guidance</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02527v1" data-paper-url="./papers/250502527v1-text-to-image-generation-and-editing-a-survey.html" onclick="toggleFavorite(this, '2505.02527v1', 'Text to Image Generation and Editing: A Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250505495v1-learning-3d-persistent-embodied-world-models.html">Learning 3D Persistent Embodied World Models</a></td>
  <td>æå‡ºæŒä¹…æ€§å…·èº«ä¸–ç•Œæ¨¡å‹ä»¥è§£å†³é•¿è¿œè§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05495v1" data-paper-url="./papers/250505495v1-learning-3d-persistent-embodied-world-models.html" onclick="toggleFavorite(this, '2505.05495v1', 'Learning 3D Persistent Embodied World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250502831v4-no-other-representation-component-is-needed-diffusion-transformers-c.html">No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</a></td>
  <td>æå‡ºè‡ªæˆ‘è¡¨å¾å¯¹é½æ–¹æ³•ä»¥ä¼˜åŒ–æ‰©æ•£å˜æ¢å™¨çš„ç”Ÿæˆè´¨é‡</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02831v4" data-paper-url="./papers/250502831v4-no-other-representation-component-is-needed-diffusion-transformers-c.html" onclick="toggleFavorite(this, '2505.02831v4', 'No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>19</td>
  <td><a href="./papers/250502753v1-advancing-generalizable-tumor-segmentation-with-anomaly-aware-open-v.html">Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models</a></td>
  <td>æå‡ºDiffuGTSä»¥è§£å†³è‚¿ç˜¤åˆ†å‰²çš„é€šç”¨æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02753v1" data-paper-url="./papers/250502753v1-advancing-generalizable-tumor-segmentation-with-anomaly-aware-open-v.html" onclick="toggleFavorite(this, '2505.02753v1', 'Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250502704v3-vgld-visually-guided-linguistic-disambiguation-for-monocular-depth-s.html">VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery</a></td>
  <td>æå‡ºVGLDä»¥è§£å†³å•ç›®æ·±åº¦ä¼°è®¡ä¸­çš„è¯­è¨€æ­§ä¹‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02704v3" data-paper-url="./papers/250502704v3-vgld-visually-guided-linguistic-disambiguation-for-monocular-depth-s.html" onclick="toggleFavorite(this, '2505.02704v3', 'VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250502335v1-6d-pose-estimation-on-spoons-and-hands.html">6D Pose Estimation on Spoons and Hands</a></td>
  <td>æå‡ºåŸºäº6Då§¿æ€ä¼°è®¡çš„é¥®é£Ÿç›‘æµ‹ç³»ç»Ÿä»¥è§£å†³é¥®é£Ÿè¡Œä¸ºåˆ†æé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">6D pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02335v1" data-paper-url="./papers/250502335v1-6d-pose-estimation-on-spoons-and-hands.html" onclick="toggleFavorite(this, '2505.02335v1', '6D Pose Estimation on Spoons and Hands')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250502593v1-delta-dense-depth-from-events-and-lidar-using-transformers-attention.html">DELTA: Dense Depth from Events and LiDAR using Transformer's Attention</a></td>
  <td>æå‡ºDELTAä»¥èåˆäº‹ä»¶ç›¸æœºä¸LiDARæ•°æ®è§£å†³æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02593v1" data-paper-url="./papers/250502593v1-delta-dense-depth-from-events-and-lidar-using-transformers-attention.html" onclick="toggleFavorite(this, '2505.02593v1', 'DELTA: Dense Depth from Events and LiDAR using Transformer&#39;s Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250502388v1-metascenes-towards-automated-replica-creation-for-real-world-3d-scan.html">MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans</a></td>
  <td>æå‡ºMetaScenesä»¥è§£å†³çœŸå®ä¸–ç•Œ3Dåœºæ™¯å¤åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim-to-real</span> <span class="paper-tag">embodied AI</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02388v1" data-paper-url="./papers/250502388v1-metascenes-towards-automated-replica-creation-for-real-world-3d-scan.html" onclick="toggleFavorite(this, '2505.02388v1', 'MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250502654v1-sim2real-in-endoscopy-segmentation-with-a-novel-structure-aware-imag.html">Sim2Real in endoscopy segmentation with a novel structure aware image translation</a></td>
  <td>æå‡ºä¸€ç§æ–°é¢–çš„å›¾åƒç¿»è¯‘æ¨¡å‹ä»¥è§£å†³å†…é•œå›¾åƒåˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02654v1" data-paper-url="./papers/250502654v1-sim2real-in-endoscopy-segmentation-with-a-novel-structure-aware-imag.html" onclick="toggleFavorite(this, '2505.02654v1', 'Sim2Real in endoscopy segmentation with a novel structure aware image translation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250502836v1-scenethesis-a-language-and-vision-agentic-framework-for-3d-scene-gen.html">Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation</a></td>
  <td>æå‡ºScenethesisä»¥è§£å†³3Dåœºæ™¯ç”Ÿæˆä¸­çš„ç©ºé—´ç°å®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">physically plausible</span> <span class="paper-tag">penetration</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02836v1" data-paper-url="./papers/250502836v1-scenethesis-a-language-and-vision-agentic-framework-for-3d-scene-gen.html" onclick="toggleFavorite(this, '2505.02836v1', 'Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)