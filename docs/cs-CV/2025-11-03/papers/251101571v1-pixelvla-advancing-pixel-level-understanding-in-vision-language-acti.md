---
layout: default
title: "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model"
---

# PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.01571" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.01571v1</a>
  <a href="https://arxiv.org/pdf/2511.01571.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01571v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.01571v1', 'PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wenqi Liang, Gan Sun, Yao He, Jiahua Dong, Suyan Dai, Ivan Laptev, Salman Khan, Yang Cong

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03

**å¤‡æ³¨**: 17pages,7 figures, 5 tabels

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PixelVLAï¼šé€šè¿‡åƒç´ çº§ç†è§£å’Œå¤šæ¨¡æ€æç¤ºï¼Œæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `åƒç´ çº§ç†è§£` `å¤šæ¨¡æ€æç¤º` `æœºå™¨äººæ§åˆ¶` `è§†è§‰è¿åŠ¨æŒ‡ä»¤è°ƒä¼˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨åƒç´ çº§åœºæ™¯ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”è¿‡åº¦ä¾èµ–æ–‡æœ¬æç¤ºï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. PixelVLAé€šè¿‡å¼•å…¥å¤šå°ºåº¦åƒç´ æ„ŸçŸ¥ç¼–ç å™¨å’Œè§†è§‰æç¤ºç¼–ç å™¨ï¼Œæ”¯æŒåƒç´ çº§æ¨ç†å’Œå¤šæ¨¡æ€æç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPixelVLAåœ¨æ“çºµæˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸”è®­ç»ƒæˆæœ¬æ›´ä½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(VLA)æ­£æˆä¸ºå­¦ä¹ é€šç”¨è§†è§‰è¿åŠ¨æ§åˆ¶ç­–ç•¥çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰çš„VLAä¸»è¦åœ¨å¤§å‹å›¾åƒ-æ–‡æœ¬-åŠ¨ä½œæ•°æ®ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨ä¸¤ä¸ªå…³é”®æ–¹é¢å—åˆ°é™åˆ¶ï¼š(i)å®ƒä»¬éš¾ä»¥è¿›è¡Œåƒç´ çº§åœºæ™¯ç†è§£ï¼Œä»¥åŠ(ii)å®ƒä»¬ä¸¥é‡ä¾èµ–æ–‡æœ¬æç¤ºï¼Œè¿™é™ä½äº†å®ƒä»¬åœ¨ç°å®ç¯å¢ƒä¸­çš„çµæ´»æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PixelVLAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨æ”¯æŒåƒç´ çº§æ¨ç†å’Œæ–‡æœ¬ä¸è§†è§‰è¾“å…¥çš„å¤šæ¨¡æ€æç¤ºçš„VLAæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨ä¸€ä¸ªæ–°çš„è§†è§‰è¿åŠ¨æŒ‡ä»¤è°ƒä¼˜æ¡†æ¶ä¹‹ä¸Šï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¤šå°ºåº¦åƒç´ æ„ŸçŸ¥ç¼–ç å™¨å’Œè§†è§‰æç¤ºç¼–ç å™¨ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒPixelVLAï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µè‡ªåŠ¨æ ‡æ³¨æµç¨‹ï¼Œç”Ÿæˆäº†Pixel-160Kï¼Œè¿™æ˜¯ä¸€ä¸ªä»ç°æœ‰æœºå™¨äººæ•°æ®ä¸­æå–çš„å…·æœ‰åƒç´ çº§æ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†VLAåŸºå‡†å’Œä¸¤ä¸ªVLAæ¨¡å‹å˜ä½“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPixelVLAæ¯”OpenVLAçš„æ“çºµæˆåŠŸç‡æé«˜äº†10.1%-17.8%ï¼Œè€Œä»…éœ€å…¶1.5%çš„é¢„è®­ç»ƒæˆæœ¬ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPixelVLAå¯ä»¥é›†æˆåˆ°ç°æœ‰çš„VLAä¸­ï¼Œä»è€Œåœ¨å¤æ‚çš„ç¯å¢ƒä¸­å®ç°æ›´å‡†ç¡®ã€é«˜æ•ˆå’Œé€šç”¨çš„æœºå™¨äººæ§åˆ¶ã€‚æ•°æ®é›†å’Œä»£ç å°†å¼€æºå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰éš¾ä»¥è¿›è¡Œåƒç´ çº§åˆ«çš„åœºæ™¯ç†è§£ï¼Œå¹¶ä¸”è¿‡åº¦ä¾èµ–æ–‡æœ¬æç¤ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚å’ŒçœŸå®çš„æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç°æœ‰çš„VLAæ¨¡å‹é€šå¸¸åªå…³æ³¨å…¨å±€å›¾åƒç‰¹å¾å’Œæ–‡æœ¬æŒ‡ä»¤ï¼Œå¿½ç•¥äº†å›¾åƒä¸­ç‰¹å®šåƒç´ åŒºåŸŸçš„é‡è¦æ€§ï¼Œå¯¼è‡´æ— æ³•ç²¾ç¡®åœ°æ‰§è¡Œéœ€è¦ç»†ç²’åº¦æ„ŸçŸ¥çš„ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPixelVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥åƒç´ çº§çš„ä¿¡æ¯å¤„ç†èƒ½åŠ›ï¼Œå¢å¼ºVLAæ¨¡å‹å¯¹åœºæ™¯çš„ç†è§£ã€‚åŒæ—¶ï¼Œæ¨¡å‹æ”¯æŒå¤šæ¨¡æ€çš„æç¤ºæ–¹å¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ï¼Œä»è€Œæé«˜æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶ç²¾ç¡®åœ°æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPixelVLAçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šå°ºåº¦åƒç´ æ„ŸçŸ¥ç¼–ç å™¨ï¼šç”¨äºæå–å›¾åƒçš„åƒç´ çº§ç‰¹å¾ï¼Œå¹¶èåˆä¸åŒå°ºåº¦çš„ä¿¡æ¯ã€‚2) è§†è§‰æç¤ºç¼–ç å™¨ï¼šç”¨äºå¤„ç†è§†è§‰æç¤ºä¿¡æ¯ï¼Œä¾‹å¦‚ç›®æ ‡ç‰©ä½“çš„å›¾åƒåŒºåŸŸã€‚3) æŒ‡ä»¤è°ƒä¼˜æ¡†æ¶ï¼šå°†åƒç´ çº§ç‰¹å¾ã€è§†è§‰æç¤ºå’Œæ–‡æœ¬æŒ‡ä»¤è¿›è¡Œèåˆï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„åŠ¨ä½œæŒ‡ä»¤ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè‡ªåŠ¨æ ‡æ³¨æµç¨‹ï¼Œç”Ÿæˆå¤§è§„æ¨¡åƒç´ çº§æ ‡æ³¨æ•°æ®é›†Pixel-160Kã€‚

**å…³é”®åˆ›æ–°**ï¼šPixelVLAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åƒç´ çº§ç†è§£èƒ½åŠ›å’Œå¤šæ¨¡æ€æç¤ºæœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„VLAæ¨¡å‹ç›¸æ¯”ï¼ŒPixelVLAèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°ç†è§£åœºæ™¯ä¸­çš„ç»†èŠ‚ä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç”¨æˆ·çš„è§†è§‰æç¤ºè¿›è¡Œæ›´çµæ´»çš„æ§åˆ¶ã€‚æ­¤å¤–ï¼ŒPixel-160Kæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºåƒç´ çº§VLAæ¨¡å‹çš„è®­ç»ƒæä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šPixelVLAé‡‡ç”¨äº†å¤šå°ºåº¦å·ç§¯ç¥ç»ç½‘ç»œæ¥æå–åƒç´ çº§ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥èåˆä¸åŒå°ºåº¦çš„ä¿¡æ¯ã€‚è§†è§‰æç¤ºç¼–ç å™¨ä½¿ç”¨Transformerç»“æ„æ¥å¤„ç†è§†è§‰æç¤ºä¿¡æ¯ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åŠ¨ä½œé¢„æµ‹æŸå¤±å’Œåƒç´ çº§ç†è§£æŸå¤±ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°é¢„æµ‹åŠ¨ä½œå¹¶ç†è§£åœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PixelVLAåœ¨ä¸‰ä¸ªæ ‡å‡†VLAåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºOpenVLAï¼Œæ“çºµæˆåŠŸç‡æå‡äº†10.1%-17.8%ã€‚åŒæ—¶ï¼ŒPixelVLAä»…éœ€OpenVLA 1.5%çš„é¢„è®­ç»ƒæˆæœ¬ï¼Œè¡¨æ˜å…¶å…·æœ‰æ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†PixelVLAåœ¨æå‡VLAæ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PixelVLAåœ¨æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨åŒ–è£…é…ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥ç”¨äºå¼•å¯¼æœºå™¨äººå®Œæˆç²¾ç»†çš„è£…é…ä»»åŠ¡ï¼Œæˆ–è€…æ ¹æ®ç”¨æˆ·çš„è§†è§‰æŒ‡ä»¤æ§åˆ¶æ™ºèƒ½å®¶å±…è®¾å¤‡ã€‚è¯¥ç ”ç©¶çš„æˆæœæœ‰åŠ©äºæå‡æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.

