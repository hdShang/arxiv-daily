---
layout: default
title: Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning
---

# Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.01502" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.01502v1</a>
  <a href="https://arxiv.org/pdf/2511.01502.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01502v1" onclick="toggleFavorite(this, '2511.01502v1', 'Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03

**å¤‡æ³¨**: 18 pages, 14 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiMoDEæ¡†æ¶ï¼Œé€šè¿‡åŒºåˆ†è¿åŠ¨åˆ†é‡æå‡æ·±åº¦å’Œè‡ªè¿åŠ¨è”åˆå­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ·±åº¦ä¼°è®¡` `è‡ªè¿åŠ¨ä¼°è®¡` `æ— ç›‘ç£å­¦ä¹ ` `å‡ ä½•çº¦æŸ` `è¿åŠ¨åˆ†å‰²` `ä¸‰ç»´é‡å»º` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å’Œè‡ªè¿åŠ¨è”åˆå­¦ä¹ æ–¹æ³•å¯¹ä¸åŒè¿åŠ¨ç±»å‹å¤„ç†æ–¹å¼ç²—ç³™ï¼Œé™åˆ¶äº†å‡ ä½•çº¦æŸçš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´é²æ£’æ€§ä¸è¶³ã€‚
2. DiMoDEæ¡†æ¶é€šè¿‡åŒºåˆ†å¤„ç†è¿åŠ¨åˆ†é‡ï¼Œåˆ©ç”¨åˆšæ€§æµçš„å‡ ä½•è§„å¾‹ï¼Œå¯¹è‡ªè¿åŠ¨åˆ†é‡æ–½åŠ æ›´ç²¾ç¡®çš„çº¦æŸï¼Œæå‡æ·±åº¦å’Œè‡ªè¿åŠ¨ä¼°è®¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDiMoDEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ·±åº¦å’Œè‡ªè¿åŠ¨çš„æ— ç›‘ç£å­¦ä¹ å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™ä¸¤è€…æ˜¯åŸºç¡€çš„3Dæ„ŸçŸ¥ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•å°†è‡ªè¿åŠ¨è§†ä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œè¦ä¹ˆæ··åˆæ‰€æœ‰è¿åŠ¨ç±»å‹ï¼Œè¦ä¹ˆåœ¨ç›‘ç£ä¸­æ’é™¤æ·±åº¦æ— å…³çš„æ—‹è½¬è¿åŠ¨ã€‚è¿™ç§è®¾è®¡é™åˆ¶äº†å¼ºå‡ ä½•çº¦æŸçš„å¼•å…¥ï¼Œé™ä½äº†åœ¨å„ç§æ¡ä»¶ä¸‹çš„å¯é æ€§å’Œé²æ£’æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŒºåˆ†è¿åŠ¨åˆ†é‡çš„å¤„ç†æ–¹æ³•ï¼Œåˆ©ç”¨å„è‡ªåˆšæ€§æµçš„å‡ ä½•è§„å¾‹æ¥æ”¹å–„æ·±åº¦å’Œè‡ªè¿åŠ¨ä¼°è®¡ã€‚ç»™å®šè¿ç»­è§†é¢‘å¸§ï¼Œç½‘ç»œé¦–å…ˆå¯¹é½æºç›¸æœºå’Œç›®æ ‡ç›¸æœºçš„å…‰è½´å’Œæˆåƒå¹³é¢ã€‚å¸§ä¹‹é—´çš„å…‰æµé€šè¿‡è¿™äº›å¯¹é½è¿›è¡Œå˜æ¢ï¼Œå¹¶é‡åŒ–åå·®ï¼Œä»è€Œå¯¹æ¯ä¸ªè‡ªè¿åŠ¨åˆ†é‡å•ç‹¬æ–½åŠ å‡ ä½•çº¦æŸï¼Œå®ç°æ›´æœ‰é’ˆå¯¹æ€§çš„ç»†åŒ–ã€‚è¿™äº›å¯¹é½è¿›ä¸€æ­¥å°†è”åˆå­¦ä¹ è¿‡ç¨‹é‡æ–°æ„å»ºä¸ºåŒè½´å’Œå…±é¢å½¢å¼ï¼Œå…¶ä¸­æ·±åº¦å’Œæ¯ä¸ªå¹³ç§»åˆ†é‡å¯ä»¥é€šè¿‡é—­å¼å‡ ä½•å…³ç³»ç›¸äº’æ¨å¯¼ï¼Œå¼•å…¥äº’è¡¥çº¦æŸï¼Œæé«˜æ·±åº¦é²æ£’æ€§ã€‚DiMoDEæ˜¯ä¸€ä¸ªé€šç”¨çš„æ·±åº¦å’Œè‡ªè¿åŠ¨è”åˆå­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†è¿™äº›è®¾è®¡ï¼Œåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†å’Œä¸€ä¸ªæ–°æ”¶é›†çš„çœŸå®ä¸–ç•Œå¤šæ ·åŒ–æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ã€‚æˆ‘ä»¬çš„æºä»£ç å°†åœ¨å‘å¸ƒååœ¨mias.group/DiMoDEä¸Šå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ— ç›‘ç£æ·±åº¦å’Œè‡ªè¿åŠ¨è”åˆå­¦ä¹ æ–¹æ³•é€šå¸¸å°†è‡ªè¿åŠ¨è§†ä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œè¦ä¹ˆæ··åˆæ‰€æœ‰ç±»å‹çš„è¿åŠ¨ï¼Œè¦ä¹ˆç›´æ¥æ’é™¤ä¸æ·±åº¦æ— å…³çš„æ—‹è½¬è¿åŠ¨ã€‚è¿™ç§å¤„ç†æ–¹å¼æ— æ³•å……åˆ†åˆ©ç”¨è¿åŠ¨é—´çš„å‡ ä½•å…³ç³»ï¼Œå¯¼è‡´æ·±åº¦å’Œè‡ªè¿åŠ¨ä¼°è®¡çš„ç²¾åº¦å’Œé²æ£’æ€§å—é™ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDiMoDEçš„æ ¸å¿ƒæ€è·¯åœ¨äºå¯¹ä¸åŒçš„è¿åŠ¨åˆ†é‡è¿›è¡ŒåŒºåˆ†å¤„ç†ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬å„è‡ªçš„å‡ ä½•è§„å¾‹æ¥ç›¸äº’çº¦æŸå’Œä¼˜åŒ–ã€‚é€šè¿‡å¯¹é½ç›¸æœºå…‰è½´å’Œæˆåƒå¹³é¢ï¼Œå°†å¤æ‚çš„è¿åŠ¨åˆ†è§£ä¸ºæ›´æ˜“äºå¤„ç†çš„åŒè½´å’Œå…±é¢å½¢å¼ï¼Œä»è€Œå¯ä»¥æ›´ç²¾ç¡®åœ°æ–½åŠ å‡ ä½•çº¦æŸï¼Œå¹¶å»ºç«‹æ·±åº¦å’Œè‡ªè¿åŠ¨åˆ†é‡ä¹‹é—´çš„é—­å¼å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDiMoDEæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è¾“å…¥è¿ç»­è§†é¢‘å¸§ï¼›2) ç½‘ç»œé¢„æµ‹æ·±åº¦ã€å…‰æµå’Œè‡ªè¿åŠ¨å‚æ•°ï¼›3) é€šè¿‡å…‰è½´å’Œæˆåƒå¹³é¢å¯¹é½ï¼Œå°†è¿åŠ¨åˆ†è§£ä¸ºæ—‹è½¬å’Œå¹³ç§»åˆ†é‡ï¼›4) å¯¹æ¯ä¸ªè‡ªè¿åŠ¨åˆ†é‡æ–½åŠ ç‹¬ç«‹çš„å‡ ä½•çº¦æŸï¼Œé‡åŒ–åå·®å¹¶è¿›è¡Œä¼˜åŒ–ï¼›5) åˆ©ç”¨åŒè½´å’Œå…±é¢å…³ç³»ï¼Œå»ºç«‹æ·±åº¦å’Œæ¯ä¸ªå¹³ç§»åˆ†é‡ä¹‹é—´çš„é—­å¼å…³ç³»ï¼Œå¼•å…¥äº’è¡¥çº¦æŸï¼›6) è”åˆä¼˜åŒ–æ·±åº¦å’Œè‡ªè¿åŠ¨å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šDiMoDEçš„å…³é”®åˆ›æ–°åœ¨äºå¯¹è¿åŠ¨åˆ†é‡çš„åŒºåˆ†å¤„ç†å’Œå‡ ä½•çº¦æŸçš„ç²¾ç»†åŒ–åº”ç”¨ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒDiMoDEä¸æ˜¯ç®€å•åœ°æ··åˆæˆ–æ’é™¤æŸäº›è¿åŠ¨ç±»å‹ï¼Œè€Œæ˜¯å……åˆ†åˆ©ç”¨äº†æ¯ç§è¿åŠ¨ç±»å‹çš„å‡ ä½•ç‰¹æ€§ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯ç”¨äºä¼˜åŒ–æ·±åº¦å’Œè‡ªè¿åŠ¨ä¼°è®¡çš„çº¦æŸæ¡ä»¶ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å‡ ä½•ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDiMoDEçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å…‰è½´å’Œæˆåƒå¹³é¢å¯¹é½æ¨¡å—ï¼Œç”¨äºå°†è¿åŠ¨åˆ†è§£ä¸ºåŒè½´å’Œå…±é¢å½¢å¼ï¼›2) ç‹¬ç«‹çš„å‡ ä½•çº¦æŸæŸå¤±å‡½æ•°ï¼Œç”¨äºå¯¹æ¯ä¸ªè‡ªè¿åŠ¨åˆ†é‡æ–½åŠ çº¦æŸï¼›3) åŸºäºé—­å¼å…³ç³»çš„æ·±åº¦å’Œå¹³ç§»åˆ†é‡äº’å¯¼æ¨¡å—ï¼Œç”¨äºå¼•å…¥äº’è¡¥çº¦æŸï¼›4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç»¼åˆè€ƒè™‘äº†å…‰åº¦ä¸€è‡´æ€§æŸå¤±ã€å¹³æ»‘æŸå¤±å’Œå‡ ä½•çº¦æŸæŸå¤±ï¼Œä»¥å®ç°æ·±åº¦å’Œè‡ªè¿åŠ¨çš„è”åˆä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DiMoDEåœ¨KITTIã€Cityscapeså’Œæ–°æ”¶é›†çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶åœ¨æ·±åº¦å’Œè‡ªè¿åŠ¨ä¼°è®¡æ–¹é¢å‡å–å¾—äº†SOTAæ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸‹ï¼ŒDiMoDEçš„æ€§èƒ½æå‡æ›´ä¸ºæ˜¾è‘—ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®å°†åœ¨è®ºæ–‡å‘è¡¨åå…¬å¼€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æ·±åº¦å’Œè‡ªè¿åŠ¨ä¼°è®¡çš„ç²¾åº¦å’Œé²æ£’æ€§ï¼Œå¯ä»¥æå‡è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¢å¼ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œå¹¶ä¸ºå¢å¼ºç°å®åº”ç”¨æä¾›æ›´å‡†ç¡®çš„åœºæ™¯ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication.

