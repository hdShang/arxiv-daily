---
layout: default
title: Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models
---

# Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.01618" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.01618v1</a>
  <a href="https://arxiv.org/pdf/2511.01618.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01618v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.01618v1', 'Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, Lei Bai, Wanli Ouyang, Yuanqi Li, Jie Guo, Yanwen Guo

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Actialï¼šé€šè¿‡è§†è§’å­¦ä¹ æ¿€æ´»å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `è§†è§’å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é›†` `è·¨è§†è§’ä¸€è‡´æ€§` `3Dåœºæ™¯ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨3Dæ¨ç†ä¸­ç¼ºä¹æœ‰æ•ˆçš„ç©ºé—´ä¿¡æ¯æ•æ‰ï¼Œå°¤å…¶åœ¨è·¨è§†è§’ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚
2. æå‡ºè§†è§’å­¦ä¹ ä»»åŠ¡å’ŒViewpoint-100Kæ•°æ®é›†ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥æå‡ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æ¿€æ´»MLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œæå‡äº†åŸŸå†…å’ŒåŸŸå¤–æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)åœ¨2Dè§†è§‰ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¿€å‘äº†äººä»¬å°†å…¶åº”ç”¨äºå¤æ‚3Dæ¨ç†ä»»åŠ¡çš„å…´è¶£ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ˜¯å¦èƒ½æœ‰æ•ˆæ•æ‰åˆ°ç¨³å¥çš„çœŸå®ä¸–ç•Œæ€§èƒ½æ‰€éœ€çš„è¯¦ç»†ç©ºé—´ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾ç¡®3Dæ¨ç†çš„å…³é”®è¦æ±‚â€”â€”è·¨è§†è§’ä¸€è‡´æ€§æ–¹é¢ï¼Œä»ç„¶ä¸æ¸…æ¥šã€‚è€ƒè™‘åˆ°è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§’å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€é¡¹æ—¨åœ¨è¯„ä¼°å’Œæé«˜MLLMç©ºé—´æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†Viewpoint-100Kæ•°æ®é›†ï¼ŒåŒ…å«10ä¸‡ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å›¾åƒå¯¹ï¼Œå…·æœ‰ä¸åŒçš„è§†è§’å’Œç›¸åº”çš„é—®ç­”å¯¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼šé¦–å…ˆï¼Œé€šè¿‡åœ¨Viewpoint-100Kä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒ(SFT)ï¼Œå°†åŸºç¡€çŸ¥è¯†æ³¨å…¥åˆ°åŸºçº¿MLLMä¸­ï¼Œä»è€Œåœ¨å¤šä¸ªä»»åŠ¡ä¸­è·å¾—æ˜¾è‘—æ”¹è¿›ï¼›å…¶æ¬¡ï¼Œé€šè¿‡åœ¨æ›´å¹¿æ³›çš„é—®é¢˜é›†ä¸Šä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ··åˆå†·å¯åŠ¨åˆå§‹åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶å­¦ä¹ è§†è§’è¡¨ç¤ºå¹¶ä¿æŒè¿è´¯çš„æ¨ç†æ€ç»´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æ¿€æ´»äº†MLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†åœ¨åŸŸå†…å’ŒåŸŸå¤–æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å‘ç°å¼ºè°ƒäº†åœ¨MLLMä¸­å‘å±•åŸºç¡€ç©ºé—´æŠ€èƒ½çš„ä»·å€¼ï¼Œæ”¯æŒæœºå™¨äººã€è‡ªä¸»ç³»ç»Ÿå’Œ3Dåœºæ™¯ç†è§£æ–¹é¢çš„æœªæ¥è¿›å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£ä¸åŒè§†è§’ä¸‹çš„ç‰©ä½“å…³ç³»å’Œä¿æŒè·¨è§†è§’ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰éœ€è¦ç²¾ç¡®3Dåœºæ™¯ç†è§£é¢†åŸŸçš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰å’Œåˆ©ç”¨å›¾åƒä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†ç»“æœä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡â€œè§†è§’å­¦ä¹ â€æ¥æå‡MLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡æ„å»ºåŒ…å«å¤§é‡ä¸åŒè§†è§’å›¾åƒå¯¹çš„æ•°æ®é›†ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„é—®ç­”ä»»åŠ¡ï¼Œè®©æ¨¡å‹å­¦ä¹ ä»ä¸åŒè§†è§’è§‚å¯ŸåŒä¸€ç‰©ä½“æ—¶åº”è¯¥å…·å¤‡çš„ç†è§£èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨è®©æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ¨ç†3Dç©ºé—´ä¸­çš„ç‰©ä½“å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ç›‘ç£å¾®è°ƒ(SFT)ï¼šåœ¨Viewpoint-100Kæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ç›‘ç£å­¦ä¹ çš„æ–¹å¼å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œæ³¨å…¥åŸºç¡€çš„ç©ºé—´çŸ¥è¯†ã€‚2) å¼ºåŒ–å­¦ä¹ ï¼šä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ç®—æ³•ï¼Œåœ¨æ›´å¹¿æ³›çš„é—®é¢˜é›†ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨äº†æ··åˆå†·å¯åŠ¨åˆå§‹åŒ–æ–¹æ³•ï¼Œä»¥åŒæ—¶å­¦ä¹ è§†è§’è¡¨ç¤ºå¹¶ä¿æŒæ¨ç†è¿è´¯æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†â€œè§†è§’å­¦ä¹ â€è¿™ä¸€æ¦‚å¿µï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„Viewpoint-100Kæ•°æ®é›†å’Œä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæœ‰æ•ˆåœ°æ¿€æ´»äº†MLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æ··åˆå†·å¯åŠ¨åˆå§‹åŒ–æ–¹æ³•ä¹Ÿæ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤ŸåŒæ—¶å­¦ä¹ è§†è§’è¡¨ç¤ºå¹¶ä¿æŒæ¨ç†çš„è¿è´¯æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šViewpoint-100Kæ•°æ®é›†åŒ…å«10ä¸‡ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å›¾åƒå¯¹ï¼Œæ¯ä¸ªå›¾åƒå¯¹åŒ…å«ä¸åŒè§†è§’çš„åŒä¸€ç‰©ä½“å›¾åƒï¼Œå¹¶é…æœ‰ç›¸åº”çš„é—®ç­”å¯¹ã€‚ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ä¸­ï¼ŒSFTé˜¶æ®µä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ŒGRPOé˜¶æ®µä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ã€‚æ··åˆå†·å¯åŠ¨åˆå§‹åŒ–æ–¹æ³•çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•æ˜¾è‘—æå‡äº†MLLMåœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨Viewpoint-100Kæ•°æ®é›†ä¸Šï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨åŸŸå¤–æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€3Dåœºæ™¯ç†è§£ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡MLLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„å¯¼èˆªå’Œæ“ä½œã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¯ä»¥æé«˜è½¦è¾†å¯¹å¤æ‚äº¤é€šåœºæ™¯çš„ç†è§£å’Œåˆ¤æ–­èƒ½åŠ›ï¼Œä»è€Œæå‡å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸï¼Œæä¾›æ›´é€¼çœŸçš„ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.

