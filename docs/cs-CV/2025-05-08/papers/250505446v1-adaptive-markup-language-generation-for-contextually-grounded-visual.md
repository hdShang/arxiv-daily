---
layout: default
title: Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding
---

# Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05446v1</a>
  <a href="https://arxiv.org/pdf/2505.05446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05446v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05446v1', 'Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Han Xiao, Yina Xie, Guanxin Tan, Yinghao Chen, Rui Hu, Ke Wang, Aojun Zhou, Hao Li, Hao Shao, Xudong Lu, Peng Gao, Yafei Wen, Xiaoxin Chen, Shuai Ren, Hongsheng Li

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08

**å¤‡æ³¨**: CVPR2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”æ ‡è®°è¯­è¨€ç”Ÿæˆä»¥è§£å†³è§†è§‰æ–‡æ¡£ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æ–‡æ¡£ç†è§£` `è‡ªé€‚åº”ç”Ÿæˆ` `æ ‡è®°è¯­è¨€` `å¤šæ¨¡æ€å­¦ä¹ ` `ä¸Šä¸‹æ–‡ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†è§‰æ–‡æ¡£ç†è§£ä¸­é¢ä¸´æ•´åˆè§†è§‰ä¸æ–‡æœ¬çš„æŒ‘æˆ˜ï¼Œä¸”ç¼ºä¹è¯¦ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´ç†è§£ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨è‡ªé€‚åº”ç”Ÿæˆæ ‡è®°è¯­è¨€æ„å»ºç»“æ„åŒ–æ–‡æ¡£è¡¨ç¤ºï¼Œå¢å¼ºå¯¹å¤æ‚æ–‡æ¡£çš„ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œæå‡äº†æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ–‡æœ¬ä¸°å¯Œçš„è§†è§‰å†…å®¹çš„å¢åŠ ï¼Œè§†è§‰æ–‡æ¡£ç†è§£å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸé¢ä¸´ç€æœ‰æ•ˆæ•´åˆè§†è§‰æ„ŸçŸ¥å’Œæ–‡æœ¬ç†è§£çš„é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å¸ƒå±€çš„å¤šæ ·æ–‡æ¡£ç±»å‹ä¸­ã€‚ç°æœ‰çš„å¾®è°ƒæ•°æ®é›†å¾€å¾€ç¼ºä¹è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´ç†è§£ä¸è¶³å’Œç©ºé—´å…³ç³»çš„å¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç®¡é“ï¼Œåˆ©ç”¨è‡ªé€‚åº”ç”Ÿæˆæ ‡è®°è¯­è¨€ï¼ˆå¦‚Markdownã€JSONã€HTMLå’ŒTiKZï¼‰æ¥æ„å»ºé«˜åº¦ç»“æ„åŒ–çš„æ–‡æ¡£è¡¨ç¤ºï¼Œå¹¶æä¾›åŸºäºä¸Šä¸‹æ–‡çš„å“åº”ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªç»†ç²’åº¦ç»“æ„åŒ–æ•°æ®é›†ï¼šDocMark-Pileå’ŒDocMark-Instructï¼Œå®éªŒè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè§†è§‰æ–‡æ¡£ç†è§£åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰æ–‡æ¡£ç†è§£ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¸ƒå±€æ–‡æ¡£æ—¶å¸¸å¸¸å‡ºç°ç†è§£åå·®å’Œç©ºé—´å…³ç³»çš„å¹»è§‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡è‡ªé€‚åº”ç”Ÿæˆæ ‡è®°è¯­è¨€ï¼Œæ„å»ºç»“æ„åŒ–çš„æ–‡æ¡£è¡¨ç¤ºï¼Œå¢å¼ºæ¨¡å‹å¯¹æ–‡æ¡£å†…å®¹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚æ­¤è®¾è®¡æ—¨åœ¨æä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜ç†è§£çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ ‡è®°è¯­è¨€ç”Ÿæˆã€æ–‡æ¡£è§£æå’Œä¸Šä¸‹æ–‡å“åº”ç”Ÿæˆç­‰ä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡DocMark-Pileè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååˆ©ç”¨DocMark-Instructè¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°æ›´å¥½çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†è‡ªé€‚åº”æ ‡è®°è¯­è¨€ç”Ÿæˆçš„æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æ¡£å†…å®¹åŠ¨æ€ç”Ÿæˆé€‚åˆçš„æ ‡è®°è¯­è¨€ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ–‡æ¡£ç†è§£çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ ‡è®°è¯­è¨€ç”Ÿæˆçš„è´¨é‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å¤šå±‚æ¬¡çš„ç‰¹å¾æå–æ¨¡å—ï¼Œä»¥æ›´å¥½åœ°æ•æ‰æ–‡æ¡£ä¸­çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„æ–‡æ¡£å¸ƒå±€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¨¡å‹åœ¨å¤šä¸ªè§†è§‰æ–‡æ¡£ç†è§£åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸­çš„å…ˆè¿›æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æ–‡æ¡£å¤„ç†ã€è‡ªåŠ¨åŒ–ä¿¡æ¯æå–å’Œäººæœºäº¤äº’ç³»ç»Ÿç­‰ã€‚é€šè¿‡æä¾›æ›´å‡†ç¡®çš„æ–‡æ¡£ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ•™è‚²ã€æ³•å¾‹ã€åŒ»ç–—ç­‰è¡Œä¸šä¸­æå‡ä¿¡æ¯å¤„ç†æ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual Document Understanding has become essential with the increase of text-rich visual content. This field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. Moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. To address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML, and TiKZ, to build highly structured document representations and deliver contextually-grounded responses. We introduce two fine-grained structured datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data annotations for grounded instruction following. Extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-theart MLLMs across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios. Our code and models are released at https://github. com/Euphoria16/DocMark.

