---
layout: default
title: "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation"
---

# TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05422" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05422v2</a>
  <a href="https://arxiv.org/pdf/2505.05422.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05422v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05422v2', 'TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, Ying Shan

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-08 (æ›´æ–°: 2025-08-15)

**å¤‡æ³¨**: Technical Report

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TencentARC/TokLIP)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTokLIPä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¸­çš„é«˜è®¡ç®—å¼€é”€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `è§†è§‰tokenåŒ–` `CLIP` `è‡ªå›å½’è®­ç»ƒ` `ç”Ÿæˆæ¨¡å‹` `é«˜å±‚è¯­ä¹‰` `æ•°æ®æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ–¹æ³•åœ¨ç†è§£æ€§èƒ½å’Œè®¡ç®—å¼€é”€ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. TokLIPé€šè¿‡è¯­ä¹‰åŒ–VQ tokenå¹¶ç»“åˆCLIPè¯­ä¹‰ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰tokenåŒ–å™¨ï¼Œæ”¯æŒé«˜æ•ˆçš„å¤šæ¨¡æ€è®­ç»ƒã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTokLIPåœ¨æ•°æ®æ•ˆç‡å’Œç”Ÿæˆèƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œé€‚ç”¨äºå¤šç§è‡ªå›å½’ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤šæ¨¡æ€ç»Ÿä¸€çš„åŸºç¡€ä¸Šï¼Œç°æœ‰çš„åŸºäºtokençš„æ–¹æ³•å¦‚Chameleonå’ŒEmu3é¢ä¸´ç€é«˜è®­ç»ƒè®¡ç®—å¼€é”€å’Œç†è§£æ€§èƒ½ä¸è¶³çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºä¹é«˜å±‚è¯­ä¹‰ã€‚æœ¬æ–‡æå‡ºäº†TokLIPï¼Œä¸€ç§è§†è§‰tokenåŒ–å™¨ï¼Œé€šè¿‡è¯­ä¹‰åŒ–å‘é‡é‡åŒ–ï¼ˆVQï¼‰tokenå¹¶ç»“åˆCLIPçº§åˆ«çš„è¯­ä¹‰ï¼Œå¢å¼ºäº†ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶æ”¯æŒæ ‡å‡†VQ tokençš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€è‡ªå›å½’è®­ç»ƒã€‚TokLIPå°†ä½å±‚ç¦»æ•£VQ tokenåŒ–å™¨ä¸åŸºäºViTçš„tokenç¼–ç å™¨ç›¸ç»“åˆï¼Œä»¥æ•æ‰é«˜å±‚è¿ç»­è¯­ä¹‰ã€‚ä¸ä»¥å¾€æ–¹æ³•ï¼ˆå¦‚VILA-Uï¼‰ä¸åŒï¼ŒTokLIPè§£è€¦äº†ç†è§£å’Œç”Ÿæˆçš„è®­ç»ƒç›®æ ‡ï¼Œä½¿å¾—å¯ä»¥ç›´æ¥åº”ç”¨å…ˆè¿›çš„VQ tokenåŒ–å™¨ï¼Œè€Œæ— éœ€å®šåˆ¶çš„é‡åŒ–æ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTokLIPåœ¨æ•°æ®æ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèµ‹äºˆè§†è§‰tokené«˜å±‚è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶å¢å¼ºä½å±‚ç”Ÿæˆèƒ½åŠ›ï¼Œé€‚ç”¨äºè‡ªå›å½’Transformerçš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ–¹æ³•åœ¨ç†è§£æ€§èƒ½å’Œè®¡ç®—å¼€é”€ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯ç¼ºä¹é«˜å±‚è¯­ä¹‰å¯¼è‡´çš„æ€§èƒ½é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTokLIPé€šè¿‡å°†ä½å±‚ç¦»æ•£VQ tokenä¸ViTåŸºç¡€çš„tokenç¼–ç å™¨ç»“åˆï¼Œè¯­ä¹‰åŒ–tokenä»¥æ•æ‰é«˜å±‚è¯­ä¹‰ï¼Œä»è€Œæå‡ç†è§£èƒ½åŠ›å’Œç”Ÿæˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTokLIPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªä½å±‚çš„ç¦»æ•£VQ tokenåŒ–å™¨å’Œä¸€ä¸ªåŸºäºViTçš„tokenç¼–ç å™¨ï¼Œæ”¯æŒç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€è‡ªå›å½’è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šTokLIPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºè§£è€¦ç†è§£å’Œç”Ÿæˆçš„è®­ç»ƒç›®æ ‡ï¼Œä½¿å¾—å¯ä»¥ç›´æ¥åº”ç”¨å…ˆè¿›çš„VQ tokenåŒ–å™¨ï¼Œè€Œæ— éœ€å¤æ‚çš„é‡åŒ–æ“ä½œï¼Œè¿™ä¸ä»¥å¾€æ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒTokLIPé‡‡ç”¨äº†æ ‡å‡†çš„VQ tokenåŒ–å™¨ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TokLIPåœ¨å®éªŒä¸­å±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰tokençš„é«˜å±‚è¯­ä¹‰ç†è§£èƒ½åŠ›å’Œä½å±‚ç”Ÿæˆèƒ½åŠ›ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒTokLIPåœ¨å¤šä¸ªè‡ªå›å½’ä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½çš„æ˜¾è‘—æå‡ï¼Œå…·ä½“æ•°æ®æœªæä¾›ï¼Œä½†æ•ˆæœæ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TokLIPåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒä¸æ–‡æœ¬çš„ç»“åˆã€è§†é¢‘ç†è§£ä»¥åŠäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„è®­ç»ƒå’Œç”Ÿæˆèƒ½åŠ›å°†æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ï¼Œæå‡å¤šæ¨¡æ€åº”ç”¨çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pioneering token-based works such as Chameleon and Emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (VQ) tokens and incorporating CLIP-level semantics while enabling end-to-end multimodal autoregressive training with standard VQ tokens. TokLIP integrates a low-level discrete VQ tokenizer with a ViT-based token encoder to capture high-level continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize high-level features, TokLIP disentangles training objectives for comprehension and generation, allowing the direct application of advanced VQ tokenizers without the need for tailored quantization operations. Our empirical results demonstrate that TokLIP achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive Transformers in both comprehension and generation tasks. The code and models are available at https://github.com/TencentARC/TokLIP.

