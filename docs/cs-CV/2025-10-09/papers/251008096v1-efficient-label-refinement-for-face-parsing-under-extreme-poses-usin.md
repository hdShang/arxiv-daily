---
layout: default
title: Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting
---

# Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08096" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08096v1</a>
  <a href="https://arxiv.org/pdf/2510.08096.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08096v1" onclick="toggleFavorite(this, '2510.08096v1', 'Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ankit Gahlawat, Anirban Mukherjee, Dinesh Babu Jayagopi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Accepted to VCIP 2025 (International Conference on Visual Communications and Image Processing 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨3Dé«˜æ–¯æº…å°„è¿›è¡Œäººè„¸è§£ææ ‡ç­¾ä¼˜åŒ–ï¼Œæå‡æç«¯å§¿æ€ä¸‹çš„è§£æç²¾åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `äººè„¸è§£æ` `3Dé«˜æ–¯æº…å°„` `æ ‡ç­¾ä¼˜åŒ–` `æç«¯å§¿æ€` `å¤šè§†è§’ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æç«¯å§¿æ€ä¸‹äººè„¸è§£æç²¾åº¦ä½ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹è¶³å¤Ÿæ ‡æ³¨æ•°æ®ï¼Œä¸”æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜ã€‚
2. æå‡ºä¸€ç§åŸºäº3Dé«˜æ–¯æº…å°„çš„æ ‡ç­¾ä¼˜åŒ–æµç¨‹ï¼Œé€šè¿‡å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸï¼Œä»å°‘é‡å›¾åƒç”Ÿæˆå¤§é‡å¸¦ç²¾ç¡®æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›°éš¾å§¿æ€ä¸‹æ˜¾è‘—æå‡äººè„¸è§£æç²¾åº¦ï¼Œä¸”æ— éœ€3Dæ ‡æ³¨ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æç«¯è§†è§’ä¸‹è¿›è¡Œå‡†ç¡®çš„äººè„¸è§£æä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨è¿™äº›å§¿æ€ä¸‹çš„å¸¦æ ‡ç­¾æ•°æ®æœ‰é™ã€‚æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”é€šå¸¸åœ¨è§„æ¨¡ä¸Šä¸åˆ‡å®é™…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ ‡ç­¾ä¼˜åŒ–æµç¨‹ï¼Œè¯¥æµç¨‹åˆ©ç”¨3Dé«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ä»å˜ˆæ‚çš„å¤šè§†è§’é¢„æµ‹ä¸­ç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²æ©ç ã€‚é€šè¿‡è”åˆæ‹Ÿåˆä¸¤ä¸ª3DGSæ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºRGBå›¾åƒï¼Œä¸€ä¸ªç”¨äºå…¶åˆå§‹åˆ†å‰²å›¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å…±äº«å‡ ä½•ä½“æ¥å¼ºåˆ¶æ‰§è¡Œå¤šè§†è§’ä¸€è‡´æ€§ï¼Œä»è€Œèƒ½å¤Ÿåˆæˆå…·æœ‰å§¿æ€å¤šæ ·æ€§çš„è®­ç»ƒæ•°æ®ï¼Œåªéœ€æœ€å°‘çš„åå¤„ç†ã€‚åœ¨æ­¤ç²¾ç‚¼æ•°æ®é›†ä¸Šå¾®è°ƒäººè„¸è§£ææ¨¡å‹å¯æ˜¾è‘—æé«˜åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤´éƒ¨å§¿æ€ä¸‹çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒåœ¨æ ‡å‡†è§†å›¾ä¸Šçš„å¼ºå¤§æ€§èƒ½ã€‚åŒ…æ‹¬äººå·¥è¯„ä¼°åœ¨å†…çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å“è¶Šçš„ç»“æœï¼Œå°½ç®¡ä¸éœ€è¦ground-truth 3Dæ³¨é‡Šï¼Œå¹¶ä¸”ä»…ä½¿ç”¨ä¸€å°éƒ¨åˆ†åˆå§‹å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæé«˜ç°å®ä¸–ç•Œç¯å¢ƒä¸­äººè„¸è§£æçš„é²æ£’æ€§æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æç«¯å§¿æ€ä¸‹äººè„¸è§£æç²¾åº¦ä½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†åœ¨æç«¯å§¿æ€ä¸‹ï¼Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºä¸”æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚æ­¤å¤–ï¼Œç›´æ¥è®­ç»ƒçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥å¤„ç†æœªè§è¿‡çš„å§¿æ€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨3Dé«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰æŠ€æœ¯ï¼Œä»å°‘é‡å¸¦å™ªå£°çš„å¤šè§†è§’äººè„¸åˆ†å‰²é¢„æµ‹ä¸­ï¼Œç”Ÿæˆå¤§é‡é«˜è´¨é‡ã€å§¿æ€å¤šæ ·çš„è®­ç»ƒæ•°æ®ã€‚é€šè¿‡åœ¨3Dç©ºé—´ä¸­å»ºæ¨¡äººè„¸çš„å‡ ä½•ç»“æ„å’Œåˆ†å‰²ä¿¡æ¯ï¼Œå¹¶å¼ºåˆ¶å¤šè§†è§’ä¸€è‡´æ€§ï¼Œå®ç°æ ‡ç­¾çš„è‡ªåŠ¨ä¼˜åŒ–å’Œç²¾ç‚¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨ç°æœ‰çš„åˆ†å‰²æ¨¡å‹å¯¹å°‘é‡å¤šè§†è§’å›¾åƒè¿›è¡Œåˆå§‹åˆ†å‰²é¢„æµ‹ï¼›2) æ„å»ºä¸¤ä¸ª3DGSæ¨¡å‹ï¼Œåˆ†åˆ«ç”¨äºRGBå›¾åƒå’Œåˆå§‹åˆ†å‰²å›¾ï¼›3) è”åˆä¼˜åŒ–è¿™ä¸¤ä¸ª3DGSæ¨¡å‹ï¼Œé€šè¿‡å…±äº«å‡ ä½•ç»“æ„ï¼Œå¼ºåˆ¶å¤šè§†è§’åˆ†å‰²ä¸€è‡´æ€§ï¼›4) ä½¿ç”¨ä¼˜åŒ–åçš„3DGSæ¨¡å‹æ¸²æŸ“ç”Ÿæˆå¤§é‡å¸¦ç²¾ç¡®æ ‡ç­¾çš„ã€å§¿æ€å¤šæ ·çš„åˆæˆæ•°æ®ï¼›5) ä½¿ç”¨åˆæˆæ•°æ®å¾®è°ƒäººè„¸è§£ææ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨3DGSæŠ€æœ¯è¿›è¡Œæ ‡ç­¾ä¼˜åŒ–ï¼Œä»è€Œåœ¨æ— éœ€3Dæ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜è´¨é‡åˆæˆæ•°æ®çš„ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå§¿æ€æ›´åŠ å¤šæ ·ã€æ ‡ç­¾æ›´åŠ ç²¾ç¡®çš„æ•°æ®ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç›´æ¥ä½¿ç”¨3Dæ¨¡å‹è¿›è¡Œæ¸²æŸ“çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€ç²¾ç¡®çš„3Däººè„¸æ¨¡å‹ï¼Œé™ä½äº†å¯¹å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„3DGSæ¨¡å‹ï¼Œåˆ†åˆ«å»ºæ¨¡RGBå›¾åƒå’Œåˆ†å‰²å›¾ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å…±äº«å‡ ä½•ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œé«˜æ–¯åˆ†å¸ƒçš„ä¸­å¿ƒä½ç½®å’Œåæ–¹å·®çŸ©é˜µï¼‰ï¼Œå¼ºåˆ¶ä¸¤ä¸ªæ¨¡å‹ä¿æŒä¸€è‡´ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ¸²æŸ“æŸå¤±ã€åˆ†å‰²æŸå¤±å’Œæ­£åˆ™åŒ–æŸå¤±ï¼Œç”¨äºä¿è¯æ¸²æŸ“å›¾åƒçš„è´¨é‡ã€åˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ¨¡å‹çš„å¹³æ»‘æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ï¼ˆä¾‹å¦‚ï¼Œé«˜æ–¯åˆ†å¸ƒçš„æ•°é‡ã€ä¼˜åŒ–å™¨çš„é€‰æ‹©ï¼‰éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤´éƒ¨å§¿æ€ä¸‹æ˜¾è‘—æå‡äº†äººè„¸è§£æçš„å‡†ç¡®æ€§ï¼Œä¼˜äºç°æœ‰çš„state-of-the-artæ–¹æ³•ã€‚é€šè¿‡äººå·¥è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•ç”Ÿæˆçš„åˆæˆæ•°æ®çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä»…éœ€å°‘é‡åˆå§‹å›¾åƒï¼Œæ— éœ€3Dæ ‡æ³¨ï¼Œå…·æœ‰å¾ˆé«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºäººè„¸è¯†åˆ«ã€äººè„¸åŠ¨ç”»ã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæå‡åœ¨å¤æ‚å…‰ç…§å’Œå§¿æ€å˜åŒ–ä¸‹çš„é¢éƒ¨è¯†åˆ«å‡†ç¡®ç‡ï¼Œæ”¹å–„è™šæ‹ŸåŒ–èº«åœ¨ä¸åŒè§†è§’ä¸‹çš„æ¸²æŸ“æ•ˆæœï¼Œä»¥åŠå¢å¼ºäººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•æ— éœ€å¤§é‡äººå·¥æ ‡æ³¨ï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼å’Œæ¨å¹¿æ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.

