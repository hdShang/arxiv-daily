---
layout: default
title: UniVideo: Unified Understanding, Generation, and Editing for Videos
---

# UniVideo: Unified Understanding, Generation, and Editing for Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08377" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08377v2</a>
  <a href="https://arxiv.org/pdf/2510.08377.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08377v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08377v2', 'UniVideo: Unified Understanding, Generation, and Editing for Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: Project Website https://congwei1230.github.io/UniVideo/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniVideoï¼šç»Ÿä¸€è§†é¢‘ç†è§£ã€ç”Ÿæˆä¸ç¼–è¾‘çš„å¤šæ¨¡æ€æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `è§†é¢‘ç¼–è¾‘` `å¤šæ¨¡æ€å­¦ä¹ ` `ç»Ÿä¸€å»ºæ¨¡` `æ‰©æ•£æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `æŒ‡ä»¤å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨å›¾åƒé¢†åŸŸï¼Œè§†é¢‘ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ç¼ºä¹ç»Ÿä¸€å»ºæ¨¡ã€‚
2. UniVideoé‡‡ç”¨åŒæµæ¶æ„ï¼Œç»“åˆMLLMè¿›è¡ŒæŒ‡ä»¤ç†è§£ï¼ŒMMDiTè¿›è¡Œè§†é¢‘ç”Ÿæˆï¼Œå®ç°å¤šä»»åŠ¡ç»Ÿä¸€ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUniVideoåœ¨å¤šé¡¹è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç‰¹å®šä»»åŠ¡çš„SOTAæ¨¡å‹ï¼Œå¹¶å…·å¤‡ä»»åŠ¡ç»„åˆèƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†UniVideoï¼Œä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œå°†ç»Ÿä¸€å»ºæ¨¡æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚UniVideoé‡‡ç”¨åŒæµè®¾è®¡ï¼Œç»“åˆäº†ç”¨äºæŒ‡ä»¤ç†è§£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œç”¨äºè§†é¢‘ç”Ÿæˆçš„å¤šæ¨¡æ€æ‰©æ•£Transformerï¼ˆMMDiTï¼‰ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿå‡†ç¡®åœ°è§£é‡Šå¤æ‚çš„å¤šæ¨¡æ€æŒ‡ä»¤ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ã€‚åŸºäºæ­¤æ¶æ„ï¼ŒUniVideoå°†å„ç§è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ç»Ÿä¸€åœ¨å•ä¸€çš„å¤šæ¨¡æ€æŒ‡ä»¤èŒƒå¼ä¸‹ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œè”åˆè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniVideoåœ¨æ–‡æœ¬/å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆå’Œä¸Šä¸‹æ–‡è§†é¢‘ç¼–è¾‘æ–¹é¢ä¸æœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡åŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¿‡å®ƒä»¬ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒUniVideoçš„ç»Ÿä¸€è®¾è®¡å®ç°äº†ä¸¤ç§å½¢å¼çš„æ³›åŒ–ã€‚é¦–å…ˆï¼ŒUniVideoæ”¯æŒä»»åŠ¡ç»„åˆï¼Œä¾‹å¦‚é€šè¿‡åœ¨å•ä¸ªæŒ‡ä»¤ä¸­é›†æˆå¤šä¸ªåŠŸèƒ½æ¥å°†ç¼–è¾‘ä¸é£æ ¼è¿ç§»ç›¸ç»“åˆã€‚å…¶æ¬¡ï¼Œå³ä½¿æ²¡æœ‰ç»è¿‡è‡ªç”±å½¢å¼è§†é¢‘ç¼–è¾‘çš„æ˜¾å¼è®­ç»ƒï¼ŒUniVideoä¹Ÿèƒ½å°†å…¶ç¼–è¾‘èƒ½åŠ›ä»å¤§è§„æ¨¡å›¾åƒç¼–è¾‘æ•°æ®è½¬ç§»åˆ°æ­¤è®¾ç½®ï¼Œå¤„ç†è¯¸å¦‚ç»¿å±è§’è‰²æˆ–æ›´æ”¹è§†é¢‘ä¸­çš„æè´¨ç­‰æœªè§è¿‡çš„æŒ‡ä»¤ã€‚é™¤äº†è¿™äº›æ ¸å¿ƒåŠŸèƒ½å¤–ï¼ŒUniVideoè¿˜æ”¯æŒåŸºäºè§†è§‰æç¤ºçš„è§†é¢‘ç”Ÿæˆï¼Œå…¶ä¸­MLLMè§£é‡Šè§†è§‰æç¤ºå¹¶åœ¨åˆæˆè¿‡ç¨‹ä¸­æŒ‡å¯¼MMDiTã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨å›¾åƒé¢†åŸŸï¼Œç¼ºä¹å¯¹è§†é¢‘ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡çš„ç»Ÿä¸€å»ºæ¨¡èƒ½åŠ›ã€‚é’ˆå¯¹è§†é¢‘çš„ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡æ¨¡å‹ï¼Œæ³›åŒ–èƒ½åŠ›å’Œç»„åˆèƒ½åŠ›è¾ƒå¼±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniVideoçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªå¤šæ¨¡æ€æŒ‡ä»¤èŒƒå¼ä¸‹ï¼Œé€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¥å¤„ç†ä¸åŒçš„ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡åŒæµæ¶æ„ï¼Œåˆ†åˆ«å¤„ç†æŒ‡ä»¤ç†è§£å’Œè§†é¢‘ç”Ÿæˆï¼Œä»è€Œå®ç°å¯¹å¤æ‚æŒ‡ä»¤çš„å‡†ç¡®ç†è§£å’Œè§†è§‰ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniVideoé‡‡ç”¨åŒæµæ¶æ„ã€‚ç¬¬ä¸€è·¯æ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œè´Ÿè´£ç†è§£è¾“å…¥çš„æ–‡æœ¬æˆ–å›¾åƒæŒ‡ä»¤ã€‚ç¬¬äºŒè·¯æ˜¯å¤šæ¨¡æ€æ‰©æ•£Transformerï¼ˆMMDiTï¼‰ï¼Œè´Ÿè´£æ ¹æ®MLLMçš„è¾“å‡ºç”Ÿæˆæˆ–ç¼–è¾‘è§†é¢‘ã€‚MLLMå°†æŒ‡ä»¤ç¼–ç æˆè§†è§‰ç‰¹å¾ï¼Œç„¶åMMDiTåˆ©ç”¨è¿™äº›ç‰¹å¾ç”Ÿæˆè§†é¢‘ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡è”åˆè®­ç»ƒï¼Œå­¦ä¹ ä¸åŒä»»åŠ¡ä¹‹é—´çš„å…±äº«çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniVideoçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„å»ºæ¨¡æ–¹å¼å’ŒåŒæµæ¶æ„ã€‚å®ƒå°†ä¸åŒçš„è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸‹ï¼Œé¿å…äº†ä¸ºæ¯ä¸ªä»»åŠ¡å•ç‹¬è®¾è®¡æ¨¡å‹ã€‚åŒæµæ¶æ„ä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†æŒ‡ä»¤ç†è§£å’Œè§†é¢‘ç”Ÿæˆï¼Œä»è€Œå®ç°æ›´å‡†ç¡®å’Œä¸€è‡´çš„ç»“æœã€‚æ­¤å¤–ï¼ŒUniVideoè¿˜å±•ç¤ºäº†ä»å›¾åƒç¼–è¾‘åˆ°è§†é¢‘ç¼–è¾‘çš„è¿ç§»èƒ½åŠ›ï¼Œä»¥åŠä»»åŠ¡ç»„åˆèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šMLLMé‡‡ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå¹¶è¿›è¡Œå¤šæ¨¡æ€å¾®è°ƒï¼Œä»¥ç†è§£æ–‡æœ¬å’Œå›¾åƒæŒ‡ä»¤ã€‚MMDiTé‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œé€šè¿‡é€æ­¥å»å™ªçš„æ–¹å¼ç”Ÿæˆè§†é¢‘ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ç”ŸæˆæŸå¤±å’ŒæŒ‡ä»¤å¯¹é½æŸå¤±ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ä¸æŒ‡ä»¤ä¸€è‡´ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€è¦å‚è€ƒè®ºæ–‡å…¨æ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UniVideoåœ¨æ–‡æœ¬/å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆå’Œä¸Šä¸‹æ–‡è§†é¢‘ç¼–è¾‘æ–¹é¢ä¸æœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡åŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¿‡å®ƒä»¬ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒUniVideoå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå°†å›¾åƒç¼–è¾‘èƒ½åŠ›è¿ç§»åˆ°è§†é¢‘ç¼–è¾‘ï¼Œå¹¶æ”¯æŒä»»åŠ¡ç»„åˆï¼Œä¾‹å¦‚å°†ç¼–è¾‘ä¸é£æ ¼è¿ç§»ç»“åˆã€‚è¿™äº›ç»“æœè¡¨æ˜UniVideoå…·æœ‰å¾ˆå¼ºçš„å®ç”¨æ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniVideoå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è§†é¢‘å†…å®¹åˆ›ä½œã€è§†é¢‘ç¼–è¾‘ã€è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆå„ç§ç±»å‹çš„è§†é¢‘ï¼Œä¾‹å¦‚åŠ¨ç”»ã€ç‰¹æ•ˆè§†é¢‘ã€äº§å“æ¼”ç¤ºè§†é¢‘ç­‰ã€‚æ­¤å¤–ï¼ŒUniVideoè¿˜å¯ä»¥ç”¨äºè§†é¢‘ç¼–è¾‘ï¼Œä¾‹å¦‚ä¿®æ”¹è§†é¢‘å†…å®¹ã€æ”¹å˜è§†é¢‘é£æ ¼ã€æ·»åŠ ç‰¹æ•ˆç­‰ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºé™ä½äº†è§†é¢‘åˆ›ä½œå’Œç¼–è¾‘çš„é—¨æ§›ï¼Œæé«˜äº†æ•ˆç‡ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›äº†æ›´å¤šçš„åˆ›ä½œå¯èƒ½æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.

