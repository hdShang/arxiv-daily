---
layout: default
title: NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints
---

# NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08565" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08565v1</a>
  <a href="https://arxiv.org/pdf/2510.08565.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08565v1" onclick="toggleFavorite(this, '2510.08565v1', 'NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Accepted by NeurIPS 2025. 22 pages, link: https://github.com/OpenGVLab/NaViL

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NaViLï¼šæ•°æ®çº¦æŸä¸‹åŸç”Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç¼©æ”¾ç‰¹æ€§çš„å†æ€è€ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `åŸç”Ÿè®­ç»ƒ` `æ•°æ®çº¦æŸ` `ç¼©æ”¾ç‰¹æ€§` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMä¾èµ–ç»„åˆè®­ç»ƒï¼Œè§†è§‰ç¼–ç å™¨å’ŒLLMåˆ†ç¦»è®­ç»ƒï¼Œé™åˆ¶äº†å¤šæ¨¡æ€ç¼©æ”¾ç‰¹æ€§çš„æ¢ç´¢ã€‚
2. è®ºæ–‡æå‡ºåŸç”Ÿç«¯åˆ°ç«¯è®­ç»ƒMLLMï¼Œç ”ç©¶æ•°æ®çº¦æŸä¸‹çš„è®¾è®¡ç©ºé—´å’Œç¼©æ”¾ç‰¹æ€§ï¼Œä¼˜åŒ–å…ƒæ¶æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„NaViLåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸Šè¡¨ç°å‡ºç«äº‰æ€§èƒ½ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›è§è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šå¸¸é‡‡ç”¨ç»„åˆè®­ç»ƒèŒƒå¼ï¼Œå³é€šè¿‡è¿ç»­çš„å¤šæ¨¡æ€é¢„è®­ç»ƒå°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸é¢„è®­ç»ƒçš„LLMè¿æ¥èµ·æ¥ã€‚ç„¶è€Œï¼Œç”±äºåˆ†ç¦»çš„è®­ç»ƒæ–¹å¼ï¼Œè¿™ç§èŒƒå¼çš„å¤šæ¨¡æ€ç¼©æ”¾ç‰¹æ€§éš¾ä»¥æ¢ç´¢ã€‚æœ¬æ–‡ç€é‡ç ”ç©¶MLLMsçš„åŸç”Ÿç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¹¶åœ¨å®é™…çš„æ•°æ®çº¦æŸç¯å¢ƒä¸‹ç³»ç»Ÿåœ°ç ”ç©¶å…¶è®¾è®¡ç©ºé—´å’Œç¼©æ”¾ç‰¹æ€§ã€‚é€šè¿‡å¯¹MLLMä¸­å„ç§é€‰æ‹©çš„ä»”ç»†ç ”ç©¶ï¼Œæˆ‘ä»¬è·å¾—äº†æœ€ä½³çš„å…ƒæ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæœ€å¥½åœ°å¹³è¡¡æ€§èƒ½å’Œè®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†åŸç”ŸMLLMçš„ç¼©æ”¾ç‰¹æ€§ï¼Œå¹¶è¡¨æ˜è§†è§‰ç¼–ç å™¨å’ŒLLMä¹‹é—´å­˜åœ¨æ­£ç›¸å…³çš„ç¼©æ”¾å…³ç³»ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºNaViLçš„åŸç”ŸMLLMï¼Œå¹¶ç»“åˆäº†ä¸€ä¸ªç®€å•ä¸”ç»æµé«˜æ•ˆçš„æ–¹æ¡ˆã€‚åœ¨14ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¯å®äº†NaViLç›¸å¯¹äºç°æœ‰MLLMçš„ç«äº‰æ€§èƒ½ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å‘ç°å’Œç»“æœä¸ºæœªæ¥åŸç”ŸMLLMçš„ç ”ç©¶æä¾›äº†æ·±å…¥çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMé€šå¸¸é‡‡ç”¨ç»„åˆè®­ç»ƒï¼Œå³å…ˆåˆ†åˆ«é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’ŒLLMï¼Œç„¶åé€šè¿‡å¤šæ¨¡æ€é¢„è®­ç»ƒå°†äºŒè€…è¿æ¥ã€‚è¿™ç§æ–¹å¼é™åˆ¶äº†å¯¹MLLMæ•´ä½“ç¼©æ”¾ç‰¹æ€§çš„ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®å—é™çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•å¹³è¡¡è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„è§„æ¨¡ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨åŸç”Ÿç«¯åˆ°ç«¯è®­ç»ƒMLLMï¼Œå°†è§†è§‰ç¼–ç å™¨å’ŒLLMä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢MLLMçš„è®¾è®¡ç©ºé—´ï¼Œæ‰¾åˆ°åœ¨æ•°æ®çº¦æŸä¸‹æ€§èƒ½å’Œè®­ç»ƒæˆæœ¬çš„æœ€ä½³å¹³è¡¡ç‚¹ã€‚åŒæ—¶ï¼Œç ”ç©¶è§†è§‰ç¼–ç å™¨å’ŒLLMä¹‹é—´çš„ç¼©æ”¾å…³ç³»ï¼ŒæŒ‡å¯¼æ¨¡å‹è®¾è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNaViLçš„æ•´ä½“æ¶æ„åŒ…å«è§†è§‰ç¼–ç å™¨ã€å¤šæ¨¡æ€è¿æ¥å™¨å’ŒLLMã€‚è§†è§‰ç¼–ç å™¨è´Ÿè´£æå–å›¾åƒç‰¹å¾ï¼Œå¤šæ¨¡æ€è¿æ¥å™¨å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°LLMçš„è¾“å…¥ç©ºé—´ï¼ŒLLMè´Ÿè´£ç”Ÿæˆæ–‡æœ¬ã€‚è®­ç»ƒè¿‡ç¨‹æ˜¯ç«¯åˆ°ç«¯çš„ï¼Œå³åŒæ—¶ä¼˜åŒ–è§†è§‰ç¼–ç å™¨ã€è¿æ¥å™¨å’ŒLLMçš„å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå¯¹åŸç”ŸMLLMçš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼ŒåŒ…æ‹¬å¯¹ä¸åŒè§†è§‰ç¼–ç å™¨ã€å¤šæ¨¡æ€è¿æ¥å™¨å’ŒLLMçš„ç»„åˆè¿›è¡Œè¯„ä¼°ï¼Œä»¥åŠå¯¹è§†è§‰å’Œè¯­è¨€æ¨¡å‹ç¼©æ”¾å…³ç³»çš„æ¢ç´¢ã€‚é€šè¿‡å®éªŒç¡®å®šäº†æœ€ä½³çš„å…ƒæ¶æ„ï¼Œå¹¶åœ¨æ•°æ®çº¦æŸä¸‹å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä¸€ä¸ªå…³é”®çš„è®¾è®¡æ˜¯æ¢ç´¢äº†ä¸åŒè§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ViTã€ConvNeXtï¼‰å’ŒLLMï¼ˆå¦‚LLaMAï¼‰çš„ç»„åˆã€‚æ­¤å¤–ï¼Œè¿˜ç ”ç©¶äº†ä¸åŒçš„å¤šæ¨¡æ€è¿æ¥å™¨ï¼Œå¦‚çº¿æ€§å±‚ã€MLPç­‰ã€‚é€šè¿‡å®éªŒç¡®å®šäº†åœ¨æ•°æ®çº¦æŸä¸‹æ€§èƒ½æœ€ä½³çš„ç»„åˆæ–¹å¼ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„è¯­è¨€æ¨¡å‹æŸå¤±ï¼Œå³æœ€å¤§åŒ–ç”Ÿæˆæ–‡æœ¬çš„æ¦‚ç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

NaViLåœ¨14ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸ç°æœ‰MLLMç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨VQAä»»åŠ¡ä¸Šï¼ŒNaViLå–å¾—äº†ä¸ç°æœ‰SOTAæ¨¡å‹ç›¸è¿‘çš„ç»“æœï¼ŒåŒæ—¶è®­ç»ƒæˆæœ¬æ›´ä½ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†è§†è§‰ç¼–ç å™¨å’ŒLLMä¹‹é—´çš„æ­£ç›¸å…³ç¼©æ”¾å…³ç³»ï¼Œä¸ºæœªæ¥MLLMçš„è®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç†è§£å›¾åƒå’Œæ–‡æœ¬çš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€è§†è§‰æ¨ç†ç­‰ã€‚é€šè¿‡ä¼˜åŒ–MLLMçš„æ¶æ„å’Œè®­ç»ƒæ–¹å¼ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨æ•°æ®å—é™åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿå¤šæ¨¡æ€AIçš„åº”ç”¨è½åœ°ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘ç†è§£ã€æœºå™¨äººæ§åˆ¶ç­‰æ›´å¤æ‚çš„é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.

