---
layout: default
title: D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition
---

# D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08818" target="_blank" class="toolbar-btn">arXiv: 2510.08818v1</a>
    <a href="https://arxiv.org/pdf/2510.08818.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08818v1" 
            onclick="toggleFavorite(this, '2510.08818v1', 'D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yiyang Huang, Yizhou Wang, Yun Fu

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

**Â§áÊ≥®**: This paper has been accepted to EMNLP 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/hukcc/D-CoDe)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**D-CoDeÔºöÈÄöËøáÂä®ÊÄÅÂéãÁº©ÂíåÈóÆÈ¢òÂàÜËß£ÔºåÂ∞ÜÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÁöÑVLMÊâ©Â±ïÂà∞ËßÜÈ¢ëÈ¢ÜÂüü**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÁêÜËß£` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Âä®ÊÄÅÂéãÁº©` `ÈóÆÈ¢òÂàÜËß£` `ÈïøËßÜÈ¢ëÂ§ÑÁêÜ` `ËÆ≠ÁªÉËá™Áî±` `Ëá™ÈÄÇÂ∫îÊ°ÜÊû∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÁöÑVLMÊâ©Â±ïÂà∞ËßÜÈ¢ëÈ¢ÜÂüüÈù¢‰∏¥ÊÑüÁü•Áì∂È¢àÂíåtokenËøáËΩΩÁöÑÊåëÊàòÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ
2. D-CoDeÈÄöËøáÂä®ÊÄÅÂéãÁº©Ëá™ÈÄÇÂ∫îÈÄâÊã©ÂÖ≥ÈîÆÂ∏ßÂπ∂ËÅöÂêàÁ©∫Èó¥tokenÔºåÂêåÊó∂Âà©Áî®ÈóÆÈ¢òÂàÜËß£Â∞ÜÂ§çÊùÇÈóÆÈ¢òÊãÜËß£‰∏∫Â≠êÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåD-CoDeÂú®Â§ö‰∏™ËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®ÈïøËßÜÈ¢ë‰ªªÂä°‰∏äÂ±ïÁé∞‰∫ÜÂ∑®Â§ßÊΩúÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫D-CoDeÁöÑËÆ≠ÁªÉËá™Áî±Ê°ÜÊû∂ÔºåÊó®Âú®Â∞ÜÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÊúâÊïàÊâ©Â±ïÂà∞ËßÜÈ¢ëÈ¢ÜÂüü„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂØÜÈõÜÂíåÊó∂Â∫èÈïøÁöÑËßÜÈ¢ëËæìÂÖ•Êó∂Èù¢‰∏¥ÊÑüÁü•Áì∂È¢àÂíåtokenËøáËΩΩÁöÑÊåëÊàò„ÄÇD-CoDeÈÄöËøáÂä®ÊÄÅÂéãÁº©ÁºìËß£ÊÑüÁü•Áì∂È¢àÔºåËá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©‰ª£Ë°®ÊÄßÂ∏ßÔºåÂπ∂ËøõË°åÂÜÖÂÆπÊÑüÁü•ÁöÑÁ©∫Èó¥tokenËÅöÂêàÔºå‰ªéËÄåÂáèÂ∞ëÂÜó‰ΩôÂπ∂‰øùÁïô‰ø°ÊÅØÂÜÖÂÆπ„ÄÇÂêåÊó∂ÔºåÈÄöËøáÈóÆÈ¢òÂàÜËß£ÁºìËß£tokenËøáËΩΩÔºåÂ∞ÜÂéüÂßãÊü•ËØ¢ÂàÜËß£‰∏∫Â≠êÈóÆÈ¢òÔºåÂºïÂØºÊ®°ÂûãÂÖ≥Ê≥®ËßÜÈ¢ëÁöÑ‰∏çÂêåÊñπÈù¢ÔºåÂÆûÁé∞Êõ¥ÂÖ®Èù¢ÁöÑÁêÜËß£„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåD-CoDeÊúâÊïàÂú∞ÊèêÂçá‰∫ÜÂêÑÁßçÂü∫ÂáÜÊµãËØï‰∏≠ÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÁ™ÅÊòæ‰∫ÜD-CoDeÂú®Â§ÑÁêÜÂ§çÊùÇËßÜÈ¢ëËØ≠Ë®Ä‰ªªÂä°ÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜÈ¢ëËØ≠Ë®ÄÊ®°ÂûãÔºàVid-LLMÔºâÊûÑÂª∫ÊñπÊ≥ïÈÄöÂ∏∏Âü∫‰∫éÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ„ÄÇÁÑ∂ËÄåÔºåÁõ¥Êé•Â∞ÜÂõæÂÉèVLMÂ∫îÁî®‰∫éËßÜÈ¢ëÊó∂ÔºåÁî±‰∫éËßÜÈ¢ëÊï∞ÊçÆÈáèÂ§ß„ÄÅÊó∂Â∫èÈïøÔºåÂØºËá¥Ê®°ÂûãÈù¢‰∏¥‰∏§‰∏™‰∏ªË¶ÅÈóÆÈ¢òÔºö‰∏ÄÊòØÊÑüÁü•Áì∂È¢àÔºåÂç≥Ê®°ÂûãÈöæ‰ª•‰ªéÂ§ßÈáèÂ∏ß‰∏≠ÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØÔºõ‰∫åÊòØtokenËøáËΩΩÔºåÂç≥ËøáÂ§öÁöÑËßÜËßâtokenË∂ÖËøá‰∫ÜÊ®°ÂûãÁöÑÂ§ÑÁêÜËÉΩÂäõÔºåÂΩ±ÂìçÁêÜËß£ÊïàÊûú„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöD-CoDeÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂä®ÊÄÅÂéãÁº©ÂíåÈóÆÈ¢òÂàÜËß£Êù•Ëß£ÂÜ≥‰∏äËø∞ÈóÆÈ¢ò„ÄÇÂä®ÊÄÅÂéãÁº©Êó®Âú®ÂáèÂ∞ëËæìÂÖ•ËßÜÈ¢ëÁöÑÂÜó‰Ωô‰ø°ÊÅØÔºå‰øùÁïôÂÖ≥ÈîÆÂ∏ßÂíåÈáçË¶ÅÂå∫ÂüüÔºå‰ªéËÄåÁºìËß£ÊÑüÁü•Áì∂È¢à„ÄÇÈóÆÈ¢òÂàÜËß£ÂàôÂ∞ÜÂ§çÊùÇÁöÑËßÜÈ¢ëÁêÜËß£ÈóÆÈ¢òÂàÜËß£‰∏∫Â§ö‰∏™Â≠êÈóÆÈ¢òÔºåÂºïÂØºÊ®°ÂûãÈÄêÊ≠•ÁêÜËß£ËßÜÈ¢ëÂÜÖÂÆπÔºåÂáèËΩªtokenËøáËΩΩÁöÑÂΩ±Âìç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöD-CoDeÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Ê®°ÂùóÔºöÂä®ÊÄÅÂéãÁº©Ê®°ÂùóÂíåÈóÆÈ¢òÂàÜËß£Ê®°Âùó„ÄÇÂä®ÊÄÅÂéãÁº©Ê®°ÂùóÈ¶ñÂÖàÂØπËßÜÈ¢ëÂ∏ßËøõË°åÈáçË¶ÅÊÄßËØÑ‰º∞ÔºåÈÄâÊã©‰ª£Ë°®ÊÄßÂ∏ß„ÄÇÁÑ∂ÂêéÔºåÂØπÈÄâÂÆöÁöÑÂ∏ßËøõË°åÂÜÖÂÆπÊÑüÁü•ÁöÑÁ©∫Èó¥tokenËÅöÂêàÔºåÂáèÂ∞ëÂÜó‰Ωô‰ø°ÊÅØ„ÄÇÈóÆÈ¢òÂàÜËß£Ê®°ÂùóÂ∞ÜÂéüÂßãÈóÆÈ¢òÂàÜËß£‰∏∫Â§ö‰∏™Â≠êÈóÆÈ¢òÔºåÊØè‰∏™Â≠êÈóÆÈ¢òÂÖ≥Ê≥®ËßÜÈ¢ëÁöÑ‰∏çÂêåÊñπÈù¢„ÄÇÊúÄÂêéÔºåÊ®°ÂûãÊ†πÊçÆÂ≠êÈóÆÈ¢òÁöÑÁ≠îÊ°àÁªºÂêàÁêÜËß£ËßÜÈ¢ëÂÜÖÂÆπ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöD-CoDeÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ËÆ≠ÁªÉËá™Áî±ÁöÑËá™ÈÄÇÂ∫îÊ°ÜÊû∂„ÄÇ‰∏éÈúÄË¶ÅÂ§ßÈáèËÆ≠ÁªÉÊï∞ÊçÆÁöÑ‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåD-CoDeÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÂç≥ÂèØÂ∫îÁî®‰∫éÁé∞ÊúâÁöÑÂõæÂÉèÈ¢ÑËÆ≠ÁªÉVLM„ÄÇÂä®ÊÄÅÂéãÁº©Ê®°ÂùóÂíåÈóÆÈ¢òÂàÜËß£Ê®°ÂùóÁöÑËÆæËÆ°ËÉΩÂ§üÊúâÊïàÂú∞ÁºìËß£ÊÑüÁü•Áì∂È¢àÂíåtokenËøáËΩΩÈóÆÈ¢òÔºåÊèêÂçáÊ®°ÂûãÂú®ËßÜÈ¢ëÁêÜËß£‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂä®ÊÄÅÂéãÁº©Ê®°Âùó‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•ËØÑ‰º∞ËßÜÈ¢ëÂ∏ßÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂ÈÄâÊã©top-k‰∏™Â∏ß„ÄÇÁ©∫Èó¥tokenËÅöÂêàÈááÁî®ÂÜÖÂÆπÊÑüÁü•ÁöÑÊ±†ÂåñÊìç‰ΩúÔºåÊ†πÊçÆtokenÁöÑÈáçË¶ÅÊÄßËøõË°åÂä†ÊùÉÂπ≥Âùá„ÄÇÈóÆÈ¢òÂàÜËß£Ê®°Âùó‰ΩøÁî®È¢ÑÂÆö‰πâÁöÑÊ®°ÊùøÂ∞ÜÂéüÂßãÈóÆÈ¢òÂàÜËß£‰∏∫Â§ö‰∏™Â≠êÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

D-CoDeÂú®Â§ö‰∏™ËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂ∞§ÂÖ∂Âú®ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠ÔºåD-CoDeË°®Áé∞Âá∫Âº∫Â§ßÁöÑÂ§ÑÁêÜËÉΩÂäõÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§çÊùÇËßÜÈ¢ëËØ≠Ë®Ä‰ªªÂä°‰∏≠ÁöÑÊΩúÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåD-CoDeËÉΩÂ§üÊúâÊïàÂú∞ÁºìËß£ÊÑüÁü•Áì∂È¢àÂíåtokenËøáËΩΩÈóÆÈ¢òÔºåÊèêÂçáËßÜÈ¢ëÁêÜËß£ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

D-CoDeÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éËßÜÈ¢ëÂÜÖÂÆπÁêÜËß£„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅËßÜÈ¢ëÊ£ÄÁ¥¢„ÄÅËßÜÈ¢ëÊëòË¶ÅÁîüÊàêÁ≠âÈ¢ÜÂüü„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÂçáËßÜÈ¢ëËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈïøËßÜÈ¢ëÂíåÂ§çÊùÇÈóÆÈ¢òÊó∂ÁöÑÊÄßËÉΩÔºå‰∏∫ÂºÄÂèëÊõ¥Êô∫ËÉΩÁöÑËßÜÈ¢ëÂàÜÊûêÁ≥ªÁªüÊèê‰æõÊäÄÊúØÊîØÊåÅÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÊú™Êù•ÂèëÂ±ïÊΩúÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Video large language models (Vid-LLMs), which excel in diverse video-language tasks, can be effectively constructed by adapting image-pretrained vision-language models (VLMs). However, this adaptation remains challenging, as it requires processing dense and temporally extended visual inputs that exceed the capacity of image-based models. This paper identifies the perception bottleneck and token overload as key challenges in extending image-based VLMs to the video domain. To address these issues, we propose D-CoDe, a training-free adaptation framework that incorporates dynamic compression and question decomposition. Specifically, dynamic compression alleviates the perception bottleneck through adaptive selection of representative frames and content-aware aggregation of spatial tokens, thereby reducing redundancy while preserving informative content. In parallel, question decomposition mitigates token overload by reformulating the original query into sub-questions, guiding the model to focus on distinct aspects of the video and enabling more comprehensive understanding. Experiments demonstrate that D-CoDe effectively improves video understanding across various benchmarks. Furthermore, strong performance on the challenging long-video benchmark highlights the potential of D-CoDe in handling complex video-language tasks. Code is available at https://github.com/hukcc/D-CoDe.

