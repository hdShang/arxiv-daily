---
layout: default
title: D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition
---

# D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08818" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08818v1</a>
  <a href="https://arxiv.org/pdf/2510.08818.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08818v1" onclick="toggleFavorite(this, '2510.08818v1', 'D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiyang Huang, Yizhou Wang, Yun Fu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: This paper has been accepted to EMNLP 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hukcc/D-CoDe)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**D-CoDeï¼šé€šè¿‡åŠ¨æ€å‹ç¼©å’Œé—®é¢˜åˆ†è§£ï¼Œå°†å›¾åƒé¢„è®­ç»ƒçš„VLMæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `è§†è§‰è¯­è¨€æ¨¡å‹` `åŠ¨æ€å‹ç¼©` `é—®é¢˜åˆ†è§£` `é•¿è§†é¢‘å¤„ç†` `è®­ç»ƒè‡ªç”±` `è‡ªé€‚åº”æ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å›¾åƒé¢„è®­ç»ƒçš„VLMæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸé¢ä¸´æ„ŸçŸ¥ç“¶é¢ˆå’Œtokenè¿‡è½½çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. D-CoDeé€šè¿‡åŠ¨æ€å‹ç¼©è‡ªé€‚åº”é€‰æ‹©å…³é”®å¸§å¹¶èšåˆç©ºé—´tokenï¼ŒåŒæ—¶åˆ©ç”¨é—®é¢˜åˆ†è§£å°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºå­é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒD-CoDeåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨é•¿è§†é¢‘ä»»åŠ¡ä¸Šå±•ç°äº†å·¨å¤§æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºD-CoDeçš„è®­ç»ƒè‡ªç”±æ¡†æ¶ï¼Œæ—¨åœ¨å°†å›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æœ‰æ•ˆæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¯†é›†å’Œæ—¶åºé•¿çš„è§†é¢‘è¾“å…¥æ—¶é¢ä¸´æ„ŸçŸ¥ç“¶é¢ˆå’Œtokenè¿‡è½½çš„æŒ‘æˆ˜ã€‚D-CoDeé€šè¿‡åŠ¨æ€å‹ç¼©ç¼“è§£æ„ŸçŸ¥ç“¶é¢ˆï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©ä»£è¡¨æ€§å¸§ï¼Œå¹¶è¿›è¡Œå†…å®¹æ„ŸçŸ¥çš„ç©ºé—´tokenèšåˆï¼Œä»è€Œå‡å°‘å†—ä½™å¹¶ä¿ç•™ä¿¡æ¯å†…å®¹ã€‚åŒæ—¶ï¼Œé€šè¿‡é—®é¢˜åˆ†è§£ç¼“è§£tokenè¿‡è½½ï¼Œå°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨è§†é¢‘çš„ä¸åŒæ–¹é¢ï¼Œå®ç°æ›´å…¨é¢çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD-CoDeæœ‰æ•ˆåœ°æå‡äº†å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œçªæ˜¾äº†D-CoDeåœ¨å¤„ç†å¤æ‚è§†é¢‘è¯­è¨€ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVid-LLMï¼‰æ„å»ºæ–¹æ³•é€šå¸¸åŸºäºå›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚ç„¶è€Œï¼Œç›´æ¥å°†å›¾åƒVLMåº”ç”¨äºè§†é¢‘æ—¶ï¼Œç”±äºè§†é¢‘æ•°æ®é‡å¤§ã€æ—¶åºé•¿ï¼Œå¯¼è‡´æ¨¡å‹é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯æ„ŸçŸ¥ç“¶é¢ˆï¼Œå³æ¨¡å‹éš¾ä»¥ä»å¤§é‡å¸§ä¸­æå–å…³é”®ä¿¡æ¯ï¼›äºŒæ˜¯tokenè¿‡è½½ï¼Œå³è¿‡å¤šçš„è§†è§‰tokenè¶…è¿‡äº†æ¨¡å‹çš„å¤„ç†èƒ½åŠ›ï¼Œå½±å“ç†è§£æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šD-CoDeçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€å‹ç¼©å’Œé—®é¢˜åˆ†è§£æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚åŠ¨æ€å‹ç¼©æ—¨åœ¨å‡å°‘è¾“å…¥è§†é¢‘çš„å†—ä½™ä¿¡æ¯ï¼Œä¿ç•™å…³é”®å¸§å’Œé‡è¦åŒºåŸŸï¼Œä»è€Œç¼“è§£æ„ŸçŸ¥ç“¶é¢ˆã€‚é—®é¢˜åˆ†è§£åˆ™å°†å¤æ‚çš„è§†é¢‘ç†è§£é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œå¼•å¯¼æ¨¡å‹é€æ­¥ç†è§£è§†é¢‘å†…å®¹ï¼Œå‡è½»tokenè¿‡è½½çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šD-CoDeæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šåŠ¨æ€å‹ç¼©æ¨¡å—å’Œé—®é¢˜åˆ†è§£æ¨¡å—ã€‚åŠ¨æ€å‹ç¼©æ¨¡å—é¦–å…ˆå¯¹è§†é¢‘å¸§è¿›è¡Œé‡è¦æ€§è¯„ä¼°ï¼Œé€‰æ‹©ä»£è¡¨æ€§å¸§ã€‚ç„¶åï¼Œå¯¹é€‰å®šçš„å¸§è¿›è¡Œå†…å®¹æ„ŸçŸ¥çš„ç©ºé—´tokenèšåˆï¼Œå‡å°‘å†—ä½™ä¿¡æ¯ã€‚é—®é¢˜åˆ†è§£æ¨¡å—å°†åŸå§‹é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œæ¯ä¸ªå­é—®é¢˜å…³æ³¨è§†é¢‘çš„ä¸åŒæ–¹é¢ã€‚æœ€åï¼Œæ¨¡å‹æ ¹æ®å­é—®é¢˜çš„ç­”æ¡ˆç»¼åˆç†è§£è§†é¢‘å†…å®¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šD-CoDeçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è®­ç»ƒè‡ªç”±çš„è‡ªé€‚åº”æ¡†æ¶ã€‚ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒD-CoDeæ— éœ€é¢å¤–è®­ç»ƒå³å¯åº”ç”¨äºç°æœ‰çš„å›¾åƒé¢„è®­ç»ƒVLMã€‚åŠ¨æ€å‹ç¼©æ¨¡å—å’Œé—®é¢˜åˆ†è§£æ¨¡å—çš„è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£æ„ŸçŸ¥ç“¶é¢ˆå’Œtokenè¿‡è½½é—®é¢˜ï¼Œæå‡æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€å‹ç¼©æ¨¡å—ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è¯„ä¼°è§†é¢‘å¸§çš„é‡è¦æ€§ï¼Œå¹¶é€‰æ‹©top-kä¸ªå¸§ã€‚ç©ºé—´tokenèšåˆé‡‡ç”¨å†…å®¹æ„ŸçŸ¥çš„æ± åŒ–æ“ä½œï¼Œæ ¹æ®tokençš„é‡è¦æ€§è¿›è¡ŒåŠ æƒå¹³å‡ã€‚é—®é¢˜åˆ†è§£æ¨¡å—ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡æ¿å°†åŸå§‹é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

D-CoDeåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å°¤å…¶åœ¨é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒD-CoDeè¡¨ç°å‡ºå¼ºå¤§çš„å¤„ç†èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è§†é¢‘è¯­è¨€ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD-CoDeèƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£æ„ŸçŸ¥ç“¶é¢ˆå’Œtokenè¿‡è½½é—®é¢˜ï¼Œæå‡è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

D-CoDeå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºè§†é¢‘å†…å®¹ç†è§£ã€æ™ºèƒ½ç›‘æ§ã€è§†é¢‘æ£€ç´¢ã€è§†é¢‘æ‘˜è¦ç”Ÿæˆç­‰é¢†åŸŸã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘å’Œå¤æ‚é—®é¢˜æ—¶çš„æ€§èƒ½ï¼Œä¸ºå¼€å‘æ›´æ™ºèƒ½çš„è§†é¢‘åˆ†æç³»ç»Ÿæä¾›æŠ€æœ¯æ”¯æŒï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæœªæ¥å‘å±•æ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video large language models (Vid-LLMs), which excel in diverse video-language tasks, can be effectively constructed by adapting image-pretrained vision-language models (VLMs). However, this adaptation remains challenging, as it requires processing dense and temporally extended visual inputs that exceed the capacity of image-based models. This paper identifies the perception bottleneck and token overload as key challenges in extending image-based VLMs to the video domain. To address these issues, we propose D-CoDe, a training-free adaptation framework that incorporates dynamic compression and question decomposition. Specifically, dynamic compression alleviates the perception bottleneck through adaptive selection of representative frames and content-aware aggregation of spatial tokens, thereby reducing redundancy while preserving informative content. In parallel, question decomposition mitigates token overload by reformulating the original query into sub-questions, guiding the model to focus on distinct aspects of the video and enabling more comprehensive understanding. Experiments demonstrate that D-CoDe effectively improves video understanding across various benchmarks. Furthermore, strong performance on the challenging long-video benchmark highlights the potential of D-CoDe in handling complex video-language tasks. Code is available at https://github.com/hukcc/D-CoDe.

