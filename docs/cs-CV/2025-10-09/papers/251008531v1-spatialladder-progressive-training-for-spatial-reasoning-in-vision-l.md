---
layout: default
title: SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models
---

# SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08531" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08531v1</a>
  <a href="https://arxiv.org/pdf/2510.08531.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08531v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08531v1', 'SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Project Page: https://zju-real.github.io/SpatialLadder/ Code: https://github.com/ZJU-REAL/SpatialLadder

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SpatialLadderï¼šé€šè¿‡æ¸è¿›å¼è®­ç»ƒæå‡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `æ¸è¿›å¼å­¦ä¹ ` `å¤šæ¨¡æ€æ•°æ®é›†` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹å¯¹ç©ºé—´æ„ŸçŸ¥å’Œç†è§£çš„å±‚çº§åŸºç¡€çš„æ„å»ºã€‚
2. è®ºæ–‡æå‡ºSpatialLadderæ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå¤šæ¨¡æ€æ•°æ®é›†å’Œè®¾è®¡ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒæ¡†æ¶ï¼Œé€æ­¥æå‡æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatialLadderæ¨¡å‹åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç©ºé—´æ¨ç†èƒ½åŠ›çš„ç»¼åˆæ–¹æ³•ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ„ŸçŸ¥å’Œç†è§£çš„å±‚çº§åŸºç¡€çš„å»ºç«‹ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«26610ä¸ªæ ·æœ¬çš„å¤šæ¨¡æ€æ•°æ®é›†SpatialLadder-26kï¼Œæ¶µç›–äº†å¯¹è±¡å®šä½ã€å•å›¾åƒã€å¤šè§†è§’å’Œè§†é¢‘ç©ºé—´æ¨ç†ä»»åŠ¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®¾è®¡äº†ä¸€ä¸ªä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡å¯¹è±¡å®šä½å»ºç«‹ç©ºé—´æ„ŸçŸ¥ï¼Œç„¶åé€šè¿‡å¤šç»´ç©ºé—´ä»»åŠ¡å‘å±•ç©ºé—´ç†è§£ï¼Œæœ€åé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯çš„å¥–åŠ±æ¥åŠ å¼ºå¤æ‚æ¨ç†ã€‚ç”±æ­¤äº§ç”Ÿçš„SpatialLadderæ¨¡å‹ï¼ˆ30äº¿å‚æ•°ï¼‰åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡æ¯”åŸºçº¿æ¨¡å‹æé«˜äº†23.4%ï¼Œè¶…è¿‡GPT-4o 20.8%ï¼Œè¶…è¿‡Gemini-2.0-Flash 10.1%ã€‚æ­¤å¤–ï¼ŒSpatialLadderåœ¨é¢†åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæé«˜äº†7.2%ï¼Œè¡¨æ˜ä»æ„ŸçŸ¥åˆ°æ¨ç†çš„æ¸è¿›å¼è®­ç»ƒå¯¹äºé²æ£’çš„ç©ºé—´æ™ºèƒ½è‡³å…³é‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆç†è§£å’Œæ¨ç†å›¾åƒæˆ–è§†é¢‘ä¸­çš„ç©ºé—´å…³ç³»ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºï¼Œç°æœ‰æ–¹æ³•ç›´æ¥å­¦ä¹ å¤æ‚çš„ç©ºé—´æ¨ç†ï¼Œè€Œå¿½ç•¥äº†ç©ºé—´æ„ŸçŸ¥å’Œç†è§£çš„å±‚çº§åŸºç¡€ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¸è¿›å¼è®­ç»ƒï¼Œé€æ­¥æ„å»ºæ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹è±¡å®šä½å»ºç«‹ç©ºé—´æ„ŸçŸ¥ï¼›ç„¶åï¼Œé€šè¿‡å¤šç»´ç©ºé—´ä»»åŠ¡å‘å±•ç©ºé—´ç†è§£ï¼›æœ€åï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åŠ å¼ºå¤æ‚æ¨ç†ã€‚è¿™ç§ç”±æµ…å…¥æ·±ã€å¾ªåºæ¸è¿›çš„æ–¹å¼ï¼Œèƒ½å¤Ÿä½¿æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ å’ŒæŒæ¡ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpatialLadderçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š(1) **ç©ºé—´æ„ŸçŸ¥é˜¶æ®µ**ï¼šåˆ©ç”¨å¯¹è±¡å®šä½ä»»åŠ¡è®­ç»ƒæ¨¡å‹è¯†åˆ«å’Œå®šä½å›¾åƒä¸­çš„ç‰©ä½“ã€‚(2) **ç©ºé—´ç†è§£é˜¶æ®µ**ï¼šé€šè¿‡å•å›¾åƒã€å¤šè§†è§’å’Œè§†é¢‘ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œè®­ç»ƒæ¨¡å‹ç†è§£ç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚(3) **å¤æ‚æ¨ç†é˜¶æ®µ**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œè®­ç»ƒæ¨¡å‹è¿›è¡Œæ›´å¤æ‚çš„ç©ºé—´æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªæ¸è¿›å¼è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¨¡æ‹Ÿäº†äººç±»å­¦ä¹ ç©ºé—´æ¨ç†çš„è¿‡ç¨‹ï¼Œä»ç®€å•çš„æ„ŸçŸ¥åˆ°å¤æ‚çš„æ¨ç†ï¼Œé€æ­¥æå‡æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ã€‚æ­¤å¤–ï¼ŒSpatialLadder-26kæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè¯¥æ¡†æ¶çš„è®­ç»ƒæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šSpatialLadder-26kæ•°æ®é›†åŒ…å«å¯¹è±¡å®šä½ã€å•å›¾åƒã€å¤šè§†è§’å’Œè§†é¢‘ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œè¦†ç›–äº†ä¸åŒçš„æ¨¡æ€å’Œéš¾åº¦çº§åˆ«ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œå¯¹è±¡å®šä½ï¼Œå¹¶è®¾è®¡äº†ç‰¹å®šçš„å¥–åŠ±å‡½æ•°ç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚æ¨¡å‹é‡‡ç”¨3Bå‚æ•°çš„æ¶æ„ï¼Œå…·ä½“ç½‘ç»œç»“æ„ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SpatialLadderæ¨¡å‹åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹³å‡æ¯”åŸºçº¿æ¨¡å‹æé«˜äº†23.4%ï¼Œè¶…è¿‡GPT-4o 20.8%ï¼Œè¶…è¿‡Gemini-2.0-Flash 10.1%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒSpatialLadderåœ¨é¢†åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œæé«˜äº†7.2%ï¼Œè¯æ˜äº†å…¶è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SpatialLadderçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æœºå™¨æ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„å†³ç­–å’Œè¡ŒåŠ¨ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.

