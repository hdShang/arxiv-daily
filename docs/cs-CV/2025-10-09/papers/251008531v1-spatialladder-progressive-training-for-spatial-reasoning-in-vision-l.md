---
layout: default
title: SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models
---

# SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08531" target="_blank" class="toolbar-btn">arXiv: 2510.08531v1</a>
    <a href="https://arxiv.org/pdf/2510.08531.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08531v1" 
            onclick="toggleFavorite(this, '2510.08531v1', 'SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

**Â§áÊ≥®**: Project Page: https://zju-real.github.io/SpatialLadder/ Code: https://github.com/ZJU-REAL/SpatialLadder

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SpatialLadderÔºöÈÄöËøáÊ∏êËøõÂºèËÆ≠ÁªÉÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Á©∫Èó¥Êé®ÁêÜ` `Ê∏êËøõÂºèÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜ` `Âº∫ÂåñÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Á©∫Èó¥Êé®ÁêÜÊñπÈù¢Ë°®Áé∞‰∏çË∂≥Ôºå‰∏ªË¶ÅÂéüÂõ†ÊòØÁº∫‰πèÂØπÁ©∫Èó¥ÊÑüÁü•ÂíåÁêÜËß£ÁöÑÂ±ÇÁ∫ßÂü∫Á°ÄÁöÑÊûÑÂª∫„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫SpatialLadderÊñπÊ≥ïÔºåÈÄöËøáÊûÑÂª∫Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÂíåËÆæËÆ°‰∏âÈò∂ÊÆµÊ∏êËøõÂºèËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÈÄêÊ≠•ÊèêÂçáÊ®°ÂûãÁöÑÁ©∫Èó¥Êô∫ËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSpatialLadderÊ®°ÂûãÂú®Á©∫Èó¥Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÁªºÂêàÊñπÊ≥ï„ÄÇ‰ΩúËÄÖÊåáÂá∫ÔºåÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπÊÑüÁü•ÂíåÁêÜËß£ÁöÑÂ±ÇÁ∫ßÂü∫Á°ÄÁöÑÂª∫Á´ãÔºåÂØºËá¥ÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºå‰ΩúËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´26610‰∏™Ê†∑Êú¨ÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜSpatialLadder-26kÔºåÊ∂µÁõñ‰∫ÜÂØπË±°ÂÆö‰Ωç„ÄÅÂçïÂõæÂÉè„ÄÅÂ§öËßÜËßíÂíåËßÜÈ¢ëÁ©∫Èó¥Êé®ÁêÜ‰ªªÂä°„ÄÇÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∏âÈò∂ÊÆµÊ∏êËøõÂºèËÆ≠ÁªÉÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂È¶ñÂÖàÈÄöËøáÂØπË±°ÂÆö‰ΩçÂª∫Á´ãÁ©∫Èó¥ÊÑüÁü•ÔºåÁÑ∂ÂêéÈÄöËøáÂ§öÁª¥Á©∫Èó¥‰ªªÂä°ÂèëÂ±ïÁ©∫Èó¥ÁêÜËß£ÔºåÊúÄÂêéÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂíåÂèØÈ™åËØÅÁöÑÂ•ñÂä±Êù•Âä†Âº∫Â§çÊùÇÊé®ÁêÜ„ÄÇÁî±Ê≠§‰∫ßÁîüÁöÑSpatialLadderÊ®°ÂûãÔºà30‰∫øÂèÇÊï∞ÔºâÂú®Á©∫Èó¥Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂπ≥ÂùáÊØîÂü∫Á∫øÊ®°ÂûãÊèêÈ´ò‰∫Ü23.4%ÔºåË∂ÖËøáGPT-4o 20.8%ÔºåË∂ÖËøáGemini-2.0-Flash 10.1%„ÄÇÊ≠§Â§ñÔºåSpatialLadderÂú®È¢ÜÂüüÂ§ñÂü∫ÂáÜÊµãËØï‰∏≠‰øùÊåÅ‰∫ÜÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåÊèêÈ´ò‰∫Ü7.2%ÔºåË°®Êòé‰ªéÊÑüÁü•Âà∞Êé®ÁêÜÁöÑÊ∏êËøõÂºèËÆ≠ÁªÉÂØπ‰∫éÈ≤ÅÊ£íÁöÑÁ©∫Èó¥Êô∫ËÉΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Á©∫Èó¥Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊó†Ê≥ïÊúâÊïàÁêÜËß£ÂíåÊé®ÁêÜÂõæÂÉèÊàñËßÜÈ¢ë‰∏≠ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇ‰∏ªË¶ÅÁóõÁÇπÂú®‰∫éÔºåÁé∞ÊúâÊñπÊ≥ïÁõ¥Êé•Â≠¶‰π†Â§çÊùÇÁöÑÁ©∫Èó¥Êé®ÁêÜÔºåËÄåÂøΩÁï•‰∫ÜÁ©∫Èó¥ÊÑüÁü•ÂíåÁêÜËß£ÁöÑÂ±ÇÁ∫ßÂü∫Á°ÄÔºåÂØºËá¥Ê®°ÂûãÈöæ‰ª•Ê≥õÂåñÂà∞Êñ∞ÁöÑÂú∫ÊôØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊ∏êËøõÂºèËÆ≠ÁªÉÔºåÈÄêÊ≠•ÊûÑÂª∫Ê®°ÂûãÁöÑÁ©∫Èó¥Êô∫ËÉΩ„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÂØπË±°ÂÆö‰ΩçÂª∫Á´ãÁ©∫Èó¥ÊÑüÁü•ÔºõÁÑ∂ÂêéÔºåÈÄöËøáÂ§öÁª¥Á©∫Èó¥‰ªªÂä°ÂèëÂ±ïÁ©∫Èó¥ÁêÜËß£ÔºõÊúÄÂêéÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Âä†Âº∫Â§çÊùÇÊé®ÁêÜ„ÄÇËøôÁßçÁî±ÊµÖÂÖ•Ê∑±„ÄÅÂæ™Â∫èÊ∏êËøõÁöÑÊñπÂºèÔºåËÉΩÂ§ü‰ΩøÊ®°ÂûãÊõ¥Â•ΩÂú∞Â≠¶‰π†ÂíåÊéåÊè°Á©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSpatialLadderÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö(1) **Á©∫Èó¥ÊÑüÁü•Èò∂ÊÆµ**ÔºöÂà©Áî®ÂØπË±°ÂÆö‰Ωç‰ªªÂä°ËÆ≠ÁªÉÊ®°ÂûãËØÜÂà´ÂíåÂÆö‰ΩçÂõæÂÉè‰∏≠ÁöÑÁâ©‰Ωì„ÄÇ(2) **Á©∫Èó¥ÁêÜËß£Èò∂ÊÆµ**ÔºöÈÄöËøáÂçïÂõæÂÉè„ÄÅÂ§öËßÜËßíÂíåËßÜÈ¢ëÁ©∫Èó¥Êé®ÁêÜ‰ªªÂä°ÔºåËÆ≠ÁªÉÊ®°ÂûãÁêÜËß£Áâ©‰Ωì‰πãÈó¥ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇ(3) **Â§çÊùÇÊé®ÁêÜÈò∂ÊÆµ**Ôºö‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÔºåÈÄöËøáÂèØÈ™åËØÅÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåËÆ≠ÁªÉÊ®°ÂûãËøõË°åÊõ¥Â§çÊùÇÁöÑÁ©∫Èó¥Êé®ÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ê∏êËøõÂºèËÆ≠ÁªÉÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Ê®°Êãü‰∫Ü‰∫∫Á±ªÂ≠¶‰π†Á©∫Èó¥Êé®ÁêÜÁöÑËøáÁ®ãÔºå‰ªéÁÆÄÂçïÁöÑÊÑüÁü•Âà∞Â§çÊùÇÁöÑÊé®ÁêÜÔºåÈÄêÊ≠•ÊèêÂçáÊ®°ÂûãÁöÑÁ©∫Èó¥Êô∫ËÉΩ„ÄÇÊ≠§Â§ñÔºåSpatialLadder-26kÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫‰πü‰∏∫ËØ•Ê°ÜÊû∂ÁöÑËÆ≠ÁªÉÊèê‰æõ‰∫ÜÊúâÂäõÊîØÊåÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSpatialLadder-26kÊï∞ÊçÆÈõÜÂåÖÂê´ÂØπË±°ÂÆö‰Ωç„ÄÅÂçïÂõæÂÉè„ÄÅÂ§öËßÜËßíÂíåËßÜÈ¢ëÁ©∫Èó¥Êé®ÁêÜ‰ªªÂä°ÔºåË¶ÜÁõñ‰∫Ü‰∏çÂêåÁöÑÊ®°ÊÄÅÂíåÈöæÂ∫¶Á∫ßÂà´„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®‰∫Ü‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ËøõË°åÂØπË±°ÂÆö‰ΩçÔºåÂπ∂ËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÂ•ñÂä±ÂáΩÊï∞Áî®‰∫éÂº∫ÂåñÂ≠¶‰π†„ÄÇÊ®°ÂûãÈááÁî®3BÂèÇÊï∞ÁöÑÊû∂ÊûÑÔºåÂÖ∑‰ΩìÁΩëÁªúÁªìÊûÑÁªÜËäÇÊú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SpatialLadderÊ®°ÂûãÂú®Á©∫Èó¥Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ≥ÂùáÊØîÂü∫Á∫øÊ®°ÂûãÊèêÈ´ò‰∫Ü23.4%ÔºåË∂ÖËøáGPT-4o 20.8%ÔºåË∂ÖËøáGemini-2.0-Flash 10.1%„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåSpatialLadderÂú®È¢ÜÂüüÂ§ñÂü∫ÂáÜÊµãËØï‰∏≠‰πüË°®Áé∞Âá∫Ëâ≤ÔºåÊèêÈ´ò‰∫Ü7.2%ÔºåËØÅÊòé‰∫ÜÂÖ∂ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SpatialLadderÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÊú∫Âô®Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™‰∏ªÁöÑÂÜ≥Á≠ñÂíåË°åÂä®„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂú®Êô∫ËÉΩÂÆ∂Â±Ö„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÁ≠âÈ¢ÜÂüüÂèëÊå•ÈáçË¶Å‰ΩúÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.

