---
layout: default
title: MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning
---

# MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08567" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08567v3</a>
  <a href="https://arxiv.org/pdf/2510.08567.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08567v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08567v3', 'MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: We have come across a recent approach that has not been properly attributed at the time of submission and compared in a fair setting. Therefore, we would like to withdraw the paper to address these concerns

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mbzuai-oryx/MATRIX)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMATRIXæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€Agentè°ƒä¼˜å®ç°ç¨³å¥çš„å·¥å…·ä½¿ç”¨æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `å·¥å…·ä½¿ç”¨` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `åå¥½å­¦ä¹ ` `Agentè°ƒä¼˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨å·¥å…·ä½¿ç”¨æ¨ç†æ–¹é¢å—é™äºé«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®çš„ç¨€ç¼ºä»¥åŠé«˜æ˜‚çš„äººå·¥æ ‡æ³¨æˆæœ¬ã€‚
2. æå‡ºMATRIXæ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨åˆæˆå¤šæ¨¡æ€è½¨è¿¹å’Œåå¥½å¯¹ï¼Œå®ç°VLMæ§åˆ¶å™¨çš„ç¨³å¥å·¥å…·ä½¿ç”¨æ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMATRIXåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰å¼€æºå’Œé—­æºVLMï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºæ§åˆ¶å™¨ï¼Œç”¨äºè®¿é—®å¤–éƒ¨å·¥å…·ä»¥è¿›è¡Œå¤æ‚çš„æ¨ç†å’Œå†³ç­–ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä»ç„¶å—åˆ°é«˜è´¨é‡å¤šæ¨¡æ€è½¨è¿¹ç¨€ç¼ºå’Œæ‰‹åŠ¨æ ‡æ³¨æˆæœ¬çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥è§†è§‰ä¸ºä¸­å¿ƒAgentè°ƒä¼˜æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è‡ªåŠ¨åˆæˆå¤šæ¨¡æ€è½¨è¿¹ï¼Œç”Ÿæˆé€æ­¥åå¥½å¯¹ï¼Œå¹¶è®­ç»ƒVLMæ§åˆ¶å™¨ä»¥å®ç°ç¨³å¥çš„å·¥å…·ä½¿ç”¨æ¨ç†ã€‚è¯¥æµç¨‹é¦–å…ˆæ„å»ºM-TRACEï¼Œä¸€ä¸ªåŒ…å«28.5Kå¤šæ¨¡æ€ä»»åŠ¡å’Œ177KéªŒè¯è½¨è¿¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»è€Œå®ç°åŸºäºæ¨¡ä»¿çš„è½¨è¿¹è°ƒä¼˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼€å‘äº†MATRIX Agentï¼Œä¸€ä¸ªåœ¨M-TRACEä¸Šè¿›è¡Œå¾®è°ƒçš„æ§åˆ¶å™¨ï¼Œç”¨äºé€æ­¥å·¥å…·æ¨ç†ã€‚ä¸ºäº†å®ç°æ›´ç²¾ç»†çš„å¯¹é½ï¼Œè¿›ä¸€æ­¥å¼•å…¥äº†Pref-Xï¼Œä¸€ä¸ªåŒ…å«11Kè‡ªåŠ¨ç”Ÿæˆçš„åå¥½å¯¹çš„é›†åˆï¼Œå¹¶é€šè¿‡é€æ­¥åå¥½å­¦ä¹ å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚åœ¨Agent-Xã€GTAå’ŒGAIAä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMATRIXå§‹ç»ˆä¼˜äºå¼€æºå’Œé—­æºVLMï¼Œå±•ç¤ºäº†å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†å’Œå†³ç­–ä»»åŠ¡ä¸­ï¼Œéœ€è¦å€ŸåŠ©å¤–éƒ¨å·¥å…·ï¼Œä½†ç¼ºä¹è¶³å¤Ÿçš„é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®æ¥è®­ç»ƒå’Œä¼˜åŒ–è¿™äº›æ¨¡å‹ï¼ŒåŒæ—¶äººå·¥æ ‡æ³¨æˆæœ¬è¿‡é«˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´å·¥å…·ä½¿ç”¨æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡çš„å¤šæ¨¡æ€è½¨è¿¹æ•°æ®å’Œåå¥½å¯¹æ•°æ®ï¼Œæ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œåå¥½å­¦ä¹ ï¼Œå¯¹VLMæ§åˆ¶å™¨è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæå‡å…¶å·¥å…·ä½¿ç”¨æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMATRIXæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **M-TRACEæ•°æ®é›†æ„å»º**ï¼šè‡ªåŠ¨åˆæˆå¤§è§„æ¨¡å¤šæ¨¡æ€ä»»åŠ¡å’ŒéªŒè¯è½¨è¿¹ã€‚2) **Agentåˆå§‹åŒ–**ï¼šåœ¨M-TRACEæ•°æ®é›†ä¸Šè¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼Œè®­ç»ƒå¾—åˆ°MATRIX Agentã€‚3) **Pref-Xæ•°æ®é›†æ„å»º**ï¼šè‡ªåŠ¨ç”Ÿæˆåå¥½å¯¹æ•°æ®ï¼Œç”¨äºåå¥½å­¦ä¹ ã€‚4) **åå¥½å­¦ä¹ **ï¼šåˆ©ç”¨Pref-Xæ•°æ®é›†å¯¹MATRIX Agentè¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå¤šæ¨¡æ€è½¨è¿¹å’Œåå¥½å¯¹æ•°æ®çš„æ–¹æ³•ï¼Œä»è€Œè§£å†³äº†VLMåœ¨å·¥å…·ä½¿ç”¨æ¨ç†æ–¹é¢çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œé¿å…äº†æ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šM-TRACEæ•°æ®é›†åŒ…å«28.5Kå¤šæ¨¡æ€ä»»åŠ¡å’Œ177KéªŒè¯è½¨è¿¹ã€‚Pref-Xæ•°æ®é›†åŒ…å«11Kè‡ªåŠ¨ç”Ÿæˆçš„åå¥½å¯¹ã€‚MATRIX AgentåŸºäºVLMè¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨æ¨¡ä»¿å­¦ä¹ å’Œåå¥½å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ æŸå¤±å’Œåå¥½å­¦ä¹ æŸå¤±ã€‚å…·ä½“VLMæ¶æ„å’Œè¶…å‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼ˆæ­¤å¤„æœªçŸ¥ï¼Œæ ¹æ®è®ºæ–‡è¡¥å……ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMATRIXåœ¨Agent-Xã€GTAå’ŒGAIAä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œé—­æºVLMã€‚å…·ä½“æ¥è¯´ï¼ŒMATRIXåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºå¯¹æ¯”æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾å…·ä½“æ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¤æ‚æ¨ç†å’Œå†³ç­–çš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€æ™ºèƒ½åŠ©æ‰‹ã€æ¸¸æˆAIç­‰ã€‚é€šè¿‡æå‡VLMçš„å·¥å…·ä½¿ç”¨æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„ç³»ç»Ÿï¼Œä»è€Œæé«˜ç”Ÿäº§æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ›´å¤šæ¨¡æ€å’Œæ›´å¤æ‚çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.

