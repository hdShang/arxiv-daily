---
layout: default
title: MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions
---

# MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07828" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07828v3</a>
  <a href="https://arxiv.org/pdf/2510.07828.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07828v3" onclick="toggleFavorite(this, '2510.07828v3', 'MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaen Kogashi, Anoop Cherian, Meng-Yu Jennifer Kuo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09 (æ›´æ–°: 2025-12-04)

**å¤‡æ³¨**: Accepted to WACV 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMHOIæ•°æ®é›†å’ŒMMHOI-Netï¼Œç”¨äºå»ºæ¨¡å¤æ‚3Då¤šäººå¤šç‰©äº¤äº’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `äºº-ç‰©äº¤äº’` `3Dåœºæ™¯ç†è§£` `å¤šäººäº¤äº’` `Transformerç½‘ç»œ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Däºº-ç‰©äº¤äº’æ•°æ®é›†éš¾ä»¥æ•æ‰çœŸå®åœºæ™¯ä¸­å¤æ‚çš„å¤šäººå¤šç‰©äº¤äº’ï¼Œé™åˆ¶äº†ç›¸å…³ç ”ç©¶çš„è¿›å±•ã€‚
2. æå‡ºMMHOIæ•°æ®é›†å’ŒMMHOI-Netï¼Œåˆ©ç”¨ç»“æ„åŒ–åŒpatchè¡¨ç¤ºå»ºæ¨¡å¯¹è±¡åŠå…¶äº¤äº’ï¼Œå¹¶ç»“åˆåŠ¨ä½œè¯†åˆ«æå‡äº¤äº’é¢„æµ‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMMHOI-Netåœ¨MMHOIå’ŒCORE4Dæ•°æ®é›†ä¸Šå‡å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å‡†ç¡®æ€§å’Œé‡å»ºè´¨é‡ä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çœŸå®åœºæ™¯é€šå¸¸åŒ…å«å¤šäººä¸å¤šç‰©ä½“çš„å› æœã€ç›®æ ‡å¯¼å‘æˆ–åä½œäº¤äº’ã€‚ç°æœ‰çš„3Däºº-ç‰©äº¤äº’(HOI)åŸºå‡†æµ‹è¯•ä»…è€ƒè™‘äº†è¿™äº›å¤æ‚äº¤äº’çš„ä¸€å°éƒ¨åˆ†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MMHOIâ€”â€”ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šäººå¤šç‰©äº¤äº’æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª12ä¸ªæ—¥å¸¸åœºæ™¯çš„å›¾åƒã€‚MMHOIä¸ºæ¯ä¸ªäººå’Œç‰©ä½“æä¾›å®Œæ•´çš„3Då½¢çŠ¶å’Œå§¿åŠ¿æ ‡æ³¨ï¼Œä»¥åŠ78ä¸ªåŠ¨ä½œç±»åˆ«å’Œ14ä¸ªäº¤äº’ç‰¹å®šèº«ä½“éƒ¨ä½çš„æ ‡ç­¾ï¼Œä¸ºä¸‹ä¸€ä»£HOIç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æµ‹è¯•å¹³å°ã€‚åŸºäºMMHOIï¼Œæˆ‘ä»¬æå‡ºäº†MMHOI-Netï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„åŸºäºTransformerçš„ç¥ç»ç½‘ç»œï¼Œç”¨äºè”åˆä¼°è®¡äºº-ç‰©3Då‡ ä½•å½¢çŠ¶ã€å®ƒä»¬çš„äº¤äº’å’Œç›¸å…³åŠ¨ä½œã€‚æˆ‘ä»¬æ¡†æ¶çš„ä¸€ä¸ªå…³é”®åˆ›æ–°æ˜¯ç”¨äºå»ºæ¨¡å¯¹è±¡åŠå…¶äº¤äº’çš„ç»“æ„åŒ–åŒpatchè¡¨ç¤ºï¼Œç»“åˆåŠ¨ä½œè¯†åˆ«æ¥å¢å¼ºäº¤äº’é¢„æµ‹ã€‚åœ¨MMHOIå’Œæœ€è¿‘æå‡ºçš„CORE4Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šHOIå»ºæ¨¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‡†ç¡®æ€§å’Œé‡å»ºè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚MMHOIæ•°æ®é›†å¯åœ¨https://zenodo.org/records/17711786å…¬å¼€è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Däºº-ç‰©äº¤äº’(HOI)æ•°æ®é›†æ— æ³•å……åˆ†å»ºæ¨¡çœŸå®åœºæ™¯ä¸­å¤æ‚çš„å¤šäººå¤šç‰©äº¤äº’çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨å•äººä¸å•ç‰©çš„äº¤äº’ï¼Œå¿½ç•¥äº†å¤šäººä¹‹é—´çš„åä½œã€ç‰©ä½“ä¹‹é—´çš„å…³ç³»ä»¥åŠäº¤äº’çš„å› æœæ€§å’Œç›®æ ‡å¯¼å‘æ€§ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€æ ‡æ³¨å…¨é¢çš„å¤šäººå¤šç‰©äº¤äº’æ•°æ®é›†MMHOIï¼Œå¹¶è®¾è®¡ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œMMHOI-Netæ¥è”åˆä¼°è®¡äºº-ç‰©3Då‡ ä½•å½¢çŠ¶ã€äº¤äº’å’ŒåŠ¨ä½œã€‚é€šè¿‡å¼•å…¥ç»“æ„åŒ–çš„åŒpatchè¡¨ç¤ºæ¥å»ºæ¨¡å¯¹è±¡åŠå…¶äº¤äº’ï¼Œå¹¶ç»“åˆåŠ¨ä½œè¯†åˆ«æ¥å¢å¼ºäº¤äº’é¢„æµ‹ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æ›´å…¨é¢åœ°æ•æ‰å¤æ‚äº¤äº’çš„æœ¬è´¨ï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMHOI-Netæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„ç«¯åˆ°ç«¯ç¥ç»ç½‘ç»œï¼Œå…¶æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šç”¨äºæå–å›¾åƒä¸­äººå’Œç‰©ä½“çš„è§†è§‰ç‰¹å¾ã€‚2) 3Då‡ ä½•ä¼°è®¡æ¨¡å—ï¼šç”¨äºä¼°è®¡äººå’Œç‰©ä½“çš„3Då½¢çŠ¶å’Œå§¿åŠ¿ã€‚3) äº¤äº’å»ºæ¨¡æ¨¡å—ï¼šä½¿ç”¨ç»“æ„åŒ–çš„åŒpatchè¡¨ç¤ºæ¥å»ºæ¨¡å¯¹è±¡åŠå…¶äº¤äº’ã€‚4) åŠ¨ä½œè¯†åˆ«æ¨¡å—ï¼šç”¨äºè¯†åˆ«äººçš„åŠ¨ä½œã€‚5) äº¤äº’é¢„æµ‹æ¨¡å—ï¼šç»“åˆè§†è§‰ç‰¹å¾ã€3Då‡ ä½•ä¿¡æ¯å’ŒåŠ¨ä½œä¿¡æ¯æ¥é¢„æµ‹äºº-ç‰©äº¤äº’ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ç»“æ„åŒ–çš„åŒpatchè¡¨ç¤ºæ¥å»ºæ¨¡å¯¹è±¡åŠå…¶äº¤äº’ã€‚ä¼ ç»Ÿçš„HOIå»ºæ¨¡æ–¹æ³•é€šå¸¸å°†äººå’Œç‰©ä½“è§†ä¸ºç‹¬ç«‹çš„ä¸ªä½“ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚åŒpatchè¡¨ç¤ºå°†æ¯ä¸ªç‰©ä½“è¡¨ç¤ºä¸ºä¸¤ä¸ªpatchï¼šä¸€ä¸ªè¡¨ç¤ºç‰©ä½“çš„æ•´ä½“å¤–è§‚ï¼Œå¦ä¸€ä¸ªè¡¨ç¤ºç‰©ä½“ä¸äººäº¤äº’çš„åŒºåŸŸã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰äº¤äº’çš„å±€éƒ¨ç‰¹å¾ï¼Œæé«˜äº¤äº’é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MMHOI-Netä¸­ï¼ŒåŒpatchè¡¨ç¤ºé€šè¿‡Transformerç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œä»¥å­¦ä¹ patchä¹‹é—´çš„å…³ç³»ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬3Då‡ ä½•é‡å»ºæŸå¤±ã€åŠ¨ä½œè¯†åˆ«æŸå¤±å’Œäº¤äº’é¢„æµ‹æŸå¤±ã€‚ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®ç»è¿‡äº†å¤§é‡çš„å®éªŒéªŒè¯ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MMHOI-Netåœ¨MMHOIæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨äº¤äº’é¢„æµ‹å‡†ç¡®ç‡å’Œ3Då‡ ä½•é‡å»ºè´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨CORE4Dæ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚HOIå»ºæ¨¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†æœ‰è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººã€è™šæ‹Ÿç°å®ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯ç†è§£äººç±»çš„æ„å›¾ï¼Œä»è€Œæ›´å¥½åœ°ä¸äººç±»åä½œå®Œæˆä»»åŠ¡ã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼Œè¯¥æŠ€æœ¯å¯ä»¥åˆ›å»ºæ›´é€¼çœŸçš„äº¤äº’ä½“éªŒã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯è¯†åˆ«è¡Œäººä¸å‘¨å›´ç‰©ä½“çš„äº¤äº’ï¼Œä»è€Œåšå‡ºæ›´å®‰å…¨çš„å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world scenes often feature multiple humans interacting with multiple objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D human-object interaction (HOI) benchmarks consider only a fraction of these complex interactions. To close this gap, we present MMHOI -- a large-scale, Multi-human Multi-object Interaction dataset consisting of images from 12 everyday scenarios. MMHOI offers complete 3D shape and pose annotations for every person and object, along with labels for 78 action categories and 14 interaction-specific body parts, providing a comprehensive testbed for next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an end-to-end transformer-based neural network for jointly estimating human-object 3D geometries, their interactions, and associated actions. A key innovation in our framework is a structured dual-patch representation for modeling objects and their interactions, combined with action recognition to enhance the interaction prediction. Experiments on MMHOI and the recently proposed CORE4D datasets demonstrate that our approach achieves state-of-the-art performance in multi-HOI modeling, excelling in both accuracy and reconstruction quality. The MMHOI dataset is publicly available at https://zenodo.org/records/17711786.

