---
layout: default
title: LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation
---

# LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08318" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08318v2</a>
  <a href="https://arxiv.org/pdf/2510.08318.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08318v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08318v2', 'LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yushi Huang, Xingtong Ge, Ruihao Gong, Chengtao Lv, Jun Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09 (æ›´æ–°: 2025-11-21)

**å¤‡æ³¨**: Code will be released upon acceptance

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LinVideoï¼šä¸€ç§åè®­ç»ƒæ¡†æ¶ï¼Œå®ç°é«˜æ•ˆè§†é¢‘ç”Ÿæˆä¸­O(n)å¤æ‚åº¦Attention**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `çº¿æ€§æ³¨æ„åŠ›` `åè®­ç»ƒ` `æ¨¡å‹åŠ é€Ÿ` `é€‰æ‹©æ€§è¿ç§»` `åˆ†å¸ƒåŒ¹é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†é¢‘æ‰©æ•£æ¨¡å‹è®¡ç®—æˆæœ¬éšåºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œä¸»è¦ç“¶é¢ˆåœ¨äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦ã€‚
2. LinVideoæå‡ºä¸€ç§åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°å°†éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºçº¿æ€§æ³¨æ„åŠ›å±‚ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLinVideoåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡è’¸é¦è¿›ä¸€æ­¥é™ä½å»¶è¿Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘æ‰©æ•£æ¨¡å‹(DMs)å®ç°äº†é«˜è´¨é‡çš„è§†é¢‘åˆæˆã€‚ç„¶è€Œï¼Œç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦ï¼Œå…¶è®¡ç®—æˆæœ¬éšåºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚è™½ç„¶çº¿æ€§æ³¨æ„åŠ›é™ä½äº†æˆæœ¬ï¼Œä½†ç”±äºçº¿æ€§æ³¨æ„åŠ›æœ‰é™çš„è¡¨è¾¾èƒ½åŠ›ä»¥åŠè§†é¢‘ç”Ÿæˆä¸­æ—¶ç©ºå»ºæ¨¡çš„å¤æ‚æ€§ï¼Œå®Œå…¨æ›¿æ¢äºŒæ¬¡æ³¨æ„åŠ›éœ€è¦æ˜‚è´µçš„é¢„è®­ç»ƒã€‚æœ¬æ–‡æå‡ºLinVideoï¼Œä¸€ç§é«˜æ•ˆçš„æ— æ•°æ®åè®­ç»ƒæ¡†æ¶ï¼Œç”¨çº¿æ€§æ³¨æ„åŠ›æ›¿æ¢ç›®æ ‡æ•°é‡çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ŒåŒæ—¶ä¿æŒåŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸åŒå±‚çš„å¯æ›¿æ¢æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬æ²¡æœ‰é‡‡ç”¨æ‰‹åŠ¨æˆ–å¯å‘å¼é€‰æ‹©ï¼Œè€Œæ˜¯å°†å±‚é€‰æ‹©å®šä¹‰ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œå¹¶æå‡ºäº†é€‰æ‹©æ€§è¿ç§»ï¼Œè‡ªåŠ¨ä¸”æ¸è¿›åœ°å°†å±‚è½¬æ¢ä¸ºçº¿æ€§æ³¨æ„åŠ›ï¼ŒåŒæ—¶å¯¹æ€§èƒ½çš„å½±å“æœ€å°ã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœç°æœ‰ç›®æ ‡å‡½æ•°åœ¨æ­¤è¿ç§»è¿‡ç¨‹ä¸­çš„æ— æ•ˆæ€§å’Œä½æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éšæ—¶åˆ†å¸ƒåŒ¹é…(ADM)ç›®æ ‡ï¼Œè¯¥ç›®æ ‡å¯¹é½äº†æ²¿é‡‡æ ·è½¨è¿¹ä»»ä½•æ—¶é—´æ­¥é•¿çš„æ ·æœ¬åˆ†å¸ƒã€‚è¯¥ç›®æ ‡æ˜¯é«˜æ•ˆçš„ï¼Œå¹¶ä¸”å¯ä»¥æ¢å¤æ¨¡å‹æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†1.25-2.00å€çš„åŠ é€Ÿï¼Œè€Œæˆ‘ä»¬çš„4æ­¥è’¸é¦æ¨¡å‹è¿›ä¸€æ­¥å®ç°äº†15.92å€çš„å»¶è¿Ÿé™ä½ï¼Œä¸”è§†è§‰è´¨é‡ä¸‹é™æœ€å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚ç›´æ¥ç”¨çº¿æ€§æ³¨æ„åŠ›æ›¿æ¢æ‰€æœ‰è‡ªæ³¨æ„åŠ›å±‚ä¼šä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ï¼Œéœ€è¦æ˜‚è´µçš„é¢„è®­ç»ƒæ‰èƒ½å¼¥è¡¥æ€§èƒ½æŸå¤±ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸è¿›è¡Œé¢å¤–é¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆåœ°é™ä½è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLinVideoçš„æ ¸å¿ƒæ€è·¯æ˜¯å¹¶éæ‰€æœ‰è‡ªæ³¨æ„åŠ›å±‚éƒ½åŒç­‰é‡è¦ï¼Œéƒ¨åˆ†å±‚å¯ä»¥ç”¨çº¿æ€§æ³¨æ„åŠ›æ›¿æ¢è€Œå¯¹æ€§èƒ½å½±å“å¾ˆå°ã€‚é€šè¿‡é€‰æ‹©æ€§åœ°è¿ç§»éƒ¨åˆ†å±‚åˆ°çº¿æ€§æ³¨æ„åŠ›ï¼Œå¯ä»¥åœ¨é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œæœ€å¤§ç¨‹åº¦åœ°ä¿ç•™åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®¾è®¡æœ‰æ•ˆçš„è®­ç»ƒç›®æ ‡æ¥å¼¥è¡¥æ›¿æ¢æ³¨æ„åŠ›æœºåˆ¶å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLinVideoæ˜¯ä¸€ä¸ªåè®­ç»ƒæ¡†æ¶ï¼Œä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1) **é€‰æ‹©æ€§è¿ç§»**ï¼šå°†å±‚é€‰æ‹©é—®é¢˜å»ºæ¨¡ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œè‡ªåŠ¨é€‰æ‹©å¯ä»¥æ›¿æ¢ä¸ºçº¿æ€§æ³¨æ„åŠ›çš„å±‚ã€‚2) **éšæ—¶åˆ†å¸ƒåŒ¹é… (ADM)**ï¼šä½¿ç”¨ADMç›®æ ‡å‡½æ•°æ¥å¯¹é½åŸå§‹æ¨¡å‹å’Œæ›¿æ¢åçš„æ¨¡å‹åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­çš„åˆ†å¸ƒï¼Œä»è€Œæ¢å¤æ¨¡å‹æ€§èƒ½ã€‚æ•´ä½“æµç¨‹æ˜¯åœ¨é¢„è®­ç»ƒå¥½çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸Šï¼Œå…ˆè¿›è¡Œé€‰æ‹©æ€§è¿ç§»ï¼Œç„¶åä½¿ç”¨ADMç›®æ ‡å‡½æ•°è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šLinVideoçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **é€‰æ‹©æ€§è¿ç§»ç­–ç•¥**ï¼šé€šè¿‡äºŒå…ƒåˆ†ç±»çš„æ–¹å¼è‡ªåŠ¨é€‰æ‹©å¯æ›¿æ¢çš„å±‚ï¼Œé¿å…äº†æ‰‹åŠ¨æˆ–å¯å‘å¼é€‰æ‹©çš„å±€é™æ€§ã€‚2) **éšæ—¶åˆ†å¸ƒåŒ¹é… (ADM) ç›®æ ‡å‡½æ•°**ï¼šADMç›®æ ‡å‡½æ•°èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹é½é‡‡æ ·è½¨è¿¹ä¸Šä»»æ„æ—¶é—´æ­¥é•¿çš„æ ·æœ¬åˆ†å¸ƒï¼Œä»è€Œæ¢å¤æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡é«˜ã€‚

**å…³é”®è®¾è®¡**ï¼š1) **é€‰æ‹©æ€§è¿ç§»çš„äºŒå…ƒåˆ†ç±»å™¨**ï¼šä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„åˆ†ç±»å™¨æ¥é¢„æµ‹æ¯ä¸ªè‡ªæ³¨æ„åŠ›å±‚æ˜¯å¦å¯ä»¥è¢«çº¿æ€§æ³¨æ„åŠ›å±‚æ›¿æ¢ã€‚åˆ†ç±»å™¨çš„è¾“å…¥æ˜¯è¯¥å±‚çš„æ¿€æ´»å€¼ï¼Œè¾“å‡ºæ˜¯äºŒå…ƒæ ‡ç­¾ï¼ˆå¯æ›¿æ¢/ä¸å¯æ›¿æ¢ï¼‰ã€‚2) **ADMç›®æ ‡å‡½æ•°**ï¼šADMç›®æ ‡å‡½æ•°çš„ç›®æ ‡æ˜¯æœ€å°åŒ–åŸå§‹æ¨¡å‹å’Œæ›¿æ¢åçš„æ¨¡å‹åœ¨é‡‡æ ·è½¨è¿¹ä¸Šä»»æ„æ—¶é—´æ­¥é•¿ä¸Šçš„æ ·æœ¬åˆ†å¸ƒå·®å¼‚ã€‚å…·ä½“å®ç°ä¸Šï¼Œå¯ä»¥ä½¿ç”¨KLæ•£åº¦æˆ–JSæ•£åº¦æ¥è¡¡é‡åˆ†å¸ƒå·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LinVideoåœ¨å¤šä¸ªè§†é¢‘ç”Ÿæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒLinVideoå¯ä»¥åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°1.25-2.00å€çš„åŠ é€Ÿã€‚é€šè¿‡4æ­¥è’¸é¦ï¼ŒLinVideoè¿›ä¸€æ­¥å®ç°äº†15.92å€çš„å»¶è¿Ÿé™ä½ï¼Œä¸”è§†è§‰è´¨é‡ä¸‹é™æœ€å°ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLinVideoæ˜¯ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„è§†é¢‘ç”ŸæˆåŠ é€Ÿæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LinVideoå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦é«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„åœºæ™¯ï¼Œä¾‹å¦‚å®æ—¶è§†é¢‘ç¼–è¾‘ã€ä½å»¶è¿Ÿè§†é¢‘æµåª’ä½“ã€ç§»åŠ¨è®¾å¤‡ä¸Šçš„è§†é¢‘ç”Ÿæˆç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒLinVideoä½¿å¾—åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œé«˜è´¨é‡è§†é¢‘ç”Ÿæˆæˆä¸ºå¯èƒ½ï¼Œå¹¶åŠ é€Ÿäº†è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.

