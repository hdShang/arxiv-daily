---
layout: default
title: "CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning"
---

# CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08003" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08003v1</a>
  <a href="https://arxiv.org/pdf/2510.08003.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08003v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08003v1', 'CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weihuang Lin, Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCIR-CoTï¼Œé€šè¿‡ç«¯åˆ°ç«¯æ€ç»´é“¾æ¨ç†å®ç°å¯è§£é‡Šçš„ç»„åˆå›¾åƒæ£€ç´¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»„åˆå›¾åƒæ£€ç´¢` `æ€ç»´é“¾æ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `å¯è§£é‡Šæ€§` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç»„åˆå›¾åƒæ£€ç´¢æ–¹æ³•ï¼Œå¦‚åŸºäºVLMå’ŒMLLMçš„æ¨¡å‹ï¼Œé€šå¸¸æ˜¯é»‘ç›’ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥ç†è§£æ£€ç´¢é€»è¾‘ã€‚
2. CIR-CoTé€šè¿‡å¼•å…¥æ˜¾å¼çš„æ€ç»´é“¾(CoT)æ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚
3. è®ºæ–‡åˆ›å»ºäº†åŒ…å«æè¿°ã€æ¨ç†å’Œç»“è®ºçš„ç»“æ„åŒ–CoTæ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶åœ¨FashionIQã€CIRRå’ŒCIRCOæ•°æ®é›†ä¸ŠéªŒè¯äº†CIR-CoTçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»„åˆå›¾åƒæ£€ç´¢(CIR)æ—¨åœ¨æ ¹æ®å‚è€ƒå›¾åƒå’Œä¿®æ”¹æ–‡æœ¬æ‰¾åˆ°ç›®æ ‡å›¾åƒï¼Œå…¶æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæ‰§è¡Œè·¨è§†è§‰å’Œè¯­ä¹‰æ¨¡æ€çš„ç»Ÿä¸€æ¨ç†ã€‚å½“å‰åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹(VLMs, å¦‚CLIP)å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLMs, å¦‚Qwen-VL)çš„æ–¹æ³•è™½ç„¶å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ä¸»è¦ä½œä¸ºâ€œé»‘ç›’â€è¿è¡Œã€‚è¿™ç§å›ºæœ‰çš„ä¸é€æ˜æ€§ä¸ä»…é˜»æ­¢ç”¨æˆ·ç†è§£æ£€ç´¢çš„åŸºæœ¬åŸç†ï¼Œè¿˜é™åˆ¶äº†æ¨¡å‹éµå¾ªå¤æ‚ã€ç»†ç²’åº¦æŒ‡ä»¤çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†CIR-CoTï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé¢å‘æ£€ç´¢çš„ç«¯åˆ°ç«¯MLLMï¼Œæ—¨åœ¨é›†æˆæ˜¾å¼çš„æ€ç»´é“¾(CoT)æ¨ç†ã€‚é€šè¿‡è¿«ä½¿æ¨¡å‹é¦–å…ˆç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†é“¾ï¼ŒCIR-CoTå¢å¼ºäº†å…¶æ•è·å…³é”®è·¨æ¨¡æ€äº¤äº’çš„èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„æ£€ç´¢ï¼ŒåŒæ—¶ä½¿å…¶å†³ç­–è¿‡ç¨‹é€æ˜åŒ–ã€‚ç”±äºç°æœ‰çš„æ•°æ®é›†(å¦‚FashionIQå’ŒCIRR)ç¼ºä¹å¿…è¦çš„æ¨ç†æ•°æ®ï¼Œæˆ‘ä»¬å·¥ä½œçš„ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯ä½¿ç”¨åŒ…å«æè¿°ã€æ¨ç†å’Œç»“è®ºçš„ä¸‰é˜¶æ®µè¿‡ç¨‹åˆ›å»ºç»“æ„åŒ–çš„CoTæ ‡æ³¨ã€‚ç„¶åï¼Œå¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆè¿™ç§ç»“æ„åŒ–çš„è¾“å‡ºï¼Œç„¶åå°†å…¶æœ€ç»ˆæ£€ç´¢æ„å›¾ç¼–ç åˆ°ä¸“ç”¨åµŒå…¥ä¸­ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCIR-CoTåœ¨é¢†åŸŸå†…æ•°æ®é›†(FashionIQ, CIRR)ä¸Šå–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶åœ¨é¢†åŸŸå¤–CIRCOæ•°æ®é›†ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ›´æœ‰æ•ˆå’Œå€¼å¾—ä¿¡èµ–çš„æ£€ç´¢ç³»ç»Ÿå¼€è¾Ÿäº†ä¸€æ¡æ–°é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç»„åˆå›¾åƒæ£€ç´¢(CIR)ä»»åŠ¡æ—¨åœ¨æ ¹æ®ç»™å®šçš„å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œä»å›¾åƒåº“ä¸­æ£€ç´¢å‡ºç¬¦åˆæè¿°çš„ç›®æ ‡å›¾åƒã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºVLMå’ŒMLLMçš„æ–¹æ³•ï¼Œé€šå¸¸ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œç”¨æˆ·éš¾ä»¥ç†è§£æ¨¡å‹åšå‡ºæ£€ç´¢å†³ç­–çš„åŸå› ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å¤æ‚ã€ç»†ç²’åº¦çš„æŒ‡ä»¤æ—¶èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCIR-CoTçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥æ€ç»´é“¾(Chain-of-Thought, CoT)æ¨ç†ï¼Œä½¿æ¨¡å‹åœ¨æ£€ç´¢ä¹‹å‰å…ˆç”Ÿæˆä¸€ä¸ªå¯è§£é‡Šçš„æ¨ç†é“¾ã€‚è¿™ä¸ªæ¨ç†é“¾æ˜ç¡®åœ°è¡¨è¾¾äº†æ¨¡å‹å¦‚ä½•ç†è§£å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œä»¥åŠå¦‚ä½•å°†å®ƒä»¬ç»“åˆèµ·æ¥è¿›è¡Œæ£€ç´¢ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹ä¸ä»…æé«˜äº†æ£€ç´¢çš„å‡†ç¡®æ€§ï¼Œè¿˜å¢å¼ºäº†å…¶å¯è§£é‡Šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCIR-CoTçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¾“å…¥æ¨¡å—ï¼šæ¥æ”¶å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ã€‚2) CoTç”Ÿæˆæ¨¡å—ï¼šåˆ©ç”¨MLLMç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„æ¨ç†é“¾ï¼ŒåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬çš„æè¿°ã€æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç»“è®ºã€‚3) åµŒå…¥ç¼–ç æ¨¡å—ï¼šå°†ç”Ÿæˆçš„æ¨ç†é“¾ç¼–ç æˆä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œç”¨äºæ£€ç´¢ã€‚4) æ£€ç´¢æ¨¡å—ï¼šæ ¹æ®åµŒå…¥å‘é‡ä»å›¾åƒåº“ä¸­æ£€ç´¢å‡ºæœ€åŒ¹é…çš„ç›®æ ‡å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šCIR-CoTæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†æ€ç»´é“¾æ¨ç†å¼•å…¥åˆ°ç»„åˆå›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCIR-CoTä¸ä»…æé«˜äº†æ£€ç´¢çš„å‡†ç¡®æ€§ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥ç†è§£æ¨¡å‹åšå‡ºæ£€ç´¢å†³ç­–çš„åŸå› ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ›å»ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„CoTæ ‡æ³¨æ•°æ®é›†ï¼Œä¸ºè®­ç»ƒCIR-CoTæ¨¡å‹æä¾›äº†å¿…è¦çš„æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šCIR-CoTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨MLLMï¼ˆä¾‹å¦‚Qwen-VLï¼‰ä½œä¸ºCoTç”Ÿæˆæ¨¡å—çš„åŸºç¡€æ¨¡å‹ã€‚2) è®¾è®¡äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„CoTæ ‡æ³¨è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æè¿°ã€æ¨ç†å’Œç»“è®ºã€‚3) ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°æ¥è®­ç»ƒåµŒå…¥ç¼–ç æ¨¡å—ï¼Œä½¿å¾—ç›¸ä¼¼çš„å›¾åƒå’Œæ–‡æœ¬æè¿°å…·æœ‰ç›¸ä¼¼çš„åµŒå…¥å‘é‡ã€‚4) å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆç»“æ„åŒ–çš„CoTè¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CIR-CoTåœ¨FashionIQå’ŒCIRRç­‰æ•°æ®é›†ä¸Šå–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶åœ¨é¢†åŸŸå¤–CIRCOæ•°æ®é›†ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†å¼ºè°ƒäº†å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ä¼˜è¶Šè¡¨ç°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CIR-CoTåœ¨ç”µå•†ã€æ—¶å°šã€å®¤å†…è®¾è®¡ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸Šä¼ ä¸€å¼ è¡£æœçš„å›¾ç‰‡ï¼Œå¹¶è¾“å…¥â€œæ¢ä¸ªé¢œè‰²â€ç­‰æè¿°ï¼Œå¿«é€Ÿæ‰¾åˆ°æ»¡è¶³è¦æ±‚çš„å•†å“ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡æ£€ç´¢ç³»ç»Ÿçš„ç”¨æˆ·ä½“éªŒå’Œä¿¡ä»»åº¦ï¼Œå¹¶ä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´å¯ä¿¡èµ–çš„AIç³»ç»Ÿæä¾›å€Ÿé‰´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Composed Image Retrieval (CIR), which aims to find a target image from a reference image and a modification text, presents the core challenge of performing unified reasoning across visual and semantic modalities. While current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown progress, they predominantly function as ``black boxes." This inherent opacity not only prevents users from understanding the retrieval rationale but also restricts the models' ability to follow complex, fine-grained instructions. To overcome these limitations, we introduce CIR-CoT, the first end-to-end retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT) reasoning. By compelling the model to first generate an interpretable reasoning chain, CIR-CoT enhances its ability to capture crucial cross-modal interactions, leading to more accurate retrieval while making its decision process transparent. Since existing datasets like FashionIQ and CIRR lack the necessary reasoning data, a key contribution of our work is the creation of structured CoT annotations using a three-stage process involving a caption, reasoning, and conclusion. Our model is then fine-tuned to produce this structured output before encoding its final retrieval intent into a dedicated embedding. Comprehensive experiments show that CIR-CoT achieves highly competitive performance on in-domain datasets (FashionIQ, CIRR) and demonstrates remarkable generalization on the out-of-domain CIRCO dataset, establishing a new path toward more effective and trustworthy retrieval systems.

