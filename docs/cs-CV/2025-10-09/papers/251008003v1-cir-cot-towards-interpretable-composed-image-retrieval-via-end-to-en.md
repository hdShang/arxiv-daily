---
layout: default
title: CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning
---

# CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08003" target="_blank" class="toolbar-btn">arXiv: 2510.08003v1</a>
    <a href="https://arxiv.org/pdf/2510.08003.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08003v1" 
            onclick="toggleFavorite(this, '2510.08003v1', 'CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Weihuang Lin, Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CIR-CoTÔºåÈÄöËøáÁ´ØÂà∞Á´ØÊÄùÁª¥ÈìæÊé®ÁêÜÂÆûÁé∞ÂèØËß£ÈáäÁöÑÁªÑÂêàÂõæÂÉèÊ£ÄÁ¥¢**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁªÑÂêàÂõæÂÉèÊ£ÄÁ¥¢` `ÊÄùÁª¥ÈìæÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂèØËß£ÈáäÊÄß` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁªÑÂêàÂõæÂÉèÊ£ÄÁ¥¢ÊñπÊ≥ïÔºåÂ¶ÇÂü∫‰∫éVLMÂíåMLLMÁöÑÊ®°ÂûãÔºåÈÄöÂ∏∏ÊòØÈªëÁõíÔºåÁº∫‰πèÂèØËß£ÈáäÊÄßÔºåÈöæ‰ª•ÁêÜËß£Ê£ÄÁ¥¢ÈÄªËæë„ÄÇ
2. CIR-CoTÈÄöËøáÂºïÂÖ•ÊòæÂºèÁöÑÊÄùÁª¥Èìæ(CoT)Êé®ÁêÜÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÁîüÊàêÂèØËß£ÈáäÁöÑÊé®ÁêÜËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÊ£ÄÁ¥¢ÁöÑÂáÜÁ°ÆÊÄßÂíåÈÄèÊòéÂ∫¶„ÄÇ
3. ËÆ∫ÊñáÂàõÂª∫‰∫ÜÂåÖÂê´ÊèèËø∞„ÄÅÊé®ÁêÜÂíåÁªìËÆ∫ÁöÑÁªìÊûÑÂåñCoTÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºåÂπ∂Âú®FashionIQ„ÄÅCIRRÂíåCIRCOÊï∞ÊçÆÈõÜ‰∏äÈ™åËØÅ‰∫ÜCIR-CoTÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁªÑÂêàÂõæÂÉèÊ£ÄÁ¥¢(CIR)Êó®Âú®Ê†πÊçÆÂèÇËÄÉÂõæÂÉèÂíå‰øÆÊîπÊñáÊú¨ÊâæÂà∞ÁõÆÊ†áÂõæÂÉèÔºåÂÖ∂Ê†∏ÂøÉÊåëÊàòÂú®‰∫éÊâßË°åË∑®ËßÜËßâÂíåËØ≠‰πâÊ®°ÊÄÅÁöÑÁªü‰∏ÄÊé®ÁêÜ„ÄÇÂΩìÂâçÂü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLMs, Â¶ÇCLIP)ÂíåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLMs, Â¶ÇQwen-VL)ÁöÑÊñπÊ≥ïËôΩÁÑ∂ÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜ‰∏ªË¶Å‰Ωú‰∏∫‚ÄúÈªëÁõí‚ÄùËøêË°å„ÄÇËøôÁßçÂõ∫ÊúâÁöÑ‰∏çÈÄèÊòéÊÄß‰∏ç‰ªÖÈòªÊ≠¢Áî®Êà∑ÁêÜËß£Ê£ÄÁ¥¢ÁöÑÂü∫Êú¨ÂéüÁêÜÔºåËøòÈôêÂà∂‰∫ÜÊ®°ÂûãÈÅµÂæ™Â§çÊùÇ„ÄÅÁªÜÁ≤íÂ∫¶Êåá‰ª§ÁöÑËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜCIR-CoTÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Èù¢ÂêëÊ£ÄÁ¥¢ÁöÑÁ´ØÂà∞Á´ØMLLMÔºåÊó®Âú®ÈõÜÊàêÊòæÂºèÁöÑÊÄùÁª¥Èìæ(CoT)Êé®ÁêÜ„ÄÇÈÄöËøáËø´‰ΩøÊ®°ÂûãÈ¶ñÂÖàÁîüÊàêÂèØËß£ÈáäÁöÑÊé®ÁêÜÈìæÔºåCIR-CoTÂ¢ûÂº∫‰∫ÜÂÖ∂ÊçïËé∑ÂÖ≥ÈîÆË∑®Ê®°ÊÄÅ‰∫§‰∫íÁöÑËÉΩÂäõÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂáÜÁ°ÆÁöÑÊ£ÄÁ¥¢ÔºåÂêåÊó∂‰ΩøÂÖ∂ÂÜ≥Á≠ñËøáÁ®ãÈÄèÊòéÂåñ„ÄÇÁî±‰∫éÁé∞ÊúâÁöÑÊï∞ÊçÆÈõÜ(Â¶ÇFashionIQÂíåCIRR)Áº∫‰πèÂøÖË¶ÅÁöÑÊé®ÁêÜÊï∞ÊçÆÔºåÊàë‰ª¨Â∑•‰ΩúÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆË¥°ÁåÆÊòØ‰ΩøÁî®ÂåÖÂê´ÊèèËø∞„ÄÅÊé®ÁêÜÂíåÁªìËÆ∫ÁöÑ‰∏âÈò∂ÊÆµËøáÁ®ãÂàõÂª∫ÁªìÊûÑÂåñÁöÑCoTÊ†áÊ≥®„ÄÇÁÑ∂ÂêéÔºåÂØπÊàë‰ª¨ÁöÑÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ª•ÁîüÊàêËøôÁßçÁªìÊûÑÂåñÁöÑËæìÂá∫ÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂ÊúÄÁªàÊ£ÄÁ¥¢ÊÑèÂõæÁºñÁ†ÅÂà∞‰∏ìÁî®ÂµåÂÖ•‰∏≠„ÄÇÁªºÂêàÂÆûÈ™åË°®ÊòéÔºåCIR-CoTÂú®È¢ÜÂüüÂÜÖÊï∞ÊçÆÈõÜ(FashionIQ, CIRR)‰∏äÂèñÂæó‰∫ÜÊûÅÂÖ∑Á´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåÂπ∂Âú®È¢ÜÂüüÂ§ñCIRCOÊï∞ÊçÆÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰∏∫Êõ¥ÊúâÊïàÂíåÂÄºÂæó‰ø°ËµñÁöÑÊ£ÄÁ¥¢Á≥ªÁªüÂºÄËæü‰∫Ü‰∏ÄÊù°Êñ∞ÈÅìË∑Ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁªÑÂêàÂõæÂÉèÊ£ÄÁ¥¢(CIR)‰ªªÂä°Êó®Âú®Ê†πÊçÆÁªôÂÆöÁöÑÂèÇËÄÉÂõæÂÉèÂíåÊñáÊú¨ÊèèËø∞Ôºå‰ªéÂõæÂÉèÂ∫ì‰∏≠Ê£ÄÁ¥¢Âá∫Á¨¶ÂêàÊèèËø∞ÁöÑÁõÆÊ†áÂõæÂÉè„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éVLMÂíåMLLMÁöÑÊñπÊ≥ïÔºåÈÄöÂ∏∏Áº∫‰πèÂèØËß£ÈáäÊÄßÔºåÁî®Êà∑Èöæ‰ª•ÁêÜËß£Ê®°ÂûãÂÅöÂá∫Ê£ÄÁ¥¢ÂÜ≥Á≠ñÁöÑÂéüÂõ†„ÄÇÊ≠§Â§ñÔºåËøô‰∫õÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇ„ÄÅÁªÜÁ≤íÂ∫¶ÁöÑÊåá‰ª§Êó∂ËÉΩÂäõÊúâÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCIR-CoTÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂºïÂÖ•ÊÄùÁª¥Èìæ(Chain-of-Thought, CoT)Êé®ÁêÜÔºå‰ΩøÊ®°ÂûãÂú®Ê£ÄÁ¥¢‰πãÂâçÂÖàÁîüÊàê‰∏Ä‰∏™ÂèØËß£ÈáäÁöÑÊé®ÁêÜÈìæ„ÄÇËøô‰∏™Êé®ÁêÜÈìæÊòéÁ°ÆÂú∞Ë°®Ëææ‰∫ÜÊ®°ÂûãÂ¶Ç‰ΩïÁêÜËß£ÂèÇËÄÉÂõæÂÉèÂíåÊñáÊú¨ÊèèËø∞Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÂ∞ÜÂÆÉ‰ª¨ÁªìÂêàËµ∑Êù•ËøõË°åÊ£ÄÁ¥¢„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°Âûã‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊ£ÄÁ¥¢ÁöÑÂáÜÁ°ÆÊÄßÔºåËøòÂ¢ûÂº∫‰∫ÜÂÖ∂ÂèØËß£ÈáäÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCIR-CoTÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËæìÂÖ•Ê®°ÂùóÔºöÊé•Êî∂ÂèÇËÄÉÂõæÂÉèÂíåÊñáÊú¨ÊèèËø∞‰Ωú‰∏∫ËæìÂÖ•„ÄÇ2) CoTÁîüÊàêÊ®°ÂùóÔºöÂà©Áî®MLLMÁîüÊàê‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÊé®ÁêÜÈìæÔºåÂåÖÊã¨ÂõæÂÉèÂíåÊñáÊú¨ÁöÑÊèèËø∞„ÄÅÊé®ÁêÜËøáÁ®ãÂíåÊúÄÁªàÁªìËÆ∫„ÄÇ3) ÂµåÂÖ•ÁºñÁ†ÅÊ®°ÂùóÔºöÂ∞ÜÁîüÊàêÁöÑÊé®ÁêÜÈìæÁºñÁ†ÅÊàê‰∏Ä‰∏™ÂµåÂÖ•ÂêëÈáèÔºåÁî®‰∫éÊ£ÄÁ¥¢„ÄÇ4) Ê£ÄÁ¥¢Ê®°ÂùóÔºöÊ†πÊçÆÂµåÂÖ•ÂêëÈáè‰ªéÂõæÂÉèÂ∫ì‰∏≠Ê£ÄÁ¥¢Âá∫ÊúÄÂåπÈÖçÁöÑÁõÆÊ†áÂõæÂÉè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCIR-CoTÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÊÄùÁª¥ÈìæÊé®ÁêÜÂºïÂÖ•Âà∞ÁªÑÂêàÂõæÂÉèÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÊ®°ÂûãÊù•ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCIR-CoT‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊ£ÄÁ¥¢ÁöÑÂáÜÁ°ÆÊÄßÔºåËøòÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÔºå‰ΩøÂæóÁî®Êà∑ÂèØ‰ª•ÁêÜËß£Ê®°ÂûãÂÅöÂá∫Ê£ÄÁ¥¢ÂÜ≥Á≠ñÁöÑÂéüÂõ†„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑCoTÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºå‰∏∫ËÆ≠ÁªÉCIR-CoTÊ®°ÂûãÊèê‰æõ‰∫ÜÂøÖË¶ÅÁöÑÊï∞ÊçÆÊîØÊåÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCIR-CoTÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®MLLMÔºà‰æãÂ¶ÇQwen-VLÔºâ‰Ωú‰∏∫CoTÁîüÊàêÊ®°ÂùóÁöÑÂü∫Á°ÄÊ®°Âûã„ÄÇ2) ËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∏âÈò∂ÊÆµÁöÑCoTÊ†áÊ≥®ËøáÁ®ãÔºåÂåÖÊã¨ÊèèËø∞„ÄÅÊé®ÁêÜÂíåÁªìËÆ∫„ÄÇ3) ‰ΩøÁî®ÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÂáΩÊï∞Êù•ËÆ≠ÁªÉÂµåÂÖ•ÁºñÁ†ÅÊ®°ÂùóÔºå‰ΩøÂæóÁõ∏‰ººÁöÑÂõæÂÉèÂíåÊñáÊú¨ÊèèËø∞ÂÖ∑ÊúâÁõ∏‰ººÁöÑÂµåÂÖ•ÂêëÈáè„ÄÇ4) ÂØπMLLMËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ËÉΩÂ§üÁîüÊàêÁªìÊûÑÂåñÁöÑCoTËæìÂá∫„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CIR-CoTÂú®FashionIQÂíåCIRRÁ≠âÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊûÅÂÖ∑Á´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåÂπ∂Âú®È¢ÜÂüüÂ§ñCIRCOÊï∞ÊçÆÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Âú®ÊëòË¶Å‰∏≠ÁªôÂá∫Ôºå‰ΩÜÂº∫Ë∞É‰∫ÜÂÖ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑ‰ºòË∂äË°®Áé∞ÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CIR-CoTÂú®ÁîµÂïÜ„ÄÅÊó∂Â∞ö„ÄÅÂÆ§ÂÜÖËÆæËÆ°Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøá‰∏ä‰º†‰∏ÄÂº†Ë°£ÊúçÁöÑÂõæÁâáÔºåÂπ∂ËæìÂÖ•‚ÄúÊç¢‰∏™È¢úËâ≤‚ÄùÁ≠âÊèèËø∞ÔºåÂø´ÈÄüÊâæÂà∞Êª°Ë∂≥Ë¶ÅÊ±ÇÁöÑÂïÜÂìÅ„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊèêÂçáÊ£ÄÁ¥¢Á≥ªÁªüÁöÑÁî®Êà∑‰ΩìÈ™åÂíå‰ø°‰ªªÂ∫¶ÔºåÂπ∂‰∏∫ÂºÄÂèëÊõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÂèØ‰ø°ËµñÁöÑAIÁ≥ªÁªüÊèê‰æõÂÄüÈâ¥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Composed Image Retrieval (CIR), which aims to find a target image from a reference image and a modification text, presents the core challenge of performing unified reasoning across visual and semantic modalities. While current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown progress, they predominantly function as ``black boxes." This inherent opacity not only prevents users from understanding the retrieval rationale but also restricts the models' ability to follow complex, fine-grained instructions. To overcome these limitations, we introduce CIR-CoT, the first end-to-end retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT) reasoning. By compelling the model to first generate an interpretable reasoning chain, CIR-CoT enhances its ability to capture crucial cross-modal interactions, leading to more accurate retrieval while making its decision process transparent. Since existing datasets like FashionIQ and CIRR lack the necessary reasoning data, a key contribution of our work is the creation of structured CoT annotations using a three-stage process involving a caption, reasoning, and conclusion. Our model is then fine-tuned to produce this structured output before encoding its final retrieval intent into a dedicated embedding. Comprehensive experiments show that CIR-CoT achieves highly competitive performance on in-domain datasets (FashionIQ, CIRR) and demonstrates remarkable generalization on the out-of-domain CIRCO dataset, establishing a new path toward more effective and trustworthy retrieval systems.

