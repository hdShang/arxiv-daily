---
layout: default
title: Reinforcing Diffusion Models by Direct Group Preference Optimization
---

# Reinforcing Diffusion Models by Direct Group Preference Optimization

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08425" target="_blank" class="toolbar-btn">arXiv: 2510.08425v1</a>
    <a href="https://arxiv.org/pdf/2510.08425.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08425v1" 
            onclick="toggleFavorite(this, '2510.08425v1', 'Reinforcing Diffusion Models by Direct Group Preference Optimization')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yihong Luo, Tianyang Hu, Jing Tang

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Luo-Yihong/DGPO)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Áõ¥Êé•Áæ§‰ΩìÂÅèÂ•Ω‰ºòÂåñ(DGPO)ÔºåÂä†ÈÄüÂπ∂ÊèêÂçáÊâ©Êï£Ê®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Ê®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Áæ§‰ΩìÂÅèÂ•Ω‰ºòÂåñ` `Á°ÆÂÆöÊÄßÈááÊ†∑` `Âú®Á∫øÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Êâ©Êï£Ê®°Âûã‰∏äÂ∫îÁî®Âº∫ÂåñÂ≠¶‰π†Êó∂Ôºå‰æùËµñ‰ΩéÊïàÁöÑSDEÈááÊ†∑Âô®ÂºïÂÖ•ÈöèÊú∫ÊÄßÔºåÂØºËá¥ËÆ≠ÁªÉÁºìÊÖ¢„ÄÇ
2. DGPOÁõ¥Êé•‰ªéÁæ§‰ΩìÂÅèÂ•ΩÂ≠¶‰π†ÔºåÊó†ÈúÄÁ≠ñÁï•Ê¢ØÂ∫¶Ôºå‰ªéËÄåÂèØ‰ª•‰ΩøÁî®È´òÊïàÁöÑÁ°ÆÂÆöÊÄßODEÈááÊ†∑Âô®„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDGPOÊØîÁé∞ÊúâÊñπÊ≥ïÂø´20ÂÄçÔºåÂπ∂Âú®È¢ÜÂüüÂÜÖÂíåÈ¢ÜÂüüÂ§ñÂ•ñÂä±ÊåáÊ†á‰∏äË°®Áé∞Êõ¥‰ºò„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∞ΩÁÆ°Áæ§‰ΩìÁõ∏ÂØπÂÅèÂ•Ω‰ºòÂåñ(GRPO)Á≠âÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÊòæËëóÊèêÂçá‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩÜÂ∞ÜÂÖ∂Â∫îÁî®‰∫éÊâ©Êï£Ê®°Âûã‰ªçÁÑ∂ÂÖÖÊª°ÊåëÊàò„ÄÇÁâπÂà´ÊòØÔºåGRPOÈúÄË¶Å‰∏Ä‰∏™ÈöèÊú∫Á≠ñÁï•ÔºåËÄåÊúÄÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÊâ©Êï£ÈááÊ†∑Âô®ÊòØÂü∫‰∫éÁ°ÆÂÆöÊÄßODEÁöÑ„ÄÇÊúÄËøëÁöÑÂ∑•‰ΩúÈÄöËøá‰ΩøÁî®‰ΩéÊïàÁöÑÂü∫‰∫éSDEÁöÑÈááÊ†∑Âô®Êù•ÂºïÂÖ•ÈöèÊú∫ÊÄßÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºå‰ΩÜËøôÁßçÂØπÊ®°ÂûãÊó†ÂÖ≥ÁöÑÈ´òÊñØÂô™Â£∞ÁöÑ‰æùËµñÂØºËá¥Êî∂ÊïõÈÄüÂ∫¶ÁºìÊÖ¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÂÜ≤Á™ÅÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï‚Äî‚ÄîÁõ¥Êé•Áæ§‰ΩìÂÅèÂ•Ω‰ºòÂåñ(DGPO)ÔºåÂÆÉÂÆåÂÖ®ÊëíÂºÉ‰∫ÜÁ≠ñÁï•Ê¢ØÂ∫¶Ê°ÜÊû∂„ÄÇDGPOÁõ¥Êé•‰ªéÁæ§‰ΩìÂ±ÇÈù¢ÁöÑÂÅèÂ•Ω‰∏≠Â≠¶‰π†ÔºåÂà©Áî®Áæ§‰ΩìÂÜÖÊ†∑Êú¨ÁöÑÁõ∏ÂØπ‰ø°ÊÅØ„ÄÇËøôÁßçËÆæËÆ°Ê∂àÈô§‰∫ÜÂØπ‰ΩéÊïàÈöèÊú∫Á≠ñÁï•ÁöÑÈúÄÊ±ÇÔºå‰ªéËÄåÂèØ‰ª•‰ΩøÁî®È´òÊïàÁöÑÁ°ÆÂÆöÊÄßODEÈááÊ†∑Âô®Âπ∂Âä†Âø´ËÆ≠ÁªÉÈÄüÂ∫¶„ÄÇÂ§ßÈáèÁªìÊûúË°®ÊòéÔºåDGPOÁöÑËÆ≠ÁªÉÈÄüÂ∫¶ÊØîÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÂø´Á∫¶20ÂÄçÔºåÂπ∂‰∏îÂú®È¢ÜÂüüÂÜÖÂíåÈ¢ÜÂüüÂ§ñÁöÑÂ•ñÂä±ÊåáÊ†á‰∏äÈÉΩÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊÄßËÉΩ„ÄÇ‰ª£Á†ÅÂèØÂú®https://github.com/Luo-Yihong/DGPOËé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÂú®Â∞ÜÂº∫ÂåñÂ≠¶‰π†Â∫îÁî®‰∫éÊâ©Êï£Ê®°ÂûãÊó∂ÔºåÈù¢‰∏¥‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàòÔºöGRPOÁ≠âÁÆóÊ≥ïÈúÄË¶ÅÈöèÊú∫Á≠ñÁï•ÔºåËÄåÊâ©Êï£Ê®°Âûã‰∏≠ÊúÄÊúâÊïàÁöÑÈááÊ†∑Âô®ÊòØÂü∫‰∫éÁ°ÆÂÆöÊÄßÂ∏∏ÂæÆÂàÜÊñπÁ®ã(ODE)ÁöÑ„ÄÇ‰∏∫‰∫ÜÊª°Ë∂≥GRPOÁöÑÈúÄÊ±ÇÔºå‰∏Ä‰∫õÂ∑•‰ΩúÂ∞ùËØï‰ΩøÁî®Âü∫‰∫éÈöèÊú∫ÂæÆÂàÜÊñπÁ®ã(SDE)ÁöÑÈááÊ†∑Âô®Êù•ÂºïÂÖ•ÈöèÊú∫ÊÄßÔºå‰ΩÜËøôÁßçÊñπÊ≥ïÊïàÁéá‰Ωé‰∏ãÔºåÊî∂ÊïõÈÄüÂ∫¶ÊÖ¢„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÈ´òÊïàÂú∞Â∞ÜÂº∫ÂåñÂ≠¶‰π†Â∫îÁî®‰∫éÊâ©Êï£Ê®°ÂûãÔºåÂêåÊó∂ÈÅøÂÖç‰ΩøÁî®‰ΩéÊïàÁöÑSDEÈááÊ†∑Âô®ÔºåÊòØ‰∏Ä‰∏™‰∫üÂæÖËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDGPOÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁõ¥Êé•‰ªéÁæ§‰ΩìÂ±ÇÈù¢ÁöÑÂÅèÂ•Ω‰∏≠Â≠¶‰π†ÔºåËÄåÊó†ÈúÄÊòæÂºèÂú∞ÊûÑÂª∫Âíå‰ºòÂåñÁ≠ñÁï•„ÄÇÂÆÉÂà©Áî®Áæ§‰ΩìÂÜÖÊ†∑Êú¨ÁöÑÁõ∏ÂØπ‰ø°ÊÅØÔºåÈÄöËøáÊØîËæÉ‰∏çÂêåÊ†∑Êú¨ÁöÑ‰ºòÂä£Êù•ÊåáÂØºÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÂØπÈöèÊú∫Á≠ñÁï•ÁöÑ‰æùËµñÔºå‰ªéËÄåÂèØ‰ª•‰ΩøÁî®È´òÊïàÁöÑÁ°ÆÂÆöÊÄßODEÈááÊ†∑Âô®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDGPOÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) ‰ªéÊâ©Êï£Ê®°Âûã‰∏≠ÈááÊ†∑‰∏ÄÁªÑÊ†∑Êú¨Ôºõ2) Ê†πÊçÆÂ•ñÂä±ÂáΩÊï∞Êàñ‰∫∫Á±ªÂèçÈ¶àÔºåÂØπËøô‰∫õÊ†∑Êú¨ËøõË°åÊéíÂ∫èÔºåÂΩ¢ÊàêÁæ§‰ΩìÂÅèÂ•ΩÔºõ3) ‰ΩøÁî®Áæ§‰ΩìÂÅèÂ•Ω‰ø°ÊÅØÔºåÁõ¥Êé•Êõ¥Êñ∞Êâ©Êï£Ê®°ÂûãÁöÑÂèÇÊï∞ÔºåËÄåÊó†ÈúÄËÆ°ÁÆóÁ≠ñÁï•Ê¢ØÂ∫¶„ÄÇDGPOÁÆóÊ≥ïÊòØ‰∏Ä‰∏™Âú®Á∫øÂ≠¶‰π†ÁÆóÊ≥ïÔºåËøôÊÑèÂë≥ÁùÄÂÆÉÂèØ‰ª•Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰∏çÊñ≠Âú∞Êî∂ÈõÜÊï∞ÊçÆÂπ∂Êõ¥Êñ∞Ê®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDGPOÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂÆÉÊëíÂºÉ‰∫Ü‰º†ÁªüÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶Ê°ÜÊû∂ÔºåÁõ¥Êé•‰ªéÁæ§‰ΩìÂÅèÂ•Ω‰∏≠Â≠¶‰π†„ÄÇËøô‰ΩøÂæóÂÆÉËÉΩÂ§üÈÅøÂÖçÂØπÈöèÊú∫Á≠ñÁï•ÁöÑ‰æùËµñÔºå‰ªéËÄåÂèØ‰ª•‰ΩøÁî®È´òÊïàÁöÑÁ°ÆÂÆöÊÄßODEÈááÊ†∑Âô®„ÄÇÊ≠§Â§ñÔºåDGPOËøòÂà©Áî®‰∫ÜÁæ§‰ΩìÂÜÖÊ†∑Êú¨ÁöÑÁõ∏ÂØπ‰ø°ÊÅØÔºåËøôÊúâÂä©‰∫éÊèêÈ´òÂ≠¶‰π†ÊïàÁéáÂíåÁ®≥ÂÆöÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDGPOÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Áæ§‰ΩìÂÅèÂ•ΩÊçüÂ§±ÂáΩÊï∞ÔºåËØ•ÂáΩÊï∞ÈºìÂä±Ê®°ÂûãÁîüÊàêÊõ¥Á¨¶ÂêàÁæ§‰ΩìÂÅèÂ•ΩÁöÑÊ†∑Êú¨Ôºõ2) ‰ΩøÁî®È´òÊïàÁöÑÁ°ÆÂÆöÊÄßODEÈááÊ†∑Âô®Ôºå‰æãÂ¶ÇDDIMÊàñPLMSÔºåÊù•ÁîüÊàêÊ†∑Êú¨Ôºõ3) ‰ΩøÁî®Âú®Á∫øÂ≠¶‰π†ÁöÑÊñπÂºèÔºå‰∏çÊñ≠Âú∞Êî∂ÈõÜÊï∞ÊçÆÂπ∂Êõ¥Êñ∞Ê®°Âûã„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂΩ¢ÂºèÂíå‰ºòÂåñÁÆóÊ≥ïÂèØ‰ª•Ê†πÊçÆÂÖ∑‰ΩìÁöÑÂ∫îÁî®Âú∫ÊôØËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DGPOÂú®ÂÆûÈ™å‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËÆ≠ÁªÉÈÄüÂ∫¶ÊØîÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÂø´Á∫¶20ÂÄçÔºåÂπ∂‰∏îÂú®È¢ÜÂüüÂÜÖÂíåÈ¢ÜÂüüÂ§ñÁöÑÂ•ñÂä±ÊåáÊ†á‰∏äÈÉΩÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊÄßËÉΩ„ÄÇËøôË°®ÊòéDGPOËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Áæ§‰ΩìÂÅèÂ•Ω‰ø°ÊÅØÔºåÂπ∂ÈÅøÂÖçÂØπ‰ΩéÊïàÈöèÊú∫Á≠ñÁï•ÁöÑ‰æùËµñ„ÄÇÂÆûÈ™åÁªìÊûúÂÖÖÂàÜËØÅÊòé‰∫ÜDGPOÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DGPOÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂõæÂÉèÁîüÊàê„ÄÅÊñáÊú¨ÁîüÊàê„ÄÅÈü≥È¢ëÁîüÊàêÁ≠â„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÈ´òË¥®ÈáèÁöÑÊâ©Êï£Ê®°ÂûãÔºå‰ªéËÄåÁîüÊàêÊõ¥ÈÄºÁúü„ÄÅÊõ¥Á¨¶ÂêàÁî®Êà∑ÈúÄÊ±ÇÁöÑÊ†∑Êú¨„ÄÇÊ≠§Â§ñÔºåDGPOËøòÂèØ‰ª•Â∫îÁî®‰∫é‰∏™ÊÄßÂåñÊé®Ëçê„ÄÅÈ£éÊ†ºËøÅÁßªÁ≠âÈ¢ÜÂüüÔºå‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Âä†‰∏™ÊÄßÂåñÁöÑÊúçÂä°„ÄÇÊú™Êù•ÔºåDGPOÊúâÊúõÊàê‰∏∫Êâ©Êï£Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO.

