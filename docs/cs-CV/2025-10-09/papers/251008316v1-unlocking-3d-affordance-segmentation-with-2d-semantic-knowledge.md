---
layout: default
title: Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge
---

# Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08316" target="_blank" class="toolbar-btn">arXiv: 2510.08316v1</a>
    <a href="https://arxiv.org/pdf/2510.08316.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08316v1" 
            onclick="toggleFavorite(this, '2510.08316v1', 'Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

**Â§áÊ≥®**: Work in process

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CMATÂíåCASTÔºåÂà©Áî®2DËØ≠‰πâÁü•ËØÜÊèêÂçá3DÂèØ‰æõÊÄßÂàÜÂâ≤ÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `3DÂèØ‰æõÊÄßÂàÜÂâ≤` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `ËØ≠‰πâÁü•ËØÜËøÅÁßª` `ËßÜËßâÂü∫Á°ÄÊ®°Âûã` `Transformer`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ3DÂèØ‰æõÊÄßÂàÜÂâ≤ÊñπÊ≥ï‰æùËµñÈÄöÁî®ÁÇπ‰∫ëÁºñÁ†ÅÂô®ÔºåÈöæ‰ª•Â∫îÂØπ3DÊï∞ÊçÆÁöÑÁ®ÄÁñèÊÄßÂíåÂá†‰ΩïÊ≠ß‰πâ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫CMATÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂ∞Ü2DËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑËØ≠‰πâÁü•ËØÜËøÅÁßªÂà∞3DÈ¢ÜÂüüÔºåÊèêÂçáÁâπÂæÅÁöÑËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇ
3. ÊûÑÂª∫CASTÊ®°ÂûãÔºåËûçÂêàÂ§öÊ®°ÊÄÅÊèêÁ§∫ÂíåÈ¢ÑËÆ≠ÁªÉÁâπÂæÅÔºåÂú®Ê†áÂáÜÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂèØ‰æõÊÄßÂàÜÂâ≤Êó®Âú®Â∞Ü3DÁâ©‰ΩìËß£Êûê‰∏∫ÂäüËÉΩ‰∏ä‰∏çÂêåÁöÑÈÉ®ÂàÜÔºå‰ªéËÄåËøûÊé•ËØÜÂà´Âíå‰∫§‰∫íÔºåÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅÂÖ∑Ë∫´Êô∫ËÉΩÂíåÂ¢ûÂº∫Áé∞ÂÆû„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñÁÇπ‰∫ëÁºñÁ†ÅÂô®‰Ωú‰∏∫ÈÄöÁî®ÁâπÂæÅÊèêÂèñÂô®ÔºåÂøΩÁï•‰∫Ü3DÊï∞ÊçÆÁöÑÁ®ÄÁñèÊÄß„ÄÅÂô™Â£∞ÂíåÂá†‰ΩïÊ≠ß‰πâÁ≠âÂÜÖÂú®ÊåëÊàò„ÄÇÂõ†Ê≠§ÔºåÂ≠§Á´ãÂ≠¶‰π†ÁöÑ3DÁâπÂæÅÈÄöÂ∏∏Áº∫‰πèÊ∏ÖÊô∞‰∏îËØ≠‰πâ‰∏ÄËá¥ÁöÑÂäüËÉΩËæπÁïå„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™Áì∂È¢àÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËØ≠‰πâÂºïÂØºÁöÑÂ≠¶‰π†ËåÉÂºèÔºåÂ∞ÜÊù•Ëá™Â§ßËßÑÊ®°2DËßÜËßâÂü∫Á°ÄÊ®°Âûã(VFMs)ÁöÑ‰∏∞ÂØåËØ≠‰πâÁü•ËØÜËΩ¨ÁßªÂà∞3DÈ¢ÜÂüü„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜË∑®Ê®°ÊÄÅ‰∫≤ÂíåÂäõËΩ¨Áßª(CMAT)ÔºåËøôÊòØ‰∏ÄÁßçÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰Ωø3DÁºñÁ†ÅÂô®‰∏éÊèêÂçáÁöÑ2DËØ≠‰πâÂØπÈΩêÔºåÂπ∂ËÅîÂêà‰ºòÂåñÈáçÂª∫„ÄÅ‰∫≤ÂíåÂäõÂíåÂ§öÊ†∑ÊÄßÔºå‰ª•‰∫ßÁîüËØ≠‰πâÁªÑÁªáÁöÑË°®Á§∫„ÄÇÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ËÆæËÆ°‰∫ÜË∑®Ê®°ÊÄÅÂèØ‰æõÊÄßÂàÜÂâ≤Transformer (CAST)ÔºåÂÆÉÂ∞ÜÂ§öÊ®°ÊÄÅÊèêÁ§∫‰∏éCMATÈ¢ÑËÆ≠ÁªÉÁöÑÁâπÂæÅÈõÜÊàêÔºå‰ª•ÁîüÊàêÁ≤æÁ°ÆÁöÑ„ÄÅÊèêÁ§∫ÊÑüÁü•ÁöÑÂàÜÂâ≤Âõæ„ÄÇÂú®Ê†áÂáÜÂü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂‰∏∫3DÂèØ‰æõÊÄßÂàÜÂâ≤Âª∫Á´ã‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑ3DÂèØ‰æõÊÄßÂàÜÂâ≤ÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÁõ¥Êé•Âú®3DÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÁÇπ‰∫ëÁºñÁ†ÅÂô®„ÄÇËøô‰∫õÊñπÊ≥ïÂøΩÁï•‰∫Ü3DÊï∞ÊçÆÁöÑÂõ∫ÊúâÊåëÊàòÔºå‰æãÂ¶ÇÊï∞ÊçÆÁ®ÄÁñè„ÄÅÂô™Â£∞‰ª•ÂèäÂá†‰ΩïÂΩ¢Áä∂ÁöÑÊ®°Á≥äÊÄß„ÄÇÂõ†Ê≠§ÔºåÂ≠¶‰π†Âà∞ÁöÑ3DÁâπÂæÅÂæÄÂæÄÁº∫‰πèÊ∏ÖÊô∞ÁöÑ„ÄÅËØ≠‰πâ‰∏ÄËá¥ÁöÑÂäüËÉΩËæπÁïåÔºåÈôêÂà∂‰∫ÜÂàÜÂâ≤ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞Ü2DËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºàVFMsÔºâ‰∏≠Ëï¥Âê´ÁöÑ‰∏∞ÂØåËØ≠‰πâÁü•ËØÜËøÅÁßªÂà∞3DÈ¢ÜÂüü„ÄÇ2DÂõæÂÉèÊï∞ÊçÆÈáèÂ§ßÔºåËØ≠‰πâ‰ø°ÊÅØ‰∏∞ÂØåÔºåÈÄöËøáÂêàÈÄÇÁöÑËøÅÁßªÁ≠ñÁï•ÔºåÂèØ‰ª•ÊúâÊïàÊèêÂçá3DÁâπÂæÅÁöÑËØ≠‰πâË°®ËææËÉΩÂäõÔºå‰ªéËÄåÊîπÂñÑ3DÂèØ‰æõÊÄßÂàÜÂâ≤ÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöCMATÈ¢ÑËÆ≠ÁªÉÂíåCASTÂàÜÂâ≤„ÄÇÈ¶ñÂÖàÔºåCMATÔºàCross-Modal Affinity TransferÔºâÈ¢ÑËÆ≠ÁªÉÊ®°ÂùóÁî®‰∫éÂ∞Ü2DËØ≠‰πâÁü•ËØÜËøÅÁßªÂà∞3DÁºñÁ†ÅÂô®„ÄÇÁÑ∂ÂêéÔºåCASTÔºàCross-modal Affordance Segmentation TransformerÔºâÂàÜÂâ≤Ê®°ÂùóÂà©Áî®CMATÈ¢ÑËÆ≠ÁªÉÁöÑÁâπÂæÅÔºåÁªìÂêàÂ§öÊ®°ÊÄÅÊèêÁ§∫ÔºåÁîüÊàêÊúÄÁªàÁöÑÂàÜÂâ≤ÁªìÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜCMATÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•„ÄÇCMATÈÄöËøáËÅîÂêà‰ºòÂåñÈáçÂª∫ÊçüÂ§±„ÄÅ‰∫≤ÂíåÂäõÊçüÂ§±ÂíåÂ§öÊ†∑ÊÄßÊçüÂ§±Ôºå‰ΩøÂæó3DÁºñÁ†ÅÂô®ËÉΩÂ§üÂ≠¶‰π†Âà∞‰∏é2DËØ≠‰πâÂØπÈΩêÁöÑ„ÄÅÂÖ∑ÊúâËâØÂ•ΩËØ≠‰πâÁªÑÁªáÊÄßÁöÑË°®Á§∫„ÄÇËøôÁßçË∑®Ê®°ÊÄÅÁöÑÁü•ËØÜËøÅÁßªÊñπÂºèÔºåÊúâÊïàÂÖãÊúç‰∫Ü3DÊï∞ÊçÆËá™Ë∫´ÁöÑÂ±ÄÈôêÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCMATÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºå‰ΩøÁî®‰∫Ü‰∏âÁßçÊçüÂ§±ÂáΩÊï∞Ôºö1) ÈáçÂª∫ÊçüÂ§±Ôºå‰øùËØÅ3DÁâπÂæÅËÉΩÂ§üÈáçÂª∫ÂéüÂßã3DÊï∞ÊçÆÔºõ2) ‰∫≤ÂíåÂäõÊçüÂ§±Ôºå‰øÉ‰Ωø3DÁâπÂæÅÂú®ËØ≠‰πâÁõ∏‰ººÁöÑÁÇπ‰πãÈó¥ÂÖ∑ÊúâÊõ¥È´òÁöÑ‰∫≤ÂíåÂäõÔºõ3) Â§öÊ†∑ÊÄßÊçüÂ§±ÔºåÈºìÂä±3DÁâπÂæÅÂú®‰∏çÂêåÁ±ªÂà´‰πãÈó¥ÂÖ∑ÊúâÊõ¥È´òÁöÑÂå∫ÂàÜÂ∫¶„ÄÇCASTÂàÜÂâ≤Èò∂ÊÆµÔºå‰ΩøÁî®‰∫ÜTransformerÁªìÊûÑÔºåÂ∞ÜÂ§öÊ®°ÊÄÅÊèêÁ§∫Ôºà‰æãÂ¶ÇÊñáÊú¨ÊàñËßÜËßâÊèêÁ§∫Ôºâ‰∏éCMATÈ¢ÑËÆ≠ÁªÉÁöÑ3DÁâπÂæÅËøõË°åËûçÂêàÔºå‰ªéËÄåÁîüÊàêprompt-awareÁöÑÂàÜÂâ≤ÁªìÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÂú®Ê†áÂáÜ3DÂèØ‰æõÊÄßÂàÜÂâ≤Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂ§ßÈáèÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑCMATÂíåCASTÊ°ÜÊû∂ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊñπÊ≥ïÔºåÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÈÉΩÂèñÂæó‰∫ÜÊòéÊòæÁöÑÊèêÂçáÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅÂÖ∑Ë∫´Êô∫ËÉΩÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âà©Áî®ÂèØ‰æõÊÄßÂàÜÂâ≤ÁöÑÁªìÊûúÔºåÊõ¥Â•ΩÂú∞ÁêÜËß£Áâ©‰ΩìÁöÑÂäüËÉΩÔºå‰ªéËÄåÊâßË°åÊõ¥Â§çÊùÇÁöÑÊäìÂèñ„ÄÅÊîæÁΩÆÁ≠âÊìç‰Ωú„ÄÇÂú®ARÂ∫îÁî®‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑ‰∫§‰∫íÊÑèÂõæÔºåÂä®ÊÄÅÂú∞ÂàÜÂâ≤3DÂú∫ÊôØ‰∏≠ÁöÑÁâ©‰ΩìÔºåÊèê‰æõÊõ¥Êô∫ËÉΩÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Affordance segmentation aims to parse 3D objects into functionally distinct parts, bridging recognition and interaction for applications in robotic manipulation, embodied AI, and AR. While recent studies leverage visual or textual prompts to guide this process, they often rely on point cloud encoders as generic feature extractors, overlooking the intrinsic challenges of 3D data such as sparsity, noise, and geometric ambiguity. As a result, 3D features learned in isolation frequently lack clear and semantically consistent functional boundaries. To address this bottleneck, we propose a semantic-grounded learning paradigm that transfers rich semantic knowledge from large-scale 2D Vision Foundation Models (VFMs) into the 3D domain. Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training strategy that aligns a 3D encoder with lifted 2D semantics and jointly optimizes reconstruction, affinity, and diversity to yield semantically organized representations. Building on this backbone, we further design the Cross-modal Affordance Segmentation Transformer (CAST), which integrates multi-modal prompts with CMAT-pretrained features to generate precise, prompt-aware segmentation maps. Extensive experiments on standard benchmarks demonstrate that our framework establishes new state-of-the-art results for 3D affordance segmentation.

