---
layout: default
title: Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge
---

# Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08316" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08316v1</a>
  <a href="https://arxiv.org/pdf/2510.08316.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08316v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08316v1', 'Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Work in process

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCMATå’ŒCASTï¼Œåˆ©ç”¨2Dè¯­ä¹‰çŸ¥è¯†æå‡3Då¯ä¾›æ€§åˆ†å‰²æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Då¯ä¾›æ€§åˆ†å‰²` `è·¨æ¨¡æ€å­¦ä¹ ` `è¯­ä¹‰çŸ¥è¯†è¿ç§»` `è§†è§‰åŸºç¡€æ¨¡å‹` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Då¯ä¾›æ€§åˆ†å‰²æ–¹æ³•ä¾èµ–é€šç”¨ç‚¹äº‘ç¼–ç å™¨ï¼Œéš¾ä»¥åº”å¯¹3Dæ•°æ®çš„ç¨€ç–æ€§å’Œå‡ ä½•æ­§ä¹‰ã€‚
2. è®ºæ–‡æå‡ºCMATé¢„è®­ç»ƒç­–ç•¥ï¼Œå°†2Dè§†è§‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰çŸ¥è¯†è¿ç§»åˆ°3Dé¢†åŸŸï¼Œæå‡ç‰¹å¾çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚
3. æ„å»ºCASTæ¨¡å‹ï¼Œèåˆå¤šæ¨¡æ€æç¤ºå’Œé¢„è®­ç»ƒç‰¹å¾ï¼Œåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯ä¾›æ€§åˆ†å‰²æ—¨åœ¨å°†3Dç‰©ä½“è§£æä¸ºåŠŸèƒ½ä¸Šä¸åŒçš„éƒ¨åˆ†ï¼Œä»è€Œè¿æ¥è¯†åˆ«å’Œäº¤äº’ï¼Œåº”ç”¨äºæœºå™¨äººæ“ä½œã€å…·èº«æ™ºèƒ½å’Œå¢å¼ºç°å®ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ç‚¹äº‘ç¼–ç å™¨ä½œä¸ºé€šç”¨ç‰¹å¾æå–å™¨ï¼Œå¿½ç•¥äº†3Dæ•°æ®çš„ç¨€ç–æ€§ã€å™ªå£°å’Œå‡ ä½•æ­§ä¹‰ç­‰å†…åœ¨æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå­¤ç«‹å­¦ä¹ çš„3Dç‰¹å¾é€šå¸¸ç¼ºä¹æ¸…æ™°ä¸”è¯­ä¹‰ä¸€è‡´çš„åŠŸèƒ½è¾¹ç•Œã€‚ä¸ºäº†è§£å†³è¿™ä¸ªç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰å¼•å¯¼çš„å­¦ä¹ èŒƒå¼ï¼Œå°†æ¥è‡ªå¤§è§„æ¨¡2Dè§†è§‰åŸºç¡€æ¨¡å‹(VFMs)çš„ä¸°å¯Œè¯­ä¹‰çŸ¥è¯†è½¬ç§»åˆ°3Dé¢†åŸŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ¨¡æ€äº²å’ŒåŠ›è½¬ç§»(CMAT)ï¼Œè¿™æ˜¯ä¸€ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œä½¿3Dç¼–ç å™¨ä¸æå‡çš„2Dè¯­ä¹‰å¯¹é½ï¼Œå¹¶è”åˆä¼˜åŒ–é‡å»ºã€äº²å’ŒåŠ›å’Œå¤šæ ·æ€§ï¼Œä»¥äº§ç”Ÿè¯­ä¹‰ç»„ç»‡çš„è¡¨ç¤ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†è·¨æ¨¡æ€å¯ä¾›æ€§åˆ†å‰²Transformer (CAST)ï¼Œå®ƒå°†å¤šæ¨¡æ€æç¤ºä¸CMATé¢„è®­ç»ƒçš„ç‰¹å¾é›†æˆï¼Œä»¥ç”Ÿæˆç²¾ç¡®çš„ã€æç¤ºæ„ŸçŸ¥çš„åˆ†å‰²å›¾ã€‚åœ¨æ ‡å‡†åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸º3Då¯ä¾›æ€§åˆ†å‰²å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„3Då¯ä¾›æ€§åˆ†å‰²æ–¹æ³•ä¸»è¦ä¾èµ–äºç›´æ¥åœ¨3Dæ•°æ®ä¸Šè®­ç»ƒçš„ç‚¹äº‘ç¼–ç å™¨ã€‚è¿™äº›æ–¹æ³•å¿½ç•¥äº†3Dæ•°æ®çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ•°æ®ç¨€ç–ã€å™ªå£°ä»¥åŠå‡ ä½•å½¢çŠ¶çš„æ¨¡ç³Šæ€§ã€‚å› æ­¤ï¼Œå­¦ä¹ åˆ°çš„3Dç‰¹å¾å¾€å¾€ç¼ºä¹æ¸…æ™°çš„ã€è¯­ä¹‰ä¸€è‡´çš„åŠŸèƒ½è¾¹ç•Œï¼Œé™åˆ¶äº†åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†2Dè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ä¸­è•´å«çš„ä¸°å¯Œè¯­ä¹‰çŸ¥è¯†è¿ç§»åˆ°3Dé¢†åŸŸã€‚2Då›¾åƒæ•°æ®é‡å¤§ï¼Œè¯­ä¹‰ä¿¡æ¯ä¸°å¯Œï¼Œé€šè¿‡åˆé€‚çš„è¿ç§»ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆæå‡3Dç‰¹å¾çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ï¼Œä»è€Œæ”¹å–„3Då¯ä¾›æ€§åˆ†å‰²çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šCMATé¢„è®­ç»ƒå’ŒCASTåˆ†å‰²ã€‚é¦–å…ˆï¼ŒCMATï¼ˆCross-Modal Affinity Transferï¼‰é¢„è®­ç»ƒæ¨¡å—ç”¨äºå°†2Dè¯­ä¹‰çŸ¥è¯†è¿ç§»åˆ°3Dç¼–ç å™¨ã€‚ç„¶åï¼ŒCASTï¼ˆCross-modal Affordance Segmentation Transformerï¼‰åˆ†å‰²æ¨¡å—åˆ©ç”¨CMATé¢„è®­ç»ƒçš„ç‰¹å¾ï¼Œç»“åˆå¤šæ¨¡æ€æç¤ºï¼Œç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†CMATé¢„è®­ç»ƒç­–ç•¥ã€‚CMATé€šè¿‡è”åˆä¼˜åŒ–é‡å»ºæŸå¤±ã€äº²å’ŒåŠ›æŸå¤±å’Œå¤šæ ·æ€§æŸå¤±ï¼Œä½¿å¾—3Dç¼–ç å™¨èƒ½å¤Ÿå­¦ä¹ åˆ°ä¸2Dè¯­ä¹‰å¯¹é½çš„ã€å…·æœ‰è‰¯å¥½è¯­ä¹‰ç»„ç»‡æ€§çš„è¡¨ç¤ºã€‚è¿™ç§è·¨æ¨¡æ€çš„çŸ¥è¯†è¿ç§»æ–¹å¼ï¼Œæœ‰æ•ˆå…‹æœäº†3Dæ•°æ®è‡ªèº«çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCMATé¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨äº†ä¸‰ç§æŸå¤±å‡½æ•°ï¼š1) é‡å»ºæŸå¤±ï¼Œä¿è¯3Dç‰¹å¾èƒ½å¤Ÿé‡å»ºåŸå§‹3Dæ•°æ®ï¼›2) äº²å’ŒåŠ›æŸå¤±ï¼Œä¿ƒä½¿3Dç‰¹å¾åœ¨è¯­ä¹‰ç›¸ä¼¼çš„ç‚¹ä¹‹é—´å…·æœ‰æ›´é«˜çš„äº²å’ŒåŠ›ï¼›3) å¤šæ ·æ€§æŸå¤±ï¼Œé¼“åŠ±3Dç‰¹å¾åœ¨ä¸åŒç±»åˆ«ä¹‹é—´å…·æœ‰æ›´é«˜çš„åŒºåˆ†åº¦ã€‚CASTåˆ†å‰²é˜¶æ®µï¼Œä½¿ç”¨äº†Transformerç»“æ„ï¼Œå°†å¤šæ¨¡æ€æç¤ºï¼ˆä¾‹å¦‚æ–‡æœ¬æˆ–è§†è§‰æç¤ºï¼‰ä¸CMATé¢„è®­ç»ƒçš„3Dç‰¹å¾è¿›è¡Œèåˆï¼Œä»è€Œç”Ÿæˆprompt-awareçš„åˆ†å‰²ç»“æœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨æ ‡å‡†3Då¯ä¾›æ€§åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„CMATå’ŒCASTæ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜æ˜¾çš„æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œã€å…·èº«æ™ºèƒ½å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨å¯ä¾›æ€§åˆ†å‰²çš„ç»“æœï¼Œæ›´å¥½åœ°ç†è§£ç‰©ä½“çš„åŠŸèƒ½ï¼Œä»è€Œæ‰§è¡Œæ›´å¤æ‚çš„æŠ“å–ã€æ”¾ç½®ç­‰æ“ä½œã€‚åœ¨ARåº”ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·çš„äº¤äº’æ„å›¾ï¼ŒåŠ¨æ€åœ°åˆ†å‰²3Dåœºæ™¯ä¸­çš„ç‰©ä½“ï¼Œæä¾›æ›´æ™ºèƒ½çš„äº¤äº’ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Affordance segmentation aims to parse 3D objects into functionally distinct parts, bridging recognition and interaction for applications in robotic manipulation, embodied AI, and AR. While recent studies leverage visual or textual prompts to guide this process, they often rely on point cloud encoders as generic feature extractors, overlooking the intrinsic challenges of 3D data such as sparsity, noise, and geometric ambiguity. As a result, 3D features learned in isolation frequently lack clear and semantically consistent functional boundaries. To address this bottleneck, we propose a semantic-grounded learning paradigm that transfers rich semantic knowledge from large-scale 2D Vision Foundation Models (VFMs) into the 3D domain. Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training strategy that aligns a 3D encoder with lifted 2D semantics and jointly optimizes reconstruction, affinity, and diversity to yield semantically organized representations. Building on this backbone, we further design the Cross-modal Affordance Segmentation Transformer (CAST), which integrates multi-modal prompts with CMAT-pretrained features to generate precise, prompt-aware segmentation maps. Extensive experiments on standard benchmarks demonstrate that our framework establishes new state-of-the-art results for 3D affordance segmentation.

