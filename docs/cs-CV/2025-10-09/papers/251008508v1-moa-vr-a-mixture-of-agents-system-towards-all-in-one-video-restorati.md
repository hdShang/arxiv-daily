---
layout: default
title: MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration
---

# MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08508" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08508v1</a>
  <a href="https://arxiv.org/pdf/2510.08508.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08508v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08508v1', 'MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lu Liu, Chunlei Cai, Shaocheng Shen, Jianfeng Liang, Weimin Ouyang, Tianxiao Ye, Jian Mao, Huiyu Duan, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoA-VRï¼Œä¸€ä¸ªæ··åˆAgentçš„é€šç”¨è§†é¢‘ä¿®å¤ç³»ç»Ÿï¼Œæœ‰æ•ˆå¤„ç†å¤æ‚é€€åŒ–ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ä¿®å¤` `æ··åˆAgent` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†é¢‘è´¨é‡è¯„ä¼°` `è‡ªé€‚åº”è·¯ç”±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ä¿®å¤æ–¹æ³•é€šå¸¸éœ€è¦æ‰‹åŠ¨é€‰æ‹©ä¸“é—¨æ¨¡å‹ï¼Œæˆ–ä¾èµ–äºéš¾ä»¥æ³›åŒ–çš„å•ä½“æ¶æ„ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†çœŸå®åœºæ™¯ä¸­å¤æ‚å¤šæ ·çš„è§†é¢‘é€€åŒ–ã€‚
2. MoA-VRç³»ç»Ÿæ¨¡ä»¿äººç±»ä¸“å®¶ï¼Œé€šè¿‡é€€åŒ–è¯†åˆ«ã€è·¯ç”±ä¸ä¿®å¤ã€ä¿®å¤è´¨é‡è¯„ä¼°ä¸‰ä¸ªAgentååŒå·¥ä½œï¼Œå®ç°å¯¹å¤æ‚é€€åŒ–çš„æœ‰æ•ˆå¤„ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMoA-VRåœ¨å®¢è§‚æŒ‡æ ‡å’Œæ„ŸçŸ¥è´¨é‡ä¸Šå‡è¶…è¶Šç°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€æ™ºèƒ½å’Œæ¨¡å—åŒ–æ¨ç†åœ¨è§†é¢‘ä¿®å¤ä¸­çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMoA-VRï¼Œé¦–ä¸ªæ··åˆAgentè§†é¢‘ä¿®å¤ç³»ç»Ÿï¼Œæ—¨åœ¨æ¨¡ä»¿ä¸“ä¸šäººå‘˜çš„æ¨ç†å’Œå¤„ç†æµç¨‹ï¼Œé€šè¿‡ä¸‰ä¸ªååŒAgentå®ç°ï¼šé€€åŒ–è¯†åˆ«ã€è·¯ç”±ä¸ä¿®å¤ã€ä¿®å¤è´¨é‡è¯„ä¼°ã€‚æ„å»ºäº†å¤§è§„æ¨¡é«˜åˆ†è¾¨ç‡è§†é¢‘é€€åŒ–è¯†åˆ«åŸºå‡†ï¼Œå¹¶æ„å»ºäº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é©±åŠ¨çš„é€€åŒ–è¯†åˆ«å™¨ã€‚å¼•å…¥ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„è‡ªé€‚åº”è·¯ç”±å™¨ï¼Œé€šè¿‡è§‚å¯Ÿå·¥å…·ä½¿ç”¨æ¨¡å¼è‡ªä¸»å­¦ä¹ æœ‰æ•ˆçš„ä¿®å¤ç­–ç•¥ã€‚ä¸ºäº†è¯„ä¼°ä¸­é—´å’Œæœ€ç»ˆå¤„ç†çš„è§†é¢‘è´¨é‡ï¼Œæ„å»ºäº†ä¿®å¤è§†é¢‘è´¨é‡ï¼ˆRes-VQï¼‰æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ä¸“é—¨çš„åŸºäºVLMçš„è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰æ¨¡å‹ï¼Œä¸“ä¸ºä¿®å¤ä»»åŠ¡å®šåˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoA-VRèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å„ç§å¤æ‚é€€åŒ–ï¼Œåœ¨å®¢è§‚æŒ‡æ ‡å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚è¿™äº›ç»“æœçªå‡ºäº†åœ¨é€šç”¨è§†é¢‘ä¿®å¤ç³»ç»Ÿä¸­é›†æˆå¤šæ¨¡æ€æ™ºèƒ½å’Œæ¨¡å—åŒ–æ¨ç†çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°å®ä¸–ç•Œçš„è§†é¢‘ç”±äºé‡‡é›†å’Œä¼ è¾“æ¡ä»¶çš„å¤šæ ·æ€§ï¼Œç»å¸¸å—åˆ°å¤æ‚çš„é€€åŒ–å½±å“ï¼Œä¾‹å¦‚å™ªå£°ã€å‹ç¼©ä¼ªå½±å’Œä½å…‰å¤±çœŸã€‚ç°æœ‰çš„ä¿®å¤æ–¹æ³•é€šå¸¸éœ€è¦ä¸“ä¸šäººå‘˜æ‰‹åŠ¨é€‰æ‹©ä¸“é—¨çš„æ¨¡å‹ï¼Œæˆ–è€…ä¾èµ–äºæ— æ³•åœ¨ä¸åŒé€€åŒ–æƒ…å†µä¸‹æ³›åŒ–çš„å•ä½“æ¶æ„ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥åº”å¯¹çœŸå®åœºæ™¯ä¸­å¤æ‚ä¸”æ··åˆçš„è§†é¢‘é€€åŒ–é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoA-VRçš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡ä»¿äººç±»ä¸“å®¶è¿›è¡Œè§†é¢‘ä¿®å¤çš„æµç¨‹ã€‚äººç±»ä¸“å®¶ä¼šé¦–å…ˆè¯†åˆ«è§†é¢‘çš„é€€åŒ–ç±»å‹ï¼Œç„¶åé€‰æ‹©åˆé€‚çš„ä¿®å¤å·¥å…·å’Œç­–ç•¥ï¼Œæœ€åè¯„ä¼°ä¿®å¤è´¨é‡ã€‚MoA-VRé€šè¿‡æ„å»ºä¸‰ä¸ªååŒå·¥ä½œçš„Agentæ¥å®ç°è¿™ä¸€è¿‡ç¨‹ï¼šé€€åŒ–è¯†åˆ«Agentã€è·¯ç”±ä¸ä¿®å¤Agentã€ä¿®å¤è´¨é‡è¯„ä¼°Agentã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoA-VRç³»ç»ŸåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) **é€€åŒ–è¯†åˆ«Agent**ï¼šä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯†åˆ«è§†é¢‘ä¸­çš„é€€åŒ–ç±»å‹ã€‚è¯¥AgentåŸºäºå¤§è§„æ¨¡é«˜åˆ†è¾¨ç‡è§†é¢‘é€€åŒ–è¯†åˆ«åŸºå‡†è¿›è¡Œè®­ç»ƒã€‚2) **è·¯ç”±ä¸ä¿®å¤Agent**ï¼šè¯¥Agentç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨ï¼Œæ ¹æ®é€€åŒ–è¯†åˆ«Agentçš„è¾“å‡ºï¼Œè‡ªä¸»é€‰æ‹©åˆé€‚çš„ä¿®å¤å·¥å…·å’Œç­–ç•¥ã€‚è¯¥Agenté€šè¿‡è§‚å¯Ÿå·¥å…·ä½¿ç”¨æ¨¡å¼å­¦ä¹ æœ‰æ•ˆçš„ä¿®å¤ç­–ç•¥ã€‚3) **ä¿®å¤è´¨é‡è¯„ä¼°Agent**ï¼šä½¿ç”¨åŸºäºVLMçš„è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰æ¨¡å‹è¯„ä¼°ä¿®å¤åçš„è§†é¢‘è´¨é‡ã€‚è¯¥AgentåŸºäºä¿®å¤è§†é¢‘è´¨é‡ï¼ˆRes-VQï¼‰æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šMoA-VRçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ··åˆAgentæ¶æ„å’Œè‡ªé€‚åº”è·¯ç”±ç­–ç•¥ã€‚ä¼ ç»Ÿçš„è§†é¢‘ä¿®å¤æ–¹æ³•é€šå¸¸ä½¿ç”¨å•ä½“æ¨¡å‹ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚å¤šæ ·çš„é€€åŒ–ã€‚MoA-VRé€šè¿‡å°†ä¿®å¤è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„Agentï¼Œæ¯ä¸ªAgentè´Ÿè´£ä¸åŒçš„ä»»åŠ¡ï¼Œä»è€Œæé«˜äº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è‡ªé€‚åº”è·¯ç”±ç­–ç•¥å…è®¸ç³»ç»Ÿæ ¹æ®ä¸åŒçš„é€€åŒ–ç±»å‹é€‰æ‹©ä¸åŒçš„ä¿®å¤å·¥å…·å’Œç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†ä¿®å¤æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šé€€åŒ–è¯†åˆ«Agentä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹è§†é¢‘é€€åŒ–è¯†åˆ«ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚è·¯ç”±ä¸ä¿®å¤Agentä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ§åˆ¶å™¨ï¼Œæ ¹æ®é€€åŒ–è¯†åˆ«ç»“æœé€‰æ‹©åˆé€‚çš„ä¿®å¤å·¥å…·ã€‚ä¿®å¤è´¨é‡è¯„ä¼°Agentä½¿ç”¨åŸºäºVLMçš„è§†é¢‘è´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é’ˆå¯¹ä¿®å¤ä»»åŠ¡è¿›è¡Œäº†ä¸“é—¨çš„è®­ç»ƒã€‚Res-VQæ•°æ®é›†åŒ…å«å„ç§é€€åŒ–ç±»å‹å’Œä¿®å¤æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°ä¿®å¤è´¨é‡è¯„ä¼°Agentã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMoA-VRåœ¨å¤„ç†å„ç§å¤æ‚é€€åŒ–æ—¶ï¼Œåœ¨å®¢è§‚æŒ‡æ ‡ï¼ˆå¦‚PSNRã€SSIMï¼‰å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒMoA-VRåœ¨å¤šä¸ªè§†é¢‘ä¿®å¤æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoA-VRå¯åº”ç”¨äºå„ç§è§†é¢‘ä¿®å¤åœºæ™¯ï¼Œä¾‹å¦‚è€æ—§è§†é¢‘ä¿®å¤ã€ç›‘æ§è§†é¢‘å¢å¼ºã€ä½è´¨é‡è§†é¢‘ä¿®å¤ç­‰ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆæé«˜è§†é¢‘çš„è§†è§‰è´¨é‡ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼ŒMoA-VRæœ‰æœ›é›†æˆåˆ°å„ç§è§†é¢‘å¤„ç†å¹³å°å’Œåº”ç”¨ä¸­ï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ ä¾¿æ·é«˜æ•ˆçš„è§†é¢‘ä¿®å¤æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo \underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \underline{Res}tored \underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.

