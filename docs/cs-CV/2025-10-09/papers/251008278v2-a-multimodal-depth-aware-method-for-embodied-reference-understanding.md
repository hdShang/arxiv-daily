---
layout: default
title: A Multimodal Depth-Aware Method For Embodied Reference Understanding
---

# A Multimodal Depth-Aware Method For Embodied Reference Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08278" target="_blank" class="toolbar-btn">arXiv: 2510.08278v2</a>
    <a href="https://arxiv.org/pdf/2510.08278.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08278v2" 
            onclick="toggleFavorite(this, '2510.08278v2', 'A Multimodal Depth-Aware Method For Embodied Reference Understanding')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Fevziye Irem Eyiokur, Dogucan Yaman, HazÄ±m Kemal Ekenel, Alexander Waibel

**åˆ†ç±»**: cs.CV, cs.HC, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09 (æ›´æ–°: 2025-10-10)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¤šæ¨¡æ€æ·±åº¦æ„ŸçŸ¥æ–¹æ³•ï¼Œç”¨äºå…·èº«å¼•ç”¨ç†è§£ä»»åŠ¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«å¼•ç”¨ç†è§£` `å¤šæ¨¡æ€èåˆ` `æ·±åº¦å­¦ä¹ ` `æ·±åº¦æ„ŸçŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°æ®å¢å¼º` `ç›®æ ‡æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·èº«å¼•ç”¨ç†è§£æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆåŒºåˆ†å¤šä¸ªç›¸ä¼¼å€™é€‰å¯¹è±¡ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼ŒèåˆLLMæ•°æ®å¢å¼ºã€æ·±åº¦ä¿¡æ¯å’Œæ·±åº¦æ„ŸçŸ¥å†³ç­–æ¨¡å—ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæå‡äº†æŒ‡ä»£å¯¹è±¡æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«å¼•ç”¨ç†è§£ï¼ˆEmbodied Reference Understanding, ERUï¼‰éœ€è¦åœ¨è§†è§‰åœºæ™¯ä¸­ï¼Œæ ¹æ®è¯­è¨€æŒ‡ä»¤å’ŒæŒ‡å‘çº¿ç´¢è¯†åˆ«ç›®æ ‡å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•åœ¨å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨å­˜åœ¨å¤šä¸ªå€™é€‰å¯¹è±¡çš„æ¨¡ç³Šåœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ERUæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆåˆ©ç”¨åŸºäºLLMçš„æ•°æ®å¢å¼ºã€æ·±åº¦å›¾æ¨¡æ€å’Œæ·±åº¦æ„ŸçŸ¥çš„å†³ç­–æ¨¡å—ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿç¨³å¥åœ°æ•´åˆè¯­è¨€å’Œå…·èº«çº¿ç´¢ï¼Œä»è€Œæé«˜å¤æ‚æˆ–æ··ä¹±ç¯å¢ƒä¸­æ¶ˆæ­§èƒ½åŠ›ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå®ç°äº†æ›´å‡†ç¡®ã€æ›´å¯é çš„æŒ‡ä»£å¯¹è±¡æ£€æµ‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå…·èº«å¼•ç”¨ç†è§£ä»»åŠ¡æ—¨åœ¨æ ¹æ®è¯­è¨€æŒ‡ä»¤å’ŒæŒ‡å‘çº¿ç´¢ï¼Œåœ¨è§†è§‰åœºæ™¯ä¸­å‡†ç¡®è¯†åˆ«ç›®æ ‡å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å­˜åœ¨å¤šä¸ªç›¸ä¼¼å€™é€‰å¯¹è±¡çš„å¤æ‚åœºæ™¯æ—¶ï¼Œç”±äºç¼ºä¹æœ‰æ•ˆçš„æ¶ˆæ­§æœºåˆ¶ï¼Œæ€§èƒ½å—åˆ°é™åˆ¶ã€‚å°¤å…¶æ˜¯åœ¨å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹ä¸­ï¼Œå¦‚ä½•å‡†ç¡®ç†è§£è¯­è¨€æŒ‡ä»¤å¹¶å°†å…¶ä¸è§†è§‰ä¿¡æ¯å¯¹é½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦ä¿¡æ¯ä½œä¸ºé¢å¤–çš„æ¨¡æ€ï¼Œå¹¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ•°æ®å¢å¼ºï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ•´åˆè¯­è¨€å’Œå…·èº«çº¿ç´¢ã€‚é€šè¿‡å¼•å…¥æ·±åº¦æ„ŸçŸ¥çš„å†³ç­–æ¨¡å—ï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨æ·±åº¦ä¿¡æ¯æ¥åŒºåˆ†ä¸åŒè·ç¦»çš„å¯¹è±¡ï¼Œä»è€Œæé«˜æ¶ˆæ­§èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) åŸºäºLLMçš„æ•°æ®å¢å¼ºæ¨¡å—ï¼Œç”¨äºç”Ÿæˆæ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ï¼›2) æ·±åº¦å›¾æ¨¡æ€å¤„ç†æ¨¡å—ï¼Œç”¨äºæå–åœºæ™¯çš„æ·±åº¦ä¿¡æ¯ï¼›3) å¤šæ¨¡æ€èåˆæ¨¡å—ï¼Œå°†è¯­è¨€æŒ‡ä»¤ã€è§†è§‰ä¿¡æ¯å’Œæ·±åº¦ä¿¡æ¯è¿›è¡Œèåˆï¼›4) æ·±åº¦æ„ŸçŸ¥çš„å†³ç­–æ¨¡å—ï¼Œæ ¹æ®èåˆåçš„ä¿¡æ¯ï¼Œåˆ¤æ–­ç›®æ ‡å¯¹è±¡ã€‚æ•´ä½“æµç¨‹æ˜¯ä»è¾“å…¥è¯­è¨€æŒ‡ä»¤ã€è§†è§‰å›¾åƒå’Œæ·±åº¦å›¾å¼€å§‹ï¼Œç»è¿‡å„ä¸ªæ¨¡å—çš„å¤„ç†ï¼Œæœ€ç»ˆè¾“å‡ºç›®æ ‡å¯¹è±¡çš„é¢„æµ‹ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ·±åº¦æ„ŸçŸ¥çš„å†³ç­–æ¨¡å—çš„è®¾è®¡ï¼Œä»¥åŠå°†æ·±åº¦ä¿¡æ¯ä½œä¸ºå…³é”®æ¨¡æ€èå…¥åˆ°å…·èº«å¼•ç”¨ç†è§£ä»»åŠ¡ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨åœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œä»è€Œæé«˜åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ¶ˆæ­§èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè¿›è¡Œæ•°æ®å¢å¼ºä¹Ÿæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å¯èƒ½æ¶‰åŠçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šæ·±åº¦å›¾çš„è¡¨ç¤ºæ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œæ·±åº¦å€¼çš„å½’ä¸€åŒ–èŒƒå›´ï¼‰ï¼Œæ·±åº¦æ„ŸçŸ¥å†³ç­–æ¨¡å—çš„å…·ä½“ç½‘ç»œç»“æ„ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæˆ–Transformerï¼‰ï¼Œå¤šæ¨¡æ€èåˆçš„æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œä»¥åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼ˆä¾‹å¦‚ï¼Œäº¤å‰ç†µæŸå¤±æˆ–IoUæŸå¤±ï¼‰ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚å®éªŒç»“æœè¯æ˜äº†æ·±åº¦ä¿¡æ¯å’ŒLLMæ•°æ®å¢å¼ºåœ¨å…·èº«å¼•ç”¨ç†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠæ·±åº¦æ„ŸçŸ¥å†³ç­–æ¨¡å—çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€äººæœºäº¤äº’ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·çš„è¯­éŸ³æŒ‡ä»¤å’Œæ‰‹åŠ¿ï¼Œå‡†ç¡®è¯†åˆ«å¹¶æŠ“å–ç›®æ ‡ç‰©ä½“ã€‚åœ¨æ™ºèƒ½å®¶å±…åœºæ™¯ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ§åˆ¶å®¶ç”µè®¾å¤‡ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied Reference Understanding requires identifying a target object in a visual scene based on both language instructions and pointing cues. While prior works have shown progress in open-vocabulary object detection, they often fail in ambiguous scenarios where multiple candidate objects exist in the scene. To address these challenges, we propose a novel ERU framework that jointly leverages LLM-based data augmentation, depth-map modality, and a depth-aware decision module. This design enables robust integration of linguistic and embodied cues, improving disambiguation in complex or cluttered environments. Experimental results on two datasets demonstrate that our approach significantly outperforms existing baselines, achieving more accurate and reliable referent detection.

