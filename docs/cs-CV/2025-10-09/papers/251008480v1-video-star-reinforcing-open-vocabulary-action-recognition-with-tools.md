---
layout: default
title: Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools
---

# Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08480" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08480v1</a>
  <a href="https://arxiv.org/pdf/2510.08480.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08480v1" onclick="toggleFavorite(this, '2510.08480v1', 'Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Video-STARï¼šåˆ©ç”¨å·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«` `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å­åŠ¨ä½œåˆ†è§£` `å·¥å…·å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«æ–¹æ³•éš¾ä»¥åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼çš„åŠ¨ä½œï¼Œä¸”æ˜“å—æ–‡æœ¬å…ˆéªŒçŸ¥è¯†å½±å“ï¼Œäº§ç”Ÿè·¨æ¨¡æ€å¹»è§‰ã€‚
2. Video-STARå°†åŠ¨ä½œåˆ†è§£ä¸ºå¯åŒºåˆ†çš„å­åŠ¨ä½œï¼Œå¹¶åˆ©ç”¨å·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ åŠ¨æ€è°ƒç”¨é¢†åŸŸå·¥å…·ï¼Œå®ç°ç»†ç²’åº¦åŒ¹é…å’Œç±»åˆ«æ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-STARåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œæœ‰æ•ˆåŒºåˆ†ç»†ç²’åº¦åŠ¨ä½œå¹¶å‡å°‘è·¨æ¨¡æ€å¹»è§‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨è¿æ¥è§†è§‰å’Œæ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¯¹ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ï¼Œå¸¸å¸¸é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼åŠ¨ä½œçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Video-STARï¼Œä¸€ä¸ªå°†ä¸Šä¸‹æ–‡å­åŠ¨ä½œåˆ†è§£ä¸å·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ¡†æ¶ï¼Œç”¨äºå¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«(OVAR)ã€‚ä¸å…ˆå‰å°†åŠ¨ä½œè§†ä¸ºå•ä¸€å®ä½“çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ›æ–°æ€§åœ°å°†åŠ¨ä½œåˆ†è§£ä¸ºå¯åŒºåˆ†çš„å­åŠ¨ä½œä»¥è¿›è¡Œç»†ç²’åº¦åŒ¹é…ï¼ŒåŒæ—¶åŠ¨æ€åœ°è°ƒç”¨ç‰¹å®šé¢†åŸŸçš„å·¥å…·è¿›è¡Œè·¨æ¨¡æ€äº¤é”™ï¼Œä»è€Œå®ç°ç‰¹å®šç±»åˆ«çš„æ¨ç†èƒ½åŠ›å¹¶å‡å°‘è·¨æ¨¡æ€å¹»è§‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾è®¡ä¸€ä¸ªåˆ†å±‚å¥–åŠ±ï¼Œå¹³è¡¡å·¥å…·ä½¿ç”¨æ•ˆç‡ã€å­åŠ¨ä½œç›¸å…³æ€§å’Œæ¨ç†ä¸­çš„ç»“æ„è¿è´¯æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è‡ªä¸»åœ°åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥ä¼˜å…ˆè€ƒè™‘å­åŠ¨ä½œæ¨¡å¼ï¼Œè€Œæ— éœ€æ˜¾å¼ç›‘ç£ï¼Œä»è€Œä»ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ¨ç†è¿‡æ¸¡åˆ°è§†è§‰åŸºç¡€çš„æ¨ç†ã€‚åœ¨HMDB-51ã€UCF-101ã€SSv2ã€Kinetics-400å’ŒKinetics-600æ•°æ®é›†ä¸Šçš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨åŒºåˆ†ç»†ç²’åº¦åŠ¨ä½œå’Œå¤„ç†è·¨æ¨¡æ€å¹»è§‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬å“è¶Šçš„é²æ£’æ€§å’Œæ³›åŒ–æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡åŠ¨ä½œè¯†åˆ«ï¼ˆOVARï¼‰ä¸­ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰éš¾ä»¥åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼åŠ¨ä½œï¼Œä¸”å®¹æ˜“å—åˆ°æ–‡æœ¬å…ˆéªŒçŸ¥è¯†å½±å“ï¼Œäº§ç”Ÿè·¨æ¨¡æ€å¹»è§‰çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†åŠ¨ä½œè§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œå¿½ç•¥äº†åŠ¨ä½œå†…éƒ¨çš„ç»†ç²’åº¦ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¸”è¿‡åº¦ä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼Œå¯¼è‡´è§†è§‰ä¿¡æ¯åˆ©ç”¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åŠ¨ä½œåˆ†è§£ä¸ºæ›´å…·åŒºåˆ†æ€§çš„å­åŠ¨ä½œï¼ˆsub-motionsï¼‰ï¼Œå¹¶åˆ©ç”¨å·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆTool-augmented Reinforcement Learningï¼‰åŠ¨æ€åœ°è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œä»è€Œå®ç°ç»†ç²’åº¦çš„è·¨æ¨¡æ€æ¨ç†ã€‚é€šè¿‡å­åŠ¨ä½œåˆ†è§£ï¼Œæ¨¡å‹å¯ä»¥å…³æ³¨åŠ¨ä½œçš„å±€éƒ¨ç»†èŠ‚ï¼Œä»è€Œæ›´å¥½åœ°åŒºåˆ†ç›¸ä¼¼åŠ¨ä½œã€‚é€šè¿‡å·¥å…·å¢å¼ºï¼Œæ¨¡å‹å¯ä»¥åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æ¥è¾…åŠ©æ¨ç†ï¼Œå‡å°‘å¯¹æ–‡æœ¬å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVideo-STARæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å­åŠ¨ä½œåˆ†è§£æ¨¡å—ï¼šå°†è¾“å…¥çš„è§†é¢‘åˆ†è§£ä¸ºä¸€ç³»åˆ—å­åŠ¨ä½œã€‚å…·ä½“å¦‚ä½•åˆ†è§£ï¼Œè®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå¯èƒ½æ˜¯é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹æˆ–äººå·¥æ ‡æ³¨çš„æ–¹å¼ã€‚2) å·¥å…·å¢å¼ºæ¨¡å—ï¼šæ ¹æ®å½“å‰çš„çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œå·²ç»è¯†åˆ«çš„å­åŠ¨ä½œï¼‰ï¼ŒåŠ¨æ€åœ°é€‰æ‹©åˆé€‚çš„å¤–éƒ¨å·¥å…·ã€‚å·¥å…·çš„å…·ä½“ç±»å‹æœªçŸ¥ï¼Œå¯èƒ½åŒ…æ‹¬åŠ¨ä½œå±æ€§æŸ¥è¯¢ã€åœºæ™¯ä¿¡æ¯æ£€ç´¢ç­‰ã€‚3) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œè¯¥ç­–ç•¥ç½‘ç»œè´Ÿè´£é€‰æ‹©åˆé€‚çš„å­åŠ¨ä½œå’Œå·¥å…·ï¼Œå¹¶æ ¹æ®ç¯å¢ƒçš„åé¦ˆï¼ˆå¥–åŠ±ï¼‰ä¸æ–­ä¼˜åŒ–ç­–ç•¥ã€‚4) åŠ¨ä½œè¯†åˆ«æ¨¡å—ï¼šæ ¹æ®æœ€ç»ˆçš„å­åŠ¨ä½œåºåˆ—å’Œå·¥å…·è°ƒç”¨ç»“æœï¼Œé¢„æµ‹åŠ¨ä½œçš„ç±»åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§åŸºäºå­åŠ¨ä½œåˆ†è§£çš„åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åŠ¨ä½œçš„ç»†ç²’åº¦ä¿¡æ¯ã€‚2) å¼•å…¥äº†å·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŠ¨æ€åœ°åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æ¥è¾…åŠ©æ¨ç†ï¼Œä»è€Œå‡å°‘å¯¹æ–‡æœ¬å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚3) è®¾è®¡äº†ä¸€ç§åˆ†å±‚å¥–åŠ±å‡½æ•°ï¼Œèƒ½å¤Ÿå¹³è¡¡å·¥å…·ä½¿ç”¨æ•ˆç‡ã€å­åŠ¨ä½œç›¸å…³æ€§å’Œæ¨ç†çš„ç»“æ„è¿è´¯æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³äºå­åŠ¨ä½œåˆ†è§£æ¨¡å—å’Œå·¥å…·å¢å¼ºæ¨¡å—çš„å…·ä½“å®ç°ç»†èŠ‚æè¿°è¾ƒå°‘ï¼Œä¾‹å¦‚ï¼Œå¦‚ä½•è¿›è¡Œå­åŠ¨ä½œåˆ†è§£ï¼Œæœ‰å“ªäº›ç±»å‹çš„å·¥å…·ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„å·¥å…·ç­‰ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ï¼Œéœ€è¦å¹³è¡¡å¤šä¸ªç›®æ ‡ï¼ŒåŒ…æ‹¬å·¥å…·ä½¿ç”¨æ•ˆç‡ã€å­åŠ¨ä½œç›¸å…³æ€§å’Œæ¨ç†çš„ç»“æ„è¿è´¯æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Video-STARåœ¨HMDB-51ã€UCF-101ã€SSv2ã€Kinetics-400å’ŒKinetics-600ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨åŒºåˆ†ç»†ç²’åº¦åŠ¨ä½œå’Œå¤„ç†è·¨æ¨¡æ€å¹»è§‰æ–¹é¢è¡¨ç°çªå‡ºï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€è§†é¢‘å†…å®¹åˆ†æã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯æ›´å‡†ç¡®åœ°è¯†åˆ«å¼‚å¸¸è¡Œä¸ºï¼›åœ¨è§†é¢‘å†…å®¹åˆ†æä¸­ï¼Œå¯ä»¥è‡ªåŠ¨è¯†åˆ«è§†é¢‘ä¸­çš„åŠ¨ä½œï¼Œä»è€Œå®ç°è§†é¢‘å†…å®¹çš„è‡ªåŠ¨æ ‡æ³¨å’Œæ£€ç´¢ï¼›åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥è¯†åˆ«ç”¨æˆ·çš„åŠ¨ä½œï¼Œä»è€Œå®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have demonstrated remarkable potential in bridging visual and textual reasoning, yet their reliance on text-centric priors often limits their ability to disentangle semantically similar actions in open-vocabulary scenarios. To address this, we propose Video-STAR, a framework that harmonizes contextual sub-motion decomposition with tool-augmented reinforcement learning for open-vocabulary action recognition (OVAR). Unlike prior methods that treat actions as monolithic entities, our approach innovatively decomposes actions into discriminative sub-motions for fine-grained matching while dynamically invoking domain-specific tools for cross-modal interleaving, thereby enabling category-specific reasoning capacity and reducing cross-modal hallucination. Moreover, by designing a hierarchical reward that balances tool-usage efficiency, sub-motion relevance, and structural coherence in reasoning, our method autonomously leverages external tools to prioritize sub-motion patterns without explicit supervision, transmitting from text-centric reasoning to visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2, Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art performance, outperforming existing methods in distinguishing fine-grained actions and handling cross-modal hallucination, validating our excellent robustness and generalization.

