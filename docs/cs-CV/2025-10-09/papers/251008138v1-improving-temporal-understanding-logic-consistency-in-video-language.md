---
layout: default
title: Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement
---

# Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08138" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08138v1</a>
  <a href="https://arxiv.org/pdf/2510.08138.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08138v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08138v1', 'Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengzhi Li, Heyan Huang, Ping Jian, Zhen Yang, Yaning Tian

**åˆ†ç±»**: cs.CV, cs.AI, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶åºæ¡ä»¶æ³¨æ„åŠ›é”åŒ–(TCAS)æ–¹æ³•ï¼Œæå‡è§†é¢‘è¯­è¨€æ¨¡å‹æ—¶åºç†è§£é€»è¾‘ä¸€è‡´æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘è¯­è¨€æ¨¡å‹` `æ—¶åºç†è§£` `é€»è¾‘ä¸€è‡´æ€§` `æ³¨æ„åŠ›æœºåˆ¶` `è·¨æ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŸºäºç›¸åŒè§†é¢‘å†…å®¹çš„ä¸åŒæé—®æ—¶ï¼Œå®¹æ˜“äº§ç”Ÿé€»è¾‘ä¸ä¸€è‡´çš„å›ç­”ï¼Œæ ¹æœ¬åŸå› å°šä¸æ˜ç¡®ã€‚
2. è®ºæ–‡æå‡ºæ—¶åºæ¡ä»¶æ³¨æ„åŠ›é”åŒ–(TCAS)æ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºæ¨¡å‹åŒºåˆ†ä¸åŒæ—¶é—´æˆ³è§†é¢‘tokençš„èƒ½åŠ›ï¼Œæå‡æ—¶åºç†è§£é€»è¾‘ä¸€è‡´æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTCASæ˜¾è‘—æå‡äº†è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ—¶é—´é€»è¾‘ä¸€è‡´æ€§ï¼Œå¹¶åœ¨è§†é¢‘æ—¶åº grounding ä»»åŠ¡ä¸­å–å¾—äº†æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¸¸å¸¸äº§ç”Ÿè‡ªç›¸çŸ›ç›¾çš„è¾“å‡ºï¼Œä¸¥é‡å½±å“äº†å®ƒä»¬çš„å¯é æ€§ï¼Œå¹¶é˜»ç¢äº†å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„é‡‡ç”¨ã€‚åœ¨è§†é¢‘è¯­è¨€æ¨¡å‹(Video-LLMs)ä¸­ï¼Œè¿™ç§ç°è±¡æœ€è¿‘å¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å…³æ³¨ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™äº›æ¨¡å‹æ— æ³•å¯¹å…¶åŸºäº grounding è¾“å‡ºçš„é‡Šä¹‰é—®é¢˜æä¾›é€»è¾‘ä¸Šä¸€è‡´çš„å“åº”ã€‚ç„¶è€Œï¼Œè¿™ç§ç°è±¡çš„æ ¹æœ¬åŸå› ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¯è§£é‡Šæ€§é©±åŠ¨çš„æ–¹æ³•æ¥åˆ†æã€ç»Ÿè®¡æ€»ç»“å’Œå¹²é¢„è¯¥ç°è±¡çš„æ½œåœ¨å› ç´ ã€‚æˆ‘ä»¬å‘ç°ï¼Œå“åº”ä¸ä¸€è‡´çš„ä¸»è¦åŸå› ä¹‹ä¸€æ˜¯è·¨æ¨¡æ€æ³¨æ„åŠ›å¤´æ— æ³•æœ‰æ•ˆåœ°åŒºåˆ†ä¸åŒæ—¶é—´æˆ³çš„è§†é¢‘ tokensã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ—¶åºæ¡ä»¶æ³¨æ„åŠ›é”åŒ–(TCAS)çš„æ³¨æ„åŠ›å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›åŒºåˆ†çš„å¢å¼ºç›®æ ‡ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ—¶é—´åˆ†è¾¨ç‡èƒ½åŠ›ï¼Œä»è€Œæé«˜å…¶æ—¶é—´ç†è§£é€»è¾‘ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº† Video-LLMs çš„æ—¶é—´é€»è¾‘ä¸€è‡´æ€§ã€‚è¿›ä¸€æ­¥çš„å¯è§£é‡Šæ€§åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®å®æé«˜äº†æ³¨æ„åŠ›å¤´çš„æ—¶é—´å¯åŒºåˆ†æ€§ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç»“è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€èˆ¬çš„è§†é¢‘æ—¶åº grounding ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†æ€§èƒ½æå‡ï¼Œçªå‡ºäº†æ—¶é—´é€»è¾‘ä¸€è‡´æ€§æ˜¯æ—¶é—´ç†è§£çš„ç“¶é¢ˆã€‚é€šè¿‡å¢å¼ºä¸€è‡´æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¨åŠ¨äº†è§†é¢‘æ—¶é—´ç†è§£çš„æ˜¾è‘—è¿›å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘è¯­è¨€æ¨¡å‹åœ¨ç†è§£è§†é¢‘å†…å®¹æ—¶ï¼Œå¯¹äºåŸºäºç›¸åŒè§†é¢‘å†…å®¹çš„ä¸åŒæé—®ï¼Œç»å¸¸ç»™å‡ºé€»è¾‘ä¸ä¸€è‡´çš„å›ç­”ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è¿™ç§ä¸ä¸€è‡´æ€§çš„æ·±å…¥åˆ†æï¼Œä»¥åŠæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç—›ç‚¹åœ¨äºè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶æ— æ³•æœ‰æ•ˆåŒºåˆ†ä¸åŒæ—¶é—´æˆ³çš„è§†é¢‘ tokensï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•å‡†ç¡®æ•æ‰è§†é¢‘ä¸­çš„æ—¶åºä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢å¼ºæ¨¡å‹å¯¹ä¸åŒæ—¶é—´æˆ³è§†é¢‘ tokens çš„åŒºåˆ†èƒ½åŠ›ï¼Œä»è€Œæé«˜å…¶æ—¶åºç†è§£çš„é€»è¾‘ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªé¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±æ³¨æ„åŠ›å¤´æ›´åŠ å…³æ³¨ä¸åŒæ—¶é—´æˆ³ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ—¶é—´åˆ†è¾¨ç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶æ˜¯åœ¨ç°æœ‰çš„è§†é¢‘è¯­è¨€æ¨¡å‹åŸºç¡€ä¸Šï¼Œå¢åŠ ä¸€ä¸ªæ—¶åºæ¡ä»¶æ³¨æ„åŠ›é”åŒ–(TCAS)æ¨¡å—ã€‚è¯¥æ¨¡å—ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä»è§†é¢‘ä¸­æå–è§†è§‰ç‰¹å¾ï¼›2) ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å°†è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾è¿›è¡Œèåˆï¼›3) è®¡ç®—æ³¨æ„åŠ›å¤´å¯¹ä¸åŒæ—¶é—´æˆ³è§†é¢‘ tokens çš„åŒºåˆ†åº¦ï¼›4) ä½¿ç”¨ TCAS æŸå¤±å‡½æ•°ä¼˜åŒ–æ¨¡å‹ï¼Œå¢å¼ºæ³¨æ„åŠ›å¤´çš„æ—¶é—´åˆ†è¾¨ç‡èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†æ—¶åºæ¡ä»¶æ³¨æ„åŠ›é”åŒ–(TCAS)æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ„å»ºä¸€ä¸ªåŸºäºæ³¨æ„åŠ›åŒºåˆ†çš„å¢å¼ºç›®æ ‡ï¼Œæ˜¾å¼åœ°æå‡æ¨¡å‹çš„æ—¶é—´åˆ†è¾¨ç‡èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTCAS æ›´åŠ å…³æ³¨æ¨¡å‹å†…éƒ¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›å¤´çš„è¡Œä¸ºæ¥æé«˜æ¨¡å‹çš„æ—¶åºç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šTCAS çš„å…³é”®è®¾è®¡åœ¨äº TCAS æŸå¤±å‡½æ•°ã€‚è¯¥æŸå¤±å‡½æ•°çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ³¨æ„åŠ›å¤´å¯¹ä¸åŒæ—¶é—´æˆ³è§†é¢‘ tokens çš„åŒºåˆ†åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªæ³¨æ„åŠ›å¤´ï¼Œè®¡ç®—å…¶å¯¹ä¸åŒæ—¶é—´æˆ³è§†é¢‘ tokens çš„æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥é¼“åŠ±è¿™äº›åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…è¿‡åº¦ä¼˜åŒ–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œé™åˆ¶æ³¨æ„åŠ›æƒé‡çš„å˜åŒ–å¹…åº¦ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„ TCAS æ–¹æ³•æ˜¾è‘—æé«˜äº†è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ—¶é—´é€»è¾‘ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æå‡ºçš„è¯„ä¼°æŒ‡æ ‡ä¸Šï¼ŒTCAS æ–¹æ³•ç›¸æ¯”åŸºçº¿æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼ŒTCAS æ–¹æ³•è¿˜åœ¨ä¸€èˆ¬çš„è§†é¢‘æ—¶åº grounding ä»»åŠ¡ä¸­å–å¾—äº†æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ—¶é—´é€»è¾‘ä¸€è‡´æ€§æ˜¯æ—¶é—´ç†è§£çš„ç“¶é¢ˆã€‚å¯è§£é‡Šæ€§åˆ†æè¡¨æ˜ï¼ŒTCAS æ–¹æ³•ç¡®å®æé«˜äº†æ³¨æ„åŠ›å¤´çš„æ—¶é—´å¯åŒºåˆ†æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½è§†é¢‘åˆ†æã€è§†é¢‘é—®ç­”ã€è§†é¢‘æ‘˜è¦ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ—¶åºç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°ç†è§£è§†é¢‘å†…å®¹ï¼Œä»è€Œä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®ã€æ›´å¯é çš„æœåŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§é¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯æ¥è¯†åˆ«å¼‚å¸¸è¡Œä¸ºï¼›åœ¨è§†é¢‘æœç´¢é¢†åŸŸï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯æ¥æé«˜æœç´¢ç»“æœçš„å‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) often generate self-contradictory outputs, which severely impacts their reliability and hinders their adoption in practical applications. In video-language models (Video-LLMs), this phenomenon recently draws the attention of researchers. Specifically, these models fail to provide logically consistent responses to rephrased questions based on their grounding outputs. However, the underlying causes of this phenomenon remain underexplored. In this work, we adopt an interpretability-driven approach to analyze, statistically summarize, and intervention the potential factors of the phenomenon. We find that one of the primary reasons for the inconsistency in responses lies in the inability of cross-modal attention heads to effectively distinguish video tokens across different timestamps. To address this, we propose an attention enhancement method called Temporally Conditioned Attention Sharpening (TCAS), which constructs an enhancement objective based on attention distinctions to enhance the model's temporal resolution capability, thereby improving its temporal understanding logic consistency. Experimental results demonstrate that our method significantly enhances the temporal logic consistency of Video-LLMs. Further interpretability analyses reveal that our method indeed improves the temporal discriminability of attention heads, validating our conclusions. Additionally, our method achieves performance improvements in general video temporal grounding tasks, highlighting that temporal logic consistency is a bottleneck in temporal understanding. By enhancing consistency, our method drives significant progress in video temporal understanding.

