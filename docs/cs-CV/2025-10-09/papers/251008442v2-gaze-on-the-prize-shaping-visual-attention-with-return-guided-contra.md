---
layout: default
title: Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning
---

# Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08442" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08442v2</a>
  <a href="https://arxiv.org/pdf/2510.08442.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08442v2" onclick="toggleFavorite(this, '2510.08442v2', 'Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09 (æ›´æ–°: 2025-12-12)

**å¤‡æ³¨**: Project page: https://andrewcwlee.github.io/gaze-on-the-prize

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå›æŠ¥å¼•å¯¼å¯¹æ¯”å­¦ä¹ çš„è§†è§‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ ·æœ¬æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰å¼ºåŒ–å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶` `å¯¹æ¯”å­¦ä¹ ` `å›æŠ¥å¼•å¯¼` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è§†è§‰å¼ºåŒ–å­¦ä¹ é¢ä¸´é«˜ç»´å›¾åƒæ•°æ®ä¸­ä»»åŠ¡ç›¸å…³åƒç´ å æ¯”å°çš„é—®é¢˜ï¼Œå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½å’Œå­¦ä¹ ä¸ç¨³å®šã€‚
2. è®ºæ–‡æå‡ºâ€œGaze on the Prizeâ€æ¡†æ¶ï¼Œåˆ©ç”¨å›æŠ¥å·®å¼‚å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œèšç„¦äºåŒºåˆ†æˆåŠŸä¸å¤±è´¥çš„å…³é”®ç‰¹å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸Šæå‡æ˜¾è‘—ï¼Œå¹¶èƒ½è§£å†³åŸºçº¿ç®—æ³•æ— æ³•è§£å†³çš„å¤æ‚ä»»åŠ¡ï¼Œæ— éœ€ä¿®æ”¹åº•å±‚ç®—æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“å¿…é¡»åŸºäºé«˜ç»´å›¾åƒæ•°æ®å­¦ä¹ è¡ŒåŠ¨ç­–ç•¥ï¼Œä½†åªæœ‰ä¸€å°éƒ¨åˆ†åƒç´ ä¸ä»»åŠ¡ç›¸å…³ã€‚è¿™å¯¼è‡´æ™ºèƒ½ä½“åœ¨ä¸ç›¸å…³ç‰¹å¾ä¸Šæµªè´¹æ¢ç´¢å’Œè®¡ç®—èµ„æºï¼Œé€ æˆæ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œå­¦ä¹ ä¸ç¨³å®šã€‚å—äººç±»è§†è§‰æ³¨è§†çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œGaze on the Prizeâ€æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„æ³¨è§†æ³¨æ„åŠ›æœºåˆ¶ï¼ˆGazeï¼‰å¢å¼ºè§†è§‰RLï¼Œè¯¥æœºåˆ¶ç”±æ¥è‡ªæ™ºèƒ½ä½“è¿½æ±‚æ›´é«˜å›æŠ¥ç»éªŒçš„è‡ªç›‘ç£ä¿¡å·ï¼ˆthe Prizeï¼‰å¼•å¯¼ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå›æŠ¥å·®å¼‚æ­ç¤ºäº†æœ€é‡è¦çš„ä¿¡æ¯ï¼šå¦‚æœä¸¤ä¸ªç›¸ä¼¼çš„è¡¨å¾äº§ç”Ÿä¸åŒçš„ç»“æœï¼Œé‚£ä¹ˆå®ƒä»¬ä¹‹é—´çš„åŒºåˆ«ç‰¹å¾å¾ˆå¯èƒ½ä¸ä»»åŠ¡ç›¸å…³ï¼Œæ³¨è§†åº”è¯¥ç›¸åº”åœ°å…³æ³¨å®ƒä»¬ã€‚è¿™æ˜¯é€šè¿‡å›æŠ¥å¼•å¯¼çš„å¯¹æ¯”å­¦ä¹ å®ç°çš„ï¼Œè¯¥å­¦ä¹ è®­ç»ƒæ³¨æ„åŠ›æœºåˆ¶ä»¥åŒºåˆ†ä¸æˆåŠŸå’Œå¤±è´¥ç›¸å…³çš„ç‰¹å¾ã€‚æˆ‘ä»¬æ ¹æ®å›æŠ¥å·®å¼‚å°†ç›¸ä¼¼çš„è§†è§‰è¡¨å¾åˆ†ç»„ä¸ºæ­£ä¾‹å’Œè´Ÿä¾‹ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆçš„æ ‡ç­¾æ„å»ºå¯¹æ¯”ä¸‰å…ƒç»„ã€‚è¿™äº›ä¸‰å…ƒç»„æä¾›äº†è®­ç»ƒä¿¡å·ï¼Œä½¿æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿä¸ºä¸ä¸åŒç»“æœç›¸å…³çš„çŠ¶æ€ç”Ÿæˆå¯åŒºåˆ†çš„è¡¨å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢æé«˜äº†é«˜è¾¾2.52å€ï¼Œå¹¶ä¸”å¯ä»¥è§£å†³ManiSkill3åŸºå‡†æµ‹è¯•ä¸­çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè€ŒåŸºçº¿ç®—æ³•æ— æ³•å­¦ä¹ ï¼Œä¸”æ— éœ€ä¿®æ”¹åº•å±‚ç®—æ³•æˆ–è¶…å‚æ•°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨å¤„ç†é«˜ç»´å›¾åƒè¾“å…¥æ—¶ï¼Œé¢ä¸´ç€å¤§é‡æ— å…³åƒç´ çš„å¹²æ‰°ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å¹³ç­‰åœ°å¤„ç†æ‰€æœ‰åƒç´ ï¼Œå¯¼è‡´æ™ºèƒ½ä½“æµªè´¹è®¡ç®—èµ„æºå’Œæ¢ç´¢æ—¶é—´åœ¨ä¸ç›¸å…³çš„ç‰¹å¾ä¸Šï¼Œä»è€Œé™ä½äº†æ ·æœ¬æ•ˆç‡ï¼Œä½¿å¾—å­¦ä¹ è¿‡ç¨‹ä¸ç¨³å®šï¼Œéš¾ä»¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ¨¡ä»¿äººç±»çš„è§†è§‰æ³¨è§†æœºåˆ¶ï¼Œè®©æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªåŠ¨åœ°å…³æ³¨å›¾åƒä¸­ä¸ä»»åŠ¡æœ€ç›¸å…³çš„åŒºåŸŸã€‚é€šè¿‡åˆ†ææ™ºèƒ½ä½“åœ¨ä¸åŒçŠ¶æ€ä¸‹è·å¾—çš„å›æŠ¥å·®å¼‚ï¼Œå¯ä»¥æ¨æ–­å‡ºå“ªäº›ç‰¹å¾å¯¹äºæˆåŠŸè‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œè®ºæ–‡åˆ©ç”¨å›æŠ¥å·®å¼‚æ¥å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤ŸåŒºåˆ†ä¸æˆåŠŸå’Œå¤±è´¥ç›¸å…³çš„ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä¸€ä¸ªè§†è§‰å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å’Œä¸€ä¸ªå¯å­¦ä¹ çš„æ³¨è§†æ³¨æ„åŠ›æœºåˆ¶ï¼ˆGazeï¼‰ã€‚æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†ç»éªŒæ•°æ®ã€‚Gazeæ¨¡å—æ¥æ”¶æ™ºèƒ½ä½“çš„è§†è§‰è¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ³¨æ„åŠ›æƒé‡å›¾ï¼Œç”¨äºçªå‡ºæ˜¾ç¤ºå›¾åƒä¸­é‡è¦çš„åŒºåŸŸã€‚ç„¶åï¼Œæ™ºèƒ½ä½“åŸºäºåŠ æƒåçš„è§†è§‰è¾“å…¥è¿›è¡Œå†³ç­–ã€‚Gazeæ¨¡å—é€šè¿‡å›æŠ¥å¼•å¯¼çš„å¯¹æ¯”å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»¥åŒºåˆ†ä¸ä¸åŒç»“æœç›¸å…³çš„çŠ¶æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å›æŠ¥å¼•å¯¼çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒè§†è§‰æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸ä¾èµ–äºäººå·¥æ ‡æ³¨æˆ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨æ™ºèƒ½ä½“è‡ªèº«çš„ç»éªŒæ•°æ®æ¥å­¦ä¹ ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼ŒGazeæ¨¡å—èƒ½å¤Ÿå­¦ä¹ åˆ°åŒºåˆ†æˆåŠŸå’Œå¤±è´¥çš„å…³é”®ç‰¹å¾ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“çš„æ ·æœ¬æ•ˆç‡å’Œå­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨å¯¹æ¯”ä¸‰å…ƒç»„æŸå¤±å‡½æ•°æ¥è®­ç»ƒGazeæ¨¡å—ã€‚ä¸‰å…ƒç»„ç”±ä¸€ä¸ªé”šç‚¹æ ·æœ¬ã€ä¸€ä¸ªæ­£ä¾‹æ ·æœ¬å’Œä¸€ä¸ªè´Ÿä¾‹æ ·æœ¬ç»„æˆã€‚é”šç‚¹æ ·æœ¬å’Œæ­£ä¾‹æ ·æœ¬æ˜¯å…·æœ‰ç›¸ä¼¼è§†è§‰è¡¨å¾ä½†äº§ç”Ÿä¸åŒå›æŠ¥çš„çŠ¶æ€ã€‚è´Ÿä¾‹æ ·æœ¬æ˜¯ä¸é”šç‚¹æ ·æœ¬å…·æœ‰ä¸åŒè§†è§‰è¡¨å¾çš„çŠ¶æ€ã€‚æŸå¤±å‡½æ•°çš„ç›®æ ‡æ˜¯ä½¿é”šç‚¹æ ·æœ¬å’Œæ­£ä¾‹æ ·æœ¬ä¹‹é—´çš„è·ç¦»å°äºé”šç‚¹æ ·æœ¬å’Œè´Ÿä¾‹æ ·æœ¬ä¹‹é—´çš„è·ç¦»ã€‚Gazeæ¨¡å—çš„ç½‘ç»œç»“æ„å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œé€‰æ‹©ï¼Œä¾‹å¦‚å¯ä»¥ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæˆ–Transformerã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ManiSkill3åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢ï¼Œè¯¥æ–¹æ³•æ¯”åŸºçº¿ç®—æ³•æé«˜äº†é«˜è¾¾2.52å€ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿè§£å†³åŸºçº¿ç®—æ³•æ— æ³•è§£å†³çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¾‹å¦‚éœ€è¦ç²¾ç¡®æ“ä½œçš„ä»»åŠ¡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼æ™ºèƒ½ä½“å…³æ³¨å…³é”®è§†è§‰ä¿¡æ¯ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ä»é«˜ç»´è§†è§‰è¾“å…¥ä¸­å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚é€šè¿‡å¼•å¯¼æ™ºèƒ½ä½“å…³æ³¨å…³é”®è§†è§‰ä¿¡æ¯ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ï¼Œé™ä½å¯¹å¤§é‡è®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œå¹¶ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.52x improvement in sample efficiency and can solve challenging tasks from the ManiSkill3 benchmark that the baseline fails to learn, without modifying the underlying algorithm or hyperparameters.

