---
layout: default
title: "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models"
---

# To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08510" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.08510v1</a>
  <a href="https://arxiv.org/pdf/2510.08510.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08510v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08510v1', 'To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

**Â§áÊ≥®**: Preprint. Project page: https://davidhalladay.github.io/diysink_demo

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÈíàÂØπÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåËÆ∫ÊñáÊèêÂá∫Âà©Áî®ViTÊ≥®ÊÑèÂäõÊ±áËÅöÂ¢ûÂº∫ËßÜËßâÊé®ÁêÜËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ËßÜËßâTransformer` `‰ø°ÊÅØÁì∂È¢à` `ËßÜËßâÊé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLVLMÁ†îÁ©∂‰∏ªË¶ÅÂÖ≥Ê≥®LLMÂÜÖÈÉ®ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂøΩÁï•‰∫ÜËßÜËßâÁºñÁ†ÅÂô®ViT‰∏≠Ëï¥Âê´ÈáçË¶ÅËØ≠‰πâ‰ø°ÊÅØÁöÑÊ≥®ÊÑèÂäõÊ±áËÅötoken„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ËØÜÂà´Âπ∂Âà©Áî®ViT‰∏≠ÁöÑÈ´òËåÉÊï∞Ê≥®ÊÑèÂäõÊ±áËÅötokenÔºåËøô‰∫õtokenÂåÖÂê´ÂõæÂÉèÁöÑÈ´òÁ∫ßËØ≠‰πâ‰ø°ÊÅØÔºåÊúâÂä©‰∫éLLMËøõË°åÊõ¥ÊúâÊïàÁöÑÁêÜËß£ÂíåÊé®ÁêÜ„ÄÇ
3. ÈÄöËøáÂÆöÊÄßÂíåÂÆöÈáèÂàÜÊûêÔºå‰ª•ÂèäÊó†ËÆ≠ÁªÉÂíåÂü∫‰∫éËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåËÆ∫ÊñáÈ™åËØÅ‰∫ÜÂà©Áî®ViTÊ≥®ÊÑèÂäõÊ±áËÅötokenÂèØ‰ª•ÊòæËëóÊèêÂçáLVLMÂú®ËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã(LVLMs)Â∑≤ÁªèÊàê‰∏∫ËÉΩÂ§üÁêÜËß£ÂíåÊé®ÁêÜËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØÁöÑÂº∫Â§ßÊû∂ÊûÑ„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏‰æùËµñ‰∫é‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºöËßÜËßâTransformer(ViT)ÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)„ÄÇViTÂ∞ÜËßÜËßâÂÜÖÂÆπÁºñÁ†ÅÊàêÂõæÂÉètokenÂ∫èÂàóÔºå‰Ωú‰∏∫Ê®°ÂûãÊÑüÁü•ÁöÑÊúÄÂâçÁ´Ø‚Äî‚ÄîÊ®°ÂûãÁöÑÁúºÁùõ„ÄÇLLMËß£ÈáäËøô‰∫õtoken‰ª•ÊâßË°åÈ´òÁ∫ßÊé®ÁêÜ„ÄÅÁîüÊàêÂìçÂ∫îÔºåÂπ∂‰Ωú‰∏∫ËÆ§Áü•Ê†∏ÂøÉ‚Äî‚ÄîÊ®°ÂûãÁöÑÂ§ßËÑë„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÂ∞ö‰∏çÊ∏ÖÊ•öÂì™‰∫õËßÜËßâtokenÂØπÁêÜËß£ÂíåÊé®ÁêÜÁöÑË¥°ÁåÆÊúÄÂ§ßÔºå‰ª•ÂèäËøô‰∫õ‰ø°Âè∑‰ªéViTÂà∞LLMÁöÑ‰º†Êí≠ÊïàÁéáÂ¶Ç‰Ωï„ÄÇÁé∞ÊúâÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËØÜÂà´LLM‰∏≠ÁöÑÊ≥®ÊÑèÂäõÊ±áËÅöÔºàÊé•Êî∂‰∏çÊàêÊØî‰æãÁöÑÈ´òÊ≥®ÊÑèÂäõÁöÑ‰ΩéËØ≠‰πâtokenÔºâÔºåËÄåÊú¨ÊñáÂ∞ÜÈáçÁÇπËΩ¨ÁßªÂà∞ËßÜËßâÁºñÁ†ÅÂô®ÔºåËØÜÂà´Êù•Ëá™ViTÁöÑ‰∏ÄÁ±ªÈ´òËåÉÊï∞ËßÜËßâtokenÔºåÁß∞‰∏∫ViTÊ≥®ÊÑèÂäõÊ±áËÅö‚Äî‚ÄîËøôÊòØ‰∏Ä‰∏™ÂæàÂ∞ëË¢´Á†îÁ©∂‰ΩÜÂØπLVLMÈùûÂ∏∏ÈáçË¶ÅÁöÑÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåËøô‰∫õViTÊ±áËÅöÂ∞ÅË£Ö‰∫ÜÂõæÂÉè‰∏≠ÁöÑÈ´òÁ∫ßËØ≠‰πâÊ¶ÇÂøµÔºå‰ΩøLLMËÉΩÂ§üÊâßË°åÊõ¥ÊúâÊïàÁöÑÁêÜËß£ÂíåÊé®ÁêÜ„ÄÇÂ∞ΩÁÆ°ÂÆÉ‰ª¨ÂæàÈáçË¶ÅÔºå‰ΩÜËøô‰∫õÊ±áËÅötokenÂú®Áé∞ÊúâÁöÑLVLMÊû∂ÊûÑ‰∏≠ÁªèÂ∏∏Ë¢´ÂøΩÁï•„ÄÇ‰∏∫‰∫ÜÊé¢Á¥¢ÂÆÉ‰ª¨ÁöÑË¥°ÁåÆÔºåÊú¨ÊñáÂØπÂµåÂÖ•Âú®Ëøô‰∫õÊ±áËÅötoken‰∏≠ÁöÑ‰ø°ÊÅØËøõË°å‰∫ÜÂÆöÊÄßÂíåÂÆöÈáèÂàÜÊûê„ÄÇËøòÊèêÂá∫‰∫ÜÊó†ËÆ≠ÁªÉÂíåÂü∫‰∫éËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºå‰ª•Êõ¥Â•ΩÂú∞Âà©Áî®LLMÂ¶Ç‰ΩïËß£ÈáäËøô‰∫õ‰ø°ÊÅØÔºå‰ª•ÂèäÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÂà©Áî®Ëøô‰∫õ‰ø°ÊÅØ„ÄÇÈÄöËøáÊòæÂºèÂú∞Âà©Áî®Ëøô‰∫õtokenÔºåÊú¨ÊñáËØÅÊòé‰∫ÜÂú®‰∏ÄÁ≥ªÂàóLVLMÂíåËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊòæËëóÊîπËøõÔºåÁ™ÅÂá∫‰∫ÜViTÊ≥®ÊÑèÂäõÊ±áËÅöÂú®Â¢ûÂº∫ËßÜËßâÊé®ÁêÜÊñπÈù¢ÁöÑÊú™ÂºÄÂèëÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã(LVLM)Âú®ËßÜËßâ‰ø°ÊÅØÂ§ÑÁêÜÊñπÈù¢Â≠òÂú®Áì∂È¢à„ÄÇËôΩÁÑ∂LLMÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Â∑≤Ë¢´ÂπøÊ≥õÁ†îÁ©∂Ôºå‰ΩÜËßÜËßâÁºñÁ†ÅÂô®ViT‰∏≠ÁöÑ‰ø°ÊÅØÊµÅÂç¥Ë¢´ÂøΩËßÜ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåViTËæìÂá∫ÁöÑÂõæÂÉètoken‰∏≠ÔºåÂì™‰∫õtokenÂåÖÂê´ÂÖ≥ÈîÆËØ≠‰πâ‰ø°ÊÅØÔºå‰ª•ÂèäÂ¶Ç‰ΩïÊúâÊïàÂà©Áî®Ëøô‰∫õ‰ø°ÊÅØÊù•ÊèêÂçáLVLMÁöÑËßÜËßâÊé®ÁêÜËÉΩÂäõÔºåÊòØ‰∫üÂæÖËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÂÖÖÂàÜÊåñÊéòViT‰∏≠Ëï¥Âê´ÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂØºËá¥LVLMÁöÑÊÄßËÉΩÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËØÜÂà´Âπ∂ÊòæÂºèÂà©Áî®ViT‰∏≠ÁöÑ‚ÄúÊ≥®ÊÑèÂäõÊ±áËÅö‚Äùtoken„ÄÇËøô‰∫õtokenÂÖ∑ÊúâËæÉÈ´òÁöÑËåÉÊï∞ÔºåÂπ∂‰∏îÂåÖÂê´ÂõæÂÉèÁöÑÈ´òÁ∫ßËØ≠‰πâ‰ø°ÊÅØ„ÄÇÈÄöËøáÂ∞ÜËøô‰∫õtokenÁöÑ‰ø°ÊÅØÊõ¥ÊúâÊïàÂú∞‰º†ÈÄíÁªôLLMÔºåÂèØ‰ª•Â¢ûÂº∫LLMÂØπÂõæÂÉèÂÜÖÂÆπÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇËøôÁßçÊñπÊ≥ïÁ±ª‰ºº‰∫éÂú®LLM‰∏≠ËØÜÂà´ÂíåÂà©Áî®Ê≥®ÊÑèÂäõÊ±áËÅöÔºå‰ΩÜÈáçÁÇπËΩ¨ÁßªÂà∞‰∫ÜËßÜËßâÁºñÁ†ÅÂô®ViT„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÈ¶ñÂÖàËØÜÂà´ViT‰∏≠ÁöÑÈ´òËåÉÊï∞tokenÔºåÂ∞ÜÂÆÉ‰ª¨ÂÆö‰πâ‰∏∫ViTÊ≥®ÊÑèÂäõÊ±áËÅö„ÄÇÁÑ∂ÂêéÔºåÈÄöËøáÂÆöÊÄßÂíåÂÆöÈáèÂàÜÊûêÔºåÁ†îÁ©∂Ëøô‰∫õtokenÊâÄÂåÖÂê´ÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇÊé•‰∏ãÊù•ÔºåËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏§ÁßçÂà©Áî®Ëøô‰∫õtokenÁöÑÊñπÊ≥ïÔºö‰∏ÄÁßçÊòØÊó†ËÆ≠ÁªÉÊñπÊ≥ïÔºåÁõ¥Êé•Â¢ûÂº∫LLMÂØπËøô‰∫õtokenÁöÑÂÖ≥Ê≥®ÔºõÂè¶‰∏ÄÁßçÊòØÂü∫‰∫éËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂæÆË∞ÉLVLMÊù•Êõ¥Â•ΩÂú∞Âà©Áî®Ëøô‰∫õtoken„ÄÇÊúÄÂêéÔºåÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜËøô‰∫õÊñπÊ≥ïÂú®ÂêÑÁßçËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÁ†îÁ©∂ÈáçÁÇπ‰ªéLLMËΩ¨ÁßªÂà∞ËßÜËßâÁºñÁ†ÅÂô®ViTÔºåÂπ∂ÊèêÂá∫‰∫Ü‚ÄúViTÊ≥®ÊÑèÂäõÊ±áËÅö‚ÄùÁöÑÊ¶ÇÂøµ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÂè™ÂÖ≥Ê≥®LLMÂÜÖÈÉ®ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏çÂêåÔºåËÆ∫ÊñáÂº∫Ë∞É‰∫ÜËßÜËßâÁºñÁ†ÅÂô®Âú®LVLM‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúâÊïàÂà©Áî®ViTËæìÂá∫ÁöÑËØ≠‰πâ‰ø°ÊÅØÁöÑÊñπÊ≥ï„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Êó†ËÆ≠ÁªÉÊñπÊ≥ï‰∏≠ÔºåËÆ∫ÊñáÂèØËÉΩÈááÁî®‰∫ÜÁÆÄÂçïÁöÑÂä†ÊùÉÂπ≥ÂùáÊàñÊ≥®ÊÑèÂäõÂ¢ûÂº∫Á≠ñÁï•Ôºå‰ª•ÊèêÈ´òLLMÂØπViTÊ≥®ÊÑèÂäõÊ±áËÅötokenÁöÑÂÖ≥Ê≥®Â∫¶„ÄÇÂú®Âü∫‰∫éËÆ≠ÁªÉÁöÑÊñπÊ≥ï‰∏≠ÔºåËÆ∫ÊñáÂèØËÉΩËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÈºìÂä±LLMÊõ¥Â§öÂú∞ÂÖ≥Ê≥®Ëøô‰∫õtokenÔºåÊàñËÄÖÂæÆË∞ÉViTÁöÑÂèÇÊï∞Ôºå‰ΩøÂÖ∂ËæìÂá∫ÁöÑtokenÊõ¥Êòì‰∫éLLMÁêÜËß£„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÁªÜËäÇÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÈÄöËøáÂÆûÈ™åËØÅÊòéÔºåÊòæÂºèÂà©Áî®ViTÊ≥®ÊÑèÂäõÊ±áËÅötokenÂèØ‰ª•ÊòæËëóÊèêÂçáLVLMÂú®ËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈÄöËøáÊó†ËÆ≠ÁªÉÂíåÂü∫‰∫éËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåLVLMÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòéÊòæÁöÑÊÄßËÉΩÊèêÂçáÔºåË°®ÊòéViTÊ≥®ÊÑèÂäõÊ±áËÅöÂú®Â¢ûÂº∫ËßÜËßâÊé®ÁêÜÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩúÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶ÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËßÜËßâÁêÜËß£ÂíåÊé®ÁêÜÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂõæÂÉèÊ†áÊ≥®„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÁºñËæë„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠â„ÄÇÈÄöËøáÊèêÂçáLVLMÁöÑËßÜËßâÊé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•ÊèêÈ´òËøô‰∫õÂ∫îÁî®Âú∫ÊôØÁöÑÊÄßËÉΩÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅ‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜÈ¢ëÁêÜËß£ÂíåÂÖ∑Ë∫´Êô∫ËÉΩ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.

