---
layout: default
title: BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities
---

# BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08759" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08759v1</a>
  <a href="https://arxiv.org/pdf/2510.08759.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08759v1" onclick="toggleFavorite(this, '2510.08759v1', 'BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Qi, Haibo Zhao, Ziyu Guo, Siyuan Ma, Ziyan Chen, Yaokun Han, Renrui Zhang, Zitiantao Lin, Shiji Xin, Yijian Huang, Kai Cheng, Peiheng Wang, Jiazheng Liu, Jiayi Zhang, Yizhe Zhu, Wenqing Wang, Yiran Qin, Xupeng Zhu, Haojie Huang, Lawson L. S. Wong

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BEARï¼šåŸå­å…·èº«èƒ½åŠ›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸å¢å¼º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å…·èº«æ™ºèƒ½` `åŸºå‡†æµ‹è¯•` `è§†è§‰æ„ŸçŸ¥` `3Dç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·èº«æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šé¢†åŸŸï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŸå­å…·èº«èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚
2. è®ºæ–‡æå‡ºBEARåŸºå‡†æµ‹è¯•ï¼Œå¹¶è®¾è®¡BEAR-Agentï¼Œé€šè¿‡é›†æˆé¢„è®­ç»ƒè§†è§‰æ¨¡å‹æ¥å¢å¼ºMLLMçš„æ„ŸçŸ¥ã€3Dç†è§£å’Œè§„åˆ’èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒBEAR-Agentæ˜¾è‘—æå‡äº†MLLMåœ¨BEARåŸºå‡†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«èƒ½åŠ›æ˜¯æŒ‡æ™ºèƒ½ä½“æ„ŸçŸ¥ã€ç†è§£å’Œä¸ç‰©ç†ä¸–ç•Œäº¤äº’çš„ä¸€ç³»åˆ—åŸºæœ¬èƒ½åŠ›ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å…·èº«æ™ºèƒ½ä½“æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¯¹å…¶å…·èº«èƒ½åŠ›çš„å…¨é¢å’Œç³»ç»Ÿè¯„ä¼°ä»æœªå……åˆ†æ¢ç´¢ï¼Œå› ä¸ºç°æœ‰åŸºå‡†ä¸»è¦ä¾§é‡äºè§„åˆ’æˆ–ç©ºé—´ç†è§£ç­‰ç‰¹å®šé¢†åŸŸã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BEARï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ã€ç»†ç²’åº¦çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MLLMåœ¨åŸå­å…·èº«èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚BEARåŒ…å«4469ä¸ªäº¤é”™çš„å›¾åƒ-è§†é¢‘-æ–‡æœ¬æ¡ç›®ï¼Œæ¶µç›–6ä¸ªç±»åˆ«ä¸­çš„14ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬ä»ä½çº§æŒ‡å‘ã€è½¨è¿¹ç†è§£ã€ç©ºé—´æ¨ç†åˆ°é«˜çº§è§„åˆ’çš„ä»»åŠ¡ã€‚å¯¹20ä¸ªä»£è¡¨æ€§MLLMçš„å¹¿æ³›è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå®ƒä»¬åœ¨å…·èº«èƒ½åŠ›çš„å„ä¸ªé¢†åŸŸéƒ½å­˜åœ¨æŒç»­çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†BEAR-Agentï¼Œä¸€ç§å¤šæ¨¡æ€å¯å¯¹è¯æ™ºèƒ½ä½“ï¼Œå®ƒé›†æˆäº†é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼Œä»¥å¢å¼ºMLLMçš„æ„ŸçŸ¥ã€3Dç†è§£å’Œè§„åˆ’èƒ½åŠ›ã€‚å®ƒæ˜¾è‘—æé«˜äº†MLLMåœ¨BEARä¸Šå„ç§å…·èº«èƒ½åŠ›ä¸Šçš„æ€§èƒ½ï¼Œè·å¾—äº†9.12%çš„ç»å¯¹å¢ç›Šå’ŒGPT-5ä¸Š17.5%çš„ç›¸å¯¹æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæé«˜MLLMçš„å…·èº«èƒ½åŠ›å¯ä»¥ä½¿æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å…·èº«ä»»åŠ¡å—ç›Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMåœ¨å…·èº«æ™ºèƒ½ä½“ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†ç¼ºä¹ä¸€ä¸ªå…¨é¢ç»†ç²’åº¦çš„åŸºå‡†æ¥è¯„ä¼°å…¶åŸå­å…·èº«èƒ½åŠ›ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€åªå…³æ³¨ç‰¹å®šé¢†åŸŸï¼Œå¦‚è§„åˆ’æˆ–ç©ºé—´ç†è§£ï¼Œæ— æ³•å……åˆ†åæ˜ MLLMåœ¨æ„ŸçŸ¥ã€ç†è§£å’Œäº¤äº’æ–¹é¢çš„ç»¼åˆèƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªæ›´å…¨é¢çš„åŸºå‡†æ¥æ­ç¤ºMLLMåœ¨å…·èº«èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æŒ‡å¯¼æ¨¡å‹æ”¹è¿›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•BEARï¼Œæ¶µç›–å¤šç§åŸå­å…·èº«èƒ½åŠ›ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªå¢å¼ºå‹æ™ºèƒ½ä½“BEAR-Agentï¼Œé€šè¿‡é›†æˆé¢„è®­ç»ƒè§†è§‰æ¨¡å‹æ¥æå‡MLLMçš„æ„ŸçŸ¥ã€3Dç†è§£å’Œè§„åˆ’èƒ½åŠ›ã€‚é€šè¿‡BEARåŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥ç³»ç»Ÿåœ°è¯„ä¼°MLLMçš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶åˆ©ç”¨BEAR-AgentéªŒè¯æ”¹è¿›æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBEAR-Agentçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼šç”¨äºå¢å¼ºMLLMçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ã€‚2) 3Dç†è§£æ¨¡å—ï¼šç”¨äºæå‡MLLMå¯¹ä¸‰ç»´åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œä¾‹å¦‚æ·±åº¦ä¼°è®¡ã€åœºæ™¯é‡å»ºç­‰ã€‚3) è§„åˆ’æ¨¡å—ï¼šç”¨äºå¢å¼ºMLLMçš„è§„åˆ’èƒ½åŠ›ï¼Œä¾‹å¦‚è·¯å¾„è§„åˆ’ã€åŠ¨ä½œåºåˆ—ç”Ÿæˆç­‰ã€‚è¿™äº›æ¨¡å—ä¸MLLMè¿›è¡Œé›†æˆï¼Œå½¢æˆä¸€ä¸ªå¯å¯¹è¯çš„æ™ºèƒ½ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸå­å…·èº«èƒ½åŠ›åŸºå‡†æµ‹è¯•BEARï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸå’Œä»»åŠ¡ã€‚2) è®¾è®¡äº†BEAR-Agentï¼Œé€šè¿‡é›†æˆé¢„è®­ç»ƒè§†è§‰æ¨¡å‹æ¥å¢å¼ºMLLMçš„æ„ŸçŸ¥ã€3Dç†è§£å’Œè§„åˆ’èƒ½åŠ›ã€‚3) å®éªŒç»“æœè¡¨æ˜ï¼ŒBEAR-Agentæ˜¾è‘—æå‡äº†MLLMåœ¨BEARåŸºå‡†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šBEARåŸºå‡†æµ‹è¯•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç»†ç²’åº¦çš„ä»»åŠ¡åˆ’åˆ†ï¼Œæ¶µç›–14ä¸ªé¢†åŸŸå’Œ6ä¸ªç±»åˆ«ã€‚2) å¤šæ¨¡æ€æ•°æ®è¾“å…¥ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬ã€‚3) å¤šæ ·åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ç­‰ã€‚BEAR-Agentçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼Œä¾‹å¦‚CLIPã€DINOç­‰ã€‚2) è®¾è®¡æœ‰æ•ˆçš„é›†æˆç­–ç•¥ï¼Œå°†è§†è§‰æ¨¡å‹ä¸MLLMè¿›è¡Œèåˆã€‚3) ä¼˜åŒ–è®­ç»ƒç›®æ ‡ï¼Œä½¿BEAR-Agentèƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå…·èº«ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBEAR-Agentåœ¨BEARåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè·å¾—äº†9.12%çš„ç»å¯¹å¢ç›Šå’ŒGPT-5ä¸Š17.5%çš„ç›¸å¯¹æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†æé«˜MLLMçš„å…·èº«èƒ½åŠ›å¯ä»¥ä½¿æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å…·èº«ä»»åŠ¡å—ç›Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒBEAR-Agentæ˜¯ä¸€ç§æœ‰æ•ˆçš„MLLMå…·èº«èƒ½åŠ›å¢å¼ºæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡MLLMçš„å…·èº«èƒ½åŠ›ï¼Œå¯ä»¥ä½¿æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£å’Œä¸ç‰©ç†ä¸–ç•Œäº¤äº’ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„åº”ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººé¢†åŸŸï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°å®Œæˆå¯¼èˆªã€æ“ä½œç­‰ä»»åŠ¡ï¼›åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¯ä»¥ä½¿è½¦è¾†æ›´å¥½åœ°æ„ŸçŸ¥å’Œç†è§£å‘¨å›´ç¯å¢ƒï¼›åœ¨è™šæ‹Ÿç°å®é¢†åŸŸï¼Œå¯ä»¥ä½¿è™šæ‹Ÿè§’è‰²æ›´é€¼çœŸåœ°ä¸ç”¨æˆ·äº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied capabilities refer to a suite of fundamental abilities for an agent to perceive, comprehend, and interact with the physical world. While multimodal large language models (MLLMs) show promise as embodied agents, a thorough and systematic evaluation of their embodied capabilities remains underexplored, as existing benchmarks primarily focus on specific domains such as planning or spatial understanding. To bridge this gap, we introduce BEAR, a comprehensive and fine-grained benchmark that evaluates MLLMs on atomic embodied capabilities. BEAR comprises 4,469 interleaved image-video-text entries across 14 domains in 6 categories, including tasks from low-level pointing, trajectory understanding, spatial reasoning, to high-level planning. Extensive evaluation results of 20 representative MLLMs reveal their persistent limitations across all domains of embodied capabilities. To tackle the shortfall, we propose BEAR-Agent, a multimodal conversable agent that integrates pretrained vision models to strengthen MLLM perception, 3D understanding, and planning capabilities. It substantially enhances MLLM performance across diverse embodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative improvement of 17.5% on GPT-5. Furthermore, our experiments indicate that improving MLLM embodied capabilities can benefit embodied tasks in simulated environments. Project website: https://bear-official66.github.io/

