---
layout: default
title: An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images
---

# An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.07817" target="_blank" class="toolbar-btn">arXiv: 2510.07817v1</a>
    <a href="https://arxiv.org/pdf/2510.07817.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07817v1" 
            onclick="toggleFavorite(this, '2510.07817v1', 'An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kanglin Ning, Ruzhao Chen, Penghong Wang, Xingtao Wang, Ruiqin Xiong, Xiaopeng Fan

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/emiyaning/RGCNet)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÂÆ§ÂÜÖÂÖ®ÊôØÂõæÂÉèÁöÑÁ´ØÂà∞Á´Ø„ÄÅÂü∫‰∫éÊàøÈó¥Âá†‰ΩïÁ∫¶ÊùüÁöÑÊ∑±Â∫¶‰º∞ËÆ°Ê°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶‰º∞ËÆ°` `ÂÆ§ÂÜÖÂÖ®ÊôØÂõæÂÉè` `ÊàøÈó¥Âá†‰ΩïÁ∫¶Êùü` `Â∏ÉÂ±ÄÈ¢ÑÊµã` `ËÉåÊôØÂàÜÂâ≤` `Á´ØÂà∞Á´ØÂ≠¶‰π†` `ËÆ°ÁÆóÊú∫ËßÜËßâ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ïÂú®Â§ÑÁêÜÂÆ§ÂÜÖÂÖ®ÊôØÂõæÂÉèÊó∂ÔºåÂ≠òÂú®ÊàøÈó¥ËßíËêΩËøáÂ∫¶Âπ≥ÊªëÂíåÂØπÂô™Â£∞ÊïèÊÑüÁöÑÈóÆÈ¢ò„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊàøÈó¥Âá†‰ΩïÁ∫¶ÊùüÁöÑÊ∑±Â∫¶‰º∞ËÆ°Ê°ÜÊû∂ÔºåÂà©Áî®Â∏ÉÂ±ÄÈ¢ÑÊµãÂíåËÉåÊôØÂàÜÂâ≤Êù•ÊèêÂçáÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ
3. Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊàøÈó¥Âá†‰ΩïÁ∫¶ÊùüÁöÑÊ∑±Â∫¶‰º∞ËÆ°Ê°ÜÊû∂ÔºåÁî®‰∫é‰ªéÂçïÁõÆ360¬∞ÂÆ§ÂÜÖÂÖ®ÊôØÂõæÂÉè‰∏≠È¢ÑÊµãÁêÉÈù¢ÂÉèÁ¥†Ê∑±Â∫¶„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æßÈáç‰∫éÂÉèÁ¥†Á∫ßÁ≤æÂ∫¶ÔºåÂØºËá¥ÊàøÈó¥ËßíËêΩËøáÂ∫¶Âπ≥ÊªëÂíåÂØπÂô™Â£∞ÊïèÊÑü„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂ∏ÉÂ±ÄÈ¢ÑÊµãÊèêÂèñÊàøÈó¥Âá†‰Ωï‰ø°ÊÅØÔºåÂπ∂ÈÄöËøáËÉåÊôØÂàÜÂâ≤Êú∫Âà∂Â∞ÜËøô‰∫õ‰ø°ÊÅØÈõÜÊàêÂà∞Ê∑±Â∫¶‰º∞ËÆ°ËøáÁ®ã‰∏≠„ÄÇÊ®°ÂûãÂ±ÇÈù¢ÔºåËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏Ä‰∏™ÂÖ±‰∫´ÁâπÂæÅÁºñÁ†ÅÂô®ÔºåÂêéÊé•Áî®‰∫éÂ∏ÉÂ±Ä‰º∞ËÆ°„ÄÅÊ∑±Â∫¶‰º∞ËÆ°ÂíåËÉåÊôØÂàÜÂâ≤ÁöÑÁâπÂÆö‰ªªÂä°Ëß£Á†ÅÂô®„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ËøòÂåÖÂê´‰∏§ÁßçÁ≠ñÁï•ÔºöÂü∫‰∫éÊàøÈó¥Âá†‰ΩïÁöÑËÉåÊôØÊ∑±Â∫¶Ëß£ÊûêÁ≠ñÁï•ÂíåËÉåÊôØÂàÜÂâ≤ÂºïÂØºÁöÑËûçÂêàÊú∫Âà∂„ÄÇÂú®Stanford2D3D„ÄÅMatterport3DÂíåStructured3DÊï∞ÊçÆÈõÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ï‰ºò‰∫éÁé∞ÊúâÂºÄÊ∫êÊñπÊ≥ï„ÄÇ‰ª£Á†ÅÂ∑≤ÂºÄÊ∫ê„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰ªéÂçïÁõÆ360¬∞ÂÆ§ÂÜÖÂÖ®ÊôØÂõæÂÉè‰∏≠ËøõË°åÁ≤æÁ°ÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®ÂÉèÁ¥†Á∫ßÂà´ÁöÑÁ≤æÂ∫¶ÔºåÂøΩÁï•‰∫ÜÂÆ§ÂÜÖÂú∫ÊôØÁöÑÁªìÊûÑÂåñ‰ø°ÊÅØÔºåÂØºËá¥‰º∞ËÆ°ÁöÑÊ∑±Â∫¶ÂõæÂú®ÊàøÈó¥ËßíËêΩÁ≠âÂå∫ÂüüÂá∫Áé∞ËøáÂ∫¶Âπ≥ÊªëÔºåÂπ∂‰∏îÂÆπÊòìÂèóÂà∞Âô™Â£∞ÁöÑÂΩ±Âìç„ÄÇËøô‰∫õÈóÆÈ¢òÈôêÂà∂‰∫ÜÊ∑±Â∫¶‰º∞ËÆ°Âú®ÂêéÁª≠‰∏âÁª¥ÈáçÂª∫„ÄÅÂØºËà™Á≠â‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÊàøÈó¥ÁöÑÂá†‰ΩïÁªìÊûÑ‰ø°ÊÅØÊù•Á∫¶ÊùüÊ∑±Â∫¶‰º∞ËÆ°ËøáÁ®ã„ÄÇÈÄöËøáÈ¢ÑÊµãÊàøÈó¥ÁöÑÂ∏ÉÂ±ÄÔºåÂèØ‰ª•Ëé∑ÂæóÊàøÈó¥ÁöÑÂ¢ôÂ£Å„ÄÅÂ§©Ëä±ÊùøÂíåÂú∞ÊùøÁ≠âÁªìÊûÑ‰ø°ÊÅØÔºåËøô‰∫õ‰ø°ÊÅØÂèØ‰ª•‰Ωú‰∏∫ÂÖàÈ™åÁü•ËØÜÊù•ÊåáÂØºÊ∑±Â∫¶‰º∞ËÆ°Ôºå‰ªéËÄåÊèêÈ´òÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇÂêåÊó∂ÔºåÈÄöËøáËÉåÊôØÂàÜÂâ≤ÔºåÂèØ‰ª•Â∞ÜÂú∫ÊôØ‰∏≠ÁöÑÁâ©‰Ωì‰∏éËÉåÊôØÂå∫ÂàÜÂºÄÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Â§ÑÁêÜÈÅÆÊå°ÂíåÂô™Â£∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏Ä‰∏™ÂÖ±‰∫´ÁâπÂæÅÁºñÁ†ÅÂô®Âíå‰∏â‰∏™ÁâπÂÆö‰ªªÂä°ÁöÑËß£Á†ÅÂô®ÔºåÂàÜÂà´Áî®‰∫éÂ∏ÉÂ±Ä‰º∞ËÆ°„ÄÅÊ∑±Â∫¶‰º∞ËÆ°ÂíåËÉåÊôØÂàÜÂâ≤„ÄÇÈ¶ñÂÖàÔºåÂÖ±‰∫´ÁºñÁ†ÅÂô®ÊèêÂèñÂ§öÂ∞∫Â∫¶ÁâπÂæÅ„ÄÇÁÑ∂ÂêéÔºå‰∏â‰∏™Ëß£Á†ÅÂô®ÂàÜÂà´ÁîüÊàêÂàùÂßãÁöÑÊ∑±Â∫¶Âõæ„ÄÅÊàøÈó¥Â∏ÉÂ±ÄÂõæÂíåËÉåÊôØÂàÜÂâ≤Âõæ„ÄÇÊé•‰∏ãÊù•ÔºåÂà©Áî®Âü∫‰∫éÊàøÈó¥Âá†‰ΩïÁöÑËÉåÊôØÊ∑±Â∫¶Ëß£ÊûêÁ≠ñÁï•ÔºåÊ†πÊçÆÊàøÈó¥Â∏ÉÂ±ÄÂíåÊ∑±Â∫¶Ëß£Á†ÅÂô®ÁöÑËæìÂá∫ÁîüÊàêËÉåÊôØÊ∑±Â∫¶Âõæ„ÄÇÊúÄÂêéÔºåÂà©Áî®ËÉåÊôØÂàÜÂâ≤ÂºïÂØºÁöÑËûçÂêàÁ≠ñÁï•ÔºåÊ†πÊçÆÂàÜÂâ≤Ëß£Á†ÅÂô®ÁöÑÈ¢ÑÊµãÁªìÊûúÔºå‰∏∫ËÉåÊôØÊ∑±Â∫¶ÂõæÂíåÂàùÂßãÊ∑±Â∫¶ÂõæÁîüÊàêËûçÂêàÊùÉÈáçÔºåÂæóÂà∞ÊúÄÁªàÁöÑÊ∑±Â∫¶Âõæ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÊàøÈó¥Âá†‰ΩïÁ∫¶ÊùüÊòæÂºèÂú∞ÂºïÂÖ•Âà∞Ê∑±Â∫¶‰º∞ËÆ°ËøáÁ®ã‰∏≠„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÂ∏ÉÂ±ÄÈ¢ÑÊµãÊù•Ëé∑ÂèñÊàøÈó¥ÁöÑÁªìÊûÑ‰ø°ÊÅØÔºåÂπ∂Âà©Áî®Ëøô‰∫õ‰ø°ÊÅØÊù•ÊåáÂØºÊ∑±Â∫¶‰º∞ËÆ°„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜÂü∫‰∫éÊàøÈó¥Âá†‰ΩïÁöÑËÉåÊôØÊ∑±Â∫¶Ëß£ÊûêÁ≠ñÁï•ÂíåËÉåÊôØÂàÜÂâ≤ÂºïÂØºÁöÑËûçÂêàÊú∫Âà∂ÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠ÔºåÂÖ±‰∫´ÁºñÁ†ÅÂô®ÈááÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÊèêÂèñÂ§öÂ∞∫Â∫¶ÁâπÂæÅ„ÄÇÂ∏ÉÂ±Ä‰º∞ËÆ°Ëß£Á†ÅÂô®È¢ÑÊµãÊàøÈó¥ÁöÑÂ∏ÉÂ±ÄÂõæÔºåÊ∑±Â∫¶‰º∞ËÆ°Ëß£Á†ÅÂô®È¢ÑÊµãÂàùÂßãÊ∑±Â∫¶ÂõæÔºåËÉåÊôØÂàÜÂâ≤Ëß£Á†ÅÂô®È¢ÑÊµãËÉåÊôØÂàÜÂâ≤Âõæ„ÄÇÂü∫‰∫éÊàøÈó¥Âá†‰ΩïÁöÑËÉåÊôØÊ∑±Â∫¶Ëß£ÊûêÁ≠ñÁï•Âà©Áî®ÊàøÈó¥Â∏ÉÂ±Ä‰ø°ÊÅØÔºåÂ∞ÜËÉåÊôØÂå∫ÂüüÁöÑÊ∑±Â∫¶ÂÄºËÆæÁΩÆ‰∏∫‰∏éÂ¢ôÂ£Å„ÄÅÂ§©Ëä±ÊùøÊàñÂú∞ÊùøÁöÑË∑ùÁ¶ª„ÄÇËÉåÊôØÂàÜÂâ≤ÂºïÂØºÁöÑËûçÂêàÁ≠ñÁï•‰ΩøÁî®ÂàÜÂâ≤Ëß£Á†ÅÂô®ÁöÑËæìÂá∫‰Ωú‰∏∫ÊùÉÈáçÔºåÂ∞ÜÂàùÂßãÊ∑±Â∫¶ÂõæÂíåËÉåÊôØÊ∑±Â∫¶ÂõæËøõË°åËûçÂêà„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨Ê∑±Â∫¶ÊçüÂ§±„ÄÅÂ∏ÉÂ±ÄÊçüÂ§±ÂíåÂàÜÂâ≤ÊçüÂ§±ÔºåÁî®‰∫éËÆ≠ÁªÉÊï¥‰∏™ÁΩëÁªú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Stanford2D3D„ÄÅMatterport3DÂíåStructured3DÊï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰∏éÁé∞ÊúâÂºÄÊ∫êÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞‰º∞ËÆ°ÊàøÈó¥ËßíËêΩÁöÑÊ∑±Â∫¶ÔºåÂπ∂‰∏îÂØπÂô™Â£∞ÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂÆ§ÂÜÖ‰∏âÁª¥ÈáçÂª∫„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÁ≤æÁ°ÆÁöÑÂÆ§ÂÜÖÊ∑±Â∫¶‰º∞ËÆ°ËÉΩÂ§üÂ∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºå‰ªéËÄåÂÆûÁé∞Ëá™‰∏ªÂØºËà™ÂíåÁâ©‰ΩìËØÜÂà´„ÄÇÂú®ËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÂ∫îÁî®‰∏≠ÔºåÂèØ‰ª•Êèê‰æõÊõ¥ÈÄºÁúüÁöÑÂú∫ÊôØÊ∏≤ÊüìÂíå‰∫§‰∫í‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØÁî®‰∫éÂÆ§ÂÜÖÂú∫ÊôØÁöÑËá™Âä®Âª∫Ê®°ÂíåÂèØËßÜÂåñ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Predicting spherical pixel depth from monocular $360^{\circ}$ indoor panoramas is critical for many vision applications. However, existing methods focus on pixel-level accuracy, causing oversmoothed room corners and noise sensitivity. In this paper, we propose a depth estimation framework based on room geometry constraints, which extracts room geometry information through layout prediction and integrates those information into the depth estimation process through background segmentation mechanism. At the model level, our framework comprises a shared feature encoder followed by task-specific decoders for layout estimation, depth estimation, and background segmentation. The shared encoder extracts multi-scale features, which are subsequently processed by individual decoders to generate initial predictions: a depth map, a room layout map, and a background segmentation map. Furthermore, our framework incorporates two strategies: a room geometry-based background depth resolving strategy and a background-segmentation-guided fusion mechanism. The proposed room-geometry-based background depth resolving strategy leverages the room layout and the depth decoder's output to generate the corresponding background depth map. Then, a background-segmentation-guided fusion strategy derives fusion weights for the background and coarse depth maps from the segmentation decoder's predictions. Extensive experimental results on the Stanford2D3D, Matterport3D and Structured3D datasets show that our proposed methods can achieve significantly superior performance than current open-source methods. Our code is available at https://github.com/emiyaning/RGCNet.

