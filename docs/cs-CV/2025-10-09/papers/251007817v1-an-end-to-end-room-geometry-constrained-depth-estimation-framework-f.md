---
layout: default
title: An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images
---

# An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07817" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07817v1</a>
  <a href="https://arxiv.org/pdf/2510.07817.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07817v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.07817v1', 'An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kanglin Ning, Ruzhao Chen, Penghong Wang, Xingtao Wang, Ruiqin Xiong, Xiaopeng Fan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/emiyaning/RGCNet)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å®¤å†…å…¨æ™¯å›¾åƒçš„ç«¯åˆ°ç«¯ã€åŸºäºæˆ¿é—´å‡ ä½•çº¦æŸçš„æ·±åº¦ä¼°è®¡æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æ·±åº¦ä¼°è®¡` `å®¤å†…å…¨æ™¯å›¾åƒ` `æˆ¿é—´å‡ ä½•çº¦æŸ` `å¸ƒå±€é¢„æµ‹` `èƒŒæ™¯åˆ†å‰²` `ç«¯åˆ°ç«¯å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨å¤„ç†å®¤å†…å…¨æ™¯å›¾åƒæ—¶ï¼Œå­˜åœ¨æˆ¿é—´è§’è½è¿‡åº¦å¹³æ»‘å’Œå¯¹å™ªå£°æ•æ„Ÿçš„é—®é¢˜ã€‚
2. è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæˆ¿é—´å‡ ä½•çº¦æŸçš„æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œåˆ©ç”¨å¸ƒå±€é¢„æµ‹å’ŒèƒŒæ™¯åˆ†å‰²æ¥æå‡æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæˆ¿é—´å‡ ä½•çº¦æŸçš„æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œç”¨äºä»å•ç›®360Â°å®¤å†…å…¨æ™¯å›¾åƒä¸­é¢„æµ‹çƒé¢åƒç´ æ·±åº¦ã€‚ç°æœ‰æ–¹æ³•ä¾§é‡äºåƒç´ çº§ç²¾åº¦ï¼Œå¯¼è‡´æˆ¿é—´è§’è½è¿‡åº¦å¹³æ»‘å’Œå¯¹å™ªå£°æ•æ„Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡å¸ƒå±€é¢„æµ‹æå–æˆ¿é—´å‡ ä½•ä¿¡æ¯ï¼Œå¹¶é€šè¿‡èƒŒæ™¯åˆ†å‰²æœºåˆ¶å°†è¿™äº›ä¿¡æ¯é›†æˆåˆ°æ·±åº¦ä¼°è®¡è¿‡ç¨‹ä¸­ã€‚æ¨¡å‹å±‚é¢ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªå…±äº«ç‰¹å¾ç¼–ç å™¨ï¼Œåæ¥ç”¨äºå¸ƒå±€ä¼°è®¡ã€æ·±åº¦ä¼°è®¡å’ŒèƒŒæ™¯åˆ†å‰²çš„ç‰¹å®šä»»åŠ¡è§£ç å™¨ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åŒ…å«ä¸¤ç§ç­–ç•¥ï¼šåŸºäºæˆ¿é—´å‡ ä½•çš„èƒŒæ™¯æ·±åº¦è§£æç­–ç•¥å’ŒèƒŒæ™¯åˆ†å‰²å¼•å¯¼çš„èåˆæœºåˆ¶ã€‚åœ¨Stanford2D3Dã€Matterport3Då’ŒStructured3Dæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰å¼€æºæ–¹æ³•ã€‚ä»£ç å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»å•ç›®360Â°å®¤å†…å…¨æ™¯å›¾åƒä¸­è¿›è¡Œç²¾ç¡®æ·±åº¦ä¼°è®¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨åƒç´ çº§åˆ«çš„ç²¾åº¦ï¼Œå¿½ç•¥äº†å®¤å†…åœºæ™¯çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¯¼è‡´ä¼°è®¡çš„æ·±åº¦å›¾åœ¨æˆ¿é—´è§’è½ç­‰åŒºåŸŸå‡ºç°è¿‡åº¦å¹³æ»‘ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°å™ªå£°çš„å½±å“ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†æ·±åº¦ä¼°è®¡åœ¨åç»­ä¸‰ç»´é‡å»ºã€å¯¼èˆªç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æˆ¿é—´çš„å‡ ä½•ç»“æ„ä¿¡æ¯æ¥çº¦æŸæ·±åº¦ä¼°è®¡è¿‡ç¨‹ã€‚é€šè¿‡é¢„æµ‹æˆ¿é—´çš„å¸ƒå±€ï¼Œå¯ä»¥è·å¾—æˆ¿é—´çš„å¢™å£ã€å¤©èŠ±æ¿å’Œåœ°æ¿ç­‰ç»“æ„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥ä½œä¸ºå…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼æ·±åº¦ä¼°è®¡ï¼Œä»è€Œæé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡èƒŒæ™¯åˆ†å‰²ï¼Œå¯ä»¥å°†åœºæ™¯ä¸­çš„ç‰©ä½“ä¸èƒŒæ™¯åŒºåˆ†å¼€ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†é®æŒ¡å’Œå™ªå£°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªå…±äº«ç‰¹å¾ç¼–ç å™¨å’Œä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„è§£ç å™¨ï¼Œåˆ†åˆ«ç”¨äºå¸ƒå±€ä¼°è®¡ã€æ·±åº¦ä¼°è®¡å’ŒèƒŒæ™¯åˆ†å‰²ã€‚é¦–å…ˆï¼Œå…±äº«ç¼–ç å™¨æå–å¤šå°ºåº¦ç‰¹å¾ã€‚ç„¶åï¼Œä¸‰ä¸ªè§£ç å™¨åˆ†åˆ«ç”Ÿæˆåˆå§‹çš„æ·±åº¦å›¾ã€æˆ¿é—´å¸ƒå±€å›¾å’ŒèƒŒæ™¯åˆ†å‰²å›¾ã€‚æ¥ä¸‹æ¥ï¼Œåˆ©ç”¨åŸºäºæˆ¿é—´å‡ ä½•çš„èƒŒæ™¯æ·±åº¦è§£æç­–ç•¥ï¼Œæ ¹æ®æˆ¿é—´å¸ƒå±€å’Œæ·±åº¦è§£ç å™¨çš„è¾“å‡ºç”ŸæˆèƒŒæ™¯æ·±åº¦å›¾ã€‚æœ€åï¼Œåˆ©ç”¨èƒŒæ™¯åˆ†å‰²å¼•å¯¼çš„èåˆç­–ç•¥ï¼Œæ ¹æ®åˆ†å‰²è§£ç å™¨çš„é¢„æµ‹ç»“æœï¼Œä¸ºèƒŒæ™¯æ·±åº¦å›¾å’Œåˆå§‹æ·±åº¦å›¾ç”Ÿæˆèåˆæƒé‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ·±åº¦å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æˆ¿é—´å‡ ä½•çº¦æŸæ˜¾å¼åœ°å¼•å…¥åˆ°æ·±åº¦ä¼°è®¡è¿‡ç¨‹ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å¸ƒå±€é¢„æµ‹æ¥è·å–æˆ¿é—´çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥æŒ‡å¯¼æ·±åº¦ä¼°è®¡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†åŸºäºæˆ¿é—´å‡ ä½•çš„èƒŒæ™¯æ·±åº¦è§£æç­–ç•¥å’ŒèƒŒæ™¯åˆ†å‰²å¼•å¯¼çš„èåˆæœºåˆ¶ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ï¼Œå…±äº«ç¼–ç å™¨é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œæå–å¤šå°ºåº¦ç‰¹å¾ã€‚å¸ƒå±€ä¼°è®¡è§£ç å™¨é¢„æµ‹æˆ¿é—´çš„å¸ƒå±€å›¾ï¼Œæ·±åº¦ä¼°è®¡è§£ç å™¨é¢„æµ‹åˆå§‹æ·±åº¦å›¾ï¼ŒèƒŒæ™¯åˆ†å‰²è§£ç å™¨é¢„æµ‹èƒŒæ™¯åˆ†å‰²å›¾ã€‚åŸºäºæˆ¿é—´å‡ ä½•çš„èƒŒæ™¯æ·±åº¦è§£æç­–ç•¥åˆ©ç”¨æˆ¿é—´å¸ƒå±€ä¿¡æ¯ï¼Œå°†èƒŒæ™¯åŒºåŸŸçš„æ·±åº¦å€¼è®¾ç½®ä¸ºä¸å¢™å£ã€å¤©èŠ±æ¿æˆ–åœ°æ¿çš„è·ç¦»ã€‚èƒŒæ™¯åˆ†å‰²å¼•å¯¼çš„èåˆç­–ç•¥ä½¿ç”¨åˆ†å‰²è§£ç å™¨çš„è¾“å‡ºä½œä¸ºæƒé‡ï¼Œå°†åˆå§‹æ·±åº¦å›¾å’ŒèƒŒæ™¯æ·±åº¦å›¾è¿›è¡Œèåˆã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ·±åº¦æŸå¤±ã€å¸ƒå±€æŸå¤±å’Œåˆ†å‰²æŸå¤±ï¼Œç”¨äºè®­ç»ƒæ•´ä¸ªç½‘ç»œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Stanford2D3Dã€Matterport3Då’ŒStructured3Dæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰å¼€æºæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¼°è®¡æˆ¿é—´è§’è½çš„æ·±åº¦ï¼Œå¹¶ä¸”å¯¹å™ªå£°å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå®¤å†…ä¸‰ç»´é‡å»ºã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ç²¾ç¡®çš„å®¤å†…æ·±åº¦ä¼°è®¡èƒ½å¤Ÿå¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°è‡ªä¸»å¯¼èˆªå’Œç‰©ä½“è¯†åˆ«ã€‚åœ¨è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®åº”ç”¨ä¸­ï¼Œå¯ä»¥æä¾›æ›´é€¼çœŸçš„åœºæ™¯æ¸²æŸ“å’Œäº¤äº’ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºå®¤å†…åœºæ™¯çš„è‡ªåŠ¨å»ºæ¨¡å’Œå¯è§†åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Predicting spherical pixel depth from monocular $360^{\circ}$ indoor panoramas is critical for many vision applications. However, existing methods focus on pixel-level accuracy, causing oversmoothed room corners and noise sensitivity. In this paper, we propose a depth estimation framework based on room geometry constraints, which extracts room geometry information through layout prediction and integrates those information into the depth estimation process through background segmentation mechanism. At the model level, our framework comprises a shared feature encoder followed by task-specific decoders for layout estimation, depth estimation, and background segmentation. The shared encoder extracts multi-scale features, which are subsequently processed by individual decoders to generate initial predictions: a depth map, a room layout map, and a background segmentation map. Furthermore, our framework incorporates two strategies: a room geometry-based background depth resolving strategy and a background-segmentation-guided fusion mechanism. The proposed room-geometry-based background depth resolving strategy leverages the room layout and the depth decoder's output to generate the corresponding background depth map. Then, a background-segmentation-guided fusion strategy derives fusion weights for the background and coarse depth maps from the segmentation decoder's predictions. Extensive experimental results on the Stanford2D3D, Matterport3D and Structured3D datasets show that our proposed methods can achieve significantly superior performance than current open-source methods. Our code is available at https://github.com/emiyaning/RGCNet.

