---
layout: default
title: Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection
---

# Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08073" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08073v1</a>
  <a href="https://arxiv.org/pdf/2510.08073.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08073v1" onclick="toggleFavorite(this, '2510.08073v1', 'Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuhai Zhang, ZiHao Lian, Jiahao Yang, Daiyuan Li, Guoxuan Pang, Feng Liu, Bo Han, Shutao Li, Mingkui Tan

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Accepted at NeurIPS 2025 spotlight

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ZSHsh98/NSG-VD)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç‰©ç†é©±åŠ¨çš„æ—¶ç©ºå»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆè§†é¢‘**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `AIç”Ÿæˆè§†é¢‘æ£€æµ‹` `ç‰©ç†é©±åŠ¨å»ºæ¨¡` `æ—¶ç©ºæ¢¯åº¦` `æ¦‚ç‡æµå®ˆæ’` `æ‰©æ•£æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰AIç”Ÿæˆè§†é¢‘æ£€æµ‹æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡é«˜ç»´æ—¶ç©ºåŠ¨æ€ï¼Œä¸”éš¾ä»¥æ•æ‰è¿åç‰©ç†å®šå¾‹çš„ç»†å¾®å¼‚å¸¸ã€‚
2. è®ºæ–‡æå‡ºåŸºäºæ¦‚ç‡æµå®ˆæ’åŸç†çš„ç‰©ç†é©±åŠ¨æ–¹æ³•ï¼Œé€šè¿‡å½’ä¸€åŒ–æ—¶ç©ºæ¢¯åº¦ï¼ˆNSGï¼‰æ˜¾å¼æ•è·è§†é¢‘åŠ¨æ€åå·®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒNSG-VDåœ¨å¬å›ç‡å’ŒF1-Scoreä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶å“è¶Šçš„æ£€æµ‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

AIç”Ÿæˆçš„è§†é¢‘å·²ç»è¾¾åˆ°äº†è¿‘ä¹å®Œç¾çš„è§†è§‰çœŸå®åº¦ï¼ˆä¾‹å¦‚Soraï¼‰ï¼Œå› æ­¤è¿«åˆ‡éœ€è¦å¯é çš„æ£€æµ‹æœºåˆ¶ã€‚ç„¶è€Œï¼Œæ£€æµ‹æ­¤ç±»è§†é¢‘é¢ä¸´ç€å»ºæ¨¡é«˜ç»´æ—¶ç©ºåŠ¨æ€ä»¥åŠè¯†åˆ«è¿åç‰©ç†å®šå¾‹çš„ç»†å¾®å¼‚å¸¸çš„é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡æµå®ˆæ’åŸç†çš„ç‰©ç†é©±åŠ¨çš„AIç”Ÿæˆè§†é¢‘æ£€æµ‹èŒƒä¾‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå½’ä¸€åŒ–æ—¶ç©ºæ¢¯åº¦ï¼ˆNSGï¼‰çš„ç»Ÿè®¡é‡ï¼Œå®ƒé‡åŒ–äº†ç©ºé—´æ¦‚ç‡æ¢¯åº¦ä¸æ—¶é—´å¯†åº¦å˜åŒ–çš„æ¯”ç‡ï¼Œä»è€Œæ˜¾å¼åœ°æ•è·äº†ä¸è‡ªç„¶è§†é¢‘åŠ¨æ€çš„åå·®ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§NSGä¼°è®¡å™¨ï¼Œé€šè¿‡ç©ºé—´æ¢¯åº¦è¿‘ä¼¼å’Œè¿åŠ¨æ„ŸçŸ¥çš„æ—¶é—´å»ºæ¨¡ï¼Œæ— éœ€å¤æ‚çš„è¿åŠ¨åˆ†è§£ï¼ŒåŒæ—¶ä¿ç•™äº†ç‰©ç†çº¦æŸã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºNSGçš„è§†é¢‘æ£€æµ‹æ–¹æ³•ï¼ˆNSG-VDï¼‰ï¼Œè¯¥æ–¹æ³•è®¡ç®—æµ‹è¯•è§†é¢‘å’ŒçœŸå®è§†é¢‘çš„NSGç‰¹å¾ä¹‹é—´çš„æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰ä½œä¸ºæ£€æµ‹æŒ‡æ ‡ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å¯¼äº†çœŸå®è§†é¢‘å’Œç”Ÿæˆè§†é¢‘ä¹‹é—´NSGç‰¹å¾è·ç¦»çš„ä¸Šé™ï¼Œè¯æ˜äº†ç”±äºåˆ†å¸ƒåç§»ï¼Œç”Ÿæˆè§†é¢‘è¡¨ç°å‡ºæ”¾å¤§çš„å·®å¼‚ã€‚å¤§é‡å®éªŒè¯å®ï¼ŒNSG-VDåœ¨å¬å›ç‡ä¸Šæ¯”æœ€å…ˆè¿›çš„åŸºçº¿é«˜å‡º16.00%ï¼Œåœ¨F1-Scoreä¸Šé«˜å‡º10.75%ï¼ŒéªŒè¯äº†NSG-VDçš„å“è¶Šæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰AIç”Ÿæˆè§†é¢‘æ£€æµ‹æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡è§†é¢‘ä¸­çš„é«˜ç»´æ—¶ç©ºåŠ¨æ€ï¼Œå¹¶ä¸”éš¾ä»¥æ•æ‰åˆ°è¿åç‰©ç†å®šå¾‹çš„ç»†å¾®å¼‚å¸¸ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå­¦ä¹ è§†é¢‘ä¸­çš„ç»Ÿè®¡ç‰¹å¾ï¼Œä½†ç¼ºä¹å¯¹è§†é¢‘å†…åœ¨ç‰©ç†è§„å¾‹çš„çº¦æŸï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¦‚ç‡æµå®ˆæ’åŸç†ï¼Œå°†è§†é¢‘è§†ä¸ºæ¦‚ç‡å¯†åº¦éšæ—¶é—´æ¼”åŒ–çš„è¿‡ç¨‹ã€‚é€šè¿‡é‡åŒ–ç©ºé—´æ¦‚ç‡æ¢¯åº¦ä¸æ—¶é—´å¯†åº¦å˜åŒ–çš„æ¯”ç‡ï¼Œå³å½’ä¸€åŒ–æ—¶ç©ºæ¢¯åº¦ï¼ˆNSGï¼‰ï¼Œå¯ä»¥æ˜¾å¼åœ°æ•è·AIç”Ÿæˆè§†é¢‘ä¸­ä¸è‡ªç„¶è§†é¢‘åŠ¨æ€çš„åå·®ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ£€æµ‹å‡ºè¿åç‰©ç†è§„å¾‹çš„å¼‚å¸¸ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNSG-VDæ–¹æ³•çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–è§†é¢‘çš„ç©ºé—´æ¢¯åº¦ä¿¡æ¯ï¼›2) é€šè¿‡è¿åŠ¨æ„ŸçŸ¥çš„æ—¶é—´å»ºæ¨¡ï¼Œä¼°è®¡è§†é¢‘çš„æ—¶é—´å¯†åº¦å˜åŒ–ï¼›3) è®¡ç®—å½’ä¸€åŒ–æ—¶ç©ºæ¢¯åº¦ï¼ˆNSGï¼‰ç‰¹å¾ï¼›4) ä½¿ç”¨æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰åº¦é‡æµ‹è¯•è§†é¢‘å’ŒçœŸå®è§†é¢‘çš„NSGç‰¹å¾ä¹‹é—´çš„å·®å¼‚ï¼Œä½œä¸ºæ£€æµ‹æŒ‡æ ‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å½’ä¸€åŒ–æ—¶ç©ºæ¢¯åº¦ï¼ˆNSGï¼‰è¿™ä¸€ç»Ÿè®¡é‡ï¼Œå®ƒèƒ½å¤Ÿæ˜¾å¼åœ°æ•è·AIç”Ÿæˆè§†é¢‘ä¸­è¿åç‰©ç†è§„å¾‹çš„åŠ¨æ€åå·®ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒNSG-VDæ–¹æ³•æ›´åŠ å…³æ³¨è§†é¢‘å†…åœ¨çš„ç‰©ç†è§„å¾‹ï¼Œä»è€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ£€æµ‹AIç”Ÿæˆè§†é¢‘ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œç©ºé—´æ¢¯åº¦è¿‘ä¼¼ï¼Œé¿å…äº†å¤æ‚çš„è¿åŠ¨åˆ†è§£ï¼ŒåŒæ—¶ä¿ç•™äº†ç‰©ç†çº¦æŸã€‚

**å…³é”®è®¾è®¡**ï¼šNSGçš„è®¡ç®—å…¬å¼ä¸ºç©ºé—´æ¦‚ç‡æ¢¯åº¦ä¸æ—¶é—´å¯†åº¦å˜åŒ–çš„æ¯”ç‡ã€‚ç©ºé—´æ¦‚ç‡æ¢¯åº¦é€šè¿‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¼°è®¡ï¼Œæ—¶é—´å¯†åº¦å˜åŒ–é€šè¿‡è¿åŠ¨æ„ŸçŸ¥çš„æ—¶é—´å»ºæ¨¡å®ç°ã€‚MMDåº¦é‡ç”¨äºè®¡ç®—æµ‹è¯•è§†é¢‘å’ŒçœŸå®è§†é¢‘çš„NSGç‰¹å¾ä¹‹é—´çš„å·®å¼‚ï¼Œä½œä¸ºæ£€æµ‹æŒ‡æ ‡ã€‚è®ºæ–‡è¿˜æ¨å¯¼äº†çœŸå®è§†é¢‘å’Œç”Ÿæˆè§†é¢‘ä¹‹é—´NSGç‰¹å¾è·ç¦»çš„ä¸Šé™ï¼Œè¯æ˜äº†ç”Ÿæˆè§†é¢‘ç”±äºåˆ†å¸ƒåç§»ä¼šè¡¨ç°å‡ºæ”¾å¤§çš„å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNSG-VDæ–¹æ³•åœ¨AIç”Ÿæˆè§†é¢‘æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨RecallæŒ‡æ ‡ä¸Šï¼ŒNSG-VDæ¯”æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•é«˜å‡º16.00%ï¼Œåœ¨F1-ScoreæŒ‡æ ‡ä¸Šé«˜å‡º10.75%ã€‚è¿™äº›ç»“æœéªŒè¯äº†NSG-VDæ–¹æ³•åœ¨æ£€æµ‹AIç”Ÿæˆè§†é¢‘æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå†…å®¹å®‰å…¨é¢†åŸŸï¼Œä¾‹å¦‚æ£€æµ‹å’Œè¯†åˆ«æ·±åº¦ä¼ªé€ è§†é¢‘ï¼Œé˜²æ­¢è™šå‡ä¿¡æ¯çš„ä¼ æ’­ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ç”¨äºè§†é¢‘ç›‘æ§ã€æ™ºèƒ½äº¤é€šç­‰é¢†åŸŸï¼Œç”¨äºæ£€æµ‹å¼‚å¸¸äº‹ä»¶æˆ–è¡Œä¸ºï¼Œæé«˜ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„AIç”Ÿæˆå†…å®¹æ£€æµ‹ï¼Œä¾‹å¦‚å›¾åƒã€éŸ³é¢‘ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> AI-generated videos have achieved near-perfect visual realism (e.g., Sora), urgently necessitating reliable detection mechanisms. However, detecting such videos faces significant challenges in modeling high-dimensional spatiotemporal dynamics and identifying subtle anomalies that violate physical laws. In this paper, we propose a physics-driven AI-generated video detection paradigm based on probability flow conservation principles. Specifically, we propose a statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the ratio of spatial probability gradients to temporal density changes, explicitly capturing deviations from natural video dynamics. Leveraging pre-trained diffusion models, we develop an NSG estimator through spatial gradients approximation and motion-aware temporal modeling without complex motion decomposition while preserving physical constraints. Building on this, we propose an NSG-based video detection method (NSG-VD) that computes the Maximum Mean Discrepancy (MMD) between NSG features of the test and real videos as a detection metric. Last, we derive an upper bound of NSG feature distances between real and generated videos, proving that generated videos exhibit amplified discrepancies due to distributional shifts. Extensive experiments confirm that NSG-VD outperforms state-of-the-art baselines by 16.00% in Recall and 10.75% in F1-Score, validating the superior performance of NSG-VD. The source code is available at https://github.com/ZSHsh98/NSG-VD.

