---
layout: default
title: PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment
---

# PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07636" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07636v1</a>
  <a href="https://arxiv.org/pdf/2510.07636.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07636v1" onclick="toggleFavorite(this, '2510.07636v1', 'PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shashank Gupta, Gregoire Phillips, Alan C. Bovik

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Oral presentation at ICIP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPIT-QMMï¼Œä¸€ç§ç”¨äºæ— å‚è€ƒç‚¹äº‘è´¨é‡è¯„ä¼°çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç‚¹äº‘è´¨é‡è¯„ä¼°` `æ— å‚è€ƒè´¨é‡è¯„ä¼°` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹å¤šæ¨¡æ€æ¨¡å‹` `3Dè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— å‚è€ƒç‚¹äº‘è´¨é‡è¯„ä¼°æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¯¼è‡´è¯„ä¼°ç²¾åº¦å—é™ã€‚
2. PIT-QMMé€šè¿‡èåˆæ–‡æœ¬æè¿°ã€2DæŠ•å½±å’Œ3Dç‚¹äº‘è§†å›¾ï¼Œå®ç°æ›´å…¨é¢çš„è´¨é‡è¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPIT-QMMåœ¨åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å…·å¤‡å¤±çœŸå®šä½èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)æœ€è¿‘åœ¨å›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™ç§è¿›æ­¥å°šæœªåœ¨3Dèµ„äº§é¢†åŸŸå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æœ‰å…´è¶£ä½¿ç”¨è¿™äº›æ¨¡å‹è¿›è¡Œæ— å‚è€ƒç‚¹äº‘è´¨é‡è¯„ä¼°(NR-PCQA)ï¼Œå…¶ç›®æ ‡æ˜¯åœ¨æ²¡æœ‰å‚è€ƒçš„æƒ…å†µä¸‹è‡ªåŠ¨è¯„ä¼°ç‚¹äº‘çš„æ„ŸçŸ¥è´¨é‡ã€‚æˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°ï¼Œä¸åŒçš„æ•°æ®æ¨¡æ€â€”â€”æ–‡æœ¬æè¿°ã€2DæŠ•å½±å’Œ3Dç‚¹äº‘è§†å›¾â€”â€”æä¾›äº†å…³äºç‚¹äº‘è´¨é‡çš„äº’è¡¥ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºäº†PIT-QMMï¼Œä¸€ç§ç”¨äºNR-PCQAçš„æ–°å‹LMMï¼Œå®ƒèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°ä½¿ç”¨æ–‡æœ¬ã€å›¾åƒå’Œç‚¹äº‘æ¥é¢„æµ‹è´¨é‡åˆ†æ•°ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æµè¡Œçš„åŸºå‡†æµ‹è¯•ä¸­ä»¥æ˜¾è‘—çš„ä¼˜åŠ¿ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”è®­ç»ƒè¿­ä»£æ¬¡æ•°æ›´å°‘ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿè¿›è¡Œå¤±çœŸå®šä½å’Œè¯†åˆ«ï¼Œè¿™ä¸ºæ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œäº¤äº’æ€§å¼€è¾Ÿäº†ä¸€æ¡æ–°çš„é“è·¯ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨https://www.github.com/shngt/pit-qmmè·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ— å‚è€ƒç‚¹äº‘è´¨é‡è¯„ä¼°ï¼ˆNR-PCQAï¼‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„ç‰¹å¾æˆ–æµ…å±‚å­¦ä¹ æ¨¡å‹ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨ç‚¹äº‘æ•°æ®çš„å¤æ‚æ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¯¼è‡´è¯„ä¼°ç²¾åº¦ä¸é«˜ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥å®šä½å’Œè¯†åˆ«ç‚¹äº‘ä¸­çš„å¤±çœŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¼ºå¤§è¡¨å¾å­¦ä¹ èƒ½åŠ›ï¼Œå°†æ–‡æœ¬æè¿°ã€2DæŠ•å½±å’Œ3Dç‚¹äº‘è§†å›¾ç­‰å¤šæ¨¡æ€ä¿¡æ¯èåˆèµ·æ¥ï¼Œä»è€Œæ›´å…¨é¢ã€å‡†ç¡®åœ°è¯„ä¼°ç‚¹äº‘çš„è´¨é‡ã€‚é€šè¿‡ç«¯åˆ°ç«¯çš„å­¦ä¹ æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨æå–ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”æ€§ï¼Œå¹¶å­¦ä¹ åˆ°æ›´é²æ£’çš„è´¨é‡è¯„ä¼°ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPIT-QMMçš„æ•´ä½“æ¶æ„åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ–‡æœ¬ç¼–ç å™¨ã€å›¾åƒç¼–ç å™¨å’Œç‚¹äº‘ç¼–ç å™¨ã€‚æ–‡æœ¬ç¼–ç å™¨è´Ÿè´£æå–æ–‡æœ¬æè¿°çš„è¯­ä¹‰ç‰¹å¾ï¼Œå›¾åƒç¼–ç å™¨è´Ÿè´£æå–2DæŠ•å½±çš„è§†è§‰ç‰¹å¾ï¼Œç‚¹äº‘ç¼–ç å™¨è´Ÿè´£æå–3Dç‚¹äº‘çš„å‡ ä½•ç‰¹å¾ã€‚ç„¶åï¼Œä¸€ä¸ªå¤šæ¨¡æ€èåˆæ¨¡å—å°†è¿™ä¸‰ç§ç‰¹å¾èåˆèµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªç»Ÿä¸€çš„è´¨é‡è¡¨å¾ã€‚æœ€åï¼Œä¸€ä¸ªå›å½’æ¨¡å—å°†è¯¥è¡¨å¾æ˜ å°„åˆ°è´¨é‡åˆ†æ•°ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹å¼ï¼Œé€šè¿‡æœ€å°åŒ–é¢„æµ‹è´¨é‡åˆ†æ•°ä¸çœŸå®è´¨é‡åˆ†æ•°ä¹‹é—´çš„å·®å¼‚æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šPIT-QMMçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€èåˆç­–ç•¥å’Œç«¯åˆ°ç«¯çš„å­¦ä¹ æ–¹å¼ã€‚é€šè¿‡å°†æ–‡æœ¬ã€å›¾åƒå’Œç‚¹äº‘ä¸‰ç§æ¨¡æ€çš„ä¿¡æ¯èåˆèµ·æ¥ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£ç‚¹äº‘çš„è´¨é‡ã€‚ç«¯åˆ°ç«¯çš„å­¦ä¹ æ–¹å¼ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ åˆ°ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”æ€§ï¼Œé¿å…äº†æ‰‹å·¥è®¾è®¡ç‰¹å¾çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼ŒPIT-QMMè¿˜å…·å¤‡å¤±çœŸå®šä½å’Œè¯†åˆ«èƒ½åŠ›ï¼Œè¿™ä¸ºæ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œäº¤äº’æ€§æä¾›äº†æ–°çš„é€”å¾„ã€‚

**å…³é”®è®¾è®¡**ï¼šæ–‡æœ¬ç¼–ç å™¨é‡‡ç”¨é¢„è®­ç»ƒçš„Transformeræ¨¡å‹ï¼Œå›¾åƒç¼–ç å™¨é‡‡ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œç‚¹äº‘ç¼–ç å™¨é‡‡ç”¨PointNet++ã€‚å¤šæ¨¡æ€èåˆæ¨¡å—é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„æƒé‡ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼Œç”¨äºè¡¡é‡é¢„æµ‹è´¨é‡åˆ†æ•°ä¸çœŸå®è´¨é‡åˆ†æ•°ä¹‹é—´çš„å·®å¼‚ã€‚æ¨¡å‹é‡‡ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º1e-4ï¼Œbatch sizeè®¾ç½®ä¸º32ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PIT-QMMåœ¨å…¬å¼€çš„NR-PCQAåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„state-of-the-artæ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒPIT-QMMåœ¨æµ‹è¯•é›†ä¸Šçš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰åˆ†åˆ«é™ä½äº†10%å’Œ15%ã€‚æ­¤å¤–ï¼ŒPIT-QMMè¿˜å±•ç¤ºäº†å¼ºå¤§çš„å¤±çœŸå®šä½å’Œè¯†åˆ«èƒ½åŠ›ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°å®šä½ç‚¹äº‘ä¸­çš„å¤±çœŸåŒºåŸŸï¼Œå¹¶è¯†åˆ«å¤±çœŸçš„ç±»å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦è¯„ä¼°ç‚¹äº‘è´¨é‡çš„åœºæ™¯ï¼Œä¾‹å¦‚3Dæ‰«æã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ç­‰ã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°ç‚¹äº‘è´¨é‡ï¼Œå¯ä»¥æé«˜3Dæ¨¡å‹çš„ç”Ÿäº§æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºç›‘æ§å’Œç»´æŠ¤3Dèµ„äº§ï¼ŒåŠæ—¶å‘ç°å’Œä¿®å¤è´¨é‡é—®é¢˜ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå•†ä¸šå‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Multimodal Models (LMMs) have recently enabled considerable advances in the realm of image and video quality assessment, but this progress has yet to be fully explored in the domain of 3D assets. We are interested in using these models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where the aim is to automatically evaluate the perceptual quality of a point cloud in absence of a reference. We begin with the observation that different modalities of data - text descriptions, 2D projections, and 3D point cloud views - provide complementary information about point cloud quality. We then construct PIT-QMM, a novel LMM for NR-PCQA that is capable of consuming text, images and point clouds end-to-end to predict quality scores. Extensive experimentation shows that our proposed method outperforms the state-of-the-art by significant margins on popular benchmarks with fewer training iterations. We also demonstrate that our framework enables distortion localization and identification, which paves a new way forward for model explainability and interactivity. Code and datasets are available at https://www.github.com/shngt/pit-qmm.

