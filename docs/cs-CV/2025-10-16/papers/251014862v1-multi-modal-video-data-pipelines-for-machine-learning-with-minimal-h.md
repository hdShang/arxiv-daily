---
layout: default
title: Multi-modal video data-pipelines for machine learning with minimal human supervision
---

# Multi-modal video data-pipelines for machine learning with minimal human supervision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14862" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14862v1</a>
  <a href="https://arxiv.org/pdf/2510.14862.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14862v1" onclick="toggleFavorite(this, '2510.14862v1', 'Multi-modal video data-pipelines for machine learning with minimal human supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mihai-Cristian PÃ®rvu, Marius Leordeanu

**åˆ†ç±»**: cs.CV, cs.DC

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºå¼±ç›‘ç£å¤šæ¨¡æ€è§†é¢‘æ•°æ®ç®¡é“çš„æœºå™¨å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†é¢‘æ•°æ®ç®¡é“` `å¼±ç›‘ç£å­¦ä¹ ` `é¢„è®­ç»ƒæ¨¡å‹` `æ¨¡å‹è’¸é¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸æ˜¯å•æ¨¡æ€æˆ–åŒæ¨¡æ€ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç°å®ä¸–ç•Œä¸­ä¸°å¯Œçš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§å…¨è‡ªåŠ¨æ•°æ®ç®¡é“ï¼Œåˆ©ç”¨é¢„è®­ç»ƒä¸“å®¶æ¨¡å‹ç»„åˆå¤šç§è§†è§‰æ¨¡æ€ï¼Œæ— éœ€è¿‡å¤šäººå·¥å¹²é¢„ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç»è¿‡è’¸é¦çš„å°å‚æ•°å¤šæ¨¡æ€æ¨¡å‹ï¼ˆPHG-MAEï¼‰æ€§èƒ½å¯ä¸å¤§å‚æ•°æ¨¡å‹åª²ç¾ï¼Œå¹¶å¯éƒ¨ç½²äºå®æ—¶åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œæœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ã€‚æˆ‘ä»¬çš„å·¥å…·ä»¥æ•°å­—å½¢å¼ï¼ˆå¦‚è§†é¢‘æˆ–å£°éŸ³ï¼‰è§‚å¯Ÿå¹¶è®°å½•å®ƒï¼Œä½†å¤§éƒ¨åˆ†ä¿¡æ¯éƒ½ä¸¢å¤±äº†ã€‚åŒæ ·ï¼Œåœ¨äººä¸äººä¹‹é—´çš„è¡Œä¸ºå’Œä¿¡æ¯ä¼ é€’ä¸­ï¼Œè¯­è¨€è¢«ç”¨ä½œä¹¦é¢äº¤æµå½¢å¼ã€‚ä¼ ç»Ÿä¸Šï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹æ˜¯å•æ¨¡æ€çš„ï¼ˆä¾‹å¦‚ï¼Œrgb -> è¯­ä¹‰æˆ–æ–‡æœ¬ -> æƒ…æ„Ÿç±»åˆ«ï¼‰ã€‚æœ€è¿‘çš„è¶‹åŠ¿æ˜¯åŒæ¨¡æ€ï¼Œå…¶ä¸­å›¾åƒå’Œæ–‡æœ¬ä¸€èµ·å­¦ä¹ ï¼Œä½†æ˜¯ï¼Œä¸ºäº†çœŸæ­£ç†è§£ä¸–ç•Œï¼Œæˆ‘ä»¬éœ€è¦æ•´åˆæ‰€æœ‰è¿™äº›ç‹¬ç«‹çš„æ¨¡æ€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°è¯•åœ¨å‡ ä¹æ²¡æœ‰äººä¸ºç›‘ç£çš„æƒ…å†µä¸‹ç»„åˆå°½å¯èƒ½å¤šçš„è§†è§‰æ¨¡æ€ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸“å®¶å’Œå®ƒä»¬ä¹‹é—´çš„ç¨‹åºç»„åˆï¼Œåœ¨åŸå§‹è§†é¢‘ä¹‹ä¸Šä½¿ç”¨å®Œå…¨è‡ªä¸»çš„æ•°æ®ç®¡é“ï¼Œæˆ‘ä»¬ä¹Ÿå°†å…¶å¼€æºã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨ PHG-MAEï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§é«˜æ•ˆåœ°æç‚¼æˆä½å‚æ•°ï¼ˆ<1Mï¼‰çš„æ¨¡å‹å¯ä»¥å…·æœ‰ä¸çº¦ 300M å‚æ•°çš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æˆ‘ä»¬éƒ¨ç½²æ­¤æ¨¡å‹å¹¶åˆ†æåœ¨å•†å“ç¡¬ä»¶ä¸Šæ‰‹æŒè®¾å¤‡æˆ–ç½‘ç»œæ‘„åƒå¤´å®æ—¶è¯­ä¹‰åˆ†å‰²çš„ç”¨ä¾‹ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„æ¡†æ¶éƒ¨ç½²å…¶ä»–ç°æˆçš„æ¨¡å‹ï¼Œä¾‹å¦‚ç”¨äºè¿‘å®æ—¶æ·±åº¦ä¼°è®¡çš„ DPTã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•é«˜æ•ˆåœ°åˆ©ç”¨è§†é¢‘ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œæœºå™¨å­¦ä¹ çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äººå·¥æ ‡æ³¨æˆ–ä»…é™äºå•æ¨¡æ€æˆ–åŒæ¨¡æ€æ•°æ®ï¼Œæ— æ³•å……åˆ†æŒ–æ˜è§†é¢‘ä¸­è•´å«çš„ä¸°å¯Œä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè®­ç»ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå…¨è‡ªåŠ¨çš„å¤šæ¨¡æ€æ•°æ®ç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹ä»åŸå§‹è§†é¢‘ä¸­æå–å¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç»„åˆæˆå¤šæ¨¡æ€æ•°æ®ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œç¨‹åºåŒ–ç»„åˆï¼Œå¯ä»¥å‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œå¹¶æé«˜æ•°æ®å¤„ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ•°æ®ç®¡é“ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åŸå§‹è§†é¢‘è¾“å…¥ï¼›2) ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡ç­‰ï¼‰æå–å¤šç§è§†è§‰æ¨¡æ€ä¿¡æ¯ï¼›3) ä½¿ç”¨ç¨‹åºåŒ–ç»„åˆæ–¹æ³•å°†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯è¿›è¡Œèåˆï¼›4) ä½¿ç”¨ PHG-MAE æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€å­¦ä¹ ã€‚è¯¥ç®¡é“æ˜¯å®Œå…¨è‡ªä¸»çš„ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªå…¨è‡ªåŠ¨çš„å¤šæ¨¡æ€æ•°æ®ç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿé«˜æ•ˆåœ°ä»åŸå§‹è§†é¢‘ä¸­æå–å’Œç»„åˆå¤šç§æ¨¡æ€çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä½¿ç”¨äº† PHG-MAE æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸“é—¨è®¾è®¡ç”¨äºåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡è’¸é¦æŠ€æœ¯å°†å…¶å‹ç¼©æˆå°å‚æ•°æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†å¤šç§é¢„è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹ï¼Œä¾‹å¦‚ç”¨äºç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡çš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥ä»åŸå§‹è§†é¢‘ä¸­æå–ä¸åŒçš„è§†è§‰æ¨¡æ€ä¿¡æ¯ã€‚è®ºæ–‡è¿˜ä½¿ç”¨äº†ç¨‹åºåŒ–ç»„åˆæ–¹æ³•ï¼Œä¾‹å¦‚å°†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯è¿›è¡Œæ‹¼æ¥æˆ–åŠ æƒèåˆã€‚PHG-MAE æ¨¡å‹çš„å…·ä½“ç»“æ„å’Œè®­ç»ƒç»†èŠ‚æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶é«˜æ•ˆæ€§å’Œå¯è’¸é¦æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ•°æ®ç®¡é“å’Œ PHG-MAE æ¨¡å‹è®­ç»ƒçš„å°å‚æ•°æ¨¡å‹ï¼ˆ<1Mï¼‰å¯ä»¥è¾¾åˆ°ä¸ 300M å‚æ•°çš„å¤§å‹æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯ä»¥éƒ¨ç½²åœ¨æ‰‹æŒè®¾å¤‡æˆ–ç½‘ç»œæ‘„åƒå¤´ä¸Šï¼Œå®ç°å®æ—¶çš„è¯­ä¹‰åˆ†å‰²ã€‚è®ºæ–‡è¿˜å±•ç¤ºäº†ä½¿ç”¨è¯¥æ¡†æ¶éƒ¨ç½²å…¶ä»–ç°æˆæ¨¡å‹ï¼ˆå¦‚ DPTï¼‰è¿›è¡Œè¿‘å®æ—¶æ·±åº¦ä¼°è®¡çš„èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç›‘æ§ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¯ä»¥æé«˜æœºå™¨å¯¹ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„ä»»åŠ¡æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è§†è§‰ã€æ·±åº¦å’Œè¯­ä¹‰ä¿¡æ¯æ¥æé«˜è½¦è¾†çš„å¯¼èˆªå’Œé¿éšœèƒ½åŠ›ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®‰é˜²ï¼Œé€šè¿‡åˆ†æè§†é¢‘ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯æ¥è¯†åˆ«å¼‚å¸¸è¡Œä¸ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The real-world is inherently multi-modal at its core. Our tools observe and take snapshots of it, in digital form, such as videos or sounds, however much of it is lost. Similarly for actions and information passing between humans, languages are used as a written form of communication. Traditionally, Machine Learning models have been unimodal (i.e. rgb -> semantic or text -> sentiment_class). Recent trends go towards bi-modality, where images and text are learned together, however, in order to truly understand the world, we need to integrate all these independent modalities. In this work we try to combine as many visual modalities as we can using little to no human supervision. In order to do this, we use pre-trained experts and procedural combinations between them on top of raw videos using a fully autonomous data-pipeline, which we also open-source. We then make use of PHG-MAE, a model specifically designed to leverage multi-modal data. We show that this model which was efficiently distilled into a low-parameter (<1M) can have competitive results compared to models of ~300M parameters. We deploy this model and analyze the use-case of real-time semantic segmentation from handheld devices or webcams on commodity hardware. Finally, we deploy other off-the-shelf models using the same framework, such as DPT for near real-time depth estimation.

