---
layout: default
title: CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection
---

# CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14792" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.14792v1</a>
  <a href="https://arxiv.org/pdf/2510.14792.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14792v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14792v1', 'CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

**Â§áÊ≥®**: 28 pages, 13 Figures, 12 Tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CoT-PLÊ°ÜÊû∂ÔºåÈÄöËøáËßÜËßâÈìæÂºèÊé®ÁêÜÂíå‰º™Ê†áÁ≠æÊèêÂçáÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµãÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµã` `ËßÜËßâÈìæÂºèÊé®ÁêÜ` `‰º™Ê†áÁ≠æ` `ÂØπÊØîÂ≠¶‰π†` `ËÉåÊôØÊé•Âú∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ï‰æùËµñÁõ¥Êé•ÂõæÂÉè-ÊñáÊú¨ÂåπÈÖçÔºåÂøΩÁï•‰∫ÜÊé®ÁêÜÊ≠•È™§ÔºåÂØºËá¥Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÈ≤ÅÊ£íÊÄß‰∏çË∂≥„ÄÇ
2. CoT-PLÊ°ÜÊû∂ÈÄöËøáËßÜËßâÈìæÂºèÊé®ÁêÜÂàÜËß£ÂØπË±°ÁêÜËß£‰∏∫Âå∫ÂüüÊÑüÁü•„ÄÅÁ±ªÂà´ËØÜÂà´ÂíåËÉåÊôØÊé•Âú∞‰∏â‰∏™Ê≠•È™§ÔºåÊèêÂçá‰º™Ê†áÁ≠æË¥®Èáè„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåCoT-PLÂú®ÂºÄÊîæËØçÊ±áCOCOÂíåLVISÊï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜÊñ∞Á±ªÂà´ÁöÑÊ£ÄÊµãÊÄßËÉΩÔºåËææÂà∞Êñ∞ÁöÑSOTA„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµã(OVD)Êó®Âú®ËØÜÂà´ÂíåÂÆö‰ΩçËÆ≠ÁªÉÊúüÈó¥Êú™ËßÅËøáÁöÑÂØπË±°Á±ªÂà´„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLMs)ÈÄöËøáÂõæÂÉè-ÊñáÊú¨ÂØπÈΩêÁîüÊàê‰º™Ê†áÁ≠æÔºå‰ΩøÊ£ÄÊµãÂô®ËÉΩÂ§üÊé®ÂπøÂà∞Êú™ËßÅËøáÁöÑÁ±ªÂà´ËÄåÊó†ÈúÄÊòæÂºèÁõëÁù£„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ï‰∏•Èáç‰æùËµñÁõ¥Êé•ÁöÑÂõæÂÉè-ÊñáÊú¨ÂåπÈÖçÔºåÂøΩÁï•‰∫ÜËß£ÈáäËØ≠‰πâÂ§çÊùÇÂú∫ÊôØÊâÄÂøÖÈúÄÁöÑ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§„ÄÇËøôÂØºËá¥Âú®Êã•Êå§ÊàñÈÅÆÊå°ÁöÑËßÜËßâÁéØÂ¢É‰∏≠È≤ÅÊ£íÊÄßÊúâÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂CoT-PLÔºåËØ•Ê°ÜÊû∂Âú®‰º™Ê†áÁ≠æËøáÁ®ã‰∏≠ÈááÁî®‰∫ÜÁªìÊûÑÂåñÁöÑËßÜËßâÈìæÂºèÊÄùËÄÉ(CoT)Êé®ÁêÜ„ÄÇCoT-PLÂ∞ÜÂØπË±°ÁêÜËß£ÂàÜËß£‰∏∫‰∏â‰∏™ÂèØËß£ÈáäÁöÑÊ≠•È™§Ôºö(1)Âç≥‰ΩøÂØπ‰∫éÊú™ËßÅËøáÁöÑÂØπË±°Ôºå‰πüËÉΩËøõË°åÂå∫ÂüüÊÑüÁü•Ôºõ(2)ÈÄöËøáÈõ∂Ê†∑Êú¨Êé®ÁêÜËøõË°åÁ±ªÂà´ËØÜÂà´Ôºõ(3)ËÉåÊôØÊé•Âú∞‰ª•ÂàÜÁ¶ªËØ≠‰πâÂ§çÊùÇÁöÑÂØπË±°„ÄÇÂÖ≥ÈîÆÊòØÔºåÁ¨¨‰∏âÊ≠•Ëá™ÁÑ∂Âú∞ÊøÄÂèë‰∫ÜÊàë‰ª¨ÁöÑÂØπÊØîËÉåÊôØÂ≠¶‰π†(CBL)ÔºåÂÆÉ‰ΩøÁî®È¢ÑÂÖàËÆ°ÁÆóÁöÑËÉåÊôØÁ∫øÁ¥¢‰Ωú‰∏∫Ë¥üÊ†∑Êú¨Ôºå‰ª•‰øÉËøõÂØπË±°ÂíåËÉåÊôØ‰πãÈó¥ÁöÑÁâπÂæÅËß£ËÄ¶„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåCoTÊé®ÁêÜÂíåCBLÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™ÈõÜÊàêÁöÑpipelineÔºå‰∏ìÈó®Áî®‰∫éÂú®Êã•Êå§ÊàñÈÅÆÊå°Âú∫ÊôØ‰∏≠ËøõË°åÈ≤ÅÊ£íÁöÑ‰º™Ê†áÁ≠æ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂú®Ëøô‰∏§ÁßçËÆæÁΩÆ‰∏≠ÔºåÊàë‰ª¨Êñ∞Á±ªÂà´ÁöÑ‰º™Ê†áÁ≠æË¥®ÈáèÂàÜÂà´ÊØîÊúÄÂ•ΩÁöÑÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü103.4%Âíå168.4%„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåCoT-PLÂú®ÂºÄÊîæËØçÊ±áCOCO‰∏äÂÆûÁé∞‰∫Ü+7.7 AP50ÔºåÂú®LVIS‰∏äÂÆûÁé∞‰∫Ü+2.9 mask APÔºå‰∏∫Êñ∞Á±ªÂà´ËÆæÂÆö‰∫ÜÊñ∞ÁöÑstate of the art„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµãÊó®Âú®ËØÜÂà´ËÆ≠ÁªÉÈõÜ‰∏≠Êú™Âá∫Áé∞ÁöÑÁâ©‰ΩìÁ±ªÂà´„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁîüÊàê‰º™Ê†áÁ≠æÔºå‰ΩÜÁõ¥Êé•ÁöÑÂõæÂÉè-ÊñáÊú¨ÂåπÈÖçÂøΩÁï•‰∫ÜÊé®ÁêÜËøáÁ®ãÔºåÂØºËá¥Âú®Êã•Êå§ÊàñÈÅÆÊå°Âú∫ÊôØ‰∏ãÊÄßËÉΩ‰∏ãÈôç„ÄÇÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂå∫ÂàÜÁâ©‰ΩìÂíåÂ§çÊùÇËÉåÊôØÔºåÂØºËá¥‰º™Ê†áÁ≠æË¥®Èáè‰∏çÈ´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCoT-PLÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂ§çÊùÇÁöÑÁâ©‰ΩìËØÜÂà´‰ªªÂä°ÂàÜËß£‰∏∫Â§ö‰∏™ÂèØËß£ÈáäÁöÑÊ≠•È™§ÔºåÊ®°Êãü‰∫∫Á±ªÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÈÄöËøáÊòæÂºèÂú∞Âª∫Ê®°Âå∫ÂüüÊÑüÁü•„ÄÅÁ±ªÂà´ËØÜÂà´ÂíåËÉåÊôØÊé•Âú∞ÔºåÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞ÁîüÊàê‰º™Ê†áÁ≠æÔºå‰ªéËÄåÊèêÂçáÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµãÁöÑÊÄßËÉΩ„ÄÇÂØπÊØîËÉåÊôØÂ≠¶‰π†(CBL)Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜÊ®°ÂûãÂå∫ÂàÜÁâ©‰ΩìÂíåËÉåÊôØÁöÑËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCoT-PLÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) Âå∫ÂüüÊÑüÁü•ÔºöÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÊ®°ÂûãÊ£ÄÊµãÂõæÂÉè‰∏≠ÁöÑÊΩúÂú®Áâ©‰ΩìÂå∫ÂüüÔºåÂç≥‰ΩøËøô‰∫õÁâ©‰ΩìÂ±û‰∫éÊú™ËßÅËøáÁöÑÁ±ªÂà´„ÄÇ2) Á±ªÂà´ËØÜÂà´Ôºö‰ΩøÁî®Èõ∂Ê†∑Êú¨ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂØπÊ£ÄÊµãÂà∞ÁöÑÂå∫ÂüüËøõË°åÂàÜÁ±ªÔºåÁîüÊàêÂÄôÈÄâÁöÑ‰º™Ê†áÁ≠æ„ÄÇ3) ËÉåÊôØÊé•Âú∞ÔºöÈÄöËøáÂØπÊØîÂ≠¶‰π†ÔºåÂå∫ÂàÜÁâ©‰ΩìÂíåËÉåÊôØÔºåËøáÊª§ÊéâÈîôËØØÁöÑ‰º™Ê†áÁ≠æ„ÄÇCBLÊ®°ÂùóÂà©Áî®È¢ÑËÆ°ÁÆóÁöÑËÉåÊôØÁ∫øÁ¥¢‰Ωú‰∏∫Ë¥üÊ†∑Êú¨Ôºå‰øÉËøõÁâπÂæÅËß£ËÄ¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCoT-PLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜËßÜËßâÈìæÂºèÊé®ÁêÜ(CoT)Âà∞‰º™Ê†áÁ≠æÁîüÊàêËøáÁ®ã‰∏≠„ÄÇ‰∏éÁõ¥Êé•ÂõæÂÉè-ÊñáÊú¨ÂåπÈÖçÁöÑÊñπÊ≥ï‰∏çÂêåÔºåCoT-PLÊòæÂºèÂú∞Âª∫Ê®°‰∫ÜÁâ©‰ΩìËØÜÂà´ÁöÑ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§Ôºå‰ªéËÄåÊèêÈ´ò‰∫Ü‰º™Ê†áÁ≠æÁöÑË¥®ÈáèÂíåÈ≤ÅÊ£íÊÄß„ÄÇÂØπÊØîËÉåÊôØÂ≠¶‰π†(CBL)ÊòØÂè¶‰∏Ä‰∏™ÂàõÊñ∞ÁÇπÔºåÂÆÉÈÄöËøáÂà©Áî®ËÉåÊôØ‰ø°ÊÅØ‰Ωú‰∏∫Ë¥üÊ†∑Êú¨ÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂå∫ÂàÜÁâ©‰ΩìÂíåËÉåÊôØÁöÑËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCoT-PLÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÊ®°ÂûãÔºàÂ¶ÇCLIPÔºâËøõË°åÂå∫ÂüüÊÑüÁü•ÂíåÁ±ªÂà´ËØÜÂà´„ÄÇ2) ËÆæËÆ°ÂØπÊØîÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éÂØπÊØîËÉåÊôØÂ≠¶‰π†ÔºåÈºìÂä±Ê®°ÂûãÂ≠¶‰π†Âå∫ÂàÜÁâ©‰ΩìÂíåËÉåÊôØÁöÑÁâπÂæÅ„ÄÇ3) ‰ΩøÁî®È¢ÑËÆ°ÁÆóÁöÑËÉåÊôØÁ∫øÁ¥¢‰Ωú‰∏∫Ë¥üÊ†∑Êú¨ÔºåÊèêÈ´òCBLÁöÑÊïàÁéá„ÄÇ4) ÈÄöËøáÂÆûÈ™åË∞ÉÊï¥ÂØπÊØîÊçüÂ§±ÁöÑÊùÉÈáçÔºå‰ª•Âπ≥Ë°°Áâ©‰ΩìÂíåËÉåÊôØÁöÑÂ≠¶‰π†„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CoT-PLÂú®ÂºÄÊîæËØçÊ±áCOCOÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü+7.7 AP50ÁöÑÊèêÂçáÔºåÂú®LVISÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫Ü+2.9 mask APÁöÑÊèêÂçáÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑstate-of-the-art„ÄÇÂú®Êã•Êå§ÂíåÈÅÆÊå°Âú∫ÊôØ‰∏ãÔºåCoT-PLÁöÑ‰º™Ê†áÁ≠æË¥®ÈáèÂàÜÂà´ÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫Ü103.4%Âíå168.4%ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CoT-PLÂú®ÂºÄÊîæËØçÊ±áÁõÆÊ†áÊ£ÄÊµã‰∏äÁöÑÁ™ÅÁ†¥Ôºå‰∏∫Êô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈ¢ÜÂüüÂ∏¶Êù•‰∫ÜÊΩúÂú®ÁöÑÂ∫îÁî®‰ª∑ÂÄº„ÄÇËØ•ÊäÄÊúØÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ËØÜÂà´ÂíåÂÆö‰ΩçÊú™Áü•ÁöÑÁâ©‰ΩìÔºåÊèêÈ´òÂÖ∂ÈÄÇÂ∫îÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñËßÜËßâ‰ªªÂä°ÔºåÂ¶ÇÂõæÂÉèÊèèËø∞ÂíåËßÜËßâÈóÆÁ≠î„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art.

