---
layout: default
title: VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning
---

# VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14672" target="_blank" class="toolbar-btn">arXiv: 2510.14672v1</a>
    <a href="https://arxiv.org/pdf/2510.14672.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14672v1" 
            onclick="toggleFavorite(this, '2510.14672v1', 'VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

**Â§áÊ≥®**: Accepted by ICCV 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VTimeCoTÔºöÈÄöËøáÁªòÂà∂ËßÜÈ¢ëËøõÂ∫¶Êù°ËøõË°åËßÜÈ¢ëÊó∂Â∫èÂÆö‰Ωç‰∏éÊé®ÁêÜ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÊó∂Â∫èÂÆö‰Ωç` `ËßÜÈ¢ëÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËßÜËßâÊó∂Â∫èCoT`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMÂú®ËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÂíåÊé®ÁêÜÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖËßÜÈ¢ëÁêÜËß£Á≥ªÁªü‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. VTimeCoTÈÄöËøáÊ®°Êãü‰∫∫Á±ª‰ΩøÁî®ËßÜÈ¢ëÊí≠ÊîæÂô®ËøõÂ∫¶Êù°ÁöÑÊñπÂºèÔºåÂºïÂÖ•ËßÜËßâÊó∂Â∫èCoTËøõË°åË∑®Ê®°ÊÄÅÊé®ÁêÜ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVTimeCoTÂú®ËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÂíåÊé®ÁêÜÈóÆÁ≠î‰ªªÂä°‰∏äÔºåÊòæËëóÊèêÂçá‰∫ÜQwen2VL-7BÂíåGPT4oÁ≠âÂü∫Á∫øÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÂü∫‰∫éÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑËßÜÈ¢ëÈóÆÁ≠îÂõ†LLMÁöÑÊòæËëóËøõÊ≠•ËÄåÂ§áÂèóÂÖ≥Ê≥®„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂú®ËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÂíåÊé®ÁêÜÊñπÈù¢Â≠òÂú®ÊòéÊòæÁöÑ‰∏çË∂≥ÔºåÂØπÊúâÊïàÁé∞ÂÆû‰∏ñÁïåËßÜÈ¢ëÁêÜËß£Á≥ªÁªüÁöÑÂèëÂ±ïÊûÑÊàêÊåëÊàò„ÄÇÂèóÂà∞‰∫∫Á±ª‰ΩøÁî®ËßÜÈ¢ëÊí≠ÊîæÂô®‰∏éËøõÂ∫¶Êù°‰∫§‰∫í‰ª•ÁêÜËß£ËßÜÈ¢ëÁöÑÂêØÂèëÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜVTimeCoTÔºåËøôÊòØ‰∏Ä‰∏™ÁÆÄÂçï‰ΩÜÊúâÊïàÁöÑÂÖçËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰∏ì‰∏∫È´òÊÄßËÉΩËßÜÈ¢ëÂÆö‰ΩçÂíåÊé®ÁêÜËÄåËÆæËÆ°„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏§‰∏™Êñ∞È¢ñÁöÑËøõÂ∫¶Êù°ËßÜËßâÂ∑•ÂÖ∑ÔºöÂç≥ÊèíÂç≥Áî®ËøõÂ∫¶Êù°ÈõÜÊàêÂ∑•ÂÖ∑ÂíåÈ´òÊïàÈ´ò‰∫ÆÂ∑•ÂÖ∑„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜËß£ÂÜ≥‰º†ÁªüÂü∫‰∫éÊñáÊú¨ÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçËßÜËßâÊó∂Â∫èCoTËøáÁ®ãÔºåËØ•ËøáÁ®ãÈõÜÊàê‰∫ÜËßÜÈ¢ëÂíåÊñáÊú¨‰πãÈó¥ÁöÑË∑®Ê®°ÊÄÅÊé®ÁêÜ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Qwen2VL-7BÂíåGPT4oÂü∫Á∫ø‰∏äÔºåÂú®ËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÂíåÂü∫‰∫éÊé®ÁêÜÁöÑÈóÆÁ≠î‰ªªÂä°‰∏≠ÈÉΩË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂ÂÆûÁé∞‰∫ÜÁªÑÂêàÂºèÂíåÂèØËß£ÈáäÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÂú®ËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÂíåÊé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÊñáÊú¨ÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÔºåÁº∫‰πèÂØπËßÜÈ¢ëÊó∂Â∫è‰ø°ÊÅØÁöÑÊúâÊïàÂà©Áî®ÔºåÂØºËá¥Âú®ÈúÄË¶ÅÁ≤æÁ°ÆÂÆö‰ΩçÂíåÂ§çÊùÇÊé®ÁêÜÁöÑËßÜÈ¢ëÈóÆÁ≠î‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂÄüÈâ¥‰∫∫Á±ª‰ΩøÁî®ËßÜÈ¢ëÊí≠ÊîæÂô®ËøõÂ∫¶Êù°ËøõË°åËßÜÈ¢ëÁêÜËß£ÁöÑÊñπÂºèÔºåÂ∞ÜËßÜÈ¢ëÊó∂Â∫è‰ø°ÊÅØ‰ª•ËßÜËßâÂåñÁöÑÂΩ¢ÂºèËûçÂÖ•Âà∞Êé®ÁêÜËøáÁ®ã‰∏≠„ÄÇÈÄöËøáÊ®°ÊãüËøõÂ∫¶Êù°ÁöÑ‰∫§‰∫íÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£ËßÜÈ¢ëÁöÑÊó∂Èó¥ÁªìÊûÑÔºå‰ªéËÄåÊèêÈ´òÊó∂Â∫èÂÆö‰ΩçÂíåÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVTimeCoTÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê†∏ÂøÉÁªÑ‰ª∂Ôºö1) Âç≥ÊèíÂç≥Áî®ËøõÂ∫¶Êù°ÈõÜÊàêÂ∑•ÂÖ∑ÔºåÁî®‰∫éÂ∞ÜËßÜÈ¢ëËøõÂ∫¶Êù°‰ø°ÊÅØÂµåÂÖ•Âà∞MLLM‰∏≠Ôºõ2) È´òÊïàÈ´ò‰∫ÆÂ∑•ÂÖ∑ÔºåÁî®‰∫éÁ™ÅÂá∫ÊòæÁ§∫ËßÜÈ¢ë‰∏≠ÁöÑÂÖ≥ÈîÆÂ∏ßÊàñÁâáÊÆµÔºõ3) ËßÜËßâÊó∂Â∫èCoTËøáÁ®ãÔºåÁî®‰∫éÊï¥ÂêàËßÜÈ¢ëÂíåÊñáÊú¨‰ø°ÊÅØÔºåËøõË°åË∑®Ê®°ÊÄÅÊé®ÁêÜ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºåÈ¶ñÂÖàÂà©Áî®ËøõÂ∫¶Êù°ÈõÜÊàêÂ∑•ÂÖ∑ÂíåÈ´ò‰∫ÆÂ∑•ÂÖ∑ÊèêÂèñËßÜÈ¢ëÁöÑÊó∂Â∫èÁâπÂæÅÔºåÁÑ∂ÂêéÂ∞ÜËøô‰∫õÁâπÂæÅ‰∏éÊñáÊú¨‰ø°ÊÅØ‰∏ÄËµ∑ËæìÂÖ•Âà∞MLLM‰∏≠ÔºåÊúÄÂêéÈÄöËøáËßÜËßâÊó∂Â∫èCoTËøáÁ®ãËøõË°åÊé®ÁêÜÂíåÈóÆÁ≠î„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÊèêÂá∫‰∫ÜËßÜËßâÊó∂Â∫èCoTËøáÁ®ãÔºåÂÆÉÂ∞ÜËßÜÈ¢ëÁöÑÊó∂Â∫è‰ø°ÊÅØ‰ª•ËßÜËßâÂåñÁöÑÊñπÂºèËûçÂÖ•Âà∞Êé®ÁêÜËøáÁ®ã‰∏≠ÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñáÊú¨CoTÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåVTimeCoTËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®ËßÜÈ¢ëÁöÑÊó∂Èó¥ÁªìÊûÑÔºå‰ªéËÄåÊèêÈ´òÊó∂Â∫èÂÆö‰ΩçÂíåÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËøõÂ∫¶Êù°ÈõÜÊàêÂ∑•ÂÖ∑ÂíåÈ´ò‰∫ÆÂ∑•ÂÖ∑ÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÊú™Áü•ÔºåËÆ∫ÊñáÈáçÁÇπÂº∫Ë∞É‰∫ÜËßÜËßâÊó∂Â∫èCoTËøáÁ®ãÁöÑËÆæËÆ°„ÄÇËØ•ËøáÁ®ãÈÄöËøáËø≠‰ª£Âú∞Âú®ËßÜÈ¢ëÂíåÊñáÊú¨‰πãÈó¥ËøõË°åÊé®ÁêÜÔºåÈÄêÊ≠•Áº©Â∞èÁõÆÊ†áÁâáÊÆµÁöÑËåÉÂõ¥ÔºåÊúÄÁªàÂÆûÁé∞Á≤æÁ°ÆÂÆö‰ΩçÂíåÂáÜÁ°ÆÂõûÁ≠î„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠Ê≤°ÊúâËØ¶ÁªÜÊèèËø∞ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VTimeCoTÂú®Qwen2VL-7BÂíåGPT4oÂü∫Á∫ø‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®ËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÂíåÊé®ÁêÜÈóÆÁ≠î‰ªªÂä°‰∏≠ÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Âú®ËÆ∫Êñá‰∏≠Ê≤°ÊúâÊòéÁ°ÆÁªôÂá∫ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ‰ΩÜËÆ∫ÊñáÂº∫Ë∞É‰∫ÜVTimeCoTÂÆûÁé∞‰∫ÜÁªÑÂêàÂºèÂíåÂèØËß£ÈáäÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VTimeCoTÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçËßÜÈ¢ëÁêÜËß£‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜÈ¢ëÈóÆÁ≠î„ÄÅËßÜÈ¢ëÊëòË¶Å„ÄÅËßÜÈ¢ëÊ£ÄÁ¥¢Á≠â„ÄÇËØ•Á†îÁ©∂ÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂú®‰∫éÊèêÂçá‰∫ÜMLLMÂú®ËßÜÈ¢ëÊó∂Â∫èÂÆö‰ΩçÂíåÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂà©Áî®ËßÜÈ¢ëÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÇÊú™Êù•ÔºåËØ•Ê°ÜÊû∂ÊúâÊúõÂ∫îÁî®‰∫éÊô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂú®Á∫øÊïôËÇ≤Á≠âÈ¢ÜÂüüÔºå‰∏∫‰∫∫‰ª¨Êèê‰æõÊõ¥Êô∫ËÉΩ„ÄÅÊõ¥‰æøÊç∑ÁöÑËßÜÈ¢ëÊúçÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: https://vtimecot.github.io

