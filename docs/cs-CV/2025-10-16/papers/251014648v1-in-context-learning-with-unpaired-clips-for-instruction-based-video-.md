---
layout: default
title: In-Context Learning with Unpaired Clips for Instruction-based Video Editing
---

# In-Context Learning with Unpaired Clips for Instruction-based Video Editing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14648" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14648v1</a>
  <a href="https://arxiv.org/pdf/2510.14648.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14648v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14648v1', 'In-Context Learning with Unpaired Clips for Instruction-based Video Editing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyao Liao, Xianfang Zeng, Ziye Song, Zhoujie Fu, Gang Yu, Guosheng Lin

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºéé…å¯¹è§†é¢‘ç‰‡æ®µçš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæŒ‡ä»¤é©±åŠ¨çš„è§†é¢‘ç¼–è¾‘ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æŒ‡ä»¤é©±åŠ¨è§†é¢‘ç¼–è¾‘` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `éé…å¯¹æ•°æ®` `è§†é¢‘ç”Ÿæˆæ¨¡å‹` `é¢„è®­ç»ƒ` `å¾®è°ƒ` `HunyuanVideoT2V`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æŒ‡ä»¤é©±åŠ¨çš„è§†é¢‘ç¼–è¾‘é¢ä¸´é…å¯¹æ•°æ®é›†æ„å»ºæˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å‘å±•ã€‚
2. åˆ©ç”¨éé…å¯¹è§†é¢‘ç‰‡æ®µçš„ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œé¢„è®­ç»ƒï¼Œèµ‹äºˆæ¨¡å‹é€šç”¨çš„è§†é¢‘ç¼–è¾‘èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‡ä»¤å¯¹é½å’Œè§†è§‰è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬çš„æŒ‡ä»¤é©±åŠ¨è§†é¢‘ç¼–è¾‘é¢„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨éé…å¯¹è§†é¢‘ç‰‡æ®µçš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è¯¥ç­–ç•¥é¢„è®­ç»ƒä¸€ä¸ªåŸºç¡€è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿèµ‹äºˆå…¶é€šç”¨çš„ç¼–è¾‘èƒ½åŠ›ï¼Œä¾‹å¦‚æ ¹æ®è¾“å…¥çš„ç¼–è¾‘æŒ‡ä»¤è¿›è¡Œæ·»åŠ ã€æ›¿æ¢æˆ–åˆ é™¤æ“ä½œã€‚é¢„è®­ç»ƒæ¨¡å‹éšåå¯ä»¥ä½¿ç”¨å°‘é‡é«˜è´¨é‡çš„é…å¯¹ç¼–è¾‘æ•°æ®è¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚è¯¥æ¡†æ¶åŸºäºHunyuanVideoT2Vï¼Œé¦–å…ˆåœ¨å¤§çº¦100ä¸‡ä¸ªçœŸå®è§†é¢‘ç‰‡æ®µä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ åŸºæœ¬çš„ç¼–è¾‘æ¦‚å¿µï¼Œç„¶åä½¿ç”¨å°‘äº15ä¸‡ä¸ªç²¾å¿ƒç­–åˆ’çš„ç¼–è¾‘å¯¹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ‰©å±•æ›´å¤šçš„ç¼–è¾‘ä»»åŠ¡å¹¶æé«˜ç¼–è¾‘è´¨é‡ã€‚å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‡ä»¤å¯¹é½å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰çš„åŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œåœ¨ç¼–è¾‘æŒ‡ä»¤éµå¾ªæ–¹é¢æé«˜äº†12ï¼…ï¼Œåœ¨ç¼–è¾‘è´¨é‡æ–¹é¢æé«˜äº†15ï¼…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæŒ‡ä»¤é©±åŠ¨çš„è§†é¢‘ç¼–è¾‘æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æŒ‡ä»¤ä¿®æ”¹è§†é¢‘å†…å®¹ã€‚ç„¶è€Œï¼Œæ„å»ºå¤§è§„æ¨¡çš„é…å¯¹è§†é¢‘ç¼–è¾‘æ•°æ®é›†ï¼ˆå³åŸå§‹è§†é¢‘å’Œç¼–è¾‘åè§†é¢‘ä»¥åŠå¯¹åº”çš„æŒ‡ä»¤ï¼‰æˆæœ¬éå¸¸é«˜ï¼Œè¿™æˆä¸ºäº†è¯¥é¢†åŸŸå‘å±•çš„ä¸»è¦ç“¶é¢ˆã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºåˆæˆæ•°æ®æˆ–å°è§„æ¨¡æ•°æ®é›†ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§é‡çš„éé…å¯¹è§†é¢‘ç‰‡æ®µè¿›è¡Œé¢„è®­ç»ƒï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹å¼è®©æ¨¡å‹å­¦ä¹ è§†é¢‘ç¼–è¾‘çš„åŸºæœ¬æ¦‚å¿µå’Œæ“ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­å‡ºè§†é¢‘ä¸­å¯èƒ½å‘ç”Ÿçš„ç¼–è¾‘ï¼Œä»è€Œè·å¾—åˆæ­¥çš„ç¼–è¾‘èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¤§è§„æ¨¡é…å¯¹æ•°æ®çš„ä¾èµ–ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŸºäºHunyuanVideoT2Væ¨¡å‹ï¼Œä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µå’Œå¾®è°ƒé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨å¤§çº¦100ä¸‡ä¸ªéé…å¯¹çš„çœŸå®è§†é¢‘ç‰‡æ®µä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ è§†é¢‘ç¼–è¾‘çš„é€šç”¨æ¦‚å¿µã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨å°‘é‡ï¼ˆå°‘äº15ä¸‡ä¸ªï¼‰é«˜è´¨é‡çš„é…å¯¹ç¼–è¾‘æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ‰©å±•æ›´å¤šçš„ç¼–è¾‘ä»»åŠ¡å¹¶æé«˜ç¼–è¾‘è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨éé…å¯¹æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œä»è€Œé™ä½äº†å¯¹å¤§è§„æ¨¡é…å¯¹æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿä»éé…å¯¹æ•°æ®ä¸­å­¦ä¹ åˆ°è§†é¢‘ç¼–è¾‘çš„åŸºæœ¬æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§ä½æˆæœ¬ä¸”æœ‰æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹é‡‡ç”¨Transformeræ¶æ„ï¼Œè¾“å…¥æ˜¯è§†é¢‘ç‰‡æ®µå’Œç¼–è¾‘æŒ‡ä»¤ï¼ˆä¾‹å¦‚â€œæ·»åŠ ä¸€åªçŒ«â€ï¼‰ï¼Œè¾“å‡ºæ˜¯ç¼–è¾‘åçš„è§†é¢‘ç‰‡æ®µã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æŸå¤±å’ŒL1æŸå¤±ï¼Œä»¥ä¿è¯ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’ŒçœŸå®æ€§ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨é…å¯¹æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç‰¹å®šç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‡ä»¤å¯¹é½å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰çš„åŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ç¼–è¾‘æŒ‡ä»¤éµå¾ªæ–¹é¢ï¼Œè¯¥æ–¹æ³•å–å¾—äº†12ï¼…çš„æå‡ï¼›åœ¨ç¼–è¾‘è´¨é‡æ–¹é¢ï¼Œå–å¾—äº†15ï¼…çš„æå‡ã€‚è¿™äº›æ•°æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ ¹æ®æŒ‡ä»¤ä¿®æ”¹è§†é¢‘å†…å®¹ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§è§†é¢‘ç¼–è¾‘åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨åŒ–è§†é¢‘å†…å®¹ç”Ÿæˆã€ä¸ªæ€§åŒ–è§†é¢‘ç¼–è¾‘ã€ä»¥åŠè§†é¢‘ä¿®å¤ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé™ä½è§†é¢‘ç¼–è¾‘çš„é—¨æ§›ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æŒ‡ä»¤å¿«é€Ÿç”Ÿæˆæˆ–ä¿®æ”¹è§†é¢‘å†…å®¹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå•†ä¸šä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\% improvement in editing instruction following and a 15\% improvement in editing quality.

