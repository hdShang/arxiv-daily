---
layout: default
title: "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures"
---

# Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14179" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14179v1</a>
  <a href="https://arxiv.org/pdf/2510.14179.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14179v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14179v1', 'Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Levent TaÅŸel, Yiwei Zhao, Ryan Burgert, Mingming He, Oliver Hermann, Oliver Pilarski, Rahul Garg, Paul Debevec, Ning Yu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

**å¤‡æ³¨**: Accepted to SIGGRAPH Asia 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://eyeline-labs.github.io/Virtually-Being)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šè§†è§’è¡¨æ¼”æ•æ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å®šåˆ¶æ¡†æ¶ï¼Œå®ç°ç›¸æœºå¯æ§å’Œè§’è‰²ä¸€è‡´æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†é¢‘æ‰©æ•£æ¨¡å‹` `å¤šè§†è§’ä¸€è‡´æ€§` `ç›¸æœºæ§åˆ¶` `è™šæ‹Ÿåˆ¶ä½œ` `4Dé«˜æ–¯æº…å°„` `è§†é¢‘å…‰ç…§é‡æ„` `è§’è‰²å®šåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å¤šè§†è§’ä¸€è‡´æ€§å’Œç›¸æœºæ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³è™šæ‹Ÿåˆ¶ä½œçš„éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºå¤šè§†è§’è¡¨æ¼”æ•æ‰çš„å®šåˆ¶æ•°æ®æµç¨‹ï¼Œå¾®è°ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå®ç°è§’è‰²ä¸€è‡´æ€§å’Œç›¸æœºæ§åˆ¶ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†è§†é¢‘è´¨é‡ã€ä¸ªæ€§åŒ–å‡†ç¡®æ€§ï¼Œå¹¶å¢å¼ºäº†ç›¸æœºæ§åˆ¶å’Œå…‰ç…§é€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸€ç§æ–°é¢–çš„å®šåˆ¶æ•°æ®æµç¨‹ï¼Œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å®ç°å¤šè§†è§’è§’è‰²ä¸€è‡´æ€§å’Œ3Dç›¸æœºæ§åˆ¶ã€‚æˆ‘ä»¬ä½¿ç”¨é€šè¿‡4Dé«˜æ–¯æº…å°„ï¼ˆ4DGSï¼‰é‡æ–°æ¸²æŸ“çš„ä½“ç§¯æ•è·è¡¨æ¼”è®°å½•æ¥è®­ç»ƒè§’è‰²ä¸€è‡´æ€§ç»„ä»¶ï¼Œè¿™äº›è®°å½•å…·æœ‰ä¸åŒçš„ç›¸æœºè½¨è¿¹ï¼Œå¹¶é€šè¿‡è§†é¢‘å…‰ç…§é‡æ„æ¨¡å‹è·å¾—å…‰ç…§å˜åŒ–ã€‚æˆ‘ä»¬åœ¨æ­¤æ•°æ®ä¸Šå¾®è°ƒæœ€å…ˆè¿›çš„å¼€æºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥æä¾›å¼ºå¤§çš„å¤šè§†è§’èº«ä»½ä¿æŒã€ç²¾ç¡®çš„ç›¸æœºæ§åˆ¶å’Œå…‰ç…§é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜æ”¯æŒè™šæ‹Ÿåˆ¶ä½œçš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸¤ç§æ–¹æ³•è¿›è¡Œå¤šä¸»ä½“ç”Ÿæˆï¼šè”åˆè®­ç»ƒå’Œå™ªå£°æ··åˆï¼Œåè€…èƒ½å¤Ÿåœ¨æ¨ç†æ—¶æœ‰æ•ˆç»„åˆç‹¬ç«‹å®šåˆ¶çš„æ¨¡å‹ï¼›å®ƒè¿˜å®ç°äº†åœºæ™¯å’ŒçœŸå®ç”Ÿæ´»è§†é¢‘çš„å®šåˆ¶ï¼Œä»¥åŠåœ¨å®šåˆ¶æœŸé—´å¯¹è¿åŠ¨å’Œç©ºé—´å¸ƒå±€çš„æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè§†é¢‘è´¨é‡å¾—åˆ°æ”¹å–„ï¼Œä¸ªæ€§åŒ–å‡†ç¡®æ€§æ›´é«˜ï¼Œç›¸æœºæ§åˆ¶å’Œå…‰ç…§é€‚åº”æ€§å¾—åˆ°å¢å¼ºï¼Œä»è€Œæ¨åŠ¨äº†è§†é¢‘ç”Ÿæˆä¸è™šæ‹Ÿåˆ¶ä½œçš„é›†æˆã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä½äºï¼šhttps://eyeline-labs.github.io/Virtually-Beingã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿåˆ¶ä½œä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒå¤šè§†è§’è§’è‰²ä¸€è‡´æ€§å’Œå®ç°ç²¾ç¡®ç›¸æœºæ§åˆ¶æ–¹é¢ã€‚ç°æœ‰çš„æ–¹æ³•éš¾ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿è¯è§’è‰²åœ¨ä¸åŒè§†è§’ä¸‹çš„ä¸€è‡´æ€§ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹ç›¸æœºè¿åŠ¨çš„ç²¾ç»†æ§åˆ¶ï¼Œé™åˆ¶äº†å…¶åœ¨è™šæ‹Ÿåˆ¶ä½œä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®šåˆ¶åŒ–çš„æ•°æ®æµç¨‹æ¥å¾®è°ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°å¤šè§†è§’è§’è‰²ä¸€è‡´æ€§å’Œç›¸æœºæ§åˆ¶ã€‚é€šè¿‡ä½¿ç”¨å¤šè§†è§’è¡¨æ¼”æ•æ‰æ•°æ®ï¼Œå¹¶ç»“åˆ4Dé«˜æ–¯æº…å°„å’Œè§†é¢‘å…‰ç…§é‡æ„æŠ€æœ¯ï¼Œç”Ÿæˆå…·æœ‰ä¸°å¯Œè§†è§’å’Œå…‰ç…§å˜åŒ–çš„æ•°æ®é›†ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ§åˆ¶èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å¤šè§†è§’è¡¨æ¼”æ•æ‰ï¼šä½¿ç”¨å¤šç›¸æœºç³»ç»Ÿè®°å½•è§’è‰²çš„è¡¨æ¼”æ•°æ®ã€‚2) 4Dé«˜æ–¯æº…å°„ï¼ˆ4DGSï¼‰ï¼šåˆ©ç”¨4DGSæŠ€æœ¯ï¼Œå°†æ•æ‰åˆ°çš„è¡¨æ¼”æ•°æ®æ¸²æŸ“æˆå…·æœ‰ä¸åŒç›¸æœºè½¨è¿¹çš„è§†é¢‘ã€‚3) è§†é¢‘å…‰ç…§é‡æ„ï¼šä½¿ç”¨è§†é¢‘å…‰ç…§é‡æ„æ¨¡å‹ï¼Œä¸ºæ¸²æŸ“çš„è§†é¢‘æ·»åŠ å…‰ç…§å˜åŒ–ã€‚4) è§†é¢‘æ‰©æ•£æ¨¡å‹å¾®è°ƒï¼šä½¿ç”¨ç”Ÿæˆçš„æ•°æ®é›†å¾®è°ƒç°æœ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚5) å¤šä¸»ä½“ç”Ÿæˆï¼šæ”¯æŒè”åˆè®­ç»ƒå’Œå™ªå£°æ··åˆä¸¤ç§æ–¹æ³•ï¼Œå®ç°å¤šä¸»ä½“ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§å®šåˆ¶åŒ–çš„æ•°æ®æµç¨‹ï¼Œè¯¥æµç¨‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¤šè§†è§’ã€å¤šå…‰ç…§çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œæœ‰æ•ˆåœ°æå‡è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨è§’è‰²ä¸€è‡´æ€§å’Œç›¸æœºæ§åˆ¶æ–¹é¢çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå™ªå£°æ··åˆæ–¹æ³•èƒ½å¤Ÿåœ¨æ¨ç†æ—¶é«˜æ•ˆåœ°ç»„åˆç‹¬ç«‹å®šåˆ¶çš„æ¨¡å‹ï¼Œä¸ºå¤šä¸»ä½“ç”Ÿæˆæä¾›äº†æ–°çš„æ€è·¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®ç”Ÿæˆæ–¹é¢ï¼Œä½¿ç”¨äº†4Dé«˜æ–¯æº…å°„æŠ€æœ¯æ¥æ¸²æŸ“ä¸åŒç›¸æœºè½¨è¿¹çš„è§†é¢‘ï¼Œå¹¶ä½¿ç”¨è§†é¢‘å…‰ç…§é‡æ„æ¨¡å‹æ¥æ·»åŠ å…‰ç…§å˜åŒ–ã€‚åœ¨æ¨¡å‹å¾®è°ƒæ–¹é¢ï¼Œé€‰æ‹©äº†æœ€å…ˆè¿›çš„å¼€æºè§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹å¤šè§†è§’ä¸€è‡´æ€§å’Œç›¸æœºæ§åˆ¶è¿›è¡Œäº†ä¼˜åŒ–ã€‚åœ¨å¤šä¸»ä½“ç”Ÿæˆæ–¹é¢ï¼Œå™ªå£°æ··åˆæ–¹æ³•é€šè¿‡åœ¨å™ªå£°ç©ºé—´ä¸­æ··åˆä¸åŒè§’è‰²çš„ç‰¹å¾ï¼Œå®ç°äº†é«˜æ•ˆçš„ç»„åˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘è´¨é‡ã€ä¸ªæ€§åŒ–å‡†ç¡®æ€§ã€ç›¸æœºæ§åˆ¶å’Œå…‰ç…§é€‚åº”æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é€šè¿‡å®šé‡è¯„ä¼°å’Œå®šæ€§æ¯”è¾ƒï¼Œè¯æ˜äº†è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿåˆ¶ä½œä¸­çš„æ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨å¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè™šæ‹Ÿåˆ¶ä½œã€æ¸¸æˆå¼€å‘ã€ç”µå½±ç‰¹æ•ˆç­‰é¢†åŸŸã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼Œç”¨æˆ·å¯ä»¥è½»æ¾å®šåˆ¶å…·æœ‰å¤šè§†è§’ä¸€è‡´æ€§å’Œç›¸æœºå¯æ§æ€§çš„è™šæ‹Ÿè§’è‰²ï¼Œå¹¶å°†å…¶é›†æˆåˆ°å„ç§è™šæ‹Ÿåœºæ™¯ä¸­ï¼Œä»è€Œæå‡è™šæ‹Ÿåˆ¶ä½œçš„æ•ˆç‡å’Œè´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„æ•°å­—æ›¿èº«ï¼Œåº”ç”¨äºè¿œç¨‹ä¼šè®®ã€è™šæ‹Ÿç¤¾äº¤ç­‰åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: https://eyeline-labs.github.io/Virtually-Being.

