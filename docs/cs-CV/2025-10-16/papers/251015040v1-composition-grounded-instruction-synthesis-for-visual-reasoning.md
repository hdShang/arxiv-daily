---
layout: default
title: Composition-Grounded Instruction Synthesis for Visual Reasoning
---

# Composition-Grounded Instruction Synthesis for Visual Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15040" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15040v1</a>
  <a href="https://arxiv.org/pdf/2510.15040.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15040v1" onclick="toggleFavorite(this, '2510.15040v1', 'Composition-Grounded Instruction Synthesis for Visual Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyi Gu, Jiayuan Mao, Zhang-Wei Hong, Zhuoran Yu, Pengyuan Li, Dhiraj Joshi, Rogerio Feris, Zexue He

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCOGSæ¡†æ¶ä»¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `åˆæˆé—®ç­”å¯¹` `æ•°æ®é«˜æ•ˆæ€§` `å›¾è¡¨æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å› å­åˆ†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„é¢†åŸŸã€‚
2. COGSæ¡†æ¶é€šè¿‡å°†ç§å­é—®é¢˜åˆ†è§£ä¸ºåŸºæœ¬çš„æ„ŸçŸ¥å’Œæ¨ç†å› ç´ ï¼Œç³»ç»Ÿæ€§åœ°ç”Ÿæˆåˆæˆé—®ç­”å¯¹ï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOGSåœ¨å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æœªè§é—®é¢˜çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¤æ‚æ€§è¾ƒé«˜çš„é—®é¢˜ä¸Šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ³¨é‡Šéš¾ä»¥æ”¶é›†çš„é¢†åŸŸæ¨ç†èƒ½åŠ›æœ‰é™ã€‚æœ¬ç ”ç©¶èšç„¦äºäººå·¥å›¾åƒé¢†åŸŸï¼Œå¦‚å›¾è¡¨ã€æ¸²æŸ“æ–‡æ¡£å’Œç½‘é¡µï¼Œè¿™äº›é¢†åŸŸåœ¨å®è·µä¸­ä¸°å¯Œä½†ç¼ºä¹å¤§è§„æ¨¡äººç±»æ³¨é‡Šçš„æ¨ç†æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºäº†COGSï¼ˆCOmposition-Grounded instruction Synthesisï¼‰ï¼Œä¸€ä¸ªæ•°æ®é«˜æ•ˆçš„æ¡†æ¶ï¼Œé€šè¿‡å°‘é‡ç§å­é—®é¢˜èµ‹äºˆMLLMsé«˜çº§æ¨ç†èƒ½åŠ›ã€‚å…³é”®æ€æƒ³æ˜¯å°†æ¯ä¸ªç§å­é—®é¢˜åˆ†è§£ä¸ºåŸå§‹æ„ŸçŸ¥å’Œæ¨ç†å› ç´ ï¼Œç„¶åä¸æ–°å›¾åƒç³»ç»Ÿæ€§é‡ç»„ï¼Œä»¥ç”Ÿæˆå¤§é‡åˆæˆé—®ç­”å¯¹ã€‚å®éªŒè¡¨æ˜ï¼ŒCOGSåœ¨æœªè§é—®é¢˜ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å’Œç»„åˆæ€§é—®é¢˜ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®æ—¶çš„æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨äººå·¥å›¾åƒé¢†åŸŸã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„COGSæ¡†æ¶é€šè¿‡å°†ç§å­é—®é¢˜åˆ†è§£ä¸ºåŸºæœ¬çš„æ„ŸçŸ¥å’Œæ¨ç†å› ç´ ï¼Œåˆ©ç”¨è¿™äº›å› ç´ ä¸æ–°å›¾åƒè¿›è¡Œé‡ç»„ï¼Œç”Ÿæˆå¤§é‡åˆæˆé—®ç­”å¯¹ï¼Œä»è€Œæœ‰æ•ˆæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCOGSæ¡†æ¶åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œè¯†åˆ«å’Œåˆ†è§£ç§å­é—®é¢˜ï¼›å…¶æ¬¡ï¼Œç”Ÿæˆåˆæˆé—®ç­”å¯¹ï¼›æœ€åï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œåˆ©ç”¨å› å­çº§è¿‡ç¨‹å¥–åŠ±è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šCOGSçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ•°æ®é«˜æ•ˆæ€§å’Œç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡å› å­åˆ†è§£ä¸é‡ç»„ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæå‡äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒCOGSé‡‡ç”¨äº†å› å­çº§æ··åˆçš„ç§å­æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç»“åˆäº†å¤šç§æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥å®ç°æ›´å¥½çš„è·¨æ•°æ®é›†è¿ç§»èƒ½åŠ›ï¼Œé¿å…äº†æ•°æ®é›†ç‰¹å®šçš„è¿‡æ‹Ÿåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOGSåœ¨å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æœªè§é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å’Œç»„åˆæ€§é—®é¢˜ä¸Šï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ•°æ®é›†é—´çš„è¿ç§»èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†å¢å¼ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®åˆ†æã€ä¿¡æ¯å¯è§†åŒ–å’Œç½‘é¡µå†…å®¹ç†è§£ç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒCOGSæ¡†æ¶å¯ä»¥åœ¨æ•™è‚²ã€å•†ä¸šæ™ºèƒ½å’Œè‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„å†³ç­–æ”¯æŒç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Pretrained multi-modal large language models (MLLMs) demonstrate strong performance on diverse multimodal tasks, but remain limited in reasoning capabilities for domains where annotations are difficult to collect. In this work, we focus on artificial image domains such as charts, rendered documents, and webpages, which are abundant in practice yet lack large-scale human annotated reasoning datasets. We introduce COGS (COmposition-Grounded instruction Synthesis), a data-efficient framework for equipping MLLMs with advanced reasoning abilities from a small set of seed questions. The key idea is to decompose each seed question into primitive perception and reasoning factors, which can then be systematically recomposed with new images to generate large collections of synthetic question-answer pairs. Each generated question is paired with subquestions and intermediate answers, enabling reinforcement learning with factor-level process rewards. Experiments on chart reasoning show that COGS substantially improves performance on unseen questions, with the largest gains on reasoning-heavy and compositional questions. Moreover, training with a factor-level mixture of different seed data yields better transfer across multiple datasets, suggesting that COGS induces generalizable capabilities rather than dataset-specific overfitting. We further demonstrate that the framework extends beyond charts to other domains such as webpages.

