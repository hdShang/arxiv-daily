---
layout: default
title: Spatial Preference Rewarding for MLLMs Spatial Understanding
---

# Spatial Preference Rewarding for MLLMs Spatial Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14374" target="_blank" class="toolbar-btn">arXiv: 2510.14374v1</a>
    <a href="https://arxiv.org/pdf/2510.14374.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14374v1" 
            onclick="toggleFavorite(this, '2510.14374v1', 'Spatial Preference Rewarding for MLLMs Spatial Understanding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

**Â§áÊ≥®**: ICCV 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/hanqiu-hq/SPR)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Á©∫Èó¥ÂÅèÂ•ΩÂ•ñÂä±SPRÔºåÊèêÂçáMLLMÂú®ÁªÜÁ≤íÂ∫¶Á©∫Èó¥ÁêÜËß£‰∏äÁöÑËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Á©∫Èó¥ÁêÜËß£` `ÂÅèÂ•ΩÂ≠¶‰π†` `ÁõÆÊ†áÂÆö‰Ωç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMÂú®ÁªÜÁ≤íÂ∫¶Á©∫Èó¥ÊÑüÁü•‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÈöæ‰ª•ÁîüÊàêÁ≤æÁ°ÆÂå∫ÂüüÊèèËø∞ÊàñÂÆö‰ΩçÂØπË±°Ôºå‰∏îÁº∫‰πèÂØπÊ®°ÂûãÂìçÂ∫îÁöÑÁõ¥Êé•ÁõëÁù£„ÄÇ
2. SPRÈÄöËøáÂ•ñÂä±MLLMÂØπÁ≤æÁ°ÆÂØπË±°ÂÆö‰ΩçÁöÑËØ¶ÁªÜÂìçÂ∫îÔºåÊÉ©ÁΩöÊ®°Á≥äÊàñ‰∏çÂáÜÁ°ÆÁöÑÂìçÂ∫îÔºå‰ªéËÄåÊèêÂçáÁ©∫Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSPRÂú®ÂºïÁî®ÂíåÂÆö‰ΩçÂü∫ÂáÜÊµãËØï‰∏≠Ôºå‰ª•ËæÉÂ∞èÁöÑËÆ≠ÁªÉÂºÄÈîÄÊúâÊïàÊèêÂçá‰∫ÜMLLMÁöÑÁ©∫Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLMs)Âú®Á©∫Èó¥ÁêÜËß£ÊñπÈù¢Â±ïÁé∞Âá∫ÊΩúÂäõÔºå‰æãÂ¶ÇÂºïÁî®ÂíåÂÆö‰ΩçÂØπË±°ÊèèËø∞„ÄÇÂ∞ΩÁÆ°Â¶ÇÊ≠§ÔºåMLLMsÂú®ÁªÜÁ≤íÂ∫¶Á©∫Èó¥ÊÑüÁü•ËÉΩÂäõÊñπÈù¢‰ªçÊúâ‰∏çË∂≥ÔºåÂ¶ÇÁîüÊàêËØ¶ÁªÜÁöÑÂå∫ÂüüÊèèËø∞ÊàñÁ≤æÁ°ÆÂÆö‰ΩçÂØπË±°„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰ª¨Â∏∏Â∏∏Êó†Ê≥ïÊª°Ë∂≥Áî®Êà∑ÂØπÁªÜÁ≤íÂ∫¶Á©∫Èó¥ÁêÜËß£ÁöÑÈúÄÊ±Ç„ÄÇËøô‰∏™ÈóÆÈ¢òÂèØËÉΩÊ∫ê‰∫éÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠‰∫éË∞ÉÊï¥MLLMs‰ª•Âª∫Ê®°È¢ÑÂÖàÊ†áÊ≥®ÁöÑÊåá‰ª§Êï∞ÊçÆÊù•Ê≥®ÂÖ•Á©∫Èó¥Áü•ËØÜÔºåËÄåÁº∫‰πèÂØπMLLMsÂÆûÈôÖÂìçÂ∫îÁöÑÁõ¥Êé•ÁõëÁù£„ÄÇÊàë‰ª¨ÈÄöËøáÁ©∫Èó¥ÂÅèÂ•ΩÂ•ñÂä±(SPR)ÊñπÊ≥ïÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåËØ•ÊñπÊ≥ïÈÄöËøáÂ•ñÂä±MLLMsÂØπÁ≤æÁ°ÆÂØπË±°ÂÆö‰ΩçÁöÑËØ¶ÁªÜÂìçÂ∫îÔºåËÄå‰∏çÊòØÊ®°Á≥äÊàñ‰∏çÂáÜÁ°ÆÁöÑÂìçÂ∫îÔºå‰ªéËÄåÂ¢ûÂº∫MLLMsÁöÑÁ©∫Èó¥ËÉΩÂäõ„ÄÇSPR‰ªéMLLMs‰∏≠ÈöèÊú∫ÈÄâÊã©ÂõæÂÉèÂå∫ÂüüÂíåÂå∫ÂüüÊèèËø∞ÔºåÂºïÂÖ•ËØ≠‰πâÂíåÂÆö‰ΩçÂàÜÊï∞Êù•ÂÖ®Èù¢ËØÑ‰º∞MLLMÁîüÊàêÁöÑÊèèËø∞‰∏≠ÁöÑÊñáÊú¨Ë¥®ÈáèÂíåÂÆö‰ΩçË¥®Èáè„ÄÇÊàë‰ª¨Ëøò‰ΩøÁî®Êõ¥Â•ΩÁöÑÂÆö‰ΩçÁ≤æÂ∫¶Êù•ÊîπËøõMLLMÊèèËø∞ÔºåÂπ∂Â∞ÜÊúÄ‰Ω≥ÂæóÂàÜÁöÑÊîπËøõ‰∏éÊúÄ‰ΩéÂæóÂàÜÁöÑÂàùÂßãÊèèËø∞ÈÖçÂØπÔºå‰ª•ËøõË°åÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºå‰ªéËÄåÂ¢ûÂº∫‰∏éËßÜËßâËæìÂÖ•ÁöÑÁªÜÁ≤íÂ∫¶ÂØπÈΩê„ÄÇÂú®Ê†áÂáÜÂºïÁî®ÂíåÂÆö‰ΩçÂü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåSPR‰ª•ÊúÄÂ∞èÁöÑËÆ≠ÁªÉÂºÄÈîÄÊúâÊïàÂú∞ÊèêÈ´ò‰∫ÜMLLMÁöÑÁ©∫Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöMLLMÂú®ÁªÜÁ≤íÂ∫¶Á©∫Èó¥ÁêÜËß£ÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂÖ∑‰ΩìË°®Áé∞‰∏∫Êó†Ê≥ïÁîüÊàêËØ¶ÁªÜÁöÑÂå∫ÂüüÊèèËø∞ÊàñÁ≤æÁ°ÆÂÆö‰ΩçÂØπË±°ÔºåÂπ∂‰∏îÈöæ‰ª•Êª°Ë∂≥Áî®Êà∑ÂØπÁªÜÁ≤íÂ∫¶Á©∫Èó¥ÁêÜËß£ÁöÑÈúÄÊ±Ç„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÈ¢ÑÊ†áÊ≥®ÁöÑÊåá‰ª§Êï∞ÊçÆÊù•ËÆ≠ÁªÉMLLMÔºåÁº∫‰πèÂØπÊ®°ÂûãÂÆûÈôÖÂìçÂ∫îÁöÑÁõ¥Êé•ÁõëÁù£ÔºåÂØºËá¥Ê®°ÂûãÊó†Ê≥ïÊúâÊïàÂ≠¶‰π†Âà∞ÁªÜÁ≤íÂ∫¶ÁöÑÁ©∫Èó¥Áü•ËØÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSPRÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂ•ñÂä±Êú∫Âà∂Êù•ÂºïÂØºMLLMÁîüÊàêÊõ¥Á≤æÁ°Æ„ÄÅÊõ¥Á¨¶ÂêàÁî®Êà∑ÊúüÊúõÁöÑÁ©∫Èó¥ÊèèËø∞„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåSPR‰ºöËØÑ‰º∞MLLMÁîüÊàêÁöÑÊèèËø∞ÁöÑËØ≠‰πâË¥®ÈáèÂíåÂÆö‰ΩçË¥®ÈáèÔºåÂπ∂Ê†πÊçÆËØÑ‰º∞ÁªìÊûúÂØπÊ®°ÂûãËøõË°åÂ•ñÂä±ÊàñÊÉ©ÁΩö„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞Â¶Ç‰ΩïÁîüÊàêÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ËØ¶ÁªÜÁöÑÁ©∫Èó¥ÊèèËø∞Ôºå‰ªéËÄåÊèêÂçáÂÖ∂ÁªÜÁ≤íÂ∫¶Á©∫Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSPRÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ‰ªéMLLM‰∏≠ÈöèÊú∫ÈÄâÊã©ÂõæÂÉèÂå∫ÂüüÂíåÂå∫ÂüüÊèèËø∞Ôºõ2) ÂºïÂÖ•ËØ≠‰πâÂíåÂÆö‰ΩçÂàÜÊï∞Êù•ËØÑ‰º∞MLLMÁîüÊàêÁöÑÊèèËø∞ÁöÑÊñáÊú¨Ë¥®ÈáèÂíåÂÆö‰ΩçË¥®ÈáèÔºõ3) ‰ΩøÁî®Êõ¥Â•ΩÁöÑÂÆö‰ΩçÁ≤æÂ∫¶Êù•ÊîπËøõMLLMÊèèËø∞Ôºõ4) Â∞ÜÊúÄ‰Ω≥ÂæóÂàÜÁöÑÊîπËøõ‰∏éÊúÄ‰ΩéÂæóÂàÜÁöÑÂàùÂßãÊèèËø∞ÈÖçÂØπÔºå‰ª•ËøõË°åÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÈÄöËøá‰∏çÊñ≠Âú∞‰ºòÂåñMLLMÁöÑÂìçÂ∫îÔºå‰ªéËÄåÊèêÂçáÂÖ∂Á©∫Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSPRÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÁ©∫Èó¥ÂÅèÂ•ΩÂ•ñÂä±Êú∫Âà∂ÔºåÈÄöËøáÁõ¥Êé•ÁõëÁù£MLLMÁöÑÂìçÂ∫îÊù•ÊèêÂçáÂÖ∂ÁªÜÁ≤íÂ∫¶Á©∫Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSPR‰∏çÈúÄË¶Å‰æùËµñÂ§ßÈáèÁöÑÈ¢ÑÊ†áÊ≥®Êï∞ÊçÆÔºåËÄåÊòØÈÄöËøáÂ•ñÂä±Êú∫Âà∂Êù•ÂºïÂØºÊ®°ÂûãÂ≠¶‰π†Ôºå‰ªéËÄåÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÔºåÂπ∂ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSPRÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËØ≠‰πâÂíåÂÆö‰ΩçÂàÜÊï∞ÁöÑËÆ°ÁÆóÊñπÊ≥ïÔºåÁî®‰∫éËØÑ‰º∞MLLMÁîüÊàêÁöÑÊèèËø∞ÁöÑË¥®ÈáèÔºõ2) ÊîπËøõMLLMÊèèËø∞ÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÊèêÈ´òÊèèËø∞ÁöÑÂÆö‰ΩçÁ≤æÂ∫¶Ôºõ3) Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÁÆóÊ≥ïÔºåÁî®‰∫éËÆ≠ÁªÉMLLMÁîüÊàêÊõ¥Á¨¶ÂêàÁî®Êà∑ÊúüÊúõÁöÑÂìçÂ∫î„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞Á≠âÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SPRÂú®Ê†áÂáÜÂºïÁî®ÂíåÂÆö‰ΩçÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSPRËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òMLLMÁöÑÁ©∫Èó¥ÁêÜËß£ËÉΩÂäõÔºåÂπ∂‰∏îÂè™ÈúÄË¶ÅÊúÄÂ∞èÁöÑËÆ≠ÁªÉÂºÄÈîÄ„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÂØπÊØîÂü∫Á∫øÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÁöÑÂ±ïÁ§∫ÔºåËØÅÊòé‰∫ÜSPRÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÁõëÊéß„ÄÅÂõæÂÉèÁºñËæëÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫ÂØºËà™‰∏≠ÔºåÂèØ‰ª•Âà©Áî®ËØ•ÊäÄÊúØ‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÁêÜËß£‰∫∫Á±ªÁöÑÊåá‰ª§Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÂÆåÊàêÂØºËà™‰ªªÂä°„ÄÇÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåÂèØ‰ª•ÊèêÈ´òËΩ¶ËæÜÂØπÂë®Âõ¥ÁéØÂ¢ÉÁöÑÊÑüÁü•ËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´òÈ©æÈ©∂ÂÆâÂÖ®ÊÄß„ÄÇÂú®Êô∫ËÉΩÁõëÊéß‰∏≠ÔºåÂèØ‰ª•ÂÆûÁé∞ÂØπÁâπÂÆöÂå∫ÂüüÁöÑÁ≤æÁªÜÂåñÁõëÊéßÔºå‰ªéËÄåÊèêÈ´òÁõëÊéßÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models~(MLLMs) have demonstrated promising spatial understanding capabilities, such as referencing and grounding object descriptions. Despite their successes, MLLMs still fall short in fine-grained spatial perception abilities, such as generating detailed region descriptions or accurately localizing objects. Additionally, they often fail to respond to the user's requirements for desired fine-grained spatial understanding. This issue might arise because existing approaches primarily focus on tuning MLLMs to model pre-annotated instruction data to inject spatial knowledge, without direct supervision of MLLMs' actual responses. We address this issue by SPR, a Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial capabilities by rewarding MLLMs' detailed responses with precise object localization over vague or inaccurate responses. With randomly selected image regions and region descriptions from MLLMs, SPR introduces semantic and localization scores to comprehensively evaluate the text quality and localization quality in MLLM-generated descriptions. We also refine the MLLM descriptions with better localization accuracy and pair the best-scored refinement with the initial descriptions of the lowest score for direct preference optimization, thereby enhancing fine-grained alignment with visual input. Extensive experiments over standard referring and grounding benchmarks show that SPR improves MLLM spatial understanding capabilities effectively with minimal overhead in training. Data and code will be released at https://github.com/hanqiu-hq/SPR

