---
layout: default
title: Spatial Preference Rewarding for MLLMs Spatial Understanding
---

# Spatial Preference Rewarding for MLLMs Spatial Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14374" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14374v1</a>
  <a href="https://arxiv.org/pdf/2510.14374.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14374v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14374v1', 'Spatial Preference Rewarding for MLLMs Spatial Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

**å¤‡æ³¨**: ICCV 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hanqiu-hq/SPR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç©ºé—´åå¥½å¥–åŠ±SPRï¼Œæå‡MLLMåœ¨ç»†ç²’åº¦ç©ºé—´ç†è§£ä¸Šçš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç©ºé—´ç†è§£` `åå¥½å­¦ä¹ ` `ç›®æ ‡å®šä½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨ç»†ç²’åº¦ç©ºé—´æ„ŸçŸ¥ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ç”Ÿæˆç²¾ç¡®åŒºåŸŸæè¿°æˆ–å®šä½å¯¹è±¡ï¼Œä¸”ç¼ºä¹å¯¹æ¨¡å‹å“åº”çš„ç›´æ¥ç›‘ç£ã€‚
2. SPRé€šè¿‡å¥–åŠ±MLLMå¯¹ç²¾ç¡®å¯¹è±¡å®šä½çš„è¯¦ç»†å“åº”ï¼Œæƒ©ç½šæ¨¡ç³Šæˆ–ä¸å‡†ç¡®çš„å“åº”ï¼Œä»è€Œæå‡ç©ºé—´ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSPRåœ¨å¼•ç”¨å’Œå®šä½åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»¥è¾ƒå°çš„è®­ç»ƒå¼€é”€æœ‰æ•ˆæå‡äº†MLLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ç©ºé—´ç†è§£æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä¾‹å¦‚å¼•ç”¨å’Œå®šä½å¯¹è±¡æè¿°ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒMLLMsåœ¨ç»†ç²’åº¦ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›æ–¹é¢ä»æœ‰ä¸è¶³ï¼Œå¦‚ç”Ÿæˆè¯¦ç»†çš„åŒºåŸŸæè¿°æˆ–ç²¾ç¡®å®šä½å¯¹è±¡ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¸¸å¸¸æ— æ³•æ»¡è¶³ç”¨æˆ·å¯¹ç»†ç²’åº¦ç©ºé—´ç†è§£çš„éœ€æ±‚ã€‚è¿™ä¸ªé—®é¢˜å¯èƒ½æºäºç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºè°ƒæ•´MLLMsä»¥å»ºæ¨¡é¢„å…ˆæ ‡æ³¨çš„æŒ‡ä»¤æ•°æ®æ¥æ³¨å…¥ç©ºé—´çŸ¥è¯†ï¼Œè€Œç¼ºä¹å¯¹MLLMså®é™…å“åº”çš„ç›´æ¥ç›‘ç£ã€‚æˆ‘ä»¬é€šè¿‡ç©ºé—´åå¥½å¥–åŠ±(SPR)æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¥–åŠ±MLLMså¯¹ç²¾ç¡®å¯¹è±¡å®šä½çš„è¯¦ç»†å“åº”ï¼Œè€Œä¸æ˜¯æ¨¡ç³Šæˆ–ä¸å‡†ç¡®çš„å“åº”ï¼Œä»è€Œå¢å¼ºMLLMsçš„ç©ºé—´èƒ½åŠ›ã€‚SPRä»MLLMsä¸­éšæœºé€‰æ‹©å›¾åƒåŒºåŸŸå’ŒåŒºåŸŸæè¿°ï¼Œå¼•å…¥è¯­ä¹‰å’Œå®šä½åˆ†æ•°æ¥å…¨é¢è¯„ä¼°MLLMç”Ÿæˆçš„æè¿°ä¸­çš„æ–‡æœ¬è´¨é‡å’Œå®šä½è´¨é‡ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æ›´å¥½çš„å®šä½ç²¾åº¦æ¥æ”¹è¿›MLLMæè¿°ï¼Œå¹¶å°†æœ€ä½³å¾—åˆ†çš„æ”¹è¿›ä¸æœ€ä½å¾—åˆ†çš„åˆå§‹æè¿°é…å¯¹ï¼Œä»¥è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œä»è€Œå¢å¼ºä¸è§†è§‰è¾“å…¥çš„ç»†ç²’åº¦å¯¹é½ã€‚åœ¨æ ‡å‡†å¼•ç”¨å’Œå®šä½åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSPRä»¥æœ€å°çš„è®­ç»ƒå¼€é”€æœ‰æ•ˆåœ°æé«˜äº†MLLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šMLLMåœ¨ç»†ç²’åº¦ç©ºé—´ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå…·ä½“è¡¨ç°ä¸ºæ— æ³•ç”Ÿæˆè¯¦ç»†çš„åŒºåŸŸæè¿°æˆ–ç²¾ç¡®å®šä½å¯¹è±¡ï¼Œå¹¶ä¸”éš¾ä»¥æ»¡è¶³ç”¨æˆ·å¯¹ç»†ç²’åº¦ç©ºé—´ç†è§£çš„éœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé¢„æ ‡æ³¨çš„æŒ‡ä»¤æ•°æ®æ¥è®­ç»ƒMLLMï¼Œç¼ºä¹å¯¹æ¨¡å‹å®é™…å“åº”çš„ç›´æ¥ç›‘ç£ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆå­¦ä¹ åˆ°ç»†ç²’åº¦çš„ç©ºé—´çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSPRçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¥–åŠ±æœºåˆ¶æ¥å¼•å¯¼MLLMç”Ÿæˆæ›´ç²¾ç¡®ã€æ›´ç¬¦åˆç”¨æˆ·æœŸæœ›çš„ç©ºé—´æè¿°ã€‚å…·ä½“æ¥è¯´ï¼ŒSPRä¼šè¯„ä¼°MLLMç”Ÿæˆçš„æè¿°çš„è¯­ä¹‰è´¨é‡å’Œå®šä½è´¨é‡ï¼Œå¹¶æ ¹æ®è¯„ä¼°ç»“æœå¯¹æ¨¡å‹è¿›è¡Œå¥–åŠ±æˆ–æƒ©ç½šã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¦‚ä½•ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´è¯¦ç»†çš„ç©ºé—´æè¿°ï¼Œä»è€Œæå‡å…¶ç»†ç²’åº¦ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPRæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä»MLLMä¸­éšæœºé€‰æ‹©å›¾åƒåŒºåŸŸå’ŒåŒºåŸŸæè¿°ï¼›2) å¼•å…¥è¯­ä¹‰å’Œå®šä½åˆ†æ•°æ¥è¯„ä¼°MLLMç”Ÿæˆçš„æè¿°çš„æ–‡æœ¬è´¨é‡å’Œå®šä½è´¨é‡ï¼›3) ä½¿ç”¨æ›´å¥½çš„å®šä½ç²¾åº¦æ¥æ”¹è¿›MLLMæè¿°ï¼›4) å°†æœ€ä½³å¾—åˆ†çš„æ”¹è¿›ä¸æœ€ä½å¾—åˆ†çš„åˆå§‹æè¿°é…å¯¹ï¼Œä»¥è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡ä¸æ–­åœ°ä¼˜åŒ–MLLMçš„å“åº”ï¼Œä»è€Œæå‡å…¶ç©ºé—´ç†è§£èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šSPRçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†ç©ºé—´åå¥½å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡ç›´æ¥ç›‘ç£MLLMçš„å“åº”æ¥æå‡å…¶ç»†ç²’åº¦ç©ºé—´ç†è§£èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSPRä¸éœ€è¦ä¾èµ–å¤§é‡çš„é¢„æ ‡æ³¨æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡å¥–åŠ±æœºåˆ¶æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šSPRçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è¯­ä¹‰å’Œå®šä½åˆ†æ•°çš„è®¡ç®—æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°MLLMç”Ÿæˆçš„æè¿°çš„è´¨é‡ï¼›2) æ”¹è¿›MLLMæè¿°çš„æ–¹æ³•ï¼Œç”¨äºæé«˜æè¿°çš„å®šä½ç²¾åº¦ï¼›3) ç›´æ¥åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºè®­ç»ƒMLLMç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æœŸæœ›çš„å“åº”ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SPRåœ¨æ ‡å‡†å¼•ç”¨å’Œå®šä½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPRèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜MLLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸”åªéœ€è¦æœ€å°çš„è®­ç»ƒå¼€é”€ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å±•ç¤ºï¼Œè¯æ˜äº†SPRçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€å›¾åƒç¼–è¾‘ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯ä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£äººç±»çš„æŒ‡ä»¤ï¼Œä»è€Œæ›´å¥½åœ°å®Œæˆå¯¼èˆªä»»åŠ¡ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥æé«˜è½¦è¾†å¯¹å‘¨å›´ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥å®ç°å¯¹ç‰¹å®šåŒºåŸŸçš„ç²¾ç»†åŒ–ç›‘æ§ï¼Œä»è€Œæé«˜ç›‘æ§æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models~(MLLMs) have demonstrated promising spatial understanding capabilities, such as referencing and grounding object descriptions. Despite their successes, MLLMs still fall short in fine-grained spatial perception abilities, such as generating detailed region descriptions or accurately localizing objects. Additionally, they often fail to respond to the user's requirements for desired fine-grained spatial understanding. This issue might arise because existing approaches primarily focus on tuning MLLMs to model pre-annotated instruction data to inject spatial knowledge, without direct supervision of MLLMs' actual responses. We address this issue by SPR, a Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial capabilities by rewarding MLLMs' detailed responses with precise object localization over vague or inaccurate responses. With randomly selected image regions and region descriptions from MLLMs, SPR introduces semantic and localization scores to comprehensively evaluate the text quality and localization quality in MLLM-generated descriptions. We also refine the MLLM descriptions with better localization accuracy and pair the best-scored refinement with the initial descriptions of the lowest score for direct preference optimization, thereby enhancing fine-grained alignment with visual input. Extensive experiments over standard referring and grounding benchmarks show that SPR improves MLLM spatial understanding capabilities effectively with minimal overhead in training. Data and code will be released at https://github.com/hanqiu-hq/SPR

