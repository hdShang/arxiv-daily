---
layout: default
title: Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection
---

# Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14896" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14896v1</a>
  <a href="https://arxiv.org/pdf/2510.14896.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14896v1" onclick="toggleFavorite(this, '2510.14896v1', 'Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šæ¨¡æ€LLMæè¿°çš„åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œæå‡å¤æ‚å¼‚å¸¸æ£€æµ‹èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¼‚å¸¸æ£€æµ‹` `åŠç›‘ç£å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¯è§£é‡Šæ€§` `å¯¹è±¡äº¤äº’` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŠç›‘ç£VADæ–¹æ³•åœ¨å¤„ç†å¤æ‚äº¤äº’å¼‚å¸¸æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚
2. åˆ©ç”¨MLLMæå–è§†é¢‘ä¸­å¯¹è±¡æ´»åŠ¨å’Œäº¤äº’çš„æ–‡æœ¬æè¿°ï¼Œä½œä¸ºé«˜çº§è¡¨ç¤ºè¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚äº¤äº’å¼‚å¸¸æ£€æµ‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨æ— äº¤äº’å¼‚å¸¸æ•°æ®é›†ä¸Šè¾¾åˆ°SOTAã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹(VAD)æ–¹æ³•é€šå¸¸éš¾ä»¥æ£€æµ‹æ¶‰åŠå¯¹è±¡äº¤äº’çš„å¤æ‚å¼‚å¸¸ï¼Œå¹¶ä¸”æ™®éç¼ºä¹å¯è§£é‡Šæ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„VADæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)ã€‚ä¸ä¹‹å‰åŸºäºMLLMçš„ç›´æ¥åœ¨å¸§çº§åˆ«è¿›è¡Œå¼‚å¸¸åˆ¤æ–­çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¾§é‡äºæå–å’Œè§£é‡Šå¯¹è±¡æ´»åŠ¨å’Œéšæ—¶é—´æ¨ç§»çš„äº¤äº’ã€‚é€šè¿‡ä½¿ç”¨å¯¹è±¡å¯¹åœ¨ä¸åŒæ—¶åˆ»çš„è§†è§‰è¾“å…¥æŸ¥è¯¢MLLMï¼Œæˆ‘ä»¬ç”Ÿæˆäº†æ­£å¸¸è§†é¢‘ä¸­æ´»åŠ¨å’Œäº¤äº’çš„æ–‡æœ¬æè¿°ã€‚è¿™äº›æ–‡æœ¬æè¿°å……å½“è§†é¢‘ä¸­å¯¹è±¡æ´»åŠ¨å’Œäº¤äº’çš„é«˜çº§è¡¨ç¤ºã€‚é€šè¿‡å°†æµ‹è¯•æ—¶çš„æ–‡æœ¬æè¿°ä¸æ­£å¸¸è®­ç»ƒè§†é¢‘ä¸­çš„æ–‡æœ¬æè¿°è¿›è¡Œæ¯”è¾ƒï¼Œæ¥æ£€æµ‹å¼‚å¸¸ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ¬èº«å°±æä¾›äº†å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”å¯ä»¥ä¸è®¸å¤šä¼ ç»Ÿçš„VADæ–¹æ³•ç›¸ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå…¶å¯è§£é‡Šæ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æœ‰æ•ˆåœ°æ£€æµ‹äº†åŸºäºå¤æ‚äº¤äº’çš„å¼‚å¸¸ï¼Œè€Œä¸”åœ¨æ²¡æœ‰äº¤äº’å¼‚å¸¸çš„æ•°æ®é›†ä¸Šä¹Ÿå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨æ£€æµ‹æ¶‰åŠå¯¹è±¡ä¹‹é—´å¤æ‚äº¤äº’çš„å¼‚å¸¸æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹å¼‚å¸¸åŸå› çš„è§£é‡Šï¼Œä½¿å¾—ç”¨æˆ·éš¾ä»¥ç†è§£å’Œä¿¡ä»»æ£€æµ‹ç»“æœã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å¤æ‚äº¤äº’å¼‚å¸¸å¹¶æä¾›å¯è§£é‡Šæ€§çš„VADæ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç†è§£è§†é¢‘å†…å®¹ï¼Œå¹¶ç”Ÿæˆå…³äºå¯¹è±¡æ´»åŠ¨å’Œäº¤äº’çš„æ–‡æœ¬æè¿°ã€‚è¿™äº›æ–‡æœ¬æè¿°ä½œä¸ºè§†é¢‘å†…å®¹çš„é«˜çº§è¯­ä¹‰è¡¨ç¤ºï¼Œå¯ä»¥ç”¨äºåŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸è¡Œä¸ºã€‚é€šè¿‡æ¯”è¾ƒæµ‹è¯•è§†é¢‘ä¸æ­£å¸¸è§†é¢‘çš„æ–‡æœ¬æè¿°ï¼Œå¯ä»¥æ£€æµ‹å‡ºå¼‚å¸¸äº‹ä»¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **å¯¹è±¡æ£€æµ‹ä¸è·Ÿè¸ª**ï¼šä»è§†é¢‘å¸§ä¸­æ£€æµ‹å’Œè·Ÿè¸ªå¯¹è±¡ã€‚2) **å¯¹è±¡å¯¹æå–**ï¼šé€‰æ‹©è§†é¢‘ä¸­éœ€è¦åˆ†æçš„å¯¹è±¡å¯¹ã€‚3) **MLLMæŸ¥è¯¢**ï¼šä½¿ç”¨å¯¹è±¡å¯¹åœ¨ä¸åŒæ—¶åˆ»çš„è§†è§‰ä¿¡æ¯æŸ¥è¯¢MLLMï¼Œç”Ÿæˆå…³äºå¯¹è±¡æ´»åŠ¨å’Œäº¤äº’çš„æ–‡æœ¬æè¿°ã€‚4) **å¼‚å¸¸æ£€æµ‹**ï¼šæ¯”è¾ƒæµ‹è¯•è§†é¢‘çš„æ–‡æœ¬æè¿°ä¸æ­£å¸¸è§†é¢‘çš„æ–‡æœ¬æè¿°ï¼Œåˆ©ç”¨ç›¸ä¼¼åº¦åº¦é‡æˆ–åˆ†ç±»å™¨åˆ¤æ–­æ˜¯å¦å­˜åœ¨å¼‚å¸¸ã€‚5) **å¯è§£é‡Šæ€§**ï¼šé€šè¿‡åˆ†æMLLMç”Ÿæˆçš„æ–‡æœ¬æè¿°ï¼Œæä¾›å¯¹å¼‚å¸¸åŸå› çš„è§£é‡Šã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨MLLMç”Ÿæˆè§†é¢‘å†…å®¹çš„é«˜çº§æ–‡æœ¬æè¿°ï¼Œä»è€Œå°†è§†è§‰å¼‚å¸¸æ£€æµ‹é—®é¢˜è½¬åŒ–ä¸ºæ–‡æœ¬æ¯”è¾ƒé—®é¢˜ã€‚ä¸ç›´æ¥ä½¿ç”¨MLLMè¿›è¡Œå¼‚å¸¸åˆ¤æ–­çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¾§é‡äºæå–å’Œè§£é‡Šå¯¹è±¡æ´»åŠ¨å’Œäº¤äº’ï¼Œä»è€Œæé«˜äº†æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•é€‰æ‹©åˆé€‚çš„MLLMæ¨¡å‹ã€‚2) å¦‚ä½•è®¾è®¡MLLMçš„æŸ¥è¯¢æ–¹å¼ï¼Œä»¥è·å¾—å‡†ç¡®å’Œä¸°å¯Œçš„æ–‡æœ¬æè¿°ã€‚3) å¦‚ä½•å®šä¹‰æ–‡æœ¬æè¿°ä¹‹é—´çš„ç›¸ä¼¼åº¦åº¦é‡ï¼Œä»¥æœ‰æ•ˆåŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸è¡Œä¸ºã€‚4) å¦‚ä½•å°†è¯¥æ–¹æ³•ä¸ä¼ ç»Ÿçš„VADæ–¹æ³•ç›¸ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜å…¶ä¸ä»…èƒ½æœ‰æ•ˆæ£€æµ‹åŸºäºå¤æ‚äº¤äº’çš„å¼‚å¸¸ï¼Œè€Œä¸”åœ¨æ²¡æœ‰äº¤äº’å¼‚å¸¸çš„æ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨æ‘˜è¦ä¸­æœªæåŠï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚è¯¥æ–¹æ³•çš„å¯è§£é‡Šæ€§æ˜¯å¦ä¸€ä¸ªäº®ç‚¹ï¼Œé€šè¿‡åˆ†æMLLMç”Ÿæˆçš„æ–‡æœ¬æè¿°ï¼Œå¯ä»¥æä¾›å¯¹å¼‚å¸¸åŸå› çš„è§£é‡Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€å·¥ä¸šå®‰å…¨ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥æ£€æµ‹å¼‚å¸¸äººç¾¤è¡Œä¸ºæˆ–ç‰©ä½“ç§»åŠ¨ï¼›åœ¨å·¥ä¸šå®‰å…¨ä¸­ï¼Œå¯ä»¥æ£€æµ‹è®¾å¤‡æ•…éšœæˆ–è¿è§„æ“ä½œï¼›åœ¨åŒ»ç–—è¯Šæ–­ä¸­ï¼Œå¯ä»¥è¾…åŠ©åŒ»ç”Ÿåˆ¤æ–­ç—…ç¶æˆ–å¼‚å¸¸ç”Ÿç†æŒ‡æ ‡ã€‚è¯¥æ–¹æ³•çš„å¯è§£é‡Šæ€§æœ‰åŠ©äºç”¨æˆ·ç†è§£å¼‚å¸¸æ£€æµ‹ç»“æœï¼Œä»è€Œæé«˜å†³ç­–æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.

