---
layout: default
title: "MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning"
---

# MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15026" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15026v1</a>
  <a href="https://arxiv.org/pdf/2510.15026.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15026v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15026v1', 'MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mattia Segu, Marta Tintore Gazulla, Yongqin Xian, Luc Van Gool, Federico Tombari

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

**å¤‡æ³¨**: ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MOBIUSï¼šé€šè¿‡å¤šæ¨¡æ€ç“¶é¢ˆèåˆä¸æ ¡å‡†è§£ç å™¨å‰ªæå®ç°Big-to-Mobileé€šç”¨å®ä¾‹åˆ†å‰²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®ä¾‹åˆ†å‰²` `æ¨¡å‹å‹ç¼©` `ç§»åŠ¨è®¾å¤‡` `å¤šæ¨¡æ€èåˆ` `è§£ç å™¨å‰ªæ` `ä¸ç¡®å®šæ€§æ ¡å‡†` `ç“¶é¢ˆç»“æ„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å®ä¾‹åˆ†å‰²æ¨¡å‹è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚
2. MOBIUSé€šè¿‡ç“¶é¢ˆåƒç´ è§£ç å™¨ã€è¯­è¨€å¼•å¯¼çš„ä¸ç¡®å®šæ€§æ ¡å‡†æŸå¤±å’Œç»Ÿä¸€è®­ç»ƒç­–ç•¥ï¼Œé™ä½è®­ç»ƒå’Œæ¨ç†éœ€æ±‚ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMOBIUSåœ¨æ˜¾è‘—é™ä½è®¡ç®—é‡çš„åŒæ—¶ï¼Œä¿æŒäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°äº†é«˜æ•ˆåˆ†å‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMOBIUSï¼Œä¸€ä¸ªé¢å‘é€šç”¨å®ä¾‹åˆ†å‰²çš„åŸºç¡€æ¨¡å‹å®¶æ—ï¼Œæ—¨åœ¨å®ç°Paretoæœ€ä¼˜çš„ç¼©æ”¾ï¼Œä»¥æ”¯æŒä»é«˜ç«¯åŠ é€Ÿå™¨åˆ°ç§»åŠ¨ç¡¬ä»¶çš„éƒ¨ç½²ã€‚ä¸ºäº†é™ä½è®­ç»ƒå’Œæ¨ç†éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†ï¼šï¼ˆiï¼‰ç”¨äºé«˜æ•ˆå¤šå°ºåº¦å’Œå¤šæ¨¡æ€èåˆçš„ç“¶é¢ˆåƒç´ è§£ç å™¨ï¼›ï¼ˆiiï¼‰ç”¨äºè‡ªé€‚åº”è§£ç å™¨å‰ªæçš„è¯­è¨€å¼•å¯¼çš„ä¸ç¡®å®šæ€§æ ¡å‡†æŸå¤±ï¼›ï¼ˆiiiï¼‰ç®€åŒ–çš„ç»Ÿä¸€è®­ç»ƒç­–ç•¥ã€‚ä¸ä»¥ç‰ºç‰²ç²¾åº¦ä¸ºä»£ä»·æ¥é™ä½å¤æ‚æ€§çš„é«˜æ•ˆåŸºçº¿ä¸åŒï¼ŒMOBIUSå°†åƒç´ å’ŒTransformerè§£ç å™¨çš„FLOPsåˆ†åˆ«é™ä½é«˜è¾¾55%å’Œ75%ï¼ŒåŒæ—¶åœ¨ä»…ä¸‰åˆ†ä¹‹ä¸€çš„è®­ç»ƒè¿­ä»£ä¸­ä¿æŒäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚MOBIUSä¸ºé«˜æ€§èƒ½è®¡ç®—å¹³å°å’Œç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆåˆ†å‰²å»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å®ä¾‹åˆ†å‰²æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„å¤§å‹æ¨¡å‹ï¼Œè™½ç„¶åœ¨ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶åºå¤§çš„è®¡ç®—é‡å’Œå†…å­˜éœ€æ±‚ä½¿å…¶éš¾ä»¥åœ¨ç§»åŠ¨è®¾å¤‡ç­‰èµ„æºå—é™çš„å¹³å°ä¸Šéƒ¨ç½²ã€‚ç°æœ‰çš„æ¨¡å‹å‹ç¼©æ–¹æ³•é€šå¸¸ä»¥ç‰ºç‰²ç²¾åº¦ä¸ºä»£ä»·æ¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæ— æ³•åœ¨ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMOBIUSçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è®¾è®¡é«˜æ•ˆçš„æ¨¡å—å’Œè®­ç»ƒç­–ç•¥ï¼Œåœ¨ä¸æ˜¾è‘—é™ä½ç²¾åº¦çš„å‰æä¸‹ï¼Œå¤§å¹…é™ä½æ¨¡å‹çš„è®¡ç®—é‡å’Œå†…å­˜éœ€æ±‚ï¼Œä»è€Œå®ç°æ¨¡å‹åœ¨ä¸åŒç¡¬ä»¶å¹³å°ä¸Šçš„çµæ´»éƒ¨ç½²ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å¤šæ¨¡æ€ç“¶é¢ˆèåˆå‡å°‘ç‰¹å¾ç»´åº¦ï¼Œå¹¶é€šè¿‡è¯­è¨€å¼•å¯¼çš„ä¸ç¡®å®šæ€§æ ¡å‡†æŸå¤±å®ç°è‡ªé€‚åº”çš„è§£ç å™¨å‰ªæã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMOBIUSçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š(1) Backboneç½‘ç»œï¼šç”¨äºæå–å›¾åƒç‰¹å¾ã€‚(2) ç“¶é¢ˆåƒç´ è§£ç å™¨ï¼šç”¨äºé«˜æ•ˆåœ°èåˆå¤šå°ºåº¦å’Œå¤šæ¨¡æ€ç‰¹å¾ï¼Œé™ä½ç‰¹å¾ç»´åº¦ã€‚(3) åˆ†å‰²å¤´ï¼šç”¨äºé¢„æµ‹å®ä¾‹åˆ†å‰²ç»“æœã€‚(4) è¯­è¨€å¼•å¯¼çš„ä¸ç¡®å®šæ€§æ ¡å‡†æ¨¡å—ï¼šç”¨äºè¯„ä¼°è§£ç å™¨çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æŒ‡å¯¼è§£ç å™¨çš„å‰ªæã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹é‡‡ç”¨ç»Ÿä¸€çš„è®­ç»ƒç­–ç•¥ï¼Œç®€åŒ–äº†è®­ç»ƒæµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMOBIUSçš„å…³é”®åˆ›æ–°åœ¨äºï¼š(1) ç“¶é¢ˆåƒç´ è§£ç å™¨ï¼šé€šè¿‡å¼•å…¥ç“¶é¢ˆç»“æ„ï¼Œé™ä½äº†ç‰¹å¾ç»´åº¦ï¼Œä»è€Œå‡å°‘äº†è®¡ç®—é‡ã€‚(2) è¯­è¨€å¼•å¯¼çš„ä¸ç¡®å®šæ€§æ ¡å‡†æŸå¤±ï¼šåˆ©ç”¨è¯­è¨€ä¿¡æ¯æ¥è¯„ä¼°è§£ç å™¨çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æŒ‡å¯¼è§£ç å™¨çš„å‰ªæï¼Œä»è€Œå®ç°äº†è‡ªé€‚åº”çš„è§£ç å™¨å‹ç¼©ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMOBIUSèƒ½å¤Ÿåœ¨ä¿æŒç²¾åº¦çš„å‰æä¸‹ï¼Œæ›´æœ‰æ•ˆåœ°é™ä½è®¡ç®—é‡ã€‚

**å…³é”®è®¾è®¡**ï¼š(1) ç“¶é¢ˆåƒç´ è§£ç å™¨ï¼šé‡‡ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä½œä¸ºç“¶é¢ˆç»“æ„ï¼Œå°†é«˜ç»´ç‰¹å¾æ˜ å°„åˆ°ä½ç»´ç©ºé—´ã€‚(2) è¯­è¨€å¼•å¯¼çš„ä¸ç¡®å®šæ€§æ ¡å‡†æŸå¤±ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥æå–å›¾åƒæè¿°çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨äºè¯„ä¼°è§£ç å™¨çš„ä¸ç¡®å®šæ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨é¼“åŠ±æ¨¡å‹å­¦ä¹ åˆ°æ›´å‡†ç¡®çš„åˆ†å‰²ç»“æœï¼Œå¹¶é™ä½å¯¹ä¸ç¡®å®šåŒºåŸŸçš„è¿‡åº¦å…³æ³¨ã€‚(3) ç»Ÿä¸€è®­ç»ƒç­–ç•¥ï¼šé‡‡ç”¨ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹å¼ï¼Œç®€åŒ–äº†è®­ç»ƒæµç¨‹ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MOBIUSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„é«˜æ•ˆå®ä¾‹åˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼ŒMOBIUSåœ¨æ˜¾è‘—é™ä½è®¡ç®—é‡çš„åŒæ—¶ï¼Œä¿æŒäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒMOBIUSå°†åƒç´ å’ŒTransformerè§£ç å™¨çš„FLOPsåˆ†åˆ«é™ä½é«˜è¾¾55%å’Œ75%ï¼ŒåŒæ—¶åœ¨ä»…ä¸‰åˆ†ä¹‹ä¸€çš„è®­ç»ƒè¿­ä»£ä¸­ä¿æŒäº†ç›¸å½“ç”šè‡³æ›´å¥½çš„ç²¾åº¦ã€‚è¿™è¡¨æ˜MOBIUSåœ¨æ•ˆç‡å’Œç²¾åº¦ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MOBIUSå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ç§»åŠ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚å®ƒèƒ½å¤Ÿä½¿è¿™äº›åº”ç”¨åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå®ç°é«˜æ€§èƒ½çš„å®ä¾‹åˆ†å‰²ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒMOBIUSè¿˜å¯ä»¥åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€é¥æ„Ÿå›¾åƒå¤„ç†ç­‰é¢†åŸŸï¼Œä¸ºè¿™äº›é¢†åŸŸæä¾›é«˜æ•ˆçš„å›¾åƒåˆ†å‰²è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Scaling up model size and training data has advanced foundation models for instance-level perception, achieving state-of-the-art in-domain and zero-shot performance across object detection and segmentation. However, their high computational cost limits adoption on resource-constrained platforms. We first examine the limitations of existing architectures in enabling efficient edge deployment without compromising performance. We then introduce MOBIUS, a family of foundation models for universal instance segmentation, designed for Pareto-optimal downscaling to support deployment across devices ranging from high-end accelerators to mobile hardware. To reduce training and inference demands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale and multi-modal fusion, (ii) a language-guided uncertainty calibration loss for adaptive decoder pruning, and (iii) a streamlined, unified training strategy. Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUS reduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively, while maintaining state-of-the-art performance in just a third of the training iterations. MOBIUS establishes a new benchmark for efficient segmentation on both high-performance computing platforms and mobile devices.

