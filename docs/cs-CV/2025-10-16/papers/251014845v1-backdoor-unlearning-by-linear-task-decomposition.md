---
layout: default
title: Backdoor Unlearning by Linear Task Decomposition
---

# Backdoor Unlearning by Linear Task Decomposition

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14845" target="_blank" class="toolbar-btn">arXiv: 2510.14845v1</a>
    <a href="https://arxiv.org/pdf/2510.14845.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14845v1" 
            onclick="toggleFavorite(this, '2510.14845v1', 'Backdoor Unlearning by Linear Task Decomposition')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard

**ÂàÜÁ±ª**: cs.LG, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÁ∫øÊÄß‰ªªÂä°ÂàÜËß£ÁöÑÂêéÈó®ÊîªÂáªËß£Â≠¶‰π†ÊñπÊ≥ïÔºåÊúâÊïàÁßªÈô§Ê®°ÂûãÂêéÈó®Âπ∂‰øùÊåÅÊ®°ÂûãÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂêéÈó®ÊîªÂáª` `Ëß£Â≠¶‰π†` `Á∫øÊÄß‰ªªÂä°ÂàÜËß£` `Ê®°ÂûãÂÆâÂÖ®` `ÂØπÊäóÊÄßÊîªÂáª` `Ê∑±Â∫¶Â≠¶‰π†` `Ê®°ÂûãÈ≤ÅÊ£íÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂêéÈó®Èò≤Âæ°ÊñπÊ≥ï‰æùËµñ‰ª£‰ª∑È´òÊòÇÁöÑÂæÆË∞ÉÔºåÂΩ±ÂìçÊ®°ÂûãÂú®ÂÖ∂‰ªñ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÔºåÁº∫‰πèÈÄöÁî®ÊÄß„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÁ∫øÊÄß‰ªªÂä°ÂàÜËß£ÁöÑËß£Â≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáÂàÜÁ¶ªÂêéÈó®‰ªªÂä°‰∏éÂÖ∂‰ªñËâØÊÄß‰ªªÂä°ÔºåÂÆûÁé∞Á≤æÂáÜÁßªÈô§„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â∑≤Áü•ÊîªÂáª‰∏ãËÉΩËøë‰πéÂÆåÁæéÂú∞ÁßªÈô§ÂêéÈó®ÔºåÂêåÊó∂‰øùÁïô96%ÁöÑÂπ≤ÂáÄÊï∞ÊçÆÂáÜÁ°ÆÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âü∫Á°ÄÊ®°ÂûãÈÄöËøáÂú®ÂêÑÁßç‰ªªÂä°‰∏äÁöÑÂπøÊ≥õÊ≥õÂåñÔºåÂΩªÂ∫ïÊîπÂèò‰∫ÜËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüü„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨‰ªçÁÑ∂ÊûÅÊòìÂèóÂà∞ÂØπÊäóÊÄßÊâ∞Âä®ÂíåÊúâÈíàÂØπÊÄßÁöÑÂêéÈó®ÊîªÂáªÁöÑÂΩ±Âìç„ÄÇÁºìËß£Ëøô‰∫õÊºèÊ¥û‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂºÄÊîæÁöÑÊåëÊàòÔºåÁâπÂà´ÊòØËÄÉËôëÂà∞Ê®°ÂûãÁöÑÂ§ßËßÑÊ®°ÊÄßÁ¶ÅÊ≠¢‰∫ÜÈáçÊñ∞ËÆ≠ÁªÉ‰ª•Á°Æ‰øùÂÆâÂÖ®„ÄÇÁé∞ÊúâÁöÑÂêéÈó®ÁßªÈô§ÊñπÊ≥ï‰æùËµñ‰∫é‰ª£‰ª∑È´òÊòÇÁöÑÂæÆË∞ÉÊù•Ë¶ÜÁõñÊúâÂÆ≥Ë°å‰∏∫ÔºåÂπ∂‰∏îÂ∏∏Â∏∏‰ºöÈôç‰ΩéÊ®°ÂûãÂú®ÂÖ∂‰ªñ‰∏çÁõ∏ÂÖ≥‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇËøôÂ∞±ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢òÔºöÊòØÂê¶ÂèØ‰ª•Âú®‰∏çÊçüÂÆ≥Ê®°ÂûãÈÄöÁî®ËÉΩÂäõÁöÑÊÉÖÂÜµ‰∏ãÁßªÈô§ÂêéÈó®ÔºüÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºåÂπ∂Á†îÁ©∂‰∫ÜÂêéÈó®ÊòØÂ¶Ç‰ΩïÂú®Ê®°ÂûãÊùÉÈáçÁ©∫Èó¥‰∏≠ÁºñÁ†ÅÁöÑÔºåÂèëÁé∞ÂÆÉ‰ª¨‰∏éÂÖ∂‰ªñËâØÊÄß‰ªªÂä°ÊòØÂàÜÁ¶ªÁöÑ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËøôÁßçÂàÜÁ¶ª‰ΩøÂæóËÉΩÂ§üÈöîÁ¶ªÂíåÊ∂àÈô§ÂêéÈó®ÂØπÊ®°ÂûãÁöÑÂΩ±ÂìçÔºåÂêåÊó∂ÂØπÂπ≤ÂáÄÊï∞ÊçÆÁöÑÊÄßËÉΩÂΩ±ÂìçÊúÄÂ∞è„ÄÇÂü∫‰∫éËøô‰∏ÄÊ¥ûÂØüÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÁöÑËß£Â≠¶‰π†ÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂà©Áî®‰∫ÜËøôÁßçÂàÜÁ¶ª„ÄÇÈÄöËøáÂØπÂü∫‰∫éCLIPÁöÑÊ®°ÂûãÂíåÂ∏∏ËßÅÁöÑÂØπÊäóÊÄßËß¶ÂèëÂô®ÁöÑÂπøÊ≥õÂÆûÈ™åÔºåÊàë‰ª¨Ë°®ÊòéÔºåÂú®Â∑≤Áü•ÊîªÂáªÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂÆûÁé∞‰∫ÜËøë‰ººÂÆåÁæéÁöÑËß£Â≠¶‰π†ÔºåÂêåÊó∂Âπ≥Âùá‰øùÁïô‰∫Ü96%ÁöÑÂπ≤ÂáÄÊï∞ÊçÆÂáÜÁ°ÆÁéá„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÂç≥‰ΩøÂú®ÊîªÂáªÂèäÂÖ∂Â≠òÂú®Êú™Áü•ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰πüËÉΩÈÄöËøá‰ΩøÁî®ÈÄÜÂêëÂ∑•Á®ãËß¶ÂèëÂô®ÁöÑÈÄÇÂΩì‰º∞ËÆ°ÊàêÂäüÂú∞Ëß£Â≠¶‰π†ÂêéÈó®„ÄÇÊÄªÁöÑÊù•ËØ¥Ôºå‰∏éÁõÆÂâçÊúÄÂÖàËøõÁöÑÈò≤Âæ°ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂßãÁªà‰∫ßÁîüÊõ¥Â•ΩÁöÑËß£Â≠¶‰π†ÂíåÂπ≤ÂáÄÊï∞ÊçÆÂáÜÁ°ÆÁéáÁöÑÊùÉË°°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Ê∑±Â∫¶Â≠¶‰π†Ê®°Âûã‰∏≠ÂêéÈó®ÊîªÂáªÁöÑËß£Â≠¶‰π†ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÂæÆË∞ÉÔºåÂú®ÁßªÈô§ÂêéÈó®ÁöÑÂêåÊó∂ÔºåÂæÄÂæÄ‰ºöÊòæËëóÈôç‰ΩéÊ®°ÂûãÂú®Âπ≤ÂáÄÊï∞ÊçÆ‰∏äÁöÑÊÄßËÉΩÔºåÂπ∂‰∏îËÆ°ÁÆóÊàêÊú¨È´òÊòÇ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®‰∏çÁâ∫Áâ≤Ê®°ÂûãÈÄöÁî®ËÉΩÂäõÁöÑÂâçÊèê‰∏ãÊúâÊïàÁßªÈô§ÂêéÈó®ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËßÇÂØüÂà∞ÂêéÈó®ÊîªÂáªÂú®Ê®°ÂûãÊùÉÈáçÁ©∫Èó¥‰∏≠‰∏éÂÖ∂‰ªñËâØÊÄß‰ªªÂä°ÊòØËß£ËÄ¶ÁöÑ„ÄÇËøôÊÑèÂë≥ÁùÄÂêéÈó®ÁöÑÂΩ±ÂìçÂèØ‰ª•Ë¢´ÈöîÁ¶ªÂíåÊ∂àÈô§ÔºåËÄå‰∏ç‰ºöÂØπÊ®°ÂûãÁöÑÂÖ∂‰ªñÂäüËÉΩ‰∫ßÁîüÈáçÂ§ßÂΩ±Âìç„ÄÇÈÄöËøáËØÜÂà´Âπ∂ÁßªÈô§‰∏éÂêéÈó®Áõ∏ÂÖ≥ÁöÑÊùÉÈáçÈÉ®ÂàÜÔºåÂèØ‰ª•ÂÆûÁé∞ÊúâÊïàÁöÑËß£Â≠¶‰π†„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö1) **ÂêéÈó®Ê£ÄÊµãÔºàÂèØÈÄâÔºâ**ÔºöÂ¶ÇÊûúÊîªÂáªÊú™Áü•ÔºåÂàôÈúÄË¶ÅÂÖàËøõË°åÂêéÈó®Ê£ÄÊµãÔºå‰æãÂ¶ÇÈÄöËøáÈÄÜÂêëÂ∑•Á®ãËß¶ÂèëÂô®Êù•‰º∞ËÆ°ÊîªÂáªÊ®°Âºè„ÄÇ2) **‰ªªÂä°ÂàÜËß£**ÔºöÂ∞ÜÊ®°ÂûãÊùÉÈáçÂàÜËß£‰∏∫‰∏éÂêéÈó®‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜÂíå‰∏éÂπ≤ÂáÄ‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜ„ÄÇËøôÈÄöÂ∏∏ÈÄöËøáÁ∫øÊÄß‰ª£Êï∞ÊñπÊ≥ïÂÆûÁé∞„ÄÇ3) **ÂêéÈó®ÁßªÈô§**ÔºöÁßªÈô§Êàñ‰øÆÊîπ‰∏éÂêéÈó®‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÊùÉÈáçÈÉ®ÂàÜ„ÄÇ4) **Ê®°ÂûãËØÑ‰º∞**ÔºöËØÑ‰º∞Ëß£Â≠¶‰π†ÂêéÁöÑÊ®°ÂûãÂú®Âπ≤ÂáÄÊï∞ÊçÆÂíåÂêéÈó®Êï∞ÊçÆ‰∏äÁöÑÊÄßËÉΩÔºå‰ª•È™åËØÅËß£Â≠¶‰π†ÁöÑÊïàÊûú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂèëÁé∞‰∫ÜÂêéÈó®ÊîªÂáªÂú®Ê®°ÂûãÊùÉÈáçÁ©∫Èó¥‰∏≠ÁöÑËß£ËÄ¶ÁâπÊÄßÔºåÂπ∂Âà©Áî®Á∫øÊÄß‰ªªÂä°ÂàÜËß£Êù•ÂÆûÁé∞Á≤æÂáÜÁöÑÂêéÈó®ÁßªÈô§„ÄÇ‰∏é‰º†ÁªüÁöÑÂæÆË∞ÉÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÈÅøÂÖç‰∫ÜÂØπÊï¥‰∏™Ê®°ÂûãËøõË°åË∞ÉÊï¥Ôºå‰ªéËÄåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÂπ∂Êõ¥Â•ΩÂú∞‰øùÁïô‰∫ÜÊ®°ÂûãÁöÑÈÄöÁî®ËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) **Á∫øÊÄß‰ªªÂä°ÂàÜËß£ÁöÑÂÖ∑‰ΩìÊñπÊ≥ï**Ôºö‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®‰∏ªÊàêÂàÜÂàÜÊûêÔºàPCAÔºâÊàñÂÖ∂‰ªñÁ∫øÊÄßÈôçÁª¥ÊäÄÊúØÊù•ËØÜÂà´‰∏éÂêéÈó®‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÊùÉÈáçÂ≠êÁ©∫Èó¥„ÄÇ2) **ÂêéÈó®ÊùÉÈáçÁßªÈô§Á≠ñÁï•**ÔºöÂèØ‰ª•Áõ¥Êé•Â∞Ü‰∏éÂêéÈó®Áõ∏ÂÖ≥ÁöÑÊùÉÈáçËÆæÁΩÆ‰∏∫Èõ∂ÔºåÊàñËÄÖ‰ΩøÁî®Êõ¥Á≤æÁªÜÁöÑÊùÉÈáçË∞ÉÊï¥ÊñπÊ≥ï„ÄÇ3) **ËØÑ‰º∞ÊåáÊ†á**ÔºöÈúÄË¶ÅÂêåÊó∂ËØÑ‰º∞Ê®°ÂûãÂú®Âπ≤ÂáÄÊï∞ÊçÆ‰∏äÁöÑÂáÜÁ°ÆÁéáÂíåÂú®ÂêéÈó®Êï∞ÊçÆ‰∏äÁöÑÊîªÂáªÊàêÂäüÁéáÔºå‰ª•Ë°°ÈáèËß£Â≠¶‰π†ÁöÑÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Â∑≤Áü•ÊîªÂáªÁöÑÊÉÖÂÜµ‰∏ãÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞Ëøë‰ººÂÆåÁæéÁöÑÂêéÈó®Ëß£Â≠¶‰π†ÔºåÂêåÊó∂Âπ≥Âùá‰øùÁïô96%ÁöÑÂπ≤ÂáÄÊï∞ÊçÆÂáÜÁ°ÆÁéá„ÄÇÂç≥‰ΩøÂú®ÊîªÂáªÊú™Áü•ÁöÑÊÉÖÂÜµ‰∏ãÔºåÈÄöËøáÈÄÜÂêëÂ∑•Á®ãËß¶ÂèëÂô®ËøõË°å‰º∞ËÆ°ÔºåËØ•ÊñπÊ≥ï‰πüËÉΩÊàêÂäüËß£Â≠¶‰π†ÂêéÈó®ÔºåÂπ∂‰∏îÂú®Ëß£Â≠¶‰π†ÊïàÊûúÂíåÂπ≤ÂáÄÊï∞ÊçÆÂáÜÁ°ÆÁéá‰πãÈó¥ÂèñÂæó‰∫ÜÊØîÁé∞ÊúâÊñπÊ≥ïÊõ¥Â•ΩÁöÑÊùÉË°°„ÄÇËøô‰∫õÁªìÊûúËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÂú®ÂêéÈó®Èò≤Âæ°ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶Å‰øùÊä§Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÂÖçÂèóÂêéÈó®ÊîªÂáªÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅ‰∫∫ËÑ∏ËØÜÂà´„ÄÅÂåªÁñóËØäÊñ≠Á≠â„ÄÇÈÄöËøáËØ•ÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÁâ∫Áâ≤Ê®°ÂûãÊÄßËÉΩÁöÑÂâçÊèê‰∏ãÔºåÊúâÊïàÊèêÈ´òÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÔºåÈò≤Ê≠¢ÊÅ∂ÊÑèÊîªÂáªËÄÖÂà©Áî®ÂêéÈó®ÊéßÂà∂Ê®°ÂûãË°å‰∏∫„ÄÇËØ•ÊäÄÊúØËøòÊúâÂä©‰∫éÊèêÂçáÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÂèØ‰ø°Â∫¶Ôºå‰øÉËøõÊ∑±Â∫¶Â≠¶‰π†ÊäÄÊúØÂú®ÂÆâÂÖ®ÊïèÊÑüÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Foundation models have revolutionized computer vision by enabling broad generalization across diverse tasks. Yet, they remain highly susceptible to adversarial perturbations and targeted backdoor attacks. Mitigating such vulnerabilities remains an open challenge, especially given that the large-scale nature of the models prohibits retraining to ensure safety. Existing backdoor removal approaches rely on costly fine-tuning to override the harmful behavior, and can often degrade performance on other unrelated tasks. This raises the question of whether backdoors can be removed without compromising the general capabilities of the models. In this work, we address this question and study how backdoors are encoded in the model weight space, finding that they are disentangled from other benign tasks. Specifically, this separation enables the isolation and erasure of the backdoor's influence on the model with minimal impact on clean performance. Building on this insight, we introduce a simple unlearning method that leverages such disentanglement. Through extensive experiments with CLIP-based models and common adversarial triggers, we show that, given the knowledge of the attack, our method achieves approximately perfect unlearning, while retaining, on average, 96% of clean accuracy. Additionally, we demonstrate that even when the attack and its presence are unknown, our method successfully unlearns backdoors by proper estimation using reverse-engineered triggers. Overall, our method consistently yields better unlearning and clean accuracy tradeoffs when compared to present state-of-the-art defenses.

