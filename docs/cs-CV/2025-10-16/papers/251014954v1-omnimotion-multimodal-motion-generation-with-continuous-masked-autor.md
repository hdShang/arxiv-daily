---
layout: default
title: OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression
---

# OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14954" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14954v1</a>
  <a href="https://arxiv.org/pdf/2510.14954.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14954v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14954v1', 'OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniMotionï¼šæå‡ºè¿ç»­æ©ç è‡ªå›å½’Transformerï¼Œç”¨äºå¤šæ¨¡æ€å…¨èº«äººä½“è¿åŠ¨ç”Ÿæˆã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ` `è¿ç»­æ©ç è‡ªå›å½’` `è¿åŠ¨Transformer` `å› æœæ³¨æ„åŠ›` `é—¨æ§çº¿æ€§æ³¨æ„åŠ›` `DiTç»“æ„` `æ–‡æœ¬åˆ°è¿åŠ¨` `è¯­éŸ³åˆ°æ‰‹åŠ¿`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆä¸­ï¼Œç¼ºä¹æœ‰æ•ˆçš„è¿åŠ¨ç”Ÿæˆæœºåˆ¶å’Œæ¨¡æ€èåˆæ–¹æ³•ï¼Œéš¾ä»¥å¤„ç†å¤æ‚å’Œå¼‚æ„çš„æ•°æ®ã€‚
2. æå‡ºè¿ç»­æ©ç è‡ªå›å½’è¿åŠ¨Transformerï¼Œåˆ©ç”¨å› æœæ³¨æ„åŠ›æœºåˆ¶å’Œé—¨æ§çº¿æ€§æ³¨æ„åŠ›ï¼Œå…³æ³¨å…³é”®åŠ¨ä½œå¹¶æŠ‘åˆ¶ä¸ç¨³å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ–‡æœ¬åˆ°è¿åŠ¨ã€è¯­éŸ³åˆ°æ‰‹åŠ¿å’ŒéŸ³ä¹åˆ°èˆè¹ˆç­‰ä»»åŠ¡ä¸Šï¼Œå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†ç”Ÿæˆè´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…¨èº«å¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯æ„å»ºæœ‰æ•ˆçš„è¿åŠ¨ç”Ÿæˆæœºåˆ¶ï¼ŒäºŒæ˜¯å°†æ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ç­‰å¤šç§æ¨¡æ€æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ã€‚ä¸ä»¥å¾€é€šå¸¸é‡‡ç”¨ç¦»æ•£æ©ç å»ºæ¨¡æˆ–è‡ªå›å½’å»ºæ¨¡çš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è¿ç»­æ©ç è‡ªå›å½’è¿åŠ¨Transformerï¼Œå®ƒåœ¨è€ƒè™‘äººä½“è¿åŠ¨çš„åºåˆ—æ€§è´¨æ—¶æ‰§è¡Œå› æœæ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨è¯¥Transformerä¸­ï¼Œå¼•å…¥äº†é—¨æ§çº¿æ€§æ³¨æ„åŠ›å’ŒRMSNormæ¨¡å—ï¼Œä»¥é©±åŠ¨Transformerå…³æ³¨å…³é”®åŠ¨ä½œï¼Œå¹¶æŠ‘åˆ¶ç”±å¼‚å¸¸è¿åŠ¨æˆ–å¤šæ¨¡æ€ä¸­çš„å¼‚æ„åˆ†å¸ƒå¼•èµ·çš„ä¸ç¨³å®šæ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¿åŠ¨ç”Ÿæˆå’Œå¤šæ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼Œæœ¬æ–‡é‡‡ç”¨DiTç»“æ„å°†Transformerä¸­çš„æ¡ä»¶æ‰©æ•£åˆ°ç›®æ ‡ã€‚ä¸ºäº†èåˆä¸åŒçš„æ¨¡æ€ï¼Œåˆ©ç”¨AdaLNå’Œäº¤å‰æ³¨æ„åŠ›æ¥æ³¨å…¥æ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶åœ¨æ‰€æœ‰æ¨¡æ€ï¼ˆåŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€è¯­éŸ³åˆ°æ‰‹åŠ¿å’ŒéŸ³ä¹åˆ°èˆè¹ˆï¼‰ä¸Šå‡ä¼˜äºä»¥å¾€çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„ä»£ç å°†ä¼šå¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å…¨èº«å¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç¦»æ•£æ©ç å»ºæ¨¡æˆ–ç®€å•çš„è‡ªå›å½’å»ºæ¨¡ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨äººä½“è¿åŠ¨çš„åºåˆ—ç‰¹æ€§ï¼Œå¹¶ä¸”åœ¨èåˆæ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ç­‰å¤šç§æ¨¡æ€æ—¶ï¼Œå®¹æ˜“å—åˆ°å¼‚æ„åˆ†å¸ƒçš„å½±å“ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸ç¨³å®šå’Œè´¨é‡ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¿ç»­æ©ç è‡ªå›å½’Transformerï¼Œç»“åˆå› æœæ³¨æ„åŠ›å’Œé—¨æ§æœºåˆ¶ï¼Œå®ç°æ›´æœ‰æ•ˆçš„è¿åŠ¨ç”Ÿæˆå’Œå¤šæ¨¡æ€èåˆã€‚é€šè¿‡è¿ç»­æ©ç å»ºæ¨¡ï¼Œå¯ä»¥æ›´å¥½åœ°æ•æ‰äººä½“è¿åŠ¨çš„è¿ç»­æ€§å’Œä¾èµ–å…³ç³»ã€‚é—¨æ§çº¿æ€§æ³¨æ„åŠ›å¯ä»¥å¸®åŠ©æ¨¡å‹å…³æ³¨å…³é”®åŠ¨ä½œï¼ŒæŠ‘åˆ¶å™ªå£°å’Œä¸ç¨³å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¿ç»­æ©ç è‡ªå›å½’è¿åŠ¨Transformerï¼šç”¨äºç”Ÿæˆäººä½“è¿åŠ¨åºåˆ—ï¼Œé‡‡ç”¨å› æœæ³¨æ„åŠ›æœºåˆ¶å’Œé—¨æ§çº¿æ€§æ³¨æ„åŠ›ã€‚2) DiTç»“æ„ï¼šå°†Transformerä¸­çš„æ¡ä»¶æ‰©æ•£åˆ°ç›®æ ‡ï¼Œå¢å¼ºè¿åŠ¨ç”Ÿæˆå’Œå¤šæ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚3) å¤šæ¨¡æ€èåˆæ¨¡å—ï¼šåˆ©ç”¨AdaLNå’Œäº¤å‰æ³¨æ„åŠ›æ¥æ³¨å…¥æ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è¿ç»­æ©ç è‡ªå›å½’è¿åŠ¨Transformerï¼Œå¹¶ç»“åˆäº†é—¨æ§çº¿æ€§æ³¨æ„åŠ›å’ŒDiTç»“æ„ã€‚ä¸ä¼ ç»Ÿçš„ç¦»æ•£æ©ç å»ºæ¨¡å’Œè‡ªå›å½’å»ºæ¨¡ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººä½“è¿åŠ¨çš„è¿ç»­æ€§å’Œä¾èµ–å…³ç³»ï¼Œå¹¶æœ‰æ•ˆåœ°èåˆå¤šç§æ¨¡æ€çš„ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Transformerä¸­ï¼Œä½¿ç”¨äº†é—¨æ§çº¿æ€§æ³¨æ„åŠ›æ¥å…³æ³¨å…³é”®åŠ¨ä½œï¼Œå¹¶ä½¿ç”¨RMSNormæ¨¡å—æ¥æŠ‘åˆ¶ä¸ç¨³å®šæ€§ã€‚ä¸ºäº†èåˆä¸åŒçš„æ¨¡æ€ï¼Œä½¿ç”¨äº†AdaLNå’Œäº¤å‰æ³¨æ„åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†DiTç»“æ„æ¥æ‰©æ•£æ¡ä»¶ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†è¿åŠ¨ç”Ÿæˆå’Œå¤šæ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç­‰ç»†èŠ‚å°†åœ¨ä»£ç å¼€æºåå…¬å¼€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniMotionåœ¨æ–‡æœ¬åˆ°è¿åŠ¨ã€è¯­éŸ³åˆ°æ‰‹åŠ¿å’ŒéŸ³ä¹åˆ°èˆè¹ˆç­‰ä»»åŠ¡ä¸Šï¼Œå‡ä¼˜äºä»¥å¾€çš„æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®å°†åœ¨è®ºæ–‡ä¸­è¯¦ç»†å±•ç¤ºï¼Œä»£ç å¼€æºåå¯å¤ç°ã€‚è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°ã€è¯­éŸ³æŒ‡ä»¤æˆ–éŸ³ä¹èŠ‚å¥ï¼Œè‡ªåŠ¨ç”Ÿæˆé€¼çœŸçš„äººä½“è¿åŠ¨åŠ¨ç”»ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œåˆ›ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½åº·å¤ã€è¿åŠ¨è®­ç»ƒç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.

