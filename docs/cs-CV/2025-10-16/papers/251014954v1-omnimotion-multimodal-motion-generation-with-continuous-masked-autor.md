---
layout: default
title: OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression
---

# OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14954" target="_blank" class="toolbar-btn">arXiv: 2510.14954v1</a>
    <a href="https://arxiv.org/pdf/2510.14954.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14954v1" 
            onclick="toggleFavorite(this, '2510.14954v1', 'OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**OmniMotionÔºöÊèêÂá∫ËøûÁª≠Êé©Á†ÅËá™ÂõûÂΩíTransformerÔºåÁî®‰∫éÂ§öÊ®°ÊÄÅÂÖ®Ë∫´‰∫∫‰ΩìËøêÂä®ÁîüÊàê„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅËøêÂä®ÁîüÊàê` `ËøûÁª≠Êé©Á†ÅËá™ÂõûÂΩí` `ËøêÂä®Transformer` `Âõ†ÊûúÊ≥®ÊÑèÂäõ` `Èó®ÊéßÁ∫øÊÄßÊ≥®ÊÑèÂäõ` `DiTÁªìÊûÑ` `ÊñáÊú¨Âà∞ËøêÂä®` `ËØ≠Èü≥Âà∞ÊâãÂäø`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅ‰∫∫‰ΩìËøêÂä®ÁîüÊàê‰∏≠ÔºåÁº∫‰πèÊúâÊïàÁöÑËøêÂä®ÁîüÊàêÊú∫Âà∂ÂíåÊ®°ÊÄÅËûçÂêàÊñπÊ≥ïÔºåÈöæ‰ª•Â§ÑÁêÜÂ§çÊùÇÂíåÂºÇÊûÑÁöÑÊï∞ÊçÆ„ÄÇ
2. ÊèêÂá∫ËøûÁª≠Êé©Á†ÅËá™ÂõûÂΩíËøêÂä®TransformerÔºåÂà©Áî®Âõ†ÊûúÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåÈó®ÊéßÁ∫øÊÄßÊ≥®ÊÑèÂäõÔºåÂÖ≥Ê≥®ÂÖ≥ÈîÆÂä®‰ΩúÂπ∂ÊäëÂà∂‰∏çÁ®≥ÂÆöÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®ÊñáÊú¨Âà∞ËøêÂä®„ÄÅËØ≠Èü≥Âà∞ÊâãÂäøÂíåÈü≥‰πêÂà∞ËàûËπàÁ≠â‰ªªÂä°‰∏äÔºåÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜÁîüÊàêË¥®Èáè„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂÖ®Ë∫´Â§öÊ®°ÊÄÅ‰∫∫‰ΩìËøêÂä®ÁîüÊàêÈù¢‰∏¥‰∏§Â§ßÊåëÊàòÔºö‰∏ÄÊòØÊûÑÂª∫ÊúâÊïàÁöÑËøêÂä®ÁîüÊàêÊú∫Âà∂Ôºå‰∫åÊòØÂ∞ÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÈü≥‰πêÁ≠âÂ§öÁßçÊ®°ÊÄÅÊï¥ÂêàÂà∞‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂‰∏≠„ÄÇ‰∏é‰ª•ÂæÄÈÄöÂ∏∏ÈááÁî®Á¶ªÊï£Êé©Á†ÅÂª∫Ê®°ÊàñËá™ÂõûÂΩíÂª∫Ê®°ÁöÑÊñπÊ≥ï‰∏çÂêåÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËøûÁª≠Êé©Á†ÅËá™ÂõûÂΩíËøêÂä®TransformerÔºåÂÆÉÂú®ËÄÉËôë‰∫∫‰ΩìËøêÂä®ÁöÑÂ∫èÂàóÊÄßË¥®Êó∂ÊâßË°åÂõ†ÊûúÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÇÂú®ËØ•Transformer‰∏≠ÔºåÂºïÂÖ•‰∫ÜÈó®ÊéßÁ∫øÊÄßÊ≥®ÊÑèÂäõÂíåRMSNormÊ®°ÂùóÔºå‰ª•È©±Âä®TransformerÂÖ≥Ê≥®ÂÖ≥ÈîÆÂä®‰ΩúÔºåÂπ∂ÊäëÂà∂Áî±ÂºÇÂ∏∏ËøêÂä®ÊàñÂ§öÊ®°ÊÄÅ‰∏≠ÁöÑÂºÇÊûÑÂàÜÂ∏ÉÂºïËµ∑ÁöÑ‰∏çÁ®≥ÂÆöÊÄß„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Â¢ûÂº∫ËøêÂä®ÁîüÊàêÂíåÂ§öÊ®°ÊÄÅÊ≥õÂåñËÉΩÂäõÔºåÊú¨ÊñáÈááÁî®DiTÁªìÊûÑÂ∞ÜTransformer‰∏≠ÁöÑÊù°‰ª∂Êâ©Êï£Âà∞ÁõÆÊ†á„ÄÇ‰∏∫‰∫ÜËûçÂêà‰∏çÂêåÁöÑÊ®°ÊÄÅÔºåÂà©Áî®AdaLNÂíå‰∫§ÂèâÊ≥®ÊÑèÂäõÊù•Ê≥®ÂÖ•ÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÈü≥‰πê‰ø°Âè∑„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊú¨ÊñáÊèêÂá∫ÁöÑÊ°ÜÊû∂Âú®ÊâÄÊúâÊ®°ÊÄÅÔºàÂåÖÊã¨ÊñáÊú¨Âà∞ËøêÂä®„ÄÅËØ≠Èü≥Âà∞ÊâãÂäøÂíåÈü≥‰πêÂà∞ËàûËπàÔºâ‰∏äÂùá‰ºò‰∫é‰ª•ÂæÄÁöÑÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÁöÑ‰ª£Á†ÅÂ∞Ü‰ºöÂºÄÊ∫ê„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂÖ®Ë∫´Â§öÊ®°ÊÄÅ‰∫∫‰ΩìËøêÂä®ÁîüÊàêÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®Á¶ªÊï£Êé©Á†ÅÂª∫Ê®°ÊàñÁÆÄÂçïÁöÑËá™ÂõûÂΩíÂª∫Ê®°ÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®‰∫∫‰ΩìËøêÂä®ÁöÑÂ∫èÂàóÁâπÊÄßÔºåÂπ∂‰∏îÂú®ËûçÂêàÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÈü≥‰πêÁ≠âÂ§öÁßçÊ®°ÊÄÅÊó∂ÔºåÂÆπÊòìÂèóÂà∞ÂºÇÊûÑÂàÜÂ∏ÉÁöÑÂΩ±ÂìçÔºåÂØºËá¥ÁîüÊàêÁªìÊûú‰∏çÁ®≥ÂÆöÂíåË¥®Èáè‰∏çÈ´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËøûÁª≠Êé©Á†ÅËá™ÂõûÂΩíTransformerÔºåÁªìÂêàÂõ†ÊûúÊ≥®ÊÑèÂäõÂíåÈó®ÊéßÊú∫Âà∂ÔºåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑËøêÂä®ÁîüÊàêÂíåÂ§öÊ®°ÊÄÅËûçÂêà„ÄÇÈÄöËøáËøûÁª≠Êé©Á†ÅÂª∫Ê®°ÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâ‰∫∫‰ΩìËøêÂä®ÁöÑËøûÁª≠ÊÄßÂíå‰æùËµñÂÖ≥Á≥ª„ÄÇÈó®ÊéßÁ∫øÊÄßÊ≥®ÊÑèÂäõÂèØ‰ª•Â∏ÆÂä©Ê®°ÂûãÂÖ≥Ê≥®ÂÖ≥ÈîÆÂä®‰ΩúÔºåÊäëÂà∂Âô™Â£∞Âíå‰∏çÁ®≥ÂÆöÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËøûÁª≠Êé©Á†ÅËá™ÂõûÂΩíËøêÂä®TransformerÔºöÁî®‰∫éÁîüÊàê‰∫∫‰ΩìËøêÂä®Â∫èÂàóÔºåÈááÁî®Âõ†ÊûúÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåÈó®ÊéßÁ∫øÊÄßÊ≥®ÊÑèÂäõ„ÄÇ2) DiTÁªìÊûÑÔºöÂ∞ÜTransformer‰∏≠ÁöÑÊù°‰ª∂Êâ©Êï£Âà∞ÁõÆÊ†áÔºåÂ¢ûÂº∫ËøêÂä®ÁîüÊàêÂíåÂ§öÊ®°ÊÄÅÊ≥õÂåñËÉΩÂäõ„ÄÇ3) Â§öÊ®°ÊÄÅËûçÂêàÊ®°ÂùóÔºöÂà©Áî®AdaLNÂíå‰∫§ÂèâÊ≥®ÊÑèÂäõÊù•Ê≥®ÂÖ•ÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÈü≥‰πê‰ø°Âè∑„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜËøûÁª≠Êé©Á†ÅËá™ÂõûÂΩíËøêÂä®TransformerÔºåÂπ∂ÁªìÂêà‰∫ÜÈó®ÊéßÁ∫øÊÄßÊ≥®ÊÑèÂäõÂíåDiTÁªìÊûÑ„ÄÇ‰∏é‰º†ÁªüÁöÑÁ¶ªÊï£Êé©Á†ÅÂª∫Ê®°ÂíåËá™ÂõûÂΩíÂª∫Ê®°Áõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâ‰∫∫‰ΩìËøêÂä®ÁöÑËøûÁª≠ÊÄßÂíå‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂ÊúâÊïàÂú∞ËûçÂêàÂ§öÁßçÊ®°ÊÄÅÁöÑ‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Transformer‰∏≠Ôºå‰ΩøÁî®‰∫ÜÈó®ÊéßÁ∫øÊÄßÊ≥®ÊÑèÂäõÊù•ÂÖ≥Ê≥®ÂÖ≥ÈîÆÂä®‰ΩúÔºåÂπ∂‰ΩøÁî®RMSNormÊ®°ÂùóÊù•ÊäëÂà∂‰∏çÁ®≥ÂÆöÊÄß„ÄÇ‰∏∫‰∫ÜËûçÂêà‰∏çÂêåÁöÑÊ®°ÊÄÅÔºå‰ΩøÁî®‰∫ÜAdaLNÂíå‰∫§ÂèâÊ≥®ÊÑèÂäõ„ÄÇÊ≠§Â§ñÔºåËøòÈááÁî®‰∫ÜDiTÁªìÊûÑÊù•Êâ©Êï£Êù°‰ª∂ÔºåËøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜËøêÂä®ÁîüÊàêÂíåÂ§öÊ®°ÊÄÅÊ≥õÂåñËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞Á≠âÁªÜËäÇÂ∞ÜÂú®‰ª£Á†ÅÂºÄÊ∫êÂêéÂÖ¨ÂºÄ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmniMotionÂú®ÊñáÊú¨Âà∞ËøêÂä®„ÄÅËØ≠Èü≥Âà∞ÊâãÂäøÂíåÈü≥‰πêÂà∞ËàûËπàÁ≠â‰ªªÂä°‰∏äÔºåÂùá‰ºò‰∫é‰ª•ÂæÄÁöÑÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂ∞ÜÂú®ËÆ∫Êñá‰∏≠ËØ¶ÁªÜÂ±ïÁ§∫Ôºå‰ª£Á†ÅÂºÄÊ∫êÂêéÂèØÂ§çÁé∞„ÄÇËØ•ÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅ‰∫∫‰ΩìËøêÂä®ÁîüÊàêÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊèêÂçáÔºå‰∏∫Áõ∏ÂÖ≥È¢ÜÂüüÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂºÄÂèë„ÄÅÂä®ÁîªÂà∂‰Ωú„ÄÅ‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑÊñáÊú¨ÊèèËø∞„ÄÅËØ≠Èü≥Êåá‰ª§ÊàñÈü≥‰πêËäÇÂ•èÔºåËá™Âä®ÁîüÊàêÈÄºÁúüÁöÑ‰∫∫‰ΩìËøêÂä®Âä®ÁîªÔºå‰ªéËÄåÊèêÂçáÁî®Êà∑‰ΩìÈ™åÂíåÂàõ‰ΩúÊïàÁéá„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÊõ¥ÂπøÊ≥õÁöÑÂú∫ÊôØÔºåÂ¶ÇÊô∫ËÉΩÂ∫∑Â§ç„ÄÅËøêÂä®ËÆ≠ÁªÉÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.

