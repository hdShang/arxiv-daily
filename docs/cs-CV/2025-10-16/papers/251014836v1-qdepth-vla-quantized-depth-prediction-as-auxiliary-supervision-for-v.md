---
layout: default
title: QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models
---

# QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14836" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14836v1</a>
  <a href="https://arxiv.org/pdf/2510.14836.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14836v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14836v1', 'QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**QDepth-VLAï¼šåˆ©ç”¨é‡åŒ–æ·±åº¦é¢„æµ‹è¾…åŠ©è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæå‡ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æ·±åº¦é¢„æµ‹` `è¾…åŠ©ç›‘ç£å­¦ä¹ ` `ç©ºé—´æ¨ç†` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨ç†è§£å’Œæ¨ç†ç²¾ç»†æ“ä½œä»»åŠ¡æ‰€éœ€çš„3Dç»“æ„æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. QDepth-VLAé€šè¿‡å¼•å…¥è¾…åŠ©æ·±åº¦é¢„æµ‹ä»»åŠ¡ï¼Œä½¿æ¨¡å‹å­¦ä¹ æ·±åº¦æ„ŸçŸ¥çš„è¡¨ç¤ºï¼Œä»è€Œæ•è·å…³é”®çš„å‡ ä½•ä¿¡æ¯ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒQDepth-VLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç©ºé—´æ¨ç†èƒ½åŠ›å’Œæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºQDepth-VLAï¼Œä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡è¾…åŠ©æ·±åº¦é¢„æµ‹ä»»åŠ¡æ¥å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æå‡VLAæ¨¡å‹åœ¨ç²¾ç»†æ“ä½œä»»åŠ¡ä¸­çš„ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚QDepth-VLAè®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„æ·±åº¦ä¸“å®¶ï¼Œç”¨äºé¢„æµ‹ä»VQ-VAEç¼–ç å™¨è·å¾—çš„æ·±åº¦å›¾çš„é‡åŒ–æ½œåœ¨tokensï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ•è·å…³é”®å‡ ä½•çº¿ç´¢çš„æ·±åº¦æ„ŸçŸ¥è¡¨ç¤ºã€‚åœ¨æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒQDepth-VLAåœ¨æ“ä½œä»»åŠ¡ä¸Šäº§ç”Ÿäº†å¼ºå¤§çš„ç©ºé—´æ¨ç†èƒ½åŠ›å’Œå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æ‰§è¡Œç²¾ç»†æ“ä½œä»»åŠ¡æ—¶ï¼Œç¼ºä¹å¯¹åœºæ™¯ä¸­ç‰©ä½“3Dç»“æ„çš„æœ‰æ•ˆç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨ç©ºé—´ä¿¡æ¯ï¼Œå¯¼è‡´æ“ä½œç²¾åº¦å—é™ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡VLAæ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›æ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¸€ä¸ªè¾…åŠ©çš„æ·±åº¦é¢„æµ‹ä»»åŠ¡ï¼Œè®©VLAæ¨¡å‹å­¦ä¹ åˆ°æ·±åº¦æ„ŸçŸ¥çš„è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹éœ€è¦é¢„æµ‹åœºæ™¯çš„æ·±åº¦ä¿¡æ¯ï¼Œä»è€Œæ˜¾å¼åœ°å­¦ä¹ åœºæ™¯çš„å‡ ä½•ç»“æ„ã€‚è¿™ç§æ·±åº¦ä¿¡æ¯å¯ä»¥ä½œä¸ºä¸€ç§é¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œæ¨ç†åœºæ™¯ä¸­çš„ç©ºé—´å…³ç³»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šQDepth-VLAæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) VLAæ¨¡å‹ï¼šä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œè´Ÿè´£å¤„ç†è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œå¹¶ç”ŸæˆåŠ¨ä½œæŒ‡ä»¤ã€‚2) VQ-VAEç¼–ç å™¨ï¼šç”¨äºå°†æ·±åº¦å›¾ç¼–ç æˆé‡åŒ–çš„æ½œåœ¨tokensã€‚3) æ·±åº¦ä¸“å®¶ï¼šä¸€ä¸ªä¸“é—¨è®¾è®¡çš„ç½‘ç»œæ¨¡å—ï¼Œç”¨äºé¢„æµ‹VQ-VAEç¼–ç å™¨è¾“å‡ºçš„é‡åŒ–æ·±åº¦tokensã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ·±åº¦ä¸“å®¶æ¥æ”¶è§†è§‰è¾“å…¥ï¼Œå¹¶é¢„æµ‹å¯¹åº”çš„æ·±åº¦tokensã€‚VLAæ¨¡å‹çš„è®­ç»ƒç›®æ ‡æ˜¯å®Œæˆæ“ä½œä»»åŠ¡ï¼ŒåŒæ—¶æ·±åº¦ä¸“å®¶éœ€è¦å°½å¯èƒ½å‡†ç¡®åœ°é¢„æµ‹æ·±åº¦ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†é‡åŒ–çš„æ·±åº¦é¢„æµ‹ä½œä¸ºVLAæ¨¡å‹çš„è¾…åŠ©ç›‘ç£ä»»åŠ¡ã€‚ä¸ç›´æ¥é¢„æµ‹è¿ç»­çš„æ·±åº¦å€¼ç›¸æ¯”ï¼Œé¢„æµ‹é‡åŒ–çš„æ·±åº¦tokenså¯ä»¥é™ä½å­¦ä¹ éš¾åº¦ï¼Œå¹¶æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡VQ-VAEç¼–ç å™¨ï¼Œå¯ä»¥å°†æ·±åº¦å›¾å‹ç¼©æˆæ›´ç´§å‡‘çš„è¡¨ç¤ºï¼Œä»è€Œå‡å°‘è®¡ç®—é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ·±åº¦ä¸“å®¶é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ï¼Œæ¥æ”¶VLAæ¨¡å‹çš„è§†è§‰ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºé‡åŒ–æ·±åº¦tokensçš„é¢„æµ‹ç»“æœã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ“ä½œä»»åŠ¡çš„æŸå¤±å’Œæ·±åº¦é¢„æµ‹çš„äº¤å‰ç†µæŸå¤±ã€‚æ·±åº¦é¢„æµ‹çš„æŸå¤±æƒé‡éœ€è¦ä»”ç»†è°ƒæ•´ï¼Œä»¥å¹³è¡¡æ“ä½œä»»åŠ¡å’Œæ·±åº¦é¢„æµ‹ä»»åŠ¡ä¹‹é—´çš„å…³ç³»ã€‚VQ-VAEçš„ç æœ¬å¤§å°æ˜¯ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQDepth-VLAåœ¨æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸé¡¹æ“ä½œä»»åŠ¡ä¸­ï¼ŒQDepth-VLAç›¸æ¯”åŸºçº¿æ–¹æ³•æå‡äº†10%çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒéªŒè¯äº†æ·±åº¦é¢„æµ‹è¾…åŠ©ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ˜¾è‘—æå‡VLAæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

QDepth-VLAæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦ç²¾ç»†æ“ä½œçš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€è£…é…ã€ä»¥åŠåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æœºå™¨äººåœ¨çœŸå®ä¸–ç•Œä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œé€‚åº”ç¯å¢ƒå˜åŒ–ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.

