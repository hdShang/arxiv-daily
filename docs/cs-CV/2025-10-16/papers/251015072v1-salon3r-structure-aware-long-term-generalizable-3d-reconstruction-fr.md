---
layout: default
title: "SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images"
---

# SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15072" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15072v1</a>
  <a href="https://arxiv.org/pdf/2510.15072.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15072v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15072v1', 'SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaxin Guo, Tongfan Guan, Wenzhen Dong, Wenzhao Zheng, Wenting Wang, Yue Wang, Yeung Yam, Yun-Hui Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://wrld.github.io/SaLon3R/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SaLon3Rï¼šç»“æ„æ„ŸçŸ¥çš„é•¿æœŸé€šç”¨3Dé‡å»ºï¼Œè§£å†³å†—ä½™å’Œå‡ ä½•ä¸ä¸€è‡´é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `3Dé‡å»º` `é«˜æ–¯æº…å°„` `é€šç”¨é‡å»º` `é•¿æœŸåºåˆ—` `ç»“æ„æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é€šç”¨3Dé«˜æ–¯æº…å°„æ–¹æ³•åœ¨é•¿æ—¶åºè§†é¢‘é‡å»ºä¸­å­˜åœ¨å†—ä½™é«˜æ–¯å’Œå‡ ä½•ä¸ä¸€è‡´é—®é¢˜ï¼Œé™åˆ¶äº†æ•ˆç‡å’Œé‡å»ºè´¨é‡ã€‚
2. SaLon3Ré€šè¿‡å¼•å…¥ç´§å‡‘çš„é”šç‚¹åŸºå…ƒå’Œæ˜¾è‘—æ€§æ„ŸçŸ¥é«˜æ–¯é‡åŒ–ï¼Œæœ‰æ•ˆå»é™¤å†—ä½™ï¼Œå¹¶åˆ©ç”¨3D Point Transformerè§£å†³å‡ ä½•ä¸ä¸€è‡´é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSaLon3Råœ¨novel view synthesiså’Œæ·±åº¦ä¼°è®¡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ•ˆç‡æ›´é«˜ï¼Œæ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºSaLon3Rï¼Œä¸€ç§ç»“æ„æ„ŸçŸ¥çš„é•¿æœŸ3Dé«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰é‡å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿æ—¶åºè§†é¢‘åºåˆ—æ—¶å­˜åœ¨çš„å†—ä½™å’Œå‡ ä½•ä¸ä¸€è‡´é—®é¢˜ã€‚SaLon3Ræ˜¯é¦–ä¸ªèƒ½å¤Ÿä»¥è¶…è¿‡10 FPSçš„é€Ÿåº¦é‡å»ºè¶…è¿‡50ä¸ªè§†è§’çš„åœ¨çº¿é€šç”¨GSæ–¹æ³•ï¼Œå¹¶èƒ½å»é™¤50%åˆ°90%çš„å†—ä½™ã€‚è¯¥æ–¹æ³•å¼•å…¥ç´§å‡‘çš„é”šç‚¹åŸºå…ƒï¼Œé€šè¿‡å¯å¾®çš„æ˜¾è‘—æ€§æ„ŸçŸ¥é«˜æ–¯é‡åŒ–æ¶ˆé™¤å†—ä½™ï¼Œå¹¶ç»“åˆ3D Point Transformeræ¥ç»†åŒ–é”šç‚¹å±æ€§å’Œæ˜¾è‘—æ€§ï¼Œä»è€Œè§£å†³è·¨å¸§çš„å‡ ä½•å’Œå…‰åº¦ä¸ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨3Dé‡å»ºéª¨å¹²ç½‘ç»œé¢„æµ‹å¯†é›†çš„é€åƒç´ é«˜æ–¯å’Œç¼–ç åŒºåŸŸå‡ ä½•å¤æ‚åº¦çš„æ˜¾è‘—æ€§å›¾ã€‚ç„¶åï¼Œé€šè¿‡ä¼˜å…ˆè€ƒè™‘é«˜å¤æ‚åº¦åŒºåŸŸï¼Œå°†å†—ä½™é«˜æ–¯å‹ç¼©ä¸ºç´§å‡‘çš„é”šç‚¹ã€‚3D Point Transformerä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ 3Dç©ºé—´ä¸­çš„ç©ºé—´ç»“æ„å…ˆéªŒï¼Œä»¥ç»†åŒ–é”šç‚¹å±æ€§å’Œæ˜¾è‘—æ€§ï¼Œä»è€Œå®ç°åŒºåŸŸè‡ªé€‚åº”é«˜æ–¯è§£ç ä»¥ä¿è¯å‡ ä½•ä¿çœŸåº¦ã€‚æ— éœ€å·²çŸ¥çš„ç›¸æœºå‚æ•°æˆ–æµ‹è¯•æ—¶ä¼˜åŒ–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¼ªå½±å¹¶ä¿®å‰ªå†—ä½™çš„3DGSã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨novel view synthesiså’Œæ·±åº¦ä¼°è®¡æ–¹é¢å‡è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†é•¿æœŸé€šç”¨3Dé‡å»ºçš„å“è¶Šæ•ˆç‡ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäº3Dé«˜æ–¯æº…å°„çš„é€šç”¨ä¸‰ç»´é‡å»ºæ–¹æ³•ï¼Œåœ¨å¤„ç†é•¿æ—¶åºè§†é¢‘åºåˆ—æ—¶ï¼Œç”±äºæ¯ä¸ªè§†è§’éƒ½é¢„æµ‹é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶å°†æ‰€æœ‰è§†è§’çš„é«˜æ–¯åˆ†å¸ƒç»„åˆæˆåœºæ™¯è¡¨ç¤ºï¼Œå¯¼è‡´å¤§é‡å†—ä½™å’Œå‡ ä½•ä¸ä¸€è‡´æ€§ã€‚è¿™é™ä½äº†é‡å»ºæ•ˆç‡ï¼Œå¹¶å½±å“äº†é‡å»ºè´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§’æ•°é‡è¾ƒå¤šæ—¶ï¼Œé—®é¢˜æ›´åŠ çªå‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSaLon3Rçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ç´§å‡‘çš„é”šç‚¹åŸºå…ƒæ¥æ¶ˆé™¤å†—ä½™ï¼Œå¹¶åˆ©ç”¨3D Point Transformeræ¥å­¦ä¹ ç©ºé—´ç»“æ„å…ˆéªŒï¼Œä»è€Œè§£å†³è·¨å¸§çš„å‡ ä½•å’Œå…‰åº¦ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡æ˜¾è‘—æ€§æ„ŸçŸ¥çš„é«˜æ–¯é‡åŒ–ï¼Œå°†å†—ä½™çš„é«˜æ–¯åˆ†å¸ƒå‹ç¼©åˆ°å°‘é‡çš„é”šç‚¹ä¸Šï¼Œä»è€Œå‡å°‘äº†è®¡ç®—é‡å’Œå†…å­˜å ç”¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSaLon3Rçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) 3Dé‡å»ºéª¨å¹²ç½‘ç»œé¢„æµ‹å¯†é›†çš„é€åƒç´ é«˜æ–¯åˆ†å¸ƒå’Œæ˜¾è‘—æ€§å›¾ï¼›2) é€šè¿‡æ˜¾è‘—æ€§æ„ŸçŸ¥çš„é«˜æ–¯é‡åŒ–ï¼Œå°†å†—ä½™é«˜æ–¯å‹ç¼©ä¸ºç´§å‡‘çš„é”šç‚¹ï¼›3) 3D Point Transformerå­¦ä¹ ç©ºé—´ç»“æ„å…ˆéªŒï¼Œå¹¶ç»†åŒ–é”šç‚¹å±æ€§å’Œæ˜¾è‘—æ€§ï¼›4) åŒºåŸŸè‡ªé€‚åº”é«˜æ–¯è§£ç ï¼Œç”¨äºç”Ÿæˆæœ€ç»ˆçš„3Dåœºæ™¯è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šSaLon3Rçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) å¼•å…¥äº†ç´§å‡‘çš„é”šç‚¹åŸºå…ƒï¼Œæœ‰æ•ˆå‡å°‘äº†å†—ä½™ï¼›2) æå‡ºäº†å¯å¾®çš„æ˜¾è‘—æ€§æ„ŸçŸ¥é«˜æ–¯é‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®åŒºåŸŸå‡ ä½•å¤æ‚åº¦è‡ªé€‚åº”åœ°å‹ç¼©é«˜æ–¯åˆ†å¸ƒï¼›3) åˆ©ç”¨3D Point Transformerå­¦ä¹ ç©ºé—´ç»“æ„å…ˆéªŒï¼Œä»è€Œæé«˜äº†å‡ ä½•ä¸€è‡´æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSaLon3Rèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œæµ‹è¯•æ—¶ä¼˜åŒ–çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°è§£å†³ä¼ªå½±å¹¶ä¿®å‰ªå†—ä½™çš„3DGSã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ˜¾è‘—æ€§æ„ŸçŸ¥é«˜æ–¯é‡åŒ–ä¸­ï¼Œæ˜¾è‘—æ€§å›¾ç”¨äºæŒ‡å¯¼é”šç‚¹çš„é€‰æ‹©ï¼Œä¼˜å…ˆé€‰æ‹©é«˜å¤æ‚åº¦åŒºåŸŸçš„é”šç‚¹ã€‚3D Point Transformerçš„ç½‘ç»œç»“æ„åŒ…æ‹¬å¤šä¸ªTransformerå±‚ï¼Œç”¨äºå­¦ä¹ é”šç‚¹ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ç»†åŒ–é”šç‚¹çš„å±æ€§å’Œæ˜¾è‘—æ€§ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬novel view synthesisæŸå¤±å’Œæ·±åº¦ä¼°è®¡æŸå¤±ï¼Œç”¨äºä¼˜åŒ–ç½‘ç»œçš„å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SaLon3Råœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜å…¶åœ¨novel view synthesiså’Œæ·±åº¦ä¼°è®¡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ•°æ®é›†ä¸Šï¼ŒSaLon3Rçš„PSNRæŒ‡æ ‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†2dBï¼ŒåŒæ—¶èƒ½å¤Ÿå»é™¤50%åˆ°90%çš„å†—ä½™ã€‚æ­¤å¤–ï¼ŒSaLon3Rèƒ½å¤Ÿåœ¨è¶…è¿‡10 FPSçš„é€Ÿåº¦ä¸‹é‡å»ºè¶…è¿‡50ä¸ªè§†è§’ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ•ˆç‡å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SaLon3Råœ¨æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºæ„å»ºé«˜ç²¾åº¦ã€ä½å†—ä½™çš„3Dåœºæ™¯æ¨¡å‹ï¼Œä»è€Œæé«˜æœºå™¨äººå¯¹ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¢å¼ºAR/VRåº”ç”¨çš„æ²‰æµ¸æ„Ÿï¼Œå¹¶ä¸ºè‡ªåŠ¨é©¾é©¶æä¾›æ›´å¯é çš„ç¯å¢ƒä¿¡æ¯ã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºæ¨åŠ¨é€šç”¨3Dé‡å»ºæŠ€æœ¯çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”ç”¨äºå„ç§å®é™…åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable, on-the-fly reconstruction of sequential input views. However, existing methods often predict per-pixel Gaussians and combine Gaussians from all views as the scene representation, leading to substantial redundancies and geometric inconsistencies in long-duration video sequences. To address this, we propose SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction. To our best knowledge, SaLon3R is the first online generalizable GS method capable of reconstructing over 50 views in over 10 FPS, with 50% to 90% redundancy removal. Our method introduces compact anchor primitives to eliminate redundancy through differentiable saliency-aware Gaussian quantization, coupled with a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame geometric and photometric inconsistencies. Specifically, we first leverage a 3D reconstruction backbone to predict dense per-pixel Gaussians and a saliency map encoding regional geometric complexity. Redundant Gaussians are compressed into compact anchors by prioritizing high-complexity regions. The 3D Point Transformer then learns spatial structural priors in 3D space from training data to refine anchor attributes and saliency, enabling regionally adaptive Gaussian decoding for geometric fidelity. Without known camera parameters or test-time optimization, our approach effectively resolves artifacts and prunes the redundant 3DGS in a single feed-forward pass. Experiments on multiple datasets demonstrate our state-of-the-art performance on both novel view synthesis and depth estimation, demonstrating superior efficiency, robustness, and generalization ability for long-term generalizable 3D reconstruction. Project Page: https://wrld.github.io/SaLon3R/.

