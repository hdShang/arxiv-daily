---
layout: default
title: Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference
---

# Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14624" target="_blank" class="toolbar-btn">arXiv: 2510.14624v1</a>
    <a href="https://arxiv.org/pdf/2510.14624.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14624v1" 
            onclick="toggleFavorite(this, '2510.14624v1', 'Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫È´òÊïàËßÜÈ¢ëÈááÊ†∑EVSÔºåÈÄöËøáÂâ™ÊûùÊó∂Â∫èÂÜó‰ΩôtokenÂä†ÈÄüVLMÊé®ÁêÜ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÁêÜËß£` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `tokenÂâ™Êûù` `È´òÊïàÊé®ÁêÜ` `ÈïøËßÜÈ¢ëÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLMÂ§ÑÁêÜÈïøËßÜÈ¢ëÊó∂ÔºåÁî±‰∫étokenÊï∞ÈáèÈôêÂà∂ÂíåËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÈù¢‰∏¥Êé®ÁêÜÈÄüÂ∫¶ÊÖ¢Âíå‰∏ä‰∏ãÊñá‰ø°ÊÅØ‰∏¢Â§±ÁöÑÊåëÊàò„ÄÇ
2. EVSÈÄöËøáËØÜÂà´Âπ∂Ââ™ÊûùËßÜÈ¢ë‰∏≠Êó∂Â∫èÂÜó‰ΩôÁöÑÈùôÊÄÅÂõæÂÉèÂùóÔºåÂáèÂ∞ëtokenÊï∞ÈáèÔºå‰ªéËÄåÂä†ÈÄüÊé®ÁêÜÂπ∂ÊîØÊåÅÊõ¥ÈïøÁöÑËæìÂÖ•Â∫èÂàó„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåEVSËÉΩÊòæËëóÈôç‰ΩéLLMÁöÑTTFTÔºåÂêåÊó∂‰øùÊåÅËæÉÈ´òÁöÑÁ≤æÂ∫¶ÔºåÂπ∂‰∏îÈÄöËøáuptrainingËÉΩÂ¢ûÂº∫Ê®°ÂûãÂØπ‰∏çÂêåÂéãÁº©ÁéáÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)Â∑≤‰ªéÈùôÊÄÅÂõæÂÉèÁêÜËß£Êâ©Â±ïÂà∞ËßÜÈ¢ëÊé®ÁêÜÔºå‰ΩÜÂÖ∂ÂèØÊâ©Â±ïÊÄßÂèóÂà∞Â§ÑÁêÜÂØÜÈõÜÂ∏ßÂ∫èÂàóÁöÑ‰∫åÊ¨°ÊñπÊàêÊú¨ÁöÑÈôêÂà∂„ÄÇÈïøËßÜÈ¢ëÁªèÂ∏∏Ë∂ÖÂá∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑtokenÈ¢ÑÁÆóÔºåÂØºËá¥‰∏•ÈáçÁöÑ‰∏ä‰∏ãÊñáÈôêÂà∂ÂíåÂª∂ËøüÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÈ´òÊïàËßÜÈ¢ëÈááÊ†∑(EVS)ÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÂç≥ÊèíÂç≥Áî®ÊñπÊ≥ïÔºåÈÄöËøáËØÜÂà´ÂíåÂâ™ÊûùÊó∂Èó¥‰∏äÈùôÊÄÅÁöÑpatchÊù•ÂáèÂ∞ëËßÜÈ¢ë‰∏≠ÁöÑtokenÂÜó‰Ωô‚Äî‚ÄîËøô‰∫õÁ©∫Èó¥Âå∫ÂüüÂú®ËøûÁª≠Â∏ß‰∏≠‰øùÊåÅ‰∏çÂèò„ÄÇEVS‰øùÁïô‰∫Ü‰ΩçÁΩÆÊ†áËØÜÔºå‰∏çÈúÄË¶ÅÊû∂ÊûÑÊõ¥ÊîπÊàñÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåEVSÂú®‰øùÊåÅËØ≠‰πâ‰øùÁúüÂ∫¶ÁöÑÂêåÊó∂ÔºåÊòæËëóÂáèÂ∞ë‰∫ÜtokenÊï∞ÈáèÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊõ¥Âø´ÁöÑÊé®ÁêÜÂíåÊõ¥ÈïøÁöÑËæìÂÖ•Â∫èÂàó„ÄÇÂú®Êé®ÁêÜÊó∂Â∫îÁî®EVSÔºåÂèØÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)ÁöÑtime-to-first-token (TTFT)ÊúÄÂ§öÂáèÂ∞ë4ÂÄçÔºåËÄåÁ≤æÂ∫¶ÊçüÂ§±ÊûÅÂ∞è„ÄÇÂΩìÁªìÂêà‰ΩøÁî®ÈöèÊú∫Ââ™ÊûùÁéáÁöÑuptrainingÈò∂ÊÆµÊó∂ÔºåEVS‰∫ßÁîüÁöÑÊ®°ÂûãÂØπ‰∏çÂêåÁöÑÂéãÁº©Á∫ßÂà´ÂÖ∑ÊúâÈ≤ÅÊ£íÊÄßÔºåÂπ∂Âú®ÊøÄËøõÁöÑÂâ™Êûù‰∏ã‰øùÊåÅÂÆåÊï¥ÁöÑÊÄßËÉΩ„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåEVSÂßãÁªàÂ¶Ç‰∏ÄÂú∞ÊèêÈ´ò‰∫ÜÊïàÁéá-Á≤æÂ∫¶ÊùÉË°°Ôºå‰ªéËÄåÂú®‰∏çÁâ∫Áâ≤Ë¥®ÈáèÁöÑÂâçÊèê‰∏ãÔºåÂÆûÁé∞‰∫ÜÂèØÊâ©Â±ïÁöÑËßÜÈ¢ë-ËØ≠Ë®ÄÁêÜËß£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜÈ¢ë-ËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈïøËßÜÈ¢ëÊó∂Èù¢‰∏¥ÁöÑËÆ°ÁÆóÊïàÁéáÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ§ÑÁêÜÈïøËßÜÈ¢ëÊó∂ÔºåÁî±‰∫éÈúÄË¶ÅÂ§ÑÁêÜÂ§ßÈáèÁöÑÂ∏ßÔºåÂØºËá¥tokenÊï∞ÈáèËøáÂ§öÔºåË∂ÖËøá‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§ÑÁêÜËÉΩÂäõÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶ÂíåÂèØÂ§ÑÁêÜÁöÑËßÜÈ¢ëÈïøÂ∫¶„ÄÇÊ≠§Â§ñÔºåÂÜó‰ΩôÁöÑÂ∏ß‰ø°ÊÅØ‰πüÂ¢ûÂä†‰∫ÜËÆ°ÁÆóË¥üÊãÖÔºåÈôç‰Ωé‰∫ÜÊïàÁéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËØÜÂà´Âπ∂Ââ™ÊûùËßÜÈ¢ë‰∏≠Êó∂Èó¥‰∏äÂÜó‰ΩôÁöÑtoken„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ∞±ÊòØÊ£ÄÊµãËøûÁª≠Â∏ß‰πãÈó¥Ê≤°ÊúâÂèëÁîüÂèòÂåñÁöÑÂõæÂÉèÂå∫ÂüüÔºàÈùôÊÄÅpatchÔºâÔºåÂπ∂Â∞ÜÂÖ∂‰ªéËæìÂÖ•Â∫èÂàó‰∏≠ÁßªÈô§„ÄÇËøôÊ†∑ÂèØ‰ª•ÊòæËëóÂáèÂ∞ëÈúÄË¶ÅÂ§ÑÁêÜÁöÑtokenÊï∞ÈáèÔºå‰ªéËÄåÂä†ÈÄüÊé®ÁêÜËøáÁ®ãÔºåÂπ∂ÂÖÅËÆ∏Ê®°ÂûãÂ§ÑÁêÜÊõ¥ÈïøÁöÑËßÜÈ¢ëÂ∫èÂàó„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEVSÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÈùôÊÄÅpatchËØÜÂà´ÂíåtokenÂâ™Êûù„ÄÇÈ¶ñÂÖàÔºåÂØπËßÜÈ¢ëÂ∏ßËøõË°åÂàÜÂùóÔºåÁÑ∂ÂêéËÆ°ÁÆóÁõ∏ÈÇªÂ∏ß‰πãÈó¥ÂØπÂ∫îÂõæÂÉèÂùóÁöÑÂ∑ÆÂºÇ„ÄÇÂ¶ÇÊûúÂ∑ÆÂºÇÂ∞è‰∫éÊüê‰∏™ÈòàÂÄºÔºåÂàôËÆ§‰∏∫ËØ•ÂõæÂÉèÂùóÊòØÈùôÊÄÅÁöÑ„ÄÇÊé•‰∏ãÊù•ÔºåÂ∞ÜÈùôÊÄÅÂõæÂÉèÂùóÂØπÂ∫îÁöÑtoken‰ªéËæìÂÖ•Â∫èÂàó‰∏≠ÁßªÈô§Ôºå‰ªéËÄåÂáèÂ∞ëtokenÊï∞Èáè„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•‰Ωú‰∏∫Êèí‰ª∂ÈõÜÊàêÂà∞Áé∞ÊúâÁöÑVLMÊû∂ÊûÑ‰∏≠ÔºåÊó†ÈúÄ‰øÆÊîπÊ®°ÂûãÁªìÊûÑÊàñÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEVSÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÁÆÄÂçïÊÄßÂíåÊúâÊïàÊÄß„ÄÇÂÆÉ‰∏çÈúÄË¶ÅÂ§çÊùÇÁöÑÊ®°ÂûãÁªìÊûÑÊàñËÆ≠ÁªÉËøáÁ®ãÔºåÂ∞±ÂèØ‰ª•ÊòæËëóÂáèÂ∞ëËßÜÈ¢ë‰∏≠ÁöÑtokenÊï∞ÈáèÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇÊ≠§Â§ñÔºåEVS‰øùÁïô‰∫Ü‰ΩçÁΩÆ‰ø°ÊÅØÔºåËøôÂØπ‰∫éÈúÄË¶ÅÁêÜËß£ËßÜÈ¢ë‰∏≠Áâ©‰ΩìËøêÂä®Âíå‰∫§‰∫íÁöÑÊ®°ÂûãËá≥ÂÖ≥ÈáçË¶Å„ÄÇÈÄöËøáÁªìÂêà‰ΩøÁî®ÈöèÊú∫Ââ™ÊûùÁéáÁöÑuptrainingÈò∂ÊÆµÔºåEVSÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÊ®°ÂûãÂØπ‰∏çÂêåÂéãÁº©Á∫ßÂà´ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöEVSÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1ÔºâÈùôÊÄÅpatchÁöÑÂ∑ÆÂºÇÈòàÂÄºÔºöËØ•ÈòàÂÄºÂÜ≥ÂÆö‰∫ÜÂì™‰∫õÂõæÂÉèÂùóË¢´ËÆ§‰∏∫ÊòØÈùôÊÄÅÁöÑ„ÄÇÈòàÂÄºËÆæÁΩÆËøáÈ´ò‰ºöÂØºËá¥ËøáÂ∫¶Ââ™ÊûùÔºåÈôç‰ΩéÊ®°ÂûãÁ≤æÂ∫¶ÔºõÈòàÂÄºËÆæÁΩÆËøá‰ΩéÂàôÊó†Ê≥ïÊúâÊïàÂáèÂ∞ëtokenÊï∞Èáè„ÄÇ2ÔºâUptrainingÈò∂ÊÆµÁöÑÈöèÊú∫Ââ™ÊûùÁéáÔºöÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂºïÂÖ•ÈöèÊú∫Ââ™ÊûùÁéáÔºåÂèØ‰ª•‰ΩøÊ®°ÂûãÂØπ‰∏çÂêåÁöÑÂéãÁº©Á∫ßÂà´Êõ¥Âä†È≤ÅÊ£í„ÄÇ3Ôºâ‰ΩçÁΩÆ‰ø°ÊÅØÁöÑ‰øùÁïôÔºöEVSÂú®Ââ™ÊûùËøáÁ®ã‰∏≠‰øùÁïô‰∫ÜtokenÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØÔºåËøôÂØπ‰∫éÈúÄË¶ÅÁêÜËß£ËßÜÈ¢ë‰∏≠Áâ©‰ΩìËøêÂä®Âíå‰∫§‰∫íÁöÑÊ®°ÂûãËá≥ÂÖ≥ÈáçË¶Å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEVSÂú®‰øùÊåÅËæÉÈ´òÁ≤æÂ∫¶ÁöÑÂâçÊèê‰∏ãÔºåÂèØ‰ª•Â∞ÜLLMÁöÑTTFTÊúÄÂ§öÂáèÂ∞ë4ÂÄç„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∏™ËßÜÈ¢ëÈóÆÁ≠î‰ªªÂä°‰∏≠ÔºåEVSÂú®Á≤æÂ∫¶ÊçüÂ§±Â∞è‰∫é1%ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞ÜÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü3ÂÄç„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÁªìÂêà‰ΩøÁî®ÈöèÊú∫Ââ™ÊûùÁéáÁöÑuptrainingÈò∂ÊÆµÔºåEVSÂèØ‰ª•‰ΩøÊ®°ÂûãÂØπ‰∏çÂêåÁöÑÂéãÁº©Á∫ßÂà´Êõ¥Âä†È≤ÅÊ£íÔºåÂπ∂Âú®ÊøÄËøõÁöÑÂâ™Êûù‰∏ã‰øùÊåÅÂÆåÊï¥ÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EVSÊñπÊ≥ïÂèØ‰ª•ÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂ§ÑÁêÜÈïøËßÜÈ¢ëÁöÑËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇËßÜÈ¢ëÈóÆÁ≠î„ÄÅËßÜÈ¢ëÊëòË¶Å„ÄÅËßÜÈ¢ëÊèèËø∞Á≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÊòæËëóÊèêÈ´òËøô‰∫õ‰ªªÂä°ÁöÑÊé®ÁêÜÈÄüÂ∫¶ÂíåÂèØÂ§ÑÁêÜÁöÑËßÜÈ¢ëÈïøÂ∫¶Ôºå‰ªéËÄåÊâ©Â±ï‰∫ÜVLMÁöÑÂ∫îÁî®ËåÉÂõ¥„ÄÇÊ≠§Â§ñÔºåEVSËøòÂèØ‰ª•Â∫îÁî®‰∫éËßÜÈ¢ëÂéãÁº©Âíå‰º†ËæìÈ¢ÜÂüüÔºåÈÄöËøáÂáèÂ∞ëËßÜÈ¢ë‰∏≠ÁöÑÂÜó‰Ωô‰ø°ÊÅØÔºåÈôç‰ΩéÂ∏¶ÂÆΩÈúÄÊ±Ç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences. Long videos often exceed the token budget of modern language models, leading to severe context limitations and latency issues. We introduce Efficient Video Sampling (EVS), a simple, plug-and-play method for reducing token redundancy in videos by identifying and pruning temporally static patches -- spatial regions that remain unchanged across consecutive frames. EVS preserves positional identity, requires no architectural changes or retraining. We show that EVS substantially reduces token count while maintaining semantic fidelity, enabling faster inference and longer input sequences. Applied at inference time, EVS reduces large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal accuracy loss. When combined with an uptraining phase using stochastic pruning rates, EVS yields models that are robust to varying compression levels and retain full performance under aggressive pruning. Extensive experiments demonstrate that EVS consistently improves efficiency-accuracy trade-offs, unlocking scalable video-language understanding without sacrificing quality.

