---
layout: default
title: Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference
---

# Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14624" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14624v1</a>
  <a href="https://arxiv.org/pdf/2510.14624.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14624v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14624v1', 'Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆè§†é¢‘é‡‡æ ·EVSï¼Œé€šè¿‡å‰ªææ—¶åºå†—ä½™tokenåŠ é€ŸVLMæ¨ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `è§†è§‰è¯­è¨€æ¨¡å‹` `tokenå‰ªæ` `é«˜æ•ˆæ¨ç†` `é•¿è§†é¢‘å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMå¤„ç†é•¿è§†é¢‘æ—¶ï¼Œç”±äºtokenæ•°é‡é™åˆ¶å’Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œé¢ä¸´æ¨ç†é€Ÿåº¦æ…¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ä¸¢å¤±çš„æŒ‘æˆ˜ã€‚
2. EVSé€šè¿‡è¯†åˆ«å¹¶å‰ªæè§†é¢‘ä¸­æ—¶åºå†—ä½™çš„é™æ€å›¾åƒå—ï¼Œå‡å°‘tokenæ•°é‡ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†å¹¶æ”¯æŒæ›´é•¿çš„è¾“å…¥åºåˆ—ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEVSèƒ½æ˜¾è‘—é™ä½LLMçš„TTFTï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„ç²¾åº¦ï¼Œå¹¶ä¸”é€šè¿‡uptrainingèƒ½å¢å¼ºæ¨¡å‹å¯¹ä¸åŒå‹ç¼©ç‡çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)å·²ä»é™æ€å›¾åƒç†è§£æ‰©å±•åˆ°è§†é¢‘æ¨ç†ï¼Œä½†å…¶å¯æ‰©å±•æ€§å—åˆ°å¤„ç†å¯†é›†å¸§åºåˆ—çš„äºŒæ¬¡æ–¹æˆæœ¬çš„é™åˆ¶ã€‚é•¿è§†é¢‘ç»å¸¸è¶…å‡ºè¯­è¨€æ¨¡å‹çš„tokené¢„ç®—ï¼Œå¯¼è‡´ä¸¥é‡çš„ä¸Šä¸‹æ–‡é™åˆ¶å’Œå»¶è¿Ÿé—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†é«˜æ•ˆè§†é¢‘é‡‡æ ·(EVS)ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„å³æ’å³ç”¨æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«å’Œå‰ªææ—¶é—´ä¸Šé™æ€çš„patchæ¥å‡å°‘è§†é¢‘ä¸­çš„tokenå†—ä½™â€”â€”è¿™äº›ç©ºé—´åŒºåŸŸåœ¨è¿ç»­å¸§ä¸­ä¿æŒä¸å˜ã€‚EVSä¿ç•™äº†ä½ç½®æ ‡è¯†ï¼Œä¸éœ€è¦æ¶æ„æ›´æ”¹æˆ–é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒEVSåœ¨ä¿æŒè¯­ä¹‰ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†tokenæ•°é‡ï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„æ¨ç†å’Œæ›´é•¿çš„è¾“å…¥åºåˆ—ã€‚åœ¨æ¨ç†æ—¶åº”ç”¨EVSï¼Œå¯å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„time-to-first-token (TTFT)æœ€å¤šå‡å°‘4å€ï¼Œè€Œç²¾åº¦æŸå¤±æå°ã€‚å½“ç»“åˆä½¿ç”¨éšæœºå‰ªæç‡çš„uptrainingé˜¶æ®µæ—¶ï¼ŒEVSäº§ç”Ÿçš„æ¨¡å‹å¯¹ä¸åŒçš„å‹ç¼©çº§åˆ«å…·æœ‰é²æ£’æ€§ï¼Œå¹¶åœ¨æ¿€è¿›çš„å‰ªæä¸‹ä¿æŒå®Œæ•´çš„æ€§èƒ½ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒEVSå§‹ç»ˆå¦‚ä¸€åœ°æé«˜äº†æ•ˆç‡-ç²¾åº¦æƒè¡¡ï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²è´¨é‡çš„å‰æä¸‹ï¼Œå®ç°äº†å¯æ‰©å±•çš„è§†é¢‘-è¯­è¨€ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†é¢‘-è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œç”±äºéœ€è¦å¤„ç†å¤§é‡çš„å¸§ï¼Œå¯¼è‡´tokenæ•°é‡è¿‡å¤šï¼Œè¶…è¿‡äº†è¯­è¨€æ¨¡å‹çš„å¤„ç†èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œå¯å¤„ç†çš„è§†é¢‘é•¿åº¦ã€‚æ­¤å¤–ï¼Œå†—ä½™çš„å¸§ä¿¡æ¯ä¹Ÿå¢åŠ äº†è®¡ç®—è´Ÿæ‹…ï¼Œé™ä½äº†æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è¯†åˆ«å¹¶å‰ªæè§†é¢‘ä¸­æ—¶é—´ä¸Šå†—ä½™çš„tokenã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯æ£€æµ‹è¿ç»­å¸§ä¹‹é—´æ²¡æœ‰å‘ç”Ÿå˜åŒ–çš„å›¾åƒåŒºåŸŸï¼ˆé™æ€patchï¼‰ï¼Œå¹¶å°†å…¶ä»è¾“å…¥åºåˆ—ä¸­ç§»é™¤ã€‚è¿™æ ·å¯ä»¥æ˜¾è‘—å‡å°‘éœ€è¦å¤„ç†çš„tokenæ•°é‡ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œå¹¶å…è®¸æ¨¡å‹å¤„ç†æ›´é•¿çš„è§†é¢‘åºåˆ—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEVSæ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé™æ€patchè¯†åˆ«å’Œtokenå‰ªæã€‚é¦–å…ˆï¼Œå¯¹è§†é¢‘å¸§è¿›è¡Œåˆ†å—ï¼Œç„¶åè®¡ç®—ç›¸é‚»å¸§ä¹‹é—´å¯¹åº”å›¾åƒå—çš„å·®å¼‚ã€‚å¦‚æœå·®å¼‚å°äºæŸä¸ªé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºè¯¥å›¾åƒå—æ˜¯é™æ€çš„ã€‚æ¥ä¸‹æ¥ï¼Œå°†é™æ€å›¾åƒå—å¯¹åº”çš„tokenä»è¾“å…¥åºåˆ—ä¸­ç§»é™¤ï¼Œä»è€Œå‡å°‘tokenæ•°é‡ã€‚è¯¥æ–¹æ³•å¯ä»¥ä½œä¸ºæ’ä»¶é›†æˆåˆ°ç°æœ‰çš„VLMæ¶æ„ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„æˆ–é‡æ–°è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šEVSçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç®€å•æ€§å’Œæœ‰æ•ˆæ€§ã€‚å®ƒä¸éœ€è¦å¤æ‚çš„æ¨¡å‹ç»“æ„æˆ–è®­ç»ƒè¿‡ç¨‹ï¼Œå°±å¯ä»¥æ˜¾è‘—å‡å°‘è§†é¢‘ä¸­çš„tokenæ•°é‡ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒEVSä¿ç•™äº†ä½ç½®ä¿¡æ¯ï¼Œè¿™å¯¹äºéœ€è¦ç†è§£è§†é¢‘ä¸­ç‰©ä½“è¿åŠ¨å’Œäº¤äº’çš„æ¨¡å‹è‡³å…³é‡è¦ã€‚é€šè¿‡ç»“åˆä½¿ç”¨éšæœºå‰ªæç‡çš„uptrainingé˜¶æ®µï¼ŒEVSå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹å¯¹ä¸åŒå‹ç¼©çº§åˆ«çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šEVSçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰é™æ€patchçš„å·®å¼‚é˜ˆå€¼ï¼šè¯¥é˜ˆå€¼å†³å®šäº†å“ªäº›å›¾åƒå—è¢«è®¤ä¸ºæ˜¯é™æ€çš„ã€‚é˜ˆå€¼è®¾ç½®è¿‡é«˜ä¼šå¯¼è‡´è¿‡åº¦å‰ªæï¼Œé™ä½æ¨¡å‹ç²¾åº¦ï¼›é˜ˆå€¼è®¾ç½®è¿‡ä½åˆ™æ— æ³•æœ‰æ•ˆå‡å°‘tokenæ•°é‡ã€‚2ï¼‰Uptrainingé˜¶æ®µçš„éšæœºå‰ªæç‡ï¼šé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥éšæœºå‰ªæç‡ï¼Œå¯ä»¥ä½¿æ¨¡å‹å¯¹ä¸åŒçš„å‹ç¼©çº§åˆ«æ›´åŠ é²æ£’ã€‚3ï¼‰ä½ç½®ä¿¡æ¯çš„ä¿ç•™ï¼šEVSåœ¨å‰ªæè¿‡ç¨‹ä¸­ä¿ç•™äº†tokençš„ä½ç½®ä¿¡æ¯ï¼Œè¿™å¯¹äºéœ€è¦ç†è§£è§†é¢‘ä¸­ç‰©ä½“è¿åŠ¨å’Œäº¤äº’çš„æ¨¡å‹è‡³å…³é‡è¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEVSåœ¨ä¿æŒè¾ƒé«˜ç²¾åº¦çš„å‰æä¸‹ï¼Œå¯ä»¥å°†LLMçš„TTFTæœ€å¤šå‡å°‘4å€ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªè§†é¢‘é—®ç­”ä»»åŠ¡ä¸­ï¼ŒEVSåœ¨ç²¾åº¦æŸå¤±å°äº1%çš„æƒ…å†µä¸‹ï¼Œå°†æ¨ç†é€Ÿåº¦æé«˜äº†3å€ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆä½¿ç”¨éšæœºå‰ªæç‡çš„uptrainingé˜¶æ®µï¼ŒEVSå¯ä»¥ä½¿æ¨¡å‹å¯¹ä¸åŒçš„å‹ç¼©çº§åˆ«æ›´åŠ é²æ£’ï¼Œå¹¶åœ¨æ¿€è¿›çš„å‰ªæä¸‹ä¿æŒå®Œæ•´çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EVSæ–¹æ³•å¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é•¿è§†é¢‘çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†é¢‘é—®ç­”ã€è§†é¢‘æ‘˜è¦ã€è§†é¢‘æè¿°ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜è¿™äº›ä»»åŠ¡çš„æ¨ç†é€Ÿåº¦å’Œå¯å¤„ç†çš„è§†é¢‘é•¿åº¦ï¼Œä»è€Œæ‰©å±•äº†VLMçš„åº”ç”¨èŒƒå›´ã€‚æ­¤å¤–ï¼ŒEVSè¿˜å¯ä»¥åº”ç”¨äºè§†é¢‘å‹ç¼©å’Œä¼ è¾“é¢†åŸŸï¼Œé€šè¿‡å‡å°‘è§†é¢‘ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œé™ä½å¸¦å®½éœ€æ±‚ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences. Long videos often exceed the token budget of modern language models, leading to severe context limitations and latency issues. We introduce Efficient Video Sampling (EVS), a simple, plug-and-play method for reducing token redundancy in videos by identifying and pruning temporally static patches -- spatial regions that remain unchanged across consecutive frames. EVS preserves positional identity, requires no architectural changes or retraining. We show that EVS substantially reduces token count while maintaining semantic fidelity, enabling faster inference and longer input sequences. Applied at inference time, EVS reduces large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal accuracy loss. When combined with an uptraining phase using stochastic pruning rates, EVS yields models that are robust to varying compression levels and retain full performance under aggressive pruning. Extensive experiments demonstrate that EVS consistently improves efficiency-accuracy trade-offs, unlocking scalable video-language understanding without sacrificing quality.

