---
layout: default
title: Train a Unified Multimodal Data Quality Classifier with Synthetic Data
---

# Train a Unified Multimodal Data Quality Classifier with Synthetic Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15162" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15162v1</a>
  <a href="https://arxiv.org/pdf/2510.15162.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15162v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15162v1', 'Train a Unified Multimodal Data Quality Classifier with Synthetic Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weizhi Wang, Rongmei Lin, Shiyang Li, Colin Lockard, Ritesh Sarkhel, Sanket Lokegaonkar, Jingbo Shang, Xifeng Yan, Nasser Zalmout, Xian Li

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

**å¤‡æ³¨**: EMNLP 2025 Findings

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniFilterï¼šä¸€ç§åŸºäºåˆæˆæ•°æ®çš„ç»Ÿä¸€å¤šæ¨¡æ€æ•°æ®è´¨é‡åˆ†ç±»å™¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®è´¨é‡` `æ•°æ®è¿‡æ»¤` `åˆæˆæ•°æ®` `å¤§å‹è¯­è¨€æ¨¡å‹` `é¢„è®­ç»ƒ` `å›¾åƒæ–‡æœ¬` `äº¤é”™æ–‡æ¡£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®è¿‡æ»¤æ˜¯MLLMé¢„è®­ç»ƒçš„å…³é”®ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å›¾åƒ-æ–‡æœ¬äº¤é”™æ–‡æ¡£æ•°æ®æ—¶å­˜åœ¨ä¸è¶³ã€‚
2. UniFilteré€šè¿‡åŠåˆæˆæ–¹æ³•ç”Ÿæˆå¤šè´¨é‡çº§åˆ«çš„æ•°æ®ï¼Œç”¨äºè®­ç»ƒç»Ÿä¸€çš„å¤šæ¨¡æ€æ•°æ®è´¨é‡åˆ†ç±»å™¨ï¼Œæœ‰æ•ˆè¿‡æ»¤æ•°æ®ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨UniFilterè¿‡æ»¤çš„æ•°æ®é¢„è®­ç»ƒçš„MLLMåœ¨é›¶æ ·æœ¬æ¨ç†å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)æŒç»­åœ°åœ¨å›¾åƒ-æ–‡æœ¬æè¿°æ•°æ®å’Œäº¤é”™æ–‡æ¡£æ•°æ®çš„æ··åˆæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶è€Œï¼Œé’ˆå¯¹å›¾åƒ-æ–‡æœ¬äº¤é”™æ–‡æ¡£æ•°æ®çš„é«˜è´¨é‡æ•°æ®è¿‡æ»¤ç ”ç©¶ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºè®­ç»ƒä¸€ä¸ªé«˜æ•ˆçš„MLLMï¼Œä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ•°æ®è´¨é‡åˆ†ç±»å™¨ï¼Œä»¥è¿‡æ»¤é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬æè¿°å’Œäº¤é”™æ•°æ®(UniFilter)ã€‚ä¸ºäº†è§£å†³æ”¶é›†å¤šæ ·åŒ–æ ‡æ³¨å¤šæ¨¡æ€æ•°æ®çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠåˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç°æˆçš„åŸå§‹å›¾åƒï¼Œå¹¶ç”Ÿæˆè·¨å››ä¸ªè´¨é‡çº§åˆ«çš„ç›¸åº”æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¸ºæè¿°å’Œäº¤é”™æ–‡æ¡£æ•°æ®åˆ›å»ºæ ·æœ¬-åˆ†æ•°å¯¹ï¼Œä»¥è®­ç»ƒUniFilterã€‚æˆ‘ä»¬å°†UniFilteråº”ç”¨äºä»DataCompæè¿°æ•°æ®é›†å’ŒOBELICSå›¾åƒ-æ–‡æœ¬äº¤é”™æ•°æ®é›†ç­›é€‰é«˜è´¨é‡çš„æè¿°æ•°æ®å’Œäº¤é”™æ•°æ®ã€‚åœ¨è¿‡æ»¤åçš„æ•°æ®ä¸Šé¢„è®­ç»ƒçš„MLLMç›¸æ¯”äºåœ¨åŸºçº¿è¿‡æ»¤æ•°æ®ä¸Šè®­ç»ƒçš„MLLMï¼Œè¡¨ç°å‡ºæ˜¾è‘—å¢å¼ºçš„èƒ½åŠ›ï¼Œå®ç°äº†æ›´å¼ºçš„é›¶æ ·æœ¬æ¨ç†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚åœ¨è§†è§‰ç›‘ç£å¾®è°ƒåï¼Œè¿™äº›UniFilterè¯±å¯¼çš„MLLMåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ›´å¼ºçš„æ€§èƒ½ï¼Œçªå‡ºäº†é«˜è´¨é‡å¤šæ¨¡æ€é¢„è®­ç»ƒçš„ä¸‹æ¸¸ä¼˜åŠ¿ã€‚æˆ‘ä»¬å°†ç”¨äºè®­ç»ƒUniFilterçš„åˆæˆè®­ç»ƒæ•°æ®ã€UniFilteræ¨¡å‹æ£€æŸ¥ç‚¹ä»¥åŠç”±UniFilterç­–åˆ’çš„é«˜è´¨é‡äº¤é”™æ–‡æ¡£å­é›†OBELICS-HQå‘å¸ƒç»™ç¤¾åŒºï¼Œä»¥ä¾›é‡ç°å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå›¾åƒ-æ–‡æœ¬äº¤é”™æ–‡æ¡£æ•°æ®è´¨é‡ä¸é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„æ•°æ®è¿‡æ»¤æœºåˆ¶ï¼Œå¯¼è‡´é¢„è®­ç»ƒçš„MLLMæ€§èƒ½å—é™ã€‚é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®éš¾ä»¥è·å–ï¼Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œæˆä¸ºä¸€ä¸ªä¸»è¦ç—›ç‚¹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ•°æ®è´¨é‡åˆ†ç±»å™¨ï¼ˆUniFilterï¼‰ï¼Œç”¨äºåŒºåˆ†é«˜è´¨é‡å’Œä½è´¨é‡çš„å›¾åƒ-æ–‡æœ¬æ•°æ®ï¼ŒåŒ…æ‹¬å›¾åƒ-æ–‡æœ¬æè¿°æ•°æ®å’Œäº¤é”™æ–‡æ¡£æ•°æ®ã€‚é€šè¿‡åŠåˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œé™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ï¼Œå¹¶ä¸ºUniFilteræä¾›å……è¶³çš„è®­ç»ƒæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniFilterçš„è®­ç»ƒæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆ©ç”¨ç°æˆçš„åŸå§‹å›¾åƒï¼Œé€šè¿‡åŠåˆæˆæ–¹æ³•ç”Ÿæˆä¸åŒè´¨é‡çº§åˆ«çš„å›¾åƒ-æ–‡æœ¬æ•°æ®å¯¹ï¼ŒåŒ…æ‹¬æè¿°æ•°æ®å’Œäº¤é”™æ–‡æ¡£æ•°æ®ã€‚2) ä½¿ç”¨ç”Ÿæˆçš„åˆæˆæ•°æ®è®­ç»ƒUniFilterï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹å¤šæ¨¡æ€æ•°æ®çš„è´¨é‡è¿›è¡Œè¯„åˆ†ã€‚3) å°†UniFilteråº”ç”¨äºå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆå¦‚DataCompå’ŒOBELICSï¼‰ï¼Œè¿‡æ»¤å‡ºé«˜è´¨é‡çš„æ•°æ®å­é›†ã€‚4) ä½¿ç”¨è¿‡æ»¤åçš„é«˜è´¨é‡æ•°æ®é¢„è®­ç»ƒMLLMï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åŠåˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºåˆ›å»ºå¤šè´¨é‡çº§åˆ«çš„å›¾åƒ-æ–‡æœ¬æ•°æ®ã€‚è¿™ç§æ–¹æ³•é¿å…äº†äººå·¥æ ‡æ³¨çš„æˆæœ¬ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆè¶³å¤Ÿå¤šçš„è®­ç»ƒæ•°æ®ï¼Œç”¨äºè®­ç»ƒUniFilterã€‚æ­¤å¤–ï¼ŒUniFilterçš„è®¾è®¡ä½¿å…¶èƒ½å¤ŸåŒæ—¶å¤„ç†å›¾åƒ-æ–‡æœ¬æè¿°æ•°æ®å’Œäº¤é”™æ–‡æ¡£æ•°æ®ï¼Œå…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠåˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡åˆ©ç”¨ç°æœ‰çš„å›¾åƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ä¸åŒçš„è¯­è¨€æ¨¡å‹æˆ–ä¸åŒçš„è§£ç ç­–ç•¥ï¼‰ç”Ÿæˆå¯¹åº”äºä¸åŒè´¨é‡çº§åˆ«çš„æ–‡æœ¬æè¿°ã€‚ä¾‹å¦‚ï¼Œé«˜è´¨é‡çš„æ–‡æœ¬æè¿°å¯èƒ½åŒ…å«æ›´è¯¦ç»†ã€æ›´å‡†ç¡®çš„ä¿¡æ¯ï¼Œè€Œä½è´¨é‡çš„æ–‡æœ¬æè¿°å¯èƒ½åŒ…å«é”™è¯¯ã€ä¸ç›¸å…³çš„ä¿¡æ¯æˆ–è¯­æ³•é”™è¯¯ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ç›®æ ‡æ˜¯ä½¿UniFilterèƒ½å¤Ÿå‡†ç¡®åœ°é¢„æµ‹å¤šæ¨¡æ€æ•°æ®çš„è´¨é‡å¾—åˆ†ï¼Œä»è€ŒåŒºåˆ†é«˜è´¨é‡å’Œä½è´¨é‡çš„æ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨UniFilterè¿‡æ»¤åçš„æ•°æ®é¢„è®­ç»ƒçš„MLLMåœ¨é›¶æ ·æœ¬æ¨ç†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä¸Šæ˜¾è‘—æå‡ã€‚ç»è¿‡è§†è§‰ç›‘ç£å¾®è°ƒåï¼Œè¿™äº›MLLMåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ›´å¼ºçš„æ€§èƒ½ï¼ŒéªŒè¯äº†é«˜è´¨é‡å¤šæ¨¡æ€é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚UniFilterèƒ½å¤Ÿæœ‰æ•ˆæå‡MLLMçš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®æ¸…æ´—ã€æ•°æ®å¢å¼ºå’Œæ¨¡å‹ä¼˜åŒ–ã€‚é€šè¿‡UniFilterè¿‡æ»¤é«˜è´¨é‡æ•°æ®ï¼Œèƒ½å¤Ÿæå‡MLLMåœ¨å›¾åƒç†è§£ã€æ–‡æœ¬ç”Ÿæˆã€è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶å¯åº”ç”¨äºæ™ºèƒ½å®¢æœã€å†…å®¹åˆ›ä½œã€æ•™è‚²ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The Multimodal Large Language Models (MLLMs) are continually pre-trained on a mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, we introduce a semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. We apply UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. We release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to the community for reproduction and further development.

