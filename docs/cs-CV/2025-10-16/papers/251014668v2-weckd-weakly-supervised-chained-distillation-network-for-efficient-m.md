---
layout: default
title: "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging"
---

# WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14668" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14668v2</a>
  <a href="https://arxiv.org/pdf/2510.14668.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14668v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14668v2', 'WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16 (æ›´æ–°: 2025-11-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWeCKDï¼šä¸€ç§å¼±ç›‘ç£é“¾å¼è’¸é¦ç½‘ç»œï¼Œç”¨äºé«˜æ•ˆå¤šæ¨¡æ€åŒ»å­¦å½±åƒåˆ†æã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `å¼±ç›‘ç£å­¦ä¹ ` `é“¾å¼ç½‘ç»œ` `åŒ»å­¦å½±åƒåˆ†æ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸçŸ¥è¯†è’¸é¦ä¾èµ–å¼ºæ•™å¸ˆæ¨¡å‹å’Œå¤§é‡æ ‡æ³¨æ•°æ®ï¼Œå­˜åœ¨çŸ¥è¯†é€€åŒ–å’Œç›‘ç£æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚
2. WeCKDæ„å»ºé“¾å¼è’¸é¦ç»“æ„ï¼Œæ¯ä¸ªæ¨¡å‹ä»å‰ä»»å­¦ä¹ å¹¶æç‚¼çŸ¥è¯†ï¼Œå®ç°æ¸è¿›å¼çŸ¥è¯†è½¬ç§»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒWeCKDåœ¨å¤šä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç²¾åº¦æå‡é«˜è¾¾+23%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦(KD)ä¼ ç»Ÿä¸Šä¾èµ–äºé™æ€çš„å¸ˆç”Ÿæ¡†æ¶ï¼Œå…¶ä¸­ä¸€ä¸ªå¤§å‹ã€è®­ç»ƒæœ‰ç´ çš„æ•™å¸ˆæ¨¡å‹å°†çŸ¥è¯†ä¼ é€’ç»™å•ä¸ªå­¦ç”Ÿæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­˜åœ¨çŸ¥è¯†é€€åŒ–ã€ç›‘ç£æ•ˆç‡ä½ä¸‹ä»¥åŠä¾èµ–äºéå¸¸å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹æˆ–å¤§å‹æ ‡è®°æ•°æ®é›†çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªå¼±ç›‘ç£é“¾å¼KDç½‘ç»œ(WeCKD)ï¼Œå®ƒé€šè¿‡äº’è¿æ¨¡å‹çš„ç»“æ„åŒ–åºåˆ—é‡æ–°å®šä¹‰äº†çŸ¥è¯†è½¬ç§»ã€‚ä¸ä¼ ç»Ÿçš„KDä¸åŒï¼Œå®ƒå½¢æˆäº†ä¸€ä¸ªæ¸è¿›çš„è’¸é¦é“¾ï¼Œå…¶ä¸­æ¯ä¸ªæ¨¡å‹ä¸ä»…ä»å…¶å‰ä»»å­¦ä¹ ï¼Œè€Œä¸”åœ¨ä¼ é€’çŸ¥è¯†ä¹‹å‰å¯¹å…¶è¿›è¡Œæç‚¼ã€‚è¿™ç§ç»“æ„åŒ–çš„çŸ¥è¯†è½¬ç§»è¿›ä¸€æ­¥å¢å¼ºäº†ç‰¹å¾å­¦ä¹ ï¼Œå¹¶è§£å†³äº†å•æ­¥KDçš„å±€é™æ€§ã€‚é“¾ä¸­çš„æ¯ä¸ªæ¨¡å‹ä»…åœ¨æ•°æ®é›†çš„ä¸€å°éƒ¨åˆ†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¡¨æ˜å¯ä»¥é€šè¿‡æœ€å°çš„ç›‘ç£å®ç°æœ‰æ•ˆçš„å­¦ä¹ ã€‚åœ¨è€³é•œã€æ˜¾å¾®é•œå’Œç£å…±æŒ¯æˆåƒç­‰å…­ä¸ªæˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œå®ƒå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„è’¸é¦é“¾æ¯”åœ¨ç›¸åŒæœ‰é™æ•°æ®ä¸Šè®­ç»ƒçš„å•ä¸ªéª¨å¹²ç½‘ç»œå®ç°äº†é«˜è¾¾+23%çš„ç´¯ç§¯ç²¾åº¦æå‡ï¼Œçªå‡ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­å­˜åœ¨çš„çŸ¥è¯†é€€åŒ–ã€ç›‘ç£æ•ˆç‡ä½ä¸‹ä»¥åŠå¯¹å¼ºæ•™å¸ˆæ¨¡å‹æˆ–å¤§é‡æ ‡æ³¨æ•°æ®ä¾èµ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹è®­ç»ƒå‡ºé«˜æ•ˆä¸”å‡†ç¡®çš„æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åŒ»å­¦åº”ç”¨ä¸­çš„æ¨å¹¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé“¾å¼çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œå…¶ä¸­æ¯ä¸ªæ¨¡å‹ä¸ä»…ä»å‰ä¸€ä¸ªæ¨¡å‹å­¦ä¹ ï¼Œè€Œä¸”åœ¨ä¼ é€’çŸ¥è¯†ä¹‹å‰å¯¹çŸ¥è¯†è¿›è¡Œæç‚¼ã€‚é€šè¿‡è¿™ç§æ¸è¿›å¼çš„çŸ¥è¯†è½¬ç§»ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ï¼Œå¹¶é¿å…çŸ¥è¯†é€€åŒ–çš„é—®é¢˜ã€‚é“¾å¼ç»“æ„å…è®¸æ¨¡å‹é€æ­¥å­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWeCKDç”±ä¸€ç³»åˆ—äº’è¿çš„æ¨¡å‹ç»„æˆï¼Œå½¢æˆä¸€ä¸ªè’¸é¦é“¾ã€‚ç¬¬ä¸€ä¸ªæ¨¡å‹ä½¿ç”¨éƒ¨åˆ†æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç„¶åå°†å…¶çŸ¥è¯†ä¼ é€’ç»™ä¸‹ä¸€ä¸ªæ¨¡å‹ã€‚åç»­çš„æ¨¡å‹åœ¨å‰ä¸€ä¸ªæ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼Œä½¿ç”¨å‰©ä½™çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶è¿›ä¸€æ­¥æç‚¼çŸ¥è¯†ã€‚æ•´ä¸ªè¿‡ç¨‹å½¢æˆä¸€ä¸ªå¾ªç¯ï¼Œç›´åˆ°æ‰€æœ‰æ¨¡å‹éƒ½å¾—åˆ°å……åˆ†è®­ç»ƒã€‚æœ€ç»ˆçš„æ¨¡å‹å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºé›†æˆæ¨¡å‹çš„ä¸€éƒ¨åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šWeCKDçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é“¾å¼è’¸é¦ç»“æ„ï¼Œå®ƒå…è®¸æ¨¡å‹é€æ­¥å­¦ä¹ å’Œæç‚¼çŸ¥è¯†ï¼Œä»è€Œé¿å…äº†ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•ä¸­çš„çŸ¥è¯†é€€åŒ–é—®é¢˜ã€‚æ­¤å¤–ï¼ŒWeCKDé‡‡ç”¨å¼±ç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œæ¯ä¸ªæ¨¡å‹ä»…ä½¿ç”¨éƒ¨åˆ†æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»è€Œé™ä½äº†å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåŒ»å­¦å½±åƒåˆ†æé¢†åŸŸï¼Œå› ä¸ºæ ‡æ³¨åŒ»å­¦å½±åƒæ•°æ®é€šå¸¸éå¸¸æ˜‚è´µå’Œè€—æ—¶ã€‚

**å…³é”®è®¾è®¡**ï¼šWeCKDä¸­çš„æ¯ä¸ªæ¨¡å‹éƒ½å¯ä»¥æ˜¯ä»»ä½•ç±»å‹çš„ç¥ç»ç½‘ç»œï¼Œä¾‹å¦‚å·ç§¯ç¥ç»ç½‘ç»œ(CNN)æˆ–Transformerã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†ResNetä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯åˆ†ç±»æŸå¤±ï¼Œç”¨äºè¡¡é‡æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼›å¦ä¸€éƒ¨åˆ†æ˜¯è’¸é¦æŸå¤±ï¼Œç”¨äºè¡¡é‡æ¨¡å‹ä¸å‰ä¸€ä¸ªæ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†å·®å¼‚ã€‚è’¸é¦æŸå¤±å¯ä»¥ä½¿ç”¨å¤šç§æ–¹æ³•è®¡ç®—ï¼Œä¾‹å¦‚KLæ•£åº¦æˆ–L2æŸå¤±ã€‚é“¾çš„é•¿åº¦å’Œæ¯ä¸ªæ¨¡å‹ä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹æ˜¯é‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒWeCKDåœ¨å…­ä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚ä¸åœ¨ç›¸åŒæœ‰é™æ•°æ®ä¸Šè®­ç»ƒçš„å•ä¸ªéª¨å¹²ç½‘ç»œç›¸æ¯”ï¼ŒWeCKDå®ç°äº†é«˜è¾¾+23%çš„ç´¯ç§¯ç²¾åº¦æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒWeCKDæ˜¯ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„åŒ»å­¦å½±åƒåˆ†ææ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

WeCKDåœ¨åŒ»å­¦å½±åƒåˆ†æé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚ç–¾ç—…è¯Šæ–­ã€ç—…ç¶æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ã€‚å®ƒå¯ä»¥ç”¨äºå¤„ç†å„ç§åŒ»å­¦å½±åƒæ¨¡æ€ï¼ŒåŒ…æ‹¬è€³é•œã€æ˜¾å¾®é•œå’Œç£å…±æŒ¯æˆåƒã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºæ ‡æ³¨æ•°æ®æœ‰é™çš„åœºæ™¯ï¼Œå¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®ã€æ›´é«˜æ•ˆåœ°è¿›è¡Œç–¾ç—…è¯Šæ–­å’Œæ²»ç–—ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning and addresses the limitations of one-step KD. Each model in the chain is trained on only a fraction of the dataset and shows that effective learning can be achieved with minimal supervision. Extensive evaluation on six imaging datasets across otoscopic, microscopic, and magnetic resonance imaging modalities shows that it generalizes and outperforms existing methods. Furthermore, the proposed distillation chain resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.

