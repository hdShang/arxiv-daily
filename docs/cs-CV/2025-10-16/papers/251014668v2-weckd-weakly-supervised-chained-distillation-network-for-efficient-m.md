---
layout: default
title: WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging
---

# WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14668" target="_blank" class="toolbar-btn">arXiv: 2510.14668v2</a>
    <a href="https://arxiv.org/pdf/2510.14668.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14668v2" 
            onclick="toggleFavorite(this, '2510.14668v2', 'WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16 (Êõ¥Êñ∞: 2025-11-04)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫WeCKDÔºö‰∏ÄÁßçÂº±ÁõëÁù£ÈìæÂºèËí∏È¶èÁΩëÁªúÔºåÁî®‰∫éÈ´òÊïàÂ§öÊ®°ÊÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `Âº±ÁõëÁù£Â≠¶‰π†` `ÈìæÂºèÁΩëÁªú` `ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÁü•ËØÜËí∏È¶è‰æùËµñÂº∫ÊïôÂ∏àÊ®°ÂûãÂíåÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÔºåÂ≠òÂú®Áü•ËØÜÈÄÄÂåñÂíåÁõëÁù£ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇ
2. WeCKDÊûÑÂª∫ÈìæÂºèËí∏È¶èÁªìÊûÑÔºåÊØè‰∏™Ê®°Âûã‰ªéÂâç‰ªªÂ≠¶‰π†Âπ∂ÊèêÁÇºÁü•ËØÜÔºåÂÆûÁé∞Ê∏êËøõÂºèÁü•ËØÜËΩ¨Áßª„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåWeCKDÂú®Â§ö‰∏™ÂåªÂ≠¶ÂΩ±ÂÉèÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÁ≤æÂ∫¶ÊèêÂçáÈ´òËææ+23%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áü•ËØÜËí∏È¶è(KD)‰º†Áªü‰∏ä‰æùËµñ‰∫éÈùôÊÄÅÁöÑÂ∏àÁîüÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Â§ßÂûã„ÄÅËÆ≠ÁªÉÊúâÁ¥†ÁöÑÊïôÂ∏àÊ®°ÂûãÂ∞ÜÁü•ËØÜ‰º†ÈÄíÁªôÂçï‰∏™Â≠¶ÁîüÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Â≠òÂú®Áü•ËØÜÈÄÄÂåñ„ÄÅÁõëÁù£ÊïàÁéá‰Ωé‰∏ã‰ª•Âèä‰æùËµñ‰∫éÈùûÂ∏∏Âº∫Â§ßÁöÑÊïôÂ∏àÊ®°ÂûãÊàñÂ§ßÂûãÊ†áËÆ∞Êï∞ÊçÆÈõÜÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈ¶ñ‰∏™Âº±ÁõëÁù£ÈìæÂºèKDÁΩëÁªú(WeCKD)ÔºåÂÆÉÈÄöËøá‰∫íËøûÊ®°ÂûãÁöÑÁªìÊûÑÂåñÂ∫èÂàóÈáçÊñ∞ÂÆö‰πâ‰∫ÜÁü•ËØÜËΩ¨Áßª„ÄÇ‰∏é‰º†ÁªüÁöÑKD‰∏çÂêåÔºåÂÆÉÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™Ê∏êËøõÁöÑËí∏È¶èÈìæÔºåÂÖ∂‰∏≠ÊØè‰∏™Ê®°Âûã‰∏ç‰ªÖ‰ªéÂÖ∂Ââç‰ªªÂ≠¶‰π†ÔºåËÄå‰∏îÂú®‰º†ÈÄíÁü•ËØÜ‰πãÂâçÂØπÂÖ∂ËøõË°åÊèêÁÇº„ÄÇËøôÁßçÁªìÊûÑÂåñÁöÑÁü•ËØÜËΩ¨ÁßªËøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜÁâπÂæÅÂ≠¶‰π†ÔºåÂπ∂Ëß£ÂÜ≥‰∫ÜÂçïÊ≠•KDÁöÑÂ±ÄÈôêÊÄß„ÄÇÈìæ‰∏≠ÁöÑÊØè‰∏™Ê®°Âûã‰ªÖÂú®Êï∞ÊçÆÈõÜÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜ‰∏äËøõË°åËÆ≠ÁªÉÔºåË°®ÊòéÂèØ‰ª•ÈÄöËøáÊúÄÂ∞èÁöÑÁõëÁù£ÂÆûÁé∞ÊúâÊïàÁöÑÂ≠¶‰π†„ÄÇÂú®ËÄ≥Èïú„ÄÅÊòæÂæÆÈïúÂíåÁ£ÅÂÖ±ÊåØÊàêÂÉèÁ≠âÂÖ≠‰∏™ÊàêÂÉèÊï∞ÊçÆÈõÜ‰∏äÁöÑÂπøÊ≥õËØÑ‰º∞Ë°®ÊòéÔºåÂÆÉÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂‰∏î‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊâÄÊèêÂá∫ÁöÑËí∏È¶èÈìæÊØîÂú®Áõ∏ÂêåÊúâÈôêÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÂçï‰∏™È™®Âπ≤ÁΩëÁªúÂÆûÁé∞‰∫ÜÈ´òËææ+23%ÁöÑÁ¥ØÁßØÁ≤æÂ∫¶ÊèêÂçáÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ïÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê‰∏≠Â≠òÂú®ÁöÑÁü•ËØÜÈÄÄÂåñ„ÄÅÁõëÁù£ÊïàÁéá‰Ωé‰∏ã‰ª•ÂèäÂØπÂº∫ÊïôÂ∏àÊ®°ÂûãÊàñÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆ‰æùËµñÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•Âú®ÊúâÈôêÊ†áÊ≥®Êï∞ÊçÆ‰∏ãËÆ≠ÁªÉÂá∫È´òÊïà‰∏îÂáÜÁ°ÆÁöÑÊ®°ÂûãÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂåªÂ≠¶Â∫îÁî®‰∏≠ÁöÑÊé®Âπø„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™ÈìæÂºèÁöÑÁü•ËØÜËí∏È¶èÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠ÊØè‰∏™Ê®°Âûã‰∏ç‰ªÖ‰ªéÂâç‰∏Ä‰∏™Ê®°ÂûãÂ≠¶‰π†ÔºåËÄå‰∏îÂú®‰º†ÈÄíÁü•ËØÜ‰πãÂâçÂØπÁü•ËØÜËøõË°åÊèêÁÇº„ÄÇÈÄöËøáËøôÁßçÊ∏êËøõÂºèÁöÑÁü•ËØÜËΩ¨ÁßªÔºåÂèØ‰ª•ÊúâÊïàÂú∞Âà©Áî®ÊúâÈôêÁöÑÊ†áÊ≥®Êï∞ÊçÆÔºåÂπ∂ÈÅøÂÖçÁü•ËØÜÈÄÄÂåñÁöÑÈóÆÈ¢ò„ÄÇÈìæÂºèÁªìÊûÑÂÖÅËÆ∏Ê®°ÂûãÈÄêÊ≠•Â≠¶‰π†Êõ¥Â§çÊùÇÁöÑÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´òÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöWeCKDÁî±‰∏ÄÁ≥ªÂàó‰∫íËøûÁöÑÊ®°ÂûãÁªÑÊàêÔºåÂΩ¢Êàê‰∏Ä‰∏™Ëí∏È¶èÈìæ„ÄÇÁ¨¨‰∏Ä‰∏™Ê®°Âûã‰ΩøÁî®ÈÉ®ÂàÜÊ†áÊ≥®Êï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂Áü•ËØÜ‰º†ÈÄíÁªô‰∏ã‰∏Ä‰∏™Ê®°Âûã„ÄÇÂêéÁª≠ÁöÑÊ®°ÂûãÂú®Ââç‰∏Ä‰∏™Ê®°ÂûãÁöÑÊåáÂØº‰∏ãÔºå‰ΩøÁî®Ââ©‰ΩôÁöÑÊ†áÊ≥®Êï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂπ∂Ëøõ‰∏ÄÊ≠•ÊèêÁÇºÁü•ËØÜ„ÄÇÊï¥‰∏™ËøáÁ®ãÂΩ¢Êàê‰∏Ä‰∏™Âæ™ÁéØÔºåÁõ¥Âà∞ÊâÄÊúâÊ®°ÂûãÈÉΩÂæóÂà∞ÂÖÖÂàÜËÆ≠ÁªÉ„ÄÇÊúÄÁªàÁöÑÊ®°ÂûãÂèØ‰ª•Áã¨Á´ã‰ΩøÁî®Ôºå‰πüÂèØ‰ª•‰Ωú‰∏∫ÈõÜÊàêÊ®°ÂûãÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöWeCKDÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÈìæÂºèËí∏È¶èÁªìÊûÑÔºåÂÆÉÂÖÅËÆ∏Ê®°ÂûãÈÄêÊ≠•Â≠¶‰π†ÂíåÊèêÁÇºÁü•ËØÜÔºå‰ªéËÄåÈÅøÂÖç‰∫Ü‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ï‰∏≠ÁöÑÁü•ËØÜÈÄÄÂåñÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåWeCKDÈááÁî®Âº±ÁõëÁù£Â≠¶‰π†Á≠ñÁï•ÔºåÊØè‰∏™Ê®°Âûã‰ªÖ‰ΩøÁî®ÈÉ®ÂàÜÊ†áÊ≥®Êï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜÂØπÊ†áÊ≥®Êï∞ÊçÆÁöÑÈúÄÊ±Ç„ÄÇËøôÁßçÊñπÊ≥ïÁâπÂà´ÈÄÇÁî®‰∫éÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûêÈ¢ÜÂüüÔºåÂõ†‰∏∫Ê†áÊ≥®ÂåªÂ≠¶ÂΩ±ÂÉèÊï∞ÊçÆÈÄöÂ∏∏ÈùûÂ∏∏ÊòÇË¥µÂíåËÄóÊó∂„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöWeCKD‰∏≠ÁöÑÊØè‰∏™Ê®°ÂûãÈÉΩÂèØ‰ª•ÊòØ‰ªª‰ΩïÁ±ªÂûãÁöÑÁ•ûÁªèÁΩëÁªúÔºå‰æãÂ¶ÇÂç∑ÁßØÁ•ûÁªèÁΩëÁªú(CNN)ÊàñTransformer„ÄÇËÆ∫Êñá‰∏≠‰ΩøÁî®‰∫ÜResNet‰Ωú‰∏∫Âü∫Á°ÄÊ®°Âûã„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨‰∏§ÈÉ®ÂàÜÔºö‰∏ÄÈÉ®ÂàÜÊòØÂàÜÁ±ªÊçüÂ§±ÔºåÁî®‰∫éË°°ÈáèÊ®°ÂûãÁöÑÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÊ†áÁ≠æ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºõÂè¶‰∏ÄÈÉ®ÂàÜÊòØËí∏È¶èÊçüÂ§±ÔºåÁî®‰∫éË°°ÈáèÊ®°Âûã‰∏éÂâç‰∏Ä‰∏™Ê®°Âûã‰πãÈó¥ÁöÑÁü•ËØÜÂ∑ÆÂºÇ„ÄÇËí∏È¶èÊçüÂ§±ÂèØ‰ª•‰ΩøÁî®Â§öÁßçÊñπÊ≥ïËÆ°ÁÆóÔºå‰æãÂ¶ÇKLÊï£Â∫¶ÊàñL2ÊçüÂ§±„ÄÇÈìæÁöÑÈïøÂ∫¶ÂíåÊØè‰∏™Ê®°Âûã‰ΩøÁî®ÁöÑÊï∞ÊçÆÊØî‰æãÊòØÈáçË¶ÅÁöÑË∂ÖÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWeCKDÂú®ÂÖ≠‰∏™ÂåªÂ≠¶ÂΩ±ÂÉèÊï∞ÊçÆÈõÜ‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ï„ÄÇ‰∏éÂú®Áõ∏ÂêåÊúâÈôêÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÂçï‰∏™È™®Âπ≤ÁΩëÁªúÁõ∏ÊØîÔºåWeCKDÂÆûÁé∞‰∫ÜÈ´òËææ+23%ÁöÑÁ¥ØÁßØÁ≤æÂ∫¶ÊèêÂçá„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåWeCKDÊòØ‰∏ÄÁßçÈ´òÊïà‰∏îÊúâÊïàÁöÑÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûêÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

WeCKDÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûêÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÁñæÁóÖËØäÊñ≠„ÄÅÁóÖÁÅ∂Ê£ÄÊµãÂíåÂõæÂÉèÂàÜÂâ≤„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÂ§ÑÁêÜÂêÑÁßçÂåªÂ≠¶ÂΩ±ÂÉèÊ®°ÊÄÅÔºåÂåÖÊã¨ËÄ≥Èïú„ÄÅÊòæÂæÆÈïúÂíåÁ£ÅÂÖ±ÊåØÊàêÂÉè„ÄÇËØ•ÊñπÊ≥ïÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÊ†áÊ≥®Êï∞ÊçÆÊúâÈôêÁöÑÂú∫ÊôØÔºåÂèØ‰ª•Â∏ÆÂä©ÂåªÁîüÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥È´òÊïàÂú∞ËøõË°åÁñæÁóÖËØäÊñ≠ÂíåÊ≤ªÁñó„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning and addresses the limitations of one-step KD. Each model in the chain is trained on only a fraction of the dataset and shows that effective learning can be achieved with minimal supervision. Extensive evaluation on six imaging datasets across otoscopic, microscopic, and magnetic resonance imaging modalities shows that it generalizes and outperforms existing methods. Furthermore, the proposed distillation chain resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.

