---
layout: default
title: Vision-Centric Activation and Coordination for Multimodal Large Language Models
---

# Vision-Centric Activation and Coordination for Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14349" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14349v3</a>
  <a href="https://arxiv.org/pdf/2510.14349.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14349v3" onclick="toggleFavorite(this, '2510.14349v3', 'Vision-Centric Activation and Coordination for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16 (æ›´æ–°: 2025-10-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVaCoï¼Œé€šè¿‡è§†è§‰ä¸­å¿ƒæ¿€æ´»ä¸åè°ƒæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰ç†è§£` `è§†è§‰ä¸­å¿ƒæ¿€æ´»` `è§†è§‰åŸºç¡€æ¨¡å‹` `ä»»åŠ¡æŸ¥è¯¢` `è§†è§‰å¯¹é½` `è¡¨ç¤ºåè°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMè®­ç»ƒä¸»è¦ä¾èµ–æ–‡æœ¬ç›‘ç£ï¼Œå¿½ç•¥äº†è§†è§‰ä¿¡æ¯å¯¹æ¨¡å‹åˆ†æèƒ½åŠ›çš„é‡è¦æ€§ã€‚
2. VaCoé€šè¿‡è§†è§‰ä¸­å¿ƒæ¿€æ´»å’Œåè°ƒï¼Œåˆ©ç”¨å¤šä¸ªè§†è§‰åŸºç¡€æ¨¡å‹ä¼˜åŒ–MLLMçš„è§†è§‰è¡¨ç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVaCoæ˜¾è‘—æå‡äº†MLLMåœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)é›†æˆäº†è§†è§‰ç¼–ç å™¨çš„å›¾åƒç‰¹å¾å’ŒLLMsï¼Œå±•ç°äº†å…ˆè¿›çš„ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸»æµMLLMsä»…é€šè¿‡æ–‡æœ¬tokençš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹è¿›è¡Œç›‘ç£ï¼Œå¿½ç•¥äº†å¯¹åˆ†æèƒ½åŠ›è‡³å…³é‡è¦çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VaCoï¼Œå®ƒé€šè¿‡æ¥è‡ªå¤šä¸ªè§†è§‰åŸºç¡€æ¨¡å‹(VFMs)çš„è§†è§‰ä¸­å¿ƒæ¿€æ´»å’Œåè°ƒæ¥ä¼˜åŒ–MLLMçš„è¡¨ç¤ºã€‚VaCoå¼•å…¥äº†è§†è§‰åˆ¤åˆ«å¯¹é½ï¼Œä»¥æ•´åˆä»VFMsæå–çš„å…·æœ‰ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›çš„æ„ŸçŸ¥ç‰¹å¾ï¼Œä»è€Œç»Ÿä¸€äº†MLLMsä¸­æ–‡æœ¬å’Œè§†è§‰è¾“å‡ºçš„ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å¯å­¦ä¹ çš„æ¨¡å—åŒ–ä»»åŠ¡æŸ¥è¯¢(MTQs)å’Œè§†è§‰å¯¹é½å±‚(VALs)é›†æˆåˆ°MLLMsä¸­ï¼Œåœ¨ä¸åŒVFMsçš„ç›‘ç£ä¸‹æ¿€æ´»ç‰¹å®šçš„è§†è§‰ä¿¡å·ã€‚ä¸ºäº†åè°ƒVFMsä¹‹é—´çš„è¡¨ç¤ºå†²çªï¼Œç²¾å¿ƒè®¾è®¡çš„Token Gateway Mask (TGM)é™åˆ¶äº†å¤šç»„MTQsä¹‹é—´çš„ä¿¡æ¯æµåŠ¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVaCoæ˜¾è‘—æé«˜äº†ä¸åŒMLLMsåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ä¸»æµå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦ä¾èµ–äºæ–‡æœ¬tokençš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹è¿›è¡Œè®­ç»ƒï¼Œç¼ºä¹å¯¹è§†è§‰ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨å’Œç›‘ç£ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨éœ€è¦æ·±å…¥è§†è§‰ç†è§£å’Œåˆ†æçš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œæ— æ³•å……åˆ†å‘æŒ¥è§†è§‰ä¿¡æ¯çš„æ½œåŠ›ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„çŸ¥è¯†ï¼Œå¹¶ä¸”å¿½ç•¥äº†ä¸åŒVFMsä¹‹é—´å¯èƒ½å­˜åœ¨çš„è¡¨ç¤ºå†²çªã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVaCoçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è§†è§‰ä¸­å¿ƒæ¿€æ´»å’Œåè°ƒæœºåˆ¶ï¼Œå°†å¤šä¸ªVFMsçš„çŸ¥è¯†èå…¥åˆ°MLLMçš„è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚å…·ä½“æ¥è¯´ï¼ŒVaCoåˆ©ç”¨VFMsæå–å…·æœ‰ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡è§†è§‰åˆ¤åˆ«å¯¹é½æ¥ç»Ÿä¸€æ–‡æœ¬å’Œè§†è§‰è¾“å‡ºçš„ä¼˜åŒ–ã€‚é€šè¿‡æ¨¡å—åŒ–ä»»åŠ¡æŸ¥è¯¢ï¼ˆMTQsï¼‰å’Œè§†è§‰å¯¹é½å±‚ï¼ˆVALsï¼‰ï¼ŒVaCoèƒ½å¤Ÿæ¿€æ´»ç‰¹å®šçš„è§†è§‰ä¿¡å·ï¼Œä»è€Œæå‡MLLMçš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒToken Gateway Maskï¼ˆTGMï¼‰ç”¨äºåè°ƒä¸åŒVFMsä¹‹é—´çš„è¡¨ç¤ºå†²çªï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ¥è‡ªå¤šä¸ªVFMsçš„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVaCoçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼šç”¨äºæå–è¾“å…¥å›¾åƒçš„è§†è§‰ç‰¹å¾ã€‚2) æ¨¡å—åŒ–ä»»åŠ¡æŸ¥è¯¢ï¼ˆMTQsï¼‰ï¼šå¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡ï¼Œç”¨äºæ¿€æ´»ç‰¹å®šçš„è§†è§‰ä¿¡å·ã€‚3) è§†è§‰å¯¹é½å±‚ï¼ˆVALsï¼‰ï¼šç”¨äºå°†MTQsä¸è§†è§‰ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä»è€Œå®ç°è§†è§‰ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚4) Token Gateway Maskï¼ˆTGMï¼‰ï¼šç”¨äºé™åˆ¶ä¸åŒMTQsä¹‹é—´çš„ä¿¡æ¯æµåŠ¨ï¼Œä»è€Œåè°ƒä¸åŒVFMsä¹‹é—´çš„è¡¨ç¤ºå†²çªã€‚5) è¯­è¨€æ¨¡å‹ï¼šç”¨äºç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚æ•´ä¸ªæ¡†æ¶çš„è®­ç»ƒè¿‡ç¨‹é€šè¿‡è§†è§‰åˆ¤åˆ«å¯¹é½æŸå¤±å‡½æ•°è¿›è¡Œç›‘ç£ï¼Œè¯¥æŸå¤±å‡½æ•°æ—¨åœ¨ä½¿MLLMçš„è§†è§‰è¡¨ç¤ºä¸VFMsçš„è§†è§‰è¡¨ç¤ºå°½å¯èƒ½ä¸€è‡´ã€‚

**å…³é”®åˆ›æ–°**ï¼šVaCoçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) å¼•å…¥äº†è§†è§‰ä¸­å¿ƒæ¿€æ´»å’Œåè°ƒæœºåˆ¶ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å¤šä¸ªVFMsçš„çŸ¥è¯†ã€‚2) æå‡ºäº†æ¨¡å—åŒ–ä»»åŠ¡æŸ¥è¯¢ï¼ˆMTQsï¼‰å’Œè§†è§‰å¯¹é½å±‚ï¼ˆVALsï¼‰ï¼Œä»è€Œèƒ½å¤Ÿæ¿€æ´»ç‰¹å®šçš„è§†è§‰ä¿¡å·ï¼Œæå‡MLLMçš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚3) è®¾è®¡äº†Token Gateway Maskï¼ˆTGMï¼‰ï¼Œä»è€Œèƒ½å¤Ÿåè°ƒä¸åŒVFMsä¹‹é—´çš„è¡¨ç¤ºå†²çªï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ¥è‡ªå¤šä¸ªVFMsçš„ä¿¡æ¯ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVaCoèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—æå‡MLLMåœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šVaCoçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) MTQsçš„æ•°é‡å’Œç»´åº¦ï¼šMTQsçš„æ•°é‡å†³å®šäº†æ¨¡å‹èƒ½å¤Ÿæ¿€æ´»çš„è§†è§‰ä¿¡å·çš„æ•°é‡ï¼Œç»´åº¦å†³å®šäº†MTQsçš„è¡¨è¾¾èƒ½åŠ›ã€‚2) VALsçš„ç»“æ„ï¼šVALsçš„ç»“æ„å†³å®šäº†MTQsä¸è§†è§‰ç‰¹å¾çš„å¯¹é½æ–¹å¼ã€‚3) TGMçš„maskingç­–ç•¥ï¼šTGMçš„maskingç­–ç•¥å†³å®šäº†ä¸åŒMTQsä¹‹é—´çš„ä¿¡æ¯æµåŠ¨æ–¹å¼ã€‚4) è§†è§‰åˆ¤åˆ«å¯¹é½æŸå¤±å‡½æ•°ï¼šè¯¥æŸå¤±å‡½æ•°æ—¨åœ¨ä½¿MLLMçš„è§†è§‰è¡¨ç¤ºä¸VFMsçš„è§†è§‰è¡¨ç¤ºå°½å¯èƒ½ä¸€è‡´ï¼Œå…¶å…·ä½“å½¢å¼å¯ä»¥æ ¹æ®ä¸åŒçš„VFMsè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVaCoåœ¨å¤šä¸ªè§†è§‰ç†è§£åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æå‡äº†MLLMçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨VQAä»»åŠ¡ä¸Šï¼ŒVaCoå°†æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†5%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒVaCoè¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°åè°ƒä¸åŒVFMsä¹‹é—´çš„è¡¨ç¤ºå†²çªï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVaCoæ˜¯ä¸€ç§æœ‰æ•ˆçš„æå‡MLLMè§†è§‰ç†è§£èƒ½åŠ›çš„æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VaCoçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¤šæ¨¡æ€ä¿¡æ¯èåˆçš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰æ¨ç†ã€æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æå‡MLLMçš„è§†è§‰ç†è§£èƒ½åŠ›ï¼ŒVaCoå¯ä»¥å¸®åŠ©å¼€å‘æ›´æ™ºèƒ½ã€æ›´å¯é çš„AIç³»ç»Ÿï¼Œåœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸå…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘ç†è§£ã€3Dåœºæ™¯ç†è§£ç­‰æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) integrate image features from visual encoders with LLMs, demonstrating advanced comprehension capabilities. However, mainstream MLLMs are solely supervised by the next-token prediction of textual tokens, neglecting critical vision-centric information essential for analytical abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM representations through Vision-Centric activation and Coordination from multiple vision foundation models (VFMs). VaCo introduces visual discriminative alignment to integrate task-aware perceptual features extracted from VFMs, thereby unifying the optimization of both textual and visual outputs in MLLMs. Specifically, we incorporate the learnable Modular Task Queries (MTQs) and Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals under the supervision of diverse VFMs. To coordinate representation conflicts across VFMs, the crafted Token Gateway Mask (TGM) restricts the information flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo significantly improves the performance of different MLLMs on various benchmarks, showcasing its superior capabilities in visual comprehension.

