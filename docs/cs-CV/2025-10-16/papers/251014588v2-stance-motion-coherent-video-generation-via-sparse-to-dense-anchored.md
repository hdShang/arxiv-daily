---
layout: default
title: "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding"
---

# STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14588" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14588v2</a>
  <a href="https://arxiv.org/pdf/2510.14588.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14588v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14588v2', 'STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhifei Chen, Tianshuo Xu, Leyi Wu, Luozhou Wang, Dongyu Yan, Zihan You, Wenting Luo, Guo Zhang, Yingcong Chen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16 (æ›´æ–°: 2025-10-19)

**å¤‡æ³¨**: Code, model, and demos can be found at https://envision-research.github.io/STANCE/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**STANCEï¼šé€šè¿‡ç¨€ç–åˆ°ç¨ å¯†é”šå®šç¼–ç å®ç°è¿åŠ¨è¿è´¯çš„è§†é¢‘ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `è¿åŠ¨è¿è´¯æ€§` `ç¨€ç–åˆ°ç¨ å¯†ç¼–ç ` `æ—‹è½¬ä½ç½®ç¼–ç ` `å®ä¾‹çº¿ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•éš¾ä»¥ä¿æŒç‰©ä½“è¿åŠ¨å’Œäº¤äº’çš„è¿è´¯æ€§ï¼Œä¸»è¦åŸå› æ˜¯è¿åŠ¨æç¤ºä¿¡æ¯åœ¨ç¼–ç åæŸå¤±è¿‡å¤šï¼Œä»¥åŠå¤–è§‚å’Œè¿åŠ¨ä¼˜åŒ–ç›¸äº’å¹²æ‰°ã€‚
2. STANCEé€šè¿‡å¼•å…¥Instance Cueså°†ç¨€ç–çš„ç”¨æˆ·æç¤ºè½¬åŒ–ä¸ºç¨ å¯†çš„2.5Dè¿åŠ¨åœºï¼Œå¹¶ä½¿ç”¨Dense RoPEä¿ç•™è¿åŠ¨ä¿¡æ¯åœ¨tokenç©ºé—´ä¸­çš„æ˜¾è‘—æ€§ã€‚
3. è¯¥æ¨¡å‹é€šè¿‡è”åˆé¢„æµ‹RGBå’Œè¾…åŠ©åœ°å›¾ï¼ˆåˆ†å‰²æˆ–æ·±åº¦ï¼‰ï¼Œåœ¨ç¨³å®šä¼˜åŒ–çš„åŒæ—¶æé«˜äº†æ—¶é—´è¿è´¯æ€§ï¼Œæ— éœ€é€å¸§è½¨è¿¹è„šæœ¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘ç”ŸæˆæŠ€æœ¯è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è§†è§‰è¿›å±•ï¼Œä½†ä¿æŒè¿è´¯çš„ç‰©ä½“è¿åŠ¨å’Œäº¤äº’ä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚æˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªå®é™…ç“¶é¢ˆï¼šï¼ˆiï¼‰äººä¸ºæä¾›çš„è¿åŠ¨æç¤ºï¼ˆä¾‹å¦‚ï¼Œå°çš„2Dåœ°å›¾ï¼‰åœ¨ç¼–ç åé€šå¸¸ä¼šåç¼©ä¸ºè¿‡å°‘çš„æœ‰æ•ˆtokenï¼Œä»è€Œå‰Šå¼±äº†æŒ‡å¯¼ä½œç”¨ï¼›ï¼ˆiiï¼‰åœ¨å•ä¸ªheadä¸­åŒæ—¶ä¼˜åŒ–å¤–è§‚å’Œè¿åŠ¨å¯èƒ½ä¼šåå‘çº¹ç†è€Œéæ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†STANCEï¼Œä¸€ä¸ªå›¾åƒåˆ°è§†é¢‘çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªç®€å•çš„ç»„ä»¶è§£å†³äº†è¿™ä¸¤ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†Instance Cuesâ€”â€”ä¸€ç§åƒç´ å¯¹é½çš„æ§åˆ¶ä¿¡å·ï¼Œå®ƒé€šè¿‡å¹³å‡æ¯ä¸ªå®ä¾‹çš„æµå¹¶ä½¿ç”¨å•ç›®æ·±åº¦å¢å¼ºå®ä¾‹æ©ç ï¼Œå°†ç¨€ç–çš„ã€ç”¨æˆ·å¯ç¼–è¾‘çš„æç¤ºè½¬æ¢ä¸ºå¯†é›†çš„2.5Dï¼ˆç›¸æœºç›¸å¯¹ï¼‰è¿åŠ¨åœºã€‚ä¸2Dç®­å¤´è¾“å…¥ç›¸æ¯”ï¼Œè¿™å‡å°‘äº†æ·±åº¦æ¨¡ç³Šï¼ŒåŒæ—¶ä¿æŒäº†æ˜“ç”¨æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨Dense RoPEä¿ç•™äº†è¿™äº›çº¿ç´¢åœ¨tokenç©ºé—´ä¸­çš„æ˜¾è‘—æ€§ï¼ŒDense RoPEç”¨ç©ºé—´å¯å¯»å€çš„æ—‹è½¬åµŒå…¥æ ‡è®°ä¸€å°ç»„è¿åŠ¨tokenï¼ˆé”šå®šåœ¨ç¬¬ä¸€å¸§ä¸Šï¼‰ã€‚ç»“åˆè”åˆRGBå’Œè¾…åŠ©åœ°å›¾é¢„æµ‹ï¼ˆåˆ†å‰²æˆ–æ·±åº¦ï¼‰ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨RGBå¤„ç†å¤–è§‚çš„åŒæ—¶é”šå®šç»“æ„ï¼Œç¨³å®šä¼˜åŒ–å¹¶æé«˜æ—¶é—´è¿è´¯æ€§ï¼Œè€Œæ— éœ€é€å¸§è½¨è¿¹è„šæœ¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶ï¼Œéš¾ä»¥ç”Ÿæˆè¿åŠ¨è¿è´¯çš„è§†é¢‘ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºï¼Œç”¨æˆ·æä¾›çš„ç¨€ç–è¿åŠ¨æç¤ºåœ¨ç»è¿‡ç¼–ç åï¼Œæœ‰æ•ˆä¿¡æ¯å¤§å¹…å‡å°‘ï¼Œæ— æ³•æœ‰æ•ˆæŒ‡å¯¼è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒåŒæ—¶ä¼˜åŒ–è§†é¢‘çš„å¤–è§‚å’Œè¿åŠ¨å®¹æ˜“å¯¼è‡´æ¨¡å‹åå‘äºçº¹ç†ç»†èŠ‚ï¼Œè€Œå¿½ç•¥äº†æ—¶é—´ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSTANCEçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¨€ç–çš„ç”¨æˆ·è¿åŠ¨æç¤ºè½¬åŒ–ä¸ºç¨ å¯†çš„2.5Dè¿åŠ¨åœºï¼Œä»è€Œæä¾›æ›´å¼ºçš„è¿åŠ¨æŒ‡å¯¼ä¿¡å·ã€‚åŒæ—¶ï¼Œé€šè¿‡è§£è€¦å¤–è§‚å’Œç»“æ„ä¿¡æ¯çš„å¤„ç†ï¼Œé¿å…ä¸¤è€…ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ï¼Œä»è€Œæé«˜ç”Ÿæˆè§†é¢‘çš„æ—¶é—´è¿è´¯æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSTANCEæ˜¯ä¸€ä¸ªå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆæ¡†æ¶ï¼Œä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šInstance Cueså’ŒDense RoPEã€‚Instance Cuesæ¨¡å—è´Ÿè´£å°†ç¨€ç–çš„ç”¨æˆ·æç¤ºè½¬åŒ–ä¸ºç¨ å¯†çš„2.5Dè¿åŠ¨åœºã€‚Dense RoPEæ¨¡å—åˆ™è´Ÿè´£åœ¨tokenç©ºé—´ä¸­ä¿ç•™è¿åŠ¨ä¿¡æ¯çš„æ˜¾è‘—æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜é‡‡ç”¨è”åˆé¢„æµ‹RGBå’Œè¾…åŠ©åœ°å›¾ï¼ˆåˆ†å‰²æˆ–æ·±åº¦ï¼‰çš„æ–¹å¼ï¼Œä»¥ç¨³å®šä¼˜åŒ–å¹¶æé«˜æ—¶é—´è¿è´¯æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSTANCEçš„å…³é”®åˆ›æ–°åœ¨äºInstance Cueså’ŒDense RoPEçš„å¼•å…¥ã€‚Instance Cuesé€šè¿‡å¹³å‡æ¯ä¸ªå®ä¾‹çš„æµå¹¶ä½¿ç”¨å•ç›®æ·±åº¦å¢å¼ºå®ä¾‹æ©ç ï¼Œæœ‰æ•ˆåœ°å°†ç¨€ç–æç¤ºè½¬åŒ–ä¸ºç¨ å¯†è¿åŠ¨åœºï¼Œå‡å°‘äº†æ·±åº¦æ¨¡ç³Šã€‚Dense RoPEåˆ™é€šè¿‡ç©ºé—´å¯å¯»å€çš„æ—‹è½¬åµŒå…¥ï¼Œä¿ç•™äº†è¿åŠ¨ä¿¡æ¯åœ¨tokenç©ºé—´ä¸­çš„æ˜¾è‘—æ€§ï¼Œé¿å…äº†ä¿¡æ¯æŸå¤±ã€‚

**å…³é”®è®¾è®¡**ï¼šInstance Cuesæ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºä½¿ç”¨2.5Dè¿åŠ¨åœºè¡¨ç¤ºè¿åŠ¨ä¿¡æ¯ï¼Œè¿™æ—¢å‡å°‘äº†æ·±åº¦æ¨¡ç³Šï¼Œåˆä¿æŒäº†æ˜“ç”¨æ€§ã€‚Dense RoPEæ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºä½¿ç”¨ç©ºé—´å¯å¯»å€çš„æ—‹è½¬åµŒå…¥ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰è¿åŠ¨ä¿¡æ¯åœ¨ç©ºé—´ä¸­çš„åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæ¨¡å‹é‡‡ç”¨è”åˆé¢„æµ‹RGBå’Œè¾…åŠ©åœ°å›¾çš„æ–¹å¼ï¼Œé€šè¿‡è¾…åŠ©ä»»åŠ¡æ¥çº¦æŸæ¨¡å‹çš„å­¦ä¹ ï¼Œä»è€Œæé«˜ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºçš„STANCEæ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡å¼•å…¥Instance Cueså’ŒDense RoPEï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¿åŠ¨æ›´åŠ è¿è´¯ã€æ—¶é—´ä¸€è‡´æ€§æ›´é«˜çš„è§†é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTANCEæ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶ï¼Œä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿ä¿¡æ¯åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

STANCEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è§†é¢‘ç¼–è¾‘ã€æ¸¸æˆå¼€å‘ã€ç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆå…·æœ‰é€¼çœŸè¿åŠ¨æ•ˆæœçš„è§†é¢‘å†…å®¹ï¼Œä¾‹å¦‚å°†é™æ€å›¾åƒè½¬åŒ–ä¸ºåŠ¨æ€è§†é¢‘ï¼Œæˆ–è€…æ ¹æ®ç”¨æˆ·æä¾›çš„è¿åŠ¨æç¤ºç”Ÿæˆç‰¹å®šçš„è§†é¢‘ç‰‡æ®µã€‚è¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ æ²‰æµ¸å¼çš„ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components. First, we introduce Instance Cues -- a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D arrow inputs while remaining easy to use. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings. Paired with joint RGB \(+\) auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.

