---
layout: default
title: Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization
---

# Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.18267" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.18267v1</a>
  <a href="https://arxiv.org/pdf/2510.18267.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.18267v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.18267v1', 'Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiang Zhang, Suping Wu, Sheng Yang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-21

**å¤‡æ³¨**: Accepted by ICME2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ½œåœ¨ä¿¡æ¯å’Œä½ç»´å­¦ä¹ çš„äººä½“ç½‘æ ¼æ¢å¤ä¸å¹¶è¡Œä¼˜åŒ–æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `äººä½“ç½‘æ ¼æ¢å¤` `3Däººä½“å§¿æ€ä¼°è®¡` `æ½œåœ¨ä¿¡æ¯æå–` `ä½ç»´å­¦ä¹ ` `å¹¶è¡Œä¼˜åŒ–` `é¢‘åŸŸç‰¹å¾` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Däººä½“ç½‘æ ¼æ¢å¤æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨æ½œåœ¨ä¿¡æ¯ï¼Œå¯¼è‡´é‡å»ºç»“æœå­˜åœ¨è‚¢ä½“é”™ä½å’Œç»†èŠ‚ä¸è¶³ç­‰é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§ä¸¤é˜¶æ®µç½‘ç»œï¼Œé€šè¿‡æŒ–æ˜å›¾åƒç‰¹å¾çš„é¢‘åŸŸä¿¡æ¯ï¼Œæå–æ··åˆæ½œåœ¨ç‰¹å¾ï¼Œå¹¶ç”¨äºå¢å¼º2Dåˆ°3Dçš„å§¿åŠ¿å­¦ä¹ ã€‚
3. è®ºæ–‡è®¾è®¡äº†ä¸€ç§ä½ç»´ç½‘æ ¼å§¿åŠ¿äº¤äº’æ–¹æ³•ï¼Œé€šè¿‡é™ç»´å’Œå¹¶è¡Œä¼˜åŒ–ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒäº†é‡å»ºç²¾åº¦ï¼Œå®éªŒç»“æœè¡¨æ˜ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„3Däººä½“ç½‘æ ¼æ¢å¤æ–¹æ³•é€šå¸¸æœªèƒ½å……åˆ†åˆ©ç”¨æ½œåœ¨ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œäººä½“è¿åŠ¨ã€å½¢çŠ¶å¯¹é½ï¼‰ï¼Œå¯¼è‡´é‡å»ºçš„äººä½“ç½‘æ ¼å‡ºç°è‚¢ä½“é”™ä½å’Œå±€éƒ¨ç»†èŠ‚ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡ç½‘æ ¼é¡¶ç‚¹å’Œå§¿åŠ¿èŠ‚ç‚¹äº¤äº’æ‰€å¸¦æ¥çš„æ€§èƒ½æå‡ä¼´éšç€é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨ä¿¡æ¯å’Œä½ç»´å­¦ä¹ çš„ä¸¤é˜¶æ®µäººä½“ç½‘æ ¼æ¢å¤ç½‘ç»œã€‚å…·ä½“è€Œè¨€ï¼Œç½‘ç»œçš„ç¬¬ä¸€é˜¶æ®µå……åˆ†æŒ–æ˜å›¾åƒç‰¹å¾çš„ä½é¢‘å’Œé«˜é¢‘åˆ†é‡ä¸­çš„å…¨å±€ï¼ˆä¾‹å¦‚ï¼Œæ•´ä½“å½¢çŠ¶å¯¹é½ï¼‰å’Œå±€éƒ¨ï¼ˆä¾‹å¦‚ï¼Œçº¹ç†ã€ç»†èŠ‚ï¼‰ä¿¡æ¯ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯èšåˆåˆ°æ··åˆæ½œåœ¨é¢‘åŸŸç‰¹å¾ä¸­ã€‚è¿™ç§ç­–ç•¥æœ‰æ•ˆåœ°æå–äº†æ½œåœ¨ä¿¡æ¯ã€‚éšåï¼Œåˆ©ç”¨æå–çš„æ··åˆæ½œåœ¨é¢‘åŸŸç‰¹å¾ååŒå¢å¼º2Då§¿åŠ¿åˆ°3Dçš„å­¦ä¹ ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåœ¨æ··åˆæ½œåœ¨ç‰¹å¾çš„è¾…åŠ©ä¸‹ï¼Œæˆ‘ä»¬å¯¹ç²—ç³™çš„3Däººä½“ç½‘æ ¼æ¨¡æ¿å’Œ3Då§¿åŠ¿ä¹‹é—´çš„äº¤äº’å­¦ä¹ è¿›è¡Œå»ºæ¨¡ï¼Œä¼˜åŒ–äººä½“ç½‘æ ¼çš„å§¿åŠ¿å’Œå½¢çŠ¶ã€‚ä¸ç°æœ‰çš„ç½‘æ ¼å§¿åŠ¿äº¤äº’æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é€šè¿‡é™ç»´å’Œå¹¶è¡Œä¼˜åŒ–å®ç°çš„ä½ç»´ç½‘æ ¼å§¿åŠ¿äº¤äº’æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç‰ºç‰²é‡å»ºç²¾åº¦çš„å‰æä¸‹æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚åœ¨å¤§å‹å…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰3Däººä½“ç½‘æ ¼æ¢å¤æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•å……åˆ†åˆ©ç”¨å›¾åƒä¸­çš„æ½œåœ¨ä¿¡æ¯ï¼Œä¾‹å¦‚äººä½“è¿åŠ¨å’Œå½¢çŠ¶å¯¹é½ç­‰ï¼Œå¯¼è‡´é‡å»ºçš„ç½‘æ ¼åœ¨è‚¢ä½“å¯¹é½å’Œå±€éƒ¨ç»†èŠ‚ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸‹ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡ç½‘æ ¼é¡¶ç‚¹å’Œå§¿åŠ¿èŠ‚ç‚¹ä¹‹é—´çš„äº¤äº’è™½ç„¶å¯ä»¥æå‡æ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æå–å›¾åƒç‰¹å¾ä¸­çš„æ½œåœ¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶èå…¥åˆ°ç½‘æ ¼æ¢å¤è¿‡ç¨‹ä¸­ï¼Œä»è€Œæå‡é‡å»ºè´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡åˆ©ç”¨å›¾åƒç‰¹å¾çš„ä½é¢‘å’Œé«˜é¢‘åˆ†é‡æ¥åˆ†åˆ«æå–å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯èåˆåˆ°æ··åˆæ½œåœ¨é¢‘åŸŸç‰¹å¾ä¸­ã€‚åŒæ—¶ï¼Œä¸ºäº†é™ä½è®¡ç®—æˆæœ¬ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ç§ä½ç»´çš„ç½‘æ ¼å§¿åŠ¿äº¤äº’æ–¹æ³•ï¼Œé€šè¿‡é™ç»´å’Œå¹¶è¡Œä¼˜åŒ–æ¥å®ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç½‘ç»œç»“æ„ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œä»å›¾åƒç‰¹å¾ä¸­æå–æ··åˆæ½œåœ¨é¢‘åŸŸç‰¹å¾ï¼Œç”¨äºå¢å¼º2Då§¿åŠ¿åˆ°3Då§¿åŠ¿çš„å­¦ä¹ ã€‚ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨æå–çš„æ··åˆæ½œåœ¨ç‰¹å¾ï¼Œå¯¹ç²—ç³™çš„3Däººä½“ç½‘æ ¼æ¨¡æ¿å’Œ3Då§¿åŠ¿ä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¼˜åŒ–äººä½“ç½‘æ ¼çš„å§¿åŠ¿å’Œå½¢çŠ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹ä¸¤ç‚¹ï¼šä¸€æ˜¯æå‡ºäº†æ··åˆæ½œåœ¨é¢‘åŸŸç‰¹å¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–å›¾åƒä¸­çš„å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨äºå¢å¼ºç½‘æ ¼æ¢å¤è¿‡ç¨‹ã€‚äºŒæ˜¯è®¾è®¡äº†ä¸€ç§ä½ç»´ç½‘æ ¼å§¿åŠ¿äº¤äº’æ–¹æ³•ï¼Œé€šè¿‡é™ç»´å’Œå¹¶è¡Œä¼˜åŒ–ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒäº†é‡å»ºç²¾åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ›´ä½çš„è®¡ç®—æˆæœ¬ä¸‹å®ç°æ›´é«˜çš„é‡å»ºè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šæ··åˆæ½œåœ¨é¢‘åŸŸç‰¹å¾çš„æå–æ–¹å¼ï¼Œå…·ä½“å¦‚ä½•èåˆä½é¢‘å’Œé«˜é¢‘ä¿¡æ¯ï¼›ä½ç»´ç½‘æ ¼å§¿åŠ¿äº¤äº’æ–¹æ³•çš„å…·ä½“å®ç°ï¼ŒåŒ…æ‹¬é™ç»´çš„æ–¹å¼å’Œå¹¶è¡Œä¼˜åŒ–çš„ç­–ç•¥ï¼›ä»¥åŠæŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œå¦‚ä½•å¹³è¡¡å§¿åŠ¿å’Œå½¢çŠ¶çš„é‡å»ºè¯¯å·®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨å¤§å‹å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦å¼ºè°ƒäº†è¯¥æ–¹æ³•åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒæˆ–æå‡äº†é‡å»ºç²¾åº¦ã€‚å®éªŒç»“æœéªŒè¯äº†æ··åˆæ½œåœ¨é¢‘åŸŸç‰¹å¾å’Œä½ç»´ç½‘æ ¼å§¿åŠ¿äº¤äº’æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œã€è¿åŠ¨åˆ†æã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸã€‚é€šè¿‡æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆåœ°æ¢å¤äººä½“ç½‘æ ¼ï¼Œå¯ä»¥æå‡ç”¨æˆ·åœ¨è™šæ‹Ÿç¯å¢ƒä¸­çš„æ²‰æµ¸æ„Ÿå’Œäº¤äº’ä½“éªŒï¼Œä¹Ÿå¯ä»¥ä¸ºè¿åŠ¨åˆ†æå’Œæ™ºèƒ½ç›‘æ§æä¾›æ›´å¯é çš„æ•°æ®æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨äººæœºäº¤äº’ã€æ™ºèƒ½ç©¿æˆ´è®¾å¤‡ç­‰é¢†åŸŸå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing 3D human mesh recovery methods often fail to fully exploit the latent information (e.g., human motion, shape alignment), leading to issues with limb misalignment and insufficient local details in the reconstructed human mesh (especially in complex scenes). Furthermore, the performance improvement gained by modelling mesh vertices and pose node interactions using attention mechanisms comes at a high computational cost. To address these issues, we propose a two-stage network for human mesh recovery based on latent information and low dimensional learning. Specifically, the first stage of the network fully excavates global (e.g., the overall shape alignment) and local (e.g., textures, detail) information from the low and high-frequency components of image features and aggregates this information into a hybrid latent frequency domain feature. This strategy effectively extracts latent information. Subsequently, utilizing extracted hybrid latent frequency domain features collaborates to enhance 2D poses to 3D learning. In the second stage, with the assistance of hybrid latent features, we model the interaction learning between the rough 3D human mesh template and the 3D pose, optimizing the pose and shape of the human mesh. Unlike existing mesh pose interaction methods, we design a low-dimensional mesh pose interaction method through dimensionality reduction and parallel optimization that significantly reduces computational costs without sacrificing reconstruction accuracy. Extensive experimental results on large publicly available datasets indicate superiority compared to the most state-of-the-art.

