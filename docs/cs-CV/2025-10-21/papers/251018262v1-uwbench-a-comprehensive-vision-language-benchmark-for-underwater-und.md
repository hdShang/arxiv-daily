---
layout: default
title: UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding
---

# UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.18262" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.18262v1</a>
  <a href="https://arxiv.org/pdf/2510.18262.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.18262v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.18262v1', 'UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Da Zhang, Chenggang Rong, Bingyu Li, Feiyu Wang, Zhiyuan Zhao, Junyu Gao, Xuelong Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-21

**å¤‡æ³¨**: We have released V1, which only reports the test results. Our work is still ongoing, and the next version will be coming soon

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UWBenchï¼šç”¨äºæ°´ä¸‹ç¯å¢ƒç†è§£çš„ç»¼åˆæ€§è§†è§‰-è¯­è¨€åŸºå‡†æ•°æ®é›†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ°´ä¸‹è§†è§‰` `è§†è§‰-è¯­è¨€æ¨¡å‹` `åŸºå‡†æ•°æ®é›†` `æ°´ä¸‹ç¯å¢ƒç†è§£` `æµ·æ´‹ç”Ÿæ€` `è§†è§‰é—®ç­”` `å›¾åƒå­—å¹•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ°´ä¸‹ç¯å¢ƒç†è§£æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œé¢ä¸´å…‰è¡°å‡ã€é¢œè‰²å¤±çœŸç­‰æŒ‘æˆ˜ï¼Œä¸”ç¼ºä¹é’ˆå¯¹æ°´ä¸‹ç¯å¢ƒçš„ä¸“ä¸šçŸ¥è¯†ã€‚
2. UWBenché€šè¿‡æ„å»ºåŒ…å«é«˜è´¨é‡å›¾åƒã€æŒ‡ä»£è¡¨è¾¾å¼å’Œé—®ç­”å¯¹çš„æ°´ä¸‹è§†è§‰-è¯­è¨€æ•°æ®é›†ï¼Œä¸ºæ°´ä¸‹ç¯å¢ƒç†è§£æä¾›åŸºå‡†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨UWBenchä¸Šè¡¨ç°ä¸ä½³ï¼Œè¯æ˜äº†æ°´ä¸‹è§†è§‰-è¯­è¨€ç†è§£çš„æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)åœ¨è‡ªç„¶åœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶åœ¨æ°´ä¸‹ç¯å¢ƒä¸­çš„åº”ç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ°´ä¸‹å›¾åƒé¢ä¸´ç€ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸¥é‡çš„å…‰è¡°å‡ã€é¢œè‰²å¤±çœŸå’Œæ‚¬æµ®é¢—ç²’æ•£å°„ï¼ŒåŒæ—¶éœ€è¦æµ·æ´‹ç”Ÿæ€ç³»ç»Ÿå’Œç”Ÿç‰©åˆ†ç±»å­¦çš„ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UWBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæ°´ä¸‹è§†è§‰-è¯­è¨€ç†è§£è®¾è®¡çš„ç»¼åˆåŸºå‡†ã€‚UWBenchåŒ…å«15,003å¼ é«˜åˆ†è¾¨ç‡æ°´ä¸‹å›¾åƒï¼Œè¿™äº›å›¾åƒæ˜¯åœ¨ä¸åŒçš„æ°´ç”Ÿç¯å¢ƒä¸­æ•è·çš„ï¼ŒåŒ…æ‹¬æµ·æ´‹ã€çŠç‘šç¤å’Œæ·±æµ·æ –æ¯åœ°ã€‚æ¯å¼ å›¾åƒéƒ½ç»è¿‡äººå·¥éªŒè¯çš„æ³¨é‡Šï¼ŒåŒ…æ‹¬15,281ä¸ªç²¾ç¡®æè¿°æµ·æ´‹ç”Ÿç‰©å’Œæ°´ä¸‹ç»“æ„çš„ç‰©ä½“æŒ‡ä»£è¡¨è¾¾å¼ï¼Œä»¥åŠ124,983ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–äº†ä»ç‰©ä½“è¯†åˆ«åˆ°ç”Ÿæ€å…³ç³»ç†è§£çš„å„ç§æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†æ•æ‰äº†èƒ½è§åº¦ã€å…‰ç…§æ¡ä»¶å’Œæ°´æµŠåº¦çš„ä¸°å¯Œå˜åŒ–ï¼Œä¸ºæ¨¡å‹è¯„ä¼°æä¾›äº†ä¸€ä¸ªçœŸå®çš„æµ‹è¯•å¹³å°ã€‚åŸºäºUWBenchï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸‰ä¸ªç»¼åˆåŸºå‡†ï¼šç”¨äºç”Ÿæˆç”Ÿæ€ä¿¡æ¯åœºæ™¯æè¿°çš„è¯¦ç»†å›¾åƒå­—å¹•ã€ç”¨äºç²¾ç¡®å®šä½æµ·æ´‹ç”Ÿç‰©çš„è§†è§‰å®šä½ï¼Œä»¥åŠç”¨äºå¯¹æ°´ä¸‹ç¯å¢ƒè¿›è¡Œå¤šæ¨¡æ€æ¨ç†çš„è§†è§‰é—®ç­”ã€‚å¯¹æœ€å…ˆè¿›çš„VLMè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ°´ä¸‹ç†è§£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬çš„åŸºå‡†ä¸ºæ¨è¿›æ°´ä¸‹ç¯å¢ƒä¸­çš„è§†è§‰-è¯­è¨€ç ”ç©¶ä»¥åŠæ”¯æŒæµ·æ´‹ç§‘å­¦ã€ç”Ÿæ€ç›‘æµ‹å’Œè‡ªä¸»æ°´ä¸‹å‹˜æ¢ä¸­çš„åº”ç”¨æä¾›äº†é‡è¦èµ„æºã€‚æˆ‘ä»¬çš„ä»£ç å’ŒåŸºå‡†å°†ä¼šå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ°´ä¸‹ç¯å¢ƒç†è§£æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•åœ¨è‡ªç„¶åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†ç”±äºæ°´ä¸‹ç¯å¢ƒç‰¹æœ‰çš„å…‰ç…§ã€é¢œè‰²å’Œæ•£å°„ç­‰é—®é¢˜ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹æ°´ä¸‹ç”Ÿç‰©å’Œç¯å¢ƒçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¯¼è‡´å…¶åœ¨æ°´ä¸‹å›¾åƒç†è§£æ–¹é¢è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé«˜è´¨é‡ã€å¤§è§„æ¨¡çš„æ°´ä¸‹è§†è§‰-è¯­è¨€æ•°æ®é›†ï¼Œå³UWBenchï¼Œä½œä¸ºè¯„ä¼°å’Œæ”¹è¿›ç°æœ‰æ¨¡å‹åœ¨æ°´ä¸‹ç¯å¢ƒç†è§£èƒ½åŠ›çš„æ ‡å‡†åŸºå‡†ã€‚é€šè¿‡æä¾›ä¸°å¯Œçš„å›¾åƒã€æŒ‡ä»£è¡¨è¾¾å¼å’Œé—®ç­”å¯¹ï¼Œä¿ƒè¿›æ¨¡å‹å­¦ä¹ æ°´ä¸‹ç¯å¢ƒçš„ç‰¹å¾å’ŒçŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUWBenchæ•°æ®é›†çš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ï¼šæ”¶é›†æ¥è‡ªä¸åŒæ°´ç”Ÿç¯å¢ƒï¼ˆæµ·æ´‹ã€çŠç‘šç¤ã€æ·±æµ·ï¼‰çš„é«˜åˆ†è¾¨ç‡æ°´ä¸‹å›¾åƒã€‚2) æ•°æ®æ ‡æ³¨ï¼šå¯¹å›¾åƒè¿›è¡Œäººå·¥æ ‡æ³¨ï¼ŒåŒ…æ‹¬ç‰©ä½“æŒ‡ä»£è¡¨è¾¾å¼ï¼ˆæè¿°å›¾åƒä¸­ç‰¹å®šç‰©ä½“ï¼‰å’Œé—®é¢˜-ç­”æ¡ˆå¯¹ï¼ˆæ¶µç›–ç‰©ä½“è¯†åˆ«ã€ç”Ÿæ€å…³ç³»ç­‰ï¼‰ã€‚3) åŸºå‡†å»ºç«‹ï¼šåŸºäºUWBenchæ•°æ®é›†ï¼Œå»ºç«‹ä¸‰ä¸ªç»¼åˆåŸºå‡†ï¼ŒåŒ…æ‹¬å›¾åƒå­—å¹•ã€è§†è§‰å®šä½å’Œè§†è§‰é—®ç­”ã€‚

**å…³é”®åˆ›æ–°**ï¼šUWBenchçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºæ°´ä¸‹è§†è§‰-è¯­è¨€ç†è§£è®¾è®¡çš„ç»¼åˆæ€§åŸºå‡†æ•°æ®é›†ã€‚å®ƒä¸ä»…åŒ…å«äº†å¤§é‡é«˜è´¨é‡çš„æ°´ä¸‹å›¾åƒï¼Œè¿˜æä¾›äº†ä¸°å¯Œçš„ã€äººå·¥éªŒè¯çš„æŒ‡ä»£è¡¨è¾¾å¼å’Œé—®ç­”å¯¹ï¼Œæ¶µç›–äº†æ°´ä¸‹ç¯å¢ƒç†è§£çš„å¤šä¸ªæ–¹é¢ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒUWBenchæ›´å…·é’ˆå¯¹æ€§å’Œä¸“ä¸šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ ‡æ³¨æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†äººå·¥éªŒè¯çš„æ–¹å¼ï¼Œç¡®ä¿æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚åœ¨é—®é¢˜-ç­”æ¡ˆå¯¹çš„è®¾è®¡ä¸Šï¼Œè®ºæ–‡æ¶µç›–äº†å¤šç§æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç‰©ä½“è¯†åˆ«ã€å±æ€§è¯†åˆ«ã€å…³ç³»æ¨ç†å’Œå¸¸è¯†æ¨ç†ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†è¿˜è€ƒè™‘äº†æ°´ä¸‹ç¯å¢ƒçš„å„ç§å˜åŒ–ï¼Œå¦‚èƒ½è§åº¦ã€å…‰ç…§æ¡ä»¶å’Œæ°´æµŠåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨UWBenchæ•°æ®é›†ä¸Šçš„è¡¨ç°è¿œä½äºåœ¨è‡ªç„¶åœºæ™¯æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¿™è¡¨æ˜æ°´ä¸‹è§†è§‰-è¯­è¨€ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸ºXX%ï¼Œä¸äººç±»æ°´å¹³å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¿™çªæ˜¾äº†UWBenchæ•°æ®é›†çš„ä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ˜ç¡®çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UWBenchæ•°æ®é›†çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æµ·æ´‹ç§‘å­¦ç ”ç©¶ã€ç”Ÿæ€ç¯å¢ƒç›‘æµ‹å’Œè‡ªä¸»æ°´ä¸‹æœºå™¨äººå¯¼èˆªã€‚è¯¥æ•°æ®é›†å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å¼ºå¤§çš„æ°´ä¸‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œä¿æŠ¤æµ·æ´‹ç¯å¢ƒã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºè‡ªåŠ¨è¯†åˆ«æµ·æ´‹ç”Ÿç‰©ã€è¯„ä¼°çŠç‘šç¤å¥åº·çŠ¶å†µå’Œè¾…åŠ©æ°´ä¸‹æœºå™¨äººè¿›è¡Œç›®æ ‡æœç´¢å’Œç¯å¢ƒæ¢ç´¢ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models (VLMs) have achieved remarkable success in natural scene understanding, yet their application to underwater environments remains largely unexplored. Underwater imagery presents unique challenges including severe light attenuation, color distortion, and suspended particle scattering, while requiring specialized knowledge of marine ecosystems and organism taxonomy. To bridge this gap, we introduce UWBench, a comprehensive benchmark specifically designed for underwater vision-language understanding. UWBench comprises 15,003 high-resolution underwater images captured across diverse aquatic environments, encompassing oceans, coral reefs, and deep-sea habitats. Each image is enriched with human-verified annotations including 15,281 object referring expressions that precisely describe marine organisms and underwater structures, and 124,983 question-answer pairs covering diverse reasoning capabilities from object recognition to ecological relationship understanding. The dataset captures rich variations in visibility, lighting conditions, and water turbidity, providing a realistic testbed for model evaluation. Based on UWBench, we establish three comprehensive benchmarks: detailed image captioning for generating ecologically informed scene descriptions, visual grounding for precise localization of marine organisms, and visual question answering for multimodal reasoning about underwater environments. Extensive experiments on state-of-the-art VLMs demonstrate that underwater understanding remains challenging, with substantial room for improvement. Our benchmark provides essential resources for advancing vision-language research in underwater contexts and supporting applications in marine science, ecological monitoring, and autonomous underwater exploration. Our code and benchmark will be available.

