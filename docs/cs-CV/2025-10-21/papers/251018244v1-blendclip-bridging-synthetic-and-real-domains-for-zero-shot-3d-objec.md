---
layout: default
title: BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining
---

# BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.18244" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.18244v1</a>
  <a href="https://arxiv.org/pdf/2510.18244.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.18244v1" onclick="toggleFavorite(this, '2510.18244v1', 'BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ajinkya Khoche, GergÅ‘ LÃ¡szlÃ³ Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-21

**å¤‡æ³¨**: Under Review

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/kesu1/BlendCLIP)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BlendCLIPï¼šé€šè¿‡å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡¥æ¥åˆæˆä¸çœŸå®åŸŸï¼Œå®ç°é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é›¶æ ·æœ¬å­¦ä¹ ` `3Dç‰©ä½“åˆ†ç±»` `å¤šæ¨¡æ€å­¦ä¹ ` `é¢†åŸŸè‡ªé€‚åº”` `è¯¾ç¨‹å­¦ä¹ ` `CLIP` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»æ–¹æ³•éš¾ä»¥åº”å¯¹åˆæˆæ•°æ®ä¸çœŸå®LiDARæ•°æ®é—´çš„é¢†åŸŸå·®å¼‚ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. BlendCLIPé€šè¿‡å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œç»“åˆåˆæˆæ•°æ®çš„è¯­ä¹‰ä¿¡æ¯å’ŒçœŸå®æ•°æ®çš„ç‰¹å¾ï¼Œå¼¥åˆé¢†åŸŸé¸¿æ²Ÿã€‚
3. å®éªŒè¡¨æ˜ï¼Œä»…éœ€å°‘é‡çœŸå®æ•°æ®å³å¯æ˜¾è‘—æå‡é›¶æ ·æœ¬åˆ†ç±»ç²¾åº¦ï¼Œå¹¶åœ¨nuScenesæ•°æ®é›†ä¸Šå–å¾—SOTAç»“æœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»å¯¹äºè‡ªåŠ¨é©¾é©¶ç­‰å®é™…åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†å¸¸å—åˆ°è®­ç»ƒæ‰€ç”¨çš„åˆæˆæ•°æ®ä¸çœŸå®ä¸–ç•Œä¸­ç¨€ç–ã€å˜ˆæ‚çš„æ¿€å…‰é›·è¾¾æ‰«æä¹‹é—´æ˜¾è‘—çš„é¢†åŸŸå·®è·çš„é˜»ç¢ã€‚å½“å‰ä»…åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ–¹æ³•æ— æ³•æ³›åŒ–åˆ°å®¤å¤–åœºæ™¯ï¼Œè€Œä»…åœ¨çœŸå®æ•°æ®ä¸Šè®­ç»ƒçš„æ–¹æ³•ç¼ºä¹è¯†åˆ«ç¨€æœ‰æˆ–æœªè§ç‰©ä½“çš„è¯­ä¹‰å¤šæ ·æ€§ã€‚æˆ‘ä»¬å¼•å…¥BlendCLIPï¼Œä¸€ä¸ªå¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç­–ç•¥æ€§åœ°ç»“åˆä¸¤ä¸ªé¢†åŸŸçš„ä¼˜åŠ¿æ¥å¼¥åˆè¿™ç§åˆæˆåˆ°çœŸå®çš„å·®è·ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªpipelineï¼Œç”¨äºç”Ÿæˆå¤§è§„æ¨¡çš„ç‰©ä½“çº§åˆ«ä¸‰å…ƒç»„æ•°æ®é›†ï¼ŒåŒ…å«ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œç›´æ¥ä»çœŸå®ä¸–ç•Œé©¾é©¶æ•°æ®å’Œäººå·¥æ ‡æ³¨çš„3Dæ¡†ä¸­æŒ–æ˜ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè´¡çŒ®æ˜¯ä¸€ç§åŸºäºè¯¾ç¨‹çš„æ•°æ®æ··åˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥é¦–å…ˆå°†æ¨¡å‹ç½®äºè¯­ä¹‰ä¸°å¯Œçš„åˆæˆCADæ•°æ®ä¸­ï¼Œç„¶åå†é€æ­¥å°†å…¶é€‚åº”çœŸå®ä¸–ç•Œæ‰«æçš„ç‰¹å®šç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„æ ‡ç­¾æ•ˆç‡ï¼šåœ¨è®­ç»ƒä¸­æ¯æ‰¹æ¬¡å¼•å…¥ä½è‡³1.5ï¼…çš„çœŸå®ä¸–ç•Œæ ·æœ¬ï¼Œå³å¯å°†nuScenesåŸºå‡†ä¸Šçš„é›¶æ ·æœ¬ç²¾åº¦æé«˜27ï¼…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å¤–æ•°æ®é›†ï¼ˆå¦‚nuSceneså’ŒTruckScenesï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨nuScenesä¸Šæ¯”æœ€ä½³ç°æœ‰æ–¹æ³•æé«˜äº†19.3ï¼…ï¼ŒåŒæ—¶åœ¨å„ç§åˆæˆåŸºå‡†ä¸Šä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œæœ‰æ•ˆçš„é¢†åŸŸè‡ªé€‚åº”ï¼Œè€Œä¸æ˜¯å…¨é¢çš„çœŸå®ä¸–ç•Œæ ‡æ³¨ï¼Œæ˜¯è§£é”é²æ£’çš„å¼€æ”¾è¯æ±‡3Dæ„ŸçŸ¥çš„å…³é”®ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†åœ¨æ¥å—åå‘å¸ƒåœ¨https://github.com/kesu1/BlendCLIPã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»ä¸­ï¼Œæ¨¡å‹åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒåï¼Œéš¾ä»¥æ³›åŒ–åˆ°çœŸå®ä¸–ç•ŒLiDARæ•°æ®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–å¤§é‡çœŸå®æ•°æ®æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ï¼›è¦ä¹ˆä»…ä½¿ç”¨åˆæˆæ•°æ®ï¼Œé¢†åŸŸå·®å¼‚å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œç»“åˆåˆæˆæ•°æ®çš„è¯­ä¹‰ä¿¡æ¯å’ŒçœŸå®æ•°æ®çš„ç‰¹å¾ï¼Œé€šè¿‡è¯¾ç¨‹å­¦ä¹ çš„æ–¹å¼ï¼Œé€æ­¥å°†æ¨¡å‹ä»åˆæˆåŸŸè¿ç§»åˆ°çœŸå®åŸŸã€‚è¿™æ ·æ—¢èƒ½åˆ©ç”¨åˆæˆæ•°æ®çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œåˆèƒ½é€‚åº”çœŸå®æ•°æ®çš„ç‰¹ç‚¹ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBlendCLIPæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«åˆæˆCADæ¨¡å‹å’ŒçœŸå®ä¸–ç•Œé©¾é©¶æ•°æ®ï¼ˆç‚¹äº‘ã€å›¾åƒã€æ–‡æœ¬æè¿°ï¼‰ï¼›2) ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå…ˆåœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹å­¦ä¹ é€šç”¨çš„3Dç‰©ä½“è¡¨ç¤ºï¼›3) é€æ­¥å¼•å…¥çœŸå®æ•°æ®ï¼Œé€šè¿‡æ•°æ®æ··åˆçš„æ–¹å¼ï¼Œä½¿æ¨¡å‹é€‚åº”çœŸå®æ•°æ®çš„ç‰¹å¾ï¼›4) åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŸºäºè¯¾ç¨‹çš„æ•°æ®æ··åˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†æ¨¡å‹ä»åˆæˆåŸŸè¿ç§»åˆ°çœŸå®åŸŸã€‚ä¸ç›´æ¥åœ¨æ··åˆæ•°æ®é›†ä¸Šè®­ç»ƒç›¸æ¯”ï¼Œè¯¾ç¨‹å­¦ä¹ èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨åˆæˆæ•°æ®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é€æ­¥é€‚åº”çœŸå®æ•°æ®çš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•°æ®é›†æ„å»ºï¼šä»çœŸå®ä¸–ç•Œé©¾é©¶æ•°æ®ä¸­æŒ–æ˜ç‰©ä½“çº§åˆ«çš„ä¸‰å…ƒç»„æ•°æ®ï¼ˆç‚¹äº‘ã€å›¾åƒã€æ–‡æœ¬æè¿°ï¼‰ï¼›2) è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼šè®¾è®¡åˆé€‚çš„è¯¾ç¨‹ï¼Œæ§åˆ¶åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®çš„æ··åˆæ¯”ä¾‹ï¼Œé€æ­¥å¢åŠ çœŸå®æ•°æ®çš„æ¯”ä¾‹ï¼›3) å¤šæ¨¡æ€èåˆï¼šä½¿ç”¨CLIPæ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå°†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯èåˆåœ¨ä¸€èµ·ï¼Œå­¦ä¹ ç»Ÿä¸€çš„ç‰©ä½“è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

BlendCLIPåœ¨nuScenesæ•°æ®é›†ä¸Šå®ç°äº†SOTAçš„é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»æ€§èƒ½ï¼Œç›¸æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æå‡äº†19.3%ã€‚é€šè¿‡å¼•å…¥å°‘é‡ï¼ˆ1.5%ï¼‰çœŸå®æ•°æ®ï¼Œå³å¯å°†é›¶æ ·æœ¬ç²¾åº¦æå‡27%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ‰æ•ˆçš„é¢†åŸŸè‡ªé€‚åº”æ˜¯æå‡é›¶æ ·æœ¬3Dæ„ŸçŸ¥èƒ½åŠ›çš„å…³é”®ï¼Œè€Œæ— éœ€ä¾èµ–å¤§è§„æ¨¡çš„çœŸå®æ•°æ®æ ‡æ³¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®‰é˜²ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»çš„æ€§èƒ½ï¼Œå¯ä»¥ä½¿æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­æ›´å¥½åœ°æ„ŸçŸ¥å’Œç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´å®‰å…¨ã€æ›´æ™ºèƒ½çš„è‡ªä¸»è¡Œä¸ºã€‚è¯¥æ–¹æ³•é™ä½äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Zero-shot 3D object classification is crucial for real-world applications like autonomous driving, however it is often hindered by a significant domain gap between the synthetic data used for training and the sparse, noisy LiDAR scans encountered in the real-world. Current methods trained solely on synthetic data fail to generalize to outdoor scenes, while those trained only on real data lack the semantic diversity to recognize rare or unseen objects.
>   We introduce BlendCLIP, a multimodal pretraining framework that bridges this synthetic-to-real gap by strategically combining the strengths of both domains. We first propose a pipeline to generate a large-scale dataset of object-level triplets -- consisting of a point cloud, image, and text description -- mined directly from real-world driving data and human annotated 3D boxes. Our core contribution is a curriculum-based data mixing strategy that first grounds the model in the semantically rich synthetic CAD data before progressively adapting it to the specific characteristics of real-world scans.
>   Our experiments show that our approach is highly label-efficient: introducing as few as 1.5\% real-world samples per batch into training boosts zero-shot accuracy on the nuScenes benchmark by 27\%. Consequently, our final model achieves state-of-the-art performance on challenging outdoor datasets like nuScenes and TruckScenes, improving over the best prior method by 19.3\% on nuScenes, while maintaining strong generalization on diverse synthetic benchmarks. Our findings demonstrate that effective domain adaptation, not full-scale real-world annotation, is the key to unlocking robust open-vocabulary 3D perception. Our code and dataset will be released upon acceptance on https://github.com/kesu1/BlendCLIP.

