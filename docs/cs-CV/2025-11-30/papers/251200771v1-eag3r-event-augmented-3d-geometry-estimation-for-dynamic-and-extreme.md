---
layout: default
title: EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes
---

# EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00771" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00771v1</a>
  <a href="https://arxiv.org/pdf/2512.00771.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00771v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.00771v1', 'EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoshan Wu, Yifei Yu, Xiaoyang Lyu, Yihua Huang, Bo Wang, Baoheng Zhang, Zhongrui Wang, Xiaojuan Qi

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**å¤‡æ³¨**: Accepted at NeurIPS 2025 (spotlight)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EAG3Rï¼šäº‹ä»¶ç›¸æœºå¢å¼ºçš„3Då‡ ä½•ä¼°è®¡ï¼Œè§£å†³åŠ¨æ€å’Œæç«¯å…‰ç…§åœºæ™¯é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Då‡ ä½•ä¼°è®¡` `äº‹ä»¶ç›¸æœº` `åŠ¨æ€åœºæ™¯` `ä½å…‰ç…§` `å¤šæ¨¡æ€èåˆ` `ä¿¡å™ªæ¯”æ„ŸçŸ¥` `å…‰åº¦ä¸€è‡´æ€§` `ç‚¹äº‘é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸRGBç›¸æœºåœ¨åŠ¨æ€ç‰©ä½“å’Œæç«¯å…‰ç…§æ¡ä»¶ä¸‹è¿›è¡Œ3Då‡ ä½•ä¼°è®¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶å’ŒSLAMç­‰é¢†åŸŸçš„åº”ç”¨ã€‚
2. EAG3Råˆ©ç”¨äº‹ä»¶ç›¸æœºæä¾›çš„å¼‚æ­¥äº‹ä»¶æµï¼Œé€šè¿‡ä¿¡å™ªæ¯”æ„ŸçŸ¥èåˆæœºåˆ¶è‡ªé€‚åº”åœ°ç»“åˆRGBå’Œäº‹ä»¶ç‰¹å¾ï¼Œæå‡å‡ ä½•ä¼°è®¡çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEAG3Råœ¨åŠ¨æ€ä½å…‰åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºçº¯RGBæ–¹æ³•ï¼Œæ— éœ€é¢å¤–å¤œé—´æ•°æ®è®­ç»ƒå³å¯å®ç°é«˜æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºEAG3Rï¼Œä¸€ç§æ–°é¢–çš„å‡ ä½•ä¼°è®¡æ¡†æ¶ï¼Œåˆ©ç”¨å¼‚æ­¥äº‹ä»¶æµå¢å¼ºåŸºäºç‚¹äº‘çš„é‡å»ºã€‚EAG3RåŸºäºMonST3Rä¸»å¹²ç½‘ç»œï¼Œå¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯å—Retinexå¯å‘çš„å›¾åƒå¢å¼ºæ¨¡å—å’Œè½»é‡çº§äº‹ä»¶é€‚é…å™¨ï¼Œé‡‡ç”¨ä¿¡å™ªæ¯”æ„ŸçŸ¥èåˆæœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°ç»“åˆRGBå’Œäº‹ä»¶ç‰¹å¾ï¼›äºŒæ˜¯åŸºäºäº‹ä»¶çš„å…‰åº¦ä¸€è‡´æ€§æŸå¤±ï¼Œå¢å¼ºå…¨å±€ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•æ— éœ€åœ¨å¤œé—´æ•°æ®ä¸Šé‡æ–°è®­ç»ƒï¼Œå³å¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŠ¨æ€ä½å…‰åœºæ™¯ä¸­å®ç°é²æ£’çš„å‡ ä½•ä¼°è®¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEAG3Råœ¨å•ç›®æ·±åº¦ä¼°è®¡ã€ç›¸æœºå§¿æ€è·Ÿè¸ªå’ŒåŠ¨æ€é‡å»ºä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„çº¯RGBæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºRGBå›¾åƒçš„3Då‡ ä½•ä¼°è®¡æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯å’Œæç«¯å…‰ç…§æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ã€‚ä¼ ç»Ÿç›¸æœºæ›å…‰æ—¶é—´å›ºå®šï¼Œå®¹æ˜“å—åˆ°è¿åŠ¨æ¨¡ç³Šå’Œå…‰ç…§å˜åŒ–çš„å½±å“ï¼Œå¯¼è‡´ç‰¹å¾æå–å’ŒåŒ¹é…å›°éš¾ï¼Œè¿›è€Œå½±å“é‡å»ºç²¾åº¦å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨äº‹ä»¶ç›¸æœºå¯¹å…‰ç…§å˜åŒ–çš„å¿«é€Ÿå“åº”å’Œé«˜åŠ¨æ€èŒƒå›´ç‰¹æ€§ï¼Œå¼¥è¡¥ä¼ ç»ŸRGBç›¸æœºçš„ä¸è¶³ã€‚é€šè¿‡èåˆRGBå›¾åƒå’Œäº‹ä»¶æµçš„ä¿¡æ¯ï¼Œæé«˜åœ¨åŠ¨æ€å’Œæç«¯å…‰ç…§æ¡ä»¶ä¸‹å‡ ä½•ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æ ¸å¿ƒåœ¨äºè‡ªé€‚åº”åœ°èåˆä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨äº‹ä»¶ä¿¡æ¯å¢å¼ºæ—¶ç©ºä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEAG3Ræ¡†æ¶åŸºäºMonST3Rï¼Œä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼š1) Retinexå¯å‘å¼å›¾åƒå¢å¼ºæ¨¡å—ï¼Œç”¨äºæå‡ä½å…‰ç…§RGBå›¾åƒçš„è´¨é‡ï¼›2) è½»é‡çº§äº‹ä»¶é€‚é…å™¨ï¼Œç”¨äºæå–äº‹ä»¶ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨ä¿¡å™ªæ¯”æ„ŸçŸ¥èåˆæœºåˆ¶ä¸RGBç‰¹å¾èåˆï¼›3) åŸºäºäº‹ä»¶çš„å…‰åº¦ä¸€è‡´æ€§æŸå¤±ï¼Œç”¨äºçº¦æŸå…¨å±€ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¢å¼ºæ—¶ç©ºä¸€è‡´æ€§ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šè¾“å…¥RGBå›¾åƒå’Œäº‹ä»¶æµï¼Œåˆ†åˆ«æå–ç‰¹å¾å¹¶èåˆï¼Œç„¶åè¿›è¡Œç‚¹äº‘å›å½’ï¼Œæœ€åé€šè¿‡å…¨å±€ä¼˜åŒ–å¾—åˆ°æœ€ç»ˆçš„3Då‡ ä½•ä¼°è®¡ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šEAG3Rçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§ä¿¡å™ªæ¯”æ„ŸçŸ¥èåˆæœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å±€éƒ¨å¯é æ€§è‡ªé€‚åº”åœ°ç»“åˆRGBå’Œäº‹ä»¶ç‰¹å¾ï¼Œé¿å…äº†ç®€å•èåˆå¯èƒ½å¸¦æ¥çš„å™ªå£°å¹²æ‰°ï¼›2) æå‡ºäº†ä¸€ç§åŸºäºäº‹ä»¶çš„å…‰åº¦ä¸€è‡´æ€§æŸå¤±ï¼Œåˆ©ç”¨äº‹ä»¶æµçš„æ—¶ç©ºä¿¡æ¯ï¼Œå¢å¼ºå…¨å±€ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§çº¦æŸï¼Œæé«˜äº†é‡å»ºçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šä¿¡å™ªæ¯”æ„ŸçŸ¥èåˆæœºåˆ¶é€šè¿‡è®¡ç®—RGBå’Œäº‹ä»¶ç‰¹å¾çš„ä¿¡å™ªæ¯”ï¼Œä½œä¸ºèåˆæƒé‡çš„ä¾æ®ã€‚ä¿¡å™ªæ¯”çš„è®¡ç®—æ–¹å¼æœªçŸ¥ï¼Œä½†å…¶ç›®çš„æ˜¯ä¸ºäº†æ›´å¯é åœ°èåˆä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯ã€‚åŸºäºäº‹ä»¶çš„å…‰åº¦ä¸€è‡´æ€§æŸå¤±åˆ©ç”¨äº‹ä»¶æµä¸­çš„æ—¶é—´æˆ³ä¿¡æ¯ï¼Œå¯¹ç›¸é‚»å¸§ä¹‹é—´çš„å…‰åº¦å˜åŒ–è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œçº¦æŸå…¨å±€ä¼˜åŒ–è¿‡ç¨‹ã€‚å…·ä½“ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEAG3Råœ¨å•ç›®æ·±åº¦ä¼°è®¡ã€ç›¸æœºå§¿æ€è·Ÿè¸ªå’ŒåŠ¨æ€é‡å»ºä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„çº¯RGBæ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŠ¨æ€ä½å…‰åœºæ™¯ä¸‹ï¼ŒEAG3Ræ— éœ€åœ¨å¤œé—´æ•°æ®ä¸Šé‡æ–°è®­ç»ƒå³å¯å®ç°é«˜æ€§èƒ½ï¼Œä½“ç°äº†å…¶è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EAG3Råœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€SLAMå’Œ3Dåœºæ™¯é‡å»ºç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å°¤å…¶æ˜¯åœ¨å…‰ç…§æ¡ä»¶æ¶åŠ£æˆ–å­˜åœ¨å¿«é€Ÿè¿åŠ¨ç‰©ä½“çš„åœºæ™¯ä¸‹ï¼ŒEAG3Rèƒ½å¤Ÿæä¾›æ›´å‡†ç¡®å’Œé²æ£’çš„3Då‡ ä½•ä¿¡æ¯ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºå¤œé—´æˆ–éš§é“ç­‰å¤æ‚ç¯å¢ƒä¸‹çš„è‡ªåŠ¨é©¾é©¶ï¼Œä»¥åŠåŠ¨æ€ç¯å¢ƒä¸‹çš„æœºå™¨äººæ“ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.

