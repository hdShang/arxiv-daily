---
layout: default
title: HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics
---

# HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00885" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00885v1</a>
  <a href="https://arxiv.org/pdf/2512.00885.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00885v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.00885v1', 'HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Masatoshi Tateno, Gido Kato, Hirokatsu Kataoka, Yoichi Sato, Takuma Yagi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**å¤‡æ³¨**: Project page: https://masatate.github.io/HanDyVQA-project-page/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HanDyVQAï¼šä¸€ä¸ªç”¨äºç»†ç²’åº¦æ‰‹-ç‰©äº¤äº’åŠ¨æ€çš„è§†é¢‘é—®ç­”åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `è§†é¢‘é—®ç­”` `æ‰‹-ç‰©äº¤äº’` `ç»†ç²’åº¦ç†è§£` `è§†é¢‘ç†è§£` `HOI` `æ—¶ç©ºæ¨ç†` `éƒ¨ä»¶çº§åˆ«ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰HOIåŸºå‡†ç¼ºä¹ç»†ç²’åº¦çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥æ•æ‰æ‰‹-ç‰©äº¤äº’ä¸­çš„åŠ¨æ€è¿‡ç¨‹ã€‚
2. HanDyVQAåŸºå‡†é€šè¿‡å…­ç§é—®é¢˜ç±»å‹å’Œç‰©ä½“éƒ¨ä»¶åˆ†å‰²æ©ç ï¼Œå…¨é¢è¯„ä¼°HOIçš„æ“ä½œå’Œæ•ˆæœã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰è§†é¢‘åŸºç¡€æ¨¡å‹åœ¨HanDyVQAä¸Šè¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜HOIåŠ¨æ€ç†è§£ä»å…·æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰‹-ç‰©äº¤äº’(HOI)æœ¬è´¨ä¸Šæ¶‰åŠåŠ¨æ€è¿‡ç¨‹ï¼Œå…¶ä¸­äººç±»æ“ä½œä¼šå¯¹ç‰©ä½“äº§ç”Ÿç‹¬ç‰¹çš„æ—¶ç©ºå½±å“ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­ä¹‰HOIåŸºå‡†è¦ä¹ˆä¾§é‡äºæ“ä½œï¼Œè¦ä¹ˆä¾§é‡äºç²—ç•¥å±‚é¢çš„ç»“æœå½±å“ï¼Œç¼ºä¹æ•è·HOIåº•å±‚åŠ¨æ€çš„ç»†ç²’åº¦æ—¶ç©ºæ¨ç†ã€‚æˆ‘ä»¬æ¨å‡ºäº†HanDyVQAï¼Œè¿™æ˜¯ä¸€ä¸ªç»†ç²’åº¦çš„è§†é¢‘é—®ç­”åŸºå‡†ï¼Œå…¨é¢æ¶µç›–äº†HOIçš„æ“ä½œå’Œæ•ˆæœä¸¤æ–¹é¢ã€‚HanDyVQAåŒ…å«å…­ç§äº’è¡¥çš„é—®é¢˜ç±»å‹ï¼ˆåŠ¨ä½œã€è¿‡ç¨‹ã€ç‰©ä½“ã€ä½ç½®ã€çŠ¶æ€å˜åŒ–å’Œç‰©ä½“éƒ¨ä»¶ï¼‰ï¼Œæ€»è®¡11.1Kä¸ªå¤šé¡¹é€‰æ‹©QAå¯¹ã€‚æ”¶é›†çš„QAå¯¹èƒ½å¤Ÿè¯†åˆ«æ“ä½œé£æ ¼ã€æ‰‹/ç‰©ä½“çš„è¿åŠ¨ä»¥åŠéƒ¨ä»¶çº§åˆ«çš„çŠ¶æ€å˜åŒ–ã€‚HanDyVQAè¿˜åŒ…æ‹¬10.3Kä¸ªç‰©ä½“å’Œç‰©ä½“éƒ¨ä»¶çš„åˆ†å‰²æ©ç ï¼Œä»è€Œèƒ½å¤Ÿè¯„ä¼°è§†é¢‘ç‰©ä½“åˆ†å‰²ä¸­ç‰©ä½“/éƒ¨ä»¶çº§åˆ«çš„æ¨ç†ã€‚æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„åŸºå‡†ä¸Šè¯„ä¼°äº†æœ€æ–°çš„è§†é¢‘åŸºç¡€æ¨¡å‹ï¼Œå‘ç°å³ä½¿æ˜¯æ€§èƒ½æœ€ä½³çš„æ¨¡å‹Gemini-2.5-Proä¹Ÿä»…è¾¾åˆ°73%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¿œä½äºäººç±»çš„è¡¨ç°ï¼ˆ97%ï¼‰ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œç©ºé—´å…³ç³»ã€è¿åŠ¨å’Œéƒ¨ä»¶çº§åˆ«çš„å‡ ä½•ç†è§£ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå°†æ˜¾å¼çš„HOIç›¸å…³çº¿ç´¢é›†æˆåˆ°è§†è§‰ç‰¹å¾ä¸­å¯ä»¥æé«˜æ€§èƒ½ï¼Œè¿™ä¸ºå¼€å‘æœªæ¥å…·æœ‰æ›´æ·±å…¥HOIåŠ¨æ€ç†è§£çš„æ¨¡å‹æä¾›äº†è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰HOIè§†é¢‘ç†è§£æ–¹æ³•ä¸»è¦å…³æ³¨ç²—ç²’åº¦çš„æ“ä½œæˆ–ç»“æœï¼Œç¼ºä¹å¯¹ç»†ç²’åº¦æ—¶ç©ºåŠ¨æ€çš„å»ºæ¨¡èƒ½åŠ›ã€‚è¿™å¯¼è‡´æ¨¡å‹éš¾ä»¥ç†è§£æ‰‹éƒ¨åŠ¨ä½œå¦‚ä½•å½±å“ç‰©ä½“çŠ¶æ€ï¼Œä»¥åŠç‰©ä½“éƒ¨ä»¶ä¹‹é—´çš„äº¤äº’å…³ç³»ã€‚ç°æœ‰åŸºå‡†æ— æ³•å……åˆ†è¯„ä¼°æ¨¡å‹åœ¨è¿™äº›æ–¹é¢çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHanDyVQAçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŒ…å«ç»†ç²’åº¦æ ‡æ³¨çš„è§†é¢‘é—®ç­”æ•°æ®é›†ï¼Œæ¶µç›–æ‰‹-ç‰©äº¤äº’çš„å„ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬åŠ¨ä½œã€è¿‡ç¨‹ã€ç‰©ä½“ã€ä½ç½®ã€çŠ¶æ€å˜åŒ–å’Œç‰©ä½“éƒ¨ä»¶ã€‚é€šè¿‡è®¾è®¡ä¸åŒç±»å‹çš„é—®é¢˜ï¼Œè¿«ä½¿æ¨¡å‹è¿›è¡Œæ›´æ·±å…¥çš„æ—¶ç©ºæ¨ç†å’Œéƒ¨ä»¶çº§åˆ«çš„ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHanDyVQAæ•°æ®é›†åŒ…å«11.1Kä¸ªå¤šé¡¹é€‰æ‹©QAå¯¹å’Œ10.3Kä¸ªç‰©ä½“å’Œç‰©ä½“éƒ¨ä»¶çš„åˆ†å‰²æ©ç ã€‚é—®é¢˜ç±»å‹åŒ…æ‹¬ï¼šAction (åŠ¨ä½œ)ã€Process (è¿‡ç¨‹)ã€Objects (ç‰©ä½“)ã€Location (ä½ç½®)ã€State Change (çŠ¶æ€å˜åŒ–)å’ŒObject Parts (ç‰©ä½“éƒ¨ä»¶)ã€‚åˆ†å‰²æ©ç ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç‰©ä½“å’Œéƒ¨ä»¶çº§åˆ«çš„åˆ†å‰²èƒ½åŠ›ã€‚æ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹åŒ…æ‹¬è§†é¢‘æ”¶é›†ã€é—®é¢˜ç”Ÿæˆã€ç­”æ¡ˆæ ‡æ³¨å’Œåˆ†å‰²æ©ç æ ‡æ³¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šHanDyVQAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»†ç²’åº¦çš„æ ‡æ³¨å’Œå…¨é¢çš„é—®é¢˜ç±»å‹è®¾è®¡ã€‚ä¸ç°æœ‰HOIåŸºå‡†ç›¸æ¯”ï¼ŒHanDyVQAæ›´å…³æ³¨æ‰‹éƒ¨åŠ¨ä½œå¯¹ç‰©ä½“çŠ¶æ€çš„å½±å“ï¼Œä»¥åŠç‰©ä½“éƒ¨ä»¶ä¹‹é—´çš„äº¤äº’å…³ç³»ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†è¿˜æä¾›äº†ç‰©ä½“å’Œéƒ¨ä»¶çš„åˆ†å‰²æ©ç ï¼Œä»è€Œèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹åœ¨ç‰©ä½“å’Œéƒ¨ä»¶çº§åˆ«çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†ä¸­çš„é—®é¢˜è®¾è®¡æ—¨åœ¨æ¶µç›–HOIçš„å„ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬åŠ¨ä½œç±»å‹ã€æ“ä½œè¿‡ç¨‹ã€æ¶‰åŠçš„ç‰©ä½“ã€ç‰©ä½“çš„ä½ç½®ã€çŠ¶æ€å˜åŒ–ä»¥åŠç‰©ä½“éƒ¨ä»¶ã€‚åˆ†å‰²æ©ç çš„æ ‡æ³¨é‡‡ç”¨äº†äººå·¥æ ‡æ³¨çš„æ–¹å¼ï¼Œä¿è¯äº†æ ‡æ³¨çš„å‡†ç¡®æ€§ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å¹³å‡å‡†ç¡®ç‡å’Œåˆ†å‰²æŒ‡æ ‡ï¼ˆå¦‚IoUï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨HanDyVQAåŸºå‡†ä¸Šï¼Œå³ä½¿æ˜¯æ€§èƒ½æœ€ä½³çš„è§†é¢‘åŸºç¡€æ¨¡å‹Gemini-2.5-Proä¹Ÿä»…è¾¾åˆ°73%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¿œä½äºäººç±»çš„è¡¨ç°ï¼ˆ97%ï¼‰ã€‚è¿™è¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨ç»†ç²’åº¦æ‰‹-ç‰©äº¤äº’ç†è§£æ–¹é¢ä»å­˜åœ¨å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå°†æ˜¾å¼çš„HOIç›¸å…³çº¿ç´¢é›†æˆåˆ°è§†è§‰ç‰¹å¾ä¸­å¯ä»¥æé«˜æ€§èƒ½ï¼Œè¿™ä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HanDyVQAå¯ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è§†é¢‘ç†è§£æ¨¡å‹åœ¨æ‰‹-ç‰©äº¤äº’æ–¹é¢çš„èƒ½åŠ›ï¼Œä¿ƒè¿›æ™ºèƒ½æœºå™¨äººã€äººæœºäº¤äº’ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸçš„å‘å±•ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çš„æŒ‡ä»¤å¹¶æ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†è¿˜å¯ä»¥ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„è§†é¢‘ç›‘æ§ç³»ç»Ÿï¼Œèƒ½å¤Ÿè¯†åˆ«å¼‚å¸¸è¡Œä¸ºå¹¶åŠæ—¶å‘å‡ºè­¦æŠ¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.

