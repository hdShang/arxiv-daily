---
layout: default
title: Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction
---

# Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00960" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00960v2</a>
  <a href="https://arxiv.org/pdf/2512.00960.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00960v2" onclick="toggleFavorite(this, '2512.00960v2', 'Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boran Wen, Ye Lu, Keyan Wan, Sirui Wang, Jiahong Zhou, Junxuan Liang, Xinpeng Liu, Bang Xiao, Dingbang Huang, Ruiyang Liu, Yong-Lu Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30 (æ›´æ–°: 2025-12-06)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://wenboran2002.github.io/open4dhoi/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º4DHOISolveræ¡†æ¶ï¼Œç»“åˆäººå·¥æ ‡æ³¨ï¼Œé«˜æ•ˆé‡å»ºå•ç›®è§†é¢‘ä¸­çš„äºº-ç‰©äº¤äº’è¿åŠ¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `äºº-ç‰©äº¤äº’` `4Dé‡å»º` `å•ç›®è§†é¢‘` `è¿åŠ¨æ•æ‰` `å¼ºåŒ–å­¦ä¹ ` `æ•°æ®é›†` `æ¥è§¦ç‚¹æ ‡æ³¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä»å•ç›®è§†é¢‘ä¸­å‡†ç¡®ä¸”å¯æ‰©å±•åœ°æå–4Däºº-ç‰©äº¤äº’æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„éš¾é¢˜ï¼Œé˜»ç¢äº†æœºå™¨äººä»äº’è”ç½‘è§†é¢‘ä¸­å­¦ä¹ ã€‚
2. è®ºæ–‡æå‡ºäº†4DHOISolveræ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç¨€ç–çš„äººå·¥æ¥è§¦ç‚¹æ ‡æ³¨ï¼Œçº¦æŸ4D HOIé‡å»ºé—®é¢˜ï¼Œä¿è¯æ—¶ç©ºä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§ã€‚
3. æ„å»ºäº†å¤§è§„æ¨¡4D HOIæ•°æ®é›†Open4DHOIï¼Œå¹¶éªŒè¯äº†é‡å»ºç»“æœåœ¨å¼ºåŒ–å­¦ä¹ æ¨¡ä»¿ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶æŒ‡å‡ºç°æœ‰3Dæ¨¡å‹åœ¨æ¥è§¦é¢„æµ‹æ–¹é¢çš„ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†ä½¿é€šç”¨æœºå™¨äººèƒ½å¤Ÿä»å¤šæ ·åŒ–ã€å¤§è§„æ¨¡çš„äºº-ç‰©äº¤äº’(HOI)ä¸­å­¦ä¹ ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–é«˜æ•ˆçš„ä¼˜åŒ–æ¡†æ¶4DHOISolverï¼Œç”¨äºçº¦æŸç—…æ€çš„4D HOIé‡å»ºé—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¨€ç–çš„äººå·¥æ¥è§¦ç‚¹æ ‡æ³¨ï¼ŒåŒæ—¶ä¿æŒé«˜æ—¶ç©ºä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡4D HOIæ•°æ®é›†Open4DHOIï¼ŒåŒ…å«144ç§ç‰©ä½“ç±»å‹å’Œ103ç§åŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼Œé‡å»ºç»“æœèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä½“æ¨¡ä»¿ã€‚ç„¶è€Œï¼Œå¯¹ç°æœ‰3DåŸºç¡€æ¨¡å‹çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œè‡ªåŠ¨é¢„æµ‹ç²¾ç¡®çš„äºº-ç‰©æ¥è§¦å¯¹åº”å…³ç³»ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ï¼Œçªæ˜¾äº†äººå·¥å‚ä¸ç­–ç•¥çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºç¤¾åŒºæå‡ºäº†ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚æ•°æ®å’Œä»£ç å°†åœ¨æŒ‡å®šç½‘å€å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•éš¾ä»¥ä»å•ç›®è§†é¢‘ä¸­å‡†ç¡®ä¸”å¯æ‰©å±•åœ°é‡å»º4Däºº-ç‰©äº¤äº’ï¼ˆHOIï¼‰è¿åŠ¨ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºï¼šå•ç›®è§†è§‰çš„å›ºæœ‰æ­§ä¹‰æ€§å¯¼è‡´é‡å»ºç»“æœä¸å‡†ç¡®ï¼Œç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡çš„4D HOIæ•°æ®é›†ï¼Œä»¥åŠè‡ªåŠ¨é¢„æµ‹ç²¾ç¡®çš„äºº-ç‰©æ¥è§¦å¯¹åº”å…³ç³»ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç¨€ç–çš„äººå·¥æ¥è§¦ç‚¹æ ‡æ³¨æ¥çº¦æŸ4D HOIé‡å»ºè¿‡ç¨‹ï¼Œä»è€Œè§£å†³å•ç›®è§†è§‰çš„æ­§ä¹‰æ€§é—®é¢˜ã€‚äººå·¥æ ‡æ³¨æä¾›å…³é”®çš„äº¤äº’ä¿¡æ¯ï¼Œå¸®åŠ©ä¼˜åŒ–ç®—æ³•æ‰¾åˆ°æ›´å‡†ç¡®çš„è§£ã€‚åŒæ—¶ï¼Œé€šè¿‡ä¼˜åŒ–æ¡†æ¶ä¿è¯é‡å»ºç»“æœçš„æ—¶ç©ºä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼š4DHOISolveræ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘è¾“å…¥å’Œé¢„å¤„ç†ï¼šä»å•ç›®è§†é¢‘ä¸­æå–äººä½“å’Œç‰©ä½“çš„2D/3Dä¿¡æ¯ã€‚2) äººå·¥æ¥è§¦ç‚¹æ ‡æ³¨ï¼šäººå·¥æ ‡æ³¨è§†é¢‘å¸§ä¸­äººä¸ç‰©ä½“çš„æ¥è§¦ç‚¹ã€‚3) ä¼˜åŒ–æ¡†æ¶ï¼šåˆ©ç”¨äººå·¥æ ‡æ³¨çš„æ¥è§¦ç‚¹ä½œä¸ºçº¦æŸï¼Œä¼˜åŒ–äººä½“å’Œç‰©ä½“çš„4Då§¿æ€ï¼ŒåŒæ—¶ä¿è¯æ—¶ç©ºä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§ã€‚4) è¿åŠ¨é‡å»ºï¼šè¾“å‡ºé‡å»ºçš„4D HOIè¿åŠ¨åºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºç»“åˆäº†äººå·¥æ ‡æ³¨å’Œä¼˜åŒ–æ¡†æ¶ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å•ç›®4D HOIé‡å»ºçš„ç—…æ€é—®é¢˜ã€‚ä¸å®Œå…¨ä¾èµ–è‡ªåŠ¨ç®—æ³•çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè·å¾—æ›´å‡†ç¡®ã€æ›´å¯é çš„é‡å»ºç»“æœã€‚æ­¤å¤–ï¼Œæ„å»ºäº†å¤§è§„æ¨¡çš„Open4DHOIæ•°æ®é›†ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å®è´µçš„æ•°æ®èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç¨€ç–æ¥è§¦ç‚¹æ ‡æ³¨ç­–ç•¥ï¼šåªéœ€è¦å°‘é‡çš„äººå·¥æ ‡æ³¨å³å¯æœ‰æ•ˆçº¦æŸé‡å»ºè¿‡ç¨‹ã€‚2) æ—¶ç©ºä¸€è‡´æ€§æŸå¤±ï¼šä¿è¯é‡å»ºç»“æœåœ¨æ—¶é—´ä¸Šçš„å¹³æ»‘æ€§å’Œä¸€è‡´æ€§ã€‚3) ç‰©ç†åˆç†æ€§æŸå¤±ï¼šä¿è¯é‡å»ºç»“æœç¬¦åˆç‰©ç†è§„å¾‹ï¼Œä¾‹å¦‚é¿å…ç©¿é€ç­‰ã€‚4) ä¼˜åŒ–ç®—æ³•ï¼šé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç®—æ³•ï¼Œä¾‹å¦‚åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°å¹¶è·å¾—æœ€ä¼˜çš„é‡å»ºç»“æœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æ„å»ºäº†åŒ…å«144ç§ç‰©ä½“ç±»å‹å’Œ103ç§åŠ¨ä½œçš„å¤§è§„æ¨¡4D HOIæ•°æ®é›†Open4DHOIã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨4DHOISolveré‡å»ºçš„è¿åŠ¨æ•°æ®èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä½“æ¨¡ä»¿ä»»åŠ¡ï¼ŒéªŒè¯äº†é‡å»ºç»“æœçš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œè®ºæ–‡å¯¹ç°æœ‰3DåŸºç¡€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å…¶åœ¨è‡ªåŠ¨é¢„æµ‹ç²¾ç¡®çš„äºº-ç‰©æ¥è§¦å¯¹åº”å…³ç³»æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå­¦ä¹ ã€è™šæ‹Ÿç°å®ã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ äººç±»ä¸ç‰©ä½“çš„äº¤äº’æ–¹å¼ï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œæ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œä»è€Œåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡ã€‚é‡å»ºçš„4D HOIæ•°æ®å¯ä»¥ç”¨äºè®­ç»ƒæœºå™¨äººæ§åˆ¶ç­–ç•¥ï¼Œæé«˜æœºå™¨äººçš„æ“ä½œæŠ€èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºåˆ›å»ºé€¼çœŸçš„è™šæ‹Ÿç°å®ä½“éªŒï¼Œä¾‹å¦‚æ¨¡æ‹Ÿäººç±»æ“ä½œç‰©ä½“çš„è¿‡ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/

