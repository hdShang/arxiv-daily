---
layout: default
title: Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders
---

# Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26027" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26027v1</a>
  <a href="https://arxiv.org/pdf/2510.26027.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26027v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.26027v1', 'Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-29

**å¤‡æ³¨**: Accepted to NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://alirasekh.github.io/STAVEQ2/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSTAVEï¼Œé€šè¿‡åœ¨è§†è§‰ç¼–ç å™¨ä¸­å †å æ—¶é—´æ³¨æ„åŠ›å¢å¼ºVideo-LLMçš„æ—¶é—´ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Video-LLM` `æ—¶é—´æ³¨æ„åŠ›` `è§†è§‰ç¼–ç å™¨` `è§†é¢‘ç†è§£` `åŠ¨ä½œè¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Video-LLMåœ¨ç†è§£è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç†è§£åŠ¨ä½œåºåˆ—å’Œæ—¶é—´è¿›å±•çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºåœ¨è§†è§‰ç¼–ç å™¨ä¸­å †å æ—¶é—´æ³¨æ„åŠ›æ¨¡å—ï¼ˆSTAVEï¼‰ï¼Œä»¥å¢å¼ºæ¨¡å‹æ•æ‰åŠ¨ä½œè¿›å±•å’Œå¸§é—´å…³ç³»çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç†è§£è§†é¢‘ä¸­å¤æ‚çš„æ—¶åºåŠ¨æ€ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹(Video-LLM)æ¶æ„åœ¨æ—¶é—´ç†è§£æ–¹é¢å­˜åœ¨ä¸¥é‡å±€é™ï¼Œéš¾ä»¥å¤„ç†éœ€è¦è¯¦ç»†ç†è§£åŠ¨ä½œåºåˆ—å’Œæ—¶é—´è¿›å±•çš„ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§Video-LLMæ¶æ„ï¼Œè¯¥æ¶æ„ç›´æ¥åœ¨è§†è§‰ç¼–ç å™¨ä¸­å¼•å…¥å †å çš„æ—¶é—´æ³¨æ„åŠ›æ¨¡å—ã€‚è¿™ç§è®¾è®¡åœ¨è§†è§‰ç¼–ç å™¨ä¸­åŠ å…¥äº†æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åŠ¨ä½œçš„è¿›å±•å’Œå¸§ä¹‹é—´çš„å…³ç³»ï¼Œç„¶åå†å°†è§†è§‰tokensä¼ é€’ç»™LLMã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡(ç‰¹åˆ«æ˜¯åŠ¨ä½œè¯†åˆ«)ä¸­ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨VITATECSã€MVBenchå’ŒVideo-MMEç­‰åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†é«˜è¾¾+5.5%ã€‚é€šè¿‡ä½¿ç”¨æ—¶é—´ç»“æ„å¢å¼ºè§†è§‰ç¼–ç å™¨ï¼Œæˆ‘ä»¬è§£å†³äº†Video-LLMè§†é¢‘ç†è§£ä¸­çš„ä¸€ä¸ªå…³é”®ç¼ºå£ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯åœ¨https://alirasekh.github.io/STAVEQ2/ä¸Šæ‰¾åˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„Video-LLMåœ¨å¤„ç†éœ€è¦æ·±å…¥ç†è§£æ—¶é—´åºåˆ—ä¿¡æ¯çš„è§†é¢‘ä»»åŠ¡æ—¶è¡¨ç°ä¸è¶³ï¼Œæ— æ³•å‡†ç¡®æ•æ‰åŠ¨ä½œçš„æ¼”å˜å’Œå¸§ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä¾‹å¦‚éœ€è¦ç†è§£åŠ¨ä½œé¡ºåºçš„è§†é¢‘é—®ç­”ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨è§†è§‰ç¼–ç å™¨ä¸­å¼•å…¥æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å°†è§†è§‰ä¿¡æ¯ä¼ é€’ç»™LLMä¹‹å‰ï¼Œæ›´å¥½åœ°ç†è§£è§†é¢‘å¸§ä¹‹é—´çš„æ—¶é—´å…³ç³»å’ŒåŠ¨ä½œçš„æ¼”å˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥Video-LLMæ¶æ„çš„æ ¸å¿ƒåœ¨äºå…¶è§†è§‰ç¼–ç å™¨ï¼Œå…¶ä¸­é›†æˆäº†å †å çš„æ—¶é—´æ³¨æ„åŠ›æ¨¡å—ï¼ˆStacked Temporal Attentionï¼‰ã€‚è¯¥æ¶æ„é¦–å…ˆä½¿ç”¨è§†è§‰ç¼–ç å™¨æå–è§†é¢‘å¸§çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡å †å çš„æ—¶é—´æ³¨æ„åŠ›æ¨¡å—æ¥æ•æ‰å¸§ä¹‹é—´çš„æ—¶é—´å…³ç³»ã€‚æœ€åï¼Œå°†æå–çš„ç‰¹å¾ä¼ é€’ç»™LLMè¿›è¡Œåç»­å¤„ç†ï¼Œä¾‹å¦‚è§†é¢‘é—®ç­”ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ—¶é—´æ³¨æ„åŠ›æ¨¡å—ç›´æ¥é›†æˆåˆ°è§†è§‰ç¼–ç å™¨ä¸­ã€‚ä¸ä¼ ç»Ÿçš„åœ¨LLMç«¯å¤„ç†æ—¶é—´ä¿¡æ¯çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•å…è®¸æ¨¡å‹åœ¨æ—©æœŸé˜¶æ®µå°±å­¦ä¹ åˆ°è§†é¢‘çš„æ—¶é—´ç»“æ„ï¼Œä»è€Œæé«˜äº†æ—¶é—´æ¨ç†çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ—¶é—´æ³¨æ„åŠ›æ¨¡å—çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹å…¶åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥è®¡ç®—ä¸åŒå¸§ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶æ ¹æ®ç›¸å…³æ€§å¯¹å¸§ç‰¹å¾è¿›è¡ŒåŠ æƒã€‚å †å å¤šä¸ªæ—¶é—´æ³¨æ„åŠ›æ¨¡å—å¯ä»¥ä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´å¤æ‚çš„æ—¶é—´ä¾èµ–å…³ç³»ã€‚æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥çš„å…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨VITATECSã€MVBenchå’ŒVideo-MMEç­‰è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ï¼Œæœ€é«˜æå‡å¹…åº¦è¾¾åˆ°+5.5%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡åœ¨è§†è§‰ç¼–ç å™¨ä¸­å¼•å…¥æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜Video-LLMçš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°æ›´å‡ºè‰²ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦ç†è§£è§†é¢‘æ—¶é—´åŠ¨æ€çš„é¢†åŸŸï¼Œä¾‹å¦‚è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…å’ŒåŒ»ç–—è¯Šæ–­ã€‚é€šè¿‡æé«˜Video-LLMçš„æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´æ™ºèƒ½çš„è§†é¢‘åˆ†æå’Œç†è§£ï¼Œä»è€Œä¸ºè¿™äº›é¢†åŸŸå¸¦æ¥æ›´é«˜æ•ˆã€æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚æœªæ¥çš„å½±å“åŒ…æ‹¬æ›´ç²¾ç¡®çš„åŠ¨ä½œè¯†åˆ«ã€æ›´æ™ºèƒ½çš„è§†é¢‘æœç´¢å’Œæ›´è‡ªç„¶çš„è§†é¢‘äº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite significant advances in Multimodal Large Language Models (MLLMs), understanding complex temporal dynamics in videos remains a major challenge. Our experiments show that current Video Large Language Model (Video-LLM) architectures have critical limitations in temporal understanding, struggling with tasks that require detailed comprehension of action sequences and temporal progression. In this work, we propose a Video-LLM architecture that introduces stacked temporal attention modules directly within the vision encoder. This design incorporates a temporal attention in vision encoder, enabling the model to better capture the progression of actions and the relationships between frames before passing visual tokens to the LLM. Our results show that this approach significantly improves temporal reasoning and outperforms existing models in video question answering tasks, specifically in action recognition. We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to +5.5%. By enhancing the vision encoder with temporal structure, we address a critical gap in video understanding for Video-LLMs. Project page and code are available at: https://alirasekh.github.io/STAVEQ2/.

