---
layout: default
title: RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models
---

# RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.25257" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.25257v1</a>
  <a href="https://arxiv.org/pdf/2510.25257.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.25257v1" onclick="toggleFavorite(this, '2510.25257v1', 'RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zijun Liao, Yian Zhao, Xin Shan, Yu Yan, Chang Liu, Lei Lu, Xiangyang Ji, Jie Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-29

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RT-DETRv4ï¼šåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæ— ç—›æå‡å®æ—¶ç›®æ ‡æ£€æµ‹æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®æ—¶ç›®æ ‡æ£€æµ‹` `çŸ¥è¯†è’¸é¦` `è§†è§‰åŸºç¡€æ¨¡å‹` `æ·±åº¦è¯­ä¹‰æ³¨å…¥` `æ¢¯åº¦è‡ªé€‚åº”è°ƒåˆ¶` `è½»é‡çº§æ¨¡å‹` `COCOæ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å®æ—¶ç›®æ ‡æ£€æµ‹å™¨å—é™äºè½»é‡åŒ–è®¾è®¡ï¼Œç‰¹å¾è¡¨è¾¾èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºè§†è§‰åŸºç¡€æ¨¡å‹çš„è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡æ·±åº¦è¯­ä¹‰æ³¨å…¥å’Œæ¢¯åº¦å¼•å¯¼è‡ªé€‚åº”è°ƒåˆ¶å®ç°çŸ¥è¯†è¿ç§»ã€‚
3. RT-DETRv4æ¨¡å‹åœ¨COCOæ•°æ®é›†ä¸Šå–å¾—SOTAç»“æœï¼Œå¹¶åœ¨ä¸åŒé€Ÿåº¦ä¸‹å‡æœ‰æ˜¾è‘—çš„APæå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®æ—¶ç›®æ ‡æ£€æµ‹é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„å’Œä¼˜åŒ–ç­–ç•¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œé€šè¿‡è½»é‡çº§ç½‘ç»œè®¾è®¡è¿½æ±‚é«˜é€Ÿæ¨ç†é€šå¸¸ä¼šå¯¼è‡´ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ä¸‹é™ï¼Œè¿™é˜»ç¢äº†æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡å’Œå®é™…çš„è®¾å¤‡ç«¯éƒ¨ç½²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»æµé«˜æ•ˆä¸”é«˜åº¦é€‚åº”æ€§çš„è’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨å¿«é€Ÿå‘å±•çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„èƒ½åŠ›æ¥å¢å¼ºè½»é‡çº§ç›®æ ‡æ£€æµ‹å™¨ã€‚è€ƒè™‘åˆ°VFMså’Œèµ„æºå—é™çš„æ£€æµ‹å™¨ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ¶æ„å’Œå­¦ä¹ ç›®æ ‡å·®å¼‚ï¼Œå®ç°ç¨³å®šä¸”ä»»åŠ¡å¯¹é½çš„è¯­ä¹‰è¿ç§»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ·±åº¦è¯­ä¹‰æ³¨å…¥å™¨ï¼ˆDSIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—æœ‰åŠ©äºå°†VFMsçš„é«˜çº§è¡¨ç¤ºé›†æˆåˆ°æ£€æµ‹å™¨çš„æ·±å±‚ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ¢¯åº¦å¼•å¯¼çš„è‡ªé€‚åº”è°ƒåˆ¶ï¼ˆGAMï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®æ¢¯åº¦èŒƒæ•°æ¯”ç‡åŠ¨æ€è°ƒæ•´è¯­ä¹‰è¿ç§»çš„å¼ºåº¦ã€‚åœ¨ä¸å¢åŠ éƒ¨ç½²å’Œæ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºäºDETRçš„æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—ä¸”ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œçªæ˜¾äº†å…¶åœ¨å®æ—¶æ£€æµ‹ä¸­çš„å®é™…æ•ˆç”¨ã€‚æˆ‘ä»¬çš„æ–°æ¨¡å‹ç³»åˆ—RT-DETRv4åœ¨COCOä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨273/169/124/78 FPSçš„é€Ÿåº¦ä¸‹ï¼ŒAPåˆ†æ•°åˆ†åˆ«è¾¾åˆ°49.7/53.5/55.4/57.0ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å®æ—¶ç›®æ ‡æ£€æµ‹ä¸­ï¼Œè½»é‡çº§æ¨¡å‹å› ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ä¸è¶³è€Œå¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸ºäº†è¿½æ±‚é€Ÿåº¦ï¼Œç‰ºç‰²äº†æ¨¡å‹æ·±åº¦å’Œå¤æ‚åº¦ï¼Œä½¿å¾—æ£€æµ‹ç²¾åº¦éš¾ä»¥è¿›ä¸€æ­¥æå‡ã€‚åŒæ—¶ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„å¼ºå¤§è¯­ä¹‰ä¿¡æ¯è¿ç§»åˆ°è½»é‡çº§æ£€æµ‹å™¨ä¸­ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„å¼ºå¤§ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦çš„æ–¹å¼æå‡è½»é‡çº§ç›®æ ‡æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³VFMså’Œè½»é‡çº§æ£€æµ‹å™¨ä¹‹é—´çš„å·®å¼‚ï¼Œè®ºæ–‡è®¾è®¡äº†æ·±åº¦è¯­ä¹‰æ³¨å…¥å™¨ï¼ˆDSIï¼‰å’Œæ¢¯åº¦å¼•å¯¼çš„è‡ªé€‚åº”è°ƒåˆ¶ï¼ˆGAMï¼‰ç­–ç•¥ï¼Œä»¥å®ç°æ›´ç¨³å®šå’Œæœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œå’Œä¸€ä¸ªè½»é‡çº§ç›®æ ‡æ£€æµ‹å™¨ä½œä¸ºå­¦ç”Ÿæ¨¡å‹ã€‚DSIæ¨¡å—è´Ÿè´£å°†VFMæå–çš„é«˜çº§è¯­ä¹‰ç‰¹å¾æ³¨å…¥åˆ°å­¦ç”Ÿæ¨¡å‹çš„æ·±å±‚ç‰¹å¾ä¸­ã€‚GAMç­–ç•¥æ ¹æ®æ¢¯åº¦ä¿¡æ¯åŠ¨æ€è°ƒæ•´è¯­ä¹‰è¿ç§»çš„å¼ºåº¦ï¼Œä»¥é¿å…è´Ÿè¿ç§»ã€‚æœ€ç»ˆï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ä¸‹è¿›è¡Œè®­ç»ƒï¼Œä»è€Œæå‡æ£€æµ‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºDSIæ¨¡å—å’ŒGAMç­–ç•¥ã€‚DSIæ¨¡å—é€šè¿‡ç‰¹å®šçš„ç½‘ç»œç»“æ„è®¾è®¡ï¼Œä½¿å¾—VFMçš„ç‰¹å¾èƒ½å¤Ÿæœ‰æ•ˆåœ°èå…¥åˆ°å­¦ç”Ÿæ¨¡å‹çš„ç‰¹å¾ä¸­ï¼Œå¼¥è¡¥äº†å­¦ç”Ÿæ¨¡å‹ç‰¹å¾è¡¨è¾¾èƒ½åŠ›çš„ä¸è¶³ã€‚GAMç­–ç•¥åˆ™é€šè¿‡æ¢¯åº¦ä¿¡æ¯æ¥åŠ¨æ€è°ƒæ•´çŸ¥è¯†è¿ç§»çš„å¼ºåº¦ï¼Œé¿å…äº†VFMçš„ç‰¹å¾å¯¹å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒäº§ç”Ÿå¹²æ‰°ï¼Œä»è€Œå®ç°äº†æ›´ç¨³å®šå’Œæœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚

**å…³é”®è®¾è®¡**ï¼šDSIæ¨¡å—çš„å…·ä½“ç»“æ„æœªçŸ¥ï¼Œä½†å…¶ç›®æ ‡æ˜¯å°†VFMçš„é«˜çº§è¯­ä¹‰ç‰¹å¾æ³¨å…¥åˆ°å­¦ç”Ÿæ¨¡å‹çš„æ·±å±‚ç‰¹å¾ä¸­ã€‚GAMç­–ç•¥çš„å…³é”®åœ¨äºå¦‚ä½•è®¡ç®—æ¢¯åº¦èŒƒæ•°æ¯”ç‡ï¼Œå¹¶å°†å…¶ç”¨äºè°ƒæ•´è¯­ä¹‰è¿ç§»çš„å¼ºåº¦ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡ä¹ŸæœªçŸ¥ï¼Œä½†éœ€è¦ä¿è¯å­¦ç”Ÿæ¨¡å‹åœ¨å­¦ä¹ VFMçŸ¥è¯†çš„åŒæ—¶ï¼Œä¿æŒè‡ªèº«çš„æ£€æµ‹èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RT-DETRv4æ¨¡å‹åœ¨COCOæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨ä¸åŒçš„é€Ÿåº¦ä¸‹å‡è¾¾åˆ°äº†SOTAæ°´å¹³ã€‚ä¾‹å¦‚ï¼Œåœ¨273 FPSçš„é€Ÿåº¦ä¸‹ï¼ŒAPè¾¾åˆ°äº†49.7ï¼›åœ¨78 FPSçš„é€Ÿåº¦ä¸‹ï¼ŒAPè¾¾åˆ°äº†57.0ã€‚ç›¸è¾ƒäºä¹‹å‰çš„RT-DETRæ¨¡å‹ï¼ŒRT-DETRv4åœ¨ç²¾åº¦å’Œé€Ÿåº¦ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å®æ—¶ç›®æ ‡æ£€æµ‹çš„åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€æœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†ï¼Œå¯ä»¥æ˜¾è‘—æå‡è½»é‡çº§æ£€æµ‹å™¨çš„æ€§èƒ½ï¼Œä½¿å…¶åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šä¹Ÿèƒ½å®ç°é«˜ç²¾åº¦çš„ç›®æ ‡æ£€æµ‹ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ¨å¹¿åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å›¾åƒåˆ†å‰²ã€ç›®æ ‡è·Ÿè¸ªç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Real-time object detection has achieved substantial progress through meticulously designed architectures and optimization strategies. However, the pursuit of high-speed inference via lightweight network designs often leads to degraded feature representation, which hinders further performance improvements and practical on-device deployment. In this paper, we propose a cost-effective and highly adaptable distillation framework that harnesses the rapidly evolving capabilities of Vision Foundation Models (VFMs) to enhance lightweight object detectors. Given the significant architectural and learning objective disparities between VFMs and resource-constrained detectors, achieving stable and task-aligned semantic transfer is challenging. To address this, on one hand, we introduce a Deep Semantic Injector (DSI) module that facilitates the integration of high-level representations from VFMs into the deep layers of the detector. On the other hand, we devise a Gradient-guided Adaptive Modulation (GAM) strategy, which dynamically adjusts the intensity of semantic transfer based on gradient norm ratios. Without increasing deployment and inference overhead, our approach painlessly delivers striking and consistent performance gains across diverse DETR-based models, underscoring its practical utility for real-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art results on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding speeds of 273/169/124/78 FPS.

