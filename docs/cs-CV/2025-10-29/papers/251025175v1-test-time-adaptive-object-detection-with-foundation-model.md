---
layout: default
title: Test-Time Adaptive Object Detection with Foundation Model
---

# Test-Time Adaptive Object Detection with Foundation Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.25175" target="_blank" class="toolbar-btn">arXiv: 2510.25175v1</a>
    <a href="https://arxiv.org/pdf/2510.25175.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.25175v1" 
            onclick="toggleFavorite(this, '2510.25175v1', 'Test-Time Adaptive Object Detection with Foundation Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-29

**Â§áÊ≥®**: Accepted by NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/gaoyingjay/ttaod_foundation)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÁöÑÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ï‰ª•Ëß£ÂÜ≥Ê∫êÊï∞ÊçÆ‰æùËµñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁõÆÊ†áÊ£ÄÊµã` `Ëá™ÈÄÇÂ∫îÂ≠¶‰π†` `Â§öÊ®°ÊÄÅËûçÂêà` `Âü∫Á°ÄÊ®°Âûã` `‰º™Ê†áÁ≠æ` `Âú®Á∫øÈ¢ÜÂüüÈÄÇÂ∫î` `ËßÜËßâÊèêÁ§∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïËøá‰∫é‰æùËµñÊ∫êÊï∞ÊçÆÔºå‰∏îÂÅáËÆæÊ∫êÂüü‰∏éÁõÆÊ†áÂüüÂÖ±‰∫´Áõ∏ÂêåÁöÑÁ±ªÂà´Á©∫Èó¥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂü∫Á°ÄÊ®°ÂûãÁöÑÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïÔºåÂÆåÂÖ®Ê∂àÈô§‰∫ÜÂØπÊ∫êÊï∞ÊçÆÁöÑ‰æùËµñÔºåÈááÁî®Â§öÊ®°ÊÄÅÊèêÁ§∫Ë∞É‰ºò„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑË∑®ÂüüÂíåË∑®Á±ªÂà´ÁõÆÊ†áÊï∞ÊçÆ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµãÂõ†ÂÖ∂Âú®Âú®Á∫øÈ¢ÜÂüüÈÄÇÂ∫î‰∏≠ÁöÑÁã¨Áâπ‰ºòÂäøËÄåÂèóÂà∞Ë∂äÊù•Ë∂äÂ§öÁöÑÂÖ≥Ê≥®„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ï‰∏•Èáç‰æùËµñÊ∫êÊï∞ÊçÆÁöÑÁªüËÆ°ÁâπÂæÅÔºåÂπ∂ÂÅáËÆæÊ∫êÂüüÂíåÁõÆÊ†áÂüüÂÖ±‰∫´Áõ∏ÂêåÁöÑÁ±ªÂà´Á©∫Èó¥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÈ¶ñ‰∏™Êó†ÈúÄÊ∫êÊï∞ÊçÆÁöÑÂü∫Á°ÄÊ®°ÂûãÈ©±Âä®ÁöÑÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÈó≠ÈõÜÁöÑÈôêÂà∂„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Â§öÊ®°ÊÄÅÊèêÁ§∫Âü∫Á°ÄÁöÑÂùáÂÄºÊïôÂ∏àÊ°ÜÊû∂ÔºåÈÄöËøáÊñáÊú¨ÂíåËßÜËßâÊèêÁ§∫Ë∞É‰ºò‰ª•È´òÊïàÂú∞ÈÄÇÂ∫îÊµãËØïÊï∞ÊçÆÁöÑËØ≠Ë®ÄÂíåËßÜËßâË°®Á§∫Á©∫Èó¥„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈíàÂØπËßÜËßâÊèêÁ§∫ÁöÑÊµãËØïÊó∂ÁÉ≠ÂêØÂä®Á≠ñÁï•Ôºå‰ª•ÊúâÊïà‰øùÁïôËßÜËßâÂàÜÊîØÁöÑË°®Á§∫ËÉΩÂäõ„ÄÇÈÄöËøáÂú®‰∫§ÂèâËÖêËöÄÂíå‰∫§ÂèâÊï∞ÊçÆÈõÜÂü∫ÂáÜ‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂπ∂ËÉΩÂ§üÈÄÇÂ∫î‰ªªÊÑèË∑®ÂüüÂíåË∑®Á±ªÂà´ÁöÑÁõÆÊ†áÊï∞ÊçÆ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïÂØπÊ∫êÊï∞ÊçÆÁöÑ‰æùËµñÂèäÂÖ∂Âú®Ë∑®ÂüüÂíåË∑®Á±ªÂà´ÈÄÇÂ∫î‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÂÅáËÆæÊ∫êÂüüÂíåÁõÆÊ†áÂüüÂÖ±‰∫´Áõ∏ÂêåÁöÑÁ±ªÂà´Á©∫Èó¥ÔºåÂØºËá¥Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÈúÄÊ∫êÊï∞ÊçÆÁöÑÂü∫Á°ÄÊ®°ÂûãÈ©±Âä®ÁöÑÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïÔºåÈÄöËøáËÆæËÆ°Â§öÊ®°ÊÄÅÊèêÁ§∫Âü∫Á°ÄÁöÑÂùáÂÄºÊïôÂ∏àÊ°ÜÊû∂ÔºåÁªìÂêàÊñáÊú¨ÂíåËßÜËßâÊèêÁ§∫Ë∞É‰ºòÔºå‰ª•È´òÊïàÈÄÇÂ∫îÊµãËØïÊï∞ÊçÆÁöÑË°®Á§∫Á©∫Èó¥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Â§ö‰∏™Ê®°ÂùóÔºöÈ¶ñÂÖàÊòØÂ§öÊ®°ÊÄÅÊèêÁ§∫Ë∞É‰ºòÊ®°ÂùóÔºåË¥üË¥£ÂØπËØ≠Ë®ÄÂíåËßÜËßâË°®Á§∫ËøõË°åÈÄÇÂ∫îÔºõÂÖ∂Ê¨°ÊòØÊµãËØïÊó∂ÁÉ≠ÂêØÂä®Á≠ñÁï•ÔºåÊó®Âú®‰øùÁïôËßÜËßâÂàÜÊîØÁöÑË°®Á§∫ËÉΩÂäõÔºõÊúÄÂêéÊòØÂÆû‰æãÂä®ÊÄÅËÆ∞ÂøÜÊ®°ÂùóÔºåÁî®‰∫éÂ≠òÂÇ®È´òË¥®ÈáèÁöÑ‰º™Ê†áÁ≠æÂπ∂ËøõË°åÂ¢ûÂº∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨Á†îÁ©∂ÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÊó†ÈúÄÊ∫êÊï∞ÊçÆÁöÑÊµãËØïÊó∂Ëá™ÈÄÇÂ∫îÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïÔºåÁ™ÅÁ†¥‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÈó≠ÈõÜÈôêÂà∂ÔºåÂπ∂ÈÄöËøáÂ§öÊ®°ÊÄÅÊèêÁ§∫Ë∞É‰ºòÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑÂèÇÊï∞ÈÄÇÂ∫î„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊäÄÊúØÁªÜËäÇ‰∏äÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÊèêÁ§∫Ë∞É‰ºòËøáÁ®ãÔºåÂπ∂ÈááÁî®‰∫ÜÂÆû‰æãÂä®ÊÄÅËÆ∞ÂøÜÊ®°ÂùóÊù•Â≠òÂÇ®ÂíåÂà©Áî®È´òË¥®Èáè‰º™Ê†áÁ≠æÔºåÁ°Æ‰øùÊØè‰∏™ÊµãËØïÊâπÊ¨°ÁöÑ‰º™Ê†áÁ≠æË¥®Èáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÊñπÊ≥ïÂú®‰∫§ÂèâËÖêËöÄÂíå‰∫§ÂèâÊï∞ÊçÆÈõÜÂü∫ÂáÜ‰∏äÂùáÊòæËëó‰ºò‰∫éÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞10%‰ª•‰∏äÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Ë∑®ÂüüÂíåË∑®Á±ªÂà´ÈÄÇÂ∫î‰∏≠ÁöÑÂº∫Â§ßËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂø´ÈÄüÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÁöÑËá™Âä®È©æÈ©∂„ÄÅÁõëÊéßÁ≥ªÁªüÂíåÊú∫Âô®‰∫∫ËßÜËßâÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊ∂àÈô§ÂØπÊ∫êÊï∞ÊçÆÁöÑ‰æùËµñÔºåËÉΩÂ§üÂú®Â§öÂèòÁöÑÂÆûÈôÖÂú∫ÊôØ‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑÁõÆÊ†áÊ£ÄÊµãÔºåÊèêÂçáÁ≥ªÁªüÁöÑÁÅµÊ¥ªÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In recent years, test-time adaptive object detection has attracted increasing attention due to its unique advantages in online domain adaptation, which aligns more closely with real-world application scenarios. However, existing approaches heavily rely on source-derived statistical characteristics while making the strong assumption that the source and target domains share an identical category space. In this paper, we propose the first foundation model-powered test-time adaptive object detection method that eliminates the need for source data entirely and overcomes traditional closed-set limitations. Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for vision-language detector-driven test-time adaptation, which incorporates text and visual prompt tuning to adapt both language and vision representation spaces on the test data in a parameter-efficient manner. Correspondingly, we propose a Test-time Warm-start strategy tailored for the visual prompts to effectively preserve the representation capability of the vision branch. Furthermore, to guarantee high-quality pseudo-labels in every test batch, we maintain an Instance Dynamic Memory (IDM) module that stores high-quality pseudo-labels from previous test samples, and propose two novel strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's high-quality instances for enhancing original predictions and hallucinating images without available pseudo-labels, respectively. Extensive experiments on cross-corruption and cross-dataset benchmarks demonstrate that our method consistently outperforms previous state-of-the-art methods, and can adapt to arbitrary cross-domain and cross-category target data. Code is available at https://github.com/gaoyingjay/ttaod_foundation.

