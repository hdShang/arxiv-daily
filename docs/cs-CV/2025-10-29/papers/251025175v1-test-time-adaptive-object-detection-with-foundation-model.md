---
layout: default
title: Test-Time Adaptive Object Detection with Foundation Model
---

# Test-Time Adaptive Object Detection with Foundation Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.25175" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.25175v1</a>
  <a href="https://arxiv.org/pdf/2510.25175.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.25175v1" onclick="toggleFavorite(this, '2510.25175v1', 'Test-Time Adaptive Object Detection with Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-29

**å¤‡æ³¨**: Accepted by NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/gaoyingjay/ttaod_foundation)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåŸºç¡€æ¨¡å‹çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ä»¥è§£å†³æºæ•°æ®ä¾èµ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›®æ ‡æ£€æµ‹` `è‡ªé€‚åº”å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `åŸºç¡€æ¨¡å‹` `ä¼ªæ ‡ç­¾` `åœ¨çº¿é¢†åŸŸé€‚åº”` `è§†è§‰æç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•è¿‡äºä¾èµ–æºæ•°æ®ï¼Œä¸”å‡è®¾æºåŸŸä¸ç›®æ ‡åŸŸå…±äº«ç›¸åŒçš„ç±»åˆ«ç©ºé—´ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸºç¡€æ¨¡å‹çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹æºæ•°æ®çš„ä¾èµ–ï¼Œé‡‡ç”¨å¤šæ¨¡æ€æç¤ºè°ƒä¼˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„è·¨åŸŸå’Œè·¨ç±»åˆ«ç›®æ ‡æ•°æ®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹å› å…¶åœ¨åœ¨çº¿é¢†åŸŸé€‚åº”ä¸­çš„ç‹¬ç‰¹ä¼˜åŠ¿è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–æºæ•°æ®çš„ç»Ÿè®¡ç‰¹å¾ï¼Œå¹¶å‡è®¾æºåŸŸå’Œç›®æ ‡åŸŸå…±äº«ç›¸åŒçš„ç±»åˆ«ç©ºé—´ã€‚æœ¬æ–‡æå‡ºäº†é¦–ä¸ªæ— éœ€æºæ•°æ®çš„åŸºç¡€æ¨¡å‹é©±åŠ¨çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå…‹æœäº†ä¼ ç»Ÿé—­é›†çš„é™åˆ¶ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€æç¤ºåŸºç¡€çš„å‡å€¼æ•™å¸ˆæ¡†æ¶ï¼Œé€šè¿‡æ–‡æœ¬å’Œè§†è§‰æç¤ºè°ƒä¼˜ä»¥é«˜æ•ˆåœ°é€‚åº”æµ‹è¯•æ•°æ®çš„è¯­è¨€å’Œè§†è§‰è¡¨ç¤ºç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹è§†è§‰æç¤ºçš„æµ‹è¯•æ—¶çƒ­å¯åŠ¨ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆä¿ç•™è§†è§‰åˆ†æ”¯çš„è¡¨ç¤ºèƒ½åŠ›ã€‚é€šè¿‡åœ¨äº¤å‰è…èš€å’Œäº¤å‰æ•°æ®é›†åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”ä»»æ„è·¨åŸŸå’Œè·¨ç±»åˆ«çš„ç›®æ ‡æ•°æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•å¯¹æºæ•°æ®çš„ä¾èµ–åŠå…¶åœ¨è·¨åŸŸå’Œè·¨ç±»åˆ«é€‚åº”ä¸­çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å‡è®¾æºåŸŸå’Œç›®æ ‡åŸŸå…±äº«ç›¸åŒçš„ç±»åˆ«ç©ºé—´ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€æºæ•°æ®çš„åŸºç¡€æ¨¡å‹é©±åŠ¨çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡å¤šæ¨¡æ€æç¤ºåŸºç¡€çš„å‡å€¼æ•™å¸ˆæ¡†æ¶ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰æç¤ºè°ƒä¼˜ï¼Œä»¥é«˜æ•ˆé€‚åº”æµ‹è¯•æ•°æ®çš„è¡¨ç¤ºç©ºé—´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆæ˜¯å¤šæ¨¡æ€æç¤ºè°ƒä¼˜æ¨¡å—ï¼Œè´Ÿè´£å¯¹è¯­è¨€å’Œè§†è§‰è¡¨ç¤ºè¿›è¡Œé€‚åº”ï¼›å…¶æ¬¡æ˜¯æµ‹è¯•æ—¶çƒ­å¯åŠ¨ç­–ç•¥ï¼Œæ—¨åœ¨ä¿ç•™è§†è§‰åˆ†æ”¯çš„è¡¨ç¤ºèƒ½åŠ›ï¼›æœ€åæ˜¯å®ä¾‹åŠ¨æ€è®°å¿†æ¨¡å—ï¼Œç”¨äºå­˜å‚¨é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾å¹¶è¿›è¡Œå¢å¼ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†æ— éœ€æºæ•°æ®çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„é—­é›†é™åˆ¶ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æç¤ºè°ƒä¼˜å®ç°äº†é«˜æ•ˆçš„å‚æ•°é€‚åº”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æç¤ºè°ƒä¼˜è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨äº†å®ä¾‹åŠ¨æ€è®°å¿†æ¨¡å—æ¥å­˜å‚¨å’Œåˆ©ç”¨é«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œç¡®ä¿æ¯ä¸ªæµ‹è¯•æ‰¹æ¬¡çš„ä¼ªæ ‡ç­¾è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨äº¤å‰è…èš€å’Œäº¤å‰æ•°æ®é›†åŸºå‡†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨è·¨åŸŸå’Œè·¨ç±»åˆ«é€‚åº”ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿé€‚åº”æ–°ç¯å¢ƒçš„è‡ªåŠ¨é©¾é©¶ã€ç›‘æ§ç³»ç»Ÿå’Œæœºå™¨äººè§†è§‰ç­‰é¢†åŸŸã€‚é€šè¿‡æ¶ˆé™¤å¯¹æºæ•°æ®çš„ä¾èµ–ï¼Œèƒ½å¤Ÿåœ¨å¤šå˜çš„å®é™…åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„ç›®æ ‡æ£€æµ‹ï¼Œæå‡ç³»ç»Ÿçš„çµæ´»æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, test-time adaptive object detection has attracted increasing attention due to its unique advantages in online domain adaptation, which aligns more closely with real-world application scenarios. However, existing approaches heavily rely on source-derived statistical characteristics while making the strong assumption that the source and target domains share an identical category space. In this paper, we propose the first foundation model-powered test-time adaptive object detection method that eliminates the need for source data entirely and overcomes traditional closed-set limitations. Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for vision-language detector-driven test-time adaptation, which incorporates text and visual prompt tuning to adapt both language and vision representation spaces on the test data in a parameter-efficient manner. Correspondingly, we propose a Test-time Warm-start strategy tailored for the visual prompts to effectively preserve the representation capability of the vision branch. Furthermore, to guarantee high-quality pseudo-labels in every test batch, we maintain an Instance Dynamic Memory (IDM) module that stores high-quality pseudo-labels from previous test samples, and propose two novel strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's high-quality instances for enhancing original predictions and hallucinating images without available pseudo-labels, respectively. Extensive experiments on cross-corruption and cross-dataset benchmarks demonstrate that our method consistently outperforms previous state-of-the-art methods, and can adapt to arbitrary cross-domain and cross-category target data. Code is available at https://github.com/gaoyingjay/ttaod_foundation.

