---
layout: default
title: Informative Sample Selection Model for Skeleton-based Action Recognition with Limited Training Samples
---

# Informative Sample Selection Model for Skeleton-based Action Recognition with Limited Training Samples

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.25345" target="_blank" class="toolbar-btn">arXiv: 2510.25345v1</a>
    <a href="https://arxiv.org/pdf/2510.25345.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.25345v1" 
            onclick="toggleFavorite(this, '2510.25345v1', 'Informative Sample Selection Model for Skeleton-based Action Recognition with Limited Training Samples')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhigang Tu, Zhengbo Zhang, Jia Gong, Junsong Yuan, Bo Du

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-29

**Â§áÊ≥®**: Accepted by IEEE Transactions on Image Processing (TIP), 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éMDPÁöÑÈ™®È™ºÂä®‰ΩúËØÜÂà´‰ø°ÊÅØÊ†∑Êú¨ÈÄâÊã©Ê®°ÂûãÔºåÊèêÂçáÊúâÈôêÊ†∑Êú¨‰∏ãÁöÑËØÜÂà´Á≤æÂ∫¶„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)**

**ÂÖ≥ÈîÆËØç**: `È™®È™ºÂä®‰ΩúËØÜÂà´` `ÂçäÁõëÁù£Â≠¶‰π†` `‰∏ªÂä®Â≠¶‰π†` `È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã` `Âº∫ÂåñÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ‰∏ªÂä®Â≠¶‰π†ÊñπÊ≥ïÈÄâÊã©‰ª£Ë°®ÊÄßÊ†∑Êú¨Ôºå‰ΩÜÂøΩÁï•‰∫ÜÊ®°ÂûãÂ∑≤Â≠¶‰π†ÁöÑÁü•ËØÜÔºåÂØºËá¥‰ø°ÊÅØÂÜó‰Ωô„ÄÇ
2. ËÆ∫ÊñáÂ∞ÜÊ†∑Êú¨ÈÄâÊã©Âª∫Ê®°‰∏∫MDPÔºåËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìÈÄâÊã©ÊúÄÂÖ∑‰ø°ÊÅØÈáèÁöÑÊ†∑Êú¨ÔºåÊèêÂçáÊ®°ÂûãÂ≠¶‰π†ÊïàÁéá„ÄÇ
3. ÈÄöËøáÂèåÊõ≤Á©∫Èó¥ÊäïÂΩ±Â¢ûÂº∫Áä∂ÊÄÅË°®Á§∫ÔºåÂπ∂ÂºïÂÖ•ÂÖÉË∞É‰ºòÂä†ÈÄüÊ®°ÂûãÈÉ®ÁΩ≤ÔºåÂÆûÈ™åÈ™åËØÅ‰∫ÜÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÈíàÂØπÊúâÈôêËÆ≠ÁªÉÊ†∑Êú¨‰∏ãÁöÑÈ™®È™ºÂä®‰ΩúËØÜÂà´ÈóÆÈ¢òÔºåÂç≥ÂçäÁõëÁù£3DÂä®‰ΩúËØÜÂà´ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ø°ÊÅØÊ†∑Êú¨ÈÄâÊã©Ê®°Âûã„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®‰∏ªÂä®Â≠¶‰π†Á≠ñÁï•ÔºåÈÄöËøáÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®Ê°ÜÊû∂Â∞ÜÈ™®È™ºÂ∫èÂàóÂµåÂÖ•Âà∞ÊΩúÂú®Á©∫Èó¥ÔºåÂπ∂ÁªìÂêàËÅöÁ±ª‰ø°ÊÅØÂíåÂü∫‰∫émarginÁöÑÈÄâÊã©Á≠ñÁï•Êù•ÈÄâÊã©ÊúÄÂÖ∑‰ø°ÊÅØÈáèÁöÑÊú™Ê†áÊ≥®Ê†∑Êú¨„ÄÇÁÑ∂ËÄåÔºåÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑÊ†∑Êú¨‰∏ç‰∏ÄÂÆöÊúÄÂÖ∑‰ø°ÊÅØÈáè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÂ∞ÜÂçäÁõëÁù£3DÂä®‰ΩúËØÜÂà´ÈóÆÈ¢òÈáçÊñ∞Âª∫Ê®°‰∏∫‰∏Ä‰∏™È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàMDPÔºâÔºåÂπ∂ËÆ≠ÁªÉ‰∏Ä‰∏™‰ø°ÊÅØÊ†∑Êú¨ÈÄâÊã©Ê®°ÂûãÊù•ÊåáÂØºÊ†∑Êú¨ÈÄâÊã©„ÄÇ‰∏∫‰∫ÜÂ¢ûÂº∫Áä∂ÊÄÅ-Âä®‰ΩúÂØπ‰∏≠Âõ†Á¥†ÁöÑË°®ÂæÅËÉΩÂäõÔºåÊú¨ÊñáÂ∞ÜÂÖ∂‰ªéÊ¨ßÂá†ÈáåÂæóÁ©∫Èó¥ÊäïÂΩ±Âà∞ÂèåÊõ≤Á©∫Èó¥„ÄÇÊ≠§Â§ñÔºåËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂÖÉË∞É‰ºòÁ≠ñÁï•Êù•Âä†ÈÄüËØ•ÊñπÊ≥ïÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇÂú®‰∏â‰∏™3DÂä®‰ΩúËØÜÂà´Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®Êòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÊúâÈôêËÆ≠ÁªÉÊ†∑Êú¨‰∏ãÔºåÂü∫‰∫éÈ™®È™ºÁöÑ3DÂä®‰ΩúËØÜÂà´ÈóÆÈ¢ò„ÄÇÁé∞Êúâ‰∏ªÂä®Â≠¶‰π†ÊñπÊ≥ïÂú®ÈÄâÊã©Áî®‰∫éÊ†áÊ≥®ÁöÑÊ†∑Êú¨Êó∂ÔºåÂæÄÂæÄ‰æßÈáç‰∫éÈÄâÊã©ÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑÊ†∑Êú¨ÔºåËÄåÂøΩÁï•‰∫ÜÊ®°ÂûãÂ∑≤ÁªèÂ≠¶‰π†Âà∞ÁöÑÁü•ËØÜ„ÄÇËøôÂØºËá¥ÈÄâÊã©ÁöÑÊ†∑Êú¨ÂèØËÉΩÂåÖÂê´ÂÜó‰Ωô‰ø°ÊÅØÔºåÈôç‰Ωé‰∫ÜÊ®°ÂûãËÆ≠ÁªÉÁöÑÊïàÁéáÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê†áÊ≥®ÊàêÊú¨È´òÊòÇÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ¶Ç‰ΩïÈ´òÊïàÂú∞ÈÄâÊã©‰ø°ÊÅØÈáèÂ§ßÁöÑÊ†∑Êú¨Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂçäÁõëÁù£3DÂä®‰ΩúËØÜÂà´‰∏≠ÁöÑ‰∏ªÂä®Â≠¶‰π†ËøáÁ®ãÂª∫Ê®°‰∏∫‰∏Ä‰∏™È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàMDPÔºâ„ÄÇÈÄöËøáËÆ≠ÁªÉ‰∏Ä‰∏™Êô∫ËÉΩ‰ΩìÔºàAgentÔºâÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊ†πÊçÆÂΩìÂâçÊ®°ÂûãÁöÑÁä∂ÊÄÅÔºåÈÄâÊã©ÊúÄÂÖ∑‰ø°ÊÅØÈáèÁöÑÊ†∑Êú¨ËøõË°åÊ†áÊ≥®Ôºå‰ªéËÄåÊúÄÂ§ßÂåñÊ®°ÂûãÊÄßËÉΩÁöÑÊèêÂçá„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÂä®ÊÄÅÂú∞Ë∞ÉÊï¥Ê†∑Êú¨ÈÄâÊã©Á≠ñÁï•ÔºåÈÅøÂÖçÈÄâÊã©ÂÜó‰Ωô‰ø°ÊÅØÔºåÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÁöÑÊäÄÊúØÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºöÁî®‰∫éÊèêÂèñÈ™®È™ºÂ∫èÂàóÁöÑÁâπÂæÅË°®Á§∫„ÄÇ2) MDPÂª∫Ê®°Ê®°ÂùóÔºöÂ∞ÜÊ†∑Êú¨ÈÄâÊã©ËøáÁ®ãÂª∫Ê®°‰∏∫MDPÔºåÂÆö‰πâÁä∂ÊÄÅ„ÄÅÂä®‰ΩúÂíåÂ•ñÂä±ÂáΩÊï∞„ÄÇÁä∂ÊÄÅË°®Á§∫ÂΩìÂâçÊ®°ÂûãÁöÑÁä∂ÊÄÅÂíåÊú™Ê†áÊ≥®Ê†∑Êú¨ÁöÑ‰ø°ÊÅØÔºåÂä®‰ΩúË°®Á§∫ÈÄâÊã©Âì™‰∏™Ê†∑Êú¨ËøõË°åÊ†áÊ≥®ÔºåÂ•ñÂä±ÂáΩÊï∞ÂèçÊò†‰∫ÜÈÄâÊã©ËØ•Ê†∑Êú¨ÂêéÊ®°ÂûãÊÄßËÉΩÁöÑÊèêÂçá„ÄÇ3) Êô∫ËÉΩ‰ΩìËÆ≠ÁªÉÊ®°ÂùóÔºö‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂ≠¶‰π†Âà∞ÊúÄ‰ºòÁöÑÊ†∑Êú¨ÈÄâÊã©Á≠ñÁï•„ÄÇ4) ÂèåÊõ≤Á©∫Èó¥ÊäïÂΩ±Ê®°ÂùóÔºöÂ∞ÜÁä∂ÊÄÅÂíåÂä®‰Ωú‰ªéÊ¨ßÂá†ÈáåÂæóÁ©∫Èó¥ÊäïÂΩ±Âà∞ÂèåÊõ≤Á©∫Èó¥Ôºå‰ª•Â¢ûÂº∫ÂÖ∂Ë°®ÂæÅËÉΩÂäõ„ÄÇ5) ÂÖÉË∞É‰ºòÊ®°ÂùóÔºöÁî®‰∫éÂä†ÈÄüÊ®°ÂûãÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞Ü‰∏ªÂä®Â≠¶‰π†ËøáÁ®ãÂª∫Ê®°‰∏∫MDPÔºåÂπ∂‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìËøõË°åÊ†∑Êú¨ÈÄâÊã©„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∏ªÂä®Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂä®ÊÄÅÂú∞Ë∞ÉÊï¥Ê†∑Êú¨ÈÄâÊã©Á≠ñÁï•ÔºåÈÅøÂÖçÈÄâÊã©ÂÜó‰Ωô‰ø°ÊÅØÔºåÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®ÂèåÊõ≤Á©∫Èó¥ÊäïÂΩ±Â¢ûÂº∫Áä∂ÊÄÅÂíåÂä®‰ΩúÁöÑË°®ÂæÅËÉΩÂäõ‰πüÊòØ‰∏Ä‰∏™ÂàõÊñ∞ÁÇπ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®MDPÂª∫Ê®°‰∏≠ÔºåÁä∂ÊÄÅÁ©∫Èó¥ÁöÑËÆæËÆ°ÈúÄË¶ÅÂÖÖÂàÜËÄÉËôëÊ®°ÂûãÁöÑÁä∂ÊÄÅÂíåÊú™Ê†áÊ≥®Ê†∑Êú¨ÁöÑ‰ø°ÊÅØ„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÈúÄË¶ÅËÉΩÂ§üÂáÜÁ°ÆÂú∞ÂèçÊò†ÈÄâÊã©Ê†∑Êú¨ÂêéÊ®°ÂûãÊÄßËÉΩÁöÑÊèêÂçá„ÄÇÂú®Êô∫ËÉΩ‰ΩìËÆ≠ÁªÉ‰∏≠ÔºåÂèØ‰ª•ÈÄâÊã©ÂêàÈÄÇÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂ¶ÇQ-learningÊàñPolicy GradientÁ≠â„ÄÇÂèåÊõ≤Á©∫Èó¥ÊäïÂΩ±ÂèØ‰ª•‰ΩøÁî®Poincar√© ballÊ®°ÂûãÊàñHyperboloidÊ®°Âûã„ÄÇÂÖÉË∞É‰ºòÂèØ‰ª•‰ΩøÁî®Model-Agnostic Meta-Learning (MAML)Á≠âÊñπÊ≥ï„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú®‰∏â‰∏™3DÂä®‰ΩúËØÜÂà´Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéËØ•ÊñπÊ≥ïËÉΩÂ§üÊòæËëóÊèêÈ´òÊ®°ÂûãÂú®ÊúâÈôêÊ†áÊ≥®Êï∞ÊçÆ‰∏ãÁöÑËØÜÂà´Á≤æÂ∫¶„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰∏éÁé∞ÊúâÁöÑ‰∏ªÂä®Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®Áõ∏ÂêåÊ†áÊ≥®Êï∞ÊçÆÈáè‰∏ãÔºåËÉΩÂ§üÂèñÂæóÊõ¥È´òÁöÑËØÜÂà´ÂáÜÁ°ÆÁéáÔºåÂπ∂‰∏îËÉΩÂ§üÊõ¥Âø´Âú∞ËææÂà∞Áõ∏ÂêåÁöÑÊÄßËÉΩÊ∞¥Âπ≥„ÄÇÂÆûÈ™åÁªìÊûúÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËøõË°åÂä®‰ΩúËØÜÂà´ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËßÜÈ¢ëÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅÂ∫∑Â§çËÆ≠ÁªÉÁ≠â„ÄÇÂú®Ëøô‰∫õÂú∫ÊôØ‰∏≠ÔºåÊ†áÊ≥®Êï∞ÊçÆÂæÄÂæÄÊòØÊòÇË¥µÁöÑÔºåÂõ†Ê≠§Âà©Áî®Â∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÂíåÂ§ßÈáèÊú™Ê†áÊ≥®Êï∞ÊçÆËøõË°åËÆ≠ÁªÉÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄº„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÂú®ÊúâÈôêÊ†áÊ≥®Êï∞ÊçÆ‰∏ãÁöÑËØÜÂà´Á≤æÂ∫¶ÔºåÈôç‰ΩéÊ†áÊ≥®ÊàêÊú¨ÔºåÂä†ÈÄüÊ®°ÂûãÈÉ®ÁΩ≤„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Skeleton-based human action recognition aims to classify human skeletal sequences, which are spatiotemporal representations of actions, into predefined categories. To reduce the reliance on costly annotations of skeletal sequences while maintaining competitive recognition accuracy, the task of 3D Action Recognition with Limited Training Samples, also known as semi-supervised 3D Action Recognition, has been proposed. In addition, active learning, which aims to proactively select the most informative unlabeled samples for annotation, has been explored in semi-supervised 3D Action Recognition for training sample selection. Specifically, researchers adopt an encoder-decoder framework to embed skeleton sequences into a latent space, where clustering information, combined with a margin-based selection strategy using a multi-head mechanism, is utilized to identify the most informative sequences in the unlabeled set for annotation. However, the most representative skeleton sequences may not necessarily be the most informative for the action recognizer, as the model may have already acquired similar knowledge from previously seen skeleton samples. To solve it, we reformulate Semi-supervised 3D action recognition via active learning from a novel perspective by casting it as a Markov Decision Process (MDP). Built upon the MDP framework and its training paradigm, we train an informative sample selection model to intelligently guide the selection of skeleton sequences for annotation. To enhance the representational capacity of the factors in the state-action pairs within our method, we project them from Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning strategy to accelerate the deployment of our method in real-world scenarios. Extensive experiments on three 3D action recognition benchmarks demonstrate the effectiveness of our method.

