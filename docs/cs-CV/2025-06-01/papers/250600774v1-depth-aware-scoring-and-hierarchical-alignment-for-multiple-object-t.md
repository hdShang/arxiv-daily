---
layout: default
title: Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking
---

# Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00774" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00774v1</a>
  <a href="https://arxiv.org/pdf/2506.00774.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00774v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00774v1', 'Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Milad Khanchi, Maria Amer, Charalambos Poullis

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-06-01

**å¤‡æ³¨**: ICIP 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Milad-Khanchi/DepthMOT)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ·±åº¦æ„ŸçŸ¥è¯„åˆ†ä¸åˆ†å±‚å¯¹é½ä»¥è§£å†³å¤šç›®æ ‡è·Ÿè¸ªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¤šç›®æ ‡è·Ÿè¸ª` `æ·±åº¦æ„ŸçŸ¥` `åˆ†å±‚å¯¹é½` `è§†è§‰ç›¸ä¼¼å¯¹è±¡` `é®æŒ¡å¤„ç†` `æ— ç›‘ç£å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šç›®æ ‡è·Ÿè¸ªæ–¹æ³•åœ¨å¤„ç†é®æŒ¡å’Œè§†è§‰ç›¸ä¼¼å¯¹è±¡æ—¶ï¼Œä¾èµ–äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å¯¼è‡´æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦æ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡é›¶æ ·æœ¬ä¼°è®¡æ·±åº¦å¹¶å°†å…¶ä½œä¸ºç‹¬ç«‹ç‰¹å¾ç”¨äºå¯¹è±¡å…³è”ã€‚
3. è¯¥æ¡†æ¶åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰åŸºäºè¿åŠ¨çš„å¤šç›®æ ‡è·Ÿè¸ªï¼ˆMOTï¼‰æ–¹æ³•åœ¨å¯¹è±¡å…³è”ä¸Šè¿‡äºä¾èµ–äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ï¼Œåœ¨é®æŒ¡æˆ–è§†è§‰ç›¸ä¼¼å¯¹è±¡çš„åœºæ™¯ä¸­æ•ˆæœä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ·±åº¦æ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡é›¶æ ·æœ¬æ–¹æ³•ä¼°è®¡æ·±åº¦ï¼Œå¹¶å°†å…¶ä½œä¸ºç‹¬ç«‹ç‰¹å¾èå…¥å…³è”è¿‡ç¨‹ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚å¯¹é½è¯„åˆ†ï¼Œé€šè¿‡æ•´åˆç²—ç•¥è¾¹ç•Œæ¡†é‡å å’Œç»†ç²’åº¦ï¼ˆåƒç´ çº§ï¼‰å¯¹é½æ¥æ”¹è¿›å…³è”å‡†ç¡®æ€§ï¼Œè€Œæ— éœ€é¢å¤–çš„å¯å­¦ä¹ å‚æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é¦–ä¸ªåœ¨å…³è”æ­¥éª¤ä¸­å°†3Dç‰¹å¾ï¼ˆå•ç›®æ·±åº¦ï¼‰ä½œä¸ºç‹¬ç«‹å†³ç­–çŸ©é˜µçš„MOTæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ— éœ€ä»»ä½•è®­ç»ƒæˆ–å¾®è°ƒã€‚ä»£ç å¯åœ¨https://github.com/Milad-Khanchi/DepthMOTè·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šç›®æ ‡è·Ÿè¸ªæ–¹æ³•åœ¨é®æŒ¡å’Œè§†è§‰ç›¸ä¼¼å¯¹è±¡åœºæ™¯ä¸‹çš„å…³è”å‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ï¼Œå¯¼è‡´åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦æ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡é›¶æ ·æœ¬æ–¹æ³•ä¼°è®¡æ·±åº¦ï¼Œå¹¶å°†å…¶ä½œä¸ºç‹¬ç«‹ç‰¹å¾èå…¥åˆ°å¯¹è±¡å…³è”è¿‡ç¨‹ä¸­ï¼Œä»¥æé«˜è·Ÿè¸ªçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬æ·±åº¦ä¼°è®¡æ¨¡å—å’Œåˆ†å±‚å¯¹é½è¯„åˆ†æ¨¡å—ã€‚æ·±åº¦ä¼°è®¡æ¨¡å—è´Ÿè´£ç”Ÿæˆæ¯ä¸ªå¯¹è±¡çš„æ·±åº¦ä¿¡æ¯ï¼Œè€Œåˆ†å±‚å¯¹é½è¯„åˆ†æ¨¡å—åˆ™ç»“åˆç²—ç•¥çš„è¾¹ç•Œæ¡†é‡å å’Œç»†ç²’åº¦çš„åƒç´ çº§å¯¹é½æ¥ä¼˜åŒ–å¯¹è±¡å…³è”ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡å°†3Dç‰¹å¾ï¼ˆå•ç›®æ·±åº¦ï¼‰ä½œä¸ºç‹¬ç«‹å†³ç­–çŸ©é˜µå¼•å…¥åˆ°å¤šç›®æ ‡è·Ÿè¸ªçš„å…³è”æ­¥éª¤ä¸­ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æé«˜äº†åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è·Ÿè¸ªæ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šæˆ‘ä»¬åœ¨è®¾è®¡ä¸­æ²¡æœ‰å¼•å…¥é¢å¤–çš„å¯å­¦ä¹ å‚æ•°ï¼Œè€Œæ˜¯é€šè¿‡åˆ†å±‚å¯¹é½è¯„åˆ†æ¥ä¼˜åŒ–IoUï¼Œç¡®ä¿äº†æ–¹æ³•çš„ç®€æ´æ€§å’Œé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºIoUçš„è·Ÿè¸ªæ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒæˆ–å¾®è°ƒï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ç›®æ ‡è·Ÿè¸ªèƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å¯èƒ½æ¨åŠ¨æ›´å¤šåŸºäºæ·±åº¦ä¿¡æ¯çš„è·Ÿè¸ªæŠ€æœ¯çš„å‘å±•ï¼Œæå‡å¤šç›®æ ‡è·Ÿè¸ªçš„å®ç”¨æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current motion-based multiple object tracking (MOT) approaches rely heavily on Intersection-over-Union (IoU) for object association. Without using 3D features, they are ineffective in scenarios with occlusions or visually similar objects. To address this, our paper presents a novel depth-aware framework for MOT. We estimate depth using a zero-shot approach and incorporate it as an independent feature in the association process. Additionally, we introduce a Hierarchical Alignment Score that refines IoU by integrating both coarse bounding box overlap and fine-grained (pixel-level) alignment to improve association accuracy without requiring additional learnable parameters. To our knowledge, this is the first MOT framework to incorporate 3D features (monocular depth) as an independent decision matrix in the association step. Our framework achieves state-of-the-art results on challenging benchmarks without any training nor fine-tuning. The code is available at https://github.com/Milad-Khanchi/DepthMOT

