---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-18
---

# cs.CVï¼ˆ2025-05-18ï¼‰

ğŸ“Š å…± **12** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250512307v2-logicocr-do-your-large-multimodal-models-excel-at-logical-reasoning-.html">LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?</a></td>
  <td>æå‡ºLogicOCRä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹åœ¨æ–‡æœ¬ä¸°å¯Œå›¾åƒä¸Šçš„é€»è¾‘æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12307v2" data-paper-url="./papers/250512307v2-logicocr-do-your-large-multimodal-models-excel-at-logical-reasoning-.html" onclick="toggleFavorite(this, '2505.12307v2', 'LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250514714v2-kgalign-joint-semantic-structural-knowledge-encoding-for-multimodal-.html">KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection</a></td>
  <td>æå‡ºKGAlignä»¥è§£å†³å¤šæ¨¡æ€å‡æ–°é—»æ£€æµ‹ä¸­çš„çŸ¥è¯†ç¼–ç é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14714v2" data-paper-url="./papers/250514714v2-kgalign-joint-semantic-structural-knowledge-encoding-for-multimodal-.html" onclick="toggleFavorite(this, '2505.14714v2', 'KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250512254v1-mms-vpr-multimodal-street-level-visual-place-recognition-dataset-and.html">MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark</a></td>
  <td>æå‡ºMMS-VPRæ•°æ®é›†ä»¥è§£å†³è¡—æ™¯è§†è§‰ä½ç½®è¯†åˆ«çš„å¤šæ¨¡æ€ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12254v1" data-paper-url="./papers/250512254v1-mms-vpr-multimodal-street-level-visual-place-recognition-dataset-and.html" onclick="toggleFavorite(this, '2505.12254v1', 'MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250512251v1-smfusion-semantic-preserving-fusion-of-multimodal-medical-images-for.html">SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis</a></td>
  <td>æå‡ºSMFusionä»¥è§£å†³å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆä¸­çš„è¯­ä¹‰ä¿¡æ¯ç¼ºå¤±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12251v1" data-paper-url="./papers/250512251v1-smfusion-semantic-preserving-fusion-of-multimodal-medical-images-for.html" onclick="toggleFavorite(this, '2505.12251v1', 'SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250512363v4-towards-visuospatial-cognition-via-hierarchical-fusion-of-visual-exp.html">Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</a></td>
  <td>æå‡ºViCA2ä»¥è§£å†³è§†è§‰ç©ºé—´è®¤çŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12363v4" data-paper-url="./papers/250512363v4-towards-visuospatial-cognition-via-hierarchical-fusion-of-visual-exp.html" onclick="toggleFavorite(this, '2505.12363v4', 'Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250512312v4-visuospatial-cognitive-assistant.html">Visuospatial Cognitive Assistant</a></td>
  <td>æå‡ºViCAä»¥è§£å†³è§†é¢‘åŸºç¡€ç©ºé—´è®¤çŸ¥æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12312v4" data-paper-url="./papers/250512312v4-visuospatial-cognitive-assistant.html" onclick="toggleFavorite(this, '2505.12312v4', 'Visuospatial Cognitive Assistant')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250512237v1-from-shots-to-stories-llm-assisted-video-editing-with-unified-langua.html">From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations</a></td>
  <td>æå‡ºL-Storyboardä»¥è§£å†³è§†é¢‘ç¼–è¾‘ä¸­çš„è¯­è¨€ä¸è§†è§‰ä¿¡æ¯èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12237v1" data-paper-url="./papers/250512237v1-from-shots-to-stories-llm-assisted-video-editing-with-unified-langua.html" onclick="toggleFavorite(this, '2505.12237v1', 'From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250512253v1-llava-4d-embedding-spatiotemporal-prompt-into-lmms-for-4d-scene-unde.html">LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding</a></td>
  <td>æå‡ºLLaVA-4Dä»¥è§£å†³åŠ¨æ€åœºæ™¯ç†è§£ä¸­çš„æ—¶ç©ºè¡¨ç¤ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12253v1" data-paper-url="./papers/250512253v1-llava-4d-embedding-spatiotemporal-prompt-into-lmms-for-4d-scene-unde.html" onclick="toggleFavorite(this, '2505.12253v1', 'LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250512207v3-can-large-multimodal-models-understand-agricultural-scenes-benchmark.html">Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind</a></td>
  <td>æå‡ºAgroMindä»¥è§£å†³å†œä¸šé¥æ„ŸåŸºå‡†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12207v3" data-paper-url="./papers/250512207v3-can-large-multimodal-models-understand-agricultural-scenes-benchmark.html" onclick="toggleFavorite(this, '2505.12207v3', 'Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250512549v2-vggt-slam-dense-rgb-slam-optimized-on-the-sl4-manifold.html">VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold</a></td>
  <td>æå‡ºVGGT-SLAMä»¥è§£å†³æ— æ ‡å®šå•ç›®ç›¸æœºçš„ç¨ å¯†RGB SLAMé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene reconstruction</span> <span class="paper-tag">VGGT</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12549v2" data-paper-url="./papers/250512549v2-vggt-slam-dense-rgb-slam-optimized-on-the-sl4-manifold.html" onclick="toggleFavorite(this, '2505.12549v2', 'VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250512233v1-preti-patient-aware-retinal-foundation-model-via-metadata-guided-rep.html">PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning</a></td>
  <td>æå‡ºPRETIä»¥è§£å†³è§†ç½‘è†œå›¾åƒåˆ†æä¸­çš„æ•°æ®ä¾èµ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12233v1" data-paper-url="./papers/250512233v1-preti-patient-aware-retinal-foundation-model-via-metadata-guided-rep.html" onclick="toggleFavorite(this, '2505.12233v1', 'PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250512434v4-videorft-incentivizing-video-reasoning-capability-in-mllms-via-reinf.html">VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning</a></td>
  <td>æå‡ºVideoRFTä»¥è§£å†³è§†é¢‘æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12434v4" data-paper-url="./papers/250512434v4-videorft-incentivizing-video-reasoning-capability-in-mllms-via-reinf.html" onclick="toggleFavorite(this, '2505.12434v4', 'VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)