---
layout: default
title: LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?
---

# LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12307" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.12307v2</a>
  <a href="https://arxiv.org/pdf/2505.12307.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12307v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12307v2', 'LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Maoyuan Ye, Haibin He, Qihuang Zhong, Jing Zhang, Juhua Liu, Bo Du

**ÂàÜÁ±ª**: cs.CV, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-18 (Êõ¥Êñ∞: 2025-11-26)

**Â§áÊ≥®**: GitHub: https://github.com/MiliLab/LogicOCR

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/MiliLab/LogicOCR)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LogicOCR‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊ®°ÂûãÂú®ÊñáÊú¨‰∏∞ÂØåÂõæÂÉè‰∏äÁöÑÈÄªËæëÊé®ÁêÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊ®°Âûã` `ÈÄªËæëÊé®ÁêÜ` `ÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´` `ÊñáÊú¨‰∏∞ÂØåÂõæÂÉè` `Âü∫ÂáÜÊµãËØï` `ÁîüÊàêÂõæÂÉè` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÊñáÊú¨Á∫øÁ¥¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§ÑÁêÜÊñáÊú¨‰∏∞ÂØåÂõæÂÉèÊó∂ÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÔºåÂ∞öÊú™ÂÖÖÂàÜÊé¢Á¥¢ÂÖ∂ÊΩúÂäõ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫LogicOCRÂü∫ÂáÜÔºåÈÄöËøáÁîüÊàêÂíåÁúüÂÆûÂõæÂÉèÁöÑÂ§öÊ†∑ÂåñÈóÆÈ¢òÔºåËØÑ‰º∞LMMsÂú®ÈÄªËæëÊé®ÁêÜ‰∏äÁöÑË°®Áé∞„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLMMsÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏ä‰ªçËêΩÂêé‰∫éÊñáÊú¨ËæìÂÖ•ÔºåÊèêÂá∫ÁöÑTextCueÊñπÊ≥ïÊúâÊïàÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÂáÜÁ°ÆÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÁöÑÂèëÂ±ïÊûÅÂ§ßÂú∞ÊèêÂçá‰∫ÜÂÖ∂Êé®ÁêÜÂíåÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´ÔºàOCRÔºâËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨Âú®ÊñáÊú¨‰∏∞ÂØåÂõæÂÉè‰∏äÁöÑÂ§çÊùÇÈÄªËæëÊé®ÁêÜË°®Áé∞‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜLogicOCRÂü∫ÂáÜÔºåÂåÖÂê´2780‰∏™ÈóÆÈ¢òÔºåÂàÜ‰∏∫LogicOCR-GenÂíåLogicOCR-Real‰∏§‰∏™Â≠êÈõÜ„ÄÇÊàë‰ª¨È¶ñÂÖà‰ªé‰∏≠ÂõΩÂõΩÂÆ∂ÂÖ¨Âä°ÂëòËÄÉËØï‰∏≠Êï¥ÁêÜÊñáÊú¨ËØ≠ÊñôÔºåÂπ∂ÂÆöÂà∂Ëá™Âä®ÂåñÊµÅÁ®ãÂºïÂØºGPT-Image-1ÁîüÊàêÂÖ∑ÊúâÂ§öÊ†∑Â∏ÉÂ±ÄÂíåÂ≠ó‰ΩìÁöÑÂõæÂÉèÔºåÁ°Æ‰øù‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄßÂíåËßÜËßâÁúüÂÆûÊÑü„ÄÇÁÑ∂ÂêéÔºåÁîüÊàêÁöÑÂõæÂÉèÁªèËøá‰∫∫Â∑•È™åËØÅ„ÄÇÊàë‰ª¨Âú®Chain-of-ThoughtÔºàCoTÔºâÂíåÁõ¥Êé•ÂõûÁ≠îËÆæÁΩÆ‰∏ãËØÑ‰º∞‰∫Ü‰∏ÄÁ≥ªÂàó‰ª£Ë°®ÊÄßÁöÑLMMsÔºåÂàÜÊûêÁªìÊûúÊè≠Á§∫‰∫ÜÊµãËØïÊó∂Èó¥Áº©Êîæ„ÄÅËæìÂÖ•Ê®°ÊÄÅÂ∑ÆÂºÇÂíåËßÜËßâ-ÊñáÊú¨ÊñπÂêëÊïèÊÑüÊÄßÁ≠âÂÖ≥ÈîÆËßÅËß£„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåLMMsÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜÊñπÈù¢‰ªçËêΩÂêé‰∫éÊñáÊú¨ËæìÂÖ•ÔºåË°®ÊòéÂÆÉ‰ª¨Â∞öÊú™ÂÆåÂÖ®ÂÆûÁé∞ËßÜËßâÈòÖËØª‰∏éÊé®ÁêÜÁöÑÁªìÂêà„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜTextCueÔºå‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåÂ¢ûÂº∫LMMsÂØπËß£ÂÜ≥ÈóÆÈ¢òÊó∂ÈáçË¶ÅÊñáÊú¨Á∫øÁ¥¢ÁöÑÊÑüÁü•„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®CoTËÆæÁΩÆ‰∏ãÁõ∏ËæÉ‰∫éLLaVA-OV-1.5-8BÊèêÂçá‰∫Ü1.8%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÊàë‰ª¨ÁöÑÂü∫ÂáÜÂèØÂú®https://github.com/MiliLab/LogicOCRËé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®ÊñáÊú¨‰∏∞ÂØåÂõæÂÉè‰∏äÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇËßÜËßâ-ÊñáÊú¨‰ø°ÊÅØÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊú™ËÉΩÂÖÖÂàÜÂèëÊå•ÂÖ∂ÊΩúÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫LogicOCRÂü∫ÂáÜÔºåÂåÖÂê´Â§öÊ†∑ÂåñÁöÑÁîüÊàêÂíåÁúüÂÆûÂõæÂÉèÈóÆÈ¢òÔºå‰ª•ÂÖ®Èù¢ËØÑ‰º∞LMMsÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ„ÄÇÂêåÊó∂ÔºåÊèêÂá∫TextCueÊñπÊ≥ïÔºåÂ¢ûÂº∫Ê®°ÂûãÂØπÈáçË¶ÅÊñáÊú¨Á∫øÁ¥¢ÁöÑÊÑüÁü•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÔºöLogicOCR-GenÂíåLogicOCR-Real„ÄÇÂâçËÄÖÈÄöËøáËá™Âä®ÂåñÊµÅÁ®ãÁîüÊàêÂ§öÊ†∑ÂåñÂõæÂÉèÔºåÂêéËÄÖÂàôÂü∫‰∫éÁúüÂÆûÂõæÂÉèËÆæËÆ°Ëá™Áî±ÂΩ¢ÂºèÈóÆÈ¢ò„ÄÇËØÑ‰º∞Èò∂ÊÆµ‰ΩøÁî®Chain-of-ThoughtÂíåÁõ¥Êé•ÂõûÁ≠î‰∏§ÁßçËÆæÁΩÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÊòØÊèêÂá∫‰∫ÜTextCueÊñπÊ≥ïÔºåÈÄöËøáÂà©Áî®LMMsÁöÑÊ≥®ÊÑèÂäõÂõæÂíåÊñáÊú¨ÂàÜÂâ≤ÊäÄÊúØÔºåÂ¢ûÂº∫Ê®°ÂûãÂØπÈáçË¶ÅÊñáÊú¨Âå∫ÂüüÁöÑÂÖ≥Ê≥®„ÄÇËøô‰∏ÄÊñπÊ≥ïÂú®ÈÄªËæëÊé®ÁêÜ‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁîüÊàêÂõæÂÉèÊó∂ÔºåÈááÁî®‰∫ÜÂ§öÊ†∑ÁöÑÂ∏ÉÂ±ÄÂíåÂ≠ó‰ΩìÔºåÁ°Æ‰øùÁîüÊàêÂÜÖÂÆπÁöÑ‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄß„ÄÇTextCueÊñπÊ≥ï‰∏≠ÔºåÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨ÂØπÊ≥®ÊÑèÂäõÂõæÁöÑÂàÜÊûêÂíåÊñáÊú¨Âå∫ÂüüÁöÑË£ÅÂâ™‰∏éÊîæÂ§ßÔºå‰ª•Â¢ûÂº∫ËßÜËßâ‰ø°ÊÅØÁöÑÊúâÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLogicOCRÂü∫ÂáÜ‰∏ãÔºåLMMsÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏éÊñáÊú¨ËæìÂÖ•ÁöÑÂØπÊØî‰∏≠„ÄÇÈÄöËøáÂºïÂÖ•TextCueÊñπÊ≥ïÔºåÊ®°ÂûãÂú®CoTËÆæÁΩÆ‰∏ãÂÆûÁé∞‰∫Ü1.8%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçáÔºåÁõ∏ËæÉ‰∫éÂü∫Á∫øLLaVA-OV-1.5-8BÔºåÂ±ïÁé∞‰∫ÜÊòæËëóÁöÑÊîπËøõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÊñáÊ°£ÂàÜÊûêÂíåÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§çÊùÇËßÜËßâ-ÊñáÊú¨‰ªªÂä°‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõÔºåËÉΩÂ§ü‰∏∫ÂÆûÈôÖÂ∫îÁî®Êèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÁªìÊûúÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑÂèëÂ±ï‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in Large Multimodal Models (LMMs) have revolutionized their reasoning and Optical Character Recognition (OCR) capabilities. However, their complex logical reasoning performance on text-rich images remains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark comprising 2780 questions with two subsets, i.e., LogicOCR-Gen with 1100 multi-choice questions on generated images, and LogicOCR-Real with 1680 meticulously designed free-form questions on real-world images. For constructing LogicOCR-Gen, we first curate a text corpus from the Chinese National Civil Servant Examination, and customize an automatic pipeline to steer GPT-Image-1 to generate images with varied layouts and fonts, ensuring contextual relevance and visual realism. Then, the generated images are manually verified. We evaluate a range of representative LMMs under Chain-of-Thought (CoT) and direct-answer settings. Our multi-dimensional analysis reveals key insights, such as the impact of test-time scaling, input modality differences, and sensitivity to visual-text orientation. Notably, LMMs still lag in multimodal reasoning compared to text-only inputs, indicating that they have not fully bridged visual reading with reasoning. Moreover, we propose TextCue, a training-free method that enhances LMMs' perception of image regions containing important text cues for solving questions. We leverage LMMs' attention maps and an off-the-shelf text segmentation specialist to determine the region, which is then cropped and enlarged to augment the original image. Experiments show its effectiveness, e.g., a 1.8% accuracy gain over LLaVA-OV-1.5-8B under the CoT setting. Our benchmark is available at https://github.com/MiliLab/LogicOCR.

