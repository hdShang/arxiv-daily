---
layout: default
title: Visuospatial Cognitive Assistant
---

# Visuospatial Cognitive Assistant

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12312" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12312v4</a>
  <a href="https://arxiv.org/pdf/2505.12312.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12312v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12312v4', 'Visuospatial Cognitive Assistant')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Feng

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-18 (æ›´æ–°: 2025-09-09)

**å¤‡æ³¨**: 31 pages, 10 figures, 6 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViCAä»¥è§£å†³è§†é¢‘åŸºç¡€ç©ºé—´è®¤çŸ¥æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘åŸºç¡€ç©ºé—´è®¤çŸ¥` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤æ‚æ¨ç†` `æ•°æ®é›†æ„å»º` `æœºå™¨äººå¯¼èˆª` `å¢å¼ºç°å®` `æ™ºèƒ½å®¶å±…` `ç©ºé—´æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†é¢‘åŸºç¡€ç©ºé—´è®¤çŸ¥æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†å’Œ3Då…ƒæ•°æ®æŸ¥è¯¢æ–¹é¢ã€‚
2. æœ¬æ–‡æå‡ºäº†ViCA-322Kæ•°æ®é›†å’ŒViCA-7Bæ¨¡å‹ï¼Œå‰è€…æä¾›äº†ä¸°å¯Œçš„é—®ç­”å¯¹ï¼Œåè€…åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½çªç ´ã€‚
3. ViCA-7Båœ¨VSI-Benchä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»å¯¹è·ç¦»ä¸Šæå‡äº†26.1ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘åŸºç¡€çš„ç©ºé—´è®¤çŸ¥å¯¹äºæœºå™¨äººå’Œå…·èº«äººå·¥æ™ºèƒ½è‡³å…³é‡è¦ï¼Œä½†å¯¹ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æå‡ºäº†æŒ‘æˆ˜ã€‚æœ¬æ–‡çš„ä¸¤é¡¹å…³é”®è´¡çŒ®é¦–å…ˆæ˜¯å¼•å…¥ViCA-322Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«322,003ä¸ªæ¥è‡ªçœŸå®å®¤å†…è§†é¢‘çš„é—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œæä¾›äº†å¯¹3Då…ƒæ•°æ®é©±åŠ¨æŸ¥è¯¢å’Œè§†é¢‘åŸºç¡€å¤æ‚æ¨ç†çš„ç›‘ç£ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ViCA-7Bï¼Œåœ¨ViCA-322Kä¸Šè¿›è¡Œå¾®è°ƒï¼Œåœ¨æ‰€æœ‰å…«ä¸ªVSI-Benchä»»åŠ¡ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬æ›´å¤§çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œåœ¨ç»å¯¹è·ç¦»ä¸Šæå‡äº†26.1ï¼‰ã€‚ä¸ºäº†æé«˜å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ViCA-Thinking-2.68Kï¼Œä¸€ä¸ªåŒ…å«æ˜ç¡®æ¨ç†é“¾çš„æ•°æ®é›†ï¼Œå¹¶å¾®è°ƒViCA-7Bä»¥åˆ›å»ºViCA-7B-Thinkingï¼Œä¸€ä¸ªèƒ½å¤Ÿé˜è¿°å…¶ç©ºé—´æ¨ç†çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†é’ˆå¯¹æ€§æ•°æ®çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†æ”¹è¿›æ—¶é—´-ç©ºé—´å»ºæ¨¡çš„è·¯å¾„ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰èµ„æºä»¥ä¿ƒè¿›ç¨³å¥çš„è§†è§‰ç©ºé—´æ™ºèƒ½ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘åŸºç¡€ç©ºé—´è®¤çŸ¥å’Œå¤æ‚æ¨ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†3Då…ƒæ•°æ®æŸ¥è¯¢æ—¶çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºViCA-322Kæ•°æ®é›†ï¼Œæä¾›ä¸°å¯Œçš„é—®ç­”å¯¹ï¼Œç»“åˆViCA-7Bæ¨¡å‹çš„å¾®è°ƒï¼Œæå‡æ¨¡å‹åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†é“¾ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æä¾›äº†å¤šæ ·åŒ–çš„é—®ç­”å¯¹ï¼Œæ¨¡å‹åˆ™åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒä»¥ä¼˜åŒ–æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºViCA-322Kæ•°æ®é›†çš„æ„å»ºå’ŒViCA-7Bæ¨¡å‹çš„è®¾è®¡ï¼Œä½¿å…¶åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„è®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œç©ºé—´æ¨ç†å’Œç”Ÿæˆæ˜ç¡®çš„æ¨ç†é“¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ViCA-7Båœ¨æ‰€æœ‰å…«ä¸ªVSI-Benchä»»åŠ¡ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»å¯¹è·ç¦»ä»»åŠ¡ä¸Šæå‡äº†26.1ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æ›´å¤§æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®å’Œæ™ºèƒ½å®¶å±…ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨ç©ºé—´è®¤çŸ¥å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´ä¸ºæ™ºèƒ½å’Œçµæ´»çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.

