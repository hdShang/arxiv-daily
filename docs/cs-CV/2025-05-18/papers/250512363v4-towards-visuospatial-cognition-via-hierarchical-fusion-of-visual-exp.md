---
layout: default
title: Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts
---

# Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12363" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12363v4</a>
  <a href="https://arxiv.org/pdf/2505.12363.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12363v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12363v4', 'Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qi Feng

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-18 (æ›´æ–°: 2025-09-09)

**å¤‡æ³¨**: 26 pages, 19 figures, 4 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViCA2ä»¥è§£å†³è§†è§‰ç©ºé—´è®¤çŸ¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰ç©ºé—´è®¤çŸ¥` `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `SigLIP` `Hiera` `æ•°æ®é›†ViCA-322K` `ä»¤ç‰Œæ¯”ä¾‹æ§åˆ¶` `VSI-Bench`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç©ºé—´è®¤çŸ¥ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œç¼ºä¹å¿…è¦çš„æ¶æ„å’Œè®­ç»ƒæ•°æ®ã€‚
2. ViCA2é€šè¿‡åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œç»“åˆSigLIPå’ŒHieraï¼Œæå‡ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼•å…¥ä»¤ç‰Œæ¯”ä¾‹æ§åˆ¶æœºåˆ¶ä»¥æé«˜æ•ˆç‡ã€‚
3. ViCA2-7Bæ¨¡å‹åœ¨VSI-BenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—56.8çš„å¹³å‡åˆ†ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å¤§å‹æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰ç©ºé—´æ™ºèƒ½æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è§†è§‰ç©ºé—´è®¤çŸ¥â€”â€”å…³äºç©ºé—´å¸ƒå±€ã€å…³ç³»å’ŒåŠ¨æ€çš„æ¨ç†â€”â€”ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹å¾€å¾€ç¼ºä¹å¿…è¦çš„æ¶æ„ç»„ä»¶å’Œä¸“é—¨çš„è®­ç»ƒæ•°æ®ä»¥å®ç°ç»†ç²’åº¦çš„ç©ºé—´ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ViCA2ï¼ˆè§†è§‰ç©ºé—´è®¤çŸ¥åŠ©æ‰‹2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„MLLMï¼Œæ—¨åœ¨å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ViCA2é‡‡ç”¨åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œé›†æˆäº†SigLIPç”¨äºè¯­ä¹‰ç†è§£å’ŒHieraç”¨äºç©ºé—´ç»“æ„ï¼ŒåŒæ—¶é…å¤‡äº†ä»¤ç‰Œæ¯”ä¾‹æ§åˆ¶æœºåˆ¶ä»¥æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ViCA-322Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡322,000ä¸ªç©ºé—´åŸºç¡€çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œç”¨äºé’ˆå¯¹æ€§æŒ‡ä»¤è°ƒä¼˜ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†ä¸Šï¼Œæˆ‘ä»¬çš„ViCA2-7Bæ¨¡å‹å–å¾—äº†56.8çš„æœ€æ–°å¹³å‡åˆ†ï¼Œæ˜¾è‘—è¶…è¶Šäº†æ›´å¤§çš„å¼€æºæ¨¡å‹ï¼ˆå¦‚LLaVA-NeXT-Video-72Bï¼Œ40.9ï¼‰å’Œé¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼ˆGemini-1.5 Proï¼Œ45.4ï¼‰ã€‚è¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°å¼ºå¤§çš„è§†è§‰ç©ºé—´æ™ºèƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†ViCA2ã€å…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰ç©ºé—´è®¤çŸ¥ä¸­çš„æ¨ç†é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç©ºé—´å¸ƒå±€å’Œå…³ç³»ç†è§£ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œç¼ºä¹ä¸“é—¨çš„æ¶æ„å’Œè®­ç»ƒæ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šViCA2çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œåˆ†åˆ«å¤„ç†è¯­ä¹‰å’Œç©ºé—´ç»“æ„ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æ­¤è®¾è®¡æ—¨åœ¨å¼¥è¡¥ç°æœ‰æ¨¡å‹åœ¨ç»†ç²’åº¦ç©ºé—´ç†è§£ä¸Šçš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šViCA2çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šSigLIPç”¨äºè¯­ä¹‰ç†è§£ï¼ŒHieraç”¨äºç©ºé—´ç»“æ„çš„è§£æã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜å¼•å…¥äº†ä»¤ç‰Œæ¯”ä¾‹æ§åˆ¶æœºåˆ¶ï¼Œä»¥æé«˜å¤„ç†æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šViCA2çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŒè§†è§‰ç¼–ç å™¨æ¶æ„çš„è®¾è®¡ï¼Œç»“åˆäº†è¯­ä¹‰å’Œç©ºé—´ç»“æ„çš„å¤„ç†èƒ½åŠ›ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€å¤„ç†æ–¹å¼å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–ç©ºé—´æ¨ç†çš„æ•ˆæœã€‚åŒæ—¶ï¼Œä»¤ç‰Œæ¯”ä¾‹æ§åˆ¶æœºåˆ¶çš„å¼•å…¥ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„é«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ViCA2-7Bæ¨¡å‹åœ¨VSI-BenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—56.8çš„å¹³å‡åˆ†ï¼Œæ˜¾è‘—è¶…è¶Šäº†LLaVA-NeXT-Video-72Bï¼ˆ40.9ï¼‰å’ŒGemini-1.5 Proï¼ˆ45.4ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰ç©ºé—´æ™ºèƒ½æ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©æœºå™¨æ›´å¥½åœ°ç†è§£å’Œæ¨ç†ç©ºé—´ä¿¡æ¯ï¼Œä»è€Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼ŒViCA2æœ‰æœ›åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œè§†è§‰ç†è§£é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.

