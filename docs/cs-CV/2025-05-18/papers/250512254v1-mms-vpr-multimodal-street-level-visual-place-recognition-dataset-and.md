---
layout: default
title: MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark
---

# MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12254" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12254v1</a>
  <a href="https://arxiv.org/pdf/2505.12254.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12254v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12254v1', 'MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiwei Ou, Xiaobin Ren, Ronggui Sun, Guansong Gao, Ziyi Jiang, Kaiqi Zhao, Manfredo Manfredini

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-18

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/Yiwei-Ou)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMS-VPRæ•°æ®é›†ä»¥è§£å†³è¡—æ™¯è§†è§‰ä½ç½®è¯†åˆ«çš„å¤šæ¨¡æ€ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰ä½ç½®è¯†åˆ«` `å¤šæ¨¡æ€æ•°æ®` `è¡—æ™¯è¯†åˆ«` `å›¾ç¥ç»ç½‘ç»œ` `æ•°æ®é›†æ„å»º` `ç©ºé—´å›¾` `è¡Œäººç¯å¢ƒ` `åŸå¸‚è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰ä½ç½®è¯†åˆ«æ•°æ®é›†ç¼ºä¹å¤šæ¨¡æ€æ•°æ®ï¼Œä¸”åœ¨éè¥¿æ–¹åŸå¸‚ç¯å¢ƒä¸­è¡¨ç°ä¸è¶³ï¼Œé™åˆ¶äº†ç ”ç©¶çš„å¹¿åº¦å’Œæ·±åº¦ã€‚
2. MMS-VPRæ•°æ®é›†é€šè¿‡ç³»ç»ŸåŒ–çš„æ•°æ®æ”¶é›†åè®®ï¼Œæä¾›äº†ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ¶µç›–å¤æ‚çš„è¡Œäººç¯å¢ƒï¼Œé™ä½äº†æ•°æ®é›†åˆ›å»ºçš„é—¨æ§›ã€‚
3. åŸºäºä¼ ç»ŸVPRæ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œçš„å¹¿æ³›åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œåˆ©ç”¨å¤šæ¨¡æ€å’Œç»“æ„çº¿ç´¢çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œæ¨åŠ¨äº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†è§‰ä½ç½®è¯†åˆ«ï¼ˆVPRï¼‰æ•°æ®é›†ä¸»è¦ä¾èµ–äºè½¦è¾†æ‹æ‘„çš„å›¾åƒï¼Œç¼ºä¹å¤šæ¨¡æ€å¤šæ ·æ€§ï¼Œå¹¶ä¸”åœ¨éè¥¿æ–¹åŸå¸‚ç¯å¢ƒä¸­çš„å¯†é›†æ··åˆä½¿ç”¨è¡—é“ç©ºé—´è¡¨ç°ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MMS-VPRï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸“æ³¨äºå¤æ‚çš„è¡Œäººä¸“ç”¨ç¯å¢ƒä¸­çš„è¡—æ™¯ä½ç½®è¯†åˆ«ã€‚è¯¥æ•°æ®é›†åŒ…å«78,575å¼ æ ‡æ³¨å›¾åƒå’Œ2,512ä¸ªè§†é¢‘ç‰‡æ®µï¼Œè¦†ç›–ä¸­å›½æˆéƒ½ä¸€ä¸ªçº¦70,800å¹³æ–¹ç±³çš„å¼€æ”¾å•†ä¸šåŒºçš„207ä¸ªåœ°ç‚¹ã€‚æ¯å¼ å›¾åƒéƒ½æ ‡æ³¨äº†ç²¾ç¡®çš„GPSåæ ‡ã€æ—¶é—´æˆ³å’Œæ–‡æœ¬å…ƒæ•°æ®ï¼Œæ¶µç›–äº†ä¸åŒçš„å…‰ç…§æ¡ä»¶ã€è§†è§’å’Œæ—¶é—´æ¡†æ¶ã€‚MMS-VPRéµå¾ªç³»ç»ŸåŒ–å’Œå¯å¤åˆ¶çš„æ•°æ®æ”¶é›†åè®®ï¼Œé™ä½äº†æ•°æ®é›†åˆ›å»ºçš„é—¨æ§›ã€‚æ•°æ®é›†å½¢æˆäº†ä¸€ä¸ªå†…åœ¨çš„ç©ºé—´å›¾ï¼Œæ”¯æŒç»“æ„æ„ŸçŸ¥çš„ä½ç½®è¯†åˆ«ï¼Œå¹¶å®šä¹‰äº†ä¸¤ä¸ªåº”ç”¨ç‰¹å®šçš„å­é›†ä»¥æ”¯æŒç»†ç²’åº¦å’ŒåŸºäºå›¾çš„è¯„ä¼°ä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰ä½ç½®è¯†åˆ«æ•°æ®é›†åœ¨å¤šæ¨¡æ€æ€§å’Œéè¥¿æ–¹åŸå¸‚ç¯å¢ƒä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¯†é›†çš„è¡—é“åœºæ™¯ä¸­ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè½¦è¾†æ‹æ‘„çš„å›¾åƒï¼Œç¼ºä¹å¯¹è¡Œäººä¸“ç”¨ç¯å¢ƒçš„æœ‰æ•ˆæ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºMMS-VPRæ•°æ®é›†ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„æ•°æ®æ”¶é›†æ–¹æ³•ï¼Œæ¶µç›–å¤šç§å…‰ç…§æ¡ä»¶å’Œè§†è§’ï¼Œæä¾›ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ”¯æŒå¤æ‚ç¯å¢ƒä¸­çš„ä½ç½®è¯†åˆ«ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMS-VPRæ•°æ®é›†çš„æ„å»ºåŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ ‡æ³¨å’Œç»“æ„åŒ–å›¾çš„å½¢æˆã€‚æ•°æ®é‡‡é›†é˜¶æ®µä½¿ç”¨ç®€å•è®¾å¤‡ï¼Œæ ‡æ³¨é˜¶æ®µç¡®ä¿æ¯å¼ å›¾åƒçš„GPSåæ ‡å’Œæ—¶é—´æˆ³å‡†ç¡®ï¼Œæœ€åå½¢æˆä¸€ä¸ªåŒ…å«125æ¡è¾¹å’Œ81ä¸ªèŠ‚ç‚¹çš„ç©ºé—´å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šMMS-VPRçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€æ•°æ®çš„ä¸°å¯Œæ€§å’Œç»“æ„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå½¢æˆçš„ç©ºé—´å›¾ä½¿å¾—ä½ç½®è¯†åˆ«ä¸ä»…ä¾èµ–äºå›¾åƒä¿¡æ¯ï¼Œè¿˜èƒ½åˆ©ç”¨ç©ºé—´å…³ç³»è¿›è¡Œæ›´ç²¾ç¡®çš„è¯†åˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†ä¸­æ¯å¼ å›¾åƒéƒ½é™„å¸¦è¯¦ç»†çš„å…ƒæ•°æ®ï¼Œé‡‡ç”¨äº†ç³»ç»ŸåŒ–çš„æ ‡æ³¨æµç¨‹ï¼Œç¡®ä¿æ•°æ®çš„é«˜è´¨é‡å’Œå¯ç”¨æ€§ã€‚å®éªŒä¸­ä½¿ç”¨çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”å¤šæ¨¡æ€å’Œå›¾ç»“æ„çš„ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MMS-VPRæ•°æ®é›†çš„ä¼ ç»ŸVPRæ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šæ¨¡æ€å’Œç»“æ„çº¿ç´¢çš„åˆ©ç”¨ä¸‹ï¼Œè¯†åˆ«å‡†ç¡®ç‡æé«˜äº†XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MMS-VPRæ•°æ®é›†çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€åŸå¸‚è§„åˆ’ã€å¢å¼ºç°å®å’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æä¾›ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œç ”ç©¶äººå‘˜å¯ä»¥åœ¨è§†è§‰è¯†åˆ«ã€åœ°ç†ç©ºé—´ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†ç­‰é¢†åŸŸè¿›è¡Œæ·±å…¥ç ”ç©¶ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing visual place recognition (VPR) datasets predominantly rely on vehicle-mounted imagery, lack multimodal diversity and underrepresent dense, mixed-use street-level spaces, especially in non-Western urban contexts. To address these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for street-level place recognition in complex, pedestrian-only environments. The dataset comprises 78,575 annotated images and 2,512 video clips captured across 207 locations in a ~70,800 $\mathrm{m}^2$ open-air commercial district in Chengdu, China. Each image is labeled with precise GPS coordinates, timestamp, and textual metadata, and covers varied lighting conditions, viewpoints, and timeframes. MMS-VPR follows a systematic and replicable data collection protocol with minimal device requirements, lowering the barrier for scalable dataset creation. Importantly, the dataset forms an inherent spatial graph with 125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place recognition. We further define two application-specific subsets -- Dataset_Edges and Dataset_Points -- to support fine-grained and graph-based evaluation tasks. Extensive benchmarks using conventional VPR models, graph neural networks, and multimodal baselines show substantial improvements when leveraging multimodal and structural cues. MMS-VPR facilitates future research at the intersection of computer vision, geospatial understanding, and multimodal reasoning. The dataset is publicly available at https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.

