---
layout: default
title: LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding
---

# LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12253v1</a>
  <a href="https://arxiv.org/pdf/2505.12253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12253v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12253v1', 'LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanyu Zhou, Gim Hee Lee

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLaVA-4Dä»¥è§£å†³åŠ¨æ€åœºæ™¯ç†è§£ä¸­çš„æ—¶ç©ºè¡¨ç¤ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€åœºæ™¯ç†è§£` `æ—¶ç©ºæç¤º` `å¤šæ¨¡æ€æ¨¡å‹` `4Dè§†è§‰` `è¯­è¨€åµŒå…¥` `æ•°æ®é›†æ„å»º` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„3D LMMsä¸»è¦ä¾èµ–å›ºå®šç©ºé—´æç¤ºï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†åŠ¨æ€ç‰©ä½“çš„æ—¶å˜ç‰¹æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ—¶ç©ºæç¤ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å°†3Dä½ç½®å’Œ1Dæ—¶é—´ç¼–ç ä¸ºåŠ¨æ€æ„ŸçŸ¥çš„4Dåæ ‡åµŒå…¥ï¼Œå¢å¼ºäº†åŠ¨æ€åœºæ™¯çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
3. é€šè¿‡ä¸è¯­è¨€åµŒå…¥å¯¹é½ï¼ŒLLaVA-4Dèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£é™æ€èƒŒæ™¯å’ŒåŠ¨æ€ç‰©ä½“çš„ç©ºé—´ä¸æ—¶é—´ç‰¹å¾ï¼Œå®éªŒç»“æœæ˜¾ç¤ºåœ¨å¤šä¸ªä»»åŠ¡ä¸­å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡åœ¨2Då›¾åƒç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ç‰©ç†ä¸–ç•Œä¸­çš„è¡¨ç°ä»ç„¶å—é™äºç©ºé—´è¡¨ç¤ºçš„ç¼ºä¹ã€‚ç°æœ‰çš„3D LMMsä¸»è¦å°†3Dä½ç½®ä½œä¸ºå›ºå®šç©ºé—´æç¤ºåµŒå…¥è§†è§‰ç‰¹å¾ä¸­ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰åŠ¨æ€ç‰©ä½“çš„æ—¶å˜ç‰¹æ€§ã€‚æœ¬æ–‡æå‡ºäº†LLaVA-4Dï¼Œä¸€ä¸ªé€šç”¨çš„LMMæ¡†æ¶ï¼Œé€šè¿‡ç”ŸæˆåŠ¨æ€æ„ŸçŸ¥çš„4Dåæ ‡åµŒå…¥ï¼Œå°†æ—¶ç©ºæç¤ºåµŒå…¥è§†è§‰ç‰¹å¾ä¸­ï¼Œä»è€Œå¢å¼º4Dåœºæ™¯ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰æ—¶ç©ºåæ ‡æ³¨é‡Šçš„4Dè§†è§‰-è¯­è¨€æ•°æ®é›†ï¼Œä»¥ä¾¿å¯¹LMMsè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚å¤§é‡å®éªŒè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨4Dåœºæ™¯ç†è§£çš„ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰3D LMMsåœ¨åŠ¨æ€åœºæ™¯ç†è§£ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯æ— æ³•æ•æ‰åŠ¨æ€ç‰©ä½“çš„æ—¶å˜ç‰¹æ€§ã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–å›ºå®šçš„ç©ºé—´æç¤ºï¼Œå¯¼è‡´å¯¹åŠ¨æ€èƒŒæ™¯çš„ç†è§£èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLLaVA-4Dçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥åŠ¨æ€æ„ŸçŸ¥çš„4Dåæ ‡åµŒå…¥ï¼Œå°†3Dç©ºé—´ä¿¡æ¯ä¸1Dæ—¶é—´ä¿¡æ¯ç»“åˆï¼Œç”Ÿæˆæ—¶ç©ºæç¤ºï¼Œä»è€Œæå‡æ¨¡å‹å¯¹åŠ¨æ€åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLLaVA-4Dæ¡†æ¶åŒ…æ‹¬æ—¶ç©ºæç¤ºç”Ÿæˆæ¨¡å—ã€è§†è§‰ç‰¹å¾æå–æ¨¡å—å’Œè¯­è¨€åµŒå…¥å¯¹é½æ¨¡å—ã€‚é¦–å…ˆï¼Œæ¨¡å‹é€šè¿‡ç¼–ç 3Dä½ç½®å’Œ1Dæ—¶é—´ç”Ÿæˆ4Dåæ ‡åµŒå…¥ï¼Œç„¶åå°†å…¶åµŒå…¥åˆ°è§†è§‰ç‰¹å¾ä¸­ï¼Œæœ€åä¸è¯­è¨€åµŒå…¥è¿›è¡Œå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†åŠ¨æ€æ„ŸçŸ¥çš„4Dåæ ‡åµŒå…¥æ–¹æ³•ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†é™æ€èƒŒæ™¯ä¸åŠ¨æ€ç‰©ä½“ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿçš„å›ºå®šç©ºé—´æç¤ºæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ—¶ç©ºæç¤ºä¸è§†è§‰ç‰¹å¾çš„å¯¹é½æ•ˆæœï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥æ”¯æŒ4Dåæ ‡åµŒå…¥çš„æœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ª4Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­ï¼ŒLLaVA-4Dç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šæå‡äº†15%ä»¥ä¸Šï¼Œå°¤å…¶åœ¨åŠ¨æ€ç‰©ä½“è¯†åˆ«å’ŒèƒŒæ™¯åˆ†ç¦»ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºï¼ŒéªŒè¯äº†å…¶æ—¶ç©ºæç¤ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LLaVA-4Dçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡æå‡åŠ¨æ€åœºæ™¯ç†è§£èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒå¤æ‚ç¯å¢ƒä¸‹çš„å†³ç­–ä¸äº¤äº’ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite achieving significant progress in 2D image understanding, large multimodal models (LMMs) struggle in the physical world due to the lack of spatial representation. Typically, existing 3D LMMs mainly embed 3D positions as fixed spatial prompts within visual features to represent the scene. However, these methods are limited to understanding the static background and fail to capture temporally varying dynamic objects. In this paper, we propose LLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visual representation in 4D scene understanding. The spatiotemporal prompt is generated by encoding 3D position and 1D time into a dynamic-aware 4D coordinate embedding. Moreover, we demonstrate that spatial and temporal components disentangled from visual features are more effective in distinguishing the background from objects. This motivates embedding the 4D spatiotemporal prompt into these features to enhance the dynamic scene representation. By aligning visual spatiotemporal embeddings with language embeddings, LMMs gain the ability to understand both spatial and temporal characteristics of static background and dynamic objects in the physical world. Additionally, we construct a 4D vision-language dataset with spatiotemporal coordinate annotations for instruction fine-tuning LMMs. Extensive experiments have been conducted to demonstrate the effectiveness of our method across different tasks in 4D scene understanding.

