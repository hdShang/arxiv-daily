---
layout: default
title: Test-time Vocabulary Adaptation for Language-driven Object Detection
---

# Test-time Vocabulary Adaptation for Language-driven Object Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00333" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00333v1</a>
  <a href="https://arxiv.org/pdf/2506.00333.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00333v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00333v1', 'Test-time Vocabulary Adaptation for Language-driven Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mingxuan Liu, Tyler L. Hayes, Massimiliano Mancini, Elisa Ricci, Riccardo Volpi, Gabriela Csurka

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**å¤‡æ³¨**: Accepted as a conference paper at ICIP 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVocAdaä»¥è§£å†³å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹ä¸­çš„è¯æ±‡é€‚åº”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡æ£€æµ‹` `ç‰©ä½“æ£€æµ‹` `å›¾åƒæè¿°` `è¯æ±‡é€‚åº”` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹æ–¹æ³•åœ¨ç”¨æˆ·å®šä¹‰çš„è¯æ±‡è¿‡äºå®½æ³›æˆ–é”™è¯¯æ—¶ï¼Œæ€§èƒ½å—åˆ°æ˜¾è‘—å½±å“ã€‚
2. æœ¬æ–‡æå‡ºVocAdaï¼Œé€šè¿‡å›¾åƒæè¿°å’Œåè¯è§£æï¼Œè‡ªåŠ¨ä¼˜åŒ–ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ï¼Œæå‡æ£€æµ‹ç²¾åº¦ã€‚
3. åœ¨COCOå’ŒObjects365æ•°æ®é›†ä¸Šï¼ŒVocAdaåœ¨ä¸‰ç§æœ€å…ˆè¿›çš„æ£€æµ‹å™¨ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹æ¨¡å‹å…è®¸ç”¨æˆ·åœ¨æµ‹è¯•æ—¶è‡ªç”±æŒ‡å®šè‡ªç„¶è¯­è¨€ç±»è¯æ±‡ï¼Œä»è€ŒæŒ‡å¯¼æ‰€éœ€ç‰©ä½“çš„æ£€æµ‹ã€‚ç„¶è€Œï¼Œè¯æ±‡å¯èƒ½è¿‡äºå®½æ³›æˆ–é”™è¯¯æŒ‡å®šï¼Œå½±å“æ£€æµ‹å™¨çš„æ•´ä½“æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„è¯æ±‡é€‚é…å™¨VocAdaï¼Œè‡ªåŠ¨è°ƒæ•´ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ï¼Œä½¿å…¶ä¸ç»™å®šå›¾åƒç›¸å…³çš„ç±»åˆ«ç›¸åŒ¹é…ã€‚VocAdaåœ¨æ¨ç†æ—¶æ— éœ€ä»»ä½•è®­ç»ƒï¼Œåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆä½¿ç”¨å›¾åƒæè¿°å™¨æè¿°å¯è§ç‰©ä½“ï¼Œå…¶æ¬¡ä»æè¿°ä¸­è§£æåè¯ï¼Œæœ€åä»ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ä¸­é€‰æ‹©ç›¸å…³ç±»åˆ«ï¼Œä¸¢å¼ƒä¸ç›¸å…³çš„ç±»åˆ«ã€‚åœ¨COCOå’ŒObjects365æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVocAdaå§‹ç»ˆæé«˜äº†æ£€æµ‹æ€§èƒ½ï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§ã€‚ä»£ç å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹ä¸­ç”¨æˆ·å®šä¹‰è¯æ±‡çš„é€‚åº”æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¯æ±‡ä¸å‡†ç¡®æˆ–è¿‡äºå®½æ³›æ—¶ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVocAdaçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å›¾åƒæè¿°å’Œåè¯è§£æï¼Œè‡ªåŠ¨ç­›é€‰å‡ºä¸å›¾åƒå†…å®¹ç›¸å…³çš„ç±»åˆ«ï¼Œä»è€Œä¼˜åŒ–ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ã€‚æ­¤è®¾è®¡æ—¨åœ¨æé«˜æ£€æµ‹å™¨çš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVocAdaçš„æ•´ä½“æ¶æ„åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼šç¬¬ä¸€æ­¥ï¼Œä½¿ç”¨å›¾åƒæè¿°å™¨ç”Ÿæˆå¯è§ç‰©ä½“çš„æè¿°ï¼›ç¬¬äºŒæ­¥ï¼Œä»æè¿°ä¸­æå–åè¯ï¼›ç¬¬ä¸‰æ­¥ï¼Œä»ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ä¸­é€‰æ‹©ç›¸å…³ç±»åˆ«ï¼Œä¸¢å¼ƒä¸ç›¸å…³çš„ç±»åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šVocAdaçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— éœ€è®­ç»ƒå³å¯åœ¨æ¨ç†æ—¶è‡ªåŠ¨è°ƒæ•´è¯æ±‡ï¼Œæ˜¾è‘—æé«˜äº†å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ä¾èµ–äºå›ºå®šè¯æ±‡çš„è®¾è®¡å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šVocAdaçš„è®¾è®¡ä¸­ï¼Œå›¾åƒæè¿°å™¨çš„é€‰æ‹©ã€åè¯è§£æçš„å‡†ç¡®æ€§ä»¥åŠç”¨æˆ·è¯æ±‡çš„åŠ¨æ€ç­›é€‰æ˜¯å…³é”®å› ç´ ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨COCOå’ŒObjects365æ•°æ®é›†ä¸Šï¼ŒVocAdaåœ¨ä¸‰ç§æœ€å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹å™¨ä¸Šå‡æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°äº†X%ï¼ˆå…·ä½“æ•°æ®éœ€æŸ¥é˜…åŸæ–‡ï¼‰ã€‚è¯¥æ–¹æ³•çš„å¼€æºä»£ç ä¸ºåç»­ç ”ç©¶æä¾›äº†ä¾¿åˆ©ï¼Œä¿ƒè¿›äº†ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VocAdaçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰ã€‚é€šè¿‡æé«˜ç‰©ä½“æ£€æµ‹çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ï¼ŒVocAdaèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”å¤æ‚çš„ç¯å¢ƒï¼Œæå‡äººæœºäº¤äº’çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯èƒ½æ¨åŠ¨å¼€æ”¾è¯æ±‡æ£€æµ‹åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Open-vocabulary object detection models allow users to freely specify a class vocabulary in natural language at test time, guiding the detection of desired objects. However, vocabularies can be overly broad or even mis-specified, hampering the overall performance of the detector. In this work, we propose a plug-and-play Vocabulary Adapter (VocAda) to refine the user-defined vocabulary, automatically tailoring it to categories that are relevant for a given image. VocAda does not require any training, it operates at inference time in three steps: i) it uses an image captionner to describe visible objects, ii) it parses nouns from those captions, and iii) it selects relevant classes from the user-defined vocabulary, discarding irrelevant ones. Experiments on COCO and Objects365 with three state-of-the-art detectors show that VocAda consistently improves performance, proving its versatility. The code is open source.

