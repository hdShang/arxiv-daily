---
layout: default
title: "HueManity: Probing Fine-Grained Visual Perception in MLLMs"
---

# HueManity: Probing Fine-Grained Visual Perception in MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.03194" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.03194v4</a>
  <a href="https://arxiv.org/pdf/2506.03194.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.03194v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.03194v4', 'HueManity: Probing Fine-Grained Visual Perception in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rynaa Grover, Jayant Sravan Tamarapalli, Sahiti Yerramilli, Nilay Pande

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31 (æ›´æ–°: 2025-09-12)

**æœŸåˆŠ**: ICML 2025 Workshop on Assessing World Models

**DOI**: [10.48550/arXiv.2506.03194](https://doi.org/10.48550/arXiv.2506.03194)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHueManityåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰æ„ŸçŸ¥` `åŸºå‡†æ•°æ®é›†` `æ¨¡å¼è¯†åˆ«` `Ishiharaæµ‹è¯•` `è®¡ç®—æœºè§†è§‰` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†è‡´çš„è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚
2. HueManityåŸºå‡†é€šè¿‡åµŒå…¥Ishiharaæµ‹è¯•é£æ ¼çš„å›¾åƒï¼Œä¸“æ³¨äºè¯„ä¼°MLLMsçš„ç²¾ç¡®æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³MLLMåœ¨ç®€å•ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¸º33.6%ï¼Œè€Œäººç±»å‚ä¸è€…æ¥è¿‘å®Œç¾ï¼Œæ­ç¤ºäº†æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ç¼ºé™·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é«˜å±‚æ¬¡è§†è§‰æ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç»†è‡´çš„æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å´ç›¸å¯¹æœ‰é™ã€‚æœ¬æ–‡æå‡ºHueManityï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMsè§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«83,850å¼ å›¾åƒï¼Œå›¾åƒä¸­åµŒå…¥äº†ä»¥Ishiharaæµ‹è¯•é£æ ¼çš„ç‚¹é˜µæ¨¡å¼å‘ˆç°çš„ä¸¤å­—ç¬¦å­—æ¯æ•°å­—å­—ç¬¦ä¸²ï¼ŒæŒ‘æˆ˜æ¨¡å‹è¿›è¡Œç²¾ç¡®çš„æ¨¡å¼è¯†åˆ«ã€‚å¯¹ä¹ç§æœ€å…ˆè¿›çš„MLLMsåœ¨HueManityä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸äººç±»å’Œä¼ ç»Ÿè®¡ç®—æœºè§†è§‰åŸºçº¿ç›¸æ¯”ï¼Œæ¨¡å‹çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¡¨ç°æœ€ä½³çš„MLLMåœ¨æ•°å­—â€œç®€å•â€ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¸º33.6%ï¼Œè€Œåœ¨å­—æ¯æ•°å­—â€œå›°éš¾â€ä»»åŠ¡ä¸Šä»…ä¸º3%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å‚ä¸è€…çš„å¾—åˆ†æ¥è¿‘å®Œç¾ï¼ˆ100%å’Œ95.6%ï¼‰ï¼Œè€Œç»è¿‡å¾®è°ƒçš„ResNet50æ¨¡å‹çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º96.5%å’Œ94.5%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å½“å‰MLLMsåœ¨è§†è§‰èƒ½åŠ›æ–¹é¢çš„å…³é”®å·®è·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†å¯èƒ½å¯¼è‡´è¿™ä¸€æ„ŸçŸ¥å·®è·çš„æ¶æ„å’Œè®­ç»ƒèŒƒå¼å› ç´ ï¼Œå¹¶å¼€æºHueManityæ•°æ®é›†å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›å¯¹MLLMsæ„ŸçŸ¥é²æ£’æ€§çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†è‡´è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨ç²¾ç¡®æ¨¡å¼è¯†åˆ«ä¸Šå­˜åœ¨æ˜æ˜¾çŸ­æ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHueManityåŸºå‡†é€šè¿‡è®¾è®¡å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾åƒæ•°æ®é›†ï¼Œè¯„ä¼°MLLMsåœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„èƒ½åŠ›ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹çš„å±€é™æ€§å¹¶ä¿ƒè¿›æ”¹è¿›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHueManityæ•°æ®é›†åŒ…å«83,850å¼ å›¾åƒï¼Œé‡‡ç”¨Ishiharaæµ‹è¯•é£æ ¼çš„ç‚¹é˜µæ¨¡å¼ï¼Œæ¨¡å‹éœ€è¯†åˆ«åµŒå…¥çš„å­—æ¯æ•°å­—å­—ç¬¦ä¸²ã€‚è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæ¯”è¾ƒMLLMsä¸äººç±»åŠä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šHueManityåŸºå‡†çš„è®¾è®¡æ˜¯ä¸€ä¸ªé‡è¦åˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡çš„ç»†ç²’åº¦è¯„ä¼°ä¸Šï¼Œä¸ç°æœ‰çš„é«˜å±‚æ¬¡æ¨ç†ä»»åŠ¡å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†çš„æ„å»ºä¸­ï¼Œé€‰æ‹©äº†ç‰¹å®šçš„å›¾åƒæ ·å¼å’Œå­—ç¬¦ç»„åˆï¼Œä»¥ç¡®ä¿ä»»åŠ¡çš„éš¾åº¦å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶åœ¨è¯„ä¼°ä¸­ä½¿ç”¨äº†å¤šç§å…ˆè¿›çš„MLLMsè¿›è¡Œå¯¹æ¯”åˆ†æã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³è¡¨ç°çš„MLLMåœ¨æ•°å­—â€œç®€å•â€ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¸º33.6%ï¼Œè€Œåœ¨å­—æ¯æ•°å­—â€œå›°éš¾â€ä»»åŠ¡ä¸Šä»…ä¸º3%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å‚ä¸è€…çš„å¾—åˆ†æ¥è¿‘å®Œç¾ï¼ˆ100%å’Œ95.6%ï¼‰ï¼Œè€Œå¾®è°ƒçš„ResNet50æ¨¡å‹çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º96.5%å’Œ94.5%ï¼Œæ­ç¤ºäº†å½“å‰MLLMsåœ¨è§†è§‰æ„ŸçŸ¥ä¸Šçš„æ˜¾è‘—ä¸è¶³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HueManityåŸºå‡†çš„æå‡ºä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†æ–°çš„è¯„ä¼°æ ‡å‡†ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ä»…å¯ä»¥å¸®åŠ©æ”¹è¿›ç°æœ‰æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œè¿˜èƒ½ä¸ºæœªæ¥çš„å¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶æä¾›é‡è¦å‚è€ƒï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨è§†è§‰æ„ŸçŸ¥é¢†åŸŸçš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) excel at high-level visual reasoning, but their performance on nuanced perceptual tasks remains surprisingly limited. We present HueManity, a benchmark designed to assess visual perception in MLLMs. The dataset comprises 83,850 images featuring two-character alphanumeric strings embedded in Ishihara test style dot patterns, challenging models on precise pattern recognition. Our evaluation of nine state-of-the-art MLLMs on HueManity demonstrates a significant performance deficit compared to human and traditional computer vision baselines. The best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a striking 3% on the alphanumeric `hard' task. In contrast, human participants achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model reached accuracies of 96.5% and 94.5%. These results highlight a critical gap in the visual capabilities of current MLLMs. Our analysis further explores potential architectural and training-paradigm factors contributing to this perceptual gap in MLLMs. We open-source HueManity dataset and code to foster further research in improving perceptual robustness of MLLMs.

