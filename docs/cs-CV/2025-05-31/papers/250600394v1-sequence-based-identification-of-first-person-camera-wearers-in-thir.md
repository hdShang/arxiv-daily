---
layout: default
title: Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views
---

# Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00394" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00394v1</a>
  <a href="https://arxiv.org/pdf/2506.00394.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00394v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00394v1', 'Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziwei Zhao, Xizi Wang, Yuchen Wang, Feng Cheng, David Crandall

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTF2025æ•°æ®é›†ä¸åºåˆ—è¯†åˆ«æ–¹æ³•ä»¥è§£å†³å¤šæ‘„åƒå¤´äº¤äº’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `ç¬¬ä¸€äººç§°è§†è§‰` `å¤šæ‘„åƒå¤´äº¤äº’` `äººç‰©é‡è¯†åˆ«` `è¿åŠ¨ç‰¹å¾` `æ•°æ®é›†TF2025`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ‘„åƒå¤´ä½©æˆ´è€…äº¤äº’çš„è¯†åˆ«ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€ç¯å¢ƒä¸­éš¾ä»¥å‡†ç¡®åŒºåˆ†ä¸åŒä½©æˆ´è€…ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåºåˆ—çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¿åŠ¨çº¿ç´¢ä¸äººç‰©é‡è¯†åˆ«æŠ€æœ¯ï¼Œæå‡äº†ç¬¬ä¸€äººç§°ä½©æˆ´è€…åœ¨ç¬¬ä¸‰äººç§°è§†é¢‘ä¸­çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡æ˜¾è‘—æé«˜äº†è¯†åˆ«æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ç¬¬ä¸€äººç§°æ‘„åƒæœºçš„æ™®åŠï¼Œç ”ç©¶å…±äº«ç¯å¢ƒä¸­çš„å¤šæ‘„åƒå¤´äº¤äº’å˜å¾—æ„ˆå‘é‡è¦ã€‚å°½ç®¡Ego4Då’ŒEgo-Exo4Dç­‰å¤§è§„æ¨¡æ•°æ®é›†æ¨åŠ¨äº†ç¬¬ä¸€äººç§°è§†è§‰ç ”ç©¶ï¼Œä½†å¤šæ‘„åƒå¤´ä½©æˆ´è€…ä¹‹é—´çš„äº¤äº’ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ï¼Œè¿™å¯¹äºæ²‰æµ¸å¼å­¦ä¹ å’Œåä½œæœºå™¨äººç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†TF2025ï¼Œä¸€ä¸ªæ‰©å±•çš„æ•°æ®é›†ï¼ŒåŒ…å«åŒæ­¥çš„ç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§†è§’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºåºåˆ—çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¿åŠ¨çº¿ç´¢å’Œäººç‰©é‡è¯†åˆ«æŠ€æœ¯ï¼Œè¯†åˆ«ç¬¬ä¸‰äººç§°è§†é¢‘ä¸­çš„ç¬¬ä¸€äººç§°ä½©æˆ´è€…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç¬¬ä¸‰äººç§°è§†è§’ä¸‹è¯†åˆ«ç¬¬ä¸€äººç§°æ‘„åƒæœºä½©æˆ´è€…çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯ä¸­å¯¹å¤šä½©æˆ´è€…çš„äº¤äº’è¯†åˆ«æ•ˆæœä¸ä½³ï¼Œå¯¼è‡´è¯†åˆ«ç²¾åº¦ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆè¿åŠ¨çº¿ç´¢å’Œäººç‰©é‡è¯†åˆ«çš„åºåˆ—è¯†åˆ«æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åˆ†ææ—¶é—´åºåˆ—æ•°æ®æ¥æé«˜è¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è¿åŠ¨ç‰¹å¾æå–ã€äººç‰©é‡è¯†åˆ«æ¨¡å—å’Œæœ€ç»ˆçš„è¯†åˆ«å†³ç­–é˜¶æ®µã€‚é¦–å…ˆå¯¹è¾“å…¥çš„è§†é¢‘æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæå–è¿åŠ¨ç‰¹å¾ï¼Œç„¶åé€šè¿‡é‡è¯†åˆ«æ¨¡å—è¿›è¡Œèº«ä»½åŒ¹é…ï¼Œæœ€åè¾“å‡ºè¯†åˆ«ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åºåˆ—æ•°æ®åˆ†æï¼Œç»“åˆäº†è¿åŠ¨ä¿¡æ¯ä¸è§†è§‰ç‰¹å¾ï¼Œä½¿å¾—è¯†åˆ«è¿‡ç¨‹æ›´åŠ ç²¾å‡†ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„ç»“åˆï¼Œä¼˜åŒ–äº†æŸå¤±å‡½æ•°ä»¥å¢å¼ºå¯¹è¿åŠ¨ç‰¹å¾çš„æ•æ„Ÿæ€§ï¼ŒåŒæ—¶è®¾å®šäº†é€‚å½“çš„è¶…å‚æ•°ä»¥æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡15%çš„è¯†åˆ«å‡†ç¡®ç‡æå‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯†åˆ«ç²¾åº¦æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ²‰æµ¸å¼å­¦ä¹ ã€åä½œæœºå™¨äººå’Œæ™ºèƒ½ç›‘æ§ç­‰ã€‚é€šè¿‡å‡†ç¡®è¯†åˆ«ç¬¬ä¸€äººç§°ä½©æˆ´è€…ï¼Œèƒ½å¤Ÿæå‡å¤šæ‘„åƒå¤´ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„äº¤äº’èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The increasing popularity of egocentric cameras has generated growing interest in studying multi-camera interactions in shared environments. Although large-scale datasets such as Ego4D and Ego-Exo4D have propelled egocentric vision research, interactions between multiple camera wearers remain underexplored-a key gap for applications like immersive learning and collaborative robotics. To bridge this, we present TF2025, an expanded dataset with synchronized first- and third-person views. In addition, we introduce a sequence-based method to identify first-person wearers in third-person footage, combining motion cues and person re-identification.

