---
layout: default
title: From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models
---

# From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00718" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00718v1</a>
  <a href="https://arxiv.org/pdf/2506.00718.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00718v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00718v1', 'From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianqin Li, Ziqi Wen, Leiran Song, Jun Liu, Zhi Jing, Tai Sing Lee

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDiSRTä»¥è¯„ä¼°è‡ªç›‘ç£è§†è§‰æ¨¡å‹çš„æ•´ä½“æ„ŸçŸ¥èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `è§†è§‰å˜æ¢å™¨` `æ ¼å¼å¡”åŸåˆ™` `æ•´ä½“æ„ŸçŸ¥` `æ‰­æ›²ç©ºé—´å…³ç³»æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰æ¨¡å‹åœ¨å¤„ç†å±€éƒ¨çº¿ç´¢æ—¶ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ•´ä½“æ„ŸçŸ¥èƒ½åŠ›ï¼Œéš¾ä»¥æ¨¡æ‹Ÿäººç±»è§†è§‰çš„æ ¼å¼å¡”ç»„ç»‡ç‰¹æ€§ã€‚
2. è®ºæ–‡æå‡ºäº†æ‰­æ›²ç©ºé—´å…³ç³»æµ‹è¯•å¹³å°ï¼ˆDiSRTï¼‰ï¼Œç”¨äºè¯„ä¼°è§†è§‰æ¨¡å‹å¯¹å…¨çƒç©ºé—´ç»“æ„çš„æ•æ„Ÿæ€§ï¼Œæ¢ç´¢è‡ªç›‘ç£å­¦ä¹ çš„æ½œåŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚MAEã€CLIPï¼‰åœ¨æ•´ä½“æ„ŸçŸ¥èƒ½åŠ›ä¸Šä¼˜äºç›‘ç£åŸºçº¿ï¼Œä¸”ConvNeXtæ¨¡å‹åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä¹Ÿè¡¨ç°å‡ºç±»ä¼¼èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»è§†è§‰é€šè¿‡æ ¼å¼å¡”åŸåˆ™å°†å±€éƒ¨çº¿ç´¢ç»„ç»‡æˆä¸€è‡´çš„æ•´ä½“å½¢å¼ã€‚æœ¬æ–‡ç ”ç©¶ç°ä»£è§†è§‰æ¨¡å‹æ˜¯å¦è¡¨ç°å‡ºç±»ä¼¼è¡Œä¸ºï¼Œä»¥åŠåœ¨ä½•ç§è®­ç»ƒæ¡ä»¶ä¸‹è¿™äº›è¡Œä¸ºä¼šå‡ºç°ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ©è”½è‡ªç¼–ç ï¼ˆMAEï¼‰è®­ç»ƒçš„è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰å±•ç°å‡ºä¸æ ¼å¼å¡”æ³•åˆ™ä¸€è‡´çš„æ¿€æ´»æ¨¡å¼ï¼ŒåŒ…æ‹¬è™šå¹»è½®å»“å®Œæˆã€å‡¸æ€§åå¥½å’ŒåŠ¨æ€å›¾å½¢-èƒŒæ™¯åˆ†ç¦»ã€‚é€šè¿‡å¼•å…¥æ‰­æ›²ç©ºé—´å…³ç³»æµ‹è¯•å¹³å°ï¼ˆDiSRTï¼‰ï¼Œè¯„ä¼°æ¨¡å‹å¯¹å…¨çƒç©ºé—´æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œç»“æœè¡¨æ˜è‡ªç›‘ç£æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç›‘ç£åŸºçº¿ï¼Œç”šè‡³æœ‰æ—¶è¶…è¿‡äº†äººç±»è¡¨ç°ã€‚ConvNeXtæ¨¡å‹åœ¨MAEè®­ç»ƒä¸‹ä¹Ÿå±•ç°å‡ºå…¼å®¹æ ¼å¼å¡”çš„è¡¨ç¤ºï¼Œè¡¨æ˜è¿™ç§æ•æ„Ÿæ€§å¯ä»¥åœ¨æ²¡æœ‰æ³¨æ„åŠ›æ¶æ„çš„æƒ…å†µä¸‹å‡ºç°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨æ¢è®¨ç°ä»£è§†è§‰æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œé€šè¿‡å±€éƒ¨çº¿ç´¢å½¢æˆæ•´ä½“æ„ŸçŸ¥ï¼Œç°æœ‰æ–¹æ³•åœ¨è¿™ä¸€ç‚¹ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å…¨çƒç©ºé—´ç»“æ„æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥æ‰­æ›²ç©ºé—´å…³ç³»æµ‹è¯•å¹³å°ï¼ˆDiSRTï¼‰ï¼Œè¯„ä¼°æ¨¡å‹å¯¹å…¨çƒç©ºé—´æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œè¿›è€Œåˆ†æè‡ªç›‘ç£æ¨¡å‹åœ¨æ•´ä½“æ„ŸçŸ¥ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é¦–å…ˆè®­ç»ƒè§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰å’ŒConvNeXtæ¨¡å‹ï¼Œä½¿ç”¨æ©è”½è‡ªç¼–ç ï¼ˆMAEï¼‰è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œç„¶åé€šè¿‡DiSRTè¿›è¡Œè¯„ä¼°ï¼Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥DiSRTä½œä¸ºä¸€ç§æ–°é¢–çš„è¯„ä¼°å·¥å…·ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæµ‹é‡æ¨¡å‹å¯¹å…¨çƒç©ºé—´ç»“æ„çš„æ•æ„Ÿæ€§ï¼Œå¹¶å‘ç°è‡ªç›‘ç£æ¨¡å‹åœ¨è¿™ä¸€æ–¹é¢çš„ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨æ©è”½è‡ªç¼–ç æŠ€æœ¯ï¼Œå¹¶é€šè¿‡è°ƒæ•´æ¿€æ´»ç¨€ç–æ€§æœºåˆ¶ï¼Œæ¢å¤æ¨¡å‹çš„å…¨çƒæ•æ„Ÿæ€§ï¼Œç¡®ä¿åœ¨åˆ†ç±»å¾®è°ƒè¿‡ç¨‹ä¸­ä¸ä¼šæ˜¾è‘—é™ä½æ•´ä½“æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚MAEã€CLIPï¼‰åœ¨DiSRTæµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç›‘ç£åŸºçº¿ï¼Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†äººç±»è¡¨ç°ã€‚ConvNeXtæ¨¡å‹åœ¨MAEè®­ç»ƒä¸‹ä¹Ÿå±•ç°å‡ºå…¼å®¹æ ¼å¼å¡”çš„è¡¨ç¤ºï¼Œæ˜¾ç¤ºå‡ºè‡ªç›‘ç£å­¦ä¹ çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººæ„ŸçŸ¥å’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æå‡è§†è§‰æ¨¡å‹çš„æ•´ä½“æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯ä»¥æ”¹å–„è¿™äº›é¢†åŸŸä¸­çš„å¯¹è±¡è¯†åˆ«ã€åœºæ™¯ç†è§£å’Œå†³ç­–åˆ¶å®šç­‰ä»»åŠ¡çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human vision organizes local cues into coherent global forms using Gestalt principles like closure, proximity, and figure-ground assignment -- functions reliant on global spatial structure. We investigate whether modern vision models show similar behaviors, and under what training conditions these emerge. We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE) exhibit activation patterns consistent with Gestalt laws, including illusory contour completion, convexity preference, and dynamic figure-ground segregation. To probe the computational basis, we hypothesize that modeling global dependencies is necessary for Gestalt-like organization. We introduce the Distorted Spatial Relationship Testbench (DiSRT), which evaluates sensitivity to global spatial perturbations while preserving local textures. Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform supervised baselines and sometimes even exceed human performance. ConvNeXt models trained with MAE also exhibit Gestalt-compatible representations, suggesting such sensitivity can arise without attention architectures. However, classification finetuning degrades this ability. Inspired by biological vision, we show that a Top-K activation sparsity mechanism can restore global sensitivity. Our findings identify training conditions that promote or suppress Gestalt-like perception and establish DiSRT as a diagnostic for global structure sensitivity across models.

