---
layout: default
title: "Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning"
---

# Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00318" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00318v1</a>
  <a href="https://arxiv.org/pdf/2506.00318.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00318v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00318v1', 'Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sara Ghazanfari, Francesco Croce, Nicolas Flammarion, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-31

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/SaraGhazanfari/CoF)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¸§æ„ŸçŸ¥æ¨ç†çš„è§†é¢‘ç†è§£æ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€LLMsæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `å¤šæ¨¡æ€LLMs` `æ¨ç†é“¾` `å¸§æ„ŸçŸ¥` `æ•°æ®é›†æ„å»º` `æ¨¡å‹å¾®è°ƒ` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç†è§£æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼ºä¹å¯¹å…·ä½“è§†é¢‘å¸§çš„æ˜ç¡®å¼•ç”¨ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡åˆ›å»ºCoF-Dataæ•°æ®é›†ï¼Œè®­ç»ƒè§†é¢‘LLMsç”ŸæˆåŸºäºå¸§çš„æ¨ç†é“¾ï¼Œä»¥æå‡ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºCoFçš„æ–¹æ³•åœ¨Video-MMEã€MVBenchå’ŒVSI-Benchç­‰åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰é¢†å…ˆæ¨¡å‹ï¼Œä¸”å¹»è§‰ç‡æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œåœ¨å›ç­”ç”¨æˆ·è¯·æ±‚ä¹‹å‰ï¼Œä¿ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆè‡ªç„¶è¯­è¨€æ¨ç†è½¨è¿¹å¯ä»¥æ˜¾è‘—æå‡å…¶åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœ¬æ–‡æ‰©å±•äº†è¿™ä¸€æ–¹æ³•è‡³å¤šæ¨¡æ€LLMsï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹è¾“å…¥å›¾åƒå’Œè§†é¢‘å†…å®¹ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘LLMsï¼Œå…¶æ¨ç†æ­¥éª¤åŸºäºç›¸å…³è§†é¢‘å¸§ï¼Œå¹¶æ˜ç¡®å¼•ç”¨è¿™äº›å¸§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºäº†CoF-Dataï¼Œä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–é—®é¢˜ã€ç­”æ¡ˆåŠç›¸åº”å¸§åŸºç¡€æ¨ç†è½¨è¿¹çš„å¤§å‹æ•°æ®é›†ï¼Œæ¶µç›–è‡ªç„¶å’Œåˆæˆè§†é¢‘çš„å¤šç§ä¸»é¢˜å’Œä»»åŠ¡ã€‚éšåï¼Œæˆ‘ä»¬åœ¨æ­¤é“¾å¸§æ•°æ®ä¸Šå¾®è°ƒç°æœ‰è§†é¢‘LLMsã€‚æˆ‘ä»¬çš„ç®€å•è‡ªåŒ…å«çš„æ–¹æ³•ä¸éœ€è¦è¾…åŠ©ç½‘ç»œæ¥é€‰æ‹©æˆ–æ ‡æ³¨ç›¸å…³å¸§ï¼Œç»“æœæ˜¾ç¤ºåŸºäºCoFçš„æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®å¼•ç”¨å…³é”®å¸§ç”Ÿæˆæ€ç»´é“¾ï¼Œä»è€Œåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†ä¸Šæå‡æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½å¹»è§‰ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç†è§£æ–¹æ³•ä¸­æ¨ç†æ­¥éª¤æœªèƒ½æ˜ç¡®å¼•ç”¨è§†é¢‘å¸§çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ç†è§£å’Œå›ç­”æ—¶çš„å‡†ç¡®æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åˆ›å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–é—®é¢˜å’Œå¸§åŸºç¡€æ¨ç†è½¨è¿¹çš„æ•°æ®é›†ï¼ˆCoF-Dataï¼‰ï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šå¾®è°ƒè§†é¢‘LLMsï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆä¸è§†é¢‘å¸§ç›¸å…³çš„æ€ç»´é“¾ï¼Œä»è€Œæå‡ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¾®è°ƒå’Œæ¨ç†ç”Ÿæˆä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œæ„å»ºCoF-Dataæ•°æ®é›†ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨è¯¥æ•°æ®é›†å¯¹ç°æœ‰è§†é¢‘LLMsè¿›è¡Œå¾®è°ƒï¼›æœ€åï¼Œç”ŸæˆåŸºäºå¸§çš„æ¨ç†é“¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºä¸ä¾èµ–äºè¾…åŠ©ç½‘ç»œæ¥é€‰æ‹©æˆ–æ ‡æ³¨ç›¸å…³å¸§ï¼Œè€Œæ˜¯ç›´æ¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•ç”¨å…³é”®å¸§ï¼Œä»è€Œç®€åŒ–äº†æ¨¡å‹çš„ç»“æ„å’Œè®­ç»ƒè¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†é“¾çš„å‡†ç¡®æ€§ï¼Œå¹¶ç¡®ä¿ç”Ÿæˆçš„æ¨ç†é“¾èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¸è§†é¢‘å¸§å¯¹åº”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºCoFçš„æ•°æ®é›†è®­ç»ƒåï¼Œæ¨¡å‹åœ¨Video-MMEã€MVBenchå’ŒVSI-Benchç­‰å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰é¢†å…ˆçš„è§†é¢‘LLMsï¼Œæ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œä¸”å¹»è§‰ç‡é™ä½ï¼Œè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘å†…å®¹åˆ†æã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æå‡è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€äº¤äº’ã€ä¿¡æ¯æ£€ç´¢å’Œäººæœºåä½œç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent work has shown that eliciting Large Language Models (LLMs) to generate reasoning traces in natural language before answering the user's request can significantly improve their performance across tasks. This approach has been extended to multimodal LLMs, where the models can produce chain-of-thoughts (CoT) about the content of input images and videos. In this work, we propose to obtain video LLMs whose reasoning steps are grounded in, and explicitly refer to, the relevant video frames. For this, we first create CoF-Data, a large dataset of diverse questions, answers, and corresponding frame-grounded reasoning traces about both natural and synthetic videos, spanning various topics and tasks. Then, we fine-tune existing video LLMs on this chain-of-frames (CoF) data. Our approach is simple and self-contained, and, unlike existing approaches for video CoT, does not require auxiliary networks to select or caption relevant frames. We show that our models based on CoF are able to generate chain-of-thoughts that accurately refer to the key frames to answer the given question. This, in turn, leads to improved performance across multiple video understanding benchmarks, for example, surpassing leading video LLMs on Video-MME, MVBench, and VSI-Bench, and notably reducing the hallucination rate. Code available at https://github.com/SaraGhazanfari/CoF}{github.com/SaraGhazanfari/CoF.

