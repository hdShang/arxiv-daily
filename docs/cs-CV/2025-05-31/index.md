---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-31
---

# cs.CVï¼ˆ2025-05-31ï¼‰

ğŸ“Š å…± **15** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250600600v2-satdreamer360-multiview-consistent-generation-of-ground-level-scenes.html">SatDreamer360: Multiview-Consistent Generation of Ground-Level Scenes from Satellite Imagery</a></td>
  <td>æå‡ºSatDreamer360ä»¥è§£å†³å«æ˜Ÿå›¾åƒç”Ÿæˆå¤šè§†è§’ä¸€è‡´åœ°é¢åœºæ™¯é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dreamer</span> <span class="paper-tag">height map</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00600v2" data-paper-url="./papers/250600600v2-satdreamer360-multiview-consistent-generation-of-ground-level-scenes.html" onclick="toggleFavorite(this, '2506.00600v2', 'SatDreamer360: Multiview-Consistent Generation of Ground-Level Scenes from Satellite Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250600523v1-senseflow-scaling-distribution-matching-for-flow-based-text-to-image.html">SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation</a></td>
  <td>æå‡ºSenseFlowä»¥è§£å†³å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒè’¸é¦çš„æ”¶æ•›é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00523v1" data-paper-url="./papers/250600523v1-senseflow-scaling-distribution-matching-for-flow-based-text-to-image.html" onclick="toggleFavorite(this, '2506.00523v1', 'SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250600718v1-from-local-cues-to-global-percepts-emergent-gestalt-organization-in-.html">From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models</a></td>
  <td>æå‡ºDiSRTä»¥è¯„ä¼°è‡ªç›‘ç£è§†è§‰æ¨¡å‹çš„æ•´ä½“æ„ŸçŸ¥èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00718v1" data-paper-url="./papers/250600718v1-from-local-cues-to-global-percepts-emergent-gestalt-organization-in-.html" onclick="toggleFavorite(this, '2506.00718v1', 'From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250600568v2-creft-cad-boosting-orthographic-projection-reasoning-for-cad-via-rei.html">CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning</a></td>
  <td>æå‡ºCReFT-CADä»¥è§£å†³CADä¸­æ­£æŠ•å½±æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00568v2" data-paper-url="./papers/250600568v2-creft-cad-boosting-orthographic-projection-reasoning-for-cad-via-rei.html" onclick="toggleFavorite(this, '2506.00568v2', 'CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>5</td>
  <td><a href="./papers/250603191v1-multimodal-generative-ai-with-autoregressive-llms-for-human-motion-u.html">Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward</a></td>
  <td>æå‡ºå¤šæ¨¡æ€ç”ŸæˆAIä¸è‡ªå›å½’LLMä»¥æå‡äººç±»åŠ¨ä½œç†è§£ä¸ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion synthesis</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03191v1" data-paper-url="./papers/250603191v1-multimodal-generative-ai-with-autoregressive-llms-for-human-motion-u.html" onclick="toggleFavorite(this, '2506.03191v1', 'Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250600599v2-xyz-ibd-a-high-precision-bin-picking-dataset-for-object-6d-pose-esti.html">XYZ-IBD: A High-precision Bin-picking Dataset for Object 6D Pose Estimation Capturing Real-world Industrial Complexity</a></td>
  <td>æå‡ºXYZ-IBDæ•°æ®é›†ä»¥è§£å†³å·¥ä¸šç¯å¢ƒä¸­çš„6Då§¿æ€ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">6D pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00599v2" data-paper-url="./papers/250600599v2-xyz-ibd-a-high-precision-bin-picking-dataset-for-object-6d-pose-esti.html" onclick="toggleFavorite(this, '2506.00599v2', 'XYZ-IBD: A High-precision Bin-picking Dataset for Object 6D Pose Estimation Capturing Real-world Industrial Complexity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250600562v1-seed-a-benchmark-dataset-for-sequential-facial-attribute-editing-wit.html">SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with Diffusion Models</a></td>
  <td>æå‡ºSEEDæ•°æ®é›†ä»¥è§£å†³é¡ºåºé¢éƒ¨å±æ€§ç¼–è¾‘çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00562v1" data-paper-url="./papers/250600562v1-seed-a-benchmark-dataset-for-sequential-facial-attribute-editing-wit.html" onclick="toggleFavorite(this, '2506.00562v1', 'SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250600318v1-chain-of-frames-advancing-video-understanding-in-multimodal-llms-via.html">Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning</a></td>
  <td>æå‡ºåŸºäºå¸§æ„ŸçŸ¥æ¨ç†çš„è§†é¢‘ç†è§£æ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€LLMsæ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00318v1" data-paper-url="./papers/250600318v1-chain-of-frames-advancing-video-understanding-in-multimodal-llms-via.html" onclick="toggleFavorite(this, '2506.00318v1', 'Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250603194v4-huemanity-probing-fine-grained-visual-perception-in-mllms.html">HueManity: Probing Fine-Grained Visual Perception in MLLMs</a></td>
  <td>æå‡ºHueManityåŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.03194v4" data-paper-url="./papers/250603194v4-huemanity-probing-fine-grained-visual-perception-in-mllms.html" onclick="toggleFavorite(this, '2506.03194v4', 'HueManity: Probing Fine-Grained Visual Perception in MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250600721v1-common-inpainted-objects-in-n-out-of-context.html">Common Inpainted Objects In-N-Out of Context</a></td>
  <td>æå‡ºCOinCOæ•°æ®é›†ä»¥è§£å†³è§†è§‰æ•°æ®é›†ä¸­ç¼ºä¹ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00721v1" data-paper-url="./papers/250600721v1-common-inpainted-objects-in-n-out-of-context.html" onclick="toggleFavorite(this, '2506.00721v1', 'Common Inpainted Objects In-N-Out of Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250600324v1-improving-optical-flow-and-stereo-depth-estimation-by-leveraging-unc.html">Improving Optical Flow and Stereo Depth Estimation by Leveraging Uncertainty-Based Learning Difficulties</a></td>
  <td>æå‡ºåŸºäºä¸ç¡®å®šæ€§å­¦ä¹ çš„å…‰æµä¸ç«‹ä½“æ·±åº¦ä¼°è®¡æ”¹è¿›æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">stereo depth</span> <span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00324v1" data-paper-url="./papers/250600324v1-improving-optical-flow-and-stereo-depth-estimation-by-leveraging-unc.html" onclick="toggleFavorite(this, '2506.00324v1', 'Improving Optical Flow and Stereo Depth Estimation by Leveraging Uncertainty-Based Learning Difficulties')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250600333v1-test-time-vocabulary-adaptation-for-language-driven-object-detection.html">Test-time Vocabulary Adaptation for Language-driven Object Detection</a></td>
  <td>æå‡ºVocAdaä»¥è§£å†³å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹ä¸­çš„è¯æ±‡é€‚åº”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00333v1" data-paper-url="./papers/250600333v1-test-time-vocabulary-adaptation-for-language-driven-object-detection.html" onclick="toggleFavorite(this, '2506.00333v1', 'Test-time Vocabulary Adaptation for Language-driven Object Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250600394v1-sequence-based-identification-of-first-person-camera-wearers-in-thir.html">Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views</a></td>
  <td>æå‡ºTF2025æ•°æ®é›†ä¸åºåˆ—è¯†åˆ«æ–¹æ³•ä»¥è§£å†³å¤šæ‘„åƒå¤´äº¤äº’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span> <span class="paper-tag">Ego4D</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00394v1" data-paper-url="./papers/250600394v1-sequence-based-identification-of-first-person-camera-wearers-in-thir.html" onclick="toggleFavorite(this, '2506.00394v1', 'Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250600607v1-parallel-rescaling-rebalancing-consistency-guidance-for-personalized.html">Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models</a></td>
  <td>æå‡ºå¹¶è¡Œé‡æ ‡å®šæŠ€æœ¯ä»¥è§£å†³ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00607v1" data-paper-url="./papers/250600607v1-parallel-rescaling-rebalancing-consistency-guidance-for-personalized.html" onclick="toggleFavorite(this, '2506.00607v1', 'Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250600578v1-event-based-multi-view-photogrammetry-for-high-dynamic-high-velocity.html">Event-based multi-view photogrammetry for high-dynamic, high-velocity target measurement</a></td>
  <td>æå‡ºåŸºäºäº‹ä»¶çš„å¤šè§†è§’æ‘„å½±æµ‹é‡æ–¹æ³•ä»¥è§£å†³é«˜é€ŸåŠ¨æ€ç›®æ ‡æµ‹é‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00578v1" data-paper-url="./papers/250600578v1-event-based-multi-view-photogrammetry-for-high-dynamic-high-velocity.html" onclick="toggleFavorite(this, '2506.00578v1', 'Event-based multi-view photogrammetry for high-dynamic, high-velocity target measurement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)