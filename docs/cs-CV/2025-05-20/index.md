---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-20
---

# cs.CVï¼ˆ2025-05-20ï¼‰

ğŸ“Š å…± **41** ç¯‡è®ºæ–‡
 | ğŸ”— **13** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (16 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ğŸ”—5)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (16 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250514231v1-univg-r1-reasoning-guided-universal-visual-grounding-with-reinforcem.html">UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning</a></td>
  <td>æå‡ºUniVG-R1ä»¥è§£å†³å¤æ‚å¤šæ¨¡æ€è§†è§‰å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14231v1" data-paper-url="./papers/250514231v1-univg-r1-reasoning-guided-universal-visual-grounding-with-reinforcem.html" onclick="toggleFavorite(this, '2505.14231v1', 'UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250514682v1-unigen-enhanced-training-test-time-strategies-for-unified-multimodal.html">UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation</a></td>
  <td>æå‡ºUniGenä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">direct preference optimization</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14682v1" data-paper-url="./papers/250514682v1-unigen-enhanced-training-test-time-strategies-for-unified-multimodal.html" onclick="toggleFavorite(this, '2505.14682v1', 'UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250514677v3-visionary-r1-mitigating-shortcuts-in-visual-reasoning-with-reinforce.html">Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning</a></td>
  <td>æå‡ºVisionary-R1ä»¥è§£å†³è§†è§‰æ¨ç†ä¸­çš„å¿«æ·å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14677v3" data-paper-url="./papers/250514677v3-visionary-r1-mitigating-shortcuts-in-visual-reasoning-with-reinforce.html" onclick="toggleFavorite(this, '2505.14677v3', 'Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250514948v1-programmatic-video-prediction-using-large-language-models.html">Programmatic Video Prediction Using Large Language Models</a></td>
  <td>æå‡ºProgGenä»¥è§£å†³è§†é¢‘å¸§é¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14948v1" data-paper-url="./papers/250514948v1-programmatic-video-prediction-using-large-language-models.html" onclick="toggleFavorite(this, '2505.14948v1', 'Programmatic Video Prediction Using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250514405v1-investigating-and-enhancing-the-robustness-of-large-multimodal-model.html">Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency</a></td>
  <td>æå‡ºTemRobBenchä¸PanoDPOä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹çš„æ—¶é—´ä¸€è‡´æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">direct preference optimization</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14405v1" data-paper-url="./papers/250514405v1-investigating-and-enhancing-the-robustness-of-large-multimodal-model.html" onclick="toggleFavorite(this, '2505.14405v1', 'Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250514460v2-visualquality-r1-reasoning-induced-image-quality-assessment-via-rein.html">VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank</a></td>
  <td>æå‡ºVisualQuality-R1ä»¥è§£å†³å›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14460v2" data-paper-url="./papers/250514460v2-visualquality-r1-reasoning-induced-image-quality-assessment-via-rein.html" onclick="toggleFavorite(this, '2505.14460v2', 'VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250514362v2-deepeyes-incentivizing-thinking-with-images-via-reinforcement-learni.html">DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning</a></td>
  <td>æå‡ºDeepEyesä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„è§†è§‰ä¸æ–‡æœ¬æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14362v2" data-paper-url="./papers/250514362v2-deepeyes-incentivizing-thinking-with-images-via-reinforcement-learni.html" onclick="toggleFavorite(this, '2505.14362v2', 'DeepEyes: Incentivizing &quot;Thinking with Images&quot; via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250514197v1-towards-omnidirectional-reasoning-with-360-r1-a-dataset-benchmark-an.html">Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method</a></td>
  <td>æå‡ºOmniVQAæ•°æ®é›†ä¸360-R1æ–¹æ³•ä»¥è§£å†³å…¨æ™¯è§†è§‰é—®ç­”é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14197v1" data-paper-url="./papers/250514197v1-towards-omnidirectional-reasoning-with-360-r1-a-dataset-benchmark-an.html" onclick="toggleFavorite(this, '2505.14197v1', 'Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250513997v2-stpr-spatiotemporal-preservation-and-routing-for-exemplar-free-video.html">StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning</a></td>
  <td>æå‡ºStPRæ¡†æ¶ä»¥è§£å†³è§†é¢‘ç±»å¢é‡å­¦ä¹ ä¸­çš„é—å¿˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13997v2" data-paper-url="./papers/250513997v2-stpr-spatiotemporal-preservation-and-routing-for-exemplar-free-video.html" onclick="toggleFavorite(this, '2505.13997v2', 'StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250514124v1-intra-class-patch-swap-for-self-distillation.html">Intra-class Patch Swap for Self-Distillation</a></td>
  <td>æå‡ºåŸºäºç±»å†…è¡¥ä¸äº¤æ¢çš„è‡ªè’¸é¦æ–¹æ³•ä»¥ç®€åŒ–çŸ¥è¯†è’¸é¦</td>
  <td class="tags-cell"><span class="paper-tag">teacher-student</span> <span class="paper-tag">distillation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14124v1" data-paper-url="./papers/250514124v1-intra-class-patch-swap-for-self-distillation.html" onclick="toggleFavorite(this, '2505.14124v1', 'Intra-class Patch Swap for Self-Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250514951v1-multimae-meets-earth-observation-pre-training-multi-modal-multi-task.html">MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation Tasks</a></td>
  <td>æå‡ºMultiMAEä»¥è§£å†³å¤šæ¨¡æ€åœ°çƒè§‚æµ‹ä»»åŠ¡çš„é¢„è®­ç»ƒé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">masked autoencoder</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14951v1" data-paper-url="./papers/250514951v1-multimae-meets-earth-observation-pre-training-multi-modal-multi-task.html" onclick="toggleFavorite(this, '2505.14951v1', 'MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250514319v2-retro-rethinking-tactile-representation-learning-with-material-prior.html">RETRO: REthinking Tactile Representation Learning with Material PriOrs</a></td>
  <td>æå‡ºææ–™å…ˆéªŒä»¥æå‡è§¦è§‰è¡¨å¾å­¦ä¹ çš„å‡†ç¡®æ€§</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14319v2" data-paper-url="./papers/250514319v2-retro-rethinking-tactile-representation-learning-with-material-prior.html" onclick="toggleFavorite(this, '2505.14319v2', 'RETRO: REthinking Tactile Representation Learning with Material PriOrs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250514156v1-unify-graph-learning-with-text-unleashing-llm-potentials-for-session.html">Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search</a></td>
  <td>æå‡ºç¬¦å·å›¾æ’åºå™¨ä»¥è§£å†³ä¼šè¯æœç´¢ä¸­çš„ä¿¡æ¯ç»“æ„å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14156v1" data-paper-url="./papers/250514156v1-unify-graph-learning-with-text-unleashing-llm-potentials-for-session.html" onclick="toggleFavorite(this, '2505.14156v1', 'Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250514062v2-scaling-vision-mamba-across-resolutions-via-fractal-traversal.html">Scaling Vision Mamba Across Resolutions via Fractal Traversal</a></td>
  <td>æå‡ºFractalMamba++ä»¥è§£å†³è§†è§‰è¾“å…¥åˆ†è¾¨ç‡é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14062v2" data-paper-url="./papers/250514062v2-scaling-vision-mamba-across-resolutions-via-fractal-traversal.html" onclick="toggleFavorite(this, '2505.14062v2', 'Scaling Vision Mamba Across Resolutions via Fractal Traversal')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250513812v2-physics-driven-local-whole-elastic-deformation-modeling-for-point-cl.html">Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning</a></td>
  <td>æå‡ºç‰©ç†é©±åŠ¨çš„å±€éƒ¨-æ•´ä½“å¼¹æ€§å˜å½¢å»ºæ¨¡ä»¥æå‡ç‚¹äº‘è¡¨ç¤ºå­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13812v2" data-paper-url="./papers/250513812v2-physics-driven-local-whole-elastic-deformation-modeling-for-point-cl.html" onclick="toggleFavorite(this, '2505.13812v2', 'Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250513788v1-ground-v-teaching-vlms-to-ground-complex-instructions-in-pixels.html">Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels</a></td>
  <td>æå‡ºGround-Vä»¥è§£å†³å¤æ‚æŒ‡ä»¤çš„åƒç´ çº§å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13788v1" data-paper-url="./papers/250513788v1-ground-v-teaching-vlms-to-ground-complex-instructions-in-pixels.html" onclick="toggleFavorite(this, '2505.13788v1', 'Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250514260v1-speculative-decoding-reimagined-for-multimodal-large-language-models.html">Speculative Decoding Reimagined for Multimodal Large Language Models</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ¨æµ‹è§£ç ä»¥åŠ é€Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14260v1" data-paper-url="./papers/250514260v1-speculative-decoding-reimagined-for-multimodal-large-language-models.html" onclick="toggleFavorite(this, '2505.14260v1', 'Speculative Decoding Reimagined for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250517090v1-emosign-a-multimodal-dataset-for-understanding-emotions-in-american-.html">EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language</a></td>
  <td>æå‡ºEmoSignæ•°æ®é›†ä»¥è§£å†³æ‰‹è¯­æƒ…æ„Ÿç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.17090v1" data-paper-url="./papers/250517090v1-emosign-a-multimodal-dataset-for-understanding-emotions-in-american-.html" onclick="toggleFavorite(this, '2505.17090v1', 'EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250514462v1-ravenea-a-benchmark-for-multimodal-retrieval-augmented-visual-cultur.html">RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding</a></td>
  <td>æå‡ºRAVENEAä»¥è§£å†³å¤šæ¨¡æ€æ–‡åŒ–ç†è§£ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14462v1" data-paper-url="./papers/250514462v1-ravenea-a-benchmark-for-multimodal-retrieval-augmented-visual-cultur.html" onclick="toggleFavorite(this, '2505.14462v1', 'RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250514454v2-video-compression-commander-plug-and-play-inference-acceleration-for.html">Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models</a></td>
  <td>æå‡ºè§†é¢‘å‹ç¼©æŒ‡æŒ¥å®˜ä»¥è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14454v2" data-paper-url="./papers/250514454v2-video-compression-commander-plug-and-play-inference-acceleration-for.html" onclick="toggleFavorite(this, '2505.14454v2', 'Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250514404v2-vic-bench-benchmarking-visual-interleaved-chain-of-thought-capabilit.html">ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability in MLLMs with Free-Style Intermediate State Representations</a></td>
  <td>æå‡ºViC-Benchä»¥è§£å†³ç°æœ‰MLLMsè¯„ä¼°ä¸­IVSå›ºå®šé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14404v2" data-paper-url="./papers/250514404v2-vic-bench-benchmarking-visual-interleaved-chain-of-thought-capabilit.html" onclick="toggleFavorite(this, '2505.14404v2', 'ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability in MLLMs with Free-Style Intermediate State Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250513928v3-lovr-a-benchmark-for-long-video-retrieval-in-multimodal-contexts.html">LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts</a></td>
  <td>æå‡ºLoVRåŸºå‡†ä»¥è§£å†³é•¿è§†é¢‘æ£€ç´¢ä¸­çš„å¤šæ¨¡æ€æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13928v3" data-paper-url="./papers/250513928v3-lovr-a-benchmark-for-long-video-retrieval-in-multimodal-contexts.html" onclick="toggleFavorite(this, '2505.13928v3', 'LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250514336v2-scaling-and-enhancing-llm-based-avsr-a-sparse-mixture-of-projectors-.html">Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach</a></td>
  <td>æå‡ºLlama-SMoPä»¥è§£å†³èµ„æºå—é™ç¯å¢ƒä¸‹çš„AVSRé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14336v2" data-paper-url="./papers/250514336v2-scaling-and-enhancing-llm-based-avsr-a-sparse-mixture-of-projectors-.html" onclick="toggleFavorite(this, '2505.14336v2', 'Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250514318v2-radar-enhancing-radiology-report-generation-with-supplementary-knowl.html">RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection</a></td>
  <td>æå‡ºRADARæ¡†æ¶ä»¥è§£å†³æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„çŸ¥è¯†æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14318v2" data-paper-url="./papers/250514318v2-radar-enhancing-radiology-report-generation-with-supplementary-knowl.html" onclick="toggleFavorite(this, '2505.14318v2', 'RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250514640v1-videoeval-pro-robust-and-realistic-long-video-understanding-evaluati.html">VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation</a></td>
  <td>æå‡ºVideoEval-Proä»¥è§£å†³é•¿è§†é¢‘ç†è§£è¯„ä¼°çš„æœ‰æ•ˆæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14640v1" data-paper-url="./papers/250514640v1-videoeval-pro-robust-and-realistic-long-video-understanding-evaluati.html" onclick="toggleFavorite(this, '2505.14640v1', 'VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250514100v2-unlocking-the-power-of-sam-2-for-few-shot-segmentation.html">Unlocking the Power of SAM 2 for Few-Shot Segmentation</a></td>
  <td>æå‡ºä¼ªæç¤ºç”Ÿæˆå™¨ä¸è¿­ä»£è®°å¿†ç²¾ç‚¼ä»¥è§£å†³å°‘æ ·æœ¬åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14100v2" data-paper-url="./papers/250514100v2-unlocking-the-power-of-sam-2-for-few-shot-segmentation.html" onclick="toggleFavorite(this, '2505.14100v2', 'Unlocking the Power of SAM 2 for Few-Shot Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250514059v1-dolphin-document-image-parsing-via-heterogeneous-anchor-prompting.html">Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</a></td>
  <td>æå‡ºDolphinä»¥è§£å†³æ–‡æ¡£å›¾åƒè§£æä¸­çš„å¤æ‚å…ƒç´ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14059v1" data-paper-url="./papers/250514059v1-dolphin-document-image-parsing-via-heterogeneous-anchor-prompting.html" onclick="toggleFavorite(this, '2505.14059v1', 'Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250514029v1-applegrowthvision-a-large-scale-stereo-dataset-for-phenological-anal.html">AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards</a></td>
  <td>æå‡ºAppleGrowthVisionä»¥è§£å†³è‹¹æœå›­ç›‘æµ‹æ•°æ®é›†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14029v1" data-paper-url="./papers/250514029v1-applegrowthvision-a-large-scale-stereo-dataset-for-phenological-anal.html" onclick="toggleFavorite(this, '2505.14029v1', 'AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250513839v1-mgstream-motion-aware-3d-gaussian-for-streamable-dynamic-scene-recon.html">MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction</a></td>
  <td>æå‡ºMGStreamä»¥è§£å†³åŠ¨æ€åœºæ™¯é‡å»ºä¸­çš„é—ªçƒå’Œå­˜å‚¨æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13839v1" data-paper-url="./papers/250513839v1-mgstream-motion-aware-3d-gaussian-for-streamable-dynamic-scene-recon.html" onclick="toggleFavorite(this, '2505.13839v1', 'MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250514159v2-m3depth-wavelet-enhanced-depth-estimation-on-mars-via-mutual-boostin.html">M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting of Dual-Modal Data</a></td>
  <td>æå‡ºM3Depthä»¥è§£å†³ç«æ˜Ÿç¯å¢ƒä¸‹æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">stereo depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14159v2" data-paper-url="./papers/250514159v2-m3depth-wavelet-enhanced-depth-estimation-on-mars-via-mutual-boostin.html" onclick="toggleFavorite(this, '2505.14159v2', 'M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting of Dual-Modal Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/250514537v2-personalize-your-gaussian-consistent-3d-scene-personalization-from-a.html">Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image</a></td>
  <td>æå‡ºCP-GSæ¡†æ¶ä»¥è§£å†³å•å›¾åƒ3Dåœºæ™¯ä¸ªæ€§åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14537v2" data-paper-url="./papers/250514537v2-personalize-your-gaussian-consistent-3d-scene-personalization-from-a.html" onclick="toggleFavorite(this, '2505.14537v2', 'Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250514008v1-multi-label-stereo-matching-for-transparent-scene-depth-estimation.html">Multi-Label Stereo Matching for Transparent Scene Depth Estimation</a></td>
  <td>æå‡ºå¤šæ ‡ç­¾ç«‹ä½“åŒ¹é…æ–¹æ³•ä»¥è§£å†³é€æ˜åœºæ™¯æ·±åº¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14008v1" data-paper-url="./papers/250514008v1-multi-label-stereo-matching-for-transparent-scene-depth-estimation.html" onclick="toggleFavorite(this, '2505.14008v1', 'Multi-Label Stereo Matching for Transparent Scene Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250514414v2-diving-into-the-fusion-of-monocular-priors-for-generalized-stereo-ma.html">Diving into the Fusion of Monocular Priors for Generalized Stereo Matching</a></td>
  <td>æå‡ºäºŒå…ƒå±€éƒ¨æ’åºå›¾ä»¥è§£å†³ç«‹ä½“åŒ¹é…ä¸­çš„å•ç›®å…ˆéªŒèåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">monocular depth</span> <span class="paper-tag">scene flow</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14414v2" data-paper-url="./papers/250514414v2-diving-into-the-fusion-of-monocular-priors-for-generalized-stereo-ma.html" onclick="toggleFavorite(this, '2505.14414v2', 'Diving into the Fusion of Monocular Priors for Generalized Stereo Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250513905v1-4d-rolls-4d-radar-occupancy-learning-via-lidar-supervision.html">4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision</a></td>
  <td>æå‡º4D-ROLLSä»¥è§£å†³4Dé›·è¾¾å ç”¨ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">height map</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.13905v1" data-paper-url="./papers/250513905v1-4d-rolls-4d-radar-occupancy-learning-via-lidar-supervision.html" onclick="toggleFavorite(this, '2505.13905v1', '4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>35</td>
  <td><a href="./papers/250514683v3-emerging-properties-in-unified-multimodal-pretraining.html">Emerging Properties in Unified Multimodal Pretraining</a></td>
  <td>æå‡ºBAGELæ¨¡å‹ä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14683v3" data-paper-url="./papers/250514683v3-emerging-properties-in-unified-multimodal-pretraining.html" onclick="toggleFavorite(this, '2505.14683v3', 'Emerging Properties in Unified Multimodal Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250514357v2-vid2world-crafting-video-diffusion-models-to-interactive-world-model.html">Vid2World: Crafting Video Diffusion Models to Interactive World Models</a></td>
  <td>æå‡ºVid2Worldä»¥è§£å†³ç°æœ‰ä¸–ç•Œæ¨¡å‹ä½ä¿çœŸåº¦é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14357v2" data-paper-url="./papers/250514357v2-vid2world-crafting-video-diffusion-models-to-interactive-world-model.html" onclick="toggleFavorite(this, '2505.14357v2', 'Vid2World: Crafting Video Diffusion Models to Interactive World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250514246v1-visual-agentic-reinforcement-fine-tuning.html">Visual Agentic Reinforcement Fine-Tuning</a></td>
  <td>æå‡ºè§†è§‰ä»£ç†å¼ºåŒ–å¾®è°ƒæ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14246v1" data-paper-url="./papers/250514246v1-visual-agentic-reinforcement-fine-tuning.html" onclick="toggleFavorite(this, '2505.14246v1', 'Visual Agentic Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>38</td>
  <td><a href="./papers/250514654v1-beyond-words-multimodal-llm-knows-when-to-speak.html">Beyond Words: Multimodal LLM Knows When to Speak</a></td>
  <td>æå‡ºMM-When2Speakä»¥è§£å†³å¯¹è¯ä¸­ååº”æ—¶æœºé¢„æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dyadic interaction</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14654v1" data-paper-url="./papers/250514654v1-beyond-words-multimodal-llm-knows-when-to-speak.html" onclick="toggleFavorite(this, '2505.14654v1', 'Beyond Words: Multimodal LLM Knows When to Speak')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/250514014v1-egformer-towards-efficient-and-generalizable-multimodal-semantic-seg.html">EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation</a></td>
  <td>æå‡ºEGFormerä»¥è§£å†³å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²çš„æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MDM</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14014v1" data-paper-url="./papers/250514014v1-egformer-towards-efficient-and-generalizable-multimodal-semantic-seg.html" onclick="toggleFavorite(this, '2505.14014v1', 'EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>40</td>
  <td><a href="./papers/250514346v2-egocentric-action-aware-inertial-localization-in-point-clouds-with-v.html">Egocentric Action-aware Inertial Localization in Point Clouds with Vision-Language Guidance</a></td>
  <td>æå‡ºä»¥è‡ªæˆ‘ä¸­å¿ƒåŠ¨ä½œæ„ŸçŸ¥çš„æƒ¯æ€§å®šä½æ¡†æ¶è§£å†³3Dç‚¹äº‘ä¸­çš„å®šä½æ¼‚ç§»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14346v2" data-paper-url="./papers/250514346v2-egocentric-action-aware-inertial-localization-in-point-clouds-with-v.html" onclick="toggleFavorite(this, '2505.14346v2', 'Egocentric Action-aware Inertial Localization in Point Clouds with Vision-Language Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>41</td>
  <td><a href="./papers/250514556v1-dynadiff-single-stage-decoding-of-images-from-continuously-evolving-.html">Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI</a></td>
  <td>æå‡ºDynadiffä»¥è§£å†³åŠ¨æ€fMRIå›¾åƒè§£ç é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">diff-sim</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14556v1" data-paper-url="./papers/250514556v1-dynadiff-single-stage-decoding-of-images-from-continuously-evolving-.html" onclick="toggleFavorite(this, '2505.14556v1', 'Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)