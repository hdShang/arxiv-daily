---
layout: default
title: "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation"
---

# VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14640" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14640v1</a>
  <a href="https://arxiv.org/pdf/2505.14640.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14640v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14640v1', 'VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wentao Ma, Weiming Ren, Yiming Jia, Zhuofeng Li, Ping Nie, Ge Zhang, Wenhu Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: Dataset: https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro, Project Webpage: https://tiger-ai-lab.github.io/VideoEval-Pro

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideoEval-Proä»¥è§£å†³é•¿è§†é¢‘ç†è§£è¯„ä¼°çš„æœ‰æ•ˆæ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `å¤šæ¨¡æ€æ¨¡å‹` `è¯„ä¼°åŸºå‡†` `å¼€æ”¾å¼é—®é¢˜` `è§†é¢‘åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿è§†é¢‘ç†è§£è¯„ä¼°åŸºå‡†ä¾èµ–å¤šé¡¹é€‰æ‹©é¢˜ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœå¯èƒ½è¢«çŒœæµ‹å½±å“ï¼Œç¼ºä¹æœ‰æ•ˆæ€§ã€‚
2. æœ¬æ–‡æå‡ºVideoEval-ProåŸºå‡†ï¼Œé‡‡ç”¨å¼€æ”¾å¼çŸ­ç­”æ¡ˆé—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹çœŸæ­£ç†è§£è§†é¢‘å†…å®¹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè§†é¢‘LMMsåœ¨å¼€æ”¾å¼é—®é¢˜ä¸Šçš„è¡¨ç°è¾ƒMCQsæ˜¾è‘—ä¸‹é™ï¼Œä¸”å¢åŠ è¾“å…¥å¸§æ•°å¯¹è¯„ä¼°æ•ˆæœæœ‰ç§¯æå½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†ç°æœ‰è¯„ä¼°åŸºå‡†å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚é¦–å…ˆï¼Œè®¸å¤šåŸºå‡†ä¾èµ–å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œå…¶ç»“æœå¯èƒ½å› çŒœæµ‹è€Œè¢«å¤¸å¤§ï¼›å…¶æ¬¡ï¼Œéƒ¨åˆ†é—®é¢˜å…·æœ‰å¼ºå…ˆéªŒï¼Œä½¿å¾—æ¨¡å‹æ— éœ€è§‚çœ‹è§†é¢‘å³å¯å›ç­”ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºVideoEval-Proï¼Œä¸€ä¸ªåŒ…å«å¼€æ”¾å¼çŸ­ç­”æ¡ˆçš„é—®é¢˜åŸºå‡†ï¼ŒçœŸæ­£è¦æ±‚ç†è§£æ•´ä¸ªè§†é¢‘ã€‚é€šè¿‡è¯„ä¼°21ä¸ªè§†é¢‘LMMsï¼Œå‘ç°å¼€æ”¾å¼é—®é¢˜çš„è¡¨ç°è¾ƒMCQsä¸‹é™è¶…è¿‡25%ï¼Œä¸”MCQså¾—åˆ†é«˜å¹¶ä¸æ„å‘³ç€å¼€æ”¾å¼å¾—åˆ†é«˜ã€‚VideoEval-Proæä¾›äº†æ›´å¯é çš„é•¿è§†é¢‘ç†è§£è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰é•¿è§†é¢‘ç†è§£è¯„ä¼°åŸºå‡†çš„æœ‰æ•ˆæ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å› ä¾èµ–å¤šé¡¹é€‰æ‹©é¢˜è€Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¯é ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºVideoEval-ProåŸºå‡†ï¼Œé‡‡ç”¨å¼€æ”¾å¼çŸ­ç­”æ¡ˆé—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹å…¨é¢ç†è§£è§†é¢‘å†…å®¹ï¼Œé¿å…ç®€å•çŒœæµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVideoEval-Proè¯„ä¼°åŒ…æ‹¬æ®µè½çº§å’Œå…¨è§†é¢‘ç†è§£ï¼Œç»“åˆæ„ŸçŸ¥å’Œæ¨ç†ä»»åŠ¡ï¼Œç¡®ä¿æ¨¡å‹å¯¹è§†é¢‘çš„å…¨é¢ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å¼€æ”¾å¼é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°çš„çœŸå®æ€§å’Œå¯é æ€§ï¼Œä¸ä¼ ç»Ÿçš„å¤šé¡¹é€‰æ‹©é¢˜è¯„ä¼°æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé—®é¢˜è®¾ç½®å¼ºè°ƒå¯¹è§†é¢‘å†…å®¹çš„æ·±åº¦ç†è§£ï¼Œè¯„ä¼°æ¡†æ¶æ¶µç›–å¤šç§ä»»åŠ¡ç±»å‹ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°å‡å¾—åˆ°è€ƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè§†é¢‘LMMsåœ¨å¼€æ”¾å¼é—®é¢˜ä¸Šçš„è¡¨ç°è¾ƒå¤šé¡¹é€‰æ‹©é¢˜ä¸‹é™è¶…è¿‡25%ã€‚æ­¤å¤–ï¼ŒVideoEval-Proçš„è®¾è®¡ä½¿å¾—å¢åŠ è¾“å…¥å¸§æ•°å¯¹è¯„ä¼°æ•ˆæœçš„æå‡æ›´ä¸ºæ˜¾è‘—ï¼Œæä¾›äº†æ›´çœŸå®çš„é•¿è§†é¢‘ç†è§£è¯„ä¼°æ ‡å‡†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘å†…å®¹åˆ†æã€æ™ºèƒ½ç›‘æ§ã€æ•™è‚²è§†é¢‘è¯„ä¼°ç­‰ã€‚é€šè¿‡æä¾›æ›´å¯é çš„è¯„ä¼°åŸºå‡†ï¼ŒVideoEval-Proèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶è€…å’Œå¼€å‘è€…æ›´å¥½åœ°ç†è§£å’Œæ”¹è¿›é•¿è§†é¢‘ç†è§£æ¨¡å‹ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50\% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance ($>$25\%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain.

