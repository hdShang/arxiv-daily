---
layout: default
title: "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning"
---

# UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14231" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.14231v1</a>
  <a href="https://arxiv.org/pdf/2505.14231.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14231v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14231v1', 'UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, Yansong Tang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-20

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://amap-ml.github.io/UniVG-R1-page/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫UniVG-R1‰ª•Ëß£ÂÜ≥Â§çÊùÇÂ§öÊ®°ÊÄÅËßÜËßâÂÆö‰ΩçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅËßÜËßâÂÆö‰Ωç` `Êé®ÁêÜÂºïÂØº` `Âº∫ÂåñÂ≠¶‰π†` `ÊÄùÁª¥ÈìæÊï∞ÊçÆÈõÜ` `Èõ∂Ê†∑Êú¨Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâÂÆö‰ΩçÊñπÊ≥ï‰∏ªË¶ÅÈíàÂØπÂçï‰∏ÄÂõæÂÉèÂíåÁÆÄÂçïÊñáÊú¨ÔºåÈöæ‰ª•Â∫îÂØπÂ§çÊùÇÁöÑÂ§öÊ®°ÊÄÅÊåá‰ª§ÂíåÂ§öÂõæÂÉèÂú∫ÊôØ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫UniVG-R1ÔºåÈÄöËøáÊé®ÁêÜÂºïÂØºÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåUniVG-R1Âú®MIG-Bench‰∏äÂÆûÁé∞‰∫Ü9.1%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜ‰∏äÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰º†ÁªüÁöÑËßÜËßâÂÆö‰ΩçÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂçïÂõæÂÉèÂú∫ÊôØÂíåÁÆÄÂçïÊñáÊú¨ÂºïÁî®‰∏ä„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜËøô‰∫õÊñπÊ≥ïÊâ©Â±ïÂà∞Ê∂âÂèäÈöêÂê´ÂíåÂ§çÊùÇÊåá‰ª§ÁöÑÁúüÂÆûÂú∫ÊôØÔºåÂ∞§ÂÖ∂ÊòØÂ§öÂõæÂÉèÁöÑÊÉÖÂÜµ‰∏ãÔºåÈù¢‰∏¥ÁùÄÈáçÂ§ßÊåëÊàòÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÁº∫‰πèÂú®Â§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñá‰∏≠ËøõË°åÈ´òÁ∫ßÊé®ÁêÜÁöÑËÉΩÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜUniVG-R1ÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÊé®ÁêÜÂºïÂØºÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÈÄöËøáÁªìÂêàÂÜ∑ÂêØÂä®Êï∞ÊçÆÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊù•Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨È¶ñÂÖàÊûÑÂª∫‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÂÆö‰ΩçÊï∞ÊçÆÈõÜÔºåÂπ∂ÈÄöËøáÁõëÁù£ÂæÆË∞ÉÂºïÂØºÊ®°ÂûãËµ∞ÂêëÊ≠£Á°ÆÁöÑÊé®ÁêÜË∑ØÂæÑ„ÄÇÈöèÂêéÔºåÊàë‰ª¨ÈááÁî®Âü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†Êù•ÈºìÂä±Ê®°ÂûãËØÜÂà´Ê≠£Á°ÆÁöÑÊé®ÁêÜÈìæÔºå‰ªéËÄåÊøÄÂä±ÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUniVG-R1Âú®MIG-Bench‰∏äÂÆûÁé∞‰∫Ü9.1%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂Âú®Âõõ‰∏™ÂõæÂÉèÂíåËßÜÈ¢ëÊé®ÁêÜÂÆö‰ΩçÂü∫ÂáÜ‰∏äÂÆûÁé∞‰∫Ü23.4%ÁöÑÈõ∂Ê†∑Êú¨ÊÄßËÉΩÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§çÊùÇÂ§öÊ®°ÊÄÅËßÜËßâÂÆö‰Ωç‰ªªÂä°‰∏≠Êé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§öÂõæÂÉèÂíåÈöêÂê´Êåá‰ª§Êó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÁº∫‰πèÊúâÊïàÁöÑÊé®ÁêÜÊú∫Âà∂„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöUniVG-R1ÈÄöËøáÊûÑÂª∫È´òË¥®ÈáèÁöÑÊÄùÁª¥ÈìæÊï∞ÊçÆÈõÜÔºåÂπ∂ÁªìÂêàÂº∫ÂåñÂ≠¶‰π†Êù•ÂºïÂØºÊ®°ÂûãÂ≠¶‰π†Ê≠£Á°ÆÁöÑÊé®ÁêÜË∑ØÂæÑÔºå‰ªéËÄåÊèêÂçáÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÂåÖÊã¨Êï∞ÊçÆÈõÜÊûÑÂª∫„ÄÅÁõëÁù£ÂæÆË∞ÉÂíåÂü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÈ¶ñÂÖàÔºåÊûÑÂª∫ÊÄùÁª¥ÈìæÊï∞ÊçÆÈõÜ‰ª•ÊåáÂØºÊ®°ÂûãÂ≠¶‰π†ÔºõÁÑ∂ÂêéÔºåÈÄöËøáÂæÆË∞ÉÂ¢ûÂº∫Ê®°ÂûãÁöÑÂàùÊ≠•Êé®ÁêÜËÉΩÂäõÔºõÊúÄÂêéÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Ëøõ‰∏ÄÊ≠•‰ºòÂåñÊé®ÁêÜÈìæÁöÑÈÄâÊã©„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÊé®ÁêÜÂºïÂØºÁöÑÂº∫ÂåñÂ≠¶‰π†Êú∫Âà∂ÔºåÂπ∂ÊèêÂá∫‰∫ÜÈöæÂ∫¶ÊÑüÁü•ÁöÑÊùÉÈáçË∞ÉÊï¥Á≠ñÁï•Ôºå‰ª•Â∫îÂØπËÆ≠ÁªÉËøáÁ®ã‰∏≠Ê†∑Êú¨ÈöæÂ∫¶ÂÅèÂ∑ÆÁöÑÈóÆÈ¢ò„ÄÇËøô‰∏ÄËÆæËÆ°‰ΩøÂæóÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞Êõ¥‰∏∫Âá∫Ëâ≤„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆ≠ÁªÉ‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•Âπ≥Ë°°Êé®ÁêÜÈìæÁöÑÈÄâÊã©ÔºåÂπ∂ËÆæÁΩÆ‰∫ÜÈÄÇÂΩìÁöÑË∂ÖÂèÇÊï∞‰ª•‰ºòÂåñÂ≠¶‰π†ËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåÈöæÂ∫¶ÊÑüÁü•ÊùÉÈáçË∞ÉÊï¥Á≠ñÁï•ÁöÑÂºïÂÖ•ÔºåÁ°Æ‰øù‰∫ÜÊ®°ÂûãÂú®‰∏çÂêåÈöæÂ∫¶Ê†∑Êú¨‰∏äÁöÑÂ≠¶‰π†ÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUniVG-R1Âú®MIG-Bench‰∏äÂÆûÁé∞‰∫Ü9.1%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºåÂú®Âõõ‰∏™ÂõæÂÉèÂíåËßÜÈ¢ëÊé®ÁêÜÂÆö‰ΩçÂü∫ÂáÜ‰∏äÂπ≥ÂùáÊèêÂçá23.4%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéËØ•Ê®°ÂûãÂú®Â§çÊùÇÂ§öÊ®°ÊÄÅËßÜËßâÂÆö‰Ωç‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈúÄË¶ÅÁêÜËß£Â§çÊùÇÊåá‰ª§ÂíåÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÂçáËßÜËßâÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜËÉΩÂäõÔºåUniVG-R1ÂèØ‰ª•Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÊòæËëóÊèêÈ´òÁ≥ªÁªüÁöÑÊô∫ËÉΩÊ∞¥Âπ≥ÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØËÉΩÊé®Âä®Êõ¥ÂπøÊ≥õÁöÑÂ§öÊ®°ÊÄÅ‰∫§‰∫íÂíåÊô∫ËÉΩÂÜ≥Á≠ñÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, a reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct a high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised fine-tuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify a difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose a difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.

