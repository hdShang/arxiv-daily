---
layout: default
title: Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models
---

# Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14454" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14454v2</a>
  <a href="https://arxiv.org/pdf/2505.14454.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14454v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14454v2', 'Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-18)

**å¤‡æ³¨**: EMNLP 2025 main

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/xuyang-liu16/VidCom2)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†é¢‘å‹ç¼©æŒ‡æŒ¥å®˜ä»¥è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `æ ‡è®°å‹ç¼©` `æ¨ç†åŠ é€Ÿ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤§é‡è§†è§‰æ ‡è®°æ—¶æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±å’Œå®ç°ä¸Šçš„ä¸å…¼å®¹æ€§ã€‚
2. è®ºæ–‡æå‡ºäº†è§†é¢‘å‹ç¼©æŒ‡æŒ¥å®˜ï¼ˆVidCom2ï¼‰ï¼Œé€šè¿‡é‡åŒ–å¸§çš„ç‹¬ç‰¹æ€§ï¼Œè‡ªé€‚åº”è°ƒæ•´å‹ç¼©å¼ºåº¦ï¼Œè§£å†³ä¿¡æ¯å†—ä½™é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVidCom2åœ¨ä½¿ç”¨25%è§†è§‰æ ‡è®°çš„æƒ…å†µä¸‹ï¼Œä»èƒ½ä¿æŒ99.6%çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—é™ä½ç”Ÿæˆå»¶è¿Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºè§†è§‰æ ‡è®°çš„å¹³æ–¹å¤æ‚æ€§é¢ä¸´æ•ˆç‡æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¯¹VideoLLMçš„æ ‡è®°å‹ç¼©æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå‘ç°äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯å¿½è§†äº†å¸§é—´ç‹¬ç‰¹çš„è§†è§‰ä¿¡å·ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼›äºŒæ˜¯å—é™äºå®ç°çº¦æŸï¼Œä¸ç°ä»£æ¶æ„æˆ–é«˜æ•ˆæ“ä½œä¸å…¼å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æç‚¼äº†VideoLLMæ ‡è®°å‹ç¼©çš„ä¸‰é¡¹è®¾è®¡åŸåˆ™ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¯æ’æ‹”çš„æ¨ç†åŠ é€Ÿæ¡†æ¶â€œè§†é¢‘å‹ç¼©æŒ‡æŒ¥å®˜â€ï¼ˆVidCom2ï¼‰ã€‚é€šè¿‡é‡åŒ–æ¯å¸§çš„ç‹¬ç‰¹æ€§ï¼ŒVidCom2è‡ªé€‚åº”åœ°è°ƒæ•´å¸§é—´çš„å‹ç¼©å¼ºåº¦ï¼Œæœ‰æ•ˆä¿ç•™å…³é”®ä¿¡æ¯ï¼ŒåŒæ—¶å‡å°‘è§†é¢‘åºåˆ—ä¸­çš„å†—ä½™ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVidCom2åœ¨å¤šç§VideoLLMå’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨ä»…25%çš„è§†è§‰æ ‡è®°ï¼ŒVidCom2åœ¨LLaVA-OVä¸Šå®ç°äº†99.6%çš„åŸå§‹æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†70.8%çš„LLMç”Ÿæˆå»¶è¿Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„æ˜¯è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤§é‡è§†è§‰æ ‡è®°æ—¶çš„æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å‹ç¼©è¿‡ç¨‹ä¸­å®¹æ˜“å¿½è§†å¸§é—´çš„ç‹¬ç‰¹ä¿¡æ¯ï¼Œå¯¼è‡´ä¿¡æ¯æŸå¤±å’Œå®ç°ä¸Šçš„ä¸å…¼å®¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡é‡åŒ–æ¯å¸§çš„ç‹¬ç‰¹æ€§ï¼ŒåŠ¨æ€è°ƒæ•´å‹ç¼©å¼ºåº¦ï¼Œä»è€Œåœ¨ä¿ç•™å…³é”®ä¿¡æ¯çš„åŒæ—¶å‡å°‘å†—ä½™ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜è§†é¢‘ç†è§£çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šå¸§ç‹¬ç‰¹æ€§é‡åŒ–æ¨¡å—ã€å‹ç¼©å¼ºåº¦è°ƒæ•´æ¨¡å—å’Œæ¨ç†åŠ é€Ÿæ¨¡å—ã€‚é¦–å…ˆï¼Œé‡åŒ–æ¯å¸§çš„ç‹¬ç‰¹æ€§ï¼Œç„¶åæ ¹æ®é‡åŒ–ç»“æœè‡ªé€‚åº”è°ƒæ•´å‹ç¼©å¼ºåº¦ï¼Œæœ€åè¿›è¡Œé«˜æ•ˆæ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†å¸§å‹ç¼©è°ƒæ•´ç­–ç•¥ï¼Œä½¿å¾—VidCom2èƒ½å¤Ÿä¸å…¶ä»–æ ‡è®°å‹ç¼©æ–¹æ³•å…¼å®¹ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚è¿™ä¸€ç­–ç•¥åœ¨åŠ¨æ€è°ƒæ•´å‹ç¼©å¼ºåº¦æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå…³é”®å‚æ•°åŒ…æ‹¬å‹ç¼©å¼ºåº¦çš„è‡ªé€‚åº”è°ƒæ•´æœºåˆ¶ï¼Œä»¥åŠæŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼Œä»¥ç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­å°½å¯èƒ½ä¿ç•™é‡è¦ä¿¡æ¯ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒVidCom2é‡‡ç”¨äº†ä¸ç°ä»£æ¶æ„å…¼å®¹çš„è®¾è®¡ï¼Œç¡®ä¿é«˜æ•ˆçš„æ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVidCom2åœ¨LLaVA-OVä¸Šä½¿ç”¨ä»…25%çš„è§†è§‰æ ‡è®°ï¼Œä»èƒ½å®ç°99.6%çš„åŸå§‹æ€§èƒ½ï¼ŒåŒæ—¶å°†LLMç”Ÿæˆå»¶è¿Ÿé™ä½äº†70.8%ã€‚è¿™ä¸€æ˜¾è‘—æå‡å±•ç¤ºäº†VidCom2åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘åˆ†æã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è§†é¢‘å¤„ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œéšç€è§†é¢‘æ•°æ®é‡çš„æŒç»­å¢é•¿ï¼ŒVidCom2çš„æŠ€æœ¯å°†ä¸ºå®æ—¶è§†é¢‘ç†è§£å’Œå¤„ç†æä¾›é‡è¦æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video large language models (VideoLLM) excel at video understanding, but face efficiency challenges due to the quadratic complexity of abundant visual tokens. Our systematic analysis of token compression methods for VideoLLMs reveals two critical issues: (i) overlooking distinctive visual signals across frames, leading to information loss; (ii) suffering from implementation constraints, causing incompatibility with modern architectures or efficient operators. To address these challenges, we distill three design principles for VideoLLM token compression and propose a plug-and-play inference acceleration framework "Video Compression Commander" (VidCom2). By quantifying each frame's uniqueness, VidCom2 adaptively adjusts compression intensity across frames, effectively preserving essential information while reducing redundancy in video sequences. Extensive experiments across various VideoLLMs and benchmarks demonstrate the superior performance and efficiency of our VidCom2. With only 25% visual tokens, VidCom2 achieves 99.6% of the original performance on LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame Compression Adjustment strategy is compatible with other token compression methods to further improve their performance. Our code is available at https://github.com/xuyang-liu16/VidCom2.

