---
layout: default
title: LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts
---

# LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13928" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13928v3</a>
  <a href="https://arxiv.org/pdf/2505.13928.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13928v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13928v3', 'LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qifeng Cai, Hao Liang, Hejun Dong, Meiyi Qiang, Ruichuan An, Zhaoyang Han, Zhengzhou Zhu, Bin Cui, Wentao Zhang

**åˆ†ç±»**: cs.CV, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-13)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/TechNomad-ds/LoVR-benchmark)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoVRåŸºå‡†ä»¥è§£å†³é•¿è§†é¢‘æ£€ç´¢ä¸­çš„å¤šæ¨¡æ€æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `è§†é¢‘-æ–‡æœ¬æ£€ç´¢` `å¤šæ¨¡æ€å­¦ä¹ ` `å­—å¹•ç”Ÿæˆ` `æ•°æ®é›†æ„å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘-æ–‡æœ¬æ£€ç´¢åŸºå‡†åœ¨è§†é¢‘æ—¶é•¿ã€å­—å¹•è´¨é‡å’Œæ³¨é‡Šç²’åº¦ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œé™åˆ¶äº†è¯„ä¼°çš„å…¨é¢æ€§ã€‚
2. æœ¬æ–‡æå‡ºLoVRåŸºå‡†ï¼ŒåŒ…å«467ä¸ªé•¿è§†é¢‘å’Œé«˜è´¨é‡ç»†ç²’åº¦å­—å¹•ï¼Œæ—¨åœ¨æå‡è§†é¢‘-æ–‡æœ¬æ£€ç´¢çš„è¯„ä¼°æ ‡å‡†ã€‚
3. é€šè¿‡å¯¹å¤šç§å…ˆè¿›åµŒå…¥æ¨¡å‹çš„å®éªŒï¼ŒLoVRå±•ç¤ºäº†å½“å‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿è§†é¢‘åŒ…å«å¤§é‡ä¿¡æ¯ï¼Œä½¿å¾—è§†é¢‘-æ–‡æœ¬æ£€ç´¢æˆä¸ºå¤šæ¨¡æ€å­¦ä¹ ä¸­çš„ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†å­˜åœ¨è§†é¢‘æ—¶é•¿æœ‰é™ã€ä½è´¨é‡å­—å¹•å’Œç²—ç³™æ³¨é‡Šç²’åº¦ç­‰é—®é¢˜ï¼Œé˜»ç¢äº†å…ˆè¿›è§†é¢‘-æ–‡æœ¬æ£€ç´¢æ–¹æ³•çš„è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†LoVRåŸºå‡†ï¼Œä¸“é—¨ç”¨äºé•¿è§†é¢‘-æ–‡æœ¬æ£€ç´¢ã€‚LoVRåŒ…å«467ä¸ªé•¿è§†é¢‘å’Œè¶…è¿‡40,804ä¸ªé«˜è´¨é‡å­—å¹•çš„ç»†ç²’åº¦å‰ªè¾‘ã€‚ä¸ºå…‹æœæœºå™¨ç”Ÿæˆæ³¨é‡Šè´¨é‡å·®çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å­—å¹•ç”Ÿæˆæ¡†æ¶ï¼Œé›†æˆäº†VLMè‡ªåŠ¨ç”Ÿæˆã€å­—å¹•è´¨é‡è¯„åˆ†å’ŒåŠ¨æ€ä¼˜åŒ–ã€‚è¯¥æµç¨‹æé«˜äº†æ³¨é‡Šçš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§è¯­ä¹‰èåˆæ–¹æ³•ï¼Œä»¥ç”Ÿæˆè¿è´¯çš„å®Œæ•´è§†é¢‘å­—å¹•è€Œä¸ä¸¢å¤±é‡è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬çš„åŸºå‡†å¼•å…¥äº†æ›´é•¿çš„è§†é¢‘ã€æ›´è¯¦ç»†çš„å­—å¹•å’Œæ›´å¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œä¸ºè§†é¢‘ç†è§£å’Œæ£€ç´¢æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é•¿è§†é¢‘-æ–‡æœ¬æ£€ç´¢ä¸­çš„è¯„ä¼°ä¸è¶³ï¼Œç°æœ‰åŸºå‡†å› è§†é¢‘æ—¶é•¿çŸ­ã€å­—å¹•è´¨é‡ä½å’Œæ³¨é‡Šç²—ç³™è€Œæ— æ³•å…¨é¢è¯„ä¼°å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºLoVRåŸºå‡†ï¼Œé€šè¿‡å¼•å…¥é•¿è§†é¢‘å’Œé«˜è´¨é‡ç»†ç²’åº¦å­—å¹•ï¼Œæ”¹å–„ç°æœ‰è¯„ä¼°æ ‡å‡†ï¼ŒåŒæ—¶è®¾è®¡é«˜æ•ˆçš„å­—å¹•ç”Ÿæˆæ¡†æ¶ä»¥æå‡æ³¨é‡Šè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬è§†é¢‘æ•°æ®æ”¶é›†ã€å­—å¹•ç”Ÿæˆã€è´¨é‡è¯„åˆ†å’ŒåŠ¨æ€ä¼˜åŒ–å››ä¸ªä¸»è¦æ¨¡å—ï¼Œç¡®ä¿ç”Ÿæˆçš„å­—å¹•æ—¢å‡†ç¡®åˆå…·æœ‰å¯æ‰©å±•æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ç»“åˆVLMè‡ªåŠ¨ç”Ÿæˆå’ŒåŠ¨æ€ä¼˜åŒ–çš„å­—å¹•ç”Ÿæˆæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†æ³¨é‡Šçš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å­—å¹•ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§è´¨é‡è¯„åˆ†æœºåˆ¶å’ŒåŠ¨æ€è°ƒæ•´ç­–ç•¥ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å­—å¹•åœ¨è¯­ä¹‰ä¸Šè¿è´¯ä¸”ä¿¡æ¯ä¸°å¯Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç§å…ˆè¿›åµŒå…¥æ¨¡å‹çš„å®éªŒä¸­ï¼ŒLoVRåŸºå‡†å±•ç¤ºäº†å…¶æŒ‘æˆ˜æ€§ï¼Œæ­ç¤ºäº†å½“å‰æ–¹æ³•çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LoVRè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨è§†é¢‘-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘æ£€ç´¢ã€å†…å®¹æ¨èå’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„é•¿è§†é¢‘æ•°æ®é›†ï¼ŒLoVRèƒ½å¤Ÿä¿ƒè¿›è§†é¢‘ç†è§£æŠ€æœ¯çš„å‘å±•ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long videos contain a vast amount of information, making video-text retrieval an essential and challenging task in multimodal learning. However, existing benchmarks suffer from limited video duration, low-quality captions, and coarse annotation granularity, which hinder the evaluation of advanced video-text retrieval methods. To address these limitations, we introduce LoVR, a benchmark specifically designed for long video-text retrieval. LoVR contains 467 long videos and over 40,804 fine-grained clips with high-quality captions. To overcome the issue of poor machine-generated annotations, we propose an efficient caption generation framework that integrates VLM automatic generation, caption quality scoring, and dynamic refinement. This pipeline improves annotation accuracy while maintaining scalability. Furthermore, we introduce a semantic fusion method to generate coherent full-video captions without losing important contextual information. Our benchmark introduces longer videos, more detailed captions, and a larger-scale dataset, presenting new challenges for video understanding and retrieval. Extensive experiments on various advanced embedding models demonstrate that LoVR is a challenging benchmark, revealing the limitations of current approaches and providing valuable insights for future research. We release the code and dataset link at https://github.com/TechNomad-ds/LoVR-benchmark

