---
layout: default
title: Visual Agentic Reinforcement Fine-Tuning
---

# Visual Agentic Reinforcement Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14246" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14246v1</a>
  <a href="https://arxiv.org/pdf/2505.14246.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14246v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14246v1', 'Visual Agentic Reinforcement Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: project url: https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§†è§‰ä»£ç†å¼ºåŒ–å¾®è°ƒæ–¹æ³•ä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰ä»£ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€æ¨ç†` `å›¾åƒå¤„ç†` `ä¿¡æ¯æ£€ç´¢` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼€æºç ”ç©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ä»£ç†èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶ç›¸å¯¹ä¸è¶³ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŸºå‡†å’Œè¯„ä¼°æ‰‹æ®µã€‚
2. æœ¬ç ”ç©¶æå‡ºè§†è§‰ä»£ç†å¼ºåŒ–å¾®è°ƒï¼ˆVisual-ARFTï¼‰ï¼Œä½¿LVLMså…·å¤‡å®æ—¶ä¿¡æ¯è·å–å’Œå›¾åƒå¤„ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisual-ARFTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚OpenAIçš„o3ï¼‰ä¸­ï¼ŒåŸç”Ÿçš„ä»£ç†èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿä½¿ç”¨å¤–éƒ¨å·¥å…·è¿›è¡Œæœç´¢å’Œä»£ç æ‰§è¡Œï¼Œä»è€Œå®ç°å›¾åƒæ€è€ƒã€‚å°½ç®¡åœ¨è¯­è¨€ä»£ç†èƒ½åŠ›æ–¹é¢å·²æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†å¤šæ¨¡æ€ä»£ç†èƒ½åŠ›çš„å‘å±•ä»è¾ƒå°‘æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºè§†è§‰ä»£ç†å¼ºåŒ–å¾®è°ƒï¼ˆVisual-ARFTï¼‰ï¼Œæœ‰æ•ˆæå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„çµæ´»æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡Visual-ARFTï¼Œå¼€æºLVLMsèƒ½å¤Ÿå®æ—¶æµè§ˆç½‘ç«™è·å–ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ä»£ç å¯¹è¾“å…¥å›¾åƒè¿›è¡Œå¤„ç†å’Œåˆ†æã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†å¤šæ¨¡æ€ä»£ç†å·¥å…·åŸºå‡†ï¼ˆMATï¼‰ï¼Œç”¨äºè¯„ä¼°LVLMsçš„æœç´¢å’Œç¼–ç èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisual-ARFTåœ¨MAT-Codingå’ŒMAT-Searchä¸Šåˆ†åˆ«æ¯”åŸºçº¿æå‡äº†+18.6% F1å’Œ+10.3% F1ï¼Œè¶…è¶Šäº†GPT-4oï¼Œå¹¶åœ¨å¤šè·³é—®ç­”åŸºå‡†ä¸Šä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»£ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶ä¿¡æ¯è·å–å’Œå›¾åƒå¤„ç†èƒ½åŠ›çš„ç¼ºä¹ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºè¯­è¨€ä»£ç†èƒ½åŠ›ï¼Œæœªèƒ½æœ‰æ•ˆæ•´åˆå›¾åƒæ€è€ƒèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„Visual-ARFTæ–¹æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼ï¼Œç»“åˆè§†è§‰ä¿¡æ¯ä¸è¯­è¨€æ¨¡å‹ï¼Œæå‡æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¿¡æ¯æ£€ç´¢æ¨¡å—å’Œå›¾åƒå¤„ç†æ¨¡å—ã€‚ä¿¡æ¯æ£€ç´¢æ¨¡å—è´Ÿè´£å®æ—¶è·å–ç½‘é¡µä¿¡æ¯ï¼Œè€Œå›¾åƒå¤„ç†æ¨¡å—åˆ™é€šè¿‡ç¼–å†™ä»£ç å®ç°å¯¹è¾“å…¥å›¾åƒçš„æ“ä½œï¼Œå¦‚è£å‰ªå’Œæ—‹è½¬ã€‚

**å…³é”®åˆ›æ–°**ï¼šVisual-ARFTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è§†è§‰ä¿¡æ¯ä¸è¯­è¨€æ¨¡å‹çš„ä»£ç†èƒ½åŠ›ç»“åˆï¼Œå½¢æˆä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¨ç†æœºåˆ¶ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£æ–‡æœ¬ï¼Œè¿˜èƒ½æœ‰æ•ˆå¤„ç†å›¾åƒä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è¯­è¨€å’Œè§†è§‰ä¿¡æ¯çš„å­¦ä¹ ï¼ŒåŒæ—¶è®¾ç½®äº†å¤šç§è¶…å‚æ•°ä»¥ä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisual-ARFTåœ¨MAT-Codingå’ŒMAT-SearchåŸºå‡†ä¸Šåˆ†åˆ«æå‡äº†+18.6% F1å’Œ+10.3% F1ï¼Œè¶…è¶Šäº†GPT-4oã€‚æ­¤å¤–ï¼Œåœ¨å¤šè·³é—®ç­”åŸºå‡†å¦‚2Wikiå’ŒHotpotQAä¸Šï¼ŒVisual-ARFTä¹Ÿå®ç°äº†+29.3% F1å’Œ+25.9% EMçš„æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆã€å›¾åƒåˆ†æå’Œå¤šæ¨¡æ€æœç´¢å¼•æ“ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­æä¾›æ›´å‡†ç¡®å’Œçµæ´»çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents.

