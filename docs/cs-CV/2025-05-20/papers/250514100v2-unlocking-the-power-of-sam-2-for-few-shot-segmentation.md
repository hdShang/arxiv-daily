---
layout: default
title: Unlocking the Power of SAM 2 for Few-Shot Segmentation
---

# Unlocking the Power of SAM 2 for Few-Shot Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14100" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14100v2</a>
  <a href="https://arxiv.org/pdf/2505.14100.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14100v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14100v2', 'Unlocking the Power of SAM 2 for Few-Shot Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-21)

**å¤‡æ³¨**: This paper is accepted by ICML'25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¼ªæç¤ºç”Ÿæˆå™¨ä¸è¿­ä»£è®°å¿†ç²¾ç‚¼ä»¥è§£å†³å°‘æ ·æœ¬åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°‘æ ·æœ¬åˆ†å‰²` `ä¼ªæç¤ºç”Ÿæˆå™¨` `è¿­ä»£è®°å¿†ç²¾ç‚¼` `è§†é¢‘åˆ†å‰²` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°‘æ ·æœ¬åˆ†å‰²ä¸­å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆï¼Œä¸”åœ¨è§†é¢‘æ•°æ®ä¸­ï¼Œå‰æ™¯å¯¹è±¡çš„èº«ä»½ä¸€è‡´æ€§å¯¼è‡´åŒ¹é…æ­¥éª¤ä¸å…¼å®¹ã€‚
2. æœ¬æ–‡æå‡ºä¼ªæç¤ºç”Ÿæˆå™¨ä»¥ç¼–ç ä¼ªæŸ¥è¯¢è®°å¿†ï¼Œå¹¶è®¾è®¡è¿­ä»£è®°å¿†ç²¾ç‚¼æ¥å¢å¼ºè®°å¿†çš„å‡†ç¡®æ€§ï¼ŒæŠ‘åˆ¶æ„å¤–çš„èƒŒæ™¯ç‰¹å¾ã€‚
3. åœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$ä¸Šè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œ1-shot mIoUæ¯”æœ€ä½³åŸºçº¿æå‡äº†4.2%ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°‘æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨é€šè¿‡å°‘é‡ç±»åˆ«å­¦ä¹ ç±»æ— å…³çš„åˆ†å‰²ï¼Œä½†å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œéƒ¨åˆ†æ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAMï¼‰çš„çŸ¥è¯†æ¥ç®€åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚SAM 2æ‰©å±•äº†è§†é¢‘åˆ†å‰²çš„æ”¯æŒï¼Œå…¶ç±»æ— å…³åŒ¹é…èƒ½åŠ›å¯¹FSSéå¸¸æœ‰ç”¨ã€‚æœ¬æ–‡è®¾è®¡äº†ä¼ªæç¤ºç”Ÿæˆå™¨ä»¥ç¼–ç ä¼ªæŸ¥è¯¢è®°å¿†ï¼Œå¹¶é€šè¿‡è¿­ä»£è®°å¿†ç²¾ç‚¼æ¥å¢å¼ºè®°å¿†çš„å‡†ç¡®æ€§ï¼Œæœ€ç»ˆåœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œ1-shot mIoUæ¯”æœ€ä½³åŸºçº¿æé«˜äº†4.2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å°‘æ ·æœ¬åˆ†å‰²ä¸­ç”±äºå‰æ™¯å¯¹è±¡èº«ä»½ä¸€è‡´æ€§å¯¼è‡´çš„åŒ¹é…ä¸å…¼å®¹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è§†é¢‘æ•°æ®æ—¶ï¼Œå‰æ™¯å¯¹è±¡çš„èº«ä»½å§‹ç»ˆç›¸åŒï¼Œè€Œåœ¨FSSä¸­ï¼Œå‰æ™¯å¯¹è±¡èº«ä»½å´æ˜¯ä¸åŒçš„ï¼Œè¿™é€ æˆäº†åŒ¹é…æ­¥éª¤çš„å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¼ªæç¤ºç”Ÿæˆå™¨æ¥ç¼–ç ä¼ªæŸ¥è¯¢è®°å¿†ï¼Œä½¿å…¶èƒ½å¤Ÿä¸æŸ¥è¯¢ç‰¹å¾ä»¥å…¼å®¹çš„æ–¹å¼è¿›è¡ŒåŒ¹é…ã€‚åŒæ—¶ï¼Œé€šè¿‡è¿­ä»£è®°å¿†ç²¾ç‚¼æ¥å¢å¼ºè®°å¿†çš„å‡†ç¡®æ€§ï¼ŒæŠ‘åˆ¶æ„å¤–çš„èƒŒæ™¯ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¼ªæç¤ºç”Ÿæˆå™¨ã€è¿­ä»£è®°å¿†ç²¾ç‚¼æ¨¡å—å’Œæ”¯æŒæ ¡å‡†è®°å¿†æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¼ªæç¤ºç”Ÿæˆå™¨è´Ÿè´£ç”Ÿæˆä¼ªæŸ¥è¯¢è®°å¿†ï¼Œè¿­ä»£è®°å¿†ç²¾ç‚¼åˆ™ç”¨äºä¸æ–­ä¼˜åŒ–è®°å¿†å†…å®¹ï¼Œæ”¯æŒæ ¡å‡†è®°å¿†æ³¨æ„åŠ›æœºåˆ¶åˆ™ç”¨äºæŠ‘åˆ¶èƒŒæ™¯ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºæå‡ºäº†ä¼ªæç¤ºç”Ÿæˆå™¨å’Œè¿­ä»£è®°å¿†ç²¾ç‚¼æœºåˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„è®¾è®¡æ€è·¯æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜å°‘æ ·æœ¬åˆ†å‰²çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæœ¬æ–‡å¯¹è®°å¿†çš„ç¼–ç æ–¹å¼è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å‰æ™¯ä¸èƒŒæ™¯ç‰¹å¾çš„åŒºåˆ†ï¼Œç½‘ç»œç»“æ„ä¸Šåˆ™é‡‡ç”¨äº†å¤šå±‚æ¬¡çš„æ³¨æ„åŠ›æœºåˆ¶æ¥æå‡æ¨¡å‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œ1-shot mIoUæ¯”æœ€ä½³åŸºçº¿æé«˜äº†4.2%ï¼ŒéªŒè¯äº†ä¼ªæç¤ºç”Ÿæˆå™¨å’Œè¿­ä»£è®°å¿†ç²¾ç‚¼çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†å°‘æ ·æœ¬åˆ†å‰²çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰é‡è¦åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºåŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç›‘æ§ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜åˆ†å‰²ç²¾åº¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒæ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–ä¸æ“ä½œï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ä¸åº”ç”¨æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few classes to segment arbitrary classes, but at the risk of overfitting. To address this, some methods use the well-learned knowledge of foundation models (e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM by supporting video segmentation, whose class-agnostic matching ability is useful to FSS. A simple idea is to encode support foreground (FG) features as memory, with which query FG features are matched and fused. Unfortunately, the FG objects in different frames of SAM 2's video data are always the same identity, while those in FSS are different identities, i.e., the matching step is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo query memory, matching with query features in a compatible way. However, the memories can never be as accurate as the real ones, i.e., they are likely to contain incomplete query FG, and some unexpected query background (BG) features, leading to wrong segmentation. Hence, we further design Iterative Memory Refinement to fuse more query FG features into the memory, and devise a Support-Calibrated Memory Attention to suppress the unexpected query BG features in memory. Extensive experiments have been conducted on PASCAL-5$^i$ and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot mIoU can be 4.2% better than the best baseline.

