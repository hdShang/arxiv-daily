---
layout: default
title: VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank
---

# VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14460" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14460v2</a>
  <a href="https://arxiv.org/pdf/2505.14460.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14460v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14460v2', 'VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, Kede Ma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-21)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVisualQuality-R1ä»¥è§£å†³å›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„æ¨ç†ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒè´¨é‡è¯„ä¼°` `æ— å‚è€ƒè¯„ä¼°` `å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `æ·±åº¦å­¦ä¹ ` `ç›¸å¯¹è¯„åˆ†` `å¤šæ•°æ®é›†è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•å¾€å¾€ä¾èµ–äºç¦»æ•£çš„æ ‡ç­¾ï¼Œç¼ºä¹å¯¹ç›¸å¯¹è´¨é‡çš„æ·±å…¥æ¨ç†èƒ½åŠ›ã€‚
2. VisualQuality-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œé‡‡ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç”Ÿæˆå¤šä¸ªè´¨é‡è¯„åˆ†ï¼Œæå‡äº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisualQuality-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æ·±åº¦å­¦ä¹ NR-IQAæ¨¡å‹ï¼Œä¸”æ”¯æŒå¤šæ•°æ®é›†è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

DeepSeek-R1åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å±•ç°äº†å‡ºè‰²çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰é¢†åŸŸçš„æ¨ç†è®¡ç®—æ½œåŠ›å°šæœªè¢«å……åˆ†æŒ–æ˜ã€‚æœ¬æ–‡æå‡ºäº†VisualQuality-R1ï¼Œä¸€ä¸ªæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ’åè®­ç»ƒï¼Œä¸“æ³¨äºå›¾åƒè´¨é‡çš„ç›¸å¯¹æ¯”è¾ƒã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç”Ÿæˆå¤šä¸ªè´¨é‡è¯„åˆ†ï¼Œå¹¶åˆ©ç”¨è¿ç»­çš„ä¿çœŸåº¦åº¦é‡å®šä¹‰å¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒVisualQuality-R1åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„æ·±åº¦å­¦ä¹ NR-IQAæ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„è´¨é‡æè¿°ï¼Œé€‚ç”¨äºå¤šæ•°æ®é›†è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›å’Œç›¸å¯¹è´¨é‡æ¯”è¾ƒä¸Šçš„ä¸è¶³ï¼Œä¼ ç»Ÿæ–¹æ³•å¤šä¾èµ–äºç¦»æ•£çš„æ ‡ç­¾ï¼Œæ— æ³•å……åˆ†åæ˜ å›¾åƒè´¨é‡çš„ç›¸å¯¹æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVisualQuality-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ’åè®­ç»ƒï¼Œåˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç”Ÿæˆå¤šä¸ªè´¨é‡è¯„åˆ†ï¼Œè¿›è€Œè®¡ç®—å›¾åƒé—´çš„æ¯”è¾ƒæ¦‚ç‡ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å›¾åƒè¾“å…¥æ¨¡å—ã€è´¨é‡è¯„åˆ†ç”Ÿæˆæ¨¡å—å’Œå¥–åŠ±è®¡ç®—æ¨¡å—ã€‚é¦–å…ˆè¾“å…¥å›¾åƒå¯¹ï¼Œç„¶åç”Ÿæˆå¤šä¸ªè´¨é‡è¯„åˆ†ï¼Œæœ€åé€šè¿‡å¥–åŠ±æœºåˆ¶ä¼˜åŒ–æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šVisualQuality-R1çš„ä¸»è¦åˆ›æ–°åœ¨äºä½¿ç”¨è¿ç»­çš„ä¿çœŸåº¦åº¦é‡ä½œä¸ºå¥–åŠ±ï¼Œè€Œéä¼ ç»Ÿçš„ç¦»æ•£æ ‡ç­¾ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°å›¾åƒè´¨é‡çš„ç›¸å¯¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œå¹¶å®šä¹‰äº†åŸºäºThurstoneæ¨¡å‹çš„æ¯”è¾ƒæ¦‚ç‡è®¡ç®—æ–¹å¼ï¼Œç¡®ä¿äº†è¯„åˆ†çš„ç›¸å¯¹æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVisualQuality-R1åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ·±åº¦å­¦ä¹ NR-IQAæ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆè´¨é‡æè¿°æ–¹é¢è¡¨ç°çªå‡ºã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¨¡å‹åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­æå‡äº†è¶…è¿‡10%çš„è¯„ä¼°å‡†ç¡®ç‡ï¼Œå±•ç°äº†å…¶åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VisualQuality-R1åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å›¾åƒç”Ÿæˆç­‰é¢†åŸŸã€‚å…¶èƒ½å¤Ÿæä¾›å¯é çš„è´¨é‡è¯„ä¼°ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆåœ¨å›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›æ­¥ä¸­è¿›è¡Œæœ‰æ•ˆçš„ç›‘æµ‹å’Œè¯„ä¼°ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computation has not been thoroughly explored in the context of image quality assessment (IQA), a task depending critically on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.

