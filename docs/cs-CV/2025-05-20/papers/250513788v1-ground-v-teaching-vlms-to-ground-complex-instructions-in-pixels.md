---
layout: default
title: "Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels"
---

# Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13788" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13788v1</a>
  <a href="https://arxiv.org/pdf/2505.13788.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13788v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13788v1', 'Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongshuo Zong, Qin Zhang, Dongsheng An, Zhihua Li, Xiang Xu, Linghan Xu, Zhuowen Tu, Yifan Xing, Onkar Dabeer

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: Accepted to CVPR'25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGround-Vä»¥è§£å†³å¤æ‚æŒ‡ä»¤çš„åƒç´ çº§å®šä½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `çŸ¥è¯†è’¸é¦` `åƒç´ çº§å®šä½` `å¤æ‚æŒ‡ä»¤` `å¤šå¯¹è±¡åœºæ™¯` `æ•°æ®é›†ç”Ÿæˆ` `è‡ªåŠ¨åŒ–æ ‡æ³¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´è™šå‡å¼•ç”¨ã€å¤šå¯¹è±¡åœºæ™¯å’Œæ¨ç†ç­‰å¤šé‡æŒ‘æˆ˜ï¼Œå¯¼è‡´å®šä½ç²¾åº¦ä¸è¶³ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡çŸ¥è¯†è’¸é¦ç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤-å“åº”å¯¹ï¼Œç»“åˆç°æœ‰åƒç´ çº§æ³¨é‡Šï¼Œå‡å°‘äººå·¥æ ‡æ³¨éœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºGround-Vè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨gRefCOCOä¸Šè¾¾åˆ°83.3%çš„N-Accã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„å·¥ä½œæµç¨‹ï¼Œæ—¨åœ¨è‡ªåŠ¨æ‰©å±•æŒ‡ä»¤è·Ÿéšæ•°æ®ï¼Œä»¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚æŒ‡ä»¤ä¸‹å®ç°åƒç´ çº§çš„å®šä½èƒ½åŠ›ã€‚æˆ‘ä»¬ç‰¹åˆ«è§£å†³äº†æ–‡æœ¬æŒ‡ä»¤åŸºç¡€å®šä½ä¸­çš„äº”ä¸ªå…³é”®ç°å®æŒ‘æˆ˜ï¼šè™šå‡å¼•ç”¨ã€å¤šå¯¹è±¡åœºæ™¯ã€æ¨ç†ã€å¤šç²’åº¦å’Œéƒ¨åˆ†å¼•ç”¨ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è’¸é¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†ä¸ç°æœ‰åƒç´ çº§æ³¨é‡Šç›¸å…³çš„é«˜è´¨é‡æŒ‡ä»¤-å“åº”å¯¹ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘äº†å¯¹æ˜‚è´µäººå·¥æ³¨é‡Šçš„éœ€æ±‚ã€‚ç”Ÿæˆçš„æ•°æ®é›†Ground-Væ•æ‰äº†ä¸°å¯Œçš„å¯¹è±¡å®šä½çŸ¥è¯†å’Œç»†è‡´çš„åƒç´ çº§å¼•ç”¨è¡¨è¾¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºGround-Vè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šç§å®šä½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå…·ä½“è€Œè¨€ï¼Œåœ¨LISAå’ŒPSALMçš„è®­ç»ƒä¸­ï¼Œå¹³å‡å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†4.4%å’Œ7.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„åƒç´ çº§å®šä½é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šå¯¹è±¡åœºæ™¯å’Œæ¨ç†æ—¶ï¼Œå¸¸å¸¸å‡ºç°è™šå‡å¼•ç”¨å’Œå®šä½ä¸å‡†ç¡®çš„æƒ…å†µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºé€šè¿‡çŸ¥è¯†è’¸é¦ä»é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ä¸­æå–ä¿¡æ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤-å“åº”å¯¹ï¼Œä»¥æ­¤æ¥å¢å¼ºæ¨¡å‹çš„åƒç´ çº§å®šä½èƒ½åŠ›ï¼Œå‡å°‘å¯¹äººå·¥æ³¨é‡Šçš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆã€çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹ç”ŸæˆæŒ‡ä»¤-å“åº”å¯¹ï¼Œç„¶ååˆ©ç”¨è¿™äº›æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»¥æé«˜å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡çŸ¥è¯†è’¸é¦ç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤-å“åº”å¯¹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„åƒç´ çº§å®šä½èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„äººå·¥æ ‡æ³¨æ–¹å¼ç›¸æ¯”ï¼Œæ•ˆç‡æ›´é«˜ä¸”æˆæœ¬æ›´ä½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æŒ‡ä»¤ä¸å“åº”ä¹‹é—´çš„åŒ¹é…åº¦ï¼Œå¹¶è®¾è®¡äº†é€‚åˆå¤šç²’åº¦å¼•ç”¨çš„ç½‘ç»œç»“æ„ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Ground-Vè®­ç»ƒçš„æ¨¡å‹åœ¨LISAå’ŒPSALMä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†4.4%å’Œ7.9%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å¦‚RefCOCO/+/gä¸Šè®¾ç«‹äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚åœ¨gRefCOCOä¸Šï¼Œæ¨¡å‹çš„N-Accè¾¾åˆ°äº†83.3%ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³è¶…è¿‡20%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œå¤æ‚çš„è§†è§‰æŒ‡ä»¤ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„åƒç´ çº§å®šä½èƒ½åŠ›ï¼Œæœªæ¥å¯ä»¥åœ¨å¤šç§å®é™…åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„äº¤äº’å’Œæ“ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work presents a simple yet effective workflow for automatically scaling instruction-following data to elicit pixel-level grounding capabilities of VLMs under complex instructions. In particular, we address five critical real-world challenges in text-instruction-based grounding: hallucinated references, multi-object scenarios, reasoning, multi-granularity, and part-level references. By leveraging knowledge distillation from a pre-trained teacher model, our approach generates high-quality instruction-response pairs linked to existing pixel-level annotations, minimizing the need for costly human annotation. The resulting dataset, Ground-V, captures rich object localization knowledge and nuanced pixel-level referring expressions. Experiment results show that models trained on Ground-V exhibit substantial improvements across diverse grounding tasks. Specifically, incorporating Ground-V during training directly achieves an average accuracy boost of 4.4% for LISA and a 7.9% for PSALM across six benchmarks on the gIoU metric. It also sets new state-of-the-art results on standard benchmarks such as RefCOCO/+/g. Notably, on gRefCOCO, we achieve an N-Acc of 83.3%, exceeding the previous state-of-the-art by more than 20%.

