---
layout: default
title: Programmatic Video Prediction Using Large Language Models
---

# Programmatic Video Prediction Using Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14948" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14948v1</a>
  <a href="https://arxiv.org/pdf/2505.14948.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14948v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14948v1', 'Programmatic Video Prediction Using Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hao Tang, Kevin Ellis, Suhas Lohit, Michael J. Jones, Moitreya Chatterjee

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºProgGenä»¥è§£å†³è§†é¢‘å¸§é¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘é¢„æµ‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¥ç»ç¬¦å·` `å¯è§£é‡Šæ€§` `åŠ¨æ€å»ºæ¨¡` `åäº‹å®æ¨ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘é¢„æµ‹æ–¹æ³•åœ¨åŠ¨æ€å»ºæ¨¡å’Œå¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚åœºæ™¯ã€‚
2. æœ¬æ–‡æå‡ºProgGenï¼Œé€šè¿‡ç¥ç»ç¬¦å·æ–¹æ³•ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå»ºç«‹å¯è§£é‡Šçš„çŠ¶æ€è¡¨ç¤ºæ¥è¿›è¡Œè§†é¢‘å¸§é¢„æµ‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProgGenåœ¨PhyWorldå’ŒCart Poleç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç«äº‰æŠ€æœ¯ï¼Œæå‡äº†è§†é¢‘å¸§é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼°è®¡æè¿°çœŸå®ä¸–ç•Œè¿‡ç¨‹åŠ¨æ€çš„ä¸–ç•Œæ¨¡å‹å¯¹äºé¢„æµ‹å’Œå‡†å¤‡æœªæ¥ç»“æœè‡³å…³é‡è¦ã€‚é’ˆå¯¹è§†é¢‘ç›‘æ§ã€æœºå™¨äººåº”ç”¨å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œæœ¬æ–‡æå‡ºProgGenï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLM/VLMï¼‰çš„å½’çº³åå·®ï¼Œé‡‡ç”¨ç¥ç»ç¬¦å·çš„å¯è§£é‡ŠçŠ¶æ€é›†æ¥è¿›è¡Œè§†é¢‘å¸§é¢„æµ‹ã€‚ProgGençš„ä¸»è¦ä»»åŠ¡åŒ…æ‹¬ï¼šåœ¨ç»™å®šè§†è§‰ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ä¼°è®¡è§†é¢‘çŠ¶æ€ã€é¢„æµ‹æœªæ¥æ—¶é—´æ­¥çš„çŠ¶æ€ä»¥åŠå°†é¢„æµ‹çŠ¶æ€æ¸²æŸ“ä¸ºè§†è§‰RGBå¸§ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒProgGenåœ¨PhyWorldå’ŒCart Poleä¸¤ä¸ªæŒ‘æˆ˜æ€§ç¯å¢ƒä¸­ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸”æ”¯æŒåäº‹å®æ¨ç†å’Œå¯è§£é‡Šçš„è§†é¢‘ç”Ÿæˆï¼Œè¯æ˜äº†å…¶åœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘å¸§é¢„æµ‹ä¸­çš„åŠ¨æ€å»ºæ¨¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹å¯è§£é‡Šæ€§å’Œå¯¹å¤æ‚åœºæ™¯çš„é€‚åº”èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLM/VLMï¼‰ï¼ŒProgGenèƒ½å¤Ÿç”Ÿæˆç¨‹åºæ¥ä¼°è®¡è§†é¢‘çŠ¶æ€ã€é¢„æµ‹æœªæ¥çŠ¶æ€å¹¶æ¸²æŸ“ä¸ºRGBå¸§ï¼Œä»è€Œå®ç°å¯è§£é‡Šçš„è§†é¢‘é¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šProgGençš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šçŠ¶æ€ä¼°è®¡æ¨¡å—ã€çŠ¶æ€é¢„æµ‹æ¨¡å—å’Œæ¸²æŸ“æ¨¡å—ã€‚çŠ¶æ€ä¼°è®¡æ¨¡å—åˆ©ç”¨è§†è§‰ä¸Šä¸‹æ–‡ç”Ÿæˆå½“å‰çŠ¶æ€ï¼ŒçŠ¶æ€é¢„æµ‹æ¨¡å—åŸºäºè½¬ç§»åŠ¨æ€é¢„æµ‹æœªæ¥çŠ¶æ€ï¼Œæ¸²æŸ“æ¨¡å—å°†é¢„æµ‹çŠ¶æ€è½¬æ¢ä¸ºå¯è§†åŒ–å¸§ã€‚

**å…³é”®åˆ›æ–°**ï¼šProgGençš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨ç¥ç»ç¬¦å·æ–¹æ³•ç»“åˆLLM/VLMï¼Œæä¾›äº†ä¸€ç§å¯è§£é‡Šçš„åŠ¨æ€å»ºæ¨¡æ–¹å¼ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚åœºæ™¯å’Œæä¾›åäº‹å®æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒProgGené‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–çŠ¶æ€é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šç»“åˆäº†ç¬¦å·æ¨ç†ä¸æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒProgGenåœ¨PhyWorldå’ŒCart Poleç¯å¢ƒä¸­çš„è§†é¢‘å¸§é¢„æµ‹ä»»åŠ¡ä¸Šï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºç°æœ‰æŠ€æœ¯ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚åŠ¨æ€åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œé¢„æµ‹åŠ¨æ€åœºæ™¯ï¼Œä»è€Œæå‡å†³ç­–èƒ½åŠ›å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼ŒProgGenæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The task of estimating the world model describing the dynamics of a real world process assumes immense importance for anticipating and preparing for future outcomes. For applications such as video surveillance, robotics applications, autonomous driving, etc. this objective entails synthesizing plausible visual futures, given a few frames of a video to set the visual context. Towards this end, we propose ProgGen, which undertakes the task of video frame prediction by representing the dynamics of the video using a set of neuro-symbolic, human-interpretable set of states (one per frame) by leveraging the inductive biases of Large (Vision) Language Models (LLM/VLM). In particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate the states of the video, given the visual context (i.e. the frames); (ii) to predict the states corresponding to future time steps by estimating the transition dynamics; (iii) to render the predicted states as visual RGB-frames. Empirical evaluations reveal that our proposed method outperforms competing techniques at the task of video frame prediction in two challenging environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits counter-factual reasoning and interpretable video generation attesting to its effectiveness and generalizability for video generation tasks.

