---
layout: default
title: "Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method"
---

# Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14197" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14197v1</a>
  <a href="https://arxiv.org/pdf/2505.14197.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14197v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14197v1', 'Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinshen Zhang, Zhen Ye, Xu Zheng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOmniVQAæ•°æ®é›†ä¸360-R1æ–¹æ³•ä»¥è§£å†³å…¨æ™¯è§†è§‰é—®ç­”é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…¨æ™¯è§†è§‰é—®ç­”` `å¤šæ¨¡æ€å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°è®¾è®¡` `æ•°æ®é›†æ„å»º` `æ¨¡å‹è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¨æ™¯è§†è§‰é—®ç­”ä¸­å­˜åœ¨å¯¹è±¡å®šä½å’Œç‰¹å¾æå–ç­‰æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚
2. æœ¬æ–‡æå‡ºOmniVQAæ•°æ®é›†å’Œ360-R1æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ–°é¢–çš„å¥–åŠ±å‡½æ•°æ¥æ”¹è¿›å…¨æ™¯é—®ç­”èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œ360-R1æ–¹æ³•åœ¨å…¨æ™¯è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šç›¸è¾ƒäºç°æœ‰æ–¹æ³•æå‡äº†6%çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…¨æ™¯å›¾åƒï¼ˆODIsï¼‰ä»¥å…¶360Â°è§†é‡ä¸ºå¢å¼ºç°å®å’Œå…·èº«äººå·¥æ™ºèƒ½ç­‰æ²‰æµ¸å¼åº”ç”¨æä¾›äº†æ— ä¸ä¼¦æ¯”çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£å’Œæ¨ç†å…¨æ™¯åœºæ™¯æ–¹é¢çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥OmniVQAæ•°æ®é›†å’ŒåŸºå‡†ï¼Œé¦–æ¬¡å¯¹å…¨æ™¯è§†è§‰é—®ç­”è¿›è¡Œè¯„ä¼°ï¼Œæ­ç¤ºäº†ç°æœ‰MLLMsåœ¨å¯¹è±¡å®šä½ã€ç‰¹å¾æå–å’Œå¹»è§‰æŠ‘åˆ¶ç­‰æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚åŸºäºOmniVQAæ•°æ®é›†ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºQwen2.5-VL-Instructçš„è§„åˆ™å¼ºåŒ–å­¦ä¹ æ–¹æ³•360-R1ï¼Œè®¾è®¡äº†ä¸‰ç§æ–°é¢–çš„å¥–åŠ±å‡½æ•°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å…¨æ™¯ç©ºé—´ä¸Šæœ‰6%çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¨æ™¯è§†è§‰é—®ç­”ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹è±¡å®šä½ã€ç‰¹å¾æå–å’Œå¹»è§‰æŠ‘åˆ¶æ–¹é¢çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†OmniVQAæ•°æ®é›†å’Œ360-R1æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å’Œæ–°é¢–çš„å¥–åŠ±å‡½æ•°ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨å…¨æ™¯å›¾åƒä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€åŸºå‡†æµ‹è¯•å’Œ360-R1æ–¹æ³•çš„å®ç°ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬å¥–åŠ±å‡½æ•°è®¾è®¡å’Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸‰ç§æ–°é¢–çš„å¥–åŠ±å‡½æ•°ï¼Œåˆ†åˆ«æ˜¯æ¨ç†è¿‡ç¨‹ç›¸ä¼¼æ€§å¥–åŠ±ã€ç­”æ¡ˆè¯­ä¹‰å‡†ç¡®æ€§å¥–åŠ±å’Œç»“æ„æ ¼å¼åˆè§„å¥–åŠ±ï¼Œè¿™äº›è®¾è®¡æ—¨åœ¨æ›´å¥½åœ°é€‚åº”å…¨æ™¯è§†è§‰é—®ç­”çš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨360-R1æ–¹æ³•ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨å…¨æ™¯åœºæ™¯ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œå›ç­”ï¼ŒåŒæ—¶é‡‡ç”¨äº†Qwen2.5-VL-Instructä½œä¸ºåŸºç¡€æ¨¡å‹è¿›è¡Œæ”¹è¿›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œ360-R1æ–¹æ³•åœ¨å…¨æ™¯è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹æå‡äº†6%çš„æ€§èƒ½ï¼Œå……åˆ†éªŒè¯äº†æ–°é¢–å¥–åŠ±å‡½æ•°çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®å’Œå…·èº«äººå·¥æ™ºèƒ½ç­‰ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´ç²¾å‡†çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚é€šè¿‡æå‡å…¨æ™¯è§†è§‰é—®ç­”çš„æ€§èƒ½ï¼Œæœªæ¥å¯åœ¨æ™ºèƒ½åŠ©æ‰‹ã€æ•™è‚²å’Œå¨±ä¹ç­‰å¤šä¸ªåœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Omnidirectional images (ODIs), with their 360Â° field of view, provide unparalleled spatial awareness for immersive applications like augmented reality and embodied AI. However, the capability of existing multi-modal large language models (MLLMs) to comprehend and reason about such panoramic scenes remains underexplored. This paper addresses this gap by introducing OmniVQA, the first dataset and conducting the first benchmark for omnidirectional visual question answering. Our evaluation of state-of-the-art MLLMs reveals significant limitations in handling omnidirectional visual question answering, highlighting persistent challenges in object localization, feature extraction, and hallucination suppression within panoramic contexts. These results underscore the disconnect between current MLLM capabilities and the demands of omnidirectional visual understanding, which calls for dedicated architectural or training innovations tailored to 360Â° imagery. Building on the OmniVQA dataset and benchmark, we further introduce a rule-based reinforcement learning method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group relative policy optimization (GRPO) by proposing three novel reward functions: (1) reasoning process similarity reward, (2) answer semantic accuracy reward, and (3) structured format compliance reward. Extensive experiments on our OmniVQA demonstrate the superiority of our proposed method in omnidirectional space (+6% improvement).

