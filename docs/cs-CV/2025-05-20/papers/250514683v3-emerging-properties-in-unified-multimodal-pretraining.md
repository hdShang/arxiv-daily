---
layout: default
title: Emerging Properties in Unified Multimodal Pretraining
---

# Emerging Properties in Unified Multimodal Pretraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14683" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14683v3</a>
  <a href="https://arxiv.org/pdf/2505.14683.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14683v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14683v3', 'Emerging Properties in Unified Multimodal Pretraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-07-27)

**å¤‡æ³¨**: 37 pages, 17 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBAGELæ¨¡å‹ä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç†è§£` `å¤šæ¨¡æ€ç”Ÿæˆ` `é¢„è®­ç»ƒæ¨¡å‹` `å¤æ‚æ¨ç†` `å¼€æºæ¨¡å‹` `å›¾åƒæ“ä½œ` `è§†é¢‘é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚
2. BAGELæ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„è§£ç å™¨æ¶æ„ï¼Œåˆ©ç”¨å¤§è§„æ¨¡äº¤é”™æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œæ”¯æŒå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒBAGELåœ¨å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼Œå±•ç°å‡ºå…ˆè¿›çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶ä»‹ç»äº†BAGELï¼Œä¸€ä¸ªå¼€æºçš„åŸºç¡€æ¨¡å‹ï¼ŒåŸç”Ÿæ”¯æŒå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆã€‚BAGELæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§£ç å™¨æ¨¡å‹ï¼Œé¢„è®­ç»ƒäºæ¥è‡ªå¤§è§„æ¨¡äº¤é”™æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç½‘ç»œæ•°æ®çš„æ•°ä¸‡äº¿ä¸ªæ ‡è®°ã€‚é€šè¿‡è¿™ç§å¤šæ ·åŒ–çš„æ•°æ®ï¼ŒBAGELå±•ç°å‡ºåœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„æ–°å…´èƒ½åŠ›ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼€æºç»Ÿä¸€æ¨¡å‹ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·å¤‡è‡ªç”±å½¢å¼å›¾åƒæ“ä½œã€æœªæ¥å¸§é¢„æµ‹ã€3Dæ“ä½œå’Œä¸–ç•Œå¯¼èˆªç­‰é«˜çº§å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºä¿ƒè¿›å¤šæ¨¡æ€ç ”ç©¶çš„å‘å±•ï¼Œç ”ç©¶å›¢é˜Ÿåˆ†äº«äº†å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚ã€æ•°æ®åˆ›å»ºåè®®ï¼Œå¹¶å‘ç¤¾åŒºå‘å¸ƒäº†ä»£ç å’Œæ£€æŸ¥ç‚¹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ•´åˆå¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šBAGELæ¨¡å‹é‡‡ç”¨ç»Ÿä¸€çš„è§£ç å™¨æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œé€šè¿‡é¢„è®­ç»ƒäºå¤§è§„æ¨¡äº¤é”™æ•°æ®é›†ï¼Œæå‡æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBAGELçš„æ•´ä½“æ¶æ„ä¸ºè§£ç å™¨æ¨¡å‹ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µã€‚æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡çš„æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ç­‰æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œç¡®ä¿å…¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šBAGELçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç»Ÿä¸€çš„è§£ç å™¨è®¾è®¡å’Œå¤§è§„æ¨¡äº¤é”™æ•°æ®çš„ä½¿ç”¨ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¶æ„å’Œæ•°æ®å¤„ç†ä¸Šå­˜åœ¨æœ¬è´¨åŒºåˆ«ï¼Œåè€…é€šå¸¸é‡‡ç”¨åˆ†å¼€å¤„ç†ä¸åŒæ¨¡æ€çš„æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒBAGELé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¤šæ¨¡æ€ä»»åŠ¡çš„è¡¨ç°ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒæ¨¡æ€çš„æ•°æ®ç‰¹æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBAGELæ¨¡å‹åœ¨å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰å¼€æºç»Ÿä¸€æ¨¡å‹ï¼Œå±•ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼ŒBAGELåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·å¤‡è‡ªç”±å½¢å¼å›¾åƒæ“ä½œå’Œæœªæ¥å¸§é¢„æµ‹ç­‰èƒ½åŠ›ï¼Œæå‡å¹…åº¦è¾¾åˆ°XX%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BAGELæ¨¡å‹çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚å…¶å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›å¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´è‡ªç„¶çš„äº¤äº’ä½“éªŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open-source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder-only model pretrained on trillions of tokens curated from large-scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/

