---
layout: default
title: Intra-class Patch Swap for Self-Distillation
---

# Intra-class Patch Swap for Self-Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14124" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14124v1</a>
  <a href="https://arxiv.org/pdf/2505.14124.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14124v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14124v1', 'Intra-class Patch Swap for Self-Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongjun Choi, Eun Som Jeon, Ankita Shukla, Pavan Turaga

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: Accepted for publication in Neurocomputing

**DOI**: [10.1016/j.neucom.2025.130408](https://doi.org/10.1016/j.neucom.2025.130408)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/hchoi71/Intra-class-Patch-Swap)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºç±»å†…è¡¥ä¸äº¤æ¢çš„è‡ªè’¸é¦æ–¹æ³•ä»¥ç®€åŒ–çŸ¥è¯†è’¸é¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `è‡ªè’¸é¦` `ç±»å†…è¡¥ä¸äº¤æ¢` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä¾èµ–äºé¢„è®­ç»ƒçš„æ•™å¸ˆç½‘ç»œï¼Œå¯¼è‡´å†…å­˜éœ€æ±‚é«˜ã€è®­ç»ƒæˆæœ¬å¢åŠ å’Œé€‰æ‹©æ•™å¸ˆæ¨¡å‹çš„å›°éš¾ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ— æ•™å¸ˆçš„è‡ªè’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨ç±»å†…è¡¥ä¸äº¤æ¢å¢å¼ºæ¥æ¨¡æ‹Ÿæ•™å¸ˆ-å­¦ç”ŸåŠ¨æ€ï¼Œç®€åŒ–äº†è’¸é¦è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦å’Œè‡ªè’¸é¦æ–¹æ³•ï¼Œè¡¨æ˜å¢å¼ºè®¾è®¡çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ˜¯ä¸€ç§å°†å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©ä¸ºé€‚åˆè¾¹ç¼˜è®¾å¤‡çš„å°å‹ç½‘ç»œçš„æœ‰æ•ˆæŠ€æœ¯ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„KDæ¡†æ¶ä¾èµ–äºé¢„è®­ç»ƒçš„é«˜å®¹é‡æ•™å¸ˆç½‘ç»œï¼Œè¿™å¸¦æ¥äº†å†…å­˜/å­˜å‚¨éœ€æ±‚å¢åŠ ã€é¢å¤–è®­ç»ƒæˆæœ¬ä»¥åŠé€‰æ‹©åˆé€‚æ•™å¸ˆæ¨¡å‹çš„æ¨¡ç³Šæ€§ç­‰æŒ‘æˆ˜ã€‚å°½ç®¡æ— æ•™å¸ˆè’¸é¦ï¼ˆè‡ªè’¸é¦ï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆå‡ºç°ï¼Œä½†è®¸å¤šç°æœ‰æ–¹æ³•ä»ä¾èµ–äºæ¶æ„ä¿®æ”¹æˆ–å¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ— æ•™å¸ˆè’¸é¦çš„æ–°æ¡†æ¶ï¼Œä½¿ç”¨å•ä¸€å­¦ç”Ÿç½‘ç»œï¼Œæ— éœ€ä»»ä½•è¾…åŠ©ç»„ä»¶ã€æ¶æ„ä¿®æ”¹æˆ–é¢å¤–å¯å­¦ä¹ å‚æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„å¢å¼ºæŠ€æœ¯ï¼Œç§°ä¸ºç±»å†…è¡¥ä¸äº¤æ¢å¢å¼ºï¼Œé€šè¿‡ç”Ÿæˆå…·æœ‰ä¸åŒç½®ä¿¡åº¦çš„ç±»å†…æ ·æœ¬å¯¹ï¼Œæ¨¡æ‹Ÿå•ä¸€æ¨¡å‹ä¸­çš„æ•™å¸ˆ-å­¦ç”ŸåŠ¨æ€ï¼Œå¹¶åº”ç”¨å®ä¾‹é—´è’¸é¦æ¥å¯¹é½å…¶é¢„æµ‹åˆ†å¸ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„è‡ªè’¸é¦åŸºçº¿å’Œä¼ ç»Ÿçš„åŸºäºæ•™å¸ˆçš„KDæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•ä¸­å¯¹é«˜å®¹é‡æ•™å¸ˆç½‘ç»œçš„ä¾èµ–ï¼Œå¸¦æ¥çš„å†…å­˜å’Œè®­ç»ƒæˆæœ¬é—®é¢˜ï¼Œä»¥åŠé€‰æ‹©åˆé€‚æ•™å¸ˆæ¨¡å‹çš„å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§åŸºäºç±»å†…è¡¥ä¸äº¤æ¢çš„è‡ªè’¸é¦æ–¹æ³•ï¼Œé€šè¿‡åœ¨å•ä¸€å­¦ç”Ÿç½‘ç»œä¸­ç”Ÿæˆç±»å†…æ ·æœ¬å¯¹ï¼Œæ¨¡æ‹Ÿæ•™å¸ˆ-å­¦ç”Ÿçš„åŠ¨æ€å…³ç³»ï¼Œä»è€Œç®€åŒ–è’¸é¦è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿç½‘ç»œå’Œä¸€ä¸ªç±»å†…è¡¥ä¸äº¤æ¢å¢å¼ºæ¨¡å—ã€‚è¯¥æ¨¡å—ç”Ÿæˆä¸åŒç½®ä¿¡åº¦çš„æ ·æœ¬å¯¹ï¼Œå¹¶é€šè¿‡å®ä¾‹é—´è’¸é¦å¯¹å…¶é¢„æµ‹åˆ†å¸ƒè¿›è¡Œå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ç±»å†…è¡¥ä¸äº¤æ¢å¢å¼ºï¼Œè¿™ä¸€æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„æ•™å¸ˆç½‘ç»œæˆ–å¤æ‚çš„æ¶æ„ä¿®æ”¹ï¼Œå…·æœ‰æ¨¡å‹æ— å…³æ€§å’Œæ˜“å®ç°æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•ä»…éœ€ä¸€ä¸ªå¢å¼ºå‡½æ•°ï¼Œä¸”åœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šé‡‡ç”¨äº†å®ä¾‹é—´è’¸é¦ç­–ç•¥ï¼Œç¡®ä¿äº†æ ·æœ¬å¯¹çš„é¢„æµ‹åˆ†å¸ƒèƒ½å¤Ÿæœ‰æ•ˆå¯¹é½ã€‚æ•´ä½“è®¾è®¡ç®€æ´ï¼Œæ˜“äºåœ¨ä¸åŒæ¨¡å‹ä¸Šåº”ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„è‡ªè’¸é¦åŸºçº¿å’Œä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°X%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚é€šè¿‡ç®€åŒ–çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ¨¡å‹æ€§èƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge distillation (KD) is a valuable technique for compressing large deep learning models into smaller, edge-suitable networks. However, conventional KD frameworks rely on pre-trained high-capacity teacher networks, which introduce significant challenges such as increased memory/storage requirements, additional training costs, and ambiguity in selecting an appropriate teacher for a given student model. Although a teacher-free distillation (self-distillation) has emerged as a promising alternative, many existing approaches still rely on architectural modifications or complex training procedures, which limit their generality and efficiency.
>   To address these limitations, we propose a novel framework based on teacher-free distillation that operates using a single student network without any auxiliary components, architectural modifications, or additional learnable parameters. Our approach is built on a simple yet highly effective augmentation, called intra-class patch swap augmentation. This augmentation simulates a teacher-student dynamic within a single model by generating pairs of intra-class samples with varying confidence levels, and then applying instance-to-instance distillation to align their predictive distributions. Our method is conceptually simple, model-agnostic, and easy to implement, requiring only a single augmentation function. Extensive experiments across image classification, semantic segmentation, and object detection show that our method consistently outperforms both existing self-distillation baselines and conventional teacher-based KD approaches. These results suggest that the success of self-distillation could hinge on the design of the augmentation itself. Our codes are available at https://github.com/hchoi71/Intra-class-Patch-Swap.

