---
layout: default
title: Diving into the Fusion of Monocular Priors for Generalized Stereo Matching
---

# Diving into the Fusion of Monocular Priors for Generalized Stereo Matching

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14414" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.14414v2</a>
  <a href="https://arxiv.org/pdf/2505.14414.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14414v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14414v2', 'Diving into the Fusion of Monocular Priors for Generalized Stereo Matching')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Chengtang Yao, Lidong Yu, Zhidan Liu, Jiaxi Zeng, Yuwei Wu, Yunde Jia

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-20 (Êõ¥Êñ∞: 2025-08-18)

**Â§áÊ≥®**: Code: https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching

**ÊúüÂàä**: ICCV 2025 Oral

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∫åÂÖÉÂ±ÄÈÉ®ÊéíÂ∫èÂõæ‰ª•Ëß£ÂÜ≥Á´ã‰ΩìÂåπÈÖç‰∏≠ÁöÑÂçïÁõÆÂÖàÈ™åËûçÂêàÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Á´ã‰ΩìÂåπÈÖç` `ÂçïÁõÆÂÖàÈ™å` `Ê∑±Â∫¶Â≠¶‰π†` `ËÆ°ÁÆóÊú∫ËßÜËßâ` `ÂõæÂÉèÂ§ÑÁêÜ` `ËßÜËßâÂü∫Á°ÄÊ®°Âûã` `Â±ÄÈÉ®ÊéíÂ∫èÂõæ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ´ã‰ΩìÂåπÈÖçÊñπÊ≥ïÂú®Â§ÑÁêÜÈÅÆÊå°ÂíåÈùûÊúó‰ºØË°®Èù¢Á≠â‰∏çËâØÂå∫ÂüüÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂØºËá¥ÂåπÈÖçÁªìÊûú‰∏çÂáÜÁ°Æ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøá‰∫åÂÖÉÂ±ÄÈÉ®ÊéíÂ∫èÂõæÊù•ÊåáÂØºÂçïÁõÆÂÖàÈ™åÁöÑËûçÂêàÔºåËß£ÂÜ≥‰∫ÜÊ∑±Â∫¶Âõæ‰∏éËßÜÂ∑Æ‰πãÈó¥ÁöÑÂØπÈΩêÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜÁ´ã‰ΩìÂåπÈÖçÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÈ´òÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á´ã‰ΩìÂåπÈÖçÂú®Â§ÑÁêÜÈÅÆÊå°ÂíåÈùûÊúó‰ºØË°®Èù¢Á≠â‰∏çËâØÂå∫ÂüüÊó∂Èù¢‰∏¥ÊåëÊàò„ÄÇËûçÂêàÂçïÁõÆÂÖàÈ™åÂ∑≤Ë¢´ËØÅÊòéÂØπËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÊúâÂ∏ÆÂä©Ôºå‰ΩÜ‰ªéÂ∞èÂûãÁ´ã‰ΩìÊï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†ÁöÑÂÅèÁΩÆÂçïÁõÆÂÖàÈ™åÈôêÂà∂‰∫ÜÂÖ∂Ê≥õÂåñËÉΩÂäõ„ÄÇÊú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫Ü‰ªéËßÜËßâÂü∫Á°ÄÊ®°Âûã‰∏≠Ëé∑ÂèñÁöÑÊó†ÂÅèÂçïÁõÆÂÖàÈ™åÂú®‰∏çËâØÂå∫ÂüüÁöÑÂ∫îÁî®ÔºåËØÜÂà´Âá∫ÂΩ±ÂìçËûçÂêàÁöÑ‰∏â‰∏™‰∏ªË¶ÅÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∫åÂÖÉÂ±ÄÈÉ®ÊéíÂ∫èÂõæÊù•ÊåáÂØºËûçÂêàÔºåÂ∞ÜÊ∑±Â∫¶ÂõæËΩ¨Êç¢‰∏∫‰∫åÂÖÉÁõ∏ÂØπÊ†ºÂºèÔºåÁªü‰∏ÄÁõ∏ÂØπÂíåÁªùÂØπÊ∑±Â∫¶Ë°®Á§∫ÔºåÂπ∂ÈÄöËøáÈáçÂä†ÊùÉÂàùÂßãËßÜÂ∑ÆÊõ¥Êñ∞Êù•Ëß£ÂÜ≥Â±ÄÈÉ®ÊúÄ‰ºòÂíåÂô™Â£∞ÈóÆÈ¢ò„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Â∞ÜÂçïÁõÆÊ∑±Â∫¶ÁöÑÁõ¥Êé•ËûçÂêàÂΩ¢ÂºèÂåñ‰∏∫‰∏Ä‰∏™Ê≥®ÂÜåÈóÆÈ¢òÔºåÂà©Áî®ÂÉèÁ¥†Á∫ßÁ∫øÊÄßÂõûÂΩíÊ®°ÂùóËøõË°åÂÖ®Â±ÄÂíåËá™ÈÄÇÂ∫îÂØπÈΩê„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰ªéSceneFlowÊ≥õÂåñÂà∞MiddleburyÂíåBoosterÊï∞ÊçÆÈõÜÊó∂ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåÂêåÊó∂Âá†‰πéÊ≤°ÊúâÈôç‰ΩéÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Á´ã‰ΩìÂåπÈÖç‰∏≠Âõ†ÈÅÆÊå°ÂíåÈùûÊúó‰ºØË°®Èù¢ÂØºËá¥ÁöÑÂåπÈÖçÂõ∞ÈöæÔºåÁé∞ÊúâÊñπÊ≥ïÂõ†ÂÅèÁΩÆÂçïÁõÆÂÖàÈ™åÁöÑÈôêÂà∂ËÄåÈöæ‰ª•Ê≥õÂåñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∫åÂÖÉÂ±ÄÈÉ®ÊéíÂ∫èÂõæÔºåÈÄöËøáÂ∞ÜÊ∑±Â∫¶ÂõæËΩ¨Êç¢‰∏∫‰∫åÂÖÉÁõ∏ÂØπÊ†ºÂºèÔºåÁªü‰∏ÄÁõ∏ÂØπÂíåÁªùÂØπÊ∑±Â∫¶Ë°®Á§∫Ôºå‰ªéËÄåÊåáÂØºÂçïÁõÆÂÖàÈ™åÁöÑÊúâÊïàËûçÂêà„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàÔºåËÆ°ÁÆóÂçïÁõÆÊ∑±Â∫¶ÂõæÔºõÂÖ∂Ê¨°ÔºåÂà©Áî®‰∫åÂÖÉÂ±ÄÈÉ®ÊéíÂ∫èÂõæÈáçÂä†ÊùÉÂàùÂßãËßÜÂ∑ÆÊõ¥Êñ∞ÔºõÊúÄÂêéÔºåÂ∞ÜÂçïÁõÆÊ∑±Â∫¶‰∏éËßÜÂ∑ÆÁöÑÁõ¥Êé•ËûçÂêàÂΩ¢ÂºèÂåñ‰∏∫Ê≥®ÂÜåÈóÆÈ¢òÔºå‰ΩøÁî®Á∫øÊÄßÂõûÂΩíÊ®°ÂùóËøõË°åÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰∫åÂÖÉÂ±ÄÈÉ®ÊéíÂ∫èÂõæÔºåËøô‰∏ÄÊñπÊ≥ïÊúâÊïàËß£ÂÜ≥‰∫ÜÊ∑±Â∫¶Âõæ‰∏éËßÜÂ∑Æ‰πãÈó¥ÁöÑÂØπÈΩêÈóÆÈ¢òÔºåÂπ∂ÁºìËß£‰∫ÜÂ±ÄÈÉ®ÊúÄ‰ºòÂíåÂô™Â£∞ÂΩ±Âìç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®‰∫ÜÈÄÇÂ∫îÊÄßÈáçÂä†ÊùÉÊú∫Âà∂ÔºåÂπ∂ËÆæËÆ°‰∫ÜÂÉèÁ¥†Á∫ßÁ∫øÊÄßÂõûÂΩíÊ®°ÂùóÔºå‰ª•Á°Æ‰øùÊ∑±Â∫¶‰∏éËßÜÂ∑ÆÁöÑÂÖ®Â±ÄÂØπÈΩê„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®SceneFlowÂà∞MiddleburyÂíåBoosterÊï∞ÊçÆÈõÜÁöÑÊ≥õÂåñËøáÁ®ã‰∏≠ÔºåÊÄßËÉΩÊòæËëóÊèêÂçáÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈ´òÊïàÊÄßÔºåÂá†‰πéÊ≤°ÊúâÈôç‰ΩéËÆ°ÁÆóÈÄüÂ∫¶„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Ëá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™ÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÂú∫ÊôØ‰∏≠ÔºåËÉΩÂ§üÊèêÈ´òÁ´ã‰ΩìËßÜËßâÁ≥ªÁªüÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÁöÑÊé®ÂπøÂèØËÉΩ‰ºöÊé®Âä®Êõ¥Â§öÂü∫‰∫éËßÜËßâÁöÑÊô∫ËÉΩÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The matching formulation makes it naturally hard for the stereo matching to handle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing monocular priors has been proven helpful for ill-posed matching, but the biased monocular prior learned from small stereo datasets constrains the generalization. Recently, stereo matching has progressed by leveraging the unbiased monocular prior from the vision foundation model (VFM) to improve the generalization in ill-posed regions. We dive into the fusion process and observe three main problems limiting the fusion of the VFM monocular prior. The first problem is the misalignment between affine-invariant relative monocular depth and absolute depth of disparity. Besides, when we use the monocular feature in an iterative update structure, the over-confidence in the disparity update leads to local optima results. A direct fusion of a monocular depth map could alleviate the local optima problem, but noisy disparity results computed at the first several iterations will misguide the fusion. In this paper, we propose a binary local ordering map to guide the fusion, which converts the depth map into a binary relative format, unifying the relative and absolute depth representation. The computed local ordering map is also used to re-weight the initial disparity update, resolving the local optima and noisy problem. In addition, we formulate the final direct fusion of monocular depth to the disparity as a registration problem, where a pixel-wise linear regression module can globally and adaptively align them. Our method fully exploits the monocular prior to support stereo matching results effectively and efficiently. We significantly improve the performance from the experiments when generalizing from SceneFlow to Middlebury and Booster datasets while barely reducing the efficiency.

