---
layout: default
title: "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning"
---

# DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14362" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14362v2</a>
  <a href="https://arxiv.org/pdf/2505.14362.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14362v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14362v2', 'DeepEyes: Incentivizing &quot;Thinking with Images&quot; via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, Xing Yu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-26)

**å¤‡æ³¨**: Ziwei, Michael, Jack, and Chenxiao are equal-contribution. The list order is random

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Visual-Agent/DeepEyes)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDeepEyesä»¥è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„è§†è§‰ä¸æ–‡æœ¬æ•´åˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·ä½¿ç”¨` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬æ¨ç†ï¼Œéš¾ä»¥å®ç°è§†è§‰ä¸æ–‡æœ¬çš„æœ‰æ•ˆæ•´åˆï¼Œé™åˆ¶äº†æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ã€‚
2. DeepEyesæ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±â€œå›¾åƒæ€ç»´â€ï¼Œå®ç°äº†è§†è§‰è¾“å…¥å¤„ç†ä¸æ¨ç†æœºåˆ¶çš„æœ‰æ•ˆç»“åˆï¼Œæå‡äº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepEyesåœ¨ç»†ç²’åº¦æ„ŸçŸ¥å’Œæ¨ç†åŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æå‡ï¼Œå¹¶åœ¨åŸºç¡€èƒ½åŠ›å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸»è¦å—é™äºåŸºäºæ–‡æœ¬çš„æ¨ç†è¿‡ç¨‹ã€‚å®ç°è§†è§‰ä¸æ–‡æœ¬æ¨ç†çš„æ— ç¼æ•´åˆï¼Œç±»ä¼¼äºäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢è®¨äº†äº¤é”™å¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œæå‡ºäº†DeepEyesæ¨¡å‹ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¿€åŠ±â€œå›¾åƒæ€ç»´â€èƒ½åŠ›ï¼Œä¸”æ— éœ€å†·å¯åŠ¨çš„SFTã€‚è¯¥èƒ½åŠ›åœ¨æ¨¡å‹å†…éƒ¨è‡ªç„¶äº§ç”Ÿï¼Œåˆ©ç”¨å…¶å›ºæœ‰çš„åŸºç¡€èƒ½åŠ›ä½œä¸ºå·¥å…·ï¼Œè€Œä¸ä¾èµ–äºå•ç‹¬çš„ä¸“ä¸šæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥å·¥å…·ä½¿ç”¨ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©æœºåˆ¶å’Œå¥–åŠ±ç­–ç•¥ï¼Œä»¥é¼“åŠ±æˆåŠŸçš„å·¥å…·è¾…åŠ©æ¨ç†è½¨è¿¹ã€‚DeepEyesåœ¨ç»†ç²’åº¦æ„ŸçŸ¥å’Œæ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨åŸºç¡€èƒ½åŠ›ã€å¹»è§‰å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ”¹å–„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­è§†è§‰ä¸æ–‡æœ¬æ•´åˆçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–æ–‡æœ¬æ¨ç†ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºDeepEyesæ¨¡å‹ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¿€åŠ±â€œå›¾åƒæ€ç»´â€ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶åœ°è¿›è¡Œè§†è§‰æ¨ç†ï¼Œè€Œæ— éœ€ä¾èµ–å†·å¯åŠ¨çš„SFTã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDeepEyesçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é€‰æ‹©æœºåˆ¶ã€å¥–åŠ±ç­–ç•¥å’Œæ¨ç†æ¨¡å—ï¼Œæ—¨åœ¨é€šè¿‡å·¥å…·ä½¿ç”¨ä¿ƒè¿›æ¨ç†è¿‡ç¨‹çš„ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šDeepEyesçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å†…ç½®çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä»¥å·¥å…·ä½¿ç”¨ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©æœºåˆ¶ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„å¥–åŠ±ç­–ç•¥ï¼Œä»¥é¼“åŠ±æ¨¡å‹æ¢ç´¢å’Œåˆ©ç”¨å·¥å…·è¿›è¡Œæœ‰æ•ˆæ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DeepEyesåœ¨ç»†ç²’åº¦æ„ŸçŸ¥å’Œæ¨ç†åŸºå‡†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æé«˜äº†15%-30%çš„å‡†ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºç¡€èƒ½åŠ›å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DeepEyesçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (VLMs) have shown strong capabilities in multimodal understanding and reasoning, yet they are primarily constrained by text-based reasoning processes. However, achieving seamless integration of visual and textual reasoning which mirrors human cognitive processes remains a significant challenge. In particular, effectively incorporating advanced visual input processing into reasoning mechanisms is still an open question. Thus, in this paper, we explore the interleaved multimodal reasoning paradigm and introduce DeepEyes, a model with "thinking with images" capabilities incentivized through end-to-end reinforcement learning without the need for cold-start SFT. Notably, this ability emerges natively within the model itself, leveraging its inherent grounding ability as a tool instead of depending on separate specialized models. Specifically, we propose a tool-use-oriented data selection mechanism and a reward strategy to encourage successful tool-assisted reasoning trajectories. DeepEyes achieves significant performance gains on fine-grained perception and reasoning benchmarks and also demonstrates improvement in grounding, hallucination, and mathematical reasoning tasks. Interestingly, we observe the distinct evolution of tool-calling behavior from initial exploration to efficient and accurate exploitation, and diverse thinking patterns that closely mirror human visual reasoning processes. Code is available at https://github.com/Visual-Agent/DeepEyes.

