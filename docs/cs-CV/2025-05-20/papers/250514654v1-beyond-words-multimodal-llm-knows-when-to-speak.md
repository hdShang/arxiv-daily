---
layout: default
title: "Beyond Words: Multimodal LLM Knows When to Speak"
---

# Beyond Words: Multimodal LLM Knows When to Speak

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14654" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14654v1</a>
  <a href="https://arxiv.org/pdf/2505.14654.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14654v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14654v1', 'Beyond Words: Multimodal LLM Knows When to Speak')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: Project page: https://github.com/lzk901372/MM-When2Speak

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMM-When2Speakä»¥è§£å†³å¯¹è¯ä¸­ååº”æ—¶æœºé¢„æµ‹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å¯¹è¯ç³»ç»Ÿ` `ååº”æ—¶æœºé¢„æµ‹` `è§†è§‰éŸ³é¢‘èåˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMèŠå¤©æœºå™¨äººåœ¨å¯¹è¯ä¸­éš¾ä»¥æŠŠæ¡ä½•æ—¶å‘è¨€ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿååº”çš„åœºæ™¯ä¸­è¡¨ç°ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºMM-When2Speakæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆè§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå®æ—¶é¢„æµ‹å¯¹è¯ä¸­çš„ååº”æ—¶æœºå’Œç±»å‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMM-When2Speakåœ¨ååº”æ—¶æœºå‡†ç¡®æ€§ä¸Šæ¯”é¢†å…ˆçš„å•†ä¸šLLMæå‡äº†4å€ï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€è¾“å…¥çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŠå¤©æœºå™¨äººåœ¨ç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„å›å¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç†è§£ä½•æ—¶å‘è¨€ï¼Œå°¤å…¶æ˜¯åœ¨æŒç»­å¯¹è¯ä¸­æä¾›ç®€çŸ­åŠæ—¶çš„ååº”æ—¶ä»å­˜åœ¨å›°éš¾ã€‚è¯¥ç ”ç©¶èšç„¦äºå®æ—¶é¢„æµ‹ååº”ç±»å‹ï¼Œå¼ºè°ƒä¾èµ–äºè§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ç­‰å¤šæ¨¡æ€ä¿¡å·çš„çŸ­æœŸååº”ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®å¯¹è¯è§†é¢‘ä¸­æ—¶é—´å¯¹é½çš„è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æµã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæå‡ºäº†MM-When2Speakæ¨¡å‹ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”æ•´åˆå¤šç§ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé¢„æµ‹ä½•æ—¶åº”ä½œå‡ºååº”åŠé€‚å½“çš„ååº”ç±»å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ååº”æ—¶æœºå‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å•æ¨¡æ€å’ŒLLMåŸºçº¿ï¼Œæœ€é«˜æå‡è¾¾4å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ä¸­ååº”æ—¶æœºé¢„æµ‹çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬è¾“å…¥ï¼Œç¼ºä¹å¯¹çœŸå®å¯¹è¯ä¸­ä¸°å¯Œä¸Šä¸‹æ–‡çº¿ç´¢çš„ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤šæ¨¡æ€ä¿¡å·ï¼ˆè§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ï¼‰ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹MM-When2Speakï¼Œèƒ½å¤Ÿå®æ—¶é¢„æµ‹ä½•æ—¶åº”ä½œå‡ºååº”ä»¥åŠååº”çš„ç±»å‹ï¼Œä»è€Œæå‡å¯¹è¯çš„è‡ªç„¶æ€§å’ŒåŠæ—¶æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–å’Œååº”é¢„æµ‹ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é¢„å¤„ç†é˜¶æ®µå¯¹å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¯¹é½ï¼Œç‰¹å¾æå–é˜¶æ®µåˆ™åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æå–è§†è§‰å’ŒéŸ³é¢‘ç‰¹å¾ï¼Œæœ€ååœ¨ååº”é¢„æµ‹é˜¶æ®µç»“åˆè¿™äº›ç‰¹å¾è¿›è¡Œå†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šMM-When2Speakçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€èåˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡ååº”æ—¶æœºçš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„ï¼Œç»“åˆäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºç‰¹å¾ä¹‹é—´çš„å…³è”æ€§ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†æ—¶é—´æ•æ„Ÿæ€§ï¼Œä»¥ä¼˜åŒ–ååº”æ—¶æœºçš„é¢„æµ‹æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMM-When2Speakåœ¨ååº”æ—¶æœºçš„å‡†ç¡®æ€§ä¸Šæ¯”ç°æœ‰çš„å•æ¨¡æ€å’ŒLLMåŸºçº¿æ¨¡å‹æ˜¾è‘—æå‡ï¼Œæœ€é«˜è¾¾4å€çš„å‡†ç¡®æ€§æå‡ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€è¾“å…¥åœ¨å¯¹è¯ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€è™šæ‹ŸåŠ©æ‰‹å’Œç¤¾äº¤æœºå™¨äººç­‰ï¼Œèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨ä¸ç”¨æˆ·äº’åŠ¨æ—¶çš„è‡ªç„¶æ€§å’Œå“åº”é€Ÿåº¦ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„å¯¹è¯ç³»ç»Ÿä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI.

