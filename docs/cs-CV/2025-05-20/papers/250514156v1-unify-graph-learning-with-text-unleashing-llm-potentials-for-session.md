---
layout: default
title: "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search"
---

# Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14156" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14156v1</a>
  <a href="https://arxiv.org/pdf/2505.14156.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14156v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14156v1', 'Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan

**åˆ†ç±»**: cs.CV, cs.AI, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**DOI**: [10.1145/3589334.3645574](https://doi.org/10.1145/3589334.3645574)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¬¦å·å›¾æ’åºå™¨ä»¥è§£å†³ä¼šè¯æœç´¢ä¸­çš„ä¿¡æ¯ç»“æ„å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¼šè¯æœç´¢` `å›¾å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç›‘ç£å­¦ä¹ ` `ä¿¡æ¯æ£€ç´¢` `ç¬¦å·è¯­æ³•` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¼šè¯æœç´¢æ–¹æ³•å¤šé›†ä¸­äºé¡ºåºå»ºæ¨¡ï¼Œå¿½è§†äº†äº¤äº’ä¸­çš„å›¾ç»“æ„ï¼Œå¯¼è‡´ä¿¡æ¯å»ºæ¨¡ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºç¬¦å·å›¾æ’åºå™¨ï¼ˆSGRï¼‰ï¼Œé€šè¿‡ç¬¦å·è¯­æ³•è§„åˆ™å°†ä¼šè¯å›¾è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç»“åˆLLMsçš„èƒ½åŠ›è¿›è¡Œä¿¡æ¯å¤„ç†ã€‚
3. åœ¨AOLå’ŒTiangong-STæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSGRåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼šè¯æœç´¢æ¶‰åŠä¸€ç³»åˆ—äº¤äº’æŸ¥è¯¢å’Œæ“ä½œï¼Œä»¥æ»¡è¶³ç”¨æˆ·å¤æ‚çš„ä¿¡æ¯éœ€æ±‚ã€‚ç°æœ‰ç­–ç•¥é€šå¸¸ä¼˜å…ˆè€ƒè™‘é¡ºåºå»ºæ¨¡ï¼Œå¿½è§†äº¤äº’ä¸­çš„å›¾ç»“æ„ã€‚æœ¬æ–‡æå‡ºç¬¦å·å›¾æ’åºå™¨ï¼ˆSGRï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œç»“åˆæ–‡æœ¬å’Œå›¾å½¢æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥ä¸€ç»„ç¬¦å·è¯­æ³•è§„åˆ™ï¼Œå°†ä¼šè¯å›¾è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œä»è€Œæ— ç¼æ•´åˆä¼šè¯å†å²ã€äº¤äº’è¿‡ç¨‹å’Œä»»åŠ¡æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹LLMsåœ¨æ–‡æœ¬è¯­æ–™ä¸Šé¢„è®­ç»ƒä¸æˆ‘ä»¬ç”Ÿæˆçš„ç¬¦å·è¯­è¨€ä¹‹é—´çš„è‡ªç„¶å·®å¼‚ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç³»åˆ—è‡ªç›‘ç£ç¬¦å·å­¦ä¹ ä»»åŠ¡ï¼Œä»¥å¢å¼ºLLMsæ•æ‰å›¾ç»“æ„çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AOLå’ŒTiangong-STä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼šè¯æœç´¢ä¸­ä¿¡æ¯å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–é¡ºåºå»ºæ¨¡ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨äº¤äº’ä¸­çš„å›¾ç»“æ„ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºç¬¦å·å›¾æ’åºå™¨ï¼ˆSGRï¼‰ï¼Œé€šè¿‡ç¬¦å·è¯­æ³•è§„åˆ™å°†ä¼šè¯å›¾è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ·±åº¦è¯­ä¹‰ç†è§£ï¼Œä»è€Œå®ç°å›¾ç»“æ„ä¸æ–‡æœ¬ä¿¡æ¯çš„æœ‰æ•ˆç»“åˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSGRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç¬¦å·è¯­æ³•è§„åˆ™ç”Ÿæˆæ¨¡å—ã€LLMè¾“å…¥æ•´åˆæ¨¡å—å’Œè‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡æ¨¡å—ã€‚é¦–å…ˆï¼Œå°†ä¼šè¯å›¾è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œç„¶åå°†å…¶è¾“å…¥LLMè¿›è¡Œå¤„ç†ï¼Œæœ€åé€šè¿‡è‡ªç›‘ç£ä»»åŠ¡å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥ç¬¦å·è¯­æ³•è§„åˆ™ï¼Œå°†å›¾ç»“æ„ä¿¡æ¯ä»¥æ–‡æœ¬å½¢å¼å‘ˆç°ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å’Œç†è§£å›¾ç»“æ„ä¿¡æ¯ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å›¾è¡¨ç¤ºæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼ŒåŒ…æ‹¬é“¾æ¥é¢„æµ‹ã€èŠ‚ç‚¹å†…å®¹ç”Ÿæˆå’Œç”Ÿæˆå¯¹æ¯”å­¦ä¹ ï¼Œä»¥ä¿ƒè¿›LLMsä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦æ•æ‰æ‹“æ‰‘ä¿¡æ¯ï¼ŒæŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSGRåœ¨AOLå’ŒTiangong-STæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼ŒéªŒè¯äº†å…¶åœ¨ä¼šè¯æœç´¢ä¸­çš„æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¿¡æ¯æ£€ç´¢ã€æ™ºèƒ½åŠ©æ‰‹å’Œä¸ªæ€§åŒ–æ¨èç³»ç»Ÿç­‰ã€‚é€šè¿‡æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„ä¼šè¯å†å²å’Œéœ€æ±‚ï¼Œèƒ½å¤Ÿæä¾›æ›´ç²¾å‡†çš„æœç´¢ç»“æœå’Œæ¨èï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†å’Œå¤æ‚æŸ¥è¯¢ç†è§£ç­‰é¢†åŸŸå‘æŒ¥æ›´å¤§ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.

