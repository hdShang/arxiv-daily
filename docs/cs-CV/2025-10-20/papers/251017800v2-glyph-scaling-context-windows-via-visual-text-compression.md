---
layout: default
title: Glyph: Scaling Context Windows via Visual-Text Compression
---

# Glyph: Scaling Context Windows via Visual-Text Compression

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.17800" target="_blank" class="toolbar-btn">arXiv: 2510.17800v2</a>
    <a href="https://arxiv.org/pdf/2510.17800.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17800v2" 
            onclick="toggleFavorite(this, '2510.17800v2', 'Glyph: Scaling Context Windows via Visual-Text Compression')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang

**ÂàÜÁ±ª**: cs.CV, cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-20 (Êõ¥Êñ∞: 2025-10-21)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/thu-coai/Glyph)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**GlyphÔºöÈÄöËøáËßÜËßâ-ÊñáÊú¨ÂéãÁº©Êâ©Â±ïÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÊñáÊú¨Â§ÑÁêÜ` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `ÊñáÊú¨ÂéãÁº©` `‰∏ä‰∏ãÊñáÁ™óÂè£` `ÈÅó‰º†ÊêúÁ¥¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜÈïøÊñáÊú¨Êó∂ÔºåËÆ°ÁÆóÂíåÂÜÖÂ≠òÊàêÊú¨Èöè‰∏ä‰∏ãÊñáÁ™óÂè£Á∫øÊÄßÂ¢ûÈïøÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ
2. GlyphÂ∞ÜÈïøÊñáÊú¨Ê∏≤Êüì‰∏∫ÂõæÂÉèÔºåÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜÔºåÂÆûÁé∞ÊñáÊú¨ÂéãÁº©Âπ∂‰øùÁïôËØ≠‰πâ‰ø°ÊÅØ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåGlyphÂú®‰øùÊåÅÂáÜÁ°ÆÊÄßÁöÑÂâçÊèê‰∏ãÔºåÂÆûÁé∞‰∫Ü3-4ÂÄçÁöÑtokenÂéãÁº©ÂíåÊòæËëóÁöÑËÆ≠ÁªÉÂä†ÈÄü„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâË∂äÊù•Ë∂ä‰æùËµñÈïø‰∏ä‰∏ãÊñáÂª∫Ê®°Êù•Â§ÑÁêÜÊñáÊ°£ÁêÜËß£„ÄÅ‰ª£Á†ÅÂàÜÊûêÂíåÂ§öÊ≠•È™§Êé®ÁêÜÁ≠â‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåÂ∞Ü‰∏ä‰∏ãÊñáÁ™óÂè£Êâ©Â±ïÂà∞Áôæ‰∏átokenÁ∫ßÂà´‰ºöÂ∏¶Êù•Â∑®Â§ßÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÊàêÊú¨ÔºåÈôêÂà∂‰∫ÜÈïø‰∏ä‰∏ãÊñáLLMÁöÑÂÆûÁî®ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏çÂêåÁöÑËßÜËßí‚Äî‚ÄîËßÜËßâ‰∏ä‰∏ãÊñáÁº©Êîæ‚Äî‚ÄîÊù•Â∫îÂØπËøô‰∏ÄÊåëÊàò„ÄÇÊàë‰ª¨Ê≤°ÊúâÊâ©Â±ïÂü∫‰∫étokenÁöÑÂ∫èÂàóÔºåËÄåÊòØÊèêÂá∫‰∫ÜGlyphÔºå‰∏Ä‰∏™Â∞ÜÈïøÊñáÊú¨Ê∏≤ÊüìÊàêÂõæÂÉèÂπ∂‰ΩøÁî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂ§ÑÁêÜÂÆÉ‰ª¨ÁöÑÊ°ÜÊû∂„ÄÇËøôÁßçÊñπÊ≥ïÂú®‰øùÁïôËØ≠‰πâ‰ø°ÊÅØÁöÑÂêåÊó∂ÔºåÂ§ßÂ§ßÂéãÁº©‰∫ÜÊñáÊú¨ËæìÂÖ•„ÄÇÊàë‰ª¨ËøòËÆæËÆ°‰∫Ü‰∏ÄÁßçÁî±LLMÈ©±Âä®ÁöÑÈÅó‰º†ÊêúÁ¥¢Ôºå‰ª•ËØÜÂà´Áî®‰∫éÂπ≥Ë°°ÂáÜÁ°ÆÊÄßÂíåÂéãÁº©ÁöÑÊúÄ‰Ω≥ËßÜËßâÊ∏≤ÊüìÈÖçÁΩÆ„ÄÇÈÄöËøáÂ§ßÈáèÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÂÆûÁé∞‰∫Ü3-4ÂÄçÁöÑtokenÂéãÁº©ÔºåÂêåÊó∂Âú®ÂêÑÁßçÈïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠‰øùÊåÅ‰∫Ü‰∏éQwen3-8BÁ≠âÈ¢ÜÂÖàLLMÁõ∏ÂΩìÁöÑÂáÜÁ°ÆÊÄß„ÄÇËøôÁßçÂéãÁº©ËøòÂ∏¶Êù•‰∫ÜÂ§ßÁ∫¶4ÂÄçÁöÑÈ¢ÑÂ°´ÂÖÖÂíåËß£Á†ÅÈÄüÂ∫¶ÊèêÂçáÔºå‰ª•ÂèäÂ§ßÁ∫¶2ÂÄçÁöÑSFTËÆ≠ÁªÉÈÄüÂ∫¶ÊèêÂçá„ÄÇÊ≠§Â§ñÔºåÂú®ÊûÅÁ´ØÂéãÁº©‰∏ãÔºå‰∏Ä‰∏™128K‰∏ä‰∏ãÊñáÁöÑVLMÂèØ‰ª•Êâ©Â±ïÂà∞Â§ÑÁêÜÁôæ‰∏átokenÁ∫ßÂà´ÁöÑÊñáÊú¨‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåÊ∏≤ÊüìÁöÑÊñáÊú¨Êï∞ÊçÆÊúâÂà©‰∫éÁé∞ÂÆû‰∏ñÁïåÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°Ôºå‰æãÂ¶ÇÊñáÊ°£ÁêÜËß£„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÊñáÊú¨Êó∂ÔºåÈúÄË¶ÅÊ∂àËÄóÂ§ßÈáèÁöÑËÆ°ÁÆóËµÑÊ∫êÂíåÂÜÖÂ≠òÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÈÄöÂ∏∏‰∏é‰∏ä‰∏ãÊñáÈïøÂ∫¶ÊàêÊ≠£ÊØî„ÄÇËøô‰ΩøÂæóÂ∞Ü‰∏ä‰∏ãÊñáÁ™óÂè£Êâ©Â±ïÂà∞Áôæ‰∏átokenÁ∫ßÂà´ÂèòÂæóÈùûÂ∏∏Âõ∞ÈöæÔºåÈòªÁ¢ç‰∫ÜÈïø‰∏ä‰∏ãÊñáLLMÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ºòÂåñTransformerÊû∂ÊûÑÊàñ‰ΩøÁî®Á®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩÜÊïàÊûúÊúâÈôêÔºå‰∏îÂæÄÂæÄÈúÄË¶ÅÂ§çÊùÇÁöÑÂ∑•Á®ãÂÆûÁé∞„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöGlyphÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÈïøÊñáÊú¨ËΩ¨Êç¢‰∏∫ÂõæÂÉèÔºåÁÑ∂ÂêéÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÊù•Â§ÑÁêÜËøô‰∫õÂõæÂÉè„ÄÇÈÄöËøáÂ∞ÜÊñáÊú¨Ê∏≤ÊüìÊàêÂõæÂÉèÔºåÂèØ‰ª•Âà©Áî®ÂõæÂÉèÁöÑÂéãÁº©ÁâπÊÄßÔºå‰ªéËÄåÂú®‰øùÁïôËØ≠‰πâ‰ø°ÊÅØÁöÑÂêåÊó∂ÔºåÊòæËëóÂáèÂ∞ëÈúÄË¶ÅÂ§ÑÁêÜÁöÑÊï∞ÊçÆÈáè„ÄÇËøôÁßçÊñπÊ≥ïÂ∞ÜÊñáÊú¨Â§ÑÁêÜÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÂõæÂÉèÂ§ÑÁêÜÈóÆÈ¢òÔºå‰ªéËÄåÂèØ‰ª•Âà©Áî®VLMÂú®ÂõæÂÉèÁêÜËß£ÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGlyphÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ÊñáÊú¨Ê∏≤ÊüìÔºöÂ∞ÜÈïøÊñáÊú¨Ê∏≤ÊüìÊàêÂõæÂÉèÔºåÂèØ‰ª•‰ΩøÁî®‰∏çÂêåÁöÑÂ≠ó‰Ωì„ÄÅÈ¢úËâ≤„ÄÅÂ∏ÉÂ±ÄÁ≠âËßÜËßâÂÖÉÁ¥†„ÄÇ2) ËßÜËßâÁºñÁ†ÅÔºö‰ΩøÁî®VLMÂØπÊ∏≤ÊüìÂêéÁöÑÂõæÂÉèËøõË°åÁºñÁ†ÅÔºåÊèêÂèñËßÜËßâÁâπÂæÅ„ÄÇ3) ËØ≠Ë®ÄËß£Á†ÅÔºöÂ∞ÜËßÜËßâÁâπÂæÅËæìÂÖ•Âà∞ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÔºåÁîüÊàêÊñáÊú¨ËæìÂá∫„ÄÇ4) ÈÅó‰º†ÊêúÁ¥¢Ôºö‰ΩøÁî®LLMÈ©±Âä®ÁöÑÈÅó‰º†ÊêúÁ¥¢ÁÆóÊ≥ïÔºåËá™Âä®ÂØªÊâæÊúÄ‰Ω≥ÁöÑËßÜËßâÊ∏≤ÊüìÈÖçÁΩÆÔºå‰ª•Âπ≥Ë°°ÂáÜÁ°ÆÊÄßÂíåÂéãÁº©Áéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGlyphÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂÆÉÂ∞ÜÊñáÊú¨Â§ÑÁêÜÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÂõæÂÉèÂ§ÑÁêÜÈóÆÈ¢òÔºåÂπ∂Âà©Áî®VLMÊù•Â§ÑÁêÜÈïøÊñáÊú¨„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫étokenÁöÑÊñáÊú¨Â§ÑÁêÜÊñπÊ≥ïÁõ∏ÊØîÔºåGlyphÂèØ‰ª•ÂÆûÁé∞Êõ¥È´òÁöÑÂéãÁº©ÁéáÔºåÂπ∂‰∏îÂèØ‰ª•Âà©Áî®VLMÂú®ÂõæÂÉèÁêÜËß£ÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇÊ≠§Â§ñÔºåGlyphËøò‰ΩøÁî®LLMÈ©±Âä®ÁöÑÈÅó‰º†ÊêúÁ¥¢ÁÆóÊ≥ïÊù•Ëá™Âä®‰ºòÂåñËßÜËßâÊ∏≤ÊüìÈÖçÁΩÆÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊèêÈ´òÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊñáÊú¨Ê∏≤ÊüìÈò∂ÊÆµÔºåGlyph‰ΩøÁî®‰∫ÜÂ§öÁßçÂ≠ó‰Ωì„ÄÅÈ¢úËâ≤ÂíåÂ∏ÉÂ±ÄÈÄâÈ°πÔºåÂπ∂‰ΩøÁî®ÈÅó‰º†ÊêúÁ¥¢ÁÆóÊ≥ïÊù•ÂØªÊâæÊúÄ‰Ω≥ÁöÑÊ∏≤ÊüìÈÖçÁΩÆ„ÄÇÂú®ËßÜËßâÁºñÁ†ÅÈò∂ÊÆµÔºåGlyph‰ΩøÁî®‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑVLMÔºå‰æãÂ¶ÇCLIPÊàñALIGN„ÄÇÂú®ËØ≠Ë®ÄËß£Á†ÅÈò∂ÊÆµÔºåGlyph‰ΩøÁî®‰∫ÜÊ†áÂáÜÁöÑTransformerËß£Á†ÅÂô®„ÄÇÊçüÂ§±ÂáΩÊï∞‰∏ªË¶ÅÂåÖÊã¨‰∫§ÂèâÁÜµÊçüÂ§±ÂíåÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñVLMÂíåËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGlyphÂú®ÂêÑÁßçÈïø‰∏ä‰∏ãÊñáÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫Ü3-4ÂÄçÁöÑtokenÂéãÁº©ÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰∏éQwen3-8BÁ≠âÈ¢ÜÂÖàLLMÁõ∏ÂΩìÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊ≠§Â§ñÔºåGlyphËøòÂ∏¶Êù•‰∫ÜÂ§ßÁ∫¶4ÂÄçÁöÑÈ¢ÑÂ°´ÂÖÖÂíåËß£Á†ÅÈÄüÂ∫¶ÊèêÂçáÔºå‰ª•ÂèäÂ§ßÁ∫¶2ÂÄçÁöÑSFTËÆ≠ÁªÉÈÄüÂ∫¶ÊèêÂçá„ÄÇÂú®ÊûÅÁ´ØÂéãÁº©‰∏ãÔºå‰∏Ä‰∏™128K‰∏ä‰∏ãÊñáÁöÑVLMÂèØ‰ª•Êâ©Â±ïÂà∞Â§ÑÁêÜÁôæ‰∏átokenÁ∫ßÂà´ÁöÑÊñáÊú¨‰ªªÂä°„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

GlyphÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂåÖÊã¨ÊñáÊ°£ÁêÜËß£„ÄÅ‰ª£Á†ÅÂàÜÊûê„ÄÅÂ§öÊ≠•È™§Êé®ÁêÜÁ≠âÈúÄË¶ÅÂ§ÑÁêÜÈïøÊñáÊú¨ÁöÑ‰ªªÂä°„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•Èôç‰ΩéÈïøÊñáÊú¨Â§ÑÁêÜÁöÑËÆ°ÁÆóÊàêÊú¨ÂíåÂÜÖÂ≠òÈúÄÊ±ÇÔºå‰ΩøÂæóÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÂ§ÑÁêÜÊõ¥ÈïøÁöÑ‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÊèêÈ´òÂÖ∂ÊÄßËÉΩÂíåÂÆûÁî®ÊÄß„ÄÇÊ≠§Â§ñÔºåGlyphËøòÂèØ‰ª•Â∫îÁî®‰∫éÂ§öÊ®°ÊÄÅ‰ªªÂä°Ôºå‰æãÂ¶ÇÂ∞ÜÊñáÊú¨ÂíåÂõæÂÉèÁªìÂêàËµ∑Êù•ËøõË°åÊñáÊ°£ÁêÜËß£„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.

