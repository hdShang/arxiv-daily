---
layout: default
title: Glyph: Scaling Context Windows via Visual-Text Compression
---

# Glyph: Scaling Context Windows via Visual-Text Compression

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17800" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17800v2</a>
  <a href="https://arxiv.org/pdf/2510.17800.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17800v2" onclick="toggleFavorite(this, '2510.17800v2', 'Glyph: Scaling Context Windows via Visual-Text Compression')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20 (æ›´æ–°: 2025-10-21)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/thu-coai/Glyph)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Glyphï¼šé€šè¿‡è§†è§‰-æ–‡æœ¬å‹ç¼©æ‰©å±•å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿æ–‡æœ¬å¤„ç†` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ–‡æœ¬å‹ç¼©` `ä¸Šä¸‹æ–‡çª—å£` `é—ä¼ æœç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œè®¡ç®—å’Œå†…å­˜æˆæœ¬éšä¸Šä¸‹æ–‡çª—å£çº¿æ€§å¢é•¿ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. Glyphå°†é•¿æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å¤„ç†ï¼Œå®ç°æ–‡æœ¬å‹ç¼©å¹¶ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGlyphåœ¨ä¿æŒå‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œå®ç°äº†3-4å€çš„tokenå‹ç¼©å’Œæ˜¾è‘—çš„è®­ç»ƒåŠ é€Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šä¾èµ–é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡æ¥å¤„ç†æ–‡æ¡£ç†è§£ã€ä»£ç åˆ†æå’Œå¤šæ­¥éª¤æ¨ç†ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°†ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ°ç™¾ä¸‡tokençº§åˆ«ä¼šå¸¦æ¥å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ï¼Œé™åˆ¶äº†é•¿ä¸Šä¸‹æ–‡LLMçš„å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸åŒçš„è§†è§’â€”â€”è§†è§‰ä¸Šä¸‹æ–‡ç¼©æ”¾â€”â€”æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ²¡æœ‰æ‰©å±•åŸºäºtokençš„åºåˆ—ï¼Œè€Œæ˜¯æå‡ºäº†Glyphï¼Œä¸€ä¸ªå°†é•¿æ–‡æœ¬æ¸²æŸ“æˆå›¾åƒå¹¶ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¤„ç†å®ƒä»¬çš„æ¡†æ¶ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿ç•™è¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œå¤§å¤§å‹ç¼©äº†æ–‡æœ¬è¾“å…¥ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ç”±LLMé©±åŠ¨çš„é—ä¼ æœç´¢ï¼Œä»¥è¯†åˆ«ç”¨äºå¹³è¡¡å‡†ç¡®æ€§å’Œå‹ç¼©çš„æœ€ä½³è§†è§‰æ¸²æŸ“é…ç½®ã€‚é€šè¿‡å¤§é‡çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†3-4å€çš„tokenå‹ç¼©ï¼ŒåŒæ—¶åœ¨å„ç§é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ä¸Qwen3-8Bç­‰é¢†å…ˆLLMç›¸å½“çš„å‡†ç¡®æ€§ã€‚è¿™ç§å‹ç¼©è¿˜å¸¦æ¥äº†å¤§çº¦4å€çš„é¢„å¡«å……å’Œè§£ç é€Ÿåº¦æå‡ï¼Œä»¥åŠå¤§çº¦2å€çš„SFTè®­ç»ƒé€Ÿåº¦æå‡ã€‚æ­¤å¤–ï¼Œåœ¨æç«¯å‹ç¼©ä¸‹ï¼Œä¸€ä¸ª128Kä¸Šä¸‹æ–‡çš„VLMå¯ä»¥æ‰©å±•åˆ°å¤„ç†ç™¾ä¸‡tokençº§åˆ«çš„æ–‡æœ¬ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæ¸²æŸ“çš„æ–‡æœ¬æ•°æ®æœ‰åˆ©äºç°å®ä¸–ç•Œçš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æ¡£ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œéœ€è¦æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºå’Œå†…å­˜ï¼Œå› ä¸ºå®ƒä»¬çš„è®¡ç®—å¤æ‚åº¦é€šå¸¸ä¸ä¸Šä¸‹æ–‡é•¿åº¦æˆæ­£æ¯”ã€‚è¿™ä½¿å¾—å°†ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ°ç™¾ä¸‡tokençº§åˆ«å˜å¾—éå¸¸å›°éš¾ï¼Œé˜»ç¢äº†é•¿ä¸Šä¸‹æ–‡LLMåœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–Transformeræ¶æ„æˆ–ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†æ•ˆæœæœ‰é™ï¼Œä¸”å¾€å¾€éœ€è¦å¤æ‚çš„å·¥ç¨‹å®ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGlyphçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é•¿æ–‡æœ¬è½¬æ¢ä¸ºå›¾åƒï¼Œç„¶ååˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥å¤„ç†è¿™äº›å›¾åƒã€‚é€šè¿‡å°†æ–‡æœ¬æ¸²æŸ“æˆå›¾åƒï¼Œå¯ä»¥åˆ©ç”¨å›¾åƒçš„å‹ç¼©ç‰¹æ€§ï¼Œä»è€Œåœ¨ä¿ç•™è¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘éœ€è¦å¤„ç†çš„æ•°æ®é‡ã€‚è¿™ç§æ–¹æ³•å°†æ–‡æœ¬å¤„ç†é—®é¢˜è½¬åŒ–ä¸ºå›¾åƒå¤„ç†é—®é¢˜ï¼Œä»è€Œå¯ä»¥åˆ©ç”¨VLMåœ¨å›¾åƒç†è§£æ–¹é¢çš„ä¼˜åŠ¿ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGlyphæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ–‡æœ¬æ¸²æŸ“ï¼šå°†é•¿æ–‡æœ¬æ¸²æŸ“æˆå›¾åƒï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„å­—ä½“ã€é¢œè‰²ã€å¸ƒå±€ç­‰è§†è§‰å…ƒç´ ã€‚2) è§†è§‰ç¼–ç ï¼šä½¿ç”¨VLMå¯¹æ¸²æŸ“åçš„å›¾åƒè¿›è¡Œç¼–ç ï¼Œæå–è§†è§‰ç‰¹å¾ã€‚3) è¯­è¨€è§£ç ï¼šå°†è§†è§‰ç‰¹å¾è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚4) é—ä¼ æœç´¢ï¼šä½¿ç”¨LLMé©±åŠ¨çš„é—ä¼ æœç´¢ç®—æ³•ï¼Œè‡ªåŠ¨å¯»æ‰¾æœ€ä½³çš„è§†è§‰æ¸²æŸ“é…ç½®ï¼Œä»¥å¹³è¡¡å‡†ç¡®æ€§å’Œå‹ç¼©ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šGlyphæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå®ƒå°†æ–‡æœ¬å¤„ç†é—®é¢˜è½¬åŒ–ä¸ºå›¾åƒå¤„ç†é—®é¢˜ï¼Œå¹¶åˆ©ç”¨VLMæ¥å¤„ç†é•¿æ–‡æœ¬ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºtokençš„æ–‡æœ¬å¤„ç†æ–¹æ³•ç›¸æ¯”ï¼ŒGlyphå¯ä»¥å®ç°æ›´é«˜çš„å‹ç¼©ç‡ï¼Œå¹¶ä¸”å¯ä»¥åˆ©ç”¨VLMåœ¨å›¾åƒç†è§£æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒGlyphè¿˜ä½¿ç”¨LLMé©±åŠ¨çš„é—ä¼ æœç´¢ç®—æ³•æ¥è‡ªåŠ¨ä¼˜åŒ–è§†è§‰æ¸²æŸ“é…ç½®ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–‡æœ¬æ¸²æŸ“é˜¶æ®µï¼ŒGlyphä½¿ç”¨äº†å¤šç§å­—ä½“ã€é¢œè‰²å’Œå¸ƒå±€é€‰é¡¹ï¼Œå¹¶ä½¿ç”¨é—ä¼ æœç´¢ç®—æ³•æ¥å¯»æ‰¾æœ€ä½³çš„æ¸²æŸ“é…ç½®ã€‚åœ¨è§†è§‰ç¼–ç é˜¶æ®µï¼ŒGlyphä½¿ç”¨äº†é¢„è®­ç»ƒçš„VLMï¼Œä¾‹å¦‚CLIPæˆ–ALIGNã€‚åœ¨è¯­è¨€è§£ç é˜¶æ®µï¼ŒGlyphä½¿ç”¨äº†æ ‡å‡†çš„Transformerè§£ç å™¨ã€‚æŸå¤±å‡½æ•°ä¸»è¦åŒ…æ‹¬äº¤å‰ç†µæŸå¤±å’Œå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œç”¨äºä¼˜åŒ–VLMå’Œè¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGlyphåœ¨å„ç§é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†3-4å€çš„tokenå‹ç¼©ï¼ŒåŒæ—¶ä¿æŒäº†ä¸Qwen3-8Bç­‰é¢†å…ˆLLMç›¸å½“çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒGlyphè¿˜å¸¦æ¥äº†å¤§çº¦4å€çš„é¢„å¡«å……å’Œè§£ç é€Ÿåº¦æå‡ï¼Œä»¥åŠå¤§çº¦2å€çš„SFTè®­ç»ƒé€Ÿåº¦æå‡ã€‚åœ¨æç«¯å‹ç¼©ä¸‹ï¼Œä¸€ä¸ª128Kä¸Šä¸‹æ–‡çš„VLMå¯ä»¥æ‰©å±•åˆ°å¤„ç†ç™¾ä¸‡tokençº§åˆ«çš„æ–‡æœ¬ä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Glyphå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æ–‡æ¡£ç†è§£ã€ä»£ç åˆ†æã€å¤šæ­¥éª¤æ¨ç†ç­‰éœ€è¦å¤„ç†é•¿æ–‡æœ¬çš„ä»»åŠ¡ã€‚è¯¥æ–¹æ³•å¯ä»¥é™ä½é•¿æ–‡æœ¬å¤„ç†çš„è®¡ç®—æˆæœ¬å’Œå†…å­˜éœ€æ±‚ï¼Œä½¿å¾—å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜å…¶æ€§èƒ½å’Œå®ç”¨æ€§ã€‚æ­¤å¤–ï¼ŒGlyphè¿˜å¯ä»¥åº”ç”¨äºå¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚å°†æ–‡æœ¬å’Œå›¾åƒç»“åˆèµ·æ¥è¿›è¡Œæ–‡æ¡£ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.

