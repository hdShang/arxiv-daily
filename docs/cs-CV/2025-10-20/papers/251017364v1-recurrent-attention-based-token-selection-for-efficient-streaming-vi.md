---
layout: default
title: Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs
---

# Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17364" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17364v1</a>
  <a href="https://arxiv.org/pdf/2510.17364.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17364v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17364v1', 'Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vaggelis Dorovatas, Soroush Seifi, Gunshi Gupta, Rahaf Aljundi

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¾ªç¯æ³¨æ„åŠ›çš„Tokené€‰æ‹©æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆçš„æµå¼è§†é¢‘-LLM**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æµå¼è§†é¢‘ç†è§£` `è§†é¢‘å¤§è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `Tokené€‰æ‹©` `å¾ªç¯ç¥ç»ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Video-LLMåœ¨å¤„ç†é•¿è§†é¢‘æµæ—¶ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚
2. è¯¥æ–¹æ³•åˆ©ç”¨LLMçš„æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©å…³é”®è§†è§‰tokenï¼Œå¹¶å¾ªç¯å¤„ç†ï¼Œä»è€Œå‡å°‘è®¡ç®—é‡å¹¶ä¿æŒæ—¶é—´è¿è´¯æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æµå¼è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆæ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(Video-LLM)æ“…é•¿ç†è§£è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œä½†å‰ææ˜¯å®ƒä»¬åœ¨å›ç­”é—®é¢˜æ—¶å¯ä»¥å®Œå…¨è®¿é—®è§†é¢‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æµå¼ä¼ è¾“åœºæ™¯ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦åœ¨çº¿å¤„ç†é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘ï¼Œå¹¶åŠæ—¶å“åº”é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸æ ‡å‡†Video-LLMå…¼å®¹çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨ä¸‰ä¸ªå…³é”®æ¦‚å¿µï¼š1) LLMæ„ŸçŸ¥çš„è§†è§‰tokené€‰æ‹©ï¼Œä»¥è¯†åˆ«LLMå·²å…³æ³¨å¹¶æœ‰åŠ©äºå…¶ç†è§£æ¯ä¸ªçŸ­ç‰‡æ®µçš„tokenã€‚åŸºäºæ³¨æ„åŠ›çš„é€‰æ‹©ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸¢å¼ƒé«˜è¾¾çº¦95%çš„ä¸é‡è¦è§†è§‰tokenï¼Œè€Œæ€§èƒ½æŸå¤±æœ€å°ï¼›2) å¾ªç¯å¤„ç†è¿‡å»é€‰æ‹©çš„tokenï¼Œä»¥ç”Ÿæˆå¯¹æ¯ä¸ªå·²å¤„ç†ç‰‡æ®µçš„æ—¶é—´è¿è´¯ç†è§£ï¼›3) åŸºäºå­—å¹•çš„é—®ç­”ï¼Œä»¥å®ç°è½»é‡çº§å’Œå‡†ç¡®çš„å“åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æµå¼è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ•ˆç‡å’Œæœ‰æ•ˆæ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Video-LLMåœ¨å¤„ç†æµå¼é•¿è§†é¢‘æ—¶ï¼Œéœ€è¦å¤„ç†å¤§é‡çš„è§†è§‰tokenï¼Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚å°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿå“åº”ç”¨æˆ·æé—®çš„åœºæ™¯ä¸‹ï¼Œæ•ˆç‡é—®é¢˜æ›´åŠ çªå‡ºã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¯¹æ•´ä¸ªè§†é¢‘è¿›è¡Œç¼–ç ï¼Œæ— æ³•é€‚åº”æµå¼å¤„ç†çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨LLMè‡ªèº«çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ¨æ€åœ°é€‰æ‹©å¯¹ç†è§£è§†é¢‘å†…å®¹è‡³å…³é‡è¦çš„è§†è§‰tokenï¼Œå¹¶ä¸¢å¼ƒå†—ä½™ä¿¡æ¯ã€‚é€šè¿‡å¾ªç¯å¤„ç†é€‰å®šçš„tokenï¼Œæ¨¡å‹å¯ä»¥ç»´æŠ¤å¯¹è§†é¢‘å†…å®¹çš„æ—¶é—´è¿è´¯æ€§ç†è§£ï¼Œä»è€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) **LLM-informed Token Selection**: åˆ©ç”¨LLMçš„æ³¨æ„åŠ›æƒé‡ï¼Œé€‰æ‹©å¯¹å½“å‰è§†é¢‘ç‰‡æ®µç†è§£è´¡çŒ®æœ€å¤§çš„è§†è§‰tokenã€‚2) **Recurrent Processing**: å°†é€‰å®šçš„tokenè¾“å…¥å¾ªç¯ç¥ç»ç½‘ç»œï¼Œä»¥æ•æ‰è§†é¢‘ç‰‡æ®µä¹‹é—´çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œç”Ÿæˆæ—¶é—´è¿è´¯çš„è§†é¢‘è¡¨ç¤ºã€‚3) **Caption-based Question Answering**: åŸºäºç”Ÿæˆçš„è§†é¢‘è¡¨ç¤ºå’Œé—®é¢˜ï¼Œç”Ÿæˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨LLMè‡ªèº«çš„æ³¨æ„åŠ›ä¿¡æ¯è¿›è¡Œtokené€‰æ‹©ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å¹¶ä¿ç•™å¯¹LLMç†è§£è§†é¢‘å†…å®¹è‡³å…³é‡è¦çš„tokenï¼ŒåŒæ—¶ä¸¢å¼ƒå†—ä½™ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ é«˜æ•ˆï¼Œä¸”æ˜“äºé›†æˆåˆ°ç°æœ‰çš„Video-LLMæ¡†æ¶ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šTokené€‰æ‹©æ¨¡å—ä½¿ç”¨LLMåœ¨å¤„ç†å½“å‰è§†é¢‘ç‰‡æ®µæ—¶äº§ç”Ÿçš„æ³¨æ„åŠ›æƒé‡ï¼Œé€‰æ‹©æƒé‡æœ€é«˜çš„tokenã€‚å¾ªç¯å¤„ç†æ¨¡å—å¯ä»¥ä½¿ç”¨GRUæˆ–LSTMç­‰å¾ªç¯ç¥ç»ç½‘ç»œã€‚Caption-based Question Answeringæ¨¡å—å¯ä»¥ä½¿ç”¨æ ‡å‡†çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚Transformerã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å¯ä»¥æ ¹æ®å…·ä½“çš„Video-LLMå’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨æµå¼è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼ŒåŒæ—¶èƒ½å¤Ÿä¸¢å¼ƒé«˜è¾¾95%çš„è§†è§‰tokenï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é™ä½è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶æ›´é€‚ç”¨äºå®é™…åº”ç”¨åœºæ™¯ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®å’Œå¯¹æ¯”åŸºçº¿ä¿¡æ¯éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å®æ—¶è§†é¢‘ç†è§£çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€åœ¨çº¿æ•™è‚²ã€è§†é¢‘ä¼šè®®ç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—æˆæœ¬ï¼Œè¯¥æ–¹æ³•ä½¿å¾—Video-LLMèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œå¹¶èƒ½å¤Ÿæ›´å¿«åœ°å“åº”ç”¨æˆ·æé—®ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video Large Language Models (Video-LLMs) excel at understanding videos in-context, provided they have full access to the video when answering queries. However, these models face challenges in streaming scenarios where hour-long videos must be processed online, and questions need timely responses. In this work, we propose a training-free approach compatible with standard Video-LLMs, leveraging three key concepts: 1) LLM-informed selection of visual tokens to identify those that the LLM has attended to and contributed to its understanding of each short clip. Our attention-based selection allows us to discard up to ~95% of unimportant visual tokens with minimal performance loss; 2) Recurrent processing of past selected tokens to generate temporally coherent understanding of each processed clip; 3) Caption-based question answering for lightweight and accurate responses. Our method achieves state-of-the-art performance on streaming video benchmarks, striking a balance between efficiency and effectiveness.

