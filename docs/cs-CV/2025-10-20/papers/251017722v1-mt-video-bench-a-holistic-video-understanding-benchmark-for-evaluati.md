---
layout: default
title: "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues"
---

# MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17722" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17722v1</a>
  <a href="https://arxiv.org/pdf/2510.17722.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17722v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17722v1', 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yaning Pan, Zekun Wang, Qianqian Xie, Yongqian Wen, Yuanxing Zhang, Guohui Zhang, Haoxuan Hu, Zhiyu Pan, Yibing Huang, Zhidong Gan, Yonghong Lin, An Ping, Tianhao Peng, Jiaheng Liu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: Project Website: https://github.com/NJU-LINK/MT-Video-Bench

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMT-Video-Benchï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€LLMåœ¨å¤šè½®å¯¹è¯ä¸­çš„è§†é¢‘ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†é¢‘ç†è§£` `å¤šè½®å¯¹è¯` `å¤§å‹è¯­è¨€æ¨¡å‹` `è¯„ä¼°åŸºå‡†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç†è§£è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­äºå•è½®é—®ç­”ï¼Œç¼ºä¹å¯¹å¤šè½®äº¤äº’åœºæ™¯çš„æœ‰æ•ˆè¯„ä¼°ã€‚
2. MT-Video-Benché€šè¿‡æ„å»ºåŒ…å«987ä¸ªå¤šè½®å¯¹è¯çš„åŸºå‡†ï¼Œå…¨é¢è¯„ä¼°MLLMçš„æ„ŸçŸ¥å’Œäº¤äº’èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰MLLMåœ¨å¤„ç†å¤šè½®è§†é¢‘å¯¹è¯æ–¹é¢çš„æ€§èƒ½å·®å¼‚å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›å‚è€ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æœ€æ–°å‘å±•æ˜¾è‘—æå‡äº†AIç†è§£è§†è§‰æ¨¡æ€çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†ä»ç„¶å±€é™äºå•è½®é—®ç­”ï¼Œå¿½ç•¥äº†çœŸå®åœºæ™¯ä¸­å¤šè½®å¯¹è¯çš„å¤æ‚æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MT-Video-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è§†é¢‘ç†è§£åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MLLMåœ¨å¤šè½®å¯¹è¯ä¸­çš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼ŒMT-Video-Benchä¸»è¦è¯„ä¼°å…­é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œä¾§é‡äºæ„ŸçŸ¥å’Œäº¤äº’æ€§ï¼ŒåŒ…å«æ¥è‡ªä¸åŒé¢†åŸŸçš„987ä¸ªç²¾å¿ƒç­–åˆ’çš„å¤šè½®å¯¹è¯ã€‚è¿™äº›èƒ½åŠ›ä¸å®é™…åº”ç”¨ä¸¥æ ¼å¯¹é½ï¼Œä¾‹å¦‚äº¤äº’å¼ä½“è‚²åˆ†æå’ŒåŸºäºè§†é¢‘çš„å¤šè½®æ™ºèƒ½è¾…å¯¼ã€‚é€šè¿‡MT-Video-Benchï¼Œæˆ‘ä»¬å¹¿æ³›è¯„ä¼°äº†å„ç§æœ€å…ˆè¿›çš„å¼€æºå’Œé—­æºMLLMï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¤šè½®è§†é¢‘å¯¹è¯æ–¹é¢çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚å’Œå±€é™æ€§ã€‚è¯¥åŸºå‡†å°†å…¬å¼€æä¾›ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMçš„è§†é¢‘ç†è§£è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨å•è½®é—®ç­”ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸‹çš„å¤šè½®äº¤äº’èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›å’Œäº¤äº’èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ï¼Œéš¾ä»¥åæ˜ æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å®é™…åº”ç”¨æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMT-Video-Benchçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŒ…å«å¤šè½®å¯¹è¯çš„è§†é¢‘ç†è§£åŸºå‡†ï¼Œé€šè¿‡æ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸‹çš„äº¤äº’è¿‡ç¨‹ï¼Œå…¨é¢è¯„ä¼°MLLMçš„æ„ŸçŸ¥èƒ½åŠ›å’Œäº¤äº’èƒ½åŠ›ã€‚è¯¥åŸºå‡†ä¾§é‡äºè¯„ä¼°æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­ç†è§£è§†é¢‘å†…å®¹ã€è¿›è¡Œæ¨ç†å’Œç”Ÿæˆå›å¤çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMT-Video-BenchåŒ…å«987ä¸ªç²¾å¿ƒç­–åˆ’çš„å¤šè½®å¯¹è¯ï¼Œæ¶µç›–ä¸åŒçš„é¢†åŸŸå’Œåœºæ™¯ã€‚æ¯ä¸ªå¯¹è¯éƒ½åŒ…å«è§†é¢‘ç‰‡æ®µå’Œä¸€ç³»åˆ—é—®é¢˜ï¼Œé—®é¢˜è®¾è®¡æ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„å…­é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼šæ„ŸçŸ¥èƒ½åŠ›ï¼ˆä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€åœºæ™¯ç†è§£ï¼‰ã€äº¤äº’èƒ½åŠ›ï¼ˆä¾‹å¦‚é—®é¢˜å›ç­”ã€å¯¹è¯ç”Ÿæˆï¼‰ã€‚åŸºå‡†è¿˜æä¾›è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ¨¡å‹åœ¨ä¸åŒèƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šMT-Video-Benchçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šè½®å¯¹è¯çš„è®¾è®¡ï¼Œèƒ½å¤Ÿæ›´çœŸå®åœ°åæ˜ æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚ä¸ç°æœ‰å•è½®é—®ç­”åŸºå‡†ç›¸æ¯”ï¼ŒMT-Video-Benchèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›å’Œäº¤äº’èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†è¿˜æ¶µç›–äº†ä¸åŒçš„é¢†åŸŸå’Œåœºæ™¯ï¼Œå…·æœ‰æ›´å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šMT-Video-Benchçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šæ ·åŒ–çš„è§†é¢‘å†…å®¹ï¼Œæ¶µç›–ä¸åŒçš„é¢†åŸŸå’Œåœºæ™¯ï¼›2) ç²¾å¿ƒè®¾è®¡çš„å¤šè½®å¯¹è¯ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸‹çš„äº¤äº’è¿‡ç¨‹ï¼›3) æ˜ç¡®å®šä¹‰çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ¨¡å‹åœ¨ä¸åŒèƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼›4) å…­é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬ç›®æ ‡æ£€æµ‹ã€åœºæ™¯ç†è§£ã€é—®é¢˜å›ç­”ã€å¯¹è¯ç”Ÿæˆç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

é€šè¿‡åœ¨MT-Video-Benchä¸Šå¯¹å¤šç§å¼€æºå’Œé—­æºMLLMè¿›è¡Œè¯„ä¼°ï¼Œè®ºæ–‡æ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¤šè½®è§†é¢‘å¯¹è¯æ–¹é¢çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚å’Œå±€é™æ€§ã€‚ä¾‹å¦‚ï¼Œä¸€äº›æ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹å’Œåœºæ™¯ç†è§£æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¯¹è¯ç”Ÿæˆæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰MLLMåœ¨å¤„ç†å¤æ‚çš„å¤šè½®è§†é¢‘å¯¹è¯æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MT-Video-Benchå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„è§†é¢‘ç†è§£ç³»ç»Ÿï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è§†é¢‘ç›‘æ§ã€æ™ºèƒ½æ•™è‚²ç­‰ã€‚é€šè¿‡è¯„ä¼°å’Œæå‡MLLMåœ¨å¤šè½®è§†é¢‘å¯¹è¯ä¸­çš„èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶ã€æ›´é«˜æ•ˆçš„äººæœºäº¤äº’ã€‚è¯¥åŸºå‡†çš„å‘å¸ƒå°†ä¿ƒè¿›å¤šæ¨¡æ€å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶ï¼Œæ¨åŠ¨è§†é¢‘ç†è§£æŠ€æœ¯çš„è¿›æ­¥ï¼Œå¹¶ä¸ºç›¸å…³äº§ä¸šå¸¦æ¥æ½œåœ¨çš„å•†ä¸šä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.

