---
layout: default
title: SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference
---

# SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17777" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17777v1</a>
  <a href="https://arxiv.org/pdf/2510.17777.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17777v1" onclick="toggleFavorite(this, '2510.17777v1', 'SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Samir Khaki, Junxian Guo, Jiaming Tang, Shang Yang, Yukang Chen, Konstantinos N. Plataniotis, Yao Lu, Song Han, Zhijian Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SparseVILAï¼šè§£è€¦è§†è§‰ç¨€ç–æ€§ï¼ŒåŠ é€Ÿé«˜æ•ˆVLMæ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `VLM` `è§†è§‰ç¨€ç–æ€§` `æ¨ç†åŠ é€Ÿ` `é•¿è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMæ¨ç†å—é™äºè§†è§‰tokenæ•°é‡åºå¤§ï¼Œå¯¼è‡´å»¶è¿Ÿé«˜ï¼Œæ‰©å±•æ€§å—é™ã€‚
2. SparseVILAè§£è€¦é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„è§†è§‰ç¨€ç–æ€§ï¼Œå®ç°é«˜æ•ˆVLMæ¨ç†ã€‚
3. SparseVILAåœ¨é•¿è§†é¢‘ä»»åŠ¡ä¸ŠåŠ é€Ÿ2.6å€ï¼Œå¹¶æå‡æ–‡æ¡£ç†è§£å’Œæ¨ç†å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨é›†æˆè§†è§‰å’Œæ–‡æœ¬æ¨ç†æ–¹é¢å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œæ¨åŠ¨äº†é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ã€é•¿è§†é¢‘åˆ†æå’Œå¤šè½®å¯¹è¯ç­‰åº”ç”¨ã€‚ç„¶è€Œï¼Œè§†è§‰tokenæ•°é‡çš„å¢é•¿é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸»å¯¼äº†æ¨ç†å»¶è¿Ÿã€‚æˆ‘ä»¬æå‡ºäº†SparseVILAï¼Œä¸€ç§é«˜æ•ˆVLMæ¨ç†çš„æ–°èŒƒå¼ï¼Œå®ƒè§£è€¦äº†é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„è§†è§‰ç¨€ç–æ€§ã€‚SparseVILAé€šè¿‡åœ¨é¢„å¡«å……æœŸé—´å‰ªæå†—ä½™è§†è§‰tokenï¼Œå¹¶åœ¨è§£ç æœŸé—´ä»…æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„tokenï¼Œä»è€Œåœ¨å„ä¸ªé˜¶æ®µåˆ†é…ç¨€ç–æ€§ã€‚è¿™ç§è§£è€¦è®¾è®¡ä¸é¢†å…ˆçš„é¢„å¡«å……å‰ªææ–¹æ³•ç›¸åŒ¹é…ï¼ŒåŒæ—¶é€šè¿‡ä¿ç•™å¤§éƒ¨åˆ†è§†è§‰ç¼“å­˜æ¥ä¿æŒå¤šè½®ä¿çœŸåº¦ï¼Œä»¥ä¾¿å¯ä»¥åœ¨æ¯ä¸ªå¯¹è¯è½®æ¬¡æ£€ç´¢æ„ŸçŸ¥æŸ¥è¯¢çš„tokenã€‚SparseVILAå»ºç«‹åœ¨AWQä¼˜åŒ–çš„æ¨ç†ç®¡é“ä¹‹ä¸Šï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡è§†é¢‘ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾4.0å€çš„é¢„å¡«å……åŠ é€Ÿã€2.5å€çš„è§£ç åŠ é€Ÿä»¥åŠ2.6å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼ŒåŒæ—¶æé«˜äº†æ–‡æ¡£ç†è§£å’Œæ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚é€šè¿‡è§£è€¦ä¸æŸ¥è¯¢æ— å…³çš„å‰ªæå’Œä¸æŸ¥è¯¢ç›¸å…³çš„æ£€ç´¢ï¼ŒSparseVILAä¸ºé«˜æ•ˆå¤šæ¨¡æ€æ¨ç†å»ºç«‹äº†ä¸€ä¸ªæ–°æ–¹å‘ï¼Œæä¾›äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒã€æ¶æ„æ— å…³çš„æ¡†æ¶ï¼Œç”¨äºåŠ é€Ÿå¤§å‹VLMï¼Œè€Œä¸ä¼šç‰ºç‰²èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒã€é•¿è§†é¢‘ç­‰ä»»åŠ¡æ—¶ï¼Œéœ€è¦å¤„ç†å¤§é‡çš„è§†è§‰tokenï¼Œè¿™å¯¼è‡´äº†å·¨å¤§çš„è®¡ç®—å¼€é”€å’Œæ¨ç†å»¶è¿Ÿï¼Œé™åˆ¶äº†VLMçš„å®é™…åº”ç”¨ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸éš¾ä»¥åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè½®å¯¹è¯ç­‰éœ€è¦é•¿æœŸä¸Šä¸‹æ–‡çš„ä»»åŠ¡ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSparseVILAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰ä¿¡æ¯çš„ç¨€ç–åŒ–è¿‡ç¨‹è§£è€¦ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¢„å¡«å……é˜¶æ®µçš„ä¸æŸ¥è¯¢æ— å…³çš„å‰ªæï¼Œä»¥åŠè§£ç é˜¶æ®µçš„ä¸æŸ¥è¯¢ç›¸å…³çš„æ£€ç´¢ã€‚é€šè¿‡è¿™ç§è§£è€¦ï¼Œå¯ä»¥åœ¨é¢„å¡«å……é˜¶æ®µå»é™¤å†—ä½™çš„è§†è§‰tokenï¼Œå‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶åœ¨è§£ç é˜¶æ®µåªå…³æ³¨ä¸å½“å‰æŸ¥è¯¢ç›¸å…³çš„tokenï¼Œæé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSparseVILAçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„å¡«å……é˜¶æ®µå’Œè§£ç é˜¶æ®µã€‚åœ¨é¢„å¡«å……é˜¶æ®µï¼Œä½¿ç”¨å‰ªæç®—æ³•å»é™¤å†—ä½™çš„è§†è§‰tokenï¼Œç”Ÿæˆç¨€ç–çš„è§†è§‰è¡¨ç¤ºã€‚åœ¨è§£ç é˜¶æ®µï¼Œæ ¹æ®å½“å‰æŸ¥è¯¢ï¼Œä»ç¨€ç–çš„è§†è§‰è¡¨ç¤ºä¸­æ£€ç´¢ç›¸å…³çš„tokenï¼Œå¹¶å°†å…¶ä¸æ–‡æœ¬ä¿¡æ¯èåˆï¼Œè¿›è¡Œæ¨ç†ã€‚è¯¥æ¡†æ¶å¯ä»¥ä¸ç°æœ‰çš„VLMæ¶æ„ç›¸ç»“åˆï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šSparseVILAçš„å…³é”®åˆ›æ–°åœ¨äºè§£è€¦äº†è§†è§‰ç¨€ç–æ€§ï¼Œå°†å‰ªæå’Œæ£€ç´¢è¿‡ç¨‹åˆ†ç¦»ã€‚è¿™ç§è§£è€¦ä½¿å¾—å¯ä»¥åœ¨é¢„å¡«å……é˜¶æ®µè¿›è¡Œå…¨å±€çš„ã€ä¸æŸ¥è¯¢æ— å…³çš„å‰ªæï¼Œå‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶åœ¨è§£ç é˜¶æ®µè¿›è¡Œå±€éƒ¨çš„ã€ä¸æŸ¥è¯¢ç›¸å…³çš„æ£€ç´¢ï¼Œæé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSparseVILAèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šSparseVILAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨AWQè¿›è¡Œé‡åŒ–ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ï¼›2) è®¾è®¡äº†åˆé€‚çš„å‰ªæç­–ç•¥ï¼Œåœ¨é¢„å¡«å……é˜¶æ®µå»é™¤å†—ä½™çš„è§†è§‰tokenï¼›3) è®¾è®¡äº†é«˜æ•ˆçš„æ£€ç´¢ç®—æ³•ï¼Œåœ¨è§£ç é˜¶æ®µå¿«é€Ÿæ‰¾åˆ°ä¸æŸ¥è¯¢ç›¸å…³çš„tokenã€‚å…·ä½“çš„å‰ªæç­–ç•¥å’Œæ£€ç´¢ç®—æ³•çš„é€‰æ‹©å–å†³äºå…·ä½“çš„åº”ç”¨åœºæ™¯å’ŒVLMæ¶æ„ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SparseVILAåœ¨é•¿ä¸Šä¸‹æ–‡è§†é¢‘ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾4.0å€çš„é¢„å¡«å……åŠ é€Ÿã€2.5å€çš„è§£ç åŠ é€Ÿä»¥åŠ2.6å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚åŒæ—¶ï¼ŒSparseVILAåœ¨æ–‡æ¡£ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSparseVILAèƒ½å¤Ÿåœ¨æ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æé«˜VLMçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SparseVILAé€‚ç”¨äºéœ€è¦å¤„ç†å¤§é‡è§†è§‰ä¿¡æ¯çš„è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨ï¼Œä¾‹å¦‚é•¿è§†é¢‘ç†è§£ã€é«˜åˆ†è¾¨ç‡å›¾åƒåˆ†æã€å¤šè½®å¯¹è¯ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€å’Œæ¨ç†å»¶è¿Ÿï¼Œæé«˜VLMçš„å®é™…åº”ç”¨ä»·å€¼ï¼Œå¹¶æ¨åŠ¨VLMåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚æœªæ¥ï¼ŒSparseVILAå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.

