---
layout: default
title: SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference
---

# SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.17777" target="_blank" class="toolbar-btn">arXiv: 2510.17777v1</a>
    <a href="https://arxiv.org/pdf/2510.17777.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17777v1" 
            onclick="toggleFavorite(this, '2510.17777v1', 'SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Samir Khaki, Junxian Guo, Jiaming Tang, Shang Yang, Yukang Chen, Konstantinos N. Plataniotis, Yao Lu, Song Han, Zhijian Liu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SparseVILAÔºöËß£ËÄ¶ËßÜËßâÁ®ÄÁñèÊÄßÔºåÂä†ÈÄüÈ´òÊïàVLMÊé®ÁêÜ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `VLM` `ËßÜËßâÁ®ÄÁñèÊÄß` `Êé®ÁêÜÂä†ÈÄü` `ÈïøËßÜÈ¢ëÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLMÊé®ÁêÜÂèóÈôê‰∫éËßÜËßâtokenÊï∞ÈáèÂ∫ûÂ§ßÔºåÂØºËá¥Âª∂ËøüÈ´òÔºåÊâ©Â±ïÊÄßÂèóÈôê„ÄÇ
2. SparseVILAËß£ËÄ¶È¢ÑÂ°´ÂÖÖÂíåËß£Á†ÅÈò∂ÊÆµÁöÑËßÜËßâÁ®ÄÁñèÊÄßÔºåÂÆûÁé∞È´òÊïàVLMÊé®ÁêÜ„ÄÇ
3. SparseVILAÂú®ÈïøËßÜÈ¢ë‰ªªÂä°‰∏äÂä†ÈÄü2.6ÂÄçÔºåÂπ∂ÊèêÂçáÊñáÊ°£ÁêÜËß£ÂíåÊé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)Âú®ÈõÜÊàêËßÜËßâÂíåÊñáÊú¨Êé®ÁêÜÊñπÈù¢ÂèñÂæó‰∫ÜÂø´ÈÄüËøõÂ±ïÔºåÊé®Âä®‰∫ÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁêÜËß£„ÄÅÈïøËßÜÈ¢ëÂàÜÊûêÂíåÂ§öËΩÆÂØπËØùÁ≠âÂ∫îÁî®„ÄÇÁÑ∂ËÄåÔºåËßÜËßâtokenÊï∞ÈáèÁöÑÂ¢ûÈïøÈôêÂà∂‰∫ÜÂÖ∂ÂèØÊâ©Â±ïÊÄßÔºåÂπ∂‰∏ªÂØº‰∫ÜÊé®ÁêÜÂª∂Ëøü„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜSparseVILAÔºå‰∏ÄÁßçÈ´òÊïàVLMÊé®ÁêÜÁöÑÊñ∞ËåÉÂºèÔºåÂÆÉËß£ËÄ¶‰∫ÜÈ¢ÑÂ°´ÂÖÖÂíåËß£Á†ÅÈò∂ÊÆµÁöÑËßÜËßâÁ®ÄÁñèÊÄß„ÄÇSparseVILAÈÄöËøáÂú®È¢ÑÂ°´ÂÖÖÊúüÈó¥Ââ™ÊûùÂÜó‰ΩôËßÜËßâtokenÔºåÂπ∂Âú®Ëß£Á†ÅÊúüÈó¥‰ªÖÊ£ÄÁ¥¢‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑtokenÔºå‰ªéËÄåÂú®ÂêÑ‰∏™Èò∂ÊÆµÂàÜÈÖçÁ®ÄÁñèÊÄß„ÄÇËøôÁßçËß£ËÄ¶ËÆæËÆ°‰∏éÈ¢ÜÂÖàÁöÑÈ¢ÑÂ°´ÂÖÖÂâ™ÊûùÊñπÊ≥ïÁõ∏ÂåπÈÖçÔºåÂêåÊó∂ÈÄöËøá‰øùÁïôÂ§ßÈÉ®ÂàÜËßÜËßâÁºìÂ≠òÊù•‰øùÊåÅÂ§öËΩÆ‰øùÁúüÂ∫¶Ôºå‰ª•‰æøÂèØ‰ª•Âú®ÊØè‰∏™ÂØπËØùËΩÆÊ¨°Ê£ÄÁ¥¢ÊÑüÁü•Êü•ËØ¢ÁöÑtoken„ÄÇSparseVILAÂª∫Á´ãÂú®AWQ‰ºòÂåñÁöÑÊé®ÁêÜÁÆ°ÈÅì‰πã‰∏äÔºåÂú®Èïø‰∏ä‰∏ãÊñáËßÜÈ¢ë‰ªªÂä°‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ4.0ÂÄçÁöÑÈ¢ÑÂ°´ÂÖÖÂä†ÈÄü„ÄÅ2.5ÂÄçÁöÑËß£Á†ÅÂä†ÈÄü‰ª•Âèä2.6ÂÄçÁöÑÁ´ØÂà∞Á´ØÂä†ÈÄüÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜÊñáÊ°£ÁêÜËß£ÂíåÊé®ÁêÜ‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÈÄöËøáËß£ËÄ¶‰∏éÊü•ËØ¢Êó†ÂÖ≥ÁöÑÂâ™ÊûùÂíå‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊ£ÄÁ¥¢ÔºåSparseVILA‰∏∫È´òÊïàÂ§öÊ®°ÊÄÅÊé®ÁêÜÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Êñ∞ÊñπÂêëÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êó†ÈúÄËÆ≠ÁªÉ„ÄÅÊû∂ÊûÑÊó†ÂÖ≥ÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÂä†ÈÄüÂ§ßÂûãVLMÔºåËÄå‰∏ç‰ºöÁâ∫Áâ≤ËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂú®Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉè„ÄÅÈïøËßÜÈ¢ëÁ≠â‰ªªÂä°Êó∂ÔºåÈúÄË¶ÅÂ§ÑÁêÜÂ§ßÈáèÁöÑËßÜËßâtokenÔºåËøôÂØºËá¥‰∫ÜÂ∑®Â§ßÁöÑËÆ°ÁÆóÂºÄÈîÄÂíåÊé®ÁêÜÂª∂ËøüÔºåÈôêÂà∂‰∫ÜVLMÁöÑÂÆûÈôÖÂ∫îÁî®„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏Èöæ‰ª•Âú®ÊïàÁéáÂíåÊÄßËÉΩ‰πãÈó¥ÂèñÂæóÂπ≥Ë°°ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öËΩÆÂØπËØùÁ≠âÈúÄË¶ÅÈïøÊúü‰∏ä‰∏ãÊñáÁöÑ‰ªªÂä°‰∏≠„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSparseVILAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËßÜËßâ‰ø°ÊÅØÁöÑÁ®ÄÁñèÂåñËøáÁ®ãËß£ËÄ¶‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÈ¢ÑÂ°´ÂÖÖÈò∂ÊÆµÁöÑ‰∏éÊü•ËØ¢Êó†ÂÖ≥ÁöÑÂâ™ÊûùÔºå‰ª•ÂèäËß£Á†ÅÈò∂ÊÆµÁöÑ‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊ£ÄÁ¥¢„ÄÇÈÄöËøáËøôÁßçËß£ËÄ¶ÔºåÂèØ‰ª•Âú®È¢ÑÂ°´ÂÖÖÈò∂ÊÆµÂéªÈô§ÂÜó‰ΩôÁöÑËßÜËßâtokenÔºåÂáèÂ∞ëËÆ°ÁÆóÈáèÔºåÂêåÊó∂Âú®Ëß£Á†ÅÈò∂ÊÆµÂè™ÂÖ≥Ê≥®‰∏éÂΩìÂâçÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑtokenÔºåÊèêÈ´òÊé®ÁêÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSparseVILAÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¢ÑÂ°´ÂÖÖÈò∂ÊÆµÂíåËß£Á†ÅÈò∂ÊÆµ„ÄÇÂú®È¢ÑÂ°´ÂÖÖÈò∂ÊÆµÔºå‰ΩøÁî®Ââ™ÊûùÁÆóÊ≥ïÂéªÈô§ÂÜó‰ΩôÁöÑËßÜËßâtokenÔºåÁîüÊàêÁ®ÄÁñèÁöÑËßÜËßâË°®Á§∫„ÄÇÂú®Ëß£Á†ÅÈò∂ÊÆµÔºåÊ†πÊçÆÂΩìÂâçÊü•ËØ¢Ôºå‰ªéÁ®ÄÁñèÁöÑËßÜËßâË°®Á§∫‰∏≠Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÁöÑtokenÔºåÂπ∂Â∞ÜÂÖ∂‰∏éÊñáÊú¨‰ø°ÊÅØËûçÂêàÔºåËøõË°åÊé®ÁêÜ„ÄÇËØ•Ê°ÜÊû∂ÂèØ‰ª•‰∏éÁé∞ÊúâÁöÑVLMÊû∂ÊûÑÁõ∏ÁªìÂêàÔºåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSparseVILAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éËß£ËÄ¶‰∫ÜËßÜËßâÁ®ÄÁñèÊÄßÔºåÂ∞ÜÂâ™ÊûùÂíåÊ£ÄÁ¥¢ËøáÁ®ãÂàÜÁ¶ª„ÄÇËøôÁßçËß£ËÄ¶‰ΩøÂæóÂèØ‰ª•Âú®È¢ÑÂ°´ÂÖÖÈò∂ÊÆµËøõË°åÂÖ®Â±ÄÁöÑ„ÄÅ‰∏éÊü•ËØ¢Êó†ÂÖ≥ÁöÑÂâ™ÊûùÔºåÂáèÂ∞ëËÆ°ÁÆóÈáèÔºåÂêåÊó∂Âú®Ëß£Á†ÅÈò∂ÊÆµËøõË°åÂ±ÄÈÉ®ÁöÑ„ÄÅ‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊ£ÄÁ¥¢ÔºåÊèêÈ´òÊé®ÁêÜÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSparseVILAËÉΩÂ§üÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂºÄÈîÄ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSparseVILAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®AWQËøõË°åÈáèÂåñÔºåËøõ‰∏ÄÊ≠•Âä†ÈÄüÊé®ÁêÜÔºõ2) ËÆæËÆ°‰∫ÜÂêàÈÄÇÁöÑÂâ™ÊûùÁ≠ñÁï•ÔºåÂú®È¢ÑÂ°´ÂÖÖÈò∂ÊÆµÂéªÈô§ÂÜó‰ΩôÁöÑËßÜËßâtokenÔºõ3) ËÆæËÆ°‰∫ÜÈ´òÊïàÁöÑÊ£ÄÁ¥¢ÁÆóÊ≥ïÔºåÂú®Ëß£Á†ÅÈò∂ÊÆµÂø´ÈÄüÊâæÂà∞‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑtoken„ÄÇÂÖ∑‰ΩìÁöÑÂâ™ÊûùÁ≠ñÁï•ÂíåÊ£ÄÁ¥¢ÁÆóÊ≥ïÁöÑÈÄâÊã©ÂèñÂÜ≥‰∫éÂÖ∑‰ΩìÁöÑÂ∫îÁî®Âú∫ÊôØÂíåVLMÊû∂ÊûÑ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SparseVILAÂú®Èïø‰∏ä‰∏ãÊñáËßÜÈ¢ë‰ªªÂä°‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ4.0ÂÄçÁöÑÈ¢ÑÂ°´ÂÖÖÂä†ÈÄü„ÄÅ2.5ÂÄçÁöÑËß£Á†ÅÂä†ÈÄü‰ª•Âèä2.6ÂÄçÁöÑÁ´ØÂà∞Á´ØÂä†ÈÄü„ÄÇÂêåÊó∂ÔºåSparseVILAÂú®ÊñáÊ°£ÁêÜËß£ÂíåÊé®ÁêÜ‰ªªÂä°‰∏äÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄß„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåSparseVILAËÉΩÂ§üÂú®ÊòæËëóÊèêÈ´òÊé®ÁêÜÊïàÁéáÁöÑÂêåÊó∂Ôºå‰øùÊåÅÁîöËá≥ÊèêÈ´òVLMÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SparseVILAÈÄÇÁî®‰∫éÈúÄË¶ÅÂ§ÑÁêÜÂ§ßÈáèËßÜËßâ‰ø°ÊÅØÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®Ôºå‰æãÂ¶ÇÈïøËßÜÈ¢ëÁêÜËß£„ÄÅÈ´òÂàÜËæ®ÁéáÂõæÂÉèÂàÜÊûê„ÄÅÂ§öËΩÆÂØπËØùÁ≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂºÄÈîÄÂíåÊé®ÁêÜÂª∂ËøüÔºåÊèêÈ´òVLMÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÔºåÂπ∂Êé®Âä®VLMÂú®ËµÑÊ∫êÂèóÈôêËÆæÂ§á‰∏äÁöÑÈÉ®ÁΩ≤„ÄÇÊú™Êù•ÔºåSparseVILAÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅ‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.

