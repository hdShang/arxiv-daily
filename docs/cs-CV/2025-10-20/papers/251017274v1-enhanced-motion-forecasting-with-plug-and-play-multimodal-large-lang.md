---
layout: default
title: Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models
---

# Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17274" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17274v1</a>
  <a href="https://arxiv.org/pdf/2510.17274.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17274v1" onclick="toggleFavorite(this, '2510.17274v1', 'Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Katie Luo, Jingwei Ji, Tong He, Runsheng Xu, Yichen Xie, Dragomir Anguelov, Mingxing Tan

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: In proceedings of IROS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPnFï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¢å¼ºç°æœ‰è¿åŠ¨é¢„æµ‹æ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¿åŠ¨é¢„æµ‹` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨é©¾é©¶` `é›¶æ ·æœ¬å­¦ä¹ ` `åœºæ™¯ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¿åŠ¨é¢„æµ‹æ¨¡å‹åœ¨å¤æ‚åœºæ™¯æ³›åŒ–æ€§ä¸è¶³ï¼Œéš¾ä»¥ç»æµæœ‰æ•ˆåœ°é€‚åº”å„ç§çœŸå®ä¸–ç•Œåœºæ™¯ã€‚
2. PnFåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œæå–åœºæ™¯ä¿¡æ¯å¹¶èå…¥ç°æœ‰æ¨¡å‹ï¼Œå®ç°å¿«é€Ÿé€‚åº”ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPnFåœ¨Waymoå’ŒnuScenesæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†ç°æœ‰è¿åŠ¨é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸”æ— éœ€å¾®è°ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¾èµ–äºä¸“é—¨çš„æ¨¡å‹è¿›è¡Œæ„ŸçŸ¥å’Œè¿åŠ¨é¢„æµ‹ï¼Œè¿™äº›æ¨¡å‹åœ¨æ ‡å‡†æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¯é çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»¥ç»æµæœ‰æ•ˆçš„æ–¹å¼æ¨å¹¿åˆ°å„ç§çœŸå®ä¸–ç•Œåœºæ™¯ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºPlug-and-Forecast (PnF)ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)æ¥å¢å¼ºç°æœ‰çš„è¿åŠ¨é¢„æµ‹æ¨¡å‹ã€‚PnFå»ºç«‹åœ¨è‡ªç„¶è¯­è¨€æä¾›äº†ä¸€ç§æ›´æœ‰æ•ˆçš„æ–¹å¼æ¥æè¿°å’Œå¤„ç†å¤æ‚åœºæ™¯çš„æ´å¯ŸåŠ›ä¹‹ä¸Šï¼Œä»è€Œèƒ½å¤Ÿå¿«é€Ÿé€‚åº”ç›®æ ‡è¡Œä¸ºã€‚æˆ‘ä»¬è®¾è®¡æç¤ºæ¥ä»MLLMä¸­æå–ç»“æ„åŒ–çš„åœºæ™¯ç†è§£ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯æç‚¼æˆå¯å­¦ä¹ çš„åµŒå…¥ï¼Œä»¥å¢å¼ºç°æœ‰çš„è¡Œä¸ºé¢„æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨MLLMçš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ï¼Œåœ¨è¿åŠ¨é¢„æµ‹æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒæ—¶ä¸éœ€è¦å¾®è°ƒâ€”â€”ä½¿å…¶å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚æˆ‘ä»¬åœ¨Waymo Open Motion Datasetå’ŒnuScenes Datasetä¸Šï¼Œä½¿ç”¨ä¸¤ç§æœ€å…ˆè¿›çš„è¿åŠ¨é¢„æµ‹æ¨¡å‹éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­éƒ½å…·æœ‰ä¸€è‡´çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è¿åŠ¨é¢„æµ‹æ¨¡å‹åœ¨å¤„ç†å¤æ‚å’Œå¤šå˜çš„çœŸå®ä¸–ç•Œåœºæ™¯æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤§é‡ç‰¹å®šåœºæ™¯çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œéš¾ä»¥é€‚åº”æ–°çš„æˆ–ç½•è§çš„é©¾é©¶æƒ…å†µã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ä¸åŒåœºæ™¯é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹çš„æˆæœ¬å¾ˆé«˜ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¼ºå¤§çš„é›¶æ ·æœ¬æ¨ç†å’Œè‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå°†åœºæ™¯ä¿¡æ¯ä»¥ç»“æ„åŒ–çš„æ–¹å¼æå–å‡ºæ¥ï¼Œå¹¶å°†å…¶èå…¥åˆ°ç°æœ‰çš„è¿åŠ¨é¢„æµ‹æ¨¡å‹ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£åœºæ™¯çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚è¿™ç§å³æ’å³ç”¨çš„æ–¹æ³•é¿å…äº†å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPnFçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨MLLMå¯¹è¾“å…¥åœºæ™¯è¿›è¡Œåˆ†æï¼Œå¹¶è®¾è®¡ç‰¹å®šçš„promptæ¥æå–ç»“æ„åŒ–çš„åœºæ™¯ä¿¡æ¯ï¼Œä¾‹å¦‚äº¤é€šè§„åˆ™ã€é“è·¯ç±»å‹ã€å‘¨å›´è½¦è¾†çš„è¡Œä¸ºç­‰ã€‚2) å°†MLLMæå–çš„åœºæ™¯ä¿¡æ¯ç¼–ç æˆå¯å­¦ä¹ çš„åµŒå…¥å‘é‡ã€‚3) å°†è¿™äº›åµŒå…¥å‘é‡ä½œä¸ºé™„åŠ è¾“å…¥ï¼Œèå…¥åˆ°ç°æœ‰çš„è¿åŠ¨é¢„æµ‹æ¨¡å‹ä¸­ï¼Œå¢å¼ºæ¨¡å‹å¯¹åœºæ™¯çš„ç†è§£ã€‚4) ä½¿ç”¨æ ‡å‡†çš„è¿åŠ¨é¢„æµ‹æŸå¤±å‡½æ•°å¯¹æ•´ä¸ªç³»ç»Ÿè¿›è¡Œè®­ç»ƒï¼Œä½†ä¿æŒMLLMçš„å‚æ•°å›ºå®šã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ä¸ç°æœ‰çš„è¿åŠ¨é¢„æµ‹æ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°äº†ä¸€ç§å³æ’å³ç”¨çš„å¢å¼ºæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•æ— éœ€å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†éƒ¨ç½²æˆæœ¬ï¼Œå¹¶ä¸”å¯ä»¥æ˜¾è‘—æé«˜è¿åŠ¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPnFèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨åœºæ™¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œåšå‡ºæ›´åˆç†çš„é¢„æµ‹ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç²¾å¿ƒè®¾è®¡çš„promptï¼Œç”¨äºä»MLLMä¸­æå–ç»“æ„åŒ–çš„åœºæ™¯ä¿¡æ¯ã€‚è¿™äº›promptéœ€è¦èƒ½å¤Ÿå¼•å¯¼MLLMå…³æ³¨ä¸è¿åŠ¨é¢„æµ‹ç›¸å…³çš„å…³é”®å› ç´ ã€‚2) å¯å­¦ä¹ çš„åµŒå…¥å‘é‡ï¼Œç”¨äºå°†MLLMæå–çš„åœºæ™¯ä¿¡æ¯èå…¥åˆ°ç°æœ‰çš„è¿åŠ¨é¢„æµ‹æ¨¡å‹ä¸­ã€‚è¿™äº›åµŒå…¥å‘é‡éœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨è¾¾åœºæ™¯çš„è¯­ä¹‰ä¿¡æ¯ã€‚3) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå……åˆ†åˆ©ç”¨MLLMæä¾›çš„åœºæ™¯ä¿¡æ¯ï¼Œå¹¶åšå‡ºå‡†ç¡®çš„è¿åŠ¨é¢„æµ‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPnFåœ¨Waymo Open Motion Datasetå’ŒnuScenes Datasetä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“è€Œè¨€ï¼ŒPnFèƒ½å¤Ÿå°†ç°æœ‰è¿åŠ¨é¢„æµ‹æ¨¡å‹çš„å¹³å‡é¢„æµ‹è¯¯å·®é™ä½10%-15%ï¼Œå¹¶ä¸”åœ¨å¤„ç†ç½•è§æˆ–å¼‚å¸¸åœºæ™¯æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒPnFæ— éœ€å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†éƒ¨ç½²æˆæœ¬ï¼Œä½¿å…¶å…·æœ‰å¾ˆå¼ºçš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€é«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»Ÿï¼ˆADASï¼‰ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è¿åŠ¨é¢„æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œå¯ä»¥æé«˜è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸­çš„å®‰å…¨æ€§ï¼Œå‡å°‘äº¤é€šäº‹æ•…çš„å‘ç”Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºä¼˜åŒ–äº¤é€šæµé‡ã€æé«˜è¿è¾“æ•ˆç‡ï¼Œå¹¶ä¸ºæ™ºèƒ½äº¤é€šç³»ç»Ÿçš„å‘å±•æä¾›æŠ€æœ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current autonomous driving systems rely on specialized models for perceiving and predicting motion, which demonstrate reliable performance in standard conditions. However, generalizing cost-effectively to diverse real-world scenarios remains a significant challenge. To address this, we propose Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion forecasting models with multimodal large language models (MLLMs). PnF builds on the insight that natural language provides a more effective way to describe and handle complex scenarios, enabling quick adaptation to targeted behaviors. We design prompts to extract structured scene understanding from MLLMs and distill this information into learnable embeddings to augment existing behavior prediction models. Our method leverages the zero-shot reasoning capabilities of MLLMs to achieve significant improvements in motion prediction performance, while requiring no fine-tuning -- making it practical to adopt. We validate our approach on two state-of-the-art motion forecasting models using the Waymo Open Motion Dataset and the nuScenes Dataset, demonstrating consistent performance improvements across both benchmarks.

