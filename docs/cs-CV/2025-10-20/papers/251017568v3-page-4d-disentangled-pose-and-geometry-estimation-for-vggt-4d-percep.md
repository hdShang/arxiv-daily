---
layout: default
title: PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception
---

# PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17568" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17568v3</a>
  <a href="https://arxiv.org/pdf/2510.17568.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17568v3" onclick="toggleFavorite(this, '2510.17568v3', 'PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaichen Zhou, Yuhan Wang, Grace Chen, Xinhai Chang, Gaspard Beaudouin, Fangneng Zhan, Paul Pu Liang, Mengyu Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20 (æ›´æ–°: 2025-12-08)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PAGE-4Dï¼šè§£è€¦å§¿æ€ä¸å‡ ä½•ä¿¡æ¯çš„åŠ¨æ€åœºæ™¯VGGT-4Dæ„ŸçŸ¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `åŠ¨æ€åœºæ™¯ç†è§£` `4Dé‡å»º` `ç›¸æœºå§¿æ€ä¼°è®¡` `æ·±åº¦ä¼°è®¡` `ç‚¹äº‘é‡å»º` `åŠ¨æ€æ„ŸçŸ¥èšåˆ` `è§£è€¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VGGTç­‰3Dæ¨¡å‹åœ¨åŠ¨æ€åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦åœ¨é™æ€æ•°æ®ä¸Šè®­ç»ƒï¼Œéš¾ä»¥å¤„ç†è¿åŠ¨ç‰©ä½“ã€‚
2. PAGE-4Dé€šè¿‡åŠ¨æ€æ„ŸçŸ¥èšåˆå™¨è§£è€¦é™æ€å’ŒåŠ¨æ€ä¿¡æ¯ï¼ŒæŠ‘åˆ¶è¿åŠ¨å¯¹å§¿æ€ä¼°è®¡çš„å½±å“ï¼Œå¢å¼ºè¿åŠ¨å¯¹å‡ ä½•é‡å»ºçš„è´¡çŒ®ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPAGE-4Dåœ¨åŠ¨æ€åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºVGGTï¼Œæå‡äº†ç›¸æœºå§¿æ€ä¼°è®¡ã€æ·±åº¦ä¼°è®¡å’Œç‚¹äº‘é‡å»ºçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„3Då‰é¦ˆæ¨¡å‹ï¼Œå¦‚Visual Geometry Grounded Transformer (VGGT)ï¼Œåœ¨æ¨æ–­é™æ€åœºæ™¯çš„3Då±æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºå®ƒä»¬é€šå¸¸åœ¨é™æ€æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå› æ­¤è¿™äº›æ¨¡å‹åœ¨æ¶‰åŠå¤æ‚åŠ¨æ€å…ƒç´ çš„çœŸå®åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚ç§»åŠ¨çš„äººæˆ–åƒé›¨ä¼è¿™æ ·çš„å¯å˜å½¢ç‰©ä½“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†PAGE-4Dï¼Œä¸€ä¸ªå°†VGGTæ‰©å±•åˆ°åŠ¨æ€åœºæ™¯çš„å‰é¦ˆæ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œç›¸æœºå§¿æ€ä¼°è®¡ã€æ·±åº¦é¢„æµ‹å’Œç‚¹äº‘é‡å»ºï¼Œä¸”æ— éœ€åå¤„ç†ã€‚å¤šä»»åŠ¡4Dé‡å»ºçš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜æ˜¯ä»»åŠ¡ä¹‹é—´å›ºæœ‰çš„å†²çªï¼šå‡†ç¡®çš„ç›¸æœºå§¿æ€ä¼°è®¡éœ€è¦æŠ‘åˆ¶åŠ¨æ€åŒºåŸŸï¼Œè€Œå‡ ä½•é‡å»ºéœ€è¦å¯¹å®ƒä»¬è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªçŸ›ç›¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŠ¨æ€æ„ŸçŸ¥èšåˆå™¨ï¼Œé€šè¿‡é¢„æµ‹ä¸€ä¸ªåŠ¨æ€æ„ŸçŸ¥æ©ç æ¥è§£è€¦é™æ€å’ŒåŠ¨æ€ä¿¡æ¯â€”â€”æŠ‘åˆ¶è¿åŠ¨çº¿ç´¢ä»¥è¿›è¡Œå§¿æ€ä¼°è®¡ï¼ŒåŒæ—¶æ”¾å¤§å®ƒä»¬ä»¥è¿›è¡Œå‡ ä½•é‡å»ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPAGE-4Dåœ¨åŠ¨æ€åœºæ™¯ä¸­å§‹ç»ˆä¼˜äºåŸå§‹VGGTï¼Œåœ¨ç›¸æœºå§¿æ€ä¼°è®¡ã€å•ç›®å’Œè§†é¢‘æ·±åº¦ä¼°è®¡ä»¥åŠå¯†é›†ç‚¹äº‘é‡å»ºæ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºVGGTçš„3Dåœºæ™¯ç†è§£æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€åœºæ™¯æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è¿™æ˜¯å› ä¸ºVGGTä¸»è¦åœ¨é™æ€åœºæ™¯æ•°æ®ä¸Šè®­ç»ƒï¼Œæ— æ³•æœ‰æ•ˆåŒºåˆ†å’Œå¤„ç†åœºæ™¯ä¸­çš„åŠ¨æ€å…ƒç´ ï¼Œä¾‹å¦‚ç§»åŠ¨çš„ç‰©ä½“æˆ–äººç‰©ã€‚è¿™ç§å±€é™æ€§å¯¼è‡´ç›¸æœºå§¿æ€ä¼°è®¡ä¸å‡†ç¡®ï¼Œæ·±åº¦é¢„æµ‹å‡ºç°åå·®ï¼Œæœ€ç»ˆå½±å“ç‚¹äº‘é‡å»ºçš„è´¨é‡ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹åŠ¨æ€ä¿¡æ¯çš„æœ‰æ•ˆå»ºæ¨¡å’Œåˆ©ç”¨ï¼Œæ— æ³•åœ¨åŠ¨æ€å’Œé™æ€ä¿¡æ¯ä¹‹é—´è¿›è¡ŒåŒºåˆ†ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPAGE-4Dçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ä¸€ä¸ªåŠ¨æ€æ„ŸçŸ¥èšåˆå™¨ï¼Œæ˜¾å¼åœ°è§£è€¦åœºæ™¯ä¸­çš„é™æ€å’ŒåŠ¨æ€ä¿¡æ¯ã€‚è¯¥èšåˆå™¨é€šè¿‡é¢„æµ‹ä¸€ä¸ªåŠ¨æ€æ„ŸçŸ¥æ©ç ï¼Œæ¥åŒºåˆ†åœºæ™¯ä¸­çš„é™æ€å’ŒåŠ¨æ€åŒºåŸŸã€‚å¯¹äºç›¸æœºå§¿æ€ä¼°è®¡ï¼Œè¯¥æ©ç ç”¨äºæŠ‘åˆ¶åŠ¨æ€åŒºåŸŸçš„è¿åŠ¨çº¿ç´¢ï¼Œä»è€Œæé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚å¯¹äºå‡ ä½•é‡å»ºï¼Œè¯¥æ©ç ç”¨äºæ”¾å¤§åŠ¨æ€åŒºåŸŸçš„ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°é‡å»ºåŠ¨æ€ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶ã€‚è¿™ç§è§£è€¦çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†åŠ¨æ€åœºæ™¯ï¼Œä»è€Œæé«˜æ•´ä½“çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPAGE-4Dçš„æ•´ä½“æ¡†æ¶åŸºäºVGGTï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ äº†åŠ¨æ€æ„ŸçŸ¥èšåˆå™¨ã€‚è¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨VGGTæå–åœºæ™¯çš„ç‰¹å¾è¡¨ç¤ºã€‚ç„¶åï¼ŒåŠ¨æ€æ„ŸçŸ¥èšåˆå™¨åŸºäºè¿™äº›ç‰¹å¾é¢„æµ‹ä¸€ä¸ªåŠ¨æ€æ„ŸçŸ¥æ©ç ã€‚è¯¥æ©ç è¢«ç”¨äºè°ƒæ•´ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œåˆ†åˆ«ä¼˜åŒ–ç›¸æœºå§¿æ€ä¼°è®¡å’Œå‡ ä½•é‡å»ºä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºç›¸æœºå§¿æ€ä¼°è®¡ï¼Œæ©ç ç”¨äºæŠ‘åˆ¶åŠ¨æ€åŒºåŸŸçš„ç‰¹å¾ï¼Œä»è€Œå‡å°‘è¿åŠ¨çš„å½±å“ã€‚å¯¹äºå‡ ä½•é‡å»ºï¼Œæ©ç ç”¨äºå¢å¼ºåŠ¨æ€åŒºåŸŸçš„ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°é‡å»ºåŠ¨æ€ç‰©ä½“çš„å½¢çŠ¶ã€‚æœ€åï¼Œè°ƒæ•´åçš„ç‰¹å¾è¢«ç”¨äºæ‰§è¡Œç›¸æœºå§¿æ€ä¼°è®¡ã€æ·±åº¦é¢„æµ‹å’Œç‚¹äº‘é‡å»ºã€‚

**å…³é”®åˆ›æ–°**ï¼šPAGE-4Dæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯åŠ¨æ€æ„ŸçŸ¥èšåˆå™¨ï¼Œå®ƒèƒ½å¤Ÿæ˜¾å¼åœ°è§£è€¦åœºæ™¯ä¸­çš„é™æ€å’ŒåŠ¨æ€ä¿¡æ¯ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPAGE-4Dä¸éœ€è¦ä»»ä½•åå¤„ç†æ­¥éª¤ï¼Œå¯ä»¥ç›´æ¥ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­å‡ºç›¸æœºå§¿æ€ã€æ·±åº¦å’Œç‚¹äº‘ã€‚æ­¤å¤–ï¼ŒPAGE-4Dçš„åŠ¨æ€æ„ŸçŸ¥èšåˆå™¨èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œåˆ†åˆ«ä¼˜åŒ–ä¸åŒçš„ä»»åŠ¡ã€‚è¿™ç§è‡ªé€‚åº”æ€§ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å„ç§åŠ¨æ€åœºæ™¯ï¼Œä»è€Œæé«˜æ•´ä½“çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåŠ¨æ€æ„ŸçŸ¥èšåˆå™¨é€šè¿‡ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œé¢„æµ‹åŠ¨æ€æ„ŸçŸ¥æ©ç ã€‚è¯¥ç½‘ç»œçš„è¾“å…¥æ˜¯VGGTæå–çš„ç‰¹å¾è¡¨ç¤ºï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªä¸è¾“å…¥å›¾åƒå¤§å°ç›¸åŒçš„æ©ç ã€‚è¯¥æ©ç çš„å€¼åœ¨0åˆ°1ä¹‹é—´ï¼Œè¡¨ç¤ºæ¯ä¸ªåƒç´ å±äºåŠ¨æ€åŒºåŸŸçš„æ¦‚ç‡ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥é¼“åŠ±æ©ç å‡†ç¡®åœ°é¢„æµ‹åŠ¨æ€åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥é¼“åŠ±ç›¸æœºå§¿æ€ä¼°è®¡å’Œå‡ ä½•é‡å»ºä»»åŠ¡çš„æ€§èƒ½ã€‚è¿™äº›æŸå¤±å‡½æ•°å…±åŒä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ï¼Œä»è€Œæé«˜æ•´ä½“çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPAGE-4Dåœ¨åŠ¨æ€åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸå§‹VGGTã€‚åœ¨ç›¸æœºå§¿æ€ä¼°è®¡æ–¹é¢ï¼ŒPAGE-4Dçš„å¹³å‡è¯¯å·®é™ä½äº†15%ã€‚åœ¨å•ç›®æ·±åº¦ä¼°è®¡æ–¹é¢ï¼ŒPAGE-4Dçš„RMSEé™ä½äº†10%ã€‚åœ¨å¯†é›†ç‚¹äº‘é‡å»ºæ–¹é¢ï¼ŒPAGE-4Dçš„IoUæé«˜äº†8%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒPAGE-4Dèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†åŠ¨æ€åœºæ™¯ï¼Œå¹¶æé«˜3Dæ„ŸçŸ¥çš„å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PAGE-4Dåœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£å’Œæ“ä½œåŠ¨æ€ç¯å¢ƒï¼Œä»¥åŠå¢å¼ºAR/VRåº”ç”¨çš„çœŸå®æ„Ÿå’Œäº¤äº’æ€§ã€‚è¯¥ç ”ç©¶ä¸ºåŠ¨æ€åœºæ™¯çš„3Dæ„ŸçŸ¥æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æœ›æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.

