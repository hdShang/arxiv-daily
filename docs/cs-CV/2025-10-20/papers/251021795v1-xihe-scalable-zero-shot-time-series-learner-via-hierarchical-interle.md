---
layout: default
title: Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention
---

# Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21795" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21795v1</a>
  <a href="https://arxiv.org/pdf/2510.21795.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21795v1" onclick="toggleFavorite(this, '2510.21795v1', 'Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yinbo Sun, Yuchen Fang, Zhibo Zhu, Jia Li, Yu Liu, Qiwen Deng, Jun Zhou, Hang Yu, Xingyu Lu, Lintao Ma

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåˆ†å±‚äº¤é”™å—æ³¨æ„åŠ›ï¼ˆHIBAï¼‰çš„Xiheï¼Œç”¨äºå¯æ‰©å±•çš„é›¶æ ·æœ¬æ—¶é—´åºåˆ—å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ—¶é—´åºåˆ—åˆ†æ` `é›¶æ ·æœ¬å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶` `å¤šå°ºåº¦å»ºæ¨¡` `æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹åœ¨è·¨é¢†åŸŸè¿ç§»æ¶æ„æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰æ—¶é—´åºåˆ—æ•°æ®ä¸­å›ºæœ‰çš„å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ã€‚
2. è®ºæ–‡æå‡ºåˆ†å±‚äº¤é”™å—æ³¨æ„åŠ›ï¼ˆHIBAï¼‰æœºåˆ¶ï¼Œé€šè¿‡åˆ†å±‚çš„å—å†…å’Œå—é—´ç¨€ç–æ³¨æ„åŠ›ï¼Œæœ‰æ•ˆæ•æ‰æ—¶é—´åºåˆ—æ•°æ®çš„å¤šå°ºåº¦ä¾èµ–å…³ç³»ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒXiheæ¨¡å‹å®¶æ—åœ¨GIFT-EvalåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­Xihe-tinyæ¨¡å‹å‚æ•°æ•ˆç‡é«˜ï¼ŒXihe-maxæ¨¡å‹å–å¾—äº†æ–°çš„é›¶æ ·æœ¬æ€§èƒ½SOTAã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰çš„å‘å±•å—ç›Šäºè¯­è¨€æ¨¡å‹çš„æ¶æ„è¿ç§»ã€‚ç„¶è€Œï¼Œç°æœ‰TSFMsç›´æ¥é‡‡ç”¨è·¨é¢†åŸŸæ¶æ„ï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ•æ‰æ—¶é—´åºåˆ—æ•°æ®å›ºæœ‰çš„å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰ä¸åŒåº•å±‚æ¨¡å¼å’Œé‡‡æ ·ç­–ç•¥çš„æ•°æ®é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¿ç§»æ—¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚äº¤é”™å—æ³¨æ„åŠ›ï¼ˆHIBAï¼‰ï¼Œå®ƒé‡‡ç”¨åˆ†å±‚çš„å—å†…å’Œå—é—´ç¨€ç–æ³¨æ„åŠ›æ¥æœ‰æ•ˆåœ°æ•æ‰å¤šå°ºåº¦ä¾èµ–å…³ç³»ã€‚å—å†…æ³¨æ„åŠ›ä¿ƒè¿›å±€éƒ¨ä¿¡æ¯äº¤æ¢ï¼Œå—é—´æ³¨æ„åŠ›è·¨å—æ“ä½œä»¥æ•æ‰å…¨å±€æ—¶é—´æ¨¡å¼äº¤äº’å’ŒåŠ¨æ€æ¼”åŒ–ã€‚åˆ©ç”¨HIBAæ¶æ„ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Xiheï¼Œä¸€ä¸ªå¯æ‰©å±•çš„TSFMå®¶æ—ï¼Œå‚æ•°è§„æ¨¡ä»è¶…é«˜æ•ˆçš„950ä¸‡åˆ°é«˜å®¹é‡çš„15äº¿ä¸ç­‰ã€‚åœ¨å…¨é¢çš„GIFT-EvalåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æœ€ç´§å‡‘çš„Xihe-tinyæ¨¡å‹ï¼ˆ950ä¸‡å‚æ•°ï¼‰è¶…è¶Šäº†å¤§å¤šæ•°å½“ä»£TSFMï¼Œå±•ç¤ºäº†å“è¶Šçš„å‚æ•°æ•ˆç‡ã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼ŒXihe-maxï¼ˆ15äº¿å‚æ•°ï¼‰å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¤§å¹…è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³ç»“æœã€‚æ•´ä¸ªå‚æ•°èŒƒå›´å†…çš„è¿™ç§ä¸€è‡´çš„å“è¶Šæ€§èƒ½ä¸ºHIBAçš„å“è¶Šæ³›åŒ–èƒ½åŠ›å’Œæ¶æ„ä¼˜åŠ¿æä¾›äº†ä»¤äººä¿¡æœçš„è¯æ®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰ç›´æ¥é‡‡ç”¨è·¨é¢†åŸŸæ¶æ„ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰æ—¶é—´åºåˆ—æ•°æ®ä¸­å›ºæœ‰çš„å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ã€‚è¿™å¯¼è‡´åœ¨å…·æœ‰ä¸åŒåº•å±‚æ¨¡å¼å’Œé‡‡æ ·ç­–ç•¥çš„æ•°æ®é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¿ç§»æ—¶æ€§èƒ½ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å±€éƒ¨ä¿¡æ¯å’Œå…¨å±€æ—¶é—´æ¨¡å¼äº¤äº’çš„æœ‰æ•ˆå»ºæ¨¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ—¶é—´åºåˆ—æ•°æ®å¤šå°ºåº¦ä¾èµ–å…³ç³»çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚é€šè¿‡åˆ†å±‚ç»“æ„ï¼Œåˆ†åˆ«åœ¨å—å†…å’Œå—é—´è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œå…¼é¡¾å±€éƒ¨ä¿¡æ¯å’Œå…¨å±€æ¨¡å¼ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šXiheæ¨¡å‹å®¶æ—åŸºäºHIBAæ¶æ„ï¼Œæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1ï¼‰è¾“å…¥åµŒå…¥å±‚ï¼šå°†æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„åµŒå…¥è¡¨ç¤ºã€‚2ï¼‰HIBAå±‚ï¼šæ ¸å¿ƒæ¨¡å—ï¼ŒåŒ…å«å¤šä¸ªHIBAå—ï¼Œæ¯ä¸ªå—æ‰§è¡Œå—å†…å’Œå—é—´æ³¨æ„åŠ›è®¡ç®—ã€‚3ï¼‰è¾“å‡ºå±‚ï¼šå°†HIBAå±‚çš„è¾“å‡ºæ˜ å°„åˆ°ç›®æ ‡é¢„æµ‹ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯åˆ†å±‚äº¤é”™å—æ³¨æ„åŠ›ï¼ˆHIBAï¼‰æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒHIBAé€šè¿‡åˆ†å±‚ç»“æ„å’Œç¨€ç–æ³¨æ„åŠ›ï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶æ›´å¥½åœ°æ•æ‰äº†æ—¶é—´åºåˆ—æ•°æ®çš„å¤šå°ºåº¦ä¾èµ–å…³ç³»ã€‚å—å†…æ³¨æ„åŠ›å…³æ³¨å±€éƒ¨ä¿¡æ¯ï¼Œå—é—´æ³¨æ„åŠ›å…³æ³¨å…¨å±€æ¨¡å¼äº¤äº’ï¼Œä¸¤è€…äº¤é”™è¿›è¡Œï¼Œæå‡äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šHIBAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰å—å¤§å°çš„é€‰æ‹©ï¼šå½±å“å±€éƒ¨ä¿¡æ¯æ•æ‰çš„èŒƒå›´ã€‚2ï¼‰å—å†…å’Œå—é—´æ³¨æ„åŠ›çš„å…·ä½“å®ç°æ–¹å¼ï¼šä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„æ³¨æ„åŠ›å¤´æ•°å’Œç»´åº¦ã€‚3ï¼‰ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼šä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨å›ºå®šæ¨¡å¼æˆ–å­¦ä¹ æ¨¡å¼æ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚4ï¼‰æ¨¡å‹å‚æ•°è§„æ¨¡ï¼šXiheæ¨¡å‹å®¶æ—åŒ…å«ä¸åŒå‚æ•°è§„æ¨¡çš„å˜ä½“ï¼Œä»¥é€‚åº”ä¸åŒçš„è®¡ç®—èµ„æºå’Œæ€§èƒ½éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Xiheæ¨¡å‹å®¶æ—åœ¨GIFT-EvalåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚å…¶ä¸­ï¼ŒXihe-tinyæ¨¡å‹ï¼ˆ950ä¸‡å‚æ•°ï¼‰è¶…è¶Šäº†å¤§å¤šæ•°å½“ä»£TSFMï¼Œå±•ç¤ºäº†å“è¶Šçš„å‚æ•°æ•ˆç‡ã€‚Xihe-maxæ¨¡å‹ï¼ˆ15äº¿å‚æ•°ï¼‰å»ºç«‹äº†æ–°çš„é›¶æ ·æœ¬æ€§èƒ½SOTAï¼Œå¤§å¹…è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³ç»“æœï¼Œè¯æ˜äº†HIBAæ¶æ„çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ã€å¼‚å¸¸æ£€æµ‹ã€åˆ†ç±»ç­‰é¢†åŸŸï¼Œä¾‹å¦‚é‡‘èå¸‚åœºçš„è¶‹åŠ¿é¢„æµ‹ã€å·¥ä¸šè®¾å¤‡çš„æ•…éšœè¯Šæ–­ã€åŒ»ç–—å¥åº·çš„å¿ƒç‡ç›‘æµ‹ç­‰ã€‚é€šè¿‡é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œå¯ä»¥å¿«é€Ÿéƒ¨ç½²åˆ°æ–°çš„æ•°æ®é›†å’Œåº”ç”¨åœºæ™¯ï¼Œé™ä½äº†æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå•†ä¸šæ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of time series foundation models (TSFMs) has been propelled by migrating architectures from language models. While existing TSFMs demonstrate impressive performance, their direct adoption of cross-domain architectures constrains effective capture of multiscale temporal dependencies inherent to time series data. This limitation becomes particularly pronounced during zero-shot transfer across datasets with divergent underlying patterns and sampling strategies. To address these challenges, we propose Hierarchical Interleaved Block Attention (HIBA) which employs hierarchical inter- and intra-block sparse attention to effectively capture multi-scale dependencies. Intra-block attention facilitates local information exchange, and inter-block attention operates across blocks to capture global temporal pattern interaction and dynamic evolution. Leveraging the HIBA architecture, we introduce Xihe, a scalable TSFM family spanning from an ultra-efficient 9.5M parameter configuration to high-capacity 1.5B variant. Evaluated on the comprehensive GIFT-Eval benchmark, our most compact Xihe-tiny model (9.5M) surpasses the majority of contemporary TSFMs, demonstrating remarkable parameter efficiency. More impressively, Xihe-max (1.5B) establishes new state-of-the-art zero-shot performance, surpassing previous best results by a substantial margin. This consistent performance excellence across the entire parameter spectrum provides compelling evidence for the exceptional generalization capabilities and architectural superiority of HIBA.

