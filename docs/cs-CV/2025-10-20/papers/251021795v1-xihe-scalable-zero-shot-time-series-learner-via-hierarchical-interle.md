---
layout: default
title: Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention
---

# Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21795" target="_blank" class="toolbar-btn">arXiv: 2510.21795v1</a>
    <a href="https://arxiv.org/pdf/2510.21795.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21795v1" 
            onclick="toggleFavorite(this, '2510.21795v1', 'Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yinbo Sun, Yuchen Fang, Zhibo Zhu, Jia Li, Yu Liu, Qiwen Deng, Jun Zhou, Hang Yu, Xingyu Lu, Lintao Ma

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂàÜÂ±Ç‰∫§ÈîôÂùóÊ≥®ÊÑèÂäõÔºàHIBAÔºâÁöÑXiheÔºåÁî®‰∫éÂèØÊâ©Â±ïÁöÑÈõ∂Ê†∑Êú¨Êó∂Èó¥Â∫èÂàóÂ≠¶‰π†„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êó∂Èó¥Â∫èÂàóÂàÜÊûê` `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `Â§öÂ∞∫Â∫¶Âª∫Ê®°` `Êó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÂú®Ë∑®È¢ÜÂüüËøÅÁßªÊû∂ÊûÑÊó∂ÔºåÈöæ‰ª•ÊúâÊïàÊçïÊçâÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆ‰∏≠Âõ∫ÊúâÁöÑÂ§öÂ∞∫Â∫¶Êó∂Èó¥‰æùËµñÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Èõ∂Ê†∑Êú¨Âú∫ÊôØ‰∏ã„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÂàÜÂ±Ç‰∫§ÈîôÂùóÊ≥®ÊÑèÂäõÔºàHIBAÔºâÊú∫Âà∂ÔºåÈÄöËøáÂàÜÂ±ÇÁöÑÂùóÂÜÖÂíåÂùóÈó¥Á®ÄÁñèÊ≥®ÊÑèÂäõÔºåÊúâÊïàÊçïÊçâÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÁöÑÂ§öÂ∞∫Â∫¶‰æùËµñÂÖ≥Á≥ª„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåXiheÊ®°ÂûãÂÆ∂ÊóèÂú®GIFT-EvalÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∂‰∏≠Xihe-tinyÊ®°ÂûãÂèÇÊï∞ÊïàÁéáÈ´òÔºåXihe-maxÊ®°ÂûãÂèñÂæó‰∫ÜÊñ∞ÁöÑÈõ∂Ê†∑Êú¨ÊÄßËÉΩSOTA„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÔºàTSFMsÔºâÁöÑÂèëÂ±ïÂèóÁõä‰∫éËØ≠Ë®ÄÊ®°ÂûãÁöÑÊû∂ÊûÑËøÅÁßª„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâTSFMsÁõ¥Êé•ÈááÁî®Ë∑®È¢ÜÂüüÊû∂ÊûÑÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊúâÊïàÊçïÊçâÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÂõ∫ÊúâÁöÑÂ§öÂ∞∫Â∫¶Êó∂Èó¥‰æùËµñÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÖ∑Êúâ‰∏çÂêåÂ∫ïÂ±ÇÊ®°ÂºèÂíåÈááÊ†∑Á≠ñÁï•ÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åÈõ∂Ê†∑Êú¨ËøÅÁßªÊó∂„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂàÜÂ±Ç‰∫§ÈîôÂùóÊ≥®ÊÑèÂäõÔºàHIBAÔºâÔºåÂÆÉÈááÁî®ÂàÜÂ±ÇÁöÑÂùóÂÜÖÂíåÂùóÈó¥Á®ÄÁñèÊ≥®ÊÑèÂäõÊù•ÊúâÊïàÂú∞ÊçïÊçâÂ§öÂ∞∫Â∫¶‰æùËµñÂÖ≥Á≥ª„ÄÇÂùóÂÜÖÊ≥®ÊÑèÂäõ‰øÉËøõÂ±ÄÈÉ®‰ø°ÊÅØ‰∫§Êç¢ÔºåÂùóÈó¥Ê≥®ÊÑèÂäõË∑®ÂùóÊìç‰Ωú‰ª•ÊçïÊçâÂÖ®Â±ÄÊó∂Èó¥Ê®°Âºè‰∫§‰∫íÂíåÂä®ÊÄÅÊºîÂåñ„ÄÇÂà©Áî®HIBAÊû∂ÊûÑÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜXiheÔºå‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑTSFMÂÆ∂ÊóèÔºåÂèÇÊï∞ËßÑÊ®°‰ªéË∂ÖÈ´òÊïàÁöÑ950‰∏áÂà∞È´òÂÆπÈáèÁöÑ15‰∫ø‰∏çÁ≠â„ÄÇÂú®ÂÖ®Èù¢ÁöÑGIFT-EvalÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÊàë‰ª¨ÊúÄÁ¥ßÂáëÁöÑXihe-tinyÊ®°ÂûãÔºà950‰∏áÂèÇÊï∞ÔºâË∂ÖË∂ä‰∫ÜÂ§ßÂ§öÊï∞ÂΩì‰ª£TSFMÔºåÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÂèÇÊï∞ÊïàÁéá„ÄÇÊõ¥‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊòØÔºåXihe-maxÔºà15‰∫øÂèÇÊï∞ÔºâÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁöÑÈõ∂Ê†∑Êú¨ÊÄßËÉΩÔºåÂ§ßÂπÖË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊúÄ‰Ω≥ÁªìÊûú„ÄÇÊï¥‰∏™ÂèÇÊï∞ËåÉÂõ¥ÂÜÖÁöÑËøôÁßç‰∏ÄËá¥ÁöÑÂçìË∂äÊÄßËÉΩ‰∏∫HIBAÁöÑÂçìË∂äÊ≥õÂåñËÉΩÂäõÂíåÊû∂ÊûÑ‰ºòÂäøÊèê‰æõ‰∫Ü‰ª§‰∫∫‰ø°ÊúçÁöÑËØÅÊçÆ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊó∂Èó¥Â∫èÂàóÂü∫Á°ÄÊ®°ÂûãÔºàTSFMsÔºâÁõ¥Êé•ÈááÁî®Ë∑®È¢ÜÂüüÊû∂ÊûÑÔºåÊó†Ê≥ïÊúâÊïàÊçïÊçâÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆ‰∏≠Âõ∫ÊúâÁöÑÂ§öÂ∞∫Â∫¶Êó∂Èó¥‰æùËµñÊÄß„ÄÇËøôÂØºËá¥Âú®ÂÖ∑Êúâ‰∏çÂêåÂ∫ïÂ±ÇÊ®°ÂºèÂíåÈááÊ†∑Á≠ñÁï•ÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åÈõ∂Ê†∑Êú¨ËøÅÁßªÊó∂ÊÄßËÉΩ‰∏ãÈôç„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπÂ±ÄÈÉ®‰ø°ÊÅØÂíåÂÖ®Â±ÄÊó∂Èó¥Ê®°Âºè‰∫§‰∫íÁöÑÊúâÊïàÂª∫Ê®°„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËÆæËÆ°‰∏ÄÁßçËÉΩÂ§üÊúâÊïàÊçïÊçâÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÂ§öÂ∞∫Â∫¶‰æùËµñÂÖ≥Á≥ªÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÇÈÄöËøáÂàÜÂ±ÇÁªìÊûÑÔºåÂàÜÂà´Âú®ÂùóÂÜÖÂíåÂùóÈó¥ËøõË°åÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºå‰ªéËÄåÂÖºÈ°æÂ±ÄÈÉ®‰ø°ÊÅØÂíåÂÖ®Â±ÄÊ®°Âºè„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®ÊèêÈ´òÊ®°ÂûãÂú®Èõ∂Ê†∑Êú¨Âú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöXiheÊ®°ÂûãÂÆ∂ÊóèÂü∫‰∫éHIBAÊû∂ÊûÑÔºåÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1ÔºâËæìÂÖ•ÂµåÂÖ•Â±ÇÔºöÂ∞ÜÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆËΩ¨Êç¢‰∏∫Ê®°ÂûãÂèØÂ§ÑÁêÜÁöÑÂµåÂÖ•Ë°®Á§∫„ÄÇ2ÔºâHIBAÂ±ÇÔºöÊ†∏ÂøÉÊ®°ÂùóÔºåÂåÖÂê´Â§ö‰∏™HIBAÂùóÔºåÊØè‰∏™ÂùóÊâßË°åÂùóÂÜÖÂíåÂùóÈó¥Ê≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇ3ÔºâËæìÂá∫Â±ÇÔºöÂ∞ÜHIBAÂ±ÇÁöÑËæìÂá∫Êò†Â∞ÑÂà∞ÁõÆÊ†áÈ¢ÑÊµã‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÊòØÂàÜÂ±Ç‰∫§ÈîôÂùóÊ≥®ÊÑèÂäõÔºàHIBAÔºâÊú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÂÖ®Â±ÄÊ≥®ÊÑèÂäõÊú∫Âà∂Áõ∏ÊØîÔºåHIBAÈÄöËøáÂàÜÂ±ÇÁªìÊûÑÂíåÁ®ÄÁñèÊ≥®ÊÑèÂäõÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÂêåÊó∂Êõ¥Â•ΩÂú∞ÊçïÊçâ‰∫ÜÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÁöÑÂ§öÂ∞∫Â∫¶‰æùËµñÂÖ≥Á≥ª„ÄÇÂùóÂÜÖÊ≥®ÊÑèÂäõÂÖ≥Ê≥®Â±ÄÈÉ®‰ø°ÊÅØÔºåÂùóÈó¥Ê≥®ÊÑèÂäõÂÖ≥Ê≥®ÂÖ®Â±ÄÊ®°Âºè‰∫§‰∫íÔºå‰∏§ËÄÖ‰∫§ÈîôËøõË°åÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöHIBAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1ÔºâÂùóÂ§ßÂ∞èÁöÑÈÄâÊã©ÔºöÂΩ±ÂìçÂ±ÄÈÉ®‰ø°ÊÅØÊçïÊçâÁöÑËåÉÂõ¥„ÄÇ2ÔºâÂùóÂÜÖÂíåÂùóÈó¥Ê≥®ÊÑèÂäõÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÔºö‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®‰∏çÂêåÁöÑÊ≥®ÊÑèÂäõÂ§¥Êï∞ÂíåÁª¥Â∫¶„ÄÇ3ÔºâÁ®ÄÁñèÊ≥®ÊÑèÂäõÊ®°ÂºèÔºö‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®Âõ∫ÂÆöÊ®°ÂºèÊàñÂ≠¶‰π†Ê®°ÂºèÊù•Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ4ÔºâÊ®°ÂûãÂèÇÊï∞ËßÑÊ®°ÔºöXiheÊ®°ÂûãÂÆ∂ÊóèÂåÖÂê´‰∏çÂêåÂèÇÊï∞ËßÑÊ®°ÁöÑÂèò‰ΩìÔºå‰ª•ÈÄÇÂ∫î‰∏çÂêåÁöÑËÆ°ÁÆóËµÑÊ∫êÂíåÊÄßËÉΩÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

XiheÊ®°ÂûãÂÆ∂ÊóèÂú®GIFT-EvalÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÊûú„ÄÇÂÖ∂‰∏≠ÔºåXihe-tinyÊ®°ÂûãÔºà950‰∏áÂèÇÊï∞ÔºâË∂ÖË∂ä‰∫ÜÂ§ßÂ§öÊï∞ÂΩì‰ª£TSFMÔºåÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÂèÇÊï∞ÊïàÁéá„ÄÇXihe-maxÊ®°ÂûãÔºà15‰∫øÂèÇÊï∞ÔºâÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÈõ∂Ê†∑Êú¨ÊÄßËÉΩSOTAÔºåÂ§ßÂπÖË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊúÄ‰Ω≥ÁªìÊûúÔºåËØÅÊòé‰∫ÜHIBAÊû∂ÊûÑÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã„ÄÅÂºÇÂ∏∏Ê£ÄÊµã„ÄÅÂàÜÁ±ªÁ≠âÈ¢ÜÂüüÔºå‰æãÂ¶ÇÈáëËûçÂ∏ÇÂú∫ÁöÑË∂ãÂäøÈ¢ÑÊµã„ÄÅÂ∑•‰∏öËÆæÂ§áÁöÑÊïÖÈöúËØäÊñ≠„ÄÅÂåªÁñóÂÅ•Â∫∑ÁöÑÂøÉÁéáÁõëÊµãÁ≠â„ÄÇÈÄöËøáÈõ∂Ê†∑Êú¨ËøÅÁßªËÉΩÂäõÔºåÂèØ‰ª•Âø´ÈÄüÈÉ®ÁΩ≤Âà∞Êñ∞ÁöÑÊï∞ÊçÆÈõÜÂíåÂ∫îÁî®Âú∫ÊôØÔºåÈôç‰Ωé‰∫ÜÊ®°ÂûãËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤ÁöÑÊàêÊú¨ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂïÜ‰∏öÊΩúÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The rapid advancement of time series foundation models (TSFMs) has been propelled by migrating architectures from language models. While existing TSFMs demonstrate impressive performance, their direct adoption of cross-domain architectures constrains effective capture of multiscale temporal dependencies inherent to time series data. This limitation becomes particularly pronounced during zero-shot transfer across datasets with divergent underlying patterns and sampling strategies. To address these challenges, we propose Hierarchical Interleaved Block Attention (HIBA) which employs hierarchical inter- and intra-block sparse attention to effectively capture multi-scale dependencies. Intra-block attention facilitates local information exchange, and inter-block attention operates across blocks to capture global temporal pattern interaction and dynamic evolution. Leveraging the HIBA architecture, we introduce Xihe, a scalable TSFM family spanning from an ultra-efficient 9.5M parameter configuration to high-capacity 1.5B variant. Evaluated on the comprehensive GIFT-Eval benchmark, our most compact Xihe-tiny model (9.5M) surpasses the majority of contemporary TSFMs, demonstrating remarkable parameter efficiency. More impressively, Xihe-max (1.5B) establishes new state-of-the-art zero-shot performance, surpassing previous best results by a substantial margin. This consistent performance excellence across the entire parameter spectrum provides compelling evidence for the exceptional generalization capabilities and architectural superiority of HIBA.

