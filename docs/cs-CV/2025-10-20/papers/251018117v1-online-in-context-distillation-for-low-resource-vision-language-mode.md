---
layout: default
title: Online In-Context Distillation for Low-Resource Vision Language Models
---

# Online In-Context Distillation for Low-Resource Vision Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.18117" target="_blank" class="toolbar-btn">arXiv: 2510.18117v1</a>
    <a href="https://arxiv.org/pdf/2510.18117.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.18117v1" 
            onclick="toggleFavorite(this, '2510.18117v1', 'Online In-Context Distillation for Low-Resource Vision Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhiqi Kang, Rahaf Aljundi, Vaggelis Dorovatas, Karteek Alahari

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âú®Á∫ø‰∏ä‰∏ãÊñáËí∏È¶èÊñπÊ≥ïÔºåÊèêÂçá‰ΩéËµÑÊ∫êËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `‰∏ä‰∏ãÊñáÂ≠¶‰π†` `Áü•ËØÜËí∏È¶è` `‰ΩéËµÑÊ∫êÂ≠¶‰π†` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ§ßÂûãVLMÈÉ®ÁΩ≤ÊàêÊú¨È´òÔºåÂ∞èÂûãVLMÈúÄÂ§ßÈáèÂæÆË∞ÉÔºåÈöæ‰ª•ÈÄÇÂ∫î‰ΩéËµÑÊ∫êÂú∫ÊôØ„ÄÇ
2. ÊèêÂá∫Âú®Á∫ø‰∏ä‰∏ãÊñáËí∏È¶è(ICD)ÊñπÊ≥ïÔºåÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÁü•ËØÜÊèêÂçáÂ∞èÂûãVLMÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåICDÊñπÊ≥ï‰ªÖÁî®Â∞ëÈáèÊïôÂ∏àÊ†áÊ≥®Âç≥ÂèØÊòæËëóÊèêÂçáÂ∞èÂûãÊ®°ÂûãÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÂÖ≥Ê≥®Â¶Ç‰ΩïÂú®‰ΩéËµÑÊ∫ê„ÄÅÈ¢ÑÁÆóÂèóÈôêÁöÑÁéØÂ¢É‰∏≠Â∫îÁî®ËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)„ÄÇÂ§ßÂûãVLMÊÄßËÉΩÂº∫Â§ßÔºå‰ΩÜÈÉ®ÁΩ≤ÊàêÊú¨È´òÊòÇ„ÄÇÂ∞èÂûãVLMÊïàÁéáÈ´òÔºå‰ΩÜÈúÄË¶ÅÊòÇË¥µÁöÑÂæÆË∞ÉÊâçËÉΩÁº©Â∞è‰∏éÂ§ßÂûãÊ®°ÂûãÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÂèó‰∏ä‰∏ãÊñáÂ≠¶‰π†Ê°ÜÊû∂ÁöÑÂêØÂèëÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®Á∫ø‰∏ä‰∏ãÊñáËí∏È¶è(ICD)ÊñπÊ≥ïÔºåÂÖ∂‰∏≠Â∞èÂûãVLMÂú®Êé®ÁêÜÊó∂‰∏éÊõ¥Âº∫Â§ßÁöÑÊïôÂ∏àÊ®°ÂûãÂçè‰ΩúÔºåÈÄöËøáÁ®ÄÁñèÁöÑÊºîÁ§∫Êù•ÊèêÂèñÁü•ËØÜÔºå‰ªéËÄåÊúâÊïàÂú∞Âº•Âêà‰∏§ËÄÖ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂü∫‰∫éÊ∑±ÂÖ•ÁöÑÂàÜÊûêÔºåÁ°ÆÂÆö‰∫ÜËßÜËßâËØ≠Ë®ÄICLÁõÆÂâçÂèØË°åÁöÑÊ®°ÂûãËßÑÊ®°ÂíåÈÄâÊã©ÔºåÂπ∂ËØÅÊòé‰∫ÜÂú®ÂèóÈôêÁöÑËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãÔºåICL‰ºò‰∫éÂæÆË∞É„ÄÇÊàë‰ª¨ÈÄöËøá‰∏ÄÁßçÊñ∞È¢ñÁöÑË∑®Ê®°ÊÄÅÊºîÁ§∫ÈÄâÊã©Á≠ñÁï•„ÄÅÊïôÂ∏àÊµãËØïÊó∂Áº©Êîæ‰ª•ÂáèÂ∞ëÂô™Â£∞‰ª•ÂèäÂ≠¶Áîü‰∏çÁ°ÆÂÆöÊÄßÊù°‰ª∂Êù•Âä®ÊÄÅÂ°´ÂÖÖÊºîÁ§∫Ê±†Âπ∂ÊúÄÂ∞èÂåñÊïôÂ∏àÊü•ËØ¢Ôºå‰ªéËÄåÂ¢ûÂº∫‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨ÁöÑICDÊñπÊ≥ï‰ΩøÁî®Á®ÄÁº∫ÁöÑÊïôÂ∏àÊ†áÊ≥®Ôºà‰ΩéËá≥4%ÔºâÊòæÁùÄÊèêÈ´ò‰∫ÜÂ∞èÂûãÊ®°ÂûãÁöÑÊÄßËÉΩÔºàÈ´òËææ33%ÔºâÔºåÂπ∂‰∏îÂèØ‰ª•‰∏éÊïôÂ∏àÁöÑÈõ∂Ê†∑Êú¨ÊÄßËÉΩÁõ∏Â™≤Áæé„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏ãÔºåÂ∞èÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÊÄßËÉΩ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇÂæÆË∞ÉÈúÄË¶ÅÂ§ßÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆÂíåËÆ°ÁÆóËµÑÊ∫êÔºåËÄåÂ§ßÂûãVLMËôΩÁÑ∂ÊÄßËÉΩÂº∫Â§ßÔºå‰ΩÜÈÉ®ÁΩ≤ÊàêÊú¨ËøáÈ´òÔºåÈöæ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠‰ΩøÁî®„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®ÊúâÈôêÁöÑËµÑÊ∫ê‰∏ãÊèêÂçáÂ∞èÂûãVLMÁöÑÊÄßËÉΩÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®‰∏ä‰∏ãÊñáÂ≠¶‰π†ÔºàIn-Context Learning, ICLÔºâÁöÑÊÄùÊÉ≥ÔºåËÆ©Â∞èÂûãVLMÂú®Êé®ÁêÜÊó∂Âä®ÊÄÅÂú∞‰ªé‰∏Ä‰∏™Êõ¥Âº∫Â§ßÁöÑÊïôÂ∏àÊ®°Âûã‰∏≠Â≠¶‰π†Áü•ËØÜ„ÄÇÈÄöËøáÊûÑÂª∫ÂêàÈÄÇÁöÑ‰∏ä‰∏ãÊñáÁ§∫‰æãÔºåÂ∞ÜÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜ‚ÄúËí∏È¶è‚ÄùÂà∞Â≠¶ÁîüÊ®°Âûã‰∏≠Ôºå‰ªéËÄåÊèêÂçáÂ≠¶ÁîüÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÈÅøÂÖç‰∫ÜÊòÇË¥µÁöÑÂæÆË∞ÉËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏Ä‰∏™Â∞èÂûãÂ≠¶ÁîüVLMÂíå‰∏Ä‰∏™Â§ßÂûãÊïôÂ∏àVLM„ÄÇÂú®Êé®ÁêÜÈò∂ÊÆµÔºåÈ¶ñÂÖàÊ†πÊçÆÂ≠¶ÁîüÊ®°ÂûãÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂä®ÊÄÅÂú∞ÈÄâÊã©‰∏ÄÈÉ®ÂàÜÊ†∑Êú¨„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®Ë∑®Ê®°ÊÄÅÊºîÁ§∫ÈÄâÊã©Á≠ñÁï•Ôºå‰ªéÊïôÂ∏àÊ®°Âûã‰∏≠ÈÄâÊã©‰∏éÂΩìÂâçÊ†∑Êú¨Áõ∏ÂÖ≥ÁöÑÁ§∫‰æã„ÄÇËøô‰∫õÁ§∫‰æã‰∏éÂΩìÂâçÊ†∑Êú¨‰∏ÄËµ∑ÊûÑÊàê‰∏ä‰∏ãÊñáÔºåËæìÂÖ•Âà∞Â≠¶ÁîüÊ®°Âûã‰∏≠ËøõË°åÊé®ÁêÜ„ÄÇ‰∏∫‰∫ÜÂáèÂ∞ëÊïôÂ∏àÊ®°ÂûãÁöÑÂô™Â£∞ÔºåÈááÁî®‰∫ÜÊïôÂ∏àÊµãËØïÊó∂Áº©ÊîæÊäÄÊúØ„ÄÇÊúÄÂêéÔºåÂ≠¶ÁîüÊ®°ÂûãÂà©Áî®‰ªéÊïôÂ∏àÊ®°Âûã‰∏≠Â≠¶‰π†Âà∞ÁöÑÁü•ËØÜÔºåËøõË°åÈ¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂú®Á∫ø‰∏ä‰∏ãÊñáËí∏È¶èÔºàICDÔºâÊñπÊ≥ïÔºåÂ∞Ü‰∏ä‰∏ãÊñáÂ≠¶‰π†‰∏éÁü•ËØÜËí∏È¶èÁõ∏ÁªìÂêàÔºåÂÆûÁé∞‰∫ÜÂú®‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏ãÊèêÂçáÂ∞èÂûãVLMÊÄßËÉΩÁöÑÁõÆÊ†á„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜË∑®Ê®°ÊÄÅÊºîÁ§∫ÈÄâÊã©Á≠ñÁï•ÂíåÊïôÂ∏àÊµãËØïÊó∂Áº©ÊîæÊäÄÊúØÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜICDÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁöÑÂæÆË∞ÉÊñπÊ≥ïÁõ∏ÊØîÔºåICDÊñπÊ≥ïÂè™ÈúÄË¶ÅÂ∞ëÈáèÁöÑÊïôÂ∏àÊ†áÊ≥®ÔºåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöË∑®Ê®°ÊÄÅÊºîÁ§∫ÈÄâÊã©Á≠ñÁï•Êó®Âú®ÈÄâÊã©‰∏éÂΩìÂâçÊ†∑Êú¨ÊúÄÁõ∏ÂÖ≥ÁöÑÁ§∫‰æãÔºå‰ª•ÊèêÈ´ò‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁöÑÊïàÁéá„ÄÇÊïôÂ∏àÊµãËØïÊó∂Áº©ÊîæÊäÄÊúØÈÄöËøáË∞ÉÊï¥ÊïôÂ∏àÊ®°ÂûãÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÂáèÂ∞ëÂô™Â£∞ÁöÑÂΩ±Âìç„ÄÇÂ≠¶Áîü‰∏çÁ°ÆÂÆöÊÄßÊù°‰ª∂Áî®‰∫éÂä®ÊÄÅÂú∞ÈÄâÊã©Ê†∑Êú¨ÔºåÁ°Æ‰øùÂ≠¶ÁîüÊ®°ÂûãËÉΩÂ§ü‰ªéÊúÄÈúÄË¶ÅÂ≠¶‰π†ÁöÑÊ†∑Êú¨‰∏≠Ëé∑ÂèñÁü•ËØÜ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞Ôºå‰ΩÜÊ≠§Â§ÑÊú™Êèê‰æõÂÖ∑‰ΩìÊï∞ÂÄº„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåICDÊñπÊ≥ïÂú®‰ªÖ‰ΩøÁî®4%ÁöÑÊïôÂ∏àÊ†áÊ≥®ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØ‰ª•Â∞ÜÂ∞èÂûãÊ®°ÂûãÁöÑÊÄßËÉΩÊèêÂçáÈ´òËææ33%ÔºåÂπ∂‰∏îÂèØ‰ª•‰∏éÊïôÂ∏àÊ®°ÂûãÁöÑÈõ∂Ê†∑Êú¨ÊÄßËÉΩÁõ∏Â™≤Áæé„ÄÇËøôË°®ÊòéICDÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®ÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜÔºåÊèêÂçáÂ∞èÂûãÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂêåÊó∂Èôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨„ÄÇËØ•ÊñπÊ≥ïÂú®‰ΩéËµÑÊ∫êËßÜËßâËØ≠Ë®Ä‰ªªÂä°‰∏≠ÂÖ∑ÊúâÊòæËëóÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÁßªÂä®ËÆæÂ§á„ÄÅÂµåÂÖ•ÂºèÁ≥ªÁªüÁ≠âËµÑÊ∫êÂèóÈôêÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÊâãÊú∫‰∏äÁöÑÂõæÂÉèËØÜÂà´„ÄÅËá™Âä®È©æÈ©∂‰∏≠ÁöÑÁõÆÊ†áÊ£ÄÊµãÁ≠â„ÄÇÈÄöËøáÂú®Á∫ø‰∏ä‰∏ãÊñáËí∏È¶èÔºåÂèØ‰ª•Âú®‰∏çÂ¢ûÂä†ËøáÂ§öËÆ°ÁÆóË¥üÊãÖÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèêÂçáÂ∞èÂûãVLMÁöÑÊÄßËÉΩÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂêÑÁßçÂÆûÈôÖÂ∫îÁî®ÈúÄÊ±Ç„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõËøõ‰∏ÄÊ≠•Êé®ÂπøÂà∞ÂÖ∂‰ªñ‰ΩéËµÑÊ∫êÊú∫Âô®Â≠¶‰π†‰ªªÂä°‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> As the field continues its push for ever more resources, this work turns the spotlight on a critical question: how can vision-language models (VLMs) be adapted to thrive in low-resource, budget-constrained settings? While large VLMs offer strong performance, they are impractical to deploy in such settings. Small VLMs, on the other hand, are efficient but typically require costly fine-tuning to close the performance gap with larger models in the deployment domain. Inspired by the in-context learning framework, we propose an online In-Context Distillation (ICD) method, in which a small VLM collaborates with a stronger teacher model at inference time, distilling its knowledge via sparse demonstrations to efficiently bridge the gap between them. Our method is built on an in-depth analysis that identifies the scale and the choice of models for which vision-language ICL is currently feasible, and demonstrates the advantage of ICL over fine-tuning under constrained compute budgets. We enhance our method with a novel cross-modal demonstration selection strategy, teacher test-time scaling to reduce noise, and student uncertainty conditioning to dynamically populate a demonstration pool and minimize teacher queries. Our ICD method significantly boosts the performance of small models (up to 33%) using scarce teacher annotations (as low as 4%), and competes with the teacher's zero-shot performance.

