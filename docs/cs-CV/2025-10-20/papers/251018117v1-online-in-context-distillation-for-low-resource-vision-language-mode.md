---
layout: default
title: Online In-Context Distillation for Low-Resource Vision Language Models
---

# Online In-Context Distillation for Low-Resource Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.18117" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.18117v1</a>
  <a href="https://arxiv.org/pdf/2510.18117.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.18117v1" onclick="toggleFavorite(this, '2510.18117v1', 'Online In-Context Distillation for Low-Resource Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiqi Kang, Rahaf Aljundi, Vaggelis Dorovatas, Karteek Alahari

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåœ¨çº¿ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•ï¼Œæå‡ä½èµ„æºè§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `çŸ¥è¯†è’¸é¦` `ä½èµ„æºå­¦ä¹ ` `è·¨æ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹VLMéƒ¨ç½²æˆæœ¬é«˜ï¼Œå°å‹VLMéœ€å¤§é‡å¾®è°ƒï¼Œéš¾ä»¥é€‚åº”ä½èµ„æºåœºæ™¯ã€‚
2. æå‡ºåœ¨çº¿ä¸Šä¸‹æ–‡è’¸é¦(ICD)æ–¹æ³•ï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹çŸ¥è¯†æå‡å°å‹VLMæ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒICDæ–¹æ³•ä»…ç”¨å°‘é‡æ•™å¸ˆæ ‡æ³¨å³å¯æ˜¾è‘—æå‡å°å‹æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å…³æ³¨å¦‚ä½•åœ¨ä½èµ„æºã€é¢„ç®—å—é™çš„ç¯å¢ƒä¸­åº”ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLM)ã€‚å¤§å‹VLMæ€§èƒ½å¼ºå¤§ï¼Œä½†éƒ¨ç½²æˆæœ¬é«˜æ˜‚ã€‚å°å‹VLMæ•ˆç‡é«˜ï¼Œä½†éœ€è¦æ˜‚è´µçš„å¾®è°ƒæ‰èƒ½ç¼©å°ä¸å¤§å‹æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚å—ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨çº¿ä¸Šä¸‹æ–‡è’¸é¦(ICD)æ–¹æ³•ï¼Œå…¶ä¸­å°å‹VLMåœ¨æ¨ç†æ—¶ä¸æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹åä½œï¼Œé€šè¿‡ç¨€ç–çš„æ¼”ç¤ºæ¥æå–çŸ¥è¯†ï¼Œä»è€Œæœ‰æ•ˆåœ°å¼¥åˆä¸¤è€…ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæ·±å…¥çš„åˆ†æï¼Œç¡®å®šäº†è§†è§‰è¯­è¨€ICLç›®å‰å¯è¡Œçš„æ¨¡å‹è§„æ¨¡å’Œé€‰æ‹©ï¼Œå¹¶è¯æ˜äº†åœ¨å—é™çš„è®¡ç®—é¢„ç®—ä¸‹ï¼ŒICLä¼˜äºå¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ã€æ•™å¸ˆæµ‹è¯•æ—¶ç¼©æ”¾ä»¥å‡å°‘å™ªå£°ä»¥åŠå­¦ç”Ÿä¸ç¡®å®šæ€§æ¡ä»¶æ¥åŠ¨æ€å¡«å……æ¼”ç¤ºæ± å¹¶æœ€å°åŒ–æ•™å¸ˆæŸ¥è¯¢ï¼Œä»è€Œå¢å¼ºäº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ICDæ–¹æ³•ä½¿ç”¨ç¨€ç¼ºçš„æ•™å¸ˆæ ‡æ³¨ï¼ˆä½è‡³4%ï¼‰æ˜¾ç€æé«˜äº†å°å‹æ¨¡å‹çš„æ€§èƒ½ï¼ˆé«˜è¾¾33%ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥ä¸æ•™å¸ˆçš„é›¶æ ·æœ¬æ€§èƒ½ç›¸åª²ç¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä½èµ„æºåœºæ™¯ä¸‹ï¼Œå°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚å¾®è°ƒéœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè€Œå¤§å‹VLMè™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†éƒ¨ç½²æˆæœ¬è¿‡é«˜ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä½¿ç”¨ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æœ‰é™çš„èµ„æºä¸‹æå‡å°å‹VLMçš„æ€§èƒ½æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learning, ICLï¼‰çš„æ€æƒ³ï¼Œè®©å°å‹VLMåœ¨æ¨ç†æ—¶åŠ¨æ€åœ°ä»ä¸€ä¸ªæ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ä¸­å­¦ä¹ çŸ¥è¯†ã€‚é€šè¿‡æ„å»ºåˆé€‚çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œå°†æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†â€œè’¸é¦â€åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œä»è€Œæå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶é¿å…äº†æ˜‚è´µçš„å¾®è°ƒè¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªå°å‹å­¦ç”ŸVLMå’Œä¸€ä¸ªå¤§å‹æ•™å¸ˆVLMã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé¦–å…ˆæ ¹æ®å­¦ç”Ÿæ¨¡å‹çš„ä¸ç¡®å®šæ€§åŠ¨æ€åœ°é€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬ã€‚ç„¶åï¼Œåˆ©ç”¨è·¨æ¨¡æ€æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ï¼Œä»æ•™å¸ˆæ¨¡å‹ä¸­é€‰æ‹©ä¸å½“å‰æ ·æœ¬ç›¸å…³çš„ç¤ºä¾‹ã€‚è¿™äº›ç¤ºä¾‹ä¸å½“å‰æ ·æœ¬ä¸€èµ·æ„æˆä¸Šä¸‹æ–‡ï¼Œè¾“å…¥åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­è¿›è¡Œæ¨ç†ã€‚ä¸ºäº†å‡å°‘æ•™å¸ˆæ¨¡å‹çš„å™ªå£°ï¼Œé‡‡ç”¨äº†æ•™å¸ˆæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ã€‚æœ€åï¼Œå­¦ç”Ÿæ¨¡å‹åˆ©ç”¨ä»æ•™å¸ˆæ¨¡å‹ä¸­å­¦ä¹ åˆ°çš„çŸ¥è¯†ï¼Œè¿›è¡Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åœ¨çº¿ä¸Šä¸‹æ–‡è’¸é¦ï¼ˆICDï¼‰æ–¹æ³•ï¼Œå°†ä¸Šä¸‹æ–‡å­¦ä¹ ä¸çŸ¥è¯†è’¸é¦ç›¸ç»“åˆï¼Œå®ç°äº†åœ¨ä½èµ„æºåœºæ™¯ä¸‹æå‡å°å‹VLMæ€§èƒ½çš„ç›®æ ‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†è·¨æ¨¡æ€æ¼”ç¤ºé€‰æ‹©ç­–ç•¥å’Œæ•™å¸ˆæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æé«˜äº†ICDæ–¹æ³•çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒICDæ–¹æ³•åªéœ€è¦å°‘é‡çš„æ•™å¸ˆæ ‡æ³¨ï¼Œå¤§å¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šè·¨æ¨¡æ€æ¼”ç¤ºé€‰æ‹©ç­–ç•¥æ—¨åœ¨é€‰æ‹©ä¸å½“å‰æ ·æœ¬æœ€ç›¸å…³çš„ç¤ºä¾‹ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ•ˆç‡ã€‚æ•™å¸ˆæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯é€šè¿‡è°ƒæ•´æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒï¼Œå‡å°‘å™ªå£°çš„å½±å“ã€‚å­¦ç”Ÿä¸ç¡®å®šæ€§æ¡ä»¶ç”¨äºåŠ¨æ€åœ°é€‰æ‹©æ ·æœ¬ï¼Œç¡®ä¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿä»æœ€éœ€è¦å­¦ä¹ çš„æ ·æœ¬ä¸­è·å–çŸ¥è¯†ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä½†æ­¤å¤„æœªæä¾›å…·ä½“æ•°å€¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒICDæ–¹æ³•åœ¨ä»…ä½¿ç”¨4%çš„æ•™å¸ˆæ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å°†å°å‹æ¨¡å‹çš„æ€§èƒ½æå‡é«˜è¾¾33%ï¼Œå¹¶ä¸”å¯ä»¥ä¸æ•™å¸ˆæ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ç›¸åª²ç¾ã€‚è¿™è¡¨æ˜ICDæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼Œæå‡å°å‹æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨ä½èµ„æºè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿç­‰èµ„æºå—é™çš„åœºæ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½æ‰‹æœºä¸Šçš„å›¾åƒè¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ä¸­çš„ç›®æ ‡æ£€æµ‹ç­‰ã€‚é€šè¿‡åœ¨çº¿ä¸Šä¸‹æ–‡è’¸é¦ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ è¿‡å¤šè®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œæå‡å°å‹VLMçš„æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å„ç§å®é™…åº”ç”¨éœ€æ±‚ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ¨å¹¿åˆ°å…¶ä»–ä½èµ„æºæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As the field continues its push for ever more resources, this work turns the spotlight on a critical question: how can vision-language models (VLMs) be adapted to thrive in low-resource, budget-constrained settings? While large VLMs offer strong performance, they are impractical to deploy in such settings. Small VLMs, on the other hand, are efficient but typically require costly fine-tuning to close the performance gap with larger models in the deployment domain. Inspired by the in-context learning framework, we propose an online In-Context Distillation (ICD) method, in which a small VLM collaborates with a stronger teacher model at inference time, distilling its knowledge via sparse demonstrations to efficiently bridge the gap between them. Our method is built on an in-depth analysis that identifies the scale and the choice of models for which vision-language ICL is currently feasible, and demonstrates the advantage of ICL over fine-tuning under constrained compute budgets. We enhance our method with a novel cross-modal demonstration selection strategy, teacher test-time scaling to reduce noise, and student uncertainty conditioning to dynamically populate a demonstration pool and minimize teacher queries. Our ICD method significantly boosts the performance of small models (up to 33%) using scarce teacher annotations (as low as 4%), and competes with the teacher's zero-shot performance.

