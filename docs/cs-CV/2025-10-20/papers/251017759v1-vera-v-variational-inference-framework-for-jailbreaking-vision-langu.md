---
layout: default
title: VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models
---

# VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.17759" target="_blank" class="toolbar-btn">arXiv: 2510.17759v1</a>
    <a href="https://arxiv.org/pdf/2510.17759.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17759v1" 
            onclick="toggleFavorite(this, '2510.17759v1', 'VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Qilin Liao, Anamika Lochab, Ruqi Zhang

**ÂàÜÁ±ª**: cs.CR, cs.CL, cs.CV, cs.LG, stat.ML

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-20

**Â§áÊ≥®**: 18 pages, 7 Figures,

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VERA-VÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊºèÊ¥ûÂèëÁé∞ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÊ®°Âûã` `ÂØπÊäóÊîªÂáª` `ÂèòÂàÜÊé®Êñ≠` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `ÂÆâÂÖ®ÊÄßÊµãËØï` `Ê®°ÂûãËÑÜÂº±ÊÄß` `ÈöêËîΩÊîªÂáª` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÊîªÂáªÊñπÊ≥ï‰æùËµñËÑÜÂº±Ê®°ÊùøÔºå‰∏îÈõÜ‰∏≠‰∫éÂçï‰∏ÄÊîªÂáªÂú∫ÊôØÔºåÂØºËá¥ÊºèÊ¥ûÂèëÁé∞ÁöÑÂ±ÄÈôêÊÄß„ÄÇ
2. VERA-VÊ°ÜÊû∂ÈÄöËøáÂ≠¶‰π†ÊñáÊú¨-ÂõæÂÉèÊèêÁ§∫ÁöÑËÅîÂêàÂêéÈ™åÂàÜÂ∏ÉÔºåÁîüÊàêÈöêËîΩÁöÑÂØπÊäóËæìÂÖ•Ôºå‰ªéËÄåÊúâÊïàÁªïËøáÊ®°ÂûãÁöÑ‰øùÊä§Êú∫Âà∂„ÄÇ
3. Âú®HarmBenchÂíåHADESÂü∫ÂáÜÊµãËØï‰∏≠ÔºåVERA-VÂú®ÊîªÂáªÊàêÂäüÁéá‰∏äÊòæËëóÊèêÂçáÔºåÊúÄÈ´òÂèØËææ53.75%ÁöÑÂ¢ûÂπÖ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÊâ©Â±ï‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂÖ∂Â§öÊ®°ÊÄÅËÆæËÆ°‰πüÂºïÂÖ•‰∫ÜÊñ∞ÁöÑ„ÄÅÂ∞öÊú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑËÑÜÂº±ÊÄß„ÄÇÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÊîªÂáªÊñπÊ≥ï‰∏ªË¶Å‰æùËµñËÑÜÂº±ÁöÑÊ®°ÊùøÔºåÈõÜ‰∏≠‰∫éÂçï‰∏ÄÊîªÂáªÂú∫ÊôØÔºåÂπ∂‰ªÖÊö¥Èú≤Âá∫Áã≠Á™ÑÁöÑÊºèÊ¥û„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÂ±ÄÈôêÊÄßÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜVERA-VÔºå‰∏Ä‰∏™ÂèòÂàÜÊé®Êñ≠Ê°ÜÊû∂ÔºåÂ∞ÜÂ§öÊ®°ÊÄÅË∂äÁã±ÂèëÁé∞ÈáçÊñ∞Ë°®Ëø∞‰∏∫Â≠¶‰π†ÊàêÂØπÊñáÊú¨-ÂõæÂÉèÊèêÁ§∫ÁöÑËÅîÂêàÂêéÈ™åÂàÜÂ∏É„ÄÇËøôÁßçÊ¶ÇÁéáËßÜËßí‰ΩøÂæóÁîüÊàêÈöêËîΩÁöÑ„ÄÅËÄ¶ÂêàÁöÑÂØπÊäóËæìÂÖ•Êàê‰∏∫ÂèØËÉΩÔºå‰ªéËÄåÁªïËøáÊ®°ÂûãÁöÑ‰øùÊä§Êú∫Âà∂„ÄÇÊàë‰ª¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™ËΩªÈáèÁ∫ßÊîªÂáªËÄÖÊù•Ëøë‰ººÂêéÈ™åÔºå‰ªéËÄåÈ´òÊïàÈááÊ†∑Â§öÊ†∑ÁöÑË∂äÁã±ËæìÂÖ•ÔºåÂπ∂Êèê‰æõÂØπÊºèÊ¥ûÁöÑÂàÜÂ∏ÉÊÄßÊ¥ûÂØü„ÄÇVERA-VËøòÊï¥Âêà‰∫Ü‰∏âÁßç‰∫íË°•Á≠ñÁï•ÔºöÂü∫‰∫éÊéíÁâàÁöÑÊñáÊú¨ÊèêÁ§∫„ÄÅÂü∫‰∫éÊâ©Êï£ÁöÑÂõæÂÉèÂêàÊàê‰ª•ÂèäÁªìÊûÑÂåñÂπ≤Êâ∞Áâ©Ôºå‰ª•ÂàÜÊï£VLMÁöÑÊ≥®ÊÑèÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVERA-VÂú®HarmBenchÂíåHADESÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÊåÅÁª≠Ë∂ÖË∂äÁé∞ÊúâÊúÄÂÖàËøõÁöÑÂü∫Á∫øÔºåÂú®GPT-4o‰∏äÊîªÂáªÊàêÂäüÁéáÊèêÈ´ò‰∫Ü53.75%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Ë∂äÁã±ÊîªÂáª‰∏≠ÁöÑËÑÜÂº±ÊÄßÔºåÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éËÑÜÂº±ÁöÑÊ®°ÊùøÔºåÊó†Ê≥ïÂÖ®Èù¢Êè≠Á§∫Ê®°ÂûãÁöÑÊºèÊ¥û„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVERA-VÊ°ÜÊû∂ÈÄöËøáÂèòÂàÜÊé®Êñ≠ÁöÑÊñπÂºèÔºåÂ∞ÜÂ§öÊ®°ÊÄÅË∂äÁã±ÈóÆÈ¢òËΩ¨Âåñ‰∏∫Â≠¶‰π†ÊñáÊú¨‰∏éÂõæÂÉèÊèêÁ§∫ÁöÑËÅîÂêàÂêéÈ™åÂàÜÂ∏ÉÔºå‰ªéËÄåÁîüÊàêÈöêËîΩÁöÑÂØπÊäóËæìÂÖ•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVERA-VÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËΩªÈáèÁ∫ßÊîªÂáªËÄÖÁî®‰∫éËøë‰ººÂêéÈ™åÂàÜÂ∏ÉÔºõ2) Âü∫‰∫éÊéíÁâàÁöÑÊñáÊú¨ÊèêÁ§∫ÂµåÂÖ•ÊúâÂÆ≥Á∫øÁ¥¢Ôºõ3) Âü∫‰∫éÊâ©Êï£ÁöÑÂõæÂÉèÂêàÊàêÂºïÂÖ•ÂØπÊäó‰ø°Âè∑„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVERA-VÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞ÜË∂äÁã±ÂèëÁé∞ËßÜ‰∏∫Ê¶ÇÁéáÈóÆÈ¢òÔºåÂÖÅËÆ∏ÁîüÊàêËÄ¶ÂêàÁöÑÂØπÊäóËæìÂÖ•ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊîªÂáªÁöÑÈöêËîΩÊÄßÂíåÊàêÂäüÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÂêéÈ™åËøë‰ººÔºåÁΩëÁªúÁªìÊûÑÁªèËøáÁ≤æÁÆÄ‰ª•ÊèêÈ´òÈááÊ†∑ÊïàÁéáÔºåÂêåÊó∂ÂºïÂÖ•ÁªìÊûÑÂåñÂπ≤Êâ∞Áâ©‰ª•ÂàÜÊï£Ê®°ÂûãÁöÑÊ≥®ÊÑèÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVERA-VÂú®HarmBenchÂíåHADESÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫éÊúÄ‰Ω≥Âü∫Á∫øÂú®GPT-4o‰∏äÊîªÂáªÊàêÂäüÁéáÊèêÈ´ò‰∫Ü53.75%ÔºåÂ±ïÁé∞‰∫ÜÂÖ∂Âú®Â§öÊ®°ÊÄÅË∂äÁã±ÊîªÂáª‰∏≠ÁöÑÊúâÊïàÊÄßÂíå‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂÆâÂÖ®ÊÄßÊµãËØï„ÄÅÂØπÊäóÊ†∑Êú¨ÁîüÊàê‰ª•ÂèäÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßËØÑ‰º∞„ÄÇÈÄöËøáÊè≠Á§∫Ê®°ÂûãÁöÑËÑÜÂº±ÊÄßÔºåVERA-VËÉΩÂ§üÂ∏ÆÂä©ÂºÄÂèëÊõ¥ÂÆâÂÖ®ÁöÑËßÜËßâ-ËØ≠Ë®ÄÁ≥ªÁªüÔºåÊèêÂçáÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÈù†ÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Models (VLMs) extend large language models with visual reasoning, but their multimodal design also introduces new, underexplored vulnerabilities. Existing multimodal red-teaming methods largely rely on brittle templates, focus on single-attack settings, and expose only a narrow subset of vulnerabilities. To address these limitations, we introduce VERA-V, a variational inference framework that recasts multimodal jailbreak discovery as learning a joint posterior distribution over paired text-image prompts. This probabilistic view enables the generation of stealthy, coupled adversarial inputs that bypass model guardrails. We train a lightweight attacker to approximate the posterior, allowing efficient sampling of diverse jailbreaks and providing distributional insights into vulnerabilities. VERA-V further integrates three complementary strategies: (i) typography-based text prompts that embed harmful cues, (ii) diffusion-based image synthesis that introduces adversarial signals, and (iii) structured distractors to fragment VLM attention. Experiments on HarmBench and HADES benchmarks show that VERA-V consistently outperforms state-of-the-art baselines on both open-source and frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the best baseline on GPT-4o.

