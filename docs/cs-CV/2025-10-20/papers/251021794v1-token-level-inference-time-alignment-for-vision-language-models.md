---
layout: default
title: Token-Level Inference-Time Alignment for Vision-Language Models
---

# Token-Level Inference-Time Alignment for Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21794" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21794v1</a>
  <a href="https://arxiv.org/pdf/2510.21794.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21794v1" onclick="toggleFavorite(this, '2510.21794v1', 'Token-Level Inference-Time Alignment for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kejia Chen, Jiawen Zhang, Jiacong Hu, Kewei Gao, Jian Lou, Zunlei Feng, Mingli Song

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTITAï¼šä¸€ç§ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹Tokençº§æ¨ç†æ—¶å¯¹é½çš„è½»é‡çº§æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ¨ç†æ—¶å¯¹é½` `å¹»è§‰å‡å°‘` `å¥–åŠ±æ¨¡å‹` `ç›´æ¥åå¥½ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œè¾“å‡ºä¸è§†è§‰è¾“å…¥ä¸ç¬¦ï¼Œä¸”ç°æœ‰å¯¹é½æ–¹æ³•æˆæœ¬é«˜æ˜‚æˆ–åé¦ˆä¸è¶³ã€‚
2. TITAæ¡†æ¶é€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹è¿‘ä¼¼VLMåˆ†å¸ƒï¼Œæå–tokençº§éšå¼åå¥½ä¿¡å·ï¼Œå®ç°æ¨ç†æ—¶å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTITAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œæœ‰æ•ˆå‡å°‘äº†å¹»è§‰ï¼Œä¸”æ¨ç†å¼€é”€æå°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)å·²æˆä¸ºç°ä»£å¤šæ¨¡æ€æ™ºèƒ½çš„å…³é”®éª¨å¹²ï¼Œä½†å…¶è¾“å‡ºä»ç„¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå³ç”Ÿæˆä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´çš„çœ‹ä¼¼åˆç†çš„æ–‡æœ¬ã€‚ç°æœ‰çš„å¯¹é½æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„ã€å¸¦æœ‰æ ‡æ³¨åå¥½æ•°æ®çš„å¾®è°ƒï¼Œæˆ–è€…ä»…æä¾›ç²—ç•¥ã€å»¶è¿Ÿåé¦ˆçš„åºåˆ—çº§æ¨ç†ç­–ç•¥ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†TITA(Token-level Inference-Time Alignment)ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œå®ƒå†»ç»“äº†åŸºç¡€VLMï¼Œè€Œæ˜¯è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥è¿‘ä¼¼å…¶åˆ†å¸ƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œéšå¼åå¥½ä¿¡å·è¢«æå–ä¸ºå¥–åŠ±æ¨¡å‹å’Œç›®æ ‡VLMä¹‹é—´çš„å¯¹æ•°æ¦‚ç‡æ¯”ï¼Œä»è€Œäº§ç”Ÿå¯†é›†çš„è‡ªå›å½’åé¦ˆã€‚è¿™ç§å…¬å¼å¯ä»¥è¢«çœ‹ä½œæ˜¯ç›´æ¥åå¥½ä¼˜åŒ–(DPO)çš„æ¨ç†æ—¶å˜ä½“ï¼Œæä¾›tokençº§åˆ«çš„æ ¡æ­£ä¿¡å·ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒéª¨å¹²ç½‘ç»œã€‚åœ¨LLaVA-1.5-7Bå’Œ13Bä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œåœ¨12ä¸ªåŸºå‡†æµ‹è¯•ä¸­éƒ½è·å¾—äº†æŒç»­çš„æ”¶ç›Šï¼Œåœ¨MMVetä¸Šæé«˜äº†8.6%ï¼Œåœ¨POPEä¸Šæé«˜äº†6.7%ï¼Œè¡¨æ˜æ›´å¼ºçš„é€šç”¨ç†è§£å’Œå‡å°‘çš„å¹»è§‰ã€‚åœ¨Qwen2.5-VL-7Bå’ŒDeepSeek-VL2-27.5Bä¸Šçš„é¢å¤–å®éªŒæ˜¾ç¤ºäº†ç›¸å½“çš„æ”¶ç›Šï¼Œå°¤å…¶æ˜¯åœ¨å‡å°‘å¹»è§‰å’ŒVQAå‡†ç¡®æ€§æ–¹é¢ï¼ŒåŒæ—¶äº§ç”Ÿçš„æ¨ç†å¼€é”€å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå³ç”Ÿæˆä¸å›¾åƒå†…å®¹ä¸ç¬¦ä½†è¯­æ³•ä¸Šåˆç†çš„æ–‡æœ¬ã€‚ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œå¦‚å¾®è°ƒæˆ–åºåˆ—çº§æ¨ç†ï¼Œå­˜åœ¨æˆæœ¬é«˜ã€åé¦ˆå»¶è¿Ÿæˆ–ä¸å¤Ÿç²¾ç»†çš„é—®é¢˜ï¼Œéš¾ä»¥æœ‰æ•ˆè§£å†³å¹»è§‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTITAçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æ¨ç†æ—¶è¿›è¡Œtokençº§åˆ«çš„å¯¹é½ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ¯ä¸ªtokençš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶åˆ©ç”¨å¥–åŠ±æ¨¡å‹ä¸åŸå§‹VLMçš„æ¦‚ç‡æ¯”ä½œä¸ºåé¦ˆä¿¡å·ï¼Œå¼•å¯¼VLMç”Ÿæˆæ›´ç¬¦åˆè§†è§‰å†…å®¹çš„æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒVLMï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTITAæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå†»ç»“çš„åŸºç¡€VLMå’Œä¸€ä¸ªè®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒVLMè‡ªå›å½’åœ°ç”Ÿæˆtokenï¼ŒåŒæ—¶å¥–åŠ±æ¨¡å‹è¯„ä¼°æ¯ä¸ªtokençš„è´¨é‡ã€‚ç„¶åï¼Œè®¡ç®—å¥–åŠ±æ¨¡å‹å’ŒVLMçš„å¯¹æ•°æ¦‚ç‡æ¯”ï¼Œä½œä¸ºtokençº§åˆ«çš„æ ¡æ­£ä¿¡å·ï¼Œç”¨äºè°ƒæ•´VLMçš„ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒã€‚æ•´ä¸ªè¿‡ç¨‹å¯ä»¥çœ‹ä½œæ˜¯DPOï¼ˆDirect Preference Optimizationï¼‰åœ¨æ¨ç†æ—¶çš„ä¸€ç§å˜ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šTITAçš„å…³é”®åˆ›æ–°åœ¨äºtokençº§åˆ«çš„æ¨ç†æ—¶å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„åºåˆ—çº§å¯¹é½æ–¹æ³•ç›¸æ¯”ï¼ŒTITAèƒ½å¤Ÿæä¾›æ›´ç²¾ç»†çš„åé¦ˆï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å‡å°‘å¹»è§‰ã€‚æ­¤å¤–ï¼ŒTITAæ— éœ€é‡æ–°è®­ç»ƒVLMï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶æ›´æ˜“äºéƒ¨ç½²å’Œåº”ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±æ¨¡å‹çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦èƒ½å¤Ÿå‡†ç¡®è¯„ä¼°tokençš„ç”Ÿæˆè´¨é‡ã€‚æŸå¤±å‡½æ•°é€šå¸¸é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æˆ–åå¥½å­¦ä¹ çš„æ–¹å¼ï¼Œä¾‹å¦‚DPOã€‚åœ¨æ¨ç†æ—¶ï¼Œæ ¡æ­£ä¿¡å·çš„å¼ºåº¦éœ€è¦ä»”ç»†è°ƒæ•´ï¼Œä»¥é¿å…è¿‡åº¦å¹²é¢„VLMçš„ç”Ÿæˆè¿‡ç¨‹ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨LLaVA-1.5-7Bå’Œ13Bæ¨¡å‹ä¸Šï¼ŒTITAåœ¨12ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æå‡ï¼ŒMMVetä¸Šæå‡8.6%ï¼ŒPOPEä¸Šæå‡6.7%ï¼Œè¡¨æ˜TITAèƒ½æœ‰æ•ˆæå‡VLMçš„é€šç”¨ç†è§£èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰ã€‚åœ¨Qwen2.5-VL-7Bå’ŒDeepSeek-VL2-27.5Bä¸Šçš„å®éªŒä¹Ÿæ˜¾ç¤ºäº†ç›¸ä¼¼çš„å¢ç›Šï¼Œå°¤å…¶æ˜¯åœ¨å¹»è§‰å‡å°‘å’ŒVQAå‡†ç¡®æ€§æ–¹é¢ï¼Œä¸”æ¨ç†å¼€é”€å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TITAæ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€è§†è§‰æ¨ç†ç­‰ã€‚é€šè¿‡å‡å°‘å¹»è§‰ï¼Œå¯ä»¥æé«˜VLMåœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å°¤å…¶é€‚ç”¨äºå¯¹å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have become essential backbones of modern multimodal intelligence, yet their outputs remain prone to hallucination-plausible text misaligned with visual inputs. Existing alignment approaches often rely on expensive fine-tuning with annotated preference data or sequence-level inference strategies that provide only coarse, delayed feedback. To overcome these limitations, we present TITA (Token-level Inference-Time Alignment), a lightweight framework that freezes the base VLM and instead trains a reward model to approximate its distribution. During inference, implicit preference signals are extracted as log-probability ratios between the reward model and the target VLM, yielding dense autoregressive feedback. This formulation can be viewed as an inference-time variant of Direct Preference Optimization (DPO), providing token-level corrective signals without retraining the backbone. Extensive evaluations on LLaVA-1.5-7B and 13B show consistent gains across 12 benchmarks, with improvements of 8.6% on MMVet and 6.7% on POPE, indicating stronger general understanding and reduced hallucinations. Additional experiments on Qwen2.5-VL-7B and DeepSeek-VL2-27.5B show comparable gains, especially in hallucination reduction and VQA accuracy, while incurring negligible inference overhead.

