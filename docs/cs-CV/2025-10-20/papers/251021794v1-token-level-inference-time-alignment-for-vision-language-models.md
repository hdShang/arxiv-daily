---
layout: default
title: Token-Level Inference-Time Alignment for Vision-Language Models
---

# Token-Level Inference-Time Alignment for Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21794" target="_blank" class="toolbar-btn">arXiv: 2510.21794v1</a>
    <a href="https://arxiv.org/pdf/2510.21794.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21794v1" 
            onclick="toggleFavorite(this, '2510.21794v1', 'Token-Level Inference-Time Alignment for Vision-Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kejia Chen, Jiawen Zhang, Jiacong Hu, Kewei Gao, Jian Lou, Zunlei Feng, Mingli Song

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-20

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫TITAÔºö‰∏ÄÁßçÁî®‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãTokenÁ∫ßÊé®ÁêÜÊó∂ÂØπÈΩêÁöÑËΩªÈáèÁ∫ßÊ°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜÊó∂ÂØπÈΩê` `ÂπªËßâÂáèÂ∞ë` `Â•ñÂä±Ê®°Âûã` `Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊòì‰∫ßÁîüÂπªËßâÔºåËæìÂá∫‰∏éËßÜËßâËæìÂÖ•‰∏çÁ¨¶Ôºå‰∏îÁé∞ÊúâÂØπÈΩêÊñπÊ≥ïÊàêÊú¨È´òÊòÇÊàñÂèçÈ¶à‰∏çË∂≥„ÄÇ
2. TITAÊ°ÜÊû∂ÈÄöËøáËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãËøë‰ººVLMÂàÜÂ∏ÉÔºåÊèêÂèñtokenÁ∫ßÈöêÂºèÂÅèÂ•Ω‰ø°Âè∑ÔºåÂÆûÁé∞Êé®ÁêÜÊó∂ÂØπÈΩê„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåTITAÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÊúâÊïàÂáèÂ∞ë‰∫ÜÂπªËßâÔºå‰∏îÊé®ÁêÜÂºÄÈîÄÊûÅÂ∞è„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLMs)Â∑≤Êàê‰∏∫Áé∞‰ª£Â§öÊ®°ÊÄÅÊô∫ËÉΩÁöÑÂÖ≥ÈîÆÈ™®Âπ≤Ôºå‰ΩÜÂÖ∂ËæìÂá∫‰ªçÁÑ∂ÂÆπÊòì‰∫ßÁîüÂπªËßâÔºåÂç≥ÁîüÊàê‰∏éËßÜËßâËæìÂÖ•‰∏ç‰∏ÄËá¥ÁöÑÁúã‰ººÂêàÁêÜÁöÑÊñáÊú¨„ÄÇÁé∞ÊúâÁöÑÂØπÈΩêÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÊòÇË¥µÁöÑ„ÄÅÂ∏¶ÊúâÊ†áÊ≥®ÂÅèÂ•ΩÊï∞ÊçÆÁöÑÂæÆË∞ÉÔºåÊàñËÄÖ‰ªÖÊèê‰æõÁ≤óÁï•„ÄÅÂª∂ËøüÂèçÈ¶àÁöÑÂ∫èÂàóÁ∫ßÊé®ÁêÜÁ≠ñÁï•„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜTITA(Token-level Inference-Time Alignment)ÔºåËøôÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÊ°ÜÊû∂ÔºåÂÆÉÂÜªÁªì‰∫ÜÂü∫Á°ÄVLMÔºåËÄåÊòØËÆ≠ÁªÉ‰∏Ä‰∏™Â•ñÂä±Ê®°ÂûãÊù•Ëøë‰ººÂÖ∂ÂàÜÂ∏É„ÄÇÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÔºåÈöêÂºèÂÅèÂ•Ω‰ø°Âè∑Ë¢´ÊèêÂèñ‰∏∫Â•ñÂä±Ê®°ÂûãÂíåÁõÆÊ†áVLM‰πãÈó¥ÁöÑÂØπÊï∞Ê¶ÇÁéáÊØîÔºå‰ªéËÄå‰∫ßÁîüÂØÜÈõÜÁöÑËá™ÂõûÂΩíÂèçÈ¶à„ÄÇËøôÁßçÂÖ¨ÂºèÂèØ‰ª•Ë¢´Áúã‰ΩúÊòØÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ(DPO)ÁöÑÊé®ÁêÜÊó∂Âèò‰ΩìÔºåÊèê‰æõtokenÁ∫ßÂà´ÁöÑÊ†°Ê≠£‰ø°Âè∑ÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉÈ™®Âπ≤ÁΩëÁªú„ÄÇÂú®LLaVA-1.5-7BÂíå13B‰∏äÁöÑÂπøÊ≥õËØÑ‰º∞Ë°®ÊòéÔºåÂú®12‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÈÉΩËé∑Âæó‰∫ÜÊåÅÁª≠ÁöÑÊî∂ÁõäÔºåÂú®MMVet‰∏äÊèêÈ´ò‰∫Ü8.6%ÔºåÂú®POPE‰∏äÊèêÈ´ò‰∫Ü6.7%ÔºåË°®ÊòéÊõ¥Âº∫ÁöÑÈÄöÁî®ÁêÜËß£ÂíåÂáèÂ∞ëÁöÑÂπªËßâ„ÄÇÂú®Qwen2.5-VL-7BÂíåDeepSeek-VL2-27.5B‰∏äÁöÑÈ¢ùÂ§ñÂÆûÈ™åÊòæÁ§∫‰∫ÜÁõ∏ÂΩìÁöÑÊî∂ÁõäÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂáèÂ∞ëÂπªËßâÂíåVQAÂáÜÁ°ÆÊÄßÊñπÈù¢ÔºåÂêåÊó∂‰∫ßÁîüÁöÑÊé®ÁêÜÂºÄÈîÄÂèØ‰ª•ÂøΩÁï•‰∏çËÆ°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂú®ÁîüÊàêÊñáÊú¨Êó∂ÂÆπÊòì‰∫ßÁîüÂπªËßâÔºåÂç≥ÁîüÊàê‰∏éÂõæÂÉèÂÜÖÂÆπ‰∏çÁ¨¶‰ΩÜËØ≠Ê≥ï‰∏äÂêàÁêÜÁöÑÊñáÊú¨„ÄÇÁé∞ÊúâÁöÑÂØπÈΩêÊñπÊ≥ïÔºåÂ¶ÇÂæÆË∞ÉÊàñÂ∫èÂàóÁ∫ßÊé®ÁêÜÔºåÂ≠òÂú®ÊàêÊú¨È´ò„ÄÅÂèçÈ¶àÂª∂ËøüÊàñ‰∏çÂ§üÁ≤æÁªÜÁöÑÈóÆÈ¢òÔºåÈöæ‰ª•ÊúâÊïàËß£ÂÜ≥ÂπªËßâÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöTITAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂú®Êé®ÁêÜÊó∂ËøõË°åtokenÁ∫ßÂà´ÁöÑÂØπÈΩêÔºåÈÄöËøáÂ•ñÂä±Ê®°ÂûãÊù•ËØÑ‰º∞ÊØè‰∏™tokenÁöÑÁîüÊàêË¥®ÈáèÔºåÂπ∂Âà©Áî®Â•ñÂä±Ê®°Âûã‰∏éÂéüÂßãVLMÁöÑÊ¶ÇÁéáÊØî‰Ωú‰∏∫ÂèçÈ¶à‰ø°Âè∑ÔºåÂºïÂØºVLMÁîüÊàêÊõ¥Á¨¶ÂêàËßÜËßâÂÜÖÂÆπÁöÑÊñáÊú¨„ÄÇËøôÁßçÊñπÊ≥ïÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉVLMÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöTITAÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂÜªÁªìÁöÑÂü∫Á°ÄVLMÂíå‰∏Ä‰∏™ËÆ≠ÁªÉÂ•ΩÁöÑÂ•ñÂä±Ê®°Âûã„ÄÇÂú®Êé®ÁêÜÈò∂ÊÆµÔºåVLMËá™ÂõûÂΩíÂú∞ÁîüÊàêtokenÔºåÂêåÊó∂Â•ñÂä±Ê®°ÂûãËØÑ‰º∞ÊØè‰∏™tokenÁöÑË¥®Èáè„ÄÇÁÑ∂ÂêéÔºåËÆ°ÁÆóÂ•ñÂä±Ê®°ÂûãÂíåVLMÁöÑÂØπÊï∞Ê¶ÇÁéáÊØîÔºå‰Ωú‰∏∫tokenÁ∫ßÂà´ÁöÑÊ†°Ê≠£‰ø°Âè∑ÔºåÁî®‰∫éË∞ÉÊï¥VLMÁöÑÁîüÊàêÊ¶ÇÁéáÂàÜÂ∏É„ÄÇÊï¥‰∏™ËøáÁ®ãÂèØ‰ª•Áúã‰ΩúÊòØDPOÔºàDirect Preference OptimizationÔºâÂú®Êé®ÁêÜÊó∂ÁöÑ‰∏ÄÁßçÂèò‰Ωì„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöTITAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫étokenÁ∫ßÂà´ÁöÑÊé®ÁêÜÊó∂ÂØπÈΩê„ÄÇ‰∏é‰º†ÁªüÁöÑÂ∫èÂàóÁ∫ßÂØπÈΩêÊñπÊ≥ïÁõ∏ÊØîÔºåTITAËÉΩÂ§üÊèê‰æõÊõ¥Á≤æÁªÜÁöÑÂèçÈ¶àÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞ÂáèÂ∞ëÂπªËßâ„ÄÇÊ≠§Â§ñÔºåTITAÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉVLMÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨Ôºå‰ΩøÂÖ∂Êõ¥Êòì‰∫éÈÉ®ÁΩ≤ÂíåÂ∫îÁî®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ•ñÂä±Ê®°ÂûãÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÈúÄË¶ÅËÉΩÂ§üÂáÜÁ°ÆËØÑ‰º∞tokenÁöÑÁîüÊàêË¥®Èáè„ÄÇÊçüÂ§±ÂáΩÊï∞ÈÄöÂ∏∏ÈááÁî®ÂØπÊØîÂ≠¶‰π†ÊàñÂÅèÂ•ΩÂ≠¶‰π†ÁöÑÊñπÂºèÔºå‰æãÂ¶ÇDPO„ÄÇÂú®Êé®ÁêÜÊó∂ÔºåÊ†°Ê≠£‰ø°Âè∑ÁöÑÂº∫Â∫¶ÈúÄË¶Å‰ªîÁªÜË∞ÉÊï¥Ôºå‰ª•ÈÅøÂÖçËøáÂ∫¶Âπ≤È¢ÑVLMÁöÑÁîüÊàêËøáÁ®ã„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁöÑÈÄâÊã©ÂèØËÉΩÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®LLaVA-1.5-7BÂíå13BÊ®°Âûã‰∏äÔºåTITAÂú®12‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåMMVet‰∏äÊèêÂçá8.6%ÔºåPOPE‰∏äÊèêÂçá6.7%ÔºåË°®ÊòéTITAËÉΩÊúâÊïàÊèêÂçáVLMÁöÑÈÄöÁî®ÁêÜËß£ËÉΩÂäõÂπ∂ÂáèÂ∞ëÂπªËßâ„ÄÇÂú®Qwen2.5-VL-7BÂíåDeepSeek-VL2-27.5B‰∏äÁöÑÂÆûÈ™å‰πüÊòæÁ§∫‰∫ÜÁõ∏‰ººÁöÑÂ¢ûÁõäÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂπªËßâÂáèÂ∞ëÂíåVQAÂáÜÁ°ÆÊÄßÊñπÈù¢Ôºå‰∏îÊé®ÁêÜÂºÄÈîÄÂèØ‰ª•ÂøΩÁï•‰∏çËÆ°„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

TITAÊ°ÜÊû∂ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°Ôºå‰æãÂ¶ÇÂõæÂÉèÊèèËø∞„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅËßÜËßâÊé®ÁêÜÁ≠â„ÄÇÈÄöËøáÂáèÂ∞ëÂπªËßâÔºåÂèØ‰ª•ÊèêÈ´òVLMÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÈù†ÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÂØπÂÆâÂÖ®ÊÄßË¶ÅÊ±ÇËæÉÈ´òÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂåªÁñóÂΩ±ÂÉèÂàÜÊûê„ÄÅËá™Âä®È©æÈ©∂Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language Models (VLMs) have become essential backbones of modern multimodal intelligence, yet their outputs remain prone to hallucination-plausible text misaligned with visual inputs. Existing alignment approaches often rely on expensive fine-tuning with annotated preference data or sequence-level inference strategies that provide only coarse, delayed feedback. To overcome these limitations, we present TITA (Token-level Inference-Time Alignment), a lightweight framework that freezes the base VLM and instead trains a reward model to approximate its distribution. During inference, implicit preference signals are extracted as log-probability ratios between the reward model and the target VLM, yielding dense autoregressive feedback. This formulation can be viewed as an inference-time variant of Direct Preference Optimization (DPO), providing token-level corrective signals without retraining the backbone. Extensive evaluations on LLaVA-1.5-7B and 13B show consistent gains across 12 benchmarks, with improvements of 8.6% on MMVet and 6.7% on POPE, indicating stronger general understanding and reduced hallucinations. Additional experiments on Qwen2.5-VL-7B and DeepSeek-VL2-27.5B show comparable gains, especially in hallucination reduction and VQA accuracy, while incurring negligible inference overhead.

