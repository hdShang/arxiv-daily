---
layout: default
title: ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues
---

# ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.18016" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.18016v2</a>
  <a href="https://arxiv.org/pdf/2510.18016.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.18016v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.18016v2', 'ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: 10 pages, 4 figures, 2 tables

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/prateek-gothwal/ViBED-Net)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ViBED-Netï¼šåˆ©ç”¨äººè„¸å’Œåœºæ™¯æ—¶ç©ºçº¿ç´¢è¿›è¡Œè§†é¢‘å‚ä¸åº¦æ£€æµ‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å‚ä¸åº¦æ£€æµ‹` `åœ¨çº¿å­¦ä¹ ` `äººè„¸è¯†åˆ«` `åœºæ™¯ç†è§£` `æ—¶ç©ºå»ºæ¨¡` `æ·±åº¦å­¦ä¹ ` `åŒæµç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åœ¨çº¿å­¦ä¹ å‚ä¸åº¦æ£€æµ‹æ–¹æ³•ç¼ºä¹å¯¹å­¦ç”Ÿé¢éƒ¨è¡¨æƒ…å’Œå­¦ä¹ åœºæ™¯ä¸Šä¸‹æ–‡çš„ç»¼åˆè€ƒè™‘ï¼Œé™åˆ¶äº†æ£€æµ‹ç²¾åº¦ã€‚
2. ViBED-Neté‡‡ç”¨åŒæµæ¶æ„ï¼Œåˆ†åˆ«å¤„ç†äººè„¸å’Œåœºæ™¯ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨LSTMå’ŒTransformerè¿›è¡Œæ—¶åºå»ºæ¨¡ï¼Œèåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚
3. åœ¨DAiSEEæ•°æ®é›†ä¸Šï¼ŒViBED-Netç»“åˆLSTMå®ç°äº†73.43%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºViBED-Netï¼ˆåŸºäºè§†é¢‘çš„å‚ä¸åº¦æ£€æµ‹ç½‘ç»œï¼‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŒæµæ¶æ„ä»è§†é¢‘æ•°æ®ä¸­è¯„ä¼°å­¦ç”Ÿçš„å‚ä¸åº¦ï¼Œä»è€Œæ”¹è¿›åœ¨çº¿å­¦ä¹ ç¯å¢ƒä¸­çš„å­¦ç”Ÿæˆæœå¹¶å®ç°ä¸ªæ€§åŒ–æ•™å­¦ã€‚ViBED-Neté€šè¿‡EfficientNetV2å¤„ç†äººè„¸è£å‰ªå’Œå®Œæ•´è§†é¢‘å¸§ï¼Œä»¥æå–ç©ºé—´ç‰¹å¾ï¼Œä»è€Œæ•è·é¢éƒ¨è¡¨æƒ…å’Œå®Œæ•´åœºæ™¯ä¸Šä¸‹æ–‡ã€‚ç„¶åï¼Œä½¿ç”¨é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ç½‘ç»œå’ŒTransformerç¼–ç å™¨ä¸¤ç§æ—¶é—´å»ºæ¨¡ç­–ç•¥æ¥åˆ†æè¿™äº›ç‰¹å¾éšæ—¶é—´çš„å˜åŒ–ã€‚è¯¥æ¨¡å‹åœ¨DAiSEEæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”µå­å­¦ä¹ ä¸­æƒ…æ„ŸçŠ¶æ€è¯†åˆ«çš„å¤§è§„æ¨¡åŸºå‡†ã€‚ä¸ºäº†æé«˜åœ¨ä»£è¡¨æ€§ä¸è¶³çš„å‚ä¸åº¦ç±»åˆ«ä¸Šçš„æ€§èƒ½ï¼Œæˆ‘ä»¬åº”ç”¨äº†æœ‰é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚åœ¨æµ‹è¯•çš„å˜ä½“ä¸­ï¼Œå¸¦æœ‰LSTMçš„ViBED-Netå®ç°äº†73.43ï¼…çš„å‡†ç¡®ç‡ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ViBED-Netè¡¨æ˜ï¼Œç»“åˆäººè„¸æ„ŸçŸ¥å’Œåœºæ™¯æ„ŸçŸ¥çš„æ—¶ç©ºçº¿ç´¢å¯ä»¥æ˜¾ç€æé«˜å‚ä¸åº¦æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶å¯ä»¥çµæ´»åœ°åº”ç”¨äºæ•™è‚²ã€ç”¨æˆ·ä½“éªŒç ”ç©¶å’Œå†…å®¹ä¸ªæ€§åŒ–ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ä¸ºå®é™…å‚ä¸åº¦åˆ†ææä¾›å¯æ‰©å±•çš„é«˜æ€§èƒ½è§£å†³æ–¹æ¡ˆï¼Œä»è€Œæ¨è¿›äº†åŸºäºè§†é¢‘çš„æƒ…æ„Ÿè®¡ç®—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨çº¿å­¦ä¹ ç¯å¢ƒä¸­å­¦ç”Ÿå‚ä¸åº¦æ£€æµ‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨é¢éƒ¨ç‰¹å¾æˆ–åœºæ™¯ä¿¡æ¯ï¼Œç¼ºä¹å¯¹ä¸¤è€…ä¹‹é—´å…³ç³»çš„å»ºæ¨¡ï¼Œå¯¼è‡´æ£€æµ‹ç²¾åº¦ä¸é«˜ï¼Œéš¾ä»¥å‡†ç¡®åæ˜ å­¦ç”Ÿçš„çœŸå®å‚ä¸çŠ¶æ€ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åŒæ—¶åˆ©ç”¨äººè„¸æ„ŸçŸ¥å’Œåœºæ™¯æ„ŸçŸ¥çš„æ—¶ç©ºçº¿ç´¢è¿›è¡Œå‚ä¸åº¦æ£€æµ‹ã€‚é€šè¿‡åŒæµç½‘ç»œåˆ†åˆ«æå–äººè„¸å’Œåœºæ™¯çš„ç©ºé—´ç‰¹å¾ï¼Œç„¶ååˆ©ç”¨æ—¶åºæ¨¡å‹æ•æ‰ç‰¹å¾éšæ—¶é—´çš„å˜åŒ–ï¼Œæœ€åèåˆä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯è¿›è¡Œé¢„æµ‹ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£å­¦ç”Ÿçš„å‚ä¸çŠ¶æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šViBED-Neté‡‡ç”¨åŒæµæ¶æ„ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) äººè„¸æµï¼šä½¿ç”¨äººè„¸æ£€æµ‹å™¨æå–äººè„¸åŒºåŸŸï¼Œç„¶åä½¿ç”¨EfficientNetV2æå–äººè„¸çš„ç©ºé—´ç‰¹å¾ã€‚2) åœºæ™¯æµï¼šç›´æ¥ä½¿ç”¨EfficientNetV2æå–å®Œæ•´è§†é¢‘å¸§çš„ç©ºé—´ç‰¹å¾ã€‚3) æ—¶åºå»ºæ¨¡ï¼šåˆ†åˆ«ä½¿ç”¨LSTMç½‘ç»œå’ŒTransformerç¼–ç å™¨å¯¹äººè„¸å’Œåœºæ™¯çš„æ—¶åºç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚4) èåˆä¸åˆ†ç±»ï¼šå°†äººè„¸å’Œåœºæ™¯çš„æ—¶åºç‰¹å¾è¿›è¡Œèåˆï¼Œç„¶åä½¿ç”¨å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»ï¼Œé¢„æµ‹å­¦ç”Ÿçš„å‚ä¸åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åŒæµæ¶æ„ï¼ŒåŒæ—¶è€ƒè™‘äººè„¸å’Œåœºæ™¯ä¿¡æ¯ï¼Œæ›´å…¨é¢åœ°ç†è§£å­¦ç”Ÿçš„å‚ä¸çŠ¶æ€ã€‚2) æ¢ç´¢äº†LSTMå’ŒTransformerä¸¤ç§æ—¶åºå»ºæ¨¡æ–¹æ³•ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬åœ¨å‚ä¸åº¦æ£€æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚3) é’ˆå¯¹æ•°æ®é›†ä¸­ç±»åˆ«ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œé‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹åœ¨å°‘æ•°ç±»åˆ«ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨EfficientNetV2ä½œä¸ºç©ºé—´ç‰¹å¾æå–å™¨ï¼Œå› ä¸ºå®ƒå…·æœ‰è¾ƒé«˜çš„æ•ˆç‡å’Œå‡†ç¡®ç‡ã€‚2) å°è¯•äº†LSTMå’ŒTransformerä¸¤ç§æ—¶åºå»ºæ¨¡æ–¹æ³•ï¼Œå¹¶å‘ç°LSTMåœ¨DAiSEEæ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½ã€‚3) é‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ—‹è½¬ã€ç¼©æ”¾ã€å¹³ç§»ç­‰ï¼Œä»¥å¢åŠ å°‘æ•°ç±»åˆ«çš„æ ·æœ¬æ•°é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ViBED-Netåœ¨DAiSEEæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚å…¶ä¸­ï¼Œç»“åˆLSTMçš„ViBED-Netå˜ä½“è¾¾åˆ°äº†73.43%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„state-of-the-artæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒæ—¶è€ƒè™‘äººè„¸å’Œåœºæ™¯ä¿¡æ¯èƒ½å¤Ÿæ˜¾è‘—æé«˜å‚ä¸åº¦æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæ•°æ®å¢å¼ºæŠ€æœ¯ä¹Ÿæœ‰æ•ˆåœ°æå‡äº†æ¨¡å‹åœ¨å°‘æ•°ç±»åˆ«ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ViBED-Netå¯åº”ç”¨äºåœ¨çº¿æ•™è‚²å¹³å°ï¼Œå®æ—¶ç›‘æµ‹å­¦ç”Ÿçš„å‚ä¸åº¦ï¼Œä¸ºæ•™å¸ˆæä¾›åé¦ˆï¼Œä»¥ä¾¿è°ƒæ•´æ•™å­¦ç­–ç•¥ï¼Œæé«˜æ•™å­¦æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºç”¨æˆ·ä½“éªŒç ”ç©¶ï¼Œåˆ†æç”¨æˆ·åœ¨ä½¿ç”¨äº§å“æˆ–æœåŠ¡æ—¶çš„å‚ä¸åº¦ï¼Œä»è€Œä¼˜åŒ–äº§å“è®¾è®¡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºå†…å®¹ä¸ªæ€§åŒ–æ¨èï¼Œæ ¹æ®ç”¨æˆ·çš„å‚ä¸åº¦æ¨èæ›´ç¬¦åˆå…¶å…´è¶£çš„å†…å®¹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Engagement detection in online learning environments is vital for improving student outcomes and personalizing instruction. We present ViBED-Net (Video-Based Engagement Detection Network), a novel deep learning framework designed to assess student engagement from video data using a dual-stream architecture. ViBED-Net captures both facial expressions and full-scene context by processing facial crops and entire video frames through EfficientNetV2 for spatial feature extraction. These features are then analyzed over time using two temporal modeling strategies: Long Short-Term Memory (LSTM) networks and Transformer encoders. Our model is evaluated on the DAiSEE dataset, a large-scale benchmark for affective state recognition in e-learning. To enhance performance on underrepresented engagement classes, we apply targeted data augmentation techniques. Among the tested variants, ViBED-Net with LSTM achieves 73.43\% accuracy, outperforming existing state-of-the-art approaches. ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporal cues significantly improves engagement detection accuracy. Its modular design allows flexibility for application across education, user experience research, and content personalization. This work advances video-based affective computing by offering a scalable, high-performing solution for real-world engagement analysis. The source code for this project is available on https://github.com/prateek-gothwal/ViBED-Net .

