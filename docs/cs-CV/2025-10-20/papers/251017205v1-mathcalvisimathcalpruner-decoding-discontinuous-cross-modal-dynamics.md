---
layout: default
title: $\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs
---

# $\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17205" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.17205v1</a>
  <a href="https://arxiv.org/pdf/2510.17205.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17205v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17205v1', '$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yingqi Fan, Anhao Zhao, Jinlan Fu, Junlong Tong, Hui Su, Yijie Pan, Wei Zhang, Xiaoyu Shen

**ÂàÜÁ±ª**: cs.CV, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-20

**Â§áÊ≥®**: EMNLP 2025 Main

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/EIT-NLP/VisiPruner)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VisiPrunerÔºöËß£Á†ÅÂ§öÊ®°ÊÄÅLLM‰∏≠ÁöÑÈùûËøûÁª≠Ë∑®Ê®°ÊÄÅÂä®ÊÄÅÔºåÂÆûÁé∞È´òÊïàÂâ™Êûù**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `tokenÂâ™Êûù` `Ë∑®Ê®°ÊÄÅ‰∫§‰∫í` `ËÆ°ÁÆóÊïàÁéá` `ËßÜËßâËØ≠Ë®Ä‰ªªÂä°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMÁöÑtokenÂâ™ÊûùÊñπÊ≥ïÁº∫‰πèÂØπË∑®Ê®°ÊÄÅ‰ø°ÊÅØÂ§ÑÁêÜÂíåËûçÂêàÊú∫Âà∂ÁöÑÊ∑±ÂÖ•ÁêÜËß£„ÄÇ
2. VisiPrunerÈÄöËøáÂàÜÊûêMLLMÁöÑË∑®Ê®°ÊÄÅ‰∫§‰∫íËøáÁ®ãÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑÂâ™ÊûùÊ°ÜÊû∂„ÄÇ
3. VisiPrunerÂú®LLaVA-v1.5 7B‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑËÆ°ÁÆóÊïàÁéáÊèêÂçáÔºåÂπ∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLMs)Âú®ËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁî±‰∫éÊ≥®ÊÑèÂäõËÆ°ÁÆóÈöèÂ§öÊ®°ÊÄÅtokenÊï∞ÈáèÂëà‰∫åÊ¨°Â¢ûÈïøÔºåÂØºËá¥ËÆ°ÁÆóÂºÄÈîÄÂ∑®Â§ß„ÄÇÂ∞ΩÁÆ°Â∑≤ÁªèÊúâ‰∏Ä‰∫õÂÖ≥‰∫éMLLM‰∏≠tokenÂâ™ÊûùÁöÑÂ∑•‰ΩúÔºå‰ΩÜÂÆÉ‰ª¨Áº∫‰πèÂØπMLLMÂ¶Ç‰ΩïÂ§ÑÁêÜÂíåËûçÂêàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÊ†πÊú¨ÁêÜËß£„ÄÇÈÄöËøáÁ≥ªÁªüÂàÜÊûêÔºåÊàë‰ª¨Êè≠Á§∫‰∫Ü‰∏Ä‰∏™‰∏âÈò∂ÊÆµÁöÑË∑®Ê®°ÊÄÅ‰∫§‰∫íËøáÁ®ãÔºö(1)ÊµÖÂ±ÇËØÜÂà´‰ªªÂä°ÊÑèÂõæÔºåËßÜËßâtokenÂÖÖÂΩìË¢´Âä®Ê≥®ÊÑèÂäõÊé•Êî∂Âô®Ôºõ(2)Ë∑®Ê®°ÊÄÅËûçÂêàÂú®‰∏≠Èó¥Â±ÇÁ™ÅÁÑ∂ÂèëÁîüÔºåÁî±Â∞ëÊï∞ÂÖ≥ÈîÆËßÜËßâtokenÈ©±Âä®Ôºõ(3)Ê∑±Â±Ç‰∏¢ÂºÉËßÜËßâtokenÔºå‰ªÖÂÖ≥Ê≥®ËØ≠Ë®ÄÁªÜÂåñ„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVisiPrunerÔºå‰∏Ä‰∏™Êó†ÈúÄËÆ≠ÁªÉÁöÑÂâ™ÊûùÊ°ÜÊû∂ÔºåÂú®LLaVA-v1.5 7B‰∏äÊúÄÂ§öÂèØÂáèÂ∞ë99%ÁöÑËßÜËßâÁõ∏ÂÖ≥Ê≥®ÊÑèÂäõËÆ°ÁÆóÂíå53.9%ÁöÑFLOPs„ÄÇÂÆÉÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑtokenÂâ™ÊûùÊñπÊ≥ïÔºåÂπ∂ÂèØÊé®ÂπøÂà∞‰∏çÂêåÁöÑMLLM„ÄÇÈô§‰∫ÜÂâ™ÊûùÔºåÊàë‰ª¨ÁöÑËßÅËß£Ëøò‰∏∫ÈÄöËøáÂ∞ÜÊ®°ÂûãÊû∂ÊûÑ‰∏éÂÖ∂ÂÜÖÂú®ÁöÑÈÄêÂ±ÇÂ§ÑÁêÜÂä®ÊÄÅÂØπÈΩêÊù•ËÆ≠ÁªÉÈ´òÊïàÁöÑMLLMÊèê‰æõ‰∫ÜÂèØÊìç‰ΩúÁöÑÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÁâπÂà´ÊòØÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑËÆ°ÁÆóÈáèÈöèËæìÂÖ•tokenÊï∞ÈáèÂëàÂπ≥ÊñπÂ¢ûÈïø„ÄÇÁé∞ÊúâÁöÑtokenÂâ™ÊûùÊñπÊ≥ïËôΩÁÑ∂Â∞ùËØïÂáèÂ∞ëËÆ°ÁÆóÈáèÔºå‰ΩÜÁº∫‰πèÂØπMLLMÂÜÖÈÉ®Ë∑®Ê®°ÊÄÅ‰ø°ÊÅØÂ§ÑÁêÜÊú∫Âà∂ÁöÑÊ∑±ÂÖ•ÁêÜËß£ÔºåÂØºËá¥Ââ™ÊûùÊïàÊûú‰∏ç‰Ω≥ÊàñÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËØ•ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊ∑±ÂÖ•ÂàÜÊûêMLLMÂÜÖÈÉ®ÁöÑË∑®Ê®°ÊÄÅ‰∫§‰∫íËøáÁ®ãÔºåÊè≠Á§∫‰∏çÂêåÂ±ÇÂØπËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÁöÑÂ§ÑÁêÜÊñπÂºèÔºå‰ªéËÄåÊúâÈíàÂØπÊÄßÂú∞ËøõË°åtokenÂâ™Êûù„ÄÇ‰ΩúËÄÖÂèëÁé∞MLLMÂ≠òÂú®‰∏Ä‰∏™‰∏âÈò∂ÊÆµÁöÑË∑®Ê®°ÊÄÅ‰∫§‰∫íËøáÁ®ãÔºåÂπ∂ÊçÆÊ≠§ËÆæËÆ°Ââ™ÊûùÁ≠ñÁï•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVisiPrunerÊòØ‰∏Ä‰∏™Êó†ÈúÄËÆ≠ÁªÉÁöÑÂâ™ÊûùÊ°ÜÊû∂ÔºåÂÆÉ‰æùËµñ‰∫éÂØπÈ¢ÑËÆ≠ÁªÉMLLMÁöÑÂàÜÊûê„ÄÇËØ•Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ÈÄöËøáÂÆûÈ™åÂàÜÊûêMLLMÂêÑÂ±ÇÂØπËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÁöÑÂ§ÑÁêÜÊñπÂºèÔºåÁ°ÆÂÆöË∑®Ê®°ÊÄÅ‰∫§‰∫íÁöÑÂÖ≥ÈîÆÂ±ÇÔºõ2) Âü∫‰∫éÂàÜÊûêÁªìÊûúÔºåËÆæËÆ°ÈíàÂØπ‰∏çÂêåÂ±ÇÁöÑÂâ™ÊûùÁ≠ñÁï•Ôºå‰æãÂ¶ÇÂú®ÊµÖÂ±Ç‰øùÁïôÊõ¥Â§öËßÜËßâtoken‰ª•ËæÖÂä©‰ªªÂä°ÊÑèÂõæËØÜÂà´ÔºåÂú®Ê∑±Â±ÇÂàôÊõ¥Â§öÂÖ≥Ê≥®ËØ≠Ë®Ä‰ø°ÊÅØÔºõ3) Â∫îÁî®Ââ™ÊûùÁ≠ñÁï•ÔºåÂáèÂ∞ëËßÜËßâÁõ∏ÂÖ≥ÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂØπMLLMË∑®Ê®°ÊÄÅ‰∫§‰∫íËøáÁ®ãÁöÑÊ∑±ÂÖ•ÂàÜÊûêÔºåÊè≠Á§∫‰∫ÜÂÖ∂‰∏âÈò∂ÊÆµÁâπÊÄßÔºåÂπ∂ÊçÆÊ≠§ÊèêÂá∫‰∫ÜÈíàÂØπÊÄßÁöÑÂâ™ÊûùÁ≠ñÁï•„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåVisiPruner‰∏çÊòØÁõ≤ÁõÆÂú∞ËøõË°åtokenÂâ™ÊûùÔºåËÄåÊòØÂü∫‰∫éÂØπÊ®°ÂûãÂÜÖÈÉ®Êú∫Âà∂ÁöÑÁêÜËß£Ôºå‰ªéËÄåÂú®‰øùËØÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÈáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVisiPrunerÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂÖ∂Êó†ÈúÄËÆ≠ÁªÉÁöÑÁâπÊÄßÔºåÈÅøÂÖç‰∫ÜÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÂºÄÈîÄ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÊ†πÊçÆ‰∏çÂêåÂ±ÇÁöÑÁâπÁÇπÈááÁî®‰∏çÂêåÁöÑÂâ™ÊûùÁ≠ñÁï•Ôºå‰æãÂ¶ÇÂú®ÊµÖÂ±ÇÔºåËßÜËßâtoken‰∏ªË¶Å‰Ωú‰∏∫‚ÄúÊ≥®ÊÑèÂäõÊé•Êî∂Âô®‚ÄùÔºåÂõ†Ê≠§ÂèØ‰ª•ÈÄÇÂΩì‰øùÁïôÔºõËÄåÂú®Ê∑±Â±ÇÔºåËßÜËßâtokenÁöÑÈáçË¶ÅÊÄßÈôç‰ΩéÔºåÂèØ‰ª•ËøõË°åÊõ¥ÊøÄËøõÁöÑÂâ™Êûù„ÄÇÂÖ∑‰ΩìÁöÑÂâ™ÊûùÊØî‰æãÂíåÁ≠ñÁï•ÈúÄË¶ÅÊ†πÊçÆÂÆûÈ™åÂàÜÊûêÁ°ÆÂÆö„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VisiPrunerÂú®LLaVA-v1.5 7BÊ®°Âûã‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÊúÄÂ§öÂèØÂáèÂ∞ë99%ÁöÑËßÜËßâÁõ∏ÂÖ≥Ê≥®ÊÑèÂäõËÆ°ÁÆóÂíå53.9%ÁöÑFLOPsÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVisiPruner‰ºò‰∫éÁé∞ÊúâÁöÑtokenÂâ™ÊûùÊñπÊ≥ïÔºåÂπ∂‰∏îÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•Â∫îÁî®‰∫é‰∏çÂêåÁöÑMLLM„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VisiPrunerÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈ´òÊïàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂ§ÑÁêÜÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á‰∏äÁöÑËßÜËßâÈóÆÁ≠î„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÊô∫ËÉΩÁõëÊéßÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéMLLMÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåËØ•Á†îÁ©∂ÊúâÂä©‰∫éÂ∞ÜËøô‰∫õÂº∫Â§ßÁöÑÊ®°ÂûãÈÉ®ÁΩ≤Âà∞ËµÑÊ∫êÂèóÈôêÁöÑÂπ≥Âè∞‰∏äÔºåÂπ∂Âä†ÈÄüÂ§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁöÑÊôÆÂèä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal Large Language Models (MLLMs) have achieved strong performance across vision-language tasks, but suffer from significant computational overhead due to the quadratic growth of attention computations with the number of multimodal tokens. Though efforts have been made to prune tokens in MLLMs, \textit{they lack a fundamental understanding of how MLLMs process and fuse multimodal information.} Through systematic analysis, we uncover a \textbf{three-stage} cross-modal interaction process: (1) Shallow layers recognize task intent, with visual tokens acting as passive attention sinks; (2) Cross-modal fusion occurs abruptly in middle layers, driven by a few critical visual tokens; (3) Deep layers discard vision tokens, focusing solely on linguistic refinement. Based on these findings, we propose \emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It significantly outperforms existing token pruning methods and generalizes across diverse MLLMs. Beyond pruning, our insights further provide actionable guidelines for training efficient MLLMs by aligning model architecture with its intrinsic layer-wise processing dynamics. Our code is available at: https://github.com/EIT-NLP/VisiPruner.

