---
layout: default
title: Elastic ViTs from Pretrained Models without Retraining
---

# Elastic ViTs from Pretrained Models without Retraining

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17700" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17700v1</a>
  <a href="https://arxiv.org/pdf/2510.17700.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17700v1" onclick="toggleFavorite(this, '2510.17700v1', 'Elastic ViTs from Pretrained Models without Retraining')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Walter Simoncini, Michael Dorkenwald, Tijmen Blankevoort, Cees G. M. Snoek, Yuki M. Asano

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: Accepted at NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSnapViTï¼Œæ— éœ€é‡è®­ç»ƒå³å¯ä»é¢„è®­ç»ƒViTæ¨¡å‹ä¸­è·å¾—å¼¹æ€§è®¡ç®—èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `Vision Transformer` `æ¨¡å‹å‰ªæ` `å¼¹æ€§æ¨ç†` `åè®­ç»ƒä¼˜åŒ–` `è‡ªç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰åŸºç¡€æ¨¡å‹å°ºå¯¸å›ºå®šï¼Œéš¾ä»¥é€‚åº”ä¸åŒè®¡ç®—èµ„æºçº¦æŸä¸‹çš„éƒ¨ç½²éœ€æ±‚ã€‚
2. SnapViTé€šè¿‡ç»“åˆæ¢¯åº¦ä¿¡æ¯å’Œè·¨ç½‘ç»œç»“æ„ç›¸å…³æ€§ï¼Œå®ç°é«˜æ•ˆçš„åé¢„è®­ç»ƒå‰ªæã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSnapViTåœ¨å¤šç§æ¨¡å‹å’Œç¨€ç–åº¦ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ— éœ€é‡è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰åŸºç¡€æ¨¡å‹æ€§èƒ½å“è¶Šï¼Œä½†é€šå¸¸åªæœ‰é¢„å®šä¹‰çš„å‡ ç§å°ºå¯¸ï¼Œåœ¨å®é™…éƒ¨ç½²ä¸­å­˜åœ¨è®¡ç®—èµ„æºçº¦æŸæ—¶ï¼Œé€‰æ‹©å—é™ã€‚æœ¬æ–‡æå‡ºSnapViTï¼Œä¸€ç§ç”¨äºå‰ªæVision Transformerçš„å•æ¬¡ç½‘ç»œè¿‘ä¼¼æ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ç§åé¢„è®­ç»ƒçš„ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è¿ç»­çš„è®¡ç®—é¢„ç®—èŒƒå›´å†…å®ç°å¼¹æ€§æ¨ç†ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°ç»“åˆäº†æ¢¯åº¦ä¿¡æ¯å’Œè·¨ç½‘ç»œç»“æ„ç›¸å…³æ€§ï¼ˆé€šè¿‡è¿›åŒ–ç®—æ³•è¿‘ä¼¼ï¼‰ï¼Œä¸éœ€è¦æ ‡æ³¨æ•°æ®ï¼Œå¯ä»¥æ¨å¹¿åˆ°æ²¡æœ‰åˆ†ç±»å¤´çš„æ¨¡å‹ï¼Œå¹¶ä¸”æ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨DINOã€SigLIPv2ã€DeITå’ŒAugRegæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§ç¨€ç–åº¦ä¸‹éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåªéœ€åœ¨å•ä¸ªA100 GPUä¸ŠèŠ±è´¹ä¸åˆ°äº”åˆ†é’Ÿå³å¯ç”Ÿæˆå¯è°ƒæ•´åˆ°ä»»ä½•è®¡ç®—é¢„ç®—çš„å¼¹æ€§æ¨¡å‹ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šä¸€ç§ç”¨äºé¢„è®­ç»ƒVision Transformerçš„é«˜æ•ˆå‰ªæç­–ç•¥ï¼Œä¸€ç§ç”¨äºHessiançŸ©é˜µéå¯¹è§’çº¿ç»“æ„çš„æ–°å‹è¿›åŒ–è¿‘ä¼¼ï¼Œä»¥åŠä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒæˆ–æ ‡ç­¾å³å¯ä¿æŒå¼ºå¤§æ€§èƒ½çš„è‡ªç›‘ç£é‡è¦æ€§è¯„åˆ†æœºåˆ¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Vision Transformeræ¨¡å‹é€šå¸¸ä»¥å›ºå®šçš„å°ºå¯¸è¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è®¡ç®—èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹ç‰¹å®šç¡¬ä»¶æˆ–å»¶è¿Ÿè¦æ±‚ï¼Œéœ€è¦é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œæˆæœ¬é«˜æ˜‚ã€‚ç°æœ‰çš„å‰ªææ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®æˆ–å¤æ‚çš„è®­ç»ƒæµç¨‹ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSnapViTçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ç»“æ„åŒ–å‰ªæï¼Œå¿«é€Ÿç”Ÿæˆä¸€ç³»åˆ—å…·æœ‰ä¸åŒè®¡ç®—å¤æ‚åº¦çš„å­ç½‘ç»œï¼Œä»è€Œå®ç°å¼¹æ€§æ¨ç†ã€‚å®ƒé¿å…äº†é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒçš„éœ€è¦ï¼Œå……åˆ†åˆ©ç”¨äº†é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œå¹¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ä¸åŒçš„è®¡ç®—é¢„ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSnapViTä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **é‡è¦æ€§è¯„åˆ†**ï¼šä½¿ç”¨è‡ªç›‘ç£çš„æ–¹å¼å¯¹ç½‘ç»œä¸­çš„ä¸åŒç»“æ„ï¼ˆä¾‹å¦‚ï¼ŒTransformer blockï¼‰è¿›è¡Œé‡è¦æ€§è¯„åˆ†ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€‚2) **ç»“æ„ç›¸å…³æ€§å»ºæ¨¡**ï¼šåˆ©ç”¨è¿›åŒ–ç®—æ³•è¿‘ä¼¼HessiançŸ©é˜µçš„éå¯¹è§’çº¿ç»“æ„ï¼Œä»è€Œæ•æ‰ä¸åŒç»“æ„ä¹‹é—´çš„ç›¸å…³æ€§ã€‚3) **å‰ªæå†³ç­–**ï¼šåŸºäºé‡è¦æ€§è¯„åˆ†å’Œç»“æ„ç›¸å…³æ€§ï¼Œé€‰æ‹©è¦å‰ªæçš„ç»“æ„ï¼Œç”Ÿæˆå…·æœ‰ä¸åŒè®¡ç®—å¤æ‚åº¦çš„å­ç½‘ç»œã€‚4) **æ¨¡å‹éƒ¨ç½²**ï¼šæ ¹æ®å®é™…çš„è®¡ç®—èµ„æºçº¦æŸï¼Œé€‰æ‹©åˆé€‚çš„å­ç½‘ç»œè¿›è¡Œéƒ¨ç½²ã€‚

**å…³é”®åˆ›æ–°**ï¼šSnapViTçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **é«˜æ•ˆçš„å‰ªæç­–ç•¥**ï¼šç»“åˆæ¢¯åº¦ä¿¡æ¯å’Œè·¨ç½‘ç»œç»“æ„ç›¸å…³æ€§ï¼Œå®ç°é«˜æ•ˆçš„ç»“æ„åŒ–å‰ªæã€‚2) **HessiançŸ©é˜µéå¯¹è§’çº¿ç»“æ„çš„è¿›åŒ–è¿‘ä¼¼**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è¿›åŒ–ç®—æ³•ï¼Œç”¨äºè¿‘ä¼¼HessiançŸ©é˜µçš„éå¯¹è§’çº¿ç»“æ„ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ä¸åŒç»“æ„ä¹‹é—´çš„ç›¸å…³æ€§ã€‚3) **è‡ªç›‘ç£é‡è¦æ€§è¯„åˆ†æœºåˆ¶**ï¼šæå‡ºäº†ä¸€ç§è‡ªç›‘ç£çš„é‡è¦æ€§è¯„åˆ†æœºåˆ¶ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®å³å¯è¯„ä¼°ç½‘ç»œä¸­ä¸åŒç»“æ„çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šSnapViTçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ¢¯åº¦ä¿¡æ¯çš„è‡ªç›‘ç£é‡è¦æ€§è¯„åˆ†ï¼Œä¾‹å¦‚ï¼ŒåŸºäºDINOçš„è‡ªç›‘ç£ä¿¡å·ã€‚2) ä½¿ç”¨è¿›åŒ–ç®—æ³•æ¥è¿‘ä¼¼HessiançŸ©é˜µçš„éå¯¹è§’çº¿ç»“æ„ï¼Œä»¥æ•æ‰ä¸åŒå±‚ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚3) ç»“æ„åŒ–å‰ªæç­–ç•¥ï¼Œä¾‹å¦‚ï¼Œå‰ªææ•´ä¸ªTransformer blockï¼Œä»¥å‡å°‘è®¡ç®—é‡ã€‚4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œæ—¨åœ¨ä¿æŒå‰ªæåæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨DINOã€SigLIPv2ã€DeITå’ŒAugRegç­‰é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜SnapViTåœ¨å„ç§ç¨€ç–åº¦ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨DINOé¢„è®­ç»ƒæ¨¡å‹ï¼ŒSnapViTåœ¨ä¿æŒç›¸åŒæ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å°†è®¡ç®—é‡å‡å°‘50%ä»¥ä¸Šã€‚ç”Ÿæˆå¼¹æ€§æ¨¡å‹æ‰€éœ€çš„æ—¶é—´ä¹Ÿå¾ˆçŸ­ï¼Œåœ¨å•ä¸ªA100 GPUä¸Šåªéœ€ä¸åˆ°äº”åˆ†é’Ÿã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SnapViTé€‚ç”¨äºå„ç§è®¡ç®—èµ„æºå—é™çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚å®ƒå¯ä»¥å¸®åŠ©å¼€å‘è€…å¿«é€Ÿç”Ÿæˆé€‚åº”ä¸åŒç¡¬ä»¶å¹³å°çš„æ¨¡å‹ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ï¼Œå¹¶æé«˜æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºæ¨¡å‹å‹ç¼©ã€çŸ¥è¯†è’¸é¦ç­‰é¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision foundation models achieve remarkable performance but are only available in a limited set of pre-determined sizes, forcing sub-optimal deployment choices under real-world constraints. We introduce SnapViT: Single-shot network approximation for pruned Vision Transformers, a new post-pretraining structured pruning method that enables elastic inference across a continuum of compute budgets. Our approach efficiently combines gradient information with cross-network structure correlations, approximated via an evolutionary algorithm, does not require labeled data, generalizes to models without a classification head, and is retraining-free. Experiments on DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over state-of-the-art methods across various sparsities, requiring less than five minutes on a single A100 GPU to generate elastic models that can be adjusted to any computational budget. Our key contributions include an efficient pruning strategy for pretrained Vision Transformers, a novel evolutionary approximation of Hessian off-diagonal structures, and a self-supervised importance scoring mechanism that maintains strong performance without requiring retraining or labels. Code and pruned models are available at: https://elastic.ashita.nl/

