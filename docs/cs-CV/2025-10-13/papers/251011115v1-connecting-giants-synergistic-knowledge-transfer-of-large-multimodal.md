---
layout: default
title: Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning
---

# Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11115" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11115v1</a>
  <a href="https://arxiv.org/pdf/2510.11115.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11115v1" onclick="toggleFavorite(this, '2510.11115v1', 'Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hao Tang, Shengfeng He, Jing Qin

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**å¤‡æ³¨**: Accepted by IJCAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSynTransæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ååŒçŸ¥è¯†è¿ç§»æå‡å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å°‘æ ·æœ¬å­¦ä¹ ` `çŸ¥è¯†è¿ç§»` `å¤šæ¨¡æ€å­¦ä¹ ` `ååŒå­¦ä¹ ` `è§†è§‰è¯­ä¹‰æ¡¥æ¥` `çŸ¥è¯†è’¸é¦` `CLIP`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å°‘æ ·æœ¬å­¦ä¹ é¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åˆ©ç”¨å°è§„æ¨¡æ¨¡å‹çš„è¯­ä¹‰çŸ¥è¯†ï¼Œä½†æ˜“å¼•å…¥å™ªå£°å’Œåå·®ã€‚
2. SynTransæ¡†æ¶é€šè¿‡çŸ¥è¯†è’¸é¦ã€ååŒçŸ¥è¯†æŒ–æ˜å’Œè§†è§‰-è¯­ä¹‰æ¡¥æ¥ï¼Œå®ç°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çŸ¥è¯†çš„æœ‰æ•ˆè¿ç§»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSynTranså³ä½¿æ­é…ç®€å•çš„è§†è§‰ç¼–ç å™¨ï¼Œä¹Ÿèƒ½æ˜¾è‘—è¶…è¶Šç°æœ‰å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ååŒçŸ¥è¯†è¿ç§»ï¼ˆSynTransï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆè¿ç§»å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­å¤šæ ·ä¸”äº’è¡¥çš„çŸ¥è¯†ï¼Œä»è€Œå¢å¼ºç°æœ‰å°‘æ ·æœ¬å­¦ä¹ å™¨çš„èƒ½åŠ›ã€‚SynTransé‡‡ç”¨CLIPä½œä¸ºå¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å°‘æ ·æœ¬è§†è§‰ç¼–ç å™¨ä½œä¸ºå¼±å­¦ç”Ÿæ¨¡å‹ï¼Œé€šè¿‡æ— ç›‘ç£ä»£ç†ä»»åŠ¡æç‚¼è¯­ä¹‰å¯¹é½çš„è§†è§‰çŸ¥è¯†ã€‚éšåï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ååŒçŸ¥è¯†æŒ–æ˜æ¨¡å—ä¿ƒè¿›å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¹‹é—´çš„åä½œï¼Œä»¥æå–é«˜è´¨é‡çš„è¯­ä¹‰çŸ¥è¯†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè§†è§‰-è¯­ä¹‰æ¡¥æ¥æ¨¡å—å®ç°äº†è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¹‹é—´çš„åŒå‘çŸ¥è¯†è¿ç§»ï¼Œå°†æ˜¾å¼çš„è§†è§‰çŸ¥è¯†å’Œéšå¼çš„è¯­ä¹‰çŸ¥è¯†è½¬åŒ–ä¸ºç‰¹å®šç±»åˆ«çš„åˆ†ç±»å™¨æƒé‡ã€‚æœ€åï¼ŒSynTranså¼•å…¥äº†è§†è§‰æƒé‡ç”Ÿæˆå™¨å’Œè¯­ä¹‰æƒé‡é‡æ„å™¨ï¼Œä»¥è‡ªé€‚åº”åœ°æ„å»ºæœ€ä¼˜çš„å¤šæ¨¡æ€å°‘æ ·æœ¬å­¦ä¹ åˆ†ç±»å™¨ã€‚åœ¨å››ä¸ªå°‘æ ·æœ¬å­¦ä¹ æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿ä¸ç®€å•çš„å°‘æ ·æœ¬è§†è§‰ç¼–ç å™¨é…å¯¹ï¼ŒSynTransä¹Ÿæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå°‘æ ·æœ¬å­¦ä¹ æ—¨åœ¨ä»…åˆ©ç”¨å°‘é‡æ ·æœ¬å¯¹æ–°ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚ç°æœ‰æ–¹æ³•å°è¯•åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œä½†é€šå¸¸ä¾èµ–äºè§„æ¨¡è¾ƒå°çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æä¾›çš„è¯­ä¹‰çŸ¥è¯†å¯èƒ½ä¸å¤Ÿä¸°å¯Œï¼Œå¹¶ä¸”å®¹æ˜“å¼•å…¥å™ªå£°å’Œåå·®ï¼Œé™åˆ¶äº†å°‘æ ·æœ¬å­¦ä¹ çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSynTransçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸­è•´å«çš„ä¸°å¯Œè§†è§‰å’Œè¯­ä¹‰çŸ¥è¯†ï¼Œé€šè¿‡çŸ¥è¯†è¿ç§»çš„æ–¹å¼æå‡å°‘æ ·æœ¬å­¦ä¹ å™¨çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ååŒå¤šä¸ªå¤§å‹æ¨¡å‹ï¼Œæå–äº’è¡¥çš„çŸ¥è¯†ï¼Œå¹¶è®¾è®¡æ¨¡å—å®ç°è§†è§‰å’Œè¯­ä¹‰çŸ¥è¯†çš„æœ‰æ•ˆèåˆï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­çŸ¥è¯†æ¥æºå•ä¸€å’Œæ˜“å¼•å…¥å™ªå£°çš„é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSynTransæ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **çŸ¥è¯†è’¸é¦æ¨¡å—**ï¼šä½¿ç”¨CLIPä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œå°†è¯­ä¹‰å¯¹é½çš„è§†è§‰çŸ¥è¯†è’¸é¦åˆ°å°‘æ ·æœ¬è§†è§‰ç¼–ç å™¨ä¸­ã€‚2) **ååŒçŸ¥è¯†æŒ–æ˜æ¨¡å—**ï¼šä¿ƒè¿›å¤šä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¹‹é—´çš„åä½œï¼Œæå–é«˜è´¨é‡çš„è¯­ä¹‰çŸ¥è¯†ã€‚3) **è§†è§‰-è¯­ä¹‰æ¡¥æ¥æ¨¡å—**ï¼šå®ç°è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¹‹é—´çš„åŒå‘çŸ¥è¯†è¿ç§»ï¼Œå°†è§†è§‰å’Œè¯­ä¹‰çŸ¥è¯†è½¬åŒ–ä¸ºåˆ†ç±»å™¨æƒé‡ã€‚4) **æƒé‡ç”Ÿæˆä¸é‡æ„æ¨¡å—**ï¼šè‡ªé€‚åº”åœ°ç”Ÿæˆè§†è§‰æƒé‡å’Œé‡æ„è¯­ä¹‰æƒé‡ï¼Œæ„å»ºæœ€ä¼˜çš„å¤šæ¨¡æ€å°‘æ ·æœ¬å­¦ä¹ åˆ†ç±»å™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šSynTransçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **ååŒçŸ¥è¯†è¿ç§»**ï¼šé€šè¿‡ååŒå¤šä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæå–äº’è¡¥çš„çŸ¥è¯†ï¼Œé¿å…äº†å•ä¸€çŸ¥è¯†æ¥æºçš„å±€é™æ€§ã€‚2) **è§†è§‰-è¯­ä¹‰æ¡¥æ¥**ï¼šå®ç°äº†è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¹‹é—´çš„åŒå‘çŸ¥è¯†è¿ç§»ï¼Œå……åˆ†åˆ©ç”¨äº†è§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯ã€‚3) **è‡ªé€‚åº”æƒé‡ç”Ÿæˆä¸é‡æ„**ï¼šèƒ½å¤Ÿæ ¹æ®ä¸åŒç±»åˆ«çš„ç‰¹ç‚¹ï¼Œè‡ªé€‚åº”åœ°ç”Ÿæˆè§†è§‰æƒé‡å’Œé‡æ„è¯­ä¹‰æƒé‡ï¼Œæå‡äº†åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨çŸ¥è¯†è’¸é¦æ¨¡å—ä¸­ï¼Œä½¿ç”¨æ— ç›‘ç£ä»£ç†ä»»åŠ¡æ¥å¯¹é½è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ã€‚åœ¨ååŒçŸ¥è¯†æŒ–æ˜æ¨¡å—ä¸­ï¼Œè®¾è®¡ç‰¹å®šçš„ç­–ç•¥æ¥ä¿ƒè¿›ä¸åŒæ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†å…±äº«ã€‚åœ¨è§†è§‰-è¯­ä¹‰æ¡¥æ¥æ¨¡å—ä¸­ï¼Œä½¿ç”¨çº¿æ€§å˜æ¢å°†è§†è§‰å’Œè¯­ä¹‰ç‰¹å¾æ˜ å°„åˆ°åŒä¸€ç©ºé—´ã€‚åœ¨æƒé‡ç”Ÿæˆä¸é‡æ„æ¨¡å—ä¸­ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥ç”Ÿæˆè§†è§‰æƒé‡å’Œé‡æ„è¯­ä¹‰æƒé‡ï¼Œå¹¶ä½¿ç”¨æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è¿™äº›æƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SynTransåœ¨å››ä¸ªå°‘æ ·æœ¬å­¦ä¹ æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å³ä½¿ä¸ç®€å•çš„å°‘æ ·æœ¬è§†è§‰ç¼–ç å™¨æ­é…ä½¿ç”¨ï¼ŒSynTransä¹Ÿèƒ½å–å¾—ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶çŸ¥è¯†è¿ç§»ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“å®éªŒæ•°æ®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†å±•ç¤ºï¼Œè¡¨æ˜SynTransåœ¨å°‘æ ·æœ¬å­¦ä¹ é¢†åŸŸå…·æœ‰å¼ºå¤§çš„ç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SynTransæ¡†æ¶å¯åº”ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå°¤å…¶é€‚ç”¨äºæ•°æ®æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„åœºæ™¯ï¼Œä¾‹å¦‚åŒ»å­¦å›¾åƒåˆ†æã€é¥æ„Ÿå›¾åƒè§£è¯‘ç­‰ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„çŸ¥è¯†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Few-shot learning (FSL) addresses the challenge of classifying novel classes with limited training samples. While some methods leverage semantic knowledge from smaller-scale models to mitigate data scarcity, these approaches often introduce noise and bias due to the data's inherent simplicity. In this paper, we propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which effectively transfers diverse and complementary knowledge from large multimodal models to empower the off-the-shelf few-shot learner. Specifically, SynTrans employs CLIP as a robust teacher and uses a few-shot vision encoder as a weak student, distilling semantic-aligned visual knowledge via an unsupervised proxy task. Subsequently, a training-free synergistic knowledge mining module facilitates collaboration among large multimodal models to extract high-quality semantic knowledge. Building upon this, a visual-semantic bridging module enables bi-directional knowledge transfer between visual and semantic spaces, transforming explicit visual and implicit semantic knowledge into category-specific classifier weights. Finally, SynTrans introduces a visual weight generator and a semantic weight reconstructor to adaptively construct optimal multimodal FSL classifiers. Experimental results on four FSL datasets demonstrate that SynTrans, even when paired with a simple few-shot vision encoder, significantly outperforms current state-of-the-art methods.

