---
layout: default
title: Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning
---

# Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11115" target="_blank" class="toolbar-btn">arXiv: 2510.11115v1</a>
    <a href="https://arxiv.org/pdf/2510.11115.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11115v1" 
            onclick="toggleFavorite(this, '2510.11115v1', 'Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hao Tang, Shengfeng He, Jing Qin

**ÂàÜÁ±ª**: cs.CV, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

**Â§áÊ≥®**: Accepted by IJCAI 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SynTransÊ°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂçèÂêåÁü•ËØÜËøÅÁßªÊèêÂçáÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â∞ëÊ†∑Êú¨Â≠¶‰π†` `Áü•ËØÜËøÅÁßª` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂçèÂêåÂ≠¶‰π†` `ËßÜËßâËØ≠‰πâÊ°•Êé•` `Áü•ËØÜËí∏È¶è` `CLIP`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â∞ëÊ†∑Êú¨Â≠¶‰π†Èù¢‰∏¥Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÊåëÊàòÔºåÁé∞ÊúâÊñπÊ≥ïÂà©Áî®Â∞èËßÑÊ®°Ê®°ÂûãÁöÑËØ≠‰πâÁü•ËØÜÔºå‰ΩÜÊòìÂºïÂÖ•Âô™Â£∞ÂíåÂÅèÂ∑Æ„ÄÇ
2. SynTransÊ°ÜÊû∂ÈÄöËøáÁü•ËØÜËí∏È¶è„ÄÅÂçèÂêåÁü•ËØÜÊåñÊéòÂíåËßÜËßâ-ËØ≠‰πâÊ°•Êé•ÔºåÂÆûÁé∞Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁü•ËØÜÁöÑÊúâÊïàËøÅÁßª„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSynTransÂç≥‰ΩøÊê≠ÈÖçÁÆÄÂçïÁöÑËßÜËßâÁºñÁ†ÅÂô®Ôºå‰πüËÉΩÊòæËëóË∂ÖË∂äÁé∞ÊúâÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂçèÂêåÁü•ËØÜËøÅÁßªÔºàSynTransÔºâÊ°ÜÊû∂ÔºåÊó®Âú®ÊúâÊïàËøÅÁßªÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã‰∏≠Â§öÊ†∑‰∏î‰∫íË°•ÁöÑÁü•ËØÜÔºå‰ªéËÄåÂ¢ûÂº∫Áé∞ÊúâÂ∞ëÊ†∑Êú¨Â≠¶‰π†Âô®ÁöÑËÉΩÂäõ„ÄÇSynTransÈááÁî®CLIP‰Ωú‰∏∫Âº∫Â§ßÁöÑÊïôÂ∏àÊ®°ÂûãÔºåÂπ∂‰ΩøÁî®Â∞ëÊ†∑Êú¨ËßÜËßâÁºñÁ†ÅÂô®‰Ωú‰∏∫Âº±Â≠¶ÁîüÊ®°ÂûãÔºåÈÄöËøáÊó†ÁõëÁù£‰ª£ÁêÜ‰ªªÂä°ÊèêÁÇºËØ≠‰πâÂØπÈΩêÁöÑËßÜËßâÁü•ËØÜ„ÄÇÈöèÂêéÔºå‰∏Ä‰∏™Êó†ÈúÄËÆ≠ÁªÉÁöÑÂçèÂêåÁü•ËØÜÊåñÊéòÊ®°Âùó‰øÉËøõÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã‰πãÈó¥ÁöÑÂçè‰ΩúÔºå‰ª•ÊèêÂèñÈ´òË¥®ÈáèÁöÑËØ≠‰πâÁü•ËØÜ„ÄÇÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåËßÜËßâ-ËØ≠‰πâÊ°•Êé•Ê®°ÂùóÂÆûÁé∞‰∫ÜËßÜËßâÂíåËØ≠‰πâÁ©∫Èó¥‰πãÈó¥ÁöÑÂèåÂêëÁü•ËØÜËøÅÁßªÔºåÂ∞ÜÊòæÂºèÁöÑËßÜËßâÁü•ËØÜÂíåÈöêÂºèÁöÑËØ≠‰πâÁü•ËØÜËΩ¨Âåñ‰∏∫ÁâπÂÆöÁ±ªÂà´ÁöÑÂàÜÁ±ªÂô®ÊùÉÈáç„ÄÇÊúÄÂêéÔºåSynTransÂºïÂÖ•‰∫ÜËßÜËßâÊùÉÈáçÁîüÊàêÂô®ÂíåËØ≠‰πâÊùÉÈáçÈáçÊûÑÂô®Ôºå‰ª•Ëá™ÈÄÇÂ∫îÂú∞ÊûÑÂª∫ÊúÄ‰ºòÁöÑÂ§öÊ®°ÊÄÅÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÂàÜÁ±ªÂô®„ÄÇÂú®Âõõ‰∏™Â∞ëÊ†∑Êú¨Â≠¶‰π†Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂç≥‰Ωø‰∏éÁÆÄÂçïÁöÑÂ∞ëÊ†∑Êú¨ËßÜËßâÁºñÁ†ÅÂô®ÈÖçÂØπÔºåSynTrans‰πüÊòæËëó‰ºò‰∫éÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ∞ëÊ†∑Êú¨Â≠¶‰π†Êó®Âú®‰ªÖÂà©Áî®Â∞ëÈáèÊ†∑Êú¨ÂØπÊñ∞Á±ªÂà´ËøõË°åÂàÜÁ±ª„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ∞ùËØïÂà©Áî®Â§ñÈÉ®Áü•ËØÜÔºå‰ΩÜÈÄöÂ∏∏‰æùËµñ‰∫éËßÑÊ®°ËæÉÂ∞èÁöÑÊ®°ÂûãÔºåËøô‰∫õÊ®°ÂûãÊèê‰æõÁöÑËØ≠‰πâÁü•ËØÜÂèØËÉΩ‰∏çÂ§ü‰∏∞ÂØåÔºåÂπ∂‰∏îÂÆπÊòìÂºïÂÖ•Âô™Â£∞ÂíåÂÅèÂ∑ÆÔºåÈôêÂà∂‰∫ÜÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÁöÑÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSynTransÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàÂ¶ÇCLIPÔºâ‰∏≠Ëï¥Âê´ÁöÑ‰∏∞ÂØåËßÜËßâÂíåËØ≠‰πâÁü•ËØÜÔºåÈÄöËøáÁü•ËØÜËøÅÁßªÁöÑÊñπÂºèÊèêÂçáÂ∞ëÊ†∑Êú¨Â≠¶‰π†Âô®ÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂçèÂêåÂ§ö‰∏™Â§ßÂûãÊ®°ÂûãÔºåÊèêÂèñ‰∫íË°•ÁöÑÁü•ËØÜÔºåÂπ∂ËÆæËÆ°Ê®°ÂùóÂÆûÁé∞ËßÜËßâÂíåËØ≠‰πâÁü•ËØÜÁöÑÊúâÊïàËûçÂêàÔºå‰ªéËÄåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ï‰∏≠Áü•ËØÜÊù•Ê∫êÂçï‰∏ÄÂíåÊòìÂºïÂÖ•Âô™Â£∞ÁöÑÈóÆÈ¢ò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSynTransÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) **Áü•ËØÜËí∏È¶èÊ®°Âùó**Ôºö‰ΩøÁî®CLIP‰Ωú‰∏∫ÊïôÂ∏àÊ®°ÂûãÔºåÂ∞ÜËØ≠‰πâÂØπÈΩêÁöÑËßÜËßâÁü•ËØÜËí∏È¶èÂà∞Â∞ëÊ†∑Êú¨ËßÜËßâÁºñÁ†ÅÂô®‰∏≠„ÄÇ2) **ÂçèÂêåÁü•ËØÜÊåñÊéòÊ®°Âùó**Ôºö‰øÉËøõÂ§ö‰∏™Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã‰πãÈó¥ÁöÑÂçè‰ΩúÔºåÊèêÂèñÈ´òË¥®ÈáèÁöÑËØ≠‰πâÁü•ËØÜ„ÄÇ3) **ËßÜËßâ-ËØ≠‰πâÊ°•Êé•Ê®°Âùó**ÔºöÂÆûÁé∞ËßÜËßâÂíåËØ≠‰πâÁ©∫Èó¥‰πãÈó¥ÁöÑÂèåÂêëÁü•ËØÜËøÅÁßªÔºåÂ∞ÜËßÜËßâÂíåËØ≠‰πâÁü•ËØÜËΩ¨Âåñ‰∏∫ÂàÜÁ±ªÂô®ÊùÉÈáç„ÄÇ4) **ÊùÉÈáçÁîüÊàê‰∏éÈáçÊûÑÊ®°Âùó**ÔºöËá™ÈÄÇÂ∫îÂú∞ÁîüÊàêËßÜËßâÊùÉÈáçÂíåÈáçÊûÑËØ≠‰πâÊùÉÈáçÔºåÊûÑÂª∫ÊúÄ‰ºòÁöÑÂ§öÊ®°ÊÄÅÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÂàÜÁ±ªÂô®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSynTransÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) **ÂçèÂêåÁü•ËØÜËøÅÁßª**ÔºöÈÄöËøáÂçèÂêåÂ§ö‰∏™Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊèêÂèñ‰∫íË°•ÁöÑÁü•ËØÜÔºåÈÅøÂÖç‰∫ÜÂçï‰∏ÄÁü•ËØÜÊù•Ê∫êÁöÑÂ±ÄÈôêÊÄß„ÄÇ2) **ËßÜËßâ-ËØ≠‰πâÊ°•Êé•**ÔºöÂÆûÁé∞‰∫ÜËßÜËßâÂíåËØ≠‰πâÁ©∫Èó¥‰πãÈó¥ÁöÑÂèåÂêëÁü•ËØÜËøÅÁßªÔºåÂÖÖÂàÜÂà©Áî®‰∫ÜËßÜËßâÂíåËØ≠‰πâ‰ø°ÊÅØ„ÄÇ3) **Ëá™ÈÄÇÂ∫îÊùÉÈáçÁîüÊàê‰∏éÈáçÊûÑ**ÔºöËÉΩÂ§üÊ†πÊçÆ‰∏çÂêåÁ±ªÂà´ÁöÑÁâπÁÇπÔºåËá™ÈÄÇÂ∫îÂú∞ÁîüÊàêËßÜËßâÊùÉÈáçÂíåÈáçÊûÑËØ≠‰πâÊùÉÈáçÔºåÊèêÂçá‰∫ÜÂàÜÁ±ªÂô®ÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Áü•ËØÜËí∏È¶èÊ®°Âùó‰∏≠Ôºå‰ΩøÁî®Êó†ÁõëÁù£‰ª£ÁêÜ‰ªªÂä°Êù•ÂØπÈΩêËßÜËßâÂíåËØ≠‰πâÁ©∫Èó¥„ÄÇÂú®ÂçèÂêåÁü•ËØÜÊåñÊéòÊ®°Âùó‰∏≠ÔºåËÆæËÆ°ÁâπÂÆöÁöÑÁ≠ñÁï•Êù•‰øÉËøõ‰∏çÂêåÊ®°Âûã‰πãÈó¥ÁöÑÁü•ËØÜÂÖ±‰∫´„ÄÇÂú®ËßÜËßâ-ËØ≠‰πâÊ°•Êé•Ê®°Âùó‰∏≠Ôºå‰ΩøÁî®Á∫øÊÄßÂèòÊç¢Â∞ÜËßÜËßâÂíåËØ≠‰πâÁâπÂæÅÊò†Â∞ÑÂà∞Âêå‰∏ÄÁ©∫Èó¥„ÄÇÂú®ÊùÉÈáçÁîüÊàê‰∏éÈáçÊûÑÊ®°Âùó‰∏≠Ôºå‰ΩøÁî®Á•ûÁªèÁΩëÁªúÊù•ÁîüÊàêËßÜËßâÊùÉÈáçÂíåÈáçÊûÑËØ≠‰πâÊùÉÈáçÔºåÂπ∂‰ΩøÁî®ÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñËøô‰∫õÊùÉÈáç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

SynTransÂú®Âõõ‰∏™Â∞ëÊ†∑Êú¨Â≠¶‰π†Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåË∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇÂç≥‰Ωø‰∏éÁÆÄÂçïÁöÑÂ∞ëÊ†∑Êú¨ËßÜËßâÁºñÁ†ÅÂô®Êê≠ÈÖç‰ΩøÁî®ÔºåSynTrans‰πüËÉΩÂèñÂæó‰ºòÂºÇÁöÑÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂Áü•ËØÜËøÅÁßªÁ≠ñÁï•ÁöÑÊúâÊïàÊÄß„ÄÇÂÖ∑‰ΩìÂÆûÈ™åÊï∞ÊçÆÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÂ±ïÁ§∫ÔºåË°®ÊòéSynTransÂú®Â∞ëÊ†∑Êú¨Â≠¶‰π†È¢ÜÂüüÂÖ∑ÊúâÂº∫Â§ßÁöÑÁ´û‰∫âÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

SynTransÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂõæÂÉèÂàÜÁ±ª„ÄÅÁõÆÊ†áÊ£ÄÊµãÁ≠âÂ§öÁßçËÆ°ÁÆóÊú∫ËßÜËßâ‰ªªÂä°ÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÊï∞ÊçÆÊ†áÊ≥®ÊàêÊú¨È´òÊòÇÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂåªÂ≠¶ÂõæÂÉèÂàÜÊûê„ÄÅÈÅ•ÊÑüÂõæÂÉèËß£ËØëÁ≠â„ÄÇÈÄöËøáÂà©Áî®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÁü•ËØÜÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊòæËëóÈôç‰ΩéÂØπÊ†áÊ≥®Êï∞ÊçÆÁöÑÈúÄÊ±ÇÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆûÁî®ÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Few-shot learning (FSL) addresses the challenge of classifying novel classes with limited training samples. While some methods leverage semantic knowledge from smaller-scale models to mitigate data scarcity, these approaches often introduce noise and bias due to the data's inherent simplicity. In this paper, we propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which effectively transfers diverse and complementary knowledge from large multimodal models to empower the off-the-shelf few-shot learner. Specifically, SynTrans employs CLIP as a robust teacher and uses a few-shot vision encoder as a weak student, distilling semantic-aligned visual knowledge via an unsupervised proxy task. Subsequently, a training-free synergistic knowledge mining module facilitates collaboration among large multimodal models to extract high-quality semantic knowledge. Building upon this, a visual-semantic bridging module enables bi-directional knowledge transfer between visual and semantic spaces, transforming explicit visual and implicit semantic knowledge into category-specific classifier weights. Finally, SynTrans introduces a visual weight generator and a semantic weight reconstructor to adaptively construct optimal multimodal FSL classifiers. Experimental results on four FSL datasets demonstrate that SynTrans, even when paired with a simple few-shot vision encoder, significantly outperforms current state-of-the-art methods.

