---
layout: default
title: Scaling Language-Centric Omnimodal Representation Learning
---

# Scaling Language-Centric Omnimodal Representation Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11693" target="_blank" class="toolbar-btn">arXiv: 2510.11693v1</a>
    <a href="https://arxiv.org/pdf/2510.11693.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11693v1" 
            onclick="toggleFavorite(this, '2510.11693v1', 'Scaling Language-Centric Omnimodal Representation Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

**Â§áÊ≥®**: NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/LCO-Embedding/LCO-Embedding)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LCO-EmbÊ°ÜÊû∂ÔºåÈÄöËøáËØ≠Ë®Ä‰∏≠ÂøÉÁöÑÂ§öÊ®°ÊÄÅË°®ÂæÅÂ≠¶‰π†ÔºåÊèêÂçáË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢ÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ë∑®Ê®°ÊÄÅÊ£ÄÁ¥¢` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÂØπÊØîÂ≠¶‰π†` `Ë°®ÂæÅÂ≠¶‰π†` `ÁîüÊàêÂºèÈ¢ÑËÆ≠ÁªÉ` `ËßÜËßâ-ËØ≠Ë®Ä`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éMLLMÁöÑÂ§öÊ®°ÊÄÅÂµåÂÖ•ÊñπÊ≥ïÁº∫‰πèÂØπÂÖ∂‰ºòË∂äÊÄßÁöÑÊ∑±ÂÖ•ÁêÜËß£ÔºåÈúÄË¶ÅÊé¢Á©∂ÂÖ∂ÂÜÖÂú®Êú∫Âà∂„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫LCO-EmbÊ°ÜÊû∂ÔºåÂà©Áî®MLLMÁîüÊàêÂºèÈ¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÈöêÂºèË∑®Ê®°ÊÄÅÂØπÈΩêÔºåÂπ∂ÈÄöËøáÂØπÊØîÂ≠¶‰π†ËøõË°å‰ºòÂåñ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåLCO-EmbÂú®Â§öÁßçÊ®°ÊÄÅ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂπ∂È™åËØÅ‰∫ÜÁîüÊàêËÉΩÂäõ‰∏éË°®Á§∫ËÉΩÂäõ‰πãÈó¥ÁöÑÁº©ÊîæÂÆöÂæã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊó®Âú®Êé¢Á¥¢Âü∫‰∫éÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÂπ∂ÈááÁî®ÂØπÊØîÂ≠¶‰π†ÔºàCLÔºâËøõË°åÂæÆË∞ÉÁöÑÂ§öÊ®°ÊÄÅÂµåÂÖ•ÊñπÊ≥ïÁöÑ‰ºòÂäø„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåMLLMÊñπÊ≥ïÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆ‰ºòÂäøÂú®‰∫éÁîüÊàêÂºèÈ¢ÑËÆ≠ÁªÉÊúüÈó¥ÂÆûÁé∞ÁöÑÈöêÂºèË∑®Ê®°ÊÄÅÂØπÈΩêÔºåÂÖ∂‰∏≠ËØ≠Ë®ÄËß£Á†ÅÂô®Â≠¶‰π†Âà©Áî®ÂÖ±‰∫´Ë°®Á§∫Á©∫Èó¥ÂÜÖÁöÑÂ§öÊ®°ÊÄÅ‰ø°Âè∑Êù•ÁîüÊàêÂçïÊ®°ÊÄÅËæìÂá∫„ÄÇÈÄöËøáÂØπÂêÑÂêëÂºÇÊÄßÂíåÊ†∏Áõ∏‰ººÊÄßÁªìÊûÑÁöÑÂàÜÊûêÔºåËØÅÂÆû‰∫ÜMLLMË°®Á§∫‰∏≠ÊΩúÂú®ÂØπÈΩêÁöÑÂá∫Áé∞Ôºå‰ΩøÂæóCLËÉΩÂ§ü‰Ωú‰∏∫ËΩªÈáèÁ∫ßÁöÑ‰ºòÂåñÈò∂ÊÆµ„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰ª•ËØ≠Ë®Ä‰∏∫‰∏≠ÂøÉÁöÑÂÖ®Ê®°ÊÄÅÂµåÂÖ•Ê°ÜÊû∂ÔºåÁß∞‰∏∫LCO-Emb„ÄÇÂú®‰∏çÂêåÁöÑÈ™®Âπ≤ÁΩëÁªúÂíåÂü∫ÂáÜÊµãËØï‰∏≠ËøõË°åÁöÑÂ§ßÈáèÂÆûÈ™åËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÔºåÂπ∂Âú®ÂêÑÁßçÊ®°ÊÄÅ‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂèëÁé∞‰∫Ü‰∏Ä‰∏™ÁîüÊàê-Ë°®Á§∫Áº©ÊîæÂÆöÂæãÔºàGRSLÔºâÔºåË°®ÊòéÈÄöËøáÂØπÊØî‰ºòÂåñËé∑ÂæóÁöÑË°®Á§∫ËÉΩÂäõ‰∏éMLLMÁöÑÁîüÊàêËÉΩÂäõÂëàÊ≠£Áõ∏ÂÖ≥„ÄÇËøôË°®ÊòéÔºåÊèêÈ´òÁîüÊàêËÉΩÂäõÊòØÂ¢ûÂº∫Ë°®Á§∫Ë¥®ÈáèÁöÑÊúâÊïàËåÉ‰æã„ÄÇÊàë‰ª¨ÂØπGRSLËøõË°å‰∫ÜÁêÜËÆ∫Ëß£ÈáäÔºåÂ∞ÜMLLMÁöÑÁîüÊàêË¥®Èáè‰∏éÂÖ∂Ë°®Á§∫ÊÄßËÉΩÁöÑ‰∏äÈôêÊ≠£ÂºèËÅîÁ≥ªËµ∑Êù•ÔºåÂπ∂Âú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ΩéËµÑÊ∫êËßÜËßâ-ÊñáÊ°£Ê£ÄÁ¥¢‰ªªÂä°‰∏≠È™åËØÅ‰∫ÜÂÆÉÔºåË°®ÊòéÂú®CL‰πãÂâçËøõË°åÊåÅÁª≠ÁöÑÁîüÊàêÂºèÈ¢ÑËÆ≠ÁªÉÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫Ê®°ÂûãÂµåÂÖ•ËÉΩÂäõÁöÑÊΩúÂäõ„ÄÇ‰ª£Á†Å„ÄÅÊ®°ÂûãÂíåËµÑÊ∫êÂèØÂú®https://github.com/LCO-Embedding/LCO-EmbeddingËé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â¶Ç‰ΩïÊúâÊïàÂ≠¶‰π†Â§öÊ®°ÊÄÅÊï∞ÊçÆÁöÑÁªü‰∏ÄË°®ÂæÅÔºå‰ªéËÄåÊèêÂçáË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢Á≠â‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éÂØπÊØîÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåËôΩÁÑ∂ÂèñÂæó‰∫Ü‰∏ÄÂÆöÁöÑËøõÂ±ïÔºå‰ΩÜÁº∫‰πèÂØπMLLMÂú®Â§öÊ®°ÊÄÅË°®ÂæÅÂ≠¶‰π†‰∏≠‰ΩúÁî®ÁöÑÊ∑±ÂÖ•ÁêÜËß£Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÊõ¥Â•ΩÂú∞Âà©Áî®MLLMÁöÑÁîüÊàêËÉΩÂäõÊù•ÊèêÂçáË°®ÂæÅË¥®Èáè„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÔºåMLLMÂú®ÁîüÊàêÂºèÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÂ∑≤ÁªèÂ≠¶‰π†Âà∞‰∫ÜÈöêÂºèÁöÑË∑®Ê®°ÊÄÅÂØπÈΩêÔºåËøôÁßçÂØπÈΩê‰ΩøÂæóËØ≠Ë®ÄËß£Á†ÅÂô®ËÉΩÂ§üÂà©Áî®Â§öÊ®°ÊÄÅ‰ø°Âè∑ÁîüÊàêÂçïÊ®°ÊÄÅËæìÂá∫„ÄÇÂõ†Ê≠§ÔºåÂèØ‰ª•ÈÄöËøáÂØπÊØîÂ≠¶‰π†ÂØπMLLMÁöÑË°®ÂæÅËøõË°åÂæÆË∞ÉÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊèêÂçáË°®ÂæÅÁöÑË¥®Èáè„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜÁîüÊàê-Ë°®Á§∫Áº©ÊîæÂÆöÂæãÔºàGRSLÔºâÔºåËÆ§‰∏∫MLLMÁöÑÁîüÊàêËÉΩÂäõË∂äÂº∫ÔºåÂÖ∂Ë°®ÂæÅËÉΩÂäõ‰πüË∂äÂº∫„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLCO-EmbÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºö1) Âà©Áî®MLLMËøõË°åÁîüÊàêÂºèÈ¢ÑËÆ≠ÁªÉÔºåÂ≠¶‰π†Ë∑®Ê®°ÊÄÅÁöÑÈöêÂºèÂØπÈΩêÔºõ2) ‰ΩøÁî®ÂØπÊØîÂ≠¶‰π†ÂØπMLLMÁöÑË°®ÂæÅËøõË°åÂæÆË∞ÉÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáË°®ÂæÅÁöÑË¥®Èáè„ÄÇÊ°ÜÊû∂ÂèØ‰ª•ÈááÁî®‰∏çÂêåÁöÑMLLM‰Ωú‰∏∫È™®Âπ≤ÁΩëÁªúÔºå‰æãÂ¶ÇBLIP-2„ÄÅInstructBLIPÁ≠â„ÄÇÂú®ÂØπÊØîÂ≠¶‰π†Èò∂ÊÆµÔºåÂèØ‰ª•‰ΩøÁî®‰∏çÂêåÁöÑÂØπÊØîÊçüÂ§±ÂáΩÊï∞Ôºå‰æãÂ¶ÇInfoNCE„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÊèêÂá∫‰∫ÜLCO-EmbÊ°ÜÊû∂ÔºåÂπ∂Êè≠Á§∫‰∫ÜMLLMÂú®Â§öÊ®°ÊÄÅË°®ÂæÅÂ≠¶‰π†‰∏≠ÁöÑ‰ΩúÁî®„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåLCO-EmbÊõ¥Âä†Ê≥®ÈáçÂà©Áî®MLLMÁöÑÁîüÊàêËÉΩÂäõÔºåÂπ∂ÈÄöËøáÂØπÊØîÂ≠¶‰π†ËøõË°å‰ºòÂåñ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜÁîüÊàê-Ë°®Á§∫Áº©ÊîæÂÆöÂæãÔºàGRSLÔºâÔºå‰∏∫Â§öÊ®°ÊÄÅË°®ÂæÅÂ≠¶‰π†Êèê‰æõ‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËßÜËßí„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁîüÊàêÂºèÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºåÂèØ‰ª•‰ΩøÁî®‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíåËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÂú®ÂØπÊØîÂ≠¶‰π†Èò∂ÊÆµÔºåÈúÄË¶ÅÈÄâÊã©ÂêàÈÄÇÁöÑÂØπÊØîÊçüÂ§±ÂáΩÊï∞ÂíåË¥üÊ†∑Êú¨ÈááÊ†∑Á≠ñÁï•„ÄÇËÆ∫Êñá‰∏≠‰ΩøÁî®‰∫ÜInfoNCEÊçüÂ§±ÂáΩÊï∞ÔºåÂπ∂ÈááÁî®hard negative miningÁ≠ñÁï•„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂØπMLLMÁöÑÁªìÊûÑËøõË°å‰∫Ü‰∏Ä‰∫õË∞ÉÊï¥Ôºå‰æãÂ¶ÇÊ∑ªÂä†‰∫ÜÈ¢ùÂ§ñÁöÑÁ∫øÊÄßÂ±ÇÊù•Êò†Â∞Ñ‰∏çÂêåÊ®°ÊÄÅÁöÑÁâπÂæÅ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

LCO-EmbÂú®Â§ö‰∏™Ë∑®Ê®°ÊÄÅÊ£ÄÁ¥¢‰ªªÂä°‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®ËßÜËßâ-ÊñáÊ°£Ê£ÄÁ¥¢‰ªªÂä°‰∏≠ÔºåLCO-EmbÁõ∏ÊØî‰∫éÁé∞ÊúâÊñπÊ≥ïÂèñÂæó‰∫ÜÊòæËëóÁöÑÊèêÂçá„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åËøòÈ™åËØÅ‰∫ÜÁîüÊàê-Ë°®Á§∫Áº©ÊîæÂÆöÂæãÔºàGRSLÔºâÔºåË°®ÊòéÈÄöËøáÊåÅÁª≠ÁöÑÁîüÊàêÂºèÈ¢ÑËÆ≠ÁªÉÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÂçáÊ®°ÂûãÂµåÂÖ•ËÉΩÂäõÁöÑÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éË∑®Ê®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞ÁîüÊàêÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáÂ§öÊ®°ÊÄÅË°®ÂæÅÁöÑË¥®ÈáèÔºåÂèØ‰ª•ÊîπÂñÑÁî®Êà∑Âú®‰∏çÂêåÊ®°ÊÄÅÊï∞ÊçÆ‰πãÈó¥ËøõË°å‰ø°ÊÅØ‰∫§‰∫íÁöÑ‰ΩìÈ™åÔºå‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØ‰ª•ÈÄöËøáÊñáÊú¨Êü•ËØ¢Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÁöÑÂõæÂÉèÊàñËßÜÈ¢ëÔºåÊàñËÄÖÈÄöËøáÂõæÂÉèÊü•ËØ¢Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÁöÑÊñáÊ°£„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at https://github.com/LCO-Embedding/LCO-Embedding.

