---
layout: default
title: QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs
---

# QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11696" target="_blank" class="toolbar-btn">arXiv: 2510.11696v1</a>
    <a href="https://arxiv.org/pdf/2510.11696.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11696v1" 
            onclick="toggleFavorite(this, '2510.11696v1', 'QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen

**ÂàÜÁ±ª**: cs.LG, cs.CL, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

**Â§áÊ≥®**: Code is available at https://github.com/NVlabs/QeRL

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**QeRLÔºöÈáèÂåñÂ¢ûÂº∫ÁöÑLLMÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊèêÂçáÊïàÁéáÂπ∂Â¢ûÂº∫Êé¢Á¥¢ËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ÈáèÂåñ` `‰ΩéÁß©ÈÄÇÂ∫î` `Ê®°ÂûãÂéãÁº©` `Á≠ñÁï•Êé¢Á¥¢` `Ëá™ÈÄÇÂ∫îÂô™Â£∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLLMÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉËµÑÊ∫êÊ∂àËÄóÂ§ßÔºåÈúÄË¶ÅÂ§ßÈáèGPUÂÜÖÂ≠òÂíåÈïøÊó∂Èó¥ÁöÑrolloutÔºåÊïàÁéáÊòØ‰∏ªË¶ÅÁì∂È¢à„ÄÇ
2. QeRLÁªìÂêàNVFP4ÈáèÂåñÂíåLoRAÔºåÂä†ÈÄürolloutÂπ∂Èôç‰ΩéÂÜÖÂ≠òÂç†Áî®ÔºåÂêåÊó∂Âà©Áî®ÈáèÂåñÂô™Â£∞Â¢ûÂº∫Êé¢Á¥¢„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåQeRLÂä†ÈÄürolloutÔºåÈôç‰ΩéÂÜÖÂ≠òÈúÄÊ±ÇÔºåÂπ∂Âú®Êï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞ÊàñË∂ÖËøáÁé∞ÊúâÊñπÊ≥ïÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜQeRLÔºå‰∏Ä‰∏™Áî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÈáèÂåñÂ¢ûÂº∫Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂„ÄÇÂº∫ÂåñÂ≠¶‰π†ÂØπ‰∫éÊèêÂçáLLMsÁöÑÊé®ÁêÜËÉΩÂäõËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÂÖ∂ËµÑÊ∫êÊ∂àËÄóÂ∑®Â§ßÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑGPUÂÜÖÂ≠òÂíåËæÉÈïøÁöÑrolloutÊó∂Èó¥„ÄÇQeRLÈÄöËøáÁªìÂêàNVFP4ÈáèÂåñÂíå‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ªéËÄåÂä†ÈÄüÂº∫ÂåñÂ≠¶‰π†ÁöÑrolloutÈò∂ÊÆµÂπ∂Èôç‰ΩéÂÜÖÂ≠òÂºÄÈîÄ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈáèÂåñÂô™Â£∞Â¢ûÂä†‰∫ÜÁ≠ñÁï•ÁÜµÔºåÂ¢ûÂº∫‰∫ÜÊé¢Á¥¢ËÉΩÂäõÔºå‰ªéËÄåÂú®Âº∫ÂåñÂ≠¶‰π†ÊúüÈó¥ËÉΩÂ§üÂèëÁé∞Êõ¥Â•ΩÁöÑÁ≠ñÁï•„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•‰ºòÂåñÊé¢Á¥¢ÔºåQeRLÂºïÂÖ•‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÈáèÂåñÂô™Â£∞ÔºàAQNÔºâÊú∫Âà∂ÔºåËØ•Êú∫Âà∂Âú®ËÆ≠ÁªÉÊúüÈó¥Âä®ÊÄÅË∞ÉÊï¥Âô™Â£∞„ÄÇÂÆûÈ™åË°®ÊòéÔºåQeRLÂú®rolloutÈò∂ÊÆµÊèê‰æõ‰∫ÜË∂ÖËøá1.5ÂÄçÁöÑÂä†ÈÄü„ÄÇÊ≠§Â§ñÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ËÉΩÂ§üÂú®Âçï‰∏™H100 80GB GPU‰∏äÂØπ32B LLMËøõË°åÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÊ°ÜÊû∂ÔºåÂêåÊó∂Êèê‰æõ‰∫ÜÊï¥‰ΩìÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÂä†ÈÄü„ÄÇÂú®7BÊ®°Âûã‰∏äÔºåÂÆÉËøòÂÆûÁé∞‰∫ÜÊØî16‰ΩçLoRAÂíåQLoRAÊõ¥Âø´ÁöÑÂ•ñÂä±Â¢ûÈïøÂíåÊõ¥È´òÁöÑÊúÄÁªàÂáÜÁ°ÆÁéáÔºåÂêåÊó∂Âú®GSM8KÔºà90.8%ÔºâÂíåMATH 500Ôºà77.4%ÔºâÁ≠âÊï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠‰∏éÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÁöÑÊÄßËÉΩÁõ∏ÂåπÈÖç„ÄÇËøô‰∫õÁªìÊûúÁ°ÆÁ´ã‰∫ÜQeRL‰Ωú‰∏∫LLMs‰∏≠Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÈ´òÊïà‰∏îÊúâÊïàÁöÑÊ°ÜÊû∂„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉËøáÁ®ã‰∏≠ËµÑÊ∫êÊ∂àËÄóËøáÈ´òÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂ∫îÁî®‰∫éLLMsÊó∂ÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑGPUÂÜÖÂ≠òÂíåËæÉÈïøÁöÑrolloutÊó∂Èó¥ÔºåËøôÈôêÂà∂‰∫ÜÊ®°ÂûãËßÑÊ®°ÂíåËÆ≠ÁªÉÊïàÁéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÈáèÂåñÊù•Èôç‰ΩéÂÜÖÂ≠òÂç†Áî®ÂíåÂä†ÈÄüËÆ°ÁÆóÔºåÂêåÊó∂Âà©Áî®ÈáèÂåñËøáÁ®ãÂºïÂÖ•ÁöÑÂô™Â£∞Êù•Â¢ûÂº∫Á≠ñÁï•Êé¢Á¥¢„ÄÇÈÄöËøáÁªìÂêàNVFP4ÈáèÂåñÂíå‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÔºåÂú®‰øùËØÅÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰ΩéËµÑÊ∫êÈúÄÊ±Ç„ÄÇËá™ÈÄÇÂ∫îÈáèÂåñÂô™Â£∞ÔºàAQNÔºâÊú∫Âà∂Ëøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÊé¢Á¥¢ËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöQeRLÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÔºå‰ΩøÁî®NVFP4ÈáèÂåñÂØπLLMËøõË°åÈáèÂåñÔºåÈôç‰ΩéÂÜÖÂ≠òÂç†Áî®„ÄÇÁÑ∂ÂêéÔºåÁªìÂêàLoRAËøõË°åÂèÇÊï∞È´òÊïàÁöÑÂæÆË∞É„ÄÇÂú®Âº∫ÂåñÂ≠¶‰π†ÁöÑrolloutÈò∂ÊÆµÔºåÂà©Áî®ÈáèÂåñÂêéÁöÑÊ®°ÂûãËøõË°åÁ≠ñÁï•ËØÑ‰º∞ÂíåÊ†∑Êú¨ÁîüÊàê„ÄÇÊúÄÂêéÔºå‰ΩøÁî®Ëá™ÈÄÇÂ∫îÈáèÂåñÂô™Â£∞ÔºàAQNÔºâÊú∫Âà∂Âä®ÊÄÅË∞ÉÊï¥ÈáèÂåñÂô™Â£∞Ôºå‰ª•‰ºòÂåñÊé¢Á¥¢ËøáÁ®ã„ÄÇÊï¥‰∏™Ê°ÜÊû∂Êó®Âú®Âú®Èôç‰ΩéËµÑÊ∫êÊ∂àËÄóÁöÑÂêåÊó∂ÔºåÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöQeRLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÈáèÂåñÊäÄÊúØ‰∏éÂº∫ÂåñÂ≠¶‰π†Áõ∏ÁªìÂêàÔºåÂπ∂Âà©Áî®ÈáèÂåñÂô™Â£∞Êù•Â¢ûÂº∫Á≠ñÁï•Êé¢Á¥¢„ÄÇ‰º†ÁªüÁöÑÈáèÂåñÈÄöÂ∏∏Ë¢´ËßÜ‰∏∫‰∏ÄÁßçÈôç‰ΩéÊ®°ÂûãÂ§ßÂ∞èÂíåÂä†ÈÄüÊé®ÁêÜÁöÑÊâãÊÆµÔºåËÄåQeRLÂàôÂàõÊñ∞ÊÄßÂú∞Âà©Áî®ÈáèÂåñÂô™Â£∞Êù•‰øÉËøõÁ≠ñÁï•ÁöÑÂ§öÊ†∑ÊÄßÔºå‰ªéËÄåÈÅøÂÖçÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇÊ≠§Â§ñÔºåËá™ÈÄÇÂ∫îÈáèÂåñÂô™Â£∞ÔºàAQNÔºâÊú∫Âà∂ËÉΩÂ§üÊ†πÊçÆËÆ≠ÁªÉËøõÂ∫¶Âä®ÊÄÅË∞ÉÊï¥Âô™Â£∞Ê∞¥Âπ≥ÔºåËøõ‰∏ÄÊ≠•‰ºòÂåñÊé¢Á¥¢ËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöQeRLÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®NVFP4ÈáèÂåñÔºåÂú®Á≤æÂ∫¶ÂíåÊïàÁéá‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇ2) ÁªìÂêàLoRAËøõË°åÂèÇÊï∞È´òÊïàÁöÑÂæÆË∞ÉÔºåÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨„ÄÇ3) ÂºïÂÖ•Ëá™ÈÄÇÂ∫îÈáèÂåñÂô™Â£∞ÔºàAQNÔºâÊú∫Âà∂ÔºåÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥ÈáèÂåñÂô™Â£∞ÁöÑÂπÖÂ∫¶Êù•ÊéßÂà∂Êé¢Á¥¢ÁöÑÁ®ãÂ∫¶„ÄÇ AQNÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÔºà‰æãÂ¶ÇÔºåÂ¶Ç‰ΩïÊ†πÊçÆËÆ≠ÁªÉËøõÂ∫¶Ë∞ÉÊï¥Âô™Â£∞Ê∞¥Âπ≥ÔºâÂú®ËÆ∫Êñá‰∏≠ÂèØËÉΩÂåÖÂê´Êõ¥ËØ¶ÁªÜÁöÑÊèèËø∞Ôºå‰ΩÜÊëòË¶Å‰∏≠Êú™ÊòéÁ°ÆÊåáÂá∫ÂÖ∑‰ΩìÁöÑÂÖ¨ÂºèÊàñÁÆóÊ≥ï„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

QeRLÂú®rolloutÈò∂ÊÆµÂÆûÁé∞‰∫ÜË∂ÖËøá1.5ÂÄçÁöÑÂä†ÈÄüÔºåÂπ∂‰∏îÈ¶ñÊ¨°ÂÆûÁé∞‰∫ÜÂú®Âçï‰∏™H100 80GB GPU‰∏äÂØπ32B LLMËøõË°åÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇÂú®7BÊ®°Âûã‰∏äÔºåQeRLÊØî16‰ΩçLoRAÂíåQLoRAÂÆûÁé∞‰∫ÜÊõ¥Âø´ÁöÑÂ•ñÂä±Â¢ûÈïøÂíåÊõ¥È´òÁöÑÊúÄÁªàÂáÜÁ°ÆÁéáÔºåÂêåÊó∂Âú®GSM8KÔºà90.8%ÔºâÂíåMATH 500Ôºà77.4%ÔºâÁ≠âÊï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠‰∏éÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÁöÑÊÄßËÉΩÁõ∏ÂåπÈÖç„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

QeRLÊ°ÜÊû∂ÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÈúÄË¶ÅÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰æãÂ¶ÇÂØπËØùÁ≥ªÁªü„ÄÅÊô∫ËÉΩÂä©Êâã„ÄÅÊ∏∏ÊàèAIÁ≠â„ÄÇËØ•ÊñπÊ≥ïÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨ÂíåËµÑÊ∫êÈúÄÊ±ÇÔºå‰ΩøÂæóÊõ¥Â§ßËßÑÊ®°ÁöÑÊ®°ÂûãÂíåÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°Êàê‰∏∫ÂèØËÉΩ„ÄÇÊú™Êù•ÔºåQeRLÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÊ®°ÂûãÂéãÁº©ÊäÄÊúØÂíåÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºå‰∏∫LLMÁöÑÈÉ®ÁΩ≤ÂíåÂ∫îÁî®Â∏¶Êù•Êõ¥ÂπøÈòîÁöÑÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.

