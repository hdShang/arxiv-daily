---
layout: default
title: "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis"
---

# MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11579" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11579v1</a>
  <a href="https://arxiv.org/pdf/2510.11579.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11579v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11579v1', 'MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyu Zhu, Lin Chen, Mounim A. El-Yacoubi, Mingsheng Shang

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**å¤‡æ³¨**: Under Review

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HongyuZhu-s/MS-Mix)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMS-Mixä»¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ` `æ•°æ®å¢å¼º` `æƒ…æ„Ÿæ„ŸçŸ¥` `æ·±åº¦å­¦ä¹ ` `è‡ªæ³¨æ„åŠ›æœºåˆ¶` `æƒ…æ„Ÿå¯¹é½` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œæ ‡ç­¾æ¨¡ç³Šç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚
2. MS-Mixé€šè¿‡æƒ…æ„Ÿæ„ŸçŸ¥æ ·æœ¬é€‰æ‹©ã€åŠ¨æ€æ··åˆæ¯”ä¾‹è®¡ç®—å’Œæƒ…æ„Ÿå¯¹é½æŸå¤±ç­‰æœºåˆ¶ï¼Œä¼˜åŒ–å¤šæ¨¡æ€æ ·æœ¬æ··åˆè¿‡ç¨‹ã€‚
3. åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMS-Mixåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…­ç§æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæå‡äº†æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆMSAï¼‰æ—¨åœ¨é€šè¿‡æ•´åˆæ–‡æœ¬ã€è§†é¢‘å’ŒéŸ³é¢‘ç­‰å¼‚æ„æ•°æ®æºæ¥è¯†åˆ«å’Œè§£é‡Šäººç±»æƒ…æ„Ÿã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç½‘ç»œæ¶æ„è®¾è®¡ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†ä»å—åˆ°ç¨€ç¼ºçš„å¤šæ¨¡æ€æ ‡æ³¨æ•°æ®çš„é™åˆ¶ã€‚è™½ç„¶åŸºäºMixupçš„å¢å¼ºæ–¹æ³•åœ¨å•æ¨¡æ€ä»»åŠ¡ä¸­æå‡äº†æ³›åŒ–èƒ½åŠ›ï¼Œä½†å…¶åœ¨MSAä¸­çš„ç›´æ¥åº”ç”¨å¼•å…¥äº†æ ‡ç­¾æ¨¡ç³Šå’Œè¯­ä¹‰ä¸ä¸€è‡´ç­‰å…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MS-Mixï¼Œä¸€ä¸ªè‡ªé€‚åº”ã€æƒ…æ„Ÿæ•æ„Ÿçš„å¢å¼ºæ¡†æ¶ï¼Œè‡ªåŠ¨ä¼˜åŒ–å¤šæ¨¡æ€è®¾ç½®ä¸­çš„æ ·æœ¬æ··åˆã€‚MS-Mixçš„å…³é”®ç»„ä»¶åŒ…æ‹¬æƒ…æ„Ÿæ„ŸçŸ¥æ ·æœ¬é€‰æ‹©ç­–ç•¥ã€æƒ…æ„Ÿå¼ºåº¦å¼•å¯¼æ¨¡å—å’Œæƒ…æ„Ÿå¯¹é½æŸå¤±ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMS-Mixåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºç¨³å¥çš„å¤šæ¨¡æ€æƒ…æ„Ÿå¢å¼ºå»ºç«‹äº†æ–°æ ‡å‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­ç”±äºæ•°æ®ç¨€ç¼ºå¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ä¸è¶³é—®é¢˜ï¼Œç°æœ‰çš„Mixupæ–¹æ³•åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å¼•å…¥äº†æ ‡ç­¾æ¨¡ç³Šå’Œè¯­ä¹‰ä¸ä¸€è‡´çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMS-Mixçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æƒ…æ„Ÿæ„ŸçŸ¥çš„æ ·æœ¬æ··åˆç­–ç•¥ï¼Œé¿å…æ··åˆæ ·æœ¬ä¹‹é—´çš„æƒ…æ„ŸçŸ›ç›¾ï¼Œä»è€Œæå‡æ¨¡å‹çš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMS-Mixçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæƒ…æ„Ÿæ„ŸçŸ¥æ ·æœ¬é€‰æ‹©ï¼ˆSASSï¼‰ã€æƒ…æ„Ÿå¼ºåº¦å¼•å¯¼æ¨¡å—ï¼ˆSIGï¼‰å’Œæƒ…æ„Ÿå¯¹é½æŸå¤±ï¼ˆSALï¼‰ï¼Œé€šè¿‡è¿™äº›æ¨¡å—å®ç°æ ·æœ¬çš„æ™ºèƒ½æ··åˆå’Œæƒ…æ„Ÿä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMS-Mixçš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†æƒ…æ„Ÿæ„ŸçŸ¥çš„æ ·æœ¬é€‰æ‹©å’ŒåŠ¨æ€çš„æ··åˆæ¯”ä¾‹è®¡ç®—æœºåˆ¶ï¼Œæ˜¾è‘—æ”¹å–„äº†å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„æ ‡ç­¾æ¨¡ç³Šé—®é¢˜ï¼Œä¸ä¼ ç»Ÿçš„Mixupæ–¹æ³•å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒSIGæ¨¡å—åˆ©ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€è®¡ç®—å„æ¨¡æ€çš„æ··åˆæ¯”ä¾‹ï¼ŒSALåˆ™é€šè¿‡Kullback-LeibleræŸå¤±ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œè”åˆè®­ç»ƒæƒ…æ„Ÿå¼ºåº¦é¢„æµ‹å™¨å’Œä¸»å¹²ç½‘ç»œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMS-Mixåœ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­ç›¸è¾ƒäºå…­ç§æœ€å…ˆè¿›çš„æ¨¡å‹å‡æœ‰æ˜¾è‘—æå‡ï¼Œå…·ä½“è¡¨ç°ä¸ºå‡†ç¡®ç‡æé«˜äº†5%è‡³10%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMS-Mixä¸ºå¤šæ¨¡æ€æƒ…æ„Ÿå¢å¼ºè®¾ç«‹äº†æ–°çš„æ ‡å‡†ï¼Œå…·æœ‰è‰¯å¥½çš„å®ç”¨æ€§å’Œæ¨å¹¿ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æƒ…æ„Ÿåˆ†æã€ç¤¾äº¤åª’ä½“ç›‘æµ‹å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§ï¼ŒMS-Mixèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šæ›´å¥½åœ°ç†è§£ç”¨æˆ·æƒ…æ„Ÿï¼Œä»è€Œä¼˜åŒ–äº§å“å’ŒæœåŠ¡ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–éœ€è¦æƒ…æ„Ÿç†è§£çš„åœºæ™¯ï¼Œå¦‚å¿ƒç†å¥åº·ç›‘æµ‹å’Œæƒ…æ„Ÿè®¡ç®—ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Sentiment Analysis (MSA) aims to identify and interpret human emotions by integrating information from heterogeneous data sources such as text, video, and audio. While deep learning models have advanced in network architecture design, they remain heavily limited by scarce multimodal annotated data. Although Mixup-based augmentation improves generalization in unimodal tasks, its direct application to MSA introduces critical challenges: random mixing often amplifies label ambiguity and semantic inconsistency due to the lack of emotion-aware mixing mechanisms. To overcome these issues, we propose MS-Mix, an adaptive, emotion-sensitive augmentation framework that automatically optimizes sample mixing in multimodal settings. The key components of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS) strategy that effectively prevents semantic confusion caused by mixing samples with contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module using multi-head self-attention to compute modality-specific mixing ratios dynamically based on their respective emotional intensities. (3) a Sentiment Alignment Loss (SAL) that aligns the prediction distributions across modalities, and incorporates the Kullback-Leibler-based loss as an additional regularization term to train the emotion intensity predictor and the backbone network jointly. Extensive experiments on three benchmark datasets with six state-of-the-art backbones confirm that MS-Mix consistently outperforms existing methods, establishing a new standard for robust multimodal sentiment augmentation. The source code is available at: https://github.com/HongyuZhu-s/MS-Mix.

