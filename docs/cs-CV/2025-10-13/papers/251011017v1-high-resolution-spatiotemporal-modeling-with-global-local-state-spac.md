---
layout: default
title: High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation
---

# High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11017" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.11017v1</a>
  <a href="https://arxiv.org/pdf/2510.11017.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11017v1" onclick="toggleFavorite(this, '2510.11017v1', 'High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Runyang Feng, Hyung Jin Chang, Tze Ho Elden Tse, Boeun Kim, Yi Chang, Yixing Gao

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

**Â§áÊ≥®**: This paper is accepted to ICCV 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂÖ®Â±Ä-Â±ÄÈÉ®Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÁöÑÈ´òÂàÜËæ®ÁéáÊó∂Á©∫Âª∫Ê®°ÊñπÊ≥ïÔºåÁî®‰∫éËßÜÈ¢ë‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ë‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°` `Áä∂ÊÄÅÁ©∫Èó¥Ê®°Âûã` `Êó∂Á©∫Âª∫Ê®°` `È´òÂàÜËæ®Áéá` `ÂÖ®Â±Ä-Â±ÄÈÉ®Âª∫Ê®°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVHPEÊñπÊ≥ïÈöæ‰ª•Âπ≥Ë°°ÂÖ®Â±ÄÂä®ÊÄÅ‰∏ä‰∏ãÊñáÂíåÂ±ÄÈÉ®ËøêÂä®ÁªÜËäÇÁöÑÂª∫Ê®°ÔºåÂÆπÊòìÂÅèÂêë‰∏ÄÊñπÔºåÂØºËá¥ÊÄßËÉΩÊ¨°‰ºò„ÄÇ
2. ÊèêÂá∫ÂÖ®Â±ÄÊó∂Á©∫MambaÂíåÂ±ÄÈÉ®ÁªÜÂåñMambaÔºåÂàÜÂà´Â≠¶‰π†ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®È´òÂàÜËæ®ÁéáÊó∂Á©∫Ë°®Á§∫ÔºåÊèêÂçáÂª∫Ê®°ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®Âõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®ËÆ°ÁÆóÊïàÁéá‰∏äÊúâÊâÄÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éËßÜÈ¢ë‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°(VHPE)ÔºåËØ•Ê°ÜÊû∂Êâ©Â±ï‰∫ÜÁä∂ÊÄÅÁ©∫Èó¥Ê®°Âûã(Mamba)ÔºåÂàÜÂà´Â≠¶‰π†ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®È´òÂàÜËæ®ÁéáÊó∂Á©∫Ë°®Á§∫„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈ¶ñÂÖàÊèêÂá∫‰∫ÜÂÖ®Â±ÄÊó∂Á©∫MambaÔºåÊâßË°å6DÈÄâÊã©ÊÄßÊó∂Á©∫Êâ´Êèè‰ª•ÂèäÁ©∫Èó¥ÂíåÊó∂Èó¥Ë∞ÉÂà∂Êâ´ÊèèÂêàÂπ∂Ôºå‰ª•ÊúâÊïàÂú∞‰ªéÈ´òÂàÜËæ®ÁéáÂ∫èÂàó‰∏≠ÊèêÂèñÂÖ®Â±ÄË°®Á§∫„ÄÇÂÖ∂Ê¨°ÔºåÂºïÂÖ•‰∫ÜÂü∫‰∫éÁ™óÂè£Êó∂Á©∫Êâ´ÊèèÁöÑÂ±ÄÈÉ®ÁªÜÂåñMambaÔºå‰ª•Â¢ûÂº∫Â±ÄÈÉ®ÂÖ≥ÈîÆÁÇπËøêÂä®ÁöÑÈ´òÈ¢ëÁªÜËäÇ„ÄÇÂú®Âõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊ®°Âûã‰ºò‰∫éÊúÄÂÖàËøõÁöÑVHPEÊñπÊ≥ïÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜÊõ¥Â•ΩÁöÑËÆ°ÁÆóÊùÉË°°„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜÈ¢ë‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°(VHPE)ÈúÄË¶ÅÂª∫Ê®°È´òÂàÜËæ®ÁéáÁöÑÊó∂Á©∫Ë°®Á§∫ÔºåÂåÖÊã¨ÂÖ®Â±ÄÂä®ÊÄÅ‰∏ä‰∏ãÊñáÔºàÂ¶ÇÊï¥‰Ωì‰∫∫‰ΩìËøêÂä®Ë∂ãÂäøÔºâÂíåÂ±ÄÈÉ®ËøêÂä®ÁªÜËäÇÔºàÂ¶ÇÂÖ≥ÈîÆÁÇπÁöÑÈ´òÈ¢ëÂèòÂåñÔºâ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®Âçï‰∏ÄÁ±ªÂûãÁöÑÂª∫Ê®°ÁªìÊûÑÔºàÂç∑ÁßØÊàñÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºâÁªü‰∏ÄËøõË°åÊó∂Á©∫Â≠¶‰π†ÔºåÈöæ‰ª•Âπ≥Ë°°ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Âä®ÊÄÅÂª∫Ê®°ÔºåÂÆπÊòìÂÅèÂêë‰∏ÄÊñπ„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâVHPEÊ®°ÂûãÂú®ÊçïËé∑ÂÖ®Â±Ä‰æùËµñÂÖ≥Á≥ªÊó∂Èù¢‰∏¥‰∫åÊ¨°Â§çÊùÇÂ∫¶ÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®È´òÂàÜËæ®ÁéáÂ∫èÂàó‰∏äÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Êó∂Á©∫Âª∫Ê®°Ëß£ËÄ¶ÔºåÂàÜÂà´‰ΩøÁî®‰∏çÂêåÁöÑÁä∂ÊÄÅÁ©∫Èó¥Ê®°Âûã(SSM)ËøõË°åÂ§ÑÁêÜ„ÄÇÂÖ®Â±ÄÊó∂Á©∫MambaË¥üË¥£ÊçïËé∑Êï¥‰ΩìËøêÂä®Ë∂ãÂäøÔºåÂ±ÄÈÉ®ÁªÜÂåñMambaË¥üË¥£Â¢ûÂº∫ÂÖ≥ÈîÆÁÇπËøêÂä®ÁöÑÁªÜËäÇ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞Âπ≥Ë°°ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Âª∫Ê®°ÔºåÂπ∂Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂÖ®Â±ÄÊó∂Á©∫MambaÂíåÂ±ÄÈÉ®ÁªÜÂåñMamba„ÄÇÈ¶ñÂÖàÔºåËæìÂÖ•ËßÜÈ¢ëÂ∫èÂàóÁªèËøáÈ¢ÑÂ§ÑÁêÜÂíåÁâπÂæÅÊèêÂèñÔºåÂæóÂà∞È´òÂàÜËæ®ÁéáÁöÑÊó∂Á©∫ÁâπÂæÅË°®Á§∫„ÄÇÁÑ∂ÂêéÔºåÂÖ®Â±ÄÊó∂Á©∫MambaÂØπËøô‰∫õÁâπÂæÅËøõË°å6DÈÄâÊã©ÊÄßÊó∂Á©∫Êâ´ÊèèÂíåÁ©∫Èó¥-Êó∂Èó¥Ë∞ÉÂà∂Êâ´ÊèèÂêàÂπ∂ÔºåÊèêÂèñÂÖ®Â±ÄÊó∂Á©∫Ë°®Á§∫„ÄÇÊé•ÁùÄÔºåÂ±ÄÈÉ®ÁªÜÂåñMambaÂú®Â±ÄÈÉ®Á™óÂè£ÂÜÖËøõË°åÊó∂Á©∫Êâ´ÊèèÔºåÂ¢ûÂº∫ÂÖ≥ÈîÆÁÇπËøêÂä®ÁöÑÈ´òÈ¢ëÁªÜËäÇ„ÄÇÊúÄÂêéÔºåÂ∞ÜÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Ë°®Á§∫ËûçÂêàÔºåÈ¢ÑÊµã‰∫∫‰ΩìÂßøÊÄÅ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜMambaÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÊâ©Â±ïÂà∞È´òÂàÜËæ®ÁéáÊó∂Á©∫Âª∫Ê®°ÔºåÂπ∂ÂàÜÂà´Áî®‰∫éÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÁâπÂæÅÁöÑÂ≠¶‰π†„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊçïËé∑ÈïøÁ®ã‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫ÁöÑ6DÈÄâÊã©ÊÄßÊó∂Á©∫Êâ´ÊèèÂíåÁ©∫Èó¥-Êó∂Èó¥Ë∞ÉÂà∂Êâ´ÊèèÂêàÂπ∂Á≠ñÁï•ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊèêÂèñÂÖ®Â±ÄÊó∂Á©∫‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ®Â±ÄÊó∂Á©∫MambaÈááÁî®6DÈÄâÊã©ÊÄßÊó∂Á©∫Êâ´ÊèèÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®ÂÖ≠‰∏™Áª¥Â∫¶ÔºàÊó∂Èó¥„ÄÅÁ©∫Èó¥x„ÄÅÁ©∫Èó¥y„ÄÅÈÄöÈÅì„ÄÅbatch„ÄÅÂ∫èÂàóÈïøÂ∫¶Ôºâ‰∏äËøõË°åÈÄâÊã©ÊÄßÊâ´ÊèèÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÊçïËé∑ÂÖ®Â±ÄÊó∂Á©∫‰æùËµñÂÖ≥Á≥ª„ÄÇÁ©∫Èó¥-Êó∂Èó¥Ë∞ÉÂà∂Êâ´ÊèèÂêàÂπ∂ÈÄöËøáË∞ÉÂà∂Á©∫Èó¥ÂíåÊó∂Èó¥Áª¥Â∫¶‰∏äÁöÑÊâ´ÊèèÈ°∫Â∫èÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÂÖ®Â±ÄË°®Á§∫ÁöÑË¥®Èáè„ÄÇÂ±ÄÈÉ®ÁªÜÂåñMambaÈááÁî®Á™óÂè£ÂåñÁöÑÊó∂Á©∫Êâ´ÊèèÔºåÈôêÂà∂‰∫ÜËÆ°ÁÆóËåÉÂõ¥ÔºåÂπ∂‰∏ìÊ≥®‰∫éÂ¢ûÂº∫Â±ÄÈÉ®ÂÖ≥ÈîÆÁÇπËøêÂä®ÁöÑÁªÜËäÇ„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®Â∏∏Áî®ÁöÑÂùáÊñπËØØÂ∑Æ(MSE)ÊçüÂ§±Ôºå‰ºòÂåñÈ¢ÑÊµãÂßøÊÄÅ‰∏éÁúüÂÆûÂßøÊÄÅ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®Âõõ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºàÂåÖÊã¨COCO„ÄÅMPIIÂíåPoseTrackÔºâ‰∏äÂùáÂèñÂæó‰∫Ü‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®PoseTrack 2018Êï∞ÊçÆÈõÜ‰∏äÔºåËØ•Ê®°ÂûãÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÂèñÂæó‰∫ÜÊòæËëóÊèêÂçáÔºåÂπ≥ÂùáÁ≤æÂ∫¶(mAP)ÊèêÈ´ò‰∫ÜË∂ÖËøá2‰∏™ÁôæÂàÜÁÇπÔºåÂêåÊó∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Êõ¥‰Ωé„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈ´òÁ≤æÂ∫¶‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËßÜÈ¢ëÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅËôöÊãüÁé∞ÂÆû„ÄÅËøêÂä®ÂàÜÊûêÂíåÂ∫∑Â§çËÆ≠ÁªÉÁ≠â„ÄÇÈÄöËøáÊõ¥ÂáÜÁ°ÆÂú∞ÊçïÊçâ‰∫∫‰ΩìËøêÂä®ÁöÑÂÖ®Â±ÄË∂ãÂäøÂíåÂ±ÄÈÉ®ÁªÜËäÇÔºåÂèØ‰ª•ÊèêÂçáËøô‰∫õÂ∫îÁî®ÁöÑÁî®Êà∑‰ΩìÈ™åÂíåÊÄßËÉΩÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Modeling high-resolution spatiotemporal representations, including both global dynamic contexts (e.g., holistic human motion tendencies) and local motion details (e.g., high-frequency changes of keypoints), is essential for video-based human pose estimation (VHPE). Current state-of-the-art methods typically unify spatiotemporal learning within a single type of modeling structure (convolution or attention-based blocks), which inherently have difficulties in balancing global and local dynamic modeling and may bias the network to one of them, leading to suboptimal performance. Moreover, existing VHPE models suffer from quadratic complexity when capturing global dependencies, limiting their applicability especially for high-resolution sequences. Recently, the state space models (known as Mamba) have demonstrated significant potential in modeling long-range contexts with linear complexity; however, they are restricted to 1D sequential data. In this paper, we present a novel framework that extends Mamba from two aspects to separately learn global and local high-resolution spatiotemporal representations for VHPE. Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D selective space-time scan and spatial- and temporal-modulated scan merging to efficiently extract global representations from high-resolution sequences. We further introduce a windowed space-time scan-based Local Refinement Mamba to enhance the high-frequency details of localized keypoint motions. Extensive experiments on four benchmark datasets demonstrate that the proposed model outperforms state-of-the-art VHPE approaches while achieving better computational trade-offs.

