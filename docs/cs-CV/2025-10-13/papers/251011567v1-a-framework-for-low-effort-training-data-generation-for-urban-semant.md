---
layout: default
title: A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation
---

# A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11567" target="_blank" class="toolbar-btn">arXiv: 2510.11567v1</a>
    <a href="https://arxiv.org/pdf/2510.11567.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11567v1" 
            onclick="toggleFavorite(this, '2510.11567v1', 'A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Denis Zavadski, Damjan KalÅ¡an, Tim KÃ¼chler, Haebom Lee, Stefan Roth, Carsten Rother

**åˆ†ç±»**: cs.CV, cs.GR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„ä½æˆæœ¬è®­ç»ƒæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæå‡åŸå¸‚è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `åŸå¸‚è¯­ä¹‰åˆ†å‰²` `åˆæˆæ•°æ®ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `é¢†åŸŸè‡ªé€‚åº”` `ä½æˆæœ¬è®­ç»ƒæ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨åˆæˆæ•°æ®è®­ç»ƒåŸå¸‚è¯­ä¹‰åˆ†å‰²æ¨¡å‹æ—¶ï¼Œå­˜åœ¨ä¸çœŸå®å›¾åƒçš„é¢†åŸŸå·®è·ï¼Œé™åˆ¶äº†ä¸‹æ¸¸æ€§èƒ½ã€‚
2. è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¼ªæ ‡ç­¾å°†åˆæˆæ•°æ®é€‚é…åˆ°ç›®æ ‡é¢†åŸŸï¼Œç”Ÿæˆé«˜ä¿çœŸã€ç›®æ ‡å¯¹é½çš„å›¾åƒã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„åˆ†å‰²æ€§èƒ½æå‡ï¼Œè¯æ˜äº†ä½æˆæœ¬åˆæˆæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œä»…ä½¿ç”¨ä¸å®Œå–„çš„ä¼ªæ ‡ç­¾ï¼Œå³å¯å°†æ¨¡å‹é€‚é…åˆ°ç›®æ ‡é¢†åŸŸã€‚è®­ç»ƒå®Œæˆåï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»ä»»ä½•åˆæˆæ•°æ®é›†çš„è¯­ä¹‰åœ°å›¾ç”Ÿæˆé«˜ä¿çœŸã€ç›®æ ‡å¯¹é½çš„å›¾åƒï¼ŒåŒ…æ‹¬é‚£äº›ä½æˆæœ¬ã€å¿«é€Ÿæ„å»ºçš„æ•°æ®é›†ã€‚è¯¥æ–¹æ³•è¿‡æ»¤æ¬¡ä¼˜ç”Ÿæˆç»“æœï¼Œæ ¡æ­£å›¾åƒ-æ ‡ç­¾é”™ä½ï¼Œå¹¶æ ‡å‡†åŒ–è·¨æ•°æ®é›†çš„è¯­ä¹‰ï¼Œä»è€Œå°†å¼±åˆæˆæ•°æ®è½¬åŒ–ä¸ºå…·æœ‰ç«äº‰åŠ›çš„çœŸå®é¢†åŸŸè®­ç»ƒé›†ã€‚åœ¨äº”ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®ç›®æ ‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„è½¬æ¢æ–¹æ³•ç›¸æ¯”ï¼Œåˆ†å‰²æ€§èƒ½æå‡é«˜è¾¾+8.0%pt mIoUã€‚è¿™ä½¿å¾—å¿«é€Ÿæ„å»ºçš„åˆæˆæ•°æ®é›†ä¸éœ€è¦å¤§é‡äººå·¥è®¾è®¡çš„é«˜æˆæœ¬ã€è€—æ—¶åˆæˆæ•°æ®é›†ä¸€æ ·æœ‰æ•ˆã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†ä¸€ä¸ªæœ‰ä»·å€¼çš„åä½œæ¨¡å¼ï¼Œå³å¿«é€Ÿè¯­ä¹‰åŸå‹è®¾è®¡ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œèƒ½å¤Ÿä¸ºåŸå¸‚åœºæ™¯ç†è§£å®ç°å¯æ‰©å±•ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®åˆ›å»ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸå¸‚è¯­ä¹‰åˆ†å‰²ä¸­ï¼Œä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹æ—¶ï¼Œåˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ä¹‹é—´å­˜åœ¨çš„é¢†åŸŸå·®è·é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚ç›´æ¥ä½¿ç”¨åˆæˆæ•°æ®æˆ–ä½¿ç”¨å›¾åƒè½¬æ¢æ–¹æ³•ï¼Œæ— æ³•æœ‰æ•ˆå¼¥åˆè¿™ä¸€å·®è·ï¼Œå¯¼è‡´æ¨¡å‹åœ¨çœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚é«˜ç²¾åº¦çš„åˆæˆæ•°æ®åˆ¶ä½œæˆæœ¬é«˜æ˜‚ï¼Œæ— æ³•æ»¡è¶³ä½æˆæœ¬è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå°†ä½æˆæœ¬åˆæˆæ•°æ®é›†çš„è¯­ä¹‰ä¿¡æ¯è½¬åŒ–ä¸ºé«˜ä¿çœŸã€ç›®æ ‡é¢†åŸŸå¯¹é½çš„å›¾åƒã€‚é€šè¿‡åœ¨ç›®æ ‡é¢†åŸŸæ•°æ®ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°ç›®æ ‡é¢†åŸŸçš„å›¾åƒé£æ ¼å’Œç‰¹å¾åˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆæ›´é€¼çœŸçš„åˆæˆæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨ä½æˆæœ¬çš„åˆæˆæ•°æ®é›†ç”Ÿæˆè¯­ä¹‰åœ°å›¾ï¼›2) ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼Œå°†è¯­ä¹‰åœ°å›¾è½¬æ¢ä¸ºå›¾åƒï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç›®æ ‡é¢†åŸŸæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ ç›®æ ‡é¢†åŸŸçš„å›¾åƒé£æ ¼ï¼›3) å¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œè¿‡æ»¤ï¼Œå»é™¤è´¨é‡è¾ƒå·®çš„å›¾åƒï¼›4) å¯¹å›¾åƒå’Œæ ‡ç­¾è¿›è¡Œæ ¡æ­£ï¼Œä»¥è§£å†³å›¾åƒ-æ ‡ç­¾é”™ä½é—®é¢˜ï¼›5) å¯¹ä¸åŒæ•°æ®é›†çš„è¯­ä¹‰è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä»¥ä¿è¯è¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€ç›®æ ‡é¢†åŸŸå¯¹é½çš„åˆæˆæ•°æ®ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒè½¬æ¢æ–¹æ³•ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç›®æ ‡é¢†åŸŸçš„å›¾åƒé£æ ¼å’Œç‰¹å¾åˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆæ›´é€¼çœŸçš„å›¾åƒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æå‡ºäº†ä¸€ç³»åˆ—åå¤„ç†æ­¥éª¤ï¼Œå¦‚å›¾åƒè¿‡æ»¤ã€å›¾åƒ-æ ‡ç­¾æ ¡æ­£å’Œè¯­ä¹‰æ ‡å‡†åŒ–ï¼Œä»¥è¿›ä¸€æ­¥æé«˜åˆæˆæ•°æ®çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ‰©æ•£æ¨¡å‹é‡‡ç”¨æ ‡å‡†çš„U-Netç»“æ„ï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæŸå¤±ã€‚å›¾åƒè¿‡æ»¤é‡‡ç”¨åŸºäºå›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡çš„æ–¹æ³•ï¼Œå¦‚FID scoreã€‚å›¾åƒ-æ ‡ç­¾æ ¡æ­£é‡‡ç”¨åŸºäºå…‰æµçš„æ–¹æ³•ï¼Œå¯¹å›¾åƒå’Œæ ‡ç­¾è¿›è¡Œå¯¹é½ã€‚è¯­ä¹‰æ ‡å‡†åŒ–é‡‡ç”¨è¯­ä¹‰æ˜ å°„çš„æ–¹æ³•ï¼Œå°†ä¸åŒæ•°æ®é›†çš„è¯­ä¹‰æ˜ å°„åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®ç›®æ ‡æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„åˆ†å‰²æ€§èƒ½æå‡ï¼Œä¸æœ€å…ˆè¿›çš„è½¬æ¢æ–¹æ³•ç›¸æ¯”ï¼Œåˆ†å‰²æ€§èƒ½æå‡é«˜è¾¾+8.0%pt mIoUã€‚è¿™è¡¨æ˜ï¼Œé€šè¿‡è¯¥æ–¹æ³•ç”Ÿæˆçš„ä½æˆæœ¬åˆæˆæ•°æ®é›†ï¼Œå¯ä»¥è¾¾åˆ°ç”šè‡³è¶…è¿‡é«˜æˆæœ¬åˆæˆæ•°æ®é›†çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€åŸå¸‚è§„åˆ’ç­‰é¢†åŸŸã€‚é€šè¿‡ä½æˆæœ¬ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥é™ä½æ¨¡å‹è®­ç»ƒçš„æˆæœ¬å’Œæ—¶é—´ï¼ŒåŠ é€Ÿç›¸å…³æŠ€æœ¯çš„ç ”å‘å’Œåº”ç”¨ã€‚è¯¥æ–¹æ³•è¿˜å¯ç”¨äºç”Ÿæˆç‰¹å®šåœºæ™¯ä¸‹çš„åˆæˆæ•°æ®ï¼Œä¾‹å¦‚æ¶åŠ£å¤©æ°”ã€å…‰ç…§ä¸è¶³ç­‰æƒ…å†µï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding.

