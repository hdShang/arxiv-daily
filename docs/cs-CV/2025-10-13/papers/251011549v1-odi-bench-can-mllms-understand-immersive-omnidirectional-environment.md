---
layout: default
title: ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?
---

# ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11549" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11549v1</a>
  <a href="https://arxiv.org/pdf/2510.11549.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11549v1" onclick="toggleFavorite(this, '2510.11549v1', 'ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Liu Yang, Huiyu Duan, Ran Tao, Juntao Cheng, Sijing Wu, Yunhao Li, Jing Liu, Xiongkuo Min, Guangtao Zhai

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºODI-Benchï¼Œè¯„ä¼°MLLMåœ¨å…¨æ™¯å›¾åƒç†è§£ä¸­çš„èƒ½åŠ›å¹¶æå‡ºOmni-CoTæ–¹æ³•ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…¨æ™¯å›¾åƒç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•` `é“¾å¼æ¨ç†` `VR/AR` `å…·èº«æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨å…¨æ™¯å›¾åƒç†è§£æ–¹é¢èƒ½åŠ›ä¸è¶³ï¼Œç¼ºä¹ä¸“é—¨çš„è¯„ä¼°åŸºå‡†ã€‚
2. æå‡ºODI-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«é«˜è´¨é‡å…¨æ™¯å›¾åƒå’Œç»†ç²’åº¦é—®ç­”å¯¹ï¼Œç”¨äºå…¨é¢è¯„ä¼°MLLMã€‚
3. å¼•å…¥Omni-CoTæ–¹æ³•ï¼Œé€šè¿‡é“¾å¼æ¨ç†å¢å¼ºMLLMå¯¹å…¨æ™¯ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…¨æ™¯å›¾åƒ(ODIs)æä¾›å®Œæ•´çš„360x180è§†è§’ï¼Œè¢«å¹¿æ³›åº”ç”¨äºVRã€ARå’Œå…·èº«æ™ºèƒ½åº”ç”¨ä¸­ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ä¼ ç»Ÿçš„2Då›¾åƒå’Œè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ç†è§£ç”±ODIsæ•è·çš„æ²‰æµ¸å¼ç¯å¢ƒçš„èƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ODI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå…¨æ™¯å›¾åƒç†è§£è€Œè®¾è®¡çš„æ–°å‹ç»¼åˆåŸºå‡†ã€‚ODI-BenchåŒ…å«2,000å¼ é«˜è´¨é‡å…¨æ™¯å›¾åƒå’Œ4,000å¤šä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„é—®ç­”(QA)å¯¹ï¼Œæ¶µç›–10ä¸ªç»†ç²’åº¦ä»»åŠ¡ï¼ŒåŒ…æ‹¬é€šç”¨çº§åˆ«å’Œç©ºé—´çº§åˆ«çš„ODIç†è§£ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥è¯„ä¼°20ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„MLLMï¼ŒåŒ…æ‹¬ä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹ï¼Œåœ¨å°é—­å¼å’Œå¼€æ”¾å¼è®¾ç½®ä¸‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„MLLMä»ç„¶éš¾ä»¥æ•æ‰ODIsæä¾›çš„æ²‰æµ¸å¼ä¸Šä¸‹æ–‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†Omni-CoTï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬ä¿¡æ¯å’Œè§†è§‰çº¿ç´¢çš„é“¾å¼æ¨ç†ï¼Œæ˜¾è‘—æé«˜äº†MLLMåœ¨å…¨æ™¯ç¯å¢ƒä¸­çš„ç†è§£èƒ½åŠ›ã€‚åŸºå‡†æµ‹è¯•å’Œä»£ç å°†åœ¨å‘å¸ƒåå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£å…¨æ™¯å›¾åƒï¼ˆODIsï¼‰æ—¶è¡¨ç°å‡ºçš„ä¸è¶³ã€‚ç°æœ‰çš„MLLMsåœ¨å¤„ç†ä¼ ç»Ÿ2Då›¾åƒæ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¯¹å…¨æ™¯å›¾åƒæ‰€æä¾›çš„æ²‰æµ¸å¼ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨VR/ARç­‰é¢†åŸŸçš„åº”ç”¨ã€‚ç°æœ‰çš„æ–¹æ³•æ²¡æœ‰é’ˆå¯¹å…¨æ™¯å›¾åƒçš„ä¸“é—¨è¯„ä¼°åŸºå‡†ï¼Œä¹Ÿç¼ºä¹æœ‰æ•ˆçš„å…¨æ™¯å›¾åƒç†è§£å¢å¼ºæ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªä¸“é—¨é’ˆå¯¹å…¨æ™¯å›¾åƒç†è§£çš„åŸºå‡†æµ‹è¯•é›†ï¼ˆODI-Benchï¼‰ï¼Œå¹¶æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„é“¾å¼æ¨ç†æ–¹æ³•ï¼ˆOmni-CoTï¼‰æ¥å¢å¼ºMLLMså¯¹å…¨æ™¯å›¾åƒçš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡ODI-Benchï¼Œå¯ä»¥ç³»ç»Ÿåœ°è¯„ä¼°ç°æœ‰MLLMsåœ¨å…¨æ™¯å›¾åƒç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚Omni-CoTåˆ™åˆ©ç”¨æ–‡æœ¬ä¿¡æ¯å’Œè§†è§‰çº¿ç´¢è¿›è¡Œé“¾å¼æ¨ç†ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰å…¨æ™¯å›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šODI-BenchåŸºå‡†æµ‹è¯•å’ŒOmni-CoTæ–¹æ³•ã€‚ODI-BenchåŒ…å«2000å¼ é«˜è´¨é‡å…¨æ™¯å›¾åƒå’Œ4000å¤šä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–10ä¸ªç»†ç²’åº¦ä»»åŠ¡ã€‚Omni-CoTæ–¹æ³•åˆ™æ˜¯åœ¨ç°æœ‰MLLMsçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡prompt engineeringçš„æ–¹å¼ï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡Œé“¾å¼æ¨ç†ï¼Œä»è€Œæé«˜å…¶å…¨æ™¯å›¾åƒç†è§£èƒ½åŠ›ã€‚å…·ä½“æµç¨‹æ˜¯ï¼šè¾“å…¥å…¨æ™¯å›¾åƒå’Œé—®é¢˜ï¼Œæ¨¡å‹é¦–å…ˆç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œç„¶ååŸºäºè¿™äº›æ­¥éª¤ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å…¨æ™¯å›¾åƒç†è§£çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•é›†ODI-Benchï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚2) æå‡ºäº†Omni-CoTæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå³å¯æ˜¾è‘—æé«˜MLLMså¯¹å…¨æ™¯å›¾åƒçš„ç†è§£èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOmni-CoTä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ¨¡å‹å‚æ•°ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šODI-Benchçš„å…³é”®è®¾è®¡åœ¨äºå…¶å¤šæ ·æ€§å’Œç»†ç²’åº¦ã€‚å®ƒåŒ…å«äº†å„ç§åœºæ™¯çš„å…¨æ™¯å›¾åƒï¼Œå¹¶è®¾è®¡äº†10ä¸ªç»†ç²’åº¦ä»»åŠ¡ï¼Œæ¶µç›–äº†é€šç”¨çº§åˆ«å’Œç©ºé—´çº§åˆ«çš„ç†è§£ã€‚Omni-CoTçš„å…³é”®è®¾è®¡åœ¨äºå…¶é“¾å¼æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„promptï¼Œå¼•å¯¼æ¨¡å‹é€æ­¥æ¨ç†ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰å…¨æ™¯å›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…·ä½“çš„promptè®¾è®¡åŒ…æ‹¬ï¼šé¦–å…ˆè¦æ±‚æ¨¡å‹æè¿°å›¾åƒä¸­çš„ä¸»è¦å¯¹è±¡å’Œåœºæ™¯ï¼Œç„¶åè¦æ±‚æ¨¡å‹åˆ†æå¯¹è±¡ä¹‹é—´çš„å…³ç³»ï¼Œæœ€åè¦æ±‚æ¨¡å‹å›ç­”é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰MLLMåœ¨ODI-Benchä¸Šçš„è¡¨ç°è¿œä½äºäººç±»æ°´å¹³ï¼Œè¡¨æ˜å…¶åœ¨å…¨æ™¯å›¾åƒç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚Omni-CoTæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜MLLMåœ¨ODI-Benchä¸Šçš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜æ˜¾çš„æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ç©ºé—´å…³ç³»ç†è§£ä»»åŠ¡ä¸Šï¼ŒOmni-CoTå°†æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†10%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºVR/ARã€æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ODI-Benchä¸ºè¯„ä¼°å’Œæ”¹è¿›MLLMåœ¨å…¨æ™¯å›¾åƒç†è§£æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ ‡å‡†å¹³å°ã€‚Omni-CoTæ–¹æ³•å¯ä»¥å¸®åŠ©MLLMæ›´å¥½åœ°ç†è§£æ²‰æµ¸å¼ç¯å¢ƒï¼Œä»è€Œæé«˜ç›¸å…³åº”ç”¨çš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°è§†é¢‘ç†è§£ã€ä¸‰ç»´åœºæ™¯ç†è§£ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Omnidirectional images (ODIs) provide full 360x180 view which are widely adopted in VR, AR and embodied intelligence applications. While multi-modal large language models (MLLMs) have demonstrated remarkable performance on conventional 2D image and video understanding benchmarks, their ability to comprehend the immersive environments captured by ODIs remains largely unexplored. To address this gap, we first present ODI-Bench, a novel comprehensive benchmark specifically designed for omnidirectional image understanding. ODI-Bench contains 2,000 high-quality omnidirectional images and over 4,000 manually annotated question-answering (QA) pairs across 10 fine-grained tasks, covering both general-level and spatial-level ODI understanding. Extensive experiments are conducted to benchmark 20 representative MLLMs, including proprietary and open-source models, under both close-ended and open-ended settings. Experimental results reveal that current MLLMs still struggle to capture the immersive context provided by ODIs. To this end, we further introduce Omni-CoT, a training-free method which significantly enhances MLLMs' comprehension ability in the omnidirectional environment through chain-of-thought reasoning across both textual information and visual cues. Both the benchmark and the code will be released upon the publication.

