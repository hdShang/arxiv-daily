---
layout: default
title: mmWalk: Towards Multi-modal Multi-view Walking Assistance
---

# mmWalk: Towards Multi-modal Multi-view Walking Assistance

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11520" target="_blank" class="toolbar-btn">arXiv: 2510.11520v2</a>
    <a href="https://arxiv.org/pdf/2510.11520.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11520v2" 
            onclick="toggleFavorite(this, '2510.11520v2', 'mmWalk: Towards Multi-modal Multi-view Walking Assistance')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13 (Êõ¥Êñ∞: 2025-10-23)

**Â§áÊ≥®**: Accepted by NeurIPS 2025 Datasets and Benchmarks Track. Data and Code: https://github.com/KediYing/mmWalk

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**mmWalkÔºöÈù¢ÂêëÁõ≤‰∫∫Êàñ‰ΩéËßÜÂäõ‰∫∫Áæ§ÁöÑÂ§öÊ®°ÊÄÅÂ§öËßÜËßíÊ≠•Ë°åËæÖÂä©Êï∞ÊçÆÈõÜ‰∏éÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê≠•Ë°åËæÖÂä©` `ËßÜËßâÈöúÁ¢ç` `Êï∞ÊçÆÈõÜÊûÑÂª∫` `ËßÜËßâÈóÆÁ≠î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÁêÜËß£Â§çÊùÇÁéØÂ¢ÉÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÈöæ‰ª•ÊúâÊïàËæÖÂä©Áõ≤‰∫∫Êàñ‰ΩéËßÜÂäõ‰∫∫Áæ§Âú®Êà∑Â§ñÂÆâÂÖ®Ë°åËµ∞„ÄÇ
2. mmWalkÊï∞ÊçÆÈõÜÈÄöËøáÈõÜÊàêÂ§öËßÜËßí‰º†ÊÑüÂô®Êï∞ÊçÆÂíåÂèØËÆøÈóÆÊÄßÁâπÂæÅÔºåÊ®°ÊãüÁúüÂÆû‰∏ñÁïåÂú∫ÊôØÔºå‰∏∫Ê≠•Ë°åËæÖÂä©Êèê‰æõÊõ¥ÂÖ®Èù¢ÁöÑ‰ø°ÊÅØ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂú®mmWalk‰∏äÂæÆË∞ÉÁöÑÊ®°ÂûãÂú®ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÈ™åËØÅ‰∫ÜÊï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÈíàÂØπÁõ≤‰∫∫Êàñ‰ΩéËßÜÂäõ(BLV)‰∫∫Áæ§Âú®ÊûÅÁ´ØÊàñÂ§çÊùÇÁéØÂ¢É‰∏≠Ë°åËµ∞ËæÖÂä©ÁöÑÊåëÊàòÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫mmWalkÁöÑÊ®°ÊãüÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜÈõÜÊàê‰∫ÜÂ§öËßÜËßí‰º†ÊÑüÂô®ÂíåÈù¢ÂêëÂèØËÆøÈóÆÊÄßÁöÑÁâπÂæÅÔºåÁî®‰∫éÊà∑Â§ñÂÆâÂÖ®ÂØºËà™„ÄÇmmWalkÂåÖÂê´120Êù°ÊâãÂä®ÊéßÂà∂„ÄÅÂú∫ÊôØÂàÜÁ±ªÁöÑË°åËµ∞ËΩ®ËøπÔºåÂÖ±ËÆ°62kÂêåÊ≠•Â∏ßÔºåË∂ÖËøá559kÂº†RGB„ÄÅÊ∑±Â∫¶ÂíåËØ≠‰πâÊ®°ÊÄÅÁöÑÂÖ®ÊôØÂõæÂÉè„ÄÇ‰∏∫‰∫ÜÂº∫Ë∞ÉÁúüÂÆû‰∏ñÁïåÁöÑÂÖ≥ËÅîÊÄßÔºåÊØèÊù°ËΩ®ËøπÈÉΩÂåÖÂê´Êà∑Â§ñÊûÅÁ´ØÊÉÖÂÜµÂíåBLVÁî®Êà∑ÁöÑÂèØËÆøÈóÆÊÄßÁâπÂÆöÂú∞Ê†á„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÁîüÊàê‰∫ÜmmWalkVQAÔºå‰∏Ä‰∏™ÂåÖÂê´Ë∂ÖËøá69k‰∏™ËßÜËßâÈóÆÁ≠î‰∏âÂÖÉÁªÑÁöÑVQAÂü∫ÂáÜÔºåÊ∂µÁõñ9‰∏™Á±ªÂà´Ôºå‰∏ì‰∏∫ÂÆâÂÖ®ÂíåÁü•ÊÉÖÁöÑÊ≠•Ë°åËæÖÂä©ËÄåÂÆöÂà∂„ÄÇÈÄöËøáÈõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨ËÆæÁΩÆËØÑ‰º∞‰∫ÜÊúÄÂÖàËøõÁöÑËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)ÔºåÂèëÁé∞ÂÆÉ‰ª¨Âú®È£éÈô©ËØÑ‰º∞ÂíåÂØºËà™‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇÊúÄÂêéÔºåÂú®ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏äÈ™åËØÅ‰∫ÜmmWalkÂæÆË∞ÉÊ®°ÂûãÁöÑÊúâÊïàÊÄßÔºåËØÅÊòé‰∫ÜËØ•Êï∞ÊçÆÈõÜÂú®Êé®ËøõÂ§öÊ®°ÊÄÅÊ≠•Ë°åËæÖÂä©ÊñπÈù¢ÁöÑ‰ª∑ÂÄº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊ≠•Ë°åËæÖÂä©Á≥ªÁªüÁº∫‰πèÂØπÂ§çÊùÇÁéØÂ¢ÉÁöÑÂÖ®Èù¢ÁêÜËß£ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊûÅÁ´ØÊàñÂ§çÊùÇÂú∫ÊôØ‰∏ãÔºåÊó†Ê≥ï‰∏∫Áõ≤‰∫∫Êàñ‰ΩéËßÜÂäõ‰∫∫Áæ§Êèê‰æõÂÖÖÂàÜÁöÑÂÆâÂÖ®‰øùÈöú„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÊï¥ÂêàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåÂπ∂Áº∫‰πèÈíàÂØπBLVÁî®Êà∑ÁöÑÁâπÂÆöÂú∫ÊôØÂíåÂú∞Ê†áÁöÑËÄÉËôë„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºàRGB„ÄÅÊ∑±Â∫¶„ÄÅËØ≠‰πâÔºâÂíåÈíàÂØπBLVÁî®Êà∑ÁöÑÁâπÂÆöÂú∫ÊôØÂíåÂú∞Ê†áÁöÑÊ®°ÊãüÊï∞ÊçÆÈõÜmmWalkÔºå‰ªéËÄåËÆ≠ÁªÉÊõ¥ÊúâÊïàÁöÑÊ≠•Ë°åËæÖÂä©Ê®°Âûã„ÄÇÈÄöËøáÊ®°ÊãüÁúüÂÆû‰∏ñÁïåÁöÑÂ§çÊùÇÂú∫ÊôØÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ∫îÂØπÂÆûÈôÖÊåëÊàò„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºömmWalkÊï∞ÊçÆÈõÜÂåÖÂê´120Êù°ÊâãÂä®ÊéßÂà∂ÁöÑË°åËµ∞ËΩ®ËøπÔºåÊØèÊù°ËΩ®ËøπÂåÖÂê´ÂêåÊ≠•ÁöÑRGB„ÄÅÊ∑±Â∫¶ÂíåËØ≠‰πâÂõæÂÉè„ÄÇÊ≠§Â§ñÔºåËøòÊûÑÂª∫‰∫ÜmmWalkVQAÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®ËßÜËßâÈóÆÁ≠îÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÈááÈõÜ„ÄÅÂú∫ÊôØÂàÜÁ±ª„ÄÅÊï∞ÊçÆÊ†áÊ≥®ÔºàÂåÖÊã¨ËØ≠‰πâÂàÜÂâ≤ÂíåVQAÊ†áÊ≥®Ôºâ‰ª•ÂèäÊ®°ÂûãËÆ≠ÁªÉÂíåËØÑ‰º∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊï∞ÊçÆÈõÜÁöÑËÆæËÆ°ÔºåÂÆÉ‰∏ç‰ªÖÂåÖÂê´Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåËøòÁâπÂà´ÂÖ≥Ê≥®‰∫ÜBLVÁî®Êà∑ÁöÑÈúÄÊ±ÇÔºå‰æãÂ¶ÇÔºåÂåÖÂê´‰∫ÜÂèØËÆøÈóÆÊÄßÁâπÂÆöÂú∞Ê†áÂíåÊà∑Â§ñÊûÅÁ´ØÊÉÖÂÜµ„ÄÇÊ≠§Â§ñÔºåmmWalkVQAÂü∫ÂáÜÁöÑÊûÑÂª∫‰πü‰∏∫ËØÑ‰º∞Ê®°ÂûãÂú®Ê≠•Ë°åËæÖÂä©ÊñπÈù¢ÁöÑËÉΩÂäõÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂπ≥Âè∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊï∞ÊçÆÈõÜÂåÖÂê´62kÂêåÊ≠•Â∏ßÔºåË∂ÖËøá559kÂº†ÂÖ®ÊôØÂõæÂÉè„ÄÇmmWalkVQAÂåÖÂê´Ë∂ÖËøá69k‰∏™ËßÜËßâÈóÆÁ≠î‰∏âÂÖÉÁªÑÔºåÊ∂µÁõñ9‰∏™Á±ªÂà´Ôºå‰æãÂ¶Ç‚ÄúÊòØÂê¶Â≠òÂú®ÈöúÁ¢çÁâ©Ôºü‚Äù„ÄÅ‚ÄúË°å‰∫∫ÈÅìÊòØÂê¶ÁïÖÈÄöÔºü‚ÄùÁ≠â„ÄÇÂú®Ê®°ÂûãËÆ≠ÁªÉÊñπÈù¢Ôºå‰ΩøÁî®‰∫ÜÊ†áÂáÜÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåÂπ∂Âú®mmWalkÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂæÆË∞É„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®mmWalkÊï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÈ™åËØÅ‰∫ÜmmWalkÊï∞ÊçÆÈõÜÁöÑÊúâÊïàÊÄß„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂú®È£éÈô©ËØÑ‰º∞ÂíåÂØºËà™‰ªªÂä°‰∏≠ÔºåÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãÁõ∏ÊØî‰∫éÈõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÊñπÊ≥ïÔºåÂèñÂæó‰∫ÜÊòéÊòæÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåmmWalkÊï∞ÊçÆÈõÜËÉΩÂ§üÊúâÊïàÂú∞ÊèêÂçáÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÊ≠•Ë°åËæÖÂä©ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂºÄÂèëÊõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÂÆâÂÖ®ÁöÑÊ≠•Ë°åËæÖÂä©Á≥ªÁªüÔºåÂ∏ÆÂä©Áõ≤‰∫∫Êàñ‰ΩéËßÜÂäõ‰∫∫Áæ§Âú®Êà∑Â§ñÁéØÂ¢É‰∏≠Áã¨Á´ãË°åËµ∞„ÄÇÈÄöËøáÁªìÂêàÂ§öÊ®°ÊÄÅ‰º†ÊÑüÂô®‰ø°ÊÅØÂíåÂèØËÆøÈóÆÊÄßÁâπÂæÅÔºåÂèØ‰ª•ÊèêÈ´òÊ≠•Ë°åËæÖÂä©Á≥ªÁªüÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÂíåÈ£éÈô©ËØÑ‰º∞ËÉΩÂäõÔºå‰ªéËÄåÊèêÂçáBLVÁî®Êà∑ÁöÑÂá∫Ë°å‰ΩìÈ™åÂíåÁîüÊ¥ªË¥®Èáè„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñËæÖÂä©Êú∫Âô®‰∫∫È¢ÜÂüüÔºå‰æãÂ¶ÇËÄÅÂπ¥‰∫∫ËæÖÂä©ÂíåÊÆãÁñæ‰∫∫ËæÖÂä©„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Walking assistance in extreme or complex environments remains a significant challenge for people with blindness or low vision (BLV), largely due to the lack of a holistic scene understanding. Motivated by the real-world needs of the BLV community, we build mmWalk, a simulated multi-modal dataset that integrates multi-view sensor and accessibility-oriented features for outdoor safe navigation. Our dataset comprises 120 manually controlled, scenario-categorized walking trajectories with 62k synchronized frames. It contains over 559k panoramic images across RGB, depth, and semantic modalities. Furthermore, to emphasize real-world relevance, each trajectory involves outdoor corner cases and accessibility-specific landmarks for BLV users. Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual question-answer triplets across 9 categories tailored for safe and informed walking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs) using zero- and few-shot settings and found they struggle with our risk assessment and navigational tasks. We validate our mmWalk-finetuned model on real-world datasets and show the effectiveness of our dataset for advancing multi-modal walking assistance.

