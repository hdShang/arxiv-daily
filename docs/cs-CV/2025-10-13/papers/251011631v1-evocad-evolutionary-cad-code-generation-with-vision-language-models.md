---
layout: default
title: EvoCAD: Evolutionary CAD Code Generation with Vision Language Models
---

# EvoCAD: Evolutionary CAD Code Generation with Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11631" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11631v1</a>
  <a href="https://arxiv.org/pdf/2510.11631.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11631v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11631v1', 'EvoCAD: Evolutionary CAD Code Generation with Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tobias Preintner, Weixuan Yuan, Adrian KÃ¶nig, Thomas BÃ¤ck, Elena Raponi, Niki van Stein

**åˆ†ç±»**: cs.CV, cs.AI, cs.NE

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**å¤‡æ³¨**: Accepted to IEEE ICTAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EvoCADï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸è¿›åŒ–ç®—æ³•ç”ŸæˆCADä»£ç **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `CADæ¨¡å‹ç”Ÿæˆ` `è§†è§‰è¯­è¨€æ¨¡å‹` `è¿›åŒ–ç®—æ³•` `æ‹“æ‰‘ç»“æ„è¯„ä¼°` `3Då¯¹è±¡` `ç¬¦å·è¡¨ç¤º` `GPT-4V` `GPT-4o`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰CADå¯¹è±¡ç”Ÿæˆæ–¹æ³•åœ¨æ‹“æ‰‘ç»“æ„æ­£ç¡®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆå¯¹è±¡çš„è¯­ä¹‰åˆç†æ€§ã€‚
2. EvoCADç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œè¿›åŒ–ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–CADå¯¹è±¡çš„ç¬¦å·è¡¨ç¤ºï¼Œæå‡ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEvoCADåœ¨CADPromptæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ‹“æ‰‘ç»“æ„æ­£ç¡®æ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºEvoCADï¼Œä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œè¿›åŒ–è®¡ç®—ç®—æ³•ï¼Œé€šè¿‡ç¬¦å·è¡¨ç¤ºç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡(CAD)å¯¹è±¡çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆé‡‡æ ·å¤šä¸ªCADå¯¹è±¡ï¼Œç„¶ååˆ©ç”¨è§†è§‰è¯­è¨€å’Œæ¨ç†è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è¿›åŒ–ç®—æ³•è¿›è¡Œä¼˜åŒ–ã€‚ä½¿ç”¨GPT-4Vå’ŒGPT-4oåœ¨CADPromptåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°EvoCADï¼Œå¹¶ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸¤ä¸ªåŸºäºæ¬§æ‹‰ç‰¹å¾å®šä¹‰çš„æ‹“æ‰‘å±æ€§çš„æ–°æŒ‡æ ‡ï¼Œç”¨äºæ•æ‰3Då¯¹è±¡ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEvoCADåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆæ‹“æ‰‘ç»“æ„æ­£ç¡®çš„å¯¹è±¡æ–¹é¢ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æˆ‘ä»¬æå‡ºçš„ä¸¤ä¸ªæ–°æŒ‡æ ‡è¿›è¡Œæœ‰æ•ˆè¯„ä¼°ï¼Œä»è€Œè¡¥å……ç°æœ‰çš„ç©ºé—´æŒ‡æ ‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰CADå¯¹è±¡ç”Ÿæˆæ–¹æ³•éš¾ä»¥ä¿è¯ç”Ÿæˆå¯¹è±¡çš„æ‹“æ‰‘ç»“æ„æ­£ç¡®æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„3Dæ¨¡å‹å¯èƒ½åœ¨è¯­ä¹‰ä¸Šä¸åˆç†ï¼Œä¾‹å¦‚å‡ºç°ä¸è¿é€šæˆ–å­”æ´ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œç¼ºä¹æœ‰æ•ˆçš„æ‹“æ‰‘ç»“æ„è¯„ä¼°æŒ‡æ ‡ï¼Œéš¾ä»¥å‡†ç¡®è¡¡é‡ç”Ÿæˆæ¨¡å‹çš„è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEvoCADçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆèƒ½åŠ›å’Œè¿›åŒ–ç®—æ³•çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œé€šè¿‡è¿­ä»£æ”¹è¿›CADå¯¹è±¡çš„ç¬¦å·è¡¨ç¤ºï¼Œä½¿å…¶åœ¨è§†è§‰å’Œè¯­ä¹‰ä¸Šæ›´ç¬¦åˆç›®æ ‡ã€‚é€šè¿‡è¿›åŒ–ç®—æ³•ï¼Œé€‰æ‹©å’Œå˜å¼‚CADå¯¹è±¡ï¼Œå¹¶ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°å…¶è´¨é‡ï¼Œä»è€Œé€æ­¥ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEvoCADçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) åˆå§‹åŒ–ï¼šéšæœºç”Ÿæˆå¤šä¸ªCADå¯¹è±¡çš„ç¬¦å·è¡¨ç¤ºã€‚2) è¯„ä¼°ï¼šä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4Væˆ–GPT-4oï¼‰è¯„ä¼°æ¯ä¸ªCADå¯¹è±¡çš„è´¨é‡ï¼Œå¹¶ç»“åˆæ¨ç†è¯­è¨€æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥çš„è¯­ä¹‰è¯„ä¼°ã€‚3) é€‰æ‹©ï¼šæ ¹æ®è¯„ä¼°ç»“æœï¼Œé€‰æ‹©è¡¨ç°æœ€å¥½çš„CADå¯¹è±¡ä½œä¸ºçˆ¶ä»£ã€‚4) å˜å¼‚ï¼šå¯¹çˆ¶ä»£CADå¯¹è±¡çš„ç¬¦å·è¡¨ç¤ºè¿›è¡Œå˜å¼‚ï¼Œç”Ÿæˆæ–°çš„CADå¯¹è±¡ã€‚5) è¿­ä»£ï¼šé‡å¤è¯„ä¼°ã€é€‰æ‹©å’Œå˜å¼‚è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è¿­ä»£æ¬¡æ•°æˆ–æ»¡è¶³æ”¶æ•›æ¡ä»¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šEvoCADçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œè¿›åŒ–ç®—æ³•ï¼Œå®ç°äº†CADå¯¹è±¡çš„è¿­ä»£ä¼˜åŒ–ã€‚2) æå‡ºäº†åŸºäºæ¬§æ‹‰ç‰¹å¾çš„æ‹“æ‰‘ç»“æ„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¡¡é‡ç”Ÿæˆå¯¹è±¡çš„æ‹“æ‰‘æ­£ç¡®æ€§ã€‚3) åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒCADå¯¹è±¡çš„è´¨é‡è¯„ä¼°ï¼Œèƒ½å¤Ÿæ•æ‰æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šEvoCADçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨ç¬¦å·è¡¨ç¤ºæ¥æè¿°CADå¯¹è±¡ï¼Œä¾¿äºè¯­è¨€æ¨¡å‹å¤„ç†å’Œè¿›åŒ–ç®—æ³•æ“ä½œã€‚2) ä½¿ç”¨GPT-4Væˆ–GPT-4oç­‰è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒCADå¯¹è±¡çš„è§†è§‰è´¨é‡è¯„ä¼°ï¼Œå¹¶ç»“åˆæ¨ç†è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥ã€‚3) å®šä¹‰äº†åŸºäºæ¬§æ‹‰ç‰¹å¾çš„æ‹“æ‰‘ç»“æ„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬è¿é€šåˆ†é‡æ•°å’Œå­”æ´æ•°ï¼Œç”¨äºè¡¡é‡ç”Ÿæˆå¯¹è±¡çš„æ‹“æ‰‘æ­£ç¡®æ€§ã€‚4) è¿›åŒ–ç®—æ³•é‡‡ç”¨é€‰æ‹©ã€äº¤å‰å’Œå˜å¼‚ç­‰æ“ä½œï¼Œä»¥ä¼˜åŒ–CADå¯¹è±¡çš„ç¬¦å·è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EvoCADåœ¨CADPromptæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ‹“æ‰‘ç»“æ„æ­£ç¡®æ€§æ–¹é¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEvoCADåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”é€šè¿‡æå‡ºçš„åŸºäºæ¬§æ‹‰ç‰¹å¾çš„æ–°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„æ‹“æ‰‘è´¨é‡ã€‚ä¾‹å¦‚ï¼ŒEvoCADåœ¨ç”Ÿæˆæ‹“æ‰‘ç»“æ„æ­£ç¡®çš„å¯¹è±¡æ–¹é¢ï¼Œç›¸æ¯”äºåŸºçº¿æ–¹æ³•æå‡äº†XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EvoCADå¯åº”ç”¨äºè‡ªåŠ¨åŒ–CADæ¨¡å‹è®¾è®¡ã€å®šåˆ¶åŒ–äº§å“ç”Ÿæˆã€æ¸¸æˆèµ„äº§åˆ›å»ºç­‰é¢†åŸŸã€‚é€šè¿‡ç»“åˆè‡ªç„¶è¯­è¨€æè¿°å’Œè§†è§‰ä¿¡æ¯ï¼ŒEvoCADèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·å¿«é€Ÿç”Ÿæˆæ»¡è¶³ç‰¹å®šéœ€æ±‚çš„3Dæ¨¡å‹ï¼Œé™ä½è®¾è®¡é—¨æ§›ï¼Œæé«˜è®¾è®¡æ•ˆç‡ã€‚æœªæ¥ï¼ŒEvoCADæœ‰æœ›ä¸è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰æŠ€æœ¯ç»“åˆï¼Œå®ç°æ›´åŠ æ™ºèƒ½å’Œäº¤äº’å¼çš„è®¾è®¡ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Combining large language models with evolutionary computation algorithms represents a promising research direction leveraging the remarkable generative and in-context learning capabilities of LLMs with the strengths of evolutionary algorithms. In this work, we present EvoCAD, a method for generating computer-aided design (CAD) objects through their symbolic representations using vision language models and evolutionary optimization. Our method samples multiple CAD objects, which are then optimized using an evolutionary approach with vision language and reasoning language models. We assess our method using GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and comparing it to prior methods. Additionally, we introduce two new metrics based on topological properties defined by the Euler characteristic, which capture a form of semantic similarity between 3D objects. Our results demonstrate that EvoCAD outperforms previous approaches on multiple metrics, particularly in generating topologically correct objects, which can be efficiently evaluated using our two novel metrics that complement existing spatial metrics.

