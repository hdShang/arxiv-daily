---
layout: default
title: FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model
---

# FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10921" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10921v2</a>
  <a href="https://arxiv.org/pdf/2510.10921.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10921v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10921v2', 'FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Ji Ao, Dawei Leng, Yuhui Yin

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13 (æ›´æ–°: 2025-10-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFG-CLIP 2ï¼Œç”¨äºæå‡è‹±æ±‰åŒè¯­ç¯å¢ƒä¸‹çš„ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½` `åŒè¯­æ¨¡å‹` `åŒºåŸŸ-æ–‡æœ¬åŒ¹é…` `é•¿æ–‡æœ¬å»ºæ¨¡` `å¯¹æ¯”å­¦ä¹ ` `ä¸­æ–‡å¤šæ¨¡æ€ç†è§£` `TICæŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦ç†è§£ä¸Šä¸è¶³ï¼Œå°¤å…¶åœ¨éè‹±è¯­ç¯å¢ƒä¸‹ï¼Œéš¾ä»¥ç²¾ç¡®å¯¹é½å›¾åƒç»†èŠ‚ä¸æ–‡æœ¬æè¿°ã€‚
2. FG-CLIP 2åˆ©ç”¨åŒºåŸŸ-æ–‡æœ¬åŒ¹é…ã€é•¿æ–‡æœ¬å»ºæ¨¡ç­‰ç»†ç²’åº¦ç›‘ç£ï¼Œå¹¶å¼•å…¥TICæŸå¤±åŒºåˆ†ç›¸ä¼¼æ–‡æœ¬ï¼Œæå‡åŒè¯­å¯¹é½èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFG-CLIP 2åœ¨å¤šç§ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ–°æå‡ºçš„ä¸­æ–‡å¤šæ¨¡æ€åŸºå‡†ä¸Šå–å¾—é¢†å…ˆæˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰-è¯­è¨€ç†è§£æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éè‹±è¯­ç¯å¢ƒä¸­ï¼Œéš¾ä»¥ç²¾ç¡®å¯¹é½è§†è§‰å†…å®¹å’Œè¯­è¨€æè¿°ã€‚å°½ç®¡CLIPç­‰æ¨¡å‹åœ¨å…¨å±€å¯¹é½æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ•æ‰å¯¹è±¡å±æ€§ã€ç©ºé—´å…³ç³»å’Œè¯­è¨€è¡¨è¾¾çš„ç»†ç²’åº¦ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸”å¯¹åŒè¯­ç†è§£çš„æ”¯æŒæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FG-CLIP 2ï¼Œä¸€ç§æ—¨åœ¨æå‡è‹±è¯­å’Œæ±‰è¯­ç»†ç²’åº¦å¯¹é½çš„åŒè¯­è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸°å¯Œçš„ç»†ç²’åº¦ç›‘ç£ï¼ŒåŒ…æ‹¬åŒºåŸŸ-æ–‡æœ¬åŒ¹é…å’Œé•¿æ–‡æœ¬å»ºæ¨¡ï¼Œä»¥åŠå¤šä¸ªåˆ¤åˆ«æ€§ç›®æ ‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†æ–‡æœ¬å†…æ¨¡æ€å¯¹æ¯”ï¼ˆTICï¼‰æŸå¤±ï¼Œä»¥æ›´å¥½åœ°åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬æè¿°ã€‚FG-CLIP 2åœ¨ç²¾å¿ƒç­–åˆ’çš„å¤§è§„æ¨¡è‹±è¯­å’Œæ±‰è¯­æ··åˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†å¼ºå¤§çš„åŒè¯­æ€§èƒ½ã€‚ä¸ºäº†è¿›è¡Œä¸¥æ ¼çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä¸­æ–‡å¤šæ¨¡æ€ç†è§£åŸºå‡†ï¼ŒåŒ…æ‹¬é•¿æ–‡æœ¬æ£€ç´¢å’Œè¾¹ç•Œæ¡†åˆ†ç±»ã€‚åœ¨8ä¸ªä»»åŠ¡çš„29ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFG-CLIP 2ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ä¸¤ç§è¯­è¨€ä¸­éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹ã€ä»£ç å’ŒåŸºå‡†ï¼Œä»¥ä¿ƒè¿›æœªæ¥å¯¹åŒè¯­ç»†ç²’åº¦å¯¹é½çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¦‚CLIPï¼Œåœ¨å…¨å±€å±‚é¢å¯¹é½å›¾åƒå’Œæ–‡æœ¬è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç»†ç²’åº¦å±‚é¢ï¼Œä¾‹å¦‚å¯¹è±¡å±æ€§ã€ç©ºé—´å…³ç³»ä»¥åŠæ›´å¤æ‚çš„è¯­è¨€è¡¨è¾¾ä¸Šï¼Œå¯¹é½æ•ˆæœä¸ä½³ã€‚å°¤å…¶æ˜¯åœ¨éè‹±è¯­è¯­å¢ƒä¸‹ï¼Œç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä¸­æ–‡ç­‰è¯­è¨€ä¸Šçš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬æè¿°ï¼Œå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨å¾åŒºåˆ†æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFG-CLIP 2çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ›´ä¸°å¯Œçš„ç»†ç²’åº¦ç›‘ç£ä¿¡æ¯ï¼Œä»¥åŠä¸“é—¨è®¾è®¡çš„æŸå¤±å‡½æ•°ï¼Œæ¥æå‡æ¨¡å‹åœ¨è§†è§‰å’Œè¯­è¨€ç‰¹å¾ç©ºé—´ä¸­çš„å¯¹é½èƒ½åŠ›ã€‚é€šè¿‡åŒºåŸŸ-æ–‡æœ¬åŒ¹é…ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å›¾åƒå±€éƒ¨åŒºåŸŸä¸å¯¹åº”æ–‡æœ¬æè¿°ä¹‹é—´çš„å…³ç³»ã€‚é•¿æ–‡æœ¬å»ºæ¨¡åˆ™å¸®åŠ©æ¨¡å‹ç†è§£æ›´å¤æ‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚TICæŸå¤±åˆ™æ—¨åœ¨å¢å¼ºæ¨¡å‹åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼æ–‡æœ¬çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFG-CLIP 2çš„æ•´ä½“æ¡†æ¶åŸºäºCLIPæ¨¡å‹ï¼Œä½†è¿›è¡Œäº†æ”¹è¿›ä»¥é€‚åº”ç»†ç²’åº¦å¯¹é½çš„éœ€æ±‚ã€‚ä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼šå›¾åƒç¼–ç å™¨ï¼ˆç”¨äºæå–å›¾åƒç‰¹å¾ï¼‰ã€æ–‡æœ¬ç¼–ç å™¨ï¼ˆç”¨äºæå–æ–‡æœ¬ç‰¹å¾ï¼‰ã€åŒºåŸŸ-æ–‡æœ¬åŒ¹é…æ¨¡å—ï¼ˆç”¨äºå¯¹é½å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬ç‰‡æ®µï¼‰ã€é•¿æ–‡æœ¬å»ºæ¨¡æ¨¡å—ï¼ˆç”¨äºå¤„ç†é•¿æ–‡æœ¬æè¿°ï¼‰ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›®æ ‡å‡½æ•°ï¼ŒåŒ…æ‹¬CLIPæŸå¤±ã€åŒºåŸŸ-æ–‡æœ¬åŒ¹é…æŸå¤±ã€é•¿æ–‡æœ¬å»ºæ¨¡æŸå¤±å’ŒTICæŸå¤±ã€‚

**å…³é”®åˆ›æ–°**ï¼šFG-CLIP 2çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) å¼•å…¥äº†ä¸°å¯Œçš„ç»†ç²’åº¦ç›‘ç£ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŒºåŸŸ-æ–‡æœ¬åŒ¹é…å’Œé•¿æ–‡æœ¬å»ºæ¨¡ã€‚2) æå‡ºäº†TICæŸå¤±ï¼Œç”¨äºå¢å¼ºæ¨¡å‹åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼æ–‡æœ¬çš„èƒ½åŠ›ã€‚3) æ„å»ºäº†ä¸€ä¸ªæ–°çš„ä¸­æ–‡å¤šæ¨¡æ€ç†è§£åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸­æ–‡ç¯å¢ƒä¸‹çš„ç»†ç²’åº¦å¯¹é½èƒ½åŠ›ã€‚4) æ¨¡å‹åœ¨è‹±è¯­å’Œä¸­æ–‡ä¸¤ç§è¯­è¨€ä¸Šéƒ½å–å¾—äº†state-of-the-artçš„ç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åŒºåŸŸ-æ–‡æœ¬åŒ¹é…æ–¹é¢ï¼Œä½¿ç”¨äº†é¢„è®­ç»ƒçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹æ¥æå–å›¾åƒåŒºåŸŸï¼Œå¹¶ä½¿ç”¨Transformerç½‘ç»œæ¥å­¦ä¹ åŒºåŸŸç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚åœ¨é•¿æ–‡æœ¬å»ºæ¨¡æ–¹é¢ï¼Œä½¿ç”¨äº†Transformer-XLæ¨¡å‹æ¥å¤„ç†é•¿æ–‡æœ¬ä¾èµ–å…³ç³»ã€‚TICæŸå¤±çš„å…·ä½“å½¢å¼ä¸ºå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œæ—¨åœ¨æ‹‰è¿‘åŒä¸€å›¾åƒçš„ä¸åŒæ–‡æœ¬æè¿°ä¹‹é—´çš„è·ç¦»ï¼ŒåŒæ—¶æ¨è¿œä¸åŒå›¾åƒçš„æ–‡æœ¬æè¿°ä¹‹é—´çš„è·ç¦»ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œéœ€è¦å‚è€ƒè®ºæ–‡å…¨æ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FG-CLIP 2åœ¨29ä¸ªæ•°æ®é›†ä¸Šçš„8ä¸ªä»»åŠ¡ä¸­å–å¾—äº†state-of-the-artçš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨æ–°æå‡ºçš„ä¸­æ–‡å¤šæ¨¡æ€ç†è§£åŸºå‡†ä¸Šï¼ŒFG-CLIP 2æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸­æ–‡ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FG-CLIP 2åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å›¾åƒæ£€ç´¢ã€è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ç”Ÿæˆã€ç»†ç²’åº¦å›¾åƒåˆ†ç±»ç­‰ã€‚è¯¥æ¨¡å‹å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¢æœã€ç”µå•†æ¨èã€æ•™è‚²è¾…åŠ©ç­‰åœºæ™¯ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œå·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æ¨¡å‹å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤šè¯­è¨€å’Œæ¨¡æ€ï¼Œä¾‹å¦‚è§†é¢‘ã€è¯­éŸ³ç­‰ï¼Œå®ç°æ›´å…¨é¢çš„å¤šæ¨¡æ€ç†è§£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, a capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, a bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. Our approach leverages rich fine-grained supervision, including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance. To enable rigorous evaluation, we present a new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages. We release the model, code, and benchmark to facilitate future research on bilingual fine-grained alignment.

