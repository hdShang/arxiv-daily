---
layout: default
title: AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model
---

# AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11496" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11496v2</a>
  <a href="https://arxiv.org/pdf/2510.11496.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11496v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11496v2', 'AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, Haonan Lu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13 (æ›´æ–°: 2025-10-14)

**å¤‡æ³¨**: Tech report of OPPO AndesVL Team

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/OPPOer)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AndesVLï¼šé¢å‘ç§»åŠ¨ç«¯çš„é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç§»åŠ¨ç«¯éƒ¨ç½²` `æ¨¡å‹å‹ç¼©` `é‡åŒ–æ„ŸçŸ¥å¾®è°ƒ` `ç¼“å­˜æ·˜æ±°ç®—æ³•` `æ¨æµ‹è§£ç ` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰äº‘ç«¯MLLMå‚æ•°é‡å·¨å¤§ï¼Œè¾¹ç¼˜è®¾å¤‡åœ¨å†…å­˜ã€åŠŸè€—å’Œç®—åŠ›ä¸Šéš¾ä»¥æ»¡è¶³éœ€æ±‚ã€‚
2. AndesVLåŸºäºQwen3ï¼Œé€šè¿‡ä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹ï¼Œæ„å»ºäº†å‚æ•°é‡ä¸º0.6B-4Bçš„ç§»åŠ¨ç«¯MLLMã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAndesVLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸€æµæ°´å¹³ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–ç­–ç•¥æ˜¾è‘—æå‡äº†ç§»åŠ¨ç«¯éƒ¨ç½²æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†AndesVLï¼Œä¸€å¥—åŸºäºQwen3çš„LLMå’Œå¤šç§è§†è§‰ç¼–ç å™¨çš„ç§»åŠ¨ç«¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä»0.6Båˆ°4Bã€‚è®ºæ–‡å…¨é¢æ¦‚è¿°äº†AndesVLçš„æ¨¡å‹æ¶æ„ã€è®­ç»ƒæµç¨‹å’Œè®­ç»ƒæ•°æ®ã€‚ä¸åŒç­‰è§„æ¨¡çš„SOTAæ¨¡å‹ç›¸æ¯”ï¼ŒAndesVLåœ¨ä¸€ç³»åˆ—å¼€æºåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œæ¶µç›–äº†å¯Œæ–‡æœ¬å›¾åƒç†è§£ã€æ¨ç†ä¸æ•°å­¦ã€å¤šå›¾åƒç†è§£ã€é€šç”¨VQAã€å¹»è§‰ç¼“è§£ã€å¤šè¯­è¨€ç†è§£å’ŒGUIç›¸å…³ä»»åŠ¡ç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§1+N LoRAæ¶æ„ä»¥åŠé‡åŒ–æ„ŸçŸ¥LoRAå¾®è°ƒï¼ˆQALFTï¼‰æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›AndesVLåœ¨ç§»åŠ¨ç«¯éƒ¨ç½²æœŸé—´çš„æœ‰æ•ˆä»»åŠ¡é€‚é…å’Œæ¨¡å‹å‹ç¼©ã€‚è€Œä¸”ï¼Œé€šè¿‡ä½¿ç”¨ç¼“å­˜æ·˜æ±°ç®—æ³•OKVä»¥åŠå®šåˆ¶çš„æ¨æµ‹è§£ç å’Œå‹ç¼©ç­–ç•¥ï¼Œåœ¨è”å‘ç§‘å¤©ç‘9500èŠ¯ç‰‡ä¸Šéƒ¨ç½²AndesVL-4Bæ—¶ï¼Œå®ç°äº†6.7å€çš„å³°å€¼è§£ç é€Ÿåº¦æå‡ï¼Œé«˜è¾¾30.9%çš„å†…å­˜å‡å°‘ä»¥åŠ1.8 bits-per-weightçš„æ¨¡å‹å‹ç¼©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¦‚QwenVLã€InternVLã€GPT-4oç­‰ï¼Œè™½ç„¶åœ¨äº‘ç«¯è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºæ¨¡å‹è§„æ¨¡åºå¤§ï¼ˆæ•°ç™¾äº¿å‚æ•°ï¼‰ï¼Œå¯¹ç§»åŠ¨ç«¯ç­‰è¾¹ç¼˜è®¾å¤‡çš„å†…å­˜ã€åŠŸè€—å’Œè®¡ç®—èƒ½åŠ›æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ç§»åŠ¨ç«¯éƒ¨ç½²é«˜æ€§èƒ½çš„MLLMæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAndesVLçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå‚æ•°è§„æ¨¡é€‚ä¸­ï¼ˆ0.6B-4Bï¼‰ï¼Œä½†æ€§èƒ½æ¥è¿‘ç”šè‡³è¶…è¿‡åŒç­‰è§„æ¨¡SOTAæ¨¡å‹çš„MLLMã€‚é€šè¿‡æ¨¡å‹æ¶æ„ä¼˜åŒ–ã€é«˜æ•ˆçš„è®­ç»ƒæµç¨‹å’Œæ•°æ®ï¼Œä»¥åŠé’ˆå¯¹ç§»åŠ¨ç«¯éƒ¨ç½²çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAndesVLçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) åŸºäºQwen3çš„LLMä½œä¸ºè¯­è¨€æ¨¡å‹çš„åŸºç¡€ï¼›2) å¤šç§è§†è§‰ç¼–ç å™¨ç”¨äºå¤„ç†å›¾åƒè¾“å…¥ï¼›3) 1+N LoRAæ¶æ„å’ŒQALFTæ¡†æ¶ç”¨äºä»»åŠ¡é€‚é…å’Œæ¨¡å‹å‹ç¼©ï¼›4) OKVç¼“å­˜æ·˜æ±°ç®—æ³•ã€æ¨æµ‹è§£ç å’Œå‹ç¼©ç­–ç•¥ç”¨äºåŠ é€Ÿæ¨ç†å’Œå‡å°‘å†…å­˜å ç”¨ã€‚æ•´ä½“æµç¨‹æ˜¯ä»æ•°æ®å‡†å¤‡å’Œæ¨¡å‹è®­ç»ƒå¼€å§‹ï¼Œç„¶åè¿›è¡Œä»»åŠ¡å¾®è°ƒå’Œæ¨¡å‹å‹ç¼©ï¼Œæœ€åéƒ¨ç½²åˆ°ç§»åŠ¨ç«¯è®¾å¤‡ä¸Šã€‚

**å…³é”®åˆ›æ–°**ï¼šAndesVLçš„å‡ ä¸ªå…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š1) é’ˆå¯¹ç§»åŠ¨ç«¯è®¾è®¡çš„è½»é‡çº§æ¨¡å‹æ¶æ„ï¼›2) é‡åŒ–æ„ŸçŸ¥LoRAå¾®è°ƒï¼ˆQALFTï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨¡å‹å‹ç¼©çš„åŒæ—¶ä¿æŒæ€§èƒ½ï¼›3) OKVç¼“å­˜æ·˜æ±°ç®—æ³•ï¼Œæœ‰æ•ˆç®¡ç†ç§»åŠ¨ç«¯æœ‰é™çš„å†…å­˜èµ„æºï¼›4) å®šåˆ¶çš„æ¨æµ‹è§£ç ç­–ç•¥ï¼ŒåŠ é€Ÿæ¨¡å‹æ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼ŒAndesVLé€‰æ‹©äº†Qwen3ä½œä¸ºLLMçš„åŸºç¡€ï¼Œå¹¶æ¢ç´¢äº†ä¸åŒçš„è§†è§‰ç¼–ç å™¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯å’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚QALFTæ¡†æ¶çš„å…³é”®åœ¨äºåœ¨é‡åŒ–è¿‡ç¨‹ä¸­å¼•å…¥LoRAæ¨¡å—ï¼Œå¹¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŒæ—¶ä¼˜åŒ–é‡åŒ–å‚æ•°å’ŒLoRAå‚æ•°ã€‚OKVç®—æ³•çš„å…³é”®åœ¨äºæ ¹æ®tokençš„ä½¿ç”¨é¢‘ç‡å’Œé‡è¦æ€§è¿›è¡Œç¼“å­˜ç®¡ç†ï¼Œä¼˜å…ˆä¿ç•™é‡è¦çš„tokenã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AndesVLåœ¨å¤šä¸ªå¼€æºåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œä¸åŒç­‰è§„æ¨¡çš„SOTAæ¨¡å‹ç›¸æ¯”ï¼Œåœ¨æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒç†è§£ã€æ¨ç†å’Œæ•°å­¦ã€å¤šå›¾åƒç†è§£ã€é€šç”¨VQAã€å¹»è§‰ç¼“è§£ã€å¤šè¯­è¨€ç†è§£å’ŒGUIç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨è”å‘ç§‘å¤©ç‘9500èŠ¯ç‰‡ä¸Šéƒ¨ç½²AndesVL-4Bæ—¶ï¼Œå®ç°äº†6.7å€çš„å³°å€¼è§£ç é€Ÿåº¦æå‡ï¼Œé«˜è¾¾30.9%çš„å†…å­˜å‡å°‘ä»¥åŠ1.8 bits-per-weightçš„æ¨¡å‹å‹ç¼©ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AndesVLå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ç§»åŠ¨ç«¯çš„æ™ºèƒ½åŠ©æ‰‹ã€å›¾åƒæœç´¢ã€è§†è§‰é—®ç­”ã€æ–‡æ¡£ç†è§£ã€GUIè‡ªåŠ¨åŒ–ç­‰ã€‚å®ƒå¯ä»¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå®ç°é«˜æ€§èƒ½çš„å¤šæ¨¡æ€äº¤äº’ï¼Œä¸ºç”¨æˆ·æä¾›æ›´æ™ºèƒ½ã€æ›´ä¾¿æ·çš„æœåŠ¡ã€‚æœªæ¥ï¼ŒAndesVLæœ‰æœ›æ¨åŠ¨è¾¹ç¼˜è®¡ç®—å’Œäººå·¥æ™ºèƒ½çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.

