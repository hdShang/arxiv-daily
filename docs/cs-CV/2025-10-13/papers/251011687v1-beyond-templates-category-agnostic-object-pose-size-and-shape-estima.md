---
layout: default
title: Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View
---

# Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11687" target="_blank" class="toolbar-btn">arXiv: 2510.11687v1</a>
    <a href="https://arxiv.org/pdf/2510.11687.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11687v1" 
            onclick="toggleFavorite(this, '2510.11687v1', 'Beyond \'Templates\': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÁ±ªÂà´Êó†ÂÖ≥ÁöÑÂçïËßÜÂõæÁâ©‰Ωì‰ΩçÂßø„ÄÅÂ∞∫ÂØ∏ÂíåÂΩ¢Áä∂‰º∞ËÆ°Ê°ÜÊû∂„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `6D‰ΩçÂßø‰º∞ËÆ°` `ÂΩ¢Áä∂‰º∞ËÆ°` `Á±ªÂà´Êó†ÂÖ≥` `Transformer` `Èõ∂Ê†∑Êú¨Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÁâ©‰ΩìÁâπÂÆöÂÖàÈ™åÊàñÂ≠òÂú®‰ΩçÂßø-ÂΩ¢Áä∂Á∫†Áº†ÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜË∑®Á±ªÂà´ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÁªü‰∏ÄÊ°ÜÊû∂ÔºåÊó†ÈúÄÊ®°ÊùøÊàñCADÊ®°ÂûãÔºåÂç≥ÂèØ‰ªéÂçïÂº†RGB-DÂõæÂÉè‰∏≠È¢ÑÊµã6D‰ΩçÂßø„ÄÅÂ∞∫ÂØ∏ÂíåÂΩ¢Áä∂„ÄÇ
3. Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÈ™åËØÅÔºåÂç≥‰ΩøÂú®Êú™ËßÅËøáÁöÑÁ±ªÂà´‰∏äÔºå‰πüË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑ„ÄÅÁ±ªÂà´Êó†ÂÖ≥ÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫é‰ªéÂçï‰∏™RGB-DÂõæÂÉè‰∏≠ÂêåÊó∂È¢ÑÊµãÁâ©‰ΩìÁöÑ6D‰ΩçÂßø„ÄÅÂ∞∫ÂØ∏ÂíåÂØÜÈõÜÂΩ¢Áä∂ÔºåËÄåÊó†ÈúÄÊ®°Êùø„ÄÅCADÊ®°ÂûãÊàñÊµãËØïÊó∂ÁöÑÁ±ªÂà´Ê†áÁ≠æ„ÄÇËØ•Ê®°ÂûãËûçÂêà‰∫ÜËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÂØÜÈõÜ2DÁâπÂæÅÂíåÈÉ®ÂàÜ3DÁÇπ‰∫ëÔºå‰ΩøÁî®TransformerÁºñÁ†ÅÂô®ÔºàÈÄöËøáÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÂ¢ûÂº∫ÔºâÔºåÂπ∂ÈááÁî®Âπ∂Ë°åËß£Á†ÅÂô®ËøõË°å‰ΩçÂßø-Â∞∫ÂØ∏‰º∞ËÆ°ÂíåÂΩ¢Áä∂ÈáçÂª∫ÔºåÂÆûÁé∞‰∫Ü28 FPSÁöÑÂÆûÊó∂Êé®ÁêÜÈÄüÂ∫¶„ÄÇËØ•Ê°ÜÊû∂‰ªÖÂú®SOPEÊï∞ÊçÆÈõÜ‰∏≠149‰∏™Á±ªÂà´ÁöÑÂêàÊàêÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºåÂπ∂Âú®SOPE„ÄÅROPE„ÄÅObjaversePoseÂíåHANDALÂõõ‰∏™‰∏çÂêåÁöÑÂü∫ÂáÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÊ∂µÁõñ‰∫Ü300Â§ö‰∏™Á±ªÂà´„ÄÇÂú®Â∑≤ËßÅÁ±ªÂà´‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁ≤æÂ∫¶ÔºåÂêåÊó∂ÂØπÊú™ËßÅÁúüÂÆû‰∏ñÁïåÁâ©‰ΩìË°®Áé∞Âá∫ÈùûÂ∏∏Âº∫ÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºå‰∏∫Êú∫Âô®‰∫∫ÂíåÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩ‰∏≠ÁöÑÂºÄÊîæÈõÜ6DÁêÜËß£Âª∫Á´ã‰∫ÜÊñ∞ÁöÑÊ†áÂáÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÂú®ËøõË°å6D‰ΩçÂßø„ÄÅÂ∞∫ÂØ∏ÂíåÂΩ¢Áä∂‰º∞ËÆ°Êó∂ÔºåÈÄöÂ∏∏‰æùËµñ‰∫éÁâ©‰ΩìÁâπÂÆöÁöÑCADÊ®°ÂûãÊàñÊ®°ÊùøÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂºÄÊîæÁéØÂ¢É‰∏ãÁöÑÂ∫îÁî®„ÄÇÊ≠§Â§ñÔºå‰∏Ä‰∫õÊñπÊ≥ïÂ≠òÂú®‰ΩçÂßøÂíåÂΩ¢Áä∂ÁöÑÁ∫†Áº†ÈóÆÈ¢òÔºå‰ª•ÂèäÂ§öÈò∂ÊÆµpipelineÂØºËá¥ÁöÑËØØÂ∑ÆÁ¥ØÁßØÔºåÈöæ‰ª•ÂÆûÁé∞È´òÊïàÂíåÂáÜÁ°ÆÁöÑ‰º∞ËÆ°„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂÆûÁé∞Á±ªÂà´Êó†ÂÖ≥ÁöÑ„ÄÅÈ´òÊïàÁöÑ„ÄÅÂáÜÁ°ÆÁöÑ6D‰ΩçÂßø„ÄÅÂ∞∫ÂØ∏ÂíåÂΩ¢Áä∂‰º∞ËÆ°ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÊèêÂèñÁöÑÈÄöÁî®2DÁâπÂæÅÔºåÂπ∂Â∞ÜÂÖ∂‰∏é3DÁÇπ‰∫ë‰ø°ÊÅØËûçÂêàÔºå‰ªéËÄåÂÆûÁé∞ÂØπÁâ©‰Ωì‰ΩçÂßø„ÄÅÂ∞∫ÂØ∏ÂíåÂΩ¢Áä∂ÁöÑËß£ËÄ¶‰º∞ËÆ°„ÄÇÈÄöËøáTransformerÊû∂ÊûÑÂ≠¶‰π†2DÂíå3DÁâπÂæÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂‰ΩøÁî®Âπ∂Ë°åËß£Á†ÅÂô®ÂàÜÂà´È¢ÑÊµã‰ΩçÂßø-Â∞∫ÂØ∏ÂíåÂΩ¢Áä∂ÔºåÈÅøÂÖç‰∫Ü‰ø°ÊÅØ‰πãÈó¥ÁöÑÁõ∏‰∫íÂπ≤Êâ∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê®°ÂùóÔºöÁâπÂæÅÊèêÂèñÊ®°Âùó„ÄÅTransformerÁºñÁ†ÅÂô®Ê®°ÂùóÂíåÂπ∂Ë°åËß£Á†ÅÂô®Ê®°Âùó„ÄÇÈ¶ñÂÖàÔºåÂà©Áî®ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÊèêÂèñRGB-DÂõæÂÉèÁöÑ2DÁâπÂæÅÔºåÂπ∂ÂØπ3DÁÇπ‰∫ëËøõË°åÂ§ÑÁêÜ„ÄÇÁÑ∂ÂêéÔºå‰ΩøÁî®TransformerÁºñÁ†ÅÂô®ËûçÂêà2DÂíå3DÁâπÂæÅÔºåÂπ∂ÈÄöËøáÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÂ¢ûÂº∫ÁâπÂæÅË°®ËææËÉΩÂäõ„ÄÇÊúÄÂêéÔºå‰ΩøÁî®‰∏§‰∏™Âπ∂Ë°åËß£Á†ÅÂô®Ôºå‰∏Ä‰∏™Áî®‰∫éÈ¢ÑÊµã6D‰ΩçÂßøÂíåÂ∞∫ÂØ∏ÔºåÂè¶‰∏Ä‰∏™Áî®‰∫éÈáçÂª∫Áâ©‰ΩìÁöÑÂØÜÈõÜÂΩ¢Áä∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂÖ∂Á±ªÂà´Êó†ÂÖ≥ÊÄßÔºåÂç≥Êó†ÈúÄÈíàÂØπÁâπÂÆöÁâ©‰ΩìËøõË°åËÆ≠ÁªÉÊàñÊèê‰æõCADÊ®°ÂûãÔºåÂç≥ÂèØÂÆûÁé∞ÂØπÊú™Áü•Áâ©‰ΩìÁöÑ6D‰ΩçÂßø„ÄÅÂ∞∫ÂØ∏ÂíåÂΩ¢Áä∂‰º∞ËÆ°„ÄÇÊ≠§Â§ñÔºåÈÄöËøáTransformerÊû∂ÊûÑÂíåÂπ∂Ë°åËß£Á†ÅÂô®ÁöÑËÆæËÆ°ÔºåÊúâÊïàÂú∞Ëß£ËÄ¶‰∫Ü‰ΩçÂßø„ÄÅÂ∞∫ÂØ∏ÂíåÂΩ¢Áä∂‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÊèêÈ´ò‰∫Ü‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöTransformerÁºñÁ†ÅÂô®ÈááÁî®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁî®‰∫éÂ≠¶‰π†2DÂíå3DÁâπÂæÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÁî®‰∫éÂ¢ûÂº∫ÁâπÂæÅË°®ËææËÉΩÂäõÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ‰ΩçÂßø-Â∞∫ÂØ∏Ëß£Á†ÅÂô®ÈááÁî®ÂõûÂΩíÁöÑÊñπÂºèÈ¢ÑÊµã6D‰ΩçÂßøÂèÇÊï∞ÂíåÂ∞∫ÂØ∏ÂèÇÊï∞„ÄÇÂΩ¢Áä∂Ëß£Á†ÅÂô®ÈááÁî®ÁÇπ‰∫ëÈáçÂª∫ÁöÑÊñπÂºèÈ¢ÑÊµãÁâ©‰ΩìÁöÑÂØÜÈõÜÂΩ¢Áä∂„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨‰ΩçÂßøÊçüÂ§±„ÄÅÂ∞∫ÂØ∏ÊçüÂ§±ÂíåÂΩ¢Áä∂ÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Ê°ÜÊû∂Âú®SOPE„ÄÅROPE„ÄÅObjaversePoseÂíåHANDALÂõõ‰∏™Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂú®Â∑≤ËßÅÁ±ªÂà´‰∏äÂèñÂæó‰∫Üstate-of-the-artÁöÑÁ≤æÂ∫¶„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåËØ•Ê°ÜÊû∂Âú®Êú™ËßÅÁ±ªÂà´‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®ObjaversePoseÊï∞ÊçÆÈõÜ‰∏äÔºåËØ•ÊñπÊ≥ïÂú®Êú™ËßÅÁ±ªÂà´‰∏äÁöÑÊÄßËÉΩÊèêÂçáË∂ÖËøá‰∫Ü10%„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ÂÆûÁé∞‰∫Ü28 FPSÁöÑÂÆûÊó∂Êé®ÁêÜÈÄüÂ∫¶ÔºåÊª°Ë∂≥‰∫ÜÂÆûÈôÖÂ∫îÁî®ÁöÑÈúÄÊ±Ç„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âà©Áî®ËØ•ÊäÄÊúØËØÜÂà´ÂíåÊäìÂèñÂêÑÁßçÂΩ¢Áä∂ÂíåÂ§ßÂ∞èÁöÑÁâ©‰ΩìÔºåËá™Âä®È©æÈ©∂Á≥ªÁªüÂèØ‰ª•Âà©Áî®ËØ•ÊäÄÊúØÊÑüÁü•Âë®Âõ¥ÁéØÂ¢É‰∏≠ÁöÑÁâ©‰ΩìÔºåÂ¢ûÂº∫Áé∞ÂÆûÂ∫îÁî®ÂèØ‰ª•Âà©Áî®ËØ•ÊäÄÊúØÂ∞ÜËôöÊãüÁâ©‰Ωì‰∏éÁúüÂÆûÂú∫ÊôØËøõË°åÁ≤æÁ°ÆÂØπÈΩê„ÄÇËØ•ÊäÄÊúØÁöÑÂèëÂ±ïÂ∞ÜÊúâÂä©‰∫éÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™‰∏ªÁöÑÊú∫Âô®‰∫∫Âíå‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Estimating an object's 6D pose, size, and shape from visual input is a fundamental problem in computer vision, with critical applications in robotic grasping and manipulation. Existing methods either rely on object-specific priors such as CAD models or templates, or suffer from limited generalization across categories due to pose-shape entanglement and multi-stage pipelines. In this work, we propose a unified, category-agnostic framework that simultaneously predicts 6D pose, size, and dense shape from a single RGB-D image, without requiring templates, CAD models, or category labels at test time. Our model fuses dense 2D features from vision foundation models with partial 3D point clouds using a Transformer encoder enhanced by a Mixture-of-Experts, and employs parallel decoders for pose-size estimation and shape reconstruction, achieving real-time inference at 28 FPS. Trained solely on synthetic data from 149 categories in the SOPE dataset, our framework is evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL, spanning over 300 categories. It achieves state-of-the-art accuracy on seen categories while demonstrating remarkably strong zero-shot generalization to unseen real-world objects, establishing a new standard for open-set 6D understanding in robotics and embodied AI.

