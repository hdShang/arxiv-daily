---
layout: default
title: Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View
---

# Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11687" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11687v1</a>
  <a href="https://arxiv.org/pdf/2510.11687.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11687v1" onclick="toggleFavorite(this, '2510.11687v1', 'Beyond \'Templates\': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§ç±»åˆ«æ— å…³çš„å•è§†å›¾ç‰©ä½“ä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ä¼°è®¡æ¡†æ¶ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `6Dä½å§¿ä¼°è®¡` `å½¢çŠ¶ä¼°è®¡` `ç±»åˆ«æ— å…³` `Transformer` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºç‰©ä½“ç‰¹å®šå…ˆéªŒæˆ–å­˜åœ¨ä½å§¿-å½¢çŠ¶çº ç¼ é—®é¢˜ï¼Œé™åˆ¶äº†è·¨ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æå‡ºä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œæ— éœ€æ¨¡æ¿æˆ–CADæ¨¡å‹ï¼Œå³å¯ä»å•å¼ RGB-Då›¾åƒä¸­é¢„æµ‹6Dä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå³ä½¿åœ¨æœªè§è¿‡çš„ç±»åˆ«ä¸Šï¼Œä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ã€ç±»åˆ«æ— å…³çš„æ¡†æ¶ï¼Œç”¨äºä»å•ä¸ªRGB-Då›¾åƒä¸­åŒæ—¶é¢„æµ‹ç‰©ä½“çš„6Dä½å§¿ã€å°ºå¯¸å’Œå¯†é›†å½¢çŠ¶ï¼Œè€Œæ— éœ€æ¨¡æ¿ã€CADæ¨¡å‹æˆ–æµ‹è¯•æ—¶çš„ç±»åˆ«æ ‡ç­¾ã€‚è¯¥æ¨¡å‹èåˆäº†è§†è§‰åŸºç¡€æ¨¡å‹çš„å¯†é›†2Dç‰¹å¾å’Œéƒ¨åˆ†3Dç‚¹äº‘ï¼Œä½¿ç”¨Transformerç¼–ç å™¨ï¼ˆé€šè¿‡æ··åˆä¸“å®¶æ¨¡å‹å¢å¼ºï¼‰ï¼Œå¹¶é‡‡ç”¨å¹¶è¡Œè§£ç å™¨è¿›è¡Œä½å§¿-å°ºå¯¸ä¼°è®¡å’Œå½¢çŠ¶é‡å»ºï¼Œå®ç°äº†28 FPSçš„å®æ—¶æ¨ç†é€Ÿåº¦ã€‚è¯¥æ¡†æ¶ä»…åœ¨SOPEæ•°æ®é›†ä¸­149ä¸ªç±»åˆ«çš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨SOPEã€ROPEã€ObjaversePoseå’ŒHANDALå››ä¸ªä¸åŒçš„åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†300å¤šä¸ªç±»åˆ«ã€‚åœ¨å·²è§ç±»åˆ«ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç²¾åº¦ï¼ŒåŒæ—¶å¯¹æœªè§çœŸå®ä¸–ç•Œç‰©ä½“è¡¨ç°å‡ºéå¸¸å¼ºçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæœºå™¨äººå’Œå…·èº«äººå·¥æ™ºèƒ½ä¸­çš„å¼€æ”¾é›†6Dç†è§£å»ºç«‹äº†æ–°çš„æ ‡å‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨è¿›è¡Œ6Dä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ä¼°è®¡æ—¶ï¼Œé€šå¸¸ä¾èµ–äºç‰©ä½“ç‰¹å®šçš„CADæ¨¡å‹æˆ–æ¨¡æ¿ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¼€æ”¾ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œä¸€äº›æ–¹æ³•å­˜åœ¨ä½å§¿å’Œå½¢çŠ¶çš„çº ç¼ é—®é¢˜ï¼Œä»¥åŠå¤šé˜¶æ®µpipelineå¯¼è‡´çš„è¯¯å·®ç´¯ç§¯ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆå’Œå‡†ç¡®çš„ä¼°è®¡ã€‚å› æ­¤ï¼Œå¦‚ä½•å®ç°ç±»åˆ«æ— å…³çš„ã€é«˜æ•ˆçš„ã€å‡†ç¡®çš„6Dä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ä¼°è®¡æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æå–çš„é€šç”¨2Dç‰¹å¾ï¼Œå¹¶å°†å…¶ä¸3Dç‚¹äº‘ä¿¡æ¯èåˆï¼Œä»è€Œå®ç°å¯¹ç‰©ä½“ä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶çš„è§£è€¦ä¼°è®¡ã€‚é€šè¿‡Transformeræ¶æ„å­¦ä¹ 2Då’Œ3Dç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä½¿ç”¨å¹¶è¡Œè§£ç å™¨åˆ†åˆ«é¢„æµ‹ä½å§¿-å°ºå¯¸å’Œå½¢çŠ¶ï¼Œé¿å…äº†ä¿¡æ¯ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šç‰¹å¾æå–æ¨¡å—ã€Transformerç¼–ç å™¨æ¨¡å—å’Œå¹¶è¡Œè§£ç å™¨æ¨¡å—ã€‚é¦–å…ˆï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æå–RGB-Då›¾åƒçš„2Dç‰¹å¾ï¼Œå¹¶å¯¹3Dç‚¹äº‘è¿›è¡Œå¤„ç†ã€‚ç„¶åï¼Œä½¿ç”¨Transformerç¼–ç å™¨èåˆ2Då’Œ3Dç‰¹å¾ï¼Œå¹¶é€šè¿‡æ··åˆä¸“å®¶æ¨¡å‹å¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚æœ€åï¼Œä½¿ç”¨ä¸¤ä¸ªå¹¶è¡Œè§£ç å™¨ï¼Œä¸€ä¸ªç”¨äºé¢„æµ‹6Dä½å§¿å’Œå°ºå¯¸ï¼Œå¦ä¸€ä¸ªç”¨äºé‡å»ºç‰©ä½“çš„å¯†é›†å½¢çŠ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶ç±»åˆ«æ— å…³æ€§ï¼Œå³æ— éœ€é’ˆå¯¹ç‰¹å®šç‰©ä½“è¿›è¡Œè®­ç»ƒæˆ–æä¾›CADæ¨¡å‹ï¼Œå³å¯å®ç°å¯¹æœªçŸ¥ç‰©ä½“çš„6Dä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ä¼°è®¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡Transformeræ¶æ„å’Œå¹¶è¡Œè§£ç å™¨çš„è®¾è®¡ï¼Œæœ‰æ•ˆåœ°è§£è€¦äº†ä½å§¿ã€å°ºå¯¸å’Œå½¢çŠ¶ä¹‹é—´çš„å…³ç³»ï¼Œæé«˜äº†ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šTransformerç¼–ç å™¨é‡‡ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå­¦ä¹ 2Då’Œ3Dç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚æ··åˆä¸“å®¶æ¨¡å‹ç”¨äºå¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½å§¿-å°ºå¯¸è§£ç å™¨é‡‡ç”¨å›å½’çš„æ–¹å¼é¢„æµ‹6Dä½å§¿å‚æ•°å’Œå°ºå¯¸å‚æ•°ã€‚å½¢çŠ¶è§£ç å™¨é‡‡ç”¨ç‚¹äº‘é‡å»ºçš„æ–¹å¼é¢„æµ‹ç‰©ä½“çš„å¯†é›†å½¢çŠ¶ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬ä½å§¿æŸå¤±ã€å°ºå¯¸æŸå¤±å’Œå½¢çŠ¶æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ¡†æ¶åœ¨SOPEã€ROPEã€ObjaversePoseå’ŒHANDALå››ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨å·²è§ç±»åˆ«ä¸Šå–å¾—äº†state-of-the-artçš„ç²¾åº¦ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¯¥æ¡†æ¶åœ¨æœªè§ç±»åˆ«ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ObjaversePoseæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ€§èƒ½æå‡è¶…è¿‡äº†10%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å®ç°äº†28 FPSçš„å®æ—¶æ¨ç†é€Ÿåº¦ï¼Œæ»¡è¶³äº†å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯è¯†åˆ«å’ŒæŠ“å–å„ç§å½¢çŠ¶å’Œå¤§å°çš„ç‰©ä½“ï¼Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒä¸­çš„ç‰©ä½“ï¼Œå¢å¼ºç°å®åº”ç”¨å¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯å°†è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯è¿›è¡Œç²¾ç¡®å¯¹é½ã€‚è¯¥æŠ€æœ¯çš„å‘å±•å°†æœ‰åŠ©äºå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººå’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Estimating an object's 6D pose, size, and shape from visual input is a fundamental problem in computer vision, with critical applications in robotic grasping and manipulation. Existing methods either rely on object-specific priors such as CAD models or templates, or suffer from limited generalization across categories due to pose-shape entanglement and multi-stage pipelines. In this work, we propose a unified, category-agnostic framework that simultaneously predicts 6D pose, size, and dense shape from a single RGB-D image, without requiring templates, CAD models, or category labels at test time. Our model fuses dense 2D features from vision foundation models with partial 3D point clouds using a Transformer encoder enhanced by a Mixture-of-Experts, and employs parallel decoders for pose-size estimation and shape reconstruction, achieving real-time inference at 28 FPS. Trained solely on synthetic data from 149 categories in the SOPE dataset, our framework is evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL, spanning over 300 categories. It achieves state-of-the-art accuracy on seen categories while demonstrating remarkably strong zero-shot generalization to unseen real-world objects, establishing a new standard for open-set 6D understanding in robotics and embodied AI.

