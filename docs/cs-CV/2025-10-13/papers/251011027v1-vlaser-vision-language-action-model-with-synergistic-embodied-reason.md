---
layout: default
title: Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning
---

# Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11027" target="_blank" class="toolbar-btn">arXiv: 2510.11027v1</a>
    <a href="https://arxiv.org/pdf/2510.11027.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11027v1" 
            onclick="toggleFavorite(this, '2510.11027v1', 'Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VlaserÔºöÊèêÂá∫ÂÖ∑ÊúâÂçèÂêåÂÖ∑Ë∫´Êé®ÁêÜËÉΩÂäõÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºåÂº•ÂêàVLMÊé®ÁêÜ‰∏éVLAÁ≠ñÁï•Â≠¶‰π†ÁöÑÈ∏øÊ≤ü„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `ÂÖ∑Ë∫´Êé®ÁêÜ` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Á≠ñÁï•Â≠¶‰π†` `È¢ÜÂüüËá™ÈÄÇÂ∫î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàË°îÊé•‰∏äÊ∏∏VLMÊé®ÁêÜ‰∏é‰∏ãÊ∏∏VLAÁ≠ñÁï•Â≠¶‰π†ÔºåÈôêÂà∂‰∫ÜÁ´ØÂà∞Á´ØÊú∫Âô®‰∫∫ÊéßÂà∂ÁöÑÊÄßËÉΩ„ÄÇ
2. VlaserÊ®°ÂûãÈÄöËøáÊï¥ÂêàÈ´òÂ±ÇÊé®ÁêÜÂíåÂ∫ïÂ±ÇÊéßÂà∂ÔºåÊó®Âú®Âº•ÂêàVLMÊé®ÁêÜ‰∏éVLAÁ≠ñÁï•Â≠¶‰π†‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ
3. VlaserÂú®Â§ö‰∏™ÂÖ∑Ë∫´Êé®ÁêÜ‰ªªÂä°‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂπ∂Âú®Êú∫Âô®‰∫∫ÊéßÂà∂Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Á´û‰∫âÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊó®Âú®Âº•ÂêàÂü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)ÁöÑÊé®ÁêÜ‰∏éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Á≠ñÁï•Â≠¶‰π†‰πãÈó¥ÁöÑÂÖ≥ÈîÆÂ∑ÆË∑ù„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVlaserÔºå‰∏Ä‰∏™ÂÖ∑ÊúâÂçèÂêåÂÖ∑Ë∫´Êé®ÁêÜËÉΩÂäõÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã„ÄÇVlaserÊòØ‰∏Ä‰∏™Âü∫Á°ÄËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®Â∞ÜÈ´òÂ±ÇÊé®ÁêÜ‰∏éÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÂ∫ïÂ±ÇÊéßÂà∂Áõ∏ÁªìÂêà„ÄÇÂü∫‰∫éÈ´òË¥®ÈáèÁöÑVlaser-6MÊï∞ÊçÆÈõÜÔºåVlaserÂú®‰∏ÄÁ≥ªÂàóÂÖ∑Ë∫´Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂåÖÊã¨Á©∫Èó¥Êé®ÁêÜ„ÄÅÂÖ∑Ë∫´ÂÆö‰Ωç„ÄÅÂÖ∑Ë∫´ÈóÆÁ≠îÂíå‰ªªÂä°ËßÑÂàí„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Á≥ªÁªüÂú∞Á†îÁ©∂‰∫Ü‰∏çÂêåÁöÑVLMÂàùÂßãÂåñÂ¶Ç‰ΩïÂΩ±ÂìçÁõëÁù£VLAÂæÆË∞ÉÔºå‰∏∫ÂáèËΩª‰∫íËÅîÁΩëËßÑÊ®°È¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏éÁâπÂÆö‰∫éÂÖ∑Ë∫´Á≠ñÁï•Â≠¶‰π†Êï∞ÊçÆ‰πãÈó¥ÁöÑÈ¢ÜÂüüËΩ¨ÁßªÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÅËß£„ÄÇÂü∫‰∫éËøô‰∫õËßÅËß£ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®WidowXÂü∫ÂáÜÊµãËØï‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂπ∂Âú®Google RobotÂü∫ÂáÜÊµãËØï‰∏äÂèñÂæó‰∫ÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâVLAÊ®°ÂûãÈÄöÂ∏∏‰æùËµñ‰∫éÁã¨Á´ãÁöÑVLMËøõË°åÊé®ÁêÜÔºåÁÑ∂ÂêéÂ∞ÜÊé®ÁêÜÁªìÊûú‰º†ÈÄíÁªôÁ≠ñÁï•Â≠¶‰π†Ê®°Âùó„ÄÇËøôÁßçÂàÜÁ¶ªÂØºËá¥‰ø°ÊÅØÊçüÂ§±ÂíåÊ¨°‰ºòÁöÑÁ´ØÂà∞Á´ØÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºå‰∫íËÅîÁΩëËßÑÊ®°ÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏éÂÖ∑Ë∫´ÁéØÂ¢É‰∏ãÁöÑÁ≠ñÁï•Â≠¶‰π†Êï∞ÊçÆÂ≠òÂú®ÊòæËëóÁöÑÈ¢ÜÂüüÂ∑ÆÂºÇÔºåËøõ‰∏ÄÊ≠•Âä†Ââß‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVlaserÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÈ´òÂ±ÇÊé®ÁêÜÂíåÂ∫ïÂ±ÇÊéßÂà∂ÈõÜÊàêÂà∞‰∏Ä‰∏™Áªü‰∏ÄÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã‰∏≠„ÄÇÈÄöËøáËÅîÂêàËÆ≠ÁªÉÔºåVlaserËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂÖ∑Ë∫´ÁéØÂ¢ÉÔºåÂπ∂ÁîüÊàêÊõ¥ÊúâÊïàÁöÑÂä®‰ΩúÁ≠ñÁï•„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÁ†îÁ©∂‰∫Ü‰∏çÂêåÁöÑVLMÂàùÂßãÂåñÁ≠ñÁï•Ôºå‰ª•ÂáèËΩªÈ¢ÜÂüüÂ∑ÆÂºÇÂ∏¶Êù•ÁöÑÂΩ±Âìç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVlaserÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÂê´ËßÜËßâÁºñÁ†ÅÂô®„ÄÅËØ≠Ë®ÄÁºñÁ†ÅÂô®„ÄÅÂä®‰ΩúËß£Á†ÅÂô®‰ª•Âèä‰∏Ä‰∏™Áî®‰∫éËûçÂêàËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÁöÑË∑®Ê®°ÊÄÅ‰∫§‰∫íÊ®°Âùó„ÄÇËßÜËßâÁºñÁ†ÅÂô®Ë¥üË¥£ÊèêÂèñÂõæÂÉèÁâπÂæÅÔºåËØ≠Ë®ÄÁºñÁ†ÅÂô®Ë¥üË¥£Â§ÑÁêÜÊñáÊú¨Êåá‰ª§ÔºåË∑®Ê®°ÊÄÅ‰∫§‰∫íÊ®°ÂùóÂ∞ÜËßÜËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØËûçÂêàÔºåÊúÄÂêéÂä®‰ΩúËß£Á†ÅÂô®ÁîüÊàêÁõ∏Â∫îÁöÑÂä®‰ΩúÊåá‰ª§„ÄÇÊï¥‰∏™Ê®°ÂûãÈááÁî®Á´ØÂà∞Á´ØÁöÑÊñπÂºèËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVlaserÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÂçèÂêåÂÖ∑Ë∫´Êé®ÁêÜËÉΩÂäõÔºåÂç≥Ê®°ÂûãËÉΩÂ§üÂêåÊó∂ËøõË°åÈ´òÂ±ÇÊé®ÁêÜÂíåÂ∫ïÂ±ÇÊéßÂà∂Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìË°å‰∏∫„ÄÇÊ≠§Â§ñÔºåÂØπVLMÂàùÂßãÂåñÁ≠ñÁï•ÁöÑÁ†îÁ©∂‰πü‰∏∫ÁºìËß£È¢ÜÂüüÂ∑ÆÂºÇÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVlaser‰ΩøÁî®‰∫ÜTransformerÊû∂ÊûÑ‰Ωú‰∏∫ÂÖ∂Ê†∏ÂøÉÊûÑÂª∫ÂùóÔºåÂπ∂ÈááÁî®‰∫ÜÂØπÊØîÂ≠¶‰π†ÊçüÂ§±Êù•Â¢ûÂº∫ËßÜËßâÂíåËØ≠Ë®ÄË°®Á§∫‰πãÈó¥ÁöÑÂØπÈΩê„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊé¢Á¥¢‰∫Ü‰∏çÂêåÁöÑVLMÂàùÂßãÂåñÁ≠ñÁï•ÔºåÂåÖÊã¨Áõ¥Êé•‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑVLMÊùÉÈáç„ÄÅÂØπÈ¢ÑËÆ≠ÁªÉÁöÑVLMËøõË°åÂæÆË∞É‰ª•Âèä‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉVLM„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VlaserÂú®Á©∫Èó¥Êé®ÁêÜ„ÄÅÂÖ∑Ë∫´ÂÆö‰Ωç„ÄÅÂÖ∑Ë∫´ÈóÆÁ≠îÂíå‰ªªÂä°ËßÑÂàíÁ≠âÂ§ö‰∏™ÂÖ∑Ë∫´Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂú®WidowXÊú∫Âô®‰∫∫ÊéßÂà∂Âü∫ÂáÜÊµãËØï‰∏äÔºåVlaser‰πüÂèñÂæó‰∫ÜSOTAÁªìÊûúÔºåÂπ∂Âú®Google RobotÂü∫ÂáÜÊµãËØï‰∏äË°®Áé∞Âá∫Á´û‰∫âÂäõ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéVlaserÂú®ÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÈ¢ÜÂüüÂÖ∑ÊúâÊòæËëóÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VlaserÊ®°ÂûãÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°Ôºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÊú∫Âô®‰∫∫ÂíåÊêúÁ¥¢ÊïëÊè¥Êú∫Âô®‰∫∫„ÄÇÈÄöËøáÂ∞ÜÈ´òÂ±ÇÊé®ÁêÜ‰∏éÂ∫ïÂ±ÇÊéßÂà∂Áõ∏ÁªìÂêàÔºåVlaserËÉΩÂ§ü‰ΩøÊú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£‰∫∫Á±ªÊåá‰ª§ÔºåÂπ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÊâßË°å‰ªªÂä°„ÄÇËØ•Á†îÁ©∂ÁöÑÊú™Êù•ÂΩ±ÂìçÂåÖÊã¨ÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑËá™‰∏ªÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊúçÂä°‰∫é‰∫∫Á±ª„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.

