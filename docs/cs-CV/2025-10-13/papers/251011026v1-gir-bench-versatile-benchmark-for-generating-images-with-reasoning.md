---
layout: default
title: GIR-Bench: Versatile Benchmark for Generating Images with Reasoning
---

# GIR-Bench: Versatile Benchmark for Generating Images with Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11026" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11026v1</a>
  <a href="https://arxiv.org/pdf/2510.11026.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11026v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11026v1', 'GIR-Bench: Versatile Benchmark for Generating Images with Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Long Chen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://hkust-longgroup.github.io/GIR-Bench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGIR-Benchä»¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `å›¾åƒç”Ÿæˆ` `åŸºå‡†è¯„ä¼°` `è§†è§‰ä»»åŠ¡` `ç†è§£ä¸ç”Ÿæˆ` `GIR-Bench`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹ç¼ºä¹ç³»ç»Ÿçš„åŸºå‡†æ¥è¯„ä¼°ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å¯¹é½åŠå…¶æ³›åŒ–èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºGIR-BenchåŸºå‡†ï¼Œé€šè¿‡ä¸‰ä¸ªè§†è§’è¯„ä¼°æ¨¡å‹çš„ç†è§£-ç”Ÿæˆä¸€è‡´æ€§ã€æ¨ç†é©±åŠ¨çš„ç”Ÿæˆå’Œå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»Ÿä¸€æ¨¡å‹åœ¨æ¨ç†é©±åŠ¨ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ï¼Œä½†ç†è§£ä¸ç”Ÿæˆä¹‹é—´ä»å­˜åœ¨å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸å›¾åƒç†è§£å’Œç”Ÿæˆç›¸ç»“åˆï¼Œå±•ç°å‡ºåœ¨é«˜çº§å¤šæ¨¡æ€æ™ºèƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰ç¼ºä¹ä¸€ä¸ªç³»ç»Ÿçš„ã€ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„åŸºå‡†æ¥è¯„ä¼°ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å¯¹é½åŠå…¶åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†GIR-Benchï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œè¯„ä¼°ç»Ÿä¸€æ¨¡å‹åœ¨ä¸‰ä¸ªäº’è¡¥è§†è§’ä¸‹çš„è¡¨ç°ã€‚é¦–å…ˆï¼Œè¯„ä¼°ç†è§£-ç”Ÿæˆä¸€è‡´æ€§ï¼ˆGIR-Bench-UGCï¼‰ï¼Œå…¶æ¬¡ï¼Œè¯„ä¼°æ¨ç†é©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆGIR-Bench-T2Iï¼‰ï¼Œæœ€åï¼Œè¯„ä¼°å¤šæ­¥æ¨ç†åœ¨ç¼–è¾‘ä¸­çš„åº”ç”¨ï¼ˆGIR-Bench-Editï¼‰ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ç‰¹å®šè¯„ä¼°æµç¨‹ï¼Œæœ¬æ–‡å®ç°äº†ç»†ç²’åº¦å’Œå¯è§£é‡Šçš„è¯„ä¼°ï¼Œå¹¶å‡è½»äº†ç°æœ‰è¯„ä¼°æ–¹æ³•çš„åè§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ç»Ÿä¸€æ¨¡å‹åœ¨æ¨ç†é©±åŠ¨çš„è§†è§‰ä»»åŠ¡ä¸­æ›´å…·èƒ½åŠ›ï¼Œä½†ç†è§£ä¸ç”Ÿæˆä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰ç¼ºä¹æ¨ç†ä¸­å¿ƒçš„åŸºå‡†æ¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸­çš„ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿™æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œæ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºGIR-BenchåŸºå‡†ï¼Œé€šè¿‡è®¾è®¡ä¸‰ä¸ªäº’è¡¥çš„è¯„ä¼°è§†è§’ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°æ¨¡å‹åœ¨ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œç»†è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGIR-BenchåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç†è§£-ç”Ÿæˆä¸€è‡´æ€§è¯„ä¼°ï¼ˆGIR-Bench-UGCï¼‰ã€æ¨ç†é©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¯„ä¼°ï¼ˆGIR-Bench-T2Iï¼‰å’Œå¤šæ­¥æ¨ç†ç¼–è¾‘è¯„ä¼°ï¼ˆGIR-Bench-Editï¼‰ã€‚æ¯ä¸ªæ¨¡å—éƒ½æœ‰ç‰¹å®šçš„è¯„ä¼°æµç¨‹ï¼Œæ—¨åœ¨ç»†è‡´è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šGIR-Benchçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ¨ç†ä¸­å¿ƒçš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”é€šè¿‡è®¾è®¡ç‰¹å®šçš„è¯„ä¼°æµç¨‹æ¥å‡è½»åè§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œè®¾è®¡äº†å¤šç§ä»»åŠ¡ç‰¹å®šçš„è¯„ä¼°ç®¡é“ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡çš„è¯„ä¼°éƒ½æ˜¯é’ˆå¯¹æ€§çš„ã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº†ç»†ç²’åº¦çš„è¯„ä¼°æ ‡å‡†ï¼Œä»¥æé«˜è¯„ä¼°çš„å¯è§£é‡Šæ€§ã€‚å®éªŒä¸­è¿˜è¿›è¡Œäº†å¤§é‡çš„æ¶ˆèå®éªŒï¼Œä»¥éªŒè¯ä¸åŒæ¨¡å‹çš„è¡¨ç°å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç»Ÿä¸€æ¨¡å‹åœ¨æ¨ç†é©±åŠ¨çš„è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä½†åœ¨ç†è§£ä¸ç”Ÿæˆä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å…·ä½“è€Œè¨€ï¼Œç»Ÿä¸€æ¨¡å‹åœ¨GIR-Bench-UGCå’ŒGIR-Bench-T2Iä»»åŠ¡ä¸­çš„è¡¨ç°æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GIR-Benchçš„æå‡ºä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°æä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„å·¥å…·ï¼Œèƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚å…¶æ½œåœ¨ä»·å€¼åœ¨äºæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ï¼Œæ¨åŠ¨å¤šæ¨¡æ€æ™ºèƒ½çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹ç­‰å®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce \textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at \href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.

