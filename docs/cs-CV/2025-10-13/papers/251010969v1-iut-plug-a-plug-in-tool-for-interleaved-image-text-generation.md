---
layout: default
title: IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation
---

# IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10969" target="_blank" class="toolbar-btn">arXiv: 2510.10969v1</a>
    <a href="https://arxiv.org/pdf/2510.10969.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10969v1" 
            onclick="toggleFavorite(this, '2510.10969v1', 'IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zeteng Lin, Xingxing Li, Wen You, Xiaoyang Li, Zehan Lu, Yujun Cai, Jing Tang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫IUT-PlugÊèí‰ª∂ÔºåÈÄöËøáÊòæÂºèÁªìÊûÑÂåñÊé®ÁêÜÂ¢ûÂº∫Â§öÊ®°ÊÄÅÂõæÊñáÁîüÊàê‰∏≠‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÁîüÊàê` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ÂõæÂÉèÁêÜËß£` `ÁªìÊûÑÂåñÊé®ÁêÜ` `‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÁîüÊàê‰∏≠Èöæ‰ª•‰øùÊåÅÈÄªËæë„ÄÅÂØπË±°ÂíåÈ£éÊ†º‰∏ÄËá¥ÊÄßÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. IUT-PlugÈÄöËøáÂõæÂÉèÁêÜËß£Ê†ëËøõË°åÊòæÂºèÁªìÊûÑÂåñÊé®ÁêÜÔºåÂ¢ûÂº∫Áé∞ÊúâÊ®°ÂûãÔºåÂáèËΩªÈÄªËæë„ÄÅË∫´‰ªΩÂíåÈ£éÊ†º‰∏äÁöÑ‰∏ä‰∏ãÊñáÊºÇÁßª„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåIUT-Plug‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÔºåËøòÊúâÊïàÁºìËß£‰∫ÜÂ§öÊ®°ÊÄÅÈóÆÁ≠î‰∏≠Â§öÁßçÂΩ¢ÂºèÁöÑ‰∏ä‰∏ãÊñáÊºÇÁßª„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÔºåÂåÖÊã¨GPT-4ÂíåDALL-EÔºåÂú®Â§öÊ®°ÊÄÅÂõæÊñáÁîüÊàê‰∏≠Â∏∏Â∏∏Èöæ‰ª•‰øùÊåÅÈÄªËæë„ÄÅÂØπË±°Ë∫´‰ªΩÂíåÈ£éÊ†ºÁöÑ‰∏ÄËá¥ÊÄß„ÄÇËøôÁßçÂ±ÄÈôêÊÄß‰∏•ÈáçÈòªÁ¢ç‰∫ÜVLMsÂú®Â§çÊùÇÂõæÊñáËæìÂÖ•ËæìÂá∫Âú∫ÊôØ‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜIUT-PlugÔºå‰∏Ä‰∏™Âü∫‰∫éÂõæÂÉèÁêÜËß£Ê†ëÔºàIUTÔºâÁöÑÊ®°ÂùóÔºåÈÄöËøáÊòæÂºèÁªìÊûÑÂåñÊé®ÁêÜÊù•Â¢ûÂº∫Áé∞ÊúâÁöÑ‰∫§ÈîôÂºèVLMsÔºå‰ªéËÄåÂáèËΩªÈÄªËæë„ÄÅÂÆû‰ΩìË∫´‰ªΩÂíåÈ£éÊ†º‰∏äÁöÑ‰∏ä‰∏ãÊñáÊºÇÁßª„ÄÇËØ•Ê°ÜÊû∂ÂàÜ‰∏§‰∏™Èò∂ÊÆµËøêË°åÔºöÔºà1ÔºâÂä®ÊÄÅIUT-PlugÊèêÂèñÊ®°ÂùóÂ∞ÜËßÜËßâÂú∫ÊôØËß£Êûê‰∏∫ÂàÜÂ±ÇÁ¨¶Âè∑ÁªìÊûÑ„ÄÇÔºà2ÔºâÂçèË∞ÉÁöÑÂèô‰∫ãÊµÅÁ®ãÂíåÂõæÂÉèÂêàÊàêÊú∫Âà∂Á°Æ‰øùË∑®Ê®°ÊÄÅ‰∏ÄËá¥ÊÄß„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞Êàë‰ª¨ÁöÑÊñπÊ≥ïÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºåÂü∫‰∫é3000‰∏™ÁúüÂÆûÁöÑ‰∫∫Â∑•ÁîüÊàêÁöÑÈóÆÈ¢ò-Á≠îÊ°àÂØπÔºåÂπ∂ÂØπÂæÆË∞ÉÁöÑÂ§ßÊ®°ÂûãËøõË°åËØÑ‰º∞ÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂä®ÊÄÅËØÑ‰º∞ÂçèËÆÆÔºåÁî®‰∫éÈáèÂåñ‰∫§ÈîôÂºèVLMs‰∏≠ÁöÑ‰∏ä‰∏ãÊñáÊºÇÁßª„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåIUT-Plug‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÂú®Â∑≤Âª∫Á´ãÂü∫ÂáÜ‰∏äÁöÑÂáÜÁ°ÆÊÄßÔºåËÄå‰∏îÊúâÊïàÂú∞ÁºìËß£‰∫ÜÂêÑÁßçÂ§öÊ®°ÊÄÅÈóÆÁ≠îÔºàQAÔºâÂú∫ÊôØ‰∏≠‰∏âÁßçÂÖ≥ÈîÆÂΩ¢ÂºèÁöÑ‰∏ä‰∏ãÊñáÊºÇÁßª„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§ÑÁêÜ‰∫§ÈîôÂºèÂõæÊñáÁîüÊàê‰ªªÂä°Êó∂ÔºåÂÆπÊòìÂá∫Áé∞‰∏ä‰∏ãÊñáÊºÇÁßªÈóÆÈ¢òÔºåÂÖ∑‰ΩìË°®Áé∞‰∏∫ÈÄªËæëÊ∑∑‰π±„ÄÅÂØπË±°Ë∫´‰ªΩ‰∏ç‰∏ÄËá¥‰ª•ÂèäÈ£éÊ†ºÁ™ÅÂèò„ÄÇËøô‰∫õÈóÆÈ¢òÂØºËá¥ÁîüÊàêÁöÑÂÜÖÂÆπË¥®Èáè‰∏ãÈôçÔºåÂΩ±Âìç‰∫ÜÊ®°ÂûãÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®ÊïàÊûú„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπÂõæÂÉèÂÜÖÂÆπÁöÑÁªìÊûÑÂåñÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÔºåÈöæ‰ª•‰øùËØÅÁîüÊàêËøáÁ®ã‰∏≠ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöIUT-PlugÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•ÂõæÂÉèÁêÜËß£Ê†ëÔºàIUTÔºâÊù•ÂØπËßÜËßâÂú∫ÊôØËøõË°åÁªìÊûÑÂåñËß£ÊûêÔºå‰ªéËÄåÂÆûÁé∞ÊòæÂºèÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇIUTËÉΩÂ§üÂ∞ÜÂõæÂÉèÂàÜËß£‰∏∫ÂàÜÂ±ÇÁöÑÁ¨¶Âè∑ÁªìÊûÑÔºåÊçïÊçâÂõæÂÉè‰∏≠ÂØπË±°‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂíåÂ±ûÊÄß„ÄÇÈÄöËøáËøôÁßçÁªìÊûÑÂåñÁöÑË°®Á§∫ÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£ÂõæÂÉèÂÜÖÂÆπÔºåÂπ∂Âú®ÁîüÊàêËøáÁ®ã‰∏≠‰øùÊåÅ‰∏ä‰∏ãÊñáÁöÑ‰∏ÄËá¥ÊÄß„ÄÇËøôÁßçËÆæËÆ°Êó®Âú®Âº•Ë°•Áé∞ÊúâVLMsÂú®ÁêÜËß£ÂíåÊé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöIUT-PlugÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöIUT-PlugÊèêÂèñÊ®°ÂùóÂíåÂçèË∞ÉÁöÑÂèô‰∫ãÊµÅÁ®ã‰∏éÂõæÂÉèÂêàÊàêÊú∫Âà∂„ÄÇÈ¶ñÂÖàÔºåIUT-PlugÊèêÂèñÊ®°ÂùóÂä®ÊÄÅÂú∞Â∞ÜËßÜËßâÂú∫ÊôØËß£Êûê‰∏∫ÂàÜÂ±ÇÁöÑÁ¨¶Âè∑ÁªìÊûÑÔºåÊûÑÂª∫ÂõæÂÉèÁêÜËß£Ê†ë„ÄÇÁÑ∂ÂêéÔºåÂçèË∞ÉÁöÑÂèô‰∫ãÊµÅÁ®ãÂíåÂõæÂÉèÂêàÊàêÊú∫Âà∂Âà©Áî®IUTÊèê‰æõÁöÑÁªìÊûÑÂåñ‰ø°ÊÅØÔºåÁîüÊàê‰∏éÂõæÂÉèÂÜÖÂÆπ‰∏ÄËá¥ÁöÑÊñáÊú¨ÊèèËø∞ÂíåÂõæÂÉè„ÄÇËøô‰∏§‰∏™Èò∂ÊÆµÂçèÂêåÂ∑•‰ΩúÔºåÁ°Æ‰øùË∑®Ê®°ÊÄÅÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöIUT-PlugÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÂõæÂÉèÁêÜËß£Ê†ëÔºàIUTÔºâ‰Ωú‰∏∫ÊòæÂºèÁªìÊûÑÂåñÊé®ÁêÜÁöÑÂ∑•ÂÖ∑„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåIUTËÉΩÂ§üÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑÂõæÂÉè‰ø°ÊÅØÔºåÂπ∂ÊîØÊåÅÊõ¥Â§çÊùÇÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåÂä®ÊÄÅIUT-PlugÊèêÂèñÊ®°ÂùóËÉΩÂ§üÊ†πÊçÆ‰∏çÂêåÁöÑËßÜËßâÂú∫ÊôØËá™ÈÄÇÂ∫îÂú∞ÊûÑÂª∫IUTÔºåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÁÅµÊ¥ªÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇËøôÁßçÊòæÂºèÁªìÊûÑÂåñÊé®ÁêÜÁöÑÊñπÂºèÊòØ‰∏éÁé∞ÊúâÊñπÊ≥ïÊúÄÊú¨Ë¥®ÁöÑÂå∫Âà´„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöIUT-PlugÊèêÂèñÊ®°ÂùóÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÊú™Áü•Ôºå‰ΩÜÂèØ‰ª•Êé®ÊµãÂÖ∂ÂèØËÉΩÊ∂âÂèäÁõÆÊ†áÊ£ÄÊµã„ÄÅÂú∫ÊôØÂõæÁîüÊàêÁ≠âÊäÄÊúØ„ÄÇÂçèË∞ÉÁöÑÂèô‰∫ãÊµÅÁ®ãÂíåÂõæÂÉèÂêàÊàêÊú∫Âà∂ÂèØËÉΩÈááÁî®TransformerÊû∂ÊûÑÔºåÂπ∂Âà©Áî®IUTÊèê‰æõÁöÑÁªìÊûÑÂåñ‰ø°ÊÅØËøõË°åÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂºïÂØº„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÂèØËÉΩÂåÖÊã¨Ë∑®Ê®°ÊÄÅ‰∏ÄËá¥ÊÄßÊçüÂ§±Ôºå‰ª•Á°Æ‰øùÁîüÊàêÁöÑÊñáÊú¨ÂíåÂõæÂÉèÂú®ËØ≠‰πâ‰∏ä‰øùÊåÅ‰∏ÄËá¥„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÂèØËÉΩÊúâÊâÄÊèèËø∞Ôºå‰ΩÜÊ≠§Â§ÑÊó†Ê≥ïÂæóÁü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºåÂü∫‰∫é3000‰∏™‰∫∫Â∑•ÁîüÊàêÁöÑÈóÆÈ¢ò-Á≠îÊ°àÂØπÔºåÁî®‰∫éËØÑ‰º∞‰∫§ÈîôÂºèVLMs‰∏≠ÁöÑ‰∏ä‰∏ãÊñáÊºÇÁßª„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåIUT-Plug‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÂú®Â∑≤Âª∫Á´ãÂü∫ÂáÜ‰∏äÁöÑÂáÜÁ°ÆÊÄßÔºåËÄå‰∏îÊúâÊïàÂú∞ÁºìËß£‰∫ÜÂêÑÁßçÂ§öÊ®°ÊÄÅÈóÆÁ≠îÔºàQAÔºâÂú∫ÊôØ‰∏≠‰∏âÁßçÂÖ≥ÈîÆÂΩ¢ÂºèÁöÑ‰∏ä‰∏ãÊñáÊºÇÁßª„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÊú™Áü•Ôºå‰ΩÜÊï¥‰ΩìÊïàÊûúÊòæËëó„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

IUT-PlugÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÂõæÂÉèÁºñËæë„ÄÅÂàõÊÑèÂÜÖÂÆπÁîüÊàê„ÄÅËßÜËßâÊïÖ‰∫ãËÆ≤Ëø∞„ÄÅ‰ª•ÂèäÂ§öÊ®°ÊÄÅ‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüü„ÄÇËØ•ÊäÄÊúØËÉΩÂ§üÊèêÂçáÁîüÊàêÂÜÖÂÆπÁöÑË¥®ÈáèÂíå‰∏ÄËá¥ÊÄßÔºå‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Ëá™ÁÑ∂„ÄÅÊõ¥Êô∫ËÉΩÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåIUT-PlugÊúâÊúõÊàê‰∏∫Â§öÊ®°ÊÄÅÂÜÖÂÆπÁîüÊàêÈ¢ÜÂüüÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Existing vision language models (VLMs), including GPT-4 and DALL-E, often struggle to preserve logic, object identity, and style in multimodal image-text generation. This limitation significantly hinders the generalization capability of VLMs in complex image-text input-output scenarios. To address this issue, we propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which enhances existing interleaved VLMs through explicit structured reasoning, thereby mitigating context drift in logic, entity identity, and style. The proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction module parses visual scenes into hierarchical symbolic structures. (2) A coordinated narrative-flow and image synthesis mechanism ensures cross-modal consistency. To evaluate our approach, we construct a novel benchmark based on 3,000 real human-generated question-answer pairs over fine-tuned large models, introducing a dynamic evaluation protocol for quantifying context drift in interleaved VLMs. Experimental results demonstrate that IUT-Plug not only improves accuracy on established benchmarks but also effectively alleviates the three critical forms of context drift across diverse multimodal question answering (QA) scenarios.

