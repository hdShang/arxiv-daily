---
layout: default
title: "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation"
---

# IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10969" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10969v1</a>
  <a href="https://arxiv.org/pdf/2510.10969.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10969v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10969v1', 'IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeteng Lin, Xingxing Li, Wen You, Xiaoyang Li, Zehan Lu, Yujun Cai, Jing Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIUT-Plugæ’ä»¶ï¼Œé€šè¿‡æ˜¾å¼ç»“æ„åŒ–æ¨ç†å¢å¼ºå¤šæ¨¡æ€å›¾æ–‡ç”Ÿæˆä¸­ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€ç”Ÿæˆ` `è§†è§‰è¯­è¨€æ¨¡å‹` `å›¾åƒç†è§£` `ç»“æ„åŒ–æ¨ç†` `ä¸Šä¸‹æ–‡ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç”Ÿæˆä¸­éš¾ä»¥ä¿æŒé€»è¾‘ã€å¯¹è±¡å’Œé£æ ¼ä¸€è‡´æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. IUT-Plugé€šè¿‡å›¾åƒç†è§£æ ‘è¿›è¡Œæ˜¾å¼ç»“æ„åŒ–æ¨ç†ï¼Œå¢å¼ºç°æœ‰æ¨¡å‹ï¼Œå‡è½»é€»è¾‘ã€èº«ä»½å’Œé£æ ¼ä¸Šçš„ä¸Šä¸‹æ–‡æ¼‚ç§»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒIUT-Plugä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè¿˜æœ‰æ•ˆç¼“è§£äº†å¤šæ¨¡æ€é—®ç­”ä¸­å¤šç§å½¢å¼çš„ä¸Šä¸‹æ–‡æ¼‚ç§»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼ŒåŒ…æ‹¬GPT-4å’ŒDALL-Eï¼Œåœ¨å¤šæ¨¡æ€å›¾æ–‡ç”Ÿæˆä¸­å¸¸å¸¸éš¾ä»¥ä¿æŒé€»è¾‘ã€å¯¹è±¡èº«ä»½å’Œé£æ ¼çš„ä¸€è‡´æ€§ã€‚è¿™ç§å±€é™æ€§ä¸¥é‡é˜»ç¢äº†VLMsåœ¨å¤æ‚å›¾æ–‡è¾“å…¥è¾“å‡ºåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†IUT-Plugï¼Œä¸€ä¸ªåŸºäºå›¾åƒç†è§£æ ‘ï¼ˆIUTï¼‰çš„æ¨¡å—ï¼Œé€šè¿‡æ˜¾å¼ç»“æ„åŒ–æ¨ç†æ¥å¢å¼ºç°æœ‰çš„äº¤é”™å¼VLMsï¼Œä»è€Œå‡è½»é€»è¾‘ã€å®ä½“èº«ä»½å’Œé£æ ¼ä¸Šçš„ä¸Šä¸‹æ–‡æ¼‚ç§»ã€‚è¯¥æ¡†æ¶åˆ†ä¸¤ä¸ªé˜¶æ®µè¿è¡Œï¼šï¼ˆ1ï¼‰åŠ¨æ€IUT-Plugæå–æ¨¡å—å°†è§†è§‰åœºæ™¯è§£æä¸ºåˆ†å±‚ç¬¦å·ç»“æ„ã€‚ï¼ˆ2ï¼‰åè°ƒçš„å™äº‹æµç¨‹å’Œå›¾åƒåˆæˆæœºåˆ¶ç¡®ä¿è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼ŒåŸºäº3000ä¸ªçœŸå®çš„äººå·¥ç”Ÿæˆçš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œå¹¶å¯¹å¾®è°ƒçš„å¤§æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¼•å…¥äº†ä¸€ç§åŠ¨æ€è¯„ä¼°åè®®ï¼Œç”¨äºé‡åŒ–äº¤é”™å¼VLMsä¸­çš„ä¸Šä¸‹æ–‡æ¼‚ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIUT-Plugä¸ä»…æé«˜äº†åœ¨å·²å»ºç«‹åŸºå‡†ä¸Šçš„å‡†ç¡®æ€§ï¼Œè€Œä¸”æœ‰æ•ˆåœ°ç¼“è§£äº†å„ç§å¤šæ¨¡æ€é—®ç­”ï¼ˆQAï¼‰åœºæ™¯ä¸­ä¸‰ç§å…³é”®å½¢å¼çš„ä¸Šä¸‹æ–‡æ¼‚ç§»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†äº¤é”™å¼å›¾æ–‡ç”Ÿæˆä»»åŠ¡æ—¶ï¼Œå®¹æ˜“å‡ºç°ä¸Šä¸‹æ–‡æ¼‚ç§»é—®é¢˜ï¼Œå…·ä½“è¡¨ç°ä¸ºé€»è¾‘æ··ä¹±ã€å¯¹è±¡èº«ä»½ä¸ä¸€è‡´ä»¥åŠé£æ ¼çªå˜ã€‚è¿™äº›é—®é¢˜å¯¼è‡´ç”Ÿæˆçš„å†…å®¹è´¨é‡ä¸‹é™ï¼Œå½±å“äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„åº”ç”¨æ•ˆæœã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å›¾åƒå†…å®¹çš„ç»“æ„åŒ–ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIUT-Plugçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å›¾åƒç†è§£æ ‘ï¼ˆIUTï¼‰æ¥å¯¹è§†è§‰åœºæ™¯è¿›è¡Œç»“æ„åŒ–è§£æï¼Œä»è€Œå®ç°æ˜¾å¼çš„æ¨ç†è¿‡ç¨‹ã€‚IUTèƒ½å¤Ÿå°†å›¾åƒåˆ†è§£ä¸ºåˆ†å±‚çš„ç¬¦å·ç»“æ„ï¼Œæ•æ‰å›¾åƒä¸­å¯¹è±¡ä¹‹é—´çš„å…³ç³»å’Œå±æ€§ã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–çš„è¡¨ç¤ºï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£å›¾åƒå†…å®¹ï¼Œå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¼¥è¡¥ç°æœ‰VLMsåœ¨ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIUT-Plugæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šIUT-Plugæå–æ¨¡å—å’Œåè°ƒçš„å™äº‹æµç¨‹ä¸å›¾åƒåˆæˆæœºåˆ¶ã€‚é¦–å…ˆï¼ŒIUT-Plugæå–æ¨¡å—åŠ¨æ€åœ°å°†è§†è§‰åœºæ™¯è§£æä¸ºåˆ†å±‚çš„ç¬¦å·ç»“æ„ï¼Œæ„å»ºå›¾åƒç†è§£æ ‘ã€‚ç„¶åï¼Œåè°ƒçš„å™äº‹æµç¨‹å’Œå›¾åƒåˆæˆæœºåˆ¶åˆ©ç”¨IUTæä¾›çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œç”Ÿæˆä¸å›¾åƒå†…å®¹ä¸€è‡´çš„æ–‡æœ¬æè¿°å’Œå›¾åƒã€‚è¿™ä¸¤ä¸ªé˜¶æ®µååŒå·¥ä½œï¼Œç¡®ä¿è·¨æ¨¡æ€çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šIUT-Plugçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†å›¾åƒç†è§£æ ‘ï¼ˆIUTï¼‰ä½œä¸ºæ˜¾å¼ç»“æ„åŒ–æ¨ç†çš„å·¥å…·ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIUTèƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„å›¾åƒä¿¡æ¯ï¼Œå¹¶æ”¯æŒæ›´å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€IUT-Plugæå–æ¨¡å—èƒ½å¤Ÿæ ¹æ®ä¸åŒçš„è§†è§‰åœºæ™¯è‡ªé€‚åº”åœ°æ„å»ºIUTï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ˜¾å¼ç»“æ„åŒ–æ¨ç†çš„æ–¹å¼æ˜¯ä¸ç°æœ‰æ–¹æ³•æœ€æœ¬è´¨çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šIUT-Plugæå–æ¨¡å—çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œä½†å¯ä»¥æ¨æµ‹å…¶å¯èƒ½æ¶‰åŠç›®æ ‡æ£€æµ‹ã€åœºæ™¯å›¾ç”Ÿæˆç­‰æŠ€æœ¯ã€‚åè°ƒçš„å™äº‹æµç¨‹å’Œå›¾åƒåˆæˆæœºåˆ¶å¯èƒ½é‡‡ç”¨Transformeræ¶æ„ï¼Œå¹¶åˆ©ç”¨IUTæä¾›çš„ç»“æ„åŒ–ä¿¡æ¯è¿›è¡Œæ³¨æ„åŠ›æœºåˆ¶çš„å¼•å¯¼ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…æ‹¬è·¨æ¨¡æ€ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ–‡æœ¬å’Œå›¾åƒåœ¨è¯­ä¹‰ä¸Šä¿æŒä¸€è‡´ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€æè¿°ï¼Œä½†æ­¤å¤„æ— æ³•å¾—çŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼ŒåŸºäº3000ä¸ªäººå·¥ç”Ÿæˆçš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œç”¨äºè¯„ä¼°äº¤é”™å¼VLMsä¸­çš„ä¸Šä¸‹æ–‡æ¼‚ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIUT-Plugä¸ä»…æé«˜äº†åœ¨å·²å»ºç«‹åŸºå‡†ä¸Šçš„å‡†ç¡®æ€§ï¼Œè€Œä¸”æœ‰æ•ˆåœ°ç¼“è§£äº†å„ç§å¤šæ¨¡æ€é—®ç­”ï¼ˆQAï¼‰åœºæ™¯ä¸­ä¸‰ç§å…³é”®å½¢å¼çš„ä¸Šä¸‹æ–‡æ¼‚ç§»ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†æ•´ä½“æ•ˆæœæ˜¾è‘—ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IUT-Plugå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å›¾åƒç¼–è¾‘ã€åˆ›æ„å†…å®¹ç”Ÿæˆã€è§†è§‰æ•…äº‹è®²è¿°ã€ä»¥åŠå¤šæ¨¡æ€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿæå‡ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œä¸€è‡´æ€§ï¼Œä¸ºç”¨æˆ·æä¾›æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼ŒIUT-Plugæœ‰æœ›æˆä¸ºå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆé¢†åŸŸçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing vision language models (VLMs), including GPT-4 and DALL-E, often struggle to preserve logic, object identity, and style in multimodal image-text generation. This limitation significantly hinders the generalization capability of VLMs in complex image-text input-output scenarios. To address this issue, we propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which enhances existing interleaved VLMs through explicit structured reasoning, thereby mitigating context drift in logic, entity identity, and style. The proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction module parses visual scenes into hierarchical symbolic structures. (2) A coordinated narrative-flow and image synthesis mechanism ensures cross-modal consistency. To evaluate our approach, we construct a novel benchmark based on 3,000 real human-generated question-answer pairs over fine-tuned large models, introducing a dynamic evaluation protocol for quantifying context drift in interleaved VLMs. Experimental results demonstrate that IUT-Plug not only improves accuracy on established benchmarks but also effectively alleviates the three critical forms of context drift across diverse multimodal question answering (QA) scenarios.

