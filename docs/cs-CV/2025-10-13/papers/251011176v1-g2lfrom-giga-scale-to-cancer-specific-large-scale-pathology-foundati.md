---
layout: default
title: G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation
---

# G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11176" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.11176v1</a>
  <a href="https://arxiv.org/pdf/2510.11176.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11176v1" onclick="toggleFavorite(this, '2510.11176v1', 'G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫G2LÊ°ÜÊû∂ÔºåÈÄöËøáÁü•ËØÜËí∏È¶èÂ∞ÜÂçÉ‰∫øÁ∫ßÁóÖÁêÜÊ®°ÂûãËÉΩÂäõËøÅÁßªËá≥ÁôåÁóáÁâπÂºÇÊÄßÂ§ßÂûãÊ®°Âûã„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÁóÖÁêÜÂ≠¶` `Âü∫Á°ÄÊ®°Âûã` `Áü•ËØÜËí∏È¶è` `ÁôåÁóáËØäÊñ≠` `Â§ßÂûãÊ®°Âûã` `ËøÅÁßªÂ≠¶‰π†` `ËÆ°ÁÆóÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁóÖÁêÜÂ≠¶Âü∫Á°ÄÊ®°ÂûãËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÁôåÁóáËØäÊñ≠‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. G2LÊ°ÜÊû∂ÈÄöËøáÁü•ËØÜËí∏È¶èÔºåÂ∞ÜÂ§ßÂûãÊ®°ÂûãÁöÑÊÄßËÉΩÊèêÂçáËá≥‰∏éÂçÉ‰∫øÁ∫ßÊ®°ÂûãÁõ∏ÂΩìÁöÑÊ∞¥Âπ≥„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåG2LÊ°ÜÊû∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éÂêåÁ≠âËßÑÊ®°ÁöÑÊ®°ÂûãÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÊïôÂ∏àÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁóÖÁêÜÂ≠¶Âü∫Á°ÄÊ®°ÂûãÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÊâ©Â§ßËÆ≠ÁªÉÊï∞ÊçÆËßÑÊ®°„ÄÅÂ§öÊ†∑ÂåñÁôåÁóáÁ±ªÂûãÂíåÂ¢ûÂä†Ê®°ÂûãÂ§ßÂ∞èËÉΩÂ§üÊåÅÁª≠ÊèêÈ´òÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂú®Êï∞ÂçÅÁßçÁôåÁóáÁ±ªÂûãÂíåÊï∞ÂçÅ‰∏áÂº†ÂàáÁâá‰∏äËÆ≠ÁªÉÁöÑÂçÉ‰∫øÁ∫ßÂü∫Á°ÄÊ®°ÂûãÔºåÁî±‰∫éÂÖ∂Âú®ÂºÄÂèëÂíåÈÉ®ÁΩ≤‰∏≠Â∑®Â§ßÁöÑËÆ°ÁÆóÊàêÊú¨ÔºåÁªôÂÆûÈôÖÂ∫îÁî®Â∏¶Êù•‰∫ÜÈáçÂ§ßÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫G2LÊ°ÜÊû∂ÁöÑÊñ∞Á≠ñÁï•ÔºåÊó®Âú®Â∞ÜÂ§ßÂûãÊ®°ÂûãÁöÑÊÄßËÉΩÊèêÂçáÂà∞‰∏éÂçÉ‰∫øÁ∫ßÊ®°ÂûãÁõ∏ÂΩìÁöÑÊ∞¥Âπ≥ÔºåËÄåÂ§ßÂûãÊ®°Âûã‰ªÖÂåÖÂê´ÂçÉ‰∫øÁ∫ßÊ®°Âûã15%ÁöÑÂèÇÊï∞„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈááÁî®Áü•ËØÜËí∏È¶èÔºå‰ªÖ‰ΩøÁî®ÁõÆÊ†áÁôåÁóáÔºàÂ¶Ç‰π≥ËÖ∫Áôå„ÄÅÂâçÂàóËÖ∫ÁôåÁ≠âÔºâÁöÑ1KÂº†ÁóÖÁêÜÂàáÁâáÔºåÂ∞ÜÂçÉ‰∫øÁ∫ßÊ®°ÂûãÁöÑËÉΩÂäõËΩ¨ÁßªÂà∞Â§ßÂûãÊ®°Âûã„ÄÇÊâÄÂæóÂà∞ÁöÑËí∏È¶èÊ®°Âûã‰∏ç‰ªÖÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éÂêåÁ≠âËßÑÊ®°ÔºàÂç≥Â§ßÂûãÔºâÁöÑÊúÄÂÖàËøõÊ®°ÂûãÔºåËÄå‰∏îÊúâË∂£ÁöÑÊòØÔºåÂú®Êüê‰∫õÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖËøá‰∫ÜÂçÉ‰∫øÁ∫ßÊïôÂ∏àÊ®°ÂûãÂíåÂ∑®ÂûãÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåËí∏È¶èÊ®°ÂûãË°®Áé∞Âá∫Êõ¥È´òÁöÑÈ≤ÅÊ£íÊÄßÊåáÊ†áÔºåË°®ÊòéÂÖ∂ÂØπÊù•Ëá™Â§ö‰∏™Êú∫ÊûÑÁöÑÂõæÂÉèÂèòÂºÇÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈÄÇÂ∫îÊÄß„ÄÇËøô‰∫õÂèëÁé∞Ë°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÈíàÂØπÂ§ßÂûãÊ®°ÂûãÁöÑËí∏È¶èÊñπÊ≥ïÊòØ‰∏ÄÁßçÊï∞ÊçÆÂíåÂèÇÊï∞È´òÊïàÁöÑÊñπÂºèÔºåÂèØ‰ª•Âú®Ê≤°ÊúâËøáÈ´òËÆ°ÁÆóË¥üÊãÖÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞ÁôåÁóáÁâπÂºÇÊÄßÂ∫îÁî®ÁöÑÂçÉ‰∫øÁ∫ßÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂçÉ‰∫øÁ∫ßÁóÖÁêÜÂ≠¶Âü∫Á°ÄÊ®°ÂûãËÆ°ÁÆóÊàêÊú¨ËøáÈ´òÔºåÈöæ‰ª•ÂÆûÈôÖÈÉ®ÁΩ≤ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïË¶Å‰πàËÆ°ÁÆóËµÑÊ∫êÊ∂àËÄóÂ∑®Â§ßÔºåË¶Å‰πàÊ®°ÂûãÊÄßËÉΩ‰∏çË∂≥ÔºåÊó†Ê≥ïÊª°Ë∂≥ÁôåÁóáÁâπÂºÇÊÄßËØäÊñ≠ÁöÑÈúÄÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Áü•ËØÜËí∏È¶èÔºåÂ∞ÜÂçÉ‰∫øÁ∫ßÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞ÂèÇÊï∞ÈáèÊõ¥Â∞èÁöÑÂ§ßÂûãÊ®°Âûã‰∏ä„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•Âú®‰øùÊåÅÊ®°ÂûãÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨Ôºå‰ΩøÂÖ∂Êõ¥Êòì‰∫éÈÉ®ÁΩ≤ÂíåÂ∫îÁî®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöG2LÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÔºåËÆ≠ÁªÉ‰∏Ä‰∏™ÂçÉ‰∫øÁ∫ßÁöÑÂ§ßÂûãÊïôÂ∏àÊ®°ÂûãÔºõÁÑ∂ÂêéÔºå‰ΩøÁî®Â∞ëÈáèÁõÆÊ†áÁôåÁóáÁöÑÁóÖÁêÜÂàáÁâáÔºåÈÄöËøáÁü•ËØÜËí∏È¶èÂ∞ÜÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â§ßÂûãÂ≠¶ÁîüÊ®°Âûã‰∏ä„ÄÇÂ≠¶ÁîüÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Â≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÁöÑÈ¢ÑÊµãÁªìÊûúÔºå‰ªéËÄåËé∑Âæó‰∏éÊïôÂ∏àÊ®°ÂûãÁõ∏‰ººÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÔºåÂÆÉËÉΩÂ§üÂú®ÊòæËëóÈôç‰ΩéÊ®°ÂûãÂèÇÊï∞ÈáèÂíåËÆ°ÁÆóÊàêÊú¨ÁöÑÂêåÊó∂Ôºå‰øùÊåÅÁîöËá≥Ë∂ÖË∂äÊïôÂ∏àÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËøô‰∏ªË¶ÅÂΩíÂäü‰∫éÁü•ËØÜËí∏È¶èÊäÄÊúØÔºåÂÆÉËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÂ§ßÂûãÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â∞èÂûãÊ®°Âûã‰∏äÔºåÈÅøÂÖç‰∫Ü‰ªéÂ§¥ËÆ≠ÁªÉÂ∏¶Êù•ÁöÑÂõ∞Èöæ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1ÔºâÈÄâÊã©ÂêàÈÄÇÁöÑÊïôÂ∏àÊ®°ÂûãÂíåÂ≠¶ÁîüÊ®°ÂûãÔºõ2ÔºâËÆæËÆ°ÊúâÊïàÁöÑÁü•ËØÜËí∏È¶èÊçüÂ§±ÂáΩÊï∞Ôºå‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®KLÊï£Â∫¶Êù•Ë°°ÈáèÂ≠¶ÁîüÊ®°ÂûãÂíåÊïôÂ∏àÊ®°ÂûãÈ¢ÑÊµãÁªìÊûú‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºõ3ÔºâÈÄâÊã©ÂêàÈÄÇÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®1KÂº†ÁõÆÊ†áÁôåÁóáÁöÑÁóÖÁêÜÂàáÁâáËøõË°åËí∏È¶èËÆ≠ÁªÉ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåG2LÊ°ÜÊû∂Âú®Â§ö‰∏™ÁôåÁóáËØäÊñ≠Âü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫éÂêåÁ≠âËßÑÊ®°ÁöÑÊúÄÂÖàËøõÊ®°ÂûãÔºåÁîöËá≥Âú®Êüê‰∫õÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖËøá‰∫ÜÂçÉ‰∫øÁ∫ßÊïôÂ∏àÊ®°ÂûãÂíåÂ∑®ÂûãÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåËí∏È¶èÊ®°ÂûãË°®Áé∞Âá∫Êõ¥È´òÁöÑÈ≤ÅÊ£íÊÄßÊåáÊ†áÔºåË°®ÊòéÂÖ∂ÂØπÊù•Ëá™Â§ö‰∏™Êú∫ÊûÑÁöÑÂõæÂÉèÂèòÂºÇÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÁôåÁóáËØäÊñ≠„ÄÅÈ¢ÑÂêéÈ¢ÑÊµãÂíåÊ≤ªÁñóÊñπÊ°àÈÄâÊã©Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÂ∞ÜÂçÉ‰∫øÁ∫ßÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â∞èÂûãÊ®°Âûã‰∏äÔºåÂèØ‰ª•Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨Ôºå‰ΩøÂÖ∂Êõ¥Êòì‰∫éÈÉ®ÁΩ≤Âú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠Ôºå‰æãÂ¶ÇÂåªÈô¢ÊàñËØäÊâÄ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÂºÄÂèëÈíàÂØπÁâπÂÆöÁôåÁóáÁ±ªÂûãÁöÑ‰∏ìÁî®Ê®°ÂûãÔºåÊèêÈ´òËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.

