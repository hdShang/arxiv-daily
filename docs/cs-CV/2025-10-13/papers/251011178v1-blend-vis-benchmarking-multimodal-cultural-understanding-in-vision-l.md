---
layout: default
title: "BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models"
---

# BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11178" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11178v1</a>
  <a href="https://arxiv.org/pdf/2510.11178.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11178v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11178v1', 'BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bryan Chen Zhengyu Tan, Zheng Weihua, Zhengyuan Liu, Nancy F. Chen, Hwaran Lee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee

**åˆ†ç±»**: cs.CV, cs.CY

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**å¤‡æ³¨**: Code and Dataset to be released

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BLEnD-Visï¼šæ„å»ºå¤šæ¨¡æ€æ–‡åŒ–ç†è§£åŸºå‡†ï¼Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–çŸ¥è¯†é²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æ–‡åŒ–ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `åŸºå‡†æ•°æ®é›†` `é²æ£’æ€§è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–çŸ¥è¯†ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹å¯¹æ–‡åŒ–èƒŒæ™¯çŸ¥è¯†çš„é²æ£’æ€§å’Œå¯è¿ç§»æ€§çš„æœ‰æ•ˆè¯„ä¼°ã€‚
2. BLEnD-Visé€šè¿‡æ„å»ºå¤šæ¨¡æ€ã€å¤šå…ƒæ–‡åŒ–åŸºå‡†ï¼Œåˆ©ç”¨è¯­è¨€é‡è¿°å’Œè§†è§‰æ¨¡æ€å˜åŒ–ï¼Œç³»ç»Ÿè¯„ä¼°VLMçš„æ–‡åŒ–çŸ¥è¯†ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼Œç°æœ‰VLMåœ¨æ–‡åŒ–çŸ¥è¯†ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—è„†å¼±æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è·¨æ¨¡æ€ä¸€è‡´æ€§å’Œä½èµ„æºåœ°åŒºçš„è¡¨ç°è¾ƒå·®ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨å…¨çƒèŒƒå›´å†…çš„éƒ¨ç½²ï¼Œå…¶ç†è§£æ–‡åŒ–èƒŒæ™¯çŸ¥è¯†çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦ä¾§é‡äºé™æ€å¬å›æˆ–å­¤ç«‹çš„è§†è§‰å®šä½ï¼Œæœªèƒ½å……åˆ†è¯„ä¼°VLMæ˜¯å¦å…·å¤‡é²æ£’ä¸”å¯è¿ç§»çš„æ–‡åŒ–ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†BLEnD-Visï¼Œä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šå…ƒæ–‡åŒ–åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°VLMåœ¨ä¸åŒè¯­è¨€è¡¨è¾¾å’Œè§†è§‰æ¨¡æ€ä¸‹å¯¹æ—¥å¸¸æ–‡åŒ–çŸ¥è¯†çš„é²æ£’æ€§ã€‚BLEnD-VisåŸºäºBLEnDæ•°æ®é›†ï¼Œæ„å»ºäº†313ä¸ªæ–‡åŒ–ç›¸å…³çš„é—®ç­”æ¨¡æ¿ï¼Œæ¶µç›–16ä¸ªåœ°åŒºï¼Œå¹¶ç”Ÿæˆäº†ä¸‰ç§å¯¹é½çš„å¤šé¡¹é€‰æ‹©é¢˜å½¢å¼ï¼š(i) ä»…æ–‡æœ¬åŸºçº¿ï¼ŒæŸ¥è¯¢åŒºåŸŸåˆ°å®ä½“ï¼›(ii) åè½¬çš„ä»…æ–‡æœ¬å˜ä½“ï¼ŒæŸ¥è¯¢å®ä½“åˆ°åŒºåŸŸï¼›(iii) VQAé£æ ¼çš„ç‰ˆæœ¬ï¼Œå¸¦æœ‰ç”Ÿæˆçš„å›¾åƒã€‚è¯¥åŸºå‡†åŒ…å«4,916å¼ å›¾åƒå’Œè¶…è¿‡21,000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜å®ä¾‹ï¼Œå¹¶é€šè¿‡äººå·¥æ ‡æ³¨è¿›è¡ŒéªŒè¯ã€‚BLEnD-Visæ­ç¤ºäº†å½“å‰VLMæ–‡åŒ–çŸ¥è¯†çš„æ˜¾è‘—è„†å¼±æ€§ï¼›æ¨¡å‹åœ¨è¯­è¨€é‡è¿°ä¸‹æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸”è™½ç„¶è§†è§‰çº¿ç´¢é€šå¸¸æœ‰åŠ©äºæé«˜æ€§èƒ½ï¼Œä½†è¾ƒä½çš„è·¨æ¨¡æ€ä¸€è‡´æ€§çªæ˜¾äº†åœ¨ç¨³å¥åœ°æ•´åˆæ–‡æœ¬å’Œè§†è§‰ç†è§£æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä½èµ„æºåœ°åŒºã€‚å› æ­¤ï¼ŒBLEnD-Visä¸ºç³»ç»Ÿåœ°åˆ†ææ–‡åŒ–é²æ£’æ€§å’Œå¤šæ¨¡æ€åŸºç¡€æä¾›äº†å…³é”®çš„æµ‹è¯•å¹³å°ï¼Œæš´éœ²äº†å±€é™æ€§ï¼Œå¹¶æŒ‡å¯¼å¼€å‘æ›´å…·æ–‡åŒ–èƒ½åŠ›çš„VLMã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨ç†è§£æ–‡åŒ–èƒŒæ™¯çŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é™æ€å¬å›æˆ–å­¤ç«‹çš„è§†è§‰å®šä½ï¼Œæ— æ³•å…¨é¢è¯„ä¼°VLMå¯¹æ–‡åŒ–çŸ¥è¯†çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ä½¿å¾—æˆ‘ä»¬éš¾ä»¥äº†è§£VLMæ˜¯å¦çœŸæ­£ç†è§£äº†æ–‡åŒ–çŸ¥è¯†ï¼Œä»¥åŠå®ƒä»¬åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡¨ç°å¦‚ä½•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šå…ƒæ–‡åŒ–çš„åŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡è®¾è®¡ä¸åŒçš„è¯­è¨€è¡¨è¾¾å’Œè§†è§‰æ¨¡æ€ï¼Œæ¥ç³»ç»Ÿåœ°è¯„ä¼°VLMå¯¹æ–‡åŒ–çŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è¯­è¨€é‡è¿°å’Œè§†è§‰ä¿¡æ¯ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è€ƒå¯ŸVLMåœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡¨ç°ï¼Œä»è€Œæ­ç¤ºå…¶åœ¨æ–‡åŒ–çŸ¥è¯†ç†è§£æ–¹é¢çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBLEnD-VisåŸºå‡†çš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) åŸºäºBLEnDæ•°æ®é›†ï¼Œæ„å»º313ä¸ªæ–‡åŒ–ç›¸å…³çš„é—®ç­”æ¨¡æ¿ï¼Œæ¶µç›–16ä¸ªåœ°åŒºã€‚2) é’ˆå¯¹æ¯ä¸ªé—®ç­”æ¨¡æ¿ï¼Œç”Ÿæˆä¸‰ç§å¤šé¡¹é€‰æ‹©é¢˜å½¢å¼ï¼šæ–‡æœ¬åˆ°å®ä½“ã€å®ä½“åˆ°æ–‡æœ¬ã€VQAé£æ ¼ã€‚3) å¯¹äºVQAé£æ ¼çš„é—®é¢˜ï¼Œç”Ÿæˆç›¸åº”çš„å›¾åƒã€‚4) é€šè¿‡äººå·¥æ ‡æ³¨å¯¹ç”Ÿæˆçš„æ•°æ®è¿›è¡ŒéªŒè¯ã€‚æœ€ç»ˆï¼ŒBLEnD-VisåŒ…å«4,916å¼ å›¾åƒå’Œè¶…è¿‡21,000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜å®ä¾‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šå…ƒæ–‡åŒ–çš„åŸºå‡†æ•°æ®é›†BLEnD-Visï¼Œä¸“é—¨ç”¨äºè¯„ä¼°VLMå¯¹æ–‡åŒ–çŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼ŒBLEnD-Visæ›´åŠ æ³¨é‡å¯¹æ–‡åŒ–çŸ¥è¯†çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„è¯„ä¼°ï¼Œé€šè¿‡å¼•å…¥è¯­è¨€é‡è¿°å’Œè§†è§‰ä¿¡æ¯ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è€ƒå¯ŸVLMåœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šBLEnD-Visçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šç§å¤šé¡¹é€‰æ‹©é¢˜å½¢å¼ï¼šæ–‡æœ¬åˆ°å®ä½“ã€å®ä½“åˆ°æ–‡æœ¬ã€VQAé£æ ¼ï¼Œå¯ä»¥ä»ä¸åŒè§’åº¦è¯„ä¼°VLMçš„æ–‡åŒ–çŸ¥è¯†ç†è§£èƒ½åŠ›ã€‚2) å›¾åƒç”Ÿæˆï¼šå¯¹äºVQAé£æ ¼çš„é—®é¢˜ï¼Œéœ€è¦ç”Ÿæˆç›¸åº”çš„å›¾åƒï¼Œè¿™éœ€è¦è€ƒè™‘å›¾åƒä¸é—®é¢˜ä¹‹é—´çš„ç›¸å…³æ€§ã€‚3) äººå·¥æ ‡æ³¨ï¼šé€šè¿‡äººå·¥æ ‡æ³¨å¯¹ç”Ÿæˆçš„æ•°æ®è¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿æ•°æ®çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰VLMåœ¨BLEnD-VisåŸºå‡†ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—çš„è„†å¼±æ€§ã€‚æ¨¡å‹åœ¨è¯­è¨€é‡è¿°ä¸‹çš„æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼Œä¸”è·¨æ¨¡æ€ä¸€è‡´æ€§è¾ƒä½ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºåœ°åŒºçš„è¡¨ç°æ›´å·®ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å½“å‰VLMåœ¨æ–‡åŒ–çŸ¥è¯†ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

BLEnD-VisåŸºå‡†å¯ç”¨äºè¯„ä¼°å’Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œä»è€Œä¿ƒè¿›æ›´å…·æ–‡åŒ–æ•æ„Ÿæ€§å’Œé€‚åº”æ€§çš„AIç³»ç»Ÿçš„å¼€å‘ã€‚è¿™å¯¹äºåœ¨å…¨çƒèŒƒå›´å†…éƒ¨ç½²çš„AIåº”ç”¨è‡³å…³é‡è¦ï¼Œä¾‹å¦‚æ™ºèƒ½åŠ©æ‰‹ã€è·¨æ–‡åŒ–äº¤æµå·¥å…·å’Œæ•™è‚²å¹³å°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As vision-language models (VLMs) are deployed globally, their ability to understand culturally situated knowledge becomes essential. Yet, existing evaluations largely assess static recall or isolated visual grounding, leaving unanswered whether VLMs possess robust and transferable cultural understanding. We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to evaluate the robustness of everyday cultural knowledge in VLMs across linguistic rephrasings and visual modalities. Building on the BLEnD dataset, BLEnD-Vis constructs 313 culturally grounded question templates spanning 16 regions and generates three aligned multiple-choice formats: (i) a text-only baseline querying from Region $\to$ Entity, (ii) an inverted text-only variant (Entity $\to$ Region), and (iii) a VQA-style version of (ii) with generated images. The resulting benchmark comprises 4,916 images and over 21,000 multiple-choice question (MCQ) instances, validated through human annotation. BLEnD-Vis reveals significant fragility in current VLM cultural knowledge; models exhibit performance drops under linguistic rephrasing and, whilst visual cues often aid performance, low cross-modal consistency highlights challenges in robustly integrating textual and visual understanding, particularly for lower-resource regions. BLEnD-Vis thus provides a crucial testbed for systematically analysing cultural robustness and multimodal grounding, exposing limitations and guiding the development of more culturally competent VLMs.

