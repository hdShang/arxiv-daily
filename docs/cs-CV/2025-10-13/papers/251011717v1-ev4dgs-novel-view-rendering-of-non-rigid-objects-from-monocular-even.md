---
layout: default
title: "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams"
---

# Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11717" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11717v1</a>
  <a href="https://arxiv.org/pdf/2510.11717.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11717v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11717v1', 'Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**æœŸåˆŠ**: British Machine Vision Conference (BMVC) 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEv4DGSä»¥è§£å†³å•ç›®äº‹ä»¶æµä¸‹éåˆšæ€§ç‰©ä½“çš„æ–°è§†è§’æ¸²æŸ“é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `äº‹ä»¶ç›¸æœº` `æ–°è§†è§’æ¸²æŸ“` `éåˆšæ€§ç‰©ä½“` `å•ç›®äº‹ä»¶æµ` `3Då˜å½¢æ¨¡å‹` `è®¡ç®—æœºè§†è§‰` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éåˆšæ€§ç‰©ä½“æ—¶ä¾èµ–ç¨€ç–RGBè¾“å…¥ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚
2. Ev4DGSé€šè¿‡å•ç›®äº‹ä»¶æµç›´æ¥æ¸²æŸ“éåˆšæ€§ç‰©ä½“ï¼Œé¿å…äº†å¯¹RGBå›¾åƒçš„ä¾èµ–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEv4DGSåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡ä¼˜äºå¤šä¸ªåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äº‹ä»¶ç›¸æœºåœ¨æ–°è§†è§’æ¸²æŸ“æ–¹é¢ç›¸è¾ƒäºåŒæ­¥å·¥ä½œçš„RGBç›¸æœºå…·æœ‰å¤šç§ä¼˜åŠ¿ã€‚å°½ç®¡å·²æœ‰é’ˆå¯¹åˆšæ€§åœºæ™¯çš„é«˜æ•ˆäº‹ä»¶é©±åŠ¨æŠ€æœ¯ï¼Œä½†åœ¨éåˆšæ€§ç‰©ä½“çš„æƒ…å†µä¸‹ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ç¨€ç–çš„RGBè¾“å…¥ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨æ˜¾è‘—é™åˆ¶ã€‚æœ¬æ–‡æå‡ºEv4DGSï¼Œé¦–æ¬¡å®ç°äº†ä»å•ç›®äº‹ä»¶æµä¸­æ¸²æŸ“éåˆšæ€§å˜å½¢ç‰©ä½“çš„RGBæˆ–ç°åº¦å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡æŸå¤±å‡½æ•°å°†ä¼°è®¡æ¨¡å‹çš„è¾“å‡ºä¸2Däº‹ä»¶è§‚æµ‹ç©ºé—´å…³è”ï¼Œå¹¶åˆ©ç”¨ä»äº‹ä»¶ç”Ÿæˆçš„äºŒè¿›åˆ¶æ©è†œè®­ç»ƒç²—ç•¥çš„3Då˜å½¢æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEv4DGSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»å•ç›®äº‹ä»¶æµä¸­æ¸²æŸ“éåˆšæ€§å˜å½¢ç‰©ä½“çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ç¨€ç–çš„RGBè¾“å…¥ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEv4DGSé€šè¿‡å›å½’å¯å˜å½¢çš„3Dé«˜æ–¯ç‚¹äº‘è¡¨ç¤ºï¼Œåˆ©ç”¨äº‹ä»¶æµç›´æ¥ç”ŸæˆRGBæˆ–ç°åº¦å›¾åƒï¼Œé¿å…äº†å¯¹RGBå›¾åƒçš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼š1) é€šè¿‡æŸå¤±å‡½æ•°å°†æ¨¡å‹è¾“å‡ºä¸2Däº‹ä»¶è§‚æµ‹ç©ºé—´å…³è”ï¼›2) ä»äº‹ä»¶ç”Ÿæˆçš„äºŒè¿›åˆ¶æ©è†œä¸­è®­ç»ƒç²—ç•¥çš„3Då˜å½¢æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šEv4DGSæ˜¯é¦–ä¸ªä»…åŸºäºå•ç›®äº‹ä»¶æµå®ç°éåˆšæ€§ç‰©ä½“æ–°è§†è§’æ¸²æŸ“çš„æ–¹æ³•ï¼Œçªç ´äº†å¯¹RGBè¾“å…¥çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚

**å…³é”®è®¾è®¡**ï¼šæŸå¤±å‡½æ•°è®¾è®¡ä¸ºå°†æ¨¡å‹è¾“å‡ºä¸2Däº‹ä»¶è§‚æµ‹ç©ºé—´è¿›è¡Œæœ‰æ•ˆå…³è”ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸å®é™…è§‚æµ‹ç›¸ç¬¦ã€‚åŒæ—¶ï¼Œé‡‡ç”¨ä»äº‹ä»¶ç”Ÿæˆçš„äºŒè¿›åˆ¶æ©è†œæ¥è®­ç»ƒ3Då˜å½¢æ¨¡å‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEv4DGSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºå¤šä¸ªåŸºçº¿æ–¹æ³•ï¼Œå…¶æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡å±•ç¤ºäº†è‰¯å¥½çš„æ•ˆæœï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººè§†è§‰ã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡å®ç°éåˆšæ€§ç‰©ä½“çš„æ–°è§†è§’æ¸²æŸ“ï¼ŒEv4DGSå¯ä»¥æå‡è¿™äº›é¢†åŸŸä¸­çš„ç‰©ä½“è¯†åˆ«ã€è·Ÿè¸ªå’Œäº¤äº’èƒ½åŠ›ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. This paper sheds light on this challenging open question and introduces Ev4DGS, i.e., the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or greyscale images) from monocular event streams. Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting. We will release our models and the datasets used in the evaluation for research purposes; see the project webpage: https://4dqv.mpi-inf.mpg.de/Ev4DGS/.

