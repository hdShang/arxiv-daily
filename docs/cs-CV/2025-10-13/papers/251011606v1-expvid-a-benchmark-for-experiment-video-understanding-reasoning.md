---
layout: default
title: "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning"
---

# ExpVid: A Benchmark for Experiment Video Understanding & Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11606" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11606v1</a>
  <a href="https://arxiv.org/pdf/2510.11606.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11606v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11606v1', 'ExpVid: A Benchmark for Experiment Video Understanding & Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**å¤‡æ³¨**: Data & Code: https://github.com/OpenGVLab/ExpVid

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ExpVidï¼šç”¨äºå®éªŒè§†é¢‘ç†è§£ä¸æ¨ç†çš„åŸºå‡†æ•°æ®é›†ï¼ŒæŒ‘æˆ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦å®éªŒä¸­çš„åº”ç”¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®éªŒè§†é¢‘ç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç§‘å­¦æ¨ç†` `åŸºå‡†æ•°æ®é›†` `è§†è§‰ä¸­å¿ƒ` `ç»†ç²’åº¦æ„ŸçŸ¥` `ç¨‹åºç†è§£` `äººå·¥æ™ºèƒ½è¾…åŠ©ç§‘ç ”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†æµ‹è¯•ç¼ºä¹å¯¹çœŸå®å®éªŒå®¤ç¯å¢ƒä¸­ç»†ç²’åº¦å’Œé•¿æ—¶é—´åºåˆ—å®éªŒçš„è¯„ä¼°ï¼Œæ— æ³•å‡†ç¡®è¡¡é‡MLLMåœ¨ç§‘å­¦é¢†åŸŸçš„æ½œåŠ›ã€‚
2. ExpVidé€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«ç»†ç²’åº¦æ„ŸçŸ¥ã€ç¨‹åºç†è§£å’Œç§‘å­¦æ¨ç†ä¸‰çº§ä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†ï¼Œç³»ç»Ÿåœ°è¯„ä¼°MLLMåœ¨å®éªŒè§†é¢‘ä¸Šçš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰MLLMåœ¨ç»†ç²’åº¦è¯†åˆ«ã€çŠ¶æ€è·Ÿè¸ªå’Œç§‘å­¦æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”ä¸“æœ‰æ¨¡å‹åœ¨é«˜é˜¶æ¨ç†æ–¹é¢ä¼˜äºå¼€æºæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)æœ‰æœ›é€šè¿‡è§£é‡Šå¤æ‚çš„å®éªŒç¨‹åºæ¥åŠ é€Ÿç§‘å­¦å‘ç°ã€‚ç„¶è€Œï¼Œç”±äºç°æœ‰åŸºå‡†å¿½ç•¥äº†çœŸå®å®éªŒå®¤å·¥ä½œï¼Œç‰¹åˆ«æ˜¯æ¹¿å®éªŒå®¤ç¯å¢ƒä¸­ç»†ç²’åº¦å’Œé•¿æ—¶ç¨‹çš„ç‰¹æ€§ï¼Œå› æ­¤å¯¹å®ƒä»¬çš„èƒ½åŠ›äº†è§£ä¸è¶³ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ExpVidï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°MLLMåœ¨ç§‘å­¦å®éªŒè§†é¢‘ä¸Šçš„åŸºå‡†ã€‚ExpVidä»åŒè¡Œè¯„å®¡çš„è§†é¢‘å‡ºç‰ˆç‰©ä¸­æ•´ç†è€Œæ¥ï¼Œå…·æœ‰ä¸€ä¸ªæ–°çš„ä¸‰çº§ä»»åŠ¡å±‚æ¬¡ç»“æ„ï¼Œåæ˜ äº†ç§‘å­¦è¿‡ç¨‹ï¼š(1)å¯¹å·¥å…·ã€ææ–™å’ŒåŠ¨ä½œçš„ç»†ç²’åº¦æ„ŸçŸ¥ï¼›(2)å¯¹æ­¥éª¤é¡ºåºå’Œå®Œæ•´æ€§çš„ç¨‹åºç†è§£ï¼›(3)å°†å®Œæ•´å®éªŒä¸å…¶å·²å‘è¡¨ç»“è®ºè”ç³»èµ·æ¥çš„ç§‘å­¦æ¨ç†ã€‚æˆ‘ä»¬çš„è§†è§‰ä¸­å¿ƒæ³¨é‡Šæµç¨‹ï¼Œç»“åˆäº†è‡ªåŠ¨ç”Ÿæˆå’Œå¤šå­¦ç§‘ä¸“å®¶éªŒè¯ï¼Œç¡®ä¿ä»»åŠ¡éœ€è¦è§†è§‰åŸºç¡€ã€‚æˆ‘ä»¬åœ¨ExpVidä¸Šè¯„ä¼°äº†19ä¸ªé¢†å…ˆçš„MLLMï¼Œå‘ç°å®ƒä»¬æ“…é•¿ç²—ç²’åº¦è¯†åˆ«ï¼Œä½†åœ¨æ¶ˆé™¤ç»†å¾®ç»†èŠ‚çš„æ­§ä¹‰ã€è·Ÿè¸ªéšæ—¶é—´çš„çŠ¶æ€å˜åŒ–ä»¥åŠå°†å®éªŒç¨‹åºä¸ç§‘å­¦ç»“æœè”ç³»èµ·æ¥æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é˜¶æ¨ç†æ–¹é¢ã€‚ExpVidä¸ä»…æä¾›äº†ä¸€ç§è¯Šæ–­å·¥å…·ï¼Œè¿˜ä¸ºå¼€å‘èƒ½å¤Ÿæˆä¸ºç§‘å­¦å®éªŒä¸­å€¼å¾—ä¿¡èµ–çš„åˆä½œä¼™ä¼´çš„MLLMè§„åˆ’äº†è·¯çº¿å›¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰MLLMåœ¨ç†è§£å’Œæ¨ç†ç§‘å­¦å®éªŒè§†é¢‘æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ä½“ç°åœ¨æ— æ³•å‡†ç¡®è¯†åˆ«ç»†ç²’åº¦çš„å®éªŒæ­¥éª¤ã€å·¥å…·å’Œææ–™ï¼Œéš¾ä»¥è·Ÿè¸ªå®éªŒè¿‡ç¨‹ä¸­çš„çŠ¶æ€å˜åŒ–ï¼Œä»¥åŠæ— æ³•å°†å®éªŒè¿‡ç¨‹ä¸æœ€ç»ˆçš„ç§‘å­¦ç»“è®ºè”ç³»èµ·æ¥ã€‚ç°æœ‰åŸºå‡†æ•°æ®é›†ç¼ºä¹å¯¹è¿™äº›èƒ½åŠ›çš„é’ˆå¯¹æ€§è¯„ä¼°ï¼Œé˜»ç¢äº†MLLMåœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šExpVidçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªæ›´è´´è¿‘çœŸå®ç§‘ç ”åœºæ™¯çš„åŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡è®¾è®¡åŒ…å«ç»†ç²’åº¦æ„ŸçŸ¥ã€ç¨‹åºç†è§£å’Œç§‘å­¦æ¨ç†ä¸‰ä¸ªå±‚çº§çš„ä»»åŠ¡ï¼Œå…¨é¢è¯„ä¼°MLLMåœ¨ç†è§£å’Œæ¨ç†å®éªŒè§†é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹å¼ºè°ƒè§†è§‰åŸºç¡€ï¼Œç¡®ä¿æ¨¡å‹éœ€è¦åˆ©ç”¨è§†è§‰ä¿¡æ¯æ¥å®Œæˆä»»åŠ¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šExpVidæ•°æ®é›†çš„æ„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä»åŒè¡Œè¯„å®¡çš„è§†é¢‘å‡ºç‰ˆç‰©ä¸­æ”¶é›†å®éªŒè§†é¢‘ï¼›2) è®¾è®¡ä¸‰çº§ä»»åŠ¡å±‚æ¬¡ç»“æ„ï¼ŒåŒ…æ‹¬ç»†ç²’åº¦æ„ŸçŸ¥ã€ç¨‹åºç†è§£å’Œç§‘å­¦æ¨ç†ï¼›3) é‡‡ç”¨è§†è§‰ä¸­å¿ƒæ³¨é‡Šæµç¨‹ï¼Œç»“åˆè‡ªåŠ¨ç”Ÿæˆå’Œå¤šå­¦ç§‘ä¸“å®¶éªŒè¯ï¼Œä¸ºæ¯ä¸ªè§†é¢‘ç”Ÿæˆé«˜è´¨é‡çš„æ ‡æ³¨ï¼›4) è¯„ä¼°ç°æœ‰MLLMåœ¨ExpVidä¸Šçš„æ€§èƒ½ï¼Œå¹¶åˆ†æå…¶ä¼˜ç¼ºç‚¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šExpVidçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é’ˆå¯¹ç§‘å­¦å®éªŒè§†é¢‘ç†è§£ä¸æ¨ç†ä»»åŠ¡çš„ç‰¹æ€§ï¼Œè®¾è®¡äº†ä¸‰çº§ä»»åŠ¡å±‚æ¬¡ç»“æ„ï¼Œå¹¶é‡‡ç”¨äº†è§†è§‰ä¸­å¿ƒæ³¨é‡Šæµç¨‹ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæ›´å…¨é¢ã€æ›´å‡†ç¡®åœ°è¯„ä¼°MLLMåœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒExpVidæ˜¯é¦–ä¸ªä¸“æ³¨äºå®éªŒè§†é¢‘ç†è§£çš„åŸºå‡†æ•°æ®é›†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚

**å…³é”®è®¾è®¡**ï¼šExpVidçš„ä¸‰çº§ä»»åŠ¡å±‚æ¬¡ç»“æ„æ˜¯å…¶å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡è¦æ±‚æ¨¡å‹è¯†åˆ«å®éªŒä¸­çš„å·¥å…·ã€ææ–™å’ŒåŠ¨ä½œï¼›ç¨‹åºç†è§£ä»»åŠ¡è¦æ±‚æ¨¡å‹ç†è§£å®éªŒæ­¥éª¤çš„é¡ºåºå’Œå®Œæ•´æ€§ï¼›ç§‘å­¦æ¨ç†ä»»åŠ¡è¦æ±‚æ¨¡å‹å°†å®éªŒè¿‡ç¨‹ä¸æœ€ç»ˆçš„ç§‘å­¦ç»“è®ºè”ç³»èµ·æ¥ã€‚æ­¤å¤–ï¼Œè§†è§‰ä¸­å¿ƒæ³¨é‡Šæµç¨‹ä¹Ÿè‡³å…³é‡è¦ï¼Œå®ƒç¡®ä¿æ ‡æ³¨ä¿¡æ¯ä¸è§†é¢‘å†…å®¹ç´§å¯†ç›¸å…³ï¼Œé¿å…æ¨¡å‹ä»…ä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ExpVidå¯¹19ä¸ªé¢†å…ˆçš„MLLMè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹åœ¨ç²—ç²’åº¦è¯†åˆ«æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç»†ç²’åº¦ç»†èŠ‚åŒºåˆ†ã€æ—¶é—´çŠ¶æ€è·Ÿè¸ªä»¥åŠå®éªŒç¨‹åºä¸ç§‘å­¦ç»“æœçš„å…³è”æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚ä¸“æœ‰æ¨¡å‹åœ¨é«˜é˜¶æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜æ˜¾ä¼˜äºå¼€æºæ¨¡å‹ï¼Œæ­ç¤ºäº†å½“å‰MLLMåœ¨ç§‘å­¦æ¨ç†èƒ½åŠ›ä¸Šçš„å·®è·ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ExpVidçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘èƒ½å¤Ÿè¾…åŠ©ç§‘å­¦å®¶è¿›è¡Œå®éªŒè®¾è®¡ã€æ•°æ®åˆ†æå’Œç»“æœè§£é‡Šçš„æ™ºèƒ½ç³»ç»Ÿã€‚è¿™äº›ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨ç†è§£å®éªŒè§†é¢‘ï¼Œæå–å…³é”®ä¿¡æ¯ï¼Œå¹¶æä¾›æœ‰ä»·å€¼çš„å»ºè®®ï¼Œä»è€ŒåŠ é€Ÿç§‘å­¦å‘ç°çš„è¿›ç¨‹ã€‚æ­¤å¤–ï¼ŒExpVidè¿˜å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ”¹è¿›MLLMåœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨èƒ½åŠ›ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) hold promise for accelerating scientific discovery by interpreting complex experimental procedures. However, their true capabilities are poorly understood, as existing benchmarks neglect the fine-grained and long-horizon nature of authentic laboratory work, especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the first benchmark designed to systematically evaluate MLLMs on scientific experiment videos. Curated from peer-reviewed video publications, ExpVid features a new three-level task hierarchy that mirrors the scientific process: (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural Understanding of step order and completeness; and (3) Scientific Reasoning that connects the full experiment to its published conclusions. Our vision-centric annotation pipeline, combining automated generation with multi-disciplinary expert validation, ensures that tasks require visual grounding. We evaluate 19 leading MLLMs on ExpVid and find that while they excel at coarse-grained recognition, they struggle with disambiguating fine details, tracking state changes over time, and linking experimental procedures to scientific outcomes. Our results reveal a notable performance gap between proprietary and open-source models, particularly in high-order reasoning. ExpVid not only provides a diagnostic tool but also charts a roadmap for developing MLLMs capable of becoming trustworthy partners in scientific experimentation.

