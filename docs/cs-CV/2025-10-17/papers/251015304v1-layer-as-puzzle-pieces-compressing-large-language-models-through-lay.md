---
layout: default
title: "Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation"
---

# Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15304" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15304v1</a>
  <a href="https://arxiv.org/pdf/2510.15304.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15304v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15304v1', 'Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fei Wang, Li Shen, Liang Ding, Chao Xue, Ye Liu, Changxing Ding

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-17

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MPI-Lab/CoMe)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCoMeï¼šé€šè¿‡å±‚æ‹¼æ¥å‹ç¼©å¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ˜¾è‘—å‰ªæçš„åŒæ—¶ä¿æŒæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹å‹ç¼©` `ç»“æ„åŒ–å‰ªæ` `å±‚æ‹¼æ¥` `çŸ¥è¯†è’¸é¦` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§æ¨¡å‹å‰ªææ–¹æ³•ç›´æ¥ç§»é™¤å±‚æˆ–ç®€å•èšåˆæƒé‡ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸”ç¼ºä¹æœ‰æ•ˆçš„åè®­ç»ƒæ¢å¤æœºåˆ¶ã€‚
2. CoMeé€šè¿‡é€šé“æ•æ„Ÿåº¦æŒ‡æ ‡é€‰æ‹©é‡è¦é€šé“ï¼Œåˆ©ç”¨å±‚æ‹¼æ¥èåˆç›¸é‚»å±‚ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚è’¸é¦è¿›è¡ŒçŸ¥è¯†è¿ç§»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCoMeåœ¨å‰ªæLLaMA-2-7b 30%å‚æ•°åï¼Œä»èƒ½ä¿æŒåŸå§‹æ¨¡å‹83%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¾¾åˆ°SOTAæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å¯¼è‡´äº†é«˜æ˜‚çš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ã€‚ç›®å‰çš„å·¥ä½œè¯•å›¾é€šè¿‡é€å±‚ç»“æ„åŒ–å‰ªææ¥å‡å°æ¨¡å‹å°ºå¯¸ï¼Œä½†å¾€å¾€å¿½ç•¥äº†ä¿ç•™è¢«å‰ªæéƒ¨åˆ†çš„èƒ½åŠ›ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†ç»“æ„åŒ–å‰ªæèŒƒå¼ï¼Œæ­ç¤ºäº†å‡ ä¸ªå…³é”®é™åˆ¶ï¼š1) ç›´æ¥ç§»é™¤å±‚å¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼›2) çº¿æ€§æƒé‡å±‚èšåˆèƒ½åŠ›ä¸è¶³ï¼›3) ç¼ºä¹æœ‰æ•ˆçš„åè®­ç»ƒæ¢å¤æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†CoMeï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ¸è¿›å¼å±‚å‰ªææ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰åŸºäºæ‹¼æ¥çš„åˆå¹¶æŠ€æœ¯å’Œåˆ†å±‚è’¸é¦åè®­ç»ƒè¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€šé“æ•æ„Ÿåº¦æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åˆ©ç”¨æ¿€æ´»å¼ºåº¦å’Œæƒé‡èŒƒæ•°è¿›è¡Œç»†ç²’åº¦çš„é€šé“é€‰æ‹©ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ‹¼æ¥çš„å±‚åˆå¹¶æ–¹æ³•æ¥èåˆç›¸é‚»å±‚ä¸­æœ€å…³é”®çš„é€šé“ï¼Œä»è€Œå®ç°æ¨¡å‹å°ºå¯¸çš„é€æ­¥å‡å°ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚è’¸é¦åè®®ï¼Œè¯¥åè®®åˆ©ç”¨åœ¨å‰ªææœŸé—´å»ºç«‹çš„åŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹å±‚ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoMe å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å½“å‰ªæ LLaMA-2-7b çš„ 30% å‚æ•°æ—¶ï¼Œå‰ªæåçš„æ¨¡å‹ä¿ç•™äº†å…¶åŸå§‹å¹³å‡å‡†ç¡®ç‡çš„ 83%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½“ç§¯è¿‡å¤§ï¼Œå¯¼è‡´è®¡ç®—å’Œå­˜å‚¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œå¦‚ç›´æ¥ç§»é™¤å±‚æˆ–ä½¿ç”¨ç®€å•çš„çº¿æ€§æƒé‡èšåˆï¼Œä¼šå¯¼è‡´ä¸¥é‡çš„æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸”ç¼ºä¹æœ‰æ•ˆçš„åè®­ç»ƒæ¢å¤æœºåˆ¶ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è¢«å‰ªæéƒ¨åˆ†è•´å«çš„çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ¸è¿›å¼çš„å±‚å‰ªæå’Œæ‹¼æ¥åˆå¹¶ï¼Œä»¥åŠåˆ†å±‚è’¸é¦ï¼Œåœ¨å‡å°æ¨¡å‹å°ºå¯¸çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡é€šé“æ•æ„Ÿåº¦æŒ‡æ ‡é€‰æ‹©é‡è¦çš„é€šé“ï¼Œå¹¶å°†å…¶æ‹¼æ¥åˆå¹¶åˆ°ç›¸é‚»å±‚ï¼Œä»è€Œå®ç°æ¨¡å‹çš„å‹ç¼©ã€‚åˆ†å±‚è’¸é¦åˆ™ç”¨äºå°†åŸå§‹æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å‹ç¼©åçš„æ¨¡å‹ï¼Œå¼¥è¡¥å‰ªæå¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoMeæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **æ¸è¿›å¼å±‚å‰ªæ**ï¼šä½¿ç”¨é€šé“æ•æ„Ÿåº¦æŒ‡æ ‡é€‰æ‹©è¦å‰ªæçš„é€šé“ã€‚2) **åŸºäºæ‹¼æ¥çš„å±‚åˆå¹¶**ï¼šå°†ç›¸é‚»å±‚ä¸­é€‰æ‹©å‡ºçš„é‡è¦é€šé“è¿›è¡Œæ‹¼æ¥åˆå¹¶ï¼Œå‡å°‘æ¨¡å‹å‚æ•°é‡ã€‚3) **åˆ†å±‚è’¸é¦**ï¼šåˆ©ç”¨åŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹å±‚ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œè¿›è¡ŒçŸ¥è¯†è¿ç§»ï¼Œæå‡å‰ªæåæ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoMeçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **åŸºäºæ‹¼æ¥çš„å±‚åˆå¹¶æ–¹æ³•**ï¼šä¸åŒäºä¼ ç»Ÿçš„æƒé‡èšåˆæ–¹æ³•ï¼ŒCoMeé€šè¿‡æ‹¼æ¥é€šé“æ¥èåˆç›¸é‚»å±‚ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿ç•™è¢«å‰ªæéƒ¨åˆ†çš„ä¿¡æ¯ã€‚2) **åˆ†å±‚è’¸é¦åè®®**ï¼šåˆ©ç”¨å‰ªæè¿‡ç¨‹ä¸­å»ºç«‹çš„åŸå§‹æ¨¡å‹å’Œå‰ªææ¨¡å‹å±‚ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œè¿›è¡Œæ›´æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚

**å…³é”®è®¾è®¡**ï¼šé€šé“æ•æ„Ÿåº¦æŒ‡æ ‡ç»“åˆäº†æ¿€æ´»å¼ºåº¦å’Œæƒé‡èŒƒæ•°ï¼Œç”¨äºç»†ç²’åº¦çš„é€šé“é€‰æ‹©ã€‚æ‹¼æ¥åˆå¹¶æ“ä½œå°†ç›¸é‚»å±‚çš„é‡è¦é€šé“åœ¨é€šé“ç»´åº¦ä¸Šè¿›è¡Œæ‹¼æ¥ã€‚åˆ†å±‚è’¸é¦æŸå¤±å‡½æ•°åŒ…æ‹¬äº†ä¸­é—´å±‚ç‰¹å¾çš„è’¸é¦æŸå¤±ï¼Œä»¥åŠæœ€ç»ˆè¾“å‡ºçš„è’¸é¦æŸå¤±ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°æƒé‡å’Œè®­ç»ƒepochç­‰è¶…å‚æ•°éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CoMeåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚åœ¨å‰ªæLLaMA-2-7bæ¨¡å‹30%çš„å‚æ•°åï¼Œå‰ªæåçš„æ¨¡å‹ä»ç„¶ä¿ç•™äº†åŸå§‹æ¨¡å‹83%çš„å¹³å‡å‡†ç¡®ç‡ã€‚ç›¸è¾ƒäºå…¶ä»–å‰ªææ–¹æ³•ï¼ŒCoMeåœ¨ç›¸åŒå‰ªææ¯”ä¾‹ä¸‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹çš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—è®¾å¤‡ç­‰èµ„æºå—é™çš„ç¯å¢ƒã€‚é€šè¿‡å‹ç¼©æ¨¡å‹å°ºå¯¸ï¼Œå¯ä»¥é™ä½è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ï¼Œæé«˜æ¨ç†é€Ÿåº¦ï¼Œä»è€Œä½¿LLMèƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„é¢†åŸŸå¾—åˆ°åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ¨¡å‹åŠ é€Ÿå’Œæ¨¡å‹å®‰å…¨ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models excel at natural language processing tasks, but their massive size leads to high computational and storage demands. Recent works have sought to reduce their model size through layer-wise structured pruning. However, they tend to ignore retaining the capabilities in the pruned part. In this work, we re-examine structured pruning paradigms and uncover several key limitations: 1) notable performance degradation due to direct layer removal, 2) incompetent linear weight layer aggregation, and 3) the lack of effective post-training recovery mechanisms. To address these limitations, we propose CoMe, including a progressive layer pruning framework with a Concatenation-based Merging technology and a hierarchical distillation post-training process. Specifically, we introduce a channel sensitivity metric that utilizes activation intensity and weight norms for fine-grained channel selection. Subsequently, we employ a concatenation-based layer merging method to fuse the most critical channels across adjacent layers, enabling progressive model size reduction. Finally, we propose a hierarchical distillation protocol that leverages the correspondences between the original and pruned model layers established during pruning, thereby enabling efficient knowledge transfer. Experiments on seven benchmarks show that CoMe achieves state-of-the-art performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model retains 83% of its original average accuracy. Our code is available at https://github.com/MPI-Lab/CoMe.

