---
layout: default
title: "DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification"
---

# DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15725" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.15725v1</a>
  <a href="https://arxiv.org/pdf/2510.15725.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15725v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15725v1', 'DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig

**ÂàÜÁ±ª**: cs.CV, cs.AI, eess.IV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-17

**Â§áÊ≥®**: 9 pages, accepted at ACMMM2025 SUMAC

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/linty5/DGME-T)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DGME-TÔºåÈÄöËøáÊñπÂêëÁΩëÊ†ºËøêÂä®ÁºñÁ†ÅÂ¢ûÂº∫TransformerÂú®ÂéÜÂè≤ÂΩ±ÂÉèÈïúÂ§¥ËøêÂä®ÂàÜÁ±ª‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `ÈïúÂ§¥ËøêÂä®ÂàÜÁ±ª` `ÂéÜÂè≤ÂΩ±ÂÉèÂàÜÊûê` `Transformer` `ÂÖâÊµÅ` `ËøêÂä®ÁºñÁ†Å` `ËßÜÈ¢ëÁêÜËß£` `Ë∑®ÂüüÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈïúÂ§¥ËøêÂä®ÂàÜÁ±ªÊ®°ÂûãÂú®Â§ÑÁêÜÂéÜÂè≤ÂΩ±ÂÉèÊó∂ÔºåÁî±‰∫éÂô™Â£∞„ÄÅ‰∏¢Â∏ßÁ≠âÈóÆÈ¢òÔºåÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇ
2. DGME-TÈÄöËøáÂºïÂÖ•ÊñπÂêëÁΩëÊ†ºËøêÂä®ÁºñÁ†ÅÔºåÂπ∂ÈááÁî®ÂèØÂ≠¶‰π†ÁöÑÂêéÊúüËûçÂêàÂ±ÇÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂØπËøêÂä®‰ø°ÊÅØÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDGME-TÂú®Áé∞‰ª£ÂíåÂéÜÂè≤ÂΩ±ÂÉè‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂéÜÂè≤ÂΩ±ÂÉè‰∏ä„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÈíàÂØπÂú®Áé∞‰ª£È´òË¥®ÈáèËßÜÈ¢ë‰∏äËÆ≠ÁªÉÁöÑÈïúÂ§¥ËøêÂä®ÂàÜÁ±ª(CMC)Ê®°ÂûãÂ∫îÁî®‰∫éÂéÜÂè≤ÂΩ±ÂÉèÊó∂ÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢òÔºåÂéÜÂè≤ÂΩ±ÂÉèÈÄöÂ∏∏Â≠òÂú®Âô™Â£∞„ÄÅ‰∏¢Â∏ßÂíå‰ΩéÂØπÊØîÂ∫¶Á≠âÈóÆÈ¢òÔºåÂØºËá¥ËøêÂä®Á∫øÁ¥¢Ê®°Á≥ä„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂü∫ÂáÜÔºåÂ∞Ü‰∏§‰∏™Áé∞‰ª£ËØ≠ÊñôÂ∫ìÊï¥Âêà‰∏∫Âõõ‰∏™Ê†áÂáÜÁ±ªÂà´ÔºåÂπ∂Â∞ÜHISTORIANÊï∞ÊçÆÈõÜÈáçÊûÑ‰∏∫‰∫î‰∏™Âπ≥Ë°°Á±ªÂà´„ÄÇÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜDGME-TÔºåÂÆÉÊòØVideo Swin TransformerÁöÑ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÊâ©Â±ïÔºåÈÄöËøáÂèØÂ≠¶‰π†ÂíåÂΩí‰∏ÄÂåñÁöÑÂêéÊúüËûçÂêàÂ±ÇÊ≥®ÂÖ•‰ªéÂÖâÊµÅ‰∏≠ÊèêÂèñÁöÑÊñπÂêëÁΩëÊ†ºËøêÂä®ÁºñÁ†Å„ÄÇDGME-TÂú®Áé∞‰ª£ËßÜÈ¢ëÁâáÊÆµ‰∏äÁöÑtop-1ÂáÜÁ°ÆÁéá‰ªé81.78%ÊèêÈ´òÂà∞86.14%ÔºåÂÆèF1‰ªé82.08%ÊèêÈ´òÂà∞87.81%ÔºåÂêåÊó∂Âú®‰∫åÊàòÊó∂ÊúüÂΩ±ÂÉè‰∏äÁöÑÂáÜÁ°ÆÁéá‰ªé83.43%ÊèêÈ´òÂà∞84.62%ÔºåÂÆèF1‰ªé81.72%ÊèêÈ´òÂà∞82.63%„ÄÇË∑®ÂüüÁ†îÁ©∂Ëøõ‰∏ÄÊ≠•Ë°®ÊòéÔºåÂú®Áé∞‰ª£Êï∞ÊçÆ‰∏äËøõË°å‰∏≠Èó¥ÂæÆË∞ÉÈò∂ÊÆµÂèØÂ∞ÜÂéÜÂè≤ÂΩ±ÂÉèÁöÑÊÄßËÉΩÊèêÈ´ò‰∫î‰∏™ÁôæÂàÜÁÇπ‰ª•‰∏ä„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÁªìÊûÑÂåñËøêÂä®ÂÖàÈ™åÂíåTransformerË°®Á§∫ÊòØ‰∫íË°•ÁöÑÔºåÂç≥‰ΩøÊòØ‰∏Ä‰∏™Â∞èÁöÑ„ÄÅÁªèËøá‰ªîÁªÜÊ†°ÂáÜÁöÑËøêÂä®Â§¥‰πüÂèØ‰ª•ÊòæËëóÊèêÈ´òÈÄÄÂåñÂΩ±ÂÉèÂàÜÊûêÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÁõ∏ÂÖ≥ËµÑÊ∫êÂèØÂú®https://github.com/linty5/DGME-TËé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïúÂ§¥ËøêÂä®ÂàÜÁ±ª‰ªªÂä°Âú®ÂéÜÂè≤ÂΩ±ÂÉè‰∏äÁöÑÊÄßËÉΩÁì∂È¢à„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Áé∞‰ª£È´òË¥®ÈáèËßÜÈ¢ë‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÁõ¥Êé•Â∫îÁî®‰∫éÂéÜÂè≤ÂΩ±ÂÉèÊó∂ÔºåÁî±‰∫éÂô™Â£∞„ÄÅ‰ΩéÂØπÊØîÂ∫¶„ÄÅ‰∏¢Â∏ßÁ≠âÈóÆÈ¢òÔºåÂØºËá¥ËøêÂä®‰ø°ÊÅØÈöæ‰ª•ÊèêÂèñÔºåÊ®°ÂûãÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•ÁªìÊûÑÂåñÁöÑËøêÂä®ÂÖàÈ™åÁü•ËØÜÔºåÂç≥ÊñπÂêëÁΩëÊ†ºËøêÂä®ÁºñÁ†Å(DGME)ÔºåÊù•Â¢ûÂº∫Ê®°ÂûãÂØπËøêÂä®‰ø°ÊÅØÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇDGME‰ªéÂÖâÊµÅ‰∏≠ÊèêÂèñÔºåËÉΩÂ§üÊúâÊïàÂú∞ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑËøêÂä®Ê®°Âºè„ÄÇÂêåÊó∂ÔºåÈááÁî®TransformerÊû∂ÊûÑÊù•Âª∫Ê®°ËßÜÈ¢ëÁöÑÊó∂Â∫èÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDGME-TÂü∫‰∫éVideo Swin TransformerÊû∂ÊûÑÔºå‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ËßÜÈ¢ëÂ∏ßËæìÂÖ•Ôºõ2) Video Swin Transformer backboneÊèêÂèñËßÜËßâÁâπÂæÅÔºõ3) ÂÖâÊµÅËÆ°ÁÆóÊ®°ÂùóÔºåËÆ°ÁÆóËøûÁª≠Â∏ß‰πãÈó¥ÁöÑÂÖâÊµÅ‰ø°ÊÅØÔºõ4) ÊñπÂêëÁΩëÊ†ºËøêÂä®ÁºñÁ†Å(DGME)Ê®°ÂùóÔºå‰ªéÂÖâÊµÅ‰∏≠ÊèêÂèñËøêÂä®ÁâπÂæÅÔºõ5) ÂèØÂ≠¶‰π†ÁöÑÂêéÊúüËûçÂêàÂ±ÇÔºåÂ∞ÜËßÜËßâÁâπÂæÅÂíåËøêÂä®ÁâπÂæÅËøõË°åËûçÂêàÔºõ6) ÂàÜÁ±ªÂô®ÔºåËæìÂá∫ÈïúÂ§¥ËøêÂä®ÁöÑÁ±ªÂà´„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊñπÂêëÁΩëÊ†ºËøêÂä®ÁºñÁ†Å(DGME)ÁöÑÂºïÂÖ•ÂíåÂèØÂ≠¶‰π†ÁöÑÂêéÊúüËûçÂêàÂ±Ç„ÄÇDGMEËÉΩÂ§üÊúâÊïàÂú∞ÊçïÊçâËßÜÈ¢ë‰∏≠ÁöÑËøêÂä®Ê®°ÂºèÔºå‰∏∫Ê®°ÂûãÊèê‰æõÁªìÊûÑÂåñÁöÑËøêÂä®ÂÖàÈ™åÁü•ËØÜ„ÄÇÂèØÂ≠¶‰π†ÁöÑÂêéÊúüËûçÂêàÂ±ÇËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥ËßÜËßâÁâπÂæÅÂíåËøêÂä®ÁâπÂæÅÁöÑÊùÉÈáçÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ËûçÂêà‰∏§ÁßçÊ®°ÊÄÅÁöÑ‰ø°ÊÅØ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåDGME-TËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂéÜÂè≤ÂΩ±ÂÉè‰∏≠ÁöÑÂô™Â£∞Âíå‰ΩéË¥®ÈáèÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDGMEÊ®°ÂùóÂ∞ÜÂÖâÊµÅÂú∫ÂàíÂàÜ‰∏∫ÁΩëÊ†ºÔºåÂπ∂ËÆ°ÁÆóÊØè‰∏™ÁΩëÊ†ºÂÜÖÁöÑÂπ≥ÂùáËøêÂä®ÊñπÂêëÂíåÂπÖÂ∫¶„ÄÇËøô‰∫õËøêÂä®‰ø°ÊÅØË¢´ÁºñÁ†ÅÊàêÂêëÈáèÔºå‰Ωú‰∏∫ËøêÂä®ÁâπÂæÅ„ÄÇÂèØÂ≠¶‰π†ÁöÑÂêéÊúüËûçÂêàÂ±ÇÈááÁî®‰∏Ä‰∏™Â∞èÁöÑÂÖ®ËøûÊé•ÁΩëÁªúÔºåÂ∞ÜËßÜËßâÁâπÂæÅÂíåËøêÂä®ÁâπÂæÅÊò†Â∞ÑÂà∞Âêå‰∏ÄÁª¥Â∫¶Á©∫Èó¥ÔºåÁÑ∂ÂêéËøõË°åÂä†ÊùÉËûçÂêà„ÄÇÊùÉÈáçÈÄöËøáÂèçÂêë‰º†Êí≠ËøõË°åÂ≠¶‰π†„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DGME-TÂú®Áé∞‰ª£ËßÜÈ¢ëÁâáÊÆµ‰∏äÁöÑtop-1ÂáÜÁ°ÆÁéá‰ªé81.78%ÊèêÈ´òÂà∞86.14%ÔºåÂÆèF1‰ªé82.08%ÊèêÈ´òÂà∞87.81%„ÄÇÂú®‰∫åÊàòÊó∂ÊúüÂΩ±ÂÉè‰∏äÁöÑÂáÜÁ°ÆÁéá‰ªé83.43%ÊèêÈ´òÂà∞84.62%ÔºåÂÆèF1‰ªé81.72%ÊèêÈ´òÂà∞82.63%„ÄÇË∑®ÂüüÂÆûÈ™åË°®ÊòéÔºåÂú®Áé∞‰ª£Êï∞ÊçÆ‰∏äËøõË°å‰∏≠Èó¥ÂæÆË∞ÉÈò∂ÊÆµÂèØÂ∞ÜÂéÜÂè≤ÂΩ±ÂÉèÁöÑÊÄßËÉΩÊèêÈ´ò‰∫î‰∏™ÁôæÂàÜÁÇπ‰ª•‰∏ä„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂéÜÂè≤ÂΩ±ÂÉèËµÑÊñôÁöÑËá™Âä®ÂàÜÁ±ªÂíåÂàÜÊûêÔºå‰æãÂ¶ÇÔºåÂØπÂéÜÂè≤ÁîµÂΩ±„ÄÅÁ∫™ÂΩïÁâáÁ≠âËøõË°åÈïúÂ§¥ËøêÂä®ÂàÜÊûêÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£ÂΩ±ÂÉèÂÜÖÂÆπÔºåÊèêÈ´òÂΩ±ÂÉèÊ£ÄÁ¥¢ÂíåÁÆ°ÁêÜÊïàÁéá„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ï‰πüÂèØÊé®ÂπøÂà∞ÂÖ∂‰ªñ‰ΩéË¥®ÈáèËßÜÈ¢ëÁöÑÂàÜÊûê‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÁõëÊéßËßÜÈ¢ëÂàÜÊûê„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Camera movement classification (CMC) models trained on contemporary, high-quality footage often degrade when applied to archival film, where noise, missing frames, and low contrast obscure motion cues. We bridge this gap by assembling a unified benchmark that consolidates two modern corpora into four canonical classes and restructures the HISTORIAN collection into five balanced categories. Building on this benchmark, we introduce DGME-T, a lightweight extension to the Video Swin Transformer that injects directional grid motion encoding, derived from optical flow, via a learnable and normalised late-fusion layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and its macro F1 from 82.08% to 87.81% on modern clips, while still improving the demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72% to 82.63% macro F1. A cross-domain study further shows that an intermediate fine-tuning stage on modern data increases historical performance by more than five percentage points. These results demonstrate that structured motion priors and transformer representations are complementary and that even a small, carefully calibrated motion head can substantially enhance robustness in degraded film analysis. Related resources are available at https://github.com/linty5/DGME-T.

