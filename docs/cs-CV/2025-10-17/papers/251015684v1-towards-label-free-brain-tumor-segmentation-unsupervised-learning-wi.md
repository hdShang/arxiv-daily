---
layout: default
title: Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI
---

# Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15684" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15684v1</a>
  <a href="https://arxiv.org/pdf/2510.15684.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15684v1" onclick="toggleFavorite(this, '2510.15684v1', 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier, Noel E. O'Connor, Ferran Marques

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-17

**å¤‡æ³¨**: 10 pages, 5 figures, BraTS GoAT 2025 challenge

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šæ¨¡æ€MRIçš„æ— ç›‘ç£è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼Œè§£å†³æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è„‘è‚¿ç˜¤åˆ†å‰²` `æ— ç›‘ç£å­¦ä¹ ` `å¤šæ¨¡æ€MRI` `Vision Transformer` `å¼‚å¸¸æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è„‘è‚¿ç˜¤åˆ†å‰²ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”æ•°æ®ä¸ä¸€è‡´ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€ Vision Transformer è‡ªç¼–ç å™¨çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä»…ä½¿ç”¨å¥åº·è„‘éƒ¨ MRI è¿›è¡Œè®­ç»ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‚¿ç˜¤å®šä½æ–¹é¢å…·æœ‰ä¸´åºŠæ„ä¹‰ï¼Œå¹¶åœ¨å¼‚å¸¸æ£€æµ‹ç‡ä¸Šè¾¾åˆ°äº† 89.4% çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€ Vision Transformer è‡ªç¼–ç å™¨ (MViT-AE)ï¼Œç”¨äºç£å…±æŒ¯æˆåƒ (MRI) ä¸­çš„æ— ç›‘ç£è„‘è‚¿ç˜¤åˆ†å‰²ã€‚è¯¥æ¨¡å‹ä»…åœ¨å¥åº·è„‘éƒ¨ MRI ä¸Šè®­ç»ƒï¼Œé€šè¿‡é‡å»ºè¯¯å·®å›¾æ£€æµ‹å’Œå®šä½è‚¿ç˜¤ã€‚è¿™ç§æ— ç›‘ç£èŒƒå¼æ— éœ€äººå·¥æ ‡æ³¨å³å¯å®ç°åˆ†å‰²ï¼Œè§£å†³äº†ç¥ç»å½±åƒå·¥ä½œæµç¨‹ä¸­çš„å…³é”®å¯æ‰©å±•æ€§ç“¶é¢ˆã€‚è¯¥æ–¹æ³•åœ¨ BraTS-GoAT 2025 Lighthouse æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šç§ç±»å‹çš„è‚¿ç˜¤ï¼Œå¦‚ç¥ç»èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤å’Œå„¿ç«¥è„‘è‚¿ç˜¤ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œå¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€æ—©æ™šèåˆç­–ç•¥ï¼Œåˆ©ç”¨å¤šä¸ª MRI åºåˆ—ä¸­çš„äº’è¡¥ä¿¡æ¯ï¼Œä»¥åŠä¸€ä¸ªé›†æˆ Segment Anything Model (SAM) çš„åå¤„ç†æµç¨‹æ¥ç»†åŒ–é¢„æµ‹çš„è‚¿ç˜¤è½®å»“ã€‚å°½ç®¡æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ£€æµ‹å°å‹æˆ–éå¢å¼ºç—…ç¶æ–¹é¢ï¼Œä½†è¯¥æ–¹æ³•å®ç°äº†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„è‚¿ç˜¤å®šä½ï¼Œåœ¨æµ‹è¯•é›†ä¸Šç—…ç¶æ–¹é¢çš„ Dice ç›¸ä¼¼ç³»æ•°åˆ†åˆ«ä¸º 0.437 (å…¨è‚¿ç˜¤)ã€0.316 (è‚¿ç˜¤æ ¸å¿ƒ) å’Œ 0.350 (å¢å¼ºè‚¿ç˜¤)ï¼Œåœ¨éªŒè¯é›†ä¸Šçš„å¼‚å¸¸æ£€æµ‹ç‡ä¸º 89.4%ã€‚è¿™äº›å‘ç°çªå‡ºäº†åŸºäº Transformer çš„æ— ç›‘ç£æ¨¡å‹ä½œä¸ºç¥ç»è‚¿ç˜¤æˆåƒçš„å¯æ‰©å±•ã€æ ‡ç­¾é«˜æ•ˆå·¥å…·çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œç„¶è€Œï¼Œè·å–é«˜è´¨é‡çš„è„‘è‚¿ç˜¤æ ‡æ³¨æ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œä¸”ä¸åŒæœºæ„æˆ–ä¸“å®¶ä¹‹é—´çš„æ ‡æ³¨å¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®ä¸ä¸€è‡´æ€§ã€‚è¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ å¥åº·è„‘éƒ¨ MRI çš„åˆ†å¸ƒï¼Œå°†è‚¿ç˜¤åŒºåŸŸè§†ä¸ºå¼‚å¸¸ã€‚æ¨¡å‹ä»…åœ¨å¥åº·æ•°æ®ä¸Šè®­ç»ƒï¼Œå­¦ä¹ æ­£å¸¸è„‘ç»„ç»‡çš„ç‰¹å¾è¡¨ç¤ºï¼Œå½“è¾“å…¥åŒ…å«è‚¿ç˜¤çš„ MRI æ—¶ï¼Œæ¨¡å‹æ— æ³•å‡†ç¡®é‡å»ºè‚¿ç˜¤åŒºåŸŸï¼Œä»è€Œé€šè¿‡é‡å»ºè¯¯å·®å›¾å®šä½è‚¿ç˜¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) å¤šæ¨¡æ€ MRI æ•°æ®è¾“å…¥ï¼›2) MViT-AE æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ï¼Œç”Ÿæˆé‡å»ºè¯¯å·®å›¾ï¼›3) åå¤„ç†ï¼Œåˆ©ç”¨ SAM æ¨¡å‹ç»†åŒ–è‚¿ç˜¤è½®å»“ã€‚MViT-AE æ¨¡å‹é‡‡ç”¨ Vision Transformer ç»“æ„ï¼Œå¹¶ç»“åˆäº†å¤šæ¨¡æ€æ—©æ™šèåˆç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨ä¸åŒ MRI åºåˆ—çš„ä¿¡æ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨ Vision Transformer ç»“æ„è¿›è¡Œæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œå¹¶ç»“åˆå¤šæ¨¡æ€èåˆå’Œ SAM åå¤„ç†ï¼Œå®ç°äº†åœ¨æ²¡æœ‰æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œè„‘è‚¿ç˜¤åˆ†å‰²ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€äººå·¥æ ‡æ³¨ï¼Œå…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šMViT-AE æ¨¡å‹é‡‡ç”¨ Vision Transformer ä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œå¹¶é’ˆå¯¹å¤šæ¨¡æ€ MRI æ•°æ®è®¾è®¡äº†æ—©æ™šèåˆç­–ç•¥ã€‚æ—©èåˆå°†ä¸åŒæ¨¡æ€çš„æ•°æ®åœ¨è¾“å…¥å±‚è¿›è¡Œæ‹¼æ¥ï¼Œæ™šèåˆåˆ™åœ¨ç¼–ç å™¨è¾“å‡ºå±‚è¿›è¡Œç‰¹å¾èåˆã€‚æŸå¤±å‡½æ•°é‡‡ç”¨å‡æ–¹è¯¯å·® (MSE) ä½œä¸ºé‡å»ºæŸå¤±ï¼Œç”¨äºè¡¡é‡é‡å»ºå›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚åå¤„ç†é˜¶æ®µï¼Œåˆ©ç”¨ SAM æ¨¡å‹å¯¹é‡å»ºè¯¯å·®å›¾è¿›è¡Œåˆ†å‰²ï¼Œå¹¶æ ¹æ®ä¸´åºŠç»éªŒè®¾ç½®é˜ˆå€¼ï¼Œä»¥å»é™¤å‡é˜³æ€§åŒºåŸŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨ BraTS-GoAT 2025 Lighthouse æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚åœ¨æµ‹è¯•é›†ä¸Šï¼Œå…¨è‚¿ç˜¤ã€è‚¿ç˜¤æ ¸å¿ƒå’Œå¢å¼ºè‚¿ç˜¤çš„ Dice ç›¸ä¼¼ç³»æ•°åˆ†åˆ«ä¸º 0.437ã€0.316 å’Œ 0.350ã€‚åœ¨éªŒè¯é›†ä¸Šï¼Œå¼‚å¸¸æ£€æµ‹ç‡è¾¾åˆ°äº† 89.4%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— ç›‘ç£è„‘è‚¿ç˜¤åˆ†å‰²æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä¸´åºŠè¾…åŠ©è¯Šæ–­ï¼Œå¸®åŠ©åŒ»ç”Ÿå¿«é€Ÿå®šä½è„‘è‚¿ç˜¤åŒºåŸŸï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–åŒ»å­¦å½±åƒåˆ†æä»»åŠ¡ï¼Œä¾‹å¦‚ç—…ç¶æ£€æµ‹ã€å™¨å®˜åˆ†å‰²ç­‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•æé«˜æ¨¡å‹å¯¹å°å‹æˆ–éå¢å¼ºç—…ç¶çš„æ£€æµ‹èƒ½åŠ›ï¼Œä»¥åŠå¦‚ä½•å°†è¯¥æ–¹æ³•ä¸ä¸´åºŠä¿¡æ¯ç›¸ç»“åˆï¼Œä»¥æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unsupervised anomaly detection (UAD) presents a complementary alternative to supervised learning for brain tumor segmentation in magnetic resonance imaging (MRI), particularly when annotated datasets are limited, costly, or inconsistent. In this work, we propose a novel Multimodal Vision Transformer Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and localize tumors via reconstruction-based error maps. This unsupervised paradigm enables segmentation without reliance on manual labels, addressing a key scalability bottleneck in neuroimaging workflows. Our method is evaluated in the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors such as gliomas, meningiomas, and pediatric brain tumors. To enhance performance, we introduce a multimodal early-late fusion strategy that leverages complementary information across multiple MRI sequences, and a post-processing pipeline that integrates the Segment Anything Model (SAM) to refine predicted tumor contours. Despite the known challenges of UAD, particularly in detecting small or non-enhancing lesions, our method achieves clinically meaningful tumor localization, with lesion-wise Dice Similarity Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the validation set. These findings highlight the potential of transformer-based unsupervised models to serve as scalable, label-efficient tools for neuro-oncological imaging.

