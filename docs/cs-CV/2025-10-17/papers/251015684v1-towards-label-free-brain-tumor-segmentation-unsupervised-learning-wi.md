---
layout: default
title: Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI
---

# Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.15684" target="_blank" class="toolbar-btn">arXiv: 2510.15684v1</a>
    <a href="https://arxiv.org/pdf/2510.15684.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15684v1" 
            onclick="toggleFavorite(this, '2510.15684v1', 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier, Noel E. O'Connor, Ferran Marques

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-17

**Â§áÊ≥®**: 10 pages, 5 figures, BraTS GoAT 2025 challenge

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂ§öÊ®°ÊÄÅMRIÁöÑÊó†ÁõëÁù£ËÑëËÇøÁò§ÂàÜÂâ≤ÊñπÊ≥ïÔºåËß£ÂÜ≥Ê†áÊ≥®Êï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËÑëËÇøÁò§ÂàÜÂâ≤` `Êó†ÁõëÁù£Â≠¶‰π†` `Â§öÊ®°ÊÄÅMRI` `Vision Transformer` `ÂºÇÂ∏∏Ê£ÄÊµã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ËÑëËÇøÁò§ÂàÜÂâ≤‰æùËµñÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÔºå‰ΩÜÊ†áÊ≥®ÊàêÊú¨È´òÊòÇ‰∏îÊï∞ÊçÆ‰∏ç‰∏ÄËá¥ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÂ§öÊ®°ÊÄÅ Vision Transformer Ëá™ÁºñÁ†ÅÂô®ÁöÑÊó†ÁõëÁù£ÂºÇÂ∏∏Ê£ÄÊµãÊñπÊ≥ïÔºå‰ªÖ‰ΩøÁî®ÂÅ•Â∫∑ËÑëÈÉ® MRI ËøõË°åËÆ≠ÁªÉ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ËÇøÁò§ÂÆö‰ΩçÊñπÈù¢ÂÖ∑Êúâ‰∏¥Â∫äÊÑè‰πâÔºåÂπ∂Âú®ÂºÇÂ∏∏Ê£ÄÊµãÁéá‰∏äËææÂà∞‰∫Ü 89.4% ÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ§öÊ®°ÊÄÅ Vision Transformer Ëá™ÁºñÁ†ÅÂô® (MViT-AE)ÔºåÁî®‰∫éÁ£ÅÂÖ±ÊåØÊàêÂÉè (MRI) ‰∏≠ÁöÑÊó†ÁõëÁù£ËÑëËÇøÁò§ÂàÜÂâ≤„ÄÇËØ•Ê®°Âûã‰ªÖÂú®ÂÅ•Â∫∑ËÑëÈÉ® MRI ‰∏äËÆ≠ÁªÉÔºåÈÄöËøáÈáçÂª∫ËØØÂ∑ÆÂõæÊ£ÄÊµãÂíåÂÆö‰ΩçËÇøÁò§„ÄÇËøôÁßçÊó†ÁõëÁù£ËåÉÂºèÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®Âç≥ÂèØÂÆûÁé∞ÂàÜÂâ≤ÔºåËß£ÂÜ≥‰∫ÜÁ•ûÁªèÂΩ±ÂÉèÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÂÖ≥ÈîÆÂèØÊâ©Â±ïÊÄßÁì∂È¢à„ÄÇËØ•ÊñπÊ≥ïÂú® BraTS-GoAT 2025 Lighthouse Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåËØ•Êï∞ÊçÆÈõÜÂåÖÂê´Â§öÁßçÁ±ªÂûãÁöÑËÇøÁò§ÔºåÂ¶ÇÁ•ûÁªèËÉ∂Ë¥®Áò§„ÄÅËÑëËÜúÁò§ÂíåÂÑøÁ´•ËÑëËÇøÁò§„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÊÄßËÉΩÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊó©ÊôöËûçÂêàÁ≠ñÁï•ÔºåÂà©Áî®Â§ö‰∏™ MRI Â∫èÂàó‰∏≠ÁöÑ‰∫íË°•‰ø°ÊÅØÔºå‰ª•Âèä‰∏Ä‰∏™ÈõÜÊàê Segment Anything Model (SAM) ÁöÑÂêéÂ§ÑÁêÜÊµÅÁ®ãÊù•ÁªÜÂåñÈ¢ÑÊµãÁöÑËÇøÁò§ËΩÆÂªì„ÄÇÂ∞ΩÁÆ°Êó†ÁõëÁù£ÂºÇÂ∏∏Ê£ÄÊµãÂ≠òÂú®ÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê£ÄÊµãÂ∞èÂûãÊàñÈùûÂ¢ûÂº∫ÁóÖÁÅ∂ÊñπÈù¢Ôºå‰ΩÜËØ•ÊñπÊ≥ïÂÆûÁé∞‰∫ÜÂÖ∑Êúâ‰∏¥Â∫äÊÑè‰πâÁöÑËÇøÁò§ÂÆö‰ΩçÔºåÂú®ÊµãËØïÈõÜ‰∏äÁóÖÁÅ∂ÊñπÈù¢ÁöÑ Dice Áõ∏‰ººÁ≥ªÊï∞ÂàÜÂà´‰∏∫ 0.437 (ÂÖ®ËÇøÁò§)„ÄÅ0.316 (ËÇøÁò§Ê†∏ÂøÉ) Âíå 0.350 (Â¢ûÂº∫ËÇøÁò§)ÔºåÂú®È™åËØÅÈõÜ‰∏äÁöÑÂºÇÂ∏∏Ê£ÄÊµãÁéá‰∏∫ 89.4%„ÄÇËøô‰∫õÂèëÁé∞Á™ÅÂá∫‰∫ÜÂü∫‰∫é Transformer ÁöÑÊó†ÁõëÁù£Ê®°Âûã‰Ωú‰∏∫Á•ûÁªèËÇøÁò§ÊàêÂÉèÁöÑÂèØÊâ©Â±ï„ÄÅÊ†áÁ≠æÈ´òÊïàÂ∑•ÂÖ∑ÁöÑÊΩúÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÑëËÇøÁò§ÂàÜÂâ≤‰ªªÂä°ÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆËøõË°åÁõëÁù£Â≠¶‰π†ÔºåÁÑ∂ËÄåÔºåËé∑ÂèñÈ´òË¥®ÈáèÁöÑËÑëËÇøÁò§Ê†áÊ≥®Êï∞ÊçÆÊàêÊú¨È´òÊòÇÔºå‰∏î‰∏çÂêåÊú∫ÊûÑÊàñ‰∏ìÂÆ∂‰πãÈó¥ÁöÑÊ†áÊ≥®ÂèØËÉΩÂ≠òÂú®Â∑ÆÂºÇÔºåÂØºËá¥Êï∞ÊçÆ‰∏ç‰∏ÄËá¥ÊÄß„ÄÇËøôÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆûÈôÖÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Êó†ÁõëÁù£ÂºÇÂ∏∏Ê£ÄÊµãÊñπÊ≥ïÔºåÈÄöËøáÂ≠¶‰π†ÂÅ•Â∫∑ËÑëÈÉ® MRI ÁöÑÂàÜÂ∏ÉÔºåÂ∞ÜËÇøÁò§Âå∫ÂüüËßÜ‰∏∫ÂºÇÂ∏∏„ÄÇÊ®°Âûã‰ªÖÂú®ÂÅ•Â∫∑Êï∞ÊçÆ‰∏äËÆ≠ÁªÉÔºåÂ≠¶‰π†Ê≠£Â∏∏ËÑëÁªÑÁªáÁöÑÁâπÂæÅË°®Á§∫ÔºåÂΩìËæìÂÖ•ÂåÖÂê´ËÇøÁò§ÁöÑ MRI Êó∂ÔºåÊ®°ÂûãÊó†Ê≥ïÂáÜÁ°ÆÈáçÂª∫ËÇøÁò§Âå∫ÂüüÔºå‰ªéËÄåÈÄöËøáÈáçÂª∫ËØØÂ∑ÆÂõæÂÆö‰ΩçËÇøÁò§„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) Â§öÊ®°ÊÄÅ MRI Êï∞ÊçÆËæìÂÖ•Ôºõ2) MViT-AE Ê®°ÂûãËÆ≠ÁªÉÂíåÊé®ÁêÜÔºåÁîüÊàêÈáçÂª∫ËØØÂ∑ÆÂõæÔºõ3) ÂêéÂ§ÑÁêÜÔºåÂà©Áî® SAM Ê®°ÂûãÁªÜÂåñËÇøÁò§ËΩÆÂªì„ÄÇMViT-AE Ê®°ÂûãÈááÁî® Vision Transformer ÁªìÊûÑÔºåÂπ∂ÁªìÂêà‰∫ÜÂ§öÊ®°ÊÄÅÊó©ÊôöËûçÂêàÁ≠ñÁï•Ôºå‰ª•ÂÖÖÂàÜÂà©Áî®‰∏çÂêå MRI Â∫èÂàóÁöÑ‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂà©Áî® Vision Transformer ÁªìÊûÑËøõË°åÊó†ÁõëÁù£ÂºÇÂ∏∏Ê£ÄÊµãÔºåÂπ∂ÁªìÂêàÂ§öÊ®°ÊÄÅËûçÂêàÂíå SAM ÂêéÂ§ÑÁêÜÔºåÂÆûÁé∞‰∫ÜÂú®Ê≤°ÊúâÊ†áÊ≥®Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãËøõË°åËÑëËÇøÁò§ÂàÜÂâ≤„ÄÇ‰∏é‰º†ÁªüÁöÑÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®ÔºåÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMViT-AE Ê®°ÂûãÈááÁî® Vision Transformer ‰Ωú‰∏∫‰∏ªÂπ≤ÁΩëÁªúÔºåÂπ∂ÈíàÂØπÂ§öÊ®°ÊÄÅ MRI Êï∞ÊçÆËÆæËÆ°‰∫ÜÊó©ÊôöËûçÂêàÁ≠ñÁï•„ÄÇÊó©ËûçÂêàÂ∞Ü‰∏çÂêåÊ®°ÊÄÅÁöÑÊï∞ÊçÆÂú®ËæìÂÖ•Â±ÇËøõË°åÊãºÊé•ÔºåÊôöËûçÂêàÂàôÂú®ÁºñÁ†ÅÂô®ËæìÂá∫Â±ÇËøõË°åÁâπÂæÅËûçÂêà„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®ÂùáÊñπËØØÂ∑Æ (MSE) ‰Ωú‰∏∫ÈáçÂª∫ÊçüÂ§±ÔºåÁî®‰∫éË°°ÈáèÈáçÂª∫ÂõæÂÉè‰∏éÂéüÂßãÂõæÂÉè‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÂêéÂ§ÑÁêÜÈò∂ÊÆµÔºåÂà©Áî® SAM Ê®°ÂûãÂØπÈáçÂª∫ËØØÂ∑ÆÂõæËøõË°åÂàÜÂâ≤ÔºåÂπ∂Ê†πÊçÆ‰∏¥Â∫äÁªèÈ™åËÆæÁΩÆÈòàÂÄºÔºå‰ª•ÂéªÈô§ÂÅáÈò≥ÊÄßÂå∫Âüü„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú® BraTS-GoAT 2025 Lighthouse Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂèñÂæó‰∫ÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÁªìÊûú„ÄÇÂú®ÊµãËØïÈõÜ‰∏äÔºåÂÖ®ËÇøÁò§„ÄÅËÇøÁò§Ê†∏ÂøÉÂíåÂ¢ûÂº∫ËÇøÁò§ÁöÑ Dice Áõ∏‰ººÁ≥ªÊï∞ÂàÜÂà´‰∏∫ 0.437„ÄÅ0.316 Âíå 0.350„ÄÇÂú®È™åËØÅÈõÜ‰∏äÔºåÂºÇÂ∏∏Ê£ÄÊµãÁéáËææÂà∞‰∫Ü 89.4%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êó†ÁõëÁù£ËÑëËÇøÁò§ÂàÜÂâ≤ÊñπÈù¢ÂÖ∑ÊúâÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫é‰∏¥Â∫äËæÖÂä©ËØäÊñ≠ÔºåÂ∏ÆÂä©ÂåªÁîüÂø´ÈÄüÂÆö‰ΩçËÑëËÇøÁò§Âå∫ÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê†áÊ≥®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊûê‰ªªÂä°Ôºå‰æãÂ¶ÇÁóÖÁÅ∂Ê£ÄÊµã„ÄÅÂô®ÂÆòÂàÜÂâ≤Á≠âÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÊú™Êù•ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Á†îÁ©∂Â¶Ç‰ΩïÊèêÈ´òÊ®°ÂûãÂØπÂ∞èÂûãÊàñÈùûÂ¢ûÂº∫ÁóÖÁÅ∂ÁöÑÊ£ÄÊµãËÉΩÂäõÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂ∞ÜËØ•ÊñπÊ≥ï‰∏é‰∏¥Â∫ä‰ø°ÊÅØÁõ∏ÁªìÂêàÔºå‰ª•ÊèêÈ´òËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Unsupervised anomaly detection (UAD) presents a complementary alternative to supervised learning for brain tumor segmentation in magnetic resonance imaging (MRI), particularly when annotated datasets are limited, costly, or inconsistent. In this work, we propose a novel Multimodal Vision Transformer Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and localize tumors via reconstruction-based error maps. This unsupervised paradigm enables segmentation without reliance on manual labels, addressing a key scalability bottleneck in neuroimaging workflows. Our method is evaluated in the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors such as gliomas, meningiomas, and pediatric brain tumors. To enhance performance, we introduce a multimodal early-late fusion strategy that leverages complementary information across multiple MRI sequences, and a post-processing pipeline that integrates the Segment Anything Model (SAM) to refine predicted tumor contours. Despite the known challenges of UAD, particularly in detecting small or non-enhancing lesions, our method achieves clinically meaningful tumor localization, with lesion-wise Dice Similarity Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the validation set. These findings highlight the potential of transformer-based unsupervised models to serve as scalable, label-efficient tools for neuro-oncological imaging.

