---
layout: default
title: DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification
---

# DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.04281" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.04281v1</a>
  <a href="https://arxiv.org/pdf/2511.04281.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.04281v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.04281v1', 'DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yujie Yang, Shuang Li, Jun Ye, Neng Dong, Fan Li, Huafeng Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDinoGRLæ¡†æ¶ï¼Œåˆ©ç”¨DINOv2é©±åŠ¨çš„æ­¥æ€ç‰¹å¾å­¦ä¹ ï¼Œè§£å†³è§†é¢‘å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘è¡Œäººé‡è¯†åˆ«` `å¯è§å…‰-çº¢å¤–` `æ­¥æ€ç‰¹å¾å­¦ä¹ ` `DINOv2` `è·¨æ¨¡æ€æ£€ç´¢` `ç‰¹å¾èåˆ` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VVI-ReIDæ–¹æ³•å¿½ç•¥äº†æ­¥æ€ç‰¹å¾ä¸­è•´å«çš„æ—¶ç©ºåŠ¨æ€ä¿¡æ¯ï¼Œé™åˆ¶äº†è·¨æ¨¡æ€è§†é¢‘åŒ¹é…çš„èƒ½åŠ›ã€‚
2. DinoGRLæ¡†æ¶åˆ©ç”¨DINOv2çš„è§†è§‰å…ˆéªŒå­¦ä¹ æ­¥æ€ç‰¹å¾ï¼Œå¹¶è®¾è®¡SASGLå’ŒPBMGEæ¨¡å—å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚
3. åœ¨HITSZ-VCMå’ŒBUPTæ•°æ®é›†ä¸Šï¼ŒDinoGRLæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§DINOv2é©±åŠ¨çš„æ­¥æ€è¡¨ç¤ºå­¦ä¹ (DinoGRL)æ¡†æ¶ï¼Œç”¨äºè§£å†³åŸºäºè§†é¢‘çš„å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«(VVI-ReID)é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾§é‡äºåˆ©ç”¨æ¨¡æ€ä¸å˜çš„è§†è§‰ç‰¹å¾ï¼Œä½†å¿½ç•¥äº†æ­¥æ€ç‰¹å¾ï¼Œè€Œæ­¥æ€ç‰¹å¾ä¸ä»…æ¨¡æ€ä¸å˜ï¼Œè€Œä¸”å¯Œå«æ—¶é—´åŠ¨æ€ä¿¡æ¯ï¼Œé™åˆ¶äº†å®ƒä»¬å¯¹è·¨æ¨¡æ€è§†é¢‘åŒ¹é…è‡³å…³é‡è¦çš„æ—¶ç©ºä¸€è‡´æ€§è¿›è¡Œå»ºæ¨¡çš„èƒ½åŠ›ã€‚DinoGRLæ¡†æ¶åˆ©ç”¨DINOv2ä¸°å¯Œçš„è§†è§‰å…ˆéªŒçŸ¥è¯†æ¥å­¦ä¹ æ­¥æ€ç‰¹å¾ï¼Œä½œä¸ºå¤–è§‚çº¿ç´¢çš„è¡¥å……ï¼Œä»è€Œä¿ƒè¿›äº†é²æ£’çš„åºåˆ—çº§è·¨æ¨¡æ€æ£€ç´¢è¡¨ç¤ºã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥è½®å»“å’Œæ­¥æ€å­¦ä¹ (SASGL)æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨DINOv2çš„é€šç”¨è¯­ä¹‰å…ˆéªŒç”Ÿæˆå¹¶å¢å¼ºè½®å»“è¡¨ç¤ºï¼Œå¹¶å°†å…¶ä¸ReIDç›®æ ‡è”åˆä¼˜åŒ–ï¼Œä»¥å®ç°è¯­ä¹‰ä¸°å¯Œçš„ä»»åŠ¡è‡ªé€‚åº”æ­¥æ€ç‰¹å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¸è¿›å¼åŒå‘å¤šç²’åº¦å¢å¼º(PBMGE)æ¨¡å—ï¼Œé€šè¿‡åœ¨å¤šä¸ªç©ºé—´ç²’åº¦ä¸Šå®ç°æ­¥æ€å’Œå¤–è§‚æµä¹‹é—´çš„åŒå‘äº¤äº’æ¥é€æ­¥ç»†åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œå……åˆ†åˆ©ç”¨å®ƒä»¬çš„äº’è¡¥æ€§æ¥å¢å¼ºå…·æœ‰ä¸°å¯Œå±€éƒ¨ç»†èŠ‚çš„å…¨å±€è¡¨ç¤ºï¼Œå¹¶äº§ç”Ÿé«˜åº¦åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚åœ¨HITSZ-VCMå’ŒBUPTæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«(VVI-ReID)æ—¨åœ¨ä»è§†é¢‘åºåˆ—ä¸­æ£€ç´¢è·¨å¯è§å…‰å’Œçº¢å¤–æ¨¡æ€çš„åŒä¸€è¡Œäººã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæ¨¡æ€ä¸å˜çš„è§†è§‰ç‰¹å¾ï¼Œä½†å¿½ç•¥äº†æ­¥æ€ç‰¹å¾ï¼Œè€Œæ­¥æ€ç‰¹å¾å…·æœ‰æ¨¡æ€ä¸å˜æ€§å’Œä¸°å¯Œçš„æ—¶åºä¿¡æ¯ï¼Œå¯¹äºè·¨æ¨¡æ€è§†é¢‘åŒ¹é…è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ­¥æ€ç‰¹å¾ï¼Œæå‡VVI-ReIDçš„æ€§èƒ½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„DINOv2æ¨¡å‹æä¾›çš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæ¥æŒ‡å¯¼æ­¥æ€ç‰¹å¾çš„å­¦ä¹ ï¼Œå¹¶å°†å…¶ä¸å¤–è§‚ç‰¹å¾è¿›è¡Œäº’è¡¥å¢å¼ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å­¦ä¹ åˆ°æ›´å…·åˆ¤åˆ«æ€§å’Œé²æ£’æ€§çš„åºåˆ—çº§ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæå‡è·¨æ¨¡æ€æ£€ç´¢çš„å‡†ç¡®ç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDinoGRLæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šè¯­ä¹‰æ„ŸçŸ¥è½®å»“å’Œæ­¥æ€å­¦ä¹ (SASGL)æ¨¡å‹å’Œæ¸è¿›å¼åŒå‘å¤šç²’åº¦å¢å¼º(PBMGE)æ¨¡å—ã€‚SASGLæ¨¡å‹è´Ÿè´£ç”Ÿæˆå’Œå¢å¼ºè½®å»“è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨DINOv2çš„è¯­ä¹‰å…ˆéªŒè¿›è¡ŒæŒ‡å¯¼ã€‚PBMGEæ¨¡å—åˆ™é€šè¿‡åœ¨å¤šä¸ªç©ºé—´ç²’åº¦ä¸Šè¿›è¡Œæ­¥æ€å’Œå¤–è§‚ç‰¹å¾çš„åŒå‘äº¤äº’ï¼Œé€æ­¥ç»†åŒ–ç‰¹å¾è¡¨ç¤ºã€‚æ•´ä½“æµç¨‹æ˜¯é¦–å…ˆé€šè¿‡SASGLå­¦ä¹ æ­¥æ€ç‰¹å¾ï¼Œç„¶åé€šè¿‡PBMGEå°†æ­¥æ€ç‰¹å¾å’Œå¤–è§‚ç‰¹å¾èåˆå¢å¼ºï¼Œæœ€åç”¨äºè¡Œäººé‡è¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼š1) åˆ©ç”¨DINOv2çš„è§†è§‰å…ˆéªŒæ¥æŒ‡å¯¼æ­¥æ€ç‰¹å¾å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ€è·¯ï¼Œå¯ä»¥æœ‰æ•ˆæå‡æ­¥æ€ç‰¹å¾çš„è´¨é‡ã€‚2) æå‡ºäº†SASGLæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå’Œå¢å¼ºè½®å»“è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨DINOv2çš„è¯­ä¹‰å…ˆéªŒè¿›è¡ŒæŒ‡å¯¼ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å…·åˆ¤åˆ«æ€§çš„æ­¥æ€ç‰¹å¾ã€‚3) æå‡ºäº†PBMGEæ¨¡å—ï¼Œé€šè¿‡åœ¨å¤šä¸ªç©ºé—´ç²’åº¦ä¸Šè¿›è¡Œæ­¥æ€å’Œå¤–è§‚ç‰¹å¾çš„åŒå‘äº¤äº’ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å®ƒä»¬çš„äº’è¡¥æ€§ï¼Œä»è€Œæå‡æ•´ä½“çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šSASGLæ¨¡å‹ä½¿ç”¨DINOv2æå–çš„è¯­ä¹‰ä¿¡æ¯æ¥å¢å¼ºè½®å»“è¡¨ç¤ºï¼Œå…·ä½“å®ç°æ–¹å¼æœªçŸ¥ã€‚PBMGEæ¨¡å—é‡‡ç”¨æ¸è¿›å¼çš„æ–¹å¼ï¼Œé€æ­¥èåˆä¸åŒç²’åº¦çš„ç‰¹å¾ï¼Œå…·ä½“ç²’åº¦åˆ’åˆ†å’Œèåˆæ–¹å¼æœªçŸ¥ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé™¤äº†ReIDçš„æŸå¤±å‡½æ•°å¤–ï¼Œå¯èƒ½è¿˜ä½¿ç”¨äº†å…¶ä»–çš„è¾…åŠ©æŸå¤±å‡½æ•°æ¥çº¦æŸæ­¥æ€ç‰¹å¾çš„å­¦ä¹ ï¼Œå…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDinoGRLæ¡†æ¶åœ¨HITSZ-VCMå’ŒBUPTæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œä½†æ‘˜è¦ä¸­æœªæ˜ç¡®æåŠã€‚è¿™äº›ç»“æœéªŒè¯äº†DinoGRLæ¡†æ¶åœ¨VVI-ReIDä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®‰é˜²ã€æ™ºæ…§åŸå¸‚ç­‰é¢†åŸŸï¼Œä¾‹å¦‚åœ¨è·¨æ‘„åƒå¤´åœºæ™¯ä¸‹è¿›è¡Œè¡Œäººè¿½è¸ªå’Œèº«ä»½è¯†åˆ«ã€‚é€šè¿‡ç»“åˆå¯è§å…‰å’Œçº¢å¤–æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¯ä»¥æé«˜åœ¨å…‰ç…§æ¡ä»¶ä¸ä½³æˆ–å­˜åœ¨é®æŒ¡æƒ…å†µä¸‹çš„è¡Œäººé‡è¯†åˆ«å‡†ç¡®ç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€çš„è¡Œäººé‡è¯†åˆ«ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video-based Visible-Infrared person re-identification (VVI-ReID) aims to retrieve the same pedestrian across visible and infrared modalities from video sequences. Existing methods tend to exploit modality-invariant visual features but largely overlook gait features, which are not only modality-invariant but also rich in temporal dynamics, thus limiting their ability to model the spatiotemporal consistency essential for cross-modal video matching. To address these challenges, we propose a DINOv2-Driven Gait Representation Learning (DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn gait features complementary to appearance cues, facilitating robust sequence-level representations for cross-modal retrieval. Specifically, we introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which generates and enhances silhouette representations with general-purpose semantic priors from DINOv2 and jointly optimizes them with the ReID objective to achieve semantically enriched and task-adaptive gait feature learning. Furthermore, we develop a Progressive Bidirectional Multi-Granularity Enhancement (PBMGE) module, which progressively refines feature representations by enabling bidirectional interactions between gait and appearance streams across multiple spatial granularities, fully leveraging their complementarity to enhance global representations with rich local details and produce highly discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets demonstrate the superiority of our approach, significantly outperforming existing state-of-the-art methods.

