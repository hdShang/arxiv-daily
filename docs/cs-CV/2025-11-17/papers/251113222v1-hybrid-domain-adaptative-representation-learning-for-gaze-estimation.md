---
layout: default
title: Hybrid-Domain Adaptative Representation Learning for Gaze Estimation
---

# Hybrid-Domain Adaptative Representation Learning for Gaze Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.13222" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.13222v1</a>
  <a href="https://arxiv.org/pdf/2511.13222.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13222v1" onclick="toggleFavorite(this, '2511.13222v1', 'Hybrid-Domain Adaptative Representation Learning for Gaze Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qida Tan, Hongyu Yang, Wenchao Du

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

**å¤‡æ³¨**: AAAI2026

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/da60266/HARL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆé¢†åŸŸè‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ ä»¥è§£å†³æ³¨è§†ä¼°è®¡ä¸­çš„è·¨åŸŸé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ³¨è§†ä¼°è®¡` `é¢†åŸŸé€‚åº”` `è¡¨ç¤ºå­¦ä¹ ` `å›¾åƒå¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ³¨è§†ä¼°è®¡æ–¹æ³•åœ¨è·¨åŸŸè¯„ä¼°ä¸­è¡¨ç°ä¸ä½³ï¼Œå—åˆ°è¡¨æƒ…ã€ä½©æˆ´ç‰©å’Œå›¾åƒè´¨é‡ç­‰å› ç´ çš„å¹²æ‰°ã€‚
2. æœ¬æ–‡æå‡ºçš„HARLæ¡†æ¶é€šè¿‡æ— ç›‘ç£é¢†åŸŸé€‚åº”ï¼Œä»ä½è´¨é‡é¢éƒ¨å›¾åƒä¸­æå–æ³¨è§†ç›¸å…³è¡¨ç¤ºï¼Œå‡å°‘äº†è®¡ç®—å’Œæ¨ç†æˆæœ¬ã€‚
3. åœ¨EyeDiapã€MPIIFaceGazeå’ŒGaze360æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«è¾¾åˆ°äº†5.02Â°ã€3.36Â°å’Œ9.26Â°çš„å‡†ç¡®æ€§ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºå¤–è§‚çš„æ³¨è§†ä¼°è®¡æ—¨åœ¨ä»å•å¼ é¢éƒ¨å›¾åƒä¸­é¢„æµ‹å‡†ç¡®çš„3Dæ³¨è§†æ–¹å‘ï¼Œè¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•åœ¨è·¨åŸŸè¯„ä¼°ä¸­è¡¨ç°ä¸ä½³ï¼Œå—åˆ°ä¸æ³¨è§†æ— å…³å› ç´ çš„å¹²æ‰°ï¼Œå¦‚è¡¨æƒ…ã€ä½©æˆ´ç‰©å’Œå›¾åƒè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆé¢†åŸŸè‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼ˆHARLï¼‰ï¼Œåˆ©ç”¨å¤šæºæ··åˆæ•°æ®é›†å­¦ä¹ ç¨³å¥çš„æ³¨è§†è¡¨ç¤ºã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡æ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹å¼å¯¹é½ä»é«˜è´¨é‡è¿‘çœ¼å›¾åƒæå–çš„ç‰¹å¾ï¼Œä»¥ä»ä½è´¨é‡é¢éƒ¨å›¾åƒä¸­è§£è€¦æ³¨è§†ç›¸å…³è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†å¤´éƒ¨å§¿æ€çš„å½±å“ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•è€Œé«˜æ•ˆçš„ç¨€ç–å›¾èåˆæ¨¡å—ï¼Œä»¥æ¢ç´¢æ³¨è§†æ–¹å‘ä¸å¤´éƒ¨å§¿æ€ä¹‹é—´çš„å‡ ä½•çº¦æŸï¼Œä»è€Œè·å¾—å¯†é›†ä¸”ç¨³å¥çš„æ³¨è§†è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºäºå¤–è§‚çš„æ³¨è§†ä¼°è®¡åœ¨è·¨åŸŸè¯„ä¼°ä¸­æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ä¸åŒç¯å¢ƒå’Œæ¡ä»¶æ—¶ï¼Œå®¹æ˜“å—åˆ°ä¸æ³¨è§†æ— å…³å› ç´ çš„å¹²æ‰°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„HARLæ¡†æ¶é€šè¿‡åˆ©ç”¨å¤šæºæ··åˆæ•°æ®é›†ï¼Œé‡‡ç”¨æ— ç›‘ç£é¢†åŸŸé€‚åº”çš„æ–¹æ³•ï¼Œä»ä½è´¨é‡å›¾åƒä¸­æå–å‡ºæ³¨è§†ç›¸å…³çš„ç‰¹å¾è¡¨ç¤ºï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHARLæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šç‰¹å¾å¯¹é½æ¨¡å—å’Œç¨€ç–å›¾èåˆæ¨¡å—ã€‚ç‰¹å¾å¯¹é½æ¨¡å—è´Ÿè´£ä»é«˜è´¨é‡å›¾åƒä¸­æå–ç‰¹å¾å¹¶ä¸ä½è´¨é‡å›¾åƒè¿›è¡Œå¯¹é½ï¼Œè€Œç¨€ç–å›¾èåˆæ¨¡å—åˆ™ç”¨äºæ¢ç´¢æ³¨è§†æ–¹å‘ä¸å¤´éƒ¨å§¿æ€ä¹‹é—´çš„å‡ ä½•å…³ç³»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡æ— ç›‘ç£æ–¹å¼å®ç°ä½è´¨é‡å›¾åƒä¸é«˜è´¨é‡å›¾åƒç‰¹å¾çš„å¯¹é½ï¼Œæ˜¾è‘—é™ä½äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŒæ—¶å¼•å…¥ç¨€ç–å›¾èåˆæ¨¡å—æ¥å¢å¼ºæ³¨è§†è¡¨ç¤ºçš„å‡ ä½•çº¦æŸã€‚

**å…³é”®è®¾è®¡**ï¼šæˆ‘ä»¬åœ¨æ¨¡å‹ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç‰¹å¾å¯¹é½è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†ç¨€ç–å›¾èåˆçš„ç»“æ„ï¼Œä»¥æœ‰æ•ˆæ•æ‰æ³¨è§†æ–¹å‘ä¸å¤´éƒ¨å§¿æ€ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHARLæ¡†æ¶åœ¨EyeDiapã€MPIIFaceGazeå’ŒGaze360æ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†5.02Â°ã€3.36Â°å’Œ9.26Â°çš„æ³¨è§†ä¼°è®¡å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­çš„ç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæå‡ç³»ç»Ÿå¯¹ç”¨æˆ·æ³¨è§†è¡Œä¸ºçš„ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒHARLæ¡†æ¶æœ‰æœ›åœ¨æ™ºèƒ½è®¾å¤‡å’Œè¾…åŠ©æŠ€æœ¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨æ³¨è§†ä¼°è®¡æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

