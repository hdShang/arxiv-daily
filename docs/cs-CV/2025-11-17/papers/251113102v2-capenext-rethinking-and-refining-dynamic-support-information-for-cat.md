---
layout: default
title: CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation
---

# CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.13102" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.13102v2</a>
  <a href="https://arxiv.org/pdf/2511.13102.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13102v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.13102v2', 'CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Zhu, Dan Zeng, Shuiwang Li, Qijun Zhao, Qiaomu Shen, Bo Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17 (æ›´æ–°: 2025-12-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CapeNextï¼šé€šè¿‡ä¼˜åŒ–åŠ¨æ€æ”¯æŒä¿¡æ¯ï¼Œæ”¹è¿›ç±»åˆ«æ— å…³çš„å§¿æ€ä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ç±»åˆ«æ— å…³å§¿æ€ä¼°è®¡` `è·¨æ¨¡æ€äº¤äº’` `åŒæµç‰¹å¾ç»†åŒ–` `åŠ¨æ€æ”¯æŒä¿¡æ¯` `å§¿æ€ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç±»åˆ«æ— å…³å§¿æ€ä¼°è®¡æ–¹æ³•ä¾èµ–é™æ€æ–‡æœ¬æè¿°ï¼Œå¿½ç•¥äº†è·¨ç±»åˆ«å’Œç±»åˆ«å†…çš„è§†è§‰å·®å¼‚ã€‚
2. CapeNexté€šè¿‡åˆ†å±‚è·¨æ¨¡æ€äº¤äº’å’ŒåŒæµç‰¹å¾ç»†åŒ–ï¼Œèåˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ï¼Œå¢å¼ºè”åˆåµŒå…¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCapeNextåœ¨MP-100æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç±»åˆ«æ— å…³çš„å§¿æ€ä¼°è®¡(CAPE)é¢†åŸŸçš„ç ”ç©¶ï¼Œé€šå¸¸é‡‡ç”¨å›ºå®šçš„æ–‡æœ¬å…³é”®ç‚¹æè¿°ä½œä¸ºè¯­ä¹‰å…ˆéªŒï¼Œç”¨äºä¸¤é˜¶æ®µçš„å§¿æ€åŒ¹é…æ¡†æ¶ã€‚è¿™ç§èŒƒå¼é€šè¿‡è§£è€¦æ”¯æŒå›¾åƒçš„ä¾èµ–æ€§ï¼Œå¢å¼ºäº†é²æ£’æ€§å’Œçµæ´»æ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†é™æ€è”åˆåµŒå…¥çš„ä¸¤ä¸ªå›ºæœ‰å±€é™æ€§ï¼š(1)å¤šä¹‰æ€§å¯¼è‡´åŒ¹é…è¿‡ç¨‹ä¸­çš„è·¨ç±»åˆ«æ­§ä¹‰ï¼ˆä¾‹å¦‚ï¼Œâ€œè…¿â€åœ¨äººç±»å’Œå®¶å…·ä¸­è¡¨ç°å‡ºä¸åŒçš„è§†è§‰å½¢æ€ï¼‰ï¼›(2)å¯¹äºç»†ç²’åº¦çš„ç±»åˆ«å†…å·®å¼‚ï¼Œåˆ¤åˆ«æ€§ä¸è¶³ï¼ˆä¾‹å¦‚ï¼Œä¸€åªç¡è§‰çš„ç™½è‰²çŒ«å’Œä¸€åªç«™ç«‹çš„é»‘è‰²çŒ«åœ¨å§¿åŠ¿å’Œæ¯›çš®ä¸Šçš„å·®å¼‚ï¼‰ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ›æ–°æ€§åœ°å°†åˆ†å±‚è·¨æ¨¡æ€äº¤äº’ä¸åŒæµç‰¹å¾ç»†åŒ–ç›¸ç»“åˆï¼Œåˆ©ç”¨æ–‡æœ¬æè¿°å’Œç‰¹å®šå›¾åƒä¸­çš„ç±»çº§åˆ«å’Œå®ä¾‹ç‰¹å®šçº¿ç´¢æ¥å¢å¼ºè”åˆåµŒå…¥ã€‚åœ¨MP-100æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ— è®ºç½‘ç»œéª¨å¹²å¦‚ä½•ï¼ŒCapeNextå§‹ç»ˆå¤§å¹…ä¼˜äºæœ€å…ˆè¿›çš„CAPEæ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç±»åˆ«æ— å…³å§¿æ€ä¼°è®¡æ—¨åœ¨ä¼°è®¡å›¾åƒä¸­ç‰©ä½“çš„å…³é”®ç‚¹ä½ç½®ï¼Œè€Œæ— éœ€é¢„å…ˆçŸ¥é“ç‰©ä½“çš„ç±»åˆ«ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å›ºå®šçš„æ–‡æœ¬æè¿°ä½œä¸ºå…³é”®ç‚¹çš„è¯­ä¹‰å…ˆéªŒï¼Œä½†è¿™ç§æ–¹æ³•å¿½ç•¥äº†ä¸åŒç±»åˆ«ä¹‹é—´ä»¥åŠåŒä¸€ç±»åˆ«å†…éƒ¨çš„è§†è§‰å·®å¼‚ï¼Œå¯¼è‡´åŒ¹é…æ¨¡ç³Šå’Œåˆ¤åˆ«æ€§ä¸è¶³ã€‚ä¾‹å¦‚ï¼Œâ€œè…¿â€åœ¨äººå’Œæ¡Œå­ä¸Šçš„è§†è§‰è¡¨ç°å·®å¼‚å¾ˆå¤§ï¼Œè€ŒåŒä¸€å“ç§çš„çŒ«ä¹Ÿå¯èƒ½å› ä¸ºå§¿åŠ¿å’Œæ¯›è‰²ä¸åŒè€Œéš¾ä»¥åŒºåˆ†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCapeNextçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åŠ¨æ€åœ°èåˆæ–‡æœ¬æè¿°å’Œå›¾åƒä¿¡æ¯ï¼Œæ¥å¢å¼ºå…³é”®ç‚¹çš„è¯­ä¹‰è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ©ç”¨åˆ†å±‚è·¨æ¨¡æ€äº¤äº’æ¥æ•æ‰ç±»çº§åˆ«çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨åŒæµç‰¹å¾ç»†åŒ–æ¥æ•æ‰å®ä¾‹çº§åˆ«çš„è§†è§‰ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCapeNextå¯ä»¥æ›´å‡†ç¡®åœ°è¡¨ç¤ºå…³é”®ç‚¹çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCapeNextçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1)æ–‡æœ¬ç¼–ç å™¨ï¼šç”¨äºå°†æ–‡æœ¬æè¿°ç¼–ç æˆæ–‡æœ¬ç‰¹å¾ã€‚2)å›¾åƒç¼–ç å™¨ï¼šç”¨äºå°†å›¾åƒç¼–ç æˆå›¾åƒç‰¹å¾ã€‚3)åˆ†å±‚è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼šç”¨äºèåˆæ–‡æœ¬ç‰¹å¾å’Œå›¾åƒç‰¹å¾ï¼Œç”Ÿæˆç±»çº§åˆ«çš„è¯­ä¹‰è¡¨ç¤ºã€‚4)åŒæµç‰¹å¾ç»†åŒ–æ¨¡å—ï¼šç”¨äºè¿›ä¸€æ­¥ç»†åŒ–å›¾åƒç‰¹å¾ï¼Œç”Ÿæˆå®ä¾‹çº§åˆ«çš„è§†è§‰è¡¨ç¤ºã€‚5)å§¿æ€ä¼°è®¡æ¨¡å—ï¼šç”¨äºæ ¹æ®èåˆåçš„ç‰¹å¾ï¼Œä¼°è®¡å…³é”®ç‚¹çš„ä½ç½®ã€‚

**å…³é”®åˆ›æ–°**ï¼šCapeNextçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŠ¨æ€åœ°èåˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯çš„æ–¹å¼ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒCapeNextä¸æ˜¯ç®€å•åœ°å°†æ–‡æœ¬æè¿°ä½œä¸ºå›ºå®šçš„è¯­ä¹‰å…ˆéªŒï¼Œè€Œæ˜¯åˆ©ç”¨åˆ†å±‚è·¨æ¨¡æ€äº¤äº’å’ŒåŒæµç‰¹å¾ç»†åŒ–ï¼Œæ¥åŠ¨æ€åœ°è°ƒæ•´å…³é”®ç‚¹çš„è¯­ä¹‰è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´å¥½åœ°é€‚åº”ä¸åŒç±»åˆ«å’Œä¸åŒå®ä¾‹ä¹‹é—´çš„è§†è§‰å·®å¼‚ï¼Œä»è€Œæé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨åˆ†å±‚è·¨æ¨¡æ€äº¤äº’æ¨¡å—ä¸­ï¼ŒCapeNextä½¿ç”¨äº†å¤šå±‚Transformerç»“æ„ï¼Œæ¥æ•æ‰æ–‡æœ¬ç‰¹å¾å’Œå›¾åƒç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚åœ¨åŒæµç‰¹å¾ç»†åŒ–æ¨¡å—ä¸­ï¼ŒCapeNextä½¿ç”¨äº†æ®‹å·®è¿æ¥å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¥å¢å¼ºç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCapeNextè¿˜ä½¿ç”¨äº†å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œæ¥é¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´å…·åˆ¤åˆ«æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CapeNextåœ¨MP-100æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºä½¿ç”¨ä½•ç§ç½‘ç»œéª¨å¹²ï¼ŒCapeNextéƒ½å¤§å¹…ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨ResNet-50ä½œä¸ºéª¨å¹²ç½‘ç»œæ—¶ï¼ŒCapeNextçš„å¹³å‡ç²¾åº¦(AP)æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†5ä¸ªç™¾åˆ†ç‚¹ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒCapeNextèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ç±»åˆ«æ— å…³å§¿æ€ä¼°è®¡ä¸­çš„è·¨ç±»åˆ«æ­§ä¹‰å’Œç±»åˆ«å†…å·®å¼‚é—®é¢˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CapeNextåœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººé¢†åŸŸï¼ŒCapeNextå¯ä»¥å¸®åŠ©æœºå™¨äººè¯†åˆ«å’Œæ“ä½œå„ç§ç‰©ä½“ï¼Œè€Œæ— éœ€é¢„å…ˆçŸ¥é“ç‰©ä½“çš„ç±»åˆ«ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼ŒCapeNextå¯ä»¥å¸®åŠ©è½¦è¾†è¯†åˆ«è¡Œäººã€è½¦è¾†ç­‰äº¤é€šå‚ä¸è€…ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åœ¨æ™ºèƒ½ç›‘æ§é¢†åŸŸï¼ŒCapeNextå¯ä»¥å¸®åŠ©ç›‘æ§ç³»ç»Ÿè¯†åˆ«å¼‚å¸¸è¡Œä¸ºï¼Œä»è€Œæé«˜å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

