---
layout: default
title: Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding
---

# Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.13924" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.13924v1</a>
  <a href="https://arxiv.org/pdf/2511.13924.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13924v1" onclick="toggleFavorite(this, '2511.13924v1', 'Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingyang Yan, Guangyao Chen, Yixiong Zou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

**å¤‡æ³¨**: AAAI 2026 (Oral)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/qyoung-yan/CuRPO)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–CuRPOï¼Œæå‡è§†è§‰å®šä½ä»»åŠ¡ä¸­CoTæ¨ç†çš„æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰å®šä½` `Chain-of-Thought` `è¯¾ç¨‹å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„CoTæ¨ç†åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æˆ–å†—é•¿çš„CoTè¾“å‡ºæ—¶ã€‚
2. æå‡ºCuRPOï¼Œåˆ©ç”¨CoTé•¿åº¦å’ŒgIoUå¥–åŠ±ä½œä¸ºå¤æ‚åº¦æŒ‡æ ‡ï¼Œé€šè¿‡è¯¾ç¨‹å­¦ä¹ çš„æ–¹å¼ï¼Œç”±ç®€å…¥éš¾åœ°ç»„ç»‡è®­ç»ƒæ•°æ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCuRPOåœ¨å¤šä¸ªè§†è§‰å®šä½æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å‘ç°ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„CoTæ¨ç†åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨CoTè¾“å‡ºå†—é•¿æˆ–å¤æ‚æ—¶ï¼Œåè€Œä¼šé™ä½æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç”±äºæ•°æ®å¤æ‚åº¦çš„å·®å¼‚ï¼Œå¢åŠ æ•°æ®é›†å¤§å°å¹¶ä¸æ€»èƒ½æé«˜æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥â€”â€”åŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆCuRPOï¼‰ã€‚CuRPOåˆ©ç”¨CoTé•¿åº¦å’Œå¹¿ä¹‰äº¤å¹¶æ¯”ï¼ˆgIoUï¼‰å¥–åŠ±ä½œä¸ºå¤æ‚åº¦æŒ‡æ ‡ï¼Œé€æ­¥æ„å»ºè®­ç»ƒæ•°æ®ï¼Œä»ç®€å•åˆ°æ›´å…·æŒ‘æˆ˜æ€§çš„ç¤ºä¾‹ã€‚åœ¨RefCOCOã€RefCOCO+ã€RefCOCOgå’ŒLISAæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚CuRPOå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬Visual-RFTï¼Œåœ¨RefCOCOä¸Šå®ç°äº†é«˜è¾¾+12.52 mAPçš„æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒCuRPOè¡¨ç°å‡ºå“è¶Šçš„æ•ˆç‡å’Œé²æ£’æ€§ï¼Œå³ä½¿åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ä¹Ÿèƒ½æä¾›å¼ºå¤§çš„å®šä½æ€§èƒ½ï¼Œå°¤å…¶æœ‰åˆ©äºä»¥æ¨¡ç³Šå’Œå¤æ‚çš„æ–‡æœ¬æè¿°ä¸ºç‰¹å¾çš„ä»»åŠ¡ã€‚ä»£ç å·²å¼€æºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰å®šä½ä»»åŠ¡æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°ï¼Œåœ¨å›¾åƒä¸­å®šä½åˆ°å¯¹åº”çš„ç›®æ ‡åŒºåŸŸã€‚ç°æœ‰çš„åŸºäºChain-of-Thought (CoT) çš„æ–¹æ³•è™½ç„¶åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­ï¼Œç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒCoTæ¨ç†åè€Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨CoTè¾“å‡ºå˜å¾—å¤æ‚æˆ–å†—é•¿æ—¶ã€‚æ­¤å¤–ï¼Œç®€å•åœ°å¢åŠ æ•°æ®é›†å¤§å°å¹¶ä¸ä¸€å®šèƒ½æå‡æ€§èƒ½ï¼Œå› ä¸ºæ•°æ®é›†ä¸­çš„æ ·æœ¬å¤æ‚åº¦å„ä¸ç›¸åŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCuRPOçš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ çš„æ–¹å¼ï¼Œé€æ­¥å¼•å…¥æ›´å¤æ‚çš„è®­ç»ƒæ ·æœ¬ï¼Œä»è€Œé¿å…æ¨¡å‹åœ¨è®­ç»ƒåˆæœŸå°±è¢«è¿‡äºå¤æ‚çš„CoTæ¨ç†è¿‡ç¨‹æ‰€å›°æ‰°ã€‚é€šè¿‡å°†CoTé•¿åº¦å’ŒgIoUå¥–åŠ±ä½œä¸ºå¤æ‚åº¦æŒ‡æ ‡ï¼ŒCuRPOèƒ½å¤Ÿæœ‰æ•ˆåœ°ç»„ç»‡è®­ç»ƒæ•°æ®ï¼Œä»ç®€å•åˆ°å¤æ‚åœ°è¿›è¡Œå­¦ä¹ ã€‚è¿™ç§ç”±ç®€å…¥éš¾çš„å­¦ä¹ æ–¹å¼æœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°ç†è§£æ–‡æœ¬æè¿°å’Œå›¾åƒä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜å®šä½ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCuRPOçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) CoTæ¨ç†æ¨¡å—ï¼šç”¨äºç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼›2) ç­–ç•¥ä¼˜åŒ–æ¨¡å—ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¼˜åŒ–CoTæ¨ç†ç­–ç•¥ï¼›3) è¯¾ç¨‹å­¦ä¹ æ¨¡å—ï¼šæ ¹æ®CoTé•¿åº¦å’ŒgIoUå¥–åŠ±åŠ¨æ€è°ƒæ•´è®­ç»ƒæ•°æ®çš„éš¾åº¦ã€‚è®­ç»ƒè¿‡ç¨‹é¦–å…ˆä»ç®€å•çš„æ ·æœ¬å¼€å§‹ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ­¥å¼•å…¥æ›´å¤æ‚çš„æ ·æœ¬ã€‚åœ¨æ¯ä¸ªè®­ç»ƒè¿­ä»£ä¸­ï¼Œæ¨¡å‹æ ¹æ®å½“å‰çš„ç­–ç•¥ç”ŸæˆCoTæ¨ç†è¿‡ç¨‹ï¼Œå¹¶æ ¹æ®gIoUå¥–åŠ±æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šCuRPOæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†è¯¾ç¨‹å­¦ä¹ ä¸ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç›¸ç»“åˆï¼Œä»è€Œæœ‰æ•ˆåœ°è§£å†³äº†CoTæ¨ç†åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­é‡åˆ°çš„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒCuRPOä¸æ˜¯ç›´æ¥ä¼˜åŒ–æ•´ä¸ªCoTæ¨ç†è¿‡ç¨‹ï¼Œè€Œæ˜¯é€šè¿‡è¯¾ç¨‹å­¦ä¹ çš„æ–¹å¼ï¼Œé€æ­¥å¼•å…¥æ›´å¤æ‚çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œé¿å…äº†æ¨¡å‹åœ¨è®­ç»ƒåˆæœŸå°±è¢«è¿‡äºå¤æ‚çš„æ¨ç†è¿‡ç¨‹æ‰€å›°æ‰°ã€‚æ­¤å¤–ï¼ŒCuRPOè¿˜ä½¿ç”¨äº†ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨äº†CoTæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šCuRPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨CoTé•¿åº¦å’ŒgIoUå¥–åŠ±ä½œä¸ºå¤æ‚åº¦æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡è®­ç»ƒæ ·æœ¬çš„éš¾åº¦ï¼›2) è®¾è®¡äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´è®­ç»ƒæ•°æ®çš„éš¾åº¦ï¼›3) ä½¿ç”¨ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨CoTæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´ä¿¡æ¯ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºå…·ä½“çš„è§†è§‰å®šä½ä»»åŠ¡å’Œæ•°æ®é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CuRPOåœ¨RefCOCOã€RefCOCO+ã€RefCOCOgå’ŒLISAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜CuRPOå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬Visual-RFTã€‚åœ¨RefCOCOæ•°æ®é›†ä¸Šï¼ŒCuRPOå®ç°äº†é«˜è¾¾+12.52 mAPçš„æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒCuRPOåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CuRPOåœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯åº”ç”¨äºæ™ºèƒ½é›¶å”®ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½é›¶å”®ä¸­ï¼ŒCuRPOå¯ä»¥å¸®åŠ©æœºå™¨äººæ ¹æ®é¡¾å®¢çš„è¯­éŸ³æŒ‡ä»¤ï¼Œå‡†ç¡®åœ°å®šä½åˆ°è´§æ¶ä¸Šçš„å•†å“ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒCuRPOå¯ä»¥å¸®åŠ©è½¦è¾†æ ¹æ®äº¤é€šæ ‡å¿—çš„æ–‡æœ¬æè¿°ï¼Œå‡†ç¡®åœ°è¯†åˆ«äº¤é€šæ ‡å¿—ã€‚æœªæ¥ï¼ŒCuRPOæœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Chain-of-Thought (CoT) prompting has recently shown significant promise across various NLP and computer vision tasks by explicitly generating intermediate reasoning steps. However, we find that reinforcement learning (RL)-based fine-tuned CoT reasoning can paradoxically degrade performance in Visual Grounding tasks, particularly as CoT outputs become lengthy or complex. Additionally, our analysis reveals that increased dataset size does not always enhance performance due to varying data complexities. Motivated by these findings, we propose Curriculum-based Relative Policy Optimization (CuRPO), a novel training strategy that leverages CoT length and generalized Intersection over Union (gIoU) rewards as complexity indicators to progressively structure training data from simpler to more challenging examples. Extensive experiments on RefCOCO, RefCOCO+, RefCOCOg, and LISA datasets demonstrate the effectiveness of our approach. CuRPO consistently outperforms existing methods, including Visual-RFT, with notable improvements of up to +12.52 mAP on RefCOCO. Moreover, CuRPO exhibits exceptional efficiency and robustness, delivering strong localization performance even in few-shot learning scenarios, particularly benefiting tasks characterized by ambiguous and intricate textual descriptions.The code is released on https://github.com/qyoung-yan/CuRPO.

