---
layout: default
title: CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation
---

# CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.12919" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.12919v3</a>
  <a href="https://arxiv.org/pdf/2511.12919.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12919v3" onclick="toggleFavorite(this, '2511.12919v3', 'CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dexin Zuo, Ang Li, Wei Wang, Wenxian Yu, Danping Zou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17 (æ›´æ–°: 2025-12-15)

**å¤‡æ³¨**: 7 pages, accepted by AAAI 2026 (oral)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CoordARï¼šåŸºäºè‡ªå›å½’åæ ‡å›¾ç”Ÿæˆçš„å•å‚è€ƒæ–°ç‰©ä½“6Dä½å§¿ä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `6Dä½å§¿ä¼°è®¡` `å•å‚è€ƒå›¾åƒ` `è‡ªå›å½’æ¨¡å‹` `åæ ‡å›¾ç”Ÿæˆ` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå•å‚è€ƒå›¾åƒçš„6Dä½å§¿ä¼°è®¡æ–¹æ³•ä¾èµ–å®å€¼åæ ‡å›å½’ï¼Œç¼ºä¹å…¨å±€ä¸€è‡´æ€§ï¼Œéš¾ä»¥å¤„ç†å¯¹ç§°å’Œé®æŒ¡æƒ…å†µã€‚
2. CoordARæå‡ºä¸€ç§è‡ªå›å½’æ¡†æ¶ï¼Œå°†3D-3Då¯¹åº”å…³ç³»å»ºæ¨¡ä¸ºç¦»æ•£tokençš„æ˜ å°„ï¼Œé€šè¿‡æ¦‚ç‡è‡ªå›å½’æ–¹å¼é¢„æµ‹åæ ‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCoordARåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¯¹å¯¹ç§°ã€é®æŒ¡ç­‰æƒ…å†µå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºCoordARï¼Œä¸€ç§ç”¨äºæ–°ç‰©ä½“å•å‚è€ƒ6Dä½å§¿ä¼°è®¡çš„è‡ªå›å½’æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–å®å€¼åæ ‡å›å½’ï¼Œå­˜åœ¨å…¨å±€ä¸€è‡´æ€§ä¸è¶³ã€å¯¹ç§°æˆ–é®æŒ¡åœºæ™¯ä¸‹ç¼ºä¹ä¸ç¡®å®šæ€§å»ºæ¨¡ç­‰é—®é¢˜ï¼ŒCoordARå°†å‚è€ƒè§†å›¾å’ŒæŸ¥è¯¢è§†å›¾ä¹‹é—´çš„3D-3Då¯¹åº”å…³ç³»å»ºæ¨¡ä¸ºç¦»æ•£tokençš„æ˜ å°„ï¼Œå¹¶é€šè¿‡è‡ªå›å½’å’Œæ¦‚ç‡çš„æ–¹å¼è·å¾—ã€‚ä¸ºäº†å®ç°ç²¾ç¡®çš„å¯¹åº”å…³ç³»å›å½’ï¼ŒCoordARå¼•å…¥äº†ï¼š1) ä¸€ç§æ–°é¢–çš„åæ ‡å›¾tokenåŒ–æ–¹æ³•ï¼Œæ”¯æŒåœ¨ç¦»æ•£3Dç©ºé—´ä¸Šçš„æ¦‚ç‡é¢„æµ‹ï¼›2) ä¸€ç§æ¨¡æ€è§£è€¦ç¼–ç ç­–ç•¥ï¼Œåˆ†åˆ«ç¼–ç RGBå¤–è§‚å’Œåæ ‡çº¿ç´¢ï¼›3) ä¸€ä¸ªè‡ªå›å½’Transformerè§£ç å™¨ï¼Œä»¥ä½ç½®å¯¹é½çš„æŸ¥è¯¢ç‰¹å¾å’Œéƒ¨åˆ†ç”Ÿæˆçš„tokenåºåˆ—ä¸ºæ¡ä»¶ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCoordARæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œæµ‹è¯•ä¸­è¡¨ç°å‡ºå¯¹å¯¹ç§°æ€§ã€é®æŒ¡å’Œå…¶ä»–æŒ‘æˆ˜çš„å¼ºå¤§é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå•å‚è€ƒå›¾åƒçš„6Dä½å§¿ä¼°è®¡æ–¹æ³•ï¼Œä¾èµ–äºç›´æ¥å›å½’3Dåæ ‡ï¼Œè¿™ç§æ–¹æ³•å—é™äºå·ç§¯ç¥ç»ç½‘ç»œçš„å±€éƒ¨æ€§ï¼Œéš¾ä»¥ä¿è¯å…¨å±€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œåœ¨ç‰©ä½“å…·æœ‰å¯¹ç§°æ€§æˆ–å­˜åœ¨é®æŒ¡æ—¶ï¼Œåæ ‡å›å½’çš„ä¸ç¡®å®šæ€§éš¾ä»¥å»ºæ¨¡ï¼Œå¯¼è‡´ä½å§¿ä¼°è®¡ç²¾åº¦ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoordARçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†3Dåæ ‡å›å½’é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªç¦»æ•£tokençš„ç”Ÿæˆé—®é¢˜ï¼Œåˆ©ç”¨è‡ªå›å½’æ¨¡å‹é¢„æµ‹å‚è€ƒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„3D-3Då¯¹åº”å…³ç³»ã€‚é€šè¿‡å°†3Dç©ºé—´ç¦»æ•£åŒ–ä¸ºä¸€ç³»åˆ—tokenï¼Œå¹¶ä½¿ç”¨Transformerè¿›è¡Œåºåˆ—å»ºæ¨¡ï¼Œå¯ä»¥æ›´å¥½åœ°æ•æ‰å…¨å±€ä¿¡æ¯å’Œå»ºæ¨¡ä¸ç¡®å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoordARçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) åæ ‡å›¾tokenåŒ–æ¨¡å—ï¼Œå°†3Dç©ºé—´ç¦»æ•£åŒ–ä¸ºtokenï¼›2) æ¨¡æ€è§£è€¦ç¼–ç å™¨ï¼Œåˆ†åˆ«æå–å‚è€ƒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒçš„RGBå¤–è§‚ç‰¹å¾å’Œåæ ‡ä¿¡æ¯ï¼›3) è‡ªå›å½’Transformerè§£ç å™¨ï¼Œä»¥ä½ç½®å¯¹é½çš„æŸ¥è¯¢ç‰¹å¾å’Œéƒ¨åˆ†ç”Ÿæˆçš„tokenåºåˆ—ä¸ºæ¡ä»¶ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œä»è€Œç”Ÿæˆå®Œæ•´çš„åæ ‡å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoordARçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åæ ‡å›¾tokenåŒ–æ–¹æ³•ï¼Œå°†è¿ç»­çš„3Dåæ ‡ç©ºé—´ç¦»æ•£åŒ–ä¸ºtokenï¼Œä¾¿äºä½¿ç”¨åºåˆ—æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼›2) é‡‡ç”¨äº†æ¨¡æ€è§£è€¦ç¼–ç ç­–ç•¥ï¼Œåˆ†åˆ«ç¼–ç RGBå¤–è§‚å’Œåæ ‡ä¿¡æ¯ï¼Œé¿å…äº†ä¸åŒæ¨¡æ€ä¿¡æ¯ä¹‹é—´çš„å¹²æ‰°ï¼›3) ä½¿ç”¨è‡ªå›å½’Transformerè§£ç å™¨ï¼Œèƒ½å¤Ÿæ•æ‰å…¨å±€ä¿¡æ¯å’Œå»ºæ¨¡ä¸ç¡®å®šæ€§ï¼Œä»è€Œæé«˜ä½å§¿ä¼°è®¡çš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåæ ‡å›¾tokenåŒ–ï¼šå°†3Dç©ºé—´åˆ’åˆ†ä¸ºå¤šä¸ªä½“ç´ ï¼Œæ¯ä¸ªä½“ç´ å¯¹åº”ä¸€ä¸ªtokenã€‚æ¨¡æ€è§£è€¦ç¼–ç å™¨ï¼šä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„å·ç§¯ç¥ç»ç½‘ç»œåˆ†åˆ«æå–RGBå¤–è§‚ç‰¹å¾å’Œåæ ‡ä¿¡æ¯ã€‚è‡ªå›å½’Transformerè§£ç å™¨ï¼šä½¿ç”¨æ ‡å‡†çš„Transformerç»“æ„ï¼Œå¹¶å¼•å…¥ä½ç½®ç¼–ç æ¥è¡¨ç¤ºtokençš„ä½ç½®ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°ï¼šä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒè‡ªå›å½’Transformerè§£ç å™¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CoordARåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨MOPEDæ•°æ®é›†ä¸Šï¼ŒCoordARçš„ä½å§¿ä¼°è®¡ç²¾åº¦æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†X%ã€‚æ­¤å¤–ï¼ŒCoordARåœ¨å¤„ç†å¯¹ç§°ç‰©ä½“å’Œé®æŒ¡åœºæ™¯æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚çœŸå®ä¸–ç•Œæµ‹è¯•ä¹ŸéªŒè¯äº†CoordARçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoordARåœ¨æœºå™¨äººæ“ä½œã€å¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæŠ“å–ä»»åŠ¡ä¸­ï¼Œå¯ä»¥åˆ©ç”¨CoordARä¼°è®¡ç‰©ä½“çš„6Dä½å§¿ï¼Œä»è€Œå¼•å¯¼æœºå™¨äººå‡†ç¡®æŠ“å–ç‰©ä½“ã€‚åœ¨å¢å¼ºç°å®åº”ç”¨ä¸­ï¼Œå¯ä»¥å°†è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯è¿›è¡Œç²¾ç¡®å¯¹é½ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæ¨åŠ¨æ— æ¨¡å‹ç‰©ä½“ä½å§¿ä¼°è®¡æŠ€æœ¯çš„å‘å±•ï¼Œé™ä½å¯¹3Dæ¨¡å‹çš„ä¾èµ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

