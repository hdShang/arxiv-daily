---
layout: default
title: GRLoc: Geometric Representation Regression for Visual Localization
---

# GRLoc: Geometric Representation Regression for Visual Localization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.13864" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.13864v1</a>
  <a href="https://arxiv.org/pdf/2511.13864.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13864v1" onclick="toggleFavorite(this, '2511.13864v1', 'GRLoc: Geometric Representation Regression for Visual Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Changyang Li, Xuejian Ma, Lixiang Liu, Zhan Li, Qingan Yan, Yi Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGRLocï¼šé€šè¿‡å‡ ä½•è¡¨ç¤ºå›å½’å®ç°æ›´é²æ£’çš„è§†è§‰å®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰å®šä½` `ç»å¯¹ä½å§¿å›å½’` `å‡ ä½•è¡¨ç¤ºå­¦ä¹ ` `é€†æ¸²æŸ“` `ç›¸æœºä½å§¿ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿç»å¯¹ä½å§¿å›å½’æ¨¡å‹ç¼ºä¹å¯¹3Dåœºæ™¯å‡ ä½•çš„ç†è§£ï¼Œå®¹æ˜“è®°å¿†è®­ç»ƒæ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. è®ºæ–‡æå‡ºå‡ ä½•è¡¨ç¤ºå›å½’(GRR)æ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹è§£è€¦çš„å‡ ä½•è¡¨ç¤ºï¼ˆå…‰çº¿æŸæ–¹å‘å’Œç‚¹å›¾ï¼‰æ¥ä¼°è®¡ç›¸æœºä½å§¿ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGRRåœ¨7-Sceneså’ŒCambridge Landmarksæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼ŒéªŒè¯äº†é€†æ¸²æŸ“å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»å¯¹ä½å§¿å›å½’(APR)å·²æˆä¸ºè§†è§‰å®šä½çš„ä¸€ç§å¼•äººæ³¨ç›®çš„èŒƒä¾‹ã€‚ç„¶è€Œï¼ŒAPRæ¨¡å‹é€šå¸¸ä½œä¸ºé»‘ç›’è¿è¡Œï¼Œç›´æ¥ä»æŸ¥è¯¢å›¾åƒå›å½’6è‡ªç”±åº¦(6-DoF)ä½å§¿ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹è®°å¿†è®­ç»ƒè§†å›¾ï¼Œè€Œä¸æ˜¯ç†è§£3Dåœºæ™¯å‡ ä½•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•çš„æ›¿ä»£æ–¹æ¡ˆã€‚å—åˆ°æ–°è§†è§’åˆæˆçš„å¯å‘ï¼Œè¯¥æ–¹æ³•ä»ä¸­é—´å‡ ä½•è¡¨ç¤ºæ¸²æŸ“å›¾åƒï¼Œæˆ‘ä»¬å°†APRé‡æ–°å®šä¹‰ä¸ºå…¶é€†è¿‡ç¨‹ï¼Œå³ç›´æ¥ä»å›¾åƒå›å½’æ½œåœ¨çš„3Dè¡¨ç¤ºï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå‡ ä½•è¡¨ç¤ºå›å½’(GRR)ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¾å¼åœ°é¢„æµ‹ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä¸¤ä¸ªè§£è€¦çš„å‡ ä½•è¡¨ç¤ºï¼š(1)ç”¨äºä¼°è®¡ç›¸æœºæ—‹è½¬çš„å…‰çº¿æŸæ–¹å‘ï¼Œä»¥åŠ(2)ç”¨äºä¼°è®¡ç›¸æœºå¹³ç§»çš„ç›¸åº”ç‚¹å›¾ã€‚ç„¶åä½¿ç”¨å¯å¾®ç¡®å®šæ€§æ±‚è§£å™¨ä»è¿™äº›å‡ ä½•åˆ†é‡ä¸­æ¢å¤æœ€ç»ˆçš„6-DoFç›¸æœºä½å§¿ã€‚è¿™ç§è§£è€¦æ–¹æ³•å°†å­¦ä¹ åˆ°çš„è§†è§‰åˆ°å‡ ä½•çš„æ˜ å°„ä¸æœ€ç»ˆçš„ä½å§¿è®¡ç®—åˆ†ç¦»ï¼Œä»è€Œå°†å¼ºå¤§çš„å‡ ä½•å…ˆéªŒå¼•å…¥ç½‘ç»œã€‚æˆ‘ä»¬å‘ç°ï¼Œæ˜¾å¼åœ°è§£è€¦æ—‹è½¬å’Œå¹³ç§»é¢„æµ‹å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨7-Sceneså’ŒCambridge Landmarksæ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å¯¹é€†æ¸²æŸ“è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡æ˜¯å®ç°é€šç”¨ç»å¯¹ä½å§¿ä¼°è®¡çš„æ›´é²æ£’çš„é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç»å¯¹ä½å§¿å›å½’(APR)æ—¨åœ¨ç›´æ¥ä»å›¾åƒé¢„æµ‹ç›¸æœºçš„6è‡ªç”±åº¦ä½å§¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„APRæ–¹æ³•é€šå¸¸å°†æ¨¡å‹è§†ä¸ºé»‘ç›’ï¼Œç¼ºä¹å¯¹åœºæ™¯å‡ ä½•çš„æ˜¾å¼å»ºæ¨¡ï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥åº”å¯¹å…‰ç…§å˜åŒ–ã€è§†è§’å·®å¼‚ç­‰æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†APRé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªé€†æ¸²æŸ“é—®é¢˜ã€‚é€šè¿‡é¢„æµ‹å›¾åƒå¯¹åº”çš„3Då‡ ä½•è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å›å½’ä½å§¿ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£åœºæ™¯ç»“æ„ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹é¢„æµ‹ä¸¤ä¸ªè§£è€¦çš„å‡ ä½•è¡¨ç¤ºï¼šå…‰çº¿æŸæ–¹å‘ï¼ˆç”¨äºä¼°è®¡æ—‹è½¬ï¼‰å’Œç‚¹å›¾ï¼ˆç”¨äºä¼°è®¡å¹³ç§»ï¼‰ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGRLocæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šæå–è¾“å…¥å›¾åƒçš„è§†è§‰ç‰¹å¾ã€‚2) å‡ ä½•è¡¨ç¤ºå›å½’æ¨¡å—ï¼šåŸºäºè§†è§‰ç‰¹å¾ï¼Œé¢„æµ‹å…‰çº¿æŸæ–¹å‘å’Œç‚¹å›¾ã€‚3) ä½å§¿æ±‚è§£æ¨¡å—ï¼šåˆ©ç”¨å¯å¾®çš„ç¡®å®šæ€§æ±‚è§£å™¨ï¼Œä»å…‰çº¿æŸæ–¹å‘å’Œç‚¹å›¾ä¸­æ¢å¤ç›¸æœºçš„6è‡ªç”±åº¦ä½å§¿ã€‚æ•´ä¸ªæ¡†æ¶æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šGRLocçš„å…³é”®åˆ›æ–°åœ¨äºå°†ç»å¯¹ä½å§¿å›å½’é—®é¢˜è½¬åŒ–ä¸ºå‡ ä½•è¡¨ç¤ºå›å½’é—®é¢˜ï¼Œå¹¶æ˜¾å¼åœ°è§£è€¦äº†æ—‹è½¬å’Œå¹³ç§»çš„é¢„æµ‹ã€‚è¿™ç§è§£è€¦æ–¹å¼å¼•å…¥äº†æ›´å¼ºçš„å‡ ä½•å…ˆéªŒï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åœºæ™¯ç»“æ„ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚ä¸ç›´æ¥å›å½’ä½å§¿çš„æ–¹æ³•ç›¸æ¯”ï¼ŒGRLocæ›´å…³æ³¨å­¦ä¹ å›¾åƒä¸3Då‡ ä½•ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‡ ä½•è¡¨ç¤ºå›å½’æ¨¡å—ä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†å·ç§¯ç¥ç»ç½‘ç»œæ¥é¢„æµ‹å…‰çº¿æŸæ–¹å‘å’Œç‚¹å›¾ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬å…‰çº¿æŸæ–¹å‘çš„å›å½’æŸå¤±å’Œç‚¹å›¾çš„å›å½’æŸå¤±ã€‚ä½å§¿æ±‚è§£æ¨¡å—é‡‡ç”¨äº†ä¸€ç§å¯å¾®çš„è¿­ä»£æœ€è¿‘ç‚¹(ICP)ç®—æ³•ï¼Œä»¥ç¡®ä¿æ•´ä¸ªæ¡†æ¶çš„å¯å¾®æ€§ã€‚å…·ä½“ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GRLocåœ¨7-Sceneså’ŒCambridge Landmarksæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨7-Scenesæ•°æ®é›†ä¸Šï¼ŒGRLocçš„å¹³å‡å®šä½è¯¯å·®æ˜¾è‘—ä½äºç°æœ‰æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ˜¾å¼åœ°è§£è€¦æ—‹è½¬å’Œå¹³ç§»é¢„æµ‹å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼ŒéªŒè¯äº†é€†æ¸²æŸ“å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¢å¼ºç°å®ã€æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡æ›´å‡†ç¡®çš„è§†è§‰å®šä½ï¼Œå¯ä»¥æå‡ARåº”ç”¨çš„æ²‰æµ¸æ„Ÿï¼Œæé«˜æœºå™¨äººå¯¼èˆªçš„ç²¾åº¦å’Œé²æ£’æ€§ï¼Œä»¥åŠå¢å¼ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡ã€æ›´å¤æ‚çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Absolute Pose Regression (APR) has emerged as a compelling paradigm for visual localization. However, APR models typically operate as black boxes, directly regressing a 6-DoF pose from a query image, which can lead to memorizing training views rather than understanding 3D scene geometry. In this work, we propose a geometrically-grounded alternative. Inspired by novel view synthesis, which renders images from intermediate geometric representations, we reformulate APR as its inverse that regresses the underlying 3D representations directly from the image, and we name this paradigm Geometric Representation Regression (GRR). Our model explicitly predicts two disentangled geometric representations in the world coordinate system: (1) a ray bundle's directions to estimate camera rotation, and (2) a corresponding pointmap to estimate camera translation. The final 6-DoF camera pose is then recovered from these geometric components using a differentiable deterministic solver. This disentangled approach, which separates the learned visual-to-geometry mapping from the final pose calculation, introduces a strong geometric prior into the network. We find that the explicit decoupling of rotation and translation predictions measurably boosts performance. We demonstrate state-of-the-art performance on 7-Scenes and Cambridge Landmarks datasets, validating that modeling the inverse rendering process is a more robust path toward generalizable absolute pose estimation.

