---
layout: default
title: Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views
---

# Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.12878" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.12878v3</a>
  <a href="https://arxiv.org/pdf/2511.12878.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12878v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.12878v3', 'Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junyi Ma, Wentao Bao, Jingyi Xu, Guanzhong Sun, Yu Zheng, Erhang Zhang, Xieyuanli Chen, Hesheng Wang

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17 (æ›´æ–°: 2025-12-05)

**å¤‡æ³¨**: Extended journal version of MMTwin (IROS'25). Code and data: https://github.com/IRMVLab/UniHand

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Uni-Handï¼šç”¨äºç¬¬ä¸€äººç§°è§†è§’çš„é€šç”¨æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹` `ç¬¬ä¸€äººç§°è§†è§’` `å¤šæ¨¡æ€èåˆ` `æ‰©æ•£æ¨¡å‹` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹æ–¹æ³•åœ¨é¢„æµ‹ç›®æ ‡ã€æ¨¡æ€èåˆã€è¿åŠ¨è§£è€¦å’Œä¸‹æ¸¸ä»»åŠ¡éªŒè¯æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
2. Uni-Handé€šè¿‡å¤šæ¨¡æ€èåˆã€åŒåˆ†æ”¯æ‰©æ•£å’Œç›®æ ‡æŒ‡ç¤ºå™¨ï¼Œå®ç°å¤šç»´åº¦ã€å¤šç›®æ ‡çš„æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUni-Handåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°äº†è‰¯å¥½çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ¡†æ¶Uni-Handï¼Œæ—¨åœ¨è§£å†³ç¬¬ä¸€äººç§°è§†è§’ä¸‹æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ä¸­é¢„æµ‹ç›®æ ‡ä¸è¶³ã€æ¨¡æ€å·®è·ã€æ‰‹-å¤´è¿åŠ¨è€¦åˆä»¥åŠä¸‹æ¸¸ä»»åŠ¡éªŒè¯æœ‰é™ç­‰é—®é¢˜ã€‚Uni-Handé€šè¿‡è§†è§‰-è¯­è¨€èåˆã€å…¨å±€ä¸Šä¸‹æ–‡èåˆä»¥åŠä»»åŠ¡æ„ŸçŸ¥æ–‡æœ¬åµŒå…¥æ³¨å…¥æ¥åè°ƒå¤šæ¨¡æ€è¾“å…¥ï¼Œä»è€Œé¢„æµ‹2Då’Œ3Dç©ºé—´ä¸­çš„æ‰‹éƒ¨è½¨è¿¹ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåŒæ—¶é¢„æµ‹å¤´éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨ï¼Œæ•æ‰å®ƒä»¬åœ¨ç¬¬ä¸€äººç§°è§†è§’ä¸­çš„è¿åŠ¨ååŒã€‚é€šè¿‡å¼•å…¥ç›®æ ‡æŒ‡ç¤ºå™¨ï¼Œè¯¥æ¨¡å‹å¯ä»¥é¢„æµ‹æ‰‹è…•æˆ–æ‰‹æŒ‡çš„ç‰¹å®šå…³èŠ‚è½¨è¿¹ç‚¹ï¼Œè€Œä¸ä»…ä»…æ˜¯æ‰‹éƒ¨ä¸­å¿ƒç‚¹ã€‚Uni-Handè¿˜èƒ½å¤Ÿé¢„æµ‹æ‰‹-ç‰©äº¤äº’çŠ¶æ€ï¼ˆæ¥è§¦/åˆ†ç¦»ï¼‰ï¼Œä»¥æ›´å¥½åœ°ä¿ƒè¿›ä¸‹æ¸¸ä»»åŠ¡ã€‚ä½œä¸ºé¦–ä¸ªå°†ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°çº³å…¥æ–‡çŒ®çš„å·¥ä½œï¼Œæˆ‘ä»¬æ„å»ºäº†æ–°çš„åŸºå‡†æ¥è¯„ä¼°æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ç®—æ³•çš„å®é™…åº”ç”¨æ€§ã€‚åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†å’Œæˆ‘ä»¬æ–°æå‡ºçš„åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-Handåœ¨å¤šç»´åº¦å’Œå¤šç›®æ ‡æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¹¿æ³›éªŒè¯ä¹Ÿå±•ç¤ºäº†å…¶ä»¤äººå°è±¡æ·±åˆ»çš„äºº-æœºå™¨äººç­–ç•¥è¿ç§»èƒ½åŠ›ï¼Œä»è€Œå®ç°æœºå™¨äººæ“ä½œï¼Œå¹¶æœ‰æ•ˆå¢å¼ºåŠ¨ä½œé¢„æµ‹/è¯†åˆ«ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹æ–¹æ³•é€šå¸¸åªå…³æ³¨æ‰‹éƒ¨ä¸­å¿ƒç‚¹çš„é¢„æµ‹ï¼Œå¿½ç•¥äº†æ‰‹æŒ‡ç­‰å…¶ä»–å…³é”®éƒ¨ä½çš„è¿åŠ¨é¢„æµ‹ã€‚æ­¤å¤–ï¼Œç¬¬ä¸€äººç§°è§†è§’ä¸‹æ‰‹éƒ¨å’Œå¤´éƒ¨çš„è¿åŠ¨å­˜åœ¨è€¦åˆå…³ç³»ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè§£è€¦ã€‚åŒæ—¶ï¼Œç¼ºä¹é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆéªŒè¯ï¼Œéš¾ä»¥è¯„ä¼°ç®—æ³•çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUni-Handçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªé€šç”¨çš„æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œé¢„æµ‹å¤šç»´åº¦çš„æ‰‹éƒ¨è¿åŠ¨è½¨è¿¹ï¼Œå¹¶è€ƒè™‘æ‰‹éƒ¨ä¸å¤´éƒ¨çš„è¿åŠ¨ååŒã€‚é€šè¿‡å¼•å…¥ç›®æ ‡æŒ‡ç¤ºå™¨ï¼Œå®ç°å¯¹ä¸åŒæ‰‹éƒ¨å…³èŠ‚çš„ç²¾ç»†åŒ–é¢„æµ‹ã€‚åŒæ—¶ï¼Œé€šè¿‡é¢„æµ‹æ‰‹-ç‰©äº¤äº’çŠ¶æ€ï¼Œå¢å¼ºæ¨¡å‹å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUni-Handçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬å¤šæ¨¡æ€èåˆæ¨¡å—ã€åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°æ¨¡å—ã€‚å¤šæ¨¡æ€èåˆæ¨¡å—è´Ÿè´£æ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œæå–å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹åŒæ—¶é¢„æµ‹å¤´éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨è½¨è¿¹ã€‚ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°æ¨¡å—åˆ™ç”¨äºéªŒè¯æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šUni-Handçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) æå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶é¢„æµ‹å¤´éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨ï¼Œæ•æ‰å®ƒä»¬ä¹‹é—´çš„è¿åŠ¨ååŒã€‚2) å¼•å…¥äº†ç›®æ ‡æŒ‡ç¤ºå™¨ï¼Œå®ç°äº†å¯¹ä¸åŒæ‰‹éƒ¨å…³èŠ‚çš„ç²¾ç»†åŒ–é¢„æµ‹ã€‚3) å°†æ‰‹-ç‰©äº¤äº’çŠ¶æ€çš„é¢„æµ‹çº³å…¥æ¡†æ¶ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ã€‚4) æ„å»ºäº†æ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ç®—æ³•çš„å®é™…åº”ç”¨æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šUni-Handé‡‡ç”¨äº†è§†è§‰-è¯­è¨€èåˆç­–ç•¥ï¼Œåˆ©ç”¨Transformerç½‘ç»œæå–è§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèåˆã€‚åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹é‡‡ç”¨äº†U-Netç»“æ„ï¼Œåˆ†åˆ«é¢„æµ‹å¤´éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨è½¨è¿¹ã€‚ç›®æ ‡æŒ‡ç¤ºå™¨é€šè¿‡one-hotç¼–ç è¡¨ç¤ºä¸åŒçš„æ‰‹éƒ¨å…³èŠ‚ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è½¨è¿¹é¢„æµ‹æŸå¤±ã€æ‰‹-ç‰©äº¤äº’çŠ¶æ€é¢„æµ‹æŸå¤±å’Œå¯¹æŠ—æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Uni-Handåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†å’Œæ–°æå‡ºçš„åŸºå‡†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒUni-Handçš„å¹³å‡é¢„æµ‹è¯¯å·®æ¯”ç°æœ‰æ–¹æ³•é™ä½äº†15%ã€‚åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒUni-Handèƒ½å¤ŸæˆåŠŸå®ŒæˆæŠ“å–ã€æ”¾ç½®ç­‰å¤æ‚æ“ä½œï¼ŒæˆåŠŸç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†20%ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-Handå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Uni-Handå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å¢å¼ºç°å®ã€äººæœºäº¤äº’ã€æœºå™¨äººæ“ä½œå’ŒåŠ¨ä½œé¢„æµ‹ç­‰ã€‚åœ¨å¢å¼ºç°å®ä¸­ï¼Œå¯ä»¥åˆ©ç”¨Uni-Handé¢„æµ‹ç”¨æˆ·çš„æ‰‹éƒ¨è¿åŠ¨ï¼Œä»è€Œå®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚åœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œå¯ä»¥å°†Uni-Handé¢„æµ‹çš„æ‰‹éƒ¨è¿åŠ¨è½¨è¿¹ä½œä¸ºæœºå™¨äººçš„æ§åˆ¶æŒ‡ä»¤ï¼Œå®ç°æ›´ç²¾ç¡®çš„æœºå™¨äººæ“ä½œã€‚æ­¤å¤–ï¼ŒUni-Handè¿˜å¯ä»¥ç”¨äºåŠ¨ä½œé¢„æµ‹ï¼Œä¾‹å¦‚é¢„æµ‹ç”¨æˆ·ä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„åŠ¨ä½œï¼Œä»è€Œæä¾›æ›´æ™ºèƒ½çš„æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Forecasting how human hands move in egocentric views is critical for applications like augmented reality and human-robot policy transfer. Recently, several hand trajectory prediction (HTP) methods have been developed to generate future possible hand waypoints, which still suffer from insufficient prediction targets, inherent modality gaps, entangled hand-head motion, and limited validation in downstream tasks. To address these limitations, we present a universal hand motion forecasting framework considering multi-modal input, multi-dimensional and multi-target prediction patterns, and multi-task affordances for downstream applications. We harmonize multiple modalities by vision-language fusion, global context incorporation, and task-aware text embedding injection, to forecast hand waypoints in both 2D and 3D spaces. A novel dual-branch diffusion is proposed to concurrently predict human head and hand movements, capturing their motion synergy in egocentric vision. By introducing target indicators, the prediction model can forecast the specific joint waypoints of the wrist or the fingers, besides the widely studied hand center points. In addition, we enable Uni-Hand to additionally predict hand-object interaction states (contact/separation) to facilitate downstream tasks better. As the first work to incorporate downstream task evaluation in the literature, we build novel benchmarks to assess the real-world applicability of hand motion forecasting algorithms. The experimental results on multiple publicly available datasets and our newly proposed benchmarks demonstrate that Uni-Hand achieves the state-of-the-art performance in multi-dimensional and multi-target hand motion forecasting. Extensive validation in multiple downstream tasks also presents its impressive human-robot policy transfer to enable robotic manipulation, and effective feature enhancement for action anticipation/recognition.

