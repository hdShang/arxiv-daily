---
layout: default
title: Distribution Matching Distillation Meets Reinforcement Learning
---

# Distribution Matching Distillation Meets Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.13649" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.13649v3</a>
  <a href="https://arxiv.org/pdf/2511.13649.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.13649v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.13649v3', 'Distribution Matching Distillation Meets Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dengyang Jiang, Dongyang Liu, Zanyi Wang, Qilong Wu, Liuzhuozheng Li, Hengzhuang Li, Xin Jin, David Liu, Zhen Li, Bo Zhang, Mengmeng Wang, Steven Hoi, Peng Gao, Harry Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-17 (æ›´æ–°: 2025-12-08)

**å¤‡æ³¨**: The synergy of reinforcement learning and distribution matching distillation. See more: https://github.com/vvvvvjdy/dmdr

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDMDRæ¡†æ¶ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼Œæå‡å°‘æ­¥æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `åˆ†å¸ƒåŒ¹é…è’¸é¦` `å¼ºåŒ–å­¦ä¹ ` `å›¾åƒç”Ÿæˆ` `å°‘æ­¥æ¨ç†` `æ¨¡å‹å‹ç¼©` `ç­–ç•¥ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿåˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰å—é™äºæ•™å¸ˆæ¨¡å‹æ€§èƒ½ï¼Œæ— æ³•å……åˆ†å‘æŒ¥å°‘æ­¥ç”Ÿæˆå™¨çš„æ½œåŠ›ã€‚
2. DMDRæ¡†æ¶ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸DMDï¼Œåˆ©ç”¨DMDæŸå¤±ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œå¹¶ç”¨RLæŒ‡å¯¼æ¨¡å¼è¦†ç›–ï¼Œè§£é”å°‘æ­¥ç”Ÿæˆå™¨èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDMDRåœ¨è§†è§‰è´¨é‡å’Œæç¤ºä¸€è‡´æ€§æ–¹é¢è¡¨ç°é¢†å…ˆï¼Œç”šè‡³è¶…è¶Šäº†å¤šæ­¥æ•™å¸ˆæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDMDRçš„æ–°æ¡†æ¶ï¼Œå®ƒå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯èå…¥åˆ°åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰è¿‡ç¨‹ä¸­ã€‚DMDæ—¨åœ¨å°†é¢„è®­ç»ƒçš„å¤šæ­¥æ‰©æ•£æ¨¡å‹æç‚¼ä¸ºå°‘æ­¥æ¨¡å‹ï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡ï¼Œä½†å…¶æ€§èƒ½é€šå¸¸å—é™äºæ•™å¸ˆæ¨¡å‹ã€‚DMDRé€šè¿‡å°†DMDæŸå¤±æœ¬èº«ä½œä¸ºå°‘æ­¥ç”Ÿæˆå™¨RLçš„æœ‰æ•ˆæ­£åˆ™åŒ–é¡¹ï¼Œå…‹æœäº†è¿™ä¸€é™åˆ¶ã€‚åè¿‡æ¥ï¼ŒRLå¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼DMDä¸­çš„æ¨¡å¼è¦†ç›–è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡åŒæ—¶è¿›è¡Œè’¸é¦å’ŒRLæ¥é‡Šæ”¾å°‘æ­¥ç”Ÿæˆå™¨çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŠ¨æ€åˆ†å¸ƒæŒ‡å¯¼å’ŒåŠ¨æ€é‡å™ªå£°é‡‡æ ·è®­ç»ƒç­–ç•¥ï¼Œä»¥æ”¹å–„åˆå§‹è’¸é¦è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDMDRå¯ä»¥åœ¨å°‘æ­¥æ–¹æ³•ä¸­å®ç°é¢†å…ˆçš„è§†è§‰è´¨é‡å’Œæç¤ºä¸€è‡´æ€§ï¼Œç”šè‡³è¡¨ç°å‡ºè¶…è¿‡å¤šæ­¥æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰æ–¹æ³•ï¼Œè™½ç„¶èƒ½å¤Ÿå°†å¤šæ­¥æ‰©æ•£æ¨¡å‹å‹ç¼©ä¸ºå°‘æ­¥æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦ï¼Œä½†å°‘æ­¥æ¨¡å‹çš„æ€§èƒ½å¾€å¾€å—åˆ°é¢„è®­ç»ƒçš„å¤šæ­¥æ•™å¸ˆæ¨¡å‹çš„é™åˆ¶ï¼Œéš¾ä»¥çªç ´æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚ç°æœ‰çš„DMDæ–¹æ³•åœ¨æ¨¡å¼è¦†ç›–æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDMDRçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¼•å…¥åˆ°DMDè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨RLæ¥ä¼˜åŒ–å°‘æ­¥ç”Ÿæˆå™¨çš„ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´å¤šæ ·æ€§çš„å›¾åƒã€‚åŒæ—¶ï¼Œå°†DMDæŸå¤±æœ¬èº«ä½œä¸ºRLçš„æ­£åˆ™åŒ–é¡¹ï¼Œé¿å…äº†ä¼ ç»Ÿæ­£åˆ™åŒ–æ–¹æ³•å¯èƒ½å¼•å…¥çš„åå·®ã€‚é€šè¿‡DMDå’ŒRLçš„ååŒä½œç”¨ï¼ŒDMDRèƒ½å¤Ÿçªç ´æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½é™åˆ¶ï¼Œå……åˆ†å‘æŒ¥å°‘æ­¥ç”Ÿæˆå™¨çš„æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDMDRæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ¨¡å—ï¼š1) **DMDè’¸é¦æ¨¡å—**ï¼šä½¿ç”¨åˆ†å¸ƒåŒ¹é…æŸå¤±å°†å¤šæ­¥æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°‘æ­¥å­¦ç”Ÿæ¨¡å‹ã€‚2) **RLä¼˜åŒ–æ¨¡å—**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚ï¼ŒPPOï¼‰ä¼˜åŒ–å°‘æ­¥ç”Ÿæˆå™¨çš„ç­–ç•¥ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚3) **åŠ¨æ€åˆ†å¸ƒæŒ‡å¯¼æ¨¡å—**ï¼šåœ¨åˆå§‹è’¸é¦é˜¶æ®µï¼ŒåŠ¨æ€è°ƒæ•´åˆ†å¸ƒæŒ‡å¯¼çš„å¼ºåº¦ï¼Œä»¥æ”¹å–„è’¸é¦æ•ˆæœã€‚4) **åŠ¨æ€é‡å™ªå£°é‡‡æ ·æ¨¡å—**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŠ¨æ€è°ƒæ•´å™ªå£°çš„é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹æ˜¯DMDè’¸é¦å’ŒRLä¼˜åŒ–äº¤æ›¿è¿›è¡Œï¼Œç›¸äº’ä¿ƒè¿›ã€‚

**å…³é”®åˆ›æ–°**ï¼šDMDRçš„å…³é”®åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä¸åˆ†å¸ƒåŒ¹é…è’¸é¦ç›¸ç»“åˆï¼Œå¹¶åˆ©ç”¨DMDæŸå¤±ä½œä¸ºRLçš„æ­£åˆ™åŒ–é¡¹ã€‚è¿™ç§ç»“åˆæ–¹å¼èƒ½å¤Ÿæœ‰æ•ˆåœ°å…‹æœä¼ ç»ŸDMDæ–¹æ³•çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶å……åˆ†å‘æŒ¥å°‘æ­¥ç”Ÿæˆå™¨çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€åˆ†å¸ƒæŒ‡å¯¼å’ŒåŠ¨æ€é‡å™ªå£°é‡‡æ ·ç­–ç•¥ä¹Ÿè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RLä¼˜åŒ–æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†Proximal Policy Optimization (PPO) ç®—æ³•ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œé€šå¸¸ç»“åˆäº†å›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼ŒFIDã€Inception Scoreï¼‰å’Œæç¤ºä¸€è‡´æ€§æŒ‡æ ‡ã€‚DMDæŸå¤±è¢«ç”¨ä½œRLçš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥çº¦æŸç­–ç•¥çš„æ›´æ–°æ–¹å‘ï¼Œé¿å…ç”Ÿæˆå™¨åç¦»æ•™å¸ˆæ¨¡å‹çš„åˆ†å¸ƒå¤ªè¿œã€‚åŠ¨æ€åˆ†å¸ƒæŒ‡å¯¼æ¨¡å—é€šè¿‡è°ƒæ•´åˆ†å¸ƒåŒ¹é…æŸå¤±çš„æƒé‡æ¥å®ç°ï¼ŒåŠ¨æ€é‡å™ªå£°é‡‡æ ·æ¨¡å—åˆ™é€šè¿‡è°ƒæ•´å™ªå£°çš„é‡‡æ ·èŒƒå›´æ¥å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDMDRåœ¨è§†è§‰è´¨é‡å’Œæç¤ºä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„å°‘æ­¥æ‰©æ•£æ¨¡å‹ã€‚åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒDMDRç”Ÿæˆçš„å›¾åƒçš„FIDå¾—åˆ†æ˜¾è‘—ä½äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œè¡¨æ˜å…¶ç”Ÿæˆå›¾åƒçš„è´¨é‡æ›´é«˜ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒDMDRç”šè‡³èƒ½å¤Ÿè¶…è¶Šå¤šæ­¥æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶èƒ½å¤Ÿæœ‰æ•ˆçªç ´æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šï¼ŒDMDRçš„FIDå¾—åˆ†æ¯”æ•™å¸ˆæ¨¡å‹é™ä½äº†5%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DMDRæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€è§†é¢‘ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆé«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡çš„å›¾åƒå’Œè§†é¢‘ï¼Œå¹¶å¯ä»¥åº”ç”¨äºæ¸¸æˆå¼€å‘ã€ç”µå½±åˆ¶ä½œã€å¹¿å‘Šè®¾è®¡ç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼ŒDMDRè¿˜å¯ä»¥ç”¨äºæ•°æ®å¢å¼ºï¼Œé€šè¿‡ç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç”±äºå…¶å°‘æ­¥æ¨ç†çš„ç‰¹æ€§ï¼ŒDMDRåœ¨å¯¹å®æ—¶æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ä¸‹å…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

