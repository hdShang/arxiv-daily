---
layout: default
title: Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training
---

# Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06254" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.06254v1</a>
  <a href="https://arxiv.org/pdf/2510.06254.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06254v1" onclick="toggleFavorite(this, '2510.06254v1', 'Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaochen Zhao, Chengting Yu, Kairong Yu, Lei Liu, Aili Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-04

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¢å¼ºå‹è‡ªè’¸é¦æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆè„‰å†²ç¥ç»ç½‘ç»œè®­ç»ƒ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è„‰å†²ç¥ç»ç½‘ç»œ` `è‡ªè’¸é¦` `ç¥ç»å½¢æ€è®¡ç®—` `é«˜æ•ˆè®­ç»ƒ` `æ›¿ä»£æ¢¯åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸSNNè®­ç»ƒæ–¹æ³•ä¾èµ–æ›¿ä»£æ¢¯åº¦å’ŒBPTTï¼Œå­˜åœ¨æ€§èƒ½è½åäºANNï¼Œè®¡ç®—å’Œå†…å­˜å¼€é”€å¤§çš„é—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºå¢å¼ºå‹è‡ªè’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«äº§ç”Ÿçš„é«˜è´¨é‡çŸ¥è¯†ï¼Œé€šè¿‡ANNåˆ†æ”¯ä¼˜åŒ–SNNå­ç»“æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½è®­ç»ƒå¤æ‚åº¦çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ€§èƒ½çš„SNNè®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è„‰å†²ç¥ç»ç½‘ç»œ(SNNs)ç”±äºå…¶ç¨€ç–æ¿€æ´»æ¨¡å¼ï¼Œåœ¨ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½æ•ˆã€‚ç„¶è€Œï¼ŒåŸºäºæ›¿ä»£æ¢¯åº¦å’Œæ—¶é—´åå‘ä¼ æ’­(BPTT)çš„ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•ä¸ä»…åœ¨æ€§èƒ½ä¸Šè½åäºäººå·¥ç¥ç»ç½‘ç»œ(ANNs)ï¼Œè€Œä¸”è¿˜äº§ç”Ÿæ˜¾è‘—çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œè¿™äº›å¼€é”€éšæ—¶é—´ç»´åº¦çº¿æ€§å¢é•¿ã€‚ä¸ºäº†åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°é«˜æ€§èƒ½SNNè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºå‹è‡ªè’¸é¦æ¡†æ¶ï¼Œå¹¶ä¸åŸºäºé€Ÿç‡çš„åå‘ä¼ æ’­è”åˆä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œä¸­é—´SNNå±‚çš„å‘æ”¾ç‡è¢«æŠ•å½±åˆ°è½»é‡çº§ANNåˆ†æ”¯ä¸Šï¼Œå¹¶ä¸”ç”±æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„é«˜è´¨é‡çŸ¥è¯†è¢«ç”¨äºé€šè¿‡ANNè·¯å¾„ä¼˜åŒ–å­ç»“æ„ã€‚ä¸ä¼ ç»Ÿçš„è‡ªè’¸é¦èŒƒå¼ä¸åŒï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä½è´¨é‡çš„è‡ªç”ŸæˆçŸ¥è¯†å¯èƒ½ä¼šé˜»ç¢æ”¶æ•›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†æ•™å¸ˆä¿¡å·è§£è€¦ä¸ºå¯é å’Œä¸å¯é çš„ç»„æˆéƒ¨åˆ†ï¼Œç¡®ä¿åªæœ‰å¯é çš„çŸ¥è¯†è¢«ç”¨æ¥æŒ‡å¯¼æ¨¡å‹çš„ä¼˜åŒ–ã€‚åœ¨CIFAR-10ã€CIFAR-100ã€CIFAR10-DVSå’ŒImageNetä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é™ä½äº†è®­ç»ƒå¤æ‚åº¦ï¼ŒåŒæ—¶å®ç°äº†é«˜æ€§èƒ½çš„SNNè®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„SNNè®­ç»ƒæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ›¿ä»£æ¢¯åº¦å’Œæ—¶é—´åå‘ä¼ æ’­(BPTT)çš„æ–¹æ³•ï¼Œåœ¨æ€§èƒ½ä¸Šä¸å¦‚äººå·¥ç¥ç»ç½‘ç»œ(ANN)ï¼Œå¹¶ä¸”è®¡ç®—å’Œå†…å­˜å¼€é”€éšç€æ—¶é—´æ­¥é•¿çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ã€‚è¿™ä½¿å¾—åœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šè®­ç»ƒé«˜æ€§èƒ½SNNå˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆçš„SNNè®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡SNNçš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªè’¸é¦æŠ€æœ¯ï¼Œå°†SNNä¸­é—´å±‚çš„æ¿€æ´»ä¿¡æ¯ï¼ˆå‘æ”¾ç‡ï¼‰ä¼ é€’ç»™è½»é‡çº§çš„ANNåˆ†æ”¯ï¼Œå¹¶åˆ©ç”¨ANNåˆ†æ”¯æä¾›çš„æ¢¯åº¦ä¿¡æ¯æ¥ä¼˜åŒ–SNNã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSNNå¯ä»¥ä»è‡ªèº«å­¦ä¹ ï¼Œå¹¶åˆ©ç”¨ANNçš„ä¼˜åŠ¿æ¥åŠ é€Ÿè®­ç»ƒå’Œæé«˜æ€§èƒ½ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜å…³æ³¨åˆ°è‡ªè’¸é¦è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ä½è´¨é‡çŸ¥è¯†é—®é¢˜ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªSNNå’Œä¸€ä¸ªæˆ–å¤šä¸ªè½»é‡çº§çš„ANNåˆ†æ”¯ã€‚SNNæ¥æ”¶è¾“å…¥æ•°æ®ï¼Œå¹¶äº§ç”Ÿä¸­é—´å±‚çš„å‘æ”¾ç‡ã€‚è¿™äº›å‘æ”¾ç‡è¢«æŠ•å½±åˆ°ANNåˆ†æ”¯ä¸Šï¼ŒANNåˆ†æ”¯è¿›è¡Œå‰å‘ä¼ æ’­å¹¶è®¡ç®—æŸå¤±ã€‚ç„¶åï¼ŒANNåˆ†æ”¯çš„æ¢¯åº¦è¢«åå‘ä¼ æ’­åˆ°SNNï¼Œç”¨äºæ›´æ–°SNNçš„å‚æ•°ã€‚ä¸ºäº†è§£å†³ä½è´¨é‡çŸ¥è¯†çš„é—®é¢˜ï¼Œè®ºæ–‡å°†æ•™å¸ˆä¿¡å·ï¼ˆå³ANNåˆ†æ”¯çš„è¾“å‡ºï¼‰åˆ†è§£ä¸ºå¯é å’Œä¸å¯é çš„ç»„æˆéƒ¨åˆ†ï¼Œåªä½¿ç”¨å¯é çš„çŸ¥è¯†æ¥æŒ‡å¯¼SNNçš„ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¢å¼ºå‹è‡ªè’¸é¦æ¡†æ¶ï¼Œå®ƒä¸ä»…åˆ©ç”¨äº†è‡ªè’¸é¦çš„ä¼˜åŠ¿ï¼Œè¿˜è§£å†³äº†è‡ªè’¸é¦è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ä½è´¨é‡çŸ¥è¯†é—®é¢˜ã€‚é€šè¿‡å°†æ•™å¸ˆä¿¡å·è§£è€¦ä¸ºå¯é å’Œä¸å¯é çš„ç»„æˆéƒ¨åˆ†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è‡ªç”ŸæˆçŸ¥è¯†æ¥æŒ‡å¯¼SNNçš„ä¼˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„è‡ªè’¸é¦æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ é²æ£’ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„æ•°æ®é›†å’Œç½‘ç»œç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•å°†SNNä¸­é—´å±‚çš„å‘æ”¾ç‡æŠ•å½±åˆ°ANNåˆ†æ”¯ä¸Šï¼›2) å¦‚ä½•è®¾è®¡ANNåˆ†æ”¯çš„ç»“æ„ï¼Œä½¿å…¶æ—¢èƒ½æä¾›æœ‰æ•ˆçš„æ¢¯åº¦ä¿¡æ¯ï¼Œåˆä¸ä¼šå¢åŠ è¿‡å¤šçš„è®¡ç®—è´Ÿæ‹…ï¼›3) å¦‚ä½•å°†æ•™å¸ˆä¿¡å·è§£è€¦ä¸ºå¯é å’Œä¸å¯é çš„ç»„æˆéƒ¨åˆ†ï¼Œå¹¶ç¡®å®šå“ªäº›çŸ¥è¯†æ˜¯å¯é çš„ï¼›4) å¦‚ä½•è®¾è®¡æŸå¤±å‡½æ•°ï¼Œä»¥å¹³è¡¡SNNè‡ªèº«çš„æŸå¤±å’Œæ¥è‡ªANNåˆ†æ”¯çš„è’¸é¦æŸå¤±ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å¯èƒ½å› ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡è€Œå¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CIFAR-10ã€CIFAR-100ã€CIFAR10-DVSå’ŒImageNetç­‰æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒå¤æ‚åº¦ã€‚å…·ä½“æ€§èƒ½æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚è¯¥æ–¹æ³•ä¸ç°æœ‰çš„SNNè®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ç²¾åº¦å’Œæ•ˆç‡ä¸Šéƒ½å…·æœ‰ä¼˜åŠ¿ï¼Œä¸ºSNNçš„å®é™…åº”ç”¨æä¾›äº†æœ‰åŠ›çš„æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºä½åŠŸè€—ã€é«˜æ•ˆç‡çš„ç¥ç»å½¢æ€è®¡ç®—é¢†åŸŸï¼Œä¾‹å¦‚è¾¹ç¼˜è®¡ç®—è®¾å¤‡ã€æœºå™¨äººã€ç‰©è”ç½‘è®¾å¤‡ç­‰ã€‚é€šè¿‡é™ä½SNNçš„è®­ç»ƒå¤æ‚åº¦å’Œæé«˜å…¶æ€§èƒ½ï¼Œå¯ä»¥ä½¿å¾—SNNåœ¨è¿™äº›èµ„æºå—é™çš„åœºæ™¯ä¸­å¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ä¿ƒè¿›SNNåœ¨è§†è§‰ã€è¯­éŸ³ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶ä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´èŠ‚èƒ½çš„AIç³»ç»Ÿæä¾›æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on neuromorphic hardware due to their sparse activation patterns. However, conventional training methods based on surrogate gradients and Backpropagation Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in performance, but also incur significant computational and memory overheads that grow linearly with the temporal dimension. To enable high-performance SNN training under limited computational resources, we propose an enhanced self-distillation framework, jointly optimized with rate-based backpropagation. Specifically, the firing rates of intermediate SNN layers are projected onto lightweight ANN branches, and high-quality knowledge generated by the model itself is used to optimize substructures through the ANN pathways. Unlike traditional self-distillation paradigms, we observe that low-quality self-generated knowledge may hinder convergence. To address this, we decouple the teacher signal into reliable and unreliable components, ensuring that only reliable knowledge is used to guide the optimization of the model. Extensive experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that our method reduces training complexity while achieving high-performance SNN training. Our code is available at https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.

