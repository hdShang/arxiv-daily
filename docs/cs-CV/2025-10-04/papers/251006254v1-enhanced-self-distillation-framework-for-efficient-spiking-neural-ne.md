---
layout: default
title: Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training
---

# Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.06254" target="_blank" class="toolbar-btn">arXiv: 2510.06254v1</a>
    <a href="https://arxiv.org/pdf/2510.06254.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06254v1" 
            onclick="toggleFavorite(this, '2510.06254v1', 'Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiaochen Zhao, Chengting Yu, Kairong Yu, Lei Liu, Aili Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-04

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â¢ûÂº∫ÂûãËá™Ëí∏È¶èÊ°ÜÊû∂ÔºåÁî®‰∫éÈ´òÊïàËÑâÂÜ≤Á•ûÁªèÁΩëÁªúËÆ≠ÁªÉ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ËÑâÂÜ≤Á•ûÁªèÁΩëÁªú` `Ëá™Ëí∏È¶è` `Á•ûÁªèÂΩ¢ÊÄÅËÆ°ÁÆó` `È´òÊïàËÆ≠ÁªÉ` `Êõø‰ª£Ê¢ØÂ∫¶`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüSNNËÆ≠ÁªÉÊñπÊ≥ï‰æùËµñÊõø‰ª£Ê¢ØÂ∫¶ÂíåBPTTÔºåÂ≠òÂú®ÊÄßËÉΩËêΩÂêé‰∫éANNÔºåËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄÂ§ßÁöÑÈóÆÈ¢ò„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Â¢ûÂº∫ÂûãËá™Ëí∏È¶èÊ°ÜÊû∂ÔºåÂà©Áî®Ê®°ÂûãËá™Ë∫´‰∫ßÁîüÁöÑÈ´òË¥®ÈáèÁü•ËØÜÔºåÈÄöËøáANNÂàÜÊîØ‰ºòÂåñSNNÂ≠êÁªìÊûÑ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Èôç‰ΩéËÆ≠ÁªÉÂ§çÊùÇÂ∫¶ÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÈ´òÊÄßËÉΩÁöÑSNNËÆ≠ÁªÉÔºåÂπ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÈ™åËØÅÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËÑâÂÜ≤Á•ûÁªèÁΩëÁªú(SNNs)Áî±‰∫éÂÖ∂Á®ÄÁñèÊøÄÊ¥ªÊ®°ÂºèÔºåÂú®Á•ûÁªèÂΩ¢ÊÄÅÁ°¨‰ª∂‰∏äË°®Áé∞Âá∫ÂçìË∂äÁöÑËÉΩÊïà„ÄÇÁÑ∂ËÄåÔºåÂü∫‰∫éÊõø‰ª£Ê¢ØÂ∫¶ÂíåÊó∂Èó¥ÂèçÂêë‰º†Êí≠(BPTT)ÁöÑ‰º†ÁªüËÆ≠ÁªÉÊñπÊ≥ï‰∏ç‰ªÖÂú®ÊÄßËÉΩ‰∏äËêΩÂêé‰∫é‰∫∫Â∑•Á•ûÁªèÁΩëÁªú(ANNs)ÔºåËÄå‰∏îËøò‰∫ßÁîüÊòæËëóÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄÔºåËøô‰∫õÂºÄÈîÄÈöèÊó∂Èó¥Áª¥Â∫¶Á∫øÊÄßÂ¢ûÈïø„ÄÇ‰∏∫‰∫ÜÂú®ÊúâÈôêÁöÑËÆ°ÁÆóËµÑÊ∫ê‰∏ãÂÆûÁé∞È´òÊÄßËÉΩSNNËÆ≠ÁªÉÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ¢ûÂº∫ÂûãËá™Ëí∏È¶èÊ°ÜÊû∂ÔºåÂπ∂‰∏éÂü∫‰∫éÈÄüÁéáÁöÑÂèçÂêë‰º†Êí≠ËÅîÂêà‰ºòÂåñ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰∏≠Èó¥SNNÂ±ÇÁöÑÂèëÊîæÁéáË¢´ÊäïÂΩ±Âà∞ËΩªÈáèÁ∫ßANNÂàÜÊîØ‰∏äÔºåÂπ∂‰∏îÁî±Ê®°ÂûãËá™Ë∫´ÁîüÊàêÁöÑÈ´òË¥®ÈáèÁü•ËØÜË¢´Áî®‰∫éÈÄöËøáANNË∑ØÂæÑ‰ºòÂåñÂ≠êÁªìÊûÑ„ÄÇ‰∏é‰º†ÁªüÁöÑËá™Ëí∏È¶èËåÉÂºè‰∏çÂêåÔºåÊàë‰ª¨ËßÇÂØüÂà∞‰ΩéË¥®ÈáèÁöÑËá™ÁîüÊàêÁü•ËØÜÂèØËÉΩ‰ºöÈòªÁ¢çÊî∂Êïõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Â∞ÜÊïôÂ∏à‰ø°Âè∑Ëß£ËÄ¶‰∏∫ÂèØÈù†Âíå‰∏çÂèØÈù†ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºåÁ°Æ‰øùÂè™ÊúâÂèØÈù†ÁöÑÁü•ËØÜË¢´Áî®Êù•ÊåáÂØºÊ®°ÂûãÁöÑ‰ºòÂåñ„ÄÇÂú®CIFAR-10„ÄÅCIFAR-100„ÄÅCIFAR10-DVSÂíåImageNet‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÂ§çÊùÇÂ∫¶ÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜÈ´òÊÄßËÉΩÁöÑSNNËÆ≠ÁªÉ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑSNNËÆ≠ÁªÉÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éÊõø‰ª£Ê¢ØÂ∫¶ÂíåÊó∂Èó¥ÂèçÂêë‰º†Êí≠(BPTT)ÁöÑÊñπÊ≥ïÔºåÂú®ÊÄßËÉΩ‰∏ä‰∏çÂ¶Ç‰∫∫Â∑•Á•ûÁªèÁΩëÁªú(ANN)ÔºåÂπ∂‰∏îËÆ°ÁÆóÂíåÂÜÖÂ≠òÂºÄÈîÄÈöèÁùÄÊó∂Èó¥Ê≠•ÈïøÁöÑÂ¢ûÂä†ËÄåÁ∫øÊÄßÂ¢ûÈïø„ÄÇËøô‰ΩøÂæóÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁ°¨‰ª∂‰∏äËÆ≠ÁªÉÈ´òÊÄßËÉΩSNNÂèòÂæóÂõ∞Èöæ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÊõ¥È´òÊïàÁöÑSNNËÆ≠ÁªÉÊñπÊ≥ïÔºåËÉΩÂ§üÂú®Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÁöÑÂêåÊó∂Ôºå‰øùÊåÅÁîöËá≥ÊèêÂçáSNNÁöÑÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ëá™Ëí∏È¶èÊäÄÊúØÔºåÂ∞ÜSNN‰∏≠Èó¥Â±ÇÁöÑÊøÄÊ¥ª‰ø°ÊÅØÔºàÂèëÊîæÁéáÔºâ‰º†ÈÄíÁªôËΩªÈáèÁ∫ßÁöÑANNÂàÜÊîØÔºåÂπ∂Âà©Áî®ANNÂàÜÊîØÊèê‰æõÁöÑÊ¢ØÂ∫¶‰ø°ÊÅØÊù•‰ºòÂåñSNN„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåSNNÂèØ‰ª•‰ªéËá™Ë∫´Â≠¶‰π†ÔºåÂπ∂Âà©Áî®ANNÁöÑ‰ºòÂäøÊù•Âä†ÈÄüËÆ≠ÁªÉÂíåÊèêÈ´òÊÄßËÉΩ„ÄÇÂêåÊó∂ÔºåËÆ∫ÊñáËøòÂÖ≥Ê≥®Âà∞Ëá™Ëí∏È¶èËøáÁ®ã‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑ‰ΩéË¥®ÈáèÁü•ËØÜÈóÆÈ¢òÔºåÂπ∂ÊèêÂá∫‰∫ÜÁõ∏Â∫îÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏Ä‰∏™SNNÂíå‰∏Ä‰∏™ÊàñÂ§ö‰∏™ËΩªÈáèÁ∫ßÁöÑANNÂàÜÊîØ„ÄÇSNNÊé•Êî∂ËæìÂÖ•Êï∞ÊçÆÔºåÂπ∂‰∫ßÁîü‰∏≠Èó¥Â±ÇÁöÑÂèëÊîæÁéá„ÄÇËøô‰∫õÂèëÊîæÁéáË¢´ÊäïÂΩ±Âà∞ANNÂàÜÊîØ‰∏äÔºåANNÂàÜÊîØËøõË°åÂâçÂêë‰º†Êí≠Âπ∂ËÆ°ÁÆóÊçüÂ§±„ÄÇÁÑ∂ÂêéÔºåANNÂàÜÊîØÁöÑÊ¢ØÂ∫¶Ë¢´ÂèçÂêë‰º†Êí≠Âà∞SNNÔºåÁî®‰∫éÊõ¥Êñ∞SNNÁöÑÂèÇÊï∞„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥‰ΩéË¥®ÈáèÁü•ËØÜÁöÑÈóÆÈ¢òÔºåËÆ∫ÊñáÂ∞ÜÊïôÂ∏à‰ø°Âè∑ÔºàÂç≥ANNÂàÜÊîØÁöÑËæìÂá∫ÔºâÂàÜËß£‰∏∫ÂèØÈù†Âíå‰∏çÂèØÈù†ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºåÂè™‰ΩøÁî®ÂèØÈù†ÁöÑÁü•ËØÜÊù•ÊåáÂØºSNNÁöÑ‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ¢ûÂº∫ÂûãËá™Ëí∏È¶èÊ°ÜÊû∂ÔºåÂÆÉ‰∏ç‰ªÖÂà©Áî®‰∫ÜËá™Ëí∏È¶èÁöÑ‰ºòÂäøÔºåËøòËß£ÂÜ≥‰∫ÜËá™Ëí∏È¶èËøáÁ®ã‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑ‰ΩéË¥®ÈáèÁü•ËØÜÈóÆÈ¢ò„ÄÇÈÄöËøáÂ∞ÜÊïôÂ∏à‰ø°Âè∑Ëß£ËÄ¶‰∏∫ÂèØÈù†Âíå‰∏çÂèØÈù†ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Ëá™ÁîüÊàêÁü•ËØÜÊù•ÊåáÂØºSNNÁöÑ‰ºòÂåñ„ÄÇ‰∏é‰º†ÁªüÁöÑËá™Ëí∏È¶èÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊõ¥Âä†È≤ÅÊ£íÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíåÁΩëÁªúÁªìÊûÑ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Â¶Ç‰ΩïÂ∞ÜSNN‰∏≠Èó¥Â±ÇÁöÑÂèëÊîæÁéáÊäïÂΩ±Âà∞ANNÂàÜÊîØ‰∏äÔºõ2) Â¶Ç‰ΩïËÆæËÆ°ANNÂàÜÊîØÁöÑÁªìÊûÑÔºå‰ΩøÂÖ∂Êó¢ËÉΩÊèê‰æõÊúâÊïàÁöÑÊ¢ØÂ∫¶‰ø°ÊÅØÔºåÂèà‰∏ç‰ºöÂ¢ûÂä†ËøáÂ§öÁöÑËÆ°ÁÆóË¥üÊãÖÔºõ3) Â¶Ç‰ΩïÂ∞ÜÊïôÂ∏à‰ø°Âè∑Ëß£ËÄ¶‰∏∫ÂèØÈù†Âíå‰∏çÂèØÈù†ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºåÂπ∂Á°ÆÂÆöÂì™‰∫õÁü•ËØÜÊòØÂèØÈù†ÁöÑÔºõ4) Â¶Ç‰ΩïËÆæËÆ°ÊçüÂ§±ÂáΩÊï∞Ôºå‰ª•Âπ≥Ë°°SNNËá™Ë∫´ÁöÑÊçüÂ§±ÂíåÊù•Ëá™ANNÂàÜÊîØÁöÑËí∏È¶èÊçüÂ§±„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÂèØËÉΩÂõ†‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíå‰ªªÂä°ËÄåÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®CIFAR-10„ÄÅCIFAR-100„ÄÅCIFAR10-DVSÂíåImageNetÁ≠âÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂêåÊó∂Èôç‰Ωé‰∫ÜËÆ≠ÁªÉÂ§çÊùÇÂ∫¶„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇËØ•ÊñπÊ≥ï‰∏éÁé∞ÊúâÁöÑSNNËÆ≠ÁªÉÊñπÊ≥ïÁõ∏ÊØîÔºåÂú®Á≤æÂ∫¶ÂíåÊïàÁéá‰∏äÈÉΩÂÖ∑Êúâ‰ºòÂäøÔºå‰∏∫SNNÁöÑÂÆûÈôÖÂ∫îÁî®Êèê‰æõ‰∫ÜÊúâÂäõÁöÑÊîØÊåÅ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫é‰ΩéÂäüËÄó„ÄÅÈ´òÊïàÁéáÁöÑÁ•ûÁªèÂΩ¢ÊÄÅËÆ°ÁÆóÈ¢ÜÂüüÔºå‰æãÂ¶ÇËæπÁºòËÆ°ÁÆóËÆæÂ§á„ÄÅÊú∫Âô®‰∫∫„ÄÅÁâ©ËÅîÁΩëËÆæÂ§áÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéSNNÁöÑËÆ≠ÁªÉÂ§çÊùÇÂ∫¶ÂíåÊèêÈ´òÂÖ∂ÊÄßËÉΩÔºåÂèØ‰ª•‰ΩøÂæóSNNÂú®Ëøô‰∫õËµÑÊ∫êÂèóÈôêÁöÑÂú∫ÊôØ‰∏≠ÂæóÂà∞Êõ¥ÂπøÊ≥õÁöÑÂ∫îÁî®„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•‰øÉËøõSNNÂú®ËßÜËßâ„ÄÅËØ≠Èü≥Á≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÂπ∂‰∏∫ÂºÄÂèëÊõ¥Êô∫ËÉΩ„ÄÅÊõ¥ËäÇËÉΩÁöÑAIÁ≥ªÁªüÊèê‰æõÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on neuromorphic hardware due to their sparse activation patterns. However, conventional training methods based on surrogate gradients and Backpropagation Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in performance, but also incur significant computational and memory overheads that grow linearly with the temporal dimension. To enable high-performance SNN training under limited computational resources, we propose an enhanced self-distillation framework, jointly optimized with rate-based backpropagation. Specifically, the firing rates of intermediate SNN layers are projected onto lightweight ANN branches, and high-quality knowledge generated by the model itself is used to optimize substructures through the ANN pathways. Unlike traditional self-distillation paradigms, we observe that low-quality self-generated knowledge may hinder convergence. To address this, we decouple the teacher signal into reliable and unreliable components, ensuring that only reliable knowledge is used to guide the optimization of the model. Extensive experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that our method reduces training complexity while achieving high-performance SNN training. Our code is available at https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.

