---
layout: default
title: Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops
---

# Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.03606" target="_blank" class="toolbar-btn">arXiv: 2510.03606v1</a>
    <a href="https://arxiv.org/pdf/2510.03606.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03606v1" 
            onclick="toggleFavorite(this, '2510.03606v1', 'Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Mattia Scardecchia

**åˆ†ç±»**: cs.CV, cs.LG, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-04

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DINOv2æ·±åº¦è§£è¯»ï¼šéç›‘ç£Transformeré¢„è®­ç»ƒï¼Œè‡ªè’¸é¦ä¸å‡å€¼æ•™å¸ˆæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `Transformer` `è§†è§‰ç‰¹å¾` `è‡ªè’¸é¦` `å‡å€¼æ•™å¸ˆ` `å¤šè£å‰ªè§†å›¾` `DINOv2`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æ•æ‰å›¾åƒé«˜çº§è¯­ä¹‰å’Œç²¾ç»†ç©ºé—´ç»“æ„æ–¹é¢ä»æœ‰æå‡ç©ºé—´ï¼ŒDINOv2æ—¨åœ¨è¿›ä¸€æ­¥æå‡è§†è§‰ç‰¹å¾å­¦ä¹ çš„æ€§èƒ½ã€‚
2. DINOv2çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¤šè£å‰ªè§†å›¾å¢å¼ºå’ŒåŸºäºå‡å€¼æ•™å¸ˆçš„è‡ªè’¸é¦ï¼Œä»è€Œå­¦ä¹ æ›´é²æ£’å’Œæ³›åŒ–çš„è§†è§‰ç‰¹å¾ã€‚
3. DINOv2åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¶…è¶Šäº†å…¶ä»–è‡ªç›‘ç£å’Œå¼±ç›‘ç£æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å­¦ä¹ åˆ°çš„è§†è§‰ç‰¹å¾çš„ä¼˜è¶Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æ·±å…¥ç ”ç©¶äº†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯DINOv2ï¼Œå®ƒåœ¨å­¦ä¹ é€šç”¨è§†è§‰ç‰¹å¾æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œèƒ½å¤Ÿæ•æ‰å›¾åƒçš„é«˜çº§è¯­ä¹‰å’Œç²¾ç»†ç©ºé—´ç»“æ„ã€‚DINOv2è¶…è¶Šäº†åƒOpenCLIPè¿™æ ·çš„å¼±ç›‘ç£æ–¹æ³•ï¼ˆWSLï¼‰ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç¡®ç«‹äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚æœ¬æ–‡è€ƒå¯Ÿäº†DINOv2æ–¹æ³•èƒŒåçš„æ ¸å¿ƒæ€æƒ³ï¼ŒåŒ…æ‹¬å¤šè£å‰ªè§†å›¾å¢å¼ºå’ŒåŸºäºå‡å€¼æ•™å¸ˆçš„è‡ªè’¸é¦ï¼Œå¹¶è¿½æº¯äº†è¿™äº›æ€æƒ³åœ¨å…ˆå‰å·¥ä½œä¸­çš„å‘å±•å†ç¨‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¯”è¾ƒäº†DINOå’ŒDINOv2ä¸å…¶ä»–SSLå’ŒWSLæ–¹æ³•åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†å®ƒä»¬ä½¿ç”¨Transformeréª¨å¹²ç½‘ç»œå­¦ä¹ åˆ°çš„ç‰¹å¾çš„ä¸€äº›æ˜¾è‘—æ¶Œç°å±æ€§ã€‚æœ€åï¼Œç®€è¦è®¨è®ºäº†DINOv2çš„å±€é™æ€§ã€å½±å“ä»¥åŠæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡ä¸»è¦å…³æ³¨å¦‚ä½•é€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå­¦ä¹ åˆ°é«˜è´¨é‡çš„é€šç”¨è§†è§‰ç‰¹å¾è¡¨ç¤ºã€‚ç°æœ‰æ–¹æ³•åœ¨æ•æ‰å›¾åƒçš„å…¨å±€è¯­ä¹‰ä¿¡æ¯å’Œå±€éƒ¨ç»†èŠ‚ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚DINOv2æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡è‡ªç›‘ç£å­¦ä¹ çš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDINOv2çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªè’¸é¦æ¡†æ¶ï¼Œé€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œç»“æ„ï¼Œè®©å­¦ç”Ÿç½‘ç»œå­¦ä¹ æ•™å¸ˆç½‘ç»œçš„è¾“å‡ºï¼Œä»è€Œæå‡å­¦ç”Ÿç½‘ç»œçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé‡‡ç”¨å¤šè£å‰ªè§†å›¾å¢å¼ºç­–ç•¥ï¼Œå¢åŠ è¾“å…¥æ•°æ®çš„å¤šæ ·æ€§ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚å‡å€¼æ•™å¸ˆæ¨¡å‹åˆ™é€šè¿‡å¯¹æ•™å¸ˆç½‘ç»œå‚æ•°è¿›è¡ŒæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œæä¾›æ›´ç¨³å®šçš„å­¦ä¹ ç›®æ ‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDINOv2çš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªå­¦ç”Ÿç½‘ç»œå’Œä¸€ä¸ªæ•™å¸ˆç½‘ç»œã€‚è¾“å…¥å›¾åƒç»è¿‡å¤šè£å‰ªè§†å›¾å¢å¼ºåï¼Œåˆ†åˆ«è¾“å…¥åˆ°å­¦ç”Ÿç½‘ç»œå’Œæ•™å¸ˆç½‘ç»œä¸­ã€‚å­¦ç”Ÿç½‘ç»œçš„è¾“å‡ºä¸æ•™å¸ˆç½‘ç»œçš„è¾“å‡ºè¿›è¡Œè‡ªè’¸é¦ï¼Œé€šè¿‡æœ€å°åŒ–ä¸¤è€…ä¹‹é—´çš„å·®å¼‚æ¥æ›´æ–°å­¦ç”Ÿç½‘ç»œçš„å‚æ•°ã€‚æ•™å¸ˆç½‘ç»œçš„å‚æ•°åˆ™é€šè¿‡å¯¹å­¦ç”Ÿç½‘ç»œå‚æ•°è¿›è¡ŒæŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥æ›´æ–°ã€‚

**å…³é”®åˆ›æ–°**ï¼šDINOv2çš„å…³é”®åˆ›æ–°åœ¨äºå°†å¤šè£å‰ªè§†å›¾å¢å¼ºã€è‡ªè’¸é¦å’Œå‡å€¼æ•™å¸ˆæ¨¡å‹æœ‰æ•ˆåœ°ç»“åˆèµ·æ¥ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´é²æ£’å’Œæ³›åŒ–çš„è§†è§‰ç‰¹å¾ã€‚ä¸ä¹‹å‰çš„DINOç›¸æ¯”ï¼ŒDINOv2åœ¨æ¨¡å‹ç»“æ„å’Œè®­ç»ƒç­–ç•¥ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šDINOv2ä½¿ç”¨äº†Transformerä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶é‡‡ç”¨äº†ViTï¼ˆVision Transformerï¼‰çš„ç»“æ„ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç”¨äºè¡¡é‡å­¦ç”Ÿç½‘ç»œå’Œæ•™å¸ˆç½‘ç»œè¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚å¤šè£å‰ªè§†å›¾å¢å¼ºç­–ç•¥ä¸­ï¼Œä½¿ç”¨äº†ä¸åŒå¤§å°å’Œæ¯”ä¾‹çš„è£å‰ªï¼Œä»¥å¢åŠ è¾“å…¥æ•°æ®çš„å¤šæ ·æ€§ã€‚å‡å€¼æ•™å¸ˆæ¨¡å‹çš„åŠ¨é‡ç³»æ•°æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DINOv2åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å’Œå¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒDINOv2çš„æ€§èƒ½è¶…è¿‡äº†OpenCLIPç­‰å¼±ç›‘ç£æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒDINOv2å­¦ä¹ åˆ°çš„ç‰¹å¾è¿˜è¡¨ç°å‡ºä¸€äº›æœ‰è¶£çš„æ¶Œç°å±æ€§ï¼Œä¾‹å¦‚èƒ½å¤Ÿè‡ªåŠ¨å‘ç°å›¾åƒä¸­çš„è¯­ä¹‰åˆ†å‰²ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DINOv2å­¦ä¹ åˆ°çš„é€šç”¨è§†è§‰ç‰¹å¾å¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ã€‚å…¶å¼ºå¤§çš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›å¯ä»¥æå‡è¿™äº›ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶é™ä½å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æ­¤å¤–ï¼ŒDINOv2è¿˜å¯ä»¥åº”ç”¨äºæœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œä¸ºè¿™äº›é¢†åŸŸæä¾›æ›´å¯é çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in self-supervised learning (SSL) have made it possible to learn general-purpose visual features that capture both the high-level semantics and the fine-grained spatial structure of images. Most notably, the recent DINOv2 has established a new state of the art by surpassing weakly supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we examine the core ideas behind its approach, multi-crop view augmentation and self-distillation with a mean teacher, and trace their development in previous work. We then compare the performance of DINO and DINOv2 with other SSL and WSL methods across various downstream tasks, and highlight some remarkable emergent properties of their learned features with transformer backbones. We conclude by briefly discussing DINOv2's limitations, its impact, and future research directions.

