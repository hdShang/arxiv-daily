---
layout: default
title: The Overlooked Value of Test-time Reference Sets in Visual Place Recognition
---

# The Overlooked Value of Test-time Reference Sets in Visual Place Recognition

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.03751" target="_blank" class="toolbar-btn">arXiv: 2510.03751v1</a>
    <a href="https://arxiv.org/pdf/2510.03751.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03751v1" 
            onclick="toggleFavorite(this, '2510.03751v1', 'The Overlooked Value of Test-time Reference Sets in Visual Place Recognition')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Mubariz Zaffar, Liangliang Nan, Sebastian Scherer, Julian F. P. Kooij

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-04

**Â§áÊ≥®**: Accepted at ICCV 2025 Workshop CrocoDL

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèÇËÄÉÈõÜÂæÆË∞ÉÊñπÊ≥ïÔºåÊèêÂçáËßÜËßâÂÆö‰ΩçÂú®Ë∑®ÂüüÂú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÂÆö‰Ωç` `ÂüüÈÄÇÂ∫î` `ÂèÇËÄÉÈõÜÂæÆË∞É` `Êú∫Âô®‰∫∫ÂØºËà™` `ÂõæÂÉèÊ£ÄÁ¥¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVPRÊñπÊ≥ïÂú®Ë∑®ÂüüÂú∫ÊôØ‰∏ãÊ≥õÂåñÊÄß‰∏çË∂≥ÔºåÊµãËØïÁéØÂ¢É‰∏éËÆ≠ÁªÉÊï∞ÊçÆÂ∑ÆÂºÇÂ§ßÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. Âà©Áî®ÊµãËØïÊó∂ÂèØÁî®ÁöÑÂèÇËÄÉÈõÜÔºàÂú∞ÂõæÔºâ‰ø°ÊÅØÔºåÈÄöËøáÂæÆË∞ÉVPRÊ®°ÂûãÊù•ÈÄÇÂ∫îÁõÆÊ†áÂüü„ÄÇ
3. ÂèÇËÄÉÈõÜÂæÆË∞ÉÔºàRSFÔºâÂú®Â§ö‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊï∞ÊçÆÈõÜ‰∏äÊèêÂçá‰∫ÜSOTAÊÄßËÉΩÔºå‰∏î‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâÂÆö‰ΩçÔºàVPRÔºâÊó®Âú®ÁªôÂÆöÊü•ËØ¢ÂõæÂÉèÔºå‰ªéÂèÇËÄÉÊï∞ÊçÆÂ∫ì‰∏≠Ê£ÄÁ¥¢Âêå‰∏ÄÂú∞ÁÇπÁöÑÂõæÂÉèÔºåÂêåÊó∂ÂØπËßÜËßíÂíåÂ§ñËßÇÂèòÂåñ‰øùÊåÅÈ≤ÅÊ£íÊÄß„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºå‰∏Ä‰∫õVPRÂü∫ÂáÜÊµãËØïÂèØ‰ª•ÈÄöËøá‰ΩøÁî®ËßÜËßâÂü∫Á°ÄÊ®°Âûã‰Ωú‰∏∫È™®Âπ≤ÁΩëÁªúÔºåÂπ∂Âú®Â§ßËßÑÊ®°ÂíåÂ§öÊ†∑ÂåñÁöÑVPRÁâπÂÆöÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁöÑÊñπÊ≥ïÊù•Ëß£ÂÜ≥„ÄÇÁÑ∂ËÄåÔºåÂΩìÊµãËØïÁéØÂ¢É‰∏éÂ∏∏ËßÅÁöÑVPRËÆ≠ÁªÉÊï∞ÊçÆÈõÜÊòæËëó‰∏çÂêåÊó∂Ôºå‰∏Ä‰∫õÂü∫ÂáÜÊµãËØï‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∫íË°•ÁöÑ„ÄÅÊú™Ë¢´ÂÖÖÂàÜÂà©Áî®ÁöÑ‰ø°ÊÅØÊù•Ê∫êÔºå‰ª•Âº•ÂêàËÆ≠ÁªÉ-ÊµãËØïÂüüÁöÑÂ∑ÆË∑ùÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊèêÈ´òÊúÄÂÖàËøõÔºàSOTAÔºâVPRÊñπÊ≥ïÂú®Ëøô‰∫õÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂü∫ÂáÜÊµãËØï‰∏äÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÂèëÁé∞ÊµãËØïÊó∂ÁöÑÂèÇËÄÉÈõÜÔºàÂç≥‚ÄúÂú∞Âõæ‚ÄùÔºâÂåÖÂê´ÁõÆÊ†áÂüüÁöÑÂõæÂÉèÂíåÂßøÊÄÅÔºåÂπ∂‰∏îÂú®ËÆ∏Â§öVPRÂ∫îÁî®‰∏≠ÂøÖÈ°ªÂú®Êé•Êî∂Âà∞ÊµãËØïÊó∂Êü•ËØ¢‰πãÂâçÂèØÁî®„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫Âú®Âú∞Âõæ‰∏äÂØπVPRÊ®°ÂûãÊâßË°åÁÆÄÂçïÁöÑÂèÇËÄÉÈõÜÂæÆË∞ÉÔºàRSFÔºâÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜËøô‰∫õÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊï∞ÊçÆÈõÜ‰∏äÁöÑSOTAÊÄßËÉΩÔºàÂπ≥ÂùáRecall@1ÊèêÈ´ò‰∫ÜÁ∫¶2.3%Ôºâ„ÄÇÂæÆË∞ÉÂêéÁöÑÊ®°Âûã‰øùÁïô‰∫ÜÊ≥õÂåñËÉΩÂäõÔºåÂπ∂‰∏îRSFÈÄÇÁî®‰∫é‰∏çÂêåÁöÑÊµãËØïÊï∞ÊçÆÈõÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜËßâÂÆö‰ΩçÔºàVPRÔºâÊó®Âú®Ëß£ÂÜ≥Âú®‰∏çÂêåËßÜËßíÂíåÂÖâÁÖßÊù°‰ª∂‰∏ãÔºå‰ªéÂèÇËÄÉÂõæÂÉèÊï∞ÊçÆÂ∫ì‰∏≠Ê£ÄÁ¥¢‰∏éÊü•ËØ¢ÂõæÂÉèÂØπÂ∫î‰ΩçÁΩÆÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ËÆ≠ÁªÉÊï∞ÊçÆ‰∏éÊµãËØïÁéØÂ¢ÉÁõ∏‰ººÊó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂΩìÊµãËØïÁéØÂ¢É‰∏éËÆ≠ÁªÉÊï∞ÊçÆÂ≠òÂú®ÊòæËëóÂ∑ÆÂºÇÊó∂ÔºåÊÄßËÉΩ‰ºöÊÄ•Ââß‰∏ãÈôç„ÄÇÁé∞ÊúâÊñπÊ≥ïÂøΩÁï•‰∫ÜÊµãËØïÊó∂ÂèØÁî®ÁöÑÂèÇËÄÉÈõÜ‰ø°ÊÅØÔºåÊú™ËÉΩÂÖÖÂàÜÂà©Áî®ÁõÆÊ†áÂüüÁöÑÊï∞ÊçÆÊù•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÊµãËØïÊó∂ÂèØÁî®ÁöÑÂèÇËÄÉÈõÜÔºàÂç≥ÁõÆÊ†áÁéØÂ¢ÉÁöÑÂú∞ÂõæÔºâÂØπVPRÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ªéËÄå‰ΩøÊ®°ÂûãÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁõÆÊ†áÂüüÁöÑÁâπÂæÅÂàÜÂ∏É„ÄÇËøôÁßçÊñπÊ≥ïÂÅáËÆæÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÁõÆÊ†áÁéØÂ¢ÉÁöÑÂú∞ÂõæÈÄöÂ∏∏ÊòØÂ∑≤Áü•ÁöÑÔºåÂèØ‰ª•Âú®Êé•Êî∂Âà∞Êü•ËØ¢ÂõæÂÉè‰πãÂâç‰ΩøÁî®„ÄÇÈÄöËøáÂú®ÁõÆÊ†áÂüüÊï∞ÊçÆ‰∏äËøõË°åÂæÆË∞ÉÔºåÂèØ‰ª•ÊúâÊïàÂáèÂ∞èËÆ≠ÁªÉÂüüÂíåÊµãËØïÂüü‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÊèêÈ´òVPRÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÔºå‰ΩøÁî®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÈ¢ÑËÆ≠ÁªÉVPRÊ®°ÂûãÔºõÁÑ∂ÂêéÔºåÂú®ÊµãËØïÊó∂ÔºåÂà©Áî®ÁõÆÊ†áÁéØÂ¢ÉÁöÑÂèÇËÄÉÈõÜÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇÂæÆË∞ÉËøáÁ®ã‰ΩøÁî®ÂèÇËÄÉÈõÜ‰∏≠ÁöÑÂõæÂÉèÂíåÂØπÂ∫îÁöÑÂßøÊÄÅ‰ø°ÊÅØÔºåÈÄöËøá‰ºòÂåñÊ®°ÂûãÂèÇÊï∞Ôºå‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁõÆÊ†áÂüüÁöÑÁâπÂæÅÂàÜÂ∏É„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÁÆÄÂçïÊòìÂÆûÁé∞ÔºåÂèØ‰ª•Êñπ‰æøÂú∞ÈõÜÊàêÂà∞Áé∞ÊúâÁöÑVPRÁ≥ªÁªü‰∏≠„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂèÇËÄÉÈõÜÂæÆË∞ÉÔºàRSFÔºâÁöÑÊ¶ÇÂøµÔºåÂπ∂Â∞ÜÂÖ∂Â∫îÁî®‰∫éVPR‰ªªÂä°‰∏≠„ÄÇ‰∏é‰º†ÁªüÁöÑVPRÊñπÊ≥ï‰∏çÂêåÔºåRSFÂÖÖÂàÜÂà©Áî®‰∫ÜÊµãËØïÊó∂ÂèØÁî®ÁöÑÁõÆÊ†áÂüü‰ø°ÊÅØÔºåÈÄöËøáÂæÆË∞ÉÊ®°ÂûãÊù•ÂáèÂ∞èÂüüÂ∑ÆÂºÇ„ÄÇËøôÁßçÊñπÊ≥ï‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂè™ÈúÄË¶ÅÁõÆÊ†áÁéØÂ¢ÉÁöÑÂèÇËÄÉÈõÜÔºåÂõ†Ê≠§ÂÖ∑ÊúâÂæàÈ´òÁöÑÂÆûÁî®‰ª∑ÂÄº„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöRSFÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨ÈÄâÊã©ÂêàÈÄÇÁöÑÂæÆË∞ÉÁ≠ñÁï•ÂíåÊçüÂ§±ÂáΩÊï∞„ÄÇËÆ∫Êñá‰∏≠‰ΩøÁî®‰∫ÜÁÆÄÂçïÁöÑÂæÆË∞ÉÁ≠ñÁï•ÔºåÂç≥Âõ∫ÂÆöÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÈÉ®ÂàÜÂèÇÊï∞ÔºåÂè™ÂæÆË∞ÉÈÉ®ÂàÜÂèÇÊï∞Ôºå‰ª•Èò≤Ê≠¢ËøáÊãüÂêà„ÄÇÊçüÂ§±ÂáΩÊï∞ÂèØ‰ª•ÈÄâÊã©Â∏∏Áî®ÁöÑVPRÊçüÂ§±ÂáΩÊï∞ÔºåÂ¶ÇTriplet LossÊàñContrastive Loss„ÄÇÊ≠§Â§ñÔºåÂèÇËÄÉÈõÜÁöÑÂ§ßÂ∞èÂíåË¥®Èáè‰πü‰ºöÂΩ±ÂìçÂæÆË∞ÉÊïàÊûúÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÂ∫îÁî®Âú∫ÊôØËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂèÇËÄÉÈõÜÂæÆË∞ÉÔºàRSFÔºâÂú®Â§ö‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑVPRÊï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜSOTAÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∫õÊï∞ÊçÆÈõÜ‰∏äÔºåRecall@1ÊåáÊ†áÂπ≥ÂùáÊèêÂçá‰∫ÜÁ∫¶2.3%„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åËøòËØÅÊòé‰∫ÜÂæÆË∞ÉÂêéÁöÑÊ®°Âûã‰ªçÁÑ∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•Âú®‰∏çÂêåÁöÑÊµãËØïÊï∞ÊçÆÈõÜ‰∏äÂèñÂæóËâØÂ•ΩÁöÑÊïàÊûú„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåRSFÊòØ‰∏ÄÁßçÊúâÊïàÁöÑË∑®ÂüüVPRÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÂú®Ëøô‰∫õÂ∫îÁî®‰∏≠ÔºåÊú∫Âô®‰∫∫ÊàñËΩ¶ËæÜÈúÄË¶ÅÂú®Êú™Áü•ÁéØÂ¢É‰∏≠ËøõË°åÂÆö‰ΩçÂíåÂØºËà™„ÄÇÈÄöËøáÂà©Áî®È¢ÑÂÖàÊûÑÂª∫ÁöÑÂú∞ÂõæÔºàÂèÇËÄÉÈõÜÔºâÂØπVPRÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåÂèØ‰ª•ÊèêÈ´òÂÆö‰ΩçÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂèØÈù†ÁöÑËá™‰∏ªÂØºËà™„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éË∑®Â≠£ËäÇ„ÄÅË∑®ÂÖâÁÖßÁ≠âÂ§çÊùÇÁéØÂ¢É‰∏ãÁöÑËßÜËßâÂÆö‰Ωç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Given a query image, Visual Place Recognition (VPR) is the task of retrieving an image of the same place from a reference database with robustness to viewpoint and appearance changes. Recent works show that some VPR benchmarks are solved by methods using Vision-Foundation-Model backbones and trained on large-scale and diverse VPR-specific datasets. Several benchmarks remain challenging, particularly when the test environments differ significantly from the usual VPR training datasets. We propose a complementary, unexplored source of information to bridge the train-test domain gap, which can further improve the performance of State-of-the-Art (SOTA) VPR methods on such challenging benchmarks. Concretely, we identify that the test-time reference set, the "map", contains images and poses of the target domain, and must be available before the test-time query is received in several VPR applications. Therefore, we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these challenging datasets. Finetuned models retain generalization, and RSF works across diverse test datasets.

