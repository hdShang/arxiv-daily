---
layout: default
title: The Overlooked Value of Test-time Reference Sets in Visual Place Recognition
---

# The Overlooked Value of Test-time Reference Sets in Visual Place Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03751" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03751v1</a>
  <a href="https://arxiv.org/pdf/2510.03751.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03751v1" onclick="toggleFavorite(this, '2510.03751v1', 'The Overlooked Value of Test-time Reference Sets in Visual Place Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mubariz Zaffar, Liangliang Nan, Sebastian Scherer, Julian F. P. Kooij

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-04

**å¤‡æ³¨**: Accepted at ICCV 2025 Workshop CrocoDL

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‚è€ƒé›†å¾®è°ƒæ–¹æ³•ï¼Œæå‡è§†è§‰å®šä½åœ¨è·¨åŸŸåœºæ™¯ä¸‹çš„æ³›åŒ–æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å®šä½` `åŸŸé€‚åº”` `å‚è€ƒé›†å¾®è°ƒ` `æœºå™¨äººå¯¼èˆª` `å›¾åƒæ£€ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VPRæ–¹æ³•åœ¨è·¨åŸŸåœºæ™¯ä¸‹æ³›åŒ–æ€§ä¸è¶³ï¼Œæµ‹è¯•ç¯å¢ƒä¸è®­ç»ƒæ•°æ®å·®å¼‚å¤§å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. åˆ©ç”¨æµ‹è¯•æ—¶å¯ç”¨çš„å‚è€ƒé›†ï¼ˆåœ°å›¾ï¼‰ä¿¡æ¯ï¼Œé€šè¿‡å¾®è°ƒVPRæ¨¡å‹æ¥é€‚åº”ç›®æ ‡åŸŸã€‚
3. å‚è€ƒé›†å¾®è°ƒï¼ˆRSFï¼‰åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šæå‡äº†SOTAæ€§èƒ½ï¼Œä¸”ä¿æŒäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å®šä½ï¼ˆVPRï¼‰æ—¨åœ¨ç»™å®šæŸ¥è¯¢å›¾åƒï¼Œä»å‚è€ƒæ•°æ®åº“ä¸­æ£€ç´¢åŒä¸€åœ°ç‚¹çš„å›¾åƒï¼ŒåŒæ—¶å¯¹è§†è§’å’Œå¤–è§‚å˜åŒ–ä¿æŒé²æ£’æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸€äº›VPRåŸºå‡†æµ‹è¯•å¯ä»¥é€šè¿‡ä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶åœ¨å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„VPRç‰¹å®šæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ–¹æ³•æ¥è§£å†³ã€‚ç„¶è€Œï¼Œå½“æµ‹è¯•ç¯å¢ƒä¸å¸¸è§çš„VPRè®­ç»ƒæ•°æ®é›†æ˜¾è‘—ä¸åŒæ—¶ï¼Œä¸€äº›åŸºå‡†æµ‹è¯•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§äº’è¡¥çš„ã€æœªè¢«å……åˆ†åˆ©ç”¨çš„ä¿¡æ¯æ¥æºï¼Œä»¥å¼¥åˆè®­ç»ƒ-æµ‹è¯•åŸŸçš„å·®è·ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æœ€å…ˆè¿›ï¼ˆSOTAï¼‰VPRæ–¹æ³•åœ¨è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å‘ç°æµ‹è¯•æ—¶çš„å‚è€ƒé›†ï¼ˆå³â€œåœ°å›¾â€ï¼‰åŒ…å«ç›®æ ‡åŸŸçš„å›¾åƒå’Œå§¿æ€ï¼Œå¹¶ä¸”åœ¨è®¸å¤šVPRåº”ç”¨ä¸­å¿…é¡»åœ¨æ¥æ”¶åˆ°æµ‹è¯•æ—¶æŸ¥è¯¢ä¹‹å‰å¯ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºåœ¨åœ°å›¾ä¸Šå¯¹VPRæ¨¡å‹æ‰§è¡Œç®€å•çš„å‚è€ƒé›†å¾®è°ƒï¼ˆRSFï¼‰ï¼Œä»è€Œæé«˜äº†è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„SOTAæ€§èƒ½ï¼ˆå¹³å‡Recall@1æé«˜äº†çº¦2.3%ï¼‰ã€‚å¾®è°ƒåçš„æ¨¡å‹ä¿ç•™äº†æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”RSFé€‚ç”¨äºä¸åŒçš„æµ‹è¯•æ•°æ®é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰å®šä½ï¼ˆVPRï¼‰æ—¨åœ¨è§£å†³åœ¨ä¸åŒè§†è§’å’Œå…‰ç…§æ¡ä»¶ä¸‹ï¼Œä»å‚è€ƒå›¾åƒæ•°æ®åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢å›¾åƒå¯¹åº”ä½ç½®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒæ•°æ®ä¸æµ‹è¯•ç¯å¢ƒç›¸ä¼¼æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†å½“æµ‹è¯•ç¯å¢ƒä¸è®­ç»ƒæ•°æ®å­˜åœ¨æ˜¾è‘—å·®å¼‚æ—¶ï¼Œæ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•å¿½ç•¥äº†æµ‹è¯•æ—¶å¯ç”¨çš„å‚è€ƒé›†ä¿¡æ¯ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ç›®æ ‡åŸŸçš„æ•°æ®æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æµ‹è¯•æ—¶å¯ç”¨çš„å‚è€ƒé›†ï¼ˆå³ç›®æ ‡ç¯å¢ƒçš„åœ°å›¾ï¼‰å¯¹VPRæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œä½¿æ¨¡å‹æ›´å¥½åœ°é€‚åº”ç›®æ ‡åŸŸçš„ç‰¹å¾åˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•å‡è®¾åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç›®æ ‡ç¯å¢ƒçš„åœ°å›¾é€šå¸¸æ˜¯å·²çŸ¥çš„ï¼Œå¯ä»¥åœ¨æ¥æ”¶åˆ°æŸ¥è¯¢å›¾åƒä¹‹å‰ä½¿ç”¨ã€‚é€šè¿‡åœ¨ç›®æ ‡åŸŸæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æœ‰æ•ˆå‡å°è®­ç»ƒåŸŸå’Œæµ‹è¯•åŸŸä¹‹é—´çš„å·®å¼‚ï¼Œæé«˜VPRçš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†é¢„è®­ç»ƒVPRæ¨¡å‹ï¼›ç„¶åï¼Œåœ¨æµ‹è¯•æ—¶ï¼Œåˆ©ç”¨ç›®æ ‡ç¯å¢ƒçš„å‚è€ƒé›†å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å¾®è°ƒè¿‡ç¨‹ä½¿ç”¨å‚è€ƒé›†ä¸­çš„å›¾åƒå’Œå¯¹åº”çš„å§¿æ€ä¿¡æ¯ï¼Œé€šè¿‡ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ç›®æ ‡åŸŸçš„ç‰¹å¾åˆ†å¸ƒã€‚æ•´ä¸ªæ¡†æ¶ç®€å•æ˜“å®ç°ï¼Œå¯ä»¥æ–¹ä¾¿åœ°é›†æˆåˆ°ç°æœ‰çš„VPRç³»ç»Ÿä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†å‚è€ƒé›†å¾®è°ƒï¼ˆRSFï¼‰çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºVPRä»»åŠ¡ä¸­ã€‚ä¸ä¼ ç»Ÿçš„VPRæ–¹æ³•ä¸åŒï¼ŒRSFå……åˆ†åˆ©ç”¨äº†æµ‹è¯•æ—¶å¯ç”¨çš„ç›®æ ‡åŸŸä¿¡æ¯ï¼Œé€šè¿‡å¾®è°ƒæ¨¡å‹æ¥å‡å°åŸŸå·®å¼‚ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®ï¼Œåªéœ€è¦ç›®æ ‡ç¯å¢ƒçš„å‚è€ƒé›†ï¼Œå› æ­¤å…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚

**å…³é”®è®¾è®¡**ï¼šRSFçš„å…³é”®è®¾è®¡åŒ…æ‹¬é€‰æ‹©åˆé€‚çš„å¾®è°ƒç­–ç•¥å’ŒæŸå¤±å‡½æ•°ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†ç®€å•çš„å¾®è°ƒç­–ç•¥ï¼Œå³å›ºå®šé¢„è®­ç»ƒæ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ï¼Œåªå¾®è°ƒéƒ¨åˆ†å‚æ•°ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚æŸå¤±å‡½æ•°å¯ä»¥é€‰æ‹©å¸¸ç”¨çš„VPRæŸå¤±å‡½æ•°ï¼Œå¦‚Triplet Lossæˆ–Contrastive Lossã€‚æ­¤å¤–ï¼Œå‚è€ƒé›†çš„å¤§å°å’Œè´¨é‡ä¹Ÿä¼šå½±å“å¾®è°ƒæ•ˆæœï¼Œéœ€è¦æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå‚è€ƒé›†å¾®è°ƒï¼ˆRSFï¼‰åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„VPRæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†SOTAæ–¹æ³•çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒRecall@1æŒ‡æ ‡å¹³å‡æå‡äº†çº¦2.3%ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¯æ˜äº†å¾®è°ƒåçš„æ¨¡å‹ä»ç„¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æµ‹è¯•æ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½çš„æ•ˆæœã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRSFæ˜¯ä¸€ç§æœ‰æ•ˆçš„è·¨åŸŸVPRæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œæœºå™¨äººæˆ–è½¦è¾†éœ€è¦åœ¨æœªçŸ¥ç¯å¢ƒä¸­è¿›è¡Œå®šä½å’Œå¯¼èˆªã€‚é€šè¿‡åˆ©ç”¨é¢„å…ˆæ„å»ºçš„åœ°å›¾ï¼ˆå‚è€ƒé›†ï¼‰å¯¹VPRæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æé«˜å®šä½çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œä»è€Œå®ç°æ›´å¯é çš„è‡ªä¸»å¯¼èˆªã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè·¨å­£èŠ‚ã€è·¨å…‰ç…§ç­‰å¤æ‚ç¯å¢ƒä¸‹çš„è§†è§‰å®šä½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Given a query image, Visual Place Recognition (VPR) is the task of retrieving an image of the same place from a reference database with robustness to viewpoint and appearance changes. Recent works show that some VPR benchmarks are solved by methods using Vision-Foundation-Model backbones and trained on large-scale and diverse VPR-specific datasets. Several benchmarks remain challenging, particularly when the test environments differ significantly from the usual VPR training datasets. We propose a complementary, unexplored source of information to bridge the train-test domain gap, which can further improve the performance of State-of-the-Art (SOTA) VPR methods on such challenging benchmarks. Concretely, we identify that the test-time reference set, the "map", contains images and poses of the target domain, and must be available before the test-time query is received in several VPR applications. Therefore, we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these challenging datasets. Finetuned models retain generalization, and RSF works across diverse test datasets.

