---
layout: default
title: Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs
---

# Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.03955" target="_blank" class="toolbar-btn">arXiv: 2510.03955v1</a>
    <a href="https://arxiv.org/pdf/2510.03955.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03955v1" 
            onclick="toggleFavorite(this, '2510.03955v1', 'Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Sameep Vani, Shreyas Jena, Maitreya Patel, Chitta Baral, Somak Aditya, Yezhou Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-04

**å¤‡æ³¨**: 17 pages, 9 figures, 6 tables. Presents TimeWarp, a synthetic preference data framework to improve temporal understanding in Video-LLMs, showing consistent gains across seven benchmarks. Includes supplementary material in the Appendix

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/sameepv21/timewarp)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TimeWarpï¼šåˆ©ç”¨åˆæˆåå¥½æ•°æ®å¢å¼ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´ç†è§£èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¤§è¯­è¨€æ¨¡å‹` `æ—¶é—´ç†è§£` `åˆæˆæ•°æ®` `åå¥½å­¦ä¹ ` `è§†é¢‘åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Video-LLMåœ¨æ—¶é—´ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼ŒåŸå› æ˜¯å¾®è°ƒæ•°æ®é›†ç¼ºä¹è§†è§‰å¤æ‚æ€§å’Œæ—¶é—´ç»†å¾®å·®åˆ«ã€‚
2. TimeWarpé€šè¿‡ç³»ç»Ÿåœ°åˆ›å»ºåˆæˆæ—¶é—´æ•°æ®é›†ï¼Œå¾®è°ƒæ¨¡å‹å“åº”ï¼Œä½¿å…¶æ›´å…³æ³¨è§†é¢‘è¾“å…¥ï¼Œä»è€Œæå‡æ—¶é—´ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeWarpæ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤šä¸ªæ—¶é—´ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(Video-LLMs)åœ¨é€šç”¨è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘å­—å¹•å’Œæè¿°æ€§ä»»åŠ¡ä¸­ï¼Œä½†åœ¨éœ€è¦ç»†ç²’åº¦æ—¶é—´ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è¿™ç§å±€é™æ€§æºäºå½“å‰å¾®è°ƒæ•°æ®é›†ä¸­ç¼ºä¹è§†è§‰å¤æ‚æ€§å’Œæ—¶é—´ç»†å¾®å·®åˆ«ï¼Œå¯¼è‡´è¿™äº›æ¨¡å‹è¿‡åº¦ä¾èµ–åŸºäºè¯­è¨€çš„æ¨ç†ï¼Œè€Œä¸æ˜¯çœŸæ­£ç†è§£è§†é¢‘åŠ¨æ€ã€‚æœ¬æ–‡æå‡ºTimeWarpï¼Œä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œç”¨äºåˆ›å»ºæœ‰é’ˆå¯¹æ€§çš„åˆæˆæ—¶é—´æ•°æ®é›†ï¼Œä»¥å¾®è°ƒæ¨¡å‹å“åº”ï¼Œé¼“åŠ±å…¶å…³æ³¨ç»™å®šçš„è¾“å…¥è§†é¢‘ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä½¿ç”¨TimeWarpåˆ›å»ºçš„å¤§è§„æ¨¡åå¥½æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ•è·äº†ç»å¸¸è¢«å¿½è§†çš„å¤æ‚æ—¶é—´åŠ¨æ€ï¼Œå°†æ¨¡å‹çš„å“åº”ä¸è§†è§‰å’Œæ—¶é—´ä¿¡æ¯è”ç³»èµ·æ¥ã€‚å®éªŒè¡¨æ˜ï¼Œå½“æˆ‘ä»¬çš„æ–¹æ³•åº”ç”¨äºç°æœ‰æ¨¡å‹æ—¶ï¼Œå®ƒæ˜¾è‘—æé«˜äº†æ—¶é—´ç†è§£åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œçªå‡ºäº†æˆ‘ä»¬æå‡ºçš„æ•°æ®é›†åœ¨æå‡Video-LLMsæ—¶é—´ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ç»å¯¹æ€§èƒ½æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰åœ¨ç»†ç²’åº¦æ—¶é—´ç†è§£ä»»åŠ¡ä¸­çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–çš„å¾®è°ƒæ•°æ®é›†ç¼ºä¹è¶³å¤Ÿçš„è§†è§‰å¤æ‚æ€§å’Œæ—¶é—´ç»†å¾®å·®åˆ«ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦ä¾èµ–è¯­è¨€æ¨ç†ï¼Œæ— æ³•çœŸæ­£ç†è§£è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€å˜åŒ–ã€‚è¿™é™åˆ¶äº†æ¨¡å‹åœ¨éœ€è¦ç²¾ç¡®æ—¶é—´æ¨ç†çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¾‹å¦‚äº‹ä»¶æ’åºã€å› æœå…³ç³»åˆ¤æ–­ç­‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åˆæˆæ•°æ®æ¥å¢å¼ºæ¨¡å‹çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡TimeWarpæ–¹æ³•ç”Ÿæˆå…·æœ‰ä¸°å¯Œæ—¶é—´ä¿¡æ¯çš„åˆæˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨è¯¥æ•°æ®é›†å¯¹Video-LLMè¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨è®©æ¨¡å‹æ›´å¤šåœ°å…³æ³¨è§†é¢‘å†…å®¹æœ¬èº«ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–è¯­è¨€å…ˆéªŒçŸ¥è¯†ã€‚é€šè¿‡åå¥½å­¦ä¹ çš„æ–¹å¼ï¼Œè®©æ¨¡å‹å­¦ä¹ åˆ°ä¸åŒæ—¶é—´äº‹ä»¶å‘ç”Ÿçš„åˆç†æ€§ï¼Œä»è€Œæå‡æ—¶é—´ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTimeWarpæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨ç°æœ‰çš„Video-LLMç”Ÿæˆåˆå§‹çš„è§†é¢‘æè¿°ï¼›2) é€šè¿‡å¯¹è§†é¢‘å¸§è¿›è¡Œæ—¶é—´ä¸Šçš„æ‰°åŠ¨ï¼ˆä¾‹å¦‚ï¼Œäº¤æ¢å¸§çš„é¡ºåºã€åˆ é™¤æˆ–é‡å¤å¸§ï¼‰æ¥åˆ›å»ºå¤šä¸ªæ—¶é—´æ‰­æ›²çš„è§†é¢‘ç‰ˆæœ¬ï¼›3) ä½¿ç”¨Video-LLMå¯¹æ¯ä¸ªæ‰­æ›²çš„è§†é¢‘ç‰ˆæœ¬ç”Ÿæˆæè¿°ï¼›4) ä½¿ç”¨äººå·¥æˆ–è‡ªåŠ¨çš„æ–¹å¼å¯¹ä¸åŒæè¿°è¿›è¡Œåå¥½æ’åºï¼Œä»è€Œæ„å»ºåå¥½æ•°æ®é›†ï¼›5) ä½¿ç”¨åå¥½æ•°æ®é›†å¯¹Video-LLMè¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•ï¼ˆTimeWarpï¼‰æ¥ç”Ÿæˆç”¨äºå¢å¼ºVideo-LLMæ—¶é—´ç†è§£èƒ½åŠ›çš„åˆæˆåå¥½æ•°æ®é›†ã€‚ä¸ä»¥å¾€ä¾èµ–äººå·¥æ ‡æ³¨æˆ–ç°æœ‰æ•°æ®é›†çš„æ–¹æ³•ä¸åŒï¼ŒTimeWarpèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤§é‡å…·æœ‰æ—¶é—´ä¿¡æ¯çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œæœ‰æ•ˆåœ°è§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œä½¿ç”¨åå¥½å­¦ä¹ çš„æ–¹å¼ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ä¸åŒæ—¶é—´äº‹ä»¶å‘ç”Ÿçš„åˆç†æ€§ï¼Œä»è€Œæå‡æ—¶é—´ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šTimeWarpçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ—¶é—´æ‰°åŠ¨ç­–ç•¥ï¼šè®¾è®¡ä¸åŒçš„æ—¶é—´æ‰°åŠ¨æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œå¸§äº¤æ¢ã€åˆ é™¤ã€é‡å¤ï¼‰æ¥åˆ›å»ºä¸åŒçš„è§†é¢‘ç‰ˆæœ¬ï¼›2) åå¥½æ’åºæ–¹æ³•ï¼šä½¿ç”¨äººå·¥æˆ–è‡ªåŠ¨çš„æ–¹å¼å¯¹ä¸åŒè§†é¢‘æè¿°è¿›è¡Œåå¥½æ’åºï¼Œæ„å»ºåå¥½æ•°æ®é›†ï¼›3) åå¥½å­¦ä¹ æŸå¤±å‡½æ•°ï¼šé€‰æ‹©åˆé€‚çš„åå¥½å­¦ä¹ æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œpairwise ranking lossï¼‰æ¥è®­ç»ƒVideo-LLMï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°ä¸åŒæ—¶é—´äº‹ä»¶å‘ç”Ÿçš„åˆç†æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeWarpæ–¹æ³•åœ¨ä¸ƒä¸ªæ—¶é—´ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ç»å¯¹æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹çš„æ€§èƒ½æå‡äº†è¶…è¿‡10%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒTimeWarpæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡Video-LLMçš„æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè§†é¢‘å†…å®¹ç†è§£ã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯æå‡æ¨¡å‹å¯¹å¼‚å¸¸äº‹ä»¶çš„æ£€æµ‹èƒ½åŠ›ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥å¸®åŠ©è½¦è¾†æ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒçš„æ—¶é—´åŠ¨æ€å˜åŒ–ï¼Œä»è€Œåšå‡ºæ›´å®‰å…¨çš„å†³ç­–ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ¨åŠ¨è§†é¢‘æ™ºèƒ½åˆ†æçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While Video Large Language Models (Video-LLMs) have demonstrated remarkable performance across general video understanding benchmarks-particularly in video captioning and descriptive tasks-they consistently underperform on tasks that require fine-grained temporal understanding. This limitation arises due to the lack of visual complexity and temporal nuance in current fine-tuning datasets, leading these models to rely heavily on language-based reasoning rather than truly understanding video dynamics. In this work, we propose TimeWarp, a systematic method to create a targeted synthetic temporal dataset to fine-tune the model's responses to encourage it to focus on the given input video. We introduce a large-scale preference dataset, created using TimeWarp, that captures intricate temporal dynamics often overlooked, grounding the model's responses to visual and temporal information. We demonstrate that when our method is applied to existing models, it significantly improves performance on temporal understanding benchmarks, highlighting the effectiveness of our proposed datasets in advancing temporal understanding in Video-LLMs, resulting in an absolute improvement in performance across seven benchmarks. Code is available at https://github.com/sameepv21/timewarp.

