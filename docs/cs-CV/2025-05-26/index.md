---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-26
---

# cs.CVï¼ˆ2025-05-26ï¼‰

ğŸ“Š å…± **48** ç¯‡è®ºæ–‡
 | ğŸ”— **18** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (15 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (15 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—4)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ğŸ”—2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction" class="interest-badge">æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250520405v1-what-changed-detecting-and-evaluating-instruction-guided-image-edits.html">What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models</a></td>
  <td>æå‡ºDICEä»¥è§£å†³å›¾åƒç¼–è¾‘ç»“æœè¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20405v1" data-paper-url="./papers/250520405v1-what-changed-detecting-and-evaluating-instruction-guided-image-edits.html" onclick="toggleFavorite(this, '2505.20405v1', 'What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250520106v1-from-data-to-modeling-fully-open-vocabulary-scene-graph-generation.html">From Data to Modeling: Fully Open-vocabulary Scene Graph Generation</a></td>
  <td>æå‡ºOvSGTRä»¥è§£å†³ä¼ ç»Ÿåœºæ™¯å›¾ç”Ÿæˆçš„å¼€æ”¾è¯æ±‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20106v1" data-paper-url="./papers/250520106v1-from-data-to-modeling-fully-open-vocabulary-scene-graph-generation.html" onclick="toggleFavorite(this, '2505.20106v1', 'From Data to Modeling: Fully Open-vocabulary Scene Graph Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250519877v1-vad-r1-towards-video-anomaly-reasoning-via-perception-to-cognition-c.html">Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought</a></td>
  <td>æå‡ºVad-R1ä»¥è§£å†³è§†é¢‘å¼‚å¸¸æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19877v1" data-paper-url="./papers/250519877v1-vad-r1-towards-video-anomaly-reasoning-via-perception-to-cognition-c.html" onclick="toggleFavorite(this, '2505.19877v1', 'Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250520152v3-mmgeolm-hard-negative-contrastive-learning-for-fine-grained-geometri.html">MMGeoLM: Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models</a></td>
  <td>æå‡ºMMGeoLMä»¥è§£å†³å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„å‡ ä½•ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20152v3" data-paper-url="./papers/250520152v3-mmgeolm-hard-negative-contrastive-learning-for-fine-grained-geometri.html" onclick="toggleFavorite(this, '2505.20152v3', 'MMGeoLM: Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250519952v1-multimodal-reasoning-agent-for-zero-shot-composed-image-retrieval.html">Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ¨ç†ä»£ç†ä»¥è§£å†³é›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19952v1" data-paper-url="./papers/250519952v1-multimodal-reasoning-agent-for-zero-shot-composed-image-retrieval.html" onclick="toggleFavorite(this, '2505.19952v1', 'Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250519863v1-fruitnerf-a-generalized-multi-fruit-counting-method-utilizing-contra.html">FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields</a></td>
  <td>æå‡ºFruitNeRF++ä»¥è§£å†³å¤šç§æ°´æœè®¡æ•°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">neural radiance field</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19863v1" data-paper-url="./papers/250519863v1-fruitnerf-a-generalized-multi-fruit-counting-method-utilizing-contra.html" onclick="toggleFavorite(this, '2505.19863v1', 'FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250519779v1-advancements-in-medical-image-classification-through-fine-tuning-nat.html">Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models</a></td>
  <td>é€šè¿‡å¾®è°ƒè‡ªç„¶é¢†åŸŸåŸºç¡€æ¨¡å‹æå‡åŒ»å­¦å›¾åƒåˆ†ç±»æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">MAE</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19779v1" data-paper-url="./papers/250519779v1-advancements-in-medical-image-classification-through-fine-tuning-nat.html" onclick="toggleFavorite(this, '2505.19779v1', 'Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250520032v1-vitapes-visuotactile-position-encodings-for-cross-modal-alignment-in.html">ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers</a></td>
  <td>æå‡ºViTaPEsä»¥è§£å†³å¤šæ¨¡æ€å¯¹é½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20032v1" data-paper-url="./papers/250520032v1-vitapes-visuotactile-position-encodings-for-cross-modal-alignment-in.html" onclick="toggleFavorite(this, '2505.20032v1', 'ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250520024v2-reasonplan-unified-scene-prediction-and-decision-reasoning-for-close.html">ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving</a></td>
  <td>æå‡ºReasonPlanä»¥è§£å†³é—­ç¯è‡ªä¸»é©¾é©¶ä¸­çš„å†³ç­–æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20024v2" data-paper-url="./papers/250520024v2-reasonplan-unified-scene-prediction-and-decision-reasoning-for-close.html" onclick="toggleFavorite(this, '2505.20024v2', 'ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250520256v1-omni-r1-reinforcement-learning-for-omnimodal-reasoning-via-two-syste.html">Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration</a></td>
  <td>æå‡ºOmni-R1ä»¥è§£å†³é•¿è§†é¢‘éŸ³é¢‘æ¨ç†ä¸ç»†ç²’åº¦åƒç´ ç†è§£çš„çŸ›ç›¾é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20256v1" data-paper-url="./papers/250520256v1-omni-r1-reinforcement-learning-for-omnimodal-reasoning-via-two-syste.html" onclick="toggleFavorite(this, '2505.20256v1', 'Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250520147v3-fudoki-discrete-flow-based-unified-understanding-and-generation-via-.html">FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities</a></td>
  <td>æå‡ºFUDOKIä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å±€é™æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">flow matching</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20147v3" data-paper-url="./papers/250520147v3-fudoki-discrete-flow-based-unified-understanding-and-generation-via-.html" onclick="toggleFavorite(this, '2505.20147v3', 'FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250520272v2-ground-r1-incentivizing-grounded-visual-reasoning-via-reinforcement-.html">Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning</a></td>
  <td>æå‡ºGround-R1ä»¥è§£å†³è§†è§‰æ¨ç†ä¸­çš„ç›‘ç£æˆæœ¬é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20272v2" data-paper-url="./papers/250520272v2-ground-r1-incentivizing-grounded-visual-reasoning-via-reinforcement-.html" onclick="toggleFavorite(this, '2505.20272v2', 'Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250519868v1-harnessing-the-power-of-training-free-techniques-in-text-to-2d-gener.html">Harnessing the Power of Training-Free Techniques in Text-to-2D Generation for Text-to-3D Generation via Score Distillation Sampling</a></td>
  <td>æå‡ºè®­ç»ƒæ— å…³æŠ€æœ¯ä»¥æå‡æ–‡æœ¬åˆ°3Dç”Ÿæˆè´¨é‡</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">classifier-free guidance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19868v1" data-paper-url="./papers/250519868v1-harnessing-the-power-of-training-free-techniques-in-text-to-2d-gener.html" onclick="toggleFavorite(this, '2505.19868v1', 'Harnessing the Power of Training-Free Techniques in Text-to-2D Generation for Text-to-3D Generation via Score Distillation Sampling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250520171v1-long-context-state-space-video-world-models.html">Long-Context State-Space Video World Models</a></td>
  <td>æå‡ºé•¿ä¸Šä¸‹æ–‡çŠ¶æ€ç©ºé—´è§†é¢‘ä¸–ç•Œæ¨¡å‹ä»¥è§£å†³é•¿æ—¶è®°å¿†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">SSM</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20171v1" data-paper-url="./papers/250520171v1-long-context-state-space-video-world-models.html" onclick="toggleFavorite(this, '2505.20171v1', 'Long-Context State-Space Video World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250520289v2-visualtoolagent-vista-a-reinforcement-learning-framework-for-visual-.html">VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection</a></td>
  <td>æå‡ºVisTAæ¡†æ¶ä»¥è§£å†³å·¥å…·é€‰æ‹©çš„åŠ¨æ€æ¢ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20289v2" data-paper-url="./papers/250520289v2-visualtoolagent-vista-a-reinforcement-learning-framework-for-visual-.html" onclick="toggleFavorite(this, '2505.20289v2', 'VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250520510v2-cpathagent-an-agent-based-foundation-model-for-interpretable-high-re.html">CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic</a></td>
  <td>æå‡ºCPathAgentä»¥è§£å†³ç—…ç†å›¾åƒåˆ†æä¸­çš„å¯è§£é‡Šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20510v2" data-paper-url="./papers/250520510v2-cpathagent-an-agent-based-foundation-model-for-interpretable-high-re.html" onclick="toggleFavorite(this, '2505.20510v2', 'CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists&#39; Diagnostic Logic')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250520053v1-multimodal-llm-guided-semantic-correction-in-text-to-image-diffusion.html">Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion</a></td>
  <td>æå‡ºMLLMå¼•å¯¼çš„è¯­ä¹‰æ ¡æ­£æ–¹æ³•ä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20053v1" data-paper-url="./papers/250520053v1-multimodal-llm-guided-semantic-correction-in-text-to-image-diffusion.html" onclick="toggleFavorite(this, '2505.20053v1', 'Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250519901v3-dynamic-i2v-exploring-image-to-video-generation-models-via-multimoda.html">Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM</a></td>
  <td>æå‡ºDynamic-I2Vä»¥è§£å†³å¤æ‚åœºæ™¯ä¸‹å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19901v3" data-paper-url="./papers/250519901v3-dynamic-i2v-exploring-image-to-video-generation-models-via-multimoda.html" onclick="toggleFavorite(this, '2505.19901v3', 'Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250519874v1-stylear-customizing-multimodal-autoregressive-model-for-style-aligne.html">StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation</a></td>
  <td>æå‡ºStyleARä»¥è§£å†³é£æ ¼å¯¹é½æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19874v1" data-paper-url="./papers/250519874v1-stylear-customizing-multimodal-autoregressive-model-for-style-aligne.html" onclick="toggleFavorite(this, '2505.19874v1', 'StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250520426v5-mmperspective-do-mllms-understand-perspective-a-comprehensive-benchm.html">MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness</a></td>
  <td>æå‡ºMMPerspectiveä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§’ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20426v5" data-paper-url="./papers/250520426v5-mmperspective-do-mllms-understand-perspective-a-comprehensive-benchm.html" onclick="toggleFavorite(this, '2505.20426v5', 'MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250520202v1-pathbench-a-comprehensive-comparison-benchmark-for-pathology-foundat.html">PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology</a></td>
  <td>æå‡ºPathBenchä»¥è§£å†³ç—…ç†åŸºç¡€æ¨¡å‹è¯„ä¼°æ ‡å‡†åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20202v1" data-paper-url="./papers/250520202v1-pathbench-a-comprehensive-comparison-benchmark-for-pathology-foundat.html" onclick="toggleFavorite(this, '2505.20202v1', 'PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250520100v1-adatp-attention-debiased-token-pruning-for-video-large-language-mode.html">AdaTP: Attention-Debiased Token Pruning for Video Large Language Models</a></td>
  <td>æå‡ºAdaTPä»¥è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›åå·®é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20100v1" data-paper-url="./papers/250520100v1-adatp-attention-debiased-token-pruning-for-video-large-language-mode.html" onclick="toggleFavorite(this, '2505.20100v1', 'AdaTP: Attention-Debiased Token Pruning for Video Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250520236v1-seeing-is-believing-but-how-much-a-comprehensive-analysis-of-verbali.html">Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models</a></td>
  <td>æå‡ºè§†è§‰ä¿¡å¿ƒæ„ŸçŸ¥æç¤ºä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„æ ¡å‡†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20236v1" data-paper-url="./papers/250520236v1-seeing-is-believing-but-how-much-a-comprehensive-analysis-of-verbali.html" onclick="toggleFavorite(this, '2505.20236v1', 'Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250519812v1-efficient-multi-modal-long-context-learning-for-training-free-adapta.html">Efficient Multi-modal Long Context Learning for Training-free Adaptation</a></td>
  <td>æå‡ºEMLoCä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19812v1" data-paper-url="./papers/250519812v1-efficient-multi-modal-long-context-learning-for-training-free-adapta.html" onclick="toggleFavorite(this, '2505.19812v1', 'Efficient Multi-modal Long Context Learning for Training-free Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/250520460v2-dipo-dual-state-images-controlled-articulated-object-generation-powe.html">DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data</a></td>
  <td>æå‡ºDIPOæ¡†æ¶ä»¥å®ç°å¯æ§çš„å…³èŠ‚ç‰©ä½“ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20460v2" data-paper-url="./papers/250520460v2-dipo-dual-state-images-controlled-articulated-object-generation-powe.html" onclick="toggleFavorite(this, '2505.20460v2', 'DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250521556v1-benign-to-toxic-jailbreaking-inducing-harmful-responses-from-harmles.html">Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts</a></td>
  <td>æå‡ºBenign-to-Toxicæ–¹æ³•ä»¥è§£å†³å®‰å…¨æœºåˆ¶å¤±æ•ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21556v1" data-paper-url="./papers/250521556v1-benign-to-toxic-jailbreaking-inducing-harmful-responses-from-harmles.html" onclick="toggleFavorite(this, '2505.21556v1', 'Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/250520156v2-hunyuanvideo-avatar-high-fidelity-audio-driven-human-animation-for-m.html">HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters</a></td>
  <td>æå‡ºHunyuanVideo-Avatarä»¥è§£å†³å¤šè§’è‰²éŸ³é¢‘é©±åŠ¨äººç±»åŠ¨ç”»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20156v2" data-paper-url="./papers/250520156v2-hunyuanvideo-avatar-high-fidelity-audio-driven-human-animation-for-m.html" onclick="toggleFavorite(this, '2505.20156v2', 'HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250520038v1-towards-video-to-piano-music-generation-with-chain-of-perform-suppor.html">Towards Video to Piano Music Generation with Chain-of-Perform Support Benchmarks</a></td>
  <td>æå‡ºCoPåŸºå‡†æ•°æ®é›†ä»¥è§£å†³è§†é¢‘åˆ°é’¢ç´éŸ³ä¹ç”Ÿæˆçš„è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20038v1" data-paper-url="./papers/250520038v1-towards-video-to-piano-music-generation-with-chain-of-perform-suppor.html" onclick="toggleFavorite(this, '2505.20038v1', 'Towards Video to Piano Music Generation with Chain-of-Perform Support Benchmarks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/250520021v1-decomposing-complex-visual-comprehension-into-atomic-visual-skills-f.html">Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models</a></td>
  <td>æå‡ºåŸå­è§†è§‰æŠ€èƒ½ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºæœ¬ä»»åŠ¡æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20021v1" data-paper-url="./papers/250520021v1-decomposing-complex-visual-comprehension-into-atomic-visual-skills-f.html" onclick="toggleFavorite(this, '2505.20021v1', 'Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/250520001v4-next-multi-grained-mixture-of-experts-via-text-modulation-for-multi-.html">NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-Identification</a></td>
  <td>æå‡ºNEXTæ¡†æ¶ä»¥è§£å†³å¤šæ¨¡æ€ç‰©ä½“é‡è¯†åˆ«ä¸­çš„ç»†ç²’åº¦ç‰¹å¾å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20001v4" data-paper-url="./papers/250520001v4-next-multi-grained-mixture-of-experts-via-text-modulation-for-multi-.html" onclick="toggleFavorite(this, '2505.20001v4', 'NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250520469v2-ccl-lgs-contrastive-codebook-learning-for-3d-language-gaussian-splat.html">CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting</a></td>
  <td>æå‡ºCCL-LGSä»¥è§£å†³3Dè¯­ä¹‰ç†è§£ä¸­çš„è§†è§’ä¸ä¸€è‡´é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20469v2" data-paper-url="./papers/250520469v2-ccl-lgs-contrastive-codebook-learning-for-3d-language-gaussian-splat.html" onclick="toggleFavorite(this, '2505.20469v2', 'CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/250520126v1-ob3d-a-new-dataset-for-benchmarking-omnidirectional-3d-reconstructio.html">OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender</a></td>
  <td>æå‡ºOB3Dæ•°æ®é›†ä»¥è§£å†³å…¨æ™¯3Dé‡å»ºä¸­çš„å‡ ä½•å¤±çœŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20126v1" data-paper-url="./papers/250520126v1-ob3d-a-new-dataset-for-benchmarking-omnidirectional-3d-reconstructio.html" onclick="toggleFavorite(this, '2505.20126v1', 'OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/250519854v2-sparse2dgs-sparse-view-surface-reconstruction-using-2d-gaussian-spla.html">Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud</a></td>
  <td>æå‡ºSparse2DGSä»¥è§£å†³ç¨€ç–è§†å›¾ä¸‹çš„3Dé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19854v2" data-paper-url="./papers/250519854v2-sparse2dgs-sparse-view-surface-reconstruction-using-2d-gaussian-spla.html" onclick="toggleFavorite(this, '2505.19854v2', 'Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/250519793v1-depth-guided-bundle-sampling-for-efficient-generalizable-neural-radi.html">Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction</a></td>
  <td>æå‡ºæ·±åº¦å¼•å¯¼çš„æŸé‡‡æ ·ç­–ç•¥ä»¥åŠ é€Ÿç¥ç»è¾å°„åœºé‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19793v1" data-paper-url="./papers/250519793v1-depth-guided-bundle-sampling-for-efficient-generalizable-neural-radi.html" onclick="toggleFavorite(this, '2505.19793v1', 'Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/250520582v1-total-editing-head-avatar-with-editable-appearance-motion-and-lighti.html">Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting</a></td>
  <td>æå‡ºTotal-Editingæ¡†æ¶ä»¥å®ç°å¤´åƒçš„å¯ç¼–è¾‘å¤–è§‚ã€è¿åŠ¨ä¸å…‰ç…§</td>
  <td class="tags-cell"><span class="paper-tag">neural radiance field</span> <span class="paper-tag">geometric consistency</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20582v1" data-paper-url="./papers/250520582v1-total-editing-head-avatar-with-editable-appearance-motion-and-lighti.html" onclick="toggleFavorite(this, '2505.20582v1', 'Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250519813v1-golf-nrt-integrating-global-context-and-local-geometry-for-few-shot-.html">GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis</a></td>
  <td>æå‡ºGoLF-NRTä»¥è§£å†³å°‘é‡è§†å›¾åˆæˆè´¨é‡ä¸‹é™é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span> <span class="paper-tag">scene reconstruction</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19813v1" data-paper-url="./papers/250519813v1-golf-nrt-integrating-global-context-and-local-geometry-for-few-shot-.html" onclick="toggleFavorite(this, '2505.19813v1', 'GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250519919v1-weather-magician-reconstruction-and-rendering-framework-for-4d-weath.html">Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time</a></td>
  <td>æå‡ºWeather-Magicianæ¡†æ¶ä»¥è§£å†³å®æ—¶å¤©æ°”åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19919v1" data-paper-url="./papers/250519919v1-weather-magician-reconstruction-and-rendering-framework-for-4d-weath.html" onclick="toggleFavorite(this, '2505.19919v1', 'Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250519883v2-erpgs-equirectangular-image-rendering-enhanced-with-3d-gaussian-regu.html">ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization</a></td>
  <td>æå‡ºErpGSä»¥è§£å†³360åº¦å›¾åƒæ¸²æŸ“å¤±çœŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19883v2" data-paper-url="./papers/250519883v2-erpgs-equirectangular-image-rendering-enhanced-with-3d-gaussian-regu.html" onclick="toggleFavorite(this, '2505.19883v2', 'ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/250520271v1-in-context-brush-zero-shot-customized-subject-insertion-with-context.html">In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation</a></td>
  <td>æå‡ºIn-Context Brushä»¥è§£å†³å®šåˆ¶åŒ–ä¸»é¢˜æ’å…¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20271v1" data-paper-url="./papers/250520271v1-in-context-brush-zero-shot-customized-subject-insertion-with-context.html" onclick="toggleFavorite(this, '2505.20271v1', 'In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/250520498v2-controltac-force-and-position-controlled-tactile-data-augmentation-w.html">ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image</a></td>
  <td>æå‡ºControlTacä»¥è§£å†³å¤§è§„æ¨¡è§¦è§‰æ•°æ®æ”¶é›†æˆæœ¬é«˜çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">physically plausible</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20498v2" data-paper-url="./papers/250520498v2-controltac-force-and-position-controlled-tactile-data-augmentation-w.html" onclick="toggleFavorite(this, '2505.20498v2', 'ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/250519911v2-attention-your-vision-language-model-could-be-maliciously-manipulate.html">Attention! Your Vision Language Model Could Be Maliciously Manipulated</a></td>
  <td>æå‡ºè§†è§‰è¯­è¨€æ¨¡å‹æ“æ§æ”»å‡»ä»¥åº”å¯¹æ¨¡å‹è„†å¼±æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19911v2" data-paper-url="./papers/250519911v2-attention-your-vision-language-model-could-be-maliciously-manipulate.html" onclick="toggleFavorite(this, '2505.19911v2', 'Attention! Your Vision Language Model Could Be Maliciously Manipulated')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>42</td>
  <td><a href="./papers/250520056v1-pamd-plausibility-aware-motion-diffusion-model-for-long-dance-genera.html">PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation</a></td>
  <td>æå‡ºPAMDä»¥è§£å†³é•¿èˆè¹ˆç”Ÿæˆä¸­çš„ç‰©ç†åˆç†æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion diffusion model</span> <span class="paper-tag">motion diffusion</span> <span class="paper-tag">physically plausible</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20056v1" data-paper-url="./papers/250520056v1-pamd-plausibility-aware-motion-diffusion-model-for-long-dance-genera.html" onclick="toggleFavorite(this, '2505.20056v1', 'PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/250520287v1-motionpro-a-precise-motion-controller-for-image-to-video-generation.html">MotionPro: A Precise Motion Controller for Image-to-Video Generation</a></td>
  <td>æå‡ºMotionProä»¥è§£å†³å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„ç²¾ç¡®è¿åŠ¨æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion synthesis</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20287v1" data-paper-url="./papers/250520287v1-motionpro-a-precise-motion-controller-for-image-to-video-generation.html" onclick="toggleFavorite(this, '2505.20287v1', 'MotionPro: A Precise Motion Controller for Image-to-Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>44</td>
  <td><a href="./papers/250520129v3-agentic-3d-scene-generation-with-spatially-contextualized-vlms.html">Agentic 3D Scene Generation with Spatially Contextualized VLMs</a></td>
  <td>æå‡ºä¸€ç§æ–°èŒƒå¼ä»¥è§£å†³VLMåœ¨3Dåœºæ™¯ç”Ÿæˆä¸­çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20129v3" data-paper-url="./papers/250520129v3-agentic-3d-scene-generation-with-spatially-contextualized-vlms.html" onclick="toggleFavorite(this, '2505.20129v3', 'Agentic 3D Scene Generation with Spatially Contextualized VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/250520279v2-vlm-3r-vision-language-models-augmented-with-instruction-aligned-3d-.html">VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction</a></td>
  <td>æå‡ºVLM-3Rä»¥è§£å†³3Dåœºæ™¯ç†è§£çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20279v2" data-paper-url="./papers/250520279v2-vlm-3r-vision-language-models-augmented-with-instruction-aligned-3d-.html" onclick="toggleFavorite(this, '2505.20279v2', 'VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äº”äº¤äº’ä¸ååº”-interaction-reaction">ğŸ”¬ æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>46</td>
  <td><a href="./papers/250520507v2-electrolyzers-hsi-close-range-multi-scene-hyperspectral-imaging-benc.html">Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset</a></td>
  <td>æå‡ºElectrolyzers-HSIæ•°æ®é›†ä»¥åŠ é€Ÿç”µè§£å™¨ææ–™åˆ†ç±»</td>
  <td class="tags-cell"><span class="paper-tag">HSI</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20507v2" data-paper-url="./papers/250520507v2-electrolyzers-hsi-close-range-multi-scene-hyperspectral-imaging-benc.html" onclick="toggleFavorite(this, '2505.20507v2', 'Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>47</td>
  <td><a href="./papers/250520255v2-anicrafter-customizing-realistic-human-centric-animation-via-avatar-.html">AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models</a></td>
  <td>æå‡ºAniCrafterä»¥è§£å†³åŠ¨æ€èƒŒæ™¯ä¸‹äººç±»åŠ¨ç”»çš„å±€é™æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">SMPL</span> <span class="paper-tag">SMPL-X</span> <span class="paper-tag">character animation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20255v2" data-paper-url="./papers/250520255v2-anicrafter-customizing-realistic-human-centric-animation-via-avatar-.html" onclick="toggleFavorite(this, '2505.20255v2', 'AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>48</td>
  <td><a href="./papers/250519985v2-structured-initialization-for-vision-transformers.html">Structured Initialization for Vision Transformers</a></td>
  <td>æå‡ºç»“æ„åŒ–åˆå§‹åŒ–æ–¹æ³•ä»¥æå‡è§†è§‰å˜æ¢å™¨æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">PULSE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19985v2" data-paper-url="./papers/250519985v2-structured-initialization-for-vision-transformers.html" onclick="toggleFavorite(this, '2505.19985v2', 'Structured Initialization for Vision Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)