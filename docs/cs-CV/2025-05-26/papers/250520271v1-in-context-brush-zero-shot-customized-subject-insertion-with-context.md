---
layout: default
title: In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation
---

# In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20271" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20271v1</a>
  <a href="https://arxiv.org/pdf/2505.20271.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20271v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20271v1', 'In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Xu, Fan Tang, You Wu, Lin Gao, Oliver Deussen, Hongbin Yan, Jintao Li, Juan Cao, Tong-Yee Lee

**åˆ†ç±»**: cs.CV, cs.AI, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIn-Context Brushä»¥è§£å†³å®šåˆ¶åŒ–ä¸»é¢˜æ’å…¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®šåˆ¶ä¸»é¢˜æ’å…¥` `æ‰©æ•£æ¨¡å‹` `å¤šæ¨¡æ€ç”Ÿæˆ` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ½œåœ¨ç©ºé—´æ“ä½œ` `å›¾åƒç¼–è¾‘` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦æ’å…¥å®šåˆ¶ä¸»é¢˜å’Œä¸ç”¨æˆ·æ„å›¾å¯¹é½æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†In-Context Brushæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼å®ç°é›¶-shotå®šåˆ¶ä¸»é¢˜æ’å…¥ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èº«ä»½ä¿ç•™ã€æ–‡æœ¬å¯¹é½å’Œå›¾åƒè´¨é‡ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æå‡äº†å¤šæ¨¡æ€å¼•å¯¼çš„è§†è§‰ç”Ÿæˆèƒ½åŠ›ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡æœ¬æç¤ºå°†å®šåˆ¶åŒ–å¯¹è±¡æ— ç¼æ’å…¥åˆ°ç»™å®šå›¾åƒä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦æ’å…¥å®šåˆ¶ä¸»é¢˜å’Œä¸ç”¨æˆ·æ„å›¾å¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†â€œIn-Context Brushâ€ï¼Œä¸€ä¸ªé›¶-shotæ¡†æ¶ï¼Œé€šè¿‡å°†ä»»åŠ¡é‡æ–°æ„å»ºä¸ºä¸Šä¸‹æ–‡å­¦ä¹ çš„èŒƒå¼ï¼Œæ¥å®ç°å®šåˆ¶ä¸»é¢˜çš„æ’å…¥ã€‚æˆ‘ä»¬åœ¨é¢„è®­ç»ƒçš„MMDiTåŸºç¡€ä¸Šï¼Œé‡‡ç”¨åŒå±‚æ½œåœ¨ç©ºé—´æ“ä½œè¿›è¡Œæµ‹è¯•æ—¶å¢å¼ºï¼Œæ˜¾è‘—æé«˜äº†èº«ä»½ä¿ç•™ã€æ–‡æœ¬å¯¹é½å’Œå›¾åƒè´¨é‡ï¼Œä¸”æ— éœ€ä¸“é—¨è®­ç»ƒæˆ–é¢å¤–æ•°æ®æ”¶é›†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å®šåˆ¶ä¸»é¢˜æ’å…¥æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦å’Œç”¨æˆ·æ„å›¾å¯¹é½æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸è¿›è¡Œæ¨¡å‹è°ƒä¼˜çš„æƒ…å†µä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†å¯¹è±¡å›¾åƒå’Œæ–‡æœ¬æç¤ºè§†ä¸ºè·¨æ¨¡æ€ç¤ºä¾‹ï¼Œå°†ç›®æ ‡å›¾åƒä¸è¢«é®æŒ¡åŒºåŸŸè§†ä¸ºæŸ¥è¯¢ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼è¿›è¡Œå®šåˆ¶ä¸»é¢˜æ’å…¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŸºäºé¢„è®­ç»ƒçš„MMDiTç½‘ç»œï¼Œé‡‡ç”¨åŒå±‚æ½œåœ¨ç©ºé—´æ“ä½œï¼ŒåŒ…æ‹¬å†…éƒ¨æ³¨æ„åŠ›å¤´çš„æ½œåœ¨ç‰¹å¾ç§»åŠ¨å’Œä¸åŒæ³¨æ„åŠ›å¤´ä¹‹é—´çš„æ³¨æ„åŠ›é‡åŠ æƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›è¾“å‡ºå’Œå·®å¼‚åŒ–çš„æ³¨æ„åŠ›ä¼˜å…ˆçº§ï¼Œå¢å¼ºäº†å¯¹æ–‡æœ¬æç¤ºçš„æ§åˆ¶èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†æ’å…¥æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬æ½œåœ¨ç‰¹å¾ç§»åŠ¨çš„å…·ä½“å®ç°ã€æ³¨æ„åŠ›é‡åŠ æƒçš„ç­–ç•¥ï¼Œä»¥åŠåœ¨æµ‹è¯•æ—¶å¦‚ä½•è¿›è¡Œå¢å¼ºè€Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–æ•°æ®æ”¶é›†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIn-Context Brushåœ¨èº«ä»½ä¿ç•™ã€æ–‡æœ¬å¯¹é½å’Œå›¾åƒè´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æ•°æ®æ”¶é›†ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å®ç”¨æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒç¼–è¾‘ã€è™šæ‹Ÿç°å®å’Œæ¸¸æˆå¼€å‘ç­‰ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´ä¸ºçµæ´»å’Œä¸ªæ€§åŒ–çš„è§†è§‰å†…å®¹ç”Ÿæˆä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ›´å¤šåŸºäºç”¨æˆ·æ„å›¾çš„è‡ªåŠ¨åŒ–å›¾åƒå¤„ç†å·¥å…·çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in diffusion models have enhanced multimodal-guided visual generation, enabling customized subject insertion that seamlessly "brushes" user-specified objects into a given image guided by textual prompts. However, existing methods often struggle to insert customized subjects with high fidelity and align results with the user's intent through textual prompts. In this work, we propose "In-Context Brush", a zero-shot framework for customized subject insertion by reformulating the task within the paradigm of in-context learning. Without loss of generality, we formulate the object image and the textual prompts as cross-modal demonstrations, and the target image with the masked region as the query. The goal is to inpaint the target image with the subject aligning textual prompts without model tuning. Building upon a pretrained MMDiT-based inpainting network, we perform test-time enhancement via dual-level latent space manipulation: intra-head "latent feature shifting" within each attention head that dynamically shifts attention outputs to reflect the desired subject semantics and inter-head "attention reweighting" across different heads that amplifies prompt controllability through differential attention prioritization. Extensive experiments and applications demonstrate that our approach achieves superior identity preservation, text alignment, and image quality compared to existing state-of-the-art methods, without requiring dedicated training or additional data collection.

