---
layout: default
title: Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models
---

# Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20021" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20021v1</a>
  <a href="https://arxiv.org/pdf/2505.20021.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20021v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20021v1', 'Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hyunsik Chae, Seungwoo Yoon, Jaden Park, Chloe Yewon Chun, Yongin Cho, Mu Cai, Yong Jae Lee, Ernest K. Ryu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: 69 pages, 16 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸå­è§†è§‰æŠ€èƒ½ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºæœ¬ä»»åŠ¡æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `åŸå­è§†è§‰æŠ€èƒ½` `å¤šæ¨¡æ€ç†è§£` `æ•°æ®é›†æ„å»º` `å‡ ä½•æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç®€å•è§†è§‰ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨åŸºæœ¬çš„å‡ ä½•ç†è§£ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ–¹æ³•ï¼Œå°†è§†è§‰æ„ŸçŸ¥æŠ€èƒ½ç»†åˆ†ä¸ºä¸å¯åˆ†å‰²çš„åŸå­è§†è§‰æŠ€èƒ½ï¼Œå¹¶æ„å»ºäº†ç›¸åº”çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„VLMsåœ¨åŸå­è§†è§‰æŠ€èƒ½ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºè¿™äº›æ¨¡å‹åœ¨åŸºç¡€è§†è§‰ç†è§£æ–¹é¢çš„å±€é™æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸€äº›ç®€å•çš„è§†è§‰ä»»åŠ¡ä¸Šå´å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡èšç„¦äºåŸºæœ¬çš„äºŒç»´æ¬§å‡ é‡Œå¾—å‡ ä½•ï¼Œç³»ç»Ÿåœ°å¯¹åŸºæœ¬çš„ã€ä¸å¯åˆ†å‰²çš„è§†è§‰æ„ŸçŸ¥æŠ€èƒ½è¿›è¡Œäº†åˆ†ç±»ï¼Œç§°ä¹‹ä¸ºåŸå­è§†è§‰æŠ€èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸå­è§†è§‰æŠ€èƒ½æ•°æ®é›†ï¼ˆAVSDï¼‰ï¼Œç”¨äºè¯„ä¼°VLMsåœ¨è¿™äº›æŠ€èƒ½ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡ä½¿ç”¨AVSDï¼Œæˆ‘ä»¬å¯¹å½“å‰æœ€å…ˆè¿›çš„VLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å®ƒä»¬åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå°½ç®¡å¯¹äºæˆäººè€Œè¨€è¿™äº›ä»»åŠ¡æ˜¯å¾®ä¸è¶³é“çš„ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ä¸ºè®­ç»ƒå’Œè¯„ä¼°VLMsè€Œä¸“é—¨æ„å»ºæ•°æ®é›†çš„å¿…è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸå­è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸºæœ¬è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç®€å•çš„äºŒç»´å‡ ä½•ä»»åŠ¡ä¸Šã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†è¿™äº›åŸå­çº§åˆ«çš„è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç³»ç»Ÿåœ°å°†è§†è§‰æ„ŸçŸ¥æŠ€èƒ½åˆ†è§£ä¸ºåŸå­è§†è§‰æŠ€èƒ½ï¼Œå¹¶é€šè¿‡æ„å»ºä¸“é—¨çš„æ•°æ®é›†æ¥è¯„ä¼°å’Œè®­ç»ƒVLMsã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æ¨¡å‹åœ¨åŸºæœ¬è§†è§‰ä»»åŠ¡ä¸­çš„ä¸è¶³ä¹‹å¤„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åŸå­è§†è§‰æŠ€èƒ½çš„å®šä¹‰ã€æ•°æ®é›†çš„æ„å»ºä»¥åŠåŸºäºè¯¥æ•°æ®é›†çš„æ¨¡å‹è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æŠ€èƒ½åˆ†ç±»ã€æ•°æ®é›†è®¾è®¡å’Œæ¨¡å‹åŸºå‡†æµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†åŸå­è§†è§‰æŠ€èƒ½çš„æ¦‚å¿µï¼Œå¹¶æ„å»ºäº†åŸå­è§†è§‰æŠ€èƒ½æ•°æ®é›†ï¼ˆAVSDï¼‰ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç»¼åˆè§†è§‰ä»»åŠ¡è¯„ä¼°å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ ·åŒ–çš„è§†è§‰ä»»åŠ¡å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿å¯¹VLMsçš„å…¨é¢è¯„ä¼°ã€‚åŒæ—¶ï¼Œæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹é‡‡ç”¨äº†é’ˆå¯¹åŸå­è§†è§‰æŠ€èƒ½çš„ç‰¹å®šæŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸå­è§†è§‰æŠ€èƒ½ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä½äºäººç±»æ°´å¹³ï¼Œå…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æ™®éä½äº50%ï¼Œè€Œäººç±»åˆ™èƒ½å¤Ÿè½»æ¾å®Œæˆè¿™äº›ä»»åŠ¡ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†é’ˆå¯¹åŸå­è§†è§‰æŠ€èƒ½çš„è®­ç»ƒå’Œè¯„ä¼°çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æœºå™¨äººè§†è§‰å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸºæœ¬è§†è§‰ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®è§†è§‰ç†è§£çš„åœºæ™¯ä¸­ã€‚æœªæ¥ï¼Œéšç€æ•°æ®é›†çš„ä¸æ–­å®Œå–„ï¼Œå¯èƒ½ä¼šæ¨åŠ¨æ›´é«˜çº§åˆ«çš„è§†è§‰æ¨ç†å’Œç†è§£èƒ½åŠ›çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal comprehension and reasoning capabilities, yet they often struggle with trivially simple visual tasks. In this work, we focus on the domain of basic 2D Euclidean geometry and systematically categorize the fundamental, indivisible visual perception skills, which we refer to as atomic visual skills. We then introduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the atomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find that they struggle with these tasks, despite being trivial for adult humans. Our findings highlight the need for purpose-built datasets to train and evaluate VLMs on atomic, rather than composite, visual perception tasks.

