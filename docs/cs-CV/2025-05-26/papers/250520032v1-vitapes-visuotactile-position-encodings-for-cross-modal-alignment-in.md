---
layout: default
title: "ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers"
---

# ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20032" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.20032v1</a>
  <a href="https://arxiv.org/pdf/2505.20032.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20032v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20032v1', 'ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Fotios Lygerakis, Ozan √ñzdenizci, Elmar R√ºckert

**ÂàÜÁ±ª**: cs.CV, cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-26

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ViTaPEs‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂØπÈΩêÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅËûçÂêà` `ËßÜËßâËß¶ËßâÊÑüÁü•` `ÂèòÊç¢Âô®Êû∂ÊûÑ` `‰ΩçÁΩÆÁºñÁ†Å` `Êú∫Âô®‰∫∫ÊäìÂèñ` `ËøÅÁßªÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ËßÜËßâ‰∏éËß¶ËßâÊ®°ÊÄÅÁöÑËûçÂêàÂíåË∑®‰ªªÂä°Ê≥õÂåñÊñπÈù¢Â≠òÂú®ÊåëÊàòÔºå‰∏îÂØπÈ¢ÑËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰æùËµñËæÉÈáç„ÄÇ
2. ViTaPEsÈÄöËøáÂºïÂÖ•Êñ∞È¢ñÁöÑÂ§öÂ∞∫Â∫¶‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ°àÔºåÁ®≥ÂÅ•Âú∞Êï¥ÂêàËßÜËßâÂíåËß¶ËßâÊï∞ÊçÆÔºåÂ≠¶‰π†‰ªªÂä°Êó†ÂÖ≥ÁöÑË°®Á§∫„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåViTaPEsÂú®Â§ö‰∏™ËØÜÂà´‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÂü∫Á∫øÔºåÂπ∂Âú®Êú∫Âô®‰∫∫ÊäìÂèñ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëß¶ËßâÊÑüÁü•Êèê‰æõ‰∫Ü‰∏éËßÜËßâÊÑüÁü•‰∫íË°•ÁöÑÂ±ÄÈÉ®ÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂ¶ÇÁ∫πÁêÜ„ÄÅÈ°∫Â∫îÊÄßÂíåÂäõ„ÄÇÂ∞ΩÁÆ°Âú®ËßÜËßâËß¶ËßâË°®Á§∫Â≠¶‰π†ÊñπÈù¢ÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÂú®ËûçÂêàËøô‰∫õÊ®°ÊÄÅÂíåÂú®‰∏çÂêå‰ªªÂä°ÂèäÁéØÂ¢É‰∏≠ËøõË°åÊ≥õÂåñÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÊú™Á†îÁ©∂‰ΩçÁΩÆÁºñÁ†ÅÔºåÂøΩËßÜ‰∫ÜÊçïÊçâÁªÜÁ≤íÂ∫¶ËßÜËßâËß¶ËßâÂÖ≥ËÅîÊâÄÈúÄÁöÑÂ§öÂ∞∫Â∫¶Á©∫Èó¥Êé®ÁêÜ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜViTaPEsÔºå‰∏Ä‰∏™Âü∫‰∫éÂèòÊç¢Âô®ÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§üÁ®≥ÂÅ•Âú∞Êï¥ÂêàËßÜËßâÂíåËß¶ËßâËæìÂÖ•Êï∞ÊçÆÔºå‰ª•Â≠¶‰π†‰ªªÂä°Êó†ÂÖ≥ÁöÑËßÜËßâËß¶ËßâÊÑüÁü•Ë°®Á§∫„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ§öÂ∞∫Â∫¶‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ°àÊù•ÊçïÊçâÊ®°ÊÄÅÂÜÖÁªìÊûÑÔºåÂêåÊó∂Âª∫Ê®°Ë∑®Ê®°ÊÄÅÁ∫øÁ¥¢„ÄÇ‰∏é‰ª•ÂæÄÂ∑•‰Ωú‰∏çÂêåÔºåÊàë‰ª¨Êèê‰æõ‰∫ÜËßÜËßâËß¶ËßâËûçÂêàÁöÑÂèØËØÅÊòé‰øùËØÅÔºåÊòæÁ§∫Êàë‰ª¨ÁöÑÁºñÁ†ÅÊòØÂçïÂ∞Ñ„ÄÅÂàö‰ΩìËøêÂä®Á≠âÂèò‰∏î‰ø°ÊÅØ‰øùÊåÅÁöÑÔºåÂπ∂ÈÄöËøáÂÆûÈ™åËØÅÂÆû‰∫ÜËøô‰∫õÁâπÊÄß„ÄÇÂú®Â§ö‰∏™Â§ßËßÑÊ®°ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåViTaPEs‰∏ç‰ªÖÂú®ÂêÑÁßçËØÜÂà´‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÂü∫Á∫øÔºåËøòÂ±ïÁ§∫‰∫ÜÂØπÊú™ËßÅÈ¢ÜÂüüÂú∫ÊôØÁöÑÈõ∂-shotÊ≥õÂåñËÉΩÂäõ„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•Â±ïÁ§∫‰∫ÜViTaPEsÂú®Êú∫Âô®‰∫∫ÊäìÂèñ‰ªªÂä°‰∏≠ÁöÑËøÅÁßªÂ≠¶‰π†ËÉΩÂäõÔºåÂÖ∂Âú®È¢ÑÊµãÊäìÂèñÊàêÂäüÁéáÊñπÈù¢‰ºò‰∫éÊúÄÂÖàËøõÁöÑÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâ‰∏éËß¶ËßâÊ®°ÊÄÅËûçÂêà‰∏≠ÁöÑÂ§öÂ∞∫Â∫¶Á©∫Èó¥Êé®ÁêÜÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑËßÜËßâËß¶ËßâÂÖ≥ËÅîÔºå‰∏îÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑ‰æùËµñÊÄßËæÉÂº∫„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöViTaPEsÈÄöËøáÂºïÂÖ•Â§öÂ∞∫Â∫¶‰ΩçÁΩÆÁºñÁ†ÅÔºåÂ¢ûÂº∫‰∫ÜÂØπÊ®°ÊÄÅÂÜÖÁªìÊûÑÁöÑÊçïÊçâËÉΩÂäõÔºåÂêåÊó∂Âª∫Ê®°Ë∑®Ê®°ÊÄÅ‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑËßÜËßâËß¶ËßâËûçÂêà„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂Âü∫‰∫éÂèòÊç¢Âô®Êû∂ÊûÑÔºå‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨ËßÜËßâËæìÂÖ•Â§ÑÁêÜ„ÄÅËß¶ËßâËæìÂÖ•Â§ÑÁêÜÂíåÂ§öÂ∞∫Â∫¶‰ΩçÁΩÆÁºñÁ†ÅÊ®°ÂùóÔºåÊúÄÁªàËæìÂá∫‰ªªÂä°Êó†ÂÖ≥ÁöÑËßÜËßâËß¶ËßâË°®Á§∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöViTaPEsÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂‰ΩçÁΩÆÁºñÁ†ÅÁöÑËÆæËÆ°ÔºåÁ°Æ‰øù‰∫ÜÁºñÁ†ÅÁöÑÂçïÂ∞ÑÊÄß„ÄÅÂàö‰ΩìËøêÂä®Á≠âÂèòÊÄßÂíå‰ø°ÊÅØ‰øùÊåÅÊÄßÔºåËøô‰∫õÁâπÊÄßÂú®‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏≠Êú™ÂæóÂà∞ÂÖÖÂàÜÊé¢ËÆ®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁΩëÁªúÁªìÊûÑ‰∏äÔºåViTaPEsÈááÁî®‰∫ÜÂ§öÂ±ÇÂèòÊç¢Âô®Êû∂ÊûÑÔºåÁªìÂêà‰∫ÜËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Âíå‰ΩçÁΩÆÁºñÁ†ÅÔºåÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÂàôÊ≥®Èáç‰∫é‰øùÊåÅÊ®°ÊÄÅÈó¥‰ø°ÊÅØÁöÑ‰∏ÄËá¥ÊÄßÂíåÂÆåÊï¥ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Â§ßËßÑÊ®°ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåViTaPEsÂú®ÂêÑÁßçËØÜÂà´‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÊúÄÂÖàËøõÁöÑÂü∫Á∫øÔºåÂÖ∑‰ΩìÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%„ÄÇÊ≠§Â§ñÔºåÂú®Êú∫Âô®‰∫∫ÊäìÂèñ‰ªªÂä°‰∏≠ÔºåViTaPEsÂú®È¢ÑÊµãÊäìÂèñÊàêÂäüÁéáÊñπÈù¢‰πüË°®Áé∞‰ºòÂºÇÔºåËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÂÖ∂ËøÅÁßªÂ≠¶‰π†ËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊäìÂèñ„ÄÅÊô∫ËÉΩ‰∫∫Êú∫‰∫§‰∫íÂíåÂ§öÊ®°ÊÄÅÊÑüÁü•Á≥ªÁªüÁ≠â„ÄÇÈÄöËøáÊúâÊïàËûçÂêàËßÜËßâ‰∏éËß¶Ëßâ‰ø°ÊÅØÔºåViTaPEsËÉΩÂ§üÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúËÉΩÂäõÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Tactile sensing provides local essential information that is complementary to visual perception, such as texture, compliance, and force. Despite recent advances in visuotactile representation learning, challenges remain in fusing these modalities and generalizing across tasks and environments without heavy reliance on pre-trained vision-language models. Moreover, existing methods do not study positional encodings, thereby overlooking the multi-scale spatial reasoning needed to capture fine-grained visuotactile correlations. We introduce ViTaPEs, a transformer-based framework that robustly integrates visual and tactile input data to learn task-agnostic representations for visuotactile perception. Our approach exploits a novel multi-scale positional encoding scheme to capture intra-modal structures, while simultaneously modeling cross-modal cues. Unlike prior work, we provide provable guarantees in visuotactile fusion, showing that our encodings are injective, rigid-motion-equivariant, and information-preserving, validating these properties empirically. Experiments on multiple large-scale real-world datasets show that ViTaPEs not only surpasses state-of-the-art baselines across various recognition tasks but also demonstrates zero-shot generalization to unseen, out-of-domain scenarios. We further demonstrate the transfer-learning strength of ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art baselines in predicting grasp success. Project page: https://sites.google.com/view/vitapes

