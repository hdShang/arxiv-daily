---
layout: default
title: "Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM"
---

# Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19901" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19901v3</a>
  <a href="https://arxiv.org/pdf/2505.19901.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19901v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19901v3', 'Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peng Liu, Xiaoming Ren, Fengkai Liu, Qingsong Xie, Quanlong Zheng, Yanhao Zhang, Haonan Lu, Yujiu Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-06-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDynamic-I2Vä»¥è§£å†³å¤æ‚åœºæ™¯ä¸‹å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ‰©æ•£å˜æ¢å™¨` `åŠ¨æ€è§†é¢‘è¯„ä¼°` `è¿åŠ¨å¯æ§æ€§` `æ—¶é—´ä¸€è‡´æ€§` `ç”Ÿæˆæ¨¡å‹` `è§†é¢‘è´¨é‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆç†è§£ç»†å¾®çš„è¿åŠ¨å’Œå¤æ‚çš„ç‰©ä½“-åŠ¨ä½œå…³ç³»ã€‚
2. Dynamic-I2Væ¡†æ¶é€šè¿‡æ•´åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¢å¼ºäº†å¯¹è§†è§‰å’Œæ–‡æœ¬æ¡ä»¶çš„ç¼–ç èƒ½åŠ›ï¼Œä»è€Œæ”¹å–„äº†ç”Ÿæˆè§†é¢‘çš„è¿åŠ¨å¯æ§æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDynamic-I2Våœ¨åŠ¨æ€èŒƒå›´ã€å¯æ§æ€§å’Œè§†é¢‘è´¨é‡ä¸Šåˆ†åˆ«æå‡äº†42.5%ã€7.9%å’Œ11.8%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆçš„è¿›å±•åœ¨å¸¸è§„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£ç»†å¾®è¿åŠ¨å’Œå¤æ‚ç‰©ä½“-åŠ¨ä½œå…³ç³»æ–¹é¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Dynamic-I2Væ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…±åŒç¼–ç è§†è§‰å’Œæ–‡æœ¬æ¡ä»¶ï¼Œä»¥å¢å¼ºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¶æ„çš„æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨MLLMsçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ¨¡å‹æ˜¾è‘—æé«˜äº†åˆæˆè§†é¢‘çš„è¿åŠ¨å¯æ§æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒDynamic-I2Vçš„å¤šæ¨¡æ€ç‰¹æ€§æ”¯æŒå¤šæ ·çš„æ¡ä»¶è¾“å…¥ï¼Œæ‰©å±•äº†å…¶åœ¨ä¸‹æ¸¸ç”Ÿæˆä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†DIVEè¯„ä¼°åŸºå‡†ï¼Œä»¥è§£å†³ç°æœ‰I2VåŸºå‡†åœ¨åŠ¨æ€è§†é¢‘è¯„ä¼°ä¸­çš„åå·®é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynamic-I2Våœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŠ¨æ€èŒƒå›´ã€å¯æ§æ€§å’Œè´¨é‡åˆ†åˆ«æå‡42.5%ã€7.9%å’Œ11.8%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å¯¹ç»†å¾®è¿åŠ¨å’Œå¤æ‚ç‰©ä½“-åŠ¨ä½œå…³ç³»çš„ç†è§£èƒ½åŠ›ä¸è¶³ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨åå‘ä½åŠ¨æ€è§†é¢‘çš„é—®é¢˜ï¼Œå¯¼è‡´è¯„ä¼°ä¸å…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDynamic-I2Væ¡†æ¶é€šè¿‡å¼•å…¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå®ç°è§†è§‰å’Œæ–‡æœ¬æ¡ä»¶çš„è”åˆç¼–ç ï¼Œä»è€Œæå‡ç”Ÿæˆè§†é¢‘çš„è¿åŠ¨å¯æ§æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚è¿™ç§è®¾è®¡åˆ©ç”¨äº†MLLMsåœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDynamic-I2Vçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆæ˜¯è§†è§‰å’Œæ–‡æœ¬æ¡ä»¶çš„è¾“å…¥æ¨¡å—ï¼Œç„¶åæ˜¯åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„ç”Ÿæˆæ¨¡å—ï¼Œæœ€åæ˜¯è¾“å‡ºè§†é¢‘çš„åå¤„ç†æ¨¡å—ã€‚å„æ¨¡å—ååŒå·¥ä½œï¼Œç¡®ä¿ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»ŸI2Væ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„æ·±åº¦èåˆå’Œç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è¿åŠ¨å¤æ‚æ€§å’Œè§†è§‰è´¨é‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”å¤šæ¨¡æ€è¾“å…¥çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDynamic-I2Våœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€èŒƒå›´ã€å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢ï¼Œåˆ†åˆ«æå‡äº†42.5%ã€7.9%å’Œ11.8%ã€‚è¿™äº›ç»“æœé€šè¿‡æ–°çš„DIVEè¯„ä¼°åŸºå‡†å¾—ä»¥éªŒè¯ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç”ŸæˆåŠ¨æ€è§†é¢‘æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Dynamic-I2Vçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬ç”µå½±åˆ¶ä½œã€æ¸¸æˆå¼€å‘ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€è§†é¢‘ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿä¸ºåˆ›ä½œè€…æä¾›æ›´ä¸°å¯Œçš„å†…å®¹ç”Ÿæˆå·¥å…·ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€å¹¿å‘Šç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨å¤šåª’ä½“å†…å®¹çš„åˆ›æ–°ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in image-to-video (I2V) generation have shown promising performance in conventional scenarios. However, these methods still encounter significant challenges when dealing with complex scenes that require a deep understanding of nuanced motion and intricate object-action relationships. To address these challenges, we present Dynamic-I2V, an innovative framework that integrates Multimodal Large Language Models (MLLMs) to jointly encode visual and textual conditions for a diffusion transformer (DiT) architecture. By leveraging the advanced multimodal understanding capabilities of MLLMs, our model significantly improves motion controllability and temporal coherence in synthesized videos. The inherent multimodality of Dynamic-I2V further enables flexible support for diverse conditional inputs, extending its applicability to various downstream generation tasks. Through systematic analysis, we identify a critical limitation in current I2V benchmarks: a significant bias towards favoring low-dynamic videos, stemming from an inadequate balance between motion complexity and visual quality metrics. To resolve this evaluation gap, we propose DIVE - a novel assessment benchmark specifically designed for comprehensive dynamic quality measurement in I2V generation. In conclusion, extensive quantitative and qualitative experiments confirm that Dynamic-I2V attains state-of-the-art performance in image-to-video generation, particularly revealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range, controllability, and quality, respectively, as assessed by the DIVE metric in comparison to existing methods.

