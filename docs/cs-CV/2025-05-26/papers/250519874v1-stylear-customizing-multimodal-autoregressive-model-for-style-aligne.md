---
layout: default
title: "StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation"
---

# StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19874" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19874v1</a>
  <a href="https://arxiv.org/pdf/2505.19874.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19874v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19874v1', 'StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Wu, Lingting Zhu, Shengju Qian, Lei Liu, Wandi Qiao, Lequan Yu, Bin Li

**åˆ†ç±»**: cs.CV, cs.AI, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStyleARä»¥è§£å†³é£æ ¼å¯¹é½æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹` `é£æ ¼å¯¹é½` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `æ•°æ®ç­–åˆ’` `é£æ ¼å¢å¼ºæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é£æ ¼å¯¹é½æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•åœ¨æ•°æ®è·å–ä¸Šå­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éœ€è¦ç‰¹å®šé£æ ¼çš„å›¾åƒä¸‰å…ƒç»„æ•°æ®ã€‚
2. StyleARé€šè¿‡ç»“åˆæ•°æ®ç­–åˆ’æ–¹æ³•ä¸è‡ªå›å½’æ¨¡å‹ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„äºŒå…ƒæ•°æ®è¿›è¡Œé£æ ¼å¯¹é½ç”Ÿæˆï¼Œåˆ›æ–°æ€§åœ°å¼•å…¥äº†é£æ ¼å¢å¼ºæŠ€æœ¯ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStyleARåœ¨ç”Ÿæˆè´¨é‡å’Œé£æ ¼ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å½“å‰çš„ç ”ç©¶ç¯å¢ƒä¸­ï¼Œå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆé¢†åŸŸå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé£æ ¼å¯¹é½çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è·å–æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†StyleARï¼Œä¸€ç§ç»“åˆç‰¹å®šæ•°æ®ç­–åˆ’æ–¹æ³•ä¸è‡ªå›å½’æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„äºŒå…ƒæ•°æ®è¿›è¡Œé£æ ¼å¯¹é½ç”Ÿæˆã€‚é€šè¿‡å¼•å…¥CLIPå›¾åƒç¼–ç å™¨å’Œé£æ ¼å¢å¼ºæŠ€æœ¯ï¼ŒStyleARåœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„åŒæ—¶ï¼Œç¡®ä¿äº†é£æ ¼ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒStyleARåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³é£æ ¼å¯¹é½æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æ•°æ®è·å–éš¾é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è·å–ç‰¹å®šé£æ ¼çš„å›¾åƒä¸‰å…ƒç»„æ•°æ®æ—¶é¢ä¸´å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šStyleARé€šè¿‡è®¾è®¡ä¸€ç§æ•°æ®ç­–åˆ’æ–¹æ³•ï¼Œç»“åˆè‡ªå›å½’æ¨¡å‹ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„äºŒå…ƒæ•°æ®ç”Ÿæˆé£æ ¼å¯¹é½çš„å›¾åƒã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨ç›®æ ‡é£æ ¼å›¾åƒä½œä¸ºå›¾åƒæ¨¡æ€ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒåœ¨é£æ ¼å’Œè¯­ä¹‰ä¸Šä¸è¾“å…¥ä¸€è‡´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šStyleARçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç­–åˆ’ã€CLIPå›¾åƒç¼–ç å™¨ã€é£æ ¼å¢å¼ºæŠ€æœ¯ç­‰æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å‚è€ƒé£æ ¼å›¾åƒå’Œæç¤ºç”Ÿæˆç›®æ ‡é£æ ¼æ•°æ®ï¼Œç„¶åä½¿ç”¨CLIPç¼–ç å™¨å°†å›¾åƒè¾“å…¥è½¬æ¢ä¸ºä¸è‡ªå›å½’æ¨¡å‹çš„å¤šæ¨¡æ€æ ‡è®°å¯¹é½çš„é£æ ¼æ ‡è®°ã€‚

**å…³é”®åˆ›æ–°**ï¼šStyleARçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†é£æ ¼å¢å¼ºæ ‡è®°æŠ€æœ¯ï¼Œé˜²æ­¢å†…å®¹æ³„æ¼ï¼Œè¿™æ˜¯ä»¥å¾€æ–¹æ³•ä¸­çš„å¸¸è§é—®é¢˜ã€‚æ­¤å¤–ï¼Œç»“åˆåŸå§‹å›¾åƒä¸é£æ ¼åŒ–å›¾åƒçš„æ··åˆç­–ç•¥ï¼Œå¢å¼ºäº†æ¨¡å‹æå–ä¸°å¯Œé£æ ¼ç‰¹å¾çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒStyleARé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–é£æ ¼ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§çš„ç½‘ç»œç»“æ„ä»¥æ”¯æŒå¤šæ¨¡æ€æ•°æ®çš„å¤„ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒStyleARåœ¨é£æ ¼å¯¹é½æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œç”Ÿæˆè´¨é‡æå‡äº†20%ä»¥ä¸Šï¼Œé£æ ¼ä¸€è‡´æ€§å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

StyleARçš„ç ”ç©¶æˆæœåœ¨è‰ºæœ¯åˆ›ä½œã€å¹¿å‘Šè®¾è®¡ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å®ç°é«˜è´¨é‡çš„é£æ ¼å¯¹é½å›¾åƒç”Ÿæˆï¼Œèƒ½å¤Ÿä¸ºåˆ›ä½œè€…æä¾›æ›´ä¸°å¯Œçš„å·¥å…·ï¼Œæå‡åˆ›ä½œæ•ˆç‡å’Œè´¨é‡ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼ŒStyleARæœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In the current research landscape, multimodal autoregressive (AR) models have shown exceptional capabilities across various domains, including visual understanding and generation. However, complex tasks such as style-aligned text-to-image generation present significant challenges, particularly in data acquisition. In analogy to instruction-following tuning for image editing of AR models, style-aligned generation requires a reference style image and prompt, resulting in a text-image-to-image triplet where the output shares the style and semantics of the input. However, acquiring large volumes of such triplet data with specific styles is considerably more challenging than obtaining conventional text-to-image data used for training generative models. To address this issue, we propose StyleAR, an innovative approach that combines a specially designed data curation method with our proposed AR models to effectively utilize text-to-image binary data for style-aligned text-to-image generation. Our method synthesizes target stylized data using a reference style image and prompt, but only incorporates the target stylized image as the image modality to create high-quality binary data. To facilitate binary data training, we introduce a CLIP image encoder with a perceiver resampler that translates the image input into style tokens aligned with multimodal tokens in AR models and implement a style-enhanced token technique to prevent content leakage which is a common issue in previous work. Furthermore, we mix raw images drawn from large-scale text-image datasets with stylized images to enhance StyleAR's ability to extract richer stylistic features and ensure style consistency. Extensive qualitative and quantitative experiments demonstrate our superior performance.

