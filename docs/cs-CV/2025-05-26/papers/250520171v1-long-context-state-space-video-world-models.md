---
layout: default
title: Long-Context State-Space Video World Models
---

# Long-Context State-Space Video World Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20171" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20171v1</a>
  <a href="https://arxiv.org/pdf/2505.20171.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20171v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20171v1', 'Long-Context State-Space Video World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, Xun Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: Project website: https://ryanpo.com/ssm_wm

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé•¿ä¸Šä¸‹æ–‡çŠ¶æ€ç©ºé—´è§†é¢‘ä¸–ç•Œæ¨¡å‹ä»¥è§£å†³é•¿æ—¶è®°å¿†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `çŠ¶æ€ç©ºé—´æ¨¡å‹` `å› æœåºåˆ—å»ºæ¨¡` `é•¿æœŸè®°å¿†` `è§†é¢‘æ‰©æ•£æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨é•¿åºåˆ—å¤„ç†æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬ï¼Œå¯¼è‡´é•¿æœŸè®°å¿†ä¿æŒèƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºåˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰æ¥æ‰©å±•æ—¶é—´è®°å¿†ï¼Œé‡‡ç”¨å—çŠ¶SSMæ‰«ææ–¹æ¡ˆä¸å¯†é›†å±€éƒ¨æ³¨æ„åŠ›ç»“åˆã€‚
3. åœ¨Memory Mazeå’ŒMinecraftæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨é•¿è·ç¦»è®°å¿†ä¿æŒä¸Šè¶…è¶ŠåŸºçº¿ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦é€‚åˆäº¤äº’åº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨åŸºäºåŠ¨ä½œçš„è‡ªå›å½’å¸§é¢„æµ‹ä¸­å±•ç°äº†ä¸–ç•Œå»ºæ¨¡çš„æ½œåŠ›ï¼Œä½†åœ¨å¤„ç†é•¿åºåˆ—æ—¶ç”±äºè®¡ç®—æˆæœ¬é«˜è€Œéš¾ä»¥ç»´æŒé•¿æœŸè®°å¿†ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¶æ„ï¼Œåˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡çš„æƒ…å†µä¸‹æ‰©å±•æ—¶é—´è®°å¿†ã€‚ä¸ä»¥å¾€å°†SSMsæ”¹é€ ä¸ºéå› æœè§†è§‰ä»»åŠ¡çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å……åˆ†åˆ©ç”¨äº†SSMsåœ¨å› æœåºåˆ—å»ºæ¨¡ä¸­çš„å›ºæœ‰ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„è®¾è®¡æ ¸å¿ƒæ˜¯å—çŠ¶SSMæ‰«ææ–¹æ¡ˆï¼Œæˆ˜ç•¥æ€§åœ°åœ¨ç©ºé—´ä¸€è‡´æ€§ä¸æ‰©å±•æ—¶é—´è®°å¿†ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¹¶ç»“åˆå¯†é›†å±€éƒ¨æ³¨æ„åŠ›ä»¥ç¡®ä¿è¿ç»­å¸§ä¹‹é—´çš„è¿è´¯æ€§ã€‚é€šè¿‡åœ¨Memory Mazeå’ŒMinecraftæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒé•¿è·ç¦»è®°å¿†çš„åŒæ—¶ï¼Œå±•ç°äº†é€‚åˆäº¤äº’åº”ç”¨çš„å®é™…æ¨ç†é€Ÿåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„é•¿æœŸè®°å¿†ä¿æŒèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬é«˜çš„æƒ…å†µä¸‹éš¾ä»¥æœ‰æ•ˆç»´æŒé•¿æ—¶è®°å¿†ï¼Œå½±å“äº†æ¨¡å‹çš„æ€§èƒ½å’Œåº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„æ–°æ¶æ„ï¼Œå……åˆ†åˆ©ç”¨SSMsåœ¨å› æœåºåˆ—å»ºæ¨¡ä¸­çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å—çŠ¶SSMæ‰«ææ–¹æ¡ˆæ¥æ‰©å±•æ—¶é—´è®°å¿†ï¼ŒåŒæ—¶ç»“åˆå¯†é›†å±€éƒ¨æ³¨æ„åŠ›ä»¥ç¡®ä¿å¸§é—´è¿è´¯æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å—çŠ¶SSMæ‰«ææ¨¡å—å’Œå¯†é›†å±€éƒ¨æ³¨æ„åŠ›æ¨¡å—ã€‚å—çŠ¶SSMæ‰«ææ¨¡å—è´Ÿè´£å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œè€Œå¯†é›†å±€éƒ¨æ³¨æ„åŠ›æ¨¡å—åˆ™ç¡®ä¿è¿ç»­å¸§ä¹‹é—´çš„ç©ºé—´ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†SSMsåº”ç”¨äºå› æœåºåˆ—å»ºæ¨¡ï¼Œå……åˆ†å‘æŒ¥å…¶åœ¨é•¿æœŸè®°å¿†ä¿æŒæ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨é•¿åºåˆ—å¤„ç†ä¸­çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å—çŠ¶æ‰«æç­–ç•¥ä»¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†å¯†é›†å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºå¸§é—´çš„è¿è´¯æ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨Memory Mazeå’ŒMinecraftæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨é•¿è·ç¦»è®°å¿†ä¿æŒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ¨ç†é€Ÿåº¦ä¹Ÿä¿æŒåœ¨é€‚åˆäº¤äº’åº”ç”¨çš„èŒƒå›´å†…ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„å®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ¸¸æˆAIã€æœºå™¨äººå¯¼èˆªå’Œè§†é¢‘åˆ†æç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„é•¿æœŸè®°å¿†èƒ½åŠ›ï¼Œå¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´æ™ºèƒ½çš„å†³ç­–å’Œäº¤äº’ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video diffusion models have recently shown promise for world modeling through autoregressive frame prediction conditioned on actions. However, they struggle to maintain long-term memory due to the high computational cost associated with processing extended sequences in attention layers. To overcome this limitation, we propose a novel architecture leveraging state-space models (SSMs) to extend temporal memory without compromising computational efficiency. Unlike previous approaches that retrofit SSMs for non-causal vision tasks, our method fully exploits the inherent advantages of SSMs in causal sequence modeling. Central to our design is a block-wise SSM scanning scheme, which strategically trades off spatial consistency for extended temporal memory, combined with dense local attention to ensure coherence between consecutive frames. We evaluate the long-term memory capabilities of our model through spatial retrieval and reasoning tasks over extended horizons. Experiments on Memory Maze and Minecraft datasets demonstrate that our approach surpasses baselines in preserving long-range memory, while maintaining practical inference speeds suitable for interactive applications.

