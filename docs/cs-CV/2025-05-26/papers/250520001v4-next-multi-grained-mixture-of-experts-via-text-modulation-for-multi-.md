---
layout: default
title: "NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-Identification"
---

# NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-Identification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20001" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.20001v4</a>
  <a href="https://arxiv.org/pdf/2505.20001.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20001v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20001v4', 'NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-Identification')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shihao Li, Aihua Zheng, Andong Lu, Jin Tang, Jixin Ma

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-26 (Êõ¥Êñ∞: 2025-08-10)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫NEXTÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÁâ©‰ΩìÈáçËØÜÂà´‰∏≠ÁöÑÁªÜÁ≤íÂ∫¶ÁâπÂæÅÂª∫Ê®°ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÈáçËØÜÂà´` `ÊñáÊú¨Ë∞ÉÂà∂` `‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°Âûã` `ÁªÜÁ≤íÂ∫¶ÁâπÂæÅ` `ÁªìÊûÑ‰∏ÄËá¥ÊÄß` `ÁâπÂæÅËÅöÂêà` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÁâ©‰ΩìÈáçËØÜÂà´ÊñπÊ≥ïÂ§ö‰æùËµñÈöêÂºèÁâπÂæÅËûçÂêàÔºåÈöæ‰ª•ÊúâÊïàÂª∫Ê®°ÁªÜÁ≤íÂ∫¶ÁöÑËØÜÂà´Ê®°Âºè„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑNEXTÊ°ÜÊû∂ÈÄöËøáÊñáÊú¨Ë∞ÉÂà∂ÁöÑÂ§öÁ≤íÂ∫¶‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºåËß£ËÄ¶ËØ≠‰πâ‰∏éÁªìÊûÑÁâπÂæÅÁöÑÊçïÊçâÔºåÊèêÂçáËØÜÂà´Á≤æÂ∫¶„ÄÇ
3. Âú®Âõõ‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåNEXTÊ°ÜÊû∂ÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜËØÜÂà´ÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÁâ©‰ΩìÈáçËØÜÂà´ÔºàReIDÔºâÊó®Âú®Ë∑®ÂºÇÊûÑÊ®°ÊÄÅËé∑ÂèñÂáÜÁ°ÆÁöÑË∫´‰ªΩÁâπÂæÅ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ï‰æùËµñÈöêÂºèÁâπÂæÅËûçÂêàÊ®°ÂùóÔºåÈöæ‰ª•Âú®Áé∞ÂÆû‰∏ñÁïåÁöÑÂêÑÁßçÊåëÊàò‰∏ãÂª∫Ê®°ÁªÜÁ≤íÂ∫¶ËØÜÂà´Ê®°Âºè„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ±ûÊÄßÁΩÆ‰ø°Â∫¶ÁöÑÂèØÈù†Â≠óÂπïÁîüÊàêÁÆ°ÈÅìÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑÊú™Áü•ËØÜÂà´ÁéáÔºåÂπ∂ÊèêÈ´ò‰∫ÜÁîüÊàêÊñáÊú¨ÁöÑË¥®Èáè„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑReIDÊ°ÜÊû∂NEXTÔºåÂç≥ÈÄöËøáÊñáÊú¨Ë∞ÉÂà∂ÁöÑÂ§öÁ≤íÂ∫¶‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºåÊó®Âú®Âª∫Ê®°Â§öÊ†∑ÁöÑË∫´‰ªΩÊ®°Âºè„ÄÇÈÄöËøáËß£ËÄ¶ËØÜÂà´ÈóÆÈ¢ò‰∏∫ËØ≠‰πâÂíåÁªìÊûÑ‰∏§‰∏™ÂàÜÊîØÔºåÊàë‰ª¨ÂàÜÂà´ÊçïÊçâÁªÜÁ≤íÂ∫¶Â§ñËßÇÁâπÂæÅÂíåÁ≤óÁ≤íÂ∫¶ÁªìÊûÑÁâπÂæÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Âõõ‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÁâ©‰ΩìÈáçËØÜÂà´‰∏≠ÁöÑÁªÜÁ≤íÂ∫¶ÁâπÂæÅÂª∫Ê®°ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂæÄÂæÄ‰æùËµñÈöêÂºèÁâπÂæÅËûçÂêàÔºåÂØºËá¥Âú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑËØÜÂà´ÊÄßËÉΩ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊàë‰ª¨ÊèêÂá∫NEXTÊ°ÜÊû∂ÔºåÈÄöËøáÊñáÊú¨Ë∞ÉÂà∂ÁöÑÂ§öÁ≤íÂ∫¶‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºåÂàÜÂà´ÊçïÊçâËØ≠‰πâÂíåÁªìÊûÑÁâπÂæÅÔºå‰ª•Â∫îÂØπÂ§öÊ†∑ÂåñÁöÑË∫´‰ªΩÊ®°Âºè„ÄÇËøôÊ†∑ÁöÑËÆæËÆ°‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®Â§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåÊèêÂçáËØÜÂà´Á≤æÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöNEXTÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÊñáÊú¨Ë∞ÉÂà∂ËØ≠‰πâ‰∏ìÂÆ∂ÔºàTMSEÔºâ„ÄÅ‰∏ä‰∏ãÊñáÂÖ±‰∫´ÁªìÊûÑ‰∏ìÂÆ∂ÔºàCSSEÔºâÂíåÂ§öÁ≤íÂ∫¶ÁâπÂæÅËÅöÂêàÔºàMGFAÔºâ„ÄÇTMSEÈÄöËøáÈ´òË¥®ÈáèÂ≠óÂπïË∞ÉÂà∂‰∏ìÂÆ∂‰ª•ÊçïÊçâËØ≠‰πâÁâπÂæÅÔºåCSSEÂàôÂÖ≥Ê≥®Êï¥‰ΩìÁâ©‰ΩìÁªìÊûÑÂπ∂ÈÄöËøáËΩØË∑ØÁî±Êú∫Âà∂‰øùÊåÅË∫´‰ªΩÁªìÊûÑ‰∏ÄËá¥ÊÄßÔºåMGFAÂàôË¥üË¥£Â∞ÜÂ§öÁ≤íÂ∫¶‰∏ìÂÆ∂ÁöÑÁâπÂæÅÊúâÊïàËûçÂêà„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÊñáÊú¨Ë∞ÉÂà∂Êú∫Âà∂ÂíåÂ§öÁ≤íÂ∫¶‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÈöêÂºèÁâπÂæÅËûçÂêàÂΩ¢Êàê‰∫ÜÊú¨Ë¥®Âå∫Âà´ÔºåËÉΩÂ§üÊõ¥Á≤æÁ°ÆÂú∞ÊçïÊçâÁªÜÁ≤íÂ∫¶ÁâπÂæÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÊàë‰ª¨ËÆæÁΩÆ‰∫ÜÈ´òË¥®ÈáèÂ≠óÂπïÁöÑÈöèÊú∫ÈááÊ†∑Êú∫Âà∂ÔºåÈááÁî®‰∫ÜËΩØË∑ØÁî±Êú∫Âà∂Êù•Áª¥Êä§ÁªìÊûÑ‰∏ÄËá¥ÊÄßÔºåÂπ∂Âú®ÁâπÂæÅËÅöÂêàÈò∂ÊÆµÈááÁî®Áªü‰∏ÄÁöÑËûçÂêàÁ≠ñÁï•Ôºå‰ª•Á°Æ‰øùÂ§öÁ≤íÂ∫¶ÁâπÂæÅÁöÑÊúâÊïàÊï¥Âêà„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Âõõ‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNEXTÊ°ÜÊû∂Âú®ËØÜÂà´ÂáÜÁ°ÆÁéá‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÂæÖË°•ÂÖÖÔºâÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂÆâÈò≤Á≥ªÁªüÁ≠âÔºåËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆûÁé∞È´òÊïàÁöÑÁâ©‰ΩìÈáçËØÜÂà´„ÄÇÈÄöËøáÊèêÂçáÂ§öÊ®°ÊÄÅËØÜÂà´ÁöÑÂáÜÁ°ÆÊÄßÔºåÊú™Êù•ÂèØ‰∏∫‰∫∫Êú∫‰∫§‰∫í„ÄÅÊô∫ËÉΩÂüéÂ∏ÇÁ≠âÈ¢ÜÂüüÂ∏¶Êù•Êõ¥Â§ßÁöÑ‰ª∑ÂÄºÂíåÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multi-modal object Re-Identification (ReID) aims to obtain accurate identity features across heterogeneous modalities. However, most existing methods rely on implicit feature fusion modules, making it difficult to model fine-grained recognition patterns under various challenges in real world. Benefiting from the powerful Multi-modal Large Language Models (MLLMs), the object appearances are effectively translated into descriptive captions. In this paper, we propose a reliable caption generation pipeline based on attribute confidence, which significantly reduces the unknown recognition rate of MLLMs and improves the quality of generated text. Additionally, to model diverse identity patterns, we propose a novel ReID framework, named NEXT, the Multi-grained Mixture of Experts via Text-Modulation for Multi-modal Object Re-Identification. Specifically, we decouple the recognition problem into semantic and structural branches to separately capture fine-grained appearance features and coarse-grained structure features. For semantic recognition, we first propose a Text-Modulated Semantic Experts (TMSE), which randomly samples high-quality captions to modulate experts capturing semantic features and mining inter-modality complementary cues. Second, to recognize structure features, we propose a Context-Shared Structure Experts (CSSE), which focuses on the holistic object structure and maintains identity structural consistency via a soft routing mechanism. Finally, we propose a Multi-Grained Features Aggregation (MGFA), which adopts a unified fusion strategy to effectively integrate multi-grained experts into the final identity representations. Extensive experiments on four public datasets demonstrate the effectiveness of our method and show that it significantly outperforms existing state-of-the-art methods.

