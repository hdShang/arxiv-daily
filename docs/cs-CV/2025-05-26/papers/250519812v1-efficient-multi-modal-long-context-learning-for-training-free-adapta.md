---
layout: default
title: Efficient Multi-modal Long Context Learning for Training-free Adaptation
---

# Efficient Multi-modal Long Context Learning for Training-free Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19812" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19812v1</a>
  <a href="https://arxiv.org/pdf/2505.19812.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19812v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19812v1', 'Efficient Multi-modal Long Context Learning for Training-free Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zehong Ma, Shiliang Zhang, Longhui Wei, Qi Tian

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: Accepted to ICML2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Zehong-Ma/EMLoC)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEMLoCä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `é•¿ä¸Šä¸‹æ–‡` `æ¨¡å‹é€‚åº”` `å‹ç¼©æŠ€æœ¯` `å‰ªææŠ€æœ¯` `è®­ç»ƒæ— å…³` `è§†è§‰-è¯­è¨€`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€‚åº”æ–°ä»»åŠ¡çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„å¾®è°ƒï¼Œè®¡ç®—å’Œå†…å­˜å¼€é”€å·¨å¤§ã€‚
2. EMLoCé€šè¿‡å°†ç¤ºä¾‹åµŒå…¥æ¨¡å‹è¾“å…¥ï¼Œç»“åˆåˆ†å—å‹ç¼©å’Œå±‚çº§è‡ªé€‚åº”å‰ªæï¼Œæä¾›äº†ä¸€ç§è®­ç»ƒæ— å…³çš„é€‚åº”æ–¹æ¡ˆã€‚
3. åœ¨å¤šç§è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEMLoCçš„æ€§èƒ½ä¸ä¼ ç»Ÿé•¿ä¸Šä¸‹æ–‡æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€‚åº”æ–°ä»»åŠ¡çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¾®è°ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ— å…³çš„æ›¿ä»£æ–¹æ¡ˆâ€”â€”é«˜æ•ˆå¤šæ¨¡æ€é•¿ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆEMLoCï¼‰ï¼Œé€šè¿‡å°†ç¤ºä¾‹ç›´æ¥åµŒå…¥æ¨¡å‹è¾“å…¥ï¼Œæä¾›äº†ä¸€ç§æ›´é«˜æ•ˆã€çµæ´»å’Œå¯æ‰©å±•çš„ä»»åŠ¡é€‚åº”è§£å†³æ–¹æ¡ˆã€‚EMLoCå¼•å…¥äº†åˆ†å—å‹ç¼©æœºåˆ¶å’Œå±‚çº§è‡ªé€‚åº”å‰ªæï¼Œèƒ½å¤Ÿå°†é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€è¾“å…¥å‹ç¼©ä¸ºç´§å‡‘çš„ä»»åŠ¡ç‰¹å®šè®°å¿†è¡¨ç¤ºã€‚é€šè¿‡åœ¨æ¯ä¸€å±‚è‡ªé€‚åº”å‰ªæä»¤ç‰Œï¼ŒåŸºäºJensen-Shannonæ•£åº¦çº¦æŸï¼Œæ˜¾è‘—é™ä½æ¨ç†å¤æ‚åº¦è€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚EMLoCæ˜¯é¦–ä¸ªå°†å‹ç¼©ä¸å‰ªææŠ€æœ¯æ— ç¼ç»“åˆç”¨äºå¤šæ¨¡æ€é•¿ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¼ ç»Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶çš„é«˜è®¡ç®—å’Œå†…å­˜å¼€é”€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–å¾®è°ƒï¼Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEMLoCé€šè¿‡å°†ç¤ºä¾‹ç›´æ¥åµŒå…¥æ¨¡å‹è¾“å…¥ï¼Œé¿å…äº†å¾®è°ƒè¿‡ç¨‹ï¼ŒåŒæ—¶å¼•å…¥åˆ†å—å‹ç¼©å’Œå±‚çº§è‡ªé€‚åº”å‰ªæï¼Œä»¥å‡å°‘é•¿ä¸Šä¸‹æ–‡è¾“å…¥çš„å¤æ‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEMLoCçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥ç¤ºä¾‹çš„åµŒå…¥ã€åˆ†å—å‹ç¼©æœºåˆ¶å’Œå±‚çº§è‡ªé€‚åº”å‰ªæä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼Œç¡®ä¿åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½æ¨ç†å¤æ‚åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šEMLoCé¦–æ¬¡å°†å‹ç¼©å’Œå‰ªææŠ€æœ¯ç»“åˆåº”ç”¨äºå¤šæ¨¡æ€é•¿ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†é€‚åº”æ€§å’Œæ•ˆç‡ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºJensen-Shannonæ•£åº¦çš„å‰ªæç­–ç•¥ï¼Œç¡®ä¿åœ¨æ¯ä¸€å±‚è‡ªé€‚åº”åœ°å‡å°‘ä¸å¿…è¦çš„ä»¤ç‰Œï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç§è§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEMLoCçš„æ€§èƒ½ä¸ä¼ ç»Ÿé•¿ä¸Šä¸‹æ–‡æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨ç†å¤æ‚åº¦ä¸Šçš„æ˜¾è‘—é™ä½ï¼Œå…·ä½“å®éªŒç»“æœè¡¨æ˜ï¼ŒEMLoCåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡å®ç°äº†è¶…è¿‡20%çš„æ¨ç†æ•ˆç‡æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EMLoCçš„ç ”ç©¶æˆæœåœ¨å¤šæ¨¡æ€æ¨¡å‹çš„é€‚åº”æ€§æ–¹é¢å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¡ç®—ç­‰åœºæ™¯ã€‚å…¶é«˜æ•ˆçš„ä»»åŠ¡é€‚åº”èƒ½åŠ›å¯ä»¥æ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸçš„å‘å±•ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Traditional approaches to adapting multi-modal large language models (MLLMs) to new tasks have relied heavily on fine-tuning. This paper introduces Efficient Multi-Modal Long Context Learning (EMLoC), a novel training-free alternative that embeds demonstration examples directly into the model input. EMLoC offers a more efficient, flexible, and scalable solution for task adaptation. Because extremely lengthy inputs introduce prohibitive computational and memory overhead, EMLoC contributes a chunk-wise compression mechanism combined with layer-wise adaptive pruning. It condenses long-context multimodal inputs into compact, task-specific memory representations. By adaptively pruning tokens at each layer under a Jensen-Shannon divergence constraint, our method achieves a dramatic reduction in inference complexity without sacrificing performance. This approach is the first to seamlessly integrate compression and pruning techniques for multi-modal long-context learning, offering a scalable and efficient solution for real-world applications. Extensive experiments on diverse vision-language benchmarks demonstrate that EMLoC achieves performance on par with or superior to naive long-context approaches. Our results highlight the potential of EMLoC as a groundbreaking framework for efficient and flexible adaptation of multi-modal models in resource-constrained environments. Codes are publicly available at https://github.com/Zehong-Ma/EMLoC.

