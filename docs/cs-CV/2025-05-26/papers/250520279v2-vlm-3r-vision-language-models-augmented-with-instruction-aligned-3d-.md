---
layout: default
title: VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction
---

# VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20279" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20279v2</a>
  <a href="https://arxiv.org/pdf/2505.20279.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20279v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20279v2', 'VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, Rakesh Ranjan

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-06-01)

**å¤‡æ³¨**: Project Page: https://vlm-3r.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVLM-3Rä»¥è§£å†³3Dåœºæ™¯ç†è§£çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dé‡å»º` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `å•ç›®è§†é¢‘ç†è§£` `å¤šæ¨¡æ€èåˆ` `æ—¶é—´æ™ºèƒ½` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£ä¸­ä¾èµ–å¤–éƒ¨æ·±åº¦ä¼ æ„Ÿå™¨ï¼Œé™åˆ¶äº†å…¶åœ¨å•ç›®è§†é¢‘è¾“å…¥å’Œæ—¶é—´æ•æ„Ÿåº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚
2. VLM-3Ré€šè¿‡å‡ ä½•ç¼–ç å™¨å¤„ç†å•ç›®è§†é¢‘å¸§ï¼Œç”Ÿæˆéšå¼3Dæ ‡è®°ï¼Œå¹¶ç»“åˆè¶…è¿‡20ä¸‡å¯¹3Dé‡å»ºæŒ‡ä»¤è°ƒä¼˜é—®ç­”å¯¹ï¼Œå®ç°ç©ºé—´ä¸Šä¸‹æ–‡ä¸è¯­è¨€æŒ‡ä»¤çš„æœ‰æ•ˆå¯¹é½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVLM-3Råœ¨è§†è§‰-ç©ºé—´æ¨ç†å’Œæ—¶é—´3Dä¸Šä¸‹æ–‡å˜åŒ–ç†è§£æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨2Då›¾åƒå’Œè§†é¢‘é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œæ‰©å±•è¿™äº›æ¨¡å‹ä»¥ç†è§£3Dåœºæ™¯çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚ç„¶è€Œï¼Œå®ç°ä¸äººç±»èƒ½åŠ›ç›¸å½“çš„æ·±åº¦ç©ºé—´ç†è§£é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å¤–éƒ¨æ·±åº¦ä¼ æ„Ÿå™¨æˆ–ç°æˆç®—æ³•è¿›è¡Œå‡ ä½•æ•æ‰ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºçš„VLM-3Ræ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œç»“åˆäº†3Dé‡å»ºæŒ‡ä»¤è°ƒä¼˜ï¼Œèƒ½å¤Ÿå¤„ç†å•ç›®è§†é¢‘å¸§å¹¶ç”Ÿæˆéšå¼3Dæ ‡è®°ï¼Œä»è€Œæœ‰æ•ˆå¯¹é½ç°å®ä¸–ç•Œçš„ç©ºé—´ä¸Šä¸‹æ–‡ä¸è¯­è¨€æŒ‡ä»¤ã€‚é€šè¿‡å¼•å…¥è§†è§‰-ç©ºé—´-æ—¶é—´æ™ºèƒ½åŸºå‡†ï¼ŒVLM-3Råœ¨å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰3Dåœºæ™¯ç†è§£æ–¹æ³•å¯¹å¤–éƒ¨æ·±åº¦ä¼ æ„Ÿå™¨çš„ä¾èµ–ï¼Œå¯¼è‡´çš„å¯æ‰©å±•æ€§ä¸è¶³å’Œå¯¹å•ç›®è§†é¢‘è¾“å…¥çš„é€‚åº”æ€§å·®çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLM-3Ré€šè¿‡å¼•å…¥å‡ ä½•ç¼–ç å™¨ï¼Œå¤„ç†å•ç›®è§†é¢‘å¸§å¹¶ç”Ÿæˆéšå¼3Dæ ‡è®°ï¼Œä»è€Œå®ç°ç©ºé—´ç†è§£ä¸è¯­è¨€æŒ‡ä»¤çš„å¯¹é½ï¼Œå¢å¼ºæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLM-3Rçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šå‡ ä½•ç¼–ç å™¨ã€ç©ºé—´-è§†è§‰-è§†å›¾èåˆæ¨¡å—ä»¥åŠ3Dé‡å»ºæŒ‡ä»¤è°ƒä¼˜æ¨¡å—ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤„ç†å•ç›®è§†é¢‘å¸§ç”Ÿæˆ3Dæ ‡è®°ï¼Œå¹¶ä¸è¯­è¨€æŒ‡ä»¤è¿›è¡Œæœ‰æ•ˆå¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLM-3Rçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆäº†3Dé‡å»ºæŒ‡ä»¤è°ƒä¼˜ï¼Œåˆ©ç”¨å¤§é‡é—®ç­”å¯¹è¿›è¡Œè®­ç»ƒï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç¼ºä¹æ·±åº¦ä¼ æ„Ÿå™¨çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å®ç°é«˜æ•ˆçš„ç©ºé—´ç†è§£å’Œæ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç©ºé—´ç†è§£çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ç½‘ç»œç»“æ„æ¥å¤„ç†è§†é¢‘å¸§å’Œè¯­è¨€æŒ‡ä»¤çš„èåˆï¼Œç¡®ä¿æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒVLM-3Råœ¨è§†è§‰-ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºç°æœ‰åŸºçº¿ï¼Œå°¤å…¶åœ¨å¤„ç†æ—¶é—´å˜åŒ–çš„3Dä¸Šä¸‹æ–‡æ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°æå‡å¹…åº¦è¶…è¿‡20%ã€‚æ­¤å¤–ï¼ŒVLM-3Råœ¨äº”ä¸ªä¸åŒä»»åŠ¡ä¸Šçš„ç»¼åˆè¡¨ç°ä¹Ÿæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLM-3Rçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡å®ç°å¯¹3Dåœºæ™¯çš„æ·±åº¦ç†è§£ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä¸ºäººæœºäº¤äº’æä¾›æ›´è‡ªç„¶çš„ä½“éªŒï¼Œå¹¶åœ¨å¤æ‚ç¯å¢ƒä¸­æ”¯æŒæ›´æ™ºèƒ½çš„å†³ç­–ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼ŒVLM-3Ræœ‰æœ›åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of Large Multimodal Models (LMMs) for 2D images and videos has motivated extending these models to understand 3D scenes, aiming for human-like visual-spatial intelligence. Nevertheless, achieving deep spatial understanding comparable to human capabilities poses significant challenges in model encoding and data acquisition. Existing methods frequently depend on external depth sensors for geometry capture or utilize off-the-shelf algorithms for pre-constructing 3D maps, thereby limiting their scalability, especially with prevalent monocular video inputs and for time-sensitive applications. In this work, we introduce VLM-3R, a unified framework for Vision-Language Models (VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes monocular video frames by employing a geometry encoder to derive implicit 3D tokens that represent spatial understanding. Leveraging our Spatial-Visual-View Fusion and over 200K curated 3D reconstructive instruction tuning question-answer (QA) pairs, VLM-3R effectively aligns real-world spatial context with language instructions. This enables monocular 3D spatial assistance and embodied reasoning. To facilitate the evaluation of temporal reasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark, featuring over 138.6K QA pairs across five distinct tasks focused on evolving spatial relationships. Extensive experiments demonstrate that our model, VLM-3R, not only facilitates robust visual-spatial reasoning but also enables the understanding of temporal 3D context changes, excelling in both accuracy and scalability.

