---
layout: default
title: Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval
---

# Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19952" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19952v1</a>
  <a href="https://arxiv.org/pdf/2505.19952.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19952v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19952v1', 'Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rong-Cheng Tu, Wenhao Sun, Hanzhe You, Yingjie Wang, Jiaxing Huang, Li Shen, Dacheng Tao

**åˆ†ç±»**: cs.CV, cs.IR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ¨ç†ä»£ç†ä»¥è§£å†³é›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é›¶æ ·æœ¬æ£€ç´¢` `å¤šæ¨¡æ€æ¨ç†` `å›¾åƒæ£€ç´¢` `å¯¹æ¯”å­¦ä¹ ` `æ— ç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–ä¸­ä»‹æ–‡æœ¬è¿›è¡Œç»„åˆæŸ¥è¯¢ä¸ç›®æ ‡å›¾åƒçš„å¯¹é½ï¼Œå¯¼è‡´é”™è¯¯ä¼ æ’­å’Œæ€§èƒ½ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºçš„å¤šæ¨¡æ€æ¨ç†ä»£ç†ï¼ˆMRAï¼‰ç›´æ¥æ„å»ºå›¾åƒä¸æ–‡æœ¬çš„ä¸‰å…ƒç»„ï¼Œæ¶ˆé™¤äº†å¯¹æ–‡æœ¬ä¸­ä»‹çš„ä¾èµ–ã€‚
3. åœ¨FashionIQã€CIRRå’ŒCIRCOæ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæå‡å¹…åº¦å¯è¾¾9.6%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢ï¼ˆZS-CIRï¼‰æ—¨åœ¨æ ¹æ®ç»„åˆæŸ¥è¯¢ï¼ˆåŒ…å«å‚è€ƒå›¾åƒå’Œä¿®æ”¹æ–‡æœ¬ï¼‰æ£€ç´¢ç›®æ ‡å›¾åƒï¼Œè€Œæ— éœ€ä¾èµ–æ ‡æ³¨è®­ç»ƒæ•°æ®ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆç›®æ ‡æ–‡æœ¬ï¼Œä½œä¸ºç»„åˆæŸ¥è¯¢ä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ä¸­ä»‹é”šç‚¹ã€‚ç„¶è€Œï¼Œè¿™ç§å¯¹ä¸­ä»‹æ–‡æœ¬çš„ä¾èµ–å¼•å…¥äº†é”™è¯¯ä¼ æ’­ï¼Œå¯¼è‡´æ£€ç´¢æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé‡‡ç”¨å¤šæ¨¡æ€æ¨ç†ä»£ç†ï¼ˆMRAï¼‰ç›´æ¥æ„å»ºä¸‰å…ƒç»„<å‚è€ƒå›¾åƒã€ä¿®æ”¹æ–‡æœ¬ã€ç›®æ ‡å›¾åƒ>ï¼Œä»…ä½¿ç”¨æœªæ ‡è®°çš„å›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡åœ¨è¿™äº›åˆæˆä¸‰å…ƒç»„ä¸Šè®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿç›´æ¥æ•æ‰ç»„åˆæŸ¥è¯¢ä¸å€™é€‰å›¾åƒä¹‹é—´çš„å…³ç³»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†CIRåŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ£€ç´¢æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³é›¶æ ·æœ¬ç»„åˆå›¾åƒæ£€ç´¢ï¼ˆZS-CIRï¼‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–ç”Ÿæˆçš„ä¸­ä»‹æ–‡æœ¬ï¼Œå¯¼è‡´é”™è¯¯ä¼ æ’­å’Œæ£€ç´¢æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¤šæ¨¡æ€æ¨ç†ä»£ç†ï¼ˆMRAï¼‰ï¼Œé€šè¿‡ç›´æ¥æ„å»º<å‚è€ƒå›¾åƒã€ä¿®æ”¹æ–‡æœ¬ã€ç›®æ ‡å›¾åƒ>ä¸‰å…ƒç»„ï¼Œæ¶ˆé™¤å¯¹ä¸­ä»‹æ–‡æœ¬çš„ä¾èµ–ï¼Œä»è€Œæé«˜æ£€ç´¢ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€ä¸‰å…ƒç»„æ„å»ºå’Œæ¨¡å‹è®­ç»ƒä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œä»æœªæ ‡è®°çš„å›¾åƒæ•°æ®ä¸­ç”Ÿæˆåˆæˆä¸‰å…ƒç»„ï¼Œç„¶ååˆ©ç”¨è¿™äº›ä¸‰å…ƒç»„è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šMRAçš„æœ€å¤§åˆ›æ–°åœ¨äºç›´æ¥åˆ©ç”¨å›¾åƒæ•°æ®æ„å»ºä¸‰å…ƒç»„ï¼Œé¿å…äº†ä¸­ä»‹æ–‡æœ¬çš„å¼•å…¥ï¼Œä»æ ¹æœ¬ä¸Šå‡å°‘äº†é”™è¯¯ä¼ æ’­çš„é£é™©ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨å¯¹æ¯”å­¦ä¹ çš„æŸå¤±å‡½æ•°ï¼Œé€šè¿‡ä¼˜åŒ–ç»„åˆæŸ¥è¯¢ä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬å­¦ä¹ ç‡å’Œæ‰¹é‡å¤§å°ç­‰ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨FashionIQæ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•çš„å¹³å‡R@10æå‡è‡³å°‘7.5%ï¼›åœ¨CIRRæ•°æ®é›†ä¸Šï¼ŒR@1æå‡9.6%ï¼›åœ¨CIRCOæ•°æ®é›†ä¸Šï¼ŒmAP@5æå‡9.5%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå­å•†åŠ¡ã€ç¤¾äº¤åª’ä½“å’Œå†…å®¹æ£€ç´¢ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´é«˜æ•ˆåœ°æ‰¾åˆ°ç¬¦åˆç‰¹å®šéœ€æ±‚çš„å›¾åƒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ— ç›‘ç£å­¦ä¹ å’Œå¤šæ¨¡æ€æ£€ç´¢æŠ€æœ¯çš„å‘å±•ï¼Œæå‡å›¾åƒæ£€ç´¢çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a compositional query, consisting of a reference image and a modifying text-without relying on annotated training data. Existing approaches often generate a synthetic target text using large language models (LLMs) to serve as an intermediate anchor between the compositional query and the target image. Models are then trained to align the compositional query with the generated text, and separately align images with their corresponding texts using contrastive learning. However, this reliance on intermediate text introduces error propagation, as inaccuracies in query-to-text and text-to-image mappings accumulate, ultimately degrading retrieval performance. To address these problems, we propose a novel framework by employing a Multimodal Reasoning Agent (MRA) for ZS-CIR. MRA eliminates the dependence on textual intermediaries by directly constructing triplets, <reference image, modification text, target image>, using only unlabeled image data. By training on these synthetic triplets, our model learns to capture the relationships between compositional queries and candidate images directly. Extensive experiments on three standard CIR benchmarks demonstrate the effectiveness of our approach. On the FashionIQ dataset, our method improves Average R@10 by at least 7.5\% over existing baselines; on CIRR, it boosts R@1 by 9.6\%; and on CIRCO, it increases mAP@5 by 9.5\%.

