---
layout: default
title: "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought"
---

# Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19877" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19877v1</a>
  <a href="https://arxiv.org/pdf/2505.19877.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19877v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19877v1', 'Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chao Huang, Benfeng Wang, Jie Wen, Chengliang Liu, Wei Wang, Li Shen, Xiaochun Cao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: 9 pages, 4 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/wbfwonderful/Vad-R1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVad-R1ä»¥è§£å†³è§†é¢‘å¼‚å¸¸æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¼‚å¸¸æ£€æµ‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æ·±åº¦æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¼‚å¸¸è¡Œä¸ºè¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºMLLMçš„è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä»…èƒ½æä¾›æµ…å±‚çš„å¼‚å¸¸æè¿°ï¼Œç¼ºä¹æ·±åº¦æ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†Vad-R1æ¡†æ¶ï¼Œé€šè¿‡æ„ŸçŸ¥åˆ°è®¤çŸ¥é“¾æ€ç»´ï¼ˆP2C-CoTï¼‰å¼•å¯¼MLLMé€æ­¥æ¨ç†å¼‚å¸¸ï¼Œä»è€Œå®ç°è§†é¢‘å¼‚å¸¸æ¨ç†ï¼ˆVARï¼‰ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVad-R1åœ¨VADå’ŒVARä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œå±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºMLLMçš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æ–¹æ³•ä»…é™äºæµ…å±‚å¼‚å¸¸æè¿°ï¼Œç¼ºä¹æ·±åº¦æ¨ç†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ–°ä»»åŠ¡â€”â€”è§†é¢‘å¼‚å¸¸æ¨ç†ï¼ˆVARï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è¦æ±‚MLLMåœ¨å›ç­”å‰è¿›è¡Œæ˜ç¡®æ€è€ƒï¼Œä»è€Œå®ç°å¯¹è§†é¢‘ä¸­å¼‚å¸¸çš„æ·±åº¦åˆ†æä¸ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Vad-R1ï¼Œä¸€ä¸ªåŸºäºMLLMçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œè®¾è®¡äº†æ¨¡æ‹Ÿäººç±»è¯†åˆ«å¼‚å¸¸è¿‡ç¨‹çš„æ„ŸçŸ¥åˆ°è®¤çŸ¥é“¾æ€ç»´ï¼ˆP2C-CoTï¼‰ï¼Œå¼•å¯¼MLLMé€æ­¥æ¨ç†å¼‚å¸¸ã€‚åŸºäºç»“æ„åŒ–çš„P2C-CoTï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸“é—¨ç”¨äºVARçš„Vad-Reasoningæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•AVA-GRPOï¼Œé€šè¿‡æœ‰é™æ³¨é‡Šçš„è‡ªæˆ‘éªŒè¯æœºåˆ¶ï¼Œæ˜ç¡®æ¿€åŠ±MLLMçš„å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVad-R1åœ¨VADå’ŒVARä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¤šç§å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨æ·±åº¦æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä»…èƒ½æä¾›è¡¨é¢å¼‚å¸¸æè¿°ï¼Œæ— æ³•è¿›è¡Œæ·±å…¥åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVad-R1æ¡†æ¶é€šè¿‡è®¾è®¡æ„ŸçŸ¥åˆ°è®¤çŸ¥é“¾æ€ç»´ï¼ˆP2C-CoTï¼‰ï¼Œæ¨¡æ‹Ÿäººç±»çš„å¼‚å¸¸è¯†åˆ«è¿‡ç¨‹ï¼Œä¿ƒä½¿MLLMåœ¨å›ç­”å‰è¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»è€Œå®ç°å¯¹è§†é¢‘å¼‚å¸¸çš„æ·±åº¦ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVad-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ„ŸçŸ¥æ¨¡å—ã€è®¤çŸ¥æ¨¡å—å’Œæ¨ç†æ¨¡å—ã€‚æ„ŸçŸ¥æ¨¡å—è´Ÿè´£æå–è§†é¢‘ç‰¹å¾ï¼Œè®¤çŸ¥æ¨¡å—é€šè¿‡P2C-CoTå¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œæ¨ç†æ¨¡å—åˆ™ç”Ÿæˆæœ€ç»ˆçš„å¼‚å¸¸åˆ¤æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºP2C-CoTçš„è®¾è®¡ï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–çš„æ€ç»´é“¾æ¡å¼•å¯¼MLLMè¿›è¡Œæ·±åº¦æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•AVA-GRPOï¼Œç»“åˆè‡ªæˆ‘éªŒè¯æœºåˆ¶ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†Vad-Reasoningçš„æ„å»ºä¸ºVARä»»åŠ¡æä¾›äº†ä¸°å¯Œçš„æ ‡æ³¨æ•°æ®ï¼Œæ”¯æŒæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Vad-R1åœ¨VADå’ŒVARä»»åŠ¡ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æå‡äº†è¶…è¿‡15%çš„å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨æ·±åº¦æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬å®‰å…¨ç›‘æ§ã€äº¤é€šç›‘æµ‹å’Œæ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œåˆ†æè§†é¢‘ä¸­çš„å¼‚å¸¸è¡Œä¸ºï¼Œæå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„è§†è§‰ç†è§£ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨è§†é¢‘åˆ†ææŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in reasoning capability of Multimodal Large Language Models (MLLMs) demonstrate its effectiveness in tackling complex visual tasks. However, existing MLLM-based Video Anomaly Detection (VAD) methods remain limited to shallow anomaly descriptions without deep reasoning. In this paper, we propose a new task named Video Anomaly Reasoning (VAR), which aims to enable deep analysis and understanding of anomalies in the video by requiring MLLMs to think explicitly before answering. To this end, we propose Vad-R1, an end-to-end MLLM-based framework for VAR. Specifically, we design a Perception-to-Cognition Chain-of-Thought (P2C-CoT) that simulates the human process of recognizing anomalies, guiding the MLLM to reason anomaly step-by-step. Based on the structured P2C-CoT, we construct Vad-Reasoning, a dedicated dataset for VAR. Furthermore, we propose an improved reinforcement learning algorithm AVA-GRPO, which explicitly incentivizes the anomaly reasoning capability of MLLMs through a self-verification mechanism with limited annotations. Experimental results demonstrate that Vad-R1 achieves superior performance, outperforming both open-source and proprietary models on VAD and VAR tasks. Codes and datasets will be released at https://github.com/wbfwonderful/Vad-R1.

