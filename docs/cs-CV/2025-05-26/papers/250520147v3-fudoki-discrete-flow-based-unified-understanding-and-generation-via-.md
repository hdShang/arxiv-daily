---
layout: default
title: FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities
---

# FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20147" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20147v3</a>
  <a href="https://arxiv.org/pdf/2505.20147.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20147v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20147v3', 'FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, Ping Luo

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-07-24)

**å¤‡æ³¨**: 37 pages, 12 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFUDOKIä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å±€é™æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç¦»æ•£æµåŒ¹é…` `è‡ªå›å½’æ¶æ„` `å›¾åƒç”Ÿæˆ` `è§†è§‰ç†è§£` `ä¸Šä¸‹æ–‡æ•´åˆ` `ç”Ÿæˆæœºåˆ¶` `åŠ¨é‡æœ€ä¼˜é€Ÿåº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸»è¦ä¾èµ–è‡ªå›å½’æ¶æ„ï¼Œå¯¼è‡´å›¾åƒç”Ÿæˆå’Œå› æœæ¨ç†èƒ½åŠ›å—é™ã€‚
2. FUDOKIé€šè¿‡ç¦»æ•£æµåŒ¹é…æ–¹æ³•ï¼Œå…‹æœäº†ä¼ ç»ŸARæ¨¡å‹çš„å±€é™ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„ç”Ÿæˆè¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFUDOKIåœ¨å¤šä¸ªè§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šä¸æœ€å…ˆè¿›çš„ARæ¨¡å‹æ€§èƒ½ç›¸å½“ï¼Œä¸”å…·æœ‰æ›´å¥½çš„æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨ç»Ÿä¸€è§†è§‰ç†è§£ä¸å›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLLMså¤§å¤šä¾èµ–è‡ªå›å½’ï¼ˆARï¼‰æ¶æ„ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å›¾åƒç”Ÿæˆä¸­çš„é¡ºåºå¤„ç†å’Œå› æœä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºFUDOKIï¼Œä¸€ä¸ªåŸºäºç¦»æ•£æµåŒ¹é…çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œä½œä¸ºä¼ ç»ŸARèŒƒå¼çš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡åˆ©ç”¨åº¦é‡è¯±å¯¼çš„æ¦‚ç‡è·¯å¾„ä¸åŠ¨é‡æœ€ä¼˜é€Ÿåº¦ï¼ŒFUDOKIå®ç°äº†è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›å’Œæ›´ä¸°å¯Œçš„åŒå‘ä¸Šä¸‹æ–‡æ•´åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFUDOKIåœ¨è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸æœ€å…ˆè¿›çš„ARæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åŸºç¡€çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¤§å¤šä¾èµ–è‡ªå›å½’ï¼ˆARï¼‰æ¶æ„ï¼Œè¿™ç§æ–¹æ³•åœ¨å›¾åƒç”Ÿæˆæ—¶å­˜åœ¨æ …æ ¼æ‰«æé¡ºåºçš„é™åˆ¶ï¼Œå¹¶ä¸”åœ¨å› æœä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­æ¨ç†èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFUDOKIæå‡ºäº†ä¸€ç§åŸºäºç¦»æ•£æµåŒ¹é…çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æ›¿ä»£ä¼ ç»Ÿçš„ARæ–¹æ³•ã€‚é€šè¿‡å¼•å…¥åº¦é‡è¯±å¯¼çš„æ¦‚ç‡è·¯å¾„ä¸åŠ¨é‡æœ€ä¼˜é€Ÿåº¦ï¼ŒFUDOKIèƒ½å¤Ÿå®ç°è‡ªæˆ‘ä¿®æ­£å’ŒåŒå‘ä¸Šä¸‹æ–‡æ•´åˆï¼Œä»è€Œæå‡ç”Ÿæˆè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFUDOKIçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯ä»é¢„è®­ç»ƒçš„ARæ¨¡å‹åˆå§‹åŒ–ï¼Œç„¶åé€šè¿‡é€‚åº”æ€§è½¬å˜åˆ°ç¦»æ•£æµåŒ¹é…èŒƒå¼ã€‚è¯¥æ¡†æ¶æ”¯æŒè¿­ä»£ç²¾ç‚¼å’Œä¸Šä¸‹æ–‡æ•´åˆï¼Œå½¢æˆä¸€ä¸ªé—­ç¯ç”Ÿæˆè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šFUDOKIçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å®Œå…¨åŸºäºç¦»æ•£æµåŒ¹é…çš„ç”Ÿæˆæœºåˆ¶ï¼Œçªç ´äº†ä¼ ç»ŸARæ¨¡å‹çš„å±€é™ï¼Œèƒ½å¤Ÿå®ç°æ›´çµæ´»çš„ç”Ÿæˆç­–ç•¥å’Œæ›´é«˜çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒFUDOKIé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†åŠ¨é‡æœ€ä¼˜é€Ÿåº¦çš„æ¦‚å¿µï¼Œä»¥æå‡æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œç”Ÿæˆæ•ˆæœã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFUDOKIåœ¨è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFUDOKIçš„ç”Ÿæˆè´¨é‡å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å‡æœ‰æ˜¾è‘—æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FUDOKIçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ™ºèƒ½å›¾åƒç”Ÿæˆã€è§†è§‰é—®ç­”ç³»ç»Ÿä»¥åŠå¤šæ¨¡æ€å†…å®¹åˆ›ä½œç­‰ã€‚å…¶åˆ›æ–°çš„ç”Ÿæˆæœºåˆ¶å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå¯èƒ½ä¸ºæœªæ¥çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›åšå®çš„åŸºç¡€ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning.

