---
layout: default
title: Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning
---

# Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20272" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20272v2</a>
  <a href="https://arxiv.org/pdf/2505.20272.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20272v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20272v2', 'Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Meng Cao, Haoze Zhao, Can Zhang, Xiaojun Chang, Ian Reid, Xiaodan Liang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-06-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGround-R1ä»¥è§£å†³è§†è§‰æ¨ç†ä¸­çš„ç›‘ç£æˆæœ¬é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€ä»»åŠ¡` `å¯è§£é‡Šæ€§` `ç›‘ç£æˆæœ¬`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰æ¨ç†æ–¹æ³•ä¾èµ–æ˜‚è´µçš„ç›‘ç£ï¼Œå¯¼è‡´å¯æ‰©å±•æ€§å·®ï¼Œè¾“å‡ºç»“æœä¸å¯é ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚
2. Ground-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶å®ç°åŸºäºè§†è§‰çš„æ¨ç†ï¼Œæ— éœ€æ˜¾å¼çš„è¯æ®æˆ–æ¨ç†æ ‡æ³¨ï¼Œé™ä½äº†ç›‘ç£æˆæœ¬ã€‚
3. åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†ä¸Šï¼ŒGround-R1å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œæå‡äº†ä¸ç¡®å®šæ€§æ„è¯†å’Œç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹å¸¸å¸¸è¾“å‡ºä¸å¯é ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŸºäºè§†è§‰è¯æ®çš„æ¨ç†é€æ¸æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„ç›‘ç£ï¼Œå¦‚è¾¹ç•Œæ¡†æ ‡æ³¨å’Œå¤–éƒ¨å·¥å…·è°ƒç”¨ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºäº†Ground-R1ï¼Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿å¾—åœ¨ä¸éœ€è¦æ˜¾å¼è¯æ®æˆ–æ¨ç†æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°åŸºäºè§†è§‰çš„æ¨ç†ã€‚Ground-R1åŒ…æ‹¬ä¸€ä¸ªç”Ÿæˆè¯æ®åŒºåŸŸçš„åŸºç¡€é˜¶æ®µå’Œä¸€ä¸ªåŸºäºç­”æ¡ˆæ­£ç¡®æ€§å’Œæ ¼å¼éµå¾ªå¥–åŠ±çš„å›ç­”é˜¶æ®µã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGround-R1åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¹¶å±•ç°å‡ºä¸ç¡®å®šæ€§æ„è¯†ã€ç©ºé—´æ„ŸçŸ¥å’Œè¿­ä»£ä¼˜åŒ–ç­‰è®¤çŸ¥è¡Œä¸ºï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰æ¨ç†æ–¹æ³•ä¾èµ–æ˜‚è´µç›‘ç£çš„é—®é¢˜ï¼Œå¯¼è‡´å…¶å¯æ‰©å±•æ€§å’Œè¾“å‡ºå¯é æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGround-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶å®ç°åŸºäºè§†è§‰çš„æ¨ç†ï¼Œé¿å…äº†å¯¹æ˜¾å¼è¯æ®å’Œæ¨ç†æ ‡æ³¨çš„ä¾èµ–ï¼Œä»è€Œé™ä½äº†ç›‘ç£æˆæœ¬ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGround-R1åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç”Ÿæˆè¯æ®åŒºåŸŸçš„åŸºç¡€é˜¶æ®µå’ŒåŸºäºç­”æ¡ˆæ­£ç¡®æ€§åŠæ ¼å¼éµå¾ªçš„å›ç­”é˜¶æ®µã€‚åŸºç¡€é˜¶æ®µç”Ÿæˆç¬¦åˆæ ¼å¼çº¦æŸçš„è¯æ®åŒºåŸŸï¼Œè€Œå›ç­”é˜¶æ®µåˆ™æ ¹æ®å¥–åŠ±æœºåˆ¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šGround-R1çš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ— éœ€æ˜¾å¼çš„è¯æ®æˆ–æ¨ç†æ ‡æ³¨ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å®ç°äº†å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„è§†è§‰æ¨ç†ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒGround-R1é‡‡ç”¨äº†ç‰¹å®šçš„å¥–åŠ±æœºåˆ¶ï¼Œå¼ºè°ƒç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œæ ¼å¼éµå¾ªï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æ”¯æŒè¯æ®åŒºåŸŸçš„ç”Ÿæˆå’Œç­”æ¡ˆçš„ç”Ÿæˆã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­ç»è¿‡è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†ä¸Šï¼ŒGround-R1çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šæå‡äº†10%ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼Œå¹¶å±•ç°å‡ºä¸ç¡®å®šæ€§æ„è¯†å’Œç©ºé—´æ„ŸçŸ¥ç­‰è®¤çŸ¥è¡Œä¸ºï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œåˆ›æ–°æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰å¤šæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡æä¾›å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼ŒGround-R1èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æå‡ç³»ç»Ÿçš„å¯é æ€§å’Œç”¨æˆ·ä¿¡ä»»åº¦ï¼Œæœªæ¥å¯èƒ½å¯¹äººæœºäº¤äº’å’Œè‡ªåŠ¨åŒ–å†³ç­–äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have demonstrated impressive general capabilities across a wide range of multi-modal tasks. However, the reasoning processes of LVLMs often suffer from unreliable outputs and limited interpretability. To address this, grounded visual reasoning has emerged as a promising paradigm that enforces responses anchored on salient visual evidence regions. However, existing approaches typically rely on costly supervision such as bounding box annotations, chain-of-thought rationale or external tool calls, limiting their scalability. In this work, we propose Ground-R1, a reinforcement learning framework that enables grounded visual reasoning without requiring explicit evidence or rationale annotations. Ground-R1 consists of a grounding phase that generates evidence region rollouts based on format constraints, and an answering phase that produces responses guided by both answer correctness and format adherence rewards. Extensive experiments across multiple visual reasoning benchmarks manifest that Ground-R1 achieves superior performance and exhibits emergent cognitive behaviors such as uncertainty awareness, spatial perception, and iterative refinement, offering a scalable and interpretable alternative to existing approaches.

