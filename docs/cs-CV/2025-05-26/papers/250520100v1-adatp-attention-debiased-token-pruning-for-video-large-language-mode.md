---
layout: default
title: "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models"
---

# AdaTP: Attention-Debiased Token Pruning for Video Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20100" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20100v1</a>
  <a href="https://arxiv.org/pdf/2505.20100.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20100v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20100v1', 'AdaTP: Attention-Debiased Token Pruning for Video Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fengyuan Sun, Leqi Shen, Hui Chen, Sicheng Zhao, Jungong Han, Guiguang Ding

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAdaTPä»¥è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›åå·®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘å¤§è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `æ ‡è®°ä¿®å‰ª` `è®¡ç®—æ•ˆç‡` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤§é‡è§†è§‰æ ‡è®°æ—¶é¢ä¸´è®¡ç®—å¼€é”€å¤§çš„æŒ‘æˆ˜ï¼Œå½±å“äº†å…¶åº”ç”¨æ•ˆç‡ã€‚
2. æå‡ºçš„AdaTPé€šè¿‡é›†æˆå…¨å±€å’Œå±€éƒ¨å»åå·®æ¨¡å—ï¼Œæœ‰æ•ˆä¿®å‰ªè§†è§‰æ ‡è®°ï¼Œå‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaTPåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½¿ç”¨çš„FLOPsä»…ä¸ºåŸæ¨¡å‹çš„27.3%ï¼Œæ€§èƒ½æœªå—å½±å“ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ç”±äºä»å¤šä¸ªè§†é¢‘å¸§ç”Ÿæˆçš„å¤§é‡è§†è§‰æ ‡è®°ï¼Œè®¡ç®—å¼€é”€è¾ƒå¤§ã€‚ç°æœ‰çš„è§†è§‰æ ‡è®°å‹ç¼©æ–¹æ³•é€šå¸¸ä¾èµ–äºè¯­è¨€æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†æ•°ä½œä¸ºæŒ‡å¯¼ï¼Œä½†è¿™äº›åˆ†æ•°å­˜åœ¨å›ºæœ‰åå·®ï¼šå…¨å±€åå·®ä½¿å¾—æ¨¡å‹å€¾å‘äºå…³æ³¨è§†è§‰æ ‡è®°åºåˆ—çš„ä¸¤ç«¯ï¼Œè€Œå±€éƒ¨åå·®åˆ™å¯¼è‡´åœ¨ä¸åŒå¸§ä¸­å¯¹ç›¸åŒç©ºé—´ä½ç½®çš„è¿‡åº¦é›†ä¸­ã€‚ä¸ºäº†è§£å†³æ³¨æ„åŠ›åå·®é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ³¨æ„åŠ›å»åå·®æ ‡è®°ä¿®å‰ªï¼ˆAdaTPï¼‰ï¼Œè¯¥æ–¹æ³•é›†æˆäº†ä¸¤ä¸ªä¸“é—¨çš„å»åå·®æ¨¡å—ï¼Œåˆ†åˆ«é’ˆå¯¹å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›åå·®ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒAdaTPåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†è§†é¢‘LLMsçš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­ç”±äºè§†è§‰æ ‡è®°æ•°é‡åºå¤§è€Œå¯¼è‡´çš„è®¡ç®—å¼€é”€é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–çš„æ³¨æ„åŠ›åˆ†æ•°å­˜åœ¨å…¨å±€å’Œå±€éƒ¨åå·®ï¼Œå½±å“äº†æ ‡è®°çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„AdaTPæ–¹æ³•é€šè¿‡å¼•å…¥å…¨å±€å’Œå±€éƒ¨å»åå·®æ¨¡å—ï¼Œé’ˆå¯¹æ€§åœ°ä¿®å‰ªè§†è§‰æ ‡è®°ï¼Œæ—¨åœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†è§†é¢‘æ•°æ®æ—¶æ›´åŠ é«˜æ•ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAdaTPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå…¨å±€å»åå·®æ¨¡å—å’Œå±€éƒ¨å»åå·®æ¨¡å—ã€‚å…¨å±€æ¨¡å—å…³æ³¨è§†è§‰æ ‡è®°åºåˆ—çš„æ•´ä½“ç»“æ„ï¼Œè€Œå±€éƒ¨æ¨¡å—åˆ™é’ˆå¯¹åŒä¸€ç©ºé—´ä½ç½®çš„æ ‡è®°è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šAdaTPçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å»åå·®æœºåˆ¶ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æ¨¡å—æœ‰æ•ˆæ¶ˆé™¤äº†æ³¨æ„åŠ›åå·®ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰æ ‡è®°çš„é€‰æ‹©æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œæ›´ä½çš„è®¡ç®—éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒAdaTPä¸éœ€è¦é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç›´æ¥åœ¨ç°æœ‰æ¨¡å‹ä¸Šè¿›è¡Œæ ‡è®°ä¿®å‰ªã€‚å…³é”®å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¿®å‰ªåä»èƒ½ä¿æŒé«˜æ•ˆçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaTPåœ¨LLaVA-OneVision-7Bæ¨¡å‹ä¸Šå®ç°äº†æ€§èƒ½çš„æ— æŸä¿æŒï¼ŒåŒæ—¶è®¡ç®—å¼€é”€ä»…ä¸ºåŸæ¨¡å‹çš„27.3%ã€‚è¿™ä¸€ç»“æœå±•ç¤ºäº†AdaTPåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è§†é¢‘åˆ†æã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è§†é¢‘ç†è§£ä»»åŠ¡çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼ŒAdaTPæœ‰æœ›åœ¨æ›´å¹¿æ³›çš„å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video Large Language Models (Video LLMs) have achieved remarkable results in video understanding tasks. However, they often suffer from heavy computational overhead due to the large number of visual tokens generated from multiple video frames. Existing visual token compression methods often rely on attention scores from language models as guidance. However, these scores exhibit inherent biases: global bias reflects a tendency to focus on the two ends of the visual token sequence, while local bias leads to an over-concentration on the same spatial positions across different frames. To address the issue of attention bias, we propose $\textbf{A}$ttention-$\textbf{D}$ebi$\textbf{a}$sed $\textbf{T}$oken $\textbf{P}$runing for Video Large Language Models ($\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP integrates two dedicated debiasing modules into the pipeline, targeting global attention bias and local attention bias, respectively. Without the need for additional training, our method significantly reduces the computational overhead of Video LLMs while retaining the performance of vanilla models. Extensive evaluation shows that AdaTP achieves state-of-the-art performance in various commonly used video understanding benchmarks. In particular, on LLaVA-OneVision-7B, AdaTP maintains performance without degradation while using only up to $27.3\%$ FLOPs compared to the vanilla model. Our code will be released soon.

