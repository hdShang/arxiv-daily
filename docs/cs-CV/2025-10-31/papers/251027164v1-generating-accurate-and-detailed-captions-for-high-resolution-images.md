---
layout: default
title: Generating Accurate and Detailed Captions for High-Resolution Images
---

# Generating Accurate and Detailed Captions for High-Resolution Images

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.27164" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.27164v1</a>
  <a href="https://arxiv.org/pdf/2510.27164.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27164v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.27164v1', 'Generating Accurate and Detailed Captions for High-Resolution Images')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hankyeol Lee, Gawon Seo, Kyounggyu Lee, Dogun Kim, Kyungwoo Song, Jiyoung Jung

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

**å¤‡æ³¨**: Work conducted in 2024; released for archival purposes

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å¤šé˜¶æ®µæµç¨‹ï¼Œèåˆè§†è§‰-è¯­è¨€æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç›®æ ‡æ£€æµ‹ï¼Œä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆæ›´å‡†ç¡®ã€è¯¦ç»†çš„æè¿°ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é«˜åˆ†è¾¨ç‡å›¾åƒæè¿°` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç›®æ ‡æ£€æµ‹` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒæè¿°ç”Ÿæˆä¸­ï¼Œå› é¢„è®­ç»ƒæ•°æ®åˆ†è¾¨ç‡ä½ï¼Œæ˜“ä¸¢å¤±ç»†èŠ‚å’Œå¿½ç•¥é‡è¦å¯¹è±¡ã€‚
2. æå‡ºèåˆè§†è§‰-è¯­è¨€æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç›®æ ‡æ£€æµ‹çš„å¤šé˜¶æ®µæµç¨‹ï¼Œæå‡æè¿°çš„å‡†ç¡®æ€§å’Œç»†èŠ‚ä¸°å¯Œåº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æµç¨‹èƒ½ç”Ÿæˆæ›´è¯¦ç»†å¯é çš„å›¾åƒæè¿°ï¼Œæœ‰æ•ˆå‡å°‘å¹»è§‰ï¼Œæå‡æè¿°è´¨é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)é€šå¸¸éš¾ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„å‡†ç¡®å’Œè¯¦ç»†æè¿°ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸åœ¨ä½åˆ†è¾¨ç‡è¾“å…¥(ä¾‹å¦‚ï¼Œ224x224æˆ–336x336åƒç´ )ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚å°†é«˜åˆ†è¾¨ç‡å›¾åƒç¼©å°åˆ°è¿™äº›å°ºå¯¸å¯èƒ½ä¼šå¯¼è‡´è§†è§‰ç»†èŠ‚çš„ä¸¢å¤±å’Œé‡è¦å¯¹è±¡çš„é—æ¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµç¨‹ï¼Œè¯¥æµç¨‹é›†æˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å’Œç›®æ ‡æ£€æµ‹ç³»ç»Ÿï¼Œä»¥æé«˜æè¿°è´¨é‡ã€‚æˆ‘ä»¬æå‡ºçš„æµç¨‹é€šè¿‡ä¸€ç§æ–°é¢–çš„å¤šé˜¶æ®µè¿‡ç¨‹æ¥æ”¹è¿›æè¿°ã€‚ç»™å®šä¸€ä¸ªé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œé¦–å…ˆä½¿ç”¨VLMç”Ÿæˆåˆå§‹æè¿°ï¼Œç„¶åç”±LLMè¯†åˆ«å›¾åƒä¸­çš„å…³é”®å¯¹è±¡ã€‚LLMé¢„æµ‹å¯èƒ½ä¸å·²è¯†åˆ«çš„å…³é”®å¯¹è±¡å…±åŒå‡ºç°çš„å…¶ä»–å¯¹è±¡ï¼Œå¹¶ä¸”è¿™äº›é¢„æµ‹ç”±ç›®æ ‡æ£€æµ‹ç³»ç»ŸéªŒè¯ã€‚æœªåœ¨åˆå§‹æè¿°ä¸­æåŠçš„æ–°æ£€æµ‹åˆ°çš„å¯¹è±¡ä¼šè¿›è¡Œé›†ä¸­çš„ã€ç‰¹å®šäºåŒºåŸŸçš„æè¿°ï¼Œä»¥ç¡®ä¿å®ƒä»¬è¢«åŒ…å«åœ¨å†…ã€‚æ­¤è¿‡ç¨‹ä¸°å¯Œäº†æè¿°ç»†èŠ‚ï¼ŒåŒæ—¶é€šè¿‡åˆ é™¤å¯¹æœªæ£€æµ‹åˆ°çš„å¯¹è±¡çš„å¼•ç”¨æ¥å‡å°‘å¹»è§‰ã€‚æˆ‘ä»¬ä½¿ç”¨æˆå¯¹æ¯”è¾ƒå’Œæ¥è‡ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å®šé‡è¯„åˆ†ï¼Œä»¥åŠå¹»è§‰æ£€æµ‹çš„åŸºå‡†æ¥è¯„ä¼°å¢å¼ºçš„æè¿°ã€‚åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒçš„ç²¾é€‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æµç¨‹å¯ä»¥ç”Ÿæˆæ›´è¯¦ç»†å’Œå¯é çš„å›¾åƒæè¿°ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°å‡å°‘å¹»è§‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒæè¿°ç”Ÿæˆä¸­ï¼Œç”±äºæ¨¡å‹é€šå¸¸åœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸Šé¢„è®­ç»ƒï¼Œå¯¼è‡´ç”Ÿæˆæè¿°æ—¶ä¸¢å¤±å›¾åƒç»†èŠ‚ï¼Œå¿½ç•¥é‡è¦å¯¹è±¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç›´æ¥å°†é«˜åˆ†è¾¨ç‡å›¾åƒé™é‡‡æ ·åˆ°ä½åˆ†è¾¨ç‡ï¼Œé€ æˆä¿¡æ¯æŸå¤±ï¼Œå½±å“æè¿°çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„çŸ¥è¯†æ¨ç†èƒ½åŠ›å’Œç›®æ ‡æ£€æµ‹ç³»ç»Ÿçš„ç²¾ç¡®è¯†åˆ«èƒ½åŠ›ï¼Œå¯¹è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)ç”Ÿæˆçš„åˆå§‹æè¿°è¿›è¡Œå¢å¼ºå’Œä¿®æ­£ã€‚é€šè¿‡LLMæ¨æ–­å›¾åƒä¸­å¯èƒ½å­˜åœ¨çš„ç›¸å…³å¯¹è±¡ï¼Œå¹¶åˆ©ç”¨ç›®æ ‡æ£€æµ‹ç³»ç»ŸéªŒè¯è¿™äº›å¯¹è±¡çš„å­˜åœ¨æ€§ï¼Œä»è€Œè¡¥å……VLMé—æ¼çš„ä¿¡æ¯ï¼ŒåŒæ—¶å‡å°‘å¹»è§‰ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) VLMç”Ÿæˆåˆå§‹æè¿°ï¼›2) LLMåŸºäºåˆå§‹æè¿°è¯†åˆ«å…³é”®å¯¹è±¡ï¼›3) LLMé¢„æµ‹ä¸å…³é”®å¯¹è±¡ç›¸å…³çš„å…¶ä»–å¯èƒ½å­˜åœ¨çš„å¯¹è±¡ï¼›4) ç›®æ ‡æ£€æµ‹ç³»ç»ŸéªŒè¯LLMé¢„æµ‹çš„å¯¹è±¡ï¼›5) å¯¹æ–°æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡ŒåŒºåŸŸç‰¹å®šçš„æè¿°ç”Ÿæˆï¼Œå¹¶æ•´åˆåˆ°æœ€ç»ˆæè¿°ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†LLMçš„çŸ¥è¯†æ¨ç†èƒ½åŠ›å’Œç›®æ ‡æ£€æµ‹ç³»ç»Ÿçš„ç²¾ç¡®è¯†åˆ«èƒ½åŠ›ç»“åˆèµ·æ¥ï¼Œç”¨äºå¢å¼ºå’Œä¿®æ­£VLMç”Ÿæˆçš„å›¾åƒæè¿°ã€‚è¿™ç§å¤šæ¨¡æ€èåˆçš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¥å……VLMé—æ¼çš„ä¿¡æ¯ï¼Œå‡å°‘å¹»è§‰ï¼Œä»è€Œæé«˜æè¿°çš„å‡†ç¡®æ€§å’Œç»†èŠ‚ä¸°å¯Œåº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¯¹VLMè¿›è¡Œé‡æ–°è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡åå¤„ç†çš„æ–¹å¼æå‡æè¿°è´¨é‡ï¼Œæ›´å…·çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¯¹è±¡é¢„æµ‹é˜¶æ®µï¼ŒLLMè¢«ç”¨äºé¢„æµ‹ä¸åˆå§‹æè¿°ä¸­è¯†åˆ«çš„å…³é”®å¯¹è±¡å¯èƒ½å…±åŒå‡ºç°çš„å…¶ä»–å¯¹è±¡ã€‚ç›®æ ‡æ£€æµ‹ç³»ç»Ÿç”¨äºéªŒè¯LLMé¢„æµ‹çš„å¯¹è±¡çš„å­˜åœ¨æ€§ï¼Œåªæœ‰è¢«æ£€æµ‹åˆ°çš„å¯¹è±¡æ‰ä¼šè¢«æ·»åŠ åˆ°æè¿°ä¸­ã€‚å¯¹äºæ–°æ£€æµ‹åˆ°çš„å¯¹è±¡ï¼Œé‡‡ç”¨åŒºåŸŸç‰¹å®šçš„æè¿°ç”Ÿæˆæ–¹æ³•ï¼Œä»¥ç¡®ä¿è¿™äº›å¯¹è±¡èƒ½å¤Ÿè¢«å‡†ç¡®åœ°æè¿°ã€‚è®ºæ–‡è¿˜ä½¿ç”¨äº†ä¸“é—¨è®¾è®¡çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒLLMï¼Œä»¥æé«˜å…¶é¢„æµ‹ç›¸å…³å¯¹è±¡çš„èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒæè¿°æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚é€šè¿‡ä¸ç°æœ‰æ–¹æ³•çš„å¯¹æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´è¯¦ç»†ã€æ›´å‡†ç¡®çš„æè¿°ï¼Œå¹¶æœ‰æ•ˆåœ°å‡å°‘å¹»è§‰ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœ‰æ‰€ä½“ç°ï¼Œä½†æœªåœ¨æ­¤å¤„è¯¦ç»†åˆ—å‡ºã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½ç›¸å†Œã€å›¾åƒæœç´¢å¼•æ“ã€è¾…åŠ©è§†è§‰ç­‰é¢†åŸŸã€‚é€šè¿‡ç”Ÿæˆæ›´å‡†ç¡®ã€è¯¦ç»†çš„å›¾åƒæè¿°ï¼Œå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œæé«˜ä¿¡æ¯æ£€ç´¢æ•ˆç‡ï¼Œå¹¶ä¸ºè§†è§‰éšœç¢äººå£«æä¾›æ›´å¥½çš„è¾…åŠ©å·¥å…·ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸï¼Œä¸ºæœºå™¨æä¾›æ›´å…¨é¢çš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) often struggle to generate accurate and detailed captions for high-resolution images since they are typically pre-trained on low-resolution inputs (e.g., 224x224 or 336x336 pixels). Downscaling high-resolution images to these dimensions may result in the loss of visual details and the omission of important objects. To address this limitation, we propose a novel pipeline that integrates vision-language models, large language models (LLMs), and object detection systems to enhance caption quality. Our proposed pipeline refines captions through a novel, multi-stage process. Given a high-resolution image, an initial caption is first generated using a VLM, and key objects in the image are then identified by an LLM. The LLM predicts additional objects likely to co-occur with the identified key objects, and these predictions are verified by object detection systems. Newly detected objects not mentioned in the initial caption undergo focused, region-specific captioning to ensure they are incorporated. This process enriches caption detail while reducing hallucinations by removing references to undetected objects. We evaluate the enhanced captions using pairwise comparison and quantitative scoring from large multimodal models, along with a benchmark for hallucination detection. Experiments on a curated dataset of high-resolution images demonstrate that our pipeline produces more detailed and reliable image captions while effectively minimizing hallucinations.

