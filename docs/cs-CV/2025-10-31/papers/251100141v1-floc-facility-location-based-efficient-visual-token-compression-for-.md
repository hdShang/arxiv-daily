---
layout: default
title: FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding
---

# FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.00141" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.00141v1</a>
  <a href="https://arxiv.org/pdf/2511.00141.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00141v1" onclick="toggleFavorite(this, '2511.00141v1', 'FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FLoCï¼šåŸºäºè®¾æ–½é€‰å€çš„é•¿è§†é¢‘é«˜æ•ˆè§†è§‰Tokenå‹ç¼©æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `è§†è§‰Tokenå‹ç¼©` `è®¾æ–½é€‰å€` `æ‡’æƒ°è´ªå©ªç®—æ³•` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é•¿è§†é¢‘ç†è§£ä¾èµ–å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œä½†é•¿è§†é¢‘åºåˆ—äº§ç”Ÿçš„æµ·é‡è§†è§‰Tokené™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚
2. FLoCåŸºäºè®¾æ–½é€‰å€å‡½æ•°ï¼Œåˆ©ç”¨æ‡’æƒ°è´ªå©ªç®—æ³•é«˜æ•ˆé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§å’Œå¤šæ ·æ€§çš„è§†è§‰Tokenå­é›†ï¼Œå®ç°Tokenå‹ç¼©ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFLoCåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰å‹ç¼©æŠ€æœ¯ï¼Œå±•ç°äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºFLoCï¼Œä¸€ç§é«˜æ•ˆçš„è§†è§‰Tokenå‹ç¼©æ¡†æ¶ï¼Œç”¨äºé•¿è§†é¢‘ç†è§£ã€‚è¯¥æ¡†æ¶åŸºäºè®¾æ–½é€‰å€å‡½æ•°ï¼Œé€šè¿‡åœ¨é¢„å®šä¹‰çš„Tokenæ•°é‡é¢„ç®—å†…ï¼Œå¿«é€Ÿé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§å’Œå¤šæ ·æ€§çš„è§†è§‰Tokenå­é›†ã€‚é€šè¿‡é›†æˆæ‡’æƒ°è´ªå©ªç®—æ³•ï¼ŒFLoCèƒ½å¤Ÿé«˜æ•ˆåœ°é€‰æ‹©ç´§å‡‘çš„Tokenå­é›†ï¼Œæ˜¾è‘—å‡å°‘è§†è§‰Tokençš„æ•°é‡ï¼ŒåŒæ—¶ä¿è¯æ¥è¿‘æœ€ä¼˜çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œä¸æ¨¡å‹å’ŒæŸ¥è¯¢æ— å…³ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å„ç§è§†é¢‘-LLMå’Œç°æœ‰å·¥ä½œæµç¨‹ä¸­ã€‚åœ¨Video-MMEã€MLVUå’ŒLongVideoBenchç­‰å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°è¡¨æ˜ï¼ŒFLoCå§‹ç»ˆä¼˜äºç°æœ‰çš„å‹ç¼©æŠ€æœ¯ï¼Œçªæ˜¾äº†å…¶åœ¨è§£å†³é•¿è§†é¢‘ç†è§£å…³é”®æŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€é²æ£’æ€§å’Œå¤„ç†é€Ÿåº¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼Œç”±é•¿è§†é¢‘åºåˆ—äº§ç”Ÿçš„è§†è§‰Tokenæ•°é‡å·¨å¤§ï¼Œä¸¥é‡é™åˆ¶äº†ç°æœ‰è§†é¢‘-LLMæ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å‹ç¼©è§†è§‰Tokenæ—¶ï¼Œå¯èƒ½æ— æ³•å¾ˆå¥½åœ°ä¿ç•™è§†é¢‘çš„å…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFLoCçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰Tokenå‹ç¼©é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªè®¾æ–½é€‰å€é—®é¢˜ã€‚æ¯ä¸ªè§†è§‰Tokenè¢«è§†ä¸ºä¸€ä¸ªæ½œåœ¨çš„â€œå®¢æˆ·â€ï¼Œè€Œé€‰æ‹©çš„Tokenå­é›†åˆ™ä»£è¡¨â€œè®¾æ–½â€ã€‚ç›®æ ‡æ˜¯é€‰æ‹©ä¸€ç»„â€œè®¾æ–½â€ï¼Œä½¿å¾—æ‰€æœ‰â€œå®¢æˆ·â€åˆ°å…¶æœ€è¿‘â€œè®¾æ–½â€çš„è·ç¦»ä¹‹å’Œæœ€å°ï¼Œä»è€Œä¿è¯é€‰æ‹©çš„Tokenå­é›†å…·æœ‰ä»£è¡¨æ€§å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFLoCæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) **ç‰¹å¾æå–**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼ˆä¾‹å¦‚ViTï¼‰æå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ï¼Œå¾—åˆ°ä¸€ç³»åˆ—è§†è§‰Tokenã€‚2) **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šè®¡ç®—æ‰€æœ‰è§†è§‰Tokenä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µã€‚3) **è®¾æ–½é€‰å€**ï¼šä½¿ç”¨æ‡’æƒ°è´ªå©ªç®—æ³•ï¼ŒåŸºäºè®¾æ–½é€‰å€å‡½æ•°ï¼Œä»æ‰€æœ‰Tokenä¸­é€‰æ‹©ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§å’Œå¤šæ ·æ€§çš„å­é›†ã€‚4) **Tokenå‹ç¼©**ï¼šå°†é€‰æ‹©çš„Tokenå­é›†è¾“å…¥åˆ°ä¸‹æ¸¸çš„è§†é¢‘-LLMæ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šFLoCçš„å…³é”®åˆ›æ–°åœ¨äºå°†è§†è§‰Tokenå‹ç¼©é—®é¢˜å»ºæ¨¡ä¸ºè®¾æ–½é€‰å€é—®é¢˜ï¼Œå¹¶åˆ©ç”¨æ‡’æƒ°è´ªå©ªç®—æ³•é«˜æ•ˆåœ°è§£å†³è¯¥é—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFLoCæ— éœ€è®­ç»ƒï¼Œä¸æ¨¡å‹å’ŒæŸ¥è¯¢æ— å…³ï¼Œå…·æœ‰æ›´å¥½çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œè®¾æ–½é€‰å€å‡½æ•°èƒ½å¤Ÿä¿è¯é€‰æ‹©çš„Tokenå­é›†å…·æœ‰ä»£è¡¨æ€§å’Œå¤šæ ·æ€§ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™è§†é¢‘çš„å…³é”®ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šFLoCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **è®¾æ–½é€‰å€å‡½æ•°**ï¼šè¯¥å‡½æ•°ç”¨äºè¡¡é‡é€‰æ‹©çš„Tokenå­é›†çš„è´¨é‡ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ‰€æœ‰Tokenåˆ°å…¶æœ€è¿‘é€‰æ‹©Tokençš„è·ç¦»ä¹‹å’Œã€‚2) **æ‡’æƒ°è´ªå©ªç®—æ³•**ï¼šè¯¥ç®—æ³•ç”¨äºé«˜æ•ˆåœ°é€‰æ‹©Tokenå­é›†ï¼Œé¿å…äº†å¯¹æ‰€æœ‰å¯èƒ½çš„å­é›†è¿›è¡Œè¯„ä¼°ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚3) **ç›¸ä¼¼åº¦åº¦é‡**ï¼šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¡é‡è§†è§‰Tokenä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Video-MMEã€MLVUå’ŒLongVideoBenchç­‰å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFLoCåœ¨å„ç§Tokenå‹ç¼©æ¯”ä¾‹ä¸‹å‡ä¼˜äºç°æœ‰çš„å‹ç¼©æŠ€æœ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨Video-MMEæ•°æ®é›†ä¸Šï¼ŒFLoCåœ¨å‹ç¼©æ¯”ä¾‹ä¸º50%æ—¶ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•2-3ä¸ªç™¾åˆ†ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFLoCèƒ½å¤Ÿæœ‰æ•ˆåœ°å‹ç¼©è§†è§‰Tokenï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„é•¿è§†é¢‘ç†è§£æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FLoCå¯åº”ç”¨äºå„ç§é•¿è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘é—®ç­”ã€è§†é¢‘æ‘˜è¦ã€è§†é¢‘æ£€ç´¢ç­‰ã€‚é€šè¿‡é«˜æ•ˆå‹ç¼©è§†è§‰Tokenï¼ŒFLoCèƒ½å¤Ÿæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜éœ€æ±‚ï¼Œä½¿å¾—è§†é¢‘-LLMæ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è§†é¢‘åºåˆ—ï¼Œä»è€Œæé«˜é•¿è§†é¢‘ç†è§£çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è¿˜å¯ç”¨äºç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¡ç®—ç­‰èµ„æºå—é™çš„åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.

