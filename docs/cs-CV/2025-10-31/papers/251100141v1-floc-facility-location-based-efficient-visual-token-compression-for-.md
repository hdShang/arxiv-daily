---
layout: default
title: FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding
---

# FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.00141" target="_blank" class="toolbar-btn">arXiv: 2511.00141v1</a>
    <a href="https://arxiv.org/pdf/2511.00141.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00141v1" 
            onclick="toggleFavorite(this, '2511.00141v1', 'FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-31

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**FLoCÔºöÂü∫‰∫éËÆæÊñΩÈÄâÂùÄÁöÑÈïøËßÜÈ¢ëÈ´òÊïàËßÜËßâTokenÂéãÁº©ÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøËßÜÈ¢ëÁêÜËß£` `ËßÜËßâTokenÂéãÁº©` `ËÆæÊñΩÈÄâÂùÄ` `ÊáíÊÉ∞Ë¥™Â©™ÁÆóÊ≥ï` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÈïøËßÜÈ¢ëÁêÜËß£‰æùËµñÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºå‰ΩÜÈïøËßÜÈ¢ëÂ∫èÂàó‰∫ßÁîüÁöÑÊµ∑ÈáèËßÜËßâTokenÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ
2. FLoCÂü∫‰∫éËÆæÊñΩÈÄâÂùÄÂáΩÊï∞ÔºåÂà©Áî®ÊáíÊÉ∞Ë¥™Â©™ÁÆóÊ≥ïÈ´òÊïàÈÄâÊã©ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÂíåÂ§öÊ†∑ÊÄßÁöÑËßÜËßâTokenÂ≠êÈõÜÔºåÂÆûÁé∞TokenÂéãÁº©„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåFLoCÂú®Â§ö‰∏™ÈïøËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂äÁé∞ÊúâÂéãÁº©ÊäÄÊúØÔºåÂ±ïÁé∞‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫FLoCÔºå‰∏ÄÁßçÈ´òÊïàÁöÑËßÜËßâTokenÂéãÁº©Ê°ÜÊû∂ÔºåÁî®‰∫éÈïøËßÜÈ¢ëÁêÜËß£„ÄÇËØ•Ê°ÜÊû∂Âü∫‰∫éËÆæÊñΩÈÄâÂùÄÂáΩÊï∞ÔºåÈÄöËøáÂú®È¢ÑÂÆö‰πâÁöÑTokenÊï∞ÈáèÈ¢ÑÁÆóÂÜÖÔºåÂø´ÈÄüÈÄâÊã©ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÂíåÂ§öÊ†∑ÊÄßÁöÑËßÜËßâTokenÂ≠êÈõÜ„ÄÇÈÄöËøáÈõÜÊàêÊáíÊÉ∞Ë¥™Â©™ÁÆóÊ≥ïÔºåFLoCËÉΩÂ§üÈ´òÊïàÂú∞ÈÄâÊã©Á¥ßÂáëÁöÑTokenÂ≠êÈõÜÔºåÊòæËëóÂáèÂ∞ëËßÜËßâTokenÁöÑÊï∞ÈáèÔºåÂêåÊó∂‰øùËØÅÊé•ËøëÊúÄ‰ºòÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÊó†ÈúÄËÆ≠ÁªÉÔºå‰∏éÊ®°ÂûãÂíåÊü•ËØ¢Êó†ÂÖ≥ÔºåËÉΩÂ§üÊó†ÁºùÈõÜÊàêÂà∞ÂêÑÁßçËßÜÈ¢ë-LLMÂíåÁé∞ÊúâÂ∑•‰ΩúÊµÅÁ®ã‰∏≠„ÄÇÂú®Video-MME„ÄÅMLVUÂíåLongVideoBenchÁ≠âÂ§ßËßÑÊ®°Âü∫ÂáÜÊµãËØï‰∏≠ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåFLoCÂßãÁªà‰ºò‰∫éÁé∞ÊúâÁöÑÂéãÁº©ÊäÄÊúØÔºåÁ™ÅÊòæ‰∫ÜÂÖ∂Âú®Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÁêÜËß£ÂÖ≥ÈîÆÊåëÊàòÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂíåÂ§ÑÁêÜÈÄüÂ∫¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°‰∏≠ÔºåÁî±ÈïøËßÜÈ¢ëÂ∫èÂàó‰∫ßÁîüÁöÑËßÜËßâTokenÊï∞ÈáèÂ∑®Â§ßÔºå‰∏•ÈáçÈôêÂà∂‰∫ÜÁé∞ÊúâËßÜÈ¢ë-LLMÊ®°ÂûãÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÂéãÁº©ËßÜËßâTokenÊó∂ÔºåÂèØËÉΩÊó†Ê≥ïÂæàÂ•ΩÂú∞‰øùÁïôËßÜÈ¢ëÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöFLoCÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËßÜËßâTokenÂéãÁº©ÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™ËÆæÊñΩÈÄâÂùÄÈóÆÈ¢ò„ÄÇÊØè‰∏™ËßÜËßâTokenË¢´ËßÜ‰∏∫‰∏Ä‰∏™ÊΩúÂú®ÁöÑ‚ÄúÂÆ¢Êà∑‚ÄùÔºåËÄåÈÄâÊã©ÁöÑTokenÂ≠êÈõÜÂàô‰ª£Ë°®‚ÄúËÆæÊñΩ‚Äù„ÄÇÁõÆÊ†áÊòØÈÄâÊã©‰∏ÄÁªÑ‚ÄúËÆæÊñΩ‚ÄùÔºå‰ΩøÂæóÊâÄÊúâ‚ÄúÂÆ¢Êà∑‚ÄùÂà∞ÂÖ∂ÊúÄËøë‚ÄúËÆæÊñΩ‚ÄùÁöÑË∑ùÁ¶ª‰πãÂíåÊúÄÂ∞èÔºå‰ªéËÄå‰øùËØÅÈÄâÊã©ÁöÑTokenÂ≠êÈõÜÂÖ∑Êúâ‰ª£Ë°®ÊÄßÂíåÂ§öÊ†∑ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöFLoCÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö1) **ÁâπÂæÅÊèêÂèñ**Ôºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÁºñÁ†ÅÂô®Ôºà‰æãÂ¶ÇViTÔºâÊèêÂèñËßÜÈ¢ëÂ∏ßÁöÑËßÜËßâÁâπÂæÅÔºåÂæóÂà∞‰∏ÄÁ≥ªÂàóËßÜËßâToken„ÄÇ2) **Áõ∏‰ººÂ∫¶ËÆ°ÁÆó**ÔºöËÆ°ÁÆóÊâÄÊúâËßÜËßâToken‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶Áü©Èòµ„ÄÇ3) **ËÆæÊñΩÈÄâÂùÄ**Ôºö‰ΩøÁî®ÊáíÊÉ∞Ë¥™Â©™ÁÆóÊ≥ïÔºåÂü∫‰∫éËÆæÊñΩÈÄâÂùÄÂáΩÊï∞Ôºå‰ªéÊâÄÊúâToken‰∏≠ÈÄâÊã©‰∏Ä‰∏™ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÂíåÂ§öÊ†∑ÊÄßÁöÑÂ≠êÈõÜ„ÄÇ4) **TokenÂéãÁº©**ÔºöÂ∞ÜÈÄâÊã©ÁöÑTokenÂ≠êÈõÜËæìÂÖ•Âà∞‰∏ãÊ∏∏ÁöÑËßÜÈ¢ë-LLMÊ®°Âûã‰∏≠ËøõË°åÂ§ÑÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöFLoCÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜËßÜËßâTokenÂéãÁº©ÈóÆÈ¢òÂª∫Ê®°‰∏∫ËÆæÊñΩÈÄâÂùÄÈóÆÈ¢òÔºåÂπ∂Âà©Áî®ÊáíÊÉ∞Ë¥™Â©™ÁÆóÊ≥ïÈ´òÊïàÂú∞Ëß£ÂÜ≥ËØ•ÈóÆÈ¢ò„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåFLoCÊó†ÈúÄËÆ≠ÁªÉÔºå‰∏éÊ®°ÂûãÂíåÊü•ËØ¢Êó†ÂÖ≥ÔºåÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÈÄöÁî®ÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇÊ≠§Â§ñÔºåËÆæÊñΩÈÄâÂùÄÂáΩÊï∞ËÉΩÂ§ü‰øùËØÅÈÄâÊã©ÁöÑTokenÂ≠êÈõÜÂÖ∑Êúâ‰ª£Ë°®ÊÄßÂíåÂ§öÊ†∑ÊÄßÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞‰øùÁïôËßÜÈ¢ëÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöFLoCÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) **ËÆæÊñΩÈÄâÂùÄÂáΩÊï∞**ÔºöËØ•ÂáΩÊï∞Áî®‰∫éË°°ÈáèÈÄâÊã©ÁöÑTokenÂ≠êÈõÜÁöÑË¥®ÈáèÔºåÁõÆÊ†áÊòØÊúÄÂ∞èÂåñÊâÄÊúâTokenÂà∞ÂÖ∂ÊúÄËøëÈÄâÊã©TokenÁöÑË∑ùÁ¶ª‰πãÂíå„ÄÇ2) **ÊáíÊÉ∞Ë¥™Â©™ÁÆóÊ≥ï**ÔºöËØ•ÁÆóÊ≥ïÁî®‰∫éÈ´òÊïàÂú∞ÈÄâÊã©TokenÂ≠êÈõÜÔºåÈÅøÂÖç‰∫ÜÂØπÊâÄÊúâÂèØËÉΩÁöÑÂ≠êÈõÜËøõË°åËØÑ‰º∞ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ3) **Áõ∏‰ººÂ∫¶Â∫¶Èáè**Ôºö‰ΩøÁî®‰ΩôÂº¶Áõ∏‰ººÂ∫¶Êù•Ë°°ÈáèËßÜËßâToken‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Video-MME„ÄÅMLVUÂíåLongVideoBenchÁ≠âÂ§ßËßÑÊ®°Âü∫ÂáÜÊµãËØï‰∏≠ÔºåFLoCÂú®ÂêÑÁßçTokenÂéãÁº©ÊØî‰æã‰∏ãÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÂéãÁº©ÊäÄÊúØ„ÄÇ‰æãÂ¶ÇÔºåÂú®Video-MMEÊï∞ÊçÆÈõÜ‰∏äÔºåFLoCÂú®ÂéãÁº©ÊØî‰æã‰∏∫50%Êó∂ÔºåÊÄßËÉΩ‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï2-3‰∏™ÁôæÂàÜÁÇπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFLoCËÉΩÂ§üÊúâÊïàÂú∞ÂéãÁº©ËßÜËßâTokenÔºåÂêåÊó∂‰øùÊåÅËæÉÈ´òÁöÑÈïøËßÜÈ¢ëÁêÜËß£ÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

FLoCÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜÈ¢ëÈóÆÁ≠î„ÄÅËßÜÈ¢ëÊëòË¶Å„ÄÅËßÜÈ¢ëÊ£ÄÁ¥¢Á≠â„ÄÇÈÄöËøáÈ´òÊïàÂéãÁº©ËßÜËßâTokenÔºåFLoCËÉΩÂ§üÊòæËëóÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÂíåÂÜÖÂ≠òÈúÄÊ±ÇÔºå‰ΩøÂæóËßÜÈ¢ë-LLMÊ®°ÂûãËÉΩÂ§üÂ§ÑÁêÜÊõ¥ÈïøÁöÑËßÜÈ¢ëÂ∫èÂàóÔºå‰ªéËÄåÊèêÈ´òÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïËøòÂèØÁî®‰∫éÁßªÂä®ËÆæÂ§áÊàñËæπÁºòËÆ°ÁÆóÁ≠âËµÑÊ∫êÂèóÈôêÁöÑÂú∫ÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.

