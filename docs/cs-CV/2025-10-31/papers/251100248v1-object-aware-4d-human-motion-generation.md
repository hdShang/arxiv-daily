---
layout: default
title: Object-Aware 4D Human Motion Generation
---

# Object-Aware 4D Human Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.00248" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.00248v1</a>
  <a href="https://arxiv.org/pdf/2511.00248.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00248v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.00248v1', 'Object-Aware 4D Human Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shurui Gui, Deep Anil Patel, Xiner Li, Martin Renqiang Min

**åˆ†ç±»**: cs.CV, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMSDIæ¡†æ¶ï¼Œåˆ©ç”¨è¿åŠ¨æ‰©æ•£å…ˆéªŒç”Ÿæˆé€¼çœŸä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„4Däººä½“è¿åŠ¨**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `4Däººä½“è¿åŠ¨ç”Ÿæˆ` `è¿åŠ¨æ‰©æ•£æ¨¡å‹` `3Dé«˜æ–¯è¡¨ç¤º` `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆ†æ•°æç‚¼` `å¯¹è±¡æ„ŸçŸ¥` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•ç¼ºä¹3Dç‰©ç†å…ˆéªŒï¼Œå¯¼è‡´ç”Ÿæˆçš„äººä½“è¿åŠ¨ä¸çœŸå®ã€è¿åè¯­ä¹‰å’Œç‰©ç†è§„å¾‹ã€‚
2. æå‡ºMSDIæ¡†æ¶ï¼Œç»“åˆ3Dé«˜æ–¯è¡¨ç¤ºã€è¿åŠ¨æ‰©æ•£å…ˆéªŒå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°å¯¹è±¡æ„ŸçŸ¥çš„4Däººä½“è¿åŠ¨ç”Ÿæˆã€‚
3. MSDIæ— éœ€é‡æ–°è®­ç»ƒï¼Œå³å¯ç”Ÿæˆè‡ªç„¶ä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„äººä½“è¿åŠ¨ï¼Œåœ¨3Dç©ºé—´ä¸Šä¸‹æ–‡ä¸­è¡¨ç°è‰¯å¥½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›è§†é¢‘ä»ç„¶å­˜åœ¨ä¸çœŸå®çš„å˜å½¢ã€è¯­ä¹‰è¿èƒŒå’Œç‰©ç†ä¸ä¸€è‡´æ€§ï¼Œè¿™ä¸»è¦æºäºç¼ºä¹3Dç‰©ç†å…ˆéªŒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäº3Dé«˜æ–¯è¡¨ç¤ºå’Œè¿åŠ¨æ‰©æ•£å…ˆéªŒçš„ã€å…·æœ‰å¯¹è±¡æ„ŸçŸ¥çš„4Däººä½“è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œåä¸ºè¿åŠ¨åˆ†æ•°æç‚¼äº¤äº’ï¼ˆMSDIï¼‰ã€‚MSDIåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ç©ºé—´å’Œæç¤ºè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æå‡ºçš„è¿åŠ¨æ‰©æ•£åˆ†æ•°æç‚¼é‡‡æ ·ï¼ˆMSDSï¼‰åˆ©ç”¨è¿åŠ¨å…ˆéªŒã€‚MSDSä¸LLMçš„ç»“åˆå®ç°äº†æˆ‘ä»¬çš„ç©ºé—´æ„ŸçŸ¥è¿åŠ¨ä¼˜åŒ–ï¼Œè¯¥ä¼˜åŒ–ä»é¢„è®­ç»ƒçš„è¿åŠ¨æ‰©æ•£æ¨¡å‹ä¸­æå–åˆ†æ•°æ¢¯åº¦ï¼Œä»¥åœ¨å°Šé‡å¯¹è±¡å’Œè¯­ä¹‰çº¦æŸçš„åŒæ—¶ç»†åŒ–äººä½“è¿åŠ¨ã€‚ä¸ä»¥å¾€éœ€è¦åœ¨æœ‰é™çš„äº¤äº’æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬æ–¹æ³•é¿å…äº†é‡æ–°è®­ç»ƒï¼Œå¹¶æ¨å¹¿åˆ°åˆ†å¸ƒå¤–çš„å¯¹è±¡æ„ŸçŸ¥äººä½“è¿åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ç”Ÿæˆè‡ªç„¶ä¸”ç‰©ç†ä¸Šåˆç†çš„äººä½“è¿åŠ¨ï¼Œå°Šé‡3Dç©ºé—´ä¸Šä¸‹æ–‡ï¼Œä¸ºé€¼çœŸçš„4Dç”Ÿæˆæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆäººä½“è¿åŠ¨æ—¶ï¼Œç”±äºç¼ºä¹3Dç‰©ç†å…ˆéªŒï¼Œå®¹æ˜“å‡ºç°ä¸çœŸå®çš„å½¢å˜ã€è¯­ä¹‰è¿èƒŒå’Œç‰©ç†ä¸ä¸€è‡´æ€§ã€‚ä»¥å¾€æ–¹æ³•é€šå¸¸éœ€è¦åœ¨ç‰¹å®šäº¤äº’æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿç”Ÿæˆé€¼çœŸã€ç¬¦åˆç‰©ç†è§„å¾‹ä¸”å…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›çš„4Däººä½“è¿åŠ¨ç”Ÿæˆæ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMSDIçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„è¿åŠ¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡åˆ†æ•°æç‚¼çš„æ–¹å¼ä¼˜åŒ–äººä½“è¿åŠ¨ã€‚é€šè¿‡3Dé«˜æ–¯è¡¨ç¤ºå¯¹äººä½“å’Œç‰©ä½“è¿›è¡Œå»ºæ¨¡ï¼Œä¿è¯äº†ç©ºé—´ä¸€è‡´æ€§ã€‚è¿™ç§æ–¹æ³•é¿å…äº†åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä»è€Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMSDIæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆ3Däººä½“å’Œç‰©ä½“ï¼›2) åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–åœºæ™¯çš„ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼›3) é€šè¿‡è¿åŠ¨æ‰©æ•£åˆ†æ•°æç‚¼é‡‡æ ·ï¼ˆMSDSï¼‰ä»é¢„è®­ç»ƒçš„è¿åŠ¨æ‰©æ•£æ¨¡å‹ä¸­æå–åˆ†æ•°æ¢¯åº¦ï¼›4) åˆ©ç”¨æå–çš„åˆ†æ•°æ¢¯åº¦ä¼˜åŒ–äººä½“è¿åŠ¨ï¼Œä½¿å…¶ç¬¦åˆç‰©ä½“å’Œè¯­ä¹‰çº¦æŸã€‚

**å…³é”®åˆ›æ–°**ï¼šMSDIçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è¿åŠ¨æ‰©æ•£åˆ†æ•°æç‚¼é‡‡æ ·ï¼ˆMSDSï¼‰æ–¹æ³•ã€‚MSDSèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»é¢„è®­ç»ƒçš„è¿åŠ¨æ‰©æ•£æ¨¡å‹ä¸­æå–åˆ†æ•°æ¢¯åº¦ï¼Œå¹¶å°†å…¶ç”¨äºä¼˜åŒ–äººä½“è¿åŠ¨ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹è¿åŠ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMSDIæ¡†æ¶ç»“åˆäº†3Dé«˜æ–¯è¡¨ç¤ºå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†å¯¹è±¡æ„ŸçŸ¥çš„è¿åŠ¨ç”Ÿæˆã€‚

**å…³é”®è®¾è®¡**ï¼šMSDIä½¿ç”¨3Dé«˜æ–¯è¡¨ç¤ºæ¥å»ºæ¨¡äººä½“å’Œç‰©ä½“ï¼Œä¿è¯äº†ç©ºé—´ä¸€è‡´æ€§ã€‚è¿åŠ¨æ‰©æ•£æ¨¡å‹é‡‡ç”¨æ ‡å‡†çš„æ‰©æ•£æ¨¡å‹ç»“æ„ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„æƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚æŸå¤±å‡½æ•°ä¸»è¦åŒ…æ‹¬è¿åŠ¨æŸå¤±ã€ç‰©ä½“äº¤äº’æŸå¤±å’Œè¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ã€‚è¿åŠ¨æŸå¤±ç”¨äºä¿è¯è¿åŠ¨çš„è‡ªç„¶æ€§ï¼Œç‰©ä½“äº¤äº’æŸå¤±ç”¨äºä¿è¯äººä½“ä¸ç‰©ä½“ä¹‹é—´çš„ç‰©ç†äº¤äº’ï¼Œè¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ç”¨äºä¿è¯è¿åŠ¨ä¸åœºæ™¯è¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMSDIæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆè‡ªç„¶ä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„äººä½“è¿åŠ¨ï¼Œå¹¶åœ¨3Dç©ºé—´ä¸Šä¸‹æ–‡ä¸­è¡¨ç°è‰¯å¥½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMSDIæ— éœ€åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶é›¶æ ·æœ¬ç‰¹æ€§å’Œå¯¹3Dç©ºé—´ä¸Šä¸‹æ–‡çš„å°Šé‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MSDIæ¡†æ¶å¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆã€ç”µå½±åˆ¶ä½œç­‰é¢†åŸŸï¼Œç”Ÿæˆé€¼çœŸä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„è™šæ‹Ÿäººç‰©è¿åŠ¨ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿæå‡ç”¨æˆ·åœ¨è™šæ‹Ÿç¯å¢ƒä¸­çš„æ²‰æµ¸æ„Ÿå’Œäº¤äº’ä½“éªŒï¼Œå¹¶ä¸ºå†…å®¹åˆ›ä½œè€…æä¾›æ›´é«˜æ•ˆçš„åŠ¨ç”»åˆ¶ä½œå·¥å…·ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæœºå™¨äººæ§åˆ¶é¢†åŸŸï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ä¸äººç±»è¿›è¡Œäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.

