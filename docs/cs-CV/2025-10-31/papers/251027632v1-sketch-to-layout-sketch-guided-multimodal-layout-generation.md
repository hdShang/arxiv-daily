---
layout: default
title: Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation
---

# Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.27632" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.27632v1</a>
  <a href="https://arxiv.org/pdf/2510.27632.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27632v1" onclick="toggleFavorite(this, '2510.27632v1', 'Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Riccardo Brioschi, Aleksandr Alekseev, Emanuele Nevali, Berkay DÃ¶ner, Omar El Malki, Blagoj Mitrevski, Leandro Kieliger, Mark Collier, Andrii Maksai, Jesse Berent, Claudiu Musat, Efi Kokiopoulou

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

**å¤‡æ³¨**: 15 pages, 18 figures, GitHub link: https://github.com/google-deepmind/sketch_to_layout, accept at ICCV 2025 Workshop (HiGen)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/google-deepmind/sketch_to_layout)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSketch-to-Layoutæ¡†æ¶ï¼Œåˆ©ç”¨è‰å›¾å¼•å¯¼å¤šæ¨¡æ€å¸ƒå±€ç”Ÿæˆï¼Œæå‡è®¾è®¡ä½“éªŒã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¸ƒå±€ç”Ÿæˆ` `è‰å›¾å¼•å¯¼` `å¤šæ¨¡æ€å­¦ä¹ ` `Transformer` `åˆæˆæ•°æ®` `ç”¨æˆ·çº¦æŸ` `å›¾å½¢è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¸ƒå±€ç”Ÿæˆæ–¹æ³•ä¾èµ–å¤æ‚çš„ç”¨æˆ·çº¦æŸï¼Œé™ä½äº†æ˜“ç”¨æ€§ï¼Œéš¾ä»¥æ»¡è¶³ç”¨æˆ·ç›´è§‚çš„è®¾è®¡éœ€æ±‚ã€‚
2. æå‡ºSketch-to-Layoutæ¡†æ¶ï¼Œåˆ©ç”¨ç”¨æˆ·è‰å›¾ä½œä¸ºç›´è§‚çº¦æŸï¼ŒæŒ‡å¯¼å¤šæ¨¡æ€Transformerç”Ÿæˆé«˜è´¨é‡å¸ƒå±€ã€‚
3. é€šè¿‡åˆæˆè‰å›¾æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨PubLayNetç­‰æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œè¶…è¶Šç°æœ‰çº¦æŸæ–¹æ³•ï¼Œæå‡è®¾è®¡ä½“éªŒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›¾å½¢å¸ƒå±€ç”Ÿæˆæ˜¯ä¸€ä¸ªæ–°å…´çš„ç ”ç©¶é¢†åŸŸï¼Œä¸“æ³¨äºç”Ÿæˆä»æµ·æŠ¥è®¾è®¡åˆ°æ–‡æ¡£ç­‰ç¾è§‚çš„å¸ƒå±€ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†ç»“åˆç”¨æˆ·çº¦æŸæ¥æŒ‡å¯¼å¸ƒå±€ç”Ÿæˆçš„æ–¹æ³•ï¼Œä½†è¿™äº›çº¦æŸé€šå¸¸éœ€è¦å¤æ‚çš„è§„èŒƒï¼Œä»è€Œé™ä½äº†å¯ç”¨æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç”¨æˆ·æä¾›çš„è‰å›¾ä½œä¸ºç›´è§‚çš„çº¦æŸï¼Œå¹¶ä»ç»éªŒä¸Šè¯æ˜äº†è¿™ç§æ–°æŒ‡å¯¼æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œå°†è‰å›¾åˆ°å¸ƒå±€é—®é¢˜ç¡®ç«‹ä¸ºä¸€ä¸ªæœ‰å‰æ™¯ä½†ç›®å‰å°šæœªå……åˆ†æ¢ç´¢çš„ç ”ç©¶æ–¹å‘ã€‚ä¸ºäº†è§£å†³è‰å›¾åˆ°å¸ƒå±€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€Transformerçš„è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨è‰å›¾å’Œå†…å®¹èµ„äº§ä½œä¸ºè¾“å…¥æ¥ç”Ÿæˆé«˜è´¨é‡çš„å¸ƒå±€ã€‚ç”±äºä»äººå·¥æ ‡æ³¨è€…é‚£é‡Œæ”¶é›†è‰å›¾è®­ç»ƒæ•°æ®æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹æˆæœ¬å¾ˆé«˜ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥å¤§è§„æ¨¡åœ°åˆæˆç”Ÿæˆè®­ç»ƒè‰å›¾ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†PubLayNetã€DocLayNetå’ŒSlidesVQAä¸Šè®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œç»“æœè¡¨æ˜å®ƒä¼˜äºæœ€å…ˆè¿›çš„åŸºäºçº¦æŸçš„æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†æ›´ç›´è§‚çš„è®¾è®¡ä½“éªŒã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥çš„è‰å›¾åˆ°å¸ƒå±€ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸Šè¿°å…¬å…±æ•°æ®é›†çš„O(200k)ä¸ªåˆæˆç”Ÿæˆçš„è‰å›¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å›¾å½¢å¸ƒå±€ç”Ÿæˆä¸­ç”¨æˆ·çº¦æŸå¤æ‚ã€ä¸æ˜“ç”¨çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éœ€è¦ç”¨æˆ·è¿›è¡Œå¤æ‚çš„å‚æ•°è®¾ç½®æˆ–è§„åˆ™å®šä¹‰ï¼Œæ— æ³•ç›´æ¥è¡¨è¾¾è®¾è®¡æ„å›¾ï¼Œé™åˆ¶äº†å¸ƒå±€ç”Ÿæˆç³»ç»Ÿçš„å¯ç”¨æ€§å’Œç”¨æˆ·ä½“éªŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç”¨æˆ·æä¾›çš„è‰å›¾ä½œä¸ºå¸ƒå±€ç”Ÿæˆçš„ç›´è§‚çº¦æŸã€‚è‰å›¾èƒ½å¤Ÿç®€æ´æ˜äº†åœ°è¡¨è¾¾ç”¨æˆ·å¯¹å¸ƒå±€çš„æœŸæœ›ï¼Œé¿å…äº†å¤æ‚çš„å‚æ•°è®¾ç½®ã€‚é€šè¿‡å°†è‰å›¾ä¸å†…å®¹èµ„äº§ç›¸ç»“åˆï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾ä¸”ç¾è§‚çš„å¸ƒå±€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é‡‡ç”¨åŸºäºTransformerçš„å¤šæ¨¡æ€æ¶æ„ã€‚è¾“å…¥åŒ…æ‹¬ç”¨æˆ·è‰å›¾å’Œå†…å®¹èµ„äº§ï¼ˆä¾‹å¦‚æ–‡æœ¬ã€å›¾åƒï¼‰ã€‚æ¨¡å‹é¦–å…ˆå¯¹è‰å›¾å’Œå†…å®¹è¿›è¡Œç¼–ç ï¼Œç„¶åé€šè¿‡Transformerè¿›è¡Œèåˆå’Œå¸ƒå±€é¢„æµ‹ã€‚è¾“å‡ºæ˜¯å¸ƒå±€ä¸­å„ä¸ªå…ƒç´ çš„ä½ç½®å’Œå¤§å°ç­‰ä¿¡æ¯ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬è‰å›¾è¾“å…¥ã€å†…å®¹è¾“å…¥ã€ç‰¹å¾ç¼–ç ã€Transformerèåˆã€å¸ƒå±€é¢„æµ‹ç­‰é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ç”¨æˆ·è‰å›¾ä½œä¸ºå¸ƒå±€ç”Ÿæˆçš„ç›´æ¥çº¦æŸã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è‰å›¾æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åˆæˆè‰å›¾ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚è¿™ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•æ˜¯è®­ç»ƒæ¨¡å‹çš„é‡è¦ä¿éšœã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨Transformeræ¶æ„ï¼Œå…·ä½“ç½‘ç»œç»“æ„ç»†èŠ‚æœªè¯¦ç»†æè¿°ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ç›®æ ‡æ˜¯ä½¿ç”Ÿæˆçš„å¸ƒå±€ä¸ç”¨æˆ·è‰å›¾å°½å¯èƒ½ä¸€è‡´ï¼ŒåŒæ—¶ä¿è¯å¸ƒå±€çš„ç¾è§‚æ€§å’Œåˆç†æ€§ã€‚åˆæˆè‰å›¾ç”Ÿæˆæ–¹æ³•å¯èƒ½æ¶‰åŠéšæœºæ‰°åŠ¨ã€å½¢çŠ¶ç»„åˆç­‰æŠ€æœ¯ï¼Œå…·ä½“ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ¨¡å‹åœ¨PubLayNetã€DocLayNetå’ŒSlidesVQAä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„åŸºäºçº¦æŸçš„å¸ƒå±€ç”Ÿæˆæ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†è¯¥æ–¹æ³•åœ¨æä¾›æ›´ç›´è§‚è®¾è®¡ä½“éªŒæ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæµ·æŠ¥è®¾è®¡ã€æ–‡æ¡£æ’ç‰ˆã€å¹»ç¯ç‰‡åˆ¶ä½œç­‰é¢†åŸŸã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„è‰å›¾å¿«é€Ÿç”Ÿæˆç¬¦åˆéœ€æ±‚çš„å¸ƒå±€ï¼Œé™ä½è®¾è®¡é—¨æ§›ï¼Œæé«˜è®¾è®¡æ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›é›†æˆåˆ°å„ç§è®¾è®¡è½¯ä»¶å’Œå¹³å°ä¸­ï¼Œèµ‹èƒ½æ›´å¹¿æ³›çš„ç”¨æˆ·ç¾¤ä½“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Graphic layout generation is a growing research area focusing on generating aesthetically pleasing layouts ranging from poster designs to documents. While recent research has explored ways to incorporate user constraints to guide the layout generation, these constraints often require complex specifications which reduce usability. We introduce an innovative approach exploiting user-provided sketches as intuitive constraints and we demonstrate empirically the effectiveness of this new guidance method, establishing the sketch-to-layout problem as a promising research direction, which is currently under-explored. To tackle the sketch-to-layout problem, we propose a multimodal transformer-based solution using the sketch and the content assets as inputs to produce high quality layouts. Since collecting sketch training data from human annotators to train our model is very costly, we introduce a novel and efficient method to synthetically generate training sketches at scale. We train and evaluate our model on three publicly available datasets: PubLayNet, DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art constraint-based methods, while offering a more intuitive design experience. In order to facilitate future sketch-to-layout research, we release O(200k) synthetically-generated sketches for the public datasets above. The datasets are available at https://github.com/google-deepmind/sketch_to_layout.

