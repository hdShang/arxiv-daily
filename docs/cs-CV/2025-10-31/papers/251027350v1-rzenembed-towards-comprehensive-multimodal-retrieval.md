---
layout: default
title: RzenEmbed: Towards Comprehensive Multimodal Retrieval
---

# RzenEmbed: Towards Comprehensive Multimodal Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.27350" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.27350v1</a>
  <a href="https://arxiv.org/pdf/2510.27350.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27350v1" onclick="toggleFavorite(this, '2510.27350v1', 'RzenEmbed: Towards Comprehensive Multimodal Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weijian Jian, Yajun Zhang, Dawei Liang, Chunyu Xie, Yixiao He, Dawei Leng, Yuhui Yin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/qihoo360/RzenEmbed)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RzenEmbedï¼šæå‡ºç»Ÿä¸€å¤šæ¨¡æ€åµŒå…¥æ¡†æ¶ï¼Œæ˜¾è‘—æå‡è§†é¢‘å’Œæ–‡æ¡£æ£€ç´¢æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `è·¨æ¨¡æ€å­¦ä¹ ` `è§†é¢‘æ£€ç´¢` `æ–‡æ¡£æ£€ç´¢` `å¯¹æ¯”å­¦ä¹ ` `InfoNCEæŸå¤±` `ç¡¬åº¦åŠ æƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æ£€ç´¢ä¸­å¯¹è§†é¢‘å’Œè§†è§‰æ–‡æ¡£ç­‰æ¨¡æ€æ”¯æŒä¸è¶³ï¼Œé™åˆ¶äº†é€šç”¨åµŒå…¥çš„åº”ç”¨ã€‚
2. RzenEmbedé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç»“åˆç¡¬åº¦åŠ æƒInfoNCEæŸå¤±å’Œå‡é˜´æ€§ç¼“è§£ï¼Œæå‡åˆ¤åˆ«èƒ½åŠ›ã€‚
3. RzenEmbedåœ¨MMEBåŸºå‡†æµ‹è¯•ä¸­å–å¾—SOTAï¼Œå°¤å…¶åœ¨è§†é¢‘å’Œè§†è§‰æ–‡æ¡£æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†åŸºäºCLIPçš„æ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿä¸ºæ£€ç´¢ä»»åŠ¡ç”Ÿæˆå¼ºå¤§çš„é€šç”¨åµŒå…¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºè‡ªç„¶å›¾åƒï¼Œå¯¹è§†é¢‘å’Œè§†è§‰æ–‡æ¡£ç­‰å…¶ä»–å…³é”®è§†è§‰æ¨¡æ€çš„æ”¯æŒæœ‰é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†RzenEmbedï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ è·¨å¤šç§æ¨¡æ€ï¼ˆåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œè§†è§‰æ–‡æ¡£ï¼‰çš„åµŒå…¥ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥å­¦ä¹ åˆ¤åˆ«æ€§è¡¨ç¤ºã€‚ç¬¬ä¸€é˜¶æ®µä¾§é‡äºåŸºç¡€æ–‡æœ¬å’Œå¤šæ¨¡æ€æ£€ç´¢ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„InfoNCEæŸå¤±ï¼Œå…¶ä¸­åŒ…å«ä¸¤é¡¹å…³é”®å¢å¼ºï¼šé¦–å…ˆï¼Œä¸€ç§ç¡¬åº¦åŠ æƒæœºåˆ¶é€šè¿‡åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬åˆ†é…æ›´é«˜çš„æƒé‡æ¥å¼•å¯¼æ¨¡å‹ä¼˜å…ˆå¤„ç†è¿™äº›æ ·æœ¬ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ç§æ–¹æ³•æ¥å‡è½»å‡é˜´æ€§çš„å½±å“å¹¶ç¼“è§£æ•°æ®å™ªå£°ã€‚è¿™ç§ç­–ç•¥ä¸ä»…å¢å¼ºäº†æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œè¿˜æé«˜äº†å…¶æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°å’Œæ¨¡å‹èåˆè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚RzenEmbedåœ¨MMEBåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ–°çš„state-of-the-artã€‚å®ƒä¸ä»…å®ç°äº†æœ€ä½³çš„æ€»ä½“å¾—åˆ†ï¼Œè€Œä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘å’Œè§†è§‰æ–‡æ¡£æ£€ç´¢ä»»åŠ¡ä¸Šä¹Ÿä¼˜äºæ‰€æœ‰å…ˆå‰çš„å·¥ä½œã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯åœ¨https://huggingface.co/qihoo360/RzenEmbed ä¸Šæ‰¾åˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºCLIPçš„å¤šæ¨¡æ€æ£€ç´¢æ–¹æ³•ä¸»è¦é’ˆå¯¹è‡ªç„¶å›¾åƒï¼Œå¿½ç•¥äº†è§†é¢‘å’Œè§†è§‰æ–‡æ¡£ç­‰é‡è¦æ¨¡æ€ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨å¤„ç†è¿™äº›æ¨¡æ€æ—¶æ€§èƒ½ä¸‹é™ï¼Œæ— æ³•æ»¡è¶³é€šç”¨å¤šæ¨¡æ€æ£€ç´¢çš„éœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°å™ªå£°æ•°æ®å’Œå‡é˜´æ€§çš„å½±å“ï¼Œé™ä½äº†æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRzenEmbedçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å­¦ä¹ æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œè§†è§‰æ–‡æ¡£çš„åµŒå…¥è¡¨ç¤ºã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè¿›è¡ŒåŸºç¡€çš„æ–‡æœ¬å’Œå¤šæ¨¡æ€æ£€ç´¢è®­ç»ƒï¼Œç„¶åé€šè¿‡æ”¹è¿›çš„InfoNCEæŸå¤±è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRzenEmbedé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ åŸºç¡€çš„å¤šæ¨¡æ€å¯¹é½ã€‚ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹åœ¨åŒ…å«è§†é¢‘å’Œè§†è§‰æ–‡æ¡£çš„æ›´å¹¿æ³›æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ”¹è¿›çš„InfoNCEæŸå¤±å‡½æ•°ã€‚è¯¥æ¡†æ¶è¿˜åŒ…æ‹¬å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°å’Œæ¨¡å‹èåˆç­‰æŠ€æœ¯ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šRzenEmbedçš„å…³é”®åˆ›æ–°åœ¨äºæ”¹è¿›çš„InfoNCEæŸå¤±å‡½æ•°ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªå…³é”®å¢å¼ºï¼šç¡¬åº¦åŠ æƒæœºåˆ¶å’Œå‡é˜´æ€§ç¼“è§£ã€‚ç¡¬åº¦åŠ æƒæœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´åŠ å…³æ³¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚å‡é˜´æ€§ç¼“è§£æœºåˆ¶åˆ™å¯ä»¥å‡å°‘å™ªå£°æ•°æ®å¯¹æ¨¡å‹è®­ç»ƒçš„å½±å“ã€‚

**å…³é”®è®¾è®¡**ï¼šç¡¬åº¦åŠ æƒæœºåˆ¶é€šè¿‡è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±å€¼ï¼Œå¹¶æ ¹æ®æŸå¤±å€¼çš„å¤§å°åˆ†é…ä¸åŒçš„æƒé‡ã€‚æŸå¤±å€¼è¶Šå¤§çš„æ ·æœ¬ï¼Œæƒé‡è¶Šé«˜ã€‚å‡é˜´æ€§ç¼“è§£æœºåˆ¶é€šè¿‡å¼•å…¥é¢å¤–çš„è´Ÿæ ·æœ¬ï¼Œå¹¶å¯¹è¿™äº›è´Ÿæ ·æœ¬è¿›è¡ŒåŠ æƒï¼Œä»è€Œå‡å°‘å‡é˜´æ€§çš„å½±å“ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜ä½¿ç”¨äº†å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°æ¥è°ƒæ•´InfoNCEæŸå¤±å‡½æ•°çš„æ¸©åº¦ï¼Œå¹¶é‡‡ç”¨æ¨¡å‹èåˆæŠ€æœ¯æ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RzenEmbedåœ¨MMEBåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„state-of-the-artï¼Œæ€»ä½“å¾—åˆ†è¶…è¿‡äº†æ‰€æœ‰å…ˆå‰çš„å·¥ä½œã€‚å°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘å’Œè§†è§‰æ–‡æ¡£æ£€ç´¢ä»»åŠ¡ä¸Šï¼ŒRzenEmbedçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†éè‡ªç„¶å›¾åƒæ¨¡æ€æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RzenEmbedå¯åº”ç”¨äºå„ç§å¤šæ¨¡æ€æ£€ç´¢åœºæ™¯ï¼Œä¾‹å¦‚è§†é¢‘å†…å®¹æœç´¢ã€æ–‡æ¡£å›¾åƒæ£€ç´¢ã€è·¨æ¨¡æ€ä¿¡æ¯æ¨èç­‰ã€‚è¯¥ç ”ç©¶æˆæœæœ‰åŠ©äºæå‡å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œä¸ºç”¨æˆ·æä¾›æ›´ä¼˜è´¨çš„æœç´¢ä½“éªŒï¼Œå¹¶æ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of Multimodal Large Language Models (MLLMs) has extended CLIP-based frameworks to produce powerful, universal embeddings for retrieval tasks. However, existing methods primarily focus on natural images, offering limited support for other crucial visual modalities such as videos and visual documents. To bridge this gap, we introduce RzenEmbed, a unified framework to learn embeddings across a diverse set of modalities, including text, images, videos, and visual documents. We employ a novel two-stage training strategy to learn discriminative representations. The first stage focuses on foundational text and multimodal retrieval. In the second stage, we introduce an improved InfoNCE loss, incorporating two key enhancements. Firstly, a hardness-weighted mechanism guides the model to prioritize challenging samples by assigning them higher weights within each batch. Secondly, we implement an approach to mitigate the impact of false negatives and alleviate data noise. This strategy not only enhances the model's discriminative power but also improves its instruction-following capabilities. We further boost performance with learnable temperature parameter and model souping. RzenEmbed sets a new state-of-the-art on the MMEB benchmark. It not only achieves the best overall score but also outperforms all prior work on the challenging video and visual document retrieval tasks. Our models are available in https://huggingface.co/qihoo360/RzenEmbed.

