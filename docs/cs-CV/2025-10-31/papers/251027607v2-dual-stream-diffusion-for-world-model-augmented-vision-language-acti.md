---
layout: default
title: Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model
---

# Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.27607" target="_blank" class="toolbar-btn">arXiv: 2510.27607v2</a>
    <a href="https://arxiv.org/pdf/2510.27607.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27607v2" 
            onclick="toggleFavorite(this, '2510.27607v2', 'Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-31 (Êõ¥Êñ∞: 2025-11-04)

**Â§áÊ≥®**: 20 pages, 10 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂèåÊµÅÊâ©Êï£Ê®°ÂûãDUSTÔºåÂ¢ûÂº∫‰∏ñÁïåÊ®°ÂûãÂú®ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã‰∏≠ÁöÑÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `‰∏ñÁïåÊ®°Âûã` `Êâ©Êï£Ê®°Âûã` `ÂèåÊµÅÊû∂ÊûÑ` `Êú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁªìÂêà‰∏ñÁïåÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†‰∏≠Â±ïÁé∞ÊΩúÂäõÔºå‰ΩÜËÅîÂêàÈ¢ÑÊµã‰∏ã‰∏ÄÁä∂ÊÄÅËßÇÊµãÂíåÂä®‰ΩúÂ∫èÂàó‰ªçÂÖ∑ÊåëÊàò„ÄÇ
2. DUSTÈÄöËøáÂèåÊµÅÊâ©Êï£TransformerÊû∂ÊûÑÔºåÊòæÂºèÁª¥Êä§Ê®°ÊÄÅÊµÅÂπ∂ÂÖ±‰∫´Áü•ËØÜÔºåÁªìÂêàËß£ËÄ¶ËÆ≠ÁªÉÂíåÂºÇÊ≠•ÈááÊ†∑ÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDUSTÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠Âùá‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºå‰∏îÂèØÊúâÊïàÂà©Áî®Â§ßËßÑÊ®°Êó†Âä®‰ΩúËßÜÈ¢ëËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÂèåÊµÅÊâ©Êï£ÔºàDUSTÔºâÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÂ¢ûÂº∫ÂÖ∑Êúâ‰∏ñÁïåÊ®°ÂûãÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºàVLAÔºâ„ÄÇËØ•Ê°ÜÊû∂Êó®Âú®Ëß£ÂÜ≥‰∏çÂêåÊ®°ÊÄÅÔºàËßÜËßâÂíåÂä®‰ΩúÔºâ‰πãÈó¥Âõ∫ÊúâÁöÑÂ∑ÆÂºÇÂ∏¶Êù•ÁöÑÊåëÊàòÔºå‰ªéËÄåÊèêÂçáVLAÂú®ÂêÑÁßç‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇDUSTÈááÁî®Â§öÊ®°ÊÄÅÊâ©Êï£TransformerÊû∂ÊûÑÔºåÊòæÂºèÂú∞Áª¥Êä§Áã¨Á´ãÁöÑÊ®°ÊÄÅÊµÅÔºåÂêåÊó∂‰øÉËøõË∑®Ê®°ÊÄÅÁü•ËØÜÂÖ±‰∫´„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜÁã¨Á´ãÁöÑÊ®°ÊÄÅÂô™Â£∞Êâ∞Âä®ÂíåËß£ËÄ¶ÁöÑÊµÅÂåπÈÖçÊçüÂ§±Á≠âËÆ≠ÁªÉÊäÄÊúØÔºå‰ΩøÊ®°ÂûãËÉΩÂ§ü‰ª•ÂèåÂêëÊñπÂºèÂ≠¶‰π†ËÅîÂêàÂàÜÂ∏ÉÔºåËÄåÊó†ÈúÄÁªü‰∏ÄÁöÑÊΩúÂú®Á©∫Èó¥„ÄÇÂü∫‰∫éËß£ËÄ¶ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂºÇÊ≠•ÈááÊ†∑ÊñπÊ≥ïÔºåÂú®Êé®ÁêÜÊó∂‰ª•‰∏çÂêåÁöÑÈÄüÁéáÂØπÂä®‰ΩúÂíåËßÜËßâtokenËøõË°åÈááÊ†∑Ôºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊèêÂçáÊÄßËÉΩ„ÄÇÂÆûÈ™åË°®ÊòéÔºåDUSTÂú®Ê®°ÊãüÁéØÂ¢ÉÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºàVLAÔºâÂú®ÁªìÂêà‰∏ñÁïåÊ®°ÂûãÊó∂ÔºåÈöæ‰ª•ÊúâÊïàÂú∞Â§ÑÁêÜËßÜËßâËßÇÊµãÂíåÂä®‰ΩúÂ∫èÂàóËøô‰∏§ÁßçÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÁõ¥Êé•Â∞ÜÂÆÉ‰ª¨Êò†Â∞ÑÂà∞Áªü‰∏ÄÁöÑÊΩúÂú®Á©∫Èó¥ÂèØËÉΩ‰ºöÂØºËá¥‰ø°ÊÅØÊçüÂ§±ÂíåÊ¨°‰ºòÁöÑÁ≠ñÁï•Â≠¶‰π†ÊïàÊûú„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊõ¥Â•ΩÂú∞Âª∫Ê®°Ëøô‰∏§ÁßçÊ®°ÊÄÅÁöÑËÅîÂêàÂàÜÂ∏ÉÔºåÂπ∂Âà©Áî®‰∏ñÁïåÊ®°ÂûãÊù•Â¢ûÂº∫VLAÁöÑÊÄßËÉΩÔºåÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDUSTÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈááÁî®ÂèåÊµÅÊâ©Êï£Ê®°ÂûãÔºåÊòæÂºèÂú∞Áª¥Êä§ËßÜËßâÂíåÂä®‰Ωú‰∏§‰∏™Áã¨Á´ãÁöÑÊ®°ÊÄÅÊµÅ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâÊØèÁßçÊ®°ÊÄÅÁöÑÁâπÊÄßÔºåÈÅøÂÖç‰∫ÜÂ∞ÜÂÆÉ‰ª¨Âº∫Âà∂Êò†Â∞ÑÂà∞Áªü‰∏ÄÊΩúÂú®Á©∫Èó¥ÊâÄÂ∏¶Êù•ÁöÑ‰ø°ÊÅØÊçüÂ§±„ÄÇÂêåÊó∂ÔºåÈÄöËøáË∑®Ê®°ÊÄÅÁöÑÁü•ËØÜÂÖ±‰∫´Êú∫Âà∂ÔºåÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞ËßÜËßâÂíåÂä®‰Ωú‰πãÈó¥ÁöÑÂÖ≥ËÅîÊÄßÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞È¢ÑÊµãÊú™Êù•ÁöÑÁä∂ÊÄÅÂíåÂä®‰Ωú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDUSTÁöÑÊï¥‰ΩìÊû∂ÊûÑÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÔºåÂπ∂ÈááÁî®Transformer‰Ωú‰∏∫ÂÖ∂Ê†∏ÂøÉÊûÑÂª∫Âùó„ÄÇÂÆÉÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÁöÑÊµÅÔºöËßÜËßâÊµÅÂíåÂä®‰ΩúÊµÅ„ÄÇÊØè‰∏™ÊµÅÈÉΩË¥üË¥£Â§ÑÁêÜÂØπÂ∫îÊ®°ÊÄÅÁöÑ‰ø°ÊÅØ„ÄÇËøô‰∏§‰∏™ÊµÅÈÄöËøáË∑®Ê≥®ÊÑèÂäõÊú∫Âà∂ËøõË°å‰∫§‰∫íÔºåÂÆûÁé∞Ë∑®Ê®°ÊÄÅÁöÑÁü•ËØÜÂÖ±‰∫´„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÈ¶ñÂÖàÂØπËßÜËßâÂíåÂä®‰ΩúÊï∞ÊçÆËøõË°åÁºñÁ†ÅÔºåÁÑ∂ÂêéÊ∑ªÂä†Âô™Â£∞„ÄÇÊé•‰∏ãÊù•ÔºåÊ®°ÂûãÂ≠¶‰π†Â¶Ç‰Ωï‰ªéÂô™Â£∞‰∏≠ÊÅ¢Â§çÂéüÂßãÊï∞ÊçÆ„ÄÇÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÈÄöËøáËø≠‰ª£Âú∞ÂéªÂô™Êù•ÁîüÊàêÊú™Êù•ÁöÑÁä∂ÊÄÅÂíåÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDUSTÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂÖ∂ÂèåÊµÅÊâ©Êï£Êû∂ÊûÑÂíåËß£ËÄ¶ÁöÑËÆ≠ÁªÉÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïÊµÅÊâ©Êï£Ê®°ÂûãÁõ∏ÊØîÔºåÂèåÊµÅÊû∂ÊûÑËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜ‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇËß£ËÄ¶ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÂåÖÊã¨Áã¨Á´ãÁöÑÂô™Â£∞Êâ∞Âä®ÂíåËß£ËÄ¶ÁöÑÊµÅÂåπÈÖçÊçüÂ§±Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§ü‰ª•ÂèåÂêëÁöÑÊñπÂºèÂ≠¶‰π†ËÅîÂêàÂàÜÂ∏ÉÔºåËÄåÊó†ÈúÄÁªü‰∏ÄÁöÑÊΩúÂú®Á©∫Èó¥„ÄÇÊ≠§Â§ñÔºåÂºÇÊ≠•ÈááÊ†∑ÊñπÊ≥ï‰πüÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂàõÊñ∞ÔºåÂÆÉÂÖÅËÆ∏Ê®°ÂûãÂú®Êé®ÁêÜÊó∂‰ª•‰∏çÂêåÁöÑÈÄüÁéáÂØπÂä®‰ΩúÂíåËßÜËßâtokenËøõË°åÈááÊ†∑Ôºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊèêÂçáÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDUSTÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Áã¨Á´ãÁöÑÂô™Â£∞Êâ∞Âä®ÔºöÂØπËßÜËßâÂíåÂä®‰ΩúÊï∞ÊçÆÊ∑ªÂä†‰∏çÂêåÁ®ãÂ∫¶ÁöÑÂô™Â£∞Ôºå‰ª•Êõ¥Â•ΩÂú∞ÂèçÊò†ÂÆÉ‰ª¨ÂêÑËá™ÁöÑÁâπÊÄß„ÄÇ2) Ëß£ËÄ¶ÁöÑÊµÅÂåπÈÖçÊçüÂ§±Ôºö‰ΩøÁî®Áã¨Á´ãÁöÑÊçüÂ§±ÂáΩÊï∞Êù•ËÆ≠ÁªÉËßÜËßâÊµÅÂíåÂä®‰ΩúÊµÅÔºåÈÅøÂÖç‰∫ÜÂÆÉ‰ª¨‰πãÈó¥ÁöÑÁõ∏‰∫íÂπ≤Êâ∞„ÄÇ3) Ë∑®Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºöÂÖÅËÆ∏ËßÜËßâÊµÅÂíåÂä®‰ΩúÊµÅ‰πãÈó¥ËøõË°å‰ø°ÊÅØ‰∫§‰∫íÔºå‰ªéËÄåÂ≠¶‰π†Âà∞ÂÆÉ‰ª¨‰πãÈó¥ÁöÑÂÖ≥ËÅîÊÄß„ÄÇ4) ÂºÇÊ≠•ÈááÊ†∑ÊñπÊ≥ïÔºöÂú®Êé®ÁêÜÊó∂Ôºå‰ª•‰∏çÂêåÁöÑÈÄüÁéáÂØπÂä®‰ΩúÂíåËßÜËßâtokenËøõË°åÈááÊ†∑Ôºå‰ª•Âπ≥Ë°°È¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DUSTÂú®RoboCasaÂíåGR-1Á≠âÊ®°ÊãüÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÁõ∏ÊØîÊ†áÂáÜVLAÂü∫Á∫øÂíåÈöêÂºè‰∏ñÁïåÂª∫Ê®°ÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÈ´òËææ6%„ÄÇÈÄöËøáÊé®ÁêÜÊó∂Áº©ÊîæÊñπÊ≥ïÔºåÊàêÂäüÁéáÈ¢ùÂ§ñÊèêÂçá2-5%„ÄÇÂú®Franka Research 3ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÔºåDUSTÁöÑÊàêÂäüÁéáÊØîÂü∫Á∫øÊñπÊ≥ïÈ´òÂá∫13%„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂú®BridgeV2Êï∞ÊçÆÈõÜ‰∏äËøõË°åÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÔºåDUSTÂú®ËøÅÁßªÂà∞RoboCasaÂü∫ÂáÜÊµãËØïÊó∂ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DUSTÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÊìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÁªìÂêàËßÜËßâ‰ø°ÊÅØÂíåÂä®‰ΩúÊåá‰ª§ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºåÂπ∂ÂÅöÂá∫Êõ¥ÂêàÁêÜÁöÑÂÜ≥Á≠ñ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÁ≠âÈ¢ÜÂüüÔºåÁîüÊàêÊõ¥ÈÄºÁúü„ÄÅÊõ¥Êô∫ËÉΩÁöÑËôöÊãüËßíËâ≤„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and a decoupled flow matching loss, which enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Furthermore, based on the decoupled training framework, we introduce a sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over a standard VLA baseline and implicit world-modeling methods, with our inference-time scaling approach providing an additional 2-5% gain on success rate. On real-world tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark.

