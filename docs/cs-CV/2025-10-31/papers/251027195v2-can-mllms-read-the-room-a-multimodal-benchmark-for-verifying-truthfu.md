---
layout: default
title: Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions
---

# Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.27195" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.27195v2</a>
  <a href="https://arxiv.org/pdf/2510.27195.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27195v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.27195v2', 'Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Caixin Kang, Yifei Huang, Liangyang Ouyang, Mingfang Zhang, Yoichi Sato

**åˆ†ç±»**: cs.CV, cs.CL, cs.SI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31 (æ›´æ–°: 2025-11-04)

**å¤‡æ³¨**: ICCV2025 Workshop

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMIVAåŸºå‡†ï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šäººç¤¾äº¤äº’åŠ¨ä¸­è¯†åˆ«è°è¨€çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è°è¨€è¯†åˆ«` `ç¤¾äº¤æ™ºèƒ½` `å¤§è¯­è¨€æ¨¡å‹` `å¤šæ–¹äº¤äº’` `çœŸå®æ€§è¯„ä¼°` `ç‹¼äººæ€` `è§†è§‰è¯­è¨€ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨åŠ¨æ€å¤šäººå¯¹è¯ä¸­è¿›è¡Œæ¬ºéª—æ£€æµ‹ï¼Œç¼ºä¹å¯¹è¯­è¨€å’Œéè¯­è¨€çº¿ç´¢çš„æœ‰æ•ˆèåˆã€‚
2. æå‡ºå¤šæ¨¡æ€äº¤äº’çœŸå®æ€§è¯„ä¼°ï¼ˆMIVAï¼‰ä»»åŠ¡ï¼Œå¹¶æ„å»ºåŸºäºâ€œç‹¼äººæ€â€æ¸¸æˆçš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯GPT-4oç­‰å…ˆè¿›MLLMä¹Ÿéš¾ä»¥å¯é åŒºåˆ†çœŸå‡ï¼Œå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€äººå·¥æ™ºèƒ½ç³»ç»Ÿæ—¥ç›Šèå…¥äººç±»ç”Ÿæ´»ï¼Œèµ‹äºˆå®ƒä»¬å¼ºå¤§çš„ç¤¾äº¤æ™ºèƒ½å·²æˆä¸ºä¸€ä¸ªå…³é”®å‰æ²¿ã€‚è¿™ç§æ™ºèƒ½çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯è¾¨åˆ«çœŸå‡ï¼Œè¿™æ˜¯äººç±»äº’åŠ¨ä¸­æ™®éå­˜åœ¨çš„è¦ç´ ï¼Œé€šè¿‡å£å¤´è¯­è¨€å’Œéè¯­è¨€è§†è§‰çº¿ç´¢çš„å¤æ‚ç›¸äº’ä½œç”¨æ¥ä¼ è¾¾ã€‚ç„¶è€Œï¼Œåœ¨åŠ¨æ€çš„å¤šæ–¹å¯¹è¯ä¸­è‡ªåŠ¨è¿›è¡Œæ¬ºéª—æ£€æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘å¼ºå¤§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å…´èµ·ï¼Œå‡­å€Ÿå…¶åœ¨è§†è§‰å’Œæ–‡æœ¬ç†è§£æ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºè¿™é¡¹ä»»åŠ¡çš„å¤©ç„¶å€™é€‰è€…ã€‚å› æ­¤ï¼Œå®ƒä»¬åœ¨è¿™ä¸ªå…³é”®é¢†åŸŸçš„èƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯æœªé‡åŒ–çš„ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œå³å¤šæ¨¡æ€äº¤äº’çœŸå®æ€§è¯„ä¼°ï¼ˆMIVAï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæºè‡ªç¤¾äº¤æ¨ç†æ¸¸æˆâ€œç‹¼äººæ€â€çš„æ–°å‹å¤šæ¨¡æ€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¸ºæ¯ä¸ªé™ˆè¿°æä¾›åŒæ­¥çš„è§†é¢‘ã€æ–‡æœ¬ä»¥åŠå¯éªŒè¯çš„çœŸå®æ ‡ç­¾ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œè¯„ä¼°æœ€å…ˆè¿›çš„MLLMï¼Œæ­ç¤ºäº†ä¸€ä¸ªæ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼šå³ä½¿æ˜¯åƒGPT-4oè¿™æ ·å¼ºå¤§çš„æ¨¡å‹ä¹Ÿéš¾ä»¥å¯é åœ°åŒºåˆ†çœŸå‡ã€‚æˆ‘ä»¬å¯¹å¤±è´¥æ¨¡å¼çš„åˆ†æè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹æœªèƒ½æœ‰æ•ˆåœ°å°†è¯­è¨€æ‰æ ¹äºè§†è§‰ç¤¾äº¤çº¿ç´¢ä¸­ï¼Œå¹¶ä¸”å¯èƒ½åœ¨å¯¹é½æ–¹é¢è¿‡äºä¿å®ˆï¼Œçªæ˜¾äº†è¿«åˆ‡éœ€è¦æ–°çš„æ–¹æ³•æ¥æ„å»ºæ›´å…·æ´å¯ŸåŠ›å’Œå€¼å¾—ä¿¡èµ–çš„AIç³»ç»Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ–¹ç¤¾äº¤äº’åŠ¨åœºæ™¯ä¸‹çš„è°è¨€è¯†åˆ«é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€å¯¹è¯å’Œèåˆå¤šæ¨¡æ€ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰è¯­è¨€å’Œéè¯­è¨€çº¿ç´¢ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šäººäº’åŠ¨ä¸­ï¼Œæ¬ºéª—è¡Œä¸ºå¾€å¾€æ›´åŠ éšè”½ï¼Œéœ€è¦æ›´å¼ºçš„æ¨ç†èƒ½åŠ›å’Œå¯¹ç¤¾äº¤ç¯å¢ƒçš„ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¼ºå¤§è§†è§‰å’Œæ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡åˆ†æå‚ä¸è€…çš„è¯­è¨€è¡¨è¾¾å’Œéè¯­è¨€è¡Œä¸ºï¼ˆå¦‚é¢éƒ¨è¡¨æƒ…ã€è‚¢ä½“åŠ¨ä½œï¼‰æ¥åˆ¤æ–­å…¶é™ˆè¿°çš„çœŸå®æ€§ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«åŒæ­¥è§†é¢‘ã€æ–‡æœ¬å’ŒçœŸå®æ ‡ç­¾çš„æ•°æ®é›†ï¼Œä¸ºMLLMæä¾›å­¦ä¹ å’Œè¯„ä¼°çš„å¹³å°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€äº¤äº’çœŸå®æ€§è¯„ä¼°ï¼ˆMIVAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®æ”¶é›†ä¸æ ‡æ³¨ï¼šä»â€œç‹¼äººæ€â€æ¸¸æˆä¸­æ”¶é›†è§†é¢‘å’Œæ–‡æœ¬æ•°æ®ï¼Œå¹¶ä¸ºæ¯ä¸ªé™ˆè¿°æ ‡æ³¨çœŸå®æ ‡ç­¾ã€‚2) ç‰¹å¾æå–ï¼šåˆ©ç”¨MLLMæå–è§†é¢‘å’Œæ–‡æœ¬çš„ç‰¹å¾è¡¨ç¤ºã€‚3) çœŸå®æ€§é¢„æµ‹ï¼šåŸºäºæå–çš„ç‰¹å¾ï¼Œåˆ©ç”¨åˆ†ç±»å™¨é¢„æµ‹é™ˆè¿°çš„çœŸå®æ€§ã€‚4) æ€§èƒ½è¯„ä¼°ï¼šä½¿ç”¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ç­‰æŒ‡æ ‡è¯„ä¼°MLLMçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºï¼š1) æå‡ºäº†å¤šæ¨¡æ€äº¤äº’çœŸå®æ€§è¯„ä¼°ï¼ˆMIVAï¼‰ä»»åŠ¡ï¼Œå¡«è¡¥äº†å¤šæ–¹ç¤¾äº¤äº’åŠ¨åœºæ™¯ä¸‹è°è¨€è¯†åˆ«ç ”ç©¶çš„ç©ºç™½ã€‚2) æ„å»ºäº†ä¸€ä¸ªåŸºäºâ€œç‹¼äººæ€â€æ¸¸æˆçš„æ–°å‹å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸ºMLLMçš„å­¦ä¹ å’Œè¯„ä¼°æä¾›äº†æ•°æ®åŸºç¡€ã€‚3) å¯¹æ¯”è¯„ä¼°äº†å¤šç§å…ˆè¿›çš„MLLMåœ¨MIVAä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨ç†è§£å’Œèåˆå¤šæ¨¡æ€ç¤¾äº¤çº¿ç´¢æ–¹é¢çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ•°æ®é›†çš„æ„å»ºï¼šç²¾å¿ƒè®¾è®¡äº†æ•°æ®æ”¶é›†å’Œæ ‡æ³¨æµç¨‹ï¼Œä¿è¯äº†æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚2) ç‰¹å¾æå–æ–¹æ³•ï¼šé€‰æ‹©äº†åˆé€‚çš„MLLMæ¥æå–è§†é¢‘å’Œæ–‡æœ¬çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ¢ç´¢äº†ä¸åŒçš„ç‰¹å¾èåˆç­–ç•¥ã€‚3) è¯„ä¼°æŒ‡æ ‡ï¼šé‡‡ç”¨äº†å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥å…¨é¢è¯„ä¼°MLLMçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯GPT-4oç­‰å…ˆè¿›çš„MLLMåœ¨MIVAæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿè¿œä½äºäººç±»æ°´å¹³ï¼Œå‡†ç¡®ç‡ä»…ä¸ºæœªçŸ¥ç™¾åˆ†æ¯”ã€‚è¿™è¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨ç†è§£å’Œèåˆå¤šæ¨¡æ€ç¤¾äº¤çº¿ç´¢æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¢æœã€åœ¨çº¿æ•™è‚²ã€æ‹›è˜é¢è¯•ç­‰é¢†åŸŸï¼Œå¸®åŠ©ç³»ç»Ÿè¯†åˆ«ç”¨æˆ·æˆ–å‚ä¸è€…çš„æ¬ºéª—è¡Œä¸ºï¼Œæé«˜äº¤äº’çš„çœŸå®æ€§å’Œå¯é æ€§ã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ‰©å±•åˆ°ç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ã€é‡‘èé£é™©è¯„ä¼°ç­‰é¢†åŸŸï¼Œæ„å»ºæ›´å€¼å¾—ä¿¡èµ–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As AI systems become increasingly integrated into human lives, endowing them with robust social intelligence has emerged as a critical frontier. A key aspect of this intelligence is discerning truth from deception, a ubiquitous element of human interaction that is conveyed through a complex interplay of verbal language and non-verbal visual cues. However, automatic deception detection in dynamic, multi-party conversations remains a significant challenge. The recent rise of powerful Multimodal Large Language Models (MLLMs), with their impressive abilities in visual and textual understanding, makes them natural candidates for this task. Consequently, their capabilities in this crucial domain are mostly unquantified. To address this gap, we introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and present a novel multimodal dataset derived from the social deduction game Werewolf. This dataset provides synchronized video, text, with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating state-of-the-art MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to ground language in visual social cues effectively and may be overly conservative in their alignment, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems.

