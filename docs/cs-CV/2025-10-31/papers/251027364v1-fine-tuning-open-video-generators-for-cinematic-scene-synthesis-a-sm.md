---
layout: default
title: Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V
---

# Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.27364" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.27364v1</a>
  <a href="https://arxiv.org/pdf/2510.27364.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.27364v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.27364v1', 'Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Meftun Akarsu, Kerem Catay, Sedat Bin Vedat, Enes Kutay Yarkan, Ilke Senturk, Arda Sar, Dafne Eksioglu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

**å¤‡æ³¨**: video generation, image-to-video, dif- fusion transformer, LoRA, fine-tuning, cinematic scene synthesis, multi-GPU inference, fully sharded data parallelism, computational efficiency

**DOI**: [10.5281/zenodo.17370356](https://doi.org/10.5281/zenodo.17370356)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLoRAå¾®è°ƒçš„è§†é¢‘ç”Ÿæˆç®¡çº¿ï¼Œç”¨äºç”µå½±åœºæ™¯åˆæˆï¼Œè§£å†³å°æ•°æ®é›†éš¾é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `ç”µå½±åœºæ™¯åˆæˆ` `LoRAå¾®è°ƒ` `æ‰©æ•£æ¨¡å‹` `é£æ ¼è¿ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç”µå½±åœºæ™¯åˆæˆä¸­ï¼Œé¢ä¸´æ•°æ®é›†ç¨€ç¼ºå’Œé£æ ¼è¿ç§»å›°éš¾çš„æŒ‘æˆ˜ã€‚
2. åˆ©ç”¨LoRAé«˜æ•ˆå¾®è°ƒWan2.1 I2Væ¨¡å‹ï¼Œè§£è€¦è§†è§‰é£æ ¼å­¦ä¹ å’Œè¿åŠ¨ç”Ÿæˆï¼Œå®ç°å¿«é€Ÿé¢†åŸŸé€‚åº”ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”µå½±ä¿çœŸåº¦å’Œæ—¶é—´ç¨³å®šæ€§ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶å¼€æºäº†å®Œæ•´ç®¡çº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ç”¨çš„ç®¡çº¿ï¼Œç”¨äºå¾®è°ƒå¼€æºè§†é¢‘æ‰©æ•£Transformerï¼Œä»è€Œåˆ©ç”¨å°æ•°æ®é›†åˆæˆç”µå½±åœºæ™¯ï¼ŒæœåŠ¡äºç”µè§†å’Œç”µå½±åˆ¶ä½œã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œå°†è§†è§‰é£æ ¼å­¦ä¹ ä¸è¿åŠ¨ç”Ÿæˆè§£è€¦ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œå°†LoRAæ¨¡å—é›†æˆåˆ°Wan2.1 I2V-14Bæ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å±‚ä¸­ï¼Œä½¿ç”¨æ¥è‡ªAy Yapimå†å²ç”µè§†å‰§ã€ŠEl Turcoã€‹çš„çŸ­ç‰‡æ®µæ•°æ®é›†æ¥è°ƒæ•´å…¶è§†è§‰è¡¨ç¤ºã€‚è¿™å®ç°äº†åœ¨å•ä¸ªGPUä¸Šæ•°å°æ—¶å†…å®Œæˆé«˜æ•ˆçš„é¢†åŸŸè¿ç§»ã€‚ç¬¬äºŒé˜¶æ®µï¼Œå¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆé£æ ¼ä¸€è‡´çš„å…³é”®å¸§ï¼Œä¿ç•™æœè£…ã€å…‰ç…§å’Œè‰²å½©åˆ†çº§ï¼Œç„¶åé€šè¿‡æ¨¡å‹çš„è§†é¢‘è§£ç å™¨å°†è¿™äº›å…³é”®å¸§åœ¨æ—¶é—´ä¸Šæ‰©å±•ä¸ºè¿è´¯çš„720påºåˆ—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åº”ç”¨è½»é‡çº§å¹¶è¡ŒåŒ–å’Œåºåˆ—åˆ†å‰²ç­–ç•¥æ¥åŠ é€Ÿæ¨ç†ï¼Œä¸”ä¸é™ä½è´¨é‡ã€‚ä½¿ç”¨FVDã€CLIP-SIMå’ŒLPIPSæŒ‡æ ‡è¿›è¡Œçš„å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œä»¥åŠå°å‹ä¸“å®¶ç”¨æˆ·ç ”ç©¶çš„æ”¯æŒï¼Œè¯æ˜äº†ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œç”µå½±ä¿çœŸåº¦å’Œæ—¶é—´ç¨³å®šæ€§æ–¹é¢æœ‰æ˜¾è‘—æé«˜ã€‚å®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†ç®¡çº¿å·²å‘å¸ƒï¼Œä»¥æ”¯æŒå¯é‡å¤æ€§ä»¥åŠåœ¨ç”µå½±é¢†åŸŸä¸­çš„æ”¹ç¼–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç”µå½±å’Œç”µè§†åˆ¶ä½œä¸­ï¼Œåˆ©ç”¨å°æ•°æ®é›†ç”Ÿæˆé«˜è´¨é‡ç”µå½±åœºæ™¯çš„é—®é¢˜ã€‚ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¸”éš¾ä»¥é€‚åº”ç‰¹å®šç”µå½±é£æ ¼ï¼Œå¯¼è‡´ç”Ÿæˆæ•ˆæœä¸ä½³ï¼Œé£æ ¼ä¸ä¸€è‡´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰é£æ ¼çš„å­¦ä¹ ä¸è¿åŠ¨ç”Ÿæˆè§£è€¦ï¼Œé€šè¿‡LoRAå¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ç›®æ ‡ç”µå½±çš„è§†è§‰é£æ ¼ã€‚ç„¶åï¼Œåˆ©ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆå…³é”®å¸§ï¼Œå†é€šè¿‡è§†é¢‘è§£ç å™¨ç”Ÿæˆå®Œæ•´çš„è§†é¢‘åºåˆ—ã€‚è¿™ç§æ–¹æ³•é™ä½äº†å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„ä¾èµ–ï¼Œå¹¶æé«˜äº†é£æ ¼è¿ç§»çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) è§†è§‰é£æ ¼å¾®è°ƒé˜¶æ®µï¼šä½¿ç”¨LoRAæ¨¡å—å¾®è°ƒWan2.1 I2V-14Bæ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œä½¿å…¶é€‚åº”ç›®æ ‡ç”µå½±çš„è§†è§‰é£æ ¼ã€‚2) è§†é¢‘ç”Ÿæˆé˜¶æ®µï¼šåˆ©ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆé£æ ¼ä¸€è‡´çš„å…³é”®å¸§ï¼Œç„¶åä½¿ç”¨æ¨¡å‹çš„è§†é¢‘è§£ç å™¨å°†è¿™äº›å…³é”®å¸§æ‰©å±•ä¸ºè¿è´¯çš„è§†é¢‘åºåˆ—ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è½»é‡çº§å¹¶è¡ŒåŒ–å’Œåºåˆ—åˆ†å‰²ç­–ç•¥æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨LoRAè¿›è¡Œé«˜æ•ˆçš„è§†è§‰é£æ ¼è¿ç§»ï¼Œèƒ½å¤Ÿåœ¨å°æ•°æ®é›†ä¸Šå¿«é€Ÿå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºç”µå½±åœºæ™¯åˆæˆã€‚æ­¤å¤–ï¼Œå°†è§†è§‰é£æ ¼å­¦ä¹ ä¸è¿åŠ¨ç”Ÿæˆè§£è€¦ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶ç”Ÿæˆè§†é¢‘çš„é£æ ¼å’Œå†…å®¹ã€‚

**å…³é”®è®¾è®¡**ï¼šLoRAæ¨¡å—è¢«é›†æˆåˆ°Wan2.1 I2V-14Bæ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å±‚ä¸­ï¼Œé€šè¿‡å­¦ä¹ ä½ç§©çŸ©é˜µæ¥è°ƒæ•´æ¨¡å‹çš„æƒé‡ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é£æ ¼è¿ç§»ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨æ¥è‡ªç›®æ ‡ç”µå½±çš„çŸ­ç‰‡æ®µæ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨AdamWä¼˜åŒ–å™¨è¿›è¡Œä¼˜åŒ–ã€‚æ¨ç†é˜¶æ®µï¼Œé‡‡ç”¨åºåˆ—åˆ†å‰²ç­–ç•¥å°†é•¿è§†é¢‘åˆ†å‰²æˆå¤šä¸ªçŸ­è§†é¢‘ç‰‡æ®µï¼Œç„¶åå¹¶è¡Œç”Ÿæˆè¿™äº›ç‰‡æ®µï¼Œæœ€åå°†å®ƒä»¬æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”µå½±ä¿çœŸåº¦å’Œæ—¶é—´ç¨³å®šæ€§æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œä½¿ç”¨FVDã€CLIP-SIMå’ŒLPIPSæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•ç”Ÿæˆçš„è§†é¢‘åœ¨è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œå°å‹ä¸“å®¶ç”¨æˆ·ç ”ç©¶ä¹Ÿè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è§†é¢‘æ›´ç¬¦åˆç”µå½±åˆ¶ä½œçš„è¦æ±‚ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç”µå½±å’Œç”µè§†åˆ¶ä½œé¢†åŸŸï¼Œå¸®åŠ©åˆ¶ä½œäººå‘˜å¿«é€Ÿç”Ÿæˆå…·æœ‰ç‰¹å®šé£æ ¼çš„ç”µå½±åœºæ™¯ï¼Œé™ä½åˆ¶ä½œæˆæœ¬ï¼Œæé«˜åˆ¶ä½œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸï¼Œç”Ÿæˆé«˜è´¨é‡çš„è™šæ‹Ÿåœºæ™¯å’Œè§’è‰²åŠ¨ç”»ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥å‘å±•ï¼Œå®ç°æ›´åŠ é€¼çœŸå’Œä¸ªæ€§åŒ–çš„è§†é¢‘ç”Ÿæˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a practical pipeline for fine-tuning open-source video diffusion transformers to synthesize cinematic scenes for television and film production from small datasets. The proposed two-stage process decouples visual style learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA) modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B model to adapt its visual representations using a compact dataset of short clips from Ay Yapim's historical television film El Turco. This enables efficient domain transfer within hours on a single GPU. In the second stage, the fine-tuned model produces stylistically consistent keyframes that preserve costume, lighting, and color grading, which are then temporally expanded into coherent 720p sequences through the model's video decoder. We further apply lightweight parallelization and sequence partitioning strategies to accelerate inference without quality degradation. Quantitative and qualitative evaluations using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study, demonstrate measurable improvements in cinematic fidelity and temporal stability over the base model. The complete training and inference pipeline is released to support reproducibility and adaptation across cinematic domains.

