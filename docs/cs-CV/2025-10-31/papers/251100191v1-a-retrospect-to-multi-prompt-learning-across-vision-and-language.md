---
layout: default
title: A Retrospect to Multi-prompt Learning across Vision and Language
---

# A Retrospect to Multi-prompt Learning across Vision and Language

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.00191" target="_blank" class="toolbar-btn">arXiv: 2511.00191v1</a>
    <a href="https://arxiv.org/pdf/2511.00191.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00191v1" 
            onclick="toggleFavorite(this, '2511.00191v1', 'A Retrospect to Multi-prompt Learning across Vision and Language')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ziliang Chen, Xin Huang, Quanlong Guan, Liang Lin, Weiqi Luo

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-31

**Â§áÊ≥®**: ICCV

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ËÉΩÈáèÈ©±Âä®ÁöÑÂ§öÊèêÁ§∫Â≠¶‰π†ÊñπÊ≥ïÔºåÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊèêÁ§∫Â≠¶‰π†` `ËßÜËßâ-ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉ` `ËÉΩÈáèÊ®°Âûã` `ÊèêÁ§∫Â∑•Á®ã` `ÂºÄÊîæËØçÊ±áÊ≥õÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏ªË¶ÅÈááÁî®ÂçïÊèêÁ§∫Â≠¶‰π†ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ËÉΩÈáèÈ©±Âä®ÁöÑÂ§öÊèêÁ§∫Â≠¶‰π†ÔºàEMPLÔºâÊñπÊ≥ïÔºåÈÄöËøáËÉΩÈáèÊ®°ÂûãÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊèêÁ§∫ÂµåÂÖ•ÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEMPLÊñπÊ≥ïÂú®È¢ÜÂüüÂÜÖÂíåÈ¢ÜÂüüÂ§ñ‰ªªÂä°‰∏≠ÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâÁ§æÂå∫Ê≠£ÁªèÂéÜÁùÄËßÜËßâ-ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàVLMsÔºâÂ∏¶Êù•ÁöÑÂâçÊâÄÊú™ÊúâÁöÑËøõÊ≠•„ÄÇÊèêÁ§∫Â≠¶‰π†ÊòØÂà©Áî®VLMsÁöÑÂÖ≥ÈîÆÔºåÂõ†‰∏∫ÂÆÉËÉΩÂ§ü‰ª•ÊúâÈôêÁöÑËµÑÊ∫êÂø´ÈÄüÈÄÇÂ∫î‰∏ãÊ∏∏‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂçïÊèêÁ§∫ËåÉÂºè‰∏äÔºåÂæàÂ∞ëÁ†îÁ©∂Â§öÊèêÁ§∫Â≠¶‰π†ÁöÑÊΩúÂäõ„ÄÇÊú¨ÊñáÊó®Âú®ÂØπËßÜËßâ-ËØ≠Ë®ÄÂ§öÊèêÁ§∫Â≠¶‰π†ËøõË°åÊúâÂéüÂàôÁöÑÂõûÈ°æ„ÄÇÊàë‰ª¨Â∞ÜÊúÄËøëÁöÑÊ®°ÊÄÅÂ∑ÆË∑ùÁé∞Ë±°Êâ©Â±ïÂà∞ÂèØÂ≠¶‰π†ÁöÑÊèêÁ§∫ÔºåÂπ∂ÈÄöËøáÂÆûÈ™åÂíåÁêÜËÆ∫ËØÅÊòé‰∫Ü‰ΩøÁî®Â§öÊèêÁ§∫Â¢ûÂº∫ËøõË°åËßÜËßâ-ËØ≠Ë®ÄËøÅÁßªÁöÑ‰ºòË∂äÊÄß„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËÉΩÈáèÁöÑÂ§öÊèêÁ§∫Â≠¶‰π†ÔºàEMPLÔºâÊñπÊ≥ïÔºåÈÄöËøá‰ªéÁî±VLMsÈöêÂºèÂÆö‰πâÁöÑÂü∫‰∫éËÉΩÈáèÁöÑÂàÜÂ∏É‰∏≠ÊäΩÂèñÂÆû‰æãÊù•ÁîüÊàêÂ§ö‰∏™ÊèêÁ§∫ÂµåÂÖ•„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÁöÑEMPL‰∏ç‰ªÖÂèÇÊï∞È´òÊïàÔºåËÄå‰∏î‰∏•Ê†ºÂú∞ÂÆûÁé∞‰∫ÜÈ¢ÜÂüüÂÜÖÂíåÈ¢ÜÂüüÂ§ñÂºÄÊîæËØçÊ±áÊ≥õÂåñ‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇÂÖ®Èù¢ÁöÑÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑ‰∏ªÂº†ÂíåEMPLÁöÑÂçìË∂äÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàVLMsÔºâÁöÑÊèêÁ§∫Â≠¶‰π†ÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂçïÊèêÁ§∫ËåÉÂºèÔºåÂç≥‰ΩøÁî®Âçï‰∏™ÊñáÊú¨ÊèêÁ§∫Êù•ÂºïÂØºÊ®°ÂûãÂÆåÊàê‰∏ãÊ∏∏‰ªªÂä°„ÄÇËøôÁßçÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÂú®‰∫éÔºåÂçï‰∏™ÊèêÁ§∫ÂèØËÉΩÊó†Ê≥ïÂÖÖÂàÜË°®Ëææ‰ªªÂä°ÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂØºËá¥Ê®°ÂûãÊÄßËÉΩÂèóÈôêÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂºÄÊîæËØçÊ±áÊ≥õÂåñÊñπÈù¢Ë°®Áé∞‰∏çË∂≥„ÄÇÊ≠§Â§ñÔºåÂ¶Ç‰ΩïÊúâÊïàÂú∞Âà©Áî®ÊúâÈôêÁöÑËµÑÊ∫êÊù•Â≠¶‰π†Êõ¥ÂÖ∑Ë°®ËææËÉΩÂäõÁöÑÊèêÁ§∫‰πüÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËÉΩÈáèÊ®°ÂûãÊù•ÁîüÊàêÂ§ö‰∏™‰∏çÂêåÁöÑÊèêÁ§∫ÂµåÂÖ•Ôºå‰ªéËÄåÊõ¥ÂÖ®Èù¢Âú∞Ë°®Ëææ‰ªªÂä°ÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇÈÄöËøá‰ªéÁî±VLMsÈöêÂºèÂÆö‰πâÁöÑËÉΩÈáèÂàÜÂ∏É‰∏≠ÈááÊ†∑ÔºåÂèØ‰ª•Ëé∑ÂæóÂ§öÊ†∑ÂåñÁöÑÊèêÁ§∫ÔºåËøô‰∫õÊèêÁ§∫ËÉΩÂ§üÊçïÊçâÂà∞‰ªªÂä°ÁöÑ‰∏çÂêåÊñπÈù¢Ôºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®Âπ≥Ë°°È¢ÜÂüüÂÜÖÊÄßËÉΩÂíåÈ¢ÜÂüüÂ§ñÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEMPLÊñπÊ≥ïÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) Âà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàVLMsÔºâÊèêÂèñÂõæÂÉèÂíåÊñáÊú¨ÁâπÂæÅÔºõ2) ÂÆö‰πâ‰∏Ä‰∏™Âü∫‰∫éËÉΩÈáèÁöÑÂàÜÂ∏ÉÔºåËØ•ÂàÜÂ∏ÉÁî±VLMsÈöêÂºèÂÆö‰πâÔºåÁî®‰∫éÁîüÊàêÂ§ö‰∏™ÊèêÁ§∫ÂµåÂÖ•Ôºõ3) ‰ªéËÉΩÈáèÂàÜÂ∏É‰∏≠ÈááÊ†∑ÔºåÁîüÊàêÂ§ö‰∏™‰∏çÂêåÁöÑÊèêÁ§∫ÂµåÂÖ•Ôºõ4) Â∞ÜÁîüÊàêÁöÑÊèêÁ§∫ÂµåÂÖ•‰∏éÂõæÂÉèÁâπÂæÅËøõË°åËûçÂêàÔºåÂæóÂà∞ÊúÄÁªàÁöÑË°®Á§∫Ôºõ5) Âà©Áî®ÊúÄÁªàÁöÑË°®Á§∫ËøõË°å‰∏ãÊ∏∏‰ªªÂä°ÁöÑÈ¢ÑÊµã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEMPLÊñπÊ≥ïÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂà©Áî®ËÉΩÈáèÊ®°ÂûãÊù•ÁîüÊàêÂ§ö‰∏™ÊèêÁ§∫ÂµåÂÖ•„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïÊèêÁ§∫Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåEMPLËÉΩÂ§üÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊèêÁ§∫Ôºå‰ªéËÄåÊõ¥ÂÖ®Èù¢Âú∞Ë°®Ëææ‰ªªÂä°ÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåEMPLÊñπÊ≥ïÊòØÂèÇÊï∞È´òÊïàÁöÑÔºåÂõ†‰∏∫ÂÆÉ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂèÇÊï∞Êù•Â≠¶‰π†ÊèêÁ§∫ÂµåÂÖ•ÔºåËÄåÊòØÁõ¥Êé•‰ªéVLMs‰∏≠ÊèêÂèñ‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöEMPLÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ËÉΩÈáèÂáΩÊï∞ÁöÑÂÆö‰πâÔºåËØ•ÂáΩÊï∞Áî®‰∫éË°°ÈáèÊèêÁ§∫ÂµåÂÖ•‰∏éÂõæÂÉèÁâπÂæÅ‰πãÈó¥ÁöÑÂÖºÂÆπÊÄßÔºõ2) ÈááÊ†∑Á≠ñÁï•ÔºåÁî®‰∫é‰ªéËÉΩÈáèÂàÜÂ∏É‰∏≠ÁîüÊàêÂ§ö‰∏™ÊèêÁ§∫ÂµåÂÖ•Ôºõ3) ËûçÂêàÊú∫Âà∂ÔºåÁî®‰∫éÂ∞ÜÁîüÊàêÁöÑÊèêÁ§∫ÂµåÂÖ•‰∏éÂõæÂÉèÁâπÂæÅËøõË°åËûçÂêà„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑ‰∏ãÊ∏∏‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÊèêÂá∫ÁöÑEMPLÊñπÊ≥ïÂú®Â§ö‰∏™ËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåEMPLÊñπÊ≥ïÁõ∏ÊØî‰∫é‰º†ÁªüÁöÑÂçïÊèêÁ§∫Â≠¶‰π†ÊñπÊ≥ïÔºåÂáÜÁ°ÆÁéáÊèêÂçá‰∫ÜX%„ÄÇÊ≠§Â§ñÔºåEMPLÊñπÊ≥ïÂú®È¢ÜÂüüÂ§ñÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞‰πü‰ºò‰∫éÂÖ∂‰ªñÂü∫Á∫øÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂºÄÊîæËØçÊ±áÁéØÂ¢É‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°Ôºå‰æãÂ¶ÇÂõæÂÉèÂàÜÁ±ª„ÄÅÂõæÂÉèÊ£ÄÁ¥¢„ÄÅËßÜËßâÈóÆÁ≠îÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÂú®ÂºÄÊîæËØçÊ±áÁéØÂ¢É‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•ÊúâÊïàËß£ÂÜ≥ÂÆûÈôÖÂ∫îÁî®‰∏≠Êï∞ÊçÆÂàÜÂ∏É‰∏çÂåπÈÖçÁöÑÈóÆÈ¢òÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÊΩúÂäõ„ÄÇÊú™Êù•ÂèØËøõ‰∏ÄÊ≠•Êé¢Á¥¢Â¶Ç‰ΩïÂ∞ÜEMPLÊñπÊ≥ïÂ∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑËßÜËßâ-ËØ≠Ë®ÄÂú∫ÊôØÔºå‰æãÂ¶ÇËßÜÈ¢ëÁêÜËß£ÂíåÂ§öÊ®°ÊÄÅÂØπËØù„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.

