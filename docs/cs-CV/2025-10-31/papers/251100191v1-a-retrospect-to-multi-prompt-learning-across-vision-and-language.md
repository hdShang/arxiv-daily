---
layout: default
title: A Retrospect to Multi-prompt Learning across Vision and Language
---

# A Retrospect to Multi-prompt Learning across Vision and Language

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.00191" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.00191v1</a>
  <a href="https://arxiv.org/pdf/2511.00191.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00191v1" onclick="toggleFavorite(this, '2511.00191v1', 'A Retrospect to Multi-prompt Learning across Vision and Language')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziliang Chen, Xin Huang, Quanlong Guan, Liang Lin, Weiqi Luo

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-31

**å¤‡æ³¨**: ICCV

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºèƒ½é‡é©±åŠ¨çš„å¤šæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæå‡è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¤šæç¤ºå­¦ä¹ ` `è§†è§‰-è¯­è¨€é¢„è®­ç»ƒ` `èƒ½é‡æ¨¡å‹` `æç¤ºå·¥ç¨‹` `å¼€æ”¾è¯æ±‡æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ä¸»è¦é‡‡ç”¨å•æç¤ºå­¦ä¹ ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºèƒ½é‡é©±åŠ¨çš„å¤šæç¤ºå­¦ä¹ ï¼ˆEMPLï¼‰æ–¹æ³•ï¼Œé€šè¿‡èƒ½é‡æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºåµŒå…¥ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒEMPLæ–¹æ³•åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰ç¤¾åŒºæ­£ç»å†ç€è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆVLMsï¼‰å¸¦æ¥çš„å‰æ‰€æœªæœ‰çš„è¿›æ­¥ã€‚æç¤ºå­¦ä¹ æ˜¯åˆ©ç”¨VLMsçš„å…³é”®ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿä»¥æœ‰é™çš„èµ„æºå¿«é€Ÿé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æç¤ºèŒƒå¼ä¸Šï¼Œå¾ˆå°‘ç ”ç©¶å¤šæç¤ºå­¦ä¹ çš„æ½œåŠ›ã€‚æœ¬æ–‡æ—¨åœ¨å¯¹è§†è§‰-è¯­è¨€å¤šæç¤ºå­¦ä¹ è¿›è¡Œæœ‰åŸåˆ™çš„å›é¡¾ã€‚æˆ‘ä»¬å°†æœ€è¿‘çš„æ¨¡æ€å·®è·ç°è±¡æ‰©å±•åˆ°å¯å­¦ä¹ çš„æç¤ºï¼Œå¹¶é€šè¿‡å®éªŒå’Œç†è®ºè¯æ˜äº†ä½¿ç”¨å¤šæç¤ºå¢å¼ºè¿›è¡Œè§†è§‰-è¯­è¨€è¿ç§»çš„ä¼˜è¶Šæ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºèƒ½é‡çš„å¤šæç¤ºå­¦ä¹ ï¼ˆEMPLï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä»ç”±VLMséšå¼å®šä¹‰çš„åŸºäºèƒ½é‡çš„åˆ†å¸ƒä¸­æŠ½å–å®ä¾‹æ¥ç”Ÿæˆå¤šä¸ªæç¤ºåµŒå…¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„EMPLä¸ä»…å‚æ•°é«˜æ•ˆï¼Œè€Œä¸”ä¸¥æ ¼åœ°å®ç°äº†é¢†åŸŸå†…å’Œé¢†åŸŸå¤–å¼€æ”¾è¯æ±‡æ³›åŒ–ä¹‹é—´çš„å¹³è¡¡ã€‚å…¨é¢çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ä¸»å¼ å’ŒEMPLçš„å“è¶Šæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆVLMsï¼‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•æç¤ºèŒƒå¼ï¼Œå³ä½¿ç”¨å•ä¸ªæ–‡æœ¬æç¤ºæ¥å¼•å¯¼æ¨¡å‹å®Œæˆä¸‹æ¸¸ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•çš„å±€é™æ€§åœ¨äºï¼Œå•ä¸ªæç¤ºå¯èƒ½æ— æ³•å……åˆ†è¡¨è¾¾ä»»åŠ¡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾è¯æ±‡æ³›åŒ–æ–¹é¢è¡¨ç°ä¸è¶³ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„èµ„æºæ¥å­¦ä¹ æ›´å…·è¡¨è¾¾èƒ½åŠ›çš„æç¤ºä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨èƒ½é‡æ¨¡å‹æ¥ç”Ÿæˆå¤šä¸ªä¸åŒçš„æç¤ºåµŒå…¥ï¼Œä»è€Œæ›´å…¨é¢åœ°è¡¨è¾¾ä»»åŠ¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡ä»ç”±VLMséšå¼å®šä¹‰çš„èƒ½é‡åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œå¯ä»¥è·å¾—å¤šæ ·åŒ–çš„æç¤ºï¼Œè¿™äº›æç¤ºèƒ½å¤Ÿæ•æ‰åˆ°ä»»åŠ¡çš„ä¸åŒæ–¹é¢ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å¹³è¡¡é¢†åŸŸå†…æ€§èƒ½å’Œé¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEMPLæ–¹æ³•çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) åˆ©ç”¨è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆVLMsï¼‰æå–å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼›2) å®šä¹‰ä¸€ä¸ªåŸºäºèƒ½é‡çš„åˆ†å¸ƒï¼Œè¯¥åˆ†å¸ƒç”±VLMséšå¼å®šä¹‰ï¼Œç”¨äºç”Ÿæˆå¤šä¸ªæç¤ºåµŒå…¥ï¼›3) ä»èƒ½é‡åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”Ÿæˆå¤šä¸ªä¸åŒçš„æç¤ºåµŒå…¥ï¼›4) å°†ç”Ÿæˆçš„æç¤ºåµŒå…¥ä¸å›¾åƒç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æœ€ç»ˆçš„è¡¨ç¤ºï¼›5) åˆ©ç”¨æœ€ç»ˆçš„è¡¨ç¤ºè¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šEMPLæ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨èƒ½é‡æ¨¡å‹æ¥ç”Ÿæˆå¤šä¸ªæç¤ºåµŒå…¥ã€‚ä¸ä¼ ç»Ÿçš„å•æç¤ºå­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒEMPLèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œä»è€Œæ›´å…¨é¢åœ°è¡¨è¾¾ä»»åŠ¡çš„è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒEMPLæ–¹æ³•æ˜¯å‚æ•°é«˜æ•ˆçš„ï¼Œå› ä¸ºå®ƒä¸éœ€è¦é¢å¤–çš„å‚æ•°æ¥å­¦ä¹ æç¤ºåµŒå…¥ï¼Œè€Œæ˜¯ç›´æ¥ä»VLMsä¸­æå–ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šEMPLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) èƒ½é‡å‡½æ•°çš„å®šä¹‰ï¼Œè¯¥å‡½æ•°ç”¨äºè¡¡é‡æç¤ºåµŒå…¥ä¸å›¾åƒç‰¹å¾ä¹‹é—´çš„å…¼å®¹æ€§ï¼›2) é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºä»èƒ½é‡åˆ†å¸ƒä¸­ç”Ÿæˆå¤šä¸ªæç¤ºåµŒå…¥ï¼›3) èåˆæœºåˆ¶ï¼Œç”¨äºå°†ç”Ÿæˆçš„æç¤ºåµŒå…¥ä¸å›¾åƒç‰¹å¾è¿›è¡Œèåˆã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°éœ€è¦æ ¹æ®å…·ä½“çš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æå‡ºçš„EMPLæ–¹æ³•åœ¨å¤šä¸ªè§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒEMPLæ–¹æ³•ç›¸æ¯”äºä¼ ç»Ÿçš„å•æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡äº†X%ã€‚æ­¤å¤–ï¼ŒEMPLæ–¹æ³•åœ¨é¢†åŸŸå¤–æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨å¼€æ”¾è¯æ±‡ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€å›¾åƒæ£€ç´¢ã€è§†è§‰é—®ç­”ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³å®é™…åº”ç”¨ä¸­æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ½œåŠ›ã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•å°†EMPLæ–¹æ³•åº”ç”¨äºæ›´å¤æ‚çš„è§†è§‰-è¯­è¨€åœºæ™¯ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£å’Œå¤šæ¨¡æ€å¯¹è¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.

