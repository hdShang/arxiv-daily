---
layout: default
title: Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study
---

# Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21160" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21160v1</a>
  <a href="https://arxiv.org/pdf/2510.21160.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21160v1" onclick="toggleFavorite(this, '2510.21160v1', 'Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanlin Wu, Boyan Su, Yang Zhao, Pu Wang, Yichen Lin, Hao Frank Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

**å¤‡æ³¨**: NeurIPS 2025 (Spotlight)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSIGç»“æ„åŒ–ç©ºé—´æ™ºèƒ½ç½‘æ ¼ï¼Œæå‡è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç©ºé—´æ™ºèƒ½` `è‡ªåŠ¨é©¾é©¶` `å¤šæ¨¡æ€å­¦ä¹ ` `ç‰©ç†å…ˆéªŒ` `åœºæ™¯ç†è§£` `å¤§è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-ç©ºé—´æ™ºèƒ½è¯„ä¼°æ–¹æ³•ä¾èµ–æ–‡æœ¬æç¤ºå’ŒVQAè¯„åˆ†ï¼Œæ˜“å—è¯­è¨€åå·®å½±å“ï¼Œéš¾ä»¥å‡†ç¡®è¯„ä¼°æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚
2. æå‡ºç©ºé—´æ™ºèƒ½ç½‘æ ¼ï¼ˆSIGï¼‰ï¼Œæ˜¾å¼ç¼–ç å¯¹è±¡å¸ƒå±€ã€å…³ç³»å’Œç‰©ç†å…ˆéªŒï¼Œä¸ºæ¨¡å‹æä¾›åœºæ™¯ç»“æ„çš„å¿ å®è¡¨ç¤ºï¼Œè¾…åŠ©ç©ºé—´æ¨ç†ã€‚
3. SIGåœ¨å¤šæ¨¡æ€LLMçš„å°‘é‡æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå¹¶å‘å¸ƒSIGBenchåŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒæœºå™¨å’Œç±»äººè§†è§‰-ç©ºé—´æ™ºèƒ½ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¦‚ä½•æ•´åˆå’ŒéªŒè¯åŸºç¡€æ¨¡å‹ä¸­çš„ç©ºé—´æ™ºèƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚ç›®å‰çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨çº¯æ–‡æœ¬æç¤ºå’ŒVQAé£æ ¼çš„è¯„åˆ†æ¥ä»£ç†è§†è§‰-ç©ºé—´æ™ºèƒ½(VSI)ï¼Œè¿™æ¨¡ç³Šäº†å‡ ä½•ä¿¡æ¯ï¼Œå¼•å…¥äº†è¯­è¨€æ·å¾„ï¼Œå¹¶å‰Šå¼±äº†å¯¹çœŸæ­£ç©ºé—´æŠ€èƒ½çš„å½’å› ã€‚æœ¬æ–‡æå‡ºäº†ç©ºé—´æ™ºèƒ½ç½‘æ ¼(SIG)ï¼šä¸€ç§ç»“æ„åŒ–çš„ã€åŸºäºç½‘æ ¼çš„æ¨¡å¼ï¼Œå®ƒæ˜¾å¼åœ°ç¼–ç äº†å¯¹è±¡å¸ƒå±€ã€å¯¹è±¡é—´çš„å…³ç³»å’Œç‰©ç†å…ˆéªŒã€‚ä½œä¸ºæ–‡æœ¬çš„è¡¥å……é€šé“ï¼ŒSIGä¸ºåŸºç¡€æ¨¡å‹çš„æ¨ç†æä¾›äº†åœºæ™¯ç»“æ„çš„å¿ å®ã€ç»„åˆè¡¨ç¤ºã€‚åŸºäºSIGï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†SIG-informedè¯„ä¼°æŒ‡æ ‡ï¼Œé‡åŒ–æ¨¡å‹å›ºæœ‰çš„VSIï¼Œå°†ç©ºé—´èƒ½åŠ›ä¸è¯­è¨€å…ˆéªŒåˆ†ç¦»ã€‚åœ¨ä½¿ç”¨æœ€å…ˆè¿›çš„å¤šæ¨¡æ€LLM(ä¾‹å¦‚GPTå’ŒGeminiç³»åˆ—æ¨¡å‹)è¿›è¡Œå°‘é‡æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ æ—¶ï¼Œä¸ä»…VQAè¡¨ç¤ºç›¸æ¯”ï¼ŒSIGåœ¨æ‰€æœ‰VSIæŒ‡æ ‡ä¸Šäº§ç”Ÿäº†ä¸€è‡´çš„ã€æ›´å¤§çš„ã€æ›´ç¨³å®šçš„å’Œæ›´å…¨é¢çš„å¢ç›Šï¼Œè¡¨æ˜äº†å®ƒä½œä¸ºå­¦ä¹ VSIçš„æ•°æ®æ ‡è®°å’Œè®­ç»ƒæ¨¡å¼çš„æ½œåŠ›ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†SIGBenchï¼Œä¸€ä¸ªåŒ…å«1.4Ké©¾é©¶å¸§çš„åŸºå‡†æµ‹è¯•ï¼Œæ ‡æ³¨äº†ground-truth SIGæ ‡ç­¾å’Œäººç±»è§†çº¿è½¨è¿¹ï¼Œæ”¯æŒè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­åŸºäºç½‘æ ¼çš„æœºå™¨VSIä»»åŠ¡å’Œæ³¨æ„åŠ›é©±åŠ¨çš„ã€ç±»äººçš„VSIä»»åŠ¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°è§†è§‰-ç©ºé—´æ™ºèƒ½æ—¶ï¼Œè¿‡åº¦ä¾èµ–æ–‡æœ¬æç¤ºå’ŒVQAé£æ ¼çš„è¯„åˆ†ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœå®¹æ˜“å—åˆ°è¯­è¨€åå·®çš„å½±å“ï¼Œæ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹çœŸæ­£çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨åœºæ™¯ä¸­çš„å‡ ä½•ä¿¡æ¯å’Œç‰©ç†å…ˆéªŒçŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ç§ç»“æ„åŒ–çš„ã€åŸºäºç½‘æ ¼çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå³ç©ºé—´æ™ºèƒ½ç½‘æ ¼ï¼ˆSIGï¼‰ï¼Œæ¥æ˜¾å¼åœ°ç¼–ç åœºæ™¯ä¸­çš„å¯¹è±¡å¸ƒå±€ã€å¯¹è±¡é—´çš„å…³ç³»ä»¥åŠç‰©ç†å…ˆéªŒçŸ¥è¯†ã€‚SIGä½œä¸ºæ–‡æœ¬ä¿¡æ¯çš„è¡¥å……ï¼Œä¸ºæ¨¡å‹æä¾›æ›´å‡†ç¡®ã€æ›´ä¸°å¯Œçš„åœºæ™¯ç»“æ„ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æ„å»ºSIGï¼šå°†åœºæ™¯åˆ’åˆ†ä¸ºç½‘æ ¼ï¼Œå¹¶åœ¨æ¯ä¸ªç½‘æ ¼ä¸­ç¼–ç å¯¹è±¡ä¿¡æ¯ã€å¯¹è±¡å…³ç³»å’Œç‰©ç†å…ˆéªŒï¼›2) å¤šæ¨¡æ€è¾“å…¥ï¼šå°†SIGä¸æ–‡æœ¬ä¿¡æ¯ç»“åˆï¼Œä½œä¸ºå¤šæ¨¡æ€LLMçš„è¾“å…¥ï¼›3) ç©ºé—´æ¨ç†ï¼šåˆ©ç”¨å¤šæ¨¡æ€LLMè¿›è¡Œç©ºé—´æ¨ç†ï¼Œä¾‹å¦‚é¢„æµ‹å¯¹è±¡ä¹‹é—´çš„å…³ç³»ã€åˆ¤æ–­åœºæ™¯çš„ç‰©ç†åˆç†æ€§ç­‰ï¼›4) SIG-informedè¯„ä¼°ï¼šä½¿ç”¨åŸºäºSIGçš„è¯„ä¼°æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ï¼Œé¿å…è¯­è¨€åå·®ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†SIGè¿™ç§ç»“æ„åŒ–çš„ç©ºé—´è¡¨ç¤ºæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„æ–‡æœ¬æè¿°ç›¸æ¯”ï¼ŒSIGèƒ½å¤Ÿæ›´å‡†ç¡®ã€æ›´å®Œæ•´åœ°è¡¨è¾¾åœºæ™¯ä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ˜¾å¼åœ°ç¼–ç ç‰©ç†å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†åŸºäºSIGçš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ›´å®¢è§‚åœ°è¯„ä¼°æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šSIGçš„è®¾è®¡åŒ…æ‹¬ä»¥ä¸‹å…³é”®è¦ç´ ï¼š1) ç½‘æ ¼åˆ’åˆ†ï¼šæ ¹æ®åœºæ™¯çš„å¤§å°å’Œå¤æ‚åº¦ï¼Œé€‰æ‹©åˆé€‚çš„ç½‘æ ¼å¤§å°ï¼›2) å¯¹è±¡ç¼–ç ï¼šåœ¨æ¯ä¸ªç½‘æ ¼ä¸­ç¼–ç å¯¹è±¡çš„ä½ç½®ã€å¤§å°ã€ç±»åˆ«ç­‰ä¿¡æ¯ï¼›3) å…³ç³»ç¼–ç ï¼šç¼–ç å¯¹è±¡ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œä¾‹å¦‚è·ç¦»ã€æ–¹å‘ã€é®æŒ¡å…³ç³»ç­‰ï¼›4) ç‰©ç†å…ˆéªŒï¼šç¼–ç åœºæ™¯ä¸­çš„ç‰©ç†è§„åˆ™ï¼Œä¾‹å¦‚é‡åŠ›ã€ç¢°æ’ç­‰ï¼›5) SIG-informedè¯„ä¼°æŒ‡æ ‡ï¼šè®¾è®¡èƒ½å¤Ÿé‡åŒ–æ¨¡å‹åœ¨SIGä¸Šçš„æ¨ç†èƒ½åŠ›çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚å¯¹è±¡å…³ç³»é¢„æµ‹å‡†ç¡®ç‡ã€ç‰©ç†åˆç†æ€§åˆ¤æ–­å‡†ç¡®ç‡ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨VQAè¡¨ç¤ºç›¸æ¯”ï¼ŒSIGåœ¨å¤šæ¨¡æ€LLMçš„å°‘é‡æ ·æœ¬å­¦ä¹ ä¸­ï¼Œåœ¨æ‰€æœ‰VSIæŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ›´å¤§ã€æ›´ç¨³å®šå’Œæ›´å…¨é¢çš„å¢ç›Šã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹è±¡å…³ç³»é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨SIGçš„æ¨¡å‹å‡†ç¡®ç‡æå‡äº†15%ã€‚æ­¤å¤–ï¼ŒSIGBenchåŸºå‡†æµ‹è¯•çš„å‘å¸ƒï¼Œä¸ºè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹çš„ç©ºé—´æ™ºèƒ½ç ”ç©¶æä¾›äº†æ–°çš„æ•°æ®èµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹åœºæ™¯ç©ºé—´ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ï¼Œå¢å¼ºæœºå™¨äººçš„ç¯å¢ƒé€‚åº”æ€§ï¼Œå¹¶æ”¹å–„å¢å¼ºç°å®çš„ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼ŒSIGçš„ç»“æ„åŒ–è¡¨ç¤ºæ–¹æ³•ä¹Ÿä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> How to integrate and verify spatial intelligence in foundation models remains an open challenge. Current practice often proxies Visual-Spatial Intelligence (VSI) with purely textual prompts and VQA-style scoring, which obscures geometry, invites linguistic shortcuts, and weakens attribution to genuinely spatial skills. We introduce Spatial Intelligence Grid (SIG): a structured, grid-based schema that explicitly encodes object layouts, inter-object relations, and physically grounded priors. As a complementary channel to text, SIG provides a faithful, compositional representation of scene structure for foundation-model reasoning. Building on SIG, we derive SIG-informed evaluation metrics that quantify a model's intrinsic VSI, which separates spatial capability from language priors. In few-shot in-context learning with state-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG yields consistently larger, more stable, and more comprehensive gains across all VSI metrics compared to VQA-only representations, indicating its promise as a data-labeling and training schema for learning VSI. We also release SIGBench, a benchmark of 1.4K driving frames annotated with ground-truth SIG labels and human gaze traces, supporting both grid-based machine VSI tasks and attention-driven, human-like VSI tasks in autonomous-driving scenarios.

