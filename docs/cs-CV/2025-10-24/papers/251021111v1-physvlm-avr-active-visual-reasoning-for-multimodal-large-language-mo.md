---
layout: default
title: PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments
---

# PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21111" target="_blank" class="toolbar-btn">arXiv: 2510.21111v1</a>
    <a href="https://arxiv.org/pdf/2510.21111.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21111v1" 
            onclick="toggleFavorite(this, '2510.21111v1', 'PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Weijie Zhou, Xuantang Xiong, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-24

**Â§áÊ≥®**: 39th Conference on Neural Information Processing Systemss (NeurIPS 2025)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PhysVLM-AVR‰ª•Ëß£ÂÜ≥Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑËßÜËßâÊé®ÁêÜÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰∏ªÂä®ËßÜËßâÊé®ÁêÜ` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `‰ø°ÊÅØËé∑Âèñ` `Âä®ÊÄÅÁéØÂ¢É` `Êé®ÁêÜÊ≠£Á°ÆÊÄß` `‰∫§‰∫íÁéØÂ¢É` `ÈìæÂºèÊÄùÁª¥`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÈùôÊÄÅÁéØÂ¢É‰∏≠ËøõË°åËßÜËßâÊé®ÁêÜÔºåÊó†Ê≥ïÊúâÊïàÂ∫îÂØπÁé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑ‰ø°ÊÅØ‰∏çÂÆåÊï¥ÊÄßÂíåÂä®ÊÄÅÂèòÂåñ„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∏ªÂä®ËßÜËßâÊé®ÁêÜÔºàAVRÔºâ‰ªªÂä°ÔºåË¶ÅÊ±ÇÊô∫ËÉΩ‰ΩìÈÄöËøáÁâ©ÁêÜÂä®‰Ωú‰∏ªÂä®Ëé∑Âèñ‰ø°ÊÅØÔºåÂπ∂Âú®Â§öÊ≠•È™§‰∏≠Êï¥ÂêàËßÇÂØüÁªìÊûú‰ª•ËøõË°åÊé®ÁêÜ„ÄÇ
3. PhysVLM-AVRÂú®CLEVR-AVRÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ËøõË°åÊúâÊïàÊé®ÁêÜÁöÑËÉΩÂäõÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâÊ®°ÂûãÁöÑ‰∏çË∂≥„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ‰∏≠ÔºåËßÜËßâÊé®ÁêÜ‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÈùôÊÄÅ„ÄÅÂÆåÂÖ®ÂèØËßÇÂØüÁöÑÁéØÂ¢É‰∏≠ÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ‰∫∫Á±ªÈÄöËøá‰∏ªÂä®Êé¢Á¥¢Âíå‰∏éÁéØÂ¢É‰∫íÂä®Êù•Ëé∑Âèñ‰ø°ÊÅØ„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ªÂä®ËßÜËßâÊé®ÁêÜÔºàAVRÔºâ‰ªªÂä°ÔºåÊâ©Â±ï‰∫ÜËßÜËßâÊé®ÁêÜËá≥ÈÉ®ÂàÜÂèØËßÇÂØüÁöÑ‰∫íÂä®ÁéØÂ¢É„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜCLEVR-AVRÂü∫ÂáÜÔºåËØÑ‰º∞Êé®ÁêÜÁöÑÊ≠£Á°ÆÊÄßÂíå‰ø°ÊÅØËé∑ÂèñÁöÑÊïàÁéáÔºåÂπ∂ÊûÑÂª∫‰∫ÜAVR-152kÊï∞ÊçÆÈõÜÔºåÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑÈìæÂºèÊÄùÁª¥Ê≥®Èáä„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜPhysVLM-AVRÔºåÂÆûÁé∞Âú®CLEVR-AVRÁ≠â‰ªªÂä°‰∏äÁöÑÊúÄÂÖàËøõÊÄßËÉΩÔºåÂêåÊó∂Êè≠Á§∫‰∫ÜÁé∞ÊúâÊ®°ÂûãÂú®‰∏ªÂä®Ëé∑ÂèñÂíåÊï¥ÂêàÊñ∞‰ø°ÊÅØÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Âä®ÊÄÅ„ÄÅÈÉ®ÂàÜÂèØËßÇÂØüÁéØÂ¢É‰∏≠ËøõË°åËßÜËßâÊé®ÁêÜÁöÑ‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜ‰ø°ÊÅØÁöÑ‰∏çÂÆåÊï¥ÊÄßÂíåÂä®ÊÄÅÂèòÂåñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫‰∏ªÂä®ËßÜËßâÊé®ÁêÜÔºàAVRÔºâ‰ªªÂä°ÔºåÊô∫ËÉΩ‰ΩìÈÄöËøáÁâ©ÁêÜÂä®‰Ωú‰∏ªÂä®Ëé∑Âèñ‰ø°ÊÅØÔºåÂπ∂Âú®Â§öÊ≠•È™§‰∏≠Êï¥ÂêàËßÇÂØüÁªìÊûú‰ª•ËøõË°åÊé®ÁêÜÔºåÂä®ÊÄÅË∞ÉÊï¥ÂÜ≥Á≠ñ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰ø°ÊÅØËé∑ÂèñÊ®°Âùó„ÄÅËßÇÂØüÊï¥ÂêàÊ®°ÂùóÂíåÂÜ≥Á≠ñË∞ÉÊï¥Ê®°ÂùóÔºåÊô∫ËÉΩ‰ΩìÂú®ÊØè‰∏™Ê≠•È™§‰∏≠Ê†πÊçÆËßÜËßâÂèçÈ¶àËøõË°åÂÜ≥Á≠ñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂºïÂÖ•CLEVR-AVRÂü∫ÂáÜÂíåAVR-152kÊï∞ÊçÆÈõÜÔºåÊèê‰æõÂ§öËΩÆ‰∫§‰∫íÁéØÂ¢ÉÁöÑËØÑ‰º∞Ê†áÂáÜÔºåÂº∫Ë∞É‰ø°ÊÅØËé∑ÂèñÊïàÁéáÂíåÊé®ÁêÜÊ≠£Á°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈááÁî®ÈìæÂºèÊÄùÁª¥Ê≥®ÈáäÔºåËÆæËÆ°‰∫ÜÈ´òÈò∂È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂÖ≥ÈîÆÂèÇÊï∞ÂíåÊçüÂ§±ÂáΩÊï∞ÁªèËøáÁ≤æÂøÉË∞ÉÊï¥‰ª•‰ºòÂåñÊô∫ËÉΩ‰ΩìÁöÑÂ≠¶‰π†ÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

PhysVLM-AVRÂú®CLEVR-AVRÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÊúâÊïàÊé®ÁêÜÁöÑËÉΩÂäõ„ÄÇ‰∏éÁé∞ÊúâÊ®°ÂûãÁõ∏ÊØîÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ÊòæËëóÔºåÂ∞§ÂÖ∂Âú®‰ø°ÊÅØËé∑ÂèñÂíåÊé®ÁêÜÁöÑÊï¥ÂêàÊñπÈù¢Ë°®Áé∞Á™ÅÂá∫„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÂØºËà™„ÄÅÊô∫ËÉΩÂä©ÊâãÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÔºåËÉΩÂ§üÊèêÂçáÊô∫ËÉΩ‰ΩìÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõÂíå‰∫§‰∫íÊïàÁéá„ÄÇÊú™Êù•ÔºåÈöèÁùÄÊäÄÊúØÁöÑËøõÊ≠•ÔºåPhysVLM-AVRÊúâÊúõÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÆûÁé∞Êõ¥È´òÊ∞¥Âπ≥ÁöÑÊô∫ËÉΩÂåñÂíåËá™‰∏ªÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Visual reasoning in multimodal large language models (MLLMs) has primarily been studied in static, fully observable settings, limiting their effectiveness in real-world environments where information is often incomplete due to occlusion or limited field of view. Humans, in contrast, actively explore and interact with their environment-moving, examining, and manipulating objects-to gather information through a closed-loop process integrating perception, reasoning, and action. Inspired by this human capability, we introduce the Active Visual Reasoning (AVR) task, extending visual reasoning to partially observable, interactive environments. AVR necessitates agents to: (1) actively acquire information via sequential physical actions, (2) integrate observations across multiple steps for coherent reasoning, and (3) dynamically adjust decisions based on evolving visual feedback. To rigorously evaluate AVR, we introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive environments designed to assess both reasoning correctness and information-gathering efficiency. We present AVR-152k, a large-scale dataset that offers rich Chain-of-Thought (CoT) annotations detailing iterative reasoning for uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection, crucial for training agents in a higher-order Markov Decision Process. Building on this, we develop PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR, embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath, Geometry30K). Our analysis also reveals that current embodied MLLMs, despite detecting information incompleteness, struggle to actively acquire and integrate new information through interaction, highlighting a fundamental gap in active reasoning capabilities.

