---
layout: default
title: Sparser Block-Sparse Attention via Token Permutation
---

# Sparser Block-Sparse Attention via Token Permutation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21270" target="_blank" class="toolbar-btn">arXiv: 2510.21270v1</a>
    <a href="https://arxiv.org/pdf/2510.21270.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21270v1" 
            onclick="toggleFavorite(this, '2510.21270v1', 'Sparser Block-Sparse Attention via Token Permutation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xinghao Wang, Pengyu Wang, Dong Zhang, Chenkun Tan, Shaojun Zhou, Zhaoxiang Liu, Shiguo Lian, Fangxu Liu, Kai Song, Xipeng Qiu

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-24

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/xinghaow99/pbs-attn)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éTokenÁΩÆÊç¢ÁöÑÁ®ÄÁñèÂùóÊ≥®ÊÑèÂäõÊú∫Âà∂PBS-AttnÔºåÂä†ÈÄüÈïøÊñáÊú¨LLMÈ¢ÑÂ°´ÂÖÖ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÊñáÊú¨Â§ÑÁêÜ` `Á®ÄÁñèÊ≥®ÊÑèÂäõ` `ÂùóÁ®ÄÁñèÊÄß` `TokenÁΩÆÊç¢` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËÆ°ÁÆóÊïàÁéá` `FlashAttention`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÈïøÊñáÊú¨Â§ÑÁêÜ‰∏≠ÔºåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂπ≥ÊñπÂ§çÊùÇÂ∫¶ÊòØLLMÊâ©Â±ï‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑ‰∏ªË¶ÅÁì∂È¢à„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫PBS-AttnÔºåÈÄöËøátokenÁΩÆÊç¢Â¢ûÂä†ÂùóÁ®ÄÁñèÊÄßÔºåÊèêÂçáËÆ°ÁÆóÊïàÁéáÔºå‰∏îÊòì‰∫éÈõÜÊàê„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåPBS-AttnÂú®Á≤æÂ∫¶‰∏ä‰ºò‰∫éÁé∞ÊúâÂùóÁ®ÄÁñèÊñπÊ≥ïÔºåÂπ∂Êé•ËøëÂÆåÊï¥Ê≥®ÊÑèÂäõÔºåÂä†ÈÄüÈ´òËææ2.75ÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êâ©Â±ïÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂÖ∑ÊúâÊòæËëó‰ºòÂäøÔºå‰ΩÜËÆ°ÁÆóÊàêÊú¨ÂæàÈ´ò„ÄÇËøôÁßçÊàêÊú¨‰∏ªË¶ÅÊ∫ê‰∫éËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂÖ∂Áõ∏ÂØπ‰∫éÂ∫èÂàóÈïøÂ∫¶ÁöÑ$O(N^2)$Â§çÊùÇÂ∫¶ÂØπÂÜÖÂ≠òÂíåÂª∂ËøüÊûÑÊàê‰∫Ü‰∏ªË¶ÅÁì∂È¢à„ÄÇÂπ∏ËøêÁöÑÊòØÔºåÊ≥®ÊÑèÂäõÁü©ÈòµÈÄöÂ∏∏ÊòØÁ®ÄÁñèÁöÑÔºåÁâπÂà´ÊòØÂØπ‰∫éÈïøÂ∫èÂàóÔºåËøôË°®ÊòéÂ≠òÂú®‰ºòÂåñÁöÑÊú∫‰ºö„ÄÇÂùóÁ®ÄÁñèÊ≥®ÊÑèÂäõÂ∑≤ÁªèÊàê‰∏∫‰∏ÄÁßçÊúâÂâçÈÄîÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂÆÉÂ∞ÜÂ∫èÂàóÂàíÂàÜ‰∏∫ÂùóÔºåÂπ∂Ë∑≥ËøáÈÉ®ÂàÜÂùóÁöÑËÆ°ÁÆó„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÈ´òÂ∫¶‰æùËµñ‰∫éÂ∫ïÂ±ÇÁöÑÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåËøôÂèØËÉΩÂØºËá¥Ê¨°‰ºòÁöÑÂùóÁ∫ßÁ®ÄÁñèÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂçï‰∏™ÂùóÂÜÖÊü•ËØ¢ÁöÑÈáçË¶ÅÈîÆtokenÂèØËÉΩÂàÜÊï£Âú®ËÆ∏Â§öÂÖ∂‰ªñÂùó‰∏≠ÔºåÂØºËá¥ËÆ°ÁÆóÂÜó‰Ωô„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁΩÆÊç¢ÂùóÁ®ÄÁñèÊ≥®ÊÑèÂäõÔºàPBS-AttnÔºâÔºåËøôÊòØ‰∏ÄÁßçÂç≥ÊèíÂç≥Áî®ÁöÑÊñπÊ≥ïÔºåÂÆÉÂà©Áî®Ê≥®ÊÑèÂäõÁöÑÁΩÆÊç¢Â±ûÊÄßÊù•Â¢ûÂä†ÂùóÁ∫ßÁ®ÄÁñèÊÄßÂπ∂ÊèêÈ´òLLMÈ¢ÑÂ°´ÂÖÖÁöÑËÆ°ÁÆóÊïàÁéá„ÄÇÊàë‰ª¨Âú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÁúüÂÆûÈïø‰∏ä‰∏ãÊñáÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂÆûÈ™åÔºåË°®ÊòéPBS-AttnÂú®Ê®°ÂûãÁ≤æÂ∫¶ÊñπÈù¢ÂßãÁªà‰ºò‰∫éÁé∞ÊúâÁöÑÂùóÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÔºåÂπ∂‰∏î‰∏éÂÆåÊï¥Ê≥®ÊÑèÂäõÂü∫Á∫øÈùûÂ∏∏Êé•Ëøë„ÄÇÂú®Êàë‰ª¨ÁöÑÂÆöÂà∂ÁΩÆÊç¢FlashAttentionÂÜÖÊ†∏ÁöÑÊîØÊåÅ‰∏ãÔºåPBS-AttnÂú®Èïø‰∏ä‰∏ãÊñáÈ¢ÑÂ°´ÂÖÖ‰∏≠ÂÆûÁé∞‰∫ÜÈ´òËææ2.75ÂÄçÁöÑÁ´ØÂà∞Á´ØÂä†ÈÄüÔºåËØÅÂÆû‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØË°åÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïøÊñáÊú¨Â§ÑÁêÜ‰∏≠ÔºåËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ËÆ°ÁÆóÂ§çÊùÇÂ∫¶ËøáÈ´òÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÂùóÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïËôΩÁÑ∂ËÉΩÈôç‰ΩéËÆ°ÁÆóÈáèÔºå‰ΩÜÂÖ∂ÊÄßËÉΩÂèóÈôê‰∫éÂ∫ïÂ±ÇÊ≥®ÊÑèÂäõÊ®°ÂºèÔºåÂèØËÉΩÂØºËá¥Ê¨°‰ºòÁöÑÂùóÁ∫ßÁ®ÄÁñèÊÄßÂíåËÆ°ÁÆóÂÜó‰Ωô„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®tokenÁΩÆÊç¢ÁöÑÁâπÊÄßÔºåÈáçÊñ∞ÊéíÂàótokenÁöÑÈ°∫Â∫èÔºå‰ΩøÂæóÂéüÊú¨ÂàÜÊï£Âú®‰∏çÂêåÂùó‰∏≠ÁöÑÈáçË¶ÅtokenËÉΩÂ§üÈõÜ‰∏≠Âà∞Â∞ëÊï∞Âùó‰∏≠Ôºå‰ªéËÄåÊèêÈ´òÂùóÁ®ÄÁñèÊÄßÔºåÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆó„ÄÇÈÄöËøáÁΩÆÊç¢Êìç‰ΩúÔºå‰ΩøÂæóÊ≥®ÊÑèÂäõÊõ¥Âä†ÈõÜ‰∏≠Ôºå‰ªéËÄåÊèêÂçáÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPBS-AttnÊòØ‰∏Ä‰∏™Âç≥ÊèíÂç≥Áî®ÁöÑÊ®°ÂùóÔºåÂèØ‰ª•ÂµåÂÖ•Âà∞Áé∞ÊúâÁöÑTransformerÊû∂ÊûÑ‰∏≠„ÄÇÂÖ∂‰∏ªË¶ÅÊµÅÁ®ãÂåÖÊã¨Ôºö1) ËæìÂÖ•Â∫èÂàóÂàÜÂùóÔºõ2) ÂØπÊØè‰∏™ÂùóÂÜÖÁöÑtokenËøõË°åÁΩÆÊç¢Ôºõ3) ËøõË°åÂùóÁ®ÄÁñèÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºõ4) ÂØπËæìÂá∫ËøõË°åÈÄÜÁΩÆÊç¢ÔºåÊÅ¢Â§çÂéüÂßãÈ°∫Â∫è„ÄÇÊï¥‰∏™ËøáÁ®ãÊó†ÈúÄ‰øÆÊîπÂéüÊúâÁöÑÊ®°ÂûãÁªìÊûÑÔºåÊòì‰∫éÈõÜÊàê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂü∫‰∫étokenÁΩÆÊç¢ÁöÑÂùóÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑÂùóÁ®ÄÁñèÊñπÊ≥ï‰∏çÂêåÔºåPBS-Attn‰∏çÊòØÁÆÄÂçïÂú∞Ë∑≥ËøáÊüê‰∫õÂùóÁöÑËÆ°ÁÆóÔºåËÄåÊòØÈÄöËøáÁΩÆÊç¢Êìç‰ΩúÊîπÂèò‰∫ÜÊ≥®ÊÑèÂäõÊ®°ÂºèÔºå‰ΩøÂæóÊ≥®ÊÑèÂäõÊõ¥Âä†ÈõÜ‰∏≠Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂùóÁ®ÄÁñèÊÄßÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂÆöÂà∂‰∫Üpermuted-FlashAttentionÂÜÖÊ†∏ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÁΩÆÊç¢Á≠ñÁï•ÁöÑÈÄâÊã©ÔºöËÆ∫ÊñáÂèØËÉΩÊé¢Á¥¢‰∫Ü‰∏çÂêåÁöÑÁΩÆÊç¢Á≠ñÁï•Ôºå‰æãÂ¶ÇÈöèÊú∫ÁΩÆÊç¢„ÄÅÂü∫‰∫éÊ≥®ÊÑèÂäõÊùÉÈáçÁöÑÁΩÆÊç¢Á≠âÔºå‰ª•ÊâæÂà∞ÊúÄ‰ºòÁöÑÁΩÆÊç¢ÊñπÂºè„ÄÇ2) ÂùóÂ§ßÂ∞èÁöÑËÆæÁΩÆÔºöÂùóÂ§ßÂ∞èÁöÑÈÄâÊã©‰ºöÂΩ±ÂìçÂùóÁ®ÄÁñèÊÄßÁöÑÊïàÊûúÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑ‰ªªÂä°ÂíåÊï∞ÊçÆÈõÜËøõË°åË∞ÉÊï¥„ÄÇ3) ÂÆöÂà∂ÁöÑpermuted-FlashAttentionÂÜÖÊ†∏ÔºöËØ•ÂÜÖÊ†∏ÈíàÂØπÁΩÆÊç¢ÂêéÁöÑÊï∞ÊçÆËøõË°å‰∫Ü‰ºòÂåñÔºåËÉΩÂ§üÂÖÖÂàÜÂà©Áî®Á°¨‰ª∂ËµÑÊ∫êÔºåÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPBS-AttnÂú®Èïø‰∏ä‰∏ãÊñáÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÂùóÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÔºåÂπ∂‰∏îÂú®Ê®°ÂûãÁ≤æÂ∫¶‰∏ä‰∏éÂÆåÊï¥Ê≥®ÊÑèÂäõÂü∫Á∫øÈùûÂ∏∏Êé•Ëøë„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÈÄöËøáÂÆöÂà∂ÁöÑpermuted-FlashAttentionÂÜÖÊ†∏ÔºåPBS-AttnÂú®Èïø‰∏ä‰∏ãÊñáÈ¢ÑÂ°´ÂÖÖ‰∏≠ÂÆûÁé∞‰∫ÜÈ´òËææ2.75ÂÄçÁöÑÁ´ØÂà∞Á´ØÂä†ÈÄüÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØË°åÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PBS-AttnÂèØÂ∫îÁî®‰∫éÈúÄË¶ÅÂ§ÑÁêÜÈïøÊñáÊú¨ÁöÑÂêÑÁßçÂú∫ÊôØÔºåÂ¶ÇÊñáÊ°£ÊëòË¶Å„ÄÅÊú∫Âô®ÁøªËØë„ÄÅ‰ª£Á†ÅÁîüÊàê„ÄÅÂØπËØùÁ≥ªÁªüÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊîØÊåÅÊõ¥ÈïøÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶Ôºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåPBS-AttnÁöÑÂç≥ÊèíÂç≥Áî®ÁâπÊÄß‰ΩøÂÖ∂Êòì‰∫éÈõÜÊàêÂà∞Áé∞ÊúâÁöÑLLM‰∏≠ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose $O(N^2)$ complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (\textbf{PBS-Attn}), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to $2.75\times$ in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn

