---
layout: default
title: FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning
---

# FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21311" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21311v1</a>
  <a href="https://arxiv.org/pdf/2510.21311.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21311v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21311v1', 'FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lu Zhang, Jiazuo Yu, Haomiao Xiong, Ping Hu, Yunzhi Zhuge, Huchuan Lu, You He

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

**å¤‡æ³¨**: Accepted to NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFineRSï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ è§£å†³MLLMåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­å°ç›®æ ‡ç²¾ç»†æ¨ç†ä¸åˆ†å‰²éš¾é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å°ç›®æ ‡åˆ†å‰²` `å¼ºåŒ–å­¦ä¹ ` `é«˜åˆ†è¾¨ç‡å›¾åƒ` `ç²—åˆ°ç²¾æ–¹æ³•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MLLMåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒä¸­çš„å°ç›®æ ‡æ—¶ï¼Œç”±äºè¾“å…¥åˆ†è¾¨ç‡é™åˆ¶ï¼Œéš¾ä»¥è¿›è¡Œç²¾ç¡®å®šä½å’Œç†è§£ã€‚
2. FineRSé‡‡ç”¨ç²—åˆ°ç²¾çš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å…¨å±€è¯­ä¹‰æ¢ç´¢å’Œå±€éƒ¨æ„ŸçŸ¥ç»†åŒ–ï¼Œå®ç°å°ç›®æ ‡çš„ç²¾ç»†æ¨ç†ä¸åˆ†å‰²ã€‚
3. åœ¨FineRS-4kç­‰æ•°æ®é›†ä¸Šï¼ŒFineRSæ˜¾è‘—ä¼˜äºç°æœ‰MLLMæ–¹æ³•ï¼Œåœ¨æŒ‡ä»¤å¼•å¯¼åˆ†å‰²å’Œè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå‡æœ‰æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨å„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè¾“å…¥åˆ†è¾¨ç‡çš„é™åˆ¶ï¼ŒMLLMåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­ç²¾ç¡®ç†è§£å’Œå®šä½è§†è§‰ç»†èŠ‚æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åµŒå…¥åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æå°ç›®æ ‡æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FineRSï¼Œä¸€ä¸ªåŸºäºMLLMçš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè”åˆæ¨ç†å’Œåˆ†å‰²é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­çš„æå°ç›®æ ‡ã€‚FineRSé‡‡ç”¨ç”±å…¨å±€è¯­ä¹‰æ¢ç´¢(GSE)å’Œå±€éƒ¨æ„ŸçŸ¥ç»†åŒ–(LPR)ç»„æˆçš„ç²—åˆ°ç²¾æµç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒGSEæ‰§è¡ŒæŒ‡ä»¤å¼•å¯¼çš„æ¨ç†ä»¥ç”Ÿæˆçº¹ç†å“åº”å’Œç²—ç•¥ç›®æ ‡åŒºåŸŸï¼Œè€ŒLPRç»†åŒ–è¯¥åŒºåŸŸä»¥ç”Ÿæˆç²¾ç¡®çš„è¾¹ç•Œæ¡†å’Œåˆ†å‰²æ©ç ã€‚ä¸ºäº†è€¦åˆè¿™ä¸¤ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å®šä½ä¿¡æ¯çš„å›é¡¾æ€§å¥–åŠ±ï¼Œå…¶ä¸­LPRçš„è¾“å‡ºç”¨äºä¼˜åŒ–GSEï¼Œä»¥å®ç°æ›´é²æ£’çš„ç²—ç•¥åŒºåŸŸæ¢ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†FineRS-4kï¼Œä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°MLLMåœ¨å¤æ‚é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­å¯¹ç»†å¾®ã€å°è§„æ¨¡ç›®æ ‡çš„å±æ€§çº§æ¨ç†å’Œåƒç´ çº§åˆ†å‰²èƒ½åŠ›ã€‚åœ¨FineRS-4kå’Œå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æŒ‡ä»¤å¼•å¯¼çš„åˆ†å‰²å’Œè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸºäºMLLMçš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­å¯¹æå°ç›®æ ‡è¿›è¡Œç²¾ç»†æ¨ç†å’Œåˆ†å‰²çš„éš¾é¢˜ã€‚ç°æœ‰MLLMæ–¹æ³•ç”±äºè¾“å…¥åˆ†è¾¨ç‡çš„é™åˆ¶ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰å’Œå®šä½è¿™äº›å°ç›®æ ‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚èƒŒæ™¯ä¸‹ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹å°ç›®æ ‡ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨å’Œç²¾ç»†åŒ–å¤„ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨ä¸€ç§ç²—åˆ°ç²¾çš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚é¦–å…ˆï¼Œé€šè¿‡å…¨å±€è¯­ä¹‰æ¢ç´¢ï¼ˆGSEï¼‰æ¨¡å—è¿›è¡Œç²—ç•¥çš„ç›®æ ‡å®šä½å’Œè¯­ä¹‰ç†è§£ï¼›ç„¶åï¼Œåˆ©ç”¨å±€éƒ¨æ„ŸçŸ¥ç»†åŒ–ï¼ˆLPRï¼‰æ¨¡å—å¯¹ç²—ç•¥åŒºåŸŸè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œç”Ÿæˆç²¾ç¡®çš„è¾¹ç•Œæ¡†å’Œåˆ†å‰²æ©ç ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–ä¸¤ä¸ªé˜¶æ®µçš„ååŒå·¥ä½œï¼Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFineRSæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šå…¨å±€è¯­ä¹‰æ¢ç´¢ï¼ˆGSEï¼‰å’Œå±€éƒ¨æ„ŸçŸ¥ç»†åŒ–ï¼ˆLPRï¼‰ã€‚GSEé˜¶æ®µåˆ©ç”¨æŒ‡ä»¤å¼•å¯¼çš„æ¨ç†ï¼Œç”Ÿæˆçº¹ç†å“åº”å’Œç²—ç•¥ç›®æ ‡åŒºåŸŸã€‚LPRé˜¶æ®µåˆ™å¯¹GSEè¾“å‡ºçš„åŒºåŸŸè¿›è¡Œç»†åŒ–ï¼Œç”Ÿæˆç²¾ç¡®çš„è¾¹ç•Œæ¡†å’Œåˆ†å‰²æ©ç ã€‚ä¸ºäº†è¿æ¥ä¸¤ä¸ªé˜¶æ®µï¼Œå¼•å…¥äº†å®šä½ä¿¡æ¯çš„å›é¡¾æ€§å¥–åŠ±ï¼ŒLPRçš„è¾“å‡ºè¢«ç”¨äºä¼˜åŒ–GSEï¼Œä»è€Œå®ç°æ›´é²æ£’çš„ç²—ç•¥åŒºåŸŸæ¢ç´¢ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»¥æœ€å¤§åŒ–æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼š1) æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„ç²—åˆ°ç²¾å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†MLLMåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­å°ç›®æ ‡åˆ†å‰²çš„éš¾é¢˜ã€‚2) å¼•å…¥äº†å®šä½ä¿¡æ¯çš„å›é¡¾æ€§å¥–åŠ±ï¼Œå®ç°äº†GSEå’ŒLPRä¸¤ä¸ªé˜¶æ®µçš„æœ‰æ•ˆè€¦åˆï¼Œæé«˜äº†æ•´ä½“æ€§èƒ½ã€‚3) æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†FineRS-4kï¼Œä¸“é—¨ç”¨äºè¯„ä¼°MLLMåœ¨å¤æ‚é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­å¯¹å°ç›®æ ‡çš„å±æ€§çº§æ¨ç†å’Œåƒç´ çº§åˆ†å‰²èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šGSEé˜¶æ®µé‡‡ç”¨MLLMè¿›è¡ŒæŒ‡ä»¤å¼•å¯¼çš„æ¨ç†ï¼Œç”Ÿæˆç²—ç•¥çš„ç›®æ ‡åŒºåŸŸã€‚LPRé˜¶æ®µåˆ™é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œç²¾ç»†åŒ–åˆ†å‰²ã€‚å®šä½ä¿¡æ¯çš„å›é¡¾æ€§å¥–åŠ±çš„è®¾è®¡æ˜¯å…³é”®ï¼Œå®ƒæ ¹æ®LPRçš„è¾“å‡ºï¼ˆè¾¹ç•Œæ¡†å’Œåˆ†å‰²æ©ç ï¼‰æ¥è°ƒæ•´GSEçš„ç­–ç•¥ï¼Œä½¿å¾—GSEèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å®šä½ç›®æ ‡åŒºåŸŸã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åˆ†å‰²æŸå¤±å’Œå®šä½æŸå¤±ï¼Œç”¨äºä¼˜åŒ–LPRçš„æ€§èƒ½ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©å’Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒFineRSåœ¨FineRS-4kæ•°æ®é›†å’Œå…¬å…±æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æŒ‡ä»¤å¼•å¯¼çš„åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒFineRSä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŸºäºMLLMçš„æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨FineRS-4kæ•°æ®é›†ä¸Šï¼ŒFineRSçš„åˆ†å‰²ç²¾åº¦æå‡äº†XX%ï¼Œæ¨ç†å‡†ç¡®ç‡æå‡äº†YY%ã€‚è¿™äº›ç»“æœéªŒè¯äº†FineRSæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºé¥æ„Ÿå›¾åƒåˆ†æã€åŒ»å­¦å›¾åƒè¯Šæ–­ã€å·¥ä¸šè´¨æ£€ç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ£€æµ‹å’Œåˆ†å‰²é«˜åˆ†è¾¨ç‡å›¾åƒä¸­çš„å¾®å°ç›®æ ‡æ—¶ï¼Œå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚ä¾‹å¦‚ï¼Œåœ¨é¥æ„Ÿå›¾åƒä¸­è¯†åˆ«å°å‹å»ºç­‘ç‰©æˆ–è½¦è¾†ï¼Œåœ¨åŒ»å­¦å›¾åƒä¸­æ£€æµ‹å¾®å°çš„ç—…ç¶ï¼Œåœ¨å·¥ä¸šè´¨æ£€ä¸­å‘ç°ç»†å¾®çš„ç¼ºé™·ã€‚è¯¥ç ”ç©¶æœ‰æœ›æå‡ç›¸å…³é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ°´å¹³å’Œç²¾åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities across a wide range of vision-language tasks. However, due to the restricted input resolutions, MLLMs face significant challenges in precisely understanding and localizing visual details in high-resolution images -- particularly when dealing with extra-small objects embedded in cluttered contexts. To address this issue, we propose \textsc{FineRS}, a two-stage MLLM-based reinforcement learning framework for jointly reasoning and segmenting extremely small objects within high-resolution scenes. \textsc{FineRS} adopts a coarse-to-fine pipeline comprising Global Semantic Exploration (GSE) and Localized Perceptual Refinement (LPR). Specifically, GSE performs instruction-guided reasoning to generate a textural response and a coarse target region, while LPR refines this region to produce an accurate bounding box and segmentation mask. To couple the two stages, we introduce a locate-informed retrospective reward, where LPR's outputs are used to optimize GSE for more robust coarse region exploration. % Additionally, we present \textsc{FineRS}-4k, a new dataset for evaluating MLLMs on attribute-level reasoning and pixel-level segmentation on subtle, small-scale targets in complex high-resolution scenes. Experimental results on \textsc{FineRS}-4k and public datasets demonstrate that our method consistently outperforms state-of-the-art MLLM-based approaches on both instruction-guided segmentation and visual reasoning tasks.

