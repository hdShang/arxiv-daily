---
layout: default
title: "ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models"
---

# ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21069" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21069v1</a>
  <a href="https://arxiv.org/pdf/2510.21069.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21069v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21069v1', 'ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pranav Saxena, Jimmy Chiun

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ZING-3Dï¼šåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å®ç°é›¶æ ·æœ¬å¢é‡å¼3Dåœºæ™¯å›¾æ„å»º**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `3Dåœºæ™¯å›¾` `è§†è§‰-è¯­è¨€æ¨¡å‹` `é›¶æ ·æœ¬å­¦ä¹ ` `å¢é‡å­¦ä¹ ` `å…·èº«æ™ºèƒ½` `å¼€æ”¾è¯æ±‡è¯†åˆ«` `å‡ ä½•å®šä½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dåœºæ™¯å›¾ç”Ÿæˆæ–¹æ³•ä¸»è¦å±€é™äºå•è§†è§’ï¼Œç¼ºä¹å¢é‡æ›´æ–°èƒ½åŠ›ï¼Œä¸”ç¼ºä¹æ˜ç¡®çš„3Då‡ ä½•å®šä½ï¼Œéš¾ä»¥åº”ç”¨äºå…·èº«æ™ºèƒ½ä»»åŠ¡ã€‚
2. ZING-3Dåˆ©ç”¨é¢„è®­ç»ƒVLMçš„çŸ¥è¯†ï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼è¿›è¡Œå¼€æ”¾è¯æ±‡è¯†åˆ«ï¼Œå¹¶æ„å»ºå…·æœ‰å‡ ä½•å®šä½å’Œå¢é‡æ›´æ–°èƒ½åŠ›çš„3Dåœºæ™¯å›¾ã€‚
3. åœ¨Replicaå’ŒHM3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒZING-3Dèƒ½å¤Ÿæœ‰æ•ˆæ•è·ç©ºé—´å’Œå…³ç³»çŸ¥è¯†ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºZING-3Dæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¼ºå¤§çŸ¥è¯†ï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼å®ç°å¼€æ”¾è¯æ±‡è¯†åˆ«ï¼Œå¹¶ç”Ÿæˆåœºæ™¯çš„ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºã€‚è¯¥æ¡†æ¶æ”¯æŒå¢é‡å¼æ›´æ–°å’Œ3Dç©ºé—´ä¸­çš„å‡ ä½•å®šä½ï¼Œé€‚ç”¨äºå…·èº«æ™ºèƒ½åœºæ™¯ã€‚ZING-3Dåˆ©ç”¨VLMæ¨ç†ç”Ÿæˆä¸°å¯Œçš„2Dåœºæ™¯å›¾ï¼Œå¹¶ä½¿ç”¨æ·±åº¦ä¿¡æ¯å°†å…¶å®šä½åˆ°3Dç©ºé—´ä¸­ã€‚èŠ‚ç‚¹è¡¨ç¤ºå…·æœ‰ç‰¹å¾ã€3Dä½ç½®å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡çš„å¼€æ”¾è¯æ±‡å¯¹è±¡ï¼Œè¾¹è¡¨ç¤ºå…·æœ‰å¯¹è±¡é—´è·ç¦»çš„ç©ºé—´å’Œè¯­ä¹‰å…³ç³»ã€‚åœ¨Replicaå’ŒHM3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒZING-3Dèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·ç©ºé—´å’Œå…³ç³»çŸ¥è¯†ï¼Œè€Œæ— éœ€ç‰¹å®šäºä»»åŠ¡çš„è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰3Dåœºæ™¯å›¾ç”Ÿæˆæ–¹æ³•åœ¨å…·èº«æ™ºèƒ½åœºæ™¯ä¸­å­˜åœ¨å±€é™æ€§ã€‚å®ƒä»¬é€šå¸¸ä¾èµ–äºå•è§†è§’æ•°æ®ï¼Œæ— æ³•éšç€æ–°è§‚æµ‹çš„åˆ°æ¥è¿›è¡Œå¢é‡æ›´æ–°ï¼Œå¹¶ä¸”ç¼ºä¹åœ¨3Dç©ºé—´ä¸­çš„æ˜ç¡®å‡ ä½•å®šä½ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨éœ€è¦æŒç»­æ„ŸçŸ¥å’Œäº¤äº’çš„æœºå™¨äººåº”ç”¨ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šZING-3Dçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¼ºå¤§è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œç»“åˆæ·±åº¦ä¿¡æ¯ï¼Œå°†2Dåœºæ™¯å›¾è½¬åŒ–ä¸ºå…·æœ‰å‡ ä½•ä¿¡æ¯çš„3Dåœºæ™¯å›¾ã€‚é€šè¿‡VLMçš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå®ç°å¼€æ”¾è¯æ±‡çš„åœºæ™¯ç†è§£ï¼Œå¹¶æ”¯æŒå¢é‡å¼æ›´æ–°ï¼Œä»è€Œé€‚åº”åŠ¨æ€ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šZING-3Dæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆ©ç”¨VLMå¯¹å•è§†è§’å›¾åƒè¿›è¡Œåˆ†æï¼Œç”Ÿæˆ2Dåœºæ™¯å›¾ï¼ŒèŠ‚ç‚¹åŒ…å«å¯¹è±¡ç±»åˆ«å’Œç‰¹å¾ï¼Œè¾¹åŒ…å«å¯¹è±¡é—´çš„è¯­ä¹‰å…³ç³»ï¼›2) åˆ©ç”¨æ·±åº¦ä¿¡æ¯å°†2Dåœºæ™¯å›¾ä¸­çš„å¯¹è±¡å®šä½åˆ°3Dç©ºé—´ä¸­ï¼Œä¸ºæ¯ä¸ªèŠ‚ç‚¹èµ‹äºˆ3Dåæ ‡ï¼›3) æ„å»º3Dåœºæ™¯å›¾ï¼ŒèŠ‚ç‚¹è¡¨ç¤º3Då¯¹è±¡ï¼Œè¾¹è¡¨ç¤ºå¯¹è±¡é—´çš„ç©ºé—´å’Œè¯­ä¹‰å…³ç³»ï¼ŒåŒ…æ‹¬è·ç¦»ç­‰å‡ ä½•ä¿¡æ¯ï¼›4) å½“æ–°çš„è§‚æµ‹æ•°æ®åˆ°æ¥æ—¶ï¼Œåˆ©ç”¨VLMå’Œæ·±åº¦ä¿¡æ¯å¯¹æ–°è§‚æµ‹è¿›è¡Œåˆ†æï¼Œå¹¶å°†æ–°å¯¹è±¡å’Œå…³ç³»å¢é‡å¼åœ°æ·»åŠ åˆ°ç°æœ‰çš„3Dåœºæ™¯å›¾ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šZING-3Dçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) åˆ©ç”¨VLMè¿›è¡Œé›¶æ ·æœ¬å¼€æ”¾è¯æ±‡è¯†åˆ«ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®ï¼›2) å°†2Dåœºæ™¯å›¾ä¸æ·±åº¦ä¿¡æ¯èåˆï¼Œå®ç°3Dåœºæ™¯å›¾çš„å‡ ä½•å®šä½ï¼›3) æ”¯æŒå¢é‡å¼æ›´æ–°ï¼Œèƒ½å¤Ÿé€‚åº”åŠ¨æ€å˜åŒ–çš„åœºæ™¯ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒZING-3Dæ›´å…·é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šZING-3Dçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„VLMæ¨¡å‹ï¼Œä¾‹å¦‚CLIPç­‰ï¼Œä»¥è·å¾—è‰¯å¥½çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼›2) è®¾è®¡æœ‰æ•ˆçš„æ·±åº¦ä¿¡æ¯èåˆç­–ç•¥ï¼Œå°†2Då¯¹è±¡å‡†ç¡®åœ°å®šä½åˆ°3Dç©ºé—´ä¸­ï¼›3) è®¾è®¡å¢é‡å¼æ›´æ–°æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—å’Œä¿¡æ¯å†—ä½™ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€é€‰çš„VLMæ¨¡å‹å’Œæ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ZING-3Dåœ¨Replicaå’ŒHM3Dæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·åœºæ™¯ä¸­çš„ç©ºé—´å’Œå…³ç³»çŸ¥è¯†ï¼Œè€Œæ— éœ€è¿›è¡Œç‰¹å®šäºä»»åŠ¡çš„è®­ç»ƒã€‚è™½ç„¶è®ºæ–‡ä¸­æ²¡æœ‰ç»™å‡ºå…·ä½“çš„æ€§èƒ½æŒ‡æ ‡ï¼Œä½†å¼ºè°ƒäº†ZING-3Dåœ¨é›¶æ ·æœ¬å­¦ä¹ å’Œå¢é‡å¼æ›´æ–°æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä½¿å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­å…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ZING-3Dåœ¨æœºå™¨äººå¯¼èˆªã€åœºæ™¯ç†è§£ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£å‘¨å›´ç¯å¢ƒï¼Œè¿›è¡Œè‡ªä¸»å¯¼èˆªå’Œç‰©ä½“äº¤äº’ã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼ŒZING-3Då¯ä»¥ç”¨äºæ„å»ºé€¼çœŸçš„3Dåœºæ™¯ï¼Œå¹¶æ”¯æŒç”¨æˆ·è¿›è¡Œäº¤äº’ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œæå‡ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding and reasoning about complex 3D environments requires structured scene representations that capture not only objects but also their semantic and spatial relationships. While recent works on 3D scene graph generation have leveraged pretrained VLMs without task-specific fine-tuning, they are largely confined to single-view settings, fail to support incremental updates as new observations arrive and lack explicit geometric grounding in 3D space, all of which are essential for embodied scenarios. In this paper, we propose, ZING-3D, a framework that leverages the vast knowledge of pretrained foundation models to enable open-vocabulary recognition and generate a rich semantic representation of the scene in a zero-shot manner while also enabling incremental updates and geometric grounding in 3D space, making it suitable for downstream robotics applications. Our approach leverages VLM reasoning to generate a rich 2D scene graph, which is grounded in 3D using depth information. Nodes represent open-vocabulary objects with features, 3D locations, and semantic context, while edges capture spatial and semantic relations with inter-object distances. Our experiments on scenes from the Replica and HM3D dataset show that ZING-3D is effective at capturing spatial and relational knowledge without the need of task-specific training.

