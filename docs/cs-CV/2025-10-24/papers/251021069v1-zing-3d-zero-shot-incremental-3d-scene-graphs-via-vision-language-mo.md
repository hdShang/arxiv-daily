---
layout: default
title: ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models
---

# ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21069" target="_blank" class="toolbar-btn">arXiv: 2510.21069v1</a>
    <a href="https://arxiv.org/pdf/2510.21069.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21069v1" 
            onclick="toggleFavorite(this, '2510.21069v1', 'ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Pranav Saxena, Jimmy Chiun

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ZING-3DÔºöÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂÆûÁé∞Èõ∂Ê†∑Êú¨Â¢ûÈáèÂºè3DÂú∫ÊôØÂõæÊûÑÂª∫**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `3DÂú∫ÊôØÂõæ` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `Â¢ûÈáèÂ≠¶‰π†` `ÂÖ∑Ë∫´Êô∫ËÉΩ` `ÂºÄÊîæËØçÊ±áËØÜÂà´` `Âá†‰ΩïÂÆö‰Ωç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ3DÂú∫ÊôØÂõæÁîüÊàêÊñπÊ≥ï‰∏ªË¶ÅÂ±ÄÈôê‰∫éÂçïËßÜËßíÔºåÁº∫‰πèÂ¢ûÈáèÊõ¥Êñ∞ËÉΩÂäõÔºå‰∏îÁº∫‰πèÊòéÁ°ÆÁöÑ3DÂá†‰ΩïÂÆö‰ΩçÔºåÈöæ‰ª•Â∫îÁî®‰∫éÂÖ∑Ë∫´Êô∫ËÉΩ‰ªªÂä°„ÄÇ
2. ZING-3DÂà©Áî®È¢ÑËÆ≠ÁªÉVLMÁöÑÁü•ËØÜÔºå‰ª•Èõ∂Ê†∑Êú¨ÊñπÂºèËøõË°åÂºÄÊîæËØçÊ±áËØÜÂà´ÔºåÂπ∂ÊûÑÂª∫ÂÖ∑ÊúâÂá†‰ΩïÂÆö‰ΩçÂíåÂ¢ûÈáèÊõ¥Êñ∞ËÉΩÂäõÁöÑ3DÂú∫ÊôØÂõæ„ÄÇ
3. Âú®ReplicaÂíåHM3DÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåZING-3DËÉΩÂ§üÊúâÊïàÊçïËé∑Á©∫Èó¥ÂíåÂÖ≥Á≥ªÁü•ËØÜÔºåÊó†ÈúÄÁâπÂÆö‰ªªÂä°ËÆ≠ÁªÉ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫ZING-3DÊ°ÜÊû∂ÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂº∫Â§ßÁü•ËØÜÔºå‰ª•Èõ∂Ê†∑Êú¨ÊñπÂºèÂÆûÁé∞ÂºÄÊîæËØçÊ±áËØÜÂà´ÔºåÂπ∂ÁîüÊàêÂú∫ÊôØÁöÑ‰∏∞ÂØåËØ≠‰πâË°®Á§∫„ÄÇËØ•Ê°ÜÊû∂ÊîØÊåÅÂ¢ûÈáèÂºèÊõ¥Êñ∞Âíå3DÁ©∫Èó¥‰∏≠ÁöÑÂá†‰ΩïÂÆö‰ΩçÔºåÈÄÇÁî®‰∫éÂÖ∑Ë∫´Êô∫ËÉΩÂú∫ÊôØ„ÄÇZING-3DÂà©Áî®VLMÊé®ÁêÜÁîüÊàê‰∏∞ÂØåÁöÑ2DÂú∫ÊôØÂõæÔºåÂπ∂‰ΩøÁî®Ê∑±Â∫¶‰ø°ÊÅØÂ∞ÜÂÖ∂ÂÆö‰ΩçÂà∞3DÁ©∫Èó¥‰∏≠„ÄÇËäÇÁÇπË°®Á§∫ÂÖ∑ÊúâÁâπÂæÅ„ÄÅ3D‰ΩçÁΩÆÂíåËØ≠‰πâ‰∏ä‰∏ãÊñáÁöÑÂºÄÊîæËØçÊ±áÂØπË±°ÔºåËæπË°®Á§∫ÂÖ∑ÊúâÂØπË±°Èó¥Ë∑ùÁ¶ªÁöÑÁ©∫Èó¥ÂíåËØ≠‰πâÂÖ≥Á≥ª„ÄÇÂú®ReplicaÂíåHM3DÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåZING-3DËÉΩÂ§üÊúâÊïàÂú∞ÊçïËé∑Á©∫Èó¥ÂíåÂÖ≥Á≥ªÁü•ËØÜÔºåËÄåÊó†ÈúÄÁâπÂÆö‰∫é‰ªªÂä°ÁöÑËÆ≠ÁªÉ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ3DÂú∫ÊôØÂõæÁîüÊàêÊñπÊ≥ïÂú®ÂÖ∑Ë∫´Êô∫ËÉΩÂú∫ÊôØ‰∏≠Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇÂÆÉ‰ª¨ÈÄöÂ∏∏‰æùËµñ‰∫éÂçïËßÜËßíÊï∞ÊçÆÔºåÊó†Ê≥ïÈöèÁùÄÊñ∞ËßÇÊµãÁöÑÂà∞Êù•ËøõË°åÂ¢ûÈáèÊõ¥Êñ∞ÔºåÂπ∂‰∏îÁº∫‰πèÂú®3DÁ©∫Èó¥‰∏≠ÁöÑÊòéÁ°ÆÂá†‰ΩïÂÆö‰Ωç„ÄÇËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®ÈúÄË¶ÅÊåÅÁª≠ÊÑüÁü•Âíå‰∫§‰∫íÁöÑÊú∫Âô®‰∫∫Â∫îÁî®‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöZING-3DÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑÂº∫Â§ßËØ≠‰πâÁêÜËß£ËÉΩÂäõÔºåÁªìÂêàÊ∑±Â∫¶‰ø°ÊÅØÔºåÂ∞Ü2DÂú∫ÊôØÂõæËΩ¨Âåñ‰∏∫ÂÖ∑ÊúâÂá†‰Ωï‰ø°ÊÅØÁöÑ3DÂú∫ÊôØÂõæ„ÄÇÈÄöËøáVLMÁöÑÈõ∂Ê†∑Êú¨Â≠¶‰π†ËÉΩÂäõÔºåÂÆûÁé∞ÂºÄÊîæËØçÊ±áÁöÑÂú∫ÊôØÁêÜËß£ÔºåÂπ∂ÊîØÊåÅÂ¢ûÈáèÂºèÊõ¥Êñ∞Ôºå‰ªéËÄåÈÄÇÂ∫îÂä®ÊÄÅÁéØÂ¢É„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöZING-3DÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) Âà©Áî®VLMÂØπÂçïËßÜËßíÂõæÂÉèËøõË°åÂàÜÊûêÔºåÁîüÊàê2DÂú∫ÊôØÂõæÔºåËäÇÁÇπÂåÖÂê´ÂØπË±°Á±ªÂà´ÂíåÁâπÂæÅÔºåËæπÂåÖÂê´ÂØπË±°Èó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ªÔºõ2) Âà©Áî®Ê∑±Â∫¶‰ø°ÊÅØÂ∞Ü2DÂú∫ÊôØÂõæ‰∏≠ÁöÑÂØπË±°ÂÆö‰ΩçÂà∞3DÁ©∫Èó¥‰∏≠Ôºå‰∏∫ÊØè‰∏™ËäÇÁÇπËµã‰∫à3DÂùêÊ†áÔºõ3) ÊûÑÂª∫3DÂú∫ÊôØÂõæÔºåËäÇÁÇπË°®Á§∫3DÂØπË±°ÔºåËæπË°®Á§∫ÂØπË±°Èó¥ÁöÑÁ©∫Èó¥ÂíåËØ≠‰πâÂÖ≥Á≥ªÔºåÂåÖÊã¨Ë∑ùÁ¶ªÁ≠âÂá†‰Ωï‰ø°ÊÅØÔºõ4) ÂΩìÊñ∞ÁöÑËßÇÊµãÊï∞ÊçÆÂà∞Êù•Êó∂ÔºåÂà©Áî®VLMÂíåÊ∑±Â∫¶‰ø°ÊÅØÂØπÊñ∞ËßÇÊµãËøõË°åÂàÜÊûêÔºåÂπ∂Â∞ÜÊñ∞ÂØπË±°ÂíåÂÖ≥Á≥ªÂ¢ûÈáèÂºèÂú∞Ê∑ªÂä†Âà∞Áé∞ÊúâÁöÑ3DÂú∫ÊôØÂõæ‰∏≠„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöZING-3DÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) Âà©Áî®VLMËøõË°åÈõ∂Ê†∑Êú¨ÂºÄÊîæËØçÊ±áËØÜÂà´ÔºåÊó†ÈúÄÁâπÂÆö‰ªªÂä°ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºõ2) Â∞Ü2DÂú∫ÊôØÂõæ‰∏éÊ∑±Â∫¶‰ø°ÊÅØËûçÂêàÔºåÂÆûÁé∞3DÂú∫ÊôØÂõæÁöÑÂá†‰ΩïÂÆö‰ΩçÔºõ3) ÊîØÊåÅÂ¢ûÈáèÂºèÊõ¥Êñ∞ÔºåËÉΩÂ§üÈÄÇÂ∫îÂä®ÊÄÅÂèòÂåñÁöÑÂú∫ÊôØ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåZING-3DÊõ¥ÂÖ∑ÈÄöÁî®ÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöZING-3DÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÈÄâÊã©ÂêàÈÄÇÁöÑVLMÊ®°ÂûãÔºå‰æãÂ¶ÇCLIPÁ≠âÔºå‰ª•Ëé∑ÂæóËâØÂ•ΩÁöÑËØ≠‰πâÁêÜËß£ËÉΩÂäõÔºõ2) ËÆæËÆ°ÊúâÊïàÁöÑÊ∑±Â∫¶‰ø°ÊÅØËûçÂêàÁ≠ñÁï•ÔºåÂ∞Ü2DÂØπË±°ÂáÜÁ°ÆÂú∞ÂÆö‰ΩçÂà∞3DÁ©∫Èó¥‰∏≠Ôºõ3) ËÆæËÆ°Â¢ûÈáèÂºèÊõ¥Êñ∞Êú∫Âà∂ÔºåÈÅøÂÖçÈáçÂ§çËÆ°ÁÆóÂíå‰ø°ÊÅØÂÜó‰Ωô„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÂèñÂÜ≥‰∫éÊâÄÈÄâÁöÑVLMÊ®°ÂûãÂíåÊ∑±Â∫¶‰º∞ËÆ°ÊñπÊ≥ï„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ZING-3DÂú®ReplicaÂíåHM3DÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ÊçïËé∑Âú∫ÊôØ‰∏≠ÁöÑÁ©∫Èó¥ÂíåÂÖ≥Á≥ªÁü•ËØÜÔºåËÄåÊó†ÈúÄËøõË°åÁâπÂÆö‰∫é‰ªªÂä°ÁöÑËÆ≠ÁªÉ„ÄÇËôΩÁÑ∂ËÆ∫Êñá‰∏≠Ê≤°ÊúâÁªôÂá∫ÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊåáÊ†áÔºå‰ΩÜÂº∫Ë∞É‰∫ÜZING-3DÂú®Èõ∂Ê†∑Êú¨Â≠¶‰π†ÂíåÂ¢ûÈáèÂºèÊõ¥Êñ∞ÊñπÈù¢ÁöÑ‰ºòÂäøÔºå‰ΩøÂÖ∂Âú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ZING-3DÂú®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅÂú∫ÊôØÁêÜËß£„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºåËøõË°åËá™‰∏ªÂØºËà™ÂíåÁâ©‰Ωì‰∫§‰∫í„ÄÇÂú®ËôöÊãüÁé∞ÂÆû‰∏≠ÔºåZING-3DÂèØ‰ª•Áî®‰∫éÊûÑÂª∫ÈÄºÁúüÁöÑ3DÂú∫ÊôØÔºåÂπ∂ÊîØÊåÅÁî®Êà∑ËøõË°å‰∫§‰∫í„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Â∫îÁî®‰∫éÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüüÔºåÊèêÂçáÁ≥ªÁªüÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Understanding and reasoning about complex 3D environments requires structured scene representations that capture not only objects but also their semantic and spatial relationships. While recent works on 3D scene graph generation have leveraged pretrained VLMs without task-specific fine-tuning, they are largely confined to single-view settings, fail to support incremental updates as new observations arrive and lack explicit geometric grounding in 3D space, all of which are essential for embodied scenarios. In this paper, we propose, ZING-3D, a framework that leverages the vast knowledge of pretrained foundation models to enable open-vocabulary recognition and generate a rich semantic representation of the scene in a zero-shot manner while also enabling incremental updates and geometric grounding in 3D space, making it suitable for downstream robotics applications. Our approach leverages VLM reasoning to generate a rich 2D scene graph, which is grounded in 3D using depth information. Nodes represent open-vocabulary objects with features, 3D locations, and semantic context, while edges capture spatial and semantic relations with inter-object distances. Our experiments on scenes from the Replica and HM3D dataset show that ZING-3D is effective at capturing spatial and relational knowledge without the need of task-specific training.

