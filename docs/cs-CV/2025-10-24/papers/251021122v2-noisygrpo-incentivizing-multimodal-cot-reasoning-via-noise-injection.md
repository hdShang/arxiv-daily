---
layout: default
title: NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation
---

# NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21122" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.21122v2</a>
  <a href="https://arxiv.org/pdf/2510.21122.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21122v2" onclick="toggleFavorite(this, '2510.21122v2', 'NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Longtian Qiu, Shan Ning, Jiaxuan Sun, Xuming He

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-24 (Êõ¥Êñ∞: 2025-10-29)

**Â§áÊ≥®**: Accepted by Neurips2025, Project page at at https://artanic30.github.io/project_pages/NoisyGRPO/

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://artanic30.github.io/project_pages/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**NoisyGRPOÔºöÈÄöËøáÂô™Â£∞Ê≥®ÂÖ•ÂíåË¥ùÂè∂ÊñØ‰º∞ËÆ°ÊøÄÂä±Â§öÊ®°ÊÄÅCoTÊé®ÁêÜ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `ÈìæÂºèÊÄùËÄÉ` `Âô™Â£∞Ê≥®ÂÖ•` `Ë¥ùÂè∂ÊñØ‰º∞ËÆ°` `ËßÜËßâÊé®ÁêÜ` `Â§ßËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Âú®ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÄöÁî®CoTÊé®ÁêÜËÉΩÂäõÊó∂ÔºåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈöæ‰ª•Ë∂ÖË∂äËÆ≠ÁªÉÂàÜÂ∏É„ÄÇ
2. NoisyGRPOÈÄöËøáÂêëËßÜËßâËæìÂÖ•Ê≥®ÂÖ•Âô™Â£∞Êù•ÈºìÂä±Êõ¥ÂπøÊ≥õÁöÑÊé¢Á¥¢ÔºåÂπ∂‰ΩøÁî®Ë¥ùÂè∂ÊñØÊ°ÜÊû∂Âª∫Ê®°‰ºòÂäø‰º∞ËÆ°ÔºåÊèêÂçáÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåNoisyGRPOÊòæËëóÊèêÈ´ò‰∫ÜÊ≥õÂåñÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∞èËßÑÊ®°MLLM‰∏äË°®Áé∞Á™ÅÂá∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫NoisyGRPOÁöÑÂ§öÊ®°ÊÄÅÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÈÄöÁî®Chain-of-Thought (CoT)Êé®ÁêÜËÉΩÂäõ„ÄÇÁé∞ÊúâÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Âú®ÊèêÂçáÈÄöÁî®CoTÊé®ÁêÜËÉΩÂäõÊó∂ÔºåÊ≥õÂåñËÉΩÂäõÂæÄÂæÄÂèóÈôê‰∫éËÆ≠ÁªÉÂàÜÂ∏É„ÄÇNoisyGRPOÈÄöËøáÂêëËßÜËßâËæìÂÖ•‰∏≠ÂºïÂÖ•ÂèØÊéßÂô™Â£∞Êù•Â¢ûÂº∫Êé¢Á¥¢ÔºåÂπ∂ÈááÁî®Ë¥ùÂè∂ÊñØÊ°ÜÊû∂ÊòæÂºèÂú∞Âª∫Ê®°‰ºòÂäø‰º∞ËÆ°ËøáÁ®ã„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåNoisyGRPOÈÄöËøáÂô™Â£∞Ê≥®ÂÖ•Êé¢Á¥¢Á≠ñÁï•Ôºà‰ΩøÁî®È´òÊñØÂô™Â£∞Êâ∞Âä®ËßÜËßâËæìÂÖ•ÔºåÈºìÂä±Êõ¥ÂπøÊ≥õÁöÑËßÜËßâÂú∫ÊôØÊé¢Á¥¢ÔºâÂíåË¥ùÂè∂ÊñØ‰ºòÂäø‰º∞ËÆ°ÔºàÂ∞Ü‰ºòÂäø‰º∞ËÆ°Âª∫Ê®°‰∏∫Ë¥ùÂè∂ÊñØÊé®ÁêÜÈóÆÈ¢òÔºåÂô™Â£∞Ê∞¥Âπ≥‰Ωú‰∏∫ÂÖàÈ™åÔºåËßÇÊµãÂà∞ÁöÑËΩ®ËøπÂ•ñÂä±‰Ωú‰∏∫‰ººÁÑ∂ÔºâÊù•ÊîπËøõÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇÂÆûÈ™åË°®ÊòéÔºåNoisyGRPOÊòæËëóÊèêÈ´ò‰∫ÜÊ≥õÂåñÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Qwen2.5-VL 3BÁ≠âÂ∞èËßÑÊ®°MLLMÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆæÁΩÆ‰∏≠„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑChain-of-Thought (CoT)Êé®ÁêÜÊñπÊ≥ïÔºåÂú®ÊèêÂçáÈÄöÁî®Êé®ÁêÜËÉΩÂäõÊó∂ÔºåÂ≠òÂú®Ê≥õÂåñÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÊ®°ÂûãÂÆπÊòìËøáÊãüÂêàËÆ≠ÁªÉÊï∞ÊçÆÔºåÈöæ‰ª•ÈÄÇÂ∫îÊñ∞ÁöÑ„ÄÅÊú™Áü•ÁöÑËßÜËßâÂú∫ÊôØ„ÄÇÁé∞ÊúâÊñπÊ≥ïÂØπÊé¢Á¥¢ÁöÑÈºìÂä±‰∏çË∂≥Ôºå‰ºòÂäø‰º∞ËÆ°‰∏çÂ§üÂáÜÁ°ÆÔºåÂØºËá¥Ê®°ÂûãÈöæ‰ª•Â≠¶‰π†Âà∞ÁúüÊ≠£ÊúâÊïàÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöNoisyGRPOÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•Âô™Â£∞Êù•Â¢ûÂº∫Ê®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõÔºåÂπ∂‰ΩøÁî®Ë¥ùÂè∂ÊñØÊñπÊ≥ïÊù•Êõ¥ÂáÜÁ°ÆÂú∞‰º∞ËÆ°‰ºòÂäøÂáΩÊï∞„ÄÇÈÄöËøáÂêëËßÜËßâËæìÂÖ•Ê∑ªÂä†Âô™Â£∞ÔºåÊ®°ÂûãÂèØ‰ª•Êé•Ëß¶Âà∞Êõ¥Â§öÊ†∑ÂåñÁöÑËßÜËßâÂú∫ÊôØÔºå‰ªéËÄåÊèêÈ´òÊ≥õÂåñËÉΩÂäõ„ÄÇË¥ùÂè∂ÊñØ‰ºòÂäø‰º∞ËÆ°ÂàôÂà©Áî®Âô™Â£∞Ê∞¥Âπ≥‰Ωú‰∏∫ÂÖàÈ™å‰ø°ÊÅØÔºåÁªìÂêàËßÇÊµãÂà∞ÁöÑÂ•ñÂä±ÔºåÂæóÂà∞Êõ¥È≤ÅÊ£íÁöÑ‰ºòÂäøÂáΩÊï∞‰º∞ËÆ°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöNoisyGRPOÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÊ®°ÂùóÔºöÂô™Â£∞Ê≥®ÂÖ•Êé¢Á¥¢Á≠ñÁï•ÂíåË¥ùÂè∂ÊñØ‰ºòÂäø‰º∞ËÆ°„ÄÇÈ¶ñÂÖàÔºåÂô™Â£∞Ê≥®ÂÖ•Êé¢Á¥¢Á≠ñÁï•ÈÄöËøáÂêëËßÜËßâËæìÂÖ•Ê∑ªÂä†È´òÊñØÂô™Â£∞Êù•Êâ∞Âä®ËæìÂÖ•ÔºåÈºìÂä±Ê®°ÂûãÊé¢Á¥¢Êõ¥ÂπøÊ≥õÁöÑËßÜËßâÂú∫ÊôØ„ÄÇÁÑ∂ÂêéÔºåË¥ùÂè∂ÊñØ‰ºòÂäø‰º∞ËÆ°Ê®°ÂùóÂ∞Ü‰ºòÂäø‰º∞ËÆ°Âª∫Ê®°‰∏∫‰∏Ä‰∏™Ë¥ùÂè∂ÊñØÊé®ÁêÜÈóÆÈ¢òÔºåÂÖ∂‰∏≠Âô™Â£∞Ê∞¥Âπ≥‰Ωú‰∏∫ÂÖàÈ™åÔºåËßÇÊµãÂà∞ÁöÑËΩ®ËøπÂ•ñÂä±‰Ωú‰∏∫‰ººÁÑ∂„ÄÇÈÄöËøáË¥ùÂè∂ÊñØÊé®ÁêÜÔºåÂèØ‰ª•ÂæóÂà∞‰∏Ä‰∏™Êõ¥ÂáÜÁ°ÆÁöÑ‰ºòÂäøÂáΩÊï∞‰º∞ËÆ°ÔºåÁî®‰∫éÊåáÂØºÊ®°ÂûãÁöÑÁ≠ñÁï•Â≠¶‰π†„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöNoisyGRPOÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂô™Â£∞Ê≥®ÂÖ•ÂíåË¥ùÂè∂ÊñØ‰º∞ËÆ°Áõ∏ÁªìÂêàÔºåÁî®‰∫éÊèêÂçáÂ§öÊ®°ÊÄÅCoTÊé®ÁêÜÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåNoisyGRPOÊòæÂºèÂú∞Âª∫Ê®°‰∫ÜÂô™Â£∞ÂØπ‰ºòÂäøÂáΩÊï∞ÁöÑÂΩ±ÂìçÔºåÂπ∂Âà©Áî®Ë¥ùÂè∂ÊñØÊñπÊ≥ïËøõË°åÊé®ÁêÜÔºå‰ªéËÄåÂæóÂà∞Êõ¥È≤ÅÊ£íÁöÑ‰ºòÂäøÂáΩÊï∞‰º∞ËÆ°„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂú∞ÊèêÈ´òÊ®°ÂûãÂú®Êú™Áü•ËßÜËßâÂú∫ÊôØ‰∏ãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Âô™Â£∞Ê≥®ÂÖ•Êé¢Á¥¢Á≠ñÁï•‰∏≠ÔºåÈ´òÊñØÂô™Â£∞ÁöÑÊñπÂ∑ÆÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑË∂ÖÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇÂú®Ë¥ùÂè∂ÊñØ‰ºòÂäø‰º∞ËÆ°‰∏≠ÔºåÂÖàÈ™åÂàÜÂ∏ÉÁöÑÈÄâÊã©‰πü‰ºöÂΩ±ÂìçÊúÄÁªàÁöÑ‰º∞ËÆ°ÁªìÊûú„ÄÇËÆ∫Êñá‰∏≠ÂèØËÉΩ‰ΩøÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÊ®°ÂûãÁöÑÁ≠ñÁï•Ôºå‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®Á≠ñÁï•Ê¢ØÂ∫¶ÁÆóÊ≥ïÊù•Êõ¥Êñ∞Ê®°ÂûãÁöÑÂèÇÊï∞„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÁªÜËäÇÔºà‰æãÂ¶ÇÔºåMLLMÁöÑÂÖ∑‰ΩìÊû∂ÊûÑÔºâÂèØËÉΩÂõ†‰∏çÂêåÁöÑÂÆûÈ™åËÆæÁΩÆËÄåÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

NoisyGRPOÂú®Ê†áÂáÜCoTË¥®Èáè„ÄÅÈÄöÁî®ËÉΩÂäõÂíåÂπªËßâÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Ê≥õÂåñÊÄßÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ÁöÑÊòæËëóÊèêÂçá„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Â∞èËßÑÊ®°MLLMÔºàÂ¶ÇQwen2.5-VL 3BÔºâ‰∏äÔºåNoisyGRPOÁöÑ‰ºòÂäøÊõ¥‰∏∫ÊòéÊòæ„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶ÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•ÊâæÔºå‰ΩÜÊÄª‰ΩìËÄåË®ÄÔºåNoisyGRPO‰∏∫Â§öÊ®°ÊÄÅCoTÊé®ÁêÜÊèê‰æõ‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

NoisyGRPOÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÊèêÂçáÂêÑÁßçÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰æãÂ¶ÇËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÊèêÈ´òÊ®°ÂûãÂú®Â§çÊùÇ„ÄÅÊú™Áü•ÁöÑÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ§ÑÁêÜÁúüÂÆû‰∏ñÁïåÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåNoisyGRPOÁöÑÊÄùË∑Ø‰πüÂèØ‰ª•Êé®ÂπøÂà∞ÂÖ∂‰ªñÂº∫ÂåñÂ≠¶‰π†‰ªªÂä°‰∏≠ÔºåÁî®‰∫éÊèêÈ´òÊ®°ÂûãÁöÑÊé¢Á¥¢ËÉΩÂäõÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) has shown promise in enhancing the general Chain-of-Thought (CoT) reasoning capabilities of multimodal large language models (MLLMs). However, when applied to improve general CoT reasoning, existing RL frameworks often struggle to generalize beyond the training distribution. To address this, we propose NoisyGRPO, a systematic multimodal RL framework that introduces controllable noise into visual inputs for enhanced exploration and explicitly models the advantage estimation process via a Bayesian framework. Specifically, NoisyGRPO improves RL training by: (1) Noise-Injected Exploration Policy: Perturbing visual inputs with Gaussian noise to encourage exploration across a wider range of visual scenarios; and (2) Bayesian Advantage Estimation: Formulating advantage estimation as a principled Bayesian inference problem, where the injected noise level serves as a prior and the observed trajectory reward as the likelihood. This Bayesian modeling fuses both sources of information to compute a robust posterior estimate of trajectory advantage, effectively guiding MLLMs to prefer visually grounded trajectories over noisy ones. Experiments on standard CoT quality, general capability, and hallucination benchmarks demonstrate that NoisyGRPO substantially improves generalization and robustness, especially in RL settings with small-scale MLLMs such as Qwen2.5-VL 3B. The project page is available at https://artanic30.github.io/project_pages/NoisyGRPO/.

