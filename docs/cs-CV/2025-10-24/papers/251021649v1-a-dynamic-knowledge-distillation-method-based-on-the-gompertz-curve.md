---
layout: default
title: A Dynamic Knowledge Distillation Method Based on the Gompertz Curve
---

# A Dynamic Knowledge Distillation Method Based on the Gompertz Curve

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21649" target="_blank" class="toolbar-btn">arXiv: 2510.21649v1</a>
    <a href="https://arxiv.org/pdf/2510.21649.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21649v1" 
            onclick="toggleFavorite(this, '2510.21649v1', 'A Dynamic Knowledge Distillation Method Based on the Gompertz Curve')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Han Yang, Guangjun Qin

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-24

**Â§áÊ≥®**: 15 pages, 2 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Gompertz-CNNÔºåÂà©Áî®GompertzÊõ≤Á∫øÂä®ÊÄÅË∞ÉÊï¥Áü•ËØÜËí∏È¶èÔºåÊèêÂçáÂ≠¶ÁîüÊ®°ÂûãÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `GompertzÊõ≤Á∫ø` `Âä®ÊÄÅÊùÉÈáçË∞ÉÊï¥` `Ê®°ÂûãÂéãÁº©` `Ê∑±Â∫¶Â≠¶‰π†` `WassersteinË∑ùÁ¶ª` `Ê¢ØÂ∫¶ÂåπÈÖç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ïÂøΩÁï•‰∫ÜÂ≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†ËÉΩÂäõÈöèËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂèòÂåñÔºåÂØºËá¥Áü•ËØÜ‰º†ÈÄíÊïàÁéáÈôç‰Ωé„ÄÇ
2. Gompertz-CNNÂà©Áî®GompertzÊõ≤Á∫øÂä®ÊÄÅË∞ÉÊï¥Ëí∏È¶èÊçüÂ§±ÊùÉÈáçÔºå‰ΩøÁü•ËØÜ‰º†ÈÄí‰∏éÂ≠¶ÁîüÊ®°ÂûãÁöÑÂ≠¶‰π†Èò∂ÊÆµÁõ∏ÂåπÈÖç„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGompertz-CNNÂú®CIFAR-10ÂíåCIFAR-100Êï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂáÜÁ°ÆÁéáÂàÜÂà´ÊèêÂçáÈ´òËææ8%Âíå4%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂä®ÊÄÅÁü•ËØÜËí∏È¶èÊ°ÜÊû∂Gompertz-CNNÔºåËØ•Ê°ÜÊû∂Â∞ÜGompertzÂ¢ûÈïøÊ®°ÂûãËûçÂÖ•ËÆ≠ÁªÉËøáÁ®ãÔºå‰ª•Ëß£ÂÜ≥‰º†ÁªüÁü•ËØÜËí∏È¶èÁöÑÂ±ÄÈôêÊÄß„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöÂ∏∏Êó†Ê≥ïÊçïÊçâÂ≠¶ÁîüÊ®°Âûã‰∏çÊñ≠ÂèëÂ±ïÁöÑËÆ§Áü•ËÉΩÂäõÔºåÂØºËá¥Ê¨°‰ºòÁöÑÁü•ËØÜËΩ¨Áßª„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∏ÄÁÇπÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈò∂ÊÆµÊÑüÁü•ÁöÑËí∏È¶èÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•Âü∫‰∫éGompertzÊõ≤Á∫øÂä®ÊÄÅË∞ÉÊï¥Ëí∏È¶èÊçüÂ§±ÁöÑÊùÉÈáçÔºåÂèçÊò†‰∫ÜÂ≠¶ÁîüÁöÑÂ≠¶‰π†ËøáÁ®ãÔºöÂàùÂßãÈò∂ÊÆµÁºìÊÖ¢Â¢ûÈïøÔºå‰∏≠ÊúüÂø´ÈÄüÊîπËøõÔºå‰ª•ÂèäÂêéÊúüÈ•±Âíå„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÁªìÂêà‰∫ÜWassersteinË∑ùÁ¶ªÊù•Ë°°ÈáèÁâπÂæÅÂ±ÇÈù¢ÁöÑÂ∑ÆÂºÇÔºåÂπ∂ÁªìÂêàÊ¢ØÂ∫¶ÂåπÈÖçÊù•ÂØπÈΩêÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°Âûã‰πãÈó¥ÁöÑÂèçÂêë‰º†Êí≠Ë°å‰∏∫„ÄÇËøô‰∫õÁªÑ‰ª∂Áªü‰∏ÄÂú®‰∏Ä‰∏™Â§öÊçüÂ§±ÁõÆÊ†á‰∏ãÔºåÂÖ∂‰∏≠GompertzÊõ≤Á∫øË∞ÉËäÇËí∏È¶èÊçüÂ§±ÈöèÊó∂Èó¥ÁöÑÂΩ±Âìç„ÄÇÂú®CIFAR-10ÂíåCIFAR-100‰∏ä‰ΩøÁî®ÂêÑÁßçÊïôÂ∏à-Â≠¶ÁîüÊû∂ÊûÑÔºà‰æãÂ¶ÇÔºåResNet50ÂíåMobileNet_v2ÔºâËøõË°åÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåGompertz-CNNÂßãÁªà‰ºò‰∫é‰º†ÁªüÁöÑËí∏È¶èÊñπÊ≥ïÔºåÂú®CIFAR-10ÂíåCIFAR-100‰∏äÂàÜÂà´ÂÆûÁé∞‰∫ÜÈ´òËææ8%Âíå4%ÁöÑÂáÜÁ°ÆÁéáÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**Ôºö‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®Âõ∫ÂÆöÁöÑÊçüÂ§±ÊùÉÈáçÔºåÊó†Ê≥ïÈÄÇÂ∫îÂ≠¶ÁîüÊ®°ÂûãÂú®‰∏çÂêåËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂ≠¶‰π†ËÉΩÂäõÂèòÂåñ„ÄÇÂ≠¶ÁîüÊ®°ÂûãÂú®ËÆ≠ÁªÉÂàùÊúüÂ≠¶‰π†ËÉΩÂäõËæÉÂº±ÔºåÂêéÊúüÈÄêÊ∏êÈ•±ÂíåÔºåÂõ∫ÂÆöÁöÑËí∏È¶èÊçüÂ§±ÊùÉÈáçÂèØËÉΩÂØºËá¥Ê¨†ÊãüÂêàÊàñËøáÊãüÂêàÔºåÂΩ±ÂìçÁü•ËØÜ‰º†ÈÄíÁöÑÊïàÁéáÂíåÊúÄÁªàÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®GompertzÂ¢ûÈïøÊ®°ÂûãÊù•Ê®°ÊãüÂ≠¶ÁîüÊ®°ÂûãÁöÑÂ≠¶‰π†ËøáÁ®ãÔºåÂπ∂Âü∫‰∫éGompertzÊõ≤Á∫øÂä®ÊÄÅË∞ÉÊï¥Ëí∏È¶èÊçüÂ§±ÁöÑÊùÉÈáç„ÄÇGompertzÊõ≤Á∫øËÉΩÂ§üÂæàÂ•ΩÂú∞ÊèèËø∞Â≠¶‰π†ËøáÁ®ã‰∏≠ÁöÑSÂûãÂ¢ûÈïøÊ®°ÂºèÔºåÂç≥ÂàùÂßãÈò∂ÊÆµÁºìÊÖ¢Â¢ûÈïøÔºå‰∏≠ÊúüÂø´ÈÄüÂ¢ûÈïøÔºåÂêéÊúüÈÄêÊ∏êÈ•±Âíå„ÄÇÈÄöËøáÂ∞ÜËí∏È¶èÊçüÂ§±ÁöÑÊùÉÈáç‰∏éGompertzÊõ≤Á∫øÁõ∏ÁªìÂêàÔºåÂèØ‰ª•‰ΩøÁü•ËØÜ‰º†ÈÄíÊõ¥Âä†Á¨¶ÂêàÂ≠¶ÁîüÊ®°ÂûãÁöÑÂ≠¶‰π†ËßÑÂæãÔºå‰ªéËÄåÊèêÈ´òÁü•ËØÜ‰º†ÈÄíÁöÑÊïàÁéáÂíåÊúÄÁªàÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGompertz-CNNÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) ÊïôÂ∏àÊ®°ÂûãÂíåÂ≠¶ÁîüÊ®°ÂûãÔºõ2) Âü∫‰∫éGompertzÊõ≤Á∫øÁöÑÂä®ÊÄÅÊùÉÈáçË∞ÉÊï¥Ê®°ÂùóÔºõ3) ÁâπÂæÅÂ±ÇÈù¢ÁöÑWassersteinË∑ùÁ¶ªËÆ°ÁÆóÊ®°ÂùóÔºõ4) Ê¢ØÂ∫¶ÂåπÈÖçÊ®°ÂùóÔºõ5) Â§öÊçüÂ§±ÁõÆÊ†áÂáΩÊï∞„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÈ¶ñÂÖàËÆ°ÁÆóÊïôÂ∏àÊ®°ÂûãÂíåÂ≠¶ÁîüÊ®°ÂûãÁöÑËæìÂá∫ÁâπÂæÅÔºåÁÑ∂Âêé‰ΩøÁî®WassersteinË∑ùÁ¶ªË°°ÈáèÁâπÂæÅÂ∑ÆÂºÇÔºåÂπ∂ËøõË°åÊ¢ØÂ∫¶ÂåπÈÖç„ÄÇGompertzÊõ≤Á∫øÊ†πÊçÆËÆ≠ÁªÉepochÂä®ÊÄÅË∞ÉÊï¥Ëí∏È¶èÊçüÂ§±ÁöÑÊùÉÈáçÔºåÊúÄÂêéÈÄöËøáÂ§öÊçüÂ§±ÁõÆÊ†áÂáΩÊï∞‰ºòÂåñÂ≠¶ÁîüÊ®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜGompertzÂ¢ûÈïøÊ®°ÂûãÂºïÂÖ•Áü•ËØÜËí∏È¶èÊ°ÜÊû∂ÔºåÂπ∂Âà©Áî®GompertzÊõ≤Á∫øÂä®ÊÄÅË∞ÉÊï¥Ëí∏È¶èÊçüÂ§±ÁöÑÊùÉÈáç„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåGompertz-CNNËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ≠¶ÁîüÊ®°ÂûãÁöÑÂ≠¶‰π†ËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÁü•ËØÜ‰º†ÈÄíÁöÑÊïàÁéáÂíåÊúÄÁªàÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÁªìÂêàWassersteinË∑ùÁ¶ªÂíåÊ¢ØÂ∫¶ÂåπÈÖçËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÁü•ËØÜ‰º†ÈÄíÁöÑË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöGompertzÊõ≤Á∫øÁöÑÂèÇÊï∞ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥Ôºå‰æãÂ¶ÇÊõ≤Á∫øÁöÑÂ¢ûÈïøÈÄüÁéáÂíåÈ•±ÂíåÂÄº„ÄÇWassersteinË∑ùÁ¶ªÁî®‰∫éË°°ÈáèÊïôÂ∏àÊ®°ÂûãÂíåÂ≠¶ÁîüÊ®°ÂûãÂú®ÁâπÂæÅÂ±ÇÈù¢ÁöÑÂ∑ÆÂºÇÔºåÂèØ‰ª•ÈááÁî®‰∏çÂêåÁöÑË∑ùÁ¶ªÂ∫¶ÈáèÊñπÂºè„ÄÇÊ¢ØÂ∫¶ÂåπÈÖçÈÄöËøáÊúÄÂ∞èÂåñÊïôÂ∏àÊ®°ÂûãÂíåÂ≠¶ÁîüÊ®°ÂûãÊ¢ØÂ∫¶‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊù•ÂÆûÁé∞ÔºåÂèØ‰ª•ÈááÁî®‰∏çÂêåÁöÑÊ¢ØÂ∫¶ÂØπÈΩêÁ≠ñÁï•„ÄÇÂ§öÊçüÂ§±ÁõÆÊ†áÂáΩÊï∞Â∞ÜÂàÜÁ±ªÊçüÂ§±„ÄÅËí∏È¶èÊçüÂ§±„ÄÅWassersteinË∑ùÁ¶ªÊçüÂ§±ÂíåÊ¢ØÂ∫¶ÂåπÈÖçÊçüÂ§±ËøõË°åÂä†ÊùÉÁªÑÂêàÔºåÈúÄË¶Å‰ªîÁªÜË∞ÉÊï¥ÂêÑ‰∏™ÊçüÂ§±ÁöÑÊùÉÈáç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGompertz-CNNÂú®CIFAR-10ÂíåCIFAR-100Êï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®CIFAR-10‰∏äÔºåGompertz-CNNÁõ∏ÊØî‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ïÔºåÂáÜÁ°ÆÁéáÊèêÂçáÈ´òËææ8%„ÄÇÂú®CIFAR-100‰∏äÔºåÂáÜÁ°ÆÁéáÊèêÂçáÈ´òËææ4%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåGompertz-CNNËÉΩÂ§üÊúâÊïàÊèêÈ´òÂ≠¶ÁîüÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂπ∂‰ºò‰∫é‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Gompertz-CNNÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊ®°ÂûãÂéãÁº©ÂíåÂä†ÈÄüÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§á‰∏äÁöÑÂõæÂÉèËØÜÂà´„ÄÅËá™Âä®È©æÈ©∂‰∏≠ÁöÑÁõÆÊ†áÊ£ÄÊµã„ÄÅ‰ª•ÂèäËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏ãÁöÑÊ®°ÂûãÈÉ®ÁΩ≤„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÊèêÂçáÂ≠¶ÁîüÊ®°ÂûãÁöÑÊÄßËÉΩÔºå‰ΩøÂÖ∂Âú®‰øùÊåÅËæÉ‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÁöÑÂêåÊó∂ÔºåËææÂà∞Êé•ËøëÊïôÂ∏àÊ®°ÂûãÁöÑÁ≤æÂ∫¶„ÄÇÊú™Êù•ÂèØËøõ‰∏ÄÊ≠•Êé¢Á¥¢ÂÖ∂Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper introduces a novel dynamic knowledge distillation framework, Gompertz-CNN, which integrates the Gompertz growth model into the training process to address the limitations of traditional knowledge distillation. Conventional methods often fail to capture the evolving cognitive capacity of student models, leading to suboptimal knowledge transfer. To overcome this, we propose a stage-aware distillation strategy that dynamically adjusts the weight of distillation loss based on the Gompertz curve, reflecting the student's learning progression: slow initial growth, rapid mid-phase improvement, and late-stage saturation. Our framework incorporates Wasserstein distance to measure feature-level discrepancies and gradient matching to align backward propagation behaviors between teacher and student models. These components are unified under a multi-loss objective, where the Gompertz curve modulates the influence of distillation losses over time. Extensive experiments on CIFAR-10 and CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and MobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms traditional distillation methods, achieving up to 8% and 4% accuracy gains on CIFAR-10 and CIFAR-100, respectively.

