---
layout: default
title: A Dynamic Knowledge Distillation Method Based on the Gompertz Curve
---

# A Dynamic Knowledge Distillation Method Based on the Gompertz Curve

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21649" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21649v1</a>
  <a href="https://arxiv.org/pdf/2510.21649.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21649v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21649v1', 'A Dynamic Knowledge Distillation Method Based on the Gompertz Curve')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Han Yang, Guangjun Qin

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

**å¤‡æ³¨**: 15 pages, 2 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGompertz-CNNï¼Œåˆ©ç”¨Gompertzæ›²çº¿åŠ¨æ€è°ƒæ•´çŸ¥è¯†è’¸é¦ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `Gompertzæ›²çº¿` `åŠ¨æ€æƒé‡è°ƒæ•´` `æ¨¡å‹å‹ç¼©` `æ·±åº¦å­¦ä¹ ` `Wassersteinè·ç¦»` `æ¢¯åº¦åŒ¹é…`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•å¿½ç•¥äº†å­¦ç”Ÿæ¨¡å‹å­¦ä¹ èƒ½åŠ›éšè®­ç»ƒé˜¶æ®µçš„å˜åŒ–ï¼Œå¯¼è‡´çŸ¥è¯†ä¼ é€’æ•ˆç‡é™ä½ã€‚
2. Gompertz-CNNåˆ©ç”¨Gompertzæ›²çº¿åŠ¨æ€è°ƒæ•´è’¸é¦æŸå¤±æƒé‡ï¼Œä½¿çŸ¥è¯†ä¼ é€’ä¸å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ é˜¶æ®µç›¸åŒ¹é…ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGompertz-CNNåœ¨CIFAR-10å’ŒCIFAR-100æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æå‡é«˜è¾¾8%å’Œ4%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€çŸ¥è¯†è’¸é¦æ¡†æ¶Gompertz-CNNï¼Œè¯¥æ¡†æ¶å°†Gompertzå¢é•¿æ¨¡å‹èå…¥è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥è§£å†³ä¼ ç»ŸçŸ¥è¯†è’¸é¦çš„å±€é™æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸æ— æ³•æ•æ‰å­¦ç”Ÿæ¨¡å‹ä¸æ–­å‘å±•çš„è®¤çŸ¥èƒ½åŠ›ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„çŸ¥è¯†è½¬ç§»ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é˜¶æ®µæ„ŸçŸ¥çš„è’¸é¦ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºGompertzæ›²çº¿åŠ¨æ€è°ƒæ•´è’¸é¦æŸå¤±çš„æƒé‡ï¼Œåæ˜ äº†å­¦ç”Ÿçš„å­¦ä¹ è¿‡ç¨‹ï¼šåˆå§‹é˜¶æ®µç¼“æ…¢å¢é•¿ï¼Œä¸­æœŸå¿«é€Ÿæ”¹è¿›ï¼Œä»¥åŠåæœŸé¥±å’Œã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†Wassersteinè·ç¦»æ¥è¡¡é‡ç‰¹å¾å±‚é¢çš„å·®å¼‚ï¼Œå¹¶ç»“åˆæ¢¯åº¦åŒ¹é…æ¥å¯¹é½æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„åå‘ä¼ æ’­è¡Œä¸ºã€‚è¿™äº›ç»„ä»¶ç»Ÿä¸€åœ¨ä¸€ä¸ªå¤šæŸå¤±ç›®æ ‡ä¸‹ï¼Œå…¶ä¸­Gompertzæ›²çº¿è°ƒèŠ‚è’¸é¦æŸå¤±éšæ—¶é—´çš„å½±å“ã€‚åœ¨CIFAR-10å’ŒCIFAR-100ä¸Šä½¿ç”¨å„ç§æ•™å¸ˆ-å­¦ç”Ÿæ¶æ„ï¼ˆä¾‹å¦‚ï¼ŒResNet50å’ŒMobileNet_v2ï¼‰è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGompertz-CNNå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„è’¸é¦æ–¹æ³•ï¼Œåœ¨CIFAR-10å’ŒCIFAR-100ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾8%å’Œ4%çš„å‡†ç¡®ç‡æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„æŸå¤±æƒé‡ï¼Œæ— æ³•é€‚åº”å­¦ç”Ÿæ¨¡å‹åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µçš„å­¦ä¹ èƒ½åŠ›å˜åŒ–ã€‚å­¦ç”Ÿæ¨¡å‹åœ¨è®­ç»ƒåˆæœŸå­¦ä¹ èƒ½åŠ›è¾ƒå¼±ï¼ŒåæœŸé€æ¸é¥±å’Œï¼Œå›ºå®šçš„è’¸é¦æŸå¤±æƒé‡å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆæˆ–è¿‡æ‹Ÿåˆï¼Œå½±å“çŸ¥è¯†ä¼ é€’çš„æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Gompertzå¢é•¿æ¨¡å‹æ¥æ¨¡æ‹Ÿå­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶åŸºäºGompertzæ›²çº¿åŠ¨æ€è°ƒæ•´è’¸é¦æŸå¤±çš„æƒé‡ã€‚Gompertzæ›²çº¿èƒ½å¤Ÿå¾ˆå¥½åœ°æè¿°å­¦ä¹ è¿‡ç¨‹ä¸­çš„Så‹å¢é•¿æ¨¡å¼ï¼Œå³åˆå§‹é˜¶æ®µç¼“æ…¢å¢é•¿ï¼Œä¸­æœŸå¿«é€Ÿå¢é•¿ï¼ŒåæœŸé€æ¸é¥±å’Œã€‚é€šè¿‡å°†è’¸é¦æŸå¤±çš„æƒé‡ä¸Gompertzæ›²çº¿ç›¸ç»“åˆï¼Œå¯ä»¥ä½¿çŸ¥è¯†ä¼ é€’æ›´åŠ ç¬¦åˆå­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ è§„å¾‹ï¼Œä»è€Œæé«˜çŸ¥è¯†ä¼ é€’çš„æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGompertz-CNNæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ï¼›2) åŸºäºGompertzæ›²çº¿çš„åŠ¨æ€æƒé‡è°ƒæ•´æ¨¡å—ï¼›3) ç‰¹å¾å±‚é¢çš„Wassersteinè·ç¦»è®¡ç®—æ¨¡å—ï¼›4) æ¢¯åº¦åŒ¹é…æ¨¡å—ï¼›5) å¤šæŸå¤±ç›®æ ‡å‡½æ•°ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆè®¡ç®—æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºç‰¹å¾ï¼Œç„¶åä½¿ç”¨Wassersteinè·ç¦»è¡¡é‡ç‰¹å¾å·®å¼‚ï¼Œå¹¶è¿›è¡Œæ¢¯åº¦åŒ¹é…ã€‚Gompertzæ›²çº¿æ ¹æ®è®­ç»ƒepochåŠ¨æ€è°ƒæ•´è’¸é¦æŸå¤±çš„æƒé‡ï¼Œæœ€åé€šè¿‡å¤šæŸå¤±ç›®æ ‡å‡½æ•°ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†Gompertzå¢é•¿æ¨¡å‹å¼•å…¥çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨Gompertzæ›²çº¿åŠ¨æ€è°ƒæ•´è’¸é¦æŸå¤±çš„æƒé‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒGompertz-CNNèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œæé«˜çŸ¥è¯†ä¼ é€’çš„æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œç»“åˆWassersteinè·ç¦»å’Œæ¢¯åº¦åŒ¹é…è¿›ä¸€æ­¥æå‡äº†çŸ¥è¯†ä¼ é€’çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šGompertzæ›²çº¿çš„å‚æ•°éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚æ›²çº¿çš„å¢é•¿é€Ÿç‡å’Œé¥±å’Œå€¼ã€‚Wassersteinè·ç¦»ç”¨äºè¡¡é‡æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹åœ¨ç‰¹å¾å±‚é¢çš„å·®å¼‚ï¼Œå¯ä»¥é‡‡ç”¨ä¸åŒçš„è·ç¦»åº¦é‡æ–¹å¼ã€‚æ¢¯åº¦åŒ¹é…é€šè¿‡æœ€å°åŒ–æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹æ¢¯åº¦ä¹‹é—´çš„å·®å¼‚æ¥å®ç°ï¼Œå¯ä»¥é‡‡ç”¨ä¸åŒçš„æ¢¯åº¦å¯¹é½ç­–ç•¥ã€‚å¤šæŸå¤±ç›®æ ‡å‡½æ•°å°†åˆ†ç±»æŸå¤±ã€è’¸é¦æŸå¤±ã€Wassersteinè·ç¦»æŸå¤±å’Œæ¢¯åº¦åŒ¹é…æŸå¤±è¿›è¡ŒåŠ æƒç»„åˆï¼Œéœ€è¦ä»”ç»†è°ƒæ•´å„ä¸ªæŸå¤±çš„æƒé‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGompertz-CNNåœ¨CIFAR-10å’ŒCIFAR-100æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨CIFAR-10ä¸Šï¼ŒGompertz-CNNç›¸æ¯”ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾8%ã€‚åœ¨CIFAR-100ä¸Šï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾4%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒGompertz-CNNèƒ½å¤Ÿæœ‰æ•ˆæé«˜å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Gompertz-CNNå¯åº”ç”¨äºå„ç§éœ€è¦æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿçš„åœºæ™¯ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„å›¾åƒè¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ä¸­çš„ç›®æ ‡æ£€æµ‹ã€ä»¥åŠèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶åœ¨ä¿æŒè¾ƒä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œè¾¾åˆ°æ¥è¿‘æ•™å¸ˆæ¨¡å‹çš„ç²¾åº¦ã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces a novel dynamic knowledge distillation framework, Gompertz-CNN, which integrates the Gompertz growth model into the training process to address the limitations of traditional knowledge distillation. Conventional methods often fail to capture the evolving cognitive capacity of student models, leading to suboptimal knowledge transfer. To overcome this, we propose a stage-aware distillation strategy that dynamically adjusts the weight of distillation loss based on the Gompertz curve, reflecting the student's learning progression: slow initial growth, rapid mid-phase improvement, and late-stage saturation. Our framework incorporates Wasserstein distance to measure feature-level discrepancies and gradient matching to align backward propagation behaviors between teacher and student models. These components are unified under a multi-loss objective, where the Gompertz curve modulates the influence of distillation losses over time. Extensive experiments on CIFAR-10 and CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and MobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms traditional distillation methods, achieving up to 8% and 4% accuracy gains on CIFAR-10 and CIFAR-100, respectively.

