---
layout: default
title: DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning
---

# DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21635" target="_blank" class="toolbar-btn">arXiv: 2510.21635v1</a>
    <a href="https://arxiv.org/pdf/2510.21635.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21635v1" 
            onclick="toggleFavorite(this, '2510.21635v1', 'DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ziqi Gao, Qiufu Li, Linlin Shen

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-24

**Â§áÊ≥®**: 14 pages, 7 figures, conference

**ÊúüÂàä**: International Conference on Computer Vision 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**DAP-MAEÔºöÈ¢ÜÂüüËá™ÈÄÇÂ∫îÁÇπ‰∫ëÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºåÊèêÂçáË∑®ÂüüÂ≠¶‰π†ÊïàÊûú**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÁÇπ‰∫ëÂàÜÊûê` `Êé©Á†ÅËá™ÁºñÁ†ÅÂô®` `È¢ÜÂüüËá™ÈÄÇÂ∫î` `Ë∑®ÂüüÂ≠¶‰π†` `È¢ÑËÆ≠ÁªÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÊ∑∑Âêà‰∏çÂêåÈ¢ÜÂüüÁÇπ‰∫ëÊï∞ÊçÆËøõË°åMAEÈ¢ÑËÆ≠ÁªÉÔºå‰ΩÜÈ¢ÜÂüüÂ∑ÆÂºÇÂØºËá¥Ê®°ÂûãÊÄßËÉΩ‰∏ãÈôç„ÄÇ
2. DAP-MAEÈÄöËøáÂºÇÊûÑÈ¢ÜÂüüÈÄÇÈÖçÂô®ÂíåÈ¢ÜÂüüÁâπÂæÅÁîüÊàêÂô®ÔºåËá™ÈÄÇÂ∫îÂú∞Êï¥ÂêàË∑®È¢ÜÂüüÁü•ËØÜ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDAP-MAEÂú®Â§ö‰∏™ÁÇπ‰∫ëÂàÜÊûê‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏é2DÊï∞ÊçÆÁõ∏ÊØîÔºåÂèØÁî®‰∫éËÆ≠ÁªÉÁöÑ‰∏çÂêåÈ¢ÜÂüüÁÇπ‰∫ëÊï∞ÊçÆËßÑÊ®°ÊúâÈôê„ÄÇÁ†îÁ©∂‰∫∫ÂëòÂ∞ùËØïÁªìÂêà‰∏çÂêåÈ¢ÜÂüüÁöÑÊï∞ÊçÆËøõË°åÊé©Á†ÅËá™ÁºñÁ†ÅÂô®(MAE)È¢ÑËÆ≠ÁªÉÔºå‰ª•ÁºìËß£Êï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢ò„ÄÇÁÑ∂ËÄåÔºå‰ªéÊ∑∑ÂêàÈ¢ÜÂüüÂ≠¶‰π†Âà∞ÁöÑÂÖàÈ™åÁü•ËØÜÂèØËÉΩ‰∏é‰∏ãÊ∏∏3DÁÇπ‰∫ëÂàÜÊûê‰ªªÂä°‰∏çÂÆåÂÖ®‰∏ÄËá¥ÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ¢ÜÂüüËá™ÈÄÇÂ∫îÁÇπ‰∫ëÊé©Á†ÅËá™ÁºñÁ†ÅÂô®(DAP-MAE)ÔºåËøôÊòØ‰∏ÄÁßçMAEÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÁî®‰∫éËá™ÈÄÇÂ∫îÂú∞Êï¥ÂêàË∑®È¢ÜÂüüÊï∞ÊçÆÈõÜÁöÑÁü•ËØÜÔºå‰ª•ËøõË°åÈÄöÁî®ÁÇπ‰∫ëÂàÜÊûê„ÄÇÂú®DAP-MAE‰∏≠ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÂºÇÊûÑÈ¢ÜÂüüÈÄÇÈÖçÂô®ÔºåÂú®È¢ÑËÆ≠ÁªÉÊúüÈó¥ÈááÁî®ÈÄÇÈÖçÊ®°ÂºèÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂÖ®Èù¢Â≠¶‰π†Êù•Ëá™‰∏çÂêåÈ¢ÜÂüüÁöÑÁÇπ‰∫ë‰ø°ÊÅØÔºåÂêåÊó∂Âú®ÂæÆË∞É‰∏≠ÈááÁî®ËûçÂêàÊ®°ÂºèÊù•Â¢ûÂº∫ÁÇπ‰∫ëÁâπÂæÅ„ÄÇÂêåÊó∂ÔºåDAP-MAEÂåÖÂê´‰∏Ä‰∏™È¢ÜÂüüÁâπÂæÅÁîüÊàêÂô®Ôºå‰ª•ÊåáÂØºÁÇπ‰∫ëÁâπÂæÅÈÄÇÂ∫îÂêÑÁßç‰∏ãÊ∏∏‰ªªÂä°„ÄÇ‰ªÖÈÄöËøá‰∏ÄÊ¨°È¢ÑËÆ≠ÁªÉÔºåDAP-MAEÂú®Âõõ‰∏™‰∏çÂêåÁöÑÁÇπ‰∫ëÂàÜÊûê‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÂá∫Ëâ≤ÁöÑÊÄßËÉΩÔºåÂú®ScanObjectNN‰∏äÁöÑÂØπË±°ÂàÜÁ±ª‰∏≠ËææÂà∞95.18%ÔºåÂú®Bosphorus‰∏äÁöÑÈù¢ÈÉ®Ë°®ÊÉÖËØÜÂà´‰∏≠ËææÂà∞88.45%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Ë∑®È¢ÜÂüüÁÇπ‰∫ëÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉ‰∏≠ÔºåÁî±‰∫éÈ¢ÜÂüüÂ∑ÆÂºÇÂØºËá¥ÁöÑÊ®°ÂûãÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁõ¥Êé•Ê∑∑Âêà‰∏çÂêåÈ¢ÜÂüüÁöÑÊï∞ÊçÆËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂøΩÁï•‰∫ÜÈ¢ÜÂüü‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊÄßÔºåÂØºËá¥Ê®°ÂûãÂ≠¶‰π†Âà∞ÁöÑÁâπÂæÅ‰∏éÁâπÂÆö‰∏ãÊ∏∏‰ªªÂä°‰∏çÂåπÈÖçÔºå‰ªéËÄåÂΩ±ÂìçÊúÄÁªàÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËÆæËÆ°‰∏Ä‰∏™È¢ÜÂüüËá™ÈÄÇÂ∫îÁöÑÈ¢ÑËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂú∞Â≠¶‰π†ÂíåÂà©Áî®Êù•Ëá™‰∏çÂêåÈ¢ÜÂüüÁöÑÊï∞ÊçÆÔºåÂêåÊó∂‰øùÊåÅÂØπÁâπÂÆö‰ªªÂä°ÁöÑÈÄÇÂ∫îÊÄß„ÄÇÈÄöËøáÈ¢ÜÂüüÈÄÇÈÖçÂô®ÂíåÈ¢ÜÂüüÁâπÂæÅÁîüÊàêÂô®ÔºåÊ®°ÂûãËÉΩÂ§üÂå∫ÂàÜÂíåÊï¥Âêà‰∏çÂêåÈ¢ÜÂüüÁöÑÁâπÂæÅÔºåÂπ∂Ê†πÊçÆ‰∏ãÊ∏∏‰ªªÂä°ÁöÑÈúÄÊ±ÇËøõË°åË∞ÉÊï¥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDAP-MAEÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºöÁÇπ‰∫ëÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºàMAEÔºâ„ÄÅÂºÇÊûÑÈ¢ÜÂüüÈÄÇÈÖçÂô®ÂíåÈ¢ÜÂüüÁâπÂæÅÁîüÊàêÂô®„ÄÇÈ¶ñÂÖàÔºåMAEÁî®‰∫éÂ≠¶‰π†ÁÇπ‰∫ëÊï∞ÊçÆÁöÑÈÄöÁî®Ë°®Á§∫„ÄÇÁÑ∂ÂêéÔºåÂºÇÊûÑÈ¢ÜÂüüÈÄÇÈÖçÂô®Âú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÈááÁî®ÈÄÇÈÖçÊ®°ÂºèÔºåÂ≠¶‰π†Ë∑®È¢ÜÂüüÁÇπ‰∫ë‰ø°ÊÅØÔºåÂú®ÂæÆË∞ÉÈò∂ÊÆµÈááÁî®ËûçÂêàÊ®°ÂºèÂ¢ûÂº∫ÁÇπ‰∫ëÁâπÂæÅ„ÄÇÊúÄÂêéÔºåÈ¢ÜÂüüÁâπÂæÅÁîüÊàêÂô®Áî®‰∫éÁîüÊàêÁâπÂÆö‰∫éÈ¢ÜÂüüÁöÑÁâπÂæÅÔºå‰ª•ÊåáÂØºÊ®°ÂûãÈÄÇÂ∫î‰∏çÂêåÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDAP-MAEÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºÇÊûÑÈ¢ÜÂüüÈÄÇÈÖçÂô®ÂíåÈ¢ÜÂüüÁâπÂæÅÁîüÊàêÂô®ÁöÑËÆæËÆ°„ÄÇÂºÇÊûÑÈ¢ÜÂüüÈÄÇÈÖçÂô®ËÉΩÂ§üÂå∫ÂàÜÂíåÊï¥Âêà‰∏çÂêåÈ¢ÜÂüüÁöÑÁâπÂæÅÔºåËÄåÈ¢ÜÂüüÁâπÂæÅÁîüÊàêÂô®ËÉΩÂ§üÊ†πÊçÆ‰∏ãÊ∏∏‰ªªÂä°ÁöÑÈúÄÊ±ÇÁîüÊàêÁâπÂÆö‰∫éÈ¢ÜÂüüÁöÑÁâπÂæÅ„ÄÇËøôÁßçËÆæËÆ°‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®Ë∑®È¢ÜÂüüÊï∞ÊçÆÔºåÂπ∂ÊèêÈ´òÂØπÁâπÂÆö‰ªªÂä°ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåDAP-MAEËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â≠¶‰π†ÂíåÂà©Áî®Ë∑®È¢ÜÂüüÊï∞ÊçÆÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂºÇÊûÑÈ¢ÜÂüüÈÄÇÈÖçÂô®ÂåÖÂê´ÈÄÇÈÖçÊ®°ÂºèÂíåËûçÂêàÊ®°Âºè„ÄÇÈÄÇÈÖçÊ®°ÂºèÁî®‰∫éÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºåÊó®Âú®Â≠¶‰π†‰∏çÂêåÈ¢ÜÂüüÁöÑÁâπÂæÅË°®Á§∫ÔºåËûçÂêàÊ®°ÂºèÁî®‰∫éÂæÆË∞ÉÈò∂ÊÆµÔºåÊó®Âú®ËûçÂêà‰∏çÂêåÈ¢ÜÂüüÁöÑÁâπÂæÅÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇÈ¢ÜÂüüÁâπÂæÅÁîüÊàêÂô®ÈÄöËøáÂ≠¶‰π†È¢ÜÂüüÁõ∏ÂÖ≥ÁöÑÁâπÂæÅÂêëÈáèÔºåÊåáÂØºÊ®°ÂûãÈÄÇÂ∫î‰∏çÂêåÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Êú™Áü•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DAP-MAEÂú®Âõõ‰∏™‰∏çÂêåÁöÑÁÇπ‰∫ëÂàÜÊûê‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®ScanObjectNNÂØπË±°ÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåDAP-MAEËææÂà∞‰∫Ü95.18%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÂú®BosphorusÈù¢ÈÉ®Ë°®ÊÉÖËØÜÂà´‰ªªÂä°‰∏≠ÔºåDAP-MAEËææÂà∞‰∫Ü88.45%ÁöÑÂáÜÁ°ÆÁéá„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåDAP-MAEËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Ë∑®È¢ÜÂüüÊï∞ÊçÆÔºåÂπ∂ÊèêÈ´òÊ®°ÂûãÂú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DAP-MAEÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂà©Áî®Ë∑®È¢ÜÂüüÁÇπ‰∫ëÊï∞ÊçÆÁöÑ3DËßÜËßâ‰ªªÂä°Ôºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÂåªÁñóÂΩ±ÂÉèÂàÜÊûêÁ≠â„ÄÇÈÄöËøáÈ¢ÑËÆ≠ÁªÉÔºåÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞ÈÄöÁî®ÁöÑÁÇπ‰∫ëË°®Á§∫Ôºå‰ªéËÄåÂáèÂ∞ëÂØπÁâπÂÆöÈ¢ÜÂüüÊï∞ÊçÆÁöÑ‰æùËµñÔºåÈôç‰ΩéÊ†áÊ≥®ÊàêÊú¨ÔºåÂπ∂ÊèêÈ´òÊ®°ÂûãÂú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊé®Âä®3DËßÜËßâÊäÄÊúØÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Compared to 2D data, the scale of point cloud data in different domains available for training, is quite limited. Researchers have been trying to combine these data of different domains for masked autoencoder (MAE) pre-training to leverage such a data scarcity issue. However, the prior knowledge learned from mixed domains may not align well with the downstream 3D point cloud analysis tasks, leading to degraded performance. To address such an issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE), an MAE pre-training method, to adaptively integrate the knowledge of cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a heterogeneous domain adapter that utilizes an adaptation mode during pre-training, enabling the model to comprehensively learn information from point clouds across different domains, while employing a fusion mode in the fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a domain feature generator to guide the adaptation of point cloud features to various downstream tasks. With only one pre-training, DAP-MAE achieves excellent performance across four different point cloud analysis tasks, reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial expression recognition on Bosphorus.

