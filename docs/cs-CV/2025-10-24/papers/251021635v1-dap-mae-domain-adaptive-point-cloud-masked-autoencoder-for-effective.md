---
layout: default
title: "DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning"
---

# DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21635" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21635v1</a>
  <a href="https://arxiv.org/pdf/2510.21635.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21635v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21635v1', 'DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ziqi Gao, Qiufu Li, Linlin Shen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

**å¤‡æ³¨**: 14 pages, 7 figures, conference

**æœŸåˆŠ**: International Conference on Computer Vision 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DAP-MAEï¼šé¢†åŸŸè‡ªé€‚åº”ç‚¹äº‘æ©ç è‡ªç¼–ç å™¨ï¼Œæå‡è·¨åŸŸå­¦ä¹ æ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç‚¹äº‘åˆ†æ` `æ©ç è‡ªç¼–ç å™¨` `é¢†åŸŸè‡ªé€‚åº”` `è·¨åŸŸå­¦ä¹ ` `é¢„è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•æ··åˆä¸åŒé¢†åŸŸç‚¹äº‘æ•°æ®è¿›è¡ŒMAEé¢„è®­ç»ƒï¼Œä½†é¢†åŸŸå·®å¼‚å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
2. DAP-MAEé€šè¿‡å¼‚æ„é¢†åŸŸé€‚é…å™¨å’Œé¢†åŸŸç‰¹å¾ç”Ÿæˆå™¨ï¼Œè‡ªé€‚åº”åœ°æ•´åˆè·¨é¢†åŸŸçŸ¥è¯†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDAP-MAEåœ¨å¤šä¸ªç‚¹äº‘åˆ†æä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸2Dæ•°æ®ç›¸æ¯”ï¼Œå¯ç”¨äºè®­ç»ƒçš„ä¸åŒé¢†åŸŸç‚¹äº‘æ•°æ®è§„æ¨¡æœ‰é™ã€‚ç ”ç©¶äººå‘˜å°è¯•ç»“åˆä¸åŒé¢†åŸŸçš„æ•°æ®è¿›è¡Œæ©ç è‡ªç¼–ç å™¨(MAE)é¢„è®­ç»ƒï¼Œä»¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ç„¶è€Œï¼Œä»æ··åˆé¢†åŸŸå­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†å¯èƒ½ä¸ä¸‹æ¸¸3Dç‚¹äº‘åˆ†æä»»åŠ¡ä¸å®Œå…¨ä¸€è‡´ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢†åŸŸè‡ªé€‚åº”ç‚¹äº‘æ©ç è‡ªç¼–ç å™¨(DAP-MAE)ï¼Œè¿™æ˜¯ä¸€ç§MAEé¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºè‡ªé€‚åº”åœ°æ•´åˆè·¨é¢†åŸŸæ•°æ®é›†çš„çŸ¥è¯†ï¼Œä»¥è¿›è¡Œé€šç”¨ç‚¹äº‘åˆ†æã€‚åœ¨DAP-MAEä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¼‚æ„é¢†åŸŸé€‚é…å™¨ï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´é‡‡ç”¨é€‚é…æ¨¡å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…¨é¢å­¦ä¹ æ¥è‡ªä¸åŒé¢†åŸŸçš„ç‚¹äº‘ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨å¾®è°ƒä¸­é‡‡ç”¨èåˆæ¨¡å¼æ¥å¢å¼ºç‚¹äº‘ç‰¹å¾ã€‚åŒæ—¶ï¼ŒDAP-MAEåŒ…å«ä¸€ä¸ªé¢†åŸŸç‰¹å¾ç”Ÿæˆå™¨ï¼Œä»¥æŒ‡å¯¼ç‚¹äº‘ç‰¹å¾é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ä»…é€šè¿‡ä¸€æ¬¡é¢„è®­ç»ƒï¼ŒDAP-MAEåœ¨å››ä¸ªä¸åŒçš„ç‚¹äº‘åˆ†æä»»åŠ¡ä¸­å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ScanObjectNNä¸Šçš„å¯¹è±¡åˆ†ç±»ä¸­è¾¾åˆ°95.18%ï¼Œåœ¨Bosphorusä¸Šçš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä¸­è¾¾åˆ°88.45%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è·¨é¢†åŸŸç‚¹äº‘æ•°æ®é¢„è®­ç»ƒä¸­ï¼Œç”±äºé¢†åŸŸå·®å¼‚å¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç›´æ¥æ··åˆä¸åŒé¢†åŸŸçš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¿½ç•¥äº†é¢†åŸŸä¹‹é—´çš„å·®å¼‚æ€§ï¼Œå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°çš„ç‰¹å¾ä¸ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ä¸åŒ¹é…ï¼Œä»è€Œå½±å“æœ€ç»ˆæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ä¸ªé¢†åŸŸè‡ªé€‚åº”çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å’Œåˆ©ç”¨æ¥è‡ªä¸åŒé¢†åŸŸçš„æ•°æ®ï¼ŒåŒæ—¶ä¿æŒå¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”æ€§ã€‚é€šè¿‡é¢†åŸŸé€‚é…å™¨å’Œé¢†åŸŸç‰¹å¾ç”Ÿæˆå™¨ï¼Œæ¨¡å‹èƒ½å¤ŸåŒºåˆ†å’Œæ•´åˆä¸åŒé¢†åŸŸçš„ç‰¹å¾ï¼Œå¹¶æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDAP-MAEçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šç‚¹äº‘æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰ã€å¼‚æ„é¢†åŸŸé€‚é…å™¨å’Œé¢†åŸŸç‰¹å¾ç”Ÿæˆå™¨ã€‚é¦–å…ˆï¼ŒMAEç”¨äºå­¦ä¹ ç‚¹äº‘æ•°æ®çš„é€šç”¨è¡¨ç¤ºã€‚ç„¶åï¼Œå¼‚æ„é¢†åŸŸé€‚é…å™¨åœ¨é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨é€‚é…æ¨¡å¼ï¼Œå­¦ä¹ è·¨é¢†åŸŸç‚¹äº‘ä¿¡æ¯ï¼Œåœ¨å¾®è°ƒé˜¶æ®µé‡‡ç”¨èåˆæ¨¡å¼å¢å¼ºç‚¹äº‘ç‰¹å¾ã€‚æœ€åï¼Œé¢†åŸŸç‰¹å¾ç”Ÿæˆå™¨ç”¨äºç”Ÿæˆç‰¹å®šäºé¢†åŸŸçš„ç‰¹å¾ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šDAP-MAEçš„å…³é”®åˆ›æ–°åœ¨äºå¼‚æ„é¢†åŸŸé€‚é…å™¨å’Œé¢†åŸŸç‰¹å¾ç”Ÿæˆå™¨çš„è®¾è®¡ã€‚å¼‚æ„é¢†åŸŸé€‚é…å™¨èƒ½å¤ŸåŒºåˆ†å’Œæ•´åˆä¸åŒé¢†åŸŸçš„ç‰¹å¾ï¼Œè€Œé¢†åŸŸç‰¹å¾ç”Ÿæˆå™¨èƒ½å¤Ÿæ ¹æ®ä¸‹æ¸¸ä»»åŠ¡çš„éœ€æ±‚ç”Ÿæˆç‰¹å®šäºé¢†åŸŸçš„ç‰¹å¾ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è·¨é¢†åŸŸæ•°æ®ï¼Œå¹¶æé«˜å¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDAP-MAEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å’Œåˆ©ç”¨è·¨é¢†åŸŸæ•°æ®ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šå¼‚æ„é¢†åŸŸé€‚é…å™¨åŒ…å«é€‚é…æ¨¡å¼å’Œèåˆæ¨¡å¼ã€‚é€‚é…æ¨¡å¼ç”¨äºé¢„è®­ç»ƒé˜¶æ®µï¼Œæ—¨åœ¨å­¦ä¹ ä¸åŒé¢†åŸŸçš„ç‰¹å¾è¡¨ç¤ºï¼Œèåˆæ¨¡å¼ç”¨äºå¾®è°ƒé˜¶æ®µï¼Œæ—¨åœ¨èåˆä¸åŒé¢†åŸŸçš„ç‰¹å¾ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚é¢†åŸŸç‰¹å¾ç”Ÿæˆå™¨é€šè¿‡å­¦ä¹ é¢†åŸŸç›¸å…³çš„ç‰¹å¾å‘é‡ï¼ŒæŒ‡å¯¼æ¨¡å‹é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°è®¾è®¡æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DAP-MAEåœ¨å››ä¸ªä¸åŒçš„ç‚¹äº‘åˆ†æä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ScanObjectNNå¯¹è±¡åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒDAP-MAEè¾¾åˆ°äº†95.18%çš„å‡†ç¡®ç‡ã€‚åœ¨Bosphorusé¢éƒ¨è¡¨æƒ…è¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒDAP-MAEè¾¾åˆ°äº†88.45%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDAP-MAEèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è·¨é¢†åŸŸæ•°æ®ï¼Œå¹¶æé«˜æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DAP-MAEå¯åº”ç”¨äºå„ç§éœ€è¦åˆ©ç”¨è·¨é¢†åŸŸç‚¹äº‘æ•°æ®çš„3Dè§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€åŒ»ç–—å½±åƒåˆ†æç­‰ã€‚é€šè¿‡é¢„è®­ç»ƒï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°é€šç”¨çš„ç‚¹äº‘è¡¨ç¤ºï¼Œä»è€Œå‡å°‘å¯¹ç‰¹å®šé¢†åŸŸæ•°æ®çš„ä¾èµ–ï¼Œé™ä½æ ‡æ³¨æˆæœ¬ï¼Œå¹¶æé«˜æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨3Dè§†è§‰æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Compared to 2D data, the scale of point cloud data in different domains available for training, is quite limited. Researchers have been trying to combine these data of different domains for masked autoencoder (MAE) pre-training to leverage such a data scarcity issue. However, the prior knowledge learned from mixed domains may not align well with the downstream 3D point cloud analysis tasks, leading to degraded performance. To address such an issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE), an MAE pre-training method, to adaptively integrate the knowledge of cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a heterogeneous domain adapter that utilizes an adaptation mode during pre-training, enabling the model to comprehensively learn information from point clouds across different domains, while employing a fusion mode in the fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a domain feature generator to guide the adaptation of point cloud features to various downstream tasks. With only one pre-training, DAP-MAE achieves excellent performance across four different point cloud analysis tasks, reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial expression recognition on Bosphorus.

