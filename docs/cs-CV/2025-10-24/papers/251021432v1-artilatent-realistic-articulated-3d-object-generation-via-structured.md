---
layout: default
title: "ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents"
---

# ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21432" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21432v1</a>
  <a href="https://arxiv.org/pdf/2510.21432.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21432v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21432v1', 'ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Honghua Chen, Yushi Lan, Yongwei Chen, Xingang Pan

**åˆ†ç±»**: cs.CV, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

**å¤‡æ³¨**: accepted to SIGGRAPH Asia; Project page: https://chenhonghua.github.io/MyProjects/ArtiLatent/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ArtiLatentï¼šé€šè¿‡ç»“æ„åŒ–éšç©ºé—´ç”Ÿæˆé€¼çœŸå¯åŠ¨3Dç‰©ä½“**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `3Dç‰©ä½“ç”Ÿæˆ` `å¯åŠ¨ç‰©ä½“` `éšç©ºé—´è¡¨ç¤º` `æ‰©æ•£æ¨¡å‹` `é“°æ¥æ„ŸçŸ¥` `å‡ ä½•å»ºæ¨¡` `å¤–è§‚å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥ç”Ÿæˆå…·æœ‰ç²¾ç»†å‡ ä½•ã€ç²¾ç¡®é“°æ¥å’Œé€¼çœŸå¤–è§‚çš„å¯åŠ¨3Dç‰©ä½“ï¼Œå°¤å…¶æ˜¯åœ¨é“°æ¥çŠ¶æ€å˜åŒ–æ—¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ã€‚
2. ArtiLatenté€šè¿‡ç»“æ„åŒ–éšç©ºé—´è”åˆå»ºæ¨¡éƒ¨ä»¶å‡ ä½•å’Œé“°æ¥åŠ¨æ€ï¼Œåˆ©ç”¨éšæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·ä¸”ç‰©ç†å¯ä¿¡çš„æ ·æœ¬ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒArtiLatentåœ¨å‡ ä½•ä¸€è‡´æ€§å’Œå¤–è§‚ä¿çœŸåº¦ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨é“°æ¥çŠ¶æ€å˜åŒ–æ—¶èƒ½ä¿æŒè§†è§‰çœŸå®æ„Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†ArtiLatentï¼Œä¸€ä¸ªç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºåˆæˆå…·æœ‰ç²¾ç»†å‡ ä½•å½¢çŠ¶ã€ç²¾ç¡®é“°æ¥å’Œé€¼çœŸå¤–è§‚çš„äººé€ 3Dç‰©ä½“ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨å°†ç¨€ç–ä½“ç´ è¡¨ç¤ºå’Œç›¸å…³çš„é“°æ¥å±æ€§ï¼ˆåŒ…æ‹¬å…³èŠ‚ç±»å‹ã€è½´ã€åŸç‚¹ã€èŒƒå›´å’Œéƒ¨ä»¶ç±»åˆ«ï¼‰åµŒå…¥åˆ°ç»Ÿä¸€çš„éšç©ºé—´ä¸­ï¼Œä»è€Œè”åˆå»ºæ¨¡éƒ¨ä»¶å‡ ä½•å½¢çŠ¶å’Œé“°æ¥åŠ¨æ€ã€‚ç„¶åï¼Œåœ¨è¿™ä¸ªç©ºé—´ä¸Šè®­ç»ƒä¸€ä¸ªéšæ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°å¤šæ ·ä½†ç‰©ç†ä¸Šåˆç†çš„é‡‡æ ·ã€‚ä¸ºäº†é‡å»ºé€¼çœŸçš„3Då½¢çŠ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé“°æ¥æ„ŸçŸ¥é«˜æ–¯è§£ç å™¨ï¼Œè¯¥è§£ç å™¨è€ƒè™‘äº†é“°æ¥ç›¸å…³çš„å¯è§æ€§å˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œæ‰“å¼€æŠ½å±‰æ—¶éœ²å‡ºå†…éƒ¨ï¼‰ã€‚é€šè¿‡å°†å¤–è§‚è§£ç å»ºç«‹åœ¨é“°æ¥çŠ¶æ€çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºé™æ€å§¿åŠ¿ä¸­é€šå¸¸è¢«é®æŒ¡çš„åŒºåŸŸåˆ†é…åˆç†çš„çº¹ç†ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†å„ç§é“°æ¥é…ç½®ä¸­çš„è§†è§‰çœŸå®æ„Ÿã€‚åœ¨PartNet-Mobilityå’ŒACDæ•°æ®é›†ä¸­ï¼Œå¯¹ç±»ä¼¼å®¶å…·çš„ç‰©ä½“è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒArtiLatentåœ¨å‡ ä½•ä¸€è‡´æ€§å’Œå¤–è§‚ä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¯åŠ¨3Dç‰©ä½“çš„åˆæˆå’Œæ“ä½œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿçš„å¯åŠ¨3Dç‰©ä½“çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å¤æ‚å‡ ä½•å½¢çŠ¶ã€ç²¾ç¡®é“°æ¥å’Œé€¼çœŸå¤–è§‚çš„3Dç‰©ä½“æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é“°æ¥éƒ¨ä»¶çš„é®æŒ¡å’Œçº¹ç†å˜åŒ–æ—¶ï¼Œéš¾ä»¥ä¿è¯è§†è§‰ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†éƒ¨ä»¶çš„å‡ ä½•å½¢çŠ¶å’Œé“°æ¥å±æ€§ç¼–ç åˆ°ç»Ÿä¸€çš„éšç©ºé—´ä¸­ï¼Œå¹¶åˆ©ç”¨éšæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æ ·æœ¬ã€‚é€šè¿‡é“°æ¥æ„ŸçŸ¥çš„é«˜æ–¯è§£ç å™¨ï¼Œè€ƒè™‘é“°æ¥çŠ¶æ€å¯¹å¯è§æ€§çš„å½±å“ï¼Œä»è€Œä¸ºé€šå¸¸è¢«é®æŒ¡çš„åŒºåŸŸåˆ†é…åˆç†çš„çº¹ç†ç‰¹å¾ï¼Œæé«˜è§†è§‰çœŸå®æ„Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šArtiLatentæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œç”¨äºå°†ç¨€ç–ä½“ç´ è¡¨ç¤ºå’Œé“°æ¥å±æ€§ç¼–ç åˆ°éšç©ºé—´ï¼›2) éšæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåœ¨éšç©ºé—´ä¸­ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼›3) é“°æ¥æ„ŸçŸ¥é«˜æ–¯è§£ç å™¨ï¼Œç”¨äºå°†éšç©ºé—´ä¸­çš„æ ·æœ¬è§£ç ä¸º3Då½¢çŠ¶ï¼Œå¹¶è€ƒè™‘é“°æ¥çŠ¶æ€å¯¹å¯è§æ€§çš„å½±å“ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆé€šè¿‡VAEå°†3Dç‰©ä½“ç¼–ç åˆ°éšç©ºé—´ï¼Œç„¶ååˆ©ç”¨éšæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°çš„éšç©ºé—´å‘é‡ï¼Œæœ€åé€šè¿‡é“°æ¥æ„ŸçŸ¥é«˜æ–¯è§£ç å™¨å°†éšç©ºé—´å‘é‡è§£ç ä¸º3Dç‰©ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„éšç©ºé—´æ¥è”åˆå»ºæ¨¡éƒ¨ä»¶å‡ ä½•å½¢çŠ¶å’Œé“°æ¥åŠ¨æ€ï¼›2) å¼•å…¥äº†é“°æ¥æ„ŸçŸ¥é«˜æ–¯è§£ç å™¨ï¼Œè€ƒè™‘äº†é“°æ¥çŠ¶æ€å¯¹å¯è§æ€§çš„å½±å“ï¼Œä»è€Œæé«˜äº†è§†è§‰çœŸå®æ„Ÿï¼›3) åˆ©ç”¨éšæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æ ·æœ¬ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒArtiLatentèƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸã€æ›´å…·å‡ ä½•ä¸€è‡´æ€§çš„å¯åŠ¨3Dç‰©ä½“ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†ç¨€ç–ä½“ç´ è¡¨ç¤ºæ¥è¡¨ç¤º3Dç‰©ä½“çš„å‡ ä½•å½¢çŠ¶ã€‚é“°æ¥å±æ€§åŒ…æ‹¬å…³èŠ‚ç±»å‹ã€è½´ã€åŸç‚¹ã€èŒƒå›´å’Œéƒ¨ä»¶ç±»åˆ«ã€‚å˜åˆ†è‡ªç¼–ç å™¨çš„æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±å’ŒKLæ•£åº¦ã€‚éšæ‰©æ•£æ¨¡å‹ä½¿ç”¨U-Netæ¶æ„ã€‚é“°æ¥æ„ŸçŸ¥é«˜æ–¯è§£ç å™¨é€šè¿‡å°†é“°æ¥çŠ¶æ€ä½œä¸ºæ¡ä»¶è¾“å…¥æ¥è°ƒæ•´è§£ç è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒArtiLatentåœ¨PartNet-Mobilityå’ŒACDæ•°æ®é›†ä¸Šï¼Œåœ¨å‡ ä½•ä¸€è‡´æ€§å’Œå¤–è§‚ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é€šè¿‡é“°æ¥æ„ŸçŸ¥è§£ç å™¨ï¼ŒArtiLatentèƒ½å¤Ÿä¸ºé€šå¸¸è¢«é®æŒ¡çš„åŒºåŸŸåˆ†é…åˆç†çš„çº¹ç†ç‰¹å¾ï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰çœŸå®æ„Ÿã€‚å®šæ€§å’Œå®šé‡ç»“æœéƒ½è¯æ˜äº†ArtiLatentçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ArtiLatentå¯åº”ç”¨äºæ¸¸æˆå¼€å‘ã€è™šæ‹Ÿç°å®ã€æœºå™¨äººä»¿çœŸç­‰é¢†åŸŸï¼Œä¸ºè¿™äº›é¢†åŸŸæä¾›é«˜è´¨é‡ã€å¯å®šåˆ¶çš„å¯åŠ¨3Dç‰©ä½“èµ„æºã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡è™šæ‹Ÿç¯å¢ƒçš„çœŸå®æ„Ÿå’Œäº¤äº’æ€§ï¼Œå¹¶ä¸ºæœºå™¨äººæ“ä½œå’Œè§„åˆ’æä¾›æ›´é€¼çœŸçš„ç¯å¢ƒæ¨¡å‹ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯æ‰©å±•åˆ°æ›´å¤æ‚çš„ç‰©ä½“å’Œåœºæ™¯ï¼Œå¹¶åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose ArtiLatent, a generative framework that synthesizes human-made 3D objects with fine-grained geometry, accurate articulation, and realistic appearance. Our approach jointly models part geometry and articulation dynamics by embedding sparse voxel representations and associated articulation properties, including joint type, axis, origin, range, and part category, into a unified latent space via a variational autoencoder. A latent diffusion model is then trained over this space to enable diverse yet physically plausible sampling. To reconstruct photorealistic 3D shapes, we introduce an articulation-aware Gaussian decoder that accounts for articulation-dependent visibility changes (e.g., revealing the interior of a drawer when opened). By conditioning appearance decoding on articulation state, our method assigns plausible texture features to regions that are typically occluded in static poses, significantly improving visual realism across articulation configurations. Extensive experiments on furniture-like objects from PartNet-Mobility and ACD datasets demonstrate that ArtiLatent outperforms existing approaches in geometric consistency and appearance fidelity. Our framework provides a scalable solution for articulated 3D object synthesis and manipulation.

