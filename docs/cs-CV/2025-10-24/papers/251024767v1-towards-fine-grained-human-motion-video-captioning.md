---
layout: default
title: Towards Fine-Grained Human Motion Video Captioning
---

# Towards Fine-Grained Human Motion Video Captioning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.24767" target="_blank" class="toolbar-btn">arXiv: 2510.24767v1</a>
    <a href="https://arxiv.org/pdf/2510.24767.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.24767v1" 
            onclick="toggleFavorite(this, '2510.24767v1', 'Towards Fine-Grained Human Motion Video Captioning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Guorui Song, Guocun Wang, Zhe Huang, Jing Lin, Xuefei Zhe, Jian Li, Haoqian Wang

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ËøêÂä®Â¢ûÂº∫ÁöÑÂ≠óÂπïÊ®°Âûã(M-ACM)ÔºåÁî®‰∫éÁîüÊàêÁªÜÁ≤íÂ∫¶ÁöÑ‰∫∫‰ΩìËøêÂä®ËßÜÈ¢ëÊèèËø∞„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÂ≠óÂπï` `‰∫∫‰ΩìËøêÂä®` `ËøêÂä®Ë°®ÂæÅ` `‰∫∫‰ΩìÁΩëÊ†ºÊÅ¢Â§ç` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜÈ¢ëÂ≠óÂπïÊ®°ÂûãÈöæ‰ª•ÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑ‰∫∫‰ΩìËøêÂä®ÁªÜËäÇÔºåÂØºËá¥ÁîüÊàêÁöÑÊèèËø∞Ê®°Á≥ä‰∏îËØ≠‰πâ‰∏ç‰∏ÄËá¥„ÄÇ
2. M-ACMÈÄöËøáÊï¥Âêà‰ªé‰∫∫‰ΩìÁΩëÊ†ºÊÅ¢Â§ç‰∏≠ÊèêÂèñÁöÑËøêÂä®Ë°®ÂæÅÔºåÊòæÂºèÂú∞Â¢ûÂº∫Ê®°ÂûãÂØπ‰∫∫‰ΩìÂä®ÊÄÅÁöÑÊÑüÁü•ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåM-ACMÂú®ÊèèËø∞Â§çÊùÇ‰∫∫‰ΩìËøêÂä®ÂíåÁªÜÂæÆÊó∂Èó¥ÂèòÂåñÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜÈ¢ëÂ≠óÂπïÊ®°ÂûãÁîüÊàêÂáÜÁ°ÆÁöÑ‰∫∫‰ΩìÂä®‰ΩúÊèèËø∞‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Èöæ‰ª•ÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑËøêÂä®ÁªÜËäÇÔºåÂØºËá¥Ê®°Á≥äÊàñËØ≠‰πâ‰∏ç‰∏ÄËá¥ÁöÑÂ≠óÂπï„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁîüÊàêÊ°ÜÊû∂‚Äî‚ÄîËøêÂä®Â¢ûÂº∫ÁöÑÂ≠óÂπïÊ®°Âûã(M-ACM)ÔºåÈÄöËøáÁªìÂêàËøêÂä®ÊÑüÁü•ÁöÑËß£Á†ÅÊù•ÊèêÈ´òÂ≠óÂπïË¥®Èáè„ÄÇM-ACMÂà©Áî®‰ªé‰∫∫‰ΩìÁΩëÊ†ºÊÅ¢Â§ç‰∏≠ÊèêÂèñÁöÑËøêÂä®Ë°®ÂæÅÔºåÊòæÂºèÂú∞Á™ÅÂá∫‰∫∫‰ΩìÂä®ÊÄÅÔºå‰ªéËÄåÂáèÂ∞ëÂπªËßâÔºåÂπ∂ÊèêÈ´òÁîüÊàêÂ≠óÂπïÁöÑËØ≠‰πâ‰øùÁúüÂ∫¶ÂíåÁ©∫Èó¥ÂØπÈΩê„ÄÇ‰∏∫‰∫ÜÊîØÊåÅËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∫∫‰ΩìËøêÂä®Ê¥ûÂØü(HMI)Êï∞ÊçÆÈõÜÔºåÂåÖÂê´115K‰∏™‰∏ìÊ≥®‰∫é‰∫∫‰ΩìËøêÂä®ÁöÑËßÜÈ¢ë-ÊèèËø∞ÂØπÔºå‰ª•ÂèäHMI-BenchÔºå‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞ËøêÂä®ËÅöÁÑ¶ÁöÑËßÜÈ¢ëÂ≠óÂπïÁöÑ‰∏ìÁî®Âü∫ÂáÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåM-ACMÂú®ÂáÜÁ°ÆÊèèËø∞Â§çÊùÇÁöÑ‰∫∫‰ΩìËøêÂä®ÂíåÁªÜÂæÆÁöÑÊó∂Èó¥ÂèòÂåñÊñπÈù¢ÊòæËëó‰ºò‰∫é‰ª•ÂâçÁöÑÊñπÊ≥ïÔºå‰∏∫‰ª•ËøêÂä®‰∏∫‰∏≠ÂøÉÁöÑËßÜÈ¢ëÂ≠óÂπïËÆæÂÆö‰∫ÜÊñ∞ÁöÑÊ†áÂáÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜÈ¢ëÂ≠óÂπïÊ®°ÂûãÂú®ÊèèËø∞‰∫∫‰ΩìËøêÂä®ËßÜÈ¢ëÊó∂ÔºåÈöæ‰ª•ÊçïÊçâÂà∞ÁªÜÁ≤íÂ∫¶ÁöÑËøêÂä®‰ø°ÊÅØÔºåÂØºËá¥ÁîüÊàêÁöÑÂ≠óÂπï‰∏çÂ§üÂáÜÁ°ÆÔºåÁîöËá≥Âá∫Áé∞ËØ≠‰πâÈîôËØØ„ÄÇËøô‰∫õÊ®°ÂûãÂæÄÂæÄÂøΩÁï•‰∫Ü‰∫∫‰ΩìËøêÂä®ÁöÑÂä®ÊÄÅÁâπÊÄßÔºåÊó†Ê≥ïÂæàÂ•ΩÂú∞ÁêÜËß£ÂíåÊèèËø∞Âä®‰ΩúÁöÑÁªÜËäÇ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöM-ACMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•ËøêÂä®‰ø°ÊÅØÊù•Â¢ûÂº∫ËßÜÈ¢ëÂ≠óÂπïÊ®°ÂûãÂØπ‰∫∫‰ΩìËøêÂä®ÁöÑÁêÜËß£„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËØ•Ê®°ÂûãÂà©Áî®‰∫∫‰ΩìÁΩëÊ†ºÊÅ¢Â§çÊäÄÊúØÊèêÂèñËßÜÈ¢ë‰∏≠ÁöÑ‰∫∫‰ΩìËøêÂä®Ë°®ÂæÅÔºåÂπ∂Â∞ÜËøô‰∫õË°®ÂæÅËûçÂÖ•Âà∞Ëß£Á†ÅËøáÁ®ã‰∏≠Ôºå‰ªéËÄå‰ΩøÊ®°ÂûãËÉΩÂ§üÁîüÊàêÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ÁªÜËá¥ÁöÑÂ≠óÂπï„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöM-ACMÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËßÜÈ¢ëÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÊèêÂèñËßÜÈ¢ëÁöÑËßÜËßâÁâπÂæÅÔºõ2) ‰∫∫‰ΩìÁΩëÊ†ºÊÅ¢Â§çÊ®°ÂùóÔºöÁî®‰∫é‰ªéËßÜÈ¢ë‰∏≠ÊèêÂèñ‰∫∫‰ΩìËøêÂä®Ë°®ÂæÅÔºõ3) ËøêÂä®Â¢ûÂº∫ÁöÑËß£Á†ÅÂô®ÔºöÂ∞ÜËßÜËßâÁâπÂæÅÂíåËøêÂä®Ë°®ÂæÅÁªìÂêàËµ∑Êù•ÔºåÁîüÊàêËßÜÈ¢ëÂ≠óÂπï„ÄÇËß£Á†ÅÂô®ÊòØM-ACMÁöÑÊ†∏ÂøÉÔºåÂÆÉÂà©Áî®Ê≥®ÊÑèÂäõÊú∫Âà∂Â∞ÜËøêÂä®‰ø°ÊÅØËûçÂÖ•Âà∞Â≠óÂπïÁîüÊàêËøáÁ®ã‰∏≠„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöM-ACMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ËøêÂä®Â¢ûÂº∫ÁöÑËß£Á†ÅÂô®„ÄÇËØ•Ëß£Á†ÅÂô®ËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®‰∫∫‰ΩìËøêÂä®Ë°®ÂæÅÊù•ÊåáÂØºÂ≠óÂπïÁîüÊàêÔºå‰ªéËÄåÊèêÈ´òÂ≠óÂπïÁöÑÂáÜÁ°ÆÊÄßÂíåÁªÜËäÇÁ®ãÂ∫¶„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåM-ACMËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâ‰∫∫‰ΩìËøêÂä®ÁöÑÂä®ÊÄÅÁâπÊÄßÔºåÂπ∂ÁîüÊàêÊõ¥Á¨¶ÂêàËßÜÈ¢ëÂÜÖÂÆπÁöÑÂ≠óÂπï„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®‰∫∫‰ΩìÁΩëÊ†ºÊÅ¢Â§çÊ®°Âùó‰∏≠ÔºåËÆ∫ÊñáÂèØËÉΩÈááÁî®‰∫ÜÁé∞ÊúâÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÔºå‰æãÂ¶ÇSMPLifyÊàñHMR„ÄÇËøêÂä®Ë°®ÂæÅÂèØËÉΩÂåÖÊã¨‰∫∫‰ΩìÂÖ≥ËäÇÁöÑ‰ΩçÁΩÆ„ÄÅÈÄüÂ∫¶ÂíåÂä†ÈÄüÂ∫¶Á≠â‰ø°ÊÅØ„ÄÇÂú®Ëß£Á†ÅÂô®‰∏≠ÔºåËÆ∫ÊñáÂèØËÉΩ‰ΩøÁî®‰∫ÜLSTMÊàñTransformerÁ≠âÂ∫èÂàóÁîüÊàêÊ®°ÂûãÔºåÂπ∂ÂºïÂÖ•‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•ÂÖ≥Ê≥®‰∏éÂΩìÂâçÁîüÊàêËØçÁõ∏ÂÖ≥ÁöÑËøêÂä®‰ø°ÊÅØ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂèØËÉΩÂåÖÊã¨‰∫§ÂèâÁÜµÊçüÂ§±Âíå‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÊ≠£ÂàôÂåñÈ°πÔºå‰ª•ÈºìÂä±Ê®°ÂûãÁîüÊàêÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥ÊµÅÁïÖÁöÑÂ≠óÂπï„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

M-ACMÂú®HMI-BenchÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåË°®ÊòéÂÖ∂Âú®ËøêÂä®ËÅöÁÑ¶ÁöÑËßÜÈ¢ëÂ≠óÂπï‰ªªÂä°‰∏≠ÂÖ∑Êúâ‰ºòË∂äÊÄß„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåM-ACMÂú®Â§ö‰∏™ÊåáÊ†á‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑËßÜÈ¢ëÂ≠óÂπïÊ®°ÂûãÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊèèËø∞Â§çÊùÇ‰∫∫‰ΩìËøêÂä®ÂíåÁªÜÂæÆÊó∂Èó¥ÂèòÂåñÊñπÈù¢Ë°®Áé∞Á™ÅÂá∫„ÄÇËøô‰∫õÂÆûÈ™åÁªìÊûúÈ™åËØÅ‰∫ÜM-ACMÈÄöËøáËøêÂä®Â¢ûÂº∫Ëß£Á†ÅÊù•ÊèêÈ´òÂ≠óÂπïË¥®ÈáèÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËßÜÈ¢ëÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅËøêÂä®ÂàÜÊûê„ÄÅÊ∏∏ÊàèÂºÄÂèëÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®ËßÜÈ¢ëÁõëÊéß‰∏≠ÔºåÂèØ‰ª•Ëá™Âä®ÁîüÊàêÂØπÂºÇÂ∏∏Ë°å‰∏∫ÁöÑÊèèËø∞ÔºõÂú®‰∫∫Êú∫‰∫§‰∫í‰∏≠ÔºåÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫ÁêÜËß£‰∫∫Á±ªÁöÑÂä®‰ΩúÊÑèÂõæÔºõÂú®ËøêÂä®ÂàÜÊûê‰∏≠ÔºåÂèØ‰ª•Êèê‰æõÊõ¥ËØ¶ÁªÜÁöÑËøêÂä®Êä•Âëä„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõËøõ‰∏ÄÊ≠•ÊèêÂçáËßÜÈ¢ëÁêÜËß£Âíå‰∫∫Êú∫‰∫§‰∫íÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Generating accurate descriptions of human actions in videos remains a challenging task for video captioning models. Existing approaches often struggle to capture fine-grained motion details, resulting in vague or semantically inconsistent captions. In this work, we introduce the Motion-Augmented Caption Model (M-ACM), a novel generative framework that enhances caption quality by incorporating motion-aware decoding. At its core, M-ACM leverages motion representations derived from human mesh recovery to explicitly highlight human body dynamics, thereby reducing hallucinations and improving both semantic fidelity and spatial alignment in the generated captions. To support research in this area, we present the Human Motion Insight (HMI) Dataset, comprising 115K video-description pairs focused on human movement, along with HMI-Bench, a dedicated benchmark for evaluating motion-focused video captioning. Experimental results demonstrate that M-ACM significantly outperforms previous methods in accurately describing complex human motions and subtle temporal variations, setting a new standard for motion-centric video captioning.

