---
layout: default
title: Towards Fine-Grained Human Motion Video Captioning
---

# Towards Fine-Grained Human Motion Video Captioning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.24767" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.24767v1</a>
  <a href="https://arxiv.org/pdf/2510.24767.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.24767v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.24767v1', 'Towards Fine-Grained Human Motion Video Captioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guorui Song, Guocun Wang, Zhe Huang, Jing Lin, Xuefei Zhe, Jian Li, Haoqian Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¿åŠ¨å¢å¼ºçš„å­—å¹•æ¨¡å‹(M-ACM)ï¼Œç”¨äºç”Ÿæˆç»†ç²’åº¦çš„äººä½“è¿åŠ¨è§†é¢‘æè¿°ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `è§†é¢‘å­—å¹•` `äººä½“è¿åŠ¨` `è¿åŠ¨è¡¨å¾` `äººä½“ç½‘æ ¼æ¢å¤` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å­—å¹•æ¨¡å‹éš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„äººä½“è¿åŠ¨ç»†èŠ‚ï¼Œå¯¼è‡´ç”Ÿæˆçš„æè¿°æ¨¡ç³Šä¸”è¯­ä¹‰ä¸ä¸€è‡´ã€‚
2. M-ACMé€šè¿‡æ•´åˆä»äººä½“ç½‘æ ¼æ¢å¤ä¸­æå–çš„è¿åŠ¨è¡¨å¾ï¼Œæ˜¾å¼åœ°å¢å¼ºæ¨¡å‹å¯¹äººä½“åŠ¨æ€çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒM-ACMåœ¨æè¿°å¤æ‚äººä½“è¿åŠ¨å’Œç»†å¾®æ—¶é—´å˜åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘å­—å¹•æ¨¡å‹ç”Ÿæˆå‡†ç¡®çš„äººä½“åŠ¨ä½œæè¿°ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„è¿åŠ¨ç»†èŠ‚ï¼Œå¯¼è‡´æ¨¡ç³Šæˆ–è¯­ä¹‰ä¸ä¸€è‡´çš„å­—å¹•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ¡†æ¶â€”â€”è¿åŠ¨å¢å¼ºçš„å­—å¹•æ¨¡å‹(M-ACM)ï¼Œé€šè¿‡ç»“åˆè¿åŠ¨æ„ŸçŸ¥çš„è§£ç æ¥æé«˜å­—å¹•è´¨é‡ã€‚M-ACMåˆ©ç”¨ä»äººä½“ç½‘æ ¼æ¢å¤ä¸­æå–çš„è¿åŠ¨è¡¨å¾ï¼Œæ˜¾å¼åœ°çªå‡ºäººä½“åŠ¨æ€ï¼Œä»è€Œå‡å°‘å¹»è§‰ï¼Œå¹¶æé«˜ç”Ÿæˆå­—å¹•çš„è¯­ä¹‰ä¿çœŸåº¦å’Œç©ºé—´å¯¹é½ã€‚ä¸ºäº†æ”¯æŒè¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†äººä½“è¿åŠ¨æ´å¯Ÿ(HMI)æ•°æ®é›†ï¼ŒåŒ…å«115Kä¸ªä¸“æ³¨äºäººä½“è¿åŠ¨çš„è§†é¢‘-æè¿°å¯¹ï¼Œä»¥åŠHMI-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°è¿åŠ¨èšç„¦çš„è§†é¢‘å­—å¹•çš„ä¸“ç”¨åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM-ACMåœ¨å‡†ç¡®æè¿°å¤æ‚çš„äººä½“è¿åŠ¨å’Œç»†å¾®çš„æ—¶é—´å˜åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œä¸ºä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è§†é¢‘å­—å¹•è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘å­—å¹•æ¨¡å‹åœ¨æè¿°äººä½“è¿åŠ¨è§†é¢‘æ—¶ï¼Œéš¾ä»¥æ•æ‰åˆ°ç»†ç²’åº¦çš„è¿åŠ¨ä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆçš„å­—å¹•ä¸å¤Ÿå‡†ç¡®ï¼Œç”šè‡³å‡ºç°è¯­ä¹‰é”™è¯¯ã€‚è¿™äº›æ¨¡å‹å¾€å¾€å¿½ç•¥äº†äººä½“è¿åŠ¨çš„åŠ¨æ€ç‰¹æ€§ï¼Œæ— æ³•å¾ˆå¥½åœ°ç†è§£å’Œæè¿°åŠ¨ä½œçš„ç»†èŠ‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šM-ACMçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥è¿åŠ¨ä¿¡æ¯æ¥å¢å¼ºè§†é¢‘å­—å¹•æ¨¡å‹å¯¹äººä½“è¿åŠ¨çš„ç†è§£ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨äººä½“ç½‘æ ¼æ¢å¤æŠ€æœ¯æå–è§†é¢‘ä¸­çš„äººä½“è¿åŠ¨è¡¨å¾ï¼Œå¹¶å°†è¿™äº›è¡¨å¾èå…¥åˆ°è§£ç è¿‡ç¨‹ä¸­ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®ã€æ›´ç»†è‡´çš„å­—å¹•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šM-ACMçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘ç¼–ç å™¨ï¼šç”¨äºæå–è§†é¢‘çš„è§†è§‰ç‰¹å¾ï¼›2) äººä½“ç½‘æ ¼æ¢å¤æ¨¡å—ï¼šç”¨äºä»è§†é¢‘ä¸­æå–äººä½“è¿åŠ¨è¡¨å¾ï¼›3) è¿åŠ¨å¢å¼ºçš„è§£ç å™¨ï¼šå°†è§†è§‰ç‰¹å¾å’Œè¿åŠ¨è¡¨å¾ç»“åˆèµ·æ¥ï¼Œç”Ÿæˆè§†é¢‘å­—å¹•ã€‚è§£ç å™¨æ˜¯M-ACMçš„æ ¸å¿ƒï¼Œå®ƒåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å°†è¿åŠ¨ä¿¡æ¯èå…¥åˆ°å­—å¹•ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šM-ACMçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è¿åŠ¨å¢å¼ºçš„è§£ç å™¨ã€‚è¯¥è§£ç å™¨èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨äººä½“è¿åŠ¨è¡¨å¾æ¥æŒ‡å¯¼å­—å¹•ç”Ÿæˆï¼Œä»è€Œæé«˜å­—å¹•çš„å‡†ç¡®æ€§å’Œç»†èŠ‚ç¨‹åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒM-ACMèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰äººä½“è¿åŠ¨çš„åŠ¨æ€ç‰¹æ€§ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆè§†é¢‘å†…å®¹çš„å­—å¹•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨äººä½“ç½‘æ ¼æ¢å¤æ¨¡å—ä¸­ï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ç°æœ‰çš„3Däººä½“å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œä¾‹å¦‚SMPLifyæˆ–HMRã€‚è¿åŠ¨è¡¨å¾å¯èƒ½åŒ…æ‹¬äººä½“å…³èŠ‚çš„ä½ç½®ã€é€Ÿåº¦å’ŒåŠ é€Ÿåº¦ç­‰ä¿¡æ¯ã€‚åœ¨è§£ç å™¨ä¸­ï¼Œè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†LSTMæˆ–Transformerç­‰åºåˆ—ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶æ¥å…³æ³¨ä¸å½“å‰ç”Ÿæˆè¯ç›¸å…³çš„è¿åŠ¨ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬äº¤å‰ç†µæŸå¤±å’Œä¸€äº›é¢å¤–çš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´æµç•…çš„å­—å¹•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

M-ACMåœ¨HMI-Benchæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¡¨æ˜å…¶åœ¨è¿åŠ¨èšç„¦çš„è§†é¢‘å­—å¹•ä»»åŠ¡ä¸­å…·æœ‰ä¼˜è¶Šæ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒM-ACMåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è§†é¢‘å­—å¹•æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨æè¿°å¤æ‚äººä½“è¿åŠ¨å’Œç»†å¾®æ—¶é—´å˜åŒ–æ–¹é¢è¡¨ç°çªå‡ºã€‚è¿™äº›å®éªŒç»“æœéªŒè¯äº†M-ACMé€šè¿‡è¿åŠ¨å¢å¼ºè§£ç æ¥æé«˜å­—å¹•è´¨é‡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç›‘æ§ã€äººæœºäº¤äº’ã€è¿åŠ¨åˆ†æã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå¯¹å¼‚å¸¸è¡Œä¸ºçš„æè¿°ï¼›åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£äººç±»çš„åŠ¨ä½œæ„å›¾ï¼›åœ¨è¿åŠ¨åˆ†æä¸­ï¼Œå¯ä»¥æä¾›æ›´è¯¦ç»†çš„è¿åŠ¨æŠ¥å‘Šã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æå‡è§†é¢‘ç†è§£å’Œäººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generating accurate descriptions of human actions in videos remains a challenging task for video captioning models. Existing approaches often struggle to capture fine-grained motion details, resulting in vague or semantically inconsistent captions. In this work, we introduce the Motion-Augmented Caption Model (M-ACM), a novel generative framework that enhances caption quality by incorporating motion-aware decoding. At its core, M-ACM leverages motion representations derived from human mesh recovery to explicitly highlight human body dynamics, thereby reducing hallucinations and improving both semantic fidelity and spatial alignment in the generated captions. To support research in this area, we present the Human Motion Insight (HMI) Dataset, comprising 115K video-description pairs focused on human movement, along with HMI-Bench, a dedicated benchmark for evaluating motion-focused video captioning. Experimental results demonstrate that M-ACM significantly outperforms previous methods in accurately describing complex human motions and subtle temporal variations, setting a new standard for motion-centric video captioning.

