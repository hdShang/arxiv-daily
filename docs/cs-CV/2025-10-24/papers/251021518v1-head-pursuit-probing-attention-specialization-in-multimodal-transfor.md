---
layout: default
title: Head Pursuit: Probing Attention Specialization in Multimodal Transformers
---

# Head Pursuit: Probing Attention Specialization in Multimodal Transformers

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21518" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21518v1</a>
  <a href="https://arxiv.org/pdf/2510.21518.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21518v1" onclick="toggleFavorite(this, '2510.21518v1', 'Head Pursuit: Probing Attention Specialization in Multimodal Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lorenzo Basile, Valentino Maiorca, Diego Doimo, Francesco Locatello, Alberto Cazzaniga

**åˆ†ç±»**: cs.CV, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

**å¤‡æ³¨**: Accepted at NeurIPS 2025 (spotlight)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºä¿¡å·å¤„ç†çš„æ³¨æ„åŠ›å¤´åˆ†ææ–¹æ³•ï¼Œç”¨äºç†è§£å’Œç¼–è¾‘å¤šæ¨¡æ€Transformeræ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ³¨æ„åŠ›æœºåˆ¶` `å¯è§£é‡Šæ€§` `Transformeræ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `ä¿¡å·å¤„ç†` `æ¨¡å‹ç¼–è¾‘` `æ¦‚å¿µç‰¹åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Transformeræ¨¡å‹å†…éƒ¨æœºåˆ¶å¤æ‚ï¼Œå¯¹æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½ç†è§£ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¯æ§æ€§ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§åŸºäºä¿¡å·å¤„ç†çš„æ³¨æ„åŠ›å¤´åˆ†ææ–¹æ³•ï¼Œé€šè¿‡æ¢æµ‹ä¸­é—´æ¿€æ´»å€¼æ¥è¯†åˆ«ç‰¹å®šè¯­ä¹‰æˆ–è§†è§‰å±æ€§ç›¸å…³çš„æ³¨æ„åŠ›å¤´ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå°‘é‡å…³é”®æ³¨æ„åŠ›å¤´çš„ç¼–è¾‘å³å¯æœ‰æ•ˆæ§åˆ¶æ¨¡å‹è¾“å‡ºï¼Œå¹¶åœ¨å¤šç§è¯­è¨€å’Œè§†è§‰-è¯­è¨€ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæ³¨æ„åŠ›å¤´å¦‚ä½•åœ¨è¯­ä¹‰æˆ–è§†è§‰å±æ€§ä¸Šè¿›è¡Œç‰¹åŒ–ã€‚åŸºäºå·²æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œä½œè€…ä»ä¿¡å·å¤„ç†çš„è§’åº¦é‡æ–°å®¡è§†äº†ä½¿ç”¨æœ€ç»ˆè§£ç å±‚æ¢æµ‹ä¸­é—´æ¿€æ´»å€¼çš„åšæ³•ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿä»¥è§„èŒƒçš„æ–¹å¼åˆ†æå¤šä¸ªæ ·æœ¬ï¼Œå¹¶æ ¹æ®æ³¨æ„åŠ›å¤´ä¸ç›®æ ‡æ¦‚å¿µçš„ç›¸å…³æ€§å¯¹å…¶è¿›è¡Œæ’åºã€‚ç»“æœè¡¨æ˜ï¼Œå•æ¨¡æ€å’Œå¤šæ¨¡æ€Transformerä¸­ï¼Œæ³¨æ„åŠ›å¤´å±‚é¢ä¸Šå­˜åœ¨ä¸€è‡´çš„ç‰¹åŒ–æ¨¡å¼ã€‚é€šè¿‡è¯¥æ–¹æ³•é€‰æ‹©çš„å°‘é‡ï¼ˆä½è‡³1%ï¼‰æ³¨æ„åŠ›å¤´ç¼–è¾‘ï¼Œå¯ä»¥å¯é åœ°æŠ‘åˆ¶æˆ–å¢å¼ºæ¨¡å‹è¾“å‡ºä¸­çš„ç›®æ ‡æ¦‚å¿µã€‚è¯¥æ–¹æ³•åœ¨é—®ç­”ã€æ¯’æ€§ç¼“è§£ç­‰è¯­è¨€ä»»åŠ¡ï¼Œä»¥åŠå›¾åƒåˆ†ç±»ã€å›¾åƒæè¿°ç­‰è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†æ³¨æ„åŠ›å±‚ä¸­å¯è§£é‡Šå’Œå¯æ§çš„ç»“æ„ï¼Œä¸ºç†è§£å’Œç¼–è¾‘å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹æä¾›äº†ç®€å•å·¥å…·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Transformeræ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å†…éƒ¨æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé»‘ç›’ã€‚ç†è§£æ³¨æ„åŠ›å¤´çš„å…·ä½“ä½œç”¨ï¼Œæœ‰åŠ©äºæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥å¯¹æ³¨æ„åŠ›å¤´è¿›è¡Œç»†ç²’åº¦çš„åˆ†æï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«ä¸ç‰¹å®šæ¦‚å¿µç›¸å…³çš„æ³¨æ„åŠ›å¤´ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ³¨æ„åŠ›å¤´çš„æ¿€æ´»å€¼è§†ä¸ºä¿¡å·ï¼Œåˆ©ç”¨ä¿¡å·å¤„ç†çš„æŠ€æœ¯æ¥åˆ†æå…¶ä¸ç‰¹å®šæ¦‚å¿µçš„ç›¸å…³æ€§ã€‚é€šè¿‡æ¢æµ‹ä¸­é—´æ¿€æ´»å€¼ï¼Œå¹¶ç»“åˆæœ€ç»ˆè§£ç å±‚çš„ä¿¡æ¯ï¼Œå¯ä»¥é‡åŒ–æ¯ä¸ªæ³¨æ„åŠ›å¤´å¯¹ç›®æ ‡æ¦‚å¿µçš„è´¡çŒ®ç¨‹åº¦ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«å‡ºè´Ÿè´£ç‰¹å®šè¯­ä¹‰æˆ–è§†è§‰å±æ€§çš„æ³¨æ„åŠ›å¤´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é€‰æ‹©ç›®æ ‡æ¦‚å¿µï¼ˆä¾‹å¦‚ï¼ŒæŸä¸ªç‰¹å®šçš„è¯è¯­æˆ–å›¾åƒå±æ€§ï¼‰ï¼›2) ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå¤šä¸ªæ ·æœ¬ï¼›3) æå–æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ä¸­é—´æ¿€æ´»å€¼ï¼›4) ä½¿ç”¨æœ€ç»ˆè§£ç å±‚çš„ä¿¡æ¯ä½œä¸ºæ¢é’ˆï¼Œåˆ†ææ¿€æ´»å€¼ä¸ç›®æ ‡æ¦‚å¿µçš„ç›¸å…³æ€§ï¼›5) æ ¹æ®ç›¸å…³æ€§å¯¹æ³¨æ„åŠ›å¤´è¿›è¡Œæ’åºã€‚æ•´ä¸ªæ¡†æ¶åˆ©ç”¨ä¿¡å·å¤„ç†çš„è§†è§’ï¼Œå°†æ³¨æ„åŠ›å¤´çš„æ¿€æ´»å€¼ä¸ç›®æ ‡æ¦‚å¿µè”ç³»èµ·æ¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†ä¿¡å·å¤„ç†çš„æ€æƒ³å¼•å…¥åˆ°Transformeræ¨¡å‹çš„å¯è§£é‡Šæ€§åˆ†æä¸­ã€‚é€šè¿‡å°†æ³¨æ„åŠ›å¤´çš„æ¿€æ´»å€¼è§†ä¸ºä¿¡å·ï¼Œå¯ä»¥åˆ©ç”¨ä¿¡å·å¤„ç†çš„å·¥å…·æ¥åˆ†æå…¶ä¸ç›®æ ‡æ¦‚å¿µçš„ç›¸å…³æ€§ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯†åˆ«å‡ºè´Ÿè´£ç‰¹å®šè¯­ä¹‰æˆ–è§†è§‰å±æ€§çš„æ³¨æ„åŠ›å¤´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æä¾›äº†ä¸€ç§é‡åŒ–çš„æ–¹å¼æ¥è¯„ä¼°æ³¨æ„åŠ›å¤´çš„é‡è¦æ€§ï¼Œä¸ºæ¨¡å‹ç¼–è¾‘æä¾›äº†ä¾æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸Šï¼Œè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†äº’ä¿¡æ¯ã€ç›¸å…³ç³»æ•°ç­‰ä¿¡å·å¤„ç†ä¸­å¸¸ç”¨çš„æŒ‡æ ‡æ¥è¡¡é‡æ³¨æ„åŠ›å¤´æ¿€æ´»å€¼ä¸ç›®æ ‡æ¦‚å¿µä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¯èƒ½è¿˜è®¾è®¡äº†ä¸€ç§æŸå¤±å‡½æ•°ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶æ³¨æ„åŠ›å¤´èƒ½å¤Ÿæ›´å¥½åœ°ç‰¹åŒ–äºç‰¹å®šçš„æ¦‚å¿µã€‚å…·ä½“çš„ç½‘ç»œç»“æ„ç»†èŠ‚å’Œå‚æ•°è®¾ç½®éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡è¯¥æ–¹æ³•é€‰æ‹©çš„å°‘é‡ï¼ˆä½è‡³1%ï¼‰æ³¨æ„åŠ›å¤´ç¼–è¾‘ï¼Œå¯ä»¥å¯é åœ°æŠ‘åˆ¶æˆ–å¢å¼ºæ¨¡å‹è¾“å‡ºä¸­çš„ç›®æ ‡æ¦‚å¿µã€‚è¯¥æ–¹æ³•åœ¨é—®ç­”ã€æ¯’æ€§ç¼“è§£ç­‰è¯­è¨€ä»»åŠ¡ï¼Œä»¥åŠå›¾åƒåˆ†ç±»ã€å›¾åƒæè¿°ç­‰è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡å¤§å‹ç”Ÿæˆæ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºç¼“è§£æ¨¡å‹ç”Ÿæˆå†…å®¹ä¸­çš„æ¯’æ€§ï¼Œå¢å¼ºç‰¹å®šæ¦‚å¿µçš„ç”Ÿæˆï¼Œæˆ–ç”¨äºè°ƒè¯•å’Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºåˆ†æä¸åŒæ¨¡æ€ä¿¡æ¯åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„äº¤äº’æ–¹å¼ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„æ”¹è¿›æä¾›æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models.

