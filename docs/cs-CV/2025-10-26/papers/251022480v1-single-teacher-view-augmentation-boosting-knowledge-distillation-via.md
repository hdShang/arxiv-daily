---
layout: default
title: "Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity"
---

# Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22480" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22480v1</a>
  <a href="https://arxiv.org/pdf/2510.22480.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22480v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22480v1', 'Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seonghoon Yu, Dongjun Nam, Dina Katabi, Jeany Son

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-26

**å¤‡æ³¨**: Accepted to NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå•æ•™å¸ˆè§†è§’å¢å¼ºçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡è§’åº¦å¤šæ ·æ€§æå‡å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çŸ¥è¯†è’¸é¦` `æ¨¡å‹å‹ç¼©` `è§†è§’å¢å¼º` `è§’åº¦å¤šæ ·æ€§` `å•æ•™å¸ˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•ä¾èµ–å¤šä¸ªæ•™å¸ˆç½‘ç»œä»¥è·å¾—å¤šæ ·æ€§ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. è¯¥è®ºæ–‡æå‡ºå•æ•™å¸ˆè§†è§’å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡åœ¨å•ä¸ªæ•™å¸ˆæ¨¡å‹ä¸Šé™„åŠ å¤šä¸ªåˆ†æ”¯æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„è§†è§’ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§é…ç½®ä¸‹ä¼˜äºç°æœ‰çŸ¥è¯†å¢å¼ºæ–¹æ³•ï¼Œä¸”èƒ½ä¸å…¶ä»–KDæ¡†æ¶å…¼å®¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çŸ¥è¯†è’¸é¦(KD)æ—¨åœ¨é€šè¿‡ä»å¤§å‹ã€é«˜å®¹é‡çš„æ•™å¸ˆæ¨¡å‹ä¸­è½¬ç§»çŸ¥è¯†æ¥è®­ç»ƒè½»é‡çº§çš„å­¦ç”Ÿæ¨¡å‹ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨å¤šæ ·åŒ–çš„æ•™å¸ˆè§†è§’å¯ä»¥æ˜¾è‘—æé«˜è’¸é¦æ€§èƒ½ï¼›ç„¶è€Œï¼Œå®ç°è¿™ç§å¤šæ ·æ€§é€šå¸¸éœ€è¦å¤šä¸ªæ•™å¸ˆç½‘ç»œï¼Œå¯¼è‡´é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ã€å…·æœ‰æˆæœ¬æ•ˆç›Šçš„çŸ¥è¯†å¢å¼ºæ–¹æ³•ï¼Œç”¨äºKDï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†å¤šä¸ªåˆ†æ”¯é™„åŠ åˆ°å•ä¸ªæ•™å¸ˆæ¥ç”Ÿæˆå¤šæ ·åŒ–çš„å¤šè§†è§’ã€‚ä¸ºäº†ç¡®ä¿å¤šè§†è§’ä¹‹é—´æœ‰æ„ä¹‰çš„è¯­ä¹‰å˜åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªè§’åº¦å¤šæ ·æ€§ç›®æ ‡ï¼š1)çº¦æŸçš„è§†è§’é—´å¤šæ ·æ€§æŸå¤±ï¼Œå®ƒæœ€å¤§åŒ–å¢å¼ºè§†è§’ä¹‹é—´çš„è§’åº¦ï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹æ•™å¸ˆè¾“å‡ºçš„æ¥è¿‘åº¦ï¼›2)è§†è§’å†…å¤šæ ·æ€§æŸå¤±ï¼Œå®ƒé¼“åŠ±è§†è§’å›´ç»•åŸå§‹è¾“å‡ºå‡åŒ€åˆ†å¸ƒã€‚æ¥è‡ªè¿™äº›è§’åº¦å¤šæ ·åŒ–è§†è§’çš„é›†æˆçŸ¥è¯†ï¼Œè¿åŒåŸå§‹æ•™å¸ˆï¼Œè¢«æç‚¼åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä»ç†è®ºä¸Šè¯æ˜ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å¢åŠ äº†é›†æˆæˆå‘˜ä¹‹é—´çš„å¤šæ ·æ€§ï¼Œä»è€Œé™ä½äº†é›†æˆé¢„æœŸæŸå¤±çš„ä¸Šé™ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„è’¸é¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„é…ç½®ä¸­è¶…è¶Šäº†ç°æœ‰çš„çŸ¥è¯†å¢å¼ºæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥å³æ’å³ç”¨åœ°ä¸å…¶ä»–KDæ¡†æ¶å…¼å®¹ï¼Œä»è€Œåœ¨æ³›åŒ–æ€§èƒ½æ–¹é¢æä¾›ä¸€è‡´çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŸ¥è¯†è’¸é¦æ—¨åœ¨å°†å¤§å‹æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤šä¸ªæ•™å¸ˆæ¨¡å‹æ¥è·å¾—çŸ¥è¯†çš„å¤šæ ·æ€§ï¼Œè¿™æ˜¾è‘—å¢åŠ äº†è®¡ç®—æˆæœ¬å’Œè®­ç»ƒè´Ÿæ‹…ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæå‡çŸ¥è¯†è’¸é¦çš„æ•ˆç‡å’Œæ€§èƒ½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å•ä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œé€šè¿‡è§†è§’å¢å¼ºçš„æ–¹å¼æ¥æ¨¡æ‹Ÿå¤šä¸ªæ•™å¸ˆæ¨¡å‹æä¾›çš„å¤šæ ·æ€§çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åœ¨æ•™å¸ˆæ¨¡å‹ä¸Šæ·»åŠ å¤šä¸ªåˆ†æ”¯ï¼Œæ¯ä¸ªåˆ†æ”¯ä»£è¡¨ä¸€ä¸ªä¸åŒçš„è§†è§’ï¼Œä»è€Œç”Ÿæˆå¤šä¸ªä¸åŒçš„è¾“å‡ºã€‚è¿™äº›ä¸åŒçš„è¾“å‡ºå¯ä»¥è¢«è§†ä¸ºæ¥è‡ªä¸åŒæ•™å¸ˆçš„çŸ¥è¯†ï¼Œç„¶åç”¨äºæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å•ä¸ªæ•™å¸ˆæ¨¡å‹ï¼›2) å¤šä¸ªè§†è§’åˆ†æ”¯ï¼Œé™„åŠ åœ¨æ•™å¸ˆæ¨¡å‹ä¸Šï¼›3) è§†è§’é—´å¤šæ ·æ€§æŸå¤±ï¼Œç”¨äºçº¦æŸä¸åŒè§†è§’ä¹‹é—´çš„å·®å¼‚ï¼›4) è§†è§’å†…å¤šæ ·æ€§æŸå¤±ï¼Œç”¨äºä¿è¯è§†è§’åˆ†å¸ƒçš„å‡åŒ€æ€§ï¼›5) çŸ¥è¯†è’¸é¦æŸå¤±ï¼Œç”¨äºå°†æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨è§†è§’å¢å¼ºæ¨¡å—ç”Ÿæˆå¤šä¸ªè§†è§’ï¼Œç„¶ååˆ©ç”¨å¤šæ ·æ€§æŸå¤±æ¥çº¦æŸè¿™äº›è§†è§’çš„å·®å¼‚ï¼Œæœ€ååˆ©ç”¨çŸ¥è¯†è’¸é¦æŸå¤±å°†æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†è§’åº¦å¤šæ ·æ€§æŸå¤±ï¼ŒåŒ…æ‹¬è§†è§’é—´å¤šæ ·æ€§æŸå¤±å’Œè§†è§’å†…å¤šæ ·æ€§æŸå¤±ã€‚è§†è§’é—´å¤šæ ·æ€§æŸå¤±æ—¨åœ¨æœ€å¤§åŒ–ä¸åŒè§†è§’ä¹‹é—´çš„è§’åº¦ï¼Œä»è€Œä¿è¯è§†è§’ä¹‹é—´çš„å·®å¼‚æ€§ã€‚è§†è§’å†…å¤šæ ·æ€§æŸå¤±æ—¨åœ¨ä¿è¯è§†è§’å›´ç»•åŸå§‹è¾“å‡ºå‡åŒ€åˆ†å¸ƒï¼Œä»è€Œé¿å…è§†è§’è¿‡äºé›†ä¸­ã€‚è¿™ä¸¤ç§æŸå¤±çš„ç»“åˆå¯ä»¥æœ‰æ•ˆåœ°æé«˜è§†è§’çš„å¤šæ ·æ€§ï¼Œä»è€Œæå‡çŸ¥è¯†è’¸é¦çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦å¤šä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡è§†è§’å¢å¼ºçš„æ–¹å¼æ¥æ¨¡æ‹Ÿå¤šä¸ªæ•™å¸ˆæ¨¡å‹æä¾›çš„å¤šæ ·æ€§çŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šè§†è§’é—´å¤šæ ·æ€§æŸå¤±é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¡¡é‡ä¸åŒè§†è§’ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æœ€å¤§åŒ–è§†è§’ä¹‹é—´çš„è§’åº¦ã€‚è§†è§’å†…å¤šæ ·æ€§æŸå¤±é‡‡ç”¨KLæ•£åº¦æ¥è¡¡é‡è§†è§’åˆ†å¸ƒä¸å‡åŒ€åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶æœ€å°åŒ–è¿™ç§å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼Œconstrained inter-angle diversify loss é€šè¿‡æœ€å¤§åŒ–ä¸åŒè§†è§’è¾“å‡ºå‘é‡ä¹‹é—´çš„å¤¹è§’æ¥å®ç°ï¼ŒåŒæ—¶åŠ å…¥çº¦æŸé¡¹ï¼Œä¿è¯å¢å¼ºåçš„è§†è§’ä¸ä¼šåç¦»åŸå§‹æ•™å¸ˆè¾“å‡ºå¤ªè¿œã€‚intra-angle diversify loss åˆ™é€šè¿‡é¼“åŠ±å„ä¸ªè§†è§’åœ¨åŸå§‹è¾“å‡ºå‘¨å›´å‡åŒ€åˆ†å¸ƒæ¥å®ç°ï¼Œé¿å…æ‰€æœ‰è§†è§’éƒ½é›†ä¸­åœ¨åŒä¸€æ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ResNet-50ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼ŒResNet-18ä½œä¸ºå­¦ç”Ÿæ¨¡å‹ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºåŸºçº¿æ–¹æ³•æå‡äº†è¶…è¿‡2%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜ä¸å…¶ä»–çŸ¥è¯†è’¸é¦æ¡†æ¶å…¼å®¹ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡å…¶æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ¨¡å‹å‹ç¼©ã€è¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨è®¾å¤‡ç­‰èµ„æºå—é™çš„åœºæ™¯ã€‚é€šè¿‡å•æ•™å¸ˆè§†è§’å¢å¼ºçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„å‰æä¸‹ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²å’Œåº”ç”¨ã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢è¯¥æ–¹æ³•åœ¨ä¸åŒç±»å‹æ¨¡å‹å’Œä»»åŠ¡ä¸Šçš„é€‚ç”¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Knowledge Distillation (KD) aims to train a lightweight student model by transferring knowledge from a large, high-capacity teacher. Recent studies have shown that leveraging diverse teacher perspectives can significantly improve distillation performance; however, achieving such diversity typically requires multiple teacher networks, leading to high computational costs. In this work, we propose a novel cost-efficient knowledge augmentation method for KD that generates diverse multi-views by attaching multiple branches to a single teacher. To ensure meaningful semantic variation across multi-views, we introduce two angular diversity objectives: 1) constrained inter-angle diversify loss, which maximizes angles between augmented views while preserving proximity to the original teacher output, and 2) intra-angle diversify loss, which encourages an even distribution of views around the original output. The ensembled knowledge from these angularly diverse views, along with the original teacher, is distilled into the student. We further theoretically demonstrate that our objectives increase the diversity among ensemble members and thereby reduce the upper bound of the ensemble's expected loss, leading to more effective distillation. Experimental results show that our method surpasses an existing knowledge augmentation method across diverse configurations. Moreover, the proposed method is compatible with other KD frameworks in a plug-and-play fashion, providing consistent improvements in generalization performance.

