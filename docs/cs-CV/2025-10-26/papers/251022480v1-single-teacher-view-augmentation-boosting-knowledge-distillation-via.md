---
layout: default
title: Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity
---

# Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.22480" target="_blank" class="toolbar-btn">arXiv: 2510.22480v1</a>
    <a href="https://arxiv.org/pdf/2510.22480.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22480v1" 
            onclick="toggleFavorite(this, '2510.22480v1', 'Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Seonghoon Yu, Dongjun Nam, Dina Katabi, Jeany Son

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-26

**Â§áÊ≥®**: Accepted to NeurIPS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂçïÊïôÂ∏àËßÜËßíÂ¢ûÂº∫ÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÔºåÈÄöËøáËßíÂ∫¶Â§öÊ†∑ÊÄßÊèêÂçáÂ≠¶ÁîüÊ®°ÂûãÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Áü•ËØÜËí∏È¶è` `Ê®°ÂûãÂéãÁº©` `ËßÜËßíÂ¢ûÂº∫` `ËßíÂ∫¶Â§öÊ†∑ÊÄß` `ÂçïÊïôÂ∏àÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁü•ËØÜËí∏È¶èÊñπÊ≥ï‰æùËµñÂ§ö‰∏™ÊïôÂ∏àÁΩëÁªú‰ª•Ëé∑ÂæóÂ§öÊ†∑ÊÄßÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫ÂçïÊïôÂ∏àËßÜËßíÂ¢ûÂº∫ÊñπÊ≥ïÔºåÈÄöËøáÂú®Âçï‰∏™ÊïôÂ∏àÊ®°Âûã‰∏äÈôÑÂä†Â§ö‰∏™ÂàÜÊîØÊù•ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËßÜËßí„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§öÁßçÈÖçÁΩÆ‰∏ã‰ºò‰∫éÁé∞ÊúâÁü•ËØÜÂ¢ûÂº∫ÊñπÊ≥ïÔºå‰∏îËÉΩ‰∏éÂÖ∂‰ªñKDÊ°ÜÊû∂ÂÖºÂÆπ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áü•ËØÜËí∏È¶è(KD)Êó®Âú®ÈÄöËøá‰ªéÂ§ßÂûã„ÄÅÈ´òÂÆπÈáèÁöÑÊïôÂ∏àÊ®°Âûã‰∏≠ËΩ¨ÁßªÁü•ËØÜÊù•ËÆ≠ÁªÉËΩªÈáèÁ∫ßÁöÑÂ≠¶ÁîüÊ®°Âûã„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂà©Áî®Â§öÊ†∑ÂåñÁöÑÊïôÂ∏àËßÜËßíÂèØ‰ª•ÊòæËëóÊèêÈ´òËí∏È¶èÊÄßËÉΩÔºõÁÑ∂ËÄåÔºåÂÆûÁé∞ËøôÁßçÂ§öÊ†∑ÊÄßÈÄöÂ∏∏ÈúÄË¶ÅÂ§ö‰∏™ÊïôÂ∏àÁΩëÁªúÔºåÂØºËá¥È´òÊòÇÁöÑËÆ°ÁÆóÊàêÊú¨„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ„ÄÅÂÖ∑ÊúâÊàêÊú¨ÊïàÁõäÁöÑÁü•ËØÜÂ¢ûÂº∫ÊñπÊ≥ïÔºåÁî®‰∫éKDÔºåËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÂ§ö‰∏™ÂàÜÊîØÈôÑÂä†Âà∞Âçï‰∏™ÊïôÂ∏àÊù•ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÂ§öËßÜËßí„ÄÇ‰∏∫‰∫ÜÁ°Æ‰øùÂ§öËßÜËßí‰πãÈó¥ÊúâÊÑè‰πâÁöÑËØ≠‰πâÂèòÂåñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏§‰∏™ËßíÂ∫¶Â§öÊ†∑ÊÄßÁõÆÊ†áÔºö1)Á∫¶ÊùüÁöÑËßÜËßíÈó¥Â§öÊ†∑ÊÄßÊçüÂ§±ÔºåÂÆÉÊúÄÂ§ßÂåñÂ¢ûÂº∫ËßÜËßí‰πãÈó¥ÁöÑËßíÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∏éÂéüÂßãÊïôÂ∏àËæìÂá∫ÁöÑÊé•ËøëÂ∫¶Ôºõ2)ËßÜËßíÂÜÖÂ§öÊ†∑ÊÄßÊçüÂ§±ÔºåÂÆÉÈºìÂä±ËßÜËßíÂõ¥ÁªïÂéüÂßãËæìÂá∫ÂùáÂåÄÂàÜÂ∏É„ÄÇÊù•Ëá™Ëøô‰∫õËßíÂ∫¶Â§öÊ†∑ÂåñËßÜËßíÁöÑÈõÜÊàêÁü•ËØÜÔºåËøûÂêåÂéüÂßãÊïôÂ∏àÔºåË¢´ÊèêÁÇºÂà∞Â≠¶ÁîüÊ®°Âûã‰∏≠„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•‰ªéÁêÜËÆ∫‰∏äËØÅÊòéÔºåÊàë‰ª¨ÁöÑÁõÆÊ†áÂ¢ûÂä†‰∫ÜÈõÜÊàêÊàêÂëò‰πãÈó¥ÁöÑÂ§öÊ†∑ÊÄßÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜÈõÜÊàêÈ¢ÑÊúüÊçüÂ§±ÁöÑ‰∏äÈôêÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑËí∏È¶è„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®‰∏çÂêåÁöÑÈÖçÁΩÆ‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÁü•ËØÜÂ¢ûÂº∫ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂèØ‰ª•Âç≥ÊèíÂç≥Áî®Âú∞‰∏éÂÖ∂‰ªñKDÊ°ÜÊû∂ÂÖºÂÆπÔºå‰ªéËÄåÂú®Ê≥õÂåñÊÄßËÉΩÊñπÈù¢Êèê‰æõ‰∏ÄËá¥ÁöÑÊîπËøõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁü•ËØÜËí∏È¶èÊó®Âú®Â∞ÜÂ§ßÂûãÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â∞èÂûãÂ≠¶ÁîüÊ®°ÂûãÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ö‰∏™ÊïôÂ∏àÊ®°ÂûãÊù•Ëé∑ÂæóÁü•ËØÜÁöÑÂ§öÊ†∑ÊÄßÔºåËøôÊòæËëóÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨ÂíåËÆ≠ÁªÉË¥üÊãÖ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂú®ËÆ°ÁÆóËµÑÊ∫êÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèêÂçáÁü•ËØÜËí∏È¶èÁöÑÊïàÁéáÂíåÊÄßËÉΩÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËØ•ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âçï‰∏™ÊïôÂ∏àÊ®°ÂûãÔºåÈÄöËøáËßÜËßíÂ¢ûÂº∫ÁöÑÊñπÂºèÊù•Ê®°ÊãüÂ§ö‰∏™ÊïôÂ∏àÊ®°ÂûãÊèê‰æõÁöÑÂ§öÊ†∑ÊÄßÁü•ËØÜ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÂú®ÊïôÂ∏àÊ®°Âûã‰∏äÊ∑ªÂä†Â§ö‰∏™ÂàÜÊîØÔºåÊØè‰∏™ÂàÜÊîØ‰ª£Ë°®‰∏Ä‰∏™‰∏çÂêåÁöÑËßÜËßíÔºå‰ªéËÄåÁîüÊàêÂ§ö‰∏™‰∏çÂêåÁöÑËæìÂá∫„ÄÇËøô‰∫õ‰∏çÂêåÁöÑËæìÂá∫ÂèØ‰ª•Ë¢´ËßÜ‰∏∫Êù•Ëá™‰∏çÂêåÊïôÂ∏àÁöÑÁü•ËØÜÔºåÁÑ∂ÂêéÁî®‰∫éÊåáÂØºÂ≠¶ÁîüÊ®°ÂûãÁöÑËÆ≠ÁªÉ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ï‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) Âçï‰∏™ÊïôÂ∏àÊ®°ÂûãÔºõ2) Â§ö‰∏™ËßÜËßíÂàÜÊîØÔºåÈôÑÂä†Âú®ÊïôÂ∏àÊ®°Âûã‰∏äÔºõ3) ËßÜËßíÈó¥Â§öÊ†∑ÊÄßÊçüÂ§±ÔºåÁî®‰∫éÁ∫¶Êùü‰∏çÂêåËßÜËßí‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºõ4) ËßÜËßíÂÜÖÂ§öÊ†∑ÊÄßÊçüÂ§±ÔºåÁî®‰∫é‰øùËØÅËßÜËßíÂàÜÂ∏ÉÁöÑÂùáÂåÄÊÄßÔºõ5) Áü•ËØÜËí∏È¶èÊçüÂ§±ÔºåÁî®‰∫éÂ∞ÜÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â≠¶ÁîüÊ®°Âûã„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºåÈ¶ñÂÖàÂà©Áî®ËßÜËßíÂ¢ûÂº∫Ê®°ÂùóÁîüÊàêÂ§ö‰∏™ËßÜËßíÔºåÁÑ∂ÂêéÂà©Áî®Â§öÊ†∑ÊÄßÊçüÂ§±Êù•Á∫¶ÊùüËøô‰∫õËßÜËßíÁöÑÂ∑ÆÂºÇÔºåÊúÄÂêéÂà©Áî®Áü•ËØÜËí∏È¶èÊçüÂ§±Â∞ÜÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªÂà∞Â≠¶ÁîüÊ®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜËßíÂ∫¶Â§öÊ†∑ÊÄßÊçüÂ§±ÔºåÂåÖÊã¨ËßÜËßíÈó¥Â§öÊ†∑ÊÄßÊçüÂ§±ÂíåËßÜËßíÂÜÖÂ§öÊ†∑ÊÄßÊçüÂ§±„ÄÇËßÜËßíÈó¥Â§öÊ†∑ÊÄßÊçüÂ§±Êó®Âú®ÊúÄÂ§ßÂåñ‰∏çÂêåËßÜËßí‰πãÈó¥ÁöÑËßíÂ∫¶Ôºå‰ªéËÄå‰øùËØÅËßÜËßí‰πãÈó¥ÁöÑÂ∑ÆÂºÇÊÄß„ÄÇËßÜËßíÂÜÖÂ§öÊ†∑ÊÄßÊçüÂ§±Êó®Âú®‰øùËØÅËßÜËßíÂõ¥ÁªïÂéüÂßãËæìÂá∫ÂùáÂåÄÂàÜÂ∏ÉÔºå‰ªéËÄåÈÅøÂÖçËßÜËßíËøá‰∫éÈõÜ‰∏≠„ÄÇËøô‰∏§ÁßçÊçüÂ§±ÁöÑÁªìÂêàÂèØ‰ª•ÊúâÊïàÂú∞ÊèêÈ´òËßÜËßíÁöÑÂ§öÊ†∑ÊÄßÔºå‰ªéËÄåÊèêÂçáÁü•ËØÜËí∏È¶èÁöÑÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåËØ•ÊñπÊ≥ï‰∏çÈúÄË¶ÅÂ§ö‰∏™ÊïôÂ∏àÊ®°ÂûãÔºåËÄåÊòØÈÄöËøáËßÜËßíÂ¢ûÂº∫ÁöÑÊñπÂºèÊù•Ê®°ÊãüÂ§ö‰∏™ÊïôÂ∏àÊ®°ÂûãÊèê‰æõÁöÑÂ§öÊ†∑ÊÄßÁü•ËØÜ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËßÜËßíÈó¥Â§öÊ†∑ÊÄßÊçüÂ§±ÈááÁî®‰ΩôÂº¶Áõ∏‰ººÂ∫¶Êù•Ë°°Èáè‰∏çÂêåËßÜËßí‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÔºåÂπ∂ÊúÄÂ§ßÂåñËßÜËßí‰πãÈó¥ÁöÑËßíÂ∫¶„ÄÇËßÜËßíÂÜÖÂ§öÊ†∑ÊÄßÊçüÂ§±ÈááÁî®KLÊï£Â∫¶Êù•Ë°°ÈáèËßÜËßíÂàÜÂ∏É‰∏éÂùáÂåÄÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂπ∂ÊúÄÂ∞èÂåñËøôÁßçÂ∑ÆÂºÇ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºåconstrained inter-angle diversify loss ÈÄöËøáÊúÄÂ§ßÂåñ‰∏çÂêåËßÜËßíËæìÂá∫ÂêëÈáè‰πãÈó¥ÁöÑÂ§πËßíÊù•ÂÆûÁé∞ÔºåÂêåÊó∂Âä†ÂÖ•Á∫¶ÊùüÈ°πÔºå‰øùËØÅÂ¢ûÂº∫ÂêéÁöÑËßÜËßí‰∏ç‰ºöÂÅèÁ¶ªÂéüÂßãÊïôÂ∏àËæìÂá∫Â§™Ëøú„ÄÇintra-angle diversify loss ÂàôÈÄöËøáÈºìÂä±ÂêÑ‰∏™ËßÜËßíÂú®ÂéüÂßãËæìÂá∫Âë®Âõ¥ÂùáÂåÄÂàÜÂ∏ÉÊù•ÂÆûÁé∞ÔºåÈÅøÂÖçÊâÄÊúâËßÜËßíÈÉΩÈõÜ‰∏≠Âú®Âêå‰∏ÄÊñπÂêë„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíåÊ®°Âûã‰∏äÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®ImageNetÊï∞ÊçÆÈõÜ‰∏äÔºå‰ΩøÁî®ResNet-50‰Ωú‰∏∫ÊïôÂ∏àÊ®°ÂûãÔºåResNet-18‰Ωú‰∏∫Â≠¶ÁîüÊ®°ÂûãÔºåËØ•ÊñπÊ≥ïÁõ∏ÊØî‰∫éÂü∫Á∫øÊñπÊ≥ïÊèêÂçá‰∫ÜË∂ÖËøá2%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøò‰∏éÂÖ∂‰ªñÁü•ËØÜËí∏È¶èÊ°ÜÊû∂ÂÖºÂÆπÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÂçáÂÖ∂ÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊ®°ÂûãÂéãÁº©„ÄÅËæπÁºòËÆ°ÁÆó„ÄÅÁßªÂä®ËÆæÂ§áÁ≠âËµÑÊ∫êÂèóÈôêÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÂçïÊïôÂ∏àËßÜËßíÂ¢ûÂº∫ÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÂ¢ûÂä†ËÆ°ÁÆóÊàêÊú¨ÁöÑÂâçÊèê‰∏ãÔºåÊèêÂçáÂ≠¶ÁîüÊ®°ÂûãÁöÑÊÄßËÉΩÔºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑÊ®°ÂûãÈÉ®ÁΩ≤ÂíåÂ∫îÁî®„ÄÇÊú™Êù•ÂèØËøõ‰∏ÄÊ≠•Êé¢Á¥¢ËØ•ÊñπÊ≥ïÂú®‰∏çÂêåÁ±ªÂûãÊ®°ÂûãÂíå‰ªªÂä°‰∏äÁöÑÈÄÇÁî®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Knowledge Distillation (KD) aims to train a lightweight student model by transferring knowledge from a large, high-capacity teacher. Recent studies have shown that leveraging diverse teacher perspectives can significantly improve distillation performance; however, achieving such diversity typically requires multiple teacher networks, leading to high computational costs. In this work, we propose a novel cost-efficient knowledge augmentation method for KD that generates diverse multi-views by attaching multiple branches to a single teacher. To ensure meaningful semantic variation across multi-views, we introduce two angular diversity objectives: 1) constrained inter-angle diversify loss, which maximizes angles between augmented views while preserving proximity to the original teacher output, and 2) intra-angle diversify loss, which encourages an even distribution of views around the original output. The ensembled knowledge from these angularly diverse views, along with the original teacher, is distilled into the student. We further theoretically demonstrate that our objectives increase the diversity among ensemble members and thereby reduce the upper bound of the ensemble's expected loss, leading to more effective distillation. Experimental results show that our method surpasses an existing knowledge augmentation method across diverse configurations. Moreover, the proposed method is compatible with other KD frameworks in a plug-and-play fashion, providing consistent improvements in generalization performance.

