---
layout: default
title: "Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views"
---

# Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22672" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22672v2</a>
  <a href="https://arxiv.org/pdf/2510.22672.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22672v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22672v2', 'Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anna Deichler, Jonas Beskow

**åˆ†ç±»**: cs.CV, cs.CL, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-26 (æ›´æ–°: 2025-10-28)

**å¤‡æ³¨**: 10 pages, 6 figures, 2 tables. Accepted to the NeurIPS 2025 Workshop on SPACE in Vision, Language, and Embodied AI (SpaVLE). Dataset: https://huggingface.co/datasets/annadeichler/KTH-ARIA-referential

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLook and Tellæ•°æ®é›†ï¼Œç”¨äºç ”ç©¶ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒè§†è§’ä¸‹çš„å¤šæ¨¡æ€æŒ‡ç¤ºäº¤æµã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `å…·èº«æ™ºèƒ½` `äººæœºäº¤äº’` `æŒ‡ç¤ºæ€§äº¤æµ` `æ•°æ®é›†` `è‡ªæˆ‘ä¸­å¿ƒè§†è§’` `å¤–éƒ¨ä¸­å¿ƒè§†è§’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç†è§£ä¸åŒè§†è§’ä¸‹çš„æŒ‡ç¤ºæ€§äº¤æµæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªæˆ‘ä¸­å¿ƒå’Œå¤–éƒ¨ä¸­å¿ƒè§†è§’è½¬æ¢æ—¶ã€‚
2. è®ºæ–‡æ ¸å¿ƒåœ¨äºæ„å»ºä¸€ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«åŒæ­¥çš„æ³¨è§†ã€è¯­éŸ³ã€è§†é¢‘ä»¥åŠ3Dåœºæ™¯é‡å»ºï¼Œä»¥ä¿ƒè¿›ç›¸å…³ç ”ç©¶ã€‚
3. è¯¥æ•°æ®é›†åŒ…å«å¤§é‡å¸¦æœ‰ä¸°å¯Œæ³¨é‡Šçš„æŒ‡ç¤ºæ€§è¡¨è¾¾ï¼Œä¸ºè¯„ä¼°ä¸åŒç©ºé—´è¡¨ç¤ºå¯¹å¤šæ¨¡æ€åŸºç¡€çš„å½±å“æä¾›åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»Look and Tellï¼Œä¸€ä¸ªç”¨äºç ”ç©¶ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒè§†è§’ä¸‹æŒ‡ç¤ºæ€§äº¤æµçš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨Meta Project Ariaæ™ºèƒ½çœ¼é•œå’Œå›ºå®šæ‘„åƒå¤´ï¼Œè®°å½•äº†25åå‚ä¸è€…åœ¨æŒ‡å¯¼åŒä¼´è¯†åˆ«å¨æˆ¿é£Ÿææ—¶çš„åŒæ­¥æ³¨è§†ã€è¯­éŸ³å’Œè§†é¢‘ã€‚ç»“åˆ3Dåœºæ™¯é‡å»ºï¼Œè¯¥è®¾ç½®æä¾›äº†ä¸€ä¸ªåŸºå‡†ï¼Œç”¨äºè¯„ä¼°ä¸åŒçš„ç©ºé—´è¡¨ç¤ºï¼ˆ2D vs. 3Dï¼›è‡ªæˆ‘ vs. å¤–éƒ¨ï¼‰å¦‚ä½•å½±å“å¤šæ¨¡æ€åŸºç¡€ã€‚è¯¥æ•°æ®é›†åŒ…å«3.67å°æ—¶çš„å½•éŸ³ï¼ŒåŒ…æ‹¬2,707ä¸ªå¸¦æœ‰ä¸°å¯Œæ³¨é‡Šçš„æŒ‡ç¤ºæ€§è¡¨è¾¾ï¼Œæ—¨åœ¨æ¨è¿›èƒ½å¤Ÿç†è§£å’Œå‚ä¸æƒ…å¢ƒå¯¹è¯çš„å…·èº«æ™ºèƒ½ä½“çš„å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½ä½“å¦‚ä½•ç†è§£å’Œå‚ä¸æƒ…å¢ƒå¯¹è¯çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠä¸åŒè§†è§’ï¼ˆè‡ªæˆ‘ä¸­å¿ƒå’Œå¤–éƒ¨ä¸­å¿ƒï¼‰çš„æŒ‡ç¤ºæ€§äº¤æµåœºæ™¯ä¸­ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†ä¸åŒè§†è§’ä¸‹çš„ç©ºé—´ä¿¡æ¯å·®å¼‚ï¼Œå¯¼è‡´ç†è§£å’Œäº¤äº’çš„å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŒ…å«å¤šæ¨¡æ€æ•°æ®ï¼ˆæ³¨è§†ã€è¯­éŸ³ã€è§†é¢‘ï¼‰ä»¥åŠ3Dåœºæ™¯é‡å»ºçš„æ•°æ®é›†ï¼Œä»è€Œä¸ºç ”ç©¶ä¸åŒè§†è§’ä¸‹çš„æŒ‡ç¤ºæ€§äº¤æµæä¾›ä¸€ä¸ªç»Ÿä¸€çš„å¹³å°ã€‚é€šè¿‡åˆ†æè¿™äº›æ•°æ®ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£ä¸åŒç©ºé—´è¡¨ç¤ºå¦‚ä½•å½±å“å¤šæ¨¡æ€åŸºç¡€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ•°æ®é›†çš„æ„å»ºæµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) æ•°æ®é‡‡é›†ï¼šä½¿ç”¨Meta Project Ariaæ™ºèƒ½çœ¼é•œå’Œå›ºå®šæ‘„åƒå¤´åŒæ­¥è®°å½•å‚ä¸è€…çš„æ³¨è§†ã€è¯­éŸ³å’Œè§†é¢‘ï¼›2) åœºæ™¯é‡å»ºï¼šåˆ©ç”¨é‡‡é›†åˆ°çš„æ•°æ®é‡å»º3Då¨æˆ¿åœºæ™¯ï¼›3) æ•°æ®æ ‡æ³¨ï¼šå¯¹å½•éŸ³ä¸­çš„æŒ‡ç¤ºæ€§è¡¨è¾¾è¿›è¡Œè¯¦ç»†æ ‡æ³¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ•°æ®é›†çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€æ€§å’Œè§†è§’å¤šæ ·æ€§ã€‚å®ƒä¸ä»…åŒ…å«äº†åŒæ­¥çš„æ³¨è§†ã€è¯­éŸ³å’Œè§†é¢‘æ•°æ®ï¼Œè¿˜æä¾›äº†3Dåœºæ™¯é‡å»ºï¼Œä»è€Œå¯ä»¥ç ”ç©¶ä¸åŒç©ºé—´è¡¨ç¤ºå¯¹å¤šæ¨¡æ€åŸºç¡€çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜åŒ…å«äº†è‡ªæˆ‘ä¸­å¿ƒå’Œå¤–éƒ¨ä¸­å¿ƒä¸¤ç§è§†è§’ï¼Œä»è€Œå¯ä»¥ç ”ç©¶ä¸åŒè§†è§’ä¸‹çš„æŒ‡ç¤ºæ€§äº¤æµã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†åŒ…å«25åå‚ä¸è€…ï¼Œå½•åˆ¶äº†3.67å°æ—¶çš„å½•éŸ³ï¼ŒåŒ…æ‹¬2,707ä¸ªå¸¦æœ‰ä¸°å¯Œæ³¨é‡Šçš„æŒ‡ç¤ºæ€§è¡¨è¾¾ã€‚ä½¿ç”¨äº†Meta Project Ariaæ™ºèƒ½çœ¼é•œè¿›è¡Œç¬¬ä¸€äººç§°è§†è§’çš„è§†é¢‘å’Œæ³¨è§†è¿½è¸ªï¼Œå¹¶ä½¿ç”¨å›ºå®šæ‘„åƒå¤´è¿›è¡Œç¬¬ä¸‰äººç§°è§†è§’çš„è§†é¢‘å½•åˆ¶ã€‚3Dåœºæ™¯é‡å»ºçš„å…·ä½“ç®—æ³•å’Œå‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ•°æ®é›†åŒ…å«3.67å°æ—¶çš„å½•éŸ³ï¼ŒåŒ…æ‹¬2,707ä¸ªå¸¦æœ‰ä¸°å¯Œæ³¨é‡Šçš„æŒ‡ç¤ºæ€§è¡¨è¾¾ï¼Œä¸ºç ”ç©¶å¤šæ¨¡æ€åŸºç¡€æä¾›äº†ä¸€ä¸ªä¸°å¯Œçš„èµ„æºã€‚é€šè¿‡ç»“åˆ3Dåœºæ™¯é‡å»ºï¼Œè¯¥æ•°æ®é›†å¯ä»¥ç”¨äºè¯„ä¼°ä¸åŒçš„ç©ºé—´è¡¨ç¤ºï¼ˆ2D vs. 3Dï¼›è‡ªæˆ‘ vs. å¤–éƒ¨ï¼‰å¦‚ä½•å½±å“å¤šæ¨¡æ€åŸºç¡€ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†ä¸€ä¸ªåŸºå‡†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„å…·èº«æ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå‚ä¸æƒ…å¢ƒå¯¹è¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºæœºå™¨äººåŠ©æ‰‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æŒ‡ç¤ºï¼Œåœ¨å¨æˆ¿æˆ–å…¶ä»–ç¯å¢ƒä¸­æ‰¾åˆ°ç‰¹å®šçš„ç‰©å“ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜å¯ä»¥ç”¨äºç ”ç©¶äººæœºäº¤äº’ã€å¤šæ¨¡æ€å­¦ä¹ ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce Look and Tell, a multimodal dataset for studying referential communication across egocentric and exocentric perspectives. Using Meta Project Aria smart glasses and stationary cameras, we recorded synchronized gaze, speech, and video as 25 participants instructed a partner to identify ingredients in a kitchen. Combined with 3D scene reconstructions, this setup provides a benchmark for evaluating how different spatial representations (2D vs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67 hours of recordings, including 2,707 richly annotated referential expressions, and is designed to advance the development of embodied agents that can understand and engage in situated dialogue.

