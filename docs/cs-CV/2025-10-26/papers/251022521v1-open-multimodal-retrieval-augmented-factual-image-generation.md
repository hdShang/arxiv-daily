---
layout: default
title: Open Multimodal Retrieval-Augmented Factual Image Generation
---

# Open Multimodal Retrieval-Augmented Factual Image Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22521" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22521v1</a>
  <a href="https://arxiv.org/pdf/2510.22521.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22521v1" onclick="toggleFavorite(this, '2510.22521v1', 'Open Multimodal Retrieval-Augmented Factual Image Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Tian, Fan Liu, Jingyuan Zhang, Wei Bi, Yupeng Hu, Liqiang Nie

**åˆ†ç±»**: cs.CV, cs.AI, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-26

**å¤‡æ³¨**: Preprint

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºORIGæ¡†æ¶ï¼Œé€šè¿‡å¼€æ”¾å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºï¼Œè§£å†³äº‹å®æ€§å›¾åƒç”Ÿæˆä¸­çŸ¥è¯†ä¸å‡†ç¡®é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äº‹å®æ€§å›¾åƒç”Ÿæˆ` `å¤šæ¨¡æ€æ£€ç´¢` `æ£€ç´¢å¢å¼º` `çŸ¥è¯†èåˆ` `å¼€æ”¾åŸŸ` `å¤§å‹å¤šæ¨¡æ€æ¨¡å‹` `å›¾åƒç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦æˆ–æ—¶æ•ˆæ€§çŸ¥è¯†æ—¶ï¼Œå®¹æ˜“äº§ç”Ÿä¸äº‹å®ç›¸æ‚–çš„ç»“æœï¼Œç¼ºä¹å¯é çš„çŸ¥è¯†æ¥æºã€‚
2. ORIGæ¡†æ¶é€šè¿‡ä»£ç†å¼å¼€æ”¾å¤šæ¨¡æ€æ£€ç´¢ï¼Œä»ç½‘ç»œè·å–å¹¶è¿‡æ»¤è¯æ®ï¼Œé€æ­¥å°†çŸ¥è¯†èå…¥æç¤ºï¼ŒæŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚
3. FIG-EvalåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒORIGåœ¨äº‹å®ä¸€è‡´æ€§å’Œå›¾åƒè´¨é‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å¼€æ”¾æ£€ç´¢å¢å¼ºçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ç”Ÿæˆç…§ç‰‡çº§çœŸå®ä¸”ä¸æç¤ºå¯¹é½çš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ç»å¸¸äº§ç”Ÿä¸å¯éªŒè¯çŸ¥è¯†ç›¸æ‚–çš„è¾“å‡ºï¼Œå°¤å…¶æ˜¯åœ¨æç¤ºæ¶‰åŠç»†ç²’åº¦å±æ€§æˆ–æ—¶é—´æ•æ„Ÿäº‹ä»¶æ—¶ã€‚ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºæ–¹æ³•è¯•å›¾é€šè¿‡å¼•å…¥å¤–éƒ¨ä¿¡æ¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç”±äºå®ƒä»¬ä¾èµ–äºé™æ€æ¥æºå’Œæµ…å±‚è¯æ®é›†æˆï¼Œå› æ­¤ä»æ ¹æœ¬ä¸Šæ— æ³•å°†ç”Ÿæˆå»ºç«‹åœ¨å‡†ç¡®å’Œä¸æ–­å‘å±•çš„çŸ¥è¯†ä¹‹ä¸Šã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ORIGï¼Œä¸€ä¸ªç”¨äºäº‹å®æ€§å›¾åƒç”Ÿæˆï¼ˆFIGï¼‰çš„ä»£ç†å¼å¼€æ”¾å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæ¡†æ¶ã€‚FIGæ˜¯ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œéœ€è¦è§†è§‰çœŸå®æ„Ÿå’Œäº‹å®åŸºç¡€ã€‚ORIGè¿­ä»£åœ°ä»ç½‘ç»œæ£€ç´¢å’Œè¿‡æ»¤å¤šæ¨¡æ€è¯æ®ï¼Œå¹¶å°†æç‚¼åçš„çŸ¥è¯†å¢é‡å¼åœ°é›†æˆåˆ°ä¸°å¯Œçš„æç¤ºä¸­ï¼Œä»¥æŒ‡å¯¼ç”Ÿæˆã€‚ä¸ºäº†æ”¯æŒç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†FIG-Evalï¼Œä¸€ä¸ªè·¨è¶Šæ„ŸçŸ¥ã€ç»„åˆå’Œæ—¶é—´ç»´åº¦åä¸ªç±»åˆ«çš„åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒORIGåœ¨äº‹å®ä¸€è‡´æ€§å’Œæ•´ä½“å›¾åƒè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œçªå‡ºäº†å¼€æ”¾å¤šæ¨¡æ€æ£€ç´¢åœ¨äº‹å®æ€§å›¾åƒç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç»†ç²’åº¦å±æ€§æˆ–æ—¶æ•ˆæ€§çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œå®¹æ˜“äº§ç”Ÿä¸äº‹å®ä¸ç¬¦çš„å›¾åƒã€‚ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºæ–¹æ³•ä¾èµ–äºé™æ€çš„çŸ¥è¯†åº“ï¼Œæ— æ³•è·å–æœ€æ–°çš„ä¿¡æ¯ï¼Œå¹¶ä¸”è¯æ®é›†æˆæ–¹å¼è¾ƒä¸ºæµ…å±‚ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆå›¾åƒçš„äº‹å®å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤ŸåŠ¨æ€è·å–å¹¶æœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†çš„å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šORIGçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªä»£ç†ï¼ˆAgentï¼‰è¿­ä»£åœ°ä»å¼€æ”¾ç½‘ç»œä¸­æ£€ç´¢ç›¸å…³çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆæ–‡æœ¬ã€å›¾åƒç­‰ï¼‰ï¼Œå¹¶å¯¹æ£€ç´¢åˆ°çš„ä¿¡æ¯è¿›è¡Œè¿‡æ»¤å’Œæç‚¼ï¼Œç„¶åå°†è¿™äº›æç‚¼åçš„çŸ¥è¯†é€æ­¥èå…¥åˆ°å›¾åƒç”Ÿæˆçš„æç¤ºè¯ä¸­ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆäº‹å®çš„å›¾åƒã€‚è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨å¼€æ”¾ç½‘ç»œä½œä¸ºåŠ¨æ€çš„çŸ¥è¯†æ¥æºï¼Œå¹¶é€šè¿‡è¿­ä»£å¼çš„æ£€ç´¢å’ŒçŸ¥è¯†èåˆï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„äº‹å®ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šORIGæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) **å¤šæ¨¡æ€æ£€ç´¢æ¨¡å—**ï¼šè´Ÿè´£ä»ç½‘ç»œä¸Šæ£€ç´¢ä¸è¾“å…¥æç¤ºç›¸å…³çš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚2) **çŸ¥è¯†è¿‡æ»¤æ¨¡å—**ï¼šå¯¹æ£€ç´¢åˆ°çš„ä¿¡æ¯è¿›è¡Œè¿‡æ»¤ï¼Œå»é™¤å™ªå£°å’Œä¸ç›¸å…³çš„ä¿¡æ¯ã€‚3) **çŸ¥è¯†èåˆæ¨¡å—**ï¼šå°†è¿‡æ»¤åçš„çŸ¥è¯†èå…¥åˆ°åŸå§‹æç¤ºä¸­ï¼Œç”Ÿæˆå¢å¼ºçš„æç¤ºã€‚4) **å›¾åƒç”Ÿæˆæ¨¡å—**ï¼šåˆ©ç”¨å¢å¼ºçš„æç¤ºç”Ÿæˆå›¾åƒã€‚æ•´ä¸ªæµç¨‹æ˜¯è¿­ä»£è¿›è¡Œçš„ï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä¼šæ£€ç´¢ã€è¿‡æ»¤å’Œèåˆæ–°çš„çŸ¥è¯†ï¼Œç›´åˆ°ç”Ÿæˆæ»¡æ„çš„å›¾åƒä¸ºæ­¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šORIGçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä»£ç†å¼çš„å¼€æ”¾å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºæ–¹æ³•ä¸åŒï¼ŒORIGä¸ä¾èµ–äºé™æ€çš„çŸ¥è¯†åº“ï¼Œè€Œæ˜¯ç›´æ¥ä»å¼€æ”¾ç½‘ç»œä¸­è·å–ä¿¡æ¯ï¼Œä»è€Œèƒ½å¤Ÿè·å–æœ€æ–°çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒORIGé‡‡ç”¨è¿­ä»£å¼çš„æ£€ç´¢å’ŒçŸ¥è¯†èåˆæ–¹å¼ï¼Œèƒ½å¤Ÿé€æ­¥æé«˜ç”Ÿæˆå›¾åƒçš„äº‹å®ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šORIGåœ¨å¤šæ¨¡æ€æ£€ç´¢æ¨¡å—ä¸­ä½¿ç”¨äº†åŸºäºæ–‡æœ¬å’Œå›¾åƒç›¸ä¼¼åº¦çš„æ£€ç´¢æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸è¾“å…¥æç¤ºç›¸å…³ã€‚åœ¨çŸ¥è¯†è¿‡æ»¤æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†åŸºäºè§„åˆ™å’Œæœºå™¨å­¦ä¹ çš„æ–¹æ³•æ¥å»é™¤å™ªå£°ä¿¡æ¯ã€‚åœ¨çŸ¥è¯†èåˆæ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ–¹æ³•æ¥å°†çŸ¥è¯†èå…¥åˆ°æç¤ºä¸­ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒORIGåœ¨FIG-EvalåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚åœ¨äº‹å®ä¸€è‡´æ€§æ–¹é¢ï¼ŒORIGçš„æŒ‡æ ‡æå‡äº†XX%ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ï¼Œåœ¨å›¾åƒè´¨é‡æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¼€æ”¾å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæé«˜å›¾åƒç”Ÿæˆçš„äº‹å®å‡†ç¡®æ€§å’Œæ•´ä½“è´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ–°é—»æŠ¥é“ã€å†å²äº‹ä»¶å¯è§†åŒ–ã€æ•™è‚²å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸï¼Œæå‡å›¾åƒç”Ÿæˆçš„äº‹å®å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ ¹æ®æ–°é—»æ ‡é¢˜ç”Ÿæˆç¬¦åˆäº‹å®çš„å›¾åƒï¼Œæˆ–æ ¹æ®å†å²äº‹ä»¶æè¿°ç”Ÿæˆç›¸åº”çš„åœºæ™¯å›¾åƒï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ½œåœ¨çš„ç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Multimodal Models (LMMs) have achieved remarkable progress in generating photorealistic and prompt-aligned images, but they often produce outputs that contradict verifiable knowledge, especially when prompts involve fine-grained attributes or time-sensitive events. Conventional retrieval-augmented approaches attempt to address this issue by introducing external information, yet they are fundamentally incapable of grounding generation in accurate and evolving knowledge due to their reliance on static sources and shallow evidence integration. To bridge this gap, we introduce ORIG, an agentic open multimodal retrieval-augmented framework for Factual Image Generation (FIG), a new task that requires both visual realism and factual grounding. ORIG iteratively retrieves and filters multimodal evidence from the web and incrementally integrates the refined knowledge into enriched prompts to guide generation. To support systematic evaluation, we build FIG-Eval, a benchmark spanning ten categories across perceptual, compositional, and temporal dimensions. Experiments demonstrate that ORIG substantially improves factual consistency and overall image quality over strong baselines, highlighting the potential of open multimodal retrieval for factual image generation.

