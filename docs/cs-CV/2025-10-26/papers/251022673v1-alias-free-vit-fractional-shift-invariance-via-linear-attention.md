---
layout: default
title: Alias-Free ViT: Fractional Shift Invariance via Linear Attention
---

# Alias-Free ViT: Fractional Shift Invariance via Linear Attention

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.22673" target="_blank" class="toolbar-btn">arXiv: 2510.22673v1</a>
    <a href="https://arxiv.org/pdf/2510.22673.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22673v1" 
            onclick="toggleFavorite(this, '2510.22673v1', 'Alias-Free ViT: Fractional Shift Invariance via Linear Attention')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Hagay Michaeli, Daniel Soudry

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-26

**å¤‡æ³¨**: Accepted at NeurIPS 2025. Code is available at https://github.com/hmichaeli/alias_free_vit

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAlias-Free ViTï¼Œé€šè¿‡çº¿æ€§æ³¨æ„åŠ›å®ç°åˆ†æ•°å¹³ç§»ä¸å˜æ€§ï¼Œæå‡ViTçš„é²æ£’æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Vision Transformer` `å¹³ç§»ä¸å˜æ€§` `æŠ—æ··å ` `çº¿æ€§æ³¨æ„åŠ›` `å›¾åƒåˆ†ç±»` `é²æ£’æ€§` `åˆ†æ•°å¹³ç§»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ViTç¼ºä¹å·ç§¯ç¥ç»ç½‘ç»œçš„å¹³ç§»ä¸å˜æ€§å½’çº³åç½®ï¼Œå¯¼è‡´å…¶å¯¹å›¾åƒå¾®å°å¹³ç§»æ•æ„Ÿï¼Œå½±å“æ€§èƒ½ã€‚
2. æå‡ºAlias-Free ViTï¼Œç»“åˆæ— æ··å ä¸‹é‡‡æ ·å’Œéçº¿æ€§ï¼Œä»¥åŠçº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›ï¼Œå®ç°åˆ†æ•°å¹³ç§»ä¸å˜æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­ä¿æŒç«äº‰åŠ›ï¼Œå¹¶åœ¨å¯¹æŠ—å¹³ç§»é²æ£’æ€§æ–¹é¢ä¼˜äºåŒç­‰è§„æ¨¡çš„æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Transformeråœ¨è§†è§‰ä»»åŠ¡ä¸­å·²æˆä¸ºå·ç§¯ç¥ç»ç½‘ç»œ(convnets)çš„æœ‰åŠ›ç«äº‰è€…ï¼Œä½†å®ƒä»¬ç¼ºä¹convnetsçš„æ¶æ„å½’çº³åç½®ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å…¶æ½œåœ¨æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼ŒVision Transformers (ViTs)ä¸å…·å¤‡å¹³ç§»ä¸å˜æ€§ï¼Œå¹¶ä¸”æ¯”æ ‡å‡†convnetså¯¹å¾®å°çš„å›¾åƒå¹³ç§»æ›´æ•æ„Ÿã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç”±äºä¸‹é‡‡æ ·å’Œéçº¿æ€§å±‚ä¸­çš„æ··å ç°è±¡ï¼Œconvnetsä¹Ÿä¸æ˜¯å®Œå…¨çš„å¹³ç§»ä¸å˜çš„ã€‚å› æ­¤ï¼Œå·²ç»æå‡ºäº†æŠ—æ··å æ–¹æ³•æ¥éªŒè¯convnetsçš„å¹³ç§»é²æ£’æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Alias-Free ViTï¼Œå®ƒç»“åˆäº†ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œå®ƒä½¿ç”¨æ— æ··å çš„ä¸‹é‡‡æ ·å’Œéçº¿æ€§ã€‚å…¶æ¬¡ï¼Œå®ƒä½¿ç”¨çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›ï¼Œè¯¥æ³¨æ„åŠ›å¯¹æ•´æ•°å’Œå¹³ç§»éƒ½å…·æœ‰ç§»ä½ç­‰å˜æ€§ï¼Œä»è€Œå®ç°ç§»ä½ä¸å˜çš„å…¨å±€è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­ä¿æŒäº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¯¹æŠ—æ€§å¹³ç§»çš„é²æ£’æ€§æ–¹é¢ä¼˜äºç±»ä¼¼å¤§å°çš„æ¨¡å‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVision Transformer (ViT) åœ¨å›¾åƒåˆ†ç±»ç­‰è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ç¼ºä¹å·ç§¯ç¥ç»ç½‘ç»œ (CNN) çš„å¹³ç§»ä¸å˜æ€§ï¼Œå¯¼è‡´å¯¹å›¾åƒçš„å¾®å°å¹³ç§»éå¸¸æ•æ„Ÿã€‚è¿™ç§æ•æ„Ÿæ€§é™ä½äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚ç°æœ‰çš„ CNN ä¹Ÿå­˜åœ¨ç”±äºä¸‹é‡‡æ ·å’Œéçº¿æ€§æ“ä½œå¼•å…¥çš„æ··å æ•ˆåº”ï¼Œå¯¼è‡´å¹³ç§»ä¸å˜æ€§ä¸å®Œç¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æŠ—æ··å æœºåˆ¶å’Œçº¿æ€§ç§»ä½ç­‰å˜æ³¨æ„åŠ›æ¥å¢å¼º ViT çš„å¹³ç§»ä¸å˜æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä½¿ç”¨æ— æ··å çš„ä¸‹é‡‡æ ·å’Œéçº¿æ€§æ¿€æ´»å‡½æ•°æ¥å‡å°‘æ··å æ•ˆåº”ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯¹æ•´æ•°å’Œå¹³ç§»éƒ½å…·æœ‰ç§»ä½ç­‰å˜æ€§ï¼Œä»è€Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°å¹³ç§»ä¸å˜çš„å…¨å±€è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAlias-Free ViT çš„æ•´ä½“æ¶æ„åŸºäºæ ‡å‡†çš„ ViT ç»“æ„ï¼Œä¸»è¦æ”¹è¿›åœ¨äºä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯å°†ä¼ ç»Ÿçš„ä¸‹é‡‡æ ·å±‚æ›¿æ¢ä¸ºæŠ—æ··å çš„ä¸‹é‡‡æ ·å±‚ï¼Œä¾‹å¦‚ä½¿ç”¨æ¨¡ç³Šæ± åŒ– (blur pooling) æˆ–å¯åˆ†ç¦»å·ç§¯ï¼›äºŒæ˜¯å°†æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ›¿æ¢ä¸ºçº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›ã€‚æ•´ä¸ªç½‘ç»œä»ç„¶ç”±å¤šä¸ª Transformer å—å †å è€Œæˆï¼Œæ¯ä¸ªå—åŒ…å«ä¸€ä¸ªçº¿æ€§æ³¨æ„åŠ›å±‚å’Œä¸€ä¸ªå‰é¦ˆç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æŠ—æ··å æŠ€æœ¯ä¸çº¿æ€§ç§»ä½ç­‰å˜æ³¨æ„åŠ›ç›¸ç»“åˆï¼Œä»è€Œæœ‰æ•ˆåœ°æé«˜äº† ViT çš„å¹³ç§»ä¸å˜æ€§ã€‚çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›æ˜¯å¦ä¸€ä¸ªå…³é”®åˆ›æ–°ï¼Œå®ƒä¸ä»…é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œè€Œä¸”ä¿è¯äº†å¯¹åˆ†æ•°å¹³ç§»çš„ç§»ä½ç­‰å˜æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ—æ··å ä¸‹é‡‡æ ·æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„æ»¤æ³¢å™¨å’Œä¸‹é‡‡æ ·ç­–ç•¥ï¼Œä¾‹å¦‚é«˜æ–¯æ¨¡ç³Šå’Œå¯åˆ†ç¦»å·ç§¯ã€‚çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›çš„å…·ä½“å®ç°æ¶‰åŠè®¡ç®— query å’Œ key ä¹‹é—´çš„äº¤å‰åæ–¹å·®çŸ©é˜µï¼Œå¹¶ä½¿ç”¨è¯¥çŸ©é˜µæ¥åŠ æƒ valueã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å‚æ•°ï¼ˆå¦‚å±‚æ•°ã€éšè—å•å…ƒæ•°ç­‰ï¼‰å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Alias-Free ViT åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†ä¸æ ‡å‡† ViT ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å¯¹æŠ—å¹³ç§»æ”»å‡»ä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æŠµæŠ—å¾®å°çš„å›¾åƒå¹³ç§»ï¼Œå¹¶ä¸”åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹ä¼˜äºåŒç­‰è§„æ¨¡çš„æ¨¡å‹ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦å–å†³äºæ•°æ®é›†å’Œæ”»å‡»ç±»å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Alias-Free ViT åœ¨å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å…¶å¢å¼ºçš„å¹³ç§»ä¸å˜æ€§ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†å…·æœ‰è½»å¾®å¹³ç§»æˆ–å½¢å˜çš„å›¾åƒæ—¶æ›´åŠ é²æ£’ï¼Œä¾‹å¦‚åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡è§†è§‰æ¨¡å‹çš„å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Transformers have emerged as a competitive alternative to convnets in vision tasks, yet they lack the architectural inductive bias of convnets, which may hinder their potential performance. Specifically, Vision Transformers (ViTs) are not translation-invariant and are more sensitive to minor image translations than standard convnets. Previous studies have shown, however, that convnets are also not perfectly shift-invariant, due to aliasing in downsampling and nonlinear layers. Consequently, anti-aliasing approaches have been proposed to certify convnets' translation robustness. Building on this line of work, we propose an Alias-Free ViT, which combines two main components. First, it uses alias-free downsampling and nonlinearities. Second, it uses linear cross-covariance attention that is shift-equivariant to both integer and fractional translations, enabling a shift-invariant global representation. Our model maintains competitive performance in image classification and outperforms similar-sized models in terms of robustness to adversarial translations.

