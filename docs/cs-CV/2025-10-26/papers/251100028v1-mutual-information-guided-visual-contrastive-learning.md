---
layout: default
title: Mutual Information guided Visual Contrastive Learning
---

# Mutual Information guided Visual Contrastive Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.00028" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.00028v1</a>
  <a href="https://arxiv.org/pdf/2511.00028.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00028v1" onclick="toggleFavorite(this, '2511.00028v1', 'Mutual Information guided Visual Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanyang Chen, Yanchao Yang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-26

**å¤‡æ³¨**: Tech Report - Undergraduate Thesis - 2023

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºäº’ä¿¡æ¯å¼•å¯¼çš„è§†è§‰å¯¹æ¯”å­¦ä¹ ï¼Œæå‡è¡¨å¾å­¦ä¹ åœ¨å¼€æ”¾ç¯å¢ƒä¸‹çš„æ³›åŒ–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `äº’ä¿¡æ¯` `è¡¨å¾å­¦ä¹ ` `æ•°æ®å¢å¼º` `è§†è§‰ä¸å˜æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„æ•°æ®å¢å¼ºä¾èµ–äººå·¥è®¾è®¡ï¼Œå¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®ä¸­çš„ä¿¡æ¯ã€‚
2. è¯¥æ–¹æ³•åˆ©ç”¨äº’ä¿¡æ¯æ¥é€‰æ‹©è®­ç»ƒæ•°æ®ï¼Œç‰¹åˆ«æ˜¯é€‰æ‹©åœ¨è‡ªç„¶æ‰°åŠ¨ä¸‹å…·æœ‰é«˜äº’ä¿¡æ¯çš„å›¾åƒå—ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæœ‰æ•ˆï¼Œå¹¶æå‡äº†è¡¨å¾å­¦ä¹ çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäº’ä¿¡æ¯å¼•å¯¼çš„è§†è§‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é€‰æ‹©æ›´å…·ä¿¡æ¯é‡çš„è®­ç»ƒæ•°æ®æ¥æå‡è¡¨å¾å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚ç°æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä¾èµ–äººå·¥è®¾è®¡çš„å›¾åƒå¢å¼ºç­–ç•¥ï¼Œå¯èƒ½å¹¶éæœ€ä¼˜ã€‚æœ¬æ–‡æ¢ç´¢äº†åŸºäºçœŸå®ä¸–ç•Œåˆ†å¸ƒè®¡ç®—çš„äº’ä¿¡æ¯æ¥é€‰æ‹©è®­ç»ƒæ•°æ®çš„æ½œåŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œè®ºæ–‡å°†è‡ªç„¶æ‰°åŠ¨ï¼ˆå¦‚é¢œè‰²å˜åŒ–å’Œè¿åŠ¨ï¼‰ä¸‹å…·æœ‰é«˜äº’ä¿¡æ¯çš„å›¾åƒå—ä½œä¸ºæ­£æ ·æœ¬ï¼Œç”¨äºå¯¹æ¯”å­¦ä¹ ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’Œå…ˆè¿›è¡¨å¾å­¦ä¹ æ¡†æ¶ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®å¢å¼ºæ–¹é¢ä¸»è¦ä¾èµ–äººå·¥è®¾è®¡çš„ç­–ç•¥ï¼Œä¾‹å¦‚é¢œè‰²æŠ–åŠ¨ï¼Œä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ç…§æ˜å˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™ç§äººå·¥è®¾è®¡çš„å¢å¼ºæ–¹å¼å¯èƒ½å¹¶éæœ€ä¼˜ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®æœ¬èº«æ‰€è•´å«çš„ä¿¡æ¯ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹åœ¨å¼€æ”¾ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•æ›´æœ‰æ•ˆåœ°é€‰æ‹©è®­ç»ƒæ•°æ®ï¼Œä½¿å…¶åŒ…å«æ›´å¤šæœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨äº’ä¿¡æ¯æ¥æŒ‡å¯¼è®­ç»ƒæ•°æ®çš„é€‰æ‹©ã€‚äº’ä¿¡æ¯å¯ä»¥è¡¡é‡ä¸¤ä¸ªéšæœºå˜é‡ä¹‹é—´çš„ä¾èµ–ç¨‹åº¦ï¼Œå› æ­¤ï¼Œé€‰æ‹©åœ¨è‡ªç„¶æ‰°åŠ¨ä¸‹å…·æœ‰é«˜äº’ä¿¡æ¯çš„å›¾åƒå—ä½œä¸ºæ­£æ ·æœ¬ï¼Œå¯ä»¥ä¿è¯æ¨¡å‹å­¦ä¹ åˆ°å¯¹è¿™äº›æ‰°åŠ¨å…·æœ‰ä¸å˜æ€§çš„ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿä½¿æ¨¡å‹æ›´å¥½åœ°æ•æ‰åˆ°æ•°æ®ä¸­çš„æœ¬è´¨ä¿¡æ¯ï¼Œä»è€Œæå‡å…¶æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•é¦–å…ˆè®¡ç®—å›¾åƒä¸­ä¸åŒåŒºåŸŸåœ¨è‡ªç„¶æ‰°åŠ¨ä¸‹çš„äº’ä¿¡æ¯ã€‚ç„¶åï¼Œé€‰æ‹©äº’ä¿¡æ¯è¾ƒé«˜çš„å›¾åƒå—ä½œä¸ºæ­£æ ·æœ¬ï¼Œä¸åŸå§‹å›¾åƒç»„æˆæ­£æ ·æœ¬å¯¹ã€‚è´Ÿæ ·æœ¬åˆ™ä»å…¶ä»–å›¾åƒä¸­éšæœºé€‰æ‹©ã€‚æœ€åï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚InfoNCE lossï¼Œæ¥è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä½¿å…¶èƒ½å¤ŸåŒºåˆ†æ­£æ ·æœ¬å¯¹å’Œè´Ÿæ ·æœ¬å¯¹ã€‚æ•´ä¸ªæµç¨‹çš„å…³é”®åœ¨äºäº’ä¿¡æ¯çš„è®¡ç®—å’Œæ­£è´Ÿæ ·æœ¬çš„é€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨äº’ä¿¡æ¯æ¥æŒ‡å¯¼å¯¹æ¯”å­¦ä¹ ä¸­çš„æ•°æ®å¢å¼ºè¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸­äººå·¥è®¾è®¡çš„å¢å¼ºç­–ç•¥ä¸åŒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®æ•°æ®æœ¬èº«çš„ç‰¹æ€§ï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©æ›´å…·ä¿¡æ¯é‡çš„è®­ç»ƒæ ·æœ¬ã€‚è¿™ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­äº’ä¿¡æ¯çš„è®¡ç®—æ–¹å¼éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯è¿›è¡Œé€‰æ‹©ã€‚ä¸€ç§å¸¸ç”¨çš„æ–¹æ³•æ˜¯åˆ©ç”¨å›¾åƒå—çš„é¢œè‰²ç›´æ–¹å›¾æ¥è®¡ç®—äº’ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ­£è´Ÿæ ·æœ¬çš„é€‰æ‹©æ¯”ä¾‹ä¹Ÿä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ï¼Œéœ€è¦è¿›è¡Œåˆç†çš„è°ƒæ•´ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼ŒInfoNCE lossæ˜¯ä¸€ç§å¸¸ç”¨çš„é€‰æ‹©ï¼Œä½†ä¹Ÿå¯ä»¥å°è¯•å…¶ä»–çš„å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬å¸¸ç”¨çš„å›¾åƒåˆ†ç±»å’Œç›®æ ‡æ£€æµ‹æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚åœ¨ImageNetæ•°æ®é›†ä¸Šï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„Top-1å‡†ç¡®ç‡æå‡äº†1-2ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é²æ£’ç‰¹å¾è¡¨ç¤ºçš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†ç±»ã€å›¾åƒæ£€ç´¢ç­‰ã€‚ç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾ç¯å¢ƒä¸‹ï¼Œç”±äºå…‰ç…§ã€è§†è§’ç­‰å› ç´ çš„å˜åŒ–ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚è¯¥æ–¹æ³•é€šè¿‡äº’ä¿¡æ¯å¼•å¯¼çš„æ•°æ®å¢å¼ºï¼Œå¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹åœ¨è¿™äº›åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Representation learning methods utilizing the InfoNCE loss have demonstrated considerable capacity in reducing human annotation effort by training invariant neural feature extractors. Although different variants of the training objective adhere to the information maximization principle between the data and learned features, data selection and augmentation still rely on human hypotheses or engineering, which may be suboptimal. For instance, data augmentation in contrastive learning primarily focuses on color jittering, aiming to emulate real-world illumination changes. In this work, we investigate the potential of selecting training data based on their mutual information computed from real-world distributions, which, in principle, should endow the learned features with better generalization when applied in open environments. Specifically, we consider patches attached to scenes that exhibit high mutual information under natural perturbations, such as color changes and motion, as positive samples for learning with contrastive loss. We evaluate the proposed mutual-information-informed data augmentation method on several benchmarks across multiple state-of-the-art representation learning frameworks, demonstrating its effectiveness and establishing it as a promising direction for future research.

