---
layout: default
title: Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs
---

# Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22603" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22603v2</a>
  <a href="https://arxiv.org/pdf/2510.22603.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22603v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22603v2', 'Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anand, Umberto Cappellazzo, Stavros Petridis, Maja Pantic

**åˆ†ç±»**: eess.AS, cs.CV, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-10-26 (æ›´æ–°: 2025-11-02)

**å¤‡æ³¨**: The code is available at https://github.com/umbertocappellazzo/Llama-AVSR

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é’ˆå¯¹AVSRä¸­LLMçš„Attention Sinké—®é¢˜ï¼Œæå‡ºè§£è€¦æŸå¤±ä»¥æå‡è¯†åˆ«ç²¾åº¦**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†å¬è¯­éŸ³è¯†åˆ«` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `Attention Sink` `è§£è€¦æŸå¤±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºLLMçš„AVSRæ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹å†…éƒ¨åŠ¨æ€çš„æ·±å…¥ç†è§£ï¼Œå­˜åœ¨Attention Sinkå’ŒMassive Activationé—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§è§£è€¦æŸå¤±ï¼Œæ—¨åœ¨é™ä½BOS tokenä¸å…¶ä»–tokençš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»è€Œç¼“è§£ä¸­é—´sinkå’Œå·¨å¤§æ¿€æ´»ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é«˜ç‰¹å¾ä¸‹é‡‡æ ·ç‡ä¸‹èƒ½æœ‰æ•ˆæå‡WERï¼Œå¹¶åœ¨ä½é‡‡æ ·ç‡ä¸‹ä¿æŒæ€§èƒ½ç¨³å®šã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨å¬è§‰è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰å’Œè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹å…¶å¾®è°ƒä¸‹çš„å†…éƒ¨åŠ¨æ€çš„ç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†æ³¨æ„åŠ›æ±‡èšï¼ˆattention sinksï¼‰ï¼Œå³å¸å¼•ä¸æˆæ¯”ä¾‹çš„é«˜æ³¨æ„åŠ›çš„tokenï¼Œä»¥åŠç›¸å…³çš„å·¨å¤§æ¿€æ´»ï¼ˆmassive activationsï¼‰ï¼Œå…¶ä¸­sink tokençš„æŸäº›ç‰¹å¾åœ¨LLMä¸­è¡¨ç°å‡ºå·¨å¤§çš„æ¿€æ´»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡ç ”ç©¶äº†å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«ä¸­çš„è¿™äº›ç°è±¡ã€‚é€šè¿‡å¯¹è§†å¬LLMçš„è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬ä¸ä»…åœ¨BOS tokenå¤„ï¼Œè€Œä¸”åœ¨ASRã€VSRå’ŒAVSRçš„ä¸­é—´ä½è¯­ä¹‰tokenå¤„è¯†åˆ«å‡ºæ³¨æ„åŠ›æ±‡èšå’Œå·¨å¤§æ¿€æ´»ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå·¨å¤§æ¿€æ´»æºäºMLPå±‚ï¼Œå¹¶ä¸”å¯¹åº”äºæ‰€æœ‰sink tokenä¸­çš„å›ºå®šç‰¹å¾ç´¢å¼•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œä¸­é—´sink tokenä¸BOS tokenè¡¨ç°å‡ºé«˜çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»è€Œæ”¾å¤§äº†æ³¨æ„åŠ›å’Œæ¿€æ´»ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•çš„è§£è€¦æŸå¤±ï¼Œè¯¥æŸå¤±é™ä½äº†BOSå’Œå…¶ä»–tokenä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»è€Œæœ‰æ•ˆåœ°å‡è½»äº†ä¸­é—´sinkå’Œå·¨å¤§æ¿€æ´»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é«˜çš„è§†å¬ç‰¹å¾ä¸‹é‡‡æ ·ä¸‹æé«˜äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼ŒåŒæ—¶åœ¨è¾ƒä½çš„ä¸‹é‡‡æ ·ç‡ä¸‹ä¿æŒç¨³å®šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŸºäºLLMçš„AVSRæ¨¡å‹ä¸­å­˜åœ¨çš„Attention Sinkå’ŒMassive Activationé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¾®è°ƒLLMæ—¶ï¼Œå¯¹æ¨¡å‹å†…éƒ¨åŠ¨æ€ç†è§£ä¸è¶³ï¼Œå¯¼è‡´æŸäº›tokenï¼ˆå°¤å…¶æ˜¯BOS tokenå’Œä¸­é—´ä½è¯­ä¹‰tokenï¼‰å¸å¼•äº†è¿‡å¤šçš„æ³¨æ„åŠ›ï¼Œè¿›è€Œå¼•å‘äº†å·¨å¤§çš„æ¿€æ´»ï¼Œå½±å“äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é™ä½BOS tokenä¸å…¶ä»–tokenä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»è€Œå‡å°‘Attention Sinkæ•ˆåº”ã€‚ä½œè€…è§‚å¯Ÿåˆ°ï¼Œä¸­é—´sink tokenä¸BOS tokenå…·æœ‰å¾ˆé«˜çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè¿™ä¼šæ”¾å¤§æ³¨æ„åŠ›å’Œæ¿€æ´»ã€‚é€šè¿‡é™ä½è¿™ç§ç›¸ä¼¼æ€§ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç¼“è§£ä¸­é—´sinkå’Œå·¨å¤§æ¿€æ´»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡ä¸»è¦ç ”ç©¶äº†åŸºäºLLMçš„AVSRæ¨¡å‹ï¼Œå¹¶é’ˆå¯¹Attention Sinké—®é¢˜æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚æ•´ä½“æ¡†æ¶åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨LLMä½œä¸ºAVSRæ¨¡å‹çš„ä¸»å¹²ç½‘ç»œï¼›2ï¼‰åˆ†ææ¨¡å‹ä¸­Attention Sinkå’ŒMassive Activationçš„ç°è±¡ï¼›3ï¼‰æå‡ºè§£è€¦æŸå¤±å‡½æ•°ï¼Œç”¨äºé™ä½BOS tokenä¸å…¶ä»–tokençš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼›4ï¼‰åœ¨AVSRä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒï¼Œè¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1ï¼‰é¦–æ¬¡åœ¨å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«é¢†åŸŸç ”ç©¶äº†Attention Sinkå’ŒMassive Activationç°è±¡ï¼›2ï¼‰æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è§£è€¦æŸå¤±å‡½æ•°ï¼Œç”¨äºç¼“è§£Attention Sinké—®é¢˜ã€‚è¯¥æŸå¤±å‡½æ•°ç›´æ¥ä½œç”¨äºtoken embeddingç©ºé—´ï¼Œé™ä½äº†BOS tokenä¸å…¶ä»–tokençš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»è€Œå‡å°‘äº†ä¸å¿…è¦çš„æ³¨æ„åŠ›é›†ä¸­ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åœ¨äºè§£è€¦æŸå¤±å‡½æ•°ã€‚è¯¥æŸå¤±å‡½æ•°çš„ç›®æ ‡æ˜¯æœ€å°åŒ–BOS tokenä¸å…¶ä»–tokenä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªtoken embeddingï¼Œè®¡ç®—å…¶ä¸BOS token embeddingçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶å°†è¿™äº›ç›¸ä¼¼åº¦ä½œä¸ºæŸå¤±é¡¹è¿›è¡Œä¼˜åŒ–ã€‚æŸå¤±å‡½æ•°çš„å…·ä½“å½¢å¼æœªçŸ¥ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é™ä½token embeddingç©ºé—´ä¸­BOS tokenä¸å…¶ä»–tokençš„ç›¸ä¼¼æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è§£è€¦æŸå¤±åœ¨é«˜è§†å¬ç‰¹å¾ä¸‹é‡‡æ ·ç‡ä¸‹èƒ½å¤Ÿæ˜¾è‘—æé«˜AVSRæ¨¡å‹çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼ŒåŒæ—¶åœ¨è¾ƒä½çš„ä¸‹é‡‡æ ·ç‡ä¸‹ä¿æŒæ€§èƒ½ç¨³å®šã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£Attention Sinké—®é¢˜ï¼Œå¹¶æé«˜æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„é²æ£’æ€§ã€‚å…·ä½“çš„WERæå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è§†å¬è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™æˆ–å™ªå£°ç¯å¢ƒä¸‹ã€‚é€šè¿‡ç¼“è§£Attention Sinké—®é¢˜ï¼Œå¯ä»¥æé«˜AVSRæ¨¡å‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œä»è€Œæ”¹å–„è¯­éŸ³åŠ©æ‰‹ã€è§†é¢‘ä¼šè®®ã€å­—å¹•ç”Ÿæˆç­‰åº”ç”¨çš„ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¯¹äºç†è§£å’Œä¼˜åŒ–å¤šæ¨¡æ€LLMå…·æœ‰é‡è¦çš„ç†è®ºä»·å€¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR). However, understanding of their internal dynamics under fine-tuning remains limited. In natural language processing, recent work has revealed attention sinks, tokens that attract disproportionately high attention, and associated massive activations in which some features of sink tokens exhibit huge activation in LLMs. In this work, we are the first to study these phenomena in multimodal speech recognition. Through a detailed analysis of audio-visual LLMs, we identify attention sinks and massive activations not only at the BOS token but also at intermediate low-semantic tokens across ASR, VSR, and AVSR. We show that massive activations originate in the MLP layers and correspond to fixed feature indices across all sink tokens. We further show that intermediate sink tokens exhibit high cosine similarity to the BOS token, thereby amplifying attention and activation. Building on these insights, we introduce a simple decorrelation loss that reduces cosine similarity between BOS and other tokens, effectively mitigating intermediate sinks and massive activations. Furthermore, our method improves word error rate (WER) under high audio-visual feature downsampling while remaining stable at lower downsampling rates.

