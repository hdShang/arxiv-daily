---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-12
---

# cs.CVï¼ˆ2025-05-12ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250507998v1-vision-foundation-model-embedding-based-semantic-anomaly-detection.html">Vision Foundation Model Embedding-Based Semantic Anomaly Detection</a></td>
  <td>æå‡ºåŸºäºè§†è§‰åŸºç¡€æ¨¡å‹åµŒå…¥çš„è¯­ä¹‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07998v1" data-paper-url="./papers/250507998v1-vision-foundation-model-embedding-based-semantic-anomaly-detection.html" onclick="toggleFavorite(this, '2505.07998v1', 'Vision Foundation Model Embedding-Based Semantic Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250507263v2-skywork-vl-reward-an-effective-reward-model-for-multimodal-understan.html">Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning</a></td>
  <td>æå‡ºSkywork-VL Rewardä»¥æå‡å¤šæ¨¡æ€ç†è§£ä¸æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07263v2" data-paper-url="./papers/250507263v2-skywork-vl-reward-an-effective-reward-model-for-multimodal-understan.html" onclick="toggleFavorite(this, '2505.07263v2', 'Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250508084v1-visually-interpretable-subtask-reasoning-for-visual-question-answeri.html">Visually Interpretable Subtask Reasoning for Visual Question Answering</a></td>
  <td>æå‡ºVISTARä»¥è§£å†³è§†è§‰é—®ç­”ä¸­çš„å¤šæ­¥éª¤æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08084v1" data-paper-url="./papers/250508084v1-visually-interpretable-subtask-reasoning-for-visual-question-answeri.html" onclick="toggleFavorite(this, '2505.08084v1', 'Visually Interpretable Subtask Reasoning for Visual Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250507172v1-critique-before-thinking-mitigating-hallucination-through-rationale-.html">Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning</a></td>
  <td>æå‡ºRe-Criticæ¡†æ¶ä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07172v1" data-paper-url="./papers/250507172v1-critique-before-thinking-mitigating-hallucination-through-rationale-.html" onclick="toggleFavorite(this, '2505.07172v1', 'Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250507721v1-gameplay-highlights-generation.html">Gameplay Highlights Generation</a></td>
  <td>æå‡ºè‡ªåŠ¨ç”Ÿæˆæ¸¸æˆç²¾å½©ç‰‡æ®µä»¥æå‡ç©å®¶åˆ†äº«ä½“éªŒ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07721v1" data-paper-url="./papers/250507721v1-gameplay-highlights-generation.html" onclick="toggleFavorite(this, '2505.07721v1', 'Gameplay Highlights Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250507556v1-self-supervised-event-representations-towards-accurate-real-time-per.html">Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs</a></td>
  <td>æå‡ºè‡ªç›‘ç£äº‹ä»¶è¡¨ç¤ºæ–¹æ³•ä»¥è§£å†³äº‹ä»¶æ•°æ®å¤„ç†æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">TAMP</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07556v1" data-paper-url="./papers/250507556v1-self-supervised-event-representations-towards-accurate-real-time-per.html" onclick="toggleFavorite(this, '2505.07556v1', 'Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250514699v2-benchmarking-graph-neural-networks-for-document-layout-analysis-in-p.html">Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs</a></td>
  <td>åŸºäºå›¾ç¥ç»ç½‘ç»œçš„æ–‡æ¡£å¸ƒå±€åˆ†ææ–¹æ³•æå‡å…¬å…±äº‹åŠ¡æ–‡æ¡£å¤„ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.14699v2" data-paper-url="./papers/250514699v2-benchmarking-graph-neural-networks-for-document-layout-analysis-in-p.html" onclick="toggleFavorite(this, '2505.14699v2', 'Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250507300v1-l-swag-layer-sample-wise-activation-with-gradients-information-for-z.html">L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers</a></td>
  <td>æå‡ºL-SWAGä»¥è§£å†³é›¶æˆæœ¬ç¥ç»æ¶æ„æœç´¢åœ¨è§†è§‰å˜æ¢å™¨ä¸­çš„åº”ç”¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07300v1" data-paper-url="./papers/250507300v1-l-swag-layer-sample-wise-activation-with-gradients-information-for-z.html" onclick="toggleFavorite(this, '2505.07300v1', 'L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250507256v1-synthetic-similarity-search-in-automotive-production.html">Synthetic Similarity Search in Automotive Production</a></td>
  <td>æå‡ºåŸºäºåˆæˆæ•°æ®çš„ç›¸ä¼¼æ€§æœç´¢ä»¥ä¼˜åŒ–æ±½è½¦ç”Ÿäº§è´¨é‡æ£€æµ‹</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07256v1" data-paper-url="./papers/250507256v1-synthetic-similarity-search-in-automotive-production.html" onclick="toggleFavorite(this, '2505.07256v1', 'Synthetic Similarity Search in Automotive Production')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250507396v2-tum2twin-introducing-the-large-scale-multimodal-urban-digital-twin-b.html">TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</a></td>
  <td>æå‡ºTUM2TWINä»¥è§£å†³åŸå¸‚æ•°å­—åŒèƒèƒæ•°æ®é›†ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07396v2" data-paper-url="./papers/250507396v2-tum2twin-introducing-the-large-scale-multimodal-urban-digital-twin-b.html" onclick="toggleFavorite(this, '2505.07396v2', 'TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250508124v2-slag-scalable-language-augmented-gaussian-splatting.html">SLAG: Scalable Language-Augmented Gaussian Splatting</a></td>
  <td>æå‡ºSLAGä»¥è§£å†³å¤§è§„æ¨¡åœºæ™¯ç¼–ç æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08124v2" data-paper-url="./papers/250508124v2-slag-scalable-language-augmented-gaussian-splatting.html" onclick="toggleFavorite(this, '2505.08124v2', 'SLAG: Scalable Language-Augmented Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250508811v1-tugs-physics-based-compact-representation-of-underwater-scenes-by-te.html">TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian</a></td>
  <td>æå‡ºTUGSä»¥è§£å†³å¤æ‚æ°´ä¸‹åœºæ™¯é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08811v1" data-paper-url="./papers/250508811v1-tugs-physics-based-compact-representation-of-underwater-scenes-by-te.html" onclick="toggleFavorite(this, '2505.08811v1', 'TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250507539v1-gifstream-4d-gaussian-based-immersive-video-with-feature-stream.html">GIFStream: 4D Gaussian-based Immersive Video with Feature Stream</a></td>
  <td>æå‡ºGIFStreamä»¥è§£å†³æ²‰æµ¸è§†é¢‘å­˜å‚¨ä¸è´¨é‡å¹³è¡¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07539v1" data-paper-url="./papers/250507539v1-gifstream-4d-gaussian-based-immersive-video-with-feature-stream.html" onclick="toggleFavorite(this, '2505.07539v1', 'GIFStream: 4D Gaussian-based Immersive Video with Feature Stream')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250507373v1-geometric-prior-guided-neural-implicit-surface-reconstruction-in-the.html">Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild</a></td>
  <td>æå‡ºå‡ ä½•å…ˆéªŒå¼•å¯¼çš„ç¥ç»éšå¼è¡¨é¢é‡å»ºæ–¹æ³•ä»¥è§£å†³å¤æ‚åœºæ™¯é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07373v1" data-paper-url="./papers/250507373v1-geometric-prior-guided-neural-implicit-surface-reconstruction-in-the.html" onclick="toggleFavorite(this, '2505.07373v1', 'Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250508126v1-asynchronous-multi-object-tracking-with-an-event-camera.html">Asynchronous Multi-Object Tracking with an Event Camera</a></td>
  <td>æå‡ºå¼‚æ­¥äº‹ä»¶å¤šç›®æ ‡è·Ÿè¸ªç®—æ³•ä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸‹çš„ç›®æ ‡æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08126v1" data-paper-url="./papers/250508126v1-asynchronous-multi-object-tracking-with-an-event-camera.html" onclick="toggleFavorite(this, '2505.08126v1', 'Asynchronous Multi-Object Tracking with an Event Camera')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250507611v2-deep-learning-advances-in-vision-based-traffic-accident-anticipation.html">Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods, Datasets, and Future Directions</a></td>
  <td>ç»¼è¿°æ·±åº¦å­¦ä¹ åœ¨åŸºäºè§†è§‰çš„äº¤é€šäº‹æ•…é¢„æµ‹ä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07611v2" data-paper-url="./papers/250507611v2-deep-learning-advances-in-vision-based-traffic-accident-anticipation.html" onclick="toggleFavorite(this, '2505.07611v2', 'Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods, Datasets, and Future Directions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250507984v2-samchat-introducing-chain-of-thought-reasoning-and-grpo-to-a-multimo.html">SAMChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Small Scale Remote Sensing</a></td>
  <td>æå‡ºSAMChatä»¥è§£å†³å°è§„æ¨¡é¥æ„Ÿå›¾åƒåˆ†æé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07984v2" data-paper-url="./papers/250507984v2-samchat-introducing-chain-of-thought-reasoning-and-grpo-to-a-multimo.html" onclick="toggleFavorite(this, '2505.07984v2', 'SAMChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Small Scale Remote Sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250507500v1-learning-to-reason-and-navigate-parameter-efficient-action-planning-.html">Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models</a></td>
  <td>æå‡ºPEAP-LLMä»¥è§£å†³å¤æ‚å®¤å†…å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">DPO</span> <span class="paper-tag">direct preference optimization</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07500v1" data-paper-url="./papers/250507500v1-learning-to-reason-and-navigate-parameter-efficient-action-planning-.html" onclick="toggleFavorite(this, '2505.07500v1', 'Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250507818v4-dancegrpo-unleashing-grpo-on-visual-generation.html">DanceGRPO: Unleashing GRPO on Visual Generation</a></td>
  <td>æå‡ºDanceGRPOä»¥è§£å†³è§†è§‰ç”Ÿæˆä¸­çš„ä¼˜åŒ–ç¨³å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">RLHF</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07818v4" data-paper-url="./papers/250507818v4-dancegrpo-unleashing-grpo-on-visual-generation.html" onclick="toggleFavorite(this, '2505.07818v4', 'DanceGRPO: Unleashing GRPO on Visual Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250507322v3-realrep-generalized-sdr-to-hdr-conversion-via-attribute-disentangled.html">RealRep: Generalized SDR-to-HDR Conversion via Attribute-Disentangled Representation Learning</a></td>
  <td>æå‡ºRealRepä»¥è§£å†³SDRåˆ°HDRè½¬æ¢ä¸­çš„è¡¨ç°å¤šæ ·æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07322v3" data-paper-url="./papers/250507322v3-realrep-generalized-sdr-to-hdr-conversion-via-attribute-disentangled.html" onclick="toggleFavorite(this, '2505.07322v3', 'RealRep: Generalized SDR-to-HDR Conversion via Attribute-Disentangled Representation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250507375v1-boosting-global-local-feature-matching-via-anomaly-synthesis-for-mul.html">Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection</a></td>
  <td>æå‡ºGLFMæ–¹æ³•ä»¥è§£å†³å¤šç±»ç‚¹äº‘å¼‚å¸¸æ£€æµ‹ä¸­çš„ç‰¹å¾æ··æ·†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07375v1" data-paper-url="./papers/250507375v1-boosting-global-local-feature-matching-via-anomaly-synthesis-for-mul.html" onclick="toggleFavorite(this, '2505.07375v1', 'Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250507715v1-hybrid-spiking-vision-transformer-for-object-detection-with-event-ca.html">Hybrid Spiking Vision Transformer for Object Detection with Event Cameras</a></td>
  <td>æå‡ºæ··åˆè„‰å†²è§†è§‰å˜æ¢å™¨ä»¥è§£å†³äº‹ä»¶æ‘„åƒå¤´ç‰©ä½“æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07715v1" data-paper-url="./papers/250507715v1-hybrid-spiking-vision-transformer-for-object-detection-with-event-ca.html" onclick="toggleFavorite(this, '2505.07715v1', 'Hybrid Spiking Vision Transformer for Object Detection with Event Cameras')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)