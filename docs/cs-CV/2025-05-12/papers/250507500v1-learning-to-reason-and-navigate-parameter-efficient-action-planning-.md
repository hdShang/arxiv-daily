---
layout: default
title: "Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models"
---

# Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07500" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07500v1</a>
  <a href="https://arxiv.org/pdf/2505.07500.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07500v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07500v1', 'Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bahram Mohammadi, Ehsan Abbasnejad, Yuankai Qi, Qi Wu, Anton Van Den Hengel, Javen Qinfeng Shi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPEAP-LLMä»¥è§£å†³å¤æ‚å®¤å†…å¯¼èˆªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¯¼èˆªè§„åˆ’` `è¿œç¨‹æŒ‡ä»£` `æœºå™¨äººæŠ€æœ¯` `æ™ºèƒ½å®¶å±…` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­å®¹æ˜“å‡ºé”™ï¼Œä¸”éœ€è¦äººå·¥å¹²é¢„ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆçš„å¯¼èˆªè§„åˆ’ã€‚
2. æœ¬æ–‡æå‡ºPEAP-LLMï¼Œé€šè¿‡LGPå’ŒLAPæ¨¡å—å®ç°é«˜æ•ˆçš„å•æ­¥æŒ‡ä»¤ç”Ÿæˆï¼Œè§£å†³äº†å¤æ‚å¯¼èˆªä¸­çš„æŒ‡ä»¤ç”Ÿæˆé—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPEAP-LLMåœ¨REVERIEä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿œç¨‹å…·èº«æŒ‡ä»£è¡¨è¾¾ï¼ˆREVERIEï¼‰ä»»åŠ¡è¦æ±‚ä»£ç†åœ¨å¤æ‚çš„å®¤å†…ç¯å¢ƒä¸­å¯¼èˆªï¼Œå¹¶æ ¹æ®é«˜å±‚æŒ‡ä»¤å®šä½è¿œç¨‹ç‰©ä½“ï¼Œå¦‚â€œç»™æˆ‘æ‹¿ä¸€ä¸ªå‹ºå­â€ï¼Œè€Œæ— éœ€é¢„å…ˆæ¢ç´¢ã€‚å› æ­¤ï¼Œé«˜æ•ˆçš„å¯¼èˆªè®¡åˆ’å¯¹æœ€ç»ˆæˆåŠŸè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆè¡ŒåŠ¨è§„åˆ’å™¨ï¼ˆPEAP-LLMï¼‰ï¼Œåœ¨æ¯ä¸ªä½ç½®ç”Ÿæˆå•æ­¥æŒ‡ä»¤ã€‚è¯¥æ¨¡å‹ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šLLMç›®æ ‡è§„åˆ’å™¨ï¼ˆLGPï¼‰å’ŒLoRAè¡ŒåŠ¨è§„åˆ’å™¨ï¼ˆLAPï¼‰ã€‚LGPä»REVERIEæŒ‡ä»¤ä¸­æå–ç›®æ ‡å¯¼å‘çš„è®¡åˆ’ï¼Œè€ŒLAPåˆ™ç»“åˆç›®æ ‡å¯¼å‘è®¡åˆ’ã€é«˜å±‚æŒ‡ä»¤å’Œå½“å‰è§†è§‰è§‚å¯Ÿç”Ÿæˆå•æ­¥æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨REVERIEä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è¿œç¨‹å…·èº«æŒ‡ä»£è¡¨è¾¾ä»»åŠ¡ä¸­çš„å¯¼èˆªè§„åˆ’é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“äº§ç”Ÿé”™è¯¯ï¼Œä¸”éœ€è¦äººå·¥å¹²é¢„ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„PEAP-LLMé€šè¿‡ä¸¤ä¸ªæ¨¡å—ï¼ˆLGPå’ŒLAPï¼‰å®ç°é«˜æ•ˆçš„å•æ­¥æŒ‡ä»¤ç”Ÿæˆï¼Œæ—¨åœ¨æé«˜å¯¼èˆªçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬LGPæ¨¡å—ç”¨äºæå–ç›®æ ‡å¯¼å‘è®¡åˆ’ï¼ŒLAPæ¨¡å—ç”¨äºç”ŸæˆåŸºäºå½“å‰ç¯å¢ƒçš„å•æ­¥æŒ‡ä»¤ã€‚ä»£ç†åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­åŠ¨æ€ä¸LAPäº¤äº’ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸¤é˜¶æ®µçš„å¾®è°ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSTFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œæœ‰æ•ˆå‡å°‘äº†ç”Ÿæˆçš„å¹»è§‰å’Œåè§ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSTFç”¨äºæé«˜æŒ‡ä»¤è´¨é‡ï¼ŒDPOåˆ©ç”¨ç¯å¢ƒåé¦ˆè¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿ç”ŸæˆæŒ‡ä»¤çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPEAP-LLMåœ¨REVERIEä»»åŠ¡ä¸Šç›¸è¾ƒäºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“è¡¨ç°ä¸ºæˆåŠŸç‡æé«˜äº†XX%ï¼Œç”ŸæˆæŒ‡ä»¤çš„å‡†ç¡®æ€§å’Œæ•ˆç‡å‡æœ‰æ˜æ˜¾æ”¹å–„ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€æœºå™¨äººå¯¼èˆªå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œå·¥ä½œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as "bring me a spoon", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art.

