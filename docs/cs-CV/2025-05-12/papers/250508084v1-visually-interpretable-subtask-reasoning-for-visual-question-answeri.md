---
layout: default
title: Visually Interpretable Subtask Reasoning for Visual Question Answering
---

# Visually Interpretable Subtask Reasoning for Visual Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08084" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08084v1</a>
  <a href="https://arxiv.org/pdf/2505.08084.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08084v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08084v1', 'Visually Interpretable Subtask Reasoning for Visual Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Cheng, Arushi Goel, Hakan Bilen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ChengJade/VISTAR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVISTARä»¥è§£å†³è§†è§‰é—®ç­”ä¸­çš„å¤šæ­¥éª¤æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰é—®ç­”` `å¤šæ­¥éª¤æ¨ç†` `å¯è§£é‡Šæ€§` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å­ä»»åŠ¡é©±åŠ¨` `æ¨¡å‹å¾®è°ƒ` `æ¨ç†ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è§†è§‰é—®ç­”ä¸­é¢ä¸´è®¡ç®—æˆæœ¬é«˜å’Œå‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ï¼Œéš¾ä»¥é€‚åº”ç›®æ ‡æ•°æ®ã€‚
2. VISTARé€šè¿‡å­ä»»åŠ¡é©±åŠ¨çš„è®­ç»ƒæ¡†æ¶ï¼Œç”Ÿæˆç»“æ„åŒ–çš„æ¨ç†åºåˆ—ï¼Œæå‡äº†å¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVISTARåœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡æé«˜äº†æ¨ç†å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„å¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å›ç­”å¤æ‚çš„è§†è§‰é—®é¢˜ï¼Œå¦‚â€˜å“ªä»¶çº¢è‰²å®¶å…·å¯ä»¥åï¼Ÿâ€™ï¼Œéœ€è¦å¤šæ­¥éª¤æ¨ç†ï¼ŒåŒ…æ‹¬ç‰©ä½“è¯†åˆ«ã€å±æ€§è¿‡æ»¤å’Œå…³ç³»ç†è§£ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ç¨‹åºæ¥æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¯è§£é‡Šæ€§ï¼Œä½†è¿™äº›æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”é€‚åº”ç›®æ ‡æ•°æ®çš„èƒ½åŠ›è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VISTARï¼ˆå¯è§†åŒ–å¯è§£é‡Šå­ä»»åŠ¡é©±åŠ¨æ¨ç†æ¨¡å‹ï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰è§£é‡Šæ¥å¢å¼ºå¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚VISTARé€šè¿‡å¾®è°ƒMLLMsç”Ÿæˆç»“æ„åŒ–çš„æ€ç»´å­ä»»åŠ¡æ¨ç†åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVISTARåœ¨ä¿æŒå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰é—®ç­”ä¸­çš„å¤šæ­¥éª¤æ¨ç†é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç”±äºè®¡ç®—å¤æ‚æ€§å’Œé€‚åº”æ€§ä¸è¶³ï¼Œå¯¼è‡´æ¨ç†å‡†ç¡®æ€§ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVISTARé€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œç”Ÿæˆç»“æ„åŒ–çš„å­ä»»åŠ¡æ¨ç†åºåˆ—ï¼Œä»è€Œæé«˜æ¨ç†çš„å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVISTARçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹å¾®è°ƒå’Œæ¨ç†ç”Ÿæˆä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œæ¨¡å‹æ¥æ”¶è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼›åœ¨å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ ç”Ÿæˆå­ä»»åŠ¡æ¨ç†åºåˆ—ï¼›æœ€åï¼Œåœ¨æ¨ç†ç”Ÿæˆé˜¶æ®µï¼Œæ¨¡å‹è¾“å‡ºå¯è§£é‡Šçš„ç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šVISTARçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å­ä»»åŠ¡é©±åŠ¨çš„è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œç›´æ¥åœ¨MLLMsä¸­ç”Ÿæˆç»“æ„åŒ–çš„æ¨ç†åºåˆ—ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ä¾èµ–å¤–éƒ¨æ¨¡å‹çš„è®¾è®¡æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒVISTARé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨ç†åºåˆ—çš„ç”Ÿæˆï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„èåˆèƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVISTARæ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ¨ç†å‡†ç¡®ç‡æå‡äº†çº¦15%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVISTARåœ¨ä¿æŒå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„è§†è§‰é—®ç­”ä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VISTARçš„ç ”ç©¶æˆæœåœ¨è§†è§‰é—®ç­”ã€æ™ºèƒ½åŠ©æ‰‹å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å¤æ‚çš„è§†è§‰ä¿¡æ¯ï¼Œæå‡äº¤äº’ä½“éªŒã€‚æœªæ¥ï¼ŒVISTARè¿˜å¯èƒ½åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.

