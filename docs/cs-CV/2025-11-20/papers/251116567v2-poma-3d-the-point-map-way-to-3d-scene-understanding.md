---
layout: default
title: POMA-3D: The Point Map Way to 3D Scene Understanding
---

# POMA-3D: The Point Map Way to 3D Scene Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.16567" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.16567v2</a>
  <a href="https://arxiv.org/pdf/2511.16567.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16567v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.16567v2', 'POMA-3D: The Point Map Way to 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ye Mao, Weixun Luo, Ranran Huang, Junpeng Jing, Krystian Mikolajczyk

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20 (æ›´æ–°: 2025-11-21)

**å¤‡æ³¨**: 11 pages, 6 tables, 5 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://matchlab-imperial.github.io/poma3d/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**POMA-3Dï¼šæå‡ºåŸºäºç‚¹å›¾çš„è‡ªç›‘ç£3Dåœºæ™¯ç†è§£æ¨¡å‹ï¼Œæå‡å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dåœºæ™¯ç†è§£` `ç‚¹å›¾è¡¨ç¤º` `è‡ªç›‘ç£å­¦ä¹ ` `é¢„è®­ç»ƒæ¨¡å‹` `å‡ ä½•è¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dè¡¨ç¤ºå­¦ä¹ ç¼ºä¹æœ‰æ•ˆçš„é¢„è®­ç»ƒæ–¹æ³•å’Œå¤§è§„æ¨¡æ•°æ®é›†ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. POMA-3Dåˆ©ç”¨ç‚¹å›¾å°†3Dä¿¡æ¯ç¼–ç ä¸º2Dç»“æ„ï¼Œå¹¶è®¾è®¡è§†è§’å¯¹é½ç­–ç•¥å’Œè”åˆåµŒå…¥é¢„æµ‹æ¶æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒPOMA-3Dåœ¨3Dé—®ç­”ã€å¯¼èˆªç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºé€šç”¨3Déª¨å¹²ç½‘ç»œçš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºPOMA-3Dï¼Œé¦–ä¸ªä»ç‚¹å›¾å­¦ä¹ çš„è‡ªç›‘ç£3Dè¡¨ç¤ºæ¨¡å‹ã€‚ç‚¹å›¾åœ¨ç»“æ„åŒ–çš„2Dç½‘æ ¼ä¸Šç¼–ç æ˜¾å¼çš„3Dåæ ‡ï¼Œä¿ç•™å…¨å±€3Då‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶å…¼å®¹2DåŸºç¡€æ¨¡å‹çš„è¾“å…¥æ ¼å¼ã€‚ä¸ºäº†å°†ä¸°å¯Œçš„2Då…ˆéªŒçŸ¥è¯†è¿ç§»åˆ°POMA-3Dä¸­ï¼Œè®¾è®¡äº†ä¸€ç§è§†è§’åˆ°åœºæ™¯çš„å¯¹é½ç­–ç•¥ã€‚æ­¤å¤–ï¼Œç”±äºç‚¹å›¾ç›¸å¯¹äºè§„èŒƒç©ºé—´æ˜¯è§†è§’ç›¸å…³çš„ï¼Œæˆ‘ä»¬å¼•å…¥POMA-JEPAï¼Œä¸€ç§è”åˆåµŒå…¥-é¢„æµ‹æ¶æ„ï¼Œç”¨äºåœ¨å¤šä¸ªè§†è§’ä¸Šå¼ºåˆ¶æ‰§è¡Œå‡ ä½•ä¸€è‡´çš„ç‚¹å›¾ç‰¹å¾ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ„å»ºäº†ScenePointæ•°æ®é›†ï¼ŒåŒ…å«6.5Kä¸ªæˆ¿é—´çº§RGB-Dåœºæ™¯å’Œ1Mä¸ª2Då›¾åƒåœºæ™¯ï¼Œä»¥ä¿ƒè¿›å¤§è§„æ¨¡POMA-3Dé¢„è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒPOMA-3Då¯ä»¥ä½œä¸ºä¸“å®¶å’Œé€šç”¨3Dç†è§£çš„å¼ºå¤§éª¨å¹²ç½‘ç»œï¼Œå¹¶èƒ½æå‡åŒ…æ‹¬3Dé—®ç­”ã€å…·èº«å¯¼èˆªã€åœºæ™¯æ£€ç´¢å’Œå…·èº«å®šä½ç­‰å¤šç§ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä»…ä½¿ç”¨å‡ ä½•è¾“å…¥ï¼ˆå³3Dåæ ‡ï¼‰ã€‚æ€»è€Œè¨€ä¹‹ï¼ŒPOMA-3Dæ¢ç´¢äº†ä¸€ç§åŸºäºç‚¹å›¾çš„3Dåœºæ™¯ç†è§£æ–¹æ³•ï¼Œè§£å†³äº†3Dè¡¨ç¤ºå­¦ä¹ ä¸­é¢„è®­ç»ƒå…ˆéªŒçŸ¥è¯†åŒ®ä¹å’Œæ•°æ®æœ‰é™çš„é—®é¢˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„3Dåœºæ™¯ç†è§£æ–¹æ³•é¢ä¸´ç€ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹æœ‰æ•ˆçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´æ¨¡å‹éœ€è¦ä»å¤´å¼€å§‹å­¦ä¹ ï¼Œæ•ˆç‡ä½ä¸‹ï¼›äºŒæ˜¯3Dæ•°æ®çš„è·å–æˆæœ¬é«˜æ˜‚ï¼Œå¯¼è‡´è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨æœ‰é™çš„3Dæ•°æ®ï¼Œå­¦ä¹ åˆ°é€šç”¨çš„ã€å¯è¿ç§»çš„3Dè¡¨ç¤ºï¼Œæ˜¯å½“å‰3Dåœºæ™¯ç†è§£é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPOMA-3Dçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†3Dç‚¹äº‘æ•°æ®è½¬æ¢ä¸º2Dç‚¹å›¾è¡¨ç¤ºï¼Œä»è€Œèƒ½å¤Ÿåˆ©ç”¨åœ¨2Då›¾åƒé¢†åŸŸé¢„è®­ç»ƒçš„å¼ºå¤§æ¨¡å‹ã€‚é€šè¿‡å°†3Dåæ ‡æ˜ å°„åˆ°2Dç½‘æ ¼ä¸Šï¼Œä¿ç•™äº†3Dåœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼ŒåŒæ—¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨2Då·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œç‰¹å¾æå–ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ç‚¹å›¾çš„è§†è§’ä¾èµ–æ€§é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†è§†è§’å¯¹é½ç­–ç•¥å’Œè”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼Œä»¥ä¿è¯æ¨¡å‹å­¦ä¹ åˆ°çš„ç‰¹å¾å…·æœ‰è§†è§’ä¸å˜æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPOMA-3Dçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šç‚¹å›¾ç”Ÿæˆã€ç‰¹å¾æå–å’Œè”åˆåµŒå…¥é¢„æµ‹ã€‚é¦–å…ˆï¼Œå°†3Dç‚¹äº‘æ•°æ®æŠ•å½±åˆ°å¤šä¸ªè§†è§’ï¼Œç”Ÿæˆå¯¹åº”çš„ç‚¹å›¾ã€‚ç„¶åï¼Œä½¿ç”¨2Då·ç§¯ç¥ç»ç½‘ç»œï¼ˆä¾‹å¦‚ï¼ŒVision Transformerï¼‰ä»ç‚¹å›¾ä¸­æå–ç‰¹å¾ã€‚æœ€åï¼Œé€šè¿‡POMA-JEPAæ¶æ„ï¼Œåˆ©ç”¨å¤šä¸ªè§†è§’çš„ç‚¹å›¾ç‰¹å¾è¿›è¡Œè”åˆåµŒå…¥é¢„æµ‹ï¼Œä»è€Œå­¦ä¹ åˆ°å…·æœ‰è§†è§’ä¸å˜æ€§çš„3Dè¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šPOMA-3Dçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ç‚¹ï¼šä¸€æ˜¯æå‡ºäº†ç‚¹å›¾è¡¨ç¤ºï¼Œå°†3Dæ•°æ®è½¬æ¢ä¸º2Dç»“æ„ï¼Œä»è€Œèƒ½å¤Ÿåˆ©ç”¨2Dé¢„è®­ç»ƒæ¨¡å‹ï¼›äºŒæ˜¯è®¾è®¡äº†è§†è§’å¯¹é½ç­–ç•¥å’ŒPOMA-JEPAæ¶æ„ï¼Œè§£å†³äº†ç‚¹å›¾çš„è§†è§’ä¾èµ–æ€§é—®é¢˜ï¼›ä¸‰æ˜¯æ„å»ºäº†å¤§è§„æ¨¡çš„ScenePointæ•°æ®é›†ï¼Œä¸º3Dè¡¨ç¤ºå­¦ä¹ æä¾›äº†å……è¶³çš„è®­ç»ƒæ•°æ®ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPOMA-3Dèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´é€šç”¨çš„ã€å¯è¿ç§»çš„3Dè¡¨ç¤ºï¼Œä»è€Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‚¹å›¾ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„æŠ•å½±æ–¹å¼å’Œåˆ†è¾¨ç‡ï¼Œä»¥ä¿è¯ç‚¹å›¾èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿ç•™3Då‡ ä½•ä¿¡æ¯ã€‚åœ¨POMA-JEPAæ¶æ„ä¸­ï¼Œä½¿ç”¨äº†InfoNCEæŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ åˆ°å…·æœ‰è§†è§’ä¸å˜æ€§çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPOMA-3Dåœ¨3Dé—®ç­”ã€å…·èº«å¯¼èˆªã€åœºæ™¯æ£€ç´¢å’Œå…·èº«å®šä½ç­‰å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨3Dé—®ç­”ä»»åŠ¡ä¸­ï¼ŒPOMA-3Dç›¸æ¯”äºç°æœ‰æ–¹æ³•å–å¾—äº†è¶…è¿‡5%çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒPOMA-3Dåœ¨ScenePointæ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ¨¡å‹åœ¨å…¶ä»–æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

POMA-3Dåœ¨æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¸®åŠ©è‡ªåŠ¨é©¾é©¶è½¦è¾†æ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶ä¸ºè™šæ‹Ÿç°å®åº”ç”¨æä¾›æ›´é€¼çœŸçš„3Dåœºæ™¯è¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒPOMA-3Dè¿˜å¯ä»¥ç”¨äº3Dåœºæ™¯æ£€ç´¢ã€3Dé—®ç­”ç­‰ä»»åŠ¡ï¼Œä¸ºç”¨æˆ·æä¾›æ›´æ™ºèƒ½åŒ–çš„æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/

