---
layout: default
title: Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight
---

# Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.16175" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.16175v1</a>
  <a href="https://arxiv.org/pdf/2511.16175.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16175v1" onclick="toggleFavorite(this, '2511.16175v1', 'Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Zipeng Xiao, Jiadi Su, You Qiaoben, Pengfei Liu, Zhijie Deng

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Mantisï¼šä¸€ç§å…·æœ‰è§£è€¦è§†è§‰é¢„æµ‹çš„å¤šåŠŸèƒ½è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `è§£è€¦è§†è§‰é¢„æµ‹` `æ‰©æ•£Transformer` `æœºå™¨äººæ“ä½œ` `æŒ‡ä»¤è·Ÿéš` `è§†è§‰é¢„æµ‹` `å…ƒæŸ¥è¯¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ç›´æ¥é¢„æµ‹é«˜ç»´è§†è§‰çŠ¶æ€æˆ–å‹ç¼©è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹èƒ½åŠ›ä¸‹é™å’Œä¿¡æ¯ç“¶é¢ˆã€‚
2. Mantisé€šè¿‡è§£è€¦è§†è§‰é¢„æµ‹ï¼Œåˆ©ç”¨å…ƒæŸ¥è¯¢å’Œæ‰©æ•£Transformerï¼Œé™ä½éª¨å¹²ç½‘ç»œè´Ÿæ‹…ï¼Œæå‡ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚
3. Mantisåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°96.7%çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ä¼˜äºç°æœ‰VLAæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMantisï¼Œä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯è§£è€¦è§†è§‰é¢„æµ‹ï¼ˆDVFï¼‰ã€‚ç°æœ‰VLAæ¨¡å‹ç›´æ¥é¢„æµ‹é«˜ç»´è§†è§‰çŠ¶æ€ä¼šåˆ†æ•£æ¨¡å‹èƒ½åŠ›å¹¶å¯¼è‡´è¿‡é«˜çš„è®­ç»ƒæˆæœ¬ï¼Œè€Œå°†è§†è§‰çŠ¶æ€å‹ç¼©ä¸ºæ›´ç´§å‡‘çš„ç›‘ç£ä¿¡å·ä¸å¯é¿å…åœ°ä¼šé€ æˆä¿¡æ¯ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œç”±äºå¿½ç•¥äº†è¯­è¨€ç›‘ç£ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å­˜åœ¨è¾ƒå·®çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚Mantisé€šè¿‡å…ƒæŸ¥è¯¢å’Œæ‰©æ•£Transformerï¼ˆDiTï¼‰å¤´çš„ç»„åˆï¼Œå°†è§†è§‰é¢„æµ‹ä¸éª¨å¹²ç½‘ç»œè§£è€¦ã€‚é€šè¿‡æ®‹å·®è¿æ¥å°†å½“å‰è§†è§‰çŠ¶æ€æä¾›ç»™DiTï¼Œä¸€ä¸ªç®€å•çš„ä¸‹ä¸€çŠ¶æ€é¢„æµ‹ç›®æ ‡ä½¿å…ƒæŸ¥è¯¢èƒ½å¤Ÿè‡ªåŠ¨æ•è·æç»˜è§†è§‰è½¨è¿¹çš„æ½œåœ¨åŠ¨ä½œï¼Œä»è€Œä¿ƒè¿›æ˜¾å¼åŠ¨ä½œçš„å­¦ä¹ ã€‚è¿™ç§è§£è€¦é™ä½äº†VLAéª¨å¹²ç½‘ç»œçš„è´Ÿæ‹…ï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡è¯­è¨€ç›‘ç£ä¿æŒç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨äººç±»æ“ä½œè§†é¢‘ã€æœºå™¨äººæ¼”ç¤ºå’Œå›¾åƒ-æ–‡æœ¬å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼ŒMantisåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­ç»è¿‡å¾®è°ƒåè¾¾åˆ°äº†96.7%çš„æˆåŠŸç‡ï¼Œè¶…è¿‡äº†å¼ºå¤§çš„åŸºçº¿ï¼ŒåŒæ—¶è¡¨ç°å‡ºå¾ˆé«˜çš„æ”¶æ•›é€Ÿåº¦ã€‚çœŸå®ä¸–ç•Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒMantisä¼˜äºé¢†å…ˆçš„å¼€æºVLAæ¨¡å‹$Ï€_{0.5}$ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€å¯¹æœªè§æŒ‡ä»¤çš„æ³›åŒ–ä»¥åŠæ¨ç†èƒ½åŠ›æ–¹é¢ã€‚ä»£ç å’Œæƒé‡å·²å‘å¸ƒä»¥æ”¯æŒå¼€æºç¤¾åŒºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤„ç†é«˜ç»´è§†è§‰çŠ¶æ€é¢„æµ‹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç›´æ¥é¢„æµ‹å¯¼è‡´æ¨¡å‹èƒ½åŠ›åˆ†æ•£å’Œè®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œè€Œå‹ç¼©è§†è§‰ä¿¡æ¯åˆ™ä¼šé€ æˆä¿¡æ¯ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è¯­è¨€ç›‘ç£ï¼Œå¯¼è‡´ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMantisçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è§£è€¦è§†è§‰é¢„æµ‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†è§†è§‰é¢„æµ‹ä»»åŠ¡ä»VLAéª¨å¹²ç½‘ç»œä¸­åˆ†ç¦»å‡ºæ¥ï¼Œåˆ©ç”¨å…ƒæŸ¥è¯¢å’Œæ‰©æ•£Transformerï¼ˆDiTï¼‰å¤´æ¥é¢„æµ‹ä¸‹ä¸€çŠ¶æ€çš„è§†è§‰ä¿¡æ¯ã€‚è¿™ç§è§£è€¦é™ä½äº†éª¨å¹²ç½‘ç»œçš„è´Ÿæ‹…ï¼Œä½¿å…¶èƒ½å¤Ÿä¸“æ³¨äºè¯­è¨€ç†è§£å’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMantisçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªVLAéª¨å¹²ç½‘ç»œå’Œä¸€ä¸ªè§£è€¦çš„è§†è§‰é¢„æµ‹æ¨¡å—ã€‚VLAéª¨å¹²ç½‘ç»œè´Ÿè´£å¤„ç†è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œå¹¶ç”ŸæˆåŠ¨ä½œæŒ‡ä»¤ã€‚è§£è€¦çš„è§†è§‰é¢„æµ‹æ¨¡å—åˆ™åˆ©ç”¨å…ƒæŸ¥è¯¢å’ŒDiTå¤´æ¥é¢„æµ‹ä¸‹ä¸€çŠ¶æ€çš„è§†è§‰ä¿¡æ¯ã€‚å½“å‰è§†è§‰çŠ¶æ€é€šè¿‡æ®‹å·®è¿æ¥æä¾›ç»™DiTï¼Œå…ƒæŸ¥è¯¢è´Ÿè´£æ•è·æ½œåœ¨åŠ¨ä½œï¼Œä»è€ŒæŒ‡å¯¼è§†è§‰è½¨è¿¹çš„é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMantisçš„å…³é”®åˆ›æ–°åœ¨äºè§£è€¦è§†è§‰é¢„æµ‹ï¼ˆDVFï¼‰ã€‚ä¸ç°æœ‰æ–¹æ³•ç›´æ¥é¢„æµ‹æˆ–å‹ç¼©è§†è§‰çŠ¶æ€ä¸åŒï¼ŒMantiså°†è§†è§‰é¢„æµ‹ä»»åŠ¡åˆ†ç¦»å‡ºæ¥ï¼Œåˆ©ç”¨å…ƒæŸ¥è¯¢å’ŒDiTå¤´è¿›è¡Œé¢„æµ‹ã€‚è¿™ç§è§£è€¦é™ä½äº†éª¨å¹²ç½‘ç»œçš„è´Ÿæ‹…ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è¯­è¨€ç›‘ç£ï¼Œä»è€Œæå‡ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šMantisçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å…ƒæŸ¥è¯¢æ¥æ•è·æ½œåœ¨åŠ¨ä½œï¼Œä»è€ŒæŒ‡å¯¼è§†è§‰è½¨è¿¹çš„é¢„æµ‹ï¼›2) ä½¿ç”¨æ‰©æ•£Transformerï¼ˆDiTï¼‰å¤´æ¥é¢„æµ‹ä¸‹ä¸€çŠ¶æ€çš„è§†è§‰ä¿¡æ¯ï¼›3) é€šè¿‡æ®‹å·®è¿æ¥å°†å½“å‰è§†è§‰çŠ¶æ€æä¾›ç»™DiTï¼Œä»è€Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ï¼›4) ä½¿ç”¨ç®€å•çš„ä¸‹ä¸€çŠ¶æ€é¢„æµ‹ç›®æ ‡æ¥è®­ç»ƒè§†è§‰é¢„æµ‹æ¨¡å—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Mantisåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œç»è¿‡å¾®è°ƒåæˆåŠŸç‡è¾¾åˆ°96.7%ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿æ¨¡å‹ã€‚åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ï¼ŒMantisåœ¨æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€å¯¹æœªè§æŒ‡ä»¤çš„æ³›åŒ–ä»¥åŠæ¨ç†èƒ½åŠ›æ–¹é¢å‡ä¼˜äºé¢†å…ˆçš„å¼€æºVLAæ¨¡å‹$Ï€_{0.5}$ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMantiså…·æœ‰å¼ºå¤§çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Mantiså…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡ç†è§£äººç±»æŒ‡ä»¤å¹¶é¢„æµ‹æœªæ¥è§†è§‰çŠ¶æ€ï¼ŒMantiså¯ä»¥ä½¿æœºå™¨äººæ›´æ™ºèƒ½åœ°æ‰§è¡Œå¤æ‚ä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©å“æŠ“å–ã€è£…é…ç­‰ã€‚æ­¤å¤–ï¼ŒMantisè¿˜å¯ä»¥ç”¨äºè‡ªåŠ¨é©¾é©¶æ±½è½¦çš„åœºæ™¯ç†è§£å’Œè¡Œä¸ºé¢„æµ‹ï¼Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åœ¨è™šæ‹Ÿç°å®é¢†åŸŸï¼ŒMantiså¯ä»¥ç”¨äºç”Ÿæˆæ›´é€¼çœŸçš„è™šæ‹Ÿç¯å¢ƒå’Œäº¤äº’ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $Ï€_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.

