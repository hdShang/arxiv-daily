---
layout: default
title: BOP-ASK: Object-Interaction Reasoning for Vision-Language Models
---

# BOP-ASK: Object-Interaction Reasoning for Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.16857" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.16857v2</a>
  <a href="https://arxiv.org/pdf/2511.16857.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16857v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.16857v2', 'BOP-ASK: Object-Interaction Reasoning for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vineet Bhat, Sungsu Kim, Valts Blukis, Greg Heinrich, Prashanth Krishnamurthy, Ramesh Karri, Stan Birchfield, Farshad Khorrami, Jonathan Tremblay

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20 (æ›´æ–°: 2025-12-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**BOP-ASKï¼šç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹çš„ç›®æ ‡äº¤äº’æ¨ç†æ•°æ®é›†ä¸åŸºå‡†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¯¹è±¡äº¤äº’æ¨ç†` `æ•°æ®é›†` `ç©ºé—´æ¨ç†` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹å¯¹å¯¹è±¡äº¤äº’çš„ç»†ç²’åº¦ç†è§£ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. BOP-ASKæ•°æ®é›†é€šè¿‡åˆ©ç”¨6Då¯¹è±¡å§¿æ€ä¿¡æ¯ï¼Œç”ŸæˆåŒ…å«æŠ“å–å§¿æ€ã€è·¯å¾„è§„åˆ’ç­‰ç»†ç²’åº¦æ ‡æ³¨çš„å¤§è§„æ¨¡é—®ç­”å¯¹ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåœ¨BOP-ASKä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¯¹è±¡å§¿æ€ä¼°è®¡ã€è½¨è¿¹è§„åˆ’å’Œç©ºé—´æ¨ç†æ–¹é¢è¡¨ç°å‡ºä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†è¿™äº›è¯„ä¼°æ©ç›–äº†å…¶åœ¨ç†è§£å¯¹è±¡äº¤äº’æ–¹é¢çš„å…³é”®å¼±ç‚¹ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¾§é‡äºé«˜çº§å…³ç³»ï¼ˆå¦‚â€œå·¦ä¾§â€ã€â€œåæ–¹â€ï¼‰ï¼Œå¿½ç•¥äº†å®é™…åº”ç”¨æ‰€éœ€çš„ç²¾ç»†ç©ºé—´ç†è§£ï¼ŒåŒ…æ‹¬ç²¾ç¡®çš„3Då®šä½ã€å¯¹è±¡é—´çš„ç‰©ç†å…¼å®¹æ€§ã€å¯¹è±¡å¯ä¾›æ€§ä»¥åŠå¤šæ­¥ç©ºé—´è§„åˆ’ã€‚æœ¬æ–‡æå‡ºäº†BOP-ASKï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯¹è±¡äº¤äº’æ¨ç†çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯ç”¨äºè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ã€‚æ•°æ®ç”Ÿæˆæµç¨‹åˆ©ç”¨äº†å¯¹è±¡å§¿æ€ä¼°è®¡åŸºå‡†(BOP)æ•°æ®é›†ä¸­çš„6Då¯¹è±¡å§¿æ€ï¼Œä»ä¸­æå–ç²¾ç»†çš„æ ‡æ³¨ï¼Œå¦‚æŠ“å–å§¿æ€ã€å‚è€ƒå¯¹è±¡å§¿æ€ã€è·¯å¾„è§„åˆ’è½¨è¿¹ã€ç›¸å¯¹ç©ºé—´å’Œæ·±åº¦å…³ç³»ä»¥åŠå¯¹è±¡é—´å…³ç³»ã€‚BOP-ASKåŒ…å«è¶…è¿‡15ä¸‡å¼ å›¾åƒå’Œ3300ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–å…­ä¸ªä»»åŠ¡ï¼ˆå…¶ä¸­å››ä¸ªæ˜¯æ–°çš„ï¼‰ï¼Œä¸ºè®­ç»ƒå’Œè¯„ä¼°VLMæä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸“æœ‰å’Œå¼€æºçš„VLMï¼Œå¹¶å¯¹BOP-ASK-coreï¼ˆä¸€ä¸ªè´¡çŒ®çš„æµ‹è¯•åŸºå‡†ï¼‰è¿›è¡Œäº†äººå·¥è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†BOP-ASK-labï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¤–åŸºå‡†ï¼Œå…¶å›¾åƒå¹¶éæ¥è‡ªBOPï¼Œç”¨äºæµ‹è¯•æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨BOP-ASKä¸Šè®­ç»ƒçš„æ¨¡å‹ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºç²¾ç¡®çš„å¯¹è±¡å’ŒæŠ“å–å§¿æ€ä¼°è®¡ã€è½¨è¿¹è§„åˆ’ä»¥åŠåœ¨æ‚ä¹±ç¯å¢ƒä¸­è¿›è¡Œç²¾ç»†çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç©ºé—´æ¨ç†ç­‰æ–°å…´èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œæ•°æ®é›†ç”Ÿæˆæµç¨‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¸»è¦å…³æ³¨é«˜çº§çš„ç©ºé—´å…³ç³»ï¼Œä¾‹å¦‚â€œå·¦è¾¹â€ã€â€œåé¢â€ç­‰ï¼Œç¼ºä¹å¯¹ç‰©ä½“ä¹‹é—´ç»†ç²’åº¦äº¤äº’å…³ç³»çš„ç†è§£ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨éœ€è¦ç²¾ç¡®3Då®šä½ã€ç‰©ç†å…¼å®¹æ€§åˆ¤æ–­ã€ç‰©ä½“å¯ä¾›æ€§åˆ†æä»¥åŠå¤šæ­¥ç©ºé—´è§„åˆ’ç­‰å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ•°æ®é›†æ— æ³•å……åˆ†è¯„ä¼°å’Œæå‡æ¨¡å‹åœ¨è¿™äº›æ–¹é¢çš„èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€ç»†ç²’åº¦çš„å¯¹è±¡äº¤äº’æ¨ç†æ•°æ®é›†BOP-ASKï¼Œè¯¥æ•°æ®é›†åŸºäºç°æœ‰çš„å¯¹è±¡å§¿æ€ä¼°è®¡åŸºå‡†BOPæ•°æ®é›†ï¼Œåˆ©ç”¨å…¶ç²¾ç¡®çš„6Då¯¹è±¡å§¿æ€ä¿¡æ¯ï¼Œç”ŸæˆåŒ…å«æŠ“å–å§¿æ€ã€å‚è€ƒå¯¹è±¡å§¿æ€ã€è·¯å¾„è§„åˆ’è½¨è¿¹ã€ç›¸å¯¹ç©ºé—´å’Œæ·±åº¦å…³ç³»ä»¥åŠå¯¹è±¡é—´å…³ç³»ç­‰å¤šç§ç±»å‹çš„æ ‡æ³¨ã€‚é€šè¿‡åœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æå‡æ¨¡å‹å¯¹å¯¹è±¡äº¤äº’çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šBOP-ASKçš„æ•°æ®é›†ç”Ÿæˆæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆ©ç”¨BOPæ•°æ®é›†ä¸­çš„6Då¯¹è±¡å§¿æ€ä¿¡æ¯ï¼›2) åŸºäºè¿™äº›å§¿æ€ä¿¡æ¯ï¼Œç”Ÿæˆå„ç§ç±»å‹çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬æŠ“å–å§¿æ€ã€å‚è€ƒå¯¹è±¡å§¿æ€ã€è·¯å¾„è§„åˆ’è½¨è¿¹ã€ç›¸å¯¹ç©ºé—´å’Œæ·±åº¦å…³ç³»ä»¥åŠå¯¹è±¡é—´å…³ç³»ï¼›3) å°†å›¾åƒå’Œå¯¹åº”çš„æ ‡æ³¨è½¬åŒ–ä¸ºé—®ç­”å¯¹çš„å½¢å¼ï¼Œæ„å»ºæœ€ç»ˆçš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ„å»ºäº†BOP-ASK-coreå’ŒBOP-ASK-labä¸¤ä¸ªæµ‹è¯•åŸºå‡†ï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨åŒåˆ†å¸ƒå’Œåˆ†å¸ƒå¤–æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šBOP-ASKçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç»†ç²’åº¦çš„å¯¹è±¡äº¤äº’æ ‡æ³¨ã€‚ä¸ç°æœ‰æ•°æ®é›†åªå…³æ³¨é«˜çº§ç©ºé—´å…³ç³»ä¸åŒï¼ŒBOP-ASKæä¾›äº†åŒ…æ‹¬æŠ“å–å§¿æ€ã€è·¯å¾„è§„åˆ’ç­‰åœ¨å†…çš„å¤šç§ç±»å‹çš„ç»†ç²’åº¦æ ‡æ³¨ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„å¯¹è±¡äº¤äº’çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒBOP-ASKè¿˜æ„å»ºäº†åˆ†å¸ƒå¤–æµ‹è¯•åŸºå‡†BOP-ASK-labï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šBOP-ASKæ•°æ®é›†åŒ…å«è¶…è¿‡15ä¸‡å¼ å›¾åƒå’Œ3300ä¸‡ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–å…­ä¸ªä»»åŠ¡ã€‚æ•°æ®é›†çš„æ ‡æ³¨ç”Ÿæˆè¿‡ç¨‹ä¾èµ–äºBOPæ•°æ®é›†æä¾›çš„ç²¾ç¡®6Då¯¹è±¡å§¿æ€ä¿¡æ¯ã€‚BOP-ASK-coreæ˜¯ä¸€ä¸ªä»BOPæ•°æ®é›†ä¸­é‡‡æ ·çš„æµ‹è¯•é›†ï¼Œè€ŒBOP-ASK-labåˆ™åŒ…å«æ¥è‡ªå…¶ä»–æ¥æºçš„å›¾åƒï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è®ºæ–‡æ²¡æœ‰è¯¦ç»†è¯´æ˜å…·ä½“çš„æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ï¼Œè€Œæ˜¯ä¾§é‡äºæ•°æ®é›†çš„æ„å»ºå’Œè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨BOP-ASKæ•°æ®é›†ä¸Šè®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¯¹è±¡å§¿æ€ä¼°è®¡ã€è½¨è¿¹è§„åˆ’å’Œç©ºé—´æ¨ç†ç­‰ä»»åŠ¡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ¨¡å‹å±•ç°å‡ºç²¾ç¡®çš„å¯¹è±¡å’ŒæŠ“å–å§¿æ€ä¼°è®¡èƒ½åŠ›ï¼Œä»¥åŠåœ¨æ‚ä¹±ç¯å¢ƒä¸­è¿›è¡Œç»†ç²’åº¦çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚BOP-ASK-labä¸Šçš„è¯„ä¼°ä¹ŸéªŒè¯äº†æ¨¡å‹å…·æœ‰ä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ¨¡å‹ç†è§£ç‰©ä½“ä¹‹é—´çš„äº¤äº’å…³ç³»ï¼Œä»è€Œæ›´å¥½åœ°å®ŒæˆæŠ“å–ã€è£…é…ç­‰ä»»åŠ¡ã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯ä»¥åˆ©ç”¨è¯¥æ¨¡å‹ç†è§£è½¦è¾†ä¸è¡Œäººã€è½¦è¾†ä¸è½¦è¾†ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜è¡Œé©¶å®‰å…¨æ€§ã€‚å¢å¼ºç°å®åº”ç”¨å¯ä»¥åˆ©ç”¨è¯¥æ¨¡å‹å®ç°æ›´è‡ªç„¶çš„ç‰©ä½“äº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.

