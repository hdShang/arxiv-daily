---
layout: default
title: TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding
---

# TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.16595" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.16595v2</a>
  <a href="https://arxiv.org/pdf/2511.16595.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16595v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.16595v2', 'TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Boshen Xu, Zihan Xiao, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Qin Jin

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20 (æ›´æ–°: 2025-11-26)

**å¤‡æ³¨**: Project page: https://xuboshen.github.io/TimeViper; Code: https://github.com/xiaomi-research/timeviper

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TimeViperï¼šä¸€ç§æ··åˆMamba-Transformerè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé«˜æ•ˆé•¿è§†é¢‘ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `Mamba` `Transformer` `æ··åˆæ¨¡å‹` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿è§†é¢‘ç†è§£æ¨¡å‹åœ¨å¤„ç†è¶…é•¿åºåˆ—æ—¶é¢ä¸´è®¡ç®—æ•ˆç‡ç“¶é¢ˆï¼Œéš¾ä»¥å…¼é¡¾æ¨¡å‹è¡¨è¾¾èƒ½åŠ›å’Œæ¨ç†é€Ÿåº¦ã€‚
2. TimeViperæå‡ºæ··åˆMamba-Transformeræ¶æ„ï¼Œåˆ©ç”¨Mambaçš„é«˜æ•ˆæ€§å’ŒTransformerçš„è¡¨è¾¾æ€§ï¼Œå¹¶è®¾è®¡TransVæ¨¡å—å‹ç¼©è§†è§‰ä¿¡æ¯ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTimeViperåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¤„ç†è¶…è¿‡10000å¸§çš„è§†é¢‘ï¼Œå¹¶æä¾›äº†æ··åˆæ¨¡å‹çš„å¯è§£é‡Šæ€§åˆ†æã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºTimeViperï¼Œä¸€ç§æ··åˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ã€‚å¤„ç†é•¿è§†é¢‘éœ€è¦é«˜æ•ˆçš„æ¨¡å‹æ¶æ„å’Œå¤„ç†æ‰©å±•æ—¶é—´ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆæœºåˆ¶ã€‚ä¸ºæ­¤ï¼ŒTimeViperé‡‡ç”¨æ··åˆMamba-Transformeréª¨å¹²ç½‘ç»œï¼Œç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ•ˆç‡å’Œæ³¨æ„åŠ›æœºåˆ¶çš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ··åˆè®¾è®¡ï¼Œæˆ‘ä»¬æ­ç¤ºäº†è§†è§‰åˆ°æ–‡æœ¬çš„ä¿¡æ¯èšåˆç°è±¡ï¼Œå³ä¿¡æ¯åœ¨LLMæ·±åº¦å¢åŠ æ—¶é€æ¸ä»è§†è§‰tokenæµå‘æ–‡æœ¬tokenï¼Œå¯¼è‡´ä¸¥é‡çš„è§†è§‰tokenå†—ä½™ã€‚å—æ­¤è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†TransVï¼Œä¸€ç§tokenä¿¡æ¯ä¼ é€’æ¨¡å—ï¼Œå°†è§†è§‰tokenä¼ é€’å’Œå‹ç¼©åˆ°æŒ‡ä»¤tokenä¸­ï¼ŒåŒæ—¶ä¿æŒå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡ä½¿TimeViperèƒ½å¤Ÿå¤„ç†è¶…è¿‡10,000å¸§çš„æ•°å°æ—¶è§†é¢‘ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTimeViperåœ¨æ‰©å±•å¸§æ•°çš„åŒæ—¶ï¼Œå¯ä»¥ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç«äº‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†Mambaå’ŒTransformerå±‚çš„æ³¨æ„åŠ›è¡Œä¸ºï¼Œä¸ºæ··åˆæ¨¡å‹çš„å¯è§£é‡Šæ€§æä¾›äº†æ–°çš„è§è§£ã€‚è¿™é¡¹å·¥ä½œä»£è¡¨äº†å¼€å‘ã€è§£é‡Šå’Œå‹ç¼©æ··åˆMamba-Transformeræ¶æ„çš„åˆæ­¥å°è¯•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé•¿è§†é¢‘ç†è§£ä»»åŠ¡é¢ä¸´è®¡ç®—é‡å¤§ã€æ—¶é—´è·¨åº¦é•¿çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚çº¯Transformeræ¨¡å‹ï¼Œåœ¨å¤„ç†é•¿åºåˆ—æ—¶è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥æ‰©å±•åˆ°æ•°å°æ—¶çš„è§†é¢‘ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆèåˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œé¿å…è§†è§‰ä¿¡æ¯å†—ä½™ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTimeViperçš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆMambaå’ŒTransformerçš„ä¼˜åŠ¿ï¼Œæ„å»ºæ··åˆæ¶æ„ã€‚Mambaæ“…é•¿å¤„ç†é•¿åºåˆ—ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼Œè€ŒTransformerå…·æœ‰å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡TransVæ¨¡å—ï¼Œå°†è§†è§‰tokenå‹ç¼©å¹¶ä¼ é€’åˆ°æ–‡æœ¬tokenï¼Œå‡å°‘è§†è§‰ä¿¡æ¯çš„å†—ä½™ï¼Œæé«˜æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTimeViperçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ï¼š1) è§†è§‰ç¼–ç å™¨ï¼šæå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ï¼›2) æ··åˆMamba-Transformerå±‚ï¼šäº¤æ›¿ä½¿ç”¨Mambaå’ŒTransformerå±‚å¤„ç†è§†è§‰å’Œæ–‡æœ¬tokenï¼›3) TransVæ¨¡å—ï¼šåœ¨æ··åˆå±‚ä¸­ï¼Œå°†è§†è§‰tokenä¿¡æ¯ä¼ é€’å¹¶å‹ç¼©åˆ°æ–‡æœ¬tokenï¼›4) è¯­è¨€æ¨¡å‹ï¼šåŸºäºèåˆçš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œé¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šTimeViperçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æ··åˆMamba-Transformeræ¶æ„ï¼Œå…¼é¡¾æ•ˆç‡å’Œè¡¨è¾¾èƒ½åŠ›ï¼›2) TransVæ¨¡å—ï¼Œæœ‰æ•ˆå‹ç¼©è§†è§‰ä¿¡æ¯ï¼Œå‡å°‘å†—ä½™ï¼Œæé«˜è®¡ç®—æ•ˆç‡ï¼›3) æ­ç¤ºäº†è§†è§‰åˆ°æ–‡æœ¬çš„ä¿¡æ¯èšåˆç°è±¡ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è®¾è®¡äº†TransVæ¨¡å—ã€‚

**å…³é”®è®¾è®¡**ï¼šTransVæ¨¡å—çš„å…·ä½“å®ç°æ–¹å¼æœªçŸ¥ï¼Œè®ºæ–‡ä¸­å¯èƒ½æ²¡æœ‰è¯¦ç»†æè¿°å…¶å†…éƒ¨ç»“æ„å’Œå‚æ•°è®¾ç½®ã€‚æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥ä¹ŸæœªçŸ¥ï¼Œä½†æ¨æµ‹ä¼šé‡‡ç”¨æ ‡å‡†çš„è§†è§‰-è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œä¾‹å¦‚å¯¹æ¯”å­¦ä¹ æˆ–ç”Ÿæˆå¼å­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TimeViperåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¤„ç†è¶…è¿‡10000å¸§çš„è§†é¢‘ã€‚è®ºæ–‡å¯¹æ¯”äº†TimeViperä¸ç°æœ‰SOTAæ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†TimeViperåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†Mambaå’ŒTransformerå±‚çš„æ³¨æ„åŠ›è¡Œä¸ºï¼Œä¸ºæ··åˆæ¨¡å‹çš„å¯è§£é‡Šæ€§æä¾›äº†æ–°çš„è§è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TimeViperåœ¨è§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºåˆ†æé•¿æ—¶é—´çš„ç›‘æ§å½•åƒï¼Œæ£€æµ‹å¼‚å¸¸äº‹ä»¶ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥å¤„ç†é•¿æ—¶é—´çš„è¡Œè½¦è®°å½•ï¼Œæé«˜å†³ç­–çš„å‡†ç¡®æ€§ï¼›åœ¨æ™ºèƒ½åŠ©æ‰‹ä¸­ï¼Œå¯ä»¥ç†è§£ç”¨æˆ·çš„é•¿ç¯‡æŒ‡ä»¤ï¼Œæä¾›æ›´æ™ºèƒ½çš„æœåŠ¡ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨é•¿è§†é¢‘ç†è§£æŠ€æœ¯çš„å‘å±•ï¼Œä¸ºç›¸å…³åº”ç”¨æä¾›æ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.

