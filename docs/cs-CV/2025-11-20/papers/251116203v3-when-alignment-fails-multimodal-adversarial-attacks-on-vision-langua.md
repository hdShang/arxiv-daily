---
layout: default
title: "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models"
---

# When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.16203" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.16203v3</a>
  <a href="https://arxiv.org/pdf/2511.16203.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16203v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.16203v3', 'When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuping Yan, Yuhan Xie, Yixin Zhang, Lingjuan Lyu, Handing Wang, Yaochu Jin

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20 (æ›´æ–°: 2025-12-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLA-Foolï¼šé’ˆå¯¹å…·èº«è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»ç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å¯¹æŠ—æ”»å‡»` `å¤šæ¨¡æ€å­¦ä¹ ` `å…·èº«æ™ºèƒ½` `é²æ£’æ€§` `è·¨æ¨¡æ€å¯¹é½` `è¯­ä¹‰ç©ºé—´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹å¯¹æŠ—é²æ£’æ€§ç ”ç©¶ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€å’Œé»‘ç›’åœºæ™¯ä¸‹ï¼Œå¿½ç•¥äº†è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½çš„é‡è¦æ€§ã€‚
2. è®ºæ–‡æå‡ºVLA-Foolæ¡†æ¶ï¼Œé€šè¿‡æ–‡æœ¬ã€è§†è§‰æ‰°åŠ¨å’Œè·¨æ¨¡æ€é”™ä½æ”»å‡»ï¼Œè¯„ä¼°VLAæ¨¡å‹åœ¨å¯¹æŠ—ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯è½»å¾®çš„å¤šæ¨¡æ€æ‰°åŠ¨ä¹Ÿä¼šå¯¼è‡´VLAæ¨¡å‹è¡Œä¸ºå‡ºç°æ˜¾è‘—åå·®ï¼Œæ­ç¤ºäº†å…¶è„†å¼±æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(VLA)åœ¨å…·èº«ç¯å¢ƒä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿé€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£è¿›è¡Œæ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ã€‚å°½ç®¡å®ƒä»¬çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†è¿™äº›ç³»ç»Ÿåœ¨ç°å®çš„å¤šæ¨¡æ€å’Œé»‘ç›’æ¡ä»¶ä¸‹çš„å¯¹æŠ—é²æ£’æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€æ‰°åŠ¨ï¼Œå¿½ç•¥äº†ä»æ ¹æœ¬ä¸Šå½±å“å…·èº«æ¨ç†å’Œå†³ç­–çš„è·¨æ¨¡æ€é”™ä½ã€‚æœ¬æ–‡æå‡ºäº†VLA-Foolï¼Œä¸€é¡¹é’ˆå¯¹å…·èº«VLAæ¨¡å‹åœ¨ç™½ç›’å’Œé»‘ç›’è®¾ç½®ä¸‹å¤šæ¨¡æ€å¯¹æŠ—é²æ£’æ€§çš„å…¨é¢ç ”ç©¶ã€‚VLA-Foolç»Ÿä¸€äº†ä¸‰ä¸ªå±‚æ¬¡çš„å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»ï¼š(1)é€šè¿‡åŸºäºæ¢¯åº¦å’ŒåŸºäºæç¤ºçš„æ“çºµè¿›è¡Œæ–‡æœ¬æ‰°åŠ¨ï¼Œ(2)é€šè¿‡è¡¥ä¸å’Œå™ªå£°å¤±çœŸè¿›è¡Œè§†è§‰æ‰°åŠ¨ï¼Œ(3)æœ‰æ„ç ´åæ„ŸçŸ¥å’ŒæŒ‡ä»¤ä¹‹é—´è¯­ä¹‰å¯¹åº”å…³ç³»çš„è·¨æ¨¡æ€é”™ä½æ”»å‡»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†VLAæ„ŸçŸ¥çš„è¯­ä¹‰ç©ºé—´æ•´åˆåˆ°è¯­è¨€æç¤ºä¸­ï¼Œå¼€å‘äº†ç¬¬ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆå’Œè¯­ä¹‰å¼•å¯¼çš„æç¤ºæ¡†æ¶ã€‚ä½¿ç”¨å¾®è°ƒçš„OpenVLAæ¨¡å‹åœ¨LIBEROåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯è½»å¾®çš„å¤šæ¨¡æ€æ‰°åŠ¨ä¹Ÿä¼šå¯¼è‡´æ˜¾è‘—çš„è¡Œä¸ºåå·®ï¼Œè¯æ˜äº†å…·èº«å¤šæ¨¡æ€å¯¹é½çš„è„†å¼±æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„è„†å¼±æ€§é—®é¢˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºå•æ¨¡æ€çš„æ‰°åŠ¨ï¼Œå¿½ç•¥äº†å¤šæ¨¡æ€ä¿¡æ¯ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œè¿™å¯¹äºVLAæ¨¡å‹åœ¨å…·èº«ç¯å¢ƒä¸­çš„æ¨ç†å’Œå†³ç­–è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¯„ä¼°å’Œæå‡VLAæ¨¡å‹åœ¨å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»ä¸‹çš„é²æ£’æ€§æ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»æ¡†æ¶VLA-Foolï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°VLAæ¨¡å‹åœ¨ä¸åŒç±»å‹çš„æ”»å‡»ä¸‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸ä»…è€ƒè™‘äº†å•æ¨¡æ€çš„æ‰°åŠ¨ï¼ˆæ–‡æœ¬å’Œè§†è§‰ï¼‰ï¼Œè¿˜ç‰¹åˆ«å…³æ³¨äº†è·¨æ¨¡æ€çš„è¯­ä¹‰é”™ä½æ”»å‡»ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å¯èƒ½å‡ºç°çš„å„ç§å¯¹æŠ—åœºæ™¯ã€‚é€šè¿‡åˆ†ææ¨¡å‹åœ¨è¿™äº›æ”»å‡»ä¸‹çš„è¡¨ç°ï¼Œå¯ä»¥æ·±å…¥äº†è§£VLAæ¨¡å‹çš„å¼±ç‚¹ï¼Œå¹¶ä¸ºåç»­çš„é²æ£’æ€§æå‡æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLA-Foolæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ–‡æœ¬æ‰°åŠ¨æ¨¡å—ã€è§†è§‰æ‰°åŠ¨æ¨¡å—å’Œè·¨æ¨¡æ€é”™ä½æ”»å‡»æ¨¡å—ã€‚æ–‡æœ¬æ‰°åŠ¨æ¨¡å—é€šè¿‡æ¢¯åº¦å’Œæç¤ºå·¥ç¨‹æ–¹æ³•ç”Ÿæˆå¯¹æŠ—æ€§æ–‡æœ¬æŒ‡ä»¤ã€‚è§†è§‰æ‰°åŠ¨æ¨¡å—åˆ™é€šè¿‡æ·»åŠ è¡¥ä¸æˆ–å™ªå£°æ¥å¹²æ‰°è§†è§‰è¾“å…¥ã€‚è·¨æ¨¡æ€é”™ä½æ”»å‡»æ¨¡å—æ—¨åœ¨ç ´åè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¼•å…¥äº†ä¸€ä¸ªVLAæ„ŸçŸ¥çš„è¯­ä¹‰ç©ºé—´ï¼Œç”¨äºæŒ‡å¯¼æç¤ºçš„ç”Ÿæˆï¼Œä»è€Œæé«˜æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ä»…è€ƒè™‘äº†å•æ¨¡æ€çš„æ‰°åŠ¨ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå¼•å…¥äº†è·¨æ¨¡æ€é”™ä½æ”»å‡»çš„æ¦‚å¿µã€‚è¿™ç§æ”»å‡»æ–¹å¼èƒ½å¤Ÿæ›´çœŸå®åœ°æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­å¯èƒ½å‡ºç°çš„å¯¹æŠ—åœºæ™¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è¯„ä¼°VLAæ¨¡å‹çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒVLAæ„ŸçŸ¥çš„è¯­ä¹‰ç©ºé—´å’Œè‡ªåŠ¨æç¤ºç”Ÿæˆæ¡†æ¶ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œèƒ½å¤Ÿæé«˜æ”»å‡»çš„æ•ˆç‡å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–‡æœ¬æ‰°åŠ¨æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†åŸºäºæ¢¯åº¦çš„æ–¹æ³•æ¥å¯»æ‰¾å¯¹æ¨¡å‹å½±å“æœ€å¤§çš„è¯è¯­è¿›è¡Œæ›¿æ¢ã€‚åœ¨è§†è§‰æ‰°åŠ¨æ¨¡å—ä¸­ï¼Œé‡‡ç”¨äº†patchæ”»å‡»å’Œnoiseæ”»å‡»ä¸¤ç§æ–¹å¼ã€‚åœ¨è·¨æ¨¡æ€é”™ä½æ”»å‡»æ¨¡å—ä¸­ï¼Œé€šè¿‡æ›¿æ¢ä¸è§†è§‰ä¿¡æ¯ä¸ç›¸å…³çš„æ–‡æœ¬æè¿°æ¥ç ´åè¯­ä¹‰ä¸€è‡´æ€§ã€‚VLAæ„ŸçŸ¥çš„è¯­ä¹‰ç©ºé—´åˆ™é€šè¿‡åˆ†æVLAæ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºæ¥æ„å»ºï¼Œç”¨äºæŒ‡å¯¼æç¤ºçš„ç”Ÿæˆï¼Œç¡®ä¿ç”Ÿæˆçš„æç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¬ºéª—æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å¾®å°çš„å¤šæ¨¡æ€æ‰°åŠ¨ä¹Ÿèƒ½æ˜¾è‘—é™ä½OpenVLAæ¨¡å‹åœ¨LIBEROåŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œè·¨æ¨¡æ€é”™ä½æ”»å‡»å¯¼è‡´æ¨¡å‹æˆåŠŸç‡ä¸‹é™è¶…è¿‡30%ã€‚VLA-Foolæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°VLAæ¨¡å‹çš„é²æ£’æ€§ï¼Œå¹¶æ­ç¤ºå…¶åœ¨å¤šæ¨¡æ€å¯¹æŠ—æ”»å‡»ä¸‹çš„è„†å¼±æ€§ï¼Œä¸ºåç»­çš„é˜²å¾¡ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…å’Œå·¥ä¸šæœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡å¢å¼ºVLAæ¨¡å‹å¯¹å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ï¼Œå¯ä»¥å‡å°‘å› æ¶æ„æ”»å‡»æˆ–ç¯å¢ƒå¹²æ‰°å¯¼è‡´çš„æ„å¤–è¡Œä¸ºï¼Œæé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿæœ‰åŠ©äºå¼€å‘æ›´å®‰å…¨çš„AIç³»ç»Ÿï¼Œé˜²æ­¢å…¶è¢«æ¶æ„åˆ©ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.

