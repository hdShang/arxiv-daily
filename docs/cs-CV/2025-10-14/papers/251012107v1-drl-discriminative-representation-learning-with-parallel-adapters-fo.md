---
layout: default
title: DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning
---

# DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.12107" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.12107v1</a>
  <a href="https://arxiv.org/pdf/2510.12107.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.12107v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.12107v1', 'DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiawei Zhan, Jun Liu, Jinlong Peng, Xiaochen Chen, Bin-Bin Gao, Yong Liu, Chengjie Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-14

**å¤‡æ³¨**: 13 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRLæ¡†æ¶ä»¥è§£å†³å¢é‡å­¦ä¹ ä¸­çš„è¡¨ç¤ºè½¬ç§»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç±»å¢é‡å­¦ä¹ ` `è¡¨ç¤ºå­¦ä¹ ` `å¢é‡å¹¶è¡Œé€‚é…å™¨` `è§£è€¦é”šç›‘ç£` `æ·±åº¦å­¦ä¹ ` `æ¨¡å‹é€‚åº”æ€§` `ç‰¹å¾å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ç±»å¢é‡å­¦ä¹ ä¸­é¢ä¸´æ¨¡å‹å¤æ‚æ€§å¢åŠ ã€è¡¨ç¤ºè½¬ç§»ä¸å¹³æ»‘å’Œä¼˜åŒ–ä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ã€‚
2. æå‡ºçš„DRLæ¡†æ¶é€šè¿‡å¢é‡å¹¶è¡Œé€‚é…å™¨ï¼ˆIPAï¼‰ç½‘ç»œå’Œè§£è€¦é”šç›‘ç£ï¼ˆDASï¼‰æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ã€‚
3. åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDRLåœ¨æ•´ä¸ªå¢é‡å­¦ä¹ è¿‡ç¨‹ä¸­æ€§èƒ½ä¼˜è¶Šï¼Œè®­ç»ƒå’Œæ¨ç†æ•ˆç‡é«˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPTMsï¼‰åœ¨éé‡æ¼”ç±»å¢é‡å­¦ä¹ ï¼ˆCILï¼‰ç ”ç©¶ä¸­çš„å‡ºè‰²è¡¨ç¤ºèƒ½åŠ›ï¼Œå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹å¤æ‚æ€§å¢åŠ ã€å¢é‡å­¦ä¹ ä¸­çš„è¡¨ç¤ºè½¬ç§»ä¸å¹³æ»‘ä»¥åŠé˜¶æ®µæ€§å­é—®é¢˜ä¼˜åŒ–ä¸å…¨å±€æ¨ç†ä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ï¼ŒCILä»ç„¶æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†åˆ¤åˆ«è¡¨ç¤ºå­¦ä¹ ï¼ˆDRLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆä¸”é«˜æ•ˆåœ°è¿›è¡Œå¢é‡å­¦ä¹ ã€‚DRLçš„ç½‘ç»œç§°ä¸ºå¢é‡å¹¶è¡Œé€‚é…å™¨ï¼ˆIPAï¼‰ç½‘ç»œï¼ŒåŸºäºPTMæ„å»ºï¼Œé€šè¿‡åœ¨æ¯ä¸ªå¢é‡é˜¶æ®µå­¦ä¹ è½»é‡çº§é€‚é…å™¨æ¥é€æ­¥å¢å¼ºæ¨¡å‹ã€‚é€‚é…å™¨é€šè¿‡è½¬ç§»é—¨ä¸å½“å‰æ¨¡å‹å¹¶è¡Œè¿æ¥ï¼Œè´Ÿè´£é€‚åº”æ–°ç±»åˆ«ï¼Œä»è€Œä¿è¯ä¸åŒå¢é‡é˜¶æ®µä¹‹é—´çš„å¹³æ»‘è¡¨ç¤ºè½¬ç§»ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†è§£è€¦é”šç›‘ç£ï¼ˆDASï¼‰ï¼Œé€šè¿‡ä¸è™šæ‹Ÿé”šç‚¹æ¯”è¾ƒæ­£è´Ÿæ ·æœ¬çš„çº¦æŸï¼Œä¿ƒè¿›åˆ¤åˆ«è¡¨ç¤ºå­¦ä¹ ï¼Œç¼©å°é˜¶æ®µæ€§å±€éƒ¨ä¼˜åŒ–ä¸å…¨å±€æ¨ç†ä¹‹é—´çš„å·®è·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDRLåœ¨æ•´ä¸ªCILè¿‡ç¨‹ä¸­å§‹ç»ˆä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä¿æŒé«˜æ•ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç±»å¢é‡å­¦ä¹ ä¸­çš„è¡¨ç¤ºè½¬ç§»ä¸å¹³æ»‘ã€æ¨¡å‹å¤æ‚æ€§å¢åŠ åŠé˜¶æ®µæ€§ä¼˜åŒ–ä¸å…¨å±€æ¨ç†ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ–°ç±»åˆ«æ—¶ï¼Œå¾€å¾€å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™å’Œè¡¨ç¤ºèƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDRLæ¡†æ¶é€šè¿‡å¢é‡å¹¶è¡Œé€‚é…å™¨ï¼ˆIPAï¼‰ç½‘ç»œï¼Œé€æ­¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼Œé€‚åº”æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿æŒè¡¨ç¤ºèƒ½åŠ›çš„ä¼ æ‰¿ä¸å¹³æ»‘è½¬ç§»ã€‚è§£è€¦é”šç›‘ç£ï¼ˆDASï¼‰åˆ™é€šè¿‡è§£è€¦æ­£è´Ÿæ ·æœ¬çš„çº¦æŸï¼Œä¿ƒè¿›ä¸åŒé˜¶æ®µç‰¹å¾ç©ºé—´çš„ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDRLæ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šå¢é‡å¹¶è¡Œé€‚é…å™¨ï¼ˆIPAï¼‰ç½‘ç»œå’Œè§£è€¦é”šç›‘ç£ï¼ˆDASï¼‰ã€‚IPAç½‘ç»œåœ¨æ¯ä¸ªå¢é‡é˜¶æ®µå­¦ä¹ è½»é‡çº§é€‚é…å™¨ï¼ŒDASåˆ™é€šè¿‡è™šæ‹Ÿé”šç‚¹å¯¹æ ·æœ¬è¿›è¡Œæ¯”è¾ƒï¼Œç¡®ä¿ç‰¹å¾è¡¨ç¤ºçš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šDRLçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡è½»é‡çº§é€‚é…å™¨å®ç°å¹³æ»‘çš„è¡¨ç¤ºè½¬ç§»ï¼Œå¹¶é€šè¿‡è§£è€¦é”šç›‘ç£ä¿ƒè¿›ä¸åŒé˜¶æ®µé—´çš„ç‰¹å¾å¯¹é½ã€‚è¿™ç§è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¢é‡å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨è½»é‡çº§é€‚é…å™¨ä»¥å‡å°‘å‚æ•°å­¦ä¹ å¼€é”€ï¼›æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼ŒDASé€šè¿‡è™šæ‹Ÿé”šç‚¹è§£è€¦æ­£è´Ÿæ ·æœ¬çš„çº¦æŸï¼Œç¡®ä¿ç‰¹å¾ç©ºé—´çš„å¯¹é½ä¸ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDRLåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨è®­ç»ƒå’Œæ¨ç†æ•ˆç‡æ–¹é¢è¡¨ç°çªå‡ºï¼Œæå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨ç±»å¢é‡å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰éœ€è¦æŒç»­å­¦ä¹ æ–°ç±»åˆ«çš„åœºæ™¯ã€‚é€šè¿‡æœ‰æ•ˆçš„å¢é‡å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸æ–­å˜åŒ–çš„ç¯å¢ƒä¸­ä¿æŒé«˜æ•ˆçš„å­¦ä¹ èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the excellent representation capabilities of Pre-Trained Models (PTMs), remarkable progress has been made in non-rehearsal Class-Incremental Learning (CIL) research. However, it remains an extremely challenging task due to three conundrums: increasingly large model complexity, non-smooth representation shift during incremental learning and inconsistency between stage-wise sub-problem optimization and global inference. In this work, we propose the Discriminative Representation Learning (DRL) framework to specifically address these challenges. To conduct incremental learning effectively and yet efficiently, the DRL's network, called Incremental Parallel Adapter (IPA) network, is built upon a PTM and increasingly augments the model by learning a lightweight adapter with a small amount of parameter learning overhead in each incremental stage. The adapter is responsible for adapting the model to new classes, it can inherit and propagate the representation capability from the current model through parallel connection between them by a transfer gate. As a result, this design guarantees a smooth representation shift between different incremental stages. Furthermore, to alleviate inconsistency and enable comparable feature representations across incremental stages, we design the Decoupled Anchor Supervision (DAS). It decouples constraints of positive and negative samples by respectively comparing them with the virtual anchor. This decoupling promotes discriminative representation learning and aligns the feature spaces learned at different stages, thereby narrowing the gap between stage-wise local optimization over a subset of data and global inference across all classes. Extensive experiments on six benchmarks reveal that our DRL consistently outperforms other state-of-the-art methods throughout the entire CIL period while maintaining high efficiency in both training and inference phases.

