---
layout: default
title: "CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection"
---

# CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15991" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15991v3</a>
  <a href="https://arxiv.org/pdf/2510.15991.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15991v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15991v3', 'CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huiming Yang, Wenzhuo Liu, Yicheng Qiao, Lei Yang, Xianzhu Zeng, Li Wang, Zhiwei Li, Zijian Zeng, Zhiying Jiang, Huaping Liu, Kunfeng Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-14 (æ›´æ–°: 2025-11-04)

**å¤‡æ³¨**: 13 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CrossRay3Dï¼šé€šè¿‡å‡ ä½•ä¸åˆ†å¸ƒå¼•å¯¼æå‡å¤šæ¨¡æ€3Dæ£€æµ‹æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€3Dæ£€æµ‹` `ç¨€ç–æ£€æµ‹å™¨` `å‡ ä½•ä¿¡æ¯` `ç±»åˆ«å¹³è¡¡` `è‡ªåŠ¨é©¾é©¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç¨€ç–å¤šæ¨¡æ€3Dæ£€æµ‹å™¨å¿½ç•¥äº†tokenè¡¨å¾çš„è´¨é‡ï¼Œå¯¼è‡´å‰æ™¯è´¨é‡ä¸ä½³ï¼Œé™åˆ¶äº†æ£€æµ‹æ€§èƒ½ã€‚
2. CrossRay3Dé€šè¿‡å¼•å…¥Ray-Aware Supervisionå’ŒClass-Balanced Supervisionï¼Œå¢å¼ºtokençš„å‡ ä½•ä¿¡æ¯å’Œç±»åˆ«è¯­ä¹‰è¡¨å¾ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCrossRay3Dåœ¨nuScenesæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨æ•°æ®ç¼ºå¤±æƒ…å†µä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCrossRay3Dçš„é«˜æ•ˆå¤šæ¨¡æ€3Dæ£€æµ‹å™¨ã€‚é’ˆå¯¹ç°æœ‰ç¨€ç–æ£€æµ‹å™¨tokenè¡¨å¾è´¨é‡ä¸é«˜ï¼Œå¯¼è‡´å‰æ™¯è´¨é‡æ¬ ä½³å’Œæ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºSparse Selector (SS)ã€‚SSçš„æ ¸å¿ƒæ¨¡å—æ˜¯Ray-Aware Supervision (RAS)ï¼Œå®ƒåœ¨è®­ç»ƒé˜¶æ®µä¿ç•™ä¸°å¯Œçš„å‡ ä½•ä¿¡æ¯ï¼Œä»¥åŠClass-Balanced Supervisionï¼Œè‡ªé€‚åº”åœ°é‡æ–°åŠ æƒç±»åˆ«è¯­ä¹‰çš„é‡è¦æ€§ï¼Œç¡®ä¿ä¸å°ç‰©ä½“ç›¸å…³çš„tokenåœ¨tokené‡‡æ ·æœŸé—´è¢«ä¿ç•™ï¼Œä»è€Œä¼˜äºå…¶ä»–ç¨€ç–å¤šæ¨¡æ€æ£€æµ‹å™¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è®¾è®¡äº†Ray Positional Encoding (Ray PE)æ¥è§£å†³LiDARæ¨¡æ€å’Œå›¾åƒä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚åœ¨nuScenesåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCrossRay3Då®ç°äº†72.4 mAPå’Œ74.7 NDSçš„state-of-the-artæ€§èƒ½ï¼Œå¹¶ä¸”æ¯”å…¶ä»–é¢†å…ˆæ–¹æ³•å¿«1.84å€ã€‚CrossRay3Dåœ¨LiDARæˆ–ç›¸æœºæ•°æ®éƒ¨åˆ†æˆ–å®Œå…¨ç¼ºå¤±çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç¨€ç–å¤šæ¨¡æ€3Dæ£€æµ‹å™¨åœ¨tokenè¡¨å¾è´¨é‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´æ£€æµ‹ç²¾åº¦å—é™ï¼Œå°¤å…¶æ˜¯åœ¨å°ç‰©ä½“æ£€æµ‹æ–¹é¢è¡¨ç°ä¸ä½³ã€‚è¿™äº›æ£€æµ‹å™¨é€šå¸¸æ— æ³•å……åˆ†åˆ©ç”¨å‡ ä½•ç»“æ„ä¿¡æ¯å’Œç±»åˆ«åˆ†å¸ƒä¿¡æ¯ï¼Œä»è€Œå½±å“äº†æ•´ä½“æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å‡ ä½•ç»“æ„å’Œç±»åˆ«åˆ†å¸ƒçš„å¼•å¯¼ï¼Œæ¥æå‡tokenè¡¨å¾çš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡Ray-Aware Supervisionä¿ç•™ä¸°å¯Œçš„å‡ ä½•ä¿¡æ¯ï¼Œå¹¶é€šè¿‡Class-Balanced Supervisionè‡ªé€‚åº”åœ°è°ƒæ•´ç±»åˆ«è¯­ä¹‰çš„é‡è¦æ€§ï¼Œä»è€Œæ”¹å–„tokençš„é‡‡æ ·å’Œé€‰æ‹©è¿‡ç¨‹ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä½¿æ£€æµ‹å™¨èƒ½å¤Ÿæ›´å¥½åœ°å…³æ³¨å‰æ™¯ç›®æ ‡ï¼Œç‰¹åˆ«æ˜¯å°ç‰©ä½“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCrossRay3Dæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¨€ç–å¤šæ¨¡æ€3Dæ£€æµ‹å™¨ï¼Œå…¶ä¸»è¦ç»„æˆéƒ¨åˆ†åŒ…æ‹¬ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼ˆç”¨äºæå–LiDARå’Œå›¾åƒçš„ç‰¹å¾ï¼‰ï¼›2) Sparse Selector (SS)æ¨¡å—ï¼ŒåŒ…å«Ray-Aware Supervision (RAS)å’ŒClass-Balanced Supervisionï¼›3) Ray Positional Encoding (Ray PE)æ¨¡å—ï¼›4) æ£€æµ‹å¤´ã€‚æ•´ä½“æµç¨‹æ˜¯é¦–å…ˆæå–å¤šæ¨¡æ€ç‰¹å¾ï¼Œç„¶åé€šè¿‡SSæ¨¡å—é€‰æ‹©é«˜è´¨é‡çš„tokenï¼Œå¹¶åˆ©ç”¨Ray PEæ¨¡å—è¿›è¡Œä½ç½®ç¼–ç ï¼Œæœ€åé€šè¿‡æ£€æµ‹å¤´è¿›è¡Œ3Dç›®æ ‡æ£€æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºSparse Selector (SS)æ¨¡å—ï¼Œç‰¹åˆ«æ˜¯å…¶ä¸­çš„Ray-Aware Supervision (RAS)å’ŒClass-Balanced Supervisionã€‚RASé€šè¿‡å°„çº¿æ„ŸçŸ¥çš„ç›‘ç£æ–¹å¼ï¼Œåœ¨è®­ç»ƒé˜¶æ®µä¿ç•™ä¸°å¯Œçš„å‡ ä½•ä¿¡æ¯ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç›‘ç£æ–¹å¼ä¸åŒï¼Œåè€…å¯èƒ½å¿½ç•¥äº†é‡è¦çš„å‡ ä½•ç»“æ„ã€‚Class-Balanced Supervisionåˆ™é€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´ç±»åˆ«æƒé‡ï¼Œè§£å†³äº†å°ç‰©ä½“æ£€æµ‹ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šRay-Aware Supervision (RAS)çš„å…·ä½“å®ç°æ–¹å¼æ˜¯ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹æ¯ä¸ªtokenå¼•å…¥å°„çº¿æ–¹å‘çš„ç›‘ç£ä¿¡å·ï¼Œé¼“åŠ±ç½‘ç»œå­¦ä¹ tokenä¸å°„çº¿ä¹‹é—´çš„å‡ ä½•å…³ç³»ã€‚Class-Balanced Supervisionçš„å…·ä½“å®ç°æ–¹å¼æ˜¯ï¼Œæ ¹æ®æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ï¼ŒåŠ¨æ€åœ°è°ƒæ•´æŸå¤±å‡½æ•°çš„æƒé‡ï¼Œä½¿å¾—å°æ ·æœ¬ç±»åˆ«èƒ½å¤Ÿè·å¾—æ›´å¤§çš„å…³æ³¨ã€‚Ray Positional Encoding (Ray PE)çš„è®¾è®¡è€ƒè™‘äº†LiDARå’Œå›¾åƒåœ¨ç©ºé—´åˆ†å¸ƒä¸Šçš„å·®å¼‚ï¼Œé€šè¿‡å¼•å…¥å°„çº¿æ–¹å‘çš„ä½ç½®ç¼–ç ï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¤šæ¨¡æ€ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CrossRay3Dåœ¨nuScenesæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒmAPè¾¾åˆ°72.4ï¼ŒNDSè¾¾åˆ°74.7ï¼Œè¶…è¶Šäº†ç°æœ‰çš„SOTAæ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒCrossRay3Dåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†1.84å€çš„åŠ é€Ÿï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´å…·ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨LiDARæˆ–ç›¸æœºæ•°æ®éƒ¨åˆ†æˆ–å®Œå…¨ç¼ºå¤±çš„æƒ…å†µä¸‹ï¼Œä»ç„¶è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œè¯æ˜äº†å…¶åœ¨æ¶åŠ£ç¯å¢ƒä¸‹çš„å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CrossRay3Då…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½äº¤é€šç­‰é¢†åŸŸã€‚å…¶é«˜æ•ˆçš„è®¡ç®—æ€§èƒ½å’Œå¼ºå¤§çš„é²æ£’æ€§ä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„å¹³å°ä¸Šéƒ¨ç½²ï¼Œå¹¶é€‚åº”å„ç§å¤æ‚çš„ç¯å¢ƒæ¡ä»¶ã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºæ¨åŠ¨å¤šæ¨¡æ€3Dæ„ŸçŸ¥æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶ä¸ºæ›´å®‰å…¨ã€æ›´æ™ºèƒ½çš„è‡ªä¸»ç³»ç»Ÿæä¾›æŠ€æœ¯æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The sparse cross-modality detector offers more advantages than its counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of adaptability for downstream tasks and computational cost savings. However, existing sparse detectors overlook the quality of token representation, leaving it with a sub-optimal foreground quality and limited performance. In this paper, we identify that the geometric structure preserved and the class distribution are the key to improving the performance of the sparse detector, and propose a Sparse Selector (SS). The core module of SS is Ray-Aware Supervision (RAS), which preserves rich geometric information during the training stage, and Class-Balanced Supervision, which adaptively reweights the salience of class semantics, ensuring that tokens associated with small objects are retained during token sampling. Thereby, outperforming other sparse multi-modal detectors in the representation of tokens. Additionally, we design Ray Positional Encoding (Ray PE) to address the distribution differences between the LiDAR modality and the image. Finally, we integrate the aforementioned module into an end-to-end sparse multi-modality detector, dubbed CrossRay3D. Experiments show that, on the challenging nuScenes benchmark, CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS, while running 1.84 faster than other leading methods. Moreover, CrossRay3D demonstrates strong robustness even in scenarios where LiDAR or camera data are partially or entirely missing.

