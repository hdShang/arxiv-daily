---
layout: default
title: VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages
---

# VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.12845" target="_blank" class="toolbar-btn">arXiv: 2510.12845v1</a>
    <a href="https://arxiv.org/pdf/2510.12845.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.12845v1" 
            onclick="toggleFavorite(this, '2510.12845v1', 'VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jesse Atuhurra, Iqra Ali, Tomoya Iwakura, Hidetaka Kamigaito, Tatsuya Hiraoka

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-14

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VLUResÔºöÊèêÂá∫Â§öËØ≠ÁßçËßÜËßâËØ≠Ë®ÄÁêÜËß£Âü∫ÂáÜÔºåËØÑ‰º∞‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁéØÂ¢É‰∏ãVLMÁöÑÁªÜÁ≤íÂ∫¶ËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Â§öËØ≠ÁßçÂü∫ÂáÜ` `‰ΩéËµÑÊ∫êËØ≠Ë®Ä` `ÈïøÊñáÊú¨ÁêÜËß£` `ÁªÜÁ≤íÂ∫¶ËØÑ‰º∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLMËØÑ‰º∞‰∏ªË¶ÅÈõÜ‰∏≠‰∫éËã±ËØ≠ÁéØÂ¢ÉÔºåÁº∫‰πèÂØπÈïøÊñáÊú¨Âíå‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÁªÜÁ≤íÂ∫¶ÁêÜËß£ËÉΩÂäõËØÑ‰º∞„ÄÇ
2. VLUResÂü∫ÂáÜÂåÖÂê´ÂÖ´‰∏™ËßÜËßâËØ≠Ë®Ä‰ªªÂä°Âíå‰∏Ä‰∏™‰∏çÁõ∏ÂÖ≥ÊÄß‰ªªÂä°ÔºåË¶ÜÁõñËã±ËØ≠„ÄÅÊó•ËØ≠„ÄÅÊñØÁì¶Â∏åÈáåËØ≠Âíå‰πåÂ∞îÈÉΩËØ≠„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂç≥‰ΩøÊòØGPT-4oÔºåÂú®‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂíåÂ§çÊùÇ‰ªªÂä°‰∏ä‰∏é‰∫∫Á±ªÊ∞¥Âπ≥‰ªçÊúâÂ∑ÆË∑ùÔºåÂºÄÊ∫êÊ®°ÂûãÂ∑ÆË∑ùÊõ¥Â§ß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂØπ‰∫éÊèêÂçáÊô∫ËÉΩ‰ª£ÁêÜÁöÑÊÑüÁü•ËÉΩÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂØπVLMÁöÑËØÑ‰º∞‰ªçÁÑ∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ª•Ëã±ËØ≠‰∏∫‰∏≠ÂøÉÁöÑÂü∫ÂáÜ‰∏äÔºåËøô‰∫õÂü∫ÂáÜ‰∏≠ÁöÑÂõæÂÉè-ÊñáÊú¨ÂØπÂåÖÂê´ÁöÑÊñáÊú¨ËæÉÁü≠„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞VLMÁöÑÁªÜÁ≤íÂ∫¶ËÉΩÂäõÔºåÊàë‰ª¨Âú®ÈïøÊñáÊú¨ËÆæÁΩÆ‰∏ãÔºåÈíàÂØπÂõõÁßçËØ≠Ë®ÄÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂ§öËØ≠ÁßçÂü∫ÂáÜVLUResÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂÖ´‰∏™ËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°Ôºå‰ª•Âèä‰∏Ä‰∏™ÂºÄÂàõÊÄßÁöÑ‰∏çÁõ∏ÂÖ≥ÊÄß‰ªªÂä°Ôºå‰ª•Êé¢ÊµãVLMÂú®Ëã±ËØ≠„ÄÅÊó•ËØ≠Âíå‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºàÊñØÁì¶Â∏åÈáåËØ≠Âíå‰πåÂ∞îÈÉΩËØ≠Ôºâ‰∏≠ÁöÑÁªÜÁ≤íÂ∫¶ËßÜËßâÂíåËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜ‰ªéÁõÆÊ†áËØ≠Ë®ÄÁöÑÁΩëÁªúËµÑÊ∫ê‰∏≠Êî∂ÈõÜÔºåÂåÖÂê´ÂçÅ‰∏™‰∏çÂêåÁöÑÂõæÂÉèÁ±ªÂà´Âíå‰∏∞ÂØåÁöÑÊñáÊú¨‰∏ä‰∏ãÊñáÔºå‰∏∫ÊñØÁì¶Â∏åÈáåËØ≠Âíå‰πåÂ∞îÈÉΩËØ≠ÂºïÂÖ•‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÜËßâËØ≠Ë®ÄËµÑÊ∫ê„ÄÇÈÄöËøáÊèêÁ§∫VLMÁîüÊàêÂìçÂ∫îÂíåÁêÜÁî±ÔºåÂπ∂Áî±Ëá™Âä®ËØÑ‰º∞ÂíåÊØçËØ≠‰∫∫Â£´ËØÑ‰º∞ÔºåÊàë‰ª¨ÂèëÁé∞‰∫ÜÊô∫ËÉΩ‰ª£ÁêÜÂÖ≥ÈîÆ‰ªªÂä°ÔºàÂ¶ÇÂØπË±°ËØÜÂà´„ÄÅÂú∫ÊôØÁêÜËß£ÂíåÂÖ≥Á≥ªÁêÜËß£Ôºâ‰∏≠‰∏çÂêåËØ≠Ë®ÄÂíå‰ªªÂä°‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆÂºÇ„ÄÇÊàë‰ª¨‰ΩøÁî®VLUResËØÑ‰º∞‰∫ÜÂçÅ‰∏™VLM„ÄÇÊÄßËÉΩÊúÄ‰Ω≥ÁöÑÊ®°ÂûãGPT-4oÂÆûÁé∞‰∫Ü90.8%ÁöÑÊÄª‰ΩìÂáÜÁ°ÆÁéáÔºåÊØî‰∫∫Á±ªÊÄßËÉΩ‰Ωé6.7%Ôºå‰ΩÜÂºÄÊ∫êÊ®°ÂûãÁöÑÂ∑ÆË∑ùÊõ¥Â§ß„ÄÇËøô‰∏ÄÂ∑ÆË∑ùÁ™ÅÊòæ‰∫ÜVLUResÂú®ÂºÄÂèëÊô∫ËÉΩ‰ª£ÁêÜ‰ª•Ëß£ÂÜ≥Â§öÊ®°ÊÄÅËßÜËßâÊé®ÁêÜÊñπÈù¢ÁöÑÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑËØÑ‰º∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëã±ËØ≠Êï∞ÊçÆÈõÜ‰∏äÔºåÂπ∂‰∏îÈÄöÂ∏∏‰ΩøÁî®Áü≠ÊñáÊú¨„ÄÇËøô‰ΩøÂæóÊàë‰ª¨Èöæ‰ª•ËØÑ‰º∞VLMÂú®ÈïøÊñáÊú¨Âíå‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁéØÂ¢É‰∏ãÁöÑÁªÜÁ≤íÂ∫¶ËßÜËßâÂíåËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÂÖÖÂàÜÊµãËØïVLMÂú®Â§çÊùÇÂú∫ÊôØÁêÜËß£„ÄÅÂÖ≥Á≥ªÊé®ÁêÜ‰ª•ÂèäÂ§ÑÁêÜ‰∏çÂêåËØ≠Ë®ÄÊñáÂåñËÉåÊôØ‰∏ãÁöÑËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Â§öËØ≠Áßç„ÄÅÈïøÊñáÊú¨ÁöÑËßÜËßâËØ≠Ë®ÄÁêÜËß£Âü∫ÂáÜÔºàVLUResÔºâÔºåËØ•Âü∫ÂáÜÂåÖÂê´Â§ö‰∏™‰ªªÂä°ÔºåÊó®Âú®ÂÖ®Èù¢ËØÑ‰º∞VLMÂú®‰∏çÂêåËØ≠Ë®ÄÂíå‰ªªÂä°‰∏äÁöÑÁªÜÁ≤íÂ∫¶ËÉΩÂäõ„ÄÇÈÄöËøáÂºïÂÖ•‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºàÊñØÁì¶Â∏åÈáåËØ≠Âíå‰πåÂ∞îÈÉΩËØ≠ÔºâÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞‰∫ÜËß£VLMÂú®ËµÑÊ∫êÂåÆ‰πèÊÉÖÂÜµ‰∏ãÁöÑË°®Áé∞„ÄÇÂêåÊó∂ÔºåÈïøÊñáÊú¨ÁöÑÂºïÂÖ•ÂèØ‰ª•ÊµãËØïVLMÂØπ‰∏ä‰∏ãÊñá‰ø°ÊÅØÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVLUResÂü∫ÂáÜÂåÖÂê´ÂÖ´‰∏™ËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°Ôºå‰ª•Âèä‰∏Ä‰∏™‰∏çÁõ∏ÂÖ≥ÊÄß‰ªªÂä°„ÄÇËøô‰∫õ‰ªªÂä°Ê∂µÁõñ‰∫ÜÂØπË±°ËØÜÂà´„ÄÅÂú∫ÊôØÁêÜËß£„ÄÅÂÖ≥Á≥ªÁêÜËß£Á≠âÂ§ö‰∏™ÊñπÈù¢„ÄÇÊï∞ÊçÆÈõÜÁöÑÊûÑÂª∫‰∏ªË¶ÅÈÄöËøá‰ªéÁõÆÊ†áËØ≠Ë®ÄÁöÑÁΩëÁªúËµÑÊ∫ê‰∏≠Êî∂ÈõÜÂõæÂÉèÂíåÊñáÊú¨ÔºåÂπ∂ËøõË°å‰∫∫Â∑•Ê†áÊ≥®ÂíåÈ™åËØÅ„ÄÇËØÑ‰º∞ËøáÁ®ãÂåÖÊã¨ÊèêÁ§∫VLMÁîüÊàêÂìçÂ∫îÂíåÁêÜÁî±ÔºåÁÑ∂Âêé‰ΩøÁî®Ëá™Âä®ËØÑ‰º∞ÊåáÊ†áÂíå‰∫∫Â∑•ËØÑ‰º∞Áõ∏ÁªìÂêàÁöÑÊñπÂºèÊù•ËØÑ‰º∞VLMÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVLUResÂü∫ÂáÜÁöÑ‰∏ªË¶ÅÂàõÊñ∞ÁÇπÂú®‰∫éÂÖ∂Â§öËØ≠ÁßçÂíåÈïøÊñáÊú¨ÁöÑÁâπÊÄß„ÄÇÂÆÉÈ¶ñÊ¨°Â∞Ü‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºàÊñØÁì¶Â∏åÈáåËØ≠Âíå‰πåÂ∞îÈÉΩËØ≠ÔºâÂºïÂÖ•Âà∞VLMÁöÑËØÑ‰º∞‰∏≠ÔºåÂπ∂‰ΩøÁî®ÈïøÊñáÊú¨Êù•ÊµãËØïVLMÁöÑ‰∏ä‰∏ãÊñáÁêÜËß£ËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåVLUResËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™‰∏çÁõ∏ÂÖ≥ÊÄß‰ªªÂä°ÔºåÁî®‰∫éËØÑ‰º∞VLMÂå∫ÂàÜÁõ∏ÂÖ≥Âíå‰∏çÁõ∏ÂÖ≥‰ø°ÊÅØÁöÑËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVLUResÂü∫ÂáÜÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1ÔºâÈÄâÊã©ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°Ôºå‰ª•ÂÖ®Èù¢ËØÑ‰º∞VLMÁöÑÁªÜÁ≤íÂ∫¶ËÉΩÂäõÔºõ2ÔºâÊûÑÂª∫ÂåÖÂê´‰∏∞ÂØåÊñáÊú¨‰∏ä‰∏ãÊñáÁöÑÊï∞ÊçÆÈõÜÔºå‰ª•ÊµãËØïVLMÁöÑ‰∏ä‰∏ãÊñáÁêÜËß£ËÉΩÂäõÔºõ3ÔºâÈááÁî®Ëá™Âä®ËØÑ‰º∞Âíå‰∫∫Â∑•ËØÑ‰º∞Áõ∏ÁªìÂêàÁöÑÊñπÂºèÔºå‰ª•Á°Æ‰øùËØÑ‰º∞ÁªìÊûúÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØÈù†ÊÄßÔºõ4ÔºâÈíàÂØπ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÔºåÈááÁî®Êï∞ÊçÆÂ¢ûÂº∫ÂíåËøÅÁßªÂ≠¶‰π†Á≠âÊäÄÊúØÔºå‰ª•ÊèêÈ´òVLMÁöÑÊÄßËÉΩÔºàÂÖ∑‰ΩìÁªÜËäÇÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGPT-4oÂú®VLUResÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊúÄ‰Ω≥ÊÄßËÉΩÔºåÊÄª‰ΩìÂáÜÁ°ÆÁéáËææÂà∞90.8%Ôºå‰ΩÜ‰∏é‰∫∫Á±ªÊÄßËÉΩ‰ªçÊúâ6.7%ÁöÑÂ∑ÆË∑ù„ÄÇÂºÄÊ∫êÊ®°ÂûãÁöÑÊÄßËÉΩÂ∑ÆË∑ùÊõ¥Â§ßÔºåË°®ÊòéÁé∞ÊúâVLMÂú®‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂíåÂ§çÊùÇ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ‰ªçÊúâÊèêÂçáÁ©∫Èó¥„ÄÇVLUResÂü∫ÂáÜÁöÑÂºïÂÖ•‰∏∫ËØÑ‰º∞ÂíåÊîπËøõVLMÂú®Â§öËØ≠ÁßçÁéØÂ¢É‰∏ãÁöÑÊÄßËÉΩÊèê‰æõ‰∫ÜÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂºÄÂèëÊõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÈÄöÁî®ÁöÑÂ§öÊ®°ÊÄÅÊô∫ËÉΩ‰ª£ÁêÜÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂíåÊñáÂåñËÉåÊôØ‰∏ã„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Áî®‰∫éÊûÑÂª∫ËÉΩÂ§üÁêÜËß£ÂΩìÂú∞ËØ≠Ë®ÄÂíåÊñáÂåñÁöÑÊô∫ËÉΩÂÆ¢Êúç„ÄÅÊïôËÇ≤Êú∫Âô®‰∫∫ÊàñÂåªÁñóÂä©Êâã„ÄÇÊ≠§Â§ñÔºåËØ•Âü∫ÂáÜËøòÂèØ‰ª•‰øÉËøõVLMÂú®Ë∑®ËØ≠Ë®Ä‰ø°ÊÅØÊ£ÄÁ¥¢„ÄÅÊú∫Âô®ÁøªËØëÂíåÂ§öÊ®°ÊÄÅÂÜÖÂÆπÁîüÊàêÁ≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision Language Models (VLMs) are pivotal for advancing perception in intelligent agents. Yet, evaluation of VLMs remains limited to predominantly English-centric benchmarks in which the image-text pairs comprise short texts. To evaluate VLM fine-grained abilities, in four languages under long-text settings, we introduce a novel multilingual benchmark VLURes featuring eight vision-and-language tasks, and a pioneering unrelatedness task, to probe the fine-grained Visual and Linguistic Understanding capabilities of VLMs across English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets, curated from web resources in the target language, encompass ten diverse image categories and rich textual context, introducing valuable vision-language resources for Swahili and Urdu. By prompting VLMs to generate responses and rationales, evaluated automatically and by native speakers, we uncover performance disparities across languages and tasks critical to intelligent agents, such as object recognition, scene understanding, and relationship understanding. We conducted evaluations of ten VLMs with VLURes. The best performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human performance by 6.7%, though the gap is larger for open-source models. The gap highlights VLURes' critical role in developing intelligent agents to tackle multi-modal visual reasoning.

