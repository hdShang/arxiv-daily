---
layout: default
title: Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback
---

# Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.12089" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.12089v2</a>
  <a href="https://arxiv.org/pdf/2510.12089.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.12089v2" onclick="toggleFavorite(this, '2510.12089v2', 'Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-14 (æ›´æ–°: 2025-11-18)

**å¤‡æ³¨**: AAAI 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Playmate2ï¼šåŸºäºæ‰©æ•£Transformerå’Œå¥–åŠ±åé¦ˆçš„å…è®­ç»ƒå¤šè§’è‰²éŸ³é¢‘é©±åŠ¨åŠ¨ç”»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘é©±åŠ¨åŠ¨ç”»` `æ‰©æ•£æ¨¡å‹` `Transformer` `å¤šè§’è‰²åŠ¨ç”»` `å…è®­ç»ƒ` `å”‡éŸ³åŒæ­¥` `é•¿è§†é¢‘ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å”‡éŸ³åŒæ­¥ç²¾åº¦ã€é•¿è§†é¢‘æ—¶é—´è¿è´¯æ€§å’Œå¤šè§’è‰²åŠ¨ç”»æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚
2. æå‡ºåŸºäºæ‰©æ•£Transformerçš„æ¡†æ¶ï¼Œç»“åˆLoRAè®­ç»ƒã€ä½ç½®åç§»æ¨ç†å’Œå¥–åŠ±åé¦ˆï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚
3. å¼•å…¥Mask-CFGå…è®­ç»ƒæ–¹æ³•ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–æ¨¡å‹ä¿®æ”¹ï¼Œå³å¯å®ç°å¤šè§’è‰²éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£Transformerï¼ˆDiTï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä»»æ„é•¿åº¦çš„é€¼çœŸè¯´è¯è§†é¢‘ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å…è®­ç»ƒæ–¹æ³•ç”¨äºå¤šè§’è‰²éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ã€‚é¦–å…ˆï¼Œé‡‡ç”¨åŸºäºLoRAçš„è®­ç»ƒç­–ç•¥ç»“åˆä½ç½®åç§»æ¨ç†æ–¹æ³•ï¼Œå®ç°é«˜æ•ˆçš„é•¿è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œç»“åˆéƒ¨åˆ†å‚æ•°æ›´æ–°å’Œå¥–åŠ±åé¦ˆï¼Œå¢å¼ºå”‡éŸ³åŒæ­¥å’Œè‡ªç„¶çš„èº«ä½“è¿åŠ¨ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§å…è®­ç»ƒæ–¹æ³•ï¼Œå³æ©ç åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆMask-CFGï¼‰ï¼Œç”¨äºå¤šè§’è‰²åŠ¨ç”»ï¼Œæ— éœ€ä¸“é—¨çš„æ•°æ®é›†æˆ–æ¨¡å‹ä¿®æ”¹ï¼Œæ”¯æŒä¸‰ä¸ªæˆ–æ›´å¤šè§’è‰²çš„éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä»¥ç®€å•ã€é«˜æ•ˆå’Œç»æµçš„æ–¹å¼å®ç°äº†é«˜è´¨é‡ã€æ—¶é—´è¿è´¯å’Œå¤šè§’è‰²çš„éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨å”‡éŸ³åŒæ­¥çš„å‡†ç¡®æ€§ã€é•¿è§†é¢‘ç”Ÿæˆçš„æ—¶é—´è¿è´¯æ€§ä»¥åŠå¤šè§’è‰²åŠ¨ç”»çš„å®ç°ä¸Šå­˜åœ¨è¯¸å¤šç—›ç‚¹ã€‚å°¤å…¶æ˜¯åœ¨å¤šè§’è‰²åœºæ™¯ä¸‹ï¼Œéœ€è¦å¤§é‡ç‰¹å®šæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ³›åŒ–æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç»“åˆè½»é‡çº§çš„LoRAè®­ç»ƒå’Œå¥–åŠ±åé¦ˆæœºåˆ¶ï¼Œæå‡å”‡éŸ³åŒæ­¥å’ŒåŠ¨ä½œçš„è‡ªç„¶æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡Mask-CFGæ–¹æ³•ï¼Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°å¤šè§’è‰²åŠ¨ç”»ï¼Œé™ä½äº†å¯¹ç‰¹å®šæ•°æ®é›†çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŸºäºæ‰©æ•£Transformerï¼ˆDiTï¼‰ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) LoRAè®­ç»ƒæ¨¡å—ï¼Œç”¨äºåœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæå‡ç”Ÿæˆæ•ˆç‡ï¼›2) ä½ç½®åç§»æ¨ç†æ¨¡å—ï¼Œç”¨äºä¿è¯é•¿è§†é¢‘ç”Ÿæˆçš„æ—¶é—´è¿è´¯æ€§ï¼›3) å¥–åŠ±åé¦ˆæ¨¡å—ï¼Œé€šè¿‡å¥–åŠ±å‡½æ•°æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆæ›´é€¼çœŸçš„å”‡éŸ³åŒæ­¥å’Œèº«ä½“åŠ¨ä½œï¼›4) Mask-CFGæ¨¡å—ï¼Œé€šè¿‡æ©ç æ§åˆ¶ä¸åŒè§’è‰²çš„è¿åŠ¨ï¼Œå®ç°å¤šè§’è‰²åŠ¨ç”»ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºMask-CFGå…è®­ç»ƒå¤šè§’è‰²åŠ¨ç”»æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ— éœ€é’ˆå¯¹å¤šè§’è‰²åœºæ™¯è¿›è¡Œé¢å¤–è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡åœ¨æ¨ç†é˜¶æ®µå¯¹ä¸åŒè§’è‰²è¿›è¡Œæ©ç ï¼Œå¹¶ç»“åˆåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼Œå®ç°å¯¹å¤šä¸ªè§’è‰²çš„ç‹¬ç«‹æ§åˆ¶ã€‚è¿™ä¸éœ€è¦å¤§é‡å¤šè§’è‰²æ•°æ®è¿›è¡Œè®­ç»ƒçš„ä¼ ç»Ÿæ–¹æ³•æœ‰ç€æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šLoRAè®­ç»ƒä¸­ï¼Œåªæ›´æ–°éƒ¨åˆ†å‚æ•°ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚å¥–åŠ±åé¦ˆæ¨¡å—ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘å”‡éŸ³åŒæ­¥çš„å‡†ç¡®æ€§å’Œèº«ä½“åŠ¨ä½œçš„è‡ªç„¶æ€§ã€‚Mask-CFGæ¨¡å—ä¸­ï¼Œæ©ç çš„è®¾è®¡éœ€è¦ä¿è¯ä¸åŒè§’è‰²ä¹‹é—´çš„ç‹¬ç«‹æ€§å’Œåè°ƒæ€§ã€‚ä½ç½®åç§»æ¨ç†æ¨¡å—é€šè¿‡åœ¨æ—¶é—´ç»´åº¦ä¸Šå¼•å…¥åç§»ï¼Œç¼“è§£é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´ä¸ä¸€è‡´é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPlaymate2åœ¨å”‡éŸ³åŒæ­¥å‡†ç¡®æ€§ã€æ—¶é—´è¿è´¯æ€§å’Œå¤šè§’è‰²åŠ¨ç”»è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šè§’è‰²åŠ¨ç”»æ–¹é¢ï¼Œè¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°ï¼Œæ˜¾è‘—é™ä½äº†æˆæœ¬ã€‚å®šæ€§ç»“æœä¹Ÿæ˜¾ç¤ºï¼Œç”Ÿæˆçš„è§†é¢‘å…·æœ‰æ›´é«˜çš„çœŸå®æ„Ÿå’Œè‡ªç„¶åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè™šæ‹Ÿä¸»æ’­ã€æ¸¸æˆè§’è‰²åŠ¨ç”»ã€ç”µå½±åˆ¶ä½œã€åœ¨çº¿æ•™è‚²ç­‰é¢†åŸŸã€‚é€šè¿‡éŸ³é¢‘é©±åŠ¨ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆé€¼çœŸä¸”å…·æœ‰è¡¨ç°åŠ›çš„è§’è‰²åŠ¨ç”»ï¼Œé™ä½äº†åŠ¨ç”»åˆ¶ä½œçš„æˆæœ¬å’Œé—¨æ§›ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ‹“å±•åˆ°æ›´å¤šé¢†åŸŸï¼Œä¾‹å¦‚è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner.

