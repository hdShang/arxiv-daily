---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-16
---

# cs.CVï¼ˆ2025-12-16ï¼‰

ğŸ“Š å…± **107** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (25)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (21)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (20)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (20 ğŸ”—3)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (7)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (25 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251214052-hypervl-an-efficient-and-dynamic-multimodal-large-language-model-for-e.html">HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices</a></td>
  <td>HyperVLï¼šé¢å‘è¾¹ç¼˜è®¾å¤‡çš„é«˜æ•ˆåŠ¨æ€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14052" onclick="toggleFavorite(this, '2512.14052', 'HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251213752-star-stacked-autoregressive-scheme-for-unified-multimodal-learning.html">STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning</a></td>
  <td>æå‡ºSTARï¼šå †å è‡ªå›å½’æ–¹æ¡ˆï¼Œç”¨äºç»Ÿä¸€å¤šæ¨¡æ€å­¦ä¹ ï¼Œæå‡ç”Ÿæˆæ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13752" onclick="toggleFavorite(this, '2512.13752', 'STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251214102-neurosymbolic-inference-on-foundation-models-for-remote-sensing-text-t.html">Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries</a></td>
  <td>æå‡ºRUNEï¼šç»“åˆç¥ç»ç¬¦å·æ¨ç†ä¸å¤§è¯­è¨€æ¨¡å‹ï¼Œæå‡é¥æ„Ÿå›¾åƒæ–‡æœ¬æ£€ç´¢çš„å¤æ‚æŸ¥è¯¢å¤„ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14102" onclick="toggleFavorite(this, '2512.14102', 'Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251214594-llm-driven-knowledge-enhancement-for-multimodal-cancer-survival-predic.html">LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction</a></td>
  <td>æå‡ºKEMMæ¨¡å‹ï¼Œåˆ©ç”¨LLMå¢å¼ºçŸ¥è¯†çš„å¤šæ¨¡æ€ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14594" onclick="toggleFavorite(this, '2512.14594', 'LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/241011160-a-unified-framework-with-multimodal-fine-tuning-for-remote-sensing-sem.html">A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation</a></td>
  <td>æå‡ºMFNetï¼Œç»“åˆå¤šæ¨¡æ€å¾®è°ƒçš„é¥æ„Ÿè¯­ä¹‰åˆ†å‰²ç»Ÿä¸€æ¡†æ¶ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2410.11160" onclick="toggleFavorite(this, '2410.11160', 'A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250502677-multimodal-deep-learning-for-stroke-prediction-and-detection-using-ret.html">Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data</a></td>
  <td>æå‡ºä¸€ç§åŸºäºè§†ç½‘è†œå½±åƒå’Œä¸´åºŠæ•°æ®çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå’ä¸­é¢„æµ‹å’Œæ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02677" onclick="toggleFavorite(this, '2505.02677', 'Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251213742-dl3m-a-vision-to-language-framework-for-expert-level-medical-reasoning.html">DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models</a></td>
  <td>DL$^3$Mï¼šç»“åˆæ·±åº¦å­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°ä¸“å®¶çº§åŒ»å­¦æ¨ç†çš„è§†è§‰-è¯­è¨€æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13742" onclick="toggleFavorite(this, '2512.13742', 'DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251214058-real-time-prediction-of-workplane-illuminance-distribution-for-dayligh.html">Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning</a></td>
  <td>æå‡ºä¸€ç§éä¾µå…¥å¼å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ—¥å…‰ç…§æ˜æ§åˆ¶çš„å®æ—¶å·¥ä½œé¢ç…§åº¦é¢„æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14058" onclick="toggleFavorite(this, '2512.14058', 'Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251214225-omnigen-unified-multimodal-sensor-generation-for-autonomous-driving.html">OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving</a></td>
  <td>OmniGenï¼šæå‡ºç»Ÿä¸€çš„å¤šæ¨¡æ€ä¼ æ„Ÿå™¨ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14225" onclick="toggleFavorite(this, '2512.14225', 'OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251214489-signit-a-comprehensive-dataset-and-multimodal-analysis-for-italian-sig.html">SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition</a></td>
  <td>SignITï¼šä¸€ä¸ªç”¨äºæ„å¤§åˆ©æ‰‹è¯­è¯†åˆ«çš„ç»¼åˆæ•°æ®é›†ä¸å¤šæ¨¡æ€åˆ†æ</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14489" onclick="toggleFavorite(this, '2512.14489', 'SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251214499-native-intelligence-emerges-from-large-scale-clinical-practice-a-retin.html">Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency</a></td>
  <td>ReVisionï¼šåŸºäºå¤§è§„æ¨¡ä¸´åºŠå®è·µçš„è§†ç½‘è†œåŸç”Ÿæ™ºèƒ½æ¨¡å‹ï¼Œæå‡éƒ¨ç½²æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14499" onclick="toggleFavorite(this, '2512.14499', 'Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250101728-multimodal-classification-of-forest-biodiversity-potential-from-2d-ort.html">Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼Œç”¨äºè¯„ä¼°æ£®æ—ç”Ÿç‰©å¤šæ ·æ€§æ½œåŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2501.01728" onclick="toggleFavorite(this, '2501.01728', 'Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251102503-adapting-general-purpose-foundation-models-for-x-ray-ptychography-in-l.html">Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes</a></td>
  <td>é’ˆå¯¹ä½æ•°æ®é‡Xå°„çº¿è¡å°„æˆåƒï¼Œæå‡ºé€šç”¨åŸºç¡€æ¨¡å‹è‡ªé€‚åº”æ–¹æ³•PtychoBench</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.02503" onclick="toggleFavorite(this, '2511.02503', 'Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251213164-a-semantically-enhanced-generative-foundation-model-improves-pathologi.html">A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</a></td>
  <td>CRAFTSï¼šä¸€ç§è¯­ä¹‰å¢å¼ºçš„ç—…ç†å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæå‡ç—…ç†å›¾åƒåˆæˆè´¨é‡</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13164" onclick="toggleFavorite(this, '2512.13164', 'A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251213731-complex-mathematical-expression-recognition-benchmark-large-scale-data.html">Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline</a></td>
  <td>æå‡ºCMER-Benchã€MER-17M/CMER-3Mæ•°æ®é›†åŠCMERNetæ¨¡å‹ï¼Œæå‡å¤æ‚æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13731" onclick="toggleFavorite(this, '2512.13731', 'Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251214017-kfs-bench-comprehensive-evaluation-of-key-frame-sampling-in-long-video.html">KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding</a></td>
  <td>æå‡ºKFS-Benché•¿è§†é¢‘QAå…³é”®å¸§é‡‡æ ·åŸºå‡†ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ•ˆç‡ä¸ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14017" onclick="toggleFavorite(this, '2512.14017', 'KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251214040-chartagent-a-chart-understanding-framework-with-tool-integrated-reason.html">ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning</a></td>
  <td>æå‡ºChartAgentï¼Œä¸€ä¸ªå·¥å…·é›†æˆæ¨ç†çš„å›¾è¡¨ç†è§£æ¡†æ¶ï¼Œæå‡ç¨€ç–æ ‡æ³¨ä¸‹çš„é²æ£’æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14040" onclick="toggleFavorite(this, '2512.14040', 'ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250711252-mfgdiffusion-mask-guided-smoke-synthesis-for-enhanced-forest-fire-dete.html">MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection</a></td>
  <td>MFGDiffusionï¼šæå‡ºæ©ç å¼•å¯¼çš„çƒŸé›¾åˆæˆæ–¹æ³•ï¼Œæå‡æ£®æ—ç«ç¾æ£€æµ‹æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.11252" onclick="toggleFavorite(this, '2507.11252', 'MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251214113-selective-controlled-and-domain-agnostic-unlearning-in-pretrained-clip.html">Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach</a></td>
  <td>æå‡ºä¸€ç§å…è®­ç»ƒå…æ•°æ®çš„CLIPå¯æ§é€‰æ‹©æ€§é¢†åŸŸæ— å…³çŸ¥è¯†é—å¿˜æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14113" onclick="toggleFavorite(this, '2512.14113', 'Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251214141-torchtraceap-a-new-benchmark-dataset-for-detecting-performance-anti-pa.html">TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models</a></td>
  <td>æå‡ºTorchTraceAPåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºæ£€æµ‹è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­çš„æ€§èƒ½åæ¨¡å¼ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14141" onclick="toggleFavorite(this, '2512.14141', 'TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251214257-enhancing-visual-programming-for-visual-reasoning-via-probabilistic-gr.html">Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs</a></td>
  <td>æå‡ºEVPGï¼Œé€šè¿‡æ¦‚ç‡å›¾å¢å¼ºè§†è§‰ç¼–ç¨‹ä»¥æå‡è§†è§‰æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14257" onclick="toggleFavorite(this, '2512.14257', 'Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251214420-discode-distribution-aware-score-decoder-for-robust-automatic-evaluati.html">DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning</a></td>
  <td>æå‡ºDISCODEï¼Œä¸€ç§åˆ†å¸ƒæ„ŸçŸ¥çš„æ— å¾®è°ƒå›¾åƒæè¿°è¯„ä¼°æ–¹æ³•ï¼Œæå‡é¢†åŸŸæ³›åŒ–æ€§ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14420" onclick="toggleFavorite(this, '2512.14420', 'DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251214574-foodlogathl-218-constructing-a-real-world-food-image-dataset-using-die.html">FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications</a></td>
  <td>FoodLogAthl-218ï¼šæ„å»ºåŸºäºè†³é£Ÿç®¡ç†åº”ç”¨é‡‡é›†çš„çœŸå®é£Ÿç‰©å›¾åƒæ•°æ®é›†</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14574" onclick="toggleFavorite(this, '2512.14574', 'FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251214654-virc-enhancing-visual-interleaved-mathematical-cot-with-reason-chunkin.html">ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking</a></td>
  <td>æå‡ºViRCæ¡†æ¶ï¼Œé€šè¿‡Reason Chunkingå¢å¼ºè§†è§‰äº¤é”™æ•°å­¦CoTæ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14654" onclick="toggleFavorite(this, '2512.14654', 'ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251019981-futrtrack-a-camera-lidar-fusion-transformer-for-3d-multiple-object-tra.html">FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking</a></td>
  <td>FutrTrackï¼šä¸€ç§ç”¨äº3Då¤šç›®æ ‡è·Ÿè¸ªçš„ç›¸æœº-æ¿€å…‰é›·è¾¾èåˆTransformer</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.19981" onclick="toggleFavorite(this, '2510.19981', 'FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (21 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251213636-minddrive-a-vision-language-action-model-for-autonomous-driving-via-on.html">MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</a></td>
  <td>MindDriveï¼šæå‡ºåŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13636" onclick="toggleFavorite(this, '2512.13636', 'MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251214095-anchorhoi-zero-shot-generation-of-4d-human-object-interaction-via-anch.html">AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation</a></td>
  <td>AnchorHOIï¼šåŸºäºé”šç‚¹çš„å…ˆéªŒçŸ¥è¯†è’¸é¦å®ç°é›¶æ ·æœ¬4Däºº-ç‰©äº¤äº’ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14095" onclick="toggleFavorite(this, '2512.14095', 'AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251214614-worldplay-towards-long-term-geometric-consistency-for-real-time-intera.html">WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling</a></td>
  <td>WorldPlayï¼šé¢å‘å®æ—¶äº¤äº’ä¸–ç•Œå»ºæ¨¡çš„é•¿æ—¶å‡ ä½•ä¸€è‡´æ€§è§†é¢‘æ‰©æ•£æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">distillation</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14614" onclick="toggleFavorite(this, '2512.14614', 'WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251214442-a4-agent-an-agentic-framework-for-zero-shot-affordance-reasoning.html">A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</a></td>
  <td>æå‡ºA4-Agentï¼Œä¸€ä¸ªé›¶æ ·æœ¬å…·èº«æ™ºèƒ½æ¡†æ¶ï¼Œç”¨äºè§£å†³é€šç”¨å¯ä¾›æ€§æ¨ç†é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">dreamer</span> <span class="paper-tag">affordance</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14442" onclick="toggleFavorite(this, '2512.14442', 'A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251214044-omnidrive-r1-reinforcement-driven-interleaved-multi-modal-chain-of-tho.html">OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</a></td>
  <td>OmniDrive-R1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ¨¡æ€äº¤é”™CoTï¼Œæå‡è‡ªåŠ¨é©¾é©¶è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">visual grounding</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14044" onclick="toggleFavorite(this, '2512.14044', 'OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251214364-unified-semantic-transformer-for-3d-scene-understanding.html">Unified Semantic Transformer for 3D Scene Understanding</a></td>
  <td>æå‡ºUNITEï¼šç”¨äº3Dåœºæ™¯ç†è§£çš„ç»Ÿä¸€è¯­ä¹‰Transformeræ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">scene understanding</span> <span class="paper-tag">open-vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14364" onclick="toggleFavorite(this, '2512.14364', 'Unified Semantic Transformer for 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251214698-timelens-rethinking-video-temporal-grounding-with-multimodal-llms.html">TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</a></td>
  <td>TimeLensï¼šé€šè¿‡å¤šæ¨¡æ€LLMé‡æ–°æ€è€ƒè§†é¢‘æ—¶åºå®šä½ï¼Œæ„å»ºé«˜è´¨é‡åŸºçº¿ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14698" onclick="toggleFavorite(this, '2512.14698', 'TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>33</td>
  <td><a href="./papers/251213573-mmhops-r1-multimodal-multi-hop-reasoning.html">MMhops-R1: Multimodal Multi-hop Reasoning</a></td>
  <td>æå‡ºMMhopsåŸºå‡†å’ŒMMhops-R1æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œæå‡å¤šæ¨¡æ€å¤šè·³æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13573" onclick="toggleFavorite(this, '2512.13573', 'MMhops-R1: Multimodal Multi-hop Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251213507-seedance-15-pro-a-native-audio-visual-joint-generation-foundation-mode.html">Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</a></td>
  <td>Seedance 1.5 proï¼šåŸç”ŸéŸ³è§†é¢‘è”åˆç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œæå‡ä¸“ä¸šçº§å†…å®¹åˆ›ä½œèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">RLHF</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13507" onclick="toggleFavorite(this, '2512.13507', 'Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251214309-psmamba-progressive-self-supervised-vision-mamba-for-plant-disease-rec.html">PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition</a></td>
  <td>PSMambaï¼šä¸€ç§ç”¨äºæ¤ç‰©ç—…å®³è¯†åˆ«çš„æ¸è¿›å¼è‡ªç›‘ç£è§†è§‰Mambaæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">teacher-student</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14309" onclick="toggleFavorite(this, '2512.14309', 'PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>36</td>
  <td><a href="./papers/250806453-text-embedded-swin-umamba-for-deeplesion-segmentation.html">Text Embedded Swin-UMamba for DeepLesion Segmentation</a></td>
  <td>æå‡ºText Embedded Swin-UMambaæ¨¡å‹ï¼Œç”¨äºèåˆæ–‡æœ¬ä¿¡æ¯çš„DeepLesionç—…ç¶åˆ†å‰²ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2508.06453" onclick="toggleFavorite(this, '2508.06453', 'Text Embedded Swin-UMamba for DeepLesion Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/250105711-from-my-view-to-yours-ego-to-exo-transfer-in-vlms-for-understanding-ac.html">From My View to Yours: Ego-to-Exo Transfer in VLMs for Understanding Activities of Daily Living</a></td>
  <td>æå‡ºEgo2ExoVLMï¼Œè§£å†³VLMåœ¨æ—¥å¸¸æ´»åŠ¨ç†è§£ä¸­è§†è§’è½¬æ¢çš„éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">human-object interaction</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2501.05711" onclick="toggleFavorite(this, '2501.05711', 'From My View to Yours: Ego-to-Exo Transfer in VLMs for Understanding Activities of Daily Living')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>38</td>
  <td><a href="./papers/250904687-guideline-consistent-segmentation-via-multi-agent-refinement.html">Guideline-Consistent Segmentation via Multi-Agent Refinement</a></td>
  <td>æå‡ºä¸€ç§å¤šæ™ºèƒ½ä½“è¿­ä»£ä¼˜åŒ–æ¡†æ¶ï¼Œå®ç°ç¬¦åˆå¤æ‚æŒ‡å—çš„è¯­ä¹‰åˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2509.04687" onclick="toggleFavorite(this, '2509.04687', 'Guideline-Consistent Segmentation via Multi-Agent Refinement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>39</td>
  <td><a href="./papers/251213874-sage-training-smart-any-horizon-agents-for-long-video-reasoning-with-r.html">SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning</a></td>
  <td>æå‡ºSAGEï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä»»æ„æ—¶åŸŸAgentï¼Œç”¨äºé•¿è§†é¢‘æ¨ç†ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13874" onclick="toggleFavorite(this, '2512.13874', 'SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>40</td>
  <td><a href="./papers/251214440-s2d-sparse-to-dense-keymask-distillation-for-unsupervised-video-instan.html">S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation</a></td>
  <td>æå‡ºS2Dï¼šä¸€ç§ç¨€ç–åˆ°ç¨ å¯†çš„Keymaskè’¸é¦æ–¹æ³•ï¼Œç”¨äºæ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14440" onclick="toggleFavorite(this, '2512.14440', 'S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>41</td>
  <td><a href="./papers/251208980-training-multi-image-vision-agents-via-end2end-reinforcement-learning.html">Training Multi-Image Vision Agents via End2End Reinforcement Learning</a></td>
  <td>æå‡ºIMAgentï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šå›¾è§†è§‰Agentï¼Œè§£å†³å¤æ‚å¤šå›¾QAä»»åŠ¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.08980" onclick="toggleFavorite(this, '2512.08980', 'Training Multi-Image Vision Agents via End2End Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>42</td>
  <td><a href="./papers/251214056-facedit-unified-talking-face-editing-and-generation-via-facial-motion-.html">FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling</a></td>
  <td>FacEDiTï¼šé€šè¿‡é¢éƒ¨è¿åŠ¨å¡«å……ç»Ÿä¸€å®ç°è¯´è¯äººè„¸ç¼–è¾‘ä¸ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">masked autoencoder</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14056" onclick="toggleFavorite(this, '2512.14056', 'FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>43</td>
  <td><a href="./papers/251214614v1-worldplay-towards-long-term-geometric-consistency-for-real-time-inte.html">WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling</a></td>
  <td>WorldPlayï¼šæå‡ºä¸€ç§å…·æœ‰é•¿æœŸå‡ ä½•ä¸€è‡´æ€§çš„å®æ—¶äº¤äº’å¼ä¸–ç•Œå»ºæ¨¡æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14614v1" onclick="toggleFavorite(this, '2512.14614v1', 'WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>44</td>
  <td><a href="./papers/251214309v1-psmamba-progressive-self-supervised-vision-mamba-for-plant-disease-r.html">PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition</a></td>
  <td>PSMambaï¼šä¸€ç§ç”¨äºæ¤ç‰©ç—…å®³è¯†åˆ«çš„æ¸è¿›å¼è‡ªç›‘ç£è§†è§‰Mambaæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">representation learning</span> <span class="paper-tag">teacher-student</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14309v1" onclick="toggleFavorite(this, '2512.14309v1', 'PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>45</td>
  <td><a href="./papers/251214442v1-a4-agent-an-agentic-framework-for-zero-shot-affordance-reasoning.html">A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</a></td>
  <td>æå‡ºA4-Agentï¼šä¸€ç§ç”¨äºé›¶æ ·æœ¬å¯ä¾›æ€§æ¨ç†çš„Agentæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">dreamer</span> <span class="paper-tag">affordance prediction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14442v1" onclick="toggleFavorite(this, '2512.14442v1', 'A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>46</td>
  <td><a href="./papers/251214056v1-facedit-unified-talking-face-editing-and-generation-via-facial-motio.html">FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling</a></td>
  <td>FacEDiTï¼šé€šè¿‡é¢éƒ¨è¿åŠ¨å¡«å……å®ç°ç»Ÿä¸€çš„è¯´è¯äººè„¸ç¼–è¾‘ä¸ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">flow matching</span> <span class="paper-tag">masked autoencoder</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14056v1" onclick="toggleFavorite(this, '2512.14056v1', 'FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (20 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>47</td>
  <td><a href="./papers/251214352-hgs-hybrid-gaussian-splatting-with-static-dynamic-decomposition-for-co.html">HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis</a></td>
  <td>æå‡ºHGSæ··åˆé«˜æ–¯æº…å°„æ–¹æ³•ï¼Œé€šè¿‡é™æ€-åŠ¨æ€åˆ†è§£å®ç°ç´§å‡‘çš„åŠ¨æ€è§†è§’åˆæˆ</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14352" onclick="toggleFavorite(this, '2512.14352', 'HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>48</td>
  <td><a href="./papers/251214536-dasp-self-supervised-nighttime-monocular-depth-estimation-with-domain-.html">DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors</a></td>
  <td>DASPï¼šåˆ©ç”¨æ—¶ç©ºå…ˆéªŒåŸŸé€‚åº”çš„è‡ªç›‘ç£å¤œé—´å•ç›®æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14536" onclick="toggleFavorite(this, '2512.14536', 'DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>49</td>
  <td><a href="./papers/250309342-gaspacho-gaussian-splatting-for-controllable-humans-and-objects.html">GASPACHO: Gaussian Splatting for Controllable Humans and Objects</a></td>
  <td>GASPACHOï¼šåŸºäºé«˜æ–¯æº…å°„çš„å¯æ§äººä¸ç‰©ä½“äº¤äº’æ¸²æŸ“</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">physically plausible</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2503.09342" onclick="toggleFavorite(this, '2503.09342', 'GASPACHO: Gaussian Splatting for Controllable Humans and Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>50</td>
  <td><a href="./papers/251214087-gaussianplant-structure-aligned-gaussian-splatting-for-3d-reconstructi.html">GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants</a></td>
  <td>GaussianPlantï¼šæå‡ºç»“æ„å¯¹é½çš„é«˜æ–¯æº…å°„æ–¹æ³•ï¼Œç”¨äºæ¤ç‰©ä¸‰ç»´é‡å»º</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14087" onclick="toggleFavorite(this, '2512.14087', 'GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>51</td>
  <td><a href="./papers/251212012-semantic-drive-democratizing-long-tail-data-curation-via-open-vocabula.html">Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</a></td>
  <td>Semantic-Driveï¼šé€šè¿‡å¼€æ”¾è¯æ±‡ grounding å’Œç¥ç»ç¬¦å· VLM å…±è¯†å®ç°é•¿å°¾æ•°æ®æŒ–æ˜</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">symbolic grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12012" onclick="toggleFavorite(this, '2512.12012', 'Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>52</td>
  <td><a href="./papers/251214200-beyond-a-single-light-a-large-scale-aerial-dataset-for-urban-scene-rec.html">Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination</a></td>
  <td>æå‡ºSkyLumeæ•°æ®é›†ï¼Œç”¨äºè§£å†³åŸå¸‚åœºæ™¯ä¸‰ç»´é‡å»ºä¸­å…‰ç…§å˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14200" onclick="toggleFavorite(this, '2512.14200', 'Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>53</td>
  <td><a href="./papers/250515208-gt2-gs-geometry-aware-texture-transfer-for-gaussian-splatting.html">GT2-GS: Geometry-aware Texture Transfer for Gaussian Splatting</a></td>
  <td>æå‡ºGT2-GSï¼Œç”¨äºé«˜æ–¯æº…å°„çš„å‡ ä½•æ„ŸçŸ¥çº¹ç†è¿ç§»ï¼Œæå‡3Då†…å®¹åˆ›ä½œæ•ˆç‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.15208" onclick="toggleFavorite(this, '2505.15208', 'GT2-GS: Geometry-aware Texture Transfer for Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>54</td>
  <td><a href="./papers/251207345-debiasing-diffusion-priors-via-3d-attention-for-consistent-gaussian-sp.html">Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting</a></td>
  <td>æå‡ºTD-Attnï¼Œé€šè¿‡3Dæ³¨æ„åŠ›æœºåˆ¶æ¶ˆé™¤æ‰©æ•£å…ˆéªŒåå·®ï¼Œæå‡é«˜æ–¯æº…å°„ä¸€è‡´æ€§</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.07345" onclick="toggleFavorite(this, '2512.07345', 'Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>55</td>
  <td><a href="./papers/251214020-deep-learning-perspective-of-scene-understanding-in-autonomous-robots.html">Deep Learning Perspective of Scene Understanding in Autonomous Robots</a></td>
  <td>ç»¼è¿°æ·±åº¦å­¦ä¹ åœ¨è‡ªä¸»æœºå™¨äººåœºæ™¯ç†è§£ä¸­çš„åº”ç”¨ï¼Œæå‡æœºå™¨äººæ„ŸçŸ¥ä¸å†³ç­–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14020" onclick="toggleFavorite(this, '2512.14020', 'Deep Learning Perspective of Scene Understanding in Autonomous Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>56</td>
  <td><a href="./papers/251214126-consistent-instance-field-for-dynamic-scene-understanding.html">Consistent Instance Field for Dynamic Scene Understanding</a></td>
  <td>æå‡ºä¸€è‡´æ€§å®ä¾‹åœºï¼Œç”¨äºåŠ¨æ€åœºæ™¯ç†è§£ä¸­çš„æ—¶ç©ºè¿ç»­å»ºæ¨¡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14126" onclick="toggleFavorite(this, '2512.14126', 'Consistent Instance Field for Dynamic Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>57</td>
  <td><a href="./papers/251214406-broadening-view-synthesis-of-dynamic-scenes-from-constrained-monocular.html">Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos</a></td>
  <td>ExpanDyNeRFï¼šæ‰©å±•åŠ¨æ€åœºæ™¯è§†è§’åˆæˆï¼Œè§£å†³å•ç›®è§†é¢‘å¤§è§’åº¦æ¸²æŸ“å¤±çœŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14406" onclick="toggleFavorite(this, '2512.14406', 'Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>58</td>
  <td><a href="./papers/251213177-mmdrive-interactive-scene-understanding-beyond-vision-with-multi-repre.html">MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</a></td>
  <td>MMDriveï¼šæå‡ºå¤šæ¨¡æ€èåˆçš„äº¤äº’å¼åœºæ™¯ç†è§£æ¡†æ¶ï¼Œè¶…è¶Šè§†è§‰é™åˆ¶ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13177" onclick="toggleFavorite(this, '2512.13177', 'MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>59</td>
  <td><a href="./papers/251204585-sam3-i-segment-anything-with-instructions.html">SAM3-I: Segment Anything with Instructions</a></td>
  <td>SAM3-Iï¼šé€šè¿‡æŒ‡ä»¤æ„ŸçŸ¥çš„çº§è”é€‚é…ï¼Œå¢å¼ºSAM3ä»¥å®ç°æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒåˆ†å‰²</td>
  <td class="tags-cell"><span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.04585" onclick="toggleFavorite(this, '2512.04585', 'SAM3-I: Segment Anything with Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>60</td>
  <td><a href="./papers/251213796-nexels-neurally-textured-surfels-for-real-time-novel-view-synthesis-wi.html">Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries</a></td>
  <td>æå‡ºåŸºäºç¥ç»çº¹ç†Surfelsçš„æ–°è§†è§’åˆæˆæ–¹æ³•ï¼Œåœ¨ç¨€ç–å‡ ä½•ä¸‹å®ç°å®æ—¶æ¸²æŸ“ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13796" onclick="toggleFavorite(this, '2512.13796', 'Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>61</td>
  <td><a href="./papers/251214028-robust-single-shot-structured-light-3d-imaging-via-neural-feature-deco.html">Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding</a></td>
  <td>æå‡ºåŸºäºç¥ç»ç‰¹å¾è§£ç çš„é²æ£’å•ç›®ç»“æ„å…‰3Dæˆåƒæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14028" onclick="toggleFavorite(this, '2512.14028', 'Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>62</td>
  <td><a href="./papers/251214039-asap-textured-gaussians-enhancing-textured-gaussians-with-adaptive-sam.html">ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization</a></td>
  <td>ASAP-Textured Gaussiansï¼šé€šè¿‡è‡ªé€‚åº”é‡‡æ ·å’Œå„å‘å¼‚æ€§å‚æ•°åŒ–å¢å¼ºçº¹ç†é«˜æ–¯æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14039" onclick="toggleFavorite(this, '2512.14039', 'ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>63</td>
  <td><a href="./papers/251214180-spherical-voronoi-directional-appearance-as-a-differentiable-partition.html">Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere</a></td>
  <td>æå‡ºçƒVoronoiå›¾ï¼Œç”¨äº3Dé«˜æ–¯æº…å°„ä¸­å¯å¾®çš„æ–¹å‘å¤–è§‚å»ºæ¨¡</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14180" onclick="toggleFavorite(this, '2512.14180', 'Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>64</td>
  <td><a href="./papers/250624121-textmesh4d-text-to-4d-mesh-generation-via-jacobian-deformation-field.html">TextMesh4D: Text-to-4D Mesh Generation via Jacobian Deformation Field</a></td>
  <td>æå‡º TextMesh4Dï¼Œé€šè¿‡é›…å¯æ¯”å½¢å˜åœºå®ç°æ–‡æœ¬é©±åŠ¨çš„4Dç½‘æ ¼ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3DGS</span> <span class="paper-tag">NeRF</span> <span class="paper-tag">spatiotemporal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.24121" onclick="toggleFavorite(this, '2506.24121', 'TextMesh4D: Text-to-4D Mesh Generation via Jacobian Deformation Field')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>65</td>
  <td><a href="./papers/251214236-elastic3d-controllable-stereo-video-conversion-with-guided-latent-deco.html">Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding</a></td>
  <td>Elastic3Dï¼šåŸºäºå¼•å¯¼å¼æ½œåœ¨è§£ç çš„å¯æ§ç«‹ä½“è§†é¢‘è½¬æ¢</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14236" onclick="toggleFavorite(this, '2512.14236', 'Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>66</td>
  <td><a href="./papers/251212165-audio-visual-camera-pose-estimation-with-passive-scene-sounds-and-in-t.html">Audio-Visual Camera Pose Estimation with Passive Scene Sounds and In-the-Wild Video</a></td>
  <td>æå‡ºä¸€ç§åˆ©ç”¨è¢«åŠ¨åœºæ™¯å£°éŸ³è¿›è¡Œç›¸æœºä½å§¿ä¼°è®¡çš„éŸ³è§†é¢‘èåˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.12165" onclick="toggleFavorite(this, '2512.12165', 'Audio-Visual Camera Pose Estimation with Passive Scene Sounds and In-the-Wild Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (20 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>67</td>
  <td><a href="./papers/251214095v1-anchorhoi-zero-shot-generation-of-4d-human-object-interaction-via-an.html">AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation</a></td>
  <td>AnchorHOIï¼šåŸºäºé”šç‚¹çš„å…ˆéªŒçŸ¥è¯†è’¸é¦å®ç°é›¶æ ·æœ¬4Däºº-ç‰©äº¤äº’ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">neural radiance</span> <span class="paper-tag">motion synthesis</span> <span class="paper-tag">human-object interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14095v1" onclick="toggleFavorite(this, '2512.14095v1', 'AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>68</td>
  <td><a href="./papers/251214352v1-hgs-hybrid-gaussian-splatting-with-static-dynamic-decomposition-for-.html">HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis</a></td>
  <td>æå‡ºHGSæ··åˆé«˜æ–¯æº…å°„æ–¹æ³•ï¼Œé€šè¿‡é™æ€-åŠ¨æ€è§£è€¦å®ç°ç´§å‡‘çš„åŠ¨æ€åœºæ™¯æ–°è§†è§’åˆæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14352v1" onclick="toggleFavorite(this, '2512.14352v1', 'HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>69</td>
  <td><a href="./papers/251214200v1-beyond-a-single-light-a-large-scale-aerial-dataset-for-urban-scene-r.html">Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination</a></td>
  <td>SkyLumeï¼šä¸€ä¸ªå¤§è§„æ¨¡å¤šå…‰ç…§åŸå¸‚é‡å»ºèˆªæ‹æ•°æ®é›†ï¼Œç”¨äºè§£å†³å…‰ç…§å˜åŒ–ä¸‹çš„ä¸‰ç»´é‡å»ºé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">neural radiance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14200v1" onclick="toggleFavorite(this, '2512.14200v1', 'Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>70</td>
  <td><a href="./papers/251214020v1-deep-learning-perspective-of-scene-understanding-in-autonomous-robot.html">Deep Learning Perspective of Scene Understanding in Autonomous Robots</a></td>
  <td>ç»¼è¿°æ·±åº¦å­¦ä¹ åœ¨è‡ªä¸»æœºå™¨äººåœºæ™¯ç†è§£ä¸­çš„åº”ç”¨ï¼Œæå‡æœºå™¨äººæ„ŸçŸ¥ä¸å†³ç­–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span> <span class="paper-tag">SLAM</span> <span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14020v1" onclick="toggleFavorite(this, '2512.14020v1', 'Deep Learning Perspective of Scene Understanding in Autonomous Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>71</td>
  <td><a href="./papers/251214536v1-dasp-self-supervised-nighttime-monocular-depth-estimation-with-domai.html">DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors</a></td>
  <td>DASPï¼šåˆ©ç”¨æ—¶ç©ºå…ˆéªŒåŸŸé€‚åº”çš„è‡ªç›‘ç£å¤œé—´å•ç›®æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14536v1" onclick="toggleFavorite(this, '2512.14536v1', 'DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>72</td>
  <td><a href="./papers/251214406v1-broadening-view-synthesis-of-dynamic-scenes-from-constrained-monocul.html">Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos</a></td>
  <td>ExpanDyNeRFï¼šæ‰©å±•åŠ¨æ€åœºæ™¯è§†è§’åˆæˆï¼Œè§£å†³å•ç›®è§†é¢‘å¤§è§’åº¦æ¸²æŸ“å¤±çœŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14406v1" onclick="toggleFavorite(this, '2512.14406v1', 'Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>73</td>
  <td><a href="./papers/251214087v1-gaussianplant-structure-aligned-gaussian-splatting-for-3d-reconstruc.html">GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants</a></td>
  <td>æå‡ºGaussianPlantä»¥è§£å†³æ¤ç‰©3Dé‡å»ºä¸­çš„ç»“æ„ä¸å¤–è§‚åˆ†ç¦»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14087v1" onclick="toggleFavorite(this, '2512.14087v1', 'GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>74</td>
  <td><a href="./papers/251214032v1-ace-slam-scene-coordinate-regression-for-neural-implicit-real-time-s.html">ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM</a></td>
  <td>ACE-SLAMï¼šåŸºäºåœºæ™¯åæ ‡å›å½’çš„ç¥ç»éšå¼å®æ—¶SLAMç³»ç»Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">SLAM</span> <span class="paper-tag">localization</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14032v1" onclick="toggleFavorite(this, '2512.14032v1', 'ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>75</td>
  <td><a href="./papers/251214560v1-clnet-cross-view-correspondence-makes-a-stronger-geo-localizationer.html">CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer</a></td>
  <td>æå‡ºCLNetï¼Œé€šè¿‡è·¨è§†è§’å¯¹åº”å…³ç³»å¢å¼ºå›¾åƒæ£€ç´¢åœ°ç†å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14560v1" onclick="toggleFavorite(this, '2512.14560v1', 'CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>76</td>
  <td><a href="./papers/251214364v1-unified-semantic-transformer-for-3d-scene-understanding.html">Unified Semantic Transformer for 3D Scene Understanding</a></td>
  <td>æå‡ºUNITEï¼šç”¨äº3Dåœºæ™¯ç†è§£çš„ç»Ÿä¸€è¯­ä¹‰Transformeræ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14364v1" onclick="toggleFavorite(this, '2512.14364v1', 'Unified Semantic Transformer for 3D Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>77</td>
  <td><a href="./papers/251214235v1-4d-radiff-latent-diffusion-for-4d-radar-point-cloud-generation.html">4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</a></td>
  <td>æå‡º4D-RaDiffï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆ4Dé›·è¾¾ç‚¹äº‘ï¼Œæå‡ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14235v1" onclick="toggleFavorite(this, '2512.14235v1', '4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>78</td>
  <td><a href="./papers/251214222v1-history-enhanced-two-stage-transformer-for-aerial-vision-and-languag.html">History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation</a></td>
  <td>æå‡ºå†å²å¢å¼ºå‹ä¸¤é˜¶æ®µTransformerï¼Œè§£å†³æ— äººæœºè§†è§‰è¯­è¨€å¯¼èˆªä¸­å…¨å±€æ¨ç†ä¸å±€éƒ¨ç†è§£çš„å¹³è¡¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">navigation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14222v1" onclick="toggleFavorite(this, '2512.14222v1', 'History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>79</td>
  <td><a href="./papers/251214180v1-spherical-voronoi-directional-appearance-as-a-differentiable-partiti.html">Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere</a></td>
  <td>æå‡ºçƒé¢Voronoiæ–¹æ³•ï¼Œç”¨äº3Dé«˜æ–¯æº…å°„ä¸­é«˜æ•ˆå¯å¾®çš„æ–¹å‘å¤–è§‚å»ºæ¨¡</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">novel view synthesis</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14180v1" onclick="toggleFavorite(this, '2512.14180v1', 'Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>80</td>
  <td><a href="./papers/251214162v1-fastddhpose-towards-unified-efficient-and-disentangled-3d-human-pose.html">FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation</a></td>
  <td>FastDDHPoseï¼šç»Ÿä¸€ã€é«˜æ•ˆã€è§£è€¦çš„3Däººä½“å§¿æ€ä¼°è®¡æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14162v1" onclick="toggleFavorite(this, '2512.14162v1', 'FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>81</td>
  <td><a href="./papers/251214126v1-consistent-instance-field-for-dynamic-scene-understanding.html">Consistent Instance Field for Dynamic Scene Understanding</a></td>
  <td>æå‡ºä¸€è‡´æ€§å®ä¾‹åœºï¼Œç”¨äºåŠ¨æ€åœºæ™¯ç†è§£ä¸­çš„æ—¶ç©ºä¸€è‡´æ€§åˆ†å‰²ä¸æŸ¥è¯¢ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14126v1" onclick="toggleFavorite(this, '2512.14126v1', 'Consistent Instance Field for Dynamic Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>82</td>
  <td><a href="./papers/251214028v1-robust-single-shot-structured-light-3d-imaging-via-neural-feature-de.html">Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding</a></td>
  <td>æå‡ºåŸºäºç¥ç»ç‰¹å¾è§£ç çš„é²æ£’å•ç›®ç»“æ„å…‰3Dæˆåƒæ–¹æ³•ï¼Œæå‡å¤æ‚åœºæ™¯ä¸‹çš„æ·±åº¦ä¼°è®¡ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">feature matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14028v1" onclick="toggleFavorite(this, '2512.14028v1', 'Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>83</td>
  <td><a href="./papers/251214039v1-asap-textured-gaussians-enhancing-textured-gaussians-with-adaptive-s.html">ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization</a></td>
  <td>ASAP-Textured Gaussiansï¼šé€šè¿‡è‡ªé€‚åº”é‡‡æ ·å’Œå„å‘å¼‚æ€§å‚æ•°åŒ–å¢å¼ºçº¹ç†é«˜æ–¯æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14039v1" onclick="toggleFavorite(this, '2512.14039v1', 'ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>84</td>
  <td><a href="./papers/251214274v1-tun-detecting-significant-points-in-persistence-diagrams-with-deep-l.html">TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning</a></td>
  <td>æå‡ºTUNç½‘ç»œï¼Œåˆ©ç”¨æ·±åº¦å­¦ä¹ è‡ªåŠ¨æ£€æµ‹æŒä¹…åŒè°ƒå›¾ä¸­æ˜¾è‘—ç‰¹å¾ç‚¹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14274v1" onclick="toggleFavorite(this, '2512.14274v1', 'TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>85</td>
  <td><a href="./papers/251214273v1-zoom-zero-reinforced-coarse-to-fine-video-understanding-via-temporal.html">Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in</a></td>
  <td>Zoom-Zeroï¼šé€šè¿‡æ—¶é—´åŸŸç¼©æ”¾å¢å¼ºè§†é¢‘ç†è§£ï¼Œè§£å†³GVQAä¸­æ—¶åºå®šä½ä¸å‡†é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14273v1" onclick="toggleFavorite(this, '2512.14273v1', 'Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>86</td>
  <td><a href="./papers/251214236v1-elastic3d-controllable-stereo-video-conversion-with-guided-latent-de.html">Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding</a></td>
  <td>Elastic3Dï¼šåŸºäºå¼•å¯¼å¼æ½œåœ¨è§£ç çš„å¯æ§ç«‹ä½“è§†é¢‘è½¬æ¢æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14236v1" onclick="toggleFavorite(this, '2512.14236v1', 'Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>87</td>
  <td><a href="./papers/251214696-crisp-contact-guided-real2sim-from-monocular-video-with-planar-scene-p.html">CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</a></td>
  <td>CRISPï¼šåŸºäºå•ç›®è§†é¢‘å’Œå¹³é¢åœºæ™¯åŸè¯­çš„æ¥è§¦å¼•å¯¼Real2Simæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid control</span> <span class="paper-tag">real2sim</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14696" onclick="toggleFavorite(this, '2512.14696', 'CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>88</td>
  <td><a href="./papers/250315770-physically-grounded-monocular-depth-via-nanophotonic-wavefront-prompti.html">Physically Grounded Monocular Depth via Nanophotonic Wavefront Prompting</a></td>
  <td>åˆ©ç”¨çº³ç±³å…‰å­æ³¢å‰è°ƒæ§ï¼Œå®ç°ç‰©ç†å¯ä¿¡çš„å•ç›®æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">metric depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2503.15770" onclick="toggleFavorite(this, '2503.15770', 'Physically Grounded Monocular Depth via Nanophotonic Wavefront Prompting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>89</td>
  <td><a href="./papers/251214217-draw2act-turning-depth-encoded-trajectories-into-robotic-demonstration.html">DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos</a></td>
  <td>DRAW2ACTï¼šæå‡ºæ·±åº¦æ„ŸçŸ¥çš„è½¨è¿¹æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºæœºå™¨äººæ“ä½œæ¼”ç¤ºè§†é¢‘ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14217" onclick="toggleFavorite(this, '2512.14217', 'DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>90</td>
  <td><a href="./papers/251214320-semantic-mismatch-and-perceptual-degradation-a-new-perspective-on-imag.html">Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</a></td>
  <td>æå‡ºSIFMï¼Œé€šè¿‡å¯¹æŠ—æ‰©æ•£æ¨¡å‹ä¸­é—´ç‰¹å¾æ‰°åŠ¨å®ç°å›¾åƒç¼–è¾‘å…ç–«ï¼Œå¹¶æå‡ºISRè¯„ä¼°æŒ‡æ ‡ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14320" onclick="toggleFavorite(this, '2512.14320', 'Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>91</td>
  <td><a href="./papers/251214336-vector-prism-animating-vector-graphics-by-stratifying-semantic-structu.html">Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure</a></td>
  <td>Vector Prismï¼šé€šè¿‡åˆ†å±‚è¯­ä¹‰ç»“æ„å®ç°çŸ¢é‡å›¾å½¢åŠ¨ç”»</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14336" onclick="toggleFavorite(this, '2512.14336', 'Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>92</td>
  <td><a href="./papers/251214341-towards-transferable-defense-against-malicious-image-edits.html">Towards Transferable Defense Against Malicious Image Edits</a></td>
  <td>æå‡ºTDAEæ¡†æ¶ï¼Œå¢å¼ºå›¾åƒå¯¹æ¶æ„ç¼–è¾‘çš„é˜²å¾¡è¿ç§»èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14341" onclick="toggleFavorite(this, '2512.14341', 'Towards Transferable Defense Against Malicious Image Edits')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>93</td>
  <td><a href="./papers/251214696v1-crisp-contact-guided-real2sim-from-monocular-video-with-planar-scene.html">CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</a></td>
  <td>CRISPï¼šåŸºäºå•ç›®è§†é¢‘å’Œå¹³é¢åœºæ™¯åŸè¯­çš„æ¥è§¦å¼•å¯¼Real2Simæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid control</span> <span class="paper-tag">real2sim</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14696v1" onclick="toggleFavorite(this, '2512.14696v1', 'CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>94</td>
  <td><a href="./papers/251214336v1-vector-prism-animating-vector-graphics-by-stratifying-semantic-struc.html">Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure</a></td>
  <td>æå‡ºVector Prismï¼Œé€šè¿‡åˆ†å±‚è¯­ä¹‰ç»“æ„å®ç°çŸ¢é‡å›¾å½¢åŠ¨ç”»</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14336v1" onclick="toggleFavorite(this, '2512.14336v1', 'Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>95</td>
  <td><a href="./papers/251214217v1-draw2act-turning-depth-encoded-trajectories-into-robotic-demonstrati.html">DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos</a></td>
  <td>æå‡ºDRAW2ACTä»¥è§£å†³æœºå™¨äººæ¼”ç¤ºè§†é¢‘ç”Ÿæˆçš„å¯æ§æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14217v1" onclick="toggleFavorite(this, '2512.14217v1', 'DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>96</td>
  <td><a href="./papers/251213840-molingo-motion-language-alignment-for-text-to-motion-generation.html">MoLingo: Motion-Language Alignment for Text-to-Motion Generation</a></td>
  <td>MoLingoï¼šé€šè¿‡è¿åŠ¨-è¯­è¨€å¯¹é½å®ç°æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆï¼Œè¾¾åˆ°æ–°çš„SOTA</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion generation</span> <span class="paper-tag">motion latent</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13840" onclick="toggleFavorite(this, '2512.13840', 'MoLingo: Motion-Language Alignment for Text-to-Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>97</td>
  <td><a href="./papers/251213747-why-text-prevails-vision-may-undermine-multimodal-medical-decision-mak.html">Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making</a></td>
  <td>ç ”ç©¶è¡¨æ˜ï¼šåœ¨åŒ»å­¦å†³ç­–ä¸­ï¼Œæ–‡æœ¬ä¿¡æ¯ä¼˜äºè§†è§‰ä¿¡æ¯ï¼Œå¤šæ¨¡æ€èåˆå¯èƒ½é€‚å¾—å…¶å</td>
  <td class="tags-cell"><span class="paper-tag">MDM</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.13747" onclick="toggleFavorite(this, '2512.13747', 'Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>98</td>
  <td><a href="./papers/251214008-sparse-lavida-sparse-multimodal-discrete-diffusion-language-models.html">Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models</a></td>
  <td>Sparse-LaViDaï¼šé€šè¿‡ç¨€ç–åŒ–é‡‡æ ·åŠ é€Ÿå¤šæ¨¡æ€ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">MDM</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14008" onclick="toggleFavorite(this, '2512.14008', 'Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>99</td>
  <td><a href="./papers/251214234-vibes-a-conversational-agent-with-behaviorally-intelligent-3d-virtual-.html">ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body</a></td>
  <td>ViBESï¼šä¸€ä¸ªå…·æœ‰è¡Œä¸ºæ™ºèƒ½çš„3Dè™šæ‹ŸåŒ–èº«å¯¹è¯ä»£ç†</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion generation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14234" onclick="toggleFavorite(this, '2512.14234', 'ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>100</td>
  <td><a href="./papers/251214677-vasa-3d-lifelike-audio-driven-gaussian-head-avatars-from-a-single-imag.html">VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</a></td>
  <td>VASA-3Dï¼šåŸºäºå•å¼ å›¾åƒçš„é€¼çœŸéŸ³é¢‘é©±åŠ¨é«˜æ–¯å¤´éƒ¨åŒ–èº«ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">motion latent</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14677" onclick="toggleFavorite(this, '2512.14677', 'VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>101</td>
  <td><a href="./papers/251214234v1-vibes-a-conversational-agent-with-behaviorally-intelligent-3d-virtua.html">ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body</a></td>
  <td>ViBESï¼šä¸€ç§å…·æœ‰è¡Œä¸ºæ™ºèƒ½çš„3Dè™šæ‹ŸåŒ–èº«å¯¹è¯ä»£ç†</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14234v1" onclick="toggleFavorite(this, '2512.14234v1', 'ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>102</td>
  <td><a href="./papers/251214677v1-vasa-3d-lifelike-audio-driven-gaussian-head-avatars-from-a-single-im.html">VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</a></td>
  <td>VASA-3Dï¼šåŸºäºå•å¼ å›¾åƒçš„é€¼çœŸéŸ³é¢‘é©±åŠ¨é«˜æ–¯å¤´éƒ¨åŒ–èº«ç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">motion latent</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14677v1" onclick="toggleFavorite(this, '2512.14677v1', 'VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>103</td>
  <td><a href="./papers/250309143-exo2ego-exocentric-knowledge-guided-mllm-for-egocentric-video-understa.html">Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding</a></td>
  <td>æå‡ºExo2Egoï¼Œåˆ©ç”¨å¤–è§†çŸ¥è¯†æŒ‡å¯¼MLLMè¿›è¡Œç¬¬ä¸€äººç§°è§†é¢‘ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2503.09143" onclick="toggleFavorite(this, '2503.09143', 'Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>104</td>
  <td><a href="./papers/250705838-inter-and-intra-image-refinement-for-few-shot-segmentation.html">Inter- and Intra-image Refinement for Few Shot Segmentation</a></td>
  <td>æå‡ºInter- and Intra-image Refinementæ¨¡å‹ï¼Œè§£å†³å°‘æ ·æœ¬åˆ†å‰²ä¸­ç±»å†…å·®å¼‚å’Œç±»é—´å¹²æ‰°é—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2507.05838" onclick="toggleFavorite(this, '2507.05838', 'Inter- and Intra-image Refinement for Few Shot Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>105</td>
  <td><a href="./papers/251116349-cristal-real-time-camera-registration-in-static-lidar-scans-using-neur.html">CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering</a></td>
  <td>CRISTALï¼šåˆ©ç”¨ç¥ç»æ¸²æŸ“åœ¨é™æ€æ¿€å…‰é›·è¾¾æ‰«æä¸­è¿›è¡Œå®æ—¶ç›¸æœºæ³¨å†Œ</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2511.16349" onclick="toggleFavorite(this, '2511.16349', 'CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>106</td>
  <td><a href="./papers/251214099-viewmask-1-to-3-multi-view-consistent-image-generation-via-multimodal-.html">ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models</a></td>
  <td>ViewMask-1-to-3ï¼šåŸºäºå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹å®ç°å¤šè§†è§’ä¸€è‡´çš„å›¾åƒç”Ÿæˆ</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14099" onclick="toggleFavorite(this, '2512.14099', 'ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>107</td>
  <td><a href="./papers/251214140-sketchassist-a-practical-assistant-for-semantic-edits-and-precise-loca.html">SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing</a></td>
  <td>SketchAssistï¼šç”¨äºè¯­ä¹‰ç¼–è¾‘å’Œç²¾ç¡®å±€éƒ¨é‡ç»˜çš„å®ç”¨è‰å›¾è¾…åŠ©å·¥å…·</td>
  <td class="tags-cell"><span class="paper-tag">structure preservation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.14140" onclick="toggleFavorite(this, '2512.14140', 'SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)