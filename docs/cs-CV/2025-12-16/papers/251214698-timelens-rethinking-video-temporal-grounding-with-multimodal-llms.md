---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14698" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14698</a>
  <a href="https://arxiv.org/pdf/2512.14698.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14698" onclick="toggleFavorite(this, '2512.14698', 'TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TimeLensï¼šé€šè¿‡å¤šæ¨¡æ€LLMé‡æ–°æ€è€ƒè§†é¢‘æ—¶åºå®šä½ï¼Œæ„å»ºé«˜è´¨é‡åŸºçº¿ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€LLM` `é«˜è´¨é‡æ•°æ®é›†` `å¼ºåŒ–å­¦ä¹ ` `è§†é¢‘ç†è§£` `æ—¶é—´è¡¨ç¤º` `æ•°æ®é‡æ ‡æ³¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ—¶åºå®šä½åŸºå‡†æµ‹è¯•å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹è¯„ä¼°ç»“æœä¸å¯é ï¼Œé˜»ç¢äº†æœ‰æ•ˆæ–¹æ³•çš„å‘å±•ã€‚
2. TimeLensé€šè¿‡é«˜è´¨é‡æ•°æ®æ„å»ºå’Œç®—æ³•è®¾è®¡ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨å¤šæ¨¡æ€LLMæå‡è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›ã€‚
3. TimeLensæ¨¡å‹åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç”šè‡³è¶…è¶Šäº†éƒ¨åˆ†é—­æºæ¨¡å‹ï¼Œä¸ºå¼€æºç¤¾åŒºæä¾›äº†å¼ºå¤§çš„åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶éæå‡ºä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘ç†è§£ä¸­çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰å»ºç«‹äº†ä¸€ä¸ªç›´æ¥ã€å¢é‡ä½†è‡³å…³é‡è¦çš„åŸºçº¿ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å®ƒä»¬ä»¥ç”¨äºVTGçš„æ–¹æ³•ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†TimeLensï¼Œå¯¹æ„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMè¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œä¸»è¦å…³æ³¨æ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œæ­ç¤ºäº†ç°æœ‰VTGåŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†TimeLens-Benchï¼Œå®ƒåŒ…å«ç»è¿‡ä¸¥æ ¼è´¨é‡æ ‡å‡†é‡æ–°æ³¨é‡Šçš„ä¸‰ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ç‰ˆæœ¬ã€‚åˆ†æè¡¨æ˜ï¼Œä¸ä¼ ç»ŸåŸºå‡†ç›¸æ¯”ï¼Œæ¨¡å‹é‡æ–°æ’åºå‘ç”Ÿäº†å·¨å¤§å˜åŒ–ï¼Œè¯å®äº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚è¿˜é€šè¿‡è‡ªåŠ¨é‡æ–°æ³¨é‡Šæµç¨‹è§£å†³äº†å˜ˆæ‚çš„è®­ç»ƒæ•°æ®é—®é¢˜ï¼Œä»è€Œäº§ç”Ÿäº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†TimeLens-100Kã€‚åœ¨æ•°æ®åŸºç¡€ä¹‹ä¸Šï¼Œæ·±å…¥æ¢ç´¢äº†ç®—æ³•è®¾è®¡åŸåˆ™ï¼Œäº§ç”Ÿäº†ä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œæœ‰æ•ˆä½†é«˜æ•ˆçš„å®è·µã€‚è¿™äº›åŒ…æ‹¬ç”¨äºæ—¶é—´è¡¨ç¤ºçš„äº¤é”™æ–‡æœ¬ç¼–ç ï¼Œä¸€ç§æ— éœ€æ€è€ƒçš„å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ–¹æ³•ä½œä¸ºè®­ç»ƒèŒƒä¾‹ï¼Œä»¥åŠä¸ºRLVRè®­ç»ƒç²¾å¿ƒè®¾è®¡çš„æ–¹æ¡ˆã€‚è¿™äº›åŠªåŠ›æœ€ç»ˆä¿ƒæˆäº†TimeLensæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—MLLMï¼Œåœ¨å¼€æºæ¨¡å‹ä¸­å…·æœ‰æœ€å…ˆè¿›çš„VTGæ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡å‹ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹éƒ½å°†å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ—¨åœ¨ä»è§†é¢‘ä¸­å®šä½ä¸ç»™å®šæ–‡æœ¬æŸ¥è¯¢ç›¸å…³çš„ç‰¹å®šæ—¶é—´ç‰‡æ®µã€‚ç°æœ‰VTGæ–¹æ³•å—é™äºä½è´¨é‡çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®ï¼Œä¸”éš¾ä»¥å…¬å¹³æ¯”è¾ƒä¸åŒæ–¹æ³•çš„ä¼˜åŠ£ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹é’ˆå¯¹MLLMåœ¨VTGä»»åŠ¡ä¸Šçš„ä¼˜åŒ–ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTimeLensçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é«˜è´¨é‡çš„æ•°æ®å’Œç®—æ³•è®¾è®¡ï¼Œå……åˆ†åˆ©ç”¨å¤šæ¨¡æ€LLMçš„æ½œåŠ›ï¼Œæå‡VTGçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡é‡æ–°æ ‡æ³¨ç°æœ‰æ•°æ®é›†ï¼Œæ„å»ºé«˜è´¨é‡çš„è®­ç»ƒå’Œè¯„ä¼°åŸºå‡†ï¼Œå¹¶æ¢ç´¢æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ¨¡å‹ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTimeLensåŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š
1. **æ•°æ®æ„å»º**ï¼šé‡æ–°æ ‡æ³¨ç°æœ‰VTGæ•°æ®é›†ï¼Œæ„å»ºé«˜è´¨é‡çš„TimeLens-Benchå’ŒTimeLens-100Kæ•°æ®é›†ã€‚
2. **æ¨¡å‹ç»“æ„**ï¼šé‡‡ç”¨å¤šæ¨¡æ€LLMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº¤é”™æ–‡æœ¬ç¼–ç ç”¨äºæ—¶é—´è¡¨ç¤ºã€‚
3. **è®­ç»ƒç­–ç•¥**ï¼šä½¿ç”¨æ— éœ€æ€è€ƒçš„å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä½œä¸ºè®­ç»ƒèŒƒä¾‹ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„è®­ç»ƒæ–¹æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šTimeLensçš„å…³é”®åˆ›æ–°åœ¨äºï¼š
1. **é«˜è´¨é‡æ•°æ®**ï¼šé€šè¿‡ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶å’Œé‡æ–°æ ‡æ³¨ï¼Œæ„å»ºäº†é«˜è´¨é‡çš„VTGæ•°æ®é›†ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†çš„è´¨é‡é—®é¢˜ã€‚
2. **RLVRè®­ç»ƒ**ï¼šé‡‡ç”¨æ— éœ€æ€è€ƒçš„å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä½œä¸ºè®­ç»ƒèŒƒä¾‹ï¼Œé¿å…äº†å¤æ‚çš„å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼š
1. **äº¤é”™æ–‡æœ¬ç¼–ç **ï¼šå°†æ—¶é—´ä¿¡æ¯ä¸æ–‡æœ¬æŸ¥è¯¢äº¤é”™ç¼–ç ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ—¶é—´ä¸Šä¸‹æ–‡ã€‚
2. **RLVRå¥–åŠ±å‡½æ•°**ï¼šè®¾è®¡äº†åŸºäºIoUï¼ˆIntersection over Unionï¼‰çš„å¯éªŒè¯å¥–åŠ±å‡½æ•°ï¼Œç”¨äºæŒ‡å¯¼å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚
3. **è®­ç»ƒæ–¹æ¡ˆ**ï¼šç²¾å¿ƒè®¾è®¡äº†RLVRè®­ç»ƒçš„è¶…å‚æ•°å’Œè®­ç»ƒæµç¨‹ï¼Œä»¥ä¿è¯æ¨¡å‹çš„æ”¶æ•›æ€§å’Œæ€§èƒ½ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14698/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14698/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14698/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

TimeLensæ¨¡å‹åœ¨TimeLens-Benchä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡äº†GPT-5å’ŒGemini-2.5-Flashç­‰é—­æºæ¨¡å‹ã€‚é€šè¿‡é«˜è´¨é‡æ•°æ®å’Œæœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ŒTimeLensè¯æ˜äº†å¤šæ¨¡æ€LLMåœ¨VTGä»»åŠ¡ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TimeLensçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè§†é¢‘å†…å®¹ç†è§£ã€æ™ºèƒ½è§†é¢‘æœç´¢ã€è§†é¢‘ç¼–è¾‘å’Œæ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸã€‚é«˜è´¨é‡çš„VTGèƒ½åŠ›å¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´å‡†ç¡®åœ°å®šä½è§†é¢‘ä¸­çš„å…³é”®æ—¶åˆ»ï¼Œæé«˜ä¿¡æ¯æ£€ç´¢æ•ˆç‡ï¼Œå¹¶ä¸ºè§†é¢‘å†…å®¹åˆ†ææä¾›æ›´ç²¾ç¡®çš„åŸºç¡€ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

