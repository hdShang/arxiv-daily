---
layout: default
title: Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in
---

# Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in

**arXiv**: [2512.14273v1](https://arxiv.org/abs/2512.14273) | [PDF](https://arxiv.org/pdf/2512.14273.pdf)

**ä½œè€…**: Xiaoqian Shen, Min-Hung Chen, Yu-Chiang Frank Wang, Mohamed Elhoseiny, Ryo Hachiuma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project page: https://xiaoqian-shen.github.io/Zoom-Zero/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZoom-Zeroæ¡†æž¶ï¼Œé€šè¿‡ç²—åˆ°ç»†çš„æ—¶åºæ”¾å¤§æœºåˆ¶è§£å†³è§†é¢‘é—®ç­”ä¸­çš„æ—¶åºå®šä½ä¸å‡†ç¡®é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è§†é¢‘é—®ç­”` `æ—¶åºå®šä½` `ç²—åˆ°ç»†æ¡†æž¶` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€èžåˆ` `é•¿è§†é¢‘ç†è§£` `è§†è§‰éªŒè¯` `ä¿¡ç”¨åˆ†é…`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¤§åž‹è§†é¢‘è¯­è¨€æ¨¡åž‹åœ¨æ—¶åºæ„ŸçŸ¥æ–¹é¢æœ‰é™ï¼ŒåŸºäºŽGRPOçš„æ–¹æ³•ä»å­˜åœ¨æ—¶åºé”™ä½å’Œå¹»è§‰é—®é¢˜ï¼Œéš¾ä»¥å¿ å®žä¾æ®è§†é¢‘è¯æ®è¿›è¡Œå›žç­”ã€‚
2. æå‡ºZoom-Zeroæ¡†æž¶ï¼Œé‡‡ç”¨ç²—åˆ°ç»†ç­–ç•¥ï¼šå…ˆç²—ç²’åº¦å®šä½ç›¸å…³ç‰‡æ®µï¼Œå†æ—¶åºæ”¾å¤§åˆ°å…³é”®å¸§è¿›è¡Œç»†ç²’åº¦è§†è§‰éªŒè¯ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚
3. åœ¨NExT-GQAå’ŒReXTimeæ•°æ®é›†ä¸Šï¼Œæ—¶åºå®šä½ç²¾åº¦åˆ†åˆ«æå‡5.2%å’Œ4.6%ï¼Œç­”æ¡ˆå‡†ç¡®çŽ‡æå‡2.4%ï¼Œé•¿è§†é¢‘ç†è§£å¹³å‡æå‡6.4%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽè§†é¢‘çš„é—®ç­”ä»»åŠ¡æ—¨åœ¨å®šä½è§†é¢‘ä¸­çš„ç›¸å…³æ—¶åºç‰‡æ®µå¹¶ç”Ÿæˆå‡†ç¡®ç­”æ¡ˆï¼Œä½†çŽ°æœ‰å¤§åž‹è§†é¢‘è¯­è¨€æ¨¡åž‹åœ¨æ—¶åºæ„ŸçŸ¥æ–¹é¢å­˜åœ¨å±€é™ã€‚è™½ç„¶åŸºäºŽç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„æ–¹æ³•è¯•å›¾æ”¹è¿›æ—¶åºå®šä½ï¼Œä½†ä»éš¾ä»¥å¿ å®žä¾æ®è§†é¢‘è¯æ®è¿›è¡Œå›žç­”ï¼Œå¯¼è‡´æ—¶åºé”™ä½å’Œå¹»è§‰ã€‚æœ¬æ–‡æå‡ºZoom-Zeroï¼Œä¸€ç§ç²—åˆ°ç»†çš„æ¡†æž¶ï¼Œé¦–å…ˆå®šä½æŸ¥è¯¢ç›¸å…³ç‰‡æ®µï¼Œç„¶åŽæ—¶åºæ”¾å¤§åˆ°æœ€æ˜¾è‘—å¸§è¿›è¡Œç»†ç²’åº¦è§†è§‰éªŒè¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°è§£å†³GVQAä»»åŠ¡ä¸­GRPOçš„å±€é™ï¼š(i) æ”¾å¤§ç²¾åº¦å¥–åŠ±ï¼ŒéªŒè¯æ—¶åºå®šä½é¢„æµ‹çš„ä¿çœŸåº¦å¹¶ä¿ƒè¿›å¯¹å®šä½å¸§çš„ç»†ç²’åº¦è§†è§‰éªŒè¯ï¼›(ii) ä»¤ç‰Œé€‰æ‹©æ€§ä¿¡ç”¨åˆ†é…ï¼Œå°†å¥–åŠ±å½’å› äºŽè´Ÿè´£æ—¶åºå®šä½æˆ–ç­”æ¡ˆç”Ÿæˆçš„ä»¤ç‰Œï¼Œç¼“è§£GRPOå¤„ç†å¤šæ–¹é¢å¥–åŠ±ä¿¡å·çš„é—®é¢˜ã€‚æ‰€ææ–¹æ³•åœ¨NExT-GQAå’ŒReXTimeæ•°æ®é›†ä¸Šåˆ†åˆ«å°†æ—¶åºå®šä½ç²¾åº¦æå‡5.2%å’Œ4.6%ï¼Œå¹³å‡ç­”æ¡ˆå‡†ç¡®çŽ‡æå‡2.4%ã€‚æ­¤å¤–ï¼ŒæŽ¨ç†è¿‡ç¨‹ä¸­çš„ç²—åˆ°ç»†æ”¾å¤§é€šè¿‡ä¿ç•™å…³é”®è§†è§‰ç»†èŠ‚è€Œä¸æŸå®³å…¨å±€ä¸Šä¸‹æ–‡ï¼Œè¿›ä¸€æ­¥æœ‰ç›ŠäºŽé•¿è§†é¢‘ç†è§£ï¼Œåœ¨é•¿è§†é¢‘åŸºå‡†ä¸Šå¹³å‡æå‡6.4%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è§£å†³åŸºäºŽè§†é¢‘çš„é—®ç­”ä»»åŠ¡ä¸­çš„æ—¶åºå®šä½ä¸å‡†ç¡®é—®é¢˜ã€‚çŽ°æœ‰å¤§åž‹è§†é¢‘è¯­è¨€æ¨¡åž‹åœ¨æ—¶åºæ„ŸçŸ¥æ–¹é¢è¡¨çŽ°æœ‰é™ï¼ŒåŸºäºŽGRPOçš„æ–¹æ³•è™½ç„¶å°è¯•æ”¹è¿›ï¼Œä½†ä»éš¾ä»¥å¿ å®žä¾æ®è§†é¢‘è¯æ®è¿›è¡Œå›žç­”ï¼Œå¯¼è‡´æ—¶åºé”™ä½å’Œå¹»è§‰ï¼Œå½±å“ç­”æ¡ˆçš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºç²—åˆ°ç»†çš„æ—¶åºæ”¾å¤§æ¡†æž¶ï¼Œé€šè¿‡å…ˆç²—ç²’åº¦å®šä½æŸ¥è¯¢ç›¸å…³ç‰‡æ®µï¼Œå†æ—¶åºæ”¾å¤§åˆ°æœ€æ˜¾è‘—å¸§è¿›è¡Œç»†ç²’åº¦è§†è§‰éªŒè¯ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ—¶åºå®šä½å’Œç­”æ¡ˆç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥æé«˜æ¨¡åž‹çš„æ—¶åºæ„ŸçŸ¥èƒ½åŠ›å’Œç­”æ¡ˆä¿çœŸåº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºç²—ç²’åº¦å®šä½ï¼Œä½¿ç”¨æ¨¡åž‹åˆæ­¥è¯†åˆ«è§†é¢‘ä¸­ä¸ŽæŸ¥è¯¢ç›¸å…³çš„æ—¶åºç‰‡æ®µï¼›ç¬¬äºŒé˜¶æ®µä¸ºç»†ç²’åº¦æ”¾å¤§ï¼Œå¯¹å®šä½ç‰‡æ®µè¿›è¡Œæ—¶åºæ”¾å¤§ï¼Œæå–å…³é”®å¸§è¿›è¡Œæ›´ç²¾ç»†çš„è§†è§‰éªŒè¯ã€‚æ¡†æž¶é›†æˆå¼ºåŒ–å­¦ä¹ ç»„ä»¶ï¼Œé€šè¿‡å¥–åŠ±æœºåˆ¶ä¼˜åŒ–å®šä½å’Œç”Ÿæˆè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼š(i) æ”¾å¤§ç²¾åº¦å¥–åŠ±ï¼Œè®¾è®¡å¥–åŠ±å‡½æ•°éªŒè¯æ—¶åºå®šä½é¢„æµ‹çš„ä¿çœŸåº¦ï¼Œå¹¶ä¿ƒè¿›å¯¹å®šä½å¸§çš„ç»†ç²’åº¦è§†è§‰éªŒè¯ï¼›(ii) ä»¤ç‰Œé€‰æ‹©æ€§ä¿¡ç”¨åˆ†é…ï¼Œå°†å¥–åŠ±å½’å› äºŽè´Ÿè´£æ—¶åºå®šä½æˆ–ç­”æ¡ˆç”Ÿæˆçš„ç‰¹å®šä»¤ç‰Œï¼Œè§£å†³GRPOåœ¨å¤„ç†å¤šæ–¹é¢å¥–åŠ±ä¿¡å·æ—¶çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œæå‡è®­ç»ƒæ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šæŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–ï¼Œå¥–åŠ±å‡½æ•°ç»“åˆæ”¾å¤§ç²¾åº¦å¥–åŠ±å’Œç­”æ¡ˆå‡†ç¡®æ€§ï¼›ç½‘ç»œç»“æž„å¯èƒ½åŒ…å«è§†é¢‘ç¼–ç å™¨ã€è¯­è¨€æ¨¡åž‹å’Œæ—¶åºå®šä½æ¨¡å—ï¼›å‚æ•°è®¾ç½®æ¶‰åŠå­¦ä¹ çŽ‡ã€å¥–åŠ±æƒé‡ç­‰ï¼Œå…·ä½“å€¼æœªçŸ¥ï¼›æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬ç­–ç•¥æ¢¯åº¦æŸå¤±å’Œè¾…åŠ©æŸå¤±ï¼Œä»¥å¹³è¡¡æ—¶åºå®šä½å’Œç­”æ¡ˆç”Ÿæˆç›®æ ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœåŒ…æ‹¬ï¼šåœ¨NExT-GQAæ•°æ®é›†ä¸Šï¼Œæ—¶åºå®šä½ç²¾åº¦æå‡5.2%ï¼›åœ¨ReXTimeæ•°æ®é›†ä¸Šï¼Œæ—¶åºå®šä½ç²¾åº¦æå‡4.6%ï¼›å¹³å‡ç­”æ¡ˆå‡†ç¡®çŽ‡æå‡2.4%ï¼›åœ¨é•¿è§†é¢‘åŸºå‡†ä¸Šï¼Œé€šè¿‡ç²—åˆ°ç»†æ”¾å¤§æœºåˆ¶ï¼Œå¹³å‡æ€§èƒ½æå‡6.4%ã€‚è¿™äº›ç»“æžœå¯¹æ¯”åŸºçº¿æ–¹æ³•ï¼ˆå¦‚åŸºäºŽGRPOçš„æ–¹æ³•ï¼‰æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼ŒéªŒè¯äº†Zoom-Zeroæ¡†æž¶çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è§†é¢‘ç†è§£é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¦‚æ™ºèƒ½è§†é¢‘ç›‘æŽ§ã€æ•™è‚²è§†é¢‘åˆ†æžã€åŒ»ç–—è§†é¢‘è¯Šæ–­å’Œè‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£ã€‚é€šè¿‡æé«˜æ—¶åºå®šä½ç²¾åº¦å’Œç­”æ¡ˆå¯é æ€§ï¼Œå¯å¢žå¼ºå¤šæ¨¡æ€AIç³»ç»Ÿåœ¨é•¿è§†é¢‘å¤„ç†ä¸­çš„èƒ½åŠ›ï¼ŒæŽ¨åŠ¨äººæœºäº¤äº’å’Œè‡ªåŠ¨åŒ–è§†é¢‘åˆ†æžæŠ€æœ¯çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½æ‰©å±•åˆ°æ›´å¤æ‚çš„è§†é¢‘æŽ¨ç†ä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.

