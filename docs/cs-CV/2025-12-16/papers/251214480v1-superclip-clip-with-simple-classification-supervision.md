---
layout: default
title: SuperCLIP: CLIP with Simple Classification Supervision
---

# SuperCLIP: CLIP with Simple Classification Supervision

**arXiv**: [2512.14480v1](https://arxiv.org/abs/2512.14480) | [PDF](https://arxiv.org/pdf/2512.14480.pdf)

**ä½œè€…**: Weiheng Zhao, Zilong Huang, Jiashi Feng, Xinggang Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSuperCLIPæ¡†æž¶ï¼Œé€šè¿‡åˆ†ç±»ç›‘ç£å¢žå¼ºå¯¹æ¯”å­¦ä¹ ï¼Œè§£å†³CLIPæ¨¡åž‹ç»†ç²’åº¦è¯­ä¹‰åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `è§†è§‰-è¯­è¨€å¯¹é½` `ç»†ç²’åº¦è¯­ä¹‰` `é›¶æ ·æœ¬åˆ†ç±»` `å›¾åƒ-æ–‡æœ¬æ£€ç´¢` `å¤šæ¨¡æ€é¢„è®­ç»ƒ` `åˆ†ç±»ç›‘ç£` `è½»é‡çº§å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CLIPæ¨¡åž‹ä»…ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥è¯å…ƒçº§ç›‘ç£ï¼Œå¯¼è‡´ç»†ç²’åº¦è¯­ä¹‰åˆ©ç”¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤„ç†é•¿æè¿°æ—¶è¡¨çŽ°æ›´å·®ã€‚
2. SuperCLIPé€šè¿‡æ·»åŠ è½»é‡çº§çº¿æ€§å±‚ï¼Œå¼•å…¥åŸºäºŽåˆ†ç±»çš„ç›‘ç£ï¼Œå¢žå¼ºè§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œæ— éœ€é¢å¤–æ•°æ®ï¼Œè®¡ç®—å¼€é”€æžå°ã€‚
3. å®žéªŒæ˜¾ç¤ºSuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€æ£€ç´¢ç­‰ä»»åŠ¡ä¸ŠæŒç»­æå‡ï¼Œå¹¶ç¼“è§£å°æ‰¹é‡æ€§èƒ½ä¸‹é™ï¼Œé€‚ç”¨äºŽå¤šç§è®­ç»ƒæ•°æ®åœºæ™¯ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å¯¹é½å›¾åƒå’Œæ–‡æœ¬æ¥å®žçŽ°è§†è§‰-è¯­è¨€ä»»åŠ¡çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å‘çŽ°ï¼ŒCLIPç±»æ¨¡åž‹ä»ç„¶æœªèƒ½å……åˆ†åˆ©ç”¨æ–‡æœ¬ä¸­çš„ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·ï¼Œè¿™ä¸€é—®é¢˜åœ¨å¤„ç†é•¿è€Œè¯¦ç»†çš„æè¿°æ—¶å°¤ä¸ºæ˜Žæ˜¾ã€‚è¿™æºäºŽCLIPçš„è®­ç»ƒç›®æ ‡ä»…ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œè€Œå¿½ç•¥äº†è¯å…ƒçº§ç›‘ç£ï¼Œé™åˆ¶äº†å…¶å®žçŽ°ç»†ç²’åº¦è§†è§‰-æ–‡æœ¬å¯¹é½çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SuperCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æž¶ï¼Œé€šè¿‡åŸºäºŽåˆ†ç±»çš„ç›‘ç£æ¥å¢žå¼ºå¯¹æ¯”å­¦ä¹ ã€‚ä»…é€šè¿‡åœ¨è§†è§‰ç¼–ç å™¨ä¸Šæ·»åŠ ä¸€ä¸ªè½»é‡çº§çº¿æ€§å±‚ï¼ŒSuperCLIPåˆ©ç”¨è¯å…ƒçº§çº¿ç´¢æ¥å¢žå¼ºè§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œæ€»FLOPsä»…å¢žåŠ 0.077%ï¼Œä¸”æ— éœ€é¢å¤–çš„æ ‡æ³¨æ•°æ®ã€‚å®žéªŒè¡¨æ˜Žï¼ŒSuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œçº¯è§†è§‰ä»»åŠ¡ä¸Šå‡èƒ½æŒç»­æå‡æ€§èƒ½ã€‚è¿™äº›æå‡æ— è®ºæ¨¡åž‹æ˜¯åœ¨åŽŸå§‹ç½‘ç»œæ•°æ®è¿˜æ˜¯ä¸°å¯Œçš„é‡æ–°æ ‡æ³¨æ•°æ®ä¸Šè®­ç»ƒéƒ½æˆç«‹ï¼Œè¯æ˜Žäº†SuperCLIPåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹æ¢å¤æ–‡æœ¬ç›‘ç£çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒSuperCLIPé€šè¿‡åŸºäºŽåˆ†ç±»çš„ç›‘ç£å‡è½»äº†CLIPåœ¨å°æ‰¹é‡æƒ…å†µä¸‹çš„æ€§èƒ½ä¸‹é™ï¼Œé¿å…äº†ä¾èµ–å¤§æ‰¹é‡å¤§å°ã€‚ä»£ç å’Œæ¨¡åž‹å°†å¼€æºã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³CLIPæ¨¡åž‹åœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­ç»†ç²’åº¦è¯­ä¹‰åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚çŽ°æœ‰CLIPæ¨¡åž‹ä»…é€šè¿‡å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†æ–‡æœ¬ä¸­çš„è¯å…ƒçº§ç›‘ç£ï¼Œå¯¼è‡´åœ¨å¤„ç†å¤æ‚æˆ–é•¿æè¿°æ—¶éš¾ä»¥å®žçŽ°ç²¾ç»†çš„è§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œé™åˆ¶äº†æ¨¡åž‹åœ¨é›¶æ ·æœ¬åˆ†ç±»ã€æ£€ç´¢ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥åŸºäºŽåˆ†ç±»çš„ç›‘ç£æ¥å¢žå¼ºå¯¹æ¯”å­¦ä¹ ï¼Œä»Žè€Œå¼¥è¡¥CLIPåœ¨è¯å…ƒçº§è¯­ä¹‰æ•æ‰ä¸Šçš„ä¸è¶³ã€‚è®¾è®¡ä¸Šï¼ŒSuperCLIPåœ¨è§†è§‰ç¼–ç å™¨åŽæ·»åŠ ä¸€ä¸ªè½»é‡çº§çº¿æ€§å±‚ï¼Œå°†å›¾åƒç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬è¯å…ƒç©ºé—´ï¼Œåˆ©ç”¨åˆ†ç±»æŸå¤±å‡½æ•°å¼ºåˆ¶æ¨¡åž‹å­¦ä¹ æ›´ç»†ç²’åº¦çš„è§†è§‰-æ–‡æœ¬å¯¹åº”å…³ç³»ï¼Œè€Œæ— éœ€æ”¹å˜åŽŸå§‹CLIPçš„å¯¹æ¯”å­¦ä¹ æ¡†æž¶ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŸºäºŽCLIPï¼ŒåŒ…å«è§†è§‰ç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ï¼š1ï¼‰åŽŸå§‹CLIPçš„å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼Œç”¨äºŽå…¨å±€å¯¹é½ï¼›2ï¼‰æ–°å¢žçš„åˆ†ç±»ç›‘ç£æ¨¡å—ï¼Œç”±ä¸€ä¸ªçº¿æ€§å±‚ç»„æˆï¼Œå°†è§†è§‰ç‰¹å¾è½¬æ¢ä¸ºæ–‡æœ¬è¯å…ƒé¢„æµ‹ï¼›3ï¼‰è®­ç»ƒé˜¶æ®µï¼ŒåŒæ—¶ä¼˜åŒ–å¯¹æ¯”æŸå¤±å’Œåˆ†ç±»æŸå¤±ï¼Œä»¥èžåˆå…¨å±€å’Œå±€éƒ¨ç›‘ç£ã€‚æµç¨‹ä¸Šï¼Œè¾“å…¥å›¾åƒå’Œæ–‡æœ¬ï¼Œè§†è§‰ç¼–ç å™¨æå–ç‰¹å¾ï¼Œæ–‡æœ¬ç¼–ç å™¨æå–è¯å…ƒç‰¹å¾ï¼Œé€šè¿‡è”åˆæŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯å°†åˆ†ç±»ç›‘ç£æ— ç¼é›†æˆåˆ°CLIPæ¡†æž¶ä¸­ï¼Œé€šè¿‡è¯å…ƒçº§é¢„æµ‹å¢žå¼ºç»†ç²’åº¦å¯¹é½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ï¼ˆå¦‚ä»…ä¾èµ–å¯¹æ¯”å­¦ä¹ æˆ–å¤æ‚å¤šä»»åŠ¡å­¦ä¹ ï¼‰çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼ŒSuperCLIPä»…æ·»åŠ æžå°‘é‡å‚æ•°ï¼ˆçº¿æ€§å±‚ï¼‰ï¼Œè®¡ç®—å¼€é”€å‡ ä¹Žå¯å¿½ç•¥ï¼ˆFLOPså¢žåŠ 0.077%ï¼‰ï¼Œä¸”æ— éœ€é¢å¤–æ ‡æ³¨æ•°æ®ï¼Œå®žçŽ°äº†é«˜æ•ˆä¸”é€šç”¨çš„æ”¹è¿›ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰çº¿æ€§å±‚å‚æ•°è®¾ç½®ï¼šè½»é‡çº§ï¼Œä»…å¢žåŠ å°‘é‡è®¡ç®—ï¼›2ï¼‰æŸå¤±å‡½æ•°ï¼šç»“åˆå¯¹æ¯”æŸå¤±ï¼ˆå¦‚InfoNCEï¼‰å’Œåˆ†ç±»æŸå¤±ï¼ˆå¦‚äº¤å‰ç†µï¼‰ï¼Œåˆ†ç±»æŸå¤±é’ˆå¯¹æ–‡æœ¬è¯å…ƒè¿›è¡Œç›‘ç£ï¼›3ï¼‰ç½‘ç»œç»“æž„ï¼šä¿æŒCLIPç¼–ç å™¨ä¸å˜ï¼Œä»…åœ¨è§†è§‰ç¼–ç å™¨è¾“å‡ºåŽæ·»åŠ çº¿æ€§å±‚ï¼›4ï¼‰è®­ç»ƒç­–ç•¥ï¼šä½¿ç”¨åŽŸå§‹æˆ–é‡æ–°æ ‡æ³¨çš„ç½‘ç»œæ•°æ®ï¼Œæ— éœ€æ•°æ®å¢žå¼ºæˆ–å¤æ‚é¢„å¤„ç†ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒSuperCLIPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æå‡æ€§èƒ½ï¼šåœ¨é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œç›¸æ¯”åŸºçº¿CLIPæ¨¡åž‹ï¼Œå‡†ç¡®çŽ‡æœ‰æ˜¾è‘—æå‡ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡æåˆ°â€œä¸€è‡´æ”¹è¿›â€ï¼‰ï¼›åœ¨å›¾åƒ-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œå¬å›žçŽ‡æŒ‡æ ‡å¾—åˆ°å¢žå¼ºï¼›åœ¨çº¯è§†è§‰ä»»åŠ¡ä¸Šï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ï¼‰ï¼Œä¹Ÿè§‚å¯Ÿåˆ°æ€§èƒ½å¢žç›Šã€‚æ­¤å¤–ï¼ŒSuperCLIPæœ‰æ•ˆç¼“è§£äº†CLIPåœ¨å°æ‰¹é‡è®­ç»ƒæ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä¸”è¿™äº›æå‡åœ¨åŽŸå§‹ç½‘ç»œæ•°æ®å’Œé‡æ–°æ ‡æ³¨æ•°æ®ä¸Šå‡æˆç«‹ï¼Œè¯æ˜Žäº†å…¶é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SuperCLIPçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„äº¤å‰ä»»åŠ¡ï¼Œå¦‚é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå†…å®¹ç”Ÿæˆã€‚å…¶å®žé™…ä»·å€¼åœ¨äºŽæå‡å¤šæ¨¡æ€æ¨¡åž‹çš„ç»†ç²’åº¦ç†è§£èƒ½åŠ›ï¼Œé€‚ç”¨äºŽéœ€è¦å¤„ç†å¤æ‚è¯­ä¹‰çš„åœºæ™¯ï¼ˆå¦‚åŒ»ç–—å›¾åƒåˆ†æžã€è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ï¼‰ã€‚æœªæ¥å½±å“å¯èƒ½æŽ¨åŠ¨æ›´é«˜æ•ˆçš„è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•ï¼Œä¿ƒè¿›AIåœ¨å¼€æ”¾åŸŸä»»åŠ¡ä¸­çš„æ³›åŒ–æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.

