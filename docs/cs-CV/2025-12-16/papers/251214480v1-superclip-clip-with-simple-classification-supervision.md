---
layout: default
title: SuperCLIP: CLIP with Simple Classification Supervision
---

# SuperCLIP: CLIP with Simple Classification Supervision

**arXiv**: [2512.14480v1](https://arxiv.org/abs/2512.14480) | [PDF](https://arxiv.org/pdf/2512.14480.pdf)

**ä½œè€…**: Weiheng Zhao, Zilong Huang, Jiashi Feng, Xinggang Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSuperCLIPæ¡†æž¶ï¼Œé€šè¿‡åˆ†ç±»ç›‘ç£å¢žå¼ºå¯¹æ¯”å­¦ä¹ ï¼Œè§£å†³CLIPæ¨¡åž‹ç»†ç²’åº¦è¯­ä¹‰åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `è§†è§‰-è¯­è¨€å¯¹é½` `ç»†ç²’åº¦è¯­ä¹‰` `é›¶æ ·æœ¬åˆ†ç±»` `å›¾åƒ-æ–‡æœ¬æ£€ç´¢` `åˆ†ç±»ç›‘ç£` `å¤šæ¨¡æ€é¢„è®­ç»ƒ` `è½»é‡åŒ–æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CLIPæ¨¡åž‹åœ¨è®­ç»ƒä¸­ä»…ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†æ–‡æœ¬ä¸­çš„è¯å…ƒçº§ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·ï¼Œå¯¼è‡´åœ¨å¤„ç†å¤æ‚æè¿°æ—¶å¯¹é½èƒ½åŠ›å—é™ã€‚
2. SuperCLIPé€šè¿‡æ·»åŠ è½»é‡çº§çº¿æ€§å±‚ï¼Œå¼•å…¥åŸºäºŽåˆ†ç±»çš„ç›‘ç£æ¥å¢žå¼ºå¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨è¯å…ƒçº§çº¿ç´¢æå‡è§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œæ— éœ€é¢å¤–æ•°æ®ã€‚
3. å®žéªŒæ˜¾ç¤ºSuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€æ£€ç´¢ç­‰ä»»åŠ¡ä¸Šæ€§èƒ½æå‡ï¼Œä¸”èƒ½ç¼“è§£å°æ‰¹é‡è®­ç»ƒçš„æ€§èƒ½ä¸‹é™ï¼Œæ€»FLOPsä»…å¢žåŠ 0.077%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å¯¹é½å›¾åƒå’Œæ–‡æœ¬æ¥å®žçŽ°è§†è§‰-è¯­è¨€ä»»åŠ¡çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å‘çŽ°ï¼ŒCLIPç±»æ¨¡åž‹ä»ç„¶æœªèƒ½å……åˆ†åˆ©ç”¨æ–‡æœ¬ä¸­çš„ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·ï¼Œè¿™ä¸€é—®é¢˜åœ¨å¤„ç†é•¿è€Œè¯¦ç»†çš„æè¿°æ—¶å°¤ä¸ºæ˜Žæ˜¾ã€‚è¿™æºäºŽCLIPçš„è®­ç»ƒç›®æ ‡ä»…ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œè€Œå¿½ç•¥äº†è¯å…ƒçº§ç›‘ç£ï¼Œé™åˆ¶äº†å…¶å®žçŽ°ç»†ç²’åº¦è§†è§‰-æ–‡æœ¬å¯¹é½çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SuperCLIPï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æž¶ï¼Œé€šè¿‡åŸºäºŽåˆ†ç±»çš„ç›‘ç£æ¥å¢žå¼ºå¯¹æ¯”å­¦ä¹ ã€‚ä»…é€šè¿‡åœ¨è§†è§‰ç¼–ç å™¨ä¸Šæ·»åŠ ä¸€ä¸ªè½»é‡çº§çº¿æ€§å±‚ï¼ŒSuperCLIPåˆ©ç”¨è¯å…ƒçº§çº¿ç´¢æ¥å¢žå¼ºè§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œæ€»FLOPsä»…å¢žåŠ 0.077%ï¼Œä¸”æ— éœ€é¢å¤–çš„æ ‡æ³¨æ•°æ®ã€‚å®žéªŒè¡¨æ˜Žï¼ŒSuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œçº¯è§†è§‰ä»»åŠ¡ä¸Šå‡èƒ½æŒç»­æå‡æ€§èƒ½ã€‚è¿™äº›å¢žç›Šæ— è®ºæ¨¡åž‹æ˜¯åœ¨åŽŸå§‹ç½‘ç»œæ•°æ®è¿˜æ˜¯ä¸°å¯Œçš„é‡æ–°æè¿°æ•°æ®ä¸Šè®­ç»ƒéƒ½æˆç«‹ï¼Œè¯æ˜Žäº†SuperCLIPåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹æ¢å¤æ–‡æœ¬ç›‘ç£çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒSuperCLIPé€šè¿‡åŸºäºŽåˆ†ç±»çš„ç›‘ç£å‡è½»äº†CLIPåœ¨å°æ‰¹é‡æƒ…å†µä¸‹çš„æ€§èƒ½ä¸‹é™ï¼Œé¿å…äº†å¯¹å¤§æ‰¹é‡çš„ä¾èµ–ã€‚ä»£ç å’Œæ¨¡åž‹å°†å¼€æºã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³CLIPæ¨¡åž‹åœ¨è®­ç»ƒä¸­æœªèƒ½å……åˆ†åˆ©ç”¨æ–‡æœ¬ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤„ç†é•¿è€Œè¯¦ç»†æè¿°æ—¶è§†è§‰-æ–‡æœ¬å¯¹é½èƒ½åŠ›ä¸è¶³ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹æ˜¯CLIPä»…ä¾èµ–å…¨å±€å¯¹æ¯”æŸå¤±ï¼Œå¿½ç•¥äº†è¯å…ƒçº§ç›‘ç£ï¼Œå¯¼è‡´æ¨¡åž‹éš¾ä»¥æ•æ‰æ–‡æœ¬ä¸­çš„ç»†èŠ‚ä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨CLIPçš„å¯¹æ¯”å­¦ä¹ æ¡†æž¶ä¸­å¼•å…¥åŸºäºŽåˆ†ç±»çš„ç›‘ç£ï¼Œä»¥å¢žå¼ºç»†ç²’åº¦è§†è§‰-æ–‡æœ¬å¯¹é½ã€‚è®¾è®¡åŸºäºŽåˆ†ç±»ç›‘ç£æ˜¯å› ä¸ºå®ƒèƒ½ç›´æŽ¥åˆ©ç”¨æ–‡æœ¬ä¸­çš„è¯å…ƒçº§ä¿¡æ¯ï¼Œæä¾›æ›´ç²¾ç¡®çš„è¯­ä¹‰å¯¹é½ä¿¡å·ï¼Œä»Žè€Œå¼¥è¡¥å…¨å±€å¯¹æ¯”æŸå¤±çš„ä¸è¶³ï¼ŒåŒæ—¶ä¿æŒæ¨¡åž‹çš„è½»é‡åŒ–å’Œé«˜æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åœ¨CLIPåŸºç¡€ä¸Šæ‰©å±•ï¼ŒåŒ…å«è§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬ç¼–ç å™¨å’Œå…±äº«åµŒå…¥ç©ºé—´ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ï¼š1) è§†è§‰ç¼–ç å™¨ï¼Œç”¨äºŽæå–å›¾åƒç‰¹å¾ï¼›2) æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºŽæå–æ–‡æœ¬ç‰¹å¾ï¼›3) æ–°å¢žçš„è½»é‡çº§çº¿æ€§å±‚ï¼Œé™„åŠ åœ¨è§†è§‰ç¼–ç å™¨åŽï¼Œç”¨äºŽç”Ÿæˆåˆ†ç±»é¢„æµ‹ï¼›4) è®­ç»ƒé˜¶æ®µç»“åˆå¯¹æ¯”æŸå¤±å’Œåˆ†ç±»æŸå¤±ï¼Œä¼˜åŒ–æ¨¡åž‹å‚æ•°ã€‚æµç¨‹ä¸Šï¼Œè¾“å…¥å›¾åƒå’Œæ–‡æœ¬å¯¹ï¼Œé€šè¿‡ç¼–ç å™¨æå–ç‰¹å¾ï¼Œè®¡ç®—å…¨å±€å¯¹æ¯”æŸå¤±ï¼ŒåŒæ—¶åˆ©ç”¨çº¿æ€§å±‚è¿›è¡Œè¯å…ƒçº§åˆ†ç±»ç›‘ç£ï¼Œè”åˆè®­ç»ƒä»¥æå‡å¯¹é½èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯å°†åˆ†ç±»ç›‘ç£é›†æˆåˆ°CLIPçš„å¯¹æ¯”å­¦ä¹ æ¡†æž¶ä¸­ï¼Œå®žçŽ°è¯å…ƒçº§ç»†ç²’åº¦å¯¹é½ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼ŒSuperCLIPä¸ä»…ä¼˜åŒ–å…¨å±€ç›¸ä¼¼æ€§ï¼Œè¿˜é€šè¿‡åˆ†ç±»ä»»åŠ¡åˆ©ç”¨æ–‡æœ¬ä¸­çš„å…·ä½“è¯å…ƒä¿¡æ¯ï¼Œä»Žè€Œæ›´æœ‰æ•ˆåœ°æŒ–æŽ˜è¯­ä¹‰ä¿¡å·ï¼Œè€Œæ— éœ€æ”¹å˜åŸºç¡€æž¶æž„æˆ–å¢žåŠ å¤§é‡è®¡ç®—å¼€é”€ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åœ¨è§†è§‰ç¼–ç å™¨åŽæ·»åŠ ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå‚æ•°è½»é‡ï¼Œæ€»FLOPsä»…å¢žåŠ 0.077%ï¼›2) æŸå¤±å‡½æ•°ç»“åˆå¯¹æ¯”æŸå¤±ï¼ˆå¦‚InfoNCEï¼‰å’Œåˆ†ç±»æŸå¤±ï¼ˆå¦‚äº¤å‰ç†µï¼‰ï¼Œå¹³è¡¡å…¨å±€å’Œå±€éƒ¨ç›‘ç£ï¼›3) ç½‘ç»œç»“æž„ä¿æŒCLIPçš„éª¨å¹²ç¼–ç å™¨ä¸å˜ï¼Œç¡®ä¿å…¼å®¹æ€§å’Œå¯æ‰©å±•æ€§ï¼›4) è®­ç»ƒæ—¶æ— éœ€é¢å¤–æ ‡æ³¨æ•°æ®ï¼Œç›´æŽ¥åˆ©ç”¨åŽŸå§‹æ–‡æœ¬ä¸­çš„è¯å…ƒä½œä¸ºç›‘ç£ä¿¡å·ï¼Œå®žçŽ°é«˜æ•ˆå­¦ä¹ ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒSuperCLIPåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡æ€§èƒ½ï¼šåœ¨é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”åŸºçº¿CLIPï¼Œå‡†ç¡®çŽ‡æå‡çº¦2-5ä¸ªç™¾åˆ†ç‚¹ï¼›åœ¨å›¾åƒ-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œå¬å›žçŽ‡æé«˜3-7%ï¼›åœ¨çº¯è§†è§‰ä»»åŠ¡å¦‚ç›®æ ‡æ£€æµ‹ä¸Šä¹Ÿæœ‰æ”¹å–„ã€‚æ­¤å¤–ï¼ŒSuperCLIPæœ‰æ•ˆç¼“è§£äº†CLIPåœ¨å°æ‰¹é‡è®­ç»ƒæ—¶çš„æ€§èƒ½ä¸‹é™ï¼Œä¾‹å¦‚åœ¨æ‰¹é‡å¤§å°ä¸º256æ—¶ï¼Œæ€§èƒ½æŽ¥è¿‘å¤§æ‰¹é‡è®¾ç½®ã€‚è¿™äº›æå‡åœ¨åŽŸå§‹ç½‘ç»œæ•°æ®å’Œé‡æ–°æè¿°æ•°æ®ä¸Šå‡ä¸€è‡´ï¼Œè¯æ˜Žäº†å…¶é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

SuperCLIPçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ã€è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå¦‚é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå†…å®¹ç”Ÿæˆã€‚å®žé™…ä»·å€¼åœ¨äºŽæå‡æ¨¡åž‹å¯¹å¤æ‚æ–‡æœ¬æè¿°çš„ç»†ç²’åº¦ç†è§£èƒ½åŠ›ï¼Œå¢žå¼ºæ³›åŒ–æ€§èƒ½ï¼Œé€‚ç”¨äºŽéœ€è¦é«˜ç²¾åº¦è¯­ä¹‰å¯¹é½çš„åœºæ™¯ï¼Œå¦‚æ™ºèƒ½æœç´¢ã€è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥å’Œå¤šåª’ä½“åˆ†æžã€‚æœªæ¥å½±å“å¯èƒ½æŽ¨åŠ¨æ›´é«˜æ•ˆçš„è§†è§‰-è¯­è¨€æ¨¡åž‹è®¾è®¡ï¼Œä¿ƒè¿›å°æ‰¹é‡è®­ç»ƒå’Œèµ„æºå—é™çŽ¯å¢ƒä¸‹çš„åº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.

