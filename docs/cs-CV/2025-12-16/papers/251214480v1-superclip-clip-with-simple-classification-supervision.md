---
layout: default
title: SuperCLIP: CLIP with Simple Classification Supervision
---

# SuperCLIP: CLIP with Simple Classification Supervision

**arXiv**: [2512.14480v1](https://arxiv.org/abs/2512.14480) | [PDF](https://arxiv.org/pdf/2512.14480.pdf)

**ä½œè€…**: Weiheng Zhao, Zilong Huang, Jiashi Feng, Xinggang Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSuperCLIPæ¡†æž¶ï¼Œé€šè¿‡åˆ†ç±»ç›‘ç£å¢žå¼ºå¯¹æ¯”å­¦ä¹ ï¼Œè§£å†³CLIPæ¨¡åž‹ç»†ç²’åº¦è¯­ä¹‰åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å¯¹æ¯”å­¦ä¹ ` `å¤šæ¨¡æ€å¯¹é½` `ç»†ç²’åº¦è¯­ä¹‰` `é›¶æ ·æœ¬åˆ†ç±»` `å›¾åƒ-æ–‡æœ¬æ£€ç´¢` `è½»é‡çº§ç›‘ç£` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `åˆ†ç±»ç›‘ç£`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. CLIPæ¨¡åž‹ä»…ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥è¯å…ƒçº§ç›‘ç£ï¼Œå¯¼è‡´ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·åˆ©ç”¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤„ç†é•¿æè¿°æ—¶è¡¨çŽ°æ›´å·®ã€‚
2. SuperCLIPé€šè¿‡æ·»åŠ è½»é‡çº§çº¿æ€§å±‚ï¼Œå¼•å…¥åŸºäºŽåˆ†ç±»çš„ç›‘ç£ï¼Œå¢žå¼ºå¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨è¯å…ƒçº§çº¿ç´¢æå‡è§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œæ— éœ€é¢å¤–æ•°æ®ã€‚
3. å®žéªŒæ˜¾ç¤ºSuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œè§†è§‰ä»»åŠ¡ä¸Šå‡æå‡æ€§èƒ½ï¼Œå¹¶ç¼“è§£å°æ‰¹é‡æ€§èƒ½ä¸‹é™ï¼Œé€‚ç”¨äºŽå¤šç§è®­ç»ƒæ•°æ®ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å¯¹é½å›¾åƒå’Œæ–‡æœ¬æ¥å®žçŽ°è§†è§‰-è¯­è¨€ä»»åŠ¡çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å‘çŽ°ï¼ŒCLIPç±»æ¨¡åž‹åœ¨å¤„ç†æ–‡æœ¬æ—¶ä»æœªèƒ½å……åˆ†åˆ©ç”¨ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·ï¼Œè¿™ä¸€é—®é¢˜åœ¨å¤„ç†é•¿è€Œè¯¦ç»†çš„æè¿°æ—¶å°¤ä¸ºçªå‡ºã€‚è¿™æºäºŽCLIPçš„è®­ç»ƒç›®æ ‡ä»…ä¼˜åŒ–å…¨å±€å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œè€Œå¿½ç•¥äº†è¯å…ƒçº§ç›‘ç£ï¼Œé™åˆ¶äº†å…¶å®žçŽ°ç»†ç²’åº¦è§†è§‰-æ–‡æœ¬å¯¹é½çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SuperCLIPï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æž¶ï¼Œé€šè¿‡åŸºäºŽåˆ†ç±»çš„ç›‘ç£æ¥å¢žå¼ºå¯¹æ¯”å­¦ä¹ ã€‚ä»…é€šè¿‡åœ¨è§†è§‰ç¼–ç å™¨ä¸Šæ·»åŠ ä¸€ä¸ªè½»é‡çº§çº¿æ€§å±‚ï¼ŒSuperCLIPåˆ©ç”¨è¯å…ƒçº§çº¿ç´¢æ¥å¢žå¼ºè§†è§‰-æ–‡æœ¬å¯¹é½ï¼Œæ€»FLOPsä»…å¢žåŠ 0.077%ï¼Œä¸”æ— éœ€é¢å¤–æ ‡æ³¨æ•°æ®ã€‚å®žéªŒè¡¨æ˜Žï¼ŒSuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œçº¯è§†è§‰ä»»åŠ¡ä¸Šå‡èƒ½æŒç»­æå‡æ€§èƒ½ã€‚è¿™äº›å¢žç›Šæ— è®ºæ¨¡åž‹æ˜¯åœ¨åŽŸå§‹ç½‘ç»œæ•°æ®è¿˜æ˜¯ä¸°å¯Œçš„é‡æ–°æè¿°æ•°æ®ä¸Šè®­ç»ƒéƒ½æˆç«‹ï¼Œè¯æ˜Žäº†SuperCLIPåœ¨ä¸¤ç§æƒ…å†µä¸‹æ¢å¤æ–‡æœ¬ç›‘ç£çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒSuperCLIPé€šè¿‡åŸºäºŽåˆ†ç±»çš„ç›‘ç£å‡è½»äº†CLIPåœ¨å°æ‰¹é‡æƒ…å†µä¸‹çš„æ€§èƒ½ä¸‹é™ï¼Œé¿å…äº†ä¾èµ–å¤§æ‰¹é‡å¤§å°ã€‚ä»£ç å’Œæ¨¡åž‹å°†å¼€æºã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

SuperCLIPæ˜¯ä¸€ä¸ªå¢žå¼ºCLIPçš„æ¡†æž¶ï¼Œæ•´ä½“åŸºäºŽå¯¹æ¯”å­¦ä¹ ï¼Œä½†å¼•å…¥åˆ†ç±»ç›‘ç£ã€‚å…³é”®åˆ›æ–°åœ¨äºŽåœ¨è§†è§‰ç¼–ç å™¨åŽæ·»åŠ ä¸€ä¸ªè½»é‡çº§çº¿æ€§å±‚ï¼Œç”¨äºŽç”Ÿæˆè¯å…ƒçº§åˆ†ç±»é¢„æµ‹ï¼Œä»Žè€Œåˆ©ç”¨æ–‡æœ¬ä¸­çš„ç»†ç²’åº¦è¯­ä¹‰ä¿¡å·ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œå®ƒä¸ä¾èµ–é¢å¤–æ ‡æ³¨æ•°æ®æˆ–å¤æ‚æž¶æž„ï¼Œä»…é€šè¿‡ç®€å•åˆ†ç±»ç›‘ç£å¼¥è¡¥CLIPçš„å…¨å±€å¯¹é½ä¸è¶³ï¼Œå®žçŽ°æ›´ç²¾ç»†çš„è§†è§‰-æ–‡æœ¬å¯¹é½ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆçŽ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

SuperCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œçº¯è§†è§‰ä»»åŠ¡ä¸Šå‡å®žçŽ°æ€§èƒ½æå‡ï¼Œæ€»FLOPsä»…å¢žåŠ 0.077%ï¼Œä¸”èƒ½ç¼“è§£CLIPçš„å°æ‰¹é‡æ€§èƒ½ä¸‹é™ï¼Œé€‚ç”¨äºŽåŽŸå§‹å’Œé‡æ–°æè¿°æ•°æ®ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽå¤šæ¨¡æ€äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¦‚é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œæœºå™¨äººè§†è§‰ç†è§£ï¼Œæå‡æ¨¡åž‹åœ¨ç»†ç²’åº¦è¯­ä¹‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå…·æœ‰å®žé™…ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.

