---
layout: default
title: OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving
---

# OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving

**arXiv**: [2512.14044v1](https://arxiv.org/abs/2512.14044) | [PDF](https://arxiv.org/pdf/2512.14044.pdf)

**ä½œè€…**: Zhenguo Zhang, Haohan Zhen, Yishen Wang, Le Xu, Tianchen Deng, Xuefeng Chen, Qu Chen, Bo Zhang, Wuxiong Huang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniDrive-R1ï¼šå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„äº¤é”™å¤šæ¨¡æ€CoTï¼Œæå‡è‡ªåŠ¨é©¾é©¶è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¯é æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)** **è‡ªåŠ¨é©¾é©¶ (Autonomous Driving)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€æ¨¡åž‹` `å¤šæ¨¡æ€æŽ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æ€ç»´é“¾` `è§†è§‰ grounding` `è·¨æ¨¡æ€ä¸€è‡´æ€§`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å­˜åœ¨å¯¹è±¡å¹»è§‰é—®é¢˜ï¼ŒæºäºŽå¯¹æ— æ ¹æ®æ–‡æœ¬CoTæŽ¨ç†çš„ä¾èµ–ï¼Œä¸”æ„ŸçŸ¥ä¸ŽæŽ¨ç†è§£è€¦ã€‚
2. OmniDrive-R1æå‡ºäº¤é”™å¤šæ¨¡æ€CoTæœºåˆ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨è§†è§‰ groundingï¼Œä½¿æ¨¡åž‹è‡ªä¸»å…³æ³¨å…³é”®åŒºåŸŸã€‚
3. åœ¨DriveLMM-o1æ•°æ®é›†ä¸Šï¼ŒOmniDrive-R1æ˜¾è‘—æå‡äº†æŽ¨ç†å¾—åˆ†å’Œç­”æ¡ˆå‡†ç¡®çŽ‡ï¼Œä¼˜äºŽåŸºçº¿æ¨¡åž‹Qwen2.5VL-7Bã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡åž‹(VLMs)åœ¨è‡ªåŠ¨é©¾é©¶(AD)ç­‰å®‰å…¨å…³é”®é¢†åŸŸçš„éƒ¨ç½²å—åˆ°å¯é æ€§é—®é¢˜çš„ä¸¥é‡é˜»ç¢ï¼Œå°¤å…¶æ˜¯å¯¹è±¡å¹»è§‰ã€‚è¿™ç§å¤±è´¥æºäºŽå®ƒä»¬å¯¹æ— æ ¹æ®çš„ã€åŸºäºŽæ–‡æœ¬çš„æ€ç»´é“¾(CoT)æŽ¨ç†çš„ä¾èµ–ã€‚çŽ°æœ‰çš„å¤šæ¨¡æ€CoTæ–¹æ³•è¯•å›¾ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªæ ¹æœ¬ç¼ºé™·ï¼š(1)è§£è€¦çš„æ„ŸçŸ¥å’ŒæŽ¨ç†é˜¶æ®µï¼Œé˜»æ­¢äº†ç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–ï¼›(2)ä¾èµ–äºŽæ˜‚è´µçš„ã€å¯†é›†çš„å®šä½æ ‡ç­¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniDrive-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„ç«¯åˆ°ç«¯VLMæ¡†æž¶ï¼Œå®ƒé€šè¿‡äº¤é”™å¤šæ¨¡æ€CoT(iMCoT)æœºåˆ¶ç»Ÿä¸€äº†æ„ŸçŸ¥å’ŒæŽ¨ç†ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°æ˜¯å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è§†è§‰ grounding èƒ½åŠ›ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿè‡ªä¸»åœ°å°†å…¶æ³¨æ„åŠ›å¯¼å‘å…³é”®åŒºåŸŸï¼Œä»¥è¿›è¡Œç»†ç²’åº¦åˆ†æžã€‚è¿™ç§èƒ½åŠ›ç”±æˆ‘ä»¬çš„çº¯ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹å’ŒClip-GRPOç®—æ³•å®žçŽ°ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼ŒClip-GRPOå¼•å…¥äº†ä¸€ç§æ— æ ‡æ³¨çš„ã€åŸºäºŽè¿‡ç¨‹çš„ grounding å¥–åŠ±ã€‚è¿™ç§å¥–åŠ±ä¸ä»…æ¶ˆé™¤äº†å¯¹å¯†é›†æ ‡ç­¾çš„éœ€æ±‚ï¼Œè€Œä¸”é€šè¿‡å¼ºåˆ¶è§†è§‰ç„¦ç‚¹å’Œæ–‡æœ¬æŽ¨ç†ä¹‹é—´çš„å®žæ—¶è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼Œè§„é¿äº†å¤–éƒ¨å·¥å…·è°ƒç”¨çš„ä¸ç¨³å®šæ€§ã€‚åœ¨DriveLMM-o1ä¸Šçš„å¤§é‡å®žéªŒè¯æ˜Žäº†æˆ‘ä»¬æ¨¡åž‹çš„æ˜¾è‘—æ”¹è¿›ã€‚ä¸ŽåŸºçº¿Qwen2.5VL-7Bç›¸æ¯”ï¼ŒOmniDrive-R1å°†æ•´ä½“æŽ¨ç†å¾—åˆ†ä»Ž51.77%æé«˜åˆ°80.35%ï¼Œæœ€ç»ˆç­”æ¡ˆå‡†ç¡®çŽ‡ä»Ž37.81%æé«˜åˆ°73.62%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰è¯­è¨€æ¨¡åž‹åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­å­˜åœ¨å¯¹è±¡å¹»è§‰é—®é¢˜ï¼Œå¯¼è‡´å†³ç­–é”™è¯¯ã€‚çŽ°æœ‰çš„å¤šæ¨¡æ€CoTæ–¹æ³•è¦ä¹ˆæ„ŸçŸ¥å’ŒæŽ¨ç†é˜¶æ®µè§£è€¦ï¼Œæ— æ³•ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œè¦ä¹ˆä¾èµ–äºŽæ˜‚è´µçš„å¯†é›†æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmniDrive-R1çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡äº¤é”™å¤šæ¨¡æ€CoT (iMCoT) æœºåˆ¶ï¼Œå°†è§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€æŽ¨ç†ç´§å¯†ç»“åˆï¼Œå®žçŽ°ç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–ã€‚åŒæ—¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ é©±åŠ¨è§†è§‰ groundingï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿè‡ªä¸»åœ°å…³æ³¨å›¾åƒä¸­çš„å…³é”®åŒºåŸŸï¼Œä»Žè€Œå‡å°‘å¯¹è±¡å¹»è§‰ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šOmniDrive-R1æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è§†è§‰è¯­è¨€æ¨¡åž‹æ¡†æž¶ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) äº¤é”™å¤šæ¨¡æ€CoT (iMCoT) æ¨¡å—ï¼Œç”¨äºŽè§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„èžåˆæŽ¨ç†ï¼›2) å¼ºåŒ–å­¦ä¹ æ¨¡å—ï¼Œç”¨äºŽè®­ç»ƒè§†è§‰ grounding èƒ½åŠ›ï¼›3) Clip-GRPO ç®—æ³•ï¼Œç”¨äºŽæä¾›æ— æ ‡æ³¨çš„ grounding å¥–åŠ±ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œæ¨¡åž‹é¦–å…ˆé€šè¿‡iMCoTè¿›è¡Œå¤šæ¨¡æ€æŽ¨ç†ï¼Œç„¶åŽåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¨¡å—ä¼˜åŒ–è§†è§‰å…³æ³¨æœºåˆ¶ï¼Œæœ€åŽé€šè¿‡Clip-GRPOç®—æ³•æä¾›å¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼æ¨¡åž‹å…³æ³¨å…³é”®åŒºåŸŸã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniDrive-R1çš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†äº¤é”™å¤šæ¨¡æ€CoT (iMCoT) æœºåˆ¶ï¼Œå®žçŽ°äº†æ„ŸçŸ¥å’ŒæŽ¨ç†çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼›2) å¼•å…¥äº†å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è§†è§‰ grounding èƒ½åŠ›ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿè‡ªä¸»åœ°å…³æ³¨å›¾åƒä¸­çš„å…³é”®åŒºåŸŸï¼›3) æå‡ºäº† Clip-GRPO ç®—æ³•ï¼Œæä¾›äº†ä¸€ç§æ— æ ‡æ³¨çš„ã€åŸºäºŽè¿‡ç¨‹çš„ grounding å¥–åŠ±ï¼Œé¿å…äº†å¯¹å¯†é›†æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šOmniDrive-R1ä½¿ç”¨äº†ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡åž‹åˆå§‹åŒ–æ¨¡åž‹å‚æ•°ã€‚ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨ Clip-GRPO ç®—æ³•è®­ç»ƒè§†è§‰ grounding èƒ½åŠ›ã€‚Clip-GRPO ç®—æ³•çš„å…³é”®åœ¨äºŽè®¾è®¡äº†ä¸€ç§åŸºäºŽè¿‡ç¨‹çš„ grounding å¥–åŠ±ï¼Œè¯¥å¥–åŠ±åŸºäºŽè§†è§‰ç„¦ç‚¹å’Œæ–‡æœ¬æŽ¨ç†ä¹‹é—´çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚å…·ä½“çš„å¥–åŠ±å‡½æ•°è®¾è®¡æœªçŸ¥ï¼Œä½†å¼ºè°ƒäº†å®žæ—¶è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

OmniDrive-R1åœ¨DriveLMM-o1æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ŽåŸºçº¿æ¨¡åž‹Qwen2.5VL-7Bç›¸æ¯”ï¼ŒOmniDrive-R1å°†æ•´ä½“æŽ¨ç†å¾—åˆ†ä»Ž51.77%æé«˜åˆ°80.35%ï¼Œæå‡äº†28.58ä¸ªç™¾åˆ†ç‚¹ï¼›æœ€ç»ˆç­”æ¡ˆå‡†ç¡®çŽ‡ä»Ž37.81%æé«˜åˆ°73.62%ï¼Œæå‡äº†35.81ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒOmniDrive-R1åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹çš„è§†è§‰è¯­è¨€æŽ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

OmniDrive-R1çš„ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½ç›‘æŽ§ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜è§†è§‰è¯­è¨€æ¨¡åž‹çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œå¯ä»¥æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ï¼Œå‡å°‘äº‹æ•…å‘ç”ŸçŽ‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºŽå…¶ä»–éœ€è¦è§†è§‰å’Œè¯­è¨€ç†è§£çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ç­‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.

