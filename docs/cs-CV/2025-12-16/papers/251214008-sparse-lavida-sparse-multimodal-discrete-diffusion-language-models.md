---
layout: default
title: Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models
---

# Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14008" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14008</a>
  <a href="https://arxiv.org/pdf/2512.14008.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14008" onclick="toggleFavorite(this, '2512.14008', 'Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Sparse-LaViDaï¼šé€šè¿‡ç¨€ç–åŒ–é‡‡æ ·åŠ é€Ÿå¤šæ¨¡æ€ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹` `ç¨€ç–é‡‡æ ·` `æ¨¡å‹åŠ é€Ÿ` `å›¾åƒç”Ÿæˆ` `å›¾åƒç¼–è¾‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MDMæ¨ç†é€Ÿåº¦å—é™äºé‡å¤å¤„ç†å†—ä½™çš„masked tokensã€‚
2. Sparse-LaViDaåŠ¨æ€æˆªæ–­ä¸å¿…è¦çš„tokensï¼Œå¹¶ç”¨register tokensä¿æŒè´¨é‡ã€‚
3. ç‰¹æ®Šattention maskä¿è¯è®­ç»ƒä¸æ¨ç†ä¸€è‡´æ€§ï¼ŒåŠ é€Ÿæ•ˆæœæ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSparse-LaViDaçš„æ–°å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€ŸMasked Discrete Diffusion Models (MDMs)çš„æ¨ç†è¿‡ç¨‹ã€‚MDMsåœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ç­‰å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºéœ€è¦åœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­é‡å¤å¤„ç†å†—ä½™çš„masked tokensï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚Sparse-LaViDaé€šè¿‡åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­åŠ¨æ€æˆªæ–­ä¸å¿…è¦çš„masked tokensæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†ä¿æŒç”Ÿæˆè´¨é‡ï¼Œå¼•å…¥äº†ä¸“é—¨çš„register tokensï¼Œä½œä¸ºè¢«æˆªæ–­tokensçš„ç´§å‡‘è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œè®¾è®¡äº†ä¸€ç§ä¸“é—¨çš„attention maskï¼Œåœ¨è®­ç»ƒæœŸé—´å¿ å®åœ°åŒ¹é…æˆªæ–­çš„é‡‡æ ·è¿‡ç¨‹ã€‚åŸºäºæœ€å…ˆè¿›çš„ç»Ÿä¸€MDM LaViDa-Oï¼ŒSparse-LaViDaåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œæ•°å­¦æ¨ç†ç­‰å¤šç§ä»»åŠ¡ä¸­å®ç°äº†é«˜è¾¾2å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„Masked Discrete Diffusion Models (MDMs)è™½ç„¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚ä¸»è¦åŸå› æ˜¯éœ€è¦åœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­é‡å¤å¤„ç†å¤§é‡çš„masked tokensï¼Œè¿™äº›tokensä¸­æœ‰å¾ˆå¤šæ˜¯å†—ä½™çš„ï¼Œä¸åŒ…å«æœ‰æ•ˆä¿¡æ¯ï¼Œå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹ã€‚å› æ­¤ï¼Œå¦‚ä½•å‡å°‘å†—ä½™è®¡ç®—ï¼ŒåŠ é€ŸMDMçš„æ¨ç†è¿‡ç¨‹æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSparse-LaViDaçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åœ°æˆªæ–­é‚£äº›ä¸å¿…è¦çš„masked tokensï¼Œä»è€Œå‡å°‘è®¡ç®—é‡ï¼ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†å¼¥è¡¥æˆªæ–­tokenså¸¦æ¥çš„ä¿¡æ¯æŸå¤±ï¼Œå¼•å…¥äº†register tokensä½œä¸ºè¢«æˆªæ–­tokensçš„ç´§å‡‘è¡¨ç¤ºï¼Œä»¥ä¿æŒç”Ÿæˆè´¨é‡ã€‚é€šè¿‡è¿™ç§ç¨€ç–åŒ–çš„é‡‡æ ·æ–¹å¼ï¼Œå¯ä»¥åœ¨ä¿è¯ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSparse-LaViDaå»ºç«‹åœ¨LaViDa-Oæ¨¡å‹ä¹‹ä¸Šï¼Œæ•´ä½“æ¡†æ¶ä»ç„¶æ˜¯æ‰©æ•£æ¨¡å‹çš„è¿­ä»£é‡‡æ ·è¿‡ç¨‹ã€‚ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ¨¡å—ï¼š1) Tokenæˆªæ–­æ¨¡å—ï¼šæ ¹æ®æŸç§ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºattention scoreï¼‰åŠ¨æ€åœ°é€‰æ‹©å¹¶æˆªæ–­ä¸€éƒ¨åˆ†masked tokensã€‚2) Register Tokenæ¨¡å—ï¼šä¸ºæ¯ä¸ªè¢«æˆªæ–­çš„tokené›†åˆç”Ÿæˆä¸€ä¸ªregister tokenï¼Œç”¨äºä¿ç•™è¢«æˆªæ–­tokençš„ä¿¡æ¯ã€‚3) Attention Maskæ¨¡å—ï¼šè®¾è®¡ä¸€ç§ç‰¹æ®Šçš„attention maskï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿæ¨ç†æ—¶çš„æˆªæ–­è¿‡ç¨‹ï¼Œä¿è¯è®­ç»ƒå’Œæ¨ç†çš„ä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šSparse-LaViDaçš„å…³é”®åˆ›æ–°åœ¨äºåŠ¨æ€ç¨€ç–åŒ–é‡‡æ ·å’Œregister tokençš„è®¾è®¡ã€‚ä¸ä¼ ç»Ÿçš„MDMæ–¹æ³•ä¸åŒï¼ŒSparse-LaViDaä¸æ˜¯åœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­å¤„ç†æ‰€æœ‰çš„masked tokensï¼Œè€Œæ˜¯åªå¤„ç†ä¸€éƒ¨åˆ†é‡è¦çš„tokensï¼Œä»è€Œå‡å°‘äº†è®¡ç®—é‡ã€‚Register tokençš„è®¾è®¡ä¿è¯äº†åœ¨æˆªæ–­tokensçš„åŒæ—¶ï¼Œå°½å¯èƒ½åœ°ä¿ç•™äº†è¢«æˆªæ–­tokensçš„ä¿¡æ¯ï¼Œé¿å…äº†ç”Ÿæˆè´¨é‡çš„ä¸‹é™ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³äºtokenæˆªæ–­æ¨¡å—ï¼Œä¸€ç§å¯èƒ½çš„å®ç°æ–¹å¼æ˜¯åŸºäºattention scoreè¿›è¡Œé€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è®¡ç®—æ¯ä¸ªmasked tokenä¸å…¶ä»–tokenä¹‹é—´çš„å¹³å‡attention scoreï¼Œç„¶åé€‰æ‹©attention scoreè¾ƒä½çš„tokensè¿›è¡Œæˆªæ–­ã€‚Register tokenå¯ä»¥é€šè¿‡ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œæ¥ç”Ÿæˆï¼Œå…¶è¾“å…¥æ˜¯è¢«æˆªæ–­çš„tokensçš„è¡¨ç¤ºï¼Œè¾“å‡ºæ˜¯register tokençš„è¡¨ç¤ºã€‚Attention maskçš„è®¾è®¡éœ€è¦ä¿è¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åªèƒ½çœ‹åˆ°æœªè¢«æˆªæ–­çš„tokenså’Œregister tokensï¼Œä»è€Œæ¨¡æ‹Ÿæ¨ç†æ—¶çš„æˆªæ–­è¿‡ç¨‹ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14008/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14008/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14008/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

Sparse-LaViDaåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œæ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­ï¼ŒSparse-LaViDaç›¸æ¯”äºLaViDa-Oå®ç°äº†é«˜è¾¾2å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ã€‚è¿™è¡¨æ˜Sparse-LaViDaæ˜¯ä¸€ç§æœ‰æ•ˆçš„åŠ é€ŸMDMæ¨ç†çš„æ–¹æ³•ï¼Œå…·æœ‰å¾ˆå¼ºçš„å®ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Sparse-LaViDaå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€è§†é¢‘ç”Ÿæˆã€3Då†…å®¹ç”Ÿæˆã€æ•°å­¦æ¨ç†ç­‰ã€‚é€šè¿‡åŠ é€Ÿå¤šæ¨¡æ€å†…å®¹çš„ç”Ÿæˆå’Œç¼–è¾‘è¿‡ç¨‹ï¼Œå¯ä»¥æé«˜ç”Ÿäº§æ•ˆç‡ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸ºåˆ›æ„è®¾è®¡æä¾›æ›´å¤šå¯èƒ½æ€§ã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚æ•™è‚²ã€å¨±ä¹ã€åŒ»ç–—ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.

