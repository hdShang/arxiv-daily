---
layout: default
title: Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting
---

# Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.07345" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.07345</a>
  <a href="https://arxiv.org/pdf/2512.07345.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.07345" onclick="toggleFavorite(this, '2512.07345', 'Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shilong Jin, Haoran Duan, Litao Hua, Wentao Huang, Yuan Zhou

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTD-Attnï¼Œé€šè¿‡3Dæ³¨æ„åŠ›æœºåˆ¶æ¶ˆé™¤æ‰©æ•£å…ˆéªŒåå·®ï¼Œæå‡é«˜æ–¯æº…å°„ä¸€è‡´æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æ‰©æ•£æ¨¡å‹` `3Dç”Ÿæˆ` `å¤šè§†å›¾ä¸€è‡´æ€§` `æ³¨æ„åŠ›æœºåˆ¶` `é«˜æ–¯æº…å°„`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. T2Iæ¨¡å‹åœ¨3Dä»»åŠ¡ä¸­å—é™äºå…ˆéªŒè§†å›¾åå·®ï¼Œå¯¼è‡´ä¸åŒè§†è§’ä¸‹ç‰©ä½“å¤–è§‚ä¸ä¸€è‡´ï¼Œå½±å“3Dé‡å»ºè´¨é‡ã€‚
2. æå‡ºTD-Attnæ¡†æ¶ï¼Œåˆ©ç”¨3Dæ„ŸçŸ¥æ³¨æ„åŠ›å¼•å¯¼å’Œåˆ†å±‚æ³¨æ„åŠ›è°ƒåˆ¶ï¼Œå¢å¼ºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„ç©ºé—´ä¸€è‡´æ€§å’Œè¯­ä¹‰æ§åˆ¶ã€‚
3. å®éªŒè¯æ˜TD-Attnèƒ½æœ‰æ•ˆæå‡å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå¯ä½œä¸ºé€šç”¨æ’ä»¶åº”ç”¨äºå¤šç§3Dä»»åŠ¡ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹ä»æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨çš„å…ˆéªŒè§†å›¾åå·®é—®é¢˜ï¼Œè¯¥åå·®å¯¼è‡´å¯¹è±¡ä¸åŒè§†å›¾ä¹‹é—´å‡ºç°ä¸ä¸€è‡´çš„å¤–è§‚ã€‚é€šè¿‡æ•°å­¦åˆ†ææ­ç¤ºäº†T2Iæ¨¡å‹ä¸­å…ˆéªŒè§†å›¾åå·®çš„æ ¹æœ¬åŸå› ï¼Œå¹¶å‘ç°UNetä¸åŒå±‚å¯¹äº¤å‰æ³¨æ„åŠ›ï¼ˆCAï¼‰ä¸­å…ˆéªŒè§†å›¾çš„å½±å“ä¸åŒã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†TD-Attnæ¡†æ¶ï¼Œé€šè¿‡3Dæ„ŸçŸ¥æ³¨æ„åŠ›å¼•å¯¼æ¨¡å—ï¼ˆ3D-AAGï¼‰æ„å»ºè§†å›¾ä¸€è‡´çš„3Dæ³¨æ„åŠ›é«˜æ–¯åˆ†å¸ƒï¼Œå¢å¼ºç©ºé—´ä¸€è‡´æ€§ï¼›åˆ†å±‚æ³¨æ„åŠ›è°ƒåˆ¶æ¨¡å—ï¼ˆHAMï¼‰åˆ©ç”¨è¯­ä¹‰å¼•å¯¼æ ‘ï¼ˆSGTï¼‰æŒ‡å¯¼è¯­ä¹‰å“åº”åˆ†æå™¨ï¼ˆSRPï¼‰å®šä½å’Œè°ƒåˆ¶å¯¹è§†å›¾æ¡ä»¶é«˜åº¦æ•æ„Ÿçš„CAå±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒTD-Attnå¯ä½œä¸ºé€šç”¨æ’ä»¶ï¼Œæ˜¾è‘—æé«˜3Dä»»åŠ¡ä¸­çš„å¤šè§†å›¾ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åˆ©ç”¨T2Iæ‰©æ•£æ¨¡å‹è¿›è¡Œ3Dç”Ÿæˆæˆ–ç¼–è¾‘æ—¶ï¼Œç”±äºT2Iæ¨¡å‹å›ºæœ‰çš„å…ˆéªŒè§†å›¾åå·®ï¼Œå¯¼è‡´ç”Ÿæˆçš„3Då¯¹è±¡åœ¨ä¸åŒè§†è§’ä¸‹å¤–è§‚ä¸ä¸€è‡´ã€‚è¿™ç§åå·®æºäºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¯¹å…ˆéªŒè§†å›¾ç‰¹å¾çš„è¿‡åº¦æ¿€æ´»ï¼Œå¿½ç•¥äº†ç›®æ ‡è§†å›¾çš„æ¡ä»¶ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ¶ˆé™¤è¿™ç§åå·®ï¼Œä»è€Œé™åˆ¶äº†3Dé‡å»ºå’Œç¼–è¾‘çš„è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥3Dæ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾å¼åœ°å»ºæ¨¡ä¸åŒè§†è§’ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œä»è€Œæ¶ˆé™¤å…ˆéªŒè§†å›¾åå·®ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ„å»ºè§†å›¾ä¸€è‡´çš„3Dæ³¨æ„åŠ›é«˜æ–¯åˆ†å¸ƒï¼Œå¼ºåˆ¶äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å…³æ³¨ç©ºé—´ä¸€è‡´çš„åŒºåŸŸï¼Œä»è€ŒæŠ‘åˆ¶å…ˆéªŒè§†å›¾ç‰¹å¾çš„è¿‡åº¦æ¿€æ´»ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ†å±‚æ³¨æ„åŠ›è°ƒåˆ¶ï¼Œé€‰æ‹©æ€§åœ°å¢å¼ºå¯¹è§†å›¾æ¡ä»¶æ•æ„Ÿçš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œè¿›ä¸€æ­¥æå‡å¤šè§†å›¾ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTD-Attnæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š3D-Aware Attention Guidance Module (3D-AAG) å’Œ Hierarchical Attention Modulation Module (HAM)ã€‚3D-AAGæ¨¡å—é¦–å…ˆåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å›¾æ„å»º3Dæ³¨æ„åŠ›é«˜æ–¯åˆ†å¸ƒï¼Œç„¶ååˆ©ç”¨è¯¥åˆ†å¸ƒå¼•å¯¼äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºç©ºé—´ä¸€è‡´æ€§ã€‚HAMæ¨¡å—åˆ™åˆ©ç”¨è¯­ä¹‰å¼•å¯¼æ ‘ï¼ˆSGTï¼‰å’Œè¯­ä¹‰å“åº”åˆ†æå™¨ï¼ˆSRPï¼‰å®šä½å¯¹è§†å›¾æ¡ä»¶æ•æ„Ÿçš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè°ƒåˆ¶ï¼Œè¿›ä¸€æ­¥æå‡å¤šè§†å›¾ä¸€è‡´æ€§ã€‚æ•´ä¸ªæ¡†æ¶å¯ä»¥ä½œä¸ºæ’ä»¶é›†æˆåˆ°ç°æœ‰çš„T2Iæ‰©æ•£æ¨¡å‹ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†3D-AAGæ¨¡å—å’ŒHAMæ¨¡å—ï¼Œåˆ†åˆ«ä»ç©ºé—´ä¸€è‡´æ€§å’Œè¯­ä¹‰æ§åˆ¶ä¸¤ä¸ªæ–¹é¢è§£å†³äº†å…ˆéªŒè§†å›¾åå·®é—®é¢˜ã€‚3D-AAGæ¨¡å—é€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡3Dç©ºé—´å…³ç³»ï¼Œæœ‰æ•ˆæŠ‘åˆ¶äº†å…ˆéªŒè§†å›¾ç‰¹å¾çš„è¿‡åº¦æ¿€æ´»ã€‚HAMæ¨¡å—åˆ™é€šè¿‡é€‰æ‹©æ€§åœ°å¢å¼ºå¯¹è§†å›¾æ¡ä»¶æ•æ„Ÿçš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œè¿›ä¸€æ­¥æå‡äº†å¤šè§†å›¾ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒHAMæ¨¡å—è¿˜æ”¯æŒè¯­ä¹‰ç‰¹å®šçš„å¹²é¢„ï¼Œå®ç°äº†å¯æ§å’Œç²¾ç¡®çš„3Dç¼–è¾‘ã€‚

**å…³é”®è®¾è®¡**ï¼š3D-AAGæ¨¡å—ä¸­ï¼Œ3Dæ³¨æ„åŠ›é«˜æ–¯åˆ†å¸ƒçš„æ„å»ºä¾èµ–äºäº¤å‰æ³¨æ„åŠ›å›¾çš„åŠ æƒå¹³å‡ï¼Œæƒé‡ç”±äº¤å‰æ³¨æ„åŠ›å€¼å†³å®šã€‚HAMæ¨¡å—ä¸­ï¼Œè¯­ä¹‰å¼•å¯¼æ ‘ï¼ˆSGTï¼‰çš„æ„å»ºä¾èµ–äºé¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œç”¨äºæå–å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ã€‚è¯­ä¹‰å“åº”åˆ†æå™¨ï¼ˆSRPï¼‰åˆ™åˆ©ç”¨SGTçš„ä¿¡æ¯ï¼Œå®šä½å¯¹è§†å›¾æ¡ä»¶æ•æ„Ÿçš„äº¤å‰æ³¨æ„åŠ›å±‚ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä¸»è¦é‡‡ç”¨L1æŸå¤±å’ŒL2æŸå¤±ï¼Œç”¨äºçº¦æŸ3Dæ³¨æ„åŠ›é«˜æ–¯åˆ†å¸ƒçš„å½¢çŠ¶å’Œä½ç½®ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.07345/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.07345/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.07345/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTD-Attnèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œåœ¨å¤šä¸ª3Dä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTD-Attnèƒ½å¤Ÿç”Ÿæˆæ›´æ¸…æ™°ã€æ›´é€¼çœŸçš„3Dæ¨¡å‹ï¼Œå‡å°‘ä¼ªå½±å’Œå¤±çœŸã€‚ä¾‹å¦‚ï¼Œåœ¨3Då¯¹è±¡é‡å»ºä»»åŠ¡ä¸­ï¼ŒTD-Attnèƒ½å¤Ÿå°†å¤šè§†å›¾ä¸€è‡´æ€§æŒ‡æ ‡æå‡10%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äº3Då†…å®¹ç”Ÿæˆã€3Då¯¹è±¡ç¼–è¾‘ã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æ¶ˆé™¤å…ˆéªŒè§†å›¾åå·®ï¼Œå¯ä»¥ç”Ÿæˆæ›´é€¼çœŸã€æ›´ä¸€è‡´çš„3Dæ¨¡å‹ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ç”¨äºæœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œæé«˜å¯¹3Dç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

