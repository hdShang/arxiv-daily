---
layout: default
title: S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation
---

# S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation

**arXiv**: [2512.14440v1](https://arxiv.org/abs/2512.14440) | [PDF](https://arxiv.org/pdf/2512.14440.pdf)

**ä½œè€…**: Leon Sick, Lukas Hoyer, Dominik Engel, Pedro Hermosilla, Timo Ropinski

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS2Dç¨€ç–åˆ°ç¨ å¯†å…³é”®æŽ©ç è’¸é¦æ–¹æ³•ï¼Œåˆ©ç”¨çœŸå®žè§†é¢‘æ•°æ®è§£å†³æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²ä¸­çš„è¿åŠ¨å»ºæ¨¡ä¸è¶³é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **åŠ¨ä½œç”Ÿæˆ**

**å…³é”®è¯**: `æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²` `ç¨€ç–åˆ°ç¨ å¯†è’¸é¦` `æ·±åº¦è¿åŠ¨å…ˆéªŒ` `å…³é”®æŽ©ç è¯†åˆ«` `æ—¶é—´ä¸€è‡´æ€§` `çœŸå®žè§†é¢‘æ•°æ®` `éšå¼æŽ©ç ä¼ æ’­` `æ—¶é—´DropLoss`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–åˆæˆè§†é¢‘æ•°æ®ï¼Œæ— æ³•å‡†ç¡®å»ºæ¨¡çœŸå®žè¿åŠ¨å¦‚è§†è§’å˜åŒ–å’Œéƒ¨åˆ†è¿åŠ¨ï¼Œå¯¼è‡´åˆ†å‰²è´¨é‡å—é™ã€‚
2. æå‡ºS2Dæ–¹æ³•ï¼Œåˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒè¯†åˆ«é«˜è´¨é‡å…³é”®æŽ©ç ï¼Œå¹¶é€šè¿‡ç¨€ç–åˆ°ç¨ å¯†è’¸é¦è®­ç»ƒæ¨¡åž‹å®žçŽ°éšå¼æŽ©ç ä¼ æ’­ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—æå‡äº†æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²çš„æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²çš„æœ€å…ˆè¿›æ–¹æ³•ä¸¥é‡ä¾èµ–ä»ŽImageNetç­‰ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å›¾åƒæ•°æ®é›†ç”Ÿæˆçš„åˆæˆè§†é¢‘æ•°æ®ã€‚ç„¶è€Œï¼Œé€šè¿‡äººå·¥å¹³ç§»å’Œç¼©æ”¾å›¾åƒå®žä¾‹æŽ©ç æ¥åˆæˆè§†é¢‘çš„æ–¹æ³•æ— æ³•å‡†ç¡®å»ºæ¨¡è§†é¢‘ä¸­çš„çœŸå®žè¿åŠ¨ï¼Œä¾‹å¦‚è§†è§’å˜åŒ–ã€å•ä¸ªæˆ–å¤šä¸ªå®žä¾‹çš„éƒ¨åˆ†è¿åŠ¨æˆ–ç›¸æœºè¿åŠ¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»…ä½¿ç”¨çœŸå®žè§†é¢‘æ•°æ®è®­ç»ƒçš„æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ¨¡åž‹ã€‚æˆ‘ä»¬ä»Žå•ä¸ªè§†é¢‘å¸§ä¸Šçš„æ— ç›‘ç£å®žä¾‹åˆ†å‰²æŽ©ç å¼€å§‹ã€‚ç„¶è€Œï¼Œè¿™äº›å•å¸§åˆ†å‰²å­˜åœ¨æ—¶é—´å™ªå£°ï¼Œä¸”å…¶è´¨é‡åœ¨æ•´ä¸ªè§†é¢‘ä¸­å˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒè¯†åˆ«è§†é¢‘ä¸­çš„é«˜è´¨é‡å…³é”®æŽ©ç æ¥å»ºç«‹æ—¶é—´ä¸€è‡´æ€§ã€‚ç¨€ç–çš„å…³é”®æŽ©ç ä¼ªæ ‡æ³¨éšåŽç”¨äºŽè®­ç»ƒä¸€ä¸ªç”¨äºŽéšå¼æŽ©ç ä¼ æ’­çš„åˆ†å‰²æ¨¡åž‹ï¼Œä¸ºæ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±æ—¶é—´DropLossè¾…åŠ©çš„ç¨€ç–åˆ°ç¨ å¯†è’¸é¦æ–¹æ³•ã€‚åœ¨æœ€ç»ˆæ¨¡åž‹ä¸Šå¯¹ç”Ÿæˆçš„ç¨ å¯†æ ‡ç­¾é›†è¿›è¡Œè®­ç»ƒåŽï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å½“å‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

è®ºæ–‡æå‡ºS2Dæ¡†æž¶ï¼Œæ•´ä½“åŸºäºŽçœŸå®žè§†é¢‘æ•°æ®è®­ç»ƒæ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ¨¡åž‹ã€‚é¦–å…ˆä»Žå•å¸§æ— ç›‘ç£åˆ†å‰²æŽ©ç å‡ºå‘ï¼Œåˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒè¯†åˆ«é«˜è´¨é‡å…³é”®æŽ©ç ä»¥å»ºç«‹æ—¶é—´ä¸€è‡´æ€§ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ç¨€ç–åˆ°ç¨ å¯†è’¸é¦æ–¹æ³•ï¼Œå°†ç¨€ç–å…³é”®æŽ©ç ä¼ªæ ‡æ³¨ç”¨äºŽè®­ç»ƒåˆ†å‰²æ¨¡åž‹è¿›è¡Œéšå¼æŽ©ç ä¼ æ’­ï¼Œå¹¶å¼•å…¥æ—¶é—´DropLossè¾…åŠ©è®­ç»ƒã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽå®Œå…¨ä¾èµ–çœŸå®žè§†é¢‘æ•°æ®è€Œéžåˆæˆæ•°æ®ï¼Œé€šè¿‡è¿åŠ¨å…ˆéªŒå’Œè’¸é¦ç­–ç•¥æœ‰æ•ˆå¤„ç†æ—¶é—´å™ªå£°å’ŒæŽ©ç è´¨é‡å˜åŒ–ï¼Œé¿å…äº†åˆæˆæ•°æ®ä¸­è¿åŠ¨å»ºæ¨¡ä¸å‡†ç¡®çš„é—®é¢˜ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒS2Dæ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ¨¡åž‹ï¼Œè¯æ˜Žäº†ä»…ä½¿ç”¨çœŸå®žè§†é¢‘æ•°æ®è®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡ç¨€ç–åˆ°ç¨ å¯†è’¸é¦å’Œæ—¶é—´DropLossæå‡äº†åˆ†å‰²æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè§†é¢‘ç›‘æŽ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººè§†è§‰å’Œè§†é¢‘ç¼–è¾‘ç­‰é¢†åŸŸï¼Œé€šè¿‡æ— ç›‘ç£å­¦ä¹ å®žçŽ°é«˜è´¨é‡çš„è§†é¢‘å®žä¾‹åˆ†å‰²ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæå‡åœ¨çœŸå®žä¸–ç•Œè§†é¢‘ä¸­çš„åˆ†å‰²å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.

