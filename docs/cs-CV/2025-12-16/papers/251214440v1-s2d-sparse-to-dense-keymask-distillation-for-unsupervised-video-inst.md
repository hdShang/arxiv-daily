---
layout: default
title: S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation
---

# S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14440" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14440v1</a>
  <a href="https://arxiv.org/pdf/2512.14440.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14440v1" onclick="toggleFavorite(this, '2512.14440v1', 'S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Leon Sick, Lukas Hoyer, Dominik Engel, Pedro Hermosilla, Timo Ropinski

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS2Dï¼šä¸€ç§ç¨€ç–åˆ°ç¨ å¯†çš„Keymaskè’¸é¦æ–¹æ³•ï¼Œç”¨äºæ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— ç›‘ç£å­¦ä¹ ` `è§†é¢‘å®ä¾‹åˆ†å‰²` `å…³é”®å¸§é€‰æ‹©` `ç¨€ç–åˆ°ç¨ å¯†` `çŸ¥è¯†è’¸é¦` `æ·±åº¦è¿åŠ¨å…ˆéªŒ` `æ—¶é—´ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²æ–¹æ³•ä¾èµ–åˆæˆæ•°æ®ï¼Œéš¾ä»¥æ¨¡æ‹ŸçœŸå®è§†é¢‘ä¸­çš„å¤æ‚è¿åŠ¨ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºçœŸå®è§†é¢‘æ•°æ®çš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å…³é”®å¸§æ©ç ä¼ æ’­å®ç°æ—¶é—´ä¸€è‡´æ€§ã€‚
3. æå‡ºçš„S2Dæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²é¢†åŸŸçš„æœ€å…ˆè¿›æ–¹æ³•ä¸¥é‡ä¾èµ–äºåˆæˆè§†é¢‘æ•°æ®ï¼Œè¿™äº›æ•°æ®é€šå¸¸ç”±ImageNetç­‰ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å›¾åƒæ•°æ®é›†ç”Ÿæˆã€‚ç„¶è€Œï¼Œé€šè¿‡äººä¸ºåœ°ç§»åŠ¨å’Œç¼©æ”¾å›¾åƒå®ä¾‹æ©ç æ¥åˆæˆè§†é¢‘ï¼Œæ— æ³•å‡†ç¡®åœ°æ¨¡æ‹Ÿè§†é¢‘ä¸­çœŸå®çš„è¿åŠ¨ï¼Œä¾‹å¦‚é€è§†å˜åŒ–ã€å•ä¸ªæˆ–å¤šä¸ªå®ä¾‹çš„éƒ¨åˆ†è¿åŠ¨æˆ–ç›¸æœºè¿åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®Œå…¨åœ¨çœŸå®è§†é¢‘æ•°æ®ä¸Šè®­ç»ƒçš„æ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬ä»å•ä¸ªè§†é¢‘å¸§ä¸Šçš„æ— ç›‘ç£å®ä¾‹åˆ†å‰²æ©ç å¼€å§‹ã€‚ç„¶è€Œï¼Œè¿™äº›å•å¸§åˆ†å‰²è¡¨ç°å‡ºæ—¶é—´å™ªå£°ï¼Œå¹¶ä¸”å…¶è´¨é‡åœ¨æ•´ä¸ªè§†é¢‘ä¸­å˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒæ¥è¯†åˆ«è§†é¢‘ä¸­çš„é«˜è´¨é‡å…³é”®æ©ç ï¼Œä»è€Œå»ºç«‹æ—¶é—´ä¸€è‡´æ€§ã€‚ç„¶åï¼Œç¨€ç–çš„å…³é”®æ©ç ä¼ªæ³¨é‡Šç”¨äºè®­ç»ƒåˆ†å‰²æ¨¡å‹ä»¥è¿›è¡Œéšå¼æ©ç ä¼ æ’­ï¼Œä¸ºæ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±æ—¶é—´DropLossè¾…åŠ©çš„ç¨€ç–åˆ°ç¨ å¯†è’¸é¦æ–¹æ³•ã€‚åœ¨ç”±æ­¤äº§ç”Ÿçš„ç¨ å¯†æ ‡ç­¾é›†ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹åï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²æ—¨åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå¯¹è§†é¢‘ä¸­çš„æ¯ä¸ªå®ä¾‹è¿›è¡Œåˆ†å‰²å’Œè·Ÿè¸ªã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºåˆæˆæ•°æ®ï¼Œä½†åˆæˆæ•°æ®éš¾ä»¥æ¨¡æ‹ŸçœŸå®è§†é¢‘ä¸­çš„å¤æ‚è¿åŠ¨ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®ã€‚æ­¤å¤–ï¼Œå•å¸§åˆ†å‰²ç»“æœå­˜åœ¨æ—¶é—´å™ªå£°ï¼Œç¼ºä¹æ—¶é—´ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çœŸå®è§†é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡å…³é”®å¸§æ©ç ä¼ æ’­æ¥å»ºç«‹æ—¶é—´ä¸€è‡´æ€§ã€‚é¦–å…ˆï¼Œåˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒé€‰æ‹©é«˜è´¨é‡çš„å…³é”®å¸§æ©ç ã€‚ç„¶åï¼Œåˆ©ç”¨è¿™äº›ç¨€ç–çš„å…³é”®å¸§æ©ç ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œé€šè¿‡ç¨€ç–åˆ°ç¨ å¯†çš„è’¸é¦æ–¹æ³•è®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œä»è€Œå®ç°éšå¼æ©ç ä¼ æ’­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) å•å¸§æ— ç›‘ç£å®ä¾‹åˆ†å‰²ï¼šå¯¹è§†é¢‘çš„æ¯ä¸€å¸§è¿›è¡Œæ— ç›‘ç£å®ä¾‹åˆ†å‰²ï¼Œå¾—åˆ°åˆå§‹çš„åˆ†å‰²æ©ç ã€‚2) å…³é”®å¸§é€‰æ‹©ï¼šåˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒï¼Œé€‰æ‹©è§†é¢‘ä¸­è´¨é‡è¾ƒé«˜çš„å…³é”®å¸§ï¼Œå¹¶å°†å…¶å¯¹åº”çš„åˆ†å‰²æ©ç ä½œä¸ºå…³é”®æ©ç ã€‚3) ç¨€ç–åˆ°ç¨ å¯†è’¸é¦ï¼šåˆ©ç”¨å…³é”®æ©ç ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œè®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå°†ç¨€ç–çš„å…³é”®æ©ç ä¼ æ’­åˆ°æ•´ä¸ªè§†é¢‘åºåˆ—ï¼Œç”Ÿæˆç¨ å¯†çš„åˆ†å‰²ç»“æœã€‚4) æ¨¡å‹è®­ç»ƒï¼šåœ¨ç”Ÿæˆçš„ç¨ å¯†æ ‡ç­¾é›†ä¸Šè®­ç»ƒæœ€ç»ˆçš„åˆ†å‰²æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§ç¨€ç–åˆ°ç¨ å¯†çš„è’¸é¦æ–¹æ³•ï¼Œç”¨äºå°†å…³é”®å¸§æ©ç ä¼ æ’­åˆ°æ•´ä¸ªè§†é¢‘åºåˆ—ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…³é”®å¸§æ©ç ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œé€šè¿‡è’¸é¦å­¦ä¹ çš„æ–¹å¼ï¼Œè®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹è§†é¢‘ä¸­æ‰€æœ‰å¸§çš„åˆ†å‰²æ©ç ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¼•å…¥äº†Temporal DropLossï¼Œç”¨äºå¢å¼ºæ¨¡å‹çš„æ—¶é—´ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•ä½¿ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒæ¥é€‰æ‹©å…³é”®å¸§ï¼Œå…·ä½“æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨å…‰æµç­‰æ–¹æ³•æ¥ä¼°è®¡è§†é¢‘å¸§ä¹‹é—´çš„è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶é€‰æ‹©è¿åŠ¨å¹…åº¦è¾ƒå°çš„å¸§ä½œä¸ºå…³é”®å¸§ã€‚åœ¨ç¨€ç–åˆ°ç¨ å¯†è’¸é¦è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å„ç§åˆ†å‰²æ¨¡å‹ä½œä¸ºå­¦ç”Ÿæ¨¡å‹ï¼Œä¾‹å¦‚Mask R-CNNç­‰ã€‚Temporal DropLossçš„è®¾è®¡ç›®æ ‡æ˜¯æƒ©ç½šç›¸é‚»å¸§ä¹‹é—´åˆ†å‰²ç»“æœçš„ä¸ä¸€è‡´æ€§ï¼Œå¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±ç­‰æ–¹æ³•æ¥å®ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ— ç›‘ç£è§†é¢‘å®ä¾‹åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„åˆ†å‰²ç²¾åº¦æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†5%ä»¥ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨çœŸå®è§†é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„åˆ†å‰²ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç›‘æ§ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ç”¨äºè¯†åˆ«å’Œè·Ÿè¸ªè½¦è¾†ã€è¡Œäººç­‰ç›®æ ‡ã€‚åœ¨è§†é¢‘ç›‘æ§ä¸­ï¼Œå¯ä»¥ç”¨äºæ£€æµ‹å¼‚å¸¸è¡Œä¸ºã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥ç”¨äºè¯†åˆ«å’Œé¿å¼€éšœç¢ç‰©ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡è®¡ç®—æœºè§†è§‰ç³»ç»Ÿåœ¨å¤æ‚çœŸå®åœºæ™¯ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.

