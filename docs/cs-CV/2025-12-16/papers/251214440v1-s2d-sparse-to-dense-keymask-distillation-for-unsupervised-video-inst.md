---
layout: default
title: S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation
---

# S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation

**arXiv**: [2512.14440v1](https://arxiv.org/abs/2512.14440) | [PDF](https://arxiv.org/pdf/2512.14440.pdf)

**ä½œè€…**: Leon Sick, Lukas Hoyer, Dominik Engel, Pedro Hermosilla, Timo Ropinski

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºS2Dç¨€ç–åˆ°ç¨ å¯†å…³é”®æŽ©ç è’¸é¦æ–¹æ³•ï¼Œä»¥è§£å†³æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²ä¸­åˆæˆæ•°æ®è¿åŠ¨å»ºæ¨¡ä¸å‡†ç¡®çš„é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **åŠ¨ä½œç”Ÿæˆ**

**å…³é”®è¯**: `æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²` `ç¨€ç–åˆ°ç¨ å¯†è’¸é¦` `å…³é”®æŽ©ç è¯†åˆ«` `æ·±åº¦è¿åŠ¨å…ˆéªŒ` `æ—¶é—´ä¸€è‡´æ€§` `çœŸå®žè§†é¢‘æ•°æ®è®­ç»ƒ` `éšå¼æŽ©ç ä¼ æ’­` `Temporal DropLoss`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–åˆæˆè§†é¢‘æ•°æ®ï¼Œä½†äººå·¥ç”Ÿæˆçš„æŽ©ç è¿åŠ¨æ— æ³•å‡†ç¡®å»ºæ¨¡çœŸå®žè§†é¢‘ä¸­çš„å¤æ‚è¿åŠ¨ï¼Œå¦‚è§†è§’å˜åŒ–å’Œéƒ¨åˆ†è¿åŠ¨ã€‚
2. æå‡ºS2Dæ–¹æ³•ï¼Œåˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒè¯†åˆ«é«˜è´¨é‡å…³é”®æŽ©ç ï¼Œå¹¶é€šè¿‡ç¨€ç–åˆ°ç¨ å¯†è’¸é¦è®­ç»ƒæ¨¡åž‹å®žçŽ°éšå¼æŽ©ç ä¼ æ’­ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—æå‡äº†æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²çš„æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²çš„æœ€å…ˆè¿›æ–¹æ³•ä¸¥é‡ä¾èµ–ä»ŽImageNetç­‰ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å›¾åƒæ•°æ®é›†ç”Ÿæˆçš„åˆæˆè§†é¢‘æ•°æ®ã€‚ç„¶è€Œï¼Œé€šè¿‡äººå·¥å¹³ç§»å’Œç¼©æ”¾å›¾åƒå®žä¾‹æŽ©ç æ¥åˆæˆè§†é¢‘ï¼Œæ— æ³•å‡†ç¡®å»ºæ¨¡è§†é¢‘ä¸­çš„çœŸå®žè¿åŠ¨ï¼Œå¦‚è§†è§’å˜åŒ–ã€å•ä¸ªæˆ–å¤šä¸ªå®žä¾‹çš„éƒ¨åˆ†è¿åŠ¨æˆ–ç›¸æœºè¿åŠ¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»…ä½¿ç”¨çœŸå®žè§†é¢‘æ•°æ®è®­ç»ƒçš„æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ¨¡åž‹ã€‚æˆ‘ä»¬ä»Žå•ä¸ªè§†é¢‘å¸§ä¸Šçš„æ— ç›‘ç£å®žä¾‹åˆ†å‰²æŽ©ç å¼€å§‹ã€‚ç„¶è€Œï¼Œè¿™äº›å•å¸§åˆ†å‰²å­˜åœ¨æ—¶é—´å™ªå£°ï¼Œä¸”å…¶è´¨é‡åœ¨æ•´ä¸ªè§†é¢‘ä¸­å˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒè¯†åˆ«è§†é¢‘ä¸­çš„é«˜è´¨é‡å…³é”®æŽ©ç æ¥å»ºç«‹æ—¶é—´ä¸€è‡´æ€§ã€‚ç¨€ç–çš„å…³é”®æŽ©ç ä¼ªæ ‡æ³¨éšåŽç”¨äºŽè®­ç»ƒä¸€ä¸ªç”¨äºŽéšå¼æŽ©ç ä¼ æ’­çš„åˆ†å‰²æ¨¡åž‹ï¼Œä¸ºæ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±æ—¶é—´DropLossè¾…åŠ©çš„ç¨€ç–åˆ°ç¨ å¯†è’¸é¦æ–¹æ³•ã€‚åœ¨æœ€ç»ˆæ¨¡åž‹ä¸Šå¯¹ç”Ÿæˆçš„ç¨ å¯†æ ‡ç­¾é›†è¿›è¡Œè®­ç»ƒåŽï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ°´å¹³ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²ä¸­ï¼ŒçŽ°æœ‰æ–¹æ³•ä¾èµ–åˆæˆè§†é¢‘æ•°æ®å¯¼è‡´è¿åŠ¨å»ºæ¨¡ä¸å‡†ç¡®çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šè¿‡äººå·¥å¹³ç§»å’Œç¼©æ”¾å›¾åƒå®žä¾‹æŽ©ç ç”Ÿæˆåˆæˆè§†é¢‘ï¼Œæ— æ³•æœ‰æ•ˆæ¨¡æ‹ŸçœŸå®žè§†é¢‘ä¸­çš„å¤æ‚è¿åŠ¨ï¼Œå¦‚è§†è§’å˜åŒ–ã€å®žä¾‹éƒ¨åˆ†è¿åŠ¨æˆ–ç›¸æœºè¿åŠ¨ï¼Œè¿™é™åˆ¶äº†æ¨¡åž‹åœ¨çœŸå®žåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»…ä½¿ç”¨çœŸå®žè§†é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡è¯†åˆ«é«˜è´¨é‡çš„å…³é”®æŽ©ç æ¥å»ºç«‹æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨ç¨€ç–åˆ°ç¨ å¯†è’¸é¦æ–¹æ³•è®­ç»ƒæ¨¡åž‹å®žçŽ°éšå¼æŽ©ç ä¼ æ’­ã€‚è¿™æ ·è®¾è®¡é¿å…äº†åˆæˆæ•°æ®çš„åå·®ï¼Œç›´æŽ¥å­¦ä¹ çœŸå®žè§†é¢‘ä¸­çš„è¿åŠ¨æ¨¡å¼ï¼Œä»Žè€Œæå‡åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æµç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä»Žå•ä¸ªè§†é¢‘å¸§çš„æ— ç›‘ç£å®žä¾‹åˆ†å‰²æŽ©ç å¼€å§‹ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒè¯†åˆ«è§†é¢‘ä¸­çš„é«˜è´¨é‡å…³é”®æŽ©ç ï¼Œå»ºç«‹ç¨€ç–çš„ä¼ªæ ‡æ³¨ï¼›ç„¶åŽï¼Œä½¿ç”¨è¿™äº›ç¨€ç–å…³é”®æŽ©ç é€šè¿‡ç¨€ç–åˆ°ç¨ å¯†è’¸é¦æ–¹æ³•è®­ç»ƒä¸€ä¸ªåˆ†å‰²æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹èƒ½å¤Ÿéšå¼ä¼ æ’­æŽ©ç ä¿¡æ¯ï¼›æœ€åŽï¼Œåœ¨ç”Ÿæˆçš„ç¨ å¯†æ ‡ç­¾é›†ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡åž‹ï¼Œå®žçŽ°ç«¯åˆ°ç«¯çš„æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æå‡ºSparse-To-Dense Distillationï¼ˆS2Dï¼‰æ–¹æ³•ï¼Œç»“åˆTemporal DropLossï¼Œç›´æŽ¥ä»ŽçœŸå®žè§†é¢‘æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œä¸ä¾èµ–åˆæˆæ•°æ®ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼Œå®ƒé€šè¿‡æ·±åº¦è¿åŠ¨å…ˆéªŒè‡ªåŠ¨ç­›é€‰é«˜è´¨é‡å…³é”®æŽ©ç ï¼Œé¿å…äº†äººå·¥åˆæˆå¸¦æ¥çš„è¿åŠ¨å¤±çœŸï¼Œä»Žè€Œæ›´å‡†ç¡®åœ°å»ºæ¨¡çœŸå®žè§†é¢‘åŠ¨æ€ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šåˆ©ç”¨æ·±åº¦è¿åŠ¨å…ˆéªŒï¼ˆå¦‚å…‰æµæˆ–è¿åŠ¨ä¼°è®¡ç½‘ç»œï¼‰æ¥è¯„ä¼°æŽ©ç è´¨é‡å¹¶è¯†åˆ«å…³é”®å¸§ï¼›è®¾è®¡Temporal DropLossï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒéƒ¨åˆ†æ—¶é—´æ­¥çš„æŸå¤±ï¼Œä»¥å¢žå¼ºæ¨¡åž‹å¯¹æ—¶é—´å™ªå£°çš„é²æ£’æ€§ï¼›é‡‡ç”¨è’¸é¦æ¡†æž¶ï¼Œå°†ç¨€ç–å…³é”®æŽ©ç çš„çŸ¥è¯†ä¼ é€’åˆ°ç¨ å¯†åˆ†å‰²æ¨¡åž‹ä¸­ï¼Œå…·ä½“å¯èƒ½æ¶‰åŠæ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œç»“æž„æˆ–æŸå¤±å‡½æ•°ä¼˜åŒ–ï¼Œä¾‹å¦‚ä½¿ç”¨äº¤å‰ç†µæŸå¤±æˆ–å¯¹æ¯”å­¦ä¹ æŸå¤±æ¥å¯¹é½æŽ©ç é¢„æµ‹ã€‚å‚æ•°è®¾ç½®å¯èƒ½åŒ…æ‹¬å…³é”®æŽ©ç çš„é€‰æ‹©é˜ˆå€¼ã€DropLossçš„ä¸¢å¼ƒçŽ‡ä»¥åŠè®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼Œä½†å…·ä½“ç»†èŠ‚åœ¨æ‘˜è¦ä¸­æœªè¯¦ç»†è¯´æ˜Žï¼Œéœ€å‚è€ƒå®Œæ•´è®ºæ–‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒS2Dæ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²æ¨¡åž‹ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªåœ¨æ‘˜è¦ä¸­æä¾›ï¼Œä½†æ ¹æ®è®ºæ–‡æè¿°ï¼Œå®ƒåœ¨å„ç§æŒ‡æ ‡ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œä¾‹å¦‚å¯èƒ½åŒ…æ‹¬æ›´é«˜çš„åˆ†å‰²ç²¾åº¦ï¼ˆå¦‚mAPï¼‰å’Œæ›´å¥½çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æå‡å¹…åº¦ä½“çŽ°åœ¨å¯¹çœŸå®žè§†é¢‘è¿åŠ¨çš„æ›´å‡†ç¡®å»ºæ¨¡ï¼Œå‡å°‘äº†åˆæˆæ•°æ®å¸¦æ¥çš„åå·®ï¼Œä»Žè€Œåœ¨å¤æ‚åœºæ™¯ä¸‹å®žçŽ°æ›´é²æ£’çš„åˆ†å‰²ç»“æžœã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯ç”¨äºŽè§†é¢‘ç›‘æŽ§ã€è‡ªåŠ¨é©¾é©¶ã€å¢žå¼ºçŽ°å®žå’Œè§†é¢‘ç¼–è¾‘ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æ— ç›‘ç£è§†é¢‘å®žä¾‹åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œå®ƒèƒ½å¤Ÿå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œé™ä½Žåº”ç”¨æˆæœ¬ï¼Œå¹¶æŽ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨åŠ¨æ€çŽ¯å¢ƒä¸­çš„å®žæ—¶ç†è§£å’Œäº¤äº’èƒ½åŠ›ã€‚æœªæ¥å¯èƒ½å½±å“è§†é¢‘åˆ†æžæŠ€æœ¯çš„å‘å±•ï¼Œä¿ƒè¿›æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–å¤„ç†å·¥å…·ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.

