---
layout: default
title: MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion
---

# MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13177" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13177</a>
  <a href="https://arxiv.org/pdf/2512.13177.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13177" onclick="toggleFavorite(this, '2512.13177', 'MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minghui Hou, Wei-Hsing Huang, Shaofeng Liang, Daizong Liu, Tai-Hao Wen, Gang Wang, Runwei Guan, Weiping Ding

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MMDriveï¼šæå‡ºå¤šæ¨¡æ€èåˆçš„äº¤äº’å¼åœºæ™¯ç†è§£æ¡†æ¶ï¼Œè¶…è¶Šè§†è§‰é™åˆ¶ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `è‡ªåŠ¨é©¾é©¶` `åœºæ™¯ç†è§£` `è§†è§‰-è¯­è¨€æ¨¡å‹` `3Dåœºæ™¯ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹å—é™äº2Då›¾åƒç†è§£ï¼Œç¼ºä¹3Dç©ºé—´æ„ŸçŸ¥å’Œæ·±åº¦è¯­ä¹‰èåˆèƒ½åŠ›ï¼Œå¯¼è‡´åœ¨å¤æ‚è‡ªåŠ¨é©¾é©¶ç¯å¢ƒä¸­è¡¨ç°æ¬ ä½³ã€‚
2. MMDriveé€šè¿‡èåˆå ç”¨åœ°å›¾ã€æ¿€å…‰é›·è¾¾ç‚¹äº‘å’Œæ–‡æœ¬æè¿°ï¼Œå¹¶è®¾è®¡è‡ªé€‚åº”è·¨æ¨¡æ€èåˆå’Œå…³é”®ä¿¡æ¯æå–æ¨¡å—ï¼Œå®ç°3Dåœºæ™¯ç†è§£ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMMDriveåœ¨DriveLMå’ŒNuScenes-QAåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒBLEU-4æå‡è‡³54.56ï¼ŒNuScenes-QAå‡†ç¡®ç‡è¾¾62.7%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†MMDriveï¼Œä¸€ä¸ªå¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨å°†ä¼ ç»Ÿçš„2Då›¾åƒç†è§£æ‰©å±•åˆ°å¹¿ä¹‰çš„3Dåœºæ™¯ç†è§£ã€‚MMDriveèåˆäº†å ç”¨åœ°å›¾ã€æ¿€å…‰é›·è¾¾ç‚¹äº‘å’Œæ–‡æœ¬åœºæ™¯æè¿°ä¸‰ç§äº’è¡¥æ¨¡æ€çš„ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸¤ä¸ªæ–°é¢–çš„ç»„ä»¶ï¼Œç”¨äºè‡ªé€‚åº”è·¨æ¨¡æ€èåˆå’Œå…³é”®ä¿¡æ¯æå–ã€‚å…·ä½“æ¥è¯´ï¼ŒText-oriented Multimodal ModulatoråŸºäºé—®é¢˜ä¸­çš„è¯­ä¹‰çº¿ç´¢åŠ¨æ€åœ°åŠ æƒæ¯ä¸ªæ¨¡æ€çš„è´¡çŒ®ï¼Œä»è€ŒæŒ‡å¯¼ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç‰¹å¾é›†æˆã€‚Cross-Modal Abstractoré‡‡ç”¨å¯å­¦ä¹ çš„æŠ½è±¡tokenæ¥ç”Ÿæˆç´§å‡‘çš„è·¨æ¨¡æ€æ‘˜è¦ï¼Œçªå‡ºå…³é”®åŒºåŸŸå’Œé‡è¦è¯­ä¹‰ã€‚åœ¨DriveLMå’ŒNuScenes-QAåŸºå‡†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒMMDriveåœ¨è‡ªåŠ¨é©¾é©¶çš„è§†è§‰-è¯­è¨€æ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨DriveLMä¸ŠBLEU-4å¾—åˆ†ä¸º54.56ï¼ŒMETEORå¾—åˆ†ä¸º41.78ï¼Œåœ¨NuScenes-QAä¸Šå‡†ç¡®ç‡å¾—åˆ†ä¸º62.7%ã€‚MMDriveæœ‰æ•ˆåœ°æ‰“ç ´äº†ä¼ ç»Ÿä»…ä¾èµ–å›¾åƒç†è§£çš„éšœç¢ï¼Œå®ç°äº†å¤æ‚é©¾é©¶ç¯å¢ƒä¸­å¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†ï¼Œå¹¶ä¸ºå¯è§£é‡Šçš„è‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£æä¾›äº†æ–°çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­ï¼Œä¸»è¦ä¾èµ–2Då›¾åƒè¿›è¡Œç†è§£å’Œæ¨ç†ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨3Dç©ºé—´ä¿¡æ¯ï¼Œå¯¼è‡´å¯¹å¤æ‚äº¤é€šçŠ¶å†µçš„ç†è§£å­˜åœ¨å±€é™æ€§ã€‚ç—›ç‚¹åœ¨äºç¼ºä¹æœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆæœºåˆ¶ï¼Œæ— æ³•å°†è§†è§‰ä¿¡æ¯ä¸æ¿€å…‰é›·è¾¾ã€åœ°å›¾ç­‰ä¿¡æ¯è¿›è¡Œæ·±åº¦æ•´åˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMMDriveçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä¼ ç»Ÿçš„2Då›¾åƒç†è§£æ‰©å±•åˆ°3Dåœºæ™¯ç†è§£ï¼Œé€šè¿‡èåˆå¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼ˆå ç”¨åœ°å›¾ã€æ¿€å…‰é›·è¾¾ç‚¹äº‘ã€æ–‡æœ¬æè¿°ï¼‰æ¥æå‡æ¨¡å‹å¯¹å¤æ‚é©¾é©¶ç¯å¢ƒçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”è·¨æ¨¡æ€èåˆå’Œå…³é”®ä¿¡æ¯æå–æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€åœ°è°ƒæ•´ä¸åŒæ¨¡æ€çš„æƒé‡ï¼Œå¹¶æå–å…³é”®ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMDriveæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šå¤šæ¨¡æ€è¾“å…¥ç¼–ç å™¨ã€Text-oriented Multimodal Modulatorå’ŒCross-Modal Abstractorã€‚é¦–å…ˆï¼Œå¤šæ¨¡æ€è¾“å…¥ç¼–ç å™¨å°†å ç”¨åœ°å›¾ã€æ¿€å…‰é›·è¾¾ç‚¹äº‘å’Œæ–‡æœ¬æè¿°åˆ†åˆ«ç¼–ç æˆç‰¹å¾å‘é‡ã€‚ç„¶åï¼ŒText-oriented Multimodal Modulatoræ ¹æ®æ–‡æœ¬æè¿°ä¸­çš„è¯­ä¹‰çº¿ç´¢ï¼ŒåŠ¨æ€åœ°è°ƒæ•´ä¸åŒæ¨¡æ€ç‰¹å¾çš„æƒé‡ï¼Œå®ç°è‡ªé€‚åº”è·¨æ¨¡æ€èåˆã€‚æœ€åï¼ŒCross-Modal Abstractoræå–èåˆåçš„ç‰¹å¾ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç´§å‡‘çš„è·¨æ¨¡æ€æ‘˜è¦ï¼Œç”¨äºåç»­çš„æ¨ç†å’Œé—®ç­”ã€‚

**å…³é”®åˆ›æ–°**ï¼šMMDriveçš„å…³é”®åˆ›æ–°åœ¨äºText-oriented Multimodal Modulatorå’ŒCross-Modal Abstractorçš„è®¾è®¡ã€‚Text-oriented Multimodal Modulatorèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°åŠ¨æ€è°ƒæ•´ä¸åŒæ¨¡æ€çš„æƒé‡ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç‰¹å¾èåˆï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­é™æ€èåˆçš„å±€é™æ€§ã€‚Cross-Modal Abstractoré€šè¿‡å¯å­¦ä¹ çš„æŠ½è±¡tokenæå–å…³é”®ä¿¡æ¯ï¼Œå‡å°‘äº†å†—ä½™ä¿¡æ¯å¯¹æ¨ç†çš„å½±å“ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMMDriveèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼Œæå‡äº†å¯¹å¤æ‚é©¾é©¶åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šText-oriented Multimodal Modulatorä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®æ–‡æœ¬æè¿°çš„åµŒå…¥å‘é‡è®¡ç®—æ¯ä¸ªæ¨¡æ€çš„æƒé‡ã€‚Cross-Modal Abstractorä½¿ç”¨Transformerç»“æ„ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æŠ½è±¡tokenä¸å¤šæ¨¡æ€ç‰¹å¾è¿›è¡Œäº¤äº’ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é—®ç­”æŸå¤±å’Œå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œè·¨æ¨¡æ€è¡¨ç¤ºèƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13177/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13177/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13177/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

MMDriveåœ¨DriveLMå’ŒNuScenes-QAä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨DriveLMä¸Šï¼ŒMMDriveçš„BLEU-4å¾—åˆ†è¾¾åˆ°54.56ï¼ŒMETEORå¾—åˆ†è¾¾åˆ°41.78ï¼Œç›¸è¾ƒäºç°æœ‰æœ€ä½³æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚åœ¨NuScenes-QAä¸Šï¼ŒMMDriveçš„å‡†ç¡®ç‡è¾¾åˆ°62.7%ï¼ŒåŒæ ·ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMMDriveèƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œæå‡å¯¹å¤æ‚é©¾é©¶åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MMDriveå¯åº”ç”¨äºé«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»Ÿï¼ˆADASï¼‰å’Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼Œæå‡è½¦è¾†å¯¹å¤æ‚äº¤é€šåœºæ™¯çš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚è¯¥ç ”ç©¶è¿˜å¯æ‰©å±•åˆ°å…¶ä»–éœ€è¦å¤šæ¨¡æ€ä¿¡æ¯èåˆçš„æœºå™¨äººåº”ç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½å·¡æ£€æœºå™¨äººã€æ™ºèƒ½å®¶å±…ç­‰ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

