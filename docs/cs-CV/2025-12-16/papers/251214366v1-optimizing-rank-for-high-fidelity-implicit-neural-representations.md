---
layout: default
title: Optimizing Rank for High-Fidelity Implicit Neural Representations
---

# Optimizing Rank for High-Fidelity Implicit Neural Representations

**arXiv**: [2512.14366v1](https://arxiv.org/abs/2512.14366) | [PDF](https://arxiv.org/pdf/2512.14366.pdf)

**ä½œè€…**: Julian McGinnis, Florian A. HÃ¶lzl, Suprosanna Shit, Florentin Bieder, Paul Friedrich, Mark MÃ¼hlau, BjÃ¶rn Menze, Daniel Rueckert, Benedikt Wiestler

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ä¼˜åŒ–ç§©æ¥æå‡é«˜ä¿çœŸéšå¼ç¥žç»è¡¨ç¤ºçš„æ€§èƒ½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **3Dæ„ŸçŸ¥ä¸ŽçŠ¶æ€ä¼°è®¡ (Perception & State Est)**

**å…³é”®è¯**: `éšå¼ç¥žç»è¡¨ç¤º` `å¤šå±‚æ„ŸçŸ¥æœº` `ç§©ä¼˜åŒ–` `é«˜é¢‘ä¿¡æ¯` `æ–°è§†è§’åˆæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽMLPçš„INRæ–¹æ³•éš¾ä»¥è¡¨ç¤ºé«˜é¢‘å†…å®¹ï¼Œé€šå¸¸éœ€è¦å¤æ‚çš„æž¶æž„è®¾è®¡ã€‚
2. è®ºæ–‡æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è°ƒèŠ‚ç½‘ç»œç§©æ¥æ”¹å–„INRçš„æ€§èƒ½ï¼Œé¿å…ç§©é€€åŒ–é—®é¢˜ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œä½¿ç”¨é«˜ç§©ä¼˜åŒ–å™¨ï¼ˆå¦‚Muonï¼‰èƒ½æ˜¾è‘—æå‡INRåœ¨å›¾åƒå’Œæ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šçš„è¡¨çŽ°ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽæ™®é€šå¤šå±‚æ„ŸçŸ¥æœº(MLP)çš„éšå¼ç¥žç»è¡¨ç¤º(INR)é€šå¸¸è¢«è®¤ä¸ºæ— æ³•è¡¨ç¤ºé«˜é¢‘å†…å®¹ã€‚å› æ­¤ï¼Œç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æž¶æž„å¹²é¢„ä¸Šï¼Œä¾‹å¦‚åæ ‡åµŒå…¥æˆ–ä¸“é—¨çš„æ¿€æ´»å‡½æ•°ï¼Œä»¥è¡¨ç¤ºé«˜é¢‘ä¿¡å·ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†æ™®é€šMLPçš„ä½Žé¢‘åç½®æ˜¯å­¦ä¹ é«˜é¢‘å†…å®¹çš„å†…åœ¨æž¶æž„é™åˆ¶çš„è§‚ç‚¹ï¼Œè€Œæ˜¯è®­ç»ƒæœŸé—´ç¨³å®šç§©é€€åŒ–çš„ç—‡çŠ¶ã€‚æˆ‘ä»¬é€šè¿‡å®žéªŒè¯æ˜Žï¼Œåœ¨è®­ç»ƒæœŸé—´è°ƒèŠ‚ç½‘ç»œçš„ç§©å¯ä»¥æ˜¾è‘—æé«˜å­¦ä¹ ä¿¡å·çš„ä¿çœŸåº¦ï¼Œä»Žè€Œä½¿ç®€å•çš„MLPæž¶æž„ä¹Ÿå…·æœ‰è¡¨è¾¾èƒ½åŠ›ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œä½¿ç”¨åƒMuonè¿™æ ·çš„ä¼˜åŒ–å™¨ï¼Œå…·æœ‰é«˜ç§©ã€è¿‘ä¹Žæ­£äº¤çš„æ›´æ–°ï¼Œå¯ä»¥æŒç»­å¢žå¼ºINRæž¶æž„ï¼Œç”šè‡³è¶…è¶Šç®€å•çš„ReLU MLPã€‚è¿™äº›æ˜¾è‘—çš„æ”¹è¿›é€‚ç”¨äºŽå„ç§é¢†åŸŸï¼ŒåŒ…æ‹¬è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒï¼Œä»¥åŠæ–°è§†è§’åˆæˆï¼Œä¸Žä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼ŒPSNRæé«˜äº†é«˜è¾¾9 dBã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼ŒåŒ…æ‹¬ä»£ç å’Œå®žéªŒç»“æžœï¼Œå¯åœ¨(https://muon-inrs.github.io)ä¸Šæ‰¾åˆ°ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„éšå¼ç¥žç»è¡¨ç¤ºæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºŽMLPçš„INRï¼Œåœ¨è¡¨ç¤ºé«˜é¢‘ç»†èŠ‚æ—¶å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚ä¼ ç»Ÿçš„è§£å†³æ–¹æ¡ˆä¾§é‡äºŽä¿®æ”¹ç½‘ç»œç»“æž„æˆ–å¼•å…¥åæ ‡ç¼–ç ï¼Œä½†è¿™äº›æ–¹æ³•å¢žåŠ äº†æ¨¡åž‹çš„å¤æ‚æ€§ï¼Œå¹¶ä¸”å¯èƒ½å¼•å…¥å…¶ä»–é—®é¢˜ã€‚è®ºæ–‡æ—¨åœ¨è§£å†³INRåœ¨é«˜é¢‘ä¿¡æ¯è¡¨ç¤ºä¸Šçš„ç“¶é¢ˆï¼Œå¹¶æŽ¢ç´¢æ›´ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¤ä¸ºMLPçš„ä½Žé¢‘åç½®å¹¶éžå†…åœ¨é™åˆ¶ï¼Œè€Œæ˜¯ç”±äºŽè®­ç»ƒè¿‡ç¨‹ä¸­ç½‘ç»œç§©çš„é€€åŒ–é€ æˆçš„ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»´æŒç½‘ç»œçš„ç§©ï¼Œå¯ä»¥æé«˜å…¶è¡¨ç¤ºé«˜é¢‘ä¿¡æ¯çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡å…³æ³¨ä¼˜åŒ–å™¨çš„é€‰æ‹©ï¼Œå¹¶æå‡ºä½¿ç”¨èƒ½å¤Ÿäº§ç”Ÿé«˜ç§©æ›´æ–°çš„ä¼˜åŒ–å™¨ï¼Œå¦‚Muonã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè®ºæ–‡æ²¡æœ‰æå‡ºå…¨æ–°çš„ç½‘ç»œæž¶æž„ï¼Œè€Œæ˜¯ä¸“æ³¨äºŽä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚æ•´ä½“æ¡†æž¶ä»ç„¶æ˜¯æ ‡å‡†çš„INRè®­ç»ƒæµç¨‹ï¼šè¾“å…¥åæ ‡ï¼Œé€šè¿‡MLPé¢„æµ‹åƒç´ å€¼æˆ–ä½“ç´ å¯†åº¦ç­‰ï¼Œç„¶åŽè®¡ç®—æŸå¤±å‡½æ•°å¹¶åå‘ä¼ æ’­ã€‚å…³é”®åœ¨äºŽä¼˜åŒ–å™¨çš„é€‰æ‹©å’Œä½¿ç”¨ï¼Œä»¥åŠå¯èƒ½å­˜åœ¨çš„ç§©æ­£åˆ™åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽå°†INRçš„æ€§èƒ½ç“¶é¢ˆå½’å› äºŽè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç§©é€€åŒ–ï¼Œå¹¶æå‡ºé€šè¿‡ä¼˜åŒ–å™¨é€‰æ‹©æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸Žä»¥å¾€å…³æ³¨ç½‘ç»œæž¶æž„æ”¹è¿›çš„æ–¹æ³•ä¸åŒï¼Œè¯¥è®ºæ–‡ä»Žä¼˜åŒ–è§’åº¦å…¥æ‰‹ï¼Œä¸ºæå‡INRæ€§èƒ½æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åœ¨äºŽä½¿ç”¨Muonä¼˜åŒ–å™¨ï¼Œè¯¥ä¼˜åŒ–å™¨å…·æœ‰é«˜ç§©å’Œè¿‘ä¹Žæ­£äº¤çš„æ›´æ–°ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¯èƒ½è¿˜æŽ¢ç´¢äº†å…¶ä»–ç§©æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚è°±èŒƒæ•°æ­£åˆ™åŒ–ã€‚æŸå¤±å‡½æ•°é€šå¸¸é‡‡ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æˆ–L1æŸå¤±ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å¯ä»¥æ˜¯ç®€å•çš„ReLU MLPï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶ä»–æ›´å¤æ‚çš„å˜ä½“ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œä½¿ç”¨Muonä¼˜åŒ–å™¨å¯ä»¥æ˜¾è‘—æé«˜INRçš„æ€§èƒ½ï¼Œåœ¨è‡ªç„¶å›¾åƒã€åŒ»å­¦å›¾åƒå’Œæ–°è§†è§’åˆæˆç­‰ä»»åŠ¡ä¸Šï¼ŒPSNRæŒ‡æ ‡æå‡é«˜è¾¾9dBï¼Œè¶…è¶Šäº†ä¹‹å‰çš„state-of-the-artæ–¹æ³•ã€‚è¿™è¡¨æ˜Žé€šè¿‡ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œå³ä½¿æ˜¯ç®€å•çš„MLPæž¶æž„ä¹Ÿèƒ½å®žçŽ°é«˜ä¿çœŸåº¦çš„éšå¼ç¥žç»è¡¨ç¤ºã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽè®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦é¢†åŸŸï¼Œä¾‹å¦‚å›¾åƒé‡å»ºã€åŒ»å­¦å›¾åƒåˆ†æžã€ä¸‰ç»´é‡å»ºã€æ–°è§†è§’åˆæˆç­‰ã€‚é€šè¿‡æå‡INRçš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¯ä»¥æé«˜ç›¸å…³ä»»åŠ¡çš„ç²¾åº¦å’Œæ•ˆçŽ‡ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—å½±åƒé¢†åŸŸï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°é‡å»ºé«˜åˆ†è¾¨çŽ‡çš„åŒ»å­¦å›¾åƒï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).

