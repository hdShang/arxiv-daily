---
layout: default
title: Optimizing Rank for High-Fidelity Implicit Neural Representations
---

# Optimizing Rank for High-Fidelity Implicit Neural Representations

**arXiv**: [2512.14366v1](https://arxiv.org/abs/2512.14366) | [PDF](https://arxiv.org/pdf/2512.14366.pdf)

**ä½œè€…**: Julian McGinnis, Florian A. HÃ¶lzl, Suprosanna Shit, Florentin Bieder, Paul Friedrich, Mark MÃ¼hlau, BjÃ¶rn Menze, Daniel Rueckert, Benedikt Wiestler

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡ä¼˜åŒ–ç½‘ç»œç§©æ¥æå‡éšå¼ç¥žç»è¡¨ç¤ºçš„é«˜é¢‘ä¿¡å·ä¿çœŸåº¦ï¼ŒæŒ‘æˆ˜ä¼ ç»Ÿæž¶æž„é™åˆ¶è§‚ç‚¹ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `éšå¼ç¥žç»è¡¨ç¤º` `ç½‘ç»œç§©ä¼˜åŒ–` `é«˜é¢‘ä¿¡å·å­¦ä¹ ` `å¤šå±‚æ„ŸçŸ¥æœº` `Muonä¼˜åŒ–å™¨` `å›¾åƒé‡å»º` `æ–°è§†è§’åˆæˆ` `åŒ»å­¦å›¾åƒå¤„ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰éšå¼ç¥žç»è¡¨ç¤ºï¼ˆINRsï¼‰ä¸­ï¼Œæ™®é€šMLPsè¢«è®¤ä¸ºæ— æ³•æœ‰æ•ˆè¡¨ç¤ºé«˜é¢‘ä¿¡å·ï¼Œå¯¼è‡´ç ”ç©¶è¿‡åº¦ä¾èµ–å¤æ‚æž¶æž„å¹²é¢„ã€‚
2. è®ºæ–‡æå‡ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šç§©é€€åŒ–æ˜¯ä½Žé¢‘åå·®çš„æ ¹æœ¬åŽŸå› ï¼Œé€šè¿‡ä¼˜åŒ–ç½‘ç»œç§©æ¥æå‡ä¿¡å·ä¿çœŸåº¦ã€‚
3. å®žéªŒæ˜¾ç¤ºï¼Œä½¿ç”¨é«˜ç§©ä¼˜åŒ–å™¨å¦‚Muonï¼Œåœ¨å¤šä¸ªé¢†åŸŸå®žçŽ°é«˜è¾¾9 dB PSNRæå‡ï¼Œæ˜¾è‘—è¶…è¶ŠçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽæ™®é€šå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰çš„éšå¼ç¥žç»è¡¨ç¤ºï¼ˆINRsï¼‰è¢«å¹¿æ³›è®¤ä¸ºæ— æ³•è¡¨ç¤ºé«˜é¢‘å†…å®¹ï¼Œè¿™å¯¼è‡´ç ”ç©¶è½¬å‘åæ ‡åµŒå…¥æˆ–ç‰¹æ®Šæ¿€æ´»å‡½æ•°ç­‰æž¶æž„å¹²é¢„ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†æ™®é€šMLPsçš„ä½Žé¢‘åå·®æ˜¯å­¦ä¹ é«˜é¢‘å†…å®¹çš„å†…åœ¨æž¶æž„é™åˆ¶è¿™ä¸€è§‚ç‚¹ï¼Œè®¤ä¸ºè¿™æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­ç¨³å®šç§©é€€åŒ–çš„ç—‡çŠ¶ã€‚æˆ‘ä»¬é€šè¿‡å®žéªŒè¯æ˜Žï¼Œåœ¨è®­ç»ƒæœŸé—´è°ƒèŠ‚ç½‘ç»œç§©èƒ½æ˜¾è‘—æé«˜å­¦ä¹ ä¿¡å·çš„ä¿çœŸåº¦ï¼Œä½¿ç®€å•çš„MLPæž¶æž„ä¹Ÿå…·æœ‰è¡¨è¾¾åŠ›ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œä½¿ç”¨å¦‚Muonç­‰å…·æœ‰é«˜ç§©ã€è¿‘æ­£äº¤æ›´æ–°çš„ä¼˜åŒ–å™¨ï¼Œèƒ½æŒç»­å¢žå¼ºINRæž¶æž„ï¼Œç”šè‡³è¶…è¶Šç®€å•çš„ReLU MLPsã€‚è¿™äº›æ˜¾è‘—æ”¹è¿›é€‚ç”¨äºŽå¤šç§é¢†åŸŸï¼ŒåŒ…æ‹¬è‡ªç„¶å’ŒåŒ»å­¦å›¾åƒä»¥åŠæ–°è§†è§’åˆæˆï¼Œç›¸æ¯”å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒPSNRæå‡é«˜è¾¾9 dBã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢åŒ…å«ä»£ç å’Œå®žéªŒç»“æžœï¼Œå¯åœ¨https://muon-inrs.github.ioè®¿é—®ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šéšå¼ç¥žç»è¡¨ç¤ºï¼ˆINRsï¼‰ä¸­ï¼Œæ™®é€šå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰è¢«å¹¿æ³›è®¤ä¸ºå­˜åœ¨ä½Žé¢‘åå·®ï¼Œæ— æ³•æœ‰æ•ˆå­¦ä¹ é«˜é¢‘ä¿¡å·ï¼Œå¯¼è‡´çŽ°æœ‰æ–¹æ³•ä¾èµ–åæ ‡åµŒå…¥æˆ–ç‰¹æ®Šæ¿€æ´»å‡½æ•°ç­‰å¤æ‚æž¶æž„å¹²é¢„ï¼Œå¢žåŠ äº†æ¨¡åž‹å¤æ‚æ€§å’Œè®¡ç®—æˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æŒ‘æˆ˜äº†ä½Žé¢‘åå·®æ˜¯MLPså†…åœ¨æž¶æž„é™åˆ¶çš„è§‚ç‚¹ï¼Œæå‡ºè¿™æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­ç¨³å®šç§©é€€åŒ–çš„ç—‡çŠ¶ã€‚é€šè¿‡è°ƒèŠ‚ç½‘ç»œç§©ï¼Œå¯ä»¥ç¼“è§£é€€åŒ–é—®é¢˜ï¼Œä½¿ç®€å•MLPæž¶æž„ä¹Ÿèƒ½è¡¨è¾¾é«˜é¢‘å†…å®¹ï¼Œä»Žè€Œé¿å…ä¸å¿…è¦çš„æž¶æž„ä¿®æ”¹ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬ä½¿ç”¨æ™®é€šMLPä½œä¸ºåŸºç¡€æž¶æž„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥ç§©ä¼˜åŒ–æœºåˆ¶ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬ç½‘ç»œåˆå§‹åŒ–ã€è®­ç»ƒå¾ªçŽ¯å’Œç§©ç›‘æŽ§ï¼Œé€šè¿‡ä¼˜åŒ–å™¨ï¼ˆå¦‚Muonï¼‰å®žçŽ°é«˜ç§©ã€è¿‘æ­£äº¤çš„æƒé‡æ›´æ–°ï¼Œä»¥ç»´æŒç½‘ç»œè¡¨è¾¾èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯å°†é«˜é¢‘ä¿¡å·å­¦ä¹ é—®é¢˜ä»Žæž¶æž„é™åˆ¶é‡æ–°å®šä¹‰ä¸ºç§©ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡è®­ç»ƒåŠ¨æ€è°ƒèŠ‚ç§©æ¥æå‡ä¿çœŸåº¦ï¼Œä¸ŽçŽ°æœ‰æ–¹æ³•æœ¬è´¨åŒºåˆ«åœ¨äºŽé¿å…å¤æ‚æž¶æž„å¹²é¢„ï¼Œä¸“æ³¨äºŽä¼˜åŒ–è¿‡ç¨‹æœ¬èº«ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®å‚æ•°è®¾ç½®åŒ…æ‹¬ä½¿ç”¨Muonä¼˜åŒ–å™¨ï¼Œå…¶è®¾è®¡ç¡®ä¿æƒé‡æ›´æ–°å…·æœ‰é«˜ç§©å’Œè¿‘æ­£äº¤æ€§ï¼›æŸå¤±å‡½æ•°é€šå¸¸åŸºäºŽä¿¡å·é‡å»ºè¯¯å·®ï¼ˆå¦‚å‡æ–¹è¯¯å·®ï¼‰ï¼›ç½‘ç»œç»“æž„ä¸ºç®€å•ReLU MLPsï¼Œæ— é¢å¤–åµŒå…¥æˆ–æ¿€æ´»å‡½æ•°ï¼Œå¼ºè°ƒç§©è°ƒèŠ‚çš„æ ¸å¿ƒä½œç”¨ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒåœ¨è‡ªç„¶å›¾åƒã€åŒ»å­¦å›¾åƒå’Œæ–°è§†è§’åˆæˆç­‰å¤šä¸ªé¢†åŸŸè¿›è¡Œï¼Œä½¿ç”¨Muonä¼˜åŒ–å™¨è°ƒèŠ‚ç½‘ç»œç§©ï¼Œç›¸æ¯”å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒPSNRæå‡é«˜è¾¾9 dBã€‚ç»“æžœè¡¨æ˜Žï¼Œå³ä½¿ç®€å•ReLU MLPsä¹Ÿèƒ½å®žçŽ°æ˜¾è‘—æ€§èƒ½æ”¹è¿›ï¼ŒéªŒè¯äº†ç§©ä¼˜åŒ–å¯¹æå‡éšå¼ç¥žç»è¡¨ç¤ºä¿çœŸåº¦çš„æœ‰æ•ˆæ€§ï¼Œä¸”æ”¹è¿›å…·æœ‰ä¸€è‡´æ€§å’Œæ™®é€‚æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦å›¾åƒå¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¦‚é«˜ä¿çœŸå›¾åƒé‡å»ºã€æ–°è§†è§’åˆæˆå’ŒåŒ»å­¦å½±åƒåˆ†æžã€‚é€šè¿‡æå‡éšå¼ç¥žç»è¡¨ç¤ºçš„ä¿¡å·ä¿çœŸåº¦ï¼Œå¯ç®€åŒ–æ¨¡åž‹æž¶æž„ï¼Œé™ä½Žè®¡ç®—æˆæœ¬ï¼ŒæŽ¨åŠ¨é«˜æ•ˆç¥žç»è¡¨ç¤ºå­¦ä¹ çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½å½±å“ç”Ÿæˆæ¨¡åž‹ã€3Dé‡å»ºå’Œå®žæ—¶æ¸²æŸ“ç­‰æ–¹å‘ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).

