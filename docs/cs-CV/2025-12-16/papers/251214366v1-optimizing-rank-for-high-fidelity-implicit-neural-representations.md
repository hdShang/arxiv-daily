---
layout: default
title: Optimizing Rank for High-Fidelity Implicit Neural Representations
---

# Optimizing Rank for High-Fidelity Implicit Neural Representations

**arXiv**: [2512.14366v1](https://arxiv.org/abs/2512.14366) | [PDF](https://arxiv.org/pdf/2512.14366.pdf)

**ä½œè€…**: Julian McGinnis, Florian A. HÃ¶lzl, Suprosanna Shit, Florentin Bieder, Paul Friedrich, Mark MÃ¼hlau, BjÃ¶rn Menze, Daniel Rueckert, Benedikt Wiestler

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé€šè¿‡ä¼˜åŒ–ç½‘ç»œç§©æ¥æå‡éšå¼ç¥žç»è¡¨ç¤ºçš„é«˜é¢‘ä¿¡å·ä¿çœŸåº¦ï¼ŒæŒ‘æˆ˜ä¼ ç»Ÿæž¶æž„é™åˆ¶è§‚ç‚¹ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `éšå¼ç¥žç»è¡¨ç¤º` `å¤šå±‚æ„ŸçŸ¥æœº` `ç§©ä¼˜åŒ–` `é«˜é¢‘ä¿¡å·` `å›¾åƒé‡å»º` `æ–°è§†è§’åˆæˆ` `åŒ»å­¦å›¾åƒåˆ†æž` `ä¼˜åŒ–å™¨è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•è®¤ä¸ºæ™®é€šMLPså› æž¶æž„é™åˆ¶æ— æ³•è¡¨ç¤ºé«˜é¢‘ä¿¡å·ï¼Œä¾èµ–åæ ‡åµŒå…¥ç­‰å¹²é¢„ï¼Œä½†æœ¬æ–‡æŒ‡å‡ºè¿™æ˜¯è®­ç»ƒä¸­ç§©é€€åŒ–å¯¼è‡´çš„ç—‡çŠ¶ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡ä¼˜åŒ–å™¨å¦‚Muonè°ƒèŠ‚ç½‘ç»œç§©ï¼Œå®žçŽ°é«˜ç§©ã€è¿‘æ­£äº¤æ›´æ–°ï¼Œä»Žè€Œæå‡INRsçš„é«˜é¢‘ä¿çœŸåº¦ï¼Œæ— éœ€å¤æ‚æž¶æž„ä¿®æ”¹ã€‚
3. å®žéªŒæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨è‡ªç„¶å›¾åƒã€åŒ»å­¦å›¾åƒå’Œæ–°è§†è§’åˆæˆä¸­ï¼ŒPSNRæå‡é«˜è¾¾9 dBï¼Œæ˜¾è‘—è¶…è¶ŠçŽ°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽæ™®é€šå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰çš„éšå¼ç¥žç»è¡¨ç¤ºï¼ˆINRsï¼‰è¢«å¹¿æ³›è®¤ä¸ºæ— æ³•è¡¨ç¤ºé«˜é¢‘å†…å®¹ï¼Œè¿™å¼•å¯¼ç ”ç©¶è½¬å‘åæ ‡åµŒå…¥æˆ–ä¸“ç”¨æ¿€æ´»å‡½æ•°ç­‰æž¶æž„å¹²é¢„ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†æ™®é€šMLPsçš„ä½Žé¢‘åå·®æ˜¯å­¦ä¹ é«˜é¢‘å†…å®¹çš„å†…åœ¨æž¶æž„é™åˆ¶çš„è§‚ç‚¹ï¼Œè®¤ä¸ºè¿™æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­ç¨³å®šç§©é€€åŒ–çš„ç—‡çŠ¶ã€‚æˆ‘ä»¬é€šè¿‡å®žéªŒè¯æ˜Žï¼Œåœ¨è®­ç»ƒæœŸé—´è°ƒèŠ‚ç½‘ç»œç§©èƒ½æ˜¾è‘—æé«˜å­¦ä¹ ä¿¡å·çš„ä¿çœŸåº¦ï¼Œä½¿ç®€å•çš„MLPæž¶æž„ä¹Ÿå…·æœ‰è¡¨è¾¾åŠ›ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œä½¿ç”¨Muonç­‰ä¼˜åŒ–å™¨è¿›è¡Œé«˜ç§©ã€è¿‘æ­£äº¤æ›´æ–°ï¼Œèƒ½æŒç»­å¢žå¼ºINRæž¶æž„ï¼Œç”šè‡³è¶…è¶Šç®€å•çš„ReLU MLPsã€‚è¿™äº›æ˜¾è‘—æ”¹è¿›é€‚ç”¨äºŽå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬è‡ªç„¶å’ŒåŒ»å­¦å›¾åƒä»¥åŠæ–°è§†è§’åˆæˆï¼Œç›¸æ¯”å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒPSNRæå‡é«˜è¾¾9 dBã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢åŒ…å«ä»£ç å’Œå®žéªŒç»“æžœï¼Œå¯åœ¨https://muon-inrs.github.ioè®¿é—®ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è§£å†³éšå¼ç¥žç»è¡¨ç¤ºï¼ˆINRsï¼‰ä¸­æ™®é€šå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰æ— æ³•æœ‰æ•ˆè¡¨ç¤ºé«˜é¢‘ä¿¡å·çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸å½’å› äºŽMLPsçš„æž¶æž„é™åˆ¶ï¼Œä¾èµ–åæ ‡åµŒå…¥æˆ–ä¸“ç”¨æ¿€æ´»å‡½æ•°ç­‰å¹²é¢„ï¼Œä½†è¿™äº›æ–¹æ³•å¯èƒ½å¿½ç•¥è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…åœ¨åŠ¨æ€ï¼Œå¯¼è‡´æ•ˆçŽ‡ä½Žä¸‹æˆ–æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æŒ‘æˆ˜ä¼ ç»Ÿè§‚ç‚¹ï¼Œè®¤ä¸ºMLPsçš„ä½Žé¢‘åå·®ä¸æ˜¯å›ºæœ‰æž¶æž„é™åˆ¶ï¼Œè€Œæ˜¯è®­ç»ƒä¸­ç¨³å®šç§©é€€åŒ–çš„ç—‡çŠ¶ã€‚é€šè¿‡è°ƒèŠ‚ç½‘ç»œç§©ï¼Œå¯ä»¥æ”¹å–„ä¿¡å·ä¿çœŸåº¦ï¼Œä½¿ç®€å•MLPæž¶æž„ä¹Ÿèƒ½è¡¨è¾¾é«˜é¢‘å†…å®¹ã€‚è¿™åŸºäºŽç§©ä¸Žè¡¨ç¤ºèƒ½åŠ›ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œé«˜ç§©æ›´æ–°æœ‰åŠ©äºŽæ•æ‰æ›´ä¸°å¯Œçš„ä¿¡å·ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…æ‹¬ä½¿ç”¨ä¼˜åŒ–å™¨å¦‚Muonè¿›è¡Œè®­ç»ƒï¼Œè¯¥ä¼˜åŒ–å™¨è®¾è®¡ç”¨äºŽç”Ÿæˆé«˜ç§©ã€è¿‘æ­£äº¤çš„æƒé‡æ›´æ–°ã€‚æµç¨‹æ¶‰åŠåˆå§‹åŒ–MLPç½‘ç»œï¼Œåº”ç”¨ç§©è°ƒèŠ‚ç­–ç•¥ï¼Œé€šè¿‡æŸå¤±å‡½æ•°ä¼˜åŒ–ç½‘ç»œå‚æ•°ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°æ€§èƒ½ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬åŸºç¡€MLPæž¶æž„ã€ç§©ä¼˜åŒ–æ¨¡å—å’Œè¯„ä¼°æ¨¡å—ï¼Œæ— éœ€é¢å¤–å¤æ‚ç»„ä»¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯æå‡ºç§©ä¼˜åŒ–ä½œä¸ºæå‡INRsæ€§èƒ½çš„å…³é”®ï¼Œè€Œéžä¾èµ–æž¶æž„ä¿®æ”¹ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼Œå®ƒä»Žè®­ç»ƒåŠ¨æ€è§’åº¦è§£å†³é—®é¢˜ï¼Œè€Œä¸æ˜¯å‡è®¾æž¶æž„ç¼ºé™·ï¼Œä»Žè€Œæä¾›æ›´é€šç”¨å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ä½¿ç”¨Muonä¼˜åŒ–å™¨ï¼Œå®ƒé€šè¿‡é«˜ç§©æ›´æ–°æœºåˆ¶ä¿ƒè¿›æƒé‡çŸ©é˜µçš„ç§©ä¿æŒï¼›ç½‘ç»œç»“æž„åŸºäºŽç®€å•ReLU MLPsï¼Œæ— éœ€åæ ‡åµŒå…¥æˆ–ç‰¹æ®Šæ¿€æ´»å‡½æ•°ï¼›æŸå¤±å‡½æ•°é€šå¸¸é‡‡ç”¨å‡æ–¹è¯¯å·®ç­‰æ ‡å‡†æŒ‡æ ‡ï¼Œå‚æ•°è®¾ç½®é’ˆå¯¹ä¸åŒä»»åŠ¡è°ƒæ•´ï¼Œä»¥ç¡®ä¿ç§©è°ƒèŠ‚çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œä½¿ç”¨ç§©ä¼˜åŒ–æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸæ˜¾è‘—æå‡æ€§èƒ½ï¼šåœ¨è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¸Šï¼ŒPSNRæå‡é«˜è¾¾9 dBï¼›åœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸­ï¼Œä¹Ÿè§‚å¯Ÿåˆ°æ˜Žæ˜¾æ”¹è¿›ã€‚å¯¹æ¯”åŸºçº¿åŒ…æ‹¬ä¼ ç»ŸINRæ–¹æ³•å’Œæœ€å…ˆè¿›æž¶æž„ï¼Œè¯¥æ–¹æ³•åœ¨ç®€å•MLPåŸºç¡€ä¸Šå®žçŽ°è¶…è¶Šï¼ŒéªŒè¯äº†ç§©è°ƒèŠ‚çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦å›¾åƒåˆ†æžé¢†åŸŸå…·æœ‰å¹¿æ³›æ½œåœ¨åº”ç”¨ï¼Œå¦‚é«˜ä¿çœŸå›¾åƒé‡å»ºã€æ–°è§†è§’åˆæˆå’ŒåŒ»å­¦å½±åƒå¢žå¼ºã€‚é€šè¿‡æå‡éšå¼ç¥žç»è¡¨ç¤ºçš„é«˜é¢‘ä¿¡å·ä¿çœŸåº¦ï¼Œå¯æŽ¨åŠ¨3Dé‡å»ºã€è™šæ‹ŸçŽ°å®žå’Œç²¾å‡†åŒ»ç–—ç­‰æŠ€æœ¯çš„å‘å±•ï¼Œæœªæ¥å¯èƒ½å½±å“æ›´å¤æ‚çš„ä¿¡å·è¡¨ç¤ºä»»åŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).

