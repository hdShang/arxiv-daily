---
layout: default
title: CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives
---

# CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14696" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14696</a>
  <a href="https://arxiv.org/pdf/2512.14696.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14696" onclick="toggleFavorite(this, '2512.14696', 'CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan

**åˆ†ç±»**: cs.CV, cs.GR, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CRISPï¼šåŸºäºå•ç›®è§†é¢‘å’Œå¹³é¢åœºæ™¯åŸè¯­çš„æ¥è§¦å¼•å¯¼Real2Simæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `Real2Sim` `å•ç›®è§†é¢‘é‡å»º` `äººä½“-åœºæ™¯äº¤äº’` `å¹³é¢åŸè¯­` `å¼ºåŒ–å­¦ä¹ ` `ç‰©ç†ä»¿çœŸ` `åœºæ™¯é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨äººä½“-åœºæ™¯è”åˆé‡å»ºä¸­å­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆä¾èµ–æ•°æ®å…ˆéªŒå’Œæ— ç‰©ç†çš„ä¼˜åŒ–ï¼Œè¦ä¹ˆé‡å»ºçš„å‡ ä½•ä½“å™ªå£°å¤§ï¼Œå¯¼è‡´äº¤äº’å¼è¿åŠ¨è·Ÿè¸ªå¤±è´¥ã€‚
2. CRISPçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¹³é¢åŸè¯­æ‹Ÿåˆç‚¹äº‘é‡å»ºï¼Œæ¢å¤å‡¸çš„ã€å¹²å‡€çš„ã€å¯ç”¨äºä»¿çœŸçš„åœºæ™¯å‡ ä½•ï¼Œå¹¶åˆ©ç”¨äººä½“-åœºæ™¯æ¥è§¦å»ºæ¨¡æ¥è¡¥å…¨é®æŒ¡åŒºåŸŸã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCRISPåœ¨äººä½“è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†è¿åŠ¨è·Ÿè¸ªå¤±è´¥ç‡ï¼Œå¹¶æé«˜äº†å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿçš„æ•ˆç‡ï¼ŒåŒæ—¶åœ¨çœŸå®è§†é¢‘ä¸­ä¹Ÿè¡¨ç°è‰¯å¥½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

CRISPæ˜¯ä¸€ç§ä»å•ç›®è§†é¢‘ä¸­æ¢å¤å¯æ¨¡æ‹Ÿçš„äººä½“è¿åŠ¨å’Œåœºæ™¯å‡ ä½•ç»“æ„çš„æ–¹æ³•ã€‚ç°æœ‰çš„äººä½“-åœºæ™¯è”åˆé‡å»ºå·¥ä½œä¾èµ–äºæ•°æ®é©±åŠ¨çš„å…ˆéªŒå’Œæ— ç‰©ç†å¼•æ“å‚ä¸çš„è”åˆä¼˜åŒ–ï¼Œæˆ–è€…æ¢å¤çš„å‡ ä½•ç»“æ„å™ªå£°å¤§ï¼Œå¯¼è‡´å¸¦æœ‰åœºæ™¯äº¤äº’çš„è¿åŠ¨è·Ÿè¸ªç­–ç•¥å¤±è´¥ã€‚CRISPçš„å…³é”®åœ¨äºé€šè¿‡æ‹Ÿåˆå¹³é¢åŸè¯­åˆ°åœºæ™¯çš„ç‚¹äº‘é‡å»ºï¼Œæ¢å¤å‡¸çš„ã€å¹²å‡€çš„ã€å¯ç”¨äºä»¿çœŸçš„å‡ ä½•ç»“æ„ï¼Œè¿™é€šè¿‡ä¸€ä¸ªç®€å•çš„æ·±åº¦ã€æ³•çº¿å’Œå…‰æµèšç±»æµç¨‹å®ç°ã€‚ä¸ºäº†é‡å»ºäº¤äº’è¿‡ç¨‹ä¸­å¯èƒ½è¢«é®æŒ¡çš„åœºæ™¯å‡ ä½•ç»“æ„ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†äººä½“-åœºæ™¯æ¥è§¦å»ºæ¨¡ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨äººä½“å§¿åŠ¿æ¥é‡å»ºæ¤…å­è¢«é®æŒ¡çš„åº§ä½ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨äººå½¢æ§åˆ¶å™¨ï¼Œç¡®ä¿äººä½“å’Œåœºæ™¯é‡å»ºåœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ã€‚åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆEMDBã€PROXï¼‰ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è¿åŠ¨è·Ÿè¸ªå¤±è´¥ç‡ä»55.2%é™ä½åˆ°6.9%ï¼ŒåŒæ—¶RLæ¨¡æ‹Ÿååé‡æé«˜äº†43%ã€‚æˆ‘ä»¬è¿˜åœ¨åŒ…æ‹¬éšæ„æ‹æ‘„çš„è§†é¢‘ã€äº’è”ç½‘è§†é¢‘ï¼Œç”šè‡³æ˜¯Soraç”Ÿæˆçš„è§†é¢‘åœ¨å†…çš„çœŸå®è§†é¢‘ä¸­éªŒè¯äº†å®ƒã€‚è¿™è¯æ˜äº†CRISPèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆç‰©ç†ä¸Šæœ‰æ•ˆçš„äººä½“è¿åŠ¨å’Œäº¤äº’ç¯å¢ƒï¼Œæå¤§åœ°æ¨è¿›äº†æœºå™¨äººå’ŒAR/VRçš„real-to-simåº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨ä»å•ç›®è§†é¢‘è¿›è¡Œäººä½“-åœºæ™¯è”åˆé‡å»ºæ—¶ï¼Œè¦ä¹ˆä¾èµ–å¤§é‡æ•°æ®å…ˆéªŒï¼Œç¼ºä¹ç‰©ç†åˆç†æ€§ï¼Œè¦ä¹ˆé‡å»ºçš„åœºæ™¯å‡ ä½•ä½“è´¨é‡å·®ï¼Œæ— æ³•ç›´æ¥ç”¨äºç‰©ç†ä»¿çœŸå’Œæ§åˆ¶ï¼Œå¯¼è‡´äº¤äº’å¼è¿åŠ¨è·Ÿè¸ªå¤±è´¥ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•èƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­é‡å»ºå‡ºé«˜è´¨é‡ã€ç‰©ç†ä¸Šåˆç†ã€å¯ç”¨äºä»¿çœŸçš„ä¸‰ç»´äººä½“å’Œåœºæ™¯æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCRISPçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¹³é¢åŸè¯­æ¥è¡¨ç¤ºåœºæ™¯å‡ ä½•ï¼Œå¹¶é€šè¿‡æ·±åº¦ã€æ³•çº¿å’Œå…‰æµä¿¡æ¯è¿›è¡Œèšç±»ï¼Œä»è€Œå¾—åˆ°å¹²å‡€ã€å‡¸çš„åœºæ™¯è¡¨ç¤ºã€‚åŒæ—¶ï¼Œåˆ©ç”¨äººä½“ä¸åœºæ™¯çš„æ¥è§¦ä¿¡æ¯æ¥æ¨æ–­è¢«é®æŒ¡çš„åœºæ™¯éƒ¨åˆ†ï¼Œä¾‹å¦‚é€šè¿‡äººä½“ååœ¨æ¤…å­ä¸Šçš„å§¿åŠ¿æ¥æ¨æ–­æ¤…å­åº§ä½çš„å½¢çŠ¶ã€‚æœ€åï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒäººå½¢æ§åˆ¶å™¨ï¼Œç¡®ä¿é‡å»ºçš„äººä½“å’Œåœºæ™¯åœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCRISPçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä»å•ç›®è§†é¢‘ä¸­é‡å»ºç‚¹äº‘ï¼›2) å¯¹ç‚¹äº‘è¿›è¡Œå¹³é¢åŸè¯­æ‹Ÿåˆï¼Œå¾—åˆ°åœºæ™¯çš„å‡ ä½•è¡¨ç¤ºï¼›3) åˆ©ç”¨äººä½“å§¿åŠ¿å’Œæ¥è§¦ä¿¡æ¯è¡¥å…¨è¢«é®æŒ¡çš„åœºæ™¯åŒºåŸŸï¼›4) ä½¿ç”¨é‡å»ºçš„äººä½“å’Œåœºæ™¯è®­ç»ƒäººå½¢æ§åˆ¶å™¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¿è¯ç‰©ç†åˆç†æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCRISPçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ä½¿ç”¨å¹³é¢åŸè¯­æ¥è¡¨ç¤ºåœºæ™¯å‡ ä½•ï¼Œç®€åŒ–äº†åœºæ™¯é‡å»ºçš„å¤æ‚åº¦ï¼Œå¹¶ä¿è¯äº†é‡å»ºç»“æœçš„å‡¸æ€§å’Œå¯ä»¿çœŸæ€§ï¼›2) åˆ©ç”¨äººä½“-åœºæ™¯æ¥è§¦ä¿¡æ¯æ¥è¡¥å…¨è¢«é®æŒ¡çš„åœºæ™¯åŒºåŸŸï¼Œæé«˜äº†åœºæ™¯é‡å»ºçš„å®Œæ•´æ€§ï¼›3) é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥ä¿è¯é‡å»ºçš„äººä½“å’Œåœºæ™¯åœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ï¼Œä½¿å¾—é‡å»ºç»“æœå¯ä»¥ç›´æ¥ç”¨äºç‰©ç†ä»¿çœŸå’Œæ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šCRISPçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨åŸºäºæ·±åº¦ã€æ³•çº¿å’Œå…‰æµçš„èšç±»ç®—æ³•æ¥æ‹Ÿåˆå¹³é¢åŸè¯­ï¼›2) è®¾è®¡äº†åŸºäºäººä½“å§¿åŠ¿çš„æ¥è§¦æ¨¡å‹æ¥æ¨æ–­è¢«é®æŒ¡çš„åœºæ™¯åŒºåŸŸï¼›3) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒäººå½¢æ§åˆ¶å™¨ï¼Œç›®æ ‡æ˜¯ä½¿æ§åˆ¶å™¨èƒ½å¤Ÿåœ¨é‡å»ºçš„åœºæ™¯ä¸­ç¨³å®šè¡Œèµ°å’Œäº¤äº’ï¼ŒæŸå¤±å‡½æ•°åŒ…æ‹¬å¹³è¡¡æŸå¤±ã€è¿åŠ¨æŸå¤±å’Œæ¥è§¦æŸå¤±ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CRISPåœ¨ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆEMDBã€PROXï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¿åŠ¨è·Ÿè¸ªå¤±è´¥ç‡ä»55.2%é™ä½åˆ°6.9%ï¼ŒåŒæ—¶RLæ¨¡æ‹Ÿååé‡æé«˜äº†43%ã€‚æ­¤å¤–ï¼ŒCRISPåœ¨çœŸå®ä¸–ç•Œçš„è§†é¢‘ï¼ˆåŒ…æ‹¬éšæ„æ‹æ‘„çš„è§†é¢‘ã€äº’è”ç½‘è§†é¢‘ï¼Œç”šè‡³æ˜¯Soraç”Ÿæˆçš„è§†é¢‘ï¼‰ä¸­ä¹Ÿè¡¨ç°å‡ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CRISPåœ¨æœºå™¨äººã€å¢å¼ºç°å®ï¼ˆARï¼‰å’Œè™šæ‹Ÿç°å®ï¼ˆVRï¼‰ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºé€¼çœŸçš„è™šæ‹Ÿç¯å¢ƒï¼Œç”¨äºè®­ç»ƒæœºå™¨äººæˆ–è¿›è¡Œè™šæ‹Ÿäº¤äº’ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨CRISPä»çœŸå®è§†é¢‘ä¸­é‡å»ºå‡ºæˆ¿é—´åœºæ™¯ï¼Œç„¶åè®©æœºå™¨äººåœ¨è™šæ‹Ÿç¯å¢ƒä¸­å­¦ä¹ å¦‚ä½•åœ¨æˆ¿é—´ä¸­å¯¼èˆªå’Œæ“ä½œç‰©ä½“ã€‚æ­¤å¤–ï¼ŒCRISPè¿˜å¯ä»¥ç”¨äºAR/VRåº”ç”¨ä¸­ï¼Œå°†è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯è¿›è¡Œäº¤äº’ï¼Œä¾‹å¦‚åœ¨çœŸå®æˆ¿é—´ä¸­æ”¾ç½®è™šæ‹Ÿå®¶å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.

