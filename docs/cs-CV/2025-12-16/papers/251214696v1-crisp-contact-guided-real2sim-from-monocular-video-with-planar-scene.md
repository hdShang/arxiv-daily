---
layout: default
title: CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives
---

# CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives

**arXiv**: [2512.14696v1](https://arxiv.org/abs/2512.14696) | [PDF](https://arxiv.org/pdf/2512.14696.pdf)

**ä½œè€…**: Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan

**åˆ†ç±»**: cs.CV, cs.GR, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CRISPï¼šåŸºäºŽå•ç›®è§†é¢‘å’Œå¹³é¢åœºæ™¯åŽŸè¯­çš„æŽ¥è§¦å¼•å¯¼Real2Simæ–¹æ³•**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **äººå½¢/å››è¶³ç§»åŠ¨æŽ§åˆ¶ (Legged Locomotion)** **3Dé‡å»ºä¸Žé«˜æ–¯ (3D Reconstruction & Gaussian)** **Sim2Realä¸Žç­–ç•¥å­¦ä¹  (Sim2Real & Policy Learning)**

**å…³é”®è¯**: `Real2Sim` `å•ç›®è§†é¢‘é‡å»º` `äººä½“-åœºæ™¯äº¤äº’` `å¹³é¢åŽŸè¯­` `å¼ºåŒ–å­¦ä¹ ` `ç‰©ç†ä»¿çœŸ` `æŽ¥è§¦å»ºæ¨¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨äººä½“-åœºæ™¯è”åˆé‡å»ºä¸­å­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆä¾èµ–æ•°æ®å…ˆéªŒï¼Œè¦ä¹ˆé‡å»ºçš„å‡ ä½•ç»“æž„å™ªå£°å¤§ï¼Œå¯¼è‡´äº¤äº’å¼è¿åŠ¨è·Ÿè¸ªå¤±è´¥ã€‚
2. CRISPé€šè¿‡æ‹Ÿåˆå¹³é¢åŽŸè¯­åˆ°ç‚¹äº‘é‡å»ºï¼Œæ¢å¤å‡¸çš„ã€å¹²å‡€çš„å‡ ä½•ç»“æž„ï¼Œå¹¶åˆ©ç”¨äººä½“-åœºæ™¯æŽ¥è§¦å»ºæ¨¡æ¥é‡å»ºé®æŒ¡åŒºåŸŸã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒCRISPæ˜¾è‘—é™ä½Žäº†è¿åŠ¨è·Ÿè¸ªå¤±è´¥çŽ‡ï¼Œæé«˜äº†å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿåžåé‡ï¼Œå¹¶åœ¨çœŸå®žè§†é¢‘ä¸­è¡¨çŽ°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

CRISPæ˜¯ä¸€ç§ä»Žå•ç›®è§†é¢‘ä¸­æ¢å¤å¯æ¨¡æ‹Ÿçš„äººä½“è¿åŠ¨å’Œåœºæ™¯å‡ ä½•ç»“æž„çš„æ–¹æ³•ã€‚çŽ°æœ‰çš„äººä½“-åœºæ™¯è”åˆé‡å»ºå·¥ä½œä¾èµ–äºŽæ•°æ®é©±åŠ¨çš„å…ˆéªŒå’Œæ— ç‰©ç†å¼•æ“Žå‚ä¸Žçš„è”åˆä¼˜åŒ–ï¼Œæˆ–è€…æ¢å¤çš„å‡ ä½•ç»“æž„å™ªå£°å¤§ï¼Œå¯¼è‡´å¸¦æœ‰åœºæ™¯äº¤äº’çš„è¿åŠ¨è·Ÿè¸ªç­–ç•¥å¤±è´¥ã€‚CRISPçš„å…³é”®åœ¨äºŽé€šè¿‡æ‹Ÿåˆå¹³é¢åŽŸè¯­åˆ°åœºæ™¯çš„ç‚¹äº‘é‡å»ºï¼Œæ¥æ¢å¤å‡¸çš„ã€å¹²å‡€çš„ã€å¯ç”¨äºŽä»¿çœŸçš„å‡ ä½•ç»“æž„ï¼Œè¿™é€šè¿‡ä¸€ä¸ªç®€å•çš„æ·±åº¦ã€æ³•çº¿å’Œå…‰æµèšç±»æµç¨‹å®žçŽ°ã€‚ä¸ºäº†é‡å»ºäº¤äº’è¿‡ç¨‹ä¸­å¯èƒ½è¢«é®æŒ¡çš„åœºæ™¯å‡ ä½•ç»“æž„ï¼ŒCRISPåˆ©ç”¨äº†äººä½“-åœºæ™¯æŽ¥è§¦å»ºæ¨¡ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨äººä½“å§¿åŠ¿æ¥é‡å»ºæ¤…å­è¢«é®æŒ¡çš„åº§ä½ï¼‰ã€‚æœ€åŽï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨äººå½¢æŽ§åˆ¶å™¨ï¼Œç¡®ä¿äººä½“å’Œåœºæ™¯é‡å»ºåœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ã€‚åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆEMDBã€PROXï¼‰ä¸­ï¼ŒCRISPå°†è¿åŠ¨è·Ÿè¸ªå¤±è´¥çŽ‡ä»Ž55.2%é™ä½Žåˆ°6.9%ï¼ŒåŒæ—¶å®žçŽ°äº†43%æ›´å¿«çš„RLæ¨¡æ‹Ÿåžåé‡ã€‚è¯¥æ–¹æ³•è¿˜åœ¨åŒ…æ‹¬éšæ„æ‹æ‘„çš„è§†é¢‘ã€äº’è”ç½‘è§†é¢‘ç”šè‡³Soraç”Ÿæˆçš„è§†é¢‘åœ¨å†…çš„çœŸå®žè§†é¢‘ä¸­å¾—åˆ°äº†éªŒè¯ã€‚è¿™è¯æ˜Žäº†CRISPèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆç‰©ç†ä¸Šæœ‰æ•ˆçš„äººä½“è¿åŠ¨å’Œäº¤äº’çŽ¯å¢ƒï¼Œæžå¤§åœ°æŽ¨åŠ¨äº†æœºå™¨äººå’ŒAR/VRçš„Real2Simåº”ç”¨ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ–¹æ³•åœ¨ä»Žå•ç›®è§†é¢‘è¿›è¡Œäººä½“-åœºæ™¯è”åˆé‡å»ºæ—¶ï¼Œè¦ä¹ˆä¾èµ–å¤§é‡æ•°æ®å…ˆéªŒï¼Œå¯¼è‡´æ³›åŒ–æ€§ä¸è¶³ï¼›è¦ä¹ˆé‡å»ºçš„åœºæ™¯å‡ ä½•ç»“æž„å™ªå£°è¾ƒå¤§ï¼Œæ— æ³•ç›´æŽ¥ç”¨äºŽç‰©ç†ä»¿çœŸå’Œäº¤äº’å¼æŽ§åˆ¶ï¼Œå¯¼è‡´è¿åŠ¨è·Ÿè¸ªç­–ç•¥å®¹æ˜“å¤±è´¥ã€‚å› æ­¤ï¼Œå¦‚ä½•ä»Žå•ç›®è§†é¢‘ä¸­é‡å»ºå‡ºå¹²å‡€ã€å¯ç”¨äºŽç‰©ç†ä»¿çœŸçš„åœºæ™¯å‡ ä½•ç»“æž„ï¼Œå¹¶ä¿è¯é‡å»ºçš„äººä½“è¿åŠ¨å’Œåœºæ™¯äº¤äº’çš„ç‰©ç†åˆç†æ€§ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCRISPçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å°†åœºæ™¯å‡ ä½•ç»“æž„å»ºæ¨¡ä¸ºå¹³é¢åŽŸè¯­çš„ç»„åˆï¼Œåˆ©ç”¨æ·±åº¦ã€æ³•çº¿å’Œå…‰æµä¿¡æ¯è¿›è¡Œèšç±»ï¼Œä»Žè€ŒèŽ·å¾—å¹²å‡€ã€å‡¸çš„å‡ ä½•è¡¨ç¤ºã€‚åŒæ—¶ï¼Œåˆ©ç”¨äººä½“ä¸Žåœºæ™¯çš„æŽ¥è§¦ä¿¡æ¯æ¥æŽ¨æ–­è¢«é®æŒ¡çš„åœºæ™¯åŒºåŸŸï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–äººä½“è¿åŠ¨ï¼Œç¡®ä¿å…¶ä¸Žé‡å»ºåœºæ™¯çš„äº¤äº’åœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å…‹æœçŽ°æœ‰æ–¹æ³•å¯¹æ•°æ®å…ˆéªŒçš„ä¾èµ–å’Œé‡å»ºå‡ ä½•ç»“æž„çš„å™ªå£°é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCRISPçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä»Žå•ç›®è§†é¢‘ä¸­é‡å»ºç‚¹äº‘ï¼›2) å¯¹ç‚¹äº‘è¿›è¡Œå¹³é¢åŽŸè¯­æ‹Ÿåˆï¼Œå¾—åˆ°åœºæ™¯çš„å‡ ä½•è¡¨ç¤ºï¼›3) åˆ©ç”¨äººä½“å§¿åŠ¿ä¿¡æ¯è¿›è¡ŒæŽ¥è§¦å»ºæ¨¡ï¼ŒæŽ¨æ–­è¢«é®æŒ¡çš„åœºæ™¯åŒºåŸŸï¼›4) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒäººå½¢æŽ§åˆ¶å™¨ï¼Œä½¿å…¶åœ¨é‡å»ºçš„åœºæ™¯ä¸­è¿›è¡Œè¿åŠ¨ï¼Œå¹¶ä¼˜åŒ–äººä½“è¿åŠ¨çš„ç‰©ç†åˆç†æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šCRISPçš„å…³é”®åˆ›æ–°åœ¨äºŽï¼š1) æå‡ºäº†ä¸€ç§åŸºäºŽå¹³é¢åŽŸè¯­æ‹Ÿåˆçš„åœºæ™¯å‡ ä½•é‡å»ºæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆå¹²å‡€ã€å‡¸çš„å‡ ä½•è¡¨ç¤ºï¼Œæ›´é€‚åˆç‰©ç†ä»¿çœŸï¼›2) åˆ©ç”¨äººä½“-åœºæ™¯æŽ¥è§¦å»ºæ¨¡æ¥æŽ¨æ–­è¢«é®æŒ¡çš„åœºæ™¯åŒºåŸŸï¼Œæé«˜äº†åœºæ™¯é‡å»ºçš„å®Œæ•´æ€§ï¼›3) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–äººä½“è¿åŠ¨ï¼Œç¡®ä¿å…¶ä¸Žé‡å»ºåœºæ™¯çš„äº¤äº’åœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¹³é¢åŽŸè¯­æ‹Ÿåˆé˜¶æ®µï¼ŒCRISPä½¿ç”¨åŸºäºŽRANSACçš„å¹³é¢æ£€æµ‹ç®—æ³•ï¼Œå¹¶ç»“åˆæ·±åº¦ã€æ³•çº¿å’Œå…‰æµä¿¡æ¯è¿›è¡Œèšç±»ï¼Œä»¥æé«˜å¹³é¢æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨æŽ¥è§¦å»ºæ¨¡é˜¶æ®µï¼ŒCRISPä½¿ç”¨é¢„è®­ç»ƒçš„äººä½“å§¿åŠ¿ä¼°è®¡æ¨¡åž‹æ¥é¢„æµ‹äººä½“ä¸Žåœºæ™¯çš„æŽ¥è§¦ç‚¹ï¼Œå¹¶åˆ©ç”¨è¿™äº›æŽ¥è§¦ç‚¹æ¥çº¦æŸè¢«é®æŒ¡åŒºåŸŸçš„é‡å»ºã€‚åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼ŒCRISPä½¿ç”¨åŸºäºŽActor-Criticçš„ç®—æ³•ï¼Œå¹¶è®¾è®¡äº†å¥–åŠ±å‡½æ•°æ¥é¼“åŠ±äººå½¢æŽ§åˆ¶å™¨åœ¨é‡å»ºçš„åœºæ™¯ä¸­è¿›è¡Œè‡ªç„¶çš„è¿åŠ¨ï¼Œå¹¶é¿å…ä¸Žåœºæ™¯å‘ç”Ÿä¸åˆç†çš„ç¢°æ’žã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

CRISPåœ¨EMDBå’ŒPROXç­‰ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œå°†è¿åŠ¨è·Ÿè¸ªå¤±è´¥çŽ‡ä»Ž55.2%æ˜¾è‘—é™ä½Žåˆ°6.9%ï¼ŒåŒæ—¶å®žçŽ°äº†43%çš„å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿåžåé‡æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜åœ¨åŒ…æ‹¬éšæ„æ‹æ‘„çš„è§†é¢‘ã€äº’è”ç½‘è§†é¢‘ç”šè‡³Soraç”Ÿæˆçš„è§†é¢‘ç­‰çœŸå®žè§†é¢‘ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜Žäº†å…¶è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

CRISPå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æœºå™¨äººä»¿çœŸã€å¢žå¼ºçŽ°å®ž/è™šæ‹ŸçŽ°å®žï¼ˆAR/VRï¼‰å†…å®¹ç”Ÿæˆã€ä»¥åŠäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»ŽçœŸå®žä¸–ç•Œçš„è§†é¢‘ä¸­è‡ªåŠ¨ç”Ÿæˆå¯ç”¨äºŽä»¿çœŸçš„çŽ¯å¢ƒå’Œäººä½“è¿åŠ¨ï¼Œä»Žè€Œé™ä½Žäº†æœºå™¨äººå¼€å‘å’ŒAR/VRå†…å®¹åˆ¶ä½œçš„æˆæœ¬ã€‚æ­¤å¤–ï¼ŒCRISPè¿˜å¯ä»¥ç”¨äºŽåˆ†æžå’Œç†è§£äººç±»åœ¨çœŸå®žåœºæ™¯ä¸­çš„è¡Œä¸ºï¼Œä¸ºäººæœºäº¤äº’è®¾è®¡æä¾›æŒ‡å¯¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.

