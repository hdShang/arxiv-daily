---
layout: default
title: The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy
---

# The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy

**arXiv**: [2512.14423v1](https://arxiv.org/abs/2512.14423) | [PDF](https://arxiv.org/pdf/2512.14423.pdf)

**ä½œè€…**: Zhuo Chen, Fanyue Wei, Runze Xu, Jingjing Li, Lixin Duan, Angela Yao, Wen Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project page:https://synps26.github.io/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSynPSæ–¹æ³•ï¼Œé€šè¿‡æ³¨æ„åŠ›ååŒæœºåˆ¶è§£å†³å¤æ‚éžåˆšæ€§å›¾åƒç¼–è¾‘ä¸­çš„è¿‡ç¼–è¾‘ä¸Žæ¬ ç¼–è¾‘é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å›¾åƒç¼–è¾‘` `æ‰©æ•£æ¨¡åž‹` `æ³¨æ„åŠ›æœºåˆ¶` `éžåˆšæ€§ç¼–è¾‘` `ä½ç½®åµŒå…¥` `è¯­ä¹‰ä¿¡æ¯` `ç¼–è¾‘å¿ å®žæ€§` `åŽ»å™ªè¿‡ç¨‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨å¤æ‚éžåˆšæ€§ç¼–è¾‘ä¸­å­˜åœ¨æ³¨æ„åŠ›å´©æºƒé—®é¢˜ï¼Œä½ç½®åµŒå…¥æˆ–è¯­ä¹‰ç‰¹å¾ä¸»å¯¼å¯¼è‡´è¿‡ç¼–è¾‘æˆ–æ¬ ç¼–è¾‘
2. æå‡ºSynPSæ–¹æ³•ï¼Œé€šè¿‡ç¼–è¾‘åº¦é‡åŠ¨æ€è°ƒèŠ‚ä½ç½®åµŒå…¥å½±å“ï¼ŒååŒåˆ©ç”¨ä½ç½®å’Œè¯­ä¹‰ä¿¡æ¯
3. å®žéªŒè¡¨æ˜ŽSynPSåœ¨å…¬å…±å’Œæ–°åŸºå‡†ä¸Šè¡¨çŽ°ä¼˜è¶Šï¼Œæœ‰æ•ˆæå‡ç¼–è¾‘å¿ å®žæ€§ï¼Œé¿å…è¿‡ç¼–è¾‘å’Œæ¬ ç¼–è¾‘

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽå¤§åž‹æ‰©æ•£æ¨¡åž‹çš„æ— è®­ç»ƒå›¾åƒç¼–è¾‘å·²å˜å¾—å®žç”¨ï¼Œä½†å¿ å®žæ‰§è¡Œå¤æ‚éžåˆšæ€§ç¼–è¾‘ï¼ˆå¦‚å§¿æ€æˆ–å½¢çŠ¶å˜åŒ–ï¼‰ä»ç„¶æžå…·æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘çŽ°ä¸€ä¸ªå…³é”®æ ¹æœ¬åŽŸå› ï¼šçŽ°æœ‰æ³¨æ„åŠ›å…±äº«æœºåˆ¶ä¸­çš„æ³¨æ„åŠ›å´©æºƒï¼Œå…¶ä¸­ä½ç½®åµŒå…¥æˆ–è¯­ä¹‰ç‰¹å¾ä¸»å¯¼è§†è§‰å†…å®¹æ£€ç´¢ï¼Œå¯¼è‡´è¿‡ç¼–è¾‘æˆ–æ¬ ç¼–è¾‘ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SynPSæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ååŒåˆ©ç”¨ä½ç½®åµŒå…¥å’Œè¯­ä¹‰ä¿¡æ¯è¿›è¡Œå¿ å®žçš„éžåˆšæ€§å›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºä¸€ç§ç¼–è¾‘åº¦é‡ï¼Œé‡åŒ–æ¯ä¸ªåŽ»å™ªæ­¥éª¤æ‰€éœ€çš„ç¼–è¾‘å¹…åº¦ã€‚åŸºäºŽæ­¤åº¦é‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ³¨æ„åŠ›ååŒæµç¨‹ï¼ŒåŠ¨æ€è°ƒèŠ‚ä½ç½®åµŒå…¥çš„å½±å“ï¼Œä½¿SynPSèƒ½å¤Ÿå¹³è¡¡è¯­ä¹‰ä¿®æ”¹å’Œä¿çœŸåº¦ä¿æŒã€‚é€šè¿‡è‡ªé€‚åº”æ•´åˆä½ç½®å’Œè¯­ä¹‰çº¿ç´¢ï¼ŒSynPSæœ‰æ•ˆé¿å…äº†è¿‡ç¼–è¾‘å’Œæ¬ ç¼–è¾‘ã€‚åœ¨å…¬å…±å’Œæ–°ç­–åˆ’çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®žéªŒè¯æ˜Žäº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½å’Œå¿ å®žæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

SynPSçš„æ•´ä½“æ¡†æž¶æ˜¯ä¸€ä¸ªåŸºäºŽæ‰©æ•£æ¨¡åž‹çš„æ³¨æ„åŠ›ååŒç¼–è¾‘æµç¨‹ã€‚é¦–å…ˆï¼Œæå‡ºç¼–è¾‘åº¦é‡æ¥é‡åŒ–æ¯ä¸ªåŽ»å™ªæ­¥éª¤çš„ç¼–è¾‘éœ€æ±‚ï¼Œè¿™æ˜¯å…³é”®åˆ›æ–°ç‚¹ï¼Œä½¿ç¼–è¾‘è¿‡ç¨‹å¯æµ‹é‡ã€‚ç„¶åŽï¼Œè®¾è®¡æ³¨æ„åŠ›ååŒæœºåˆ¶ï¼ŒåŠ¨æ€è°ƒåˆ¶ä½ç½®åµŒå…¥çš„å½±å“ï¼Œå¹³è¡¡è¯­ä¹‰ä¿®æ”¹ä¸Žä¿çœŸåº¦ä¿æŒã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼šçŽ°æœ‰æ–¹æ³•å¾€å¾€å›ºå®šä¾èµ–ä½ç½®æˆ–è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ³¨æ„åŠ›å´©æºƒï¼›è€ŒSynPSé€šè¿‡è‡ªé€‚åº”æ•´åˆä¸¤è€…ï¼Œå®žçŽ°æ›´ç²¾ç»†çš„æŽ§åˆ¶ï¼Œé¿å…è¿‡ç¼–è¾‘å’Œæ¬ ç¼–è¾‘ï¼Œæå‡å¤æ‚éžåˆšæ€§ç¼–è¾‘çš„å¿ å®žæ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨å…¬å…±åŸºå‡†å’Œæ–°ç­–åˆ’æ•°æ®é›†ä¸Šçš„å®žéªŒæ˜¾ç¤ºï¼ŒSynPSåœ¨å¤æ‚éžåˆšæ€§ç¼–è¾‘ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæœ‰æ•ˆé¿å…è¿‡ç¼–è¾‘å’Œæ¬ ç¼–è¾‘ï¼Œç¼–è¾‘å¿ å®žæ€§å¾—åˆ°å¤§å¹…æå‡ï¼Œè¯æ˜Žäº†æ³¨æ„åŠ›ååŒæœºåˆ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«é€‚ç”¨äºŽéœ€è¦é«˜ä¿çœŸåº¦çš„å¤æ‚éžåˆšæ€§å›¾åƒç¼–è¾‘ï¼Œå¦‚äººç‰©å§¿æ€è°ƒæ•´ã€ç‰©ä½“å½¢çŠ¶å˜æ¢ã€è‰ºæœ¯åˆ›ä½œå’Œå½±è§†åŽæœŸåˆ¶ä½œã€‚å…¶å®žé™…ä»·å€¼åœ¨äºŽæä¾›æ›´å¯é ã€å¯æŽ§çš„ç¼–è¾‘å·¥å…·ï¼Œæå‡è‡ªåŠ¨åŒ–ç¼–è¾‘çš„è´¨é‡å’Œæ•ˆçŽ‡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.

