---
layout: default
title: SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning
---

# SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13874" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13874</a>
  <a href="https://arxiv.org/pdf/2512.13874.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13874" onclick="toggleFavorite(this, '2512.13874', 'SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jitesh Jain, Jialuo Li, Zixian Ma, Jieyu Zhang, Chris Dongjoo Kim, Sangho Lee, Rohun Tripathi, Tanmay Gupta, Christopher Clark, Humphrey Shi

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAGEï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä»»æ„æ—¶åŸŸAgentï¼Œç”¨äºé•¿è§†é¢‘æ¨ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é•¿è§†é¢‘æ¨ç†` `ä»»æ„æ—¶åŸŸæ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `æ™ºèƒ½Agent` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ¨ç†æ¨¡å‹é€šå¸¸ä¸€æ¬¡æ€§å¤„ç†å¤§é‡å¸§å¹¶é¢„æµ‹ç­”æ¡ˆï¼Œç±»ä¼¼äºè§‚çœ‹å®Œæ•´é•¿è§†é¢‘ï¼Œæ¶ˆè€—å¤§é‡èµ„æºï¼Œç¼ºä¹çµæ´»æ€§ã€‚
2. SAGEç³»ç»Ÿé€šè¿‡å¤šè½®æ¨ç†å¤„ç†é•¿è§†é¢‘ï¼Œå¹¶èƒ½å•è½®å¤„ç†ç®€å•é—®é¢˜ï¼Œæ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œå®ç°ä»»æ„æ—¶åŸŸçš„è§†é¢‘ç†è§£ã€‚
3. é€šè¿‡Gemini-2.5-Flashç”Ÿæˆåˆæˆæ•°æ®è®­ç»ƒSAGE-MMï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒï¼Œåœ¨SAGE-Benchä¸ŠéªŒè¯äº†æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºSAGEï¼Œä¸€ä¸ªæ™ºèƒ½Agentç³»ç»Ÿï¼Œèƒ½å¤Ÿå¯¹é•¿è§†é¢‘è¿›è¡Œå¤šè½®æ¨ç†ï¼ŒåŒæ—¶ä¹Ÿèƒ½å•è½®å¤„ç†ç®€å•é—®é¢˜ï¼Œæ¨¡æ‹Ÿäººç±»åœ¨å¤„ç†ä¸åŒæ—¶é•¿è§†é¢‘æ—¶çš„è¡Œä¸ºã€‚ä¸ºäº†è®­ç»ƒSAGEçš„æ ¸å¿ƒæ¨¡å—SAGE-MMï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªç®€æ˜“çš„åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹ï¼Œè¯¥æµç¨‹ä½¿ç”¨Gemini-2.5-Flashã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ–¹æ³•ï¼Œå¯¹äºåœ¨SAGE-MMä¸­åŸ¹å…»ä»»æ„æ—¶åŸŸæ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä¸ºäº†è¯„ä¼°çœŸå®å¨±ä¹åœºæ™¯ä¸‹çš„è§†é¢‘æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡æ„å»ºäº†SAGE-Benchï¼Œå…¶å¹³å‡è§†é¢‘æ—¶é•¿è¶…è¿‡700ç§’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„ç³»ç»Ÿã€æ•°æ®å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œåœ¨å¼€æ”¾å¼è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†é«˜è¾¾6.1%çš„æ˜¾è‘—æå‡ï¼Œåœ¨è¶…è¿‡10åˆ†é’Ÿçš„è§†é¢‘ä¸Šå–å¾—äº†é«˜è¾¾8.2%çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘æ¨ç†æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œé€šå¸¸éœ€è¦ä¸€æ¬¡æ€§å¤„ç†å¤§é‡å¸§ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”ç¼ºä¹åƒäººç±»ä¸€æ ·æ ¹æ®è§†é¢‘é•¿åº¦å’Œä»»åŠ¡å¤æ‚åº¦è°ƒæ•´æ¨ç†ç­–ç•¥çš„çµæ´»æ€§ã€‚è¿™äº›æ¨¡å‹æ— æ³•æœ‰æ•ˆåˆ©ç”¨è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSAGEçš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡ä»¿äººç±»åœ¨è§‚çœ‹è§†é¢‘æ—¶çš„è¡Œä¸ºï¼Œå³æ ¹æ®è§†é¢‘çš„é•¿åº¦å’Œä»»åŠ¡çš„å¤æ‚ç¨‹åº¦ï¼Œå†³å®šæ˜¯å¿«é€Ÿæµè§ˆè¿˜æ˜¯å®Œæ•´è§‚çœ‹ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªæ™ºèƒ½Agentï¼ŒSAGEèƒ½å¤Ÿè¿›è¡Œå¤šè½®æ¨ç†ï¼Œé€æ­¥æå–è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è§†é¢‘ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSAGEç³»ç»Ÿä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šSAGE-MMï¼ˆå¤šæ¨¡æ€æ¨¡å‹ï¼‰å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ã€‚SAGE-MMè´Ÿè´£ä»è§†é¢‘ä¸­æå–ç‰¹å¾å¹¶è¿›è¡Œæ¨ç†ï¼Œè€Œå¼ºåŒ–å­¦ä¹ æ¨¡å—åˆ™ç”¨äºä¼˜åŒ–SAGE-MMçš„æ¨ç†ç­–ç•¥ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š1. è¾“å…¥è§†é¢‘ï¼›2. SAGE-MMæ ¹æ®å½“å‰çŠ¶æ€å†³å®šæ˜¯è§‚çœ‹æ›´å¤šå¸§è¿˜æ˜¯è¾“å‡ºç­”æ¡ˆï¼›3. å¦‚æœé€‰æ‹©è§‚çœ‹æ›´å¤šå¸§ï¼Œåˆ™æ›´æ–°çŠ¶æ€å¹¶é‡å¤æ­¥éª¤2ï¼›4. å¦‚æœé€‰æ‹©è¾“å‡ºç­”æ¡ˆï¼Œåˆ™æ ¹æ®ç­”æ¡ˆçš„æ­£ç¡®æ€§è·å¾—å¥–åŠ±ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ›´æ–°SAGE-MMçš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šSAGEçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä»»æ„æ—¶åŸŸæ¨ç†èƒ½åŠ›ï¼Œå³èƒ½å¤Ÿæ ¹æ®è§†é¢‘çš„é•¿åº¦å’Œä»»åŠ¡çš„å¤æ‚ç¨‹åº¦ï¼ŒåŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„å•è½®æ¨ç†æ¨¡å‹ç›¸æ¯”ï¼ŒSAGEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨Gemini-2.5-Flashç”Ÿæˆåˆæˆæ•°æ®ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šSAGE-MMé‡‡ç”¨å¤šæ¨¡æ€èåˆçš„æ–¹å¼ï¼Œå°†è§†é¢‘å¸§å’ŒéŸ³é¢‘ä¿¡æ¯ç»“åˆèµ·æ¥è¿›è¡Œæ¨ç†ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•é‡‡ç”¨Actor-Criticæ¡†æ¶ï¼Œå…¶ä¸­Actorè´Ÿè´£é€‰æ‹©åŠ¨ä½œï¼ˆè§‚çœ‹æ›´å¤šå¸§æˆ–è¾“å‡ºç­”æ¡ˆï¼‰ï¼ŒCriticè´Ÿè´£è¯„ä¼°å½“å‰çŠ¶æ€çš„ä»·å€¼ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦å¹³è¡¡æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœè¾“å‡ºçš„ç­”æ¡ˆæ­£ç¡®ï¼Œåˆ™ç»™äºˆæ­£å‘å¥–åŠ±ï¼›å¦‚æœè¾“å‡ºçš„ç­”æ¡ˆé”™è¯¯ï¼Œåˆ™ç»™äºˆè´Ÿå‘å¥–åŠ±ï¼›å¦‚æœè§‚çœ‹çš„å¸§æ•°è¿‡å¤šï¼Œåˆ™ç»™äºˆè´Ÿå‘å¥–åŠ±ï¼Œä»¥é¼“åŠ±Agentå°½å¿«è¾“å‡ºç­”æ¡ˆã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13874/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13874/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13874/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

SAGEåœ¨å¼€æ”¾å¼è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæœ€é«˜è¾¾åˆ°6.1%ã€‚å°¤å…¶æ˜¯åœ¨å¤„ç†è¶…è¿‡10åˆ†é’Ÿçš„é•¿è§†é¢‘æ—¶ï¼ŒSAGEçš„æ€§èƒ½æå‡é«˜è¾¾8.2%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSAGEçš„ä»»æ„æ—¶åŸŸæ¨ç†èƒ½åŠ›èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è§†é¢‘ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿçš„å•è½®æ¨ç†æ¨¡å‹ç›¸æ¯”ï¼ŒSAGEå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SAGEå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½è§†é¢‘ç›‘æ§ã€æ™ºèƒ½å®¢æœã€è§†é¢‘å†…å®¹æ¨èç­‰ã€‚é€šè¿‡æ¨¡ä»¿äººç±»çš„æ¨ç†æ–¹å¼ï¼ŒSAGEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç†è§£è§†é¢‘å†…å®¹ï¼Œä»è€Œä¸ºç”¨æˆ·æä¾›æ›´æ™ºèƒ½ã€æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚æœªæ¥ï¼ŒSAGEæœ‰æœ›æˆä¸ºè§†é¢‘ç†è§£é¢†åŸŸçš„é‡è¦æŠ€æœ¯ï¼Œæ¨åŠ¨ç›¸å…³äº§ä¸šçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.

