---
layout: default
title: "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning"
---

# A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14442" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14442v1</a>
  <a href="https://arxiv.org/pdf/2512.14442.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14442v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.14442v1', 'A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zixin Zhang, Kanghao Chen, Hanqing Wang, Hongfei Zhang, Harold Haodong Chen, Chenfei Liao, Litao Guo, Ying-Cong Chen

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºA4-Agentæ¡†æ¶ä»¥è§£å†³é›¶-shotå¯ä¾›æ€§æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯ä¾›æ€§é¢„æµ‹` `é›¶-shotå­¦ä¹ ` `ç”Ÿæˆæ¨¡å‹` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æœºå™¨äººäº¤äº’` `å…·èº«äººå·¥æ™ºèƒ½` `æ¨¡å‹æ³›åŒ–` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç«¯åˆ°ç«¯æ¨¡å‹åœ¨æ–°ç‰©ä½“å’Œæœªè§ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œé™åˆ¶äº†å¯ä¾›æ€§é¢„æµ‹çš„åº”ç”¨ã€‚
2. A4-Agentæ¡†æ¶é€šè¿‡å°†å¯ä¾›æ€§é¢„æµ‹è§£è€¦ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨ä¸åŒçš„åŸºç¡€æ¨¡å‹è¿›è¡ŒååŒå·¥ä½œï¼Œé¿å…äº†ä¼ ç»Ÿæ¨¡å‹çš„å±€é™æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒA4-Agentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯ä¾›æ€§é¢„æµ‹æ˜¯åŸºäºè¯­è¨€æŒ‡ä»¤è¯†åˆ«ç‰©ä½“äº¤äº’åŒºåŸŸçš„å…³é”®ä»»åŠ¡ï¼Œå¯¹äºå…·èº«äººå·¥æ™ºèƒ½è‡³å…³é‡è¦ã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯æ¨¡å‹å°†é«˜å±‚æ¨ç†ä¸ä½å±‚åŸºç¡€ç»“åˆä¸ºå•ä¸€ç®¡é“ï¼Œä¾èµ–äºæ ‡æ³¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´åœ¨æ–°ç‰©ä½“å’Œæœªè§ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æœ¬æ–‡æå‡ºäº†A4-Agentï¼Œä¸€ä¸ªæ— è®­ç»ƒçš„ä»£ç†æ¡†æ¶ï¼Œå°†å¯ä¾›æ€§é¢„æµ‹è§£è€¦ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šDreamerã€Thinkerå’ŒSpotterã€‚è¯¥æ¡†æ¶åœ¨æµ‹è¯•æ—¶åè°ƒä¸“é—¨çš„åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ç›‘ç£æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å¯¹çœŸå®ä¸–ç•Œç¯å¢ƒçš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯å¯ä¾›æ€§é¢„æµ‹ä¸­çš„æ³›åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºæ ‡æ³¨æ•°æ®é›†ï¼Œå¯¼è‡´åœ¨æ–°ç‰©ä½“å’Œç¯å¢ƒä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šA4-Agentæ¡†æ¶é€šè¿‡å°†å¯ä¾›æ€§é¢„æµ‹è§£è€¦ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„é˜¶æ®µï¼Œåˆ†åˆ«å¤„ç†äº¤äº’çš„å¯è§†åŒ–ã€å¯¹è±¡éƒ¨ä»¶çš„é€‰æ‹©å’Œäº¤äº’åŒºåŸŸçš„å®šä½ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) Dreamerï¼Œä½¿ç”¨ç”Ÿæˆæ¨¡å‹å¯è§†åŒ–äº¤äº’è¿‡ç¨‹ï¼›2) Thinkerï¼Œåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹å†³å®šä¸å“ªä¸ªç‰©ä½“éƒ¨ä»¶äº¤äº’ï¼›3) Spotterï¼Œåè°ƒè§†è§‰åŸºç¡€æ¨¡å‹ç²¾ç¡®å®šä½äº¤äº’åŒºåŸŸã€‚

**å…³é”®åˆ›æ–°**ï¼šA4-Agentçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶æ— è®­ç»ƒçš„æ¡†æ¶è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿è¿›è¡Œæœ‰æ•ˆçš„å¯ä¾›æ€§æ¨ç†ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ¡†æ¶çš„è®¾è®¡ä¸­ï¼ŒDreamerã€Thinkerå’ŒSpotterå„è‡ªé‡‡ç”¨äº†ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç¡®ä¿åœ¨æµ‹è¯•æ—¶èƒ½å¤Ÿé«˜æ•ˆååŒå·¥ä½œï¼Œå…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14442v1/x2.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14442v1/x3.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14442v1/x4.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒA4-Agentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„å¼ºæ³›åŒ–èƒ½åŠ›å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

A4-Agentæ¡†æ¶åœ¨æœºå™¨äººäº¤äº’ã€æ™ºèƒ½å®¶å±…å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜æ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½åœ¨å¤æ‚åœºæ™¯ä¸­çš„å®é™…åº”ç”¨ï¼Œæå‡äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.

