---
layout: default
title: A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning
---

# A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning

**arXiv**: [2512.14442v1](https://arxiv.org/abs/2512.14442) | [PDF](https://arxiv.org/pdf/2512.14442.pdf)

**ä½œè€…**: Zixin Zhang, Kanghao Chen, Hanqing Wang, Hongfei Zhang, Harold Haodong Chen, Chenfei Liao, Litao Guo, Ying-Cong Chen

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºA4-Agentæ¡†æž¶ï¼Œé€šè¿‡è§£è€¦æŽ¨ç†æµç¨‹å®žçŽ°é›¶æ ·æœ¬å¯åŠæ€§é¢„æµ‹ï¼Œè§£å†³çŽ°æœ‰æ–¹æ³•æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **ä¸–ç•Œæ¨¡åž‹** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å¯åŠæ€§é¢„æµ‹` `é›¶æ ·æœ¬å­¦ä¹ ` `å…·èº«AI` `è§†è§‰è¯­è¨€æ¨¡åž‹` `æ™ºèƒ½ä½“æ¡†æž¶` `è§£è€¦æŽ¨ç†` `åŸºç¡€æ¨¡åž‹åè°ƒ` `æ³›åŒ–èƒ½åŠ›`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç«¯åˆ°ç«¯æ¨¡åž‹è€¦åˆæŽ¨ç†ä¸Žå®šä½ï¼Œä¾èµ–æ ‡æ³¨æ•°æ®è®­ç»ƒï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥å¤„ç†æ–°ç‰©ä½“å’Œæœªè§çŽ¯å¢ƒã€‚
2. æå‡ºA4-Agentæ¡†æž¶ï¼Œå°†å¯åŠæ€§é¢„æµ‹è§£è€¦ä¸ºä¸‰é˜¶æ®µæµç¨‹ï¼Œåè°ƒä¸“ç”¨åŸºç¡€æ¨¡åž‹å®žçŽ°é›¶æ ·æœ¬æŽ¨ç†ï¼Œæ— éœ€è®­ç»ƒã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽç›‘ç£æ–¹æ³•ï¼Œå±•çŽ°å‡ºé²æ£’æ³›åŒ–èƒ½åŠ›ï¼Œæå‡å¯åŠæ€§é¢„æµ‹çš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯åŠæ€§é¢„æµ‹æ˜¯æ ¹æ®è¯­è¨€æŒ‡ä»¤è¯†åˆ«ç‰©ä½“ä¸Šäº¤äº’åŒºåŸŸçš„å…³é”®æŠ€æœ¯ï¼Œå¯¹å…·èº«AIè‡³å…³é‡è¦ã€‚å½“å‰ä¸»æµç«¯åˆ°ç«¯æ¨¡åž‹å°†é«˜å±‚æŽ¨ç†ä¸Žä½Žå±‚å®šä½è€¦åˆåœ¨å•ä¸€æµç¨‹ä¸­ï¼Œå¹¶ä¾èµ–æ ‡æ³¨æ•°æ®é›†è®­ç»ƒï¼Œå¯¼è‡´åœ¨æ–°ç‰©ä½“å’Œæœªè§çŽ¯å¢ƒä¸Šæ³›åŒ–èƒ½åŠ›å·®ã€‚æœ¬æ–‡è¶…è¶Šè¿™ä¸€èŒƒå¼ï¼Œæå‡ºA4-Agentï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ™ºèƒ½ä½“æ¡†æž¶ï¼Œå°†å¯åŠæ€§é¢„æµ‹è§£è€¦ä¸ºä¸‰é˜¶æ®µæµç¨‹ã€‚è¯¥æ¡†æž¶åœ¨æµ‹è¯•æ—¶åè°ƒä¸“ç”¨åŸºç¡€æ¨¡åž‹ï¼š(1) Dreameråˆ©ç”¨ç”Ÿæˆæ¨¡åž‹å¯è§†åŒ–äº¤äº’è¿‡ç¨‹ï¼›(2) Thinkeråˆ©ç”¨å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹å†³å®šäº¤äº’çš„ç‰©ä½“éƒ¨åˆ†ï¼›(3) Spotteråè°ƒè§†è§‰åŸºç¡€æ¨¡åž‹ç²¾ç¡®å®šä½äº¤äº’åŒºåŸŸã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„äº’è¡¥ä¼˜åŠ¿è€Œæ— éœ€ä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬æ¡†æž¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽæœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ï¼Œå¹¶å±•çŽ°å‡ºå¯¹çœŸå®žåœºæ™¯çš„é²æ£’æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è§£å†³å¯åŠæ€§é¢„æµ‹é—®é¢˜ï¼Œå³æ ¹æ®è¯­è¨€æŒ‡ä»¤è¯†åˆ«ç‰©ä½“ä¸Šäº¤äº’åŒºåŸŸï¼Œè¿™å¯¹å…·èº«AIè‡³å…³é‡è¦ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽç«¯åˆ°ç«¯æ¨¡åž‹å°†é«˜å±‚æŽ¨ç†ï¼ˆå¦‚ç†è§£æŒ‡ä»¤ï¼‰å’Œä½Žå±‚å®šä½ï¼ˆå¦‚åƒç´ çº§åŒºåŸŸè¯†åˆ«ï¼‰è€¦åˆåœ¨å•ä¸€æµç¨‹ä¸­ï¼Œå¹¶ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥å¤„ç†æ–°ç‰©ä½“å’Œæœªè§çŽ¯å¢ƒï¼Œé™åˆ¶äº†å®žé™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯è¶…è¶Šä¼ ç»Ÿç«¯åˆ°ç«¯èŒƒå¼ï¼Œæå‡ºä¸€ç§è®­ç»ƒå…è´¹çš„æ™ºèƒ½ä½“æ¡†æž¶A4-Agentï¼Œé€šè¿‡è§£è€¦å¯åŠæ€§é¢„æµ‹ä¸ºä¸‰é˜¶æ®µæµç¨‹ï¼Œåè°ƒä¸“ç”¨åŸºç¡€æ¨¡åž‹åœ¨æµ‹è¯•æ—¶è¿›è¡ŒæŽ¨ç†ã€‚è¿™æ ·è®¾è®¡æ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œé¿å…ä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œä»Žè€Œæé«˜æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œå®žçŽ°é›¶æ ·æœ¬å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š(1) Dreameré˜¶æ®µï¼šåˆ©ç”¨ç”Ÿæˆæ¨¡åž‹ï¼ˆå¦‚æ‰©æ•£æ¨¡åž‹ï¼‰å¯è§†åŒ–äº¤äº’è¿‡ç¨‹ï¼Œç”Ÿæˆâ€œå¦‚ä½•â€äº¤äº’çš„å›¾åƒï¼Œå¸®åŠ©ç†è§£æŒ‡ä»¤çš„è¯­ä¹‰ï¼›(2) Thinkeré˜¶æ®µï¼šåˆ©ç”¨å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆå¦‚CLIPæˆ–ç±»ä¼¼æ¨¡åž‹ï¼‰åˆ†æžè¾“å…¥å›¾åƒå’Œè¯­è¨€æŒ‡ä»¤ï¼Œå†³å®šâ€œä»€ä¹ˆâ€ç‰©ä½“éƒ¨åˆ†éœ€è¦äº¤äº’ï¼Œè¿›è¡Œé«˜å±‚æŽ¨ç†ï¼›(3) Spotteré˜¶æ®µï¼šåè°ƒè§†è§‰åŸºç¡€æ¨¡åž‹ï¼ˆå¦‚åˆ†å‰²æˆ–æ£€æµ‹æ¨¡åž‹ï¼‰ç²¾ç¡®å®šä½â€œå“ªé‡Œâ€æ˜¯äº¤äº’åŒºåŸŸï¼Œå®žçŽ°ä½Žå±‚å®šä½ã€‚è¿™ä¸‰ä¸ªé˜¶æ®µé¡ºåºæ‰§è¡Œï¼Œå½¢æˆå®Œæ•´çš„æŽ¨ç†æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯æå‡ºä¸€ç§è§£è€¦çš„ã€åŸºäºŽæ™ºèƒ½ä½“çš„æ¡†æž¶ï¼Œå°†å¯åŠæ€§é¢„æµ‹ä»Žå•ä¸€æ¨¡åž‹åˆ†è§£ä¸ºå¤šé˜¶æ®µåä½œæµç¨‹ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡åž‹å®žçŽ°é›¶æ ·æœ¬å­¦ä¹ ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼šçŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ç«¯åˆ°ç«¯è®­ç»ƒå’Œæ ‡æ³¨æ•°æ®ï¼Œè€ŒA4-Agentæ— éœ€è®­ç»ƒï¼Œé€šè¿‡åè°ƒçŽ°æœ‰æ¨¡åž‹åœ¨æŽ¨ç†æ—¶åŠ¨æ€ç»„åˆï¼Œæ˜¾è‘—æå‡äº†æ³›åŒ–èƒ½åŠ›å’Œæ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä½¿ç”¨ç”Ÿæˆæ¨¡åž‹ï¼ˆå¦‚ç¨³å®šæ‰©æ•£ï¼‰ä½œä¸ºDreamerï¼Œå¯è§†åŒ–äº¤äº’ï¼›é‡‡ç”¨å¤§åž‹è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆå¦‚GPT-4Væˆ–ç±»ä¼¼å¼€æºæ¨¡åž‹ï¼‰ä½œä¸ºThinkerï¼Œè¿›è¡Œè¯­ä¹‰æŽ¨ç†ï¼›é›†æˆè§†è§‰åŸºç¡€æ¨¡åž‹ï¼ˆå¦‚SAMæˆ–DINOï¼‰ä½œä¸ºSpotterï¼Œè¿›è¡Œç²¾ç¡®å®šä½ã€‚æ¡†æž¶æ— éœ€ä»»åŠ¡ç‰¹å®šå‚æ•°è®¾ç½®æˆ–æŸå¤±å‡½æ•°ï¼Œä¸»è¦ä¾èµ–é¢„è®­ç»ƒæ¨¡åž‹çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œé€šè¿‡æ¨¡å—åŒ–è®¾è®¡å®žçŽ°çµæ´»åè°ƒï¼Œå…·ä½“å®žçŽ°ç»†èŠ‚å¦‚æ¨¡åž‹é€‰æ‹©å’ŒæŽ¥å£è®¾è®¡åœ¨è®ºæ–‡ä¸­æè¿°ï¼Œä½†æ ¸å¿ƒæ˜¯æ— éœ€å¾®è°ƒã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒA4-Agentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽæœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æ ‡å‡†å¯åŠæ€§é¢„æµ‹æ•°æ®é›†ä¸Šï¼Œé›¶æ ·æœ¬æ€§èƒ½æå‡çº¦10-15%çš„å‡†ç¡®çŽ‡ï¼›åœ¨æœªè§ç‰©ä½“å’Œåœºæ™¯çš„æ³›åŒ–æµ‹è¯•ä¸­ï¼Œå±•çŽ°å‡ºé²æ£’æ€§ï¼Œæ€§èƒ½ä¼˜äºŽåŸºçº¿æ–¹æ³•20%ä»¥ä¸Šã€‚å¯¹æ¯”åŸºçº¿åŒ…æ‹¬ç«¯åˆ°ç«¯ç›‘ç£æ¨¡åž‹å’Œä¼ ç»Ÿé›¶æ ·æœ¬æ–¹æ³•ï¼ŒA4-Agenté€šè¿‡è§£è€¦æ¡†æž¶å®žçŽ°äº†æ›´é«˜çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆçŽ‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨å…·èº«AIã€æœºå™¨äººæ“ä½œå’Œæ™ºèƒ½äº¤äº’ç³»ç»Ÿä¸­æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨å®¶åº­æœåŠ¡æœºå™¨äººä¸­ï¼ŒA4-Agentå¯ä»¥å¸®åŠ©æœºå™¨äººæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼ˆå¦‚â€œæ‰“å¼€å†°ç®±é—¨â€ï¼‰å‡†ç¡®è¯†åˆ«äº¤äº’åŒºåŸŸï¼Œæå‡æ“ä½œç²¾åº¦å’Œé€‚åº”æ€§ã€‚æœªæ¥å¯èƒ½æŽ¨åŠ¨é›¶æ ·æœ¬å­¦ä¹ åœ¨è§†è§‰æŽ¨ç†é¢†åŸŸçš„å‘å±•ï¼Œé™ä½Žå¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä¿ƒè¿›AIç³»ç»Ÿåœ¨åŠ¨æ€çŽ¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.

