---
layout: default
title: FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling
---

# FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14056" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14056v1</a>
  <a href="https://arxiv.org/pdf/2512.14056.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14056v1" onclick="toggleFavorite(this, '2512.14056v1', 'FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kim Sung-Bin, Joohyun Chang, David Harwath, Tae-Hyun Oh

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project page: https://facedit.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FacEDiTï¼šé€šè¿‡é¢éƒ¨è¿åŠ¨å¡«å……å®ç°ç»Ÿä¸€çš„è¯´è¯äººè„¸ç¼–è¾‘ä¸ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è¯´è¯äººè„¸ç¼–è¾‘` `äººè„¸ç”Ÿæˆ` `é¢éƒ¨è¿åŠ¨å¡«å……` `æ‰©æ•£æ¨¡å‹` `Transformer`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯´è¯äººè„¸ç¼–è¾‘å’Œç”Ÿæˆæ–¹æ³•é€šå¸¸è¢«è§†ä¸ºç‹¬ç«‹ä»»åŠ¡ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚
2. FacEDiTå°†äºŒè€…ç»Ÿä¸€ä¸ºè¯­éŸ³æ¡ä»¶ä¸‹çš„é¢éƒ¨è¿åŠ¨å¡«å……é—®é¢˜ï¼Œåˆ©ç”¨æ‰©æ•£Transformerå­¦ä¹ åˆæˆå’Œç¼–è¾‘é¢éƒ¨è¿åŠ¨ã€‚
3. FacEDiTåœ¨FacEDiTBenchæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå®ç°äº†å‡†ç¡®çš„è¯­éŸ³å¯¹é½ã€èº«ä»½ä¿æŒå’Œå¹³æ»‘è¿‡æ¸¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†è§’ï¼Œå°†è¯´è¯äººè„¸ç¼–è¾‘å’Œäººè„¸ç”Ÿæˆè§†ä¸ºè¯­éŸ³æ¡ä»¶ä¸‹çš„é¢éƒ¨è¿åŠ¨å¡«å……çš„å­ä»»åŠ¡ã€‚æˆ‘ä»¬æ¢ç´¢äº†é¢éƒ¨è¿åŠ¨å¡«å……ä½œä¸ºä¸€ç§è‡ªç›‘ç£çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œå®ƒä¹Ÿä½œä¸ºåŠ¨æ€è¯´è¯äººè„¸åˆæˆçš„ç»Ÿä¸€å…¬å¼ã€‚ä¸ºäº†å®ç°è¿™ä¸ªæƒ³æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†FacEDiTï¼Œä¸€ä¸ªä½¿ç”¨æµåŒ¹é…è®­ç»ƒçš„è¯­éŸ³æ¡ä»¶æ‰©æ•£Transformerã€‚å—åˆ°æ©ç è‡ªç¼–ç å™¨çš„å¯å‘ï¼ŒFacEDiTå­¦ä¹ åˆæˆè¢«æ©ç›–çš„é¢éƒ¨è¿åŠ¨ï¼Œæ¡ä»¶æ˜¯å‘¨å›´çš„è¿åŠ¨å’Œè¯­éŸ³ã€‚è¿™ç§å…¬å¼èƒ½å¤Ÿè¿›è¡Œå±€éƒ¨ç”Ÿæˆå’Œç¼–è¾‘ï¼Œä¾‹å¦‚æ›¿æ¢ã€æ’å…¥å’Œåˆ é™¤ï¼ŒåŒæ—¶ç¡®ä¿ä¸æœªç¼–è¾‘åŒºåŸŸçš„æ— ç¼è¿‡æ¸¡ã€‚æ­¤å¤–ï¼Œæœ‰åæ³¨æ„åŠ›æœºåˆ¶å’Œæ—¶é—´å¹³æ»‘çº¦æŸå¢å¼ºäº†è¾¹ç•Œè¿ç»­æ€§å’Œå”‡éƒ¨åŒæ­¥ã€‚ä¸ºäº†è§£å†³ç¼ºä¹æ ‡å‡†ç¼–è¾‘åŸºå‡†çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FacEDiTBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯´è¯äººè„¸ç¼–è¾‘çš„æ•°æ®é›†ï¼Œå…·æœ‰å¤šæ ·åŒ–çš„ç¼–è¾‘ç±»å‹å’Œé•¿åº¦ï¼Œä»¥åŠæ–°çš„è¯„ä¼°æŒ‡æ ‡ã€‚å¤§é‡çš„å®éªŒéªŒè¯äº†è¯´è¯äººè„¸ç¼–è¾‘å’Œç”Ÿæˆæ˜¯è¯­éŸ³æ¡ä»¶è¿åŠ¨å¡«å……çš„å­ä»»åŠ¡ï¼›FacEDiTäº§ç”Ÿå‡†ç¡®çš„ã€è¯­éŸ³å¯¹é½çš„é¢éƒ¨ç¼–è¾‘ï¼Œå…·æœ‰å¼ºå¤§çš„èº«ä»½ä¿æŒå’Œå¹³æ»‘çš„è§†è§‰è¿ç»­æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°æ¨å¹¿åˆ°è¯´è¯äººè„¸ç”Ÿæˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•é€šå¸¸å°†è¯´è¯äººè„¸ç¼–è¾‘å’Œç”Ÿæˆè§†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œç¼ºä¹ç»Ÿä¸€çš„å»ºæ¨¡æ¡†æ¶ã€‚è¿™å¯¼è‡´äº†æ¨¡å‹åœ¨ç¼–è¾‘å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´éš¾ä»¥è¿ç§»ï¼Œå¹¶ä¸”éš¾ä»¥ä¿è¯ç¼–è¾‘åŒºåŸŸä¸æœªç¼–è¾‘åŒºåŸŸä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚æ­¤å¤–ï¼Œç¼ºä¹ä¸“é—¨ç”¨äºè¯´è¯äººè„¸ç¼–è¾‘çš„åŸºå‡†æ•°æ®é›†ï¼Œé˜»ç¢äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è¯´è¯äººè„¸ç¼–è¾‘å’Œç”Ÿæˆç»Ÿä¸€å»ºæ¨¡ä¸ºè¯­éŸ³æ¡ä»¶ä¸‹çš„é¢éƒ¨è¿åŠ¨å¡«å……é—®é¢˜ã€‚é€šè¿‡å­¦ä¹ åœ¨ç»™å®šè¯­éŸ³å’Œå‘¨å›´è¿åŠ¨çš„æƒ…å†µä¸‹å¡«å……ç¼ºå¤±çš„é¢éƒ¨è¿åŠ¨ï¼Œæ¨¡å‹å¯ä»¥åŒæ—¶å®ç°ç¼–è¾‘å’Œç”Ÿæˆã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº†æ©ç è‡ªç¼–ç å™¨çš„æ€æƒ³ï¼Œå…è®¸æ¨¡å‹å­¦ä¹ é¢éƒ¨è¿åŠ¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå®ç°å¹³æ»‘çš„è¿‡æ¸¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFacEDiTçš„æ•´ä½“æ¡†æ¶æ˜¯ä¸€ä¸ªè¯­éŸ³æ¡ä»¶æ‰©æ•£Transformerï¼Œå®ƒç”±ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼š1) è¯­éŸ³ç¼–ç å™¨ï¼šå°†è¾“å…¥çš„è¯­éŸ³ä¿¡å·ç¼–ç æˆè¯­éŸ³ç‰¹å¾å‘é‡ã€‚2) é¢éƒ¨è¿åŠ¨ç¼–ç å™¨ï¼šå°†è¾“å…¥çš„é¢éƒ¨è¿åŠ¨åºåˆ—ç¼–ç æˆè¿åŠ¨ç‰¹å¾å‘é‡ã€‚3) æ‰©æ•£Transformerï¼šä¸€ä¸ªåŸºäºTransformerçš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ é¢éƒ¨è¿åŠ¨çš„åˆ†å¸ƒï¼Œå¹¶æ ¹æ®è¯­éŸ³å’Œå‘¨å›´è¿åŠ¨ç”Ÿæˆæˆ–ç¼–è¾‘é¢éƒ¨è¿åŠ¨ã€‚4) æµåŒ¹é…æ¨¡å—ï¼šç”¨äºè®­ç»ƒæ‰©æ•£Transformerï¼Œé€šè¿‡æœ€å°åŒ–é¢„æµ‹è¿åŠ¨å’ŒçœŸå®è¿åŠ¨ä¹‹é—´çš„å·®å¼‚æ¥ä¼˜åŒ–æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šFacEDiTçš„å…³é”®åˆ›æ–°åœ¨äºå°†è¯´è¯äººè„¸ç¼–è¾‘å’Œç”Ÿæˆç»Ÿä¸€å»ºæ¨¡ä¸ºè¯­éŸ³æ¡ä»¶ä¸‹çš„é¢éƒ¨è¿åŠ¨å¡«å……é—®é¢˜ã€‚è¿™ç§ç»Ÿä¸€çš„è§†è§’ä½¿å¾—æ¨¡å‹å¯ä»¥åŒæ—¶å­¦ä¹ ç¼–è¾‘å’Œç”Ÿæˆï¼Œå¹¶ä¸”èƒ½å¤Ÿä¿è¯ç¼–è¾‘åŒºåŸŸä¸æœªç¼–è¾‘åŒºåŸŸä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚æ­¤å¤–ï¼ŒFacEDiTè¿˜å¼•å…¥äº†æœ‰åæ³¨æ„åŠ›æœºåˆ¶å’Œæ—¶é—´å¹³æ»‘çº¦æŸï¼Œä»¥å¢å¼ºè¾¹ç•Œè¿ç»­æ€§å’Œå”‡éƒ¨åŒæ­¥ã€‚

**å…³é”®è®¾è®¡**ï¼šFacEDiTä½¿ç”¨æ‰©æ•£Transformerä½œä¸ºå…¶æ ¸å¿ƒæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨æµåŒ¹é…æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚æ‰©æ•£Transformerç”±å¤šä¸ªTransformerå—ç»„æˆï¼Œæ¯ä¸ªå—åŒ…å«è‡ªæ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚æœ‰åæ³¨æ„åŠ›æœºåˆ¶é€šè¿‡è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä½¿å¾—æ¨¡å‹æ›´åŠ å…³æ³¨è¾¹ç•ŒåŒºåŸŸï¼Œä»è€Œå¢å¼ºè¾¹ç•Œè¿ç»­æ€§ã€‚æ—¶é—´å¹³æ»‘çº¦æŸé€šè¿‡æƒ©ç½šç›¸é‚»å¸§ä¹‹é—´çš„è¿åŠ¨å·®å¼‚ï¼Œä»è€Œä¿è¯è¿åŠ¨çš„å¹³æ»‘æ€§ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æµåŒ¹é…æŸå¤±ã€è¾¹ç•Œè¿ç»­æ€§æŸå¤±å’Œæ—¶é—´å¹³æ»‘æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FacEDiTåœ¨FacEDiTBenchæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œåœ¨å¤šä¸ªç¼–è¾‘ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFacEDiTèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„ã€è¯­éŸ³å¯¹é½çš„é¢éƒ¨ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„èº«ä»½ä¿æŒå’Œå¹³æ»‘çš„è§†è§‰è¿ç»­æ€§ã€‚æ­¤å¤–ï¼ŒFacEDiTè¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°è¯´è¯äººè„¸ç”Ÿæˆä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FacEDiTåœ¨è™šæ‹Ÿå½¢è±¡å®šåˆ¶ã€è§†é¢‘å†…å®¹åˆ›ä½œã€åœ¨çº¿ä¼šè®®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„è¯´è¯äººè„¸è§†é¢‘ï¼Œç¼–è¾‘ç°æœ‰çš„è§†é¢‘å†…å®¹ï¼Œä»¥åŠæ”¹å–„åœ¨çº¿äº¤æµçš„ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„åœºæ™¯ï¼Œä¾‹å¦‚ä¸ªæ€§åŒ–æ•™è‚²ã€è¿œç¨‹åŒ»ç–—ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.

