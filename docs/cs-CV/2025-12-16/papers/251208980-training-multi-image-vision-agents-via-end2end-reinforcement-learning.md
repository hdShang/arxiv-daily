---
layout: default
title: Training Multi-Image Vision Agents via End2End Reinforcement Learning
---

# Training Multi-Image Vision Agents via End2End Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.08980" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.08980</a>
  <a href="https://arxiv.org/pdf/2512.08980.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.08980" onclick="toggleFavorite(this, '2512.08980', 'Training Multi-Image Vision Agents via End2End Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengqi Dong, Chuhuai Yue, Hang He, Rongge Mao, Fenghe Tang, S Kevin Zhou, Zekun Xu, Xiaohan Wang, Jiajun Chai, Wei Lin, Guojun Yin

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIMAgentï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šå›¾è§†è§‰Agentï¼Œè§£å†³å¤æ‚å¤šå›¾QAä»»åŠ¡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šå›¾è§†è§‰Agent` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `å·¥å…·ä½¿ç”¨` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºVLMçš„Agentåœ¨å¤šå›¾QAä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä»…é™äºå•å›¾è¾“å…¥ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å·¥å…·ã€‚
2. IMAgenté€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¹¶å¼•å…¥è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œä½¿Agentèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤šå›¾ä¿¡æ¯å¹¶è¿›è¡Œæ¨ç†ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒIMAgentåœ¨å¤šå›¾QAæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶åœ¨å•å›¾åŸºå‡†ä¸Šä¿æŒäº†ç«äº‰åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºIMAgentï¼Œä¸€ä¸ªå¼€æºçš„è§†è§‰Agentï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä¸“é—¨ç”¨äºå¤„ç†å¤æ‚çš„å¤šå›¾ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šAgentç³»ç»Ÿç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§å’Œè§†è§‰ä¸°å¯Œæ€§çš„å¤šå›¾QAå¯¹ï¼Œä»¥å……åˆ†æ¿€æ´»åŸºç¡€VLMçš„å·¥å…·ä½¿ç”¨æ½œåŠ›ã€‚é€šè¿‡äººå·¥éªŒè¯ï¼Œæ„å»ºäº†åŒ…å«1ä¸‡ä¸ªæ ·æœ¬çš„MIFG-QAæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚é’ˆå¯¹VLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å¿½ç•¥è§†è§‰è¾“å…¥çš„é—®é¢˜ï¼Œå¼€å‘äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨é‡æ–°åˆ†é…å¯¹å›¾åƒå†…å®¹çš„æ³¨æ„åŠ›ã€‚å—ç›Šäºç²¾å¿ƒè®¾è®¡çš„åŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥ï¼ŒIMAgenté€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°äº†ç¨³å®šçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œæ— éœ€æ˜‚è´µçš„ç›‘ç£å¾®è°ƒæ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIMAgentåœ¨ç°æœ‰å•å›¾åŸºå‡†ä¸Šä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶åœ¨æå‡ºçš„å¤šå›¾æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåˆ†æç»“æœä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„Agentï¼Œå¦‚OpenAI O3ï¼Œè™½ç„¶å¯ä»¥é€šè¿‡å·¥å…·ä½¿ç”¨è¿›è¡Œâ€œå›¾åƒæ€è€ƒâ€ï¼Œä½†å¤§å¤šæ•°å¼€æºæ–¹æ³•ä»…é™äºå•å¼ å›¾åƒè¾“å…¥ã€‚è¿™ä½¿å¾—å®ƒä»¬åœ¨å¤„ç†éœ€è¦å¤šå¼ å›¾åƒä¿¡æ¯çš„çœŸå®ä¸–ç•ŒQAä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šå›¾è¾“å…¥ï¼Œå¹¶å……åˆ†åˆ©ç”¨VLMå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIMAgentçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ï¼Œè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šå›¾è¾“å…¥çš„è§†è§‰Agentã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¤šAgentç³»ç»Ÿï¼Œç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ï¼Œå¹¶è®¾è®¡è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œä½¿Agentèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å›¾åƒä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†åŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥ï¼Œä»¥ç¨³å®šå·¥å…·çš„ä½¿ç”¨è¡Œä¸ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIMAgentçš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1) å¤šAgentç³»ç»Ÿï¼šç”¨äºç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ï¼Œä»¥è®­ç»ƒAgentçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚2) è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼šç”¨äºå¸®åŠ©Agentåœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡æ–°å…³æ³¨å›¾åƒå†…å®¹ï¼Œé¿å…å¿½ç•¥è§†è§‰è¾“å…¥ã€‚3) å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼šä½¿ç”¨ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒAgentï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•æœ‰æ•ˆåœ°ä½¿ç”¨å·¥å…·å’Œå¤„ç†å¤šå›¾ä¿¡æ¯ã€‚4) åŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥ï¼šç”¨äºç¨³å®šAgentçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œé¿å…å‡ºç°ä¸ç¨³å®šçš„æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šIMAgentçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) æå‡ºäº†ä¸€ä¸ªåŸºäºç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ çš„å¤šå›¾è§†è§‰Agentè®­ç»ƒæ–¹æ³•ã€‚2) è®¾è®¡äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œä»¥è§£å†³VLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å¿½ç•¥è§†è§‰è¾“å…¥çš„é—®é¢˜ã€‚3) æå‡ºäº†åŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥ï¼Œä»¥ç¨³å®šAgentçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºã€‚4) æ„å»ºäº†ä¸€ä¸ªåŒ…å«1ä¸‡ä¸ªæ ·æœ¬çš„å¤šå›¾QAæ•°æ®é›†MIFG-QAï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°Agentçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šIMAgentçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¤šAgentç³»ç»Ÿçš„è®¾è®¡ï¼Œéœ€è¦è€ƒè™‘å¦‚ä½•ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ã€‚2) è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·çš„è®¾è®¡ï¼Œéœ€è¦è€ƒè™‘å¦‚ä½•æœ‰æ•ˆåœ°å¸®åŠ©Agenté‡æ–°å…³æ³¨å›¾åƒå†…å®¹ã€‚3) å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œéœ€è¦è€ƒè™‘å¦‚ä½•å¼•å¯¼Agentå­¦ä¹ æœ‰æ•ˆä½¿ç”¨å·¥å…·å’Œå¤„ç†å¤šå›¾ä¿¡æ¯ã€‚4) åŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥çš„è®¾è®¡ï¼Œéœ€è¦è€ƒè™‘å¦‚ä½•ç¨³å®šAgentçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.08980/figure/attention_decay4.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.08980/figure/model.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.08980/figure/data_construct.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

IMAgentåœ¨æå‡ºçš„å¤šå›¾æ•°æ®é›†MIFG-QAä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡äº†XX%ã€‚åŒæ—¶ï¼ŒIMAgentåœ¨ç°æœ‰çš„å•å›¾åŸºå‡†ä¸Šä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ä»¥åŠåŠ¨ä½œè½¨è¿¹ä¸¤çº§æ©ç ç­–ç•¥å¯¹IMAgentçš„æ€§èƒ½æå‡èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IMAgentå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©Agentæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å¤šå›¾ä¿¡æ¯ï¼Œä»è€Œæé«˜å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¢æœé¢†åŸŸï¼ŒIMAgentå¯ä»¥ç”¨äºå¤„ç†ç”¨æˆ·ä¸Šä¼ çš„å¤šå¼ å›¾ç‰‡ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„é—®é¢˜å¹¶æä¾›ç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼ŒIMAgentå¯ä»¥ç”¨äºå¤„ç†å¤šä¸ªæ‘„åƒå¤´æ‹æ‘„çš„å›¾åƒï¼Œä»è€Œæ›´å‡†ç¡®åœ°æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒå¹¶åšå‡ºç›¸åº”çš„å†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.

