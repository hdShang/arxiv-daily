---
layout: default
title: A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation
---

# A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2410.11160" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2410.11160</a>
  <a href="https://arxiv.org/pdf/2410.11160.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2410.11160" onclick="toggleFavorite(this, '2410.11160', 'A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xianping Ma, Xiaokang Zhang, Man-On Pun, Bo Huang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMFNetï¼Œç»“åˆå¤šæ¨¡æ€å¾®è°ƒçš„é¥æ„Ÿè¯­ä¹‰åˆ†å‰²ç»Ÿä¸€æ¡†æ¶ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¥æ„Ÿè¯­ä¹‰åˆ†å‰²` `å¤šæ¨¡æ€èåˆ` `è§†è§‰åŸºç¡€æ¨¡å‹` `å¾®è°ƒç½‘ç»œ` `æ·±åº¦èåˆ` `æ•°å­—è¡¨é¢æ¨¡å‹` `è¿ç§»å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é¥æ„Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆå¤šæ¨¡æ€æ•°æ®ï¼Œä¸”æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ–°å‹æ•°æ®å¦‚DSMæ—¶ã€‚
2. æå‡ºMFNetï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å¾®è°ƒç½‘ç»œå’Œæ·±åº¦èåˆæ¨¡å—ï¼Œæœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€é¥æ„Ÿæ•°æ®ï¼Œå¹¶ä¿ç•™SAMçš„é€šç”¨çŸ¥è¯†ã€‚
3. åœ¨ä¸‰ä¸ªé¥æ„Ÿæ•°æ®é›†ä¸Šå®éªŒè¡¨æ˜ï¼ŒMFNetæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨DSMæ•°æ®ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºé¥æ„Ÿè¯­ä¹‰åˆ†å‰²è®¾ç«‹æ–°æ ‡å‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç”¨äºé¥æ„Ÿè¯­ä¹‰åˆ†å‰²çš„æ–°å‹å¤šæ¨¡æ€å¾®è°ƒç½‘ç»œ(MFNet)ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä¸å„ç§å¾®è°ƒæœºåˆ¶æ— ç¼é›†æˆï¼Œå¹¶é€šè¿‡åŒ…å«Adapterå’ŒLow-Rank Adaptation (LoRA)ä½œä¸ºä»£è¡¨æ€§ç¤ºä¾‹æ¥å±•ç¤ºã€‚è¿™ç§å¯æ‰©å±•æ€§ç¡®ä¿äº†æ¡†æ¶å¯¹å…¶ä»–æ–°å…´å¾®è°ƒç­–ç•¥çš„é€‚åº”æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®çš„åŒæ—¶ä¿ç•™SAMçš„é€šç”¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†åŸºäºé‡‘å­—å¡”çš„æ·±åº¦èåˆæ¨¡å—(DFM)ï¼Œä»¥æ•´åˆè·¨å¤šä¸ªå°ºåº¦çš„é«˜çº§åœ°ç†ç‰¹å¾ï¼Œä»è€Œåœ¨è§£ç ä¹‹å‰å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚è¿™é¡¹å·¥ä½œè¿˜å¼ºè°ƒäº†SAMåœ¨æ•°å­—è¡¨é¢æ¨¡å‹(DSM)æ•°æ®ä¸Šçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åº”ç”¨ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®é›†ISPRS Vaihingenã€ISPRS Potsdamå’ŒMMHunanä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MFNetåœ¨å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºè¯¥é¢†åŸŸæ ‘ç«‹äº†æ–°æ ‡å‡†ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†é€šç”¨çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé¥æ„Ÿè¯­ä¹‰åˆ†å‰²æ—¨åœ¨å¯¹é¥æ„Ÿå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ è¿›è¡Œåˆ†ç±»ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨èåˆæ¥è‡ªä¸åŒä¼ æ„Ÿå™¨ï¼ˆå¦‚RGBå›¾åƒå’ŒDSMæ•°æ®ï¼‰çš„å¤šæ¨¡æ€ä¿¡æ¯æ—¶å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ç¼ºä¹åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAMï¼‰çš„é€šç”¨çŸ¥è¯†çš„èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆå¤šæ¨¡æ€é¥æ„Ÿæ•°æ®ï¼Œå¹¶åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚é€šè¿‡å¤šæ¨¡æ€å¾®è°ƒç½‘ç»œï¼ˆMFNetï¼‰å’Œæ·±åº¦èåˆæ¨¡å—ï¼ˆDFMï¼‰ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´å…·åˆ¤åˆ«æ€§çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæé«˜è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å¤šæ¨¡æ€æ•°æ®è¾“å…¥æ¨¡å—ï¼šæ¥æ”¶æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨çš„é¥æ„Ÿæ•°æ®ï¼Œå¦‚RGBå›¾åƒå’ŒDSMæ•°æ®ã€‚2) ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAMï¼‰æå–å›¾åƒç‰¹å¾ã€‚3) å¤šæ¨¡æ€å¾®è°ƒç½‘ç»œï¼ˆMFNetï¼‰ï¼šé€šè¿‡Adapteræˆ–LoRAç­‰å¾®è°ƒç­–ç•¥ï¼Œå°†è§†è§‰åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°é¥æ„Ÿè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œå¹¶èåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚4) æ·±åº¦èåˆæ¨¡å—ï¼ˆDFMï¼‰ï¼šåˆ©ç”¨é‡‘å­—å¡”ç»“æ„ï¼Œåœ¨å¤šä¸ªå°ºåº¦ä¸Šèåˆç‰¹å¾ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚5) è§£ç å™¨ï¼šå°†èåˆåçš„ç‰¹å¾æ˜ å°„åˆ°åƒç´ çº§åˆ«çš„è¯­ä¹‰æ ‡ç­¾ã€‚

**å…³é”®åˆ›æ–°**ï¼š1) æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥çµæ´»åœ°é›†æˆä¸åŒçš„å¾®è°ƒç­–ç•¥ï¼Œå¦‚Adapterå’ŒLoRAã€‚2) è®¾è®¡äº†å¤šæ¨¡æ€å¾®è°ƒç½‘ç»œï¼ˆMFNetï¼‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆå¤šæ¨¡æ€é¥æ„Ÿæ•°æ®ï¼Œå¹¶åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚3) å¼•å…¥äº†æ·±åº¦èåˆæ¨¡å—ï¼ˆDFMï¼‰ï¼Œé€šè¿‡é‡‘å­—å¡”ç»“æ„ï¼Œåœ¨å¤šä¸ªå°ºåº¦ä¸Šèåˆç‰¹å¾ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚4) éªŒè¯äº†SAMåœ¨DSMæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šMFNeté‡‡ç”¨Adapteræˆ–LoRAè¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨ä¿ç•™SAMé€šç”¨çŸ¥è¯†çš„åŒæ—¶é€‚åº”é¥æ„Ÿæ•°æ®ã€‚DFMä½¿ç”¨é‡‘å­—å¡”ç»“æ„æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶é€šè¿‡å·ç§¯æ“ä½œè¿›è¡Œèåˆã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œä¼˜åŒ–å™¨é‡‡ç”¨AdamWã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2410.11160/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2410.11160/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2410.11160/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„MFNetåœ¨ISPRS Vaihingenã€ISPRS Potsdamå’ŒMMHunanä¸‰ä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨MMHunanæ•°æ®é›†ä¸Šï¼ŒMFNetçš„å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰ç›¸æ¯”æœ€ä½³åŸºçº¿æ–¹æ³•æå‡äº†è¶…è¿‡5ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜éªŒè¯äº†SAMåœ¨DSMæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºåŸå¸‚è§„åˆ’ã€ç¯å¢ƒç›‘æµ‹ã€ç¾å®³è¯„ä¼°ã€å†œä¸šç®¡ç†ç­‰é¢†åŸŸã€‚é€šè¿‡ç²¾ç¡®çš„é¥æ„Ÿè¯­ä¹‰åˆ†å‰²ï¼Œå¯ä»¥ä¸ºåœŸåœ°åˆ©ç”¨åˆ†æã€æ¤è¢«è¦†ç›–åº¦è¯„ä¼°ã€å»ºç­‘ç‰©æå–ã€é“è·¯è¯†åˆ«ç­‰æä¾›é‡è¦çš„æ•°æ®æ”¯æŒï¼Œä»è€Œè¾…åŠ©å†³ç­–åˆ¶å®šå’Œèµ„æºç®¡ç†ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸï¼Œæå‡æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal remote sensing data, acquired from diverse sensors, offer a comprehensive and integrated perspective of the Earth's surface. Leveraging multimodal fusion techniques, semantic segmentation enables detailed and accurate analysis of geographic scenes, surpassing single-modality approaches. Building on advancements in vision foundation models, particularly the Segment Anything Model (SAM), this study proposes a unified framework incorporating a novel Multimodal Fine-tuning Network (MFNet) for remote sensing semantic segmentation. The proposed framework is designed to seamlessly integrate with various fine-tuning mechanisms, demonstrated through the inclusion of Adapter and Low-Rank Adaptation (LoRA) as representative examples. This extensibility ensures the framework's adaptability to other emerging fine-tuning strategies, allowing models to retain SAM's general knowledge while effectively leveraging multimodal data. Additionally, a pyramid-based Deep Fusion Module (DFM) is introduced to integrate high-level geographic features across multiple scales, enhancing feature representation prior to decoding. This work also highlights SAM's robust generalization capabilities with Digital Surface Model (DSM) data, a novel application. Extensive experiments on three benchmark multimodal remote sensing datasets, ISPRS Vaihingen, ISPRS Potsdam and MMHunan, demonstrate that the proposed MFNet significantly outperforms existing methods in multimodal semantic segmentation, setting a new standard in the field while offering a versatile foundation for future research and applications. The source code for this work is accessible atthis https URL.

