---
layout: default
title: CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer
---

# CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14560" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14560v1</a>
  <a href="https://arxiv.org/pdf/2512.14560.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14560v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.14560v1', 'CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xianwei Cao, Dou Quan, Shuang Wang, Ning Huyan, Wei Wang, Yunan Li, Licheng Jiao

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 16 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLNetï¼Œé€šè¿‡è·¨è§†è§’å¯¹åº”å…³ç³»å¢å¼ºå›¾åƒæ£€ç´¢åœ°ç†å®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è·¨è§†è§’åœ°ç†å®šä½` `å›¾åƒæ£€ç´¢` `å¯¹åº”å…³ç³»å­¦ä¹ ` `ç‰¹å¾å¯¹é½` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è·¨è§†è§’åœ°ç†å®šä½æ–¹æ³•éš¾ä»¥å»ºæ¨¡æ˜¾å¼çš„ç©ºé—´å¯¹åº”å…³ç³»ï¼Œé™åˆ¶äº†å®šä½ç²¾åº¦ã€‚
2. CLNeté€šè¿‡ç¥ç»å¯¹åº”å›¾ã€éçº¿æ€§åµŒå…¥è½¬æ¢å™¨å’Œå…¨å±€ç‰¹å¾é‡æ ¡å‡†æ¨¡å—ï¼Œæ˜¾å¼åœ°å­¦ä¹ å’Œåˆ©ç”¨è·¨è§†è§’å¯¹åº”å…³ç³»ã€‚
3. åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®éªŒè¡¨æ˜ï¼ŒCLNet è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå›¾åƒæ£€ç´¢çš„è·¨è§†è§’åœ°ç†å®šä½(IRCVGL)æ–¹æ³•ï¼Œæ—¨åœ¨åŒ¹é…ä»æ˜¾è‘—ä¸åŒè§†è§’æ•è·çš„å›¾åƒï¼Œä¾‹å¦‚å«æ˜Ÿå›¾åƒå’Œè¡—æ™¯å›¾åƒã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå­¦ä¹ é²æ£’çš„å…¨å±€è¡¨ç¤ºæˆ–éšå¼çš„ç‰¹å¾å¯¹é½ï¼Œä½†é€šå¸¸æ— æ³•å»ºæ¨¡å¯¹äºç²¾ç¡®å®šä½è‡³å…³é‡è¦çš„æ˜¾å¼ç©ºé—´å¯¹åº”å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºCLNetçš„å¯¹åº”å…³ç³»æ„ŸçŸ¥ç‰¹å¾ç»†åŒ–æ¡†æ¶ï¼Œå®ƒæ˜¾å¼åœ°å¼¥åˆäº†ä¸åŒè§†è§’ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚CLNetå°†è§†è§’å¯¹é½è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªå¯å­¦ä¹ ä¸”äº’è¡¥çš„æ¨¡å—ï¼šç¥ç»å¯¹åº”å›¾(NCM)ï¼Œé€šè¿‡æ½œåœ¨çš„å¯¹åº”å…³ç³»åœºåœ¨ç©ºé—´ä¸Šå¯¹é½è·¨è§†è§’ç‰¹å¾ï¼›éçº¿æ€§åµŒå…¥è½¬æ¢å™¨(NEC)ï¼Œä½¿ç”¨åŸºäºMLPçš„è½¬æ¢é‡æ–°æ˜ å°„è·¨è§†è§’çš„ç‰¹å¾ï¼›ä»¥åŠå…¨å±€ç‰¹å¾é‡æ ¡å‡†(GFR)æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨å­¦ä¹ åˆ°çš„ç©ºé—´çº¿ç´¢çš„æŒ‡å¯¼ä¸‹ï¼Œé‡æ–°åŠ æƒä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾é€šé“ã€‚æ‰€æå‡ºçš„CLNetå¯ä»¥è”åˆæ•è·é«˜å±‚è¯­ä¹‰å’Œç»†ç²’åº¦çš„å¯¹é½ã€‚åœ¨CVUSAã€CVACTã€VIGORå’ŒUniversity-1652å››ä¸ªå…¬å…±åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„CLNetå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè·¨è§†è§’åœ°ç†å®šä½æ—¨åœ¨åŒ¹é…æ¥è‡ªä¸åŒè§†è§’çš„å›¾åƒï¼Œä¾‹å¦‚å«æ˜Ÿå›¾åƒå’Œè¡—æ™¯å›¾åƒã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå­¦ä¹ é²æ£’çš„å…¨å±€ç‰¹å¾æˆ–éšå¼åœ°å¯¹é½ç‰¹å¾ï¼Œä½†å¿½ç•¥äº†æ˜¾å¼ç©ºé—´å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´å®šä½ç²¾åº¦å—é™ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥å¤„ç†è§†è§’å·®å¼‚å¸¦æ¥çš„å‡ ä½•å’Œè¯­ä¹‰å˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCLNetçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡è·¨è§†è§’å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»æ¥æå‡åœ°ç†å®šä½çš„å‡†ç¡®æ€§ã€‚å®ƒå°†è§†è§’å¯¹é½è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå¯å­¦ä¹ çš„æ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£ç©ºé—´å¯¹é½ã€ç‰¹å¾è½¬æ¢å’Œç‰¹å¾é‡æ ¡å‡†ï¼Œä»è€Œå¼¥åˆä¸åŒè§†è§’ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚è¿™ç§æ˜¾å¼å»ºæ¨¡å¯¹åº”å…³ç³»çš„æ–¹å¼èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨å›¾åƒä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œæé«˜å®šä½çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCLNetåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç¥ç»å¯¹åº”å›¾(NCM)ã€éçº¿æ€§åµŒå…¥è½¬æ¢å™¨(NEC)å’Œå…¨å±€ç‰¹å¾é‡æ ¡å‡†(GFR)ã€‚é¦–å…ˆï¼ŒNCMé€šè¿‡å­¦ä¹ æ½œåœ¨çš„å¯¹åº”å…³ç³»åœºï¼Œåœ¨ç©ºé—´ä¸Šå¯¹é½è·¨è§†è§’ç‰¹å¾ã€‚ç„¶åï¼ŒNECä½¿ç”¨åŸºäºMLPçš„è½¬æ¢ï¼Œå°†ç‰¹å¾é‡æ–°æ˜ å°„åˆ°ç»Ÿä¸€çš„è§†è§’ç©ºé—´ã€‚æœ€åï¼ŒGFRæ¨¡å—æ ¹æ®å­¦ä¹ åˆ°çš„ç©ºé—´çº¿ç´¢ï¼Œå¯¹ç‰¹å¾é€šé“è¿›è¡Œé‡åŠ æƒï¼Œçªå‡ºä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¼˜åŒ–è·¨è§†è§’å›¾åƒåŒ¹é…çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLNetçš„å…³é”®åˆ›æ–°åœ¨äºæ˜¾å¼åœ°å»ºæ¨¡è·¨è§†è§’å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚ä¸ä»¥å¾€ä¾èµ–å…¨å±€ç‰¹å¾æˆ–éšå¼å¯¹é½çš„æ–¹æ³•ä¸åŒï¼ŒCLNeté€šè¿‡NCMæ¨¡å—å­¦ä¹ ç©ºé—´å¯¹åº”å…³ç³»ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†è§†è§’å·®å¼‚å¸¦æ¥çš„å‡ ä½•å’Œè¯­ä¹‰å˜åŒ–ã€‚æ­¤å¤–ï¼ŒNECå’ŒGFRæ¨¡å—è¿›ä¸€æ­¥å¢å¼ºäº†ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œæå‡äº†å®šä½çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šNCMæ¨¡å—ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå­¦ä¹ è·¨è§†è§’å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»åœºã€‚NECæ¨¡å—é‡‡ç”¨å¤šå±‚æ„ŸçŸ¥æœº(MLP)è¿›è¡Œéçº¿æ€§ç‰¹å¾è½¬æ¢ã€‚GFRæ¨¡å—ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å¯¹ç‰¹å¾é€šé“è¿›è¡Œé‡åŠ æƒã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åŒ¹é…æŸå¤±å’Œå¯¹åº”å…³ç³»æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®æ ¹æ®ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CLNetåœ¨CVUSAã€CVACTã€VIGORå’ŒUniversity-1652å››ä¸ªå…¬å…±åŸºå‡†ä¸Šéƒ½å–å¾—äº†SOTAæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨CVUSAæ•°æ®é›†ä¸Šï¼ŒCLNetçš„Recall@1æŒ‡æ ‡ç›¸æ¯”äºä¹‹å‰çš„æœ€ä½³æ–¹æ³•æå‡äº†æ˜¾è‘—çš„ç™¾åˆ†æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLNetèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è§†è§’å·®å¼‚ï¼Œæé«˜è·¨è§†è§’å›¾åƒåŒ¹é…çš„å‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CLNetåœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€åŸå¸‚è§„åˆ’ã€ç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨å«æ˜Ÿå›¾åƒå’Œè¡—æ™¯å›¾åƒè¿›è¡Œç²¾ç¡®å®šä½ï¼Œå¸®åŠ©è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å¤æ‚çš„åŸå¸‚ç¯å¢ƒä¸­å®‰å…¨è¡Œé©¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæ„å»ºå¤§è§„æ¨¡çš„åœ°ç†ä¿¡æ¯ç³»ç»Ÿï¼Œä¸ºåŸå¸‚è§„åˆ’å’Œç®¡ç†æä¾›æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.

