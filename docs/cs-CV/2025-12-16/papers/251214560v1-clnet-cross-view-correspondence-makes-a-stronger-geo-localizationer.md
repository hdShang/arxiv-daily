---
layout: default
title: CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer
---

# CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer

**arXiv**: [2512.14560v1](https://arxiv.org/abs/2512.14560) | [PDF](https://arxiv.org/pdf/2512.14560.pdf)

**ä½œè€…**: Xianwei Cao, Dou Quan, Shuang Wang, Ning Huyan, Wei Wang, Yunan Li, Licheng Jiao

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 16 pages, 6 figures

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLNetæ¡†æž¶ï¼Œé€šè¿‡æ˜¾å¼è·¨è§†è§’å¯¹åº”å…³ç³»è§£å†³å›¾åƒæ£€ç´¢å¼è·¨è§†è§’åœ°ç†å®šä½é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡**

**å…³é”®è¯**: `è·¨è§†è§’åœ°ç†å®šä½` `å›¾åƒæ£€ç´¢` `å¯¹åº”å…³ç³»å»ºæ¨¡` `ç‰¹å¾å¯¹é½` `ç¥žç»ç½‘ç»œ` `è®¡ç®—æœºè§†è§‰` `è¯­ä¹‰å‡ ä½•èžåˆ` `å¯è§£é‡ŠAI`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–å…¨å±€è¡¨ç¤ºæˆ–éšå¼å¯¹é½ï¼Œéš¾ä»¥å»ºæ¨¡è·¨è§†è§’çš„æ˜¾å¼ç©ºé—´å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´åœ°ç†å®šä½ç²¾åº¦å—é™ã€‚
2. æå‡ºCLNetæ¡†æž¶ï¼Œé€šè¿‡ç¥žç»å¯¹åº”å›¾ã€éžçº¿æ€§åµŒå…¥è½¬æ¢å™¨å’Œå…¨å±€ç‰¹å¾é‡æ ¡å‡†æ¨¡å—ï¼Œæ˜¾å¼å­¦ä¹ è·¨è§†è§’çš„è¯­ä¹‰å’Œå‡ ä½•å¯¹åº”ã€‚
3. åœ¨CVUSAç­‰å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®žçŽ°SOTAæ€§èƒ½ï¼Œæ˜¾è‘—æå‡å®šä½ç²¾åº¦ï¼Œå¹¶å¢žå¼ºæ¨¡åž‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºŽå›¾åƒæ£€ç´¢çš„è·¨è§†è§’åœ°ç†å®šä½ï¼ˆIRCVGLï¼‰æ—¨åœ¨åŒ¹é…ä»Žæ˜¾è‘—ä¸åŒè§†è§’ï¼ˆå¦‚å«æ˜Ÿå’Œè¡—æ™¯ï¼‰æ•èŽ·çš„å›¾åƒã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å­¦ä¹ é²æ£’çš„å…¨å±€è¡¨ç¤ºæˆ–éšå¼ç‰¹å¾å¯¹é½ï¼Œå¾€å¾€éš¾ä»¥å»ºæ¨¡å¯¹ç²¾ç¡®å®šä½è‡³å…³é‡è¦çš„æ˜¾å¼ç©ºé—´å¯¹åº”å…³ç³»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹åº”æ„ŸçŸ¥ç‰¹å¾ç»†åŒ–æ¡†æž¶ï¼Œç§°ä¸ºCLNetï¼Œå®ƒæ˜¾å¼åœ°æ¡¥æŽ¥äº†ä¸åŒè§†è§’ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•é¸¿æ²Ÿã€‚CLNetå°†è§†è§’å¯¹é½è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªå¯å­¦ä¹ ä¸”äº’è¡¥çš„æ¨¡å—ï¼šç¥žç»å¯¹åº”å›¾ï¼ˆNCMï¼‰ï¼Œé€šè¿‡æ½œåœ¨å¯¹åº”åœºåœ¨ç©ºé—´ä¸Šå¯¹é½è·¨è§†è§’ç‰¹å¾ï¼›éžçº¿æ€§åµŒå…¥è½¬æ¢å™¨ï¼ˆNECï¼‰ï¼Œä½¿ç”¨åŸºäºŽMLPçš„å˜æ¢è·¨è§†è§’é‡æ˜ å°„ç‰¹å¾ï¼›ä»¥åŠå…¨å±€ç‰¹å¾é‡æ ¡å‡†ï¼ˆGFRï¼‰æ¨¡å—ï¼Œé€šè¿‡å­¦ä¹ åˆ°çš„ç©ºé—´çº¿ç´¢å¼•å¯¼é‡æ–°åŠ æƒä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾é€šé“ã€‚æ‰€æå‡ºçš„CLNetèƒ½å¤Ÿè”åˆæ•èŽ·é«˜çº§è¯­ä¹‰å’Œç»†ç²’åº¦å¯¹é½ã€‚åœ¨å››ä¸ªå…¬å…±åŸºå‡†æ•°æ®é›†ï¼ˆCVUSAã€CVACTã€VIGORå’ŒUniversity-1652ï¼‰ä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬æå‡ºçš„CLNetå®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è§£å†³å›¾åƒæ£€ç´¢å¼è·¨è§†è§’åœ°ç†å®šä½ï¼ˆIRCVGLï¼‰é—®é¢˜ï¼Œå³åŒ¹é…å«æ˜Ÿä¸Žè¡—æ™¯ç­‰ä¸åŒè§†è§’å›¾åƒä»¥ç¡®å®šåœ°ç†ä½ç½®ã€‚çŽ°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å­¦ä¹ å…¨å±€ç‰¹å¾è¡¨ç¤ºæˆ–è¿›è¡Œéšå¼ç‰¹å¾å¯¹é½ï¼Œç¼ºä¹å¯¹è·¨è§†è§’é—´æ˜¾å¼ç©ºé—´å¯¹åº”å…³ç³»çš„å»ºæ¨¡ï¼Œè¿™é™åˆ¶äº†å®šä½ç²¾åº¦ï¼Œå› ä¸ºä¸åŒè§†è§’çš„å›¾åƒåœ¨å‡ ä½•ç»“æž„å’Œè¯­ä¹‰å†…å®¹ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºCLNetæ¡†æž¶ï¼Œå…¶æ ¸å¿ƒæ€è·¯æ˜¯æ˜¾å¼åœ°å»ºæ¨¡è·¨è§†è§’çš„å¯¹åº”å…³ç³»ï¼Œä»¥æ¡¥æŽ¥è¯­ä¹‰å’Œå‡ ä½•é¸¿æ²Ÿã€‚é€šè¿‡å°†è§†è§’å¯¹é½è¿‡ç¨‹åˆ†è§£ä¸ºå¯å­¦ä¹ çš„æ¨¡å—ï¼ŒCLNetèƒ½å¤ŸåŒæ—¶æ•èŽ·é«˜çº§è¯­ä¹‰ä¿¡æ¯å’Œç»†ç²’åº¦çš„ç©ºé—´å¯¹é½ï¼Œä»Žè€Œæå‡ç‰¹å¾è¡¨ç¤ºçš„åˆ¤åˆ«æ€§å’Œé²æ£’æ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å…‹æœçŽ°æœ‰æ–¹æ³•ä¸­å¯¹åº”å…³ç³»å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥ç»“æž„åŒ–ç»„ä»¶æ¥å¢žå¼ºç‰¹å¾äº¤äº’ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCLNetçš„æ•´ä½“æž¶æž„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šç¥žç»å¯¹åº”å›¾ï¼ˆNCMï¼‰ã€éžçº¿æ€§åµŒå…¥è½¬æ¢å™¨ï¼ˆNECï¼‰å’Œå…¨å±€ç‰¹å¾é‡æ ¡å‡†ï¼ˆGFRï¼‰ã€‚NCMé€šè¿‡æ½œåœ¨å¯¹åº”åœºåœ¨ç©ºé—´ä¸Šå¯¹é½è·¨è§†è§’ç‰¹å¾ï¼Œå­¦ä¹ ç‰¹å¾ç‚¹ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼›NECä½¿ç”¨åŸºäºŽå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„å˜æ¢å°†ç‰¹å¾ä»Žä¸€ä¸ªè§†è§’é‡æ˜ å°„åˆ°å¦ä¸€ä¸ªè§†è§’ï¼Œä»¥é€‚åº”è§†è§’å·®å¼‚ï¼›GFRåˆ™æ ¹æ®å­¦ä¹ åˆ°çš„ç©ºé—´çº¿ç´¢é‡æ–°åŠ æƒç‰¹å¾é€šé“ï¼Œçªå‡ºä¿¡æ¯ä¸°å¯Œçš„éƒ¨åˆ†ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œå½¢æˆä¸€ä¸ªç«¯åˆ°ç«¯çš„è®­ç»ƒæ¡†æž¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºŽæ˜¾å¼åœ°å»ºæ¨¡è·¨è§†è§’å¯¹åº”å…³ç³»ï¼Œè€Œéžä¾èµ–éšå¼å¯¹é½ã€‚CLNeté€šè¿‡NCMå¼•å…¥ç©ºé—´å¯¹åº”åœºï¼Œç›´æŽ¥å­¦ä¹ ç‰¹å¾é—´çš„å‡ ä½•å¯¹åº”ï¼Œè¿™æ˜¯ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚æ­¤å¤–ï¼Œæ¨¡å—åŒ–è®¾è®¡å…è®¸è”åˆä¼˜åŒ–è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œæå‡äº†ç‰¹å¾è¡¨ç¤ºçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šNCMä½¿ç”¨å·ç§¯ç½‘ç»œç”Ÿæˆå¯¹åº”åœºï¼Œè®¡ç®—ç‰¹å¾ç›¸ä¼¼æ€§ä»¥å¯¹é½ç©ºé—´ä½ç½®ï¼›NECåŸºäºŽMLPå®žçŽ°éžçº¿æ€§å˜æ¢ï¼Œå‚æ•°å…±äº«ä»¥å¢žå¼ºæ³›åŒ–ï¼›GFRé‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®å¯¹åº”çº¿ç´¢è°ƒæ•´é€šé“æƒé‡ã€‚æŸå¤±å‡½æ•°é€šå¸¸ç»“åˆåˆ†ç±»æŸå¤±å’Œå¯¹æ¯”æŸå¤±ï¼Œä»¥ä¼˜åŒ–ç‰¹å¾åˆ¤åˆ«æ€§ã€‚ç½‘ç»œç»“æž„åŸºäºŽéª¨å¹²ç½‘ç»œï¼ˆå¦‚ResNetï¼‰æå–ç‰¹å¾ï¼ŒåŽæŽ¥CLNetæ¨¡å—è¿›è¡Œç»†åŒ–ï¼Œå…·ä½“å‚æ•°è®¾ç½®å¦‚å±‚æ•°å’Œç»´åº¦æ ¹æ®æ•°æ®é›†è°ƒæ•´ï¼Œä¾‹å¦‚åœ¨CVUSAä¸Šä½¿ç”¨ResNet-50ä½œä¸ºéª¨å¹²ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨CVUSAã€CVACTã€VIGORå’ŒUniversity-1652å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒCLNetå‡è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨CVUSAä¸Šï¼Œtop-1å‡†ç¡®çŽ‡æå‡çº¦2-3ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰åŸºçº¿æ–¹æ³•å¦‚SAFAå’ŒCVMNetã€‚å®žéªŒæ˜¾ç¤ºï¼ŒCLNetåœ¨è·¨æ•°æ®é›†æ³›åŒ–æµ‹è¯•ä¸­ä¹Ÿè¡¨çŽ°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨è‡ªåŠ¨é©¾é©¶ã€æ— äººæœºå¯¼èˆªã€å¢žå¼ºçŽ°å®žå’Œæ™ºèƒ½åŸŽå¸‚ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡è·¨è§†è§’åœ°ç†å®šä½çš„ç²¾åº¦ï¼Œå¯æ”¯æŒè½¦è¾†å®šä½ã€è·¯å¾„è§„åˆ’ã€è™šæ‹Ÿåœ°å›¾å åŠ ç­‰ä»»åŠ¡ï¼Œå¢žå¼ºç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚æœªæ¥å¯èƒ½æŽ¨åŠ¨å¤šæ¨¡æ€å®šä½ç³»ç»Ÿçš„å‘å±•ï¼Œä¸ºæœºå™¨äººè§†è§‰å’Œåœ°ç†ä¿¡æ¯ç³»ç»Ÿæä¾›æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.

