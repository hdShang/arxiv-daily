---
layout: default
title: Enhancing Interpretability for Vision Models via Shapley Value Optimization
---

# Enhancing Interpretability for Vision Models via Shapley Value Optimization

**arXiv**: [2512.14354v1](https://arxiv.org/abs/2512.14354) | [PDF](https://arxiv.org/pdf/2512.14354.pdf)

**ä½œè€…**: Kanglong Fan, Yunqiao Yang, Chen Ma

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Accepted to AAAI2026

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽShapleyå€¼ä¼˜åŒ–çš„è‡ªè§£é‡Šæ¡†æž¶ï¼Œæå‡è§†è§‰æ¨¡åž‹çš„å¯è§£é‡Šæ€§ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)** **3Dæ„ŸçŸ¥ä¸ŽçŠ¶æ€ä¼°è®¡ (Perception & State Est)**

**å…³é”®è¯**: `å¯è§£é‡Šæ€§` `Shapleyå€¼` `æ·±åº¦ç¥žç»ç½‘ç»œ` `è‡ªè§£é‡Šæ¨¡åž‹` `è®¡ç®—æœºè§†è§‰`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ·±åº¦ç¥žç»ç½‘ç»œå†³ç­–è¿‡ç¨‹ä¸é€æ˜Žï¼ŒçŽ°æœ‰äº‹åŽè§£é‡Šæ–¹æ³•éš¾ä»¥å¿ å®žåæ˜ æ¨¡åž‹è¡Œä¸ºï¼Œè‡ªè§£é‡Šç½‘ç»œåˆ™ç‰ºç‰²æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚
2. æå‡ºä¸€ç§è‡ªè§£é‡Šæ¡†æž¶ï¼Œå°†Shapleyå€¼ä¼°è®¡ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œç¡®ä¿è§£é‡Šä¸Žæ¨¡åž‹å†³ç­–é€»è¾‘å¯¹é½ï¼Œå¹¶ä¿æŒæ¨¡åž‹æ€§èƒ½ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ¨¡åž‹æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦ç¥žç»ç½‘ç»œåœ¨å„ä¸ªé¢†åŸŸéƒ½è¡¨çŽ°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶å†³ç­–è¿‡ç¨‹ä»ç„¶ä¸é€æ˜Žã€‚å°½ç®¡è®¸å¤šè§£é‡Šæ–¹æ³•è‡´åŠ›äºŽæ­ç¤ºDNNçš„æ¨¡ç³Šæ€§ï¼Œä½†å®ƒä»¬ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼šäº‹åŽè§£é‡Šæ–¹æ³•é€šå¸¸éš¾ä»¥å¿ å®žåœ°åæ˜ æ¨¡åž‹è¡Œä¸ºï¼Œè€Œè‡ªè§£é‡Šç¥žç»ç½‘ç»œç”±äºŽå…¶ä¸“é—¨çš„æž¶æž„è®¾è®¡è€Œç‰ºç‰²äº†æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªè§£é‡Šæ¡†æž¶ï¼Œè¯¥æ¡†æž¶åœ¨è®­ç»ƒæœŸé—´å°†Shapleyå€¼ä¼°è®¡ä½œä¸ºè¾…åŠ©ä»»åŠ¡é›†æˆï¼Œä»Žè€Œå®žçŽ°äº†ä¸¤ä¸ªå…³é”®è¿›å±•ï¼š1ï¼‰å°†æ¨¡åž‹é¢„æµ‹åˆ†æ•°å…¬å¹³åœ°åˆ†é…ç»™å›¾åƒå—ï¼Œç¡®ä¿è§£é‡Šä¸Žæ¨¡åž‹çš„å†³ç­–é€»è¾‘å†…åœ¨å¯¹é½ï¼›2ï¼‰é€šè¿‡å¾®å°çš„ç»“æž„ä¿®æ”¹å¢žå¼ºå¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¿æŒæ¨¡åž‹æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†ä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®žçŽ°äº†æœ€å…ˆè¿›çš„å¯è§£é‡Šæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ·±åº¦ç¥žç»ç½‘ç»œçš„å¯è§£é‡Šæ€§ä¸è¶³ï¼Œå¯¼è‡´éš¾ä»¥ç†è§£æ¨¡åž‹çš„å†³ç­–ä¾æ®ã€‚äº‹åŽè§£é‡Šæ–¹æ³•ï¼ˆpost-hoc explanation methodsï¼‰é€šå¸¸æ— æ³•å‡†ç¡®åæ˜ æ¨¡åž‹çš„çœŸå®žè¡Œä¸ºï¼Œè€Œè‡ªè§£é‡Šç¥žç»ç½‘ç»œï¼ˆself-explaining neural networksï¼‰è™½ç„¶èƒ½æä¾›ä¸€å®šçš„å¯è§£é‡Šæ€§ï¼Œä½†å¾€å¾€éœ€è¦ç‰¹å®šçš„ç½‘ç»œç»“æž„è®¾è®¡ï¼Œä»Žè€Œç‰ºç‰²äº†æ¨¡åž‹çš„æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§æ—¢èƒ½æä¾›è‰¯å¥½å¯è§£é‡Šæ€§ï¼Œåˆèƒ½ä¿æŒæ¨¡åž‹æ€§èƒ½å’Œå…¼å®¹æ€§çš„æ–¹æ³•æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†Shapleyå€¼ä¼°è®¡é›†æˆåˆ°æ¨¡åž‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½œä¸ºä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ã€‚Shapleyå€¼æ˜¯ä¸€ç§åšå¼ˆè®ºæ¦‚å¿µï¼Œå¯ä»¥å…¬å¹³åœ°å°†æ¨¡åž‹é¢„æµ‹åˆ†æ•°åˆ†é…ç»™è¾“å…¥å›¾åƒçš„å„ä¸ªéƒ¨åˆ†ï¼ˆä¾‹å¦‚å›¾åƒå—ï¼‰ã€‚é€šè¿‡ä¼˜åŒ–Shapleyå€¼ï¼Œå¯ä»¥ä½¿æ¨¡åž‹å­¦ä¹ åˆ°æ›´åŠ å¯è§£é‡Šçš„ç‰¹å¾è¡¨ç¤ºï¼Œä»Žè€Œä½¿æ¨¡åž‹çš„å†³ç­–è¿‡ç¨‹æ›´åŠ é€æ˜Žã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¿®æ”¹çŽ°æœ‰çš„ç¥žç»ç½‘ç»œç»“æž„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¢žåŠ ä¸€ä¸ªShapleyå€¼ä¼°è®¡æ¨¡å—ã€‚è¯¥æ¨¡å—è´Ÿè´£ä¼°è®¡æ¯ä¸ªè¾“å…¥å›¾åƒå—å¯¹æœ€ç»ˆé¢„æµ‹ç»“æžœçš„è´¡çŒ®ï¼Œå¹¶å°†å…¶ä½œä¸ºæ¨¡åž‹çš„è§£é‡Šã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1ï¼‰ä½¿ç”¨æ ‡å‡†çš„ç›‘ç£å­¦ä¹ ç›®æ ‡è®­ç»ƒæ¨¡åž‹ï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®åœ°è¿›è¡Œé¢„æµ‹ï¼›2ï¼‰ä½¿ç”¨Shapleyå€¼ä¼°è®¡æ¨¡å—æä¾›çš„è§£é‡Šï¼Œä¼˜åŒ–æ¨¡åž‹çš„ç‰¹å¾è¡¨ç¤ºï¼Œä½¿å…¶æ›´åŠ å¯è§£é‡Šã€‚è¿™ä¸¤ä¸ªéƒ¨åˆ†é€šè¿‡ä¸€ä¸ªè”åˆæŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œä»Žè€Œä½¿æ¨¡åž‹åœ¨ä¿æŒé¢„æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿèƒ½å¤Ÿæä¾›è‰¯å¥½çš„å¯è§£é‡Šæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå°†Shapleyå€¼ä¼°è®¡é›†æˆåˆ°æ¨¡åž‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½œä¸ºä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ã€‚ä¸Žä¼ ç»Ÿçš„äº‹åŽè§£é‡Šæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡åž‹è®­ç»ƒé˜¶æ®µå°±è€ƒè™‘äº†å¯è§£é‡Šæ€§ï¼Œä»Žè€Œä½¿æ¨¡åž‹å­¦ä¹ åˆ°æ›´åŠ å¯è§£é‡Šçš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¾®å°çš„ç»“æž„ä¿®æ”¹ï¼Œå®žçŽ°äº†å¯è§£é‡Šæ€§çš„æå‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡åž‹çš„æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•ä¼°è®¡Shapleyå€¼ï¼Œä»¥é™ä½Žè®¡ç®—å¤æ‚åº¦ï¼›2ï¼‰è®¾è®¡äº†ä¸€ä¸ªè”åˆæŸå¤±å‡½æ•°ï¼ŒåŒæ—¶ä¼˜åŒ–æ¨¡åž‹çš„é¢„æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§ï¼›3ï¼‰é€šè¿‡å¾®è°ƒçŽ°æœ‰çš„ç¥žç»ç½‘ç»œç»“æž„ï¼Œå°†Shapleyå€¼ä¼°è®¡æ¨¡å—é›†æˆåˆ°æ¨¡åž‹ä¸­ï¼Œè€Œæ— éœ€ä»Žå¤´å¼€å§‹è®¾è®¡æ–°çš„ç½‘ç»œç»“æž„ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ŽåŽŸå§‹æ¨¡åž‹ç›¸å½“çš„é¢„æµ‹æ€§èƒ½ã€‚ä¸ŽçŽ°æœ‰çš„äº‹åŽè§£é‡Šæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæä¾›æ›´åŠ å‡†ç¡®å’Œå¯é çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å…·æœ‰è‰¯å¥½çš„å…¼å®¹æ€§ï¼Œå¯ä»¥åº”ç”¨äºŽå„ç§ä¸åŒçš„ç¥žç»ç½‘ç»œç»“æž„ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå¯¹æ¨¡åž‹å¯è§£é‡Šæ€§æœ‰è¾ƒé«˜è¦æ±‚çš„é¢†åŸŸï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èžé£ŽæŽ§ç­‰ã€‚é€šè¿‡æä¾›æ¸…æ™°çš„å†³ç­–ä¾æ®ï¼Œå¯ä»¥å¢žå¼ºç”¨æˆ·å¯¹æ¨¡åž‹çš„ä¿¡ä»»ï¼Œå¹¶å¸®åŠ©é¢†åŸŸä¸“å®¶æ›´å¥½åœ°ç†è§£æ¨¡åž‹çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºŽæ¨¡åž‹è°ƒè¯•å’Œä¼˜åŒ–ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å‘çŽ°æ¨¡åž‹ä¸­çš„æ½œåœ¨é—®é¢˜ï¼Œå¹¶æ”¹è¿›æ¨¡åž‹çš„æ€§èƒ½ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.

