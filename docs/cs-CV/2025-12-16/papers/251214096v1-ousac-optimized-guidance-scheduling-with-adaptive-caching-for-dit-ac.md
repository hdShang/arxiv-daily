---
layout: default
title: OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration
---

# OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration

**arXiv**: [2512.14096v1](https://arxiv.org/abs/2512.14096) | [PDF](https://arxiv.org/pdf/2512.14096.pdf)

**ä½œè€…**: Ruitong Sun, Tianze Yang, Wei Niu, Jin Sun

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 29 pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOUSACæ¡†æž¶ä»¥è§£å†³æ‰©æ•£æ¨¡åž‹ä¸­æ— åˆ†ç±»å™¨å¼•å¯¼è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ï¼Œé€šè¿‡ä¼˜åŒ–å¼•å¯¼è°ƒåº¦ä¸Žè‡ªé€‚åº”ç¼“å­˜å®žçŽ°é«˜æ•ˆåŠ é€Ÿã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `æ‰©æ•£æ¨¡åž‹åŠ é€Ÿ` `æ— åˆ†ç±»å™¨å¼•å¯¼ä¼˜åŒ–` `ç¨€ç–è®¡ç®—è°ƒåº¦` `è‡ªé€‚åº”ç¼“å­˜` `æ‰©æ•£å˜æ¢å™¨` `è¿›åŒ–ç®—æ³•` `å›¾åƒç”Ÿæˆæ•ˆçŽ‡` `è®¡ç®—èŠ‚çœ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ‰©æ•£æ¨¡åž‹ä¸­æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰è™½æå‡è´¨é‡ï¼Œä½†éœ€åœ¨æ¯ä¸ªæ—¶é—´æ­¥æ‰§è¡Œä¸¤æ¬¡å‰å‘ä¼ æ’­ï¼Œè®¡ç®—å¼€é”€åŠ å€ï¼Œæˆä¸ºåŠ é€Ÿç“¶é¢ˆã€‚
2. æå‡ºOUSACæ¡†æž¶ï¼Œé€šè¿‡å¯å˜å¼•å¯¼å°ºåº¦å®žçŽ°ç¨€ç–è®¡ç®—ï¼Œç»“åˆè¿›åŒ–ç®—æ³•ä¼˜åŒ–è°ƒåº¦å’Œè‡ªé€‚åº”ç¼“å­˜ï¼Œå‡å°‘CFGæ­¥æ•°å¹¶ä¿æŒè´¨é‡ã€‚
3. å®žéªŒæ˜¾ç¤ºOUSACåœ¨å¤šä¸ªæ¨¡åž‹ä¸Šæ˜¾è‘—èŠ‚çœè®¡ç®—å¹¶æå‡è´¨é‡ï¼Œå¦‚DiT-XL/2èŠ‚çœ53%è®¡ç®—ä¸”è´¨é‡æå‡15%ï¼Œä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ‰©æ•£æ¨¡åž‹å·²æˆä¸ºé«˜è´¨é‡å›¾åƒç”Ÿæˆçš„ä¸»å¯¼èŒƒå¼ï¼Œä½†å…¶è¿­ä»£åŽ»å™ªè¿‡ç¨‹è®¡ç®—å¼€é”€å·¨å¤§ã€‚æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡å’Œå¯æŽ§æ€§ï¼Œä½†éœ€è¦åœ¨æ¯ä¸ªæ—¶é—´æ­¥åŒæ—¶æ‰§è¡Œæ¡ä»¶å‰å‘ä¼ æ’­å’Œæ— æ¡ä»¶å‰å‘ä¼ æ’­ï¼Œå¯¼è‡´è®¡ç®—é‡åŠ å€ã€‚æœ¬æ–‡æå‡ºäº†OUSACï¼ˆä¼˜åŒ–å¼•å¯¼è°ƒåº¦ä¸Žè‡ªé€‚åº”ç¼“å­˜ï¼‰æ¡†æž¶ï¼Œé€šè¿‡ç³»ç»Ÿä¼˜åŒ–åŠ é€Ÿæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ´žå¯Ÿæ˜¯ï¼šå¯å˜çš„å¼•å¯¼å°ºåº¦å¯ä»¥å®žçŽ°ç¨€ç–è®¡ç®—â€”â€”åœ¨æŸäº›æ—¶é—´æ­¥è°ƒæ•´å¼•å¯¼å°ºåº¦å¯ä»¥è¡¥å¿åœ¨å…¶ä»–æ—¶é—´æ­¥è·³è¿‡CFGçš„æ“ä½œï¼Œä»Žè€Œåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶å‡å°‘æ€»é‡‡æ ·æ­¥æ•°å’ŒCFGæ­¥æ•°ã€‚ç„¶è€Œï¼Œå¯å˜å¼•å¯¼æ¨¡å¼ä¼šå¼•å…¥åŽ»å™ªåå·®ï¼Œç ´åæ ‡å‡†ç¼“å­˜æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ˆè¿™äº›æ–¹æ³•å‡è®¾CFGå°ºåº¦åœ¨æ­¥é—´æ’å®šï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨åŠ¨æ€æ¡ä»¶ä¸‹ï¼Œä¸åŒçš„å˜æ¢å™¨å—å—åˆ°çš„å½±å“ç¨‹åº¦ä¸åŒã€‚æœ¬æ–‡åŸºäºŽè¿™äº›æ´žå¯Ÿå¼€å‘äº†ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨è¿›åŒ–ç®—æ³•è”åˆä¼˜åŒ–è·³è¿‡å“ªäº›æ—¶é—´æ­¥ä»¥åŠä½¿ç”¨ä½•ç§å¼•å¯¼å°ºåº¦ï¼Œæœ€å¤šå¯æ¶ˆé™¤82%çš„æ— æ¡ä»¶å‰å‘ä¼ æ’­ã€‚ç¬¬äºŒé˜¶æ®µå¼•å…¥è‡ªé€‚åº”ç§©åˆ†é…ï¼Œé’ˆå¯¹æ¯ä¸ªå˜æ¢å™¨å—å®šåˆ¶æ ¡å‡†å·¥ä½œï¼Œåœ¨å¯å˜å¼•å¯¼ä¸‹ä¿æŒç¼“å­˜æœ‰æ•ˆæ€§ã€‚å®žéªŒè¡¨æ˜Žï¼ŒOUSACæ˜¾è‘—ä¼˜äºŽæœ€å…ˆè¿›çš„åŠ é€Ÿæ–¹æ³•ï¼šåœ¨DiT-XL/2ï¼ˆImageNet 512x512ï¼‰ä¸Šå®žçŽ°53%çš„è®¡ç®—èŠ‚çœå’Œ15%çš„è´¨é‡æå‡ï¼Œåœ¨PixArt-alphaï¼ˆMSCOCOï¼‰ä¸Šå®žçŽ°60%çš„èŠ‚çœå’Œ16.1%çš„æå‡ï¼Œåœ¨FLUXä¸Šå®žçŽ°5å€åŠ é€Ÿä¸”CLIPåˆ†æ•°è¶…è¿‡50æ­¥åŸºçº¿ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

OUSACæ¡†æž¶é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ã€‚æ•´ä½“æ¡†æž¶åŒ…æ‹¬ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨è¿›åŒ–ç®—æ³•è”åˆä¼˜åŒ–æ—¶é—´æ­¥è·³è¿‡ç­–ç•¥å’Œå¼•å¯¼å°ºåº¦ï¼Œå®žçŽ°ç¨€ç–è®¡ç®—ï¼Œæœ€å¤šå‡å°‘82%æ— æ¡ä»¶å‰å‘ä¼ æ’­ï¼›ç¬¬äºŒé˜¶æ®µå¼•å…¥è‡ªé€‚åº”ç§©åˆ†é…ï¼Œé’ˆå¯¹æ‰©æ•£å˜æ¢å™¨ä¸­ä¸åŒå—åœ¨åŠ¨æ€å¼•å¯¼ä¸‹çš„æ•æ„Ÿæ€§å·®å¼‚ï¼Œå®šåˆ¶åŒ–æ ¡å‡†ç¼“å­˜ï¼Œä»¥åº”å¯¹å¯å˜å¼•å¯¼å¯¼è‡´çš„åŽ»å™ªåå·®ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå¯å˜å¼•å¯¼å°ºåº¦çš„è°ƒåº¦ä¼˜åŒ–å’Œè‡ªé€‚åº”ç¼“å­˜æœºåˆ¶ï¼Œä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼šä¼ ç»Ÿæ–¹æ³•å‡è®¾æ’å®šCFGå°ºåº¦ï¼Œè€ŒOUSACå…è®¸å°ºåº¦å˜åŒ–ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿä¼˜åŒ–å’Œè‡ªé€‚åº”è®¾è®¡ç»´æŒç¼“å­˜æœ‰æ•ˆæ€§ï¼Œä»Žè€Œåœ¨å‡å°‘è®¡ç®—çš„åŒæ—¶ä¿æŒæˆ–æå‡ç”Ÿæˆè´¨é‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

OUSACåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼šDiT-XL/2ä¸ŠèŠ‚çœ53%è®¡ç®—ä¸”è´¨é‡æå‡15%ï¼ŒPixArt-alphaä¸ŠèŠ‚çœ60%è®¡ç®—ä¸”æå‡16.1%ï¼ŒFLUXä¸Šå®žçŽ°5å€åŠ é€Ÿå¹¶è¶…è¶ŠåŸºçº¿CLIPåˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰åŠ é€Ÿæ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽéœ€è¦é«˜æ•ˆé«˜è´¨é‡å›¾åƒç”Ÿæˆçš„é¢†åŸŸï¼Œå¦‚åˆ›æ„è®¾è®¡ã€åª’ä½“å†…å®¹åˆ¶ä½œã€æ¸¸æˆå¼€å‘å’Œè™šæ‹ŸçŽ°å®žï¼Œé€šè¿‡åŠ é€Ÿæ‰©æ•£æ¨¡åž‹é™ä½Žè®¡ç®—æˆæœ¬ï¼Œæå‡å®žæ—¶æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå…·æœ‰å®žé™…å·¥ä¸šä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.

