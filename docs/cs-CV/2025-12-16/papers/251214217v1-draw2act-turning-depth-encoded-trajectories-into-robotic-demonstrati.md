---
layout: default
title: DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos
---

# DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos

**arXiv**: [2512.14217v1](https://arxiv.org/abs/2512.14217) | [PDF](https://arxiv.org/pdf/2512.14217.pdf)

**ä½œè€…**: Yang Bai, Liudi Yang, George Eskandar, Fengyi Shen, Mohammad Altillawi, Ziyuan Liu, Gitta Kutyniok

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRAW2ACTæ¡†æž¶ï¼Œé€šè¿‡æ·±åº¦æ„ŸçŸ¥è½¨è¿¹æ¡ä»¶è§†é¢‘ç”Ÿæˆï¼Œè§£å†³æœºå™¨äººæ¼”ç¤ºè§†é¢‘å¯æŽ§æ€§å’Œä¸€è‡´æ€§é—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **ä¸–ç•Œæ¨¡åž‹** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `æ·±åº¦æ„ŸçŸ¥è½¨è¿¹æ¡ä»¶` `è§†é¢‘ç”Ÿæˆ` `æœºå™¨äººæ¼”ç¤º` `å¤šæ¨¡æ€èžåˆ` `æ‰©æ•£æ¨¡åž‹` `æ—¶ç©ºä¸€è‡´æ€§` `å…·èº«AI` `æ“ä½œæˆåŠŸçŽ‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•ä¾èµ–2Dè½¨è¿¹æˆ–å•æ¨¡æ€æ¡ä»¶ï¼Œå¯¼è‡´æœºå™¨äººæ¼”ç¤ºè§†é¢‘å¯æŽ§æ€§å’Œä¸€è‡´æ€§å—é™ã€‚
2. DRAW2ACTæå–è½¨è¿¹çš„æ·±åº¦ã€è¯­ä¹‰ç­‰å¤šæ­£äº¤è¡¨ç¤ºï¼Œå¹¶æ³¨å…¥æ‰©æ•£æ¨¡åž‹ï¼Œè”åˆç”ŸæˆRGBå’Œæ·±åº¦è§†é¢‘ã€‚
3. å®žéªŒæ˜¾ç¤ºï¼ŒDRAW2ACTåœ¨è§†è§‰ä¿çœŸåº¦ã€ä¸€è‡´æ€§å’Œæ“ä½œæˆåŠŸçŽ‡ä¸Šå‡ä¼˜äºŽåŸºçº¿æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘æ‰©æ•£æ¨¡åž‹ä¸ºå…·èº«AIæä¾›äº†å¼ºå¤§çš„çœŸå®žä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œä½†åœ¨æœºå™¨äººæ“ä½œçš„å¯æŽ§æ€§æ–¹é¢ä»æœ‰é™åˆ¶ã€‚è¿‘æœŸåŸºäºŽè½¨è¿¹æ¡ä»¶çš„è§†é¢‘ç”Ÿæˆå·¥ä½œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œä½†é€šå¸¸ä¾èµ–äºŽ2Dè½¨è¿¹æˆ–å•æ¨¡æ€æ¡ä»¶ï¼Œé™åˆ¶äº†å…¶ç”Ÿæˆå¯æŽ§ä¸”ä¸€è‡´çš„æœºå™¨äººæ¼”ç¤ºçš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†DRAW2ACTï¼Œä¸€ä¸ªæ·±åº¦æ„ŸçŸ¥è½¨è¿¹æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¡†æž¶ï¼Œä»Žè¾“å…¥è½¨è¿¹ä¸­æå–å¤šä¸ªæ­£äº¤è¡¨ç¤ºï¼Œæ•æ‰æ·±åº¦ã€è¯­ä¹‰ã€å½¢çŠ¶å’Œè¿åŠ¨ï¼Œå¹¶å°†å®ƒä»¬æ³¨å…¥æ‰©æ•£æ¨¡åž‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºè”åˆç”Ÿæˆç©ºé—´å¯¹é½çš„RGBå’Œæ·±åº¦è§†é¢‘ï¼Œåˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œæ·±åº¦ç›‘ç£æ¥å¢žå¼ºæ—¶ç©ºä¸€è‡´æ€§ã€‚æœ€åŽï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºŽç”Ÿæˆçš„RGBå’Œæ·±åº¦åºåˆ—çš„å¤šæ¨¡æ€ç­–ç•¥æ¨¡åž‹ï¼Œä»¥å›žå½’æœºå™¨äººçš„å…³èŠ‚è§’åº¦ã€‚åœ¨Bridge V2ã€Berkeley Autolabå’Œæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œä¸ŽçŽ°æœ‰åŸºçº¿ç›¸æ¯”ï¼ŒDRAW2ACTå®žçŽ°äº†æ›´ä¼˜çš„è§†è§‰ä¿çœŸåº¦å’Œä¸€è‡´æ€§ï¼ŒåŒæ—¶èŽ·å¾—äº†æ›´é«˜çš„æ“ä½œæˆåŠŸçŽ‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

DRAW2ACTæ˜¯ä¸€ä¸ªæ·±åº¦æ„ŸçŸ¥è½¨è¿¹æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¡†æž¶ï¼Œæ•´ä½“åŒ…æ‹¬è½¨è¿¹è¡¨ç¤ºæå–ã€å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆå’Œç­–ç•¥æ¨¡åž‹ä¸‰éƒ¨åˆ†ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽä»Žè½¨è¿¹ä¸­æå–æ·±åº¦ã€è¯­ä¹‰ã€å½¢çŠ¶å’Œè¿åŠ¨ç­‰å¤šæ­£äº¤è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶è”åˆç”Ÿæˆç©ºé—´å¯¹é½çš„RGBå’Œæ·±åº¦è§†é¢‘ï¼Œä»¥å¢žå¼ºæ—¶ç©ºä¸€è‡´æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽå…¶æ·±åº¦æ„ŸçŸ¥èƒ½åŠ›å’Œå¤šæ¨¡æ€è”åˆç”Ÿæˆï¼Œå…‹æœäº†2Dè½¨è¿¹æˆ–å•æ¨¡æ€æ¡ä»¶çš„é™åˆ¶ï¼Œæé«˜äº†å¯æŽ§æ€§å’Œä¸€è‡´æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨Bridge V2ã€Berkeley Autolabå’Œæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDRAW2ACTå®žçŽ°äº†æ›´ä¼˜çš„è§†è§‰ä¿çœŸåº¦å’Œä¸€è‡´æ€§ï¼Œæ“ä½œæˆåŠŸçŽ‡æ˜¾è‘—é«˜äºŽçŽ°æœ‰åŸºçº¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽæœºå™¨äººæ¼”ç¤ºè§†é¢‘ç”Ÿæˆã€å…·èº«AIæ¨¡æ‹Ÿå’Œè‡ªåŠ¨åŒ–æ“ä½œä»»åŠ¡ï¼Œé€šè¿‡ç”Ÿæˆé«˜è´¨é‡ã€å¯æŽ§çš„æ¼”ç¤ºè§†é¢‘ï¼Œæå‡æœºå™¨äººå­¦ä¹ å’Œæ“ä½œçš„æˆåŠŸçŽ‡ï¼Œå…·æœ‰å®žé™…ä»·å€¼ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.

