---
layout: default
title: ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body
---

# ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body

**arXiv**: [2512.14234v1](https://arxiv.org/abs/2512.14234) | [PDF](https://arxiv.org/pdf/2512.14234.pdf)

**ä½œè€…**: Juze Zhang, Changan Chen, Xin Chen, Heng Yu, Tiange Xiang, Ali Sartaz Khan, Shrinidhi K. Lakshmikanth, Ehsan Adeli

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project page: https://ai.stanford.edu/~juze/ViBES/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViBESå¯¹è¯ä»£ç†ï¼Œé€šè¿‡è”åˆè§„åˆ’è¯­è¨€ä¸Žè¿åŠ¨è§£å†³å¤šæ¨¡æ€äº¤äº’ä¸­çš„æ—¶åºä¸Žç¤¾äº¤åŸºç¡€é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **åŠ¨ä½œç”Ÿæˆ** **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å¤šæ¨¡æ€å¯¹è¯ä»£ç†` `3Dè™šæ‹Ÿèº«ä½“` `æ··åˆæ¨¡æ€ä¸“å®¶` `è¯­éŸ³-è¯­è¨€-è¡Œä¸ºæ¨¡åž‹` `è·¨ä¸“å®¶æ³¨æ„åŠ›` `è”åˆè¿åŠ¨ç”Ÿæˆ` `ç¤¾äº¤äº¤äº’` `å¯æŽ§è¡Œä¸ºé’©å­`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•å°†äººç±»è¡Œä¸ºå»ºæ¨¡ä¸ºç¿»è¯‘ä»»åŠ¡ï¼Œå¯¼è‡´æ—¶åºè„†å¼±ã€ç¤¾äº¤åŸºç¡€è–„å¼±å’Œæ¨¡æ€å­¤ç«‹ï¼Œé™åˆ¶äº†å¤šæ¨¡æ€äº¤äº’çš„è‡ªç„¶æ€§ã€‚
2. ViBESé‡‡ç”¨æ··åˆæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰éª¨å¹²ï¼Œè”åˆè§„åˆ’è¯­è¨€ä¸Žè¿åŠ¨ï¼Œé€šè¿‡è·¨ä¸“å®¶æ³¨æ„åŠ›å®žçŽ°æ¨¡æ€é—´ä¿¡æ¯å…±äº«ï¼Œæ”¯æŒæ··åˆä¸»åŠ¨äº¤äº’ã€‚
3. åœ¨å¤šè½®å¯¹è¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒViBESåœ¨å¯¹è¯-è¿åŠ¨å¯¹é½å’Œè¡Œä¸ºè´¨é‡ä¸Šä¼˜äºŽåŸºçº¿ï¼Œå®žçŽ°å¯æŽ§ã€ç¤¾äº¤èƒ½åŠ›å¼ºçš„3Däº¤äº’ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»äº¤æµæœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€å’Œç¤¾äº¤æ€§çš„ï¼šè¯­è¨€ã€éŸµå¾‹å’Œè‚¢ä½“è¯­è¨€å…±åŒä¼ é€’æ„å›¾ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°çŽ°æœ‰ç³»ç»Ÿå°†äººç±»è¡Œä¸ºå»ºæ¨¡ä¸ºç¿»è¯‘ä»»åŠ¡ï¼ˆå¦‚ä¼´éšè¯­éŸ³æ‰‹åŠ¿æˆ–æ–‡æœ¬åˆ°è¿åŠ¨ï¼‰ï¼Œå°†å›ºå®šè¯è¯­æ˜ å°„åˆ°è¿åŠ¨ç‰‡æ®µï¼Œè€Œä¸éœ€è¦ä»£ç†å†³ç­–ä½•æ—¶ç§»åŠ¨ã€åšä»€ä¹ˆæˆ–å¦‚ä½•åœ¨å¤šè½®å¯¹è¯ä¸­é€‚åº”ã€‚è¿™å¯¼è‡´è„†å¼±çš„æ—¶åºã€è–„å¼±çš„ç¤¾äº¤åŸºç¡€ä»¥åŠè¯­éŸ³ã€æ–‡æœ¬å’Œè¿åŠ¨è¢«å­¤ç«‹è®­ç»ƒæˆ–æŽ¨æ–­çš„ç¢Žç‰‡åŒ–å †æ ˆã€‚æˆ‘ä»¬ä»‹ç»äº†ViBESï¼ˆè¯­éŸ³è¡Œä¸ºè¡¨è¾¾ä¸ŽåŒæ­¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè”åˆè§„åˆ’è¯­è¨€å’Œè¿åŠ¨å¹¶æ‰§è¡Œå¯¹è¯æ¡ä»¶åŒ–èº«ä½“åŠ¨ä½œçš„å¯¹è¯å¼3Dä»£ç†ã€‚å…·ä½“è€Œè¨€ï¼ŒViBESæ˜¯ä¸€ä¸ªå…·æœ‰æ··åˆæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰éª¨å¹²çš„è¯­éŸ³-è¯­è¨€-è¡Œä¸ºï¼ˆSLBï¼‰æ¨¡åž‹ï¼šæ¨¡æ€åˆ†åŒºçš„Transformerä¸“å®¶ç”¨äºŽè¯­éŸ³ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¿åŠ¨ã€‚è¯¥æ¨¡åž‹é€šè¿‡æ¨¡æ€ç¡¬è·¯ç”±ï¼ˆå‚æ•°æŒ‰ä¸“å®¶åˆ†å‰²ï¼‰å¤„ç†äº¤é”™çš„å¤šæ¨¡æ€ä»¤ç‰Œæµï¼ŒåŒæ—¶é€šè¿‡è·¨ä¸“å®¶æ³¨æ„åŠ›å…±äº«ä¿¡æ¯ã€‚é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒè¯­éŸ³-è¯­è¨€æ¨¡åž‹ï¼Œè¯¥ä»£ç†æ”¯æŒæ··åˆä¸»åŠ¨äº¤äº’ï¼šç”¨æˆ·å¯ä»¥åœ¨å¯¹è¯ä¸­è¯´è¯ã€æ‰“å­—æˆ–å‘å‡ºèº«ä½“åŠ¨ä½œæŒ‡ä»¤ï¼Œç³»ç»Ÿæš´éœ²å¯æŽ§è¡Œä¸ºé’©å­ä»¥è¿›è¡Œæµå¼å“åº”ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨å¤šè½®å¯¹è¯ä¸Šä½¿ç”¨å¯¹è¯-è¿åŠ¨å¯¹é½å’Œè¡Œä¸ºè´¨é‡çš„è‡ªåŠ¨æŒ‡æ ‡è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè§‚å¯Ÿåˆ°ç›¸å¯¹äºŽå¼ºå¤§çš„ä¼´éšè¯­éŸ³å’Œæ–‡æœ¬åˆ°è¿åŠ¨åŸºçº¿çš„ä¸€è‡´å¢žç›Šã€‚ViBESè¶…è¶Šäº†â€œè¯­éŸ³æ¡ä»¶åŒ–è¿åŠ¨ç”Ÿæˆâ€ï¼Œèµ°å‘ä»£ç†è™šæ‹Ÿèº«ä½“ï¼Œå…¶ä¸­è¯­è¨€ã€éŸµå¾‹å’Œè¿åŠ¨è¢«è”åˆç”Ÿæˆï¼Œå®žçŽ°å¯æŽ§ã€ç¤¾äº¤èƒ½åŠ›å¼ºçš„3Däº¤äº’ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨ai.stanford.edu/~juze/ViBES/æä¾›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¯¹è¯ä»£ç†ä¸­è¯­è¨€ã€è¯­éŸ³å’Œèº«ä½“è¿åŠ¨å­¤ç«‹å»ºæ¨¡çš„é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•å¦‚ä¼´éšè¯­éŸ³æ‰‹åŠ¿æˆ–æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆå°†è¡Œä¸ºè§†ä¸ºç¿»è¯‘ä»»åŠ¡ï¼Œå¯¼è‡´æ—¶åºä¸åè°ƒã€ç¤¾äº¤åŸºç¡€è–„å¼±å’Œæ¨¡æ€ç¢Žç‰‡åŒ–ï¼Œé™åˆ¶äº†äº¤äº’çš„è‡ªç„¶æ€§å’Œé€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºViBESï¼Œä¸€ä¸ªè”åˆè§„åˆ’è¯­è¨€å’Œè¿åŠ¨çš„å¯¹è¯å¼3Dä»£ç†ï¼Œæ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ··åˆæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰éª¨å¹²ï¼Œå°†è¯­éŸ³ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¿åŠ¨ä½œä¸ºç‹¬ç«‹æ¨¡æ€å¤„ç†ï¼ŒåŒæ—¶åˆ©ç”¨è·¨ä¸“å®¶æ³¨æ„åŠ›å®žçŽ°æ¨¡æ€é—´ä¿¡æ¯å…±äº«ï¼Œä»Žè€Œæ¨¡æ‹Ÿäººç±»å¤šæ¨¡æ€äº¤æµçš„ååŒæ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šViBESé‡‡ç”¨è¯­éŸ³-è¯­è¨€-è¡Œä¸ºï¼ˆSLBï¼‰æ¨¡åž‹æž¶æž„ï¼ŒåŸºäºŽMoMEéª¨å¹²ï¼ŒåŒ…å«æ¨¡æ€åˆ†åŒºçš„Transformerä¸“å®¶æ¨¡å—ï¼Œåˆ†åˆ«å¤„ç†è¯­éŸ³ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¿åŠ¨çš„ä»¤ç‰Œæµã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬å¤šæ¨¡æ€è¾“å…¥å¤„ç†ï¼ˆå¦‚è¯­éŸ³ã€æ–‡æœ¬å’ŒåŠ¨ä½œæŒ‡ä»¤ï¼‰ã€ç¡¬è·¯ç”±åˆ°å¯¹åº”ä¸“å®¶ã€è·¨ä¸“å®¶æ³¨æ„åŠ›èžåˆä¿¡æ¯ï¼Œä»¥åŠè”åˆç”Ÿæˆè¯­è¨€ã€éŸµå¾‹å’Œè¿åŠ¨è¾“å‡ºï¼Œæ”¯æŒæµå¼å“åº”å’Œå¯æŽ§è¡Œä¸ºé’©å­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯MoMEéª¨å¹²å’Œè·¨ä¸“å®¶æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽå°†è¡Œä¸ºå»ºæ¨¡ä»Žå­¤ç«‹ç¿»è¯‘ä»»åŠ¡æå‡ä¸ºä»£ç†å†³ç­–è¿‡ç¨‹ï¼Œå®žçŽ°è¯­è¨€ã€è¯­éŸ³å’Œè¿åŠ¨çš„è”åˆç”Ÿæˆï¼Œè€Œéžç®€å•æ˜ å°„ï¼Œä»Žè€Œå¢žå¼ºæ—¶åºåè°ƒå’Œç¤¾äº¤é€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬æ¨¡æ€ç¡¬è·¯ç”±ï¼ˆå‚æ•°æŒ‰ä¸“å®¶åˆ†å‰²ä»¥å‡å°‘å¹²æ‰°ï¼‰ã€è·¨ä¸“å®¶æ³¨æ„åŠ›å±‚ï¼ˆå…è®¸æ¨¡æ€é—´ä¿¡æ¯äº¤æ¢ï¼‰ã€é¢„è®­ç»ƒè¯­éŸ³-è¯­è¨€æ¨¡åž‹çš„é›†æˆï¼ˆå¦‚ç”¨äºŽåˆå§‹åŒ–æˆ–ç‰¹å¾æå–ï¼‰ï¼Œä»¥åŠæŸå¤±å‡½æ•°å¯èƒ½ç»“åˆå¯¹è¯å¯¹é½å’Œè¿åŠ¨è´¨é‡æŒ‡æ ‡ï¼ˆå…·ä½“ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­æä¾›ï¼Œéœ€å‚è€ƒè®ºæ–‡å…¨æ–‡ï¼‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

åœ¨å¤šè½®å¯¹è¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒViBESä½¿ç”¨è‡ªåŠ¨æŒ‡æ ‡è¯„ä¼°å¯¹è¯-è¿åŠ¨å¯¹é½å’Œè¡Œä¸ºè´¨é‡ï¼Œç›¸å¯¹äºŽå¼ºå¤§çš„ä¼´éšè¯­éŸ³å’Œæ–‡æœ¬åˆ°è¿åŠ¨åŸºçº¿ï¼ˆå…·ä½“åŸºçº¿æœªå‘½åï¼‰ï¼Œè§‚å¯Ÿåˆ°ä¸€è‡´æ€§èƒ½å¢žç›Šã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒViBESåœ¨æ—¶åºåè°ƒå’Œç¤¾äº¤é€‚åº”æ€§æ–¹é¢ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œä½†å…·ä½“æå‡å¹…åº¦å’Œæ€§èƒ½æ•°æ®éœ€å‚è€ƒè®ºæ–‡è¯¦ç»†ç»“æžœã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

ViBESçš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è™šæ‹ŸåŠ©æ‰‹ã€ç¤¾äº¤æœºå™¨äººã€æ¸¸æˆè§’è‰²å’Œè¿œç¨‹åä½œç³»ç»Ÿï¼Œé€šè¿‡å®žçŽ°å¯æŽ§ã€ç¤¾äº¤èƒ½åŠ›å¼ºçš„3Däº¤äº’ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œè‡ªç„¶åº¦ã€‚å®žé™…ä»·å€¼åœ¨äºŽä¿ƒè¿›äººæœºäº¤äº’çš„å¤šæ¨¡æ€èžåˆï¼Œæœªæ¥å¯èƒ½å½±å“æ•™è‚²ã€å¨±ä¹å’Œå¿ƒç†å¥åº·ç­‰é¢†åŸŸï¼ŒæŽ¨åŠ¨æ›´æ™ºèƒ½çš„è™šæ‹Ÿä»£ç†å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond "speech-conditioned motion generation" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/

