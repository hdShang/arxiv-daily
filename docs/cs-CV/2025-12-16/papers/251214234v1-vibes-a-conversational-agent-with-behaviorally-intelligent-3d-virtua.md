---
layout: default
title: ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body
---

# ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body

**arXiv**: [2512.14234v1](https://arxiv.org/abs/2512.14234) | [PDF](https://arxiv.org/pdf/2512.14234.pdf)

**ä½œè€…**: Juze Zhang, Changan Chen, Xin Chen, Heng Yu, Tiange Xiang, Ali Sartaz Khan, Shrinidhi K. Lakshmikanth, Ehsan Adeli

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project page: https://ai.stanford.edu/~juze/ViBES/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºViBESï¼šä¸€ä¸ªå…·æœ‰è¡Œä¸ºæ™ºèƒ½çš„3Dè™šæ‹Ÿå¯¹è¯ä»£ç†ï¼Œèƒ½è”åˆè§„åˆ’è¯­è¨€å’ŒåŠ¨ä½œã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `å¯¹è¯ä»£ç†` `3Dè™šæ‹Ÿå½¢è±¡` `è¡Œä¸ºæ™ºèƒ½` `å¤šæ¨¡æ€èžåˆ` `è¯­éŸ³-è¯­è¨€-è¡Œä¸ºæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰ç³»ç»Ÿé€šå¸¸å°†äººç±»è¡Œä¸ºå»ºæ¨¡ä¸ºç¿»è¯‘ä»»åŠ¡ï¼Œç¼ºä¹å¯¹ä½•æ—¶ç§»åŠ¨ã€åšä»€ä¹ˆä»¥åŠå¦‚ä½•é€‚åº”å¤šè½®å¯¹è¯çš„ä¸»åŠ¨å†³ç­–ã€‚
2. ViBESé€šè¿‡æ··åˆæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰éª¨å¹²ç½‘ç»œï¼Œè”åˆè§„åˆ’è¯­è¨€å’Œè¿åŠ¨ï¼Œå®žçŽ°å¯¹è¯æ¡ä»¶ä¸‹çš„èº«ä½“åŠ¨ä½œã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒViBESåœ¨å¯¹è¯-åŠ¨ä½œå¯¹é½å’Œè¡Œä¸ºè´¨é‡æ–¹é¢ï¼Œä¼˜äºŽçŽ°æœ‰çš„ååŒè¯­éŸ³å’Œæ–‡æœ¬åˆ°åŠ¨ä½œåŸºçº¿ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»ViBESï¼ˆè¯­éŸ³è¡Œä¸ºè¡¨è¾¾ä¸ŽåŒæ­¥ï¼‰ï¼Œä¸€ä¸ªä¼šè¯å¼3Dä»£ç†ï¼Œå®ƒè”åˆè§„åˆ’è¯­è¨€å’ŒåŠ¨ä½œï¼Œå¹¶æ‰§è¡Œå¯¹è¯æ¡ä»¶ä¸‹çš„èº«ä½“åŠ¨ä½œã€‚ViBESæ˜¯ä¸€ä¸ªè¯­éŸ³-è¯­è¨€-è¡Œä¸ºï¼ˆSLBï¼‰æ¨¡åž‹ï¼Œå…·æœ‰æ··åˆæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰éª¨å¹²ç½‘ç»œï¼šæ¨¡æ€åˆ’åˆ†çš„Transformerä¸“å®¶åˆ†åˆ«å¤„ç†è¯­éŸ³ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¿åŠ¨ã€‚è¯¥æ¨¡åž‹å¤„ç†äº¤é”™çš„å¤šæ¨¡æ€tokenæµï¼Œé€šè¿‡æ¨¡æ€è¿›è¡Œç¡¬è·¯ç”±ï¼ˆå‚æ•°æŒ‰ä¸“å®¶åˆ’åˆ†ï¼‰ï¼ŒåŒæ—¶é€šè¿‡è·¨ä¸“å®¶æ³¨æ„åŠ›å…±äº«ä¿¡æ¯ã€‚é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒè¯­éŸ³-è¯­è¨€æ¨¡åž‹ï¼Œè¯¥ä»£ç†æ”¯æŒæ··åˆä¸»åŠ¨äº¤äº’ï¼šç”¨æˆ·å¯ä»¥åœ¨å¯¹è¯ä¸­è¯´è¯ã€æ‰“å­—æˆ–å‘å‡ºèº«ä½“åŠ¨ä½œæŒ‡ä»¤ï¼Œç³»ç»Ÿå…¬å¼€å¯æŽ§çš„è¡Œä¸ºé’©å­ä»¥è¿›è¡Œæµå¼å“åº”ã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹è¯-åŠ¨ä½œå¯¹é½å’Œè¡Œä¸ºè´¨é‡çš„è‡ªåŠ¨æŒ‡æ ‡è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶è§‚å¯Ÿåˆ°ç›¸å¯¹äºŽå¼ºå¤§çš„ååŒè¯­éŸ³å’Œæ–‡æœ¬åˆ°åŠ¨ä½œåŸºçº¿çš„æŒç»­æå‡ã€‚ViBESè¶…è¶Šäº†â€œè¯­éŸ³æ¡ä»¶ä¸‹çš„è¿åŠ¨ç”Ÿæˆâ€ï¼Œæœç€ä»£ç†è™šæ‹Ÿèº«ä½“å‘å±•ï¼Œå…¶ä¸­è¯­è¨€ã€éŸµå¾‹å’Œè¿åŠ¨è¢«è”åˆç”Ÿæˆï¼Œä»Žè€Œå®žçŽ°å¯æŽ§çš„ã€å…·æœ‰ç¤¾äº¤èƒ½åŠ›çš„3Däº¤äº’ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨ai.stanford.edu/~juze/ViBES/ä¸Šæä¾›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„äººæœºäº¤äº’ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯è™šæ‹Ÿå½¢è±¡ï¼Œé€šå¸¸å°†è¯­éŸ³ã€æ–‡æœ¬å’ŒåŠ¨ä½œå­¤ç«‹åœ°è®­ç»ƒæˆ–æŽ¨æ–­ï¼Œå¯¼è‡´æ—¶åºåƒµç¡¬ã€ç¤¾äº¤åŸºç¡€è–„å¼±ã€‚å®ƒä»¬ç¼ºä¹åœ¨å¤šè½®å¯¹è¯ä¸­è¿›è¡Œä¸»åŠ¨å†³ç­–çš„èƒ½åŠ›ï¼Œæ— æ³•æ ¹æ®å¯¹è¯å†…å®¹åŠ¨æ€è°ƒæ•´è¡Œä¸ºï¼Œä»Žè€Œé™åˆ¶äº†äº¤äº’çš„è‡ªç„¶æ€§å’Œæµç•…æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šViBESçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è¯­éŸ³ã€è¯­è¨€å’Œè¡Œä¸ºæ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡åž‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿè”åˆè§„åˆ’è¯­è¨€å’ŒåŠ¨ä½œã€‚é€šè¿‡å¼•å…¥æ··åˆæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰æž¶æž„ï¼Œæ¨¡åž‹å¯ä»¥æ ¹æ®ä¸åŒçš„æ¨¡æ€ï¼ˆè¯­éŸ³ã€é¢éƒ¨è¡¨æƒ…ã€èº«ä½“è¿åŠ¨ï¼‰åˆ†é…ä¸åŒçš„ä¸“å®¶ï¼Œä»Žè€Œæ›´å¥½åœ°æ•æ‰å„è‡ªçš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡è·¨ä¸“å®¶æ³¨æ„åŠ›æœºåˆ¶å®žçŽ°ä¿¡æ¯å…±äº«ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ViBESèƒ½å¤Ÿæ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä¸»åŠ¨åœ°ç”Ÿæˆåˆé€‚çš„è¯­è¨€å’Œè¡Œä¸ºï¼Œä»Žè€Œå®žçŽ°æ›´è‡ªç„¶ã€æ›´å…·ç¤¾äº¤èƒ½åŠ›çš„äº¤äº’ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šViBESé‡‡ç”¨è¯­éŸ³-è¯­è¨€-è¡Œä¸ºï¼ˆSLBï¼‰æ¨¡åž‹ï¼Œå…¶æ ¸å¿ƒæ˜¯æ··åˆæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰éª¨å¹²ç½‘ç»œã€‚è¯¥ç½‘ç»œåŒ…å«å¤šä¸ªæ¨¡æ€åˆ’åˆ†çš„Transformerä¸“å®¶ï¼Œåˆ†åˆ«å¤„ç†è¯­éŸ³ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¿åŠ¨ã€‚æ¨¡åž‹æŽ¥æ”¶äº¤é”™çš„å¤šæ¨¡æ€tokenæµä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ¨¡æ€è¿›è¡Œç¡¬è·¯ç”±ï¼Œå°†tokenåˆ†é…ç»™ç›¸åº”çš„ä¸“å®¶ã€‚ä¸åŒä¸“å®¶ä¹‹é—´é€šè¿‡è·¨ä¸“å®¶æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä¿¡æ¯å…±äº«ã€‚æ¨¡åž‹è¿˜é›†æˆäº†é¢„è®­ç»ƒçš„è¯­éŸ³-è¯­è¨€æ¨¡åž‹ï¼Œä»¥å¢žå¼ºå…¶è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è¯­éŸ³ã€æ–‡æœ¬æˆ–èº«ä½“åŠ¨ä½œæŒ‡ä»¤ä¸ŽViBESè¿›è¡Œäº¤äº’ï¼Œç³»ç»Ÿåˆ™é€šè¿‡å¯æŽ§çš„è¡Œä¸ºé’©å­è¿”å›žæµå¼å“åº”ã€‚

**å…³é”®åˆ›æ–°**ï¼šViBESçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶è”åˆè§„åˆ’è¯­è¨€å’ŒåŠ¨ä½œçš„èƒ½åŠ›ï¼Œä»¥åŠæ··åˆæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰æž¶æž„çš„åº”ç”¨ã€‚ä¸Žä¼ ç»Ÿçš„â€œè¯­éŸ³æ¡ä»¶ä¸‹çš„è¿åŠ¨ç”Ÿæˆâ€æ–¹æ³•ä¸åŒï¼ŒViBESèƒ½å¤Ÿæ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡ä¸»åŠ¨åœ°ç”Ÿæˆåˆé€‚çš„è¯­è¨€å’Œè¡Œä¸ºï¼Œè€Œä¸æ˜¯ç®€å•åœ°å°†è¯­éŸ³æ˜ å°„åˆ°é¢„å®šä¹‰çš„åŠ¨ä½œåºåˆ—ã€‚MoMEæž¶æž„ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡è·¨ä¸“å®¶æ³¨æ„åŠ›æœºåˆ¶å®žçŽ°ä¿¡æ¯å…±äº«ï¼Œä»Žè€Œæé«˜äº†æ¨¡åž‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šViBESçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) æ¨¡æ€åˆ’åˆ†çš„Transformerä¸“å®¶ï¼Œç”¨äºŽå¤„ç†ä¸åŒæ¨¡æ€çš„æ•°æ®ï¼›2) è·¨ä¸“å®¶æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºŽå®žçŽ°ä¿¡æ¯å…±äº«ï¼›3) å¯æŽ§çš„è¡Œä¸ºé’©å­ï¼Œç”¨äºŽæŽ§åˆ¶è™šæ‹Ÿå½¢è±¡çš„è¡Œä¸ºï¼›4) æ··åˆä¸»åŠ¨äº¤äº’æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡å¤šç§æ–¹å¼ä¸Žç³»ç»Ÿè¿›è¡Œäº¤äº’ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†æè¿°ï¼Œå±žäºŽæœªçŸ¥ä¿¡æ¯ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®žéªŒéªŒè¯äº†ViBESçš„æœ‰æ•ˆæ€§ã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼ŒViBESåœ¨å¯¹è¯-åŠ¨ä½œå¯¹é½å’Œè¡Œä¸ºè´¨é‡æ–¹é¢ï¼Œå‡ä¼˜äºŽçŽ°æœ‰çš„ååŒè¯­éŸ³å’Œæ–‡æœ¬åˆ°åŠ¨ä½œåŸºçº¿ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨æ‘˜è¦ä¸­æœªç»™å‡ºï¼Œéœ€è¦åœ¨è®ºæ–‡æ­£æ–‡ä¸­æŸ¥æ‰¾ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒViBESèƒ½å¤Ÿç”Ÿæˆæ›´è‡ªç„¶ã€æ›´å…·ç¤¾äº¤èƒ½åŠ›çš„è™šæ‹Ÿå½¢è±¡ï¼Œä»Žè€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

ViBESå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è™šæ‹ŸåŠ©æ‰‹ã€åœ¨çº¿æ•™è‚²ã€æ¸¸æˆã€ç¤¾äº¤å¨±ä¹ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºŽåˆ›å»ºæ›´å…·å¸å¼•åŠ›å’Œäº’åŠ¨æ€§çš„è™šæ‹Ÿå½¢è±¡ï¼Œä»Žè€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚ä¾‹å¦‚ï¼Œåœ¨åœ¨çº¿æ•™è‚²ä¸­ï¼ŒViBESå¯ä»¥ä½œä¸ºè™šæ‹Ÿæ•™å¸ˆï¼Œæ ¹æ®å­¦ç”Ÿçš„æé—®å’Œååº”ï¼ŒåŠ¨æ€è°ƒæ•´æ•™å­¦å†…å®¹å’Œæ–¹å¼ã€‚åœ¨æ¸¸æˆä¸­ï¼ŒViBESå¯ä»¥ä½œä¸ºéžçŽ©å®¶è§’è‰²ï¼ˆNPCï¼‰ï¼Œä¸ŽçŽ©å®¶è¿›è¡Œæ›´è‡ªç„¶ã€æ›´å…·ç¤¾äº¤èƒ½åŠ›çš„äº’åŠ¨ã€‚æœªæ¥ï¼ŒViBESæœ‰æœ›æˆä¸ºäººæœºäº¤äº’çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond "speech-conditioned motion generation" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/

