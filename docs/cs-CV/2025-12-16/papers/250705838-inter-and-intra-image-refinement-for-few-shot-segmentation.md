---
layout: default
title: Inter- and Intra-image Refinement for Few Shot Segmentation
---

# Inter- and Intra-image Refinement for Few Shot Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2507.05838" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2507.05838</a>
  <a href="https://arxiv.org/pdf/2507.05838.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2507.05838" onclick="toggleFavorite(this, '2507.05838', 'Inter- and Intra-image Refinement for Few Shot Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ourui Fu, Hangzhou He, Kaiwen Li, Xinliang Zhang, Lei Zhu, Shuang Zeng, Zhaoheng Xie, Yanye Lu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInter- and Intra-image Refinementæ¨¡å‹ï¼Œè§£å†³å°‘æ ·æœ¬åˆ†å‰²ä¸­ç±»å†…å·®å¼‚å’Œç±»é—´å¹²æ‰°é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `å°‘æ ·æœ¬åˆ†å‰²` `è¯­ä¹‰åˆ†å‰²` `åŸå‹å­¦ä¹ ` `ç±»æ¿€æ´»æ˜ å°„` `Dropout` `è·¨åŸŸå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å°‘æ ·æœ¬åˆ†å‰²æ–¹æ³•ä¾èµ–å•åŸå‹è¡¨ç¤ºï¼Œå¯¼è‡´æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†ä¹‹é—´å­˜åœ¨è¾ƒå¤§çš„ç±»å†…å·®å¼‚ï¼Œç”Ÿæˆçš„å…ˆéªŒå›¾è°±è´¨é‡ä¸é«˜ã€‚
2. IIRæ¨¡å‹é€šè¿‡ç”Ÿæˆä¸¤ä¸ªåŸå‹ï¼Œåˆ†åˆ«å…³æ³¨æ ¸å¿ƒåŒºåˆ†ç‰¹å¾å’Œå±€éƒ¨ç‰¹å®šç‰¹å¾ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„ç±»é—´åŒ¹é…å’Œæ›´é²æ£’çš„å…ˆéªŒå›¾è°±ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒIIRæ¨¡å‹åœ¨å¤šä¸ªå°‘æ ·æœ¬åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºInter- and Intra-image Refinement (IIR) æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²(FSS)ä¸­å­˜åœ¨çš„ç±»å†…å·®å¼‚å’Œç±»é—´å¹²æ‰°é—®é¢˜ã€‚ç°æœ‰åŸºäºåŸå‹çš„æ–¹æ³•å—é™äºå•åŸå‹è¡¨ç¤ºï¼Œå¯¼è‡´å…ˆéªŒå›¾è°±åˆ†æ•£ä¸”å™ªå£°å¤§ã€‚åŒæ—¶ï¼Œè§†è§‰ç›¸ä¼¼ä½†è¯­ä¹‰ä¸åŒçš„åŒºåŸŸä¼šé€ æˆæ”¯æŒé›†å’ŒæŸ¥è¯¢é›†ç‰¹å¾åŒ¹é…ä¸ä¸€è‡´ï¼Œäº§ç”Ÿé”™è¯¯é¢„æµ‹ã€‚IIRæ¨¡å‹é€šè¿‡ç±»æ¿€æ´»æ˜ å°„ç”Ÿæˆä¸¤ä¸ªåŸå‹ï¼Œåˆ†åˆ«ç”¨äºæ ¸å¿ƒåŒºåˆ†ç‰¹å¾å’Œå±€éƒ¨ç‰¹å®šç‰¹å¾çš„åŒ¹é…ï¼Œä»è€Œç”Ÿæˆå‡†ç¡®ä¸”é²æ£’çš„å…ˆéªŒå›¾è°±ã€‚æ­¤å¤–ï¼Œå¼•å…¥æ–¹å‘æ€§Dropoutæœºåˆ¶æ¥å±è”½äº¤å‰æ³¨æ„åŠ›ä¸­ä¸ä¸€è‡´çš„æ”¯æŒé›†-æŸ¥è¯¢é›†ç‰¹å¾å¯¹ï¼Œæå‡è§£ç å™¨æ€§èƒ½ã€‚åœ¨æ ‡å‡†FSSã€éƒ¨åˆ†FSSå’Œè·¨åŸŸFSSç­‰9ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒIIRå‡å–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å°†æ¨¡å‹æ³›åŒ–åˆ°æ–°çš„ç±»åˆ«ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºåŸå‹çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†ç±»å†…å·®å¼‚å’Œç±»é—´å¹²æ‰°æ—¶å­˜åœ¨å±€é™æ€§ã€‚ç±»å†…å·®å¼‚æŒ‡çš„æ˜¯åŒä¸€ç±»åˆ«åœ¨æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†å›¾åƒä¸­å¯èƒ½å­˜åœ¨å¤–è§‚ã€å…‰ç…§ç­‰å·®å¼‚ï¼Œå¯¼è‡´å•åŸå‹è¡¨ç¤ºæ— æ³•å‡†ç¡®æ•æ‰ç±»åˆ«ç‰¹å¾ã€‚ç±»é—´å¹²æ‰°æŒ‡çš„æ˜¯è§†è§‰ä¸Šç›¸ä¼¼ä½†è¯­ä¹‰ä¸åŒçš„åŒºåŸŸä¼šå¹²æ‰°æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†ä¹‹é—´çš„ç‰¹å¾åŒ¹é…ï¼Œå¯¼è‡´åˆ†å‰²é”™è¯¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIIRæ¨¡å‹çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ›´ç²¾ç»†çš„ç‰¹å¾è¡¨ç¤ºå’Œæ›´é²æ£’çš„ç‰¹å¾åŒ¹é…æ¥ç¼“è§£ç±»å†…å·®å¼‚å’Œç±»é—´å¹²æ‰°ã€‚å…·ä½“æ¥è¯´ï¼ŒIIRæ¨¡å‹ä½¿ç”¨ä¸¤ä¸ªåŸå‹æ¥è¡¨ç¤ºæ¯ä¸ªç±»åˆ«ï¼Œä¸€ä¸ªåŸå‹å…³æ³¨æ ¸å¿ƒåŒºåˆ†ç‰¹å¾ï¼Œå¦ä¸€ä¸ªåŸå‹å…³æ³¨å±€éƒ¨ç‰¹å®šç‰¹å¾ã€‚åŒæ—¶ï¼ŒIIRæ¨¡å‹ä½¿ç”¨æ–¹å‘æ€§Dropoutæœºåˆ¶æ¥è¿‡æ»¤æ‰ä¸ä¸€è‡´çš„ç‰¹å¾åŒ¹é…ï¼Œä»è€Œæé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIIRæ¨¡å‹ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šInter-image Refinementå’ŒIntra-image Refinementã€‚Inter-image Refinementæ¨¡å—ä½¿ç”¨ç±»æ¿€æ´»æ˜ å°„ç”Ÿæˆä¸¤ä¸ªåŸå‹ï¼Œç”¨äºæ”¯æŒé›†å’ŒæŸ¥è¯¢é›†ä¹‹é—´çš„ç‰¹å¾åŒ¹é…ã€‚Intra-image Refinementæ¨¡å—ä½¿ç”¨æ–¹å‘æ€§Dropoutæœºåˆ¶æ¥è¿‡æ»¤æ‰ä¸ä¸€è‡´çš„ç‰¹å¾åŒ¹é…ã€‚æ•´ä¸ªæµç¨‹æ˜¯å…ˆé€šè¿‡Inter-image Refinementç”Ÿæˆæ›´å‡†ç¡®çš„å…ˆéªŒå›¾è°±ï¼Œç„¶åé€šè¿‡Intra-image Refinementè¿›ä¸€æ­¥æå‡è§£ç å™¨æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šIIRæ¨¡å‹çš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹ä¸¤ç‚¹ï¼š1) ä½¿ç”¨ä¸¤ä¸ªåŸå‹æ¥è¡¨ç¤ºæ¯ä¸ªç±»åˆ«ï¼Œä»è€Œæ›´å…¨é¢åœ°æ•æ‰ç±»åˆ«ç‰¹å¾ï¼Œç¼“è§£ç±»å†…å·®å¼‚ï¼›2) å¼•å…¥æ–¹å‘æ€§Dropoutæœºåˆ¶æ¥è¿‡æ»¤æ‰ä¸ä¸€è‡´çš„ç‰¹å¾åŒ¹é…ï¼Œä»è€Œæé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œç¼“è§£ç±»é—´å¹²æ‰°ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIIRæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†ç±»å†…å·®å¼‚å’Œç±»é—´å¹²æ‰°ï¼Œä»è€Œå–å¾—æ›´å¥½çš„åˆ†å‰²æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šInter-image Refinementæ¨¡å—ä½¿ç”¨ç±»æ¿€æ´»æ˜ å°„(CAM)æ¥ç”Ÿæˆä¸¤ä¸ªåŸå‹ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–(GAP)å¾—åˆ°æ¯ä¸ªç‰¹å¾å›¾çš„æƒé‡ï¼Œç„¶åæ ¹æ®æƒé‡å¯¹ç‰¹å¾å›¾è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°ç±»æ¿€æ´»å›¾ã€‚ç„¶åï¼Œä½¿ç”¨é˜ˆå€¼åˆ†å‰²å°†ç±»æ¿€æ´»å›¾åˆ†æˆä¸¤ä¸ªåŒºåŸŸï¼Œåˆ†åˆ«å¯¹åº”æ ¸å¿ƒåŒºåˆ†ç‰¹å¾å’Œå±€éƒ¨ç‰¹å®šç‰¹å¾ã€‚Intra-image Refinementæ¨¡å—ä½¿ç”¨æ–¹å‘æ€§Dropoutæœºåˆ¶æ¥è¿‡æ»¤æ‰ä¸ä¸€è‡´çš„ç‰¹å¾åŒ¹é…ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆè®¡ç®—æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼Œç„¶åæ ¹æ®ç›¸ä¼¼åº¦çŸ©é˜µå¯¹ç‰¹å¾è¿›è¡ŒDropoutï¼Œä»è€Œè¿‡æ»¤æ‰ä¸ä¸€è‡´çš„ç‰¹å¾åŒ¹é…ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2507.05838/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2507.05838/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2507.05838/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

IIRæ¨¡å‹åœ¨9ä¸ªå°‘æ ·æœ¬åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ ‡å‡†FSSã€éƒ¨åˆ†FSSå’Œè·¨åŸŸFSSã€‚ä¾‹å¦‚ï¼Œåœ¨æ ‡å‡†FSSçš„PASCAL-5iæ•°æ®é›†ä¸Šï¼ŒIIRæ¨¡å‹ç›¸æ¯”äºç°æœ‰æœ€ä½³æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™äº›å®éªŒç»“æœå……åˆ†è¯æ˜äº†IIRæ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºåŒ»ç–—å›¾åƒåˆ†æã€é¥æ„Ÿå›¾åƒè§£è¯‘ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ ‡æ³¨æ•°æ®é€šå¸¸éå¸¸æœ‰é™ï¼Œå› æ­¤å°‘æ ·æœ¬åˆ†å‰²æŠ€æœ¯å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚IIRæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ï¼Œæé«˜åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œä»è€Œä¸ºè¿™äº›é¢†åŸŸçš„åº”ç”¨æä¾›æ›´å¥½çš„æ”¯æŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ¨å¹¿åˆ°æ›´å¤šçš„å®é™…åº”ç”¨åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Deep neural networks for semantic segmentation rely on large-scale annotated datasets, leading to an annotation bottleneck that motivates few shot semantic segmentation (FSS) which aims to generalize to novel classes with minimal labeled exemplars. Most existing FSS methods adopt a prototype-based paradigm, which generates query prior map by extracting masked-area features from support images and then makes predictions guided by the prior map. However, they suffer from two critical limitations induced by inter- and intra-image discrepancies: 1) The intra-class gap between support and query images, caused by single-prototype representation, results in scattered and noisy prior maps; 2) The inter-class interference from visually similar but semantically distinct regions leads to inconsistent support-query feature matching and erroneous predictions. To address these issues, we propose the Inter- and Intra-image Refinement (IIR) model. The model contains an inter-image class activation mapping based method that generates two prototypes for class-consistent region matching, including core discriminative features and local specific features, and yields an accurate and robust prior map. For intra-image refinement, a directional dropout mechanism is introduced to mask inconsistent support-query feature pairs in cross attention, thereby enhancing decoder performance. Extensive experiments demonstrate that IIR achieves state-of-the-art performance on 9 benchmarks, covering standard FSS, part FSS, and cross-domain FSS. Our source code is available at \href{this https URL}{this https URL}.

