---
layout: default
title: MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning
---

# MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13636" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.13636</a>
  <a href="https://arxiv.org/pdf/2512.13636.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13636" onclick="toggleFavorite(this, '2512.13636', 'MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Hongwei Xie, Bing Wang, Guang Chen, Dingkang Liang, Xiang Bai

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MindDriveï¼šæå‡ºåŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `åœ¨çº¿å­¦ä¹ ` `è½¨è¿¹è§„åˆ’` `Bench2Drive`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAè‡ªåŠ¨é©¾é©¶æ¨¡å‹ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œå­˜åœ¨åˆ†å¸ƒåç§»å’Œå› æœæ··æ·†é—®é¢˜ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚
2. MindDriveé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨LLMè¿›è¡Œåœºæ™¯æ¨ç†å’Œå†³ç­–ï¼Œå¹¶å¼•å…¥åŠ¨ä½œä¸“å®¶å°†è¯­è¨€å†³ç­–æ˜ å°„ä¸ºè½¨è¿¹ã€‚
3. MindDriveåœ¨Bench2DriveåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œé©¾é©¶åˆ†æ•°è¾¾åˆ°78.04ï¼ŒæˆåŠŸç‡è¾¾åˆ°55.09%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰çš„è‡ªåŠ¨é©¾é©¶è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¸»è¦ä¾èµ–äºæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ï¼Œè¿™å¸¦æ¥äº†åˆ†å¸ƒåç§»å’Œå› æœæ··æ·†ç­‰å›ºæœ‰æŒ‘æˆ˜ã€‚åœ¨çº¿å¼ºåŒ–å­¦ä¹ é€šè¿‡è¯•é”™å­¦ä¹ ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†ä¸€æ¡æœ‰å¸Œæœ›çš„é€”å¾„ã€‚ç„¶è€Œï¼Œå°†åœ¨çº¿å¼ºåŒ–å­¦ä¹ åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„VLAæ¨¡å‹å—åˆ°è¿ç»­åŠ¨ä½œç©ºé—´ä¸­ä½æ•ˆæ¢ç´¢çš„é˜»ç¢ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†MindDriveï¼Œä¸€ä¸ªVLAæ¡†æ¶ï¼ŒåŒ…å«ä¸€ä¸ªå¸¦æœ‰ä¸¤ç»„ä¸åŒLoRAå‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ä¸€ä¸ªLLMä½œä¸ºå†³ç­–ä¸“å®¶ï¼Œç”¨äºåœºæ™¯æ¨ç†å’Œé©¾é©¶å†³ç­–ï¼Œè€Œå¦ä¸€ä¸ªä½œä¸ºåŠ¨ä½œä¸“å®¶ï¼ŒåŠ¨æ€åœ°å°†è¯­è¨€å†³ç­–æ˜ å°„åˆ°å¯è¡Œçš„è½¨è¿¹ã€‚é€šè¿‡å°†è½¨è¿¹çº§åˆ«çš„å¥–åŠ±åé¦ˆåˆ°æ¨ç†ç©ºé—´ï¼ŒMindDriveèƒ½å¤Ÿåœ¨æœ‰é™çš„ç¦»æ•£è¯­è¨€é©¾é©¶å†³ç­–é›†åˆä¸Šè¿›è¡Œè¯•é”™å­¦ä¹ ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸­æ“ä½œã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å¹³è¡¡äº†å¤æ‚åœºæ™¯ä¸­çš„æœ€ä¼˜å†³ç­–ã€ç±»äººé©¾é©¶è¡Œä¸ºä»¥åŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„é«˜æ•ˆæ¢ç´¢ã€‚ä½¿ç”¨è½»é‡çº§çš„Qwen-0.5B LLMï¼ŒMindDriveåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„Bench2DriveåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†78.04çš„é©¾é©¶åˆ†æ•°ï¼ˆDSï¼‰å’Œ55.09%çš„æˆåŠŸç‡ï¼ˆSRï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯æ˜åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¯¹è‡ªåŠ¨é©¾é©¶ä¸­VLAæ¨¡å‹æœ‰æ•ˆæ€§çš„å·¥ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰ä¾èµ–æ¨¡ä»¿å­¦ä¹ æ‰€å¸¦æ¥çš„åˆ†å¸ƒåç§»å’Œå› æœæ··æ·†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨å¤æ‚åœºæ™¯ä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ¢ç´¢å’Œå­¦ä¹ ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¼•å…¥VLAæ¨¡å‹ï¼Œé€šè¿‡è¯•é”™å­¦ä¹ æ¥ä¼˜åŒ–é©¾é©¶ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿ç»­åŠ¨ä½œç©ºé—´ä¸­æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œè®ºæ–‡å°†è¿ç»­åŠ¨ä½œç©ºé—´ç¦»æ•£åŒ–ä¸ºæœ‰é™çš„è¯­è¨€å†³ç­–é›†åˆï¼Œå¹¶åœ¨è¯¥é›†åˆä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMindDriveæ¡†æ¶åŒ…å«ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¯¥LLMé€šè¿‡ä¸¤ç»„LoRAå‚æ•°åˆ†åˆ«ä½œä¸ºå†³ç­–ä¸“å®¶å’ŒåŠ¨ä½œä¸“å®¶ã€‚å†³ç­–ä¸“å®¶è´Ÿè´£åœºæ™¯ç†è§£å’Œé©¾é©¶å†³ç­–ï¼Œè¾“å‡ºç¦»æ•£çš„è¯­è¨€æŒ‡ä»¤ã€‚åŠ¨ä½œä¸“å®¶è´Ÿè´£å°†è¿™äº›è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºå…·ä½“çš„è½¦è¾†è½¨è¿¹ã€‚ç³»ç»Ÿé€šè¿‡å°†è½¨è¿¹çº§åˆ«çš„å¥–åŠ±åé¦ˆç»™å†³ç­–ä¸“å®¶ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†åœ¨çº¿å¼ºåŒ–å­¦ä¹ åº”ç”¨äºVLAè‡ªåŠ¨é©¾é©¶æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§å°†è¿ç»­åŠ¨ä½œç©ºé—´ç¦»æ•£åŒ–ä¸ºè¯­è¨€å†³ç­–ç©ºé—´çš„æ–¹æ³•ï¼Œä»è€Œæé«˜äº†æ¢ç´¢æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡è¯•é”™å­¦ä¹ æ¥ä¼˜åŒ–é©¾é©¶ç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”å¤æ‚åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†è½»é‡çº§çš„Qwen-0.5B LLMä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚LoRAå‚æ•°ç”¨äºè°ƒæ•´LLMçš„è¡Œä¸ºï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”é©¾é©¶ä»»åŠ¡ã€‚è½¨è¿¹çº§åˆ«çš„å¥–åŠ±å‡½æ•°ç”¨äºè¯„ä¼°é©¾é©¶è¡Œä¸ºçš„ä¼˜åŠ£ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå¯ä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ å®‰å…¨ã€é«˜æ•ˆçš„é©¾é©¶ç­–ç•¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13636/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13636/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.13636/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

MindDriveåœ¨Bench2DriveåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œé©¾é©¶åˆ†æ•°ï¼ˆDSï¼‰è¾¾åˆ°78.04ï¼ŒæˆåŠŸç‡ï¼ˆSRï¼‰è¾¾åˆ°55.09%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸­å…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³æ¨¡ä»¿å­¦ä¹ æ‰€å¸¦æ¥çš„é—®é¢˜ã€‚è¯¥è®ºæ–‡æ˜¯é¦–ä¸ªè¯æ˜åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¯¹è‡ªåŠ¨é©¾é©¶ä¸­VLAæ¨¡å‹æœ‰æ•ˆæ€§çš„å·¥ä½œã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MindDriveçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è‡ªåŠ¨é©¾é©¶åœºæ™¯ï¼Œä¾‹å¦‚åŸå¸‚é“è·¯ã€é«˜é€Ÿå…¬è·¯å’Œè¶Šé‡ç¯å¢ƒã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå¹¶é™ä½å¼€å‘æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚æ— äººæœºå’Œå®¶ç”¨æœºå™¨äººã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. Using the lightweight Qwen-0.5B LLM, MindDrive achieves Driving Score (DS) of 78.04 and Success Rate (SR) of 55.09% on the challenging Bench2Drive benchmark. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

