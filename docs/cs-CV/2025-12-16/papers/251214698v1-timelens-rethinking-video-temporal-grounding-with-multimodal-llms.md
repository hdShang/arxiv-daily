---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

**arXiv**: [2512.14698v1](https://arxiv.org/abs/2512.14698) | [PDF](https://arxiv.org/pdf/2512.14698.pdf)

**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page: https://timelens-arc-lab.github.io/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TimeLensï¼šåˆ©ç”¨å¤šæ¨¡æ€LLMé‡æ–°æ€è€ƒè§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ï¼Œæž„å»ºé«˜è´¨é‡åŸºçº¿ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ ä¸Žæ¨¡ä»¿å­¦ä¹  (RL & IL)**

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€LLM` `æ•°æ®è´¨é‡` `å¼ºåŒ–å­¦ä¹ ` `è§†é¢‘ç†è§£` `åŸºå‡†æµ‹è¯•` `æ—¶é—´è¡¨ç¤º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†é¢‘æ—¶åºå®šä½åŸºå‡†æµ‹è¯•å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜ï¼Œå¯¼è‡´æ¨¡åž‹è¯„ä¼°ç»“æžœä¸å¯é ï¼Œé˜»ç¢äº†æœ‰æ•ˆæ–¹æ³•çš„å‘å±•ã€‚
2. TimeLensé€šè¿‡é«˜è´¨é‡æ•°æ®é‡æ ‡æ³¨å’Œç®—æ³•è®¾è®¡ï¼Œç³»ç»Ÿæ€§åœ°æå‡å¤šæ¨¡æ€LLMåœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
3. TimeLensæ¨¡åž‹åœ¨å¼€æºæ¨¡åž‹ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„è§†é¢‘æ—¶åºå®šä½æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†éƒ¨åˆ†ä¸“æœ‰æ¨¡åž‹ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶éžæå‡ºä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘ç†è§£ä¸­çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰å»ºç«‹äº†ä¸€ä¸ªç›´æŽ¥ã€å¢žé‡ä½†è‡³å…³é‡è¦çš„åŸºçº¿ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å®ƒä»¬ä»¥é€‚åº”VTGçš„æ–¹æ³•ä»æœªå¾—åˆ°å……åˆ†æŽ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†TimeLensï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†æž„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMï¼Œä¸»è¦å…³æ³¨æ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œæ­ç¤ºäº†çŽ°æœ‰VTGåŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†TimeLens-Benchï¼Œå®ƒåŒ…å«ç»è¿‡ä¸¥æ ¼è´¨é‡æ ‡å‡†é‡æ–°æ³¨é‡Šçš„ä¸‰ä¸ªæµè¡Œçš„åŸºå‡†æµ‹è¯•ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„åˆ†æžè¡¨æ˜Žï¼Œä¸Žä¼ ç»ŸåŸºå‡†ç›¸æ¯”ï¼Œæ¨¡åž‹æŽ’åå‘ç”Ÿäº†å·¨å¤§å˜åŒ–ï¼Œè¯å®žäº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨é‡æ–°æ³¨é‡Šæµç¨‹è§£å†³äº†å˜ˆæ‚çš„è®­ç»ƒæ•°æ®é—®é¢˜ï¼Œä»Žè€Œäº§ç”Ÿäº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†TimeLens-100Kã€‚åœ¨æ•°æ®åŸºç¡€ä¹‹ä¸Šï¼Œæˆ‘ä»¬å¯¹ç®—æ³•è®¾è®¡åŽŸåˆ™è¿›è¡Œäº†æ·±å…¥æŽ¢ç´¢ï¼Œäº§ç”Ÿäº†ä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œæœ‰æ•ˆè€Œé«˜æ•ˆçš„å®žè·µã€‚è¿™äº›åŒ…æ‹¬ç”¨äºŽæ—¶é—´è¡¨ç¤ºçš„äº¤é”™æ–‡æœ¬ç¼–ç ã€ä¸€ç§æ— éœ€æ€è€ƒçš„å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ–¹æ³•ä½œä¸ºè®­ç»ƒèŒƒä¾‹ï¼Œä»¥åŠä¸ºRLVRè®­ç»ƒç²¾å¿ƒè®¾è®¡çš„æ–¹æ¡ˆã€‚è¿™äº›åŠªåŠ›æœ€ç»ˆäº§ç”Ÿäº†TimeLensæ¨¡åž‹ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—MLLMï¼Œåœ¨å¼€æºæ¨¡åž‹ä¸­å…·æœ‰æœ€å…ˆè¿›çš„VTGæ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡åž‹éƒ½å°†å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æŸ¥è¯¢ï¼Œåœ¨è§†é¢‘ä¸­æ‰¾åˆ°å¯¹åº”çš„æ—¶é—´ç‰‡æ®µã€‚çŽ°æœ‰VTGåŸºå‡†æµ‹è¯•çš„æ•°æ®è´¨é‡å‚å·®ä¸é½ï¼Œæ ‡æ³¨å­˜åœ¨å™ªå£°ï¼Œå¯¼è‡´æ¨¡åž‹è®­ç»ƒå’Œè¯„ä¼°å—åˆ°å½±å“ï¼Œæ— æ³•çœŸå®žåæ˜ æ¨¡åž‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰è¿›è¡ŒVTGä»»åŠ¡ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTimeLensçš„æ ¸å¿ƒæ€è·¯æ˜¯â€œæ•°æ®ä¸ºçŽ‹â€ï¼Œé¦–å…ˆé€šè¿‡é«˜è´¨é‡çš„æ•°æ®é›†æž„å»ºå¯é çš„åŸºçº¿ï¼Œç„¶åŽåœ¨æ­¤åŸºç¡€ä¸ŠæŽ¢ç´¢æœ‰æ•ˆçš„ç®—æ³•è®¾è®¡ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ä¸¥æ ¼çš„è´¨é‡æŽ§åˆ¶æµç¨‹é‡æ–°æ ‡æ³¨çŽ°æœ‰æ•°æ®é›†ï¼Œå¹¶æž„å»ºå¤§è§„æ¨¡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚åŒæ—¶ï¼ŒæŽ¢ç´¢äº†æ—¶é—´è¡¨ç¤ºæ–¹æ³•ã€è®­ç»ƒèŒƒå¼å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨MLLMçš„æ½œåŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šTimeLensçš„æ•´ä½“æ¡†æž¶åŒ…æ‹¬æ•°æ®å‡†å¤‡å’Œæ¨¡åž‹è®­ç»ƒä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨æ•°æ®å‡†å¤‡é˜¶æ®µï¼Œé¦–å…ˆå¯¹çŽ°æœ‰VTGæ•°æ®é›†è¿›è¡Œè´¨é‡è¯„ä¼°ï¼Œç„¶åŽè¿›è¡Œé‡æ–°æ ‡æ³¨ï¼Œæž„å»ºé«˜è´¨é‡çš„TimeLens-Benchå’ŒTimeLens-100Kæ•°æ®é›†ã€‚åœ¨æ¨¡åž‹è®­ç»ƒé˜¶æ®µï¼Œé‡‡ç”¨å¤šæ¨¡æ€LLMä½œä¸ºåŸºç¡€æ¨¡åž‹ï¼Œå¹¶ç»“åˆäº¤é”™æ–‡æœ¬ç¼–ç ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­‰æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šTimeLensçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶å¯¹æ•°æ®è´¨é‡çš„é‡è§†å’Œç³»ç»Ÿæ€§çš„ç®—æ³•è®¾è®¡æŽ¢ç´¢ã€‚é€šè¿‡é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡åž‹çš„æ€§èƒ½ï¼Œå¹¶ä¸ºæ¨¡åž‹è®­ç»ƒæä¾›æ›´å¯é çš„æŒ‡å¯¼ã€‚æ­¤å¤–ï¼ŒTimeLensæå‡ºçš„äº¤é”™æ–‡æœ¬ç¼–ç å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æå‡MLLMåœ¨VTGä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šTimeLensçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨äº¤é”™æ–‡æœ¬ç¼–ç æ¥è¡¨ç¤ºæ—¶é—´ä¿¡æ¯ï¼Œå°†æ—¶é—´æˆ³ä¸Žæ–‡æœ¬æŸ¥è¯¢äº¤ç»‡åœ¨ä¸€èµ·ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ—¶é—´å…³ç³»ã€‚2) é‡‡ç”¨æ— éœ€æ€è€ƒçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä½œä¸ºè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¯éªŒè¯çš„å¥–åŠ±å‡½æ•°æ¥æŒ‡å¯¼æ¨¡åž‹çš„å­¦ä¹ ã€‚3) ç²¾å¿ƒè®¾è®¡RLVRè®­ç»ƒçš„æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¥–åŠ±å‡½æ•°çš„é€‰æ‹©ã€æŽ¢ç´¢ç­–ç•¥çš„è®¾ç½®ç­‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TimeLensæ¨¡åž‹åœ¨TimeLens-Benchä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†çŽ°æœ‰å¼€æºæ¨¡åž‹ï¼Œç”šè‡³è¶…è¶Šäº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œé«˜è´¨é‡çš„æ•°æ®å’Œæœ‰æ•ˆçš„ç®—æ³•è®¾è®¡æ˜¯æå‡VTGæ€§èƒ½çš„å…³é”®ã€‚TimeLens-100Kæ•°æ®é›†çš„å‘å¸ƒä¹Ÿå°†ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æœ‰åŠ›çš„æ”¯æŒã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TimeLensçš„ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæ™ºèƒ½è§†é¢‘æœç´¢ã€è§†é¢‘å†…å®¹ç†è§£ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢è§†é¢‘ä¸­çš„ç‰¹å®šäº‹ä»¶æˆ–ç‰‡æ®µï¼ŒTimeLenså¯ä»¥å¸®åŠ©å¿«é€Ÿå®šä½åˆ°ç›¸å…³å†…å®¹ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºŽæå‡è§†é¢‘ç†è§£çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå¹¶ä¸ºç›¸å…³åº”ç”¨æä¾›æ›´å‡†ç¡®ã€é«˜æ•ˆçš„æŠ€æœ¯æ”¯æŒã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

