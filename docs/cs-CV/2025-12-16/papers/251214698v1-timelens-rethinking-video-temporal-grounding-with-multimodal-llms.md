---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14698" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14698v1</a>
  <a href="https://arxiv.org/pdf/2512.14698.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14698v1" onclick="toggleFavorite(this, '2512.14698v1', 'TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page: https://timelens-arc-lab.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TimeLensï¼šé€šè¿‡å¤šæ¨¡æ€LLMé‡æ–°æ€è€ƒè§†é¢‘æ—¶åºå®šä½ï¼Œæ„å»ºé«˜è´¨é‡åŸºçº¿ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€LLM` `æ•°æ®è´¨é‡` `å¼ºåŒ–å­¦ä¹ ` `è§†é¢‘ç†è§£` `æ—¶é—´è¡¨ç¤º` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘æ—¶åºå®šä½åŸºå‡†æµ‹è¯•å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹è¯„ä¼°ç»“æœä¸å¯é ï¼Œé˜»ç¢äº†MLLMåœ¨è¯¥é¢†åŸŸçš„æœ‰æ•ˆåº”ç”¨ã€‚
2. TimeLensé€šè¿‡é«˜è´¨é‡æ•°æ®æ„å»ºå’Œç®—æ³•è®¾è®¡ï¼Œç³»ç»Ÿæ€§åœ°æå‡MLLMåœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
3. TimeLensæ¨¡å‹åœ¨VTGä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œè¶…è¶Šäº†å¼€æºæ¨¡å‹ï¼Œç”šè‡³ä¼˜äºGPT-5å’ŒGemini-2.5-Flashç­‰é—­æºæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶éæå‡ºä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘ç†è§£ä¸­çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰å»ºç«‹äº†ä¸€ä¸ªç›´æ¥ã€å¢é‡ä½†è‡³å…³é‡è¦çš„åŸºçº¿ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å®ƒä»¬ä»¥é€‚åº”VTGçš„æ–¹æ³•ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†TimeLensï¼Œå¯¹æ„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMè¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œä¸»è¦å…³æ³¨æ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œæ­ç¤ºäº†ç°æœ‰VTGåŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†TimeLens-Benchï¼Œå®ƒåŒ…å«ç»è¿‡ä¸¥æ ¼è´¨é‡æ ‡å‡†é‡æ–°æ³¨é‡Šçš„ä¸‰ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸ä¼ ç»ŸåŸºå‡†ç›¸æ¯”ï¼Œæ¨¡å‹é‡æ–°æ’åºå‘ç”Ÿäº†å·¨å¤§å˜åŒ–ï¼Œè¯å®äº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨é‡æ–°æ³¨é‡Šæµç¨‹è§£å†³äº†å˜ˆæ‚çš„è®­ç»ƒæ•°æ®é—®é¢˜ï¼Œä»è€Œäº§ç”Ÿäº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†TimeLens-100Kã€‚åœ¨æ•°æ®åŸºç¡€ä¹‹ä¸Šï¼Œæˆ‘ä»¬å¯¹ç®—æ³•è®¾è®¡åŸåˆ™è¿›è¡Œäº†æ·±å…¥æ¢ç´¢ï¼Œäº§ç”Ÿäº†ä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œæœ‰æ•ˆä¸”é«˜æ•ˆçš„å®è·µã€‚è¿™äº›åŒ…æ‹¬ç”¨äºæ—¶é—´è¡¨ç¤ºçš„äº¤é”™æ–‡æœ¬ç¼–ç ã€ä¸€ç§æ— éœ€æ€è€ƒçš„å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ–¹æ³•ä½œä¸ºè®­ç»ƒèŒƒå¼ï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡çš„RLVRè®­ç»ƒæ–¹æ³•ã€‚è¿™äº›åŠªåŠ›æœ€ç»ˆä¿ƒæˆäº†TimeLensæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—MLLMï¼Œåœ¨å¼€æºæ¨¡å‹ä¸­å…·æœ‰æœ€å…ˆè¿›çš„VTGæ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡å‹ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹éƒ½å°†å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ—¨åœ¨ä»è§†é¢‘ä¸­æ‰¾åˆ°ä¸ç»™å®šæ–‡æœ¬æŸ¥è¯¢ç›¸å¯¹åº”çš„ç‰¹å®šæ—¶é—´ç‰‡æ®µã€‚ç°æœ‰VTGåŸºå‡†æµ‹è¯•çš„æ•°æ®è´¨é‡å‚å·®ä¸é½ï¼Œæ ‡æ³¨å­˜åœ¨å™ªå£°å’Œä¸å‡†ç¡®æ€§ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å—åˆ°å½±å“ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ½œåŠ›ï¼Œç¼ºä¹é’ˆå¯¹VTGä»»åŠ¡çš„æœ‰æ•ˆä¼˜åŒ–ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTimeLensçš„æ ¸å¿ƒæ€è·¯æ˜¯â€œæ•°æ®ä¸ºç‹â€ï¼Œé¦–å…ˆé€šè¿‡é«˜è´¨é‡çš„æ•°æ®é›†æ„å»ºæ¥è§£å†³æ•°æ®è´¨é‡é—®é¢˜ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šæ¢ç´¢æœ‰æ•ˆçš„ç®—æ³•è®¾è®¡ã€‚é€šè¿‡é«˜è´¨é‡çš„æ•°æ®ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å‡†ç¡®çš„è§†é¢‘æ—¶åºå®šä½çŸ¥è¯†ï¼Œä»è€Œæå‡æ€§èƒ½ã€‚åŒæ—¶ï¼Œé’ˆå¯¹VTGä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†äº¤é”™æ–‡æœ¬ç¼–ç å’ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTimeLensçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬æ•°æ®æ„å»ºå’Œæ¨¡å‹è®­ç»ƒä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨æ•°æ®æ„å»ºé˜¶æ®µï¼Œé¦–å…ˆå¯¹ç°æœ‰VTGåŸºå‡†æµ‹è¯•è¿›è¡Œè´¨é‡è¯„ä¼°ï¼Œç„¶åè¿›è¡Œé‡æ–°æ ‡æ³¨ï¼Œæ„å»ºé«˜è´¨é‡çš„TimeLens-Benchã€‚åŒæ—¶ï¼Œé€šè¿‡è‡ªåŠ¨é‡æ–°æ ‡æ³¨æµç¨‹æ„å»ºå¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†TimeLens-100Kã€‚åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œé‡‡ç”¨äº¤é”™æ–‡æœ¬ç¼–ç æ¥è¡¨ç¤ºæ—¶é—´ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä½œä¸ºè®­ç»ƒèŒƒå¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šTimeLensçš„å…³é”®åˆ›æ–°åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) æ­ç¤ºå¹¶è§£å†³äº†ç°æœ‰VTGåŸºå‡†æµ‹è¯•ä¸­çš„æ•°æ®è´¨é‡é—®é¢˜ï¼Œæ„å»ºäº†é«˜è´¨é‡çš„TimeLens-Benchå’ŒTimeLens-100Kæ•°æ®é›†ã€‚2) æå‡ºäº†äº¤é”™æ–‡æœ¬ç¼–ç æ–¹æ³•ï¼Œæœ‰æ•ˆèåˆäº†æ–‡æœ¬å’Œæ—¶é—´ä¿¡æ¯ã€‚3) é‡‡ç”¨äº†åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä½œä¸ºè®­ç»ƒèŒƒå¼ï¼Œæå‡äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTimeLensæ›´åŠ æ³¨é‡æ•°æ®è´¨é‡å’Œé’ˆå¯¹VTGä»»åŠ¡çš„ç®—æ³•ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨äº¤é”™æ–‡æœ¬ç¼–ç ä¸­ï¼Œå°†æ—¶é—´ä¿¡æ¯ä»¥æ–‡æœ¬å½¢å¼æ’å…¥åˆ°è§†é¢‘æè¿°ä¸­ï¼Œä¾‹å¦‚â€œThe video shows [start_time] to [end_time]â€ã€‚åœ¨RLVRè®­ç»ƒä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹é¢„æµ‹çš„æ—¶é—´ç‰‡æ®µçš„è´¨é‡ã€‚å…·ä½“è€Œè¨€ï¼Œå¥–åŠ±å‡½æ•°å¯ä»¥åŸºäºé¢„æµ‹æ—¶é—´ç‰‡æ®µä¸çœŸå®æ—¶é—´ç‰‡æ®µçš„IoUï¼ˆIntersection over Unionï¼‰å€¼ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç³»åˆ—RLVRè®­ç»ƒæŠ€å·§ï¼Œä¾‹å¦‚å¥–åŠ±ç¼©æ”¾ã€æ¢ç´¢ç­–ç•¥ç­‰ï¼Œä»¥æå‡è®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TimeLensæ¨¡å‹åœ¨TimeLens-Benchä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰å¼€æºæ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡äº†GPT-5å’ŒGemini-2.5-Flashç­‰é—­æºæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨R@1æŒ‡æ ‡ä¸Šï¼ŒTimeLensæ¨¡å‹ç›¸æ¯”äºç°æœ‰æœ€ä½³å¼€æºæ¨¡å‹æå‡äº†X%ï¼Œè¯æ˜äº†å…¶åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚åŒæ—¶ï¼ŒTimeLens-Benchçš„å‘å¸ƒä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é«˜è´¨é‡çš„è¯„ä¼°åŸºå‡†ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TimeLensçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè§†é¢‘å†…å®¹ç†è§£ã€æ™ºèƒ½è§†é¢‘æœç´¢ã€è§†é¢‘ç¼–è¾‘å’Œæ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸã€‚é«˜è´¨é‡çš„è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›å¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´å‡†ç¡®åœ°æ‰¾åˆ°è§†é¢‘ä¸­çš„ç›®æ ‡ç‰‡æ®µï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

