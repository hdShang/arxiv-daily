---
layout: default
title: TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs
---

# TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs

**arXiv**: [2512.14698v1](https://arxiv.org/abs/2512.14698) | [PDF](https://arxiv.org/pdf/2512.14698.pdf)

**ä½œè€…**: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: Project Page: https://timelens-arc-lab.github.io/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTimeLensåŸºå‡†ä¸Žè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡é«˜è´¨é‡æ•°æ®å’Œç®—æ³•è®¾è®¡æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `è§†é¢‘æ—¶åºå®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `åŸºå‡†è¯„ä¼°` `æ•°æ®è´¨é‡` `å¼ºåŒ–å­¦ä¹ ` `è§†é¢‘ç†è§£` `å¼€æºæ¨¡åž‹` `ç®—æ³•è®¾è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VTGåŸºå‡†å­˜åœ¨è´¨é‡é—®é¢˜ï¼Œå¯¼è‡´æ¨¡åž‹è¯„ä¼°ä¸å¯é ï¼Œé™åˆ¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹åœ¨è§†é¢‘æ—¶åºå®šä½ä¸­çš„ä¼˜åŒ–ã€‚
2. è®ºæ–‡ä»Žæ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡å…¥æ‰‹ï¼Œæž„å»ºé«˜è´¨é‡åŸºå‡†å’Œè®­ç»ƒé›†ï¼Œå¹¶å¼•å…¥äº¤æ›¿æ–‡æœ¬ç¼–ç å’ŒRLVRè®­ç»ƒèŒƒå¼ã€‚
3. TimeLensæ¨¡åž‹åœ¨å¼€æºæ¨¡åž‹ä¸­è¾¾åˆ°SOTAæ€§èƒ½ï¼Œè¶…è¶ŠGPT-5ç­‰ä¸“æœ‰æ¨¡åž‹ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡å¹¶æœªæå‡ºæ–°æ–¹æ³•ï¼Œè€Œæ˜¯ä¸ºè§†é¢‘ç†è§£çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰å»ºç«‹äº†ä¸€ä¸ªç›´æŽ¥ã€æ¸è¿›ä½†è‡³å…³é‡è¦çš„åŸºçº¿ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†ä¼˜åŒ–å…¶VTGèƒ½åŠ›çš„æ–¹æ¡ˆä»å¾…æŽ¢ç´¢ã€‚æœ¬æ–‡æå‡ºTimeLensï¼Œä»Žæ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶å¦‚ä½•æž„å»ºå…·æœ‰å¼ºå¤§VTGèƒ½åŠ›çš„MLLMsã€‚æˆ‘ä»¬é¦–å…ˆæ­ç¤ºäº†çŽ°æœ‰VTGåŸºå‡†ä¸­çš„å…³é”®è´¨é‡é—®é¢˜ï¼Œå¹¶å¼•å…¥TimeLens-Benchï¼ŒåŒ…å«ä¸‰ä¸ªæµè¡ŒåŸºå‡†çš„ç²¾å¿ƒé‡æ–°æ ‡æ³¨ç‰ˆæœ¬ï¼Œéµå¾ªä¸¥æ ¼çš„è´¨é‡æ ‡å‡†ã€‚åˆ†æžæ˜¾ç¤ºï¼Œä¸Žæ—§åŸºå‡†ç›¸æ¯”ï¼Œæ¨¡åž‹æŽ’åå‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œè¯å®žäº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„ä¸å¯é æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨é‡æ–°æ ‡æ³¨æµç¨‹å¤„ç†å™ªå£°è®­ç»ƒæ•°æ®ï¼Œç”Ÿæˆäº†TimeLens-100Kï¼Œä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚åŸºäºŽæ•°æ®åŸºç¡€ï¼Œæˆ‘ä»¬æ·±å…¥æŽ¢ç´¢ç®—æ³•è®¾è®¡åŽŸåˆ™ï¼ŒèŽ·å¾—äº†ä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„è§è§£å’Œé«˜æ•ˆå®žç”¨çš„å®žè·µã€‚è¿™äº›åŒ…æ‹¬ç”¨äºŽæ—¶é—´è¡¨ç¤ºçš„äº¤æ›¿æ–‡æœ¬ç¼–ç ã€ä½œä¸ºè®­ç»ƒèŒƒå¼çš„å…æ€è€ƒå¼ºåŒ–å­¦ä¹ ä¸Žå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•ï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡çš„RLVRè®­ç»ƒæ–¹æ¡ˆã€‚è¿™äº›åŠªåŠ›æœ€ç»ˆå½¢æˆäº†TimeLensæ¨¡åž‹ç³»åˆ—ï¼Œåœ¨å¼€æºæ¨¡åž‹ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„VTGæ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†GPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡åž‹éƒ½å°†å‘å¸ƒä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

TimeLensçš„æ•´ä½“æ¡†æž¶åŸºäºŽå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼Œé€šè¿‡ç³»ç»Ÿä¼˜åŒ–æ•°æ®è´¨é‡å’Œç®—æ³•è®¾è®¡æ¥æå‡è§†é¢‘æ—¶åºå®šä½èƒ½åŠ›ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼šæž„å»ºTimeLens-Benché«˜è´¨é‡åŸºå‡†å’ŒTimeLens-100Kè®­ç»ƒæ•°æ®é›†ä»¥è§£å†³æ•°æ®å™ªå£°é—®é¢˜ï¼›é‡‡ç”¨äº¤æ›¿æ–‡æœ¬ç¼–ç æœ‰æ•ˆè¡¨ç¤ºæ—¶é—´ä¿¡æ¯ï¼›æå‡ºå…æ€è€ƒå¼ºåŒ–å­¦ä¹ ä¸Žå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä½œä¸ºè®­ç»ƒèŒƒå¼ï¼Œç»“åˆç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒæ–¹æ¡ˆã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºŽï¼Œå®ƒä¸å¼•å…¥æ–°æ¨¡åž‹æž¶æž„ï¼Œè€Œæ˜¯èšç„¦äºŽæ•°æ®æ¸…æ´—å’Œç®—æ³•ä¼˜åŒ–ï¼Œæä¾›å¯å¤çŽ°çš„åŸºçº¿ï¼Œå¼ºè°ƒè¯„ä¼°å¯é æ€§å’Œè®­ç»ƒæ•ˆçŽ‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TimeLensæ¨¡åž‹åœ¨å¼€æºæ¨¡åž‹ä¸­å®žçŽ°æœ€å…ˆè¿›çš„VTGæ€§èƒ½ï¼Œè¶…è¶ŠGPT-5å’ŒGemini-2.5-Flashç­‰ä¸“æœ‰æ¨¡åž‹ï¼›é‡æ–°æ ‡æ³¨çš„åŸºå‡†å¯¼è‡´æ¨¡åž‹æŽ’åæ˜¾è‘—å˜åŒ–ï¼Œå‡¸æ˜¾äº†å…ˆå‰è¯„ä¼°æ ‡å‡†çš„é—®é¢˜ï¼›é«˜è´¨é‡æ•°æ®å’Œç®—æ³•ä¼˜åŒ–å…±åŒè´¡çŒ®äº†æ€§èƒ½æå‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯åº”ç”¨äºŽè§†é¢‘å†…å®¹åˆ†æžã€æ™ºèƒ½ç›‘æŽ§ã€æ•™è‚²è§†é¢‘æ£€ç´¢å’Œè‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£ç­‰é¢†åŸŸï¼Œé€šè¿‡æå‡è§†é¢‘æ—¶åºå®šä½ç²¾åº¦ï¼Œå¢žå¼ºå¤šæ¨¡æ€AIç³»ç»Ÿåœ¨çŽ°å®žä¸–ç•Œä¸­çš„å®žç”¨æ€§å’Œå¯é æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

