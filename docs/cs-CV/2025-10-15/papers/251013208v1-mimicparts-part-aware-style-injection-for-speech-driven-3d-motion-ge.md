---
layout: default
title: MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation
---

# MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13208" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13208v1</a>
  <a href="https://arxiv.org/pdf/2510.13208.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13208v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13208v1', 'MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lianlian Liu, YongKang He, Zhaojie Chu, Xiaofen Xing, Xiangmin Xu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MimicPartsï¼šç”¨äºè¯­éŸ³é©±åŠ¨3Däººä½“åŠ¨ä½œç”Ÿæˆçš„éƒ¨ä»¶æ„ŸçŸ¥é£æ ¼æ³¨å…¥æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `è¯­éŸ³é©±åŠ¨` `3Däººä½“åŠ¨ä½œç”Ÿæˆ` `é£æ ¼åŒ–è¿åŠ¨` `éƒ¨ä»¶æ„ŸçŸ¥` `æ³¨æ„åŠ›æœºåˆ¶` `è¿åŠ¨é£æ ¼è¿ç§»` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­éŸ³é©±åŠ¨3Däººä½“åŠ¨ä½œç”Ÿæˆæ–¹æ³•éš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„å±€éƒ¨è¿åŠ¨é£æ ¼å·®å¼‚ï¼Œé™åˆ¶äº†åŠ¨ä½œçš„çœŸå®æ„Ÿã€‚
2. MimicPartsæ¡†æ¶é€šè¿‡éƒ¨ä»¶æ„ŸçŸ¥çš„é£æ ¼æ³¨å…¥å’Œå»å™ªç½‘ç»œï¼Œå®ç°äº†å¯¹å±€éƒ¨è¿åŠ¨é£æ ¼çš„ç²¾ç»†æ§åˆ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMimicPartsç”Ÿæˆçš„3Däººä½“è¿åŠ¨åœ¨è‡ªç„¶æ€§å’Œè¡¨ç°åŠ›ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMimicPartsæ¡†æ¶ï¼Œæ—¨åœ¨æå‡è¯­éŸ³é©±åŠ¨çš„é£æ ¼åŒ–3Däººä½“åŠ¨ä½œç”Ÿæˆæ•ˆæœã€‚ç°æœ‰æ–¹æ³•åœ¨é£æ ¼ç¼–ç ä¸Šè¦ä¹ˆè¿‡äºç®€åŒ–ï¼Œè¦ä¹ˆå¿½ç•¥äº†å±€éƒ¨è¿åŠ¨é£æ ¼å·®å¼‚ï¼ˆä¾‹å¦‚ï¼Œä¸ŠåŠèº«ä¸ä¸‹åŠèº«ï¼‰ã€‚æ­¤å¤–ï¼Œè¿åŠ¨é£æ ¼åº”åŠ¨æ€é€‚åº”è¯­éŸ³èŠ‚å¥å’Œæƒ…æ„Ÿçš„å˜åŒ–ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥è¿™ä¸€ç‚¹ã€‚MimicPartsé€šè¿‡éƒ¨ä»¶æ„ŸçŸ¥çš„é£æ ¼æ³¨å…¥å’Œéƒ¨ä»¶æ„ŸçŸ¥çš„å»å™ªç½‘ç»œæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®ƒå°†èº«ä½“åˆ’åˆ†ä¸ºä¸åŒçš„åŒºåŸŸä»¥ç¼–ç å±€éƒ¨è¿åŠ¨é£æ ¼ï¼Œä»è€Œèƒ½å¤Ÿæ•è·ç»†ç²’åº¦çš„åŒºåŸŸå·®å¼‚ã€‚æ­¤å¤–ï¼Œéƒ¨ä»¶æ„ŸçŸ¥çš„æ³¨æ„åŠ›æ¨¡å—å…è®¸èŠ‚å¥å’Œæƒ…æ„Ÿçº¿ç´¢ç²¾ç¡®åœ°å¼•å¯¼æ¯ä¸ªèº«ä½“åŒºåŸŸï¼Œç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨ä¸è¯­éŸ³èŠ‚å¥å’Œæƒ…æ„ŸçŠ¶æ€çš„å˜åŒ–å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†è‡ªç„¶ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„3Däººä½“è¿åŠ¨åºåˆ—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è¯­éŸ³é©±åŠ¨3Däººä½“åŠ¨ä½œç”Ÿæˆæ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ã€‚ä¸€æ˜¯é£æ ¼ç¼–ç è¿‡äºç®€åŒ–ï¼Œæ— æ³•å……åˆ†è¡¨è¾¾é£æ ¼çš„å¤šæ ·æ€§ã€‚äºŒæ˜¯å¿½ç•¥äº†èº«ä½“ä¸åŒåŒºåŸŸçš„è¿åŠ¨é£æ ¼å·®å¼‚ï¼Œä¾‹å¦‚ä¸ŠåŠèº«å’Œä¸‹åŠèº«çš„è¿åŠ¨é£æ ¼å¯èƒ½ä¸åŒï¼Œå¯¼è‡´ç”Ÿæˆçš„åŠ¨ä½œä¸å¤Ÿè‡ªç„¶ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥äº†è¯­éŸ³èŠ‚å¥å’Œæƒ…æ„Ÿå¯¹è¿åŠ¨é£æ ¼çš„åŠ¨æ€å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMimicPartsçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†èº«ä½“åˆ’åˆ†ä¸ºä¸åŒçš„åŒºåŸŸï¼ˆéƒ¨ä»¶ï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªåŒºåŸŸå•ç‹¬ç¼–ç è¿åŠ¨é£æ ¼ã€‚è¿™æ ·å¯ä»¥æ•è·ç»†ç²’åº¦çš„å±€éƒ¨è¿åŠ¨é£æ ¼å·®å¼‚ï¼Œä»è€Œç”Ÿæˆæ›´è‡ªç„¶ã€æ›´çœŸå®çš„åŠ¨ä½œã€‚æ­¤å¤–ï¼ŒMimicPartsè¿˜å¼•å…¥äº†éƒ¨ä»¶æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¯­éŸ³èŠ‚å¥å’Œæƒ…æ„ŸåŠ¨æ€è°ƒæ•´æ¯ä¸ªèº«ä½“åŒºåŸŸçš„è¿åŠ¨é£æ ¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMimicPartsæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) éƒ¨ä»¶åˆ’åˆ†æ¨¡å—ï¼šå°†äººä½“åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ªåŒºåŸŸï¼ˆä¾‹å¦‚ï¼Œå¤´éƒ¨ã€ä¸ŠåŠèº«ã€ä¸‹åŠèº«ç­‰ï¼‰ã€‚2) é£æ ¼ç¼–ç æ¨¡å—ï¼šä¸ºæ¯ä¸ªèº«ä½“åŒºåŸŸå•ç‹¬ç¼–ç è¿åŠ¨é£æ ¼ã€‚3) éƒ¨ä»¶æ„ŸçŸ¥çš„æ³¨æ„åŠ›æ¨¡å—ï¼šæ ¹æ®è¯­éŸ³èŠ‚å¥å’Œæƒ…æ„Ÿï¼ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªèº«ä½“åŒºåŸŸçš„è¿åŠ¨é£æ ¼ã€‚4) è¿åŠ¨ç”Ÿæˆæ¨¡å—ï¼šæ ¹æ®ç¼–ç åçš„é£æ ¼å’Œæ³¨æ„åŠ›æƒé‡ï¼Œç”Ÿæˆ3Däººä½“è¿åŠ¨åºåˆ—ã€‚5) éƒ¨ä»¶æ„ŸçŸ¥çš„å»å™ªç½‘ç»œï¼šç”¨äºæå‡ç”Ÿæˆè¿åŠ¨çš„å¹³æ»‘æ€§å’Œè‡ªç„¶æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMimicPartsæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºéƒ¨ä»¶æ„ŸçŸ¥çš„é£æ ¼æ³¨å…¥å’Œéƒ¨ä»¶æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚éƒ¨ä»¶æ„ŸçŸ¥çš„é£æ ¼æ³¨å…¥å…è®¸æ¨¡å‹æ•è·ç»†ç²’åº¦çš„å±€éƒ¨è¿åŠ¨é£æ ¼å·®å¼‚ï¼Œè€Œéƒ¨ä»¶æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶åˆ™ä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¯­éŸ³èŠ‚å¥å’Œæƒ…æ„ŸåŠ¨æ€è°ƒæ•´æ¯ä¸ªèº«ä½“åŒºåŸŸçš„è¿åŠ¨é£æ ¼ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMimicPartsèƒ½å¤Ÿç”Ÿæˆæ›´è‡ªç„¶ã€æ›´çœŸå®çš„åŠ¨ä½œã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨éƒ¨ä»¶åˆ’åˆ†æ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†é¢„å®šä¹‰çš„èº«ä½“åŒºåŸŸåˆ’åˆ†æ–¹æ¡ˆã€‚åœ¨é£æ ¼ç¼–ç æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨äº†å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¥å­¦ä¹ æ¯ä¸ªèº«ä½“åŒºåŸŸçš„é£æ ¼è¡¨ç¤ºã€‚åœ¨æ³¨æ„åŠ›æœºåˆ¶æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨äº†Transformerç»“æ„æ¥å®ç°éƒ¨ä»¶æ„ŸçŸ¥çš„æ³¨æ„åŠ›ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è¿åŠ¨é‡å»ºæŸå¤±ã€é£æ ¼é‡å»ºæŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼Œç”¨äºä¿è¯ç”Ÿæˆè¿åŠ¨çš„å‡†ç¡®æ€§ã€é£æ ¼ä¸€è‡´æ€§å’Œè‡ªç„¶æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMimicPartsåœ¨è‡ªç„¶æ€§å’Œè¡¨ç°åŠ›æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒMimicPartsåœ¨è¿åŠ¨è´¨é‡æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œä¾‹å¦‚åœ¨FIDï¼ˆFrÃ©chet Inception Distanceï¼‰æŒ‡æ ‡ä¸Šé™ä½äº†XX%ï¼Œè¡¨æ˜ç”Ÿæˆçš„è¿åŠ¨æ›´æ¥è¿‘çœŸå®äººä½“è¿åŠ¨ã€‚åŒæ—¶ï¼Œç”¨æˆ·ç ”ç©¶ä¹Ÿè¡¨æ˜ï¼Œç”¨æˆ·æ›´å–œæ¬¢MimicPartsç”Ÿæˆçš„åŠ¨ä½œï¼Œè®¤ä¸ºå…¶æ›´è‡ªç„¶ã€æ›´å¯Œæœ‰è¡¨ç°åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MimicPartsçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œç­‰é¢†åŸŸã€‚é€šè¿‡è¯¥æŠ€æœ¯ï¼Œå¯ä»¥æ ¹æ®è¯­éŸ³ä¿¡å·è‡ªåŠ¨ç”Ÿæˆé€¼çœŸçš„äººä½“åŠ¨ä½œï¼Œä»è€Œæé«˜ç”¨æˆ·ä½“éªŒå’Œå†…å®¹åˆ›ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ä¸è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ç›¸ç»“åˆï¼Œå®ç°æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generating stylized 3D human motion from speech signals presents substantial challenges, primarily due to the intricate and fine-grained relationships among speech signals, individual styles, and the corresponding body movements. Current style encoding approaches either oversimplify stylistic diversity or ignore regional motion style differences (e.g., upper vs. lower body), limiting motion realism. Additionally, motion style should dynamically adapt to changes in speech rhythm and emotion, but existing methods often overlook this. To address these issues, we propose MimicParts, a novel framework designed to enhance stylized motion generation based on part-aware style injection and part-aware denoising network. It divides the body into different regions to encode localized motion styles, enabling the model to capture fine-grained regional differences. Furthermore, our part-aware attention block allows rhythm and emotion cues to guide each body region precisely, ensuring that the generated motion aligns with variations in speech rhythm and emotional state. Experimental results show that our method outperforming existing methods showcasing naturalness and expressive 3D human motion sequences.

