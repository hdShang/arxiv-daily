---
layout: default
title: Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch
---

# Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17858" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17858v1</a>
  <a href="https://arxiv.org/pdf/2510.17858.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17858v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17858v1', 'Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xu Cai, Yang Wu, Qianli Chen, Haoran Wu, Lichuan Xiang, Hongkai Wen

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**å¤‡æ³¨**: NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé€Ÿåº¦åœºè‡ªè’¸é¦çš„Flow Matchingæ¨¡å‹åŠ é€Ÿæ–¹æ³•ï¼Œå®ç°é«˜æ•ˆå°‘æ­¥é‡‡æ ·**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `Flow Matching` `æ‰©æ•£æ¨¡å‹` `æ¨¡å‹åŠ é€Ÿ` `è‡ªè’¸é¦` `é€Ÿåº¦åœº` `å°‘æ ·æœ¬å­¦ä¹ ` `åè®­ç»ƒ` `é¢„è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Flow Matchingæ¨¡å‹åŠ é€Ÿæ–¹æ³•éœ€è¦ä»å¤´è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºåŸºäºé€Ÿåº¦åœºè‡ªè’¸é¦çš„shortcutæœºåˆ¶ï¼Œæ— éœ€æ­¥é•¿åµŒå…¥ï¼Œå³å¯åŠ é€Ÿç°æœ‰Flow Matchingæ¨¡å‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½é«˜æ•ˆè®­ç»ƒå°‘æ­¥Flow Matchingæ¨¡å‹ï¼Œå¹¶å®ç°æ•°åäº¿å‚æ•°æ¨¡å‹çš„å°‘æ ·æœ¬è’¸é¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è¶…é«˜æ•ˆçš„åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ–°é¢–çš„é€Ÿåº¦åœºè‡ªè’¸é¦ï¼Œå°†å¤§è§„æ¨¡é¢„è®­ç»ƒçš„Flow Matchingæ‰©æ•£æ¨¡å‹åŠ é€Ÿä¸ºé«˜æ•ˆçš„å°‘æ­¥é‡‡æ ·å™¨ã€‚Flow Matchingä¸­çš„shortcutæŠ€æœ¯è™½ç„¶æä¾›äº†çµæ´»çš„è½¨è¿¹è·³è·ƒèƒ½åŠ›ï¼Œä½†å®ƒéœ€è¦ä¸“é—¨çš„æ­¥é•¿åµŒå…¥ï¼Œè¿™ä¸ç°æœ‰æ¨¡å‹ä¸å…¼å®¹ï¼Œé™¤éä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒï¼Œè€Œé‡æ–°è®­ç»ƒçš„æˆæœ¬å‡ ä¹ä¸é¢„è®­ç»ƒæœ¬èº«ä¸€æ ·é«˜æ˜‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯èµ‹äºˆæ ‡å‡†Flow Matchingæ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒFluxï¼‰æ›´æ¿€è¿›çš„shortcutæœºåˆ¶ï¼Œåˆ©ç”¨ç‹¬ç‰¹çš„è’¸é¦åŸç†ï¼Œé¿å…äº†å¯¹æ­¥é•¿åµŒå…¥çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½œç”¨äºé€Ÿåº¦åœºè€Œä¸æ˜¯æ ·æœ¬ç©ºé—´ï¼Œå¹¶é€šè¿‡åœ¨çº¿çš„è‡ªå¼•å¯¼è’¸é¦å¿«é€Ÿå­¦ä¹ ï¼Œä»è€Œé«˜æ•ˆåœ°è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚ï¼Œåœ¨ä¸åˆ°ä¸€ä¸ªA100å¤©çš„æ—¶é—´å†…ç”Ÿæˆä¸€ä¸ª3æ­¥çš„Fluxæ¨¡å‹ã€‚é™¤äº†è’¸é¦ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥æ•´åˆåˆ°é¢„è®­ç»ƒé˜¶æ®µæœ¬èº«ï¼Œä»è€Œäº§ç”Ÿèƒ½å¤Ÿå›ºæœ‰åœ°å­¦ä¹ é«˜æ•ˆã€å°‘æ­¥æµç¨‹è€Œä¸å½±å“è´¨é‡çš„æ¨¡å‹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ç§èƒ½åŠ›è¿˜å®ç°äº†ç¬¬ä¸€ä¸ªé’ˆå¯¹æ•°åäº¿å‚æ•°æ‰©æ•£æ¨¡å‹çš„å°‘æ ·æœ¬è’¸é¦æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œ10ä¸ªæ–‡æœ¬-å›¾åƒå¯¹ï¼‰ï¼Œä»¥å‡ ä¹å…è´¹çš„æˆæœ¬æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šFlow Matchingæ‰©æ•£æ¨¡å‹è™½ç„¶ç”Ÿæˆæ•ˆæœå¥½ï¼Œä½†é‡‡æ ·é€Ÿåº¦æ…¢ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•ï¼Œå¦‚shortcutæ¨¡å‹ï¼Œéœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬å·¨å¤§ï¼Œéš¾ä»¥åº”ç”¨åˆ°å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ä¸Šã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆåœ°åŠ é€Ÿé¢„è®­ç»ƒçš„Flow Matchingæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè¿›è¡Œå°‘æ­¥é‡‡æ ·ï¼Œæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é€Ÿåº¦åœºè‡ªè’¸é¦ï¼Œè®©æ¨¡å‹å­¦ä¹ æ›´æ¿€è¿›çš„shortcutæœºåˆ¶ï¼Œä»è€Œåœ¨æ›´å°‘çš„æ­¥éª¤å†…å®Œæˆé‡‡æ ·ã€‚é€šè¿‡åœ¨é€Ÿåº¦åœºä¸Šè¿›è¡Œè’¸é¦ï¼Œé¿å…äº†å¯¹æ­¥é•¿åµŒå…¥çš„ä¾èµ–ï¼Œä½¿å¾—è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå·²æœ‰çš„é¢„è®­ç»ƒFlow Matchingæ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šåè®­ç»ƒåŠ é€Ÿå’Œé¢„è®­ç»ƒé›†æˆã€‚åœ¨åè®­ç»ƒåŠ é€Ÿé˜¶æ®µï¼Œåˆ©ç”¨é€Ÿåº¦åœºè‡ªè’¸é¦ï¼Œå°†é¢„è®­ç»ƒçš„Flow Matchingæ¨¡å‹è½¬åŒ–ä¸ºå°‘æ­¥é‡‡æ ·å™¨ã€‚åœ¨é¢„è®­ç»ƒé›†æˆé˜¶æ®µï¼Œå°†è¯¥æ–¹æ³•èå…¥åˆ°é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒæ—¶å°±å­¦ä¹ åˆ°é«˜æ•ˆçš„å°‘æ­¥æµç¨‹ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆç›®æ ‡æ•°æ®ï¼Œç„¶åä½¿ç”¨è¿™äº›æ•°æ®ä½œä¸ºteacherä¿¡å·ï¼Œè®­ç»ƒä¸€ä¸ªstudentæ¨¡å‹ï¼Œstudentæ¨¡å‹çš„ç›®æ ‡æ˜¯å­¦ä¹ teacheræ¨¡å‹çš„é€Ÿåº¦åœºï¼Œä»è€Œå®ç°è’¸é¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºé€Ÿåº¦åœºçš„è‡ªè’¸é¦æ–¹æ³•ï¼Œé¿å…äº†å¯¹æ­¥é•¿åµŒå…¥çš„ä¾èµ–ï¼Œä»è€Œå¯ä»¥é«˜æ•ˆåœ°åŠ é€Ÿå·²æœ‰çš„é¢„è®­ç»ƒFlow Matchingæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å®ç°äº†å¯¹æ•°åäº¿å‚æ•°æ‰©æ•£æ¨¡å‹çš„å°‘æ ·æœ¬è’¸é¦ï¼Œè¿™åœ¨ä»¥å‰æ˜¯éš¾ä»¥å®ç°çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥æ–¹æ³•çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é€Ÿåº¦åœºä½œä¸ºè’¸é¦çš„ç›®æ ‡ï¼Œè€Œä¸æ˜¯æ ·æœ¬ç©ºé—´ï¼›2) é‡‡ç”¨åœ¨çº¿è’¸é¦çš„æ–¹å¼ï¼Œä½¿å¾—è®­ç»ƒæ›´åŠ é«˜æ•ˆï¼›3) å°†è¯¥æ–¹æ³•èå…¥åˆ°é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒæ—¶å°±å­¦ä¹ åˆ°é«˜æ•ˆçš„å°‘æ­¥æµç¨‹ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ç›®æ ‡æ˜¯è®©studentæ¨¡å‹çš„é€Ÿåº¦åœºå°½å¯èƒ½æ¥è¿‘teacheræ¨¡å‹çš„é€Ÿåº¦åœºï¼Œé€šå¸¸é‡‡ç”¨L2æŸå¤±æˆ–ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸åˆ°ä¸€ä¸ªA100å¤©çš„æ—¶é—´å†…å°†Fluxæ¨¡å‹åŠ é€Ÿä¸º3æ­¥é‡‡æ ·å™¨ï¼Œå¹¶ä¸”åœ¨å›¾åƒç”Ÿæˆè´¨é‡ä¸Šä¸åŸå§‹æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å®ç°äº†å¯¹æ•°åäº¿å‚æ•°æ‰©æ•£æ¨¡å‹çš„å°‘æ ·æœ¬è’¸é¦ï¼Œä»…ä½¿ç”¨10ä¸ªæ–‡æœ¬-å›¾åƒå¯¹å³å¯è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå›¾åƒç”Ÿæˆã€æ–‡æœ¬ç”Ÿæˆã€éŸ³é¢‘ç”Ÿæˆç­‰é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™æˆ–éœ€è¦å¿«é€Ÿç”Ÿæˆç»“æœçš„åœºæ™¯ä¸‹ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„å›¾åƒç¼–è¾‘ã€å®æ—¶è¯­éŸ³åˆæˆç­‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜ä¸ºå¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰æœ›æ¨åŠ¨æ‰©æ•£æ¨¡å‹åœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present an ultra-efficient post-training method for shortcutting large-scale pre-trained flow matching diffusion models into efficient few-step samplers, enabled by novel velocity field self-distillation. While shortcutting in flow matching, originally introduced by shortcut models, offers flexible trajectory-skipping capabilities, it requires a specialized step-size embedding incompatible with existing models unless retraining from scratch$\unicode{x2013}$a process nearly as costly as pretraining itself.
>   Our key contribution is thus imparting a more aggressive shortcut mechanism to standard flow matching models (e.g., Flux), leveraging a unique distillation principle that obviates the need for step-size embedding. Working on the velocity field rather than sample space and learning rapidly from self-guided distillation in an online manner, our approach trains efficiently, e.g., producing a 3-step Flux less than one A100 day. Beyond distillation, our method can be incorporated into the pretraining stage itself, yielding models that inherently learn efficient, few-step flows without compromising quality. This capability also enables, to our knowledge, the first few-shot distillation method (e.g., 10 text-image pairs) for dozen-billion-parameter diffusion models, delivering state-of-the-art performance at almost free cost.

