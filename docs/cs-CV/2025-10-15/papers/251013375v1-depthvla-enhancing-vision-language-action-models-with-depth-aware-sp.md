---
layout: default
title: DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning
---

# DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.13375" target="_blank" class="toolbar-btn">arXiv: 2510.13375v1</a>
    <a href="https://arxiv.org/pdf/2510.13375.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13375v1" 
            onclick="toggleFavorite(this, '2510.13375v1', 'DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**DepthVLAÔºöÈÄöËøáÊ∑±Â∫¶ÊÑüÁü•ÁöÑÁ©∫Èó¥Êé®ÁêÜÂ¢ûÂº∫ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Ê∑±Â∫¶Â≠¶‰π†` `Á©∫Èó¥Êé®ÁêÜ` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Ê∑±Â∫¶ÊÑüÁü•`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Á≤æÁ°ÆÁ©∫Èó¥Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂéüÂõ†ÊòØVLMÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ‰∏çË∂≥Ôºå‰∏î‰æùËµñÂ§ßÈáèÂä®‰ΩúÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉ„ÄÇ
2. DepthVLAÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÊ∑±Â∫¶È¢ÑÊµãÊ®°ÂùóÊòæÂºèÂú∞ÂºïÂÖ•Á©∫Èó¥ÊÑüÁü•ÔºåÈááÁî®Ê∑∑ÂêàTransformerÊû∂ÊûÑÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑÁ©∫Èó¥Êé®ÁêÜÂ¢ûÂº∫„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDepthVLAÂú®ÁúüÂÆû‰∏ñÁïåÂíåÊ®°ÊãüÁéØÂ¢É‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜVLAÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÊúÄËøëÂ±ïÁ§∫‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊ≥õÂåñÂíåËØ≠Ë®ÄÂºïÂØºÁöÑÊìç‰ΩúËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)Âõ∫ÊúâÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÊúâÈôêÔºåÂÆÉ‰ª¨Âú®ÈúÄË¶ÅÁ≤æÁ°ÆÁ©∫Èó¥Êé®ÁêÜÁöÑ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ‰ºö‰∏ãÈôç„ÄÇÁé∞ÊúâÁöÑVLA‰æùËµñ‰∫éÂ§ßÈáèÁöÑÂä®‰ΩúÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÔºå‰ª•Â∞ÜVLMÂÆö‰ΩçÂú®3DÁ©∫Èó¥‰∏≠ÔºåËøôÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊïàÁéáÔºåÂπ∂‰∏îÂØπ‰∫éÂáÜÁ°ÆÁöÑÁ©∫Èó¥ÁêÜËß£‰ªçÁÑ∂ÊòØ‰∏çÂ§üÁöÑ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜDepthVLAÔºå‰∏Ä‰∏™ÁÆÄÂçïËÄåÊúâÊïàÁöÑVLAÊû∂ÊûÑÔºåÂÆÉÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÊ∑±Â∫¶È¢ÑÊµãÊ®°ÂùóÊòæÂºèÂú∞ÁªìÂêà‰∫ÜÁ©∫Èó¥ÊÑüÁü•„ÄÇDepthVLAÈááÁî®Ê∑∑ÂêàTransformerÁöÑËÆæËÆ°ÔºåÁªü‰∏Ä‰∫ÜVLM„ÄÅÊ∑±Â∫¶TransformerÂíåÂä®‰Ωú‰∏ìÂÆ∂ÔºåÂπ∂ÂÖ∑ÊúâÂÆåÂÖ®ÂÖ±‰∫´ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™ÂÖ∑ÊúâÂ¢ûÂº∫Á©∫Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÁ´ØÂà∞Á´ØÊ®°Âûã„ÄÇÂú®ÁúüÂÆû‰∏ñÁïåÂíåÊ®°ÊãüÁéØÂ¢É‰∏≠ÁöÑÂ§ßÈáèËØÑ‰º∞Ë°®ÊòéÔºåDepthVLA‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂú®ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÂèñÂæó‰∫Ü78.5% vs. 65.0%ÁöÑËøõÂ±ïÔºåÂú®LIBEROÊ®°ÊãüÂô®‰∏≠ÂèñÂæó‰∫Ü94.9% vs. 93.6%ÁöÑËøõÂ±ïÔºåÂú®SimplerÊ®°ÊãüÂô®‰∏≠ÂèñÂæó‰∫Ü74.8% vs. 58.8%ÁöÑËøõÂ±ï„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂ∞ÜÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®Â§ÑÁêÜÈúÄË¶ÅÁ≤æÁ°ÆÁ©∫Èó¥Êé®ÁêÜÁöÑ‰ªªÂä°Êó∂Èù¢‰∏¥ÊåëÊàò„ÄÇËøô‰∫õÊ®°Âûã‰æùËµñ‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)ÔºåËÄåVLMÊú¨Ë∫´ÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÊúâÈôê„ÄÇ‰∏∫‰∫ÜÂº•Ë°•Ëøô‰∏ÄÁº∫Èô∑ÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®Â§ßÈáèÁöÑÂä®‰ΩúÊï∞ÊçÆËøõË°åÈ¢ÑËÆ≠ÁªÉÔºå‰ª•‰ΩøVLMËÉΩÂ§üÁêÜËß£3DÁ©∫Èó¥„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïÊïàÁéá‰Ωé‰∏ãÔºåÂπ∂‰∏î‰ªçÁÑ∂Êó†Ê≥ï‰øùËØÅÂáÜÁ°ÆÁöÑÁ©∫Èó¥ÁêÜËß£„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDepthVLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊòæÂºèÂú∞ÂºïÂÖ•Ê∑±Â∫¶‰ø°ÊÅØÊù•Â¢ûÂº∫VLAÊ®°ÂûãÁöÑÁ©∫Èó¥ÊÑüÁü•ËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊ∑±Â∫¶È¢ÑÊµãÊ®°ÂùóÊù•‰º∞ËÆ°Âú∫ÊôØÁöÑÊ∑±Â∫¶ÂõæÔºåÂπ∂Â∞ÜÊ∑±Â∫¶‰ø°ÊÅØËûçÂÖ•Âà∞VLAÊ®°ÂûãÁöÑÂ§ÑÁêÜÊµÅÁ®ã‰∏≠„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞ÁêÜËß£Âú∫ÊôØÁöÑ3DÁªìÊûÑÔºå‰ªéËÄåÊèêÈ´òÁ©∫Èó¥Êé®ÁêÜÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDepthVLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÊòØ‰∏Ä‰∏™Ê∑∑ÂêàTransformerÊ®°ÂûãÔºåÂÆÉÁî±‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÁªÑÊàêÔºö‰∏Ä‰∏™ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)Ôºå‰∏Ä‰∏™Ê∑±Â∫¶TransformerÂíå‰∏Ä‰∏™Âä®‰Ωú‰∏ìÂÆ∂„ÄÇVLMË¥üË¥£Â§ÑÁêÜËßÜËßâÂíåËØ≠Ë®ÄËæìÂÖ•ÔºåÊ∑±Â∫¶TransformerË¥üË¥£Â§ÑÁêÜÊ∑±Â∫¶‰ø°ÊÅØÔºåÂä®‰Ωú‰∏ìÂÆ∂Ë¥üË¥£ÁîüÊàêÂä®‰ΩúÊåá‰ª§„ÄÇËøô‰∏â‰∏™Ê®°ÂùóÈÄöËøáÂÆåÂÖ®ÂÖ±‰∫´ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åËøûÊé•Ôºå‰ªéËÄåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDepthVLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊòæÂºèÂú∞Â∞ÜÊ∑±Â∫¶‰ø°ÊÅØËûçÂÖ•Âà∞VLAÊ®°ÂûãÁöÑÂ§ÑÁêÜÊµÅÁ®ã‰∏≠„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåDepthVLA‰∏çÈúÄË¶Å‰æùËµñÂ§ßÈáèÁöÑÂä®‰ΩúÊï∞ÊçÆËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåËÄåÊòØÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÊ∑±Â∫¶È¢ÑÊµãÊ®°ÂùóÊù•Ëé∑ÂèñÊ∑±Â∫¶‰ø°ÊÅØ„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéáÔºåËÄå‰∏îËøòËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÁêÜËß£Âú∫ÊôØÁöÑ3DÁªìÊûÑ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDepthVLAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÊ∑±Â∫¶È¢ÑÊµãÊ®°ÂùóÊù•‰º∞ËÆ°Âú∫ÊôØÁöÑÊ∑±Â∫¶ÂõæÔºõ2) ‰ΩøÁî®Ê∑±Â∫¶TransformerÊù•Â§ÑÁêÜÊ∑±Â∫¶‰ø°ÊÅØÔºõ3) ‰ΩøÁî®ÂÆåÂÖ®ÂÖ±‰∫´ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•ËøûÊé•VLM„ÄÅÊ∑±Â∫¶TransformerÂíåÂä®‰Ωú‰∏ìÂÆ∂Ôºõ4) ‰ΩøÁî®Á´ØÂà∞Á´ØÁöÑËÆ≠ÁªÉÊñπÊ≥ïÊù•‰ºòÂåñÊï¥‰∏™Ê®°Âûã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DepthVLAÂú®ÁúüÂÆû‰∏ñÁïåÂíåÊ®°ÊãüÁéØÂ¢É‰∏≠ÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÔºåDepthVLAÁöÑÊÄßËÉΩÊèêÂçá‰∫Ü78.5%ÔºåËÄåÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ï‰ªÖ‰∏∫65.0%„ÄÇÂú®LIBEROÊ®°ÊãüÂô®‰∏≠ÔºåDepthVLAÁöÑÊÄßËÉΩÊèêÂçáÂà∞‰∫Ü94.9%ÔºåËÄåÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ï‰∏∫93.6%„ÄÇÂú®SimplerÊ®°ÊãüÂô®‰∏≠ÔºåDepthVLAÁöÑÊÄßËÉΩÊèêÂçáÂà∞‰∫Ü74.8%ÔºåËÄåÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ï‰∏∫58.8%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåDepthVLAËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òVLAÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DepthVLAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥ÂáÜÁ°ÆÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºå‰ªéËÄåÊâßË°åÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°„ÄÇÂú®Ëá™Âä®È©æÈ©∂È¢ÜÂüüÔºåÂÆÉÂèØ‰ª•ÊèêÈ´òËΩ¶ËæÜÂØπÂë®Âõ¥ÁéØÂ¢ÉÁöÑÊÑüÁü•ËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´òÈ©æÈ©∂ÂÆâÂÖ®ÊÄß„ÄÇÂú®ËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÈ¢ÜÂüüÔºåÂÆÉÂèØ‰ª•Êèê‰æõÊõ¥ÈÄºÁúüÁöÑ3D‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models have recently shown impressive generalization and language-guided manipulation capabilities. However, their performance degrades on tasks requiring precise spatial reasoning due to limited spatial reasoning inherited from Vision-Language Models (VLMs). Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D space, which reduces training efficiency and is still insufficient for accurate spatial understanding. In this work, we present DepthVLA, a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction module. DepthVLA adopts a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial reasoning. Extensive evaluations in both real-world and simulated environments show that DepthVLA outperforms state-of-the-art approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs. 93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator. Our code will be made publicly available.

