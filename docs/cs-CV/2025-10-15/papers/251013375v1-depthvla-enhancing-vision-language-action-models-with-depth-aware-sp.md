---
layout: default
title: DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning
---

# DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13375" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13375v1</a>
  <a href="https://arxiv.org/pdf/2510.13375.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13375v1" onclick="toggleFavorite(this, '2510.13375v1', 'DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DepthVLAï¼šé€šè¿‡æ·±åº¦æ„ŸçŸ¥çš„ç©ºé—´æ¨ç†å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æ·±åº¦å­¦ä¹ ` `ç©ºé—´æ¨ç†` `æœºå™¨äººæ“ä½œ` `æ·±åº¦æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨ç²¾ç¡®ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼ŒåŸå› æ˜¯VLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œä¸”ä¾èµ–å¤§é‡åŠ¨ä½œæ•°æ®é¢„è®­ç»ƒã€‚
2. DepthVLAé€šè¿‡é¢„è®­ç»ƒçš„æ·±åº¦é¢„æµ‹æ¨¡å—æ˜¾å¼åœ°å¼•å…¥ç©ºé—´æ„ŸçŸ¥ï¼Œé‡‡ç”¨æ··åˆTransformeræ¶æ„ï¼Œå®ç°ç«¯åˆ°ç«¯çš„ç©ºé—´æ¨ç†å¢å¼ºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒDepthVLAåœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†VLAæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹æœ€è¿‘å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–å’Œè¯­è¨€å¼•å¯¼çš„æ“ä½œèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè§†è§‰-è¯­è¨€æ¨¡å‹(VLM)å›ºæœ‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œå®ƒä»¬åœ¨éœ€è¦ç²¾ç¡®ç©ºé—´æ¨ç†çš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼šä¸‹é™ã€‚ç°æœ‰çš„VLAä¾èµ–äºå¤§é‡çš„åŠ¨ä½œæ•°æ®é¢„è®­ç»ƒï¼Œä»¥å°†VLMå®šä½åœ¨3Dç©ºé—´ä¸­ï¼Œè¿™é™ä½äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶ä¸”å¯¹äºå‡†ç¡®çš„ç©ºé—´ç†è§£ä»ç„¶æ˜¯ä¸å¤Ÿçš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DepthVLAï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„VLAæ¶æ„ï¼Œå®ƒé€šè¿‡é¢„è®­ç»ƒçš„æ·±åº¦é¢„æµ‹æ¨¡å—æ˜¾å¼åœ°ç»“åˆäº†ç©ºé—´æ„ŸçŸ¥ã€‚DepthVLAé‡‡ç”¨æ··åˆTransformerçš„è®¾è®¡ï¼Œç»Ÿä¸€äº†VLMã€æ·±åº¦Transformerå’ŒåŠ¨ä½œä¸“å®¶ï¼Œå¹¶å…·æœ‰å®Œå…¨å…±äº«çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå½¢æˆäº†ä¸€ä¸ªå…·æœ‰å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚åœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒDepthVLAä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å–å¾—äº†78.5% vs. 65.0%çš„è¿›å±•ï¼Œåœ¨LIBEROæ¨¡æ‹Ÿå™¨ä¸­å–å¾—äº†94.9% vs. 93.6%çš„è¿›å±•ï¼Œåœ¨Simpleræ¨¡æ‹Ÿå™¨ä¸­å–å¾—äº†74.8% vs. 58.8%çš„è¿›å±•ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨å¤„ç†éœ€è¦ç²¾ç¡®ç©ºé—´æ¨ç†çš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹ä¾èµ–äºè§†è§‰-è¯­è¨€æ¨¡å‹(VLM)ï¼Œè€ŒVLMæœ¬èº«çš„ç©ºé—´æ¨ç†èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¤§é‡çš„åŠ¨ä½œæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥ä½¿VLMèƒ½å¤Ÿç†è§£3Dç©ºé—´ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æ•ˆç‡ä½ä¸‹ï¼Œå¹¶ä¸”ä»ç„¶æ— æ³•ä¿è¯å‡†ç¡®çš„ç©ºé—´ç†è§£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDepthVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ˜¾å¼åœ°å¼•å…¥æ·±åº¦ä¿¡æ¯æ¥å¢å¼ºVLAæ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ·±åº¦é¢„æµ‹æ¨¡å—æ¥ä¼°è®¡åœºæ™¯çš„æ·±åº¦å›¾ï¼Œå¹¶å°†æ·±åº¦ä¿¡æ¯èå…¥åˆ°VLAæ¨¡å‹çš„å¤„ç†æµç¨‹ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æ›´å‡†ç¡®åœ°ç†è§£åœºæ™¯çš„3Dç»“æ„ï¼Œä»è€Œæé«˜ç©ºé—´æ¨ç†çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDepthVLAçš„æ•´ä½“æ¶æ„æ˜¯ä¸€ä¸ªæ··åˆTransformeræ¨¡å‹ï¼Œå®ƒç”±ä¸‰ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šä¸€ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹(VLM)ï¼Œä¸€ä¸ªæ·±åº¦Transformerå’Œä¸€ä¸ªåŠ¨ä½œä¸“å®¶ã€‚VLMè´Ÿè´£å¤„ç†è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œæ·±åº¦Transformerè´Ÿè´£å¤„ç†æ·±åº¦ä¿¡æ¯ï¼ŒåŠ¨ä½œä¸“å®¶è´Ÿè´£ç”ŸæˆåŠ¨ä½œæŒ‡ä»¤ã€‚è¿™ä¸‰ä¸ªæ¨¡å—é€šè¿‡å®Œå…¨å…±äº«çš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè¿æ¥ï¼Œä»è€Œå®ç°ç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šDepthVLAçš„å…³é”®åˆ›æ–°åœ¨äºæ˜¾å¼åœ°å°†æ·±åº¦ä¿¡æ¯èå…¥åˆ°VLAæ¨¡å‹çš„å¤„ç†æµç¨‹ä¸­ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDepthVLAä¸éœ€è¦ä¾èµ–å¤§é‡çš„åŠ¨ä½œæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡é¢„è®­ç»ƒçš„æ·±åº¦é¢„æµ‹æ¨¡å—æ¥è·å–æ·±åº¦ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œè€Œä¸”è¿˜èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£åœºæ™¯çš„3Dç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šDepthVLAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„æ·±åº¦é¢„æµ‹æ¨¡å—æ¥ä¼°è®¡åœºæ™¯çš„æ·±åº¦å›¾ï¼›2) ä½¿ç”¨æ·±åº¦Transformeræ¥å¤„ç†æ·±åº¦ä¿¡æ¯ï¼›3) ä½¿ç”¨å®Œå…¨å…±äº«çš„æ³¨æ„åŠ›æœºåˆ¶æ¥è¿æ¥VLMã€æ·±åº¦Transformerå’ŒåŠ¨ä½œä¸“å®¶ï¼›4) ä½¿ç”¨ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹æ³•æ¥ä¼˜åŒ–æ•´ä¸ªæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DepthVLAåœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿç¯å¢ƒä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ï¼ŒDepthVLAçš„æ€§èƒ½æå‡äº†78.5%ï¼Œè€Œç°æœ‰æœ€ä½³æ–¹æ³•ä»…ä¸º65.0%ã€‚åœ¨LIBEROæ¨¡æ‹Ÿå™¨ä¸­ï¼ŒDepthVLAçš„æ€§èƒ½æå‡åˆ°äº†94.9%ï¼Œè€Œç°æœ‰æœ€ä½³æ–¹æ³•ä¸º93.6%ã€‚åœ¨Simpleræ¨¡æ‹Ÿå™¨ä¸­ï¼ŒDepthVLAçš„æ€§èƒ½æå‡åˆ°äº†74.8%ï¼Œè€Œç°æœ‰æœ€ä½³æ–¹æ³•ä¸º58.8%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDepthVLAèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜VLAæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DepthVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å‡†ç¡®åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œä»è€Œæ‰§è¡Œæ›´å¤æ‚çš„ä»»åŠ¡ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå®ƒå¯ä»¥æé«˜è½¦è¾†å¯¹å‘¨å›´ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚åœ¨è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®é¢†åŸŸï¼Œå®ƒå¯ä»¥æä¾›æ›´é€¼çœŸçš„3Dä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have recently shown impressive generalization and language-guided manipulation capabilities. However, their performance degrades on tasks requiring precise spatial reasoning due to limited spatial reasoning inherited from Vision-Language Models (VLMs). Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D space, which reduces training efficiency and is still insufficient for accurate spatial understanding. In this work, we present DepthVLA, a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction module. DepthVLA adopts a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial reasoning. Extensive evaluations in both real-world and simulated environments show that DepthVLA outperforms state-of-the-art approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs. 93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator. Our code will be made publicly available.

