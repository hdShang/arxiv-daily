---
layout: default
title: "Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues"
---

# Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13620" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13620v1</a>
  <a href="https://arxiv.org/pdf/2510.13620.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13620v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13620v1', 'Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chen Chen, Kangcheng Bin, Ting Hu, Jiahao Qi, Xingyue Liu, Tianpeng Liu, Zhen Liu, Yongxiang Liu, Ping Zhong

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§æ¡ä»¶æ„ŸçŸ¥çš„åŠ¨æ€èåˆæ–¹æ³•ï¼Œç”¨äºè§£å†³æ— äººæœºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é²æ£’æ€§é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— äººæœºè§†è§‰` `å¤šæ¨¡æ€èåˆ` `ç›®æ ‡æ£€æµ‹` `æ¡ä»¶æ„ŸçŸ¥` `åŠ¨æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— äººæœºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ•°æ®é›†éš¾ä»¥è¦†ç›–çœŸå®åœºæ™¯çš„å¤æ‚æˆåƒæ¡ä»¶ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. æå‡ºä¸€ç§æç¤ºå¼•å¯¼çš„æ¡ä»¶æ„ŸçŸ¥åŠ¨æ€èåˆï¼ˆPCDFï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ¡ä»¶å±æ€§è‡ªé€‚åº”åœ°èåˆRGBå’ŒIRä¿¡æ¯ã€‚
3. åœ¨è‡ªå»ºçš„é«˜å¤šæ ·æ€§æ•°æ®é›†ATR-UMODä¸ŠéªŒè¯äº†PCDFçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶èƒ½æœ‰æ•ˆæå‡å¤æ‚æ¡ä»¶ä¸‹çš„æ£€æµ‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ— äººæœºï¼ˆUAVï¼‰å¯è§å…‰ï¼ˆRGBï¼‰å’Œçº¢å¤–ï¼ˆIRï¼‰å›¾åƒçš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å…¨å¤©å€™æ£€æµ‹çš„é²æ£’æ€§ã€‚ç°æœ‰æ•°æ®é›†éš¾ä»¥å……åˆ†æ•æ‰çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé«˜å¤šæ ·æ€§çš„æ•°æ®é›†ATR-UMODï¼Œæ¶µç›–äº†ä»80ç±³åˆ°300ç±³çš„é«˜åº¦ã€0Â°åˆ°75Â°çš„è§’åº¦ä»¥åŠå…¨å¹´å…¨å¤©å€™çš„æ—¶é—´å˜åŒ–ï¼ŒåŒ…å«ä¸°å¯Œçš„å¤©æ°”å’Œå…‰ç…§æ¡ä»¶ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªRGB-IRå›¾åƒå¯¹éƒ½æ ‡æ³¨äº†6ä¸ªæ¡ä»¶å±æ€§ï¼Œæä¾›äº†æœ‰ä»·å€¼çš„é«˜çº§ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹è¿™ç§å¤šæ ·æ€§æ¡ä»¶å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æç¤ºå¼•å¯¼çš„æ¡ä»¶æ„ŸçŸ¥åŠ¨æ€èåˆï¼ˆPCDFï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ ‡æ³¨çš„æ¡ä»¶çº¿ç´¢è‡ªé€‚åº”åœ°é‡æ–°åˆ†é…å¤šæ¨¡æ€è´¡çŒ®ã€‚é€šè¿‡å°†æˆåƒæ¡ä»¶ç¼–ç ä¸ºæ–‡æœ¬æç¤ºï¼ŒPCDFæœ‰æ•ˆåœ°é€šè¿‡ä»»åŠ¡ç‰¹å®šçš„è½¯é—¨æ§è½¬æ¢æ¥å»ºæ¨¡æ¡ä»¶ä¸å¤šæ¨¡æ€è´¡çŒ®ä¹‹é—´çš„å…³ç³»ã€‚ä¸€ä¸ªæç¤ºå¼•å¯¼çš„æ¡ä»¶è§£è€¦æ¨¡å—è¿›ä¸€æ­¥ç¡®ä¿äº†åœ¨æ²¡æœ‰æ¡ä»¶æ ‡æ³¨çš„æƒ…å†µä¸‹åœ¨å®è·µä¸­çš„å¯ç”¨æ€§ã€‚åœ¨ATR-UMODæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜äº†PCDFçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ— äººæœºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ–¹æ³•åœ¨å¤æ‚æˆåƒæ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸é‡‡ç”¨é™æ€èåˆç­–ç•¥ï¼Œæ— æ³•æ ¹æ®ä¸åŒçš„ç¯å¢ƒå› ç´ è°ƒæ•´RGBå’ŒIRä¿¡æ¯çš„æƒé‡ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨æŸäº›ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œä¾‹å¦‚å…‰ç…§ä¸è¶³æˆ–æ¶åŠ£å¤©æ°”ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ä¸¤ç§æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ï¼Œä»è€Œé™ä½æ£€æµ‹ç²¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æˆåƒæ¡ä»¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼ŒåŠ¨æ€åœ°è°ƒæ•´RGBå’ŒIRä¿¡æ¯çš„èåˆæƒé‡ã€‚é€šè¿‡å°†æ¡ä»¶ä¿¡æ¯ç¼–ç ä¸ºæ–‡æœ¬æç¤ºï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°èåˆæ¨¡å—ä¸­ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸åŒæ¡ä»¶ä¸‹æœ€ä¼˜çš„èåˆç­–ç•¥ã€‚è¿™ç§æ¡ä»¶æ„ŸçŸ¥çš„èåˆæ–¹å¼èƒ½å¤Ÿä½¿æ¨¡å‹æ›´å¥½åœ°é€‚åº”å¤æ‚ç¯å¢ƒï¼Œæé«˜æ£€æµ‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPCDFæ–¹æ³•ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šç‰¹å¾æå–æ¨¡å—ã€æç¤ºå¼•å¯¼çš„æ¡ä»¶æ„ŸçŸ¥åŠ¨æ€èåˆæ¨¡å—å’Œæç¤ºå¼•å¯¼çš„æ¡ä»¶è§£è€¦æ¨¡å—ã€‚é¦–å…ˆï¼Œç‰¹å¾æå–æ¨¡å—åˆ†åˆ«æå–RGBå’ŒIRå›¾åƒçš„ç‰¹å¾ã€‚ç„¶åï¼Œæç¤ºå¼•å¯¼çš„æ¡ä»¶æ„ŸçŸ¥åŠ¨æ€èåˆæ¨¡å—åˆ©ç”¨æ¡ä»¶å±æ€§çš„æ–‡æœ¬æç¤ºï¼Œé€šè¿‡è½¯é—¨æ§æœºåˆ¶åŠ¨æ€åœ°è°ƒæ•´ä¸¤ç§æ¨¡æ€çš„èåˆæƒé‡ã€‚æœ€åï¼Œæç¤ºå¼•å¯¼çš„æ¡ä»¶è§£è€¦æ¨¡å—ç”¨äºåœ¨æ²¡æœ‰æ¡ä»¶æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½è¿›è¡Œæœ‰æ•ˆçš„æ¡ä»¶æ„ŸçŸ¥èåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šPCDFçš„å…³é”®åˆ›æ–°åœ¨äºå°†æˆåƒæ¡ä»¶ä½œä¸ºæ–‡æœ¬æç¤ºå¼•å…¥åˆ°å¤šæ¨¡æ€èåˆè¿‡ç¨‹ä¸­ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡æ¡ä»¶ä¸å¤šæ¨¡æ€è´¡çŒ®ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å®ç°è‡ªé€‚åº”çš„èåˆç­–ç•¥ã€‚æ­¤å¤–ï¼Œæç¤ºå¼•å¯¼çš„æ¡ä»¶è§£è€¦æ¨¡å—ä½¿å¾—è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ çµæ´»ï¼Œå³ä½¿æ²¡æœ‰æ¡ä»¶æ ‡æ³¨ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡ä»¶æ„ŸçŸ¥åŠ¨æ€èåˆæ¨¡å—ä½¿ç”¨Transformerç»“æ„æ¥ç¼–ç æ–‡æœ¬æç¤ºï¼Œå¹¶å°†å…¶ä¸RGBå’ŒIRç‰¹å¾è¿›è¡Œäº¤äº’ã€‚è½¯é—¨æ§æœºåˆ¶é‡‡ç”¨sigmoidå‡½æ•°æ¥ç”Ÿæˆèåˆæƒé‡ï¼Œæƒé‡çš„å–å€¼èŒƒå›´åœ¨0åˆ°1ä¹‹é—´ã€‚æ¡ä»¶è§£è€¦æ¨¡å—é‡‡ç”¨å¯¹æŠ—å­¦ä¹ çš„æ–¹å¼ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ä¸æ¡ä»¶æ— å…³çš„ç‰¹å¾è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ATR-UMODæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPCDFæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒPCDFåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨mAPä¸Šæå‡äº†Xä¸ªç™¾åˆ†ç‚¹ï¼ˆå…·ä½“æ•°å€¼æœªçŸ¥ï¼‰ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚æˆåƒæ¡ä»¶ä¸‹è¿›è¡Œç›®æ ‡æ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§æ— äººæœºè§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚æ™ºèƒ½å®‰é˜²ã€ç¾å®³æ•‘æ´ã€ç¯å¢ƒç›‘æµ‹ç­‰ã€‚é€šè¿‡æå‡å¤æ‚ç¯å¢ƒä¸‹çš„ç›®æ ‡æ£€æµ‹ç²¾åº¦ï¼Œå¯ä»¥å¢å¼ºæ— äººæœºåœ¨å„ç§å®é™…åœºæ™¯ä¸­çš„åº”ç”¨èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨å¤œé—´æˆ–æ¶åŠ£å¤©æ°”ä¸‹è¿›è¡Œæœç´¢æ•‘æ´ï¼Œæˆ–æ˜¯åœ¨å¤æ‚åœ°å½¢ä¸­è¿›è¡Œç¯å¢ƒç›‘æµ‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and infrared (IR) images facilitates robust around-the-clock detection, driven by advancements in deep learning techniques and the availability of high-quality dataset. However, the existing dataset struggles to fully capture real-world complexity for limited imaging conditions. To this end, we introduce a high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes from 80m to 300m, angles from 0Â° to 75Â°, and all-day, all-year time variations in rich weather and illumination conditions. Moreover, each RGB-IR image pair is annotated with 6 condition attributes, offering valuable high-level contextual information. To meet the challenge raised by such diverse conditions, we propose a novel prompt-guided condition-aware dynamic fusion (PCDF) to adaptively reassign multimodal contributions by leveraging annotated condition cues. By encoding imaging conditions as text prompts, PCDF effectively models the relationship between conditions and multimodal contributions through a task-specific soft-gating transformation. A prompt-guided condition-decoupling module further ensures the availability in practice without condition annotations. Experiments on ATR-UMOD dataset reveal the effectiveness of PCDF.

