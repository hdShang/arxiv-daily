---
layout: default
title: Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs
---

# Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13251" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13251v1</a>
  <a href="https://arxiv.org/pdf/2510.13251.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13251v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13251v1', 'Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minji Kim, Taekyung Kim, Bohyung Han

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**å¤‡æ³¨**: 23 pages, 28 figures, 8 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ­ç¤ºVideoLLMä¿¡æ¯æµåŠ¨è·¯å¾„ï¼šé€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§åˆ†ææ—¶åºæ¨ç†è¿‡ç¨‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `VideoLLM` `è§†é¢‘é—®ç­”` `æœºåˆ¶å¯è§£é‡Šæ€§` `æ—¶åºæ¨ç†` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VideoLLMåœ¨æ—¶ç©ºæ•°æ®å¤„ç†ä¸Šèƒ½åŠ›æ˜¾è‘—ï¼Œä½†å¯¹å…¶å†…éƒ¨ä¿¡æ¯æå–å’Œä¼ æ’­æœºåˆ¶çš„ç†è§£ä¸è¶³ã€‚
2. è¯¥ç ”ç©¶åˆ©ç”¨æœºåˆ¶å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œåˆ†æVideoLLMå†…éƒ¨ä¿¡æ¯æµåŠ¨ï¼Œæ­ç¤ºæ—¶åºæ¨ç†çš„å…³é”®è·¯å¾„ã€‚
3. å®éªŒè¡¨æ˜ï¼Œé€šè¿‡é€‰æ‹©æœ‰æ•ˆä¿¡æ¯è·¯å¾„å¹¶æŠ‘åˆ¶å†—ä½™æ³¨æ„åŠ›ï¼ŒVideoLLMèƒ½åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—é‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œæ·±å…¥ç ”ç©¶äº†VideoLLMå†…éƒ¨çš„ä¿¡æ¯æµåŠ¨æœºåˆ¶ï¼Œæ—¨åœ¨ç†è§£æ¨¡å‹å¦‚ä½•æå–å’Œä¼ æ’­è§†é¢‘åŠæ–‡æœ¬ä¿¡æ¯ã€‚ç ”ç©¶æ­ç¤ºäº†VideoQAä»»åŠ¡ä¸­ä¸€è‡´çš„æ¨¡å¼ï¼šæ—¶åºæ¨ç†å§‹äºæ—©æœŸåˆ°ä¸­é—´å±‚ä¸­æ´»è·ƒçš„è·¨å¸§äº¤äº’ï¼Œéšåæ˜¯ä¸­é—´å±‚ä¸­æ¸è¿›å¼çš„è§†é¢‘-è¯­è¨€èåˆï¼Œè¿™å¾—ç›Šäºè§†é¢‘è¡¨å¾ä¸åŒ…å«æ—¶é—´æ¦‚å¿µçš„è¯­è¨€åµŒå…¥ä¹‹é—´çš„å¯¹é½ã€‚å®Œæˆèåˆåï¼Œæ¨¡å‹å³å¯åœ¨ä¸­é—´åˆ°åæœŸå±‚ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆã€‚åŸºäºæ­¤åˆ†æï¼Œç ”ç©¶è¡¨æ˜VideoLLMå¯ä»¥é€šè¿‡é€‰æ‹©æœ‰æ•ˆçš„ä¿¡æ¯è·¯å¾„ï¼ŒåŒæ—¶æŠ‘åˆ¶å¤§é‡æ³¨æ„åŠ›è¾¹ï¼ˆä¾‹å¦‚ï¼ŒLLaVA-NeXT-7B-Video-FTä¸­ä¸º58%ï¼‰ï¼Œæ¥ä¿æŒå…¶VideoQAæ€§èƒ½ã€‚è¿™äº›å‘ç°ä¸ºVideoLLMå¦‚ä½•æ‰§è¡Œæ—¶åºæ¨ç†æä¾›äº†è“å›¾ï¼Œå¹¶ä¸ºæé«˜æ¨¡å‹å¯è§£é‡Šæ€§å’Œä¸‹æ¸¸æ³›åŒ–èƒ½åŠ›æä¾›äº†å®è·µè§è§£ã€‚é¡¹ç›®ä¸»é¡µåŒ…å«æºä»£ç ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVideoLLMåœ¨å¤„ç†è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰ä»»åŠ¡æ—¶ï¼Œå…¶å†…éƒ¨å¦‚ä½•è¿›è¡Œæ—¶åºæ¨ç†å’Œå¤šæ¨¡æ€ä¿¡æ¯èåˆæ˜¯ä¸€ä¸ªé»‘ç›’ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹VideoLLMå†…éƒ¨ä¿¡æ¯æµåŠ¨æœºåˆ¶çš„æ·±å…¥ç†è§£ï¼Œéš¾ä»¥è§£é‡Šæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œä¹Ÿé™åˆ¶äº†æ¨¡å‹ä¼˜åŒ–å’Œæ³›åŒ–èƒ½åŠ›çš„æå‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œè¿½è¸ªVideoLLMå†…éƒ¨çš„ä¿¡æ¯æµåŠ¨è·¯å¾„ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹è¿›è¡Œæ—¶åºæ¨ç†å’Œå¤šæ¨¡æ€èåˆçš„å…³é”®æ­¥éª¤ã€‚é€šè¿‡åˆ†æä¸åŒå±‚ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’å’Œäº¤äº’ï¼Œç†è§£æ¨¡å‹å¦‚ä½•ä»è§†é¢‘å¸§ä¸­æå–æ—¶åºä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ•´åˆï¼Œæœ€ç»ˆç”Ÿæˆç­”æ¡ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç ”ç©¶çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) é€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„VideoLLMæ¨¡å‹ï¼ˆå¦‚LLaVA-NeXT-7B-Video-FTï¼‰ï¼›2) è®¾è®¡å¤šæ ·åŒ–çš„VideoQAä»»åŠ¡ï¼›3) åˆ©ç”¨æœºåˆ¶å¯è§£é‡Šæ€§æŠ€æœ¯ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶åˆ†æã€æ¿€æ´»å€¼åˆ†æç­‰ï¼‰è¿½è¸ªæ¨¡å‹å†…éƒ¨çš„ä¿¡æ¯æµåŠ¨ï¼›4) åˆ†æä¸åŒå±‚ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’å’Œäº¤äº’æ¨¡å¼ï¼›5) è¯†åˆ«æ—¶åºæ¨ç†å’Œå¤šæ¨¡æ€èåˆçš„å…³é”®è·¯å¾„ï¼›6) é€šè¿‡é€‰æ‹©æœ‰æ•ˆä¿¡æ¯è·¯å¾„å¹¶æŠ‘åˆ¶å†—ä½™æ³¨æ„åŠ›ï¼Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æœºåˆ¶å¯è§£é‡Šæ€§æŠ€æœ¯åº”ç”¨äºVideoLLMï¼Œä»è€Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨ä¿¡æ¯æµåŠ¨çš„å…·ä½“è·¯å¾„å’Œæ¨¡å¼ã€‚é€šè¿‡åˆ†ææ³¨æ„åŠ›æœºåˆ¶å’Œæ¿€æ´»å€¼ï¼Œç ”ç©¶äººå‘˜èƒ½å¤Ÿè¯†åˆ«å‡ºæ¨¡å‹è¿›è¡Œæ—¶åºæ¨ç†å’Œå¤šæ¨¡æ€èåˆçš„å…³é”®å±‚å’Œå…³é”®èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§é€šè¿‡é€‰æ‹©æœ‰æ•ˆä¿¡æ¯è·¯å¾„å¹¶æŠ‘åˆ¶å†—ä½™æ³¨æ„åŠ›æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒè®¾è®¡æ–¹é¢ï¼Œç ”ç©¶äººå‘˜é€‰æ‹©äº†å¤šç§VideoQAä»»åŠ¡ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ—¶åºæ¨ç†èƒ½åŠ›ã€‚åœ¨æ¨¡å‹åˆ†ææ–¹é¢ï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨äº†å¤šç§æœºåˆ¶å¯è§£é‡Šæ€§æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–ã€æ¿€æ´»å€¼åˆ†æã€ä¿¡æ¯ä¼ é€’è·¯å¾„åˆ†æç­‰ã€‚åœ¨æ¨¡å‹ä¼˜åŒ–æ–¹é¢ï¼Œç ”ç©¶äººå‘˜è®¾è®¡äº†ä¸€ç§åŸºäºä¿¡æ¯æµåŠ¨è·¯å¾„é€‰æ‹©çš„æ³¨æ„åŠ›å‰ªææ–¹æ³•ï¼Œé€šè¿‡æŠ‘åˆ¶å†—ä½™æ³¨æ„åŠ›ï¼Œå‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç ”ç©¶é€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§åˆ†æï¼Œæ­ç¤ºäº†VideoLLMè¿›è¡Œæ—¶åºæ¨ç†çš„å…³é”®è·¯å¾„ï¼Œå¹¶å‘ç°æ¨¡å‹å¯ä»¥é€šè¿‡é€‰æ‹©æœ‰æ•ˆä¿¡æ¯è·¯å¾„å¹¶æŠ‘åˆ¶å†—ä½™æ³¨æ„åŠ›æ¥ä¿æŒæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨LLaVA-NeXT-7B-Video-FTæ¨¡å‹ä¸­ï¼Œå¯ä»¥æŠ‘åˆ¶é«˜è¾¾58%çš„æ³¨æ„åŠ›è¾¹ï¼ŒåŒæ—¶ä¿æŒVideoQAæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡VideoLLMçš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ï¼Œä¾‹å¦‚åœ¨è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç›‘æ§ç­‰å®‰å…¨æ”¸å…³é¢†åŸŸï¼Œç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ä¸ºä¼˜åŒ–VideoLLMçš„ç»“æ„å’Œè®­ç»ƒæä¾›äº†æŒ‡å¯¼ï¼Œæœ‰åŠ©äºå¼€å‘æ›´é«˜æ•ˆã€æ›´å¼ºå¤§çš„è§†é¢‘ç†è§£æ¨¡å‹ã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•åˆ©ç”¨è¿™äº›å‘ç°æ¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Video Large Language Models (VideoLLMs) extend the capabilities of vision-language models to spatiotemporal inputs, enabling tasks such as video question answering (VideoQA). Despite recent advances in VideoLLMs, their internal mechanisms on where and how they extract and propagate video and textual information remain less explored. In this study, we investigate the internal information flow of VideoLLMs using mechanistic interpretability techniques. Our analysis reveals consistent patterns across diverse VideoQA tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame interactions in early-to-middle layers, (2) followed by progressive video-language integration in middle layers. This is facilitated by alignment between video representations and linguistic embeddings containing temporal concepts. (3) Upon completion of this integration, the model is ready to generate correct answers in middle-to-late layers. (4) Based on our analysis, we show that VideoLLMs can retain their VideoQA performance by selecting these effective information pathways while suppressing a substantial amount of attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a blueprint on how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization. Our project page with the source code is available at https://map-the-flow.github.io

