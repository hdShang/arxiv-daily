---
layout: default
title: Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models
---

# Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13237" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13237v1</a>
  <a href="https://arxiv.org/pdf/2510.13237.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13237v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13237v1', 'Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, Jingfeng Zhang

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://edpa-attack.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ¨¡å‹æ— å…³å¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å¯¹æŠ—æ”»å‡»` `å¯¹æŠ—é˜²å¾¡` `æ¨¡å‹æ— å…³` `æœºå™¨äººå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VLAæ¨¡å‹åœ¨æœºå™¨äººå­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¯¹æŠ—é²æ£’æ€§ä¸è¶³ï¼Œå®¹æ˜“å—åˆ°æ”»å‡»ã€‚
2. æå‡ºåµŒå…¥æ‰°åŠ¨è¡¥ä¸æ”»å‡»(EDPA)ï¼Œé€šè¿‡æ‰°ä¹±è§†è§‰å’Œæ–‡æœ¬çš„è¯­ä¹‰å¯¹é½æ¥æ¬ºéª—VLAæ¨¡å‹ã€‚
3. è®¾è®¡å¯¹æŠ—å¾®è°ƒé˜²å¾¡æœºåˆ¶ï¼Œæå‡æ¨¡å‹å¯¹å¯¹æŠ—æ ·æœ¬çš„é²æ£’æ€§ï¼Œå¹¶åœ¨LIBEROåŸºå‡†ä¸ŠéªŒè¯æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨æœºå™¨äººå­¦ä¹ é¢†åŸŸå–å¾—äº†é©å‘½æ€§çš„è¿›å±•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ‰§è¡Œå¤æ‚çš„ç‰©ç†ä»»åŠ¡ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†å®ƒä»¬å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é’ˆå¯¹VLAæ¨¡å‹ï¼Œæå‡ºäº†å¯¹æŠ—è¡¥ä¸æ”»å‡»å’Œç›¸åº”çš„é˜²å¾¡ç­–ç•¥ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†åµŒå…¥æ‰°åŠ¨è¡¥ä¸æ”»å‡»(EDPA)ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„å¯¹æŠ—æ”»å‡»ï¼Œå¯ä»¥ç›´æ¥å°†ç”Ÿæˆçš„è¡¥ä¸æ”¾ç½®åœ¨æ‘„åƒå¤´çš„è§†é‡ä¸­ã€‚ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒEDPAå¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºä¸åŒçš„VLAæ¨¡å‹ï¼Œè€Œä¸éœ€è¦äº‹å…ˆäº†è§£æ¨¡å‹æ¶æ„æˆ–å—æ§çš„æœºå™¨äººæœºæ¢°è‡‚ã€‚EDPAé€šè¿‡(i)æ‰°ä¹±è§†è§‰å’Œæ–‡æœ¬æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œä»¥åŠ(ii)æœ€å¤§åŒ–å¯¹æŠ—å’Œç›¸åº”çš„å¹²å‡€è§†è§‰è¾“å…¥ä¹‹é—´æ½œåœ¨è¡¨ç¤ºçš„å·®å¼‚æ¥æ„å»ºè¿™äº›è¡¥ä¸ã€‚é€šè¿‡ä¼˜åŒ–è¿™äº›ç›®æ ‡ï¼ŒEDPAæ‰­æ›²äº†VLAå¯¹è§†è§‰ä¿¡æ¯çš„è§£é‡Šï¼Œå¯¼è‡´æ¨¡å‹é‡å¤ç”Ÿæˆä¸æ­£ç¡®çš„åŠ¨ä½œï¼Œæœ€ç»ˆå¯¼è‡´æ— æ³•å®Œæˆç»™å®šçš„æœºå™¨äººä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹è§†è§‰ç¼–ç å™¨çš„å¯¹æŠ—å¾®è°ƒæ–¹æ¡ˆï¼Œå…¶ä¸­ä¼˜åŒ–ç¼–ç å™¨ï¼Œä½¿å…¶ä¸ºå¹²å‡€çš„å’Œå¯¹æŠ—æ‰°åŠ¨çš„è§†è§‰è¾“å…¥ç”Ÿæˆç›¸ä¼¼çš„æ½œåœ¨è¡¨ç¤ºã€‚åœ¨å¹¿æ³›è®¤å¯çš„LIBEROæœºå™¨äººä»¿çœŸåŸºå‡†ä¸Šçš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒEDPAå¤§å¤§æé«˜äº†æœ€å…ˆè¿›çš„VLAæ¨¡å‹çš„ä»»åŠ¡å¤±è´¥ç‡ï¼Œè€Œæˆ‘ä»¬æå‡ºçš„é˜²å¾¡æœ‰æ•ˆåœ°ç¼“è§£äº†è¿™ç§é€€åŒ–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å®¹æ˜“å—åˆ°å¯¹æŠ—æ”»å‡»çš„å½±å“ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¦‚ä½•åœ¨ä¸äº†è§£æ¨¡å‹å†…éƒ¨ç»“æ„çš„æƒ…å†µä¸‹ï¼Œè®¾è®¡æœ‰æ•ˆçš„å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œå¹¶æå‡ºç›¸åº”çš„é˜²å¾¡ç­–ç•¥ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šæ¨¡å‹è¿›è¡Œè®¾è®¡ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§æ¨¡å‹æ— å…³çš„å¯¹æŠ—è¡¥ä¸æ”»å‡»(EDPA)ï¼Œé€šè¿‡æ‰°ä¹±è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œä½¿å¾—VLAæ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ç†è§£äº§ç”Ÿåå·®ï¼Œä»è€Œå¯¼è‡´é”™è¯¯çš„åŠ¨ä½œè¾“å‡ºã€‚åŒæ—¶ï¼Œæå‡ºä¸€ç§å¯¹æŠ—å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡è®©æ¨¡å‹å­¦ä¹ å¯¹å¯¹æŠ—æ ·æœ¬å…·æœ‰é²æ£’æ€§çš„ç‰¹å¾è¡¨ç¤ºï¼Œæ¥æé«˜æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEDPAæ”»å‡»æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç›®æ ‡ï¼šä¸€æ˜¯æ‰°ä¹±è§†è§‰å’Œæ–‡æœ¬æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼ŒäºŒæ˜¯æœ€å¤§åŒ–å¯¹æŠ—æ ·æœ¬å’Œå¹²å‡€æ ·æœ¬åœ¨æ½œåœ¨è¡¨ç¤ºä¸Šçš„å·®å¼‚ã€‚é˜²å¾¡æ¡†æ¶åˆ™é‡‡ç”¨å¯¹æŠ—å¾®è°ƒçš„æ–¹å¼ï¼Œé€šè¿‡æœ€å°åŒ–å¹²å‡€æ ·æœ¬å’Œå¯¹æŠ—æ ·æœ¬åœ¨è§†è§‰ç¼–ç å™¨è¾“å‡ºçš„æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„è·ç¦»ï¼Œæ¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆä½¿ç”¨EDPAç”Ÿæˆå¯¹æŠ—è¡¥ä¸ï¼Œç„¶åå°†è¡¥ä¸æ·»åŠ åˆ°è¾“å…¥å›¾åƒä¸­ï¼Œå†å°†å¸¦æœ‰è¡¥ä¸çš„å›¾åƒè¾“å…¥åˆ°VLAæ¨¡å‹ä¸­ï¼Œè§‚å¯Ÿæ¨¡å‹çš„è¡Œä¸ºã€‚ä¸ºäº†é˜²å¾¡æ”»å‡»ï¼Œä½¿ç”¨å¯¹æŠ—å¾®è°ƒç­–ç•¥è®­ç»ƒè§†è§‰ç¼–ç å™¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šEDPAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¨¡å‹æ— å…³æ€§ï¼Œå®ƒä¸éœ€è¦äº†è§£VLAæ¨¡å‹çš„å…·ä½“æ¶æ„ï¼Œå°±å¯ä»¥æœ‰æ•ˆåœ°æ”»å‡»æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç›´æ¥åœ¨å›¾åƒç©ºé—´ä¸­æ·»åŠ è¡¥ä¸ï¼Œä½¿å¾—æ”»å‡»æ›´å…·å®é™…æ„ä¹‰ã€‚å¯¹æŠ—å¾®è°ƒé˜²å¾¡ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ¨¡å‹å¯¹EDPAæ”»å‡»çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šEDPAæ”»å‡»çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) è¯­ä¹‰å¯¹é½æŸå¤±ï¼šç”¨äºè¡¡é‡è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œé€šè¿‡æœ€å°åŒ–è¯¥æŸå¤±æ¥æ‰°ä¹±è¯­ä¹‰å¯¹é½ã€‚(2) å·®å¼‚æœ€å¤§åŒ–æŸå¤±ï¼šç”¨äºæœ€å¤§åŒ–å¯¹æŠ—æ ·æœ¬å’Œå¹²å‡€æ ·æœ¬åœ¨æ½œåœ¨è¡¨ç¤ºä¸Šçš„å·®å¼‚ï¼Œä½¿å¾—æ”»å‡»æ›´åŠ æœ‰æ•ˆã€‚å¯¹æŠ—å¾®è°ƒé˜²å¾¡çš„å…³é”®è®¾è®¡åœ¨äºï¼š(1) å¯¹æŠ—è®­ç»ƒæ•°æ®ç”Ÿæˆï¼šä½¿ç”¨EDPAç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚(2) æŸå¤±å‡½æ•°ï¼šæœ€å°åŒ–å¹²å‡€æ ·æœ¬å’Œå¯¹æŠ—æ ·æœ¬åœ¨è§†è§‰ç¼–ç å™¨è¾“å‡ºçš„æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„è·ç¦»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEDPAæ”»å‡»èƒ½å¤Ÿæ˜¾è‘—æé«˜æœ€å…ˆè¿›VLAæ¨¡å‹çš„ä»»åŠ¡å¤±è´¥ç‡ã€‚åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­ï¼ŒEDPAæ”»å‡»ä½¿å¾—VLAæ¨¡å‹çš„ä»»åŠ¡æˆåŠŸç‡å¤§å¹…ä¸‹é™ã€‚åŒæ—¶ï¼Œæå‡ºçš„å¯¹æŠ—å¾®è°ƒé˜²å¾¡ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£è¿™ç§æ€§èƒ½ä¸‹é™ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹å¯¹EDPAæ”»å‡»çš„é²æ£’æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®åœ¨è®ºæ–‡ä¸­è¯¦ç»†å±•ç¤ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæå‡æœºå™¨äººç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¸äººç±»äº¤äº’æˆ–åœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡çš„åœºæ™¯ä¸­ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ã€åŒ»ç–—æœºå™¨äººç­‰é¢†åŸŸï¼Œæé«˜VLAæ¨¡å‹å¯¹æŠ—æ¶æ„æ”»å‡»çš„é²æ£’æ€§è‡³å…³é‡è¦ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´é«˜æ•ˆçš„é˜²å¾¡ç­–ç•¥ï¼Œä»¥åŠæ›´å…·éšè”½æ€§çš„æ”»å‡»æ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions. Despite this progress, their adversarial robustness remains underexplored. In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models. We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view. In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator. EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs. Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task. To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs. Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation. The codebase is accessible via the homepage at https://edpa-attack.github.io/.

