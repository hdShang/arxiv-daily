---
layout: default
title: "OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild"
---

# OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13660" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13660v2</a>
  <a href="https://arxiv.org/pdf/2510.13660.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13660v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13660v2', 'OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyu Qu, Jianan Wei, Xiangbo Shu, Yazhou Yao, Wenguan Wang, Jinhui Tang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15 (æ›´æ–°: 2025-10-16)

**å¤‡æ³¨**: Accepted to NeurIPS 2025; Project page: https://github.com/quhongyu/OmniGaze

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniGazeï¼šæå‡ºå¥–åŠ±é©±åŠ¨çš„é€šç”¨å‡è§†ä¼°è®¡æ¡†æ¶ï¼Œè§£å†³é‡å¤–åœºæ™¯æ³›åŒ–æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‡è§†ä¼°è®¡` `åŠç›‘ç£å­¦ä¹ ` `ä¼ªæ ‡ç­¾` `é¢†åŸŸæ³›åŒ–` `å¥–åŠ±æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Då‡è§†ä¼°è®¡æ–¹æ³•åœ¨è·¨é¢†åŸŸæ³›åŒ–æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦å—é™äºæ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§ä¸è¶³ã€‚
2. OmniGazeåˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡ä¼ªæ ‡ç­¾å’Œå¥–åŠ±æ¨¡å‹æ¥æå‡æ¨¡å‹åœ¨é‡å¤–åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOmniGazeåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„3Då‡è§†ä¼°è®¡æ–¹æ³•éš¾ä»¥åœ¨ä¸åŒçš„æ•°æ®é¢†åŸŸä¸­æ³›åŒ–ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå¸¦æ ‡æ³¨æ•°æ®é›†çš„ç¨€ç¼ºä»¥åŠæ ‡æ³¨æ•°æ®çš„å¤šæ ·æ€§ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†OmniGazeï¼Œä¸€ä¸ªåŠç›‘ç£çš„3Då‡è§†ä¼°è®¡æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä»å¤šæ ·ä¸”æ— çº¦æŸçš„çœŸå®ä¸–ç•Œç¯å¢ƒä¸­æ”¶é›†çš„å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®æ¥ç¼“è§£é¢†åŸŸåå·®ï¼Œå¹¶æ¨å¹¿é‡å¤–åœºæ™¯ä¸‹çš„å‡è§†ä¼°è®¡ã€‚é¦–å…ˆï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æ— æ ‡æ³¨é¢éƒ¨å›¾åƒé›†åˆï¼Œè¿™äº›å›¾åƒåœ¨é¢éƒ¨å¤–è§‚ã€èƒŒæ™¯ç¯å¢ƒã€å…‰ç…§æ¡ä»¶ã€å¤´éƒ¨å§¿åŠ¿å’Œçœ¼ç›é®æŒ¡æ–¹é¢å„ä¸ç›¸åŒã€‚ä¸ºäº†åˆ©ç”¨æ›´å¹¿æ³›åˆ†å¸ƒçš„æ— æ ‡æ³¨æ•°æ®ï¼ŒOmniGazeé‡‡ç”¨æ ‡å‡†çš„ä¼ªæ ‡ç­¾ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°ä¼ªæ ‡ç­¾çš„å¯é æ€§ã€‚é™¤äº†å°†ä¼ªæ ‡ç­¾ä½œä¸º3Dæ–¹å‘å‘é‡å¤–ï¼Œå¥–åŠ±æ¨¡å‹è¿˜ç»“åˆäº†ç”±ç°æˆçš„è§†è§‰ç¼–ç å™¨æå–çš„è§†è§‰åµŒå…¥ä»¥åŠé€šè¿‡æç¤ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å‡è§†è§†è§’çš„è¯­ä¹‰çº¿ç´¢ï¼Œä»¥è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°ã€‚ç„¶åï¼Œè¿™äº›åˆ†æ•°ç”¨äºé€‰æ‹©é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾å¹¶å¯¹å…¶è¿›è¡ŒåŠ æƒä»¥è¿›è¡ŒæŸå¤±è®¡ç®—ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOmniGazeåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åŒé¢†åŸŸå’Œè·¨é¢†åŸŸè®¾ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†OmniGazeä½œä¸ºå‡è§†ä¼°è®¡çš„å¯æ‰©å±•æ•°æ®å¼•æ“çš„æœ‰æ•ˆæ€§ï¼Œå®ƒåœ¨å››ä¸ªæœªè§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰3Då‡è§†ä¼°è®¡æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›å·®ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹è¶³å¤Ÿå¤šæ ·æ€§å’Œæ•°é‡çš„æ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆç‰¹å®šé¢†åŸŸçš„æ•°æ®ï¼Œéš¾ä»¥é€‚åº”é‡å¤–åœºæ™¯ä¸­å¤æ‚çš„å…‰ç…§ã€å§¿æ€å’Œé®æŒ¡ç­‰æƒ…å†µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®æ¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œä»æœªæ ‡æ³¨æ•°æ®ä¸­ç”Ÿæˆå¸¦å™ªå£°çš„æ ‡ç­¾ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°å’Œç­›é€‰é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œä»è€Œè®­ç»ƒä¸€ä¸ªæ›´é²æ£’çš„å‡è§†ä¼°è®¡æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniGazeæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ— æ ‡æ³¨æ•°æ®æ”¶é›†æ¨¡å—ï¼šæ”¶é›†æ¥è‡ªä¸åŒç¯å¢ƒå’Œåœºæ™¯çš„å¤§é‡æ— æ ‡æ³¨é¢éƒ¨å›¾åƒã€‚2) ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—ï¼šä½¿ç”¨ç°æœ‰çš„å‡è§†ä¼°è®¡æ¨¡å‹ä¸ºæ— æ ‡æ³¨æ•°æ®ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚3) å¥–åŠ±æ¨¡å‹ï¼šè¯„ä¼°ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œå¹¶ä¸ºæ¯ä¸ªä¼ªæ ‡ç­¾åˆ†é…ä¸€ä¸ªç½®ä¿¡åº¦åˆ†æ•°ã€‚4) æ¨¡å‹è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨å¸¦æƒé‡çš„ä¼ªæ ‡ç­¾å’Œå°‘é‡æ ‡æ³¨æ•°æ®æ¥è®­ç»ƒå‡è§†ä¼°è®¡æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¥–åŠ±æ¨¡å‹çš„æ„å»ºæ˜¯å…³é”®åˆ›æ–°ç‚¹ã€‚å®ƒä¸ä»…ä»…ä¾èµ–äºä¼ªæ ‡ç­¾æœ¬èº«ï¼Œè¿˜ç»“åˆäº†è§†è§‰åµŒå…¥å’Œè¯­ä¹‰çº¿ç´¢æ¥è¯„ä¼°ä¼ªæ ‡ç­¾çš„å¯é æ€§ã€‚è§†è§‰åµŒå…¥æ•æ‰äº†é¢éƒ¨å›¾åƒçš„è§†è§‰ç‰¹å¾ï¼Œè€Œè¯­ä¹‰çº¿ç´¢åˆ™é€šè¿‡æç¤ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§å¤šæ¨¡æ€èåˆçš„å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°ä¼ªæ ‡ç­¾çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±æ¨¡å‹çš„è®¾è®¡ç»†èŠ‚åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æå–è§†è§‰åµŒå…¥ã€‚2) ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹ï¼‰ç”Ÿæˆå‡è§†è§†è§’çš„è¯­ä¹‰æè¿°ã€‚3) å°†è§†è§‰åµŒå…¥ã€è¯­ä¹‰æè¿°å’Œä¼ªæ ‡ç­¾ä½œä¸ºè¾“å…¥ï¼Œè®­ç»ƒä¸€ä¸ªå›å½’æ¨¡å‹æ¥é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ•°ã€‚4) ä½¿ç”¨ç½®ä¿¡åº¦åˆ†æ•°æ¥åŠ æƒæŸå¤±å‡½æ•°ï¼Œä»è€Œæ›´é‡è§†é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OmniGazeåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨è·¨é¢†åŸŸè®¾ç½®ä¸‹çš„æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼ŒOmniGazeåœ¨å››ä¸ªæœªè§æ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨é‡å¤–åœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œæ³›åŒ–æ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniGazeå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚äººæœºäº¤äº’ã€è™šæ‹Ÿç°å®/å¢å¼ºç°å®ã€é©¾é©¶å‘˜ç›‘æ§ã€å®‰å…¨ç›‘æ§ç­‰ã€‚é€šè¿‡æé«˜å‡è§†ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶å’Œé«˜æ•ˆçš„äººæœºäº¤äº’ï¼Œå¹¶ä¸ºå„ç§åº”ç”¨æä¾›æ›´å¯é çš„è§†è§‰ä¿¡æ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current 3D gaze estimation methods struggle to generalize across diverse data domains, primarily due to i) the scarcity of annotated datasets, and ii) the insufficient diversity of labeled data. In this work, we present OmniGaze, a semi-supervised framework for 3D gaze estimation, which utilizes large-scale unlabeled data collected from diverse and unconstrained real-world environments to mitigate domain bias and generalize gaze estimation in the wild. First, we build a diverse collection of unlabeled facial images, varying in facial appearances, background environments, illumination conditions, head poses, and eye occlusions. In order to leverage unlabeled data spanning a broader distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a reward model to assess the reliability of pseudo labels. Beyond pseudo labels as 3D direction vectors, the reward model also incorporates visual embeddings extracted by an off-the-shelf visual encoder and semantic cues from gaze perspective generated by prompting a Multimodal Large Language Model to compute confidence scores. Then, these scores are utilized to select high-quality pseudo labels and weight them for loss computation. Extensive experiments demonstrate that OmniGaze achieves state-of-the-art performance on five datasets under both in-domain and cross-domain settings. Furthermore, we also evaluate the efficacy of OmniGaze as a scalable data engine for gaze estimation, which exhibits robust zero-shot generalization on four unseen datasets.

