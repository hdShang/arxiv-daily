---
layout: default
title: OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild
---

# OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.13660" target="_blank" class="toolbar-btn">arXiv: 2510.13660v2</a>
    <a href="https://arxiv.org/pdf/2510.13660.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13660v2" 
            onclick="toggleFavorite(this, '2510.13660v2', 'OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hongyu Qu, Jianan Wei, Xiangbo Shu, Yazhou Yao, Wenguan Wang, Jinhui Tang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15 (Êõ¥Êñ∞: 2025-10-16)

**Â§áÊ≥®**: Accepted to NeurIPS 2025; Project page: https://github.com/quhongyu/OmniGaze

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**OmniGazeÔºöÊèêÂá∫Â•ñÂä±È©±Âä®ÁöÑÈÄöÁî®ÂáùËßÜ‰º∞ËÆ°Ê°ÜÊû∂ÔºåËß£ÂÜ≥ÈáéÂ§ñÂú∫ÊôØÊ≥õÂåñÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂáùËßÜ‰º∞ËÆ°` `ÂçäÁõëÁù£Â≠¶‰π†` `‰º™Ê†áÁ≠æ` `È¢ÜÂüüÊ≥õÂåñ` `Â•ñÂä±Ê®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ3DÂáùËßÜ‰º∞ËÆ°ÊñπÊ≥ïÂú®Ë∑®È¢ÜÂüüÊ≥õÂåñÊÄßÊñπÈù¢Â≠òÂú®‰∏çË∂≥Ôºå‰∏ªË¶ÅÂèóÈôê‰∫éÊ†áÊ≥®Êï∞ÊçÆÁ®ÄÁº∫ÂíåÂ§öÊ†∑ÊÄß‰∏çË∂≥„ÄÇ
2. OmniGazeÂà©Áî®Â§ßËßÑÊ®°Êó†Ê†áÊ≥®Êï∞ÊçÆÔºåÈÄöËøá‰º™Ê†áÁ≠æÂíåÂ•ñÂä±Ê®°ÂûãÊù•ÊèêÂçáÊ®°ÂûãÂú®ÈáéÂ§ñÂú∫ÊôØÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåOmniGazeÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂπ∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑ3DÂáùËßÜ‰º∞ËÆ°ÊñπÊ≥ïÈöæ‰ª•Âú®‰∏çÂêåÁöÑÊï∞ÊçÆÈ¢ÜÂüü‰∏≠Ê≥õÂåñÔºåËøô‰∏ªË¶ÅÊòØÁî±‰∫éÂ∏¶Ê†áÊ≥®Êï∞ÊçÆÈõÜÁöÑÁ®ÄÁº∫‰ª•ÂèäÊ†áÊ≥®Êï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄß‰∏çË∂≥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜOmniGazeÔºå‰∏Ä‰∏™ÂçäÁõëÁù£ÁöÑ3DÂáùËßÜ‰º∞ËÆ°Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî®‰ªéÂ§öÊ†∑‰∏îÊó†Á∫¶ÊùüÁöÑÁúüÂÆû‰∏ñÁïåÁéØÂ¢É‰∏≠Êî∂ÈõÜÁöÑÂ§ßËßÑÊ®°Êó†Ê†áÊ≥®Êï∞ÊçÆÊù•ÁºìËß£È¢ÜÂüüÂÅèÂ∑ÆÔºåÂπ∂Êé®ÂπøÈáéÂ§ñÂú∫ÊôØ‰∏ãÁöÑÂáùËßÜ‰º∞ËÆ°„ÄÇÈ¶ñÂÖàÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§öÊ†∑ÂåñÁöÑÊó†Ê†áÊ≥®Èù¢ÈÉ®ÂõæÂÉèÈõÜÂêàÔºåËøô‰∫õÂõæÂÉèÂú®Èù¢ÈÉ®Â§ñËßÇ„ÄÅËÉåÊôØÁéØÂ¢É„ÄÅÂÖâÁÖßÊù°‰ª∂„ÄÅÂ§¥ÈÉ®ÂßøÂäøÂíåÁúºÁùõÈÅÆÊå°ÊñπÈù¢ÂêÑ‰∏çÁõ∏Âêå„ÄÇ‰∏∫‰∫ÜÂà©Áî®Êõ¥ÂπøÊ≥õÂàÜÂ∏ÉÁöÑÊó†Ê†áÊ≥®Êï∞ÊçÆÔºåOmniGazeÈááÁî®Ê†áÂáÜÁöÑ‰º™Ê†áÁ≠æÁ≠ñÁï•ÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Â•ñÂä±Ê®°ÂûãÊù•ËØÑ‰º∞‰º™Ê†áÁ≠æÁöÑÂèØÈù†ÊÄß„ÄÇÈô§‰∫ÜÂ∞Ü‰º™Ê†áÁ≠æ‰Ωú‰∏∫3DÊñπÂêëÂêëÈáèÂ§ñÔºåÂ•ñÂä±Ê®°ÂûãËøòÁªìÂêà‰∫ÜÁî±Áé∞ÊàêÁöÑËßÜËßâÁºñÁ†ÅÂô®ÊèêÂèñÁöÑËßÜËßâÂµåÂÖ•‰ª•ÂèäÈÄöËøáÊèêÁ§∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁöÑÂáùËßÜËßÜËßíÁöÑËØ≠‰πâÁ∫øÁ¥¢Ôºå‰ª•ËÆ°ÁÆóÁΩÆ‰ø°Â∫¶ÂàÜÊï∞„ÄÇÁÑ∂ÂêéÔºåËøô‰∫õÂàÜÊï∞Áî®‰∫éÈÄâÊã©È´òË¥®ÈáèÁöÑ‰º™Ê†áÁ≠æÂπ∂ÂØπÂÖ∂ËøõË°åÂä†ÊùÉ‰ª•ËøõË°åÊçüÂ§±ËÆ°ÁÆó„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåOmniGazeÂú®‰∫î‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂåÖÊã¨ÂêåÈ¢ÜÂüüÂíåË∑®È¢ÜÂüüËÆæÁΩÆ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòËØÑ‰º∞‰∫ÜOmniGaze‰Ωú‰∏∫ÂáùËßÜ‰º∞ËÆ°ÁöÑÂèØÊâ©Â±ïÊï∞ÊçÆÂºïÊìéÁöÑÊúâÊïàÊÄßÔºåÂÆÉÂú®Âõõ‰∏™Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ3DÂáùËßÜ‰º∞ËÆ°ÊñπÊ≥ïÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠Ê≥õÂåñËÉΩÂäõÂ∑ÆÔºå‰∏ªË¶ÅÂéüÂõ†ÊòØÁº∫‰πèË∂≥Â§üÂ§öÊ†∑ÊÄßÂíåÊï∞ÈáèÁöÑÊ†áÊ≥®Êï∞ÊçÆÔºåÂØºËá¥Ê®°ÂûãÂÆπÊòìËøáÊãüÂêàÁâπÂÆöÈ¢ÜÂüüÁöÑÊï∞ÊçÆÔºåÈöæ‰ª•ÈÄÇÂ∫îÈáéÂ§ñÂú∫ÊôØ‰∏≠Â§çÊùÇÁöÑÂÖâÁÖß„ÄÅÂßøÊÄÅÂíåÈÅÆÊå°Á≠âÊÉÖÂÜµ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂà©Áî®Â§ßËßÑÊ®°Êó†Ê†áÊ≥®Êï∞ÊçÆÊù•ÊèêÂçáÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ†∏ÂøÉÊÄùÊÉ≥ÊòØ‰ΩøÁî®‰º™Ê†áÁ≠æÊñπÊ≥ïÔºå‰ªéÊú™Ê†áÊ≥®Êï∞ÊçÆ‰∏≠ÁîüÊàêÂ∏¶Âô™Â£∞ÁöÑÊ†áÁ≠æÔºåÂπ∂ËÆæËÆ°‰∏Ä‰∏™Â•ñÂä±Ê®°ÂûãÊù•ËØÑ‰º∞ÂíåÁ≠õÈÄâÈ´òË¥®ÈáèÁöÑ‰º™Ê†áÁ≠æÔºå‰ªéËÄåËÆ≠ÁªÉ‰∏Ä‰∏™Êõ¥È≤ÅÊ£íÁöÑÂáùËßÜ‰º∞ËÆ°Ê®°Âûã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöOmniGazeÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) Êó†Ê†áÊ≥®Êï∞ÊçÆÊî∂ÈõÜÊ®°ÂùóÔºöÊî∂ÈõÜÊù•Ëá™‰∏çÂêåÁéØÂ¢ÉÂíåÂú∫ÊôØÁöÑÂ§ßÈáèÊó†Ê†áÊ≥®Èù¢ÈÉ®ÂõæÂÉè„ÄÇ2) ‰º™Ê†áÁ≠æÁîüÊàêÊ®°ÂùóÔºö‰ΩøÁî®Áé∞ÊúâÁöÑÂáùËßÜ‰º∞ËÆ°Ê®°Âûã‰∏∫Êó†Ê†áÊ≥®Êï∞ÊçÆÁîüÊàê‰º™Ê†áÁ≠æ„ÄÇ3) Â•ñÂä±Ê®°ÂûãÔºöËØÑ‰º∞‰º™Ê†áÁ≠æÁöÑË¥®ÈáèÔºåÂπ∂‰∏∫ÊØè‰∏™‰º™Ê†áÁ≠æÂàÜÈÖç‰∏Ä‰∏™ÁΩÆ‰ø°Â∫¶ÂàÜÊï∞„ÄÇ4) Ê®°ÂûãËÆ≠ÁªÉÊ®°ÂùóÔºö‰ΩøÁî®Â∏¶ÊùÉÈáçÁöÑ‰º™Ê†áÁ≠æÂíåÂ∞ëÈáèÊ†áÊ≥®Êï∞ÊçÆÊù•ËÆ≠ÁªÉÂáùËßÜ‰º∞ËÆ°Ê®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂ•ñÂä±Ê®°ÂûãÁöÑÊûÑÂª∫ÊòØÂÖ≥ÈîÆÂàõÊñ∞ÁÇπ„ÄÇÂÆÉ‰∏ç‰ªÖ‰ªÖ‰æùËµñ‰∫é‰º™Ê†áÁ≠æÊú¨Ë∫´ÔºåËøòÁªìÂêà‰∫ÜËßÜËßâÂµåÂÖ•ÂíåËØ≠‰πâÁ∫øÁ¥¢Êù•ËØÑ‰º∞‰º™Ê†áÁ≠æÁöÑÂèØÈù†ÊÄß„ÄÇËßÜËßâÂµåÂÖ•ÊçïÊçâ‰∫ÜÈù¢ÈÉ®ÂõæÂÉèÁöÑËßÜËßâÁâπÂæÅÔºåËÄåËØ≠‰πâÁ∫øÁ¥¢ÂàôÈÄöËøáÊèêÁ§∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊù•ÁîüÊàêÔºå‰ªéËÄåÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇËøôÁßçÂ§öÊ®°ÊÄÅËûçÂêàÁöÑÂ•ñÂä±Ê®°ÂûãËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞‰º™Ê†áÁ≠æÁöÑË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ•ñÂä±Ê®°ÂûãÁöÑËÆæËÆ°ÁªÜËäÇÂåÖÊã¨Ôºö1) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÁºñÁ†ÅÂô®ÊèêÂèñËßÜËßâÂµåÂÖ•„ÄÇ2) ‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºà‰æãÂ¶ÇÔºåÈÄöËøáÊèêÁ§∫Â∑•Á®ãÔºâÁîüÊàêÂáùËßÜËßÜËßíÁöÑËØ≠‰πâÊèèËø∞„ÄÇ3) Â∞ÜËßÜËßâÂµåÂÖ•„ÄÅËØ≠‰πâÊèèËø∞Âíå‰º™Ê†áÁ≠æ‰Ωú‰∏∫ËæìÂÖ•ÔºåËÆ≠ÁªÉ‰∏Ä‰∏™ÂõûÂΩíÊ®°ÂûãÊù•È¢ÑÊµãÁΩÆ‰ø°Â∫¶ÂàÜÊï∞„ÄÇ4) ‰ΩøÁî®ÁΩÆ‰ø°Â∫¶ÂàÜÊï∞Êù•Âä†ÊùÉÊçüÂ§±ÂáΩÊï∞Ôºå‰ªéËÄåÊõ¥ÈáçËßÜÈ´òË¥®ÈáèÁöÑ‰º™Ê†áÁ≠æ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

OmniGazeÂú®‰∫î‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩÔºåÂåÖÊã¨Âú®Ë∑®È¢ÜÂüüËÆæÁΩÆ‰∏ãÁöÑÊòæËëóÊèêÂçá„ÄÇÊ≠§Â§ñÔºåOmniGazeÂú®Âõõ‰∏™Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏äÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÈáéÂ§ñÂú∫ÊôØ‰∏ãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñÊÄß„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Áü•Ôºå‰ΩÜËÆ∫ÊñáÂº∫Ë∞É‰∫ÜÂÖ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

OmniGazeÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶Ç‰∫∫Êú∫‰∫§‰∫í„ÄÅËôöÊãüÁé∞ÂÆû/Â¢ûÂº∫Áé∞ÂÆû„ÄÅÈ©æÈ©∂ÂëòÁõëÊéß„ÄÅÂÆâÂÖ®ÁõëÊéßÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÂáùËßÜ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Ëá™ÁÑ∂ÂíåÈ´òÊïàÁöÑ‰∫∫Êú∫‰∫§‰∫íÔºåÂπ∂‰∏∫ÂêÑÁßçÂ∫îÁî®Êèê‰æõÊõ¥ÂèØÈù†ÁöÑËßÜËßâ‰ø°ÊÅØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Current 3D gaze estimation methods struggle to generalize across diverse data domains, primarily due to i) the scarcity of annotated datasets, and ii) the insufficient diversity of labeled data. In this work, we present OmniGaze, a semi-supervised framework for 3D gaze estimation, which utilizes large-scale unlabeled data collected from diverse and unconstrained real-world environments to mitigate domain bias and generalize gaze estimation in the wild. First, we build a diverse collection of unlabeled facial images, varying in facial appearances, background environments, illumination conditions, head poses, and eye occlusions. In order to leverage unlabeled data spanning a broader distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a reward model to assess the reliability of pseudo labels. Beyond pseudo labels as 3D direction vectors, the reward model also incorporates visual embeddings extracted by an off-the-shelf visual encoder and semantic cues from gaze perspective generated by prompting a Multimodal Large Language Model to compute confidence scores. Then, these scores are utilized to select high-quality pseudo labels and weight them for loss computation. Extensive experiments demonstrate that OmniGaze achieves state-of-the-art performance on five datasets under both in-domain and cross-domain settings. Furthermore, we also evaluate the efficacy of OmniGaze as a scalable data engine for gaze estimation, which exhibits robust zero-shot generalization on four unseen datasets.

