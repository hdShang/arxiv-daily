---
layout: default
title: OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment
---

# OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13131" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13131v1</a>
  <a href="https://arxiv.org/pdf/2510.13131.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13131v1" onclick="toggleFavorite(this, '2510.13131v1', 'OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Rongjun Chen, Chengsi Yao, Jinchang Ren, Xianxian Zeng, Peixian Wang, Jun Yuan, Jiawen Li, Huimin Zhao, Xu Lu

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOS-HGAdapterï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¢å¼ºå›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œæ˜¾è‘—æå‡è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å›¾åƒ-æ–‡æœ¬å¯¹é½` `è·¨æ¨¡æ€æ£€ç´¢` `å¤§è¯­è¨€æ¨¡å‹` `ä¿¡æ¯ç†µ` `è¶…å›¾ç¥ç»ç½‘ç»œ` `å¤šæ¨¡æ€èåˆ` `è¯­ä¹‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´ä¿¡æ¯ç†µçš„å·®å¼‚ï¼Œå¯¼è‡´è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½å—é™ã€‚
2. åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¢å¼ºæ–‡æœ¬æ¨¡æ€çš„è¯­ä¹‰ä¸°å¯Œåº¦ï¼Œå¹¶ä½¿ç”¨è¶…å›¾é€‚é…å™¨æ„å»ºå¤šè¾¹è¿æ¥ï¼Œçº æ­£åŒ¹é…é”™è¯¯å¹¶é™ä½å™ªå£°ã€‚
3. åœ¨Flickr30Kå’ŒMS-COCOæ•°æ®é›†ä¸Šï¼ŒOS-HGAdapteråœ¨è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¾¾åˆ°æ–°çš„SOTAã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹å¤šåª’ä½“å†…å®¹ç†è§£ä¸­å›¾åƒ-æ–‡æœ¬å¯¹é½è¿™ä¸€åŸºç¡€æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¼€æ”¾è¯­ä¹‰è¶…å›¾é€‚é…å™¨ï¼ˆOS-HGAdapterï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–è”åˆåµŒå…¥ç©ºé—´æ¥æå‡æ£€ç´¢ç³»ç»Ÿæ€§èƒ½ã€‚è€ƒè™‘åˆ°æ–‡æœ¬å’Œå›¾åƒä¹‹é—´ä¿¡æ¯ç†µçš„å·®å¼‚ï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨è·¨æ¨¡æ€æ£€ç´¢ä¸­å­˜åœ¨ä¸å¹³è¡¡é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€æ”¾è¯­ä¹‰çŸ¥è¯†æ¥å¼¥è¡¥ç†µçš„å·®è·ï¼Œä»è€Œæ¨¡æ‹Ÿäººç±»åœ¨æ­¤ä»»åŠ¡ä¸­çš„å¯¹é½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªæ­¥éª¤å®ç°ç†µå¢å¼ºå¯¹é½ï¼šé¦–å…ˆï¼Œè®¾è®¡äº†ä¸€ç§ä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„promptæ¨¡æ¿ï¼Œåˆ©ç”¨LLMå¢å¼ºæ–‡æœ¬æ¨¡æ€çš„å¤šä¹‰æ€§æè¿°ï¼Œä»è€Œå¢åŠ æ–‡æœ¬æ¨¡æ€ç›¸å¯¹äºè§†è§‰æ¨¡æ€çš„ä¿¡æ¯ç†µï¼›å…¶æ¬¡ï¼Œä½¿ç”¨è¶…å›¾é€‚é…å™¨æ„å»ºæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¹‹é—´çš„å¤šè¾¹è¿æ¥ï¼Œçº æ­£åŒä¹‰è¯­ä¹‰çš„åŒ¹é…é”™è¯¯ï¼Œå¹¶é€šè¿‡é™ç»´æ˜ å°„å‡å°‘å¼€æ”¾è¯­ä¹‰ç†µå¸¦æ¥çš„å™ªå£°ã€‚åœ¨Flickr30Kå’ŒMS-COCOåŸºå‡†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOS-HGAdapterä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨è·¨æ¨¡æ€æ£€ç´¢æ–¹é¢åˆ†åˆ«å®ç°äº†16.8%ï¼ˆæ–‡æœ¬åˆ°å›¾åƒï¼‰å’Œ40.1%ï¼ˆå›¾åƒåˆ°æ–‡æœ¬ï¼‰çš„å¢ç›Šï¼Œå¹¶åœ¨è¯­ä¹‰å¯¹é½ä»»åŠ¡ä¸­å»ºç«‹äº†æ–°çš„state-of-the-artæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå›¾åƒ-æ–‡æœ¬å¯¹é½æ—¨åœ¨å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œæ˜¯å¤šæ¨¡æ€ç†è§£çš„å…³é”®ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ç†µå·®å¼‚æ—¶å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ä¸ä½³ã€‚æ–‡æœ¬é€šå¸¸è¾ƒä¸ºç®€æ´ï¼Œä¿¡æ¯ç†µè¾ƒä½ï¼Œè€Œå›¾åƒåŒ…å«æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä¿¡æ¯ç†µè¾ƒé«˜ã€‚è¿™ç§å·®å¼‚ä½¿å¾—æ¨¡å‹éš¾ä»¥å­¦ä¹ åˆ°æœ‰æ•ˆçš„å¯¹é½å…³ç³»ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒåˆ°æ–‡æœ¬çš„æ£€ç´¢ä¸­è¡¨ç°æ›´å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€æ”¾è¯­ä¹‰çŸ¥è¯†æ¥å¢å¼ºæ–‡æœ¬æ¨¡æ€çš„ä¿¡æ¯ç†µï¼Œä½¿å…¶ä¸å›¾åƒæ¨¡æ€çš„ä¿¡æ¯ç†µæ›´åŠ åŒ¹é…ã€‚é€šè¿‡å¢åŠ æ–‡æœ¬çš„å¤šä¹‰æ€§æè¿°ï¼Œå¼¥è¡¥æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„ä¿¡æ¯ç†µå·®è·ï¼Œä»è€Œæé«˜æ¨¡å‹å­¦ä¹ å¯¹é½å…³ç³»çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œä½¿ç”¨è¶…å›¾é€‚é…å™¨æ¥å»ºæ¨¡æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œçº æ­£åŒ¹é…é”™è¯¯ï¼Œå¹¶é™ä½å™ªå£°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOS-HGAdapteråŒ…å«ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š1) **LLMæ–‡æœ¬å¢å¼º**ï¼šè®¾è®¡promptæ¨¡æ¿ï¼Œåˆ©ç”¨LLMç”Ÿæˆæ›´ä¸°å¯Œçš„æ–‡æœ¬æè¿°ï¼Œå¢åŠ æ–‡æœ¬æ¨¡æ€çš„ä¿¡æ¯ç†µã€‚2) **è¶…å›¾é€‚é…å™¨**ï¼šæ„å»ºæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¹‹é—´çš„è¶…å›¾ç»“æ„ï¼Œé€šè¿‡å¤šè¾¹è¿æ¥å»ºæ¨¡è¯­ä¹‰å…³ç³»ï¼Œå¹¶ä½¿ç”¨é™ç»´æ˜ å°„å‡å°‘å™ªå£°ã€‚æ•´ä½“æµç¨‹æ˜¯å…ˆä½¿ç”¨LLMå¢å¼ºæ–‡æœ¬ï¼Œç„¶åå°†å¢å¼ºåçš„æ–‡æœ¬å’Œå›¾åƒè¾“å…¥åˆ°è¶…å›¾é€‚é…å™¨ä¸­è¿›è¡Œå¯¹é½å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **åˆ©ç”¨LLMè¿›è¡Œç†µå¢å¼º**ï¼šé¦–æ¬¡å°†LLMåº”ç”¨äºå›¾åƒ-æ–‡æœ¬å¯¹é½ä»»åŠ¡ä¸­ï¼Œé€šè¿‡promptå·¥ç¨‹å¢å¼ºæ–‡æœ¬æ¨¡æ€çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¼¥è¡¥äº†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„ä¿¡æ¯ç†µå·®è·ã€‚2) **è¶…å›¾é€‚é…å™¨**ï¼šä½¿ç”¨è¶…å›¾ç»“æ„å»ºæ¨¡æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œèƒ½å¤Ÿæœ‰æ•ˆçº æ­£åŒ¹é…é”™è¯¯ï¼Œå¹¶é™ä½å™ªå£°ã€‚

**å…³é”®è®¾è®¡**ï¼š1) **Promptæ¨¡æ¿è®¾è®¡**ï¼šè®¾è®¡ä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„promptæ¨¡æ¿ï¼Œç¡®ä¿LLMèƒ½å¤Ÿç”Ÿæˆé€šç”¨çš„è¯­ä¹‰å¢å¼ºæè¿°ã€‚2) **è¶…å›¾æ„å»º**ï¼šåŸºäºæ–‡æœ¬å’Œå›¾åƒçš„ç‰¹å¾å‘é‡æ„å»ºè¶…å›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºæ–‡æœ¬æˆ–å›¾åƒï¼Œè¶…è¾¹è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚3) **é™ç»´æ˜ å°„**ï¼šé€šè¿‡é™ç»´æ˜ å°„å‡å°‘å¼€æ”¾è¯­ä¹‰ç†µå¸¦æ¥çš„å™ªå£°ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚4) **æŸå¤±å‡½æ•°**ï¼šä½¿ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ–‡æœ¬å’Œå›¾åƒçš„åµŒå…¥ç©ºé—´ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬å’Œå›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOS-HGAdapteråœ¨Flickr30Kå’ŒMS-COCOæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨Flickr30Kæ•°æ®é›†ä¸Šï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ£€ç´¢æ€§èƒ½æå‡äº†16.8%ï¼Œå›¾åƒåˆ°æ–‡æœ¬çš„æ£€ç´¢æ€§èƒ½æå‡äº†40.1%ã€‚åœ¨MS-COCOæ•°æ®é›†ä¸Šï¼Œä¹Ÿå–å¾—äº†ç±»ä¼¼çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒOS-HGAdapteråœ¨è¯­ä¹‰å¯¹é½ä»»åŠ¡ä¸­å»ºç«‹äº†æ–°çš„state-of-the-artæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè·¨æ¨¡æ€æ£€ç´¢ã€å›¾åƒæè¿°ç”Ÿæˆã€è§†è§‰é—®ç­”ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ç”µå•†é¢†åŸŸï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æè¿°å¿«é€Ÿæ£€ç´¢ç›¸å…³çš„å•†å“å›¾åƒï¼›åœ¨æ™ºèƒ½å®¢æœé¢†åŸŸï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·ä¸Šä¼ çš„å›¾åƒç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶ç»™å‡ºç›¸åº”çš„æ–‡æœ¬å›å¤ã€‚è¯¥æ–¹æ³•å…·æœ‰æå‡å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†æ•ˆç‡å’Œå‡†ç¡®æ€§çš„æ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Text-image alignment constitutes a foundational challenge in multimedia content understanding, where effective modeling of cross-modal semantic correspondences critically enhances retrieval system performance through joint embedding space optimization. Given the inherent difference in information entropy between texts and images, conventional approaches often show an imbalance in the mutual retrieval of these two modalities. To address this particular challenge, we propose to use the open semantic knowledge of Large Language Model (LLM) to fill for the entropy gap and reproduce the alignment ability of humans in these tasks. Our entropy-enhancing alignment is achieved through a two-step process: 1) a new prompt template that does not rely on explicit knowledge in the task domain is designed to use LLM to enhance the polysemy description of the text modality. By analogy, the information entropy of the text modality relative to the visual modality is increased; 2) A hypergraph adapter is used to construct multilateral connections between the text and image modalities, which can correct the positive and negative matching errors for synonymous semantics in the same fixed embedding space, whilst reducing the noise caused by open semantic entropy by mapping the reduced dimensions back to the original dimensions. Comprehensive evaluations on the Flickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic Hypergraph Adapter (OS-HGAdapter), showcasing 16.8\% (text-to-image) and 40.1\% (image-to-text) cross-modal retrieval gains over existing methods while establishing new state-of-the-art performance in semantic alignment tasks.

