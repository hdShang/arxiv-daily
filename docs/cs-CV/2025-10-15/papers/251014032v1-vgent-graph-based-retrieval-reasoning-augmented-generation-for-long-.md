---
layout: default
title: Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding
---

# Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14032" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14032v1</a>
  <a href="https://arxiv.org/pdf/2510.14032.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14032v1" onclick="toggleFavorite(this, '2510.14032v1', 'Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**å¤‡æ³¨**: NeurIPS 2025 (Spotlight). Webpage at https://xiaoqian-shen.github.io/Vgent

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://xiaoqian-shen.github.io/Vgent)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVgentï¼Œé€šè¿‡å›¾ç»“æ„æ£€ç´¢-æ¨ç†å¢å¼ºç”Ÿæˆï¼Œæå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `å›¾ç»“æ„` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `è§†é¢‘æ¨ç†` `è§†é¢‘é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LVLMséš¾ä»¥å¤„ç†é•¿è§†é¢‘ä¸­å¤§é‡çš„tokensï¼Œå¹¶ä¸”éš¾ä»¥ä¿æŒé•¿æœŸçš„æ—¶åºä¿¡æ¯ï¼Œå¯¼è‡´ç†è§£å’Œæ¨ç†èƒ½åŠ›å—é™ã€‚
2. Vgenté€šè¿‡æ„å»ºè§†é¢‘ç‰‡æ®µçš„å›¾ç»“æ„ï¼Œå¹¶å¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¢å¼ºæ£€ç´¢çš„æœ‰æ•ˆæ€§å’Œæ¨ç†çš„å‡†ç¡®æ€§ï¼Œä»è€Œæå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVgentåœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨MLVUä¸Šå–å¾—äº†3.0%~5.4%çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œå› ä¸Šä¸‹æ–‡çª—å£é™åˆ¶å’Œé•¿æœŸåºåˆ—ä¿¡æ¯ä¿æŒå›°éš¾è€Œé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾ç»“æ„çš„æ£€ç´¢-æ¨ç†å¢å¼ºç”Ÿæˆæ¡†æ¶Vgentï¼Œä»¥æå‡LVLMsçš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼š(i) ä½¿ç”¨ç»“æ„åŒ–å›¾è¡¨ç¤ºè§†é¢‘ï¼Œä¿ç•™è§†é¢‘ç‰‡æ®µä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»è€Œæé«˜æ£€ç´¢æ•ˆç‡ã€‚(ii) å¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œç¼“è§£LVLMsçš„æ¨ç†é™åˆ¶ï¼Œåˆ©ç”¨ç»“æ„åŒ–éªŒè¯å‡å°‘æ£€ç´¢å™ªå£°ï¼Œå¹¶ä¿ƒè¿›è·¨ç‰‡æ®µç›¸å…³ä¿¡æ¯çš„æ˜¾å¼èšåˆï¼Œä»è€Œäº§ç”Ÿæ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”ã€‚åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹å„ç§å¼€æºLVLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MLVUä¸Šç›¸å¯¹äºåŸºçº¿æ¨¡å‹å®ç°äº†3.0%~5.4%çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œå¹¶ä¸”ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘RAGæ–¹æ³•8.6%ã€‚ä»£ç å·²å…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé•¿è§†é¢‘ç†è§£ä»»åŠ¡å¯¹LVLMsæå‡ºäº†æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥å¤„ç†è¶…å‡ºä¸Šä¸‹æ–‡çª—å£é™åˆ¶çš„å¤§é‡è§†é¢‘tokensï¼Œå¹¶ä¸”éš¾ä»¥ä¿æŒé•¿æœŸçš„æ—¶åºä¿¡æ¯ã€‚ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œå¯èƒ½ä¼šç ´åæ—¶é—´ä¾èµ–æ€§ï¼Œå¹¶åŒ…å«ä¸ç›¸å…³çš„ä¿¡æ¯ï¼Œä»è€Œé˜»ç¢å‡†ç¡®çš„æ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVgentçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å›¾ç»“æ„æ¥è¡¨ç¤ºè§†é¢‘ï¼Œä»è€Œä¿ç•™è§†é¢‘ç‰‡æ®µä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œæé«˜æ£€ç´¢æ•ˆç‡ã€‚æ­¤å¤–ï¼Œå¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œåˆ©ç”¨ç»“æ„åŒ–éªŒè¯æ¥å‡å°‘æ£€ç´¢å™ªå£°ï¼Œå¹¶ä¿ƒè¿›è·¨ç‰‡æ®µç›¸å…³ä¿¡æ¯çš„æ˜¾å¼èšåˆï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVgentæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘ç‰‡æ®µæå–å’Œç‰¹å¾ç¼–ç ï¼›2) åŸºäºè¯­ä¹‰å…³ç³»çš„å›¾æ„å»ºï¼ŒèŠ‚ç‚¹ä»£è¡¨è§†é¢‘ç‰‡æ®µï¼Œè¾¹ä»£è¡¨ç‰‡æ®µé—´çš„å…³ç³»ï¼›3) åŸºäºå›¾ç»“æ„çš„æ£€ç´¢ï¼Œæ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³ç‰‡æ®µï¼›4) ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¯¹æ£€ç´¢ç»“æœè¿›è¡Œç»“æ„åŒ–éªŒè¯å’Œä¿¡æ¯èšåˆï¼›5) ç”Ÿæˆæ¨¡å—ï¼Œæ ¹æ®æ¨ç†ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šVgentçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) ä½¿ç”¨å›¾ç»“æ„æ¥è¡¨ç¤ºè§†é¢‘ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™è§†é¢‘ç‰‡æ®µä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œæé«˜æ£€ç´¢æ•ˆç‡ï¼›2) å¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œåˆ©ç”¨ç»“æ„åŒ–éªŒè¯æ¥å‡å°‘æ£€ç´¢å™ªå£°ï¼Œå¹¶ä¿ƒè¿›è·¨ç‰‡æ®µç›¸å…³ä¿¡æ¯çš„æ˜¾å¼èšåˆï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒVgentæ›´åŠ æ³¨é‡è§†é¢‘ç‰‡æ®µä¹‹é—´çš„å…³ç³»å’Œä¿¡æ¯çš„èšåˆã€‚

**å…³é”®è®¾è®¡**ï¼šå›¾ç»“æ„çš„æ„å»ºæ–¹å¼æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ï¼Œä¾‹å¦‚å¦‚ä½•å®šä¹‰èŠ‚ç‚¹å’Œè¾¹ï¼Œå¦‚ä½•è®¡ç®—è¾¹çš„æƒé‡ç­‰ã€‚ä¸­é—´æ¨ç†æ­¥éª¤çš„å…·ä½“å®ç°æ–¹å¼ï¼Œä¾‹å¦‚ä½¿ç”¨ä»€ä¹ˆæ ·çš„ç»“æ„åŒ–éªŒè¯æ–¹æ³•ï¼Œå¦‚ä½•è¿›è¡Œä¿¡æ¯èšåˆç­‰ï¼Œä¹Ÿæ˜¯å…³é”®è®¾è®¡ã€‚è®ºæ–‡ä¸­å¯èƒ½è¿˜æ¶‰åŠä¸€äº›è¶…å‚æ•°çš„è®¾ç½®ï¼Œä¾‹å¦‚å›¾çš„èŠ‚ç‚¹æ•°é‡ã€è¾¹çš„æƒé‡é˜ˆå€¼ç­‰ï¼Œè¿™äº›å‚æ•°çš„è®¾ç½®ä¹Ÿä¼šå½±å“æœ€ç»ˆçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Vgentåœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼ŒVgentåœ¨MLVUä¸Šç›¸å¯¹äºåŸºçº¿æ¨¡å‹å®ç°äº†3.0%~5.4%çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œå¹¶ä¸”ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘RAGæ–¹æ³•8.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVgentèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Vgentå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€è§†é¢‘æœç´¢ã€è§†é¢‘æ‘˜è¦ã€è§†é¢‘é—®ç­”ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨Vgentå¯¹ç›‘æ§è§†é¢‘è¿›è¡Œåˆ†æï¼Œå¿«é€Ÿå®šä½å¼‚å¸¸äº‹ä»¶ã€‚åœ¨è§†é¢‘æœç´¢ä¸­ï¼Œå¯ä»¥åˆ©ç”¨Vgentæé«˜æœç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚åœ¨è§†é¢‘æ‘˜è¦ä¸­ï¼Œå¯ä»¥åˆ©ç”¨Vgentæå–è§†é¢‘çš„å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´çš„æ‘˜è¦ã€‚åœ¨è§†é¢‘é—®ç­”ä¸­ï¼Œå¯ä»¥åˆ©ç”¨Vgentå›ç­”ç”¨æˆ·æå‡ºçš„å…³äºè§†é¢‘å†…å®¹çš„é—®é¢˜ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Understanding and reasoning over long videos pose significant challenges for large video language models (LVLMs) due to the difficulty in processing intensive video tokens beyond context window and retaining long-term sequential information. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in processing long context for Large Language Models (LLMs); however, applying RAG to long video faces challenges such as disrupted temporal dependencies and inclusion of irrelevant information that can hinder accurate reasoning. To address these limitations, we propose Vgent, a novel graph-based retrieval-reasoning-augmented generation framework to enhance LVLMs for long video understanding. Our approach introduces two key innovations: (i) It represents videos by structured graphs with semantic relationships across video clips preserved to improve retrieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate the reasoning limitation of LVLMs, which leverages structured verification to reduce retrieval noise and facilitate the explicit aggregation of relevant information across clips, resulting in more accurate and context-aware responses. We comprehensively evaluate our framework with various open-source LVLMs on three long-video understanding benchmarks. Our approach yielded an overall performance improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available at https://xiaoqian-shen.github.io/Vgent.

