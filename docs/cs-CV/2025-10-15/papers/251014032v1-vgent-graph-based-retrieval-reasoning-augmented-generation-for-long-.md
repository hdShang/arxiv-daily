---
layout: default
title: Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding
---

# Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14032" target="_blank" class="toolbar-btn">arXiv: 2510.14032v1</a>
    <a href="https://arxiv.org/pdf/2510.14032.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14032v1" 
            onclick="toggleFavorite(this, '2510.14032v1', 'Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15

**Â§áÊ≥®**: NeurIPS 2025 (Spotlight). Webpage at https://xiaoqian-shen.github.io/Vgent

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://xiaoqian-shen.github.io/Vgent)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VgentÔºåÈÄöËøáÂõæÁªìÊûÑÊ£ÄÁ¥¢-Êé®ÁêÜÂ¢ûÂº∫ÁîüÊàêÔºåÊèêÂçáÈïøËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøËßÜÈ¢ëÁêÜËß£` `ÂõæÁªìÊûÑ` `Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê` `ËßÜÈ¢ëÊé®ÁêÜ` `ËßÜÈ¢ëÈóÆÁ≠î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLVLMsÈöæ‰ª•Â§ÑÁêÜÈïøËßÜÈ¢ë‰∏≠Â§ßÈáèÁöÑtokensÔºåÂπ∂‰∏îÈöæ‰ª•‰øùÊåÅÈïøÊúüÁöÑÊó∂Â∫è‰ø°ÊÅØÔºåÂØºËá¥ÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÂèóÈôê„ÄÇ
2. VgentÈÄöËøáÊûÑÂª∫ËßÜÈ¢ëÁâáÊÆµÁöÑÂõæÁªìÊûÑÔºåÂπ∂ÂºïÂÖ•‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºåÂ¢ûÂº∫Ê£ÄÁ¥¢ÁöÑÊúâÊïàÊÄßÂíåÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÔºå‰ªéËÄåÊèêÂçáÈïøËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVgentÂú®ÈïøËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®MLVU‰∏äÂèñÂæó‰∫Ü3.0%~5.4%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈíàÂØπÂ§ßÂûãËßÜÈ¢ëËØ≠Ë®ÄÊ®°Âûã(LVLMs)Âú®Â§ÑÁêÜÈïøËßÜÈ¢ëÊó∂ÔºåÂõ†‰∏ä‰∏ãÊñáÁ™óÂè£ÈôêÂà∂ÂíåÈïøÊúüÂ∫èÂàó‰ø°ÊÅØ‰øùÊåÅÂõ∞ÈöæËÄåÈù¢‰∏¥ÁöÑÊåëÊàòÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂõæÁªìÊûÑÁöÑÊ£ÄÁ¥¢-Êé®ÁêÜÂ¢ûÂº∫ÁîüÊàêÊ°ÜÊû∂VgentÔºå‰ª•ÊèêÂçáLVLMsÁöÑÈïøËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫Ü‰∏§È°πÂÖ≥ÈîÆÂàõÊñ∞Ôºö(i) ‰ΩøÁî®ÁªìÊûÑÂåñÂõæË°®Á§∫ËßÜÈ¢ëÔºå‰øùÁïôËßÜÈ¢ëÁâáÊÆµ‰πãÈó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ªÔºå‰ªéËÄåÊèêÈ´òÊ£ÄÁ¥¢ÊïàÁéá„ÄÇ(ii) ÂºïÂÖ•‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºåÁºìËß£LVLMsÁöÑÊé®ÁêÜÈôêÂà∂ÔºåÂà©Áî®ÁªìÊûÑÂåñÈ™åËØÅÂáèÂ∞ëÊ£ÄÁ¥¢Âô™Â£∞ÔºåÂπ∂‰øÉËøõË∑®ÁâáÊÆµÁõ∏ÂÖ≥‰ø°ÊÅØÁöÑÊòæÂºèËÅöÂêàÔºå‰ªéËÄå‰∫ßÁîüÊõ¥ÂáÜÁ°ÆÂíå‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÂìçÂ∫î„ÄÇÂú®‰∏â‰∏™ÈïøËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÂØπÂêÑÁßçÂºÄÊ∫êLVLMsËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞„ÄÇÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®MLVU‰∏äÁõ∏ÂØπ‰∫éÂü∫Á∫øÊ®°ÂûãÂÆûÁé∞‰∫Ü3.0%~5.4%ÁöÑÊï¥‰ΩìÊÄßËÉΩÊèêÂçáÔºåÂπ∂‰∏î‰ºò‰∫éÊúÄÂÖàËøõÁöÑËßÜÈ¢ëRAGÊñπÊ≥ï8.6%„ÄÇ‰ª£Á†ÅÂ∑≤ÂÖ¨ÂºÄ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°ÂØπLVLMsÊèêÂá∫‰∫ÜÊåëÊàòÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Èöæ‰ª•Â§ÑÁêÜË∂ÖÂá∫‰∏ä‰∏ãÊñáÁ™óÂè£ÈôêÂà∂ÁöÑÂ§ßÈáèËßÜÈ¢ëtokensÔºåÂπ∂‰∏îÈöæ‰ª•‰øùÊåÅÈïøÊúüÁöÑÊó∂Â∫è‰ø°ÊÅØ„ÄÇÁé∞ÊúâÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê(RAG)ÊñπÊ≥ïÂú®Â§ÑÁêÜÈïøËßÜÈ¢ëÊó∂ÔºåÂèØËÉΩ‰ºöÁ†¥ÂùèÊó∂Èó¥‰æùËµñÊÄßÔºåÂπ∂ÂåÖÂê´‰∏çÁõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÈòªÁ¢çÂáÜÁ°ÆÁöÑÊé®ÁêÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVgentÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂõæÁªìÊûÑÊù•Ë°®Á§∫ËßÜÈ¢ëÔºå‰ªéËÄå‰øùÁïôËßÜÈ¢ëÁâáÊÆµ‰πãÈó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ªÔºåÊèêÈ´òÊ£ÄÁ¥¢ÊïàÁéá„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºåÂà©Áî®ÁªìÊûÑÂåñÈ™åËØÅÊù•ÂáèÂ∞ëÊ£ÄÁ¥¢Âô™Â£∞ÔºåÂπ∂‰øÉËøõË∑®ÁâáÊÆµÁõ∏ÂÖ≥‰ø°ÊÅØÁöÑÊòæÂºèËÅöÂêàÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVgentÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËßÜÈ¢ëÁâáÊÆµÊèêÂèñÂíåÁâπÂæÅÁºñÁ†ÅÔºõ2) Âü∫‰∫éËØ≠‰πâÂÖ≥Á≥ªÁöÑÂõæÊûÑÂª∫ÔºåËäÇÁÇπ‰ª£Ë°®ËßÜÈ¢ëÁâáÊÆµÔºåËæπ‰ª£Ë°®ÁâáÊÆµÈó¥ÁöÑÂÖ≥Á≥ªÔºõ3) Âü∫‰∫éÂõæÁªìÊûÑÁöÑÊ£ÄÁ¥¢ÔºåÊ†πÊçÆÊü•ËØ¢Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÁâáÊÆµÔºõ4) ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºåÂØπÊ£ÄÁ¥¢ÁªìÊûúËøõË°åÁªìÊûÑÂåñÈ™åËØÅÂíå‰ø°ÊÅØËÅöÂêàÔºõ5) ÁîüÊàêÊ®°ÂùóÔºåÊ†πÊçÆÊé®ÁêÜÁªìÊûúÁîüÊàêÊúÄÁªàÁ≠îÊ°à„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVgentÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ‰ΩøÁî®ÂõæÁªìÊûÑÊù•Ë°®Á§∫ËßÜÈ¢ëÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞‰øùÁïôËßÜÈ¢ëÁâáÊÆµ‰πãÈó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ªÔºåÊèêÈ´òÊ£ÄÁ¥¢ÊïàÁéáÔºõ2) ÂºïÂÖ•‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºåÂà©Áî®ÁªìÊûÑÂåñÈ™åËØÅÊù•ÂáèÂ∞ëÊ£ÄÁ¥¢Âô™Â£∞ÔºåÂπ∂‰øÉËøõË∑®ÁâáÊÆµÁõ∏ÂÖ≥‰ø°ÊÅØÁöÑÊòæÂºèËÅöÂêàÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåVgentÊõ¥Âä†Ê≥®ÈáçËßÜÈ¢ëÁâáÊÆµ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂíå‰ø°ÊÅØÁöÑËÅöÂêà„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂõæÁªìÊûÑÁöÑÊûÑÂª∫ÊñπÂºèÊòØÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏ÄÔºå‰æãÂ¶ÇÂ¶Ç‰ΩïÂÆö‰πâËäÇÁÇπÂíåËæπÔºåÂ¶Ç‰ΩïËÆ°ÁÆóËæπÁöÑÊùÉÈáçÁ≠â„ÄÇ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊñπÂºèÔºå‰æãÂ¶Ç‰ΩøÁî®‰ªÄ‰πàÊ†∑ÁöÑÁªìÊûÑÂåñÈ™åËØÅÊñπÊ≥ïÔºåÂ¶Ç‰ΩïËøõË°å‰ø°ÊÅØËÅöÂêàÁ≠âÔºå‰πüÊòØÂÖ≥ÈîÆËÆæËÆ°„ÄÇËÆ∫Êñá‰∏≠ÂèØËÉΩËøòÊ∂âÂèä‰∏Ä‰∫õË∂ÖÂèÇÊï∞ÁöÑËÆæÁΩÆÔºå‰æãÂ¶ÇÂõæÁöÑËäÇÁÇπÊï∞Èáè„ÄÅËæπÁöÑÊùÉÈáçÈòàÂÄºÁ≠âÔºåËøô‰∫õÂèÇÊï∞ÁöÑËÆæÁΩÆ‰πü‰ºöÂΩ±ÂìçÊúÄÁªàÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VgentÂú®‰∏â‰∏™ÈïøËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ËøõË°å‰∫ÜËØÑ‰º∞ÔºåÁªìÊûúË°®ÊòéÔºåVgentÂú®MLVU‰∏äÁõ∏ÂØπ‰∫éÂü∫Á∫øÊ®°ÂûãÂÆûÁé∞‰∫Ü3.0%~5.4%ÁöÑÊï¥‰ΩìÊÄßËÉΩÊèêÂçáÔºåÂπ∂‰∏î‰ºò‰∫éÊúÄÂÖàËøõÁöÑËßÜÈ¢ëRAGÊñπÊ≥ï8.6%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåVgentËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òÈïøËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VgentÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÁõëÊéß„ÄÅËßÜÈ¢ëÊêúÁ¥¢„ÄÅËßÜÈ¢ëÊëòË¶Å„ÄÅËßÜÈ¢ëÈóÆÁ≠îÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êô∫ËÉΩÁõëÊéß‰∏≠ÔºåÂèØ‰ª•Âà©Áî®VgentÂØπÁõëÊéßËßÜÈ¢ëËøõË°åÂàÜÊûêÔºåÂø´ÈÄüÂÆö‰ΩçÂºÇÂ∏∏‰∫ã‰ª∂„ÄÇÂú®ËßÜÈ¢ëÊêúÁ¥¢‰∏≠ÔºåÂèØ‰ª•Âà©Áî®VgentÊèêÈ´òÊêúÁ¥¢ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇÂú®ËßÜÈ¢ëÊëòË¶Å‰∏≠ÔºåÂèØ‰ª•Âà©Áî®VgentÊèêÂèñËßÜÈ¢ëÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÁîüÊàêÁÆÄÊ¥ÅÁöÑÊëòË¶Å„ÄÇÂú®ËßÜÈ¢ëÈóÆÁ≠î‰∏≠ÔºåÂèØ‰ª•Âà©Áî®VgentÂõûÁ≠îÁî®Êà∑ÊèêÂá∫ÁöÑÂÖ≥‰∫éËßÜÈ¢ëÂÜÖÂÆπÁöÑÈóÆÈ¢ò„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Understanding and reasoning over long videos pose significant challenges for large video language models (LVLMs) due to the difficulty in processing intensive video tokens beyond context window and retaining long-term sequential information. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in processing long context for Large Language Models (LLMs); however, applying RAG to long video faces challenges such as disrupted temporal dependencies and inclusion of irrelevant information that can hinder accurate reasoning. To address these limitations, we propose Vgent, a novel graph-based retrieval-reasoning-augmented generation framework to enhance LVLMs for long video understanding. Our approach introduces two key innovations: (i) It represents videos by structured graphs with semantic relationships across video clips preserved to improve retrieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate the reasoning limitation of LVLMs, which leverages structured verification to reduce retrieval noise and facilitate the explicit aggregation of relevant information across clips, resulting in more accurate and context-aware responses. We comprehensively evaluate our framework with various open-source LVLMs on three long-video understanding benchmarks. Our approach yielded an overall performance improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available at https://xiaoqian-shen.github.io/Vgent.

