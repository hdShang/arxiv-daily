---
layout: default
title: Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark
---

# Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13759" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13759v1</a>
  <a href="https://arxiv.org/pdf/2510.13759.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13759v1" onclick="toggleFavorite(this, '2510.13759v1', 'Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, Ziwei Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**å¤‡æ³¨**: Equal contributions from frst three authors. Project page: https://vchitect.github.io/Uni-MMMU-Project/ Code: https://github.com/vchitect/Uni-MMMU

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUni-MMMUï¼šä¸€ä¸ªå¤§è§„æ¨¡å¤šå­¦ç§‘å¤šæ¨¡æ€ç»Ÿä¸€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰ç†è§£ä¸ç”Ÿæˆæ¨¡å‹çš„åŒå‘ååŒèƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç»Ÿä¸€æ¨¡å‹` `è§†è§‰ç†è§£` `è§†è§‰ç”Ÿæˆ` `åŸºå‡†æµ‹è¯•` `è·¨æ¨¡æ€æ¨ç†` `äººå·¥æ™ºèƒ½` `è®¤çŸ¥è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä¹‹é—´çš„çœŸæ­£é›†æˆï¼Œå¿½ç•¥äº†ä¸¤è€…å†…åœ¨è€¦åˆçš„ä»»åŠ¡ã€‚
2. Uni-MMMUé€šè¿‡æ„å»ºåŒå‘è€¦åˆä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åˆ©ç”¨ç†è§£æŒ‡å¯¼ç”Ÿæˆï¼Œæˆ–åˆ©ç”¨ç”Ÿæˆè¾…åŠ©ç†è§£ï¼Œä»è€Œä¿ƒè¿›è·¨æ¨¡æ€ååŒã€‚
3. Uni-MMMUåŒ…å«å¯éªŒè¯çš„æ¨ç†æ­¥éª¤ã€ç‹¬ç‰¹çš„çœŸå€¼å’Œå¯é‡å¤çš„è¯„åˆ†åè®®ï¼Œä¸ºç»Ÿä¸€æ¨¡å‹çš„è¯„ä¼°æä¾›äº†å¯é çš„åŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æ—¨åœ¨è”åˆå®ç°è§†è§‰ç†è§£å’Œç”Ÿæˆï¼Œä½†ç›®å‰çš„åŸºå‡†å¾ˆå°‘æ£€éªŒå®ƒä»¬çš„çœŸæ­£é›†æˆã€‚ç°æœ‰çš„è¯„ä¼°è¦ä¹ˆå­¤ç«‹åœ°å¯¹å¾…è¿™ä¸¤ç§èƒ½åŠ›ï¼Œè¦ä¹ˆå¿½ç•¥äº†å†…åœ¨è€¦åˆå®ƒä»¬çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†Uni-MMMUï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ã€å­¦ç§‘æ„ŸçŸ¥çš„åŸºå‡†ï¼Œå®ƒç³»ç»Ÿåœ°å±•å¼€äº†ç”Ÿæˆå’Œç†è§£ä¹‹é—´åœ¨å…«ä¸ªä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„é¢†åŸŸï¼ˆåŒ…æ‹¬ç§‘å­¦ã€ç¼–ç ã€æ•°å­¦å’Œè°œé¢˜ï¼‰çš„åŒå‘ååŒä½œç”¨ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æ˜¯åŒå‘è€¦åˆçš„ï¼Œè¦æ±‚æ¨¡å‹ï¼ˆiï¼‰åˆ©ç”¨æ¦‚å¿µç†è§£æ¥æŒ‡å¯¼ç²¾ç¡®çš„è§†è§‰åˆæˆï¼Œæˆ–ï¼ˆiiï¼‰åˆ©ç”¨ç”Ÿæˆä½œä¸ºåˆ†ææ¨ç†çš„è®¤çŸ¥æ”¯æ¶ã€‚Uni-MMMUåŒ…å«å¯éªŒè¯çš„ä¸­é—´æ¨ç†æ­¥éª¤ã€ç‹¬ç‰¹çš„ground truthä»¥åŠæ–‡æœ¬å’Œè§†è§‰è¾“å‡ºçš„å¯é‡å¤è¯„åˆ†åè®®ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„ç»Ÿä¸€æ¨¡å‹ã€ä»…ç”Ÿæˆæ¨¡å‹å’Œä»…ç†è§£æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚å’Œè·¨æ¨¡æ€ä¾èµ–æ€§ï¼Œä¸ºè¿™äº›èƒ½åŠ›ä½•æ—¶ä»¥åŠå¦‚ä½•ç›¸äº’åŠ å¼ºæä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶ä¸ºæ¨è¿›ç»Ÿä¸€æ¨¡å‹å¥ å®šäº†å¯é çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°benchmarkï¼Œè¦ä¹ˆå­¤ç«‹åœ°è¯„ä¼°è§†è§‰ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œè¦ä¹ˆç¼ºä¹å¯¹ä¸¤è€…å†…åœ¨è”ç³»çš„æ·±å…¥è€ƒå¯Ÿï¼Œæ— æ³•æœ‰æ•ˆè¡¡é‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è·¨æ¨¡æ€ååŒèƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªèƒ½å¤Ÿå…¨é¢ã€ç³»ç»Ÿåœ°è¯„ä¼°æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆä¹‹é—´åŒå‘ååŒä½œç”¨çš„åŸºå‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUni-MMMUçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ç³»åˆ—åŒå‘è€¦åˆçš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¦æ±‚æ¨¡å‹åŒæ—¶å…·å¤‡è§†è§‰ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿåˆ©ç”¨ä¸€ç§èƒ½åŠ›æ¥è¾…åŠ©å¦ä¸€ç§èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®å¯¹ç§‘å­¦æ¦‚å¿µçš„ç†è§£æ¥ç”Ÿæˆç›¸åº”çš„å›¾åƒï¼Œæˆ–è€…åˆ©ç”¨ç”Ÿæˆçš„ä»£ç ç‰‡æ®µæ¥è¾…åŠ©æ•°å­¦é—®é¢˜çš„æ±‚è§£ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒUni-MMMUèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è·¨æ¨¡æ€ååŒèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUni-MMMUåŸºå‡†åŒ…å«å…«ä¸ªä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„é¢†åŸŸï¼ŒåŒ…æ‹¬ç§‘å­¦ã€ç¼–ç ã€æ•°å­¦å’Œè°œé¢˜ã€‚æ¯ä¸ªé¢†åŸŸéƒ½åŒ…å«å¤šä¸ªåŒå‘è€¦åˆçš„ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¦æ±‚æ¨¡å‹è¿›è¡Œè§†è§‰ç†è§£å’Œç”Ÿæˆã€‚Uni-MMMUè¿˜æä¾›å¯éªŒè¯çš„ä¸­é—´æ¨ç†æ­¥éª¤ã€ç‹¬ç‰¹çš„ground truthä»¥åŠæ–‡æœ¬å’Œè§†è§‰è¾“å‡ºçš„å¯é‡å¤è¯„åˆ†åè®®ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šè¾“å…¥å¤šæ¨¡æ€æ•°æ®ï¼ˆä¾‹å¦‚æ–‡æœ¬æè¿°å’Œå›¾åƒï¼‰ï¼Œæ¨¡å‹è¿›è¡Œæ¨ç†å’Œç”Ÿæˆï¼Œç„¶åæ ¹æ®ground truthè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šUni-MMMUçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŒå‘è€¦åˆçš„ä»»åŠ¡è®¾è®¡ï¼Œè¿™ç§è®¾è®¡èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä¹‹é—´çš„ååŒä½œç”¨ã€‚æ­¤å¤–ï¼ŒUni-MMMUè¿˜æä¾›äº†å¯éªŒè¯çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œè¿™ä½¿å¾—è¯„ä¼°è¿‡ç¨‹æ›´åŠ é€æ˜å’Œå¯è§£é‡Šã€‚ä¸ç°æœ‰benchmarkç›¸æ¯”ï¼ŒUni-MMMUæ›´æ³¨é‡è€ƒå¯Ÿæ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è·¨æ¨¡æ€èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šUni-MMMUçš„å…³é”®è®¾è®¡åŒ…æ‹¬ä»»åŠ¡çš„é€‰æ‹©å’Œè®¾è®¡ã€ground truthçš„æ„å»ºä»¥åŠè¯„åˆ†åè®®çš„åˆ¶å®šã€‚ä»»åŠ¡çš„é€‰æ‹©éœ€è¦è€ƒè™‘åˆ°ä¸åŒé¢†åŸŸçš„ç‰¹ç‚¹ï¼Œå¹¶ä¸”è¦ç¡®ä¿ä»»åŠ¡å…·æœ‰è¶³å¤Ÿçš„æŒ‘æˆ˜æ€§å’Œå¤šæ ·æ€§ã€‚Ground truthçš„æ„å»ºéœ€è¦ä¿è¯å…¶å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚è¯„åˆ†åè®®çš„åˆ¶å®šéœ€è¦ä¿è¯å…¶å¯é‡å¤æ€§å’Œå…¬å¹³æ€§ã€‚å…·ä½“å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€è¯„ä¼°çš„æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Uni-MMMUå¯¹æœ€å…ˆè¿›çš„ç»Ÿä¸€æ¨¡å‹ã€ä»…ç”Ÿæˆæ¨¡å‹å’Œä»…ç†è§£æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚å’Œè·¨æ¨¡æ€ä¾èµ–æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»Ÿä¸€æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºä»…ç”Ÿæˆæˆ–ä»…ç†è§£æ¨¡å‹ï¼Œä½†åœ¨å…¶ä»–ä»»åŠ¡ä¸Šåˆ™è¡¨ç°ä¸å¦‚ã€‚è¿™è¡¨æ˜ï¼Œä¸åŒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šå…·æœ‰ä¸åŒçš„ä¼˜åŠ¿ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Uni-MMMUçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´å¼ºå¤§çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œä¾‹å¦‚æ™ºèƒ½æ•™è‚²å¹³å°ã€è¾…åŠ©è®¾è®¡å·¥å…·å’Œæ™ºèƒ½æœºå™¨äººã€‚è¿™äº›ç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆè§†è§‰ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†å¯ä»¥æ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹åœ¨ç§‘å­¦ç ”ç©¶ã€è½¯ä»¶å¼€å‘å’Œé—®é¢˜è§£å†³ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.

