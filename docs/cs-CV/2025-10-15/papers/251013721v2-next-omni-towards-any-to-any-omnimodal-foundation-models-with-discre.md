---
layout: default
title: NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching
---

# NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.13721" target="_blank" class="toolbar-btn">arXiv: 2510.13721v2</a>
    <a href="https://arxiv.org/pdf/2510.13721.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13721v2" 
            onclick="toggleFavorite(this, '2510.13721v2', 'NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.CV, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15 (Êõ¥Êñ∞: 2025-10-16)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**NExT-OMNIÔºöÂü∫‰∫éÁ¶ªÊï£ÊµÅÂåπÈÖçÁöÑ‰ªªÊÑèÂà∞‰ªªÊÑèÂÖ®Ê®°ÊÄÅÁªü‰∏ÄÂª∫Ê®°**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂÖ®Ê®°ÊÄÅÊ®°Âûã` `Á¶ªÊï£ÊµÅÂåπÈÖç` `Ë∑®Ê®°ÊÄÅÊ£ÄÁ¥¢` `Â§öËΩÆ‰∫§‰∫í` `Áªü‰∏ÄÂª∫Ê®°` `ÁîüÊàêÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊ®°ÂûãÂèóÈôê‰∫éËá™ÂõûÂΩíÊû∂ÊûÑÔºåÈöæ‰ª•Âπ≥Ë°°ÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõÔºåÊ∑∑ÂêàÂíåËß£ËÄ¶Á≠ñÁï•ËÆæËÆ°ÂÜó‰ΩôÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Ë∑®Ê®°ÊÄÅÊ£ÄÁ¥¢Á≠âÊõ¥ÂπøÊ≥õÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. NExT-OMNIÂà©Áî®Á¶ªÊï£ÊµÅËåÉÂºèÔºåÈÄöËøáÂ∫¶ÈáèËØ±ÂØºÁöÑÊ¶ÇÁéáË∑ØÂæÑÂíåÂä®ÂäõÂ≠¶ÊúÄ‰ºòÈÄüÂ∫¶ÔºåÂÆûÁé∞‰ªªÊÑèÊ®°ÊÄÅÂà∞‰ªªÊÑèÊ®°ÊÄÅÁöÑÁªü‰∏ÄÂª∫Ê®°ÔºåÊèêÂçáÂìçÂ∫îÊïàÁéá„ÄÇ
3. NExT-OMNIÂú®Â§öÊ®°ÊÄÅÁîüÊàêÂíåÁêÜËß£‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Âú®Â§öËΩÆÂ§öÊ®°ÊÄÅ‰∫§‰∫íÂíåË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢ÊñπÈù¢Ë∂ÖË∂ä‰∫Ü‰ª•ÂæÄÁöÑÁªü‰∏ÄÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜNExT-OMNIÔºå‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂÖ®Ê®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÔºåÊó®Âú®ÈÄöËøáÁ¶ªÊï£ÊµÅËåÉÂºèÂÆûÁé∞Áªü‰∏ÄÂª∫Ê®°Ôºå‰ªéËÄåÊîØÊåÅ‰ªªÊÑèÊ®°ÊÄÅÂà∞‰ªªÊÑèÊ®°ÊÄÅÁöÑÁêÜËß£ÂíåÁîüÊàêÔºåÂπ∂ÊèêÈ´òÂìçÂ∫îÊïàÁéá„ÄÇNExT-OMNIÂà©Áî®Â∫¶ÈáèËØ±ÂØºÁöÑÊ¶ÇÁéáË∑ØÂæÑÂíåÂä®ÂäõÂ≠¶ÊúÄ‰ºòÈÄüÂ∫¶ÔºåÈÄöËøáÁÆÄÊ¥ÅÁöÑÁªü‰∏ÄË°®Á§∫ËÄåÈùû‰ªªÂä°Ëß£ËÄ¶ËÆæËÆ°ÔºåÂÆûÁé∞Êõ¥ÂπøÊ≥õÁöÑÂ∫îÁî®Âú∫ÊôØÔºå‰æãÂ¶ÇË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢„ÄÇËØ•Ê®°ÂûãÂú®Â§ßËßÑÊ®°‰∫§ÈîôÁöÑÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅËßÜÈ¢ëÂíåÈü≥È¢ëÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºåÂú®Â§öÊ®°ÊÄÅÁîüÊàêÂíåÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Á´û‰∫âÂäõÁöÑÊÄßËÉΩÔºåÂπ∂Âú®Â§öËΩÆÂ§öÊ®°ÊÄÅ‰∫§‰∫íÂíåË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢ÊñπÈù¢‰ºò‰∫é‰ª•ÂæÄÁöÑÁªü‰∏ÄÊ®°ÂûãÔºåÁ™ÅÊòæ‰∫ÜÂÖ∂‰Ωú‰∏∫‰∏ã‰∏Ä‰ª£Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÁöÑÊû∂ÊûÑ‰ºòÂäø„ÄÇ‰∏∫‰∫Ü‰øÉËøõËøõ‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂Ôºå‰ΩúËÄÖÂèëÂ∏É‰∫ÜËÆ≠ÁªÉÁªÜËäÇ„ÄÅÊï∞ÊçÆÂçèËÆÆ‰ª•Âèä‰ª£Á†ÅÂíåÊ®°ÂûãÊ£ÄÊü•ÁÇπ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÈÉ®ÂàÜÂ§öÊ®°ÊÄÅÊ®°Âûã‰æùËµñ‰∫éËá™ÂõûÂΩíÊû∂ÊûÑÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®ÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõ‰∏äÁöÑÂπ≥Ë°°„ÄÇÊ≠§Â§ñÔºåËôΩÁÑ∂‰∏Ä‰∫õÊ∑∑ÂêàÂíåËß£ËÄ¶Á≠ñÁï•Ë¢´Áî®‰∫éÁªü‰∏ÄÊ°ÜÊû∂‰∏≠Ôºå‰ΩÜÂÆÉ‰ª¨ÁöÑËÆæËÆ°ËæÉ‰∏∫ÂÜó‰ΩôÔºåÊó†Ê≥ïÂæàÂ•ΩÂú∞Â∫îÁî®‰∫éÊõ¥ÂπøÊ≥õÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏Ä‰∏™Êõ¥Âä†Áªü‰∏ÄÂíåÈ´òÊïàÁöÑÊ®°ÂûãÔºåËÉΩÂ§üÊîØÊåÅ‰ªªÊÑèÊ®°ÊÄÅ‰πãÈó¥ÁöÑËΩ¨Êç¢Âíå‰∫§‰∫í„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöNExT-OMNIÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Á¶ªÊï£ÊµÅÂåπÈÖçÔºàDiscrete Flow MatchingÔºâËåÉÂºèÔºåÂ∞Ü‰∏çÂêåÊ®°ÊÄÅÁöÑÊï∞ÊçÆÊò†Â∞ÑÂà∞Áªü‰∏ÄÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠„ÄÇÈÄöËøáÂ≠¶‰π†Â∫¶ÈáèËØ±ÂØºÁöÑÊ¶ÇÁéáË∑ØÂæÑÂíåÂä®ÂäõÂ≠¶ÊúÄ‰ºòÈÄüÂ∫¶ÔºåÊ®°ÂûãËÉΩÂ§üÈ´òÊïàÂú∞Âú®‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ËøõË°åËΩ¨Êç¢Ôºå‰ªéËÄåÂÆûÁé∞‰ªªÊÑèÊ®°ÊÄÅÂà∞‰ªªÊÑèÊ®°ÊÄÅÁöÑÁêÜËß£ÂíåÁîüÊàê„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫Ü‰º†ÁªüËá™ÂõûÂΩíÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂Êèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥Âä†ÁÆÄÊ¥ÅÂíåÁªü‰∏ÄÁöÑÂª∫Ê®°ÊñπÂºè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöNExT-OMNIÁöÑÊï¥‰ΩìÊû∂ÊûÑÂü∫‰∫éÁ¶ªÊï£ÊµÅÂåπÈÖç„ÄÇÈ¶ñÂÖàÔºå‰∏çÂêåÊ®°ÊÄÅÁöÑÊï∞ÊçÆÈÄöËøáÂêÑËá™ÁöÑÁºñÁ†ÅÂô®Êò†Â∞ÑÂà∞ÊΩúÂú®Á©∫Èó¥„ÄÇÁÑ∂ÂêéÔºåÊ®°ÂûãÂ≠¶‰π†‰∏Ä‰∏™ÊµÅÂú∫ÔºåËØ•ÊµÅÂú∫ÂÆö‰πâ‰∫Ü‰ªé‰∏Ä‰∏™Ê®°ÊÄÅÂà∞Âè¶‰∏Ä‰∏™Ê®°ÊÄÅÁöÑÊ¶ÇÁéáË∑ØÂæÑ„ÄÇÂú®ÁîüÊàêËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÊ≤øÁùÄËøô‰∏™ÊµÅÂú∫ËøõË°åÈááÊ†∑Ôºå‰ªéËÄåÁîüÊàêÁõÆÊ†áÊ®°ÊÄÅÁöÑÊï∞ÊçÆ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´Ê®°ÊÄÅÁºñÁ†ÅÂô®„ÄÅÊµÅÂú∫Â≠¶‰π†Ê®°ÂùóÂíåÊ®°ÊÄÅËß£Á†ÅÂô®‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöNExT-OMNIÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ΩøÁî®Á¶ªÊï£ÊµÅÂåπÈÖçËåÉÂºèËøõË°åÂ§öÊ®°ÊÄÅÁªü‰∏ÄÂª∫Ê®°„ÄÇ‰∏é‰º†ÁªüÁöÑËá™ÂõûÂΩíÊ®°ÂûãÁõ∏ÊØîÔºåÁ¶ªÊï£ÊµÅÂåπÈÖçËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â≠¶‰π†‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂ÊîØÊåÅ‰ªªÊÑèÊ®°ÊÄÅ‰πãÈó¥ÁöÑËΩ¨Êç¢„ÄÇÊ≠§Â§ñÔºåNExT-OMNIÈÄöËøáÂ≠¶‰π†Â∫¶ÈáèËØ±ÂØºÁöÑÊ¶ÇÁéáË∑ØÂæÑÂíåÂä®ÂäõÂ≠¶ÊúÄ‰ºòÈÄüÂ∫¶ÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜÁîüÊàêÊïàÁéáÂíåË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöNExT-OMNIÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Transformer‰Ωú‰∏∫Ê®°ÊÄÅÁºñÁ†ÅÂô®ÂíåËß£Á†ÅÂô®Ôºå‰ª•ÊçïÊçâ‰∏çÂêåÊ®°ÊÄÅÊï∞ÊçÆÁöÑÂ§çÊùÇÁâπÂæÅÔºõ2) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éÂ≠¶‰π†ÊµÅÂú∫ÔºåËØ•ÊçüÂ§±ÂáΩÊï∞ÂêåÊó∂ËÄÉËôë‰∫ÜÁîüÊàêË¥®ÈáèÂíåÊïàÁéáÔºõ3) ÈááÁî®‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÈááÊ†∑Á≠ñÁï•Ôºå‰ª•ÊèêÈ´òÁîüÊàêÁöÑÂ§öÊ†∑ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

NExT-OMNIÂú®Â§öÊ®°ÊÄÅÁîüÊàêÂíåÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Á´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÂú®Â§öËΩÆÂ§öÊ®°ÊÄÅ‰∫§‰∫íÂíåË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢ÊñπÈù¢ÔºåNExT-OMNIÊòæËëó‰ºò‰∫é‰ª•ÂæÄÁöÑÁªü‰∏ÄÊ®°ÂûãÔºåËØÅÊòé‰∫ÜÂÖ∂Êû∂ÊûÑÁöÑ‰ºòË∂äÊÄß„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂú®ËÆ∫Êñá‰∏≠ÁªôÂá∫ÔºåË°®ÊòéNExT-OMNIÂú®Â§ö‰∏™‰ªªÂä°‰∏äÈÉΩÂèñÂæó‰∫ÜSOTAÊàñÊé•ËøëSOTAÁöÑÁªìÊûú„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

NExT-OMNIÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂåÖÊã¨Â§öÊ®°ÊÄÅÂØπËØùÁ≥ªÁªü„ÄÅË∑®Ê®°ÊÄÅÂÜÖÂÆπÁîüÊàê„ÄÅÊô∫ËÉΩÂä©Êâã„ÄÅÊïôËÇ≤Â®±‰πêÁ≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Áî®‰∫éÁîüÊàêÂõæÂÉèÊèèËø∞„ÄÅËßÜÈ¢ëÊëòË¶Å„ÄÅÈü≥È¢ëËΩ¨ÂΩïÁ≠âÔºåËøòÂèØ‰ª•Áî®‰∫éÂÆûÁé∞Êõ¥Ëá™ÁÑ∂ÂíåÊô∫ËÉΩÁöÑ‰∫∫Êú∫‰∫§‰∫í„ÄÇÊú™Êù•ÔºåNExT-OMNIÊúâÊúõÊàê‰∏∫ÈÄöÁî®‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÊ†∏ÂøÉÁªÑÊàêÈÉ®ÂàÜ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval. In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.

