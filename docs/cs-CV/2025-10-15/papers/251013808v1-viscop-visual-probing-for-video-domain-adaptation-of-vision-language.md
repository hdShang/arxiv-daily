---
layout: default
title: "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models"
---

# VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13808" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13808v1</a>
  <a href="https://arxiv.org/pdf/2510.13808.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13808v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13808v1', 'VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dominick Reilly, Manish Kumar Govind, Le Xue, Srijan Das

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VisCoPï¼šé€šè¿‡è§†è§‰æ¢é’ˆå®ç°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘é¢†åŸŸçš„åŸŸè‡ªé€‚åº”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `åŸŸè‡ªé€‚åº”` `è§†è§‰æ¢é’ˆ` `è§†é¢‘ç†è§£` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨è·¨é¢†åŸŸåº”ç”¨æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå¾®è°ƒæ•´ä¸ªæ¨¡å‹æ˜“å¯¼è‡´ç¾éš¾æ€§é—å¿˜å’Œé¢†åŸŸç‰¹å¾å­¦ä¹ ä¸è¶³ã€‚
2. VisCoPé€šè¿‡å¼•å…¥å°‘é‡å¯å­¦ä¹ çš„è§†è§‰æ¢é’ˆï¼Œå¢å¼ºVLMçš„è§†è§‰ç¼–ç å™¨ï¼Œå®ç°é«˜æ•ˆçš„é¢†åŸŸè‡ªé€‚åº”ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVisCoPåœ¨è·¨è§†è§’ã€è·¨æ¨¡æ€å’Œè·¨ä»»åŠ¡çš„åŸŸè‡ªé€‚åº”ä¸­ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆä¿ç•™æºåŸŸçŸ¥è¯†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨é€šç”¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å½“åº”ç”¨äºä¸é¢„è®­ç»ƒæ•°æ®å­˜åœ¨æ˜¾è‘—åˆ†å¸ƒå·®å¼‚çš„æ–°é¢†åŸŸæ—¶ï¼Œæ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ã€‚ç°æœ‰çš„åŸŸè‡ªé€‚åº”æ–¹æ³•å¾®è°ƒä¸åŒçš„VLMç»„ä»¶ï¼Œä½†è¿™é€šå¸¸å¯¼è‡´æœ‰é™çš„é¢†åŸŸç‰¹å®šç‰¹å¾å­¦ä¹ æˆ–ç¾éš¾æ€§åœ°é—å¿˜å…ˆå‰çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰ä¸Šä¸‹æ–‡æ¢é’ˆ(VisCoP)ï¼Œå®ƒä½¿ç”¨ä¸€ç»„ç´§å‡‘çš„å¯å­¦ä¹ è§†è§‰æ¢é’ˆæ¥å¢å¼ºVLMçš„è§†è§‰ç¼–ç å™¨ã€‚è¿™äº›æ¢é’ˆèƒ½å¤Ÿä»¥æœ€å°çš„é¢„è®­ç»ƒå‚æ•°ä¿®æ”¹å®ç°é«˜æ•ˆçš„é¢†åŸŸç‰¹å®šè‡ªé€‚åº”ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸŸè‡ªé€‚åº”è®¾ç½®ä¸­è¯„ä¼°VisCoPâ€”â€”è·¨è§†è§’(ä»ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒåˆ°ä»¥ä»–äººä¸ºä¸­å¿ƒ)ã€è·¨æ¨¡æ€(RGBåˆ°æ·±åº¦)å’Œè·¨ä»»åŠ¡(äººç±»ç†è§£åˆ°æœºå™¨äººæ§åˆ¶)ã€‚å®éªŒè¡¨æ˜ï¼ŒVisCoPå§‹ç»ˆä¼˜äºç°æœ‰çš„è‡ªé€‚åº”ç­–ç•¥ï¼Œåœ¨ç›®æ ‡åŸŸä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ä¿ç•™äº†æºåŸŸçŸ¥è¯†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨é¢å¯¹ä¸é¢„è®­ç»ƒæ•°æ®å­˜åœ¨æ˜¾è‘—åˆ†å¸ƒå·®å¼‚çš„æ–°è§†é¢‘é¢†åŸŸæ—¶ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¾®è°ƒæ•´ä¸ªVLMæˆ–éƒ¨åˆ†ç»„ä»¶æ¥å®ç°åŸŸè‡ªé€‚åº”ï¼Œä½†è¿™äº›æ–¹æ³•å®¹æ˜“å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œå³æ¨¡å‹åœ¨é€‚åº”æ–°é¢†åŸŸçš„åŒæ—¶ï¼Œä¸§å¤±äº†åœ¨åŸå§‹é¢†åŸŸä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…¨å±€å¾®è°ƒå¯èƒ½æ— æ³•æœ‰æ•ˆåœ°å­¦ä¹ åˆ°é¢†åŸŸç‰¹å®šçš„ç‰¹å¾ï¼Œå¯¼è‡´è‡ªé€‚åº”æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ç»„å¯å­¦ä¹ çš„è§†è§‰æ¢é’ˆ(Visual Probes)æ¥å¢å¼ºVLMçš„è§†è§‰ç¼–ç å™¨ã€‚è¿™äº›æ¢é’ˆä»¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–¹å¼ä¸è§†è§‰ç‰¹å¾è¿›è¡Œäº¤äº’ï¼Œä»è€Œå®ç°é¢†åŸŸç‰¹å®šçš„è‡ªé€‚åº”ã€‚é€šè¿‡åªè®­ç»ƒè¿™äº›å°‘é‡çš„æ¢é’ˆå‚æ•°ï¼Œå¯ä»¥é¿å…å¯¹æ•´ä¸ªVLMè¿›è¡Œå¤§è§„æ¨¡çš„å¾®è°ƒï¼Œä»è€Œå‡è½»ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°å­¦ä¹ åˆ°é¢†åŸŸç›¸å…³çš„è§†è§‰ç‰¹å¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVisCoPæ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ¨¡å—ï¼š1) é¢„è®­ç»ƒçš„VLMï¼šä½¿ç”¨é¢„è®­ç»ƒå¥½çš„VLMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œä¾‹å¦‚CLIPç­‰ã€‚2) è§†è§‰æ¢é’ˆ(Visual Probes)ï¼šä¸€ç»„å¯å­¦ä¹ çš„å‚æ•°ï¼Œç”¨äºæå–å’Œè°ƒæ•´è§†è§‰ç‰¹å¾ã€‚è¿™äº›æ¢é’ˆè¢«æ’å…¥åˆ°VLMçš„è§†è§‰ç¼–ç å™¨çš„ä¸åŒå±‚ä¸­ï¼Œä»¥æ•æ‰ä¸åŒå±‚æ¬¡çš„è§†è§‰ä¿¡æ¯ã€‚3) ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å—ï¼šè¯¥æ¨¡å—ç”¨äºå°†è§†è§‰æ¢é’ˆæå–çš„ç‰¹å¾ä¸åŸå§‹çš„è§†è§‰ç‰¹å¾è¿›è¡Œèåˆï¼Œä»è€Œå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç‰¹å¾è¡¨ç¤ºã€‚4) è®­ç»ƒç­–ç•¥ï¼šé‡‡ç”¨ç‰¹å®šçš„è®­ç»ƒç­–ç•¥ï¼Œä¾‹å¦‚å¯¹æ¯”å­¦ä¹ æˆ–äº¤å‰ç†µæŸå¤±ï¼Œæ¥ä¼˜åŒ–è§†è§‰æ¢é’ˆçš„å‚æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç›®æ ‡é¢†åŸŸã€‚

**å…³é”®åˆ›æ–°**ï¼šVisCoPæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†è§†è§‰æ¢é’ˆ(Visual Probes)çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºVLMçš„åŸŸè‡ªé€‚åº”ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVisCoPé€šè¿‡åªè®­ç»ƒå°‘é‡çš„æ¢é’ˆå‚æ•°ï¼Œé¿å…äº†å¯¹æ•´ä¸ªVLMè¿›è¡Œå¤§è§„æ¨¡çš„å¾®è°ƒï¼Œä»è€Œå‡è½»äº†ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°å­¦ä¹ åˆ°é¢†åŸŸç›¸å…³çš„è§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å—çš„è®¾è®¡ä¹Ÿä½¿å¾—æ¢é’ˆæå–çš„ç‰¹å¾èƒ½å¤Ÿæ›´å¥½åœ°ä¸åŸå§‹è§†è§‰ç‰¹å¾è¿›è¡Œèåˆï¼Œä»è€Œæå‡äº†è‡ªé€‚åº”æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šè§†è§‰æ¢é’ˆçš„å…·ä½“å®ç°æ–¹å¼å¯ä»¥æ˜¯MLPã€Transformerå±‚æˆ–å…¶ä»–è½»é‡çº§çš„ç¥ç»ç½‘ç»œç»“æ„ã€‚æ¢é’ˆçš„æ•°é‡å’Œä½ç½®éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œå¸¸ç”¨çš„æŸå¤±å‡½æ•°åŒ…æ‹¬å¯¹æ¯”å­¦ä¹ æŸå¤±ã€äº¤å‰ç†µæŸå¤±ç­‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¯èƒ½è¿˜é‡‡ç”¨äº†å…¶ä»–ä¸€äº›æŠ€æœ¯ç»†èŠ‚ï¼Œä¾‹å¦‚å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ã€æ­£åˆ™åŒ–æ–¹æ³•ç­‰ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VisCoPåœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸŸè‡ªé€‚åº”è®¾ç½®ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬è·¨è§†è§’ã€è·¨æ¨¡æ€å’Œè·¨ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisCoPå§‹ç»ˆä¼˜äºç°æœ‰çš„è‡ªé€‚åº”ç­–ç•¥ï¼Œåœ¨ç›®æ ‡åŸŸä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ä¿ç•™äº†æºåŸŸçŸ¥è¯†ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æ˜ç¡®æŒ‡å‡ºVisCoP consistently outperforms existing adaptation strategiesã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VisCoPæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººè§†è§‰ä¸­ï¼Œå¯ä»¥å°†VisCoPåº”ç”¨äºæœºå™¨äººæ§åˆ¶ä»»åŠ¡ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œé€‚åº”ä¸åŒçš„ç¯å¢ƒã€‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒVisCoPå¯ä»¥ç”¨äºå¤„ç†ä¸åŒå¤©æ°”æ¡ä»¶ä¸‹çš„å›¾åƒï¼Œæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„é²æ£’æ€§ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼ŒVisCoPå¯ä»¥ç”¨äºå¤„ç†ä¸åŒç±»å‹çš„åŒ»å­¦å›¾åƒï¼Œä¾‹å¦‚Xå…‰ç‰‡ã€CTæ‰«æç­‰ï¼Œè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (VLMs) excel at general visual reasoning tasks but exhibit sharp performance degradation when applied to novel domains with substantial distribution shifts from pretraining data. Existing domain adaptation approaches finetune different VLM components, but this often results in limited domain-specific feature learning or catastrophic forgetting of prior capabilities. To address these issues, we introduce Vision Contextualized Probing (VisCoP), which augments the VLM's vision encoder with a compact set of learnable visual probes. These probes enable efficient domain-specific adaptation with minimal modification to pretrained parameters. We evaluate VisCoP across three challenging domain adaptation settings-cross-view (exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human understanding to robot control). Experiments show that VisCoP consistently outperforms existing adaptation strategies, achieving superior performance on target domains while effectively retaining source-domain knowledge.

