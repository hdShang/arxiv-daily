---
layout: default
title: Self-Augmented Visual Contrastive Decoding
---

# Self-Augmented Visual Contrastive Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13315" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13315v1</a>
  <a href="https://arxiv.org/pdf/2510.13315.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13315v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13315v1', 'Self-Augmented Visual Contrastive Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eun Woo Im, Muhammad Kashif Ali, Vivek Gupta

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªå¢å¼ºè§†è§‰å¯¹æ¯”è§£ç ï¼Œæå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„äº‹å®ä¸€è‡´æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¯¹æ¯”è§£ç ` `è‡ªå¢å¼º` `äº‹å®ä¸€è‡´æ€§` `å¹»è§‰æŠ‘åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰å¯¹æ¯”è§£ç æ–¹æ³•é‡‡ç”¨é€šç”¨è§†è§‰å¢å¼ºï¼Œå¿½ç•¥äº†æ–‡æœ¬æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ•ˆæœå—é™ã€‚
2. æå‡ºè‡ªå¢å¼ºæç¤ºç­–ç•¥ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«çŸ¥è¯†åŠ¨æ€å¯¹é½æŸ¥è¯¢å’Œè§†è§‰å¢å¼ºçš„è¯­ä¹‰ã€‚
3. æå‡ºè‡ªé€‚åº”é˜ˆå€¼ç®—æ³•ï¼Œæ ¹æ®è¾“å‡ºç¨€ç–æ€§è°ƒæ•´tokenå€™é€‰å¤§å°ï¼Œå……åˆ†åˆ©ç”¨logitåˆ†å¸ƒä¿¡æ¯ï¼Œæå‡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)å±•ç¤ºäº†å“è¶Šçš„å¤šæ¨¡æ€èƒ½åŠ›ï¼Œä½†ä¹Ÿç»§æ‰¿äº†åº•å±‚è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ã€‚è™½ç„¶è§†è§‰å¯¹æ¯”è§£ç å·²è¢«æå‡ºç”¨äºç¼“è§£æ­¤é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸åº”ç”¨é€šç”¨çš„è§†è§‰å¢å¼ºï¼Œå¿½ç•¥äº†æ–‡æœ¬æŸ¥è¯¢æä¾›çš„ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…è®­ç»ƒè§£ç ç­–ç•¥ï¼Œé€šè¿‡ä¸¤ä¸ªå…³é”®è´¡çŒ®è§£å†³äº†è¿™äº›é™åˆ¶ã€‚é¦–å…ˆï¼Œä¸€ç§è‡ªå¢å¼ºæç¤ºç­–ç•¥ï¼Œåˆ©ç”¨æ¨¡å‹å†…åœ¨çŸ¥è¯†æ¥åŠ¨æ€å¯¹é½æŸ¥è¯¢å’Œè§†è§‰å¢å¼ºä¹‹é—´çš„è¯­ä¹‰ã€‚å…¶æ¬¡ï¼Œä¸€ç§è‡ªé€‚åº”é˜ˆå€¼ç®—æ³•ï¼ŒåŸºäºè¾“å‡ºç¨€ç–æ€§è‡ªé€‚åº”åœ°è°ƒæ•´ä¸‹ä¸€ä¸ªtokenå€™é€‰å¤§å°ï¼Œä»è€Œåˆ©ç”¨æ¥è‡ªlogitåˆ†å¸ƒçš„å®Œæ•´ä¿¡æ¯ã€‚åœ¨å››ä¸ªLVLMå’Œä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„è§£ç æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„è§£ç æ˜¾è‘—æé«˜äº†äº‹å®ä¸€è‡´æ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†æ•´åˆæŸ¥è¯¢ç›¸å…³çš„å¢å¼ºå’Œç†µæ„ŸçŸ¥è§£ç å¯¹äºæ”¹å–„LVLMçš„æœ‰æ•ˆç”Ÿæˆçš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå³ç”Ÿæˆä¸äº‹å®ä¸ç¬¦çš„å†…å®¹ã€‚ç°æœ‰çš„è§†è§‰å¯¹æ¯”è§£ç æ–¹æ³•è¯•å›¾é€šè¿‡è§†è§‰å¢å¼ºæ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸ä½¿ç”¨ä¸æ–‡æœ¬æŸ¥è¯¢æ— å…³çš„é€šç”¨è§†è§‰å¢å¼ºï¼Œæ— æ³•æœ‰æ•ˆåœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´æå‡æ•ˆæœæœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„çŸ¥è¯†æ¥ç”Ÿæˆä¸æ–‡æœ¬æŸ¥è¯¢ç›¸å…³çš„è§†è§‰å¢å¼ºï¼Œå¹¶ç»“åˆç†µæ„ŸçŸ¥è§£ç ç­–ç•¥ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶å¹»è§‰ã€‚é€šè¿‡è®©æ¨¡å‹è‡ªå·±ç”Ÿæˆå¢å¼ºï¼Œå¯ä»¥ç¡®ä¿å¢å¼ºåçš„å›¾åƒä¸åŸå§‹å›¾åƒåœ¨è¯­ä¹‰ä¸Šä¿æŒä¸€è‡´ï¼Œä»è€Œæé«˜å¯¹æ¯”å­¦ä¹ çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1) è‡ªå¢å¼ºæç¤ºé˜¶æ®µï¼šåˆ©ç”¨æ¨¡å‹çš„å†…åœ¨çŸ¥è¯†ï¼Œæ ¹æ®æ–‡æœ¬æŸ¥è¯¢ç”Ÿæˆç›¸åº”çš„è§†è§‰å¢å¼ºã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ç‰¹å®šçš„promptï¼Œè®©æ¨¡å‹ç”Ÿæˆå¯¹å›¾åƒçš„æè¿°æˆ–è§£é‡Šï¼Œç„¶ååˆ©ç”¨è¿™äº›æè¿°æˆ–è§£é‡Šæ¥æŒ‡å¯¼è§†è§‰å¢å¼ºã€‚2) è‡ªé€‚åº”é˜ˆå€¼è§£ç é˜¶æ®µï¼šæ ¹æ®æ¨¡å‹è¾“å‡ºçš„ç¨€ç–æ€§ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´ä¸‹ä¸€ä¸ªtokenå€™é€‰çš„å¤§å°ã€‚å¦‚æœæ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæ¯”è¾ƒé›†ä¸­ï¼ˆå³ç¨€ç–æ€§è¾ƒä½ï¼‰ï¼Œåˆ™é€‰æ‹©è¾ƒå°çš„å€™é€‰é›†ï¼›åä¹‹ï¼Œå¦‚æœæ¦‚ç‡åˆ†å¸ƒæ¯”è¾ƒåˆ†æ•£ï¼ˆå³ç¨€ç–æ€§è¾ƒé«˜ï¼‰ï¼Œåˆ™é€‰æ‹©è¾ƒå¤§çš„å€™é€‰é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºï¼š1) æå‡ºäº†è‡ªå¢å¼ºæç¤ºç­–ç•¥ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æŸ¥è¯¢ç›¸å…³çš„è§†è§‰å¢å¼ºï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚2) æå‡ºäº†è‡ªé€‚åº”é˜ˆå€¼è§£ç ç®—æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨¡å‹è¾“å‡ºçš„ç¨€ç–æ€§åŠ¨æ€è°ƒæ•´tokenå€™é€‰é›†çš„å¤§å°ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè‡ªå¢å¼ºæç¤ºç­–ç•¥çš„å…³é”®åœ¨äºpromptçš„è®¾è®¡ï¼Œéœ€è¦èƒ½å¤Ÿå¼•å¯¼æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬æŸ¥è¯¢ç›¸å…³çš„å›¾åƒæè¿°æˆ–è§£é‡Šã€‚è‡ªé€‚åº”é˜ˆå€¼è§£ç ç®—æ³•çš„å…³é”®åœ¨äºé˜ˆå€¼çš„é€‰æ‹©ï¼Œéœ€è¦èƒ½å¤Ÿå¹³è¡¡ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ã€‚å…·ä½“å®ç°ç»†èŠ‚ï¼ˆå¦‚promptçš„å…·ä½“å½¢å¼ã€é˜ˆå€¼çš„è®¡ç®—æ–¹æ³•ç­‰ï¼‰åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªLVLMå’ŒåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æé«˜äº†äº‹å®ä¸€è‡´æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºæœ€å…ˆè¿›çš„è§£ç æ–¹æ³•ï¼Œäº‹å®ä¸€è‡´æ€§æå‡äº†5%ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æŠ‘åˆ¶LVLMçš„å¹»è§‰é—®é¢˜ï¼Œæé«˜ç”Ÿæˆè´¨é‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦é«˜äº‹å®ä¸€è‡´æ€§çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒé—®ç­”ã€è§†è§‰å¯¹è¯ã€å›¾åƒæè¿°ç­‰ã€‚é€šè¿‡æé«˜LVLMçš„äº‹å®ä¸€è‡´æ€§ï¼Œå¯ä»¥ä½¿å…¶åœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®é¢†åŸŸå¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æå‡ºçš„è‡ªå¢å¼ºæ€æƒ³ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal capabilities, but they inherit the tendency to hallucinate from their underlying language models. While visual contrastive decoding has been proposed to mitigate this issue, existing methods often apply generic visual augmentations that disregard the specific context provided by the text query, limiting their effectiveness. This study introduces a novel training-free decoding strategy that addresses these limitations, featuring two key contributions. First, a self-augmentation prompting strategy that leverages the intrinsic knowledge of the model to dynamically align semantics between the query and the visual augmentation. Second, an adaptive thresholding algorithm that adaptively adjusts next token candidate size based on the output sparsity, utilizing full information from the logit distribution. Extensive experiments across four LVLMs and seven benchmarks demonstrate that the proposed decoding significantly enhances factual consistency compared to state-of-the-art decoding methods. This work highlights the importance of integrating query-dependent augmentation and entropy-aware decoding for improving effective generation of LVLMs.

