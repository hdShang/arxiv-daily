---
layout: default
title: What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging
---

# What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13232" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13232v1</a>
  <a href="https://arxiv.org/pdf/2510.13232.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13232v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13232v1', 'What &quot;Not&quot; to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Inha Kang, Youngsun Lim, Seonho Lee, Jiho Choi, Junsuk Choe, Hyunjung Shim

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**å¤‡æ³¨**: 38 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNegToMeæ¨¡å—å’ŒCoVANDæ•°æ®é›†ï¼Œæå‡VLMåœ¨å¦å®šæè¿°å¯¹è±¡æ£€æµ‹ä¸­çš„æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `å¦å®šç†è§£` `å¯¹è±¡æ£€æµ‹` `æ€ç»´é“¾` `LoRAå¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMåœ¨å¦å®šç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æè¿°å¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå®¹æ˜“äº§ç”Ÿè‚¯å®šåè§ã€‚
2. æå‡ºNegToMeæ¨¡å—ï¼Œé€šè¿‡åˆå¹¶tokençš„æ–¹å¼ä¿ç•™å¦å®šè¯­ä¹‰ï¼Œå¹¶ç»“åˆLoRAå¾®è°ƒï¼Œæå‡æ¨¡å‹å¯¹å¦å®šçš„ç†è§£èƒ½åŠ›ã€‚
3. åœ¨CoVANDæ•°æ®é›†å’ŒOVDEvalåŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒNMS-APæå‡é«˜è¾¾+10.8ä¸ªç‚¹ï¼Œå¹¶å…·å¤‡æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)åœ¨ç†è§£å¦å®šæ¦‚å¿µæ—¶å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œå³æ‰€è°“çš„è‚¯å®šåè§ï¼Œåœ¨æè¿°å¯¹è±¡æ£€æµ‹(DOD)ä»»åŠ¡ä¸­å°¤ä¸ºçªå‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼š(1)ä¸€ä¸ªæ–°çš„æ•°æ®é›†æ„å»ºæµç¨‹ï¼›(2)ä¸€ç§æ–°é¢–ä¸”è½»é‡çº§çš„æ¨¡å‹é€‚é…æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†CoVANDï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç³»ç»Ÿçš„æ€ç»´é“¾(CoT)å’ŒåŸºäºVQAçš„æµç¨‹æ„å»ºçš„æ•°æ®é›†ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ã€å®ä¾‹çº§åˆ«çš„å¦å®šæ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†NegToMeï¼Œä¸€ç§æ–°é¢–çš„æ–‡æœ¬tokenåˆå¹¶æ¨¡å—ï¼Œå®ƒç›´æ¥è§£å†³äº†è‚¯å®šåè§çš„æ¶æ„åŸå› ã€‚NegToMeä»æ ¹æœ¬ä¸Šè§£å†³äº†tokenåŒ–è¿‡ç¨‹ä¸­å¦å®šçº¿ç´¢çš„ç»“æ„æ€§ä¸¢å¤±é—®é¢˜ï¼Œå°†å®ƒä»¬ä¸å±æ€§ç»„åˆæˆè¿è´¯çš„è¯­ä¹‰çŸ­è¯­ã€‚å®ƒåœ¨è¾“å…¥å±‚é¢ä¿æŒäº†æ­£ç¡®çš„ææ€§ï¼Œå³ä½¿åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°é²æ£’çš„å¦å®šç†è§£ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†é˜²æ­¢æ¨¡å‹å°†åˆ†æ•£çš„tokenâ€œnotâ€å’Œâ€œgirlâ€ç®€å•åœ°è§†ä¸ºâ€œgirlâ€ï¼ŒNegToMeå°†å®ƒä»¬ç»‘å®šåˆ°ä¸€ä¸ªtokenä¸­ï¼Œå…¶å«ä¹‰ä¸å•ç‹¬çš„â€œgirlâ€æœ‰æ˜æ˜¾åŒºåˆ«ã€‚è¯¥æ¨¡å—ä¸å‚æ•°é«˜æ•ˆä¸”å…·æœ‰ç­–ç•¥æ€§çš„LoRAå¾®è°ƒæ–¹æ³•ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„å¦å®šåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œé™ä½äº†å‡é˜³æ€§ç‡ï¼Œåœ¨OVDEvalä¸Šå°†NMS-APæé«˜äº†é«˜è¾¾+10.8ä¸ªç‚¹ï¼Œå¹¶å±•ç¤ºäº†å¯¹SoTA VLMçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€åœ¨è§£å†³ç°å®ä¸–ç•Œæ£€æµ‹åº”ç”¨ä¸­çš„å¦å®šç†è§£æ–¹é¢è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤„ç†å¦å®šæè¿°å¯¹è±¡æ£€æµ‹ï¼ˆDODï¼‰ä»»åŠ¡æ—¶ï¼Œå­˜åœ¨ä¸¥é‡çš„â€œè‚¯å®šåè§â€ï¼Œå³æ— æ³•å‡†ç¡®ç†è§£å’ŒåŒºåˆ†è‚¯å®šä¸å¦å®šçš„æè¿°ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹å¯èƒ½å°†â€œæ²¡æœ‰æˆ´å¸½å­çš„å¥³å­©â€é”™è¯¯åœ°è¯†åˆ«ä¸ºâ€œå¥³å­©â€ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºtokenåŒ–è¿‡ç¨‹ä¼šç ´åå¦å®šè¯çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åœ¨tokenåŒ–ä¹‹åï¼Œå°†å¦å®šè¯ä¸å…¶ä¿®é¥°çš„å±æ€§è¯åˆå¹¶æˆä¸€ä¸ªtokenï¼Œä»è€Œä¿ç•™å¦å®šè¯­ä¹‰ã€‚è¿™æ ·ï¼Œæ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ—¶ï¼Œå°±èƒ½å°†â€œnot girlâ€ä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œç†è§£ï¼Œè€Œä¸æ˜¯å°†å…¶æ‹†åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„tokenã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨ä»æ ¹æœ¬ä¸Šè§£å†³ç”±äºtokenåŒ–å¯¼è‡´çš„å¦å®šä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šCoVANDæ•°æ®é›†çš„æ„å»ºå’ŒNegToMeæ¨¡å—çš„é›†æˆã€‚CoVANDæ•°æ®é›†é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰å’ŒVQAæ–¹æ³•ç”Ÿæˆé«˜è´¨é‡çš„å¦å®šæè¿°æ•°æ®ã€‚NegToMeæ¨¡å—åˆ™åœ¨VLMçš„æ–‡æœ¬ç¼–ç å™¨ä¸­æ’å…¥ï¼Œç”¨äºåˆå¹¶å¦å®šè¯å’Œå±æ€§è¯çš„tokenã€‚ä¹‹åï¼Œä½¿ç”¨LoRAï¼ˆLow-Rank Adaptationï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚æ•´ä½“æµç¨‹ä¸ºï¼šè¾“å…¥å›¾åƒå’Œæ–‡æœ¬æè¿° -> æ–‡æœ¬tokenåŒ– -> NegToMeæ¨¡å—è¿›è¡Œtokenåˆå¹¶ -> VLMè¿›è¡Œè§†è§‰-è¯­è¨€èåˆ -> è¾“å‡ºæ£€æµ‹ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯NegToMeæ¨¡å—ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒNegToMeç›´æ¥åœ¨tokençº§åˆ«æ“ä½œï¼Œé€šè¿‡åˆå¹¶tokençš„æ–¹å¼æ˜¾å¼åœ°ä¿ç•™å¦å®šè¯­ä¹‰ã€‚è¿™ç§æ–¹æ³•é¿å…äº†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­éšå¼åœ°å­¦ä¹ å¦å®šå…³ç³»ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒCoVANDæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºå¦å®šç†è§£çš„ç ”ç©¶æä¾›äº†é«˜è´¨é‡çš„æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šNegToMeæ¨¡å—çš„å…³é”®è®¾è®¡åœ¨äºå¦‚ä½•ç¡®å®šå“ªäº›tokenéœ€è¦åˆå¹¶ã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œä¾‹å¦‚å°†â€œnotâ€ä¸å…¶åé¢çš„åè¯æˆ–å½¢å®¹è¯åˆå¹¶ã€‚LoRAå¾®è°ƒé‡‡ç”¨äº†ä¸€ç§ç­–ç•¥æ€§çš„æ–¹æ³•ï¼Œåªå¾®è°ƒä¸NegToMeæ¨¡å—ç›¸å…³çš„å‚æ•°ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNegToMeæ¨¡å—åœ¨OVDEvalåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒNMS-APæé«˜äº†é«˜è¾¾+10.8ä¸ªç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜é™ä½äº†å‡é˜³æ€§ç‡ï¼Œè¡¨æ˜æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«å¦å®šæè¿°çš„å¯¹è±¡ã€‚å®éªŒè¿˜è¯æ˜äº†è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„SoTA VLMã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§ã€å›¾åƒæœç´¢ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œæ¨¡å‹éœ€è¦å‡†ç¡®è¯†åˆ«â€œæ²¡æœ‰è¡Œäººçš„æ–‘é©¬çº¿â€ï¼›åœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œéœ€è¦è¯†åˆ«â€œæ²¡æœ‰æºå¸¦æ­¦å™¨çš„äººå‘˜â€ã€‚è¯¥ç ”ç©¶æå‡äº†VLMåœ¨è¿™äº›åœºæ™¯ä¸‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ½œåœ¨çš„ç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> State-of-the-art vision-language models (VLMs) suffer from a critical failure in understanding negation, often referred to as affirmative bias. This limitation is particularly severe in described object detection (DOD) tasks. To address this, we propose two primary contributions: (1) a new dataset pipeline and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a dataset constructed with a systematic chain-of-thought (CoT) and VQA-based pipeline to generate high-quality, instance-grounded negation data. Second, we propose NegToMe, a novel text token merging module that directly tackles the architectural cause of affirmative bias. NegToMe fundamentally addresses the structural loss of negation cues in tokenization, grouping them with attributes into coherent semantic phrases. It maintains correct polarity at the input level, enabling robust negation understanding even with limited data. For instance, to prevent a model from treating the fragmented tokens "not" and "girl" as simply "girl", NegToMe binds them into a single token whose meaning is correctly distinguished from that of "girl" alone. This module is integrated with a parameter-efficient and strategic LoRA fine-tuning approach. Our method significantly improves performance on challenging negation benchmarks with a lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval and demonstrating generalization to SoTA VLMs. This work marks a crucial step forward in addressing negation understanding for real-world detection applications.

