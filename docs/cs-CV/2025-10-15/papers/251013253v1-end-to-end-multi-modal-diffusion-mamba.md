---
layout: default
title: End-to-End Multi-Modal Diffusion Mamba
---

# End-to-End Multi-Modal Diffusion Mamba

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13253" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13253v1</a>
  <a href="https://arxiv.org/pdf/2510.13253.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13253v1" onclick="toggleFavorite(this, '2510.13253v1', 'End-to-End Multi-Modal Diffusion Mamba')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chunhao Lu, Qiang Lu, Meichen Dong, Jake Luo

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**å¤‡æ³¨**: Accepted by ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€æ‰©æ•£Mambaï¼ˆMDMï¼‰ï¼Œç”¨äºç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†å¹¶æå‡ç”Ÿæˆæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `Mamba` `ç«¯åˆ°ç«¯æ¨¡å‹` `å˜åˆ†è‡ªç¼–ç å™¨` `å›¾åƒç”Ÿæˆ` `è§†è§‰é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ¨¡å‹ä¾èµ–åˆ†ç¦»çš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œé˜»ç¢äº†è·¨æ¨¡æ€çš„è”åˆè¡¨ç¤ºå­¦ä¹ ã€‚
2. MDMåˆ©ç”¨åŸºäºMambaçš„å¤šæ­¥é€‰æ‹©æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„å˜åˆ†è‡ªç¼–ç å™¨å®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„ç”Ÿæˆå’Œä¼˜åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMDMåœ¨å›¾åƒç”Ÿæˆã€è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå¹¶ä¸SOTAæ¨¡å‹ç«äº‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šæ¨¡æ€æ‰©æ•£Mambaï¼ˆMDMï¼‰çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†ã€‚MDMé‡‡ç”¨åŸºäºMambaçš„å¤šæ­¥é€‰æ‹©æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„å˜åˆ†è‡ªç¼–ç å™¨è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œä»è€Œé€æ­¥ç”Ÿæˆå’Œä¼˜åŒ–ç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•ä½¿MDMåœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨åŒæ—¶ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒå’Œæ‰©å±•æ–‡æœ¬åºåˆ—æ–¹é¢ã€‚åœ¨å›¾åƒç”Ÿæˆã€å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€æ–‡æœ¬ç†è§£å’Œæ¨ç†ä»»åŠ¡ç­‰é¢†åŸŸçš„è¯„ä¼°è¡¨æ˜ï¼ŒMDMæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼ˆå¦‚MonoFormerã€LlamaGenå’ŒChameleonç­‰ï¼‰ï¼Œå¹¶èƒ½ä¸GPT-4Vã€Gemini Proå’ŒMistralç­‰SOTAæ¨¡å‹æœ‰æ•ˆç«äº‰ã€‚å®éªŒç»“æœéªŒè¯äº†MDMåœ¨ç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œä¸ºç«¯åˆ°ç«¯å¤šæ¨¡æ€æ¶æ„å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸é‡‡ç”¨åˆ†ç¦»çš„ç¼–ç å™¨å’Œè§£ç å™¨æ¥å¤„ç†ä¸åŒæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºä¿¡æ¯ã€‚è¿™ç§åˆ†ç¦»çš„è®¾è®¡é˜»ç¢äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„è”åˆè¡¨ç¤ºå­¦ä¹ ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨å¤„ç†é«˜ç»´æ•°æ®ï¼Œå¦‚é«˜åˆ†è¾¨ç‡å›¾åƒå’Œé•¿æ–‡æœ¬åºåˆ—æ—¶ï¼Œè¿™ç§é—®é¢˜æ›´åŠ çªå‡ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMDMçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¶æ„æ¥å¤„ç†æ‰€æœ‰æ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œä¿ƒè¿›è·¨æ¨¡æ€çš„çŸ¥è¯†å…±äº«å’Œèåˆã€‚å…·ä½“æ¥è¯´ï¼ŒMDMé‡‡ç”¨åŸºäºMambaçš„å¤šæ­¥é€‰æ‹©æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç»Ÿä¸€çš„å˜åˆ†è‡ªç¼–ç å™¨è¿›è¡Œç¼–ç å’Œè§£ç ã€‚é€šè¿‡æ‰©æ•£è¿‡ç¨‹ï¼Œæ¨¡å‹èƒ½å¤Ÿé€æ­¥ç”Ÿæˆå’Œä¼˜åŒ–ç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMDMçš„æ•´ä½“æ¶æ„åŒ…å«ä¸€ä¸ªç»Ÿä¸€çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œä¸€ä¸ªåŸºäºMambaçš„å¤šæ­¥é€‰æ‹©æ‰©æ•£æ¨¡å‹ã€‚VAEè´Ÿè´£å°†ä¸åŒæ¨¡æ€çš„è¾“å…¥ç¼–ç åˆ°ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œè€Œæ‰©æ•£æ¨¡å‹åˆ™è´Ÿè´£ä»æ½œåœ¨ç©ºé—´ä¸­é€æ­¥ç”Ÿæˆç›®æ ‡æ¨¡æ€çš„ä¿¡æ¯ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬ç¼–ç é˜¶æ®µå’Œè§£ç ï¼ˆæ‰©æ•£ï¼‰é˜¶æ®µã€‚åœ¨ç¼–ç é˜¶æ®µï¼Œä¸åŒæ¨¡æ€çš„è¾“å…¥é€šè¿‡VAEç¼–ç åˆ°æ½œåœ¨ç©ºé—´ã€‚åœ¨è§£ç é˜¶æ®µï¼Œæ‰©æ•£æ¨¡å‹ä»æ½œåœ¨ç©ºé—´å‡ºå‘ï¼Œé€æ­¥ç”Ÿæˆç›®æ ‡æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡Mambaç»“æ„è¿›è¡Œåºåˆ—å»ºæ¨¡å’Œé€‰æ‹©ã€‚

**å…³é”®åˆ›æ–°**ï¼šMDMçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨åŸºäºMambaçš„æ‰©æ•£æ¨¡å‹æ¥ç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†ã€‚Mambaç»“æ„å…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†é•¿åºåˆ—æ•°æ®ï¼Œè¿™ä½¿å¾—MDMåœ¨å¤„ç†é«˜ç»´å¤šæ¨¡æ€æ•°æ®æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ä¸ä¼ ç»Ÿçš„Transformerç»“æ„ç›¸æ¯”ï¼ŒMambaç»“æ„åœ¨è®¡ç®—æ•ˆç‡å’Œå»ºæ¨¡èƒ½åŠ›ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼ŒMDMé‡‡ç”¨ç»Ÿä¸€çš„VAEè¿›è¡Œç¼–ç å’Œè§£ç ï¼Œè¿›ä¸€æ­¥ä¿ƒè¿›äº†è·¨æ¨¡æ€çš„çŸ¥è¯†å…±äº«ã€‚

**å…³é”®è®¾è®¡**ï¼šMDMçš„å…³é”®è®¾è®¡åŒ…æ‹¬Mambaå—çš„å…·ä½“é…ç½®ã€æ‰©æ•£æ¨¡å‹çš„å™ªå£°è°ƒåº¦ç­–ç•¥ä»¥åŠVAEçš„ç»“æ„è®¾è®¡ã€‚è®ºæ–‡å¯èƒ½è¯¦ç»†æè¿°äº†Mambaå—ä¸­é€‰æ‹©æœºåˆ¶çš„å…·ä½“å®ç°æ–¹å¼ï¼Œä¾‹å¦‚é€‰æ‹©é—¨çš„æ¿€æ´»å‡½æ•°å’Œå‚æ•°åˆå§‹åŒ–æ–¹æ³•ã€‚æ‰©æ•£æ¨¡å‹çš„å™ªå£°è°ƒåº¦ç­–ç•¥å†³å®šäº†ç”Ÿæˆè¿‡ç¨‹çš„è´¨é‡å’Œé€Ÿåº¦ï¼Œå¯èƒ½é‡‡ç”¨äº†çº¿æ€§æˆ–éçº¿æ€§çš„å™ªå£°æ·»åŠ æ–¹å¼ã€‚VAEçš„ç»“æ„è®¾è®¡ï¼ŒåŒ…æ‹¬ç¼–ç å™¨å’Œè§£ç å™¨çš„å±‚æ•°ã€æ¿€æ´»å‡½æ•°å’Œæ®‹å·®è¿æ¥ç­‰ï¼Œä¹Ÿä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMDMåœ¨å›¾åƒç”Ÿæˆã€å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€æ–‡æœ¬ç†è§£å’Œæ¨ç†ä»»åŠ¡ç­‰å¤šä¸ªé¢†åŸŸå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒMDMç”Ÿæˆçš„å›¾åƒè´¨é‡ä¼˜äºç°æœ‰çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼ŒMDMçš„å‡†ç¡®ç‡ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œç”šè‡³å¯ä»¥ä¸GPT-4Vã€Gemini Proå’ŒMistralç­‰å¤§å‹æ¨¡å‹ç«äº‰ã€‚è¿™äº›ç»“æœéªŒè¯äº†MDMåœ¨ç»Ÿä¸€å¤šæ¨¡æ€å¤„ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MDMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆï¼ˆå¦‚æ ¹æ®æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼‰ã€è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€ä»¥åŠè·¨æ¨¡æ€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„äººæœºäº¤äº’ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚æœªæ¥ï¼ŒMDMæœ‰æœ›åº”ç”¨äºåŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œå®ç°æ›´ç²¾å‡†ã€æ›´å¯é çš„å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current end-to-end multi-modal models utilize different encoders and decoders to process input and output information. This separation hinders the joint representation learning of various modalities. To unify multi-modal processing, we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM utilizes a Mamba-based multi-step selection diffusion model to progressively generate and refine modality-specific information through a unified variational autoencoder for both encoding and decoding. This innovative approach allows MDM to achieve superior performance when processing high-dimensional data, particularly in generating high-resolution images and extended text sequences simultaneously. Our evaluations in areas such as image generation, image captioning, visual question answering, text comprehension, and reasoning tasks demonstrate that MDM significantly outperforms existing end-to-end models (MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's effectiveness in unifying multi-modal processes while maintaining computational efficiency, establishing a new direction for end-to-end multi-modal architectures.

