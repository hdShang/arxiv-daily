---
layout: default
title: Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity
---

# Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.13364" target="_blank" class="toolbar-btn">arXiv: 2510.13364v1</a>
    <a href="https://arxiv.org/pdf/2510.13364.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13364v1" 
            onclick="toggleFavorite(this, '2510.13364v1', 'Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: MingZe Tang, Jubal Chandy Jacob

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Âà©Áî®ËØ≠Ë®ÄÊ†áÁ≠æËøõË°åÈõ∂Ê†∑Êú¨Â§öÊ®°ÊÄÅÂàÜÁ±ªÔºåËß£ÂÜ≥Êï∞ÊçÆÁ®ÄÁº∫‰∏ãÁöÑÊó•Â∏∏ÂßøÊÄÅËØÜÂà´ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `Â§öÊ®°ÊÄÅÂàÜÁ±ª` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `ÊèêÁ§∫ËØçÂ∑•Á®ã` `‰∫∫‰ΩìÂßøÊÄÅËØÜÂà´`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Êï∞ÊçÆÁ®ÄÁº∫Âú∫ÊôØ‰∏ãÔºåÈöæ‰ª•ÊúâÊïàÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËøõË°åÁªÜÁ≤íÂ∫¶ÂßøÊÄÅËØÜÂà´ÔºåÊèêÁ§∫ËØçËÆæËÆ°ÁöÑÂΩ±ÂìçÂ∞ö‰∏çÊòéÁ°Æ„ÄÇ
2. ËÆ∫ÊñáÊé¢Á¥¢‰∫Ü‰∏çÂêåËØ¶ÁªÜÁ®ãÂ∫¶ÁöÑÊèêÁ§∫ËØçÂØπÈõ∂Ê†∑Êú¨ÂßøÊÄÅÂàÜÁ±ªÁöÑÂΩ±ÂìçÔºåÊó®Âú®ÊâæÂà∞ÊúÄ‰Ω≥ÁöÑÊèêÁ§∫ËØçËÆæËÆ°Á≠ñÁï•„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂØπ‰∫éÈ´òÊÄßËÉΩÊ®°ÂûãÔºåÁÆÄÂçïÊèêÁ§∫ËØç‰ºò‰∫éÂ§çÊùÇÊèêÁ§∫ËØçÔºåÂπ∂ÊèêÂá∫‰∫Ü‚ÄúÊèêÁ§∫ËØçËøáÊãüÂêà‚ÄùÁé∞Ë±°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂú®Êï∞ÊçÆÁ®ÄÁº∫Êù°‰ª∂‰∏ãÔºåÂ¶Ç‰ΩïÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËøõË°åÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÔºåÁâπÂà´ÂÖ≥Ê≥®‰∫ÜÊèêÁ§∫ËØçËÆæËÆ°ÂØπËØÜÂà´ËßÜËßâÁõ∏‰ººÁ±ªÂà´ÔºàÂ¶Ç‰∫∫‰ΩìÂßøÊÄÅÔºâÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂‰ΩøÁî®‰∏Ä‰∏™Áî±285Âº†COCOÂõæÂÉèË°çÁîüÁöÑÊï∞ÊçÆÈõÜÔºåËØÑ‰º∞‰∫ÜOpenCLIP„ÄÅMetaCLIP 2ÂíåSigLipÁ≠âÊ®°ÂûãÂú®Âùê„ÄÅÁ´ô„ÄÅËµ∞/Ë∑ë‰∏âÁßçÂßøÊÄÅÂàÜÁ±ª‰∏äÁöÑË°®Áé∞„ÄÇÈÄöËøáÁ≥ªÁªüÂú∞Â¢ûÂä†ËØ≠Ë®ÄÁªÜËäÇÁöÑ‰∏âÂ±ÇÊèêÁ§∫ËØçËÆæËÆ°ÔºåÂèëÁé∞‰∫Ü‰∏Ä‰∏™ËøùÂèçÁõ¥ËßâÁöÑË∂ãÂäøÔºöÂØπ‰∫éMetaCLIP 2ÂíåOpenCLIPÁ≠âÈ´òÊÄßËÉΩÊ®°ÂûãÔºåÊúÄÁÆÄÂçï„ÄÅÊúÄÂü∫Á°ÄÁöÑÊèêÁ§∫ËØçÊïàÊûúÊúÄ‰Ω≥„ÄÇÊ∑ªÂä†ÊèèËø∞ÊÄßÁªÜËäÇ‰ºöÊòæËëóÈôç‰ΩéÊÄßËÉΩÔºå‰æãÂ¶ÇMetaCLIP 2ÁöÑÂ§öÁ±ªÁ≤æÂ∫¶‰ªé68.8%ÈôçËá≥55.1%ÔºåËøôÁßçÁé∞Ë±°Ë¢´Áß∞‰∏∫‚ÄúÊèêÁ§∫ËØçËøáÊãüÂêà‚Äù„ÄÇÁõ∏ÂèçÔºåÂØπ‰∫éÊÄßËÉΩËæÉ‰ΩéÁöÑSigLipÊ®°ÂûãÔºåÊõ¥ÂÖ∑ÊèèËø∞ÊÄßÁöÑ„ÄÅÂü∫‰∫éË∫´‰ΩìÁ∫øÁ¥¢ÁöÑÊèêÁ§∫ËØçÂèØ‰ª•ÊîπÂñÑÊ®°Á≥äÁ±ªÂà´ÁöÑÂàÜÁ±ª„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êï∞ÊçÆÁ®ÄÁº∫Âú∫ÊôØ‰∏ãÔºåÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËøõË°å‰∫∫‰ΩìÊó•Â∏∏ÂßøÊÄÅÔºàÂùê„ÄÅÁ´ô„ÄÅËµ∞/Ë∑ëÔºâÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÊèêÁ§∫ËØçËÆæËÆ°‰∏äÁº∫‰πèÁ≥ªÁªüÊÄßÁ†îÁ©∂ÔºåÈöæ‰ª•ÂÖÖÂàÜÂèëÊå•ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜËßâÁõ∏‰ººÁ±ªÂà´ÁöÑÂå∫ÂàÜ‰∏äË°®Áé∞‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁ≥ªÁªüÊÄßÂú∞ËÆæËÆ°‰∏çÂêåËØ¶ÁªÜÁ®ãÂ∫¶ÁöÑÊèêÁ§∫ËØçÔºåÊé¢Á©∂ÊèêÁ§∫ËØçÁöÑÁâπÂºÇÊÄßÂØπÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇ‰ΩúËÄÖÂÅáËÆæÔºåÊõ¥ËØ¶ÁªÜÁöÑÊèêÁ§∫ËØçËÉΩÂ§üÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇ‰ΩÜÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂØπ‰∫éÈ´òÊÄßËÉΩÊ®°ÂûãÔºåÁÆÄÂçïÁöÑÊèêÁ§∫ËØçÂèçËÄåÊõ¥ÊúâÊïàÔºåËøôË°®ÊòéÂèØËÉΩÂ≠òÂú®‚ÄúÊèêÁ§∫ËØçËøáÊãüÂêà‚ÄùÁé∞Ë±°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÈááÁî®‰∫Ü‰∏Ä‰∏™‰∏âÂ±ÇÊèêÁ§∫ËØçËÆæËÆ°Ê°ÜÊû∂ÔºåÈÄêÊ≠•Â¢ûÂä†ÊèêÁ§∫ËØçÁöÑËØ≠Ë®ÄÁªÜËäÇ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÁ¨¨‰∏ÄÂ±Ç‰ΩøÁî®ÊúÄÁÆÄÂçïÁöÑÊèêÁ§∫ËØçÔºàÂ¶Ç‚Äúsitting‚ÄùÔºâÔºåÁ¨¨‰∫åÂ±ÇÂ¢ûÂä†‰∏Ä‰∫õÊèèËø∞ÊÄßËØçËØ≠ÔºàÂ¶Ç‚Äúa person sitting‚ÄùÔºâÔºåÁ¨¨‰∏âÂ±ÇÂàôÂåÖÂê´Êõ¥ËØ¶ÁªÜÁöÑË∫´‰ΩìÁ∫øÁ¥¢ÔºàÂ¶Ç‚Äúa person sitting with their legs bent‚ÄùÔºâ„ÄÇÁÑ∂ÂêéÔºå‰ΩøÁî®Ëøô‰∫õÊèêÁ§∫ËØçÂØπOpenCLIP„ÄÅMetaCLIP 2ÂíåSigLipÁ≠âËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËøõË°åÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÔºåÂπ∂ÊØîËæÉ‰∏çÂêåÊèêÁ§∫ËØç‰∏ãÁöÑÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂèëÁé∞‰∫Ü‚ÄúÊèêÁ§∫ËØçËøáÊãüÂêà‚ÄùÁé∞Ë±°ÔºåÂç≥ÂØπ‰∫éÈ´òÊÄßËÉΩËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÊõ¥ËØ¶ÁªÜÁöÑÊèêÁ§∫ËØçÂèçËÄå‰ºöÈôç‰ΩéÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇËøô‰∏éÁõ¥ËßâÁõ∏ÂèçÔºåË°®ÊòéÂú®Èõ∂Ê†∑Êú¨ÂàÜÁ±ª‰∏≠ÔºåÊèêÁ§∫ËØçÁöÑËÆæËÆ°ÈúÄË¶Å‰ªîÁªÜÊùÉË°°ÔºåÈÅøÂÖçËøáÂ∫¶ÊãüÂêàËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑÂÅèÂ∑Æ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰∏âÂ±ÇÊèêÁ§∫ËØçËÆæËÆ°ÔºåÁ≥ªÁªüÊÄßÂú∞ÊéßÂà∂ÊèêÁ§∫ËØçÁöÑËØ¶ÁªÜÁ®ãÂ∫¶Ôºõ2) ‰ΩøÁî®COCOÊï∞ÊçÆÈõÜË°çÁîüÁöÑ‰∏Ä‰∏™Â∞èËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÊ®°ÊãüÊï∞ÊçÆÁ®ÄÁº∫Âú∫ÊôØÔºõ3) ËØÑ‰º∞Â§ö‰∏™ÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÂåÖÊã¨OpenCLIP„ÄÅMetaCLIP 2ÂíåSigLipÔºõ4) ÈááÁî®Â§öÁ±ªÁ≤æÂ∫¶‰Ωú‰∏∫ËØÑ‰º∞ÊåáÊ†áÔºåË°°ÈáèÂàÜÁ±ªÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂØπ‰∫éMetaCLIP 2ÂíåOpenCLIPÁ≠âÈ´òÊÄßËÉΩÊ®°ÂûãÔºåÊúÄÁÆÄÂçïÁöÑÊèêÁ§∫ËØçÂèñÂæó‰∫ÜÊúÄ‰Ω≥ÁöÑÂàÜÁ±ªÁ≤æÂ∫¶„ÄÇ‰æãÂ¶ÇÔºåMetaCLIP 2Âú®‰ΩøÁî®ÊúÄÁÆÄÂçïÊèêÁ§∫ËØçÊó∂ÁöÑÂ§öÁ±ªÁ≤æÂ∫¶‰∏∫68.8%ÔºåËÄå‰ΩøÁî®ÊúÄËØ¶ÁªÜÊèêÁ§∫ËØçÊó∂ÂàôÈôçËá≥55.1%„ÄÇÁõ∏ÂèçÔºåÂØπ‰∫éSigLipÊ®°ÂûãÔºåÊõ¥ËØ¶ÁªÜÁöÑÊèêÁ§∫ËØçÂèØ‰ª•ÊèêÈ´òÂàÜÁ±ªÁ≤æÂ∫¶ÔºåËøôË°®Êòé‰∏çÂêåÊ®°ÂûãÁöÑÊèêÁ§∫ËØçËÆæËÆ°Á≠ñÁï•ÂèØËÉΩ‰∏çÂêå„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅÂ∫∑Â§çËÆ≠ÁªÉÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êô∫ËÉΩÁõëÊéß‰∏≠ÔºåÂèØ‰ª•Âà©Áî®Èõ∂Ê†∑Êú¨ÂàÜÁ±ªÊäÄÊúØËØÜÂà´ÂºÇÂ∏∏ÂßøÊÄÅÔºåÊèêÈ´òÂÆâÂÖ®È¢ÑË≠¶ËÉΩÂäõ„ÄÇÂú®‰∫∫Êú∫‰∫§‰∫í‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑÂßøÊÄÅËøõË°å‰∏™ÊÄßÂåñÊúçÂä°„ÄÇÂú®Â∫∑Â§çËÆ≠ÁªÉ‰∏≠ÔºåÂèØ‰ª•ËæÖÂä©ËØÑ‰º∞ÊÇ£ËÄÖÁöÑÂ∫∑Â§çËøõÂ±ï„ÄÇÊú™Êù•ÁöÑÁ†îÁ©∂ÂèØ‰ª•Êé¢Á¥¢Êõ¥ÊúâÊïàÁöÑÊèêÁ§∫ËØçËÆæËÆ°ÊñπÊ≥ïÔºåÊèêÈ´òÈõ∂Ê†∑Êú¨ÂàÜÁ±ªÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent Vision-Language Models (VLMs) enable zero-shot classification by aligning images and text in a shared space, a promising approach for data-scarce conditions. However, the influence of prompt design on recognizing visually similar categories, such as human postures, is not well understood. This study investigates how prompt specificity affects the zero-shot classification of sitting, standing, and walking/running on a small, 285-image COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2, and SigLip, were evaluated using a three-tiered prompt design that systematically increases linguistic detail. Our findings reveal a compelling, counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and OpenCLIP), the simplest, most basic prompts consistently achieve the best results. Adding descriptive detail significantly degrades performance for instance, MetaCLIP 2's multi-class accuracy drops from 68.8\% to 55.1\% a phenomenon we term "prompt overfitting". Conversely, the lower-performing SigLip model shows improved classification on ambiguous classes when given more descriptive, body-cue-based prompts.

