---
layout: default
title: Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs
---

# Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.13795" target="_blank" class="toolbar-btn">arXiv: 2510.13795v3</a>
    <a href="https://arxiv.org/pdf/2510.13795.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13795v3" 
            onclick="toggleFavorite(this, '2510.13795v3', 'Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15 (Êõ¥Êñ∞: 2025-11-11)

**Â§áÊ≥®**: homepage: https://open-bee.github.io/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Honey-Data-15MÊï∞ÊçÆÈõÜÂíåBee-8BÊ®°ÂûãÔºåÊèêÂçáÂÖ®ÂºÄÊ∫êÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ÂºÄÊ∫êÊï∞ÊçÆÈõÜ` `Êï∞ÊçÆÊ∏ÖÊ¥ó` `ÊÄùÁª¥Èìæ` `ÁõëÁù£ÂæÆË∞É` `Êï∞ÊçÆÂ¢ûÂº∫` `Ê®°ÂûãËÆ≠ÁªÉ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂºÄÊ∫êÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞ÊçÆË¥®ÈáèÂíåÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂ÊÄßËÉΩÊèêÂçá„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Honey-Data-15MÊï∞ÊçÆÈõÜÔºåÈÄöËøáÊ∏ÖÊ¥óÂíåÂèåÂ±ÇCoTÂ¢ûÂº∫ÔºåÊèêÂçáÊï∞ÊçÆË¥®ÈáèÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇ
3. ËÆ≠ÁªÉÁöÑBee-8BÊ®°ÂûãÂú®ÂÖ®ÂºÄÊ∫êMLLM‰∏≠ËææÂà∞Êñ∞ÁöÑSOTAÔºåÊÄßËÉΩÂèØ‰∏éÂçäÂºÄÊ∫êÊ®°ÂûãÂ™≤Áæé„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂΩìÂâçÂÖ®ÂºÄÊ∫êÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLM)ËêΩÂêé‰∫é‰∏ìÊúâÊ®°ÂûãÔºå‰∏ªË¶ÅÂéüÂõ†ÊòØÁõëÁù£ÂæÆË∞É(SFT)ÁöÑÊï∞ÊçÆË¥®ÈáèÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇÁé∞ÊúâÂºÄÊ∫êÊï∞ÊçÆÈõÜÈÄöÂ∏∏Â≠òÂú®Â§ßÈáèÂô™Â£∞Ôºå‰∏îÁº∫‰πèÂ§çÊùÇÁöÑÊé®ÁêÜÊï∞ÊçÆÔºåÂ¶ÇÊÄùÁª¥Èìæ(CoT)ÔºåËøôÈòªÁ¢ç‰∫ÜÈ´òÁ∫ßÊ®°ÂûãËÉΩÂäõÁöÑÂºÄÂèë„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊú¨ÊñáÂÅöÂá∫‰∫Ü‰∏â‰∏™‰∏ªË¶ÅË¥°ÁåÆ„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜHoney-Data-15MÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´Á∫¶1500‰∏á‰∏™QAÂØπÁöÑÊñ∞SFTÊï∞ÊçÆÈõÜÔºåÈÄöËøáÂ§öÁßçÊ∏ÖÊ¥óÊäÄÊúØÂ§ÑÁêÜÔºåÂπ∂ÈááÁî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåÂ±Ç(Áü≠ÂíåÈïø)CoTÂ¢ûÂº∫Á≠ñÁï•„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨‰ªãÁªç‰∫ÜHoneyPipeÊï∞ÊçÆÁÆ°ÁêÜÊµÅÁ®ãÂèäÂÖ∂Â∫ïÂ±ÇÊ°ÜÊû∂DataStudioÔºå‰∏∫Á§æÂå∫Êèê‰æõ‰∫Ü‰∏ÄÁßçÈÄèÊòé‰∏îÈÄÇÂ∫îÊÄßÂº∫ÁöÑÊï∞ÊçÆÁÆ°ÁêÜÊñπÊ≥ïÔºåË∂ÖË∂ä‰∫ÜÈùôÊÄÅÊï∞ÊçÆÈõÜÁöÑÂèëÂ∏É„ÄÇÊúÄÂêéÔºå‰∏∫‰∫ÜÈ™åËØÅÊàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜÂíåÊµÅÁ®ãÔºåÊàë‰ª¨Âú®Honey-Data-15M‰∏äËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™8BÊ®°ÂûãBee-8B„ÄÇÂÆûÈ™åË°®ÊòéÔºåBee-8B‰∏∫ÂÖ®ÂºÄÊ∫êMLLMÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥(SOTA)ÔºåÂÖ∂ÊÄßËÉΩ‰∏éÊúÄËøëÁöÑÂçäÂºÄÊ∫êÊ®°Âûã(Â¶ÇInternVL3.5-8B)Áõ∏ÊØîÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÁîöËá≥Ë∂ÖËøá‰∫ÜÂÆÉ‰ª¨„ÄÇÊàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏∫Á§æÂå∫Êèê‰æõ‰∫Ü‰∏ÄÂ•óÂü∫Á°ÄËµÑÊ∫êÔºåÂåÖÊã¨ÔºöHoney-Data-15MËØ≠ÊñôÂ∫ìÔºõÂåÖÂê´HoneyPipeÂíåDataStudioÁöÑÂÖ®Ê†àÂ•ó‰ª∂ÔºõËÆ≠ÁªÉÈÖçÊñπÔºõËØÑ‰º∞Â∑•ÂÖ∑Ôºõ‰ª•ÂèäÊ®°ÂûãÊùÉÈáç„ÄÇËøôÈ°πÂ∑•‰ΩúË°®ÊòéÔºåÊúâÂéüÂàôÂú∞ÂÖ≥Ê≥®Êï∞ÊçÆË¥®ÈáèÊòØÂºÄÂèë‰∏éÂçäÂºÄÊ∫êÊ®°ÂûãÂÖ∑ÊúâÈ´òÂ∫¶Á´û‰∫âÂäõÁöÑÂÖ®ÂºÄÊ∫êMLLMÁöÑÂÖ≥ÈîÆÈÄîÂæÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂºÄÊ∫êÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLM)ÁöÑÊÄßËÉΩËêΩÂêé‰∫éÈó≠Ê∫êÊ®°ÂûãÔºå‰∏ªË¶ÅÁì∂È¢àÂú®‰∫éÈ´òË¥®ÈáèÁöÑÁõëÁù£ÂæÆË∞É(SFT)Êï∞ÊçÆÂåÆ‰πè„ÄÇÁé∞ÊúâÂºÄÊ∫êÊï∞ÊçÆÈõÜÂ≠òÂú®Â§ßÈáèÂô™Â£∞Ôºå‰∏îÁº∫‰πèÂ§çÊùÇÁöÑÊé®ÁêÜÊï∞ÊçÆÔºå‰æãÂ¶ÇÊÄùÁª¥Èìæ(Chain-of-Thought, CoT)Êï∞ÊçÆÔºåËøô‰∏•ÈáçÈòªÁ¢ç‰∫ÜÊ®°ÂûãÂ≠¶‰π†Â§çÊùÇÊé®ÁêÜËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™È´òË¥®Èáè„ÄÅÂ§ßËßÑÊ®°ÁöÑSFTÊï∞ÊçÆÈõÜÊù•ÊèêÂçáÂÖ®ÂºÄÊ∫êMLLMÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈÄöËøáÂ§öËΩÆÊï∞ÊçÆÊ∏ÖÊ¥ó„ÄÅËøáÊª§Âô™Â£∞Êï∞ÊçÆÔºåÂπ∂ÈááÁî®ÂèåÂ±ÇCoTÂ¢ûÂº∫Á≠ñÁï•ÔºåÁîüÊàêÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÊï∞ÊçÆÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÊï∞ÊçÆÁÆ°ÁêÜÊµÅÁ®ãHoneyPipeÔºåÂÖ∂Â∫ïÂ±ÇÊ°ÜÊû∂ÊòØDataStudio„ÄÇËØ•ÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅÊï∞ÊçÆÊ∏ÖÊ¥ó„ÄÅÊï∞ÊçÆÂ¢ûÂº∫ÔºàÁâπÂà´ÊòØÂèåÂ±ÇCoTÂ¢ûÂº∫ÔºâÂíåÊï∞ÊçÆÈ™åËØÅÁ≠âÂ§ö‰∏™Èò∂ÊÆµ„ÄÇÊúÄÁªàÔºå‰ΩøÁî®Honey-Data-15MÊï∞ÊçÆÈõÜËÆ≠ÁªÉBee-8BÊ®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) Honey-Data-15MÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜÁªèËøá‰∏•Ê†ºÊ∏ÖÊ¥óÂíåÂèåÂ±ÇCoTÂ¢ûÂº∫ÔºåÂÖ∑ÊúâÈ´òË¥®ÈáèÂíåÂ§ßËßÑÊ®°ÁöÑÁâπÁÇπ„ÄÇ2) HoneyPipeÊï∞ÊçÆÁÆ°ÁêÜÊµÅÁ®ãÂíåDataStudioÊ°ÜÊû∂ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÈÄèÊòé‰∏îÂèØÂÆöÂà∂ÁöÑÊï∞ÊçÆÁÆ°ÁêÜÊñπÊ≥ï„ÄÇ3) ÂèåÂ±ÇCoTÂ¢ûÂº∫Á≠ñÁï•ÔºåÂêåÊó∂ÁîüÊàêÁü≠CoTÂíåÈïøCoTÊï∞ÊçÆÔºåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂèåÂ±ÇCoTÂ¢ûÂº∫Á≠ñÁï•ÊòØÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏Ä„ÄÇÁü≠CoT‰æßÈáç‰∫éÁõ¥Êé•Êé®ÁêÜÊ≠•È™§ÔºåËÄåÈïøCoTÂàôÂåÖÂê´Êõ¥ËØ¶ÁªÜÁöÑËß£ÈáäÂíåËÉåÊôØÁü•ËØÜ„ÄÇËøôÁßçÂèåÂ±ÇÁªìÊûÑÊó®Âú®Â∏ÆÂä©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ≠¶‰π†Â§çÊùÇÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåÊï∞ÊçÆÊ∏ÖÊ¥óËøáÁ®ãÈááÁî®‰∫ÜÂ§öÁßçËøáÊª§ËßÑÂàôÂíåÂêØÂèëÂºèÊñπÊ≥ïÔºå‰ª•ÂéªÈô§Âô™Â£∞Êï∞ÊçÆÂπ∂‰øùÁïôÈ´òË¥®ÈáèÁöÑÊ†∑Êú¨„ÄÇÂÖ∑‰ΩìÁöÑÊ®°ÂûãËÆ≠ÁªÉÂèÇÊï∞ÂíåÊçüÂ§±ÂáΩÊï∞Á≠âÁªÜËäÇÊú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Honey-Data-15MÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁöÑBee-8BÊ®°ÂûãÂú®ÂÖ®ÂºÄÊ∫êMLLM‰∏≠ÂèñÂæó‰∫ÜÊñ∞ÁöÑSOTAÔºåÂÖ∂ÊÄßËÉΩ‰∏éÂçäÂºÄÊ∫êÊ®°ÂûãInternVL3.5-8BÁõ∏ÊØîÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÁîöËá≥Ë∂ÖËøá‰∫ÜÂêéËÄÖ„ÄÇËøôÈ™åËØÅ‰∫ÜÈ´òË¥®ÈáèÊï∞ÊçÆÂØπ‰∫éÊèêÂçáÂÖ®ÂºÄÊ∫êMLLMÊÄßËÉΩÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊô∫ËÉΩÈóÆÁ≠î„ÄÅÂõæÂÉèÁêÜËß£„ÄÅËßÜËßâÊé®ÁêÜÁ≠âÈ¢ÜÂüü„ÄÇÈ´òË¥®ÈáèÁöÑÂºÄÊ∫êÊï∞ÊçÆÈõÜÂíåÊ®°ÂûãÊúâÂä©‰∫éÊé®Âä®ÂÖ®ÂºÄÊ∫êÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèëÂ±ïÔºåÈôç‰ΩéÁ†îÁ©∂Èó®ÊßõÔºå‰øÉËøõÁõ∏ÂÖ≥ÊäÄÊúØÁöÑÊôÆÂèäÂíåÂ∫îÁî®„ÄÇÊú™Êù•ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êé¢Á¥¢Êõ¥ÊúâÊïàÁöÑCoTÁîüÊàêÊñπÊ≥ïÂíåÊõ¥È´òÊïàÁöÑÊ®°ÂûãËÆ≠ÁªÉÁ≠ñÁï•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.

