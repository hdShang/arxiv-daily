---
layout: default
title: Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs
---

# Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13795" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13795v3</a>
  <a href="https://arxiv.org/pdf/2510.13795.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13795v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13795v3', 'Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15 (æ›´æ–°: 2025-11-11)

**å¤‡æ³¨**: homepage: https://open-bee.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHoney-Data-15Mæ•°æ®é›†å’ŒBee-8Bæ¨¡å‹ï¼Œæå‡å…¨å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¼€æºæ•°æ®é›†` `æ•°æ®æ¸…æ´—` `æ€ç»´é“¾` `ç›‘ç£å¾®è°ƒ` `æ•°æ®å¢å¼º` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°æ®è´¨é‡å’Œå¤æ‚æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½æå‡ã€‚
2. è®ºæ–‡æå‡ºHoney-Data-15Mæ•°æ®é›†ï¼Œé€šè¿‡æ¸…æ´—å’ŒåŒå±‚CoTå¢å¼ºï¼Œæå‡æ•°æ®è´¨é‡å’Œæ¨ç†èƒ½åŠ›ã€‚
3. è®­ç»ƒçš„Bee-8Bæ¨¡å‹åœ¨å…¨å¼€æºMLLMä¸­è¾¾åˆ°æ–°çš„SOTAï¼Œæ€§èƒ½å¯ä¸åŠå¼€æºæ¨¡å‹åª²ç¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰å…¨å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)è½åäºä¸“æœ‰æ¨¡å‹ï¼Œä¸»è¦åŸå› æ˜¯ç›‘ç£å¾®è°ƒ(SFT)çš„æ•°æ®è´¨é‡å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ç°æœ‰å¼€æºæ•°æ®é›†é€šå¸¸å­˜åœ¨å¤§é‡å™ªå£°ï¼Œä¸”ç¼ºä¹å¤æ‚çš„æ¨ç†æ•°æ®ï¼Œå¦‚æ€ç»´é“¾(CoT)ï¼Œè¿™é˜»ç¢äº†é«˜çº§æ¨¡å‹èƒ½åŠ›çš„å¼€å‘ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡åšå‡ºäº†ä¸‰ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†Honey-Data-15Mï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«çº¦1500ä¸‡ä¸ªQAå¯¹çš„æ–°SFTæ•°æ®é›†ï¼Œé€šè¿‡å¤šç§æ¸…æ´—æŠ€æœ¯å¤„ç†ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„åŒå±‚(çŸ­å’Œé•¿)CoTå¢å¼ºç­–ç•¥ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä»‹ç»äº†HoneyPipeæ•°æ®ç®¡ç†æµç¨‹åŠå…¶åº•å±‚æ¡†æ¶DataStudioï¼Œä¸ºç¤¾åŒºæä¾›äº†ä¸€ç§é€æ˜ä¸”é€‚åº”æ€§å¼ºçš„æ•°æ®ç®¡ç†æ–¹æ³•ï¼Œè¶…è¶Šäº†é™æ€æ•°æ®é›†çš„å‘å¸ƒã€‚æœ€åï¼Œä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ•°æ®é›†å’Œæµç¨‹ï¼Œæˆ‘ä»¬åœ¨Honey-Data-15Mä¸Šè®­ç»ƒäº†ä¸€ä¸ª8Bæ¨¡å‹Bee-8Bã€‚å®éªŒè¡¨æ˜ï¼ŒBee-8Bä¸ºå…¨å¼€æºMLLMå»ºç«‹äº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›æ°´å¹³(SOTA)ï¼Œå…¶æ€§èƒ½ä¸æœ€è¿‘çš„åŠå¼€æºæ¨¡å‹(å¦‚InternVL3.5-8B)ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†å®ƒä»¬ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºç¤¾åŒºæä¾›äº†ä¸€å¥—åŸºç¡€èµ„æºï¼ŒåŒ…æ‹¬ï¼šHoney-Data-15Mè¯­æ–™åº“ï¼›åŒ…å«HoneyPipeå’ŒDataStudioçš„å…¨æ ˆå¥—ä»¶ï¼›è®­ç»ƒé…æ–¹ï¼›è¯„ä¼°å·¥å…·ï¼›ä»¥åŠæ¨¡å‹æƒé‡ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œæœ‰åŸåˆ™åœ°å…³æ³¨æ•°æ®è´¨é‡æ˜¯å¼€å‘ä¸åŠå¼€æºæ¨¡å‹å…·æœ‰é«˜åº¦ç«äº‰åŠ›çš„å…¨å¼€æºMLLMçš„å…³é”®é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)çš„æ€§èƒ½è½åäºé—­æºæ¨¡å‹ï¼Œä¸»è¦ç“¶é¢ˆåœ¨äºé«˜è´¨é‡çš„ç›‘ç£å¾®è°ƒ(SFT)æ•°æ®åŒ®ä¹ã€‚ç°æœ‰å¼€æºæ•°æ®é›†å­˜åœ¨å¤§é‡å™ªå£°ï¼Œä¸”ç¼ºä¹å¤æ‚çš„æ¨ç†æ•°æ®ï¼Œä¾‹å¦‚æ€ç»´é“¾(Chain-of-Thought, CoT)æ•°æ®ï¼Œè¿™ä¸¥é‡é˜»ç¢äº†æ¨¡å‹å­¦ä¹ å¤æ‚æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªé«˜è´¨é‡ã€å¤§è§„æ¨¡çš„SFTæ•°æ®é›†æ¥æå‡å…¨å¼€æºMLLMçš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡å¤šè½®æ•°æ®æ¸…æ´—ã€è¿‡æ»¤å™ªå£°æ•°æ®ï¼Œå¹¶é‡‡ç”¨åŒå±‚CoTå¢å¼ºç­–ç•¥ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æ¨ç†æ•°æ®ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„æ•°æ®ç®¡ç†æµç¨‹HoneyPipeï¼Œå…¶åº•å±‚æ¡†æ¶æ˜¯DataStudioã€‚è¯¥æµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ•°æ®æ¸…æ´—ã€æ•°æ®å¢å¼ºï¼ˆç‰¹åˆ«æ˜¯åŒå±‚CoTå¢å¼ºï¼‰å’Œæ•°æ®éªŒè¯ç­‰å¤šä¸ªé˜¶æ®µã€‚æœ€ç»ˆï¼Œä½¿ç”¨Honey-Data-15Mæ•°æ®é›†è®­ç»ƒBee-8Bæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) Honey-Data-15Mæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç»è¿‡ä¸¥æ ¼æ¸…æ´—å’ŒåŒå±‚CoTå¢å¼ºï¼Œå…·æœ‰é«˜è´¨é‡å’Œå¤§è§„æ¨¡çš„ç‰¹ç‚¹ã€‚2) HoneyPipeæ•°æ®ç®¡ç†æµç¨‹å’ŒDataStudioæ¡†æ¶ï¼Œæä¾›äº†ä¸€ç§é€æ˜ä¸”å¯å®šåˆ¶çš„æ•°æ®ç®¡ç†æ–¹æ³•ã€‚3) åŒå±‚CoTå¢å¼ºç­–ç•¥ï¼ŒåŒæ—¶ç”ŸæˆçŸ­CoTå’Œé•¿CoTæ•°æ®ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåŒå±‚CoTå¢å¼ºç­–ç•¥æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚çŸ­CoTä¾§é‡äºç›´æ¥æ¨ç†æ­¥éª¤ï¼Œè€Œé•¿CoTåˆ™åŒ…å«æ›´è¯¦ç»†çš„è§£é‡Šå’ŒèƒŒæ™¯çŸ¥è¯†ã€‚è¿™ç§åŒå±‚ç»“æ„æ—¨åœ¨å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œå­¦ä¹ å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ•°æ®æ¸…æ´—è¿‡ç¨‹é‡‡ç”¨äº†å¤šç§è¿‡æ»¤è§„åˆ™å’Œå¯å‘å¼æ–¹æ³•ï¼Œä»¥å»é™¤å™ªå£°æ•°æ®å¹¶ä¿ç•™é«˜è´¨é‡çš„æ ·æœ¬ã€‚å…·ä½“çš„æ¨¡å‹è®­ç»ƒå‚æ•°å’ŒæŸå¤±å‡½æ•°ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Honey-Data-15Mæ•°æ®é›†ä¸Šè®­ç»ƒçš„Bee-8Bæ¨¡å‹åœ¨å…¨å¼€æºMLLMä¸­å–å¾—äº†æ–°çš„SOTAï¼Œå…¶æ€§èƒ½ä¸åŠå¼€æºæ¨¡å‹InternVL3.5-8Bç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†åè€…ã€‚è¿™éªŒè¯äº†é«˜è´¨é‡æ•°æ®å¯¹äºæå‡å…¨å¼€æºMLLMæ€§èƒ½çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½é—®ç­”ã€å›¾åƒç†è§£ã€è§†è§‰æ¨ç†ç­‰é¢†åŸŸã€‚é«˜è´¨é‡çš„å¼€æºæ•°æ®é›†å’Œæ¨¡å‹æœ‰åŠ©äºæ¨åŠ¨å…¨å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œé™ä½ç ”ç©¶é—¨æ§›ï¼Œä¿ƒè¿›ç›¸å…³æŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ›´æœ‰æ•ˆçš„CoTç”Ÿæˆæ–¹æ³•å’Œæ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒç­–ç•¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.

