---
layout: default
title: EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking
---

# EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.13235" target="_blank" class="toolbar-btn">arXiv: 2510.13235v1</a>
    <a href="https://arxiv.org/pdf/2510.13235.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13235v1" 
            onclick="toggleFavorite(this, '2510.13235v1', 'EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yukuan Zhang, Jiarui Zhao, Shangqing Nie, Jin Kuang, Shengsheng Wang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**EPIPTrackÔºöÂà©Áî®ÊòæÂºèÂíåÈöêÂºèÊèêÁ§∫ËøõË°åÂ§öÁõÆÊ†áË∑üË∏™ÁöÑÊèêÁ§∫Âª∫Ê®°Êñ∞ÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÁõÆÊ†áË∑üË∏™` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ÊèêÁ§∫Â≠¶‰π†` `Âä®ÊÄÅÂª∫Ê®°` `ËØ≠‰πâÂØπÈΩê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ï‰æùËµñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÈùôÊÄÅÊñáÊú¨ÊèèËø∞ÔºåÁº∫‰πèÂØπÁõÆÊ†áÁä∂ÊÄÅÂèòÂåñÁöÑÈÄÇÂ∫îÊÄßÔºå‰∏îÊòì‰∫ßÁîüÂπªËßâ„ÄÇ
2. EPIPTrackÂà©Áî®ÊòæÂºèÊèêÁ§∫ÔºàÊó∂Á©∫‰ø°ÊÅØÔºâÂíåÈöêÂºèÊèêÁ§∫ÔºàÂ§ñËßÇÂ±ûÊÄßÔºâËøõË°åÂä®ÊÄÅÁõÆÊ†áÂª∫Ê®°ÂíåËØ≠‰πâÂØπÈΩê„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåEPIPTrackÂú®MOT17„ÄÅMOT20ÂíåDanceTrackÁ≠âÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâË∑üË∏™Âô®ÔºåÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅËßÜËßâ-ËØ≠Ë®ÄË∑üË∏™Ê°ÜÊû∂EPIPTrackÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ï‰æùËµñÈùôÊÄÅÊñáÊú¨ÊèèËø∞„ÄÅÁº∫‰πèÂØπÂÆûÊó∂ÁõÆÊ†áÁä∂ÊÄÅÂèòÂåñÁöÑÈÄÇÂ∫îÊÄß‰ª•ÂèäÂÆπÊòì‰∫ßÁîüÂπªËßâÁöÑÈóÆÈ¢ò„ÄÇEPIPTrackÂà©Áî®ÊòæÂºèÂíåÈöêÂºèÊèêÁ§∫ËøõË°åÂä®ÊÄÅÁõÆÊ†áÂª∫Ê®°ÂíåËØ≠‰πâÂØπÈΩê„ÄÇÊòæÂºèÊèêÁ§∫Â∞ÜÁ©∫Èó¥ËøêÂä®‰ø°ÊÅØËΩ¨Êç¢‰∏∫Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÔºåÊèê‰æõÊó∂Á©∫ÊåáÂØº„ÄÇÈöêÂºèÊèêÁ§∫Â∞Ü‰º™ËØç‰∏éÂèØÂ≠¶‰π†ÁöÑÊèèËø∞Á¨¶Áõ∏ÁªìÂêàÔºåÊûÑÂª∫ÊçïËé∑Â§ñËßÇÂ±ûÊÄßÁöÑ‰∏™ÊÄßÂåñÁü•ËØÜË°®Á§∫„ÄÇ‰∏§ÁßçÊèêÁ§∫ÈÉΩÈÄöËøáCLIPÊñáÊú¨ÁºñÁ†ÅÂô®ËøõË°åÂä®ÊÄÅË∞ÉÊï¥Ôºå‰ª•ÂìçÂ∫îÁõÆÊ†áÁä∂ÊÄÅÁöÑÂèòÂåñ„ÄÇÊ≠§Â§ñÔºåËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™Âà§Âà´ÁâπÂæÅÂ¢ûÂº∫Âô®Êù•Â¢ûÂº∫ËßÜËßâÂíåË∑®Ê®°ÊÄÅË°®Á§∫„ÄÇÂú®MOT17„ÄÅMOT20ÂíåDanceTrack‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåEPIPTrackÂú®ÂêÑÁßçÂú∫ÊôØ‰∏≠‰ºò‰∫éÁé∞ÊúâÁöÑË∑üË∏™Âô®ÔºåË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈÄÇÂ∫îÊÄßÂíåÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§öÁõÆÊ†áË∑üË∏™ÊñπÊ≥ïÂú®Âà©Áî®ÊñáÊú¨ÊèèËø∞Á≠âÂ§öÊ®°ÊÄÅËØ≠‰πâ‰ø°ÊÅØÊó∂Ôºå‰∏ªË¶Å‰æùËµñ‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁöÑÈùôÊÄÅÊñáÊú¨ÊèèËø∞„ÄÇËøôÁßçÊñπÊ≥ïÊó†Ê≥ïÈÄÇÂ∫îÁõÆÊ†áÁä∂ÊÄÅÁöÑÂÆûÊó∂ÂèòÂåñÔºåÂπ∂‰∏îÂÆπÊòì‰∫ßÁîüÂπªËßâÔºå‰ªéËÄåÂΩ±ÂìçË∑üË∏™ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöEPIPTrackÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÊòæÂºèÂíåÈöêÂºèÊèêÁ§∫ÔºåÂä®ÊÄÅÂú∞Âª∫Ê®°ÁõÆÊ†áÁöÑÁä∂ÊÄÅÂπ∂ËøõË°åËØ≠‰πâÂØπÈΩê„ÄÇÊòæÂºèÊèêÁ§∫ÈÄöËøáÂ∞ÜÁ©∫Èó¥ËøêÂä®‰ø°ÊÅØËΩ¨Âåñ‰∏∫Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÔºåÊèê‰æõÊó∂Á©∫ÊåáÂØºÔºõÈöêÂºèÊèêÁ§∫ÂàôÈÄöËøáÁªìÂêà‰º™ËØçÂíåÂèØÂ≠¶‰π†ÊèèËø∞Á¨¶ÔºåÊûÑÂª∫‰∏™ÊÄßÂåñÁöÑÁü•ËØÜË°®Á§∫ÔºåÊçïÊçâÁõÆÊ†áÁöÑÂ§ñËßÇÂ±ûÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEPIPTrackÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÊòæÂºèÊèêÁ§∫ÁîüÊàêÊ®°ÂùóÔºåÂ∞ÜÁõÆÊ†áÁöÑËøêÂä®‰ø°ÊÅØËΩ¨Âåñ‰∏∫Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Ôºõ2) ÈöêÂºèÊèêÁ§∫ÁîüÊàêÊ®°ÂùóÔºåÂà©Áî®‰º™ËØçÂíåÂèØÂ≠¶‰π†ÊèèËø∞Á¨¶ÊûÑÂª∫ÁõÆÊ†áÁöÑÂ§ñËßÇË°®Á§∫Ôºõ3) CLIPÊñáÊú¨ÁºñÁ†ÅÂô®ÔºåÁî®‰∫éÂä®ÊÄÅË∞ÉÊï¥ÊòæÂºèÂíåÈöêÂºèÊèêÁ§∫Ôºå‰ª•ÈÄÇÂ∫îÁõÆÊ†áÁä∂ÊÄÅÁöÑÂèòÂåñÔºõ4) Âà§Âà´ÁâπÂæÅÂ¢ûÂº∫Âô®ÔºåÁî®‰∫éÂ¢ûÂº∫ËßÜËßâÂíåË∑®Ê®°ÊÄÅÁâπÂæÅË°®Á§∫„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºåÈ¶ñÂÖàÊèêÂèñÁõÆÊ†áÁöÑËßÜËßâÁâπÂæÅÔºåÁÑ∂ÂêéÁîüÊàêÊòæÂºèÂíåÈöêÂºèÊèêÁ§∫ÔºåÈÄöËøáCLIPÊñáÊú¨ÁºñÁ†ÅÂô®ËøõË°åËûçÂêàÂíåË∞ÉÊï¥ÔºåÊúÄÂêéÂà©Áî®Âà§Âà´ÁâπÂæÅÂ¢ûÂº∫Âô®Â¢ûÂº∫ÁâπÂæÅË°®Á§∫ÔºåÁî®‰∫éÁõÆÊ†áÁöÑË∑üË∏™ÂíåËØÜÂà´„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöEPIPTrackÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂêåÊó∂Âà©Áî®ÊòæÂºèÂíåÈöêÂºèÊèêÁ§∫ËøõË°åÂä®ÊÄÅÁõÆÊ†áÂª∫Ê®°„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåEPIPTrackËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁõÆÊ†áÁä∂ÊÄÅÁöÑÂèòÂåñÔºåÂπ∂‰∏îËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Â§öÊ®°ÊÄÅ‰ø°ÊÅØËøõË°åË∑üË∏™„ÄÇÊòæÂºèÊèêÁ§∫ÂíåÈöêÂºèÊèêÁ§∫ÁöÑÁªìÂêàÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂêåÊó∂ÂÖ≥Ê≥®ÁõÆÊ†áÁöÑÊó∂Á©∫‰ø°ÊÅØÂíåÂ§ñËßÇÂ±ûÊÄßÔºå‰ªéËÄåÊèêÈ´òË∑üË∏™ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊòæÂºèÊèêÁ§∫ÁöÑËÆæËÆ°ÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïÂ∞ÜÁ©∫Èó¥ËøêÂä®‰ø°ÊÅØÊúâÊïàÂú∞ËΩ¨Âåñ‰∏∫Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞„ÄÇÈöêÂºèÊèêÁ§∫ÁöÑËÆæËÆ°ÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïÈÄâÊã©ÂêàÈÄÇÁöÑ‰º™ËØçÂíåÂèØÂ≠¶‰π†ÊèèËø∞Á¨¶Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÂ∞ÜÂÆÉ‰ª¨ÊúâÊïàÂú∞ÁªìÂêàËµ∑Êù•„ÄÇÂà§Âà´ÁâπÂæÅÂ¢ûÂº∫Âô®ÁöÑËÆæËÆ°ÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïÂ¢ûÂº∫ËßÜËßâÂíåË∑®Ê®°ÊÄÅÁâπÂæÅÁöÑÂà§Âà´ÊÄßÔºå‰ªéËÄåÊèêÈ´òË∑üË∏™ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÊäÄÊúØÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EPIPTrackÂú®MOT17„ÄÅMOT20ÂíåDanceTrackÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéEPIPTrackÂú®ÂêÑÈ°πÊåáÊ†á‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÁöÑË∑üË∏™Âô®„ÄÇ‰æãÂ¶ÇÔºåÂú®MOT17Êï∞ÊçÆÈõÜ‰∏äÔºåEPIPTrackÁöÑMOTAÊåáÊ†áÊèêÂçá‰∫ÜX%ÔºåIDF1ÊåáÊ†áÊèêÂçá‰∫ÜY%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåEPIPTrackÂÖ∑ÊúâÂº∫Â§ßÁöÑÈÄÇÂ∫îÊÄßÂíåÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

EPIPTrackÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÊô∫ËÉΩÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈ¢ÜÂüü„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÈ´òÂ§öÁõÆÊ†áË∑üË∏™ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºå‰ªéËÄå‰∏∫Ëøô‰∫õÂ∫îÁî®Êèê‰æõÊõ¥ÂèØÈù†ÁöÑÂü∫Á°Ä„ÄÇÊú™Êù•ÔºåEPIPTrackÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅË∑üË∏™‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇËßÜÈ¢ëÁõÆÊ†áÂàÜÂâ≤„ÄÅËßÜÈ¢ëÊèèËø∞ÁîüÊàêÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal semantic cues, such as textual descriptions, have shown strong potential in enhancing target perception for tracking. However, existing methods rely on static textual descriptions from large language models, which lack adaptability to real-time target state changes and prone to hallucinations. To address these challenges, we propose a unified multimodal vision-language tracking framework, named EPIPTrack, which leverages explicit and implicit prompts for dynamic target modeling and semantic alignment. Specifically, explicit prompts transform spatial motion information into natural language descriptions to provide spatiotemporal guidance. Implicit prompts combine pseudo-words with learnable descriptors to construct individualized knowledge representations capturing appearance attributes. Both prompts undergo dynamic adjustment via the CLIP text encoder to respond to changes in target state. Furthermore, we design a Discriminative Feature Augmentor to enhance visual and cross-modal representations. Extensive experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack outperforms existing trackers in diverse scenarios, exhibiting robust adaptability and superior performance.

