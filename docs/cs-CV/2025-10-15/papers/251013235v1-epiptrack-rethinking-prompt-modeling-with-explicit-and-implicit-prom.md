---
layout: default
title: "EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking"
---

# EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13235" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13235v1</a>
  <a href="https://arxiv.org/pdf/2510.13235.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13235v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13235v1', 'EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yukuan Zhang, Jiarui Zhao, Shangqing Nie, Jin Kuang, Shengsheng Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EPIPTrackï¼šåˆ©ç”¨æ˜¾å¼å’Œéšå¼æç¤ºè¿›è¡Œå¤šç›®æ ‡è·Ÿè¸ªçš„æç¤ºå»ºæ¨¡æ–°æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šç›®æ ‡è·Ÿè¸ª` `è§†è§‰è¯­è¨€æ¨¡å‹` `æç¤ºå­¦ä¹ ` `åŠ¨æ€å»ºæ¨¡` `è¯­ä¹‰å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹çš„é™æ€æ–‡æœ¬æè¿°ï¼Œç¼ºä¹å¯¹ç›®æ ‡çŠ¶æ€å˜åŒ–çš„é€‚åº”æ€§ï¼Œä¸”æ˜“äº§ç”Ÿå¹»è§‰ã€‚
2. EPIPTrackåˆ©ç”¨æ˜¾å¼æç¤ºï¼ˆæ—¶ç©ºä¿¡æ¯ï¼‰å’Œéšå¼æç¤ºï¼ˆå¤–è§‚å±æ€§ï¼‰è¿›è¡ŒåŠ¨æ€ç›®æ ‡å»ºæ¨¡å’Œè¯­ä¹‰å¯¹é½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEPIPTrackåœ¨MOT17ã€MOT20å’ŒDanceTrackç­‰æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰è·Ÿè¸ªå™¨ï¼Œå…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€è§†è§‰-è¯­è¨€è·Ÿè¸ªæ¡†æ¶EPIPTrackï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¾èµ–é™æ€æ–‡æœ¬æè¿°ã€ç¼ºä¹å¯¹å®æ—¶ç›®æ ‡çŠ¶æ€å˜åŒ–çš„é€‚åº”æ€§ä»¥åŠå®¹æ˜“äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ã€‚EPIPTrackåˆ©ç”¨æ˜¾å¼å’Œéšå¼æç¤ºè¿›è¡ŒåŠ¨æ€ç›®æ ‡å»ºæ¨¡å’Œè¯­ä¹‰å¯¹é½ã€‚æ˜¾å¼æç¤ºå°†ç©ºé—´è¿åŠ¨ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œæä¾›æ—¶ç©ºæŒ‡å¯¼ã€‚éšå¼æç¤ºå°†ä¼ªè¯ä¸å¯å­¦ä¹ çš„æè¿°ç¬¦ç›¸ç»“åˆï¼Œæ„å»ºæ•è·å¤–è§‚å±æ€§çš„ä¸ªæ€§åŒ–çŸ¥è¯†è¡¨ç¤ºã€‚ä¸¤ç§æç¤ºéƒ½é€šè¿‡CLIPæ–‡æœ¬ç¼–ç å™¨è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œä»¥å“åº”ç›®æ ‡çŠ¶æ€çš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªåˆ¤åˆ«ç‰¹å¾å¢å¼ºå™¨æ¥å¢å¼ºè§†è§‰å’Œè·¨æ¨¡æ€è¡¨ç¤ºã€‚åœ¨MOT17ã€MOT20å’ŒDanceTrackä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEPIPTrackåœ¨å„ç§åœºæ™¯ä¸­ä¼˜äºç°æœ‰çš„è·Ÿè¸ªå™¨ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§å’Œå“è¶Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šç›®æ ‡è·Ÿè¸ªæ–¹æ³•åœ¨åˆ©ç”¨æ–‡æœ¬æè¿°ç­‰å¤šæ¨¡æ€è¯­ä¹‰ä¿¡æ¯æ—¶ï¼Œä¸»è¦ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„é™æ€æ–‡æœ¬æè¿°ã€‚è¿™ç§æ–¹æ³•æ— æ³•é€‚åº”ç›®æ ‡çŠ¶æ€çš„å®æ—¶å˜åŒ–ï¼Œå¹¶ä¸”å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œä»è€Œå½±å“è·Ÿè¸ªçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEPIPTrackçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ˜¾å¼å’Œéšå¼æç¤ºï¼ŒåŠ¨æ€åœ°å»ºæ¨¡ç›®æ ‡çš„çŠ¶æ€å¹¶è¿›è¡Œè¯­ä¹‰å¯¹é½ã€‚æ˜¾å¼æç¤ºé€šè¿‡å°†ç©ºé—´è¿åŠ¨ä¿¡æ¯è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œæä¾›æ—¶ç©ºæŒ‡å¯¼ï¼›éšå¼æç¤ºåˆ™é€šè¿‡ç»“åˆä¼ªè¯å’Œå¯å­¦ä¹ æè¿°ç¬¦ï¼Œæ„å»ºä¸ªæ€§åŒ–çš„çŸ¥è¯†è¡¨ç¤ºï¼Œæ•æ‰ç›®æ ‡çš„å¤–è§‚å±æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEPIPTrackæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æ˜¾å¼æç¤ºç”Ÿæˆæ¨¡å—ï¼Œå°†ç›®æ ‡çš„è¿åŠ¨ä¿¡æ¯è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼›2) éšå¼æç¤ºç”Ÿæˆæ¨¡å—ï¼Œåˆ©ç”¨ä¼ªè¯å’Œå¯å­¦ä¹ æè¿°ç¬¦æ„å»ºç›®æ ‡çš„å¤–è§‚è¡¨ç¤ºï¼›3) CLIPæ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´æ˜¾å¼å’Œéšå¼æç¤ºï¼Œä»¥é€‚åº”ç›®æ ‡çŠ¶æ€çš„å˜åŒ–ï¼›4) åˆ¤åˆ«ç‰¹å¾å¢å¼ºå™¨ï¼Œç”¨äºå¢å¼ºè§†è§‰å’Œè·¨æ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆæå–ç›®æ ‡çš„è§†è§‰ç‰¹å¾ï¼Œç„¶åç”Ÿæˆæ˜¾å¼å’Œéšå¼æç¤ºï¼Œé€šè¿‡CLIPæ–‡æœ¬ç¼–ç å™¨è¿›è¡Œèåˆå’Œè°ƒæ•´ï¼Œæœ€ååˆ©ç”¨åˆ¤åˆ«ç‰¹å¾å¢å¼ºå™¨å¢å¼ºç‰¹å¾è¡¨ç¤ºï¼Œç”¨äºç›®æ ‡çš„è·Ÿè¸ªå’Œè¯†åˆ«ã€‚

**å…³é”®åˆ›æ–°**ï¼šEPIPTrackçš„å…³é”®åˆ›æ–°åœ¨äºåŒæ—¶åˆ©ç”¨æ˜¾å¼å’Œéšå¼æç¤ºè¿›è¡ŒåŠ¨æ€ç›®æ ‡å»ºæ¨¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEPIPTrackèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç›®æ ‡çŠ¶æ€çš„å˜åŒ–ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œè·Ÿè¸ªã€‚æ˜¾å¼æç¤ºå’Œéšå¼æç¤ºçš„ç»“åˆï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒæ—¶å…³æ³¨ç›®æ ‡çš„æ—¶ç©ºä¿¡æ¯å’Œå¤–è§‚å±æ€§ï¼Œä»è€Œæé«˜è·Ÿè¸ªçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ˜¾å¼æç¤ºçš„è®¾è®¡å…³é”®åœ¨äºå¦‚ä½•å°†ç©ºé—´è¿åŠ¨ä¿¡æ¯æœ‰æ•ˆåœ°è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ã€‚éšå¼æç¤ºçš„è®¾è®¡å…³é”®åœ¨äºå¦‚ä½•é€‰æ‹©åˆé€‚çš„ä¼ªè¯å’Œå¯å­¦ä¹ æè¿°ç¬¦ï¼Œä»¥åŠå¦‚ä½•å°†å®ƒä»¬æœ‰æ•ˆåœ°ç»“åˆèµ·æ¥ã€‚åˆ¤åˆ«ç‰¹å¾å¢å¼ºå™¨çš„è®¾è®¡å…³é”®åœ¨äºå¦‚ä½•å¢å¼ºè§†è§‰å’Œè·¨æ¨¡æ€ç‰¹å¾çš„åˆ¤åˆ«æ€§ï¼Œä»è€Œæé«˜è·Ÿè¸ªçš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EPIPTrackåœ¨MOT17ã€MOT20å’ŒDanceTrackæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜EPIPTrackåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„è·Ÿè¸ªå™¨ã€‚ä¾‹å¦‚ï¼Œåœ¨MOT17æ•°æ®é›†ä¸Šï¼ŒEPIPTrackçš„MOTAæŒ‡æ ‡æå‡äº†X%ï¼ŒIDF1æŒ‡æ ‡æå‡äº†Y%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒEPIPTrackå…·æœ‰å¼ºå¤§çš„é€‚åº”æ€§å’Œå“è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EPIPTrackå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜å¤šç›®æ ‡è·Ÿè¸ªçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œä»è€Œä¸ºè¿™äº›åº”ç”¨æä¾›æ›´å¯é çš„åŸºç¡€ã€‚æœªæ¥ï¼ŒEPIPTrackå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€è·Ÿè¸ªä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†é¢‘ç›®æ ‡åˆ†å‰²ã€è§†é¢‘æè¿°ç”Ÿæˆç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal semantic cues, such as textual descriptions, have shown strong potential in enhancing target perception for tracking. However, existing methods rely on static textual descriptions from large language models, which lack adaptability to real-time target state changes and prone to hallucinations. To address these challenges, we propose a unified multimodal vision-language tracking framework, named EPIPTrack, which leverages explicit and implicit prompts for dynamic target modeling and semantic alignment. Specifically, explicit prompts transform spatial motion information into natural language descriptions to provide spatiotemporal guidance. Implicit prompts combine pseudo-words with learnable descriptors to construct individualized knowledge representations capturing appearance attributes. Both prompts undergo dynamic adjustment via the CLIP text encoder to respond to changes in target state. Furthermore, we design a Discriminative Feature Augmentor to enhance visual and cross-modal representations. Extensive experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack outperforms existing trackers in diverse scenarios, exhibiting robust adaptability and superior performance.

