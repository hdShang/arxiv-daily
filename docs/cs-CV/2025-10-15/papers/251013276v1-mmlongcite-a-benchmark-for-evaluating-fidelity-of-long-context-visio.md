---
layout: default
title: MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models
---

# MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.13276" target="_blank" class="toolbar-btn">arXiv: 2510.13276v1</a>
    <a href="https://arxiv.org/pdf/2510.13276.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13276v1" 
            onclick="toggleFavorite(this, '2510.13276v1', 'MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Keyan Zhou, Zecheng Tang, Lingfeng Ming, Guanghao Zhou, Qiguang Chen, Dan Qiao, Zheming Yang, Libo Qin, Minghui Qiu, Juntao Li, Min Zhang

**ÂàÜÁ±ª**: cs.CV, cs.CL

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MMLongCiteÂü∫ÂáÜÔºåËØÑ‰º∞Èïø‰∏ä‰∏ãÊñáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ø°ÊÅØ‰øùÁúüÂ∫¶**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èïø‰∏ä‰∏ãÊñá` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `‰ø°ÊÅØ‰øùÁúüÂ∫¶` `ËØÑ‰º∞Âü∫ÂáÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLVLMs‰∏ä‰∏ãÊñáÁ™óÂè£ËôΩÂ∑≤Êâ©Â±ïÔºå‰ΩÜÈïø‰∏ä‰∏ãÊñáÂà©Áî®Áéá‰∏çË∂≥ÔºåÂ§öÊ®°ÊÄÅÈïø‰∏ä‰∏ãÊñá‰øùÁúüÂ∫¶ËØÑ‰º∞Áº∫Â§±„ÄÇ
2. ÊèêÂá∫MMLongCiteÂü∫ÂáÜÔºåÂåÖÂê´8‰∏™‰ªªÂä°ÔºåË¶ÜÁõñ6‰∏™‰∏ä‰∏ãÊñáÈïøÂ∫¶Âå∫Èó¥ÔºåÂåÖÂê´ÊñáÊú¨„ÄÅÂõæÂÉèÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçÊ®°ÊÄÅ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÁé∞ÊúâLVLMsÂú®ÈïøÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÂ§ÑÁêÜ‰∏≠‰øùÁúüÂ∫¶ÊúâÈôêÔºåÂπ∂ÂàÜÊûê‰∫Ü‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂíåÂÖ≥ÈîÆÂÜÖÂÆπ‰ΩçÁΩÆÁöÑÂΩ±Âìç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã(LVLMs)ÁöÑÂø´ÈÄüÂèëÂ±ïÊòæËëóÊâ©Â±ï‰∫ÜÂÆÉ‰ª¨ÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£„ÄÇÁÑ∂ËÄåÔºåÊâ©Â±ïÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£Âπ∂‰∏çËÉΩ‰øùËØÅ‰∏ä‰∏ãÊñáÁöÑÊúâÊïàÂà©Áî®ÔºåËøôÂØπÂÆûÈôÖÂ∫îÁî®ÊèêÂá∫‰∫Ü‰∏•Â≥ªÁöÑÊåëÊàò„ÄÇÁõÆÂâçÂØπÊ≠§Á±ªÈïø‰∏ä‰∏ãÊñá‰øùÁúüÂ∫¶ÁöÑËØÑ‰º∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®Á∫ØÊñáÊú¨È¢ÜÂüüÔºåËÄåÂ§öÊ®°ÊÄÅËØÑ‰º∞‰ªçÁÑ∂Â±ÄÈôê‰∫éÁü≠‰∏ä‰∏ãÊñá„ÄÇ‰∏∫‰∫ÜÂº•Ë°•Ëøô‰∏ÄÂ∑ÆË∑ùÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜMMLongCiteÔºåËøôÊòØ‰∏Ä‰∏™ÁªºÂêàÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞LVLMsÂú®Èïø‰∏ä‰∏ãÊñáÂú∫ÊôØ‰∏≠ÁöÑ‰øùÁúüÂ∫¶„ÄÇMMLongCiteÂåÖÂê´8‰∏™‰∏çÂêåÁöÑ‰ªªÂä°ÔºåË∑®Ë∂ä6‰∏™‰∏ä‰∏ãÊñáÈïøÂ∫¶Âå∫Èó¥ÔºåÂπ∂ÂåÖÂê´ÊñáÊú¨„ÄÅÂõæÂÉèÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçÊ®°ÊÄÅ„ÄÇÊàë‰ª¨ÂØπÊúÄÂÖàËøõÁöÑLVLMsÁöÑËØÑ‰º∞Ë°®ÊòéÔºåÂÆÉ‰ª¨Âú®Â§ÑÁêÜÈïøÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÊó∂ÁöÑ‰øùÁúüÂ∫¶ÊúâÈôê„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊ∑±ÂÖ•ÂàÜÊûê‰∫Ü‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂíåÂÖ≥ÈîÆÂÜÖÂÆπÁöÑ‰ΩçÁΩÆÂ¶Ç‰ΩïÂΩ±ÂìçËøô‰∫õÊ®°ÂûãÁöÑ‰øùÁúüÂ∫¶„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâËôΩÁÑ∂Êâ©Â±ï‰∫Ü‰∏ä‰∏ãÊñáÁ™óÂè£Ôºå‰ΩÜÊó†Ê≥ïÊúâÊïàÂà©Áî®Èïø‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öÊ®°ÊÄÅÂú∫ÊôØ‰∏ã„ÄÇÁé∞ÊúâÁöÑÈïø‰∏ä‰∏ãÊñáËØÑ‰º∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÊñáÊú¨È¢ÜÂüüÔºåÁº∫‰πèÂØπÂ§öÊ®°ÊÄÅÔºàÂõæÂÉè„ÄÅËßÜÈ¢ëÁ≠âÔºâÈïø‰∏ä‰∏ãÊñáÁöÑ‰øùÁúüÂ∫¶ËØÑ‰º∞ÔºåËøôÈôêÂà∂‰∫ÜLVLMsÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂèØÈù†ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMMLongCiteÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™ÁªºÂêàÊÄßÁöÑÂ§öÊ®°ÊÄÅÈïø‰∏ä‰∏ãÊñáËØÑ‰º∞Âü∫ÂáÜÔºåÈÄöËøáËÆæËÆ°‰∏ÄÁ≥ªÂàó‰ªªÂä°Êù•Ë°°ÈáèLVLMsÂú®Èïø‰∏ä‰∏ãÊñáËæìÂÖ•‰∏≠ËÉΩÂê¶ÂáÜÁ°ÆÂú∞ÊèêÂèñÂíåÂà©Áî®ÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇËØ•Âü∫ÂáÜÊó®Âú®Ê®°ÊãüÁúüÂÆûÂú∫ÊôØÔºåËÄÉÂØüÊ®°ÂûãÂú®‰∏çÂêå‰∏ä‰∏ãÊñáÈïøÂ∫¶Âíå‰∏çÂêåÊ®°ÊÄÅ‰∏ãÁöÑ‰ø°ÊÅØ‰øùÁúüÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMMLongCiteÂü∫ÂáÜÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºö
1. **‰ªªÂä°ËÆæËÆ°**ÔºöËÆæËÆ°‰∫Ü8‰∏™‰∏çÂêåÁöÑ‰ªªÂä°ÔºåÊ∂µÁõñ‰∫ÜÈóÆÁ≠î„ÄÅÊëòË¶Å„ÄÅÊé®ÁêÜÁ≠âÂ§öÁßçÁ±ªÂûãÔºåÊó®Âú®ÂÖ®Èù¢ËØÑ‰º∞LVLMsÁöÑ‰∏ä‰∏ãÊñáÁêÜËß£ËÉΩÂäõ„ÄÇ
2. **‰∏ä‰∏ãÊñáÈïøÂ∫¶**ÔºöË¶ÜÁõñ6‰∏™‰∏çÂêåÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶Âå∫Èó¥Ôºå‰ªéÁü≠‰∏ä‰∏ãÊñáÂà∞Ë∂ÖÈïø‰∏ä‰∏ãÊñáÔºå‰ª•ËÄÉÂØüÊ®°ÂûãÂú®‰∏çÂêåÈïøÂ∫¶‰∏ãÁöÑÊÄßËÉΩË°®Áé∞„ÄÇ
3. **Â§öÊ®°ÊÄÅÊï∞ÊçÆ**ÔºöÂåÖÂê´ÊñáÊú¨„ÄÅÂõæÂÉèÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçÊ®°ÊÄÅÁöÑÊï∞ÊçÆÔºå‰ª•ËØÑ‰º∞Ê®°ÂûãÂú®Â§öÊ®°ÊÄÅÂú∫ÊôØ‰∏ãÁöÑ‰ø°ÊÅØÊï¥ÂêàËÉΩÂäõ„ÄÇ
4. **ËØÑ‰º∞ÊåáÊ†á**ÔºöÈááÁî®Â§öÁßçËØÑ‰º∞ÊåáÊ†áÔºåÂåÖÊã¨ÂáÜÁ°ÆÁéá„ÄÅÂè¨ÂõûÁéá„ÄÅF1ÂÄºÁ≠âÔºå‰ª•ÂÖ®Èù¢Ë°°ÈáèÊ®°ÂûãÁöÑ‰øùÁúüÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMMLongCiteÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÁªºÂêàÊÄßÂíåÂ§öÊ®°ÊÄÅÊÄß„ÄÇ‰∏éÁé∞ÊúâÁöÑÈïø‰∏ä‰∏ãÊñáËØÑ‰º∞Âü∫ÂáÜÁõ∏ÊØîÔºåMMLongCite‰∏ç‰ªÖËÄÉËôë‰∫ÜÊñáÊú¨‰ø°ÊÅØÔºåËøòÂåÖÂê´‰∫ÜÂõæÂÉèÂíåËßÜÈ¢ëÁ≠âÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåÊõ¥Ë¥¥ËøëÁúüÂÆûÂ∫îÁî®Âú∫ÊôØ„ÄÇÊ≠§Â§ñÔºåMMLongCiteËøòË¶ÜÁõñ‰∫ÜÊõ¥ÂπøÊ≥õÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ËåÉÂõ¥ÔºåËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞LVLMsÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMMLongCiteÁöÑ‰ªªÂä°ËÆæËÆ°ËÄÉËôë‰∫ÜÂ§öÁßçÂõ†Á¥†Ôºå‰æãÂ¶Ç‰ªªÂä°ÁöÑÈöæÂ∫¶„ÄÅÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄß„ÄÅËØÑ‰º∞ÊåáÊ†áÁöÑÂêàÁêÜÊÄßÁ≠â„ÄÇÊØè‰∏™‰ªªÂä°ÈÉΩÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùËÉΩÂ§üÊúâÊïàÂú∞ËØÑ‰º∞LVLMsÁöÑ‰∏ä‰∏ãÊñáÁêÜËß£ËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåMMLongCiteËøòÊèê‰æõ‰∫Ü‰∏ÄÂ•óÊ†áÂáÜÁöÑËØÑ‰º∞ÊµÅÁ®ãÂíåÂ∑•ÂÖ∑ÔºåÊñπ‰æøÁ†îÁ©∂‰∫∫ÂëòËøõË°åÂÆûÈ™åÂíåÊØîËæÉ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂØπÁé∞ÊúâLVLMsÂú®MMLongCite‰∏äÁöÑËØÑ‰º∞Ë°®ÊòéÔºåÂÆÉ‰ª¨Âú®Â§ÑÁêÜÈïøÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÊó∂Ë°®Áé∞Âá∫ÊúâÈôêÁöÑ‰øùÁúüÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúËøòË°®ÊòéÔºå‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂíåÂÖ≥ÈîÆÂÜÖÂÆπÁöÑ‰ΩçÁΩÆÂØπÊ®°ÂûãÁöÑÊÄßËÉΩÊúâÊòæËëóÂΩ±Âìç„ÄÇ‰æãÂ¶ÇÔºåÂΩìÂÖ≥ÈîÆ‰ø°ÊÅØ‰Ωç‰∫é‰∏ä‰∏ãÊñáÁöÑÊú´Â∞æÊó∂ÔºåÊ®°ÂûãÁöÑË°®Áé∞ÈÄöÂ∏∏‰ºö‰∏ãÈôç„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂíåÂØπÊØîÂü∫Á∫øÊú™Âú®ÊëòË¶Å‰∏≠ÁªôÂá∫ÔºåÈúÄÂèÇËÄÉÂéüÊñá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MMLongCiteÂü∫ÂáÜÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂÆ¢Êúç„ÄÅËßÜÈ¢ëÁêÜËß£„ÄÅÂåªÂ≠¶ËØäÊñ≠Á≠â„ÄÇÈÄöËøáÊèêÈ´òLVLMsÂú®Èïø‰∏ä‰∏ãÊñáÂ§öÊ®°ÊÄÅÂú∫ÊôØ‰∏ãÁöÑ‰ø°ÊÅØ‰øùÁúüÂ∫¶ÔºåÂèØ‰ª•ÊèêÂçáËøô‰∫õÂ∫îÁî®ÁöÑÁî®Êà∑‰ΩìÈ™åÂíåÂèØÈù†ÊÄß„ÄÇÊú™Êù•ÔºåËØ•Âü∫ÂáÜÂèØ‰ª•‰øÉËøõLVLMsÂú®Êõ¥ÂπøÊ≥õÈ¢ÜÂüüÁöÑÂ∫îÁî®Ôºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The rapid advancement of large vision language models (LVLMs) has led to a significant expansion of their context windows. However, an extended context window does not guarantee the effective utilization of the context, posing a critical challenge for real-world applications. Current evaluations of such long-context faithfulness are predominantly focused on the text-only domain, while multimodal assessments remain limited to short contexts. To bridge this gap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate the fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8 distinct tasks spanning 6 context length intervals and incorporates diverse modalities, including text, images, and videos. Our evaluation of state-of-the-art LVLMs reveals their limited faithfulness in handling long multimodal contexts. Furthermore, we provide an in-depth analysis of how context length and the position of crucial content affect the faithfulness of these models.

