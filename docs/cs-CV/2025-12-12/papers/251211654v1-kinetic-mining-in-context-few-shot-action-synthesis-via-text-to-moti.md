---
layout: default
title: Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation
---

# Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation

**arXiv**: [2512.11654v1](https://arxiv.org/abs/2512.11654) | [PDF](https://arxiv.org/pdf/2512.11654.pdf)

**ä½œè€…**: Luca Cazzola, Ahed Alboody

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-12

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://lucazzola.github.io/publications/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**KineMICï¼šé€šè¿‡æ–‡æœ¬åˆ°åŠ¨ä½œè’¸é¦å®žçŽ°å°‘æ ·æœ¬åŠ¨ä½œåˆæˆï¼Œè§£å†³HARæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `å°‘æ ·æœ¬å­¦ä¹ ` `åŠ¨ä½œåˆæˆ` `æ–‡æœ¬åˆ°åŠ¨ä½œ` `è¿ç§»å­¦ä¹ ` `äººä½“æ´»åŠ¨è¯†åˆ«` `æ‰©æ•£æ¨¡åž‹` `è¿åŠ¨æŒ–æŽ˜`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰HARæ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè€ŒT2Mæ¨¡åž‹è™½ç„¶èƒ½ç”ŸæˆåŠ¨ä½œï¼Œä½†ä¸ŽHARä»»åŠ¡éœ€æ±‚å­˜åœ¨é¢†åŸŸå·®å¼‚ã€‚
2. KineMICåˆ©ç”¨æ–‡æœ¬ç¼–ç ç©ºé—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œé€šè¿‡è¿åŠ¨æŒ–æŽ˜ç­–ç•¥ï¼Œå°†é€šç”¨T2Mæ¨¡åž‹è¿ç§»åˆ°HARé¢†åŸŸã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒKineMICåœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†åŠ¨ä½œç”Ÿæˆè´¨é‡ï¼Œå¹¶ä½¿HARå‡†ç¡®çŽ‡æé«˜äº†23.1%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é’ˆå¯¹åŸºäºŽéª¨éª¼çš„äººä½“æ´»åŠ¨è¯†åˆ«(HAR)ä¸­å¸¦æ ‡æ³¨çš„å¤§åž‹è¿åŠ¨æ•°æ®é›†èŽ·å–æˆæœ¬é«˜æ˜‚è¿™ä¸€å…³é”®ç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºKineMICï¼ˆæƒ…å¢ƒä¸­çš„è¿åŠ¨æŒ–æŽ˜ï¼‰çš„è¿ç§»å­¦ä¹ æ¡†æž¶ï¼Œç”¨äºŽå°‘æ ·æœ¬åŠ¨ä½œåˆæˆã€‚KineMICé€šè¿‡å‡è®¾æ–‡æœ¬ç¼–ç ç©ºé—´ä¸­çš„è¯­ä¹‰å¯¹åº”å…³ç³»å¯ä»¥ä¸ºè¿åŠ¨å­¦è’¸é¦æä¾›è½¯ç›‘ç£ï¼Œä»Žè€Œå°†æ–‡æœ¬åˆ°åŠ¨ä½œ(T2M)æ‰©æ•£æ¨¡åž‹é€‚é…åˆ°HARé¢†åŸŸã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡ä¸€ç§è¿åŠ¨æŒ–æŽ˜ç­–ç•¥ï¼Œåˆ©ç”¨CLIPæ–‡æœ¬åµŒå…¥æ¥å»ºç«‹ç¨€ç–HARæ ‡ç­¾å’ŒT2Mæºæ•°æ®ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚è¯¥è¿‡ç¨‹æŒ‡å¯¼å¾®è°ƒï¼Œå°†é€šç”¨T2Méª¨å¹²ç½‘ç»œè½¬æ¢ä¸ºä¸“é—¨çš„å°‘æ ·æœ¬åŠ¨ä½œåˆ°è¿åŠ¨ç”Ÿæˆå™¨ã€‚åœ¨HumanML3Dï¼ˆæºT2Mæ•°æ®é›†ï¼‰å’ŒNTU RGB+D 120å­é›†ï¼ˆç›®æ ‡HARé¢†åŸŸï¼‰ä¸ŠéªŒè¯äº†KineMICï¼Œæ¯ä¸ªåŠ¨ä½œç±»åˆ«ä»…éšæœºé€‰æ‹©10ä¸ªæ ·æœ¬ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•ç”Ÿæˆäº†æ›´è¿žè´¯çš„åŠ¨ä½œï¼Œæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®å¢žå¼ºæ¥æºï¼Œå®žçŽ°äº†+23.1%çš„å‡†ç¡®çŽ‡æå‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽéª¨éª¼çš„HARæ–¹æ³•ä¸¥é‡ä¾èµ–äºŽå¤§è§„æ¨¡ã€å¸¦æ ‡æ³¨çš„è¿åŠ¨æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®é›†çš„èŽ·å–æˆæœ¬éžå¸¸é«˜æ˜‚ã€‚è™½ç„¶æ–‡æœ¬åˆ°åŠ¨ä½œ(T2M)ç”Ÿæˆæ¨¡åž‹æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„åˆæˆæ•°æ®æ¥æºï¼Œä½†å…¶è®­ç»ƒç›®æ ‡ä¾§é‡äºŽé€šç”¨çš„è‰ºæœ¯æ€§è¿åŠ¨ï¼Œå¹¶ä¸”æ•°æ®é›†ç»“æž„ä¸ŽHARå¯¹è¿åŠ¨å­¦ç²¾ç¡®ã€ç±»åˆ«åŒºåˆ†æ€§åŠ¨ä½œçš„è¦æ±‚å­˜åœ¨æ ¹æœ¬å·®å¼‚ï¼Œå¯¼è‡´é¢†åŸŸé¸¿æ²Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šKineMICçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨T2Mæ¨¡åž‹ä¸­æ–‡æœ¬ç¼–ç ç©ºé—´ä¸­è•´å«çš„è¯­ä¹‰ä¿¡æ¯ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ çš„æ–¹å¼ï¼Œå°†é€šç”¨çš„T2Mæ¨¡åž‹é€‚é…åˆ°ç‰¹å®šçš„HARé¢†åŸŸã€‚å®ƒå‡è®¾æ–‡æœ¬ç¼–ç ç©ºé—´ä¸­çš„è¯­ä¹‰å¯¹åº”å…³ç³»å¯ä»¥ä¸ºè¿åŠ¨å­¦è’¸é¦æä¾›è½¯ç›‘ç£ï¼Œä»Žè€ŒæŒ‡å¯¼T2Mæ¨¡åž‹ç”Ÿæˆæ›´é€‚åˆHARä»»åŠ¡çš„åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šKineMICæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åˆ©ç”¨CLIPæ¨¡åž‹æå–HARæ ‡ç­¾å’ŒT2Mæ•°æ®çš„æ–‡æœ¬åµŒå…¥ï¼›2) é€šè¿‡è¿åŠ¨æŒ–æŽ˜ç­–ç•¥ï¼Œå»ºç«‹HARæ ‡ç­¾å’ŒT2Mæ•°æ®ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå³æ‰¾åˆ°ä¸ŽHARæ ‡ç­¾è¯­ä¹‰æœ€ç›¸å…³çš„T2MåŠ¨ä½œï¼›3) ä½¿ç”¨è¿™äº›å¯¹åº”å…³ç³»ä½œä¸ºè½¯ç›‘ç£ï¼Œå¯¹T2Mæ‰©æ•£æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆç¬¦åˆHARä»»åŠ¡éœ€æ±‚çš„åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šKineMICçš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†è¿åŠ¨æŒ–æŽ˜ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨CLIPæ–‡æœ¬åµŒå…¥æ¥å»ºç«‹ç¨€ç–HARæ ‡ç­¾å’ŒT2Mæºæ•°æ®ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨T2Mæ¨¡åž‹ä¸­è•´å«çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå°†å…¶è¿ç§»åˆ°HARé¢†åŸŸï¼Œä»Žè€Œè§£å†³äº†HARæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ä¸Žç›´æŽ¥ä½¿ç”¨T2Mæ¨¡åž‹ç”ŸæˆåŠ¨ä½œä¸åŒï¼ŒKineMICé€šè¿‡è¿åŠ¨æŒ–æŽ˜å’Œå¾®è°ƒï¼Œä½¿å¾—ç”Ÿæˆçš„åŠ¨ä½œæ›´å…·è¿åŠ¨å­¦ç²¾ç¡®æ€§å’Œç±»åˆ«åŒºåˆ†æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šKineMICçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨CLIPæ¨¡åž‹æå–æ–‡æœ¬åµŒå…¥ï¼Œä»¥æ•æ‰HARæ ‡ç­¾å’ŒT2Mæ•°æ®ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼›2) è®¾è®¡è¿åŠ¨æŒ–æŽ˜ç­–ç•¥ï¼Œé€šè¿‡è®¡ç®—æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œæ‰¾åˆ°ä¸ŽHARæ ‡ç­¾æœ€ç›¸å…³çš„T2MåŠ¨ä½œï¼›3) ä½¿ç”¨æ‰©æ•£æ¨¡åž‹ä½œä¸ºT2Méª¨å¹²ç½‘ç»œï¼Œå¹¶ä½¿ç”¨è¿åŠ¨æŒ–æŽ˜çš„ç»“æžœä½œä¸ºè½¯ç›‘ç£ä¿¡å·ï¼Œå¯¹æ‰©æ•£æ¨¡åž‹è¿›è¡Œå¾®è°ƒã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬é‡å»ºæŸå¤±ã€å¯¹æŠ—æŸå¤±ä»¥åŠåŸºäºŽè¿åŠ¨å­¦çº¦æŸçš„æŸå¤±é¡¹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

KineMICåœ¨NTU RGB+D 120æ•°æ®é›†çš„å­é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¯ä¸ªåŠ¨ä½œç±»åˆ«ä»…ä½¿ç”¨10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒKineMICèƒ½å¤Ÿç”Ÿæˆæ›´è¿žè´¯çš„åŠ¨ä½œï¼Œå¹¶æ˜¾è‘—æé«˜äº†HARçš„å‡†ç¡®çŽ‡ï¼Œç›¸æ¯”äºŽåŸºçº¿æ–¹æ³•ï¼Œå®žçŽ°äº†+23.1%çš„æ€§èƒ½æå‡ã€‚è¿™è¡¨æ˜ŽKineMICåœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹å…·æœ‰å¼ºå¤§çš„åŠ¨ä½œåˆæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³HARæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

KineMICå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨æ™ºèƒ½ç›‘æŽ§ã€äººæœºäº¤äº’ã€åº·å¤è®­ç»ƒç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºŽç”Ÿæˆå„ç§äººä½“æ´»åŠ¨ï¼Œä»Žè€Œæ‰©å……è®­ç»ƒæ•°æ®é›†ï¼Œæé«˜HARç³»ç»Ÿçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒKineMICè¿˜å¯ä»¥ç”¨äºŽç”Ÿæˆç‰¹å®šåœºæ™¯ä¸‹çš„åŠ¨ä½œï¼Œä¾‹å¦‚æ¨¡æ‹Ÿè€å¹´äººè·Œå€’ï¼Œå¸®åŠ©è¯„ä¼°å’Œæ”¹è¿›å®‰å…¨æŽªæ–½ã€‚è¯¥ç ”ç©¶æœ‰æœ›æŽ¨åŠ¨HARæŠ€æœ¯çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”ç”¨äºŽå®žé™…åœºæ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).

