---
layout: default
title: CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction
---

# CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.11988" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.11988v1</a>
  <a href="https://arxiv.org/pdf/2512.11988.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11988v1" onclick="toggleFavorite(this, '2512.11988v1', 'CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xianghui Xie, Bowen Wen, Yan Chang, Hesam Rabeti, Jiefeng Li, Ye Yuan, Gerard Pons-Moll, Stan Birchfield

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-12

**å¤‡æ³¨**: 14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CARI4Dï¼šæå‡ºä¸€ç§ç±»åˆ«æ— å…³çš„4Däºº-ç‰©äº¤äº’é‡å»ºæ–¹æ³•ï¼Œè§£å†³å•ç›®RGBè§†é¢‘é‡å»ºéš¾é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸ååº” (Interaction & Reaction)**

**å…³é”®è¯**: `4Dé‡å»º` `äºº-ç‰©äº¤äº’` `ç±»åˆ«æ— å…³` `å•ç›®è§†è§‰` `æ¸²æŸ“-æ¯”è¾ƒ` `ç‰©ç†çº¦æŸ` `åŸºç¡€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨4Däºº-ç‰©äº¤äº’é‡å»ºä¸­ï¼Œä¾èµ–ç‰©ä½“æ¨¡æ¿æˆ–é™åˆ¶ç‰©ä½“ç±»åˆ«ï¼Œéš¾ä»¥å¤„ç†çœŸå®åœºæ™¯çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚
2. CARI4Dé€šè¿‡æ•´åˆåŸºç¡€æ¨¡å‹çš„é¢„æµ‹ï¼Œå¹¶åˆ©ç”¨æ¸²æŸ“-æ¯”è¾ƒèŒƒä¾‹è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œå®ç°ç©ºé—´ã€æ—¶é—´å’Œåƒç´ çº§åˆ«çš„ä¸€è‡´æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCARI4Dåœ¨é‡å»ºç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºCARI4Dï¼Œä¸€ç§ç±»åˆ«æ— å…³çš„æ–¹æ³•ï¼Œç”¨äºä»å•ç›®RGBè§†é¢‘ä¸­ä»¥åº¦é‡å°ºåº¦é‡å»ºç©ºé—´å’Œæ—¶é—´ä¸Šä¸€è‡´çš„4Däºº-ç‰©äº¤äº’ã€‚ç”±äºæœªçŸ¥ç‰©ä½“å’Œäººä½“ä¿¡æ¯ã€æ·±åº¦æ¨¡ç³Šã€é®æŒ¡å’Œå¤æ‚è¿åŠ¨ï¼Œä»å•ä¸ªRGBè§†å›¾æ¨æ–­4Däº¤äº’æå…·æŒ‘æˆ˜æ€§ï¼Œé˜»ç¢äº†ä¸€è‡´çš„3Då’Œæ—¶é—´é‡å»ºã€‚å…ˆå‰çš„æ–¹æ³•é€šè¿‡å‡è®¾ground truthç‰©ä½“æ¨¡æ¿æˆ–é™åˆ¶äºæœ‰é™çš„ç‰©ä½“ç±»åˆ«æ¥ç®€åŒ–è®¾ç½®ã€‚CARI4Dé€šè¿‡ç¨³å¥åœ°æ•´åˆæ¥è‡ªåŸºç¡€æ¨¡å‹çš„ä¸ªä½“é¢„æµ‹ï¼Œå¹¶é€šè¿‡å­¦ä¹ åˆ°çš„æ¸²æŸ“-æ¯”è¾ƒèŒƒä¾‹è”åˆç»†åŒ–å®ƒä»¬ï¼Œä»¥ç¡®ä¿ç©ºé—´ã€æ—¶é—´å’Œåƒç´ å¯¹é½ï¼Œæœ€åæ¨ç†å¤æ‚çš„æ¥è§¦ä»¥è¿›ä¸€æ­¥ç»†åŒ–ï¼Œä»è€Œæ»¡è¶³ç‰©ç†çº¦æŸã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒåˆ†å¸ƒæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯38%ï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯36%ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥æ³›åŒ–åˆ°è®­ç»ƒç±»åˆ«ä¹‹å¤–ï¼Œå› æ­¤å¯ä»¥é›¶æ ·æœ¬åº”ç”¨äºé‡å¤–äº’è”ç½‘è§†é¢‘ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å•ç›®RGBè§†é¢‘ä¸­é‡å»º4Däºº-ç‰©äº¤äº’æ—¶ï¼Œé¢ä¸´ç‰©ä½“ç±»åˆ«æœªçŸ¥ã€æ·±åº¦æ¨¡ç³Šã€é®æŒ¡ä»¥åŠå¤æ‚è¿åŠ¨ç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´é‡å»ºç»“æœåœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šä¸ä¸€è‡´ã€‚ä¹‹å‰çš„ç ”ç©¶é€šå¸¸ä¾èµ–äºå·²çŸ¥çš„ç‰©ä½“æ¨¡æ¿æˆ–è€…å°†ç‰©ä½“ç±»åˆ«é™åˆ¶åœ¨ä¸€ä¸ªè¾ƒå°çš„é›†åˆå†…ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCARI4Dçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æä¾›åˆå§‹çš„äººä½“å’Œç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„æ¸²æŸ“-æ¯”è¾ƒæ¡†æ¶ï¼Œå¯¹è¿™äº›ä¼°è®¡è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥ç¡®ä¿é‡å»ºç»“æœåœ¨ç©ºé—´ã€æ—¶é—´å’Œåƒç´ çº§åˆ«ä¸Šçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜æ˜¾å¼åœ°æ¨ç†äººä¸ç‰©ä½“ä¹‹é—´çš„æ¥è§¦å…³ç³»ï¼Œå¹¶åˆ©ç”¨ç‰©ç†çº¦æŸè¿›ä¸€æ­¥æå‡é‡å»ºè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCARI4Dçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **å§¿æ€å‡è®¾ç”Ÿæˆ**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚äººä½“å§¿æ€ä¼°è®¡å™¨å’Œç‰©ä½“æ£€æµ‹å™¨ï¼‰ç”Ÿæˆåˆå§‹çš„äººä½“å’Œç‰©ä½“å§¿æ€å‡è®¾ã€‚2) **è”åˆä¼˜åŒ–**ï¼šé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„æ¸²æŸ“-æ¯”è¾ƒæ¡†æ¶ï¼Œå¯¹äººä½“å’Œç‰©ä½“çš„å§¿æ€è¿›è¡Œè”åˆä¼˜åŒ–ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¸²æŸ“é‡å»ºç»“æœï¼Œå¹¶å°†å…¶ä¸åŸå§‹å›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œè®¡ç®—æŸå¤±å‡½æ•°ï¼Œä»è€Œé©±åŠ¨å§¿æ€çš„ä¼˜åŒ–ã€‚3) **æ¥è§¦æ¨ç†**ï¼šæ˜¾å¼åœ°æ¨ç†äººä¸ç‰©ä½“ä¹‹é—´çš„æ¥è§¦å…³ç³»ï¼Œå¹¶åˆ©ç”¨ç‰©ç†çº¦æŸè¿›ä¸€æ­¥æå‡é‡å»ºè´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCARI4Dçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç±»åˆ«æ— å…³çš„é‡å»ºèƒ½åŠ›å’Œç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ¡†æ¶ã€‚ä¸ä»¥å¾€ä¾èµ–ç‰©ä½“æ¨¡æ¿æˆ–é™åˆ¶ç‰©ä½“ç±»åˆ«çš„æ–¹æ³•ä¸åŒï¼ŒCARI4Då¯ä»¥å¤„ç†ä»»æ„ç±»åˆ«çš„ç‰©ä½“ï¼Œä»è€Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCARI4Dé€šè¿‡ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ¡†æ¶ï¼Œå°†äººä½“å’Œç‰©ä½“çš„å§¿æ€ä¼°è®¡ã€æ¸²æŸ“å’Œæ¯”è¾ƒä»¥åŠæ¥è§¦æ¨ç†æ•´åˆåœ¨ä¸€èµ·ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®å’Œä¸€è‡´çš„é‡å»ºç»“æœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¸²æŸ“-æ¯”è¾ƒæ¡†æ¶ä¸­ï¼Œä½¿ç”¨äº†å¯å¾®åˆ†æ¸²æŸ“å™¨ï¼Œå…è®¸æ¢¯åº¦ä»åƒç´ ç©ºé—´åå‘ä¼ æ’­åˆ°å§¿æ€å‚æ•°ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åƒç´ çº§åˆ«çš„å›¾åƒé‡å»ºæŸå¤±ã€3Då‡ ä½•ä¸€è‡´æ€§æŸå¤±å’Œæ—¶é—´ä¸€è‡´æ€§æŸå¤±ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªæ¥è§¦æŸå¤±ï¼Œç”¨äºé¼“åŠ±æ¨¡å‹å­¦ä¹ äººä¸ç‰©ä½“ä¹‹é—´çš„åˆç†æ¥è§¦å…³ç³»ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œä½¿ç”¨äº†Transformerç½‘ç»œæ¥å»ºæ¨¡äººä½“å’Œç‰©ä½“ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œæ¥æ¨ç†æ¥è§¦å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CARI4Dåœ¨åŒåˆ†å¸ƒæ•°æ®é›†ä¸Šç›¸æ¯”ç°æœ‰æŠ€æœ¯æå‡äº†38%çš„é‡å»ºç²¾åº¦ï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šæå‡äº†36%ã€‚è¿™è¡¨æ˜CARI4Dä¸ä»…åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§çœŸå®ä¸–ç•Œçš„åœºæ™¯ã€‚è¯¥æ¨¡å‹è¿˜èƒ½å¤Ÿé›¶æ ·æœ¬åº”ç”¨äºäº’è”ç½‘è§†é¢‘ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œè®­ç»ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CARI4Dåœ¨äººæœºäº¤äº’ã€æ¸¸æˆã€æœºå™¨äººå­¦ä¹ ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºåˆ›å»ºæ›´é€¼çœŸå’Œè‡ªç„¶çš„è™šæ‹Ÿç°å®ä½“éªŒï¼Œè®­ç»ƒæœºå™¨äººè¿›è¡Œå¤æ‚çš„äºº-ç‰©äº¤äº’ä»»åŠ¡ï¼Œä»¥åŠåˆ†æäººç±»è¡Œä¸ºå’Œå§¿æ€ã€‚è¯¥ç ”ç©¶çš„çªç ´ä¸ºæ›´æ™ºèƒ½ã€æ›´å…·é€‚åº”æ€§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

