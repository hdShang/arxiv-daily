---
layout: default
title: CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction
---

# CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction

**arXiv**: [2512.11988v1](https://arxiv.org/abs/2512.11988) | [PDF](https://arxiv.org/pdf/2512.11988.pdf)

**ä½œè€…**: Xianghui Xie, Bowen Wen, Yan Chang, Hesam Rabeti, Jiefeng Li, Ye Yuan, Gerard Pons-Moll, Stan Birchfield

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-12

**å¤‡æ³¨**: 14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CARI4Dï¼šæå‡ºä¸€ç§ç±»åˆ«æ— å…³çš„4Däºº-ç‰©äº¤äº’é‡å»ºæ–¹æ³•ï¼Œè§£å†³å•ç›®RGBè§†é¢‘é‡å»ºéš¾é¢˜ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äº”ï¼šäº¤äº’ä¸Žååº” (Interaction & Reaction)**

**å…³é”®è¯**: `4Dé‡å»º` `äºº-ç‰©äº¤äº’` `ç±»åˆ«æ— å…³` `å•ç›®è§†è§‰` `æ¸²æŸ“-æ¯”è¾ƒ` `ç‰©ç†çº¦æŸ` `åŸºç¡€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨4Däºº-ç‰©äº¤äº’é‡å»ºä¸­ï¼Œä¾èµ–ç‰©ä½“æ¨¡æ¿æˆ–é™åˆ¶ç‰©ä½“ç±»åˆ«ï¼Œéš¾ä»¥å¤„ç†çœŸå®žåœºæ™¯çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚
2. CARI4Dé€šè¿‡æ•´åˆåŸºç¡€æ¨¡åž‹çš„é¢„æµ‹ï¼Œå¹¶åˆ©ç”¨æ¸²æŸ“-æ¯”è¾ƒèŒƒä¾‹è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œå®žçŽ°ç©ºé—´ã€æ—¶é—´å’Œåƒç´ çº§åˆ«çš„ä¸€è‡´æ€§ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒCARI4Dåœ¨é‡å»ºç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨çŽ°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºCARI4Dï¼Œä¸€ç§ç±»åˆ«æ— å…³çš„æ–¹æ³•ï¼Œç”¨äºŽä»Žå•ç›®RGBè§†é¢‘ä¸­ä»¥åº¦é‡å°ºåº¦é‡å»ºç©ºé—´å’Œæ—¶é—´ä¸Šä¸€è‡´çš„4Däºº-ç‰©äº¤äº’ã€‚ç”±äºŽæœªçŸ¥ç‰©ä½“å’Œäººä½“ä¿¡æ¯ã€æ·±åº¦æ¨¡ç³Šã€é®æŒ¡å’Œå¤æ‚è¿åŠ¨ï¼Œä»Žå•ä¸ªRGBè§†å›¾æŽ¨æ–­4Däº¤äº’æžå…·æŒ‘æˆ˜æ€§ï¼Œé˜»ç¢äº†ä¸€è‡´çš„3Då’Œæ—¶é—´é‡å»ºã€‚å…ˆå‰çš„æ–¹æ³•é€šè¿‡å‡è®¾ground truthç‰©ä½“æ¨¡æ¿æˆ–é™åˆ¶äºŽæœ‰é™çš„ç‰©ä½“ç±»åˆ«æ¥ç®€åŒ–è®¾ç½®ã€‚CARI4Dé€šè¿‡ç¨³å¥åœ°æ•´åˆæ¥è‡ªåŸºç¡€æ¨¡åž‹çš„ä¸ªä½“é¢„æµ‹ï¼Œå¹¶é€šè¿‡å­¦ä¹ åˆ°çš„æ¸²æŸ“-æ¯”è¾ƒèŒƒä¾‹è”åˆç»†åŒ–å®ƒä»¬ï¼Œä»¥ç¡®ä¿ç©ºé—´ã€æ—¶é—´å’Œåƒç´ å¯¹é½ï¼Œæœ€åŽæŽ¨ç†å¤æ‚çš„æŽ¥è§¦ä»¥è¿›ä¸€æ­¥ç»†åŒ–ï¼Œä»Žè€Œæ»¡è¶³ç‰©ç†çº¦æŸã€‚å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒåˆ†å¸ƒæ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æŠ€æœ¯38%ï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šä¼˜äºŽçŽ°æœ‰æŠ€æœ¯36%ã€‚æˆ‘ä»¬çš„æ¨¡åž‹å¯ä»¥æ³›åŒ–åˆ°è®­ç»ƒç±»åˆ«ä¹‹å¤–ï¼Œå› æ­¤å¯ä»¥é›¶æ ·æœ¬åº”ç”¨äºŽé‡Žå¤–äº’è”ç½‘è§†é¢‘ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡åž‹å°†å…¬å¼€å‘å¸ƒã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ–¹æ³•åœ¨å•ç›®RGBè§†é¢‘ä¸­é‡å»º4Däºº-ç‰©äº¤äº’æ—¶ï¼Œé¢ä¸´ç‰©ä½“ç±»åˆ«æœªçŸ¥ã€æ·±åº¦æ¨¡ç³Šã€é®æŒ¡ä»¥åŠå¤æ‚è¿åŠ¨ç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´é‡å»ºç»“æžœåœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šä¸ä¸€è‡´ã€‚ä¹‹å‰çš„ç ”ç©¶é€šå¸¸ä¾èµ–äºŽå·²çŸ¥çš„ç‰©ä½“æ¨¡æ¿æˆ–è€…å°†ç‰©ä½“ç±»åˆ«é™åˆ¶åœ¨ä¸€ä¸ªè¾ƒå°çš„é›†åˆå†…ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨çœŸå®žä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCARI4Dçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡åž‹æä¾›åˆå§‹çš„äººä½“å’Œç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œç„¶åŽé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„æ¸²æŸ“-æ¯”è¾ƒæ¡†æž¶ï¼Œå¯¹è¿™äº›ä¼°è®¡è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥ç¡®ä¿é‡å»ºç»“æžœåœ¨ç©ºé—´ã€æ—¶é—´å’Œåƒç´ çº§åˆ«ä¸Šçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡åž‹è¿˜æ˜¾å¼åœ°æŽ¨ç†äººä¸Žç‰©ä½“ä¹‹é—´çš„æŽ¥è§¦å…³ç³»ï¼Œå¹¶åˆ©ç”¨ç‰©ç†çº¦æŸè¿›ä¸€æ­¥æå‡é‡å»ºè´¨é‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCARI4Dçš„æ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **å§¿æ€å‡è®¾ç”Ÿæˆ**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡åž‹ï¼ˆå¦‚äººä½“å§¿æ€ä¼°è®¡å™¨å’Œç‰©ä½“æ£€æµ‹å™¨ï¼‰ç”Ÿæˆåˆå§‹çš„äººä½“å’Œç‰©ä½“å§¿æ€å‡è®¾ã€‚2) **è”åˆä¼˜åŒ–**ï¼šé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„æ¸²æŸ“-æ¯”è¾ƒæ¡†æž¶ï¼Œå¯¹äººä½“å’Œç‰©ä½“çš„å§¿æ€è¿›è¡Œè”åˆä¼˜åŒ–ã€‚è¯¥æ¡†æž¶é€šè¿‡æ¸²æŸ“é‡å»ºç»“æžœï¼Œå¹¶å°†å…¶ä¸ŽåŽŸå§‹å›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œè®¡ç®—æŸå¤±å‡½æ•°ï¼Œä»Žè€Œé©±åŠ¨å§¿æ€çš„ä¼˜åŒ–ã€‚3) **æŽ¥è§¦æŽ¨ç†**ï¼šæ˜¾å¼åœ°æŽ¨ç†äººä¸Žç‰©ä½“ä¹‹é—´çš„æŽ¥è§¦å…³ç³»ï¼Œå¹¶åˆ©ç”¨ç‰©ç†çº¦æŸè¿›ä¸€æ­¥æå‡é‡å»ºè´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCARI4Dçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶ç±»åˆ«æ— å…³çš„é‡å»ºèƒ½åŠ›å’Œç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ¡†æž¶ã€‚ä¸Žä»¥å¾€ä¾èµ–ç‰©ä½“æ¨¡æ¿æˆ–é™åˆ¶ç‰©ä½“ç±»åˆ«çš„æ–¹æ³•ä¸åŒï¼ŒCARI4Då¯ä»¥å¤„ç†ä»»æ„ç±»åˆ«çš„ç‰©ä½“ï¼Œä»Žè€Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCARI4Dé€šè¿‡ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ¡†æž¶ï¼Œå°†äººä½“å’Œç‰©ä½“çš„å§¿æ€ä¼°è®¡ã€æ¸²æŸ“å’Œæ¯”è¾ƒä»¥åŠæŽ¥è§¦æŽ¨ç†æ•´åˆåœ¨ä¸€èµ·ï¼Œä»Žè€Œå®žçŽ°æ›´å‡†ç¡®å’Œä¸€è‡´çš„é‡å»ºç»“æžœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¸²æŸ“-æ¯”è¾ƒæ¡†æž¶ä¸­ï¼Œä½¿ç”¨äº†å¯å¾®åˆ†æ¸²æŸ“å™¨ï¼Œå…è®¸æ¢¯åº¦ä»Žåƒç´ ç©ºé—´åå‘ä¼ æ’­åˆ°å§¿æ€å‚æ•°ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åƒç´ çº§åˆ«çš„å›¾åƒé‡å»ºæŸå¤±ã€3Då‡ ä½•ä¸€è‡´æ€§æŸå¤±å’Œæ—¶é—´ä¸€è‡´æ€§æŸå¤±ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªæŽ¥è§¦æŸå¤±ï¼Œç”¨äºŽé¼“åŠ±æ¨¡åž‹å­¦ä¹ äººä¸Žç‰©ä½“ä¹‹é—´çš„åˆç†æŽ¥è§¦å…³ç³»ã€‚ç½‘ç»œç»“æž„æ–¹é¢ï¼Œä½¿ç”¨äº†Transformerç½‘ç»œæ¥å»ºæ¨¡äººä½“å’Œç‰©ä½“ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åˆ©ç”¨å›¾ç¥žç»ç½‘ç»œæ¥æŽ¨ç†æŽ¥è§¦å…³ç³»ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

CARI4Dåœ¨åŒåˆ†å¸ƒæ•°æ®é›†ä¸Šç›¸æ¯”çŽ°æœ‰æŠ€æœ¯æå‡äº†38%çš„é‡å»ºç²¾åº¦ï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šæå‡äº†36%ã€‚è¿™è¡¨æ˜ŽCARI4Dä¸ä»…åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œè€Œä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§çœŸå®žä¸–ç•Œçš„åœºæ™¯ã€‚è¯¥æ¨¡åž‹è¿˜èƒ½å¤Ÿé›¶æ ·æœ¬åº”ç”¨äºŽäº’è”ç½‘è§†é¢‘ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œè®­ç»ƒã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

CARI4Dåœ¨äººæœºäº¤äº’ã€æ¸¸æˆã€æœºå™¨äººå­¦ä¹ ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºŽåˆ›å»ºæ›´é€¼çœŸå’Œè‡ªç„¶çš„è™šæ‹ŸçŽ°å®žä½“éªŒï¼Œè®­ç»ƒæœºå™¨äººè¿›è¡Œå¤æ‚çš„äºº-ç‰©äº¤äº’ä»»åŠ¡ï¼Œä»¥åŠåˆ†æžäººç±»è¡Œä¸ºå’Œå§¿æ€ã€‚è¯¥ç ”ç©¶çš„çªç ´ä¸ºæ›´æ™ºèƒ½ã€æ›´å…·é€‚åº”æ€§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

