---
layout: default
title: UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models
---

# UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.11336" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.11336v1</a>
  <a href="https://arxiv.org/pdf/2512.11336.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11336v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.11336v1', 'UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hewen Pan, Cong Wei, Dashuang Liang, Zepeng Huang, Pengfei Gao, Ziqi Zhou, Lulu Xue, Pengfei Yan, Xiaoming Wei, Minghui Li, Shengshan Hu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-12

**å¤‡æ³¨**: 22 pages, 13 figures, technical report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUFVideoï¼Œå®ç°ç»Ÿä¸€çš„å¤šç²’åº¦è§†é¢‘ååŒç†è§£ï¼Œè¶…è¶Šç°æœ‰Video LLMã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†é¢‘ç†è§£` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `è§†è§‰-è¯­è¨€å¯¹é½` `å¤šç²’åº¦ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Video LLMä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œç¼ºä¹å…¨é¢å’Œå¤šç²’åº¦çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚
2. UFVideoé€šè¿‡ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€å¼•å¯¼å¯¹é½ï¼Œåœ¨å•ä¸€æ¨¡å‹ä¸­å¤„ç†å…¨å±€ã€åƒç´ å’Œæ—¶é—´å°ºåº¦çš„è§†é¢‘ç†è§£ã€‚
3. UFVideo-Benchè¯„ä¼°å¤šç²’åº¦è§†é¢‘ç†è§£ï¼Œè¯æ˜UFVideoä¼˜äºGPT-4oï¼Œå¹¶åœ¨9ä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œè§†é¢‘LLMså¾—åˆ°äº†è¿›ä¸€æ­¥å‘å±•ï¼Œä»¥æ‰§è¡Œæ•´ä½“å’Œä¸“ä¸šçš„è§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œä»…é™äºä¸“é—¨çš„è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œæœªèƒ½å®ç°å…¨é¢å’Œå¤šç²’åº¦çš„è§†é¢‘æ„ŸçŸ¥ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UFVideoï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…·æœ‰ç»Ÿä¸€å¤šç²’åº¦ååŒç†è§£èƒ½åŠ›çš„è§†é¢‘LLMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€å¼•å¯¼å¯¹é½ï¼Œä»¥åœ¨å•ä¸ªæ¨¡å‹ä¸­çµæ´»åœ°å¤„ç†è·¨å…¨å±€ã€åƒç´ å’Œæ—¶é—´å°ºåº¦çš„è§†é¢‘ç†è§£ã€‚UFVideoåŠ¨æ€åœ°ç¼–ç ä¸åŒä»»åŠ¡çš„è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬å“åº”ã€æ—¶é—´å®šä½æˆ–æ¥åœ°çš„æ©ç ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šç²’åº¦è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œæˆ‘ä»¬æ„å»ºäº†UFVideo-Benchï¼Œå®ƒç”±å°ºåº¦å†…çš„ä¸‰ä¸ªä¸åŒçš„åä½œä»»åŠ¡ç»„æˆï¼Œè¿™è¯æ˜äº†UFVideoç›¸å¯¹äºGPT-4oçš„çµæ´»æ€§å’Œä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¶µç›–å„ç§å¸¸è§è§†é¢‘ç†è§£ä»»åŠ¡çš„9ä¸ªå…¬å…±åŸºå‡†ä¸ŠéªŒè¯äº†æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥çš„è§†é¢‘LLMsæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Video LLMé€šå¸¸é’ˆå¯¹ç‰¹å®šè§†é¢‘ç†è§£ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œä¾‹å¦‚è§†é¢‘æè¿°ã€åŠ¨ä½œè¯†åˆ«ç­‰ï¼Œç¼ºä¹ä¸€ç§èƒ½å¤ŸåŒæ—¶å¤„ç†å…¨å±€è¯­ä¹‰ç†è§£ã€åƒç´ çº§ç»†èŠ‚æ„ŸçŸ¥å’Œæ—¶é—´ç»´åº¦æ¨ç†çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¿™é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œä¾‹å¦‚éœ€è¦ç»“åˆå…¨å±€ä¸Šä¸‹æ–‡è¿›è¡Œç²¾ç»†å®šä½çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä¸åŒç²’åº¦å±‚é¢ä¸Šè¿›è¡ŒååŒç†è§£ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUFVideoçš„æ ¸å¿ƒåœ¨äºè®¾è®¡ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€å¼•å¯¼å¯¹é½æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°å¤„ç†ä¸åŒç²’åº¦çš„è§†é¢‘ç†è§£ä»»åŠ¡ã€‚é€šè¿‡åŠ¨æ€ç¼–ç è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬å“åº”ã€æ—¶é—´å®šä½æˆ–åˆ†å‰²æ©ç ï¼Œå®ç°å…¨å±€ã€åƒç´ å’Œæ—¶é—´å°ºåº¦ä¸Šçš„ååŒç†è§£ã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹æ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªé€‚åº”åœ°è°ƒæ•´å…³æ³¨ç‚¹ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUFVideoçš„æ•´ä½“æ¶æ„åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘ç¼–ç å™¨ï¼šç”¨äºæå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ã€‚2) æ–‡æœ¬ç¼–ç å™¨ï¼šç”¨äºæå–æ–‡æœ¬è¾“å…¥çš„è¯­ä¹‰ä¿¡æ¯ã€‚3) è§†è§‰-è¯­è¨€å¯¹é½æ¨¡å—ï¼šå°†è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå»ºç«‹è·¨æ¨¡æ€çš„å…³è”ã€‚4) ä»»åŠ¡è§£ç å™¨ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹ï¼Œç”Ÿæˆç›¸åº”çš„è¾“å‡ºï¼Œä¾‹å¦‚æ–‡æœ¬æè¿°ã€æ—¶é—´å®šä½æˆ–åˆ†å‰²æ©ç ã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ï¼Œå…è®¸æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨å­¦ä¹ æœ€ä½³çš„ç‰¹å¾è¡¨ç¤ºå’Œå¯¹é½ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šUFVideoæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå…¶ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€å¼•å¯¼å¯¹é½æœºåˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒUFVideoä¸æ˜¯é’ˆå¯¹æ¯ä¸ªä»»åŠ¡å•ç‹¬è®¾è®¡æ¨¡å‹ï¼Œè€Œæ˜¯é‡‡ç”¨ä¸€ç§é€šç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„ç¼–ç æ–¹å¼ï¼Œä»¥åŠä»»åŠ¡è§£ç å™¨çš„ç»“æ„ï¼Œæ¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚è¿™ç§è®¾è®¡ä½¿å¾—UFVideoå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è§†è§‰-è¯­è¨€å¯¹é½æ¨¡å—ä¸­ï¼Œé‡‡ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ¨æ€åœ°è°ƒæ•´è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾çš„æƒé‡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´åŠ å…³æ³¨ä¸ä»»åŠ¡ç›¸å…³çš„éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ›´å¥½åœ°å¤„ç†æ—¶é—´ç»´åº¦ä¸Šçš„ä¿¡æ¯ï¼Œä½¿ç”¨äº†Transformerç»“æ„æ¥å»ºæ¨¡è§†é¢‘å¸§ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé‡‡ç”¨äº†å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹å¼ï¼ŒåŒæ—¶ä¼˜åŒ–æ–‡æœ¬ç”Ÿæˆã€æ—¶é—´å®šä½å’Œåˆ†å‰²æ©ç çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

UFVideoåœ¨UFVideo-Benchä¸Šæ˜¾è‘—ä¼˜äºGPT-4oï¼Œè¯æ˜äº†å…¶åœ¨å¤šç²’åº¦è§†é¢‘ç†è§£æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œåœ¨9ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­ï¼ŒUFVideoä¹Ÿå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼ŒéªŒè¯äº†å…¶åœ¨å„ç§å¸¸è§è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªåœ¨æ‘˜è¦ä¸­æ˜ç¡®ç»™å‡ºï¼Œä½†å¼ºè°ƒäº†å…¶ä¼˜äºGPT-4oçš„ç»“è®ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UFVideoå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€è§†é¢‘ç¼–è¾‘ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºç†è§£ç›‘æ§è§†é¢‘ä¸­çš„å¼‚å¸¸è¡Œä¸ºï¼Œè¾…åŠ©è§†é¢‘ç¼–è¾‘äººå‘˜è¿›è¡Œå†…å®¹åˆ›ä½œï¼Œæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥åŠå¸®åŠ©åŒ»ç”Ÿåˆ†æåŒ»ç–—å½±åƒæ•°æ®ã€‚æœªæ¥ï¼ŒUFVideoæœ‰æœ›æˆä¸ºå„ç§è§†é¢‘ç†è§£åº”ç”¨çš„åŸºç¡€æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.

