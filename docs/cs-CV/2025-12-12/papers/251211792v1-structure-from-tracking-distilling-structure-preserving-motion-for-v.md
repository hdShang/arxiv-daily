---
layout: default
title: Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation
---

# Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.11792" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.11792v1</a>
  <a href="https://arxiv.org/pdf/2512.11792.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11792v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.11792v1', 'Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Fei, George Stoica, Jingyuan Liu, Qifeng Chen, Ranjay Krishna, Xiaojuan Wang, Benlin Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-12

**å¤‡æ³¨**: Project Website: https://sam2videox.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSAM2VideoXï¼Œé€šè¿‡è’¸é¦ç»“æ„ä¿æŒè¿åŠ¨å…ˆéªŒï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `è¿åŠ¨å…ˆéªŒ` `ç»“æ„ä¿æŒ` `è‡ªå›å½’æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹éš¾ä»¥ç”Ÿæˆä¿æŒç»“æ„ä¸€è‡´æ€§çš„è¿åŠ¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é“°æ¥å’Œå¯å˜å½¢ç‰©ä½“æ—¶ï¼Œå•çº¯å¢åŠ æ•°æ®é‡æ— æ³•è§£å†³ã€‚
2. è®ºæ–‡æå‡ºå°†è‡ªå›å½’è§†é¢‘è·Ÿè¸ªæ¨¡å‹SAM2ä¸­çš„ç»“æ„ä¿æŒè¿åŠ¨å…ˆéªŒçŸ¥è¯†ï¼Œæç‚¼åˆ°åŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹CogVideoXä¸­ï¼Œä»è€ŒæŒ‡å¯¼è§†é¢‘ç”Ÿæˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSAM2VideoXåœ¨VBenchå’Œäººç±»è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨ç»“æ„ä¿æŒå’Œè§†é¢‘è´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°å®ä¸–ç•Œæ˜¯åˆšæ€§çº¦æŸå’Œå¯å˜å½¢ç»“æ„çš„ç»“åˆã€‚å¯¹äºè§†é¢‘æ¨¡å‹è€Œè¨€ï¼Œè¿™æ„å‘³ç€ç”Ÿæˆæ—¢èƒ½ä¿æŒé€¼çœŸåº¦åˆèƒ½ä¿æŒç»“æ„çš„è¿åŠ¨ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œä½†ç”Ÿæˆé€¼çœŸçš„ã€ä¿æŒç»“æ„çš„è¿åŠ¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé“°æ¥å¼å’Œå¯å˜å½¢å¯¹è±¡ï¼Œå¦‚äººç±»å’ŒåŠ¨ç‰©ã€‚ä»…ä»…æ‰©å¤§è®­ç»ƒæ•°æ®è§„æ¨¡æœªèƒ½è§£å†³ç‰©ç†ä¸Šä¸åˆç†çš„è¿‡æ¸¡ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºä½¿ç”¨å™ªå£°è¿åŠ¨è¡¨ç¤ºï¼ˆå¦‚å…‰æµæˆ–ä½¿ç”¨å¤–éƒ¨ä¸å®Œå–„æ¨¡å‹æå–çš„éª¨éª¼ï¼‰è¿›è¡Œæ¡ä»¶çº¦æŸã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®—æ³•ï¼Œå°†ç»“æ„ä¿æŒè¿åŠ¨å…ˆéªŒä»è‡ªå›å½’è§†é¢‘è·Ÿè¸ªæ¨¡å‹(SAM2)æç‚¼åˆ°åŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹(CogVideoX)ä¸­ã€‚é€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†SAM2VideoXï¼Œå®ƒåŒ…å«ä¸¤ä¸ªåˆ›æ–°ï¼š(1)ä¸€ä¸ªåŒå‘ç‰¹å¾èåˆæ¨¡å—ï¼Œä»åƒSAM2è¿™æ ·çš„å¾ªç¯æ¨¡å‹ä¸­æå–å…¨å±€ç»“æ„ä¿æŒè¿åŠ¨å…ˆéªŒï¼›(2)ä¸€ä¸ªå±€éƒ¨GramæµæŸå¤±ï¼Œç”¨äºå¯¹é½å±€éƒ¨ç‰¹å¾çš„ç§»åŠ¨æ–¹å¼ã€‚åœ¨VBenchå’Œäººå·¥ç ”ç©¶ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM2VideoXç›¸æ¯”ä¹‹å‰çš„åŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†æŒç»­çš„æå‡ï¼ˆåœ¨VBenchä¸Š+2.60%ï¼ŒFVDé™ä½21-22%ï¼Œäººç±»åå¥½åº¦ä¸º71.4%ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨VBenchä¸Šï¼Œæˆ‘ä»¬è¾¾åˆ°äº†95.51%ï¼Œè¶…è¿‡äº†REPA(92.91%) 2.60%ï¼Œå¹¶å°†FVDé™ä½åˆ°360.57ï¼Œåˆ†åˆ«æ¯”REPAå’ŒLoRAå¾®è°ƒæé«˜äº†21.20%å’Œ22.46%ã€‚é¡¹ç›®ç½‘ç«™ä½äºhttps://sam2videox.github.io/ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„ï¼Œåœ¨ç”ŸæˆåŒ…å«å¤æ‚è¿åŠ¨ï¼ˆå¦‚äººç±»æˆ–åŠ¨ç‰©çš„è¿åŠ¨ï¼‰çš„è§†é¢‘æ—¶ï¼Œéš¾ä»¥ä¿æŒç”Ÿæˆè§†é¢‘ä¸­ç‰©ä½“çš„ç»“æ„ä¸€è‡´æ€§ã€‚ç®€å•åœ°å¢åŠ è®­ç»ƒæ•°æ®å¹¶ä¸èƒ½æœ‰æ•ˆè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè€Œä¸”ç°æœ‰æ–¹æ³•ä¾èµ–äºä¸å®Œç¾çš„å¤–éƒ¨æ¨¡å‹æå–çš„è¿åŠ¨ä¿¡æ¯ï¼ˆå¦‚å…‰æµæˆ–éª¨éª¼ï¼‰ï¼Œè¿™ä¼šå¼•å…¥å™ªå£°å¹¶é™åˆ¶ç”Ÿæˆè´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»ä¸€ä¸ªå·²ç»å…·å¤‡è¾ƒå¥½è·Ÿè¸ªèƒ½åŠ›çš„è‡ªå›å½’æ¨¡å‹ï¼ˆSAM2ï¼‰ä¸­æå–ç»“æ„ä¿æŒçš„è¿åŠ¨å…ˆéªŒï¼Œå¹¶å°†å…¶è¿ç§»åˆ°æ‰©æ•£æ¨¡å‹ï¼ˆCogVideoXï¼‰ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ‰©æ•£æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´çœŸå®çš„è¿åŠ¨æ¨¡å¼ï¼Œä»è€Œç”Ÿæˆç»“æ„æ›´ç¨³å®šçš„è§†é¢‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSAM2VideoXçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) ä½¿ç”¨åŒå‘ç‰¹å¾èåˆæ¨¡å—ä»SAM2ä¸­æå–å…¨å±€ç»“æ„ä¿æŒè¿åŠ¨å…ˆéªŒï¼›2) ä½¿ç”¨å±€éƒ¨GramæµæŸå¤±æ¥å¯¹é½å±€éƒ¨ç‰¹å¾çš„è¿åŠ¨æ–¹å¼ã€‚SAM2é¦–å…ˆä½œä¸ºè¿åŠ¨ä¿¡æ¯çš„æ¥æºï¼Œå…¶è¾“å‡ºé€šè¿‡åŒå‘ç‰¹å¾èåˆæ¨¡å—ï¼Œä¸ºCogVideoXæä¾›å…¨å±€è¿åŠ¨æŒ‡å¯¼ã€‚CogVideoXåˆ™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„æ‰©æ•£æ¨¡å‹ï¼Œè´Ÿè´£ç”Ÿæˆæœ€ç»ˆçš„è§†é¢‘å¸§ã€‚å±€éƒ¨GramæµæŸå¤±ç”¨äºç¡®ä¿ç”Ÿæˆè§†é¢‘ä¸­å±€éƒ¨ç‰¹å¾çš„è¿åŠ¨ä¸SAM2çš„é¢„æµ‹ä¸€è‡´ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†è‡ªå›å½’è·Ÿè¸ªæ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œåˆ©ç”¨è·Ÿè¸ªæ¨¡å‹æä¾›çš„ç»“æ„ä¿æŒè¿åŠ¨å…ˆéªŒæ¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚åŒå‘ç‰¹å¾èåˆæ¨¡å—å’Œå±€éƒ¨GramæµæŸå¤±æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„å…³é”®æŠ€æœ¯æ‰‹æ®µã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é¿å…äº†ç›´æ¥ä½¿ç”¨å™ªå£°è¿åŠ¨ä¿¡æ¯ä½œä¸ºæ¡ä»¶ï¼Œè€Œæ˜¯é€šè¿‡è’¸é¦çš„æ–¹å¼å­¦ä¹ è¿åŠ¨å…ˆéªŒï¼Œä»è€Œæé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œç»“æ„ä¸€è‡´æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåŒå‘ç‰¹å¾èåˆæ¨¡å—çš„å…·ä½“å®ç°ç»†èŠ‚æœªçŸ¥ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œæ¥æ•æ‰SAM2åœ¨æ—¶é—´ä¸Šçš„ä¾èµ–å…³ç³»ï¼Œä»è€Œæå–å…¨å±€è¿åŠ¨ä¿¡æ¯ã€‚å±€éƒ¨GramæµæŸå¤±é€šè¿‡è®¡ç®—ç”Ÿæˆè§†é¢‘å’ŒSAM2é¢„æµ‹çš„å±€éƒ¨ç‰¹å¾ä¹‹é—´çš„GramçŸ©é˜µï¼Œå¹¶æœ€å°åŒ–å®ƒä»¬ä¹‹é—´çš„å·®å¼‚ï¼Œæ¥ä¿è¯å±€éƒ¨è¿åŠ¨çš„ä¸€è‡´æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å½¢å¼å’Œç½‘ç»œç»“æ„ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡åŸæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SAM2VideoXåœ¨VBenchåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†95.51%çš„å¾—åˆ†ï¼Œè¶…è¿‡äº†REPAçš„92.91%ï¼Œæå‡äº†2.60%ã€‚åŒæ—¶ï¼ŒFVDæŒ‡æ ‡é™ä½åˆ°360.57ï¼Œç›¸æ¯”REPAå’ŒLoRAå¾®è°ƒåˆ†åˆ«æå‡äº†21.20%å’Œ22.46%ã€‚äººç±»è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œ71.4%çš„äººæ›´åå¥½SAM2VideoXç”Ÿæˆçš„è§†é¢‘ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¸»è§‚è§†è§‰è´¨é‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šé€¼çœŸçš„äººç‰©åŠ¨ç”»ç”Ÿæˆã€åŠ¨ç‰©è¿åŠ¨æ¨¡æ‹Ÿã€ä»¥åŠå„ç§éœ€è¦ä¿æŒç»“æ„ä¸€è‡´æ€§çš„è§†é¢‘å†…å®¹åˆ›ä½œã€‚å…¶æ½œåœ¨ä»·å€¼åœ¨äºæå‡è§†é¢‘ç”Ÿæˆçš„çœŸå®æ„Ÿå’Œå¯æ§æ€§ï¼Œä¸ºç”µå½±åˆ¶ä½œã€æ¸¸æˆå¼€å‘ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå¸¦æ¥æ–°çš„å¯èƒ½æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯å’Œæ›´ç²¾ç»†çš„è¿åŠ¨æ§åˆ¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .

