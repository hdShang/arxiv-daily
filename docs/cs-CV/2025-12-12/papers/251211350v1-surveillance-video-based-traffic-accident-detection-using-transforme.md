---
layout: default
title: Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture
---

# Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.11350" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.11350v1</a>
  <a href="https://arxiv.org/pdf/2512.11350.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.11350v1" onclick="toggleFavorite(this, '2512.11350v1', 'Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tanu Singh, Pranamesh Chakraborty, Long T. Truong

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºTransformerçš„äº¤é€šè§†é¢‘äº‹æ•…æ£€æµ‹æ¨¡å‹ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡å¹³è¡¡æ•°æ®é›†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `äº¤é€šè§†é¢‘åˆ†æ` `äº‹æ•…æ£€æµ‹` `Transformer` `æ—¶ç©ºå»ºæ¨¡` `å…‰æµ` `æ·±åº¦å­¦ä¹ ` `æ™ºèƒ½äº¤é€š` `è§†é¢‘ç›‘æ§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨äº¤é€šè§†é¢‘äº‹æ•…æ£€æµ‹ä¸­ç¼ºä¹æœ‰æ•ˆçš„æ—¶ç©ºå»ºæ¨¡èƒ½åŠ›ï¼Œæ³›åŒ–æ€§è¾ƒå·®ã€‚
2. æå‡ºä¸€ç§åŸºäºTransformerçš„äº‹æ•…æ£€æµ‹æ¨¡å‹ï¼Œåˆ©ç”¨å·ç§¯æå–å±€éƒ¨ç‰¹å¾ï¼ŒTransformerå»ºæ¨¡æ—¶åºä¾èµ–ã€‚
3. æ„å»ºäº†å¤§è§„æ¨¡å¹³è¡¡æ•°æ®é›†ï¼Œå¹¶ç»“åˆRGBå’Œå…‰æµä¿¡æ¯ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•å–å¾—äº†88.3%çš„å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é“è·¯äº¤é€šäº‹æ•…æ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œå…¶å‘ç”Ÿç‡éšç€äººå£ã€åŸå¸‚åŒ–å’ŒæœºåŠ¨åŒ–çš„å¢é•¿è€Œä¸Šå‡ã€‚æ—¥ç›Šå¢é•¿çš„äº‹æ•…ç‡å¼•å‘äº†å¯¹äº¤é€šç›‘æ§æœ‰æ•ˆæ€§çš„æ‹…å¿§ã€‚ä¼ ç»Ÿçš„è®¡ç®—æœºè§†è§‰äº‹æ•…æ£€æµ‹æ–¹æ³•åœ¨æ—¶ç©ºç†è§£å’Œè·¨é¢†åŸŸæ³›åŒ–æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚Transformeræ¶æ„åœ¨å»ºæ¨¡å…¨å±€æ—¶ç©ºä¾èµ–æ€§å’Œå¹¶è¡Œè®¡ç®—æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºå°å‹ã€éå¤šæ ·åŒ–çš„æ•°æ®é›†çš„é™åˆ¶ï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºè‡ªåŠ¨äº¤é€šäº‹æ•…æ£€æµ‹å—åˆ°é™åˆ¶ï¼Œé˜»ç¢äº†é²æ£’ã€é€šç”¨ç³»ç»Ÿçš„å¼€å‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå…¨é¢ä¸”å¹³è¡¡çš„æ•°æ®é›†ï¼Œæ•æ‰äº†å„ç§äº¤é€šç¯å¢ƒã€äº‹æ•…ç±»å‹å’Œä¸Šä¸‹æ–‡å˜åŒ–ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformeræ¶æ„çš„äº‹æ•…æ£€æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨é¢„æå–çš„ç©ºé—´è§†é¢‘ç‰¹å¾ã€‚è¯¥æ¶æ„é‡‡ç”¨å·ç§¯å±‚æ¥æå–å¸§å†…å„ç§æ¨¡å¼çš„å±€éƒ¨ç›¸å…³æ€§ï¼ŒåŒæ—¶åˆ©ç”¨Transformeræ¥æ•è·æ£€ç´¢åˆ°çš„ç‰¹å¾ä¹‹é—´çš„æ—¶åºä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶å¿½ç•¥äº†è¿åŠ¨çº¿ç´¢çš„æ•´åˆï¼Œè€Œè¿åŠ¨çº¿ç´¢å¯¹äºç†è§£åŠ¨æ€åœºæ™¯è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨äº‹æ•…å‘ç”ŸæœŸé—´ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºé™æ€ç‰¹å¾æˆ–ç²—ç•¥çš„æ—¶é—´ä¿¡æ¯ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œè¯„ä¼°äº†å¤šç§æ•´åˆè¿åŠ¨çº¿ç´¢çš„æ–¹æ³•ï¼Œä»¥ç¡®å®šæœ€æœ‰æ•ˆçš„ç­–ç•¥ã€‚åœ¨æµ‹è¯•çš„è¾“å…¥æ–¹æ³•ä¸­ï¼ŒRGBç‰¹å¾ä¸å…‰æµçš„è¿æ¥å®ç°äº†æœ€é«˜çš„å‡†ç¡®ç‡ï¼Œè¾¾åˆ°88.3%ã€‚ç»“æœè¿˜ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå¦‚GPTã€Geminiå’ŒLLaVA-NeXT-Videoè¿›è¡Œäº†æ¯”è¾ƒï¼Œä»¥è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰äº¤é€šè§†é¢‘äº‹æ•…æ£€æµ‹æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡é•¿æ—¶åºä¾èµ–å…³ç³»ï¼Œä¸”å¯¹ä¸åŒåœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ‰‹å·¥ç‰¹å¾æˆ–æµ…å±‚æ¨¡å‹ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç°æœ‰æ•°æ®é›†è§„æ¨¡è¾ƒå°ä¸”åˆ†å¸ƒä¸å¹³è¡¡ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨Transformeræ¶æ„å¼ºå¤§çš„æ—¶åºå»ºæ¨¡èƒ½åŠ›ï¼Œæ•æ‰è§†é¢‘å¸§ä¹‹é—´çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚åŒæ—¶ï¼Œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œæå–å±€éƒ¨ç©ºé—´ç‰¹å¾ï¼Œèåˆæ—¶ç©ºä¿¡æ¯ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡å¹³è¡¡æ•°æ®é›†ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¼•å…¥å…‰æµä¿¡æ¯ä½œä¸ºè¿åŠ¨çº¿ç´¢ï¼Œå¢å¼ºæ¨¡å‹å¯¹åŠ¨æ€åœºæ™¯çš„ç†è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¨¡å‹é¦–å…ˆä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæå–è§†é¢‘å¸§çš„å±€éƒ¨ç©ºé—´ç‰¹å¾ã€‚ç„¶åï¼Œå°†æå–çš„ç‰¹å¾è¾“å…¥Transformerç¼–ç å™¨ï¼Œå»ºæ¨¡å¸§ä¹‹é—´çš„æ—¶åºä¾èµ–å…³ç³»ã€‚ä¸ºäº†èåˆè¿åŠ¨ä¿¡æ¯ï¼Œå°†RGBç‰¹å¾ä¸å…‰æµç‰¹å¾è¿›è¡Œæ‹¼æ¥ã€‚æœ€åï¼Œä½¿ç”¨åˆ†ç±»å™¨åˆ¤æ–­è§†é¢‘ç‰‡æ®µæ˜¯å¦åŒ…å«äº‹æ•…ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå°†Transformeræ¶æ„åº”ç”¨äºäº¤é€šè§†é¢‘äº‹æ•…æ£€æµ‹ï¼Œå¹¶æœ‰æ•ˆèåˆäº†RGBå’Œå…‰æµä¿¡æ¯ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è§†é¢‘ä¸­çš„æ—¶ç©ºä¾èµ–å…³ç³»ï¼Œæé«˜æ£€æµ‹å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæ„å»ºå¤§è§„æ¨¡å¹³è¡¡æ•°æ®é›†ä¹Ÿæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨Transformerç¼–ç å™¨ä¸­ï¼Œä½¿ç”¨äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ä¸åŒçš„ç‰¹å¾ç»´åº¦ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œä½¿ç”¨äº†é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œä½œä¸ºç‰¹å¾æå–å™¨ã€‚åœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå¹¶å¯¹ä¸åŒç±»åˆ«çš„æ ·æœ¬è¿›è¡Œäº†åŠ æƒï¼Œä»¥è§£å†³æ•°æ®é›†ä¸å¹³è¡¡çš„é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªå»ºçš„å¤§è§„æ¨¡å¹³è¡¡æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå‡†ç¡®ç‡è¾¾åˆ°88.3%ã€‚é€šè¿‡å¯¹æ¯”å®éªŒï¼ŒéªŒè¯äº†Transformeræ¶æ„å’Œå…‰æµä¿¡æ¯èåˆçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¦‚GPTã€Geminiå’ŒLLaVA-NeXT-Videoçš„å¯¹æ¯”ï¼Œä¹Ÿè¯æ˜äº†è¯¥æ–¹æ³•åœ¨äº¤é€šè§†é¢‘äº‹æ•…æ£€æµ‹ä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½äº¤é€šç›‘æ§ç³»ç»Ÿï¼Œå®ç°äº¤é€šäº‹æ•…çš„è‡ªåŠ¨æ£€æµ‹å’Œé¢„è­¦ï¼Œæé«˜é“è·¯å®‰å…¨ç®¡ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–è§†é¢‘ç›‘æ§åœºæ™¯ï¼Œå¦‚å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ã€äººç¾¤è®¡æ•°ç­‰ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥ï¼Œç»“åˆè¾¹ç¼˜è®¡ç®—æŠ€æœ¯ï¼Œå¯å®ç°å®æ—¶äº‹æ•…æ£€æµ‹ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶æä¾›å®‰å…¨ä¿éšœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.

