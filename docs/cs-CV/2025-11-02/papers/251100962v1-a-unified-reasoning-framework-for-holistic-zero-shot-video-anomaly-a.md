---
layout: default
title: A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis
---

# A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.00962" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.00962v1</a>
  <a href="https://arxiv.org/pdf/2511.00962.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.00962v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.00962v1', 'A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dongheng Lin, Mengxue Qu, Kunyang Han, Jianbo Jiao, Xiaojie Jin, Yunchao Wei

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-02

**å¤‡æ³¨**: NeurIPS 2025 poster

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://rathgrith.github.io/Unified_Frame_VAA/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¨ç†æ¡†æ¶ï¼Œå®ç°é›¶æ ·æœ¬è§†é¢‘å¼‚å¸¸äº‹ä»¶çš„æ•´ä½“åˆ†æ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†é¢‘å¼‚å¸¸æ£€æµ‹` `é›¶æ ·æœ¬å­¦ä¹ ` `è§†è§‰è¯­è¨€æ¨¡å‹` `æç¤ºå­¦ä¹ ` `ä»»åŠ¡é“¾` `å¯è§£é‡Šæ€§` `è§†é¢‘ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•ç¼ºä¹å¯¹å¼‚å¸¸åŸå› çš„è§£é‡Šï¼Œä¸”å¯è§£é‡Šæ€§æ–¹æ³•ä¾èµ–æ•°æ®å’Œç‰¹å®šä»»åŠ¡ã€‚
2. è®ºæ–‡æå‡ºç»Ÿä¸€æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æµ‹è¯•æ—¶æ¨ç†è¿æ¥æ—¶é—´æ£€æµ‹ã€ç©ºé—´å®šä½å’Œæ–‡æœ¬è§£é‡Šä»»åŠ¡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œåœ¨è§†é¢‘å¼‚å¸¸æ£€æµ‹ã€å®šä½å’Œè§£é‡Šä»»åŠ¡ä¸Šå‡å–å¾—é¢†å…ˆæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰è§†é¢‘å¼‚å¸¸æ£€æµ‹ç ”ç©¶å¤šå±€é™äºå¸§çº§åˆ«ï¼Œç¼ºä¹å¯¹å¼‚å¸¸åŸå› çš„æ·±å…¥ç†è§£ï¼Œé€šå¸¸ä»…è¾“å‡ºå¸§çº§åˆ«çš„å¼‚å¸¸åˆ†æ•°ï¼Œç¼ºå°‘ç©ºé—´æˆ–è¯­ä¹‰ä¿¡æ¯ã€‚è¿‘æœŸçš„è§†é¢‘å¼‚å¸¸å®šä½å’Œç†è§£æ–¹æ³•è™½ç„¶æé«˜äº†å¯è§£é‡Šæ€§ï¼Œä½†ä»ç„¶ä¾èµ–æ•°æ®å’Œç‰¹å®šä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¨ç†æ¡†æ¶ï¼Œå¼¥åˆäº†æ—¶é—´æ£€æµ‹ã€ç©ºé—´å®šä½å’Œæ–‡æœ¬è§£é‡Šä¹‹é—´çš„å·®è·ã€‚è¯¥æ–¹æ³•åŸºäºé“¾å¼æµ‹è¯•æ—¶æ¨ç†è¿‡ç¨‹ï¼Œä¾æ¬¡è¿æ¥è¿™äº›ä»»åŠ¡ï¼Œä»è€Œå®ç°æ— éœ€é¢å¤–è®­ç»ƒçš„æ•´ä½“é›¶æ ·æœ¬å¼‚å¸¸åˆ†æã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä»»åŠ¡å†…æ¨ç†æ¥ä¼˜åŒ–æ—¶é—´æ£€æµ‹ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡é—´é“¾æ¥è¿›è¡Œç©ºé—´å’Œè¯­ä¹‰ç†è§£ï¼Œä»è€Œåœ¨å®Œå…¨é›¶æ ·æœ¬çš„æƒ…å†µä¸‹æé«˜å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§ã€‚åœ¨æ²¡æœ‰ä»»ä½•é¢å¤–æ•°æ®æˆ–æ¢¯åº¦çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘å¼‚å¸¸æ£€æµ‹ã€å®šä½å’Œè§£é‡ŠåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œä»»åŠ¡é“¾ï¼Œå¯ä»¥é‡Šæ”¾åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œåœ¨å®Œå…¨é›¶æ ·æœ¬çš„æƒ…å†µä¸‹å®ç°å®ç”¨ä¸”å¯è§£é‡Šçš„è§†é¢‘å¼‚å¸¸åˆ†æã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†é¢‘å¼‚å¸¸åˆ†ææ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¸§çº§åˆ«çš„å¼‚å¸¸æ£€æµ‹ï¼Œç¼ºä¹å¯¹å¼‚å¸¸äº‹ä»¶åŸå› çš„æ·±å…¥ç†è§£ï¼Œå¯è§£é‡Šæ€§å·®ã€‚å³ä½¿æ˜¯å¼‚å¸¸å®šä½å’Œç†è§£æ–¹æ³•ï¼Œä¹Ÿé«˜åº¦ä¾èµ–è®­ç»ƒæ•°æ®å’Œç‰¹å®šä»»åŠ¡ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚å› æ­¤ï¼Œå¦‚ä½•å®ç°æ— éœ€é¢å¤–è®­ç»ƒï¼Œå³å¯è¿›è¡Œæ—¶é—´æ£€æµ‹ã€ç©ºé—´å®šä½å’Œè¯­ä¹‰è§£é‡Šçš„æ•´ä½“è§†é¢‘å¼‚å¸¸åˆ†æï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼ˆPromptï¼‰å’Œä»»åŠ¡é“¾ï¼ˆTask Chainingï¼‰ï¼Œå°†æ—¶é—´æ£€æµ‹ã€ç©ºé—´å®šä½å’Œæ–‡æœ¬è§£é‡Šä¸‰ä¸ªä»»åŠ¡ä¸²è”èµ·æ¥ã€‚é€šè¿‡ä»»åŠ¡é—´çš„ç›¸äº’ä½œç”¨ï¼Œå®ç°å¯¹è§†é¢‘å¼‚å¸¸äº‹ä»¶çš„æ•´ä½“ç†è§£ï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **æ—¶é—´æ£€æµ‹**ï¼šåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–è§†é¢‘å¸§çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡è®¾è®¡çš„Promptè¿›è¡Œæ—¶é—´å¼‚å¸¸æ£€æµ‹ã€‚2) **ç©ºé—´å®šä½**ï¼šåŸºäºæ—¶é—´æ£€æµ‹çš„ç»“æœï¼Œåˆ©ç”¨Promptå¼•å¯¼æ¨¡å‹å®šä½å¼‚å¸¸åŒºåŸŸã€‚3) **æ–‡æœ¬è§£é‡Š**ï¼šæ ¹æ®æ—¶é—´å’Œç©ºé—´ä¿¡æ¯ï¼Œç”Ÿæˆå¯¹å¼‚å¸¸äº‹ä»¶çš„æ–‡æœ¬æè¿°ã€‚è¿™ä¸‰ä¸ªé˜¶æ®µé€šè¿‡ä»»åŠ¡é“¾ä¾æ¬¡è¿æ¥ï¼Œå‰ä¸€ä¸ªä»»åŠ¡çš„è¾“å‡ºä½œä¸ºåä¸€ä¸ªä»»åŠ¡çš„è¾“å…¥ï¼Œä»è€Œå®ç°æ•´ä½“çš„å¼‚å¸¸åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡é“¾å°†æ—¶é—´æ£€æµ‹ã€ç©ºé—´å®šä½å’Œæ–‡æœ¬è§£é‡Šä¸‰ä¸ªä»»åŠ¡æ•´åˆåœ¨ä¸€èµ·ï¼Œå®ç°äº†é›¶æ ·æœ¬çš„æ•´ä½“è§†é¢‘å¼‚å¸¸åˆ†æã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ¢¯åº¦ï¼Œå³å¯å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç²¾å¿ƒè®¾è®¡çš„Promptï¼šé’ˆå¯¹æ¯ä¸ªä»»åŠ¡ï¼Œè®¾è®¡äº†ç‰¹å®šçš„Promptï¼Œä»¥å¼•å¯¼é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚2) ä»»åŠ¡é“¾ï¼šé€šè¿‡ä»»åŠ¡é“¾å°†ä¸‰ä¸ªä»»åŠ¡ä¾æ¬¡è¿æ¥ï¼Œå®ç°ä»»åŠ¡é—´çš„ç›¸äº’ä½œç”¨ã€‚3) é›¶æ ·æœ¬è®¾ç½®ï¼šæ•´ä¸ªæ¡†æ¶åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¿›è¡Œè¯„ä¼°ï¼ŒéªŒè¯äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘å¼‚å¸¸æ£€æµ‹ã€å®šä½å’Œè§£é‡ŠåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ¢¯åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œä»»åŠ¡é“¾ï¼Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå®ç°å®ç”¨ä¸”å¯è§£é‡Šçš„è§†é¢‘å¼‚å¸¸åˆ†æã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿ä¿¡æ¯è¯·å‚è€ƒåŸæ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€å·¥ä¸šå®‰å…¨ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸è¡Œä¸ºå¹¶ç”ŸæˆæŠ¥å‘Šï¼›åœ¨å·¥ä¸šå®‰å…¨ä¸­ï¼Œå¯ä»¥æ£€æµ‹ç”Ÿäº§çº¿ä¸Šçš„å¼‚å¸¸æ“ä½œï¼›åœ¨åŒ»ç–—è¯Šæ–­ä¸­ï¼Œå¯ä»¥è¾…åŠ©åŒ»ç”Ÿè¯†åˆ«åŒ»å­¦å½±åƒä¸­çš„å¼‚å¸¸åŒºåŸŸã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨è§†é¢‘å¼‚å¸¸åˆ†ææŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œæé«˜å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Most video-anomaly research stops at frame-wise detection, offering little insight into why an event is abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent video anomaly localization and video anomaly understanding methods improve explainability but remain data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly analysis without any additional training. Specifically, our approach leverages intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding improved interpretability and generalization in a fully zero-shot manner. Without any additional data or gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design with task-wise chaining can unlock the reasoning power of foundation models, enabling practical, interpretable video anomaly analysis in a fully zero-shot manner. Project Page: https://rathgrith.github.io/Unified_Frame_VAA/.

