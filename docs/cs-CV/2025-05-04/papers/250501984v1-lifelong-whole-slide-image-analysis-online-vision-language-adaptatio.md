---
layout: default
title: "Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation"
---

# Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01984" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.01984v1</a>
  <a href="https://arxiv.org/pdf/2505.01984.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01984v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01984v1', 'Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Doanh C. Bui, Hoai Luan Pham, Vu Trung Duong Le, Tuan Hai Vu, Van Duy Tran, Khang Nguyen, Yasuhiko Nakashima

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-04

**Â§áÊ≥®**: IEEE Access (2025)

**DOI**: [10.1109/ACCESS.2025.3580470](https://doi.org/10.1109/ACCESS.2025.3580470)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ADaFGrad‰ª•Ëß£ÂÜ≥WSIÂàÜÊûê‰∏≠ÁöÑÊåÅÁª≠Â≠¶‰π†ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂÖ®ÂàáÁâáÂõæÂÉè` `ÊåÅÁª≠Â≠¶‰π†` `Ê¢ØÂ∫¶Ëí∏È¶è` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `ÁôåÁóáËØäÊñ≠`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂÖ®ÂàáÁâáÂõæÂÉèÊó∂Èù¢‰∏¥Â≠òÂÇ®ÂíåËÆ°ÁÆóËµÑÊ∫êÁöÑÂ∑®Â§ßÊåëÊàòÔºåÈöæ‰ª•ÂÆûÁé∞ÊúâÊïàÁöÑÊåÅÁª≠Â≠¶‰π†„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÁöÑADaFGradÊñπÊ≥ïÈÄöËøáÁªìÂêàËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂíåÊ¢ØÂ∫¶Ëí∏È¶èÊú∫Âà∂ÔºåÊèêÂçá‰∫ÜWSIÂàÜÊûêÁöÑÁªàË∫´Â≠¶‰π†ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåADaFGradÂú®Á±ªÂ¢ûÈáèÂ≠¶‰π†Âú∫ÊôØ‰∏≠ÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü+5.068%ÔºåÂπ∂Âú®Áü•ËØÜ‰øùÁïôÊñπÈù¢Ë°®Áé∞‰ºòÂºÇ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂÖ®ÂàáÁâáÂõæÂÉèÔºàWSIsÔºâÂú®ÁôåÁóáËØäÊñ≠ÂíåÈ¢ÑÂêé‰∏≠ÂèëÊå•ÁùÄÈáçË¶Å‰ΩúÁî®ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Êèê‰æõ‰∫ÜÁªÜËÉûÁ∫ßÂà´ÁöÑÁªÑÁªáÁªÜËäÇ„ÄÇÁÑ∂ËÄåÔºåWSIsÁöÑÂø´ÈÄüÂ¢ûÈïøÂ∏¶Êù•‰∫ÜÂ≠òÂÇ®„ÄÅÂ§ÑÁêÜÂíåÊ®°ÂûãËÆ≠ÁªÉÁ≠âÊñπÈù¢ÁöÑÊåëÊàò„ÄÇÂõ†Ê≠§ÔºåÂºÄÂèëÈíàÂØπWSIÂàÜÊûêÁöÑÁªàË∫´Â≠¶‰π†ÊñπÊ≥ïËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜADaFGradÔºå‰∏ÄÁßçÊó®Âú®Â¢ûÂº∫WSIÂàÜÊûêÁöÑÁªàË∫´Â≠¶‰π†ÁöÑÊñπÊ≥ï„ÄÇÈÄöËøáÂà©Áî®ÁóÖÁêÜËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°ÂûãÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ê°ÜÊû∂Ôºå‰ΩøÂæóÂàáÁâáÁöÑÂå∫ÂüüÁªÑÁªáÁâπÂæÅ‰∏éÈ¢ÑÂÆö‰πâÁöÑÊñáÊú¨ÂéüÂûãÁºìÂÜ≤Âå∫‰πãÈó¥ËÉΩÂ§üËøõË°å‰∫§‰∫í„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ¢ØÂ∫¶Ëí∏È¶èÊú∫Âà∂ÔºåÊ®°Êãü‰∫ÜÂú®ÊåÅÁª≠Â≠¶‰π†ÁéØÂ¢É‰∏≠ÔºåÂàÜÁ±ªÂ§¥ÂèÇÊï∞Áõ∏ÂØπ‰∫éÂàÜÁ±ªÈÄªËæëÁöÑÊ¢ØÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåADaFGradÂú®‰ªÖÁªèËøáÂ∞ëÈáèËÆ≠ÁªÉÂë®ÊúüÂêéÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑWSIÁâπÂÆöÂíå‰º†ÁªüÁöÑÊåÅÁª≠Â≠¶‰π†ÊñπÊ≥ïÔºåÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü+5.068%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂÖ®ÂàáÁâáÂõæÂÉèÔºàWSIÔºâÂàÜÊûê‰∏≠ÁöÑÊåÅÁª≠Â≠¶‰π†ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§ßËßÑÊ®°WSIÊó∂ÔºåÈù¢‰∏¥Â≠òÂÇ®„ÄÅÂ§ÑÁêÜÂíåÊ®°ÂûãËÆ≠ÁªÉÁöÑÊåëÊàòÔºåÂØºËá¥Áü•ËØÜÈÅóÂøòÂíåÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑADaFGradÊñπÊ≥ïÈÄöËøáÁªìÂêàÁóÖÁêÜËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°Âûã‰∏éÊ¢ØÂ∫¶Ëí∏È¶èÊú∫Âà∂ÔºåÂ¢ûÂº∫‰∫ÜWSIÂàÜÊûêÁöÑÁªàË∫´Â≠¶‰π†ËÉΩÂäõ„ÄÇËøôÁßçËÆæËÆ°‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂú®‰∏çÂêåÊó∂Èó¥ÁÇπÊúâÊïàÂú∞Âà©Áî®ÂéÜÂè≤Áü•ËØÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöADaFGradÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö‰∏ÄÊòØÂü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁâπÂæÅ‰∫§‰∫íÊ°ÜÊû∂Ôºå‰∫åÊòØÊ¢ØÂ∫¶Ëí∏È¶èÊú∫Âà∂„ÄÇÁâπÂæÅ‰∫§‰∫íÊ®°ÂùóË¥üË¥£ÊèêÂèñÂàáÁâáÁöÑÂå∫ÂüüÁâπÂæÅÂπ∂‰∏éÊñáÊú¨ÂéüÂûãËøõË°åÂÖ≥ËÅîÔºåËÄåÊ¢ØÂ∫¶Ëí∏È¶èÊ®°ÂùóÂàôÂú®ÊåÅÁª≠Â≠¶‰π†‰∏≠‰øùÊåÅÊ®°ÂûãÁöÑÁü•ËØÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöADaFGradÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÊ¢ØÂ∫¶Ëí∏È¶èÊú∫Âà∂ÔºåËØ•Êú∫Âà∂ËÉΩÂ§üÊúâÊïàÊ®°ÊãüÂàÜÁ±ªÈÄªËæëÁöÑÊ¢ØÂ∫¶Ôºå‰ªéËÄåÂú®‰∏çÂêåËø≠‰ª£‰∏≠‰øùÊåÅÊ®°ÂûãÁöÑÂ≠¶‰π†ËÉΩÂäõ„ÄÇËøô‰∏é‰º†ÁªüÊñπÊ≥ïÁöÑÁü•ËØÜÈÅóÂøòÈóÆÈ¢òÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•Âπ≥Ë°°Êñ∞ÊóßÁü•ËØÜÁöÑÂ≠¶‰π†ÔºåÂêåÊó∂‰ºòÂåñ‰∫ÜÁΩëÁªúÁªìÊûÑ‰ª•ÈÄÇÂ∫îÂ§ßËßÑÊ®°WSIÊï∞ÊçÆÁöÑÂ§ÑÁêÜÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåADaFGradÂú®Á±ªÂ¢ûÈáèÂ≠¶‰π†Âú∫ÊôØ‰∏≠ÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü+5.068%ÔºåÂπ∂Âú®Áü•ËØÜ‰øùÁïôÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåÊòæÁ§∫Âá∫ÊúÄÂ∞èÁöÑÈÅóÂøòÁéá„ÄÇÊ≠§Â§ñÔºåADaFGradÁöÑÂáÜÁ°ÆÁéáÊØîÂü∫Á∫øÊèêÈ´ò‰∫Ü+40.084%ÔºåÈ™åËØÅ‰∫ÜÊâÄÊèêÊ®°ÂùóÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂåªÈô¢Âíå‰∏¥Â∫äÁéØÂ¢É‰∏≠ÁöÑÁôåÁóáËØäÊñ≠Â∑•ÂÖ∑„ÄÇÈÄöËøáÂÆûÁé∞È´òÊïàÁöÑWSIÂàÜÊûêÔºåADaFGradËÉΩÂ§üÂ∏ÆÂä©ÁóÖÁêÜÂ≠¶ÂÆ∂Êõ¥ÂáÜÁ°ÆÂú∞ËøõË°åÁôåÁóáËØäÊñ≠Ôºå‰ªéËÄåÊèêÈ´òÊÇ£ËÄÖÁöÑÊ≤ªÁñóÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÈöèÁùÄÊï∞ÊçÆÈáèÁöÑ‰∏çÊñ≠Â¢ûÂä†ÔºåËØ•ÊñπÊ≥ïÁöÑÊåÅÁª≠Â≠¶‰π†ËÉΩÂäõÂ∞Ü‰ΩøÂÖ∂Âú®Êú™Êù•ÁöÑÂåªÁñóÂ∫îÁî®‰∏≠ÂÖ∑ÊúâÈáçË¶Å‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis and prognosis, as they provide tissue details at the cellular level. However, the rapid growth of computational tasks involving WSIs poses significant challenges. Given that WSIs are gigapixels in size, they present difficulties in terms of storage, processing, and model training. Therefore, it is essential to develop lifelong learning approaches for WSI analysis. In scenarios where slides are distributed across multiple institutes, we aim to leverage them to develop a unified online model as a computational tool for cancer diagnosis in clinical and hospital settings. In this study, we introduce ADaFGrad, a method designed to enhance lifelong learning for whole-slide image (WSI) analysis. First, we leverage pathology vision-language foundation models to develop a framework that enables interaction between a slide's regional tissue features and a predefined text-based prototype buffer. Additionally, we propose a gradient-distillation mechanism that mimics the gradient of a logit with respect to the classification-head parameters across past and current iterations in a continual-learning setting. We construct a sequence of six TCGA datasets for training and evaluation. Experimental results show that ADaFGrad outperforms both state-of-the-art WSI-specific and conventional continual-learning methods after only a few training epochs, exceeding them by up to +5.068% in the class-incremental learning scenario while exhibiting the least forgetting (i.e., retaining the most knowledge from previous tasks). Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy, further demonstrating the effectiveness of the proposed modules.

