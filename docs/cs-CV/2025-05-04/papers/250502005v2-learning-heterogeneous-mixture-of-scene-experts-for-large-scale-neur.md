---
layout: default
title: Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields
---

# Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02005" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02005v2</a>
  <a href="https://arxiv.org/pdf/2505.02005.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02005v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02005v2', 'Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04 (æ›´æ–°: 2025-08-25)

**å¤‡æ³¨**: Accepted by TPAMI

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MiZhenxing/Switch-NeRF)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSwitch-NeRF++ä»¥è§£å†³å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡çš„å¼‚è´¨æ€§ä¸æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡` `ç¥ç»è¾å°„åœº` `å¼‚è´¨æ··åˆä¸“å®¶` `åœºæ™¯åˆ†è§£` `é«˜æ•ˆæ¸²æŸ“` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰NeRFæ–¹æ³•åœ¨å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡ä¸­é¢ä¸´å¯å­¦ä¹ åˆ†è§£ã€åœºæ™¯å¼‚è´¨æ€§å»ºæ¨¡å’Œæ•ˆç‡ç­‰å…³é”®é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºSwitch-NeRF++ï¼Œé€šè¿‡é—¨æ§ç½‘ç»œå®ç°åœºæ™¯åˆ†è§£ï¼Œå¹¶å°†3Dç‚¹åˆ†é…ç»™ä¸“é—¨çš„NeRFä¸“å®¶ï¼Œæå‡å»ºæ¨¡æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡åœºæ™¯æ¸²æŸ“ä¸­å®ç°äº†8å€çš„è®­ç»ƒåŠ é€Ÿå’Œ16å€çš„æ¸²æŸ“åŠ é€Ÿï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ¸²æŸ“ç²¾åº¦ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œé’ˆå¯¹å¤§è§„æ¨¡åœºæ™¯çš„NeRFæ–¹æ³•å¼ºè°ƒäº†åœºæ™¯åˆ†è§£åœ¨å¯æ‰©å±•NeRFä¸­çš„é‡è¦æ€§ã€‚å°½ç®¡å·²æœ‰æ–¹æ³•åœ¨å¯æ‰©å±•æ€§ä¸Šå–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†ä»å­˜åœ¨å¯å­¦ä¹ åˆ†è§£ã€åœºæ™¯å¼‚è´¨æ€§å»ºæ¨¡å’Œå»ºæ¨¡æ•ˆç‡ç­‰å…³é”®é—®é¢˜æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†Switch-NeRF++ï¼Œä¸€ç§å¼‚è´¨æ··åˆå“ˆå¸Œä¸“å®¶ç½‘ç»œï¼ˆHMoHEï¼‰ï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶å†…è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é—¨æ§ç½‘ç»œå­¦ä¹ åœºæ™¯åˆ†è§£ï¼Œå¹¶å°†3Dç‚¹åˆ†é…ç»™ä¸“é—¨çš„NeRFä¸“å®¶ï¼Œé‡‡ç”¨ç¨€ç–é—¨æ§æ··åˆä¸“å®¶ï¼ˆMoEï¼‰NeRFæ¡†æ¶è¿›è¡Œå…±åŒä¼˜åŒ–ã€‚é€šè¿‡å“ˆå¸Œç½‘ç»œå’Œä¸åŒåˆ†è¾¨ç‡èŒƒå›´çš„å¼‚è´¨å“ˆå¸Œä¸“å®¶ï¼ŒSwitch-NeRF++å®ç°äº†å¯¹å¤§è§„æ¨¡åœºæ™¯çš„é«˜æ•ˆå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰å¤§è§„æ¨¡NeRFæ•°æ®é›†å’ŒUrbanBISçš„æ–°æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒé€Ÿåº¦æå‡8å€ï¼Œæ¸²æŸ“é€Ÿåº¦æå‡16å€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡ä¸­çš„å¼‚è´¨æ€§ä¸æ•ˆç‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åœºæ™¯åˆ†è§£å’Œå»ºæ¨¡æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSwitch-NeRF++é€šè¿‡å¼•å…¥é—¨æ§ç½‘ç»œå’Œå¼‚è´¨å“ˆå¸Œä¸“å®¶ï¼Œå®ç°å¯¹åœºæ™¯çš„é«˜æ•ˆåˆ†è§£ä¸å»ºæ¨¡ï¼Œé‡‡ç”¨ç¨€ç–é—¨æ§æ··åˆä¸“å®¶æ¡†æ¶å…±åŒä¼˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é—¨æ§ç½‘ç»œå’Œå¤šä¸ªå¼‚è´¨å“ˆå¸Œä¸“å®¶ï¼Œé—¨æ§ç½‘ç»œè´Ÿè´£åœºæ™¯åˆ†è§£ï¼Œå“ˆå¸Œä¸“å®¶åˆ™æ ¹æ®ä¸åŒåˆ†è¾¨ç‡å¤„ç†3Dç‚¹ï¼Œå½¢æˆé«˜æ•ˆçš„å­¦ä¹ æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†å“ˆå¸Œç½‘ç»œå’Œå¼‚è´¨ä¸“å®¶çš„ç»„åˆï¼Œä½¿å¾—å¯¹å¤§è§„æ¨¡åœºæ™¯çš„å»ºæ¨¡æ›´åŠ çµæ´»å’Œé«˜æ•ˆï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†å¤„ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¸åŒåˆ†è¾¨ç‡çš„å“ˆå¸Œç½‘æ ¼ï¼Œä¼˜åŒ–äº†é—¨æ§ç½‘ç»œçš„å­¦ä¹ è¿‡ç¨‹ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡ç¨€ç–é—¨æ§æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSwitch-NeRF++åœ¨å¤§è§„æ¨¡åœºæ™¯æ¸²æŸ“ä¸­å®ç°äº†8å€çš„è®­ç»ƒåŠ é€Ÿå’Œ16å€çš„æ¸²æŸ“åŠ é€Ÿï¼Œç›¸è¾ƒäºSwitch-NeRFï¼Œæ˜¾è‘—æå‡äº†æ¸²æŸ“ç²¾åº¦ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨åŸå¸‚å»ºæ¨¡ã€è™šæ‹Ÿç°å®å’Œæ¸¸æˆå¼€å‘ç­‰é¢†åŸŸã€‚é€šè¿‡é«˜æ•ˆçš„åœºæ™¯å»ºæ¨¡èƒ½åŠ›ï¼ŒSwitch-NeRF++èƒ½å¤Ÿä¸ºå®æ—¶æ¸²æŸ“å’Œäº¤äº’å¼åº”ç”¨æä¾›æ”¯æŒï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decompose scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released at https://github.com/MiZhenxing/Switch-NeRF.

