---
layout: default
title: Segment Any RGB-Thermal Model with Language-aided Distillation
---

# Segment Any RGB-Thermal Model with Language-aided Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.01950" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.01950v1</a>
  <a href="https://arxiv.org/pdf/2505.01950.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.01950v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.01950v1', 'Segment Any RGB-Thermal Model with Language-aided Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dong Xing, Xianxun Zhu, Wei Zhou, Qika Lin, Hang Yang, Yuqing Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04

**å¤‡æ³¨**: arXiv admin note: text overlap with arXiv:2412.04220 by other authors

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSARTMä»¥è§£å†³RGB-çƒ­æˆåƒè¯­ä¹‰åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `RGB-çƒ­æˆåƒ` `è¯­ä¹‰åˆ†å‰²` `è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦` `å¤šæ¨¡æ€èåˆ` `æ·±åº¦å­¦ä¹ ` `è®¡ç®—æœºè§†è§‰` `æ¨¡å‹å¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„Segment Anything Model (SAM)ä»…åœ¨RGBæ•°æ®ä¸Šè®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨RGB-çƒ­æˆåƒè¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ã€‚
2. æå‡ºSARTMæ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒSAMå¹¶å¼•å…¥è¯­ä¹‰ç†è§£æ¨¡å—å’Œè¯­è¨€ä¿¡æ¯ï¼Œæå‡RGB-Tè¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚
3. åœ¨MFNETã€PST900å’ŒFMBç­‰ä¸‰ä¸ªå¤šæ¨¡æ€RGB-Tè¯­ä¹‰åˆ†å‰²åŸºå‡†ä¸Šï¼ŒSARTMæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸçš„Segment Anything Model (SAM)åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„å®ä¾‹åˆ†å‰²æ€§èƒ½ã€‚ç„¶è€Œï¼ŒSAMä»…åœ¨RGBæ•°æ®ä¸Šè®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨RGB-çƒ­æˆåƒï¼ˆRGB-Tï¼‰è¯­ä¹‰åˆ†å‰²ä¸­çš„ç›´æ¥åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶SARTMï¼Œæ—¨åœ¨å®šåˆ¶å¼ºå¤§çš„SAMä»¥é€‚åº”RGB-Tè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯é‡Šæ”¾SAMçš„æ½œåŠ›ï¼ŒåŒæ—¶ä¸ºRGB-Tæ•°æ®å¯¹å¼•å…¥è¯­ä¹‰ç†è§£æ¨¡å—ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡æ·»åŠ é¢å¤–çš„LoRAå±‚å¯¹åŸå§‹SAMè¿›è¡Œå¾®è°ƒï¼Œä»¥ä¿æŒå…¶å¼ºå¤§çš„æ³›åŒ–å’Œåˆ†å‰²èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥è¯­è¨€ä¿¡æ¯ä½œä¸ºè®­ç»ƒSARTMçš„æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆCMKDï¼‰æ¨¡å—ï¼Œæœ‰æ•ˆå®ç°æ¨¡æ€é€‚åº”ï¼ŒåŒæ—¶ä¿æŒå…¶æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªå¤šæ¨¡æ€RGB-Tè¯­ä¹‰åˆ†å‰²åŸºå‡†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„SARTMåœ¨å„ç§æ¡ä»¶ä¸‹æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³RGB-çƒ­æˆåƒï¼ˆRGB-Tï¼‰è¯­ä¹‰åˆ†å‰²ä¸­ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯SAMåœ¨RGBæ•°æ®è®­ç»ƒä¸‹çš„é€‚ç”¨æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¾®è°ƒSAMå¹¶å¼•å…¥è¯­ä¹‰ç†è§£æ¨¡å—ï¼Œç»“åˆè¯­è¨€ä¿¡æ¯ä½œä¸ºè®­ç»ƒæŒ‡å¯¼ï¼Œæå‡æ¨¡å‹å¯¹RGB-Tæ•°æ®çš„é€‚åº”æ€§å’Œåˆ†å‰²æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSARTMæ¡†æ¶åŒ…æ‹¬å¯¹åŸå§‹SAMçš„å¾®è°ƒã€å¼•å…¥CMKDæ¨¡å—ä»¥å®ç°æ¨¡æ€é€‚åº”ï¼Œä»¥åŠè°ƒæ•´åˆ†å‰²å¤´å’Œå¢åŠ è¾…åŠ©è¯­ä¹‰åˆ†å‰²å¤´ä»¥æ•´åˆå¤šå°ºåº¦ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¼•å…¥CMKDæ¨¡å—æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡æ¨¡å‹åœ¨ä¸åŒè§†è§‰æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæ·»åŠ äº†LoRAå±‚ä»¥ä¿æŒSAMçš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶è°ƒæ•´äº†åˆ†å‰²å¤´ä»¥å¢å¼ºæ€§èƒ½ï¼Œé‡‡ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆç­–ç•¥ä»¥æé«˜åˆ†å‰²æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨MFNETã€PST900å’ŒFMBä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSARTMåœ¨å¤šç§æ¡ä»¶ä¸‹çš„åˆ†å‰²æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°10%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨RGB-Tè¯­ä¹‰åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„åœºæ™¯ç†è§£ï¼Œå¦‚ä½å…‰ç…§å’Œè¿‡æ›æ¡ä»¶ä¸‹çš„å›¾åƒå¤„ç†ã€‚SARTMæ¡†æ¶å¯ç”¨äºè‡ªåŠ¨é©¾é©¶ã€ç›‘æ§ç³»ç»ŸåŠæ— äººæœºå›¾åƒåˆ†æç­‰é¢†åŸŸï¼Œæå‡å¤šæ¨¡æ€æ•°æ®çš„å¤„ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒSARTMå¯èƒ½åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The recent Segment Anything Model (SAM) demonstrates strong instance segmentation performance across various downstream tasks. However, SAM is trained solely on RGB data, limiting its direct applicability to RGB-thermal (RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low light and overexposure, we propose a novel framework, SARTM, which customizes the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash the potential of SAM while introduce semantic understanding modules for RGB-T data pairs. Specifically, our framework first involves fine tuning the original SAM by adding extra LoRA layers, aiming at preserving SAM's strong generalization and segmentation capabilities for downstream tasks. Secondly, we introduce language information as guidance for training our SARTM. To address cross-modal inconsistencies, we introduce a Cross-Modal Knowledge Distillation(CMKD) module that effectively achieves modality adaptation while maintaining its generalization capabilities. This semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. Furthermore, we enhance the segmentation performance by adjusting the segmentation head of SAM and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. Extensive experiments are conducted across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900, and FMB. Both quantitative and qualitative results consistently demonstrate that the proposed SARTM significantly outperforms state-of-the-art approaches across a variety of conditions.

