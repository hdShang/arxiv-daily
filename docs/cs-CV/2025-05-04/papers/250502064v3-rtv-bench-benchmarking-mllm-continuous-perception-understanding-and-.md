---
layout: default
title: "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video"
---

# RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02064" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02064v3</a>
  <a href="https://arxiv.org/pdf/2505.02064.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02064v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02064v3', 'RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuhang Xun, Sicheng Tao, Jungang Li, Yibo Shi, Zhixin Lin, Zhanhui Zhu, Yibo Yan, Hanqian Li, Linghao Zhang, Shikang Wang, Yixin Liu, Hanbo Zhang, Ying Ma, Xuming Hu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: Accepted by NeurIPS 2025 Datasets and Benchmarks Track;

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/LJungang/RTV-Bench)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRTV-Benchä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®æ—¶è§†é¢‘åˆ†æä¸­çš„è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å®æ—¶è§†é¢‘åˆ†æ` `å¤šæ—¶é—´æˆ³é—®ç­”` `å±‚æ¬¡åŒ–é—®é¢˜ç»“æ„` `å¤šç»´åº¦è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºå‡†æœªèƒ½å……åˆ†è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­æŒç»­æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†çš„èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºRTV-Benchï¼Œé€šè¿‡å¤šæ—¶é—´æˆ³é—®ç­”ã€å±‚æ¬¡åŒ–é—®é¢˜ç»“æ„å’Œå¤šç»´åº¦è¯„ä¼°æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¼€æºå®æ—¶æ¨¡å‹åœ¨RTV-Benchä¸Šè¡¨ç°ä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œä½†ä»ä¸åŠé¡¶å°–ä¸“æœ‰æ¨¡å‹ï¼Œä¸”æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½æå‡ä¹‹é—´çš„å…³ç³»å¤æ‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰åŸºå‡†æœªèƒ½å……åˆ†è¯„ä¼°å…¶åœ¨åŠ¨æ€çœŸå®ç¯å¢ƒä¸­çš„æŒç»­ä»»åŠ¡èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†RTV-Benchï¼Œä¸€ä¸ªé’ˆå¯¹MLLMå®æ—¶è§†é¢‘åˆ†æçš„ç»†ç²’åº¦åŸºå‡†ã€‚RTV-BenchåŸºäºä¸‰ä¸ªå…³é”®åŸåˆ™ï¼šå¤šæ—¶é—´æˆ³é—®ç­”ï¼ˆMTQAï¼‰ã€å±‚æ¬¡åŒ–é—®é¢˜ç»“æ„å’Œå¤šç»´åº¦è¯„ä¼°ã€‚è¯¥åŸºå‡†åŒ…å«552ä¸ªå¤šæ ·åŒ–è§†é¢‘ï¼ˆ167.2å°æ—¶ï¼‰å’Œ4631ä¸ªé«˜è´¨é‡é—®ç­”å¯¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼€æºå®æ—¶æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œä½†ä»è½åäºé¡¶å°–çš„ä¸“æœ‰æ¨¡å‹ã€‚åˆ†æè¿˜è¡¨æ˜ï¼Œæ¨¡å‹è§„æ¨¡æˆ–å¸§é‡‡æ ·ç‡çš„æå‡å¹¶æœªæ˜¾è‘—æ”¹å–„RTV-Benchçš„è¡¨ç°ï¼Œåè€Œå¯èƒ½å¯¼è‡´è½»å¾®ä¸‹é™ï¼Œè¿™çªæ˜¾äº†ä¼˜åŒ–è§†é¢‘æµå¤„ç†å’Œé•¿åºåˆ—çš„æ¨¡å‹æ¶æ„çš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰åŸºå‡†æ— æ³•æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€è§†é¢‘åˆ†æä¸­æŒç»­æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•åœ¨çœŸå®ç¯å¢ƒä¸­çš„åº”ç”¨æ•ˆæœä¸è¶³ï¼Œæ— æ³•æ»¡è¶³å®é™…éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRTV-Benchçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¤šæ—¶é—´æˆ³é—®ç­”ï¼ˆMTQAï¼‰å’Œå±‚æ¬¡åŒ–é—®é¢˜ç»“æ„ï¼Œç»“åˆå¤šç»´åº¦è¯„ä¼°ï¼Œå…¨é¢è€ƒå¯Ÿæ¨¡å‹åœ¨è§†é¢‘åˆ†æä¸­çš„è¡¨ç°ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ æ¨¡å‹åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRTV-Benchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è§†é¢‘æ•°æ®æ”¶é›†ã€é—®ç­”å¯¹ç”Ÿæˆå’Œè¯„ä¼°æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†å¤šæ ·åŒ–çš„è§†é¢‘æ•°æ®ï¼Œç„¶åç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œæœ€åé€šè¿‡å¤šç»´åº¦è¯„ä¼°ä½“ç³»å¯¹æ¨¡å‹è¿›è¡Œç»¼åˆè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šRTV-Benchçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šæ—¶é—´æˆ³é—®ç­”æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹çš„å›ç­”èƒ½å¤Ÿéšç€åœºæ™¯å˜åŒ–è€ŒåŠ¨æ€è°ƒæ•´ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿé™æ€é—®ç­”è¯„ä¼°æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´çœŸå®åœ°åæ˜ æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡ä¸Šï¼ŒRTV-Benché‡‡ç”¨äº†552ä¸ªè§†é¢‘å’Œ4631ä¸ªé—®ç­”å¯¹ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œé«˜è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯„ä¼°è¿‡ç¨‹ä¸­è€ƒè™‘äº†æ¨¡å‹çš„å®æ—¶æ€§å’Œè¿ç»­æ€§ï¼Œå¼ºè°ƒäº†å¯¹é•¿åºåˆ—å¤„ç†èƒ½åŠ›çš„è¦æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼€æºå®æ—¶æ¨¡å‹åœ¨RTV-Benchä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œå…·ä½“è€Œè¨€ï¼Œå¼€æºå®æ—¶æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡å–å¾—äº†è¶…è¿‡20%çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå°½ç®¡æ¨¡å‹è§„æ¨¡å¢å¤§æˆ–å¸§é‡‡æ ·ç‡æé«˜ï¼Œæ€§èƒ½æå‡å¹¶ä¸æ˜æ˜¾ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹å‡ºç°è½»å¾®ä¸‹é™ï¼Œæç¤ºäº†æ¨¡å‹æ¶æ„ä¼˜åŒ–çš„å¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RTV-Benchçš„æå‡ºä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®æ—¶è§†é¢‘åˆ†æä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è¯„ä¼°æ ‡å‡†ï¼Œå…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚è¯¥åŸºå‡†å¯ç”¨äºæ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘å†…å®¹åˆ†æç­‰é¢†åŸŸï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹æ¶æ„çš„ä¼˜åŒ–ï¼ŒRTV-Benchæœ‰æœ›è¿›ä¸€æ­¥æå‡å®æ—¶è§†é¢‘åˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) increasingly excel at perception, understanding, and reasoning. However, current benchmarks inadequately evaluate their ability to perform these tasks continuously in dynamic, real-world environments. To bridge this gap, we introduce RTV-Bench, a fine-grained benchmark for MLLM real-time video analysis. RTV-Bench uses three key principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve with scene changes; (2) Hierarchical Question Structure, combining basic and advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability of continuous perception, understanding, and reasoning. RTV-Bench contains 552 diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline (Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5, InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source real-time models largely outperform offline ones but still trail top proprietary models. Our analysis also reveals that larger model size or higher frame sampling rates do not significantly boost RTV-Bench performance, sometimes causing slight decreases. This underscores the need for better model architectures optimized for video stream processing and long sequences to advance real-time video analysis with MLLMs. Our benchmark toolkit is available at: https://github.com/LJungang/RTV-Bench.

