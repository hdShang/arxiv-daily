---
layout: default
title: "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation"
---

# R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02018" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02018v1</a>
  <a href="https://arxiv.org/pdf/2505.02018.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02018v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02018v1', 'R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-Wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, Shi-min Hu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04

**å¤‡æ³¨**: 18pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR-Benchä»¥è¯„ä¼°å¤šå­¦ç§‘å¤æ‚æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤æ‚æ¨ç†` `å¤šå­¦ç§‘è¯„ä¼°` `å¤šæ¨¡æ€å­¦ä¹ ` `è¯­è¨€æ¨¡å‹` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨ç†åŸºå‡†æ— æ³•æœ‰æ•ˆè¯„ä¼°å¤æ‚ç°å®é—®é¢˜æ‰€éœ€çš„ç»†è‡´æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤šå­¦ç§‘å’Œå¤šæ¨¡æ€èƒŒæ™¯ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºR-BenchåŸºå‡†ï¼Œæ¶µç›–å¤šå­¦ç§‘é—®é¢˜ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°è¯­è¨€å’Œå¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰å…ˆè¿›æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯å¤šæ¨¡æ€æ¨ç†ï¼ŒOpenAI o1çš„å‡†ç¡®ç‡ä»…ä¸º53.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨ç†æ˜¯æ™ºèƒ½çš„åŸºçŸ³ï¼Œä½¿å¾—ç°æœ‰çŸ¥è¯†å¾—ä»¥ç»¼åˆä»¥è§£å†³å¤æ‚é—®é¢˜ã€‚å°½ç®¡å·²æœ‰æ˜¾è‘—è¿›å±•ï¼Œç°æœ‰æ¨ç†åŸºå‡†å¾€å¾€æ— æ³•ä¸¥æ ¼è¯„ä¼°å¤æ‚ç°å®é—®é¢˜è§£å†³æ‰€éœ€çš„ç»†è‡´æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤šå­¦ç§‘å’Œå¤šæ¨¡æ€èƒŒæ™¯ä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç ”ç©¶ç”Ÿçº§åˆ«çš„å¤šå­¦ç§‘åŸºå‡†R-Benchï¼Œç”¨äºè¯„ä¼°è¯­è¨€å’Œå¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚R-Benchæ¶µç›–äº†1,094ä¸ªé—®é¢˜ï¼Œæ¶‰åŠ108ä¸ªå­¦ç§‘ç”¨äºè¯­è¨€æ¨¡å‹è¯„ä¼°ï¼Œä»¥åŠ665ä¸ªé—®é¢˜ï¼Œæ¶‰åŠ83ä¸ªå­¦ç§‘ç”¨äºå¤šæ¨¡æ€æ¨¡å‹æµ‹è¯•ï¼Œæ”¯æŒä¸­è‹±æ–‡è¯„ä¼°ã€‚è¿™äº›é—®é¢˜ç»è¿‡ç²¾å¿ƒç­–åˆ’ï¼Œä»¥ç¡®ä¿ä¸¥æ ¼çš„éš¾åº¦æ ¡å‡†ã€å­¦ç§‘å¹³è¡¡å’Œè·¨è¯­è¨€å¯¹é½ï¼Œä½¿è¯„ä¼°æˆä¸ºå¥¥æ—åŒ¹å…‹çº§åˆ«çš„å¤šå­¦ç§‘åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…ˆè¿›æ¨¡å‹åœ¨å¤æ‚æ¨ç†ï¼Œå°¤å…¶æ˜¯å¤šæ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œç”šè‡³è¡¨ç°æœ€ä½³çš„OpenAI o1åœ¨å¤šæ¨¡æ€è¯„ä¼°ä¸­ä»…è¾¾åˆ°53.2%çš„å‡†ç¡®ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†åŸºå‡†æ— æ³•æœ‰æ•ˆè¯„ä¼°å¤æ‚æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šå­¦ç§‘å’Œå¤šæ¨¡æ€èƒŒæ™¯ä¸‹çš„åº”ç”¨åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•åœ¨éš¾åº¦å’Œå­¦ç§‘è¦†ç›–ä¸Šå­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³çœŸå®ä¸–ç•Œçš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„R-BenchåŸºå‡†é€šè¿‡ç²¾å¿ƒç­–åˆ’çš„å¤šå­¦ç§‘é—®é¢˜ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°è¯­è¨€å’Œå¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç¡®ä¿è¯„ä¼°çš„ä¸¥è°¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR-Benchçš„æ•´ä½“æ¶æ„åŒ…æ‹¬é—®é¢˜è®¾è®¡ã€éš¾åº¦æ ¡å‡†å’Œè·¨è¯­è¨€å¯¹é½ç­‰å¤šä¸ªæ¨¡å—ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œç§‘å­¦æ€§ã€‚é—®é¢˜æ¶µç›–å¤šä¸ªå­¦ç§‘ï¼Œæ”¯æŒä¸­è‹±æ–‡è¯„ä¼°ï¼Œé€‚ç”¨äºä¸åŒç±»å‹çš„æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šR-Benchçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤šå­¦ç§‘å’Œå¤šæ¨¡æ€çš„ç»¼åˆè¯„ä¼°èƒ½åŠ›ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨å¤æ‚æ¨ç†è¯„ä¼°ä¸­çš„ç©ºç™½ï¼Œæä¾›äº†æ›´é«˜æ ‡å‡†çš„è¯„ä¼°æ¡†æ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œé—®é¢˜çš„éš¾åº¦ã€å­¦ç§‘å¹³è¡¡å’Œè·¨è¯­è¨€å¯¹é½æ˜¯å…³é”®å‚æ•°ï¼Œç¡®ä¿äº†è¯„ä¼°çš„ä¸¥è°¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚é—®é¢˜çš„é€‰æ‹©ç»è¿‡ä¸¥æ ¼ç­›é€‰ï¼Œä»¥ä¿è¯å…¶ä»£è¡¨æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä½¿ç”¨äº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚OpenAI o1å’ŒGPT-4oï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„è¡¨ç°ä»ç„¶ä¸å°½å¦‚äººæ„ï¼ŒOpenAI o1ä»…è·å¾—53.2%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜å¤æ‚æ¨ç†èƒ½åŠ›çš„æå‡ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R-BenchåŸºå‡†çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€äººå·¥æ™ºèƒ½æ¨¡å‹è¯„ä¼°å’Œå¤šæ¨¡æ€å­¦ä¹ ç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œæå‡æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available at here.

