---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-04
---

# cs.CVï¼ˆ2025-05-04ï¼‰

ğŸ“Š å…± **16** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250502175v1-sparsplat-fast-multi-view-reconstruction-with-generalizable-2d-gauss.html">SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting</a></td>
  <td>æå‡ºSparSplatä»¥è§£å†³ç¨€ç–è§†å›¾ä¸‹çš„3Dé‡å»ºä¸æ–°è§†å›¾åˆæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02175v1" data-paper-url="./papers/250502175v1-sparsplat-fast-multi-view-reconstruction-with-generalizable-2d-gauss.html" onclick="toggleFavorite(this, '2505.02175v1', 'SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250502126v2-garmentgs-point-cloud-guided-gaussian-splatting-for-high-fidelity-no.html">GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction</a></td>
  <td>æå‡ºGarmentGSä»¥è§£å†³é«˜ä¿çœŸéå¯†é—­3Dæœè£…é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02126v2" data-paper-url="./papers/250502126v2-garmentgs-point-cloud-guided-gaussian-splatting-for-high-fidelity-no.html" onclick="toggleFavorite(this, '2505.02126v2', 'GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250502075v1-benchmarking-feature-upsampling-methods-for-vision-foundation-models.html">Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation</a></td>
  <td>æå‡ºäº¤äº’å¼åˆ†å‰²åŸºå‡†ä»¥è¯„ä¼°è§†è§‰åŸºç¡€æ¨¡å‹çš„ç‰¹å¾ä¸Šé‡‡æ ·æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02075v1" data-paper-url="./papers/250502075v1-benchmarking-feature-upsampling-methods-for-vision-foundation-models.html" onclick="toggleFavorite(this, '2505.02075v1', 'Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250502108v1-signsplat-rendering-sign-language-via-gaussian-splatting.html">SignSplat: Rendering Sign Language via Gaussian Splatting</a></td>
  <td>æå‡ºSignSplatä»¥è§£å†³æ‰‹è¯­æ¸²æŸ“ä¸­çš„å¤æ‚è¿åŠ¨å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02108v1" data-paper-url="./papers/250502108v1-signsplat-rendering-sign-language-via-gaussian-splatting.html" onclick="toggleFavorite(this, '2505.02108v1', 'SignSplat: Rendering Sign Language via Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250502005v2-learning-heterogeneous-mixture-of-scene-experts-for-large-scale-neur.html">Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields</a></td>
  <td>æå‡ºSwitch-NeRF++ä»¥è§£å†³å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡çš„å¼‚è´¨æ€§ä¸æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02005v2" data-paper-url="./papers/250502005v2-learning-heterogeneous-mixture-of-scene-experts-for-large-scale-neur.html" onclick="toggleFavorite(this, '2505.02005v2', 'Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250502178v4-sparfels-fast-reconstruction-from-sparse-unposed-imagery.html">Sparfels: Fast Reconstruction from Sparse Unposed Imagery</a></td>
  <td>æå‡ºSparseè§†å›¾é‡å»ºæ–¹æ³•ä»¥è§£å†³ç¨€ç–æ— å§¿æ€å›¾åƒé‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02178v4" data-paper-url="./papers/250502178v4-sparfels-fast-reconstruction-from-sparse-unposed-imagery.html" onclick="toggleFavorite(this, '2505.02178v4', 'Sparfels: Fast Reconstruction from Sparse Unposed Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250502109v1-unaligned-rgb-guided-hyperspectral-image-super-resolution-with-spati.html">Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance</a></td>
  <td>æå‡ºç©ºé—´-å…‰è°±ä¸€è‡´æ€§æ¡†æ¶ä»¥è§£å†³æœªå¯¹é½RGBå¼•å¯¼çš„é«˜å…‰è°±å›¾åƒè¶…åˆ†è¾¨ç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">HSI</span> <span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02109v1" data-paper-url="./papers/250502109v1-unaligned-rgb-guided-hyperspectral-image-super-resolution-with-spati.html" onclick="toggleFavorite(this, '2505.02109v1', 'Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250502079v1-handocc-nerf-based-hand-rendering-with-occupancy-networks.html">HandOcc: NeRF-based Hand Rendering with Occupancy Networks</a></td>
  <td>æå‡ºHandOccæ¡†æ¶ä»¥è§£å†³æ‰‹éƒ¨æ¸²æŸ“ä¸­çš„ç½‘æ ¼ä¾èµ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02079v1" data-paper-url="./papers/250502079v1-handocc-nerf-based-hand-rendering-with-occupancy-networks.html" onclick="toggleFavorite(this, '2505.02079v1', 'HandOcc: NeRF-based Hand Rendering with Occupancy Networks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/250501973v1-visual-dominance-and-emerging-multimodal-approaches-in-distracted-dr.html">Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques</a></td>
  <td>æå‡ºå¤šæ¨¡æ€æ–¹æ³•ä»¥è§£å†³é©¾é©¶åˆ†å¿ƒæ£€æµ‹çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01973v1" data-paper-url="./papers/250501973v1-visual-dominance-and-emerging-multimodal-approaches-in-distracted-dr.html" onclick="toggleFavorite(this, '2505.01973v1', 'Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250502064v3-rtv-bench-benchmarking-mllm-continuous-perception-understanding-and-.html">RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video</a></td>
  <td>æå‡ºRTV-Benchä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®æ—¶è§†é¢‘åˆ†æä¸­çš„è¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02064v3" data-paper-url="./papers/250502064v3-rtv-bench-benchmarking-mllm-continuous-perception-understanding-and-.html" onclick="toggleFavorite(this, '2505.02064v3', 'RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250501958v1-a-comprehensive-analysis-for-visual-object-hallucination-in-large-vi.html">A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models</a></td>
  <td>åˆ†æè§†è§‰å¯¹è±¡å¹»è§‰é—®é¢˜å¹¶æå‡ºç¼“è§£æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01958v1" data-paper-url="./papers/250501958v1-a-comprehensive-analysis-for-visual-object-hallucination-in-large-vi.html" onclick="toggleFavorite(this, '2505.01958v1', 'A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250502018v1-r-bench-graduate-level-multi-disciplinary-benchmarks-for-llm-mllm-co.html">R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation</a></td>
  <td>æå‡ºR-Benchä»¥è¯„ä¼°å¤šå­¦ç§‘å¤æ‚æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02018v1" data-paper-url="./papers/250502018v1-r-bench-graduate-level-multi-disciplinary-benchmarks-for-llm-mllm-co.html" onclick="toggleFavorite(this, '2505.02018v1', 'R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250501984v1-lifelong-whole-slide-image-analysis-online-vision-language-adaptatio.html">Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation</a></td>
  <td>æå‡ºADaFGradä»¥è§£å†³WSIåˆ†æä¸­çš„æŒç»­å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01984v1" data-paper-url="./papers/250501984v1-lifelong-whole-slide-image-analysis-online-vision-language-adaptatio.html" onclick="toggleFavorite(this, '2505.01984v1', 'Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250501950v1-segment-any-rgb-thermal-model-with-language-aided-distillation.html">Segment Any RGB-Thermal Model with Language-aided Distillation</a></td>
  <td>æå‡ºSARTMä»¥è§£å†³RGB-çƒ­æˆåƒè¯­ä¹‰åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01950v1" data-paper-url="./papers/250501950v1-segment-any-rgb-thermal-model-with-language-aided-distillation.html" onclick="toggleFavorite(this, '2505.01950v1', 'Segment Any RGB-Thermal Model with Language-aided Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250502013v1-mllm-enhanced-face-forgery-detection-a-vision-language-fusion-soluti.html">MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution</a></td>
  <td>æå‡ºVLF-FFDä»¥è§£å†³æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„å¤šæ¨¡æ€èåˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02013v1" data-paper-url="./papers/250502013v1-mllm-enhanced-face-forgery-detection-a-vision-language-fusion-soluti.html" onclick="toggleFavorite(this, '2505.02013v1', 'MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250502161v1-focus-what-matters-matchability-based-reweighting-for-local-feature-.html">Focus What Matters: Matchability-Based Reweighting for Local Feature Matching</a></td>
  <td>æå‡ºåŸºäºåŒ¹é…æ€§é‡åŠ æƒçš„å±€éƒ¨ç‰¹å¾åŒ¹é…æ–¹æ³•ä»¥æå‡åŒ¹é…ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02161v1" data-paper-url="./papers/250502161v1-focus-what-matters-matchability-based-reweighting-for-local-feature-.html" onclick="toggleFavorite(this, '2505.02161v1', 'Focus What Matters: Matchability-Based Reweighting for Local Feature Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)