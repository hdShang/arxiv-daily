---
layout: default
title: Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models
---

# Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10104" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.10104v1</a>
  <a href="https://arxiv.org/pdf/2510.10104.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10104v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10104v1', 'Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Minbin Huang, Runhui Huang, Chuanyang Zheng, Jingyao Li, Guoxuan Chen, Han Shi, Hong Cheng

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-11

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ACREÔºåÈÄöËøá‰∏ÄËá¥ÊÄßÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÂú®ËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑÊé®ÁêÜ‰∏ÄËá¥ÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ËßÜËßâÈóÆÁ≠î` `Êé®ÁêÜ‰∏ÄËá¥ÊÄß` `ËßÜÈ¢ëÊé®ÁêÜ` `Êï∞Â≠¶Êé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÁªìÊûúÈ©±Âä®ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂú®ÊèêÂçáÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁ≠îÊ°àÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåÂèØËÉΩÂØºËá¥Êé®ÁêÜÈìæ‰∏éÊúÄÁªàÁ≠îÊ°à‰∏ç‰∏ÄËá¥„ÄÇ
2. ACREÈÄöËøáÂºïÂÖ•‰∏ÄËá¥ÊÄßÈ™åËØÅÂ•ñÂä±ÔºåÈºìÂä±Ê®°ÂûãÂú®Á≠îÊ°àÈÄâÈ°πÊ¥óÁâåÂêéÔºåÂü∫‰∫éÁõ∏ÂêåÊé®ÁêÜÈìæÁªôÂá∫‰∏ÄËá¥ÁöÑÁ≠îÊ°àÔºå‰ªéËÄåÊèêÂçáÊé®ÁêÜÁöÑÂèØÈù†ÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåACREÂú®ËßÜÈ¢ëÊé®ÁêÜÂíåÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏äÔºåÁõ∏ËæÉ‰∫éGRPOÂü∫Á∫øÔºåÂàÜÂà´ÂèñÂæó‰∫ÜÂπ≥Âùá2.2%Âíå1.5%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊúÄÊñ∞ËøõÂ±ïË°®ÊòéÔºå‰ΩøÁî®ÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâÂèØ‰ª•ÈÄöËøáÁõ¥Êé•‰ºòÂåñÊ≠£Á°ÆÊÄßÊù•ÊòæËëóÂ¢ûÂº∫Êé®ÁêÜËÉΩÂäõÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖ‰æùËµñ‰∫éÁõëÁù£Ê®°‰ªø„ÄÇËøôÁßçËåÉÂºèÂ∑≤ÁªèÊâ©Â±ïÂà∞Â§öÊ®°ÊÄÅLLMÔºåÁî®‰∫éÂ§çÊùÇÁöÑËßÜÈ¢ëÂíåÂõæÂÉèÁêÜËß£‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåËôΩÁÑ∂ÁªìÊûúÈ©±Âä®ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊèêÈ´ò‰∫ÜÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄßÔºå‰ΩÜÂÆÉÂèØËÉΩ‰ºöÊó†ÊÑè‰∏≠Â∞ÜÊé®ÁêÜÈìæ‰∏éÊúÄÁªàÁ≠îÊ°àÂàÜÁ¶ªÔºåÂØºËá¥Ê®°ÂûãÂú®Êé®ÁêÜËøáÁ®ãÂíåÊúÄÁªàÁ≠îÊ°à‰πãÈó¥‰∫ßÁîü‰∏ç‰∏ÄËá¥„ÄÇÂú®Â§öÈ°πÈÄâÊã©ËßÜËßâÈóÆÁ≠î‰ªªÂä°ÁöÑÂÆûÈ™å‰∏≠ÔºåÊ†áÂáÜÁöÑGRPOÊñπÊ≥ïÂú®MMVU‰∏ä‰ªÖ‰∫ßÁîü79.7%ÁöÑÊé®ÁêÜÊ≠•È™§ÂíåÊâÄÈÄâÁ≠îÊ°à‰πãÈó¥ÁöÑ‰∏ÄËá¥ÊÄßÔºåË°®ÊòéÁ≠îÊ°àÂíåÊé®ÁêÜ‰πãÈó¥ÁªèÂ∏∏‰∏çÂåπÈÖç„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁ≠îÊ°à‰∏ÄËá¥ÊÄßÂº∫ÂåñÂ≠¶‰π†ÔºàACREÔºâÔºåÂÆÉ‰ΩøÁî®ËæÖÂä©‰∏ÄËá¥ÊÄßÊ£ÄÊü•Êù•‰øÆÊîπGRPOÁÆóÊ≥ï„ÄÇÂú®Ê®°Âûã‰∏∫ÁªôÂÆöÈóÆÈ¢òÁîüÊàêÊÄùÁª¥ÈìæÂíåÂàùÂßãÁ≠îÊ°àÂêéÔºåÊàë‰ª¨ÂØπÁ≠îÊ°àÈÄâÈ°πËøõË°åÊ¥óÁâåÔºåÂπ∂‰ΩøÁî®Áõ∏ÂêåÁöÑÊé®ÁêÜËøáÁ®ãÂÜçÊ¨°ÊèêÁ§∫Ê®°Âûã‰ª•È¢ÑÊµãÁ¨¨‰∫å‰∏™Á≠îÊ°à„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßç‰∏ÄËá¥ÊÄßÈ™åËØÅÂ•ñÂä±ÔºåÂè™ÊúâÂΩìÂéüÂßãÁ≠îÊ°àÂíåÊ¥óÁâåÂêéÁöÑÁ≠îÊ°à‰∏ÄËá¥‰∏îÊ≠£Á°ÆÊó∂ÔºåÊâç‰ºöÁªô‰∫àÈ´òÂ•ñÂä±ÔºõÂê¶ÂàôÔºåÂ∞ÜÁõ∏Â∫îÂú∞ÂàÜÈÖçËæÉ‰ΩéÁöÑÂ•ñÂä±„ÄÇËøôÁßçÊú∫Âà∂ÊÉ©ÁΩö‰∫ÜÊé®ÁêÜ-Á≠îÊ°àÁöÑ‰∏ç‰∏ÄËá¥ÔºåÂπ∂ÈòªÊ≠¢Ê®°Âûã‰æùËµñ‰∫éËôöÂÅáÊ®°ÂºèÔºå‰æãÂ¶ÇÈÄâÈ°πÊéíÂ∫èÂÅèÂ∑Æ„ÄÇÊàë‰ª¨Âú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑËßÜÈ¢ëÊé®ÁêÜÂü∫ÂáÜÂíåÂ§öÊ®°ÊÄÅÊï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜ‰∏äËØÑ‰º∞‰∫ÜACREÔºåÂú®ËßÜÈ¢ëÊé®ÁêÜÂíåÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏äÔºåÁõ∏ÂØπ‰∫éGRPOÂü∫Á∫øÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü2.2%Âíå1.5%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜËßâÈóÆÁ≠î‰ªªÂä°‰∏≠Ôºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†‰ºòÂåñÂêéÔºåÊé®ÁêÜÈìæÂíåÊúÄÁªàÁ≠îÊ°à‰πãÈó¥Âá∫Áé∞‰∏ç‰∏ÄËá¥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇGRPOËôΩÁÑ∂ËÉΩÊèêÈ´òÁ≠îÊ°àÂáÜÁ°ÆÁéáÔºå‰ΩÜÂøΩÁï•‰∫ÜÊé®ÁêÜËøáÁ®ãÁöÑÊ≠£Á°ÆÊÄßÔºåÂØºËá¥Ê®°ÂûãÂèØËÉΩ‰æùËµñ‰∫éÊï∞ÊçÆ‰∏≠ÁöÑËôöÂÅáÁõ∏ÂÖ≥ÊÄßÔºå‰æãÂ¶ÇÈÄâÈ°πÈ°∫Â∫èÂÅèÂ∑ÆÔºå‰ªéËÄå‰∫ßÁîü‰∏ç‰∏ÄËá¥ÁöÑÁ≠îÊ°à„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•Á≠îÊ°à‰∏ÄËá¥ÊÄßÈ™åËØÅÊú∫Âà∂Êù•Á∫¶ÊùüÂº∫ÂåñÂ≠¶‰π†ËøáÁ®ã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊ®°ÂûãÂú®ÁîüÊàêÊé®ÁêÜÈìæÂíåÂàùÂßãÁ≠îÊ°àÂêéÔºå‰ºöÂØπÁ≠îÊ°àÈÄâÈ°πËøõË°åÊ¥óÁâåÔºåÂπ∂Ë¶ÅÊ±ÇÊ®°ÂûãÂü∫‰∫éÁõ∏ÂêåÁöÑÊé®ÁêÜÈìæÂÜçÊ¨°È¢ÑÊµãÁ≠îÊ°à„ÄÇÂ¶ÇÊûú‰∏§Ê¨°È¢ÑÊµãÁöÑÁ≠îÊ°à‰∏ÄËá¥‰∏îÊ≠£Á°ÆÔºåÂàôÁªô‰∫àÈ´òÂ•ñÂä±ÔºõÂê¶ÂàôÔºåÁªô‰∫à‰ΩéÂ•ñÂä±„ÄÇËøôÁßçÊú∫Âà∂ËÉΩÂ§üÊúâÊïàÊÉ©ÁΩöÊé®ÁêÜ-Á≠îÊ°àÁöÑ‰∏ç‰∏ÄËá¥ÊÄßÔºåÂπ∂ÈºìÂä±Ê®°ÂûãÂ≠¶‰π†Êõ¥ÂèØÈù†ÁöÑÊé®ÁêÜÊ®°Âºè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöACREÊñπÊ≥ïÂú®GRPOÔºàGradient Regularized Policy OptimizationÔºâÁöÑÂü∫Á°Ä‰∏äËøõË°åÊîπËøõ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö
1.  Ê®°ÂûãÊé•Êî∂ËßÜËßâËæìÂÖ•ÂíåÈóÆÈ¢òÔºåÁîüÊàêÊÄùÁª¥ÈìæÔºàChain-of-ThoughtÔºâÂíåÂàùÂßãÁ≠îÊ°à„ÄÇ
2.  ÂØπÁ≠îÊ°àÈÄâÈ°πËøõË°åÊ¥óÁâå„ÄÇ
3.  Ê®°Âûã‰ΩøÁî®Áõ∏ÂêåÁöÑÊÄùÁª¥ÈìæÔºåÂÜçÊ¨°È¢ÑÊµãÁ≠îÊ°à„ÄÇ
4.  ËÆ°ÁÆó‰∏ÄËá¥ÊÄßÈ™åËØÅÂ•ñÂä±ÔºöÂ¶ÇÊûúÂéüÂßãÁ≠îÊ°àÂíåÊ¥óÁâåÂêéÁöÑÁ≠îÊ°à‰∏ÄËá¥‰∏îÊ≠£Á°ÆÔºåÂàôÁªô‰∫àÈ´òÂ•ñÂä±ÔºõÂê¶ÂàôÔºåÁªô‰∫à‰ΩéÂ•ñÂä±„ÄÇ
5.  ‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºàÂ¶ÇGRPOÔºâÊõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞ÔºåÁõÆÊ†áÊòØÊúÄÂ§ßÂåñÁ¥ØÁßØÂ•ñÂä±„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöACREÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÁ≠îÊ°à‰∏ÄËá¥ÊÄßÈ™åËØÅÂ•ñÂä±ÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÁ∫¶ÊùüÊú∫Âà∂ÔºåËÉΩÂ§üÊòæÂºèÂú∞ÈºìÂä±Ê®°ÂûãÁîüÊàê‰∏éÁ≠îÊ°à‰∏ÄËá¥ÁöÑÊé®ÁêÜÈìæ„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÁªìÊûúÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåACRE‰∏ç‰ªÖÂÖ≥Ê≥®Á≠îÊ°àÁöÑÊ≠£Á°ÆÊÄßÔºåËøòÂÖ≥Ê≥®Êé®ÁêÜËøáÁ®ãÁöÑÂèØÈù†ÊÄßÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö‰∏ÄËá¥ÊÄßÈ™åËØÅÂ•ñÂä±ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ•ñÂä±ÂáΩÊï∞ÂèØ‰ª•ÂÆö‰πâ‰∏∫Ôºö
`R = R_correctness + Œª * R_consistency`
ÂÖ∂‰∏≠Ôºå`R_correctness`ÊòØÂü∫‰∫éÁ≠îÊ°àÊ≠£Á°ÆÊÄßÁöÑÂ•ñÂä±Ôºå`R_consistency`ÊòØÂü∫‰∫éÁ≠îÊ°à‰∏ÄËá¥ÊÄßÁöÑÂ•ñÂä±ÔºåŒªÊòØ‰∏Ä‰∏™Ë∂ÖÂèÇÊï∞ÔºåÁî®‰∫éÂπ≥Ë°°Ê≠£Á°ÆÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇ`R_consistency`ÂèØ‰ª•ÂÆö‰πâ‰∏∫Ôºö
`R_consistency = 1` Â¶ÇÊûúÂéüÂßãÁ≠îÊ°àÂíåÊ¥óÁâåÂêéÁöÑÁ≠îÊ°à‰∏ÄËá¥
`R_consistency = -1` Â¶ÇÊûúÂéüÂßãÁ≠îÊ°àÂíåÊ¥óÁâåÂêéÁöÑÁ≠îÊ°à‰∏ç‰∏ÄËá¥
Ê≠§Â§ñÔºåËÆ∫ÊñáÂèØËÉΩËøòÊ∂âÂèäÂà∞‰∏Ä‰∫õÂÖ∂‰ªñÁöÑË∂ÖÂèÇÊï∞ËÆæÁΩÆÔºå‰æãÂ¶ÇÂ≠¶‰π†Áéá„ÄÅÊäòÊâ£Âõ†Â≠êÁ≠âÔºåËøô‰∫õÂèÇÊï∞ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑ‰ªªÂä°ÂíåÊï∞ÊçÆÈõÜËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåACREÂú®ËßÜÈ¢ëÊé®ÁêÜÂíåÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®ËßÜÈ¢ëÊé®ÁêÜ‰ªªÂä°‰∏≠ÔºåACREÁõ∏ËæÉ‰∫éGRPOÂü∫Á∫øÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü2.2%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÔºåACREÁõ∏ËæÉ‰∫éGRPOÂü∫Á∫øÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü1.5%ÁöÑÂáÜÁ°ÆÁéá„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåACREËÉΩÂ§üÊúâÊïàÊèêÈ´òÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ACREÊñπÊ≥ïÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØ‰ª•Â∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂèØËß£ÈáäÊÄßÂíåÂèØÈù†ÊÄßÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜÈ¢ëÁêÜËß£„ÄÅÂåªÂ≠¶ÂõæÂÉèËØäÊñ≠„ÄÅËá™Âä®È©æÈ©∂Á≠â„ÄÇÈÄöËøáÊèêÈ´òÊ®°ÂûãÊé®ÁêÜËøáÁ®ãÁöÑ‰∏ÄËá¥ÊÄßÔºåÂèØ‰ª•Â¢ûÂº∫Áî®Êà∑ÂØπÊ®°ÂûãÈ¢ÑÊµãÁªìÊûúÁöÑ‰ø°‰ªªÂ∫¶ÔºåÂπ∂ÂáèÂ∞ëÊ®°ÂûãÁäØÈîôÁöÑÈ£éÈô©„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÊèêÈ´òÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞Â∫îÂØπÂô™Â£∞ÂíåÂØπÊäóÊÄßÊîªÂáª„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in large language models (LLMs) have demonstrated that reinforcement learning with verifiable rewards (RLVR) can significantly enhance reasoning abilities by directly optimizing correctness, rather than relying solely on supervised imitation. This paradigm has been extended to multimodal LLMs for complex video and image understanding tasks. However, while outcome-driven RL improves answer accuracy, it can inadvertently decouple the reasoning chain from the final answer, leading to situations where models produce inconsistency between the reasoning trace and final answer. In our experiments on multiple-choice visual question-answering tasks, the standard GRPO method yields only 79.7\% consistency on MMVU between the reasoning steps and the chosen answers, indicating frequent mismatches between answers and reasoning. To this end, we propose Answer-Consistent Reinforcement Learning (ACRE) that modifies the GRPO algorithm with an auxiliary consistency check. After the model generates a chain of thought and an initial answer for a given question, we shuffle the answer options and prompt the model again with the same reasoning trace to predict a second answer. We design a consistency-verification reward that grants a high reward only if both the original and the post-shuffle answers agree and are correct; otherwise, a lower reward is assigned accordingly. This mechanism penalizes reasoning-answer misalignment and discourages the model from relying on spurious patterns, such as option ordering biases. We evaluate ACRE on challenging Video Reasoning benchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\% and 1.5\% improvement for Video Reasoning and Math Reasoning tasks over the GRPO baseline.

