---
layout: default
title: Complementary and Contrastive Learning for Audio-Visual Segmentation
---

# Complementary and Contrastive Learning for Audio-Visual Segmentation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10051" target="_blank" class="toolbar-btn">arXiv: 2510.10051v1</a>
    <a href="https://arxiv.org/pdf/2510.10051.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10051v1" 
            onclick="toggleFavorite(this, '2510.10051v1', 'Complementary and Contrastive Learning for Audio-Visual Segmentation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Sitong Gong, Yunzhi Zhuge, Lu Zhang, Pingping Zhang, Huchuan Lu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-11

**Â§áÊ≥®**: Accepted to IEEE Transactions on Multimedia

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/SitongGong/CCFormer)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CCFormerÔºåÈÄöËøá‰∫íË°•ÂØπÊØîÂ≠¶‰π†ÂÆûÁé∞Êõ¥Á≤æÂáÜÁöÑÈü≥ËßÜÈ¢ëÂàÜÂâ≤**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Èü≥ËßÜÈ¢ëÂàÜÂâ≤` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `Transformer` `ÂØπÊØîÂ≠¶‰π†` `Êó∂Á©∫Âª∫Ê®°` `Â§öÊ®°ÊÄÅËûçÂêà` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÈü≥ËßÜÈ¢ëÂàÜÂâ≤ÊñπÊ≥ïÈöæ‰ª•ÂÖÖÂàÜÊèêÂèñÂ§öÊ®°ÊÄÅÁ≥ªÊï∞ÂíåÊó∂Èó¥Âä®ÊÄÅÔºåÈôêÂà∂‰∫ÜÂàÜÂâ≤Á≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß„ÄÇ
2. CCFormerÈÄöËøáÊó©ÊúüÈõÜÊàê„ÄÅÂ§öÊü•ËØ¢TransformerÂíåÂèåÊ®°ÊÄÅÂØπÊØîÂ≠¶‰π†ÔºåÂÖ®Èù¢ÊçïËé∑Êó∂Á©∫‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCCFormerÂú®S4„ÄÅMS3ÂíåAVSSÊï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Èü≥ËßÜÈ¢ëÂàÜÂâ≤(AVS)Êó®Âú®ÁîüÊàê‰∏éÁâ©‰ΩìÂ£∞Èü≥‰ø°Âè∑Áõ∏ÂÖ≥ÁöÑÂÉèÁ¥†Á∫ßÂàÜÂâ≤Âõæ„ÄÇËØ•È¢ÜÂüüÊ∂åÁé∞‰∫ÜÂ§ßÈáèÂü∫‰∫éCNNÂíåTransformerÁöÑÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜÂàÜÂâ≤Á≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß„ÄÇ‰º†ÁªüCNNÊñπÊ≥ïÈÄöËøáÂ°´ÂÖÖÂíå‰πòÊ≥ïÁ≠âÂü∫Êú¨Êìç‰ΩúÁÆ°ÁêÜÈü≥ËßÜÈ¢ë‰∫§‰∫íÔºå‰ΩÜÂèóÈôê‰∫éCNNÂ±ÄÈÉ®ÊÑüÂèóÈáé„ÄÇTransformerÊñπÊ≥ïÂ∞ÜÈü≥È¢ëÁ∫øÁ¥¢‰Ωú‰∏∫Êü•ËØ¢ÔºåÂà©Áî®Ê≥®ÊÑèÂäõÊú∫Âà∂Â¢ûÂº∫Â∏ßÂÜÖÈü≥ËßÜÈ¢ëÂçè‰ΩúÔºå‰ΩÜÈÄöÂ∏∏Èöæ‰ª•ÂÖÖÂàÜÊèêÂèñÂ§öÊ®°ÊÄÅÁ≥ªÊï∞ÂíåÊó∂Èó¥Âä®ÊÄÅ„ÄÇ‰∏∫ÂÖãÊúçËøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∫íË°•ÂØπÊØîTransformer(CCFormer)Ôºå‰∏Ä‰∏™ÊìÖÈïøÂ§ÑÁêÜÂ±ÄÈÉ®ÂíåÂÖ®Â±Ä‰ø°ÊÅØÂπ∂ÂÖ®Èù¢ÊçïËé∑Êó∂Á©∫‰∏ä‰∏ãÊñáÁöÑÊñ∞Ê°ÜÊû∂„ÄÇCCFormerÈ¶ñÂÖà‰ΩøÁî®Êó©ÊúüÈõÜÊàêÊ®°Âùó(EIM)ÔºåÈááÁî®Âπ∂Ë°åÂèåËæπÊû∂ÊûÑÔºåÂ∞ÜÂ§öÂ∞∫Â∫¶ËßÜËßâÁâπÂæÅ‰∏éÈü≥È¢ëÊï∞ÊçÆËûçÂêàÔºå‰ª•Â¢ûÂº∫Ë∑®Ê®°ÊÄÅ‰∫íË°•ÊÄß„ÄÇ‰∏∫‰∫ÜÊèêÂèñÂ∏ßÂÜÖÁ©∫Èó¥ÁâπÂæÅÂπ∂‰øÉËøõÊó∂Èó¥ËøûË¥ØÊÄßÁöÑÊÑüÁü•ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂ§öÊü•ËØ¢TransformerÊ®°Âùó(MTM)ÔºåËØ•Ê®°ÂùóÂä®ÊÄÅÂú∞Ëµã‰∫àÈü≥È¢ëÊü•ËØ¢Â≠¶‰π†ËÉΩÂäõÔºåÂπ∂ÂêåÊó∂Âª∫Ê®°Â∏ßÂíåËßÜÈ¢ëÁ∫ßÂà´ÁöÑÂÖ≥Á≥ª„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂèåÊ®°ÊÄÅÂØπÊØîÂ≠¶‰π†(BCL)Êù•‰øÉËøõÁªü‰∏ÄÁâπÂæÅÁ©∫Èó¥‰∏≠‰∏§ÁßçÊ®°ÊÄÅÁöÑÂØπÈΩê„ÄÇÈÄöËøáÊúâÊïàÁªìÂêàËøô‰∫õËÆæËÆ°ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®S4„ÄÅMS3ÂíåAVSSÊï∞ÊçÆÈõÜ‰∏äÂª∫Á´ã‰∫ÜÊñ∞ÁöÑstate-of-the-artÂü∫ÂáÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÈü≥ËßÜÈ¢ëÂàÜÂâ≤(AVS)Êó®Âú®Ê†πÊçÆÁªôÂÆöÁöÑÈü≥ËßÜÈ¢ë‰ø°ÊÅØÔºåÂØπËßÜÈ¢ë‰∏≠ÁöÑÁõÆÊ†áÁâ©‰ΩìËøõË°åÂÉèÁ¥†Á∫ßÂà´ÁöÑÂàÜÂâ≤„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éCNNÂíåTransformerÁöÑÊñπÊ≥ïÔºåÂú®Â§ÑÁêÜË∑®Ê®°ÊÄÅ‰ø°ÊÅØËûçÂêàÂíåÊó∂Â∫è‰ø°ÊÅØÂª∫Ê®°ÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂØºËá¥ÂàÜÂâ≤Á≤æÂ∫¶ÂèóÈôê„ÄÇCNNÊñπÊ≥ïÊÑüÂèóÈáéÊúâÈôêÔºåTransformerÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÊèêÂèñÂ§öÊ®°ÊÄÅÁ≥ªÊï∞ÂíåÊó∂Èó¥Âä®ÊÄÅ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCCFormerÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®‰∫íË°•Â≠¶‰π†ÂíåÂØπÊØîÂ≠¶‰π†ÔºåÂÖÖÂàÜÊåñÊéòÈü≥ËßÜÈ¢ë‰πãÈó¥ÁöÑÂÖ≥ËÅîÊÄßÂíåÂ∑ÆÂºÇÊÄß„ÄÇÈÄöËøáÊó©ÊúüÈõÜÊàêÊ®°Âùó(EIM)Â¢ûÂº∫Ë∑®Ê®°ÊÄÅ‰∫íË°•ÊÄßÔºåÂ§öÊü•ËØ¢TransformerÊ®°Âùó(MTM)ÊèêÂèñÂ∏ßÂÜÖÁ©∫Èó¥ÁâπÂæÅÂíåÊó∂Èó¥ËøûË¥ØÊÄßÔºåÂèåÊ®°ÊÄÅÂØπÊØîÂ≠¶‰π†(BCL)‰øÉËøõÊ®°ÊÄÅÂØπÈΩêÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÂáÜÁöÑÈü≥ËßÜÈ¢ëÂàÜÂâ≤„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCCFormer‰∏ªË¶ÅÂåÖÂê´‰∏â‰∏™Ê®°ÂùóÔºöÊó©ÊúüÈõÜÊàêÊ®°Âùó(EIM)„ÄÅÂ§öÊü•ËØ¢TransformerÊ®°Âùó(MTM)ÂíåÂèåÊ®°ÊÄÅÂØπÊØîÂ≠¶‰π†(BCL)„ÄÇEIMË¥üË¥£ËûçÂêàÂ§öÂ∞∫Â∫¶ËßÜËßâÁâπÂæÅÂíåÈü≥È¢ëÊï∞ÊçÆÔºõMTMÂà©Áî®Â§öÊü•ËØ¢TransformerÂä®ÊÄÅÂ≠¶‰π†Èü≥È¢ëÊü•ËØ¢ÔºåÂª∫Ê®°Â∏ßÂíåËßÜÈ¢ëÁ∫ßÂà´ÁöÑÂÖ≥Á≥ªÔºõBCLÈÄöËøáÂØπÊØîÂ≠¶‰π†‰øÉËøõÈü≥ËßÜÈ¢ëÁâπÂæÅÂú®Áªü‰∏ÄÁâπÂæÅÁ©∫Èó¥ÁöÑÂØπÈΩê„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÂÖàÈÄöËøáEIMËøõË°åÂàùÊ≠•ËûçÂêàÔºåÂÜçÈÄöËøáMTMÊèêÂèñÊó∂Á©∫ÁâπÂæÅÔºåÊúÄÂêéÈÄöËøáBCLËøõË°åÁâπÂæÅÂØπÈΩêÂíå‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCCFormerÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰∏â‰∏™ÊñπÈù¢Ôºö‰∏ÄÊòØÂπ∂Ë°åÂèåËæπÊû∂ÊûÑÁöÑÊó©ÊúüÈõÜÊàêÊ®°ÂùóÔºåÊúâÊïàËûçÂêàÂ§öÂ∞∫Â∫¶ËßÜËßâÁâπÂæÅÂíåÈü≥È¢ëÊï∞ÊçÆÔºõ‰∫åÊòØÂ§öÊü•ËØ¢TransformerÊ®°ÂùóÔºåÂä®ÊÄÅÂ≠¶‰π†Èü≥È¢ëÊü•ËØ¢Âπ∂Âª∫Ê®°Êó∂Â∫èÂÖ≥Á≥ªÔºõ‰∏âÊòØÂèåÊ®°ÊÄÅÂØπÊØîÂ≠¶‰π†Ôºå‰øÉËøõÈü≥ËßÜÈ¢ëÁâπÂæÅÂú®Áªü‰∏ÄÁâπÂæÅÁ©∫Èó¥ÁöÑÂØπÈΩê„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCCFormerÊõ¥ÂÖ®Èù¢Âú∞ËÄÉËôë‰∫ÜË∑®Ê®°ÊÄÅ‰ø°ÊÅØËûçÂêàÂíåÊó∂Â∫è‰ø°ÊÅØÂª∫Ê®°Ôºå‰ªéËÄåÊèêÂçá‰∫ÜÂàÜÂâ≤Á≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöEIMÈááÁî®Âπ∂Ë°åÂèåËæπÊû∂ÊûÑÔºåÂàÜÂà´Â§ÑÁêÜËßÜËßâÂíåÈü≥È¢ë‰ø°ÊÅØÔºåÂπ∂ÈÄöËøáË∑®Ê®°ÊÄÅÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åËûçÂêà„ÄÇMTM‰ΩøÁî®Â§ö‰∏™Êü•ËØ¢Â§¥ÔºåÊØè‰∏™Êü•ËØ¢Â§¥Ë¥üË¥£Â≠¶‰π†‰∏çÂêåÁöÑÈü≥È¢ëÁâπÂæÅÔºå‰ªéËÄåÊõ¥ÂÖ®Èù¢Âú∞ÊçïÊçâÈü≥È¢ë‰ø°ÊÅØ„ÄÇBCL‰ΩøÁî®InfoNCEÊçüÂ§±ÂáΩÊï∞ÔºåÊúÄÂ§ßÂåñÊ≠£Ê†∑Êú¨ÂØπÁöÑÁõ∏‰ººÂ∫¶ÔºåÊúÄÂ∞èÂåñË¥üÊ†∑Êú¨ÂØπÁöÑÁõ∏‰ººÂ∫¶Ôºå‰ªéËÄå‰øÉËøõÈü≥ËßÜÈ¢ëÁâπÂæÅÁöÑÂØπÈΩê„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

CCFormerÂú®S4„ÄÅMS3ÂíåAVSS‰∏â‰∏™Èü≥ËßÜÈ¢ëÂàÜÂâ≤Êï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®S4Êï∞ÊçÆÈõÜ‰∏äÔºåCCFormerÁöÑmIoUÊåáÊ†áÁõ∏ÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá‰∫ÜÊòæËëóÂπÖÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúÂÖÖÂàÜÈ™åËØÅ‰∫ÜCCFormerÂú®Ë∑®Ê®°ÊÄÅ‰ø°ÊÅØËûçÂêàÂíåÊó∂Â∫è‰ø°ÊÅØÂª∫Ê®°ÊñπÈù¢ÁöÑ‰ºòÂäøÔºå‰ª•Âèä‰∫íË°•ÂØπÊØîÂ≠¶‰π†ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CCFormerÂú®Èü≥ËßÜÈ¢ëÂàÜÂâ≤È¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇËßÜÈ¢ëÁõëÊéß„ÄÅÊô∫ËÉΩÂÆâÈò≤„ÄÅËá™Âä®È©æÈ©∂„ÄÅËßÜÈ¢ëÁºñËæëÁ≠â„ÄÇÈÄöËøáÁ≤æÂáÜÁöÑÈü≥ËßÜÈ¢ëÂàÜÂâ≤ÔºåÂèØ‰ª•ÂÆûÁé∞ÂØπÁâπÂÆöÂ£∞Èü≥‰∫ã‰ª∂Áõ∏ÂÖ≥Áâ©‰ΩìÁöÑËá™Âä®ËØÜÂà´ÂíåË∑üË∏™Ôºå‰ªéËÄåÊèêÂçáÁ≥ªÁªüÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØËøòÂèØ‰ª•Â∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÈ¢ÜÂüüÔºåÂÆûÁé∞Êõ¥Ê≤âÊµ∏ÂºèÁöÑÈü≥ËßÜÈ¢ë‰∫íÂä®‰ΩìÈ™å„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps that correlate with the auditory signals of objects. This field has seen significant progress with numerous CNN and Transformer-based methods enhancing the segmentation accuracy and robustness. Traditional CNN approaches manage audio-visual interactions through basic operations like padding and multiplications but are restricted by CNNs' limited local receptive field. More recently, Transformer-based methods treat auditory cues as queries, utilizing attention mechanisms to enhance audio-visual cooperation within frames. Nevertheless, they typically struggle to extract multimodal coefficients and temporal dynamics adequately. To overcome these limitations, we present the Complementary and Contrastive Transformer (CCFormer), a novel framework adept at processing both local and global information and capturing spatial-temporal context comprehensively. Our CCFormer initiates with the Early Integration Module (EIM) that employs a parallel bilateral architecture, merging multi-scale visual features with audio data to boost cross-modal complementarity. To extract the intra-frame spatial features and facilitate the perception of temporal coherence, we introduce the Multi-query Transformer Module (MTM), which dynamically endows audio queries with learning capabilities and models the frame and video-level relations simultaneously. Furthermore, we propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across both modalities in the unified feature space. Through the effective combination of those designs, our method sets new state-of-the-art benchmarks across the S4, MS3 and AVSS datasets. Our source code and model weights will be made publicly available at https://github.com/SitongGong/CCFormer

