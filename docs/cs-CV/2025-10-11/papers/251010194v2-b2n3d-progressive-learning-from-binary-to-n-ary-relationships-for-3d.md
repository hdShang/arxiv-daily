---
layout: default
title: B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding
---

# B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10194" target="_blank" class="toolbar-btn">arXiv: 2510.10194v2</a>
    <a href="https://arxiv.org/pdf/2510.10194.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10194v2" 
            onclick="toggleFavorite(this, '2510.10194v2', 'B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Feng Xiao, Hongbin Xu, Hai Ci, Wenxiong Kang

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-11 (Êõ¥Êñ∞: 2025-12-01)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫B2N3DÊ°ÜÊû∂ÔºåÈÄöËøá‰∫åÂÖÉÂà∞NÂÖÉÂÖ≥Á≥ªÊ∏êËøõÂ≠¶‰π†ÂÆûÁé∞Êõ¥Á≤æÁ°ÆÁöÑ3DÁâ©‰ΩìÂÆö‰Ωç**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `3DÁâ©‰ΩìÂÆö‰Ωç` `Ëá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£` `NÂÖÉÂÖ≥Á≥ªÂ≠¶‰π†` `Â§öÊ®°ÊÄÅËûçÂêà` `Âú∫ÊôØÂõæ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ3DÁâ©‰ΩìÂÆö‰ΩçÊñπÊ≥ï‰ªÖÂª∫Ê®°ÊàêÂØπÁâ©‰ΩìÁöÑÂÖ≥Á≥ªÔºåÂøΩÁï•‰∫ÜNÂÖÉÂÖ≥Á≥ªÂú®Â§öÊ®°ÊÄÅÁêÜËß£‰∏≠ÁöÑÂÖ®Â±ÄÈáçË¶ÅÊÄßÔºåÂØºËá¥ÂÆö‰ΩçÁ≤æÂ∫¶ÂèóÈôê„ÄÇ
2. B2N3DÊ°ÜÊû∂ÈÄöËøáÊ∏êËøõÂºèÂ≠¶‰π†ÔºåÂ∞ÜÂÖ≥Á≥ªÂ≠¶‰π†‰ªé‰∫åÂÖÉÊâ©Â±ïÂà∞NÂÖÉÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÊçïÊçâÂú∫ÊôØ‰∏≠Áâ©‰ΩìÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ª„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåB2N3DÂú®ReferIt3DÂíåScanReferÊï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫Ü‰ºò‰∫éÁé∞ÊúâÊäÄÊúØÁöÑÊïàÊûúÔºåÈ™åËØÅ‰∫ÜNÂÖÉÂÖ≥Á≥ªÊÑüÁü•ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∏êËøõÂºèÂÖ≥Á≥ªÂ≠¶‰π†Ê°ÜÊû∂B2N3DÔºåÁî®‰∫é3DÁâ©‰ΩìÂÆö‰Ωç„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÂÖ≥Á≥ªÂ≠¶‰π†‰ªé‰∫åÂÖÉÊâ©Â±ïÂà∞NÂÖÉÔºå‰ª•ËØÜÂà´‰∏éÂèÇËÄÉÊèèËø∞ÂÖ®Â±ÄÂåπÈÖçÁöÑËßÜËßâÂÖ≥Á≥ªÔºå‰ªéËÄåËß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ï‰ªÖÂØπÊàêÂØπÁâ©‰ΩìÂª∫Ê®°ÂÖ≥Á≥ªÔºåÂøΩÁï•NÂÖÉÁªÑÂêàÂú®Â§öÊ®°ÊÄÅÂÖ≥Á≥ªÁêÜËß£‰∏≠ÁöÑÂÖ®Â±ÄÊÑüÁü•ÈáçË¶ÅÊÄßÁöÑÈóÆÈ¢ò„ÄÇÈíàÂØπËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Áº∫‰πèË¢´Êåá‰ª£Áâ©‰ΩìÁöÑÁâπÂÆöÊ†áÊ≥®ÔºåËÆæËÆ°‰∫ÜÂàÜÁªÑÁõëÁù£ÊçüÂ§±Êù•‰øÉËøõNÂÖÉÂÖ≥Á≥ªÂ≠¶‰π†„ÄÇÂú®Áî±NÂÖÉÂÖ≥Á≥ªÂàõÂª∫ÁöÑÂú∫ÊôØÂõæ‰∏≠Ôºå‰ΩøÁî®ÂÖ∑ÊúâÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂ§öÊ®°ÊÄÅÁΩëÁªúÊù•Ëøõ‰∏ÄÊ≠•ÂÆö‰ΩçNÂÖÉÁªÑÂêà‰∏≠ÁöÑÁõÆÊ†á„ÄÇÂú®ReferIt3DÂíåScanReferÂü∫ÂáÜ‰∏äÁöÑÂÆûÈ™åÂíåÊ∂àËûçÁ†îÁ©∂Ë°®ÊòéÔºåËØ•ÊñπÊ≥ï‰ºò‰∫éÁé∞ÊúâÊäÄÊúØÔºåÂπ∂ËØÅÊòé‰∫ÜNÂÖÉÂÖ≥Á≥ªÊÑüÁü•Âú®3DÂÆö‰Ωç‰∏≠ÁöÑ‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ3DÁâ©‰ΩìÂÆö‰ΩçÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫é‰∫åÂÖÉÂÖ≥Á≥ªÂª∫Ê®°ÔºåÂç≥‰ªÖËÄÉËôëÁâ©‰Ωì‰∏§‰∏§‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÁÑ∂ËÄåÔºåËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÈÄöÂ∏∏Ê∂âÂèäÂ§ö‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ªÔºàNÂÖÉÂÖ≥Á≥ªÔºâÔºå‰æãÂ¶Ç‚ÄúÂú®Á∫¢Ëâ≤Ê≤ôÂèëÂ∑¶ËæπÔºåÈù†ËøëËìùËâ≤Ê°åÂ≠êÁöÑÁâ©‰Ωì‚Äù„ÄÇÂøΩÁï•Ëøô‰∫õNÂÖÉÂÖ≥Á≥ª‰ºöÂØºËá¥Ê®°ÂûãÊó†Ê≥ïÂÖÖÂàÜÁêÜËß£Âú∫ÊôØÔºå‰ªéËÄåÂΩ±ÂìçÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπNÂÖÉÂÖ≥Á≥ªÁöÑÊúâÊïàÂª∫Ê®°ÂíåÂà©Áî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂÖ≥Á≥ªÂ≠¶‰π†‰ªéÁÆÄÂçïÁöÑ‰∫åÂÖÉÂÖ≥Á≥ªÊâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑNÂÖÉÂÖ≥Á≥ª„ÄÇÈÄöËøáÂ≠¶‰π†NÂÖÉÂÖ≥Á≥ªÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Âú∫ÊôØ‰∏≠Áâ©‰Ωì‰πãÈó¥ÁöÑÂÖ®Â±ÄÂÖ≥Á≥ªÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ÂÆö‰ΩçÁõÆÊ†áÁâ©‰Ωì„ÄÇÊ≠§Â§ñÔºåÈááÁî®Ê∏êËøõÂºèÂ≠¶‰π†Á≠ñÁï•Ôºå‰ªé‰∫åÂÖÉÂÖ≥Á≥ªÂÖ•ÊâãÔºåÈÄêÊ≠•ËøáÊ∏°Âà∞NÂÖÉÂÖ≥Á≥ªÔºåÈôç‰ΩéÂ≠¶‰π†ÈöæÂ∫¶„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöB2N3DÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) **Âú∫ÊôØÂõæÊûÑÂª∫**ÔºöÈ¶ñÂÖàÔºå‰ªé3DÂú∫ÊôØ‰∏≠ÊèêÂèñÁâ©‰ΩìÔºåÂπ∂ÊûÑÂª∫ÂåÖÂê´‰∫åÂÖÉÂíåNÂÖÉÂÖ≥Á≥ªÁöÑÂú∫ÊôØÂõæ„ÄÇ2) **ÂÖ≥Á≥ªÂ≠¶‰π†**ÔºöÂà©Áî®ÂàÜÁªÑÁõëÁù£ÊçüÂ§±ÔºåÂ≠¶‰π†‰∫åÂÖÉÂíåNÂÖÉÂÖ≥Á≥ª„ÄÇ3) **Â§öÊ®°ÊÄÅËûçÂêà**Ôºö‰ΩøÁî®ÂÖ∑ÊúâÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂ§öÊ®°ÊÄÅÁΩëÁªúÔºåËûçÂêàËßÜËßâ‰ø°ÊÅØÂíåËØ≠Ë®Ä‰ø°ÊÅØ„ÄÇ4) **ÁõÆÊ†áÂÆö‰Ωç**ÔºöÂú®ËûçÂêàÂêéÁöÑÁâπÂæÅÂü∫Á°Ä‰∏äÔºåÈ¢ÑÊµãÁõÆÊ†áÁâ©‰ΩìÁöÑ‰ΩçÁΩÆ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂºïÂÖ•‰∫ÜNÂÖÉÂÖ≥Á≥ªÂ≠¶‰π†ÔºåÂπ∂ËÆæËÆ°‰∫ÜÂàÜÁªÑÁõëÁù£ÊçüÂ§±Êù•Ëß£ÂÜ≥ËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Áº∫‰πèNÂÖÉÂÖ≥Á≥ªÊ†áÊ≥®ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáNÂÖÉÂÖ≥Á≥ªÂ≠¶‰π†ÔºåÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Âú∫ÊôØ‰∏≠Áâ©‰Ωì‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ªÔºå‰ªéËÄåÊèêÈ´òÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇÊ≠§Â§ñÔºåÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ËÉΩÂ§üÊúâÊïàÂú∞ËûçÂêàËßÜËßâ‰ø°ÊÅØÂíåËØ≠Ë®Ä‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂàÜÁªÑÁõëÁù£ÊçüÂ§±ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆ„ÄÇÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Ê≤°ÊúâÊòéÁ°ÆÁöÑNÂÖÉÂÖ≥Á≥ªÊ†áÊ≥®ÔºåÂõ†Ê≠§ÈúÄË¶ÅËÆæËÆ°‰∏ÄÁßçËá™ÁõëÁù£ÁöÑÊñπÂºèÊù•Â≠¶‰π†NÂÖÉÂÖ≥Á≥ª„ÄÇÂàÜÁªÑÁõëÁù£ÊçüÂ§±ÈÄöËøáÂ∞ÜÁõ∏ÂÖ≥ÁöÑÁâ©‰ΩìÂàÜÁªÑÔºåÂπ∂ÈºìÂä±Ê®°ÂûãÈ¢ÑÊµãÊ≠£Á°ÆÁöÑÁªÑÂà´Ôºå‰ªéËÄåÂÆûÁé∞NÂÖÉÂÖ≥Á≥ªÁöÑÂ≠¶‰π†„ÄÇÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÂåÖÊã¨Ëá™Ê≥®ÊÑèÂäõÔºàself-attentionÔºâÂíå‰∫§ÂèâÊ≥®ÊÑèÂäõÔºàcross-attentionÔºâÔºåÁî®‰∫éÂàÜÂà´ÊçïÊçâËßÜËßâ‰ø°ÊÅØÂíåËØ≠Ë®Ä‰ø°ÊÅØÂÜÖÈÉ®ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºå‰ª•ÂèäËßÜËßâ‰ø°ÊÅØÂíåËØ≠Ë®Ä‰ø°ÊÅØ‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

B2N3DÂú®ReferIt3DÂíåScanReferÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®ReferIt3DÊï∞ÊçÆÈõÜ‰∏äÔºåB2N3DÁöÑAcc@0.25ÊåáÊ†áÁõ∏ÊØî‰∫éÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá‰∫ÜË∂ÖËøá3%„ÄÇÂú®ScanReferÊï∞ÊçÆÈõÜ‰∏äÔºåB2N3D‰πüÂèñÂæó‰∫ÜÁ±ª‰ººÁöÑÊèêÂçáÔºåËØÅÊòé‰∫ÜNÂÖÉÂÖ≥Á≥ªÂ≠¶‰π†ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Âú∫ÊôØÁêÜËß£„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫Âú∫ÊôØÁêÜËß£‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÔºåÂà©Áî®ËØ•ÊñπÊ≥ïÂÆö‰ΩçÁõÆÊ†áÁâ©‰ΩìÔºå‰ªéËÄåÂÆåÊàêËØ∏Â¶Ç‚ÄúÊääÁ∫¢Ëâ≤ÁöÑËãπÊûúÊîæÂú®Ê°åÂ≠ê‰∏ä‚ÄùÁöÑ‰ªªÂä°„ÄÇÂú®Ëá™Âä®È©æÈ©∂È¢ÜÂüüÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Â∏ÆÂä©ËΩ¶ËæÜÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºå‰ªéËÄåÊèêÈ´òÈ©æÈ©∂ÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Localizing 3D objects using natural language is essential for robotic scene understanding. The descriptions often involve multiple spatial relationships to distinguish similar objects, making 3D-language alignment difficult. Current methods only model relationships for pairwise objects, ignoring the global perceptual significance of n-ary combinations in multi-modal relational understanding. To address this, we propose a novel progressive relational learning framework for 3D object grounding. We extend relational learning from binary to n-ary to identify visual relations that match the referential description globally. Given the absence of specific annotations for referred objects in the training data, we design a grouped supervision loss to facilitate n-ary relational learning. In the scene graph created with n-ary relationships, we use a multi-modal network with hybrid attention mechanisms to further localize the target within the n-ary combinations. Experiments and ablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that our method outperforms the state-of-the-art, and proves the advantages of the n-ary relational perception in 3D localization.

