---
layout: default
title: ESCA: Contextualizing Embodied Agents via Scene-Graph Generation
---

# ESCA: Contextualizing Embodied Agents via Scene-Graph Generation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.15963" target="_blank" class="toolbar-btn">arXiv: 2510.15963v2</a>
    <a href="https://arxiv.org/pdf/2510.15963.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15963v2" 
            onclick="toggleFavorite(this, '2510.15963v2', 'ESCA: Contextualizing Embodied Agents via Scene-Graph Generation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-11 (Êõ¥Êñ∞: 2025-10-27)

**Â§áÊ≥®**: Accepted as a Spotlight Paper at NeurIPS 2025

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/video-fm/LASER) | [GITHUB](https://github.com/video-fm/ESCA)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ESCAÊ°ÜÊû∂ÔºåÈÄöËøáÂú∫ÊôØÂõæÁîüÊàêÂ¢ûÂº∫ÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑ‰∏ä‰∏ãÊñáÊÑüÁü•ËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂÖ∑Ë∫´Êô∫ËÉΩ‰Ωì` `Âú∫ÊôØÂõæÁîüÊàê` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `‰∏ä‰∏ãÊñáÊÑüÁü•` `ÂºÄÊîæÂüüËßÜÈ¢ë`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâMLLMÂú®ÂÖ∑Ë∫´Êô∫ËÉΩ‰Ωì‰∏≠Â≠òÂú®ËßÜËßâÁâπÂæÅ‰∏éÊñáÊú¨ËØ≠‰πâÂÖ≥ËÅî‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÊÑüÁü•ËÉΩÂäõËæÉÂº±„ÄÇ
2. ESCAÊ°ÜÊû∂ÈÄöËøáÂ∞ÜÊô∫ËÉΩ‰ΩìÁöÑÊÑüÁü• grounding Âú®Êó∂Á©∫Âú∫ÊôØÂõæ‰∏≠ÔºåÂ¢ûÂº∫‰∫ÜÂÖ∂‰∏ä‰∏ãÊñáÊÑüÁü•ËÉΩÂäõ„ÄÇ
3. SGCLIPÊ®°ÂûãÂú®Âú∫ÊôØÂõæÁîüÊàêÂíåÂä®‰ΩúÂÆö‰Ωç‰ªªÂä°‰∏äÂèñÂæó‰∫ÜSOTAÁªìÊûúÔºåÂπ∂ÊèêÂçá‰∫ÜÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÊÑüÁü•ÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÂú®ÈÄöÁî®ÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÊñπÈù¢ÂèñÂæó‰∫ÜÂø´ÈÄüËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑMLLMÊó†Ê≥ïÂèØÈù†Âú∞ÊçïÊçâ‰ΩéÂ±ÇËßÜËßâÁâπÂæÅÂíåÈ´òÂ±ÇÊñáÊú¨ËØ≠‰πâ‰πãÈó¥ÁöÑÁªÜÁ≤íÂ∫¶ËÅîÁ≥ªÔºåÂØºËá¥Âº± grounding Âíå‰∏çÂáÜÁ°ÆÁöÑÊÑüÁü•„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∏ÄÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜESCAÔºå‰∏Ä‰∏™ÈÄöËøáÂ∞ÜÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÊÑüÁü• grounding Âú®Êó∂Á©∫Âú∫ÊôØÂõæ‰∏≠Êù•ÊÉÖÂ¢ÉÂåñÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÊ°ÜÊû∂„ÄÇÂÖ∂Ê†∏ÂøÉÊòØSGCLIPÔºå‰∏ÄÁßçÊñ∞È¢ñÁöÑ„ÄÅÂºÄÊîæÂüüÁöÑ„ÄÅÂèØÊèêÁ§∫ÁöÑÂú∫ÊôØÂõæÁîüÊàêÂü∫Á°ÄÊ®°ÂûãÔºåÂÆÉÂü∫‰∫éCLIP„ÄÇSGCLIP‰ΩøÁî®Á•ûÁªèÁ¨¶Âè∑ÁÆ°ÈÅìÂú®87K+ÂºÄÊîæÂüüËßÜÈ¢ë‰∏äËøõË°åËÆ≠ÁªÉÔºåËØ•ÁÆ°ÈÅìÂ∞ÜËá™Âä®ÁîüÊàêÁöÑÂ≠óÂπï‰∏éÊ®°ÂûãËá™Ë∫´ÁîüÊàêÁöÑÂú∫ÊôØÂõæÂØπÈΩêÔºå‰ªéËÄåÊ∂àÈô§‰∫ÜÂØπ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÈúÄÊ±Ç„ÄÇÊàë‰ª¨ËØÅÊòé‰∫ÜSGCLIPÂú®Âü∫‰∫é prompt ÁöÑÊé®ÁêÜÂíåÁâπÂÆö‰ªªÂä°ÂæÆË∞ÉÊñπÈù¢ÈÉΩË°®Áé∞Âá∫Ëâ≤ÔºåÂú®Âú∫ÊôØÂõæÁîüÊàêÂíåÂä®‰ΩúÂÆö‰ΩçÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇESCA‰∏éSGCLIPÊîπËøõ‰∫ÜÂü∫‰∫éÂºÄÊ∫êÂíåÂïÜ‰∏öMLLMÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÊÑüÁü•ËÉΩÂäõÔºåÂú®‰∏§‰∏™ÂÖ∑Ë∫´ÁéØÂ¢É‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåESCAÊòæËëóÂáèÂ∞ë‰∫ÜÊô∫ËÉΩ‰ΩìÊÑüÁü•ÈîôËØØÔºåÂπ∂‰ΩøÂºÄÊ∫êÊ®°ÂûãËÉΩÂ§üË∂ÖË∂ä‰∏ìÊúâÂü∫Á∫ø„ÄÇÊàë‰ª¨ÂèëÂ∏É‰∫ÜSGCLIPÊ®°ÂûãËÆ≠ÁªÉÁöÑÊ∫ê‰ª£Á†ÅÂú®https://github.com/video-fm/LASERÔºå‰ª•ÂèäÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑ‰ª£Á†ÅÂú®https://github.com/video-fm/ESCA„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂÖ∑Ë∫´Êô∫ËÉΩ‰Ωì‰æùËµñÁöÑMLLMÊó†Ê≥ïÂáÜÁ°ÆÊçïÊçâËßÜËßâÁâπÂæÅÂíåÊñáÊú¨ËØ≠‰πâ‰πãÈó¥ÁöÑÁªÜÁ≤íÂ∫¶ËÅîÁ≥ªÔºåÂØºËá¥ÊÑüÁü•ËÉΩÂäõ‰∏çË∂≥ÔºåÂÆπÊòì‰∫ßÁîüÈîôËØØ„ÄÇËøôÈôêÂà∂‰∫ÜÊô∫ËÉΩ‰ΩìÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊúâÊïà‰∫§‰∫íÂíåÂÜ≥Á≠ñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•Âú∫ÊôØÂõæÊù•Â¢ûÂº∫Êô∫ËÉΩ‰ΩìÁöÑ‰∏ä‰∏ãÊñáÊÑüÁü•ËÉΩÂäõ„ÄÇÂú∫ÊôØÂõæËÉΩÂ§üÊòæÂºèÂú∞Ë°®Á§∫Âú∫ÊôØ‰∏≠ÁöÑÁâ©‰Ωì„ÄÅÂÖ≥Á≥ª‰ª•ÂèäÂÆÉ‰ª¨‰πãÈó¥ÁöÑ‰∫§‰∫íÔºå‰ªéËÄå‰∏∫Êô∫ËÉΩ‰ΩìÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑÁéØÂ¢É‰ø°ÊÅØÔºåÂ∏ÆÂä©ÂÖ∂Êõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊé®ÁêÜ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöESCAÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊòØSGCLIPÊ®°ÂûãÔºåÂÆÉÊòØ‰∏Ä‰∏™Âü∫‰∫éCLIPÁöÑ„ÄÅÂºÄÊîæÂüüÁöÑ„ÄÅÂèØÊèêÁ§∫ÁöÑÂú∫ÊôØÂõæÁîüÊàêÊ®°Âûã„ÄÇSGCLIPÈÄöËøáÁ•ûÁªèÁ¨¶Âè∑ÁÆ°ÈÅìÂú®Â§ßÈáèÂºÄÊîæÂüüËßÜÈ¢ë‰∏äËøõË°åËÆ≠ÁªÉÔºåËØ•ÁÆ°ÈÅìËá™Âä®Â∞ÜÁîüÊàêÁöÑÂ≠óÂπï‰∏éÂú∫ÊôØÂõæÂØπÈΩê„ÄÇESCAÊ°ÜÊû∂Â∞ÜSGCLIPÁîüÊàêÁöÑÂú∫ÊôØÂõæ‰Ωú‰∏∫Êô∫ËÉΩ‰ΩìÁöÑËæìÂÖ•Ôºå‰ªéËÄåÂ¢ûÂº∫ÂÖ∂ÊÑüÁü•ËÉΩÂäõ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨ÔºöËßÜÈ¢ëËæìÂÖ• -> SGCLIPÁîüÊàêÂú∫ÊôØÂõæ -> Âú∫ÊôØÂõæ‰∏éËßÜËßâ‰ø°ÊÅØËæìÂÖ•MLLM -> Êô∫ËÉΩ‰ΩìÂÜ≥Á≠ñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSGCLIPÊ®°ÂûãÊòØÂÖ≥ÈîÆÂàõÊñ∞ÁÇπ„ÄÇÂÆÉÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®ÔºåËÄåÊòØÈÄöËøáÁ•ûÁªèÁ¨¶Âè∑ÁÆ°ÈÅìËá™Âä®ÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÊàêÊú¨„ÄÇÊ≠§Â§ñÔºåSGCLIPÊòØÂèØÊèêÁ§∫ÁöÑÔºåÂèØ‰ª•Ê†πÊçÆ‰∏çÂêåÁöÑ‰ªªÂä°ËøõË°åÂÆöÂà∂„ÄÇSGCLIP‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåÂÆÉËÉΩÂ§üÁîüÊàêÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥‰∏∞ÂØåÁöÑÂú∫ÊôØÂõæÔºå‰ªéËÄå‰∏∫Êô∫ËÉΩ‰ΩìÊèê‰æõÊõ¥ÂÖ®Èù¢ÁöÑÁéØÂ¢É‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSGCLIPÁöÑËÆ≠ÁªÉ‰ΩøÁî®‰∫ÜÂØπÊØîÂ≠¶‰π†ÊçüÂ§±Ôºå‰ª•ÈºìÂä±Ê®°ÂûãÂ≠¶‰π†ËßÜËßâÁâπÂæÅÂíåÊñáÊú¨ËØ≠‰πâ‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÁ•ûÁªèÁ¨¶Âè∑ÁÆ°ÈÅìÂåÖÂê´‰∏Ä‰∏™ captioning Ê®°ÂûãÂíå‰∏Ä‰∏™Âú∫ÊôØÂõæÁîüÊàêÊ®°ÂûãÔºåÂÆÉ‰ª¨ÂÖ±ÂêåÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÊ®°Âûã‰ΩøÁî®‰∫ÜTransformerÊû∂ÊûÑÔºåÂπ∂ÈíàÂØπÂú∫ÊôØÂõæÁîüÊàê‰ªªÂä°ËøõË°å‰∫Ü‰ºòÂåñ„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂèØÂú®ËÆ∫ÊñáÈôÑÂΩï‰∏≠ÊâæÂà∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåESCAÊ°ÜÊû∂ÊòæËëóÂáèÂ∞ë‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑÊÑüÁü•ÈîôËØØÔºåÂπ∂‰ΩøÂºÄÊ∫êÊ®°ÂûãËÉΩÂ§üË∂ÖË∂ä‰∏ìÊúâÂü∫Á∫ø„ÄÇSGCLIPÂú®Âú∫ÊôØÂõæÁîüÊàêÂíåÂä®‰ΩúÂÆö‰ΩçÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜSOTAÁªìÊûú„ÄÇÂú®ÂÖ∑Ë∫´ÁéØÂ¢É‰∏≠ÔºåESCA‰∏éSGCLIPÁöÑÁªìÂêàÊòæËëóÊèêÂçá‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÂ¢ûÂº∫Êô∫ËÉΩ‰ΩìÁöÑ‰∏ä‰∏ãÊñáÊÑüÁü•ËÉΩÂäõÔºåÂèØ‰ª•ÊèêÈ´òÂÖ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíå‰∫§‰∫íËÉΩÂäõÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂÆåÊàêÂêÑÁßç‰ªªÂä°„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÊé®Âä®ÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, existing MLLMs do not reliably capture fine-grained links between low-level visual features and high-level textual semantics, leading to weak grounding and inaccurate perception. To overcome this challenge, we propose ESCA, a framework that contextualizes embodied agents by grounding their perception in spatial-temporal scene graphs. At its core is SGCLIP, a novel, open-domain, promptable foundation model for generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+ open-domain videos using a neurosymbolic pipeline that aligns automatically generated captions with scene graphs produced by the model itself, eliminating the need for human-labeled annotations. We demonstrate that SGCLIP excels in both prompt-based inference and task-specific fine-tuning, achieving state-of-the-art results on scene graph generation and action localization benchmarks. ESCA with SGCLIP improves perception for embodied agents based on both open-source and commercial MLLMs, achieving state of-the-art performance across two embodied environments. Notably, ESCA significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines. We release the source code for SGCLIP model training at https://github.com/video-fm/LASER and for the embodied agent at https://github.com/video-fm/ESCA.

