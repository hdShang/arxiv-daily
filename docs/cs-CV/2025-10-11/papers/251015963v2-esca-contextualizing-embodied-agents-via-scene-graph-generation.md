---
layout: default
title: ESCA: Contextualizing Embodied Agents via Scene-Graph Generation
---

# ESCA: Contextualizing Embodied Agents via Scene-Graph Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15963" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15963v2</a>
  <a href="https://arxiv.org/pdf/2510.15963.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15963v2" onclick="toggleFavorite(this, '2510.15963v2', 'ESCA: Contextualizing Embodied Agents via Scene-Graph Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-11 (æ›´æ–°: 2025-10-27)

**å¤‡æ³¨**: Accepted as a Spotlight Paper at NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/video-fm/LASER) | [GITHUB](https://github.com/video-fm/ESCA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºESCAæ¡†æ¶ï¼Œé€šè¿‡åœºæ™¯å›¾ç”Ÿæˆå¢å¼ºå…·èº«æ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½ä½“` `åœºæ™¯å›¾ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ ` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `å¼€æ”¾åŸŸè§†é¢‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨å…·èº«æ™ºèƒ½ä½“ä¸­å­˜åœ¨è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬è¯­ä¹‰å…³è”ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ„ŸçŸ¥èƒ½åŠ›è¾ƒå¼±ã€‚
2. ESCAæ¡†æ¶é€šè¿‡å°†æ™ºèƒ½ä½“çš„æ„ŸçŸ¥ grounding åœ¨æ—¶ç©ºåœºæ™¯å›¾ä¸­ï¼Œå¢å¼ºäº†å…¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. SGCLIPæ¨¡å‹åœ¨åœºæ™¯å›¾ç”Ÿæˆå’ŒåŠ¨ä½œå®šä½ä»»åŠ¡ä¸Šå–å¾—äº†SOTAç»“æœï¼Œå¹¶æå‡äº†å…·èº«æ™ºèƒ½ä½“çš„æ„ŸçŸ¥æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨é€šç”¨å…·èº«æ™ºèƒ½ä½“æ–¹é¢å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLLMæ— æ³•å¯é åœ°æ•æ‰ä½å±‚è§†è§‰ç‰¹å¾å’Œé«˜å±‚æ–‡æœ¬è¯­ä¹‰ä¹‹é—´çš„ç»†ç²’åº¦è”ç³»ï¼Œå¯¼è‡´å¼± grounding å’Œä¸å‡†ç¡®çš„æ„ŸçŸ¥ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ESCAï¼Œä¸€ä¸ªé€šè¿‡å°†å…·èº«æ™ºèƒ½ä½“çš„æ„ŸçŸ¥ grounding åœ¨æ—¶ç©ºåœºæ™¯å›¾ä¸­æ¥æƒ…å¢ƒåŒ–å…·èº«æ™ºèƒ½ä½“çš„æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯SGCLIPï¼Œä¸€ç§æ–°é¢–çš„ã€å¼€æ”¾åŸŸçš„ã€å¯æç¤ºçš„åœºæ™¯å›¾ç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œå®ƒåŸºäºCLIPã€‚SGCLIPä½¿ç”¨ç¥ç»ç¬¦å·ç®¡é“åœ¨87K+å¼€æ”¾åŸŸè§†é¢‘ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥ç®¡é“å°†è‡ªåŠ¨ç”Ÿæˆçš„å­—å¹•ä¸æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„åœºæ™¯å›¾å¯¹é½ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹äººå·¥æ ‡æ³¨çš„éœ€æ±‚ã€‚æˆ‘ä»¬è¯æ˜äº†SGCLIPåœ¨åŸºäº prompt çš„æ¨ç†å’Œç‰¹å®šä»»åŠ¡å¾®è°ƒæ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œåœ¨åœºæ™¯å›¾ç”Ÿæˆå’ŒåŠ¨ä½œå®šä½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ESCAä¸SGCLIPæ”¹è¿›äº†åŸºäºå¼€æºå’Œå•†ä¸šMLLMçš„å…·èº«æ™ºèƒ½ä½“çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨ä¸¤ä¸ªå…·èº«ç¯å¢ƒä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒESCAæ˜¾è‘—å‡å°‘äº†æ™ºèƒ½ä½“æ„ŸçŸ¥é”™è¯¯ï¼Œå¹¶ä½¿å¼€æºæ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä¸“æœ‰åŸºçº¿ã€‚æˆ‘ä»¬å‘å¸ƒäº†SGCLIPæ¨¡å‹è®­ç»ƒçš„æºä»£ç åœ¨https://github.com/video-fm/LASERï¼Œä»¥åŠå…·èº«æ™ºèƒ½ä½“çš„ä»£ç åœ¨https://github.com/video-fm/ESCAã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…·èº«æ™ºèƒ½ä½“ä¾èµ–çš„MLLMæ— æ³•å‡†ç¡®æ•æ‰è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬è¯­ä¹‰ä¹‹é—´çš„ç»†ç²’åº¦è”ç³»ï¼Œå¯¼è‡´æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³ï¼Œå®¹æ˜“äº§ç”Ÿé”™è¯¯ã€‚è¿™é™åˆ¶äº†æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æœ‰æ•ˆäº¤äº’å’Œå†³ç­–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥åœºæ™¯å›¾æ¥å¢å¼ºæ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚åœºæ™¯å›¾èƒ½å¤Ÿæ˜¾å¼åœ°è¡¨ç¤ºåœºæ™¯ä¸­çš„ç‰©ä½“ã€å…³ç³»ä»¥åŠå®ƒä»¬ä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œä¸ºæ™ºèƒ½ä½“æä¾›æ›´ä¸°å¯Œçš„ç¯å¢ƒä¿¡æ¯ï¼Œå¸®åŠ©å…¶æ›´å¥½åœ°ç†è§£å’Œæ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šESCAæ¡†æ¶çš„æ ¸å¿ƒæ˜¯SGCLIPæ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸºäºCLIPçš„ã€å¼€æ”¾åŸŸçš„ã€å¯æç¤ºçš„åœºæ™¯å›¾ç”Ÿæˆæ¨¡å‹ã€‚SGCLIPé€šè¿‡ç¥ç»ç¬¦å·ç®¡é“åœ¨å¤§é‡å¼€æ”¾åŸŸè§†é¢‘ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥ç®¡é“è‡ªåŠ¨å°†ç”Ÿæˆçš„å­—å¹•ä¸åœºæ™¯å›¾å¯¹é½ã€‚ESCAæ¡†æ¶å°†SGCLIPç”Ÿæˆçš„åœºæ™¯å›¾ä½œä¸ºæ™ºèƒ½ä½“çš„è¾“å…¥ï¼Œä»è€Œå¢å¼ºå…¶æ„ŸçŸ¥èƒ½åŠ›ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼šè§†é¢‘è¾“å…¥ -> SGCLIPç”Ÿæˆåœºæ™¯å›¾ -> åœºæ™¯å›¾ä¸è§†è§‰ä¿¡æ¯è¾“å…¥MLLM -> æ™ºèƒ½ä½“å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šSGCLIPæ¨¡å‹æ˜¯å…³é”®åˆ›æ–°ç‚¹ã€‚å®ƒæ— éœ€äººå·¥æ ‡æ³¨ï¼Œè€Œæ˜¯é€šè¿‡ç¥ç»ç¬¦å·ç®¡é“è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒSGCLIPæ˜¯å¯æç¤ºçš„ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„ä»»åŠ¡è¿›è¡Œå®šåˆ¶ã€‚SGCLIPä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼Œå®ƒèƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®ã€æ›´ä¸°å¯Œçš„åœºæ™¯å›¾ï¼Œä»è€Œä¸ºæ™ºèƒ½ä½“æä¾›æ›´å…¨é¢çš„ç¯å¢ƒä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šSGCLIPçš„è®­ç»ƒä½¿ç”¨äº†å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œä»¥é¼“åŠ±æ¨¡å‹å­¦ä¹ è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬è¯­ä¹‰ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚ç¥ç»ç¬¦å·ç®¡é“åŒ…å«ä¸€ä¸ª captioning æ¨¡å‹å’Œä¸€ä¸ªåœºæ™¯å›¾ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒä»¬å…±åŒç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚æ¨¡å‹ä½¿ç”¨äº†Transformeræ¶æ„ï¼Œå¹¶é’ˆå¯¹åœºæ™¯å›¾ç”Ÿæˆä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å¯åœ¨è®ºæ–‡é™„å½•ä¸­æ‰¾åˆ°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒESCAæ¡†æ¶æ˜¾è‘—å‡å°‘äº†æ™ºèƒ½ä½“çš„æ„ŸçŸ¥é”™è¯¯ï¼Œå¹¶ä½¿å¼€æºæ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä¸“æœ‰åŸºçº¿ã€‚SGCLIPåœ¨åœºæ™¯å›¾ç”Ÿæˆå’ŒåŠ¨ä½œå®šä½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAç»“æœã€‚åœ¨å…·èº«ç¯å¢ƒä¸­ï¼ŒESCAä¸SGCLIPçš„ç»“åˆæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡å¢å¼ºæ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯ä»¥æé«˜å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œäº¤äº’èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå„ç§ä»»åŠ¡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨å…·èº«æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, existing MLLMs do not reliably capture fine-grained links between low-level visual features and high-level textual semantics, leading to weak grounding and inaccurate perception. To overcome this challenge, we propose ESCA, a framework that contextualizes embodied agents by grounding their perception in spatial-temporal scene graphs. At its core is SGCLIP, a novel, open-domain, promptable foundation model for generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+ open-domain videos using a neurosymbolic pipeline that aligns automatically generated captions with scene graphs produced by the model itself, eliminating the need for human-labeled annotations. We demonstrate that SGCLIP excels in both prompt-based inference and task-specific fine-tuning, achieving state-of-the-art results on scene graph generation and action localization benchmarks. ESCA with SGCLIP improves perception for embodied agents based on both open-source and commercial MLLMs, achieving state of-the-art performance across two embodied environments. Notably, ESCA significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines. We release the source code for SGCLIP model training at https://github.com/video-fm/LASER and for the embodied agent at https://github.com/video-fm/ESCA.

