---
layout: default
title: CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization
---

# CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17847" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17847v1</a>
  <a href="https://arxiv.org/pdf/2510.17847.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17847v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17847v1', 'CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yichen Yan, Ming Zhong, Qi Zhu, Xiaoling Gu, Jinpeng Chen, Huan Li

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-11

**å¤‡æ³¨**: 22 pages, 8 figures, 39th Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CoIDOï¼šé€šè¿‡è€¦åˆé‡è¦æ€§-å¤šæ ·æ€§ä¼˜åŒ–å®ç°è§†è§‰æŒ‡ä»¤è°ƒä¼˜çš„é«˜æ•ˆæ•°æ®é€‰æ‹©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰æŒ‡ä»¤è°ƒä¼˜` `æ•°æ®é€‰æ‹©` `å¤šæ¨¡æ€å­¦ä¹ ` `é‡è¦æ€§é‡‡æ ·` `å¤šæ ·æ€§ä¼˜åŒ–` `è½»é‡çº§æ¨¡å‹` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰æŒ‡ä»¤è°ƒä¼˜æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œä¸”æ•°æ®é€‰æ‹©æ–¹æ³•é€šå¸¸è®¡ç®—å¼€é”€å¤§ï¼Œé‡è¦æ€§å’Œå¤šæ ·æ€§å¤„ç†åˆ†ç¦»ã€‚
2. CoIDOé€šè¿‡è”åˆä¼˜åŒ–æ•°æ®é‡è¦æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§è¯„åˆ†å™¨åœ¨å°æ ·æœ¬ä¸Šå­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCoIDOä»…ä½¿ç”¨20%çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€‰æ‹©20%çš„æ•°æ®è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ï¼Œå³å¯è¾¾åˆ°å…¨é‡æ•°æ®å¾®è°ƒæ€§èƒ½çš„98.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸¥é‡ä¾èµ–æŒ‡ä»¤è°ƒä¼˜æ¥å¯¹é½è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ï¼Œä½†å¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒè®¡ç®—æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦ç“¶é¢ˆã€‚ç°æœ‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•æ—¨åœ¨é€šè¿‡é€‰æ‹©é‡è¦ä¸”å¤šæ ·åŒ–çš„å­é›†æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸å­˜åœ¨ä¸¤ä¸ªå…³é”®ç¼ºé™·ï¼šå¤„ç†æ•´ä¸ªæ•°æ®é›†å¸¦æ¥çš„é«˜è®¡ç®—å¼€é”€ï¼Œä»¥åŠç”±äºå¯¹é‡è¦æ€§å’Œå¤šæ ·æ€§çš„å•ç‹¬å¤„ç†è€Œå¯¼è‡´çš„æ•°æ®é€‰æ‹©æ¬¡ä¼˜ã€‚æˆ‘ä»¬å¼•å…¥äº†CoIDOï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŒç›®æ ‡æ¡†æ¶ï¼Œå®ƒè”åˆä¼˜åŒ–æ•°æ®é‡è¦æ€§å’Œå¤šæ ·æ€§ï¼Œä»¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚ä¸ç°æœ‰æ–¹æ³•éœ€è¦å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ˜‚è´µè¯„ä¼°ä¸åŒï¼ŒCoIDOé‡‡ç”¨è½»é‡çº§æ’ä»¶è¯„åˆ†å™¨ã€‚è¯¥è¯„åˆ†å™¨ä»…åœ¨å°‘é‡éšæœºæ•°æ®æ ·æœ¬ä¸Šè®­ç»ƒï¼Œä»¥å­¦ä¹ å€™é€‰é›†çš„åˆ†å¸ƒï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚é€šè¿‡åˆ©ç”¨åŸºäºåŒæ–¹å·®ä¸ç¡®å®šæ€§çš„å…¬å¼ï¼ŒCoIDOåœ¨è®­ç»ƒæœŸé—´æœ‰æ•ˆåœ°å¹³è¡¡äº†é‡è¦æ€§å’Œå¤šæ ·æ€§ï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ•°æ®é€‰æ‹©ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨20%çš„éšæœºæŠ½æ ·æ•°æ®è®­ç»ƒäº†CoIDOè¯„åˆ†å™¨ã€‚è®­ç»ƒå®Œæˆåï¼ŒCoIDOè¢«åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼Œä»¥é€‰æ‹©20%çš„å­é›†è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„LLaVA-1.5-7Bæ¨¡å‹ä¸Šï¼Œé’ˆå¯¹åä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿™ä¸ªé€‰å®šçš„å­é›†å¹³å‡å®ç°äº†å®Œæ•´æ•°æ®å¾®è°ƒæ€§èƒ½çš„98.2%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰æŒ‡ä»¤è°ƒä¼˜ä¸­ï¼Œä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒæ—¶è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•é€šå¸¸éœ€è¦å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œè®¡ç®—å¼€é”€å¤§ï¼Œå¹¶ä¸”å°†æ•°æ®çš„é‡è¦æ€§å’Œå¤šæ ·æ€§åˆ†å¼€è€ƒè™‘ï¼Œå¯¼è‡´é€‰æ‹©çš„æ•°æ®å­é›†å¹¶éæœ€ä¼˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoIDOçš„æ ¸å¿ƒæ€è·¯æ˜¯è”åˆä¼˜åŒ–æ•°æ®çš„é‡è¦æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶åœ¨ä¸€ä¸ªè½»é‡çº§çš„æ¡†æ¶ä¸‹å®ç°é«˜æ•ˆçš„æ•°æ®é€‰æ‹©ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„è¯„åˆ†å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿä»å°æ ·æœ¬æ•°æ®ä¸­å­¦ä¹ åˆ°æ•´ä¸ªæ•°æ®é›†çš„åˆ†å¸ƒï¼Œä»è€Œé¿å…å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ˜‚è´µçš„è¯„ä¼°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoIDOåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) è®­ç»ƒè½»é‡çº§è¯„åˆ†å™¨ï¼šä»åŸå§‹æ•°æ®é›†ä¸­éšæœºæŠ½å–ä¸€å°éƒ¨åˆ†æ ·æœ¬ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªè¯„åˆ†å™¨ï¼Œè¯¥è¯„åˆ†å™¨èƒ½å¤Ÿè¯„ä¼°æ•°æ®çš„é‡è¦æ€§å’Œå¤šæ ·æ€§ã€‚2) æ•°æ®é€‰æ‹©ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„è¯„åˆ†å™¨å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªæ—¢é‡è¦åˆå…·æœ‰å¤šæ ·æ€§çš„æ•°æ®å­é›†ç”¨äºæŒ‡ä»¤è°ƒä¼˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoIDOçš„å…³é”®åˆ›æ–°åœ¨äºè”åˆä¼˜åŒ–é‡è¦æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§è¯„åˆ†å™¨æ¥é™ä½è®¡ç®—æˆæœ¬ã€‚ä¸ç°æœ‰æ–¹æ³•éœ€è¦å¤„ç†æ•´ä¸ªæ•°æ®é›†ä¸åŒï¼ŒCoIDOä»…éœ€å¤„ç†ä¸€å°éƒ¨åˆ†æ ·æœ¬å³å¯å­¦ä¹ åˆ°æ•°æ®é›†çš„åˆ†å¸ƒï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ•°æ®é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒCoIDOä½¿ç”¨åŸºäºåŒæ–¹å·®ä¸ç¡®å®šæ€§çš„å…¬å¼æ¥å¹³è¡¡é‡è¦æ€§å’Œå¤šæ ·æ€§ï¼Œä»è€Œé€‰æ‹©æ›´å…·ä»£è¡¨æ€§çš„æ•°æ®å­é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šCoIDOä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»ç½‘ç»œä½œä¸ºè¯„åˆ†å™¨ï¼Œè¯¥ç½‘ç»œä»¥å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ï¼Œè¡¨ç¤ºè¯¥æ•°æ®æ ·æœ¬çš„é‡è¦æ€§å’Œå¤šæ ·æ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘æ•°æ®çš„é‡è¦æ€§å’Œå¤šæ ·æ€§ã€‚è®ºæ–‡é‡‡ç”¨åŸºäºåŒæ–¹å·®ä¸ç¡®å®šæ€§çš„å…¬å¼æ¥å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ã€‚å…·ä½“è€Œè¨€ï¼ŒæŸå¤±å‡½æ•°å¯ä»¥è®¾è®¡ä¸ºé‡è¦æ€§æŸå¤±å’Œå¤šæ ·æ€§æŸå¤±çš„åŠ æƒå’Œï¼Œæƒé‡ç”±åŒæ–¹å·®ä¸ç¡®å®šæ€§ä¼°è®¡å¾—åˆ°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCoIDOä»…ä½¿ç”¨20%çš„éšæœºæŠ½æ ·æ•°æ®è®­ç»ƒè¯„åˆ†å™¨ï¼Œå¹¶é€‰æ‹©20%çš„æ•°æ®å­é›†è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ï¼Œåœ¨LLaVA-1.5-7Bæ¨¡å‹ä¸Šï¼Œé’ˆå¯¹åä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹³å‡å®ç°äº†å®Œæ•´æ•°æ®å¾®è°ƒæ€§èƒ½çš„98.2%ã€‚è¿™è¡¨æ˜CoIDOèƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æ¥è¿‘å…¨é‡æ•°æ®è®­ç»ƒçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoIDOå¯åº”ç”¨äºå„ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚é€šè¿‡é«˜æ•ˆçš„æ•°æ®é€‰æ‹©ï¼ŒCoIDOèƒ½å¤Ÿé™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹å¼€å‘å‘¨æœŸï¼Œå¹¶æå‡æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤šæ¨¡æ€æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) rely heavily on instruction tuning to align vision and language capabilities, yet the computational cost of training on large-scale datasets remains a major bottleneck. Existing data selection methods aim to mitigate this by selecting important and diverse subsets, but they often suffer from two critical drawbacks: high computational overhead from processing the entire dataset and suboptimal data selection due to separate treatment of importance and diversity.
>   We introduce CoIDO, a novel dual-objective framework that jointly optimizes data importance and diversity to overcome these challenges. Unlike existing approaches that require costly evaluations across the whole dataset, CoIDO employs a lightweight plug-in scorer. This scorer is trained on just a small random sample of data to learn the distribution of the candidate set, drastically reducing computational demands. By leveraging a homoscedastic uncertainty-based formulation, CoIDO effectively balances importance and diversity during training, enabling efficient and scalable data selection.
>   In our experiments, we trained the CoIDO scorer using only 20 percent of randomly sampled data. Once trained, CoIDO was applied to the entire dataset to select a 20 percent subset for instruction tuning. On the widely used LLaVA-1.5-7B model across ten downstream tasks, this selected subset achieved an impressive 98.2 percent of the performance of full-data fine-tuning, on average.

