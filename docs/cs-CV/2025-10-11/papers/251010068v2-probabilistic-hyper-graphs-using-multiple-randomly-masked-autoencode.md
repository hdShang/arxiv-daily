---
layout: default
title: Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning
---

# Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10068" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10068v2</a>
  <a href="https://arxiv.org/pdf/2510.10068.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10068v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10068v2', 'Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: PÃ®rvu Mihai-Cristian, Marius Leordeanu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-11 (æ›´æ–°: 2025-11-25)

**å¤‡æ³¨**: Submitted to Neurocomputing

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPHG-MAEæ¨¡å‹ï¼Œç»“åˆç¥ç»å›¾å’Œæ©ç è‡ªç¼–ç å™¨ï¼Œç”¨äºåŠç›‘ç£å¤šæ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `åŠç›‘ç£å­¦ä¹ ` `æ©ç è‡ªç¼–ç å™¨` `æ¦‚ç‡è¶…å›¾` `çŸ¥è¯†è’¸é¦` `å¤šä»»åŠ¡å­¦ä¹ ` `æ— äººæœºåœºæ™¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆèåˆä¸åŒæ¨¡æ€ä¿¡æ¯ï¼Œä¸”ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ã€‚
2. æå‡ºPHG-MAEæ¨¡å‹ï¼Œé€šè¿‡éšæœºæ©ç æ¨¡æ€æ¨¡æ‹Ÿè¶…è¾¹åˆ†å¸ƒï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œå®ç°æ¨¡æ€èåˆã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ— äººæœºåœºæ™¯æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸”å¯é€šè¿‡çŸ¥è¯†è’¸é¦å‹ç¼©æ¨¡å‹è§„æ¨¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¡ç®—æœºè§†è§‰é¢†åŸŸå—ç›Šäºè·¨å¤šç§æ¨¡æ€çš„ä¸°å¯Œæ•°æ®ï¼Œä»è€Œæ”¹è¿›å„ç§è§†è§‰ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œäººä»¬éå¸¸å…³æ³¨é€šè¿‡æ©ç è‡ªç¼–ç å™¨(MAE)è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šå¸¸å°†å…¶ç”¨ä½œä¼˜åŒ–ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»æˆ–å›å½’ï¼‰çš„ç¬¬ä¸€æ­¥ã€‚è¿™éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä¸éœ€è¦ä»»ä½•æ‰‹åŠ¨æ ‡è®°çš„æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºæ©ç è‡ªç¼–ç å™¨çš„æ¦‚ç‡è¶…å›¾(PHG-MAE)ï¼šä¸€ç§æ–°é¢–çš„æ¨¡å‹ï¼Œå®ƒåœ¨å…±åŒçš„ç†è®ºæ¡†æ¶ä¸‹ç»Ÿä¸€äº†ç¥ç»å›¾çš„ç»å…¸å·¥ä½œä¸æ©ç è‡ªç¼–ç å™¨çš„ç°ä»£æ–¹æ³•ã€‚é€šè¿‡éšæœºæ©ç›–æ•´ä¸ªæ¨¡æ€ï¼ˆè€Œä¸ä»…ä»…æ˜¯patchesï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨æ¯æ¬¡å‰å‘ä¼ é€’ä¸­ä»è¶…è¾¹çš„åˆ†å¸ƒä¸­é‡‡æ ·ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é€šè¿‡å°†é¢„è®­ç»ƒå’Œå¾®è°ƒç»“åˆåˆ°å•ä¸ªè®­ç»ƒå¾ªç¯ä¸­æ¥æ”¹è¿›æ ‡å‡†çš„MAEç®—æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ›å»ºæ¨ç†æ—¶é›†æˆï¼Œé€šè¿‡èšåˆæ¥æé«˜æœ€ç»ˆé¢„æµ‹æ€§èƒ½å’Œä¸€è‡´æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬å¯ä»¥å¯¹é›†æˆåº”ç”¨çŸ¥è¯†è’¸é¦ï¼Œè€Œæ€§èƒ½æŸå¤±å¾ˆå°ï¼Œå³ä½¿æ˜¯å‚æ•°å°‘äº100ä¸‡çš„æ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è™½ç„¶æˆ‘ä»¬çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åŒ…å«å¤šç§ä¸–ç•Œè§£é‡Šå’Œæ¨¡æ€çš„å®¤å¤–æ— äººæœºåœºæ™¯ï¼Œä½†ç›¸åŒçš„æ­¥éª¤å¯ä»¥åœ¨å…¶ä»–ç±»ä¼¼é¢†åŸŸï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶æˆ–å®¤å†…æœºå™¨äººï¼‰ä¸­éµå¾ªã€‚ä¸ºäº†ç®€åŒ–é›†æˆç”¨äºè®¡ç®—æœºè§†è§‰å¤šæ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ (MTL)åœºæ™¯çš„å¤–éƒ¨é¢„è®­ç»ƒä¸“å®¶çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ•°æ®ç®¡é“è½¯ä»¶ã€‚ä½¿ç”¨æ­¤å·¥å…·ï¼Œæˆ‘ä»¬åˆ›å»ºå¹¶å‘å¸ƒäº†Dronescapesæ•°æ®é›†çš„å®Œå…¨è‡ªåŠ¨åŒ–æ‰©å±•ã€‚æ‰€æœ‰æŠ€æœ¯ç»†èŠ‚ã€ä»£ç å’Œé‡ç°æ­¥éª¤éƒ½å·²å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŠç›‘ç£å¤šæ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æ ‡æ³¨ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯æå‡æ¨¡å‹æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä¸”éš¾ä»¥å……åˆ†èåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¥ç»å›¾çš„æ€æƒ³ä¸æ©ç è‡ªç¼–ç å™¨ç›¸ç»“åˆï¼Œæ„å»ºæ¦‚ç‡è¶…å›¾ã€‚é€šè¿‡éšæœºæ©ç ä¸åŒçš„æ¨¡æ€ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸åŒæ¨¡æ€ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ä»è¶…è¾¹çš„åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œä»è€Œå®ç°æ¨¡æ€èåˆã€‚åŒæ—¶ï¼Œå°†é¢„è®­ç»ƒå’Œå¾®è°ƒæ•´åˆåˆ°å•ä¸ªè®­ç»ƒå¾ªç¯ä¸­ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPHG-MAEæ¨¡å‹ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) å¤šä¸ªæ©ç è‡ªç¼–ç å™¨(MAE)ï¼Œæ¯ä¸ªMAEå¤„ç†ä¸€ç§æ¨¡æ€çš„æ•°æ®ï¼Œå¹¶éšæœºæ©ç éƒ¨åˆ†æ¨¡æ€ï¼›2) æ¦‚ç‡è¶…å›¾æ„å»ºæ¨¡å—ï¼Œæ ¹æ®MAEçš„è¾“å‡ºæ„å»ºè¶…å›¾ï¼Œè¶…è¾¹è¡¨ç¤ºä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³ç³»ï¼›3) å¤šä»»åŠ¡å­¦ä¹ æ¨¡å—ï¼Œåˆ©ç”¨è¶…å›¾ä¿¡æ¯è¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œä¾‹å¦‚åˆ†ç±»ã€å›å½’ç­‰ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬æ•°æ®è¾“å…¥ã€MAEç¼–ç ã€è¶…å›¾æ„å»ºã€å¤šä»»åŠ¡å­¦ä¹ å’Œç»“æœè¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ¦‚ç‡è¶…å›¾ä¸æ©ç è‡ªç¼–ç å™¨ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡æ€èåˆæ–¹æ³•ç›¸æ¯”ï¼ŒPHG-MAEèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡éšæœºæ©ç æ¨¡æ€æ¥æ¨¡æ‹Ÿæ•°æ®ç¼ºå¤±çš„æƒ…å†µï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œå°†é¢„è®­ç»ƒå’Œå¾®è°ƒæ•´åˆåˆ°å•ä¸ªè®­ç»ƒå¾ªç¯ä¸­ï¼Œä¹Ÿæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸Šï¼Œè®ºæ–‡é‡‡ç”¨äº†éšæœºæ©ç ç­–ç•¥ï¼Œæ¯æ¬¡éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†æ¨¡æ€è¿›è¡Œæ©ç ï¼Œå¹¶ä½¿ç”¨MAEé‡å»ºè¢«æ©ç çš„æ¨¡æ€ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±å’Œå¤šä»»åŠ¡å­¦ä¹ æŸå¤±ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œå¯ä»¥é‡‡ç”¨ä¸åŒçš„MAEæ¶æ„ï¼Œä¾‹å¦‚ViTç­‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢ç´¢äº†çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå°†å¤§å‹é›†æˆæ¨¡å‹è’¸é¦åˆ°å°å‹æ¨¡å‹ä¸­ï¼Œä»¥å‡å°‘æ¨¡å‹å‚æ•°é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†PHG-MAEæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚åœ¨Dronescapesæ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æ ‡æ³¨ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦ï¼Œå¯ä»¥å°†æ¨¡å‹å‚æ•°é‡å‡å°‘åˆ°1Mä»¥ä¸‹ï¼Œè€Œæ€§èƒ½æŸå¤±å¾ˆå°ï¼Œè¿™ä½¿å¾—è¯¥æ¨¡å‹æ›´æ˜“äºéƒ¨ç½²åˆ°èµ„æºå—é™çš„è®¾å¤‡ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šç§éœ€è¦å¤šæ¨¡æ€ä¿¡æ¯èåˆçš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€é¥æ„Ÿå›¾åƒåˆ†æç­‰ã€‚é€šè¿‡èåˆä¸åŒä¼ æ„Ÿå™¨çš„æ•°æ®ï¼Œä¾‹å¦‚æ‘„åƒå¤´ã€æ¿€å…‰é›·è¾¾ã€é›·è¾¾ç­‰ï¼Œå¯ä»¥æé«˜ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæå‡ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºåŒ»ç–—å›¾åƒåˆ†æã€é‡‘èé£é™©è¯„ä¼°ç­‰é¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The computer vision domain has greatly benefited from an abundance of data across many modalities to improve on various visual tasks. Recently, there has been a lot of focus on self-supervised pre-training methods through Masked Autoencoders (MAE) \cite{he2022masked,bachmann2022multimae}, usually used as a first step before optimizing for a downstream task, such as classification or regression. This is very useful as it doesn't require any manually labeled data. In this work, we introduce Probabilistic Hyper-Graphs using Masked Autoencoders (PHG-MAE): a novel model that unifies the classical work on neural graphs \cite{leordeanu2021semi} with the modern approach of masked autoencoders under a common theoretical framework. Through random masking of entire modalities, not just patches, the model samples from the distribution of hyper-edges on each forward pass. Additionally, the model adapts the standard MAE algorithm by combining pre-training and fine-tuning into a single training loop. Moreover, our approach enables the creation of inference-time ensembles which, through aggregation, boost the final prediction performance and consistency. Lastly, we show that we can apply knowledge distillation on top of the ensembles with little loss in performance, even with models that have fewer than 1M parameters. While our work mostly focuses on outdoor UAV scenes that contain multiple world interpretations and modalities, the same steps can be followed in other similar domains, such as autonomous driving or indoor robotics. In order to streamline the process of integrating external pre-trained experts for computer vision multi-modal multi-task learning (MTL) scenarios, we developed a data-pipeline software. Using this tool, we have created and released a fully-automated extension of the Dronescapes dataset. All the technical details, code and reproduction steps are publicly released.

