---
layout: default
title: Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning
---

# Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10068" target="_blank" class="toolbar-btn">arXiv: 2510.10068v2</a>
    <a href="https://arxiv.org/pdf/2510.10068.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10068v2" 
            onclick="toggleFavorite(this, '2510.10068v2', 'Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: P√Ærvu Mihai-Cristian, Marius Leordeanu

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-11 (Êõ¥Êñ∞: 2025-11-25)

**Â§áÊ≥®**: Submitted to Neurocomputing

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PHG-MAEÊ®°ÂûãÔºåÁªìÂêàÁ•ûÁªèÂõæÂíåÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºåÁî®‰∫éÂçäÁõëÁù£Â§öÊ®°ÊÄÅÂ§ö‰ªªÂä°Â≠¶‰π†„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂçäÁõëÁù£Â≠¶‰π†` `Êé©Á†ÅËá™ÁºñÁ†ÅÂô®` `Ê¶ÇÁéáË∂ÖÂõæ` `Áü•ËØÜËí∏È¶è` `Â§ö‰ªªÂä°Â≠¶‰π†` `Êó†‰∫∫Êú∫Âú∫ÊôØ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÂ§ö‰ªªÂä°Â≠¶‰π†‰∏≠ÔºåÈöæ‰ª•ÊúâÊïàËûçÂêà‰∏çÂêåÊ®°ÊÄÅ‰ø°ÊÅØÔºå‰∏î‰æùËµñÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆ„ÄÇ
2. ÊèêÂá∫PHG-MAEÊ®°ÂûãÔºåÈÄöËøáÈöèÊú∫Êé©Á†ÅÊ®°ÊÄÅÊ®°ÊãüË∂ÖËæπÂàÜÂ∏ÉÔºåÂπ∂ÁªìÂêàÈ¢ÑËÆ≠ÁªÉÂíåÂæÆË∞ÉÔºåÂÆûÁé∞Ê®°ÊÄÅËûçÂêà„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®Êó†‰∫∫Êú∫Âú∫ÊôØÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰∏îÂèØÈÄöËøáÁü•ËØÜËí∏È¶èÂéãÁº©Ê®°ÂûãËßÑÊ®°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÂèóÁõä‰∫éË∑®Â§öÁßçÊ®°ÊÄÅÁöÑ‰∏∞ÂØåÊï∞ÊçÆÔºå‰ªéËÄåÊîπËøõÂêÑÁßçËßÜËßâ‰ªªÂä°„ÄÇÊúÄËøëÔºå‰∫∫‰ª¨ÈùûÂ∏∏ÂÖ≥Ê≥®ÈÄöËøáÊé©Á†ÅËá™ÁºñÁ†ÅÂô®(MAE)ËøõË°åËá™ÁõëÁù£È¢ÑËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåÈÄöÂ∏∏Â∞ÜÂÖ∂Áî®‰Ωú‰ºòÂåñ‰∏ãÊ∏∏‰ªªÂä°ÔºàÂ¶ÇÂàÜÁ±ªÊàñÂõûÂΩíÔºâÁöÑÁ¨¨‰∏ÄÊ≠•„ÄÇËøôÈùûÂ∏∏ÊúâÁî®ÔºåÂõ†‰∏∫ÂÆÉ‰∏çÈúÄË¶Å‰ªª‰ΩïÊâãÂä®Ê†áËÆ∞ÁöÑÊï∞ÊçÆ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÁöÑÊ¶ÇÁéáË∂ÖÂõæ(PHG-MAE)Ôºö‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ®°ÂûãÔºåÂÆÉÂú®ÂÖ±ÂêåÁöÑÁêÜËÆ∫Ê°ÜÊû∂‰∏ãÁªü‰∏Ä‰∫ÜÁ•ûÁªèÂõæÁöÑÁªèÂÖ∏Â∑•‰Ωú‰∏éÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÁöÑÁé∞‰ª£ÊñπÊ≥ï„ÄÇÈÄöËøáÈöèÊú∫Êé©ÁõñÊï¥‰∏™Ê®°ÊÄÅÔºàËÄå‰∏ç‰ªÖ‰ªÖÊòØpatchesÔºâÔºåËØ•Ê®°ÂûãÂú®ÊØèÊ¨°ÂâçÂêë‰º†ÈÄí‰∏≠‰ªéË∂ÖËæπÁöÑÂàÜÂ∏É‰∏≠ÈááÊ†∑„ÄÇÊ≠§Â§ñÔºåËØ•Ê®°ÂûãÈÄöËøáÂ∞ÜÈ¢ÑËÆ≠ÁªÉÂíåÂæÆË∞ÉÁªìÂêàÂà∞Âçï‰∏™ËÆ≠ÁªÉÂæ™ÁéØ‰∏≠Êù•ÊîπËøõÊ†áÂáÜÁöÑMAEÁÆóÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÂàõÂª∫Êé®ÁêÜÊó∂ÈõÜÊàêÔºåÈÄöËøáËÅöÂêàÊù•ÊèêÈ´òÊúÄÁªàÈ¢ÑÊµãÊÄßËÉΩÂíå‰∏ÄËá¥ÊÄß„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Ë°®ÊòéÊàë‰ª¨ÂèØ‰ª•ÂØπÈõÜÊàêÂ∫îÁî®Áü•ËØÜËí∏È¶èÔºåËÄåÊÄßËÉΩÊçüÂ§±ÂæàÂ∞èÔºåÂç≥‰ΩøÊòØÂèÇÊï∞Â∞ë‰∫é100‰∏áÁöÑÊ®°Âûã‰πüÊòØÂ¶ÇÊ≠§„ÄÇËôΩÁÑ∂Êàë‰ª¨ÁöÑÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂåÖÂê´Â§öÁßç‰∏ñÁïåËß£ÈáäÂíåÊ®°ÊÄÅÁöÑÂÆ§Â§ñÊó†‰∫∫Êú∫Âú∫ÊôØÔºå‰ΩÜÁõ∏ÂêåÁöÑÊ≠•È™§ÂèØ‰ª•Âú®ÂÖ∂‰ªñÁ±ª‰ººÈ¢ÜÂüüÔºàÂ¶ÇËá™Âä®È©æÈ©∂ÊàñÂÆ§ÂÜÖÊú∫Âô®‰∫∫Ôºâ‰∏≠ÈÅµÂæ™„ÄÇ‰∏∫‰∫ÜÁÆÄÂåñÈõÜÊàêÁî®‰∫éËÆ°ÁÆóÊú∫ËßÜËßâÂ§öÊ®°ÊÄÅÂ§ö‰ªªÂä°Â≠¶‰π†(MTL)Âú∫ÊôØÁöÑÂ§ñÈÉ®È¢ÑËÆ≠ÁªÉ‰∏ìÂÆ∂ÁöÑËøáÁ®ãÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Êï∞ÊçÆÁÆ°ÈÅìËΩØ‰ª∂„ÄÇ‰ΩøÁî®Ê≠§Â∑•ÂÖ∑ÔºåÊàë‰ª¨ÂàõÂª∫Âπ∂ÂèëÂ∏É‰∫ÜDronescapesÊï∞ÊçÆÈõÜÁöÑÂÆåÂÖ®Ëá™Âä®ÂåñÊâ©Â±ï„ÄÇÊâÄÊúâÊäÄÊúØÁªÜËäÇ„ÄÅ‰ª£Á†ÅÂíåÈáçÁé∞Ê≠•È™§ÈÉΩÂ∑≤ÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂçäÁõëÁù£Â§öÊ®°ÊÄÅÂ§ö‰ªªÂä°Â≠¶‰π†ÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®Êï∞ÊçÆÊ†áÊ≥®Á®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ¶Ç‰ΩïÊúâÊïàÂà©Áî®Â§öÊ®°ÊÄÅ‰ø°ÊÅØÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÔºå‰∏îÈöæ‰ª•ÂÖÖÂàÜËûçÂêà‰∏çÂêåÊ®°ÊÄÅÁöÑÁâπÂæÅÔºåÂØºËá¥Ê®°ÂûãÊ≥õÂåñËÉΩÂäõÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁ•ûÁªèÂõæÁöÑÊÄùÊÉ≥‰∏éÊé©Á†ÅËá™ÁºñÁ†ÅÂô®Áõ∏ÁªìÂêàÔºåÊûÑÂª∫Ê¶ÇÁéáË∂ÖÂõæ„ÄÇÈÄöËøáÈöèÊú∫Êé©Á†Å‰∏çÂêåÁöÑÊ®°ÊÄÅÔºåÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂‰ªéË∂ÖËæπÁöÑÂàÜÂ∏É‰∏≠ÈááÊ†∑Ôºå‰ªéËÄåÂÆûÁé∞Ê®°ÊÄÅËûçÂêà„ÄÇÂêåÊó∂ÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÂíåÂæÆË∞ÉÊï¥ÂêàÂà∞Âçï‰∏™ËÆ≠ÁªÉÂæ™ÁéØ‰∏≠ÔºåÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPHG-MAEÊ®°Âûã‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) Â§ö‰∏™Êé©Á†ÅËá™ÁºñÁ†ÅÂô®(MAE)ÔºåÊØè‰∏™MAEÂ§ÑÁêÜ‰∏ÄÁßçÊ®°ÊÄÅÁöÑÊï∞ÊçÆÔºåÂπ∂ÈöèÊú∫Êé©Á†ÅÈÉ®ÂàÜÊ®°ÊÄÅÔºõ2) Ê¶ÇÁéáË∂ÖÂõæÊûÑÂª∫Ê®°ÂùóÔºåÊ†πÊçÆMAEÁöÑËæìÂá∫ÊûÑÂª∫Ë∂ÖÂõæÔºåË∂ÖËæπË°®Á§∫‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºõ3) Â§ö‰ªªÂä°Â≠¶‰π†Ê®°ÂùóÔºåÂà©Áî®Ë∂ÖÂõæ‰ø°ÊÅØËøõË°åÂ§ö‰ªªÂä°Â≠¶‰π†Ôºå‰æãÂ¶ÇÂàÜÁ±ª„ÄÅÂõûÂΩíÁ≠â„ÄÇÊï¥‰∏™ÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆËæìÂÖ•„ÄÅMAEÁºñÁ†Å„ÄÅË∂ÖÂõæÊûÑÂª∫„ÄÅÂ§ö‰ªªÂä°Â≠¶‰π†ÂíåÁªìÊûúËæìÂá∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÊ¶ÇÁéáË∂ÖÂõæ‰∏éÊé©Á†ÅËá™ÁºñÁ†ÅÂô®Áõ∏ÁªìÂêàÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅËûçÂêàÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÊ®°ÊÄÅËûçÂêàÊñπÊ≥ïÁõ∏ÊØîÔºåPHG-MAEËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâ‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ªÔºåÂπ∂‰∏îÂèØ‰ª•ÈÄöËøáÈöèÊú∫Êé©Á†ÅÊ®°ÊÄÅÊù•Ê®°ÊãüÊï∞ÊçÆÁº∫Â§±ÁöÑÊÉÖÂÜµÔºåÊèêÈ´òÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊ≠§Â§ñÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÂíåÂæÆË∞ÉÊï¥ÂêàÂà∞Âçï‰∏™ËÆ≠ÁªÉÂæ™ÁéØ‰∏≠Ôºå‰πüÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂÖ∑‰ΩìÂÆûÁé∞‰∏äÔºåËÆ∫ÊñáÈááÁî®‰∫ÜÈöèÊú∫Êé©Á†ÅÁ≠ñÁï•ÔºåÊØèÊ¨°ÈöèÊú∫ÈÄâÊã©‰∏ÄÈÉ®ÂàÜÊ®°ÊÄÅËøõË°åÊé©Á†ÅÔºåÂπ∂‰ΩøÁî®MAEÈáçÂª∫Ë¢´Êé©Á†ÅÁöÑÊ®°ÊÄÅ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÈáçÂª∫ÊçüÂ§±ÂíåÂ§ö‰ªªÂä°Â≠¶‰π†ÊçüÂ§±„ÄÇÁΩëÁªúÁªìÊûÑÊñπÈù¢ÔºåÂèØ‰ª•ÈááÁî®‰∏çÂêåÁöÑMAEÊû∂ÊûÑÔºå‰æãÂ¶ÇViTÁ≠â„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊé¢Á¥¢‰∫ÜÁü•ËØÜËí∏È¶èÊäÄÊúØÔºåÂ∞ÜÂ§ßÂûãÈõÜÊàêÊ®°ÂûãËí∏È¶èÂà∞Â∞èÂûãÊ®°Âûã‰∏≠Ôºå‰ª•ÂáèÂ∞ëÊ®°ÂûãÂèÇÊï∞Èáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜPHG-MAEÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇÂú®DronescapesÊï∞ÊçÆÈõÜ‰∏äÔºåËØ•Ê®°ÂûãÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÊ†áÊ≥®Á®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÁü•ËØÜËí∏È¶èÔºåÂèØ‰ª•Â∞ÜÊ®°ÂûãÂèÇÊï∞ÈáèÂáèÂ∞ëÂà∞1M‰ª•‰∏ãÔºåËÄåÊÄßËÉΩÊçüÂ§±ÂæàÂ∞èÔºåËøô‰ΩøÂæóËØ•Ê®°ÂûãÊõ¥Êòì‰∫éÈÉ®ÁΩ≤Âà∞ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏ä„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ§öÁßçÈúÄË¶ÅÂ§öÊ®°ÊÄÅ‰ø°ÊÅØËûçÂêàÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÈÅ•ÊÑüÂõæÂÉèÂàÜÊûêÁ≠â„ÄÇÈÄöËøáËûçÂêà‰∏çÂêå‰º†ÊÑüÂô®ÁöÑÊï∞ÊçÆÔºå‰æãÂ¶ÇÊëÑÂÉèÂ§¥„ÄÅÊøÄÂÖâÈõ∑Ëææ„ÄÅÈõ∑ËææÁ≠âÔºåÂèØ‰ª•ÊèêÈ´òÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÔºå‰ªéËÄåÊèêÂçáÁ≥ªÁªüÁöÑÂÆâÂÖ®ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éÂåªÁñóÂõæÂÉèÂàÜÊûê„ÄÅÈáëËûçÈ£éÈô©ËØÑ‰º∞Á≠âÈ¢ÜÂüüÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The computer vision domain has greatly benefited from an abundance of data across many modalities to improve on various visual tasks. Recently, there has been a lot of focus on self-supervised pre-training methods through Masked Autoencoders (MAE) \cite{he2022masked,bachmann2022multimae}, usually used as a first step before optimizing for a downstream task, such as classification or regression. This is very useful as it doesn't require any manually labeled data. In this work, we introduce Probabilistic Hyper-Graphs using Masked Autoencoders (PHG-MAE): a novel model that unifies the classical work on neural graphs \cite{leordeanu2021semi} with the modern approach of masked autoencoders under a common theoretical framework. Through random masking of entire modalities, not just patches, the model samples from the distribution of hyper-edges on each forward pass. Additionally, the model adapts the standard MAE algorithm by combining pre-training and fine-tuning into a single training loop. Moreover, our approach enables the creation of inference-time ensembles which, through aggregation, boost the final prediction performance and consistency. Lastly, we show that we can apply knowledge distillation on top of the ensembles with little loss in performance, even with models that have fewer than 1M parameters. While our work mostly focuses on outdoor UAV scenes that contain multiple world interpretations and modalities, the same steps can be followed in other similar domains, such as autonomous driving or indoor robotics. In order to streamline the process of integrating external pre-trained experts for computer vision multi-modal multi-task learning (MTL) scenarios, we developed a data-pipeline software. Using this tool, we have created and released a fully-automated extension of the Dronescapes dataset. All the technical details, code and reproduction steps are publicly released.

