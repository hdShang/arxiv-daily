---
layout: default
title: "Spotlight: Identifying and Localizing Video Generation Errors Using VLMs"
---

# Spotlight: Identifying and Localizing Video Generation Errors Using VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.18102" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.18102v1</a>
  <a href="https://arxiv.org/pdf/2511.18102.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18102v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.18102v1', 'Spotlight: Identifying and Localizing Video Generation Errors Using VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Aditya Chinchure, Sahithya Ravi, Pushkar Shukla, Vered Shwartz, Leonid Sigal

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Spotlightï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¯†åˆ«å’Œå®šä½è§†é¢‘ç”Ÿæˆé”™è¯¯**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `é”™è¯¯å®šä½` `è§†è§‰è¯­è¨€æ¨¡å‹` `è§†é¢‘è¯„ä¼°` `ç»†ç²’åº¦æ ‡æ³¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹è¯„ä¼°æ–¹æ³•ç¼ºä¹å¯¹è§†é¢‘ç”Ÿæˆé”™è¯¯è¿›è¡Œç²¾ç¡®å®šä½å’Œè§£é‡Šçš„èƒ½åŠ›ï¼Œæ— æ³•æä¾›ç»†ç²’åº¦çš„åé¦ˆã€‚
2. è®ºæ–‡æå‡ºSpotlightä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡æ ‡æ³¨ç»†ç²’åº¦çš„è§†é¢‘ç”Ÿæˆé”™è¯¯ï¼Œæ¥å®šä½å’Œè§£é‡Šè¿™äº›é”™è¯¯ï¼Œä»è€Œå¼¥è¡¥ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨Spotlightä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡æå‡ºçš„æ¨ç†æ—¶ç­–ç•¥ï¼Œæ€§èƒ½å¯ä»¥æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€æ—¶é—´è¿è´¯ä¸”è§†è§‰ä¸Šé€¼çœŸçš„è§†é¢‘ã€‚ç„¶è€Œï¼Œä¸å‰ä¸€ä»£T2Væ¨¡å‹ç›¸æ¯”ï¼Œé”™è¯¯ä»ç„¶ç»å¸¸å‘ç”Ÿï¼Œå¹¶ä¸”æ›´åŠ ç»†å¾®å’Œå±€éƒ¨ã€‚è™½ç„¶å½“å‰çš„è¯„ä¼°èŒƒå¼åœ¨ä¸åŒç»´åº¦ä¸Šè¯„ä¼°è§†é¢‘æ¨¡å‹ï¼Œä½†å®ƒä»¬é€šå¸¸ä»¥æ•´ä½“æ–¹å¼è¯„ä¼°è§†é¢‘ï¼Œè€Œä¸è¯†åˆ«ç‰¹å®šé”™è¯¯ä½•æ—¶å‘ç”Ÿæˆ–æè¿°å…¶æ€§è´¨ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥Spotlightæ¥è§£å†³è¿™ä¸€å·®è·ï¼ŒSpotlightæ˜¯ä¸€é¡¹æ—¨åœ¨å®šä½å’Œè§£é‡Šè§†é¢‘ç”Ÿæˆé”™è¯¯çš„æ–°ä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨200ä¸ªä¸åŒçš„æ–‡æœ¬æç¤ºå’Œä¸‰ä¸ªæœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆå™¨ï¼ˆVeo 3ã€Seedanceå’ŒLTX-2ï¼‰ç”Ÿæˆäº†600ä¸ªè§†é¢‘ï¼Œå¹¶æ ‡æ³¨äº†è¶…è¿‡1600ä¸ªç»†ç²’åº¦é”™è¯¯ï¼Œæ¶µç›–å…­ç§ç±»å‹ï¼ŒåŒ…æ‹¬è¿åŠ¨ã€ç‰©ç†å’Œæç¤ºä¸€è‡´æ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä¸€è‡´æ€§å’Œç‰©ç†é”™è¯¯æ˜¯ä¸»è¦çš„ï¼Œå¹¶ä¸”æŒç»­å­˜åœ¨äºè¾ƒé•¿çš„ç‰‡æ®µä¸­ï¼Œè€Œå¤–è§‚æ¶ˆå¤±å’Œèº«ä½“å§¿åŠ¿é”™è¯¯åˆ™å‡ºç°åœ¨è¾ƒçŸ­çš„ç‰‡æ®µä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨Spotlightä¸Šè¯„ä¼°å½“å‰çš„VLMï¼Œå‘ç°VLMåœ¨è§†é¢‘ä¸­çš„é”™è¯¯è¯†åˆ«å’Œå®šä½æ–¹é¢æ˜æ˜¾è½åäºäººç±»ã€‚æˆ‘ä»¬æå‡ºäº†æ¨ç†æ—¶ç­–ç•¥æ¥æ¢æµ‹å½“å‰VLMåœ¨æˆ‘ä»¬ä»»åŠ¡ä¸Šçš„æé™ï¼Œå°†æ€§èƒ½æé«˜äº†è¿‘2å€ã€‚æˆ‘ä»¬çš„ä»»åŠ¡ä¸ºæ„å»ºç»†ç²’åº¦çš„è¯„ä¼°å·¥å…·å’Œæ›´å¤æ‚çš„è§†é¢‘ç”Ÿæˆå™¨å¥–åŠ±æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹è¯„ä¼°ä¸­ç¼ºä¹ç»†ç²’åº¦é”™è¯¯å®šä½å’Œè§£é‡Šçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸æ˜¯æ•´ä½“è¯„ä¼°ï¼Œæ— æ³•æŒ‡å‡ºå…·ä½“é”™è¯¯å‘ç”Ÿçš„æ—¶é—´å’Œæ€§è´¨ï¼Œè¿™é˜»ç¢äº†æ¨¡å‹çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŒ…å«è¯¦ç»†æ ‡æ³¨çš„è§†é¢‘æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸ä»…æ ‡æ³¨äº†è§†é¢‘ä¸­å­˜åœ¨çš„é”™è¯¯ï¼Œè¿˜å¯¹é”™è¯¯ç±»å‹è¿›è¡Œäº†ç»†ç²’åº¦åˆ’åˆ†ï¼Œå¹¶æä¾›äº†é”™è¯¯å‘ç”Ÿçš„æ—¶é—´ä½ç½®ã€‚é€šè¿‡åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå¯ä»¥è¡¡é‡VLMåœ¨é”™è¯¯è¯†åˆ«å’Œå®šä½æ–¹é¢çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpotlightä»»åŠ¡çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºå’Œå…ˆè¿›çš„è§†é¢‘ç”Ÿæˆå™¨ç”Ÿæˆè§†é¢‘ï¼›2) å¯¹ç”Ÿæˆçš„è§†é¢‘è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œæ ‡æ³¨å…­ç§ç±»å‹çš„ç»†ç²’åº¦é”™è¯¯ï¼ˆè¿åŠ¨ã€ç‰©ç†ã€æç¤ºä¸€è‡´æ€§ã€å¤–è§‚æ¶ˆå¤±ã€èº«ä½“å§¿åŠ¿ç­‰ï¼‰ï¼›3) ä½¿ç”¨æ ‡æ³¨å¥½çš„æ•°æ®é›†è¯„ä¼°ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼›4) æå‡ºæ¨ç†æ—¶ç­–ç•¥æ¥æå‡VLMåœ¨é”™è¯¯è¯†åˆ«å’Œå®šä½æ–¹é¢çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†Spotlightä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹é”™è¯¯è¯†åˆ«å’Œå®šä½èƒ½åŠ›çš„æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚ä¸ä»¥å¾€çš„æ•´ä½“è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼ŒSpotlightèƒ½å¤Ÿæä¾›æ›´ç»†ç²’åº¦çš„åé¦ˆï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©äº†å…·æœ‰ä»£è¡¨æ€§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆVeo 3ã€Seedanceå’ŒLTX-2ï¼‰æ¥ç”Ÿæˆè§†é¢‘ï¼›2) å®šä¹‰äº†å…­ç§å¸¸è§çš„è§†é¢‘ç”Ÿæˆé”™è¯¯ç±»å‹ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„æ ‡æ³¨æŒ‡å—ï¼›3) æå‡ºäº†æ¨ç†æ—¶ç­–ç•¥ï¼Œä¾‹å¦‚é€šè¿‡å¤šæ¬¡æŸ¥è¯¢å’Œé›†æˆç»“æœæ¥æé«˜VLMçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†çš„è§„æ¨¡ï¼ˆ600ä¸ªè§†é¢‘ï¼Œ1600+é”™è¯¯æ ‡æ³¨ï¼‰ä¹Ÿä¿è¯äº†è¯„ä¼°çš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨Spotlightä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸äººç±»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œè¡¨æ˜VLMåœ¨ç»†ç²’åº¦è§†é¢‘é”™è¯¯è¯†åˆ«å’Œå®šä½æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚é€šè¿‡æå‡ºçš„æ¨ç†æ—¶ç­–ç•¥ï¼ŒVLMçš„æ€§èƒ½å¯ä»¥æé«˜è¿‘2å€ï¼Œä½†ä»ç„¶è¿œä½äºäººç±»æ°´å¹³ï¼Œè¿™çªæ˜¾äº†è¯¥é¢†åŸŸçš„ç ”ç©¶æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°å’Œæ”¹è¿›ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥åˆ©ç”¨Spotlightä»»åŠ¡æ¥è®­ç»ƒæ›´æœ‰æ•ˆçš„å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œå¼•å¯¼è§†é¢‘ç”Ÿæˆå™¨ç”Ÿæˆæ›´ç¬¦åˆè¦æ±‚çš„è§†é¢‘ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„ç ”ç©¶ï¼Œå¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯åœ¨è§†é¢‘ç¼–è¾‘ã€å†…å®¹å®¡æ ¸ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.

