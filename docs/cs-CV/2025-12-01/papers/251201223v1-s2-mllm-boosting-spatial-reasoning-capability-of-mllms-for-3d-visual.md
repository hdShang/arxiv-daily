---
layout: default
title: S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance
---

# S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.01223" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.01223v1</a>
  <a href="https://arxiv.org/pdf/2512.01223.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.01223v1" onclick="toggleFavorite(this, '2512.01223v1', 'S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Beining Xu, Siting Zhu, Zhao Jin, Junxian Li, Hesheng Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-01

**å¤‡æ³¨**: 18 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**S$^2$-MLLMï¼šé€šè¿‡ç»“æ„å¼•å¯¼å¢å¼ºMLLMåœ¨3Dè§†è§‰å®šä½ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Dè§†è§‰å®šä½` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `ç»“æ„å¼•å¯¼` `å…·èº«æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dè§†è§‰å®šä½æ–¹æ³•ä¾èµ–äºç‚¹äº‘é‡å»ºå’Œè§†è§’æ¸²æŸ“ï¼Œæ•ˆç‡ä½ä¸”ç©ºé—´æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨MLLMçš„æ½œåŠ›ã€‚
2. S$^2$-MLLMé€šè¿‡å¼•å…¥ç©ºé—´å¼•å¯¼ç­–ç•¥ï¼Œåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿MLLMèƒ½å¤Ÿéšå¼åœ°è¿›è¡Œ3Dç©ºé—´æ¨ç†ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒS$^2$-MLLMåœ¨ScanReferã€Nr3Då’ŒSr3Dæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€æ³›åŒ–æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

3Dè§†è§‰å®šä½(3DVG)æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°åœ¨3Dåœºæ™¯ä¸­å®šä½ç‰©ä½“ï¼Œæ˜¯å…·èº«æ™ºèƒ½å’Œæœºå™¨äººæŠ€æœ¯çš„åŸºç¡€ä»»åŠ¡ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†å°†å…¶æ‰©å±•åˆ°3DVGçš„ç ”ç©¶ã€‚ç„¶è€Œï¼ŒMLLMä¸»è¦å¤„ç†2Dè§†è§‰è¾“å…¥ï¼Œéš¾ä»¥ä»…ä»è¿™äº›æœ‰é™çš„è§†è§’ç†è§£åœºæ™¯çš„3Dç©ºé—´ç»“æ„ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åˆ©ç”¨é‡å»ºç‚¹äº‘çš„è§†è§’ç›¸å…³æ¸²æŸ“ï¼Œä¸ºMLLMåœ¨3DVGä»»åŠ¡ä¸­æä¾›æ˜¾å¼ç»“æ„å¼•å¯¼ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œç©ºé—´æ¨ç†å—é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†S$^2$-MLLMï¼Œä¸€ä¸ªé€šè¿‡éšå¼ç©ºé—´æ¨ç†å¢å¼ºMLLMç©ºé—´æ¨ç†èƒ½åŠ›çš„é«˜æ•ˆæ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç©ºé—´å¼•å¯¼ç­–ç•¥ï¼Œåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡åœ¨è®­ç»ƒæœŸé—´è·å¾—3Dç»“æ„ç†è§£ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥éšå¼åœ°æ¨ç†3Dåœºæ™¯ï¼Œè€Œæ— éœ€ä¾èµ–ä½æ•ˆçš„ç‚¹äº‘é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»“æ„å¢å¼ºæ¨¡å—(SE)ï¼Œè¯¥æ¨¡å—é¦–å…ˆé‡‡ç”¨è§†å›¾å†…å’Œè§†å›¾é—´æ³¨æ„åŠ›æœºåˆ¶æ¥æ•è·è§†å›¾å†…çš„ä¾èµ–å…³ç³»å’Œè§†å›¾é—´çš„å¯¹åº”å…³ç³»ã€‚è¯¥æ¨¡å—è¿›ä¸€æ­¥é›†æˆäº†å¤šçº§ä½ç½®ç¼–ç ï¼Œå°†è§†è§‰è¡¨ç¤ºä¸ç©ºé—´ä½ç½®å’Œè§†ç‚¹ä¿¡æ¯ç›¸å…³è”ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„ç»“æ„ç†è§£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒS$^2$-MLLMç»Ÿä¸€äº†å“è¶Šçš„æ€§èƒ½ã€æ³›åŒ–æ€§å’Œæ•ˆç‡ï¼Œåœ¨ScanReferã€Nr3Då’ŒSr3Dæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ˜¾è‘—æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼š3Dè§†è§‰å®šä½ä»»åŠ¡æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°åœ¨3Dåœºæ™¯ä¸­å®šä½ç›®æ ‡ç‰©ä½“ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä»å¤šä¸ªè§†è§’æ¸²æŸ“çš„3Dç‚¹äº‘ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)æä¾›æ˜¾å¼çš„ç»“æ„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”å—é™äºç‚¹äº‘é‡å»ºçš„è´¨é‡ï¼Œå¯¼è‡´ç©ºé—´æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šS$^2$-MLLMçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡éšå¼çš„æ–¹å¼å¼•å¯¼MLLMå­¦ä¹ 3Dåœºæ™¯çš„ç»“æ„ä¿¡æ¯ï¼Œé¿å…æ˜¾å¼çš„ç‚¹äº‘é‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åˆ°3Dåœºæ™¯çš„ç»“æ„ä¿¡æ¯ï¼Œä»è€Œåœ¨æ¨ç†é˜¶æ®µèƒ½å¤Ÿéšå¼åœ°è¿›è¡Œç©ºé—´æ¨ç†ã€‚è¿™æ ·å¯ä»¥æé«˜æ•ˆç‡ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šS$^2$-MLLMæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†è§‰ç‰¹å¾æå–æ¨¡å—ï¼šç”¨äºä»å¤šè§†è§’å›¾åƒä¸­æå–è§†è§‰ç‰¹å¾ã€‚2) ç©ºé—´å¼•å¯¼æ¨¡å—ï¼šåˆ©ç”¨å‰é¦ˆ3Dé‡å»ºçš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºMLLMæä¾›ç©ºé—´å¼•å¯¼ã€‚3) ç»“æ„å¢å¼ºæ¨¡å—(SE)ï¼šé€šè¿‡è§†å›¾å†…å’Œè§†å›¾é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•è·è§†å›¾å†…çš„ä¾èµ–å…³ç³»å’Œè§†å›¾é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶ç»“åˆå¤šçº§ä½ç½®ç¼–ç ï¼Œå°†è§†è§‰è¡¨ç¤ºä¸ç©ºé—´ä½ç½®å’Œè§†ç‚¹ä¿¡æ¯ç›¸å…³è”ã€‚4) å¤šæ¨¡æ€èåˆæ¨¡å—ï¼šå°†è§†è§‰ç‰¹å¾ã€ç©ºé—´å¼•å¯¼å’Œè¯­è¨€ç‰¹å¾è¿›è¡Œèåˆï¼Œç”¨äºæœ€ç»ˆçš„3Dç›®æ ‡å®šä½ã€‚

**å…³é”®åˆ›æ–°**ï¼šS$^2$-MLLMçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†éšå¼çš„ç©ºé—´æ¨ç†æ–¹æ³•ï¼Œé¿å…äº†æ˜¾å¼çš„ç‚¹äº‘é‡å»ºï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç»“æ„å¢å¼ºæ¨¡å—(SE)é€šè¿‡è§†å›¾å†…å’Œè§†å›¾é—´æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠå¤šçº§ä½ç½®ç¼–ç ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨äº†å¤šè§†è§’ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šç»“æ„å¢å¼ºæ¨¡å—(SE)æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚å®ƒåŒ…å«intra-view attentionå’Œinter-view attentionä¸¤ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«ç”¨äºæ•è·è§†å›¾å†…çš„ä¾èµ–å…³ç³»å’Œè§†å›¾é—´çš„å¯¹åº”å…³ç³»ã€‚å¤šçº§ä½ç½®ç¼–ç å°†è§†è§‰ç‰¹å¾ä¸ç©ºé—´ä½ç½®å’Œè§†ç‚¹ä¿¡æ¯å…³è”èµ·æ¥ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œå¯èƒ½é‡‡ç”¨äº†äº¤å‰ç†µæŸå¤±æˆ–ç±»ä¼¼çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹è¿›è¡Œ3Dç›®æ ‡å®šä½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

S$^2$-MLLMåœ¨ScanReferã€Nr3Då’ŒSr3Dæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ScanReferæ•°æ®é›†ä¸Šï¼ŒS$^2$-MLLMçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS$^2$-MLLMå…·æœ‰å“è¶Šçš„æ€§èƒ½ã€æ³›åŒ–æ€§å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

S$^2$-MLLMåœ¨å…·èº«æ™ºèƒ½ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒä¸­ç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶å‡†ç¡®åœ°å®šä½ç›®æ ‡ç‰©ä½“ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.

