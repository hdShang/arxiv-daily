---
layout: default
title: When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition
---

# When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.06426" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.06426v1</a>
  <a href="https://arxiv.org/pdf/2512.06426.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.06426v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.06426v1', 'When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nzakiese Mbongo, Kailash A. Hambarde, Hugo ProenÃ§a

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-06

**å¤‡æ³¨**: 12 pages, 9 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒè·¯å¾„Transformeræ¡†æ¶ï¼Œåˆ©ç”¨CLIPè§£å†³è¿œè·ç¦»å›¾åƒæ€§åˆ«è¯†åˆ«éš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è¿œè·ç¦»è¯†åˆ«` `æ€§åˆ«è¯†åˆ«` `CLIPæ¨¡å‹` `åŒè·¯å¾„Transformer` `å±æ€§è¯†åˆ«` `è¡Œäººé‡è¯†åˆ«` `å¤šæ¨¡æ€å­¦ä¹ ` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. è¿œè·ç¦»å›¾åƒæ€§åˆ«è¯†åˆ«é¢ä¸´ç©ºé—´åˆ†è¾¨ç‡ä½ã€è§†è§’å˜åŒ–å¤§å’Œé¢éƒ¨çº¿ç´¢ä¸¢å¤±ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹ã€‚
2. æå‡ºåŒè·¯å¾„Transformeræ¡†æ¶ï¼Œç»“åˆè§†è§‰ä¿¡æ¯å’Œå±æ€§çº¿ç´¢ï¼Œåˆ©ç”¨CLIPæ¨¡å‹è¿›è¡Œè”åˆå»ºæ¨¡ï¼Œæå‡è¯†åˆ«å‡†ç¡®ç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªå»ºæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰è¡Œäººå±æ€§è¯†åˆ«å’Œé‡è¯†åˆ«æ–¹æ³•ï¼Œä¸”å¯¹è·ç¦»ã€è§’åº¦å’Œé«˜åº¦å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒè·¯å¾„Transformeræ¡†æ¶ï¼Œåˆ©ç”¨CLIPæ¨¡å‹ï¼Œè”åˆå»ºæ¨¡è§†è§‰å’Œå±æ€§é©±åŠ¨çš„çº¿ç´¢ï¼Œç”¨äºè¿œè·ç¦»å›¾åƒçš„æ€§åˆ«è¯†åˆ«ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªäº’è¡¥çš„è·¯å¾„ï¼šä¸€æ˜¯ç›´æ¥è§†è§‰è·¯å¾„ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°å¾®è°ƒé¢„è®­ç»ƒçš„CLIPå›¾åƒç¼–ç å™¨çš„ä¸Šå±‚ï¼Œæ¥ä¼˜åŒ–è§†è§‰ç‰¹å¾ï¼›äºŒæ˜¯å±æ€§ä»‹å¯¼è·¯å¾„ï¼Œä»ä¸€ç»„è½¯ç”Ÿç‰©ç‰¹å¾æç¤ºï¼ˆå¦‚å‘å‹ã€æœè£…ã€é…é¥°ï¼‰ä¸­æ¨æ–­æ€§åˆ«ï¼Œè¿™äº›æç¤ºåœ¨CLIPæ–‡æœ¬-å›¾åƒç©ºé—´ä¸­å¯¹é½ã€‚ç©ºé—´é€šé“æ³¨æ„åŠ›æ¨¡å—è¿›ä¸€æ­¥å¢å¼ºäº†é®æŒ¡å’Œä½åˆ†è¾¨ç‡ä¸‹çš„åˆ¤åˆ«å®šä½èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒå¤§è§„æ¨¡è¯„ä¼°ï¼Œæ„å»ºäº†U-DetAGReIDæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç»Ÿä¸€äº†DetReIDxå’ŒAG-ReID.v2ï¼Œå¹¶é‡‡ç”¨ä¸€è‡´çš„ä¸‰å…ƒæ ‡ç­¾æ–¹æ¡ˆï¼ˆç”·ã€å¥³ã€æœªçŸ¥ï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è§£å†³æ–¹æ¡ˆåœ¨å¤šä¸ªæŒ‡æ ‡ï¼ˆå®F1ã€å‡†ç¡®ç‡ã€AUCï¼‰ä¸Šä¼˜äºæœ€å…ˆè¿›çš„è¡Œäººå±æ€§å’Œé‡è¯†åˆ«åŸºçº¿ï¼Œå¹¶ä¸”å¯¹è·ç¦»ã€è§’åº¦å’Œé«˜åº¦å˜åŒ–å…·æœ‰ä¸€è‡´çš„é²æ£’æ€§ã€‚å®šæ€§çš„æ³¨æ„åŠ›å¯è§†åŒ–è¯å®äº†è§£é‡Šæ€§çš„å±æ€§å®šä½å’Œè´Ÿè´£ä»»çš„æ‹’ç»è¡Œä¸ºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯­è¨€å¼•å¯¼çš„åŒè·¯å¾„å­¦ä¹ ä¸ºåœ¨æ— çº¦æŸçš„è¿œè·ç¦»åœºæ™¯ä¸­è¿›è¡Œè´Ÿè´£ä»»çš„æ€§åˆ«è¯†åˆ«æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„ã€å¯æ‰©å±•çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æç«¯è¿œè·ç¦»å›¾åƒä¸­çš„æ€§åˆ«è¯†åˆ«é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä½åˆ†è¾¨ç‡ã€é®æŒ¡å’Œè§†è§’å˜åŒ–ç­‰æƒ…å†µæ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆæå–æ€§åˆ«ç›¸å…³çš„åˆ¤åˆ«ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¤§è§„æ¨¡çš„è¿œè·ç¦»æ€§åˆ«è¯†åˆ«æ•°æ®é›†ä¹Ÿé™åˆ¶äº†ç®—æ³•çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨CLIPæ¨¡å‹å¼ºå¤§çš„å¤šæ¨¡æ€è¡¨ç¤ºèƒ½åŠ›ï¼Œç»“åˆè§†è§‰ä¿¡æ¯å’Œå±æ€§ä¿¡æ¯è¿›è¡Œæ€§åˆ«è¯†åˆ«ã€‚é€šè¿‡åŒè·¯å¾„Transformeræ¡†æ¶ï¼Œåˆ†åˆ«å¤„ç†å›¾åƒçš„è§†è§‰ç‰¹å¾å’Œå±æ€§ç‰¹å¾ï¼Œå¹¶è¿›è¡Œèåˆï¼Œä»è€Œæé«˜è¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¼¥è¡¥é¢éƒ¨ç‰¹å¾ç¼ºå¤±å¸¦æ¥çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦è·¯å¾„ï¼š(1) ç›´æ¥è§†è§‰è·¯å¾„ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„CLIPå›¾åƒç¼–ç å™¨æå–å›¾åƒçš„è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡é€‰æ‹©æ€§å¾®è°ƒä¸Šå±‚ç½‘ç»œæ¥é€‚åº”æ€§åˆ«è¯†åˆ«ä»»åŠ¡ã€‚(2) å±æ€§ä»‹å¯¼è·¯å¾„ï¼šåˆ©ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨å°†è½¯ç”Ÿç‰©ç‰¹å¾æç¤ºï¼ˆå¦‚å‘å‹ã€æœè£…ç­‰ï¼‰ç¼–ç ä¸ºæ–‡æœ¬ç‰¹å¾ï¼Œç„¶ååœ¨CLIPçš„æ–‡æœ¬-å›¾åƒç©ºé—´ä¸­ä¸è§†è§‰ç‰¹å¾å¯¹é½ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨äº†ç©ºé—´é€šé“æ³¨æ„åŠ›æ¨¡å—æ¥å¢å¼ºç‰¹å¾çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é®æŒ¡å’Œä½åˆ†è¾¨ç‡æƒ…å†µä¸‹ã€‚æœ€åï¼Œå°†ä¸¤ä¸ªè·¯å¾„çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œå¹¶é€šè¿‡åˆ†ç±»å™¨è¿›è¡Œæ€§åˆ«é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŒè·¯å¾„Transformeræ¡†æ¶ï¼Œå°†è§†è§‰ä¿¡æ¯å’Œå±æ€§ä¿¡æ¯è¿›è¡Œæœ‰æ•ˆèåˆã€‚åˆ©ç”¨CLIPæ¨¡å‹å¼ºå¤§çš„å¤šæ¨¡æ€è¡¨ç¤ºèƒ½åŠ›ï¼Œå°†æ–‡æœ¬ä¿¡æ¯ï¼ˆå±æ€§æè¿°ï¼‰èå…¥åˆ°å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­ï¼Œä»è€Œæé«˜äº†è¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè‡ªå»ºçš„U-DetAGReIDæ•°æ®é›†ä¸ºè¿œè·ç¦»æ€§åˆ«è¯†åˆ«ç ”ç©¶æä¾›äº†æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç›´æ¥è§†è§‰è·¯å¾„ä¸­ï¼Œé€‰æ‹©æ€§å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨çš„ä¸Šå±‚ç½‘ç»œï¼Œé¿å…äº†å¯¹åº•å±‚ç‰¹å¾çš„ç ´åï¼ŒåŒæ—¶èƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”æ€§åˆ«è¯†åˆ«ä»»åŠ¡ã€‚åœ¨å±æ€§ä»‹å¯¼è·¯å¾„ä¸­ï¼Œä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨å°†è½¯ç”Ÿç‰©ç‰¹å¾æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ä¸è§†è§‰ç‰¹å¾å¯¹é½ã€‚ç©ºé—´é€šé“æ³¨æ„åŠ›æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´ä¸åŒé€šé“å’Œç©ºé—´ä½ç½®çš„æƒé‡ï¼Œä»è€Œå¢å¼ºç‰¹å¾çš„åˆ¤åˆ«èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è®­ç»ƒåˆ†ç±»å™¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªå»ºçš„U-DetAGReIDæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨å®F1ã€å‡†ç¡®ç‡å’ŒAUCç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„è¡Œäººå±æ€§è¯†åˆ«å’Œé‡è¯†åˆ«æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿œè·ç¦»åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®ç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†5%-10%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯¹è·ç¦»ã€è§’åº¦å’Œé«˜åº¦å˜åŒ–å…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿé€‚åº”å¤æ‚çš„å®é™…åœºæ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®‰é˜²ã€å…¬å…±å®‰å…¨ã€æ™ºæ…§åŸå¸‚ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨ç›‘æ§è§†é¢‘ä¸­è¿›è¡Œè¿œè·ç¦»æ€§åˆ«è¯†åˆ«ï¼Œè¾…åŠ©è¿›è¡Œäººç¾¤åˆ†æã€å«Œç–‘äººè¿½è¸ªç­‰ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºäººæœºäº¤äº’ã€ä¸ªæ€§åŒ–æ¨èç­‰é¢†åŸŸï¼Œä¾‹å¦‚æ ¹æ®ç”¨æˆ·çš„æ€§åˆ«æä¾›å®šåˆ¶åŒ–çš„æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

