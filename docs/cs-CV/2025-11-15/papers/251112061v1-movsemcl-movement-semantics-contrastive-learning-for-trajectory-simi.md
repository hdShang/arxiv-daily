---
layout: default
title: "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity"
---

# MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.12061" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.12061v1</a>
  <a href="https://arxiv.org/pdf/2511.12061.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12061v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.12061v1', 'MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhichen Lai, Hua Lu, Huan Li, Jialiang Li, Christian S. Jensen

**åˆ†ç±»**: cs.CV, cs.AI, cs.DB

**å‘å¸ƒæ—¥æœŸ**: 2025-11-15

**å¤‡æ³¨**: 8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMovSemCLæ¡†æ¶ï¼Œé€šè¿‡è¿åŠ¨è¯­ä¹‰å¯¹æ¯”å­¦ä¹ æå‡è½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è½¨è¿¹ç›¸ä¼¼åº¦` `å¯¹æ¯”å­¦ä¹ ` `è¿åŠ¨è¯­ä¹‰` `æ³¨æ„åŠ›æœºåˆ¶` `æ•°æ®å¢å¼º` `è½¨è¿¹èšç±»` `å¼‚å¸¸æ£€æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•åœ¨è½¨è¿¹è¯­ä¹‰å»ºæ¨¡ã€è®¡ç®—æ•ˆç‡å’Œæ•°æ®å¢å¼ºç­–ç•¥ä¸Šå­˜åœ¨ä¸è¶³ã€‚
2. MovSemCLé€šè¿‡è¿åŠ¨è¯­ä¹‰ç‰¹å¾æå–ã€åˆ†å±‚æ³¨æ„åŠ›ç¼–ç å’Œæ›²ç‡å¼•å¯¼å¢å¼ºæ¥æå‡è½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMovSemCLåœ¨ç›¸ä¼¼åº¦æœç´¢å’Œå¯å‘å¼è¿‘ä¼¼ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—æ˜¯èšç±»ã€é¢„æµ‹å’Œå¼‚å¸¸æ£€æµ‹ç­‰ä»»åŠ¡çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦é™åˆ¶ï¼š(1) è½¨è¿¹è¯­ä¹‰å’Œå±‚æ¬¡ç»“æ„å»ºæ¨¡ä¸è¶³ï¼Œç¼ºä¹è¿åŠ¨åŠ¨æ€æå–å’Œå¤šå°ºåº¦ç»“æ„è¡¨ç¤ºï¼›(2) ç”±äºé€ç‚¹ç¼–ç å¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼›(3) ä½¿ç”¨ç‰©ç†ä¸Šä¸åˆç†çš„å¢å¼ºæ–¹å¼ï¼Œæ‰­æ›²äº†è½¨è¿¹è¯­ä¹‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MovSemCLï¼Œä¸€ä¸ªç”¨äºè½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—çš„è¿åŠ¨è¯­ä¹‰å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚MovSemCLé¦–å…ˆå°†åŸå§‹GPSè½¨è¿¹è½¬æ¢ä¸ºè¿åŠ¨è¯­ä¹‰ç‰¹å¾ï¼Œç„¶åå°†å…¶åˆ†å‰²æˆpatchesã€‚æ¥ä¸‹æ¥ï¼ŒMovSemCLé‡‡ç”¨patchå†…å’Œpatché—´çš„æ³¨æ„åŠ›æœºåˆ¶æ¥ç¼–ç å±€éƒ¨å’Œå…¨å±€è½¨è¿¹æ¨¡å¼ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„åˆ†å±‚è¡¨ç¤ºå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒMovSemCLåŒ…å«ä¸€ç§æ›²ç‡å¼•å¯¼çš„å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¿ç•™ä¿¡æ¯ä¸°å¯Œçš„ç‰‡æ®µï¼ˆä¾‹å¦‚ï¼Œè½¬å¼¯å’Œäº¤å‰å£ï¼‰å¹¶å±è”½å†—ä½™ç‰‡æ®µï¼Œä»è€Œç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„å¢å¼ºè§†å›¾ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMovSemCLèƒ½å¤Ÿä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ç›¸ä¼¼åº¦æœç´¢ä»»åŠ¡ä¸­å®ç°æ¥è¿‘ç†æƒ³å€¼1çš„å¹³å‡æ’åï¼Œå¹¶åœ¨å¯å‘å¼è¿‘ä¼¼æ–¹é¢æé«˜é«˜è¾¾20.3%ï¼ŒåŒæ—¶å°†æ¨ç†å»¶è¿Ÿé™ä½é«˜è¾¾43.4%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå­¦ä¹ çš„è½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼Œæ— æ³•å……åˆ†å»ºæ¨¡è½¨è¿¹çš„è¿åŠ¨è¯­ä¹‰å’Œå±‚æ¬¡ç»“æ„ï¼Œå¯¼è‡´ç›¸ä¼¼åº¦è®¡ç®—ç²¾åº¦ä¸é«˜ã€‚åŒæ—¶ï¼Œé€ç‚¹ç¼–ç æ–¹å¼è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”æ•°æ®å¢å¼ºæ–¹æ³•ä¸åˆç†ï¼Œä¼šæ‰­æ›²è½¨è¿¹çš„çœŸå®è¯­ä¹‰ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆæå–è½¨è¿¹è¯­ä¹‰ã€é™ä½è®¡ç®—æˆæœ¬å¹¶è¿›è¡Œåˆç†æ•°æ®å¢å¼ºçš„è½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMovSemCLçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¿åŠ¨è¯­ä¹‰ç‰¹å¾æå–ï¼Œå°†åŸå§‹GPSè½¨è¿¹è½¬æ¢ä¸ºæ›´å…·è¡¨è¾¾åŠ›çš„ç‰¹å¾è¡¨ç¤ºã€‚ç„¶åï¼Œåˆ©ç”¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹è½¨è¿¹çš„å±€éƒ¨å’Œå…¨å±€æ¨¡å¼è¿›è¡Œç¼–ç ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ›²ç‡å¼•å¯¼çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä¿ç•™è½¨è¿¹ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶å±è”½å†—ä½™ä¿¡æ¯ï¼Œç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„å¢å¼ºè§†å›¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMovSemCLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) è¿åŠ¨è¯­ä¹‰ç‰¹å¾æå–ï¼šå°†åŸå§‹GPSè½¨è¿¹è½¬æ¢ä¸ºè¿åŠ¨è¯­ä¹‰ç‰¹å¾ã€‚2) è½¨è¿¹åˆ†å—ï¼šå°†è½¨è¿¹åˆ†å‰²æˆpatchesã€‚3) åˆ†å±‚æ³¨æ„åŠ›ç¼–ç ï¼šåˆ©ç”¨patchå†…å’Œpatché—´çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹å±€éƒ¨å’Œå…¨å±€è½¨è¿¹æ¨¡å¼è¿›è¡Œç¼–ç ã€‚4) æ›²ç‡å¼•å¯¼æ•°æ®å¢å¼ºï¼šæ ¹æ®è½¨è¿¹çš„æ›²ç‡ï¼Œä¿ç•™ä¿¡æ¯ä¸°å¯Œçš„ç‰‡æ®µï¼Œå¹¶å±è”½å†—ä½™ç‰‡æ®µã€‚5) å¯¹æ¯”å­¦ä¹ ï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå­¦ä¹ è½¨è¿¹çš„ç›¸ä¼¼åº¦è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šMovSemCLçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†è¿åŠ¨è¯­ä¹‰ç‰¹å¾ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¡¨è¾¾è½¨è¿¹çš„è¿åŠ¨ä¿¡æ¯ã€‚2) é‡‡ç”¨äº†åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼–ç è½¨è¿¹çš„å±€éƒ¨å’Œå…¨å±€æ¨¡å¼ã€‚3) è®¾è®¡äº†æ›²ç‡å¼•å¯¼çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œèƒ½å¤Ÿç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„å¢å¼ºè§†å›¾ã€‚

**å…³é”®è®¾è®¡**ï¼šMovSemCLçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è¿åŠ¨è¯­ä¹‰ç‰¹å¾çš„å…·ä½“è®¡ç®—æ–¹æ³•ï¼Œä¾‹å¦‚é€Ÿåº¦ã€åŠ é€Ÿåº¦ã€æ–¹å‘ç­‰ã€‚2) patchçš„å¤§å°å’Œæ•°é‡ã€‚3) æ³¨æ„åŠ›æœºåˆ¶çš„å…·ä½“å®ç°æ–¹å¼ï¼Œä¾‹å¦‚Transformerã€‚4) æ›²ç‡çš„è®¡ç®—æ–¹æ³•å’Œé˜ˆå€¼è®¾å®šã€‚5) å¯¹æ¯”å­¦ä¹ çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚InfoNCEã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMovSemCLåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ç›¸ä¼¼åº¦æœç´¢ä»»åŠ¡ä¸­å®ç°äº†æ¥è¿‘ç†æƒ³å€¼1çš„å¹³å‡æ’åï¼Œå¹¶åœ¨å¯å‘å¼è¿‘ä¼¼æ–¹é¢æé«˜äº†é«˜è¾¾20.3%ï¼ŒåŒæ—¶å°†æ¨ç†å»¶è¿Ÿé™ä½äº†é«˜è¾¾43.4%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMovSemCLèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜è½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MovSemCLå¯åº”ç”¨äºè½¨è¿¹èšç±»ã€è½¨è¿¹é¢„æµ‹ã€å¼‚å¸¸è½¨è¿¹æ£€æµ‹ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨äº¤é€šç®¡ç†ä¸­ï¼Œå¯ä»¥åˆ©ç”¨MovSemCLå¯¹è½¦è¾†è½¨è¿¹è¿›è¡Œèšç±»ï¼Œä»è€Œåˆ†æäº¤é€šæ¨¡å¼ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥åˆ©ç”¨MovSemCLé¢„æµ‹è½¦è¾†çš„æœªæ¥è½¨è¿¹ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ï¼›åœ¨é‡‘èé£æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨MovSemCLæ£€æµ‹å¼‚å¸¸äº¤æ˜“è½¨è¿¹ï¼Œä»è€Œé˜²æ­¢æ¬ºè¯ˆè¡Œä¸ºã€‚è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

