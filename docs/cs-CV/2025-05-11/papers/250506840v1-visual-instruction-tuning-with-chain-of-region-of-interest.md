---
layout: default
title: Visual Instruction Tuning with Chain of Region-of-Interest
---

# Visual Instruction Tuning with Chain of Region-of-Interest

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06840" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06840v1</a>
  <a href="https://arxiv.org/pdf/2505.06840.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06840v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06840v1', 'Visual Instruction Tuning with Chain of Region-of-Interest')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yixin Chen, Shuai Zhang, Boran Han, Bernie Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-11

**å¤‡æ³¨**: N/A

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºChain of Region-of-Interestä»¥è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒè®¡ç®—è´Ÿæ‹…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `é«˜åˆ†è¾¨ç‡å›¾åƒ` `åŒºåŸŸé€‰æ‹©` `è®¡ç®—æ•ˆç‡` `è§†è§‰ç†è§£` `ä¿¡æ¯ä¼˜å…ˆå¤„ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ï¼Œè®¡ç®—éœ€æ±‚æ˜¾è‘—å¢åŠ ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚
2. è®ºæ–‡æå‡ºçš„CoRoIæ–¹æ³•é€šè¿‡è¯†åˆ«å’Œä¼˜å…ˆå¤„ç†é‡è¦åŒºåŸŸï¼Œå‡å°‘äº†é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—è´Ÿæ‹…ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒCoRoIåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é«˜åˆ†è¾¨ç‡å›¾åƒå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯†åˆ«å’Œç†è§£èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›´æ¥æé«˜å›¾åƒåˆ†è¾¨ç‡ä¼šæ˜¾è‘—å¢åŠ è®¡ç®—éœ€æ±‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºChain of Region-of-Interestï¼ˆCoRoIï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡è½»ä¸é«˜åˆ†è¾¨ç‡å›¾åƒç›¸å…³çš„è®¡ç®—è´Ÿæ‹…ã€‚CoRoIé€šè¿‡è¯†åˆ«å’Œä¼˜å…ˆå¤„ç†æœ€å…·ä¿¡æ¯é‡çš„åŒºåŸŸï¼Œå¢å¼ºäº†å¤šæ¨¡æ€è§†è§‰ç†è§£å’Œè¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶é¿å…äº†å¤„ç†å†—é•¿çš„é«˜åˆ†è¾¨ç‡å›¾åƒä»¤ç‰Œã€‚é€šè¿‡åœ¨11ä¸ªåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†CoRoIåœ¨ä¸åŒå‚æ•°è§„æ¨¡ï¼ˆ7Bè‡³34Bï¼‰ä¸‹çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å‡ ä¹æ‰€æœ‰åŸºå‡†ä¸Šè¶…è¶Šäº†LLaVA-NeXTï¼Œå¹¶ä¸”æˆ‘ä»¬å¾®è°ƒçš„34Bæ¨¡å‹åœ¨å…­ä¸ªåŸºå‡†ä¸Šè¶…è¿‡äº†Gemini Pro 1.0ï¼Œæ­¤å¤–åœ¨MMBã€SEED-Iå’ŒMMEä¸Šä¹Ÿè¶…è¶Šäº†GPT-4Vã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ä¸­çš„è®¡ç®—è´Ÿæ‹…é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç›´æ¥æé«˜å›¾åƒåˆ†è¾¨ç‡æ—¶ï¼Œè®¡ç®—éœ€æ±‚å¤§å¹…å¢åŠ ï¼Œå½±å“äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCoRoIæ–¹æ³•çš„æ ¸å¿ƒåœ¨äºæ¨¡ä»¿äººç±»è§†è§‰ç³»ç»Ÿçš„é€‰æ‹©æ€§ï¼Œé€šè¿‡è¯†åˆ«å’Œä¼˜å…ˆå¤„ç†å›¾åƒä¸­æœ€å…·ä¿¡æ¯é‡çš„åŒºåŸŸï¼Œæ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯åœ¨ä¿è¯è§†è§‰ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œå‡å°‘å†—ä½™è®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoRoIçš„æ•´ä½“æ¶æ„åŒ…æ‹¬åŒºåŸŸé€‰æ‹©æ¨¡å—å’Œä¿¡æ¯ä¼˜å…ˆçº§è¯„ä¼°æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ†æå›¾åƒå†…å®¹ï¼Œè¯†åˆ«å‡ºå…³é”®åŒºåŸŸï¼›ç„¶åï¼ŒåŸºäºè¿™äº›åŒºåŸŸè¿›è¡Œåç»­çš„å¤šæ¨¡æ€ç†è§£å’Œè¯†åˆ«ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCoRoIçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶åŒºåŸŸä¼˜å…ˆå¤„ç†æœºåˆ¶ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹å†—é•¿é«˜åˆ†è¾¨ç‡å›¾åƒä»¤ç‰Œçš„å¤„ç†éœ€æ±‚ï¼Œä»è€Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼ŒCoRoIåœ¨7Båˆ°34Bçš„æ¨¡å‹è§„æ¨¡ä¸‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œå¼ºè°ƒäº†å¯¹é‡è¦åŒºåŸŸçš„å…³æ³¨ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°å…³é”®ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoRoIåœ¨11ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡ ä¹åœ¨æ‰€æœ‰åŸºå‡†ä¸Šè¶…è¶Šäº†LLaVA-NeXTã€‚ç‰¹åˆ«æ˜¯å¾®è°ƒåçš„34Bæ¨¡å‹åœ¨å…­ä¸ªåŸºå‡†ä¸Šè¶…è¿‡äº†Gemini Pro 1.0ï¼Œå¹¶åœ¨MMBã€SEED-Iå’ŒMMEä¸Šè¶…è¶Šäº†GPT-4Vï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ä»¥åŠäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ä¸­çš„æ•ˆç‡ï¼ŒCoRoIèƒ½å¤Ÿåœ¨æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒåˆ†æç­‰å®é™…åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> High-resolution (HR) images are pivotal for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs). However, directly increasing image resolution can significantly escalate computational demands. In this study, we propose a method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating the computational burden associated with high-resolution images for MLLMs. Drawing inspiration from the selective nature of the human visual system, we recognize that not all regions within high-resolution images carry equal importance. CoRoI seeks to identify and prioritize the most informative regions, thereby enhancing multimodal visual comprehension and recognition while circumventing the need for processing lengthy HR image tokens. Through extensive experiments on 11 benchmarks, we validate the efficacy of CoRoI across varying sizes, ranging from 7B to 34B in parameters. Our models consistently demonstrate superior performance across diverse multimodal benchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all benchmarks and our finetuned 34B model surpasses proprietary methods like Gemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB, SEED-I, and MME.

