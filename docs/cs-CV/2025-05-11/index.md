---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-05-11
---

# cs.CVï¼ˆ2025-05-11ï¼‰

ğŸ“Š å…± **16** ç¯‡è®ºæ–‡
 | ğŸ”— **4** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250507001v2-hallucination-aware-multimodal-benchmark-for-gastrointestinal-image-.html">Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models</a></td>
  <td>æå‡ºå¤šæ¨¡æ€åŸºå‡†ä»¥è§£å†³åŒ»ç–—å›¾åƒåˆ†æä¸­çš„å¹»è§‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07001v2" data-paper-url="./papers/250507001v2-hallucination-aware-multimodal-benchmark-for-gastrointestinal-image-.html" onclick="toggleFavorite(this, '2505.07001v2', 'Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250507062v1-seed15-vl-technical-report.html">Seed1.5-VL Technical Report</a></td>
  <td>æå‡ºSeed1.5-VLä»¥è§£å†³å¤šæ¨¡æ€ç†è§£ä¸æ¨ç†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07062v1" data-paper-url="./papers/250507062v1-seed15-vl-technical-report.html" onclick="toggleFavorite(this, '2505.07062v1', 'Seed1.5-VL Technical Report')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250506912v1-building-a-human-verified-clinical-reasoning-dataset-via-a-human-llm.html">Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI</a></td>
  <td>æå‡ºäººç±»éªŒè¯çš„ä¸´åºŠæ¨ç†æ•°æ®é›†ä»¥è§£å†³åŒ»ç–—AIä¿¡ä»»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06912v1" data-paper-url="./papers/250506912v1-building-a-human-verified-clinical-reasoning-dataset-via-a-human-llm.html" onclick="toggleFavorite(this, '2505.06912v1', 'Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250506840v1-visual-instruction-tuning-with-chain-of-region-of-interest.html">Visual Instruction Tuning with Chain of Region-of-Interest</a></td>
  <td>æå‡ºChain of Region-of-Interestä»¥è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒè®¡ç®—è´Ÿæ‹…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06840v1" data-paper-url="./papers/250506840v1-visual-instruction-tuning-with-chain-of-region-of-interest.html" onclick="toggleFavorite(this, '2505.06840v1', 'Visual Instruction Tuning with Chain of Region-of-Interest')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250507110v1-deepsort-driven-visual-tracking-approach-for-gesture-recognition-in-.html">DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems</a></td>
  <td>åŸºäºDeepSORTçš„è§†è§‰è·Ÿè¸ªæ–¹æ³•è§£å†³äº¤äº’ç³»ç»Ÿä¸­çš„æ‰‹åŠ¿è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07110v1" data-paper-url="./papers/250507110v1-deepsort-driven-visual-tracking-approach-for-gesture-recognition-in-.html" onclick="toggleFavorite(this, '2505.07110v1', 'DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250507057v1-dape-dual-stage-parameter-efficient-fine-tuning-for-consistent-video.html">DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models</a></td>
  <td>æå‡ºDAPEä»¥è§£å†³è§†é¢‘ç¼–è¾‘ä¸­çš„ä¸€è‡´æ€§ä¸æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07057v1" data-paper-url="./papers/250507057v1-dape-dual-stage-parameter-efficient-fine-tuning-for-consistent-video.html" onclick="toggleFavorite(this, '2505.07057v1', 'DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250507013v1-efficient-and-robust-multidimensional-attention-in-remote-physiologi.html">Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization</a></td>
  <td>æå‡ºç›®æ ‡ä¿¡å·çº¦æŸå› å­åˆ†è§£ä»¥è§£å†³è¿œç¨‹ç”Ÿç†ä¿¡å·ç›‘æµ‹çš„é²æ£’æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07013v1" data-paper-url="./papers/250507013v1-efficient-and-robust-multidimensional-attention-in-remote-physiologi.html" onclick="toggleFavorite(this, '2505.07013v1', 'Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250506796v1-multimodal-fake-news-detection-mfnd-dataset-and-shallow-deep-multita.html">Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning</a></td>
  <td>æå‡ºå¤šæ¨¡æ€å‡æ–°é—»æ£€æµ‹æ–¹æ³•ä»¥åº”å¯¹æ·±ä¼ªé€ æ”»å‡»</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06796v1" data-paper-url="./papers/250506796v1-multimodal-fake-news-detection-mfnd-dataset-and-shallow-deep-multita.html" onclick="toggleFavorite(this, '2505.06796v1', 'Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250507019v1-a-vision-language-foundation-model-for-leaf-disease-identification.html">A Vision-Language Foundation Model for Leaf Disease Identification</a></td>
  <td>æå‡ºSCOLDæ¨¡å‹ä»¥è§£å†³æ¤ç‰©å¶ç‰‡ç–¾ç—…è¯†åˆ«é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07019v1" data-paper-url="./papers/250507019v1-a-vision-language-foundation-model-for-leaf-disease-identification.html" onclick="toggleFavorite(this, '2505.07019v1', 'A Vision-Language Foundation Model for Leaf Disease Identification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250506855v1-joint-low-level-and-high-level-textual-representation-learning-with-.html">Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies</a></td>
  <td>æå‡ºå¤šé‡æ©è”½ç­–ç•¥ä»¥è§£å†³æ–‡æœ¬è¯†åˆ«ä¸­çš„ç‰¹å¾å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">masked autoencoder</span> <span class="paper-tag">MAE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06855v1" data-paper-url="./papers/250506855v1-joint-low-level-and-high-level-textual-representation-learning-with-.html" onclick="toggleFavorite(this, '2505.06855v1', 'Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250506905v3-enhancing-monocular-height-estimation-via-sparse-lidar-guided-correc.html">Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction</a></td>
  <td>æå‡ºç¨€ç–LiDARå¼•å¯¼ä¿®æ­£ä»¥æå‡å•ç›®é«˜åº¦ä¼°è®¡ç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">MAE</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06905v3" data-paper-url="./papers/250506905v3-enhancing-monocular-height-estimation-via-sparse-lidar-guided-correc.html" onclick="toggleFavorite(this, '2505.06905v3', 'Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250507007v3-mellm-a-flow-guided-large-language-model-for-micro-expression-unders.html">MELLM: A Flow-Guided Large Language Model for Micro-Expression Understanding</a></td>
  <td>æå‡ºMELLMä»¥è§£å†³å¾®è¡¨æƒ…ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07007v3" data-paper-url="./papers/250507007v3-mellm-a-flow-guided-large-language-model-for-micro-expression-unders.html" onclick="toggleFavorite(this, '2505.07007v3', 'MELLM: A Flow-Guided Large Language Model for Micro-Expression Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250506894v1-neugen-amplifying-the-neural-in-neural-radiance-fields-for-domain-ge.html">NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization</a></td>
  <td>æå‡ºNeuGenä»¥è§£å†³NeRFåœ¨é¢†åŸŸæ³›åŒ–ä¸­çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06894v1" data-paper-url="./papers/250506894v1-neugen-amplifying-the-neural-in-neural-radiance-fields-for-domain-ge.html" onclick="toggleFavorite(this, '2505.06894v1', 'NeuGen: Amplifying the &#39;Neural&#39; in Neural Radiance Fields for Domain Generalization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250506991v1-technical-report-for-icra-2025-goose-2d-semantic-segmentation-challe.html">Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding</a></td>
  <td>æå‡ºåŸºäºRoPE-Swinçš„è¯­ä¹‰åˆ†å‰²æ¡†æ¶ä»¥è§£å†³æˆ·å¤–åœºæ™¯ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06991v1" data-paper-url="./papers/250506991v1-technical-report-for-icra-2025-goose-2d-semantic-segmentation-challe.html" onclick="toggleFavorite(this, '2505.06991v1', 'Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250507040v1-differentiable-nms-via-sinkhorn-matching-for-end-to-end-fabric-defec.html">Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection</a></td>
  <td>æå‡ºå¯å¾®åˆ†NMSæ¡†æ¶ä»¥è§£å†³ç»‡ç‰©ç¼ºé™·æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07040v1" data-paper-url="./papers/250507040v1-differentiable-nms-via-sinkhorn-matching-for-end-to-end-fabric-defec.html" onclick="toggleFavorite(this, '2505.07040v1', 'Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250507085v1-privacy-of-groups-in-dense-street-imagery.html">Privacy of Groups in Dense Street Imagery</a></td>
  <td>æå‡ºéšç§ä¿æŠ¤æ¡†æ¶ä»¥åº”å¯¹å¯†é›†è¡—æ™¯å›¾åƒä¸­çš„ç¾¤ä½“éšç§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">penetration</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07085v1" data-paper-url="./papers/250507085v1-privacy-of-groups-in-dense-street-imagery.html" onclick="toggleFavorite(this, '2505.07085v1', 'Privacy of Groups in Dense Street Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)