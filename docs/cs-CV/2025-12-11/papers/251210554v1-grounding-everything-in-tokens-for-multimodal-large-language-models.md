---
layout: default
title: Grounding Everything in Tokens for Multimodal Large Language Models
---

# Grounding Everything in Tokens for Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.10554" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.10554v1</a>
  <a href="https://arxiv.org/pdf/2512.10554.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10554v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.10554v1', 'Grounding Everything in Tokens for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiangxuan Ren, Zhongdao Wang, Liping Hou, Pin Tang, Guoqing Wang, Chao Ma

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

**å¤‡æ³¨**: 19 pages, 16 figures, 12 Tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GETokï¼šé€šè¿‡tokenåŒ–å®ç°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç²¾ç¡®2Dç©ºé—´å®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `2Dç©ºé—´å®šä½` `å›¾åƒtokenåŒ–` `ç©ºé—´å…³ç³»æ¨ç†` `è§†è§‰ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MLLMä¾èµ–å›¾åƒtokenåŒ–ï¼Œä½†ç°æœ‰tokenåŒ–æ–¹æ³•éš¾ä»¥ç²¾ç¡®åœ°åœ¨2Dç©ºé—´ä¸­å®šä½ç‰©ä½“ã€‚
2. GEToké€šè¿‡å¼•å…¥ç½‘æ ¼tokenå’Œåç§»tokenï¼Œå°†ç©ºé—´å…³ç³»åµŒå…¥åˆ°tokenä¸­ï¼Œå®ç°ç²¾ç¡®å®šä½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGETokåœ¨æŒ‡ä»£ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨2Dç©ºé—´æ¨ç†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒMLLMsä½¿ç”¨çš„è‡ªå›å½’Transformeræ¶æ„éœ€è¦å¯¹è¾“å…¥å›¾åƒè¿›è¡ŒtokenåŒ–ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨2Då›¾åƒç©ºé—´å†…ç²¾ç¡®å®šä½å¯¹è±¡çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºå¯¹è±¡å®šä½çš„ç©ºé—´è¡¨ç¤ºæ–¹æ³•ï¼Œåä¸ºGETokï¼Œå®ƒå°†ä¸€ä¸ªä¸“é—¨çš„å¯å­¦ä¹ tokenè¯æ±‡è¡¨é›†æˆåˆ°MLLMsä¸­ã€‚GEToké¦–å…ˆä½¿ç”¨ç½‘æ ¼tokenå°†å›¾åƒå¹³é¢åˆ’åˆ†ä¸ºç»“æ„åŒ–çš„ç©ºé—´é”šç‚¹ï¼Œç„¶ååˆ©ç”¨åç§»tokenæ¥å®ç°å¯¹å®šä½é¢„æµ‹çš„ç²¾ç¡®å’Œè¿­ä»£ç»†åŒ–ã€‚é€šè¿‡å°†ç©ºé—´å…³ç³»ç›´æ¥åµŒå…¥åˆ°tokenä¸­ï¼ŒGETokæ˜¾è‘—æå‡äº†MLLMsåœ¨åŸç”Ÿ2Dç©ºé—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¿®æ”¹è‡ªå›å½’æ¶æ„ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸­ï¼ŒGETokåœ¨å„ç§æŒ‡ä»£ä»»åŠ¡ä¸Šéƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†è§†è§‰ä¿¡æ¯æ—¶ï¼Œéœ€è¦å°†å›¾åƒè½¬æ¢ä¸ºtokenåºåˆ—ã€‚ç„¶è€Œï¼Œç°æœ‰çš„tokenåŒ–æ–¹æ³•åœ¨å°†å›¾åƒç‰¹å¾æ˜ å°„åˆ°ç¦»æ•£tokenæ—¶ï¼Œä¼šä¸¢å¤±ç²¾ç¡®çš„ç©ºé—´ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥åœ¨2Då›¾åƒç©ºé—´ä¸­ç²¾ç¡®å®šä½å’Œç†è§£ç‰©ä½“ä¹‹é—´çš„å…³ç³»ã€‚è¿™é™åˆ¶äº†MLLMsåœ¨éœ€è¦ç²¾ç¡®ç©ºé—´æ¨ç†çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGETokçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç©ºé—´ä¿¡æ¯ç›´æ¥ç¼–ç åˆ°tokenä¸­ï¼Œä»è€Œä½¿MLLMsèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å›¾åƒä¸­çš„ç©ºé—´å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼ŒGETokå¼•å…¥äº†ä¸¤ç§æ–°çš„tokenï¼šç½‘æ ¼tokenå’Œåç§»tokenã€‚ç½‘æ ¼tokenç”¨äºå°†å›¾åƒåˆ’åˆ†ä¸ºè§„åˆ™çš„ç½‘æ ¼ï¼Œæä¾›ç²—ç•¥çš„ç©ºé—´é”šç‚¹ï¼›åç§»tokenåˆ™ç”¨äºåœ¨ç½‘æ ¼çš„åŸºç¡€ä¸Šè¿›è¡Œç²¾ç»†çš„ä½ç½®è°ƒæ•´ï¼Œå®ç°ç²¾ç¡®çš„ç‰©ä½“å®šä½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGETokçš„æ•´ä½“æ¡†æ¶æ˜¯åœ¨ç°æœ‰çš„MLLMæ¶æ„ä¸­åŠ å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„tokenè¯æ±‡è¡¨ã€‚é¦–å…ˆï¼Œä½¿ç”¨ç½‘æ ¼tokenå°†è¾“å…¥å›¾åƒåˆ†å‰²æˆå¤šä¸ªç½‘æ ¼åŒºåŸŸï¼Œæ¯ä¸ªç½‘æ ¼åŒºåŸŸå¯¹åº”ä¸€ä¸ªtokenã€‚ç„¶åï¼Œä½¿ç”¨åç§»tokenå¯¹æ¯ä¸ªç½‘æ ¼åŒºåŸŸå†…çš„ç‰©ä½“ä½ç½®è¿›è¡Œå¾®è°ƒã€‚è¿™äº›tokenä¸å›¾åƒçš„å…¶ä»–è§†è§‰tokenä¸€èµ·è¾“å…¥åˆ°MLLMä¸­è¿›è¡Œå¤„ç†ã€‚MLLMåˆ©ç”¨è¿™äº›ç©ºé—´tokenè¿›è¡Œæ¨ç†ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„2Dç©ºé—´å®šä½ã€‚

**å…³é”®åˆ›æ–°**ï¼šGETokçš„å…³é”®åˆ›æ–°åœ¨äºå°†ç©ºé—´ä¿¡æ¯æ˜¾å¼åœ°ç¼–ç åˆ°tokenä¸­ã€‚ä¸ä¼ ç»Ÿçš„tokenåŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒGETokä¸ä»…ä¿ç•™äº†å›¾åƒçš„è§†è§‰ç‰¹å¾ï¼Œè¿˜ä¿ç•™äº†ç‰©ä½“åœ¨å›¾åƒä¸­çš„ç©ºé—´ä½ç½®ä¿¡æ¯ã€‚è¿™ç§æ˜¾å¼çš„ç©ºé—´ç¼–ç æ–¹å¼ä½¿å¾—MLLMsèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å›¾åƒä¸­çš„ç©ºé—´å…³ç³»ï¼Œä»è€Œæé«˜å…¶åœ¨éœ€è¦ç²¾ç¡®ç©ºé—´æ¨ç†çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šGETokçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç½‘æ ¼tokençš„æ•°é‡å’Œå¤§å°ï¼Œéœ€è¦æ ¹æ®å›¾åƒçš„åˆ†è¾¨ç‡å’Œç‰©ä½“çš„å°ºå¯¸è¿›è¡Œè°ƒæ•´ï¼›2) åç§»tokençš„è¡¨ç¤ºæ–¹å¼ï¼Œå¯ä»¥ä½¿ç”¨ç›¸å¯¹åæ ‡æˆ–ç»å¯¹åæ ‡ï¼›3) å¦‚ä½•å°†ç½‘æ ¼tokenå’Œåç§»tokenä¸å›¾åƒçš„å…¶ä»–è§†è§‰tokenè¿›è¡Œèåˆï¼Œå¯ä»¥ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–èåˆæ–¹æ³•ï¼›4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼Œéœ€è¦è€ƒè™‘å®šä½çš„ç²¾åº¦å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGETokåœ¨å„ç§æŒ‡ä»£ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨RefCOCOgæ•°æ®é›†ä¸Šï¼ŒGETokçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†è¶…è¿‡5%ã€‚æ­¤å¤–ï¼ŒGETokåœ¨å¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸‹ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GETokå¯åº”ç”¨äºéœ€è¦ç²¾ç¡®2Dç©ºé—´å®šä½çš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ã€ç›®æ ‡æ£€æµ‹å’Œæœºå™¨äººå¯¼èˆªã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æ¨¡å‹å¯¹å›¾åƒä¸­ç‰©ä½“ç©ºé—´å…³ç³»çš„ç†è§£ï¼Œä»è€Œæé«˜ä»»åŠ¡æ€§èƒ½ã€‚æœªæ¥ï¼ŒGETokæœ‰æœ›åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ç›‘æ§å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.

