---
layout: default
title: IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation
---

# IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.10730" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.10730v1</a>
  <a href="https://arxiv.org/pdf/2512.10730.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10730v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.10730v1', 'IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuan-Ming Li, Qize Yang, Nan Lei, Shenghao Fu, Ling-An Zeng, Jian-Fang Hu, Xihan Wei, Wei-Shi Zheng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

**å¤‡æ³¨**: 25 pages, 16 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HumanMLLM/IRG-MotionLLM/tree)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIRG-MotionLLMï¼Œé€šè¿‡äº¤é”™è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–ï¼Œæå‡æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆ` `è¿åŠ¨ç”Ÿæˆ` `åŠ¨ä½œè¯„ä¼°` `åŠ¨ä½œä¼˜åŒ–` `äº¤é”™æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•é€šå¸¸å°†åŠ¨ä½œç†è§£å’Œç”Ÿæˆåˆ†ç¦»ï¼Œé™åˆ¶äº†ä»»åŠ¡é—´äº¤äº’åé¦ˆå¸¦æ¥çš„äº’ç›Šã€‚
2. æå‡ºIRMoGenèŒƒå¼ï¼Œé€šè¿‡è¿åŠ¨è¯„ä¼°å’Œä¼˜åŒ–ï¼Œå®ç°ç†è§£å’Œç”Ÿæˆä¹‹é—´çš„åŒå‘çŸ¥è¯†æµåŠ¨ã€‚
3. æ„å»ºIRG-MotionLLMæ¨¡å‹ï¼Œå¹¶åœ¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆä¸‹ï¼Œåœ¨æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°åŠ¨ä½œç”ŸæˆèŒƒå¼ï¼šè¿åŠ¨ç”Ÿæˆäº¤é”™æ¨ç†ï¼ˆIRMoGenï¼‰ã€‚è¯¥èŒƒå¼å°†è¿åŠ¨ç”Ÿæˆä¸è¯„ä¼°å’Œä¼˜åŒ–ç´§å¯†ç»“åˆï¼Œé€šè¿‡è¿­ä»£çš„æ–‡æœ¬-åŠ¨ä½œå¯¹è¯å®ç°åŒå‘çŸ¥è¯†æµåŠ¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†IRG-MotionLLMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— ç¼äº¤é”™è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–çš„æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆæ€§èƒ½ã€‚IRG-MotionLLMé€šè¿‡ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆé€æ­¥å¼€å‘ï¼Œåˆå§‹åŒ–å¹¶å¢å¼ºäº†åŸç”Ÿçš„IRMoGenèƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›å¼€å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®å¼•æ“ï¼Œç”¨äºä»ç°æœ‰çš„æ–‡æœ¬-åŠ¨ä½œæ•°æ®é›†ä¸­åˆæˆäº¤é”™æ¨ç†æ³¨é‡Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼šï¼ˆiï¼‰è¯„ä¼°å’Œä¼˜åŒ–ä»»åŠ¡æ˜¾è‘—æé«˜äº†æ–‡æœ¬-åŠ¨ä½œå¯¹é½ï¼›ï¼ˆiiï¼‰äº¤é”™è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–æ­¥éª¤åœ¨è®­ç»ƒé˜¶æ®µå§‹ç»ˆäº§ç”Ÿæ€§èƒ½æå‡ï¼›ï¼ˆiiiï¼‰IRG-MotionLLMæ˜æ˜¾ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨æ ‡å‡†æ–‡æœ¬åˆ°åŠ¨ä½œç”ŸæˆåŸºå‡†ä¸Šå–å¾—äº†å…ˆè¿›çš„æ€§èƒ½ã€‚äº¤å‰è¯„ä¼°å™¨æµ‹è¯•è¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ¨¡å‹é€šå¸¸å°†åŠ¨ä½œç†è§£å’Œç”Ÿæˆè§†ä¸ºç‹¬ç«‹çš„ä»»åŠ¡ï¼Œç¼ºä¹ä¸¤è€…ä¹‹é—´çš„æœ‰æ•ˆäº’åŠ¨å’Œåé¦ˆæœºåˆ¶ã€‚è¿™ç§åˆ†ç¦»é™åˆ¶äº†æ¨¡å‹å……åˆ†åˆ©ç”¨åŠ¨ä½œè¯„ä¼°å’Œä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡éš¾ä»¥è¿›ä¸€æ­¥æå‡ã€‚å› æ­¤ï¼Œå¦‚ä½•å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆåŠ¨ä½œç†è§£ã€ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–çš„ç»Ÿä¸€æ¡†æ¶æ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥äº¤é”™æ¨ç†ï¼ˆInterleaved Reasoningï¼‰æœºåˆ¶ï¼Œå°†åŠ¨ä½œç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–ä¸‰ä¸ªä»»åŠ¡ç´§å¯†è€¦åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡è¿­ä»£åœ°è¿›è¡Œæ–‡æœ¬-åŠ¨ä½œå¯¹è¯ï¼Œæ¨¡å‹å¯ä»¥åœ¨ç”ŸæˆåŠ¨ä½œçš„åŒæ—¶ï¼Œè¯„ä¼°å…¶è´¨é‡å¹¶è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œå®ç°åŒå‘çŸ¥è¯†æµåŠ¨ï¼Œæå‡ç”Ÿæˆæ€§èƒ½ã€‚è¿™ç§è®¾è®¡æ¨¡ä»¿äº†äººç±»åœ¨åˆ›ä½œè¿‡ç¨‹ä¸­çš„è¿­ä»£æ”¹è¿›æ–¹å¼ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ–‡æœ¬æè¿°å¹¶ç”Ÿæˆæ›´ç¬¦åˆè¦æ±‚çš„åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIRG-MotionLLMçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¿åŠ¨ç”Ÿæˆå™¨ã€è¿åŠ¨è¯„ä¼°å™¨å’Œè¿åŠ¨ä¼˜åŒ–å™¨ã€‚è¿™ä¸‰ä¸ªæ¨¡å—é€šè¿‡äº¤é”™æ¨ç†çš„æ–¹å¼è¿›è¡Œäº¤äº’ã€‚é¦–å…ˆï¼Œè¿åŠ¨ç”Ÿæˆå™¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆåˆå§‹åŠ¨ä½œï¼›ç„¶åï¼Œè¿åŠ¨è¯„ä¼°å™¨è¯„ä¼°è¯¥åŠ¨ä½œçš„è´¨é‡ï¼Œå¹¶ç»™å‡ºè¯„ä¼°ç»“æœï¼›æœ€åï¼Œè¿åŠ¨ä¼˜åŒ–å™¨æ ¹æ®è¯„ä¼°ç»“æœå¯¹åŠ¨ä½œè¿›è¡Œä¼˜åŒ–ï¼Œç”Ÿæˆæ”¹è¿›åçš„åŠ¨ä½œã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥è¿­ä»£å¤šæ¬¡ï¼Œç›´åˆ°ç”Ÿæˆæ»¡æ„çš„åŠ¨ä½œã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†äº¤é”™æ¨ç†ï¼ˆInterleaved Reasoningï¼‰çš„èŒƒå¼ï¼Œå°†è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–ä¸‰ä¸ªä»»åŠ¡æœ‰æœºåœ°ç»“åˆåœ¨ä¸€èµ·ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIRG-MotionLLMä¸å†å°†è¿™ä¸‰ä¸ªä»»åŠ¡è§†ä¸ºç‹¬ç«‹çš„æ­¥éª¤ï¼Œè€Œæ˜¯é€šè¿‡è¿­ä»£çš„æ–¹å¼è¿›è¡Œäº¤äº’ï¼Œä»è€Œå®ç°äº†åŒå‘çŸ¥è¯†æµåŠ¨ï¼Œæå‡äº†ç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®å¼•æ“ï¼Œç”¨äºåˆæˆäº¤é”™æ¨ç†æ³¨é‡Šï¼Œä¸ºæ¨¡å‹çš„è®­ç»ƒæä¾›äº†å……è¶³çš„æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šIRG-MotionLLMé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯åˆå§‹åŒ–é˜¶æ®µï¼Œä¸»è¦è®­ç»ƒè¿åŠ¨ç”Ÿæˆå™¨çš„åŸºæœ¬èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯å¢å¼ºé˜¶æ®µï¼Œä¸»è¦è®­ç»ƒè¿åŠ¨è¯„ä¼°å™¨å’Œè¿åŠ¨ä¼˜åŒ–å™¨çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰é˜¶æ®µæ˜¯äº¤é”™æ¨ç†é˜¶æ®µï¼Œä¸»è¦è®­ç»ƒä¸‰ä¸ªæ¨¡å—ä¹‹é—´çš„ååŒå·¥ä½œèƒ½åŠ›ã€‚åœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼Œæœ¬æ–‡é‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬æ–‡æœ¬-åŠ¨ä½œå¯¹é½æŸå¤±ã€åŠ¨ä½œè´¨é‡æŸå¤±å’ŒåŠ¨ä½œä¼˜åŒ–æŸå¤±ã€‚åœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼Œæœ¬æ–‡é‡‡ç”¨äº†Transformeræ¶æ„ï¼Œå¹¶é’ˆå¯¹è¿åŠ¨æ•°æ®çš„ç‰¹ç‚¹è¿›è¡Œäº†ä¸€äº›æ”¹è¿›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIRG-MotionLLMåœ¨æ ‡å‡†æ–‡æœ¬åˆ°åŠ¨ä½œç”ŸæˆåŸºå‡†ä¸Šå–å¾—äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜æ˜¾ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æ–‡æœ¬-åŠ¨ä½œå¯¹é½æ–¹é¢ï¼ŒIRG-MotionLLMçš„æ€§èƒ½æå‡äº†çº¦10%ã€‚æ­¤å¤–ï¼Œäº¤å‰è¯„ä¼°å™¨æµ‹è¯•è¿›ä¸€æ­¥éªŒè¯äº†IRG-MotionLLMçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶ç”Ÿæˆçš„åŠ¨ä½œå…·æœ‰æ›´é«˜çš„è´¨é‡å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œç­‰é¢†åŸŸï¼Œå®ç°æ›´è‡ªç„¶ã€æ›´é€¼çœŸçš„äººä½“åŠ¨ä½œç”Ÿæˆã€‚ä¾‹å¦‚ï¼Œåœ¨è™šæ‹Ÿç°å®æ¸¸æˆä¸­ï¼Œå¯ä»¥æ ¹æ®ç©å®¶çš„è¯­éŸ³æˆ–æ–‡æœ¬æŒ‡ä»¤ï¼Œå®æ—¶ç”Ÿæˆç›¸åº”çš„è§’è‰²åŠ¨ä½œï¼Œæå‡æ¸¸æˆçš„æ²‰æµ¸æ„Ÿå’Œäº’åŠ¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºåº·å¤è®­ç»ƒã€è¿åŠ¨åˆ†æç­‰é¢†åŸŸï¼Œå¸®åŠ©äººä»¬æ›´å¥½åœ°ç†è§£å’Œæ”¹å–„è¿åŠ¨è¡¨ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.

