---
layout: default
title: IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation
---

# IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation

**arXiv**: [2512.10730v1](https://arxiv.org/abs/2512.10730) | [PDF](https://arxiv.org/pdf/2512.10730.pdf)

**ä½œè€…**: Yuan-Ming Li, Qize Yang, Nan Lei, Shenghao Fu, Ling-An Zeng, Jian-Fang Hu, Xihan Wei, Wei-Shi Zheng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

**å¤‡æ³¨**: 25 pages, 16 figures

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HumanMLLM/IRG-MotionLLM/tree)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIRG-MotionLLMï¼Œé€šè¿‡äº¤é”™è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–ï¼Œæå‡æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ•ˆæžœ**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆ` `è¿åŠ¨ç”Ÿæˆ` `åŠ¨ä½œè¯„ä¼°` `åŠ¨ä½œä¼˜åŒ–` `äº¤é”™æŽ¨ç†` `å¤§åž‹è¯­è¨€æ¨¡åž‹` `å¤šæ¨¡æ€å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•é€šå¸¸å°†åŠ¨ä½œç†è§£å’Œç”Ÿæˆåˆ†ç¦»ï¼Œé™åˆ¶äº†ä»»åŠ¡é—´äº¤äº’åé¦ˆå¸¦æ¥çš„äº’ç›Šã€‚
2. æå‡ºIRMoGenèŒƒå¼ï¼Œé€šè¿‡è¿åŠ¨è¯„ä¼°å’Œä¼˜åŒ–ï¼Œå®žçŽ°ç†è§£å’Œç”Ÿæˆä¹‹é—´çš„åŒå‘çŸ¥è¯†æµåŠ¨ã€‚
3. æž„å»ºIRG-MotionLLMæ¨¡åž‹ï¼Œå¹¶åœ¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆä¸‹ï¼Œåœ¨æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—æ€§èƒ½æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°åŠ¨ä½œç”ŸæˆèŒƒå¼ï¼šè¿åŠ¨ç”Ÿæˆäº¤é”™æŽ¨ç†ï¼ˆIRMoGenï¼‰ã€‚è¯¥èŒƒå¼å°†è¿åŠ¨ç”Ÿæˆä¸Žè¯„ä¼°å’Œä¼˜åŒ–ç´§å¯†ç»“åˆï¼Œé€šè¿‡è¿­ä»£çš„æ–‡æœ¬-åŠ¨ä½œå¯¹è¯å®žçŽ°åŒå‘çŸ¥è¯†æµåŠ¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†IRG-MotionLLMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— ç¼äº¤é”™è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–çš„æ¨¡åž‹ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆæ€§èƒ½ã€‚IRG-MotionLLMé€šè¿‡ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆé€æ­¥å¼€å‘ï¼Œåˆå§‹åŒ–å¹¶å¢žå¼ºäº†åŽŸç”Ÿçš„IRMoGenèƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›å¼€å‘ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®å¼•æ“Žï¼Œç”¨äºŽä»ŽçŽ°æœ‰çš„æ–‡æœ¬-åŠ¨ä½œæ•°æ®é›†ä¸­åˆæˆäº¤é”™æŽ¨ç†æ³¨é‡Šã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼šï¼ˆiï¼‰è¯„ä¼°å’Œä¼˜åŒ–ä»»åŠ¡æ˜¾è‘—æé«˜äº†æ–‡æœ¬-åŠ¨ä½œå¯¹é½ï¼›ï¼ˆiiï¼‰äº¤é”™è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–æ­¥éª¤åœ¨è®­ç»ƒé˜¶æ®µå§‹ç»ˆäº§ç”Ÿæ€§èƒ½æå‡ï¼›ï¼ˆiiiï¼‰IRG-MotionLLMæ˜Žæ˜¾ä¼˜äºŽåŸºçº¿æ¨¡åž‹ï¼Œå¹¶åœ¨æ ‡å‡†æ–‡æœ¬åˆ°åŠ¨ä½œç”ŸæˆåŸºå‡†ä¸Šå–å¾—äº†å…ˆè¿›çš„æ€§èƒ½ã€‚äº¤å‰è¯„ä¼°å™¨æµ‹è¯•è¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ¨¡åž‹é€šå¸¸å°†åŠ¨ä½œç†è§£å’Œç”Ÿæˆè§†ä¸ºç‹¬ç«‹çš„ä»»åŠ¡ï¼Œç¼ºä¹ä¸¤è€…ä¹‹é—´çš„æœ‰æ•ˆäº’åŠ¨å’Œåé¦ˆæœºåˆ¶ã€‚è¿™ç§åˆ†ç¦»é™åˆ¶äº†æ¨¡åž‹å……åˆ†åˆ©ç”¨åŠ¨ä½œè¯„ä¼°å’Œä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡éš¾ä»¥è¿›ä¸€æ­¥æå‡ã€‚å› æ­¤ï¼Œå¦‚ä½•å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆåŠ¨ä½œç†è§£ã€ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–çš„ç»Ÿä¸€æ¡†æž¶æ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥äº¤é”™æŽ¨ç†ï¼ˆInterleaved Reasoningï¼‰æœºåˆ¶ï¼Œå°†åŠ¨ä½œç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–ä¸‰ä¸ªä»»åŠ¡ç´§å¯†è€¦åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡è¿­ä»£åœ°è¿›è¡Œæ–‡æœ¬-åŠ¨ä½œå¯¹è¯ï¼Œæ¨¡åž‹å¯ä»¥åœ¨ç”ŸæˆåŠ¨ä½œçš„åŒæ—¶ï¼Œè¯„ä¼°å…¶è´¨é‡å¹¶è¿›è¡Œä¼˜åŒ–ï¼Œä»Žè€Œå®žçŽ°åŒå‘çŸ¥è¯†æµåŠ¨ï¼Œæå‡ç”Ÿæˆæ€§èƒ½ã€‚è¿™ç§è®¾è®¡æ¨¡ä»¿äº†äººç±»åœ¨åˆ›ä½œè¿‡ç¨‹ä¸­çš„è¿­ä»£æ”¹è¿›æ–¹å¼ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ–‡æœ¬æè¿°å¹¶ç”Ÿæˆæ›´ç¬¦åˆè¦æ±‚çš„åŠ¨ä½œã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šIRG-MotionLLMçš„æ•´ä½“æ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¿åŠ¨ç”Ÿæˆå™¨ã€è¿åŠ¨è¯„ä¼°å™¨å’Œè¿åŠ¨ä¼˜åŒ–å™¨ã€‚è¿™ä¸‰ä¸ªæ¨¡å—é€šè¿‡äº¤é”™æŽ¨ç†çš„æ–¹å¼è¿›è¡Œäº¤äº’ã€‚é¦–å…ˆï¼Œè¿åŠ¨ç”Ÿæˆå™¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆåˆå§‹åŠ¨ä½œï¼›ç„¶åŽï¼Œè¿åŠ¨è¯„ä¼°å™¨è¯„ä¼°è¯¥åŠ¨ä½œçš„è´¨é‡ï¼Œå¹¶ç»™å‡ºè¯„ä¼°ç»“æžœï¼›æœ€åŽï¼Œè¿åŠ¨ä¼˜åŒ–å™¨æ ¹æ®è¯„ä¼°ç»“æžœå¯¹åŠ¨ä½œè¿›è¡Œä¼˜åŒ–ï¼Œç”Ÿæˆæ”¹è¿›åŽçš„åŠ¨ä½œã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥è¿­ä»£å¤šæ¬¡ï¼Œç›´åˆ°ç”Ÿæˆæ»¡æ„çš„åŠ¨ä½œã€‚æ•´ä¸ªæ¡†æž¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæå‡ºäº†äº¤é”™æŽ¨ç†ï¼ˆInterleaved Reasoningï¼‰çš„èŒƒå¼ï¼Œå°†è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–ä¸‰ä¸ªä»»åŠ¡æœ‰æœºåœ°ç»“åˆåœ¨ä¸€èµ·ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIRG-MotionLLMä¸å†å°†è¿™ä¸‰ä¸ªä»»åŠ¡è§†ä¸ºç‹¬ç«‹çš„æ­¥éª¤ï¼Œè€Œæ˜¯é€šè¿‡è¿­ä»£çš„æ–¹å¼è¿›è¡Œäº¤äº’ï¼Œä»Žè€Œå®žçŽ°äº†åŒå‘çŸ¥è¯†æµåŠ¨ï¼Œæå‡äº†ç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æž„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®å¼•æ“Žï¼Œç”¨äºŽåˆæˆäº¤é”™æŽ¨ç†æ³¨é‡Šï¼Œä¸ºæ¨¡åž‹çš„è®­ç»ƒæä¾›äº†å……è¶³çš„æ•°æ®æ”¯æŒã€‚

**å…³é”®è®¾è®¡**ï¼šIRG-MotionLLMé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯åˆå§‹åŒ–é˜¶æ®µï¼Œä¸»è¦è®­ç»ƒè¿åŠ¨ç”Ÿæˆå™¨çš„åŸºæœ¬èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯å¢žå¼ºé˜¶æ®µï¼Œä¸»è¦è®­ç»ƒè¿åŠ¨è¯„ä¼°å™¨å’Œè¿åŠ¨ä¼˜åŒ–å™¨çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰é˜¶æ®µæ˜¯äº¤é”™æŽ¨ç†é˜¶æ®µï¼Œä¸»è¦è®­ç»ƒä¸‰ä¸ªæ¨¡å—ä¹‹é—´çš„ååŒå·¥ä½œèƒ½åŠ›ã€‚åœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼Œæœ¬æ–‡é‡‡ç”¨äº†å¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬æ–‡æœ¬-åŠ¨ä½œå¯¹é½æŸå¤±ã€åŠ¨ä½œè´¨é‡æŸå¤±å’ŒåŠ¨ä½œä¼˜åŒ–æŸå¤±ã€‚åœ¨ç½‘ç»œç»“æž„æ–¹é¢ï¼Œæœ¬æ–‡é‡‡ç”¨äº†Transformeræž¶æž„ï¼Œå¹¶é’ˆå¯¹è¿åŠ¨æ•°æ®çš„ç‰¹ç‚¹è¿›è¡Œäº†ä¸€äº›æ”¹è¿›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒIRG-MotionLLMåœ¨æ ‡å‡†æ–‡æœ¬åˆ°åŠ¨ä½œç”ŸæˆåŸºå‡†ä¸Šå–å¾—äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜Žæ˜¾ä¼˜äºŽåŸºçº¿æ¨¡åž‹ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æ–‡æœ¬-åŠ¨ä½œå¯¹é½æ–¹é¢ï¼ŒIRG-MotionLLMçš„æ€§èƒ½æå‡äº†çº¦10%ã€‚æ­¤å¤–ï¼Œäº¤å‰è¯„ä¼°å™¨æµ‹è¯•è¿›ä¸€æ­¥éªŒè¯äº†IRG-MotionLLMçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜Žå…¶ç”Ÿæˆçš„åŠ¨ä½œå…·æœ‰æ›´é«˜çš„è´¨é‡å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè™šæ‹ŸçŽ°å®žã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œç­‰é¢†åŸŸï¼Œå®žçŽ°æ›´è‡ªç„¶ã€æ›´é€¼çœŸçš„äººä½“åŠ¨ä½œç”Ÿæˆã€‚ä¾‹å¦‚ï¼Œåœ¨è™šæ‹ŸçŽ°å®žæ¸¸æˆä¸­ï¼Œå¯ä»¥æ ¹æ®çŽ©å®¶çš„è¯­éŸ³æˆ–æ–‡æœ¬æŒ‡ä»¤ï¼Œå®žæ—¶ç”Ÿæˆç›¸åº”çš„è§’è‰²åŠ¨ä½œï¼Œæå‡æ¸¸æˆçš„æ²‰æµ¸æ„Ÿå’Œäº’åŠ¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºŽåº·å¤è®­ç»ƒã€è¿åŠ¨åˆ†æžç­‰é¢†åŸŸï¼Œå¸®åŠ©äººä»¬æ›´å¥½åœ°ç†è§£å’Œæ”¹å–„è¿åŠ¨è¡¨çŽ°ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.

