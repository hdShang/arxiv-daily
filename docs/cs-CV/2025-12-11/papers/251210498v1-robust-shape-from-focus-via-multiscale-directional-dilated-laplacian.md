---
layout: default
title: Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network
---

# Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.10498" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.10498v1</a>
  <a href="https://arxiv.org/pdf/2512.10498.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10498v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.10498v1', 'Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Khurram Ashfaq, Muhammad Tariq Mahmood

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

**å¤‡æ³¨**: Accepted to IJCV

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šå°ºåº¦æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯å’Œå¾ªç¯ç½‘ç»œçš„ç¨³å¥Shape-from-Focusæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `Shape-from-Focus` `æ·±åº¦ä¼°è®¡` `æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯` `å¾ªç¯ç¥ç»ç½‘ç»œ` `å¤šå°ºåº¦å­¦ä¹ ` `æ·±åº¦å­¦ä¹ ` `å›¾åƒå¤„ç†` `ä¸‰ç»´é‡å»º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„SFFæ–¹æ³•ä¾èµ–å¤æ‚ç‰¹å¾æå–å’Œç®€å•èšåˆï¼Œæ˜“å¼•å…¥ä¼ªå½±å’Œå™ªå£°ã€‚
2. æå‡ºæ··åˆæ¡†æ¶ï¼Œåˆ©ç”¨æ‰‹å·¥DDLæ ¸æå–é²æ£’ç„¦ç‚¹ä½“ç§¯ï¼Œå†ç”¨è½»é‡çº§GRUç½‘ç»œè¿­ä»£ä¼˜åŒ–æ·±åº¦ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†ç²¾åº¦å’Œæ³›åŒ–æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Shape-from-Focus (SFF) æ˜¯ä¸€ç§è¢«åŠ¨æ·±åº¦ä¼°è®¡æŠ€æœ¯ï¼Œé€šè¿‡åˆ†æç„¦ç‚¹å †æ ˆä¸­çš„ç„¦ç‚¹å˜åŒ–æ¥æ¨æ–­åœºæ™¯æ·±åº¦ã€‚ç›®å‰åŸºäºæ·±åº¦å­¦ä¹ çš„SFFæ–¹æ³•é€šå¸¸åˆ†ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤æ‚çš„ç‰¹å¾ç¼–ç å™¨æå–ç„¦ç‚¹ä½“ç§¯ï¼ˆç„¦ç‚¹å †æ ˆä¸­æ¯ä¸ªåƒç´ çš„ç„¦ç‚¹å¯èƒ½æ€§è¡¨ç¤ºï¼‰ï¼›ç„¶åï¼Œé€šè¿‡ç®€å•çš„å•æ­¥èšåˆæŠ€æœ¯ä¼°è®¡æ·±åº¦ï¼Œè¿™é€šå¸¸ä¼šå¼•å…¥ä¼ªå½±å¹¶æ”¾å¤§æ·±åº¦å›¾ä¸­çš„å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼ ç»Ÿä¸Šä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯ (DDL) æ ¸è®¡ç®—å¤šå°ºåº¦ç„¦ç‚¹ä½“ç§¯ï¼Œè¿™äº›æ ¸æ•è·è¿œè·ç¦»å’Œæ–¹å‘ç„¦ç‚¹å˜åŒ–ä»¥å½¢æˆç¨³å¥çš„ç„¦ç‚¹ä½“ç§¯ã€‚ç„¶åï¼Œè¿™äº›ç„¦ç‚¹ä½“ç§¯è¢«è¾“å…¥åˆ°è½»é‡çº§çš„ã€åŸºäºå¤šå°ºåº¦GRUçš„æ·±åº¦æå–æ¨¡å—ä¸­ï¼Œè¯¥æ¨¡å—ä»¥è¾ƒä½çš„åˆ†è¾¨ç‡è¿­ä»£åœ°ç»†åŒ–åˆå§‹æ·±åº¦ä¼°è®¡ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬å¾ªç¯ç½‘ç»œä¸­å­¦ä¹ åˆ°çš„å‡¸ä¸Šé‡‡æ ·æ¨¡å—é‡å»ºé«˜åˆ†è¾¨ç‡æ·±åº¦å›¾ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„åœºæ™¯ç»†èŠ‚å’Œæ¸…æ™°çš„è¾¹ç•Œã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨ä¸åŒçš„ç„¦ç‚¹æ¡ä»¶ä¸‹å®ç°äº†å“è¶Šçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šShape-from-Focus (SFF)æ—¨åœ¨ä»ä¸€ç³»åˆ—å…·æœ‰ä¸åŒç„¦ç‚¹çš„å›¾åƒä¸­æ¢å¤åœºæ™¯çš„æ·±åº¦ä¿¡æ¯ã€‚ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„SFFæ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œå³é¦–å…ˆä½¿ç”¨å¤æ‚çš„ç‰¹å¾ç¼–ç å™¨æå–ç„¦ç‚¹ä½“ç§¯ï¼Œç„¶åä½¿ç”¨ç®€å•çš„èšåˆæŠ€æœ¯ä¼°è®¡æ·±åº¦ã€‚è¿™ç§æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œå¤æ‚çš„ç‰¹å¾ç¼–ç å™¨è®¡ç®—é‡å¤§ï¼Œè€Œç®€å•çš„èšåˆæŠ€æœ¯å®¹æ˜“å¼•å…¥ä¼ªå½±ï¼Œå¹¶æ”¾å¤§æ·±åº¦å›¾ä¸­çš„å™ªå£°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆä¼ ç»Ÿæ–¹æ³•å’Œæ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿ã€‚ä¸€æ–¹é¢ï¼Œåˆ©ç”¨æ‰‹å·¥è®¾è®¡çš„æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯ (DDL) æ ¸æå–é²æ£’çš„ç„¦ç‚¹ä½“ç§¯ï¼Œä»¥å‡å°‘å¯¹å¤æ‚ç‰¹å¾ç¼–ç å™¨çš„ä¾èµ–ã€‚å¦ä¸€æ–¹é¢ï¼Œä½¿ç”¨è½»é‡çº§çš„ã€åŸºäºå¤šå°ºåº¦GRUçš„å¾ªç¯ç½‘ç»œè¿­ä»£åœ°ç»†åŒ–æ·±åº¦ä¼°è®¡ï¼Œä»¥é¿å…ç®€å•èšåˆæŠ€æœ¯å¸¦æ¥çš„é—®é¢˜ã€‚è¿™ç§æ··åˆæ–¹æ³•æ—¨åœ¨æé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æŠ€æœ¯æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨DDLæ ¸è®¡ç®—å¤šå°ºåº¦ç„¦ç‚¹ä½“ç§¯ï¼›2) å°†ç„¦ç‚¹ä½“ç§¯è¾“å…¥åˆ°åŸºäºå¤šå°ºåº¦GRUçš„æ·±åº¦æå–æ¨¡å—ï¼›3) ä½¿ç”¨å­¦ä¹ åˆ°çš„å‡¸ä¸Šé‡‡æ ·æ¨¡å—é‡å»ºé«˜åˆ†è¾¨ç‡æ·±åº¦å›¾ã€‚æ·±åº¦æå–æ¨¡å—ä»¥è¾ƒä½çš„åˆ†è¾¨ç‡è¿­ä»£åœ°ç»†åŒ–åˆå§‹æ·±åº¦ä¼°è®¡ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚å‡¸ä¸Šé‡‡æ ·æ¨¡å—ç”¨äºåœ¨é‡å»ºé«˜åˆ†è¾¨ç‡æ·±åº¦å›¾æ—¶ä¿ç•™ç²¾ç»†çš„åœºæ™¯ç»†èŠ‚å’Œæ¸…æ™°çš„è¾¹ç•Œã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæ··åˆæ¡†æ¶çš„è®¾è®¡ã€‚å®ƒç»“åˆäº†ä¼ ç»Ÿæ‰‹å·¥ç‰¹å¾æå–æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œé¿å…äº†å•ä¸€æ–¹æ³•çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨DDLæ ¸æå–é²æ£’çš„ç„¦ç‚¹ä½“ç§¯ï¼Œå‡å°‘äº†å¯¹å¤æ‚ç‰¹å¾ç¼–ç å™¨çš„ä¾èµ–ï¼ŒåŒæ—¶ä½¿ç”¨å¾ªç¯ç½‘ç»œè¿­ä»£åœ°ç»†åŒ–æ·±åº¦ä¼°è®¡ï¼Œé¿å…äº†ç®€å•èšåˆæŠ€æœ¯å¸¦æ¥çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šDDLæ ¸çš„è®¾è®¡è€ƒè™‘äº†é•¿è·ç¦»å’Œæ–¹å‘ä¸Šçš„ç„¦ç‚¹å˜åŒ–ï¼Œä»¥æé«˜ç„¦ç‚¹ä½“ç§¯çš„é²æ£’æ€§ã€‚å¤šå°ºåº¦GRUç½‘ç»œçš„è®¾è®¡å…è®¸åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸Šè¿›è¡Œæ·±åº¦ä¼°è®¡ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œç²¾åº¦ã€‚å­¦ä¹ åˆ°çš„å‡¸ä¸Šé‡‡æ ·æ¨¡å—çš„è®¾è®¡æ—¨åœ¨ä¿ç•™é«˜åˆ†è¾¨ç‡æ·±åº¦å›¾ä¸­çš„ç²¾ç»†ç»†èŠ‚å’Œæ¸…æ™°è¾¹ç•Œã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†åœ¨ä¸åŒç„¦ç‚¹æ¡ä»¶ä¸‹å®ç°äº†å“è¶Šçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€ä¸‰ç»´é‡å»ºã€æ˜¾å¾®æˆåƒç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼ŒSFFæŠ€æœ¯å¯ä»¥å¸®åŠ©æœºå™¨äººæ„ŸçŸ¥å‘¨å›´ç¯å¢ƒçš„æ·±åº¦ä¿¡æ¯ï¼Œä»è€Œå®ç°è‡ªä¸»å¯¼èˆªã€‚åœ¨ä¸‰ç»´é‡å»ºä¸­ï¼ŒSFFæŠ€æœ¯å¯ä»¥ç”¨äºé‡å»ºåœºæ™¯çš„ä¸‰ç»´æ¨¡å‹ã€‚åœ¨æ˜¾å¾®æˆåƒä¸­ï¼ŒSFFæŠ€æœ¯å¯ä»¥ç”¨äºè·å–æ ·æœ¬çš„ä¸‰ç»´ç»“æ„ä¿¡æ¯ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œä¾‹å¦‚è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.

