---
layout: default
title: Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network
---

# Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network

**arXiv**: [2512.10498v1](https://arxiv.org/abs/2512.10498) | [PDF](https://arxiv.org/pdf/2512.10498.pdf)

**ä½œè€…**: Khurram Ashfaq, Muhammad Tariq Mahmood

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

**å¤‡æ³¨**: Accepted to IJCV

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå¤šå°ºåº¦æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯å’Œå¾ªçŽ¯ç½‘ç»œçš„ç¨³å¥Shape-from-Focusæ–¹æ³•**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `Shape-from-Focus` `æ·±åº¦ä¼°è®¡` `æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯` `å¾ªçŽ¯ç¥žç»ç½‘ç»œ` `å¤šå°ºåº¦å­¦ä¹ ` `æ·±åº¦å­¦ä¹ ` `å›¾åƒå¤„ç†` `ä¸‰ç»´é‡å»º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽæ·±åº¦å­¦ä¹ çš„SFFæ–¹æ³•ä¾èµ–å¤æ‚ç‰¹å¾æå–å’Œç®€å•èšåˆï¼Œæ˜“å¼•å…¥ä¼ªå½±å’Œå™ªå£°ã€‚
2. æå‡ºæ··åˆæ¡†æž¶ï¼Œåˆ©ç”¨æ‰‹å·¥DDLæ ¸æå–é²æ£’ç„¦ç‚¹ä½“ç§¯ï¼Œå†ç”¨è½»é‡çº§GRUç½‘ç»œè¿­ä»£ä¼˜åŒ–æ·±åº¦ã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®žæ•°æ®é›†ä¸Šå‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œæå‡äº†ç²¾åº¦å’Œæ³›åŒ–æ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

Shape-from-Focus (SFF) æ˜¯ä¸€ç§è¢«åŠ¨æ·±åº¦ä¼°è®¡æŠ€æœ¯ï¼Œé€šè¿‡åˆ†æžç„¦ç‚¹å †æ ˆä¸­çš„ç„¦ç‚¹å˜åŒ–æ¥æŽ¨æ–­åœºæ™¯æ·±åº¦ã€‚ç›®å‰åŸºäºŽæ·±åº¦å­¦ä¹ çš„SFFæ–¹æ³•é€šå¸¸åˆ†ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤æ‚çš„ç‰¹å¾ç¼–ç å™¨æå–ç„¦ç‚¹ä½“ç§¯ï¼ˆç„¦ç‚¹å †æ ˆä¸­æ¯ä¸ªåƒç´ çš„ç„¦ç‚¹å¯èƒ½æ€§è¡¨ç¤ºï¼‰ï¼›ç„¶åŽï¼Œé€šè¿‡ç®€å•çš„å•æ­¥èšåˆæŠ€æœ¯ä¼°è®¡æ·±åº¦ï¼Œè¿™é€šå¸¸ä¼šå¼•å…¥ä¼ªå½±å¹¶æ”¾å¤§æ·±åº¦å›¾ä¸­çš„å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ¡†æž¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼ ç»Ÿä¸Šä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯ (DDL) æ ¸è®¡ç®—å¤šå°ºåº¦ç„¦ç‚¹ä½“ç§¯ï¼Œè¿™äº›æ ¸æ•èŽ·è¿œè·ç¦»å’Œæ–¹å‘ç„¦ç‚¹å˜åŒ–ä»¥å½¢æˆç¨³å¥çš„ç„¦ç‚¹ä½“ç§¯ã€‚ç„¶åŽï¼Œè¿™äº›ç„¦ç‚¹ä½“ç§¯è¢«è¾“å…¥åˆ°è½»é‡çº§çš„ã€åŸºäºŽå¤šå°ºåº¦GRUçš„æ·±åº¦æå–æ¨¡å—ä¸­ï¼Œè¯¥æ¨¡å—ä»¥è¾ƒä½Žçš„åˆ†è¾¨çŽ‡è¿­ä»£åœ°ç»†åŒ–åˆå§‹æ·±åº¦ä¼°è®¡ï¼Œä»¥æé«˜è®¡ç®—æ•ˆçŽ‡ã€‚æœ€åŽï¼Œæˆ‘ä»¬å¾ªçŽ¯ç½‘ç»œä¸­å­¦ä¹ åˆ°çš„å‡¸ä¸Šé‡‡æ ·æ¨¡å—é‡å»ºé«˜åˆ†è¾¨çŽ‡æ·±åº¦å›¾ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„åœºæ™¯ç»†èŠ‚å’Œæ¸…æ™°çš„è¾¹ç•Œã€‚åœ¨åˆæˆå’ŒçœŸå®žä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºŽæœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨ä¸åŒçš„ç„¦ç‚¹æ¡ä»¶ä¸‹å®žçŽ°äº†å“è¶Šçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šShape-from-Focus (SFF)æ—¨åœ¨ä»Žä¸€ç³»åˆ—å…·æœ‰ä¸åŒç„¦ç‚¹çš„å›¾åƒä¸­æ¢å¤åœºæ™¯çš„æ·±åº¦ä¿¡æ¯ã€‚çŽ°æœ‰åŸºäºŽæ·±åº¦å­¦ä¹ çš„SFFæ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œå³é¦–å…ˆä½¿ç”¨å¤æ‚çš„ç‰¹å¾ç¼–ç å™¨æå–ç„¦ç‚¹ä½“ç§¯ï¼Œç„¶åŽä½¿ç”¨ç®€å•çš„èšåˆæŠ€æœ¯ä¼°è®¡æ·±åº¦ã€‚è¿™ç§æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽï¼Œå¤æ‚çš„ç‰¹å¾ç¼–ç å™¨è®¡ç®—é‡å¤§ï¼Œè€Œç®€å•çš„èšåˆæŠ€æœ¯å®¹æ˜“å¼•å…¥ä¼ªå½±ï¼Œå¹¶æ”¾å¤§æ·±åº¦å›¾ä¸­çš„å™ªå£°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆä¼ ç»Ÿæ–¹æ³•å’Œæ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿ã€‚ä¸€æ–¹é¢ï¼Œåˆ©ç”¨æ‰‹å·¥è®¾è®¡çš„æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯ (DDL) æ ¸æå–é²æ£’çš„ç„¦ç‚¹ä½“ç§¯ï¼Œä»¥å‡å°‘å¯¹å¤æ‚ç‰¹å¾ç¼–ç å™¨çš„ä¾èµ–ã€‚å¦ä¸€æ–¹é¢ï¼Œä½¿ç”¨è½»é‡çº§çš„ã€åŸºäºŽå¤šå°ºåº¦GRUçš„å¾ªçŽ¯ç½‘ç»œè¿­ä»£åœ°ç»†åŒ–æ·±åº¦ä¼°è®¡ï¼Œä»¥é¿å…ç®€å•èšåˆæŠ€æœ¯å¸¦æ¥çš„é—®é¢˜ã€‚è¿™ç§æ··åˆæ–¹æ³•æ—¨åœ¨æé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ–¹æ³•çš„æŠ€æœ¯æ¡†æž¶ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä½¿ç”¨DDLæ ¸è®¡ç®—å¤šå°ºåº¦ç„¦ç‚¹ä½“ç§¯ï¼›2) å°†ç„¦ç‚¹ä½“ç§¯è¾“å…¥åˆ°åŸºäºŽå¤šå°ºåº¦GRUçš„æ·±åº¦æå–æ¨¡å—ï¼›3) ä½¿ç”¨å­¦ä¹ åˆ°çš„å‡¸ä¸Šé‡‡æ ·æ¨¡å—é‡å»ºé«˜åˆ†è¾¨çŽ‡æ·±åº¦å›¾ã€‚æ·±åº¦æå–æ¨¡å—ä»¥è¾ƒä½Žçš„åˆ†è¾¨çŽ‡è¿­ä»£åœ°ç»†åŒ–åˆå§‹æ·±åº¦ä¼°è®¡ï¼Œä»¥æé«˜è®¡ç®—æ•ˆçŽ‡ã€‚å‡¸ä¸Šé‡‡æ ·æ¨¡å—ç”¨äºŽåœ¨é‡å»ºé«˜åˆ†è¾¨çŽ‡æ·±åº¦å›¾æ—¶ä¿ç•™ç²¾ç»†çš„åœºæ™¯ç»†èŠ‚å’Œæ¸…æ™°çš„è¾¹ç•Œã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæ··åˆæ¡†æž¶çš„è®¾è®¡ã€‚å®ƒç»“åˆäº†ä¼ ç»Ÿæ‰‹å·¥ç‰¹å¾æå–æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œé¿å…äº†å•ä¸€æ–¹æ³•çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨DDLæ ¸æå–é²æ£’çš„ç„¦ç‚¹ä½“ç§¯ï¼Œå‡å°‘äº†å¯¹å¤æ‚ç‰¹å¾ç¼–ç å™¨çš„ä¾èµ–ï¼ŒåŒæ—¶ä½¿ç”¨å¾ªçŽ¯ç½‘ç»œè¿­ä»£åœ°ç»†åŒ–æ·±åº¦ä¼°è®¡ï¼Œé¿å…äº†ç®€å•èšåˆæŠ€æœ¯å¸¦æ¥çš„é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šDDLæ ¸çš„è®¾è®¡è€ƒè™‘äº†é•¿è·ç¦»å’Œæ–¹å‘ä¸Šçš„ç„¦ç‚¹å˜åŒ–ï¼Œä»¥æé«˜ç„¦ç‚¹ä½“ç§¯çš„é²æ£’æ€§ã€‚å¤šå°ºåº¦GRUç½‘ç»œçš„è®¾è®¡å…è®¸åœ¨ä¸åŒåˆ†è¾¨çŽ‡ä¸Šè¿›è¡Œæ·±åº¦ä¼°è®¡ï¼Œä»¥æé«˜è®¡ç®—æ•ˆçŽ‡å’Œç²¾åº¦ã€‚å­¦ä¹ åˆ°çš„å‡¸ä¸Šé‡‡æ ·æ¨¡å—çš„è®¾è®¡æ—¨åœ¨ä¿ç•™é«˜åˆ†è¾¨çŽ‡æ·±åº¦å›¾ä¸­çš„ç²¾ç»†ç»†èŠ‚å’Œæ¸…æ™°è¾¹ç•Œã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æž„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®žä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºŽæœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†åœ¨ä¸åŒç„¦ç‚¹æ¡ä»¶ä¸‹å®žçŽ°äº†å“è¶Šçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽæœºå™¨äººå¯¼èˆªã€ä¸‰ç»´é‡å»ºã€æ˜¾å¾®æˆåƒç­‰é¢†åŸŸã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼ŒSFFæŠ€æœ¯å¯ä»¥å¸®åŠ©æœºå™¨äººæ„ŸçŸ¥å‘¨å›´çŽ¯å¢ƒçš„æ·±åº¦ä¿¡æ¯ï¼Œä»Žè€Œå®žçŽ°è‡ªä¸»å¯¼èˆªã€‚åœ¨ä¸‰ç»´é‡å»ºä¸­ï¼ŒSFFæŠ€æœ¯å¯ä»¥ç”¨äºŽé‡å»ºåœºæ™¯çš„ä¸‰ç»´æ¨¡åž‹ã€‚åœ¨æ˜¾å¾®æˆåƒä¸­ï¼ŒSFFæŠ€æœ¯å¯ä»¥ç”¨äºŽèŽ·å–æ ·æœ¬çš„ä¸‰ç»´ç»“æž„ä¿¡æ¯ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œä¾‹å¦‚è™šæ‹ŸçŽ°å®žã€å¢žå¼ºçŽ°å®žç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.

