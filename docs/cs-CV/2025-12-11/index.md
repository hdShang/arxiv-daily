---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-12-11
---

# cs.CVï¼ˆ2025-12-11ï¼‰

ğŸ“Š å…± **39** ç¯‡è®ºæ–‡
 | ğŸ”— **10** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (25 ğŸ”—8)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (25 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251210369v1-breaking-the-vicious-cycle-coherent-3d-gaussian-splatting-from-spars.html">Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views</a></td>
  <td>æå‡ºCoherentGSï¼Œè§£å†³ç¨€ç–å’Œè¿åŠ¨æ¨¡ç³Šè§†å›¾ä¸‹çš„é«˜ä¿çœŸ3Dé«˜æ–¯é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10369v1" onclick="toggleFavorite(this, '2512.10369v1', 'Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251210674v1-geo6dpose-fast-zero-shot-6d-object-pose-estimation-via-geometry-filt.html">Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching</a></td>
  <td>Geo6DPoseï¼šåŸºäºå‡ ä½•æ»¤æ³¢ç‰¹å¾åŒ¹é…çš„å¿«é€Ÿé›¶æ ·æœ¬6Dç‰©ä½“å§¿æ€ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">pose estimation</span> <span class="paper-tag">feature matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10674v1" onclick="toggleFavorite(this, '2512.10674v1', 'Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251210321v1-point2pose-a-generative-framework-for-3d-human-pose-estimation-with-.html">Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset</a></td>
  <td>Point2Poseï¼šæå‡ºä¸€ç§åŸºäºå¤šè§†è§’ç‚¹äº‘æ•°æ®é›†çš„3Däººä½“å§¿æ€ä¼°è®¡ç”Ÿæˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span> <span class="paper-tag">pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10321v1" onclick="toggleFavorite(this, '2512.10321v1', 'Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251210956v1-empowering-dynamic-urban-navigation-with-stereo-and-mid-level-vision.html">Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision</a></td>
  <td>StereoWalkerï¼šèåˆåŒç›®è§†è§‰ä¸ä¸­å±‚è§†è§‰å¢å¼ºåŠ¨æ€åŸå¸‚å¯¼èˆª</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">navigation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10956v1" onclick="toggleFavorite(this, '2512.10956v1', 'Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251211130v1-fast-foundationstereo-real-time-zero-shot-stereo-matching.html">Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching</a></td>
  <td>æå‡ºFast-FoundationStereoï¼Œå®ç°é›¶æ ·æœ¬ç«‹ä½“åŒ¹é…çš„å®æ—¶æ€§ä¸é«˜ç²¾åº¦ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">stereo matching</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.11130v1" onclick="toggleFavorite(this, '2512.11130v1', 'Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251210957v1-scenemaker-open-set-3d-scene-generation-with-decoupled-de-occlusion-.html">SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</a></td>
  <td>SceneMakerï¼šè§£è€¦å»é®æŒ¡ä¸å§¿æ€ä¼°è®¡çš„å¼€æ”¾åœºæ™¯ä¸‰ç»´ç”Ÿæˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10957v1" onclick="toggleFavorite(this, '2512.10957v1', 'SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251210939v1-gaussianheadtalk-wobble-free-3d-talking-heads-with-audio-driven-gaus.html">GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting</a></td>
  <td>æå‡ºGaussianHeadTalkï¼Œåˆ©ç”¨éŸ³é¢‘é©±åŠ¨é«˜æ–¯æº…å°„ç”Ÿæˆæ— æŠ–åŠ¨3Dè¯´è¯å¤´</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10939v1" onclick="toggleFavorite(this, '2512.10939v1', 'GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251210840v1-posegam-robust-unseen-object-pose-estimation-via-geometry-aware-mult.html">PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning</a></td>
  <td>PoseGAMï¼šåŸºäºå‡ ä½•æ„ŸçŸ¥å¤šè§†è§’æ¨ç†çš„é²æ£’æœªçŸ¥ç‰©ä½“å§¿æ€ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10840v1" onclick="toggleFavorite(this, '2512.10840v1', 'PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251210683v1-optimal-transport-unlocks-end-to-end-learning-for-single-molecule-lo.html">Optimal transport unlocks end-to-end learning for single-molecule localization</a></td>
  <td>åˆ©ç”¨æœ€ä¼˜ä¼ è¾“å®ç°å•åˆ†å­å®šä½æ˜¾å¾®é•œçš„ç«¯åˆ°ç«¯å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">localization</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10683v1" onclick="toggleFavorite(this, '2512.10683v1', 'Optimal transport unlocks end-to-end learning for single-molecule localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251210660v1-navihydra-controllable-navigation-guided-end-to-end-autonomous-drivi.html">NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation</a></td>
  <td>NaviHydraï¼šåŸºäºHydraè’¸é¦çš„å¯æ§å¯¼èˆªå¼•å¯¼ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶</td>
  <td class="tags-cell"><span class="paper-tag">navigation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10660v1" onclick="toggleFavorite(this, '2512.10660v1', 'NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251210386v1-adaptive-dual-weighted-gravitational-point-cloud-denoising-method.html">Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method</a></td>
  <td>æå‡ºè‡ªé€‚åº”åŒæƒé‡å¼•åŠ›ç‚¹äº‘å»å™ªæ–¹æ³•ï¼Œæå‡ç²¾åº¦ã€æ•ˆç‡ä¸è¾¹ç¼˜ä¿æŒèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10386v1" onclick="toggleFavorite(this, '2512.10386v1', 'Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251210376v1-raliflow-scene-flow-estimation-with-4d-radar-and-lidar-point-clouds.html">RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds</a></td>
  <td>æå‡ºRaLiFlowï¼Œé¦–ä¸ªåŸºäº4Dé›·è¾¾å’Œæ¿€å…‰é›·è¾¾ç‚¹äº‘çš„åœºæ™¯æµä¼°è®¡æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10376v1" onclick="toggleFavorite(this, '2512.10376v1', 'RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251210310v1-efficient-vln-a-training-efficient-vision-language-navigation-model.html">Efficient-VLN: A Training-Efficient Vision-Language Navigation Model</a></td>
  <td>Efficient-VLNï¼šä¸€ç§è®­ç»ƒé«˜æ•ˆçš„è§†è§‰-è¯­è¨€å¯¼èˆªæ¨¡å‹ï¼Œæ˜¾è‘—é™ä½è®­ç»ƒå¼€é”€ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">navigation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10310v1" onclick="toggleFavorite(this, '2512.10310v1', 'Efficient-VLN: A Training-Efficient Vision-Language Navigation Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251210293v1-physically-aware-360circ-view-generation-from-a-single-image-using-d.html">Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings</a></td>
  <td>æå‡ºDisentangled360ï¼Œé€šè¿‡è§£è€¦åœºæ™¯åµŒå…¥å®ç°å•å›¾360åº¦è§†å›¾ç”Ÿæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">NeRF</span> <span class="paper-tag">scene reconstruction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10293v1" onclick="toggleFavorite(this, '2512.10293v1', 'Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251210950v1-e-rayzer-self-supervised-3d-reconstruction-as-spatial-visual-pre-tra.html">E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</a></td>
  <td>E-RayZerï¼šæå‡ºè‡ªç›‘ç£3Dé‡å»ºæ¡†æ¶ï¼Œä½œä¸ºç©ºé—´è§†è§‰é¢„è®­ç»ƒæ¨¡å‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">pose estimation</span> <span class="paper-tag">VGGT</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10950v1" onclick="toggleFavorite(this, '2512.10950v1', 'E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251210437v1-an-m-health-algorithmic-approach-to-identify-and-assess-physiotherap.html">An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time</a></td>
  <td>æå‡ºä¸€ç§åŸºäºç§»åŠ¨è®¾å¤‡çš„M-Healthç®—æ³•ï¼Œç”¨äºå®æ—¶è¯†åˆ«å’Œè¯„ä¼°ç†ç–—è¿åŠ¨</td>
  <td class="tags-cell"><span class="paper-tag">pose estimation</span> <span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10437v1" onclick="toggleFavorite(this, '2512.10437v1', 'An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251210251v1-the-pose-topological-prior-with-hybrid-graph-fusion-for-estimating-c.html">THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose</a></td>
  <td>THE-Poseï¼šèåˆæ‹“æ‰‘å…ˆéªŒä¸æ··åˆå›¾çš„ç±»åˆ«çº§6Dä½å§¿ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span> <span class="paper-tag">pose estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10251v1" onclick="toggleFavorite(this, '2512.10251v1', 'THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251211925v1-floraforge-llm-assisted-procedural-generation-of-editable-and-analys.html">FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications</a></td>
  <td>FloraForgeï¼šLLMè¾…åŠ©ç”Ÿæˆå¯ç¼–è¾‘ã€åˆ†æå°±ç»ªçš„3Dæ¤ç‰©å‡ ä½•æ¨¡å‹ï¼Œç”¨äºå†œä¸šåº”ç”¨</td>
  <td class="tags-cell"><span class="paper-tag">point cloud</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.11925v1" onclick="toggleFavorite(this, '2512.11925v1', 'FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251210940v1-omniview-an-all-seeing-diffusion-model-for-3d-and-4d-view-synthesis.html">OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis</a></td>
  <td>OmniViewï¼šç”¨äº3Då’Œ4Dè§†å›¾åˆæˆçš„ç»Ÿä¸€æ‰©æ•£æ¨¡å‹</td>
  <td class="tags-cell"><span class="paper-tag">novel view synthesis</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10940v1" onclick="toggleFavorite(this, '2512.10940v1', 'OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251210935v1-any4d-unified-feed-forward-metric-4d-reconstruction.html">Any4D: Unified Feed-Forward Metric 4D Reconstruction</a></td>
  <td>Any4Dï¼šç»Ÿä¸€å‰é¦ˆå¼åº¦é‡4Dé‡å»ºæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">ego-motion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10935v1" onclick="toggleFavorite(this, '2512.10935v1', 'Any4D: Unified Feed-Forward Metric 4D Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/251210725v1-video-depth-propagation.html">Video Depth Propagation</a></td>
  <td>æå‡ºVeloDepthï¼Œé€šè¿‡æ—¶ç©ºå…ˆéªŒå’Œç‰¹å¾ä¼ æ’­å®ç°é«˜æ•ˆé²æ£’çš„è§†é¢‘æ·±åº¦ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10725v1" onclick="toggleFavorite(this, '2512.10725v1', 'Video Depth Propagation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251210498v1-robust-shape-from-focus-via-multiscale-directional-dilated-laplacian.html">Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network</a></td>
  <td>æå‡ºåŸºäºå¤šå°ºåº¦æ–¹å‘æ‰©å¼ æ‹‰æ™®æ‹‰æ–¯å’Œå¾ªç¯ç½‘ç»œçš„ç¨³å¥Shape-from-Focusæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10498v1" onclick="toggleFavorite(this, '2512.10498v1', 'Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251210450v1-error-propagation-free-learned-video-compression-with-dual-domain-pr.html">Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment</a></td>
  <td>æå‡ºåŒåŸŸæ¸è¿›å¼æ—¶åºå¯¹é½çš„æ— è¯¯å·®ä¼ æ’­å­¦ä¹ è§†é¢‘å‹ç¼©æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">optical flow</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10450v1" onclick="toggleFavorite(this, '2512.10450v1', 'Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/251210342v1-cosplan-corrective-sequential-planning-via-scene-graph-incremental-u.html">CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates</a></td>
  <td>æå‡ºåŸºäºåœºæ™¯å›¾å¢é‡æ›´æ–°çš„çº é”™åºåˆ—è§„åˆ’æ–¹æ³•CoSPlanï¼Œæå‡VLMåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">navigation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10342v1" onclick="toggleFavorite(this, '2512.10342v1', 'CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>25</td>
  <td><a href="./papers/251210267v1-long-lrm-preserving-fine-details-in-feed-forward-wide-coverage-recon.html">Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction</a></td>
  <td>Long-LRM++ï¼šç»“åˆåŠæ˜¾å¼è¡¨è¾¾ä¸è½»é‡è§£ç å™¨ï¼Œå®ç°é«˜è´¨é‡ã€å®æ—¶çš„å®½è¦†ç›–åœºæ™¯é‡å»ºã€‚</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10267v1" onclick="toggleFavorite(this, '2512.10267v1', 'Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>26</td>
  <td><a href="./papers/251210419v1-translocnet-cross-modal-attention-for-aerial-ground-vehicle-localiza.html">TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning</a></td>
  <td>TransLocNetï¼šåŸºäºè·¨æ¨¡æ€æ³¨æ„åŠ›å’Œå¯¹æ¯”å­¦ä¹ çš„æ— äººæœº-åœ°é¢è½¦è¾†å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span> <span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10419v1" onclick="toggleFavorite(this, '2512.10419v1', 'TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>27</td>
  <td><a href="./papers/251210353v1-hybrid-transformer-mamba-architecture-for-weakly-supervised-volumetr.html">Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation</a></td>
  <td>æå‡ºTranSambaï¼Œä¸€ç§æ··åˆTransformer-Mambaæ¶æ„ï¼Œç”¨äºå¼±ç›‘ç£ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span> <span class="paper-tag">state space model</span> <span class="paper-tag">localization</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10353v1" onclick="toggleFavorite(this, '2512.10353v1', 'Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/251211057v1-weakly-supervised-tuberculosis-localization-in-chest-x-rays-through-.html">Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation</a></td>
  <td>åˆ©ç”¨çŸ¥è¯†è’¸é¦çš„èƒ¸éƒ¨Xå…‰ç‰‡è‚ºç»“æ ¸å¼±ç›‘ç£å®šä½æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">teacher-student</span> <span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.11057v1" onclick="toggleFavorite(this, '2512.11057v1', 'Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>29</td>
  <td><a href="./papers/251210958v1-worldlens-full-spectrum-evaluations-of-driving-world-models-in-real-.html">WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</a></td>
  <td>WorldLensï¼šçœŸå®ä¸–ç•Œä¸­é©¾é©¶ä¸–ç•Œæ¨¡å‹çš„å…¨æ–¹ä½è¯„ä¼°åŸºå‡†</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10958v1" onclick="toggleFavorite(this, '2512.10958v1', 'WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>30</td>
  <td><a href="./papers/251211061v1-vdaworld-world-modelling-via-vlm-directed-abstraction-and-simulation.html">VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation</a></td>
  <td>VDAWorldï¼šæå‡ºåŸºäºVLMå¼•å¯¼çš„æŠ½è±¡ä¸æ¨¡æ‹Ÿçš„ä¸–ç•Œå»ºæ¨¡æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">world model</span> <span class="paper-tag">latent dynamics</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.11061v1" onclick="toggleFavorite(this, '2512.11061v1', 'VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>31</td>
  <td><a href="./papers/251210226v1-latent-chain-of-thought-world-modeling-for-end-to-end-driving.html">Latent Chain-of-Thought World Modeling for End-to-End Driving</a></td>
  <td>æå‡ºLatent-CoT-Driveï¼Œåˆ©ç”¨éšç©ºé—´æ€ç»´é“¾è¿›è¡Œç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶å†³ç­–ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10226v1" onclick="toggleFavorite(this, '2512.10226v1', 'Latent Chain-of-Thought World Modeling for End-to-End Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>32</td>
  <td><a href="./papers/251210554v1-grounding-everything-in-tokens-for-multimodal-large-language-models.html">Grounding Everything in Tokens for Multimodal Large Language Models</a></td>
  <td>GETokï¼šé€šè¿‡tokenåŒ–å®ç°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç²¾ç¡®2Dç©ºé—´å®šä½</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10554v1" onclick="toggleFavorite(this, '2512.10554v1', 'Grounding Everything in Tokens for Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>33</td>
  <td><a href="./papers/251210668v1-xden-1k-a-density-field-dataset-of-real-world-objects.html">XDen-1K: A Density Field Dataset of Real-World Objects</a></td>
  <td>XDen-1Kï¼šé¦–ä¸ªå¤§è§„æ¨¡çœŸå®ç‰©ä½“å¯†åº¦åœºæ•°æ®é›†ï¼ŒåŠ©åŠ›æœºå™¨äººæ“ä½œå’Œç‰©ç†æ¨¡æ‹Ÿã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10668v1" onclick="toggleFavorite(this, '2512.10668v1', 'XDen-1K: A Density Field Dataset of Real-World Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>34</td>
  <td><a href="./papers/251210248v1-robustsora-de-watermarked-benchmark-for-robust-ai-generated-video-de.html">RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection</a></td>
  <td>RobustSoraï¼šæå‡ºå»æ°´å°åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°AIç”Ÿæˆè§†é¢‘æ£€æµ‹çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10248v1" onclick="toggleFavorite(this, '2512.10248v1', 'RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>35</td>
  <td><a href="./papers/251210209v1-feature-coding-for-scalable-machine-vision.html">Feature Coding for Scalable Machine Vision</a></td>
  <td>æå‡ºFCTMï¼Œé€šè¿‡ç‰¹å¾ç¼–ç æ˜¾è‘—é™ä½æœºå™¨è§†è§‰è¾¹ç¼˜éƒ¨ç½²çš„å¸¦å®½éœ€æ±‚ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">running</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10209v1" onclick="toggleFavorite(this, '2512.10209v1', 'Feature Coding for Scalable Machine Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>36</td>
  <td><a href="./papers/251210730v1-irg-motionllm-interleaving-motion-generation-assessment-and-refineme.html">IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation</a></td>
  <td>æå‡ºIRG-MotionLLMï¼Œé€šè¿‡äº¤é”™è¿åŠ¨ç”Ÿæˆã€è¯„ä¼°å’Œä¼˜åŒ–ï¼Œæå‡æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ•ˆæœ</td>
  <td class="tags-cell"><span class="paper-tag">text-to-motion</span> <span class="paper-tag">motion generation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10730v1" onclick="toggleFavorite(this, '2512.10730v1', 'IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>37</td>
  <td><a href="./papers/251210352v1-topology-agnostic-animal-motion-generation-from-text-prompt.html">Topology-Agnostic Animal Motion Generation from Text Prompt</a></td>
  <td>æå‡ºOmniZooæ•°æ®é›†å’Œæ‹“æ‰‘æ— å…³çš„åŠ¨ç‰©è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œè§£å†³å¼‚æ„éª¨éª¼å’Œæ–‡æœ¬é©±åŠ¨çš„åŠ¨ç‰©è¿åŠ¨ç”Ÿæˆé—®é¢˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">text-driven motion</span> <span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10352v1" onclick="toggleFavorite(this, '2512.10352v1', 'Topology-Agnostic Animal Motion Generation from Text Prompt')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>38</td>
  <td><a href="./papers/251210959v1-stereospace-depth-free-synthesis-of-stereo-geometry-via-end-to-end-d.html">StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space</a></td>
  <td>StereoSpaceï¼šæå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æ·±åº¦å•ç›®å›¾åƒåˆ°ç«‹ä½“å›¾åƒç”Ÿæˆæ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">geometric consistency</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10959v1" onclick="toggleFavorite(this, '2512.10959v1', 'StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>39</td>
  <td><a href="./papers/251210517v1-3d-blood-pulsation-maps.html">3D Blood Pulsation Maps</a></td>
  <td>æå‡ºPulse3DFaceæ•°æ®é›†ä»¥è§£å†³3Dè¡€æ¶²è„‰åŠ¨æ˜ å°„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">PULSE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.10517v1" onclick="toggleFavorite(this, '2512.10517v1', '3D Blood Pulsation Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)