---
layout: default
title: Revisiting Multimodal Positional Encoding in Vision-Language Models
---

# Revisiting Multimodal Positional Encoding in Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23095" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23095v2</a>
  <a href="https://arxiv.org/pdf/2510.23095.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23095v2" onclick="toggleFavorite(this, '2510.23095v2', 'Revisiting Multimodal Positional Encoding in Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jie Huang, Xuejing Liu, Sibo Song, Ruibing Hou, Hong Chang, Junyang Lin, Shuai Bai

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27 (æ›´æ–°: 2025-11-05)

**å¤‡æ³¨**: 16 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/JJJYmmm/Multimodal-RoPEs)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šå¤´æ—‹è½¬ä½ç½®ç¼–ç MHRoPEåŠå…¶å˜ä½“MRoPE-Iï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ä½ç½®ç¼–ç èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ä½ç½®ç¼–ç ` `æ—‹è½¬ä½ç½®åµŒå…¥` `Transformer` `å¤šå¤´æ³¨æ„åŠ›` `è·¨æ¨¡æ€ç†è§£`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä½ç½®ç¼–ç æ–¹é¢ç¼ºä¹ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚
2. è®ºæ–‡é€šè¿‡åˆ†æRoPEçš„ä¸¤ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œæå‡ºäº†MHRoPEå’ŒMRoPE-Iä¸¤ç§ä½ç½®ç¼–ç å˜ä½“ã€‚
3. å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€ä½ç½®ç¼–ç å¯¹äºè§†è§‰-è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†å¯¹å…¶çš„ç³»ç»Ÿæ€§ç ”ç©¶è¿˜ä¸è¶³ã€‚æœ¬æ–‡å¯¹å¤šæ¨¡æ€æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œè€ƒå¯Ÿäº†å…¶ä¸¤ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼šä½ç½®è®¾è®¡å’Œé¢‘ç‡åˆ†é…ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬æ€»ç»“äº†ä¸‰ä¸ªå…³é”®æŒ‡å¯¼åŸåˆ™ï¼šä½ç½®ä¸€è‡´æ€§ã€å……åˆ†çš„é¢‘ç‡åˆ©ç”¨å’Œä¿ç•™æ–‡æœ¬å…ˆéªŒâ€”â€”ç¡®ä¿æ˜ç¡®çš„å¸ƒå±€ã€ä¸°å¯Œçš„è¡¨ç¤ºä»¥åŠä»é¢„è®­ç»ƒLLMçš„å¿ å®è¿ç§»ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå¤´RoPEï¼ˆMHRoPEï¼‰å’ŒMRoPE-Interleaveï¼ˆMRoPE-Iï¼‰ï¼Œè¿™ä¸¤ç§ç®€å•ä¸”å³æ’å³ç”¨çš„å˜ä½“ï¼Œæ— éœ€æ¶æ„æ›´æ”¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨é€šç”¨å’Œç»†ç²’åº¦çš„å¤šæ¨¡æ€ç†è§£æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„å¤šæ¨¡æ€ä½ç½®ç¼–ç æ—¨åœ¨å°†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„ä½ç½®å…³ç³»èå…¥æ¨¡å‹ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ä½ç½®è®¾è®¡å’Œé¢‘ç‡åˆ†é…çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ç†è§£å¤æ‚å¤šæ¨¡æ€åœºæ™¯æ—¶æ€§èƒ½å—é™ã€‚ç°æœ‰æ–¹æ³•å¯èƒ½æ— æ³•ä¿è¯ä½ç½®ä¿¡æ¯çš„ä¸€è‡´æ€§ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨é¢‘ç‡ä¿¡æ¯è¿›è¡Œè¡¨ç¤ºï¼Œä¹Ÿå¯èƒ½æ— æ³•å¾ˆå¥½åœ°ä¿ç•™é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–‡æœ¬å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ·±å…¥åˆ†ææ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰çš„ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šä½ç½®è®¾è®¡å’Œé¢‘ç‡åˆ†é…ï¼Œä»è€Œæ‰¾åˆ°æå‡å¤šæ¨¡æ€ä½ç½®ç¼–ç æ€§èƒ½çš„å…³é”®å› ç´ ã€‚é€šè¿‡å®éªŒåˆ†æï¼Œè®ºæ–‡æ€»ç»“äº†ä¸‰ä¸ªæŒ‡å¯¼åŸåˆ™ï¼šä½ç½®ä¸€è‡´æ€§ã€å……åˆ†çš„é¢‘ç‡åˆ©ç”¨å’Œä¿ç•™æ–‡æœ¬å…ˆéªŒã€‚åŸºäºè¿™äº›åŸåˆ™ï¼Œè®ºæ–‡è®¾è®¡äº†æ–°çš„å¤šå¤´æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆMHRoPEï¼‰åŠå…¶å˜ä½“ï¼ˆMRoPE-Iï¼‰ï¼Œæ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„æ–¹æ³•æ˜¯å³æ’å³ç”¨çš„ï¼Œä¸éœ€è¦å¯¹ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹æ¶æ„è¿›è¡Œå¤§çš„æ”¹åŠ¨ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œå¯¹è¾“å…¥çš„å¤šæ¨¡æ€æ•°æ®ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰è¿›è¡Œç‰¹å¾æå–ï¼›ç„¶åï¼Œä½¿ç”¨MHRoPEæˆ–MRoPE-Iå¯¹æå–çš„ç‰¹å¾è¿›è¡Œä½ç½®ç¼–ç ï¼›æœ€åï¼Œå°†ç¼–ç åçš„ç‰¹å¾è¾“å…¥åˆ°æ¨¡å‹çš„åç»­æ¨¡å—è¿›è¡Œå¤„ç†ï¼Œä¾‹å¦‚Transformerå±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†MHRoPEå’ŒMRoPE-Iä¸¤ç§æ–°çš„å¤šæ¨¡æ€ä½ç½®ç¼–ç æ–¹æ³•ï¼Œå®ƒä»¬åœ¨RoPEçš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œæ›´å¥½åœ°æ»¡è¶³äº†ä½ç½®ä¸€è‡´æ€§ã€å……åˆ†çš„é¢‘ç‡åˆ©ç”¨å’Œä¿ç•™æ–‡æœ¬å…ˆéªŒè¿™ä¸‰ä¸ªæŒ‡å¯¼åŸåˆ™ã€‚MHRoPEé€šè¿‡å¤šå¤´æœºåˆ¶æ¥å­¦ä¹ ä¸åŒçš„ä½ç½®è¡¨ç¤ºï¼Œè€ŒMRoPE-Iåˆ™é€šè¿‡äº¤é”™çš„æ–¹å¼æ¥èåˆä¸åŒæ¨¡æ€çš„ä½ç½®ä¿¡æ¯ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å¤šæ¨¡æ€æ•°æ®ä¸­çš„ä½ç½®å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šMHRoPEçš„å…³é”®è®¾è®¡åœ¨äºä½¿ç”¨å¤šå¤´æœºåˆ¶ï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„æ—‹è½¬è§’åº¦ï¼Œä»è€Œæ•æ‰ä¸åŒçš„ä½ç½®å…³ç³»ã€‚MRoPE-Içš„å…³é”®è®¾è®¡åœ¨äºå°†è§†è§‰å’Œæ–‡æœ¬çš„ä½ç½®ç¼–ç äº¤é”™æ’åˆ—ï¼Œä»è€Œæ›´å¥½åœ°èåˆä¸¤ç§æ¨¡æ€çš„ä¿¡æ¯ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚éœ€è¦åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†æŸ¥æ‰¾ï¼Œä¾‹å¦‚å¤´çš„æ•°é‡ã€æ—‹è½¬è§’åº¦çš„è®¡ç®—æ–¹å¼ç­‰ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±æˆ–è€…å…¶ä»–é’ˆå¯¹å¤šæ¨¡æ€ä»»åŠ¡è®¾è®¡çš„æŸå¤±å‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMHRoPEå’ŒMRoPE-Iåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨XXXæ•°æ®é›†ä¸Šï¼ŒMHRoPEçš„æ€§èƒ½æå‡äº†X%ï¼ŒMRoPE-Içš„æ€§èƒ½æå‡äº†Y%ã€‚è¿™äº›ç»“æœè¯æ˜äº†è®ºæ–‡æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç»†ç²’åº¦çš„å¤šæ¨¡æ€ç†è§£æ–¹é¢ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€æ£€ç´¢å’Œè§†è§‰æ¨ç†ã€‚é€šè¿‡æå‡æ¨¡å‹å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ”¹å–„äººæœºäº¤äº’ä½“éªŒï¼Œæé«˜è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ™ºèƒ½å®¢æœã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs.

