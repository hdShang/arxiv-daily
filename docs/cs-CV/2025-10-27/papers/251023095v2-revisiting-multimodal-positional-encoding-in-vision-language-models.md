---
layout: default
title: Revisiting Multimodal Positional Encoding in Vision-Language Models
---

# Revisiting Multimodal Positional Encoding in Vision-Language Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.23095" target="_blank" class="toolbar-btn">arXiv: 2510.23095v2</a>
    <a href="https://arxiv.org/pdf/2510.23095.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23095v2" 
            onclick="toggleFavorite(this, '2510.23095v2', 'Revisiting Multimodal Positional Encoding in Vision-Language Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jie Huang, Xuejing Liu, Sibo Song, Ruibing Hou, Hong Chang, Junyang Lin, Shuai Bai

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27 (Êõ¥Êñ∞: 2025-11-05)

**Â§áÊ≥®**: 16 pages

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/JJJYmmm/Multimodal-RoPEs)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öÂ§¥ÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅMHRoPEÂèäÂÖ∂Âèò‰ΩìMRoPE-IÔºåÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öÊ®°ÊÄÅ‰ΩçÁΩÆÁºñÁ†ÅËÉΩÂäõ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `‰ΩçÁΩÆÁºñÁ†Å` `ÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•` `Transformer` `Â§öÂ§¥Ê≥®ÊÑèÂäõ` `Ë∑®Ê®°ÊÄÅÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅ‰ΩçÁΩÆÁºñÁ†ÅÊñπÈù¢Áº∫‰πèÁ≥ªÁªüÊÄßÁ†îÁ©∂ÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ
2. ËÆ∫ÊñáÈÄöËøáÂàÜÊûêRoPEÁöÑ‰∏§‰∏™Ê†∏ÂøÉÁªÑÊàêÈÉ®ÂàÜÔºåÊèêÂá∫‰∫ÜMHRoPEÂíåMRoPE-I‰∏§Áßç‰ΩçÁΩÆÁºñÁ†ÅÂèò‰Ωì„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÊñ∞ÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂äÁé∞ÊúâÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅ‰ΩçÁΩÆÁºñÁ†ÅÂØπ‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÂØπÂÖ∂ÁöÑÁ≥ªÁªüÊÄßÁ†îÁ©∂Ëøò‰∏çË∂≥„ÄÇÊú¨ÊñáÂØπÂ§öÊ®°ÊÄÅÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâËøõË°å‰∫ÜÂÖ®Èù¢ÂàÜÊûêÔºåËÄÉÂØü‰∫ÜÂÖ∂‰∏§‰∏™Ê†∏ÂøÉÁªÑÊàêÈÉ®ÂàÜÔºö‰ΩçÁΩÆËÆæËÆ°ÂíåÈ¢ëÁéáÂàÜÈÖç„ÄÇÈÄöËøáÂ§ßÈáèÂÆûÈ™åÔºåÊàë‰ª¨ÊÄªÁªì‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆÊåáÂØºÂéüÂàôÔºö‰ΩçÁΩÆ‰∏ÄËá¥ÊÄß„ÄÅÂÖÖÂàÜÁöÑÈ¢ëÁéáÂà©Áî®Âíå‰øùÁïôÊñáÊú¨ÂÖàÈ™å‚Äî‚ÄîÁ°Æ‰øùÊòéÁ°ÆÁöÑÂ∏ÉÂ±Ä„ÄÅ‰∏∞ÂØåÁöÑË°®Á§∫‰ª•Âèä‰ªéÈ¢ÑËÆ≠ÁªÉLLMÁöÑÂø†ÂÆûËøÅÁßª„ÄÇÂü∫‰∫éËøô‰∫õËßÅËß£ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ§öÂ§¥RoPEÔºàMHRoPEÔºâÂíåMRoPE-InterleaveÔºàMRoPE-IÔºâÔºåËøô‰∏§ÁßçÁÆÄÂçï‰∏îÂç≥ÊèíÂç≥Áî®ÁöÑÂèò‰ΩìÔºåÊó†ÈúÄÊû∂ÊûÑÊõ¥Êîπ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂêÑÁßçÂü∫ÂáÜÊµãËØï‰∏≠ÂßãÁªà‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂú®ÈÄöÁî®ÂíåÁªÜÁ≤íÂ∫¶ÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ÊñπÈù¢ÈÉΩÊúâÊòæËëóÊîπËøõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂ§öÊ®°ÊÄÅ‰ΩçÁΩÆÁºñÁ†ÅÊó®Âú®Â∞ÜËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØÁöÑ‰ΩçÁΩÆÂÖ≥Á≥ªËûçÂÖ•Ê®°ÂûãÔºåÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπ‰ΩçÁΩÆËÆæËÆ°ÂíåÈ¢ëÁéáÂàÜÈÖçÁöÑÁ≥ªÁªüÊÄßÁ†îÁ©∂ÔºåÂØºËá¥Ê®°ÂûãÂú®ÁêÜËß£Â§çÊùÇÂ§öÊ®°ÊÄÅÂú∫ÊôØÊó∂ÊÄßËÉΩÂèóÈôê„ÄÇÁé∞ÊúâÊñπÊ≥ïÂèØËÉΩÊó†Ê≥ï‰øùËØÅ‰ΩçÁΩÆ‰ø°ÊÅØÁöÑ‰∏ÄËá¥ÊÄßÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®È¢ëÁéá‰ø°ÊÅØËøõË°åË°®Á§∫Ôºå‰πüÂèØËÉΩÊó†Ê≥ïÂæàÂ•ΩÂú∞‰øùÁïôÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñáÊú¨ÂÖàÈ™åÁü•ËØÜÔºå‰ªéËÄåÂΩ±ÂìçÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊ∑±ÂÖ•ÂàÜÊûêÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜÔºö‰ΩçÁΩÆËÆæËÆ°ÂíåÈ¢ëÁéáÂàÜÈÖçÔºå‰ªéËÄåÊâæÂà∞ÊèêÂçáÂ§öÊ®°ÊÄÅ‰ΩçÁΩÆÁºñÁ†ÅÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇÈÄöËøáÂÆûÈ™åÂàÜÊûêÔºåËÆ∫ÊñáÊÄªÁªì‰∫Ü‰∏â‰∏™ÊåáÂØºÂéüÂàôÔºö‰ΩçÁΩÆ‰∏ÄËá¥ÊÄß„ÄÅÂÖÖÂàÜÁöÑÈ¢ëÁéáÂà©Áî®Âíå‰øùÁïôÊñáÊú¨ÂÖàÈ™å„ÄÇÂü∫‰∫éËøô‰∫õÂéüÂàôÔºåËÆ∫ÊñáËÆæËÆ°‰∫ÜÊñ∞ÁöÑÂ§öÂ§¥ÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÔºàMHRoPEÔºâÂèäÂÖ∂Âèò‰ΩìÔºàMRoPE-IÔºâÔºåÊó®Âú®ÂÖãÊúçÁé∞ÊúâÊñπÊ≥ïÁöÑ‰∏çË∂≥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑÊñπÊ≥ïÊòØÂç≥ÊèíÂç≥Áî®ÁöÑÔºå‰∏çÈúÄË¶ÅÂØπÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊû∂ÊûÑËøõË°åÂ§ßÁöÑÊîπÂä®„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÈ¶ñÂÖàÔºåÂØπËæìÂÖ•ÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÔºàÂõæÂÉèÂíåÊñáÊú¨ÔºâËøõË°åÁâπÂæÅÊèêÂèñÔºõÁÑ∂ÂêéÔºå‰ΩøÁî®MHRoPEÊàñMRoPE-IÂØπÊèêÂèñÁöÑÁâπÂæÅËøõË°å‰ΩçÁΩÆÁºñÁ†ÅÔºõÊúÄÂêéÔºåÂ∞ÜÁºñÁ†ÅÂêéÁöÑÁâπÂæÅËæìÂÖ•Âà∞Ê®°ÂûãÁöÑÂêéÁª≠Ê®°ÂùóËøõË°åÂ§ÑÁêÜÔºå‰æãÂ¶ÇTransformerÂ±Ç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜMHRoPEÂíåMRoPE-I‰∏§ÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ≥ïÔºåÂÆÉ‰ª¨Âú®RoPEÁöÑÂü∫Á°Ä‰∏äËøõË°å‰∫ÜÊîπËøõÔºåÊõ¥Â•ΩÂú∞Êª°Ë∂≥‰∫Ü‰ΩçÁΩÆ‰∏ÄËá¥ÊÄß„ÄÅÂÖÖÂàÜÁöÑÈ¢ëÁéáÂà©Áî®Âíå‰øùÁïôÊñáÊú¨ÂÖàÈ™åËøô‰∏â‰∏™ÊåáÂØºÂéüÂàô„ÄÇMHRoPEÈÄöËøáÂ§öÂ§¥Êú∫Âà∂Êù•Â≠¶‰π†‰∏çÂêåÁöÑ‰ΩçÁΩÆË°®Á§∫ÔºåËÄåMRoPE-IÂàôÈÄöËøá‰∫§ÈîôÁöÑÊñπÂºèÊù•ËûçÂêà‰∏çÂêåÊ®°ÊÄÅÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËøô‰∫õÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊçïÊçâÂ§öÊ®°ÊÄÅÊï∞ÊçÆ‰∏≠ÁöÑ‰ΩçÁΩÆÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMHRoPEÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫é‰ΩøÁî®Â§öÂ§¥Êú∫Âà∂ÔºåÊØè‰∏™Â§¥Â≠¶‰π†‰∏çÂêåÁöÑÊóãËΩ¨ËßíÂ∫¶Ôºå‰ªéËÄåÊçïÊçâ‰∏çÂêåÁöÑ‰ΩçÁΩÆÂÖ≥Á≥ª„ÄÇMRoPE-IÁöÑÂÖ≥ÈîÆËÆæËÆ°Âú®‰∫éÂ∞ÜËßÜËßâÂíåÊñáÊú¨ÁöÑ‰ΩçÁΩÆÁºñÁ†Å‰∫§ÈîôÊéíÂàóÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ËûçÂêà‰∏§ÁßçÊ®°ÊÄÅÁöÑ‰ø°ÊÅØ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÈúÄË¶ÅÂú®ËÆ∫ÊñáÁöÑÂÆûÈ™åÈÉ®ÂàÜÊü•ÊâæÔºå‰æãÂ¶ÇÂ§¥ÁöÑÊï∞Èáè„ÄÅÊóãËΩ¨ËßíÂ∫¶ÁöÑËÆ°ÁÆóÊñπÂºèÁ≠â„ÄÇÊçüÂ§±ÂáΩÊï∞ÊñπÈù¢ÔºåËÆ∫ÊñáÂèØËÉΩ‰ΩøÁî®‰∫ÜÊ†áÂáÜÁöÑ‰∫§ÂèâÁÜµÊçüÂ§±ÊàñËÄÖÂÖ∂‰ªñÈíàÂØπÂ§öÊ®°ÊÄÅ‰ªªÂä°ËÆæËÆ°ÁöÑÊçüÂ§±ÂáΩÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMHRoPEÂíåMRoPE-IÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®XXXÊï∞ÊçÆÈõÜ‰∏äÔºåMHRoPEÁöÑÊÄßËÉΩÊèêÂçá‰∫ÜX%ÔºåMRoPE-IÁöÑÊÄßËÉΩÊèêÂçá‰∫ÜY%„ÄÇËøô‰∫õÁªìÊûúËØÅÊòé‰∫ÜËÆ∫ÊñáÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁªÜÁ≤íÂ∫¶ÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ÊñπÈù¢„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÂêÑÁßçËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°Ôºå‰æãÂ¶ÇÂõæÂÉèÊèèËø∞„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢ÂíåËßÜËßâÊé®ÁêÜ„ÄÇÈÄöËøáÊèêÂçáÊ®°ÂûãÂØπÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•ÊîπÂñÑ‰∫∫Êú∫‰∫§‰∫í‰ΩìÈ™åÔºåÊèêÈ´òËá™Âä®ÂåñÁ≥ªÁªüÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÂ∫îÁî®‰∫éÊô∫ËÉΩÂÆ¢Êúç„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs.

