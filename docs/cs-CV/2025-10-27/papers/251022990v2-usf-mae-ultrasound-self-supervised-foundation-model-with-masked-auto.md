---
layout: default
title: USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding
---

# USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.22990" target="_blank" class="toolbar-btn">arXiv: 2510.22990v2</a>
    <a href="https://arxiv.org/pdf/2510.22990.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22990v2" 
            onclick="toggleFavorite(this, '2510.22990v2', 'USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Youssef Megahed, Robin Ducharme, Aylin Erman, Mark Walker, Steven Hawken, Adrian D. C. Chan

**ÂàÜÁ±ª**: eess.IV, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27 (Êõ¥Êñ∞: 2025-11-07)

**Â§áÊ≥®**: 18 pages, 8 figures, 2 tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**USF-MAEÔºöÂü∫‰∫éÊé©Á†ÅËá™ÁºñÁ†ÅÂô®ÁöÑË∂ÖÂ£∞Ëá™ÁõëÁù£È¢ÑËÆ≠ÁªÉÊ®°Âûã**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ë∂ÖÂ£∞ÂõæÂÉè` `Ëá™ÁõëÁù£Â≠¶‰π†` `Êé©Á†ÅËá™ÁºñÁ†ÅÂô®` `Vision Transformer` `È¢ÑËÆ≠ÁªÉÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÂèóÈôê‰∫éÁº∫‰πèÂ§ßËßÑÊ®°Ê†áÊ≥®Ë∂ÖÂ£∞Êï∞ÊçÆÈõÜÔºå‰∏îÈÄöÁî®ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂú®Ë∂ÖÂ£∞ÂõæÂÉè‰∏äÁöÑËøÅÁßªËÉΩÂäõÊúâÈôê„ÄÇ
2. USF-MAEÈÄöËøáÊé©Á†ÅËá™ÁºñÁ†ÅÂô®Âú®Â§ßÈáèÊó†Ê†áÊ≥®Ë∂ÖÂ£∞Êï∞ÊçÆ‰∏äËøõË°åËá™ÁõëÁù£È¢ÑËÆ≠ÁªÉÔºåÂ≠¶‰π†ÁâπÂÆö‰∫éË∂ÖÂ£∞Ê®°ÊÄÅÁöÑÁâπÂæÅË°®Á§∫„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåUSF-MAEÂú®Â§ö‰∏™Ë∂ÖÂ£∞ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠ÊòæËëó‰ºò‰∫é‰º†ÁªüCNNÂíåViTÊ®°ÂûãÔºåÂπ∂Â±ïÁé∞Âá∫ËâØÂ•ΩÁöÑË∑®Ëß£ÂâñÂå∫ÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫ÜË∂ÖÂ£∞Ëá™ÁõëÁù£Âü∫Á°ÄÊ®°ÂûãUSF-MAEÔºåËøôÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑËá™ÁõëÁù£Êé©Á†ÅËá™ÁºñÁ†ÅÊ°ÜÊû∂Ôºå‰∏ìÈó®Áî®‰∫éË∂ÖÂ£∞Êï∞ÊçÆ„ÄÇËØ•Ê®°ÂûãÂú®ÂåÖÂê´Êù•Ëá™46‰∏™ÂºÄÊ∫êÊï∞ÊçÆÈõÜÁöÑ37‰∏áÂº†2DÂíå3DË∂ÖÂ£∞ÂõæÂÉèÔºàOpenUS-46Ôºâ‰∏äËøõË°å‰∫ÜÈ¢ÑËÆ≠ÁªÉÔºåÊ∂µÁõñ‰∫Ü20Â§ö‰∏™Ëß£ÂâñÂå∫Âüü„ÄÇOpenUS-46Êï∞ÊçÆÈõÜÂ∑≤ÂÖ¨ÂºÄÔºå‰ª•‰øÉËøõËøõ‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂ÂíåÂèØÈáçÂ§çÊÄß„ÄÇUSF-MAEÈááÁî®Vision TransformerÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®Êû∂ÊûÑÔºåÈÄöËøáÈáçÂª∫Ë¢´Êé©ÁõñÁöÑÂõæÂÉèÂùóÔºåÁõ¥Êé•‰ªéÊó†Ê†áÁ≠æÊï∞ÊçÆ‰∏≠Â≠¶‰π†‰∏∞ÂØåÁöÑ„ÄÅÁâπÂÆö‰∫éÊ®°ÊÄÅÁöÑË°®Á§∫„ÄÇÈ¢ÑËÆ≠ÁªÉÁöÑÁºñÁ†ÅÂô®Âú®‰∏â‰∏™ÂÖ¨ÂÖ±‰∏ãÊ∏∏ÂàÜÁ±ªÂü∫ÂáÜÔºàBUS-BRA„ÄÅMMOTU-2DÂíåGIST514-DBÔºâ‰∏äËøõË°å‰∫ÜÂæÆË∞É„ÄÇÂú®ÊâÄÊúâ‰ªªÂä°‰∏≠ÔºåUSF-MAEÂßãÁªà‰ºò‰∫é‰º†ÁªüÁöÑCNNÂíåViTÂü∫Á∫øÔºåÂàÜÂà´ÂÆûÁé∞‰∫Ü81.6%„ÄÅ79.6%Âíå82.4%ÁöÑF1ÂàÜÊï∞„ÄÇÂ∞ΩÁÆ°Âú®È¢ÑËÆ≠ÁªÉÊúüÈó¥Êú™‰ΩøÁî®Ê†áÁ≠æÔºå‰ΩÜUSF-MAEÂú®‰π≥ËÖ∫ÁôåÂàÜÁ±ªÊñπÈù¢Êé•Ëøë‰∫ÜÊúâÁõëÁù£Âü∫Á°ÄÊ®°ÂûãUltraSamÁöÑÊÄßËÉΩÔºåÂπ∂Âú®ÂÖ∂‰ªñ‰ªªÂä°‰∏≠Ë∂ÖËøá‰∫ÜÂÆÉÔºåÂ±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑË∑®Ëß£ÂâñÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöË∂ÖÂ£∞ÂõæÂÉèËØäÊñ≠Èù¢‰∏¥Âô™Â£∞Â§ß„ÄÅÊìç‰ΩúËÄÖ‰æùËµñÊÄßÂº∫„ÄÅËßÜÈáéÊúâÈôêÁ≠âÊåëÊàòÔºåÂØºËá¥ËßÇÂØüËÄÖÈó¥Â∑ÆÂºÇÂ§ß„ÄÇÁé∞ÊúâÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ï‰æùËµñÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÔºåËÄåË∂ÖÂ£∞ÂõæÂÉèÊ†áÊ≥®ÊàêÊú¨È´òÊòÇ„ÄÇÈÄöÁî®ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂú®Ë∂ÖÂ£∞ÂõæÂÉè‰∏äÁöÑËøÅÁßªÊïàÊûú‰∏ç‰Ω≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Â∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂà©Áî®Êé©Á†ÅËá™ÁºñÁ†ÅÂô®ÔºàMAEÔºâËøõË°åËá™ÁõëÁù£Â≠¶‰π†„ÄÇÈÄöËøáÈöèÊú∫Êé©ÁõñÈÉ®ÂàÜË∂ÖÂ£∞ÂõæÂÉèÂùóÔºåÂπ∂ËÆ≠ÁªÉÊ®°ÂûãÈáçÂª∫Ëøô‰∫õË¢´Êé©ÁõñÁöÑÂå∫ÂüüÔºåËø´‰ΩøÊ®°ÂûãÂ≠¶‰π†Ë∂ÖÂ£∞ÂõæÂÉèÁöÑÂÜÖÂú®ÁªìÊûÑÂíåÁâπÂæÅË°®Á§∫„ÄÇËøôÁßçÊñπÊ≥ïÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®ÔºåÂèØ‰ª•ÊúâÊïàÂà©Áî®Â§ßÈáèÊú™Ê†áÊ≥®ÁöÑË∂ÖÂ£∞Êï∞ÊçÆ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöUSF-MAEÈááÁî®Vision TransformerÔºàViTÔºâ‰Ωú‰∏∫Âü∫Á°ÄÊû∂ÊûÑÔºåÂåÖÂê´‰∏Ä‰∏™ÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™Ëß£Á†ÅÂô®„ÄÇÁºñÁ†ÅÂô®Â§ÑÁêÜÊú™Ë¢´Êé©ÁõñÁöÑÂõæÂÉèÂùóÔºåÊèêÂèñÁâπÂæÅË°®Á§∫„ÄÇËß£Á†ÅÂô®Êé•Êî∂ÁºñÁ†ÅÂô®ÁöÑËæìÂá∫‰ª•ÂèäË¢´Êé©ÁõñÁöÑÂõæÂÉèÂùóÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØÔºåÈáçÂª∫ÂéüÂßãÂõæÂÉè„ÄÇÊï¥‰∏™ÊµÅÁ®ãÂåÖÊã¨Ôºö1ÔºâÂõæÂÉèÂùóÂàíÂàÜ‰∏éÊé©Á†ÅÔºõ2ÔºâÁºñÁ†ÅÂô®ÁâπÂæÅÊèêÂèñÔºõ3ÔºâËß£Á†ÅÂô®ÂõæÂÉèÈáçÂª∫Ôºõ4ÔºâÊçüÂ§±ËÆ°ÁÆó‰∏éÊ®°Âûã‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöUSF-MAEÊòØÈ¶ñ‰∏™‰∏ìÈó®ÈíàÂØπË∂ÖÂ£∞Êï∞ÊçÆÁöÑ„ÄÅÂ§ßËßÑÊ®°Ëá™ÁõëÁù£MAEÊ°ÜÊû∂„ÄÇÂÆÉÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´46‰∏™ÂºÄÊ∫êË∂ÖÂ£∞Êï∞ÊçÆÈõÜÁöÑOpenUS-46Êï∞ÊçÆÈõÜÔºå‰∏∫Ë∂ÖÂ£∞È¢ÜÂüüÁöÑËá™ÁõëÁù£Â≠¶‰π†Êèê‰æõ‰∫ÜÊï∞ÊçÆÂü∫Á°Ä„ÄÇ‰∏é‰º†ÁªüÁöÑÊúâÁõëÁù£Â≠¶‰π†ÊàñÈÄöÁî®ÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁõ∏ÊØîÔºåUSF-MAEËÉΩÂ§üÊõ¥Â•ΩÂú∞Â≠¶‰π†Ë∂ÖÂ£∞ÂõæÂÉèÁöÑÁâπÂÆöÊ®°ÊÄÅÁâπÂæÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöUSF-MAE‰ΩøÁî®ViT-Base‰Ωú‰∏∫ÁºñÁ†ÅÂô®ÂíåËß£Á†ÅÂô®ÁöÑ‰∏ªÂπ≤ÁΩëÁªú„ÄÇÊé©Á†ÅÊØî‰æãËÆæÁΩÆ‰∏∫75%ÔºåÂç≥ÈöèÊú∫Êé©Áõñ75%ÁöÑÂõæÂÉèÂùó„ÄÇÈáçÂª∫ÁõÆÊ†á‰∏∫ÂÉèÁ¥†ÂÄº„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®ÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÊçüÂ§±ÔºåË°°ÈáèÈáçÂª∫ÂõæÂÉè‰∏éÂéüÂßãÂõæÂÉè‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÈ¢ÑËÆ≠ÁªÉÂú®OpenUS-46Êï∞ÊçÆÈõÜ‰∏äËøõË°åÔºåÂæÆË∞ÉÂú®‰∏ãÊ∏∏ÂàÜÁ±ª‰ªªÂä°‰∏≠‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

USF-MAEÂú®‰∏â‰∏™‰∏ãÊ∏∏ÂàÜÁ±ª‰ªªÂä°ÔºàBUS-BRA„ÄÅMMOTU-2DÂíåGIST514-DBÔºâ‰∏äÂùáÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊÄßËÉΩ„ÄÇÂú®BUS-BRAÊï∞ÊçÆÈõÜ‰∏äÔºåUSF-MAEÁöÑF1ÂàÜÊï∞ËææÂà∞81.6%ÔºåÊé•ËøëÊúâÁõëÁù£Ê®°ÂûãUltraSamÁöÑÊÄßËÉΩ„ÄÇÂú®MMOTU-2DÂíåGIST514-DBÊï∞ÊçÆÈõÜ‰∏äÔºåUSF-MAEÁöÑF1ÂàÜÊï∞ÂàÜÂà´‰∏∫79.6%Âíå82.4%ÔºåË∂ÖËøá‰∫ÜUltraSam„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåUSF-MAEÂÖ∑ÊúâÂº∫Â§ßÁöÑÁâπÂæÅÂ≠¶‰π†ËÉΩÂäõÂíåË∑®Ëß£ÂâñÂå∫ÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

USF-MAEÂèØÂ∫îÁî®‰∫éÂ§öÁßçË∂ÖÂ£∞ÂõæÂÉèÂàÜÊûê‰ªªÂä°ÔºåÂ¶ÇÁóÖÁÅ∂Ê£ÄÊµã„ÄÅÂô®ÂÆòÂàÜÂâ≤„ÄÅÁñæÁóÖËØäÊñ≠Á≠â„ÄÇÈÄöËøáËá™ÁõëÁù£È¢ÑËÆ≠ÁªÉÔºåÂèØ‰ª•ÊúâÊïàÈôç‰ΩéÂØπÊ†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñÔºåÂä†ÈÄüË∂ÖÂ£∞AIÊ®°ÂûãÁöÑÂºÄÂèëÂíåÈÉ®ÁΩ≤„ÄÇËØ•Á†îÁ©∂ÊúâÊúõÊèêÈ´òË∂ÖÂ£∞ËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÔºåÂπ∂Êé®Âä®Ë∂ÖÂ£∞ÊäÄÊúØÂú®ÂåªÁñóÈ¢ÜÂüüÁöÑÊõ¥ÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Ultrasound imaging is one of the most widely used diagnostic modalities, offering real-time, radiation-free assessment across diverse clinical domains. However, interpretation of ultrasound images remains challenging due to high noise levels, operator dependence, and limited field of view, resulting in substantial inter-observer variability. Current Deep Learning approaches are hindered by the scarcity of large labeled datasets and the domain gap between general and sonographic images, which limits the transferability of models pretrained on non-medical data. To address these challenges, we introduce the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), the first large-scale self-supervised MAE framework pretrained exclusively on ultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound images curated from 46 open-source datasets, collectively termed OpenUS-46, spanning over twenty anatomical regions. This curated dataset has been made publicly available to facilitate further research and reproducibility. Using a Vision Transformer encoder-decoder architecture, USF-MAE reconstructs masked image patches, enabling it to learn rich, modality-specific representations directly from unlabeled data. The pretrained encoder was fine-tuned on three public downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D (ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all tasks, USF-MAE consistently outperformed conventional CNN and ViT baselines, achieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using labels during pretraining, USF-MAE approached the performance of the supervised foundation model UltraSam on breast cancer classification and surpassed it on the other tasks, demonstrating strong cross-anatomical generalization.

