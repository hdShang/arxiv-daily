---
layout: default
title: A Video Is Not Worth a Thousand Words
---

# A Video Is Not Worth a Thousand Words

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.23253" target="_blank" class="toolbar-btn">arXiv: 2510.23253v1</a>
    <a href="https://arxiv.org/pdf/2510.23253.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23253v1" 
            onclick="toggleFavorite(this, '2510.23253v1', 'A Video Is Not Worth a Thousand Words')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Sam Pollard, Michael Wray

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/sjpollard/a-video-is-not-worth-a-thousand-words)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éShapleyÂÄºÁöÑÁâπÂæÅÂΩíÂõ†ÂíåÊ®°ÊÄÅËØÑÂàÜÊñπÊ≥ïÔºåËØÑ‰º∞VLMÂú®VQA‰ªªÂä°‰∏≠ÁöÑÊñáÊú¨‰æùËµñÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÈóÆÁ≠î` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ShapleyÂÄº` `ÁâπÂæÅÂΩíÂõ†` `Ê®°ÊÄÅËØÑÂàÜ` `ÊñáÊú¨‰æùËµñ` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVQAÊ®°Âûã‰æùËµñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂèØËÉΩÂØºËá¥ÊñáÊú¨‰∏ªÂØºÔºåÂøΩÁï•‰∫ÜËßÜÈ¢ëÊ®°ÊÄÅÁöÑÈáçË¶ÅÊÄß„ÄÇ
2. ÊèêÂá∫Âü∫‰∫éShapleyÂÄºÁöÑÁâπÂæÅÂΩíÂõ†ÂíåÊ®°ÊÄÅËØÑÂàÜÊñπÊ≥ïÔºåÁî®‰∫éËØÑ‰º∞VLMÂØπ‰∏çÂêåÊ®°ÊÄÅÁöÑ‰æùËµñÁ®ãÂ∫¶„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁé∞ÊúâVQAÊ®°ÂûãÂú®Â§öÈ°πÈÄâÊã©‰ªªÂä°‰∏≠ËøáÂ∫¶‰æùËµñÊñáÊú¨ÔºåÂøΩÁï•‰∫ÜËßÜÈ¢ë‰ø°ÊÅØ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÊàë‰ª¨Ë∂äÊù•Ë∂ä‰æùËµñËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÊù•ÂõûÁ≠îÂÖ≥‰∫éÂë®Âõ¥‰∏ñÁïåÁöÑÈóÆÈ¢òÔºåÂ§ßÈáèÁöÑÁ†îÁ©∂Ëá¥Âäõ‰∫éÊèêÈ´òËßÜÈ¢ëÈóÆÁ≠îÔºàVQAÔºâÊï∞ÊçÆÈõÜÁöÑÈöæÂ∫¶ÂíåÊ®°ÂûãÊâÄËØÑ‰º∞ÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶„ÄÇÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫È™®Âπ≤ÁöÑ‰æùËµñÂºïÂèë‰∫ÜÂØπÊΩúÂú®ÊñáÊú¨‰∏ªÂØºÂú∞‰ΩçÁöÑÊãÖÂøßÔºåÂπ∂‰∏îÊ®°ÊÄÅ‰πãÈó¥ÁöÑ‰∫§‰∫íÊé¢Á¥¢‰∏çË∂≥„ÄÇÊàë‰ª¨Â¶Ç‰ΩïË°°ÈáèÊàë‰ª¨ÊòØÂê¶ÊúùÁùÄÊ≠£Á°ÆÁöÑÊñπÂêëÂâçËøõÔºå‰ª•ÂèäÂ§öÊ®°ÊÄÅÊ®°ÂûãÂºïÂÖ•ÁöÑÂ§çÊùÇÊÄßÔºüÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éShapleyÂÄºËÆ°ÁÆóÁâπÂæÅÂΩíÂõ†ÂíåÊ®°ÊÄÅÂàÜÊï∞ÁöÑËÅîÂêàÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÁâπÂæÅÂíåÊ®°ÊÄÅÈÉΩÊòØÂèØ‰ª•‰ªªÊÑèÂÆö‰πâÁöÑ„ÄÇ‰ΩøÁî®Ëøô‰∫õÊåáÊ†áÔºåÊàë‰ª¨ÊØîËæÉ‰∫Ü6‰∏™ÂÖ∑Êúâ‰∏çÂêå‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑVLMÊ®°ÂûãÂú®4‰∏™‰ª£Ë°®ÊÄßÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞ÔºåÈáçÁÇπÊòØÂ§öÈ°πÈÄâÊã©VQA„ÄÇÁâπÂà´ÊòØÔºåÊàë‰ª¨Â∞ÜËßÜÈ¢ëÂ∏ßÂíåÊï¥‰∏™ÊñáÊú¨ÂÖÉÁ¥†ËßÜ‰∏∫Â±ÇÊ¨°ÁªìÊûÑ‰∏≠ÁöÑÁõ∏Á≠âÁâπÂæÅÔºåÂπ∂Â∞ÜÂ§öÈ°πÈÄâÊã©VQA‰ªªÂä°ËßÜ‰∏∫ËßÜÈ¢ë„ÄÅÈóÆÈ¢òÂíåÁ≠îÊ°à‰∏âÁßçÊ®°ÊÄÅ‰πãÈó¥ÁöÑ‰∫§‰∫í„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÂØπÊñáÊú¨ÁöÑ‰æùËµñÊÄßÔºåÂπ∂Ë°®ÊòéÂ§öÈ°πÈÄâÊã©VQA‰ªªÂä°ÈÄÄÂåñ‰∏∫Ê®°ÂûãÂøΩÁï•Âπ≤Êâ∞È°πÁöÑËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂú®ËßÜÈ¢ëÈóÆÁ≠îÔºàVQAÔºâ‰ªªÂä°‰∏≠ÂØπÊñáÊú¨ÁöÑËøáÂ∫¶‰æùËµñÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫È™®Âπ≤ÔºåËøôÂèØËÉΩÂØºËá¥Ê®°ÂûãÂøΩÁï•ËßÜÈ¢ë‰ø°ÊÅØÔºåËÄåÂ∞ÜVQA‰ªªÂä°ÁÆÄÂåñ‰∏∫ÊñáÊú¨ÁêÜËß£ÂíåÊé®ÁêÜ„ÄÇÊ≠§Â§ñÔºåÂ¶Ç‰ΩïÊúâÊïàË°°ÈáèÊ®°ÂûãÂØπ‰∏çÂêåÊ®°ÊÄÅÁöÑ‰æùËµñÁ®ãÂ∫¶‰πüÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ShapleyÂÄºÊù•ËÆ°ÁÆóÁâπÂæÅÂΩíÂõ†ÂíåÊ®°ÊÄÅÂàÜÊï∞Ôºå‰ªéËÄåÈáèÂåñVLMÂØπ‰∏çÂêåÊ®°ÊÄÅÁöÑ‰æùËµñÁ®ãÂ∫¶„ÄÇShapleyÂÄºÊòØ‰∏ÄÁßçÂêà‰ΩúÂçöÂºàËÆ∫‰∏≠ÁöÑÊ¶ÇÂøµÔºåÂèØ‰ª•ÂÖ¨Âπ≥Âú∞ÂàÜÈÖçÊØè‰∏™ÁâπÂæÅÊàñÊ®°ÊÄÅÂØπÊ®°ÂûãÈ¢ÑÊµãÁöÑË¥°ÁåÆ„ÄÇÈÄöËøáÂàÜÊûêShapleyÂÄºÔºåÂèØ‰ª•ËØÜÂà´Âá∫Âì™‰∫õÁâπÂæÅÊàñÊ®°ÊÄÅÂØπÊ®°ÂûãÁöÑÂÜ≥Á≠ñËµ∑ÁùÄ‰∏ªÂØº‰ΩúÁî®„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÈ¶ñÂÖàÂ∞ÜËßÜÈ¢ëÂ∏ßÂíåÊñáÊú¨ÂÖÉÁ¥†ÔºàÈóÆÈ¢òÂíåÁ≠îÊ°àÔºâËßÜ‰∏∫Áªü‰∏ÄÁöÑÁâπÂæÅ„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®VLMÊ®°ÂûãÂØπVQA‰ªªÂä°ËøõË°åÈ¢ÑÊµã„ÄÇÊé•‰∏ãÊù•Ôºå‰ΩøÁî®ShapleyÂÄºËÆ°ÁÆóÊØè‰∏™ÁâπÂæÅÂíåÊ®°ÊÄÅÂØπÈ¢ÑÊµãÁªìÊûúÁöÑË¥°ÁåÆ„ÄÇÊúÄÂêéÔºåÈÄöËøáÂàÜÊûêÁâπÂæÅÂΩíÂõ†ÂíåÊ®°ÊÄÅÂàÜÊï∞ÔºåËØÑ‰º∞VLMÂØπ‰∏çÂêåÊ®°ÊÄÅÁöÑ‰æùËµñÁ®ãÂ∫¶„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÊ®°ÂûãÈ¢ÑÊµã„ÄÅShapleyÂÄºËÆ°ÁÆóÂíåÁªìÊûúÂàÜÊûêÂõõ‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜShapleyÂÄºÂ∫îÁî®‰∫éVQA‰ªªÂä°‰∏≠ÁöÑÁâπÂæÅÂΩíÂõ†ÂíåÊ®°ÊÄÅËØÑÂàÜ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÈáèÂåñVLMÂØπ‰∏çÂêåÊ®°ÊÄÅÁöÑ‰æùËµñÁ®ãÂ∫¶ÔºåÂπ∂ËØÜÂà´Âá∫Ê®°Âûã‰∏≠ÁöÑÊñáÊú¨‰∏ªÂØºÁé∞Ë±°„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÂÖ∑ÊúâÈÄöÁî®ÊÄßÔºåÂèØ‰ª•Â∫îÁî®‰∫é‰∏çÂêåÁöÑVLMÊ®°ÂûãÂíåVQAÊï∞ÊçÆÈõÜ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ShapleyÂÄºËÆ°ÁÆó‰∏≠ÔºåËÆ∫ÊñáÂ∞ÜËßÜÈ¢ëÂ∏ßÂíåÊñáÊú¨ÂÖÉÁ¥†ËßÜ‰∏∫Âπ≥Á≠âÁâπÂæÅÔºåÂπ∂ËÄÉËôë‰∫ÜËßÜÈ¢ë„ÄÅÈóÆÈ¢òÂíåÁ≠îÊ°à‰∏âÁßçÊ®°ÊÄÅ‰πãÈó¥ÁöÑ‰∫§‰∫í„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåËÆ∫Êñá‰ΩøÁî®ËíôÁâπÂç°Ê¥õÊñπÊ≥ïÊù•‰º∞ËÆ°ShapleyÂÄºÔºåÂπ∂ÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòËÆæËÆ°‰∫Ü‰∏ÄÁ≥ªÂàóÊåáÊ†áÊù•ËØÑ‰º∞ÁâπÂæÅÂΩíÂõ†ÂíåÊ®°ÊÄÅËØÑÂàÜÁöÑÁªìÊûúÔºå‰æãÂ¶ÇÊ®°ÊÄÅ‰æùËµñÊÄßÂàÜÊï∞ÂíåÁâπÂæÅÈáçË¶ÅÊÄßÊéíÂêç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁé∞ÊúâVQAÊ®°ÂûãÂú®Â§öÈ°πÈÄâÊã©‰ªªÂä°‰∏≠ËøáÂ∫¶‰æùËµñÊñáÊú¨ÔºåÂøΩÁï•‰∫ÜËßÜÈ¢ë‰ø°ÊÅØ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊ®°ÂûãÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùËµñ‰∫éÈóÆÈ¢òÂíåÁ≠îÊ°à‰∏≠ÁöÑÂÖ≥ÈîÆËØçÔºåËÄåÂØπËßÜÈ¢ëÂ∏ßÁöÑÂÖ≥Ê≥®ËæÉÂ∞ë„ÄÇÈÄöËøáÂØπÊØî‰∏çÂêå‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑVLMÊ®°ÂûãÔºåÂèëÁé∞Â¢ûÂä†‰∏ä‰∏ãÊñáÈïøÂ∫¶Âπ∂‰∏çËÉΩÊúâÊïàËß£ÂÜ≥ÊñáÊú¨‰∏ªÂØºÈóÆÈ¢ò„ÄÇÂÆûÈ™åËøòË°®ÊòéÔºåÂ§öÈ°πÈÄâÊã©VQA‰ªªÂä°Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÈÄÄÂåñ‰∏∫Ê®°ÂûãÂøΩÁï•Âπ≤Êâ∞È°πÁöÑËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊèêÂçáËßÜÈ¢ëÈóÆÁ≠îÁ≥ªÁªüÁöÑÊÄßËÉΩÂíåÂèØËß£ÈáäÊÄß„ÄÇÈÄöËøáÂàÜÊûêÊ®°ÂûãÂØπ‰∏çÂêåÊ®°ÊÄÅÁöÑ‰æùËµñÁ®ãÂ∫¶ÔºåÂèØ‰ª•ÊåáÂØºÊ®°ÂûãËÆæËÆ°ÔºåÂáèÂ∞ëÂØπÊñáÊú¨ÁöÑËøáÂ∫¶‰æùËµñÔºåÊèêÈ´òÂØπËßÜÈ¢ëÂÜÖÂÆπÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éËØÑ‰º∞‰∏çÂêåVLMÊ®°ÂûãÁöÑ‰ºòÂä£ÔºåÂπ∂‰∏∫Ê®°ÂûãÈÄâÊã©Êèê‰æõ‰æùÊçÆ„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅ‰ªªÂä°Ôºå‰æãÂ¶ÇËßÜÈ¢ëÊëòË¶Å„ÄÅËßÜÈ¢ëÁîüÊàêÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> As we become increasingly dependent on vision language models (VLMs) to answer questions about the world around us, there is a significant amount of research devoted to increasing both the difficulty of video question answering (VQA) datasets, and the context lengths of the models that they evaluate. The reliance on large language models as backbones has lead to concerns about potential text dominance, and the exploration of interactions between modalities is underdeveloped. How do we measure whether we're heading in the right direction, with the complexity that multi-modal models introduce? We propose a joint method of computing both feature attributions and modality scores based on Shapley values, where both the features and modalities are arbitrarily definable. Using these metrics, we compare $6$ VLM models of varying context lengths on $4$ representative datasets, focusing on multiple-choice VQA. In particular, we consider video frames and whole textual elements as equal features in the hierarchy, and the multiple-choice VQA task as an interaction between three modalities: video, question and answer. Our results demonstrate a dependence on text and show that the multiple-choice VQA task devolves into a model's ability to ignore distractors. Code available at https://github.com/sjpollard/a-video-is-not-worth-a-thousand-words.

