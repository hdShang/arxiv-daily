---
layout: default
title: Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences
---

# Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.23451" target="_blank" class="toolbar-btn">arXiv: 2510.23451v1</a>
    <a href="https://arxiv.org/pdf/2510.23451.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23451v1" 
            onclick="toggleFavorite(this, '2510.23451v1', 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao

**ÂàÜÁ±ª**: cs.CL, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27

**Â§áÊ≥®**: 48 pages, 17 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Omni-RewardÔºåÁî®‰∫éÊîØÊåÅËá™Áî±ÂΩ¢ÂºèÂÅèÂ•ΩÁöÑÈÄöÁî®ÂÖ®Ê®°ÊÄÅÂ•ñÂä±Âª∫Ê®°„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â•ñÂä±Ê®°Âûã` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÂÅèÂ•ΩÂª∫Ê®°` `ÈÄöÁî®‰∫∫Â∑•Êô∫ËÉΩ` `‰∫∫Êú∫ÂØπÈΩê` `Ëá™Áî±ÂΩ¢ÂºèÂÅèÂ•Ω` `ÂÖ®Ê®°ÊÄÅÊï∞ÊçÆ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂ•ñÂä±Ê®°Âûã‰∏ªË¶ÅÈõÜ‰∏≠‰∫éÊñáÊú¨ÂíåÂõæÂÉèÊ®°ÊÄÅÔºåÁº∫‰πèÂØπËßÜÈ¢ë„ÄÅÈü≥È¢ëÁ≠âÊ®°ÊÄÅÁöÑÊîØÊåÅÔºå‰∏îÈöæ‰ª•ÊçïÊçâ‰∏™ÊÄßÂåñÂÅèÂ•ΩÁöÑÂ§çÊùÇÊÄß„ÄÇ
2. Omni-RewardÈÄöËøáÊûÑÂª∫ÂÖ®Ê®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÔºåÂπ∂ÂºïÂÖ•Ëá™Áî±ÂΩ¢ÂºèÂÅèÂ•ΩÔºåÊó®Âú®ÊèêÂçáÂ•ñÂä±Ê®°ÂûãÂú®Â§öÊ®°ÊÄÅÊï∞ÊçÆÂíå‰∏™ÊÄßÂåñÂÅèÂ•ΩÊñπÈù¢ÁöÑÂª∫Ê®°ËÉΩÂäõ„ÄÇ
3. Omni-RewardÂú®Ëá™Âª∫ÁöÑOmni-RewardBenchÂü∫ÂáÜÊµãËØï‰ª•ÂèäÂÖ∂‰ªñÂ∏∏Áî®Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÖ®Ê®°ÊÄÅÂ•ñÂä±Âª∫Ê®°ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â•ñÂä±Ê®°ÂûãÔºàRMÔºâÂú®‰ΩøAIË°å‰∏∫‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÊñπÈù¢Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®Ôºå‰ΩÜÂÆÉ‰ª¨Èù¢‰∏¥‰∏§‰∏™Ê†πÊú¨ÊåëÊàòÔºöÔºà1ÔºâÊ®°ÊÄÅ‰∏çÂπ≥Ë°°ÔºåÂ§ßÂ§öÊï∞RM‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÊñáÊú¨ÂíåÂõæÂÉèÊ®°ÊÄÅÔºåÂØπËßÜÈ¢ë„ÄÅÈü≥È¢ëÂíåÂÖ∂‰ªñÊ®°ÊÄÅÁöÑÊîØÊåÅÊúâÈôêÔºõÔºà2ÔºâÂÅèÂ•ΩÂàöÊÄßÔºåÂú®Âõ∫ÂÆöÁöÑ‰∫åÂÖÉÂÅèÂ•ΩÂØπ‰∏äËÆ≠ÁªÉÊó†Ê≥ïÊçïÊçâ‰∏™ÊÄßÂåñÂÅèÂ•ΩÁöÑÂ§çÊùÇÊÄßÂíåÂ§öÊ†∑ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥‰∏äËø∞ÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜOmni-RewardÔºåÊó®Âú®ÂÆûÁé∞ÊîØÊåÅËá™Áî±ÂΩ¢ÂºèÂÅèÂ•ΩÁöÑÈÄöÁî®ÂÖ®Ê®°ÊÄÅÂ•ñÂä±Âª∫Ê®°ÔºåÂåÖÊã¨ÔºöÔºà1ÔºâËØÑ‰º∞ÔºöÊàë‰ª¨ÂºïÂÖ•‰∫ÜOmni-RewardBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂÖ∑ÊúâËá™Áî±ÂΩ¢ÂºèÂÅèÂ•ΩÁöÑÂÖ®Ê®°ÊÄÅRMÂü∫ÂáÜÔºåÊ∂µÁõñÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅËßÜÈ¢ë„ÄÅÈü≥È¢ëÂíå3DÁ≠â‰∫îÁßçÊ®°ÊÄÅÁöÑ‰πù‰∏™‰ªªÂä°ÔºõÔºà2ÔºâÊï∞ÊçÆÔºöÊàë‰ª¨ÊûÑÂª∫‰∫ÜOmni-RewardDataÔºå‰∏Ä‰∏™ÂåÖÂê´248K‰∏™ÈÄöÁî®ÂÅèÂ•ΩÂØπÂíå69K‰∏™Êåá‰ª§Ë∞É‰ºòÂØπÁöÑÂ§öÊ®°ÊÄÅÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºåÁî®‰∫éËÆ≠ÁªÉÈÄöÁî®ÂÖ®Ê®°ÊÄÅRMÔºõÔºà3ÔºâÊ®°ÂûãÔºöÊàë‰ª¨ÊèêÂá∫‰∫ÜOmni-RewardModelÔºåÂåÖÊã¨Âà§Âà´ÂºèÂíåÁîüÊàêÂºèRMÔºåÂπ∂Âú®Omni-RewardBench‰ª•ÂèäÂÖ∂‰ªñÂπøÊ≥õ‰ΩøÁî®ÁöÑÂ•ñÂä±Âª∫Ê®°Âü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÂº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂ•ñÂä±Ê®°Âûã‰∏ªË¶ÅÈõÜ‰∏≠‰∫éÊñáÊú¨ÂíåÂõæÂÉèÊ®°ÊÄÅÔºåÂØπËßÜÈ¢ë„ÄÅÈü≥È¢ëÁ≠âÊ®°ÊÄÅÁöÑÊîØÊåÅ‰∏çË∂≥ÔºåÂØºËá¥Âú®Â§ÑÁêÜÂ§öÊ®°ÊÄÅ‰ªªÂä°Êó∂ÊÄßËÉΩÂèóÈôê„ÄÇÊ≠§Â§ñÔºå‰º†ÁªüÁöÑ‰∫åÂÖÉÂÅèÂ•ΩÂØπËÆ≠ÁªÉÊñπÂºèÊó†Ê≥ïÊçïÊçâÁî®Êà∑‰∏™ÊÄßÂåñÂÅèÂ•ΩÁöÑÂ§çÊùÇÊÄßÂíåÂ§öÊ†∑ÊÄßÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöOmni-RewardÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™ËÉΩÂ§üÂ§ÑÁêÜÂ§öÁßçÊ®°ÊÄÅÊï∞ÊçÆÂπ∂ÊîØÊåÅËá™Áî±ÂΩ¢ÂºèÂÅèÂ•ΩÁöÑÈÄöÁî®Â•ñÂä±Ê®°Âûã„ÄÇÈÄöËøáÂºïÂÖ•ÂÖ®Ê®°ÊÄÅÊï∞ÊçÆÂíåËá™Áî±ÂΩ¢ÂºèÂÅèÂ•ΩÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£Áî®Êà∑ÊÑèÂõæÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞‰∏çÂêåÊ®°ÊÄÅÊï∞ÊçÆÁöÑË¥®Èáè„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöOmni-RewardÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÁªÑÊàêÈÉ®ÂàÜÔºöOmni-RewardBenchÔºàËØÑ‰º∞Âü∫ÂáÜÔºâ„ÄÅOmni-RewardDataÔºàÂ§öÊ®°ÊÄÅÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºâÂíåOmni-RewardModelÔºàÂ•ñÂä±Ê®°ÂûãÔºâ„ÄÇOmni-RewardBenchÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêåÊ®°ÊÄÅÂíå‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇOmni-RewardDataÁî®‰∫éËÆ≠ÁªÉÂ•ñÂä±Ê®°Âûã„ÄÇOmni-RewardModelÂåÖÂê´Âà§Âà´ÂºèÂíåÁîüÊàêÂºè‰∏§ÁßçÊ®°ÂûãÁªìÊûÑÔºåÁî®‰∫éÂ≠¶‰π†‰∏çÂêåÊ®°ÊÄÅÊï∞ÊçÆ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂíåÁî®Êà∑ÂÅèÂ•Ω„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöOmni-RewardÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÂØπÂÖ®Ê®°ÊÄÅÊï∞ÊçÆÂíåËá™Áî±ÂΩ¢ÂºèÂÅèÂ•ΩÁöÑÊîØÊåÅ„ÄÇÈÄöËøáÊûÑÂª∫ÂåÖÂê´Â§öÁßçÊ®°ÊÄÅÊï∞ÊçÆÂíåËá™Áî±ÂΩ¢ÂºèÂÅèÂ•ΩÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ËÆæËÆ°Áõ∏Â∫îÁöÑÊ®°ÂûãÁªìÊûÑÔºåOmni-RewardËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÁî®Êà∑ÊÑèÂõæÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞‰∏çÂêåÊ®°ÊÄÅÊï∞ÊçÆÁöÑË¥®Èáè„ÄÇÊ≠§Â§ñÔºåOmni-RewardBenchÁöÑÊèêÂá∫‰∏∫ÂÖ®Ê®°ÊÄÅÂ•ñÂä±Âª∫Ê®°Êèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑËØÑ‰º∞Âπ≥Âè∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöOmni-RewardModelÈááÁî®‰∫ÜÂà§Âà´ÂºèÂíåÁîüÊàêÂºè‰∏§ÁßçÊ®°ÂûãÁªìÊûÑ„ÄÇÂà§Âà´ÂºèÊ®°ÂûãÁî®‰∫éÁõ¥Êé•È¢ÑÊµãÂ•ñÂä±ÂÄºÔºåËÄåÁîüÊàêÂºèÊ®°ÂûãÁî®‰∫éÁîüÊàêÁ¨¶ÂêàÁî®Êà∑ÂÅèÂ•ΩÁöÑÂÜÖÂÆπ„ÄÇÂÖ∑‰ΩìÁöÑÊäÄÊúØÁªÜËäÇÂåÖÊã¨Ôºö‰ΩøÁî®TransformerÊû∂ÊûÑ‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºåÈááÁî®ÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÂáΩÊï∞Êù•Â≠¶‰π†‰∏çÂêåÊ®°ÊÄÅÊï∞ÊçÆ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÊù•‰ºòÂåñÊ®°ÂûãÁöÑÁîüÊàêËÉΩÂäõ„ÄÇÊï∞ÊçÆÈõÜÊûÑÂª∫ÊñπÈù¢ÔºåÈááÁî®‰∫Ü‰∫∫Â∑•Ê†áÊ≥®ÂíåËá™Âä®ÁîüÊàêÁõ∏ÁªìÂêàÁöÑÊñπÂºèÔºå‰ª•‰øùËØÅÊï∞ÊçÆÁöÑË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Omni-RewardÂú®Omni-RewardBenchÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜÈ¢ë„ÄÅÈü≥È¢ëÂíå3DÁ≠âÊ®°ÊÄÅ‰∏äÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÊúâÊòéÊòæ‰ºòÂäø„ÄÇÊ≠§Â§ñÔºåOmni-RewardÂú®ÂÖ∂‰ªñÂ∏∏Áî®ÁöÑÂ•ñÂä±Âª∫Ê®°Âü∫ÂáÜ‰∏ä‰πüË°®Áé∞Âá∫Ëâ≤ÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÈÄöÁî®ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Omni-RewardÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÁêÜËß£ÂíåÂØπÈΩê‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÔºöÂ§öÊ®°ÊÄÅÂÜÖÂÆπÊé®Ëçê„ÄÅÊô∫ËÉΩÂØπËØùÁ≥ªÁªü„ÄÅÊú∫Âô®‰∫∫Ë°å‰∏∫ËßÑÂàíÁ≠â„ÄÇÈÄöËøáÊõ¥ÂáÜÁ°ÆÂú∞ÊçïÊçâÁî®Êà∑ÊÑèÂõæÔºåOmni-RewardÂèØ‰ª•ÊèêÂçáÁî®Êà∑‰ΩìÈ™åÔºåÂπ∂‰øÉËøõ‰∫∫Êú∫Âçè‰Ωú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.

