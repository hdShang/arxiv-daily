---
layout: default
title: Finding 3D Scene Analogies with Multimodal Foundation Models
---

# Finding 3D Scene Analogies with Multimodal Foundation Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.23184" target="_blank" class="toolbar-btn">arXiv: 2510.23184v1</a>
    <a href="https://arxiv.org/pdf/2510.23184.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23184v1" 
            onclick="toggleFavorite(this, '2510.23184v1', 'Finding 3D Scene Analogies with Multimodal Foundation Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Junho Kim, Young Min Kim

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27

**Â§áÊ≥®**: Accepted to FM4RoboPlan workshop at RSS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Âà©Áî®Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÂÆûÁé∞Èõ∂Ê†∑Êú¨‰∏âÁª¥Âú∫ÊôØÁ±ªÊØîÔºåÁî®‰∫éÊú∫Âô®‰∫∫ËΩ®ËøπÂíåË∑ØÂæÑÁÇπËøÅÁßª„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `3DÂú∫ÊôØÁ±ªÊØî` `Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°Âûã` `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `Êú∫Âô®‰∫∫ÂØºËà™` `ËΩ®ËøπËøÅÁßª`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ3DÂú∫ÊôØÁ±ªÊØîÊñπÊ≥ïÈúÄË¶ÅÈ¢ùÂ§ñËÆ≠ÁªÉÂíåÂõ∫ÂÆöËØçÊ±áË°®ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂºÄÊîæÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÁöÑÊ∑∑ÂêàÁ•ûÁªèË°®Á§∫ÊñπÊ≥ïÔºåÂÆûÁé∞Èõ∂Ê†∑Êú¨„ÄÅÂºÄÊîæËØçÊ±áÁöÑ3DÂú∫ÊôØÁ±ªÊØî„ÄÇ
3. ÂÆûÈ™åËØÅÊòéËØ•ÊñπÊ≥ïËÉΩÂáÜÁ°ÆÂª∫Á´ãÂ§çÊùÇÂú∫ÊôØÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºåÂπ∂ÊàêÂäüÂ∫îÁî®‰∫éËΩ®ËøπÂíåË∑ØÂæÑÁÇπËøÅÁßª„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂà©Áî®Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÂú®Èõ∂Ê†∑Êú¨„ÄÅÂºÄÊîæËØçÊ±áÁéØÂ¢É‰∏≠ÂØªÊâæ3DÂú∫ÊôØÁ±ªÊØîÁöÑÊñπÊ≥ïÔºåÊó®Âú®Â∏ÆÂä©Êú∫Âô®‰∫∫Âú®Êñ∞ÁöÑ„ÄÅÊú™ËßÅËøáÁöÑ3DÁéØÂ¢É‰∏≠ËøõË°åÈÄÇÂ∫îÂíåËßÑÂàí„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÂíåÂõ∫ÂÆöÁöÑÂØπË±°ËØçÊ±áË°®„ÄÇËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊòØ‰∏ÄÁßçÊ∑∑ÂêàÁ•ûÁªèÂú∫ÊôØË°®Á§∫ÔºåÂÆÉÁî±Âü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁâπÂæÅÁöÑÁ®ÄÁñèÂõæÂíå‰ªé3DÂΩ¢Áä∂Âü∫Á°ÄÊ®°ÂûãÂØºÂá∫ÁöÑÁâπÂæÅÂú∫ÁªÑÊàê„ÄÇÈÄöËøáÁ≤óÂà∞Á≤æÁöÑÊñπÂºèÂØªÊâæ3DÂú∫ÊôØÁ±ªÊØîÔºåÈ¶ñÂÖàÂØπÈΩêÂõæÔºåÁÑ∂Âêé‰ΩøÁî®ÁâπÂæÅÂú∫ÁªÜÂåñÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Âú®Â§çÊùÇÂú∫ÊôØ‰πãÈó¥Âª∫Á´ãÂáÜÁ°ÆÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºåÂπ∂Â∫îÁî®‰∫éËΩ®ËøπÂíåË∑ØÂæÑÁÇπËøÅÁßª„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞Êúâ3DÂú∫ÊôØÁ±ªÊØîÊñπÊ≥ïÈúÄË¶ÅÈíàÂØπÁâπÂÆöÂú∫ÊôØËøõË°åÈ¢ùÂ§ñËÆ≠ÁªÉÔºåÂπ∂‰∏î‰æùËµñ‰∫éÂõ∫ÂÆöÁöÑÁâ©‰ΩìËØçÊ±áË°®ÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Êú™Áü•ÂíåÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®Èõ∂Ê†∑Êú¨„ÄÅÂºÄÊîæËØçÊ±áÁöÑÊù°‰ª∂‰∏ãÔºåÂ¶Ç‰ΩïÈ´òÊïàÂáÜÁ°ÆÂú∞Âª∫Á´ã‰∏çÂêå3DÂú∫ÊôØ‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºå‰ªéËÄåÂÆûÁé∞Áü•ËØÜËøÅÁßªÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÂº∫Â§ßÁöÑËØ≠‰πâÁêÜËß£ËÉΩÂäõÂíå3DÂΩ¢Áä∂Ë°®ÂæÅËÉΩÂäõÔºåÊûÑÂª∫‰∏ÄÁßçÊ∑∑ÂêàÁ•ûÁªèÂú∫ÊôØË°®Á§∫„ÄÇËØ•Ë°®Á§∫ÁªìÂêà‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®Â±ÄËØ≠‰πâ‰ø°ÊÅØÂíå3DÂΩ¢Áä∂Âü∫Á°ÄÊ®°ÂûãÁöÑÂ±ÄÈÉ®Âá†‰Ωï‰ø°ÊÅØÔºå‰ªéËÄåËÉΩÂ§üÂú®Ê≤°ÊúâÈ¢ùÂ§ñËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞Âú∫ÊôØ‰πãÈó¥ÁöÑÊúâÊïàÂØπÈΩê„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÈááÁî®Á≤óÂà∞Á≤æÁöÑÁ≠ñÁï•„ÄÇÈ¶ñÂÖàÔºåÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊèêÂèñÂú∫ÊôØ‰∏≠ÂÖ≥ÈîÆÂå∫ÂüüÁöÑËØ≠‰πâÁâπÂæÅÔºåÊûÑÂª∫Á®ÄÁñèÂõæÔºåÂπ∂ÈÄöËøáÂõæÂåπÈÖçÁÆóÊ≥ïÂÆûÁé∞Á≤óÁï•ÁöÑÂú∫ÊôØÂØπÈΩê„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®3DÂΩ¢Áä∂Âü∫Á°ÄÊ®°ÂûãÊèêÂèñÂú∫ÊôØÁöÑÂ±ÄÈÉ®Âá†‰ΩïÁâπÂæÅÔºåÊûÑÂª∫ÁâπÂæÅÂú∫ÔºåÂπ∂ÈÄöËøá‰ºòÂåñÁÆóÊ≥ïÁªÜÂåñÂú∫ÊôØÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Ôºö1) Âú∫ÊôØË°®Á§∫ÊûÑÂª∫Ôºõ2) Âü∫‰∫éÂõæÂåπÈÖçÁöÑÁ≤óÁï•ÂØπÈΩêÔºõ3) Âü∫‰∫éÁâπÂæÅÂú∫‰ºòÂåñÁöÑÁ≤æÁªÜÂØπÈΩê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÂºïÂÖ•Âà∞3DÂú∫ÊôØÁ±ªÊØî‰ªªÂä°‰∏≠ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÁ•ûÁªèÂú∫ÊôØË°®Á§∫„ÄÇËøôÁßçË°®Á§∫ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ËûçÂêàÂÖ®Â±ÄËØ≠‰πâ‰ø°ÊÅØÂíåÂ±ÄÈÉ®Âá†‰Ωï‰ø°ÊÅØÔºå‰ªéËÄåÂú®Èõ∂Ê†∑Êú¨„ÄÅÂºÄÊîæËØçÊ±áÁöÑÊù°‰ª∂‰∏ãÂÆûÁé∞ÂáÜÁ°ÆÁöÑÂú∫ÊôØÂØπÈΩê„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÔºåÂπ∂‰∏îËÉΩÂ§üÂ§ÑÁêÜÂÖ∑Êúâ‰∏çÂêåÁâ©‰ΩìËØçÊ±áË°®ÁöÑÂú∫ÊôØ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠ÂÖ≥ÈîÆÁöÑËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®CLIPÁ≠âËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊèêÂèñÂú∫ÊôØ‰∏≠ÂÖ≥ÈîÆÂå∫ÂüüÁöÑËØ≠‰πâÁâπÂæÅÔºõ2) ‰ΩøÁî®3DÂΩ¢Áä∂Âü∫Á°ÄÊ®°ÂûãÔºà‰æãÂ¶ÇShapeNetÔºâÊèêÂèñÂú∫ÊôØÁöÑÂ±ÄÈÉ®Âá†‰ΩïÁâπÂæÅÔºõ3) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂõæÂåπÈÖçÂíåÁâπÂæÅÂú∫‰ºòÂåñÁöÑ‰∏§Èò∂ÊÆµÂØπÈΩêÁÆóÊ≥ïÔºõ4) ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÂèØËÉΩÂåÖÊã¨ÂõæÂåπÈÖçÁöÑÁõ∏‰ººÂ∫¶ÊçüÂ§±ÂíåÁâπÂæÅÂú∫ÂØπÂ∫îÂÖ≥Á≥ªÁöÑÂá†‰Ωï‰∏ÄËá¥ÊÄßÊçüÂ§±ÔºàÂÖ∑‰ΩìÁªÜËäÇÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂú®Â§çÊùÇÂú∫ÊôØ‰πãÈó¥Âª∫Á´ãÂáÜÁ°ÆÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºåÂπ∂‰∏îÂú®ËΩ®ËøπÂíåË∑ØÂæÑÁÇπËøÅÁßª‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜËâØÂ•ΩÁöÑÊïàÊûú„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÔºåÂπ∂‰∏îËÉΩÂ§üÂ§ÑÁêÜÂÖ∑Êúâ‰∏çÂêåÁâ©‰ΩìËØçÊ±áË°®ÁöÑÂú∫ÊôØ„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Âú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÁöÑÂ±ïÁ§∫ÔºàÂÖ∑‰ΩìÊï∞ÂÄºÊú™Áü•Ôºâ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÊ®°‰ªøÂ≠¶‰π†„ÄÅ‰ªªÂä°ËßÑÂàíÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•ÈÄöËøáÂú∫ÊôØÁ±ªÊØîÔºåÂ∞ÜÂ∑≤Áü•ÁöÑÂØºËà™Á≠ñÁï•ËøÅÁßªÂà∞Êñ∞ÁöÑÁéØÂ¢É‰∏≠ÔºõÂèØ‰ª•ÈÄöËøáÊ®°‰ªøÂ≠¶‰π†ÔºåÂ∞Ü‰∫∫Á±ªÂú®Áõ∏‰ººÂú∫ÊôØ‰∏≠ÁöÑÊìç‰ΩúÁªèÈ™åËøÅÁßªÂà∞Êú∫Âô®‰∫∫Ë∫´‰∏äÔºõËøòÂèØ‰ª•ÈÄöËøá‰ªªÂä°ËßÑÂàíÔºåÂ∞ÜÂ∑≤Áü•ÁöÑ‰ªªÂä°ÊµÅÁ®ãËøÅÁßªÂà∞Êñ∞ÁöÑÂú∫ÊôØ‰∏≠„ÄÇËØ•Á†îÁ©∂ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Connecting current observations with prior experiences helps robots adapt and plan in new, unseen 3D environments. Recently, 3D scene analogies have been proposed to connect two 3D scenes, which are smooth maps that align scene regions with common spatial relationships. These maps enable detailed transfer of trajectories or waypoints, potentially supporting demonstration transfer for imitation learning or task plan transfer across scenes. However, existing methods for the task require additional training and fixed object vocabularies. In this work, we propose to use multimodal foundation models for finding 3D scene analogies in a zero-shot, open-vocabulary setting. Central to our approach is a hybrid neural representation of scenes that consists of a sparse graph based on vision-language model features and a feature field derived from 3D shape foundation models. 3D scene analogies are then found in a coarse-to-fine manner, by first aligning the graph and refining the correspondence with feature fields. Our method can establish accurate correspondences between complex scenes, and we showcase applications in trajectory and waypoint transfer.

