---
layout: default
title: LightFusion: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation
---

# LightFusion: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22946" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22946v4</a>
  <a href="https://arxiv.org/pdf/2510.22946.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22946v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22946v4', 'LightFusion: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, Cihang Xie

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27 (æ›´æ–°: 2025-11-20)

**å¤‡æ³¨**: Preprint. Work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LightFusionï¼šè½»é‡çº§åŒé‡èåˆæ¡†æ¶ï¼Œç”¨äºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ` `å›¾åƒç¼–è¾‘` `è½»é‡çº§æ¨¡å‹` `é¢„è®­ç»ƒæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åº”ç”¨å’Œå‘å±•ã€‚
2. LightFusioné€šè¿‡åŒé‡èåˆæœºåˆ¶ï¼Œæœ‰æ•ˆèåˆé¢„è®­ç»ƒçš„ç”Ÿæˆå’Œç†è§£æ¨¡å‹ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLightFusionåœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æœ€è¿‘åœ¨èƒ½åŠ›å’Œé€šç”¨æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œä½†å¤§å¤šæ•°é¢†å…ˆç³»ç»Ÿä»ç„¶æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚æœ¬æ–‡è¡¨æ˜ï¼Œé€šè¿‡ç­–ç•¥æ€§åœ°èåˆå…¬å¼€å¯ç”¨çš„ã€ä¸“é—¨ç”¨äºç”Ÿæˆæˆ–ç†è§£çš„æ¨¡å‹ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è·å¾—å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å…³é”®è®¾è®¡æ˜¯ä¿ç•™åŸå§‹æ¨¡å—ï¼ŒåŒæ—¶åœ¨ç½‘ç»œä¸­ç©¿æ’å¤šæ¨¡æ€è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚è¿™ç§åŒé‡èåˆæœºåˆ¶ï¼ˆ1ï¼‰æœ‰æ•ˆåœ°å®ç°äº†ä¸°å¯Œçš„å¤šæ¨¡æ€èåˆï¼ŒåŒæ—¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„åŸå§‹ä¼˜åŠ¿ï¼Œå¹¶ä¸”ï¼ˆ2ï¼‰ä¿ƒè¿›äº†æ¥è‡ªç†è§£ç¼–ç å™¨çš„é«˜çº§è¯­ä¹‰è¡¨ç¤ºä¸æ¥è‡ªç”Ÿæˆç¼–ç å™¨çš„ä½çº§ç©ºé—´ä¿¡å·çš„ååŒèåˆã€‚é€šè¿‡ä»…ä½¿ç”¨çº¦350äº¿ä¸ªtokenè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¼ºå¤§çš„ç»“æœï¼šåœ¨GenEvalä¸Šï¼Œç»„åˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¾¾åˆ°0.91ï¼›åœ¨DPG-Benchä¸Šï¼Œå¤æ‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¾¾åˆ°82.16ï¼›åœ¨GEditBenchä¸Šï¼Œå›¾åƒç¼–è¾‘è¾¾åˆ°6.06ï¼›åœ¨ImgEdit-Benchä¸Šï¼Œå›¾åƒç¼–è¾‘è¾¾åˆ°3.77ã€‚æˆ‘ä»¬å®Œå…¨å‘å¸ƒäº†æ•´å¥—ä»£ç ã€æ¨¡å‹æƒé‡å’Œæ•°æ®é›†ï¼Œå¸Œæœ›æ”¯æŒæœªæ¥å¯¹ç»Ÿä¸€å¤šæ¨¡æ€å»ºæ¨¡çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè¿™å¯¼è‡´äº†å·¨å¤§çš„è®¡ç®—èµ„æºæ¶ˆè€—å’Œæ—¶é—´æˆæœ¬ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°èåˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•å°†ç†è§£æ¨¡å‹ä¸­çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ä¸ç”Ÿæˆæ¨¡å‹ä¸­çš„ä½çº§ç©ºé—´ä¿¡æ¯ç›¸ç»“åˆï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLightFusionçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„ã€åˆ†åˆ«æ“…é•¿ç”Ÿæˆå’Œç†è§£çš„æ¨¡å‹ï¼Œé€šè¿‡ä¸€ç§è½»é‡çº§çš„èåˆæœºåˆ¶ï¼Œå°†å®ƒä»¬çš„èƒ½åŠ›ç»“åˆèµ·æ¥ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„éœ€è¦ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ã€‚åŒæ—¶ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„èåˆæ¨¡å—ï¼Œå®ç°äº†ä¸åŒæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆäº¤äº’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLightFusionæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé¢„è®­ç»ƒçš„ç¼–ç å™¨ï¼šä¸€ä¸ªç”¨äºç†è§£ï¼ˆä¾‹å¦‚ï¼Œæ–‡æœ¬ç¼–ç å™¨ï¼‰ï¼Œå¦ä¸€ä¸ªç”¨äºç”Ÿæˆï¼ˆä¾‹å¦‚ï¼Œå›¾åƒç¼–ç å™¨ï¼‰ã€‚è¿™ä¸¤ä¸ªç¼–ç å™¨çš„åŸå§‹æ¨¡å—è¢«ä¿ç•™ï¼Œå¹¶åœ¨å…¶ä¸­ç©¿æ’å¤šæ¨¡æ€è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚è¿™äº›è‡ªæ³¨æ„åŠ›æ¨¡å—è´Ÿè´£èåˆæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡æ¯ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¼˜åŒ–å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šLightFusionçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŒé‡èåˆæœºåˆ¶ã€‚ä¸€æ–¹é¢ï¼Œå®ƒä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹çš„åŸå§‹æ¨¡å—ï¼Œä»è€Œç»§æ‰¿äº†å®ƒä»¬çš„å›ºæœ‰ä¼˜åŠ¿ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒé€šè¿‡ç©¿æ’å¤šæ¨¡æ€è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°äº†ä¸åŒæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚è¿™ç§åŒé‡èåˆæœºåˆ¶ä½¿å¾—æ¨¡å‹æ—¢èƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œåˆèƒ½é™ä½è®­ç»ƒæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šLightFusionçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š(1) é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºåŸºç¡€ç¼–ç å™¨ï¼›(2) è®¾è®¡é«˜æ•ˆçš„å¤šæ¨¡æ€è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥å®ç°ä¸åŒæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆäº¤äº’ï¼›(3) ä½¿ç”¨åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚éœ€è¦åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LightFusionåœ¨GenEvalï¼ˆç»„åˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼‰ä¸Šå–å¾—äº†0.91çš„æˆç»©ï¼Œåœ¨DPG-Benchï¼ˆå¤æ‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼‰ä¸Šå–å¾—äº†82.16çš„æˆç»©ï¼Œåœ¨GEditBenchå’ŒImgEdit-Benchï¼ˆå›¾åƒç¼–è¾‘ï¼‰ä¸Šåˆ†åˆ«å–å¾—äº†6.06å’Œ3.77çš„æˆç»©ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLightFusionåœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LightFusionå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€å¯¹è¯ç­‰ã€‚è¯¥ç ”ç©¶é™ä½äº†ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„è®­ç»ƒæˆæœ¬ï¼Œä½¿å¾—æ›´å¤šç ”ç©¶è€…å’Œå¼€å‘è€…èƒ½å¤Ÿå‚ä¸åˆ°å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„ç ”ç©¶å’Œåº”ç”¨ä¸­æ¥ã€‚æœªæ¥ï¼ŒLightFusionå¯ä»¥åº”ç”¨äºæ™ºèƒ½è®¾è®¡ã€å†…å®¹åˆ›ä½œã€äººæœºäº¤äº’ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.

