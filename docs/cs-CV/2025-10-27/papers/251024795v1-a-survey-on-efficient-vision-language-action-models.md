---
layout: default
title: A Survey on Efficient Vision-Language-Action Models
---

# A Survey on Efficient Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.24795" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.24795v1</a>
  <a href="https://arxiv.org/pdf/2510.24795.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.24795v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.24795v1', 'A Survey on Efficient Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Nicu Sebe, Heng Tao Shen

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

**å¤‡æ³¨**: 26 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**å¯¹é«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆEfficient VLAï¼‰çš„ç»¼è¿°ï¼Œæ—¨åœ¨é™ä½è®¡ç®—å’Œæ•°æ®éœ€æ±‚ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å…·èº«æ™ºèƒ½` `é«˜æ•ˆæ¨¡å‹` `æ¨¡å‹å‹ç¼©` `é«˜æ•ˆè®­ç»ƒ` `æ•°æ®æ”¶é›†` `æœºå™¨äºº` `ç»¼è¿°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹è®¡ç®—å’Œæ•°æ®éœ€æ±‚å·¨å¤§ï¼Œé˜»ç¢äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚
2. è¯¥ç»¼è¿°å¯¹Efficient VLAè¿›è¡Œå…¨é¢å›é¡¾ï¼Œä»æ•°æ®ã€æ¨¡å‹å’Œè®­ç»ƒä¸‰ä¸ªç»´åº¦ç³»ç»Ÿåœ°ç»„ç»‡ç°æœ‰æ–¹æ³•ã€‚
3. æ€»ç»“äº†Efficient VLAçš„ä»£è¡¨æ€§åº”ç”¨ã€å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰æ˜¯å…·èº«æ™ºèƒ½é¢†åŸŸçš„é‡è¦å‰æ²¿ï¼Œæ—¨åœ¨è¿æ¥æ•°å­—çŸ¥è¯†ä¸ç‰©ç†ä¸–ç•Œäº¤äº’ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å±•ç¤ºäº†å“è¶Šçš„é€šç”¨èƒ½åŠ›ï¼Œä½†å…¶éƒ¨ç½²å—åˆ°åº•å±‚å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹å¸¦æ¥çš„å·¨å¤§è®¡ç®—å’Œæ•°æ®éœ€æ±‚çš„ä¸¥é‡é˜»ç¢ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬ç»¼è¿°é¦–æ¬¡å…¨é¢å›é¡¾äº†æ•´ä¸ªæ•°æ®-æ¨¡å‹-è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆEfficient VLAï¼‰ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ³•æ¥ç³»ç»Ÿåœ°ç»„ç»‡è¯¥é¢†åŸŸä¸­çš„ä¸åŒå·¥ä½œï¼Œå°†å½“å‰æŠ€æœ¯åˆ†ä¸ºä¸‰ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼šï¼ˆ1ï¼‰é«˜æ•ˆæ¨¡å‹è®¾è®¡ï¼Œä¾§é‡äºé«˜æ•ˆæ¶æ„å’Œæ¨¡å‹å‹ç¼©ï¼›ï¼ˆ2ï¼‰é«˜æ•ˆè®­ç»ƒï¼Œå‡å°‘æ¨¡å‹å­¦ä¹ æœŸé—´çš„è®¡ç®—è´Ÿæ‹…ï¼›ï¼ˆ3ï¼‰é«˜æ•ˆæ•°æ®æ”¶é›†ï¼Œè§£å†³è·å–å’Œåˆ©ç”¨æœºå™¨äººæ•°æ®çš„ç“¶é¢ˆã€‚é€šè¿‡å¯¹è¯¥æ¡†æ¶å†…æœ€å…ˆè¿›æ–¹æ³•çš„æ‰¹åˆ¤æ€§å›é¡¾ï¼Œæœ¬ç»¼è¿°ä¸ä»…ä¸ºç¤¾åŒºå»ºç«‹äº†åŸºç¡€å‚è€ƒï¼Œè¿˜æ€»ç»“äº†ä»£è¡¨æ€§åº”ç”¨ï¼Œæè¿°äº†å…³é”®æŒ‘æˆ˜ï¼Œå¹¶è§„åˆ’äº†æœªæ¥ç ”ç©¶çš„è·¯çº¿å›¾ã€‚æˆ‘ä»¬ç»´æŠ¤ä¸€ä¸ªæŒç»­æ›´æ–°çš„é¡¹ç›®é¡µé¢æ¥è·Ÿè¸ªæˆ‘ä»¬çš„æœ€æ–°è¿›å±•ï¼šhttps://evla-survey.github.io/

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰è™½ç„¶åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶å¯¹è®¡ç®—èµ„æºå’Œæ•°æ®çš„éœ€æ±‚éå¸¸é«˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å®é™…æœºå™¨äººåº”ç”¨ä¸­çš„éƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„åœºæ™¯ä¸‹ã€‚å› æ­¤ï¼Œå¦‚ä½•é™ä½VLAæ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦å’Œæ•°æ®ä¾èµ–æ€§ï¼Œå®ç°é«˜æ•ˆçš„VLAï¼Œæ˜¯å½“å‰é¢ä¸´çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥ç»¼è¿°çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç°æœ‰çš„Efficient VLAæ–¹æ³•æŒ‰ç…§æ•°æ®ã€æ¨¡å‹å’Œè®­ç»ƒä¸‰ä¸ªç»´åº¦è¿›è¡Œç³»ç»Ÿæ€§åœ°ç»„ç»‡å’Œåˆ†ç±»ã€‚é€šè¿‡è¿™ç§åˆ†ç±»ï¼Œå¯ä»¥æ›´æ¸…æ™°åœ°ç†è§£ä¸åŒæ–¹æ³•ä¹‹é—´çš„è”ç³»å’ŒåŒºåˆ«ï¼Œä»è€Œä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªå…¨é¢çš„è§†è§’ï¼Œå¹¶ä¿ƒè¿›æ–°çš„é«˜æ•ˆVLAæ–¹æ³•çš„è®¾è®¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç»¼è¿°æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ¡†æ¶ï¼Œå°†Efficient VLAæ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼š(1) é«˜æ•ˆæ¨¡å‹è®¾è®¡ï¼šå…³æ³¨æ¨¡å‹æ¶æ„çš„ä¼˜åŒ–å’Œæ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œä¾‹å¦‚çŸ¥è¯†è’¸é¦ã€å‰ªæç­‰ï¼›(2) é«˜æ•ˆè®­ç»ƒï¼šæ—¨åœ¨å‡å°‘æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—è´Ÿæ‹…ï¼Œä¾‹å¦‚ä½¿ç”¨æ›´æœ‰æ•ˆçš„ä¼˜åŒ–ç®—æ³•ã€å‡å°‘è®­ç»ƒæ•°æ®é‡ç­‰ï¼›(3) é«˜æ•ˆæ•°æ®æ”¶é›†ï¼šè§£å†³æœºå™¨äººæ•°æ®è·å–çš„ç“¶é¢ˆé—®é¢˜ï¼Œä¾‹å¦‚ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ã€æ•°æ®å¢å¼ºç­‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç»¼è¿°çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡å¯¹Efficient VLAé¢†åŸŸè¿›è¡Œäº†å…¨é¢çš„æ¢³ç†å’Œæ€»ç»“ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ¡†æ¶ã€‚è¿™ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„è·¯çº¿å›¾ï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°ç†è§£ç°æœ‰æ–¹æ³•ï¼Œå¹¶æ‰¾åˆ°æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜æŒ‡å‡ºäº†Efficient VLAé¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œä¾‹å¦‚å¦‚ä½•åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶è¿›ä¸€æ­¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥ç»¼è¿°æœ¬èº«å¹¶æ²¡æœ‰æå‡ºæ–°çš„æŠ€æœ¯è®¾è®¡ï¼Œè€Œæ˜¯å¯¹ç°æœ‰æŠ€æœ¯è¿›è¡Œäº†åˆ†ç±»å’Œæ€»ç»“ã€‚ä½†æ˜¯ï¼Œåœ¨å„ä¸ªåˆ†ç±»ä¸‹ï¼Œéƒ½åŒ…å«äº†è®¸å¤šå…³é”®çš„æŠ€æœ¯ç»†èŠ‚ã€‚ä¾‹å¦‚ï¼Œåœ¨é«˜æ•ˆæ¨¡å‹è®¾è®¡æ–¹é¢ï¼Œå¸¸ç”¨çš„æŠ€æœ¯åŒ…æ‹¬MobileNetã€ShuffleNetç­‰è½»é‡çº§ç½‘ç»œæ¶æ„ï¼Œä»¥åŠæ¨¡å‹å‰ªæã€é‡åŒ–ã€çŸ¥è¯†è’¸é¦ç­‰æ¨¡å‹å‹ç¼©æŠ€æœ¯ã€‚åœ¨é«˜æ•ˆè®­ç»ƒæ–¹é¢ï¼Œå¸¸ç”¨çš„æŠ€æœ¯åŒ…æ‹¬æ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦è®­ç»ƒã€çŸ¥è¯†è’¸é¦ç­‰ã€‚åœ¨é«˜æ•ˆæ•°æ®æ”¶é›†æ–¹é¢ï¼Œå¸¸ç”¨çš„æŠ€æœ¯åŒ…æ‹¬æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆã€æ•°æ®å¢å¼ºã€ä¸»åŠ¨å­¦ä¹ ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°å…¨é¢å›é¡¾äº†Efficient VLAé¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ¡†æ¶ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„è·¯çº¿å›¾ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„åˆ†æå’Œæ€»ç»“ï¼ŒæŒ‡å‡ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚è¯¥ç»¼è¿°ç»´æŠ¤äº†ä¸€ä¸ªæŒç»­æ›´æ–°çš„é¡¹ç›®é¡µé¢ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è·Ÿè¸ªæœ€æ–°çš„è¿›å±•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯¹æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚é€šè¿‡é™ä½VLAæ¨¡å‹çš„è®¡ç®—å’Œæ•°æ®éœ€æ±‚ï¼Œå¯ä»¥ä½¿è¿™äº›æ¨¡å‹æ›´å®¹æ˜“éƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„æœºå™¨äººåº”ç”¨ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºä½åŠŸè€—çš„ç§»åŠ¨æœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œå¯¼èˆªã€ç‰©ä½“è¯†åˆ«å’Œæ“ä½œç­‰ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/

