---
layout: default
title: A Survey on Efficient Vision-Language-Action Models
---

# A Survey on Efficient Vision-Language-Action Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.24795" target="_blank" class="toolbar-btn">arXiv: 2510.24795v1</a>
    <a href="https://arxiv.org/pdf/2510.24795.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.24795v1" 
            onclick="toggleFavorite(this, '2510.24795v1', 'A Survey on Efficient Vision-Language-Action Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Nicu Sebe, Heng Tao Shen

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27

**Â§áÊ≥®**: 26 pages, 8 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÂØπÈ´òÊïàËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºàEfficient VLAÔºâÁöÑÁªºËø∞ÔºåÊó®Âú®Èôç‰ΩéËÆ°ÁÆóÂíåÊï∞ÊçÆÈúÄÊ±Ç„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `ÂÖ∑Ë∫´Êô∫ËÉΩ` `È´òÊïàÊ®°Âûã` `Ê®°ÂûãÂéãÁº©` `È´òÊïàËÆ≠ÁªÉ` `Êï∞ÊçÆÊî∂ÈõÜ` `Êú∫Âô®‰∫∫` `ÁªºËø∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãËÆ°ÁÆóÂíåÊï∞ÊçÆÈúÄÊ±ÇÂ∑®Â§ßÔºåÈòªÁ¢ç‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇ
2. ËØ•ÁªºËø∞ÂØπEfficient VLAËøõË°åÂÖ®Èù¢ÂõûÈ°æÔºå‰ªéÊï∞ÊçÆ„ÄÅÊ®°ÂûãÂíåËÆ≠ÁªÉ‰∏â‰∏™Áª¥Â∫¶Á≥ªÁªüÂú∞ÁªÑÁªáÁé∞ÊúâÊñπÊ≥ï„ÄÇ
3. ÊÄªÁªì‰∫ÜEfficient VLAÁöÑ‰ª£Ë°®ÊÄßÂ∫îÁî®„ÄÅÂÖ≥ÈîÆÊåëÊàòÔºåÂπ∂‰∏∫Êú™Êù•Á†îÁ©∂ÊñπÂêëÊèê‰æõ‰∫ÜÊåáÂØº„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºàVLAÔºâÊòØÂÖ∑Ë∫´Êô∫ËÉΩÈ¢ÜÂüüÁöÑÈáçË¶ÅÂâçÊ≤øÔºåÊó®Âú®ËøûÊé•Êï∞Â≠óÁü•ËØÜ‰∏éÁâ©ÁêÜ‰∏ñÁïå‰∫§‰∫í„ÄÇÂ∞ΩÁÆ°Ëøô‰∫õÊ®°ÂûãÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÈÄöÁî®ËÉΩÂäõÔºå‰ΩÜÂÖ∂ÈÉ®ÁΩ≤ÂèóÂà∞Â∫ïÂ±ÇÂ§ßËßÑÊ®°Âü∫Á°ÄÊ®°ÂûãÂ∏¶Êù•ÁöÑÂ∑®Â§ßËÆ°ÁÆóÂíåÊï∞ÊçÆÈúÄÊ±ÇÁöÑ‰∏•ÈáçÈòªÁ¢ç„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊú¨ÁªºËø∞È¶ñÊ¨°ÂÖ®Èù¢ÂõûÈ°æ‰∫ÜÊï¥‰∏™Êï∞ÊçÆ-Ê®°Âûã-ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÈ´òÊïàËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºàEfficient VLAÔºâ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂàÜÁ±ªÊ≥ïÊù•Á≥ªÁªüÂú∞ÁªÑÁªáËØ•È¢ÜÂüü‰∏≠ÁöÑ‰∏çÂêåÂ∑•‰ΩúÔºåÂ∞ÜÂΩìÂâçÊäÄÊúØÂàÜ‰∏∫‰∏â‰∏™Ê†∏ÂøÉÊîØÊü±ÔºöÔºà1ÔºâÈ´òÊïàÊ®°ÂûãËÆæËÆ°Ôºå‰æßÈáç‰∫éÈ´òÊïàÊû∂ÊûÑÂíåÊ®°ÂûãÂéãÁº©ÔºõÔºà2ÔºâÈ´òÊïàËÆ≠ÁªÉÔºåÂáèÂ∞ëÊ®°ÂûãÂ≠¶‰π†ÊúüÈó¥ÁöÑËÆ°ÁÆóË¥üÊãÖÔºõÔºà3ÔºâÈ´òÊïàÊï∞ÊçÆÊî∂ÈõÜÔºåËß£ÂÜ≥Ëé∑ÂèñÂíåÂà©Áî®Êú∫Âô®‰∫∫Êï∞ÊçÆÁöÑÁì∂È¢à„ÄÇÈÄöËøáÂØπËØ•Ê°ÜÊû∂ÂÜÖÊúÄÂÖàËøõÊñπÊ≥ïÁöÑÊâπÂà§ÊÄßÂõûÈ°æÔºåÊú¨ÁªºËø∞‰∏ç‰ªÖ‰∏∫Á§æÂå∫Âª∫Á´ã‰∫ÜÂü∫Á°ÄÂèÇËÄÉÔºåËøòÊÄªÁªì‰∫Ü‰ª£Ë°®ÊÄßÂ∫îÁî®ÔºåÊèèËø∞‰∫ÜÂÖ≥ÈîÆÊåëÊàòÔºåÂπ∂ËßÑÂàí‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑË∑ØÁ∫øÂõæ„ÄÇÊàë‰ª¨Áª¥Êä§‰∏Ä‰∏™ÊåÅÁª≠Êõ¥Êñ∞ÁöÑÈ°πÁõÆÈ°µÈù¢Êù•Ë∑üË∏™Êàë‰ª¨ÁöÑÊúÄÊñ∞ËøõÂ±ïÔºöhttps://evla-survey.github.io/

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºàVLAÔºâËôΩÁÑ∂Âú®ÂÖ∑Ë∫´Êô∫ËÉΩÈ¢ÜÂüüÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑËÉΩÂäõÔºå‰ΩÜÂÖ∂ÂØπËÆ°ÁÆóËµÑÊ∫êÂíåÊï∞ÊçÆÁöÑÈúÄÊ±ÇÈùûÂ∏∏È´òÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®ÂÆûÈôÖÊú∫Âô®‰∫∫Â∫îÁî®‰∏≠ÁöÑÈÉ®ÁΩ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËµÑÊ∫êÊúâÈôêÁöÑÂú∫ÊôØ‰∏ã„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÈôç‰ΩéVLAÊ®°ÂûãÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÂíåÊï∞ÊçÆ‰æùËµñÊÄßÔºåÂÆûÁé∞È´òÊïàÁöÑVLAÔºåÊòØÂΩìÂâçÈù¢‰∏¥ÁöÑÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËØ•ÁªºËø∞ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÁé∞ÊúâÁöÑEfficient VLAÊñπÊ≥ïÊåâÁÖßÊï∞ÊçÆ„ÄÅÊ®°ÂûãÂíåËÆ≠ÁªÉ‰∏â‰∏™Áª¥Â∫¶ËøõË°åÁ≥ªÁªüÊÄßÂú∞ÁªÑÁªáÂíåÂàÜÁ±ª„ÄÇÈÄöËøáËøôÁßçÂàÜÁ±ªÔºåÂèØ‰ª•Êõ¥Ê∏ÖÊô∞Âú∞ÁêÜËß£‰∏çÂêåÊñπÊ≥ï‰πãÈó¥ÁöÑËÅîÁ≥ªÂíåÂå∫Âà´Ôºå‰ªéËÄå‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËßÜËßíÔºåÂπ∂‰øÉËøõÊñ∞ÁöÑÈ´òÊïàVLAÊñπÊ≥ïÁöÑËÆæËÆ°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÁªºËø∞ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂàÜÁ±ªÊ°ÜÊû∂ÔºåÂ∞ÜEfficient VLAÊñπÊ≥ïÂàÜ‰∏∫‰∏â‰∏™Ê†∏ÂøÉÊîØÊü±Ôºö(1) È´òÊïàÊ®°ÂûãËÆæËÆ°ÔºöÂÖ≥Ê≥®Ê®°ÂûãÊû∂ÊûÑÁöÑ‰ºòÂåñÂíåÊ®°ÂûãÂéãÁº©ÊäÄÊúØÔºå‰æãÂ¶ÇÁü•ËØÜËí∏È¶è„ÄÅÂâ™ÊûùÁ≠âÔºõ(2) È´òÊïàËÆ≠ÁªÉÔºöÊó®Âú®ÂáèÂ∞ëÊ®°ÂûãËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑËÆ°ÁÆóË¥üÊãÖÔºå‰æãÂ¶Ç‰ΩøÁî®Êõ¥ÊúâÊïàÁöÑ‰ºòÂåñÁÆóÊ≥ï„ÄÅÂáèÂ∞ëËÆ≠ÁªÉÊï∞ÊçÆÈáèÁ≠âÔºõ(3) È´òÊïàÊï∞ÊçÆÊî∂ÈõÜÔºöËß£ÂÜ≥Êú∫Âô®‰∫∫Êï∞ÊçÆËé∑ÂèñÁöÑÁì∂È¢àÈóÆÈ¢òÔºå‰æãÂ¶Ç‰ΩøÁî®Ê®°ÊãüÊï∞ÊçÆ„ÄÅÊï∞ÊçÆÂ¢ûÂº∫Á≠â„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÁªºËø∞ÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈ¶ñÊ¨°ÂØπEfficient VLAÈ¢ÜÂüüËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÊ¢≥ÁêÜÂíåÊÄªÁªìÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂàÜÁ±ªÊ°ÜÊû∂„ÄÇËøô‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê∏ÖÊô∞ÁöÑË∑ØÁ∫øÂõæÔºåÂ∏ÆÂä©‰ªñ‰ª¨Êõ¥Â•ΩÂú∞ÁêÜËß£Áé∞ÊúâÊñπÊ≥ïÔºåÂπ∂ÊâæÂà∞Êú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊ≠§Â§ñÔºåËØ•ÁªºËø∞ËøòÊåáÂá∫‰∫ÜEfficient VLAÈ¢ÜÂüüÈù¢‰∏¥ÁöÑÂÖ≥ÈîÆÊåëÊàòÔºå‰æãÂ¶ÇÂ¶Ç‰ΩïÂú®‰øùËØÅÊÄßËÉΩÁöÑÂêåÊó∂Ëøõ‰∏ÄÊ≠•Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÊúâÊïàÂú∞Âà©Áî®ÊúâÈôêÁöÑÊï∞ÊçÆËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËØ•ÁªºËø∞Êú¨Ë∫´Âπ∂Ê≤°ÊúâÊèêÂá∫Êñ∞ÁöÑÊäÄÊúØËÆæËÆ°ÔºåËÄåÊòØÂØπÁé∞ÊúâÊäÄÊúØËøõË°å‰∫ÜÂàÜÁ±ªÂíåÊÄªÁªì„ÄÇ‰ΩÜÊòØÔºåÂú®ÂêÑ‰∏™ÂàÜÁ±ª‰∏ãÔºåÈÉΩÂåÖÂê´‰∫ÜËÆ∏Â§öÂÖ≥ÈîÆÁöÑÊäÄÊúØÁªÜËäÇ„ÄÇ‰æãÂ¶ÇÔºåÂú®È´òÊïàÊ®°ÂûãËÆæËÆ°ÊñπÈù¢ÔºåÂ∏∏Áî®ÁöÑÊäÄÊúØÂåÖÊã¨MobileNet„ÄÅShuffleNetÁ≠âËΩªÈáèÁ∫ßÁΩëÁªúÊû∂ÊûÑÔºå‰ª•ÂèäÊ®°ÂûãÂâ™Êûù„ÄÅÈáèÂåñ„ÄÅÁü•ËØÜËí∏È¶èÁ≠âÊ®°ÂûãÂéãÁº©ÊäÄÊúØ„ÄÇÂú®È´òÊïàËÆ≠ÁªÉÊñπÈù¢ÔºåÂ∏∏Áî®ÁöÑÊäÄÊúØÂåÖÊã¨Ê¢ØÂ∫¶Á¥ØÁßØ„ÄÅÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ„ÄÅÁü•ËØÜËí∏È¶èÁ≠â„ÄÇÂú®È´òÊïàÊï∞ÊçÆÊî∂ÈõÜÊñπÈù¢ÔºåÂ∏∏Áî®ÁöÑÊäÄÊúØÂåÖÊã¨Ê®°ÊãüÊï∞ÊçÆÁîüÊàê„ÄÅÊï∞ÊçÆÂ¢ûÂº∫„ÄÅ‰∏ªÂä®Â≠¶‰π†Á≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÁªºËø∞ÂÖ®Èù¢ÂõûÈ°æ‰∫ÜEfficient VLAÈ¢ÜÂüüÁöÑÁ†îÁ©∂ËøõÂ±ïÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂàÜÁ±ªÊ°ÜÊû∂Ôºå‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê∏ÖÊô∞ÁöÑË∑ØÁ∫øÂõæ„ÄÇÈÄöËøáÂØπÁé∞ÊúâÊñπÊ≥ïÁöÑÂàÜÊûêÂíåÊÄªÁªìÔºåÊåáÂá∫‰∫ÜËØ•È¢ÜÂüüÈù¢‰∏¥ÁöÑÂÖ≥ÈîÆÊåëÊàòÂíåÊú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇËØ•ÁªºËø∞Áª¥Êä§‰∫Ü‰∏Ä‰∏™ÊåÅÁª≠Êõ¥Êñ∞ÁöÑÈ°πÁõÆÈ°µÈù¢ÔºåÊñπ‰æøÁ†îÁ©∂‰∫∫ÂëòË∑üË∏™ÊúÄÊñ∞ÁöÑËøõÂ±ï„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂØπÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇÈÄöËøáÈôç‰ΩéVLAÊ®°ÂûãÁöÑËÆ°ÁÆóÂíåÊï∞ÊçÆÈúÄÊ±ÇÔºåÂèØ‰ª•‰ΩøËøô‰∫õÊ®°ÂûãÊõ¥ÂÆπÊòìÈÉ®ÁΩ≤Âú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥È´òÊïàÁöÑÊú∫Âô®‰∫∫Â∫îÁî®„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Â∫îÁî®‰∫é‰ΩéÂäüËÄóÁöÑÁßªÂä®Êú∫Âô®‰∫∫Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÊâßË°åÂØºËà™„ÄÅÁâ©‰ΩìËØÜÂà´ÂíåÊìç‰ΩúÁ≠â‰ªªÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/

