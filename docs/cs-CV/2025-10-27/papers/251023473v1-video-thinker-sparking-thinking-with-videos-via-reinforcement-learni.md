---
layout: default
title: Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning
---

# Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23473" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23473v1</a>
  <a href="https://arxiv.org/pdf/2510.23473.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23473v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.23473v1', 'Video-Thinker: Sparking &quot;Thinking with Videos&quot; via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shijian Wang, Jiarui Jin, Xingjian Wang, Linxin Song, Runhao Fu, Hecheng Wang, Zongyuan Ge, Yuan Lu, Xuelian Cheng

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVideo-Thinkerï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ èµ‹èƒ½MLLMè¿›è¡Œè§†é¢‘æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ¨ç†` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ€ç»´é“¾` `è‡ªä¸»æ¨ç†` `è§†é¢‘ç†è§£` `å®šä½` `æè¿°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹åŠ¨æ€æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨è§†é¢‘ä¿¡æ¯è¿›è¡Œå¤æ‚æ¨ç†ã€‚
2. Video-Thinkeråˆ©ç”¨MLLMè‡ªèº«èƒ½åŠ›ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªä¸»ç”Ÿæˆæ¨ç†çº¿ç´¢ï¼Œæ— éœ€å¤–éƒ¨å·¥å…·ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVideo-Thinkeråœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°SOTAã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºVideo-Thinkerï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ èµ‹èƒ½å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œè§†é¢‘æ¨ç†ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªä¸»åˆ©ç”¨å…¶å†…åœ¨çš„â€œå®šä½â€å’Œâ€œæè¿°â€èƒ½åŠ›ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆæ¨ç†çº¿ç´¢ã€‚ä¸ºäº†æ¿€å‘è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºVideo-Thinker-10Kçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«åœ¨æ€ç»´é“¾æ¨ç†åºåˆ—ä¸­è‡ªä¸»å·¥å…·ä½¿ç”¨çš„ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„è®­ç»ƒç­–ç•¥é¦–å…ˆä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥å­¦ä¹ æ¨ç†æ ¼å¼ï¼Œç„¶åä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥åŠ å¼ºè¿™ç§æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒVideo-Thinkerä½¿MLLMèƒ½å¤Ÿè‡ªä¸»åœ°è¿›è¡Œè§†é¢‘æ¨ç†çš„å®šä½å’Œæè¿°ä»»åŠ¡ï¼Œè€Œæ— éœ€æ„å»ºå’Œè°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVideo-Thinkeråœ¨é¢†åŸŸå†…ä»»åŠ¡å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸå¤–è§†é¢‘æ¨ç†åŸºå‡†ï¼ˆåŒ…æ‹¬Video-Holmesã€CG-Bench-Reasoningå’ŒVRBenchï¼‰ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„Video-Thinker-7Bæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼ˆå¦‚Video-R1ï¼‰ï¼Œå¹¶åœ¨7Bå¤§å°çš„MLLMä¸­å»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç¼ºä¹å¯¹è§†é¢‘è¿›è¡ŒåŠ¨æ€æ¨ç†çš„èƒ½åŠ›ã€‚å®ƒä»¬éš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶åºä¿¡æ¯å’ŒåŠ¨æ€å˜åŒ–ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨å¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„å¤–éƒ¨å·¥å…·æˆ–æ¨¡å—ï¼Œç¼ºä¹è‡ªä¸»æ€§å’Œçµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVideo-Thinkerçš„æ ¸å¿ƒæ€è·¯æ˜¯èµ‹äºˆMLLMè‡ªä¸»â€œæ€è€ƒâ€è§†é¢‘çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œé€šè¿‡è§‚å¯Ÿå’Œåˆ†æè§†é¢‘å†…å®¹ï¼Œé€æ­¥ç”Ÿæˆæ¨ç†çº¿ç´¢ï¼Œæœ€ç»ˆå®Œæˆæ¨ç†ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹å¤–éƒ¨å·¥å…·çš„ä¾èµ–ï¼Œå……åˆ†åˆ©ç”¨äº†MLLMè‡ªèº«æ‰€å…·å¤‡çš„â€œå®šä½â€å’Œâ€œæè¿°â€èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVideo-Thinkerçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨Video-Thinker-10Kæ•°æ®é›†ä¸Šè®­ç»ƒMLLMï¼Œä½¿å…¶å­¦ä¹ è§†é¢‘æ¨ç†çš„åŸºæœ¬æ ¼å¼å’Œæµç¨‹ã€‚ç„¶åï¼Œä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›ä¸€æ­¥æå‡MLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è§†é¢‘ä¿¡æ¯ç”Ÿæˆæœ‰æ•ˆçš„æ¨ç†çº¿ç´¢ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒMLLMè‡ªä¸»åœ°è¿›è¡Œå®šä½å’Œæè¿°ä»»åŠ¡ï¼Œç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œæœ€ç»ˆå¾—åˆ°æ¨ç†ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šVideo-Thinkeræœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶è‡ªä¸»æ¨ç†çš„æ¨¡å¼ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒVideo-Thinkerä¸ä¾èµ–äºé¢„å®šä¹‰çš„å¤–éƒ¨å·¥å…·æˆ–æ¨¡å—ï¼Œè€Œæ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ èµ‹äºˆMLLMè‡ªä¸»ç”Ÿæˆæ¨ç†çº¿ç´¢çš„èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—MLLMèƒ½å¤Ÿæ›´åŠ çµæ´»åœ°é€‚åº”ä¸åŒçš„è§†é¢‘æ¨ç†ä»»åŠ¡ï¼Œå¹¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è§†é¢‘ä¸­çš„åŠ¨æ€ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šVideo-Thinkerçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) Video-Thinker-10Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«åœ¨æ€ç»´é“¾æ¨ç†åºåˆ—ä¸­è‡ªä¸»å·¥å…·ä½¿ç”¨çš„ç¤ºä¾‹ï¼Œç”¨äºè®­ç»ƒMLLMçš„æ¨ç†èƒ½åŠ›ï¼›2) ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç”¨äºè¿›ä¸€æ­¥æå‡MLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨è§†é¢‘ä¿¡æ¯ç”Ÿæˆæœ‰æ•ˆçš„æ¨ç†çº¿ç´¢ï¼›3) è‡ªä¸»å®šä½å’Œæè¿°ä»»åŠ¡ï¼ŒMLLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»åœ°è¿›è¡Œå®šä½å’Œæè¿°ä»»åŠ¡ï¼Œç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œæœ€ç»ˆå¾—åˆ°æ¨ç†ç»“æœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Video-Thinker-7Båœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚åœ¨Video-Holmesä¸Šï¼ŒVideo-Thinker-7Bçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼ˆå¦‚Video-R1ï¼‰ï¼Œå¹¶åœ¨7Bå¤§å°çš„MLLMä¸­å»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨CG-Bench-Reasoningå’ŒVRBenchç­‰å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVideo-Thinkerä¹Ÿè¡¨ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Video-Thinkerå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½ç›‘æ§ã€è§†é¢‘å†…å®¹åˆ†æã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºè¯†åˆ«å¼‚å¸¸è¡Œä¸ºã€ç†è§£è§†é¢‘å†…å®¹ã€è¾…åŠ©é©¾é©¶å†³ç­–ã€ä»¥åŠæŒ‡å¯¼æœºå™¨äººå®Œæˆå¤æ‚ä»»åŠ¡ã€‚é€šè¿‡èµ‹äºˆæœºå™¨è‡ªä¸»æ€è€ƒè§†é¢‘çš„èƒ½åŠ›ï¼ŒVideo-Thinkeræœ‰æœ›æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è§†é¢‘ç†è§£é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in image reasoning methods, particularly "Thinking with Images", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic "grounding" and "captioning" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.

