---
layout: default
title: "On the Faithfulness of Visual Thinking: Measurement and Enhancement"
---

# On the Faithfulness of Visual Thinking: Measurement and Enhancement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23482" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.23482v1</a>
  <a href="https://arxiv.org/pdf/2510.23482.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23482v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.23482v1', 'On the Faithfulness of Visual Thinking: Measurement and Enhancement')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zujing Liu, Junwen Pan, Qi She, Yuan Gao, Guisong Xia

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/EugeneLiu01/Faithful_Thinking_with_Image)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SCCMÂ≠¶‰π†Á≠ñÁï•ÔºåÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠ËßÜËßâ‰ø°ÊÅØÁöÑÂèØÈù†ÊÄßÂíåÂÖÖÂàÜÊÄß„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Â§öÊ®°ÊÄÅÊé®ÁêÜ` `ÊÄùÁª¥Èìæ` `Âº∫ÂåñÂ≠¶‰π†` `ËßÜËßâÁúüÂÆûÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öÊ®°ÊÄÅÊÄùÁª¥Èìæ(MCoT)Êé®ÁêÜËøáÁ®ãÁº∫‰πèÁúüÂÆûÊÄßÔºåËßÜËßâ‰ø°ÊÅØÂ∏∏Ë¢´ÂøΩÁï•„ÄÇ
2. ÊèêÂá∫ÂÖÖÂàÜÁªÑ‰ª∂Âõ†ÊûúÊ®°Âûã(SCCM)Â≠¶‰π†Á≠ñÁï•ÔºåÈºìÂä±MCoTÁîüÊàêÂÖÖÂàÜ‰∏îÊúÄÂ∞èÁöÑËßÜËßâÁªÑ‰ª∂‰ª•ÂÆûÁé∞Áã¨Á´ãÊé®ÁêÜ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåSCCMËÉΩÊòæËëóÊèêÂçáÊ®°ÂûãÂú®ÁªÜÁ≤íÂ∫¶ÊÑüÁü•ÂíåÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑËßÜËßâÁúüÂÆûÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂΩìÂâçÁöÑÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã(LVLMs)Âú®ÁªèËøáÂº∫ÂåñÂæÆË∞É(RFT)ÂêéÔºåËÉΩÂ§üÁîüÊàêËßÜËßâ-ÊñáÊú¨Â§öÊ®°ÊÄÅÊÄùÁª¥Èìæ(MCoT)„ÄÇÁÑ∂ËÄåÔºåÊàë‰ª¨ËßÇÂØüÂà∞MCoT‰∏≠ÂåÖÂê´ÁöÑËßÜËßâ‰ø°ÊÅØÈÄöÂ∏∏ÊòØ‰∏çÂáÜÁ°ÆÁöÑÔºåÂ∞ΩÁÆ°‰ªçÁÑ∂ÂèØ‰ª•‰∫ßÁîüÊ≠£Á°ÆÁöÑÁ≠îÊ°àÔºåËøôË°®ÊòéMCoTÊé®ÁêÜËøáÁ®ãÁº∫‰πèÁúüÂÆûÊÄß„ÄÇÊàë‰ª¨Â∞ÜËøôÁßç‰∏çÁúüÂÆûÊÄßÂΩíÂõ†‰∫éRFT‰∏≠ÁöÑÂº∫ÂåñÂ≠¶‰π†Â•ñÂä±ÔºåÂÆÉ‰ªÖ‰ªÖÊøÄÂä±‰∫§ÈîôÁöÑËßÜËßâ-ÊñáÊú¨ÊèêÁ§∫ÁöÑÊ†ºÂºèÔºåÂç≥ÂÆÉÈºìÂä±Ê®°ÂûãÂ∞ÜËßÜËßâ‰ø°ÊÅØÁ∫≥ÂÖ•ÂÖ∂ÊñáÊú¨Êé®ÁêÜÊ≠•È™§ÔºåËÄå‰∏çËÄÉËôëËßÜËßâ‰ø°ÊÅØÁöÑÊ≠£Á°ÆÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨È¶ñÂÖàÈÄöËøáÊµãÈáèÂΩìÂÖ∂ËßÜËßâÂíåÊñáÊú¨ÊÄùÊÉ≥ÂèóÂà∞Âπ≤È¢ÑÊó∂È¢ÑÊµãÁöÑÂèòÂåñÁ®ãÂ∫¶Êù•Êé¢ÊµãMCoTÁöÑÁúüÂÆûÊÄß„ÄÇ‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊòØÔºåÊ®°ÂûãÁöÑÈ¢ÑÊµãÂú®ËßÜËßâÂπ≤È¢Ñ‰∏ãÂá†‰πé‰øùÊåÅ‰∏çÂèòÔºå‰ΩÜÂú®ÊñáÊú¨Âπ≤È¢Ñ‰∏ãÂèëÁîüÊòæËëóÂèòÂåñÔºåË°®ÊòéËßÜËßâËØÅÊçÆÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äË¢´ÂøΩÁï•„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÂàÜÊûêËßÜËßâ‰ø°ÊÅØÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫éLVLMÁöÑËá™Âä®ËØÑ‰º∞ÊåáÊ†áÔºåËØ•ÊåáÊ†á‰ªéÂèØÈù†ÊÄßÂíåÂÖÖÂàÜÊÄß‰∏§‰∏™ËßíÂ∫¶ÈáèÂåñËßÜËßâÁ∫øÁ¥¢ÁöÑÁúüÂÆûÊÄß„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåÂΩìÂâçMCoTËΩ®Ëøπ‰∏≠ÁöÑËßÜËßâ‰ø°ÊÅØÂêåÊó∂ÊòØ‰∏çÂèØÈù†Âíå‰∏çÂÖÖÂàÜÁöÑ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑMCoTÂ≠¶‰π†Á≠ñÁï•ÔºåÁß∞‰∏∫ÂÖÖÂàÜÁªÑ‰ª∂Âõ†ÊûúÊ®°Âûã(SCCM)Â≠¶‰π†„ÄÇËøôÁßçÊñπÊ≥ïÈºìÂä±MCoTÁîüÊàêÂÖÖÂàÜ‰ΩÜÊúÄÂ∞èÁöÑËßÜËßâÁªÑ‰ª∂ÔºåËøô‰∫õÁªÑ‰ª∂ËÉΩÂ§üÁã¨Á´ãÂú∞ÂØºËá¥Ê≠£Á°ÆÁöÑÁ≠îÊ°à„ÄÇÊàë‰ª¨Ê≥®ÊÑèÂà∞ÔºåÊâÄÊèêÂá∫ÁöÑSCCMÊòØÊó†Ê†áÊ≥®ÁöÑÔºåÂπ∂‰∏îÂèØ‰ª•‰ª•Âç≥ÊèíÂç≥Áî®ÁöÑÊñπÂºè‰∏éÂêÑÁßçÁî®‰∫éMCoTÁöÑRFTÂÖºÂÆπ„ÄÇÁªèÈ™åÁªìÊûúË°®ÊòéÔºåSCCMÊåÅÁª≠ÊèêÈ´ò‰∫ÜÂú®‰∏ÄÁ≥ªÂàóÁªÜÁ≤íÂ∫¶ÊÑüÁü•ÂíåÊé®ÁêÜÂü∫ÂáÜ‰∏äÁöÑËßÜËßâÁúüÂÆûÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÔºàRFTÔºâÁîüÊàêÂ§öÊ®°ÊÄÅÊÄùÁª¥ÈìæÔºàMCoTÔºâÔºå‰ΩÜMCoT‰∏≠ËßÜËßâ‰ø°ÊÅØÁöÑÂáÜÁ°ÆÊÄß‰∏çË∂≥ÔºåÊ®°ÂûãÂÄæÂêë‰∫é‰æùËµñÊñáÊú¨‰ø°ÊÅØËøõË°åÊé®ÁêÜÔºåÂøΩÁï•ËßÜËßâËØÅÊçÆ„ÄÇÁé∞ÊúâRFTÊñπÊ≥ïÂè™ÂÖ≥Ê≥®ËßÜËßâ‰ø°ÊÅØÊòØÂê¶Ë¢´‰ΩøÁî®ÔºåËÄåÂøΩÁï•‰∫ÜËßÜËßâ‰ø°ÊÅØÁöÑÊ≠£Á°ÆÊÄßÔºåÂØºËá¥ËßÜËßâ‰ø°ÊÅØ‰∏çÂèØÈù†‰∏î‰∏çÂÖÖÂàÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈºìÂä±Ê®°ÂûãÁîüÊàê‚ÄúÂÖÖÂàÜ‰∏îÊúÄÂ∞è‚ÄùÁöÑËßÜËßâÁªÑ‰ª∂ÔºåËøô‰∫õÁªÑ‰ª∂ËÉΩÂ§üÁã¨Á´ãÂú∞ÊîØÊåÅÊ≠£Á°ÆÁöÑÁ≠îÊ°à„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°Âûã‰∏çÂÜç‰ªÖ‰ªÖÂ∞ÜËßÜËßâ‰ø°ÊÅØ‰Ωú‰∏∫ÊñáÊú¨Êé®ÁêÜÁöÑËæÖÂä©ÔºåËÄåÊòØÂ∞ÜÂÖ∂‰Ωú‰∏∫Áã¨Á´ãÊé®ÁêÜÁöÑ‰æùÊçÆÔºå‰ªéËÄåÊèêÈ´òËßÜËßâ‰ø°ÊÅØÁöÑÂèØÈù†ÊÄßÂíåÂÖÖÂàÜÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSCCMÂ≠¶‰π†Á≠ñÁï•ÂèØ‰ª•‰∏éÁé∞ÊúâÁöÑRFTÊñπÊ≥ïÁªìÂêà‰ΩøÁî®Ôºå‰Ωú‰∏∫‰∏Ä‰∏™Âç≥ÊèíÂç≥Áî®ÁöÑÊ®°Âùó„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Ôºö1Ôºâ‰ΩøÁî®LVLMÁîüÊàêMCoTÔºõ2Ôºâ‰ΩøÁî®SCCMÂ≠¶‰π†Á≠ñÁï•‰ºòÂåñMCoTÔºåÈºìÂä±ÁîüÊàêÂÖÖÂàÜ‰∏îÊúÄÂ∞èÁöÑËßÜËßâÁªÑ‰ª∂Ôºõ3Ôºâ‰ΩøÁî®RFTËøõË°åÂæÆË∞É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‚ÄúÂÖÖÂàÜÁªÑ‰ª∂Âõ†ÊûúÊ®°ÂûãÔºàSCCMÔºâ‚ÄùÁöÑÊ¶ÇÂøµÔºåÂπ∂ËÆæËÆ°‰∫ÜÁõ∏Â∫îÁöÑÂ≠¶‰π†Á≠ñÁï•„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåSCCM‰∏ç‰æùËµñ‰∫éÈ¢ùÂ§ñÁöÑÊ†áÊ≥®Êï∞ÊçÆÔºåËÄåÊòØÈÄöËøáËá™ÁõëÁù£ÁöÑÊñπÂºèÂ≠¶‰π†ËßÜËßâ‰ø°ÊÅØÁöÑÂõ†ÊûúÂÖ≥Á≥ªÔºå‰ªéËÄåÊèêÈ´òËßÜËßâ‰ø°ÊÅØÁöÑË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöSCCMÂ≠¶‰π†Á≠ñÁï•ÁöÑÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïÂÆö‰πâÂíåË°°ÈáèËßÜËßâÁªÑ‰ª∂ÁöÑ‚ÄúÂÖÖÂàÜÊÄß‚ÄùÂíå‚ÄúÊúÄÂ∞èÊÄß‚Äù„ÄÇËÆ∫Êñá‰∏≠ÂèØËÉΩ‰ΩøÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•ÈºìÂä±Ê®°ÂûãÁîüÊàêÊó¢ËÉΩÁã¨Á´ãÊîØÊåÅÁ≠îÊ°àÔºåÂèàÂ∞ΩÂèØËÉΩÁÆÄÊ¥ÅÁöÑËßÜËßâÁªÑ‰ª∂„ÄÇÂÖ∑‰ΩìÁöÑÊäÄÊúØÁªÜËäÇÔºàÂ¶ÇÊçüÂ§±ÂáΩÊï∞ÁöÑÂÖ∑‰ΩìÂΩ¢Âºè„ÄÅÁΩëÁªúÁªìÊûÑÁöÑË∞ÉÊï¥Á≠âÔºâÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Ëøõ‰∏ÄÊ≠•Êü•Êâæ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSCCMÂ≠¶‰π†Á≠ñÁï•ËÉΩÂ§üÊòæËëóÊèêÈ´òÊ®°ÂûãÂú®ÁªÜÁ≤íÂ∫¶ÊÑüÁü•ÂíåÊé®ÁêÜÂü∫ÂáÜ‰∏äÁöÑËßÜËßâÁúüÂÆûÊÄß„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶ÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇËØ•ÊñπÊ≥ïÊó†ÈúÄÈ¢ùÂ§ñÊ†áÊ≥®Ôºå‰∏îËÉΩ‰∏éÁé∞ÊúâRFTÊñπÊ≥ïÂÖºÂÆπÔºåÂÖ∑ÊúâËæÉÂº∫ÁöÑÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÈúÄË¶ÅÈ´òÂ∫¶ËßÜËßâ‰ø°ÊÅØÂèØ‰ø°Â∫¶ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÂåªÁñóÂΩ±ÂÉèËØäÊñ≠„ÄÅÂÆâÈò≤ÁõëÊéßÁ≠â„ÄÇÈÄöËøáÊèêÈ´òËßÜËßâ‰ø°ÊÅØÁöÑÂèØÈù†ÊÄßÂíåÂÖÖÂàÜÊÄßÔºåÂèØ‰ª•ÊèêÂçáÊ®°ÂûãÂú®Ëøô‰∫õÈ¢ÜÂüüÁöÑÂÜ≥Á≠ñËÉΩÂäõÂíåÂÆâÂÖ®ÊÄßÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁ†îÁ©∂Êèê‰æõÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.

