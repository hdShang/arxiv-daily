---
layout: default
title: Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation
---

# Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23894" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23894v1</a>
  <a href="https://arxiv.org/pdf/2510.23894.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23894v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.23894v1', 'Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinxin Zhou, Jiachen Jiang, Zhihui Zhu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

**å¤‡æ³¨**: 23 pages, 10 figures, 14 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLHT-CLIPï¼Œæ— éœ€è®­ç»ƒå³å¯æå‡CLIPåœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­çš„è§†è§‰åŒºåˆ†æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²` `CLIPæ¨¡å‹` `è§†è§‰åŒºåˆ†æ€§` `æ— éœ€è®­ç»ƒ` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°†CLIPåº”ç”¨äºè¯­ä¹‰åˆ†å‰²æ—¶ï¼Œå—é™äºCLIPå›¾åƒçº§åˆ«é¢„è®­ç»ƒä¸åƒç´ çº§åˆ«ç†è§£çš„å·®å¼‚ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ä½³ã€‚
2. LHT-CLIPé€šè¿‡åˆ†æCLIPåœ¨å±‚ã€å¤´å’Œtokençº§åˆ«çš„è§†è§‰åŒºåˆ†æ€§ï¼Œæå‡ºè¯­ä¹‰-ç©ºé—´é‡åŠ æƒã€é€‰æ‹©æ€§å¤´å¢å¼ºå’Œå¼‚å¸¸tokenæ›¿æ¢ç­‰æŠ€æœ¯ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLHT-CLIPåœ¨å¤šä¸ªè¯­ä¹‰åˆ†å‰²åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æœ‰æ•ˆæå‡åˆ†å‰²æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°†CLIPæ¨¡å‹æ‰©å±•åˆ°è¯­ä¹‰åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå…¶å›¾åƒçº§åˆ«çš„é¢„è®­ç»ƒç›®æ ‡ä¸å¯†é›†é¢„æµ‹æ‰€éœ€çš„åƒç´ çº§åˆ«è§†è§‰ç†è§£ä¸ä¸€è‡´ã€‚è™½ç„¶ä¹‹å‰çš„å·¥ä½œé€šè¿‡é‡ç»„æœ€åä¸€å±‚å’Œç‰¹å¾å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œä½†å®ƒä»¬é€šå¸¸ç»§æ‰¿äº†å‰é¢å±‚çš„å…¨å±€å¯¹é½åå·®ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„åˆ†å‰²æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†LHT-CLIPï¼Œä¸€ç§æ–°é¢–çš„æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œç³»ç»Ÿåœ°åˆ©ç”¨CLIPåœ¨å±‚ã€å¤´å’Œtokençº§åˆ«çš„è§†è§‰åŒºåˆ†æ€§ã€‚é€šè¿‡å…¨é¢çš„åˆ†æï¼Œæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®è§è§£ï¼š(i) æœ€åä¸€å±‚ä¸»è¦åŠ å¼ºå›¾åƒ-æ–‡æœ¬å¯¹é½ï¼Œç‰ºç‰²äº†è§†è§‰åŒºåˆ†æ€§ï¼ˆä¾‹å¦‚ï¼ŒViT-B/16ä¸­çš„æœ€å3å±‚å’ŒViT-L/14ä¸­çš„8å±‚ï¼‰ï¼Œéƒ¨åˆ†åŸå› æ˜¯å¼‚å¸¸tokençš„å‡ºç°ï¼›(ii) ä¸€éƒ¨åˆ†æ³¨æ„åŠ›å¤´ï¼ˆä¾‹å¦‚ï¼ŒViT-B/16ä¸­çš„144ä¸ªå¤´ä¸­çš„10ä¸ªï¼‰åœ¨æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„å¼ºè§†è§‰åŒºåˆ†æ€§ï¼›(iii) ä¸æ­£å¸¸tokenç›¸æ¯”ï¼Œå¼‚å¸¸tokenæ˜¾ç¤ºå‡ºç¨€ç–ä¸”ä¸€è‡´çš„æ¿€æ´»æ¨¡å¼ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†ä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šè¯­ä¹‰-ç©ºé—´é‡åŠ æƒã€é€‰æ‹©æ€§å¤´å¢å¼ºå’Œå¼‚å¸¸tokenæ›¿æ¢ï¼Œä»¥æœ‰æ•ˆåœ°æ¢å¤è§†è§‰åŒºåˆ†æ€§å¹¶æé«˜åˆ†å‰²æ€§èƒ½ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€è¾…åŠ©é¢„è®­ç»ƒç½‘ç»œæˆ–å¹¿æ³›çš„è¶…å‚æ•°è°ƒæ•´ã€‚åœ¨8ä¸ªå¸¸è§çš„è¯­ä¹‰åˆ†å‰²åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLHT-CLIPåœ¨å„ç§åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªå‡ºäº†å…¶æœ‰æ•ˆæ€§å’Œå®é™…éƒ¨ç½²èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šCLIPæ¨¡å‹åœ¨å›¾åƒçº§åˆ«çš„é¢„è®­ç»ƒç›®æ ‡ä¸è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æ‰€éœ€çš„åƒç´ çº§åˆ«è§†è§‰ç†è§£å­˜åœ¨åå·®ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶å°è¯•é‡ç»„CLIPçš„æœ€åä¸€å±‚å’Œç‰¹å¾ï¼Œä½†éš¾ä»¥å…‹æœå‰é¢å±‚çš„å…¨å±€å¯¹é½åå·®ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½å—é™ã€‚æ­¤å¤–ï¼Œå¼‚å¸¸tokençš„å‡ºç°è¿›ä¸€æ­¥é™ä½äº†è§†è§‰åŒºåˆ†æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLHT-CLIPçš„æ ¸å¿ƒæ€è·¯æ˜¯ç³»ç»Ÿåœ°æŒ–æ˜å’Œå¢å¼ºCLIPæ¨¡å‹åœ¨ä¸åŒå±‚ã€ä¸åŒæ³¨æ„åŠ›å¤´ä»¥åŠä¸åŒtokençº§åˆ«çš„è§†è§‰åŒºåˆ†æ€§ã€‚é€šè¿‡åˆ†æCLIPçš„å†…éƒ¨è¡¨ç¤ºï¼Œè¯†åˆ«å‡ºå…·æœ‰å¼ºè§†è§‰åŒºåˆ†æ€§çš„å±‚ã€å¤´å’Œtokenï¼Œå¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œå¢å¼ºæˆ–æ›¿æ¢ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLHT-CLIPæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šè¯­ä¹‰-ç©ºé—´é‡åŠ æƒã€é€‰æ‹©æ€§å¤´å¢å¼ºå’Œå¼‚å¸¸tokenæ›¿æ¢ã€‚é¦–å…ˆï¼Œè¯­ä¹‰-ç©ºé—´é‡åŠ æƒæ¨¡å—æ—¨åœ¨å¹³è¡¡è¯­ä¹‰ä¿¡æ¯å’Œç©ºé—´ä¿¡æ¯ï¼Œä»¥æé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œé€‰æ‹©æ€§å¤´å¢å¼ºæ¨¡å—é€‰æ‹©æ€§åœ°å¢å¼ºå…·æœ‰å¼ºè§†è§‰åŒºåˆ†æ€§çš„æ³¨æ„åŠ›å¤´ï¼ŒæŠ‘åˆ¶å™ªå£°å¤´çš„å¹²æ‰°ã€‚æœ€åï¼Œå¼‚å¸¸tokenæ›¿æ¢æ¨¡å—æ£€æµ‹å¹¶æ›¿æ¢å¼‚å¸¸tokenï¼Œä»¥å‡å°‘å…¶å¯¹åˆ†å‰²ç»“æœçš„è´Ÿé¢å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šLHT-CLIPçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¯¹CLIPæ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„æ·±å…¥åˆ†æï¼Œå¹¶åŸºäºåˆ†æç»“æœæå‡ºäº†é’ˆå¯¹æ€§çš„å¢å¼ºç­–ç•¥ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒLHT-CLIPä¸ä»…å…³æ³¨æœ€åä¸€å±‚ç‰¹å¾çš„é‡ç»„ï¼Œæ›´æ³¨é‡æŒ–æ˜å’Œåˆ©ç”¨CLIPæ¨¡å‹åœ¨ä¸åŒå±‚ã€å¤´å’Œtokençº§åˆ«çš„è§†è§‰åŒºåˆ†æ€§ã€‚æ­¤å¤–ï¼ŒLHT-CLIPæ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œé™ä½äº†è®¡ç®—æˆæœ¬å’Œéƒ¨ç½²éš¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯­ä¹‰-ç©ºé—´é‡åŠ æƒæ¨¡å—ä½¿ç”¨å¯å­¦ä¹ çš„æƒé‡æ¥å¹³è¡¡è¯­ä¹‰ä¿¡æ¯å’Œç©ºé—´ä¿¡æ¯ã€‚é€‰æ‹©æ€§å¤´å¢å¼ºæ¨¡å—é€šè¿‡è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è§†è§‰åŒºåˆ†æ€§å¾—åˆ†ï¼Œé€‰æ‹©æ€§åœ°å¢å¼ºå¾—åˆ†è¾ƒé«˜çš„å¤´ã€‚å¼‚å¸¸tokenæ›¿æ¢æ¨¡å—ä½¿ç”¨æ­£å¸¸tokençš„å¹³å‡è¡¨ç¤ºæ¥æ›¿æ¢å¼‚å¸¸tokenã€‚å…·ä½“å®ç°ç»†èŠ‚åŒ…æ‹¬è§†è§‰åŒºåˆ†æ€§å¾—åˆ†çš„è®¡ç®—æ–¹æ³•ã€å¼‚å¸¸tokençš„æ£€æµ‹é˜ˆå€¼ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LHT-CLIPåœ¨8ä¸ªå¸¸è§çš„è¯­ä¹‰åˆ†å‰²åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ADE20Kæ•°æ®é›†ä¸Šï¼ŒLHT-CLIPçš„mIoUæŒ‡æ ‡è¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒæˆ–æ•°æ®å¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLHT-CLIPèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢å¤CLIPçš„è§†è§‰åŒºåˆ†æ€§ï¼Œå¹¶æé«˜åˆ†å‰²æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LHT-CLIPå¯åº”ç”¨äºå„ç§éœ€è¦å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€åŒ»å­¦å›¾åƒåˆ†æç­‰ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ï¼Œå¯ä»¥å¿«é€Ÿéƒ¨ç½²åˆ°ç°æœ‰ç³»ç»Ÿä¸­ï¼Œæå‡è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸ºç›¸å…³åº”ç”¨å¸¦æ¥æ›´ç²¾ç¡®çš„ç¯å¢ƒæ„ŸçŸ¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Extending CLIP models to semantic segmentation remains challenging due to the misalignment between their image-level pre-training objectives and the pixel-level visual understanding required for dense prediction. While prior efforts have achieved encouraging results by reorganizing the final layer and features, they often inherit the global alignment bias of preceding layers, leading to suboptimal segmentation performance. In this work, we propose LHT-CLIP, a novel training-free framework that systematically exploits the visual discriminability of CLIP across layer, head, and token levels. Through comprehensive analysis, we reveal three key insights: (i) the final layers primarily strengthen image-text alignment with sacrifice of visual discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14), partly due to the emergence of anomalous tokens; (ii) a subset of attention heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual discriminability across datasets; (iii) abnormal tokens display sparse and consistent activation pattern compared to normal tokens. Based on these findings, we propose three complementary techniques: semantic-spatial reweighting, selective head enhancement, and abnormal token replacement to effectively restore visual discriminability and improve segmentation performance without any additional training, auxiliary pre-trained networks, or extensive hyperparameter tuning. Extensive experiments on 8 common semantic segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art performance across diverse scenarios, highlighting its effectiveness and practicality for real-world deployment.

