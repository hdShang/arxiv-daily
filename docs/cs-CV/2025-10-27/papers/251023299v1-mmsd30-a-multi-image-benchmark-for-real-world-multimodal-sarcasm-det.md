---
layout: default
title: MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection
---

# MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.23299" target="_blank" class="toolbar-btn">arXiv: 2510.23299v1</a>
    <a href="https://arxiv.org/pdf/2510.23299.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23299v1" 
            onclick="toggleFavorite(this, '2510.23299v1', 'MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Haochen Zhao, Yuyao Kong, Yongxiu Xu, Gaopeng Gou, Hongbo Xu, Yubin Wang, Haoliang Zhang

**ÂàÜÁ±ª**: cs.CV, cs.MM

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MMSD3.0Â§öÂõæËÆΩÂà∫Ê£ÄÊµãÂü∫ÂáÜÂíåCIRMÊ®°ÂûãÔºåËß£ÂÜ≥ÁúüÂÆûÂú∫ÊôØÂ§öÂõæÁ∫øÁ¥¢ËÆΩÂà∫ËØÜÂà´ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅËÆΩÂà∫Ê£ÄÊµã` `Â§öÂõæÊé®ÁêÜ` `Ë∑®Ê®°ÊÄÅËûçÂêà` `ÂõæÂÉèÂ∫èÂàóÂª∫Ê®°` `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËÆΩÂà∫Ê£ÄÊµãÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÂÖ≥Ê≥®ÂçïÂº†ÂõæÁâáÔºåÂøΩÁï•‰∫ÜÁúüÂÆûÂú∫ÊôØ‰∏≠Â§öÂõæ‰πãÈó¥Â≠òÂú®ÁöÑËØ≠‰πâÂíåÊÉÖÊÑüÂÖ≥ËÅî„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Ë∑®ÂõæÂÉèÊé®ÁêÜÊ®°ÂûãÔºàCIRMÔºâÔºåÈÄöËøáË∑®ÂõæÂÉèÂ∫èÂàóÂª∫Ê®°ÊçïËé∑ÂõæÂÉèÈó¥ÁöÑÊΩúÂú®ËÅîÁ≥ªÔºåÊèêÂçáËÆΩÂà∫Ê£ÄÊµãÊïàÊûú„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMMSD3.0ÊòØ‰∏Ä‰∏™ÊúâÊïàÂèØÈù†ÁöÑÂü∫ÂáÜÔºåCIRMÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∞ΩÁÆ°Â§öÊ®°ÊÄÅËÆΩÂà∫Ê£ÄÊµãÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÁé∞ÊúâÊï∞ÊçÆÈõÜÂíåÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂçïÂõæÂÉèÂú∫ÊôØÔºåÂøΩÁï•‰∫ÜÂ§öÂõæÂÉè‰πãÈó¥ÊΩúÂú®ÁöÑËØ≠‰πâÂíåÊÉÖÊÑüÂÖ≥Á≥ª„ÄÇËøôÂØºËá¥Âú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ÔºåÁî±Â§öÂõæÂÉèÁ∫øÁ¥¢Ëß¶ÂèëÁöÑËÆΩÂà∫ÊÉÖÂÜµÂª∫Ê®°Â≠òÂú®Â∑ÆË∑ù„ÄÇ‰∏∫‰∫ÜÂº•Ë°•Ëøô‰∏ÄÂ∑ÆË∑ùÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜMMSD3.0ÔºåËøôÊòØ‰∏Ä‰∏™ÂÆåÂÖ®Áî±Êù•Ëá™Êé®ÁâπÂíå‰∫öÈ©¨ÈÄäËØÑËÆ∫ÁöÑÂ§öÂõæÂÉèÊ†∑Êú¨ÁªÑÊàêÁöÑÊñ∞Âü∫ÂáÜ„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÊèêÂá∫‰∫ÜË∑®ÂõæÂÉèÊé®ÁêÜÊ®°ÂûãÔºàCIRMÔºâÔºåÂÆÉÊâßË°åÊúâÈíàÂØπÊÄßÁöÑË∑®ÂõæÂÉèÂ∫èÂàóÂª∫Ê®°Ôºå‰ª•ÊçïËé∑ÊΩúÂú®ÁöÑÂõæÂÉèÈó¥ËøûÊé•„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊñáÊú¨-ÂõæÂÉèÂØπÂ∫îÂÖ≥Á≥ªÁöÑÁõ∏ÂÖ≥ÊÄßÂºïÂØºÁöÑÁªÜÁ≤íÂ∫¶Ë∑®Ê®°ÊÄÅËûçÂêàÊú∫Âà∂Ôºå‰ª•ÂáèÂ∞ëÈõÜÊàêËøáÁ®ã‰∏≠ÁöÑ‰ø°ÊÅØÊçüÂ§±„ÄÇÊàë‰ª¨Âª∫Á´ã‰∫Ü‰∏ÄÂ•óÂÖ®Èù¢ÁöÑÂº∫Â§ß‰∏îÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑÂü∫Á∫øÔºåÂπ∂ËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåË°®ÊòéMMSD3.0ÊòØ‰∏Ä‰∏™ÊúâÊïà‰∏îÂèØÈù†ÁöÑÂü∫ÂáÜÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂèçÊò†ÁúüÂÆû‰∏ñÁïåÁöÑÊù°‰ª∂„ÄÇÊ≠§Â§ñÔºåCIRMÂú®MMSD„ÄÅMMSD2.0ÂíåMMSD3.0‰∏äÈÉΩË°®Áé∞Âá∫‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂçïÂõæÂÉèÂíåÂ§öÂõæÂÉèÂú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËÆΩÂà∫Ê£ÄÊµãÊñπÊ≥ïÂíåÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÈõÜ‰∏≠‰∫éÂçïÂº†ÂõæÂÉèÔºåÊó†Ê≥ïÊúâÊïàÊçïÊçâÁúüÂÆû‰∏ñÁïå‰∏≠Áî±Â§öÂº†ÂõæÁâáÂÖ±ÂêåË°®ËææÁöÑËÆΩÂà∫Âê´‰πâ„ÄÇËøô‰∫õÊñπÊ≥ïÂøΩÁï•‰∫ÜÂõæÂÉè‰πãÈó¥ÁöÑÂÖ≥ËÅîÊÄßÔºåÂØºËá¥Âú®Â§ÑÁêÜÂ§öÂõæËÆΩÂà∫Ê£ÄÊµã‰ªªÂä°Êó∂ÊÄßËÉΩÂèóÈôê„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏Ä‰∏™ËÉΩÂ§üÂèçÊò†ÁúüÂÆûÂú∫ÊôØ„ÄÅÂåÖÂê´Â§öÂõæÊ†∑Êú¨ÁöÑÊï∞ÊçÆÈõÜÔºå‰ª•ÂèäËÉΩÂ§üÊúâÊïàÂª∫Ê®°ÂõæÂÉèÈó¥ÂÖ≥Á≥ªÁöÑÊ®°Âûã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊûÑÂª∫Â§öÂõæËÆΩÂà∫Ê£ÄÊµãÊï∞ÊçÆÈõÜMMSD3.0ÔºåÂπ∂ÊèêÂá∫Ë∑®ÂõæÂÉèÊé®ÁêÜÊ®°ÂûãCIRMÔºåÊù•Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ïÂú®Â§öÂõæÂú∫ÊôØ‰∏ãÁöÑ‰∏çË∂≥„ÄÇCIRMÈÄöËøáË∑®ÂõæÂÉèÂ∫èÂàóÂª∫Ê®°ÔºåÂ≠¶‰π†ÂõæÂÉè‰πãÈó¥ÁöÑÊΩúÂú®ËÅîÁ≥ªÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£Â§öÂõæÊâÄË°®ËææÁöÑËÆΩÂà∫Âê´‰πâ„ÄÇÂêåÊó∂ÔºåÂºïÂÖ•Áõ∏ÂÖ≥ÊÄßÂºïÂØºÁöÑË∑®Ê®°ÊÄÅËûçÂêàÊú∫Âà∂ÔºåÂáèÂ∞ë‰ø°ÊÅØÊçüÂ§±„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCIRMÊ®°ÂûãÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÂõæÂÉèÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÊ®°ÂûãÔºàÂ¶ÇResNetÔºâÊèêÂèñÊØèÂº†ÂõæÂÉèÁöÑÁâπÂæÅ„ÄÇ2) ÊñáÊú¨ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºö‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇBERTÔºâÊèêÂèñÊñáÊú¨ÁâπÂæÅ„ÄÇ3) Ë∑®ÂõæÂÉèÊé®ÁêÜÊ®°ÂùóÔºö‰ΩøÁî®Â∫èÂàóÊ®°ÂûãÔºàÂ¶ÇLSTMÊàñTransformerÔºâÂØπÂõæÂÉèÁâπÂæÅÂ∫èÂàóËøõË°åÂª∫Ê®°ÔºåÊçïËé∑ÂõæÂÉè‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ4) Ë∑®Ê®°ÊÄÅËûçÂêàÊ®°ÂùóÔºö‰ΩøÁî®Áõ∏ÂÖ≥ÊÄßÂºïÂØºÁöÑËûçÂêàÊú∫Âà∂ÔºåÂ∞ÜÂõæÂÉèÂíåÊñáÊú¨ÁâπÂæÅËøõË°åËûçÂêà„ÄÇ5) ÂàÜÁ±ªÂô®Ôºö‰ΩøÁî®ÂÖ®ËøûÊé•Â±ÇÊàñsoftmaxÂ±ÇËøõË°åËÆΩÂà∫ÂàÜÁ±ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞ÁÇπÂú®‰∫éÔºö1) ÊûÑÂª∫‰∫ÜÂ§öÂõæËÆΩÂà∫Ê£ÄÊµãÊï∞ÊçÆÈõÜMMSD3.0ÔºåÂº•Ë°•‰∫ÜÁé∞ÊúâÊï∞ÊçÆÈõÜÁöÑ‰∏çË∂≥„ÄÇ2) ÊèêÂá∫‰∫ÜË∑®ÂõæÂÉèÊé®ÁêÜÊ®°ÂûãCIRMÔºåÈÄöËøáÂ∫èÂàóÂª∫Ê®°ÊçïËé∑ÂõæÂÉè‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÊúâÊïàÊèêÂçá‰∫ÜÂ§öÂõæËÆΩÂà∫Ê£ÄÊµãÁöÑÊÄßËÉΩ„ÄÇ3) ÂºïÂÖ•‰∫ÜÁõ∏ÂÖ≥ÊÄßÂºïÂØºÁöÑË∑®Ê®°ÊÄÅËûçÂêàÊú∫Âà∂ÔºåÂáèÂ∞ë‰∫Ü‰ø°ÊÅØÊçüÂ§±ÔºåÊèêÈ´ò‰∫ÜËûçÂêàÊïàÊûú„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåCIRMËÉΩÂ§üÊòæÂºèÂú∞Âª∫Ê®°ÂõæÂÉè‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåËÄåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®ÂçïÂº†ÂõæÂÉèÁöÑÁâπÂæÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ë∑®ÂõæÂÉèÊé®ÁêÜÊ®°Âùó‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî®‰∏çÂêåÁöÑÂ∫èÂàóÊ®°ÂûãÔºåÂ¶ÇLSTMÊàñTransformer„ÄÇÂú®Ë∑®Ê®°ÊÄÅËûçÂêàÊ®°Âùó‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî®‰∏çÂêåÁöÑËûçÂêàÁ≠ñÁï•ÔºåÂ¶ÇÊ≥®ÊÑèÂäõÊú∫Âà∂ÊàñÈó®ÊéßÊú∫Âà∂„ÄÇÁõ∏ÂÖ≥ÊÄßÂºïÂØºÁöÑËûçÂêàÊú∫Âà∂ÈÄöËøáËÆ°ÁÆóÊñáÊú¨ÂíåÂõæÂÉè‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÊù•ÊåáÂØºÁâπÂæÅÁöÑËûçÂêà„ÄÇÊçüÂ§±ÂáΩÊï∞ÂèØ‰ª•‰ΩøÁî®‰∫§ÂèâÁÜµÊçüÂ§±Êàñfocal loss„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÈúÄË¶ÅÊ†πÊçÆÊï∞ÊçÆÈõÜÂíåÂÆûÈ™åÁªìÊûúËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMMSD3.0ÊòØ‰∏Ä‰∏™ÊúâÊïà‰∏îÂèØÈù†ÁöÑÂü∫ÂáÜÔºåCIRMÊ®°ÂûãÂú®MMSD3.0‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩÔºåÂπ∂‰∏îÂú®MMSDÂíåMMSD2.0‰∏ä‰πüË°®Áé∞Âá∫‰∫Ü‰ºòÂºÇÁöÑÊÄßËÉΩ„ÄÇÁõ∏ËæÉ‰∫éÂÖ∂‰ªñÂü∫Á∫øÊ®°ÂûãÔºåCIRMÂú®Â§öÂõæËÆΩÂà∫Ê£ÄÊµã‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÁ§æ‰∫§Â™í‰ΩìÂÜÖÂÆπÁêÜËß£„ÄÅËàÜÊÉÖÂàÜÊûê„ÄÅÊô∫ËÉΩÂÆ¢ÊúçÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáËØÜÂà´Â§öÂõæËÆΩÂà∫ÂÜÖÂÆπÔºåÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞ÁêÜËß£Áî®Êà∑ÊÑèÂõæÔºåËøáÊª§‰∏çËâØ‰ø°ÊÅØÔºåÊèêÂçáÁî®Êà∑‰ΩìÈ™å„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØËøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÂ§öÊ®°ÊÄÅ‰ªªÂä°ÔºåÂ¶ÇËßÜÈ¢ëÂÜÖÂÆπÁêÜËß£„ÄÅÂ§öÊ®°ÊÄÅÂØπËØùÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Despite progress in multimodal sarcasm detection, existing datasets and methods predominantly focus on single-image scenarios, overlooking potential semantic and affective relations across multiple images. This leaves a gap in modeling cases where sarcasm is triggered by multi-image cues in real-world settings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed entirely of multi-image samples curated from tweets and Amazon reviews. We further propose the Cross-Image Reasoning Model (CIRM), which performs targeted cross-image sequence modeling to capture latent inter-image connections. In addition, we introduce a relevance-guided, fine-grained cross-modal fusion mechanism based on text-image correspondence to reduce information loss during integration. We establish a comprehensive suite of strong and representative baselines and conduct extensive experiments, showing that MMSD3.0 is an effective and reliable benchmark that better reflects real-world conditions. Moreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0 and MMSD3.0, validating its effectiveness in both single-image and multi-image scenarios.

