---
layout: default
title: MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection
---

# MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23299" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23299v1</a>
  <a href="https://arxiv.org/pdf/2510.23299.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23299v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.23299v1', 'MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haochen Zhao, Yuyao Kong, Yongxiu Xu, Gaopeng Gou, Hongbo Xu, Yubin Wang, Haoliang Zhang

**åˆ†ç±»**: cs.CV, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMSD3.0å¤šå›¾è®½åˆºæ£€æµ‹åŸºå‡†å’ŒCIRMæ¨¡å‹ï¼Œè§£å†³çœŸå®åœºæ™¯å¤šå›¾çº¿ç´¢è®½åˆºè¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è®½åˆºæ£€æµ‹` `å¤šå›¾æ¨ç†` `è·¨æ¨¡æ€èåˆ` `å›¾åƒåºåˆ—å»ºæ¨¡` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è®½åˆºæ£€æµ‹æ•°æ®é›†ä¸»è¦å…³æ³¨å•å¼ å›¾ç‰‡ï¼Œå¿½ç•¥äº†çœŸå®åœºæ™¯ä¸­å¤šå›¾ä¹‹é—´å­˜åœ¨çš„è¯­ä¹‰å’Œæƒ…æ„Ÿå…³è”ã€‚
2. è®ºæ–‡æå‡ºè·¨å›¾åƒæ¨ç†æ¨¡å‹ï¼ˆCIRMï¼‰ï¼Œé€šè¿‡è·¨å›¾åƒåºåˆ—å»ºæ¨¡æ•è·å›¾åƒé—´çš„æ½œåœ¨è”ç³»ï¼Œæå‡è®½åˆºæ£€æµ‹æ•ˆæœã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMMSD3.0æ˜¯ä¸€ä¸ªæœ‰æ•ˆå¯é çš„åŸºå‡†ï¼ŒCIRMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤šæ¨¡æ€è®½åˆºæ£€æµ‹å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•å›¾åƒåœºæ™¯ï¼Œå¿½ç•¥äº†å¤šå›¾åƒä¹‹é—´æ½œåœ¨çš„è¯­ä¹‰å’Œæƒ…æ„Ÿå…³ç³»ã€‚è¿™å¯¼è‡´åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œç”±å¤šå›¾åƒçº¿ç´¢è§¦å‘çš„è®½åˆºæƒ…å†µå»ºæ¨¡å­˜åœ¨å·®è·ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MMSD3.0ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨ç”±æ¥è‡ªæ¨ç‰¹å’Œäºšé©¬é€Šè¯„è®ºçš„å¤šå›¾åƒæ ·æœ¬ç»„æˆçš„æ–°åŸºå‡†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†è·¨å›¾åƒæ¨ç†æ¨¡å‹ï¼ˆCIRMï¼‰ï¼Œå®ƒæ‰§è¡Œæœ‰é’ˆå¯¹æ€§çš„è·¨å›¾åƒåºåˆ—å»ºæ¨¡ï¼Œä»¥æ•è·æ½œåœ¨çš„å›¾åƒé—´è¿æ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ–‡æœ¬-å›¾åƒå¯¹åº”å…³ç³»çš„ç›¸å…³æ€§å¼•å¯¼çš„ç»†ç²’åº¦è·¨æ¨¡æ€èåˆæœºåˆ¶ï¼Œä»¥å‡å°‘é›†æˆè¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€å¥—å…¨é¢çš„å¼ºå¤§ä¸”å…·æœ‰ä»£è¡¨æ€§çš„åŸºçº¿ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¡¨æ˜MMSD3.0æ˜¯ä¸€ä¸ªæœ‰æ•ˆä¸”å¯é çš„åŸºå‡†ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ çœŸå®ä¸–ç•Œçš„æ¡ä»¶ã€‚æ­¤å¤–ï¼ŒCIRMåœ¨MMSDã€MMSD2.0å’ŒMMSD3.0ä¸Šéƒ½è¡¨ç°å‡ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å•å›¾åƒå’Œå¤šå›¾åƒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è®½åˆºæ£€æµ‹æ–¹æ³•å’Œæ•°æ®é›†ä¸»è¦é›†ä¸­äºå•å¼ å›¾åƒï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰çœŸå®ä¸–ç•Œä¸­ç”±å¤šå¼ å›¾ç‰‡å…±åŒè¡¨è¾¾çš„è®½åˆºå«ä¹‰ã€‚è¿™äº›æ–¹æ³•å¿½ç•¥äº†å›¾åƒä¹‹é—´çš„å…³è”æ€§ï¼Œå¯¼è‡´åœ¨å¤„ç†å¤šå›¾è®½åˆºæ£€æµ‹ä»»åŠ¡æ—¶æ€§èƒ½å—é™ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªèƒ½å¤Ÿåæ˜ çœŸå®åœºæ™¯ã€åŒ…å«å¤šå›¾æ ·æœ¬çš„æ•°æ®é›†ï¼Œä»¥åŠèƒ½å¤Ÿæœ‰æ•ˆå»ºæ¨¡å›¾åƒé—´å…³ç³»çš„æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºå¤šå›¾è®½åˆºæ£€æµ‹æ•°æ®é›†MMSD3.0ï¼Œå¹¶æå‡ºè·¨å›¾åƒæ¨ç†æ¨¡å‹CIRMï¼Œæ¥è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤šå›¾åœºæ™¯ä¸‹çš„ä¸è¶³ã€‚CIRMé€šè¿‡è·¨å›¾åƒåºåˆ—å»ºæ¨¡ï¼Œå­¦ä¹ å›¾åƒä¹‹é—´çš„æ½œåœ¨è”ç³»ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å¤šå›¾æ‰€è¡¨è¾¾çš„è®½åˆºå«ä¹‰ã€‚åŒæ—¶ï¼Œå¼•å…¥ç›¸å…³æ€§å¼•å¯¼çš„è·¨æ¨¡æ€èåˆæœºåˆ¶ï¼Œå‡å°‘ä¿¡æ¯æŸå¤±ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCIRMæ¨¡å‹çš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å›¾åƒç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼ˆå¦‚ResNetï¼‰æå–æ¯å¼ å›¾åƒçš„ç‰¹å¾ã€‚2) æ–‡æœ¬ç‰¹å¾æå–æ¨¡å—ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTï¼‰æå–æ–‡æœ¬ç‰¹å¾ã€‚3) è·¨å›¾åƒæ¨ç†æ¨¡å—ï¼šä½¿ç”¨åºåˆ—æ¨¡å‹ï¼ˆå¦‚LSTMæˆ–Transformerï¼‰å¯¹å›¾åƒç‰¹å¾åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œæ•è·å›¾åƒä¹‹é—´çš„å…³ç³»ã€‚4) è·¨æ¨¡æ€èåˆæ¨¡å—ï¼šä½¿ç”¨ç›¸å…³æ€§å¼•å¯¼çš„èåˆæœºåˆ¶ï¼Œå°†å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œèåˆã€‚5) åˆ†ç±»å™¨ï¼šä½¿ç”¨å…¨è¿æ¥å±‚æˆ–softmaxå±‚è¿›è¡Œè®½åˆºåˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) æ„å»ºäº†å¤šå›¾è®½åˆºæ£€æµ‹æ•°æ®é›†MMSD3.0ï¼Œå¼¥è¡¥äº†ç°æœ‰æ•°æ®é›†çš„ä¸è¶³ã€‚2) æå‡ºäº†è·¨å›¾åƒæ¨ç†æ¨¡å‹CIRMï¼Œé€šè¿‡åºåˆ—å»ºæ¨¡æ•è·å›¾åƒä¹‹é—´çš„å…³ç³»ï¼Œæœ‰æ•ˆæå‡äº†å¤šå›¾è®½åˆºæ£€æµ‹çš„æ€§èƒ½ã€‚3) å¼•å…¥äº†ç›¸å…³æ€§å¼•å¯¼çš„è·¨æ¨¡æ€èåˆæœºåˆ¶ï¼Œå‡å°‘äº†ä¿¡æ¯æŸå¤±ï¼Œæé«˜äº†èåˆæ•ˆæœã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒCIRMèƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡å›¾åƒä¹‹é—´çš„å…³ç³»ï¼Œè€Œç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨å•å¼ å›¾åƒçš„ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è·¨å›¾åƒæ¨ç†æ¨¡å—ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„åºåˆ—æ¨¡å‹ï¼Œå¦‚LSTMæˆ–Transformerã€‚åœ¨è·¨æ¨¡æ€èåˆæ¨¡å—ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„èåˆç­–ç•¥ï¼Œå¦‚æ³¨æ„åŠ›æœºåˆ¶æˆ–é—¨æ§æœºåˆ¶ã€‚ç›¸å…³æ€§å¼•å¯¼çš„èåˆæœºåˆ¶é€šè¿‡è®¡ç®—æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæ¥æŒ‡å¯¼ç‰¹å¾çš„èåˆã€‚æŸå¤±å‡½æ•°å¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±æˆ–focal lossã€‚å…·ä½“çš„å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®æ•°æ®é›†å’Œå®éªŒç»“æœè¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMMSD3.0æ˜¯ä¸€ä¸ªæœ‰æ•ˆä¸”å¯é çš„åŸºå‡†ï¼ŒCIRMæ¨¡å‹åœ¨MMSD3.0ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶ä¸”åœ¨MMSDå’ŒMMSD2.0ä¸Šä¹Ÿè¡¨ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ¨¡å‹ï¼ŒCIRMåœ¨å¤šå›¾è®½åˆºæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç¤¾äº¤åª’ä½“å†…å®¹ç†è§£ã€èˆ†æƒ…åˆ†æã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚é€šè¿‡è¯†åˆ«å¤šå›¾è®½åˆºå†…å®¹ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·æ„å›¾ï¼Œè¿‡æ»¤ä¸è‰¯ä¿¡æ¯ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¦‚è§†é¢‘å†…å®¹ç†è§£ã€å¤šæ¨¡æ€å¯¹è¯ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite progress in multimodal sarcasm detection, existing datasets and methods predominantly focus on single-image scenarios, overlooking potential semantic and affective relations across multiple images. This leaves a gap in modeling cases where sarcasm is triggered by multi-image cues in real-world settings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed entirely of multi-image samples curated from tweets and Amazon reviews. We further propose the Cross-Image Reasoning Model (CIRM), which performs targeted cross-image sequence modeling to capture latent inter-image connections. In addition, we introduce a relevance-guided, fine-grained cross-modal fusion mechanism based on text-image correspondence to reduce information loss during integration. We establish a comprehensive suite of strong and representative baselines and conduct extensive experiments, showing that MMSD3.0 is an effective and reliable benchmark that better reflects real-world conditions. Moreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0 and MMSD3.0, validating its effectiveness in both single-image and multi-image scenarios.

