---
layout: default
title: "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting"
---

# CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23785" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23785v1</a>
  <a href="https://arxiv.org/pdf/2510.23785.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23785v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.23785v1', 'CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Md Tanvir Hossain, Akif Islam, Mohd Ruhul Ameen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

**å¤‡æ³¨**: 6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International Conference on Electrical, Computer and Telecommunication Engineering (ICECTE 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CountFormerï¼šTransformeræ¡†æ¶å­¦ä¹ è§†è§‰é‡å¤ä¸ç»“æ„ï¼Œå®ç°ç±»åˆ«æ— å…³çš„ç›®æ ‡è®¡æ•°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›®æ ‡è®¡æ•°` `Transformer` `DINOv2` `è§†è§‰é‡å¤` `ç»“æ„ä¸€è‡´æ€§` `ç±»åˆ«æ— å…³` `å¯†åº¦å›¾` `è‡ªç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è®¡æ•°æ¨¡å‹åœ¨å¤„ç†å…·æœ‰å¤æ‚å½¢çŠ¶ã€å†…éƒ¨å¯¹ç§°æˆ–é‡å çš„å¯¹è±¡æ—¶ï¼Œè®¡æ•°å‡†ç¡®ç‡è¾ƒä½ï¼Œæ— æ³•æœ‰æ•ˆæ¨¡ä»¿äººç±»çš„è®¡æ•°èƒ½åŠ›ã€‚
2. CountFormeråˆ©ç”¨Transformeræ¶æ„ï¼Œé€šè¿‡å­¦ä¹ è§†è§‰é‡å¤å’Œç»“æ„ä¸€è‡´æ€§æ¥è¿›è¡Œç±»åˆ«æ— å…³çš„ç›®æ ‡è®¡æ•°ï¼Œæ›´æ¥è¿‘äººç±»çš„è®¡æ•°æ–¹å¼ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCountFormeråœ¨FSC-147æ•°æ®é›†ä¸Šå–å¾—äº†ä¸SOTAæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç»“æ„å¤æ‚æˆ–å¯†é›†åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´ä¼˜çš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºCountFormerï¼Œä¸€ä¸ªåŸºäºTransformerçš„æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ è§†è§‰é‡å¤å’Œç»“æ„ä¸€è‡´æ€§ï¼Œä»è€Œå®ç°ç±»åˆ«æ— å…³çš„ç›®æ ‡è®¡æ•°ã€‚ä¸ä¾èµ–ç±»åˆ«ä¿¡æ¯ä¸åŒï¼Œäººç±»é€šè¿‡æ„ŸçŸ¥è§†è§‰é‡å¤å’Œç»“æ„å…³ç³»æ¥è®¡æ•°ã€‚ç°æœ‰æ¨¡å‹éš¾ä»¥å¤åˆ¶è¿™ç§èƒ½åŠ›ï¼Œåœ¨å¤„ç†å¤æ‚å½¢çŠ¶ã€å†…éƒ¨å¯¹ç§°æˆ–é‡å å¯¹è±¡æ—¶å®¹æ˜“å‡ºé”™ã€‚CountFormeråŸºäºCounTRæ¶æ„ï¼Œç”¨è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹DINOv2æ›¿æ¢è§†è§‰ç¼–ç å™¨ï¼Œä»¥äº§ç”Ÿæ›´ä¸°å¯Œå’Œç©ºé—´ä¸€è‡´çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜èåˆäº†ä½ç½®åµŒå…¥ï¼Œä»¥ä¿ç•™å‡ ä½•å…³ç³»ï¼Œç„¶åé€šè¿‡è½»é‡çº§å·ç§¯è§£ç å™¨å°†è¿™äº›ç‰¹å¾è§£ç ä¸ºå¯†åº¦å›¾ã€‚åœ¨FSC-147æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç»“æ„å¤æ‚æˆ–å¯†é›†åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé›†æˆDINOv2ç­‰é¢„è®­ç»ƒæ¨¡å‹ä½¿è®¡æ•°ç³»ç»Ÿèƒ½å¤Ÿæ¥è¿‘äººç±»èˆ¬çš„ç»“æ„æ„ŸçŸ¥ï¼Œä»è€Œæœç€çœŸæ­£é€šç”¨å’Œæ— æ ·æœ¬çš„è®¡æ•°èŒƒå¼è¿ˆè¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç±»åˆ«æ— å…³çš„ç›®æ ‡è®¡æ•°é—®é¢˜ï¼Œå³åœ¨ä¸ä¾èµ–å¯¹è±¡ç±»åˆ«ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ä¼°è®¡å›¾åƒä¸­ç‰¹å®šç›®æ ‡çš„æ•°é‡ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å¤æ‚å½¢çŠ¶ã€å†…éƒ¨å¯¹ç§°æ€§æˆ–å¯†é›†å †å çš„å¯¹è±¡æ—¶ï¼Œå¾€å¾€è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥æ•æ‰åˆ°ç›®æ ‡ä¹‹é—´çš„ç»“æ„å…³ç³»å’Œé‡å¤æ¨¡å¼ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Transformeræ¶æ„å­¦ä¹ å›¾åƒä¸­ç›®æ ‡çš„è§†è§‰é‡å¤æ¨¡å¼å’Œç»“æ„ä¸€è‡´æ€§ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„è®¡æ•°ã€‚é€šè¿‡å¼•å…¥è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹DINOv2ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå¯ä»¥æå–æ›´ä¸°å¯Œå’Œç©ºé—´ä¸€è‡´çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ç›®æ ‡çš„ç»“æ„ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCountFormerçš„æ•´ä½“æ¶æ„åŸºäºCounTRï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) DINOv2è§†è§‰ç¼–ç å™¨ï¼šç”¨äºæå–å›¾åƒçš„è§†è§‰ç‰¹å¾ï¼›2) ä½ç½®åµŒå…¥èåˆï¼šç”¨äºä¿ç•™ç‰¹å¾çš„ç©ºé—´å‡ ä½•å…³ç³»ï¼›3) è½»é‡çº§å·ç§¯è§£ç å™¨ï¼šå°†ç‰¹å¾è§£ç ä¸ºå¯†åº¦å›¾ï¼Œå¯†åº¦å›¾ä¸Šçš„ç§¯åˆ†å³ä¸ºç›®æ ‡æ•°é‡çš„ä¼°è®¡å€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šCountFormerçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨DINOv2ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå¹¶ç»“åˆä½ç½®åµŒå…¥èåˆï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ ç›®æ ‡çš„ç»“æ„ä¿¡æ¯å’Œé‡å¤æ¨¡å¼ã€‚ä¸ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼ŒTransformeræ¶æ„å…·æœ‰æ›´å¼ºçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç›®æ ‡ä¹‹é—´çš„é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šDINOv2çš„é€‰æ‹©æ˜¯å› ä¸ºå…¶è‡ªç›‘ç£å­¦ä¹ çš„ç‰¹æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´é€šç”¨çš„è§†è§‰ç‰¹å¾è¡¨ç¤ºã€‚ä½ç½®åµŒå…¥èåˆé‡‡ç”¨äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹å¼ï¼Œå°†ä½ç½®ä¿¡æ¯èå…¥åˆ°ç‰¹å¾ä¸­ï¼Œä»è€Œä¿ç•™äº†ç›®æ ‡çš„ç©ºé—´å‡ ä½•å…³ç³»ã€‚è½»é‡çº§å·ç§¯è§£ç å™¨çš„è®¾è®¡æ—¨åœ¨å‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿è¯è§£ç çš„å‡†ç¡®æ€§ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨æ ‡å‡†çš„å¯†åº¦å›¾å›å½’æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CountFormeråœ¨FSC-147æ•°æ®é›†ä¸Šå–å¾—äº†ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç»“æ„å¤æ‚æˆ–å¯†é›†åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒCountFormeråœ¨å¤„ç†å…·æœ‰å¤æ‚å½¢çŠ¶ã€å†…éƒ¨å¯¹ç§°æ€§æˆ–å¯†é›†å †å çš„å¯¹è±¡æ—¶ï¼Œè®¡æ•°å‡†ç¡®ç‡æ˜¾è‘—æå‡ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ ç›®æ ‡çš„ç»“æ„ä¿¡æ¯å’Œé‡å¤æ¨¡å¼ã€‚è¯¥æ¨¡å‹è¯æ˜äº†é¢„è®­ç»ƒæ¨¡å‹åœ¨ç›®æ ‡è®¡æ•°ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CountFormeråœ¨æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€é›¶å”®åˆ†æã€åŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥ç”¨äºç»Ÿè®¡äººç¾¤å¯†åº¦æˆ–è½¦è¾†æ•°é‡ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥ç”¨äºæ£€æµ‹å’Œè®¡æ•°é“è·¯ä¸Šçš„è¡Œäººæˆ–è½¦è¾†ï¼›åœ¨é›¶å”®åˆ†æä¸­ï¼Œå¯ä»¥ç”¨äºç»Ÿè®¡è´§æ¶ä¸Šçš„å•†å“æ•°é‡ï¼›åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œå¯ä»¥ç”¨äºè®¡æ•°ç»†èƒæ•°é‡ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨é€šç”¨è®¡æ•°æŠ€æœ¯çš„å‘å±•ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„è§†è§‰åˆ†æã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humans can effortlessly count diverse objects by perceiving visual repetition and structural relationships rather than relying on class identity. However, most existing counting models fail to replicate this ability; they often miscount when objects exhibit complex shapes, internal symmetry, or overlapping components. In this work, we introduce CountFormer, a transformer-based framework that learns to recognize repetition and structural coherence for class-agnostic object counting. Built upon the CounTR architecture, our model replaces its visual encoder with the self-supervised foundation model DINOv2, which produces richer and spatially consistent feature representations. We further incorporate positional embedding fusion to preserve geometric relationships before decoding these features into density maps through a lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model achieves performance comparable to current state-of-the-art methods while demonstrating superior accuracy on structurally intricate or densely packed scenes. Our findings indicate that integrating foundation models such as DINOv2 enables counting systems to approach human-like structural perception, advancing toward a truly general and exemplar-free counting paradigm.

