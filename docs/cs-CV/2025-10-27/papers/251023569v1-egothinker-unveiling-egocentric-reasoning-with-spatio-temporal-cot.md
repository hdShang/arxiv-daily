---
layout: default
title: "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT"
---

# EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23569" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23569v1</a>
  <a href="https://arxiv.org/pdf/2510.23569.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23569v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.23569v1', 'EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Baoqi Pei, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, Fei Wu, Yu Qiao, Jiangmiao Pang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

**å¤‡æ³¨**: Accepted at NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/InternRobotics/EgoThinker)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EgoThinkerï¼šåˆ©ç”¨æ—¶ç©ºCoTæ­ç¤ºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†é¢‘ç†è§£` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ€ç»´é“¾æ¨ç†` `æ—¶ç©ºå®šä½` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MLLMåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æ¨ç†ä¸­ï¼Œç¼ºä¹å¯¹éšè—æ„å›¾å’Œç»†ç²’åº¦äº¤äº’çš„ç†è§£ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. EgoThinkeré€šè¿‡æ—¶ç©ºCoTç›‘ç£å’Œä¸¤é˜¶æ®µå­¦ä¹ ï¼Œèµ‹äºˆMLLMå¼ºå¤§çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ¨ç†èƒ½åŠ›ã€‚
3. EgoThinkeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ç»†ç²’åº¦æ—¶ç©ºå®šä½ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æ¨ç†å…³æ³¨çš„æ˜¯ç›¸æœºèƒŒåä¸å¯è§çš„æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“åŠ¨æ€åœ°å¡‘é€ ç¯å¢ƒï¼Œéœ€è¦æ¨æ–­éšè—çš„æ„å›¾å¹¶è¯†åˆ«ç»†ç²’åº¦çš„äº¤äº’ã€‚è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜é™åˆ¶äº†å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œè¿™äº›æ¨¡å‹æ“…é•¿å¯è§äº‹ä»¶çš„æ¨ç†ï¼Œä½†ç¼ºä¹å…·èº«çš„ç¬¬ä¸€äººç§°ç†è§£ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoThinkerï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡æ—¶ç©ºæ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£å’Œä¸¤é˜¶æ®µå­¦ä¹ è¯¾ç¨‹ï¼Œèµ‹äºˆMLLMå¼ºå¤§çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ¨ç†èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoRe-5Mï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„QAæ•°æ®é›†ï¼Œç”±1300ä¸‡ä¸ªä¸åŒçš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç‰‡æ®µæ„å»ºè€Œæˆã€‚è¯¥æ•°æ®é›†åŒ…å«å¤šåˆ†é’Ÿçš„ç‰‡æ®µï¼Œå¹¶å¸¦æœ‰è¯¦ç»†çš„CoTç†ç”±å’Œå¯†é›†çš„hand-object groundingã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨EgoRe-5Mä¸Šé‡‡ç”¨SFTæ¥çŒè¾“æ¨ç†æŠ€èƒ½ï¼Œç„¶åè¿›è¡Œå¼ºåŒ–å¾®è°ƒRFTï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ—¶ç©ºå®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEgoThinkeråœ¨å¤šä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶åœ¨ç»†ç²’åº¦çš„æ—¶ç©ºå®šä½ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å®Œæ•´çš„ä»£ç å’Œæ•°æ®å·²åœ¨https://github.com/InternRobotics/EgoThinkerä¸Šå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ä¸­æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„MLLMè™½ç„¶åœ¨å¯è§äº‹ä»¶æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹å¯¹ç¬¬ä¸€äººç§°è§†è§’ä¸‹éšè—æ„å›¾å’Œç»†ç²’åº¦äº¤äº’çš„ç†è§£ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æ¨ç†ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥æ—¶ç©ºæ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£å’Œä¸¤é˜¶æ®µå­¦ä¹ è¯¾ç¨‹ï¼Œæ¥å¢å¼ºMLLMçš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒæ¨ç†èƒ½åŠ›ã€‚CoTç›‘ç£æ—¨åœ¨æä¾›æ›´è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£è§†é¢‘ä¸­çš„å› æœå…³ç³»å’Œæ„å›¾ã€‚ä¸¤é˜¶æ®µå­¦ä¹ è¯¾ç¨‹åˆ™åˆ†åˆ«ä¾§é‡äºæ¨ç†æŠ€èƒ½çš„çŒè¾“å’Œæ—¶ç©ºå®šä½çš„å¢å¼ºã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯è®©æ¨¡å‹ä¸ä»…èƒ½â€œçœ‹åˆ°â€å‘ç”Ÿäº†ä»€ä¹ˆï¼Œè¿˜èƒ½â€œç†è§£â€ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿï¼Œä»¥åŠåœ¨å“ªé‡Œå‘ç”Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEgoThinkeræ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1) åŸºäºEgoRe-5Mæ•°æ®é›†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç”¨äºå­¦ä¹ æ¨ç†èƒ½åŠ›ï¼›2) å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ï¼Œç”¨äºå¢å¼ºæ—¶ç©ºå®šä½èƒ½åŠ›ã€‚EgoRe-5Mæ•°æ®é›†åŒ…å«å¤§é‡çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç‰‡æ®µï¼Œå¹¶å¸¦æœ‰è¯¦ç»†çš„CoTç†ç”±å’Œhand-object groundingã€‚SFTé˜¶æ®µåˆ©ç”¨è¿™äº›æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ç”ŸæˆCoTæ¨ç†è¿‡ç¨‹ï¼ŒRFTé˜¶æ®µåˆ™è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ—¶ç©ºå®šä½èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†EgoThinkeræ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥æ—¶ç©ºCoTç›‘ç£å’Œä¸¤é˜¶æ®µå­¦ä¹ è¯¾ç¨‹ï¼Œæ˜¾è‘—æå‡äº†MLLMåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEgoThinkeræ›´æ³¨é‡å¯¹è§†é¢‘ä¸­éšè—æ„å›¾å’Œç»†ç²’åº¦äº¤äº’çš„ç†è§£ï¼Œä»è€Œå®ç°äº†æ›´å‡†ç¡®çš„æ¨ç†å’Œå®šä½ã€‚æ­¤å¤–ï¼ŒEgoRe-5Mæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æ¨ç†ç ”ç©¶æä¾›äº†å®è´µçš„æ•°æ®èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šEgoRe-5Mæ•°æ®é›†åŒ…å«13Mä¸ªè§†é¢‘ç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µéƒ½æ ‡æ³¨äº†è¯¦ç»†çš„CoTç†ç”±å’Œhand-object groundingä¿¡æ¯ã€‚åœ¨SFTé˜¶æ®µï¼Œæ¨¡å‹è¢«è®­ç»ƒç”Ÿæˆä¸è§†é¢‘å†…å®¹ç›¸å…³çš„CoTæ¨ç†è¿‡ç¨‹ã€‚åœ¨RFTé˜¶æ®µï¼Œä½¿ç”¨äº†ç‰¹å®šçš„å¥–åŠ±å‡½æ•°æ¥é¼“åŠ±æ¨¡å‹æ›´å‡†ç¡®åœ°è¿›è¡Œæ—¶ç©ºå®šä½ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰è¯¦ç»†æè¿°ï¼Œä½†æ‘˜è¦ä¸­æœªæåŠã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EgoThinkeråœ¨å¤šä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ç»†ç²’åº¦çš„æ—¶ç©ºå®šä½ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ï¼Œæ‘˜è¦ä¸­åªæåˆ°â€œsubstantial improvementsâ€ï¼Œè¡¨æ˜æå‡å¹…åº¦è¾ƒå¤§ã€‚EgoRe-5Mæ•°æ®é›†çš„å‘å¸ƒä¹Ÿä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EgoThinkerå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨æ™ºèƒ½åŠ©æ‰‹ã€äººæœºäº¤äº’ã€æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡ç†è§£ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§’ï¼ŒEgoThinkerå¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£äººç±»çš„æ„å›¾å’Œè¡Œä¸ºï¼Œä»è€Œå®ç°æ›´è‡ªç„¶ã€æ›´æœ‰æ•ˆçš„äº¤äº’ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥ç”¨äºåˆ†æå’Œç†è§£äººç±»æ´»åŠ¨ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—ä¿å¥ã€å®‰å…¨ç›‘æ§å’Œè¿åŠ¨åˆ†æç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models MLLMs, which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense hand-object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning RFT to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks. Full code and data are released at https://github.com/InternRobotics/EgoThinker.

