---
layout: default
title: Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds
---

# Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.05996" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.05996v1</a>
  <a href="https://arxiv.org/pdf/2511.05996.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.05996v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.05996v1', 'Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xianhui Meng, Yukang Huo, Li Zhang, Liu Liu, Haonan Jiang, Yan Zhong, Pingrui Zhang, Cewu Lu, Jun Liu

**åˆ†ç±»**: cs.CV, cs.AI, cs.MM

**å‘å¸ƒæ—¥æœŸ**: 2025-11-08

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mengxh20/PPFTracker)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPPF-Trackerä»¥è§£å†³å…³èŠ‚ç‰©ä½“å§¿æ€è·Ÿè¸ªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å…³èŠ‚ç‰©ä½“` `å§¿æ€è·Ÿè¸ª` `æœºå™¨äººæ“ä½œ` `å¢å¼ºç°å®` `ç‚¹å¯¹ç‰¹å¾` `SE(3)ä¸å˜æ€§` `è¿åŠ¨çº¦æŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å…³èŠ‚ç‰©ä½“çš„å§¿æ€è·Ÿè¸ªé¢ä¸´å›ºæœ‰çš„è¿åŠ¨çº¦æŸï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»ç‰©ä½“æ—¶æ•ˆæœä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„PPF-Trackeræ¡†æ¶ï¼Œé€šè¿‡ç‚¹å¯¹ç‰¹å¾å’ŒSE(3)çš„ä¸å˜æ€§ï¼Œå¢å¼ºäº†å§¿æ€è·Ÿè¸ªçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒPPF-Trackeråœ¨å¤šå¸§å§¿æ€è·Ÿè¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šç§å¤æ‚ç¯å¢ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…³èŠ‚ç‰©ä½“åœ¨æ—¥å¸¸ç”Ÿæ´»å’Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­æ™®éå­˜åœ¨ã€‚ç„¶è€Œï¼Œä¸åˆšæ€§ç‰©ä½“ç›¸æ¯”ï¼Œå…³èŠ‚ç‰©ä½“çš„å§¿æ€è·Ÿè¸ªä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºå…¶å›ºæœ‰çš„è¿åŠ¨çº¦æŸã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºç‚¹å¯¹çš„å§¿æ€è·Ÿè¸ªæ¡†æ¶ï¼Œç§°ä¸ºPPF-Trackerã€‚è¯¥æ¡†æ¶é¦–å…ˆåœ¨SE(3)æç¾¤ç©ºé—´ä¸­å¯¹ç‚¹äº‘è¿›è¡Œå‡†è§„èŒƒåŒ–ï¼Œç„¶ååˆ©ç”¨ç‚¹å¯¹ç‰¹å¾ï¼ˆPPFï¼‰å»ºæ¨¡å…³èŠ‚ç‰©ä½“ï¼Œé€šè¿‡åˆ©ç”¨SE(3)çš„ä¸å˜æ€§å±æ€§æ¥é¢„æµ‹å§¿æ€æŠ•ç¥¨å‚æ•°ã€‚æœ€åï¼Œç»“åˆå…³èŠ‚è½´çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥åœ¨å…³èŠ‚ç‰©ä½“çš„æ‰€æœ‰éƒ¨åˆ†æ–½åŠ ç»Ÿä¸€çš„è¿åŠ¨çº¦æŸã€‚PPF-Trackeråœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®åœºæ™¯ä¸­è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šå¸§å§¿æ€è·Ÿè¸ªä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å…³èŠ‚ç‰©ä½“çš„å§¿æ€è·Ÿè¸ªé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…³èŠ‚ç‰©ä½“æ—¶ç”±äºè¿åŠ¨çº¦æŸè€Œè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥å®ç°å‡†ç¡®è·Ÿè¸ªã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPPF-Trackeré€šè¿‡å¯¹ç‚¹äº‘è¿›è¡Œå‡†è§„èŒƒåŒ–ï¼Œå¹¶åˆ©ç”¨ç‚¹å¯¹ç‰¹å¾ï¼ˆPPFï¼‰æ¥å»ºæ¨¡å…³èŠ‚ç‰©ä½“ï¼Œç»“åˆSE(3)çš„ä¸å˜æ€§æ¥é¢„æµ‹å§¿æ€æŠ•ç¥¨å‚æ•°ï¼Œä»è€Œæå‡è·Ÿè¸ªç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯ç‚¹äº‘çš„å‡†è§„èŒƒåŒ–å¤„ç†ï¼Œå…¶æ¬¡æ˜¯åŸºäºPPFçš„å§¿æ€é¢„æµ‹ï¼Œæœ€åæ˜¯ç»“åˆå…³èŠ‚è½´çš„è¯­ä¹‰ä¿¡æ¯ä»¥æ–½åŠ è¿åŠ¨çº¦æŸã€‚

**å…³é”®åˆ›æ–°**ï¼šPPF-Trackerçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ç‚¹å¯¹ç‰¹å¾ä¸SE(3)çš„ä¸å˜æ€§ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å§¿æ€è·Ÿè¸ªæ–¹æ³•ï¼Œæ˜¾è‘—æ”¹å–„äº†å¯¹å…³èŠ‚ç‰©ä½“çš„è·Ÿè¸ªèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å§¿æ€æŠ•ç¥¨å‚æ•°ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸­å¼•å…¥äº†è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥ç¡®ä¿å…³èŠ‚ç‰©ä½“å„éƒ¨åˆ†ä¹‹é—´çš„è¿åŠ¨ä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPPF-Trackeråœ¨å¤šå¸§å§¿æ€è·Ÿè¸ªä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œè·Ÿè¸ªç²¾åº¦æé«˜äº†çº¦20%ï¼Œå¹¶åœ¨å¤šç§å¤æ‚ç¯å¢ƒä¸­å±•ç°äº†å¼ºå¤§çš„é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æœºå™¨äººæ“ä½œã€å¢å¼ºç°å®å’Œæ™ºèƒ½äº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å…³èŠ‚ç‰©ä½“çš„å§¿æ€è·Ÿè¸ªèƒ½åŠ›ï¼ŒPPF-Trackerèƒ½å¤Ÿä¿ƒè¿›æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»æ“ä½œå’Œäººæœºåä½œã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œæå‡å…¶é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Articulated objects are prevalent in daily life and robotic manipulation tasks. However, compared to rigid objects, pose tracking for articulated objects remains an underexplored problem due to their inherent kinematic constraints. To address these challenges, this work proposes a novel point-pair-based pose tracking framework, termed \textbf{PPF-Tracker}. The proposed framework first performs quasi-canonicalization of point clouds in the SE(3) Lie group space, and then models articulated objects using Point Pair Features (PPF) to predict pose voting parameters by leveraging the invariance properties of SE(3). Finally, semantic information of joint axes is incorporated to impose unified kinematic constraints across all parts of the articulated object. PPF-Tracker is systematically evaluated on both synthetic datasets and real-world scenarios, demonstrating strong generalization across diverse and challenging environments. Experimental results highlight the effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of articulated objects. We believe this work can foster advances in robotics, embodied intelligence, and augmented reality. Codes are available at https://github.com/mengxh20/PPFTracker.

