---
layout: default
title: MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering
---

# MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.05876" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.05876v3</a>
  <a href="https://arxiv.org/pdf/2511.05876.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.05876v3" onclick="toggleFavorite(this, '2511.05876v3', 'MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jian Zhu, Xin Zou, Jun Sun, Cheng Luo, Lei Liu, Lingfang Zeng, Ning Zhang, Bian Wu, Chang Tang, Lirong Dai

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-08 (æ›´æ–°: 2025-11-29)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HackerHyper/MoEGCL)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMoEGCLï¼Œé€šè¿‡æ··åˆè‡ª Ego å›¾å¯¹æ¯”å­¦ä¹ æå‡å¤šè§†å›¾èšç±»æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šè§†å›¾èšç±»` `å›¾ç¥ç»ç½‘ç»œ` `å¯¹æ¯”å­¦ä¹ ` `è‡ª Ego å›¾` `æ··åˆä¸“å®¶ç½‘ç»œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šè§†å›¾èšç±»ä¸­é‡‡ç”¨ç²—ç²’åº¦çš„å›¾èåˆç­–ç•¥ï¼Œå¿½ç•¥äº†æ ·æœ¬çº§åˆ«çš„ç»†ç²’åº¦ä¿¡æ¯ã€‚
2. MoEGCL æå‡ºæ··åˆè‡ª Ego å›¾èåˆï¼ˆMoEGFï¼‰å’Œ Ego å›¾å¯¹æ¯”å­¦ä¹ ï¼ˆEGCLï¼‰ï¼Œå®ç°æ ·æœ¬çº§åˆ«çš„ç»†ç²’åº¦å›¾èåˆå’Œè¡¨ç¤ºå¯¹é½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMoEGCL åœ¨æ·±åº¦å¤šè§†å›¾èšç±»ä»»åŠ¡ä¸­å–å¾—äº† state-of-the-art çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„è¿›æ­¥æ˜¾è‘—æ¨åŠ¨äº†å¤šè§†å›¾èšç±»ï¼ˆMVCï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ç²—ç²’åº¦å›¾èåˆçš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆä¸€ä¸ªå•ç‹¬çš„å›¾ç»“æ„ï¼Œç„¶ååœ¨è§†å›¾çº§åˆ«æ‰§è¡Œå›¾ç»“æ„çš„åŠ æƒèåˆï¼Œè¿™æ˜¯ä¸€ç§ç›¸å¯¹ç²—ç•¥çš„ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆè‡ª Ego å›¾å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ï¼ˆMoEGCLï¼‰ã€‚å®ƒä¸»è¦ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ··åˆè‡ª Ego å›¾èåˆï¼ˆMoEGFï¼‰ï¼Œå®ƒæ„å»º Ego å›¾ï¼Œå¹¶åˆ©ç”¨æ··åˆä¸“å®¶ç½‘ç»œæ¥å®ç°æ ·æœ¬çº§åˆ«ä¸Šçš„ç»†ç²’åº¦ Ego å›¾èåˆï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„è§†å›¾çº§åˆ«èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº† Ego å›¾å¯¹æ¯”å­¦ä¹ ï¼ˆEGCLï¼‰æ¨¡å—ï¼Œä»¥å°†èåˆçš„è¡¨ç¤ºä¸ç‰¹å®šäºè§†å›¾çš„è¡¨ç¤ºå¯¹é½ã€‚EGCL æ¨¡å—å¢å¼ºäº†æ¥è‡ªåŒä¸€ç°‡çš„æ ·æœ¬çš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯æ¥è‡ªåŒä¸€æ ·æœ¬çš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡äº†ç»†ç²’åº¦å›¾è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoEGCL åœ¨æ·±åº¦å¤šè§†å›¾èšç±»ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æºä»£ç å¯åœ¨ https://github.com/HackerHyper/MoEGCL å…¬å¼€è·å¾—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šè§†å›¾èšç±»æ–¹æ³•é€šå¸¸åœ¨è§†å›¾çº§åˆ«è¿›è¡Œå›¾èåˆï¼Œå¿½ç•¥äº†æ ·æœ¬çº§åˆ«çš„ç»†ç²’åº¦ä¿¡æ¯ã€‚è¿™ç§ç²—ç²’åº¦çš„èåˆæ–¹å¼æ— æ³•å……åˆ†åˆ©ç”¨ä¸åŒè§†å›¾ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¼è‡´èšç±»æ€§èƒ½å—é™ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰ä¸åŒè§†å›¾ä¸­åŒä¸€æ ·æœ¬çš„ç»†å¾®å·®å¼‚å’Œå…³è”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoEGCL çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æ ·æœ¬çº§åˆ«è¿›è¡Œç»†ç²’åº¦çš„å›¾èåˆï¼Œå¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¥å¯¹é½èåˆåçš„è¡¨ç¤ºå’Œè§†å›¾ç‰¹å®šçš„è¡¨ç¤ºã€‚é€šè¿‡æ„å»º Ego å›¾å¹¶ä½¿ç”¨æ··åˆä¸“å®¶ç½‘ç»œè¿›è¡Œèåˆï¼Œå¯ä»¥æ›´ç²¾ç»†åœ°æ•æ‰æ ·æœ¬åœ¨ä¸åŒè§†å›¾ä¸­çš„ç‰¹å¾ã€‚å¯¹æ¯”å­¦ä¹ åˆ™ç”¨äºå¢å¼ºåŒä¸€ç°‡æ ·æœ¬çš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œä»è€Œæé«˜èšç±»æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoEGCL ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šæ··åˆè‡ª Ego å›¾èåˆï¼ˆMoEGFï¼‰å’Œ Ego å›¾å¯¹æ¯”å­¦ä¹ ï¼ˆEGCLï¼‰ã€‚é¦–å…ˆï¼ŒMoEGF æ¨¡å—ä¸ºæ¯ä¸ªæ ·æœ¬æ„å»º Ego å›¾ï¼Œç„¶ååˆ©ç”¨æ··åˆä¸“å®¶ç½‘ç»œåœ¨æ ·æœ¬çº§åˆ«èåˆè¿™äº› Ego å›¾ã€‚èåˆåçš„è¡¨ç¤ºéšåè¢«è¾“å…¥åˆ° EGCL æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡å¯¹æ¯”å­¦ä¹ å°†èåˆçš„è¡¨ç¤ºä¸è§†å›¾ç‰¹å®šçš„è¡¨ç¤ºå¯¹é½ã€‚æ•´ä¸ªæ¡†æ¶æ—¨åœ¨å­¦ä¹ æ›´å…·åˆ¤åˆ«æ€§çš„å¤šè§†å›¾è¡¨ç¤ºï¼Œä»è€Œæé«˜èšç±»æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoEGCL çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ··åˆè‡ª Ego å›¾èåˆï¼ˆMoEGFï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å®ç°äº†æ ·æœ¬çº§åˆ«çš„ç»†ç²’åº¦å›¾èåˆã€‚ä¸ä¼ ç»Ÿçš„è§†å›¾çº§åˆ«èåˆç›¸æ¯”ï¼ŒMoEGF èƒ½å¤Ÿæ›´ç²¾ç»†åœ°æ•æ‰æ ·æœ¬åœ¨ä¸åŒè§†å›¾ä¸­çš„ç‰¹å¾ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´å…·åˆ¤åˆ«æ€§çš„è¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒEGCL æ¨¡å—é€šè¿‡å¯¹æ¯”å­¦ä¹ å¢å¼ºäº†åŒä¸€ç°‡æ ·æœ¬çš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œè¿›ä¸€æ­¥æé«˜äº†èšç±»æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šMoEGF æ¨¡å—ä½¿ç”¨æ··åˆä¸“å®¶ç½‘ç»œæ¥èåˆ Ego å›¾ã€‚æ¯ä¸ªä¸“å®¶ç½‘ç»œè´Ÿè´£å­¦ä¹ ç‰¹å®šè§†å›¾çš„ç‰¹å¾ï¼Œè€Œæ··åˆæƒé‡åˆ™æ ¹æ®æ ·æœ¬çš„ç‰¹å¾åŠ¨æ€è°ƒæ•´ã€‚EGCL æ¨¡å—ä½¿ç”¨ InfoNCE æŸå¤±å‡½æ•°è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œè¯¥æŸå¤±å‡½æ•°æ—¨åœ¨æœ€å¤§åŒ–åŒä¸€ç°‡æ ·æœ¬çš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–ä¸åŒç°‡æ ·æœ¬çš„è¡¨ç¤ºç›¸ä¼¼æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MoEGCL åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„å¤šè§†å›¾èšç±»æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼ŒMoEGCL çš„èšç±»å‡†ç¡®ç‡ï¼ˆACCï¼‰å’Œå½’ä¸€åŒ–äº’ä¿¡æ¯ï¼ˆNMIï¼‰æŒ‡æ ‡æå‡äº† 5% ä»¥ä¸Šã€‚å®éªŒç»“æœéªŒè¯äº† MoEGCL åœ¨ç»†ç²’åº¦å›¾èåˆå’Œå¯¹æ¯”å­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoEGCL å¯åº”ç”¨äºå¤šç§éœ€è¦å¤šè§†å›¾æ•°æ®èšç±»çš„åœºæ™¯ï¼Œä¾‹å¦‚ç¤¾äº¤ç½‘ç»œåˆ†æï¼ˆåŸºäºç”¨æˆ·è¡Œä¸ºã€å…´è¶£ç­‰å¤šè§†å›¾æ•°æ®è¿›è¡Œç”¨æˆ·èšç±»ï¼‰ã€ç”Ÿç‰©ä¿¡æ¯å­¦ï¼ˆåŸºäºåŸºå› è¡¨è¾¾ã€è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç­‰å¤šè§†å›¾æ•°æ®è¿›è¡Œç–¾ç—…äºšå‹åˆ†ç±»ï¼‰ã€å›¾åƒèšç±»ï¼ˆåŸºäºä¸åŒç‰¹å¾æå–æ–¹æ³•å¾—åˆ°çš„å¤šè§†å›¾å›¾åƒæ•°æ®è¿›è¡Œå›¾åƒèšç±»ï¼‰ç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡å¤šè§†å›¾æ•°æ®åˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.

