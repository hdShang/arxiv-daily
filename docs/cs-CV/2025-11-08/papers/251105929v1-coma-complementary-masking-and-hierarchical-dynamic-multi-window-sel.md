---
layout: default
title: CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework
---

# CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.05929" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.05929v1</a>
  <a href="https://arxiv.org/pdf/2511.05929.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.05929v1" onclick="toggleFavorite(this, '2511.05929v1', 'CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaxuan Li, Qing Xu, Xiangjian He, Ziyu Liu, Chang Xing, Zhen Chen, Daokun Zhang, Rong Qu, Chang Wen Chen

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-08

**å¤‡æ³¨**: 9 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CoMAï¼šäº’è¡¥æ©ç ä¸åˆ†å±‚åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›ï¼Œæå‡MAEé¢„è®­ç»ƒæ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªç›‘ç£å­¦ä¹ ` `æ©ç è‡ªç¼–ç å™¨` `è§†è§‰Transformer` `äº’è¡¥æ©ç ` `åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. MAEåŠå…¶å˜ä½“ä¾èµ–éšæœºæ©ç ï¼Œéœ€æ›´å¤šé¢„è®­ç»ƒè½®æ¬¡ä»¥ä¿è¯é€‚åº”æ€§ï¼Œä¸”ViTåœ¨MAEä¸­å‚æ•°åˆ©ç”¨ç‡ä½ã€‚
2. CoMAé‡‡ç”¨äº’è¡¥æ©ç ç­–ç•¥ï¼Œç¡®ä¿åƒç´ å‡åŒ€é‡‡æ ·ï¼Œæå‡ç‰¹å¾å­¦ä¹ æ•ˆç‡å’Œæ¨¡å‹é€‚åº”æ€§ã€‚
3. DyViTå¼•å…¥åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›ï¼Œå‡å°‘å‚æ•°å’Œè®¡ç®—é‡ï¼ŒåŒæ—¶æå‡ç»†ç²’åº¦ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†äº’è¡¥æ©ç è‡ªç¼–ç å™¨ï¼ˆCoMAï¼‰ï¼Œæ—¨åœ¨æå‡æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„é¢„è®­ç»ƒæ•ˆç‡å’Œä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ã€‚CoMAé‡‡ç”¨äº’è¡¥æ©ç ç­–ç•¥ï¼Œç¡®ä¿æ‰€æœ‰åƒç´ çš„å‡åŒ€é‡‡æ ·ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ æ‰€æœ‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†DyViTï¼Œä¸€ç§åˆ†å±‚è§†è§‰Transformerï¼Œå®ƒé‡‡ç”¨åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›ï¼ˆDM-MSAï¼‰ï¼Œæ˜¾è‘—å‡å°‘å‚æ•°å’ŒFLOPsï¼ŒåŒæ—¶æ”¹è¿›äº†ç»†ç²’åº¦ç‰¹å¾å­¦ä¹ ã€‚åœ¨ImageNet-1Kä¸Šä½¿ç”¨CoMAè¿›è¡Œé¢„è®­ç»ƒåï¼ŒDyViTä»…ä½¿ç”¨MAEé¢„è®­ç»ƒepochçš„12%å³å¯è¾¾åˆ°ä¸å…¶ç›¸å½“çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œå±•ç¤ºäº†æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚æ¯ä¸ªepochçš„é¢„è®­ç»ƒæ—¶é—´ä¹Ÿå‡å°‘äº†10%ï¼Œè¿›ä¸€æ­¥çªå‡ºäº†å…¶å“è¶Šçš„é¢„è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„MAEæ–¹æ³•ä¾èµ–äºéšæœºæ©ç ç­–ç•¥ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡è¾ƒä½ï¼Œéœ€è¦å¤§é‡çš„é¢„è®­ç»ƒepochæ‰èƒ½è·å¾—è‰¯å¥½çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ ‡å‡†çš„ViTç»“æ„åœ¨MAEä¸­åº”ç”¨æ—¶ï¼Œç”±äºå„å±‚ç©ºé—´åˆ†è¾¨ç‡å›ºå®šï¼Œå­˜åœ¨å‚æ•°åˆ©ç”¨ç‡ä¸é«˜çš„é—®é¢˜ã€‚å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨è§£å†³MAEé¢„è®­ç»ƒæ•ˆç‡ä½å’Œå‚æ•°åˆ©ç”¨ç‡ä¸é«˜çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡äº’è¡¥æ©ç ç­–ç•¥æ¥ä¿è¯æ‰€æœ‰åƒç´ çš„å‡åŒ€é‡‡æ ·ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ å›¾åƒçš„å„ç§ç‰¹å¾ï¼Œæé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡å°‘æ¨¡å‹çš„å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼Œæå‡ç»†ç²’åº¦ç‰¹å¾çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCoMAçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šäº’è¡¥æ©ç ç­–ç•¥å’ŒDyViTã€‚äº’è¡¥æ©ç ç­–ç•¥ç”¨äºç”Ÿæˆæ©ç ï¼Œç¡®ä¿æ¯ä¸ªåƒç´ éƒ½æœ‰ç›¸åŒçš„æ¦‚ç‡è¢«æ©ç›–ã€‚DyViTæ˜¯ä¸€ä¸ªåˆ†å±‚è§†è§‰Transformerï¼Œå®ƒä½¿ç”¨åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æå–å›¾åƒç‰¹å¾ã€‚é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡é‡å»ºè¢«æ©ç›–çš„å›¾åƒåŒºåŸŸè¿›è¡Œå­¦ä¹ ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†äº’è¡¥æ©ç ç­–ç•¥å’ŒåŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚äº’è¡¥æ©ç ç­–ç•¥ä¸éšæœºæ©ç ç­–ç•¥ä¸åŒï¼Œå®ƒä¿è¯äº†æ‰€æœ‰åƒç´ çš„å‡åŒ€é‡‡æ ·ï¼Œä»è€Œé¿å…äº†æŸäº›åƒç´ è¢«è¿‡åº¦é‡‡æ ·è€Œå¦ä¸€äº›åƒç´ è¢«å¿½ç•¥çš„é—®é¢˜ã€‚åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨ä¸åŒçš„çª—å£å¤§å°ä¸Šè¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰å›¾åƒçš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šäº’è¡¥æ©ç ç­–ç•¥é€šè¿‡å°†å›¾åƒåˆ’åˆ†ä¸ºå¤šä¸ªäº’è¡¥çš„æ©ç é›†åˆæ¥å®ç°ï¼Œæ¯ä¸ªé›†åˆä¸­çš„æ©ç è¦†ç›–å›¾åƒçš„ä¸åŒåŒºåŸŸï¼Œç¡®ä¿æ‰€æœ‰åƒç´ åœ¨æ‰€æœ‰é›†åˆä¸­éƒ½è¢«è¦†ç›–ä¸€æ¬¡ã€‚åŠ¨æ€å¤šçª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶é€šè¿‡åœ¨ä¸åŒçš„çª—å£å¤§å°ä¸Šå¹¶è¡Œè®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œå¹¶å°†ç»“æœè¿›è¡Œèåˆæ¥å®ç°ã€‚å…·ä½“çš„çª—å£å¤§å°è®¾ç½®å’Œèåˆæ–¹å¼éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CoMAé¢„è®­ç»ƒçš„DyViTä»…ä½¿ç”¨MAEé¢„è®­ç»ƒepochçš„12%å³å¯è¾¾åˆ°ä¸å…¶ç›¸å½“çš„ä¸‹æ¸¸æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªepochçš„é¢„è®­ç»ƒæ—¶é—´ä¹Ÿå‡å°‘äº†10%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒCoMAèƒ½å¤Ÿæ˜¾è‘—æé«˜MAEçš„é¢„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶æå‡æ¨¡å‹çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CoMAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚å…¶é«˜æ•ˆçš„é¢„è®­ç»ƒèƒ½åŠ›å¯ä»¥é™ä½æ¨¡å‹è®­ç»ƒæˆæœ¬ï¼ŒåŠ é€Ÿæ¨¡å‹å¼€å‘å‘¨æœŸã€‚è¯¥ç ”ç©¶å¯¹äºæ¨åŠ¨è‡ªç›‘ç£å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶å¯èƒ½ä¿ƒè¿›ç›¸å…³æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®‰é˜²ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Masked Autoencoders (MAE) achieve self-supervised learning of image representations by randomly removing a portion of visual tokens and reconstructing the original image as a pretext task, thereby significantly enhancing pretraining efficiency and yielding excellent adaptability across downstream tasks. However, MAE and other MAE-style paradigms that adopt random masking generally require more pre-training epochs to maintain adaptability. Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed spatial resolution across layers. To overcome these limitations, we propose the Complementary Masked Autoencoders (CoMA), which employ a complementary masking strategy to ensure uniform sampling across all pixels, thereby improving effective learning of all features and enhancing the model's adaptability. Furthermore, we introduce DyViT, a hierarchical vision transformer that employs a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the parameters and FLOPs while improving fine-grained feature learning. Pre-trained on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using only 12% of the pre-training epochs, demonstrating more effective learning. It also attains a 10% reduction in pre-training time per epoch, further underscoring its superior pre-training efficiency.

