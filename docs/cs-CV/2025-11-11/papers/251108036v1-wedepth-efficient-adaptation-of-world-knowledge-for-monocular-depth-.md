---
layout: default
title: WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation
---

# WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.08036" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.08036v1</a>
  <a href="https://arxiv.org/pdf/2511.08036.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08036v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.08036v1', 'WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gongshu Wang, Zhirui Wang, Kan Yang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**WEDepthï¼šé«˜æ•ˆåˆ©ç”¨ä¸–ç•ŒçŸ¥è¯†è‡ªé€‚åº”å•ç›®æ·±åº¦ä¼°è®¡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å•ç›®æ·±åº¦ä¼°è®¡` `è§†è§‰åŸºç¡€æ¨¡å‹` `çŸ¥è¯†è¿ç§»` `å¤šå±‚æ¬¡ç‰¹å¾` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å•ç›®æ·±åº¦ä¼°è®¡é¢ä¸´ä»2Då›¾åƒæ¨æ–­3Dä¿¡æ¯çš„å›ºæœ‰éš¾é¢˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨å›¾åƒä¸­çš„ä¸–ç•ŒçŸ¥è¯†ã€‚
2. WEDepthé€šè¿‡å°†è§†è§‰åŸºç¡€æ¨¡å‹(VFM)ä½œä¸ºå¤šå±‚æ¬¡ç‰¹å¾å¢å¼ºå™¨ï¼Œåœ¨ä¸åŒè¡¨ç¤ºå±‚çº§æ³¨å…¥å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°é«˜æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒWEDepthåœ¨NYU-Depth v2å’ŒKITTIæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å•ç›®æ·±åº¦ä¼°è®¡(MDE)åº”ç”¨å¹¿æ³›ï¼Œä½†ç”±äºä»2Då›¾åƒé‡å»º3Dåœºæ™¯çš„å›ºæœ‰ä¸é€‚å®šæ€§è€Œæå…·æŒ‘æˆ˜ã€‚ç°ä»£è§†è§‰åŸºç¡€æ¨¡å‹(VFMs)åœ¨å¤§å‹å¤šæ ·åŒ–æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œè¡¨ç°å‡ºå“è¶Šçš„ä¸–ç•Œç†è§£èƒ½åŠ›ï¼Œè¿™æœ‰åˆ©äºå„ç§è§†è§‰ä»»åŠ¡ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¾®è°ƒè¿™äº›VFMsï¼ŒMDEå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å—è¿™äº›è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†WEDepthï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºè°ƒæ•´VFMsä»¥è¿›è¡ŒMDEï¼Œè€Œæ— éœ€ä¿®æ”¹å…¶ç»“æ„å’Œé¢„è®­ç»ƒæƒé‡ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°å¼•å‡ºå’Œåˆ©ç”¨å…¶å›ºæœ‰çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨VFMä½œä¸ºå¤šå±‚æ¬¡ç‰¹å¾å¢å¼ºå™¨ï¼Œç³»ç»Ÿåœ°åœ¨ä¸åŒçš„è¡¨ç¤ºå±‚æ¬¡ä¸Šæ³¨å…¥å…ˆéªŒçŸ¥è¯†ã€‚åœ¨NYU-Depth v2å’ŒKITTIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒWEDepthå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›(SOTA)æ€§èƒ½ï¼Œä¸åŸºäºæ‰©æ•£çš„æ–¹æ³•(éœ€è¦å¤šæ¬¡å‰å‘ä¼ é€’)å’Œåœ¨ç›¸å¯¹æ·±åº¦ä¸Šé¢„è®­ç»ƒçš„æ–¹æ³•ç›¸æ¯”ï¼Œå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå•ç›®æ·±åº¦ä¼°è®¡æ—¨åœ¨ä»å•å¼ 2Då›¾åƒä¸­é¢„æµ‹åœºæ™¯çš„æ·±åº¦ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å›¾åƒä¸­è•´å«çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œå¯¼è‡´æ·±åº¦ä¼°è®¡ç²¾åº¦å—é™ã€‚æ­¤å¤–ï¼Œç›´æ¥å¾®è°ƒå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”å¯èƒ½ç ´åå…¶é¢„è®­ç»ƒçš„é€šç”¨çŸ¥è¯†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šWEDepthçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ä½œä¸ºçŸ¥è¯†æ¥æºï¼Œé€šè¿‡ä¸€ç§é«˜æ•ˆçš„è‡ªé€‚åº”æ–¹å¼ï¼Œå°†VFMsä¸­è•´å«çš„ä¸–ç•ŒçŸ¥è¯†è¿ç§»åˆ°å•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­ã€‚è¯¥æ–¹æ³•é¿å…äº†å¯¹VFMç»“æ„çš„ä¿®æ”¹å’Œæƒé‡çš„å¾®è°ƒï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¿ç•™äº†VFMçš„é€šç”¨æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWEDepthçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ï¼šä½œä¸ºç‰¹å¾æå–å™¨ï¼Œæå–è¾“å…¥å›¾åƒçš„å¤šå±‚æ¬¡ç‰¹å¾è¡¨ç¤ºã€‚2) å¤šå±‚æ¬¡ç‰¹å¾æ³¨å…¥ï¼šå°†VFMæå–çš„ç‰¹å¾æ³¨å…¥åˆ°æ·±åº¦ä¼°è®¡ç½‘ç»œçš„å„ä¸ªå±‚çº§ï¼Œä»è€Œå°†VFMçš„å…ˆéªŒçŸ¥è¯†ä¼ é€’ç»™æ·±åº¦ä¼°è®¡ç½‘ç»œã€‚3) æ·±åº¦ä¼°è®¡ç½‘ç»œï¼šè´Ÿè´£ä»èåˆäº†VFMç‰¹å¾çš„å›¾åƒè¡¨ç¤ºä¸­é¢„æµ‹æ·±åº¦å›¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šWEDepthçš„å…³é”®åˆ›æ–°åœ¨äºå…¶é«˜æ•ˆçš„çŸ¥è¯†è¿ç§»æœºåˆ¶ã€‚å®ƒé€šè¿‡å¤šå±‚æ¬¡ç‰¹å¾æ³¨å…¥çš„æ–¹å¼ï¼Œå°†VFMçš„çŸ¥è¯†èå…¥åˆ°æ·±åº¦ä¼°è®¡ç½‘ç»œä¸­ï¼Œè€Œæ— éœ€å¯¹VFMè¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•æ—¢é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œåˆä¿ç•™äº†VFMçš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒå±‚çº§æ³¨å…¥çŸ¥è¯†ï¼Œä½¿å¾—æ·±åº¦ä¼°è®¡ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ åˆ°ä¸åŒç²’åº¦çš„å…ˆéªŒä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šWEDepthçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) VFMçš„é€‰æ‹©ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰Transformeræ¨¡å‹ä½œä¸ºVFMã€‚2) ç‰¹å¾æ³¨å…¥æ–¹å¼ï¼šé‡‡ç”¨äº†æ®‹å·®è¿æ¥çš„æ–¹å¼å°†VFMçš„ç‰¹å¾æ³¨å…¥åˆ°æ·±åº¦ä¼°è®¡ç½‘ç»œä¸­ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚3) æŸå¤±å‡½æ•°ï¼šä½¿ç”¨äº†æ·±åº¦å›å½’å¸¸ç”¨çš„L1æŸå¤±å‡½æ•°å’Œå°ºåº¦ä¸å˜æ¢¯åº¦æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜æ·±åº¦ä¼°è®¡çš„ç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

WEDepthåœ¨NYU-Depth v2å’ŒKITTIæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚åœ¨NYU-Depth v2æ•°æ®é›†ä¸Šï¼ŒWEDepthçš„ç»å¯¹ç›¸å¯¹è¯¯å·®(Abs Rel)ä¸º0.068ï¼Œå‡æ–¹æ ¹è¯¯å·®(RMSE)ä¸º0.265ï¼Œä¼˜äºç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•å’Œåœ¨ç›¸å¯¹æ·±åº¦ä¸Šé¢„è®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒWEDepthè¿˜å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œåœ¨æœªè§è¿‡çš„åœºæ™¯ä¸­ä¹Ÿèƒ½å–å¾—è¾ƒå¥½çš„æ·±åº¦ä¼°è®¡ç»“æœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

WEDepthåœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ç²¾ç¡®çš„å•ç›®æ·±åº¦ä¼°è®¡å¯ä»¥å¸®åŠ©è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒï¼Œæé«˜å¯¼èˆªçš„å®‰å…¨æ€§ã€‚åœ¨æœºå™¨äººé¢†åŸŸï¼Œæ·±åº¦ä¿¡æ¯å¯ä»¥ç”¨äºç‰©ä½“è¯†åˆ«ã€æŠ“å–å’Œåœºæ™¯é‡å»ºã€‚å¢å¼ºç°å®åº”ç”¨åˆ™å¯ä»¥åˆ©ç”¨æ·±åº¦ä¿¡æ¯å®ç°æ›´é€¼çœŸçš„è™šæ‹Ÿç‰©ä½“å åŠ æ•ˆæœã€‚è¯¥ç ”ç©¶é™ä½äº†å•ç›®æ·±åº¦ä¼°è®¡å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œä½¿å…¶æ›´å®¹æ˜“éƒ¨ç½²åœ¨ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.

