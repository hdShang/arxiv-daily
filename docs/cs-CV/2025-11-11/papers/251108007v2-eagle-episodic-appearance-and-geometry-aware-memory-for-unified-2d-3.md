---
layout: default
title: "EAGLE: Episodic Appearance- and Geometry-aware Memory for Unified 2D-3D Visual Query Localization in Egocentric Vision"
---

# EAGLE: Episodic Appearance- and Geometry-aware Memory for Unified 2D-3D Visual Query Localization in Egocentric Vision

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.08007" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.08007v2</a>
  <a href="https://arxiv.org/pdf/2511.08007.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08007v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.08007v2', 'EAGLE: Episodic Appearance- and Geometry-aware Memory for Unified 2D-3D Visual Query Localization in Egocentric Vision')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifei Cao, Yu Liu, Guolong Wang, Zhu Liu, Kai Wang, Xianjie Zhang, Jizhe Yu, Xun Tu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11 (æ›´æ–°: 2025-11-12)

**å¤‡æ³¨**: 13 Pages, accepted by AAAI-2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EAGLEï¼šåŸºäºæƒ…æ™¯å¤–è§‚å’Œå‡ ä½•æ„ŸçŸ¥çš„è®°å¿†ï¼Œç”¨äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰æŸ¥è¯¢å®šä½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰æŸ¥è¯¢å®šä½` `ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†è§‰` `æƒ…æ™¯è®°å¿†` `å¤–è§‚æ„ŸçŸ¥` `å‡ ä½•æ„ŸçŸ¥` `2D-3Dç»Ÿä¸€` `å…ƒå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰æŸ¥è¯¢å®šä½é¢ä¸´ç›¸æœºè¿åŠ¨ã€è§†è§’å˜åŒ–å’Œå¤–è§‚å·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹ã€‚
2. EAGLEæ¡†æ¶é€šè¿‡æƒ…æ™¯å¤–è§‚å’Œå‡ ä½•æ„ŸçŸ¥çš„è®°å¿†ï¼ŒååŒæ•´åˆå¤–è§‚æ„ŸçŸ¥åˆ†å‰²å’Œå‡ ä½•æ„ŸçŸ¥è·Ÿè¸ªï¼Œå®ç°é²æ£’çš„è§†è§‰æŸ¥è¯¢å®šä½ã€‚
3. EAGLEåœ¨Ego4D-VQåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰æŸ¥è¯¢å®šä½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰æŸ¥è¯¢å®šä½å¯¹äºå…·èº«æ™ºèƒ½å’ŒVR/ARè‡³å…³é‡è¦ï¼Œä½†ç”±äºç›¸æœºè¿åŠ¨ã€è§†è§’å˜åŒ–å’Œå¤–è§‚å·®å¼‚è€Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†EAGLEï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æƒ…æ™¯å¤–è§‚å’Œå‡ ä½•æ„ŸçŸ¥çš„è®°å¿†æ¥å®ç°ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰ä¸­ç»Ÿä¸€çš„2D-3Dè§†è§‰æŸ¥è¯¢å®šä½ã€‚å—åˆ°é¸Ÿç±»è®°å¿†å·©å›ºçš„å¯å‘ï¼ŒEAGLEååŒåœ°æ•´åˆäº†ç”±å¤–è§‚æ„ŸçŸ¥å…ƒå­¦ä¹ è®°å¿†ï¼ˆAMMï¼‰å¼•å¯¼çš„åˆ†å‰²ï¼Œä»¥åŠç”±å‡ ä½•æ„ŸçŸ¥å®šä½è®°å¿†ï¼ˆGLMï¼‰é©±åŠ¨çš„è·Ÿè¸ªã€‚è¿™ç§è®°å¿†å·©å›ºæœºåˆ¶ï¼Œé€šè¿‡ç»“æ„åŒ–çš„å¤–è§‚å’Œå‡ ä½•è®°å¿†åº“ï¼Œå­˜å‚¨é«˜ç½®ä¿¡åº¦çš„æ£€ç´¢æ ·æœ¬ï¼Œæœ‰æ•ˆåœ°æ”¯æŒç›®æ ‡å¤–è§‚å˜åŒ–çš„é•¿æœŸå’ŒçŸ­æœŸå»ºæ¨¡ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿç²¾ç¡®åœ°æç»˜è½®å»“ï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„ç©ºé—´è¾¨åˆ«èƒ½åŠ›ï¼Œä»è€Œæ˜¾è‘—æé«˜æ£€ç´¢ç²¾åº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†VQL-2Dè¾“å‡ºä¸è§†è§‰å‡ ä½•æ¥åœ°çš„Transformerï¼ˆVGGTï¼‰é›†æˆï¼Œæˆ‘ä»¬å®ç°äº†2Då’Œ3Dä»»åŠ¡çš„æœ‰æ•ˆç»Ÿä¸€ï¼Œä»è€Œèƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°åæŠ•å½±åˆ°3Dç©ºé—´ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨Ego4D-VQåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰æŸ¥è¯¢å®šä½é—®é¢˜ï¼Œå³åœ¨ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ä¸­ï¼Œæ ¹æ®ç»™å®šçš„æŸ¥è¯¢å›¾åƒï¼Œå®šä½ç›®æ ‡ç‰©ä½“åœ¨è§†é¢‘ä¸­çš„ä½ç½®ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç›¸æœºè¿åŠ¨ã€è§†è§’å˜åŒ–å’Œå¤–è§‚å·®å¼‚æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´å®šä½ç²¾åº¦ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æƒ…æ™¯è®°å¿†ï¼Œæ¨¡æ‹Ÿé¸Ÿç±»è®°å¿†å·©å›ºçš„è¿‡ç¨‹ï¼Œå°†å¤–è§‚ä¿¡æ¯å’Œå‡ ä½•ä¿¡æ¯åˆ†åˆ«å­˜å‚¨åœ¨ä¸åŒçš„è®°å¿†æ¨¡å—ä¸­ï¼Œå¹¶ååŒåˆ©ç”¨è¿™äº›ä¿¡æ¯è¿›è¡Œè§†è§‰æŸ¥è¯¢å®šä½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç›®æ ‡å¤–è§‚çš„å˜åŒ–ï¼Œå¹¶æé«˜å®šä½çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEAGLEæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„è®°å¿†æ¨¡å—ï¼šå¤–è§‚æ„ŸçŸ¥å…ƒå­¦ä¹ è®°å¿†ï¼ˆAMMï¼‰å’Œå‡ ä½•æ„ŸçŸ¥å®šä½è®°å¿†ï¼ˆGLMï¼‰ã€‚AMMè´Ÿè´£å­˜å‚¨ç›®æ ‡çš„å¤–è§‚ä¿¡æ¯ï¼Œå¹¶ç”¨äºæŒ‡å¯¼å›¾åƒåˆ†å‰²ï¼›GLMè´Ÿè´£å­˜å‚¨ç›®æ ‡çš„å‡ ä½•ä¿¡æ¯ï¼Œå¹¶ç”¨äºé©±åŠ¨ç›®æ ‡è·Ÿè¸ªã€‚æ¡†æ¶é¦–å…ˆåˆ©ç”¨AMMè¿›è¡Œå›¾åƒåˆ†å‰²ï¼Œç„¶ååˆ©ç”¨GLMè¿›è¡Œç›®æ ‡è·Ÿè¸ªï¼Œæœ€åå°†2Då®šä½ç»“æœä¸è§†è§‰å‡ ä½•æ¥åœ°çš„Transformerï¼ˆVGGTï¼‰é›†æˆï¼Œå®ç°2D-3Dçš„ç»Ÿä¸€ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æƒ…æ™¯å¤–è§‚å’Œå‡ ä½•æ„ŸçŸ¥çš„è®°å¿†æœºåˆ¶ï¼Œå°†å¤–è§‚ä¿¡æ¯å’Œå‡ ä½•ä¿¡æ¯åˆ†åˆ«å­˜å‚¨åœ¨ä¸åŒçš„è®°å¿†æ¨¡å—ä¸­ï¼Œå¹¶ååŒåˆ©ç”¨è¿™äº›ä¿¡æ¯è¿›è¡Œè§†è§‰æŸ¥è¯¢å®šä½ã€‚è¿™ç§æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç›®æ ‡å¤–è§‚çš„å˜åŒ–ï¼Œå¹¶æé«˜å®šä½çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†å°†2Då®šä½ç»“æœä¸è§†è§‰å‡ ä½•æ¥åœ°çš„Transformerï¼ˆVGGTï¼‰é›†æˆçš„æ–¹æ³•ï¼Œå®ç°äº†2D-3Dçš„ç»Ÿä¸€ã€‚

**å…³é”®è®¾è®¡**ï¼šAMMé‡‡ç”¨å…ƒå­¦ä¹ çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥å¿«é€Ÿé€‚åº”æ–°çš„ç›®æ ‡å¤–è§‚ã€‚GLMé‡‡ç”¨åŸºäºå…³é”®å¸§çš„è·Ÿè¸ªæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç›¸æœºè¿åŠ¨å’Œè§†è§’å˜åŒ–ã€‚VGGTåˆ©ç”¨è§†è§‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œå°†2Då®šä½ç»“æœåæŠ•å½±åˆ°3Dç©ºé—´ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åˆ†å‰²æŸå¤±ã€è·Ÿè¸ªæŸå¤±å’Œ3Då®šä½æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EAGLEåœ¨Ego4D-VQåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼ŒEAGLEåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œä¾‹å¦‚ï¼Œåœ¨R@1æŒ‡æ ‡ä¸Šï¼ŒEAGLEçš„æ€§èƒ½æ¯”ç¬¬äºŒå¥½çš„æ–¹æ³•æé«˜äº†5%ä»¥ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAGLEæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ç›®æ ‡å¤–è§‚çš„å˜åŒ–ï¼Œå¹¶æé«˜å®šä½çš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå…·èº«æ™ºèƒ½ã€VR/ARç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å®šä½ç›®æ ‡ç‰©ä½“ï¼Œä»è€Œå®ç°è‡ªä¸»å¯¼èˆªã€‚åœ¨VR/ARåº”ç”¨ä¸­ï¼Œç”¨æˆ·å¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®šä½ç›®æ ‡ç‰©ä½“ï¼Œä»è€Œå®ç°æ›´è‡ªç„¶çš„äº¤äº’ã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºæå‡æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Egocentric visual query localization is vital for embodied AI and VR/AR, yet remains challenging due to camera motion, viewpoint changes, and appearance variations. We present EAGLE, a novel framework that leverages episodic appearance- and geometry-aware memory to achieve unified 2D-3D visual query localization in egocentric vision. Inspired by avian memory consolidation, EAGLE synergistically integrates segmentation guided by an appearance-aware meta-learning memory (AMM), with tracking driven by a geometry-aware localization memory (GLM). This memory consolidation mechanism, through structured appearance and geometry memory banks, stores high-confidence retrieval samples, effectively supporting both long- and short-term modeling of target appearance variations. This enables precise contour delineation with robust spatial discrimination, leading to significantly improved retrieval accuracy. Furthermore, by integrating the VQL-2D output with a visual geometry grounded Transformer (VGGT), we achieve a efficient unification of 2D and 3D tasks, enabling rapid and accurate back-projection into 3D space. Our method achieves state-ofthe-art performance on the Ego4D-VQ benchmark.

