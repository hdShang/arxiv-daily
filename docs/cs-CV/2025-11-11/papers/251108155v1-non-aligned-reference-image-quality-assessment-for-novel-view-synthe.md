---
layout: default
title: Non-Aligned Reference Image Quality Assessment for Novel View Synthesis
---

# Non-Aligned Reference Image Quality Assessment for Novel View Synthesis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.08155" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.08155v1</a>
  <a href="https://arxiv.org/pdf/2511.08155.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08155v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.08155v1', 'Non-Aligned Reference Image Quality Assessment for Novel View Synthesis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Abhijay Ghildyal, Rajesh Sureddi, Nabajeet Barman, Saman Zadtootaghaj, Alan Bovik

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://stootaghaj.github.io/nova-project/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºNAR-IQAæ¡†æ¶ï¼Œç”¨äºè§£å†³æ–°è§†è§’åˆæˆä¸­éå¯¹é½å‚è€ƒå›¾åƒçš„è´¨é‡è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ–°è§†è§’åˆæˆ` `å›¾åƒè´¨é‡è¯„ä¼°` `éå¯¹é½å‚è€ƒ` `å¯¹æ¯”å­¦ä¹ ` `DINOv2` `LoRA` `æ—¶é—´æ„Ÿå…´è¶£åŒºåŸŸ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…¨å‚è€ƒIQAæ–¹æ³•åœ¨å‚è€ƒå›¾åƒæœªå¯¹é½æ—¶å¤±æ•ˆï¼Œæ— å‚è€ƒIQAæ–¹æ³•æ³›åŒ–æ€§ä¸è¶³ï¼Œéš¾ä»¥è¯„ä¼°æ–°è§†è§’åˆæˆå›¾åƒè´¨é‡ã€‚
2. æå‡ºNAR-IQAæ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’ŒLoRAå¢å¼ºçš„DINOv2åµŒå…¥ï¼Œå¹¶ç»“åˆç°æœ‰IQAæ–¹æ³•çš„ç›‘ç£ä¿¡æ¯ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¯¹é½å’Œéå¯¹é½å‚è€ƒå›¾åƒä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”ä¸äººç±»ä¸»è§‚è¯„ä»·é«˜åº¦ç›¸å…³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§éå¯¹é½å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°(NAR-IQA)æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæ–°è§†è§’åˆæˆ(NVS)å›¾åƒçš„è´¨é‡è¯„ä¼°ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³åœ¨ç¼ºä¹åƒç´ çº§å¯¹é½çš„ground truthå‚è€ƒå›¾åƒæ—¶ï¼Œä¼ ç»Ÿå…¨å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°(FR-IQA)æ–¹æ³•å¤±æ•ˆä»¥åŠæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°(NR-IQA)æ–¹æ³•æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«é’ˆå¯¹æ—¶é—´æ„Ÿå…´è¶£åŒºåŸŸ(TROI)çš„åˆæˆå¤±çœŸï¼Œç”¨äºè®­ç»ƒNAR-IQAæ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†LoRAå¢å¼ºçš„DINOv2åµŒå…¥ï¼Œå¹¶åˆ©ç”¨ç°æœ‰IQAæ–¹æ³•çš„ç›‘ç£ä¿¡æ¯è¿›è¡ŒæŒ‡å¯¼ã€‚æ¨¡å‹ä»…åœ¨åˆæˆå¤±çœŸæ•°æ®ä¸Šè®­ç»ƒï¼Œé¿å…è¿‡æ‹Ÿåˆç‰¹å®šçš„çœŸå®NVSæ ·æœ¬ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„FR-IQAã€NR-IQAå’ŒNAR-IQAæ–¹æ³•ï¼Œåœ¨å¯¹é½å’Œéå¯¹é½å‚è€ƒå›¾åƒä¸Šå‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œæ”¶é›†äº†åœ¨NVSä¸­è§‚å¯Ÿéå¯¹é½å‚è€ƒå›¾åƒæ—¶çš„äººç±»åå¥½æ•°æ®ï¼Œå‘ç°æ‰€æå‡ºçš„è´¨é‡é¢„æµ‹æ¨¡å‹ä¸æ”¶é›†çš„ä¸»è§‚è¯„åˆ†ä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–°è§†è§’åˆæˆ(NVS)å›¾åƒè´¨é‡è¯„ä¼°é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹åƒç´ å¯¹é½çš„å‚è€ƒå›¾åƒæ—¶ã€‚ä¼ ç»Ÿå…¨å‚è€ƒIQAæ–¹æ³•ä¾èµ–äºåƒç´ çº§åˆ«çš„å¯¹åº”å…³ç³»ï¼Œå½“å‚è€ƒå›¾åƒä¸åˆæˆå›¾åƒæœªå¯¹é½æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚è€Œæ— å‚è€ƒIQAæ–¹æ³•è™½ç„¶ä¸éœ€è¦å‚è€ƒå›¾åƒï¼Œä½†åœ¨NVSåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥å‡†ç¡®è¯„ä¼°åˆæˆå›¾åƒçš„è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éå¯¹é½çš„å‚è€ƒå›¾åƒï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œå­¦ä¹ å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚æ ¸å¿ƒåœ¨äºæå–å‚è€ƒå›¾åƒå’Œåˆæˆå›¾åƒçš„ç‰¹å¾ï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿå®¹å¿ä¸€å®šç¨‹åº¦ä¸å¯¹é½çš„è´¨é‡è¯„ä¼°æ¨¡å‹ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„NVSåœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ•°æ®é›†æ„å»ºï¼šæ„å»ºåŒ…å«åˆæˆå¤±çœŸçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¨¡æ‹ŸNVSä¸­å¯èƒ½å‡ºç°çš„å„ç§è´¨é‡é—®é¢˜ã€‚2) ç‰¹å¾æå–ï¼šä½¿ç”¨LoRAå¢å¼ºçš„DINOv2æ¨¡å‹æå–å‚è€ƒå›¾åƒå’Œåˆæˆå›¾åƒçš„ç‰¹å¾åµŒå…¥ã€‚3) å¯¹æ¯”å­¦ä¹ ï¼šåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå­¦ä¹ å›¾åƒè´¨é‡çš„è¡¨ç¤ºï¼Œä½¿å¾—é«˜è´¨é‡çš„åˆæˆå›¾åƒä¸å‚è€ƒå›¾åƒçš„ç‰¹å¾åµŒå…¥æ›´åŠ æ¥è¿‘ã€‚4) è´¨é‡é¢„æµ‹ï¼šåŸºäºå­¦ä¹ åˆ°çš„ç‰¹å¾è¡¨ç¤ºï¼Œé¢„æµ‹åˆæˆå›¾åƒçš„è´¨é‡å¾—åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹NVSåœºæ™¯çš„NAR-IQAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨éå¯¹é½çš„å‚è€ƒå›¾åƒè¿›è¡Œè´¨é‡è¯„ä¼°ã€‚ä¸ä¼ ç»Ÿçš„FR-IQAæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶ä¸éœ€è¦åƒç´ çº§åˆ«çš„å¯¹é½ï¼Œå› æ­¤æ›´åŠ é€‚ç”¨äºå®é™…çš„NVSåº”ç”¨åœºæ™¯ã€‚ä¸NR-IQAæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº†å‚è€ƒå›¾åƒçš„ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°åˆæˆå›¾åƒçš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨LoRAå¯¹DINOv2æ¨¡å‹è¿›è¡Œå¢å¼ºï¼Œæé«˜ç‰¹å¾æå–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚2) æ„å»ºåŒ…å«æ—¶é—´æ„Ÿå…´è¶£åŒºåŸŸ(TROI)å¤±çœŸçš„åˆæˆæ•°æ®é›†ï¼Œæ¨¡æ‹ŸNVSä¸­å¯èƒ½å‡ºç°çš„å„ç§è´¨é‡é—®é¢˜ã€‚3) åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå­¦ä¹ å›¾åƒè´¨é‡çš„è¡¨ç¤ºï¼Œå¹¶ç»“åˆç°æœ‰IQAæ–¹æ³•çš„ç›‘ç£ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚4) ä»…åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé¿å…è¿‡æ‹Ÿåˆç‰¹å®šçš„çœŸå®NVSæ ·æœ¬ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨çœŸå®NVSæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜è¯¥æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„FR-IQAã€NR-IQAå’ŒNAR-IQAæ–¹æ³•ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹é¢„æµ‹çš„è´¨é‡å¾—åˆ†ä¸äººç±»ä¸»è§‚è¯„ä»·å…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼ŒéªŒè¯äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®ä¸»é¡µæä¾›äº†æ•°æ®é›†å’Œä»£ç ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æ–°è§†è§’åˆæˆç³»ç»Ÿï¼Œä¾‹å¦‚è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€è‡ªç”±è§†ç‚¹è§†é¢‘ç­‰ã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°åˆæˆå›¾åƒçš„è´¨é‡ï¼Œå¯ä»¥ä¼˜åŒ–NVSç®—æ³•ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¯„ä¼°ä¸åŒNVSç®—æ³•çš„æ€§èƒ½ï¼Œä¸ºç®—æ³•é€‰æ‹©æä¾›ä¾æ®ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°å…¶ä»–å›¾åƒç”Ÿæˆä»»åŠ¡çš„è´¨é‡è¯„ä¼°ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/

