---
layout: default
title: "RAPTR: Radar-based 3D Pose Estimation using Transformer"
---

# RAPTR: Radar-based 3D Pose Estimation using Transformer

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.08387" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.08387v1</a>
  <a href="https://arxiv.org/pdf/2511.08387.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08387v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.08387v1', 'RAPTR: Radar-based 3D Pose Estimation using Transformer')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sorachi Kato, Ryoma Yataka, Pu Perry Wang, Pedro Miraldo, Takuya Fujihashi, Petros Boufounos

**åˆ†ç±»**: cs.CV, cs.AI, eess.SP

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**å¤‡æ³¨**: 26 pages, Accepted to NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/merlresearch/radar-pose-transformer)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RAPTRï¼šåˆ©ç”¨Transformerçš„é›·è¾¾3Däººä½“å§¿æ€ä¼°è®¡ï¼Œä½¿ç”¨å¼±ç›‘ç£å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction & Matching)**

**å…³é”®è¯**: `é›·è¾¾` `3Däººä½“å§¿æ€ä¼°è®¡` `Transformer` `å¼±ç›‘ç£å­¦ä¹ ` `å®¤å†…åœºæ™¯` `å¯å˜å½¢æ³¨æ„åŠ›` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºé›·è¾¾çš„3Däººä½“å§¿æ€ä¼°è®¡ä¾èµ–äºæ˜‚è´µçš„3Då…³é”®ç‚¹æ ‡æ³¨ï¼Œéš¾ä»¥åœ¨å¤æ‚å®¤å†…ç¯å¢ƒä¸­è·å–ã€‚
2. RAPTRé€šè¿‡ä¸¤é˜¶æ®µTransformerè§£ç å™¨ï¼Œåˆ©ç”¨æ˜“äºè·å–çš„3D BBoxå’Œ2Då…³é”®ç‚¹æ ‡ç­¾è¿›è¡Œå¼±ç›‘ç£å­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRAPTRåœ¨HIBERå’ŒMMVRæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…³èŠ‚ä½ç½®è¯¯å·®åˆ†åˆ«é™ä½34.3%å’Œ76.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé›·è¾¾çš„å®¤å†…3Däººä½“å§¿æ€ä¼°è®¡æ–¹æ³•RAPTRï¼Œè¯¥æ–¹æ³•ä½¿ç”¨Transformerï¼Œå¹¶åœ¨å¼±ç›‘ç£ä¸‹è¿›è¡Œè®­ç»ƒã€‚ä¸éœ€è¦ç²¾ç»†3Då…³é”®ç‚¹æ ‡ç­¾çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒRAPTRä»…ä½¿ç”¨3D BBoxå’Œ2Då…³é”®ç‚¹æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾æ›´å®¹æ˜“ä¸”æ›´å…·å¯æ‰©å±•æ€§ã€‚RAPTRçš„ç‰¹ç‚¹æ˜¯é‡‡ç”¨ä¸¤é˜¶æ®µå§¿æ€è§£ç å™¨æ¶æ„ï¼Œå¹¶ä½¿ç”¨ä¼ª3Då¯å˜å½¢æ³¨æ„åŠ›æ¥å¢å¼ºå¤šè§†è§’é›·è¾¾ç‰¹å¾çš„å§¿æ€/å…³èŠ‚æŸ¥è¯¢ã€‚å§¿æ€è§£ç å™¨ä½¿ç”¨3Dæ¨¡æ¿æŸå¤±ä¼°è®¡åˆå§‹3Då§¿æ€ï¼Œä»¥åˆ©ç”¨3D BBoxæ ‡ç­¾å¹¶å‡è½»æ·±åº¦æ¨¡ç³Šï¼›å…³èŠ‚è§£ç å™¨ä½¿ç”¨2Då…³é”®ç‚¹æ ‡ç­¾å’Œ3Dé‡åŠ›æŸå¤±æ¥ç»†åŒ–åˆå§‹å§¿æ€ã€‚åœ¨ä¸¤ä¸ªå®¤å†…é›·è¾¾æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒRAPTRä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨HIBERä¸Šå°†å…³èŠ‚ä½ç½®è¯¯å·®é™ä½äº†34.3ï¼…ï¼Œåœ¨MMVRä¸Šé™ä½äº†76.9ï¼…ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºé›·è¾¾çš„3Däººä½“å§¿æ€ä¼°è®¡æ–¹æ³•éœ€è¦å¤§é‡çš„ç²¾ç»†3Då…³é”®ç‚¹æ ‡æ³¨ï¼Œè¿™åœ¨å¤æ‚çš„å®¤å†…ç¯å¢ƒä¸­ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨é®æŒ¡æˆ–å¤šäººåœºæ™¯ä¸‹ï¼Œæ ‡æ³¨æˆæœ¬éå¸¸é«˜æ˜‚ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨æ›´å®¹æ˜“è·å–çš„å¼±ç›‘ç£ä¿¡æ¯ï¼ˆå¦‚3D BBoxå’Œ2Då…³é”®ç‚¹ï¼‰æ¥å®ç°ç²¾ç¡®çš„3Däººä½“å§¿æ€ä¼°è®¡æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRAPTRçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨Transformeræ¶æ„ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè§£ç å™¨ï¼Œç»“åˆ3D BBoxå’Œ2Då…³é”®ç‚¹æ ‡ç­¾è¿›è¡Œå¼±ç›‘ç£å­¦ä¹ ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨3D BBoxä¿¡æ¯ä¼°è®¡åˆå§‹å§¿æ€ï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨2Då…³é”®ç‚¹ä¿¡æ¯ç»†åŒ–å§¿æ€ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨åˆ©ç”¨ä¸åŒç±»å‹æ ‡ç­¾çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶å‡è½»æ·±åº¦æ¨¡ç³Šé—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRAPTRåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) é›·è¾¾ç‰¹å¾æå–æ¨¡å—ï¼ˆå…·ä½“å®ç°æœªçŸ¥ï¼‰ï¼›2) å§¿æ€è§£ç å™¨ï¼šåˆ©ç”¨3D BBoxæ ‡ç­¾å’Œ3Dæ¨¡æ¿æŸå¤±ä¼°è®¡åˆå§‹3Då§¿æ€ï¼›3) å…³èŠ‚è§£ç å™¨ï¼šåˆ©ç”¨2Då…³é”®ç‚¹æ ‡ç­¾å’Œ3Dé‡åŠ›æŸå¤±ç»†åŒ–åˆå§‹å§¿æ€ã€‚å§¿æ€è§£ç å™¨å’Œå…³èŠ‚è§£ç å™¨éƒ½åŸºäºTransformeræ¶æ„ï¼Œå¹¶ä½¿ç”¨ä¼ª3Då¯å˜å½¢æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šRAPTRçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¼±ç›‘ç£å­¦ä¹ æ¡†æ¶å’Œä¸¤é˜¶æ®µè§£ç å™¨æ¶æ„ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRAPTRä¸éœ€è¦æ˜‚è´µçš„3Då…³é”®ç‚¹æ ‡æ³¨ï¼Œè€Œæ˜¯åˆ©ç”¨æ›´å®¹æ˜“è·å–çš„3D BBoxå’Œ2Då…³é”®ç‚¹æ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œä¸¤é˜¶æ®µè§£ç å™¨èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨ä¸åŒç±»å‹æ ‡ç­¾çš„ä¿¡æ¯ï¼Œå¹¶å‡è½»æ·±åº¦æ¨¡ç³Šé—®é¢˜ã€‚ä¼ª3Då¯å˜å½¢æ³¨æ„åŠ›æœºåˆ¶ä¹Ÿæ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°èåˆå¤šè§†è§’é›·è¾¾ç‰¹å¾ã€‚

**å…³é”®è®¾è®¡**ï¼šRAPTRçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) 3Dæ¨¡æ¿æŸå¤±ï¼šç”¨äºåˆ©ç”¨3D BBoxæ ‡ç­¾ï¼Œé¼“åŠ±é¢„æµ‹çš„3Då§¿æ€ä¸BBoxå¯¹é½ï¼›2) 3Dé‡åŠ›æŸå¤±ï¼šç”¨äºåˆ©ç”¨2Då…³é”®ç‚¹æ ‡ç­¾ï¼Œé¼“åŠ±é¢„æµ‹çš„3Då§¿æ€ç¬¦åˆé‡åŠ›æ–¹å‘ï¼›3) ä¼ª3Då¯å˜å½¢æ³¨æ„åŠ›ï¼šç”¨äºèåˆå¤šè§†è§’é›·è¾¾ç‰¹å¾ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RAPTRåœ¨HIBERå’ŒMMVRä¸¤ä¸ªå®¤å†…é›·è¾¾æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒRAPTRæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨HIBERæ•°æ®é›†ä¸Šï¼ŒRAPTRå°†å…³èŠ‚ä½ç½®è¯¯å·®é™ä½äº†34.3ï¼…ï¼Œåœ¨MMVRæ•°æ®é›†ä¸Šï¼Œå…³èŠ‚ä½ç½®è¯¯å·®é™ä½äº†76.9ï¼…ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRAPTRåœ¨åŸºäºé›·è¾¾çš„3Däººä½“å§¿æ€ä¼°è®¡æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RAPTRåœ¨å®¤å†…ç¯å¢ƒä¸­çš„äººä½“å§¿æ€ä¼°è®¡å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¶å±…ã€è€äººçœ‹æŠ¤ã€äººæœºäº¤äº’ã€å®‰é˜²ç›‘æ§ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨ä½æˆæœ¬çš„é›·è¾¾ä¼ æ„Ÿå™¨ï¼Œåœ¨å…‰çº¿ä¸è¶³æˆ–å­˜åœ¨é®æŒ¡çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®åœ°ä¼°è®¡äººä½“å§¿æ€ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½ã€æ›´å®‰å…¨çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\%$ on HIBER and $76.9\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.

