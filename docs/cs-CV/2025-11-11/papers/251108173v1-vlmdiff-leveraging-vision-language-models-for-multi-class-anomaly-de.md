---
layout: default
title: VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion
---

# VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.08173" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.08173v1</a>
  <a href="https://arxiv.org/pdf/2511.08173.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.08173v1" onclick="toggleFavorite(this, '2511.08173v1', 'VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Samet Hicsonmez, Abd El Rahman Shabayek, Djamila Aouada

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-11

**å¤‡æ³¨**: WACV 2026

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/giddyyupp/VLMDiff)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLMDiffï¼šåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰å¼‚å¸¸æ£€æµ‹` `æ‰©æ•£æ¨¡å‹` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ— ç›‘ç£å­¦ä¹ ` `å¤šç±»åˆ«åˆ†ç±»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ‰©æ•£çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¾èµ–åˆæˆå™ªå£°ï¼Œæ³›åŒ–æ€§å—é™ï¼Œä¸”éœ€é’ˆå¯¹æ¯ä¸ªç±»åˆ«å•ç‹¬è®­ç»ƒæ¨¡å‹ï¼Œæ‰©å±•æ€§å·®ã€‚
2. VLMDiffåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹æå–å›¾åƒæè¿°ï¼Œä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ï¼Œå­¦ä¹ æ­£å¸¸å›¾åƒçš„é²æ£’ç‰¹å¾è¡¨ç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVLMDiffåœ¨Real-IADå’ŒCOCO-ADæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†åƒç´ çº§å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ‰©æ•£æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVLMDiffçš„æ–°å‹æ— ç›‘ç£å¤šç±»åˆ«è§†è§‰å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä»¥å¢å¼ºå¼‚å¸¸å®šä½å’Œæ£€æµ‹èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„VLMé€šè¿‡ç®€å•çš„æç¤ºæå–è¯¦ç»†çš„å›¾åƒæè¿°ï¼Œä½œä¸ºLDMè®­ç»ƒçš„é¢å¤–æ¡ä»¶ã€‚ä¸å½“å‰ä¾èµ–åˆæˆå™ªå£°ç”Ÿæˆçš„æ‰©æ•£æ–¹æ³•ä¸åŒï¼ŒVLMDiffåˆ©ç”¨VLMè·å–æ­£å¸¸å›¾åƒçš„æè¿°ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨æˆ–é¢å¤–è®­ç»ƒï¼Œä»è€Œé¿å…äº†æ³›åŒ–æ€§é—®é¢˜å’Œæ¯ä¸ªç±»åˆ«å•ç‹¬è®­ç»ƒçš„éœ€æ±‚ã€‚è¿™äº›æè¿°ç”¨äºè°ƒèŠ‚æ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ é²æ£’çš„æ­£å¸¸å›¾åƒç‰¹å¾è¡¨ç¤ºï¼Œä»¥è¿›è¡Œå¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œåœ¨Real-IADæ•°æ®é›†ä¸Šï¼Œåƒç´ çº§åŒºåŸŸé‡å åº¦ï¼ˆPROï¼‰æŒ‡æ ‡æå‡é«˜è¾¾25ä¸ªç‚¹ï¼Œåœ¨COCO-ADæ•°æ®é›†ä¸Šæå‡8ä¸ªç‚¹ï¼Œä¼˜äºæœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šç±»åˆ«çœŸå®ä¸–ç•Œå›¾åƒä¸­çš„è§†è§‰å¼‚å¸¸æ£€æµ‹é—®é¢˜ã€‚ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–äºåˆæˆå™ªå£°çš„ç”Ÿæˆï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹æ¯ä¸ªç±»åˆ«å•ç‹¬è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨å¤„ç†å¤šç±»åˆ«é—®é¢˜æ—¶ç¼ºä¹å¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLMDiffçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥è·å–æ­£å¸¸å›¾åƒçš„æ–‡æœ¬æè¿°ï¼Œå¹¶å°†è¿™äº›æè¿°ä½œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„é¢å¤–æ¡ä»¶ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´é²æ£’çš„æ­£å¸¸å›¾åƒç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLMDiffçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„VLMï¼ˆä¾‹å¦‚CLIPï¼‰æå–è¾“å…¥å›¾åƒçš„æ–‡æœ¬æè¿°ï¼›2) å°†è¿™äº›æ–‡æœ¬æè¿°ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°LDMä¸­ï¼›3) LDMå­¦ä¹ æ­£å¸¸å›¾åƒçš„åˆ†å¸ƒï¼›4) åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡æ¯”è¾ƒè¾“å…¥å›¾åƒä¸LDMé‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚æ¥æ£€æµ‹å¼‚å¸¸ã€‚

**å…³é”®åˆ›æ–°**ï¼šVLMDiffçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨VLMæ¥è·å–å›¾åƒçš„æ–‡æœ¬æè¿°ï¼Œä»è€Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨æˆ–é¢å¤–è®­ç»ƒçš„éœ€è¦ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹é—®é¢˜ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼ŒVLMDiffä¸éœ€è¦ç”Ÿæˆåˆæˆå™ªå£°ï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨çœŸå®å›¾åƒçš„æ–‡æœ¬æè¿°æ¥æŒ‡å¯¼æ¨¡å‹çš„è®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šVLMDiffçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ä½œä¸ºVLMï¼Œä»¥æå–é«˜è´¨é‡çš„å›¾åƒæè¿°ï¼›2) å°†æ–‡æœ¬æè¿°åµŒå…¥åˆ°LDMçš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä½œä¸ºé¢å¤–çš„æ¡ä»¶ï¼›3) ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä»¥è¡¡é‡è¾“å…¥å›¾åƒä¸LDMé‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VLMDiffåœ¨Real-IADå’ŒCOCO-ADæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨Real-IADæ•°æ®é›†ä¸Šï¼Œåƒç´ çº§åŒºåŸŸé‡å åº¦ï¼ˆPROï¼‰æŒ‡æ ‡æå‡é«˜è¾¾25ä¸ªç‚¹ï¼Œåœ¨COCO-ADæ•°æ®é›†ä¸Šæå‡8ä¸ªç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVLMDiffä¼˜äºç°æœ‰çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLMDiffå¯åº”ç”¨äºå·¥ä¸šè´¨æ£€ã€åŒ»ç–—å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚åœ¨å·¥ä¸šè´¨æ£€ä¸­ï¼Œå¯ç”¨äºæ£€æµ‹äº§å“è¡¨é¢çš„ç¼ºé™·ï¼›åœ¨åŒ»ç–—å½±åƒåˆ†æä¸­ï¼Œå¯ç”¨äºè¾…åŠ©åŒ»ç”Ÿè¯Šæ–­ç–¾ç—…ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ç”¨äºæ£€æµ‹é“è·¯ä¸Šçš„å¼‚å¸¸ç‰©ä½“ã€‚è¯¥ç ”ç©¶å…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ï¼Œæœ‰æœ›æé«˜ç›¸å…³é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ°´å¹³å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.

