---
layout: default
title: FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization
---

# FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization

**arXiv**: [2512.04952v2](https://arxiv.org/abs/2512.04952) | [PDF](https://arxiv.org/pdf/2512.04952.pdf)

**ä½œè€…**: Yicheng Liu, Shiduo Zhang, Zibin Dong, Baijun Ye, Tianyuan Yuan, Xiaopeng Yu, Linqi Yin, Chenhao Lu, Junhao Shi, Luca Jiang-Tao Yu, Liangtao Zheng, Tao Jiang, Jingjing Gong, Xipeng Qiu, Hang Zhao

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04 (æ›´æ–°: 2025-12-08)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FASTerï¼šé€šè¿‡ç¥žç»åŠ¨ä½œæ ‡è®°åŒ–å®žçŽ°é«˜æ•ˆçš„è‡ªå›žå½’è§†è§‰-è¯­è¨€-åŠ¨ä½œå»ºæ¨¡**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `è‡ªå›žå½’æ¨¡åž‹` `åŠ¨ä½œæ ‡è®°åŒ–` `å‘é‡é‡åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çš„è‡ªå›žå½’VLAæ¨¡åž‹åœ¨åŠ¨ä½œæ ‡è®°åŒ–è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åœ¨é‡å»ºè´¨é‡å’ŒæŽ¨ç†æ•ˆçŽ‡ä¹‹é—´åšå‡ºæƒè¡¡ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. FASTeræ¡†æž¶é€šè¿‡å¯å­¦ä¹ çš„æ ‡è®°å™¨å’Œè‡ªå›žå½’ç­–ç•¥çš„é›†æˆï¼Œå®žçŽ°äº†é«˜æ•ˆä¸”å¯æ³›åŒ–çš„æœºå™¨äººå­¦ä¹ ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒFASTeråœ¨é‡å»ºè´¨é‡ã€ä»¤ç‰Œåˆ©ç”¨çŽ‡ã€æ³›åŒ–èƒ½åŠ›ä»¥åŠæŽ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½æ–¹é¢å‡ä¼˜äºŽçŽ°æœ‰VLAæ¨¡åž‹ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªå›žå½’è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹æœ€è¿‘åœ¨æœºå™¨äººæ“ä½œæ–¹é¢è¡¨çŽ°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶æ ¸å¿ƒçš„åŠ¨ä½œæ ‡è®°åŒ–è¿‡ç¨‹é€šå¸¸éœ€è¦åœ¨é‡å»ºä¿çœŸåº¦å’ŒæŽ¨ç†æ•ˆçŽ‡ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†FASTerï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æž¶ï¼Œç”¨äºŽé«˜æ•ˆä¸”å¯æ³›åŒ–çš„æœºå™¨äººå­¦ä¹ ï¼Œå®ƒé›†æˆäº†å¯å­¦ä¹ çš„æ ‡è®°å™¨å’ŒåŸºäºŽå®ƒçš„è‡ªå›žå½’ç­–ç•¥ã€‚FASTerVQå°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒï¼Œæ•èŽ·å…¨å±€æ—¶ç©ºä¾èµ–å…³ç³»ï¼ŒåŒæ—¶ä¿æŒé«˜åŽ‹ç¼©çŽ‡ã€‚FASTerVLAåœ¨æ­¤æ ‡è®°å™¨çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å—çŠ¶è‡ªå›žå½’è§£ç å’Œè½»é‡çº§åŠ¨ä½œä¸“å®¶ï¼Œå®žçŽ°äº†æ›´å¿«çš„æŽ¨ç†é€Ÿåº¦å’Œæ›´é«˜çš„ä»»åŠ¡æ€§èƒ½ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®žä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒFASTerVQæä¾›äº†å“è¶Šçš„é‡å»ºè´¨é‡ã€é«˜ä»¤ç‰Œåˆ©ç”¨çŽ‡ä»¥åŠå¼ºå¤§çš„è·¨ä»»åŠ¡å’Œè·¨çŽ¯å¢ƒæ³›åŒ–èƒ½åŠ›ï¼Œè€ŒFASTerVLAè¿›ä¸€æ­¥æé«˜äº†æ•´ä½“èƒ½åŠ›ï¼Œåœ¨æŽ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½æ–¹é¢å‡è¶…è¿‡äº†å…ˆå‰çš„æœ€å…ˆè¿›çš„VLAæ¨¡åž‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰çš„è‡ªå›žå½’è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡åž‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ ¸å¿ƒçš„åŠ¨ä½œæ ‡è®°åŒ–è¿‡ç¨‹é¢ä¸´ç€é‡å»ºä¿çœŸåº¦å’ŒæŽ¨ç†æ•ˆçŽ‡ä¹‹é—´çš„å›ºæœ‰çŸ›ç›¾ã€‚é«˜ä¿çœŸåº¦çš„æ ‡è®°åŒ–æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´å¤§é‡çš„åŠ¨ä½œtokenï¼Œä»Žè€Œé™ä½ŽæŽ¨ç†é€Ÿåº¦ã€‚åä¹‹ï¼Œä¸ºäº†æé«˜æ•ˆçŽ‡è€Œç‰ºç‰²é‡å»ºè´¨é‡åˆ™ä¼šå½±å“ä»»åŠ¡æ€§èƒ½ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§æ—¢èƒ½ä¿è¯åŠ¨ä½œé‡å»ºè´¨é‡ï¼Œåˆèƒ½å®žçŽ°é«˜æ•ˆæŽ¨ç†çš„VLAæ¨¡åž‹æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFASTerçš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„åŠ¨ä½œæ ‡è®°å™¨ï¼ˆFASTerVQï¼‰ï¼Œå°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒï¼Œä»Žè€Œæ•èŽ·å…¨å±€æ—¶ç©ºä¾èµ–å…³ç³»å¹¶å®žçŽ°é«˜åŽ‹ç¼©çŽ‡ã€‚ç„¶åŽï¼ŒåŸºäºŽæ­¤æ ‡è®°å™¨æž„å»ºä¸€ä¸ªè‡ªå›žå½’ç­–ç•¥ï¼ˆFASTerVLAï¼‰ï¼Œé€šè¿‡å—çŠ¶è‡ªå›žå½’è§£ç å’Œè½»é‡çº§åŠ¨ä½œä¸“å®¶ï¼Œè¿›ä¸€æ­¥æé«˜æŽ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨è§£è€¦åŠ¨ä½œè¡¨ç¤ºå­¦ä¹ å’Œç­–ç•¥å­¦ä¹ ï¼Œä»Žè€Œå®žçŽ°æ›´é«˜æ•ˆå’Œå¯æ³›åŒ–çš„æœºå™¨äººå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šFASTeræ¡†æž¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šFASTerVQå’ŒFASTerVLAã€‚FASTerVQæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡é‡åŒ–å™¨ï¼Œè´Ÿè´£å°†è¿žç»­çš„åŠ¨ä½œç©ºé—´ç¦»æ•£åŒ–ä¸ºç¦»æ•£çš„åŠ¨ä½œtokenã€‚å®ƒå°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒï¼Œåˆ©ç”¨å·ç§¯ç¥žç»ç½‘ç»œæå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡å‘é‡é‡åŒ–å±‚å°†ç‰¹å¾æ˜ å°„åˆ°ç¦»æ•£çš„tokenç©ºé—´ã€‚FASTerVLAåˆ™æ˜¯ä¸€ä¸ªåŸºäºŽTransformerçš„è‡ªå›žå½’æ¨¡åž‹ï¼Œå®ƒä»¥FASTerVQç”Ÿæˆçš„åŠ¨ä½œtokenåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹æœªæ¥çš„åŠ¨ä½œã€‚å®ƒé‡‡ç”¨å—çŠ¶è‡ªå›žå½’è§£ç ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„åŠ¨ä½œä¸“å®¶ï¼Œä»¥æé«˜æŽ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šFASTerçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶æå‡ºçš„ç¥žç»åŠ¨ä½œæ ‡è®°åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒï¼Œä»Žè€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•èŽ·å…¨å±€æ—¶ç©ºä¾èµ–å…³ç³»ï¼Œå¹¶å®žçŽ°é«˜åŽ‹ç¼©çŽ‡ã€‚ä¸Žä¼ ç»Ÿçš„å‘é‡é‡åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒFASTerVQèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™åŠ¨ä½œçš„æ—¶ç©ºä¿¡æ¯ï¼Œä»Žè€Œæé«˜é‡å»ºè´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒFASTerVLAé‡‡ç”¨å—çŠ¶è‡ªå›žå½’è§£ç å’Œè½»é‡çº§åŠ¨ä½œä¸“å®¶ï¼Œè¿›ä¸€æ­¥æé«˜äº†æŽ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šFASTerVQä½¿ç”¨å·ç§¯ç¥žç»ç½‘ç»œä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå‘é‡é‡åŒ–å±‚é‡‡ç”¨Gumbel-SoftmaxæŠ€å·§è¿›è¡Œè®­ç»ƒã€‚FASTerVLAä½¿ç”¨Transformerä½œä¸ºè‡ªå›žå½’æ¨¡åž‹ï¼Œå—å¤§å°è®¾ç½®ä¸ºå›ºå®šå€¼ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±å’Œé‡åŒ–æŸå¤±ï¼Œç”¨äºŽä¼˜åŒ–FASTerVQå’ŒFASTerVLAã€‚åŠ¨ä½œä¸“å®¶æ˜¯ä¸€ä¸ªå°åž‹ç¥žç»ç½‘ç»œï¼Œç”¨äºŽé¢„æµ‹åŠ¨ä½œçš„å‡å€¼å’Œæ–¹å·®ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒFASTerVQåœ¨åŠ¨ä½œé‡å»ºè´¨é‡æ–¹é¢ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶å…·æœ‰æ›´é«˜çš„ä»¤ç‰Œåˆ©ç”¨çŽ‡å’Œæ›´å¼ºçš„è·¨ä»»åŠ¡å’Œè·¨çŽ¯å¢ƒæ³›åŒ–èƒ½åŠ›ã€‚FASTerVLAåœ¨æŽ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½æ–¹é¢å‡è¶…è¿‡äº†å…ˆå‰çš„æœ€å…ˆè¿›çš„VLAæ¨¡åž‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒFASTerVLAçš„æŽ¨ç†é€Ÿåº¦æé«˜äº†2å€ï¼Œä»»åŠ¡æˆåŠŸçŽ‡æé«˜äº†10%ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

FASTeræ¡†æž¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œå¦‚ç‰©ä½“æŠ“å–ã€è£…é…ã€å¯¼èˆªç­‰ã€‚è¯¥ç ”ç©¶æˆæžœæœ‰åŠ©äºŽæé«˜æœºå™¨äººæ“ä½œçš„æ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„çŽ¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºŽå…¶ä»–éœ€è¦é«˜æ•ˆåŠ¨ä½œè¡¨ç¤ºå­¦ä¹ çš„é¢†åŸŸï¼Œå¦‚æ¸¸æˆAIã€è™šæ‹ŸçŽ°å®žç­‰ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.

