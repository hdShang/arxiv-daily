---
layout: default
title: ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning
---

# ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning

**arXiv**: [2512.13095v1](https://arxiv.org/abs/2512.13095) | [PDF](https://arxiv.org/pdf/2512.13095.pdf)

**ä½œè€…**: Feng Zhang, Zezhong Tan, Xinhong Ma, Ziqiang Dong, Xi Leng, Jianfei Zhao, Xin Sun, Yang Yang

**åˆ†ç±»**: cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ADHintï¼šåˆ©ç”¨éš¾åº¦å…ˆéªŒçš„è‡ªé€‚åº”æç¤ºå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æŽ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æç¤ºå­¦ä¹ ` `éš¾åº¦å…ˆéªŒ` `è‡ªé€‚åº”æç¤º` `ä¼˜åŠ¿ä¼°è®¡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽæç¤ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¿½ç•¥äº†æ ·æœ¬éš¾åº¦ï¼Œå¯¼è‡´å­¦ä¹ ä¸ç¨³å®šå’Œè¿‡åº¦æ¨¡ä»¿ç¦»ç­–ç•¥æ•°æ®ã€‚
2. ADHintå°†æ ·æœ¬éš¾åº¦çº³å…¥æç¤ºæ¯”ä¾‹è°ƒåº¦å’Œä¼˜åŠ¿ä¼°è®¡ï¼Œå¹³è¡¡æŽ¢ç´¢ä¸Žæ¨¡ä»¿ï¼Œæå‡å­¦ä¹ æ•ˆæžœã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒADHintåœ¨å¤šç§æ¨¡æ€ã€æ¨¡åž‹è§„æ¨¡å’Œé¢†åŸŸä¸­ï¼Œæ˜¾è‘—æå‡äº†æŽ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†ç»“åˆç›‘ç£å¾®è°ƒ(SFT)å’Œå¼ºåŒ–å­¦ä¹ (RL)çš„ä¼˜åŠ¿ï¼ŒçŽ°æœ‰æ–¹æ³•å°†â€œæç¤ºâ€ï¼ˆå®Œæ•´æŽ¨ç†è½¨è¿¹çš„å‰ç¼€ç‰‡æ®µï¼‰é›†æˆåˆ°åŽè®­ç»ƒä¸­ï¼Œæ—¨åœ¨å®žçŽ°å¼ºå¤§çš„çŸ¥è¯†æ‰©å±•å’ŒæŽ¨ç†æ³›åŒ–ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰çš„åŸºäºŽæç¤ºçš„RLæ–¹æ³•é€šå¸¸å¿½ç•¥äº†åœ¨è°ƒåº¦æç¤ºæ¯”ä¾‹å’Œä¼°è®¡ç›¸å¯¹ä¼˜åŠ¿æ—¶çš„éš¾åº¦ï¼Œå¯¼è‡´ä¸ç¨³å®šçš„å­¦ä¹ å’Œè¿‡åº¦æ¨¡ä»¿ç¦»ç­–ç•¥æç¤ºã€‚æœ¬æ–‡æå‡ºäº†ADHintï¼Œå®ƒå°†éš¾åº¦ä½œä¸ºæç¤ºæ¯”ä¾‹è°ƒåº¦å’Œç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡çš„å…³é”®å› ç´ ï¼Œä»¥åœ¨æŽ¢ç´¢å’Œæ¨¡ä»¿ä¹‹é—´å®žçŽ°æ›´å¥½çš„æƒè¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰æ ·æœ¬éš¾åº¦å…ˆéªŒçš„è‡ªé€‚åº”æç¤ºï¼Œå®ƒè¯„ä¼°ç­–ç•¥æ¨¡åž‹ä¸‹æ¯ä¸ªæ ·æœ¬çš„éš¾åº¦ï¼Œå¹¶ç›¸åº”åœ°è°ƒåº¦é€‚å½“çš„æç¤ºæ¯”ä¾‹æ¥æŒ‡å¯¼å…¶rolloutã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†åŸºäºŽä¸€è‡´æ€§çš„æ¢¯åº¦è°ƒåˆ¶å’Œæç¤ºä¿æŒçš„é€‰æ‹©æ€§æŽ©ç ï¼Œä»¥è°ƒåˆ¶æç¤ºå†…çš„tokençº§åˆ«æ¢¯åº¦ï¼Œé˜²æ­¢æœ‰åå·®å’Œç ´åæ€§çš„æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰Rolloutéš¾åº¦åŽéªŒçš„ä¼˜åŠ¿ä¼°è®¡ï¼Œå®ƒåˆ©ç”¨æœ‰æç¤ºå’Œæ— æç¤ºçš„rolloutçš„ç›¸å¯¹éš¾åº¦æ¥ä¼°è®¡å®ƒä»¬å„è‡ªçš„ä¼˜åŠ¿ï¼Œä»Žè€Œå®žçŽ°æ›´å¹³è¡¡çš„æ›´æ–°ã€‚åœ¨ä¸åŒçš„æ¨¡æ€ã€æ¨¡åž‹è§„æ¨¡å’Œé¢†åŸŸä¸­è¿›è¡Œçš„å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒADHintæä¾›äº†å“è¶Šçš„æŽ¨ç†èƒ½åŠ›å’Œåˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨pass@1å’Œavg@8æ–¹é¢å§‹ç»ˆä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†åœ¨è®ºæ–‡è¢«æŽ¥å—åŽå…¬å¼€å‘å¸ƒã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽæç¤ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åˆ©ç”¨æç¤ºä¿¡æ¯è¿›è¡Œç­–ç•¥ä¼˜åŒ–æ—¶ï¼Œå¿½ç•¥äº†ä¸åŒæ ·æœ¬çš„éš¾åº¦å·®å¼‚ã€‚è¿™å¯¼è‡´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯æç¤ºæ¯”ä¾‹çš„åˆ†é…ä¸åˆç†ï¼Œç®€å•æ ·æœ¬å¯èƒ½è¢«è¿‡åº¦æç¤ºï¼Œè€Œå›°éš¾æ ·æœ¬åˆ™ç¼ºä¹è¶³å¤Ÿçš„æŒ‡å¯¼ï¼›äºŒæ˜¯ä¼˜åŠ¿å‡½æ•°ä¼°è®¡ä¸å‡†ç¡®ï¼Œæ— æ³•åŒºåˆ†æç¤ºå¸¦æ¥çš„çœŸå®žæ”¶ç›Šå’Œæ ·æœ¬æœ¬èº«çš„éš¾åº¦ï¼Œä»Žè€Œå¯¼è‡´ç­–ç•¥å­¦ä¹ ä¸ç¨³å®šï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šADHintçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ ·æœ¬éš¾åº¦ä½œä¸ºå…³é”®å› ç´ ï¼Œèžå…¥åˆ°æç¤ºæ¯”ä¾‹çš„è°ƒåº¦å’Œä¼˜åŠ¿å‡½æ•°ä¼°è®¡ä¸­ã€‚é€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´æç¤ºæ¯”ä¾‹ï¼Œä¸ºä¸åŒéš¾åº¦çš„æ ·æœ¬æä¾›åˆé€‚çš„æŒ‡å¯¼ï¼ŒåŒæ—¶åˆ©ç”¨rolloutçš„éš¾åº¦åŽéªŒæ¥æ›´å‡†ç¡®åœ°ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ï¼Œä»Žè€Œå®žçŽ°æ›´æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šADHintä¸»è¦åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š1) **Adaptive Hint with Sample Difficulty Prior (AH-SDP)**ï¼šæ ¹æ®ç­–ç•¥æ¨¡åž‹è¯„ä¼°æ ·æœ¬éš¾åº¦ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´æç¤ºæ¯”ä¾‹ã€‚2) **Consistency-based Gradient Modulation and Selective Masking for Hint Preservation (CGM-SM)**ï¼šè°ƒåˆ¶æç¤ºå†…éƒ¨çš„æ¢¯åº¦ï¼Œå¹¶è¿›è¡Œé€‰æ‹©æ€§æŽ©ç ï¼Œä»¥é˜²æ­¢æç¤ºä¿¡æ¯è¢«ç ´åã€‚3) **Advantage Estimation with Rollout Difficulty Posterior (AE-RDP)**ï¼šåˆ©ç”¨æœ‰æç¤ºå’Œæ— æç¤ºrolloutçš„éš¾åº¦åŽéªŒï¼Œæ›´å‡†ç¡®åœ°ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨AH-SDPç¡®å®šæç¤ºæ¯”ä¾‹ï¼Œç„¶åŽè¿›è¡Œrolloutï¼ŒæŽ¥ç€åˆ©ç”¨CGM-SMä¿æŠ¤æç¤ºä¿¡æ¯ï¼Œæœ€åŽåˆ©ç”¨AE-RDPä¼°è®¡ä¼˜åŠ¿å‡½æ•°å¹¶æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šADHintçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†æ ·æœ¬éš¾åº¦æ˜¾å¼åœ°å»ºæ¨¡åˆ°æç¤ºå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ã€‚AH-SDPé€šè¿‡æ ·æœ¬éš¾åº¦å…ˆéªŒè‡ªé€‚åº”åœ°è°ƒæ•´æç¤ºæ¯”ä¾‹ï¼ŒCGM-SMé€šè¿‡æ¢¯åº¦è°ƒåˆ¶å’Œé€‰æ‹©æ€§æŽ©ç ä¿æŠ¤æç¤ºä¿¡æ¯ï¼ŒAE-RDPé€šè¿‡rolloutéš¾åº¦åŽéªŒæ›´å‡†ç¡®åœ°ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ã€‚è¿™äº›åˆ›æ–°å…±åŒä½œç”¨ï¼Œä½¿å¾—ADHintèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æç¤ºä¿¡æ¯ï¼Œæå‡ç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šAH-SDPä¸­ï¼Œæ ·æœ¬éš¾åº¦é€šè¿‡ç­–ç•¥æ¨¡åž‹çš„ç½®ä¿¡åº¦æˆ–é¢„æµ‹æ¦‚çŽ‡æ¥è¡¡é‡ï¼Œæç¤ºæ¯”ä¾‹æ ¹æ®æ ·æœ¬éš¾åº¦è¿›è¡Œè°ƒæ•´ï¼Œéš¾åº¦é«˜çš„æ ·æœ¬åˆ†é…æ›´é«˜çš„æç¤ºæ¯”ä¾‹ã€‚CGM-SMé€šè¿‡è®¡ç®—æç¤ºå†…éƒ¨tokençš„ä¸€è‡´æ€§æ¥è°ƒåˆ¶æ¢¯åº¦ï¼Œå¹¶å¯¹ä¸ä¸€è‡´çš„tokenè¿›è¡ŒæŽ©ç ï¼Œä»¥é˜²æ­¢æç¤ºä¿¡æ¯è¢«ç ´åã€‚AE-RDPåˆ©ç”¨æœ‰æç¤ºå’Œæ— æç¤ºrolloutçš„å¥–åŠ±å’Œéš¾åº¦ä¿¡æ¯ï¼Œè®¡ç®—rolloutéš¾åº¦åŽéªŒï¼Œå¹¶å°†å…¶ç”¨äºŽä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡ä¸­ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒADHintåœ¨å¤šä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŽ¨ç†ä»»åŠ¡ä¸­ï¼ŒADHintåœ¨pass@1å’Œavg@8æŒ‡æ ‡ä¸Šå‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒADHintåœ¨æŸäº›ä»»åŠ¡ä¸Šçš„pass@1æŒ‡æ ‡æå‡äº†è¶…è¿‡5ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨æ˜Žå…¶å…·æœ‰æ›´å¼ºçš„æŽ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚è¿™äº›ç»“æžœè¯æ˜Žäº†ADHintçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

ADHintå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºŽå„ç§éœ€è¦åˆ©ç”¨æç¤ºä¿¡æ¯è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å¯¹è¯ç”Ÿæˆã€ä»£ç ç”Ÿæˆã€æœºå™¨äººæŽ§åˆ¶ç­‰ã€‚é€šè¿‡å¼•å…¥éš¾åº¦å…ˆéªŒï¼ŒADHintèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æç¤ºä¿¡æ¯ï¼Œæå‡æ¨¡åž‹çš„æŽ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ï¼Œä»Žè€Œåœ¨å®žé™…åº”ç”¨ä¸­å–å¾—æ›´å¥½çš„æ•ˆæžœã€‚æ­¤å¤–ï¼ŒADHintè¿˜å¯ä»¥åº”ç”¨äºŽæ•™è‚²é¢†åŸŸï¼Œä¾‹å¦‚ä¸ªæ€§åŒ–è¾…å¯¼ç³»ç»Ÿï¼Œæ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ éš¾åº¦è‡ªé€‚åº”åœ°æä¾›æç¤ºä¿¡æ¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

