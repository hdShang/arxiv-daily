---
layout: default
title: Training Multi-Image Vision Agents via End2End Reinforcement Learning
---

# Training Multi-Image Vision Agents via End2End Reinforcement Learning

**arXiv**: [2512.08980v2](https://arxiv.org/abs/2512.08980) | [PDF](https://arxiv.org/pdf/2512.08980.pdf)

**ä½œè€…**: Chengqi Dong, Chuhuai Yue, Hang He, Rongge Mao, Fenghe Tang, S Kevin Zhou, Zekun Xu, Xiaohan Wang, Jiajun Chai, Wei Lin, Guojun Yin

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-05 (æ›´æ–°: 2025-12-16)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIMAgentï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤šå›¾è§†è§‰Agentï¼Œè§£å†³å¤æ‚å¤šå›¾QAä»»åŠ¡ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šå›¾QA` `è§†è§‰Agent` `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·ä½¿ç”¨` `è§†è§‰è¯­è¨€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽVLMçš„Agentåœ¨å·¥å…·ä½¿ç”¨æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¤§å¤šä»…é™äºŽå•å¼ å›¾åƒè¾“å…¥ï¼Œéš¾ä»¥åº”å¯¹çœŸå®žä¸–ç•Œçš„å¤šå›¾QAä»»åŠ¡ã€‚
2. IMAgenté€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¹¶å¼•å…¥å¤šAgentç³»ç»Ÿç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ï¼Œä»Žè€Œæå‡VLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒIMAgentåœ¨å¤šå›¾QAä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨å•å›¾åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç«žäº‰åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºIMAgentï¼Œä¸€ä¸ªå¼€æºçš„è§†è§‰Agentï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä¸“é—¨ç”¨äºŽå¤„ç†å¤æ‚çš„å¤šå›¾ä»»åŠ¡ã€‚åˆ©ç”¨å¤šAgentç³»ç»Ÿï¼Œç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§å’Œè§†è§‰ä¸°å¯Œæ€§çš„å¤šå›¾QAå¯¹ï¼Œå……åˆ†æ¿€æ´»åŸºç¡€VLMçš„å·¥å…·ä½¿ç”¨æ½œåŠ›ã€‚é€šè¿‡äººå·¥éªŒè¯ï¼Œæž„å»ºäº†åŒ…å«1ä¸‡ä¸ªæ ·æœ¬çš„MIFG-QAæ•°æ®é›†ï¼Œç”¨äºŽè®­ç»ƒå’Œè¯„ä¼°ã€‚é’ˆå¯¹VLMåœ¨æŽ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å¿½ç•¥è§†è§‰è¾“å…¥çš„é—®é¢˜ï¼Œå¼€å‘äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿåœ¨æŽ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨é‡æ–°åˆ†é…å¯¹å›¾åƒå†…å®¹çš„æ³¨æ„åŠ›ã€‚å—ç›ŠäºŽç²¾å¿ƒè®¾è®¡çš„åŠ¨ä½œè½¨è¿¹ä¸¤çº§æŽ©ç ç­–ç•¥ï¼ŒIMAgenté€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®žçŽ°äº†ç¨³å®šçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œæ— éœ€æ˜‚è´µçš„ç›‘ç£å¾®è°ƒæ•°æ®ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒIMAgentåœ¨çŽ°æœ‰å•å›¾åŸºå‡†ä¸Šä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶åœ¨æå‡ºçš„å¤šå›¾æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåˆ†æžä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚ä»£ç å’Œæ•°æ®å³å°†å‘å¸ƒã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰åŸºäºŽè§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰çš„Agentåœ¨å¤„ç†å¤šå›¾QAä»»åŠ¡æ—¶å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦åŽŸå› æ˜¯å®ƒä»¬é€šå¸¸åªæŽ¥å—å•å¼ å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨éœ€è¦ç»¼åˆå¤šå¼ å›¾åƒä¿¡æ¯æ‰èƒ½å®Œæˆçš„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚çŽ°æœ‰çš„å¼€æºæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨VLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›æ¥è§£å†³å¤æ‚çš„å¤šå›¾æŽ¨ç†é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šå¼ å›¾åƒä¿¡æ¯çš„è§†è§‰Agentã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªå¤šAgentç³»ç»Ÿæ¥ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ï¼Œä»Žè€Œè®­ç»ƒVLMçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³VLMåœ¨æŽ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å¿½ç•¥è§†è§‰è¾“å…¥çš„é—®é¢˜ï¼Œå¼•å…¥äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œä¿ƒä½¿æ¨¡åž‹æ›´åŠ å…³æ³¨å›¾åƒå†…å®¹ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šIMAgentçš„æ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šAgentæ•°æ®ç”Ÿæˆå™¨ï¼šè´Ÿè´£ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå›¾QAå¯¹ï¼Œç”¨äºŽè®­ç»ƒAgentã€‚2) åŸºäºŽVLMçš„Agentï¼šä½œä¸ºæ ¸å¿ƒæŽ¨ç†æ¨¡å—ï¼Œè´Ÿè´£æŽ¥æ”¶å›¾åƒå’Œé—®é¢˜ï¼Œå¹¶è¾“å‡ºç­”æ¡ˆã€‚3) è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼šç”¨äºŽåœ¨æŽ¨ç†è¿‡ç¨‹ä¸­é‡æ–°åˆ†é…å¯¹å›¾åƒå†…å®¹çš„æ³¨æ„åŠ›ã€‚4) å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ï¼šä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°å’ŒåŠ¨ä½œè½¨è¿¹æŽ©ç ç­–ç•¥ï¼Œè®­ç»ƒAgentçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1) æå‡ºäº†ä¸€ä¸ªåŸºäºŽå¤šAgentç³»ç»Ÿçš„å¤šå›¾QAæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„è®­ç»ƒæ•°æ®ã€‚2) è®¾è®¡äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ï¼Œè§£å†³äº†VLMåœ¨æŽ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å¿½ç•¥è§†è§‰è¾“å…¥çš„é—®é¢˜ã€‚3) æå‡ºäº†ä¸€ä¸ªåŠ¨ä½œè½¨è¿¹ä¸¤çº§æŽ©ç ç­–ç•¥ï¼Œä½¿å¾—Agentèƒ½å¤Ÿé€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®žçŽ°ç¨³å®šçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œæ— éœ€ä¾èµ–æ˜‚è´µçš„ç›‘ç£å¾®è°ƒæ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®ç”Ÿæˆæ–¹é¢ï¼Œè®¾è®¡äº†ä¸åŒçš„Agentè§’è‰²ï¼Œåˆ†åˆ«è´Ÿè´£ç”Ÿæˆé—®é¢˜ã€é€‰æ‹©å›¾åƒå’Œæä¾›ç­”æ¡ˆï¼Œä»Žè€Œä¿è¯æ•°æ®çš„å¤šæ ·æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹é¢ï¼Œä½¿ç”¨äº†ç¨€ç–å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±Agenté‡‡å–æ­£ç¡®çš„åŠ¨ä½œåºåˆ—ã€‚åŠ¨ä½œè½¨è¿¹ä¸¤çº§æŽ©ç ç­–ç•¥é€šè¿‡é™åˆ¶Agentåœ¨ä¸åŒé˜¶æ®µå¯ä»¥é‡‡å–çš„åŠ¨ä½œï¼Œä»Žè€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆçŽ‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

IMAgentåœ¨æå‡ºçš„å¤šå›¾æ•°æ®é›†MIFG-QAä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†çŽ°æœ‰çš„å•å›¾Agentã€‚åŒæ—¶ï¼ŒIMAgentåœ¨çŽ°æœ‰çš„å•å›¾åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç«žäº‰åŠ›ï¼Œè¡¨æ˜Žå…¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ¶ˆèžå®žéªŒï¼ŒéªŒè¯äº†è§†è§‰åæ€å’Œç¡®è®¤å·¥å…·ä»¥åŠåŠ¨ä½œè½¨è¿¹æŽ©ç ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†å±•ç¤ºï¼ˆæœªçŸ¥ï¼‰ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

IMAgentå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœã€åŒ»å­¦å›¾åƒè¯Šæ–­ã€é¥æ„Ÿå›¾åƒåˆ†æžç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·ä»Žå¤šå¼ å›¾åƒä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œæ·±å…¥çš„æŽ¨ç†å’Œå†³ç­–ã€‚æœªæ¥ï¼ŒIMAgentå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è§†é¢‘ç†è§£ã€æœºå™¨äººå¯¼èˆªç­‰ï¼Œä¸ºäººå·¥æ™ºèƒ½åº”ç”¨å¸¦æ¥æ›´å¼ºå¤§çš„èƒ½åŠ›ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.

