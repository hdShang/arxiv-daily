---
layout: default
title: MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding
---

# MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding

**arXiv**: [2512.12307v1](https://arxiv.org/abs/2512.12307) | [PDF](https://arxiv.org/pdf/2512.12307.pdf)

**ä½œè€…**: Benjamin Beilharz, Thomas S. A. Wallis

**åˆ†ç±»**: cs.CV, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-12-13

**å¤‡æ³¨**: 18 pages, 6 figures. Supplementary material and code will be provided at the end of January

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMRDï¼Œåˆ©ç”¨å¯å¾®æ¸²æŸ“æŽ¢ç©¶è§†è§‰æ¨¡åž‹å¯¹3Dåœºæ™¯çš„ç†è§£èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¯å¾®æ¸²æŸ“` `æ¨¡åž‹å¯è§£é‡Šæ€§` `3Dåœºæ™¯ç†è§£` `è§†è§‰æ¨¡åž‹` `ç‰©ç†æ¸²æŸ“`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰æ¨¡åž‹éš¾ä»¥è§£é‡Šå…¶å¯¹3Dåœºæ™¯çš„éšå¼ç†è§£ï¼Œç¼ºä¹æœ‰æ•ˆçš„æŽ¢ç©¶æ–¹æ³•ã€‚
2. MRDåˆ©ç”¨å¯å¾®æ¸²æŸ“ï¼Œå¯»æ‰¾ç‰©ç†ä¸Šä¸åŒä½†æ¨¡åž‹æ¿€æ´»ç›¸åŒçš„3Dåœºæ™¯å‚æ•°ï¼Œä»Žè€ŒæŽ¢ç©¶æ¨¡åž‹å¯¹åœºæ™¯å±žæ€§çš„æ•æ„Ÿæ€§ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒMRDèƒ½æœ‰æ•ˆé‡å»ºåœºæ™¯å‚æ•°ï¼Œæ­ç¤ºæ¨¡åž‹å¯¹å½¢çŠ¶å’Œæè´¨ç­‰å±žæ€§çš„å…³æ³¨ç‚¹ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è®¸å¤šè§†è§‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç†è§£å’Œè§£é‡Šè¿™äº›æ¨¡åž‹çš„è¡¨å¾å’Œå†³ç­–ä»ç„¶å¾ˆå›°éš¾ã€‚è™½ç„¶è§†è§‰æ¨¡åž‹é€šå¸¸åœ¨2Dè¾“å…¥ä¸Šè®­ç»ƒï¼Œä½†äººä»¬å¸¸å¸¸å‡è®¾å®ƒä»¬å‘å±•äº†å¯¹åº•å±‚3Dåœºæ™¯çš„éšå¼è¡¨å¾ï¼ˆä¾‹å¦‚ï¼Œå¯¹éƒ¨åˆ†é®æŒ¡çš„å®¹å¿åº¦ï¼Œæˆ–æŽ¨ç†ç›¸å¯¹æ·±åº¦çš„èƒ½åŠ›ï¼‰ã€‚æœ¬æ–‡ä»‹ç»MRDï¼ˆmetamers rendered differentiablyï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºäºŽç‰©ç†çš„å¯å¾®æ¸²æŸ“æ¥æŽ¢ç©¶è§†è§‰æ¨¡åž‹å¯¹ç”Ÿæˆå¼3Dåœºæ™¯å±žæ€§çš„éšå¼ç†è§£ï¼Œé€šè¿‡å¯»æ‰¾åœ¨ç‰©ç†ä¸Šä¸åŒä½†äº§ç”Ÿç›¸åŒæ¨¡åž‹æ¿€æ´»çš„3Dåœºæ™¯å‚æ•°ï¼ˆå³æ¨¡åž‹åŒåº¦å¼‚æž„ä½“ï¼‰ã€‚ä¸Žä¹‹å‰åŸºäºŽåƒç´ çš„è¯„ä¼°æ¨¡åž‹è¡¨å¾çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›é‡å»ºç»“æžœå§‹ç»ˆåŸºäºŽç‰©ç†åœºæ™¯æè¿°ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥æŽ¢ç©¶æ¨¡åž‹å¯¹ç‰©ä½“å½¢çŠ¶çš„æ•æ„Ÿæ€§ï¼ŒåŒæ—¶ä¿æŒæè´¨å’Œå…‰ç…§ä¸å˜ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªæ¨¡åž‹æ¢å¤å‡ ä½•å½¢çŠ¶ï¼ˆå½¢çŠ¶ï¼‰å’ŒåŒå‘åå°„åˆ†å¸ƒå‡½æ•°ï¼ˆæè´¨ï¼‰ç­‰åœºæ™¯å‚æ•°çš„èƒ½åŠ›ã€‚ç»“æžœè¡¨æ˜Žï¼Œç›®æ ‡åœºæ™¯å’Œä¼˜åŒ–åœºæ™¯ä¹‹é—´çš„æ¨¡åž‹æ¿€æ´»å…·æœ‰é«˜åº¦ç›¸ä¼¼æ€§ï¼Œä½†è§†è§‰ç»“æžœå„ä¸ç›¸åŒã€‚å®šæ€§åœ°ï¼Œè¿™äº›é‡å»ºæœ‰åŠ©äºŽç ”ç©¶æ¨¡åž‹æ•æ„Ÿæˆ–ä¸å˜çš„ç‰©ç†åœºæ™¯å±žæ€§ã€‚MRDæœ‰æœ›é€šè¿‡åˆ†æžç‰©ç†åœºæ™¯å‚æ•°å¦‚ä½•é©±åŠ¨æ¨¡åž‹å“åº”çš„å˜åŒ–ï¼Œä»Žè€Œä¿ƒè¿›æˆ‘ä»¬å¯¹è®¡ç®—æœºå’Œäººç±»è§†è§‰çš„ç†è§£ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ·±åº¦å­¦ä¹ è§†è§‰æ¨¡åž‹åœ¨2Då›¾åƒä¸Šè®­ç»ƒï¼Œè™½ç„¶è¢«è®¤ä¸ºå­¦ä¹ äº†å¯¹3Dåœºæ™¯çš„éšå¼ç†è§£ï¼Œä½†ç¼ºä¹æœ‰æ•ˆçš„æ–¹æ³•æ¥æŽ¢ç©¶å’Œè§£é‡Šè¿™ç§ç†è§£ã€‚ä¹‹å‰çš„åƒç´ çº§æ–¹æ³•æ— æ³•ä¿è¯é‡å»ºç»“æžœçš„ç‰©ç†åˆç†æ€§ï¼Œéš¾ä»¥æŽ§åˆ¶åœºæ™¯å±žæ€§ï¼Œä¾‹å¦‚ç‹¬ç«‹åœ°æ”¹å˜å½¢çŠ¶æˆ–æè´¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMRDçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¯å¾®æ¸²æŸ“æŠ€æœ¯ï¼Œå°†è§†è§‰æ¨¡åž‹çš„è¾“å‡ºä¸Ž3Dåœºæ™¯å‚æ•°è”ç³»èµ·æ¥ã€‚é€šè¿‡ä¼˜åŒ–3Dåœºæ™¯å‚æ•°ï¼Œä½¿å¾—æ¸²æŸ“å‡ºçš„å›¾åƒåœ¨è§†è§‰æ¨¡åž‹ä¸­äº§ç”Ÿä¸Žç›®æ ‡å›¾åƒç›¸ä¼¼çš„æ¿€æ´»ï¼Œä»Žè€ŒåæŽ¨å‡ºæ¨¡åž‹æ‰€â€œçœ‹åˆ°â€çš„3Dåœºæ™¯ã€‚è¿™ç§æ–¹æ³•ä¿è¯äº†é‡å»ºç»“æžœçš„ç‰©ç†ä¸€è‡´æ€§ï¼Œå¹¶å…è®¸ç ”ç©¶è€…æŽ§åˆ¶å’Œæ“çºµåœºæ™¯å±žæ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMRDçš„æ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) é€‰æ‹©ä¸€ä¸ªç›®æ ‡å›¾åƒï¼Œè¾“å…¥åˆ°é¢„è®­ç»ƒçš„è§†è§‰æ¨¡åž‹ä¸­ï¼Œæå–ç‰¹å®šå±‚çš„æ¿€æ´»ä½œä¸ºç›®æ ‡æ¿€æ´»ã€‚2) åˆå§‹åŒ–ä¸€ä¸ª3Dåœºæ™¯ï¼ŒåŒ…å«å‡ ä½•å½¢çŠ¶ã€æè´¨å’Œå…‰ç…§ç­‰å‚æ•°ã€‚3) ä½¿ç”¨å¯å¾®æ¸²æŸ“å™¨å°†3Dåœºæ™¯æ¸²æŸ“æˆ2Då›¾åƒã€‚4) å°†æ¸²æŸ“çš„å›¾åƒè¾“å…¥åˆ°ç›¸åŒçš„è§†è§‰æ¨¡åž‹ä¸­ï¼Œæå–å¯¹åº”å±‚çš„æ¿€æ´»ã€‚5) è®¡ç®—æ¸²æŸ“å›¾åƒçš„æ¿€æ´»ä¸Žç›®æ ‡æ¿€æ´»ä¹‹é—´çš„æŸå¤±å‡½æ•°ã€‚6) ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–ç®—æ³•ï¼Œè°ƒæ•´3Dåœºæ™¯å‚æ•°ï¼Œæœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚7) é‡å¤æ­¥éª¤3-6ï¼Œç›´åˆ°æŸå¤±å‡½æ•°æ”¶æ•›ã€‚

**å…³é”®åˆ›æ–°**ï¼šMRDçš„å…³é”®åˆ›æ–°åœ¨äºŽå°†å¯å¾®æ¸²æŸ“æŠ€æœ¯ä¸Žè§†è§‰æ¨¡åž‹çš„è¡¨å¾å­¦ä¹ è”ç³»èµ·æ¥ï¼Œæä¾›äº†ä¸€ç§åŸºäºŽç‰©ç†çš„ã€å¯è§£é‡Šçš„æ¨¡åž‹æŽ¢ç©¶æ–¹æ³•ã€‚ä¸Žä¼ ç»Ÿçš„åŸºäºŽåƒç´ çš„ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒMRDçš„é‡å»ºç»“æžœå…·æœ‰ç‰©ç†åˆç†æ€§ï¼Œå¹¶ä¸”å¯ä»¥æŽ§åˆ¶åœºæ™¯å±žæ€§ï¼Œä¾‹å¦‚ç‹¬ç«‹åœ°æ”¹å˜å½¢çŠ¶æˆ–æè´¨ã€‚

**å…³é”®è®¾è®¡**ï¼šMRDçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨åŸºäºŽç‰©ç†çš„æ¸²æŸ“å™¨ï¼Œä¿è¯æ¸²æŸ“ç»“æžœçš„çœŸå®žæ„Ÿå’Œç‰©ç†ä¸€è‡´æ€§ã€‚2) é€‰æ‹©åˆé€‚çš„è§†è§‰æ¨¡åž‹å±‚ï¼Œä»¥æå–å…·æœ‰ä»£è¡¨æ€§çš„æ¿€æ´»ã€‚3) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚L2æŸå¤±æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»¥è¡¡é‡æ¿€æ´»ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚4) é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç®—æ³•ï¼Œä¾‹å¦‚Adamæˆ–LBFGSï¼Œä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–3Dåœºæ™¯å‚æ•°ã€‚5) å¯¹3Dåœºæ™¯å‚æ•°è¿›è¡Œåˆç†çš„åˆå§‹åŒ–å’Œçº¦æŸï¼Œä»¥é¿å…ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å¥‡å¼‚å€¼å’Œä¸åˆç†çš„åœºæ™¯é…ç½®ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMRDèƒ½å¤Ÿæœ‰æ•ˆåœ°é‡å»º3Dåœºæ™¯å‚æ•°ï¼Œä½¿å¾—é‡å»ºåœºæ™¯åœ¨è§†è§‰æ¨¡åž‹ä¸­äº§ç”Ÿä¸Žç›®æ ‡åœºæ™¯ç›¸ä¼¼çš„æ¿€æ´»ã€‚è™½ç„¶è§†è§‰æ•ˆæžœä¸Šå¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œä½†æ¨¡åž‹æ¿€æ´»çš„ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè¡¨æ˜Žæ¨¡åž‹å¯¹æŸäº›ç‰©ç†å±žæ€§å…·æœ‰ä¸å˜æ€§ã€‚é€šè¿‡åˆ†æžé‡å»ºç»“æžœï¼Œå¯ä»¥æ­ç¤ºæ¨¡åž‹å¯¹å½¢çŠ¶ã€æè´¨å’Œå…‰ç…§ç­‰å±žæ€§çš„æ•æ„Ÿç¨‹åº¦ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

MRDå¯ç”¨äºŽåˆ†æžå’Œç†è§£è®¡ç®—æœºè§†è§‰æ¨¡åž‹çš„å†…éƒ¨è¡¨å¾ï¼Œæ­ç¤ºæ¨¡åž‹å¯¹ä¸åŒåœºæ™¯å±žæ€§çš„æ•æ„Ÿæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯åº”ç”¨äºŽå¯¹æŠ—æ ·æœ¬ç”Ÿæˆï¼Œé€šè¿‡æ“çºµ3Dåœºæ™¯å‚æ•°ç”Ÿæˆéš¾ä»¥è¢«æ¨¡åž‹è¯†åˆ«çš„å›¾åƒã€‚æœªæ¥ï¼ŒMRDæœ‰æœ›ä¿ƒè¿›è®¡ç®—æœºè§†è§‰å’Œäººç±»è§†è§‰çš„äº¤å‰ç ”ç©¶ï¼Œå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£äººç±»è§†è§‰æ„ŸçŸ¥æœºåˆ¶ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

