---
layout: default
title: UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework
---

# UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework

**arXiv**: [2512.03918v1](https://arxiv.org/abs/2512.03918) | [PDF](https://arxiv.org/pdf/2512.03918.pdf)

**ä½œè€…**: Youxin Pang, Yong Zhang, Ruizhi Shao, Xiang Deng, Feng Gao, Xu Xiaoming, Xiaoming Wei, Yebin Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

**å¤‡æ³¨**: https://carlyx.github.io/UniMo/

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniMoï¼šæå‡ºä¸€ä¸ªè‡ªå›žå½’æ¡†æž¶ï¼Œç»Ÿä¸€å»ºæ¨¡2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨ï¼Œå®žçŽ°åŒæ­¥ç”Ÿæˆä¸Žç†è§£ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `2Dè§†é¢‘ç”Ÿæˆ` `3Däººä½“è¿åŠ¨` `è‡ªå›žå½’æ¨¡åž‹` `å¤šæ¨¡æ€èžåˆ` `ç»Ÿä¸€å»ºæ¨¡` `è¿åŠ¨æ•æ‰` `VQ-VAE`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•éš¾ä»¥åŒæ—¶ç”Ÿæˆå’Œç†è§£2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨ï¼Œå› ä¸ºå®ƒä»¬åœ¨ç»“æž„å’Œåˆ†å¸ƒä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚
2. UniMoå°†2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨å»ºæ¨¡ä¸ºç»Ÿä¸€çš„tokensåºåˆ—ï¼Œåˆ©ç”¨è‡ªå›žå½’æ¨¡åž‹å®žçŽ°åŒæ­¥ç”Ÿæˆå’Œç†è§£ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒUniMoèƒ½å¤ŸåŒæ—¶ç”Ÿæˆå¯¹åº”çš„è§†é¢‘å’Œè¿åŠ¨ï¼Œå¹¶æ‰§è¡Œç²¾ç¡®çš„è¿åŠ¨æ•æ‰ï¼ŒéªŒè¯äº†ç»Ÿä¸€å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†UniMoï¼Œä¸€ä¸ªåˆ›æ–°çš„è‡ªå›žå½’æ¨¡åž‹ï¼Œç”¨äºŽåœ¨ç»Ÿä¸€æ¡†æž¶å†…è”åˆå»ºæ¨¡2Däººä½“è§†é¢‘å’Œ3Däººä½“è¿åŠ¨ï¼Œé¦–æ¬¡å®žçŽ°äº†è¿™ä¸¤ç§æ¨¡æ€çš„åŒæ­¥ç”Ÿæˆå’Œç†è§£ã€‚ç›®å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä»¥ä¸€ç§æ¨¡æ€ä¸ºæ¡ä»¶ç”Ÿæˆå¦ä¸€ç§æ¨¡æ€ï¼Œæˆ–è€…å°†å®ƒä»¬ä¸Žæ–‡æœ¬å’ŒéŸ³é¢‘ç­‰å…¶ä»–æ¨¡æ€é›†æˆã€‚ç”±äºŽ2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨åœ¨ç»“æž„å’Œåˆ†å¸ƒä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå› æ­¤ç»Ÿä¸€å®ƒä»¬è¿›è¡ŒåŒæ­¥ä¼˜åŒ–å’Œç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æŽ¢ç´¢çš„é¢†åŸŸï¼Œé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚å—LLMç»Ÿä¸€ä¸åŒæ¨¡æ€èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è§†é¢‘å’Œ3Dè¿åŠ¨å»ºæ¨¡ä¸ºç»Ÿä¸€çš„tokensåºåˆ—ï¼Œå¹¶åˆ©ç”¨å•ç‹¬çš„åµŒå…¥å±‚æ¥ç¼“è§£åˆ†å¸ƒå·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åºåˆ—å»ºæ¨¡ç­–ç•¥ï¼Œåœ¨ä¸€ä¸ªæ¡†æž¶å†…é›†æˆäº†ä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œè¯æ˜Žäº†ç»Ÿä¸€å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚è€Œä¸”ï¼Œä¸ºäº†æœ‰æ•ˆåœ°ä¸Žè§†è§‰tokenså¯¹é½å¹¶ä¿ç•™3Dç©ºé—´ä¿¡æ¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å…·æœ‰æ—¶é—´æ‰©å±•ç­–ç•¥çš„æ–°åž‹3Dè¿åŠ¨tokenizerï¼Œä½¿ç”¨å•ä¸ªVQ-VAEæ¥ç”Ÿæˆé‡åŒ–çš„è¿åŠ¨tokensã€‚å®ƒå…·æœ‰å¤šä¸ªä¸“å®¶è§£ç å™¨ï¼Œç”¨äºŽå¤„ç†èº«ä½“å½¢çŠ¶ã€å¹³ç§»ã€å…¨å±€æ–¹å‘å’Œèº«ä½“å§¿åŠ¿ï¼Œä»¥å®žçŽ°å¯é çš„3Dè¿åŠ¨é‡å»ºã€‚å¤§é‡çš„å®žéªŒè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰§è¡Œç²¾ç¡®çš„è¿åŠ¨æ•æ‰çš„åŒæ—¶ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆç›¸åº”çš„è§†é¢‘å’Œè¿åŠ¨ã€‚è¿™é¡¹å·¥ä½œæŒ–æŽ˜äº†LLMèžåˆä¸åŒæ•°æ®ç±»åž‹çš„èƒ½åŠ›ï¼Œä¸ºå°†ä»¥äººä¸ºä¸­å¿ƒçš„ä¿¡æ¯é›†æˆåˆ°çŽ°æœ‰æ¨¡åž‹ä¸­é“ºå¹³äº†é“è·¯ï¼Œå¹¶æœ‰å¯èƒ½å®žçŽ°äººç±»ã€ç‰©ä½“å’Œåœºæ™¯çš„å¤šæ¨¡æ€ã€å¯æŽ§çš„è”åˆå»ºæ¨¡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€ç”Ÿæˆæˆ–å°†2D/3Däººä½“è¿åŠ¨ä¸Žå…¶ä»–æ¨¡æ€èžåˆï¼Œç¼ºä¹å¯¹2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨çš„ç»Ÿä¸€å»ºæ¨¡å’ŒåŒæ­¥ç”Ÿæˆèƒ½åŠ›ã€‚ç”±äºŽ2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨åœ¨ç»“æž„å’Œåˆ†å¸ƒä¸Šå­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œç›´æŽ¥è¿›è¡Œè”åˆå»ºæ¨¡é¢ä¸´æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šå€Ÿé‰´LLMç»Ÿä¸€ä¸åŒæ¨¡æ€çš„èƒ½åŠ›ï¼Œå°†2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨è§†ä¸ºç»Ÿä¸€çš„tokensåºåˆ—ï¼Œé€šè¿‡è‡ªå›žå½’æ¨¡åž‹å­¦ä¹ å®ƒä»¬ä¹‹é—´çš„è”åˆåˆ†å¸ƒã€‚é€šè¿‡ç»Ÿä¸€çš„å»ºæ¨¡æ¡†æž¶ï¼Œå®žçŽ°2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨çš„åŒæ­¥ç”Ÿæˆå’Œç†è§£ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šUniMoåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) 2Dè§†é¢‘ç¼–ç å™¨ï¼šå°†è§†é¢‘å¸§ç¼–ç ä¸ºè§†è§‰tokensåºåˆ—ï¼›2) 3Dè¿åŠ¨tokenizerï¼šå°†3Däººä½“è¿åŠ¨æ•°æ®é‡åŒ–ä¸ºè¿åŠ¨tokensåºåˆ—ï¼›3) ç»Ÿä¸€çš„è‡ªå›žå½’æ¨¡åž‹ï¼šä»¥è§†è§‰tokenså’Œè¿åŠ¨tokensä½œä¸ºè¾“å…¥ï¼Œå­¦ä¹ å®ƒä»¬çš„è”åˆåˆ†å¸ƒï¼Œå®žçŽ°åŒæ­¥ç”Ÿæˆï¼›4) 3Dè¿åŠ¨è§£ç å™¨ï¼šå°†è¿åŠ¨tokensè§£ç ä¸º3Däººä½“è¿åŠ¨æ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼š1) ç»Ÿä¸€å»ºæ¨¡æ¡†æž¶ï¼šé¦–æ¬¡å°†2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨ç»Ÿä¸€åˆ°åŒä¸€ä¸ªè‡ªå›žå½’æ¨¡åž‹ä¸­ï¼Œå®žçŽ°åŒæ­¥ç”Ÿæˆå’Œç†è§£ï¼›2) 3Dè¿åŠ¨tokenizerï¼šè®¾è®¡äº†ä¸€ç§å…·æœ‰æ—¶é—´æ‰©å±•ç­–ç•¥çš„æ–°åž‹3Dè¿åŠ¨tokenizerï¼Œä½¿ç”¨å•ä¸ªVQ-VAEç”Ÿæˆé‡åŒ–çš„è¿åŠ¨tokensï¼Œå¹¶ä½¿ç”¨å¤šä¸ªä¸“å®¶è§£ç å™¨å¤„ç†èº«ä½“å½¢çŠ¶ã€å¹³ç§»ã€å…¨å±€æ–¹å‘å’Œèº«ä½“å§¿åŠ¿ï¼Œä»¥å®žçŽ°å¯é çš„3Dè¿åŠ¨é‡å»ºã€‚

**å…³é”®è®¾è®¡**ï¼š1) ä½¿ç”¨å•ç‹¬çš„åµŒå…¥å±‚æ¥ç¼“è§£2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨åœ¨åˆ†å¸ƒä¸Šçš„å·®è·ï¼›2) è®¾è®¡äº†ä¸€ç§åºåˆ—å»ºæ¨¡ç­–ç•¥ï¼Œåœ¨ä¸€ä¸ªæ¡†æž¶å†…é›†æˆäº†è§†é¢‘ç”Ÿæˆå’Œè¿åŠ¨ç”Ÿæˆä¸¤ä¸ªä»»åŠ¡ï¼›3) 3Dè¿åŠ¨tokenizeré‡‡ç”¨VQ-VAEè¿›è¡Œé‡åŒ–ï¼Œå¹¶ä½¿ç”¨å¤šä¸ªä¸“å®¶è§£ç å™¨åˆ†åˆ«å¤„ç†èº«ä½“å½¢çŠ¶ã€å¹³ç§»ã€å…¨å±€æ–¹å‘å’Œèº«ä½“å§¿åŠ¿ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

UniMoåœ¨åŒæ­¥ç”Ÿæˆ2Dè§†é¢‘å’Œ3Däººä½“è¿åŠ¨æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæžœã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒUniMoèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘å’Œè¿åŠ¨ï¼Œå¹¶ä¸”èƒ½å¤Ÿå‡†ç¡®åœ°æ•æ‰äººä½“è¿åŠ¨çš„ç»†èŠ‚ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒUniMoåœ¨è¿åŠ¨æ•æ‰ç²¾åº¦å’Œç”Ÿæˆè§†é¢‘çš„çœŸå®žæ€§æ–¹é¢å‡æœ‰æå‡ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

UniMoåœ¨è™šæ‹ŸçŽ°å®žã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºŽç”Ÿæˆé€¼çœŸçš„äººä½“è¿åŠ¨è§†é¢‘ï¼Œä¹Ÿå¯ä»¥ç”¨äºŽæ ¹æ®ç»™å®šçš„è§†é¢‘ç”Ÿæˆç›¸åº”çš„3Däººä½“è¿åŠ¨ã€‚æ­¤å¤–ï¼ŒUniMoè¿˜å¯ä»¥ä½œä¸ºå…¶ä»–å¤šæ¨¡æ€æ¨¡åž‹çš„ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºŽæž„å»ºæ›´å¤æ‚çš„äººæœºäº¤äº’ç³»ç»Ÿï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ç»“åˆæ–‡æœ¬æˆ–è¯­éŸ³è¾“å…¥æ¥æŽ§åˆ¶è™šæ‹Ÿè§’è‰²çš„è¿åŠ¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.

