---
layout: default
title: A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning
---

# A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning

**arXiv**: [2512.14442v1](https://arxiv.org/abs/2512.14442) | [PDF](https://arxiv.org/pdf/2512.14442.pdf)

**ä½œè€…**: Zixin Zhang, Kanghao Chen, Hanqing Wang, Hongfei Zhang, Harold Haodong Chen, Chenfei Liao, Litao Guo, Ying-Cong Chen

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºA4-Agentï¼šä¸€ç§ç”¨äºŽé›¶æ ·æœ¬å¯ä¾›æ€§æŽ¨ç†çš„Agentæ¡†æž¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¯ä¾›æ€§é¢„æµ‹` `å…·èº«æ™ºèƒ½` `é›¶æ ·æœ¬å­¦ä¹ ` `Agentæ¡†æž¶` `é¢„è®­ç»ƒæ¨¡åž‹` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `ç”Ÿæˆæ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰å¯ä¾›æ€§é¢„æµ‹æ¨¡åž‹ä¾èµ–ç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ³›åŒ–æ€§å·®ï¼Œéš¾ä»¥é€‚åº”æ–°ç‰©ä½“å’ŒçŽ¯å¢ƒã€‚
2. A4-Agentæ¡†æž¶è§£è€¦å¯ä¾›æ€§é¢„æµ‹ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«ç”±Dreamerã€Thinkerå’ŒSpotterå®Œæˆã€‚
3. A4-Agentæ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹ä¼˜åŠ¿ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†SOTAç›‘ç£æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯ä¾›æ€§é¢„æµ‹ï¼Œå³åŸºäºŽè¯­è¨€æŒ‡ä»¤è¯†åˆ«ç‰©ä½“ä¸Šçš„äº¤äº’åŒºåŸŸï¼Œå¯¹äºŽå…·èº«æ™ºèƒ½è‡³å…³é‡è¦ã€‚ç›®å‰ä¸»æµçš„ç«¯åˆ°ç«¯æ¨¡åž‹å°†é«˜å±‚æŽ¨ç†å’Œä½Žå±‚åŸºç¡€è€¦åˆåˆ°ä¸€ä¸ªå•ä¸€çš„pipelineä¸­ï¼Œå¹¶ä¾èµ–äºŽåœ¨æ ‡æ³¨æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼Œè¿™å¯¼è‡´äº†å¯¹æ–°ç‰©ä½“å’Œæœªè§çŽ¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æœ¬æ–‡æå‡ºA4-Agentï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„agentæ¡†æž¶ï¼Œå°†å¯ä¾›æ€§é¢„æµ‹è§£è€¦ä¸ºä¸€ä¸ªä¸‰é˜¶æ®µçš„pipelineã€‚è¯¥æ¡†æž¶åœ¨æµ‹è¯•æ—¶åè°ƒä¸“é—¨çš„åŸºç¡€æ¨¡åž‹ï¼šï¼ˆ1ï¼‰$	extbf{Dreamer}$ï¼Œå®ƒä½¿ç”¨ç”Ÿæˆæ¨¡åž‹æ¥å¯è§†åŒ–äº¤äº’çš„$	extit{æ ·å­}$ï¼›ï¼ˆ2ï¼‰$	extbf{Thinker}$ï¼Œå®ƒåˆ©ç”¨å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹æ¥å†³å®šä¸Ž$	extit{ä»€ä¹ˆ}$ç‰©ä½“éƒ¨åˆ†è¿›è¡Œäº¤äº’ï¼›ï¼ˆ3ï¼‰$	extbf{Spotter}$ï¼Œå®ƒåè°ƒè§†è§‰åŸºç¡€æ¨¡åž‹æ¥ç²¾ç¡®å®šä½äº¤äº’åŒºåŸŸçš„$	extit{ä½ç½®}$ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œæ— éœ€ä»»ä½•ç‰¹å®šäºŽä»»åŠ¡çš„å¾®è°ƒï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬æ¡†æž¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽæœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¯¹çœŸå®žä¸–ç•ŒçŽ¯å¢ƒçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¯ä¾›æ€§é¢„æµ‹ä¸­ï¼ŒçŽ°æœ‰ç«¯åˆ°ç«¯æ¨¡åž‹åœ¨æ–°ç‰©ä½“å’Œæœªè§çŽ¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¿™äº›æ¨¡åž‹é€šå¸¸ä¾èµ–äºŽå¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œéš¾ä»¥é€‚åº”çœŸå®žä¸–ç•Œä¸­å¤æ‚å¤šå˜çš„æƒ…å†µï¼Œå¹¶ä¸”å°†é«˜å±‚æŽ¨ç†å’Œä½Žå±‚æ„ŸçŸ¥è€¦åˆåœ¨ä¸€èµ·ï¼Œç¼ºä¹çµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¯ä¾›æ€§é¢„æµ‹ä»»åŠ¡è§£è€¦ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„é˜¶æ®µï¼Œåˆ†åˆ«å¯¹åº”äºŽâ€œæƒ³è±¡äº¤äº’çš„æ ·å­â€ã€â€œå†³å®šä¸Žå“ªä¸ªç‰©ä½“éƒ¨åˆ†äº¤äº’â€å’Œâ€œç²¾ç¡®å®šä½äº¤äº’åŒºåŸŸâ€ã€‚æ¯ä¸ªé˜¶æ®µéƒ½ç”±ä¸“é—¨çš„é¢„è®­ç»ƒæ¨¡åž‹è´Ÿè´£ï¼Œä»Žè€Œå……åˆ†åˆ©ç”¨äº†è¿™äº›æ¨¡åž‹çš„å…ˆéªŒçŸ¥è¯†å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§è§£è€¦çš„è®¾è®¡ä½¿å¾—æ¨¡åž‹å¯ä»¥æ›´å¥½åœ°é€‚åº”æ–°çš„ç‰©ä½“å’ŒçŽ¯å¢ƒï¼Œå¹¶ä¸”æ— éœ€è¿›è¡Œç‰¹å®šäºŽä»»åŠ¡çš„è®­ç»ƒã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šA4-Agentæ¡†æž¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šDreamerã€Thinkerå’ŒSpotterã€‚Dreamerä½¿ç”¨ç”Ÿæˆæ¨¡åž‹ï¼ˆå¦‚æ‰©æ•£æ¨¡åž‹ï¼‰æ¥å¯è§†åŒ–äº¤äº’çš„æ ·å­ï¼Œä¸ºåŽç»­çš„æŽ¨ç†æä¾›è§†è§‰ä¿¡æ¯ã€‚Thinkeråˆ©ç”¨å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆå¦‚CLIPï¼‰æ¥å†³å®šä¸Žå“ªä¸ªç‰©ä½“éƒ¨åˆ†è¿›è¡Œäº¤äº’ï¼Œå°†è¯­è¨€æŒ‡ä»¤ä¸Žè§†è§‰ä¿¡æ¯å¯¹é½ã€‚Spotteråè°ƒè§†è§‰åŸºç¡€æ¨¡åž‹ï¼ˆå¦‚åˆ†å‰²æ¨¡åž‹ï¼‰æ¥ç²¾ç¡®å®šä½äº¤äº’åŒºåŸŸï¼Œè¾“å‡ºæœ€ç»ˆçš„å¯ä¾›æ€§é¢„æµ‹ç»“æžœã€‚æ•´ä¸ªæµç¨‹æ— éœ€è®­ç»ƒï¼Œé€šè¿‡åè°ƒè¿™äº›é¢„è®­ç»ƒæ¨¡åž‹æ¥å®žçŽ°é›¶æ ·æœ¬å¯ä¾›æ€§æŽ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„agentæ¡†æž¶ï¼Œå°†å¯ä¾›æ€§é¢„æµ‹ä»»åŠ¡è§£è€¦ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„é˜¶æ®µï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹æ¥å®žçŽ°æ¯ä¸ªé˜¶æ®µçš„åŠŸèƒ½ã€‚è¿™ç§è§£è€¦çš„è®¾è®¡ä½¿å¾—æ¨¡åž‹å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„å…ˆéªŒçŸ¥è¯†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»Žè€Œåœ¨é›¶æ ·æœ¬çš„æƒ…å†µä¸‹å®žçŽ°é«˜æ€§èƒ½çš„å¯ä¾›æ€§é¢„æµ‹ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒA4-Agentæ— éœ€è¿›è¡Œç‰¹å®šäºŽä»»åŠ¡çš„è®­ç»ƒï¼Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šDreamerå¯ä»¥ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆæ¨¡åž‹ï¼Œä¾‹å¦‚Stable Diffusionï¼Œæ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆäº¤äº’çš„è§†è§‰å›¾åƒã€‚Thinkerä½¿ç”¨CLIPç­‰è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼Œå°†è¯­è¨€æŒ‡ä»¤ç¼–ç ä¸ºæ–‡æœ¬ç‰¹å¾ï¼Œå¹¶å°†ç‰©ä½“å›¾åƒç¼–ç ä¸ºè§†è§‰ç‰¹å¾ï¼Œé€šè¿‡è®¡ç®—ç›¸ä¼¼åº¦æ¥é€‰æ‹©æœ€ç›¸å…³çš„ç‰©ä½“éƒ¨åˆ†ã€‚Spotterå¯ä»¥ä½¿ç”¨Mask R-CNNç­‰åˆ†å‰²æ¨¡åž‹ï¼Œæ ¹æ®Thinkerçš„è¾“å‡ºï¼Œç²¾ç¡®å®šä½äº¤äº’åŒºåŸŸçš„åƒç´ çº§åˆ«ä½ç½®ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„å–å†³äºŽæ‰€é€‰æ‹©çš„é¢„è®­ç»ƒæ¨¡åž‹ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

A4-Agentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽæœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ï¼Œè¯æ˜Žäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ•°æ®é›†ä¸Šï¼ŒA4-Agentçš„æ€§èƒ½æ¯”SOTAæ–¹æ³•æå‡äº†10%ä»¥ä¸Šã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒA4-Agentåœ¨çœŸå®žä¸–ç•ŒçŽ¯å¢ƒä¸­ä¹Ÿè¡¨çŽ°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ï¼Œè¡¨æ˜Žå…¶å…·æœ‰å¾ˆå¼ºçš„å®žç”¨ä»·å€¼ã€‚è¿™äº›å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒA4-Agentæ˜¯ä¸€ç§éžå¸¸æœ‰å‰æ™¯çš„å¯ä¾›æ€§é¢„æµ‹æ–¹æ³•ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

A4-Agentæ¡†æž¶åœ¨æœºå™¨äººæ“ä½œã€è™šæ‹ŸåŠ©æ‰‹å’Œå¢žå¼ºçŽ°å®žç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£äººç±»çš„æŒ‡ä»¤ï¼Œå¹¶è‡ªä¸»åœ°æ‰§è¡Œå„ç§ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®â€œæ‰“å¼€æŠ½å±‰â€çš„æŒ‡ä»¤ï¼Œè‡ªåŠ¨è¯†åˆ«æŠ½å±‰çš„ä½ç½®å¹¶æ‰§è¡Œæ‰“å¼€æ“ä½œã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æž¶è¿˜å¯ä»¥ç”¨äºŽè™šæ‹ŸåŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­è¿›è¡Œäº¤äº’ã€‚åœ¨å¢žå¼ºçŽ°å®žä¸­ï¼Œå®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·è¯†åˆ«ç‰©ä½“ä¸Šçš„å¯äº¤äº’åŒºåŸŸï¼Œå¹¶æä¾›ç›¸åº”çš„æ“ä½œå»ºè®®ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.

