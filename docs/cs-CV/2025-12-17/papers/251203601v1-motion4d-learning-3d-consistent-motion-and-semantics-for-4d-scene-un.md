---
layout: default
title: Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding
---

# Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding

**arXiv**: [2512.03601v1](https://arxiv.org/abs/2512.03601) | [PDF](https://arxiv.org/pdf/2512.03601.pdf)

**ä½œè€…**: Haoran Zhou, Gim Hee Lee

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

**å¤‡æ³¨**: Accepted to NeurIPS 2025

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://hrzhou2.github.io/motion4d-web/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Motion4Dï¼šå­¦ä¹ 3Dä¸€è‡´çš„è¿åŠ¨å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œç”¨äºŽ4Dåœºæ™¯ç†è§£**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `4Dåœºæ™¯ç†è§£` `åŠ¨æ€åœºæ™¯åˆ†æž` `3Dä¸€è‡´æ€§` `é«˜æ–¯æº…å°„` `è¿åŠ¨ä¼°è®¡` `è¯­ä¹‰åˆ†å‰²` `å•ç›®è§†é¢‘` `åŸºç¡€æ¨¡åž‹`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰2Dè§†è§‰åŸºç¡€æ¨¡åž‹åœ¨åŠ¨æ€åœºæ™¯åˆ†æžä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†ç¼ºä¹3Dä¸€è‡´æ€§ï¼Œå¯¼è‡´ç©ºé—´é”™ä½å’Œæ—¶é—´é—ªçƒã€‚
2. Motion4Då°†2Då…ˆéªŒçŸ¥è¯†èžå…¥4Dé«˜æ–¯æº…å°„è¡¨ç¤ºï¼Œé€šè¿‡é¡ºåºå’Œå…¨å±€ä¼˜åŒ–ï¼Œå®žçŽ°3Dä¸€è‡´çš„è¿åŠ¨å’Œè¯­ä¹‰ç†è§£ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒMotion4Dåœ¨ç‚¹äº‘è·Ÿè¸ªã€è§†é¢‘åˆ†å‰²å’Œæ–°è§†è§’åˆæˆç­‰ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰2Då’Œ3Dæ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMotion4Dï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³å•ç›®è§†é¢‘åŠ¨æ€åœºæ™¯åˆ†æžä¸­ï¼ŒçŽ°æœ‰2DåŸºç¡€æ¨¡åž‹ç¼ºä¹3Dä¸€è‡´æ€§çš„é—®é¢˜ã€‚Motion4Då°†2DåŸºç¡€æ¨¡åž‹çš„å…ˆéªŒçŸ¥è¯†æ•´åˆåˆ°ç»Ÿä¸€çš„4Dé«˜æ–¯æº…å°„è¡¨ç¤ºä¸­ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸€ä¸ªä¸¤é˜¶æ®µè¿­ä»£ä¼˜åŒ–æ¡†æž¶ï¼š1) é¡ºåºä¼˜åŒ–ï¼Œåˆ†é˜¶æ®µæ›´æ–°è¿åŠ¨å’Œè¯­ä¹‰åœºï¼Œä»¥ä¿æŒå±€éƒ¨ä¸€è‡´æ€§ï¼›2) å…¨å±€ä¼˜åŒ–ï¼Œè”åˆä¼˜åŒ–æ‰€æœ‰å±žæ€§ï¼Œä»¥å®žçŽ°é•¿æœŸè¿žè´¯æ€§ã€‚ä¸ºäº†æé«˜è¿åŠ¨ç²¾åº¦ï¼Œå¼•å…¥äº†3Dç½®ä¿¡åº¦å›¾ï¼ŒåŠ¨æ€è°ƒæ•´è¿åŠ¨å…ˆéªŒï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”é‡é‡‡æ ·è¿‡ç¨‹ï¼ŒåŸºäºŽåƒç´ RGBå’Œè¯­ä¹‰è¯¯å·®ï¼Œåœ¨æ¬ è¡¨ç¤ºåŒºåŸŸæ’å…¥æ–°çš„é«˜æ–¯åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–è¯­ä¹‰åœºå’Œæ›´æ–°SAMçš„æç¤ºï¼Œå¢žå¼ºè¯­ä¹‰è¿žè´¯æ€§ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒMotion4Dåœ¨åŸºäºŽç‚¹çš„è·Ÿè¸ªã€è§†é¢‘å¯¹è±¡åˆ†å‰²å’Œæ–°è§†è§’åˆæˆç­‰å¤šç§åœºæ™¯ç†è§£ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºŽ2DåŸºç¡€æ¨¡åž‹å’ŒçŽ°æœ‰3Dæ–¹æ³•ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºŽ2Dè§†è§‰åŸºç¡€æ¨¡åž‹çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†å•ç›®è§†é¢‘çš„åŠ¨æ€åœºæ™¯ç†è§£æ—¶ï¼Œè™½ç„¶å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ç¼ºä¹3Dä¸€è‡´æ€§ã€‚è¿™å¯¼è‡´åœ¨å¤æ‚çš„3DçŽ¯å¢ƒä¸­å‡ºçŽ°ä¸¥é‡çš„å‡ ä½•é”™ä½å’Œæ—¶é—´ä¸Šçš„é—ªçƒçŽ°è±¡ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦ç²¾ç¡®3Dä¿¡æ¯çš„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿä¿è¯3Dä¸€è‡´æ€§çš„åŠ¨æ€åœºæ™¯ç†è§£æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMotion4Dçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†2Dè§†è§‰åŸºç¡€æ¨¡åž‹çš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†ä¸Ž3Dåœºæ™¯è¡¨ç¤ºç›¸ç»“åˆï¼Œåˆ©ç”¨é«˜æ–¯æº…å°„ï¼ˆGaussian Splattingï¼‰ä½œä¸ºç»Ÿä¸€çš„4Dè¡¨ç¤ºï¼Œå¹¶é€šè¿‡è¿­ä»£ä¼˜åŒ–æ¡†æž¶æ¥ä¿è¯è¿åŠ¨å’Œè¯­ä¹‰çš„3Dä¸€è‡´æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨2Dæ¨¡åž‹çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶å…‹æœå…¶åœ¨3Dç©ºé—´ä¸­çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMotion4Dçš„æ•´ä½“æ¡†æž¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„è¿­ä»£ä¼˜åŒ–é˜¶æ®µï¼šé¡ºåºä¼˜åŒ–å’Œå…¨å±€ä¼˜åŒ–ã€‚é¡ºåºä¼˜åŒ–é¦–å…ˆæ›´æ–°è¿åŠ¨åœºï¼Œç„¶åŽæ›´æ–°è¯­ä¹‰åœºï¼Œä»¥ä¿æŒå±€éƒ¨ä¸€è‡´æ€§ã€‚å…¨å±€ä¼˜åŒ–åˆ™è”åˆä¼˜åŒ–æ‰€æœ‰å±žæ€§ï¼Œä»¥å®žçŽ°é•¿æœŸè¿žè´¯æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æž¶è¿˜åŒ…æ‹¬ä¸€ä¸ª3Dç½®ä¿¡åº¦å›¾ï¼Œç”¨äºŽåŠ¨æ€è°ƒæ•´è¿åŠ¨å…ˆéªŒï¼Œä»¥åŠä¸€ä¸ªè‡ªé€‚åº”é‡é‡‡æ ·è¿‡ç¨‹ï¼Œç”¨äºŽåœ¨æ¬ è¡¨ç¤ºåŒºåŸŸæ’å…¥æ–°çš„é«˜æ–¯åˆ†å¸ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šMotion4Dçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶å°†2Dè§†è§‰åŸºç¡€æ¨¡åž‹çš„å…ˆéªŒçŸ¥è¯†æœ‰æ•ˆåœ°èžå…¥åˆ°3Dåœºæ™¯è¡¨ç¤ºä¸­ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¿­ä»£ä¼˜åŒ–æ¡†æž¶ï¼Œä»¥ä¿è¯è¿åŠ¨å’Œè¯­ä¹‰çš„3Dä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œ3Dç½®ä¿¡åº¦å›¾å’Œè‡ªé€‚åº”é‡é‡‡æ ·è¿‡ç¨‹è¿›ä¸€æ­¥æé«˜äº†è¿åŠ¨ç²¾åº¦å’Œåœºæ™¯è¡¨ç¤ºçš„å®Œæ•´æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMotion4Dèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„3DåŠ¨æ€åœºæ™¯ï¼Œå¹¶æä¾›æ›´ç²¾ç¡®çš„åœºæ™¯ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼š3Dç½®ä¿¡åº¦å›¾çš„è®¾è®¡ç”¨äºŽåŠ¨æ€è°ƒæ•´è¿åŠ¨å…ˆéªŒï¼Œå…¶å…·ä½“å®žçŽ°æ–¹å¼æœªçŸ¥ã€‚è‡ªé€‚åº”é‡é‡‡æ ·è¿‡ç¨‹åŸºäºŽåƒç´ RGBå’Œè¯­ä¹‰è¯¯å·®æ¥ç¡®å®šéœ€è¦æ’å…¥æ–°é«˜æ–¯åˆ†å¸ƒçš„åŒºåŸŸï¼Œå…·ä½“çš„è¯¯å·®è®¡ç®—æ–¹å¼å’Œé˜ˆå€¼è®¾ç½®æœªçŸ¥ã€‚è¯­ä¹‰ä¸€è‡´æ€§é€šè¿‡è¿­ä»£ä¼˜åŒ–è¯­ä¹‰åœºå’Œæ›´æ–°SAMçš„æç¤ºæ¥å®žçŽ°ï¼Œå…·ä½“çš„æç¤ºæ›´æ–°ç­–ç•¥æœªçŸ¥ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ç»†èŠ‚æœªçŸ¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

Motion4Dåœ¨å¤šä¸ªåœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚åœ¨ç‚¹äº‘è·Ÿè¸ªã€è§†é¢‘å¯¹è±¡åˆ†å‰²å’Œæ–°è§†è§’åˆæˆä»»åŠ¡ä¸­ï¼ŒMotion4Dæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰çš„2DåŸºç¡€æ¨¡åž‹å’Œ3Dæ–¹æ³•ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦å¼ºè°ƒäº†å…¶åœ¨å¤šç§ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œè¡¨æ˜Žäº†è¯¥æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®žç”¨ä»·å€¼ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

Motion4Dçš„ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªã€å¢žå¼ºçŽ°å®žç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›3Dä¸€è‡´çš„åŠ¨æ€åœºæ™¯ç†è§£ï¼Œå¯ä»¥æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„çŽ¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¢žå¼ºæœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œå¹¶ä¸ºARåº”ç”¨æä¾›æ›´é€¼çœŸçš„åœºæ™¯äº¤äº’ä½“éªŒã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºŽæŽ¨åŠ¨è™šæ‹ŸçŽ°å®žã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸçš„å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.

