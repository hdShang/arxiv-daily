---
layout: default
title: TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning
---

# TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning

**arXiv**: [2512.03963v2](https://arxiv.org/abs/2512.03963) | [PDF](https://arxiv.org/pdf/2512.03963.pdf)

**ä½œè€…**: Tao Wu, Li Yang, Gen Zhan, Yabin Zhang, Yiting Liao, Junlin Li, Deliang Fu, Li Zhang, Limin Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03 (æ›´æ–°: 2025-12-04)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTempR1ï¼Œé€šè¿‡æ—¶åºæ„ŸçŸ¥å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æå‡MLLMå¯¹é•¿è§†é¢‘çš„æ—¶åºç†è§£èƒ½åŠ›ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹` `æ—¶åºç†è§£` `å¼ºåŒ–å­¦ä¹ ` `å¤šä»»åŠ¡å­¦ä¹ ` `é•¿è§†é¢‘åˆ†æž`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ–¹æ³•åœ¨æå‡MLLMæ—¶åºç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦ä½“çŽ°åœ¨ä»»åŠ¡ç±»åž‹å’Œæ•°æ®æœ‰é™ï¼Œéš¾ä»¥æ³›åŒ–åˆ°å¤šæ ·åŒ–çš„æ—¶åºç†è§£åœºæ™¯ã€‚
2. TempR1çš„æ ¸å¿ƒåœ¨äºŽæž„å»ºæ—¶åºæ„ŸçŸ¥çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œé€šè¿‡å¤šä»»åŠ¡è¯­æ–™åº“å’Œå®šåˆ¶åŒ–å¥–åŠ±ï¼Œæå‡æ¨¡åž‹å¯¹ä¸åŒæ—¶åºæ¨¡å¼çš„é€‚åº”æ€§ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒTempR1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶ä¸”é€šè¿‡è”åˆä¼˜åŒ–å¢žå¼ºäº†æ³›åŒ–èƒ½åŠ›å’Œå•ä»»åŠ¡æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹(MLLM)å¯¹é•¿è§†é¢‘çš„æ—¶åºç†è§£èƒ½åŠ›ï¼Œä»Žè€ŒæŽ¨è¿›æ—¶åºå®šä½ã€åŠ¨ä½œæ£€æµ‹å’Œæ—¶é—´æ•æ„Ÿé—®ç­”ç­‰ä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºäº†TempR1ï¼Œä¸€ä¸ªæ—¶åºæ„ŸçŸ¥çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°å¢žå¼ºMLLMçš„æ—¶åºç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªå¤šä»»åŠ¡è¯­æ–™åº“ï¼Œä½¿æ¨¡åž‹èƒ½å¤ŸæŽ¥è§¦åˆ°ä¸åŒçš„æ—¶åºç»“æž„å’Œè¯­ä¹‰ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åŸºäºŽGroup Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œå®žçŽ°äº†ç¨³å®šæœ‰æ•ˆçš„è·¨ä»»åŠ¡ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ—¶åºä»»åŠ¡åˆ†ä¸ºé¢„æµ‹åŒºé—´å’ŒçœŸå®žå®žä¾‹ä¹‹é—´çš„ä¸‰ç§å¯¹åº”ç±»åž‹ï¼Œå¹¶ä¸ºæ¯ç§ç±»åž‹è®¾è®¡äº†å®šåˆ¶åŒ–çš„å®šä½å¥–åŠ±ï¼Œä½¿TempR1èƒ½å¤Ÿæ•èŽ·ç»†ç²’åº¦çš„æ—¶åºä¾èµ–å…³ç³»ï¼Œå¹¶é€‚åº”ä¸åŒçš„æ—¶åºæ¨¡å¼ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒTempR1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹äº’è¡¥ä»»åŠ¡çš„è”åˆä¼˜åŒ–äº§ç”Ÿäº†å¼ºå¤§çš„ååŒæ•ˆåº”ï¼Œå¢žå¼ºäº†æ³›åŒ–èƒ½åŠ›å’Œå•ä»»åŠ¡æ€§èƒ½ï¼Œä¸ºMLLMä¸­çš„æ—¶åºæŽ¨ç†å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æœ‰åŽŸåˆ™çš„èŒƒä¾‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰æ–¹æ³•åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰çš„æ—¶åºç†è§£èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦ä½“çŽ°åœ¨å®ƒä»¬é€šå¸¸åªå…³æ³¨æœ‰é™çš„ä»»åŠ¡ç±»åž‹å’Œæ•°æ®é›†ï¼Œå¯¼è‡´æ¨¡åž‹éš¾ä»¥æ³›åŒ–åˆ°æ›´å¹¿æ³›çš„æ—¶åºç†è§£åœºæ™¯ä¸­ã€‚è¿™äº›æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨ä¸åŒæ—¶åºä»»åŠ¡ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå¹¶ä¸”ç¼ºä¹é’ˆå¯¹ä¸åŒæ—¶åºæ¨¡å¼çš„ç»†ç²’åº¦ä¼˜åŒ–ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTempR1çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡è”åˆä¼˜åŒ–å¤šä¸ªå…·æœ‰ä¸åŒæ—¶åºç»“æž„å’Œè¯­ä¹‰çš„ä»»åŠ¡ï¼Œæ¥æå‡MLLMçš„æ—¶åºç†è§£èƒ½åŠ›ã€‚é€šè¿‡æž„å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–æ—¶åºä»»åŠ¡çš„è¯­æ–™åº“ï¼Œå¹¶è®¾è®¡é’ˆå¯¹ä¸åŒä»»åŠ¡çš„å®šåˆ¶åŒ–å¥–åŠ±å‡½æ•°ï¼ŒTempR1èƒ½å¤Ÿå¼•å¯¼æ¨¡åž‹å­¦ä¹ æ›´é²æ£’å’Œæ³›åŒ–çš„æ—¶åºè¡¨ç¤ºã€‚ è¿™ç§å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹å¼èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ä¸åŒä»»åŠ¡ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šTempR1çš„æ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šä»»åŠ¡è¯­æ–™åº“æž„å»ºæ¨¡å—ï¼Œç”¨äºŽæ”¶é›†å’Œæ•´ç†åŒ…å«ä¸åŒæ—¶åºç»“æž„å’Œè¯­ä¹‰çš„è§†é¢‘æ•°æ®ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºé€‚åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ ¼å¼ã€‚2) å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ï¼Œè¯¥æ¨¡å—åŸºäºŽGroup Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œç”¨äºŽè®­ç»ƒMLLMçš„æ—¶åºç†è§£èƒ½åŠ›ã€‚3) å¥–åŠ±å‡½æ•°è®¾è®¡æ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®ä¸åŒæ—¶åºä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œè®¾è®¡å®šåˆ¶åŒ–çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥å¼•å¯¼æ¨¡åž‹å­¦ä¹ æ­£ç¡®çš„æ—¶åºè¡Œä¸ºã€‚4) MLLMé›†æˆæ¨¡å—ï¼Œå°†è®­ç»ƒå¥½çš„æ—¶åºç†è§£èƒ½åŠ›é›†æˆåˆ°çŽ°æœ‰çš„MLLMä¸­ï¼Œä»Žè€Œæå‡å…¶åœ¨æ—¶åºç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTempR1çš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶æ—¶åºæ„ŸçŸ¥çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¡†æž¶ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTempR1èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§ä¸åŒç±»åž‹çš„æ—¶åºä»»åŠ¡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ ¹æ®ä¸åŒä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œè®¾è®¡å®šåˆ¶åŒ–çš„å¥–åŠ±å‡½æ•°ã€‚æ­¤å¤–ï¼ŒTempR1è¿˜é‡‡ç”¨äº†Group Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ä¸­çš„è´Ÿè¿ç§»é—®é¢˜ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„è®­ç»ƒæ•ˆçŽ‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šTempR1çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å°†æ—¶åºä»»åŠ¡åˆ†ä¸ºé¢„æµ‹åŒºé—´å’ŒçœŸå®žå®žä¾‹ä¹‹é—´çš„ä¸‰ç§å¯¹åº”ç±»åž‹ï¼Œå¹¶ä¸ºæ¯ç§ç±»åž‹è®¾è®¡äº†å®šåˆ¶åŒ–çš„å®šä½å¥–åŠ±ã€‚2) é‡‡ç”¨Group Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œä»¥å®žçŽ°ç¨³å®šæœ‰æ•ˆçš„è·¨ä»»åŠ¡ä¼˜åŒ–ã€‚3) æž„å»ºåŒ…å«å¤šæ ·åŒ–æ—¶åºç»“æž„å’Œè¯­ä¹‰çš„å¤šä»»åŠ¡è¯­æ–™åº“ã€‚4) ä½¿ç”¨é¢„è®­ç»ƒçš„MLLMä½œä¸ºåŸºç¡€æ¨¡åž‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ—¶åºç†è§£ä»»åŠ¡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

TempR1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œè¯æ˜Žäº†å…¶æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ—¶åºå®šä½ä»»åŠ¡ä¸Šï¼ŒTempR1çš„æ€§èƒ½æ¯”çŽ°æœ‰æœ€ä½³æ–¹æ³•æå‡äº†è¶…è¿‡5%ã€‚æ­¤å¤–ï¼Œå®žéªŒç»“æžœè¿˜è¡¨æ˜Žï¼ŒTempR1çš„è”åˆä¼˜åŒ–ç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œå•ä»»åŠ¡æ€§èƒ½ï¼ŒéªŒè¯äº†å¤šä»»åŠ¡å­¦ä¹ çš„ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

TempR1çš„ç ”ç©¶æˆæžœå¯å¹¿æ³›åº”ç”¨äºŽé•¿è§†é¢‘ç†è§£é¢†åŸŸï¼Œä¾‹å¦‚è§†é¢‘å†…å®¹åˆ†æžã€æ™ºèƒ½ç›‘æŽ§ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æå‡MLLMå¯¹è§†é¢‘æ—¶åºä¿¡æ¯çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å®žçŽ°æ›´ç²¾å‡†çš„äº‹ä»¶æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«å’Œå¼‚å¸¸è¡Œä¸ºé¢„è­¦ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥åº”ç”¨äºŽæ™ºèƒ½å®¢æœã€æ•™è‚²å¨±ä¹ç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ›´æ™ºèƒ½ã€æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.

