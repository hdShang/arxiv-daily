---
layout: default
title: DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos
---

# DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos

**arXiv**: [2512.14217v1](https://arxiv.org/abs/2512.14217) | [PDF](https://arxiv.org/pdf/2512.14217.pdf)

**ä½œè€…**: Yang Bai, Liudi Yang, George Eskandar, Fengyi Shen, Mohammad Altillawi, Ziyuan Liu, Gitta Kutyniok

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDRAW2ACTä»¥è§£å†³æœºå™¨äººæ¼”ç¤ºè§†é¢‘ç”Ÿæˆçš„å¯æŽ§æ€§é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `æœºå™¨äººæ¼”ç¤º` `å¤šæ¨¡æ€èžåˆ` `æ·±åº¦å­¦ä¹ ` `è½¨è¿¹æ¡ä»¶ç”Ÿæˆ`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çš„è½¨è¿¹æ¡ä»¶è§†é¢‘ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºŽäºŒç»´è½¨è¿¹æˆ–å•ä¸€æ¨¡æ€ï¼Œå¯¼è‡´ç”Ÿæˆçš„æœºå™¨äººæ¼”ç¤ºç¼ºä¹å¯æŽ§æ€§å’Œä¸€è‡´æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„DRAW2ACTæ¡†æž¶é€šè¿‡æå–æ·±åº¦ã€è¯­ä¹‰ã€å½¢çŠ¶å’Œè¿åŠ¨ç­‰å¤šç§æ­£äº¤è¡¨ç¤ºï¼Œå¢žå¼ºäº†è§†é¢‘ç”Ÿæˆçš„å¤šæ¨¡æ€ç‰¹æ€§ã€‚
3. åœ¨Bridge V2ã€Berkeley Autolabå’Œä»¿çœŸåŸºå‡†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼ŒDRAW2ACTåœ¨è§†è§‰è´¨é‡å’Œä¸€è‡´æ€§ä¸Šæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼Œå¹¶æé«˜äº†æ“ä½œæˆåŠŸçŽ‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†é¢‘æ‰©æ•£æ¨¡åž‹ä¸ºå…·èº«äººå·¥æ™ºèƒ½æä¾›äº†å¼ºå¤§çš„çŽ°å®žä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œä½†åœ¨æœºå™¨äººæ“ä½œçš„å¯æŽ§æ€§æ–¹é¢ä»ç„¶æœ‰é™ã€‚è¿‘æœŸçš„è½¨è¿¹æ¡ä»¶è§†é¢‘ç”Ÿæˆç ”ç©¶è™½ç„¶å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œä½†é€šå¸¸ä¾èµ–äºŽäºŒç»´è½¨è¿¹æˆ–å•ä¸€æ¨¡æ€æ¡ä»¶ï¼Œé™åˆ¶äº†å…¶ç”Ÿæˆå¯æŽ§ä¸”ä¸€è‡´çš„æœºå™¨äººæ¼”ç¤ºçš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†DRAW2ACTï¼Œä¸€ä¸ªæ·±åº¦æ„ŸçŸ¥çš„è½¨è¿¹æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¡†æž¶ï¼Œä»Žè¾“å…¥è½¨è¿¹ä¸­æå–å¤šä¸ªæ­£äº¤è¡¨ç¤ºï¼Œæ•æ‰æ·±åº¦ã€è¯­ä¹‰ã€å½¢çŠ¶å’Œè¿åŠ¨ï¼Œå¹¶å°†å…¶æ³¨å…¥æ‰©æ•£æ¨¡åž‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºè”åˆç”Ÿæˆç©ºé—´å¯¹é½çš„RGBå’Œæ·±åº¦è§†é¢‘ï¼Œåˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„æœºåˆ¶å’Œæ·±åº¦ç›‘ç£æ¥å¢žå¼ºæ—¶ç©ºä¸€è‡´æ€§ã€‚æœ€åŽï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªå¤šæ¨¡æ€ç­–ç•¥æ¨¡åž‹ï¼ŒåŸºäºŽç”Ÿæˆçš„RGBå’Œæ·±åº¦åºåˆ—å›žå½’æœºå™¨äººçš„å…³èŠ‚è§’åº¦ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒDRAW2ACTåœ¨è§†è§‰ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºŽçŽ°æœ‰åŸºçº¿ï¼ŒåŒæ—¶æé«˜äº†æ“ä½œæˆåŠŸçŽ‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çŽ°æœ‰è½¨è¿¹æ¡ä»¶è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨æœºå™¨äººæ¼”ç¤ºä¸­çš„å¯æŽ§æ€§å’Œä¸€è‡´æ€§ä¸è¶³çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•å¤šä¾èµ–äºŽäºŒç»´è½¨è¿¹æˆ–å•ä¸€æ¨¡æ€ï¼Œå¯¼è‡´ç”Ÿæˆç»“æžœçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDRAW2ACTæ¡†æž¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æ·±åº¦æ„ŸçŸ¥çš„è½¨è¿¹æ¡ä»¶ç”Ÿæˆè§†é¢‘ï¼Œæå–å¤šç§æ­£äº¤è¡¨ç¤ºï¼ˆå¦‚æ·±åº¦ã€è¯­ä¹‰ã€å½¢çŠ¶å’Œè¿åŠ¨ï¼‰ï¼Œå¹¶å°†å…¶æ³¨å…¥åˆ°æ‰©æ•£æ¨¡åž‹ä¸­ï¼Œä»¥å¢žå¼ºç”Ÿæˆè§†é¢‘çš„å¯æŽ§æ€§å’Œä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šè¯¥æ¡†æž¶åŒ…æ‹¬å¤šä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä»Žè¾“å…¥è½¨è¿¹ä¸­æå–å¤šæ¨¡æ€ç‰¹å¾ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„æœºåˆ¶ç”Ÿæˆç©ºé—´å¯¹é½çš„RGBå’Œæ·±åº¦è§†é¢‘ï¼›æœ€åŽï¼ŒåŸºäºŽç”Ÿæˆçš„è§†é¢‘åºåˆ—å›žå½’æœºå™¨äººçš„å…³èŠ‚è§’åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽè”åˆç”ŸæˆRGBå’Œæ·±åº¦è§†é¢‘ï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€æ³¨æ„æœºåˆ¶å’Œæ·±åº¦ç›‘ç£æ¥å¢žå¼ºæ—¶ç©ºä¸€è‡´æ€§ã€‚è¿™ä¸€è®¾è®¡ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽå¤šæ¨¡æ€èžåˆçš„æ·±åº¦æ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†æ·±åº¦ç›‘ç£æŸå¤±å‡½æ•°ä»¥æå‡æ·±åº¦ä¿¡æ¯çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶è®¾è®¡äº†ç‰¹å®šçš„ç½‘ç»œç»“æž„ä»¥æ”¯æŒå¤šæ¨¡æ€ç‰¹å¾çš„æå–å’Œèžåˆã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒDRAW2ACTåœ¨è§†è§‰ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰åŸºçº¿ï¼Œå…·ä½“è¡¨çŽ°ä¸ºåœ¨æ“ä½œæˆåŠŸçŽ‡ä¸Šæé«˜äº†XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•çŽ°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼ŒDRAW2ACTèƒ½å¤Ÿä¸ºå®žé™…åº”ç”¨æä¾›æ›´é«˜çš„çµæ´»æ€§å’Œå¯é æ€§ï¼ŒæŽ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.

