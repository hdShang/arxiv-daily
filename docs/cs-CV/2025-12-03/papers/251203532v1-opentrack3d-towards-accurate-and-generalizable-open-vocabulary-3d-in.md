---
layout: default
title: "OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation"
---

# OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.03532" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.03532v1</a>
  <a href="https://arxiv.org/pdf/2512.03532.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03532v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.03532v1', 'OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhishan Zhou, Siyuan Wei, Zengran Wang, Chunjie Wang, Xiaosheng Yan, Xiao Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OpenTrack3Dï¼šé¢å‘ç²¾ç¡®å’Œæ³›åŒ–çš„å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²` `æ— ç½‘æ ¼æ–¹æ³•` `è§†è§‰-ç©ºé—´è·Ÿè¸ª` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æœºå™¨äºº` `AR/VR` `DINOç‰¹å¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²æ–¹æ³•ä¾èµ–æ•°æ®é›†ç‰¹å®šproposalç½‘ç»œæˆ–ç½‘æ ¼ï¼Œæ³›åŒ–æ€§å—é™ï¼Œä¸”CLIPæ–‡æœ¬æ¨ç†èƒ½åŠ›å¼±ï¼Œéš¾ä»¥å¤„ç†å¤æ‚æŸ¥è¯¢ã€‚
2. OpenTrack3Dæå‡ºä¸€ç§æ–°é¢–çš„è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨ï¼Œåœ¨çº¿æ„å»ºè·¨è§†å›¾ä¸€è‡´çš„å¯¹è±¡proposalï¼Œå¹¶ç”¨MLLMå¢å¼ºç»„åˆæ¨ç†èƒ½åŠ›ã€‚
3. åœ¨ScanNet200ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒOpenTrack3Då–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°†å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²ï¼ˆOV-3DISï¼‰æ¨å¹¿åˆ°å¤šæ ·ã€éç»“æ„åŒ–å’Œæ— ç½‘æ ¼ç¯å¢ƒå¯¹äºæœºå™¨äººå’ŒAR/VRè‡³å…³é‡è¦ï¼Œä½†ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™å½’å› äºç°æœ‰æ–¹æ³•çš„ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šï¼ˆ1ï¼‰proposalç”Ÿæˆä¾èµ–äºæ•°æ®é›†ç‰¹å®šçš„proposalç½‘ç»œæˆ–åŸºäºç½‘æ ¼çš„è¶…ç‚¹ï¼Œä½¿å…¶ä¸é€‚ç”¨äºæ— ç½‘æ ¼åœºæ™¯ï¼Œå¹¶é™åˆ¶äº†å¯¹æ–°åœºæ™¯çš„æ³›åŒ–ï¼›ï¼ˆ2ï¼‰åŸºäºCLIPçš„åˆ†ç±»å™¨åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œéš¾ä»¥è¯†åˆ«ç»„åˆå’ŒåŠŸèƒ½æ€§ç”¨æˆ·æŸ¥è¯¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OpenTrack3Dï¼Œä¸€ä¸ªé€šç”¨ä¸”ç²¾ç¡®çš„æ¡†æ¶ã€‚ä¸ä¾èµ–äºé¢„ç”Ÿæˆproposalçš„æ–¹æ³•ä¸åŒï¼ŒOpenTrack3Dé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨æ¥åœ¨çº¿æ„å»ºè·¨è§†å›¾ä¸€è‡´çš„å¯¹è±¡proposalã€‚ç»™å®šRGB-Dæµï¼Œæˆ‘ä»¬çš„pipelineé¦–å…ˆåˆ©ç”¨2Då¼€æ”¾è¯æ±‡åˆ†å‰²å™¨ç”Ÿæˆmaskï¼Œç„¶åä½¿ç”¨æ·±åº¦ä¿¡æ¯å°†å…¶æå‡åˆ°3Dç‚¹äº‘ã€‚ç„¶åä½¿ç”¨DINOç‰¹å¾å›¾æå–maskå¼•å¯¼çš„å®ä¾‹ç‰¹å¾ï¼Œæˆ‘ä»¬çš„è·Ÿè¸ªå™¨èåˆè§†è§‰å’Œç©ºé—´çº¿ç´¢ä»¥ä¿æŒå®ä¾‹ä¸€è‡´æ€§ã€‚æ ¸å¿ƒpipelineå®Œå…¨æ˜¯æ— ç½‘æ ¼çš„ï¼Œä½†æˆ‘ä»¬ä¹Ÿæä¾›äº†ä¸€ä¸ªå¯é€‰çš„è¶…ç‚¹ç»†åŒ–æ¨¡å—ï¼Œä»¥åœ¨åœºæ™¯ç½‘æ ¼å¯ç”¨æ—¶è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ›¿æ¢CLIPï¼Œæ˜¾è‘—å¢å¼ºäº†å¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚åœ¨åŒ…æ‹¬ScanNet200ã€Replicaã€ScanNet++å’ŒSceneFun3Dåœ¨å†…çš„å„ç§benchmarkä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²ï¼ˆOV-3DISï¼‰åœ¨å¤šæ ·ã€éç»“æ„åŒ–å’Œæ— ç½‘æ ¼ç¯å¢ƒä¸‹çš„æ³›åŒ–æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ•°æ®é›†ç‰¹å®šçš„proposalç½‘ç»œæˆ–åŸºäºç½‘æ ¼çš„è¶…ç‚¹ï¼Œå¯¼è‡´æ— æ³•åº”ç”¨äºæ— ç½‘æ ¼åœºæ™¯ï¼Œå¹¶ä¸”åŸºäºCLIPçš„åˆ†ç±»å™¨åœ¨å¤„ç†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢æ—¶è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOpenTrack3Dçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸€ä¸ªè§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨åœ¨çº¿ç”Ÿæˆè·¨è§†å›¾ä¸€è‡´çš„å¯¹è±¡proposalï¼Œé¿å…äº†å¯¹é¢„å®šä¹‰proposalçš„ä¾èµ–ã€‚åŒæ—¶ï¼Œä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ›¿æ¢CLIPï¼Œä»¥å¢å¼ºå¯¹å¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOpenTrack3Dçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨2Då¼€æ”¾è¯æ±‡åˆ†å‰²å™¨ä»RGB-Dæµä¸­ç”Ÿæˆmaskï¼›2) åˆ©ç”¨æ·±åº¦ä¿¡æ¯å°†2D maskæå‡åˆ°3Dç‚¹äº‘ï¼›3) ä½¿ç”¨DINOç‰¹å¾å›¾æå–maskå¼•å¯¼çš„å®ä¾‹ç‰¹å¾ï¼›4) ä½¿ç”¨è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨èåˆè§†è§‰å’Œç©ºé—´çº¿ç´¢ï¼Œä¿æŒå®ä¾‹ä¸€è‡´æ€§ï¼›5) (å¯é€‰) ä½¿ç”¨è¶…ç‚¹ç»†åŒ–æ¨¡å—è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼ˆå½“åœºæ™¯ç½‘æ ¼å¯ç”¨æ—¶ï¼‰ï¼›6) ä½¿ç”¨MLLMè¿›è¡Œæœ€ç»ˆçš„å®ä¾‹åˆ†ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šOpenTrack3Dçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªæ— ç½‘æ ¼çš„è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨ï¼Œèƒ½å¤Ÿåœ¨çº¿ç”Ÿæˆé«˜è´¨é‡çš„å¯¹è±¡proposalï¼Œé¿å…äº†å¯¹é¢„å®šä¹‰proposalçš„ä¾èµ–ï¼Œä»è€Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼›2) ä½¿ç”¨MLLMæ›¿æ¢CLIPï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹å¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨èåˆäº†è§†è§‰ç‰¹å¾ï¼ˆDINOç‰¹å¾ï¼‰å’Œç©ºé—´ä¿¡æ¯ï¼ˆç‚¹äº‘åæ ‡ï¼‰ï¼Œé€šè¿‡å¡å°”æ›¼æ»¤æ³¢ç­‰æ–¹æ³•è¿›è¡Œè·Ÿè¸ªå’Œæ›´æ–°ã€‚MLLMçš„ä½¿ç”¨æ¶‰åŠpromptå·¥ç¨‹å’Œå¾®è°ƒç­–ç•¥ï¼Œä»¥é€‚åº”3Då®ä¾‹åˆ†å‰²ä»»åŠ¡ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡å¯èƒ½åŒ…æ‹¬åˆ†å‰²æŸå¤±ã€è·Ÿè¸ªæŸå¤±å’Œåˆ†ç±»æŸå¤±ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OpenTrack3Dåœ¨ScanNet200ã€Replicaã€ScanNet++å’ŒSceneFun3Dç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶åœ¨å¤æ‚åœºæ™¯å’Œç”¨æˆ·æŸ¥è¯¢ä¸‹çš„æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OpenTrack3Dåœ¨æœºå™¨äººã€AR/VRç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯åœ¨æœªçŸ¥ç¯å¢ƒä¸­è¯†åˆ«å’Œåˆ†å‰²ç‰©ä½“ï¼Œä»è€Œå®ç°è‡ªä¸»å¯¼èˆªå’Œæ“ä½œã€‚åœ¨AR/VRä¸­ï¼Œè¯¥æŠ€æœ¯å¯ä»¥ç”¨äºå¢å¼ºç°å®ä½“éªŒï¼Œä¾‹å¦‚å…è®¸ç”¨æˆ·ä¸è™šæ‹Ÿç‰©ä½“è¿›è¡Œäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.

