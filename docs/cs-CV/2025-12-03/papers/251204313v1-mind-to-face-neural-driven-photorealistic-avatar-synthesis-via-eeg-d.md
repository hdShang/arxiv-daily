---
layout: default
title: Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding
---

# Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.04313" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.04313v1</a>
  <a href="https://arxiv.org/pdf/2512.04313.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04313v1" onclick="toggleFavorite(this, '2512.04313v1', 'Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haolin Xiong, Tianwen Fu, Pratusha Bhuvana Prasad, Yunxuan Cai, Haiwei Chen, Wenbin Teng, Hanyuan Xiao, Yajie Zhao

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

**å¤‡æ³¨**: 16 pages, 11 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Mind-to-Faceï¼šé¦–ä¸ªåŸºäºè„‘ç”µä¿¡å·è§£ç çš„é€¼çœŸäººè„¸Avatarç”Ÿæˆæ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è„‘æœºæ¥å£` `äººè„¸Avatar` `è„‘ç”µä¿¡å·è§£ç ` `3Dé«˜æ–¯æº…å°„` `æƒ…æ„Ÿè¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰Avatarç³»ç»Ÿä¸¥é‡ä¾èµ–è§†è§‰çº¿ç´¢ï¼Œåœ¨é¢éƒ¨è¢«é®æŒ¡æˆ–æƒ…ç»ªå†…æ•›æ—¶å¤±æ•ˆï¼Œæ— æ³•å‡†ç¡®æ•æ‰å†…åœ¨æƒ…æ„Ÿã€‚
2. Mind-to-Faceé€šè¿‡CNN-Transformerå°†è„‘ç”µä¿¡å·è§£ç ä¸ºé«˜ç²¾åº¦3Dé¢éƒ¨æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨3Dé«˜æ–¯æº…å°„æ¸²æŸ“é€¼çœŸAvatarã€‚
3. å®éªŒè¯æ˜ï¼Œä»…ä½¿ç”¨è„‘ç”µä¿¡å·å³å¯é¢„æµ‹ä¸ªä½“åŒ–çš„åŠ¨æ€é¢éƒ¨è¡¨æƒ…ï¼ŒåŒ…æ‹¬ç»†å¾®çš„æƒ…ç»ªååº”ï¼Œæ•ˆæœæ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMind-to-Faceï¼Œæ˜¯é¦–ä¸ªå°†éä¾µå…¥å¼è„‘ç”µå›¾(EEG)ä¿¡å·ç›´æ¥è§£ç ä¸ºé«˜ä¿çœŸé¢éƒ¨è¡¨æƒ…çš„æ¡†æ¶ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒæ¨¡æ€è®°å½•ç³»ç»Ÿï¼Œåœ¨è¯±å‘æƒ…ç»ªçš„åˆºæ¿€ä¸‹ï¼ŒåŒæ­¥è·å–EEGå’Œå¤šè§†è§’é¢éƒ¨è§†é¢‘ï¼Œä»è€Œä¸ºç¥ç»-è§†è§‰å­¦ä¹ æä¾›ç²¾ç¡®çš„ç›‘ç£ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨CNN-Transformerç¼–ç å™¨å°†EEGä¿¡å·æ˜ å°„åˆ°å¯†é›†çš„3Dä½ç½®å›¾ï¼Œèƒ½å¤Ÿé‡‡æ ·è¶…è¿‡65kä¸ªé¡¶ç‚¹ï¼Œæ•æ‰ç²¾ç»†çš„å‡ ä½•ç»“æ„å’Œå¾®å¦™çš„æƒ…ç»ªåŠ¨æ€ï¼Œå¹¶é€šè¿‡æ”¹è¿›çš„3Dé«˜æ–¯æº…å°„æ¸²æŸ“ç®¡çº¿ç”Ÿæˆé€¼çœŸä¸”è§†è§’ä¸€è‡´çš„ç»“æœã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜ä»…å‡­EEGå°±èƒ½å¯é åœ°é¢„æµ‹åŠ¨æ€çš„ã€ä¸ªä½“åŒ–çš„é¢éƒ¨è¡¨æƒ…ï¼ŒåŒ…æ‹¬å¾®å¦™çš„æƒ…ç»ªååº”ï¼Œè¡¨æ˜ç¥ç»ä¿¡å·åŒ…å«æ¯”ä¹‹å‰è®¤ä¸ºçš„æ›´ä¸°å¯Œçš„æƒ…æ„Ÿå’Œå‡ ä½•ä¿¡æ¯ã€‚Mind-to-Faceä¸ºç¥ç»é©±åŠ¨çš„Avatarå»ºç«‹äº†ä¸€ä¸ªæ–°çš„èŒƒä¾‹ï¼Œèƒ½å¤Ÿåœ¨æ²‰æµ¸å¼ç¯å¢ƒä¸­å®ç°ä¸ªæ€§åŒ–çš„ã€æƒ…æ„Ÿæ„ŸçŸ¥çš„è¿œç¨‹å‘ˆç°å’Œè®¤çŸ¥äº¤äº’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººè„¸Avatarç”Ÿæˆç³»ç»Ÿä¸»è¦ä¾èµ–äºè§†è§‰ä¿¡æ¯ï¼Œä¾‹å¦‚é¢éƒ¨å›¾åƒæˆ–è§†é¢‘ã€‚å½“é¢éƒ¨è¢«é®æŒ¡ï¼Œæˆ–è€…äººä»¬è¯•å›¾éšè—è‡ªå·±çš„æƒ…ç»ªæ—¶ï¼Œè¿™äº›ç³»ç»Ÿå°±æ— æ³•å‡†ç¡®åœ°æ•æ‰åˆ°çœŸå®çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚å› æ­¤ï¼Œå¦‚ä½•ä»…é€šè¿‡éä¾µå…¥å¼ç¥ç»ä¿¡å·ï¼ˆå¦‚è„‘ç”µå›¾EEGï¼‰æ¥é©±åŠ¨é€¼çœŸçš„äººè„¸Avatarï¼Œæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è„‘ç”µä¿¡å·ç›´æ¥æ˜ å°„åˆ°é«˜ç²¾åº¦çš„3Dé¢éƒ¨æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨3Dé«˜æ–¯æº…å°„æŠ€æœ¯è¿›è¡Œæ¸²æŸ“ï¼Œä»è€Œç”Ÿæˆé€¼çœŸçš„äººè„¸Avatarã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ï¼Œå¯ä»¥ç›´æ¥åæ˜ ä¸ªä½“çš„æƒ…ç»ªçŠ¶æ€å’Œå†…åœ¨æƒ³æ³•ã€‚é€šè¿‡å»ºç«‹EEGä¿¡å·ä¸é¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„ç›´æ¥è”ç³»ï¼Œå¯ä»¥å®ç°æ›´åŠ è‡ªç„¶å’Œä¸ªæ€§åŒ–çš„Avataræ§åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMind-to-Faceæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) **åŒæ¨¡æ€æ•°æ®é‡‡é›†**ï¼šåŒæ­¥è®°å½•EEGä¿¡å·å’Œå¤šè§†è§’é¢éƒ¨è§†é¢‘ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ã€‚2) **CNN-Transformerç¼–ç å™¨**ï¼šå°†EEGä¿¡å·ç¼–ç ä¸ºé«˜ç»´ç‰¹å¾å‘é‡ã€‚3) **3Dä½ç½®å›¾ç”Ÿæˆå™¨**ï¼šå°†ç‰¹å¾å‘é‡æ˜ å°„åˆ°å¯†é›†çš„3Dä½ç½®å›¾ï¼Œè¯¥ä½ç½®å›¾åŒ…å«è¶…è¿‡65kä¸ªé¡¶ç‚¹ï¼Œèƒ½å¤Ÿæ•æ‰ç²¾ç»†çš„é¢éƒ¨å‡ ä½•ç»“æ„ã€‚4) **3Dé«˜æ–¯æº…å°„æ¸²æŸ“ç®¡çº¿**ï¼šå°†3Dä½ç½®å›¾æ¸²æŸ“æˆé€¼çœŸä¸”è§†è§’ä¸€è‡´çš„äººè„¸å›¾åƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **é¦–æ¬¡æå‡ºåŸºäºEEGä¿¡å·ç›´æ¥ç”Ÿæˆé€¼çœŸäººè„¸Avatarçš„æ¡†æ¶**ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ã€‚2) **åˆ©ç”¨CNN-Transformerç»“æ„æœ‰æ•ˆæå–EEGä¿¡å·ä¸­çš„æƒ…æ„Ÿå’Œå‡ ä½•ä¿¡æ¯**ã€‚3) **é‡‡ç”¨æ”¹è¿›çš„3Dé«˜æ–¯æº…å°„æ¸²æŸ“ç®¡çº¿ï¼Œå®ç°äº†é«˜è´¨é‡çš„Avataræ¸²æŸ“æ•ˆæœ**ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼Œé‡‡ç”¨äº†CNN-Transformeræ··åˆç»“æ„ï¼ŒCNNç”¨äºæå–å±€éƒ¨ç‰¹å¾ï¼ŒTransformerç”¨äºæ•æ‰å…¨å±€ä¾èµ–å…³ç³»ã€‚æŸå¤±å‡½æ•°æ–¹é¢ï¼Œä½¿ç”¨äº†L1æŸå¤±å’Œæ„ŸçŸ¥æŸå¤±ï¼Œä»¥ä¿è¯ç”Ÿæˆçš„äººè„¸å›¾åƒçš„é€¼çœŸåº¦å’Œç»†èŠ‚ã€‚åœ¨3Dé«˜æ–¯æº…å°„æ¸²æŸ“ç®¡çº¿ä¸­ï¼Œå¯¹é«˜æ–¯åˆ†å¸ƒçš„å‚æ•°è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æé«˜æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMind-to-Faceèƒ½å¤Ÿä»…å‡­EEGä¿¡å·å¯é åœ°é¢„æµ‹åŠ¨æ€çš„ã€ä¸ªä½“åŒ–çš„é¢éƒ¨è¡¨æƒ…ï¼ŒåŒ…æ‹¬å¾®å¦™çš„æƒ…ç»ªååº”ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒMind-to-Faceåœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«çš„å‡†ç¡®æ€§å’ŒAvataræ¸²æŸ“çš„é€¼çœŸåº¦æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ä¸»è§‚è¯„ä¼°ä¸­ï¼Œç”¨æˆ·æ™®éè®¤ä¸ºMind-to-Faceç”Ÿæˆçš„Avataræ›´åŠ è‡ªç„¶å’Œå¯Œæœ‰è¡¨ç°åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Mind-to-FaceæŠ€æœ¯åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿œç¨‹å‘ˆç°å’Œè™šæ‹Ÿä¼šè®®ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æˆ·çš„Avatarèƒ½å¤ŸçœŸå®åœ°åæ˜ å…¶æƒ…ç»ªçŠ¶æ€ï¼Œä»è€Œå¢å¼ºæ²Ÿé€šçš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚åœ¨æ¸¸æˆå’Œå¨±ä¹é¢†åŸŸï¼Œå¯ä»¥åˆ›å»ºæ›´åŠ ä¸ªæ€§åŒ–å’Œæ²‰æµ¸å¼çš„è§’è‰²ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºç¥ç»åé¦ˆæ²»ç–—å’Œè®¤çŸ¥åº·å¤ï¼Œå¸®åŠ©æ‚£è€…æ›´å¥½åœ°äº†è§£å’Œæ§åˆ¶è‡ªå·±çš„æƒ…ç»ªã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.

