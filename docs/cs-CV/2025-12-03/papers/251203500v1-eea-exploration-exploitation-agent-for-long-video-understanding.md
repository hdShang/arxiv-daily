---
layout: default
title: EEA: Exploration-Exploitation Agent for Long Video Understanding
---

# EEA: Exploration-Exploitation Agent for Long Video Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.03500" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.03500v1</a>
  <a href="https://arxiv.org/pdf/2512.03500.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03500v1" onclick="toggleFavorite(this, '2512.03500v1', 'EEA: Exploration-Exploitation Agent for Long Video Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Te Yang, Xiangyu Zhu, Bo Wang, Quan Chen, Peng Jiang, Zhen Lei

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEEAï¼šä¸€ç§ç”¨äºé•¿è§†é¢‘ç†è§£çš„æ¢ç´¢-åˆ©ç”¨æ™ºèƒ½ä½“æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `é•¿è§†é¢‘ç†è§£` `æ¢ç´¢-åˆ©ç”¨` `è¯­ä¹‰å¼•å¯¼` `åˆ†å±‚æ ‘æœç´¢` `è§†è§‰è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é•¿è§†é¢‘ç†è§£æ–¹æ³•åœ¨è®¡ç®—å¼€é”€å’Œæ¢ç´¢-åˆ©ç”¨å¹³è¡¡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œä¿¡æ¯è¦†ç›–ä¸å®Œæ•´ã€‚
2. EEAé€šè¿‡è¯­ä¹‰å¼•å¯¼çš„åˆ†å±‚æ ‘æœç´¢ï¼Œè‡ªä¸»å‘ç°å¹¶åŠ¨æ€æ›´æ–°ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰æŸ¥è¯¢ï¼Œå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒEEAåœ¨å¤šä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é•¿è§†é¢‘ç†è§£éœ€è¦é«˜æ•ˆåœ°å¯¼èˆªå¤§é‡çš„è§†è§‰æ•°æ®ï¼Œä»¥ç²¾ç¡®å®šä½ç¨€ç–ä½†å…³é”®çš„ä¿¡æ¯ã€‚ç›®å‰çš„æ–¹æ³•è¦ä¹ˆç”±äºå¯†é›†çš„é¢„å¤„ç†è€Œå¯¼è‡´ä¸¥é‡çš„è®¡ç®—å¼€é”€ï¼Œè¦ä¹ˆæ— æ³•æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä»è€Œå¯¼è‡´ä¿¡æ¯è¦†ç›–ä¸å®Œæ•´å’Œæ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘æ™ºèƒ½ä½“æ¡†æ¶EEAï¼Œé€šè¿‡å…·æœ‰åˆ†å±‚æ ‘æœç´¢è¿‡ç¨‹çš„è¯­ä¹‰æŒ‡å¯¼æ¥å®ç°æ¢ç´¢-åˆ©ç”¨çš„å¹³è¡¡ã€‚EEAè‡ªä¸»å‘ç°å¹¶åŠ¨æ€æ›´æ–°ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰æŸ¥è¯¢ï¼Œå¹¶æ”¶é›†ä¸è¿™äº›æŸ¥è¯¢ç´§å¯†åŒ¹é…çš„è§†é¢‘å¸§ä½œä¸ºè¯­ä¹‰é”šç‚¹ã€‚åœ¨æ ‘æœç´¢è¿‡ç¨‹ä¸­ï¼ŒEEAä¼˜å…ˆæ¢ç´¢è¯­ä¹‰ç›¸å…³çš„å¸§ï¼ŒåŒæ—¶ç¡®ä¿åœ¨æœªçŸ¥ç‰‡æ®µå†…æœ‰è¶³å¤Ÿçš„è¦†ç›–ï¼Œè€Œä¸æ˜¯ç»Ÿä¸€æ‰©å±•ã€‚æ­¤å¤–ï¼ŒEEAé€šè¿‡æ˜¾å¼å»ºæ¨¡ä¸ç¡®å®šæ€§ï¼Œè‡ªé€‚åº”åœ°å°†æ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å†…åœ¨å¥–åŠ±ä¸è¯­ä¹‰å…ˆéªŒç›¸ç»“åˆï¼Œä»¥å®ç°å¯¹è§†é¢‘ç‰‡æ®µçš„ç¨³å®šå’Œç²¾ç¡®è¯„ä¼°ã€‚åœ¨å„ç§é•¿è§†é¢‘åŸºå‡†ä¸Šçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šé•¿è§†é¢‘ç†è§£ä»»åŠ¡é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨æµ·é‡è§†é¢‘æ•°æ®ä¸­é«˜æ•ˆåœ°å®šä½å…³é”®ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ï¼šä¸€æ˜¯è¿›è¡Œå¯†é›†çš„é¢„å¤„ç†ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å·¨å¤§ï¼›äºŒæ˜¯æ— æ³•æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢ï¼ˆå¯»æ‰¾æ–°çš„ä¿¡æ¯ï¼‰å’Œåˆ©ç”¨ï¼ˆåˆ©ç”¨å·²çŸ¥ä¿¡æ¯ï¼‰ï¼Œå¯¼è‡´ä¿¡æ¯è¦†ç›–ä¸å®Œæ•´ï¼Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEEAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¯­ä¹‰å¼•å¯¼çš„æ¢ç´¢-åˆ©ç”¨ç­–ç•¥ï¼Œåœ¨é•¿è§†é¢‘ä¸­é«˜æ•ˆåœ°å®šä½å…³é”®ä¿¡æ¯ã€‚å®ƒåˆ©ç”¨è¯­ä¹‰æŸ¥è¯¢ä½œä¸ºæŒ‡å¯¼ï¼Œä¼˜å…ˆæ¢ç´¢ä¸ä»»åŠ¡ç›¸å…³çš„è§†é¢‘å¸§ï¼ŒåŒæ—¶ä¿è¯å¯¹æœªçŸ¥åŒºåŸŸçš„å……åˆ†è¦†ç›–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒEEAèƒ½å¤Ÿåœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæœ€å¤§åŒ–ä¿¡æ¯è·å–çš„æ•ˆç‡å’Œå®Œæ•´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEEAçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **è¯­ä¹‰æŸ¥è¯¢ç”Ÿæˆæ¨¡å—**ï¼šè‡ªä¸»å‘ç°å¹¶åŠ¨æ€æ›´æ–°ä¸ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰æŸ¥è¯¢ã€‚2) **è¯­ä¹‰é”šç‚¹æ„å»ºæ¨¡å—**ï¼šæ”¶é›†ä¸è¯­ä¹‰æŸ¥è¯¢ç´§å¯†åŒ¹é…çš„è§†é¢‘å¸§ä½œä¸ºè¯­ä¹‰é”šç‚¹ã€‚3) **åˆ†å±‚æ ‘æœç´¢æ¨¡å—**ï¼šåœ¨è§†é¢‘ä¸­è¿›è¡Œåˆ†å±‚æ ‘æœç´¢ï¼Œä¼˜å…ˆæ¢ç´¢è¯­ä¹‰ç›¸å…³çš„å¸§ï¼ŒåŒæ—¶ä¿è¯å¯¹æœªçŸ¥åŒºåŸŸçš„è¦†ç›–ã€‚4) **å¥–åŠ±è¯„ä¼°æ¨¡å—**ï¼šè‡ªé€‚åº”åœ°ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å†…åœ¨å¥–åŠ±å’Œè¯­ä¹‰å…ˆéªŒï¼Œå¯¹è§†é¢‘ç‰‡æ®µè¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šEEAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ¢ç´¢-åˆ©ç”¨çš„å¹³è¡¡ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„å‡åŒ€æ¢ç´¢æˆ–è´ªå©ªåˆ©ç”¨æ–¹æ³•ä¸åŒï¼ŒEEAé€šè¿‡è¯­ä¹‰å¼•å¯¼ï¼Œèƒ½å¤Ÿæ›´åŠ æ™ºèƒ½åœ°é€‰æ‹©æ¢ç´¢åŒºåŸŸï¼Œä»è€Œæé«˜ä¿¡æ¯è·å–çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒEEAè¿˜é€šè¿‡æ˜¾å¼å»ºæ¨¡ä¸ç¡®å®šæ€§ï¼Œå®ç°äº†å¯¹è§†é¢‘ç‰‡æ®µçš„ç¨³å®šå’Œç²¾ç¡®è¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šEEAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **è¯­ä¹‰æŸ¥è¯¢çš„è¡¨ç¤ºå’Œæ›´æ–°æ–¹å¼**ï¼šå…·ä½“å¦‚ä½•è¡¨ç¤ºè¯­ä¹‰æŸ¥è¯¢ï¼Œä»¥åŠå¦‚ä½•æ ¹æ®å·²æ¢ç´¢çš„ä¿¡æ¯åŠ¨æ€æ›´æ–°æŸ¥è¯¢ã€‚2) **åˆ†å±‚æ ‘æœç´¢çš„ç­–ç•¥**ï¼šå¦‚ä½•è®¾è®¡æ ‘çš„ç»“æ„å’Œæœç´¢ç®—æ³•ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ¢ç´¢å’Œåˆ©ç”¨ã€‚3) **å¥–åŠ±å‡½æ•°çš„æ„å»º**ï¼šå¦‚ä½•ç»“åˆVLMçš„å†…åœ¨å¥–åŠ±å’Œè¯­ä¹‰å…ˆéªŒï¼Œå¹¶è€ƒè™‘ä¸ç¡®å®šæ€§ï¼Œæ¥æ„å»ºä¸€ä¸ªç¨³å®šå’Œç²¾ç¡®çš„å¥–åŠ±å‡½æ•°ã€‚è¿™äº›ç»†èŠ‚å†³å®šäº†EEAçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEEAåœ¨å¤šä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨XXXæ•°æ®é›†ä¸Šï¼ŒEEAçš„æ€§èƒ½æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†X%ã€‚æ­¤å¤–ï¼ŒEEAè¿˜å±•ç°å‡ºæ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œåœ¨è¾¾åˆ°ç›¸åŒæ€§èƒ½æ°´å¹³çš„æƒ…å†µä¸‹ï¼Œæ‰€éœ€çš„è®¡ç®—èµ„æºæ›´å°‘ã€‚è¿™äº›ç»“æœéªŒè¯äº†EEAçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

EEAåœ¨é•¿è§†é¢‘ç†è§£é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚è§†é¢‘æ‘˜è¦ã€è§†é¢‘æ£€ç´¢ã€æ™ºèƒ½ç›‘æ§ã€æ•™è‚²è§†é¢‘åˆ†æç­‰ã€‚é€šè¿‡é«˜æ•ˆåœ°å®šä½å…³é”®ä¿¡æ¯ï¼ŒEEAå¯ä»¥å¸®åŠ©ç”¨æˆ·å¿«é€Ÿç†è§£é•¿è§†é¢‘çš„å†…å®¹ï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œå¹¶ä¸ºç›¸å…³é¢†åŸŸçš„æ™ºèƒ½åŒ–åº”ç”¨æä¾›æŠ€æœ¯æ”¯æŒã€‚æœªæ¥ï¼ŒEEAè¿˜å¯ä»¥åº”ç”¨äºæ›´å¤æ‚çš„è§†é¢‘åˆ†æä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘æ•…äº‹ç†è§£ã€è§†é¢‘é—®ç­”ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.

