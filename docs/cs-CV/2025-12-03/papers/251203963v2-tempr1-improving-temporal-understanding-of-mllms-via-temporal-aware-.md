---
layout: default
title: TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning
---

# TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.03963" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.03963v2</a>
  <a href="https://arxiv.org/pdf/2512.03963.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03963v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.03963v2', 'TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tao Wu, Li Yang, Gen Zhan, Yabin Zhang, Yiting Liao, Junlin Li, Deliang Fu, Li Zhang, Limin Wang

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03 (æ›´æ–°: 2025-12-04)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTempR1ï¼Œé€šè¿‡æ—¶åºæ„ŸçŸ¥å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æå‡MLLMå¯¹é•¿è§†é¢‘çš„æ—¶åºç†è§£èƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ—¶åºç†è§£` `å¼ºåŒ–å­¦ä¹ ` `å¤šä»»åŠ¡å­¦ä¹ ` `é•¿è§†é¢‘åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æå‡MLLMæ—¶åºç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦ä½“ç°åœ¨ä»»åŠ¡ç±»å‹å’Œæ•°æ®æœ‰é™ï¼Œéš¾ä»¥æ³›åŒ–åˆ°å¤šæ ·åŒ–çš„æ—¶åºç†è§£åœºæ™¯ã€‚
2. TempR1çš„æ ¸å¿ƒåœ¨äºæ„å»ºæ—¶åºæ„ŸçŸ¥çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¤šä»»åŠ¡è¯­æ–™åº“å’Œå®šåˆ¶åŒ–å¥–åŠ±ï¼Œæå‡æ¨¡å‹å¯¹ä¸åŒæ—¶åºæ¨¡å¼çš„é€‚åº”æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒTempR1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶ä¸”é€šè¿‡è”åˆä¼˜åŒ–å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›å’Œå•ä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)å¯¹é•¿è§†é¢‘çš„æ—¶åºç†è§£èƒ½åŠ›ï¼Œä»è€Œæ¨è¿›æ—¶åºå®šä½ã€åŠ¨ä½œæ£€æµ‹å’Œæ—¶é—´æ•æ„Ÿé—®ç­”ç­‰ä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºäº†TempR1ï¼Œä¸€ä¸ªæ—¶åºæ„ŸçŸ¥çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°å¢å¼ºMLLMçš„æ—¶åºç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤šä»»åŠ¡è¯­æ–™åº“ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¥è§¦åˆ°ä¸åŒçš„æ—¶åºç»“æ„å’Œè¯­ä¹‰ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åŸºäºGroup Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œå®ç°äº†ç¨³å®šæœ‰æ•ˆçš„è·¨ä»»åŠ¡ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ—¶åºä»»åŠ¡åˆ†ä¸ºé¢„æµ‹åŒºé—´å’ŒçœŸå®å®ä¾‹ä¹‹é—´çš„ä¸‰ç§å¯¹åº”ç±»å‹ï¼Œå¹¶ä¸ºæ¯ç§ç±»å‹è®¾è®¡äº†å®šåˆ¶åŒ–çš„å®šä½å¥–åŠ±ï¼Œä½¿TempR1èƒ½å¤Ÿæ•è·ç»†ç²’åº¦çš„æ—¶åºä¾èµ–å…³ç³»ï¼Œå¹¶é€‚åº”ä¸åŒçš„æ—¶åºæ¨¡å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTempR1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹äº’è¡¥ä»»åŠ¡çš„è”åˆä¼˜åŒ–äº§ç”Ÿäº†å¼ºå¤§çš„ååŒæ•ˆåº”ï¼Œå¢å¼ºäº†æ³›åŒ–èƒ½åŠ›å’Œå•ä»»åŠ¡æ€§èƒ½ï¼Œä¸ºMLLMä¸­çš„æ—¶åºæ¨ç†å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æœ‰åŸåˆ™çš„èŒƒä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ—¶åºç†è§£èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦ä½“ç°åœ¨å®ƒä»¬é€šå¸¸åªå…³æ³¨æœ‰é™çš„ä»»åŠ¡ç±»å‹å’Œæ•°æ®é›†ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æ›´å¹¿æ³›çš„æ—¶åºç†è§£åœºæ™¯ä¸­ã€‚è¿™äº›æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨ä¸åŒæ—¶åºä»»åŠ¡ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå¹¶ä¸”ç¼ºä¹é’ˆå¯¹ä¸åŒæ—¶åºæ¨¡å¼çš„ç»†ç²’åº¦ä¼˜åŒ–ç­–ç•¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTempR1çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡è”åˆä¼˜åŒ–å¤šä¸ªå…·æœ‰ä¸åŒæ—¶åºç»“æ„å’Œè¯­ä¹‰çš„ä»»åŠ¡ï¼Œæ¥æå‡MLLMçš„æ—¶åºç†è§£èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–æ—¶åºä»»åŠ¡çš„è¯­æ–™åº“ï¼Œå¹¶è®¾è®¡é’ˆå¯¹ä¸åŒä»»åŠ¡çš„å®šåˆ¶åŒ–å¥–åŠ±å‡½æ•°ï¼ŒTempR1èƒ½å¤Ÿå¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´é²æ£’å’Œæ³›åŒ–çš„æ—¶åºè¡¨ç¤ºã€‚ è¿™ç§å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹å¼èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ä¸åŒä»»åŠ¡ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTempR1çš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šä»»åŠ¡è¯­æ–™åº“æ„å»ºæ¨¡å—ï¼Œç”¨äºæ”¶é›†å’Œæ•´ç†åŒ…å«ä¸åŒæ—¶åºç»“æ„å’Œè¯­ä¹‰çš„è§†é¢‘æ•°æ®ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºé€‚åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ ¼å¼ã€‚2) å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—ï¼Œè¯¥æ¨¡å—åŸºäºGroup Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œç”¨äºè®­ç»ƒMLLMçš„æ—¶åºç†è§£èƒ½åŠ›ã€‚3) å¥–åŠ±å‡½æ•°è®¾è®¡æ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®ä¸åŒæ—¶åºä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œè®¾è®¡å®šåˆ¶åŒ–çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ æ­£ç¡®çš„æ—¶åºè¡Œä¸ºã€‚4) MLLMé›†æˆæ¨¡å—ï¼Œå°†è®­ç»ƒå¥½çš„æ—¶åºç†è§£èƒ½åŠ›é›†æˆåˆ°ç°æœ‰çš„MLLMä¸­ï¼Œä»è€Œæå‡å…¶åœ¨æ—¶åºç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTempR1çš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ—¶åºæ„ŸçŸ¥çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTempR1èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§ä¸åŒç±»å‹çš„æ—¶åºä»»åŠ¡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ ¹æ®ä¸åŒä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œè®¾è®¡å®šåˆ¶åŒ–çš„å¥–åŠ±å‡½æ•°ã€‚æ­¤å¤–ï¼ŒTempR1è¿˜é‡‡ç”¨äº†Group Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ä¸­çš„è´Ÿè¿ç§»é—®é¢˜ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šTempR1çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å°†æ—¶åºä»»åŠ¡åˆ†ä¸ºé¢„æµ‹åŒºé—´å’ŒçœŸå®å®ä¾‹ä¹‹é—´çš„ä¸‰ç§å¯¹åº”ç±»å‹ï¼Œå¹¶ä¸ºæ¯ç§ç±»å‹è®¾è®¡äº†å®šåˆ¶åŒ–çš„å®šä½å¥–åŠ±ã€‚2) é‡‡ç”¨Group Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œä»¥å®ç°ç¨³å®šæœ‰æ•ˆçš„è·¨ä»»åŠ¡ä¼˜åŒ–ã€‚3) æ„å»ºåŒ…å«å¤šæ ·åŒ–æ—¶åºç»“æ„å’Œè¯­ä¹‰çš„å¤šä»»åŠ¡è¯­æ–™åº“ã€‚4) ä½¿ç”¨é¢„è®­ç»ƒçš„MLLMä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ—¶åºç†è§£ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TempR1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ—¶åºå®šä½ä»»åŠ¡ä¸Šï¼ŒTempR1çš„æ€§èƒ½æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æå‡äº†è¶…è¿‡5%ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¿˜è¡¨æ˜ï¼ŒTempR1çš„è”åˆä¼˜åŒ–ç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå•ä»»åŠ¡æ€§èƒ½ï¼ŒéªŒè¯äº†å¤šä»»åŠ¡å­¦ä¹ çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TempR1çš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºé•¿è§†é¢‘ç†è§£é¢†åŸŸï¼Œä¾‹å¦‚è§†é¢‘å†…å®¹åˆ†æã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æå‡MLLMå¯¹è§†é¢‘æ—¶åºä¿¡æ¯çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´ç²¾å‡†çš„äº‹ä»¶æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«å’Œå¼‚å¸¸è¡Œä¸ºé¢„è­¦ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¢æœã€æ•™è‚²å¨±ä¹ç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ›´æ™ºèƒ½ã€æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.

