---
layout: default
title: SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL
---

# SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.04069" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.04069v1</a>
  <a href="https://arxiv.org/pdf/2512.04069.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04069v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.04069v1', 'SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siyi Chen, Mikaela Angelina Uy, Chan Hee Song, Faisal Ladhak, Adithyavairavan Murali, Qing Qu, Stan Birchfield, Valts Blukis, Jonathan Tremblay

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://spacetools.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SpaceToolsï¼šé€šè¿‡åŒé‡äº¤äº’å¼ºåŒ–å­¦ä¹ å¢å¼ºå·¥å…·è¾…åŠ©çš„ç©ºé—´æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç©ºé—´æ¨ç†` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·å­¦ä¹ ` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾ç¡®ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å…·èº«æ™ºèƒ½åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
2. æå‡ºåŒé‡äº¤äº’å¼ºåŒ–å­¦ä¹ (DIRL)æ¡†æ¶ï¼Œé€šè¿‡æ•™å­¦å’Œæ¢ç´¢ä¸¤ä¸ªé˜¶æ®µï¼Œä½¿VLMèƒ½å¤Ÿåè°ƒä½¿ç”¨å¤šç§å·¥å…·ã€‚
3. SpaceToolsæ¨¡å‹åœ¨å¤šä¸ªç©ºé—´ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—SOTAæ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººæ“ä½œä¸­éªŒè¯äº†æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨å®šæ€§è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å…·èº«åº”ç”¨æ‰€éœ€çš„ç²¾ç¡®ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚AgenticèŒƒå¼è®¤ä¸ºVLMå¯ä»¥ä½¿ç”¨å„ç§å·¥å…·æ¥å¢å¼ºè¿™äº›èƒ½åŠ›ï¼Œä¾‹å¦‚æ·±åº¦ä¼°è®¡å™¨ã€åˆ†å‰²æ¨¡å‹å’Œå§¿æ€ä¼°è®¡å™¨ã€‚ç„¶è€Œï¼Œå¦‚ä½•åœ¨ä¸ä¾èµ–æ‰‹å·¥è®¾è®¡çš„æç¤ºç­–ç•¥æˆ–å¼ºåˆ¶æ‰§è¡Œå›ºå®šçš„ã€é¢„å®šä¹‰çš„å·¥å…·ç®¡é“ï¼ˆé™åˆ¶äº†VLMå‘ç°æœ€ä½³å·¥å…·ä½¿ç”¨æ¨¡å¼çš„èƒ½åŠ›ï¼‰çš„æƒ…å†µä¸‹å®ç°è¿™ä¸€æ„¿æ™¯ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ å¯ä»¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½†ç”±äºå¤šå·¥å…·æ¨ç†ä¸­æœç´¢ç©ºé—´å·¨å¤§ï¼Œå› æ­¤è¿„ä»Šä¸ºæ­¢ä»…é™äºä½¿ç”¨å•ä¸ªè§†è§‰å·¥å…·è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†åŒé‡äº¤äº’å¼ºåŒ–å­¦ä¹ (DIRL)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œå…¶ä¸­VLMé€šè¿‡äº¤äº’å¼æ¢ç´¢å’Œåé¦ˆæ¥å­¦ä¹ åè°ƒå¤šä¸ªå·¥å…·ã€‚åœ¨æ•™å­¦é˜¶æ®µï¼Œæˆ‘ä»¬å°†é€šè¿‡äº¤äº’å¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å•ä¸ªå·¥å…·ä¸“å®¶çš„æ¼”ç¤ºä¸ä½¿ç”¨æ‰€æœ‰å·¥å…·çš„å‰æ²¿æ¨¡å‹çš„è½¨è¿¹ç›¸ç»“åˆã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡æŒç»­çš„å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å®Œå–„å¤šå·¥å…·åè°ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹SpaceToolså…·æœ‰å·¥å…·å¢å¼ºçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œåœ¨ç©ºé—´ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆRoboSpatial-Homeã€BLINKã€BOP-ASKï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†ä½¿ç”¨7è‡ªç”±åº¦æœºå™¨äººä½œä¸ºå·¥å…·çš„å¯é çš„çœŸå®ä¸–ç•Œæ“ä½œã€‚DIRLæ¯”vanilla SFTï¼ˆåœ¨RoboSpatialä¸Š+12%ï¼‰å’ŒRLï¼ˆåœ¨RoboSpatialä¸Š+16%ï¼‰åŸºçº¿æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹(VLM)åœ¨å…·èº«åº”ç”¨ä¸­è¿›è¡Œç²¾ç¡®ç©ºé—´æ¨ç†çš„éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–æ‰‹å·¥è®¾è®¡çš„æç¤ºï¼Œè¦ä¹ˆä½¿ç”¨å›ºå®šçš„å·¥å…·æµç¨‹ï¼Œé™åˆ¶äº†VLMå‘ç°æœ€ä¼˜å·¥å…·ä½¿ç”¨ç­–ç•¥çš„èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ è™½ç„¶æœ‰æ½œåŠ›ï¼Œä½†ç”±äºå¤šå·¥å…·ç»„åˆå¸¦æ¥çš„å·¨å¤§æœç´¢ç©ºé—´ï¼Œéš¾ä»¥æœ‰æ•ˆè®­ç»ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åŒé‡äº¤äº’å¼ºåŒ–å­¦ä¹ (DIRL)æ¡†æ¶ï¼Œåˆ†é˜¶æ®µè®­ç»ƒVLMå­¦ä¼šåè°ƒä½¿ç”¨å¤šç§å·¥å…·ã€‚DIRLé¦–å…ˆé€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼ˆæ•™å­¦é˜¶æ®µï¼‰è®©VLMå¿«é€ŸæŒæ¡å·¥å…·çš„ä½¿ç”¨ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆæ¢ç´¢é˜¶æ®µï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–å·¥å…·çš„ä½¿ç”¨ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDIRLæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæ•™å­¦é˜¶æ®µå’Œæ¢ç´¢é˜¶æ®µã€‚åœ¨æ•™å­¦é˜¶æ®µï¼Œæ¨¡å‹ç»“åˆäº†å•ä¸ªå·¥å…·ä¸“å®¶çš„æ¼”ç¤ºæ•°æ®å’Œä½¿ç”¨æ‰€æœ‰å·¥å…·çš„å‰æ²¿æ¨¡å‹çš„è½¨è¿¹ã€‚å•ä¸ªå·¥å…·ä¸“å®¶é€šè¿‡äº¤äº’å¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œèƒ½å¤Ÿç†Ÿç»ƒä½¿ç”¨å•ä¸ªå·¥å…·ã€‚å‰æ²¿æ¨¡å‹åˆ™å°è¯•ä½¿ç”¨æ‰€æœ‰å·¥å…·è§£å†³é—®é¢˜ã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡æŒç»­çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ ¹æ®ç¯å¢ƒåé¦ˆè¿›ä¸€æ­¥ä¼˜åŒ–å¤šå·¥å…·åè°ƒç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šDIRLçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åŒé˜¶æ®µè®­ç»ƒæ–¹å¼ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å¤šå·¥å…·å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢ç©ºé—´è¿‡å¤§çš„é—®é¢˜ã€‚é€šè¿‡æ•™å­¦é˜¶æ®µçš„æ¨¡ä»¿å­¦ä¹ ï¼Œæ¨¡å‹å¯ä»¥å¿«é€Ÿå­¦ä¹ åˆ°æœ‰ç”¨çš„å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œä»è€Œç¼©å°äº†æ¢ç´¢ç©ºé—´ã€‚æ¢ç´¢é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ åˆ™è¿›ä¸€æ­¥ä¼˜åŒ–äº†è¿™äº›ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥è®­ç»ƒå·¥å…·ä¸“å®¶å’Œå‰æ²¿æ¨¡å‹ã€‚å…·ä½“çš„ç®—æ³•é€‰æ‹©å’Œå‚æ•°è®¾ç½®æœªçŸ¥ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨é¼“åŠ±æ¨¡å‹æ¨¡ä»¿å·¥å…·ä¸“å®¶çš„è¡Œä¸ºï¼Œå¹¶æ ¹æ®ç¯å¢ƒåé¦ˆä¼˜åŒ–å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œè®ºæ–‡ä½¿ç”¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹å¤šå·¥å…·æ¨ç†ä»»åŠ¡è¿›è¡Œäº†è°ƒæ•´ã€‚å…·ä½“è°ƒæ•´ç»†èŠ‚æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

SpaceToolsåœ¨RoboSpatial-Homeæ•°æ®é›†ä¸Šç›¸æ¯”vanilla SFTæå‡äº†12%ï¼Œç›¸æ¯”çº¯RLæå‡äº†16%ï¼Œåœ¨BLINKå’ŒBOP-ASKæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†SOTAæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜æˆåŠŸåº”ç”¨äºçœŸå®ä¸–ç•Œçš„7è‡ªç”±åº¦æœºå™¨äººæ“ä½œï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒDIRLæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡VLMçš„å·¥å…·è¾…åŠ©ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯ç†è§£å‘¨å›´ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨å„ç§å·¥å…·å®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚ç‰©ä½“æŠ“å–ã€åœºæ™¯å¯¼èˆªç­‰ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œè½¦è¾†å¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯è¿›è¡Œæ›´ç²¾ç¡®çš„ç¯å¢ƒæ„ŸçŸ¥å’Œè¡Œä¸ºå†³ç­–ã€‚åœ¨å¢å¼ºç°å®é¢†åŸŸï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¯­éŸ³æˆ–æ‰‹åŠ¿ä¸è™šæ‹Ÿç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¹¶ä½¿ç”¨è™šæ‹Ÿå·¥å…·å®Œæˆå„ç§ä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.

