---
layout: default
title: CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding
---

# CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding

**arXiv**: [2512.03558v1](https://arxiv.org/abs/2512.03558) | [PDF](https://arxiv.org/pdf/2512.03558.pdf)

**ä½œè€…**: Huy Quang Ung, Guillaume Habault, Yasutaka Nishimura, Hao Niu, Roberto Legaspi, Tomoki Oya, Ryoichi Kojima, Masato Taya, Chihiro Ono, Atsunori Minamikawa, Yan Liu

**åˆ†ç±»**: cs.CV, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

**å¤‡æ³¨**: Accepted at SIGSPATIAL 2025 (Best paper candidates), 15 pages

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ungquanghuy-kddi/CartoMapQA.git)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CartoMapQAï¼šæå‡ºç”¨äºŽè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡åž‹åœ°å›¾ç†è§£èƒ½åŠ›çš„åŸºç¡€åŸºå‡†æ•°æ®é›†ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡åž‹` `åœ°å›¾ç†è§£` `é—®ç­”ç³»ç»Ÿ` `åŸºå‡†æ•°æ®é›†` `åœ°ç†ç©ºé—´æŽ¨ç†`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰è§†è§‰-è¯­è¨€æ¨¡åž‹åœ¨ç†è§£åœ°å›¾è¿™ç§ç‰¹æ®Šçš„è§†è§‰ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ä¸“é—¨çš„è¯„ä¼°åŸºå‡†ã€‚
2. CartoMapQAæ•°æ®é›†é€šè¿‡é—®ç­”å½¢å¼ï¼Œè€ƒå¯Ÿæ¨¡åž‹å¯¹åœ°å›¾ç¬¦å·ã€æ¯”ä¾‹å°ºã€è·¯çº¿ç­‰ä¿¡æ¯çš„ç†è§£å’ŒæŽ¨ç†èƒ½åŠ›ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒçŽ°æœ‰æ¨¡åž‹åœ¨åœ°å›¾è¯­ä¹‰ç†è§£ã€åœ°ç†ç©ºé—´æŽ¨ç†å’ŒOCRæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰çš„å…´èµ·ä¸ºæ— ç¼é›†æˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬è§£é‡Šåœ°å›¾çš„èƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªè¢«æŽ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†CartoMapQAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºŽé€šè¿‡é—®ç­”ä»»åŠ¡è¯„ä¼°LVLMså¯¹åœ°å›¾ç†è§£çš„åŸºå‡†ã€‚è¯¥æ•°æ®é›†åŒ…å«2000å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ç”±ä¸€å¼ åœ°å›¾ã€ä¸€ä¸ªé—®é¢˜ï¼ˆå¸¦æœ‰å¼€æ”¾å¼æˆ–å¤šé¡¹é€‰æ‹©ç­”æ¡ˆï¼‰å’Œä¸€ä¸ªæ ‡å‡†ç­”æ¡ˆç»„æˆã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†å…³é”®çš„ä½Žã€ä¸­ã€é«˜çº§åœ°å›¾ç†è§£æŠ€èƒ½ï¼ŒåŒ…æ‹¬ç¬¦å·è¯†åˆ«ã€åµŒå…¥ä¿¡æ¯æå–ã€æ¯”ä¾‹å°ºè§£é‡Šå’ŒåŸºäºŽè·¯çº¿çš„æŽ¨ç†ã€‚å¯¹å¼€æºå’Œä¸“æœ‰LVLMsçš„è¯„ä¼°è¡¨æ˜Žï¼Œæ¨¡åž‹åœ¨åœ°å›¾ç‰¹å®šè¯­ä¹‰ç†è§£ã€åœ°ç†ç©ºé—´æŽ¨ç†å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ç›¸å…³é”™è¯¯æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚é€šè¿‡åˆ†ç¦»è¿™äº›å¼±ç‚¹ï¼ŒCartoMapQAä¸ºæŒ‡å¯¼LVLMæž¶æž„çš„æœªæ¥æ”¹è¿›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æœ€ç»ˆï¼Œå®ƒæ”¯æŒå¼€å‘æ›´é€‚åˆä¾èµ–äºŽå¼ºå¤§ä¸”å¯é çš„åœ°å›¾ç†è§£çš„å®žé™…åº”ç”¨çš„æ¨¡åž‹ï¼Œä¾‹å¦‚å¯¼èˆªã€åœ°ç†æœç´¢å’ŒåŸŽå¸‚è§„åˆ’ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œæ•°æ®å¯åœ¨https://github.com/ungquanghuy-kddi/CartoMapQA.gitå…¬å¼€èŽ·å–ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰åœ¨ç†è§£å’Œå¤„ç†åœ°å›¾ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚åœ°å›¾åŒ…å«ä¸°å¯Œçš„ç¬¦å·ã€æ¯”ä¾‹å°ºã€è·¯çº¿ç­‰ä¿¡æ¯ï¼Œéœ€è¦æ¨¡åž‹å…·å¤‡ç‰¹å®šçš„çŸ¥è¯†å’ŒæŽ¨ç†èƒ½åŠ›ã€‚çŽ°æœ‰æ–¹æ³•ç¼ºä¹ä¸“é—¨é’ˆå¯¹åœ°å›¾ç†è§£çš„è¯„ä¼°åŸºå‡†ï¼Œéš¾ä»¥æœ‰æ•ˆè¡¡é‡æ¨¡åž‹åœ¨è¯¥é¢†åŸŸçš„æ€§èƒ½ã€‚çŽ°æœ‰æ¨¡åž‹åœ¨å¤„ç†åœ°å›¾æ—¶ï¼Œå®¹æ˜“å‡ºçŽ°OCRé”™è¯¯ã€è¯­ä¹‰ç†è§£åå·®å’Œåœ°ç†ç©ºé—´æŽ¨ç†ä¸è¶³ç­‰é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCartoMapQAçš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–åœ°å›¾å’Œå¯¹åº”é—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œé€šè¿‡é—®ç­”ä»»åŠ¡æ¥è¯„ä¼°LVLMså¯¹åœ°å›¾çš„ç†è§£èƒ½åŠ›ã€‚æ•°æ®é›†çš„è®¾è®¡æ¶µç›–äº†åœ°å›¾ç†è§£çš„å¤šä¸ªå±‚æ¬¡ï¼Œä»Žä½Žçº§çš„ç¬¦å·è¯†åˆ«åˆ°é«˜çº§çš„è·¯çº¿æŽ¨ç†ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æ¨¡åž‹çš„åœ°å›¾ç†è§£èƒ½åŠ›ã€‚é€šè¿‡åˆ†æžæ¨¡åž‹åœ¨ä¸åŒç±»åž‹é—®é¢˜ä¸Šçš„è¡¨çŽ°ï¼Œå¯ä»¥æ·±å…¥äº†è§£æ¨¡åž‹çš„ä¼˜åŠ¿å’Œä¸è¶³ï¼Œä¸ºæœªæ¥çš„æ¨¡åž‹æ”¹è¿›æä¾›æŒ‡å¯¼ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šCartoMapQAæ•°æ®é›†çš„æž„å»ºæµç¨‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) åœ°å›¾æ”¶é›†ï¼šæ”¶é›†å„ç§ç±»åž‹çš„åœ°å›¾ï¼ŒåŒ…æ‹¬é“è·¯åœ°å›¾ã€åœ°å½¢å›¾ã€åŸŽå¸‚è§„åˆ’å›¾ç­‰ã€‚2) é—®é¢˜ç”Ÿæˆï¼šé’ˆå¯¹æ¯å¼ åœ°å›¾ï¼Œè®¾è®¡ä¸€ç³»åˆ—é—®é¢˜ï¼Œæ¶µç›–ç¬¦å·è¯†åˆ«ã€ä¿¡æ¯æå–ã€æ¯”ä¾‹å°ºè§£é‡Šã€è·¯çº¿æŽ¨ç†ç­‰å¤šä¸ªæ–¹é¢ã€‚é—®é¢˜ç±»åž‹åŒ…æ‹¬å¼€æ”¾å¼é—®é¢˜å’Œå¤šé¡¹é€‰æ‹©é¢˜ã€‚3) ç­”æ¡ˆæ ‡æ³¨ï¼šä¸ºæ¯ä¸ªé—®é¢˜æä¾›æ ‡å‡†ç­”æ¡ˆã€‚4) æ•°æ®é›†åˆ’åˆ†ï¼šå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚

**å…³é”®åˆ›æ–°**ï¼šCartoMapQAçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶ä¸“é—¨é’ˆå¯¹åœ°å›¾ç†è§£ä»»åŠ¡è®¾è®¡çš„æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•ã€‚ä¸Žé€šç”¨è§†è§‰-è¯­è¨€æ•°æ®é›†ä¸åŒï¼ŒCartoMapQAæ›´åŠ å…³æ³¨æ¨¡åž‹å¯¹åœ°å›¾ç‰¹å®šè¯­ä¹‰å’Œåœ°ç†ç©ºé—´ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡é—®ç­”å½¢å¼ï¼Œå¯ä»¥æ›´ç›´æŽ¥åœ°è¯„ä¼°æ¨¡åž‹å¯¹åœ°å›¾ä¿¡æ¯çš„åˆ©ç”¨å’ŒæŽ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCartoMapQAè¿˜æ¶µç›–äº†åœ°å›¾ç†è§£çš„å¤šä¸ªå±‚æ¬¡ï¼Œå¯ä»¥å…¨é¢è¯„ä¼°æ¨¡åž‹çš„åœ°å›¾ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šCartoMapQAæ•°æ®é›†åŒ…å«è¶…è¿‡2000ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ç”±ä¸€å¼ åœ°å›¾ã€ä¸€ä¸ªé—®é¢˜å’Œä¸€ä¸ªæ ‡å‡†ç­”æ¡ˆç»„æˆã€‚é—®é¢˜ç±»åž‹åŒ…æ‹¬å¼€æ”¾å¼é—®é¢˜å’Œå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–äº†ç¬¦å·è¯†åˆ«ã€ä¿¡æ¯æå–ã€æ¯”ä¾‹å°ºè§£é‡Šã€è·¯çº¿æŽ¨ç†ç­‰å¤šä¸ªæ–¹é¢ã€‚æ•°æ®é›†çš„åˆ’åˆ†æ¯”ä¾‹æœªçŸ¥ï¼Œä½†åº”è¯¥ä¿è¯è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„æ•°æ®åˆ†å¸ƒä¸€è‡´æ€§ã€‚æ•°æ®é›†çš„è´¨é‡æŽ§åˆ¶æœªçŸ¥ï¼Œä½†åº”è¯¥ç¡®ä¿é—®é¢˜çš„åˆç†æ€§å’Œç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

è®ºæ–‡è¯„ä¼°äº†å¤šç§å¼€æºå’Œä¸“æœ‰LVLMsåœ¨CartoMapQAæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œå‘çŽ°æ¨¡åž‹åœ¨åœ°å›¾ç‰¹å®šè¯­ä¹‰ç†è§£ã€åœ°ç†ç©ºé—´æŽ¨ç†å’ŒOCRæ–¹é¢æ™®éå­˜åœ¨æŒ‘æˆ˜ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†ç»“æžœè¡¨æ˜ŽçŽ°æœ‰æ¨¡åž‹åœ¨åœ°å›¾ç†è§£æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚è¯¥æ•°æ®é›†çš„å‘å¸ƒä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªé‡è¦çš„åŸºå‡†ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

CartoMapQAçš„ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå¤šç§é¢†åŸŸï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€å¯¼èˆªç³»ç»Ÿã€åœ°ç†ä¿¡æ¯ç³»ç»Ÿã€åŸŽå¸‚è§„åˆ’ç­‰ã€‚é€šè¿‡æé«˜è§†è§‰-è¯­è¨€æ¨¡åž‹å¯¹åœ°å›¾çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å®žçŽ°æ›´æ™ºèƒ½çš„å¯¼èˆªã€æ›´å‡†ç¡®çš„åœ°ç†æœç´¢å’Œæ›´é«˜æ•ˆçš„åŸŽå¸‚è§„åˆ’ã€‚è¯¥ç ”ç©¶è¿˜æœ‰åŠ©äºŽå¼€å‘æ›´æ™ºèƒ½çš„æœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚çŽ¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»å¯¼èˆªå’Œä»»åŠ¡æ‰§è¡Œã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git

