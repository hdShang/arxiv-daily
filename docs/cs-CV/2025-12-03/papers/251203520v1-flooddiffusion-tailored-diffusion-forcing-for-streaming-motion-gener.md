---
layout: default
title: FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation
---

# FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.03520" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.03520v1</a>
  <a href="https://arxiv.org/pdf/2512.03520.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03520v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.03520v1', 'FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiyi Cai, Yuhan Wu, Kunhang Li, You Zhou, Bo Zheng, Haiyang Liu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

**å¤‡æ³¨**: 15 pages, 7 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://shandaai.github.io/FloodDiffusion/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FloodDiffusionï¼šç”¨äºæµå¼è¿åŠ¨ç”Ÿæˆçš„å®šåˆ¶æ‰©æ•£å¼ºåˆ¶æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `æµå¼è¿åŠ¨ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `æ‰©æ•£å¼ºåˆ¶` `æ–‡æœ¬é©±åŠ¨` `äººä½“åŠ¨ç”»` `åŒå‘æ³¨æ„åŠ›` `æ—¶é—´åºåˆ—ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æµå¼è¿åŠ¨ç”Ÿæˆæ–¹æ³•ä¾èµ–åˆ†å—æˆ–è‡ªå›å½’æ¨¡å‹ï¼Œéš¾ä»¥ä¿è¯è¿åŠ¨åºåˆ—çš„è¿è´¯æ€§å’Œå®æ—¶æ€§ã€‚
2. FloodDiffusioné‡‡ç”¨æ‰©æ•£å¼ºåˆ¶æ¡†æ¶ï¼Œé€šè¿‡å®šåˆ¶åŒ–çš„è®­ç»ƒå’Œè°ƒåº¦ç­–ç•¥ï¼Œæ›´å¥½åœ°å»ºæ¨¡è¿åŠ¨åˆ†å¸ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFloodDiffusionåœ¨HumanML3Dæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼ŒFIDæŒ‡æ ‡è¾¾åˆ°0.057ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºFloodDiffusionï¼Œä¸€ä¸ªç”¨äºæ–‡æœ¬é©±åŠ¨çš„æµå¼äººä½“è¿åŠ¨ç”Ÿæˆçš„æ–°æ¡†æ¶ã€‚ç»™å®šéšæ—¶é—´å˜åŒ–çš„æ–‡æœ¬æç¤ºï¼ŒFloodDiffusionèƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬å¯¹é½çš„ã€æ— ç¼çš„è¿åŠ¨åºåˆ—ï¼Œå¹¶å…·æœ‰å®æ—¶å»¶è¿Ÿã€‚ä¸ä¾èµ–äºåˆ†å—æˆ–å…·æœ‰æ‰©æ•£å¤´çš„è‡ªå›å½’æ¨¡å‹çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬é‡‡ç”¨æ‰©æ•£å¼ºåˆ¶æ¡†æ¶æ¥å»ºæ¨¡è¿™ç§æ—¶å˜æ§åˆ¶äº‹ä»¶ä¸‹çš„æ—¶é—´åºåˆ—ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œç›´æ¥å®ç°åŸå§‹æ‰©æ•£å¼ºåˆ¶ï¼ˆå¦‚ä¸ºè§†é¢‘æ¨¡å‹æå‡ºçš„ï¼‰æ— æ³•å¯¹çœŸå®çš„è¿åŠ¨åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¸ºäº†ä¿è¯å¯¹è¾“å‡ºåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¿…é¡»å¯¹åŸå§‹æ‰©æ•£å¼ºåˆ¶è¿›è¡Œå®šåˆ¶ï¼ŒåŒ…æ‹¬ï¼š(i) ä½¿ç”¨åŒå‘æ³¨æ„åŠ›è€Œä¸æ˜¯å› æœæ³¨æ„åŠ›è¿›è¡Œè®­ç»ƒï¼›(ii) å®ç°ä¸‹ä¸‰è§’æ—¶é—´è°ƒåº¦å™¨è€Œä¸æ˜¯éšæœºè°ƒåº¦å™¨ï¼›(iii) åˆ©ç”¨è¿ç»­æ—¶å˜çš„æ–¹å¼å¼•å…¥æ–‡æœ¬æ¡ä»¶ã€‚é€šè¿‡è¿™äº›æ”¹è¿›ï¼Œæˆ‘ä»¬é¦–æ¬¡è¯æ˜äº†åŸºäºæ‰©æ•£å¼ºåˆ¶çš„æ¡†æ¶åœ¨æµå¼è¿åŠ¨ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨HumanML3DåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†0.057çš„FIDã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬é©±åŠ¨çš„æµå¼äººä½“è¿åŠ¨ç”Ÿæˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºåˆ†å—æˆ–è‡ªå›å½’æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†æµå¼æ•°æ®æ—¶å­˜åœ¨é—®é¢˜ã€‚åˆ†å—æ–¹æ³•å¯èƒ½å¯¼è‡´è¿åŠ¨ä¸è¿è´¯ï¼Œè€Œè‡ªå›å½’æ¨¡å‹åˆ™å¯èƒ½å¼•å…¥å»¶è¿Ÿï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚æ­¤å¤–ï¼Œç›´æ¥å°†è§†é¢‘é¢†åŸŸçš„æ‰©æ•£å¼ºåˆ¶æ–¹æ³•åº”ç”¨äºè¿åŠ¨ç”Ÿæˆï¼Œæ— æ³•æœ‰æ•ˆå»ºæ¨¡çœŸå®çš„è¿åŠ¨åˆ†å¸ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFloodDiffusionçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ‰©æ•£å¼ºåˆ¶æ¡†æ¶ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå®šåˆ¶åŒ–æ”¹è¿›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æµå¼è¿åŠ¨ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡å®šåˆ¶åŒ–çš„è®­ç»ƒç­–ç•¥å’Œæ—¶é—´è°ƒåº¦å™¨ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°å­¦ä¹ è¿åŠ¨æ•°æ®çš„åˆ†å¸ƒï¼Œå¹¶ç”Ÿæˆè¿è´¯ã€å®æ—¶çš„è¿åŠ¨åºåˆ—ã€‚å…³é”®åœ¨äºå¦‚ä½•å°†æ‰©æ•£å¼ºåˆ¶æœ‰æ•ˆåœ°åº”ç”¨äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œå¹¶ä¿è¯ç”Ÿæˆç»“æœçš„è´¨é‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFloodDiffusionçš„æ•´ä½“æ¡†æ¶åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£å¼ºåˆ¶çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ–‡æœ¬ç¼–ç å™¨ï¼šå°†è¾“å…¥çš„æ–‡æœ¬æç¤ºè½¬æ¢ä¸ºç‰¹å¾å‘é‡ã€‚2) è¿åŠ¨æ‰©æ•£è¿‡ç¨‹ï¼šå°†çœŸå®çš„è¿åŠ¨æ•°æ®é€æ­¥åŠ å…¥å™ªå£°ï¼Œç›´è‡³å®Œå…¨å˜ä¸ºå™ªå£°ã€‚3) è¿åŠ¨å»å™ªè¿‡ç¨‹ï¼šé€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ ä»å™ªå£°ä¸­æ¢å¤åŸå§‹è¿åŠ¨æ•°æ®ã€‚4) æ‰©æ•£å¼ºåˆ¶æ¨¡å—ï¼šåœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºå¯¹è¿åŠ¨ç”Ÿæˆè¿›è¡Œå¼•å¯¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šFloodDiffusionçš„å…³é”®åˆ›æ–°åœ¨äºå¯¹æ‰©æ•£å¼ºåˆ¶æ¡†æ¶çš„å®šåˆ¶åŒ–æ”¹è¿›ï¼Œå…·ä½“åŒ…æ‹¬ï¼š1) ä½¿ç”¨åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼šç›¸æ¯”äºä¼ ç»Ÿçš„å› æœæ³¨æ„åŠ›ï¼ŒåŒå‘æ³¨æ„åŠ›èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¿åŠ¨åºåˆ—ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚2) é‡‡ç”¨ä¸‹ä¸‰è§’æ—¶é—´è°ƒåº¦å™¨ï¼šç›¸æ¯”äºéšæœºè°ƒåº¦å™¨ï¼Œä¸‹ä¸‰è§’è°ƒåº¦å™¨èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶å™ªå£°çš„åŠ å…¥è¿‡ç¨‹ï¼Œä¿è¯ç”Ÿæˆç»“æœçš„è´¨é‡ã€‚3) å¼•å…¥è¿ç»­æ—¶å˜çš„æ–‡æœ¬æ¡ä»¶ï¼šé€šè¿‡è¿ç»­çš„æ–¹å¼å°†æ–‡æœ¬ä¿¡æ¯èå…¥åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¼•å¯¼è¿åŠ¨ç”Ÿæˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒFloodDiffusioné‡‡ç”¨äº†Transformeræ¶æ„ï¼Œå¹¶å¯¹æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œäº†æ”¹è¿›ã€‚åœ¨æŸå¤±å‡½æ•°æ–¹é¢ï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„æ‰©æ•£æ¨¡å‹æŸå¤±å‡½æ•°ï¼Œå¹¶åŠ å…¥äº†æ–‡æœ¬å¯¹é½æŸå¤±ï¼Œä»¥ä¿è¯ç”Ÿæˆçš„è¿åŠ¨ä¸æ–‡æœ¬æç¤ºä¸€è‡´ã€‚åœ¨æ—¶é—´è°ƒåº¦æ–¹é¢ï¼Œé‡‡ç”¨äº†ä¸‹ä¸‰è§’è°ƒåº¦å™¨ï¼Œå¹¶å¯¹è°ƒåº¦å‚æ•°è¿›è¡Œäº†ç²¾ç»†è°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FloodDiffusionåœ¨HumanML3Dæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒFIDæŒ‡æ ‡è¾¾åˆ°äº†0.057ï¼Œè¶…è¶Šäº†ç°æœ‰çš„SOTAæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå®šåˆ¶åŒ–çš„æ‰©æ•£å¼ºåˆ¶æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡è¿åŠ¨æ•°æ®åˆ†å¸ƒï¼Œç”Ÿæˆé«˜è´¨é‡çš„æµå¼è¿åŠ¨åºåˆ—ã€‚è¯¥æ–¹æ³•åœ¨ä¿è¯å®æ—¶æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†è¿åŠ¨ç”Ÿæˆçš„è´¨é‡å’Œè¿è´¯æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FloodDiffusionåœ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€äººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆä¸ç”¨æˆ·è¯­éŸ³æˆ–æ–‡æœ¬æŒ‡ä»¤ç›¸å¯¹åº”çš„å®æ—¶äººç‰©åŠ¨ç”»ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºè¿åŠ¨åº·å¤ã€èˆè¹ˆæ•™å­¦ç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„è¿åŠ¨æŒ‡å¯¼ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/

