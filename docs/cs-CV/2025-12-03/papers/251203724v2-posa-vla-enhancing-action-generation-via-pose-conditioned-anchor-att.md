---
layout: default
title: PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention
---

# PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention

**arXiv**: [2512.03724v2](https://arxiv.org/abs/2512.03724) | [PDF](https://arxiv.org/pdf/2512.03724.pdf)

**ä½œè€…**: Ziwen Li, Xin Wang, Hanlue Zhang, Runnan Chen, Runqi Lin, Xiao He, Han Huang, Yandong Guo, Fakhri Karray, Tongliang Liu, Mingming Gong

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03 (æ›´æ–°: 2025-12-08)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PosA-VLAï¼šé€šè¿‡å§¿æ€æ¡ä»¶é”šç‚¹æ³¨æ„åŠ›å¢žå¼ºå…·èº«ä»»åŠ¡ä¸­çš„åŠ¨ä½œç”Ÿæˆ**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `å…·èº«æ™ºèƒ½` `å§¿æ€æ¡ä»¶æ³¨æ„åŠ›` `æœºå™¨äººæ“ä½œ` `åŠ¨ä½œç”Ÿæˆ` `ç›®æ ‡å¯¼å‘` `è½»é‡çº§æž¶æž„`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹åœ¨å¤æ‚çŽ¯å¢ƒä¸­æ˜“å—æ— å…³ç‰©ä½“å¹²æ‰°ï¼Œå¯¼è‡´åŠ¨ä½œå†—ä½™å’Œä¸ç¨³å®šï¼Œé™åˆ¶äº†å…¶åœ¨æ—¶é—´æ•æ„Ÿåœºæ™¯çš„åº”ç”¨ã€‚
2. PosA-VLAæ¡†æž¶é€šè¿‡å§¿æ€æ¡ä»¶ç›‘ç£é”šå®šè§†è§‰æ³¨æ„åŠ›ï¼Œå¼•å¯¼æ¨¡åž‹å…³æ³¨ä»»åŠ¡ç›¸å…³åŒºåŸŸï¼Œä»Žè€Œæå‡åŠ¨ä½œç”Ÿæˆçš„ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒPosA-VLAåœ¨å¤šç§æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºç²¾ç¡®å’Œé«˜æ•ˆçš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤æ‚çŽ¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹åœ¨å…·èº«ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å®žé™…åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„VLAæ¨¡åž‹åœ¨ç”Ÿæˆä¸€è‡´ä¸”ç²¾ç¡®çš„ã€ä»¥ç›®æ ‡ä¸ºå¯¼å‘çš„åŠ¨ä½œæ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ç»å¸¸æ²¿ç€è½¨è¿¹äº§ç”Ÿå†—ä½™æˆ–ä¸ç¨³å®šçš„è¿åŠ¨ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ—¶é—´æ•æ„Ÿåœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚æœ¬æ–‡å°†è¿™äº›å†—ä½™åŠ¨ä½œå½’å› äºŽçŽ°æœ‰VLAåœ¨ç©ºé—´ä¸Šå‡åŒ€çš„æ„ŸçŸ¥åœºï¼Œè¿™å¯¼è‡´å®ƒä»¬å®¹æ˜“è¢«ä¸Žç›®æ ‡æ— å…³çš„ç‰©ä½“åˆ†æ•£æ³¨æ„åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çŽ¯å¢ƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„PosA-VLAæ¡†æž¶ï¼Œè¯¥æ¡†æž¶é€šè¿‡å§¿æ€æ¡ä»¶ç›‘ç£æ¥é”šå®šè§†è§‰æ³¨æ„åŠ›ï¼ŒæŒç»­å¼•å¯¼æ¨¡åž‹çš„æ„ŸçŸ¥æœå‘ä¸Žä»»åŠ¡ç›¸å…³çš„åŒºåŸŸã€‚å§¿æ€æ¡ä»¶é”šç‚¹æ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°å°†æŒ‡ä»¤è¯­ä¹‰ä¸Žå¯æ“ä½œçš„è§†è§‰çº¿ç´¢å¯¹é½ï¼Œä»Žè€Œæé«˜åŠ¨ä½œç”Ÿæˆçš„ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æž¶é‡‡ç”¨è½»é‡çº§æž¶æž„ï¼Œä¸éœ€è¦è¾…åŠ©æ„ŸçŸ¥æ¨¡å—ï¼ˆä¾‹å¦‚ï¼Œåˆ†å‰²æˆ– grounding ç½‘ç»œï¼‰ï¼Œä»Žè€Œç¡®ä¿é«˜æ•ˆçš„æŽ¨ç†ã€‚å¤§é‡çš„å®žéªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­ä»¥ç²¾ç¡®å’Œæ—¶é—´é«˜æ•ˆçš„è¡Œä¸ºæ‰§è¡Œå…·èº«ä»»åŠ¡ï¼Œå¹¶åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„çŽ¯å¢ƒä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹åœ¨å…·èº«ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çŽ¯å¢ƒä¸­ï¼Œå®¹æ˜“å—åˆ°ä¸Žç›®æ ‡æ— å…³çš„ç‰©ä½“çš„å¹²æ‰°ï¼Œå¯¼è‡´ç”Ÿæˆå†—ä½™æˆ–ä¸ç¨³å®šçš„åŠ¨ä½œåºåˆ—ã€‚è¿™é™ä½Žäº†åŠ¨ä½œçš„ç²¾åº¦å’Œæ•ˆçŽ‡ï¼Œé™åˆ¶äº†å…¶åœ¨æ—¶é—´æ•æ„Ÿåœºæ™¯ä¸­çš„åº”ç”¨ã€‚çŽ°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ä»»åŠ¡ç›¸å…³åŒºåŸŸçš„æœ‰æ•ˆå…³æ³¨æœºåˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å§¿æ€æ¡ä»¶ç›‘ç£æ¥å¼•å¯¼è§†è§‰æ³¨æ„åŠ›ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°å…³æ³¨ä¸Žä»»åŠ¡ç›¸å…³çš„åŒºåŸŸã€‚é€šè¿‡å°†æ¨¡åž‹çš„æ³¨æ„åŠ›é”šå®šåœ¨ä¸Žå½“å‰å§¿æ€ç›¸å…³çš„è§†è§‰çº¿ç´¢ä¸Šï¼Œå¯ä»¥å‡å°‘æ¨¡åž‹å¯¹æ— å…³ä¿¡æ¯çš„å…³æ³¨ï¼Œä»Žè€Œæé«˜åŠ¨ä½œç”Ÿæˆçš„ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å°†æŒ‡ä»¤è¯­ä¹‰ä¸Žå¯æ“ä½œçš„è§†è§‰çº¿ç´¢å¯¹é½ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šPosA-VLAæ¡†æž¶ä¸»è¦åŒ…å«è§†è§‰ç¼–ç å™¨ã€è¯­è¨€ç¼–ç å™¨ã€å§¿æ€ç¼–ç å™¨å’ŒåŠ¨ä½œè§£ç å™¨ã€‚è§†è§‰ç¼–ç å™¨å¤„ç†è¾“å…¥çš„è§†è§‰ä¿¡æ¯ï¼Œè¯­è¨€ç¼–ç å™¨å¤„ç†æŒ‡ä»¤ä¿¡æ¯ï¼Œå§¿æ€ç¼–ç å™¨å¤„ç†å½“å‰æœºå™¨äººå§¿æ€ä¿¡æ¯ã€‚å§¿æ€ç¼–ç å™¨çš„è¾“å‡ºè¢«ç”¨äºŽè°ƒèŠ‚è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶æ›´åŠ å…³æ³¨ä¸Žå½“å‰å§¿æ€ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚åŠ¨ä½œè§£ç å™¨åˆ™æ ¹æ®ç¼–ç åŽçš„è§†è§‰ã€è¯­è¨€å’Œå§¿æ€ä¿¡æ¯ç”ŸæˆåŠ¨ä½œåºåˆ—ã€‚æ•´ä¸ªæ¡†æž¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽæå‡ºäº†å§¿æ€æ¡ä»¶é”šç‚¹æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¸Žä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¸åŒï¼Œè¯¥æœºåˆ¶åˆ©ç”¨æœºå™¨äººçš„å½“å‰å§¿æ€ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œå¼•å¯¼æ¨¡åž‹å…³æ³¨ä¸Žå§¿æ€ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘æ¨¡åž‹å¯¹æ— å…³ä¿¡æ¯çš„å…³æ³¨ï¼Œæé«˜åŠ¨ä½œç”Ÿæˆçš„ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æž¶é‡‡ç”¨è½»é‡çº§æž¶æž„ï¼Œæ— éœ€é¢å¤–çš„æ„ŸçŸ¥æ¨¡å—ï¼Œä¿è¯äº†æŽ¨ç†æ•ˆçŽ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå§¿æ€æ¡ä»¶é”šç‚¹æ³¨æ„åŠ›æœºåˆ¶çš„å…·ä½“å®žçŽ°æ–¹å¼æ˜¯ï¼šé¦–å…ˆï¼Œé€šè¿‡å§¿æ€ç¼–ç å™¨å°†æœºå™¨äººçš„å½“å‰å§¿æ€ç¼–ç æˆä¸€ä¸ªå‘é‡è¡¨ç¤ºã€‚ç„¶åŽï¼Œåˆ©ç”¨è¯¥å‘é‡è¡¨ç¤ºæ¥è°ƒèŠ‚è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œä½¿å¾—æ¨¡åž‹æ›´åŠ å…³æ³¨ä¸Žå½“å‰å§¿æ€ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚å…·ä½“æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªå°çš„ç¥žç»ç½‘ç»œæ¥å°†å§¿æ€å‘é‡æ˜ å°„åˆ°ä¸€ä¸ªæ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼Œç„¶åŽå°†è¯¥çŸ©é˜µä¸Žè§†è§‰ç‰¹å¾å›¾è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åŠ¨ä½œé¢„æµ‹æŸå¤±å’Œå§¿æ€å¯¹é½æŸå¤±ï¼Œå…¶ä¸­å§¿æ€å¯¹é½æŸå¤±ç”¨äºŽé¼“åŠ±æ¨¡åž‹å­¦ä¹ åˆ°å§¿æ€ä¸Žè§†è§‰åŒºåŸŸä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒPosA-VLAæ¡†æž¶åœ¨å¤šä¸ªæœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤æ‚çš„ç‰©ä½“æŠ“å–ä»»åŠ¡ä¸­ï¼ŒPosA-VLAçš„æˆåŠŸçŽ‡æ¯”çŽ°æœ‰æ–¹æ³•æé«˜äº†15%ä»¥ä¸Šï¼Œå¹¶ä¸”åŠ¨ä½œæ‰§è¡Œæ—¶é—´ç¼©çŸ­äº†20%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æž¶åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„çŽ¯å¢ƒä¸­è¡¨çŽ°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜Žäº†å…¶åœ¨å®žé™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

PosA-VLAæ¡†æž¶å¯åº”ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€æ”¾ç½®ã€ç»„è£…ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æ“ä½œç²¾åº¦å’Œæ•ˆçŽ‡ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å®žé™…åº”ç”¨åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æž¶è¿˜å¯ä»¥åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶ã€è™šæ‹ŸçŽ°å®žç­‰é¢†åŸŸï¼Œæé«˜æ™ºèƒ½ä½“çš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.

