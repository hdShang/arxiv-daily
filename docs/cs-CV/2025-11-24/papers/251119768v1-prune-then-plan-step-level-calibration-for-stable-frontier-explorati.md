---
layout: default
title: Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering
---

# Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.19768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.19768v1</a>
  <a href="https://arxiv.org/pdf/2511.19768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19768v1" onclick="toggleFavorite(this, '2511.19768v1', 'Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Noah Frahm, Prakrut Patel, Yue Zhang, Shoubin Yu, Mohit Bansal, Roni Sengupta

**åˆ†ç±»**: cs.CV, cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

**å¤‡æ³¨**: webpage: https://noahfrahm.github.io/Prune-Then-Plan-project-page/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Prune-Then-Planï¼šé€šè¿‡æ­¥çº§æ ¡å‡†å®ç°å…·èº«é—®ç­”ä¸­ç¨³å®šçš„è¾¹ç•Œæ¢ç´¢**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `å…·èº«é—®ç­”` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ­¥çº§æ ¡å‡†` `è¾¹ç•Œæ¢ç´¢` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·èº«é—®ç­”æ™ºèƒ½ä½“åœ¨æ­¥çº§æ¢ç´¢ä¸­ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹å¸¸å› è¿‡åº¦è‡ªä¿¡å’Œé”™è¯¯æ ¡å‡†å¯¼è‡´ä¸ç¨³å®šç§»åŠ¨ã€‚
2. Prune-Then-Planæ¡†æ¶é€šè¿‡å‰ªæä¸åˆç†çš„è¾¹ç•Œé€‰æ‹©ï¼Œå¹¶å°†å†³ç­–å§”æ‰˜ç»™åŸºäºè¦†ç›–ç‡çš„è§„åˆ’å™¨æ¥ç¨³å®šæ¢ç´¢ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰æ¥åœ°çš„SPLå’ŒLLM-MatchæŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†æ˜¾è‘—æå‡ï¼Œå¹¶æé«˜äº†åœºæ™¯è¦†ç›–ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)é€šè¿‡ä¸ºå¼€æ”¾è¯æ±‡æ¨ç†æä¾›å¼ºå¤§çš„è¯­ä¹‰å…ˆéªŒï¼Œæ”¹è¿›äº†å…·èº«é—®ç­”(EQA)æ™ºèƒ½ä½“ã€‚ç„¶è€Œï¼Œå½“ç›´æ¥ç”¨äºæ­¥çº§æ¢ç´¢æ—¶ï¼ŒVLMså¸¸å¸¸è¡¨ç°å‡ºè¾¹ç•ŒæŒ¯è¡ï¼Œå³ç”±è¿‡åº¦è‡ªä¿¡å’Œé”™è¯¯æ ¡å‡†å¯¼è‡´çš„ä¸ç¨³å®šçš„æ¥å›ç§»åŠ¨ï¼Œä»è€Œå¯¼è‡´ä½æ•ˆçš„å¯¼èˆªå’Œé™ä½çš„ç­”æ¡ˆè´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†Prune-Then-Planï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ¡†æ¶ï¼Œé€šè¿‡æ­¥çº§æ ¡å‡†æ¥ç¨³å®šæ¢ç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¿¡ä»»åŸå§‹VLMåˆ†æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨å—Holm-Bonferroniå¯å‘çš„å‰ªæç¨‹åºæ¥å‰ªé™¤ä¸åˆç†çš„è¾¹ç•Œé€‰æ‹©ï¼Œç„¶åå°†æœ€ç»ˆå†³ç­–å§”æ‰˜ç»™åŸºäºè¦†ç›–ç‡çš„è§„åˆ’å™¨ã€‚è¿™ç§åˆ†ç¦»é€šè¿‡ä¾èµ–äººç±»æ°´å¹³çš„åˆ¤æ–­æ¥æ ¡å‡†VLMsçš„æ­¥çº§è¡Œä¸ºï¼Œå°†è¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹è½¬åŒ–ä¸ºä¿å®ˆçš„ã€å¯è§£é‡Šçš„è¡ŒåŠ¨ã€‚é›†æˆåˆ°3D-Mem EQAæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰æ¥åœ°çš„SPLå’ŒLLM-MatchæŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾49%å’Œ33%çš„ç›¸å¯¹æ”¹è¿›ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨OpenEQAå’ŒEXPRESS-Benchæ•°æ®é›†ä¸Šï¼Œåœ¨ç›¸åŒçš„æ¢ç´¢é¢„ç®—ä¸‹å®ç°äº†æ›´å¥½çš„åœºæ™¯è¦†ç›–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…·èº«é—®ç­”ï¼ˆEQAï¼‰æ™ºèƒ½ä½“åœ¨è¿›è¡Œæ­¥çº§æ¢ç´¢æ—¶ï¼Œç›´æ¥ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åŸå§‹é¢„æµ‹åˆ†æ•°ï¼Œå®¹æ˜“å—åˆ°VLMè¿‡åº¦è‡ªä¿¡å’Œé”™è¯¯æ ¡å‡†çš„å½±å“ï¼Œå¯¼è‡´æ™ºèƒ½ä½“åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­å‡ºç°ä¸ç¨³å®šçš„æ¥å›ç§»åŠ¨ï¼ˆè¾¹ç•ŒæŒ¯è¡ï¼‰ï¼Œé™ä½äº†å¯¼èˆªæ•ˆç‡å’Œç­”æ¡ˆè´¨é‡ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–¹æ³•æ¥æ ¡å‡†VLMçš„æ­¥çº§è¡Œä¸ºï¼Œä½¿å…¶èƒ½å¤Ÿæ›´ç¨³å®šåœ°è¿›è¡Œæ¢ç´¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†VLMçš„é¢„æµ‹ç»“æœè¿›è¡Œæ ¡å‡†ï¼Œä½¿å…¶æ›´åŠ ä¿å®ˆå’Œå¯ä¿¡ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä½¿ç”¨å‰ªæç­–ç•¥å»é™¤VLMé¢„æµ‹ä¸­ä¸åˆç†çš„é€‰é¡¹ï¼Œç„¶åå°†æœ€ç»ˆå†³ç­–äº¤ç»™ä¸€ä¸ªåŸºäºè¦†ç›–ç‡çš„è§„åˆ’å™¨ã€‚è¿™ç§â€œå…ˆå‰ªæï¼Œåè§„åˆ’â€çš„ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°æŠ‘åˆ¶VLMçš„è¿‡åº¦è‡ªä¿¡ï¼Œå¹¶åˆ©ç”¨è§„åˆ’å™¨æ¥ä¿è¯æ¢ç´¢çš„æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPrune-Then-Planæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šå‰ªæé˜¶æ®µå’Œè§„åˆ’é˜¶æ®µã€‚åœ¨å‰ªæé˜¶æ®µï¼Œä½¿ç”¨å—Holm-Bonferroniå¯å‘çš„å‰ªæç¨‹åºï¼Œæ ¹æ®VLMçš„é¢„æµ‹åˆ†æ•°ï¼Œå»é™¤é‚£äº›ç½®ä¿¡åº¦è¾ƒä½æˆ–è€…ä¸åˆç†çš„è¾¹ç•Œé€‰æ‹©ã€‚åœ¨è§„åˆ’é˜¶æ®µï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºè¦†ç›–ç‡çš„è§„åˆ’å™¨ï¼Œæ ¹æ®å‰©ä½™çš„è¾¹ç•Œé€‰æ‹©ï¼Œé€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–åœºæ™¯è¦†ç›–ç‡çš„è¡ŒåŠ¨ã€‚æ•´ä¸ªæ¡†æ¶é›†æˆåˆ°3D-Mem EQAæ¡†æ¶ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ­¥çº§æ ¡å‡†æ¡†æ¶ï¼Œé€šè¿‡å‰ªæå’Œè§„åˆ’ç›¸ç»“åˆçš„æ–¹å¼ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†VLMåœ¨æ­¥çº§æ¢ç´¢ä¸­è¿‡åº¦è‡ªä¿¡å’Œé”™è¯¯æ ¡å‡†çš„é—®é¢˜ã€‚ä¸ç›´æ¥ä½¿ç”¨VLMçš„åŸå§‹é¢„æµ‹åˆ†æ•°ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´ä¿å®ˆã€æ›´å¯è§£é‡Šçš„è¡ŒåŠ¨ï¼Œä»è€Œæé«˜æ¢ç´¢çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå‰ªæé˜¶æ®µçš„å…³é”®åœ¨äºHolm-Bonferroni inspired pruning procedureï¼Œå®ƒæ ¹æ®VLMç»™å‡ºçš„æ¯ä¸ªfrontierçš„ç½®ä¿¡åº¦æ‰“åˆ†ï¼Œè¿›è¡Œæ’åºï¼Œç„¶åä¾æ¬¡è¿›è¡Œå‡è®¾æ£€éªŒï¼Œå¦‚æœæŸä¸ªfrontierçš„ç½®ä¿¡åº¦è¿‡ä½ï¼Œåˆ™å°†å…¶å‰ªé™¤ã€‚è§„åˆ’é˜¶æ®µçš„å…³é”®åœ¨äºcoverage-based plannerï¼Œå®ƒæ ¹æ®å‰©ä½™çš„frontierï¼Œé€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–åœºæ™¯è¦†ç›–ç‡çš„è¡ŒåŠ¨ã€‚å…·ä½“å®ç°ä¸­ï¼Œä½¿ç”¨äº†3D-Mem EQAæ¡†æ¶ä½œä¸ºåŸºç¡€ï¼Œå¹¶å¯¹å…¶ä¸­çš„æ¢ç´¢ç­–ç•¥è¿›è¡Œäº†æ”¹è¿›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒPrune-Then-Planæ¡†æ¶åœ¨OpenEQAå’ŒEXPRESS-Benchæ•°æ®é›†ä¸Šï¼Œåœ¨è§†è§‰æ¥åœ°çš„SPLæŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾49%çš„ç›¸å¯¹æ”¹è¿›ï¼Œåœ¨LLM-MatchæŒ‡æ ‡ä¸Šå®ç°äº†33%çš„ç›¸å¯¹æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ç›¸åŒçš„æ¢ç´¢é¢„ç®—ä¸‹ï¼Œå®ç°äº†æ›´å¥½çš„åœºæ™¯è¦†ç›–ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜æ¢ç´¢æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æ ¡å‡†è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ­¥çº§è¡Œä¸ºï¼Œå¯ä»¥æé«˜æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚æœç´¢ç‰¹å®šç‰©ä½“ã€å›ç­”é—®é¢˜ç­‰ã€‚è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæœ‰æœ›æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.

