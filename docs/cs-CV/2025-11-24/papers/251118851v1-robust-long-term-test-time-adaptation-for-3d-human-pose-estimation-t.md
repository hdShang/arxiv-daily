---
layout: default
title: Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization
---

# Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.18851" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.18851v1</a>
  <a href="https://arxiv.org/pdf/2511.18851.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.18851v1" onclick="toggleFavorite(this, '2511.18851v1', 'Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yilin Wen, Kechuan Dong, Yusuke Sugano

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

**å¤‡æ³¨**: Accepted by AAAI 2026, main track

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè¿åŠ¨ç¦»æ•£åŒ–çš„é²æ£’é•¿æœŸæµ‹è¯•æ—¶è‡ªé€‚åº”3Däººä½“å§¿æ€ä¼°è®¡æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `3Däººä½“å§¿æ€ä¼°è®¡` `æµ‹è¯•æ—¶è‡ªé€‚åº”` `è¿åŠ¨ç¦»æ•£åŒ–` `æ— ç›‘ç£å­¦ä¹ ` `é•¿æœŸè§†é¢‘` `è¯¯å·®ç´¯ç§¯` `è‡ªæˆ‘é‡æ”¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åœ¨çº¿è‡ªé€‚åº”çš„3Däººä½“å§¿æ€ä¼°è®¡æ˜“å—è¯¯å·®ç´¯ç§¯å½±å“ï¼Œå¯¼è‡´æ€§èƒ½éšæ—¶é—´ä¸‹é™ã€‚
2. é€šè¿‡è¿åŠ¨ç¦»æ•£åŒ–ï¼Œåˆ©ç”¨æ— ç›‘ç£èšç±»å¾—åˆ°é”šå®šè¿åŠ¨ï¼Œå¹¶å¼•å…¥è½¯é‡ç½®æœºåˆ¶ï¼Œæå‡é²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰åœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ï¼ŒéªŒè¯äº†è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡è¿åŠ¨ç¦»æ•£åŒ–æ¥è§£å†³3Däººä½“å§¿æ€ä¼°è®¡ä¸­åœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ½œåœ¨è¿åŠ¨è¡¨ç¤ºç©ºé—´ä¸­çš„æ— ç›‘ç£èšç±»æ¥è·å¾—ä¸€ç»„é”šå®šè¿åŠ¨ï¼Œè¿™äº›é”šå®šè¿åŠ¨çš„è§„å¾‹æ€§æœ‰åŠ©äºç›‘ç£äººä½“å§¿æ€ä¼°è®¡å™¨ï¼Œå¹¶å®ç°æœ‰æ•ˆçš„è‡ªæˆ‘é‡æ”¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„è½¯é‡ç½®æœºåˆ¶ï¼Œé€šè¿‡åœ¨è¿ç»­è‡ªé€‚åº”æœŸé—´å°†å§¿æ€ä¼°è®¡å™¨æ¢å¤åˆ°å…¶æŒ‡æ•°ç§»åŠ¨å¹³å‡å€¼æ¥å®ç°ã€‚é€šè¿‡æŒç»­é€‚åº”åŒä¸€å¯¹è±¡çš„åŸŸå¤–æµæµ‹è¯•è§†é¢‘æ¥æ£€æŸ¥é•¿æœŸåœ¨çº¿è‡ªé€‚åº”ï¼Œä»è€Œå¯ä»¥åœ¨æ•´ä¸ªæµè§‚å¯Ÿè¿‡ç¨‹ä¸­æ•è·ä¸€è‡´çš„ä¸ªäººå½¢çŠ¶å’Œè¿åŠ¨ç‰¹å¾ã€‚é€šè¿‡å‡è½»è¯¯å·®ç´¯ç§¯ï¼Œè¯¥è§£å†³æ–¹æ¡ˆèƒ½å¤Ÿé²æ£’åœ°åˆ©ç”¨è¿™äº›ä¸ªäººç‰¹å¾æ¥æé«˜å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥è§£å†³æ–¹æ¡ˆä¼˜äºä»¥å‰çš„åœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ï¼Œå¹¶éªŒè¯äº†è®¾è®¡é€‰æ‹©ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„åœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•åœ¨3Däººä½“å§¿æ€ä¼°è®¡ä¸­é¢ä¸´è¯¯å·®ç´¯ç§¯çš„é—®é¢˜ã€‚ç”±äºæ¨¡å‹åœ¨æœªæ ‡è®°çš„æµ‹è¯•æ•°æ®ä¸Šè¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œä¸å®Œç¾çš„é¢„æµ‹ä¼šé€æ¸ç´¯ç§¯è¯¯å·®ï¼Œå¯¼è‡´é•¿æœŸæ€§èƒ½ä¸‹é™ã€‚å°¤å…¶æ˜¯åœ¨å¤„ç†é•¿æœŸè§†é¢‘æµæ—¶ï¼Œè¿™ç§è¯¯å·®ç´¯ç§¯ä¼šä¸¥é‡å½±å“å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¿åŠ¨ç¦»æ•£åŒ–æ¥å¼•å…¥è¿åŠ¨çš„è§„å¾‹æ€§ï¼Œä»è€Œçº¦æŸå§¿æ€ä¼°è®¡å™¨çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå‡å°‘è¯¯å·®ç´¯ç§¯ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ— ç›‘ç£èšç±»å°†è¿åŠ¨è¡¨ç¤ºç©ºé—´åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ªç¦»æ•£çš„é”šå®šè¿åŠ¨ï¼Œåˆ©ç”¨è¿™äº›é”šå®šè¿åŠ¨æ¥ç›‘ç£å§¿æ€ä¼°è®¡å™¨çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œå¼•å…¥è½¯é‡ç½®æœºåˆ¶ï¼Œå®šæœŸå°†å§¿æ€ä¼°è®¡å™¨æ¢å¤åˆ°å…¶å†å²çŠ¶æ€ï¼Œä»¥é˜²æ­¢æ¨¡å‹è¿‡åº¦é€‚åº”å™ªå£°æ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) å§¿æ€ä¼°è®¡å™¨ï¼šç”¨äºä»è§†é¢‘å¸§ä¸­ä¼°è®¡3Däººä½“å§¿æ€ã€‚2) è¿åŠ¨è¡¨ç¤ºæå–å™¨ï¼šç”¨äºæå–è¿ç»­å¸§ä¹‹é—´çš„è¿åŠ¨è¡¨ç¤ºã€‚3) æ— ç›‘ç£èšç±»æ¨¡å—ï¼šç”¨äºå°†è¿åŠ¨è¡¨ç¤ºç©ºé—´åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ªç°‡ï¼Œæ¯ä¸ªç°‡ä»£è¡¨ä¸€ä¸ªé”šå®šè¿åŠ¨ã€‚4) è‡ªæˆ‘é‡æ”¾æ¨¡å—ï¼šåˆ©ç”¨é”šå®šè¿åŠ¨æ¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç”¨äºç›‘ç£å§¿æ€ä¼°è®¡å™¨çš„è®­ç»ƒã€‚5) è½¯é‡ç½®æ¨¡å—ï¼šå®šæœŸå°†å§¿æ€ä¼°è®¡å™¨æ¢å¤åˆ°å…¶æŒ‡æ•°ç§»åŠ¨å¹³å‡å€¼ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨è¿åŠ¨ç¦»æ•£åŒ–æ¥å¼•å…¥è¿åŠ¨çš„è§„å¾‹æ€§ï¼Œä»è€Œçº¦æŸå§¿æ€ä¼°è®¡å™¨çš„å­¦ä¹ è¿‡ç¨‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦äººå·¥æ ‡æ³¨çš„è¿åŠ¨æ ‡ç­¾ï¼Œè€Œæ˜¯é€šè¿‡æ— ç›‘ç£èšç±»è‡ªåŠ¨å­¦ä¹ è¿åŠ¨æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè½¯é‡ç½®æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°é˜²æ­¢æ¨¡å‹è¿‡åº¦é€‚åº”å™ªå£°æ•°æ®ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¿åŠ¨è¡¨ç¤ºæå–æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨ä¾‹å¦‚å…‰æµã€éª¨éª¼è¿åŠ¨ç­‰ç‰¹å¾ã€‚æ— ç›‘ç£èšç±»å¯ä»¥ä½¿ç”¨K-meansç­‰ç®—æ³•ã€‚è‡ªæˆ‘é‡æ”¾æ¨¡å—å¯ä»¥é€šè¿‡å°†é”šå®šè¿åŠ¨ä¸å§¿æ€ä¼°è®¡å™¨çš„è¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼Œè®¡ç®—ä¸€è‡´æ€§æŸå¤±ã€‚è½¯é‡ç½®æ¨¡å—å¯ä»¥é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ¥ç»´æŠ¤å§¿æ€ä¼°è®¡å™¨çš„å†å²çŠ¶æ€ï¼Œå¹¶å®šæœŸå°†å…¶æƒé‡æ¢å¤åˆ°EMAå€¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é•¿æœŸæµ‹è¯•æ—¶è‡ªé€‚åº”ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å‡å°‘è¯¯å·®ç´¯ç§¯ï¼Œæé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†state-of-the-artçš„ç»“æœï¼Œå¹¶ä¸”åœ¨é•¿æœŸè§†é¢‘æµä¸Šçš„æ€§èƒ½æå‡å°¤ä¸ºæ˜æ˜¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç›‘æ§ã€äººæœºäº¤äº’ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å¯¹ç›‘æ§è§†é¢‘ä¸­çš„äººä½“å§¿æ€è¿›è¡Œé•¿æœŸè·Ÿè¸ªå’Œåˆ†æï¼Œä»è€Œå®ç°å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ã€‚åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å®ç°æ›´è‡ªç„¶ã€æ›´å‡†ç¡®çš„äººä½“å§¿æ€è¯†åˆ«ï¼Œä»è€Œæé«˜äº¤äº’ä½“éªŒã€‚åœ¨è™šæ‹Ÿç°å®ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–¹æ³•å®ç°æ›´é€¼çœŸçš„äººä½“åŠ¨ä½œæ•æ‰ï¼Œä»è€Œå¢å¼ºæ²‰æµ¸æ„Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.

