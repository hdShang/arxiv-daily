---
layout: default
title: From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation
---

# From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.19149" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.19149v1</a>
  <a href="https://arxiv.org/pdf/2511.19149.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19149v1" onclick="toggleFavorite(this, '2511.19149v1', 'From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Moazzam Umer Gondal, Hamad Ul Qudous, Daniya Siddiqui, Asma Ahmad Farhan

**åˆ†ç±»**: cs.CV, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

**å¤‡æ³¨**: Submitted to Expert Systems with Applications

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ£€ç´¢å¢å¼ºçš„æ—¶å°šæè¿°ä¸æ ‡ç­¾ç”Ÿæˆæ¡†æ¶ï¼Œæå‡å±æ€§ä¿çœŸåº¦å’Œé¢†åŸŸæ³›åŒ–æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ—¶å°šå›¾åƒæè¿°` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¤šæœè£…æ£€æµ‹` `å±æ€§æ¨ç†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœè£…æ ‡ç­¾ç”Ÿæˆ` `CLIP-FAISS` `é¢†åŸŸæ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç«¯åˆ°ç«¯æ—¶å°šå›¾åƒæè¿°æ¨¡å‹åœ¨å±æ€§ä¿çœŸåº¦å’Œé¢†åŸŸæ³›åŒ–æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å‡†ç¡®æè¿°æœè£…ç»†èŠ‚å’Œé£æ ¼ã€‚
2. æå‡ºä¸€ç§æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæœè£…æ£€æµ‹ã€å±æ€§æ¨ç†å’ŒLLMæç¤ºï¼Œç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å…·é£æ ¼çš„æ—¶å°šæè¿°å’Œæ ‡ç­¾ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å±æ€§è¦†ç›–ç‡å’Œäº‹å®åŸºç¡€æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹BLIPï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆæ—¶å°šå›¾åƒçš„æè¿°å’Œæ ‡ç­¾ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šæœè£…æ£€æµ‹ã€å±æ€§æ¨ç†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç¤ºã€‚æ—¨åœ¨ä¸ºæ—¶å°šå›¾åƒç”Ÿæˆè§†è§‰ä¸Šæœ‰å…³è”ã€æè¿°æ€§å¼ºä¸”é£æ ¼æœ‰è¶£çš„æ–‡æœ¬ï¼Œå…‹æœäº†ç«¯åˆ°ç«¯æè¿°å™¨åœ¨å±æ€§ä¿çœŸåº¦å’Œé¢†åŸŸæ³›åŒ–æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æµç¨‹ç»“åˆäº†åŸºäºYOLOçš„æ£€æµ‹å™¨è¿›è¡Œå¤šæœè£…å®šä½ï¼Œk-meansèšç±»æå–ä¸»è‰²è°ƒï¼Œä»¥åŠåŸºäºç»“æ„åŒ–äº§å“ç´¢å¼•çš„CLIP-FAISSæ£€ç´¢æ¨¡å—è¿›è¡Œé¢æ–™å’Œæ€§åˆ«å±æ€§æ¨æ–­ã€‚è¿™äº›å±æ€§ä¸æ£€ç´¢åˆ°çš„é£æ ¼ç¤ºä¾‹ä¸€èµ·ï¼Œåˆ›å»ºäº†ä¸€ä¸ªäº‹å®è¯æ®åŒ…ï¼Œç”¨äºå¼•å¯¼LLMç”Ÿæˆç±»ä¼¼äººç±»çš„æè¿°å’Œä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ ‡ç­¾ã€‚ä½¿ç”¨å¾®è°ƒçš„BLIPæ¨¡å‹ä½œä¸ºæœ‰ç›‘ç£çš„åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYOLOæ£€æµ‹å™¨åœ¨ä¹ä¸ªæœè£…ç±»åˆ«ä¸­è·å¾—äº†0.71çš„å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAP@0.5ï¼‰ã€‚RAG-LLMæµç¨‹ç”Ÿæˆäº†å¯Œæœ‰è¡¨ç°åŠ›çš„å±æ€§å¯¹é½æè¿°ï¼Œå¹¶åœ¨æ ‡ç­¾ç”Ÿæˆä¸­å®ç°äº†0.80çš„å¹³å‡å±æ€§è¦†ç›–ç‡ï¼Œåœ¨50%é˜ˆå€¼ä¸‹å®ç°äº†å®Œå…¨è¦†ç›–ï¼Œè€ŒBLIPæä¾›äº†æ›´é«˜çš„è¯æ±‡é‡å å’Œæ›´ä½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ£€ç´¢å¢å¼ºæ–¹æ³•è¡¨ç°å‡ºæ›´å¥½çš„äº‹å®åŸºç¡€ã€æ›´å°‘çš„å¹»è§‰ï¼Œå¹¶ä¸”åœ¨å„ç§æœè£…é¢†åŸŸå…·æœ‰å·¨å¤§çš„å¯æ‰©å±•éƒ¨ç½²æ½œåŠ›ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ£€ç´¢å¢å¼ºç”Ÿæˆä½œä¸ºä¸€ç§æœ‰æ•ˆä¸”å¯è§£é‡Šçš„èŒƒä¾‹ï¼Œç”¨äºè‡ªåŠ¨å’Œè§†è§‰åŸºç¡€çš„æ—¶å°šå†…å®¹ç”Ÿæˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ—¶å°šå›¾åƒæè¿°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œéš¾ä»¥ä¿è¯ç”Ÿæˆæè¿°çš„å±æ€§å‡†ç¡®æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒæœè£…é¢†åŸŸæ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚è¿™äº›æ¨¡å‹å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œæ— æ³•å‡†ç¡®æ•æ‰å›¾åƒä¸­æœè£…çš„ç»†èŠ‚å’Œé£æ ¼ç‰¹å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢ä¸å›¾åƒç›¸å…³çš„å±æ€§å’Œé£æ ¼ä¿¡æ¯ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›æ›´ä¸°å¯Œçš„äº‹å®ä¾æ®ï¼Œä»è€Œå¼•å¯¼LLMç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å…·é£æ ¼çš„æè¿°å’Œæ ‡ç­¾ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å‡å°‘å¹»è§‰ï¼Œæé«˜å±æ€§ä¿çœŸåº¦ï¼Œå¹¶å¢å¼ºé¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) åŸºäºYOLOçš„å¤šæœè£…æ£€æµ‹å™¨ï¼Œç”¨äºå®šä½å›¾åƒä¸­çš„å„ä¸ªæœè£…ï¼›2) k-meansèšç±»ï¼Œç”¨äºæå–æœè£…çš„ä¸»è‰²è°ƒï¼›3) CLIP-FAISSæ£€ç´¢æ¨¡å—ï¼ŒåŸºäºç»“æ„åŒ–äº§å“ç´¢å¼•æ¨æ–­æœè£…çš„é¢æ–™å’Œæ€§åˆ«å±æ€§ï¼›4) LLMï¼Œç”¨äºç”Ÿæˆæè¿°å’Œæ ‡ç­¾ï¼Œå…¶è¾“å…¥åŒ…æ‹¬æ£€æµ‹åˆ°çš„æœè£…ã€æå–çš„å±æ€§å’Œæ£€ç´¢åˆ°çš„é£æ ¼ç¤ºä¾‹ã€‚æ•´ä¸ªæµç¨‹é¦–å…ˆå¯¹å›¾åƒè¿›è¡Œåˆ†æï¼Œæå–ç›¸å…³ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯ä½œä¸ºæç¤ºè¾“å…¥LLMï¼Œæœ€åç”±LLMç”Ÿæˆæœ€ç»ˆçš„æè¿°å’Œæ ‡ç­¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ£€ç´¢å¢å¼ºç”Ÿæˆåº”ç”¨äºæ—¶å°šå›¾åƒæè¿°å’Œæ ‡ç­¾ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡æ£€ç´¢ä¸å›¾åƒç›¸å…³çš„å±æ€§å’Œé£æ ¼ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºLLMæä¾›æ›´ä¸°å¯Œçš„äº‹å®ä¾æ®ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å…·é£æ ¼çš„æè¿°å’Œæ ‡ç­¾ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¥½çš„å±æ€§ä¿çœŸåº¦ã€æ›´å°‘çš„å¹»è§‰å’Œæ›´å¼ºçš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨CLIP-FAISSæ£€ç´¢æ¨¡å—ä¸­ï¼Œæ„å»ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„äº§å“ç´¢å¼•ï¼Œç”¨äºå­˜å‚¨æœè£…çš„å±æ€§ä¿¡æ¯ã€‚åœ¨LLMæç¤ºæ–¹é¢ï¼Œè®¾è®¡äº†ä¸€ä¸ªæœ‰æ•ˆçš„äº‹å®è¯æ®åŒ…ï¼ŒåŒ…æ‹¬æ£€æµ‹åˆ°çš„æœè£…ã€æå–çš„å±æ€§å’Œæ£€ç´¢åˆ°çš„é£æ ¼ç¤ºä¾‹ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹BLIPæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä½œä¸ºæœ‰ç›‘ç£çš„åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¹ä¸ªæœè£…ç±»åˆ«ä¸­è·å¾—äº†0.71çš„å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAP@0.5ï¼‰ã€‚RAG-LLMæµç¨‹ç”Ÿæˆäº†å¯Œæœ‰è¡¨ç°åŠ›çš„å±æ€§å¯¹é½æè¿°ï¼Œå¹¶åœ¨æ ‡ç­¾ç”Ÿæˆä¸­å®ç°äº†0.80çš„å¹³å‡å±æ€§è¦†ç›–ç‡ï¼Œåœ¨50%é˜ˆå€¼ä¸‹å®ç°äº†å®Œå…¨è¦†ç›–ã€‚ä¸åŸºçº¿æ¨¡å‹BLIPç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ›´å¥½çš„å±æ€§ä¿çœŸåº¦ã€æ›´å°‘çš„å¹»è§‰å’Œæ›´å¼ºçš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç”µå•†å¹³å°ã€æ—¶å°šåšå®¢ã€ç¤¾äº¤åª’ä½“ç­‰é¢†åŸŸï¼Œè‡ªåŠ¨ç”Ÿæˆå•†å“æè¿°å’Œæ ‡ç­¾ï¼Œæé«˜å•†å“æ›å…‰ç‡å’Œç”¨æˆ·å‚ä¸åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºè¾…åŠ©æ—¶å°šè®¾è®¡å¸ˆè¿›è¡Œé£æ ¼åˆ†æå’Œçµæ„ŸæŒ–æ˜ï¼Œä»¥åŠä¸ºæ¶ˆè´¹è€…æä¾›ä¸ªæ€§åŒ–çš„æ—¶å°šæ¨èã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.

