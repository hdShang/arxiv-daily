---
layout: default
title: RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba
---

# RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16444" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16444v1</a>
  <a href="https://arxiv.org/pdf/2510.16444.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16444v1" onclick="toggleFavorite(this, '2510.16444v1', 'RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kunyu Peng, Di Wen, Jia Fu, Jiamin Wu, Kailun Yang, Junwei Zheng, Ruiping Liu, Yufan Chen, Yuqian Fu, Danda Pani Paudel, Luc Van Gool, Rainer Stiefelhagen

**åˆ†ç±»**: cs.CV, cs.MM, cs.RO, eess.IV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-18

**å¤‡æ³¨**: Extended version of ECCV 2024 paper arXiv:2407.01872. The dataset and code are released at https://github.com/KPeng9510/refAVA2

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/KPeng9510/refAVA2)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RefAtomNet++ï¼šåˆ©ç”¨è¯­ä¹‰æ£€ç´¢çš„å¤šè½¨è¿¹Mambaæ¨è¿›æŒ‡ä»£è¡¨è¾¾å¼åŸå­è§†é¢‘åŠ¨ä½œè¯†åˆ«**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æŒ‡ä»£è¡¨è¾¾å¼ç†è§£` `åŸå­åŠ¨ä½œè¯†åˆ«` `è§†é¢‘ç†è§£` `è·¨æ¨¡æ€å­¦ä¹ ` `Mambaæ¨¡å‹` `è¯­ä¹‰å¯¹é½` `å¤šå±‚æ¬¡å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰RAVARæ–¹æ³•åœ¨è·¨æ¨¡æ€ä¿¡æ¯å¯¹é½å’Œæ£€ç´¢æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥ç²¾ç¡®å®šä½ç›®æ ‡äººç‰©å’Œé¢„æµ‹ç»†ç²’åº¦åŠ¨ä½œã€‚
2. RefAtomNet++é€šè¿‡å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½çš„äº¤å‰æ³¨æ„æœºåˆ¶å’Œå¤šè½¨è¿¹Mambaå»ºæ¨¡ï¼Œå¢å¼ºè·¨æ¨¡æ€tokençš„èšåˆèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRefAtomNet++åœ¨RefAVA++æ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†RAVARçš„å‡†ç¡®æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æŒ‡ä»£è¡¨è¾¾å¼åŸå­è§†é¢‘åŠ¨ä½œè¯†åˆ«(RAVAR)æ—¨åœ¨è¯†åˆ«åœ¨è‡ªç„¶è¯­è¨€æè¿°æ¡ä»¶ä¸‹ï¼Œç‰¹å®šäººç‰©çš„ç»†ç²’åº¦åŸå­çº§åˆ«åŠ¨ä½œã€‚ä¸ä¼ ç»Ÿçš„åŠ¨ä½œè¯†åˆ«å’Œæ£€æµ‹ä»»åŠ¡ä¸åŒï¼ŒRAVARå¼ºè°ƒç²¾ç¡®çš„è¯­è¨€å¼•å¯¼åŠ¨ä½œç†è§£ï¼Œè¿™å¯¹äºå¤æ‚å¤šäººåœºæ™¯ä¸­çš„äº¤äº’å¼äººç±»åŠ¨ä½œåˆ†æè‡³å…³é‡è¦ã€‚æœ¬æ–‡æ‰©å±•äº†ä¹‹å‰æå‡ºçš„RefAVAæ•°æ®é›†åˆ°RefAVA++ï¼Œæ€»å…±åŒ…å«è¶…è¿‡290ä¸‡å¸§å’Œè¶…è¿‡75.1kä¸ªæ ‡æ³¨äººç‰©ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªå¤šä¸ªç›¸å…³é¢†åŸŸçš„åŸºçº¿æ¨¡å‹ï¼ˆåŒ…æ‹¬åŸå­åŠ¨ä½œå®šä½ã€è§†é¢‘é—®ç­”å’Œæ–‡æœ¬è§†é¢‘æ£€ç´¢ï¼‰ä»¥åŠæˆ‘ä»¬ä¹‹å‰çš„æ¨¡å‹RefAtomNetæ¥è¯„ä¼°è¯¥æ•°æ®é›†ã€‚è™½ç„¶RefAtomNeté€šè¿‡ç»“åˆä»£ç†æ³¨æ„åŠ›æ¥çªå‡ºæ˜¾è‘—ç‰¹å¾ï¼Œä»è€Œè¶…è¶Šäº†å…¶ä»–åŸºçº¿ï¼Œä½†å…¶å¯¹é½å’Œæ£€ç´¢è·¨æ¨¡æ€ä¿¡æ¯çš„èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œå¯¼è‡´åœ¨å®šä½ç›®æ ‡äººç‰©å’Œé¢„æµ‹ç»†ç²’åº¦åŠ¨ä½œæ–¹é¢çš„æ€§èƒ½æ¬ ä½³ã€‚ä¸ºäº†å…‹æœä¸Šè¿°é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†RefAtomNet++ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½çš„äº¤å‰æ³¨æ„æœºåˆ¶ä¸éƒ¨åˆ†å…³é”®è¯ã€åœºæ™¯å±æ€§å’Œæ•´ä½“å¥å­çº§åˆ«çš„å¤šè½¨è¿¹Mambaå»ºæ¨¡ç›¸ç»“åˆï¼Œä»è€Œæ¨è¿›äº†è·¨æ¨¡æ€tokenèšåˆã€‚ç‰¹åˆ«åœ°ï¼Œæ‰«æè½¨è¿¹æ˜¯é€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´æ­¥åŠ¨æ€é€‰æ‹©æœ€è¿‘çš„è§†è§‰ç©ºé—´tokenæ¥æ„å»ºçš„ï¼Œé€‚ç”¨äºéƒ¨åˆ†å…³é”®è¯å’Œåœºæ™¯å±æ€§çº§åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½çš„äº¤å‰æ³¨æ„ç­–ç•¥ï¼Œä»è€Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èšåˆè·¨ä¸åŒè¯­ä¹‰å±‚æ¬¡çš„ç©ºé—´å’Œæ—¶é—´tokenã€‚å®éªŒè¡¨æ˜ï¼ŒRefAtomNet++å»ºç«‹äº†æ–°çš„state-of-the-artç»“æœã€‚æ•°æ®é›†å’Œä»£ç å·²åœ¨https://github.com/KPeng9510/refAVA2ä¸Šå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æŒ‡ä»£è¡¨è¾¾å¼åŸå­è§†é¢‘åŠ¨ä½œè¯†åˆ«ï¼ˆRAVARï¼‰ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨è·¨æ¨¡æ€ä¿¡æ¯å¯¹é½å’Œæ£€ç´¢æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°å°†è‡ªç„¶è¯­è¨€æè¿°ä¸è§†é¢‘ä¸­çš„äººç‰©åŠ¨ä½œå…³è”èµ·æ¥ï¼Œå¯¼è‡´ç›®æ ‡äººç‰©å®šä½å’Œç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«çš„ç²¾åº¦ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½çš„äº¤å‰æ³¨æ„æœºåˆ¶å’Œå¤šè½¨è¿¹Mambaå»ºæ¨¡ï¼Œæ›´æœ‰æ•ˆåœ°èšåˆè·¨æ¨¡æ€çš„tokenä¿¡æ¯ã€‚é€šè¿‡åœ¨éƒ¨åˆ†å…³é”®è¯ã€åœºæ™¯å±æ€§å’Œæ•´ä½“å¥å­çº§åˆ«ä¸Šè¿›è¡Œå»ºæ¨¡ï¼Œå®ç°æ›´ç²¾ç»†çš„è¯­ä¹‰ç†è§£å’Œæ›´å‡†ç¡®çš„åŠ¨ä½œè¯†åˆ«ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRefAtomNet++çš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼Œç”¨äºæå–è§†é¢‘å¸§å’Œæ–‡æœ¬æè¿°çš„ç‰¹å¾ï¼›2) å¤šè½¨è¿¹Mambaå»ºæ¨¡æ¨¡å—ï¼Œç”¨äºåœ¨éƒ¨åˆ†å…³é”®è¯å’Œåœºæ™¯å±æ€§çº§åˆ«ä¸Šæ„å»ºæ‰«æè½¨è¿¹ï¼ŒåŠ¨æ€é€‰æ‹©æœ€è¿‘çš„è§†è§‰ç©ºé—´tokenï¼›3) å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½çš„äº¤å‰æ³¨æ„æ¨¡å—ï¼Œç”¨äºèšåˆè·¨ä¸åŒè¯­ä¹‰å±‚æ¬¡çš„ç©ºé—´å’Œæ—¶é—´tokenï¼›4) åŠ¨ä½œé¢„æµ‹æ¨¡å—ï¼Œç”¨äºé¢„æµ‹ç›®æ ‡äººç‰©çš„åŸå­åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šRefAtomNet++çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†å¤šè½¨è¿¹Mambaå»ºæ¨¡ï¼Œèƒ½å¤ŸåŠ¨æ€åœ°å…³æ³¨ä¸å…³é”®è¯å’Œåœºæ™¯å±æ€§ç›¸å…³çš„è§†è§‰åŒºåŸŸï¼›2) è®¾è®¡äº†å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½çš„äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èšåˆæ¥è‡ªä¸åŒè¯­ä¹‰å±‚æ¬¡çš„ä¿¡æ¯ï¼Œä»è€Œæå‡è·¨æ¨¡æ€å¯¹é½çš„ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¤šè½¨è¿¹Mambaå»ºæ¨¡ä¸­ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©æœ€è¿‘çš„è§†è§‰ç©ºé—´tokenæ¥æ„å»ºæ‰«æè½¨è¿¹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ä¸æ–‡æœ¬æè¿°ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚åœ¨å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½çš„äº¤å‰æ³¨æ„æœºåˆ¶ä¸­ï¼Œä½¿ç”¨äº†ä¸åŒçš„æ³¨æ„åŠ›æƒé‡æ¥èšåˆæ¥è‡ªä¸åŒè¯­ä¹‰å±‚æ¬¡çš„ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´ç²¾ç»†çš„è¯­ä¹‰ç†è§£ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RefAtomNet++åœ¨RefAVA++æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå»ºç«‹äº†æ–°çš„state-of-the-artç»“æœã€‚ç›¸è¾ƒäºä¹‹å‰çš„RefAtomNetæ¨¡å‹å’Œå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ŒRefAtomNet++åœ¨ç›®æ ‡äººç‰©å®šä½å’Œç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«æ–¹é¢å‡æœ‰æ˜æ˜¾æ”¹å–„ï¼ŒéªŒè¯äº†å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½äº¤å‰æ³¨æ„æœºåˆ¶å’Œå¤šè½¨è¿¹Mambaå»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½ç›‘æ§ã€äººæœºäº¤äº’ã€è§†é¢‘å†…å®¹åˆ†æç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æŠ€æœ¯è¯†åˆ«ç‰¹å®šäººå‘˜çš„å¼‚å¸¸è¡Œä¸ºï¼›åœ¨äººæœºäº¤äº’ä¸­ï¼Œå¯ä»¥å®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„äº¤äº’æ–¹å¼ï¼›åœ¨è§†é¢‘å†…å®¹åˆ†æä¸­ï¼Œå¯ä»¥è‡ªåŠ¨è¯†åˆ«è§†é¢‘ä¸­çš„äººç‰©åŠ¨ä½œï¼Œä»è€Œæé«˜è§†é¢‘æ£€ç´¢å’Œç†è§£çš„æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Referring Atomic Video Action Recognition (RAVAR) aims to recognize fine-grained, atomic-level actions of a specific person of interest conditioned on natural language descriptions. Distinct from conventional action recognition and detection tasks, RAVAR emphasizes precise language-guided action understanding, which is particularly critical for interactive human action analysis in complex multi-person scenarios. In this work, we extend our previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million frames and >75.1k annotated persons in total. We benchmark this dataset using baselines from multiple related domains, including atomic action localization, video question answering, and text-video retrieval, as well as our earlier model, RefAtomNet. Although RefAtomNet surpasses other baselines by incorporating agent attention to highlight salient features, its ability to align and retrieve cross-modal information remains limited, leading to suboptimal performance in localizing the target person and predicting fine-grained actions. To overcome the aforementioned limitations, we introduce RefAtomNet++, a novel framework that advances cross-modal token aggregation through a multi-hierarchical semantic-aligned cross-attention mechanism combined with multi-trajectory Mamba modeling at the partial-keyword, scene-attribute, and holistic-sentence levels. In particular, scanning trajectories are constructed by dynamically selecting the nearest visual spatial tokens at each timestep for both partial-keyword and scene-attribute levels. Moreover, we design a multi-hierarchical semantic-aligned cross-attention strategy, enabling more effective aggregation of spatial and temporal tokens across different semantic hierarchies. Experiments show that RefAtomNet++ establishes new state-of-the-art results. The dataset and code are released at https://github.com/KPeng9510/refAVA2.

