---
layout: default
title: RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba
---

# RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.16444" target="_blank" class="toolbar-btn">arXiv: 2510.16444v1</a>
    <a href="https://arxiv.org/pdf/2510.16444.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16444v1" 
            onclick="toggleFavorite(this, '2510.16444v1', 'RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kunyu Peng, Di Wen, Jia Fu, Jiamin Wu, Kailun Yang, Junwei Zheng, Ruiping Liu, Yufan Chen, Yuqian Fu, Danda Pani Paudel, Luc Van Gool, Rainer Stiefelhagen

**ÂàÜÁ±ª**: cs.CV, cs.MM, cs.RO, eess.IV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-18

**Â§áÊ≥®**: Extended version of ECCV 2024 paper arXiv:2407.01872. The dataset and code are released at https://github.com/KPeng9510/refAVA2

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/KPeng9510/refAVA2)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RefAtomNet++ÔºöÂà©Áî®ËØ≠‰πâÊ£ÄÁ¥¢ÁöÑÂ§öËΩ®ËøπMambaÊé®ËøõÊåá‰ª£Ë°®ËææÂºèÂéüÂ≠êËßÜÈ¢ëÂä®‰ΩúËØÜÂà´**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êåá‰ª£Ë°®ËææÂºèÁêÜËß£` `ÂéüÂ≠êÂä®‰ΩúËØÜÂà´` `ËßÜÈ¢ëÁêÜËß£` `Ë∑®Ê®°ÊÄÅÂ≠¶‰π†` `MambaÊ®°Âûã` `ËØ≠‰πâÂØπÈΩê` `Â§öÂ±ÇÊ¨°Âª∫Ê®°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâRAVARÊñπÊ≥ïÂú®Ë∑®Ê®°ÊÄÅ‰ø°ÊÅØÂØπÈΩêÂíåÊ£ÄÁ¥¢ÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºåÈöæ‰ª•Á≤æÁ°ÆÂÆö‰ΩçÁõÆÊ†á‰∫∫Áâ©ÂíåÈ¢ÑÊµãÁªÜÁ≤íÂ∫¶Âä®‰Ωú„ÄÇ
2. RefAtomNet++ÈÄöËøáÂ§öÂ±ÇÊ¨°ËØ≠‰πâÂØπÈΩêÁöÑ‰∫§ÂèâÊ≥®ÊÑèÊú∫Âà∂ÂíåÂ§öËΩ®ËøπMambaÂª∫Ê®°ÔºåÂ¢ûÂº∫Ë∑®Ê®°ÊÄÅtokenÁöÑËÅöÂêàËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRefAtomNet++Âú®RefAVA++Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Üstate-of-the-artÁöÑÊÄßËÉΩÔºåÊòæËëóÊèêÂçá‰∫ÜRAVARÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êåá‰ª£Ë°®ËææÂºèÂéüÂ≠êËßÜÈ¢ëÂä®‰ΩúËØÜÂà´(RAVAR)Êó®Âú®ËØÜÂà´Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Êù°‰ª∂‰∏ãÔºåÁâπÂÆö‰∫∫Áâ©ÁöÑÁªÜÁ≤íÂ∫¶ÂéüÂ≠êÁ∫ßÂà´Âä®‰Ωú„ÄÇ‰∏é‰º†ÁªüÁöÑÂä®‰ΩúËØÜÂà´ÂíåÊ£ÄÊµã‰ªªÂä°‰∏çÂêåÔºåRAVARÂº∫Ë∞ÉÁ≤æÁ°ÆÁöÑËØ≠Ë®ÄÂºïÂØºÂä®‰ΩúÁêÜËß£ÔºåËøôÂØπ‰∫éÂ§çÊùÇÂ§ö‰∫∫Âú∫ÊôØ‰∏≠ÁöÑ‰∫§‰∫íÂºè‰∫∫Á±ªÂä®‰ΩúÂàÜÊûêËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊâ©Â±ï‰∫Ü‰πãÂâçÊèêÂá∫ÁöÑRefAVAÊï∞ÊçÆÈõÜÂà∞RefAVA++ÔºåÊÄªÂÖ±ÂåÖÂê´Ë∂ÖËøá290‰∏áÂ∏ßÂíåË∂ÖËøá75.1k‰∏™Ê†áÊ≥®‰∫∫Áâ©„ÄÇÊàë‰ª¨‰ΩøÁî®Êù•Ëá™Â§ö‰∏™Áõ∏ÂÖ≥È¢ÜÂüüÁöÑÂü∫Á∫øÊ®°ÂûãÔºàÂåÖÊã¨ÂéüÂ≠êÂä®‰ΩúÂÆö‰Ωç„ÄÅËßÜÈ¢ëÈóÆÁ≠îÂíåÊñáÊú¨ËßÜÈ¢ëÊ£ÄÁ¥¢Ôºâ‰ª•ÂèäÊàë‰ª¨‰πãÂâçÁöÑÊ®°ÂûãRefAtomNetÊù•ËØÑ‰º∞ËØ•Êï∞ÊçÆÈõÜ„ÄÇËôΩÁÑ∂RefAtomNetÈÄöËøáÁªìÂêà‰ª£ÁêÜÊ≥®ÊÑèÂäõÊù•Á™ÅÂá∫ÊòæËëóÁâπÂæÅÔºå‰ªéËÄåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂü∫Á∫øÔºå‰ΩÜÂÖ∂ÂØπÈΩêÂíåÊ£ÄÁ¥¢Ë∑®Ê®°ÊÄÅ‰ø°ÊÅØÁöÑËÉΩÂäõ‰ªçÁÑ∂ÊúâÈôêÔºåÂØºËá¥Âú®ÂÆö‰ΩçÁõÆÊ†á‰∫∫Áâ©ÂíåÈ¢ÑÊµãÁªÜÁ≤íÂ∫¶Âä®‰ΩúÊñπÈù¢ÁöÑÊÄßËÉΩÊ¨†‰Ω≥„ÄÇ‰∏∫‰∫ÜÂÖãÊúç‰∏äËø∞ÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜRefAtomNet++ÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÂ§öÂ±ÇÊ¨°ËØ≠‰πâÂØπÈΩêÁöÑ‰∫§ÂèâÊ≥®ÊÑèÊú∫Âà∂‰∏éÈÉ®ÂàÜÂÖ≥ÈîÆËØç„ÄÅÂú∫ÊôØÂ±ûÊÄßÂíåÊï¥‰ΩìÂè•Â≠êÁ∫ßÂà´ÁöÑÂ§öËΩ®ËøπMambaÂª∫Ê®°Áõ∏ÁªìÂêàÔºå‰ªéËÄåÊé®Ëøõ‰∫ÜË∑®Ê®°ÊÄÅtokenËÅöÂêà„ÄÇÁâπÂà´Âú∞ÔºåÊâ´ÊèèËΩ®ËøπÊòØÈÄöËøáÂú®ÊØè‰∏™Êó∂Èó¥Ê≠•Âä®ÊÄÅÈÄâÊã©ÊúÄËøëÁöÑËßÜËßâÁ©∫Èó¥tokenÊù•ÊûÑÂª∫ÁöÑÔºåÈÄÇÁî®‰∫éÈÉ®ÂàÜÂÖ≥ÈîÆËØçÂíåÂú∫ÊôØÂ±ûÊÄßÁ∫ßÂà´„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂ§öÂ±ÇÊ¨°ËØ≠‰πâÂØπÈΩêÁöÑ‰∫§ÂèâÊ≥®ÊÑèÁ≠ñÁï•Ôºå‰ªéËÄåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËÅöÂêàË∑®‰∏çÂêåËØ≠‰πâÂ±ÇÊ¨°ÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥token„ÄÇÂÆûÈ™åË°®ÊòéÔºåRefAtomNet++Âª∫Á´ã‰∫ÜÊñ∞ÁöÑstate-of-the-artÁªìÊûú„ÄÇÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÂ∑≤Âú®https://github.com/KPeng9510/refAVA2‰∏äÂèëÂ∏É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êåá‰ª£Ë°®ËææÂºèÂéüÂ≠êËßÜÈ¢ëÂä®‰ΩúËØÜÂà´ÔºàRAVARÔºâ‰ªªÂä°‰∏≠ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Ë∑®Ê®°ÊÄÅ‰ø°ÊÅØÂØπÈΩêÂíåÊ£ÄÁ¥¢ÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂú∞Â∞ÜËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰∏éËßÜÈ¢ë‰∏≠ÁöÑ‰∫∫Áâ©Âä®‰ΩúÂÖ≥ËÅîËµ∑Êù•ÔºåÂØºËá¥ÁõÆÊ†á‰∫∫Áâ©ÂÆö‰ΩçÂíåÁªÜÁ≤íÂ∫¶Âä®‰ΩúËØÜÂà´ÁöÑÁ≤æÂ∫¶‰∏çÈ´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§öÂ±ÇÊ¨°ËØ≠‰πâÂØπÈΩêÁöÑ‰∫§ÂèâÊ≥®ÊÑèÊú∫Âà∂ÂíåÂ§öËΩ®ËøπMambaÂª∫Ê®°ÔºåÊõ¥ÊúâÊïàÂú∞ËÅöÂêàË∑®Ê®°ÊÄÅÁöÑtoken‰ø°ÊÅØ„ÄÇÈÄöËøáÂú®ÈÉ®ÂàÜÂÖ≥ÈîÆËØç„ÄÅÂú∫ÊôØÂ±ûÊÄßÂíåÊï¥‰ΩìÂè•Â≠êÁ∫ßÂà´‰∏äËøõË°åÂª∫Ê®°ÔºåÂÆûÁé∞Êõ¥Á≤æÁªÜÁöÑËØ≠‰πâÁêÜËß£ÂíåÊõ¥ÂáÜÁ°ÆÁöÑÂä®‰ΩúËØÜÂà´„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRefAtomNet++ÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÊ®°ÂùóÔºåÁî®‰∫éÊèêÂèñËßÜÈ¢ëÂ∏ßÂíåÊñáÊú¨ÊèèËø∞ÁöÑÁâπÂæÅÔºõ2) Â§öËΩ®ËøπMambaÂª∫Ê®°Ê®°ÂùóÔºåÁî®‰∫éÂú®ÈÉ®ÂàÜÂÖ≥ÈîÆËØçÂíåÂú∫ÊôØÂ±ûÊÄßÁ∫ßÂà´‰∏äÊûÑÂª∫Êâ´ÊèèËΩ®ËøπÔºåÂä®ÊÄÅÈÄâÊã©ÊúÄËøëÁöÑËßÜËßâÁ©∫Èó¥tokenÔºõ3) Â§öÂ±ÇÊ¨°ËØ≠‰πâÂØπÈΩêÁöÑ‰∫§ÂèâÊ≥®ÊÑèÊ®°ÂùóÔºåÁî®‰∫éËÅöÂêàË∑®‰∏çÂêåËØ≠‰πâÂ±ÇÊ¨°ÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥tokenÔºõ4) Âä®‰ΩúÈ¢ÑÊµãÊ®°ÂùóÔºåÁî®‰∫éÈ¢ÑÊµãÁõÆÊ†á‰∫∫Áâ©ÁöÑÂéüÂ≠êÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRefAtomNet++ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫ÜÂ§öËΩ®ËøπMambaÂª∫Ê®°ÔºåËÉΩÂ§üÂä®ÊÄÅÂú∞ÂÖ≥Ê≥®‰∏éÂÖ≥ÈîÆËØçÂíåÂú∫ÊôØÂ±ûÊÄßÁõ∏ÂÖ≥ÁöÑËßÜËßâÂå∫ÂüüÔºõ2) ËÆæËÆ°‰∫ÜÂ§öÂ±ÇÊ¨°ËØ≠‰πâÂØπÈΩêÁöÑ‰∫§ÂèâÊ≥®ÊÑèÊú∫Âà∂ÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËÅöÂêàÊù•Ëá™‰∏çÂêåËØ≠‰πâÂ±ÇÊ¨°ÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÂçáË∑®Ê®°ÊÄÅÂØπÈΩêÁöÑÁ≤æÂ∫¶„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Â§öËΩ®ËøπMambaÂª∫Ê®°‰∏≠ÔºåÈÄöËøáÂä®ÊÄÅÈÄâÊã©ÊúÄËøëÁöÑËßÜËßâÁ©∫Èó¥tokenÊù•ÊûÑÂª∫Êâ´ÊèèËΩ®ËøπÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂÖ≥Ê≥®‰∏éÊñáÊú¨ÊèèËø∞Áõ∏ÂÖ≥ÁöÑËßÜËßâÂå∫Âüü„ÄÇÂú®Â§öÂ±ÇÊ¨°ËØ≠‰πâÂØπÈΩêÁöÑ‰∫§ÂèâÊ≥®ÊÑèÊú∫Âà∂‰∏≠Ôºå‰ΩøÁî®‰∫Ü‰∏çÂêåÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÊù•ËÅöÂêàÊù•Ëá™‰∏çÂêåËØ≠‰πâÂ±ÇÊ¨°ÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÁªÜÁöÑËØ≠‰πâÁêÜËß£„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

RefAtomNet++Âú®RefAVA++Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂª∫Á´ã‰∫ÜÊñ∞ÁöÑstate-of-the-artÁªìÊûú„ÄÇÁõ∏ËæÉ‰∫é‰πãÂâçÁöÑRefAtomNetÊ®°ÂûãÂíåÂÖ∂‰ªñÂü∫Á∫øÊñπÊ≥ïÔºåRefAtomNet++Âú®ÁõÆÊ†á‰∫∫Áâ©ÂÆö‰ΩçÂíåÁªÜÁ≤íÂ∫¶Âä®‰ΩúËØÜÂà´ÊñπÈù¢ÂùáÊúâÊòéÊòæÊîπÂñÑÔºåÈ™åËØÅ‰∫ÜÂ§öÂ±ÇÊ¨°ËØ≠‰πâÂØπÈΩê‰∫§ÂèâÊ≥®ÊÑèÊú∫Âà∂ÂíåÂ§öËΩ®ËøπMambaÂª∫Ê®°ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫í„ÄÅËßÜÈ¢ëÂÜÖÂÆπÂàÜÊûêÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂú®Êô∫ËÉΩÁõëÊéß‰∏≠ÔºåÂèØ‰ª•Âà©Áî®ËØ•ÊäÄÊúØËØÜÂà´ÁâπÂÆö‰∫∫ÂëòÁöÑÂºÇÂ∏∏Ë°å‰∏∫ÔºõÂú®‰∫∫Êú∫‰∫§‰∫í‰∏≠ÔºåÂèØ‰ª•ÂÆûÁé∞Êõ¥Ëá™ÁÑ∂„ÄÅÊõ¥Êô∫ËÉΩÁöÑ‰∫§‰∫íÊñπÂºèÔºõÂú®ËßÜÈ¢ëÂÜÖÂÆπÂàÜÊûê‰∏≠ÔºåÂèØ‰ª•Ëá™Âä®ËØÜÂà´ËßÜÈ¢ë‰∏≠ÁöÑ‰∫∫Áâ©Âä®‰ΩúÔºå‰ªéËÄåÊèêÈ´òËßÜÈ¢ëÊ£ÄÁ¥¢ÂíåÁêÜËß£ÁöÑÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Referring Atomic Video Action Recognition (RAVAR) aims to recognize fine-grained, atomic-level actions of a specific person of interest conditioned on natural language descriptions. Distinct from conventional action recognition and detection tasks, RAVAR emphasizes precise language-guided action understanding, which is particularly critical for interactive human action analysis in complex multi-person scenarios. In this work, we extend our previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million frames and >75.1k annotated persons in total. We benchmark this dataset using baselines from multiple related domains, including atomic action localization, video question answering, and text-video retrieval, as well as our earlier model, RefAtomNet. Although RefAtomNet surpasses other baselines by incorporating agent attention to highlight salient features, its ability to align and retrieve cross-modal information remains limited, leading to suboptimal performance in localizing the target person and predicting fine-grained actions. To overcome the aforementioned limitations, we introduce RefAtomNet++, a novel framework that advances cross-modal token aggregation through a multi-hierarchical semantic-aligned cross-attention mechanism combined with multi-trajectory Mamba modeling at the partial-keyword, scene-attribute, and holistic-sentence levels. In particular, scanning trajectories are constructed by dynamically selecting the nearest visual spatial tokens at each timestep for both partial-keyword and scene-attribute levels. Moreover, we design a multi-hierarchical semantic-aligned cross-attention strategy, enabling more effective aggregation of spatial and temporal tokens across different semantic hierarchies. Experiments show that RefAtomNet++ establishes new state-of-the-art results. The dataset and code are released at https://github.com/KPeng9510/refAVA2.

