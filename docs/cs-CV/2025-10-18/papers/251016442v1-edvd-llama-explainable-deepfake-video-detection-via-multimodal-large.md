---
layout: default
title: "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning"
---

# EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16442" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.16442v1</a>
  <a href="https://arxiv.org/pdf/2510.16442.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16442v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.16442v1', 'EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Haoran Sun, Chen Cai, Huiping Zhuang, Kong Aik Lee, Lap-Pui Chau, Yi Wang

**ÂàÜÁ±ª**: cs.CV, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-18

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫EDVD-LLaMAÊ°ÜÊû∂ÔºåÈÄöËøáÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÂÆûÁé∞ÂèØËß£ÈáäÁöÑDeepfakeËßÜÈ¢ëÊ£ÄÊµã„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `DeepfakeÊ£ÄÊµã` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ÂèØËß£ÈáäÊÄß` `ÊÄùÁª¥Èìæ` `Êó∂Á©∫ÁâπÂæÅ` `Èù¢ÈÉ®ÁâπÂæÅ` `ËßÜÈ¢ëÂàÜÊûê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâDeepfakeÊ£ÄÊµãÊñπÊ≥ïÁº∫‰πèÈÄèÊòéÊÄßÔºåÈöæ‰ª•Ëß£ÈáäÊ£ÄÊµãÁªìÊûúÔºå‰∏îÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥‰ª•Â∫îÂØπ‰∏çÊñ≠ÊºîËøõÁöÑ‰º™ÈÄ†ÊäÄÊúØ„ÄÇ
2. ÊèêÂá∫EDVD-LLaMAÊ°ÜÊû∂ÔºåÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°åÊé®ÁêÜÔºåÊèê‰æõÂèØËøΩÊ∫ØÁöÑÊé®ÁêÜËøáÁ®ãÂíåÂèØ‰ø°ÁöÑËß£ÈáäÔºåÂ¢ûÂº∫Ê£ÄÊµãÂèØËß£ÈáäÊÄß„ÄÇ
3. ÊûÑÂª∫ER-FF++Êï∞ÊçÆÈõÜÔºåÂπ∂ËÆæËÆ°ST-SITÂíåFg-MCoTÊú∫Âà∂ÔºåÂÆûÈ™åË°®ÊòéEDVD-LLaMAÂú®Ê£ÄÊµãÁ≤æÂ∫¶„ÄÅÂèØËß£ÈáäÊÄßÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê∑±Â∫¶‰º™ÈÄ†ËßÜÈ¢ëÊäÄÊúØÁöÑÂø´ÈÄüÂèëÂ±ïÂú®‰øÉËøõËâ∫ÊúØÂàõ‰ΩúÁöÑÂêåÊó∂Ôºå‰πü‰ΩøÂæóËôöÂÅá‰ø°ÊÅØÁöÑ‰º†Êí≠ÂèòÂæóÊõ¥Âä†ÂÆπÊòì„ÄÇ‰º†ÁªüÁöÑÊ∑±Â∫¶‰º™ÈÄ†ËßÜÈ¢ëÊ£ÄÊµã(DVD)ÊñπÊ≥ïÈù¢‰∏¥ÁùÄÂéüÁêÜÁº∫‰πèÈÄèÊòéÊÄß‰ª•ÂèäÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥‰ª•Â∫îÂØπ‰∏çÊñ≠ÂèëÂ±ïÁöÑ‰º™ÈÄ†ÊäÄÊúØÁ≠âÈóÆÈ¢ò„ÄÇËøôÁ™ÅÊòæ‰∫ÜÂØπËÉΩÂ§üËØÜÂà´‰º™ÈÄ†ÂÜÖÂÆπÂπ∂Êèê‰æõÂèØÈ™åËØÅÁöÑÊé®ÁêÜËØ¥ÊòéÁöÑÊ£ÄÊµãÂô®ÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂèØËß£ÈáäÁöÑÊ∑±Â∫¶‰º™ÈÄ†ËßÜÈ¢ëÊ£ÄÊµã(EDVD)‰ªªÂä°ÔºåÂπ∂ËÆæËÆ°‰∫ÜEDVD-LLaMAÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLM)Êé®ÁêÜÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âú®Êèê‰æõÂáÜÁ°ÆÊ£ÄÊµãÁªìÊûúÂíåÂèØ‰ø°Ëß£ÈáäÁöÑÂêåÊó∂ÔºåËøòÊèê‰æõ‰∫ÜÂèØËøΩÊ∫ØÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈ¶ñÂÖàÁªìÂêà‰∫ÜÊó∂Á©∫ÂæÆÂ¶ô‰ø°ÊÅØTokenization (ST-SIT)Êù•ÊèêÂèñÂíåËûçÂêàÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Ë∑®Â∏ßÊ∑±Â∫¶‰º™ÈÄ†ÁâπÂæÅÔºå‰∏∫MLLMÊé®ÁêÜÊèê‰æõ‰∏∞ÂØåÁöÑÊó∂Á©∫ËØ≠‰πâ‰ø°ÊÅØËæìÂÖ•„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÁªÜÁ≤íÂ∫¶ÁöÑÂ§öÊ®°ÊÄÅÊÄùÁª¥Èìæ(Fg-MCoT)Êú∫Âà∂ÔºåËØ•Êú∫Âà∂Âú®Êé®ÁêÜËøáÁ®ã‰∏≠ÂºïÂÖ•Èù¢ÈÉ®ÁâπÂæÅÊï∞ÊçÆ‰Ωú‰∏∫Á°¨Á∫¶ÊùüÔºå‰ª•ÂÆûÁé∞ÂÉèÁ¥†Á∫ßÁöÑÊó∂Á©∫ËßÜÈ¢ëÂÆö‰ΩçÔºåÊäëÂà∂ÂπªËßâËæìÂá∫ÔºåÂπ∂ÊèêÈ´òÊÄùÁª¥ÈìæÁöÑÂèØÈù†ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂèØËß£ÈáäÊé®ÁêÜFF++Âü∫ÂáÜÊï∞ÊçÆÈõÜ(ER-FF++set)ÔºåÂà©Áî®ÁªìÊûÑÂåñÊï∞ÊçÆÊù•Ê≥®ÈáäËßÜÈ¢ëÂπ∂Á°Æ‰øùË¥®ÈáèÊéßÂà∂Ôºå‰ªéËÄåÊîØÊåÅÂØπÊé®ÁêÜÂíåÊ£ÄÊµãÁöÑÂèåÈáçÁõëÁù£„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åË°®ÊòéÔºåEDVD-LLaMAÂú®Ê£ÄÊµãÁ≤æÂ∫¶„ÄÅÂèØËß£ÈáäÊÄß‰ª•ÂèäÂ§ÑÁêÜË∑®‰º™ÈÄ†ÊñπÊ≥ïÂíåË∑®Êï∞ÊçÆÈõÜÂú∫ÊôØÁöÑËÉΩÂäõÊñπÈù¢ÈÉΩÂèñÂæó‰∫ÜÂá∫Ëâ≤ÁöÑÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇ‰∏é‰πãÂâçÁöÑDVDÊñπÊ≥ïÁõ∏ÊØîÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êõ¥ÂèØËß£ÈáäÂíåÊõ¥‰ºòË∂äÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÊ∫ê‰ª£Á†ÅÂíåÊï∞ÊçÆÈõÜÂ∞ÜÂÖ¨ÂºÄÊèê‰æõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâDeepfakeËßÜÈ¢ëÊ£ÄÊµãÊñπÊ≥ïÁº∫‰πèÂèØËß£ÈáäÊÄßÂíåÊ≥õÂåñËÉΩÂäõÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÊòØÈªëÁõíÊ®°ÂûãÔºåÈöæ‰ª•ÁêÜËß£ÂÖ∂ÂÜ≥Á≠ñËøáÁ®ãÔºåÂπ∂‰∏îÂú®Èù¢ÂØπÊñ∞ÁöÑ‰º™ÈÄ†ÊäÄÊúØÊàñË∑®Êï∞ÊçÆÈõÜÂú∫ÊôØÊó∂ÔºåÊÄßËÉΩ‰ºöÊòæËëó‰∏ãÈôç„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§üÊèê‰æõÂèØÈ™åËØÅÊé®ÁêÜËøáÁ®ãÁöÑ„ÄÅÈ≤ÅÊ£íÊÄßÊõ¥Âº∫ÁöÑÊ£ÄÊµãÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(MLLM)ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂ∞ÜDeepfakeËßÜÈ¢ëÊ£ÄÊµãËΩ¨Âåñ‰∏∫‰∏Ä‰∏™ÂèØËß£ÈáäÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÈÄöËøáÂºïÂÖ•Êó∂Á©∫ÁâπÂæÅÂíåÈù¢ÈÉ®ÁâπÂæÅ‰Ωú‰∏∫Á∫¶ÊùüÔºåÂºïÂØºMLLMËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑÂàÜÊûêÂíåÂà§Êñ≠Ôºå‰ªéËÄåÊèêÈ´òÊ£ÄÊµãÁöÑÂáÜÁ°ÆÊÄßÂíåÂèØ‰ø°Â∫¶„ÄÇËøôÁßçÊñπÊ≥ïÂÄüÈâ¥‰∫Ü‰∫∫Á±ª‰∏ìÂÆ∂ÈÄöËøáËßÇÂØüËßÜÈ¢ëÁªÜËäÇÂπ∂ËøõË°åÈÄªËæëÊé®ÁêÜÊù•Âà§Êñ≠Áúü‰º™ÁöÑÊÄùË∑Ø„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöEDVD-LLaMAÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™Ê®°ÂùóÔºö1) **ST-SIT (Spatio-Temporal Subtle Information Tokenization)**ÔºöÊèêÂèñÂíåËûçÂêàËßÜÈ¢ë‰∏≠ÁöÑÊó∂Á©∫ÁâπÂæÅÔºå‰∏∫MLLMÊèê‰æõ‰∏∞ÂØåÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇ2) **Fg-MCoT (Fine-grained Multimodal Chain-of-Thought)**ÔºöÊûÑÂª∫ÁªÜÁ≤íÂ∫¶ÁöÑÂ§öÊ®°ÊÄÅÊÄùÁª¥ÈìæÔºåÂà©Áî®Èù¢ÈÉ®ÁâπÂæÅ‰Ωú‰∏∫Á°¨Á∫¶ÊùüÔºåÂºïÂØºMLLMËøõË°åÊé®ÁêÜ„ÄÇ3) **MLLM (Large Language Model)**Ôºö‰ΩøÁî®LLaMA‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºåËøõË°åÂ§öÊ®°ÊÄÅËæìÂÖ•ÂíåÊé®ÁêÜ„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÔºöËæìÂÖ•ËßÜÈ¢ëÂ∏ßÔºåÈÄöËøáST-SITÊèêÂèñÁâπÂæÅÔºåÁÑ∂ÂêéÂà©Áî®Fg-MCoTÂºïÂØºMLLMËøõË°åÊé®ÁêÜÔºåÊúÄÁªàËæìÂá∫Ê£ÄÊµãÁªìÊûúÂíåËß£Èáä„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜMLLMÂºïÂÖ•DeepfakeËßÜÈ¢ëÊ£ÄÊµãÔºåÂπ∂ËÆæËÆ°‰∫ÜST-SITÂíåFg-MCoTÊú∫Âà∂Êù•Â¢ûÂº∫MLLMÁöÑÊé®ÁêÜËÉΩÂäõÂíåÂèØËß£ÈáäÊÄß„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåEDVD-LLaMA‰∏ç‰ªÖËÉΩÂ§üÁªôÂá∫Ê£ÄÊµãÁªìÊûúÔºåËøòËÉΩÊèê‰æõÂèØËøΩÊ∫ØÁöÑÊé®ÁêÜËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ£ÄÊµãÁöÑÂèØ‰ø°Â∫¶„ÄÇÊ≠§Â§ñÔºåFg-MCoTÊú∫Âà∂ÈÄöËøáÂºïÂÖ•Èù¢ÈÉ®ÁâπÂæÅ‰Ωú‰∏∫Á°¨Á∫¶ÊùüÔºåÊúâÊïàÂú∞ÊäëÂà∂‰∫ÜMLLMÁöÑÂπªËßâËæìÂá∫ÔºåÊèêÈ´ò‰∫ÜÊé®ÁêÜÁöÑÂèØÈù†ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöST-SITÊ®°ÂùóÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇÊú™Áü•Ôºå‰ΩÜÂÖ∂ÁõÆÊ†áÊòØÊèêÂèñÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Ë∑®Â∏ßÁöÑDeepfakeÁâπÂæÅ„ÄÇFg-MCoTÊú∫Âà∂ÁöÑÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïÂ∞ÜÈù¢ÈÉ®ÁâπÂæÅÊúâÊïàÂú∞ËûçÂÖ•Âà∞MLLMÁöÑÊé®ÁêÜËøáÁ®ã‰∏≠ÔºåÂÖ∑‰ΩìÂÆûÁé∞ÁªÜËäÇ‰πüÊú™Áü•„ÄÇER-FF++Êï∞ÊçÆÈõÜÁöÑÊûÑÂª∫‰ΩøÁî®‰∫ÜÁªìÊûÑÂåñÊï∞ÊçÆËøõË°åÊ†áÊ≥®Ôºå‰ª•Á°Æ‰øùÊï∞ÊçÆË¥®ÈáèÂíåÊîØÊåÅÂèåÈáçÁõëÁù£„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÂèØËÉΩÂåÖÊã¨Ê£ÄÊµãÊçüÂ§±ÂíåÊé®ÁêÜÊçüÂ§±Ôºå‰ª•ÂêåÊó∂‰ºòÂåñÊ£ÄÊµãÁ≤æÂ∫¶ÂíåÊé®ÁêÜÁöÑÂêàÁêÜÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

EDVD-LLaMAÂú®Ê£ÄÊµãÁ≤æÂ∫¶„ÄÅÂèØËß£ÈáäÊÄßÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ÈÉΩÂèñÂæó‰∫ÜÊòæËëóÊèêÂçá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ë∑®‰º™ÈÄ†ÊñπÊ≥ïÂíåË∑®Êï∞ÊçÆÈõÜÂú∫ÊôØ‰∏ãÔºå‰æùÁÑ∂ËÉΩÂ§ü‰øùÊåÅËæÉÈ´òÁöÑÊ£ÄÊµãÁ≤æÂ∫¶„ÄÇ‰∏é‰πãÂâçÁöÑDVDÊñπÊ≥ïÁõ∏ÊØîÔºåEDVD-LLaMAÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êõ¥ÂèØËß£ÈáäÂíåÊõ¥‰ºòË∂äÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÊú™Áü•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊñ∞ÈóªÂ™í‰Ωì„ÄÅÁ§æ‰∫§Âπ≥Âè∞„ÄÅÂÆâÂÖ®ÁõëÊéßÁ≠âÈ¢ÜÂüüÔºåÁî®‰∫éÊ£ÄÊµãÂíåËØÜÂà´DeepfakeËßÜÈ¢ëÔºåÈò≤Ê≠¢ËôöÂÅá‰ø°ÊÅØÁöÑ‰º†Êí≠ÂíåÊÅ∂ÊÑèÊîªÂáª„ÄÇÈÄöËøáÊèê‰æõÂèØËß£ÈáäÁöÑÊ£ÄÊµãÁªìÊûúÔºåÂèØ‰ª•Â∏ÆÂä©Áî®Êà∑Êõ¥Â•ΩÂú∞ÁêÜËß£Âíå‰ø°‰ªªÊ£ÄÊµãÁ≥ªÁªüÔºå‰ªéËÄåÊèêÈ´òÁ§æ‰ºöÂØπDeepfakeÊäÄÊúØÁöÑÈò≤ËåÉÊÑèËØÜ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ benchmark dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The source code and dataset will be publicly available.

