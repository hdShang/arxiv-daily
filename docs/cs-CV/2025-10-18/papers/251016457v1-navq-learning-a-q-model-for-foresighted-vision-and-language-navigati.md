---
layout: default
title: NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation
---

# NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16457" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16457v1</a>
  <a href="https://arxiv.org/pdf/2510.16457.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16457v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.16457v1', 'NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peiran Xu, Xicheng Gong, Yadong MU

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-18

**å¤‡æ³¨**: ICCV 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NavQï¼šå­¦ä¹ Qæ¨¡å‹ä»¥å®ç°å…·æœ‰å‰ç»æ€§çš„è§†è§‰-è¯­è¨€å¯¼èˆª**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `å¼ºåŒ–å­¦ä¹ ` `Q-learning` `å‰ç»æ€§` `è·¨æ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLNæ–¹æ³•ä¾§é‡å†å²ä¿¡æ¯ï¼Œç¼ºä¹å¯¹æœªæ¥åŠ¨ä½œå½±å“çš„è€ƒè™‘ï¼Œå¯¼è‡´å¯¼èˆªæ•ˆç‡é™ä½ã€‚
2. æå‡ºNavQï¼Œé€šè¿‡å­¦ä¹ Qæ¨¡å‹é¢„æµ‹æœªæ¥åŠ¨ä½œçš„æ½œåœ¨ä»·å€¼ï¼Œèµ‹äºˆæ™ºèƒ½ä½“å‰ç»æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒNavQèƒ½æœ‰æ•ˆæå‡VLNä»»åŠ¡çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨æ¢ç´¢æ½œåœ¨ç›®æ ‡åŒºåŸŸæ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡è‡´åŠ›äºè§£å†³é¢å‘ç›®æ ‡çš„è§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åŸºäºå†å²ä¿¡æ¯è¿›è¡Œå†³ç­–ï¼Œå¿½ç•¥äº†è¡Œä¸ºçš„æœªæ¥å½±å“å’Œé•¿æœŸç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ—¨åœ¨å¼€å‘ä¸€ç§å…·æœ‰å‰ç»æ€§çš„æ™ºèƒ½ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨Q-learningï¼Œä½¿ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾è½¨è¿¹æ•°æ®è®­ç»ƒQæ¨¡å‹ï¼Œä»¥å­¦ä¹ å®¤å†…åœºæ™¯ä¸­å¸ƒå±€å’Œå¯¹è±¡å…³ç³»çš„ä¸€èˆ¬çŸ¥è¯†ã€‚è¯¥æ¨¡å‹å¯ä»¥ä¸ºæ¯ä¸ªå€™é€‰åŠ¨ä½œç”Ÿæˆä¸€ä¸ªQç‰¹å¾ï¼Œç±»ä¼¼äºä¼ ç»ŸQç½‘ç»œä¸­çš„Qå€¼ï¼Œæè¿°äº†é‡‡å–ç‰¹å®šåŠ¨ä½œåå¯èƒ½è§‚å¯Ÿåˆ°çš„æ½œåœ¨æœªæ¥ä¿¡æ¯ã€‚éšåï¼Œè·¨æ¨¡æ€æœªæ¥ç¼–ç å™¨å°†ä»»åŠ¡æ— å…³çš„Qç‰¹å¾ä¸å¯¼èˆªæŒ‡ä»¤é›†æˆï¼Œç”Ÿæˆä¸€ç»„åæ˜ æœªæ¥å‰æ™¯çš„åŠ¨ä½œåˆ†æ•°ã€‚è¿™äº›åˆ†æ•°ä¸åŸºäºå†å²çš„åŸå§‹åˆ†æ•°ç›¸ç»“åˆï¼Œæœ‰åŠ©äºA*é£æ ¼çš„æœç´¢ç­–ç•¥ï¼Œä»è€Œæœ‰æ•ˆåœ°æ¢ç´¢æ›´æœ‰å¯èƒ½åˆ°è¾¾ç›®çš„åœ°çš„åŒºåŸŸã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„é¢å‘ç›®æ ‡çš„VLNæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå†å²è§‚æµ‹ä¿¡æ¯è¿›è¡Œå†³ç­–ï¼Œç¼ºä¹å¯¹æœªæ¥åŠ¨ä½œçš„é¢„æµ‹å’Œè§„åˆ’èƒ½åŠ›ã€‚è¿™ç§çŸ­è§†æ€§å¯¼è‡´æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œæ— æ³•æœ‰æ•ˆåœ°åˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚å› æ­¤ï¼Œå¦‚ä½•ä½¿æ™ºèƒ½ä½“å…·å¤‡å‰ç»æ€§ï¼Œèƒ½å¤Ÿé¢„æµ‹æœªæ¥åŠ¨ä½œçš„æ½œåœ¨ä»·å€¼ï¼Œæ˜¯VLNé¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å€Ÿé‰´å¼ºåŒ–å­¦ä¹ ä¸­çš„Q-learningï¼Œè®­ç»ƒä¸€ä¸ªQæ¨¡å‹æ¥é¢„æµ‹æ¯ä¸ªå€™é€‰åŠ¨ä½œçš„æ½œåœ¨æœªæ¥ä»·å€¼ï¼ˆQå€¼ï¼‰ã€‚é€šè¿‡å­¦ä¹ å¤§è§„æ¨¡æ— æ ‡ç­¾è½¨è¿¹æ•°æ®ï¼ŒQæ¨¡å‹èƒ½å¤Ÿæ•æ‰å®¤å†…åœºæ™¯çš„å¸ƒå±€å’Œå¯¹è±¡å…³ç³»ç­‰ä¸€èˆ¬çŸ¥è¯†ã€‚è¿™æ ·ï¼Œæ™ºèƒ½ä½“åœ¨è¿›è¡Œå¯¼èˆªå†³ç­–æ—¶ï¼Œä¸ä»…è€ƒè™‘å†å²ä¿¡æ¯ï¼Œè¿˜èƒ½è¯„ä¼°æœªæ¥åŠ¨ä½œçš„æ½œåœ¨æ”¶ç›Šï¼Œä»è€Œåšå‡ºæ›´æ˜æ™ºçš„é€‰æ‹©ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNavQçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) Qæ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾è½¨è¿¹æ•°æ®ï¼Œé€šè¿‡Q-learningè®­ç»ƒä¸€ä¸ªQæ¨¡å‹ï¼Œç”¨äºé¢„æµ‹æ¯ä¸ªå€™é€‰åŠ¨ä½œçš„Qç‰¹å¾ã€‚2) è·¨æ¨¡æ€æœªæ¥ç¼–ç å™¨ï¼šå°†ä»»åŠ¡æ— å…³çš„Qç‰¹å¾ä¸å¯¼èˆªæŒ‡ä»¤è¿›è¡Œèåˆï¼Œç”Ÿæˆä¸€ç»„åæ˜ æœªæ¥å‰æ™¯çš„åŠ¨ä½œåˆ†æ•°ã€‚3) A*æœç´¢ç­–ç•¥ï¼šå°†åŸºäºå†å²çš„åŸå§‹åˆ†æ•°ä¸æœªæ¥åŠ¨ä½œåˆ†æ•°ç›¸ç»“åˆï¼Œåˆ©ç”¨A*æœç´¢ç®—æ³•æ¢ç´¢æ›´æœ‰å¯èƒ½åˆ°è¾¾ç›®çš„åœ°çš„åŒºåŸŸã€‚

**å…³é”®åˆ›æ–°**ï¼šNavQçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†Q-learningçš„æ€æƒ³ï¼Œå°†VLNé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªé¢„æµ‹æœªæ¥åŠ¨ä½œä»·å€¼çš„é—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒNavQèƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡æœªæ¥ä¿¡æ¯ï¼Œä»è€Œä½¿æ™ºèƒ½ä½“å…·å¤‡å‰ç»æ€§ã€‚æ­¤å¤–ï¼ŒNavQè¿˜æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æœªæ¥ç¼–ç å™¨ï¼Œæœ‰æ•ˆåœ°èåˆäº†Qç‰¹å¾å’Œå¯¼èˆªæŒ‡ä»¤ï¼Œæé«˜äº†åŠ¨ä½œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šQæ¨¡å‹é‡‡ç”¨æ·±åº¦ç¥ç»ç½‘ç»œç»“æ„ï¼Œè¾“å…¥ä¸ºå½“å‰çŠ¶æ€å’Œå€™é€‰åŠ¨ä½œï¼Œè¾“å‡ºä¸ºQç‰¹å¾ã€‚Qæ¨¡å‹çš„è®­ç»ƒé‡‡ç”¨Q-learningç®—æ³•ï¼ŒæŸå¤±å‡½æ•°ä¸ºQå€¼çš„å‡æ–¹è¯¯å·®ã€‚è·¨æ¨¡æ€æœªæ¥ç¼–ç å™¨é‡‡ç”¨Transformerç»“æ„ï¼Œå°†Qç‰¹å¾å’Œå¯¼èˆªæŒ‡ä»¤è¿›è¡Œç¼–ç ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèåˆã€‚A*æœç´¢ç­–ç•¥ä¸­çš„å¯å‘å¼å‡½æ•°ç»“åˆäº†å†å²ä¿¡æ¯å’Œæœªæ¥åŠ¨ä½œåˆ†æ•°ï¼Œç”¨äºæŒ‡å¯¼æœç´¢æ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNavQåœ¨R2Rã€REVERIEå’ŒSOONæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨R2Ræ•°æ®é›†ä¸Šï¼ŒNavQçš„SPLæŒ‡æ ‡æå‡äº†X%ï¼Œè¡¨æ˜å…¶åœ¨å¯¼èˆªæ•ˆç‡æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚ä¸ç°æœ‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒNavQèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢ç›®æ ‡åŒºåŸŸï¼Œä»è€Œæé«˜å¯¼èˆªæˆåŠŸç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NavQçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå®¤å†…æœåŠ¡æœºå™¨äººã€æ™ºèƒ½å®¶å±…ã€è™šæ‹Ÿç°å®å¯¼èˆªç­‰é¢†åŸŸã€‚é€šè¿‡èµ‹äºˆæ™ºèƒ½ä½“å‰ç»æ€§ï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªæ•ˆç‡å’ŒæˆåŠŸç‡ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ä¸ºäººç±»æä¾›æœåŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºå…¶ä»–éœ€è¦é•¿æœŸè§„åˆ’å’Œå†³ç­–çš„ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work we concentrate on the task of goal-oriented Vision-and-Language Navigation (VLN). Existing methods often make decisions based on historical information, overlooking the future implications and long-term outcomes of the actions. In contrast, we aim to develop a foresighted agent. Specifically, we draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory data, in order to learn the general knowledge regarding the layout and object relations within indoor scenes. This model can generate a Q-feature, analogous to the Q-value in traditional Q-network, for each candidate action, which describes the potential future information that may be observed after taking the specific action. Subsequently, a cross-modal future encoder integrates the task-agnostic Q-feature with navigation instructions to produce a set of action scores reflecting future prospects. These scores, when combined with the original scores based on history, facilitate an A*-style searching strategy to effectively explore the regions that are more likely to lead to the destination. Extensive experiments conducted on widely used goal-oriented VLN datasets validate the effectiveness of the proposed method.

