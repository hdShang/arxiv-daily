---
layout: default
title: NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation
---

# NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.16457" target="_blank" class="toolbar-btn">arXiv: 2510.16457v1</a>
    <a href="https://arxiv.org/pdf/2510.16457.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16457v1" 
            onclick="toggleFavorite(this, '2510.16457v1', 'NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Peiran Xu, Xicheng Gong, Yadong MU

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-18

**Â§áÊ≥®**: ICCV 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**NavQÔºöÂ≠¶‰π†QÊ®°Âûã‰ª•ÂÆûÁé∞ÂÖ∑ÊúâÂâçÁûªÊÄßÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØºËà™**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂØºËà™` `Âº∫ÂåñÂ≠¶‰π†` `Q-learning` `ÂâçÁûªÊÄß` `Ë∑®Ê®°ÊÄÅËûçÂêà`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLNÊñπÊ≥ï‰æßÈáçÂéÜÂè≤‰ø°ÊÅØÔºåÁº∫‰πèÂØπÊú™Êù•Âä®‰ΩúÂΩ±ÂìçÁöÑËÄÉËôëÔºåÂØºËá¥ÂØºËà™ÊïàÁéáÈôç‰Ωé„ÄÇ
2. ÊèêÂá∫NavQÔºåÈÄöËøáÂ≠¶‰π†QÊ®°ÂûãÈ¢ÑÊµãÊú™Êù•Âä®‰ΩúÁöÑÊΩúÂú®‰ª∑ÂÄºÔºåËµã‰∫àÊô∫ËÉΩ‰ΩìÂâçÁûªÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåNavQËÉΩÊúâÊïàÊèêÂçáVLN‰ªªÂä°ÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Êé¢Á¥¢ÊΩúÂú®ÁõÆÊ†áÂå∫ÂüüÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáËá¥Âäõ‰∫éËß£ÂÜ≥Èù¢ÂêëÁõÆÊ†áÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØºËà™ÔºàVLNÔºâ‰ªªÂä°„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âü∫‰∫éÂéÜÂè≤‰ø°ÊÅØËøõË°åÂÜ≥Á≠ñÔºåÂøΩÁï•‰∫ÜË°å‰∏∫ÁöÑÊú™Êù•ÂΩ±ÂìçÂíåÈïøÊúüÁªìÊûú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Êó®Âú®ÂºÄÂèë‰∏ÄÁßçÂÖ∑ÊúâÂâçÁûªÊÄßÁöÑÊô∫ËÉΩ‰Ωì„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Âà©Áî®Q-learningÔºå‰ΩøÁî®Â§ßËßÑÊ®°Êó†Ê†áÁ≠æËΩ®ËøπÊï∞ÊçÆËÆ≠ÁªÉQÊ®°ÂûãÔºå‰ª•Â≠¶‰π†ÂÆ§ÂÜÖÂú∫ÊôØ‰∏≠Â∏ÉÂ±ÄÂíåÂØπË±°ÂÖ≥Á≥ªÁöÑ‰∏ÄËà¨Áü•ËØÜ„ÄÇËØ•Ê®°ÂûãÂèØ‰ª•‰∏∫ÊØè‰∏™ÂÄôÈÄâÂä®‰ΩúÁîüÊàê‰∏Ä‰∏™QÁâπÂæÅÔºåÁ±ª‰ºº‰∫é‰º†ÁªüQÁΩëÁªú‰∏≠ÁöÑQÂÄºÔºåÊèèËø∞‰∫ÜÈááÂèñÁâπÂÆöÂä®‰ΩúÂêéÂèØËÉΩËßÇÂØüÂà∞ÁöÑÊΩúÂú®Êú™Êù•‰ø°ÊÅØ„ÄÇÈöèÂêéÔºåË∑®Ê®°ÊÄÅÊú™Êù•ÁºñÁ†ÅÂô®Â∞Ü‰ªªÂä°Êó†ÂÖ≥ÁöÑQÁâπÂæÅ‰∏éÂØºËà™Êåá‰ª§ÈõÜÊàêÔºåÁîüÊàê‰∏ÄÁªÑÂèçÊò†Êú™Êù•ÂâçÊôØÁöÑÂä®‰ΩúÂàÜÊï∞„ÄÇËøô‰∫õÂàÜÊï∞‰∏éÂü∫‰∫éÂéÜÂè≤ÁöÑÂéüÂßãÂàÜÊï∞Áõ∏ÁªìÂêàÔºåÊúâÂä©‰∫éA*È£éÊ†ºÁöÑÊêúÁ¥¢Á≠ñÁï•Ôºå‰ªéËÄåÊúâÊïàÂú∞Êé¢Á¥¢Êõ¥ÊúâÂèØËÉΩÂà∞ËææÁõÆÁöÑÂú∞ÁöÑÂå∫Âüü„ÄÇÂú®ÂπøÊ≥õ‰ΩøÁî®ÁöÑÈù¢ÂêëÁõÆÊ†áÁöÑVLNÊï∞ÊçÆÈõÜ‰∏äËøõË°åÁöÑÂÆûÈ™åÈ™åËØÅ‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØºËà™ÔºàVLNÔºâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÂéÜÂè≤ËßÇÊµã‰ø°ÊÅØËøõË°åÂÜ≥Á≠ñÔºåÁº∫‰πèÂØπÊú™Êù•Âä®‰ΩúÁöÑÈ¢ÑÊµãÂíåËßÑÂàíËÉΩÂäõ„ÄÇËøôÁßçÁü≠ËßÜÊÄßÂØºËá¥Êô∫ËÉΩ‰ΩìÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆπÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòÔºåÊó†Ê≥ïÊúâÊïàÂú∞Âà∞ËææÁõÆÊ†á‰ΩçÁΩÆ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰Ωï‰ΩøÊô∫ËÉΩ‰ΩìÂÖ∑Â§áÂâçÁûªÊÄßÔºåËÉΩÂ§üÈ¢ÑÊµãÊú™Êù•Âä®‰ΩúÁöÑÊΩúÂú®‰ª∑ÂÄºÔºåÊòØVLNÈ¢ÜÂüüÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂÄüÈâ¥Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑQ-learningÔºåËÆ≠ÁªÉ‰∏Ä‰∏™QÊ®°ÂûãÊù•È¢ÑÊµãÊØè‰∏™ÂÄôÈÄâÂä®‰ΩúÁöÑÊΩúÂú®Êú™Êù•‰ª∑ÂÄºÔºàQÂÄºÔºâ„ÄÇÈÄöËøáÂ≠¶‰π†Â§ßËßÑÊ®°Êó†Ê†áÁ≠æËΩ®ËøπÊï∞ÊçÆÔºåQÊ®°ÂûãËÉΩÂ§üÊçïÊçâÂÆ§ÂÜÖÂú∫ÊôØÁöÑÂ∏ÉÂ±ÄÂíåÂØπË±°ÂÖ≥Á≥ªÁ≠â‰∏ÄËà¨Áü•ËØÜ„ÄÇËøôÊ†∑ÔºåÊô∫ËÉΩ‰ΩìÂú®ËøõË°åÂØºËà™ÂÜ≥Á≠ñÊó∂Ôºå‰∏ç‰ªÖËÄÉËôëÂéÜÂè≤‰ø°ÊÅØÔºåËøòËÉΩËØÑ‰º∞Êú™Êù•Âä®‰ΩúÁöÑÊΩúÂú®Êî∂ÁõäÔºå‰ªéËÄåÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÈÄâÊã©„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöNavQÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) QÊ®°ÂûãËÆ≠ÁªÉÔºö‰ΩøÁî®Â§ßËßÑÊ®°Êó†Ê†áÁ≠æËΩ®ËøπÊï∞ÊçÆÔºåÈÄöËøáQ-learningËÆ≠ÁªÉ‰∏Ä‰∏™QÊ®°ÂûãÔºåÁî®‰∫éÈ¢ÑÊµãÊØè‰∏™ÂÄôÈÄâÂä®‰ΩúÁöÑQÁâπÂæÅ„ÄÇ2) Ë∑®Ê®°ÊÄÅÊú™Êù•ÁºñÁ†ÅÂô®ÔºöÂ∞Ü‰ªªÂä°Êó†ÂÖ≥ÁöÑQÁâπÂæÅ‰∏éÂØºËà™Êåá‰ª§ËøõË°åËûçÂêàÔºåÁîüÊàê‰∏ÄÁªÑÂèçÊò†Êú™Êù•ÂâçÊôØÁöÑÂä®‰ΩúÂàÜÊï∞„ÄÇ3) A*ÊêúÁ¥¢Á≠ñÁï•ÔºöÂ∞ÜÂü∫‰∫éÂéÜÂè≤ÁöÑÂéüÂßãÂàÜÊï∞‰∏éÊú™Êù•Âä®‰ΩúÂàÜÊï∞Áõ∏ÁªìÂêàÔºåÂà©Áî®A*ÊêúÁ¥¢ÁÆóÊ≥ïÊé¢Á¥¢Êõ¥ÊúâÂèØËÉΩÂà∞ËææÁõÆÁöÑÂú∞ÁöÑÂå∫Âüü„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöNavQÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜQ-learningÁöÑÊÄùÊÉ≥ÔºåÂ∞ÜVLNÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™È¢ÑÊµãÊú™Êù•Âä®‰Ωú‰ª∑ÂÄºÁöÑÈóÆÈ¢ò„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåNavQËÉΩÂ§üÊòæÂºèÂú∞Âª∫Ê®°Êú™Êù•‰ø°ÊÅØÔºå‰ªéËÄå‰ΩøÊô∫ËÉΩ‰ΩìÂÖ∑Â§áÂâçÁûªÊÄß„ÄÇÊ≠§Â§ñÔºåNavQËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçË∑®Ê®°ÊÄÅÊú™Êù•ÁºñÁ†ÅÂô®ÔºåÊúâÊïàÂú∞ËûçÂêà‰∫ÜQÁâπÂæÅÂíåÂØºËà™Êåá‰ª§ÔºåÊèêÈ´ò‰∫ÜÂä®‰ΩúÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöQÊ®°ÂûãÈááÁî®Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÁªìÊûÑÔºåËæìÂÖ•‰∏∫ÂΩìÂâçÁä∂ÊÄÅÂíåÂÄôÈÄâÂä®‰ΩúÔºåËæìÂá∫‰∏∫QÁâπÂæÅ„ÄÇQÊ®°ÂûãÁöÑËÆ≠ÁªÉÈááÁî®Q-learningÁÆóÊ≥ïÔºåÊçüÂ§±ÂáΩÊï∞‰∏∫QÂÄºÁöÑÂùáÊñπËØØÂ∑Æ„ÄÇË∑®Ê®°ÊÄÅÊú™Êù•ÁºñÁ†ÅÂô®ÈááÁî®TransformerÁªìÊûÑÔºåÂ∞ÜQÁâπÂæÅÂíåÂØºËà™Êåá‰ª§ËøõË°åÁºñÁ†ÅÔºåÂπ∂ÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åËûçÂêà„ÄÇA*ÊêúÁ¥¢Á≠ñÁï•‰∏≠ÁöÑÂêØÂèëÂºèÂáΩÊï∞ÁªìÂêà‰∫ÜÂéÜÂè≤‰ø°ÊÅØÂíåÊú™Êù•Âä®‰ΩúÂàÜÊï∞ÔºåÁî®‰∫éÊåáÂØºÊêúÁ¥¢ÊñπÂêë„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNavQÂú®R2R„ÄÅREVERIEÂíåSOONÊï∞ÊçÆÈõÜ‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®R2RÊï∞ÊçÆÈõÜ‰∏äÔºåNavQÁöÑSPLÊåáÊ†áÊèêÂçá‰∫ÜX%ÔºåË°®ÊòéÂÖ∂Âú®ÂØºËà™ÊïàÁéáÊñπÈù¢ÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇ‰∏éÁé∞ÊúâÂü∫Á∫øÊñπÊ≥ïÁõ∏ÊØîÔºåNavQËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Êé¢Á¥¢ÁõÆÊ†áÂå∫ÂüüÔºå‰ªéËÄåÊèêÈ´òÂØºËà™ÊàêÂäüÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

NavQÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂÆ§ÂÜÖÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅËôöÊãüÁé∞ÂÆûÂØºËà™Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáËµã‰∫àÊô∫ËÉΩ‰ΩìÂâçÁûªÊÄßÔºåÂèØ‰ª•ÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ÊïàÁéáÂíåÊàêÂäüÁéáÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞‰∏∫‰∫∫Á±ªÊèê‰æõÊúçÂä°„ÄÇÊ≠§Â§ñÔºåËØ•Á†îÁ©∂‰πü‰∏∫ÂÖ∂‰ªñÈúÄË¶ÅÈïøÊúüËßÑÂàíÂíåÂÜ≥Á≠ñÁöÑ‰ªªÂä°Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In this work we concentrate on the task of goal-oriented Vision-and-Language Navigation (VLN). Existing methods often make decisions based on historical information, overlooking the future implications and long-term outcomes of the actions. In contrast, we aim to develop a foresighted agent. Specifically, we draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory data, in order to learn the general knowledge regarding the layout and object relations within indoor scenes. This model can generate a Q-feature, analogous to the Q-value in traditional Q-network, for each candidate action, which describes the potential future information that may be observed after taking the specific action. Subsequently, a cross-modal future encoder integrates the task-agnostic Q-feature with navigation instructions to produce a set of action scores reflecting future prospects. These scores, when combined with the original scores based on history, facilitate an A*-style searching strategy to effectively explore the regions that are more likely to lead to the destination. Extensive experiments conducted on widely used goal-oriented VLN datasets validate the effectiveness of the proposed method.

