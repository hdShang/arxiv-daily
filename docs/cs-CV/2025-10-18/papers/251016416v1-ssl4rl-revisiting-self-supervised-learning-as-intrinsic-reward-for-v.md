---
layout: default
title: "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning"
---

# SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16416" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16416v1</a>
  <a href="https://arxiv.org/pdf/2510.16416.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16416v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.16416v1', 'SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaojun Guo, Runyu Zhou, Yifei Wang, Qi Zhang, Chenheng Zhang, Stefanie Jegelka, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin, Yisen Wang

**åˆ†ç±»**: cs.CV, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SSL4RLï¼šåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ä½œä¸ºè§†è§‰-è¯­è¨€æ¨ç†çš„å†…åœ¨å¥–åŠ±**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨ç†` `è‡ªç›‘ç£å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ¨¡æ€å­¦ä¹ ` `å†…åœ¨å¥–åŠ±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨åˆ©ç”¨è§†è§‰ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“ä¾èµ–è¯­è¨€å…ˆéªŒæˆ–æ–‡æœ¬æ·å¾„ï¼Œé™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›ã€‚
2. SSL4RLæ¡†æ¶å°†è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡çš„ç›®æ ‡è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–å¤æ‚çš„å¥–åŠ±å‡½æ•°è®¾è®¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSSL4RLåœ¨è§†è§‰ä¸­å¿ƒå’Œè§†è§‰-è¯­è¨€æ¨ç†ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå›¾å­¦ä¹ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¾“å…¥å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¸¸å¸¸æ— æ³•å……åˆ†åˆ©ç”¨è§†è§‰è¯æ®ï¼Œè¦ä¹ˆä¾èµ–äºè§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­çš„è¯­è¨€å…ˆéªŒï¼Œè¦ä¹ˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¾èµ–äºæ–‡æœ¬æ·å¾„ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ä½¿æ¨¡å‹ä¸æœŸæœ›çš„è¡Œä¸ºå¯¹é½ï¼Œä½†ç”±äºç¼ºä¹å¯æ‰©å±•å’Œå¯é çš„å¥–åŠ±æœºåˆ¶ï¼Œå…¶åœ¨VLMä¸­çš„åº”ç”¨å—åˆ°é˜»ç¢ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SSL4RLï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä»»åŠ¡ä½œä¸ºåŸºäºRLå¾®è°ƒçš„å¯éªŒè¯å¥–åŠ±æ¥æºã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†SSLç›®æ ‡ï¼ˆä¾‹å¦‚é¢„æµ‹å›¾åƒæ—‹è½¬æˆ–é‡å»ºæ©ç è¡¥ä¸ï¼‰é‡æ–°å®šä¹‰ä¸ºå¯†é›†ã€è‡ªåŠ¨çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹äººç±»åå¥½æ•°æ®æˆ–ä¸å¯é çš„AIè¯„ä¼°å™¨çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒSSL4RLæ˜¾è‘—æé«˜äº†è§†è§‰ä¸­å¿ƒå’Œè§†è§‰-è¯­è¨€æ¨ç†åŸºå‡†çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç³»ç»Ÿçš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬ç¡®å®šäº†å½±å“SSL4RLä»»åŠ¡æœ‰æ•ˆæ€§çš„å…³é”®å› ç´ ï¼ˆä¾‹å¦‚ä»»åŠ¡éš¾åº¦ã€æ¨¡å‹è§„æ¨¡ä»¥åŠä¸ç›®æ ‡é¢†åŸŸçš„è¯­ä¹‰å¯¹é½ï¼‰ï¼Œä¸ºæœªæ¥çš„å·¥ä½œæä¾›äº†æ–°çš„è®¾è®¡åŸåˆ™ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å°†å…¶åº”ç”¨äºå›¾å­¦ä¹ æ¥è¯æ˜äº†è¯¥æ¡†æ¶çš„é€šç”¨æ€§ï¼Œä»è€Œè·å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚SSL4RLä¸ºä½¿ç”¨å¯éªŒè¯çš„è‡ªç›‘ç£ç›®æ ‡å¯¹é½å¤šæ¨¡æ€æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªé€šç”¨ä¸”æœ‰æ•ˆçš„èŒƒä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› æ˜¯æ¨¡å‹è¿‡åº¦ä¾èµ–è¯­è¨€ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†è§†è§‰è¯æ®ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶å¯ä»¥ç”¨äºå¾®è°ƒæ¨¡å‹è¡Œä¸ºï¼Œä½†ç¼ºä¹å¯æ‰©å±•å’Œå¯é çš„å¥–åŠ±æœºåˆ¶ï¼Œä¾‹å¦‚éœ€è¦äººå·¥æ ‡æ³¨æˆ–ä¾èµ–ä¸å¯é çš„AIè¯„ä¼°å™¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä»»åŠ¡çš„ç›®æ ‡å‡½æ•°è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åˆ©ç”¨SSLä»»åŠ¡çš„å†…åœ¨ç›‘ç£ä¿¡æ¯æ¥å¼•å¯¼æ¨¡å‹çš„å­¦ä¹ ï¼Œè€Œæ— éœ€äººå·¥å¹²é¢„ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æä¾›å¯†é›†ã€è‡ªåŠ¨ä¸”å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSSL4RLæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼›2) å®šä¹‰è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæ—‹è½¬é¢„æµ‹æˆ–æ©ç å›¾åƒé‡å»ºï¼›3) å°†SSLä»»åŠ¡çš„ç›®æ ‡å‡½æ•°è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ï¼›4) ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚PPOï¼‰å¯¹è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æœ€å¤§åŒ–SSLå¥–åŠ±ã€‚æ•´ä½“æµç¨‹æ˜¯åˆ©ç”¨SSLä»»åŠ¡æä¾›çš„å†…åœ¨å¥–åŠ±æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œä»è€Œæé«˜å…¶è§†è§‰æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†è‡ªç›‘ç£å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒSSL4RLæ— éœ€äººå·¥æ ‡æ³¨æˆ–å¤æ‚çš„å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œè€Œæ˜¯åˆ©ç”¨SSLä»»åŠ¡çš„å†…åœ¨ç›‘ç£ä¿¡æ¯æ¥æä¾›å¥–åŠ±ä¿¡å·ã€‚è¿™ä½¿å¾—å¼ºåŒ–å­¦ä¹ å¯ä»¥æ›´å®¹æ˜“åœ°åº”ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¹¶æé«˜å…¶è§†è§‰æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³é”®çš„è®¾è®¡åŒ…æ‹¬ï¼š1) é€‰æ‹©åˆé€‚çš„è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæ—‹è½¬é¢„æµ‹ã€æ©ç å›¾åƒé‡å»ºç­‰ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦æ¨¡å‹ç†è§£å›¾åƒçš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼›2) è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œå°†SSLä»»åŠ¡çš„ç›®æ ‡å‡½æ•°è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ï¼›3) ä½¿ç”¨åˆé€‚çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚PPOï¼Œå¯¹è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç ”ç©¶äº†ä»»åŠ¡éš¾åº¦ã€æ¨¡å‹è§„æ¨¡ä»¥åŠä¸ç›®æ ‡é¢†åŸŸçš„è¯­ä¹‰å¯¹é½ç­‰å› ç´ å¯¹SSL4RLä»»åŠ¡æœ‰æ•ˆæ€§çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSSL4RLåœ¨è§†è§‰ä¸­å¿ƒå’Œè§†è§‰-è¯­è¨€æ¨ç†åŸºå‡†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒSSL4RLçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒè¡¨æ˜ï¼Œä»»åŠ¡éš¾åº¦ã€æ¨¡å‹è§„æ¨¡ä»¥åŠä¸ç›®æ ‡é¢†åŸŸçš„è¯­ä¹‰å¯¹é½ç­‰å› ç´ å¯¹SSL4RLä»»åŠ¡çš„æœ‰æ•ˆæ€§æœ‰é‡è¦å½±å“ã€‚è¯¥æ¡†æ¶è¿˜æˆåŠŸåº”ç”¨äºå›¾å­¦ä¹ ï¼Œå¹¶è·å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SSL4RLæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦è§†è§‰-è¯­è¨€æ¨ç†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ã€è§†è§‰å¯¼èˆªç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è¯­éŸ³-è¯­è¨€å­¦ä¹ ã€è§†é¢‘-è¯­è¨€å­¦ä¹ ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.

