---
layout: default
title: VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs
---

# VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16598" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16598v1</a>
  <a href="https://arxiv.org/pdf/2510.16598.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16598v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.16598v1', 'VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiaying Zhu, Yurui Zhu, Xin Lu, Wenrui Yan, Dong Li, Kunlin Liu, Xueyang Fu, Zheng-Jun Zha

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-18

**å¤‡æ³¨**: 22 pages, 8 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/JulietChoo/VisionSelector)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VisionSelectorï¼šç«¯åˆ°ç«¯å¯å­¦ä¹ çš„è§†è§‰Tokenå‹ç¼©ï¼Œæå‡å¤šæ¨¡æ€LLMæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `è§†è§‰Tokenå‹ç¼©` `ç«¯åˆ°ç«¯å­¦ä¹ ` `Top-Ké€‰æ‹©` `è¯¾ç¨‹å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ï¼Œè§†è§‰Tokenæ•°é‡åºå¤§ï¼Œå¯¼è‡´è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆã€‚
2. VisionSelectorå°†Tokenå‹ç¼©è½¬åŒ–ä¸ºç«¯åˆ°ç«¯å¯å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹ï¼Œè‡ªé€‚åº”é€‰æ‹©å…³é”®Tokenã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVisionSelectoråœ¨å„ç§å‹ç¼©ç‡ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡æ€§èƒ½å¹¶åŠ é€Ÿé¢„å¡«å……è¿‡ç¨‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒæˆ–å¤šå›¾åƒè¾“å…¥äº§ç”Ÿçš„å¤§é‡è§†è§‰tokensæ—¶ï¼Œé¢ä¸´ç€æ˜¾è‘—çš„è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆã€‚å…ˆå‰çš„tokenå‹ç¼©æŠ€æœ¯é€šå¸¸å—é™äºå¯å‘å¼è§„åˆ™ï¼Œè¿™å¯èƒ½å¯¼è‡´å…³é”®ä¿¡æ¯çš„ä¸¢å¤±ã€‚å®ƒä»¬å¯èƒ½é­å—è¯¸å¦‚æ³¨æ„åŠ›æ²‰æ²¡(attention sinks)ç­‰åå·®ï¼Œä»è€Œåœ¨æ¿€è¿›çš„å‹ç¼©ç‡ä¸‹å¯¼è‡´æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å°†tokenå‹ç¼©é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªè½»é‡çº§çš„å³æ’å³ç”¨æ¡†æ¶ï¼Œå°†å…¶è½¬åŒ–ä¸ºç«¯åˆ°ç«¯çš„å¯å­¦ä¹ å†³ç­–è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†VisionSelectorï¼Œä¸€ä¸ªä¸MLLMéª¨å¹²ç½‘ç»œè§£è€¦çš„è¯„åˆ†æ¨¡å—ï¼Œå®ƒç»“åˆäº†å¯å¾®åˆ†çš„Top-Kæœºåˆ¶å’Œè¯¾ç¨‹é€€ç«ç­–ç•¥ï¼Œä»¥å¼¥åˆè®­ç»ƒ-æ¨ç†å·®è·ï¼Œä»è€Œèƒ½å¤Ÿä»¥å„ç§ä»»æ„å‹ç¼©ç‡è¿›è¡Œé«˜æ•ˆå’Œè‡ªé€‚åº”çš„tokené€‰æ‹©ã€‚VisionSelectoréå¸¸è½»é‡çº§ï¼Œåªæœ‰1285ä¸‡ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå®ƒå±•ç¤ºäº†è·¨å„ç§å‹ç¼©ç‡çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½è‡ªé€‚åº”åœ°è¯†åˆ«å…³é”®tokensã€‚è¿™å¸¦æ¥äº†åœ¨æ‰€æœ‰å‹ç¼©é¢„ç®—ä¸‹çš„å“è¶Šæ€§èƒ½ï¼Œé€šè¿‡åœ¨30%ä¿ç•™é¢„ç®—ä¸‹ä¿æŒMMEçš„100%å‡†ç¡®ç‡ï¼Œåœ¨10%ä¿ç•™é¢„ç®—ä¸‹ä¼˜äºå…ˆå‰æ–¹æ³•12.14%ï¼Œä»¥åŠé¢„å¡«å……é€Ÿåº¦ç¿»å€æ¥è¯æ˜ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæˆ–å¤šå›¾è¾“å…¥æ—¶ï¼Œä¼šäº§ç”Ÿå¤§é‡çš„è§†è§‰tokensï¼Œè¿™å¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—å’Œå†…å­˜è´Ÿæ‹…ã€‚ç°æœ‰çš„tokenå‹ç¼©æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯å‘å¼è§„åˆ™ï¼Œè¿™äº›è§„åˆ™å¯èƒ½ä¼šå¯¼è‡´å…³é”®ä¿¡æ¯çš„ä¸¢å¤±ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°æ³¨æ„åŠ›æ²‰æ²¡ç­‰åå·®çš„å½±å“ï¼Œä»è€Œåœ¨è¾ƒé«˜å‹ç¼©ç‡ä¸‹å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†tokenå‹ç¼©é—®é¢˜é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªç«¯åˆ°ç«¯å¯å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹ã€‚é€šè¿‡å­¦ä¹ ä¸€ä¸ªè¯„åˆ†å‡½æ•°æ¥è¯„ä¼°æ¯ä¸ªè§†è§‰tokençš„é‡è¦æ€§ï¼Œå¹¶é€‰æ‹©æœ€é‡è¦çš„tokenå­é›†ï¼Œä»è€Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä¿ç•™å…³é”®ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯å‘å¼è§„åˆ™çš„å±€é™æ€§ï¼Œå¹¶å…è®¸æ¨¡å‹è‡ªé€‚åº”åœ°å­¦ä¹ å“ªäº›tokenå¯¹äºä¸‹æ¸¸ä»»åŠ¡æœ€é‡è¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVisionSelectoræ˜¯ä¸€ä¸ªè½»é‡çº§çš„å³æ’å³ç”¨æ¨¡å—ï¼Œå¯ä»¥ä¸ç°æœ‰çš„MLLMéª¨å¹²ç½‘ç»œé›†æˆã€‚å®ƒä¸»è¦åŒ…å«ä¸€ä¸ªè¯„åˆ†æ¨¡å—ï¼Œç”¨äºè¯„ä¼°æ¯ä¸ªè§†è§‰tokençš„é‡è¦æ€§ã€‚è¯¥è¯„åˆ†æ¨¡å—ä¸MLLMéª¨å¹²ç½‘ç»œè§£è€¦ï¼Œå…è®¸ç‹¬ç«‹è®­ç»ƒå’Œä¼˜åŒ–ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨å¯å¾®åˆ†çš„Top-Kæœºåˆ¶æ¥é€‰æ‹©æœ€é‡è¦çš„tokenå­é›†ã€‚ä¸ºäº†å¼¥åˆè®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„å·®è·ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†ä¸€ç§è¯¾ç¨‹é€€ç«ç­–ç•¥ï¼Œé€æ­¥å¢åŠ å‹ç¼©ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šVisionSelectorçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç«¯åˆ°ç«¯å¯å­¦ä¹ çš„tokenå‹ç¼©æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå¯å‘å¼è§„åˆ™çš„æ–¹æ³•ä¸åŒï¼ŒVisionSelectorèƒ½å¤Ÿè‡ªé€‚åº”åœ°å­¦ä¹ å“ªäº›tokenå¯¹äºä¸‹æ¸¸ä»»åŠ¡æœ€é‡è¦ã€‚æ­¤å¤–ï¼Œå¯å¾®åˆ†çš„Top-Kæœºåˆ¶å’Œè¯¾ç¨‹é€€ç«ç­–ç•¥æœ‰æ•ˆåœ°è§£å†³äº†è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„å·®è·ï¼Œä½¿å¾—VisionSelectorèƒ½å¤Ÿåœ¨å„ç§å‹ç¼©ç‡ä¸‹ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šVisionSelectorçš„è¯„åˆ†æ¨¡å—æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»ç½‘ç»œï¼Œå…¶è¾“å…¥æ˜¯è§†è§‰tokenï¼Œè¾“å‡ºæ˜¯æ¯ä¸ªtokençš„é‡è¦æ€§å¾—åˆ†ã€‚Top-Kæœºåˆ¶é€‰æ‹©å¾—åˆ†æœ€é«˜çš„Kä¸ªtokenã€‚è¯¾ç¨‹é€€ç«ç­–ç•¥é€šè¿‡é€æ­¥é™ä½ä¿ç•™çš„tokenæ•°é‡ï¼Œæ¥æé«˜æ¨¡å‹åœ¨è¾ƒé«˜å‹ç¼©ç‡ä¸‹çš„é²æ£’æ€§ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨æœ€å¤§åŒ–ä¿ç•™tokençš„ä¿¡æ¯é‡ï¼ŒåŒæ—¶æœ€å°åŒ–å‹ç¼©å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚å…·ä½“å‚æ•°è®¾ç½®ï¼ˆå¦‚ç½‘ç»œç»“æ„ã€å­¦ä¹ ç‡ç­‰ï¼‰åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VisionSelectoråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨MMEåŸºå‡†æµ‹è¯•ä¸­ï¼Œå³ä½¿åœ¨30%çš„ä¿ç•™é¢„ç®—ä¸‹ï¼Œä¹Ÿèƒ½ä¿æŒ100%çš„å‡†ç¡®ç‡ã€‚åœ¨10%çš„ä¿ç•™é¢„ç®—ä¸‹ï¼ŒVisionSelectorçš„æ€§èƒ½æ¯”ç°æœ‰æ–¹æ³•é«˜å‡º12.14%ã€‚æ­¤å¤–ï¼ŒVisionSelectorè¿˜èƒ½å°†é¢„å¡«å……é€Ÿåº¦æé«˜ä¸€å€ï¼Œæ˜¾è‘—æå‡äº†MLLMçš„æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VisionSelectorå¯åº”ç”¨äºå„ç§éœ€è¦å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæˆ–å¤šå›¾è¾“å…¥çš„MLLMåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒæè¿°ã€è§†è§‰æ¨ç†ç­‰ã€‚é€šè¿‡é™ä½è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œå®ƒå¯ä»¥ä½¿MLLMåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œå¹¶æé«˜å¤„ç†é€Ÿåº¦ã€‚è¯¥ç ”ç©¶å¯¹å¼€å‘æ›´é«˜æ•ˆã€æ›´å®ç”¨çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal Large Language Models (MLLMs) encounter significant computational and memory bottlenecks from the massive number of visual tokens generated by high-resolution images or multi-image inputs. Previous token compression techniques are often constrained by heuristic rules that risk discarding critical information. They may suffer from biases, such as attention sinks, that lead to sharp performance drops under aggressive compression ratios. To address these limitations, we reformulate token compression as a lightweight plug-and-play framework that reformulates token compression into an end-to-end learnable decision process. To be specific, we propose VisionSelector, a scorer module decoupled from the MLLM backbone that incorporates a differentiable Top-K mechanism and a curriculum annealing strategy to bridge the training-inference gap, enabling efficient and adaptive token selection various arbitrary compression rates. Remarkably lightweight with only 12.85M trainable parameters, VisionSelector demonstrates generalization across various compression rates and adaptively identifying critical tokens. This leads to superior performance across all compression budgets, evidenced by preserving 100% accuracy on MME with 30% retention budget, outperforming prior methods by 12.14% at 10% retention budget, and doubling prefill speed. Our code is available at https://github.com/JulietChoo/VisionSelector .

