---
layout: default
title: VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs
---

# VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.16598" target="_blank" class="toolbar-btn">arXiv: 2510.16598v1</a>
    <a href="https://arxiv.org/pdf/2510.16598.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16598v1" 
            onclick="toggleFavorite(this, '2510.16598v1', 'VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jiaying Zhu, Yurui Zhu, Xin Lu, Wenrui Yan, Dong Li, Kunlin Liu, Xueyang Fu, Zheng-Jun Zha

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-18

**Â§áÊ≥®**: 22 pages, 8 figures

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/JulietChoo/VisionSelector)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VisionSelectorÔºöÁ´ØÂà∞Á´ØÂèØÂ≠¶‰π†ÁöÑËßÜËßâTokenÂéãÁº©ÔºåÊèêÂçáÂ§öÊ®°ÊÄÅLLMÊïàÁéá**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `ËßÜËßâTokenÂéãÁº©` `Á´ØÂà∞Á´ØÂ≠¶‰π†` `Top-KÈÄâÊã©` `ËØæÁ®ãÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÊó∂ÔºåËßÜËßâTokenÊï∞ÈáèÂ∫ûÂ§ßÔºåÂØºËá¥ËÆ°ÁÆóÂíåÂÜÖÂ≠òÁì∂È¢à„ÄÇ
2. VisionSelectorÂ∞ÜTokenÂéãÁº©ËΩ¨Âåñ‰∏∫Á´ØÂà∞Á´ØÂèØÂ≠¶‰π†ÁöÑÂÜ≥Á≠ñËøáÁ®ãÔºåËá™ÈÄÇÂ∫îÈÄâÊã©ÂÖ≥ÈîÆToken„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVisionSelectorÂú®ÂêÑÁßçÂéãÁº©Áéá‰∏ãÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÂçáÊÄßËÉΩÂπ∂Âä†ÈÄüÈ¢ÑÂ°´ÂÖÖËøáÁ®ã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLM)Âú®È´òÂàÜËæ®ÁéáÂõæÂÉèÊàñÂ§öÂõæÂÉèËæìÂÖ•‰∫ßÁîüÁöÑÂ§ßÈáèËßÜËßâtokensÊó∂ÔºåÈù¢‰∏¥ÁùÄÊòæËëóÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òÁì∂È¢à„ÄÇÂÖàÂâçÁöÑtokenÂéãÁº©ÊäÄÊúØÈÄöÂ∏∏ÂèóÈôê‰∫éÂêØÂèëÂºèËßÑÂàôÔºåËøôÂèØËÉΩÂØºËá¥ÂÖ≥ÈîÆ‰ø°ÊÅØÁöÑ‰∏¢Â§±„ÄÇÂÆÉ‰ª¨ÂèØËÉΩÈÅ≠ÂèóËØ∏Â¶ÇÊ≥®ÊÑèÂäõÊ≤âÊ≤°(attention sinks)Á≠âÂÅèÂ∑ÆÔºå‰ªéËÄåÂú®ÊøÄËøõÁöÑÂéãÁº©Áéá‰∏ãÂØºËá¥ÊÄßËÉΩÊÄ•Ââß‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨Â∞ÜtokenÂéãÁº©ÈáçÊñ∞ÂÆö‰πâ‰∏∫‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÂç≥ÊèíÂç≥Áî®Ê°ÜÊû∂ÔºåÂ∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫Á´ØÂà∞Á´ØÁöÑÂèØÂ≠¶‰π†ÂÜ≥Á≠ñËøáÁ®ã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVisionSelectorÔºå‰∏Ä‰∏™‰∏éMLLMÈ™®Âπ≤ÁΩëÁªúËß£ËÄ¶ÁöÑËØÑÂàÜÊ®°ÂùóÔºåÂÆÉÁªìÂêà‰∫ÜÂèØÂæÆÂàÜÁöÑTop-KÊú∫Âà∂ÂíåËØæÁ®ãÈÄÄÁÅ´Á≠ñÁï•Ôºå‰ª•Âº•ÂêàËÆ≠ÁªÉ-Êé®ÁêÜÂ∑ÆË∑ùÔºå‰ªéËÄåËÉΩÂ§ü‰ª•ÂêÑÁßç‰ªªÊÑèÂéãÁº©ÁéáËøõË°åÈ´òÊïàÂíåËá™ÈÄÇÂ∫îÁöÑtokenÈÄâÊã©„ÄÇVisionSelectorÈùûÂ∏∏ËΩªÈáèÁ∫ßÔºåÂè™Êúâ1285‰∏á‰∏™ÂèØËÆ≠ÁªÉÂèÇÊï∞ÔºåÂÆÉÂ±ïÁ§∫‰∫ÜË∑®ÂêÑÁßçÂéãÁº©ÁéáÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂ËÉΩËá™ÈÄÇÂ∫îÂú∞ËØÜÂà´ÂÖ≥ÈîÆtokens„ÄÇËøôÂ∏¶Êù•‰∫ÜÂú®ÊâÄÊúâÂéãÁº©È¢ÑÁÆó‰∏ãÁöÑÂçìË∂äÊÄßËÉΩÔºåÈÄöËøáÂú®30%‰øùÁïôÈ¢ÑÁÆó‰∏ã‰øùÊåÅMMEÁöÑ100%ÂáÜÁ°ÆÁéáÔºåÂú®10%‰øùÁïôÈ¢ÑÁÆó‰∏ã‰ºò‰∫éÂÖàÂâçÊñπÊ≥ï12.14%Ôºå‰ª•ÂèäÈ¢ÑÂ°´ÂÖÖÈÄüÂ∫¶ÁøªÂÄçÊù•ËØÅÊòé„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÊàñÂ§öÂõæËæìÂÖ•Êó∂Ôºå‰ºö‰∫ßÁîüÂ§ßÈáèÁöÑËßÜËßâtokensÔºåËøôÂ∏¶Êù•‰∫ÜÊòæËëóÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠òË¥üÊãÖ„ÄÇÁé∞ÊúâÁöÑtokenÂéãÁº©ÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂêØÂèëÂºèËßÑÂàôÔºåËøô‰∫õËßÑÂàôÂèØËÉΩ‰ºöÂØºËá¥ÂÖ≥ÈîÆ‰ø°ÊÅØÁöÑ‰∏¢Â§±ÔºåÂπ∂‰∏îÂÆπÊòìÂèóÂà∞Ê≥®ÊÑèÂäõÊ≤âÊ≤°Á≠âÂÅèÂ∑ÆÁöÑÂΩ±ÂìçÔºå‰ªéËÄåÂú®ËæÉÈ´òÂéãÁº©Áéá‰∏ãÂØºËá¥ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜtokenÂéãÁº©ÈóÆÈ¢òÈáçÊñ∞ÂÆö‰πâ‰∏∫‰∏Ä‰∏™Á´ØÂà∞Á´ØÂèØÂ≠¶‰π†ÁöÑÂÜ≥Á≠ñËøáÁ®ã„ÄÇÈÄöËøáÂ≠¶‰π†‰∏Ä‰∏™ËØÑÂàÜÂáΩÊï∞Êù•ËØÑ‰º∞ÊØè‰∏™ËßÜËßâtokenÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂ÈÄâÊã©ÊúÄÈáçË¶ÅÁöÑtokenÂ≠êÈõÜÔºå‰ªéËÄåÂú®Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÁöÑÂêåÊó∂ÔºåÂ∞ΩÂèØËÉΩ‰øùÁïôÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÂêØÂèëÂºèËßÑÂàôÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂ÂÖÅËÆ∏Ê®°ÂûãËá™ÈÄÇÂ∫îÂú∞Â≠¶‰π†Âì™‰∫õtokenÂØπ‰∫é‰∏ãÊ∏∏‰ªªÂä°ÊúÄÈáçË¶Å„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVisionSelectorÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÂç≥ÊèíÂç≥Áî®Ê®°ÂùóÔºåÂèØ‰ª•‰∏éÁé∞ÊúâÁöÑMLLMÈ™®Âπ≤ÁΩëÁªúÈõÜÊàê„ÄÇÂÆÉ‰∏ªË¶ÅÂåÖÂê´‰∏Ä‰∏™ËØÑÂàÜÊ®°ÂùóÔºåÁî®‰∫éËØÑ‰º∞ÊØè‰∏™ËßÜËßâtokenÁöÑÈáçË¶ÅÊÄß„ÄÇËØ•ËØÑÂàÜÊ®°Âùó‰∏éMLLMÈ™®Âπ≤ÁΩëÁªúËß£ËÄ¶ÔºåÂÖÅËÆ∏Áã¨Á´ãËÆ≠ÁªÉÂíå‰ºòÂåñ„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÈááÁî®ÂèØÂæÆÂàÜÁöÑTop-KÊú∫Âà∂Êù•ÈÄâÊã©ÊúÄÈáçË¶ÅÁöÑtokenÂ≠êÈõÜ„ÄÇ‰∏∫‰∫ÜÂº•ÂêàËÆ≠ÁªÉÂíåÊé®ÁêÜ‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåËÆ∫ÊñáËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçËØæÁ®ãÈÄÄÁÅ´Á≠ñÁï•ÔºåÈÄêÊ≠•Â¢ûÂä†ÂéãÁº©Áéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVisionSelectorÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Á´ØÂà∞Á´ØÂèØÂ≠¶‰π†ÁöÑtokenÂéãÁº©ÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÂêØÂèëÂºèËßÑÂàôÁöÑÊñπÊ≥ï‰∏çÂêåÔºåVisionSelectorËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Â≠¶‰π†Âì™‰∫õtokenÂØπ‰∫é‰∏ãÊ∏∏‰ªªÂä°ÊúÄÈáçË¶Å„ÄÇÊ≠§Â§ñÔºåÂèØÂæÆÂàÜÁöÑTop-KÊú∫Âà∂ÂíåËØæÁ®ãÈÄÄÁÅ´Á≠ñÁï•ÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫ÜËÆ≠ÁªÉÂíåÊé®ÁêÜ‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰ΩøÂæóVisionSelectorËÉΩÂ§üÂú®ÂêÑÁßçÂéãÁº©Áéá‰∏ã‰øùÊåÅËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVisionSelectorÁöÑËØÑÂàÜÊ®°ÂùóÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÁ•ûÁªèÁΩëÁªúÔºåÂÖ∂ËæìÂÖ•ÊòØËßÜËßâtokenÔºåËæìÂá∫ÊòØÊØè‰∏™tokenÁöÑÈáçË¶ÅÊÄßÂæóÂàÜ„ÄÇTop-KÊú∫Âà∂ÈÄâÊã©ÂæóÂàÜÊúÄÈ´òÁöÑK‰∏™token„ÄÇËØæÁ®ãÈÄÄÁÅ´Á≠ñÁï•ÈÄöËøáÈÄêÊ≠•Èôç‰Ωé‰øùÁïôÁöÑtokenÊï∞ÈáèÔºåÊù•ÊèêÈ´òÊ®°ÂûãÂú®ËæÉÈ´òÂéãÁº©Áéá‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°Êó®Âú®ÊúÄÂ§ßÂåñ‰øùÁïôtokenÁöÑ‰ø°ÊÅØÈáèÔºåÂêåÊó∂ÊúÄÂ∞èÂåñÂéãÁº©Â∏¶Êù•ÁöÑÊÄßËÉΩÊçüÂ§±„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÔºàÂ¶ÇÁΩëÁªúÁªìÊûÑ„ÄÅÂ≠¶‰π†ÁéáÁ≠âÔºâÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VisionSelectorÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂú®MMEÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂç≥‰ΩøÂú®30%ÁöÑ‰øùÁïôÈ¢ÑÁÆó‰∏ãÔºå‰πüËÉΩ‰øùÊåÅ100%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÂú®10%ÁöÑ‰øùÁïôÈ¢ÑÁÆó‰∏ãÔºåVisionSelectorÁöÑÊÄßËÉΩÊØîÁé∞ÊúâÊñπÊ≥ïÈ´òÂá∫12.14%„ÄÇÊ≠§Â§ñÔºåVisionSelectorËøòËÉΩÂ∞ÜÈ¢ÑÂ°´ÂÖÖÈÄüÂ∫¶ÊèêÈ´ò‰∏ÄÂÄçÔºåÊòæËëóÊèêÂçá‰∫ÜMLLMÁöÑÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VisionSelectorÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂ§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÊàñÂ§öÂõæËæìÂÖ•ÁöÑMLLMÂ∫îÁî®Âú∫ÊôØÔºå‰æãÂ¶ÇËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞„ÄÅËßÜËßâÊé®ÁêÜÁ≠â„ÄÇÈÄöËøáÈôç‰ΩéËÆ°ÁÆóÂíåÂÜÖÂ≠òÈúÄÊ±ÇÔºåÂÆÉÂèØ‰ª•‰ΩøMLLMÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äËøêË°åÔºåÂπ∂ÊèêÈ´òÂ§ÑÁêÜÈÄüÂ∫¶„ÄÇËØ•Á†îÁ©∂ÂØπÂºÄÂèëÊõ¥È´òÊïà„ÄÅÊõ¥ÂÆûÁî®ÁöÑÂ§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal Large Language Models (MLLMs) encounter significant computational and memory bottlenecks from the massive number of visual tokens generated by high-resolution images or multi-image inputs. Previous token compression techniques are often constrained by heuristic rules that risk discarding critical information. They may suffer from biases, such as attention sinks, that lead to sharp performance drops under aggressive compression ratios. To address these limitations, we reformulate token compression as a lightweight plug-and-play framework that reformulates token compression into an end-to-end learnable decision process. To be specific, we propose VisionSelector, a scorer module decoupled from the MLLM backbone that incorporates a differentiable Top-K mechanism and a curriculum annealing strategy to bridge the training-inference gap, enabling efficient and adaptive token selection various arbitrary compression rates. Remarkably lightweight with only 12.85M trainable parameters, VisionSelector demonstrates generalization across various compression rates and adaptively identifying critical tokens. This leads to superior performance across all compression budgets, evidenced by preserving 100% accuracy on MME with 30% retention budget, outperforming prior methods by 12.14% at 10% retention budget, and doubling prefill speed. Our code is available at https://github.com/JulietChoo/VisionSelector .

