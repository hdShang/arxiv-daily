---
layout: default
title: Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs
---

# Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16624" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16624v1</a>
  <a href="https://arxiv.org/pdf/2510.16624.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16624v1" onclick="toggleFavorite(this, '2510.16624v1', 'Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sebastian Mocanu, Emil Slusanschi, Marius Leordeanu

**åˆ†ç±»**: cs.CV, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºè¯­ä¹‰åˆ†å‰²å’Œå•ç›®æ·±åº¦ä¼°è®¡çš„ä½æˆæœ¬æ— äººæœºè‡ªä¸»é£è¡Œæ–¹æ³•ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æ— äººæœºè‡ªä¸»å¯¼èˆª` `è¯­ä¹‰åˆ†å‰²` `å•ç›®æ·±åº¦ä¼°è®¡` `çŸ¥è¯†è’¸é¦` `è§†è§‰SLAM` `åº¦é‡æ·±åº¦ä¼°è®¡` `ä½æˆæœ¬æ— äººæœº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— äººæœºè‡ªä¸»å¯¼èˆªä¾èµ–GPSæˆ–æ¿€å…‰é›·è¾¾ç­‰æ˜‚è´µä¼ æ„Ÿå™¨ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚
2. è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰çš„è‡ªä¸»é£è¡Œç³»ç»Ÿï¼Œåˆ©ç”¨è¯­ä¹‰åˆ†å‰²å’Œå•ç›®æ·±åº¦ä¼°è®¡å®ç°é¿éšœå’Œå®‰å…¨ç€é™†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®å’Œæ•°å­—å­ªç”Ÿç¯å¢ƒä¸­å‡è¡¨ç°è‰¯å¥½ï¼Œæé«˜äº†ä»»åŠ¡æˆåŠŸç‡å’Œæ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»…ä½¿ç”¨è§†è§‰çš„å°å‹æ— äººæœºè‡ªä¸»é£è¡Œç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåœ¨å—æ§å®¤å†…ç¯å¢ƒä¸­è¿è¡Œã€‚è¯¥ç³»ç»Ÿç»“åˆäº†è¯­ä¹‰åˆ†å‰²å’Œå•ç›®æ·±åº¦ä¼°è®¡ï¼Œæ— éœ€GPSæˆ–æ˜‚è´µçš„ä¼ æ„Ÿå™¨ï¼ˆå¦‚æ¿€å…‰é›·è¾¾ï¼‰å³å¯å®ç°é¿éšœã€åœºæ™¯æ¢ç´¢å’Œè‡ªä¸»å®‰å…¨ç€é™†æ“ä½œã€‚ä¸€ä¸ªå…³é”®åˆ›æ–°æ˜¯è‡ªé€‚åº”å°ºåº¦å› å­ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡åˆ©ç”¨è¯­ä¹‰åœ°é¢å¹³é¢æ£€æµ‹å’Œç›¸æœºå†…éƒ¨å‚æ•°ï¼Œå°†éåº¦é‡å•ç›®æ·±åº¦é¢„æµ‹è½¬æ¢ä¸ºå‡†ç¡®çš„åº¦é‡è·ç¦»æµ‹é‡ï¼Œå®ç°äº†14.4å˜ç±³çš„å¹³å‡è·ç¦»è¯¯å·®ã€‚è¯¥æ–¹æ³•ä½¿ç”¨çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œå…¶ä¸­åŸºäºé¢œè‰²çš„æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ•™å¸ˆä¸ºè½»é‡çº§U-Netå­¦ç”Ÿç½‘ç»œï¼ˆ160ä¸‡ä¸ªå‚æ•°ï¼‰ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿè¿›è¡Œå®æ—¶è¯­ä¹‰åˆ†å‰²ã€‚å¯¹äºæ›´å¤æ‚çš„ç¯å¢ƒï¼ŒSVMæ•™å¸ˆå¯ä»¥æ›¿æ¢ä¸ºæœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ã€‚æµ‹è¯•åœ¨ä¸€ä¸ªå—æ§çš„5x4ç±³å®éªŒå®¤ç¯å¢ƒä¸­è¿›è¡Œï¼Œå…¶ä¸­åŒ…å«å…«ä¸ªæ¨¡æ‹ŸåŸå¸‚ç»“æ„çš„çº¸æ¿éšœç¢ç‰©ã€‚åœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œçš„30æ¬¡é£è¡Œæµ‹è¯•å’Œåœ¨æ•°å­—å­ªç”Ÿç¯å¢ƒä¸­è¿›è¡Œçš„100æ¬¡é£è¡Œæµ‹è¯•çš„å¹¿æ³›éªŒè¯è¡¨æ˜ï¼Œç»„åˆçš„åˆ†å‰²å’Œæ·±åº¦æ–¹æ³•å¢åŠ äº†ç›‘è§†æœŸé—´çš„è¡Œé©¶è·ç¦»å¹¶å‡å°‘äº†ä»»åŠ¡æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†100ï¼…çš„æˆåŠŸç‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ å¾—åˆ°è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå…¶ä¸­ç´§å‡‘çš„å­¦ç”Ÿç¥ç»ç½‘ç»œä»æˆ‘ä»¬æ€§èƒ½æœ€ä½³çš„æ–¹æ³•ç”Ÿæˆçš„æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ å®Œæ•´çš„é£è¡Œç­–ç•¥ï¼Œä»è€Œå®ç°äº†87.5ï¼…çš„è‡ªä¸»ä»»åŠ¡æˆåŠŸç‡ã€‚è¿™é¡¹å·¥ä½œæ¨è¿›äº†ç»“æ„åŒ–ç¯å¢ƒä¸­åŸºäºè§†è§‰çš„å®ç”¨æ— äººæœºå¯¼èˆªï¼Œå±•ç¤ºäº†åº¦é‡æ·±åº¦ä¼°è®¡å’Œè®¡ç®—æ•ˆç‡æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œå¯ä»¥åœ¨èµ„æºå—é™çš„å¹³å°ä¸Šè¿›è¡Œéƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæ— äººæœºåœ¨å®¤å†…ç­‰GPSå—é™ç¯å¢ƒä¸­è‡ªä¸»å¯¼èˆªï¼ŒåŒæ—¶é™ä½å¯¹æ˜‚è´µä¼ æ„Ÿå™¨çš„ä¾èµ–ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–æ¿€å…‰é›·è¾¾æˆ–ç«‹ä½“è§†è§‰ï¼Œæˆæœ¬é«˜æ˜‚ä¸”è®¡ç®—é‡å¤§ï¼Œéš¾ä»¥åœ¨ä½æˆæœ¬æ— äººæœºä¸Šéƒ¨ç½²ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨å•ç›®è§†è§‰ä¿¡æ¯ï¼Œé€šè¿‡è¯­ä¹‰åˆ†å‰²è¯†åˆ«åœºæ™¯ä¸­çš„å…³é”®å…ƒç´ ï¼ˆå¦‚åœ°é¢ï¼‰ï¼Œå¹¶ç»“åˆå•ç›®æ·±åº¦ä¼°è®¡è·å–åœºæ™¯çš„å‡ ä½•ä¿¡æ¯ã€‚é€šè¿‡è‡ªé€‚åº”å°ºåº¦å› å­ç®—æ³•ï¼Œå°†éåº¦é‡æ·±åº¦ä¿¡æ¯è½¬æ¢ä¸ºåº¦é‡ä¿¡æ¯ï¼Œä»è€Œå®ç°å‡†ç¡®çš„è·ç¦»æµ‹é‡å’Œå¯¼èˆªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç³»ç»ŸåŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è¯­ä¹‰åˆ†å‰²æ¨¡å—ï¼šä½¿ç”¨è½»é‡çº§U-Netç½‘ç»œè¿›è¡Œå®æ—¶è¯­ä¹‰åˆ†å‰²ï¼Œè¯†åˆ«åœ°é¢ç­‰å…³é”®åŒºåŸŸã€‚2) å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å—ï¼šä¼°è®¡åœºæ™¯çš„æ·±åº¦ä¿¡æ¯ã€‚3) è‡ªé€‚åº”å°ºåº¦å› å­æ¨¡å—ï¼šå°†éåº¦é‡æ·±åº¦ä¿¡æ¯è½¬æ¢ä¸ºåº¦é‡ä¿¡æ¯ã€‚4) é£è¡Œæ§åˆ¶æ¨¡å—ï¼šæ ¹æ®æ„ŸçŸ¥åˆ°çš„ç¯å¢ƒä¿¡æ¯ï¼Œæ§åˆ¶æ— äººæœºè¿›è¡Œé¿éšœã€æ¢ç´¢å’Œç€é™†ç­‰æ“ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼š1) è‡ªé€‚åº”å°ºåº¦å› å­ç®—æ³•ï¼šè¯¥ç®—æ³•åˆ©ç”¨è¯­ä¹‰åˆ†å‰²ç»“æœï¼ˆåœ°é¢æ£€æµ‹ï¼‰å’Œç›¸æœºå†…å‚ï¼Œå°†å•ç›®æ·±åº¦ä¼°è®¡çš„ç›¸å¯¹æ·±åº¦è½¬æ¢ä¸ºç»å¯¹æ·±åº¦ï¼Œä»è€Œå®ç°å‡†ç¡®çš„è·ç¦»æµ‹é‡ã€‚2) çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼šä½¿ç”¨SVMæ•™å¸ˆç½‘ç»œç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè®­ç»ƒè½»é‡çº§çš„U-Netå­¦ç”Ÿç½‘ç»œï¼Œä¿è¯äº†å®æ—¶æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼š1) è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼šé‡‡ç”¨è½»é‡çº§çš„U-Netç»“æ„ï¼Œå‚æ•°é‡ä¸º1.6Mï¼Œä¿è¯äº†å®æ—¶æ€§ã€‚2) æŸå¤±å‡½æ•°ï¼šæœªçŸ¥ã€‚3) è‡ªé€‚åº”å°ºåº¦å› å­ï¼šæ ¹æ®æ£€æµ‹åˆ°çš„åœ°é¢åŒºåŸŸï¼Œè®¡ç®—æ·±åº¦å›¾çš„å°ºåº¦å› å­ï¼Œå°†æ·±åº¦å›¾è½¬æ¢ä¸ºåº¦é‡å•ä½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç³»ç»Ÿåœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œäº†30æ¬¡é£è¡Œæµ‹è¯•ï¼Œåœ¨æ•°å­—å­ªç”Ÿç¯å¢ƒä¸­è¿›è¡Œäº†100æ¬¡é£è¡Œæµ‹è¯•ï¼Œå‡å®ç°äº†100%çš„ä»»åŠ¡æˆåŠŸç‡ã€‚è‡ªé€‚åº”å°ºåº¦å› å­ç®—æ³•å®ç°äº†14.4å˜ç±³çš„å¹³å‡è·ç¦»è¯¯å·®ã€‚é€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œç³»ç»Ÿå®ç°äº†87.5%çš„è‡ªä¸»ä»»åŠ¡æˆåŠŸç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå®¤å†…æ— äººæœºè‡ªä¸»å¯¼èˆªã€ç‰©æµé…é€ã€å®‰é˜²å·¡æ£€ç­‰é¢†åŸŸã€‚ç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œä¾‹å¦‚ä»“åº“ã€åœ°ä¸‹åœè½¦åœºã€çŸ¿äº•ç­‰ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿä»¥è¾ƒä½çš„æˆæœ¬å®ç°æ— äººæœºçš„è‡ªä¸»é£è¡Œï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå•†ä¸šå‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents a vision-only autonomous flight system for small UAVs operating in controlled indoor environments. The system combines semantic segmentation with monocular depth estimation to enable obstacle avoidance, scene exploration, and autonomous safe landing operations without requiring GPS or expensive sensors such as LiDAR. A key innovation is an adaptive scale factor algorithm that converts non-metric monocular depth predictions into accurate metric distance measurements by leveraging semantic ground plane detection and camera intrinsic parameters, achieving a mean distance error of 14.4 cm. The approach uses a knowledge distillation framework where a color-based Support Vector Machine (SVM) teacher generates training data for a lightweight U-Net student network (1.6M parameters) capable of real-time semantic segmentation. For more complex environments, the SVM teacher can be replaced with a state-of-the-art segmentation model. Testing was conducted in a controlled 5x4 meter laboratory environment with eight cardboard obstacles simulating urban structures. Extensive validation across 30 flight tests in a real-world environment and 100 flight tests in a digital-twin environment demonstrates that the combined segmentation and depth approach increases the distance traveled during surveillance and reduces mission time while maintaining 100% success rates. The system is further optimized through end-to-end learning, where a compact student neural network learns complete flight policies from demonstration data generated by our best-performing method, achieving an 87.5% autonomous mission success rate. This work advances practical vision-based drone navigation in structured environments, demonstrating solutions for metric depth estimation and computational efficiency challenges that enable deployment on resource-constrained platforms.

