---
layout: default
title: Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs
---

# Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.16624" target="_blank" class="toolbar-btn">arXiv: 2510.16624v1</a>
    <a href="https://arxiv.org/pdf/2510.16624.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16624v1" 
            onclick="toggleFavorite(this, '2510.16624v1', 'Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Sebastian Mocanu, Emil Slusanschi, Marius Leordeanu

**ÂàÜÁ±ª**: cs.CV, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-18

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éËØ≠‰πâÂàÜÂâ≤ÂíåÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑ‰ΩéÊàêÊú¨Êó†‰∫∫Êú∫Ëá™‰∏ªÈ£ûË°åÊñπÊ≥ï„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `Êó†‰∫∫Êú∫Ëá™‰∏ªÂØºËà™` `ËØ≠‰πâÂàÜÂâ≤` `ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°` `Áü•ËØÜËí∏È¶è` `ËßÜËßâSLAM` `Â∫¶ÈáèÊ∑±Â∫¶‰º∞ËÆ°` `‰ΩéÊàêÊú¨Êó†‰∫∫Êú∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊó†‰∫∫Êú∫Ëá™‰∏ªÂØºËà™‰æùËµñGPSÊàñÊøÄÂÖâÈõ∑ËææÁ≠âÊòÇË¥µ‰º†ÊÑüÂô®ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËßÜËßâÁöÑËá™‰∏ªÈ£ûË°åÁ≥ªÁªüÔºåÂà©Áî®ËØ≠‰πâÂàÜÂâ≤ÂíåÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÂÆûÁé∞ÈÅøÈöúÂíåÂÆâÂÖ®ÁùÄÈôÜ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÁúüÂÆûÂíåÊï∞Â≠óÂ≠™ÁîüÁéØÂ¢É‰∏≠ÂùáË°®Áé∞ËâØÂ•ΩÔºåÊèêÈ´ò‰∫Ü‰ªªÂä°ÊàêÂäüÁéáÂíåÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªÖ‰ΩøÁî®ËßÜËßâÁöÑÂ∞èÂûãÊó†‰∫∫Êú∫Ëá™‰∏ªÈ£ûË°åÁ≥ªÁªüÔºåËØ•Á≥ªÁªüÂú®ÂèóÊéßÂÆ§ÂÜÖÁéØÂ¢É‰∏≠ËøêË°å„ÄÇËØ•Á≥ªÁªüÁªìÂêà‰∫ÜËØ≠‰πâÂàÜÂâ≤ÂíåÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÔºåÊó†ÈúÄGPSÊàñÊòÇË¥µÁöÑ‰º†ÊÑüÂô®ÔºàÂ¶ÇÊøÄÂÖâÈõ∑ËææÔºâÂç≥ÂèØÂÆûÁé∞ÈÅøÈöú„ÄÅÂú∫ÊôØÊé¢Á¥¢ÂíåËá™‰∏ªÂÆâÂÖ®ÁùÄÈôÜÊìç‰Ωú„ÄÇ‰∏Ä‰∏™ÂÖ≥ÈîÆÂàõÊñ∞ÊòØËá™ÈÄÇÂ∫îÂ∞∫Â∫¶Âõ†Â≠êÁÆóÊ≥ïÔºåËØ•ÁÆóÊ≥ïÈÄöËøáÂà©Áî®ËØ≠‰πâÂú∞Èù¢Âπ≥Èù¢Ê£ÄÊµãÂíåÁõ∏Êú∫ÂÜÖÈÉ®ÂèÇÊï∞ÔºåÂ∞ÜÈùûÂ∫¶ÈáèÂçïÁõÆÊ∑±Â∫¶È¢ÑÊµãËΩ¨Êç¢‰∏∫ÂáÜÁ°ÆÁöÑÂ∫¶ÈáèË∑ùÁ¶ªÊµãÈáèÔºåÂÆûÁé∞‰∫Ü14.4ÂéòÁ±≥ÁöÑÂπ≥ÂùáË∑ùÁ¶ªËØØÂ∑Æ„ÄÇËØ•ÊñπÊ≥ï‰ΩøÁî®Áü•ËØÜËí∏È¶èÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠Âü∫‰∫éÈ¢úËâ≤ÁöÑÊîØÊåÅÂêëÈáèÊú∫ÔºàSVMÔºâÊïôÂ∏à‰∏∫ËΩªÈáèÁ∫ßU-NetÂ≠¶ÁîüÁΩëÁªúÔºà160‰∏á‰∏™ÂèÇÊï∞ÔºâÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆÔºåËØ•ÁΩëÁªúËÉΩÂ§üËøõË°åÂÆûÊó∂ËØ≠‰πâÂàÜÂâ≤„ÄÇÂØπ‰∫éÊõ¥Â§çÊùÇÁöÑÁéØÂ¢ÉÔºåSVMÊïôÂ∏àÂèØ‰ª•ÊõøÊç¢‰∏∫ÊúÄÂÖàËøõÁöÑÂàÜÂâ≤Ê®°Âûã„ÄÇÊµãËØïÂú®‰∏Ä‰∏™ÂèóÊéßÁöÑ5x4Á±≥ÂÆûÈ™åÂÆ§ÁéØÂ¢É‰∏≠ËøõË°åÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂÖ´‰∏™Ê®°ÊãüÂüéÂ∏ÇÁªìÊûÑÁöÑÁ∫∏ÊùøÈöúÁ¢çÁâ©„ÄÇÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°åÁöÑ30Ê¨°È£ûË°åÊµãËØïÂíåÂú®Êï∞Â≠óÂ≠™ÁîüÁéØÂ¢É‰∏≠ËøõË°åÁöÑ100Ê¨°È£ûË°åÊµãËØïÁöÑÂπøÊ≥õÈ™åËØÅË°®ÊòéÔºåÁªÑÂêàÁöÑÂàÜÂâ≤ÂíåÊ∑±Â∫¶ÊñπÊ≥ïÂ¢ûÂä†‰∫ÜÁõëËßÜÊúüÈó¥ÁöÑË°åÈ©∂Ë∑ùÁ¶ªÂπ∂ÂáèÂ∞ë‰∫Ü‰ªªÂä°Êó∂Èó¥ÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü100ÔºÖÁöÑÊàêÂäüÁéá„ÄÇËØ•Á≥ªÁªüÈÄöËøáÁ´ØÂà∞Á´ØÂ≠¶‰π†ÂæóÂà∞Ëøõ‰∏ÄÊ≠•‰ºòÂåñÔºåÂÖ∂‰∏≠Á¥ßÂáëÁöÑÂ≠¶ÁîüÁ•ûÁªèÁΩëÁªú‰ªéÊàë‰ª¨ÊÄßËÉΩÊúÄ‰Ω≥ÁöÑÊñπÊ≥ïÁîüÊàêÁöÑÊºîÁ§∫Êï∞ÊçÆ‰∏≠Â≠¶‰π†ÂÆåÊï¥ÁöÑÈ£ûË°åÁ≠ñÁï•Ôºå‰ªéËÄåÂÆûÁé∞‰∫Ü87.5ÔºÖÁöÑËá™‰∏ª‰ªªÂä°ÊàêÂäüÁéá„ÄÇËøôÈ°πÂ∑•‰ΩúÊé®Ëøõ‰∫ÜÁªìÊûÑÂåñÁéØÂ¢É‰∏≠Âü∫‰∫éËßÜËßâÁöÑÂÆûÁî®Êó†‰∫∫Êú∫ÂØºËà™ÔºåÂ±ïÁ§∫‰∫ÜÂ∫¶ÈáèÊ∑±Â∫¶‰º∞ËÆ°ÂíåËÆ°ÁÆóÊïàÁéáÊåëÊàòÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ªéËÄåÂèØ‰ª•Âú®ËµÑÊ∫êÂèóÈôêÁöÑÂπ≥Âè∞‰∏äËøõË°åÈÉ®ÁΩ≤„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊó†‰∫∫Êú∫Âú®ÂÆ§ÂÜÖÁ≠âGPSÂèóÈôêÁéØÂ¢É‰∏≠Ëá™‰∏ªÂØºËà™ÔºåÂêåÊó∂Èôç‰ΩéÂØπÊòÇË¥µ‰º†ÊÑüÂô®ÁöÑ‰æùËµñ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñÊøÄÂÖâÈõ∑ËææÊàñÁ´ã‰ΩìËßÜËßâÔºåÊàêÊú¨È´òÊòÇ‰∏îËÆ°ÁÆóÈáèÂ§ßÔºåÈöæ‰ª•Âú®‰ΩéÊàêÊú¨Êó†‰∫∫Êú∫‰∏äÈÉ®ÁΩ≤„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂà©Áî®ÂçïÁõÆËßÜËßâ‰ø°ÊÅØÔºåÈÄöËøáËØ≠‰πâÂàÜÂâ≤ËØÜÂà´Âú∫ÊôØ‰∏≠ÁöÑÂÖ≥ÈîÆÂÖÉÁ¥†ÔºàÂ¶ÇÂú∞Èù¢ÔºâÔºåÂπ∂ÁªìÂêàÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ëé∑ÂèñÂú∫ÊôØÁöÑÂá†‰Ωï‰ø°ÊÅØ„ÄÇÈÄöËøáËá™ÈÄÇÂ∫îÂ∞∫Â∫¶Âõ†Â≠êÁÆóÊ≥ïÔºåÂ∞ÜÈùûÂ∫¶ÈáèÊ∑±Â∫¶‰ø°ÊÅØËΩ¨Êç¢‰∏∫Â∫¶Èáè‰ø°ÊÅØÔºå‰ªéËÄåÂÆûÁé∞ÂáÜÁ°ÆÁöÑË∑ùÁ¶ªÊµãÈáèÂíåÂØºËà™„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Á≥ªÁªüÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËØ≠‰πâÂàÜÂâ≤Ê®°ÂùóÔºö‰ΩøÁî®ËΩªÈáèÁ∫ßU-NetÁΩëÁªúËøõË°åÂÆûÊó∂ËØ≠‰πâÂàÜÂâ≤ÔºåËØÜÂà´Âú∞Èù¢Á≠âÂÖ≥ÈîÆÂå∫Âüü„ÄÇ2) ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂùóÔºö‰º∞ËÆ°Âú∫ÊôØÁöÑÊ∑±Â∫¶‰ø°ÊÅØ„ÄÇ3) Ëá™ÈÄÇÂ∫îÂ∞∫Â∫¶Âõ†Â≠êÊ®°ÂùóÔºöÂ∞ÜÈùûÂ∫¶ÈáèÊ∑±Â∫¶‰ø°ÊÅØËΩ¨Êç¢‰∏∫Â∫¶Èáè‰ø°ÊÅØ„ÄÇ4) È£ûË°åÊéßÂà∂Ê®°ÂùóÔºöÊ†πÊçÆÊÑüÁü•Âà∞ÁöÑÁéØÂ¢É‰ø°ÊÅØÔºåÊéßÂà∂Êó†‰∫∫Êú∫ËøõË°åÈÅøÈöú„ÄÅÊé¢Á¥¢ÂíåÁùÄÈôÜÁ≠âÊìç‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**Ôºö1) Ëá™ÈÄÇÂ∫îÂ∞∫Â∫¶Âõ†Â≠êÁÆóÊ≥ïÔºöËØ•ÁÆóÊ≥ïÂà©Áî®ËØ≠‰πâÂàÜÂâ≤ÁªìÊûúÔºàÂú∞Èù¢Ê£ÄÊµãÔºâÂíåÁõ∏Êú∫ÂÜÖÂèÇÔºåÂ∞ÜÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÁõ∏ÂØπÊ∑±Â∫¶ËΩ¨Êç¢‰∏∫ÁªùÂØπÊ∑±Â∫¶Ôºå‰ªéËÄåÂÆûÁé∞ÂáÜÁ°ÆÁöÑË∑ùÁ¶ªÊµãÈáè„ÄÇ2) Áü•ËØÜËí∏È¶èÊ°ÜÊû∂Ôºö‰ΩøÁî®SVMÊïôÂ∏àÁΩëÁªúÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆÔºåËÆ≠ÁªÉËΩªÈáèÁ∫ßÁöÑU-NetÂ≠¶ÁîüÁΩëÁªúÔºå‰øùËØÅ‰∫ÜÂÆûÊó∂ÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö1) ËØ≠‰πâÂàÜÂâ≤ÁΩëÁªúÔºöÈááÁî®ËΩªÈáèÁ∫ßÁöÑU-NetÁªìÊûÑÔºåÂèÇÊï∞Èáè‰∏∫1.6MÔºå‰øùËØÅ‰∫ÜÂÆûÊó∂ÊÄß„ÄÇ2) ÊçüÂ§±ÂáΩÊï∞ÔºöÊú™Áü•„ÄÇ3) Ëá™ÈÄÇÂ∫îÂ∞∫Â∫¶Âõ†Â≠êÔºöÊ†πÊçÆÊ£ÄÊµãÂà∞ÁöÑÂú∞Èù¢Âå∫ÂüüÔºåËÆ°ÁÆóÊ∑±Â∫¶ÂõæÁöÑÂ∞∫Â∫¶Âõ†Â≠êÔºåÂ∞ÜÊ∑±Â∫¶ÂõæËΩ¨Êç¢‰∏∫Â∫¶ÈáèÂçï‰Ωç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•Á≥ªÁªüÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°å‰∫Ü30Ê¨°È£ûË°åÊµãËØïÔºåÂú®Êï∞Â≠óÂ≠™ÁîüÁéØÂ¢É‰∏≠ËøõË°å‰∫Ü100Ê¨°È£ûË°åÊµãËØïÔºåÂùáÂÆûÁé∞‰∫Ü100%ÁöÑ‰ªªÂä°ÊàêÂäüÁéá„ÄÇËá™ÈÄÇÂ∫îÂ∞∫Â∫¶Âõ†Â≠êÁÆóÊ≥ïÂÆûÁé∞‰∫Ü14.4ÂéòÁ±≥ÁöÑÂπ≥ÂùáË∑ùÁ¶ªËØØÂ∑Æ„ÄÇÈÄöËøáÁ´ØÂà∞Á´ØÂ≠¶‰π†ÔºåÁ≥ªÁªüÂÆûÁé∞‰∫Ü87.5%ÁöÑËá™‰∏ª‰ªªÂä°ÊàêÂäüÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂÆ§ÂÜÖÊó†‰∫∫Êú∫Ëá™‰∏ªÂØºËà™„ÄÅÁâ©ÊµÅÈÖçÈÄÅ„ÄÅÂÆâÈò≤Â∑°Ê£ÄÁ≠âÈ¢ÜÂüü„ÄÇÁâπÂà´ÊòØÂú®ËµÑÊ∫êÂèóÈôêÁöÑÁéØÂ¢É‰∏≠Ôºå‰æãÂ¶Ç‰ªìÂ∫ì„ÄÅÂú∞‰∏ãÂÅúËΩ¶Âú∫„ÄÅÁüø‰∫ïÁ≠âÔºåËØ•Á≥ªÁªüËÉΩÂ§ü‰ª•ËæÉ‰ΩéÁöÑÊàêÊú¨ÂÆûÁé∞Êó†‰∫∫Êú∫ÁöÑËá™‰∏ªÈ£ûË°åÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂïÜ‰∏öÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper presents a vision-only autonomous flight system for small UAVs operating in controlled indoor environments. The system combines semantic segmentation with monocular depth estimation to enable obstacle avoidance, scene exploration, and autonomous safe landing operations without requiring GPS or expensive sensors such as LiDAR. A key innovation is an adaptive scale factor algorithm that converts non-metric monocular depth predictions into accurate metric distance measurements by leveraging semantic ground plane detection and camera intrinsic parameters, achieving a mean distance error of 14.4 cm. The approach uses a knowledge distillation framework where a color-based Support Vector Machine (SVM) teacher generates training data for a lightweight U-Net student network (1.6M parameters) capable of real-time semantic segmentation. For more complex environments, the SVM teacher can be replaced with a state-of-the-art segmentation model. Testing was conducted in a controlled 5x4 meter laboratory environment with eight cardboard obstacles simulating urban structures. Extensive validation across 30 flight tests in a real-world environment and 100 flight tests in a digital-twin environment demonstrates that the combined segmentation and depth approach increases the distance traveled during surveillance and reduces mission time while maintaining 100% success rates. The system is further optimized through end-to-end learning, where a compact student neural network learns complete flight policies from demonstration data generated by our best-performing method, achieving an 87.5% autonomous mission success rate. This work advances practical vision-based drone navigation in structured environments, demonstrating solutions for metric depth estimation and computational efficiency challenges that enable deployment on resource-constrained platforms.

