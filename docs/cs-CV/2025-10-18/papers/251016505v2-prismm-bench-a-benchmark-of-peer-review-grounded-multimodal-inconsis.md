---
layout: default
title: "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies"
---

# PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16505" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16505v2</a>
  <a href="https://arxiv.org/pdf/2510.16505.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16505v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.16505v2', 'PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lukas Selch, Yufang Hou, M. Jehanzeb Mirza, Sivan Doveh, James Glass, Rogerio Feris, Wei Lin

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-18 (æ›´æ–°: 2025-10-21)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**PRISMM-Benchï¼šé¦–ä¸ªåŸºäºåŒè¡Œè¯„å®¡çš„å¤šæ¨¡æ€ä¸ä¸€è‡´æ€§åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LMMsçš„ç§‘å­¦æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç§‘å­¦æ¨ç†` `ä¸ä¸€è‡´æ€§æ£€æµ‹` `åŒè¡Œè¯„å®¡` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰LMMsåœ¨ç§‘å­¦è®ºæ–‡ç†è§£ä¸­é¢ä¸´å¤šæ¨¡æ€ä¿¡æ¯ä¸ä¸€è‡´æ€§æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†æ•æ‰çœŸå®ä¸–ç•Œå¤æ‚æ€§ã€‚
2. PRISMM-Benché€šè¿‡æŒ–æ˜åŒè¡Œè¯„å®¡æ„è§ï¼Œæ„å»ºåŒ…å«çœŸå®ä¸ä¸€è‡´æ€§çš„æ•°æ®é›†ï¼Œå¹¶è®¾è®¡ä»»åŠ¡è¯„ä¼°LMMsçš„æ£€æµ‹ã€çº æ­£å’Œæ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰LMMsåœ¨PRISMM-Benchä¸Šçš„è¡¨ç°ä¸ä½³ï¼ˆ26.1-54.2%ï¼‰ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€ç§‘å­¦æ¨ç†çš„å·¨å¤§æŒ‘æˆ˜ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºç§‘å­¦ç ”ç©¶ï¼Œä½†å®ƒä»¬æ˜¯å¦èƒ½å¯é åœ°ç†è§£å’Œæ¨ç†è®ºæ–‡ä¸­å¤æ‚çš„å¤šæ¨¡æ€ä¿¡æ¯ä»ä¸æ¸…æ¥šã€‚ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæ£€æµ‹å’Œè§£å†³æ–‡æœ¬ã€å›¾è¡¨ã€å…¬å¼ç­‰ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œè¿™äº›é—®é¢˜é€šå¸¸å¾ˆå¾®å¦™ï¼Œå…·æœ‰é¢†åŸŸç‰¹å¼‚æ€§ï¼Œå¹¶æœ€ç»ˆæŸå®³æ¸…æ™°åº¦ã€å¯é‡å¤æ€§å’Œä¿¡ä»»åº¦ã€‚ç°æœ‰åŸºå‡†å¿½ç•¥äº†è¿™ä¸ªé—®é¢˜ï¼Œè¦ä¹ˆå­¤ç«‹åœ°å¤„ç†å•ä¸€æ¨¡æ€ï¼Œè¦ä¹ˆä¾èµ–äºæœªèƒ½æ•æ‰çœŸå®ä¸–ç•Œå¤æ‚æ€§çš„åˆæˆé”™è¯¯ã€‚æˆ‘ä»¬æ¨å‡ºäº†PRISMM-Benchï¼ˆå¤šæ¨¡æ€æ¨¡å‹åŒè¡Œè¯„å®¡æ¥æºä¸ä¸€è‡´æ€§é›†ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºç§‘å­¦è®ºæ–‡ä¸­çœŸå®è¯„å®¡å‘˜æ ‡è®°çš„ä¸ä¸€è‡´æ€§çš„åŸºå‡†ã€‚é€šè¿‡è¯„å®¡æŒ–æ˜ã€LLMè¾…åŠ©è¿‡æ»¤å’Œäººå·¥éªŒè¯çš„å¤šé˜¶æ®µæµç¨‹ï¼Œæˆ‘ä»¬ä»242ç¯‡è®ºæ–‡ä¸­æ•´ç†å‡º262ä¸ªä¸ä¸€è‡´æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ä¸ªä»»åŠ¡ï¼Œå³ä¸ä¸€è‡´æ€§è¯†åˆ«ã€è¡¥æ•‘å’Œé…å¯¹åŒ¹é…ï¼Œä»¥è¯„ä¼°æ¨¡å‹æ£€æµ‹ã€çº æ­£å’Œæ¨ç†ä¸åŒæ¨¡æ€ä¹‹é—´ä¸ä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³å¤šé¡¹é€‰æ‹©è¯„ä¼°ä¸­è‡­åæ˜­è‘—çš„â€œä»…å‡­é€‰æ‹©â€çš„æ·å¾„é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†åŸºäºJSONçš„ç»“æ„åŒ–ç­”æ¡ˆè¡¨ç¤ºï¼Œé€šè¿‡å‡å°‘å¯¹è¡¨é¢æ–‡ä½“çº¿ç´¢çš„ä¾èµ–æ¥æœ€å°åŒ–è¯­è¨€åå·®ã€‚æˆ‘ä»¬å¯¹21ä¸ªé¢†å…ˆçš„LMMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¤§å‹å¼€æºæ¨¡å‹ï¼ˆGLM-4.5V 106Bã€InternVL3 78Bï¼‰å’Œä¸“æœ‰æ¨¡å‹ï¼ˆGemini 2.5 Proã€å…·æœ‰é«˜æ¨ç†èƒ½åŠ›çš„GPT-5ï¼‰ã€‚ç»“æœæ˜¾ç¤ºæ€§èƒ½éå¸¸ä½ï¼ˆ26.1-54.2%ï¼‰ï¼Œçªæ˜¾äº†å¤šæ¨¡æ€ç§‘å­¦æ¨ç†çš„æŒ‘æˆ˜ï¼Œå¹¶æ¨åŠ¨äº†å¯¹å€¼å¾—ä¿¡èµ–çš„ç§‘å­¦åŠ©æ‰‹çš„ç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ç†è§£å’Œæ¨ç†ç§‘å­¦è®ºæ–‡æ—¶ï¼Œéš¾ä»¥æ£€æµ‹å’Œè§£å†³æ–‡æœ¬ã€å›¾è¡¨ã€å…¬å¼ç­‰æ¨¡æ€é—´ä¸ä¸€è‡´æ€§çš„é—®é¢˜ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•è¦ä¹ˆåªå…³æ³¨å•ä¸€æ¨¡æ€ï¼Œè¦ä¹ˆä½¿ç”¨åˆæˆæ•°æ®ï¼Œæ— æ³•çœŸå®åæ˜ ç§‘å­¦è®ºæ–‡ä¸­å¤æ‚ä¸”ç»†å¾®çš„ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥åº”ç”¨äºå®é™…ç§‘ç ”åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŸºäºçœŸå®åŒè¡Œè¯„å®¡æ„è§çš„æ•°æ®é›†PRISMM-Benchï¼Œå…¶ä¸­åŒ…å«è¯„å®¡å‘˜åœ¨å®é™…è®ºæ–‡ä¸­å‘ç°çš„ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡è¿™ä¸ªæ•°æ®é›†ï¼Œå¯ä»¥æ›´çœŸå®åœ°è¯„ä¼°LMMsåœ¨å¤šæ¨¡æ€ç§‘å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æ¨åŠ¨æ¨¡å‹æœç€æ›´å¯é çš„ç§‘å­¦åŠ©æ‰‹æ–¹å‘å‘å±•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPRISMM-Benchçš„æ„å»ºæµç¨‹åŒ…æ‹¬ï¼š1) ä»å…¬å¼€çš„åŒè¡Œè¯„å®¡æ•°æ®ä¸­æŒ–æ˜æ½œåœ¨çš„ä¸ä¸€è‡´æ€§ï¼›2) ä½¿ç”¨LLMè¾…åŠ©è¿‡æ»¤ï¼Œåˆæ­¥ç­›é€‰å‡ºå¯èƒ½çš„ä¸ä¸€è‡´æ€§ï¼›3) é€šè¿‡äººå·¥éªŒè¯ï¼Œç¡®è®¤æœ€ç»ˆçš„ä¸ä¸€è‡´æ€§æ ·æœ¬ã€‚åŸºäºPRISMM-Benchï¼Œè®ºæ–‡è®¾è®¡äº†ä¸‰ä¸ªä»»åŠ¡ï¼šä¸ä¸€è‡´æ€§è¯†åˆ«ï¼ˆInconsistency Identificationï¼‰ã€è¡¥æ•‘ï¼ˆRemedyï¼‰å’Œé…å¯¹åŒ¹é…ï¼ˆPair Matchingï¼‰ï¼Œç”¨äºå…¨é¢è¯„ä¼°LMMsçš„èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæ„å»ºäº†ä¸€ä¸ªåŸºäºçœŸå®åŒè¡Œè¯„å®¡æ„è§çš„å¤šæ¨¡æ€ä¸ä¸€è‡´æ€§åŸºå‡†PRISMM-Benchã€‚ä¸ä»¥å¾€çš„åˆæˆæ•°æ®æˆ–å•ä¸€æ¨¡æ€åŸºå‡†ç›¸æ¯”ï¼ŒPRISMM-Benchæ›´è´´è¿‘å®é™…ç§‘ç ”åœºæ™¯ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯„ä¼°LMMsåœ¨å¤šæ¨¡æ€ç§‘å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†åŸºäºJSONçš„ç»“æ„åŒ–ç­”æ¡ˆè¡¨ç¤ºï¼Œä»¥å‡å°‘å¤šé¡¹é€‰æ‹©é¢˜ä¸­çš„è¯­è¨€åå·®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®æ„å»ºæ–¹é¢ï¼Œè®ºæ–‡é‡‡ç”¨äº†å¤šé˜¶æ®µçš„è¿‡æ»¤å’ŒéªŒè¯æµç¨‹ï¼Œä»¥ç¡®ä¿æ•°æ®é›†çš„è´¨é‡ã€‚åœ¨ä»»åŠ¡è®¾è®¡æ–¹é¢ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸‰ä¸ªä¸åŒéš¾åº¦çš„ä»»åŠ¡ï¼Œä»¥å…¨é¢è¯„ä¼°LMMsçš„èƒ½åŠ›ã€‚ä¸ºäº†å‡å°‘å¤šé¡¹é€‰æ‹©é¢˜ä¸­çš„è¯­è¨€åå·®ï¼Œè®ºæ–‡ä½¿ç”¨äº†ç»“æ„åŒ–çš„JSONç­”æ¡ˆè¡¨ç¤ºï¼Œé¿å…æ¨¡å‹ä»…ä»…ä¾èµ–äºè¡¨é¢æ–‡ä½“çº¿ç´¢è¿›è¡Œé€‰æ‹©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒ…æ‹¬GLM-4.5V 106Bã€InternVL3 78Bã€Gemini 2.5 Proå’ŒGPT-5åœ¨å†…çš„21ä¸ªé¢†å…ˆLMMsåœ¨PRISMM-Benchä¸Šçš„è¡¨ç°å‡ä¸ä½³ï¼Œæœ€é«˜æ€§èƒ½ä»…ä¸º54.2%ï¼Œæœ€ä½ä¸º26.1%ã€‚è¿™çªæ˜¾äº†ç°æœ‰LMMsåœ¨å¤šæ¨¡æ€ç§‘å­¦æ¨ç†æ–¹é¢å­˜åœ¨çš„å·¨å¤§æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¼€å‘æ›´å¯é çš„ç§‘å­¦åŠ©æ‰‹ï¼Œè¾…åŠ©ç§‘ç ”äººå‘˜è¿›è¡Œæ–‡çŒ®é˜…è¯»ã€è®ºæ–‡æ’°å†™å’Œå®éªŒè®¾è®¡ã€‚é€šè¿‡æ£€æµ‹å’Œè§£å†³å¤šæ¨¡æ€ä¿¡æ¯çš„ä¸ä¸€è‡´æ€§ï¼Œæé«˜ç§‘ç ”æˆæœçš„æ¸…æ™°åº¦ã€å¯é‡å¤æ€§å’Œå¯ä¿¡åº¦ï¼ŒåŠ é€Ÿç§‘å­¦å‘ç°çš„è¿›ç¨‹ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç§‘å­¦é¢†åŸŸï¼Œå¹¶ç”¨äºè¯„ä¼°å’Œæ”¹è¿›æ›´å¤šç±»å‹çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Multimodal Models (LMMs) are increasingly applied to scientific research, yet it remains unclear whether they can reliably understand and reason over the multimodal complexity of papers. A central challenge lies in detecting and resolving inconsistencies across text, figures, tables, and equations, issues that are often subtle, domain-specific, and ultimately undermine clarity, reproducibility, and trust. Existing benchmarks overlook this issue, either isolating single modalities or relying on synthetic errors that fail to capture real-world complexity. We introduce PRISMM-Bench (Peer-Review-sourced Inconsistency Set for Multimodal Models), the first benchmark grounded in real reviewer-flagged inconsistencies in scientific papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering and human verification, we curate 262 inconsistencies from 242 papers. Based on this set, we design three tasks, namely inconsistency identification, remedy and pair matching, which assess a model's capacity to detect, correct, and reason over inconsistencies across different modalities. Furthermore, to address the notorious problem of choice-only shortcuts in multiple-choice evaluation, where models exploit answer patterns without truly understanding the question, we further introduce structured JSON-based answer representations that minimize linguistic biases by reducing reliance on superficial stylistic cues. We benchmark 21 leading LMMs, including large open-weight models (GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5 with high reasoning). Results reveal strikingly low performance (26.1-54.2%), underscoring the challenge of multimodal scientific reasoning and motivating progress towards trustworthy scientific assistants.

