---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.CV - 2025-10-18
---

# cs.CVï¼ˆ2025-10-18ï¼‰

ğŸ“Š å…± **17** ç¯‡è®ºæ–‡
 | ğŸ”— **5** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (8 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251016442v1-edvd-llama-explainable-deepfake-video-detection-via-multimodal-large.html">EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning</a></td>
  <td>æå‡ºEDVD-LLaMAæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†å®ç°å¯è§£é‡Šçš„Deepfakeè§†é¢‘æ£€æµ‹ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16442v1" data-paper-url="./papers/251016442v1-edvd-llama-explainable-deepfake-video-detection-via-multimodal-large.html" onclick="toggleFavorite(this, '2510.16442v1', 'EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251016598v1-visionselector-end-to-end-learnable-visual-token-compression-for-eff.html">VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs</a></td>
  <td>VisionSelectorï¼šç«¯åˆ°ç«¯å¯å­¦ä¹ çš„è§†è§‰Tokenå‹ç¼©ï¼Œæå‡å¤šæ¨¡æ€LLMæ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16598v1" data-paper-url="./papers/251016598v1-visionselector-end-to-end-learnable-visual-token-compression-for-eff.html" onclick="toggleFavorite(this, '2510.16598v1', 'VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251016660v1-universal-and-transferable-attacks-on-pathology-foundation-models.html">Universal and Transferable Attacks on Pathology Foundation Models</a></td>
  <td>æå‡ºé€šç”¨å¯è¿ç§»å¯¹æŠ—æ‰°åŠ¨UTAPï¼Œæ­ç¤ºç—…ç†å­¦åŸºç¡€æ¨¡å‹çš„è„†å¼±æ€§</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16660v1" data-paper-url="./papers/251016660v1-universal-and-transferable-attacks-on-pathology-foundation-models.html" onclick="toggleFavorite(this, '2510.16660v1', 'Universal and Transferable Attacks on Pathology Foundation Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251016505v2-prismm-bench-a-benchmark-of-peer-review-grounded-multimodal-inconsis.html">PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies</a></td>
  <td>PRISMM-Benchï¼šé¦–ä¸ªåŸºäºåŒè¡Œè¯„å®¡çš„å¤šæ¨¡æ€ä¸ä¸€è‡´æ€§åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LMMsçš„ç§‘å­¦æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16505v2" data-paper-url="./papers/251016505v2-prismm-bench-a-benchmark-of-peer-review-grounded-multimodal-inconsis.html" onclick="toggleFavorite(this, '2510.16505v2', 'PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251016643v1-structured-interfaces-for-automated-reasoning-with-3d-scene-graphs.html">Structured Interfaces for Automated Reasoning with 3D Scene Graphs</a></td>
  <td>æå‡ºåŸºäºç»“æ„åŒ–æ¥å£çš„3Dåœºæ™¯å›¾æ¨ç†æ–¹æ³•ï¼Œæå‡LLMåœ¨æœºå™¨äººè‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16643v1" data-paper-url="./papers/251016643v1-structured-interfaces-for-automated-reasoning-with-3d-scene-graphs.html" onclick="toggleFavorite(this, '2510.16643v1', 'Structured Interfaces for Automated Reasoning with 3D Scene Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251016457v1-navq-learning-a-q-model-for-foresighted-vision-and-language-navigati.html">NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation</a></td>
  <td>NavQï¼šå­¦ä¹ Qæ¨¡å‹ä»¥å®ç°å…·æœ‰å‰ç»æ€§çš„è§†è§‰-è¯­è¨€å¯¼èˆª</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16457v1" data-paper-url="./papers/251016457v1-navq-learning-a-q-model-for-foresighted-vision-and-language-navigati.html" onclick="toggleFavorite(this, '2510.16457v1', 'NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251016446v1-vipamin-visual-prompt-initialization-via-embedding-selection-and-sub.html">VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion</a></td>
  <td>VIPAMINï¼šé€šè¿‡åµŒå…¥é€‰æ‹©å’Œå­ç©ºé—´æ‰©å±•å®ç°è§†è§‰Promptåˆå§‹åŒ–ï¼Œæå‡è‡ªç›‘ç£æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16446v1" data-paper-url="./papers/251016446v1-vipamin-visual-prompt-initialization-via-embedding-selection-and-sub.html" onclick="toggleFavorite(this, '2510.16446v1', 'VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251016290v1-cerberus-real-time-video-anomaly-detection-via-cascaded-vision-langu.html">Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models</a></td>
  <td>Cerberusï¼šåŸºäºçº§è”è§†è§‰-è¯­è¨€æ¨¡å‹çš„å®æ—¶è§†é¢‘å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16290v1" data-paper-url="./papers/251016290v1-cerberus-real-time-video-anomaly-detection-via-cascaded-vision-langu.html" onclick="toggleFavorite(this, '2510.16290v1', 'Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (6 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/251016624v1-self-supervised-learning-to-fly-using-efficient-semantic-segmentatio.html">Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs</a></td>
  <td>æå‡ºä¸€ç§åŸºäºè¯­ä¹‰åˆ†å‰²å’Œå•ç›®æ·±åº¦ä¼°è®¡çš„ä½æˆæœ¬æ— äººæœºè‡ªä¸»é£è¡Œæ–¹æ³•ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16624v1" data-paper-url="./papers/251016624v1-self-supervised-learning-to-fly-using-efficient-semantic-segmentatio.html" onclick="toggleFavorite(this, '2510.16624v1', 'Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251016416v1-ssl4rl-revisiting-self-supervised-learning-as-intrinsic-reward-for-v.html">SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning</a></td>
  <td>SSL4RLï¼šåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ä½œä¸ºè§†è§‰-è¯­è¨€æ¨ç†çš„å†…åœ¨å¥–åŠ±</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16416v1" data-paper-url="./papers/251016416v1-ssl4rl-revisiting-self-supervised-learning-as-intrinsic-reward-for-v.html" onclick="toggleFavorite(this, '2510.16416v1', 'SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251016664v1-hydra-hybrid-knowledge-distillation-and-spectral-reconstruction-algo.html">HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications</a></td>
  <td>æå‡ºHYDRAï¼Œé€šè¿‡æ··åˆçŸ¥è¯†è’¸é¦å’Œå…‰è°±é‡å»ºç®—æ³•å®ç°é«˜é€šé“é«˜å…‰è°±ç›¸æœºåº”ç”¨</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">HSI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16664v1" data-paper-url="./papers/251016664v1-hydra-hybrid-knowledge-distillation-and-spectral-reconstruction-algo.html" onclick="toggleFavorite(this, '2510.16664v1', 'HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251016450v1-instance-aware-pseudo-labeling-and-class-focused-contrastive-learnin.html">Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy</a></td>
  <td>é’ˆå¯¹ç”µå­æ˜¾å¾®é•œå›¾åƒï¼Œæå‡ºå®ä¾‹æ„ŸçŸ¥ä¼ªæ ‡ç­¾å’Œç±»åˆ«èšç„¦å¯¹æ¯”å­¦ä¹ çš„å¼±ç›‘ç£åŸŸè‡ªé€‚åº”åˆ†å‰²æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">contrastive learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16450v1" data-paper-url="./papers/251016450v1-instance-aware-pseudo-labeling-and-class-focused-contrastive-learnin.html" onclick="toggleFavorite(this, '2510.16450v1', 'Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251016444v1-refatomnet-advancing-referring-atomic-video-action-recognition-using.html">RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba</a></td>
  <td>RefAtomNet++ï¼šåˆ©ç”¨è¯­ä¹‰æ£€ç´¢çš„å¤šè½¨è¿¹Mambaæ¨è¿›æŒ‡ä»£è¡¨è¾¾å¼åŸå­è§†é¢‘åŠ¨ä½œè¯†åˆ«</td>
  <td class="tags-cell"><span class="paper-tag">Mamba</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16444v1" data-paper-url="./papers/251016444v1-refatomnet-advancing-referring-atomic-video-action-recognition-using.html" onclick="toggleFavorite(this, '2510.16444v1', 'RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251016333v1-rl-makes-mllms-see-better-than-sft.html">RL makes MLLMs see better than SFT</a></td>
  <td>æå‡ºPIVOTï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–MLLMè§†è§‰ç¼–ç å™¨ï¼Œæ˜¾è‘—æå‡è§†è§‰æ„ŸçŸ¥èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">multimodal</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16333v1" data-paper-url="./papers/251016333v1-rl-makes-mllms-see-better-than-sft.html" onclick="toggleFavorite(this, '2510.16333v1', 'RL makes MLLMs see better than SFT')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/251016410v2-realm-an-mllm-agent-framework-for-open-world-3d-reasoning-segmentati.html">REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting</a></td>
  <td>æå‡ºREALMæ¡†æ¶ä»¥è§£å†³å¤æ‚äººç±»æŒ‡ä»¤ä¸‹çš„3Då¯¹è±¡åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16410v2" data-paper-url="./papers/251016410v2-realm-an-mllm-agent-framework-for-open-world-3d-reasoning-segmentati.html" onclick="toggleFavorite(this, '2510.16410v2', 'REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/251016463v1-hgc-avatar-hierarchical-gaussian-compression-for-streamable-dynamic-.html">HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars</a></td>
  <td>æå‡ºHGC-Avatarï¼Œç”¨äºå¯æµå¼ä¼ è¾“çš„åŠ¨æ€3Då¤´åƒçš„é«˜æ•ˆé«˜æ–¯å‹ç¼©ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">3D gaussian splatting</span> <span class="paper-tag">3DGS</span> <span class="paper-tag">gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16463v1" data-paper-url="./papers/251016463v1-hgc-avatar-hierarchical-gaussian-compression-for-streamable-dynamic-.html" onclick="toggleFavorite(this, '2510.16463v1', 'HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/251016556v2-fit-for-purpose-deepfake-detection-in-the-real-world.html">Fit for Purpose? Deepfake Detection in the Real World</a></td>
  <td>æ„å»ºçœŸå®æ”¿æ²»DeepfakeåŸºå‡†ï¼Œæ­ç¤ºç°æœ‰æ£€æµ‹å™¨æ³›åŒ–èƒ½åŠ›ä¸è¶³</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2510.16556v2" data-paper-url="./papers/251016556v2-fit-for-purpose-deepfake-detection-in-the-real-world.html" onclick="toggleFavorite(this, '2510.16556v2', 'Fit for Purpose? Deepfake Detection in the Real World')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.CV é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)