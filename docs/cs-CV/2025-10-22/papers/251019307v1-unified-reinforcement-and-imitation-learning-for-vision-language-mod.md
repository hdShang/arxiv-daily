---
layout: default
title: Unified Reinforcement and Imitation Learning for Vision-Language Models
---

# Unified Reinforcement and Imitation Learning for Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19307" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19307v1</a>
  <a href="https://arxiv.org/pdf/2510.19307.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19307v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.19307v1', 'Unified Reinforcement and Imitation Learning for Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22

**å¤‡æ³¨**: NeurIPS 2025, Project page: https://byungkwanlee.github.io/RIL-page

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€å¼ºåŒ–ä¸æ¨¡ä»¿å­¦ä¹ (RIL)ç®—æ³•ï¼Œç”¨äºè®­ç»ƒè½»é‡çº§è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `æ¨¡å‹å‹ç¼©` `è½»é‡çº§æ¨¡å‹` `æ–‡æœ¬ç”Ÿæˆ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMæ¨¡å‹è§„æ¨¡åºå¤§ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²å’Œåº”ç”¨ã€‚
2. RILç®—æ³•ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ ï¼Œä½¿è½»é‡çº§å­¦ç”Ÿæ¨¡å‹æ¨¡ä»¿å¤§å‹æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–ä¿¡å·æå‡ç”Ÿæˆèƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRILç®—æ³•æ˜¾è‘—ç¼©å°äº†ä¸å…ˆè¿›VLMçš„æ€§èƒ½å·®è·ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†å®ƒä»¬ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLM)å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡ä½¿å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éš¾ä»¥åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–è€Œé«˜æ•ˆçš„è®­ç»ƒç®—æ³•â€”â€”ç»Ÿä¸€å¼ºåŒ–ä¸æ¨¡ä»¿å­¦ä¹ (RIL)ï¼Œæ—¨åœ¨åˆ›å»ºå¼ºå¤§çš„è½»é‡çº§VLMã€‚RILç‹¬ç‰¹åœ°ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œä½¿è¾ƒå°çš„å­¦ç”ŸVLMä¸ä»…èƒ½å¤Ÿæ¨¡ä»¿å¤§å‹æ•™å¸ˆæ¨¡å‹å¤æ‚çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜èƒ½é€šè¿‡å¼ºåŒ–ä¿¡å·ç³»ç»Ÿåœ°æé«˜å…¶ç”Ÿæˆèƒ½åŠ›ã€‚æ¨¡ä»¿æ¡†æ¶çš„å…³é”®åœ¨äºåŸºäºLLMçš„åˆ¤åˆ«å™¨ï¼Œå®ƒèƒ½å·§å¦™åœ°åŒºåˆ†å­¦ç”Ÿå’Œæ•™å¸ˆçš„è¾“å‡ºï¼Œå¹¶è¾…ä»¥å¤šä¸ªå¤§å‹æ•™å¸ˆVLMçš„æŒ‡å¯¼ï¼Œä»¥ç¡®ä¿å­¦ä¹ çš„å¤šæ ·æ€§ã€‚è¿™ç§ç»Ÿä¸€çš„å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨å¼ºåŒ–å’Œæ¨¡ä»¿ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿè·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½¿å…¶ä¸é¢†å…ˆçš„é—­æºVLMç›¸åª²ç¾ã€‚åœ¨å„ç§è§†è§‰-è¯­è¨€åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRILæ˜¾è‘—ç¼©å°äº†ä¸æœ€å…ˆè¿›çš„å¼€æºå’Œé—­æºVLMçš„æ€§èƒ½å·®è·ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†å®ƒä»¬ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šå¸¸å‚æ•°é‡å·¨å¤§ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œå¾®è°ƒï¼Œä½†è¿™ç§æ–¹å¼éš¾ä»¥è®­ç»ƒå‡ºæ—¢é«˜æ•ˆåˆå¼ºå¤§çš„å°å‹VLMã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ è®©å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¢ç´¢æ›´ä¼˜çš„ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡æ¨¡ä»¿å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¿¡å·æ¥ä¼˜åŒ–ç”Ÿæˆç­–ç•¥ï¼Œä»è€Œåœ¨ä¿æŒæ¨¡å‹è½»é‡åŒ–çš„åŒæ—¶ï¼Œæå‡æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRILæ¡†æ¶åŒ…å«ä¸€ä¸ªå­¦ç”ŸVLMï¼Œå¤šä¸ªæ•™å¸ˆVLMå’Œä¸€ä¸ªåŸºäºLLMçš„åˆ¤åˆ«å™¨ã€‚å­¦ç”ŸVLMè´Ÿè´£ç”Ÿæˆæ–‡æœ¬ï¼Œæ•™å¸ˆVLMæä¾›æ¨¡ä»¿å­¦ä¹ çš„ç›®æ ‡ï¼Œåˆ¤åˆ«å™¨ç”¨äºåŒºåˆ†å­¦ç”Ÿå’Œæ•™å¸ˆçš„è¾“å‡ºã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚åœ¨æ¨¡ä»¿å­¦ä¹ é˜¶æ®µï¼Œå­¦ç”ŸVLMå­¦ä¹ æ¨¡ä»¿æ•™å¸ˆVLMçš„è¾“å‡ºã€‚åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œå­¦ç”ŸVLMæ ¹æ®åˆ¤åˆ«å™¨çš„åé¦ˆè¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šRILçš„å…³é”®åˆ›æ–°åœ¨äºç»Ÿä¸€äº†å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ ï¼Œå¹¶ä½¿ç”¨åŸºäºLLMçš„åˆ¤åˆ«å™¨æ¥æä¾›æ›´å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ã€‚ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ å¯èƒ½å¯¼è‡´æ¨¡å‹é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œè€Œå¼ºåŒ–å­¦ä¹ å¯ä»¥å¸®åŠ©æ¨¡å‹æ¢ç´¢æ›´ä¼˜çš„ç­–ç•¥ã€‚åŸºäºLLMçš„åˆ¤åˆ«å™¨èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ï¼Œä»è€Œæä¾›æ›´æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡ä»¿å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è¡¡é‡å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨ç­–ç•¥æ¢¯åº¦ç®—æ³•æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹çš„ç”Ÿæˆç­–ç•¥ã€‚åˆ¤åˆ«å™¨é‡‡ç”¨é¢„è®­ç»ƒçš„LLMï¼Œå¹¶ä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥åŒºåˆ†å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºã€‚å…·ä½“å‚æ•°è®¾ç½®ï¼ˆå¦‚å­¦ä¹ ç‡ã€å¥–åŠ±å‡½æ•°ç­‰ï¼‰æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRILç®—æ³•åœ¨å¤šä¸ªè§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒæè¿°ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒRILç®—æ³•è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹åœ¨CIDEræŒ‡æ ‡ä¸Šè¶…è¶Šäº†å¤šä¸ªå¼€æºå’Œé—­æºçš„VLMæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒRILç®—æ³•è¿˜æ˜¾è‘—ç¼©å°äº†ä¸æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½å·®è·ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†å®ƒä»¬ï¼Œè¯æ˜äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§èµ„æºå—é™çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡ä¸Šçš„å›¾åƒæè¿°ç”Ÿæˆã€æ™ºèƒ½åŠ©æ‰‹çš„è§†è§‰é—®ç­”ç­‰ã€‚é€šè¿‡è®­ç»ƒè½»é‡çº§ä¸”é«˜æ€§èƒ½çš„VLMï¼Œå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œæé«˜å“åº”é€Ÿåº¦ï¼Œå¹¶æ‰©å±•VLMçš„åº”ç”¨èŒƒå›´ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is an LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them.

