---
layout: default
title: CARES: Context-Aware Resolution Selector for VLMs
---

# CARES: Context-Aware Resolution Selector for VLMs

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.19496" target="_blank" class="toolbar-btn">arXiv: 2510.19496v1</a>
    <a href="https://arxiv.org/pdf/2510.19496.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19496v1" 
            onclick="toggleFavorite(this, '2510.19496v1', 'CARES: Context-Aware Resolution Selector for VLMs')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Moshe Kimhi, Nimrod Shabtay, Raja Giryes, Chaim Baskin, Eli Schwartz

**ÂàÜÁ±ª**: cs.CV, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-22

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫CARES‰∏ä‰∏ãÊñáÊÑüÁü•ÂàÜËæ®ÁéáÈÄâÊã©Âô®ÔºåÈôç‰ΩéVLMËÆ°ÁÆóÊàêÊú¨Âπ∂‰øùÊåÅÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ÂàÜËæ®ÁéáÈÄâÊã©` `‰∏ä‰∏ãÊñáÊÑüÁü•` `ËÆ°ÁÆóÊïàÁéá` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLM‰∏∫‰øùËØÅÊÄßËÉΩÔºåÈÄöÂ∏∏ÈááÁî®È´òÂàÜËæ®ÁéáÂõæÂÉèÔºåÂØºËá¥ËÆ°ÁÆóÊàêÊú¨ÂíåÂª∂ËøüËøáÈ´òÔºåÂç≥‰Ωø‰ΩéÂàÜËæ®ÁéáÂõæÂÉèÂ∑≤Ë∂≥Â§ü„ÄÇ
2. CARESÈÄöËøáÈ¢ÑÊµãÂõæÂÉè-Êü•ËØ¢ÂØπÊâÄÈúÄÁöÑÊúÄÂ∞èÂàÜËæ®ÁéáÔºåÂú®È¢ÑÂ§ÑÁêÜÈò∂ÊÆµÈôç‰ΩéVLMÁöÑËÆ°ÁÆóË¥üÊãÖÔºåÂêåÊó∂‰øùÊåÅÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåCARESÂú®Â§ö‰∏™Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠ÔºåËÉΩÂ§üÊòæËëóÈôç‰ΩéËÆ°ÁÆóÈáèÔºàÈ´òËææ80%ÔºâÔºåÂêåÊó∂‰øùÊåÅ‰ªªÂä°ÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã(VLM)ÈÄöÂ∏∏‰ª•ÂéüÂßãÊàñÈ´òÂàÜËæ®ÁéáÂ§ÑÁêÜÂõæÂÉèÔºå‰ª•‰øùÊåÅË∑®‰ªªÂä°ÁöÑÊúâÊïàÊÄß„ÄÇËøôÂØºËá¥ËßÜËßâtokensÂç†ÊçÆÊÄªtokensÁöÑ97-99%ÔºåÂç≥‰Ωø‰ΩéÂàÜËæ®ÁéáÂõæÂÉèÂ∞±Ë∂≥Â§üÔºå‰πü‰ºöÂØºËá¥È´òËÆ°ÁÆóÈáèÂíåÂª∂Ëøü„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜCARES‚Äî‚Äî‰∏ä‰∏ãÊñáÊÑüÁü•ÂàÜËæ®ÁéáÈÄâÊã©Âô®ÔºåËøôÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÈ¢ÑÂ§ÑÁêÜÊ®°ÂùóÔºåÁªôÂÆöÂõæÂÉè-Êü•ËØ¢ÂØπÔºåÈ¢ÑÊµãÊúÄÂ∞èÁöÑË∂≥Â§üËæìÂÖ•ÂàÜËæ®Áéá„ÄÇCARES‰ΩøÁî®‰∏Ä‰∏™Á¥ßÂáëÁöÑVLMÔºà350MÔºâÊù•ÊèêÂèñÁâπÂæÅÔºåÂπ∂È¢ÑÊµãÁõÆÊ†áÈ¢ÑËÆ≠ÁªÉVLMÁöÑÂìçÂ∫î‰ΩïÊó∂Êî∂ÊïõÂà∞ÂÖ∂Ê≠£Á°ÆÂõûÁ≠îËÉΩÂäõÁöÑÂ≥∞ÂÄº„ÄÇËôΩÁÑ∂CARESË¢´ËÆ≠ÁªÉ‰∏∫‰∏ÄÁªÑÂèØÈÄâÂàÜËæ®Áéá‰∏äÁöÑÁ¶ªÊï£ÂàÜÁ±ªÂô®Ôºå‰ΩÜÂÆÉÂú®Êé®ÁêÜÊó∂ÊèíÂÄºËøûÁª≠ÂàÜËæ®Áéá‰ª•ËøõË°åÁªÜÁ≤íÂ∫¶ÊéßÂà∂„ÄÇÂú®Ë∑®Ë∂äÊñáÊ°£ÂíåËá™ÁÑ∂ÂõæÂÉèÁöÑ‰∫î‰∏™Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰ª•Âèä‰∏çÂêåÁöÑÁõÆÊ†áVLM‰∏≠ÔºåCARESÂú®‰øùÊåÅ‰ªªÂä°ÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÂ∞ÜËÆ°ÁÆóÈáèÈôç‰ΩéÈ´òËææ80%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰∏∫‰∫Ü‰øùËØÅÂú®ÂêÑÁßç‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÔºåÈÄöÂ∏∏Áõ¥Êé•‰ΩøÁî®È´òÂàÜËæ®ÁéáÁöÑÂõæÂÉè‰Ωú‰∏∫ËæìÂÖ•„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÂÅöÊ≥ïÂØºËá¥ËßÜËßâtokensÁöÑÊï∞ÈáèÂç†ÊçÆ‰∫ÜÊÄªtokensÁöÑÁªùÂ§ßÈÉ®ÂàÜÔºà97%-99%ÔºâÔºåÊòæËëóÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨ÂíåÂª∂Ëøü„ÄÇÂç≥‰ΩøÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÔºå‰ΩéÂàÜËæ®ÁéáÁöÑÂõæÂÉèÂ∑≤ÁªèË∂≥Â§üÂÆåÊàê‰ªªÂä°ÔºåVLM‰ªçÁÑ∂‰ºöÂ§ÑÁêÜÈ´òÂàÜËæ®ÁéáÂõæÂÉèÔºåÈÄ†Êàê‰∫Ü‰∏çÂøÖË¶ÅÁöÑËµÑÊ∫êÊµ™Ë¥π„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊ†πÊçÆÂõæÂÉèÂíåÊü•ËØ¢ÁöÑÂÜÖÂÆπÔºåËá™ÈÄÇÂ∫îÂú∞ÈÄâÊã©ÂêàÈÄÇÁöÑËæìÂÖ•ÂàÜËæ®ÁéáÔºåÊàê‰∏∫‰∫Ü‰∏Ä‰∏™‰∫üÂæÖËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöCARESÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ≠¶‰π†‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÊ®°ÂùóÔºåËØ•Ê®°ÂùóËÉΩÂ§üÊ†πÊçÆÁªôÂÆöÁöÑÂõæÂÉè-Êü•ËØ¢ÂØπÔºåÈ¢ÑÊµãÁõÆÊ†áVLMËÉΩÂ§üÊ≠£Á°ÆÂõûÁ≠îÈóÆÈ¢òÊâÄÈúÄÁöÑÊúÄÂ∞èÂàÜËæ®Áéá„ÄÇÈÄöËøáÂú®È¢ÑÂ§ÑÁêÜÈò∂ÊÆµÈÄâÊã©ÂêàÈÄÇÁöÑÂàÜËæ®ÁéáÔºåÂèØ‰ª•ÊòæËëóÂáèÂ∞ëVLMÁöÑËÆ°ÁÆóÈáèÔºåÂêåÊó∂‰øùÊåÅÂÖ∂ÊÄßËÉΩ„ÄÇCARESÁöÑÂÖ≥ÈîÆÂú®‰∫éÂ≠¶‰π†‰∏Ä‰∏™ËÉΩÂ§üÂáÜÁ°ÆÈ¢ÑÊµãÁõÆÊ†áVLMÊÄßËÉΩÁöÑ‰ª£ÁêÜÊ®°Âûã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöCARES‰Ωú‰∏∫‰∏Ä‰∏™È¢ÑÂ§ÑÁêÜÊ®°ÂùóÔºå‰Ωç‰∫éÂõæÂÉèÂíåÊü•ËØ¢ËæìÂÖ•Âà∞ÁõÆÊ†áVLM‰πãÂâç„ÄÇÂÖ∂Êï¥‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) ËæìÂÖ•ÂõæÂÉèÂíåÊü•ËØ¢Ôºõ2) CARES‰ΩøÁî®‰∏Ä‰∏™Á¥ßÂáëÁöÑVLMÔºà350MÔºâÊèêÂèñÂõæÂÉèÂíåÊü•ËØ¢ÁöÑÁâπÂæÅÔºõ3) CARESÂü∫‰∫éÊèêÂèñÁöÑÁâπÂæÅÔºåÈ¢ÑÊµãÁõÆÊ†áVLMËÉΩÂ§üÊ≠£Á°ÆÂõûÁ≠îÈóÆÈ¢òÊâÄÈúÄÁöÑÊúÄÂ∞èÂàÜËæ®ÁéáÔºõ4) Â∞ÜÂõæÂÉèÁº©ÊîæÂà∞È¢ÑÊµãÁöÑÂàÜËæ®ÁéáÔºõ5) Â∞ÜÁº©ÊîæÂêéÁöÑÂõæÂÉèÂíåÊü•ËØ¢ËæìÂÖ•Âà∞ÁõÆÊ†áVLMËøõË°åÂ§ÑÁêÜ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöCARESÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÂàÜËæ®ÁéáÈÄâÊã©Êú∫Âà∂„ÄÇ‰∏é‰ª•ÂæÄÁõ¥Êé•‰ΩøÁî®Âõ∫ÂÆöÂàÜËæ®ÁéáÁöÑÊñπÊ≥ï‰∏çÂêåÔºåCARESËÉΩÂ§üÊ†πÊçÆÂõæÂÉèÂíåÊü•ËØ¢ÁöÑÂÜÖÂÆπÔºåÂä®ÊÄÅÂú∞ÈÄâÊã©ÂêàÈÄÇÁöÑÂàÜËæ®Áéá„ÄÇÊ≠§Â§ñÔºåCARES‰ΩøÁî®‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑVLM‰Ωú‰∏∫‰ª£ÁêÜÊ®°ÂûãÔºåÈÅøÂÖç‰∫ÜÂºïÂÖ•ËøáÂ§öÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇCARESËøòÂú®Êé®ÁêÜÊó∂ÊèíÂÄºËøûÁª≠ÂàÜËæ®ÁéáÔºåÂÆûÁé∞‰∫ÜÁªÜÁ≤íÂ∫¶ÁöÑÊéßÂà∂„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöCARES‰ΩøÁî®‰∏Ä‰∏™Á¥ßÂáëÁöÑVLMÔºà350MÔºâ‰Ωú‰∏∫ÁâπÂæÅÊèêÂèñÂô®ÂíåÂàÜËæ®ÁéáÈ¢ÑÊµãÂô®„ÄÇCARESË¢´ËÆ≠ÁªÉ‰∏∫‰∏Ä‰∏™Á¶ªÊï£ÂàÜÁ±ªÂô®ÔºåÁõÆÊ†áÊòØÈ¢ÑÊµã‰∏ÄÁªÑÂèØÈÄâÂàÜËæ®Áéá‰∏≠ÁöÑÊúÄ‰Ω≥ÂàÜËæ®Áéá„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Êõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÊéßÂà∂ÔºåCARESÂú®Êé®ÁêÜÊó∂ÂØπËøûÁª≠ÂàÜËæ®ÁéáËøõË°åÊèíÂÄº„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°Êó®Âú®ÈºìÂä±CARESÈÄâÊã©ËÉΩÂ§ü‰ΩøÁõÆÊ†áVLMÊ≠£Á°ÆÂõûÁ≠îÈóÆÈ¢òÁöÑÊúÄÂ∞èÂàÜËæ®Áéá„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊçüÂ§±ÂáΩÊï∞ÂèØËÉΩÂåÖÂê´‰∏Ä‰∏™ÂàÜÁ±ªÊçüÂ§±È°πÔºà‰æãÂ¶Ç‰∫§ÂèâÁÜµÊçüÂ§±ÔºâÔºåÁî®‰∫éË°°ÈáèCARESÈ¢ÑÊµãÂàÜËæ®ÁéáÁöÑÂáÜÁ°ÆÊÄßÔºå‰ª•Âèä‰∏Ä‰∏™Ê≠£ÂàôÂåñÈ°πÔºåÁî®‰∫éÊÉ©ÁΩöÈÄâÊã©ËøáÈ´òÂàÜËæ®ÁéáÁöÑÊÉÖÂÜµ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCARESÂú®‰∫î‰∏™Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠ÔºåËÉΩÂ§üÊòæËëóÈôç‰ΩéVLMÁöÑËÆ°ÁÆóÈáèÔºåÂêåÊó∂‰øùÊåÅ‰ªªÂä°ÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåCARESÂèØ‰ª•Â∞ÜËÆ°ÁÆóÈáèÈôç‰ΩéÈ´òËææ80%ÔºåËÄåÊÄßËÉΩ‰∏ãÈôçÂèØ‰ª•ÂøΩÁï•‰∏çËÆ°„ÄÇCARESÂú®ÊñáÊ°£ÂíåËá™ÁÑ∂ÂõæÂÉè‰∏äÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂‰∏îÈÄÇÁî®‰∫é‰∏çÂêåÁöÑÁõÆÊ†áVLM„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

CARESÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØ‰ª•Â∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂ§ÑÁêÜÂõæÂÉèÂíåÊñáÊú¨ÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÂõæÂÉèÊèèËø∞„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÊñáÊ°£ÁêÜËß£Á≠â„ÄÇÈÄöËøáÈôç‰ΩéVLMÁöÑËÆ°ÁÆóÊàêÊú¨ÔºåCARESÂèØ‰ª•‰ΩøËøô‰∫õ‰ªªÂä°Âú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äËøêË°åÔºå‰æãÂ¶ÇÁßªÂä®ËÆæÂ§áÂíåÂµåÂÖ•ÂºèÁ≥ªÁªü„ÄÇÊ≠§Â§ñÔºåCARESËøòÂèØ‰ª•Áî®‰∫éÂä†ÈÄüVLMÁöÑËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ãÔºåÊèêÈ´òÂÖ∂ÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large vision-language models (VLMs) commonly process images at native or high resolution to remain effective across tasks. This inflates visual tokens ofter to 97-99% of total tokens, resulting in high compute and latency, even when low-resolution images would suffice. We introduce \emph{CARES}-a \textbf{C}ontext-\textbf{A}ware \textbf{R}esolution \textbf{S}elector, a lightweight preprocessing module that, given an image-query pair, predicts the \emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to extract features and predict when a target pretrained VLM's response converges to its peak ability to answer correctly. Though trained as a discrete classifier over a set of optional resolutions, CARES interpolates continuous resolutions at inference for fine-grained control. Across five multimodal benchmarks spanning documents and natural images, as well as diverse target VLMs, CARES preserves task performance while reducing compute by up to 80%.

