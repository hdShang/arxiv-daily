---
layout: default
title: CARES: Context-Aware Resolution Selector for VLMs
---

# CARES: Context-Aware Resolution Selector for VLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19496" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19496v1</a>
  <a href="https://arxiv.org/pdf/2510.19496.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19496v1" onclick="toggleFavorite(this, '2510.19496v1', 'CARES: Context-Aware Resolution Selector for VLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Moshe Kimhi, Nimrod Shabtay, Raja Giryes, Chaim Baskin, Eli Schwartz

**åˆ†ç±»**: cs.CV, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCARESä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¾¨ç‡é€‰æ‹©å™¨ï¼Œé™ä½VLMè®¡ç®—æˆæœ¬å¹¶ä¿æŒæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `åˆ†è¾¨ç‡é€‰æ‹©` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `è®¡ç®—æ•ˆç‡` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLMä¸ºä¿è¯æ€§èƒ½ï¼Œé€šå¸¸é‡‡ç”¨é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿè¿‡é«˜ï¼Œå³ä½¿ä½åˆ†è¾¨ç‡å›¾åƒå·²è¶³å¤Ÿã€‚
2. CARESé€šè¿‡é¢„æµ‹å›¾åƒ-æŸ¥è¯¢å¯¹æ‰€éœ€çš„æœ€å°åˆ†è¾¨ç‡ï¼Œåœ¨é¢„å¤„ç†é˜¶æ®µé™ä½VLMçš„è®¡ç®—è´Ÿæ‹…ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒCARESåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è®¡ç®—é‡ï¼ˆé«˜è¾¾80%ï¼‰ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(VLM)é€šå¸¸ä»¥åŸå§‹æˆ–é«˜åˆ†è¾¨ç‡å¤„ç†å›¾åƒï¼Œä»¥ä¿æŒè·¨ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚è¿™å¯¼è‡´è§†è§‰tokenså æ®æ€»tokensçš„97-99%ï¼Œå³ä½¿ä½åˆ†è¾¨ç‡å›¾åƒå°±è¶³å¤Ÿï¼Œä¹Ÿä¼šå¯¼è‡´é«˜è®¡ç®—é‡å’Œå»¶è¿Ÿã€‚æˆ‘ä»¬å¼•å…¥äº†CARESâ€”â€”ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¾¨ç‡é€‰æ‹©å™¨ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§é¢„å¤„ç†æ¨¡å—ï¼Œç»™å®šå›¾åƒ-æŸ¥è¯¢å¯¹ï¼Œé¢„æµ‹æœ€å°çš„è¶³å¤Ÿè¾“å…¥åˆ†è¾¨ç‡ã€‚CARESä½¿ç”¨ä¸€ä¸ªç´§å‡‘çš„VLMï¼ˆ350Mï¼‰æ¥æå–ç‰¹å¾ï¼Œå¹¶é¢„æµ‹ç›®æ ‡é¢„è®­ç»ƒVLMçš„å“åº”ä½•æ—¶æ”¶æ•›åˆ°å…¶æ­£ç¡®å›ç­”èƒ½åŠ›çš„å³°å€¼ã€‚è™½ç„¶CARESè¢«è®­ç»ƒä¸ºä¸€ç»„å¯é€‰åˆ†è¾¨ç‡ä¸Šçš„ç¦»æ•£åˆ†ç±»å™¨ï¼Œä½†å®ƒåœ¨æ¨ç†æ—¶æ’å€¼è¿ç»­åˆ†è¾¨ç‡ä»¥è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚åœ¨è·¨è¶Šæ–‡æ¡£å’Œè‡ªç„¶å›¾åƒçš„äº”ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä»¥åŠä¸åŒçš„ç›®æ ‡VLMä¸­ï¼ŒCARESåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå°†è®¡ç®—é‡é™ä½é«˜è¾¾80%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸ºäº†ä¿è¯åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œé€šå¸¸ç›´æ¥ä½¿ç”¨é«˜åˆ†è¾¨ç‡çš„å›¾åƒä½œä¸ºè¾“å…¥ã€‚ç„¶è€Œï¼Œè¿™ç§åšæ³•å¯¼è‡´è§†è§‰tokensçš„æ•°é‡å æ®äº†æ€»tokensçš„ç»å¤§éƒ¨åˆ†ï¼ˆ97%-99%ï¼‰ï¼Œæ˜¾è‘—å¢åŠ äº†è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿã€‚å³ä½¿åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½åˆ†è¾¨ç‡çš„å›¾åƒå·²ç»è¶³å¤Ÿå®Œæˆä»»åŠ¡ï¼ŒVLMä»ç„¶ä¼šå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œé€ æˆäº†ä¸å¿…è¦çš„èµ„æºæµªè´¹ã€‚å› æ­¤ï¼Œå¦‚ä½•æ ¹æ®å›¾åƒå’ŒæŸ¥è¯¢çš„å†…å®¹ï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©åˆé€‚çš„è¾“å…¥åˆ†è¾¨ç‡ï¼Œæˆä¸ºäº†ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCARESçš„æ ¸å¿ƒæ€è·¯æ˜¯å­¦ä¹ ä¸€ä¸ªè½»é‡çº§çš„æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„å›¾åƒ-æŸ¥è¯¢å¯¹ï¼Œé¢„æµ‹ç›®æ ‡VLMèƒ½å¤Ÿæ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€çš„æœ€å°åˆ†è¾¨ç‡ã€‚é€šè¿‡åœ¨é¢„å¤„ç†é˜¶æ®µé€‰æ‹©åˆé€‚çš„åˆ†è¾¨ç‡ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘VLMçš„è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒå…¶æ€§èƒ½ã€‚CARESçš„å…³é”®åœ¨äºå­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ç›®æ ‡VLMæ€§èƒ½çš„ä»£ç†æ¨¡å‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCARESä½œä¸ºä¸€ä¸ªé¢„å¤„ç†æ¨¡å—ï¼Œä½äºå›¾åƒå’ŒæŸ¥è¯¢è¾“å…¥åˆ°ç›®æ ‡VLMä¹‹å‰ã€‚å…¶æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š1) è¾“å…¥å›¾åƒå’ŒæŸ¥è¯¢ï¼›2) CARESä½¿ç”¨ä¸€ä¸ªç´§å‡‘çš„VLMï¼ˆ350Mï¼‰æå–å›¾åƒå’ŒæŸ¥è¯¢çš„ç‰¹å¾ï¼›3) CARESåŸºäºæå–çš„ç‰¹å¾ï¼Œé¢„æµ‹ç›®æ ‡VLMèƒ½å¤Ÿæ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€çš„æœ€å°åˆ†è¾¨ç‡ï¼›4) å°†å›¾åƒç¼©æ”¾åˆ°é¢„æµ‹çš„åˆ†è¾¨ç‡ï¼›5) å°†ç¼©æ”¾åçš„å›¾åƒå’ŒæŸ¥è¯¢è¾“å…¥åˆ°ç›®æ ‡VLMè¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šCARESçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆ†è¾¨ç‡é€‰æ‹©æœºåˆ¶ã€‚ä¸ä»¥å¾€ç›´æ¥ä½¿ç”¨å›ºå®šåˆ†è¾¨ç‡çš„æ–¹æ³•ä¸åŒï¼ŒCARESèƒ½å¤Ÿæ ¹æ®å›¾åƒå’ŒæŸ¥è¯¢çš„å†…å®¹ï¼ŒåŠ¨æ€åœ°é€‰æ‹©åˆé€‚çš„åˆ†è¾¨ç‡ã€‚æ­¤å¤–ï¼ŒCARESä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„VLMä½œä¸ºä»£ç†æ¨¡å‹ï¼Œé¿å…äº†å¼•å…¥è¿‡å¤šçš„è®¡ç®—å¼€é”€ã€‚CARESè¿˜åœ¨æ¨ç†æ—¶æ’å€¼è¿ç»­åˆ†è¾¨ç‡ï¼Œå®ç°äº†ç»†ç²’åº¦çš„æ§åˆ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šCARESä½¿ç”¨ä¸€ä¸ªç´§å‡‘çš„VLMï¼ˆ350Mï¼‰ä½œä¸ºç‰¹å¾æå–å™¨å’Œåˆ†è¾¨ç‡é¢„æµ‹å™¨ã€‚CARESè¢«è®­ç»ƒä¸ºä¸€ä¸ªç¦»æ•£åˆ†ç±»å™¨ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ä¸€ç»„å¯é€‰åˆ†è¾¨ç‡ä¸­çš„æœ€ä½³åˆ†è¾¨ç‡ã€‚ä¸ºäº†å®ç°æ›´ç»†ç²’åº¦çš„æ§åˆ¶ï¼ŒCARESåœ¨æ¨ç†æ—¶å¯¹è¿ç»­åˆ†è¾¨ç‡è¿›è¡Œæ’å€¼ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡æ—¨åœ¨é¼“åŠ±CARESé€‰æ‹©èƒ½å¤Ÿä½¿ç›®æ ‡VLMæ­£ç¡®å›ç­”é—®é¢˜çš„æœ€å°åˆ†è¾¨ç‡ã€‚å…·ä½“è€Œè¨€ï¼ŒæŸå¤±å‡½æ•°å¯èƒ½åŒ…å«ä¸€ä¸ªåˆ†ç±»æŸå¤±é¡¹ï¼ˆä¾‹å¦‚äº¤å‰ç†µæŸå¤±ï¼‰ï¼Œç”¨äºè¡¡é‡CARESé¢„æµ‹åˆ†è¾¨ç‡çš„å‡†ç¡®æ€§ï¼Œä»¥åŠä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºæƒ©ç½šé€‰æ‹©è¿‡é«˜åˆ†è¾¨ç‡çš„æƒ…å†µã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCARESåœ¨äº”ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½VLMçš„è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼ŒCARESå¯ä»¥å°†è®¡ç®—é‡é™ä½é«˜è¾¾80%ï¼Œè€Œæ€§èƒ½ä¸‹é™å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚CARESåœ¨æ–‡æ¡£å’Œè‡ªç„¶å›¾åƒä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”é€‚ç”¨äºä¸åŒçš„ç›®æ ‡VLMã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CARESå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å¤„ç†å›¾åƒå’Œæ–‡æœ¬çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€æ–‡æ¡£ç†è§£ç­‰ã€‚é€šè¿‡é™ä½VLMçš„è®¡ç®—æˆæœ¬ï¼ŒCARESå¯ä»¥ä½¿è¿™äº›ä»»åŠ¡åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œä¾‹å¦‚ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒCARESè¿˜å¯ä»¥ç”¨äºåŠ é€ŸVLMçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œæé«˜å…¶æ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large vision-language models (VLMs) commonly process images at native or high resolution to remain effective across tasks. This inflates visual tokens ofter to 97-99% of total tokens, resulting in high compute and latency, even when low-resolution images would suffice. We introduce \emph{CARES}-a \textbf{C}ontext-\textbf{A}ware \textbf{R}esolution \textbf{S}elector, a lightweight preprocessing module that, given an image-query pair, predicts the \emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to extract features and predict when a target pretrained VLM's response converges to its peak ability to answer correctly. Though trained as a discrete classifier over a set of optional resolutions, CARES interpolates continuous resolutions at inference for fine-grained control. Across five multimodal benchmarks spanning documents and natural images, as well as diverse target VLMs, CARES preserves task performance while reducing compute by up to 80%.

