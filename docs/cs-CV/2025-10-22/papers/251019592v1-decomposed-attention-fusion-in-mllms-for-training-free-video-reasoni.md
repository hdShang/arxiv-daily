---
layout: default
title: Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation
---

# Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.19592" target="_blank" class="toolbar-btn">arXiv: 2510.19592v1</a>
    <a href="https://arxiv.org/pdf/2510.19592.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19592v1" 
            onclick="toggleFavorite(this, '2510.19592v1', 'Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-22

**Â§áÊ≥®**: Project page: https://www.jshyun.me/projects/decaf

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/HYUNJS/DecAF)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Decomposed Attention Fusion (DecAF)ÔºåÁî®‰∫éMLLMÁöÑÂÖçËÆ≠ÁªÉËßÜÈ¢ëÊé®ÁêÜÂàÜÂâ≤**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜÈ¢ëÊé®ÁêÜÂàÜÂâ≤` `Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÂÖçËÆ≠ÁªÉÂ≠¶‰π†` `ËßÜËßâÂÆö‰Ωç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÊèêÂèñÁöÑÂéüÂßãÊ≥®ÊÑèÂäõÂõæÂô™Â£∞Â§ßÔºå‰∏éÁõÆÊ†áÂå∫ÂüüÂØπÈΩêÊïàÊûúÂ∑ÆÔºåÈôêÂà∂‰∫ÜMLLMÂú®ËßÜÈ¢ëÊé®ÁêÜÂàÜÂâ≤‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. DecAFÈÄöËøáÂØπÊØîÂØπË±°-ËÉåÊôØËûçÂêàÂíå‰∫íË°•ËßÜÈ¢ë-Â∏ßËûçÂêàÔºåÊèêÁ∫ØÊ≥®ÊÑèÂäõÂõæÔºåÂ¢ûÂº∫ÁõÆÊ†áÂå∫ÂüüÁöÑÂÖ≥Ê≥®ÔºåÊó†ÈúÄËÆ≠ÁªÉÂç≥ÂèØÁîüÊàêÂàÜÂâ≤Êé©Á†Å„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDecAFÂú®ËßÜÈ¢ëÂàÜÂâ≤‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂÖçËÆ≠ÁªÉÊñπÊ≥ïÔºåÊÄßËÉΩÂèØ‰∏éÈúÄË¶ÅËÆ≠ÁªÉÁöÑÊñπÊ≥ïÁõ∏Â™≤Áæé„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã(MLLM)ÈÄöËøáÂÖ≥Ê≥®‰∏éÊñáÊú¨Êü•ËØ¢Áõ∏ÂÖ≥ÁöÑËßÜËßâtokensÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõ„ÄÇ‰∏∫‰∫Ü‰ª•ÂÖçËÆ≠ÁªÉÁöÑÊñπÂºèÁõ¥Êé•Â∞ÜÂÖ∂Â∫îÁî®‰∫éÂÆö‰Ωç‰ªªÂä°ÔºåÊú¨ÊñáÂ∞ÜËßÜÈ¢ëÊé®ÁêÜÂàÜÂâ≤ËΩ¨Âåñ‰∏∫ËßÜÈ¢ëÈóÆÁ≠î‰ªªÂä°ÔºåÂπ∂ÈÄöËøárolloutÊú∫Âà∂ÊèêÂèñÊ≥®ÊÑèÂäõÂõæ„ÄÇÁÑ∂ËÄåÔºåÂéüÂßãÊ≥®ÊÑèÂäõÂõæÂ≠òÂú®Âô™Â£∞Ôºå‰∏î‰∏éÂØπË±°Âå∫ÂüüÂØπÈΩê‰∏çËâØ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂàÜËß£Ê≥®ÊÑèÂäõËûçÂêà(DecAF)ÔºåÈÄöËøá‰∏§ÁßçÊú∫Âà∂Êù•‰ºòÂåñËøô‰∫õÂõæÔºö(1)ÂØπÊØîÂØπË±°-ËÉåÊôØËûçÂêàÔºõ(2)‰∫íË°•ÁöÑËßÜÈ¢ë-Â∏ßËûçÂêà„ÄÇËØ•ÊñπÊ≥ïÊäëÂà∂‰∫Ü‰∏çÁõ∏ÂÖ≥ÁöÑÊøÄÊ¥ªÔºåÂπ∂Â¢ûÂº∫‰∫Ü‰ª•ÂØπË±°‰∏∫‰∏≠ÂøÉÁöÑÁ∫øÁ¥¢Ôºå‰ªéËÄåËÉΩÂ§üÂ∞ÜÊ≥®ÊÑèÂäõÂõæÁõ¥Êé•ËΩ¨Êç¢‰∏∫Á≤óÁï•ÁöÑÂàÜÂâ≤Êé©Á†Å„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÊ≥®ÊÑèÂäõÂºïÂØºÁöÑSAM2ÊèêÁ§∫Ôºå‰ª•Ëé∑ÂæóÁ≤æÁªÜÁöÑÊé©Á†Å„ÄÇ‰∏éÁé∞ÊúâÂ∞ÜMLLM‰∏éSAMËÅîÂêàËÆ≠ÁªÉÁöÑÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂÆåÂÖ®Êó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÂú®referringÂíåreasoning VOSÂü∫ÂáÜÊµãËØï‰∏≠ÔºåDecAF‰ºò‰∫éÂÖçËÆ≠ÁªÉÊñπÊ≥ïÔºåÂπ∂ÂÆûÁé∞‰∫Ü‰∏éÂü∫‰∫éËÆ≠ÁªÉÁöÑÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ‰ª£Á†ÅÂ∞ÜÂú®https://github.com/HYUNJS/DecAFÊèê‰æõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜÈ¢ëÊé®ÁêÜÂàÜÂâ≤ÈóÆÈ¢òÔºåÂç≥Ê†πÊçÆÁªôÂÆöÁöÑÊñáÊú¨ÊèèËø∞ÔºåÂú®ËßÜÈ¢ë‰∏≠ÂàÜÂâ≤Âá∫ÂØπÂ∫îÁöÑÁõÆÊ†áÂØπË±°„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÊù•ÂæÆË∞ÉMLLMÊàñËÅîÂêàËÆ≠ÁªÉÂàÜÂâ≤Ê®°ÂûãÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏îÊ≥õÂåñËÉΩÂäõÂèØËÉΩÂèóÈôê„ÄÇÁõ¥Êé•‰ΩøÁî®MLLMÁöÑÊ≥®ÊÑèÂäõÂõæËøõË°åÂàÜÂâ≤ÊïàÊûú‰∏ç‰Ω≥ÔºåÂõ†‰∏∫ÂéüÂßãÊ≥®ÊÑèÂäõÂõæÂåÖÂê´Â§ßÈáèÂô™Â£∞Ôºå‰∏éÁõÆÊ†áÂØπË±°ÁöÑÂÉèÁ¥†Á∫ßÂØπÈΩêÂ∫¶‰∏çÈ´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂàÜËß£ÂíåËûçÂêàÊ≥®ÊÑèÂäõÂõæÊù•ÊèêÁ∫ØËßÜËßâÁ∫øÁ¥¢Ôºå‰ªéËÄåÂÆûÁé∞ÂÖçËÆ≠ÁªÉÁöÑËßÜÈ¢ëÊé®ÁêÜÂàÜÂâ≤„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈ¶ñÂÖàÂà©Áî®MLLMÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÁîüÊàêÂàùÂßãÁöÑÊ≥®ÊÑèÂäõÂõæÔºåÁÑ∂ÂêéÈÄöËøáÂØπÊØîÂØπË±°-ËÉåÊôØËûçÂêàÊù•ÊäëÂà∂ËÉåÊôØÂô™Â£∞ÔºåÂπ∂ÈÄöËøá‰∫íË°•ÁöÑËßÜÈ¢ë-Â∏ßËûçÂêàÊù•Â¢ûÂº∫ÁõÆÊ†áÂØπË±°ÁöÑÁâπÂæÅË°®Ëææ„ÄÇÊúÄÂêéÔºåÂà©Áî®ÊèêÁ∫ØÂêéÁöÑÊ≥®ÊÑèÂäõÂõæÂºïÂØºSAMËøõË°åÁ≤æÁªÜÂàÜÂâ≤„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDecAFÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) **Ê≥®ÊÑèÂäõÂõæÊèêÂèñ**ÔºöÂ∞ÜËßÜÈ¢ëÊé®ÁêÜÂàÜÂâ≤‰ªªÂä°ËΩ¨Âåñ‰∏∫ËßÜÈ¢ëÈóÆÁ≠î‰ªªÂä°ÔºåÂà©Áî®MLLMÊèêÂèñ‰∏éÊñáÊú¨Êü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊ≥®ÊÑèÂäõÂõæ„ÄÇ2) **ÂØπÊØîÂØπË±°-ËÉåÊôØËûçÂêà**ÔºöÈÄöËøáËÆ°ÁÆóÂØπË±°ÂíåËÉåÊôØÂå∫ÂüüÁöÑÊ≥®ÊÑèÂäõÂ∑ÆÂºÇÔºåÊäëÂà∂ËÉåÊôØÂô™Â£∞ÔºåÁ™ÅÂá∫ÂØπË±°Âå∫ÂüüÁöÑÊøÄÊ¥ª„ÄÇ3) **‰∫íË°•ËßÜÈ¢ë-Â∏ßËûçÂêà**ÔºöËûçÂêàÊù•Ëá™‰∏çÂêåËßÜÈ¢ëÂ∏ßÁöÑÊ≥®ÊÑèÂäõÂõæÔºå‰ª•Â¢ûÂº∫Êó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÁõÆÊ†áÂØπË±°ÁöÑÂÆåÊï¥ÊÄß„ÄÇ4) **Ê≥®ÊÑèÂäõÂºïÂØºÁöÑSAM2ÊèêÁ§∫**ÔºöÂà©Áî®ÊèêÁ∫ØÂêéÁöÑÊ≥®ÊÑèÂäõÂõæ‰Ωú‰∏∫SAMÁöÑÊèêÁ§∫ÔºåÁîüÊàêÁ≤æÁªÜÁöÑÂàÜÂâ≤Êé©Á†Å„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDecAFÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÂÖçËÆ≠ÁªÉÁöÑÂàÜÂâ≤ÊñπÊ≥ïÔºå‰ª•ÂèäÂàÜËß£Ê≥®ÊÑèÂäõËûçÂêàÁ≠ñÁï•„ÄÇ‰∏éÈúÄË¶ÅÂ§ßÈáèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊñπÊ≥ï‰∏çÂêåÔºåDecAFÂèØ‰ª•Áõ¥Êé•Âà©Áî®È¢ÑËÆ≠ÁªÉÁöÑMLLMÂíåSAMÔºåÊó†ÈúÄ‰ªª‰ΩïÂæÆË∞ÉÊàñËÅîÂêàËÆ≠ÁªÉ„ÄÇÈÄöËøáÂØπÊØîÂØπË±°-ËÉåÊôØËûçÂêàÂíå‰∫íË°•ËßÜÈ¢ë-Â∏ßËûçÂêàÔºåÊúâÊïàÂú∞ÊèêÁ∫Ø‰∫ÜÊ≥®ÊÑèÂäõÂõæÔºå‰ΩøÂÖ∂Êõ¥ÂáÜÁ°ÆÂú∞ÂèçÊò†‰∫ÜÁõÆÊ†áÂØπË±°ÁöÑ‰ΩçÁΩÆÂíåÂΩ¢Áä∂„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂØπÊØîÂØπË±°-ËÉåÊôØËûçÂêà‰∏≠ÔºåËÆ∫ÊñáÈááÁî®‰∫Ü‰∏ÄÁßçÂØπÊØîÊçüÂ§±ÂáΩÊï∞ÔºåÈºìÂä±ÂØπË±°Âå∫ÂüüÁöÑÊ≥®ÊÑèÂäõÂÄºÈ´ò‰∫éËÉåÊôØÂå∫Âüü„ÄÇÂú®‰∫íË°•ËßÜÈ¢ë-Â∏ßËûçÂêà‰∏≠ÔºåËÆ∫Êñá‰ΩøÁî®‰∫Ü‰∏ÄÁßçÂä†ÊùÉÂπ≥ÂùáÁ≠ñÁï•ÔºåÊ†πÊçÆÂ∏ß‰∏éÊü•ËØ¢ÁöÑÁõ∏ÂÖ≥ÊÄßÊù•Ë∞ÉÊï¥‰∏çÂêåÂ∏ßÁöÑÊ≥®ÊÑèÂäõÂõæÁöÑÊùÉÈáç„ÄÇÂú®Ê≥®ÊÑèÂäõÂºïÂØºÁöÑSAM2ÊèêÁ§∫‰∏≠ÔºåËÆ∫ÊñáÂ∞ÜÊèêÁ∫ØÂêéÁöÑÊ≥®ÊÑèÂäõÂõæ‰Ωú‰∏∫SAMÁöÑbox promptÔºåÂºïÂØºSAMÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÂàÜÂâ≤Êé©Á†Å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DecAFÂú®Referring Video Object Segmentation (RVOS)ÂíåReasoning Video Object Segmentation (Reasoning-VOS)Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÊàêÊûú„ÄÇÂú®ÂÖçËÆ≠ÁªÉÊñπÊ≥ï‰∏≠ÔºåDecAFÂ§ßÂπÖË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂‰∏îÊÄßËÉΩ‰∏éÈúÄË¶ÅÂ§ßÈáèËÆ≠ÁªÉÁöÑSOTAÊñπÊ≥ïÁõ∏ÂΩìÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ‰æãÂ¶ÇÔºåÂú®Êüê‰∏™RVOSÊï∞ÊçÆÈõÜ‰∏äÔºåDecAFÁöÑJ&FÊåáÊ†áËææÂà∞‰∫ÜXX%ÔºåÁõ∏ÊØî‰∫é‰πãÂâçÁöÑÂÖçËÆ≠ÁªÉÊñπÊ≥ïÊèêÂçá‰∫ÜYY%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËßÜÈ¢ëÁõëÊéß„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™Á≠âÈ¢ÜÂüüÔºåÂÆûÁé∞ÂØπËßÜÈ¢ë‰∏≠ÁâπÂÆöÁõÆÊ†áÁöÑËá™Âä®ÂàÜÂâ≤ÂíåË∑üË∏™„ÄÇ‰æãÂ¶ÇÔºåÂú®ËßÜÈ¢ëÁõëÊéß‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÊñáÊú¨ÊèèËø∞Ëá™Âä®ÂàÜÂâ≤Âá∫Â´åÁñë‰∫∫ÂëòÊàñËΩ¶ËæÜÔºõÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÊñáÊú¨Êåá‰ª§ÂàÜÂâ≤Âá∫‰∫§ÈÄöÊ†áÂøóÊàñË°å‰∫∫Ôºå‰ªéËÄåÊèêÈ´òÁ≥ªÁªüÁöÑÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Multimodal large language models (MLLMs) demonstrate strong video understanding by attending to visual tokens relevant to textual queries. To directly adapt this for localization in a training-free manner, we cast video reasoning segmentation as a video QA task and extract attention maps via rollout mechanism. However, raw attention maps are noisy and poorly aligned with object regions. We propose Decomposed Attention Fusion (DecAF), which refines these maps through two mechanisms: (1) contrastive object-background fusion and (2) complementary video-frame fusion. This method suppresses irrelevant activations and enhances object-focused cues, enabling direct conversion of attention maps into coarse segmentation masks. In addition, we introduce attention-guided SAM2 prompting for obtaining fine-grained masks. Unlike existing methods that jointly train MLLMs with SAM, our method operates entirely without retraining. DecAF outperforms training-free methods and achieves performance comparable to training-based methods on both referring and reasoning VOS benchmarks. The code will be available at https://github.com/HYUNJS/DecAF.

