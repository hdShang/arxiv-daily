---
layout: default
title: Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation
---

# Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19592" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19592v1</a>
  <a href="https://arxiv.org/pdf/2510.19592.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19592v1" onclick="toggleFavorite(this, '2510.19592v1', 'Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22

**å¤‡æ³¨**: Project page: https://www.jshyun.me/projects/decaf

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HYUNJS/DecAF)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDecomposed Attention Fusion (DecAF)ï¼Œç”¨äºMLLMçš„å…è®­ç»ƒè§†é¢‘æ¨ç†åˆ†å‰²**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘æ¨ç†åˆ†å‰²` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `æ³¨æ„åŠ›æœºåˆ¶` `å…è®­ç»ƒå­¦ä¹ ` `è§†è§‰å®šä½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•æå–çš„åŸå§‹æ³¨æ„åŠ›å›¾å™ªå£°å¤§ï¼Œä¸ç›®æ ‡åŒºåŸŸå¯¹é½æ•ˆæœå·®ï¼Œé™åˆ¶äº†MLLMåœ¨è§†é¢‘æ¨ç†åˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. DecAFé€šè¿‡å¯¹æ¯”å¯¹è±¡-èƒŒæ™¯èåˆå’Œäº’è¡¥è§†é¢‘-å¸§èåˆï¼Œæçº¯æ³¨æ„åŠ›å›¾ï¼Œå¢å¼ºç›®æ ‡åŒºåŸŸçš„å…³æ³¨ï¼Œæ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆåˆ†å‰²æ©ç ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDecAFåœ¨è§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸Šè¶…è¶Šäº†å…¶ä»–å…è®­ç»ƒæ–¹æ³•ï¼Œæ€§èƒ½å¯ä¸éœ€è¦è®­ç»ƒçš„æ–¹æ³•ç›¸åª²ç¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)é€šè¿‡å…³æ³¨ä¸æ–‡æœ¬æŸ¥è¯¢ç›¸å…³çš„è§†è§‰tokensï¼Œå±•ç°å‡ºå¼ºå¤§çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†ä»¥å…è®­ç»ƒçš„æ–¹å¼ç›´æ¥å°†å…¶åº”ç”¨äºå®šä½ä»»åŠ¡ï¼Œæœ¬æ–‡å°†è§†é¢‘æ¨ç†åˆ†å‰²è½¬åŒ–ä¸ºè§†é¢‘é—®ç­”ä»»åŠ¡ï¼Œå¹¶é€šè¿‡rolloutæœºåˆ¶æå–æ³¨æ„åŠ›å›¾ã€‚ç„¶è€Œï¼ŒåŸå§‹æ³¨æ„åŠ›å›¾å­˜åœ¨å™ªå£°ï¼Œä¸”ä¸å¯¹è±¡åŒºåŸŸå¯¹é½ä¸è‰¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†è§£æ³¨æ„åŠ›èåˆ(DecAF)ï¼Œé€šè¿‡ä¸¤ç§æœºåˆ¶æ¥ä¼˜åŒ–è¿™äº›å›¾ï¼š(1)å¯¹æ¯”å¯¹è±¡-èƒŒæ™¯èåˆï¼›(2)äº’è¡¥çš„è§†é¢‘-å¸§èåˆã€‚è¯¥æ–¹æ³•æŠ‘åˆ¶äº†ä¸ç›¸å…³çš„æ¿€æ´»ï¼Œå¹¶å¢å¼ºäº†ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„çº¿ç´¢ï¼Œä»è€Œèƒ½å¤Ÿå°†æ³¨æ„åŠ›å›¾ç›´æ¥è½¬æ¢ä¸ºç²—ç•¥çš„åˆ†å‰²æ©ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ³¨æ„åŠ›å¼•å¯¼çš„SAM2æç¤ºï¼Œä»¥è·å¾—ç²¾ç»†çš„æ©ç ã€‚ä¸ç°æœ‰å°†MLLMä¸SAMè”åˆè®­ç»ƒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®Œå…¨æ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨referringå’Œreasoning VOSåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDecAFä¼˜äºå…è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å®ç°äº†ä¸åŸºäºè®­ç»ƒçš„æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨https://github.com/HYUNJS/DecAFæä¾›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†é¢‘æ¨ç†åˆ†å‰²é—®é¢˜ï¼Œå³æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°ï¼Œåœ¨è§†é¢‘ä¸­åˆ†å‰²å‡ºå¯¹åº”çš„ç›®æ ‡å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®æ¥å¾®è°ƒMLLMæˆ–è”åˆè®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”æ³›åŒ–èƒ½åŠ›å¯èƒ½å—é™ã€‚ç›´æ¥ä½¿ç”¨MLLMçš„æ³¨æ„åŠ›å›¾è¿›è¡Œåˆ†å‰²æ•ˆæœä¸ä½³ï¼Œå› ä¸ºåŸå§‹æ³¨æ„åŠ›å›¾åŒ…å«å¤§é‡å™ªå£°ï¼Œä¸ç›®æ ‡å¯¹è±¡çš„åƒç´ çº§å¯¹é½åº¦ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡åˆ†è§£å’Œèåˆæ³¨æ„åŠ›å›¾æ¥æçº¯è§†è§‰çº¿ç´¢ï¼Œä»è€Œå®ç°å…è®­ç»ƒçš„è§†é¢‘æ¨ç†åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆåˆ©ç”¨MLLMçš„æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆåˆå§‹çš„æ³¨æ„åŠ›å›¾ï¼Œç„¶åé€šè¿‡å¯¹æ¯”å¯¹è±¡-èƒŒæ™¯èåˆæ¥æŠ‘åˆ¶èƒŒæ™¯å™ªå£°ï¼Œå¹¶é€šè¿‡äº’è¡¥çš„è§†é¢‘-å¸§èåˆæ¥å¢å¼ºç›®æ ‡å¯¹è±¡çš„ç‰¹å¾è¡¨è¾¾ã€‚æœ€åï¼Œåˆ©ç”¨æçº¯åçš„æ³¨æ„åŠ›å›¾å¼•å¯¼SAMè¿›è¡Œç²¾ç»†åˆ†å‰²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDecAFçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) **æ³¨æ„åŠ›å›¾æå–**ï¼šå°†è§†é¢‘æ¨ç†åˆ†å‰²ä»»åŠ¡è½¬åŒ–ä¸ºè§†é¢‘é—®ç­”ä»»åŠ¡ï¼Œåˆ©ç”¨MLLMæå–ä¸æ–‡æœ¬æŸ¥è¯¢ç›¸å…³çš„æ³¨æ„åŠ›å›¾ã€‚2) **å¯¹æ¯”å¯¹è±¡-èƒŒæ™¯èåˆ**ï¼šé€šè¿‡è®¡ç®—å¯¹è±¡å’ŒèƒŒæ™¯åŒºåŸŸçš„æ³¨æ„åŠ›å·®å¼‚ï¼ŒæŠ‘åˆ¶èƒŒæ™¯å™ªå£°ï¼Œçªå‡ºå¯¹è±¡åŒºåŸŸçš„æ¿€æ´»ã€‚3) **äº’è¡¥è§†é¢‘-å¸§èåˆ**ï¼šèåˆæ¥è‡ªä¸åŒè§†é¢‘å¸§çš„æ³¨æ„åŠ›å›¾ï¼Œä»¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§å’Œç›®æ ‡å¯¹è±¡çš„å®Œæ•´æ€§ã€‚4) **æ³¨æ„åŠ›å¼•å¯¼çš„SAM2æç¤º**ï¼šåˆ©ç”¨æçº¯åçš„æ³¨æ„åŠ›å›¾ä½œä¸ºSAMçš„æç¤ºï¼Œç”Ÿæˆç²¾ç»†çš„åˆ†å‰²æ©ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šDecAFçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…è®­ç»ƒçš„åˆ†å‰²æ–¹æ³•ï¼Œä»¥åŠåˆ†è§£æ³¨æ„åŠ›èåˆç­–ç•¥ã€‚ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„æ–¹æ³•ä¸åŒï¼ŒDecAFå¯ä»¥ç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒçš„MLLMå’ŒSAMï¼Œæ— éœ€ä»»ä½•å¾®è°ƒæˆ–è”åˆè®­ç»ƒã€‚é€šè¿‡å¯¹æ¯”å¯¹è±¡-èƒŒæ™¯èåˆå’Œäº’è¡¥è§†é¢‘-å¸§èåˆï¼Œæœ‰æ•ˆåœ°æçº¯äº†æ³¨æ„åŠ›å›¾ï¼Œä½¿å…¶æ›´å‡†ç¡®åœ°åæ˜ äº†ç›®æ ‡å¯¹è±¡çš„ä½ç½®å’Œå½¢çŠ¶ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¯¹æ¯”å¯¹è±¡-èƒŒæ™¯èåˆä¸­ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé¼“åŠ±å¯¹è±¡åŒºåŸŸçš„æ³¨æ„åŠ›å€¼é«˜äºèƒŒæ™¯åŒºåŸŸã€‚åœ¨äº’è¡¥è§†é¢‘-å¸§èåˆä¸­ï¼Œè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åŠ æƒå¹³å‡ç­–ç•¥ï¼Œæ ¹æ®å¸§ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§æ¥è°ƒæ•´ä¸åŒå¸§çš„æ³¨æ„åŠ›å›¾çš„æƒé‡ã€‚åœ¨æ³¨æ„åŠ›å¼•å¯¼çš„SAM2æç¤ºä¸­ï¼Œè®ºæ–‡å°†æçº¯åçš„æ³¨æ„åŠ›å›¾ä½œä¸ºSAMçš„box promptï¼Œå¼•å¯¼SAMç”Ÿæˆæ›´å‡†ç¡®çš„åˆ†å‰²æ©ç ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DecAFåœ¨Referring Video Object Segmentation (RVOS)å’ŒReasoning Video Object Segmentation (Reasoning-VOS)åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚åœ¨å…è®­ç»ƒæ–¹æ³•ä¸­ï¼ŒDecAFå¤§å¹…è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”æ€§èƒ½ä¸éœ€è¦å¤§é‡è®­ç»ƒçš„SOTAæ–¹æ³•ç›¸å½“ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªRVOSæ•°æ®é›†ä¸Šï¼ŒDecAFçš„J&FæŒ‡æ ‡è¾¾åˆ°äº†XX%ï¼Œç›¸æ¯”äºä¹‹å‰çš„å…è®­ç»ƒæ–¹æ³•æå‡äº†YY%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè§†é¢‘ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªç­‰é¢†åŸŸï¼Œå®ç°å¯¹è§†é¢‘ä¸­ç‰¹å®šç›®æ ‡çš„è‡ªåŠ¨åˆ†å‰²å’Œè·Ÿè¸ªã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘ç›‘æ§ä¸­ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æè¿°è‡ªåŠ¨åˆ†å‰²å‡ºå«Œç–‘äººå‘˜æˆ–è½¦è¾†ï¼›åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æŒ‡ä»¤åˆ†å‰²å‡ºäº¤é€šæ ‡å¿—æˆ–è¡Œäººï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) demonstrate strong video understanding by attending to visual tokens relevant to textual queries. To directly adapt this for localization in a training-free manner, we cast video reasoning segmentation as a video QA task and extract attention maps via rollout mechanism. However, raw attention maps are noisy and poorly aligned with object regions. We propose Decomposed Attention Fusion (DecAF), which refines these maps through two mechanisms: (1) contrastive object-background fusion and (2) complementary video-frame fusion. This method suppresses irrelevant activations and enhances object-focused cues, enabling direct conversion of attention maps into coarse segmentation masks. In addition, we introduce attention-guided SAM2 prompting for obtaining fine-grained masks. Unlike existing methods that jointly train MLLMs with SAM, our method operates entirely without retraining. DecAF outperforms training-free methods and achieves performance comparable to training-based methods on both referring and reasoning VOS benchmarks. The code will be available at https://github.com/HYUNJS/DecAF.

