---
layout: default
title: Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes
---

# Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.19400" target="_blank" class="toolbar-btn">arXiv: 2510.19400v1</a>
    <a href="https://arxiv.org/pdf/2510.19400.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19400v1" 
            onclick="toggleFavorite(this, '2510.19400v1', 'Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-22

**Â§áÊ≥®**: The project and benchmark are publicly available at https://github.com/microsoft/MV-RoboBench

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MV-RoboBenchÂü∫ÂáÜÔºåËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Âú∫ÊôØ‰∏≠ÁöÑÂ§öËßÜËßíÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Â§öËßÜËßíÂ≠¶‰π†` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Á©∫Èó¥Êé®ÁêÜ` `Âü∫ÂáÜÊï∞ÊçÆÈõÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Â∫îÁî®‰∏≠Áº∫‰πèÂØπÂ§öËßÜËßí‰ø°ÊÅØÁöÑÊúâÊïàÂà©Áî®ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈÅÆÊå°ÂíåÊ∑±Â∫¶Ê®°Á≥äÁ≠âÈóÆÈ¢ò‰∏ä„ÄÇ
2. ÊèêÂá∫MV-RoboBenchÂü∫ÂáÜÔºåÂåÖÂê´1.7k‰∏™QAÈ°πÁõÆÔºåÊ∂µÁõñÁ©∫Èó¥ÁêÜËß£ÂíåÊú∫Âô®‰∫∫ÊâßË°å‰∏§Â§ßÁ±ªÔºåÂÖ±ÂÖ´‰∏™Â≠ê‰ªªÂä°ÔºåÁî®‰∫éËØÑ‰º∞Â§öËßÜËßíÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁé∞ÊúâVLMsÂú®Â§öËßÜËßíÊú∫Âô®‰∫∫ÊÑüÁü•ÊñπÈù¢‰ªçÊúâÂæàÂ§ßÊèêÂçáÁ©∫Èó¥Ôºå‰∏îÂçïËßÜËßíÂü∫ÂáÜ‰∏äÁöÑ‰ºòÂºÇÊÄßËÉΩ‰∏çËÉΩ‰øùËØÅÂú®Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁöÑÊàêÂäü„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLMs)ÂØπ‰∫éÂÖ∑Ë∫´Êô∫ËÉΩËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÊÑüÁü•„ÄÅÊé®ÁêÜÂíåË°åÂä®„ÄÇÂÆÉ‰ª¨‰πüÊòØÊúÄËøëÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÁöÑÂü∫Á°Ä„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∞VLMsÁöÑËØÑ‰º∞ÈÉΩÈõÜ‰∏≠Âú®ÂçïËßÜËßíËÆæÁΩÆ‰∏äÔºåÂØπÂÖ∂Êï¥ÂêàÂ§öËßÜËßí‰ø°ÊÅØÁöÑËÉΩÂäõÊé¢Á¥¢‰∏çË∂≥„ÄÇÂêåÊó∂ÔºåÂ§öÊëÑÂÉèÂ§¥ËÆæÁΩÆÂú®Êú∫Âô®‰∫∫Âπ≥Âè∞‰∏≠Ë∂äÊù•Ë∂äÊôÆÈÅçÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Êèê‰æõ‰∫Ü‰∫íË°•ÁöÑËßÜËßíÊù•ÂáèËΩªÈÅÆÊå°ÂíåÊ∑±Â∫¶Ê®°Á≥ä„ÄÇVLMsÊòØÂê¶ËÉΩÊúâÊïàÂú∞Âà©Áî®ËøôÁßçÂ§öËßÜËßíËæìÂÖ•ËøõË°åÊú∫Âô®‰∫∫Êé®ÁêÜ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÊÇ¨ËÄåÊú™ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÂº•ÂêàËøô‰∏ÄÂ∑ÆË∑ùÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜMV-RoboBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞VLMsÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠Â§öËßÜËßíÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇMV-RoboBenchÂåÖÂê´1.7k‰∏™ÊâãÂä®Á≠ñÂàíÁöÑQAÈ°πÁõÆÔºåË∑®Ë∂äÂÖ´‰∏™Â≠ê‰ªªÂä°ÔºåÂàÜ‰∏∫‰∏§‰∏™‰∏ªË¶ÅÁ±ªÂà´ÔºöÁ©∫Èó¥ÁêÜËß£ÂíåÊú∫Âô®‰∫∫ÊâßË°å„ÄÇÊàë‰ª¨ËØÑ‰º∞‰∫ÜÂêÑÁßçÁé∞ÊúâÁöÑVLMsÔºåÂåÖÊã¨ÂºÄÊ∫êÂíåÈó≠Ê∫êÊ®°ÂûãÔºå‰ª•ÂèäÁªìÂêà‰∫ÜCoTÂêØÂèëÊäÄÊúØÁöÑÂ¢ûÂº∫ÁâàÊú¨„ÄÇÁªìÊûúË°®ÊòéÔºåÊúÄÂÖàËøõÁöÑÊ®°Âûã‰ªçÁÑ∂Ëøú‰Ωé‰∫é‰∫∫Á±ªÁöÑË°®Áé∞ÔºåÁ™ÅÊòæ‰∫ÜVLMsÂú®Â§öËßÜËßíÊú∫Âô®‰∫∫ÊÑüÁü•ÊñπÈù¢Èù¢‰∏¥ÁöÑÂ∑®Â§ßÊåëÊàò„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÂàÜÊûêÊè≠Á§∫‰∫Ü‰∏§‰∏™ÂÖ≥ÈîÆÂèëÁé∞Ôºö(i)Á©∫Èó¥Êô∫ËÉΩÂíåÊú∫Âô®‰∫∫‰ªªÂä°ÊâßË°åÂú®Â§öËßÜËßíÊú∫Âô®‰∫∫Âú∫ÊôØ‰∏≠ÂëàÊ≠£Áõ∏ÂÖ≥Ôºõ(ii)Âú®Áé∞ÊúâÈÄöÁî®ÂçïËßÜËßíÁ©∫Èó¥ÁêÜËß£Âü∫ÂáÜ‰∏äÁöÑÂá∫Ëâ≤Ë°®Áé∞Âπ∂‰∏çËÉΩÂèØÈù†Âú∞ËΩ¨Âåñ‰∏∫Âú®Êàë‰ª¨ÁöÑÂü∫ÂáÜËØÑ‰º∞ÁöÑÊú∫Âô®‰∫∫Á©∫Èó¥‰ªªÂä°‰∏≠ÁöÑÊàêÂäü„ÄÇÊàë‰ª¨ÂèëÂ∏ÉMV-RoboBench‰Ωú‰∏∫‰∏Ä‰∏™ÂºÄÊîæËµÑÊ∫êÔºå‰ª•‰øÉËøõÁ©∫Èó¥ÂÆö‰ΩçÁöÑVLMsÂíåVLAsÁöÑËøõÂ±ïÔºå‰∏ç‰ªÖÊèê‰æõÊï∞ÊçÆÔºåËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™Áî®‰∫éÂ§öËßÜËßíÂÖ∑Ë∫´Êé®ÁêÜÁöÑÊ†áÂáÜÂåñËØÑ‰º∞ÂçèËÆÆ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Âú∫ÊôØ‰∏ãÔºåÂ¶Ç‰ΩïÊúâÊïàÂà©Áî®Â§öËßÜËßí‰ø°ÊÅØËøõË°åÁ©∫Èó¥Êé®ÁêÜÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠‰∫éÂçïËßÜËßíËØÑ‰º∞ÔºåÂøΩÁï•‰∫ÜÂ§öËßÜËßí‰ø°ÊÅØËûçÂêàÁöÑÈáçË¶ÅÊÄßÔºåÂØºËá¥Ê®°ÂûãÂú®ÂÆûÈôÖÊú∫Âô®‰∫∫Â∫îÁî®‰∏≠ÊÄßËÉΩÂèóÈôêÔºåÂ∞§ÂÖ∂ÊòØÂú®Â≠òÂú®ÈÅÆÊå°ÂíåÊ∑±Â∫¶Ê≠ß‰πâÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§öËßÜËßíÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜMV-RoboBench„ÄÇÈÄöËøáËÆæËÆ°ÂåÖÂê´Á©∫Èó¥ÁêÜËß£ÂíåÊú∫Âô®‰∫∫ÊâßË°å‰ªªÂä°ÁöÑQAÂØπÔºåÁ≥ªÁªüÊÄßÂú∞ÊµãËØïVLMsÂú®Â§öËßÜËßíÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞Ôºå‰ªéËÄåÊé®Âä®Áõ∏ÂÖ≥Á†îÁ©∂ÁöÑÂèëÂ±ï„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMV-RoboBenchÂü∫ÂáÜÂåÖÂê´‰ª•‰∏ãÂá†‰∏™ÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜÔºö
1.  **Âú∫ÊôØÊûÑÂª∫**ÔºöÊ®°ÊãüÁúüÂÆûÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁéØÂ¢ÉÔºå‰ΩøÁî®Â§öÊëÑÂÉèÂ§¥Ëé∑Âèñ‰∏çÂêåËßÜËßíÁöÑÂõæÂÉè„ÄÇ
2.  **‰ªªÂä°ËÆæËÆ°**ÔºöËÆæËÆ°ÂÖ´‰∏™Â≠ê‰ªªÂä°ÔºåÊ∂µÁõñÁ©∫Èó¥ÁêÜËß£Ôºà‰æãÂ¶ÇÔºåÁâ©‰ΩìÂÆö‰Ωç„ÄÅÂÖ≥Á≥ªÊé®ÁêÜÔºâÂíåÊú∫Âô®‰∫∫ÊâßË°åÔºà‰æãÂ¶ÇÔºåÊäìÂèñ„ÄÅÊîæÁΩÆÔºâ„ÄÇ
3.  **QAÁîüÊàê**ÔºöÊâãÂä®Ê†áÊ≥®È´òË¥®ÈáèÁöÑÈóÆÁ≠îÂØπÔºåÁ°Æ‰øùÈóÆÈ¢òÂÖ∑ÊúâÊåëÊàòÊÄßÔºåÁ≠îÊ°àÈúÄË¶ÅÊ®°ÂûãËøõË°åÂ§öËßÜËßí‰ø°ÊÅØËûçÂêàÂíåÁ©∫Èó¥Êé®ÁêÜ„ÄÇ
4.  **ËØÑ‰º∞ÊåáÊ†á**ÔºöÈááÁî®ÂáÜÁ°ÆÁéáÁ≠âÊåáÊ†áÔºåËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÂ§öËßÜËßíÊú∫Âô®‰∫∫Âú∫ÊôØÁöÑËßÜËßâ-ËØ≠Ë®ÄÊé®ÁêÜÂü∫ÂáÜ„ÄÇ‰∏éÁé∞ÊúâÂçïËßÜËßíÂü∫ÂáÜÁõ∏ÊØîÔºåMV-RoboBenchÊõ¥Ë¥¥ËøëÂÆûÈôÖÂ∫îÁî®ÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËØÑ‰º∞VLMsÂú®Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁöÑÊΩúÂäõ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂàÜÊûê‰∫ÜÁ©∫Èó¥Êô∫ËÉΩÂíåÊú∫Âô®‰∫∫‰ªªÂä°ÊâßË°å‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÔºå‰ª•ÂèäÂçïËßÜËßíÊÄßËÉΩ‰∏éÂ§öËßÜËßíÊÄßËÉΩ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºå‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÅËß£„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMV-RoboBenchÁöÑÊï∞ÊçÆÈõÜÂåÖÂê´1.7k‰∏™ÊâãÂä®Ê†áÊ≥®ÁöÑQAÈ°πÁõÆÔºåÊ∂µÁõñ‰∫ÜÂÖ´‰∏™Â≠ê‰ªªÂä°„ÄÇËøô‰∫õÂ≠ê‰ªªÂä°ÁöÑËÆæËÆ°ËÄÉËôë‰∫ÜÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂÖ∏ÂûãÂú∫ÊôØÂíåÊåëÊàòÔºå‰æãÂ¶ÇÔºåÈúÄË¶ÅÊ®°ÂûãÁêÜËß£Áâ©‰Ωì‰πãÈó¥ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÅÂà§Êñ≠Áâ©‰ΩìÁöÑÂèØÊìç‰ΩúÊÄß„ÄÅ‰ª•ÂèäËßÑÂàíÊú∫Âô®‰∫∫ÁöÑËøêÂä®ËΩ®Ëøπ„ÄÇËÆ∫ÊñáËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê†áÂáÜÂåñÁöÑËØÑ‰º∞ÂçèËÆÆÔºåÊñπ‰æøÁ†îÁ©∂‰∫∫ÂëòËøõË°åÊ®°ÂûãÊØîËæÉÂíåÊÄßËÉΩÂàÜÊûê„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁé∞ÊúâÊúÄÂÖàËøõÁöÑVLMsÂú®MV-RoboBench‰∏äÁöÑË°®Áé∞Ëøú‰Ωé‰∫é‰∫∫Á±ªÊ∞¥Âπ≥ÔºåÁ™ÅÊòæ‰∫ÜÂ§öËßÜËßíÊú∫Âô®‰∫∫ÊÑüÁü•ÁöÑÊåëÊàòÊÄß„ÄÇÁ†îÁ©∂ËøòÂèëÁé∞ÔºåÂú®ÈÄöÁî®ÂçïËßÜËßíÁ©∫Èó¥ÁêÜËß£Âü∫ÂáÜ‰∏äË°®Áé∞ËâØÂ•ΩÁöÑÊ®°ÂûãÔºåÂú®MV-RoboBench‰∏äÁöÑË°®Áé∞Âπ∂‰∏ç‰∏ÄÂÆöÂá∫Ëâ≤ÔºåË°®ÊòéÂ§öËßÜËßíÊú∫Âô®‰∫∫‰ªªÂä°ÈúÄË¶Å‰∏ìÈó®ÁöÑÊ®°ÂûãËÆæËÆ°ÂíåËÆ≠ÁªÉ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊÑüÁü•ÂíåÊìç‰ΩúËÉΩÂäõÔºå‰æãÂ¶ÇÊô∫ËÉΩ‰ªìÂÇ®„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÂØπÂ§öËßÜËßí‰ø°ÊÅØÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÂÆåÊàêÂêÑÁßç‰ªªÂä°Ôºå‰æãÂ¶ÇÁâ©‰ΩìËØÜÂà´„ÄÅÂØºËà™„ÄÅÊìç‰ΩúÁ≠âÔºå‰ªéËÄåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-language models (VLMs) are essential to Embodied AI, enabling robots to perceive, reason, and act in complex environments. They also serve as the foundation for the recent Vision-Language-Action (VLA) models. Yet most evaluations of VLMs focus on single-view settings, leaving their ability to integrate multi-view information underexplored. At the same time, multi-camera setups are increasingly standard in robotic platforms, as they provide complementary perspectives to mitigate occlusion and depth ambiguity. Whether VLMs can effectively leverage such multi-view inputs for robotic reasoning therefore remains an open question. To bridge this gap, we introduce MV-RoboBench, a benchmark specifically designed to evaluate the multi-view spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench consists of 1.7k manually curated QA items across eight subtasks, divided into two primary categories: spatial understanding and robotic execution. We evaluate a diverse set of existing VLMs, including both open-source and closed-source models, along with enhanced versions incorporating CoT-inspired techniques. The results show that state-of-the-art models remain far below human performance, underscoring the substantial challenges VLMs face in multi-view robotic perception. Additionally, our analysis uncovers two key findings: (i) spatial intelligence and robotic task execution are positively correlated in multi-view robotic scenarios; and (ii) strong performance on existing general-purpose single-view spatial understanding benchmarks does not reliably translate to success in the robotic spatial tasks assessed by our benchmark. We release MV-RoboBench as an open resource to foster progress in spatially grounded VLMs and VLAs, providing not only data but also a standardized evaluation protocol for multi-view embodied reasoning.

