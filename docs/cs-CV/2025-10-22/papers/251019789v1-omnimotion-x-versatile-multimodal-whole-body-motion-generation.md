---
layout: default
title: OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation
---

# OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19789" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19789v1</a>
  <a href="https://arxiv.org/pdf/2510.19789.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19789v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.19789v1', 'OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu

**åˆ†ç±»**: cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OmniMotion-Xï¼šå¤šåŠŸèƒ½å¤šæ¨¡æ€å…¨èº«è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°é€¼çœŸå¯æ§çš„äº¤äº’å¼é•¿æ—¶è¿åŠ¨ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆ` `æ‰©æ•£æ¨¡å‹` `Transformer` `å‚è€ƒè¿åŠ¨` `SMPL-X` `è¿åŠ¨é¢„æµ‹` `åŠ¨ä½œæ•æ‰`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€å…¨èº«è¿åŠ¨ç”Ÿæˆä¸­ï¼Œéš¾ä»¥å…¼é¡¾å¤šç§æ¨¡æ€è¾“å…¥å’Œä¿è¯ç”Ÿæˆè¿åŠ¨çš„è¿è´¯æ€§ä¸çœŸå®æ„Ÿã€‚
2. OmniMotion-Xåˆ©ç”¨è‡ªå›å½’æ‰©æ•£Transformerï¼Œå¹¶å¼•å…¥å‚è€ƒè¿åŠ¨ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œå¢å¼ºç”Ÿæˆè¿åŠ¨çš„ä¸€è‡´æ€§å’Œé£æ ¼ã€‚
3. OmniMotion-Xåœ¨å¤šé¡¹å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡å¤šæ¨¡æ€è¿åŠ¨æ•°æ®é›†OmniMoCap-Xã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºOmniMotion-Xï¼Œä¸€ä¸ªå¤šåŠŸèƒ½å¤šæ¨¡æ€å…¨èº«äººä½“è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒä»¥ç»Ÿä¸€çš„åºåˆ—åˆ°åºåˆ—æ–¹å¼åˆ©ç”¨è‡ªå›å½’æ‰©æ•£Transformerã€‚OmniMotion-Xé«˜æ•ˆåœ°æ”¯æŒå„ç§å¤šæ¨¡æ€ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆã€è¯­éŸ³åˆ°æ‰‹åŠ¿ï¼Œä»¥åŠå…¨å±€æ—¶ç©ºæ§åˆ¶åœºæ™¯ï¼ˆä¾‹å¦‚ï¼Œè¿åŠ¨é¢„æµ‹ã€ä¸­é—´å¸§ç”Ÿæˆã€è¿åŠ¨è¡¥å…¨å’Œå…³èŠ‚/è½¨è¿¹å¼•å¯¼çš„åˆæˆï¼‰ï¼Œä»¥åŠè¿™äº›ä»»åŠ¡çš„çµæ´»ç»„åˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å‚è€ƒè¿åŠ¨ä½œä¸ºä¸€ç§æ–°çš„æ¡ä»¶ä¿¡å·ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆå†…å®¹ã€é£æ ¼å’Œæ—¶é—´åŠ¨æ€çš„ä¸€è‡´æ€§ï¼Œè¿™å¯¹äºé€¼çœŸçš„åŠ¨ç”»è‡³å…³é‡è¦ã€‚ä¸ºäº†å¤„ç†å¤šæ¨¡æ€å†²çªï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¸è¿›çš„ç”±å¼±åˆ°å¼ºçš„æ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥ã€‚ä¸ºäº†å®ç°é«˜è´¨é‡çš„å¤šæ¨¡æ€è®­ç»ƒï¼Œæˆ‘ä»¬æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç»Ÿä¸€å¤šæ¨¡æ€è¿åŠ¨æ•°æ®é›†OmniMoCap-Xï¼Œæ•´åˆäº†æ¥è‡ª10ä¸ªä¸åŒä»»åŠ¡çš„28ä¸ªå…¬å¼€MoCapèµ„æºï¼Œå¹¶ä»¥30 fpsæ ‡å‡†åŒ–ä¸ºSMPL-Xæ ¼å¼ã€‚ä¸ºäº†ç¡®ä¿è¯¦ç»†å’Œä¸€è‡´çš„æ³¨é‡Šï¼Œæˆ‘ä»¬å°†åºåˆ—æ¸²æŸ“æˆè§†é¢‘ï¼Œå¹¶ä½¿ç”¨GPT-4oè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–å’Œåˆ†å±‚å­—å¹•ï¼Œæ•æ‰ä½çº§åŠ¨ä½œå’Œé«˜çº§è¯­ä¹‰ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¯å®ï¼ŒOmniMotion-Xæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†é€¼çœŸã€è¿è´¯å’Œå¯æ§çš„é•¿æ—¶é—´è¿åŠ¨çš„äº¤äº’å¼ç”Ÿæˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€å…¨èº«è¿åŠ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œéš¾ä»¥åŒæ—¶å¤„ç†å¤šç§æ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶ä¸”ç”Ÿæˆçš„è¿åŠ¨åœ¨è¿è´¯æ€§ã€é£æ ¼ä¸€è‡´æ€§å’Œæ—¶é—´åŠ¨æ€æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ç”Ÿæˆçš„åŠ¨ç”»ä¸å¤ŸçœŸå®ã€‚æ­¤å¤–ï¼Œå¤šæ¨¡æ€è¾“å…¥ä¹‹é—´å¯èƒ½å­˜åœ¨å†²çªï¼Œè¿›ä¸€æ­¥å¢åŠ äº†ç”Ÿæˆé«˜è´¨é‡è¿åŠ¨çš„éš¾åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOmniMotion-Xçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è‡ªå›å½’æ‰©æ•£Transformerï¼Œå¹¶å¼•å…¥å‚è€ƒè¿åŠ¨ä½œä¸ºä¸€ç§æ–°çš„æ¡ä»¶ä¿¡å·ã€‚å‚è€ƒè¿åŠ¨èƒ½å¤Ÿæä¾›å…³äºè¿åŠ¨é£æ ¼ã€æ—¶é—´åŠ¨æ€å’Œæ•´ä½“ç»“æ„çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå¼•å¯¼ç”Ÿæˆå™¨ç”Ÿæˆæ›´è¿è´¯ã€æ›´çœŸå®çš„è¿åŠ¨ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ¸è¿›çš„ç”±å¼±åˆ°å¼ºçš„æ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥ï¼Œä»¥è§£å†³å¤šæ¨¡æ€è¾“å…¥ä¹‹é—´çš„å†²çªã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOmniMotion-Xé‡‡ç”¨åºåˆ—åˆ°åºåˆ—çš„æ¡†æ¶ï¼Œä½¿ç”¨è‡ªå›å½’æ‰©æ•£Transformerä½œä¸ºç”Ÿæˆå™¨ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œå°†å„ç§æ¨¡æ€çš„è¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œæ–‡æœ¬ã€éŸ³ä¹ã€è¯­éŸ³ã€å‚è€ƒè¿åŠ¨ï¼‰ç¼–ç æˆç»Ÿä¸€çš„ç‰¹å¾è¡¨ç¤ºã€‚ç„¶åï¼Œå°†è¿™äº›ç‰¹å¾è¡¨ç¤ºè¾“å…¥åˆ°è‡ªå›å½’æ‰©æ•£Transformerä¸­ï¼Œé€æ­¥ç”Ÿæˆäººä½“è¿åŠ¨åºåˆ—ã€‚æœ€åï¼Œé€šè¿‡åå¤„ç†æ­¥éª¤ï¼Œå°†ç”Ÿæˆçš„è¿åŠ¨åºåˆ—è½¬æ¢ä¸ºSMPL-Xæ ¼å¼ï¼Œä»¥ä¾¿è¿›è¡Œå¯è§†åŒ–å’Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šOmniMotion-Xçš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºï¼š1) å¼•å…¥å‚è€ƒè¿åŠ¨ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè¿åŠ¨çš„è¿è´¯æ€§å’ŒçœŸå®æ„Ÿï¼›2) æå‡ºæ¸è¿›çš„ç”±å¼±åˆ°å¼ºçš„æ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€è¾“å…¥ä¹‹é—´çš„å†²çªï¼›3) æ„å»ºäº†å¤§è§„æ¨¡å¤šæ¨¡æ€è¿åŠ¨æ•°æ®é›†OmniMoCap-Xï¼Œä¸ºå¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆçš„ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ•°æ®èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šOmniMotion-Xçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨SMPL-Xæ ¼å¼ä½œä¸ºäººä½“è¿åŠ¨çš„ç»Ÿä¸€è¡¨ç¤ºï¼›2) é‡‡ç”¨è‡ªå›å½’æ‰©æ•£Transformerä½œä¸ºç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿé€æ­¥ç”Ÿæˆé«˜è´¨é‡çš„è¿åŠ¨åºåˆ—ï¼›3) è®¾è®¡äº†ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œä»¥é¼“åŠ±ç”Ÿæˆè¿åŠ¨ä¸å‚è€ƒè¿åŠ¨åœ¨é£æ ¼å’Œæ—¶é—´åŠ¨æ€ä¸Šä¿æŒä¸€è‡´ï¼›4) ä½¿ç”¨GPT-4oè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–å’Œåˆ†å±‚å­—å¹•ï¼Œä»¥ç¡®ä¿æ•°æ®é›†å…·æœ‰è¯¦ç»†å’Œä¸€è‡´çš„æ³¨é‡Šã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OmniMotion-Xåœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æ–‡æœ¬åˆ°è¿åŠ¨ä»»åŠ¡ä¸­ï¼ŒOmniMotion-Xç”Ÿæˆçš„è¿åŠ¨åœ¨è¿è´¯æ€§å’ŒçœŸå®æ„Ÿæ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥å‚è€ƒè¿åŠ¨ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼ŒOmniMotion-Xèƒ½å¤Ÿç”Ÿæˆæ›´å…·é£æ ¼åŒ–å’Œä¸ªæ€§åŒ–çš„è¿åŠ¨ï¼Œä»è€Œæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ã€‚OmniMoCap-Xæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºå¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆçš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OmniMotion-Xå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ã€åŠ¨ç”»åˆ¶ä½œã€åº·å¤è®­ç»ƒç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„äººä½“åŠ¨ç”»ï¼Œåˆ›å»ºæ²‰æµ¸å¼çš„è™šæ‹Ÿä½“éªŒï¼Œè¾…åŠ©æ¸¸æˆè§’è‰²è®¾è®¡ï¼ŒåŠ é€ŸåŠ¨ç”»åˆ¶ä½œæµç¨‹ï¼Œä»¥åŠä¸ºåº·å¤æ‚£è€…æä¾›ä¸ªæ€§åŒ–çš„è¿åŠ¨æŒ‡å¯¼ã€‚è¯¥ç ”ç©¶çš„æˆæœå°†æ¨åŠ¨äººæœºäº¤äº’æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶ä¸ºäººä»¬çš„ç”Ÿæ´»å¸¦æ¥æ›´å¤šä¾¿åˆ©å’Œä¹è¶£ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.

