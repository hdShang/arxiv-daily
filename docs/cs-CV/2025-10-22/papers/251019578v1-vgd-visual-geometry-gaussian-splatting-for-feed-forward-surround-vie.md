---
layout: default
title: VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction
---

# VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.19578" target="_blank" class="toolbar-btn">arXiv: 2510.19578v1</a>
    <a href="https://arxiv.org/pdf/2510.19578.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19578v1" 
            onclick="toggleFavorite(this, '2510.19578v1', 'VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Junhong Lin, Kangli Wang, Shunzhou Wang, Songlin Fan, Ge Li, Wei Gao

**ÂàÜÁ±ª**: cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-22

**Â§áÊ≥®**: 10 pages, 7 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VGDÔºöÁî®‰∫éÂâçÈ¶àÁéØËßÜÈ©æÈ©∂Âú∫ÊôØÈáçÂª∫ÁöÑËßÜËßâÂá†‰ΩïÈ´òÊñØÊ∫ÖÂ∞Ñ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰∏ÉÔºöÂä®‰ΩúÈáçÂÆöÂêë (Motion Retargeting)**

**ÂÖ≥ÈîÆËØç**: `ÁéØËßÜÈáçÂª∫` `Ëá™Âä®È©æÈ©∂` `È´òÊñØÊ∫ÖÂ∞Ñ` `Âá†‰ΩïÂ≠¶‰π†` `Êñ∞ËßÜËßíÂêàÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁéØËßÜËá™Âä®È©æÈ©∂Âú∫ÊôØÈáçÂª∫ÊñπÊ≥ïÈöæ‰ª•Âú®Êñ∞ËßÜËßí‰∏ã‰øùËØÅÂá†‰Ωï‰∏ÄËá¥ÊÄßÂíåÈáçÂª∫Ë¥®ÈáèÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈáçÂè†Âå∫ÂüüËæÉÂ∞èÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ
2. VGDÈÄöËøáÊòæÂºèÂ≠¶‰π†Âá†‰Ωï‰ø°ÊÅØÔºåÂπ∂Âà©Áî®Ëøô‰∫õÁâπÂæÅÊåáÂØºÊñ∞ËßÜËßíËØ≠‰πâË¥®ÈáèÁöÑÊèêÂçáÔºå‰ªéËÄåËß£ÂÜ≥‰∏äËø∞ÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVGDÂú®nuScenesÊï∞ÊçÆÈõÜ‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÂèØÊâ©Â±ïÊÄßÂíåÈ´ò‰øùÁúüÈáçÂª∫ËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂâçÈ¶àÁéØËßÜËá™Âä®È©æÈ©∂Âú∫ÊôØÈáçÂª∫ÊñπÊ≥ïÔºåÊó®Âú®ÂÆûÁé∞Âø´ÈÄü‰∏îÊ≥õÂåñÊÄßÂº∫ÁöÑÊé®ÁêÜÔºåÂÖ∂Ê†∏ÂøÉÊåëÊàòÂú®‰∫é‰øùËØÅÊ≥õÂåñÊÄßÁöÑÂêåÊó∂ÊèêÂçáÊñ∞ËßÜËßíÁöÑÈáçÂª∫Ë¥®Èáè„ÄÇÈíàÂØπÁéØËßÜÂõæÂÉèÈáçÂè†Âå∫ÂüüÂ∞èÁöÑÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•‰øùËØÅÊñ∞ËßÜËßíÁöÑÂá†‰Ωï‰∏ÄËá¥ÊÄßÂíåÈáçÂª∫Ë¥®Èáè„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ËÆ§‰∏∫ÂøÖÈ°ªÊòæÂºèÂú∞Â≠¶‰π†Âá†‰Ωï‰ø°ÊÅØÔºåÂπ∂Âà©Áî®Ëøô‰∫õÁâπÂæÅÊù•ÊåáÂØºÊñ∞ËßÜËßíËØ≠‰πâË¥®ÈáèÁöÑÊèêÂçá„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜËßÜËßâÈ´òÊñØÈ©æÈ©∂ÔºàVGDÔºâÔºå‰∏ÄÁßçÊñ∞È¢ñÁöÑÂâçÈ¶àÁ´ØÂà∞Á´ØÂ≠¶‰π†Ê°ÜÊû∂Êù•Ëß£ÂÜ≥Ëøô‰∏ÄÊåëÊàò„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞ÂèØÊ≥õÂåñÁöÑÂá†‰Ωï‰º∞ËÆ°ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑVGGTÂèò‰ΩìÔºå‰ª•ÊúâÊïàÂú∞Â∞ÜÈ¢ÑËÆ≠ÁªÉVGGTÁöÑÂá†‰ΩïÂÖàÈ™åÁü•ËØÜÊèêÁÇºÂà∞Âá†‰ΩïÂàÜÊîØ‰∏≠„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™È´òÊñØÂ§¥ÔºåËûçÂêàÂ§öÂ∞∫Â∫¶Âá†‰ΩïtokensÊù•È¢ÑÊµãÊñ∞ËßÜËßíÊ∏≤ÊüìÁöÑÈ´òÊñØÂèÇÊï∞ÔºåËØ•È´òÊñØÂ§¥‰∏éÂá†‰ΩïÂàÜÊîØÂÖ±‰∫´Áõ∏ÂêåÁöÑpatch backbone„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Êï¥ÂêàÊù•Ëá™Âá†‰ΩïÂàÜÊîØÂíåÈ´òÊñØÂ§¥ÂàÜÊîØÁöÑÂ§öÂ∞∫Â∫¶ÁâπÂæÅÔºåÂÖ±ÂêåÁõëÁù£ËØ≠‰πâÁªÜÂåñÊ®°ÂûãÔºåÈÄöËøáÁâπÂæÅ‰∏ÄËá¥ÊÄßÂ≠¶‰π†‰ºòÂåñÊ∏≤ÊüìË¥®Èáè„ÄÇÂú®nuScenes‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂêÑÁßçËÆæÁΩÆ‰∏ãÔºåÂú®ÂÆ¢ËßÇÊåáÊ†áÂíå‰∏ªËßÇË¥®ÈáèÊñπÈù¢ÂùáÊòæËëó‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜVGDÁöÑÂèØÊâ©Â±ïÊÄßÂíåÈ´ò‰øùÁúüÁéØËßÜÈáçÂª∫ËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÂâçÈ¶àÁéØËßÜËá™Âä®È©æÈ©∂Âú∫ÊôØÈáçÂª∫‰∏≠ÔºåÂ¶Ç‰ΩïÂú®‰øùËØÅÊ≥õÂåñÊÄßÁöÑÂâçÊèê‰∏ãÔºåÊèêÂçáÊñ∞ËßÜËßíÁöÑÈáçÂª∫Ë¥®ÈáèÂíåÂá†‰Ωï‰∏ÄËá¥ÊÄßÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁî±‰∫éÁéØËßÜÂõæÂÉèÁöÑÈáçÂè†Âå∫ÂüüÂ∞èÔºåÈöæ‰ª•Âú®Êñ∞ËßÜËßí‰∏ãÁª¥ÊåÅÂá†‰Ωï‰∏ÄËá¥ÊÄßÔºåÂØºËá¥ÈáçÂª∫Ë¥®Èáè‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊòæÂºèÂú∞Â≠¶‰π†Âá†‰Ωï‰ø°ÊÅØÔºåÂπ∂Â∞ÜËøô‰∫õÂá†‰Ωï‰ø°ÊÅØ‰Ωú‰∏∫ÂÖàÈ™åÁü•ËØÜÔºåÊåáÂØºÊñ∞ËßÜËßíÁöÑËØ≠‰πâË¥®ÈáèÊèêÂçá„ÄÇÈÄöËøáÂ≠¶‰π†Âá†‰Ωï‰ø°ÊÅØÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Âú∫ÊôØÁªìÊûÑÔºå‰ªéËÄåÊèêÈ´òÊñ∞ËßÜËßíÁöÑÈáçÂª∫Ë¥®ÈáèÂíåÂá†‰Ωï‰∏ÄËá¥ÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVGDÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂá†‰ΩïÂàÜÊîØ„ÄÅÈ´òÊñØÂ§¥ÂíåËØ≠‰πâÁªÜÂåñÊ®°Âûã„ÄÇÂá†‰ΩïÂàÜÊîØË¥üË¥£ÊèêÂèñÂá†‰ΩïÁâπÂæÅÔºåÈ´òÊñØÂ§¥Âà©Áî®Âá†‰ΩïÁâπÂæÅÈ¢ÑÊµãÈ´òÊñØÂèÇÊï∞ÔºåËØ≠‰πâÁªÜÂåñÊ®°ÂûãÂàôÊï¥ÂêàÂá†‰ΩïÂíåÈ´òÊñØÂ§¥ÁöÑ‰ø°ÊÅØÔºå‰ºòÂåñÊúÄÁªàÁöÑÊ∏≤ÊüìË¥®Èáè„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÈááÁî®Á´ØÂà∞Á´ØÁöÑÂâçÈ¶àÊñπÂºèËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊòæÂºèÂú∞Â≠¶‰π†Âá†‰Ωï‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂ËûçÂÖ•Âà∞Êñ∞ËßÜËßíÁöÑÊ∏≤ÊüìËøáÁ®ã‰∏≠„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáËΩªÈáèÁ∫ßÁöÑVGGTÂèò‰ΩìÊèêÂèñÂá†‰ΩïÁâπÂæÅÔºåÂπ∂ËÆæËÆ°È´òÊñØÂ§¥È¢ÑÊµãÈ´òÊñØÂèÇÊï∞Ôºå‰ªéËÄåÂÆûÁé∞È´òË¥®ÈáèÁöÑÊñ∞ËßÜËßíÊ∏≤Êüì„ÄÇÊ≠§Â§ñÔºåÁâπÂæÅ‰∏ÄËá¥ÊÄßÂ≠¶‰π†‰πüËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊ∏≤ÊüìË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂá†‰ΩïÂàÜÊîØÈááÁî®ËΩªÈáèÁ∫ßÁöÑVGGTÂèò‰ΩìÔºå‰ªéÈ¢ÑËÆ≠ÁªÉÁöÑVGGT‰∏≠Ëí∏È¶èÂá†‰ΩïÂÖàÈ™åÁü•ËØÜ„ÄÇÈ´òÊñØÂ§¥ËûçÂêàÂ§öÂ∞∫Â∫¶Âá†‰ΩïtokensÔºåÈ¢ÑÊµãÈ´òÊñØÂèÇÊï∞„ÄÇËØ≠‰πâÁªÜÂåñÊ®°ÂûãÊï¥ÂêàÂá†‰ΩïÂàÜÊîØÂíåÈ´òÊñØÂ§¥ÁöÑÂ§öÂ∞∫Â∫¶ÁâπÂæÅÔºåÈÄöËøáÁâπÂæÅ‰∏ÄËá¥ÊÄßÂ≠¶‰π†‰ºòÂåñÊ∏≤ÊüìË¥®Èáè„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨Âá†‰ΩïÊçüÂ§±„ÄÅÊ∏≤ÊüìÊçüÂ§±ÂíåÁâπÂæÅ‰∏ÄËá¥ÊÄßÊçüÂ§±„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VGDÂú®nuScenesÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂú®ÂÆ¢ËßÇÊåáÊ†áÂíå‰∏ªËßÇË¥®Èáè‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVGDËÉΩÂ§üÁîüÊàêÊõ¥Ê∏ÖÊô∞„ÄÅÊõ¥ÈÄºÁúüÁöÑÊñ∞ËßÜËßíÂõæÂÉèÔºåÂπ∂‰∏îÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÂá†‰Ωï‰∏ÄËá¥ÊÄß„ÄÇÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶Âú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜÁöÑÈáèÂåñ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÈ´òË¥®ÈáèÁöÑÁéØËßÜÂú∫ÊôØÈáçÂª∫ÔºåÂèØ‰ª•ÊèêÂçáËá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÔºåÊèêÈ´òÂØºËà™ÁöÑÂáÜÁ°ÆÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÂú®ËôöÊãüÁé∞ÂÆûÈ¢ÜÂüüÔºåÂèØ‰ª•ÂàõÂª∫Êõ¥ÈÄºÁúü„ÄÅÊõ¥Ê≤âÊµ∏ÂºèÁöÑËôöÊãüÁéØÂ¢É„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Feed-forward surround-view autonomous driving scene reconstruction offers fast, generalizable inference ability, which faces the core challenge of ensuring generalization while elevating novel view quality. Due to the surround-view with minimal overlap regions, existing methods typically fail to ensure geometric consistency and reconstruction quality for novel views. To tackle this tension, we claim that geometric information must be learned explicitly, and the resulting features should be leveraged to guide the elevating of semantic quality in novel views. In this paper, we introduce \textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end learning framework designed to address this challenge. To achieve generalizable geometric estimation, we design a lightweight variant of the VGGT architecture to efficiently distill its geometric priors from the pre-trained VGGT to the geometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale geometry tokens to predict Gaussian parameters for novel view rendering, which shares the same patch backbone as the geometry branch. Finally, we integrate multi-scale features from both geometry and Gaussian head branches to jointly supervise a semantic refinement model, optimizing rendering quality through feature-consistent learning. Experiments on nuScenes demonstrate that our approach significantly outperforms state-of-the-art methods in both objective metrics and subjective quality under various settings, which validates VGD's scalability and high-fidelity surround-view reconstruction.

