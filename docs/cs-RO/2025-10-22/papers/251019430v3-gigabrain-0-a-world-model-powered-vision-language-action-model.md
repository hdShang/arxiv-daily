---
layout: default
title: "GigaBrain-0: A World Model-Powered Vision-Language-Action Model"
---

# GigaBrain-0: A World Model-Powered Vision-Language-Action Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19430" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19430v3</a>
  <a href="https://arxiv.org/pdf/2510.19430.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19430v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.19430v3', 'GigaBrain-0: A World Model-Powered Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, Peng Li, Qiuping Deng, Runqi Ouyang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yilong Li, Yiran Ding, Yuan Xu, Yun Ye, Yukun Zhou, Zhehao Dong, Zhenan Wang, Zhichao Liu, Zheng Zhu

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22 (æ›´æ–°: 2025-12-04)

**å¤‡æ³¨**: https://gigabrain0.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GigaBrain-0ï¼šåŸºäºä¸–ç•Œæ¨¡å‹èµ‹èƒ½çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œé€šç”¨æœºå™¨äººæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `ä¸–ç•Œæ¨¡å‹` `æœºå™¨äººå­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `å…·èº«æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ä¾èµ–æ˜‚è´µçš„çœŸå®æœºå™¨äººæ•°æ®ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. GigaBrain-0åˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–æ•°æ®ï¼Œå‡å°‘å¯¹çœŸå®æ•°æ®çš„ä¾èµ–ï¼Œæå‡è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚
3. é€šè¿‡RGBDè¾“å…¥å’Œå…·èº«CoTç›‘ç£ï¼Œå¢å¼ºæ¨¡å‹å¯¹ç©ºé—´å‡ ä½•ã€å¯¹è±¡çŠ¶æ€å’Œé•¿æ—¶ä¾èµ–çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†GigaBrain-0ï¼Œä¸€ç§æ–°å‹çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè§†é¢‘ç”Ÿæˆã€real2realè¿ç§»ã€äººä½“è¿ç§»ã€è§†è§’è¿ç§»ã€sim2realè¿ç§»æ•°æ®ï¼‰è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡å¤§è§„æ¨¡åˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æ•°æ®ï¼ŒGigaBrain-0æ˜¾è‘—é™ä½äº†å¯¹çœŸå®æœºå™¨äººæ•°æ®çš„ä¾èµ–ï¼ŒåŒæ—¶æé«˜äº†è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è¿˜é€šè¿‡RGBDè¾“å…¥å»ºæ¨¡å’Œå…·èº«Chain-of-Thoughtï¼ˆCoTï¼‰ç›‘ç£æ¥æé«˜ç­–ç•¥çš„é²æ£’æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»åŠ¡æ‰§è¡ŒæœŸé—´æ¨ç†ç©ºé—´å‡ ä½•ã€å¯¹è±¡çŠ¶æ€å’Œé•¿æ—¶ä¾èµ–å…³ç³»ã€‚è¿™åœ¨çµå·§ã€é•¿æ—¶ç¨‹å’Œç§»åŠ¨æ“ä½œä»»åŠ¡çš„çœŸå®ä¸–ç•Œæ€§èƒ½æ–¹é¢å¸¦æ¥äº†æ˜¾è‘—çš„æå‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGigaBrain-0åœ¨å¤–è§‚ï¼ˆä¾‹å¦‚ï¼Œçº¹ç†ã€é¢œè‰²ï¼‰ã€å¯¹è±¡æ”¾ç½®å’Œç›¸æœºè§†ç‚¹çš„å˜åŒ–æ–¹é¢å®ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†GigaBrain-0-Smallï¼Œè¿™æ˜¯ä¸€ç§ä¼˜åŒ–çš„è½»é‡çº§å˜ä½“ï¼Œæ—¨åœ¨åœ¨NVIDIA Jetson AGX Orinç­‰è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹è®­ç»ƒä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡çœŸå®æœºå™¨äººæ•°æ®ï¼Œè€ŒçœŸå®æ•°æ®çš„é‡‡é›†æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œè¿™æˆä¸ºåˆ¶çº¦VLAæ¨¡å‹æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„å…³é”®ç“¶é¢ˆã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹çœŸå®ä¸–ç•Œä¸­å¤æ‚å¤šå˜çš„ç¯å¢ƒå’Œä»»åŠ¡éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGigaBrain-0çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆå¤§é‡å¤šæ ·åŒ–çš„åˆæˆæ•°æ®ï¼ŒåŒ…æ‹¬è§†é¢‘ç”Ÿæˆã€real2realè¿ç§»ã€äººä½“è¿ç§»ã€è§†è§’è¿ç§»ã€sim2realè¿ç§»ç­‰ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å¯¹çœŸå®æœºå™¨äººæ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´é€šç”¨çš„è§†è§‰å’Œç‰©ç†è§„å¾‹ï¼Œä»è€Œæå‡åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGigaBrain-0çš„æ•´ä½“æ¡†æ¶åŒ…å«æ•°æ®ç”Ÿæˆæ¨¡å—ã€æ¨¡å‹è®­ç»ƒæ¨¡å—å’Œç­–ç•¥æ‰§è¡Œæ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—åˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆå„ç§ç±»å‹çš„åˆæˆæ•°æ®ï¼Œç”¨äºæ¨¡å‹çš„é¢„è®­ç»ƒã€‚æ¨¡å‹è®­ç»ƒæ¨¡å—é‡‡ç”¨Transformeræ¶æ„ï¼Œå¹¶ç»“åˆRGBDè¾“å…¥å»ºæ¨¡å’Œå…·èº«Chain-of-Thoughtï¼ˆCoTï¼‰ç›‘ç£ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚ç­–ç•¥æ‰§è¡Œæ¨¡å—å°†è®­ç»ƒå¥½çš„æ¨¡å‹éƒ¨ç½²åˆ°çœŸå®æœºå™¨äººä¸Šï¼Œå®Œæˆå„ç§æ“ä½œä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šGigaBrain-0æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºåˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆæ•°æ®æ¥è®­ç»ƒVLAæ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–çœŸå®æ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æ›´ä½çš„æˆæœ¬ç”Ÿæˆæ›´å¤§è§„æ¨¡ã€æ›´å¤šæ ·åŒ–çš„æ•°æ®ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒRGBDè¾“å…¥å»ºæ¨¡å’Œå…·èº«CoTç›‘ç£ä¹Ÿè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šRGBDè¾“å…¥å»ºæ¨¡å…è®¸æ¨¡å‹åŒæ—¶å¤„ç†RGBå›¾åƒå’Œæ·±åº¦ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£åœºæ™¯çš„å‡ ä½•ç»“æ„ã€‚å…·èº«CoTç›‘ç£é€šè¿‡å¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡Œæ›´ç»†ç²’åº¦çš„æ¨ç†ï¼Œä»è€Œæé«˜ç­–ç•¥çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGigaBrain-0åœ¨çœŸå®ä¸–ç•Œçš„çµå·§æ“ä½œã€é•¿æ—¶ç¨‹æ“ä½œå’Œç§»åŠ¨æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†è®ºæ–‡å¼ºè°ƒï¼ŒGigaBrain-0åœ¨å¤–è§‚ã€å¯¹è±¡æ”¾ç½®å’Œç›¸æœºè§†ç‚¹å˜åŒ–ç­‰æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GigaBrain-0åœ¨é€šç”¨æœºå™¨äººé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå¼€å‘èƒ½å¤Ÿæ‰§è¡Œå„ç§å¤æ‚æ“ä½œä»»åŠ¡çš„æ™ºèƒ½æœºå™¨äººã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥åº”ç”¨äºæ™ºèƒ½åˆ¶é€ ã€ä»“å‚¨ç‰©æµã€å®¶åº­æœåŠ¡ç­‰é¢†åŸŸï¼Œå®ç°è‡ªåŠ¨åŒ–ç”Ÿäº§ã€æ™ºèƒ½åˆ†æ‹£ã€æ™ºèƒ½æ¸…æ´ç­‰åŠŸèƒ½ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„è¿›æ­¥ï¼ŒåŠ é€Ÿæœºå™¨äººåœ¨å„è¡Œå„ä¸šçš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.

