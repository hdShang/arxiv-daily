---
layout: default
title: Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning
---

# Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.19495" target="_blank" class="toolbar-btn">arXiv: 2510.19495v2</a>
    <a href="https://arxiv.org/pdf/2510.19495.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19495v2" 
            onclick="toggleFavorite(this, '2510.19495v2', 'Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kevin Huang, Rosario Scalise, Cleah Winston, Ayush Agrawal, Yunchu Zhang, Rohan Baijal, Markus Grotz, Byron Boots, Benjamin Burchfiel, Masha Itkina, Paarth Shah, Abhishek Gupta

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-22 (Êõ¥Êñ∞: 2025-10-25)

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://uwrobotlearning.github.io/RISE-offline/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Âà©Áî®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÔºåÈÄöËøáÈùû‰∏ìÂÆ∂Êï∞ÊçÆÂ¢ûÂº∫Ê®°‰ªøÂ≠¶‰π†ÁöÑÈ≤ÅÊ£íÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê®°‰ªøÂ≠¶‰π†` `Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†` `Èùû‰∏ìÂÆ∂Êï∞ÊçÆ` `Êú∫Âô®‰∫∫Êìç‰Ωú` `È≤ÅÊ£íÊÄß` `Ê≥õÂåñËÉΩÂäõ` `Á≠ñÁï•Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê®°‰ªøÂ≠¶‰π†‰æùËµñÈ´òË¥®Èáè‰∏ìÂÆ∂Êï∞ÊçÆÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÈùû‰∏ìÂÆ∂Êï∞ÊçÆÊàêÊú¨‰Ωé‰ΩÜÈöæ‰ª•ÊúâÊïàÂà©Áî®„ÄÇ
2. Âà©Áî®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÔºåÈÄöËøáÁÆóÊ≥ï‰øÆÊîπÊãìÂÆΩÁ≠ñÁï•ÂàÜÂ∏ÉÔºå‰ªéËÄåÊúâÊïàÂà©Áî®Èùû‰∏ìÂÆ∂Êï∞ÊçÆÔºåÊèêÂçáÊ®°‰ªøÂ≠¶‰π†ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊòæËëóÊèêÂçá‰∫ÜÁ≠ñÁï•Âú®Êìç‰Ωú‰ªªÂä°‰∏≠ÁöÑÊàêÂäüÁéáÂíåÊ≥õÂåñËÉΩÂäõÔºåËÉΩÂ§üÊúâÊïàÂà©Áî®Ê¨°‰ºòÊï∞ÊçÆ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê®°‰ªøÂ≠¶‰π†Âú®ËÆ≠ÁªÉÊú∫Âô®‰∫∫ÊâßË°åÂ§çÊùÇ‰ªªÂä°ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂ÂØπÈ´òË¥®Èáè„ÄÅÁâπÂÆö‰ªªÂä°Êï∞ÊçÆÁöÑ‰æùËµñÈôêÂà∂‰∫ÜÂÖ∂Âú®ÁúüÂÆû‰∏ñÁïå‰∏≠ÂØπÂêÑÁßçÁâ©‰ΩìÈÖçÁΩÆÂíåÂú∫ÊôØÁöÑÈÄÇÂ∫îÊÄß„ÄÇÈùû‰∏ìÂÆ∂Êï∞ÊçÆÔºàÂ¶ÇÊ∏∏ÊàèÊï∞ÊçÆ„ÄÅÊ¨°‰ºòÊºîÁ§∫„ÄÅÈÉ®ÂàÜ‰ªªÂä°ÂÆåÊàêÊàñÊ¨°‰ºòÁ≠ñÁï•ÁöÑrolloutÔºâÂèØ‰ª•Êèê‰æõÊõ¥ÂπøÊ≥õÁöÑË¶ÜÁõñËåÉÂõ¥ÂíåÊõ¥‰ΩéÁöÑÊî∂ÈõÜÊàêÊú¨„ÄÇÁÑ∂ËÄåÔºå‰º†ÁªüÁöÑÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÂà©Áî®Ëøô‰∫õÊï∞ÊçÆ„ÄÇÊú¨ÊñáÊèêÂá∫ÔºåÈÄöËøáÂêàÁêÜÁöÑËÆæËÆ°ÔºåÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÂèØ‰ª•‰Ωú‰∏∫‰∏ÄÁßçÂ∑•ÂÖ∑ÔºåÂà©Áî®Èùû‰∏ìÂÆ∂Êï∞ÊçÆÊù•Â¢ûÂº∫Ê®°‰ªøÂ≠¶‰π†Á≠ñÁï•ÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊ†áÂáÜÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÁúüÂÆû‰∏ñÁïå‰∏≠ÈÅáÂà∞ÁöÑÁ®ÄÁñèÊï∞ÊçÆË¶ÜÁõñËÆæÁΩÆ‰∏ãÔºåÊó†Ê≥ïÊúâÊïàÂú∞Âà©Áî®Èùû‰∏ìÂÆ∂Êï∞ÊçÆÔºå‰ΩÜÁÆÄÂçïÁöÑÁÆóÊ≥ï‰øÆÊîπÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÂÅáËÆæÁöÑÊÉÖÂÜµ‰∏ãÂà©Áî®Ëøô‰∫õÊï∞ÊçÆ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊãìÂÆΩÁ≠ñÁï•ÂàÜÂ∏ÉÁöÑÊîØÊåÅÔºå‰ΩøÈÄöËøáÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Â¢ûÂº∫ÁöÑÊ®°‰ªøÁÆóÊ≥ïËÉΩÂ§üÈ≤ÅÊ£íÂú∞Ëß£ÂÜ≥‰ªªÂä°Ôºå‰ªéËÄåÊòæËëóÊèêÈ´òÊÅ¢Â§çÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÂú®Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåËøô‰∫õÂàõÊñ∞ÊòæËëóÂ¢ûÂä†‰∫ÜÂ≠¶‰π†Á≠ñÁï•Âú®Á∫≥ÂÖ•Èùû‰∏ìÂÆ∂Êï∞ÊçÆÊó∂ÊàêÂäüÁöÑÂàùÂßãÊù°‰ª∂ËåÉÂõ¥„ÄÇÊ≠§Â§ñÔºåÁ†îÁ©∂Ë°®ÊòéËøô‰∫õÊñπÊ≥ïËÉΩÂ§üÂà©Áî®ÊâÄÊúâÊî∂ÈõÜÁöÑÊï∞ÊçÆÔºåÂåÖÊã¨ÈÉ®ÂàÜÊàñÊ¨°‰ºòÊºîÁ§∫Ôºå‰ª•ÊîØÊåÅ‰ªªÂä°ÂØºÂêëÁöÑÁ≠ñÁï•ÊÄßËÉΩ„ÄÇËøôÁ™ÅÂá∫‰∫ÜÁÆóÊ≥ïÊäÄÊúØÂú®Êú∫Âô®‰∫∫È≤ÅÊ£íÁ≠ñÁï•Â≠¶‰π†‰∏≠‰ΩøÁî®Èùû‰∏ìÂÆ∂Êï∞ÊçÆÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊ®°‰ªøÂ≠¶‰π†‰æùËµñ‰∫éÈ´òË¥®ÈáèÁöÑ‰∏ìÂÆ∂Êï∞ÊçÆÔºåËøôÈôêÂà∂‰∫ÜÂÆÉÂú®ÁúüÂÆû‰∏ñÁïåÂ§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇÊî∂ÈõÜÈ´òË¥®ÈáèÁöÑ‰∏ìÂÆ∂Êï∞ÊçÆÊàêÊú¨È´òÊòÇÔºå‰∏îÈöæ‰ª•Ë¶ÜÁõñÊâÄÊúâÂèØËÉΩÁöÑÂú∫ÊôØÂíåÁä∂ÊÄÅ„ÄÇÈùû‰∏ìÂÆ∂Êï∞ÊçÆÔºå‰æãÂ¶Ç‰∫∫Á±ªÁöÑÈùû‰∏ì‰∏öÊºîÁ§∫„ÄÅÂ§±Ë¥•ÁöÑÂ∞ùËØïÊàñÈöèÊú∫Êé¢Á¥¢ÁöÑÊï∞ÊçÆÔºåËôΩÁÑ∂Êòì‰∫éËé∑ÂèñÔºå‰ΩÜ‰º†ÁªüÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂà©Áî®ÔºåÂØºËá¥Á≠ñÁï•ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÔºàOffline RLÔºâÊù•ÊúâÊïàÂà©Áî®Èùû‰∏ìÂÆ∂Êï∞ÊçÆÔºå‰ªéËÄåÂ¢ûÂº∫Ê®°‰ªøÂ≠¶‰π†Á≠ñÁï•ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÈÄöËøáÂ∞ÜÈùû‰∏ìÂÆ∂Êï∞ÊçÆËßÜ‰∏∫Á¶ªÁ∫øÊï∞ÊçÆÈõÜÔºåÂπ∂ÈááÁî®ÁâπÂÆöÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂèØ‰ª•Â≠¶‰π†Âà∞Êõ¥ÂÖ∑Ê≥õÂåñËÉΩÂäõÁöÑÁ≠ñÁï•Ôºå‰ªéËÄåÂÖãÊúç‰º†ÁªüÊ®°‰ªøÂ≠¶‰π†ÂØπÈ´òË¥®Èáè‰∏ìÂÆ∂Êï∞ÊçÆÁöÑ‰æùËµñ„ÄÇÂÖ≥ÈîÆÂú®‰∫éËÆæËÆ°ÂêàÈÄÇÁöÑÁÆóÊ≥ïÔºå‰ΩøÂÖ∂ËÉΩÂ§ü‰ªéÂåÖÂê´Âô™Â£∞ÂíåÊ¨°‰ºòË°å‰∏∫ÁöÑÈùû‰∏ìÂÆ∂Êï∞ÊçÆ‰∏≠ÊèêÂèñÊúâÁî®ÁöÑ‰ø°ÊÅØ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊ°ÜÊû∂ÊòØÈ¶ñÂÖàÊî∂ÈõÜÂåÖÂê´‰∏ìÂÆ∂Êï∞ÊçÆÂíåÈùû‰∏ìÂÆ∂Êï∞ÊçÆÁöÑÊ∑∑ÂêàÊï∞ÊçÆÈõÜ„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂü∫‰∫éËØ•Êï∞ÊçÆÈõÜËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÂÖ≥ÈîÆÁöÑÁÆóÊ≥ï‰øÆÊîπÂú®‰∫éÊãìÂÆΩÁ≠ñÁï•ÂàÜÂ∏ÉÁöÑÊîØÊåÅÔºå‰ΩøÂæóÁ≠ñÁï•ËÉΩÂ§üË¶ÜÁõñÊõ¥ÂπøÊ≥õÁöÑÁä∂ÊÄÅÁ©∫Èó¥Ôºå‰ªéËÄåÊèêÈ´òÈ≤ÅÊ£íÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂèØ‰ª•ÈááÁî®ËØ∏Â¶ÇConservative Q-Learning (CQL) Êàñ Behavior Cloning with PerturbationÁ≠âÊñπÊ≥ïÔºåÊù•Á∫¶ÊùüÁ≠ñÁï•ÁöÑÂ≠¶‰π†ÔºåÈÅøÂÖçËøáÂ∫¶ÊãüÂêà‰∏ìÂÆ∂Êï∞ÊçÆÔºåÂπ∂ÈºìÂä±Êé¢Á¥¢Èùû‰∏ìÂÆ∂Êï∞ÊçÆ‰∏≠ÁöÑÊΩúÂú®ÊúâÁî®‰ø°ÊÅØ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éËØÅÊòé‰∫ÜÈÄöËøáÁÆÄÂçïÁöÑÁÆóÊ≥ï‰øÆÊîπÔºåÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÂèØ‰ª•ÊúâÊïàÂú∞Âà©Áî®Èùû‰∏ìÂÆ∂Êï∞ÊçÆÊù•Â¢ûÂº∫Ê®°‰ªøÂ≠¶‰π†ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ‰∏é‰º†ÁªüÁöÑÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ï‰∏çÂÜç‰ªÖ‰ªÖ‰æùËµñ‰∫éÈ´òË¥®ÈáèÁöÑ‰∏ìÂÆ∂Êï∞ÊçÆÔºåËÄåÊòØËÉΩÂ§ü‰ªéÂåÖÂê´Âô™Â£∞ÂíåÊ¨°‰ºòË°å‰∏∫ÁöÑÈùû‰∏ìÂÆ∂Êï∞ÊçÆ‰∏≠Â≠¶‰π†Ôºå‰ªéËÄåÊèêÈ´òÁ≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈÄÇÂ∫îÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ï‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂÅáËÆæÊàñÂ§çÊùÇÁöÑÊ®°ÂûãÔºåÊòì‰∫éÂÆûÁé∞ÂíåÈÉ®ÁΩ≤„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠ÊèêÂà∞ÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Conservative Q-Learning (CQL) ÊàñÁ±ª‰ººÁöÑÁÆóÊ≥ïÊù•Á∫¶ÊùüÁ≠ñÁï•ÁöÑÂ≠¶‰π†ÔºåÈÅøÂÖçËøáÂ∫¶ÊãüÂêà‰∏ìÂÆ∂Êï∞ÊçÆ„ÄÇ2) ÈÄöËøáÊï∞ÊçÆÂ¢ûÂº∫ÊàñÁ≠ñÁï•Êâ∞Âä®Á≠âÊäÄÊúØÔºåÊãìÂÆΩÁ≠ñÁï•ÂàÜÂ∏ÉÁöÑÊîØÊåÅÔºåÈºìÂä±Êé¢Á¥¢Èùû‰∏ìÂÆ∂Êï∞ÊçÆ‰∏≠ÁöÑÊΩúÂú®ÊúâÁî®‰ø°ÊÅØ„ÄÇ3) ‰ªîÁªÜË∞ÉÊï¥Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁöÑË∂ÖÂèÇÊï∞Ôºå‰æãÂ¶ÇÂ≠¶‰π†Áéá„ÄÅÊäòÊâ£Âõ†Â≠êÂíåÊ≠£ÂàôÂåñÁ≥ªÊï∞Ôºå‰ª•Á°Æ‰øùÁÆóÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞‰ªéÈùû‰∏ìÂÆ∂Êï∞ÊçÆ‰∏≠Â≠¶‰π†„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êìç‰Ωú‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÁ≠ñÁï•ÁöÑÊàêÂäüÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÂºïÂÖ•Èùû‰∏ìÂÆ∂Êï∞ÊçÆÔºåÁ≠ñÁï•ËÉΩÂ§üÊõ¥Â•ΩÂú∞Â∫îÂØπÂêÑÁßçÂàùÂßãÊù°‰ª∂ÂíåÁéØÂ¢ÉÂèòÂåñÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈ≤ÅÊ£íÊÄß„ÄÇ‰∏é‰º†ÁªüÁöÑÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂà©Áî®ÊâÄÊúâÊî∂ÈõÜÁöÑÊï∞ÊçÆÔºåÂåÖÊã¨ÈÉ®ÂàÜÊàñÊ¨°‰ºòÊºîÁ§∫Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÁ≠ñÁï•ÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶Êú™Áü•Ôºå‰ΩÜËÆ∫ÊñáÂº∫Ë∞É‰∫ÜÂú®‰∏çÂêåÂàùÂßãÊù°‰ª∂‰∏ãÊàêÂäüÁéáÁöÑÊòæËëóÂ¢ûÂä†„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊ∏∏ÊàèAIÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÂà©Áî®‰ΩéÊàêÊú¨ÁöÑÈùû‰∏ìÂÆ∂Êï∞ÊçÆÔºåÂèØ‰ª•ÊòæËëóÈôç‰ΩéËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìÁöÑÊàêÊú¨ÂíåÈöæÂ∫¶ÔºåÂä†ÈÄüÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåÂèØ‰ª•Âà©Áî®‰∫∫Á±ªÁöÑÈùû‰∏ì‰∏öÊºîÁ§∫Êï∞ÊçÆÊù•ËÆ≠ÁªÉÊú∫Âô®‰∫∫ÂÆåÊàêÂêÑÁßç‰ªªÂä°ÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇÂú®Ëá™Âä®È©æÈ©∂È¢ÜÂüüÔºåÂèØ‰ª•Âà©Áî®Â§ßÈáèÁöÑÈ©æÈ©∂Ê®°ÊãüÊï∞ÊçÆÂíåÁúüÂÆû‰∏ñÁïå‰∏≠ÁöÑÈ©æÈ©∂Êï∞ÊçÆÊù•ËÆ≠ÁªÉËá™Âä®È©æÈ©∂Á≥ªÁªüÔºåÊèêÈ´òÁ≥ªÁªüÁöÑÂÆâÂÖ®ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Imitation learning has proven effective for training robots to perform complex tasks from expert human demonstrations. However, it remains limited by its reliance on high-quality, task-specific data, restricting adaptability to the diverse range of real-world object configurations and scenarios. In contrast, non-expert data -- such as play data, suboptimal demonstrations, partial task completions, or rollouts from suboptimal policies -- can offer broader coverage and lower collection costs. However, conventional imitation learning approaches fail to utilize this data effectively. To address these challenges, we posit that with right design decisions, offline reinforcement learning can be used as a tool to harness non-expert data to enhance the performance of imitation learning policies. We show that while standard offline RL approaches can be ineffective at actually leveraging non-expert data under the sparse data coverage settings typically encountered in the real world, simple algorithmic modifications can allow for the utilization of this data, without significant additional assumptions. Our approach shows that broadening the support of the policy distribution can allow imitation algorithms augmented by offline RL to solve tasks robustly, showing considerably enhanced recovery and generalization behavior. In manipulation tasks, these innovations significantly increase the range of initial conditions where learned policies are successful when non-expert data is incorporated. Moreover, we show that these methods are able to leverage all collected data, including partial or suboptimal demonstrations, to bolster task-directed policy performance. This underscores the importance of algorithmic techniques for using non-expert data for robust policy learning in robotics. Website: https://uwrobotlearning.github.io/RISE-offline/

