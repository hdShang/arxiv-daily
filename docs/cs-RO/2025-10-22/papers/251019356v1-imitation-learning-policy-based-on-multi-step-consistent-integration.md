---
layout: default
title: Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model
---

# Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19356" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19356v1</a>
  <a href="https://arxiv.org/pdf/2510.19356.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19356v1" onclick="toggleFavorite(this, '2510.19356v1', 'Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yu Fang, Xinyu Wang, Xuehe Zhang, Wanli Xue, Mingwei Zhang, Shengyong Chen, Jie Zhao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¤šæ­¥ä¸€è‡´æ€§ç§¯åˆ†æ·å¾„æ¨¡å‹çš„æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ï¼ŒåŠ é€Ÿæœºå™¨äººç­–ç•¥æ¨ç†ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `æµé‡åŒ¹é…` `æœºå™¨äººæ§åˆ¶` `å•æ­¥æ¨ç†` `å¤šæ­¥ä¸€è‡´æ€§` `è‡ªé€‚åº”æ¢¯åº¦` `ç­–ç•¥å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæµé‡åŒ¹é…çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•æ¨ç†é€Ÿåº¦æ…¢ï¼Œè’¸é¦å’Œä¸€è‡´æ€§æ–¹æ³•æ€§èƒ½ä¸è¶³ã€‚
2. æå‡ºä¸€ç§å•æ­¥æ·å¾„æ¨¡å‹ï¼Œç»“åˆå¤šæ­¥ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥å¹³è¡¡æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ã€‚
3. å¼•å…¥è‡ªé€‚åº”æ¢¯åº¦åˆ†é…æ–¹æ³•ï¼Œè§£å†³å¤šæ­¥æŸå¤±ä¼˜åŒ–ä¸ç¨³å®šçš„é—®é¢˜ï¼Œæå‡å­¦ä¹ ç¨³å®šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æµé‡åŒ¹é…æ–¹æ³•åœ¨æœºå™¨äººæ¨¡ä»¿å­¦ä¹ ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†æ™®éå­˜åœ¨æ¨ç†æ—¶é—´è¿‡é•¿çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†è’¸é¦æ–¹æ³•å’Œä¸€è‡´æ€§æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•çš„æ€§èƒ½ä»ç„¶éš¾ä»¥ä¸åŸå§‹çš„æ‰©æ•£æ¨¡å‹å’Œæµé‡åŒ¹é…æ¨¡å‹ç›¸åª²ç¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæœºå™¨äººæ¨¡ä»¿å­¦ä¹ çš„å…·æœ‰å¤šæ­¥ç§¯åˆ†çš„å•æ­¥æ·å¾„æ–¹æ³•ã€‚ä¸ºäº†å¹³è¡¡æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨æ·å¾„æ¨¡å‹çš„åŸºç¡€ä¸Šæ‰©å±•äº†å¤šæ­¥ä¸€è‡´æ€§æŸå¤±ï¼Œå°†å•æ­¥æŸå¤±åˆ†è§£ä¸ºå¤šæ­¥æŸå¤±ï¼Œä»è€Œæé«˜äº†å•æ­¥æ¨ç†çš„æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³å¤šæ­¥æŸå¤±å’ŒåŸå§‹æµé‡åŒ¹é…æŸå¤±ä¼˜åŒ–ä¸ç¨³å®šé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ¢¯åº¦åˆ†é…æ–¹æ³•ï¼Œä»¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªæ¨¡æ‹ŸåŸºå‡†å’Œäº”ä¸ªçœŸå®ç¯å¢ƒä»»åŠ¡ä¸­è¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºæµé‡åŒ¹é…çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹é€šå¸¸éœ€è¦å¤šæ¬¡è¿­ä»£ï¼Œå¯¼è‡´æ¨ç†æ—¶é—´è¿‡é•¿ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚è’¸é¦å’Œä¸€è‡´æ€§æ–¹æ³•è™½ç„¶å¯ä»¥åŠ é€Ÿæ¨ç†ï¼Œä½†æ€§èƒ½å¾€å¾€ä¸å¦‚åŸå§‹çš„æµé‡åŒ¹é…æ¨¡å‹ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½æ¨ç†æ—¶é—´ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªå•æ­¥æ·å¾„æ¨¡å‹ï¼Œç›´æ¥é¢„æµ‹æœ€ç»ˆçš„åŠ¨ä½œï¼Œä»è€Œé¿å…å¤šæ¬¡è¿­ä»£æ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥å•æ­¥æ¨¡å‹å¯èƒ½å¸¦æ¥çš„æ€§èƒ½æŸå¤±ï¼Œå¼•å…¥å¤šæ­¥ä¸€è‡´æ€§æŸå¤±ï¼Œå°†å•æ­¥é¢„æµ‹åˆ†è§£ä¸ºå¤šä¸ªä¸­é—´æ­¥éª¤ï¼Œå¹¶çº¦æŸè¿™äº›ä¸­é—´æ­¥éª¤çš„ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜å•æ­¥æ¨¡å‹çš„é¢„æµ‹ç²¾åº¦ã€‚åŒæ—¶ï¼Œä¸ºäº†è§£å†³å¤šæ­¥æŸå¤±å¸¦æ¥çš„ä¼˜åŒ–ä¸ç¨³å®šé—®é¢˜ï¼Œè®¾è®¡è‡ªé€‚åº”æ¢¯åº¦åˆ†é…æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´ä¸åŒæŸå¤±é¡¹çš„æƒé‡ï¼Œä»¥ä¿è¯å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) æ·å¾„æ¨¡å‹ï¼šç”¨äºç›´æ¥é¢„æµ‹åŠ¨ä½œï¼Œå®ç°å•æ­¥æ¨ç†ã€‚2) å¤šæ­¥ä¸€è‡´æ€§æŸå¤±ï¼šå°†å•æ­¥é¢„æµ‹åˆ†è§£ä¸ºå¤šä¸ªä¸­é—´æ­¥éª¤ï¼Œå¹¶çº¦æŸè¿™äº›ä¸­é—´æ­¥éª¤çš„ä¸€è‡´æ€§ã€‚3) è‡ªé€‚åº”æ¢¯åº¦åˆ†é…ï¼šåŠ¨æ€è°ƒæ•´ä¸åŒæŸå¤±é¡¹çš„æƒé‡ï¼Œä¿è¯å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œåˆ©ç”¨æµé‡åŒ¹é…æ¨¡å‹ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼›ç„¶åï¼Œè®­ç»ƒæ·å¾„æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨å¤šæ­¥ä¸€è‡´æ€§æŸå¤±å’Œè‡ªé€‚åº”æ¢¯åº¦åˆ†é…æ–¹æ³•è¿›è¡Œä¼˜åŒ–ï¼›æœ€åï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„æ·å¾„æ¨¡å‹è¿›è¡Œå•æ­¥æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å•æ­¥æ·å¾„æ¨¡å‹ä¸å¤šæ­¥ä¸€è‡´æ€§æŸå¤±ç›¸ç»“åˆï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥è‡ªé€‚åº”æ¢¯åº¦åˆ†é…ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½æ¨ç†æ—¶é—´ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”æ¢¯åº¦åˆ†é…æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤šæ­¥æŸå¤±å¸¦æ¥çš„ä¼˜åŒ–ä¸ç¨³å®šé—®é¢˜ï¼Œæé«˜äº†å­¦ä¹ è¿‡ç¨‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šå¤šæ­¥ä¸€è‡´æ€§æŸå¤±çš„å…·ä½“å½¢å¼ä¸ºï¼šå°†å•æ­¥é¢„æµ‹åˆ†è§£ä¸ºNä¸ªä¸­é—´æ­¥éª¤ï¼Œå¹¶è®¡ç®—æ¯ä¸ªä¸­é—´æ­¥éª¤çš„é¢„æµ‹ç»“æœä¸çœŸå®è½¨è¿¹ä¹‹é—´çš„å·®å¼‚ï¼Œç„¶åå°†è¿™äº›å·®å¼‚åŠ æƒæ±‚å’Œï¼Œä½œä¸ºå¤šæ­¥ä¸€è‡´æ€§æŸå¤±ã€‚è‡ªé€‚åº”æ¢¯åº¦åˆ†é…çš„å…·ä½“æ–¹æ³•ä¸ºï¼šæ ¹æ®æ¯ä¸ªæŸå¤±é¡¹çš„æ¢¯åº¦å¤§å°ï¼ŒåŠ¨æ€è°ƒæ•´å…¶æƒé‡ï¼Œä½¿å¾—æ¢¯åº¦è¾ƒå¤§çš„æŸå¤±é¡¹è·å¾—æ›´å¤§çš„æƒé‡ï¼Œä»è€ŒåŠ é€Ÿå…¶æ”¶æ•›ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ä¸¤ä¸ªæ¨¡æ‹ŸåŸºå‡†å’Œäº”ä¸ªçœŸå®ç¯å¢ƒä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿è¯æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½æ¨ç†æ—¶é—´ã€‚å…·ä½“è€Œè¨€ï¼Œä¸åŸå§‹çš„æµé‡åŒ¹é…æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„æ¨ç†é€Ÿåº¦æé«˜äº†æ•°å€ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±è¾ƒå°ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”æ¢¯åº¦åˆ†é…æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæé«˜å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºéœ€è¦å¿«é€Ÿå“åº”çš„æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ— äººæœºæ§åˆ¶ã€æœºå™¨äººæ“ä½œç­‰ã€‚é€šè¿‡é™ä½æ¨ç†æ—¶é—´ï¼Œå¯ä»¥æé«˜æœºå™¨äººçš„å®æ—¶æ€§å’Œå®‰å…¨æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦åŠ é€Ÿæ¨ç†çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The wide application of flow-matching methods has greatly promoted the development of robot imitation learning. However, these methods all face the problem of high inference time. To address this issue, researchers have proposed distillation methods and consistency methods, but the performance of these methods still struggles to compete with that of the original diffusion models and flow-matching models. In this article, we propose a one-step shortcut method with multi-step integration for robot imitation learning. To balance the inference speed and performance, we extend the multi-step consistency loss on the basis of the shortcut model, split the one-step loss into multi-step losses, and improve the performance of one-step inference. Secondly, to solve the problem of unstable optimization of the multi-step loss and the original flow-matching loss, we propose an adaptive gradient allocation method to enhance the stability of the learning process. Finally, we evaluate the proposed method in two simulation benchmarks and five real-world environment tasks. The experimental results verify the effectiveness of the proposed algorithm.

