---
layout: default
title: Learning Affordances at Inference-Time for Vision-Language-Action Models
---

# Learning Affordances at Inference-Time for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19752" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19752v1</a>
  <a href="https://arxiv.org/pdf/2510.19752.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19752v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.19752v1', 'Learning Affordances at Inference-Time for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ameesh Shah, William Chen, Adwait Godbole, Federico Mora, Sanjit A. Seshia, Sergey Levine

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22

**å¤‡æ³¨**: 7 pages and appendix

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLITENï¼Œé€šè¿‡æ¨ç†æ—¶å­¦ä¹ èƒ½åŠ›æå‡VLAæ¨¡å‹åœ¨å¤æ‚æœºå™¨äººä»»åŠ¡ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ§åˆ¶` `æ¨ç†æ—¶å­¦ä¹ ` `ç¤ºèƒ½æ€§` `é•¿æ—¶ç¨‹ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­ç¼ºä¹åŠ¨æ€è°ƒæ•´èƒ½åŠ›ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ä»»åŠ¡ä¸­çš„å¤±è´¥æƒ…å†µã€‚
2. LITENé€šè¿‡åœ¨æ¨ç†æ—¶å­¦ä¹ ï¼Œåˆ©ç”¨VLMåæ€æ‰§è¡Œç»“æœï¼ŒåŠ¨æ€è°ƒæ•´VLAç­–ç•¥ï¼Œæå‡ä»»åŠ¡æˆåŠŸç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLITENèƒ½æœ‰æ•ˆå­¦ä¹ ç»éªŒï¼Œç”Ÿæˆé«˜ç¤ºèƒ½æ€§æŒ‡ä»¤ï¼Œå®Œæˆé•¿æ—¶ç¨‹ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§£å†³å¤æ‚ç°å®ä¸–ç•Œæ§åˆ¶ä»»åŠ¡é€šå¸¸éœ€è¦å¤šæ¬¡å°è¯•ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(VLA)åœ¨æœºå™¨äººé¢†åŸŸå±•ç°å‡ºè§£å†³å¤æ‚æ§åˆ¶ä»»åŠ¡çš„æ½œåŠ›ï¼Œä½†ç¼ºä¹åœ¨ä»»åŠ¡å¤±è´¥æ—¶åŠ¨æ€è°ƒæ•´è¡Œä¸ºçš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œæ¨ç†æ—¶æ‰§è¡Œå­¦ä¹ â€(LITEN)çš„æ–¹æ³•ï¼Œå®ƒå°†VLAä½çº§ç­–ç•¥ä¸é«˜çº§VLMè¿æ¥èµ·æ¥ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡åŒ…å«è¿‡å»çš„ç»éªŒæ¥è°ƒèŠ‚VLMï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ ä½çº§VLAçš„ç¤ºèƒ½æ€§å’Œèƒ½åŠ›ã€‚LITENåœ¨æ¨ç†é˜¶æ®µç”Ÿæˆå¹¶æ‰§è¡ŒVLAçš„è®¡åˆ’ï¼Œç„¶ååœ¨è¯„ä¼°é˜¶æ®µåæ€æ‰§è¡Œç»“æœï¼Œå¾—å‡ºæœ‰ç”¨çš„ç»“è®ºï¼Œå¹¶å°†å…¶çº³å…¥æœªæ¥çš„æ¨ç†ä¸Šä¸‹æ–‡ã€‚ä¸éæœºå™¨äººé¢†åŸŸä¸­ç±»ä¼¼çš„è‡ªå®Œå–„æ–¹æ³•ä¸åŒï¼ŒLITENå¿…é¡»åæ€éç»“æ„åŒ–çš„çœŸå®ä¸–ç•Œæœºå™¨äººè½¨è¿¹(ä¾‹å¦‚ï¼ŒåŸå§‹è§†é¢‘)ï¼Œè¿™éœ€è¦åœ¨è¯„ä¼°æœŸé—´æä¾›ç»“æ„åŒ–çš„æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLITENèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»è¿‡å»çš„ç»éªŒä¸­å­¦ä¹ ï¼Œç”Ÿæˆåˆ©ç”¨é«˜ç¤ºèƒ½æ€§æŒ‡ä»¤æ¥å®Œæˆé•¿æ—¶ç¨‹ä»»åŠ¡çš„è®¡åˆ’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(VLA)åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶ç¨‹å¤æ‚ä»»åŠ¡ä¸­ï¼Œå¸¸å¸¸å› ä¸ºç¼ºä¹åŠ¨æ€è°ƒæ•´èƒ½åŠ›è€Œéš¾ä»¥æˆåŠŸã€‚å½“VLAæ¨¡å‹æ‰§è¡Œå¤±è´¥æ—¶ï¼Œæ— æ³•æœ‰æ•ˆåœ°ä»å¤±è´¥ç»éªŒä¸­å­¦ä¹ ï¼Œå¹¶è°ƒæ•´åç»­çš„ç­–ç•¥ï¼Œå¯¼è‡´ä»»åŠ¡å®Œæˆæ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰çš„è‡ªå®Œå–„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨éæœºå™¨äººé¢†åŸŸï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨äºå¤„ç†æœºå™¨äººä»»åŠ¡ä¸­éç»“æ„åŒ–çš„æ•°æ®ï¼Œä¾‹å¦‚åŸå§‹è§†é¢‘æ•°æ®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLITENçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œå­¦ä¹ ï¼Œé€šè¿‡å°†VLAä½çº§ç­–ç•¥ä¸é«˜çº§VLMè¿æ¥ï¼Œåˆ©ç”¨VLMçš„æ¨ç†èƒ½åŠ›åæ€è¿‡å»çš„æ‰§è¡Œç»éªŒï¼Œå¹¶å°†å…¶çº³å…¥æœªæ¥çš„æ¨ç†ä¸Šä¸‹æ–‡ä¸­ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åŠ¨æ€åœ°å­¦ä¹ ä½çº§VLAçš„ç¤ºèƒ½æ€§å’Œèƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆæ›´æœ‰æ•ˆçš„è®¡åˆ’ã€‚é€šè¿‡è¿­ä»£æ¨ç†å’Œè¯„ä¼°ï¼ŒLITENèƒ½å¤Ÿä¸æ–­ä¼˜åŒ–ç­–ç•¥ï¼Œæé«˜ä»»åŠ¡æˆåŠŸç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLITENçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæ¨ç†é˜¶æ®µå’Œè¯„ä¼°é˜¶æ®µã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒVLMç”Ÿæˆå¹¶æ‰§è¡ŒVLAçš„è®¡åˆ’ã€‚VLAæ ¹æ®VLMæä¾›çš„æŒ‡ä»¤æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚åœ¨è¯„ä¼°é˜¶æ®µï¼ŒLITENåæ€æ‰§è¡Œç»“æœï¼Œå¹¶ä»ä¸­æå–æœ‰ç”¨çš„ä¿¡æ¯ã€‚è¿™äº›ä¿¡æ¯è¢«æ·»åŠ åˆ°æœªæ¥çš„æ¨ç†ä¸Šä¸‹æ–‡ä¸­ï¼Œç”¨äºæŒ‡å¯¼VLMç”Ÿæˆæ›´æœ‰æ•ˆçš„è®¡åˆ’ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸æ–­è¿­ä»£ï¼Œç›´åˆ°ä»»åŠ¡æˆåŠŸå®Œæˆæˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šLITENçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åœ¨æ¨ç†æ—¶å­¦ä¹ çš„èƒ½åŠ›ï¼Œä»¥åŠå…¶å¤„ç†éç»“æ„åŒ–æœºå™¨äººè½¨è¿¹æ•°æ®çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„VLAæ¨¡å‹ç›¸æ¯”ï¼ŒLITENèƒ½å¤ŸåŠ¨æ€åœ°è°ƒæ•´ç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”å¤æ‚ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒLITENé€šè¿‡ç»“æ„åŒ–çš„æŒ‡å¯¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»åŸå§‹è§†é¢‘ç­‰éç»“æ„åŒ–æ•°æ®ä¸­æå–æœ‰ç”¨çš„ä¿¡æ¯ï¼Œè¿™ä½¿å¾—å®ƒèƒ½å¤Ÿåº”ç”¨äºæ›´å¹¿æ³›çš„æœºå™¨äººæ§åˆ¶ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šLITENçš„å…³é”®è®¾è®¡åŒ…æ‹¬VLMçš„é€‰æ‹©ã€VLAçš„å®ç°ã€ä»¥åŠè¯„ä¼°é˜¶æ®µçš„ç»“æ„åŒ–æŒ‡å¯¼ã€‚VLMéœ€è¦å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ ¹æ®è¿‡å»çš„ç»éªŒç”Ÿæˆæœ‰æ•ˆçš„è®¡åˆ’ã€‚VLAéœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°æ‰§è¡ŒVLMæä¾›çš„æŒ‡ä»¤ã€‚è¯„ä¼°é˜¶æ®µçš„ç»“æ„åŒ–æŒ‡å¯¼éœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»éç»“æ„åŒ–æ•°æ®ä¸­æå–æœ‰ç”¨çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ä»»åŠ¡æ˜¯å¦æˆåŠŸã€å“ªäº›åŠ¨ä½œå¯¼è‡´äº†å¤±è´¥ç­‰ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­å¯èƒ½æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡å®éªŒç»“æœè¡¨æ˜ï¼ŒLITENèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»è¿‡å»çš„ç»éªŒä¸­å­¦ä¹ ï¼Œç”Ÿæˆåˆ©ç”¨é«˜ç¤ºèƒ½æ€§æŒ‡ä»¤æ¥å®Œæˆé•¿æ—¶ç¨‹ä»»åŠ¡çš„è®¡åˆ’ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®ã€å¯¹æ¯”åŸºçº¿ã€æå‡å¹…åº¦ç­‰ä¿¡æ¯åœ¨è®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚ä½†æ•´ä½“è€Œè¨€ï¼ŒLITENå±•ç°äº†åœ¨å¤æ‚æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­å­¦ä¹ å’Œé€‚åº”çš„èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LITENå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§éœ€è¦æœºå™¨äººè‡ªä¸»å®Œæˆå¤æ‚ä»»åŠ¡çš„åœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰ã€‚é€šè¿‡ä¸æ–­å­¦ä¹ å’Œé€‚åº”ç¯å¢ƒï¼ŒLITENèƒ½å¤Ÿä½¿æœºå™¨äººæ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ã€‚è¯¥ç ”ç©¶å¯¹æå‡æœºå™¨äººæ™ºèƒ½åŒ–æ°´å¹³å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Solving complex real-world control tasks often takes multiple tries: if we fail at first, we reflect on what went wrong, and change our strategy accordingly to avoid making the same mistake. In robotics, Vision-Language-Action models (VLAs) offer a promising path towards solving complex control tasks, but lack the ability to contextually and dynamically readjust behavior when they fail to accomplish a task. In this work, we introduce Learning from Inference-Time Execution (LITEN), which connects a VLA low-level policy to a high-level VLM that conditions on past experiences by including them in-context, allowing it to learn the affordances and capabilities of the low-level VLA. Our approach iterates between a reasoning phase that generates and executes plans for the low-level VLA, and an assessment phase that reflects on the resulting execution and draws useful conclusions to be included in future reasoning contexts. Unlike similar approaches to self-refinement in non-robotics domains, LITEN must reflect on unstructured real-world robot trajectories (e.g., raw videos), which requires structured guiderails during assessment. Our experimental results demonstrate LITEN is able to effectively learn from past experience to generate plans that use high-affordance instructions to accomplish long-horizon tasks.

