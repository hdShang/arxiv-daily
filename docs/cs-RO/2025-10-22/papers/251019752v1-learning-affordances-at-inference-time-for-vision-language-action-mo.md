---
layout: default
title: Learning Affordances at Inference-Time for Vision-Language-Action Models
---

# Learning Affordances at Inference-Time for Vision-Language-Action Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.19752" target="_blank" class="toolbar-btn">arXiv: 2510.19752v1</a>
    <a href="https://arxiv.org/pdf/2510.19752.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19752v1" 
            onclick="toggleFavorite(this, '2510.19752v1', 'Learning Affordances at Inference-Time for Vision-Language-Action Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ameesh Shah, William Chen, Adwait Godbole, Federico Mora, Sanjit A. Seshia, Sergey Levine

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-22

**Â§áÊ≥®**: 7 pages and appendix

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LITENÔºåÈÄöËøáÊé®ÁêÜÊó∂Â≠¶‰π†ËÉΩÂäõÊèêÂçáVLAÊ®°ÂûãÂú®Â§çÊùÇÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁöÑË°®Áé∞**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Êé®ÁêÜÊó∂Â≠¶‰π†` `Á§∫ËÉΩÊÄß` `ÈïøÊó∂Á®ã‰ªªÂä°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Êú∫Âô®‰∫∫ÊéßÂà∂‰∏≠Áº∫‰πèÂä®ÊÄÅË∞ÉÊï¥ËÉΩÂäõÔºåÈöæ‰ª•Â∫îÂØπÂ§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÂ§±Ë¥•ÊÉÖÂÜµ„ÄÇ
2. LITENÈÄöËøáÂú®Êé®ÁêÜÊó∂Â≠¶‰π†ÔºåÂà©Áî®VLMÂèçÊÄùÊâßË°åÁªìÊûúÔºåÂä®ÊÄÅË∞ÉÊï¥VLAÁ≠ñÁï•ÔºåÊèêÂçá‰ªªÂä°ÊàêÂäüÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåLITENËÉΩÊúâÊïàÂ≠¶‰π†ÁªèÈ™åÔºåÁîüÊàêÈ´òÁ§∫ËÉΩÊÄßÊåá‰ª§ÔºåÂÆåÊàêÈïøÊó∂Á®ã‰ªªÂä°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ëß£ÂÜ≥Â§çÊùÇÁé∞ÂÆû‰∏ñÁïåÊéßÂà∂‰ªªÂä°ÈÄöÂ∏∏ÈúÄË¶ÅÂ§öÊ¨°Â∞ùËØï„ÄÇËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã(VLA)Âú®Êú∫Âô®‰∫∫È¢ÜÂüüÂ±ïÁé∞Âá∫Ëß£ÂÜ≥Â§çÊùÇÊéßÂà∂‰ªªÂä°ÁöÑÊΩúÂäõÔºå‰ΩÜÁº∫‰πèÂú®‰ªªÂä°Â§±Ë¥•Êó∂Âä®ÊÄÅË∞ÉÊï¥Ë°å‰∏∫ÁöÑËÉΩÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÊé®ÁêÜÊó∂ÊâßË°åÂ≠¶‰π†‚Äù(LITEN)ÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞ÜVLA‰ΩéÁ∫ßÁ≠ñÁï•‰∏éÈ´òÁ∫ßVLMËøûÊé•Ëµ∑Êù•ÔºåÈÄöËøá‰∏ä‰∏ãÊñáÂåÖÂê´ËøáÂéªÁöÑÁªèÈ™åÊù•Ë∞ÉËäÇVLMÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂ≠¶‰π†‰ΩéÁ∫ßVLAÁöÑÁ§∫ËÉΩÊÄßÂíåËÉΩÂäõ„ÄÇLITENÂú®Êé®ÁêÜÈò∂ÊÆµÁîüÊàêÂπ∂ÊâßË°åVLAÁöÑËÆ°ÂàíÔºåÁÑ∂ÂêéÂú®ËØÑ‰º∞Èò∂ÊÆµÂèçÊÄùÊâßË°åÁªìÊûúÔºåÂæóÂá∫ÊúâÁî®ÁöÑÁªìËÆ∫ÔºåÂπ∂Â∞ÜÂÖ∂Á∫≥ÂÖ•Êú™Êù•ÁöÑÊé®ÁêÜ‰∏ä‰∏ãÊñá„ÄÇ‰∏éÈùûÊú∫Âô®‰∫∫È¢ÜÂüü‰∏≠Á±ª‰ººÁöÑËá™ÂÆåÂñÑÊñπÊ≥ï‰∏çÂêåÔºåLITENÂøÖÈ°ªÂèçÊÄùÈùûÁªìÊûÑÂåñÁöÑÁúüÂÆû‰∏ñÁïåÊú∫Âô®‰∫∫ËΩ®Ëøπ(‰æãÂ¶ÇÔºåÂéüÂßãËßÜÈ¢ë)ÔºåËøôÈúÄË¶ÅÂú®ËØÑ‰º∞ÊúüÈó¥Êèê‰æõÁªìÊûÑÂåñÁöÑÊåáÂØº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLITENËÉΩÂ§üÊúâÊïàÂú∞‰ªéËøáÂéªÁöÑÁªèÈ™å‰∏≠Â≠¶‰π†ÔºåÁîüÊàêÂà©Áî®È´òÁ§∫ËÉΩÊÄßÊåá‰ª§Êù•ÂÆåÊàêÈïøÊó∂Á®ã‰ªªÂä°ÁöÑËÆ°Âàí„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã(VLA)Âú®Êú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈïøÊó∂Á®ãÂ§çÊùÇ‰ªªÂä°‰∏≠ÔºåÂ∏∏Â∏∏Âõ†‰∏∫Áº∫‰πèÂä®ÊÄÅË∞ÉÊï¥ËÉΩÂäõËÄåÈöæ‰ª•ÊàêÂäü„ÄÇÂΩìVLAÊ®°ÂûãÊâßË°åÂ§±Ë¥•Êó∂ÔºåÊó†Ê≥ïÊúâÊïàÂú∞‰ªéÂ§±Ë¥•ÁªèÈ™å‰∏≠Â≠¶‰π†ÔºåÂπ∂Ë∞ÉÊï¥ÂêéÁª≠ÁöÑÁ≠ñÁï•ÔºåÂØºËá¥‰ªªÂä°ÂÆåÊàêÊïàÁéá‰Ωé‰∏ã„ÄÇÁé∞ÊúâÁöÑËá™ÂÆåÂñÑÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÈùûÊú∫Âô®‰∫∫È¢ÜÂüüÔºåÈöæ‰ª•Áõ¥Êé•Â∫îÁî®‰∫éÂ§ÑÁêÜÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÈùûÁªìÊûÑÂåñÁöÑÊï∞ÊçÆÔºå‰æãÂ¶ÇÂéüÂßãËßÜÈ¢ëÊï∞ÊçÆ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLITENÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËøõË°åÂ≠¶‰π†ÔºåÈÄöËøáÂ∞ÜVLA‰ΩéÁ∫ßÁ≠ñÁï•‰∏éÈ´òÁ∫ßVLMËøûÊé•ÔºåÂà©Áî®VLMÁöÑÊé®ÁêÜËÉΩÂäõÂèçÊÄùËøáÂéªÁöÑÊâßË°åÁªèÈ™åÔºåÂπ∂Â∞ÜÂÖ∂Á∫≥ÂÖ•Êú™Êù•ÁöÑÊé®ÁêÜ‰∏ä‰∏ãÊñá‰∏≠„ÄÇËøôÁßçÊñπÊ≥ïÂÖÅËÆ∏Ê®°ÂûãÂä®ÊÄÅÂú∞Â≠¶‰π†‰ΩéÁ∫ßVLAÁöÑÁ§∫ËÉΩÊÄßÂíåËÉΩÂäõÔºå‰ªéËÄåÁîüÊàêÊõ¥ÊúâÊïàÁöÑËÆ°Âàí„ÄÇÈÄöËøáËø≠‰ª£Êé®ÁêÜÂíåËØÑ‰º∞ÔºåLITENËÉΩÂ§ü‰∏çÊñ≠‰ºòÂåñÁ≠ñÁï•ÔºåÊèêÈ´ò‰ªªÂä°ÊàêÂäüÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLITENÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÊé®ÁêÜÈò∂ÊÆµÂíåËØÑ‰º∞Èò∂ÊÆµ„ÄÇÂú®Êé®ÁêÜÈò∂ÊÆµÔºåVLMÁîüÊàêÂπ∂ÊâßË°åVLAÁöÑËÆ°Âàí„ÄÇVLAÊ†πÊçÆVLMÊèê‰æõÁöÑÊåá‰ª§ÊâßË°åÁõ∏Â∫îÁöÑÂä®‰Ωú„ÄÇÂú®ËØÑ‰º∞Èò∂ÊÆµÔºåLITENÂèçÊÄùÊâßË°åÁªìÊûúÔºåÂπ∂‰ªé‰∏≠ÊèêÂèñÊúâÁî®ÁöÑ‰ø°ÊÅØ„ÄÇËøô‰∫õ‰ø°ÊÅØË¢´Ê∑ªÂä†Âà∞Êú™Êù•ÁöÑÊé®ÁêÜ‰∏ä‰∏ãÊñá‰∏≠ÔºåÁî®‰∫éÊåáÂØºVLMÁîüÊàêÊõ¥ÊúâÊïàÁöÑËÆ°Âàí„ÄÇËøô‰∏™ËøáÁ®ã‰∏çÊñ≠Ëø≠‰ª£ÔºåÁõ¥Âà∞‰ªªÂä°ÊàêÂäüÂÆåÊàêÊàñËææÂà∞ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLITENÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Âú®Êé®ÁêÜÊó∂Â≠¶‰π†ÁöÑËÉΩÂäõÔºå‰ª•ÂèäÂÖ∂Â§ÑÁêÜÈùûÁªìÊûÑÂåñÊú∫Âô®‰∫∫ËΩ®ËøπÊï∞ÊçÆÁöÑËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑVLAÊ®°ÂûãÁõ∏ÊØîÔºåLITENËÉΩÂ§üÂä®ÊÄÅÂú∞Ë∞ÉÊï¥Á≠ñÁï•Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇ‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåLITENÈÄöËøáÁªìÊûÑÂåñÁöÑÊåáÂØºÔºåËÉΩÂ§üÊúâÊïàÂú∞‰ªéÂéüÂßãËßÜÈ¢ëÁ≠âÈùûÁªìÊûÑÂåñÊï∞ÊçÆ‰∏≠ÊèêÂèñÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåËøô‰ΩøÂæóÂÆÉËÉΩÂ§üÂ∫îÁî®‰∫éÊõ¥ÂπøÊ≥õÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöLITENÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨VLMÁöÑÈÄâÊã©„ÄÅVLAÁöÑÂÆûÁé∞„ÄÅ‰ª•ÂèäËØÑ‰º∞Èò∂ÊÆµÁöÑÁªìÊûÑÂåñÊåáÂØº„ÄÇVLMÈúÄË¶ÅÂÖ∑Â§áÂº∫Â§ßÁöÑÊé®ÁêÜËÉΩÂäõÔºåËÉΩÂ§üÊ†πÊçÆËøáÂéªÁöÑÁªèÈ™åÁîüÊàêÊúâÊïàÁöÑËÆ°Âàí„ÄÇVLAÈúÄË¶ÅËÉΩÂ§üÂáÜÁ°ÆÂú∞ÊâßË°åVLMÊèê‰æõÁöÑÊåá‰ª§„ÄÇËØÑ‰º∞Èò∂ÊÆµÁöÑÁªìÊûÑÂåñÊåáÂØºÈúÄË¶ÅËÉΩÂ§üÊúâÊïàÂú∞‰ªéÈùûÁªìÊûÑÂåñÊï∞ÊçÆ‰∏≠ÊèêÂèñÊúâÁî®ÁöÑ‰ø°ÊÅØÔºå‰æãÂ¶Ç‰ªªÂä°ÊòØÂê¶ÊàêÂäü„ÄÅÂì™‰∫õÂä®‰ΩúÂØºËá¥‰∫ÜÂ§±Ë¥•Á≠â„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞„ÄÅÁΩëÁªúÁªìÊûÑÁ≠âÊäÄÊúØÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÂèØËÉΩÊú™ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLITENËÉΩÂ§üÊúâÊïàÂú∞‰ªéËøáÂéªÁöÑÁªèÈ™å‰∏≠Â≠¶‰π†ÔºåÁîüÊàêÂà©Áî®È´òÁ§∫ËÉΩÊÄßÊåá‰ª§Êù•ÂÆåÊàêÈïøÊó∂Á®ã‰ªªÂä°ÁöÑËÆ°Âàí„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆ„ÄÅÂØπÊØîÂü∫Á∫ø„ÄÅÊèêÂçáÂπÖÂ∫¶Á≠â‰ø°ÊÅØÂú®ËÆ∫Êñá‰∏≠Êú™ÊòéÁ°ÆÁªôÂá∫ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ‰ΩÜÊï¥‰ΩìËÄåË®ÄÔºåLITENÂ±ïÁé∞‰∫ÜÂú®Â§çÊùÇÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°‰∏≠Â≠¶‰π†ÂíåÈÄÇÂ∫îÁöÑËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LITENÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊú∫Âô®‰∫∫Ëá™‰∏ªÂÆåÊàêÂ§çÊùÇ‰ªªÂä°ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂåªÁñóËæÖÂä©Êú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøá‰∏çÊñ≠Â≠¶‰π†ÂíåÈÄÇÂ∫îÁéØÂ¢ÉÔºåLITENËÉΩÂ§ü‰ΩøÊú∫Âô®‰∫∫Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥È´òÊïàÂú∞ÂÆåÊàê‰ªªÂä°ÔºåÊèêÈ´òÁîü‰∫ßÊïàÁéáÂíåÊúçÂä°Ë¥®Èáè„ÄÇËØ•Á†îÁ©∂ÂØπÊèêÂçáÊú∫Âô®‰∫∫Êô∫ËÉΩÂåñÊ∞¥Âπ≥ÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Solving complex real-world control tasks often takes multiple tries: if we fail at first, we reflect on what went wrong, and change our strategy accordingly to avoid making the same mistake. In robotics, Vision-Language-Action models (VLAs) offer a promising path towards solving complex control tasks, but lack the ability to contextually and dynamically readjust behavior when they fail to accomplish a task. In this work, we introduce Learning from Inference-Time Execution (LITEN), which connects a VLA low-level policy to a high-level VLM that conditions on past experiences by including them in-context, allowing it to learn the affordances and capabilities of the low-level VLA. Our approach iterates between a reasoning phase that generates and executes plans for the low-level VLA, and an assessment phase that reflects on the resulting execution and draws useful conclusions to be included in future reasoning contexts. Unlike similar approaches to self-refinement in non-robotics domains, LITEN must reflect on unstructured real-world robot trajectories (e.g., raw videos), which requires structured guiderails during assessment. Our experimental results demonstrate LITEN is able to effectively learn from past experience to generate plans that use high-affordance instructions to accomplish long-horizon tasks.

