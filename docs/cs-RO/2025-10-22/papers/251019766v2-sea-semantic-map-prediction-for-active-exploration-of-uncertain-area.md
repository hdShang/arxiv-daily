---
layout: default
title: SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas
---

# SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.19766" target="_blank" class="toolbar-btn">arXiv: 2510.19766v2</a>
    <a href="https://arxiv.org/pdf/2510.19766.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19766v2" 
            onclick="toggleFavorite(this, '2510.19766v2', 'SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hongyu Ding, Xinyue Liang, Yudong Fang, You Wu, Jieqi Shi, Jing Huo, Wenbin Li, Jing Wu, Yu-Kun Lai, Yang Gao

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-22 (Êõ¥Êñ∞: 2025-12-11)

**Â§áÊ≥®**: Project page: https://robo-lavira.github.io/sea-active-exp

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**SEAÔºöÂü∫‰∫éËØ≠‰πâÂú∞ÂõæÈ¢ÑÊµãÁöÑ‰∏ªÂä®Êé¢Á¥¢‰∏çÁ°ÆÂÆöÂå∫ÂüüÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `‰∏ªÂä®Êé¢Á¥¢` `ËØ≠‰πâÂú∞ÂõæÈ¢ÑÊµã` `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÂØºËà™` `ÁéØÂ¢ÉÁêÜËß£`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÂ≠¶‰π†ÁöÑÊé¢Á¥¢ÊñπÊ≥ï‰æùËµñÂçïÊ≠•Ëà™ÁÇπÈ¢ÑÊµãÔºåÁº∫‰πèÂØπÁéØÂ¢ÉÁöÑÈïøÊúüÁêÜËß£ÔºåÂØºËá¥Êé¢Á¥¢ÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. SEAÊñπÊ≥ïÈÄöËøáËø≠‰ª£È¢ÑÊµãÁº∫Â§±Âú∞ÂõæÂå∫ÂüüÔºåÂπ∂Âà©Áî®ÂÆûÈôÖÂú∞Âõæ‰∏éÈ¢ÑÊµãÂú∞ÂõæÁöÑÂ∑ÆÂºÇÊåáÂØºÊé¢Á¥¢ÔºåÂ¢ûÂº∫‰∫ÜÈïøÊúüÁéØÂ¢ÉÁêÜËß£„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSEAÊñπÊ≥ïÂú®Áõ∏ÂêåÊó∂Èó¥Á∫¶Êùü‰∏ãÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊé¢Á¥¢Á≠ñÁï•ÔºåÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂÖ®Â±ÄÂú∞ÂõæË¶ÜÁõñÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SEAÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫é‰∏ªÂä®Êú∫Âô®‰∫∫Êé¢Á¥¢ÔºåËØ•ÊñπÊ≥ïÈÄöËøáËØ≠‰πâÂú∞ÂõæÈ¢ÑÊµãÂíåÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂàÜÂ±ÇÊé¢Á¥¢Á≠ñÁï•ÂÆûÁé∞„ÄÇ‰∏éÁé∞Êúâ‰æùËµñÂçïÊ≠•Ëà™ÁÇπÈ¢ÑÊµãÁöÑÂ≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ¢ûÂº∫‰∫ÜÊô∫ËÉΩ‰ΩìÂØπÁéØÂ¢ÉÁöÑÈïøÊúüÁêÜËß£Ôºå‰ªéËÄå‰øÉËøõÊõ¥ÊúâÊïàÁöÑÊé¢Á¥¢„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ëø≠‰ª£ÁöÑÈ¢ÑÊµã-Êé¢Á¥¢Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âü∫‰∫éÂΩìÂâçËßÇÊµãÊòæÂºèÂú∞È¢ÑÊµãÂú∞ÂõæÁöÑÁº∫Â§±Âå∫Âüü„ÄÇÁÑ∂ÂêéÔºåÂÆûÈôÖÁ¥ØÁßØÂú∞Âõæ‰∏éÈ¢ÑÊµãÁöÑÂÖ®Â±ÄÂú∞Âõæ‰πãÈó¥ÁöÑÂ∑ÆÂºÇË¢´Áî®‰∫éÊåáÂØºÊé¢Á¥¢„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ•ñÂä±Êú∫Âà∂ÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Êù•Êõ¥Êñ∞ÈïøÊúüÊé¢Á¥¢Á≠ñÁï•Ôºå‰ΩøÊàë‰ª¨ËÉΩÂ§üÂú®ÊúâÈôêÁöÑÊ≠•È™§ÂÜÖÊûÑÂª∫ÂáÜÁ°ÆÁöÑËØ≠‰πâÂú∞Âõæ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊòæËëó‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊé¢Á¥¢Á≠ñÁï•ÔºåÂú®Áõ∏ÂêåÁöÑÊó∂Èó¥Á∫¶ÊùüÂÜÖÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂÖ®Â±ÄÂú∞ÂõæË¶ÜÁõñÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂ≠¶‰π†ÁöÑÊú∫Âô®‰∫∫‰∏ªÂä®Êé¢Á¥¢ÊñπÊ≥ïÔºåÈÄöÂ∏∏‰æùËµñ‰∫éÂçïÊ≠•Ëà™ÁÇπÈ¢ÑÊµãÔºåÁº∫‰πèÂØπÁéØÂ¢ÉÁöÑÂÖ®Â±ÄÂíåÈïøÊúüÁêÜËß£„ÄÇËøôÂØºËá¥Êé¢Á¥¢ËøáÁ®ãÊïàÁéá‰Ωé‰∏ãÔºåÈöæ‰ª•Âú®ÊúâÈôêÊó∂Èó¥ÂÜÖÊûÑÂª∫ÂÆåÊï¥ÁöÑËØ≠‰πâÂú∞Âõæ„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊèêÂçáÊú∫Âô®‰∫∫ÂØπÁéØÂ¢ÉÁöÑÈïøÊúüÁêÜËß£Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÊïàÁöÑÊé¢Á¥¢ÔºåÊòØÊú¨ÊñáË¶ÅËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËØ≠‰πâÂú∞ÂõæÈ¢ÑÊµãÊù•ÊåáÂØº‰∏ªÂä®Êé¢Á¥¢„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÈÄöËøáÈ¢ÑÊµãÂΩìÂâçËßÇÊµã‰∏ãÁº∫Â§±ÁöÑÂú∞ÂõæÂå∫ÂüüÔºåÂπ∂Âà©Áî®ÂÆûÈôÖÁ¥ØÁßØÂú∞Âõæ‰∏éÈ¢ÑÊµãÂÖ®Â±ÄÂú∞Âõæ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÊù•ÂºïÂØºÊú∫Âô®‰∫∫ÂâçÂæÄ‰ø°ÊÅØÂ¢ûÁõäÊúÄÂ§ßÁöÑÂå∫ÂüüËøõË°åÊé¢Á¥¢„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§ü‰ΩøÊú∫Âô®‰∫∫Êõ¥ÊúâÊïàÂú∞Âà©Áî®ÊúâÈôêÁöÑÊé¢Á¥¢Ê≠•È™§ÔºåÊûÑÂª∫Êõ¥ÂÆåÊï¥ÁöÑËØ≠‰πâÂú∞Âõæ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSEAÊñπÊ≥ïÂåÖÂê´‰∏Ä‰∏™Ëø≠‰ª£ÁöÑÈ¢ÑÊµã-Êé¢Á¥¢Ê°ÜÊû∂„ÄÇÈ¶ñÂÖàÔºåÊú∫Âô®‰∫∫Âü∫‰∫éÂΩìÂâçËßÇÊµãÈ¢ÑÊµãÂÖ®Â±ÄËØ≠‰πâÂú∞Âõæ„ÄÇÁÑ∂ÂêéÔºåËÆ°ÁÆóÂÆûÈôÖÁ¥ØÁßØÂú∞Âõæ‰∏éÈ¢ÑÊµãÂú∞Âõæ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂæóÂà∞‰∏Ä‰∏™‚ÄúÊú™Áü•Âå∫Âüü‚ÄùÁöÑÂàÜÂ∏É„ÄÇÊé•‰∏ãÊù•ÔºåÂà©Áî®Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂàÜÂ±ÇÊé¢Á¥¢Á≠ñÁï•ÔºåÈÄâÊã©‰∏ã‰∏Ä‰∏™Êé¢Á¥¢ÁõÆÊ†áÁÇπ„ÄÇÊúÄÂêéÔºåÊú∫Âô®‰∫∫ÁßªÂä®Âà∞ÁõÆÊ†áÁÇπÔºåÊõ¥Êñ∞Âú∞ÂõæÔºåÂπ∂ÈáçÂ§ç‰∏äËø∞ËøáÁ®ã„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÈÄöËøáËø≠‰ª£È¢ÑÊµãÂíåÊé¢Á¥¢Ôºå‰∏çÊñ≠ÂÆåÂñÑËØ≠‰πâÂú∞Âõæ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSEAÊñπÊ≥ïÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜËØ≠‰πâÂú∞ÂõæÈ¢ÑÊµã‰∏éÂº∫ÂåñÂ≠¶‰π†Áõ∏ÁªìÂêàÔºåÁî®‰∫éÊåáÂØº‰∏ªÂä®Êé¢Á¥¢„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫é‰ø°ÊÅØÂ¢ûÁõäÊàñÂêØÂèëÂºèËßÑÂàôÁöÑÊé¢Á¥¢ÊñπÊ≥ï‰∏çÂêåÔºåSEAÊñπÊ≥ïËÉΩÂ§üÂ≠¶‰π†Âà∞Êõ¥ÊúâÊïàÁöÑÈïøÊúüÊé¢Á¥¢Á≠ñÁï•„ÄÇÊ≠§Â§ñÔºåSEAÊñπÊ≥ïÈÄöËøáÈ¢ÑÊµãÁº∫Â§±Âå∫ÂüüÔºåËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞‰º∞ËÆ°ÁéØÂ¢ÉÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºå‰ªéËÄåÂºïÂØºÊú∫Âô®‰∫∫ÂâçÂæÄÊúÄÈúÄË¶ÅÊé¢Á¥¢ÁöÑÂå∫Âüü„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËØ≠‰πâÂú∞ÂõæÈ¢ÑÊµãÊñπÈù¢ÔºåÂèØ‰ª•‰ΩøÁî®ÂêÑÁßçÂõæÂÉèË°•ÂÖ®ÊàñËØ≠‰πâÂàÜÂâ≤Ê®°Âûã„ÄÇÂú®Âº∫ÂåñÂ≠¶‰π†ÊñπÈù¢ÔºåÂèØ‰ª•ËÆæËÆ°‰∏Ä‰∏™ÂàÜÂ±ÇÂ•ñÂä±Êú∫Âà∂ÔºåÈºìÂä±Êú∫Âô®‰∫∫Êé¢Á¥¢Êú™Áü•Âå∫ÂüüÔºåÂπ∂ÊÉ©ÁΩöÈáçÂ§çÊé¢Á¥¢„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑÂ∫îÁî®Âú∫ÊôØËøõË°åË∞ÉÊï¥„ÄÇÊçüÂ§±ÂáΩÊï∞ÂèØ‰ª•ÂåÖÊã¨Âú∞ÂõæÈ¢ÑÊµãÁöÑÊçüÂ§±ÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ•ñÂä±ÂáΩÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSEAÊñπÊ≥ïÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊé¢Á¥¢Á≠ñÁï•„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂú®Áõ∏ÂêåÁöÑÊó∂Èó¥Á∫¶Êùü‰∏ãÔºåSEAÊñπÊ≥ïËÉΩÂ§üË¶ÜÁõñÁöÑÂÖ®Â±ÄÂú∞ÂõæÈù¢ÁßØÊØîÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊñπÊ≥ïÈ´òÂá∫Á∫¶15%-20%„ÄÇËøôË°®ÊòéSEAÊñπÊ≥ïËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ÊúâÈôêÁöÑÊé¢Á¥¢Ê≠•È™§ÔºåÊûÑÂª∫Êõ¥ÂÆåÊï¥ÁöÑËØ≠‰πâÂú∞Âõæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËá™‰∏ªÊé¢Á¥¢ÂíåÁéØÂ¢ÉÁêÜËß£ÁöÑÊú∫Âô®‰∫∫Â∫îÁî®Âú∫ÊôØÔºå‰æãÂ¶ÇÔºöÁÅæÈöæÊïëÊè¥„ÄÅÊú™Áü•ÁéØÂ¢ÉÊµãÁªò„ÄÅÂÆ§ÂÜÖÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂÜú‰∏öÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊûÑÂª∫ÂáÜÁ°ÆÁöÑËØ≠‰πâÂú∞ÂõæÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Âë®Âõ¥ÁéØÂ¢ÉÔºå‰ªéËÄåÊâßË°åÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°Ôºå‰æãÂ¶ÇÁõÆÊ†áÊêúÁ¥¢„ÄÅË∑ØÂæÑËßÑÂàíÂíåÁéØÂ¢É‰∫§‰∫í„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊèêÂçáÊú∫Âô®‰∫∫ÁöÑËá™‰∏ªÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In this paper, we propose SEA, a novel approach for active robot exploration through semantic map prediction and a reinforcement learning-based hierarchical exploration policy. Unlike existing learning-based methods that rely on one-step waypoint prediction, our approach enhances the agent's long-term environmental understanding to facilitate more efficient exploration. We propose an iterative prediction-exploration framework that explicitly predicts the missing areas of the map based on current observations. The difference between the actual accumulated map and the predicted global map is then used to guide exploration. Additionally, we design a novel reward mechanism that leverages reinforcement learning to update the long-term exploration strategies, enabling us to construct an accurate semantic map within limited steps. Experimental results demonstrate that our method significantly outperforms state-of-the-art exploration strategies, achieving superior coverage ares of the global map within the same time constraints.

