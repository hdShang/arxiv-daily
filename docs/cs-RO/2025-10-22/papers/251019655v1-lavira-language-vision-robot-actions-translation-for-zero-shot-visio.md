---
layout: default
title: LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments
---

# LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.19655" target="_blank" class="toolbar-btn">arXiv: 2510.19655v1</a>
    <a href="https://arxiv.org/pdf/2510.19655.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19655v1" 
            onclick="toggleFavorite(this, '2510.19655v1', 'LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Hongyu Ding, Ziming Xu, Yudong Fang, You Wu, Zixuan Chen, Jieqi Shi, Jing Huo, Yifan Zhang, Yang Gao

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-22

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**LaViRAÔºöÁî®‰∫éËøûÁª≠ÁéØÂ¢ÉÈõ∂Ê†∑Êú¨ËßÜËßâËØ≠Ë®ÄÂØºËà™ÁöÑËØ≠Ë®Ä-ËßÜËßâ-Êú∫Âô®‰∫∫Âä®‰ΩúÁøªËØëÊ°ÜÊû∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂØºËà™` `Èõ∂Ê†∑Êú¨Â≠¶‰π†` `ËøûÁª≠ÁéØÂ¢É` `Â§öÊ®°ÊÄÅÂ§ßÊ®°Âûã` `Êú∫Âô®‰∫∫ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLN-CEÊñπÊ≥ï‰æùËµñÁéØÂ¢ÉÁâπÂÆöËà™ÁÇπÈ¢ÑÊµãÂô®ÔºåÊ≥õÂåñÊÄßÂ∑ÆÔºåÊàñÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Â§ßÂûãÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ
2. LaViRAÂ∞ÜÂä®‰ΩúÂàÜËß£‰∏∫ËØ≠Ë®Ä„ÄÅËßÜËßâÂíåÊú∫Âô®‰∫∫Âä®‰ΩúÁöÑÂ±ÇÁ∫ßÁªìÊûÑÔºåÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁöÑ‰∏çÂêå‰ºòÂäø„ÄÇ
3. LaViRAÂú®VLN-CEÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóË∂ÖË∂äÁé∞ÊúâÊñπÊ≥ïÔºåÂ±ïÁé∞‰∫ÜÊõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆûÈôÖÈÉ®ÁΩ≤ÊΩúÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫LaViRAÔºå‰∏Ä‰∏™ÁÆÄÂçïËÄåÊúâÊïàÁöÑÈõ∂Ê†∑Êú¨Ê°ÜÊû∂ÔºåÁî®‰∫éËß£ÂÜ≥ËøûÁª≠ÁéØÂ¢É‰∏ãÁöÑÈõ∂Ê†∑Êú¨ËßÜËßâËØ≠Ë®ÄÂØºËà™ÔºàVLN-CEÔºâÈóÆÈ¢ò„ÄÇËØ•ÈóÆÈ¢òË¶ÅÊ±ÇÊô∫ËÉΩ‰ΩìÂú®Ê≤°ÊúâÈ¢ÑËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ†πÊçÆËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§Âú®Êú™ËßÅËøáÁöÑÁéØÂ¢É‰∏≠ÂØºËà™„ÄÇÁé∞ÊúâÊñπÊ≥ïÈù¢‰∏¥‰∏Ä‰∏™ÂÖ≥ÈîÆÁöÑÊùÉË°°ÔºöË¶Å‰πà‰æùËµñ‰∫éÁâπÂÆö‰∫éÁéØÂ¢ÉÁöÑËà™ÁÇπÈ¢ÑÊµãÂô®ÔºåÈôêÂà∂‰∫ÜÂú∫ÊôØÁöÑÊ≥õÂåñËÉΩÂäõÔºõË¶Å‰πàÂú®ÂØºËà™ËøáÁ®ã‰∏≠Êú™ËÉΩÂÖÖÂàÜÂà©Áî®Â§ßÂûãÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇLaViRAÈÄöËøáÂ∞ÜÂä®‰ΩúÂàÜËß£‰∏∫Á≤óÂà∞ÁªÜÁöÑÂ±ÇÊ¨°ÁªìÊûÑÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈöæÈ¢òÔºöËØ≠Ë®ÄÂä®‰ΩúÁî®‰∫éÈ´òÂ±ÇËßÑÂàíÔºåËßÜËßâÂä®‰ΩúÁî®‰∫éÊÑüÁü•ÂÆö‰ΩçÔºåÊú∫Âô®‰∫∫Âä®‰ΩúÁî®‰∫éÈ≤ÅÊ£íÂØºËà™„ÄÇËøôÁßçÊ®°ÂùóÂåñÂàÜËß£‰ΩøÂæóÊàë‰ª¨ËÉΩÂ§üÂú®ÊØè‰∏™Èò∂ÊÆµÂà©Áî®‰∏çÂêåËßÑÊ®°ÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÁã¨Áâπ‰ºòÂäøÔºå‰ªéËÄåÂàõÂª∫‰∏Ä‰∏™Âú®Êé®ÁêÜ„ÄÅÂÆö‰ΩçÂíåÂÆûÈôÖÊéßÂà∂ÊñπÈù¢ÈÉΩÂº∫Â§ßÁöÑÁ≥ªÁªü„ÄÇLaViRAÂú®VLN-CEÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂú®Êú™ËßÅËøáÁöÑÁéØÂ¢É‰∏≠ÂçìË∂äÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈÄèÊòéÂ∫¶ÂíåÊïàÁéáÔºå‰æø‰∫éÂÆûÈôÖÈÉ®ÁΩ≤„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ËøûÁª≠ÁéØÂ¢É‰∏ãÁöÑÈõ∂Ê†∑Êú¨ËßÜËßâËØ≠Ë®ÄÂØºËà™ÔºàVLN-CEÔºâÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÔºåË¶Å‰πà‰æùËµñ‰∫éÁâπÂÆöÁéØÂ¢ÉÁöÑËà™ÁÇπÈ¢ÑÊµãÂô®ÔºåÂØºËá¥Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºõË¶Å‰πàÊú™ËÉΩÂÖÖÂàÜÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂØºËà™ËøáÁ®ã‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÈôêÂà∂‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑÂÜ≥Á≠ñË¥®Èáè„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLaViRAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂØºËà™Âä®‰ΩúÂàÜËß£‰∏∫‰∏â‰∏™Â±ÇÊ¨°ÔºöËØ≠Ë®ÄÂä®‰ΩúÔºàLanguage ActionÔºâ„ÄÅËßÜËßâÂä®‰ΩúÔºàVision ActionÔºâÂíåÊú∫Âô®‰∫∫Âä®‰ΩúÔºàRobot ActionÔºâ„ÄÇËøôÁßçÂàÜËß£ÂÖÅËÆ∏Á≥ªÁªüÂú®‰∏çÂêåÂ±ÇÊ¨°‰∏äÂà©Áî®‰∏çÂêåËßÑÊ®°ÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑ‰ºòÂäøÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÂØºËà™„ÄÇÈÄöËøáËß£ËÄ¶È´òÂ±ÇËßÑÂàí„ÄÅÊÑüÁü•ÂÆö‰ΩçÂíåÂ∫ïÂ±ÇÊéßÂà∂ÔºåLaViRAËÉΩÂ§üÊõ¥Â•ΩÂú∞Â∫îÂØπÂ§çÊùÇÁéØÂ¢ÉÂíåÊåá‰ª§„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLaViRAÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂùóÔºåË¥üË¥£Ê†πÊçÆËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ËøõË°åÈ´òÂ±ÇËßÑÂàíÔºåÁ°ÆÂÆöÂØºËà™ÁõÆÊ†áÂíåÁ≠ñÁï•Ôºõ2) ËßÜËßâÂä®‰ΩúÊ®°ÂùóÔºåË¥üË¥£Ê†πÊçÆÂΩìÂâçÁéØÂ¢ÉÁöÑËßÜËßâ‰ø°ÊÅØËøõË°åÊÑüÁü•ÂÆö‰ΩçÔºåËØÜÂà´ÂÖ≥ÈîÆÂú∞Ê†áÂíåÈöúÁ¢çÁâ©Ôºõ3) Êú∫Âô®‰∫∫Âä®‰ΩúÊ®°ÂùóÔºåË¥üË¥£ÊâßË°åÂÖ∑‰ΩìÁöÑÂØºËà™Âä®‰ΩúÔºå‰æãÂ¶ÇÂâçËøõ„ÄÅËΩ¨ÂêëÁ≠âÔºåÂπ∂‰øùËØÅÂØºËà™ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇËøô‰∏â‰∏™Ê®°ÂùóÂçèÂêåÂ∑•‰ΩúÔºåÂÆåÊàêÊï¥‰∏™ÂØºËà™‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLaViRAÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂÖ∂Âä®‰ΩúÂàÜËß£ÁöÑÂ±ÇÊ¨°ÁªìÊûÑÂíåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÊïàÂà©Áî®„ÄÇÈÄöËøáÂ∞ÜÂØºËà™‰ªªÂä°ÂàÜËß£‰∏∫ËØ≠Ë®Ä„ÄÅËßÜËßâÂíåÊú∫Âô®‰∫∫Âä®‰ΩúÔºåLaViRAËÉΩÂ§üÊõ¥Â•ΩÂú∞Âà©Áî®‰∏çÂêåÊ®°ÂûãÁöÑ‰ºòÂäøÔºåÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥È≤ÅÊ£íÁöÑÂØºËà™„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåLaViRA‰∏çÈúÄË¶ÅÈíàÂØπÁâπÂÆöÁéØÂ¢ÉËøõË°åËÆ≠ÁªÉÔºåÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöLaViRAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åËØ≠Ë®ÄÂä®‰ΩúËßÑÂàíÂíåËßÜËßâÂä®‰ΩúÂÆö‰ΩçÔºõ2) ËÆæËÆ°‰∫ÜËØ≠Ë®ÄÂä®‰Ωú„ÄÅËßÜËßâÂä®‰ΩúÂíåÊú∫Âô®‰∫∫Âä®‰Ωú‰πãÈó¥ÁöÑÊé•Âè£Ôºå‰øùËØÅÊ®°Âùó‰πãÈó¥ÁöÑÂçèÂêåÂ∑•‰ΩúÔºõ3) ÈááÁî®‰∫ÜÈ≤ÅÊ£íÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂ÁÆóÊ≥ïÔºå‰øùËØÅÂØºËà™ÁöÑÁ®≥ÂÆöÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

LaViRAÂú®VLN-CEÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåLaViRAÂú®Êú™ËßÅËøáÁöÑÁéØÂ¢É‰∏≠Â±ïÁé∞‰∫ÜÂçìË∂äÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂØºËà™ÊàêÂäüÁéáÂæóÂà∞‰∫ÜÊòæËëóÊèêÈ´ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLaViRAÁöÑÂä®‰ΩúÂàÜËß£Á≠ñÁï•ÂíåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÊïàÂà©Áî®ÊòØÂÖ∂ÊàêÂäüÁöÑÂÖ≥ÈîÆ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LaViRAÁöÑÁ†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊô∫ËÉΩ‰ΩìÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ËøõË°åÂØºËà™ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅ‰ªìÂ∫ìÁâ©ÊµÅÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂Ê±ΩËΩ¶Á≠â„ÄÇËØ•Á†îÁ©∂ÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂú®‰∫éÊèêÈ´ò‰∫ÜÊô∫ËÉΩ‰ΩìÂú®Êú™Áü•ÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ËÉΩÂäõÔºåÈôç‰Ωé‰∫ÜÂØπÁéØÂ¢ÉÁöÑ‰æùËµñÊÄßÔºå‰∏∫ÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™‰∏ªÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÂ•†ÂÆö‰∫ÜÂü∫Á°Ä„ÄÇÊú™Êù•ÔºåLaViRAÊúâÊúõËøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÂíåÁéØÂ¢ÉÔºå‰æãÂ¶ÇÔºöÁÅæÈöæÊïëÊè¥„ÄÅÂüéÂ∏ÇÊé¢Á¥¢Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control. LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments, while maintaining transparency and efficiency for real-world deployment.

