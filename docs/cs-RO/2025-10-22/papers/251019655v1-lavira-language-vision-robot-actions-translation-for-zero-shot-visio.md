---
layout: default
title: LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments
---

# LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.19655" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.19655v1</a>
  <a href="https://arxiv.org/pdf/2510.19655.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.19655v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.19655v1', 'LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyu Ding, Ziming Xu, Yudong Fang, You Wu, Zixuan Chen, Jieqi Shi, Jing Huo, Yifan Zhang, Yang Gao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-22

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LaViRAï¼šç”¨äºè¿ç»­ç¯å¢ƒé›¶æ ·æœ¬è§†è§‰è¯­è¨€å¯¼èˆªçš„è¯­è¨€-è§†è§‰-æœºå™¨äººåŠ¨ä½œç¿»è¯‘æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `é›¶æ ·æœ¬å­¦ä¹ ` `è¿ç»­ç¯å¢ƒ` `å¤šæ¨¡æ€å¤§æ¨¡å‹` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLN-CEæ–¹æ³•ä¾èµ–ç¯å¢ƒç‰¹å®šèˆªç‚¹é¢„æµ‹å™¨ï¼Œæ³›åŒ–æ€§å·®ï¼Œæˆ–æœªèƒ½å……åˆ†åˆ©ç”¨å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
2. LaViRAå°†åŠ¨ä½œåˆ†è§£ä¸ºè¯­è¨€ã€è§†è§‰å’Œæœºå™¨äººåŠ¨ä½œçš„å±‚çº§ç»“æ„ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ä¸åŒä¼˜åŠ¿ã€‚
3. LaViRAåœ¨VLN-CEåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…éƒ¨ç½²æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºLaViRAï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œç”¨äºè§£å†³è¿ç»­ç¯å¢ƒä¸‹çš„é›¶æ ·æœ¬è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLN-CEï¼‰é—®é¢˜ã€‚è¯¥é—®é¢˜è¦æ±‚æ™ºèƒ½ä½“åœ¨æ²¡æœ‰é¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­å¯¼èˆªã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¸€ä¸ªå…³é”®çš„æƒè¡¡ï¼šè¦ä¹ˆä¾èµ–äºç‰¹å®šäºç¯å¢ƒçš„èˆªç‚¹é¢„æµ‹å™¨ï¼Œé™åˆ¶äº†åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ï¼›è¦ä¹ˆåœ¨å¯¼èˆªè¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†åˆ©ç”¨å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚LaViRAé€šè¿‡å°†åŠ¨ä½œåˆ†è§£ä¸ºç²—åˆ°ç»†çš„å±‚æ¬¡ç»“æ„æ¥è§£å†³è¿™ä¸ªéš¾é¢˜ï¼šè¯­è¨€åŠ¨ä½œç”¨äºé«˜å±‚è§„åˆ’ï¼Œè§†è§‰åŠ¨ä½œç”¨äºæ„ŸçŸ¥å®šä½ï¼Œæœºå™¨äººåŠ¨ä½œç”¨äºé²æ£’å¯¼èˆªã€‚è¿™ç§æ¨¡å—åŒ–åˆ†è§£ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ¯ä¸ªé˜¶æ®µåˆ©ç”¨ä¸åŒè§„æ¨¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªåœ¨æ¨ç†ã€å®šä½å’Œå®é™…æ§åˆ¶æ–¹é¢éƒ½å¼ºå¤§çš„ç³»ç»Ÿã€‚LaViRAåœ¨VLN-CEåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é€æ˜åº¦å’Œæ•ˆç‡ï¼Œä¾¿äºå®é™…éƒ¨ç½²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¿ç»­ç¯å¢ƒä¸‹çš„é›¶æ ·æœ¬è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLN-CEï¼‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œè¦ä¹ˆä¾èµ–äºç‰¹å®šç¯å¢ƒçš„èˆªç‚¹é¢„æµ‹å™¨ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼›è¦ä¹ˆæœªèƒ½å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“çš„å†³ç­–è´¨é‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLaViRAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¯¼èˆªåŠ¨ä½œåˆ†è§£ä¸ºä¸‰ä¸ªå±‚æ¬¡ï¼šè¯­è¨€åŠ¨ä½œï¼ˆLanguage Actionï¼‰ã€è§†è§‰åŠ¨ä½œï¼ˆVision Actionï¼‰å’Œæœºå™¨äººåŠ¨ä½œï¼ˆRobot Actionï¼‰ã€‚è¿™ç§åˆ†è§£å…è®¸ç³»ç»Ÿåœ¨ä¸åŒå±‚æ¬¡ä¸Šåˆ©ç”¨ä¸åŒè§„æ¨¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¼˜åŠ¿ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å¯¼èˆªã€‚é€šè¿‡è§£è€¦é«˜å±‚è§„åˆ’ã€æ„ŸçŸ¥å®šä½å’Œåº•å±‚æ§åˆ¶ï¼ŒLaViRAèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å¤æ‚ç¯å¢ƒå’ŒæŒ‡ä»¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLaViRAçš„æ•´ä½“æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¯­è¨€åŠ¨ä½œæ¨¡å—ï¼Œè´Ÿè´£æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œé«˜å±‚è§„åˆ’ï¼Œç¡®å®šå¯¼èˆªç›®æ ‡å’Œç­–ç•¥ï¼›2) è§†è§‰åŠ¨ä½œæ¨¡å—ï¼Œè´Ÿè´£æ ¹æ®å½“å‰ç¯å¢ƒçš„è§†è§‰ä¿¡æ¯è¿›è¡Œæ„ŸçŸ¥å®šä½ï¼Œè¯†åˆ«å…³é”®åœ°æ ‡å’Œéšœç¢ç‰©ï¼›3) æœºå™¨äººåŠ¨ä½œæ¨¡å—ï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„å¯¼èˆªåŠ¨ä½œï¼Œä¾‹å¦‚å‰è¿›ã€è½¬å‘ç­‰ï¼Œå¹¶ä¿è¯å¯¼èˆªçš„é²æ£’æ€§ã€‚è¿™ä¸‰ä¸ªæ¨¡å—ååŒå·¥ä½œï¼Œå®Œæˆæ•´ä¸ªå¯¼èˆªä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šLaViRAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå…¶åŠ¨ä½œåˆ†è§£çš„å±‚æ¬¡ç»“æ„å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆåˆ©ç”¨ã€‚é€šè¿‡å°†å¯¼èˆªä»»åŠ¡åˆ†è§£ä¸ºè¯­è¨€ã€è§†è§‰å’Œæœºå™¨äººåŠ¨ä½œï¼ŒLaViRAèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨ä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå®ç°æ›´é«˜æ•ˆã€æ›´é²æ£’çš„å¯¼èˆªã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLaViRAä¸éœ€è¦é’ˆå¯¹ç‰¹å®šç¯å¢ƒè¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šLaViRAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­è¨€åŠ¨ä½œè§„åˆ’å’Œè§†è§‰åŠ¨ä½œå®šä½ï¼›2) è®¾è®¡äº†è¯­è¨€åŠ¨ä½œã€è§†è§‰åŠ¨ä½œå’Œæœºå™¨äººåŠ¨ä½œä¹‹é—´çš„æ¥å£ï¼Œä¿è¯æ¨¡å—ä¹‹é—´çš„ååŒå·¥ä½œï¼›3) é‡‡ç”¨äº†é²æ£’çš„æœºå™¨äººæ§åˆ¶ç®—æ³•ï¼Œä¿è¯å¯¼èˆªçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LaViRAåœ¨VLN-CEåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒLaViRAåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­å±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¼èˆªæˆåŠŸç‡å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaViRAçš„åŠ¨ä½œåˆ†è§£ç­–ç•¥å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆåˆ©ç”¨æ˜¯å…¶æˆåŠŸçš„å…³é”®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LaViRAçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªçš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šå®¶åº­æœåŠ¡æœºå™¨äººã€ä»“åº“ç‰©æµæœºå™¨äººã€è‡ªåŠ¨é©¾é©¶æ±½è½¦ç­‰ã€‚è¯¥ç ”ç©¶çš„å®é™…ä»·å€¼åœ¨äºæé«˜äº†æ™ºèƒ½ä½“åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œé™ä½äº†å¯¹ç¯å¢ƒçš„ä¾èµ–æ€§ï¼Œä¸ºå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚æœªæ¥ï¼ŒLaViRAæœ‰æœ›è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œç¯å¢ƒï¼Œä¾‹å¦‚ï¼šç¾éš¾æ•‘æ´ã€åŸå¸‚æ¢ç´¢ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control. LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments, while maintaining transparency and efficiency for real-world deployment.

