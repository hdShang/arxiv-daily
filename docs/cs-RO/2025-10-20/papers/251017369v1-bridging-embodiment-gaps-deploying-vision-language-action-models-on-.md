---
layout: default
title: "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots"
---

# Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17369" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17369v1</a>
  <a href="https://arxiv.org/pdf/2510.17369.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17369v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17369v1', 'Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haochen Su, Cristian Meo, Francesco Stella, Andrea Peirone, Kai Junge, Josie Hughes

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: Accepted by NeurIPS 2025 SpaVLE workshop. 4 pages, 2 figures(in main paper, excluding references and supplements)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå°†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åº”ç”¨äºè½¯æœºå™¨äººä»¥è§£å†³å®‰å…¨äº¤äº’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `è½¯æœºå™¨äºº` `äººæœºäº¤äº’` `å¾®è°ƒ` `å®‰å…¨æ€§` `é€‚åº”æ€§` `å…·èº«äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸»è¦åº”ç”¨äºä¼ ç»Ÿçš„åˆšæ€§æœºæ¢°è‡‚ï¼Œç¼ºä¹åœ¨è½¯æœºå™¨äººä¸Šçš„æœ‰æ•ˆéƒ¨ç½²ï¼Œå¯¼è‡´å®‰å…¨äº¤äº’èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„å¾®è°ƒå’Œéƒ¨ç½²æµç¨‹ï¼Œé’ˆå¯¹è½¯æœºå™¨äººè¿›è¡Œè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ä¼˜åŒ–ï¼Œä»¥å®ç°å®‰å…¨çš„äººæœºäº¤äº’ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œè½¯æœºå™¨äººåœ¨ä»£è¡¨æ€§æ“ä½œä»»åŠ¡ä¸­çš„è¡¨ç°ä¸åˆšæ€§æœºå™¨äººç›¸å½“ï¼Œå±•ç¤ºäº†å¾®è°ƒçš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æœºå™¨äººç³»ç»Ÿåœ¨ä»¥äººä¸ºä¸­å¿ƒçš„éç»“æ„åŒ–ç¯å¢ƒä¸­åº”ç”¨çš„å¢åŠ ï¼Œå®‰å…¨æ€§ã€é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹è¢«æå‡ºä½œä¸ºä¸€ç§è¯­è¨€å¼•å¯¼çš„é€šç”¨æ§åˆ¶æ¡†æ¶ï¼Œä½†å…¶åº”ç”¨ä¸»è¦é™äºä¼ ç»Ÿçš„ä¸²è”æœºæ¢°è‡‚ã€‚æœ¬æ–‡å±•ç¤ºäº†åœ¨è½¯è¿ç»­æ“çºµå™¨ä¸Šéƒ¨ç½²VLAæ¨¡å‹ï¼Œä»¥å®ç°è‡ªä¸»å®‰å…¨çš„äººæœºäº¤äº’ã€‚é€šè¿‡å¯¹ä¸¤ç§æœ€å…ˆè¿›çš„VLAæ¨¡å‹ï¼ˆOpenVLA-OFTå’Œ$Ï€_0$ï¼‰è¿›è¡Œç»“æ„åŒ–å¾®è°ƒå’Œéƒ¨ç½²è¯„ä¼°ï¼Œç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ç°æˆç­–ç•¥å› ä½“ç°ä¸åŒ¹é…è€Œå¤±è´¥ï¼Œä½†é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œè½¯æœºå™¨äººèƒ½å¤Ÿä¸åˆšæ€§æœºå™¨äººè¡¨ç°ç›¸å½“ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¾®è°ƒåœ¨å¼¥åˆä½“ç°å·®è·ä¸­çš„å¿…è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†å°†VLAæ¨¡å‹ä¸è½¯æœºå™¨äººç»“åˆçš„æ½œåŠ›ï¼Œä»¥å®ç°å®‰å…¨çµæ´»çš„å…·èº«äººå·¥æ™ºèƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è½¯æœºå™¨äººä¸Šçš„åº”ç”¨é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åˆšæ€§æœºå™¨äººä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è½¯æœºå™¨äººä¸Šå› ä½“ç°ä¸åŒ¹é…è€Œå¯¼è‡´å®‰å…¨æ€§å’Œé€‚åº”æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹VLAæ¨¡å‹è¿›è¡Œç»“æ„åŒ–å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”è½¯æœºå™¨äººçš„ç‰¹æ€§ï¼Œä»è€Œå®ç°å®‰å…¨å’Œçµæ´»çš„äººæœºäº¤äº’ã€‚è¯¥æ–¹æ³•å¼ºè°ƒäº†å¾®è°ƒçš„é‡è¦æ€§ï¼Œä»¥å…‹æœç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ¨¡å‹é€‰æ‹©ã€æ•°æ®æ”¶é›†ã€å¾®è°ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆé€‰æ‹©ä¸¤ç§VLAæ¨¡å‹ï¼Œç„¶ååœ¨è½¯æœºå™¨äººä¸Šè¿›è¡Œæ•°æ®æ”¶é›†ï¼Œæ¥ç€è¿›è¡Œé’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œæœ€åè¯„ä¼°å…¶åœ¨æ“ä½œä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œä½¿å¾—VLAæ¨¡å‹èƒ½å¤Ÿåœ¨è½¯æœºå™¨äººä¸Šå®‰å…¨è¿è¡Œï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨è½¯æœºå™¨äººåº”ç”¨ä¸­çš„ä½“ç°ä¸åŒ¹é…é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”è½¯æœºå™¨äººçš„åŠ¨æ€ç‰¹æ€§ï¼ŒåŒæ—¶ä¿æŒä¸åˆšæ€§æœºå™¨äººç›¸ä¼¼çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œè½¯æœºå™¨äººåœ¨ä»£è¡¨æ€§æ“ä½œä»»åŠ¡ä¸­çš„è¡¨ç°ä¸åˆšæ€§æœºå™¨äººç›¸å½“ï¼ŒæˆåŠŸå®ç°äº†å®‰å…¨çš„äººæœºäº¤äº’ã€‚è¿™ä¸€æˆæœè¡¨æ˜ï¼Œå¾®è°ƒå¯¹äºå¼¥åˆä½“ç°å·®è·è‡³å…³é‡è¦ï¼Œä¸”è½¯æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ»ç–—æœºå™¨äººã€æœåŠ¡æœºå™¨äººå’Œäººæœºåä½œç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­å®ç°å®‰å…¨çš„äº¤äº’ã€‚æœªæ¥ï¼Œéšç€è½¯æœºå™¨äººæŠ€æœ¯çš„å‘å±•ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæå‡äººæœºåä½œçš„å®‰å…¨æ€§å’Œçµæ´»æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic systems are increasingly expected to operate in human-centered, unstructured environments where safety, adaptability, and generalization are essential. Vision-Language-Action (VLA) models have been proposed as a language guided generalized control framework for real robots. However, their deployment has been limited to conventional serial link manipulators. Coupled by their rigidity and unpredictability of learning based control, the ability to safely interact with the environment is missing yet critical. In this work, we present the deployment of a VLA model on a soft continuum manipulator to demonstrate autonomous safe human-robot interaction. We present a structured finetuning and deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and $Ï€_0$) across representative manipulation tasks, and show while out-of-the-box policies fail due to embodiment mismatch, through targeted finetuning the soft robot performs equally to the rigid counterpart. Our findings highlight the necessity of finetuning for bridging embodiment gaps, and demonstrate that coupling VLA models with soft robots enables safe and flexible embodied AI in human-shared environments.

