---
layout: default
title: R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations
---

# R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.18085" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.18085v1</a>
  <a href="https://arxiv.org/pdf/2510.18085.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.18085v1" onclick="toggleFavorite(this, '2510.18085v1', 'R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Connor Mattson, Varun Raveendra, Ellen Novoseller, Nicholas Waytowich, Vernon J. Lawhern, Daniel S. Brown

**åˆ†ç±»**: cs.RO, cs.AI, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: 9 pages, 6 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**R2BCï¼šä»å•æ™ºèƒ½ä½“æ¼”ç¤ºä¸­å­¦ä¹ å¤šæ™ºèƒ½ä½“åä½œç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ ` `è¡Œä¸ºå…‹éš†` `æœºå™¨äººåä½œ` `å•æ™ºèƒ½ä½“æ¼”ç¤º` `å¾ªç¯è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨äººç±»éš¾ä»¥æä¾›è”åˆåŠ¨ä½œç©ºé—´æ¼”ç¤ºçš„æƒ…å†µä¸‹ã€‚
2. R2BCé€šè¿‡å¾ªç¯æ–¹å¼ï¼Œè®©äººç±»ä¾æ¬¡æ¼”ç¤ºå•ä¸ªæ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œä»è€Œç®€åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®­ç»ƒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒR2BCåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šäº†éœ€è¦åŒæ­¥æ¼”ç¤ºçš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ (IL)æ˜¯äººç±»è®­ç»ƒæœºå™¨äººçš„ä¸€ç§è‡ªç„¶æ–¹å¼ï¼Œå°¤å…¶æ˜¯åœ¨å®¹æ˜“è·å¾—é«˜è´¨é‡æ¼”ç¤ºçš„æƒ…å†µä¸‹ã€‚è™½ç„¶ILå·²è¢«å¹¿æ³›åº”ç”¨äºå•æœºå™¨äººè®¾ç½®ï¼Œä½†ç›¸å¯¹è¾ƒå°‘çš„ç ”ç©¶æ¶‰åŠå°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨å•ä¸ªäººç±»å¿…é¡»ä¸ºåä½œæœºå™¨äººå›¢é˜Ÿæä¾›æ¼”ç¤ºçš„åœºæ™¯ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»å¹¶ç ”ç©¶äº†å¾ªç¯è¡Œä¸ºå…‹éš†(R2BC)ï¼Œè¿™æ˜¯ä¸€ç§ä½¿å•ä¸ªäººç±»æ“ä½œå‘˜èƒ½å¤Ÿé€šè¿‡é¡ºåºçš„å•æ™ºèƒ½ä½“æ¼”ç¤ºæœ‰æ•ˆåœ°è®­ç»ƒå¤šæœºå™¨äººç³»ç»Ÿçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸äººç±»ä¸€æ¬¡é¥æ§ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œå¹¶é€æ­¥åœ°å°†å¤šæ™ºèƒ½ä½“è¡Œä¸ºæ•™ç»™æ•´ä¸ªç³»ç»Ÿï¼Œè€Œæ— éœ€åœ¨è”åˆå¤šæ™ºèƒ½ä½“åŠ¨ä½œç©ºé—´ä¸­è¿›è¡Œæ¼”ç¤ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒR2BCæ–¹æ³•åœ¨å››ä¸ªå¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿä»»åŠ¡ä¸­åŒ¹é…ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†åœ¨ç‰¹æƒåŒæ­¥æ¼”ç¤ºä¸Šè®­ç»ƒçš„oracleè¡Œä¸ºå…‹éš†æ–¹æ³•çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å°†R2BCéƒ¨ç½²åœ¨ä¸¤ä¸ªä½¿ç”¨çœŸå®äººç±»æ¼”ç¤ºè®­ç»ƒçš„ç‰©ç†æœºå™¨äººä»»åŠ¡ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦äººç±»æä¾›æ‰€æœ‰æ™ºèƒ½ä½“åŒæ­¥çš„è”åˆåŠ¨ä½œæ¼”ç¤ºï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€éš¾ä»¥å®ç°ï¼Œç‰¹åˆ«æ˜¯å½“æ™ºèƒ½ä½“æ•°é‡è¾ƒå¤šæˆ–ä»»åŠ¡å¤æ‚æ—¶ã€‚æ­¤å¤–ï¼Œç›´æ¥å°†å•æ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ æ–¹æ³•æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¿½ç•¥äº†æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œå…³ç³»ï¼Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜çš„ç­–ç•¥ã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨å•æ™ºèƒ½ä½“æ¼”ç¤ºæœ‰æ•ˆåœ°è®­ç»ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šR2BCçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¾ªç¯çš„æ–¹å¼ï¼Œè®©äººç±»ä¾æ¬¡æ§åˆ¶å’Œæ¼”ç¤ºæ¯ä¸ªæ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚å…·ä½“æ¥è¯´ï¼Œäººç±»æ“ä½œå‘˜é¦–å…ˆæ§åˆ¶ä¸€ä¸ªæ™ºèƒ½ä½“å®Œæˆä»»åŠ¡çš„ä¸€éƒ¨åˆ†ï¼Œç„¶ååˆ‡æ¢åˆ°å¦ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°æ‰€æœ‰æ™ºèƒ½ä½“éƒ½å‚ä¸åˆ°ä»»åŠ¡ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒR2BCå°†å¤æ‚çš„å¤šæ™ºèƒ½ä½“æ¼”ç¤ºåˆ†è§£ä¸ºä¸€ç³»åˆ—ç®€å•çš„å•æ™ºèƒ½ä½“æ¼”ç¤ºï¼Œä»è€Œé™ä½äº†äººç±»æ“ä½œå‘˜çš„è´Ÿæ‹…ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR2BCçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1) äººç±»æ“ä½œå‘˜é€‰æ‹©ä¸€ä¸ªæ™ºèƒ½ä½“è¿›è¡Œé¥æ§ï¼›2) äººç±»æ“ä½œå‘˜æ§åˆ¶è¯¥æ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œï¼Œå¹¶è®°å½•æ™ºèƒ½ä½“çš„çŠ¶æ€å’ŒåŠ¨ä½œï¼›3) å°†è®°å½•çš„çŠ¶æ€-åŠ¨ä½œå¯¹ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨è¡Œä¸ºå…‹éš†ç®—æ³•è®­ç»ƒè¯¥æ™ºèƒ½ä½“çš„ç­–ç•¥ï¼›4) é‡å¤æ­¥éª¤1-3ï¼Œç›´åˆ°æ‰€æœ‰æ™ºèƒ½ä½“çš„ç­–ç•¥éƒ½å¾—åˆ°è®­ç»ƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥éƒ½æ˜¯ç‹¬ç«‹è®­ç»ƒçš„ï¼Œä½†ç”±äºäººç±»æ“ä½œå‘˜åœ¨å¾ªç¯æ¼”ç¤ºä¸­è€ƒè™‘äº†æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œå…³ç³»ï¼Œå› æ­¤æœ€ç»ˆè®­ç»ƒå¾—åˆ°çš„ç­–ç•¥èƒ½å¤Ÿå®ç°å¤šæ™ºèƒ½ä½“åä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šR2BCçš„å…³é”®åˆ›æ–°åœ¨äºå®ƒæå‡ºäº†ä¸€ç§æ–°çš„å¤šæ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ èŒƒå¼ï¼Œå³é€šè¿‡å¾ªç¯å•æ™ºèƒ½ä½“æ¼”ç¤ºæ¥è®­ç»ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦è”åˆåŠ¨ä½œç©ºé—´æ¼”ç¤ºçš„æ–¹æ³•ç›¸æ¯”ï¼ŒR2BCå¤§å¤§é™ä½äº†äººç±»æ“ä½œå‘˜çš„è´Ÿæ‹…ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å®¹æ˜“åœ°è®­ç»ƒå¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒR2BCè¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å•æ™ºèƒ½ä½“æ¼”ç¤ºä¸­çš„ä¿¡æ¯ï¼Œå­¦ä¹ åˆ°æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œå…³ç³»ã€‚

**å…³é”®è®¾è®¡**ï¼šR2BCçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¾ªç¯æ¼”ç¤ºçš„é¡ºåºï¼šå¯ä»¥éšæœºé€‰æ‹©æ™ºèƒ½ä½“çš„æ¼”ç¤ºé¡ºåºï¼Œä¹Ÿå¯ä»¥æ ¹æ®ä»»åŠ¡çš„ç‰¹ç‚¹é€‰æ‹©ç‰¹å®šçš„é¡ºåºï¼›2) è¡Œä¸ºå…‹éš†ç®—æ³•çš„é€‰æ‹©ï¼šå¯ä»¥ä½¿ç”¨ä»»ä½•æ ‡å‡†çš„è¡Œä¸ºå…‹éš†ç®—æ³•ï¼Œå¦‚ç›‘ç£å­¦ä¹ æˆ–Daggerï¼›3) ç­–ç•¥çš„è¡¨ç¤ºï¼šå¯ä»¥ä½¿ç”¨ä»»ä½•æ ‡å‡†çš„ç­–ç•¥è¡¨ç¤ºæ–¹æ³•ï¼Œå¦‚ç¥ç»ç½‘ç»œæˆ–çº¿æ€§æ¨¡å‹ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†ç®€å•çš„ç¥ç»ç½‘ç»œä½œä¸ºç­–ç•¥çš„è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ç›‘ç£å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒR2BCåœ¨å››ä¸ªå¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤ŸåŒ¹é…ç”šè‡³è¶…è¿‡åœ¨ç‰¹æƒåŒæ­¥æ¼”ç¤ºä¸Šè®­ç»ƒçš„oracleè¡Œä¸ºå…‹éš†æ–¹æ³•çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨äº¤é€šæ§åˆ¶ä»»åŠ¡ä¸­ï¼ŒR2BCèƒ½å¤Ÿæœ‰æ•ˆåœ°æ§åˆ¶å¤šä¸ªè½¦è¾†ï¼Œé¿å…äº¤é€šæ‹¥å µã€‚æ­¤å¤–ï¼ŒR2BCè¿˜åœ¨ä¸¤ä¸ªç‰©ç†æœºå™¨äººä»»åŠ¡ä¸­æˆåŠŸéƒ¨ç½²ï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„å¯è¡Œæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒR2BCæ˜¯ä¸€ç§æœ‰æ•ˆçš„å¤šæ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R2BCæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨ä»“åº“æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ— äººæœºç¼–é˜Ÿç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ç”¨äºè®­ç»ƒå¤šä¸ªæœºå™¨äººååŒå®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€äººç±»æä¾›å¤æ‚çš„è”åˆåŠ¨ä½œæ¼”ç¤ºã€‚æ­¤å¤–ï¼ŒR2BCè¿˜å¯ä»¥åº”ç”¨äºäººæœºåä½œåœºæ™¯ï¼Œè®©äººç±»æ“ä½œå‘˜èƒ½å¤Ÿæ›´å®¹æ˜“åœ°æŒ‡å¯¼æœºå™¨äººå®Œæˆä»»åŠ¡ã€‚æœªæ¥ï¼ŒR2BCæœ‰æœ›æˆä¸ºä¸€ç§é€šç”¨çš„å¤šæ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œæ¨åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation Learning (IL) is a natural way for humans to teach robots, particularly when high-quality demonstrations are easy to obtain. While IL has been widely applied to single-robot settings, relatively few studies have addressed the extension of these methods to multi-agent systems, especially in settings where a single human must provide demonstrations to a team of collaborating robots. In this paper, we introduce and study Round-Robin Behavior Cloning (R2BC), a method that enables a single human operator to effectively train multi-robot systems through sequential, single-agent demonstrations. Our approach allows the human to teleoperate one agent at a time and incrementally teach multi-agent behavior to the entire system, without requiring demonstrations in the joint multi-agent action space. We show that R2BC methods match, and in some cases surpass, the performance of an oracle behavior cloning approach trained on privileged synchronized demonstrations across four multi-agent simulated tasks. Finally, we deploy R2BC on two physical robot tasks trained using real human demonstrations.

