---
layout: default
title: Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning
---

# Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.17143" target="_blank" class="toolbar-btn">arXiv: 2510.17143v1</a>
    <a href="https://arxiv.org/pdf/2510.17143.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17143v1" 
            onclick="toggleFavorite(this, '2510.17143v1', 'Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shantnav Agarwal, Javier Alonso-Mora, Sihao Sun

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-20

**Â§áÊ≥®**: Accepted by IEEE MRS 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÊ®°‰ªøÂ≠¶‰π†ÁöÑÊó†‰∫∫Êú∫ÂçèÂêåÊìç‰ΩúÂàÜÊï£ÂºèÂÆûÊó∂ËßÑÂàíÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊó†‰∫∫Êú∫ÂçèÂêå` `Ê®°‰ªøÂ≠¶‰π†` `ÂàÜÊï£ÂºèËßÑÂàí` `Áâ©ÁêÜ‰ø°ÊÅØÁ•ûÁªèÁΩëÁªú` `ËøêÂä®ËßÑÂàí`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Â§öÊó†‰∫∫Êú∫ÂçèÂêåÊìç‰Ωú‰∏≠‰æùËµñÈõÜ‰∏≠ÊéßÂà∂ÊàñÂèØÈù†ÈÄö‰ø°ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÊ®°‰ªøÂ≠¶‰π†ÁöÑÂàÜÊï£ÂºèËßÑÂàíÊñπÊ≥ïÔºåÊó†ÈúÄÊô∫ËÉΩ‰ΩìÈó¥ÈÄö‰ø°ÔºåÊèêÂçá‰∫ÜÁ≥ªÁªüÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰ªøÁúüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÂùáË°®Áé∞ËâØÂ•ΩÔºåÊÄßËÉΩÂèØ‰∏éÈõÜ‰∏≠ÂºèÊñπÊ≥ïÂ™≤ÁæéÔºå‰∏îËÆ≠ÁªÉÊïàÁéáÈ´ò„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑÂ§öÊó†‰∫∫Êú∫ÂçèÂêåËøêËæìÊÇ¨ÊåÇË¥üËΩΩÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÈõÜ‰∏≠ÂºèÊéßÂà∂Êû∂ÊûÑÊàñÂèØÈù†ÁöÑÊô∫ËÉΩ‰ΩìÈó¥ÈÄö‰ø°„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊú∫Âô®Â≠¶‰π†ÁöÑÊñ∞ÂûãÂàÜÊï£ÂºèËøêÂä®ËßÑÂàíÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂú®ÈÉ®ÂàÜÂèØËßÇÊµãÊÄßÂíåÊó†Êô∫ËÉΩ‰ΩìÈó¥ÈÄö‰ø°ÁöÑÊÉÖÂÜµ‰∏ãÊúâÊïàËøêË°å„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®Ê®°‰ªøÂ≠¶‰π†ËÆ≠ÁªÉÊØè‰∏™Êó†‰∫∫Êú∫ÁöÑÂàÜÊï£ÂºèÂ≠¶ÁîüÁ≠ñÁï•ÔºåÈÄöËøáÊ®°‰ªøÂÖ∑ÊúâÂÖ®Â±ÄËßÇÊµãÊùÉÈôêÁöÑÈõÜ‰∏≠ÂºèËøêÂä®ËßÑÂàíÂô®„ÄÇÂ≠¶ÁîüÁ≠ñÁï•‰ΩøÁî®Áâ©ÁêÜ‰ø°ÊÅØÁ•ûÁªèÁΩëÁªúÁîüÊàêÂπ≥ÊªëËΩ®ËøπÔºå‰øùËØÅËøêÂä®‰∏≠ÁöÑÂØºÊï∞ÂÖ≥Á≥ª„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂ≠¶ÁîüÁ≠ñÁï•Âà©Áî®ÊïôÂ∏àÁ≠ñÁï•ÁîüÊàêÁöÑÂÆåÊï¥ËΩ®ËøπÔºå‰ªéËÄåÊèêÈ´òÊ†∑Êú¨ÊïàÁéá„ÄÇÊ≠§Â§ñÔºåÊØè‰∏™Â≠¶ÁîüÁ≠ñÁï•ÈÉΩÂèØ‰ª•Âú®Ê†áÂáÜÁ¨îËÆ∞Êú¨ÁîµËÑë‰∏äÂú®‰∏§Â∞èÊó∂ÂÜÖÂÆåÊàêËÆ≠ÁªÉ„ÄÇÊàë‰ª¨Âú®‰ªøÁúüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠È™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÔºå‰ΩøÂÖ∂ËÉΩÂ§üË∑üÈöèÁÅµÊ¥ªÁöÑÂèÇËÄÉËΩ®ËøπÔºåÂ±ïÁ§∫Âá∫‰∏éÈõÜ‰∏≠ÂºèÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂ§öÊó†‰∫∫Êú∫ÂçèÂêåÊìç‰ΩúÊÇ¨ÊåÇË¥üËΩΩÔºåÈúÄË¶ÅÁ≤æÁ°ÆÁöÑËøêÂä®ËßÑÂàí„ÄÇÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÈõÜ‰∏≠ÂºèÊéßÂà∂ÔºåËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºå‰∏îÂØπÈÄö‰ø°Ë¶ÅÊ±ÇÈ´òÔºåÂú®ÈÄö‰ø°ÂèóÈôêÊàñÁéØÂ¢ÉÂ§çÊùÇÁöÑÂú∫ÊôØ‰∏ãÈöæ‰ª•Â∫îÁî®„ÄÇÂàÜÊï£ÂºèÊéßÂà∂ËôΩÁÑ∂ÂèØ‰ª•Èôç‰ΩéËÆ°ÁÆóË¥üÊãÖÔºå‰ΩÜÈöæ‰ª•‰øùËØÅÂÖ®Â±ÄÂçèÂêåÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂà©Áî®Ê®°‰ªøÂ≠¶‰π†ÔºåËÆ≠ÁªÉÊØè‰∏™Êó†‰∫∫Êú∫ÁöÑÂàÜÊï£ÂºèÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊ®°‰ªøÈõÜ‰∏≠ÂºèËßÑÂàíÂô®ÁöÑË°å‰∏∫„ÄÇÈõÜ‰∏≠ÂºèËßÑÂàíÂô®Êã•ÊúâÂÖ®Â±Ä‰ø°ÊÅØÔºåÂèØ‰ª•ÁîüÊàêÈ´òË¥®ÈáèÁöÑËΩ®ËøπÔºå‰ΩÜÊó†Ê≥ïÁõ¥Êé•Â∫îÁî®‰∫éÂàÜÊï£ÂºèÂú∫ÊôØ„ÄÇÈÄöËøáÊ®°‰ªøÂ≠¶‰π†ÔºåÂ∞ÜÈõÜ‰∏≠ÂºèËßÑÂàíÂô®ÁöÑÁü•ËØÜËøÅÁßªÂà∞ÂàÜÊï£ÂºèÁ≠ñÁï•‰∏≠ÔºåÂÆûÁé∞È´òÊïàÁöÑÂçèÂêåÊìç‰Ωú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™Èò∂ÊÆµÔºöÊïôÂ∏àÁ≠ñÁï•ÁîüÊàêÂíåÂ≠¶ÁîüÁ≠ñÁï•ËÆ≠ÁªÉ„ÄÇÈ¶ñÂÖàÔºå‰ΩøÁî®ÈõÜ‰∏≠ÂºèËøêÂä®ËßÑÂàíÂô®ÔºàÊïôÂ∏àÁ≠ñÁï•ÔºâÁîüÊàêÂ§öÊó†‰∫∫Êú∫ÂçèÂêåÊìç‰ΩúÁöÑËΩ®ËøπÊï∞ÊçÆ„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®Ëøô‰∫õÊï∞ÊçÆËÆ≠ÁªÉÊØè‰∏™Êó†‰∫∫Êú∫ÁöÑÂàÜÊï£ÂºèÁ≠ñÁï•ÔºàÂ≠¶ÁîüÁ≠ñÁï•Ôºâ„ÄÇÂ≠¶ÁîüÁ≠ñÁï•ÊòØ‰∏Ä‰∏™Áâ©ÁêÜ‰ø°ÊÅØÁ•ûÁªèÁΩëÁªúÔºåËÉΩÂ§üÁîüÊàêÊª°Ë∂≥ËøêÂä®Â≠¶Á∫¶ÊùüÁöÑÂπ≥ÊªëËΩ®Ëøπ„ÄÇÂú®Êé®ÁêÜÈò∂ÊÆµÔºåÊØè‰∏™Êó†‰∫∫Êú∫Áã¨Á´ãËøêË°åÂÖ∂Â≠¶ÁîüÁ≠ñÁï•ÔºåÊó†ÈúÄÊô∫ËÉΩ‰ΩìÈó¥ÈÄö‰ø°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂà©Áî®Ê®°‰ªøÂ≠¶‰π†ÔºåÂ∞ÜÈõÜ‰∏≠ÂºèËßÑÂàíÂô®ÁöÑÂÖ®Â±ÄÁü•ËØÜËøÅÁßªÂà∞ÂàÜÊï£ÂºèÁ≠ñÁï•‰∏≠Ôºå‰ªéËÄåÂú®Êó†ÈúÄÊô∫ËÉΩ‰ΩìÈó¥ÈÄö‰ø°ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞È´òÊïàÁöÑÂçèÂêåÊìç‰Ωú„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®Áâ©ÁêÜ‰ø°ÊÅØÁ•ûÁªèÁΩëÁªú‰Ωú‰∏∫Â≠¶ÁîüÁ≠ñÁï•ÔºåËÉΩÂ§ü‰øùËØÅÁîüÊàêËΩ®ËøπÁöÑÂπ≥ÊªëÊÄßÂíåÁâ©ÁêÜÂèØË°åÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÈõÜ‰∏≠ÂºèËßÑÂàíÂô®ÈááÁî® kinodynamic ËøêÂä®ËßÑÂàíÁÆóÊ≥ïÔºåËÄÉËôë‰∫ÜÊó†‰∫∫Êú∫ÁöÑÂä®ÂäõÂ≠¶Á∫¶Êùü„ÄÇÂ≠¶ÁîüÁ≠ñÁï•ÈááÁî®Áâ©ÁêÜ‰ø°ÊÅØÁ•ûÁªèÁΩëÁªúÔºåÂÖ∂ÊçüÂ§±ÂáΩÊï∞ÂåÖÂê´‰∏§ÈÉ®ÂàÜÔºöËΩ®ËøπÊ®°‰ªøÊçüÂ§±ÂíåÁâ©ÁêÜÁ∫¶ÊùüÊçüÂ§±„ÄÇËΩ®ËøπÊ®°‰ªøÊçüÂ§±Áî®‰∫éË°°ÈáèÂ≠¶ÁîüÁ≠ñÁï•ÁîüÊàêÁöÑËΩ®Ëøπ‰∏éÊïôÂ∏àÁ≠ñÁï•ÁîüÊàêÁöÑËΩ®Ëøπ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÁâ©ÁêÜÁ∫¶ÊùüÊçüÂ§±Áî®‰∫é‰øùËØÅÁîüÊàêËΩ®ËøπÊª°Ë∂≥Êó†‰∫∫Êú∫ÁöÑËøêÂä®Â≠¶Á∫¶Êùü„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®ÊïôÂ∏àÁ≠ñÁï•ÁîüÊàêÁöÑÂÆåÊï¥ËΩ®ËøπÔºåÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÊñπÊ≥ïÂú®‰ªøÁúüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°å‰∫ÜÈ™åËØÅÔºåÁªìÊûúË°®ÊòéÂÖ∂ÊÄßËÉΩÂèØ‰∏éÈõÜ‰∏≠ÂºèÊñπÊ≥ïÂ™≤Áæé„ÄÇÂú®Ë∑üÈöèÁÅµÊ¥ªÂèÇËÄÉËΩ®ËøπÁöÑ‰ªªÂä°‰∏≠ÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞Á≤æÁ°ÆÁöÑÂçèÂêåÊìç‰ΩúÔºå‰∏îÊó†ÈúÄÊô∫ËÉΩ‰ΩìÈó¥ÈÄö‰ø°„ÄÇÊ≠§Â§ñÔºåÊØè‰∏™Â≠¶ÁîüÁ≠ñÁï•ÈÉΩÂèØ‰ª•Âú®Ê†áÂáÜÁ¨îËÆ∞Êú¨ÁîµËÑë‰∏äÂú®‰∏§Â∞èÊó∂ÂÜÖÂÆåÊàêËÆ≠ÁªÉÔºåË°®ÊòéËØ•ÊñπÊ≥ïÂÖ∑ÊúâËæÉÈ´òÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ§öÁßçÂ§öÊó†‰∫∫Êú∫ÂçèÂêåÊìç‰ΩúÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöÂ§ßÂûãÁâ©‰ΩìÁöÑÊê¨Ëøê„ÄÅÊ°•Ê¢ÅÊ£ÄÊµã„ÄÅÁÅæÈöæÊïëÊè¥Á≠â„ÄÇÂú®Ëøô‰∫õÂú∫ÊôØ‰∏≠ÔºåÊó†‰∫∫Êú∫ÈúÄË¶ÅÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂçèÂêåÂ∑•‰ΩúÔºå‰∏îÈÄö‰ø°Êù°‰ª∂ÂèØËÉΩÂèóÈôê„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊèêÈ´òÊó†‰∫∫Êú∫ÂçèÂêåÊìç‰ΩúÁöÑÊïàÁéáÂíåÈ≤ÅÊ£íÊÄßÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Existing approaches for transporting and manipulating cable-suspended loads using multiple UAVs along reference trajectories typically rely on either centralized control architectures or reliable inter-agent communication. In this work, we propose a novel machine learning based method for decentralized kinodynamic planning that operates effectively under partial observability and without inter-agent communication. Our method leverages imitation learning to train a decentralized student policy for each UAV by imitating a centralized kinodynamic motion planner with access to privileged global observations. The student policy generates smooth trajectories using physics-informed neural networks that respect the derivative relationships in motion. During training, the student policies utilize the full trajectory generated by the teacher policy, leading to improved sample efficiency. Moreover, each student policy can be trained in under two hours on a standard laptop. We validate our method in both simulation and real-world environments to follow an agile reference trajectory, demonstrating performance comparable to that of centralized approaches.

