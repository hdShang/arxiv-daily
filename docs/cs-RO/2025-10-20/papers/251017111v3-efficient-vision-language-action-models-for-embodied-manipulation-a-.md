---
layout: default
title: "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey"
---

# Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17111" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17111v3</a>
  <a href="https://arxiv.org/pdf/2510.17111.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17111v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17111v3', 'Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weifan Guan, Qinghao Hu, Aosheng Li, Jian Cheng

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20 (æ›´æ–°: 2025-10-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°é«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œè§£å†³å…·èº«æ“ä½œä¸­è®¡ç®—èµ„æºå—é™é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å…·èº«æ™ºèƒ½` `æœºå™¨äººæ§åˆ¶` `æ¨¡å‹æ•ˆç‡` `è¾¹ç¼˜è®¡ç®—`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹è®¡ç®—å’Œå†…å­˜éœ€æ±‚å·¨å¤§ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶æ§åˆ¶ã€‚
2. è¯¥ç»¼è¿°ç³»ç»Ÿæ€§åœ°åˆ†æäº†VLAæ¨¡å‹æ•ˆç‡æå‡æ–¹æ³•ï¼Œä»æ¨¡å‹æ¶æ„ã€æ„ŸçŸ¥ç‰¹å¾ã€åŠ¨ä½œç”Ÿæˆå’Œè®­ç»ƒ/æ¨ç†ç­–ç•¥å››ä¸ªç»´åº¦è¿›è¡Œåˆ†ç±»ã€‚
3. æ€»ç»“äº†å„ç±»æ•ˆç‡ä¼˜åŒ–æŠ€æœ¯çš„ä»£è¡¨æ€§æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº†æœªæ¥å‘å±•è¶‹åŠ¿å’ŒæŒ‘æˆ˜ï¼Œä¸ºé«˜æ•ˆå…·èº«æ™ºèƒ½ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹é€šè¿‡å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è§‚å¯Ÿæ˜ å°„åˆ°æœºå™¨äººåŠ¨ä½œï¼Œæ‰©å±•äº†è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å…·èº«æ§åˆ¶æ–¹é¢çš„åº”ç”¨ã€‚å°½ç®¡VLAç³»ç»Ÿå…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç”±äºå…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œå®ƒä»¬é¢ä¸´ç€ä¸¥å³»çš„æŒ‘æˆ˜ï¼Œè¿™ä¸è¾¹ç¼˜å¹³å°ï¼ˆå¦‚è½¦è½½ç§»åŠ¨æœºæ¢°è‡‚ï¼‰å¯¹å®æ—¶æ€§èƒ½çš„è¦æ±‚ç›¸å†²çªã€‚è§£å†³è¿™ç§çŸ›ç›¾å·²æˆä¸ºè¿‘æœŸç ”ç©¶çš„ä¸­å¿ƒã€‚é‰´äºåœ¨æ›´é«˜æ•ˆå’Œå¯æ‰©å±•çš„VLAç³»ç»Ÿæ–¹é¢æ—¥ç›Šå¢é•¿çš„åŠªåŠ›ï¼Œæœ¬ç»¼è¿°ç³»ç»Ÿåœ°å›é¡¾äº†æé«˜VLAæ•ˆç‡çš„æ–¹æ³•ï¼Œé‡ç‚¹æ˜¯å‡å°‘å»¶è¿Ÿã€å†…å­˜å ç”¨ä»¥åŠè®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚æˆ‘ä»¬å°†ç°æœ‰è§£å†³æ–¹æ¡ˆåˆ†ä¸ºå››ä¸ªç»´åº¦ï¼šæ¨¡å‹æ¶æ„ã€æ„ŸçŸ¥ç‰¹å¾ã€åŠ¨ä½œç”Ÿæˆå’Œè®­ç»ƒ/æ¨ç†ç­–ç•¥ï¼Œæ€»ç»“äº†æ¯ä¸ªç±»åˆ«ä¸­çš„ä»£è¡¨æ€§æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æœªæ¥çš„è¶‹åŠ¿å’Œå¼€æ”¾çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†æ¨è¿›é«˜æ•ˆå…·èº«æ™ºèƒ½çš„æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVLAæ¨¡å‹æ—¨åœ¨ä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥æ‰§è¡Œæ“ä½œä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰VLAæ¨¡å‹é€šå¸¸å‚æ•°é‡å·¨å¤§ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥éƒ¨ç½²åœ¨è®¡ç®—èµ„æºæœ‰é™çš„ç§»åŠ¨æœºå™¨äººå¹³å°ä¸Šï¼Œæ— æ³•æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚å› æ­¤ï¼Œå¦‚ä½•é™ä½VLAæ¨¡å‹çš„è®¡ç®—æˆæœ¬ã€å†…å­˜å ç”¨å’Œå»¶è¿Ÿï¼Œæ˜¯å½“å‰ç ”ç©¶é¢ä¸´çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥ç»¼è¿°çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç°æœ‰VLAæ•ˆç‡æå‡æ–¹æ³•è¿›è¡Œç³»ç»Ÿæ€§åœ°åˆ†ç±»å’Œæ€»ç»“ï¼Œä»è€Œä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªå…¨é¢çš„è§†è§’ï¼Œäº†è§£ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚é€šè¿‡å¯¹ä¸åŒç»´åº¦çš„æŠ€æœ¯è¿›è¡Œåˆ†æï¼Œå¯ä»¥æ›´å¥½åœ°æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¾‹å¦‚ï¼Œå¦‚ä½•è®¾è®¡æ›´è½»é‡çº§çš„æ¨¡å‹æ¶æ„ï¼Œå¦‚ä½•æå–æ›´æœ‰æ•ˆçš„æ„ŸçŸ¥ç‰¹å¾ï¼Œå¦‚ä½•ä¼˜åŒ–åŠ¨ä½œç”Ÿæˆç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•é‡‡ç”¨æ›´é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†æ–¹æ³•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç»¼è¿°å°†VLAæ•ˆç‡æå‡æ–¹æ³•åˆ†ä¸ºå››ä¸ªä¸»è¦ç»´åº¦ï¼š
1. **æ¨¡å‹æ¶æ„**ï¼šç ”ç©¶å¦‚ä½•è®¾è®¡æ›´ç´§å‡‘ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ç»“æ„ï¼Œä¾‹å¦‚ä½¿ç”¨è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œã€Transformerå˜ä½“ç­‰ã€‚
2. **æ„ŸçŸ¥ç‰¹å¾**ï¼šç ”ç©¶å¦‚ä½•æå–æ›´å…·ä»£è¡¨æ€§ã€æ›´ä½ç»´åº¦çš„è§†è§‰ç‰¹å¾ï¼Œä¾‹å¦‚ä½¿ç”¨ç‰¹å¾è’¸é¦ã€çŸ¥è¯†å›¾è°±ç­‰ã€‚
3. **åŠ¨ä½œç”Ÿæˆ**ï¼šç ”ç©¶å¦‚ä½•ä¼˜åŒ–åŠ¨ä½œç”Ÿæˆç­–ç•¥ï¼Œä¾‹å¦‚ä½¿ç”¨åˆ†å±‚åŠ¨ä½œç©ºé—´ã€æ¨¡ä»¿å­¦ä¹ ç­‰ã€‚
4. **è®­ç»ƒ/æ¨ç†ç­–ç•¥**ï¼šç ”ç©¶å¦‚ä½•é‡‡ç”¨æ›´é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨é‡åŒ–ã€å‰ªæã€çŸ¥è¯†è’¸é¦ç­‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥ç»¼è¿°çš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç³»ç»Ÿæ€§å’Œå…¨é¢æ€§ã€‚å®ƒä¸ä»…å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»å’Œæ€»ç»“ï¼Œè¿˜æ·±å…¥åˆ†æäº†ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜æ¢è®¨äº†æœªæ¥çš„å‘å±•è¶‹åŠ¿å’ŒæŒ‘æˆ˜ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚ä¸ä»¥å¾€çš„ç»¼è¿°ç›¸æ¯”ï¼Œè¯¥ç»¼è¿°æ›´åŠ å…³æ³¨VLAæ¨¡å‹çš„æ•ˆç‡é—®é¢˜ï¼Œå¹¶ä»å¤šä¸ªç»´åº¦æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è®¾è®¡**ï¼šè¯¥ç»¼è¿°å¹¶æ²¡æœ‰æå‡ºæ–°çš„æŠ€æœ¯è®¾è®¡ï¼Œè€Œæ˜¯å¯¹ç°æœ‰æŠ€æœ¯è¿›è¡Œäº†æ¢³ç†å’Œæ€»ç»“ã€‚åœ¨æ¨¡å‹æ¶æ„æ–¹é¢ï¼Œå…³æ³¨è½»é‡çº§ç½‘ç»œç»“æ„çš„è®¾è®¡ï¼›åœ¨æ„ŸçŸ¥ç‰¹å¾æ–¹é¢ï¼Œå…³æ³¨ç‰¹å¾æå–çš„æ•ˆç‡å’Œä¿¡æ¯é‡ï¼›åœ¨åŠ¨ä½œç”Ÿæˆæ–¹é¢ï¼Œå…³æ³¨åŠ¨ä½œç©ºé—´çš„è¡¨ç¤ºå’Œæ¢ç´¢ï¼›åœ¨è®­ç»ƒ/æ¨ç†ç­–ç•¥æ–¹é¢ï¼Œå…³æ³¨æ¨¡å‹å‹ç¼©å’ŒåŠ é€ŸæŠ€æœ¯ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚éœ€è¦å‚è€ƒåŸå§‹è®ºæ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°å…¨é¢æ€»ç»“äº†VLAæ¨¡å‹æ•ˆç‡æå‡çš„ç°æœ‰æ–¹æ³•ï¼Œå¹¶å°†å…¶åˆ†ä¸ºæ¨¡å‹æ¶æ„ã€æ„ŸçŸ¥ç‰¹å¾ã€åŠ¨ä½œç”Ÿæˆå’Œè®­ç»ƒ/æ¨ç†ç­–ç•¥å››ä¸ªç»´åº¦ã€‚é€šè¿‡å¯¹æ¯ä¸ªç»´åº¦ä¸­çš„ä»£è¡¨æ€§æŠ€æœ¯è¿›è¡Œåˆ†æï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„è·¯çº¿å›¾ï¼Œäº†è§£å¦‚ä½•æé«˜VLAæ¨¡å‹çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ï¼Œä¾‹å¦‚å¦‚ä½•è®¾è®¡æ›´é«˜æ•ˆçš„ç«¯åˆ°ç«¯VLAæ¨¡å‹ï¼Œå¦‚ä½•åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ æ¥å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ç­‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æœºå™¨äººè¿›è¡Œå…·èº«æ“ä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜VLAæ¨¡å‹çš„æ•ˆç‡ï¼Œå¯ä»¥ä½¿è¿™äº›æœºå™¨äººèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“è¯†åˆ«ã€æŠ“å–ã€æ”¾ç½®ç­‰ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯ä»¥ä¿ƒè¿›äººæœºåä½œçš„å‘å±•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çš„æŒ‡ä»¤ï¼Œå¹¶ä¸äººç±»è¿›è¡Œäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.

