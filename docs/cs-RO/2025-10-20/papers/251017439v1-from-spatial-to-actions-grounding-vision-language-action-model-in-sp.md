---
layout: default
title: From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors
---

# From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.17439" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.17439v1</a>
  <a href="https://arxiv.org/pdf/2510.17439.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.17439v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.17439v1', 'From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-20

**å¤‡æ³¨**: Project page: https://falcon-vla.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**FALCONï¼šåˆ©ç”¨ç©ºé—´åŸºç¡€å…ˆéªŒå¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„3Dç¯å¢ƒæ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `3Dç©ºé—´æ¨ç†` `ç©ºé—´åŸºç¡€æ¨¡å‹` `å…·èº«æ™ºèƒ½` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ä¾èµ–2Dç¼–ç å™¨ï¼Œç¼ºä¹3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–æ€§å’Œé€‚åº”æ€§ã€‚
2. FALCONé€šè¿‡ç©ºé—´åŸºç¡€æ¨¡å‹æå–3Dç©ºé—´tokensï¼Œå¹¶æ³¨å…¥åˆ°åŠ¨ä½œå¤´ä¸­ï¼Œå¢å¼ºç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒFALCONåœ¨å¤šä¸ªæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å‡å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹è™½ç„¶åœ¨3DçœŸå®ä¸–ç•Œä¸­è¿è¡Œï¼Œä½†é€šå¸¸æ„å»ºåœ¨2Dç¼–ç å™¨ä¹‹ä¸Šï¼Œå¯¼è‡´ç©ºé—´æ¨ç†å­˜åœ¨å·®è·ï¼Œé™åˆ¶äº†æ³›åŒ–æ€§å’Œé€‚åº”æ€§ã€‚ç›®å‰é’ˆå¯¹VLAçš„3Dé›†æˆæŠ€æœ¯è¦ä¹ˆéœ€è¦ä¸“ç”¨ä¼ æ„Ÿå™¨ï¼Œä¸”è·¨æ¨¡æ€è¿ç§»æ•ˆæœå·®ï¼Œè¦ä¹ˆæ³¨å…¥çš„å¼±çº¿ç´¢ç¼ºä¹å‡ ä½•ä¿¡æ¯ï¼Œé™ä½äº†è§†è§‰-è¯­è¨€å¯¹é½æ•ˆæœã€‚æœ¬æ–‡æå‡ºäº†FALCONï¼ˆFrom Spatial to Actionï¼‰ï¼Œä¸€ç§å°†ä¸°å¯Œçš„3Dç©ºé—´tokensæ³¨å…¥åˆ°åŠ¨ä½œå¤´çš„å…¨æ–°èŒƒä¾‹ã€‚FALCONåˆ©ç”¨ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œä»…ä»RGBå›¾åƒä¸­æä¾›å¼ºå¤§çš„å‡ ä½•å…ˆéªŒï¼Œå¹¶åŒ…å«ä¸€ä¸ªå…·èº«ç©ºé—´æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥é€‰æ‹©æ€§åœ°èåˆæ·±åº¦æˆ–å§¿æ€ä¿¡æ¯ï¼Œä»¥åœ¨å¯ç”¨æ—¶è·å¾—æ›´é«˜çš„ä¿çœŸåº¦ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–æ›´æ”¹æ¶æ„ã€‚ä¸ºäº†ä¿æŒè¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œç©ºé—´tokensç”±ç©ºé—´å¢å¼ºåŠ¨ä½œå¤´æ¶ˆè€—ï¼Œè€Œä¸æ˜¯è¿æ¥åˆ°è§†è§‰-è¯­è¨€éª¨å¹²ç½‘ç»œä¸­ã€‚è¿™äº›è®¾è®¡ä½¿FALCONèƒ½å¤Ÿè§£å†³ç©ºé—´è¡¨ç¤ºã€æ¨¡æ€å¯è¿ç§»æ€§å’Œå¯¹é½æ–¹é¢çš„å±€é™æ€§ã€‚åœ¨ä¸‰ä¸ªæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’Œåä¸€ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡çš„ç»¼åˆè¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„FALCONå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå§‹ç»ˆè¶…è¶Šäº†å…·æœ‰ç«äº‰åŠ›çš„åŸºçº¿ï¼Œå¹¶ä¸”åœ¨æ‚ä¹±ã€ç©ºé—´æç¤ºæ¡ä»¶ä»¥åŠå¯¹è±¡å°ºåº¦å’Œé«˜åº¦å˜åŒ–ä¸‹ä¿æŒç¨³å¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤„ç†çœŸå®3Dç¯å¢ƒæ—¶ï¼Œç”±äºå…¶è§†è§‰ç¼–ç å™¨ä¸»è¦åŸºäº2Då›¾åƒï¼Œç¼ºä¹å¯¹3Dç©ºé—´å‡ ä½•ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚ç°æœ‰çš„3Dé›†æˆæ–¹æ³•è¦ä¹ˆä¾èµ–ç‰¹å®šä¼ æ„Ÿå™¨ï¼ˆå¦‚æ·±åº¦ç›¸æœºï¼‰ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ï¼Œè¦ä¹ˆæä¾›çš„3Dä¿¡æ¯ä¸è¶³ï¼Œå½±å“äº†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„å¯¹é½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFALCONçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œä»RGBå›¾åƒä¸­æå–ä¸°å¯Œçš„3Dç©ºé—´å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†è¿™äº›çŸ¥è¯†ä»¥ç©ºé—´tokensçš„å½¢å¼æ³¨å…¥åˆ°åŠ¨ä½œå¤´ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸æ”¹å˜è§†è§‰-è¯­è¨€éª¨å¹²ç½‘ç»œç»“æ„çš„å‰æä¸‹ï¼Œå¢å¼ºå¯¹3Dç¯å¢ƒçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨æ·±åº¦æˆ–å§¿æ€ä¿¡æ¯ï¼ŒFALCONè¿˜è®¾è®¡äº†ä¸€ä¸ªå…·èº«ç©ºé—´æ¨¡å‹ï¼Œå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹èåˆè¿™äº›é¢å¤–ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFALCONçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†è§‰-è¯­è¨€éª¨å¹²ç½‘ç»œï¼šç”¨äºæå–å›¾åƒå’Œè¯­è¨€ç‰¹å¾ã€‚2) ç©ºé—´åŸºç¡€æ¨¡å‹ï¼šç”¨äºä»RGBå›¾åƒä¸­æå–3Dç©ºé—´tokensã€‚3) å…·èº«ç©ºé—´æ¨¡å‹ï¼ˆå¯é€‰ï¼‰ï¼šç”¨äºèåˆæ·±åº¦æˆ–å§¿æ€ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥å¢å¼ºç©ºé—´è¡¨ç¤ºã€‚4) ç©ºé—´å¢å¼ºåŠ¨ä½œå¤´ï¼šç”¨äºæ¥æ”¶è§†è§‰-è¯­è¨€ç‰¹å¾å’Œç©ºé—´tokensï¼Œå¹¶ç”ŸæˆåŠ¨ä½œæŒ‡ä»¤ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆé€šè¿‡è§†è§‰-è¯­è¨€éª¨å¹²ç½‘ç»œæå–ç‰¹å¾ï¼Œç„¶ååˆ©ç”¨ç©ºé—´åŸºç¡€æ¨¡å‹æå–ç©ºé—´ä¿¡æ¯ï¼Œæœ€åå°†è¿™äº›ä¿¡æ¯èåˆåˆ°åŠ¨ä½œå¤´ä¸­ï¼ŒæŒ‡å¯¼åŠ¨ä½œçš„ç”Ÿæˆã€‚

**å…³é”®åˆ›æ–°**ï¼šFALCONçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å°†ç©ºé—´åŸºç¡€æ¨¡å‹ä¸VLAæ¨¡å‹ç›¸ç»“åˆï¼Œé€šè¿‡ç©ºé—´tokensçš„å½¢å¼å°†3Dç©ºé—´å…ˆéªŒçŸ¥è¯†æ³¨å…¥åˆ°åŠ¨ä½œå¤´ä¸­ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥ä¿®æ”¹è§†è§‰-è¯­è¨€éª¨å¹²ç½‘ç»œï¼Œä»è€Œä¿æŒäº†è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒFALCONçš„å…·èº«ç©ºé—´æ¨¡å‹å¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹èåˆæ·±åº¦æˆ–å§¿æ€ä¿¡æ¯ï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šFALCONçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œä¾‹å¦‚3Dæ£€æµ‹æ¨¡å‹ï¼Œæå–3D bounding box æˆ– point cloud ç­‰ç©ºé—´ä¿¡æ¯ã€‚2) è®¾è®¡ç©ºé—´å¢å¼ºåŠ¨ä½œå¤´ï¼Œè¯¥æ¨¡å—æ¥æ”¶è§†è§‰-è¯­è¨€ç‰¹å¾å’Œç©ºé—´tokensï¼Œå¹¶ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–èåˆæ–¹æ³•å°†å®ƒä»¬ç»“åˆèµ·æ¥ã€‚3) å…·èº«ç©ºé—´æ¨¡å‹çš„è®¾è®¡ï¼Œå…è®¸æ¨¡å‹åœ¨å¯ç”¨æ—¶èåˆæ·±åº¦æˆ–å§¿æ€ä¿¡æ¯ï¼Œæé«˜ç©ºé—´è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FALCONåœ¨ä¸‰ä¸ªæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’Œåä¸€ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFALCONåœ¨æ‚ä¹±ç¯å¢ƒã€ç©ºé—´æç¤ºæ¡ä»¶ä»¥åŠå¯¹è±¡å°ºåº¦å’Œé«˜åº¦å˜åŒ–ä¸‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒFALCONåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨3Dç©ºé—´æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

FALCONå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°ç†è§£äººç±»æŒ‡ä»¤ï¼Œå¹¶æ‰§è¡Œå„ç§ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒFALCONè¿˜å¯ä»¥åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸï¼Œæé«˜æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œå·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼ŒFALCONæœ‰æœ›æˆä¸ºæ„å»ºæ›´æ™ºèƒ½ã€æ›´å¯é çš„æœºå™¨äººç³»ç»Ÿçš„å…³é”®æŠ€æœ¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.

