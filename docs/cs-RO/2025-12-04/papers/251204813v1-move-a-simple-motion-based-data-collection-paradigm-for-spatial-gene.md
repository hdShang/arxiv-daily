---
layout: default
title: MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation
---

# MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation

**arXiv**: [2512.04813v1](https://arxiv.org/abs/2512.04813) | [PDF](https://arxiv.org/pdf/2512.04813.pdf)

**ä½œè€…**: Huanqian Wang, Chi Bene Chen, Yang Yue, Danhua Tao, Tong Guo, Shaoxuan Xie, Denghang Huang, Shiji Song, Guocai Yao, Gao Huang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-04

**å¤‡æ³¨**: 9 pages, 9 figures

**ðŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/lucywang720/MOVE)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMOVEä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ¨¡ä»¿å­¦ä¹ ` `æ•°æ®æ”¶é›†` `ç©ºé—´æ³›åŒ–` `åŠ¨æ€æ¼”ç¤º` `æ•°æ®æ•ˆçŽ‡` `è¿åŠ¨å¢žå¼º`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®æ”¶é›†ä¸Šå­˜åœ¨å±€é™ï¼Œé€šå¸¸åªä»Žé™æ€çŽ¯å¢ƒä¸­èŽ·å–è½¨è¿¹ï¼Œå¯¼è‡´ç©ºé—´ä¿¡æ¯ä¸è¶³ã€‚
2. MOVEæ–¹æ³•é€šè¿‡åœ¨æ¼”ç¤ºä¸­å¼•å…¥åŠ¨æ€è¿åŠ¨ï¼Œå¢žå¼ºäº†æ•°æ®çš„ç©ºé—´å¤šæ ·æ€§ï¼Œä»Žè€Œæé«˜äº†å­¦ä¹ æ•ˆçŽ‡ã€‚
3. å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒMOVEåœ¨ç©ºé—´æ³›åŒ–ä»»åŠ¡ä¸­æˆåŠŸçŽ‡è¾¾åˆ°39.1%ï¼Œç›¸è¾ƒäºŽä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä¸­å±•çŽ°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶å®žé™…åº”ç”¨å—åˆ°æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶è‡´åŠ›äºŽæ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä½†åœ¨ç©ºé—´æ³›åŒ–èƒ½åŠ›ä¸Šä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æˆ‘ä»¬å‘çŽ°ï¼ŒçŽ°æœ‰æ–¹æ³•é€šå¸¸åªä»Žå•ä¸€é™æ€ç©ºé—´é…ç½®ä¸­æ”¶é›†è½¨è¿¹ï¼Œé™åˆ¶äº†å¯ç”¨äºŽå­¦ä¹ çš„ç©ºé—´ä¿¡æ¯å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ•°æ®æ•ˆçŽ‡ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†MOtion-Based Variability Enhancementï¼ˆMOVEï¼‰ï¼Œä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®æ”¶é›†èŒƒå¼ï¼Œé€šè¿‡åœ¨æ¯æ¬¡æ¼”ç¤ºä¸­ä¸ºå¯ç§»åŠ¨ç‰©ä½“æ³¨å…¥è¿åŠ¨ï¼Œéšå¼ç”Ÿæˆä¸°å¯Œçš„ç©ºé—´é…ç½®ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒMOVEåœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­å®žçŽ°äº†39.1%çš„æˆåŠŸçŽ‡ï¼Œç›¸è¾ƒäºŽé™æ€æ•°æ®æ”¶é›†æ–¹æ³•æå‡äº†76.1%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä»Žå•ä¸€é™æ€ç©ºé—´é…ç½®ä¸­æ”¶é›†è½¨è¿¹ï¼Œå¯¼è‡´ç©ºé—´ä¿¡æ¯çš„å¤šæ ·æ€§ä¸è¶³ï¼Œé™åˆ¶äº†æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†MOVEæ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¯æ¬¡æ¼”ç¤ºä¸­ä¸ºå¯ç§»åŠ¨ç‰©ä½“æ³¨å…¥è¿åŠ¨ï¼Œç”Ÿæˆä¸°å¯Œçš„ç©ºé—´é…ç½®ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æå‡æ•°æ®çš„å¤šæ ·æ€§å’Œä¸°å¯Œæ€§ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„å­¦ä¹ æ•ˆæžœã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMOVEçš„æ•´ä½“æž¶æž„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€åŠ¨æ€æ¼”ç¤ºå’Œç©ºé—´é…ç½®ç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚åœ¨æ•°æ®æ”¶é›†é˜¶æ®µï¼Œé€šè¿‡å¼•å…¥è¿åŠ¨ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„è½¨è¿¹ï¼›åœ¨åŠ¨æ€æ¼”ç¤ºé˜¶æ®µï¼Œåˆ©ç”¨å¯ç§»åŠ¨ç‰©ä½“çš„è¿åŠ¨å¢žå¼ºç©ºé—´ä¿¡æ¯ï¼›æœ€åŽï¼Œé€šè¿‡ç”Ÿæˆçš„ç©ºé—´é…ç½®è¿›è¡Œæ¨¡åž‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šMOVEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºŽå…¶åŠ¨æ€æ•°æ®æ”¶é›†ç­–ç•¥ï¼Œé€šè¿‡åœ¨æ¼”ç¤ºä¸­å¼•å…¥è¿åŠ¨ï¼Œæ˜¾è‘—æé«˜äº†ç©ºé—´é…ç½®çš„å¤šæ ·æ€§ã€‚è¿™ä¸Žä¼ ç»Ÿçš„é™æ€æ•°æ®æ”¶é›†æ–¹æ³•å½¢æˆé²œæ˜Žå¯¹æ¯”ï¼ŒåŽè€…æ— æ³•æä¾›è¶³å¤Ÿçš„ç©ºé—´ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨MOVEä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†å¤šä¸ªå¯ç§»åŠ¨ç‰©ä½“çš„è¿åŠ¨å‚æ•°ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„è½¨è¿¹åœ¨ç©ºé—´ä¸Šçš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æž„é‡‡ç”¨äº†é€‚åº”æ€§å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡åž‹å¯¹åŠ¨æ€çŽ¯å¢ƒçš„é€‚åº”èƒ½åŠ›ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

MOVEåœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼ŒæˆåŠŸçŽ‡è¾¾åˆ°39.1%ï¼Œç›¸æ¯”äºŽé™æ€æ•°æ®æ”¶é›†æ–¹æ³•çš„22.2%æå‡äº†76.1%ã€‚åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œæ•°æ®æ•ˆçŽ‡æå‡è¾¾åˆ°2è‡³5å€ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç©ºé—´æ³›åŒ–èƒ½åŠ›ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿å’Œæ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼ŒMOVEèƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººåœ¨å®žé™…åº”ç”¨ä¸­çš„çµæ´»æ€§å’Œæ•ˆçŽ‡ï¼ŒæŽ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Imitation learning method has shown immense promise for robotic manipulation, yet its practical deployment is fundamentally constrained by the data scarcity. Despite prior work on collecting large-scale datasets, there still remains a significant gap to robust spatial generalization. We identify a key limitation: individual trajectories, regardless of their length, are typically collected from a \emph{single, static spatial configuration} of the environment. This includes fixed object and target spatial positions as well as unchanging camera viewpoints, which significantly restricts the diversity of spatial information available for learning. To address this critical bottleneck in data efficiency, we propose \textbf{MOtion-Based Variability Enhancement} (\emph{MOVE}), a simple yet effective data collection paradigm that enables the acquisition of richer spatial information from dynamic demonstrations. Our core contribution is an augmentation strategy that injects motion into any movable objects within the environment for each demonstration. This process implicitly generates a dense and diverse set of spatial configurations within a single trajectory. We conduct extensive experiments in both simulation and real-world environments to validate our approach. For example, in simulation tasks requiring strong spatial generalization, \emph{MOVE} achieves an average success rate of 39.1\%, a 76.1\% relative improvement over the static data collection paradigm (22.2\%), and yields up to 2--5$\times$ gains in data efficiency on certain tasks. Our code is available at https://github.com/lucywang720/MOVE.

