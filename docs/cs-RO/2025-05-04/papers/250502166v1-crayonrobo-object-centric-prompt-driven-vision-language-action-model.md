---
layout: default
title: "CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation"
---

# CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02166" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02166v1</a>
  <a href="https://arxiv.org/pdf/2505.02166.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02166v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02166v1', 'CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoqi Li, Lingyun Xu, Mingxu Zhang, Jiaming Liu, Yan Shen, Iaroslav Ponomarenko, Jiahui Xu, Liang Heng, Siyuan Huang, Shanghang Zhang, Hao Dong

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04

**å¤‡æ³¨**: CVPR 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCrayonRoboä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„å¤šæ¨¡æ€ä»»åŠ¡ç›®æ ‡ä¼ è¾¾é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æç¤º` `æœºå™¨äººæ“ä½œ` `ä»»åŠ¡ç›®æ ‡ä¼ è¾¾` `è§†è§‰-è¯­è¨€æ¨¡å‹` `é•¿æ—¶é—´è·¨åº¦ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä¼ è¾¾ä»»åŠ¡ç›®æ ‡æ—¶å­˜åœ¨æ­§ä¹‰æ€§å’Œè¿‡åº¦è¯¦ç»†çš„é—®é¢˜ï¼Œå¯¼è‡´æœºå™¨äººæ“ä½œçš„æ•ˆç‡å’Œå‡†ç¡®æ€§é™ä½ã€‚
2. CrayonRoboé€šè¿‡ç»¼åˆå¤šæ¨¡æ€æç¤ºï¼Œç®€åŒ–äº†ä½çº§åŠ¨ä½œå’Œé«˜çº§è§„åˆ’çš„è¡¨è¾¾ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä»»åŠ¡ç›®æ ‡ã€‚
3. åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­è¯„ä¼°åï¼ŒCrayonRoboå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ“ä½œèƒ½åŠ›ï¼Œå°¤å…¶åœ¨æœªè§ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œä»»åŠ¡ç›®æ ‡å¯ä»¥é€šè¿‡è¯­è¨€ã€ç›®æ ‡å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€ä¼ è¾¾ã€‚ç„¶è€Œï¼Œè‡ªç„¶è¯­è¨€å¯èƒ½å­˜åœ¨æ­§ä¹‰ï¼Œè€Œå›¾åƒæˆ–è§†é¢‘åˆ™å¯èƒ½æä¾›è¿‡äºè¯¦ç»†çš„è§„èŒƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CrayonRoboï¼Œå®ƒåˆ©ç”¨å…¨é¢çš„å¤šæ¨¡æ€æç¤ºï¼Œæ˜ç¡®ä¼ è¾¾ä½çº§åŠ¨ä½œå’Œé«˜çº§è§„åˆ’ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸å¯¹ä»»åŠ¡åºåˆ—ä¸­çš„æ¯ä¸ªå…³é”®å¸§æ‰‹åŠ¨æˆ–è‡ªåŠ¨ç”Ÿæˆç®€å•ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„2Dè§†è§‰æç¤ºï¼Œè¿™äº›æç¤ºå åŠ åœ¨RGBå›¾åƒä¸Šï¼Œè¡¨ç¤ºæ‰€éœ€çš„ä»»åŠ¡ç›®æ ‡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£è¿™äº›è§†è§‰-è¯­è¨€æç¤ºï¼Œå¹¶é¢„æµ‹ç›¸åº”çš„æ¥è§¦å§¿æ€å’Œè¿åŠ¨æ–¹å‘ã€‚é€šè¿‡é¡ºåºæ‰§è¡Œæ‰€æœ‰å…³é”®å¸§æ­¥éª¤ï¼Œæ¨¡å‹èƒ½å¤Ÿå®Œæˆé•¿æ—¶é—´è·¨åº¦çš„ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ä¸ä»…å¸®åŠ©æ¨¡å‹æ˜ç¡®ç†è§£ä»»åŠ¡ç›®æ ‡ï¼Œè¿˜é€šè¿‡æä¾›æ˜“äºè§£é‡Šçš„æç¤ºå¢å¼ºäº†å…¶åœ¨æœªè§ä»»åŠ¡ä¸Šçš„é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­ä»»åŠ¡ç›®æ ‡ä¼ è¾¾çš„æ­§ä¹‰æ€§å’Œè¿‡åº¦è¯¦ç»†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåœ°å°†å¤æ‚çš„ä»»åŠ¡ç›®æ ‡ä»¥ç®€å•æ˜äº†çš„æ–¹å¼ä¼ è¾¾ç»™æœºå™¨äººï¼Œå¯¼è‡´æ“ä½œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCrayonRoboçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€æç¤ºï¼Œé€šè¿‡ç®€å•çš„2Dè§†è§‰æç¤ºæ¥æ˜ç¡®ä¼ è¾¾ä»»åŠ¡ç›®æ ‡ï¼ŒåŒ…æ‹¬æ¥è§¦å§¿æ€å’Œè¿åŠ¨æ–¹å‘ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æœºå™¨äººèƒ½å¤Ÿæ›´ç›´è§‚åœ°ç†è§£ä»»åŠ¡è¦æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯ç”Ÿæˆ2Dè§†è§‰æç¤ºçš„æ¨¡å—ï¼ŒäºŒæ˜¯åŸºäºè¿™äº›æç¤ºè¿›è¡Œä»»åŠ¡æ‰§è¡Œçš„æ¨¡å—ã€‚æ¨¡å‹é€šè¿‡æ‰‹åŠ¨æˆ–è‡ªåŠ¨ç”Ÿæˆæç¤ºï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å¦‚ä½•è§£è¯»è¿™äº›æç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šCrayonRoboçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€æç¤ºçš„è®¾è®¡ï¼Œä½¿å¾—æœºå™¨äººèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œç›®æ ‡ã€‚è¿™ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†ä»»åŠ¡çš„å¯è§£é‡Šæ€§å’Œæ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æç¤ºçš„ç”Ÿæˆå’Œè§£è¯»è¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”å¤šæ¨¡æ€è¾“å…¥çš„å¤„ç†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒCrayonRoboåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§ä»»åŠ¡ä¸Šçš„æ“ä½œæˆåŠŸç‡æå‡äº†20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†æ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒæœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººå¯¹ä»»åŠ¡ç›®æ ‡çš„ç†è§£èƒ½åŠ›ï¼ŒCrayonRoboå¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œæ›´ä¸ºç²¾ç¡®çš„æ“ä½œï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In robotic, task goals can be conveyed through various modalities, such as language, goal images, and goal videos. However, natural language can be ambiguous, while images or videos may offer overly detailed specifications. To tackle these challenges, we introduce CrayonRobo that leverages comprehensive multi-modal prompts that explicitly convey both low-level actions and high-level planning in a simple manner. Specifically, for each key-frame in the task sequence, our method allows for manual or automatic generation of simple and expressive 2D visual prompts overlaid on RGB images. These prompts represent the required task goals, such as the end-effector pose and the desired movement direction after contact. We develop a training strategy that enables the model to interpret these visual-language prompts and predict the corresponding contact poses and movement directions in SE(3) space. Furthermore, by sequentially executing all key-frame steps, the model can complete long-horizon tasks. This approach not only helps the model explicitly understand the task objectives but also enhances its robustness on unseen tasks by providing easily interpretable prompts. We evaluate our method in both simulated and real-world environments, demonstrating its robust manipulation capabilities.

