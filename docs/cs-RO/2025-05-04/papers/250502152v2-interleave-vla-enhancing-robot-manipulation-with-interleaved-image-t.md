---
layout: default
title: Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions
---

# Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02152" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02152v2</a>
  <a href="https://arxiv.org/pdf/2505.02152.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02152v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02152v2', 'Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04 (æ›´æ–°: 2025-10-08)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://interleave-vla.github.io/Interleave-VLA-Anonymous/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºInterleave-VLAä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æŒ‡ä»¤ç†è§£é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `é›¶-shotæ³›åŒ–` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬æŒ‡ä»¤ï¼Œéš¾ä»¥åœ¨æœªè§åœºæ™¯ä¸­å®ç°è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. Interleave-VLAé€šè¿‡äº¤é”™çš„å›¾åƒ-æ–‡æœ¬è¾“å…¥ï¼Œå¢å¼ºäº†æœºå™¨äººå¯¹æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ï¼Œæå‡äº†äººæœºäº¤äº’çš„çµæ´»æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒInterleave-VLAåœ¨æœªè§ç‰©ä½“çš„æ³›åŒ–èƒ½åŠ›ä¸Šæé«˜äº†2å€ï¼Œå¹¶æ”¯æŒå¤šæ ·åŒ–çš„é›¶-shotä»»åŠ¡æ¥å£ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºç¡€æ¨¡å‹çš„å…´èµ·ä¸ºç‰©ç†ä¸–ç•Œä¸­çš„é€šç”¨æœºå™¨äººç­–ç•¥é“ºå¹³äº†é“è·¯ã€‚ç°æœ‰ä¾èµ–æ–‡æœ¬æŒ‡ä»¤çš„æ–¹æ³•åœ¨å¤„ç†æœªè§åœºæ™¯æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è®¤ä¸ºäº¤é”™çš„å›¾åƒ-æ–‡æœ¬è¾“å…¥æä¾›äº†æ›´ä¸°å¯Œä¸”æ›´å°‘åè§çš„ä¸Šä¸‹æ–‡ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†æœªè§ä»»åŠ¡å¹¶å®ç°æ›´çµæ´»çš„äººæœºäº¤äº’ã€‚åŸºäºæ­¤ï¼ŒInterleave-VLAè¢«æå‡ºï¼Œä½œä¸ºé¦–ä¸ªèƒ½å¤Ÿç†è§£äº¤é”™å›¾åƒ-æ–‡æœ¬æŒ‡ä»¤å¹¶ç›´æ¥ç”Ÿæˆç‰©ç†ä¸–ç•Œä¸­è¿ç»­åŠ¨ä½œåºåˆ—çš„æœºå™¨äººå­¦ä¹ èŒƒå¼ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæœ€å°ä¿®æ”¹çš„åŒæ—¶ï¼Œæ‰©å±•äº†æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œå®ç°äº†å¼ºå¤§çš„é›¶-shotæ³›åŒ–èƒ½åŠ›ã€‚Interleave-VLAè¿˜åŒ…æ‹¬ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œå°†Open X-Embodimentä¸­çš„æ–‡æœ¬æŒ‡ä»¤è½¬æ¢ä¸ºäº¤é”™çš„å›¾åƒ-æ–‡æœ¬æŒ‡ä»¤ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªåŒ…å«210ké›†çš„è§„æ¨¡åºå¤§çš„çœŸå®ä¸–ç•Œäº¤é”™ä½“æ•°æ®é›†ã€‚ç»¼åˆçš„æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œè¯„ä¼°è¡¨æ˜ï¼ŒInterleave-VLAåœ¨æœªè§ç‰©ä½“çš„é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ä¸Šæé«˜äº†2å€ï¼Œå¹¶æ”¯æŒçµæ´»çš„ä»»åŠ¡æ¥å£å’Œå¤šæ ·åŒ–çš„æŒ‡ä»¤ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„æœºå™¨äººæ“ä½œæ–¹æ³•å¤šä¾èµ–æ–‡æœ¬æŒ‡ä»¤ï¼Œå¯¼è‡´åœ¨å¤„ç†æœªè§åœºæ™¯æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œé™åˆ¶äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬ç ”ç©¶æå‡ºInterleave-VLAï¼Œé€šè¿‡äº¤é”™çš„å›¾åƒ-æ–‡æœ¬è¾“å…¥ï¼Œæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºæœºå™¨äººå¯¹æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ï¼Œæå‡å…¶åœ¨æœªè§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInterleave-VLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æŒ‡ä»¤è§£ææ¨¡å—ã€åŠ¨ä½œç”Ÿæˆæ¨¡å—å’Œåé¦ˆè°ƒæ•´æ¨¡å—ã€‚æŒ‡ä»¤è§£ææ¨¡å—è´Ÿè´£å°†äº¤é”™çš„å›¾åƒ-æ–‡æœ¬è¾“å…¥è½¬åŒ–ä¸ºå¯ç†è§£çš„æŒ‡ä»¤ï¼ŒåŠ¨ä½œç”Ÿæˆæ¨¡å—åˆ™æ ¹æ®è§£æç»“æœç”Ÿæˆè¿ç»­çš„åŠ¨ä½œåºåˆ—ï¼Œåé¦ˆè°ƒæ•´æ¨¡å—ç”¨äºæ ¹æ®æ‰§è¡Œç»“æœä¼˜åŒ–æŒ‡ä»¤ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šInterleave-VLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿå¤„ç†äº¤é”™çš„å›¾åƒ-æ–‡æœ¬è¾“å…¥ï¼Œå¹¶å®ç°å¼ºå¤§çš„é›¶-shotæ³›åŒ–èƒ½åŠ›ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–æ–‡æœ¬çš„æŒ‡ä»¤ç†è§£æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€èåˆæŠ€æœ¯ï¼Œç»“åˆäº†æ¥è‡ªäº’è”ç½‘çš„å¼‚æ„æ•°æ®é›†ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè€ƒè™‘äº†å¤šæ¨¡æ€è¾“å…¥çš„ç‰¹æ€§ï¼Œä»¥æé«˜æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒInterleave-VLAåœ¨æœªè§ç‰©ä½“çš„é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ä¸Šæé«˜äº†2å€ï¼Œç›¸è¾ƒäºä»…ä½¿ç”¨æ–‡æœ¬è¾“å…¥çš„åŸºçº¿æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ”¯æŒçµæ´»çš„ä»»åŠ¡æ¥å£ï¼Œèƒ½å¤Ÿå¤„ç†å¤šæ ·åŒ–çš„æŒ‡ä»¤ï¼Œå¦‚æ‰‹ç»˜è‰å›¾ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é›¶-shotèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Interleave-VLAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒæœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹å¤æ‚æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆçš„äººæœºåä½œï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨å®é™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rise of foundation models paves the way for generalist robot policies in the physical world. Existing methods relying on text-only instructions often struggle to generalize to unseen scenarios. We argue that interleaved image-text inputs offer richer and less biased context and enable robots to better handle unseen tasks with more versatile human-robot interaction. Building on this insight, Interleave-VLA, the first robot learning paradigm capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world, is introduced. It offers a natural, flexible, and model-agnostic paradigm that extends state-of-the-art vision-language-action (VLA) models with minimal modifications while achieving strong zero-shot generalization. Interleave-VLA also includes an automatic pipeline that converts text instructions from Open X-Embodiment into interleaved image-text instructions, resulting in a large-scale real-world interleaved embodied dataset with 210k episodes. Comprehensive evaluation in simulation and the real world shows that Interleave-VLA offers two major benefits: (1) improves out-of-domain generalization to unseen objects by 2x compared to text input baselines, (2) supports flexible task interfaces and diverse instructions in a zero-shot manner, such as hand-drawn sketches. We attribute Interleave-VLA's strong zero-shot capability to the use of instruction images, which effectively mitigate hallucinations, and the inclusion of heterogeneous multimodal datasets, enriched with Internet-sourced images, offering potential for scalability. More information is available at https://interleave-vla.github.io/Interleave-VLA-Anonymous/

