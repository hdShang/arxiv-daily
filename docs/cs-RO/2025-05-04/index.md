---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-04
---

# cs.ROï¼ˆ2025-05-04ï¼‰

ğŸ“Š å…± **10** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250502152v2-interleave-vla-enhancing-robot-manipulation-with-interleaved-image-t.html">Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</a></td>
  <td>æå‡ºInterleave-VLAä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æŒ‡ä»¤ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02152v2" data-paper-url="./papers/250502152v2-interleave-vla-enhancing-robot-manipulation-with-interleaved-image-t.html" onclick="toggleFavorite(this, '2505.02152v2', 'Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250501974v1-kinedex-learning-tactile-informed-visuomotor-policies-via-kinestheti.html">KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation</a></td>
  <td>æå‡ºKineDexä»¥è§£å†³ç²¾ç»†è§¦è§‰ä¿¡æ¯ç¼ºå¤±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01974v1" data-paper-url="./papers/250501974v1-kinedex-learning-tactile-informed-visuomotor-policies-via-kinestheti.html" onclick="toggleFavorite(this, '2505.01974v1', 'KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250502166v1-crayonrobo-object-centric-prompt-driven-vision-language-action-model.html">CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation</a></td>
  <td>æå‡ºCrayonRoboä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„å¤šæ¨¡æ€ä»»åŠ¡ç›®æ ‡ä¼ è¾¾é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02166v1" data-paper-url="./papers/250502166v1-crayonrobo-object-centric-prompt-driven-vision-language-action-model.html" onclick="toggleFavorite(this, '2505.02166v1', 'CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250502291v4-dexterous-contact-rich-manipulation-via-the-contact-trust-region.html">Dexterous Contact-Rich Manipulation via the Contact Trust Region</a></td>
  <td>æå‡ºæ¥è§¦ä¿¡ä»»åŒºåŸŸä»¥è§£å†³æ¥è§¦ä¸°å¯Œæ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">in-hand manipulation</span> <span class="paper-tag">bi-manual</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02291v4" data-paper-url="./papers/250502291v4-dexterous-contact-rich-manipulation-via-the-contact-trust-region.html" onclick="toggleFavorite(this, '2505.02291v4', 'Dexterous Contact-Rich Manipulation via the Contact Trust Region')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250502232v1-prompt-responsive-object-retrieval-with-memory-augmented-student-tea.html">Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning</a></td>
  <td>æå‡ºè®°å¿†å¢å¼ºçš„å¸ˆç”Ÿå­¦ä¹ æ¡†æ¶ä»¥è§£å†³æœºå™¨äººç›®æ ‡æ£€ç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02232v1" data-paper-url="./papers/250502232v1-prompt-responsive-object-retrieval-with-memory-augmented-student-tea.html" onclick="toggleFavorite(this, '2505.02232v1', 'Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250503830v1-bridging-model-predictive-control-and-deep-learning-for-scalable-rea.html">Bridging Model Predictive Control and Deep Learning for Scalable Reachability Analysis</a></td>
  <td>æå‡ºåŸºäºæ¨¡å‹é¢„æµ‹æ§åˆ¶ä¸æ·±åº¦å­¦ä¹ çš„å¯æ‰©å±•å¯è¾¾æ€§åˆ†ææ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03830v1" data-paper-url="./papers/250503830v1-bridging-model-predictive-control-and-deep-learning-for-scalable-rea.html" onclick="toggleFavorite(this, '2505.03830v1', 'Bridging Model Predictive Control and Deep Learning for Scalable Reachability Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250502272v2-robust-localization-mapping-and-navigation-for-quadruped-robots.html">Robust Localization, Mapping, and Navigation for Quadruped Robots</a></td>
  <td>æå‡ºä½æˆæœ¬ä¼ æ„Ÿå™¨çš„å››è¶³æœºå™¨äººå®šä½ã€åœ°å›¾æ„å»ºä¸å¯¼èˆªç³»ç»Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02272v2" data-paper-url="./papers/250502272v2-robust-localization-mapping-and-navigation-for-quadruped-robots.html" onclick="toggleFavorite(this, '2505.02272v2', 'Robust Localization, Mapping, and Navigation for Quadruped Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250502123v1-driveagent-multi-agent-structured-reasoning-with-llm-and-multimodal-.html">DriveAgent: Multi-Agent Structured Reasoning with LLM and Multimodal Sensor Fusion for Autonomous Driving</a></td>
  <td>æå‡ºDriveAgentæ¡†æ¶ä»¥æå‡è‡ªä¸»é©¾é©¶çš„å†³ç­–ä¸ç†è§£èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span> <span class="paper-tag">TAMP</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.02123v1" data-paper-url="./papers/250502123v1-driveagent-multi-agent-structured-reasoning-with-llm-and-multimodal-.html" onclick="toggleFavorite(this, '2505.02123v1', 'DriveAgent: Multi-Agent Structured Reasoning with LLM and Multimodal Sensor Fusion for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>9</td>
  <td><a href="./papers/250501966v2-a-goal-oriented-reinforcement-learning-based-path-planning-algorithm.html">A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites</a></td>
  <td>æå‡ºåŸºäºç›®æ ‡å¯¼å‘å¼ºåŒ–å­¦ä¹ çš„è·¯å¾„è§„åˆ’ç®—æ³•ä»¥è§£å†³æ¨¡å—åŒ–è‡ªé‡æ„å«æ˜Ÿé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01966v2" data-paper-url="./papers/250501966v2-a-goal-oriented-reinforcement-learning-based-path-planning-algorithm.html" onclick="toggleFavorite(this, '2505.01966v2', 'A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250501956v2-safenav-safe-path-navigation-using-landmark-based-localization-in-a-.html">SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment</a></td>
  <td>æå‡ºLanBLoc-BMMä»¥è§£å†³GPSå—é™ç¯å¢ƒä¸­çš„å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual odometry</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.01956v2" data-paper-url="./papers/250501956v2-safenav-safe-path-navigation-using-landmark-based-localization-in-a-.html" onclick="toggleFavorite(this, '2505.01956v2', 'SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)