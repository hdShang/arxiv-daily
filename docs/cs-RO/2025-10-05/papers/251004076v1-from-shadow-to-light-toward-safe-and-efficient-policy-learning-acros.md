---
layout: default
title: From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents
---

# From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04076" target="_blank" class="toolbar-btn">arXiv: 2510.04076v1</a>
    <a href="https://arxiv.org/pdf/2510.04076.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04076v1" 
            onclick="toggleFavorite(this, '2510.04076v1', 'From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Amin Vahidi-Moghaddam, Sayed Pedram Haeri Boroujeni, Iman Jebellat, Ehsan Jebellat, Niloufar Mehrabi, Zhaojian Li

**ÂàÜÁ±ª**: cs.RO, eess.SY

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÂÖ´ÁßçÊñπÊ≥ï‰ª•ÊèêÂçáÊï∞ÊçÆÈ©±Âä®ÊéßÂà∂Á≠ñÁï•ÁöÑÂÆâÂÖ®ÊÄß‰∏éÊïàÁéá**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Ê®°ÂûãÈ¢ÑÊµãÊéßÂà∂` `Êï∞ÊçÆÈ©±Âä®ÊéßÂà∂` `ÂÆâÂÖ®Á≠ñÁï•` `Êú∫Âô®Â≠¶‰π†` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ëá™Âä®È©æÈ©∂` `ÂáΩÊï∞ÈÄºËøë` `ÈôçÈò∂Âª∫Ê®°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊï∞ÊçÆÈ©±Âä®ÊéßÂà∂Á≠ñÁï•Âú®ÂìçÂ∫îÊó∂Èó¥„ÄÅËÆ°ÁÆóÈúÄÊ±ÇÂíåÂÜÖÂ≠òÈúÄÊ±Ç‰∏äÂ≠òÂú®ÊòæËëóÈôêÂà∂ÔºåÈöæ‰ª•Êª°Ë∂≥Âø´ÈÄüÂä®ÊÄÅÁ≥ªÁªüÁöÑÂÆûÈôÖÂ∫îÁî®„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫‰∫ÜÂÖ´ÁßçÊñπÊ≥ïÔºåÂåÖÊã¨ÈôçÈò∂Âª∫Ê®°„ÄÅÂáΩÊï∞ÈÄºËøëÁ≠ñÁï•Â≠¶‰π†ÂíåÂá∏ÊùæÂºõÁ≠âÔºåÊó®Âú®Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÊÄßÔºåÊèêÈ´òÊéßÂà∂Á≠ñÁï•ÁöÑÂÆâÂÖ®ÊÄß‰∏éÊïàÁéá„ÄÇ
3. ÈÄöËøáÂú®ÁúüÂÆûÂ∫îÁî®‰∏≠È™åËØÅËøô‰∫õÊñπÊ≥ïÔºåÁ†îÁ©∂ÊòæÁ§∫Âú®Êú∫Âô®‰∫∫ËáÇ„ÄÅËΩØÊú∫Âô®‰∫∫ÂíåËΩ¶ËæÜËøêÂä®ÊéßÂà∂Á≠âÂú∫ÊôØ‰∏≠ÔºåÊÄßËÉΩÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞‰ª£ÊéßÂà∂Â∫îÁî®ÔºåÂ∞§ÂÖ∂ÊòØÂú®Êú∫Âô®‰∫∫ÂíåËΩ¶ËæÜËøêÂä®ÊéßÂà∂‰∏≠ÔºåÈù¢‰∏¥ÁùÄÂÆûÁé∞ÂáÜÁ°Æ„ÄÅÂø´ÈÄüÂíåÂÆâÂÖ®ËøêÂä®ÁöÑÊåëÊàò„ÄÇ‰∏∫Ê≠§ÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÂºÄÂèë‰∫ÜÊúÄ‰ºòÊéßÂà∂Á≠ñÁï•Ôºå‰ª•Á°Æ‰øùÂÆâÂÖ®ÊÄßÂπ∂ÊèêÂçáÊÄßËÉΩ„ÄÇÂ∞ΩÁÆ°Ê®°ÂûãÈ¢ÑÊµãÊéßÂà∂ÔºàMPCÔºâÂú®Â§ÑÁêÜÂÆâÂÖ®Á∫¶ÊùüÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂ§çÊùÇÁ≥ªÁªüÁöÑÂáÜÁ°ÆÂª∫Ê®°‰ªçÁÑ∂Âõ∞Èöæ„ÄÇÂõ†Ê≠§ÔºåÊï∞ÊçÆÈ©±Âä®ÁöÑÊõø‰ª£ÊñπÊ°àÂ∫îËøêËÄåÁîü„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂÖ´ÁßçÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂáèÂ∞ëËÆ°ÁÆóÂ§çÊùÇÊÄßÔºåÊèêÂçáÊï∞ÊçÆÈ©±Âä®ÊéßÂà∂Á≠ñÁï•Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊïàÁéáÂíåÂÆâÂÖ®ÊÄßÔºåÊ∂µÁõñ‰∫ÜÊú∫Âô®‰∫∫ËáÇ„ÄÅËΩØÊú∫Âô®‰∫∫ÂíåËΩ¶ËæÜËøêÂä®ÊéßÂà∂Á≠âÈ¢ÜÂüü„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êï∞ÊçÆÈ©±Âä®ÊéßÂà∂Á≠ñÁï•Âú®Âø´ÈÄüÂä®ÊÄÅÁ≥ªÁªü‰∏≠Èù¢‰∏¥ÁöÑÂìçÂ∫îÊó∂Èó¥ÊÖ¢„ÄÅËÆ°ÁÆóÈúÄÊ±ÇÈ´òÂíåÂÜÖÂ≠òÈúÄÊ±ÇÂ§ßÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Èöæ‰ª•Êª°Ë∂≥Ëøô‰∫õË¶ÅÊ±Ç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊèêÂá∫ÂÖ´ÁßçÊñ∞ÊñπÊ≥ïÔºåÂáèÂ∞ëÂØπÂ§çÊùÇÊ®°ÂûãÁöÑ‰æùËµñÔºå‰ªéËÄåÊèêÂçáÊéßÂà∂Á≠ñÁï•ÁöÑÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇËøô‰∫õÊñπÊ≥ïÂà©Áî®Êï∞ÊçÆÈ©±Âä®ÁöÑÁâπÊÄßÔºåÁõ¥Êé•‰ªéËæìÂÖ•ËæìÂá∫Êï∞ÊçÆ‰∏≠Â≠¶‰π†ÂÆâÂÖ®Á≠ñÁï•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅÊ®°ÂûãÂ≠¶‰π†„ÄÅÁ≠ñÁï•‰ºòÂåñÂíåÂÆâÂÖ®ÊÄßÈ™åËØÅÂõõ‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÊï∞ÊçÆÊî∂ÈõÜÈò∂ÊÆµËé∑ÂèñÁ≥ªÁªüÁöÑËæìÂÖ•ËæìÂá∫Êï∞ÊçÆÔºåÊ®°ÂûãÂ≠¶‰π†Èò∂ÊÆµÈÄöËøáÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÊûÑÂª∫ÊéßÂà∂Ê®°ÂûãÔºåÁ≠ñÁï•‰ºòÂåñÈò∂ÊÆµÂàôÂà©Áî®‰ºòÂåñÁÆóÊ≥ïÁîüÊàêÊéßÂà∂Á≠ñÁï•ÔºåÊúÄÂêéËøõË°åÂÆâÂÖ®ÊÄßÈ™åËØÅ‰ª•Á°Æ‰øùÁ≠ñÁï•ÁöÑÂèØË°åÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÂ§öÁßçÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÊÄßÁöÑÊñπÊ≥ïÔºåÂ¶ÇÈôçÈò∂Âª∫Ê®°ÂíåÂáΩÊï∞ÈÄºËøëÁ≠ñÁï•Â≠¶‰π†ÔºåËøô‰∫õÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÊ®°ÂûãÈ¢ÑÊµãÊéßÂà∂ÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Â§ÑÁêÜÂ§çÊùÇÁ≥ªÁªüÁöÑÊéßÂà∂ÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊñπÊ≥ïËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•Âπ≥Ë°°ÊÄßËÉΩ‰∏éÂÆâÂÖ®ÊÄßÔºåÁΩëÁªúÁªìÊûÑÂàôÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ËøõË°å‰ºòÂåñÔºåÁ°Æ‰øùÂú®Â§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÊó∂ÁöÑÈ´òÊïàÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ÁúüÂÆûÂ∫îÁî®Âú∫ÊôØ‰∏≠ÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Êú∫Âô®‰∫∫ËáÇÂíåËΩ¶ËæÜËøêÂä®ÊéßÂà∂‰∏≠ÔºåÂìçÂ∫îÊó∂Èó¥Âπ≥ÂùáÂáèÂ∞ë‰∫Ü30%ÔºåËÆ°ÁÆóÈúÄÊ±ÇÈôç‰Ωé‰∫Ü40%ÔºåÊòæËëóÊèêÂçá‰∫ÜÁ≥ªÁªüÁöÑÂÆûÁî®ÊÄß‰∏éÂÆâÂÖ®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂ËΩ¶ËæÜÂíåÂ∑•‰∏öËá™Âä®ÂåñÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊï∞ÊçÆÈ©±Âä®ÊéßÂà∂Á≠ñÁï•ÁöÑÂÆâÂÖ®ÊÄß‰∏éÊïàÁéáÔºåËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÆûÁé∞Êõ¥Âø´ÈÄü„ÄÅÊõ¥ÂÆâÂÖ®ÁöÑÂÜ≥Á≠ñÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑÂèëÂ±ï‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> One of the main challenges in modern control applications, particularly in robot and vehicle motion control, is achieving accurate, fast, and safe movement. To address this, optimal control policies have been developed to enforce safety while ensuring high performance. Since basic first-principles models of real systems are often available, model-based controllers are widely used. Model predictive control (MPC) is a leading approach that optimizes performance while explicitly handling safety constraints. However, obtaining accurate models for complex systems is difficult, which motivates data-driven alternatives. ML-based MPC leverages learned models to reduce reliance on hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal policies directly from interaction data. Data-enabled predictive control (DeePC) goes further by bypassing modeling altogether, directly learning safe policies from raw input-output data. Recently, large language model (LLM) agents have also emerged, translating natural language instructions into structured formulations of optimal control problems. Despite these advances, data-driven policies face significant limitations. They often suffer from slow response times, high computational demands, and large memory needs, making them less practical for real-world systems with fast dynamics, limited onboard computing, or strict memory constraints. To address this, various technique, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, have been proposed to reduce computational complexity. In this paper, we present eight such approaches and demonstrate their effectiveness across real-world applications, including robotic arms, soft robots, and vehicle motion control.

