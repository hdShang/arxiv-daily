---
layout: default
title: From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents
---

# From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04076" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.04076v1</a>
  <a href="https://arxiv.org/pdf/2510.04076.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04076v1" onclick="toggleFavorite(this, '2510.04076v1', 'From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amin Vahidi-Moghaddam, Sayed Pedram Haeri Boroujeni, Iman Jebellat, Ehsan Jebellat, Niloufar Mehrabi, Zhaojian Li

**åˆ†ç±»**: cs.RO, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-10-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå…«ç§æ–¹æ³•ä»¥æå‡æ•°æ®é©±åŠ¨æ§åˆ¶ç­–ç•¥çš„å®‰å…¨æ€§ä¸æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `æ•°æ®é©±åŠ¨æ§åˆ¶` `å®‰å…¨ç­–ç•¥` `æœºå™¨å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `è‡ªåŠ¨é©¾é©¶` `å‡½æ•°é€¼è¿‘` `é™é˜¶å»ºæ¨¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ•°æ®é©±åŠ¨æ§åˆ¶ç­–ç•¥åœ¨å“åº”æ—¶é—´ã€è®¡ç®—éœ€æ±‚å’Œå†…å­˜éœ€æ±‚ä¸Šå­˜åœ¨æ˜¾è‘—é™åˆ¶ï¼Œéš¾ä»¥æ»¡è¶³å¿«é€ŸåŠ¨æ€ç³»ç»Ÿçš„å®é™…åº”ç”¨ã€‚
2. è®ºæ–‡æå‡ºäº†å…«ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬é™é˜¶å»ºæ¨¡ã€å‡½æ•°é€¼è¿‘ç­–ç•¥å­¦ä¹ å’Œå‡¸æ¾å¼›ç­‰ï¼Œæ—¨åœ¨é™ä½è®¡ç®—å¤æ‚æ€§ï¼Œæé«˜æ§åˆ¶ç­–ç•¥çš„å®‰å…¨æ€§ä¸æ•ˆç‡ã€‚
3. é€šè¿‡åœ¨çœŸå®åº”ç”¨ä¸­éªŒè¯è¿™äº›æ–¹æ³•ï¼Œç ”ç©¶æ˜¾ç¤ºåœ¨æœºå™¨äººè‡‚ã€è½¯æœºå™¨äººå’Œè½¦è¾†è¿åŠ¨æ§åˆ¶ç­‰åœºæ™¯ä¸­ï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£æ§åˆ¶åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººå’Œè½¦è¾†è¿åŠ¨æ§åˆ¶ä¸­ï¼Œé¢ä¸´ç€å®ç°å‡†ç¡®ã€å¿«é€Ÿå’Œå®‰å…¨è¿åŠ¨çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†æœ€ä¼˜æ§åˆ¶ç­–ç•¥ï¼Œä»¥ç¡®ä¿å®‰å…¨æ€§å¹¶æå‡æ€§èƒ½ã€‚å°½ç®¡æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰åœ¨å¤„ç†å®‰å…¨çº¦æŸæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¤æ‚ç³»ç»Ÿçš„å‡†ç¡®å»ºæ¨¡ä»ç„¶å›°éš¾ã€‚å› æ­¤ï¼Œæ•°æ®é©±åŠ¨çš„æ›¿ä»£æ–¹æ¡ˆåº”è¿è€Œç”Ÿã€‚æœ¬æ–‡æå‡ºäº†å…«ç§æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘è®¡ç®—å¤æ‚æ€§ï¼Œæå‡æ•°æ®é©±åŠ¨æ§åˆ¶ç­–ç•¥åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œæ¶µç›–äº†æœºå™¨äººè‡‚ã€è½¯æœºå™¨äººå’Œè½¦è¾†è¿åŠ¨æ§åˆ¶ç­‰é¢†åŸŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ•°æ®é©±åŠ¨æ§åˆ¶ç­–ç•¥åœ¨å¿«é€ŸåŠ¨æ€ç³»ç»Ÿä¸­é¢ä¸´çš„å“åº”æ—¶é—´æ…¢ã€è®¡ç®—éœ€æ±‚é«˜å’Œå†…å­˜éœ€æ±‚å¤§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥æ»¡è¶³è¿™äº›è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æå‡ºå…«ç§æ–°æ–¹æ³•ï¼Œå‡å°‘å¯¹å¤æ‚æ¨¡å‹çš„ä¾èµ–ï¼Œä»è€Œæå‡æ§åˆ¶ç­–ç•¥çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚è¿™äº›æ–¹æ³•åˆ©ç”¨æ•°æ®é©±åŠ¨çš„ç‰¹æ€§ï¼Œç›´æ¥ä»è¾“å…¥è¾“å‡ºæ•°æ®ä¸­å­¦ä¹ å®‰å…¨ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹å­¦ä¹ ã€ç­–ç•¥ä¼˜åŒ–å’Œå®‰å…¨æ€§éªŒè¯å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é˜¶æ®µè·å–ç³»ç»Ÿçš„è¾“å…¥è¾“å‡ºæ•°æ®ï¼Œæ¨¡å‹å­¦ä¹ é˜¶æ®µé€šè¿‡æœºå™¨å­¦ä¹ æŠ€æœ¯æ„å»ºæ§åˆ¶æ¨¡å‹ï¼Œç­–ç•¥ä¼˜åŒ–é˜¶æ®µåˆ™åˆ©ç”¨ä¼˜åŒ–ç®—æ³•ç”Ÿæˆæ§åˆ¶ç­–ç•¥ï¼Œæœ€åè¿›è¡Œå®‰å…¨æ€§éªŒè¯ä»¥ç¡®ä¿ç­–ç•¥çš„å¯è¡Œæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†å¤šç§é™ä½è®¡ç®—å¤æ‚æ€§çš„æ–¹æ³•ï¼Œå¦‚é™é˜¶å»ºæ¨¡å’Œå‡½æ•°é€¼è¿‘ç­–ç•¥å­¦ä¹ ï¼Œè¿™äº›æ–¹æ³•ä¸ä¼ ç»Ÿçš„æ¨¡å‹é¢„æµ‹æ§åˆ¶æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚ç³»ç»Ÿçš„æ§åˆ¶é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ–¹æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡æ€§èƒ½ä¸å®‰å…¨æ€§ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶çš„é«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªçœŸå®åº”ç”¨åœºæ™¯ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººè‡‚å’Œè½¦è¾†è¿åŠ¨æ§åˆ¶ä¸­ï¼Œå“åº”æ—¶é—´å¹³å‡å‡å°‘äº†30%ï¼Œè®¡ç®—éœ€æ±‚é™ä½äº†40%ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å®ç”¨æ€§ä¸å®‰å…¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶è½¦è¾†å’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚é€šè¿‡æå‡æ•°æ®é©±åŠ¨æ§åˆ¶ç­–ç•¥çš„å®‰å…¨æ€§ä¸æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´å¿«é€Ÿã€æ›´å®‰å…¨çš„å†³ç­–ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> One of the main challenges in modern control applications, particularly in robot and vehicle motion control, is achieving accurate, fast, and safe movement. To address this, optimal control policies have been developed to enforce safety while ensuring high performance. Since basic first-principles models of real systems are often available, model-based controllers are widely used. Model predictive control (MPC) is a leading approach that optimizes performance while explicitly handling safety constraints. However, obtaining accurate models for complex systems is difficult, which motivates data-driven alternatives. ML-based MPC leverages learned models to reduce reliance on hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal policies directly from interaction data. Data-enabled predictive control (DeePC) goes further by bypassing modeling altogether, directly learning safe policies from raw input-output data. Recently, large language model (LLM) agents have also emerged, translating natural language instructions into structured formulations of optimal control problems. Despite these advances, data-driven policies face significant limitations. They often suffer from slow response times, high computational demands, and large memory needs, making them less practical for real-world systems with fast dynamics, limited onboard computing, or strict memory constraints. To address this, various technique, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, have been proposed to reduce computational complexity. In this paper, we present eight such approaches and demonstrate their effectiveness across real-world applications, including robotic arms, soft robots, and vehicle motion control.

