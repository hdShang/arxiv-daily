---
layout: default
title: ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context
---

# ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04246" target="_blank" class="toolbar-btn">arXiv: 2510.04246v1</a>
    <a href="https://arxiv.org/pdf/2510.04246.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04246v1" 
            onclick="toggleFavorite(this, '2510.04246v1', 'ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Huiwon Jang, Sihyun Yu, Heeseung Kwon, Hojin Jeon, Younggyo Seo, Jinwoo Shin

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-05

**Â§áÊ≥®**: Project page: https://huiwon-jang.github.io/contextvla

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ContextVLAÔºöÈÄöËøáÂàÜÊëäÂ§öÂ∏ß‰∏ä‰∏ãÊñáÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºåÊèêÂçáÊú∫Âô®‰∫∫‰ªªÂä°ÊÄßËÉΩ„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫` `ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã` `Êó∂Èó¥‰∏ä‰∏ãÊñá` `Â§öÂ∏ßËßÇÊµã` `Ë°å‰∏∫ÂÖãÈöÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Âú®ÈÉ®ÂàÜÂèØËßÇÊµãÁöÑÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÔºåÂà©Áî®Êó∂Èó¥‰∏ä‰∏ãÊñáËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÁé∞ÊúâÂü∫‰∫éË°å‰∏∫ÂÖãÈöÜÁöÑÊñπÊ≥ïÂú®‰ΩøÁî®Â§öÂ∏ßËßÇÊµãÊó∂ÊÄßËÉΩÊèêÂçá‰∏çÁ®≥ÂÆö„ÄÇ
2. ContextVLAÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÂ§öÂ∏ßËßÇÊµãÂéãÁº©ÊàêÂçï‰∏™‰∏ä‰∏ãÊñátokenÔºå‰ªéËÄåÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÂêåÊó∂‰øùÁïôVLMÁöÑÊó∂Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåContextVLAÂú®ÊèêÂçáÊú∫Âô®‰∫∫‰ªªÂä°ÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÈôç‰Ωé‰∫ÜËÆ≠ÁªÉÂíåÊé®ÁêÜÊó∂Èó¥Ôºå‰ºò‰∫éÂçïÂ∏ßVLA„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫ContextVLAÔºå‰∏ÄÁßçËÉΩÂ§üÊúâÊïàÂà©Áî®Â§öÂ∏ßËßÇÊµãÊù•Á®≥ÂÅ•ÊèêÂçáÊú∫Âô®‰∫∫‰ªªÂä°ÊÄßËÉΩÁöÑÁ≠ñÁï•Ê®°Âûã„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫é‰∏Ä‰∏™ÂÖ≥ÈîÆËßÇÂØüÔºöÊûÑÂª∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)‰πã‰∏äÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã(VLA)Âú®Âä®‰ΩúÁîüÊàê‰∏≠ËÉΩÊõ¥ÊúâÊïàÂú∞Âà©Áî®Â§öÂ∏ßËßÇÊµã„ÄÇËøôË°®ÊòéVLMÂõ∫ÊúâÁöÑÊó∂Èó¥ÁêÜËß£ËÉΩÂäõ‰ΩøÂÖ∂ËÉΩÂ§ü‰ªéÂ§öÂ∏ßËßÇÊµã‰∏≠ÊèêÂèñÊõ¥ÊúâÊÑè‰πâÁöÑ‰∏ä‰∏ãÊñá„ÄÇÁÑ∂ËÄåÔºåËßÜÈ¢ëËæìÂÖ•ÁöÑÈ´òÁª¥Â∫¶Â∏¶Êù•‰∫ÜÊòæËëóÁöÑËÆ°ÁÆóÂºÄÈîÄÔºå‰ΩøÂæóVLAÁöÑËÆ≠ÁªÉÂíåÊé®ÁêÜÊïàÁéá‰Ωé‰∏ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåContextVLAÂ∞ÜËøáÂéªÁöÑËßÇÊµãÂéãÁº©Êàê‰∏Ä‰∏™Âçï‰∏ÄÁöÑ‰∏ä‰∏ãÊñátokenÔºåÂÖÅËÆ∏Á≠ñÁï•Ê®°ÂûãÊúâÊïàÂú∞Âà©Áî®Êó∂Èó¥‰∏ä‰∏ãÊñáËøõË°åÂä®‰ΩúÁîüÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºåContextVLAÂßãÁªà‰ºò‰∫éÂçïÂ∏ßVLAÔºåÂπ∂ÂÆûÁé∞‰∫ÜÂÆåÊï¥Â§öÂ∏ßËÆ≠ÁªÉÁöÑ‰ºòÂäøÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉÂíåÊé®ÁêÜÊó∂Èó¥„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®ÈÉ®ÂàÜÂèØËßÇÊµãÁöÑÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÔºåÂ¶Ç‰ΩïÊúâÊïàÂà©Áî®Â§öÂ∏ßËßÇÊµãÊù•ÊèêÂçáÁ≠ñÁï•Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂü∫‰∫éË°å‰∏∫ÂÖãÈöÜÁöÑÊñπÊ≥ïÔºåÂú®‰ΩøÁî®Â§öÂ∏ßËßÇÊµãÊó∂ÊÄßËÉΩÊèêÂçá‰∏çÁ®≥ÂÆöÔºå‰∏îÁõ¥Êé•‰ΩøÁî®Â§öÂ∏ßËßÜÈ¢ë‰Ωú‰∏∫ËæìÂÖ•‰ºöÂØºËá¥ËÆ°ÁÆóÊàêÊú¨ËøáÈ´ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂõ∫ÊúâÁöÑÊó∂Èó¥ÁêÜËß£ËÉΩÂäõÔºåÂ∞ÜÂ§öÂ∏ßËßÇÊµãÂéãÁº©Êàê‰∏Ä‰∏™Âçï‰∏ÄÁöÑ‰∏ä‰∏ãÊñátoken„ÄÇËøôÊ†∑Êó¢ËÉΩ‰øùÁïôÊó∂Èó¥‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂèàËÉΩÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶Ôºå‰ªéËÄåÊèêÈ´òËÆ≠ÁªÉÂíåÊé®ÁêÜÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöContextVLAÊ®°ÂûãÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) ËßÜËßâÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÊèêÂèñÊØè‰∏ÄÂ∏ßÂõæÂÉèÁöÑËßÜËßâÁâπÂæÅ„ÄÇ2) ‰∏ä‰∏ãÊñáÂéãÁº©Âô®ÔºöÂ∞ÜÂ§öÂ∏ßËßÜËßâÁâπÂæÅÂéãÁº©Êàê‰∏Ä‰∏™‰∏ä‰∏ãÊñátoken„ÄÇ3) ËØ≠Ë®ÄÁºñÁ†ÅÂô®ÔºöÁî®‰∫éÁºñÁ†Å‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËØ≠Ë®ÄÊåá‰ª§„ÄÇ4) Âä®‰ΩúËß£Á†ÅÂô®ÔºöÁªìÂêà‰∏ä‰∏ãÊñátokenÂíåËØ≠Ë®ÄÊåá‰ª§ÔºåÁîüÊàêÊú∫Âô®‰∫∫Âä®‰Ωú„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºåÈ¶ñÂÖà‰ΩøÁî®ËßÜËßâÁºñÁ†ÅÂô®Â§ÑÁêÜÂ§öÂ∏ßÂõæÂÉèÔºåÁÑ∂Âêé‰ΩøÁî®‰∏ä‰∏ãÊñáÂéãÁº©Âô®Â∞ÜÂ§öÂ∏ß‰ø°ÊÅØÂéãÁº©ÊàêÂçï‰∏™tokenÔºåÊé•ÁùÄÁªìÂêàËØ≠Ë®ÄÊåá‰ª§ÔºåÊúÄÂêéÁî±Âä®‰ΩúËß£Á†ÅÂô®ÁîüÊàêÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöContextVLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ΩøÁî®ÂàÜÊëäÔºàamortizedÔºâÁöÑÊñπÂºèÂ≠¶‰π†‰∏ä‰∏ãÊñáÂéãÁº©Âô®ÔºåÂ∞ÜÂ§öÂ∏ß‰ø°ÊÅØÂéãÁº©ÊàêÂçï‰∏™token„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÁõ¥Êé•Â§ÑÁêÜÈ´òÁª¥ËßÜÈ¢ëËæìÂÖ•ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÂêåÊó∂‰øùÁïô‰∫ÜVLMÁöÑÊó∂Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ‰∏éÁõ¥Êé•‰ΩøÁî®Â§öÂ∏ßÂõæÂÉè‰Ωú‰∏∫ËæìÂÖ•Áõ∏ÊØîÔºåContextVLAÊõ¥Âä†È´òÊïà„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö‰∏ä‰∏ãÊñáÂéãÁº©Âô®ÂèØËÉΩÈááÁî®TransformerÁªìÊûÑÔºåÈÄöËøáËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Â≠¶‰π†Â∏ß‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂Â∞ÜÂ§öÂ∏ß‰ø°ÊÅØËÅöÂêà‰∏∫‰∏Ä‰∏™‰∏ä‰∏ãÊñáÂêëÈáè„ÄÇÊçüÂ§±ÂáΩÊï∞ÂèØËÉΩÂåÖÂê´Ë°å‰∏∫ÂÖãÈöÜÊçüÂ§±ÔºåÁî®‰∫éÊ®°‰ªø‰∏ìÂÆ∂Á≠ñÁï•ÁöÑÂä®‰ΩúÔºå‰ª•ÂèäÂØπÊØîÂ≠¶‰π†ÊçüÂ§±ÔºåÁî®‰∫éÈºìÂä±‰∏ä‰∏ãÊñátoken‰øùÁïôÂÖ≥ÈîÆÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂú®ËÆ∫Êñá‰∏≠ÂèØËÉΩÊúâÊâÄËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ContextVLAÂú®Â§ö‰∏™Êú∫Âô®‰∫∫‰ªªÂä°‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéÔºåContextVLAÂßãÁªà‰ºò‰∫éÂçïÂ∏ßVLAÔºåÂπ∂‰∏îÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÊó∂Èó¥‰∏äÈÉΩ‰ºò‰∫éÁõ¥Êé•‰ΩøÁî®Â§öÂ∏ßÂõæÂÉè‰Ωú‰∏∫ËæìÂÖ•ÁöÑVLAÊ®°Âûã„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ÂíåÂØπÊØîÂü∫Á∫øÂú®ËÆ∫Êñá‰∏≠ÂèØËÉΩÊúâÊâÄËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ContextVLAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊó∂Èó¥‰∏ä‰∏ãÊñá‰ø°ÊÅØÁöÑÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÔºöÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÁ≠â„ÄÇËØ•Á†îÁ©∂ÊúâÂä©‰∫éÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊÑüÁü•ÂíåÂÜ≥Á≠ñËÉΩÂäõÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂÆåÊàêÂêÑÁßç‰ªªÂä°ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÊú™Êù•ÂèëÂ±ïÊΩúÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.

