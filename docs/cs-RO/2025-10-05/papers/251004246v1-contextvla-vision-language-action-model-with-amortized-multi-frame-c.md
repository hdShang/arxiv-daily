---
layout: default
title: ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context
---

# ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04246" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.04246v1</a>
  <a href="https://arxiv.org/pdf/2510.04246.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04246v1" onclick="toggleFavorite(this, '2510.04246v1', 'ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Huiwon Jang, Sihyun Yu, Heeseung Kwon, Hojin Jeon, Younggyo Seo, Jinwoo Shin

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-05

**å¤‡æ³¨**: Project page: https://huiwon-jang.github.io/contextvla

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ContextVLAï¼šé€šè¿‡åˆ†æ‘Šå¤šå¸§ä¸Šä¸‹æ–‡çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæå‡æœºå™¨äººä»»åŠ¡æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äºº` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `æ—¶é—´ä¸Šä¸‹æ–‡` `å¤šå¸§è§‚æµ‹` `è¡Œä¸ºå…‹éš†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åœ¨éƒ¨åˆ†å¯è§‚æµ‹çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨æ—¶é—´ä¸Šä¸‹æ–‡è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰åŸºäºè¡Œä¸ºå…‹éš†çš„æ–¹æ³•åœ¨ä½¿ç”¨å¤šå¸§è§‚æµ‹æ—¶æ€§èƒ½æå‡ä¸ç¨³å®šã€‚
2. ContextVLAçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤šå¸§è§‚æµ‹å‹ç¼©æˆå•ä¸ªä¸Šä¸‹æ–‡tokenï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿ç•™VLMçš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒContextVLAåœ¨æå‡æœºå™¨äººä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½äº†è®­ç»ƒå’Œæ¨ç†æ—¶é—´ï¼Œä¼˜äºå•å¸§VLAã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºContextVLAï¼Œä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šå¸§è§‚æµ‹æ¥ç¨³å¥æå‡æœºå™¨äººä»»åŠ¡æ€§èƒ½çš„ç­–ç•¥æ¨¡å‹ã€‚è¯¥æ–¹æ³•åŸºäºä¸€ä¸ªå…³é”®è§‚å¯Ÿï¼šæ„å»ºäºè§†è§‰-è¯­è¨€æ¨¡å‹(VLM)ä¹‹ä¸Šçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(VLA)åœ¨åŠ¨ä½œç”Ÿæˆä¸­èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šå¸§è§‚æµ‹ã€‚è¿™è¡¨æ˜VLMå›ºæœ‰çš„æ—¶é—´ç†è§£èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿä»å¤šå¸§è§‚æµ‹ä¸­æå–æ›´æœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œè§†é¢‘è¾“å…¥çš„é«˜ç»´åº¦å¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œä½¿å¾—VLAçš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒContextVLAå°†è¿‡å»çš„è§‚æµ‹å‹ç¼©æˆä¸€ä¸ªå•ä¸€çš„ä¸Šä¸‹æ–‡tokenï¼Œå…è®¸ç­–ç•¥æ¨¡å‹æœ‰æ•ˆåœ°åˆ©ç”¨æ—¶é—´ä¸Šä¸‹æ–‡è¿›è¡ŒåŠ¨ä½œç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒContextVLAå§‹ç»ˆä¼˜äºå•å¸§VLAï¼Œå¹¶å®ç°äº†å®Œæ•´å¤šå¸§è®­ç»ƒçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶å‡å°‘äº†è®­ç»ƒå’Œæ¨ç†æ—¶é—´ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åœ¨éƒ¨åˆ†å¯è§‚æµ‹çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤šå¸§è§‚æµ‹æ¥æå‡ç­–ç•¥æ¨¡å‹çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºè¡Œä¸ºå…‹éš†çš„æ–¹æ³•ï¼Œåœ¨ä½¿ç”¨å¤šå¸§è§‚æµ‹æ—¶æ€§èƒ½æå‡ä¸ç¨³å®šï¼Œä¸”ç›´æ¥ä½¿ç”¨å¤šå¸§è§†é¢‘ä½œä¸ºè¾“å…¥ä¼šå¯¼è‡´è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å›ºæœ‰çš„æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œå°†å¤šå¸§è§‚æµ‹å‹ç¼©æˆä¸€ä¸ªå•ä¸€çš„ä¸Šä¸‹æ–‡tokenã€‚è¿™æ ·æ—¢èƒ½ä¿ç•™æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåˆèƒ½æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œæé«˜è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šContextVLAæ¨¡å‹åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼šç”¨äºæå–æ¯ä¸€å¸§å›¾åƒçš„è§†è§‰ç‰¹å¾ã€‚2) ä¸Šä¸‹æ–‡å‹ç¼©å™¨ï¼šå°†å¤šå¸§è§†è§‰ç‰¹å¾å‹ç¼©æˆä¸€ä¸ªä¸Šä¸‹æ–‡tokenã€‚3) è¯­è¨€ç¼–ç å™¨ï¼šç”¨äºç¼–ç ä»»åŠ¡ç›¸å…³çš„è¯­è¨€æŒ‡ä»¤ã€‚4) åŠ¨ä½œè§£ç å™¨ï¼šç»“åˆä¸Šä¸‹æ–‡tokenå’Œè¯­è¨€æŒ‡ä»¤ï¼Œç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆä½¿ç”¨è§†è§‰ç¼–ç å™¨å¤„ç†å¤šå¸§å›¾åƒï¼Œç„¶åä½¿ç”¨ä¸Šä¸‹æ–‡å‹ç¼©å™¨å°†å¤šå¸§ä¿¡æ¯å‹ç¼©æˆå•ä¸ªtokenï¼Œæ¥ç€ç»“åˆè¯­è¨€æŒ‡ä»¤ï¼Œæœ€åç”±åŠ¨ä½œè§£ç å™¨ç”ŸæˆåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šContextVLAçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨åˆ†æ‘Šï¼ˆamortizedï¼‰çš„æ–¹å¼å­¦ä¹ ä¸Šä¸‹æ–‡å‹ç¼©å™¨ï¼Œå°†å¤šå¸§ä¿¡æ¯å‹ç¼©æˆå•ä¸ªtokenã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥å¤„ç†é«˜ç»´è§†é¢‘è¾“å…¥ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿ç•™äº†VLMçš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚ä¸ç›´æ¥ä½¿ç”¨å¤šå¸§å›¾åƒä½œä¸ºè¾“å…¥ç›¸æ¯”ï¼ŒContextVLAæ›´åŠ é«˜æ•ˆã€‚

**å…³é”®è®¾è®¡**ï¼šä¸Šä¸‹æ–‡å‹ç¼©å™¨å¯èƒ½é‡‡ç”¨Transformerç»“æ„ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ å¸§ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å°†å¤šå¸§ä¿¡æ¯èšåˆä¸ºä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡ã€‚æŸå¤±å‡½æ•°å¯èƒ½åŒ…å«è¡Œä¸ºå…‹éš†æŸå¤±ï¼Œç”¨äºæ¨¡ä»¿ä¸“å®¶ç­–ç•¥çš„åŠ¨ä½œï¼Œä»¥åŠå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œç”¨äºé¼“åŠ±ä¸Šä¸‹æ–‡tokenä¿ç•™å…³é”®çš„æ—¶é—´ä¿¡æ¯ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ContextVLAåœ¨å¤šä¸ªæœºå™¨äººä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒContextVLAå§‹ç»ˆä¼˜äºå•å¸§VLAï¼Œå¹¶ä¸”åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä¸Šéƒ½ä¼˜äºç›´æ¥ä½¿ç”¨å¤šå¸§å›¾åƒä½œä¸ºè¾“å…¥çš„VLAæ¨¡å‹ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦å’Œå¯¹æ¯”åŸºçº¿åœ¨è®ºæ–‡ä¸­å¯èƒ½æœ‰æ‰€è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ContextVLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§éœ€è¦æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚ï¼šå®¶åº­æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå„ç§ä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæœªæ¥å‘å±•æ½œåŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.

