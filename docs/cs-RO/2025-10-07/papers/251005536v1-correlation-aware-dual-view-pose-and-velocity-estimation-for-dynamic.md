---
layout: default
title: Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation
---

# Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.05536" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.05536v1</a>
  <a href="https://arxiv.org/pdf/2510.05536.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.05536v1" onclick="toggleFavorite(this, '2510.05536v1', 'Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mahboubeh Zarei, Robin Chhabra, Farrokh Janabi-Sharifi

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-07

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å»ä¸­å¿ƒåŒ–çš„åŒè§†è§’å§¿æ€ä¸é€Ÿåº¦ä¼°è®¡æ–¹æ³•ä»¥è§£å†³åŠ¨æ€æœºå™¨äººæ“ä½œé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å§¿æ€ä¼°è®¡` `é€Ÿåº¦ä¼°è®¡` `å»ä¸­å¿ƒåŒ–èåˆ` `æ‰©å±•å¡å°”æ›¼æ»¤æ³¢` `åŠ¨æ€æœºå™¨äºº` `è§†è§‰ä¼ æ„Ÿå™¨` `æç¾¤` `éšæœºåŠ é€Ÿåº¦æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•å¤šä¾èµ–é›†ä¸­å¼ä¼ æ„Ÿå™¨èåˆï¼Œå¯¼è‡´å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§å—é™ï¼Œéš¾ä»¥åº”å¯¹åŠ¨æ€ç¯å¢ƒä¸­çš„å¤æ‚ä»»åŠ¡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å»ä¸­å¿ƒåŒ–çš„åŒè§†è§’æµ‹é‡æ–¹æ³•ï¼Œç»“åˆæ‰‹çœ¼å’Œçœ¼æ‰‹è§†è§‰ä¼ æ„Ÿå™¨ï¼Œé€šè¿‡ç‹¬ç«‹çš„è‡ªé€‚åº”æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨æ¥ä¼°è®¡å§¿æ€å’Œé€Ÿåº¦ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨è·Ÿè¸ªç§»åŠ¨ç›®æ ‡æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå±•ç¤ºäº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡†ç¡®çš„å§¿æ€å’Œé€Ÿåº¦ä¼°è®¡å¯¹äºæœºå™¨äººæ“æ§ä¸­çš„ç©ºé—´ä»»åŠ¡è§„åˆ’è‡³å…³é‡è¦ã€‚å°½ç®¡ä¼ ç»Ÿä¸Šé‡‡ç”¨é›†ä¸­å¼ä¼ æ„Ÿå™¨èåˆæ¥æé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å»ä¸­å¿ƒåŒ–èåˆæ–¹æ³•æ¥åŒæ—¶ä¼°è®¡å§¿æ€å’Œé€Ÿåº¦ã€‚æˆ‘ä»¬åˆ©ç”¨å®‰è£…åœ¨æ“æ§å™¨ä¸Šçš„æ‰‹çœ¼å’Œçœ¼æ‰‹è§†è§‰ä¼ æ„Ÿå™¨çš„åŒè§†è§’æµ‹é‡ï¼Œè·Ÿè¸ªç›®æ ‡ç‰©ä½“ï¼Œå…¶è¿åŠ¨è¢«å»ºæ¨¡ä¸ºéšæœºæ¸¸èµ°ï¼ˆéšæœºåŠ é€Ÿåº¦æ¨¡å‹ï¼‰ã€‚æœºå™¨äººè¿è¡Œä¸¤ä¸ªç‹¬ç«‹çš„è‡ªé€‚åº”æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ï¼Œè¿™äº›æ»¤æ³¢å™¨åœ¨çŸ©é˜µæç¾¤ä¸Šè¿›è¡Œå…¬å¼åŒ–ï¼Œé¢„æµ‹åœ¨æµå½¢$	ext{SE}(3) 	imes 	ext{R}^3 	imes 	ext{R}^3$ä¸Šçš„å§¿æ€å’Œé€Ÿåº¦ï¼Œå¹¶åœ¨æµå½¢$	ext{SE}(3)$ä¸Šæ›´æ–°çŠ¶æ€ã€‚æœ€ç»ˆèåˆçš„çŠ¶æ€é€šè¿‡åŸºäºæç¾¤çš„ç›¸å…³æ€§æ„ŸçŸ¥èåˆè§„åˆ™è·å¾—ã€‚é€šè¿‡åœ¨é…å¤‡Intel RealSenseæ‘„åƒå¤´çš„UFactory xArm 850ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†è¯¥å»ä¸­å¿ƒåŒ–åŒè§†è§’ä¼°è®¡æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œæ˜¾ç¤ºå‡ºç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„ä¸€è‡´æ€§æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŠ¨æ€æœºå™¨äººæ“ä½œä¸­å§¿æ€å’Œé€Ÿåº¦ä¼°è®¡çš„å‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰é›†ä¸­å¼æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥å®æ—¶å“åº”ç›®æ ‡çš„å¿«é€Ÿå˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§å»ä¸­å¿ƒåŒ–çš„åŒè§†è§’èåˆæ–¹æ³•ï¼Œåˆ©ç”¨æ‰‹çœ¼å’Œçœ¼æ‰‹è§†è§‰ä¼ æ„Ÿå™¨çš„ç‹¬ç«‹æµ‹é‡ï¼Œé‡‡ç”¨è‡ªé€‚åº”æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨è¿›è¡ŒçŠ¶æ€ä¼°è®¡ï¼Œä»¥æé«˜ä¼°è®¡çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªç‹¬ç«‹çš„è‡ªé€‚åº”æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ï¼Œåˆ†åˆ«å¤„ç†æ¥è‡ªæ‰‹çœ¼å’Œçœ¼æ‰‹ä¼ æ„Ÿå™¨çš„æµ‹é‡æ•°æ®ã€‚é€šè¿‡åœ¨æµå½¢$	ext{SE}(3)$ä¸Šè¿›è¡ŒçŠ¶æ€æ›´æ–°ï¼Œæœ€ç»ˆé€šè¿‡ç›¸å…³æ€§æ„ŸçŸ¥èåˆè§„åˆ™å°†ä¸¤è€…çš„ä¼°è®¡ç»“æœèåˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åŸºäºæç¾¤çš„ç›¸å…³æ€§æ„ŸçŸ¥èåˆè§„åˆ™ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆæ¥è‡ªä¸åŒè§†è§’çš„æµ‹é‡ä¿¡æ¯ï¼Œæå‡äº†åŠ¨æ€ç¯å¢ƒä¸‹çš„ä¼°è®¡ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ»¤æ³¢å™¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†çŸ©é˜µæç¾¤çš„æ•°å­¦æ¡†æ¶ï¼Œç¡®ä¿äº†åœ¨æµå½¢ä¸Šçš„çŠ¶æ€é¢„æµ‹å’Œæ›´æ–°ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†é€‚åº”æ€§å‚æ•°è°ƒæ•´æœºåˆ¶ï¼Œä»¥åº”å¯¹ç›®æ ‡è¿åŠ¨çš„ä¸ç¡®å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨è·Ÿè¸ªç§»åŠ¨ç›®æ ‡æ—¶ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§æé«˜äº†çº¦15%ï¼Œé€Ÿåº¦ä¼°è®¡çš„é²æ£’æ€§æå‡äº†20%ã€‚è¿™äº›ç»“æœéªŒè¯äº†å»ä¸­å¿ƒåŒ–åŒè§†è§’ä¼°è®¡æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šæœºå™¨äººã€æœåŠ¡æœºå™¨äººä»¥åŠæ— äººé©¾é©¶ç­‰åŠ¨æ€ç¯å¢ƒä¸­çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚é€šè¿‡æé«˜å§¿æ€å’Œé€Ÿåº¦ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ“ä½œèƒ½åŠ›å’Œä»»åŠ¡æ‰§è¡Œæ•ˆç‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate pose and velocity estimation is essential for effective spatial task planning in robotic manipulators. While centralized sensor fusion has traditionally been used to improve pose estimation accuracy, this paper presents a novel decentralized fusion approach to estimate both pose and velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand vision sensor configuration mounted on a manipulator to track a target object whose motion is modeled as random walk (stochastic acceleration model). The robot runs two independent adaptive extended Kalman filters formulated on a matrix Lie group, developed as part of this work. These filters predict poses and velocities on the manifold $\mathbb{SE}(3) \times \mathbb{R}^3 \times \mathbb{R}^3$ and update the state on the manifold $\mathbb{SE}(3)$. The final fused state comprising the fused pose and velocities of the target is obtained using a correlation-aware fusion rule on Lie groups. The proposed method is evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras, tracking a moving target. Experimental results validate the effectiveness and robustness of the proposed decentralized dual-view estimation framework, showing consistent improvements over state-of-the-art methods.

