---
layout: default
title: DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models
---

# DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2511.15669" target="_blank" class="toolbar-btn">arXiv: 2511.15669v1</a>
    <a href="https://arxiv.org/pdf/2511.15669.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15669v1" 
            onclick="toggleFavorite(this, '2511.15669v1', 'DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Cheng Yin, Yankai Lin, Wang Xu, Sikyuen Tam, Xiangrui Zeng, Zhiyuan Liu, Zhouping Yin

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-31

**Â§áÊ≥®**: 16 pages, 6 figures, conference

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**DeepThinkVLAÈÄöËøáÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂Âíå‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊèêÂçáVLAÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫ÊéßÂà∂` `ÈìæÂºèÊÄùËÄÉ` `Ê∑∑ÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂` `Âº∫ÂåñÂ≠¶‰π†` `Âõ†ÊûúÊé®ÁêÜ` `Â∫èÂàóÂÜ≥Á≠ñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°Âûã‰ΩøÁî®Âçï‰∏ÄËß£Á†ÅÂô®Â§ÑÁêÜÊé®ÁêÜÂíåÂä®‰ΩúÔºåÂØºËá¥ËøêÂä®ÊéßÂà∂‰∏ç‰Ω≥ÔºåÊé®ÁêÜ‰∏éÂä®‰ΩúÈó¥Âõ†ÊûúÂÖ≥Á≥ªÂº±„ÄÇ
2. DeepThinkVLAÈááÁî®Ê∑∑ÂêàÊ≥®ÊÑèÂäõËß£Á†ÅÂô®ÔºåÂÖàËøõË°åÂ∫èÂàóÊé®ÁêÜÔºåÂÜçÂπ∂Ë°åÁîüÊàêÂä®‰ΩúÔºåÂπ∂ÁªìÂêà‰∏§Èò∂ÊÆµËÆ≠ÁªÉ„ÄÇ
3. DeepThinkVLAÂú®LIBEROÊµãËØï‰∏≠ËææÂà∞97.0%ÁöÑÊàêÂäüÁéáÔºåÊ∑∑ÂêàÊû∂ÊûÑÊú¨Ë∫´ÊèêÂçá15.5%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫DeepThinkVLAÔºåÊó®Âú®ÊèêÂçáËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂÖãÊúçÁ´ØÂà∞Á´ØÊú∫Âô®‰∫∫Á≠ñÁï•ÂØπÊï∞ÊçÆÁöÑËøáÂ∫¶‰æùËµñ„ÄÇÁé∞ÊúâÊ®°Âûã‰ΩøÁî®Âçï‰∏ÄËá™ÂõûÂΩíËß£Á†ÅÂô®ËøõË°åÂ∫èÂàóÂåñÁöÑCoTÊé®ÁêÜÂíåÈ´òÁª¥Âπ∂Ë°åÊú∫Âô®‰∫∫Âä®‰ΩúÔºåÂØºËá¥ËøêÂä®ÊéßÂà∂ÊÄßËÉΩ‰∏ãÈôçÔºå‰∏îÊé®ÁêÜ‰∏éÂä®‰Ωú‰πãÈó¥Áº∫‰πèÂº∫Âõ†ÊûúÂÖ≥Á≥ª„ÄÇDeepThinkVLAÈÄöËøáÁ¥ßÂØÜÈõÜÊàêÁöÑÊû∂ÊûÑÂíåËÆ≠ÁªÉÁ≠ñÁï•Ëß£ÂÜ≥Ê≠§ÂÜ≤Á™Å„ÄÇËØ•Ê®°ÂûãÈááÁî®Ê∑∑ÂêàÊ≥®ÊÑèÂäõËß£Á†ÅÂô®Ôºå‰ΩøÁî®Âõ†ÊûúÊ≥®ÊÑèÂäõÁîüÊàêÂ∫èÂàóÂåñCoTÔºåÁÑ∂ÂêéÂàáÊç¢Âà∞ÂèåÂêëÊ≥®ÊÑèÂäõ‰ª•Âø´ÈÄüÂπ∂Ë°åËß£Á†ÅÂä®‰ΩúÂêëÈáè„ÄÇÈÖçÂêà‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊµÅÁ®ãÔºöÈ¶ñÂÖà‰ΩøÁî®ÁõëÁù£ÂæÆË∞É(SFT)ËÆ≠ÁªÉÊ®°ÂûãÁöÑÂü∫Á°ÄÊé®ÁêÜËÉΩÂäõÔºåÁÑ∂Âêé‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†(RL)Âíå‰ªªÂä°ÊàêÂäüÂ•ñÂä±Ôºå‰ΩøÂÆåÊï¥ÁöÑÊé®ÁêÜ-Âä®‰ΩúÂ∫èÂàó‰∏éÊúüÊúõÁªìÊûúÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDeepThinkVLAÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü97.0%ÁöÑÊàêÂäüÁéáÔºåËææÂà∞SOTAÊ∞¥Âπ≥„ÄÇÊ∂àËûçÂÆûÈ™åÈ™åËØÅ‰∫ÜËÆæËÆ°ÁöÑÊúâÊïàÊÄßÔºöÊ∑∑ÂêàÊû∂ÊûÑÊú¨Ë∫´ÊØîÊ†áÂáÜËß£Á†ÅÂô®ÊÄßËÉΩÈ´òÂá∫15.5%ÔºåÊúÄÁªàÁöÑRLÈò∂ÊÆµÊèê‰æõ‰∫ÜÂÖ≥ÈîÆÁöÑ2%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®Êú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°‰∏≠ÔºåÈÄöÂ∏∏ÈááÁî®Á´ØÂà∞Á´ØÁöÑÊñπÂºèÔºåÈúÄË¶ÅÂ§ßÈáèÊï∞ÊçÆËøõË°åËÆ≠ÁªÉ„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂèØËß£ÈáäÊÄßÔºåÂºïÂÖ•‰∫ÜChain-of-Thought (CoT)Êé®ÁêÜÔºåÂç≥ËÆ©Ê®°ÂûãÂú®ÊâßË°åÂä®‰ΩúÂâçÂÖàËøõË°åÊÄùËÄÉ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊ®°Âûã‰ΩøÁî®Âçï‰∏ÄÁöÑËá™ÂõûÂΩíËß£Á†ÅÂô®ÂêåÊó∂Â§ÑÁêÜÂ∫èÂàóÂåñÁöÑCoTÊé®ÁêÜÂíåÈ´òÁª¥„ÄÅÂèØÂπ∂Ë°åÁöÑÊú∫Âô®‰∫∫Âä®‰ΩúÔºåËøôÂØºËá¥‰∫Ü‰∏§‰∏™ÈóÆÈ¢òÔºö‰∏ÄÊòØËøêÂä®ÊéßÂà∂ÊÄßËÉΩ‰∏ãÈôçÔºåÂõ†‰∏∫Ëá™ÂõûÂΩíËß£Á†ÅÂô®‰∏çÈÄÇÂêàÂπ∂Ë°åÁîüÊàêÂä®‰ΩúÔºõ‰∫åÊòØÊé®ÁêÜÂíåÂä®‰Ωú‰πãÈó¥Áº∫‰πèÂº∫Âõ†ÊûúÂÖ≥Á≥ªÔºåÊ®°ÂûãÈöæ‰ª•Â≠¶‰π†Âà∞ÊúâÊïàÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDeepThinkVLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËß£ËÄ¶Êé®ÁêÜÂíåÂä®‰ΩúÁöÑÁîüÊàêËøáÁ®ãÔºåÂπ∂Âª∫Á´ãÂÆÉ‰ª¨‰πãÈó¥ÁöÑÂº∫Âõ†ÊûúÂÖ≥Á≥ª„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊ®°ÂûãÈ¶ñÂÖà‰ΩøÁî®Ëá™ÂõûÂΩíÁöÑÊñπÂºèËøõË°åCoTÊé®ÁêÜÔºåÁÑ∂ÂêéÊ†πÊçÆÊé®ÁêÜÁªìÊûúÂπ∂Ë°åÁîüÊàêÂä®‰Ωú„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÔºåËÆ∫ÊñáËÆæËÆ°‰∫Ü‰∏ÄÁßçÊ∑∑ÂêàÊ≥®ÊÑèÂäõËß£Á†ÅÂô®ÔºåÂπ∂ÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDeepThinkVLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÂê´‰∏Ä‰∏™ËßÜËßâÁºñÁ†ÅÂô®„ÄÅ‰∏Ä‰∏™ËØ≠Ë®ÄÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™Ê∑∑ÂêàÊ≥®ÊÑèÂäõËß£Á†ÅÂô®„ÄÇËßÜËßâÁºñÁ†ÅÂô®ÂíåËØ≠Ë®ÄÁºñÁ†ÅÂô®ÂàÜÂà´Áî®‰∫éÊèêÂèñÂõæÂÉèÂíåÊñáÊú¨ÁöÑÁâπÂæÅ„ÄÇÊ∑∑ÂêàÊ≥®ÊÑèÂäõËß£Á†ÅÂô®ÊòØËØ•Ê®°ÂûãÁöÑÊ†∏ÂøÉÔºåÂÆÉÈ¶ñÂÖà‰ΩøÁî®Âõ†ÊûúÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åCoTÊé®ÁêÜÔºåÁÑ∂ÂêéÂàáÊç¢Âà∞ÂèåÂêëÊ≥®ÊÑèÂäõÊú∫Âà∂‰ª•Âπ∂Ë°åÁîüÊàêÂä®‰Ωú„ÄÇËÆ≠ÁªÉËøáÁ®ãÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµÊòØÁõëÁù£ÂæÆË∞É(SFT)Ôºå‰ΩøÁî®‰∫∫Â∑•Ê†áÊ≥®ÁöÑCoTÊï∞ÊçÆËÆ≠ÁªÉÊ®°ÂûãÁöÑÂü∫Á°ÄÊé®ÁêÜËÉΩÂäõÔºõÁ¨¨‰∫åÈò∂ÊÆµÊòØÂº∫ÂåñÂ≠¶‰π†(RL)Ôºå‰ΩøÁî®‰ªªÂä°ÊàêÂäüÂ•ñÂä±Êù•‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜÂíåÂä®‰ΩúÁ≠ñÁï•Ôºå‰ªéËÄåÂª∫Á´ãÊé®ÁêÜÂíåÂä®‰Ωú‰πãÈó¥ÁöÑÂº∫Âõ†ÊûúÂÖ≥Á≥ª„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDeepThinkVLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊ∑∑ÂêàÊ≥®ÊÑèÂäõËß£Á†ÅÂô®Âíå‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÊ∑∑ÂêàÊ≥®ÊÑèÂäõËß£Á†ÅÂô®ËÉΩÂ§üÊ†πÊçÆ‰ªªÂä°ÁöÑ‰∏çÂêåÔºåÁÅµÊ¥ªÂú∞ÂàáÊç¢Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂ∫èÂàóÂåñÁöÑÊé®ÁêÜÂíåÂπ∂Ë°åÁöÑÂä®‰ΩúÁîüÊàê„ÄÇ‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÂíåÂº∫ÂåñÂ≠¶‰π†Â•ñÂä±Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ∑∑ÂêàÊ≥®ÊÑèÂäõËß£Á†ÅÂô®ÂåÖÂê´‰∏§‰∏™Ê≥®ÊÑèÂäõÊ®°ÂùóÔºö‰∏Ä‰∏™Âõ†ÊûúÊ≥®ÊÑèÂäõÊ®°ÂùóÂíå‰∏Ä‰∏™ÂèåÂêëÊ≥®ÊÑèÂäõÊ®°Âùó„ÄÇÂõ†ÊûúÊ≥®ÊÑèÂäõÊ®°ÂùóÁî®‰∫éÁîüÊàêCoTÊé®ÁêÜÔºåÂÆÉÂè™ÂÖÅËÆ∏Ê®°ÂûãÂÖ≥Ê≥®‰πãÂâçÁöÑtokenÔºå‰ªéËÄå‰øùËØÅÊé®ÁêÜÁöÑÂ∫èÂàóÊÄß„ÄÇÂèåÂêëÊ≥®ÊÑèÂäõÊ®°ÂùóÁî®‰∫éÁîüÊàêÂä®‰ΩúÔºåÂÆÉÂÖÅËÆ∏Ê®°ÂûãÂÖ≥Ê≥®ÊâÄÊúâÁöÑtokenÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Âà©Áî®‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåËÆ∫Êñá‰ΩøÁî®‰∫ÜAdam‰ºòÂåñÂô®ÔºåÂ≠¶‰π†ÁéáËÆæÁΩÆ‰∏∫1e-4„ÄÇÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµ‰ΩøÁî®‰∫ÜPPOÁÆóÊ≥ïÔºåÂ•ñÂä±ÂáΩÊï∞‰∏∫‰ªªÂä°ÊàêÂäüÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DeepThinkVLAÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü97.0%ÁöÑÊàêÂäüÁéáÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËææÂà∞‰∫ÜSOTAÊ∞¥Âπ≥„ÄÇÊ∂àËûçÂÆûÈ™åË°®ÊòéÔºåÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊû∂ÊûÑÊú¨Ë∫´ÊØîÊ†áÂáÜËß£Á†ÅÂô®ÊÄßËÉΩÈ´òÂá∫15.5%ÔºåÊúÄÁªàÁöÑÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÊèê‰æõ‰∫ÜÂÖ≥ÈîÆÁöÑ2%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫ÜDeepThinkVLAÁöÑÊúâÊïàÊÄßÔºåÂπ∂Ë°®ÊòéÊ∑∑ÂêàÊ≥®ÊÑèÂäõÊû∂ÊûÑÂíå‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÊòØÊèêÂçáVLAÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DeepThinkVLAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊé®ÁêÜËÉΩÂäõÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°Ôºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öÊú∫Âô®‰∫∫ÂíåËá™Âä®È©æÈ©∂Ê±ΩËΩ¶„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÁêÜËß£‰∫∫Á±ªÊåá‰ª§ÔºåÊõ¥ÊúâÊïàÂú∞ÂÆåÊàêÂ§çÊùÇ‰ªªÂä°ÔºåÂπ∂Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÊú™Áü•ÁéØÂ¢É„ÄÇËØ•Á†îÁ©∂ËøòÊúâÂä©‰∫éÊèêÂçáVLAÊ®°ÂûãÂú®ÂÖ∂‰ªñÈ¢ÜÂüüÁöÑÂ∫îÁî®Ôºå‰æãÂ¶ÇÊô∫ËÉΩÈóÆÁ≠îÂíåÂõæÂÉèÊèèËø∞„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.

