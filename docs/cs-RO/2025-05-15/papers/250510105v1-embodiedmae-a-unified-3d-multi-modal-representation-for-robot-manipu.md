---
layout: default
title: "EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation"
---

# EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.10105" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.10105v1</a>
  <a href="https://arxiv.org/pdf/2505.10105.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.10105v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.10105v1', 'EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zibin Dong, Fei Ni, Yifu Yuan, Yinchuan Li, Jianye Hao

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEmbodiedMAEä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„å¤šæ¨¡æ€è¡¨ç¤ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è¡¨ç¤º` `æœºå™¨äººæ“ä½œ` `æ·±åº¦å­¦ä¹ ` `è‡ªç¼–ç å™¨` `3Dè§†è§‰` `è·¨æ¨¡æ€èåˆ` `æ•°æ®é›†å¢å¼º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒæ•°æ®é›†ä¸æœºå™¨äººæ“ä½œä»»åŠ¡ä¹‹é—´å­˜åœ¨æ˜¾è‘—é¢†åŸŸå·®è·ï¼Œä¸”ç¼ºä¹æœ‰æ•ˆæ•´åˆ3Dä¿¡æ¯çš„æ¨¡å‹æ¶æ„ã€‚
2. æå‡ºäº†EmbodiedMAEï¼Œä¸€ä¸ªå¤šæ¨¡æ€æ©ç è‡ªç¼–ç å™¨ï¼Œé€šè¿‡éšæœºæ©ç å’Œè·¨æ¨¡æ€èåˆåŒæ—¶å­¦ä¹ RGBã€æ·±åº¦å’Œç‚¹äº‘æ¨¡æ€çš„è¡¨ç¤ºã€‚
3. EmbodiedMAEåœ¨70ä¸ªä»¿çœŸä»»åŠ¡å’Œ20ä¸ªçœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½å‡è¶…è¶Šäº†ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬æå‡ºäº†EmbodiedMAEï¼Œä¸€ä¸ªç»Ÿä¸€çš„3Då¤šæ¨¡æ€è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å½“å‰æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è®­ç»ƒæ•°æ®é›†ä¸å®é™…åº”ç”¨ä¹‹é—´çš„æ˜¾è‘—é¢†åŸŸå·®è·ï¼Œä»¥åŠç¼ºä¹æœ‰æ•ˆæ•´åˆ3Dä¿¡æ¯çš„æ¨¡å‹æ¶æ„çš„é—®é¢˜ã€‚é€šè¿‡å¢å¼ºDROIDæ•°æ®é›†ï¼Œæ„å»ºäº†DROID-3Dï¼Œä½œä¸º3Då…·èº«è§†è§‰ç ”ç©¶çš„æœ‰ä»·å€¼è¡¥å……ã€‚EmbodiedMAEä½œä¸ºä¸€ç§å¤šæ¨¡æ€æ©ç è‡ªç¼–ç å™¨ï¼Œé€šè¿‡éšæœºæ©ç å’Œè·¨æ¨¡æ€èåˆï¼Œèƒ½å¤ŸåŒæ—¶å­¦ä¹ RGBã€æ·±åº¦å’Œç‚¹äº‘æ¨¡æ€çš„è¡¨ç¤ºã€‚ç»è¿‡DROID-3Dè®­ç»ƒï¼ŒEmbodiedMAEåœ¨70ä¸ªä»¿çœŸä»»åŠ¡å’Œ20ä¸ªçœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå‡åœ¨è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ‰©å±•æ€§å’Œæœ‰æ•ˆçš„3Dè¾“å…¥ç­–ç•¥å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è®­ç»ƒæ•°æ®é›†ä¸å®é™…åº”ç”¨ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä»¥åŠç°æœ‰æ¨¡å‹åœ¨æ•´åˆ3Dä¿¡æ¯æ–¹é¢çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºEmbodiedMAEï¼Œé€šè¿‡å¤šæ¨¡æ€æ©ç è‡ªç¼–ç å™¨è®¾è®¡ï¼Œåˆ©ç”¨éšæœºæ©ç å’Œè·¨æ¨¡æ€èåˆæŠ€æœ¯ï¼Œå¢å¼ºæ¨¡å‹å¯¹RGBã€æ·±åº¦å’Œç‚¹äº‘ä¿¡æ¯çš„å­¦ä¹ èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEmbodiedMAEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡æ€è¾“å…¥ã€æ©ç å¤„ç†ã€ç‰¹å¾æå–å’Œè¾“å‡ºå±‚ï¼Œç¡®ä¿ä¸åŒæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆä¸å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæ„å»ºäº†DROID-3Dæ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§æ¨¡æ€çš„è‡ªç¼–ç å™¨ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨3Dç¯å¢ƒä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸åŒæ¨¡æ€çš„å­¦ä¹ ï¼Œç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºä¿¡æ¯èåˆçš„æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

EmbodiedMAEåœ¨70ä¸ªä»¿çœŸä»»åŠ¡å’Œ20ä¸ªçœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½å‡è¶…è¶Šäº†ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå±•ç¤ºå‡ºå¼ºå¤§çš„æ‰©å±•æ€§å’Œæœ‰æ•ˆçš„3Dè¾“å…¥ç­–ç•¥å­¦ä¹ èƒ½åŠ›ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€ç‰©ä½“æ“ä½œå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿä¸ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œæä¾›æ›´ç²¾å‡†çš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ã€‚æœªæ¥ï¼ŒEmbodiedMAEæœ‰æœ›æ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ä¸é€‚åº”æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present EmbodiedMAE, a unified 3D multi-modal representation for robot manipulation. Current approaches suffer from significant domain gaps between training datasets and robot manipulation tasks, while also lacking model architectures that can effectively incorporate 3D information. To overcome these limitations, we enhance the DROID dataset with high-quality depth maps and point clouds, constructing DROID-3D as a valuable supplement for 3D embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked autoencoder that simultaneously learns representations across RGB, depth, and point cloud modalities through stochastic masking and cross-modal fusion. Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art vision foundation models (VFMs) in both training efficiency and final performance across 70 simulation tasks and 20 real-world robot manipulation tasks on two robot platforms. The model exhibits strong scaling behavior with size and promotes effective policy learning from 3D inputs. Experimental results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for embodied AI systems, particularly in precise tabletop manipulation settings where spatial perception is critical.

