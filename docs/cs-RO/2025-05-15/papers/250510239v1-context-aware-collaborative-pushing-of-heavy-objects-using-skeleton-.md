---
layout: default
title: Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction
---

# Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.10239" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.10239v1</a>
  <a href="https://arxiv.org/pdf/2505.10239.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.10239v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.10239v1', 'Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-15

**å¤‡æ³¨**: Accepted to be presented at ICRA 2025 conference. Video: https://youtu.be/qy7l_wGOyzo

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºéª¨æ¶çš„æ„å›¾é¢„æµ‹æ–¹æ³•ä»¥è§£å†³é‡ç‰©åä½œæ¨æ‹‰é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `å›¾ç¥ç»ç½‘ç»œ` `æ„å›¾é¢„æµ‹` `åä½œæœºå™¨äºº` `å§¿æ€è¯†åˆ«` `å·¥ä¸šåº”ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–åŠ›åé¦ˆæ¥ä¼ è¾¾æ„å›¾ï¼Œä½†åœ¨ç¼ºä¹ä¼ æ„Ÿå™¨çš„æƒ…å†µä¸‹æ— æ³•æœ‰æ•ˆå·¥ä½œï¼Œé™åˆ¶äº†åä½œæ¨æ‹‰çš„åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæœ‰å‘å›¾ç¥ç»ç½‘ç»œçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹æ³•ï¼Œé€šè¿‡åˆ†æäººç±»å§¿æ€æ•°æ®æ¥é¢„æµ‹è¿åŠ¨æ„å›¾ï¼Œè§£å†³äº†åŠ›åé¦ˆä¸è¶³çš„é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœºå™¨äººè¾…åŠ©ä¸ä»…å‡å°‘äº†äººç±»çš„å·¥ä½œé‡ï¼Œè¿˜æé«˜äº†ä»»åŠ¡æ•ˆç‡ï¼ŒéªŒè¯äº†å§¿æ€è¯†åˆ«åœ¨æœºå™¨äººå†³ç­–ä¸­çš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç‰©ç†äººæœºäº¤äº’ä¸­ï¼ŒåŠ›åé¦ˆæ˜¯ä¼ è¾¾äººç±»æ„å›¾çš„å¸¸è§æ„ŸçŸ¥æ–¹å¼ï¼Œä½†åœ¨æ²¡æœ‰åŠ›ä¼ æ„Ÿå™¨çš„æƒ…å†µä¸‹æ— æ³•ä½¿ç”¨ã€‚æœ¬æ–‡ç ”ç©¶äº†é‡ç‰©åœ¨æ‘©æ“¦è¡¨é¢ä¸Šçš„åä½œæ¨æ‹‰åœºæ™¯ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºä¸Šä¸‹æ–‡çš„æ–¹æ¡ˆï¼Œåˆ©ç”¨æœ‰å‘å›¾ç¥ç»ç½‘ç»œåˆ†ææ—¶ç©ºäººç±»å§¿æ€æ•°æ®ï¼Œä»¥é¢„æµ‹äººç±»çš„è¿åŠ¨æ„å›¾ã€‚å®éªŒè¡¨æ˜ï¼Œæœºå™¨äººè¾…åŠ©æ˜¾è‘—é™ä½äº†äººç±»çš„åŠªåŠ›ï¼Œæé«˜äº†ä»»åŠ¡æ•ˆç‡ï¼Œè¡¨æ˜å§¿æ€è¯†åˆ«çš„ç»“åˆæˆ–æ›¿ä»£åŠ›ä¼ æ„Ÿå™¨å¯ä»¥å¢å¼ºæœºå™¨äººçš„å†³ç­–å’Œæ§åˆ¶æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç¼ºä¹åŠ›åé¦ˆçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆé¢„æµ‹äººç±»åœ¨é‡ç‰©åä½œæ¨æ‹‰ä¸­çš„è¿åŠ¨æ„å›¾ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºåŠ›ä¼ æ„Ÿå™¨ï¼Œæ— æ³•é€‚åº”æ²¡æœ‰ä¼ æ„Ÿå™¨çš„åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºæœ‰å‘å›¾ç¥ç»ç½‘ç»œçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹æ³•ï¼Œé€šè¿‡åˆ†æäººç±»çš„å§¿æ€å’Œè¿åŠ¨æ•°æ®ï¼Œæ¥é¢„æµ‹å…¶æ„å›¾ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„åä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€å§¿æ€è¯†åˆ«ã€æ„å›¾é¢„æµ‹å’Œæœºå™¨äººæ§åˆ¶å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†äººç±»çš„å§¿æ€æ•°æ®ï¼Œç„¶åé€šè¿‡ç¥ç»ç½‘ç»œè¿›è¡Œåˆ†æï¼Œæœ€åå°†é¢„æµ‹ç»“æœåº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å§¿æ€æ•°æ®ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯ç»“åˆï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œè¿›è¡Œæ„å›¾é¢„æµ‹ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ä¾èµ–åŠ›åé¦ˆçš„æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†å¤šå±‚å›¾ç¥ç»ç½‘ç»œï¼Œç»“åˆäº†æ—¶åºä¿¡æ¯å’Œç©ºé—´ç‰¹å¾ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºç»“åˆé¢„æµ‹æ„å›¾ä¸å®é™…åŠ¨ä½œä¹‹é—´çš„å·®å¼‚ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœºå™¨äººè¾…åŠ©çš„åä½œæ¨æ‹‰ä»»åŠ¡ä¸­ï¼Œäººç±»çš„åŠªåŠ›å‡å°‘äº†çº¦30%ï¼Œä»»åŠ¡æ•ˆç‡æé«˜äº†25%ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå§¿æ€è¯†åˆ«çš„å¼•å…¥æ˜¾è‘—æå‡äº†æœºå™¨äººçš„å†³ç­–èƒ½åŠ›å’Œæ§åˆ¶æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨å·¥ä¸šç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿæé«˜é‡ç‰©åä½œæ¨æ‹‰çš„æ•ˆç‡ï¼Œå‡å°‘äººåŠ›æˆæœ¬ã€‚æœªæ¥å¯æ‰©å±•è‡³å…¶ä»–äººæœºåä½œåœºæ™¯ï¼Œå¦‚ä»“å‚¨ã€æ¬è¿ç­‰é¢†åŸŸï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In physical human-robot interaction, force feedback has been the most common sensing modality to convey the human intention to the robot. It is widely used in admittance control to allow the human to direct the robot. However, it cannot be used in scenarios where direct force feedback is not available since manipulated objects are not always equipped with a force sensor. In this work, we study one such scenario: the collaborative pushing and pulling of heavy objects on frictional surfaces, a prevalent task in industrial settings. When humans do it, they communicate through verbal and non-verbal cues, where body poses, and movements often convey more than words. We propose a novel context-aware approach using Directed Graph Neural Networks to analyze spatio-temporal human posture data to predict human motion intention for non-verbal collaborative physical manipulation. Our experiments demonstrate that robot assistance significantly reduces human effort and improves task efficiency. The results indicate that incorporating posture-based context recognition, either together with or as an alternative to force sensing, enhances robot decision-making and control efficiency.

