---
layout: default
title: "IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning"
---

# IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.10442" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.10442v1</a>
  <a href="https://arxiv.org/pdf/2505.10442.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.10442v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.10442v1', 'IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Dechen Gao, Hang Wang, Hanchu Zhou, Nejib Ammar, Shatadal Mishra, Ahmadreza Moradipari, Iman Soltani, Junshan Zhang

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-15

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/ucd-dare/IN-RIL)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫IN-RIL‰ª•Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†ÂæÆË∞É‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê®°‰ªøÂ≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫Â≠¶‰π†` `Á≠ñÁï•ÂæÆË∞É` `Ê†∑Êú¨ÊïàÁéá` `Ê¢ØÂ∫¶ÂàÜÁ¶ª` `ÊÄßËÉΩÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑILÂíåRLÁªìÂêàÊñπÊ≥ïÂú®RLÂæÆË∞ÉÈò∂ÊÆµÂ∏∏Â∏∏Èù¢‰∏¥‰∏çÁ®≥ÂÆöÊÄßÂíåÊ†∑Êú¨ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇ
2. IN-RILÈÄöËøáÂú®Â§öÊ¨°RLÊõ¥Êñ∞ÂêéÂÆöÊúüÊ≥®ÂÖ•ILÊõ¥Êñ∞ÔºåÁªìÂêà‰∫ÜILÁöÑÁ®≥ÂÆöÊÄßÂíåRLÁöÑÊé¢Á¥¢ÊÄß„ÄÇ
3. Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåIN-RILÊòæËëóÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÔºåÊàêÂäüÁéáÊèêÂçáÂπÖÂ∫¶ËææÂà∞6.3ÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê®°‰ªøÂ≠¶‰π†ÔºàILÔºâÂíåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂêÑËá™‰∏∫Êú∫Âô®‰∫∫Á≠ñÁï•Â≠¶‰π†Êèê‰æõ‰∫ÜÁã¨ÁâπÁöÑ‰ºòÂäøÔºöILÈÄöËøáÁ§∫ËåÉÊèê‰æõÁ®≥ÂÆöÁöÑÂ≠¶‰π†ÔºåËÄåRLÈÄöËøáÊé¢Á¥¢‰øÉËøõÊ≥õÂåñ„ÄÇÁé∞ÊúâÁöÑÊú∫Âô®‰∫∫Â≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®ILÈ¢ÑËÆ≠ÁªÉÂêéËøõË°åRLÂæÆË∞ÉÔºå‰ΩÜËøôÁßç‰∏§Ê≠•Â≠¶‰π†ËåÉÂºèÂú®RLÂæÆË∞ÉÈò∂ÊÆµÂ∏∏Â∏∏Èù¢‰∏¥‰∏çÁ®≥ÂÆöÊÄßÂíåÊ†∑Êú¨ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜIN-RILÔºà‰∫§ÈîôÂº∫ÂåñÂ≠¶‰π†‰∏éÊ®°‰ªøÂ≠¶‰π†ÔºâÔºåËØ•ÊñπÊ≥ïÂú®Â§öÊ¨°RLÊõ¥Êñ∞ÂêéÂÆöÊúüÊ≥®ÂÖ•ILÊõ¥Êñ∞Ôºå‰ªéËÄåÂú®Êï¥‰∏™ÂæÆË∞ÉËøáÁ®ã‰∏≠Âà©Áî®ILÁöÑÁ®≥ÂÆöÊÄßÂíå‰∏ìÂÆ∂Êï∞ÊçÆÁöÑÊåáÂØº„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫ÜÊ¢ØÂ∫¶ÂàÜÁ¶ªÊú∫Âà∂Ôºå‰ª•Èò≤Ê≠¢Âú®ÂæÆË∞ÉËøáÁ®ã‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑÂπ≤Êâ∞„ÄÇÈÄöËøáÂú®14‰∏™Êú∫Âô®‰∫∫Êìç‰ΩúÂíåËøêÂä®‰ªªÂä°‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéIN-RILÊòæËëóÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÔºåÂπ∂ÂáèËΩª‰∫ÜÂú®Á∫øÂæÆË∞É‰∏≠ÁöÑÊÄßËÉΩÂ¥©Ê∫É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâILÂíåRLÁªìÂêàÊñπÊ≥ïÂú®RLÂæÆË∞ÉÈò∂ÊÆµÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂíå‰ΩéÊ†∑Êú¨ÊïàÁéáÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®RLÂæÆË∞ÉÊó∂Â∏∏Â∏∏Âá∫Áé∞ÊÄßËÉΩÂ¥©Ê∫ÉÔºåÂØºËá¥Â≠¶‰π†ÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöIN-RILÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØ‰∫§Èîô‰ΩøÁî®ILÂíåRLÔºåÈÄöËøáÂú®Â§öÊ¨°RLÊõ¥Êñ∞ÂêéÂÆöÊúüÊ≥®ÂÖ•ILÊõ¥Êñ∞ÔºåÂà©Áî®ILÁöÑÁ®≥ÂÆöÊÄßÂíå‰∏ìÂÆ∂Êï∞ÊçÆÁöÑÊåáÂØºÊù•ÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöIN-RILÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÈ¶ñÂÖàËøõË°åRLÊõ¥Êñ∞ÔºåÁÑ∂ÂêéÂú®ÈÄÇÂΩìÁöÑÊó∂Êú∫ÊèíÂÖ•ILÊõ¥Êñ∞„ÄÇÈÄöËøáËøôÁßç‰∫§ÈîôÊñπÂºèÔºåILÂíåRLÁöÑ‰ºòÁÇπÂæó‰ª•ÁªìÂêà„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜÊ¢ØÂ∫¶ÂàÜÁ¶ªÊú∫Âà∂Ôºå‰ª•Èò≤Ê≠¢Âú®ÂæÆË∞ÉËøáÁ®ã‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑÂπ≤Êâ∞„ÄÇËøôÁßçÊú∫Âà∂ÈÄöËøáÂú®Ê≠£‰∫§Â≠êÁ©∫Èó¥‰∏≠ÂàÜÁ¶ªÂèØËÉΩÂÜ≤Á™ÅÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞ÔºåÁ°Æ‰øù‰∫ÜÂ≠¶‰π†ËøáÁ®ãÁöÑÁ®≥ÂÆöÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåILÂíåRLÁöÑÊõ¥Êñ∞È¢ëÁéáÈúÄË¶ÅÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øù‰∏§ËÄÖÁöÑÊúâÊïàÁªìÂêà„ÄÇÊ≠§Â§ñÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüËÄÉËôë‰∫ÜILÂíåRLÁöÑ‰∏çÂêå‰ºòÂåñÁõÆÊ†áÔºå‰ª•ÂÆûÁé∞Êõ¥È´òÊïàÁöÑÂ≠¶‰π†„ÄÇÊï¥‰ΩìÁΩëÁªúÁªìÊûÑÂàôÈúÄÊîØÊåÅËøôÁßç‰∫§ÈîôÊõ¥Êñ∞ÁöÑÁ≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®14‰∏™Êú∫Âô®‰∫∫Êìç‰ΩúÂíåËøêÂä®‰ªªÂä°ÁöÑÂÆûÈ™å‰∏≠ÔºåIN-RILÊòæËëóÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÔºåÊàêÂäüÁéáÂú®Robomimic Transport‰ªªÂä°‰∏≠‰ªé12%ÊèêÂçáËá≥88%ÔºåÂÆûÁé∞‰∫Ü6.3ÂÄçÁöÑÊèêÂçáÔºåË°®ÊòéËØ•ÊñπÊ≥ïÂú®ÈïøÁü≠Êúü‰ªªÂä°‰∏≠ÂùáË°®Áé∞Âá∫Ëâ≤„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

IN-RILÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÂíåËøêÂä®‰ªªÂä°‰∏≠ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåËÉΩÂ§üÊúâÊïàÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÈÄÇÂ∫îËÉΩÂäõ„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØÊâ©Â±ïÂà∞Êõ¥Â§öÁöÑÊú∫Âô®‰∫∫Â≠¶‰π†Âú∫ÊôØÔºåÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Imitation learning (IL) and reinforcement learning (RL) each offer distinct advantages for robotics policy learning: IL provides stable learning from demonstrations, and RL promotes generalization through exploration. While existing robot learning approaches using IL-based pre-training followed by RL-based fine-tuning are promising, this two-step learning paradigm often suffers from instability and poor sample efficiency during the RL fine-tuning phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning and Imitation Learning, for policy fine-tuning, which periodically injects IL updates after multiple RL updates and hence can benefit from the stability of IL and the guidance of expert data for more efficient exploration throughout the entire fine-tuning process. Since IL and RL involve different optimization objectives, we develop gradient separation mechanisms to prevent destructive interference during \ABBR fine-tuning, by separating possibly conflicting gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous analysis, and our findings shed light on why interleaving IL with RL stabilizes learning and improves sample-efficiency. Extensive experiments on 14 robot manipulation and locomotion tasks across 3 benchmarks, including FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \ABBR can significantly improve sample efficiency and mitigate performance collapse during online finetuning in both long- and short-horizon tasks with either sparse or dense rewards. IN-RIL, as a general plug-in compatible with various state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g., from 12\% to 88\% with 6.3x improvement in the success rate on Robomimic Transport. Project page: https://github.com/ucd-dare/IN-RIL.

