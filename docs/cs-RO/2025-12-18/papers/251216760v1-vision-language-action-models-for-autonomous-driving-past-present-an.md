---
layout: default
title: Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future
---

# Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16760" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16760v1</a>
  <a href="https://arxiv.org/pdf/2512.16760.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16760v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16760v1', 'Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianshuai Hu, Xiaolu Liu, Song Wang, Yiyao Zhu, Ao Liang, Lingdong Kong, Guoyang Zhao, Zeying Gong, Jun Cen, Zhiyu Huang, Xiaoshuai Hao, Linfeng Li, Hang Song, Xiangtai Li, Jun Ma, Shaojie Shen, Jianke Zhu, Dacheng Tao, Ziwei Liu, Junwei Liang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°æ€§è®ºæ–‡ï¼šé¢å‘è‡ªåŠ¨é©¾é©¶çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ç ”ç©¶è¿›å±•ä¸æœªæ¥å±•æœ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç«¯åˆ°ç«¯å­¦ä¹ ` `åŒç³»ç»Ÿæ¶æ„` `ç»¼è¿°` `å†³ç­–è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿè‡ªåŠ¨é©¾é©¶ä¾èµ–â€œæ„ŸçŸ¥-å†³ç­–-è¡ŒåŠ¨â€æµç¨‹ï¼Œä½†æ‰‹å·¥è®¾è®¡ç»„ä»¶åœ¨å¤æ‚åœºæ™¯å¤±æ•ˆï¼Œä¸”æ„ŸçŸ¥è¯¯å·®ä¼šå‘ä¸‹æ¸¸ä¼ æ’­ã€‚
2. è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šè¿‡ç»“åˆè§†è§‰ç†è§£ã€è¯­è¨€æ¨ç†å’Œå¯æ‰§è¡ŒåŠ¨ä½œï¼Œæ—¨åœ¨å®ç°æ›´é€šç”¨å’Œå¯è§£é‡Šçš„è‡ªåŠ¨é©¾é©¶ç­–ç•¥ã€‚
3. è®ºæ–‡ç»¼è¿°äº†VLAåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ï¼Œåˆ†æäº†ç«¯åˆ°ç«¯å’ŒåŒç³»ç»Ÿä¸¤ç§ä¸»è¦èŒƒä¾‹ï¼Œå¹¶æ¢è®¨äº†æœªæ¥å‘å±•æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨é©¾é©¶é•¿æœŸä»¥æ¥ä¾èµ–äºæ¨¡å—åŒ–çš„â€œæ„ŸçŸ¥-å†³ç­–-è¡ŒåŠ¨â€æµç¨‹ï¼Œä½†æ‰‹å·¥è®¾è®¡çš„æ¥å£å’ŒåŸºäºè§„åˆ™çš„ç»„ä»¶åœ¨å¤æ‚æˆ–é•¿å°¾åœºæ™¯ä¸­ç»å¸¸å¤±æ•ˆã€‚å…¶çº§è”è®¾è®¡è¿›ä¸€æ­¥ä¼ æ’­æ„ŸçŸ¥è¯¯å·®ï¼Œé™ä½ä¸‹æ¸¸è§„åˆ’å’Œæ§åˆ¶çš„æ€§èƒ½ã€‚è§†è§‰-åŠ¨ä½œï¼ˆVAï¼‰æ¨¡å‹é€šè¿‡å­¦ä¹ ä»è§†è§‰è¾“å…¥åˆ°åŠ¨ä½œçš„ç›´æ¥æ˜ å°„æ¥è§£å†³ä¸€äº›å±€é™æ€§ï¼Œä½†å®ƒä»¬ä»ç„¶ä¸é€æ˜ï¼Œå¯¹åˆ†å¸ƒåç§»æ•æ„Ÿï¼Œå¹¶ä¸”ç¼ºä¹ç»“æ„åŒ–æ¨ç†æˆ–æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å­¦ä¹ çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æ¶çš„å‡ºç°ï¼Œè¯¥æ¡†æ¶å°†æ„ŸçŸ¥ä¸åŸºäºè¯­è¨€çš„å†³ç­–ç›¸ç»“åˆã€‚é€šè¿‡ç»Ÿä¸€è§†è§‰ç†è§£ã€è¯­è¨€æ¨ç†å’Œå¯æ“ä½œçš„è¾“å‡ºï¼ŒVLAä¸ºæ›´å¯è§£é‡Šã€æ›´é€šç”¨å’Œæ›´ç¬¦åˆäººç±»ä¹ æƒ¯çš„é©¾é©¶ç­–ç•¥æä¾›äº†ä¸€æ¡é€”å¾„ã€‚æœ¬æ–‡å¯¹æ–°å…´çš„è‡ªåŠ¨é©¾é©¶VLAé¢†åŸŸè¿›è¡Œäº†ç»“æ„åŒ–æè¿°ï¼Œè¿½æº¯äº†ä»æ—©æœŸVAæ–¹æ³•åˆ°ç°ä»£VLAæ¡†æ¶çš„æ¼”å˜ï¼Œå¹¶å°†ç°æœ‰æ–¹æ³•ç»„ç»‡æˆä¸¤ç§ä¸»è¦èŒƒä¾‹ï¼šç«¯åˆ°ç«¯VLAï¼Œå®ƒåœ¨å•ä¸ªæ¨¡å‹ä¸­é›†æˆäº†æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’ï¼›åŒç³»ç»ŸVLAï¼Œå®ƒå°†æ…¢é€Ÿå®¡è®®ï¼ˆé€šè¿‡VLMï¼‰ä¸å¿«é€Ÿã€å®‰å…¨å…³é”®çš„æ‰§è¡Œï¼ˆé€šè¿‡è§„åˆ’å™¨ï¼‰åˆ†å¼€ã€‚åœ¨è¿™äº›èŒƒä¾‹ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åŒºåˆ†äº†æ–‡æœ¬ä¸æ•°å€¼åŠ¨ä½œç”Ÿæˆå™¨ä»¥åŠæ˜¾å¼ä¸éšå¼æŒ‡å¯¼æœºåˆ¶ç­‰å­ç±»ã€‚æˆ‘ä»¬è¿˜æ€»ç»“äº†ç”¨äºè¯„ä¼°åŸºäºVLAçš„é©¾é©¶ç³»ç»Ÿçš„ä»£è¡¨æ€§æ•°æ®é›†å’ŒåŸºå‡†ï¼Œå¹¶å¼ºè°ƒäº†å…³é”®æŒ‘æˆ˜å’Œå¼€æ”¾æ–¹å‘ï¼ŒåŒ…æ‹¬é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’ŒæŒ‡ä»¤ä¿çœŸåº¦ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨ä¸ºæ¨è¿›äººæœºå…¼å®¹çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¥ å®šè¿è´¯çš„åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¾èµ–äºæ¨¡å—åŒ–çš„â€œæ„ŸçŸ¥-å†³ç­–-è¡ŒåŠ¨â€æµç¨‹ï¼Œè¿™äº›æµç¨‹é€šå¸¸åŒ…å«æ‰‹å·¥è®¾è®¡çš„æ¥å£å’Œè§„åˆ™ï¼Œåœ¨å¤æ‚æˆ–é•¿å°¾åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚æ­¤å¤–ï¼Œçº§è”çš„è®¾è®¡ä¼šå¯¼è‡´æ„ŸçŸ¥è¯¯å·®å‘ä¸‹æ¸¸ä¼ æ’­ï¼Œå½±å“è§„åˆ’å’Œæ§åˆ¶çš„å‡†ç¡®æ€§ã€‚è§†è§‰-åŠ¨ä½œï¼ˆVAï¼‰æ¨¡å‹è™½ç„¶å°è¯•ç›´æ¥ä»è§†è§‰è¾“å…¥æ˜ å°„åˆ°åŠ¨ä½œï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–æ•æ„Ÿï¼Œå¹¶ä¸”éš¾ä»¥è¿›è¡Œç»“æ„åŒ–æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å­¦ä¹ çš„æœ€æ–°è¿›å±•ï¼Œæ„å»ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æ¶ï¼Œå°†è§†è§‰æ„ŸçŸ¥ã€è¯­è¨€æ¨ç†å’Œå¯æ‰§è¡ŒåŠ¨ä½œç»Ÿä¸€èµ·æ¥ã€‚é€šè¿‡è¯­è¨€ä½œä¸ºæ¡¥æ¢ï¼ŒVLAæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£é©¾é©¶åœºæ™¯ï¼Œè¿›è¡Œæ¨ç†å’Œå†³ç­–ï¼Œå¹¶ç”Ÿæˆç¬¦åˆäººç±»æŒ‡ä»¤çš„é©¾é©¶è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„é€šç”¨æ€§ã€å¯è§£é‡Šæ€§å’Œäººæœºäº¤äº’èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡å°†ç°æœ‰çš„VLAæ–¹æ³•ç»„ç»‡æˆä¸¤ç§ä¸»è¦èŒƒä¾‹ï¼šç«¯åˆ°ç«¯VLAå’ŒåŒç³»ç»ŸVLAã€‚ç«¯åˆ°ç«¯VLAæ¨¡å‹å°†æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’é›†æˆåˆ°ä¸€ä¸ªå•ä¸€çš„æ¨¡å‹ä¸­ï¼Œç›´æ¥ä»è§†è§‰è¾“å…¥ç”ŸæˆåŠ¨ä½œã€‚åŒç³»ç»ŸVLAæ¨¡å‹åˆ™å°†æ…¢é€Ÿå®¡è®®ï¼ˆé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹VLMï¼‰ä¸å¿«é€Ÿã€å®‰å…¨å…³é”®çš„æ‰§è¡Œï¼ˆé€šè¿‡è§„åˆ’å™¨ï¼‰åˆ†å¼€ï¼ŒVLMè´Ÿè´£é«˜çº§å†³ç­–å’ŒæŒ‡ä»¤ç”Ÿæˆï¼Œè§„åˆ’å™¨è´Ÿè´£ä½çº§åˆ«çš„åŠ¨ä½œæ§åˆ¶ã€‚åœ¨è¿™äº›èŒƒä¾‹ä¸­ï¼Œè¿˜åŒºåˆ†äº†æ–‡æœ¬ä¸æ•°å€¼åŠ¨ä½œç”Ÿæˆå™¨ä»¥åŠæ˜¾å¼ä¸éšå¼æŒ‡å¯¼æœºåˆ¶ç­‰å­ç±»ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºå¯¹è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„VLAæ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„ç»¼è¿°å’Œåˆ†ç±»ï¼Œæå‡ºäº†ç«¯åˆ°ç«¯VLAå’ŒåŒç³»ç»ŸVLAä¸¤ç§ä¸»è¦èŒƒä¾‹ï¼Œå¹¶å¯¹å„ç§VLAæ–¹æ³•çš„ç‰¹ç‚¹å’Œä¼˜ç¼ºç‚¹è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ€»ç»“äº†ç”¨äºè¯„ä¼°VLAæ¨¡å‹çš„ä»£è¡¨æ€§æ•°æ®é›†å’ŒåŸºå‡†ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡æœ¬èº«æ˜¯ä¸€ç¯‡ç»¼è¿°ï¼Œå› æ­¤æ²¡æœ‰å…·ä½“çš„æ¨¡å‹è®¾è®¡ç»†èŠ‚ã€‚ä½†æ˜¯ï¼Œè®ºæ–‡ä¸­è®¨è®ºçš„VLAæ¨¡å‹é€šå¸¸ä¼šæ¶‰åŠåˆ°ä»¥ä¸‹å…³é”®è®¾è®¡ï¼šè§†è§‰ç¼–ç å™¨ï¼ˆä¾‹å¦‚ï¼Œå·ç§¯ç¥ç»ç½‘ç»œæˆ–Transformerï¼‰ç”¨äºæå–å›¾åƒç‰¹å¾ï¼›è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒTransformerï¼‰ç”¨äºå¤„ç†è¯­è¨€æŒ‡ä»¤å’Œè¿›è¡Œæ¨ç†ï¼›åŠ¨ä½œç”Ÿæˆå™¨ï¼ˆä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œæˆ–è§„åˆ’å™¨ï¼‰ç”¨äºç”Ÿæˆé©¾é©¶åŠ¨ä½œã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡é€šå¸¸ä¼šæ¶‰åŠåˆ°æ¨¡ä»¿å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ æˆ–ä¸¤è€…ç»“åˆçš„æ–¹æ³•ï¼Œä»¥è®­ç»ƒæ¨¡å‹ç”Ÿæˆç¬¦åˆäººç±»é©¾é©¶ä¹ æƒ¯çš„åŠ¨ä½œã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16760v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16760v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16760v1/figures/fig3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡æ˜¯ä¸€ç¯‡ç»¼è¿°æ€§æ–‡ç« ï¼Œæ²¡æœ‰å…·ä½“çš„å®éªŒç»“æœã€‚ä½†æ˜¯ï¼Œè®ºæ–‡æ€»ç»“äº†ç°æœ‰VLAæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚ä¾‹å¦‚ï¼Œå¦‚ä½•æé«˜VLAæ¨¡å‹çš„é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’ŒæŒ‡ä»¤ä¿çœŸåº¦ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹æ¥æå‡VLAæ¨¡å‹çš„æ€§èƒ½ç­‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å¯¹è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚VLAæ¨¡å‹èƒ½å¤Ÿæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„é€šç”¨æ€§ã€å¯è§£é‡Šæ€§å’Œäººæœºäº¤äº’èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„é©¾é©¶ç¯å¢ƒï¼Œå¹¶èƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œäººç±»çš„æŒ‡ä»¤ã€‚æœªæ¥ï¼ŒVLAæ¨¡å‹æœ‰æœ›è¢«åº”ç”¨äºå„ç§è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ŒåŒ…æ‹¬ä¹˜ç”¨è½¦ã€å¡è½¦å’Œæ— äººé…é€è½¦ç­‰ï¼Œä»è€Œæé«˜äº¤é€šè¿è¾“çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous driving has long relied on modular "Perception-Decision-Action" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.

