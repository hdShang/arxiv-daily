---
layout: default
title: PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence
---

# PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.16793" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.16793v1</a>
  <a href="https://arxiv.org/pdf/2512.16793.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.16793v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.16793v1', 'PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Cong Huang, Bojun Cheng, Kai Chen

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

**å¤‡æ³¨**: 17 pages, 4 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPhysBrainä»¥è§£å†³æœºå™¨äººè§†è§‰è¯­è¨€æ¨¡å‹ä¸ç‰©ç†æ™ºèƒ½çš„åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥` `ç‰©ç†æ™ºèƒ½` `è§†è§‰è¯­è¨€æ¨¡å‹` `æœºå™¨äººæ§åˆ¶` `æ•°æ®é›†æ„å»º` `å¤šå±‚æ¬¡ç›‘ç£` `å› æœç»“æ„` `é•¿è¿œè§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸»è¦ä¾èµ–ç¬¬ä¸‰äººç§°æ•°æ®ï¼Œå¯¼è‡´æœºå™¨äººåœ¨è‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
2. æå‡ºäº†Egocentric2Embodimentç¿»è¯‘ç®¡é“ï¼Œå°†ç¬¬ä¸€äººç§°è§†é¢‘è½¬åŒ–ä¸ºç»“æ„åŒ–çš„å¤šå±‚æ¬¡ç›‘ç£ï¼Œæ„å»ºäº†å¤§è§„æ¨¡E2E-3Mæ•°æ®é›†ã€‚
3. PhysBrainåœ¨E2E-3Mæ•°æ®é›†ä¸Šè®­ç»ƒåï¼Œå±•ç°å‡ºæ›´å¼ºçš„è‡ªæˆ‘ä¸­å¿ƒç†è§£èƒ½åŠ›ï¼ŒVLAå¾®è°ƒçš„æ ·æœ¬æ•ˆç‡æ˜¾è‘—æé«˜ï¼ŒæˆåŠŸç‡è¾¾åˆ°53.9%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨äººæ³›åŒ–ä¾èµ–äºç‰©ç†æ™ºèƒ½ï¼Œå³åœ¨è‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥å’Œè¡ŒåŠ¨ä¸‹æ¨ç†çŠ¶æ€å˜åŒ–ã€æ¥è§¦ä¸°å¯Œçš„äº¤äº’å’Œé•¿è¿œè§„åˆ’çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è§†è§‰è¯­è¨€æ¨¡å‹ä¸»è¦åœ¨ç¬¬ä¸‰äººç§°æ•°æ®ä¸Šè®­ç»ƒï¼Œå¯¼è‡´äººå½¢æœºå™¨äººé¢ä¸´è§†è§’ä¸åŒ¹é…çš„é—®é¢˜ã€‚æ”¶é›†æœºå™¨äººè‡ªæˆ‘ä¸­å¿ƒæ•°æ®çš„è§„æ¨¡åŒ–ä»ç„¶ä¸åˆ‡å®é™…ï¼Œè€Œå¤§è§„æ¨¡çš„äººç±»è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†Egocentric2Embodimentç¿»è¯‘ç®¡é“ï¼Œå°†ç¬¬ä¸€äººç§°è§†é¢‘è½¬åŒ–ä¸ºå¤šå±‚æ¬¡ã€åŸºäºæ¨¡å¼çš„è§†è§‰é—®ç­”ç›‘ç£ï¼Œæ„å»ºäº†Egocentric2Embodimentæ•°æ®é›†ï¼ˆE2E-3Mï¼‰ã€‚é€šè¿‡åœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè·å¾—äº†ä¸€ä¸ªè‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥çš„å…·èº«æ™ºèƒ½ä½“PhysBrainï¼Œå±•ç°å‡ºæ˜¾è‘—çš„è‡ªæˆ‘ä¸­å¿ƒç†è§£èƒ½åŠ›ï¼Œå°¤å…¶åœ¨EgoThinkè§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨è‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥ä¸‹çš„ç‰©ç†æ™ºèƒ½ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç¬¬ä¸‰äººç§°æ•°æ®ï¼Œå¯¼è‡´è§†è§’ä¸åŒ¹é…å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºEgocentric2Embodimentç¿»è¯‘ç®¡é“ï¼Œå°†äººç±»è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è®­ç»ƒç›‘ç£ï¼Œåˆ©ç”¨ä¸°å¯Œçš„äº¤äº’ä¸Šä¸‹æ–‡å’Œå› æœç»“æ„æ¥æå‡æœºå™¨äººçš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è§†é¢‘å¤„ç†ã€ç›‘ç£ç”Ÿæˆå’Œæ¨¡å‹è®­ç»ƒå››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†äººç±»è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ï¼Œç„¶åé€šè¿‡ç¿»è¯‘ç®¡é“ç”Ÿæˆå¤šå±‚æ¬¡çš„è§†è§‰é—®ç­”ç›‘ç£ï¼Œæœ€ååœ¨E2E-3Mæ•°æ®é›†ä¸Šè®­ç»ƒPhysBrainã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†åŸå§‹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è®­ç»ƒç›‘ç£ï¼Œç¡®ä¿äº†è¯æ®çš„åŸºç¡€å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­æ˜¯ç¼ºä¹çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¿»è¯‘ç®¡é“ä¸­ï¼Œé‡‡ç”¨äº†å¤šå±‚æ¬¡çš„æ¨¡å¼é©±åŠ¨æ–¹æ³•ï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ç¡®ä¿ç”Ÿæˆçš„ç›‘ç£å…·æœ‰é«˜è´¨é‡å’Œä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†ç½‘ç»œç»“æ„ä»¥é€‚åº”è‡ªæˆ‘ä¸­å¿ƒæ•°æ®çš„ç‰¹æ€§ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16793v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16793v1/fig/data_pipeline.jpg" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.16793v1/fig/data_sum.jpg" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨E2E-3Mæ•°æ®é›†ä¸Šè®­ç»ƒçš„PhysBrainå±•ç°å‡ºæ˜¾è‘—çš„è‡ªæˆ‘ä¸­å¿ƒç†è§£èƒ½åŠ›ï¼Œå°¤å…¶åœ¨EgoThinkä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸç‡è¾¾åˆ°53.9%ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒPhysBrainåœ¨æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡æˆåŠŸç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†ä»äººç±»è‡ªæˆ‘ä¸­å¿ƒç›‘ç£åˆ°æœºå™¨äººæ§åˆ¶çš„æœ‰æ•ˆè½¬ç§»ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ™ºèƒ½å®¶å±…ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨è‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥ä¸‹çš„ç†è§£èƒ½åŠ›ï¼ŒPhysBrainèƒ½å¤Ÿæ›´å¥½åœ°æ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œå¢å¼ºæœºå™¨äººåœ¨ç°å®ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œçµæ´»æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.

