---
layout: default
title: Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions
---

# Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11214" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11214v1</a>
  <a href="https://arxiv.org/pdf/2505.11214.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11214v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11214v1', 'Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Wei Zhao, Gongsheng Li, Zhefei Gong, Pengxiang Ding, Han Zhao, Donglin Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOE-VLAä»¥è§£å†³å¤šæ¨¡æ€æŒ‡ä»¤ä¸‹çš„æœºå™¨äººäº¤äº’é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `å¤šæ¨¡æ€æŒ‡ä»¤` `äººæœºäº¤äº’` `æœºå™¨äººæŠ€æœ¯` `å¼€æ”¾å¼ä»»åŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ä»…æ”¯æŒè¯­è¨€æŒ‡ä»¤ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæ ·åŒ–äººæœºäº¤äº’ä¸­çš„åº”ç”¨ã€‚
2. OE-VLAæ¨¡å‹é€šè¿‡å¼•å…¥å¼€æ”¾å¼å¤šæ¨¡æ€æŒ‡ä»¤ï¼Œæå‡äº†VLAæ¨¡å‹çš„é€‚ç”¨æ€§å’Œçµæ´»æ€§ã€‚
3. OE-VLAåœ¨å¤šç§å¼€æ”¾å¼ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»ŸVLAæ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚è¯¥æ¨¡å‹åˆ©ç”¨åœ¨å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®ä¸Šè®­ç»ƒçš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡å•ä¸€çš„ç«¯åˆ°ç«¯ç¥ç»ç½‘ç»œç›´æ¥ä»è§†è§‰è§‚å¯Ÿå’Œäººç±»æŒ‡ä»¤ç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLAæ¨¡å‹é€šå¸¸ä»…æ¥å—è¯­è¨€æŒ‡ä»¤ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¼€æ”¾å¼äººæœºäº¤äº’ä¸­çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†OE-VLAï¼Œæ¢ç´¢VLAæ¨¡å‹åœ¨å¼€æ”¾å¼å¤šæ¨¡æ€æŒ‡ä»¤ä¸‹çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOE-VLAä¸ä»…åœ¨è¯­è¨€è¾“å…¥ä¸‹çš„è¡¨ç°ä¸ä¼ ç»ŸVLAæ¨¡å‹ç›¸å½“ï¼Œè¿˜åœ¨å››ç±»å¼€æ”¾å¼ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚è¿™ä¸€æ–¹æ³•æœ‰æœ›æ˜¾è‘—æ‰©å±•VLAæ¨¡å‹åœ¨æ—¥å¸¸åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå¹¶ä¿ƒè¿›äººæœºäº¤äº’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰VLAæ¨¡å‹ä»…æ”¯æŒè¯­è¨€æŒ‡ä»¤çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¼€æ”¾å¼äººæœºäº¤äº’ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOE-VLAæ¨¡å‹é€šè¿‡å¼•å…¥å¤šæ¨¡æ€æŒ‡ä»¤ï¼ˆå¦‚å›¾åƒã€è§†é¢‘ç­‰ï¼‰ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´çµæ´»åœ°ç†è§£å’Œæ‰§è¡Œä»»åŠ¡ï¼Œä»è€Œæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOE-VLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰è¾“å…¥å¤„ç†æ¨¡å—ã€è¯­è¨€ç†è§£æ¨¡å—å’ŒåŠ¨ä½œç”Ÿæˆæ¨¡å—ï¼Œä¸‰è€…é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„ç«¯åˆ°ç«¯ç¥ç»ç½‘ç»œè¿›è¡ŒååŒå·¥ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šOE-VLAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿå¤„ç†å¤šç§å½¢å¼çš„è¾“å…¥æŒ‡ä»¤ï¼Œçªç ´äº†ä¼ ç»ŸVLAæ¨¡å‹çš„å±€é™ï¼Œä½¿å…¶åœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼ŒOE-VLAé‡‡ç”¨äº†å¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œç»“åˆäº†è§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°è®¾è®¡ä¸Šå¼•å…¥äº†å¤šä»»åŠ¡å­¦ä¹ æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–ä¸åŒç±»å‹æŒ‡ä»¤çš„å¤„ç†æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OE-VLAåœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»ŸVLAæ¨¡å‹ï¼Œå°¤å…¶åœ¨å››ç±»æ–°ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€æŒ‡ä»¤å¤„ç†ä¸Šçš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OE-VLAæ¨¡å‹çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€æœåŠ¡æœºå™¨äººã€æ•™è‚²è¾…åŠ©ç­‰é¢†åŸŸã€‚é€šè¿‡æ”¯æŒå¤šæ¨¡æ€æŒ‡ä»¤ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ›´è‡ªç„¶åœ°ä¸äººç±»ç”¨æˆ·äº’åŠ¨ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have recently become highly prominent in the field of robotics. Leveraging vision-language foundation models trained on large-scale internet data, the VLA model can generate robotic actions directly from visual observations and human instructions through a single end-to-end neural network. Despite their effectiveness, current VLA models usually accept only one form of human prompting, language instructions, which may constrain their applicability in open-ended human-robot interactions. For example, a user might expect the robot to retrieve an object shown in an image, follow an instruction written on the whiteboard, or imitate a behavior demonstrated in a video, rather than relying solely on language-based descriptions. To address this gap, we introduce OE-VLA, which explores the potential of VLA models for open-ended multimodal instructions. Extensive results demonstrate that our OE-VLA not only achieves comparable performance to traditional VLA models with linguistic input but also delivers impressive results across four additional categories of open-ended tasks. The proposed methodology could significantly expand the applications of VLA models across various everyday scenarios and facilitate human-robot interaction.

