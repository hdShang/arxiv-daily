---
layout: default
title: Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild
---

# Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11350" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11350v5</a>
  <a href="https://arxiv.org/pdf/2505.11350.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11350v5" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11350v5', 'Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Derek Ming Siang Tan, Shailesh, Boyang Liu, Alok Raj, Qi Xuan Ang, Weiheng Dai, Tanishq Duhan, Jimmy Chiun, Yuhong Cao, Florian Shkurti, Guillaume Sartoretti

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-16 (æ›´æ–°: 2025-11-07)

**å¤‡æ³¨**: Accepted for presentation at CORL 2025. Code, models, and data are available at https://search-tta.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSearch-TTAæ¡†æ¶ä»¥è§£å†³æˆ·å¤–è§†è§‰æœç´¢ä¸­çš„ä¿¡æ¯ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€é€‚åº”` `è§†è§‰æœç´¢` `å«æ˜Ÿå›¾åƒ` `æ·±åº¦å­¦ä¹ ` `æ— äººæœºå¯¼èˆª` `æ¨¡å‹ä¼˜åŒ–` `ä¸ç¡®å®šæ€§å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰æœç´¢æ–¹æ³•å¾€å¾€å‡è®¾æ²¡æœ‰å…ˆéªŒä¿¡æ¯ï¼Œæˆ–æœªè€ƒè™‘å…ˆéªŒä¿¡æ¯çš„è·å–æ–¹å¼ï¼Œå¯¼è‡´æœç´¢æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„Search-TTAæ¡†æ¶é€šè¿‡åŠ¨æ€ä¼˜åŒ–CLIPçš„é¢„æµ‹ï¼Œç»“åˆå¤šæ¨¡æ€è¾“å…¥ï¼Œæå‡äº†è§†è§‰æœç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSearch-TTAåœ¨è§„åˆ’æ€§èƒ½ä¸Šæé«˜äº†30.0%ï¼Œå¹¶åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ•°æ®ä¸è¶³çš„æƒ…å†µä¸‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è¿›è¡Œæˆ·å¤–è§†è§‰å¯¼èˆªå’Œæœç´¢ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨å«æ˜Ÿå›¾åƒç”Ÿæˆè§†è§‰å…ˆéªŒä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¾€å¾€å‡è®¾æ²¡æœ‰å…ˆéªŒä¿¡æ¯ï¼Œæˆ–ä½¿ç”¨çš„å…ˆéªŒä¿¡æ¯æœªè€ƒè™‘å…¶è·å–æ–¹å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Search-TTAï¼Œä¸€ä¸ªå¤šæ¨¡æ€æµ‹è¯•æ—¶é€‚åº”æ¡†æ¶ï¼Œæ”¯æŒå¤šç§è¾“å…¥æ¨¡æ€å’Œè§„åˆ’æ–¹æ³•ã€‚é€šè¿‡é¢„è®­ç»ƒå«æ˜Ÿå›¾åƒç¼–ç å™¨å¹¶åŠ¨æ€ä¼˜åŒ–CLIPçš„é¢„æµ‹ï¼ŒSearch-TTAåœ¨è§†è§‰æœç´¢ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨åˆå§‹é¢„æµ‹ä¸ä½³çš„æƒ…å†µä¸‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨çœŸå®æ— äººæœºä¸Šæµ‹è¯•æ—¶è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æˆ·å¤–è§†è§‰æœç´¢ä¸­ç”±äºç¼ºä¹é«˜è´¨é‡å…ˆéªŒä¿¡æ¯è€Œå¯¼è‡´çš„æœç´¢æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å«æ˜Ÿå›¾åƒæ—¶ï¼Œå¾€å¾€æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¿¡æ¯ï¼Œå¯¼è‡´è§„åˆ’ç­–ç•¥ä¸å¤Ÿç²¾å‡†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSearch-TTAæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤šæ¨¡æ€è¾“å…¥å’ŒåŠ¨æ€ä¼˜åŒ–ç­–ç•¥ï¼Œæå‡è§†è§‰æœç´¢çš„å‡†ç¡®æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œæ¡†æ¶åˆ©ç”¨å«æ˜Ÿå›¾åƒç¼–ç å™¨ä¸CLIPè§†è§‰ç¼–ç å™¨çš„å¯¹é½ï¼Œè¾“å‡ºç›®æ ‡å­˜åœ¨çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶åœ¨æœç´¢è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´é¢„æµ‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSearch-TTAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯å«æ˜Ÿå›¾åƒç¼–ç å™¨çš„é¢„è®­ç»ƒï¼Œç¡®ä¿å…¶ä¸CLIPç¼–ç å™¨çš„è¾“å‡ºä¸€è‡´ï¼›å…¶æ¬¡æ˜¯åŸºäºä¸ç¡®å®šæ€§åŠ æƒçš„æ¢¯åº¦æ›´æ–°æœºåˆ¶ï¼ŒåŠ¨æ€ä¼˜åŒ–CLIPçš„é¢„æµ‹ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§è¾“å…¥æ¨¡æ€ï¼Œå¦‚å›¾åƒã€æ–‡æœ¬å’Œå£°éŸ³ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸ç¡®å®šæ€§åŠ æƒçš„æ¢¯åº¦æ›´æ–°æ–¹æ³•ï¼Œçµæ„Ÿæ¥æºäºç©ºé—´æ³Šæ¾ç‚¹è¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœç´¢è¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°è°ƒæ•´é¢„æµ‹ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œé‡‡ç”¨äº†ä¸CLIPç›¸å¯¹é½çš„å«æ˜Ÿå›¾åƒç¼–ç å™¨ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†ä¸ç¡®å®šæ€§å› ç´ ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä¸åŒæ¨¡æ€ä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSearch-TTAåœ¨è§„åˆ’æ€§èƒ½ä¸Šæé«˜äº†30.0%ï¼Œå°¤å…¶åœ¨åˆå§‹CLIPé¢„æµ‹ä¸ä½³çš„æƒ…å†µä¸‹è¡¨ç°æ˜¾è‘—ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢ä¸æ›´å¤§è§„æ¨¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Search-TTAæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— äººæœºè§†è§‰å¯¼èˆªã€ç¯å¢ƒç›‘æµ‹å’Œæœç´¢æ•‘æ´ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡è§†è§‰æœç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¸®åŠ©æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´æœ‰æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To perform outdoor visual navigation and search, a robot may leverage satellite imagery to generate visual priors. This can help inform high-level search strategies, even when such images lack sufficient resolution for target recognition. However, many existing informative path planning or search-based approaches either assume no prior information, or use priors without accounting for how they were obtained. Recent work instead utilizes large Vision Language Models (VLMs) for generalizable priors, but their outputs can be inaccurate due to hallucination, leading to inefficient search. To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation framework with a flexible plug-and-play interface compatible with various input modalities (e.g., image, text, sound) and planning methods (e.g., RL-based). First, we pretrain a satellite image encoder to align with CLIP's visual encoder to output probability distributions of target presence used for visual search. Second, our TTA framework dynamically refines CLIP's predictions during search using uncertainty-weighted gradient updates inspired by Spatial Poisson Point Processes. To train and evaluate Search-TTA, we curate AVS-Bench, a visual search dataset based on internet-scale ecological data containing 380k images and taxonomy data. We find that Search-TTA improves planner performance by up to 30.0%, particularly in cases with poor initial CLIP predictions due to domain mismatch and limited training data. It also performs comparably with significantly larger VLMs, and achieves zero-shot generalization via emergent alignment to unseen modalities. Finally, we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation that provides onboard sensing.

