---
layout: default
title: "Counterfactual Behavior Cloning: Offline Imitation Learning from Imperfect Human Demonstrations"
---

# Counterfactual Behavior Cloning: Offline Imitation Learning from Imperfect Human Demonstrations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.10760" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.10760v1</a>
  <a href="https://arxiv.org/pdf/2505.10760.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.10760v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.10760v1', 'Counterfactual Behavior Cloning: Offline Imitation Learning from Imperfect Human Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shahabedin Sagheb, Dylan P. Losey

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCounter-BCä»¥è§£å†³äººç±»ç¤ºèŒƒä¸å®Œç¾é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `åäº‹å®æ¨ç†` `è¡Œä¸ºå…‹éš†` `æœºå™¨äººå­¦ä¹ ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä»…ä¾èµ–äºäººç±»æ•™å¸ˆçš„å…·ä½“ç¤ºèŒƒï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†ç¤ºèŒƒä¸­çš„é”™è¯¯å’Œæ¬¡ä¼˜è¡Œä¸ºã€‚
2. æœ¬æ–‡æå‡ºCounter-BCï¼Œé€šè¿‡å‡è®¾äººç±»ç¤ºèŒƒä¼ è¾¾ä¸€è‡´çš„ç­–ç•¥ï¼Œæ‰©å±•æ•°æ®é›†ä»¥åŒ…å«å¯èƒ½çš„åäº‹å®è¡Œä¸ºï¼Œä»è€Œæå–æ½œåœ¨ç­–ç•¥ã€‚
3. Counter-BCåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨å¤„ç†å™ªå£°ç¤ºèŒƒæ—¶æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å­¦ä¹ äººç±»è¡Œä¸ºå¯¹æœºå™¨äººä»»åŠ¡æ‰§è¡Œè‡³å…³é‡è¦ï¼Œä½†äººç±»ç¤ºèŒƒå¾€å¾€å­˜åœ¨é”™è¯¯å’Œæ¬¡ä¼˜æ€§ã€‚ç°æœ‰æ–¹æ³•ä»…æ¨¡ä»¿äººç±»çš„å…·ä½“è¡Œä¸ºï¼Œé™åˆ¶äº†å­¦ä¹ æ•ˆæœã€‚æœ¬æ–‡æå‡ºCounter-BCï¼Œé€šè¿‡å‡è®¾äººç±»ç¤ºèŒƒä¼ è¾¾ä¸€è‡´çš„ç­–ç•¥ï¼Œæ‰©å±•æ•°æ®é›†ä»¥åŒ…å«å¯èƒ½çš„åäº‹å®è¡Œä¸ºï¼Œä»è€Œæå–æ½œåœ¨çš„ç­–ç•¥ã€‚Counter-BCåœ¨ç†è®ºä¸Šè¯æ˜èƒ½å¤Ÿä»ä¸å®Œç¾æ•°æ®ä¸­æå–æ‰€éœ€ç­–ç•¥ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­ä¸æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»ä¸å®Œç¾äººç±»ç¤ºèŒƒä¸­å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåº”å¯¹ç¤ºèŒƒä¸­çš„é”™è¯¯å’Œæ¬¡ä¼˜è¡Œä¸ºï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœå—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šCounter-BCçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å‡è®¾æ‰€æœ‰äººç±»ç¤ºèŒƒè¯•å›¾ä¼ è¾¾ä¸€ä¸ªä¸€è‡´çš„ç­–ç•¥ï¼Œæ‰©å±•æ•°æ®é›†ä»¥åŒ…å«äººç±»å¯èƒ½æ„å›¾çš„åäº‹å®è¡Œä¸ºï¼Œä»è€Œæ›´å¥½åœ°æ¢å¤æ½œåœ¨ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCounter-BCçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ‰©å±•ã€ç¤ºèŒƒä¿®æ”¹å’Œç­–ç•¥æå–ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œæ‰©å±•æ•°æ®é›†ä»¥åŒ…å«åäº‹å®è¡Œä¸ºï¼›å…¶æ¬¡ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¿®æ”¹äººç±»ç¤ºèŒƒï¼›æœ€åï¼Œæå–ä¸€è‡´çš„æ½œåœ¨ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šCounter-BCçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ‰©å±•æ•°æ®é›†çš„èƒ½åŠ›ï¼Œä½¿å¾—æœºå™¨äººèƒ½å¤Ÿå­¦ä¹ åˆ°äººç±»æ•™å¸ˆæœªå±•ç¤ºçš„æ½œåœ¨è¡Œä¸ºï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒCounter-BCä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¡¡é‡ç¤ºèŒƒä¸åäº‹å®è¡Œä¸ºä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶è®¾è®¡äº†é€‚åº”ä¸åŒæŠ€èƒ½æ°´å¹³æ•™å¸ˆçš„ç½‘ç»œç»“æ„ï¼Œä»¥æé«˜ç­–ç•¥æå–çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒCounter-BCåœ¨å¤„ç†å¸¦å™ªå£°çš„ç¤ºèŒƒæ—¶ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„å¯¹æ¯”æ–¹æ³•ï¼Œç­–ç•¥æå–çš„å‡†ç¡®æ€§æé«˜äº†20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æœºå™¨äººå­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜æœºå™¨äººä»äººç±»ç¤ºèŒƒä¸­å­¦ä¹ çš„èƒ½åŠ›ï¼ŒCounter-BCèƒ½å¤Ÿä¿ƒè¿›æ›´æ™ºèƒ½çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼Œæå‡äººæœºåä½œçš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning from humans is challenging because people are imperfect teachers. When everyday humans show the robot a new task they want it to perform, humans inevitably make errors (e.g., inputting noisy actions) and provide suboptimal examples (e.g., overshooting the goal). Existing methods learn by mimicking the exact behaviors the human teacher provides -- but this approach is fundamentally limited because the demonstrations themselves are imperfect. In this work we advance offline imitation learning by enabling robots to extrapolate what the human teacher meant, instead of only considering what the human actually showed. We achieve this by hypothesizing that all of the human's demonstrations are trying to convey a single, consistent policy, while the noise and sub-optimality within their behaviors obfuscates the data and introduces unintentional complexity. To recover the underlying policy and learn what the human teacher meant, we introduce Counter-BC, a generalized version of behavior cloning. Counter-BC expands the given dataset to include actions close to behaviors the human demonstrated (i.e., counterfactual actions that the human teacher could have intended, but did not actually show). During training Counter-BC autonomously modifies the human's demonstrations within this expanded region to reach a simple and consistent policy that explains the underlying trends in the human's dataset. Theoretically, we prove that Counter-BC can extract the desired policy from imperfect data, multiple users, and teachers of varying skill levels. Empirically, we compare Counter-BC to state-of-the-art alternatives in simulated and real-world settings with noisy demonstrations, standardized datasets, and real human teachers. See videos of our work here: https://youtu.be/XaeOZWhTt68

