---
layout: default
title: Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem
---

# Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08768" target="_blank" class="toolbar-btn">arXiv: 2510.08768v1</a>
    <a href="https://arxiv.org/pdf/2510.08768.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08768v1" 
            onclick="toggleFavorite(this, '2510.08768v1', 'Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham\'s Pi Theorem')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Francisco Pascoa, Ian Lalonde, Alexandre Girard

**ÂàÜÁ±ª**: cs.LG, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Âà©Áî®ÁôΩÈáëÊ±âœÄÂÆöÁêÜÂÆûÁé∞Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÈõ∂Ê†∑Êú¨Á≠ñÁï•ËøÅÁßª**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Èõ∂Ê†∑Êú¨ËøÅÁßª` `ÁôΩÈáëÊ±âœÄÂÆöÁêÜ` `ÈáèÁ∫≤ÂàÜÊûê` `Êú∫Âô®‰∫∫ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Âú®Èù¢ÂØπÁâ©ÁêÜÂèÇÊï∞ÂèòÂåñÊó∂Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Âü∫‰∫éÁôΩÈáëÊ±âœÄÂÆöÁêÜÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßªÊñπÊ≥ïÔºåÈÄöËøáÁº©ÊîæÁ≠ñÁï•ÁöÑËæìÂÖ•ÂíåËæìÂá∫Êù•ÈÄÇÂ∫îÊñ∞ÁöÑÁéØÂ¢É„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Âä®ÊÄÅÁõ∏‰ººÁéØÂ¢É‰∏≠‰øùÊåÅÊÄßËÉΩÔºåÂπ∂Âú®ÈùûÁõ∏‰ººÁéØÂ¢É‰∏≠ÊòæËëó‰ºò‰∫éÊú¥Á¥†ËøÅÁßªÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†(RL)Á≠ñÁï•ÈÄöÂ∏∏Èöæ‰ª•Ê≥õÂåñÂà∞ÂÖ∑Êúâ‰∏çÂêåÁâ©ÁêÜÂèÇÊï∞ÁöÑÊñ∞Êú∫Âô®‰∫∫„ÄÅ‰ªªÂä°ÊàñÁéØÂ¢ÉÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÈÄÇÁî®ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁôΩÈáëÊ±âœÄÂÆöÁêÜÁöÑÁÆÄÂçïÈõ∂Ê†∑Êú¨ËøÅÁßªÊñπÊ≥ïÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÈôêÂà∂„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊó†ÈáèÁ∫≤Á©∫Èó¥Áº©ÊîæÂÖ∂ËæìÂÖ•ÔºàËßÇÂØüÔºâÂíåËæìÂá∫ÔºàÂä®‰ΩúÔºâÊù•‰ΩøÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÈÄÇÂ∫îÊñ∞ÁöÑÁ≥ªÁªüÁéØÂ¢ÉÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇËØ•ÊñπÊ≥ïÂú®‰∏â‰∏™Â§çÊùÇÂ∫¶ÈÄíÂ¢ûÁöÑÁéØÂ¢É‰∏≠ËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂàÜÂà´ÊòØÊ®°ÊãüÊëÜ„ÄÅÁî®‰∫éÊ®°ÊãüÂà∞ÁúüÂÆûÈ™åËØÅÁöÑÁâ©ÁêÜÊëÜ‰ª•ÂèäÈ´òÁª¥HalfCheetah„ÄÇÁªìÊûúË°®ÊòéÔºåÁº©ÊîæËøÅÁßªÂú®Âä®ÊÄÅÁõ∏‰ººÁöÑÁéØÂ¢É‰∏≠Ê≤°ÊúâÊÄßËÉΩÊçüÂ§±„ÄÇÊ≠§Â§ñÔºåÂú®ÈùûÁõ∏‰ººÁéØÂ¢É‰∏≠ÔºåÁº©ÊîæÁ≠ñÁï•ÂßãÁªà‰ºò‰∫éÊú¥Á¥†ËøÅÁßªÔºåÊòæËëóÊâ©Â§ß‰∫ÜÂéüÂßãÁ≠ñÁï•‰ªçÁÑ∂ÊúâÊïàÁöÑÁéØÂ¢ÉËåÉÂõ¥„ÄÇËøô‰∫õÂèëÁé∞Ë°®ÊòéÔºåÈáèÁ∫≤ÂàÜÊûê‰∏∫Â¢ûÂº∫RLÁ≠ñÁï•ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõÊèê‰æõ‰∫Ü‰∏ÄÁßçÂº∫Â§ßËÄåÂÆûÁî®ÁöÑÂ∑•ÂÖ∑„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Âú®‰∏çÂêåÁâ©ÁêÜÂèÇÊï∞ÁöÑÊú∫Âô®‰∫∫ÊàñÁéØÂ¢É‰∏≠Ê≥õÂåñËÉΩÂäõÂ∑ÆÔºåÂØºËá¥ÈúÄË¶ÅÈíàÂØπÊØè‰∏™Êñ∞ÁéØÂ¢ÉÈáçÊñ∞ËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÂÆûÁé∞Èõ∂Ê†∑Êú¨ËøÅÁßªÔºåÂç≥Êó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉÂç≥ÂèØÂ∞ÜÁ≠ñÁï•Â∫îÁî®Âà∞Êñ∞ÁéØÂ¢É„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂà©Áî®ÁôΩÈáëÊ±âœÄÂÆöÁêÜËøõË°åÈáèÁ∫≤ÂàÜÊûêÔºåÂ∞ÜÁâ©ÁêÜÂèÇÊï∞‰∏çÂêåÁöÑÁ≥ªÁªüÊò†Â∞ÑÂà∞Êó†ÈáèÁ∫≤Á©∫Èó¥„ÄÇÈÄöËøáÂú®Êó†ÈáèÁ∫≤Á©∫Èó¥‰∏≠Áº©ÊîæÁ≠ñÁï•ÁöÑËæìÂÖ•ÔºàËßÇÂØüÔºâÂíåËæìÂá∫ÔºàÂä®‰ΩúÔºâÔºå‰ΩøÁ≠ñÁï•ËÉΩÂ§üÈÄÇÂ∫îÊñ∞ÁöÑÁ≥ªÁªüÁéØÂ¢ÉÔºå‰ªéËÄåÂÆûÁé∞Èõ∂Ê†∑Êú¨ËøÅÁßª„ÄÇÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂà©Áî®Áâ©ÁêÜÁ≥ªÁªüÁöÑÁõ∏‰ººÊÄßÔºåÂú®Êó†ÈáèÁ∫≤Á©∫Èó¥‰∏≠ÊâæÂà∞ÂØπÂ∫îÂÖ≥Á≥ª„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Ôºö1) ÂØπÂéüÂßãÁéØÂ¢ÉËøõË°åÈáèÁ∫≤ÂàÜÊûêÔºåÁ°ÆÂÆöÊó†ÈáèÁ∫≤ÂèÇÊï∞Ôºõ2) ‰ΩøÁî®ÂéüÂßãÁéØÂ¢ÉËÆ≠ÁªÉÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Ôºõ3) ÂØπÁõÆÊ†áÁéØÂ¢ÉËøõË°åÈáèÁ∫≤ÂàÜÊûêÔºåÁ°ÆÂÆöÊó†ÈáèÁ∫≤ÂèÇÊï∞Ôºõ4) Ê†πÊçÆÊó†ÈáèÁ∫≤ÂèÇÊï∞ÁöÑÊØî‰æãÂÖ≥Á≥ªÔºåÁº©ÊîæÁõÆÊ†áÁéØÂ¢ÉÁöÑËßÇÂØüÂíåÂä®‰ΩúÁ©∫Èó¥Ôºõ5) Â∞ÜÁº©ÊîæÂêéÁöÑËßÇÂØüËæìÂÖ•Âà∞È¢ÑËÆ≠ÁªÉÁ≠ñÁï•‰∏≠ÔºåÂæóÂà∞Áº©ÊîæÂêéÁöÑÂä®‰ΩúËæìÂá∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÂ∞ÜÁôΩÈáëÊ±âœÄÂÆöÁêÜÂ∫îÁî®‰∫éÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ËøÅÁßªÔºåÈÄöËøáÈáèÁ∫≤ÂàÜÊûêÂÆûÁé∞Èõ∂Ê†∑Êú¨ËøÅÁßªÔºåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉÁ≠ñÁï•„ÄÇ‰∏éÁé∞ÊúâËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ï‰∏çÈúÄË¶Å‰ªª‰ΩïÁõÆÊ†áÁéØÂ¢ÉÁöÑÊï∞ÊçÆÔºåÊòØ‰∏ÄÁßçÁúüÊ≠£ÁöÑÈõ∂Ê†∑Êú¨ÊñπÊ≥ï„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïÈÄâÊã©ÂêàÈÄÇÁöÑÁâ©ÁêÜÂèÇÊï∞ËøõË°åÈáèÁ∫≤ÂàÜÊûêÔºå‰ª•ÂèäÂ¶Ç‰ΩïÁ°ÆÂÆöÊó†ÈáèÁ∫≤ÂèÇÊï∞ÁöÑÁº©ÊîæÊØî‰æã„ÄÇËÆ∫Êñá‰∏≠ÂÖ∑‰ΩìÂÆûÁé∞‰æùËµñ‰∫éÂØπÁâπÂÆöÁéØÂ¢ÉÁöÑÁâ©ÁêÜÁü•ËØÜÔºå‰æãÂ¶ÇÊëÜÁöÑÈïøÂ∫¶„ÄÅË¥®Èáè„ÄÅÈáçÂäõÂä†ÈÄüÂ∫¶Á≠â„ÄÇÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑ‰∏éÂéüÂßãÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Áõ∏ÂêåÔºåÊó†ÈúÄ‰øÆÊîπ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Âä®ÊÄÅÁõ∏‰ººÁöÑÁéØÂ¢É‰∏≠Ê≤°ÊúâÊÄßËÉΩÊçüÂ§±ÔºåÂú®ÈùûÁõ∏‰ººÁöÑÁéØÂ¢É‰∏≠ÊòæËëó‰ºò‰∫éÊú¥Á¥†ËøÅÁßªÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÂú®HalfCheetahÁéØÂ¢É‰∏≠ÔºåÁº©ÊîæÁ≠ñÁï•ÁöÑÊÄßËÉΩÊòéÊòæÈ´ò‰∫éÊú™Áº©ÊîæÁöÑÁ≠ñÁï•ÔºåÊâ©Â§ß‰∫ÜÁ≠ñÁï•ÊúâÊïàÁöÑÁéØÂ¢ÉËåÉÂõ¥„ÄÇÂú®Áâ©ÁêÜÊëÜÂÆûÈ™å‰∏≠ÔºåÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÂú®Ê®°ÊãüÂà∞ÁúüÂÆûËøÅÁßª‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®ÂåñÁ≥ªÁªüÁ≠âÈ¢ÜÂüüÔºåÂ∞§ÂÖ∂ÈÄÇÁî®‰∫éÈúÄË¶ÅÂø´ÈÄüÈÉ®ÁΩ≤Âà∞‰∏çÂêåÁâ©ÁêÜÂèÇÊï∞ÁéØÂ¢É‰∏≠ÁöÑÂú∫ÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Â∞Ü‰∏Ä‰∏™Âú®ÁâπÂÆöÂ∞∫ÂØ∏ÁöÑÊú∫Âô®‰∫∫‰∏äËÆ≠ÁªÉÁöÑÁ≠ñÁï•ÔºåÈõ∂Ê†∑Êú¨ËøÅÁßªÂà∞‰∏çÂêåÂ∞∫ÂØ∏ÁöÑÊú∫Âô®‰∫∫‰∏äÔºå‰ªéËÄåÈôç‰ΩéÂºÄÂèëÊàêÊú¨ÂíåÊó∂Èó¥„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Áî®‰∫éÊ®°ÊãüÂà∞ÁúüÂÆûÁöÑËøÅÁßªÔºåÊèêÈ´òÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Âú®ÁúüÂÆû‰∏ñÁïå‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) policies often fail to generalize to new robots, tasks, or environments with different physical parameters, a challenge that limits their real-world applicability. This paper presents a simple, zero-shot transfer method based on Buckingham's Pi Theorem to address this limitation. The method adapts a pre-trained policy to new system contexts by scaling its inputs (observations) and outputs (actions) through a dimensionless space, requiring no retraining. The approach is evaluated against a naive transfer baseline across three environments of increasing complexity: a simulated pendulum, a physical pendulum for sim-to-real validation, and the high-dimensional HalfCheetah. Results demonstrate that the scaled transfer exhibits no loss of performance on dynamically similar contexts. Furthermore, on non-similar contexts, the scaled policy consistently outperforms the naive transfer, significantly expanding the volume of contexts where the original policy remains effective. These findings demonstrate that dimensional analysis provides a powerful and practical tool to enhance the robustness and generalization of RL policies.

