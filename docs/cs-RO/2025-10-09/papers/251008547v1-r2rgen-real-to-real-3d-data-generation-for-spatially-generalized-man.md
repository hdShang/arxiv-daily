---
layout: default
title: R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation
---

# R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08547" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08547v1</a>
  <a href="https://arxiv.org/pdf/2510.08547.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08547v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.08547v1', 'R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiuwei Xu, Angyuan Ma, Hankun Li, Bingyao Yu, Zheng Zhu, Jie Zhou, Jiwen Lu

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Project page: https://r2rgen.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR2RGenï¼Œç”¨äºç”ŸæˆçœŸå®3Dæ•°æ®ï¼Œæå‡æœºå™¨äººç©ºé—´æ³›åŒ–æ“ä½œèƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `ç©ºé—´æ³›åŒ–` `æ•°æ®ç”Ÿæˆ` `ç‚¹äº‘å¤„ç†` `æ¨¡ä»¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œçš„ç©ºé—´æ³›åŒ–æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºæ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ä»¥åŠå¯¹å›ºå®šåœºæ™¯å’Œè§†è§’çš„é™åˆ¶ã€‚
2. R2RGené€šè¿‡ç›´æ¥å¢å¼ºçœŸå®ä¸–ç•Œçš„ç‚¹äº‘æ•°æ®ï¼Œé¿å…äº†æ¨¡æ‹Ÿå™¨å’Œæ¸²æŸ“ï¼Œä»è€Œé«˜æ•ˆåœ°ç”Ÿæˆç©ºé—´å¤šæ ·æ€§çš„è®­ç»ƒæ•°æ®ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒR2RGenæ˜¾è‘—æé«˜äº†æ•°æ®æ•ˆç‡ï¼Œå¹¶å…·å¤‡åœ¨ç§»åŠ¨æ“ä½œä¸­æ‰©å±•å’Œåº”ç”¨çš„æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å®ç°æœºå™¨äººæ“ä½œçš„æ³›åŒ–æ€§ï¼Œç©ºé—´æ³›åŒ–æ˜¯æœ€åŸºæœ¬çš„èƒ½åŠ›ï¼Œå®ƒè¦æ±‚ç­–ç•¥åœ¨ä¸åŒçš„ç‰©ä½“ç©ºé—´åˆ†å¸ƒã€ç¯å¢ƒå’Œæœºå™¨äººè‡ªèº«ä¸‹éƒ½èƒ½ç¨³å¥åœ°å·¥ä½œã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œéœ€è¦æ”¶é›†å¤§é‡çš„äººå·¥æ¼”ç¤ºï¼Œä»¥è¦†ç›–ä¸åŒçš„ç©ºé—´é…ç½®ï¼Œä»è€Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ è®­ç»ƒä¸€ä¸ªé€šç”¨çš„è§†è§‰è¿åŠ¨ç­–ç•¥ã€‚å…ˆå‰çš„å·¥ä½œæ¢ç´¢äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ï¼Œå³åˆ©ç”¨æ•°æ®ç”Ÿæˆä»æœ€å°‘çš„æºæ¼”ç¤ºä¸­è·å–ä¸°å¯Œçš„ç©ºé—´å¤šæ ·æ€§æ•°æ®ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•é¢ä¸´ç€æ˜¾è‘—çš„æ¨¡æ‹Ÿåˆ°çœŸå®å·®è·ï¼Œå¹¶ä¸”é€šå¸¸ä»…é™äºå—é™çš„è®¾ç½®ï¼Œä¾‹å¦‚å›ºå®šåŸºåº§åœºæ™¯å’Œé¢„å®šä¹‰çš„ç›¸æœºè§†è§’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªçœŸå®åˆ°çœŸå®çš„3Dæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼ˆR2RGenï¼‰ï¼Œå®ƒç›´æ¥å¢å¼ºç‚¹äº‘è§‚æµ‹-åŠ¨ä½œå¯¹ä»¥ç”ŸæˆçœŸå®ä¸–ç•Œæ•°æ®ã€‚R2RGenæ— éœ€æ¨¡æ‹Ÿå™¨å’Œæ¸²æŸ“ï¼Œå› æ­¤é«˜æ•ˆä¸”å³æ’å³ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šå•ä¸ªæºæ¼”ç¤ºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ³¨é‡Šæœºåˆ¶ï¼Œç”¨äºç»†ç²’åº¦åœ°è§£æåœºæ™¯å’Œè½¨è¿¹ã€‚æå‡ºäº†ä¸€ç§åˆ†ç»„å¢å¼ºç­–ç•¥ï¼Œä»¥å¤„ç†å¤æ‚çš„å¤šå¯¹è±¡ç»„åˆå’Œä¸åŒçš„ä»»åŠ¡çº¦æŸã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ç›¸æœºæ„ŸçŸ¥å¤„ç†ï¼Œä»¥ä½¿ç”Ÿæˆæ•°æ®çš„åˆ†å¸ƒä¸çœŸå®ä¸–ç•Œ3Dä¼ æ„Ÿå™¨å¯¹é½ã€‚ç»éªŒè¡¨æ˜ï¼ŒR2RGenåœ¨å¹¿æ³›çš„å®éªŒä¸­æ˜¾è‘—æé«˜äº†æ•°æ®æ•ˆç‡ï¼Œå¹¶å±•ç¤ºäº†åœ¨ç§»åŠ¨æ“ä½œä¸­è¿›è¡Œæ‰©å±•å’Œåº”ç”¨çš„å¼ºå¤§æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­ç©ºé—´æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®æˆ–å­˜åœ¨æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ï¼Œé™åˆ¶äº†ç­–ç•¥åœ¨çœŸå®ä¸–ç•Œå¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚è¿™äº›æ–¹æ³•é€šå¸¸åœ¨å›ºå®šåŸºåº§å’Œé¢„å®šä¹‰ç›¸æœºè§†è§’ä¸‹è¿›è¡Œï¼Œéš¾ä»¥æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šR2RGençš„æ ¸å¿ƒæ€è·¯æ˜¯ç›´æ¥åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œå¢å¼ºï¼Œé¿å…æ¨¡æ‹Ÿå™¨å¸¦æ¥çš„åå·®ã€‚é€šè¿‡å¯¹çœŸå®åœºæ™¯çš„ç‚¹äº‘æ•°æ®è¿›è¡Œç»†ç²’åº¦çš„è§£æå’Œåˆ†ç»„å¢å¼ºï¼Œç”Ÿæˆå…·æœ‰ç©ºé—´å¤šæ ·æ€§çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œæé«˜ç­–ç•¥çš„ç©ºé—´æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ— éœ€æ¸²æŸ“ï¼Œæ•ˆç‡æ›´é«˜ï¼Œæ›´æ˜“äºéƒ¨ç½²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šR2RGenæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) **æ•°æ®é‡‡é›†**ï¼šä»çœŸå®ä¸–ç•Œè·å–å°‘é‡æºæ¼”ç¤ºæ•°æ®ã€‚2) **åœºæ™¯å’Œè½¨è¿¹è§£æ**ï¼šå¯¹æºæ¼”ç¤ºæ•°æ®è¿›è¡Œç»†ç²’åº¦çš„åœºæ™¯å’Œè½¨è¿¹è§£æï¼Œæ ‡æ³¨ç‰©ä½“å’ŒåŠ¨ä½œã€‚3) **åˆ†ç»„å¢å¼º**ï¼šé‡‡ç”¨åˆ†ç»„å¢å¼ºç­–ç•¥ï¼Œå¤„ç†å¤æ‚çš„å¤šç‰©ä½“ç»„åˆå’Œä»»åŠ¡çº¦æŸï¼Œç”Ÿæˆæ–°çš„ç‚¹äº‘æ•°æ®ã€‚4) **ç›¸æœºæ„ŸçŸ¥å¤„ç†**ï¼šå¯¹ç”Ÿæˆçš„æ•°æ®è¿›è¡Œç›¸æœºæ„ŸçŸ¥å¤„ç†ï¼Œä½¿å…¶åˆ†å¸ƒä¸çœŸå®ä¸–ç•Œ3Dä¼ æ„Ÿå™¨å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šR2RGençš„å…³é”®åˆ›æ–°åœ¨äºå…¶çœŸå®åˆ°çœŸå®çš„æ•°æ®ç”Ÿæˆæ–¹å¼ï¼Œé¿å…äº†æ¨¡æ‹Ÿå™¨å¸¦æ¥çš„åå·®ã€‚æ­¤å¤–ï¼Œç»†ç²’åº¦çš„åœºæ™¯å’Œè½¨è¿¹è§£æä»¥åŠåˆ†ç»„å¢å¼ºç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„å¤šç‰©ä½“åœºæ™¯å’Œä»»åŠ¡çº¦æŸã€‚ç›¸æœºæ„ŸçŸ¥å¤„ç†è¿›ä¸€æ­¥æé«˜äº†ç”Ÿæˆæ•°æ®çš„çœŸå®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šR2RGençš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) **ç»†ç²’åº¦æ ‡æ³¨æœºåˆ¶**ï¼šç”¨äºç²¾ç¡®è§£æåœºæ™¯å’Œè½¨è¿¹ï¼Œä¸ºåç»­çš„å¢å¼ºæä¾›åŸºç¡€ã€‚2) **åˆ†ç»„å¢å¼ºç­–ç•¥**ï¼šæ ¹æ®ç‰©ä½“ä¹‹é—´çš„å…³ç³»å’Œä»»åŠ¡çº¦æŸï¼Œå¯¹ç‰©ä½“è¿›è¡Œåˆ†ç»„ï¼Œå¹¶åˆ†åˆ«è¿›è¡Œå¢å¼ºï¼Œä¿è¯ç”Ÿæˆæ•°æ®çš„åˆç†æ€§ã€‚3) **ç›¸æœºæ„ŸçŸ¥å¤„ç†**ï¼šé€šè¿‡è°ƒæ•´ç‚¹äº‘æ•°æ®çš„è§†è§’å’Œå™ªå£°ï¼Œä½¿å…¶åˆ†å¸ƒä¸çœŸå®ä¸–ç•Œ3Dä¼ æ„Ÿå™¨é‡‡é›†çš„æ•°æ®æ›´åŠ æ¥è¿‘ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­æåŠï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

R2RGené€šè¿‡çœŸå®åˆ°çœŸå®çš„æ•°æ®ç”Ÿæˆï¼Œæ˜¾è‘—æé«˜äº†æœºå™¨äººæ“ä½œçš„æ•°æ®æ•ˆç‡ã€‚æ‘˜è¦ä¸­æåˆ°ï¼ŒR2RGenåœ¨å¹¿æ³›çš„å®éªŒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†å…·ä½“çš„æ€§èƒ½æ•°æ®ã€å¯¹æ¯”åŸºçº¿å’Œæå‡å¹…åº¦ç­‰ç»†èŠ‚æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚å¯ä»¥æ¨æ–­ï¼ŒR2RGenåœ¨æ•°æ®æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

R2RGenå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæå‡å„ç§æœºå™¨äººæ“ä½œä»»åŠ¡çš„ç©ºé—´æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚ç§»åŠ¨æ“ä½œã€æŠ“å–æ”¾ç½®ã€è£…é…ç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥é™ä½å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŠ é€Ÿæœºå™¨äººç­–ç•¥çš„è®­ç»ƒå’Œéƒ¨ç½²ï¼Œå°¤å…¶é€‚ç”¨äºå¤æ‚ã€åŠ¨æ€çš„çœŸå®ä¸–ç•Œç¯å¢ƒã€‚æœªæ¥ï¼ŒR2RGenæœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation.

