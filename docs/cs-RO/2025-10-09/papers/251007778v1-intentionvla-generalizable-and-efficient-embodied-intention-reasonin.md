---
layout: default
title: IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction
---

# IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07778" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07778v1</a>
  <a href="https://arxiv.org/pdf/2510.07778.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07778v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.07778v1', 'IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yandu Chen, Kefan Gu, Yuqing Wen, Yucheng Zhao, Tiancai Wang, Liqiang Nie

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**IntentionVLAï¼šé¢å‘äººæœºäº¤äº’çš„å¯æ³›åŒ–é«˜æ•ˆå…·èº«æ„å›¾æ¨ç†æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `å…·èº«æ™ºèƒ½` `æ„å›¾æ¨ç†` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `è¯¾ç¨‹å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹ç¼ºä¹æ¨ç†å¯†é›†å‹é¢„è®­ç»ƒå’Œæ¨ç†å¼•å¯¼çš„æ“ä½œï¼Œéš¾ä»¥è¿›è¡Œå¤æ‚çœŸå®ä¸–ç•Œäº¤äº’æ‰€éœ€çš„éšå¼äººç±»æ„å›¾æ¨ç†ã€‚
2. IntentionVLAé€šè¿‡è¯¾ç¨‹å­¦ä¹ èŒƒå¼å’Œé«˜æ•ˆæ¨ç†æœºåˆ¶ï¼Œåˆ©ç”¨æ¨ç†æ•°æ®èµ‹äºˆæ¨¡å‹æ¨ç†å’Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨æ¨ç†è¾“å‡ºæŒ‡å¯¼åŠ¨ä½œç”Ÿæˆã€‚
3. å®éªŒè¡¨æ˜ï¼ŒIntentionVLAåœ¨ç›´æ¥å’Œæ„å›¾æŒ‡ä»¤ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨åˆ†å¸ƒå¤–æ„å›¾ä»»åŠ¡å’Œé›¶æ ·æœ¬äººæœºäº¤äº’ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºIntentionVLAï¼Œä¸€ä¸ªå…·æœ‰è¯¾ç¨‹å­¦ä¹ èŒƒå¼å’Œé«˜æ•ˆæ¨ç†æœºåˆ¶çš„VLAæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰VLAæ¨¡å‹åœ¨å¤æ‚çœŸå®ä¸–ç•Œäº¤äº’ä¸­è¿›è¡Œéšå¼äººç±»æ„å›¾æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚IntentionVLAé¦–å…ˆåˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„æ¨ç†æ•°æ®ï¼Œç»“åˆæ„å›¾æ¨æ–­ã€ç©ºé—´å®šä½å’Œç´§å‡‘çš„å…·èº«æ¨ç†ï¼Œèµ‹äºˆæ¨¡å‹æ¨ç†å’Œæ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨åç»­çš„å¾®è°ƒé˜¶æ®µï¼ŒIntentionVLAé‡‡ç”¨ç´§å‡‘çš„æ¨ç†è¾“å‡ºä½œä¸ºåŠ¨ä½œç”Ÿæˆçš„ä¸Šä¸‹æ–‡æŒ‡å¯¼ï¼Œä»è€Œåœ¨é—´æ¥æŒ‡ä»¤ä¸‹å®ç°å¿«é€Ÿæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIntentionVLAæ˜¾è‘—ä¼˜äº$Ï€_0$ï¼Œåœ¨ç›´æ¥æŒ‡ä»¤ä¸‹çš„æˆåŠŸç‡æé«˜äº†18ï¼…ï¼Œåœ¨æ„å›¾æŒ‡ä»¤ä¸‹æ¯”ECoTé«˜å‡º28ï¼…ã€‚åœ¨åˆ†å¸ƒå¤–çš„æ„å›¾ä»»åŠ¡ä¸Šï¼ŒIntentionVLAçš„æˆåŠŸç‡æ˜¯æ‰€æœ‰åŸºçº¿çš„ä¸¤å€ä»¥ä¸Šï¼Œå¹¶è¿›ä¸€æ­¥å®ç°äº†40ï¼…æˆåŠŸç‡çš„é›¶æ ·æœ¬äººæœºäº¤äº’ã€‚è¿™äº›ç»“æœè¡¨æ˜IntentionVLAæ˜¯ä¸‹ä¸€ä»£äººæœºäº¤äº’ç³»ç»Ÿçš„æœ‰å¸Œæœ›çš„èŒƒä¾‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰Vision-Language-Action (VLA)æ¨¡å‹ä¸»è¦åœ¨ä¸å…·èº«åœºæ™¯ç›¸å…³æ€§æœ‰é™çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå¾®è°ƒä»¥å°†æ˜¾å¼æŒ‡ä»¤æ˜ å°„åˆ°åŠ¨ä½œã€‚è¿™å¯¼è‡´æ¨¡å‹ç¼ºä¹è¶³å¤Ÿçš„æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•å¤„ç†éœ€è¦ç†è§£äººç±»éšå¼æ„å›¾çš„å¤æ‚äººæœºäº¤äº’ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºæ— æ³•æœ‰æ•ˆç»“åˆæ„ŸçŸ¥å’Œæ¨ç†ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é—´æ¥æŒ‡ä»¤æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šIntentionVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¯¾ç¨‹å­¦ä¹ çš„æ–¹å¼ï¼Œé¦–å…ˆè®©æ¨¡å‹å­¦ä¹ è¿›è¡Œæ„å›¾æ¨ç†ï¼Œç©ºé—´å®šä½å’Œç´§å‡‘çš„å…·èº«æ¨ç†ï¼Œä»è€Œèµ‹äºˆæ¨¡å‹æ¨ç†å’Œæ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶åï¼Œåœ¨å¾®è°ƒé˜¶æ®µï¼Œåˆ©ç”¨è¿™äº›æ¨ç†ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒæŒ‡å¯¼åŠ¨ä½œçš„ç”Ÿæˆã€‚è¿™æ ·ï¼Œæ¨¡å‹å°±å¯ä»¥åœ¨é—´æ¥æŒ‡ä»¤ä¸‹è¿›è¡Œå¿«é€Ÿä¸”å‡†ç¡®çš„æ¨ç†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIntentionVLAåŒ…å«é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„æ¨ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«æ„å›¾æ¨æ–­ã€ç©ºé—´å®šä½å’Œç´§å‡‘çš„å…·èº«æ¨ç†ç­‰ä»»åŠ¡ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨å…·èº«ä»»åŠ¡çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å°†é¢„è®­ç»ƒé˜¶æ®µçš„æ¨ç†ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼ŒæŒ‡å¯¼åŠ¨ä½œçš„ç”Ÿæˆã€‚æ•´ä½“æµç¨‹æ˜¯ä»è§†è§‰å’Œè¯­è¨€è¾“å…¥å¼€å§‹ï¼Œç»è¿‡æ„å›¾æ¨ç†æ¨¡å—ï¼Œç”Ÿæˆç´§å‡‘çš„æ¨ç†è¡¨ç¤ºï¼Œæœ€åé€šè¿‡åŠ¨ä½œç”Ÿæˆæ¨¡å—è¾“å‡ºæ§åˆ¶æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šIntentionVLAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è¯¾ç¨‹å­¦ä¹ èŒƒå¼å’Œé«˜æ•ˆæ¨ç†æœºåˆ¶ã€‚é€šè¿‡è¯¾ç¨‹å­¦ä¹ ï¼Œæ¨¡å‹å¯ä»¥é€æ­¥å­¦ä¹ ä»ç®€å•åˆ°å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡å°†æ¨ç†ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ„ŸçŸ¥ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒIntentionVLAæ›´æ³¨é‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†éšå¼æ„å›¾æ—¶çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šIntentionVLAçš„å…³é”®è®¾è®¡åŒ…æ‹¬æ¨ç†æ•°æ®é›†çš„è®¾è®¡å’Œæ¨ç†ç»“æœçš„è¡¨ç¤ºæ–¹å¼ã€‚æ¨ç†æ•°æ®é›†éœ€è¦åŒ…å«å„ç§æ„å›¾æ¨æ–­ã€ç©ºé—´å®šä½å’Œå…·èº«æ¨ç†çš„åœºæ™¯ï¼Œä»¥è¦†ç›–ä¸åŒçš„æ¨ç†éœ€æ±‚ã€‚æ¨ç†ç»“æœé‡‡ç”¨ç´§å‡‘çš„è¡¨ç¤ºæ–¹å¼ï¼Œä¾‹å¦‚ä½¿ç”¨ç¬¦å·æˆ–å‘é‡æ¥è¡¨ç¤ºæ„å›¾ã€ä½ç½®å’ŒåŠ¨ä½œç­‰ä¿¡æ¯ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°åŒ…æ‹¬ç”¨äºæ„å›¾åˆ†ç±»çš„äº¤å‰ç†µæŸå¤±ã€ç”¨äºç©ºé—´å®šä½çš„å›å½’æŸå¤±å’Œç”¨äºåŠ¨ä½œç”Ÿæˆçš„åºåˆ—é¢„æµ‹æŸå¤±ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨Transformerç­‰æ¨¡å‹æ¥å¤„ç†è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œå¹¶ä½¿ç”¨ä¸“é—¨çš„æ¨ç†æ¨¡å—æ¥ç”Ÿæˆæ¨ç†ç»“æœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIntentionVLAåœ¨ç›´æ¥æŒ‡ä»¤ä¸‹çš„æˆåŠŸç‡æ¯”$Ï€_0$é«˜å‡º18ï¼…ï¼Œåœ¨æ„å›¾æŒ‡ä»¤ä¸‹æ¯”ECoTé«˜å‡º28ï¼…ã€‚åœ¨åˆ†å¸ƒå¤–çš„æ„å›¾ä»»åŠ¡ä¸Šï¼ŒIntentionVLAçš„æˆåŠŸç‡æ˜¯æ‰€æœ‰åŸºçº¿çš„ä¸¤å€ä»¥ä¸Šï¼Œå¹¶å®ç°äº†40ï¼…æˆåŠŸç‡çš„é›¶æ ·æœ¬äººæœºäº¤äº’ã€‚è¿™äº›æ•°æ®è¡¨æ˜ï¼ŒIntentionVLAåœ¨å„ç§äººæœºäº¤äº’ä»»åŠ¡ä¸­éƒ½å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

IntentionVLAå¯åº”ç”¨äºå„ç§äººæœºäº¤äº’åœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—è¾…åŠ©æœºå™¨äººå’Œå·¥ä¸šåä½œæœºå™¨äººã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿä½¿æœºå™¨äººæ›´å¥½åœ°ç†è§£äººç±»çš„æ„å›¾ï¼Œä»è€Œæ›´å®‰å…¨ã€æ›´æœ‰æ•ˆåœ°ä¸äººç±»è¿›è¡Œåä½œã€‚æœªæ¥ï¼ŒIntentionVLAæœ‰æœ›æˆä¸ºä¸‹ä¸€ä»£äººæœºäº¤äº’ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ¨åŠ¨äººæœºåä½œå‘æ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„æ–¹å‘å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $Ï€_0$, achieving 18\% higher success rates with direct instructions and 28\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.

