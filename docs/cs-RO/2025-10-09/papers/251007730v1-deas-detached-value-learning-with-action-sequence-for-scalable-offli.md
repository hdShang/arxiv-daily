---
layout: default
title: DEAS: DEtached value learning with Action Sequence for Scalable Offline RL
---

# DEAS: DEtached value learning with Action Sequence for Scalable Offline RL

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.07730" target="_blank" class="toolbar-btn">arXiv: 2510.07730v1</a>
    <a href="https://arxiv.org/pdf/2510.07730.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07730v1" 
            onclick="toggleFavorite(this, '2510.07730v1', 'DEAS: DEtached value learning with Action Sequence for Scalable Offline RL')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Changyeon Kim, Haeone Lee, Younggyo Seo, Kimin Lee, Yuke Zhu

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-09

**Â§áÊ≥®**: Project website: https://changyeon.site/deas

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**DEASÔºöÂà©Áî®Âä®‰ΩúÂ∫èÂàóÂíåËß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†ÂÆûÁé∞ÂèØÊâ©Â±ïÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†` `Âä®‰ΩúÂ∫èÂàó` `‰ª∑ÂÄºÂ≠¶‰π†` `Ëß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†` `ÈïøÊó∂Â∫è‰ªªÂä°` `Êú∫Âô®‰∫∫ÊéßÂà∂` `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇ„ÄÅÈïøÊó∂Â∫èÂÜ≥Á≠ñ‰ªªÂä°Êó∂Èù¢‰∏¥ÊåëÊàòÔºåÈöæ‰ª•ÊúâÊïàÂà©Áî®Á¶ªÁ∫øÊï∞ÊçÆ„ÄÇ
2. DEASÈÄöËøáÂºïÂÖ•Âä®‰ΩúÂ∫èÂàóËøõË°å‰ª∑ÂÄºÂ≠¶‰π†ÔºåÂπ∂ÁªìÂêàËß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†Êù•ÈÅøÂÖç‰ª∑ÂÄºÈ´ò‰º∞Ôºå‰ªéËÄåÊèêÂçáÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDEASÂú®ÈïøÊó∂Â∫è‰ªªÂä°‰∏ä‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂπ∂ËÉΩÊèêÂçáËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰∏∫ËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìÊèê‰æõ‰∫Ü‰∏ÄÁßçÊó†ÈúÄÊòÇË¥µÁöÑÂú®Á∫ø‰∫§‰∫íÁöÑÊúâÊïàÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÁöÑÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑ„ÄÅÈïøÊó∂Â∫èÂ∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÊó∂‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂêç‰∏∫DEtached value learning with Action Sequence (DEAS)ÔºåÂÆÉÂà©Áî®Âä®‰ΩúÂ∫èÂàóËøõË°å‰ª∑ÂÄºÂ≠¶‰π†„ÄÇËøô‰∫õÊó∂Èó¥‰∏äÊâ©Â±ïÁöÑÂä®‰ΩúÊèê‰æõ‰∫ÜÊØîÂçïÊ≠•Âä®‰ΩúÊõ¥‰∏∞ÂØåÁöÑ‰ø°ÊÅØÔºåÂπ∂‰∏îÂèØ‰ª•ÈÄöËøáÂçäÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãQÂ≠¶‰π†ÁöÑÈÄâÈ°πÊ°ÜÊû∂ËøõË°åËß£ÈáäÔºå‰ªéËÄåÈÄöËøá‰∏ÄÊ¨°ËÄÉËôëÊõ¥ÈïøÁöÑÂ∫èÂàóÊù•ÂáèÂ∞ëÊúâÊïàÁöÑËßÑÂàíËåÉÂõ¥„ÄÇÁÑ∂ËÄåÔºåÂú®actor-criticÁÆóÊ≥ï‰∏≠Áõ¥Êé•ÈááÁî®ËøôÊ†∑ÁöÑÂ∫èÂàó‰ºöÂºïÂÖ•ËøáÂ∫¶ÁöÑ‰ª∑ÂÄºÈ´ò‰º∞ÔºåÊàë‰ª¨ÈÄöËøáËß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†Êù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåËØ•ÊñπÊ≥ïÂ∞Ü‰ª∑ÂÄº‰º∞ËÆ°ÂºïÂØºÂà∞Á¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠ÂÆûÁé∞È´òÂõûÊä•ÁöÑÂàÜÂ∏ÉÂÜÖÂä®‰Ωú„ÄÇÊàë‰ª¨ËØÅÊòé‰∫ÜDEASÂú®OGBenchÁöÑÂ§çÊùÇ„ÄÅÈïøÊó∂Â∫è‰ªªÂä°‰∏äÂßãÁªà‰ºò‰∫éÂü∫Á∫øÔºåÂπ∂‰∏îÂèØ‰ª•Â∫îÁî®‰∫éÂ¢ûÂº∫È¢ÑÊµãÂä®‰ΩúÂ∫èÂàóÁöÑÂ§ßËßÑÊ®°ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÊÄßËÉΩÔºå‰ªéËÄåÊòæËëóÊèêÈ´òRoboCasaÂé®ÊàøÊ®°Êãü‰ªªÂä°ÂíåÁúüÂÆû‰∏ñÁïåÊìç‰Ωú‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Êó®Âú®Âà©Áî®È¢ÑÂÖàÊî∂ÈõÜÂ•ΩÁöÑÈùôÊÄÅÊï∞ÊçÆÈõÜËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìÔºåÈÅøÂÖçÂú®Á∫øÊé¢Á¥¢Â∏¶Êù•ÁöÑÈ´òÊòÇÊàêÊú¨„ÄÇÁÑ∂ËÄåÔºåÂú®ÈïøÊó∂Â∫è‰ªªÂä°‰∏≠ÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂà©Áî®Á¶ªÁ∫øÊï∞ÊçÆËøõË°å‰ª∑ÂÄº‰º∞ËÆ°ÔºåÂÆπÊòì‰∫ßÁîü‰ª∑ÂÄºÈ´ò‰º∞ÈóÆÈ¢òÔºåÂØºËá¥Á≠ñÁï•ÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÔºåÂçïÊ≠•Âä®‰ΩúÁöÑ‰ª∑ÂÄº‰ø°ÊÅØ‰∏çË∂≥‰ª•ÊåáÂØºÊô∫ËÉΩ‰ΩìÂÅöÂá∫Ê≠£Á°ÆÁöÑÂÜ≥Á≠ñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDEASÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âä®‰ΩúÂ∫èÂàóÊù•Êâ©Â±ïÂä®‰ΩúÁ©∫Èó¥Ôºå‰ªéËÄåÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑ‰ª∑ÂÄº‰ø°ÊÅØ„ÄÇÈÄöËøáÂ∞ÜÂ§ö‰∏™ËøûÁª≠Âä®‰ΩúËßÜ‰∏∫‰∏Ä‰∏™‚ÄúÈÄâÈ°π‚ÄùÔºåÂèØ‰ª•ÊúâÊïàÁº©Áü≠ËßÑÂàíËßÜÈáéÔºåÂä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇÂêåÊó∂Ôºå‰∏∫‰∫ÜËß£ÂÜ≥Âä®‰ΩúÂ∫èÂàóÂ∏¶Êù•ÁöÑ‰ª∑ÂÄºÈ´ò‰º∞ÈóÆÈ¢òÔºåDEASÈááÁî®Ëß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†ÔºåÂ∞Ü‰ª∑ÂÄº‰º∞ËÆ°ÈôêÂà∂Âú®Á¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠Ë°®Áé∞ËâØÂ•ΩÁöÑÂä®‰Ωú‰∏ä„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDEASÈááÁî®Actor-CriticÊ°ÜÊû∂„ÄÇActorË¥üË¥£ÁîüÊàêÂä®‰ΩúÂ∫èÂàóÔºåCriticË¥üË¥£ËØÑ‰º∞Âä®‰ΩúÂ∫èÂàóÁöÑ‰ª∑ÂÄº„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂ¶Ç‰∏ãÔºö1) ‰ªéÁ¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠ÈááÊ†∑Áä∂ÊÄÅÔºõ2) ActorÊ†πÊçÆÂΩìÂâçÁä∂ÊÄÅÁîüÊàêÂä®‰ΩúÂ∫èÂàóÔºõ3) CriticËØÑ‰º∞ËØ•Âä®‰ΩúÂ∫èÂàóÁöÑ‰ª∑ÂÄºÔºõ4) ‰ΩøÁî®Ëß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†Êõ¥Êñ∞CriticÁΩëÁªúÔºå‰ΩøÂÖ∂Êõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞Á¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠Ë°®Áé∞ËâØÂ•ΩÁöÑÂä®‰ΩúÂ∫èÂàóÔºõ5) ‰ΩøÁî®Á≠ñÁï•Ê¢ØÂ∫¶Êõ¥Êñ∞ActorÁΩëÁªúÔºå‰ΩøÂÖ∂ÁîüÊàêÊõ¥Êúâ‰ª∑ÂÄºÁöÑÂä®‰ΩúÂ∫èÂàó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDEASÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰∏§‰∏™ÊñπÈù¢Ôºö‰∏ÄÊòØÂà©Áî®Âä®‰ΩúÂ∫èÂàóËøõË°å‰ª∑ÂÄºÂ≠¶‰π†Ôºå‰∫åÊòØÈááÁî®Ëß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†Êù•Ëß£ÂÜ≥‰ª∑ÂÄºÈ´ò‰º∞ÈóÆÈ¢ò„ÄÇÂä®‰ΩúÂ∫èÂàóËÉΩÂ§üÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂ∏ÆÂä©Êô∫ËÉΩ‰ΩìÊõ¥Â•ΩÂú∞ÁêÜËß£ÁéØÂ¢É„ÄÇËß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†ÂàôËÉΩÂ§üÊúâÊïàÂú∞ÈôêÂà∂‰ª∑ÂÄº‰º∞ËÆ°ÁöÑËåÉÂõ¥ÔºåÈÅøÂÖçËøáÂ∫¶‰πêËßÇÁöÑ‰º∞ËÆ°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDEASÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Âä®‰ΩúÂ∫èÂàóÁöÑÈïøÂ∫¶ÔºöÈúÄË¶ÅÊ†πÊçÆ‰ªªÂä°ÁöÑÂ§çÊùÇÁ®ãÂ∫¶ËøõË°åË∞ÉÊï¥ÔºåËøáÁü≠ÁöÑÂ∫èÂàóÂèØËÉΩÊó†Ê≥ïÊèê‰æõË∂≥Â§üÁöÑ‰ø°ÊÅØÔºåËøáÈïøÁöÑÂ∫èÂàóÂàô‰ºöÂ¢ûÂä†ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Ôºõ2) Ëß£ËÄ¶‰ª∑ÂÄºÂ≠¶‰π†ÁöÑÂÆûÁé∞ÊñπÂºèÔºöÂèØ‰ª•ÈÄöËøáÂºïÂÖ•È¢ùÂ§ñÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÊàñËÄÖ‰øÆÊîπ‰ª∑ÂÄºÁΩëÁªúÁöÑÁªìÊûÑÊù•ÂÆûÁé∞Ôºõ3) ActorÂíåCriticÁΩëÁªúÁöÑÁªìÊûÑÔºöÂèØ‰ª•ÈááÁî®Â∏∏ËßÅÁöÑÁ•ûÁªèÁΩëÁªúÁªìÊûÑÔºåÂ¶ÇMLPÊàñTransformer„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

DEASÂú®OGBenchÁöÑÈïøÊó∂Â∫è‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåDEASËøòÊàêÂäüÂ∫îÁî®‰∫éRoboCasaÂé®ÊàøÊ®°Êãü‰ªªÂä°ÂíåÁúüÂÆû‰∏ñÁïåÊìç‰Ωú‰ªªÂä°ÔºåÊòæËëóÊèêÂçá‰∫ÜËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDEASËÉΩÂ§üÊúâÊïàÂú∞Âà©Áî®Âä®‰ΩúÂ∫èÂàóËøõË°å‰ª∑ÂÄºÂ≠¶‰π†ÔºåÂπ∂Ëß£ÂÜ≥‰ª∑ÂÄºÈ´ò‰º∞ÈóÆÈ¢òÔºå‰ªéËÄåÊèêÂçáÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÁöÑÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DEASÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÅÊ∏∏ÊàèAI„ÄÅËá™Âä®È©æÈ©∂Á≠âÈ¢ÜÂüü„ÄÇÂú®Êú∫Âô®‰∫∫ÊéßÂà∂‰∏≠ÔºåDEASÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Â≠¶‰π†Â§çÊùÇÁöÑÂä®‰ΩúÂ∫èÂàóÔºå‰ªéËÄåÂÆåÊàêÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°„ÄÇÂú®Ê∏∏ÊàèAI‰∏≠ÔºåDEASÂèØ‰ª•ËÆ≠ÁªÉÂá∫Êõ¥Êô∫ËÉΩÁöÑAIÂØπÊâãÔºåÊèêÂçáÊ∏∏Êàè‰ΩìÈ™å„ÄÇÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåDEASÂèØ‰ª•Â∏ÆÂä©ËΩ¶ËæÜÂ≠¶‰π†Êõ¥ÂÆâÂÖ®ÁöÑÈ©æÈ©∂Á≠ñÁï•ÔºåÂáèÂ∞ë‰∫ãÊïÖÁöÑÂèëÁîü„ÄÇÊ≠§Â§ñÔºåDEASËøòÂèØ‰ª•Â∫îÁî®‰∫éÊé®ËçêÁ≥ªÁªü„ÄÅÈáëËûç‰∫§ÊòìÁ≠âÈ¢ÜÂüü„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to enhance the performance of large-scale Vision-Language-Action models that predict action sequences, significantly boosting performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks.

