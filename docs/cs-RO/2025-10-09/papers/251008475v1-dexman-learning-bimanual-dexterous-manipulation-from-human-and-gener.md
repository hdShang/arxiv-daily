---
layout: default
title: DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos
---

# DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08475" target="_blank" class="toolbar-btn">arXiv: 2510.08475v1</a>
    <a href="https://arxiv.org/pdf/2510.08475.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08475v1" 
            onclick="toggleFavorite(this, '2510.08475v1', 'DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Jhen Hsieh, Kuan-Hsun Tu, Kuo-Han Hung, Tsung-Wei Ke

**åˆ†ç±»**: cs.RO, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-09

**å¤‡æ³¨**: Video results are available at: https://embodiedai-ntu.github.io/dexman/index.html

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**DexManï¼šä»äººç±»å’Œç”Ÿæˆè§†é¢‘ä¸­å­¦ä¹ åŒæ‰‹åŠ¨çµå·§æ“ä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çµå·§æ“ä½œ` `æœºå™¨äººå­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `ç‰©ä½“å§¿æ€ä¼°è®¡` `è§†é¢‘æ¨¡ä»¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æœºå™¨äººçµå·§æ“ä½œå­¦ä¹ ä¸­ä¾èµ–ç²¾ç¡®çš„ä¼ æ„Ÿå™¨æ•°æ®å’Œæ‰‹åŠ¨æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ³›åŒ–æ€§å·®ã€‚
2. DexMané€šè¿‡ç›´æ¥ä»äººç±»è§†é¢‘ä¸­å­¦ä¹ ï¼Œç»“åˆæ¥è§¦å¥–åŠ±ï¼Œå®ç°äº†æ— éœ€ç²¾ç¡®æ•°æ®å’Œäººå·¥å¹²é¢„çš„æœºå™¨äººçµå·§æ“ä½œæŠ€èƒ½å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDexManåœ¨ç‰©ä½“å§¿æ€ä¼°è®¡å’Œæ“ä½œæˆåŠŸç‡ä¸Šå‡è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½åˆ©ç”¨åˆæˆæ•°æ®æ‰©å±•è®­ç»ƒè§„æ¨¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

DexManæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¯ä»¥å°†äººç±»è§†è§‰æ¼”ç¤ºè½¬æ¢ä¸ºäººå½¢æœºå™¨äººåœ¨ä»¿çœŸç¯å¢ƒä¸­çš„åŒæ‰‹åŠ¨çµå·§æ“ä½œæŠ€èƒ½ã€‚å®ƒç›´æ¥å¤„ç†äººç±»æ“çºµåˆšæ€§ç‰©ä½“çš„ç¬¬ä¸‰äººç§°è§†è§’è§†é¢‘ï¼Œæ— éœ€ç›¸æœºæ ¡å‡†ã€æ·±åº¦ä¼ æ„Ÿå™¨ã€æ‰«æçš„3Dç‰©ä½“èµ„äº§æˆ–çœŸå®çš„æ‰‹éƒ¨å’Œç‰©ä½“è¿åŠ¨æ ‡æ³¨ã€‚ä¸ä»…è€ƒè™‘ç®€åŒ–æµ®åŠ¨æ‰‹éƒ¨çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒDexManç›´æ¥æ§åˆ¶äººå½¢æœºå™¨äººï¼Œå¹¶åˆ©ç”¨åŸºäºæ¥è§¦çš„æ–°å‹å¥–åŠ±æ¥æ”¹è¿›ä»å˜ˆæ‚çš„æ‰‹éƒ¨-ç‰©ä½“å§¿åŠ¿ï¼ˆä»çœŸå®è§†é¢‘ä¸­ä¼°è®¡ï¼‰ä¸­è¿›è¡Œç­–ç•¥å­¦ä¹ ã€‚DexManåœ¨TACOåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç‰©ä½“å§¿åŠ¿ä¼°è®¡æ€§èƒ½ï¼ŒADD-Så’ŒVSDåˆ†åˆ«ç»å¯¹æå‡äº†0.08å’Œ0.12ã€‚åŒæ—¶ï¼Œå…¶å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨OakInk-v2ä¸Šçš„æˆåŠŸç‡è¶…è¿‡äº†ä»¥å‰çš„æ–¹æ³•19%ã€‚æ­¤å¤–ï¼ŒDexManå¯ä»¥ä»çœŸå®å’Œåˆæˆè§†é¢‘ä¸­ç”ŸæˆæŠ€èƒ½ï¼Œæ— éœ€æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ˜‚è´µçš„è¿åŠ¨æ•æ‰ï¼Œä»è€Œèƒ½å¤Ÿåˆ›å»ºå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒé€šç”¨çµå·§æ“ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººçµå·§æ“ä½œå­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºç²¾ç¡®çš„ç›¸æœºæ ‡å®šã€æ·±åº¦ä¼ æ„Ÿå™¨ã€ç‰©ä½“3Dæ¨¡å‹ä»¥åŠæ‰‹éƒ¨å’Œç‰©ä½“è¿åŠ¨çš„ç²¾ç¡®æ ‡æ³¨ã€‚è¿™äº›éœ€æ±‚é™åˆ¶äº†æ•°æ®æ”¶é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ï¼Œä½¿å¾—è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥éš¾ä»¥æ³›åŒ–åˆ°çœŸå®ä¸–ç•Œã€‚æ­¤å¤–ï¼Œä»¥å¾€æ–¹æ³•é€šå¸¸ç®€åŒ–æ‰‹éƒ¨æ¨¡å‹ï¼Œå¿½ç•¥äº†æ¥è§¦ä¿¡æ¯çš„é‡è¦æ€§ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„æ“ä½œæŠ€èƒ½ä¸å¤Ÿç²¾ç»†å’Œç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDexMançš„æ ¸å¿ƒæ€è·¯æ˜¯ä»äººç±»æ“ä½œè§†é¢‘ä¸­ç›´æ¥å­¦ä¹ ï¼Œé¿å…å¯¹ç²¾ç¡®æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯å’Œæ¥è§¦å¥–åŠ±ï¼Œå¼•å¯¼æœºå™¨äººå­¦ä¹ æ¨¡ä»¿äººç±»çš„çµå·§æ“ä½œã€‚åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨ä»¿çœŸç¯å¢ƒä¸­ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”çœŸå®ä¸–ç•Œä¸­çš„å™ªå£°å’Œä¸ç¡®å®šæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDexMançš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) **ç‰©ä½“å§¿æ€ä¼°è®¡æ¨¡å—**ï¼šä»è§†é¢‘ä¸­ä¼°è®¡æ‰‹éƒ¨å’Œç‰©ä½“çš„å§¿æ€ã€‚2) **å¥–åŠ±å‡½æ•°è®¾è®¡**ï¼šè®¾è®¡åŸºäºæ¥è§¦çš„å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æœºå™¨äººä¸ç‰©ä½“è¿›è¡Œæœ‰æ•ˆçš„äº¤äº’ã€‚3) **å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåœ¨ä»¿çœŸç¯å¢ƒä¸­ä¼˜åŒ–æœºå™¨äººçš„æ“ä½œç­–ç•¥ã€‚4) **æ•°æ®ç”Ÿæˆæ¨¡å—**ï¼šåˆ©ç”¨åˆæˆè§†é¢‘ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šDexMançš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) **ç›´æ¥ä»äººç±»è§†é¢‘ä¸­å­¦ä¹ **ï¼Œæ— éœ€ç²¾ç¡®çš„ä¼ æ„Ÿå™¨æ•°æ®å’Œäººå·¥æ ‡æ³¨ã€‚2) **åŸºäºæ¥è§¦çš„å¥–åŠ±å‡½æ•°**ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¼•å¯¼æœºå™¨äººå­¦ä¹ çµå·§æ“ä½œã€‚3) **åˆ©ç”¨åˆæˆæ•°æ®æ‰©å±•è®­ç»ƒè§„æ¨¡**ï¼Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç‰©ä½“å§¿æ€ä¼°è®¡æ¨¡å—ä¸­ï¼Œä½¿ç”¨äº†å…ˆè¿›çš„è§†è§‰ç®—æ³•ï¼Œä¾‹å¦‚TACOã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†æ‰‹éƒ¨ä¸ç‰©ä½“çš„æ¥è§¦åŠ›ã€ç›¸å¯¹ä½ç½®å’Œå§¿æ€ç­‰å› ç´ ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•é‡‡ç”¨äº†PPOç­‰å¸¸ç”¨çš„ç®—æ³•ï¼Œå¹¶é’ˆå¯¹çµå·§æ“ä½œä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚åˆæˆæ•°æ®çš„ç”Ÿæˆä½¿ç”¨äº†ç‰©ç†å¼•æ“ï¼Œæ¨¡æ‹Ÿäº†å„ç§ä¸åŒçš„æ“ä½œåœºæ™¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

DexManåœ¨TACOåŸºå‡†æµ‹è¯•ä¸­ï¼Œç‰©ä½“å§¿æ€ä¼°è®¡çš„ADD-Så’ŒVSDæŒ‡æ ‡åˆ†åˆ«æå‡äº†0.08å’Œ0.12ï¼Œè¾¾åˆ°äº†state-of-the-artæ°´å¹³ã€‚åœ¨OakInk-v2ä»»åŠ¡ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„æˆåŠŸç‡æ¯”ä¹‹å‰çš„æ–¹æ³•æé«˜äº†19%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDexManèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»äººç±»è§†é¢‘ä¸­å­¦ä¹ çµå·§æ“ä½œæŠ€èƒ½ï¼Œå¹¶åœ¨ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œä¼˜åŒ–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DexManæŠ€æœ¯å¯åº”ç”¨äºè‡ªåŠ¨åŒ–è£…é…ã€åŒ»ç–—æ‰‹æœ¯æœºå™¨äººã€å®¶åº­æœåŠ¡æœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡å­¦ä¹ äººç±»çš„çµå·§æ“ä½œæŠ€èƒ½ï¼Œæœºå™¨äººèƒ½å¤Ÿå®Œæˆæ›´åŠ å¤æ‚å’Œç²¾ç»†çš„ä»»åŠ¡ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ã€‚è¯¥æŠ€æœ¯è¿˜èƒ½ä¿ƒè¿›é€šç”¨æœºå™¨äººæ“ä½œæŠ€èƒ½çš„å‘å±•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present DexMan, an automated framework that converts human visual demonstrations into bimanual dexterous manipulation skills for humanoid robots in simulation. Operating directly on third-person videos of humans manipulating rigid objects, DexMan eliminates the need for camera calibration, depth sensors, scanned 3D object assets, or ground-truth hand and object motion annotations. Unlike prior approaches that consider only simplified floating hands, it directly controls a humanoid robot and leverages novel contact-based rewards to improve policy learning from noisy hand-object poses estimated from in-the-wild videos.
>   DexMan achieves state-of-the-art performance in object pose estimation on the TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD. Meanwhile, its reinforcement learning policy surpasses previous methods by 19% in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both real and synthetic videos, without the need for manual data collection and costly motion capture, and enabling the creation of large-scale, diverse datasets for training generalist dexterous manipulation.

