---
layout: default
title: Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments
---

# Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.15284" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.15284v1</a>
  <a href="https://arxiv.org/pdf/2511.15284.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.15284v1" onclick="toggleFavorite(this, '2511.15284v1', 'Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jonas De Maeyer, Hossein Yarahmadi, Moharram Challenger

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€ç¯å¢ƒè·¯å¾„è§„åˆ’æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è·¯å¾„è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“` `åŠ¨æ€ç¯å¢ƒ` `è”é‚¦å­¦ä¹ ` `æœºå™¨äººå¯¼èˆª` `æ™ºèƒ½äº¤é€š`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŠ¨æ€ç¯å¢ƒè·¯å¾„è§„åˆ’æ–¹æ³•éš¾ä»¥åº”å¯¹ç¯å¢ƒçš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ï¼Œå…¨å±€è§„åˆ’å™¨è®¡ç®—é‡å¤§ï¼Œéš¾ä»¥æ‰©å±•ã€‚
2. æå‡ºä¸€ç§åŒºåŸŸæ„ŸçŸ¥çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†ç¯å¢ƒåˆ†è§£ä¸ºå±€éƒ¨åŒºåŸŸï¼Œåˆ©ç”¨åˆ†å¸ƒå¼æ™ºèƒ½ä½“è¿›è¡Œå±€éƒ¨é€‚åº”ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè”é‚¦å­¦ä¹ å˜ä½“ä¼˜äºå•æ™ºèƒ½ä½“ï¼Œæ€§èƒ½æ¥è¿‘A* Oracleï¼Œå¹¶å…·æœ‰æ›´çŸ­çš„é€‚åº”æ—¶é—´å’Œè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æ™ºèƒ½äº¤é€šå’Œæœºå™¨äººé¢†åŸŸï¼ŒåŠ¨æ€ç¯å¢ƒä¸­çš„è·¯å¾„è§„åˆ’æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§çš„æŒ‘æˆ˜ï¼Œå…¶ä¸­éšœç¢ç‰©å’Œæ¡ä»¶éšæ—¶é—´å˜åŒ–ï¼Œå¼•å…¥ä¸ç¡®å®šæ€§å¹¶éœ€è¦æŒç»­é€‚åº”ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾å®Œå…¨çš„ç¯å¢ƒä¸å¯é¢„æµ‹æ€§æˆ–ä¾èµ–äºå…¨å±€è§„åˆ’å™¨ï¼Œè¿™äº›å‡è®¾é™åˆ¶äº†åœ¨å®é™…ç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§å’Œå®é™…éƒ¨ç½²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„ã€åŒºåŸŸæ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œç”¨äºåŠ¨æ€ç¯å¢ƒä¸­çš„è·¯å¾„è§„åˆ’ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºç¯å¢ƒå˜åŒ–é€šå¸¸å±€é™äºæœ‰ç•ŒåŒºåŸŸå†…çš„è§‚å¯Ÿã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¯å¢ƒçš„å±‚æ¬¡åˆ†è§£ï¼Œå¹¶éƒ¨ç½²åˆ†å¸ƒå¼RLæ™ºèƒ½ä½“ï¼Œä»¥åœ¨æœ¬åœ°é€‚åº”å˜åŒ–ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºå­ç¯å¢ƒæˆåŠŸç‡çš„é‡è®­ç»ƒæœºåˆ¶ï¼Œä»¥ç¡®å®šä½•æ—¶éœ€è¦ç­–ç•¥æ›´æ–°ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§è®­ç»ƒèŒƒå¼ï¼šå•æ™ºèƒ½ä½“Qå­¦ä¹ å’Œå¤šæ™ºèƒ½ä½“è”é‚¦Qå­¦ä¹ ï¼Œå…¶ä¸­æœ¬åœ°Qè¡¨å®šæœŸèšåˆä»¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚ä¸å…ˆå‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬åœ¨æ›´ç°å®çš„è®¾ç½®ä¸­è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå…¶ä¸­å­˜åœ¨å¤šä¸ªåŒæ—¶å‘ç”Ÿçš„éšœç¢ç‰©å˜åŒ–å’Œéš¾åº¦çº§åˆ«å¢åŠ ã€‚ç»“æœè¡¨æ˜ï¼Œè”é‚¦å˜ä½“å§‹ç»ˆä¼˜äºå…¶å•æ™ºèƒ½ä½“å¯¹åº”ç‰©ï¼Œå¹¶ä¸”åœ¨ä¿æŒæ›´çŸ­çš„é€‚åº”æ—¶é—´å’Œå¼ºå¤§çš„å¯æ‰©å±•æ€§çš„åŒæ—¶ï¼Œæ¥è¿‘A* Oracleçš„æ€§èƒ½ã€‚è™½ç„¶åˆå§‹è®­ç»ƒåœ¨å¤§ç¯å¢ƒä¸­ä»ç„¶è€—æ—¶ï¼Œä½†æˆ‘ä»¬çš„åˆ†æ•£å¼æ¡†æ¶æ¶ˆé™¤äº†å¯¹å…¨å±€è§„åˆ’å™¨çš„éœ€æ±‚ï¼Œå¹¶ä¸ºæœªæ¥ä½¿ç”¨æ·±åº¦RLå’Œçµæ´»çš„ç¯å¢ƒåˆ†è§£è¿›è¡Œæ”¹è¿›å¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³åŠ¨æ€ç¯å¢ƒä¸­è·¯å¾„è§„åˆ’é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¦‚å…¨å±€è§„åˆ’å™¨åœ¨ç¯å¢ƒå˜åŒ–é¢‘ç¹æ—¶è®¡ç®—ä»£ä»·é«˜æ˜‚ï¼Œéš¾ä»¥å®æ—¶é€‚åº”ï¼›è€ŒåŸºäºå•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°å¤æ‚ç¯å¢ƒï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¯å¢ƒè¿›è¡ŒåŒºåŸŸåˆ†è§£ï¼Œæ¯ä¸ªåŒºåŸŸéƒ¨ç½²ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæ™ºèƒ½ä½“åªå…³æ³¨å±€éƒ¨ç¯å¢ƒçš„å˜åŒ–ï¼Œä»è€Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜äº†é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è”é‚¦å­¦ä¹ çš„æ–¹å¼ï¼Œè®©å„ä¸ªæ™ºèƒ½ä½“å…±äº«å­¦ä¹ ç»éªŒï¼ŒåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ç¯å¢ƒåˆ†è§£æ¨¡å—ã€å±€éƒ¨æ™ºèƒ½ä½“å­¦ä¹ æ¨¡å—å’Œè”é‚¦å­¦ä¹ æ¨¡å—ã€‚ç¯å¢ƒåˆ†è§£æ¨¡å—å°†ç¯å¢ƒåˆ’åˆ†ä¸ºå¤šä¸ªå­åŒºåŸŸï¼›å±€éƒ¨æ™ºèƒ½ä½“å­¦ä¹ æ¨¡å—ä½¿ç”¨Q-learningç®—æ³•è®­ç»ƒæ¯ä¸ªå­åŒºåŸŸçš„æ™ºèƒ½ä½“ï¼›è”é‚¦å­¦ä¹ æ¨¡å—å®šæœŸèšåˆå„ä¸ªæ™ºèƒ½ä½“çš„Qè¡¨ï¼Œæ›´æ–°å…¨å±€ç­–ç•¥ã€‚å½“å­ç¯å¢ƒæˆåŠŸç‡ä½äºé˜ˆå€¼æ—¶ï¼Œè§¦å‘é‡è®­ç»ƒæœºåˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŒºåŸŸæ„ŸçŸ¥çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†å…¨å±€è§„åˆ’é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå±€éƒ¨è§„åˆ’é—®é¢˜ï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜äº†é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è”é‚¦å­¦ä¹ çš„æ–¹å¼ï¼ŒåŠ é€Ÿäº†å­¦ä¹ è¿‡ç¨‹ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡é‡‡ç”¨äº†Q-learningç®—æ³•ä½œä¸ºå±€éƒ¨æ™ºèƒ½ä½“çš„å­¦ä¹ ç®—æ³•ï¼Œå¥–åŠ±å‡½æ•°è®¾è®¡ä¸ºåˆ°è¾¾ç›®æ ‡å¥–åŠ±1ï¼Œç¢°æ’æƒ©ç½š-1ï¼Œå…¶ä»–æƒ…å†µä¸º-0.01ã€‚è”é‚¦å­¦ä¹ é‡‡ç”¨ç®€å•çš„å¹³å‡èšåˆæ–¹å¼ï¼Œå®šæœŸå°†å„ä¸ªæ™ºèƒ½ä½“çš„Qè¡¨è¿›è¡Œå¹³å‡ã€‚é‡è®­ç»ƒæœºåˆ¶åŸºäºå­ç¯å¢ƒæˆåŠŸç‡ï¼Œå½“æˆåŠŸç‡ä½äºé˜ˆå€¼æ—¶ï¼Œè§¦å‘é‡è®­ç»ƒã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè”é‚¦Qå­¦ä¹ å˜ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­è·¯å¾„è§„åˆ’ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºå•æ™ºèƒ½ä½“Qå­¦ä¹ ï¼Œå¹¶ä¸”æ¥è¿‘A* Oracleçš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŒæ—¶å‘ç”Ÿçš„éšœç¢ç‰©å˜åŒ–å’Œéš¾åº¦çº§åˆ«å¢åŠ çš„åœºæ™¯ä¸‹ï¼Œè”é‚¦Qå­¦ä¹ è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥åº”ç”¨äºæ›´å¤§è§„æ¨¡çš„ç¯å¢ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½äº¤é€šç³»ç»Ÿã€æœºå™¨äººå¯¼èˆªã€æ¸¸æˆAIç­‰é¢†åŸŸã€‚åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­ï¼Œå¯ä»¥ç”¨äºè½¦è¾†çš„è‡ªåŠ¨é©¾é©¶å’Œè·¯å¾„è§„åˆ’ï¼Œæé«˜äº¤é€šæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚åœ¨æœºå™¨äººå¯¼èˆªä¸­ï¼Œå¯ä»¥ç”¨äºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªå’Œé¿éšœã€‚åœ¨æ¸¸æˆAIä¸­ï¼Œå¯ä»¥ç”¨äºæ¸¸æˆè§’è‰²çš„æ™ºèƒ½è¡Œä¸ºå†³ç­–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.

