---
layout: default
title: "MTIL: Encoding Full History with Mamba for Temporal Imitation Learning"
---

# MTIL: Encoding Full History with Mamba for Temporal Imitation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12410" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.12410v3</a>
  <a href="https://arxiv.org/pdf/2505.12410.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12410v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12410v3', 'MTIL: Encoding Full History with Mamba for Temporal Imitation Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yulin Zhou, Yuankai Lin, Fanzhe Peng, Jiahui Chen, Kaiji Huang, Hua Yang, Zhouping Yin

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-18 (Êõ¥Êñ∞: 2025-10-15)

**Â§áÊ≥®**: Published in IEEE Robotics and Automation Letters (RA-L), 2025. 8 pages, 5 figures

**ÊúüÂàä**: IEEE Robotics and Automation Letters, vol. 10, no. 11, pp. 11761-11767, Nov. 2025

**DOI**: [10.1109/LRA.2025.3615520](https://doi.org/10.1109/LRA.2025.3615520)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MTIL‰ª•Ëß£ÂÜ≥ÈïøÊó∂Èó¥Â∫èÂàóÊ®°‰ªøÂ≠¶‰π†‰∏≠ÁöÑÂéÜÂè≤ÁºñÁ†ÅÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê®°‰ªøÂ≠¶‰π†` `ÈïøÊó∂Èó¥Â∫èÂàó` `Áä∂ÊÄÅÁ©∫Èó¥Ê®°Âûã` `Êú∫Âô®‰∫∫Â≠¶‰π†` `Âä®ÊÄÅÁ≥ªÁªü` `Êó∂Èó¥‰∏ä‰∏ãÊñá` `È´òÁª¥ËßÇÊµã` `ÈùûÈ©¨Â∞îÂèØÂ§´Ë°å‰∏∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÂú®ÈïøÊó∂Èó¥Â∫èÂàó‰ªªÂä°‰∏≠Èù¢‰∏¥ÂéÜÂè≤‰ø°ÊÅØÁºñÁ†Å‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÊÑüÁü•Ê®°Á≥äÊÄßÈöæ‰ª•Ëß£ÂÜ≥„ÄÇ
2. MTILÈÄöËøáÁªìÂêà‰∏ñÁïåÊ®°ÂûãÂíåÂä®ÊÄÅÁ≥ªÁªüÁöÑÊ¶ÇÂøµÔºåÂà©Áî®Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÁöÑÁ∫øÊÄßÊó∂Èó¥Âä®ÊÄÅÔºåÂ≠¶‰π†ÈöêÂºèÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåÂÖãÊúç‰∫ÜÈ©¨Â∞îÂèØÂ§´ÂÅáËÆæÁöÑÈôêÂà∂„ÄÇ
3. Âú®Ê®°ÊãüÂü∫ÂáÜÂíåÁé∞ÂÆû‰ªªÂä°‰∏≠ÁöÑÂÆûÈ™åË°®ÊòéÔºåMTILÂú®Ëß£ÂÜ≥ÈïøÊúüÊó∂Èó¥Ê®°Á≥äÊÄßÊñπÈù¢ÁöÑË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Ê†áÂáÜÁöÑÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÂú®Êú∫Âô®‰∫∫È¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóÊàêÂäüÔºå‰ΩÜÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠Â∏∏Â∏∏‰æùËµñÈ©¨Â∞îÂèØÂ§´ÂÅáËÆæÔºåÂØºËá¥Âú®Â§ÑÁêÜÂéÜÂè≤‰ø°ÊÅØÊó∂Âá∫Áé∞Âõ∞Èöæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜMamba Temporal Imitation Learning (MTIL)ÔºåÈÄöËøáÂà©Áî®Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÁöÑÁ∫øÊÄßÊó∂Èó¥ÈÄíÂΩíÂä®ÊÄÅÔºåÂ≠¶‰π†ÈöêÂºèÁöÑ„ÄÅ‰ª•Âä®‰Ωú‰∏∫ÂØºÂêëÁöÑ‰∏ñÁïåÊ®°ÂûãÔºå‰ªéËÄåÊúâÊïàÁºñÁ†ÅÊï¥‰∏™ËΩ®ËøπÂéÜÂè≤„ÄÇMTILÂú®Â§ö‰∏™Ê®°ÊãüÂü∫ÂáÜÂíåÂ§çÊùÇÁöÑÁé∞ÂÆû‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ®Èù¢Êó∂Èó¥‰∏ä‰∏ãÊñáÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïøÊó∂Èó¥Â∫èÂàóÊ®°‰ªøÂ≠¶‰π†‰∏≠ÁöÑÂéÜÂè≤‰ø°ÊÅØÁºñÁ†Å‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇTransformerÂú®Â§ÑÁêÜÈïø„ÄÅÈ´òÁª¥ËßÇÊµãÂ∫èÂàóÊó∂Èù¢‰∏¥ËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÁöÑÈôêÂà∂ÔºåÂØºËá¥Âú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠ÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMTILÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÁöÑÁ∫øÊÄßÊó∂Èó¥ÈÄíÂΩíÂä®ÊÄÅÔºåÂ≠¶‰π†‰∏Ä‰∏™ÈöêÂºèÁöÑ„ÄÅ‰ª•Âä®‰Ωú‰∏∫ÂØºÂêëÁöÑ‰∏ñÁïåÊ®°ÂûãÔºå‰ªéËÄåÊúâÊïàÂú∞ÁºñÁ†ÅÊï¥‰∏™ËΩ®ËøπÂéÜÂè≤„ÄÇËøôÁßçËÆæËÆ°‰ΩøÂæóÁ≠ñÁï•ËÉΩÂ§üÂü∫‰∫éÂÖ®Èù¢ÁöÑÊó∂Èó¥‰∏ä‰∏ãÊñáËøõË°åÊù°‰ª∂ÂåñÔºåË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑÈ©¨Â∞îÂèØÂ§´ÊñπÊ≥ï„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMTILÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Áä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÁöÑÊûÑÂª∫„ÄÅÂéÜÂè≤ËΩ®ËøπÁöÑÁºñÁ†ÅÂíåÁ≠ñÁï•ÁöÑÁîüÊàê„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÊçïÊçâÂä®ÊÄÅÁâπÂæÅÔºåÁÑ∂ÂêéÂ∞ÜÂéÜÂè≤‰ø°ÊÅØÂéãÁº©‰∏∫‰∏Ä‰∏™ÊºîÂèòÁä∂ÊÄÅÔºåÊúÄÂêéÂü∫‰∫éÊ≠§Áä∂ÊÄÅÁîüÊàêÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMTILÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂ÈÄöËøáÁ∫øÊÄßÊó∂Èó¥Âä®ÊÄÅÊúâÊïàÁºñÁ†ÅÂÆåÊï¥ÁöÑÂéÜÂè≤‰ø°ÊÅØÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüÊñπÊ≥ïÂú®ÈïøÊó∂Èó¥Â∫èÂàó‰ªªÂä°‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏ç‰ªÖÂú®ÁêÜËÆ∫‰∏äÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßíÔºå‰πüÂú®ÂÆûË∑µ‰∏≠Â±ïÁ§∫‰∫ÜÂÖ∂‰ºòË∂äÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåMTILÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÂéÜÂè≤‰ø°ÊÅØÁöÑÁºñÁ†ÅÊïàÊûúÔºåÂπ∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äËøõË°å‰∫ÜË∞ÉÊï¥Ôºå‰ª•ÈÄÇÂ∫îÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÁöÑÂä®ÊÄÅÁâπÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Â§ö‰∏™Ê®°ÊãüÂü∫ÂáÜÔºàÂ¶ÇACT„ÄÅRobomimic„ÄÅLIBEROÔºâÂíåÁé∞ÂÆû‰ªªÂä°‰∏≠ÔºåMTILÁöÑË°®Áé∞ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ¶ÇACTÂíåDiffusion PolicyÔºåÁâπÂà´ÊòØÂú®Ëß£ÂÜ≥ÈïøÊúüÊó∂Èó¥Ê®°Á≥äÊÄßÊñπÈù¢ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑËÉΩÂäõÂíåÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MTILÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊúâÊïàÂ§ÑÁêÜÈïøÊó∂Èó¥Â∫èÂàóÁöÑÂéÜÂè≤‰ø°ÊÅØÔºåMTILËÉΩÂ§üÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõÔºåËøõËÄåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑËá™‰∏ªÂ≠¶‰π†ÂíåÈÄÇÂ∫îËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Standard imitation learning (IL) methods have achieved considerable success in robotics, yet often rely on the Markov assumption, which falters in long-horizon tasks where history is crucial for resolving perceptual ambiguity. This limitation stems not only from a conceptual gap but also from a fundamental computational barrier: prevailing architectures like Transformers are often constrained by quadratic complexity, rendering the processing of long, high-dimensional observation sequences infeasible. To overcome this dual challenge, we introduce Mamba Temporal Imitation Learning (MTIL). Our approach represents a new paradigm for robotic learning, which we frame as a practical synthesis of World Model and Dynamical System concepts. By leveraging the linear-time recurrent dynamics of State Space Models (SSMs), MTIL learns an implicit, action-oriented world model that efficiently encodes the entire trajectory history into a compressed, evolving state. This allows the policy to be conditioned on a comprehensive temporal context, transcending the confines of Markovian approaches. Through extensive experiments on simulated benchmarks (ACT, Robomimic, LIBERO) and on challenging real-world tasks, MTIL demonstrates superior performance against SOTA methods like ACT and Diffusion Policy, particularly in resolving long-term temporal ambiguities. Our findings not only affirm the necessity of full temporal context but also validate MTIL as a powerful and a computationally feasible approach for learning long-horizon, non-Markovian behaviors from high-dimensional observations.

