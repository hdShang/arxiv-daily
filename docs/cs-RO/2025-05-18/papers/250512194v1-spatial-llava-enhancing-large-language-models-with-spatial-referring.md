---
layout: default
title: "Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring Expressions for Visual Understanding"
---

# Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring Expressions for Visual Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12194" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.12194v1</a>
  <a href="https://arxiv.org/pdf/2505.12194.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12194v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12194v1', 'Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring Expressions for Visual Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuefei Sun, Doncey Albin, Cecilia Mauceri, Dusty Woods, Christoffer Heckman

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSpatial-LLaVAä»¥è§£å†³è§†è§‰ç†è§£ä¸­çš„ç©ºé—´å…³ç³»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ç©ºé—´å…³ç³»` `è§†è§‰ç†è§£` `SUN-Spotæ•°æ®é›†` `è‡ªä¸»å¯¼èˆª` `äº’åŠ¨æœºå™¨äºº` `ç©ºé—´æŒ‡ç§°è¡¨è¾¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç©ºé—´å…³ç³»å’Œç‹¬ç‰¹ç‰©ä½“å®šä½ç­‰ç‰¹å®šä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†Spatial-LLaVAæ¨¡å‹ï¼Œåˆ©ç”¨SUN-Spot v2.0æ•°æ®é›†ï¼Œé€šè¿‡å¯¹è¯æ•°æ®è®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹å¯¹ç©ºé—´æŒ‡ç§°è¡¨è¾¾çš„ç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpatial-LLaVAåœ¨é›¶-shotè§†è§‰ç©ºé—´æ¨ç†åŸºå‡†æ•°æ®é›†ä¸Šæå‡äº†3.15%ï¼Œæ˜¾è‘—ä¼˜äºä»¥å¾€æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£è§†è§‰ä¸æ–‡æœ¬è¾“å…¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç‰¹å®šä»»åŠ¡æ—¶ï¼Œå¦‚ç‰©ä½“é—´çš„ç©ºé—´å…³ç³»æˆ–åœ¨ç›¸ä¼¼ç‰¹å¾ç‰©ä½“ä¸­å®šä½ç‹¬ç‰¹ç›®æ ‡ç‰©ä½“æ—¶ï¼Œå¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†SUN-Spot v2.0æ•°æ®é›†ï¼ŒåŒ…å«90kå›¾åƒ-æ–‡æœ¬å¯¹åŠåœ°æ ‡ç‰©ä½“çš„é¢å¤–æ³¨é‡Šã€‚æˆ‘ä»¬æå‡ºçš„Spatial-LLaVAæ¨¡å‹é€šè¿‡å¯¹è¯æ•°æ®è®­ç»ƒï¼Œç¡®ä¿å›¾åƒä¸­ç‰©ä½“ä¸æ–‡æœ¬ä¸­æåŠçš„ç‰©ä½“ä¹‹é—´çš„å¼ºå¯¹é½ï¼Œä»è€Œæœ‰æ•ˆå­¦ä¹ ç©ºé—´æŒ‡ç§°è¡¨è¾¾ã€‚Spatial-LLaVAåœ¨é›¶-shotè§†è§‰ç©ºé—´æ¨ç†åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œæå‡å¹…åº¦ä¸º3.15%ã€‚è¯¥æ¨¡å‹åœ¨è‡ªä¸»å¯¼èˆªå’Œäº’åŠ¨æœºå™¨äººç­‰å®é™…åœºæ™¯ä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­å¯¹ç©ºé—´å…³ç³»ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œæ— æ³•å‡†ç¡®å®šä½ç‰©ä½“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºSpatial-LLaVAï¼Œé€šè¿‡SUN-Spot v2.0æ•°æ®é›†è®­ç»ƒï¼Œåˆ©ç”¨å¯¹è¯æ•°æ®ç”Ÿæˆçš„å¼ºå¯¹é½æœºåˆ¶ï¼Œç¡®ä¿å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„ç‰©ä½“å¯¹åº”å…³ç³»ï¼Œä»è€Œæœ‰æ•ˆå­¦ä¹ ç©ºé—´æŒ‡ç§°è¡¨è¾¾ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®é›†æ„å»ºé˜¶æ®µå¼•å…¥äº†Set-of-Marksæç¤ºï¼Œæ¨¡å‹è®­ç»ƒé˜¶æ®µä½¿ç”¨å¯¹è¯æ•°æ®ï¼Œè¯„ä¼°é˜¶æ®µé€šè¿‡é›¶-shotæ–¹å¼éªŒè¯æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†Set-of-Marksæç¤ºæœºåˆ¶ï¼Œå¢å¼ºäº†å›¾åƒä¸­ç‰©ä½“ä¸æ–‡æœ¬ä¸­æåŠç‰©ä½“çš„å¯¹é½ï¼Œé¿å…äº†è¯­ä¹‰ä¿¡æ¯çš„åè§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹è®­ç»ƒä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å¯¹é½æ•ˆæœï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šæ³¨é‡å¯¹ç©ºé—´æŒ‡ç§°è¡¨è¾¾çš„å­¦ä¹ ï¼Œç¡®ä¿æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨é›¶-shotè§†è§‰ç©ºé—´æ¨ç†åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒSpatial-LLaVAæ¨¡å‹çš„æ€§èƒ½æå‡è¾¾3.15%ï¼Œæ˜¾è‘—ä¼˜äºä»¥å¾€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨ç†è§£ç©ºé—´æŒ‡ç§°è¡¨è¾¾æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Spatial-LLaVAåœ¨è‡ªä¸»å¯¼èˆªå’Œäº’åŠ¨æœºå™¨äººç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶å¯¹ç©ºé—´å…³ç³»çš„ç²¾å‡†ç†è§£èƒ½å¤Ÿæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ï¼Œå¢å¼ºäººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½åœ¨æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) have demonstrated remarkable abilities in comprehending visual input alongside text input. Typically, these models are trained on extensive data sourced from the internet, which are sufficient for general tasks such as scene understanding and question answering. However, they often underperform on specialized tasks where online data is scarce, such as determining spatial relationships between objects or localizing unique target objects within a group of objects sharing similar features. In response to this challenge, we introduce the SUN-Spot v2.0 dataset1, now comprising a total of 90k image-caption pairs and additional annotations on the landmark objects. Each image-caption pair utilizes Set-of-Marks prompting as an additional indicator, mapping each landmark object in the image to the corresponding object mentioned in the caption. Furthermore, we present Spatial-LLaVA, an MLLM trained on conversational data generated by a state-of-the-art language model using the SUNSpot v2.0 dataset. Our approach ensures a robust alignment between the objects in the images and their corresponding object mentions in the captions, enabling our model to learn spatial referring expressions without bias from the semantic information of the objects. Spatial-LLaVA outperforms previous methods by 3.15% on the zero-shot Visual Spatial Reasoning benchmark dataset. Spatial-LLaVA is specifically designed to precisely understand spatial referring expressions, making it highly applicable for tasks in real-world scenarios such as autonomous navigation and interactive robotics, where precise object recognition is critical.

