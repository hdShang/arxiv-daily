---
layout: default
title: EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models
---

# EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14666" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14666</a>
  <a href="https://arxiv.org/pdf/2512.14666.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14666" onclick="toggleFavorite(this, '2512.14666', 'EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zechen Bai, Chen Gao, Mike Zheng Shou

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºEVOLVE-VLAä»¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `è‡ªé€‚åº”å­¦ä¹ ` `ç¯å¢ƒåé¦ˆ` `æœºå™¨äººæ“ä½œ` `æŒç»­å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¾èµ–äºå¤§é‡ç¤ºèŒƒè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¯¼è‡´é€‚åº”æ€§å·®ï¼Œæ— æ³•åº”å¯¹ç¯å¢ƒå˜åŒ–ã€‚
2. EVOLVE-VLAæ¡†æ¶é€šè¿‡ç¯å¢ƒåé¦ˆè¿›è¡Œæµ‹è¯•æ—¶è®­ç»ƒï¼Œå…è®¸æ¨¡å‹åœ¨æ²¡æœ‰ä»»åŠ¡ç‰¹å®šç¤ºèŒƒçš„æƒ…å†µä¸‹è¿›è¡ŒæŒç»­å­¦ä¹ å’Œé€‚åº”ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEVOLVE-VLAåœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸Šæå‡8.6%ï¼Œåœ¨ä¸€æ¬¡å­¦ä¹ ä¸­æå‡22.0%ï¼Œå¹¶åœ¨æœªè§ä»»åŠ¡ä¸Šå®ç°20.8%çš„æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®ç°çœŸæ­£è‡ªé€‚åº”çš„å…·èº«æ™ºèƒ½éœ€è¦ä»£ç†ä¸ä»…é€šè¿‡æ¨¡ä»¿é™æ€ç¤ºèŒƒæ¥å­¦ä¹ ï¼Œè€Œæ˜¯é€šè¿‡ä¸ç¯å¢ƒçš„æŒç»­äº’åŠ¨ä¸æ–­æ”¹è¿›ã€‚å°½ç®¡è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨åŠ¨äº†æœºå™¨äººæ“ä½œçš„å‘å±•ï¼Œä½†ä»ç„¶å—åˆ°ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„é™åˆ¶ï¼Œéœ€æ•°ç™¾ä¸ªç¤ºèŒƒï¼Œä¸”åœ¨éƒ¨ç½²æ¡ä»¶åç¦»è®­ç»ƒæ—¶æ— æ³•é€‚åº”ã€‚æˆ‘ä»¬æå‡ºäº†EVOLVE-VLAï¼Œä¸€ä¸ªåœ¨æµ‹è¯•æ—¶é€šè¿‡ç¯å¢ƒäº’åŠ¨æŒç»­é€‚åº”çš„è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æœ€å°æˆ–é›¶ä»»åŠ¡ç‰¹å®šç¤ºèŒƒçš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ ã€‚å…³é”®æŠ€æœ¯æŒ‘æˆ˜æ˜¯ç”¨è‡ªä¸»åé¦ˆæ›¿ä»£ä¸å¯ç”¨çš„oracleå¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡å­¦ä¹ çš„è¿›åº¦ä¼°è®¡å™¨æä¾›å¯†é›†åé¦ˆï¼Œå¹¶è®¾è®¡äº†ä¸¤ä¸ªæœºåˆ¶æ¥â€œé©¯æœâ€è¿™ç§å›ºæœ‰çš„å™ªå£°ä¿¡å·ã€‚EVOLVE-VLAåœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸Šæå‡äº†8.6%ï¼Œåœ¨ä¸€æ¬¡å­¦ä¹ ä¸­æå‡äº†22.0%ï¼Œå¹¶å®ç°äº†è·¨ä»»åŠ¡æ³›åŒ–ï¼Œåœ¨æ²¡æœ‰ä»»åŠ¡ç‰¹å®šç¤ºèŒƒè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨æœªè§ä»»åŠ¡ä¸Šå–å¾—äº†20.8%çš„æˆåŠŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æµ‹è¯•æ—¶ç¼ºä¹é€‚åº”æ€§çš„å…·ä½“é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡çš„ç¤ºèŒƒè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¯¼è‡´åœ¨ç¯å¢ƒå˜åŒ–æ—¶æ— æ³•æœ‰æ•ˆé€‚åº”ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºçš„EVOLVE-VLAæ¡†æ¶é€šè¿‡ç¯å¢ƒåé¦ˆè¿›è¡Œæµ‹è¯•æ—¶è®­ç»ƒï¼Œå…è®¸æ¨¡å‹åœ¨æ²¡æœ‰ä»»åŠ¡ç‰¹å®šç¤ºèŒƒçš„æƒ…å†µä¸‹è¿›è¡ŒæŒç»­å­¦ä¹ ã€‚é€šè¿‡å­¦ä¹ çš„è¿›åº¦ä¼°è®¡å™¨æä¾›å¯†é›†åé¦ˆï¼Œå¹¶è®¾è®¡æœºåˆ¶æ¥å¤„ç†å™ªå£°ä¿¡å·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEVOLVE-VLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè¿›åº¦ä¼°è®¡å™¨å’Œåé¦ˆå¤„ç†æœºåˆ¶ã€‚è¿›åº¦ä¼°è®¡å™¨ç”¨äºç”Ÿæˆç¯å¢ƒåé¦ˆï¼Œè€Œåé¦ˆå¤„ç†æœºåˆ¶åˆ™é€šè¿‡å¹³æ»‘å’Œé€æ­¥æ‰©å±•ç­–ç•¥æ¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºç”¨è‡ªä¸»åé¦ˆæ›¿ä»£ä¼ ç»Ÿçš„oracleå¥–åŠ±ä¿¡å·ï¼Œå¹¶é€šè¿‡ç´¯ç§¯è¿›åº¦ä¼°è®¡å’Œå¹³æ»‘æœºåˆ¶æ¥å¤„ç†å™ªå£°ä¿¡å·ï¼Œä»è€Œå®ç°æ¨¡å‹çš„æŒç»­é€‚åº”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶æ¥å¹³æ»‘å™ªå£°ç‚¹ä¼°è®¡ï¼Œå¹¶å¼•å…¥äº†é€æ­¥æ‰©å±•ç­–ç•¥ä»¥ä¿ƒè¿›ç­–ç•¥çš„æ¸è¿›æ¼”å˜ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14666/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14666/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14666/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒEVOLVE-VLAåœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸Šæå‡äº†8.6%ï¼Œåœ¨ä¸€æ¬¡å­¦ä¹ ä¸­æå‡äº†22.0%ï¼Œå¹¶åœ¨æœªè§ä»»åŠ¡ä¸ŠæˆåŠŸç‡è¾¾åˆ°20.8%ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼ˆ0%æˆåŠŸç‡ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–åˆ¶é€ å’Œæ™ºèƒ½å®¶å±…ç­‰åœºæ™¯ã€‚é€šè¿‡å®ç°æŒç»­å­¦ä¹ å’Œé€‚åº”ï¼ŒEVOLVE-VLAèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æé«˜æœºå™¨äººæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.

