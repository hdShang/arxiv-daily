---
layout: default
title: EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models
---

# EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models

**arXiv**: [2512.14666v1](https://arxiv.org/abs/2512.14666) | [PDF](https://arxiv.org/pdf/2512.14666.pdf)

**ä½œè€…**: Zechen Bai, Chen Gao, Mike Zheng Shou

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 15 pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EVOLVE-VLAï¼šåŸºäºŽçŽ¯å¢ƒåé¦ˆçš„VLAæ¨¡åž‹æµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å…·èº«æ™ºèƒ½ (Embodied AI)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡åž‹` `æµ‹è¯•æ—¶è®­ç»ƒ` `çŽ¯å¢ƒåé¦ˆ` `æŒç»­å­¦ä¹ ` `æœºå™¨äºº` `æ³›åŒ–èƒ½åŠ›` `è¿›åº¦ä¼°è®¡` `ç­–ç•¥ä¼˜åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹ä¾èµ–å¤§é‡æ¼”ç¤ºæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæ³›åŒ–èƒ½åŠ›å—é™ï¼Œéš¾ä»¥é€‚åº”æ–°çŽ¯å¢ƒã€‚
2. EVOLVE-VLAé€šè¿‡å­¦ä¹ è¿›åº¦ä¼°è®¡å™¨æä¾›å¯†é›†åé¦ˆï¼Œå¹¶é‡‡ç”¨å¹³æ»‘å’Œæ¸è¿›å¼ç­–ç•¥æ¥é©¯æœå™ªå£°ä¿¡å·ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒEVOLVE-VLAåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ã€å•æ ·æœ¬å­¦ä¹ å’Œè·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºEVOLVE-VLAï¼Œä¸€ä¸ªæµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶ï¼Œä½¿è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹èƒ½å¤Ÿé€šè¿‡çŽ¯å¢ƒäº¤äº’æŒç»­é€‚åº”ï¼Œä¸”åªéœ€æžå°‘ç”šè‡³æ— éœ€ç‰¹å®šä»»åŠ¡çš„æ¼”ç¤ºã€‚è¯¥æ¡†æž¶æ—¨åœ¨è§£å†³VLAæ¨¡åž‹ä¾èµ–å¤§é‡æ¼”ç¤ºæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒ(SFT)çš„å±€é™æ€§ï¼Œä»¥åŠåœ¨éƒ¨ç½²æ¡ä»¶åç¦»è®­ç»ƒæ•°æ®æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºŽç”¨è‡ªä¸»åé¦ˆæ›¿ä»£æµ‹è¯•æ—¶ä¸å¯ç”¨çš„oracleå¥–åŠ±ä¿¡å·ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªå­¦ä¹ åˆ°çš„è¿›åº¦ä¼°è®¡å™¨æ¥æä¾›å¯†é›†åé¦ˆï¼Œå¹¶é€šè¿‡ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶å¹³æ»‘å™ªå£°ç‚¹ä¼°è®¡ï¼Œä»¥åŠæ¸è¿›å¼horizonæ‰©å±•ç­–ç•¥å®žçŽ°ç­–ç•¥çš„é€æ­¥æ¼”è¿›ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒEVOLVE-VLAåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸Šå–å¾—+8.6%çš„æå‡ï¼Œåœ¨å•æ ·æœ¬å­¦ä¹ ä¸Šå–å¾—+22.0%çš„æå‡ï¼Œå¹¶å®žçŽ°äº†è·¨ä»»åŠ¡æ³›åŒ–ï¼Œåœ¨æœªè§ä»»åŠ¡ä¸Šæ— éœ€ç‰¹å®šä»»åŠ¡æ¼”ç¤ºè®­ç»ƒå³å¯è¾¾åˆ°20.8%çš„æˆåŠŸçŽ‡ï¼ˆçº¯SFTä¸º0%ï¼‰ã€‚å®šæ€§åˆ†æžæ­ç¤ºäº†æ¼”ç¤ºæ•°æ®ä¸­ä¸å­˜åœ¨çš„æ¶ŒçŽ°èƒ½åŠ›ï¼ŒåŒ…æ‹¬é”™è¯¯æ¢å¤å’Œæ–°ç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œæ˜¯VLAæ¨¡åž‹ä»Žé™æ€æ¨¡ä»¿èµ°å‘æŒç»­è‡ªæˆ‘æ”¹è¿›çš„å…³é”®ä¸€æ­¥ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹ä¸»è¦ä¾èµ–äºŽç›‘ç£å¾®è°ƒ(SFT)ï¼Œéœ€è¦å¤§é‡ç‰¹å®šä»»åŠ¡çš„æ¼”ç¤ºæ•°æ®ï¼Œä¸”éš¾ä»¥é€‚åº”è®­ç»ƒæ•°æ®ä¹‹å¤–çš„æ–°çŽ¯å¢ƒã€‚è¿™äº›æ¨¡åž‹æœ¬è´¨ä¸Šæ˜¯è®°å¿†è½¨è¿¹ï¼Œç¼ºä¹çœŸæ­£çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•è®©VLAæ¨¡åž‹åœ¨å®žé™…éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¸ŽçŽ¯å¢ƒçš„äº¤äº’æŒç»­å­¦ä¹ å’Œæ”¹è¿›ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEVOLVE-VLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æµ‹è¯•æ—¶ï¼Œåˆ©ç”¨çŽ¯å¢ƒåé¦ˆè¿›è¡ŒæŒç»­è®­ç»ƒï¼Œä»Žè€Œä½¿VLAæ¨¡åž‹èƒ½å¤Ÿé€‚åº”æ–°çš„çŽ¯å¢ƒå’Œä»»åŠ¡ã€‚å…³é”®åœ¨äºŽå¦‚ä½•æ›¿ä»£æµ‹è¯•æ—¶ä¸å¯ç”¨çš„oracleå¥–åŠ±ä¿¡å·ï¼Œå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨çŽ¯å¢ƒåé¦ˆè¿›è¡Œå­¦ä¹ ã€‚é€šè¿‡å­¦ä¹ ä¸€ä¸ªè¿›åº¦ä¼°è®¡å™¨æ¥æä¾›å¯†é›†çš„åé¦ˆä¿¡å·ï¼Œå¹¶è®¾è®¡æœºåˆ¶æ¥å¤„ç†åé¦ˆä¿¡å·ä¸­çš„å™ªå£°ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šEVOLVE-VLAæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) VLAæ¨¡åž‹ï¼šä½œä¸ºåŸºç¡€ç­–ç•¥ï¼ŒæŽ¥æ”¶è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œè¾“å‡ºåŠ¨ä½œã€‚2) è¿›åº¦ä¼°è®¡å™¨ï¼šå­¦ä¹ é¢„æµ‹å½“å‰çŠ¶æ€ä¸‹ä»»åŠ¡çš„å®Œæˆè¿›åº¦ï¼Œæä¾›å¯†é›†çš„åé¦ˆä¿¡å·ã€‚3) ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶ï¼šé€šè¿‡ç´¯ç§¯ä¸€æ®µæ—¶é—´å†…çš„è¿›åº¦ä¼°è®¡å€¼ï¼Œå¹³æ»‘å™ªå£°ç‚¹ä¼°è®¡ã€‚4) æ¸è¿›å¼horizonæ‰©å±•ç­–ç•¥ï¼šé€æ­¥å¢žåŠ è®­ç»ƒæ—¶è€ƒè™‘çš„æ—¶é—´èŒƒå›´ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿé€æ­¥æ¼”è¿›ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼ŒVLAæ¨¡åž‹ä¸ŽçŽ¯å¢ƒäº¤äº’ï¼Œè¿›åº¦ä¼°è®¡å™¨è¯„ä¼°å½“å‰çŠ¶æ€ï¼Œç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶å¹³æ»‘åé¦ˆï¼Œç„¶åŽåˆ©ç”¨è¯¥åé¦ˆä¿¡å·æ›´æ–°VLAæ¨¡åž‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šEVOLVE-VLAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºŽæå‡ºäº†ä¸€ä¸ªæµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶ï¼Œä½¿VLAæ¨¡åž‹èƒ½å¤Ÿé€šè¿‡çŽ¯å¢ƒäº¤äº’è¿›è¡ŒæŒç»­å­¦ä¹ ï¼Œè€Œæ— éœ€ä¾èµ–å¤§é‡çš„ç‰¹å®šä»»åŠ¡æ¼”ç¤ºæ•°æ®ã€‚æ­¤å¤–ï¼Œé€šè¿‡å­¦ä¹ è¿›åº¦ä¼°è®¡å™¨å’Œè®¾è®¡ç›¸åº”çš„æœºåˆ¶æ¥å¤„ç†åé¦ˆä¿¡å·ä¸­çš„å™ªå£°ï¼Œæ˜¯å®žçŽ°æœ‰æ•ˆæµ‹è¯•æ—¶è®­ç»ƒçš„å…³é”®ã€‚

**å…³é”®è®¾è®¡**ï¼šè¿›åº¦ä¼°è®¡å™¨å¯ä»¥ä½¿ç”¨å„ç§å›žå½’æ¨¡åž‹ï¼Œä¾‹å¦‚ç¥žç»ç½‘ç»œã€‚ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶å¯ä»¥é€šè¿‡æ»‘åŠ¨å¹³å‡ç­‰æ–¹æ³•å®žçŽ°ã€‚æ¸è¿›å¼horizonæ‰©å±•ç­–ç•¥å¯ä»¥é€æ­¥å¢žåŠ è®­ç»ƒæ—¶è€ƒè™‘çš„æ—¶é—´æ­¥æ•°ã€‚æŸå¤±å‡½æ•°å¯ä»¥ä½¿ç”¨å‡æ–¹è¯¯å·®ç­‰å›žå½’æŸå¤±å‡½æ•°ï¼Œç”¨äºŽè®­ç»ƒVLAæ¨¡åž‹å’Œè¿›åº¦ä¼°è®¡å™¨ã€‚å…·ä½“å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

EVOLVE-VLAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸Šï¼ŒæˆåŠŸçŽ‡æå‡äº†8.6%ã€‚åœ¨å•æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼ŒæˆåŠŸçŽ‡æå‡äº†22.0%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒEVOLVE-VLAå®žçŽ°äº†è·¨ä»»åŠ¡æ³›åŒ–ï¼Œåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šï¼Œæ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„æ¼”ç¤ºæ•°æ®ï¼ŒæˆåŠŸçŽ‡è¾¾åˆ°äº†20.8%ï¼Œè€Œä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨è¯¥åœºæ™¯ä¸‹çš„æˆåŠŸçŽ‡ä¸º0%ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

EVOLVE-VLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„çŽ¯å¢ƒï¼Œå®Œæˆå„ç§ä»»åŠ¡ï¼Œå¹¶å…·å¤‡æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚è¯¥ç ”ç©¶çš„çªç ´å°†æŽ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æœåŠ¡äºŽäººç±»ç¤¾ä¼šã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.

