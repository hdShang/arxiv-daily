---
layout: default
title: EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models
---

# EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models

**arXiv**: [2512.14666v1](https://arxiv.org/abs/2512.14666) | [PDF](https://arxiv.org/pdf/2512.14666.pdf)

**ä½œè€…**: Zechen Bai, Chen Gao, Mike Zheng Shou

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 15 pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EVOLVE-VLAï¼šåŸºäºŽçŽ¯å¢ƒåé¦ˆçš„VLAæ¨¡åž‹æµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æµ‹è¯•æ—¶è®­ç»ƒ` `çŽ¯å¢ƒåé¦ˆ` `æŒç»­å­¦ä¹ ` `æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `è‡ªé€‚åº”` `æ³›åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹ä¾èµ–å¤§é‡æ¼”ç¤ºæ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥é€‚åº”éƒ¨ç½²çŽ¯å¢ƒçš„å˜åŒ–ã€‚
2. EVOLVE-VLAé€šè¿‡çŽ¯å¢ƒäº¤äº’è¿›è¡Œæµ‹è¯•æ—¶è®­ç»ƒï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„è¿›åº¦ä¼°è®¡å™¨æä¾›åé¦ˆï¼Œå®žçŽ°æŒç»­é€‚åº”ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒEVOLVE-VLAåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ã€å•æ ·æœ¬å­¦ä¹ å’Œè·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºEVOLVE-VLAï¼Œä¸€ä¸ªæµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶ï¼Œä½¿è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹èƒ½å¤Ÿé€šè¿‡çŽ¯å¢ƒäº¤äº’æŒç»­é€‚åº”ï¼Œä¸”åªéœ€æžå°‘ç”šè‡³æ— éœ€ç‰¹å®šä»»åŠ¡çš„æ¼”ç¤ºã€‚è¯¥æ¡†æž¶æ—¨åœ¨è§£å†³VLAæ¨¡åž‹ä¾èµ–å¤§é‡æ¼”ç¤ºæ•°æ®ã€è®°å¿†è½¨è¿¹ã€ä»¥åŠåœ¨éƒ¨ç½²çŽ¯å¢ƒä¸Žè®­ç»ƒçŽ¯å¢ƒä¸åŒæ—¶æ— æ³•é€‚åº”çš„é—®é¢˜ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºŽç”¨è‡ªä¸»åé¦ˆæ›¿ä»£æµ‹è¯•æ—¶ä¸å¯ç”¨çš„oracleå¥–åŠ±ä¿¡å·ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªå­¦ä¹ åˆ°çš„è¿›åº¦ä¼°è®¡å™¨æ¥æä¾›å¯†é›†åé¦ˆï¼Œå¹¶é€šè¿‡ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶å¹³æ»‘å™ªå£°ç‚¹ä¼°è®¡ï¼Œä»¥åŠæ¸è¿›å¼horizonæ‰©å±•ç­–ç•¥å®žçŽ°ç­–ç•¥çš„é€æ­¥æ¼”è¿›ã€‚å®žéªŒè¡¨æ˜Žï¼ŒEVOLVE-VLAåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸Šå–å¾—+8.6%çš„æå‡ï¼Œåœ¨å•æ ·æœ¬å­¦ä¹ ä¸Šå–å¾—+22.0%çš„æå‡ï¼Œå¹¶å®žçŽ°äº†è·¨ä»»åŠ¡æ³›åŒ–ï¼Œåœ¨æœªè§ä»»åŠ¡ä¸Šæ— éœ€ç‰¹å®šä»»åŠ¡æ¼”ç¤ºè®­ç»ƒå³å¯è¾¾åˆ°20.8%çš„æˆåŠŸçŽ‡ï¼ˆçº¯SFTä¸º0%ï¼‰ã€‚å®šæ€§åˆ†æžæ­ç¤ºäº†æ¼”ç¤ºæ•°æ®ä¸­ä¸å­˜åœ¨çš„é”™è¯¯æ¢å¤å’Œæ–°ç­–ç•¥ç­‰æ¶ŒçŽ°èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæ˜¯VLAæ¨¡åž‹ä»Žé™æ€æ¨¡ä»¿èµ°å‘æŒç»­è‡ªæˆ‘æ”¹è¿›çš„å…³é”®ä¸€æ­¥ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹ä¸»è¦ä¾èµ–äºŽç›‘ç£å¾®è°ƒ(SFT)ï¼Œéœ€è¦å¤§é‡ç‰¹å®šä»»åŠ¡çš„æ¼”ç¤ºæ•°æ®ï¼Œå¹¶ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´åœ¨éƒ¨ç½²çŽ¯å¢ƒä¸­ï¼Œç‰¹åˆ«æ˜¯å½“çŽ¯å¢ƒä¸Žè®­ç»ƒçŽ¯å¢ƒå­˜åœ¨å·®å¼‚æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡åž‹éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡ä¸Šï¼Œç¼ºä¹çœŸæ­£çš„é€‚åº”æ€§å’Œè‡ªä¸»å­¦ä¹ èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¦‚ä½•ä½¿VLAæ¨¡åž‹èƒ½å¤Ÿåœ¨å®žé™…çŽ¯å¢ƒä¸­æŒç»­å­¦ä¹ å’Œæ”¹è¿›ï¼Œæ‘†è„±å¯¹å¤§é‡æ¼”ç¤ºæ•°æ®çš„ä¾èµ–ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEVOLVE-VLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æµ‹è¯•æ—¶åˆ©ç”¨çŽ¯å¢ƒåé¦ˆè¿›è¡ŒæŒç»­è®­ç»ƒã€‚ç”±äºŽåœ¨å®žé™…éƒ¨ç½²çŽ¯å¢ƒä¸­ï¼Œé€šå¸¸æ— æ³•èŽ·å¾—oracleå¥–åŠ±ä¿¡å·ï¼Œå› æ­¤éœ€è¦è®¾è®¡ä¸€ç§è‡ªä¸»çš„åé¦ˆæœºåˆ¶ã€‚è®ºæ–‡é€šè¿‡å­¦ä¹ ä¸€ä¸ªè¿›åº¦ä¼°è®¡å™¨æ¥æä¾›å¯†é›†çš„åé¦ˆä¿¡å·ï¼Œå¹¶é‡‡ç”¨ç´¯ç§¯ä¼°è®¡å’Œæ¸è¿›å¼horizonæ‰©å±•ç­–ç•¥æ¥å¤„ç†åé¦ˆä¿¡å·ä¸­çš„å™ªå£°ï¼Œä»Žè€Œå®žçŽ°ç­–ç•¥çš„ç¨³å®šæ¼”è¿›ã€‚è¿™ç§æ–¹æ³•å…è®¸VLAæ¨¡åž‹åœ¨ä¸ŽçŽ¯å¢ƒäº¤äº’çš„è¿‡ç¨‹ä¸­ä¸æ–­ä¼˜åŒ–è‡ªèº«ç­–ç•¥ï¼Œæé«˜é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šEVOLVE-VLAæ¡†æž¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) VLAæ¨¡åž‹ï¼šä½œä¸ºåŸºç¡€ç­–ç•¥æ¨¡åž‹ï¼Œè´Ÿè´£æ ¹æ®è§†è§‰å’Œè¯­è¨€è¾“å…¥ç”ŸæˆåŠ¨ä½œï¼›2) è¿›åº¦ä¼°è®¡å™¨ï¼šå­¦ä¹ é¢„æµ‹å½“å‰çŠ¶æ€ä¸‹ä»»åŠ¡çš„å®Œæˆè¿›åº¦ï¼Œæä¾›å¯†é›†çš„åé¦ˆä¿¡å·ï¼›3) ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶ï¼šé€šè¿‡å¯¹ä¸€æ®µæ—¶é—´å†…çš„è¿›åº¦ä¼°è®¡è¿›è¡Œç´¯ç§¯ï¼Œå¹³æ»‘å™ªå£°ï¼Œæé«˜åé¦ˆä¿¡å·çš„å¯é æ€§ï¼›4) æ¸è¿›å¼horizonæ‰©å±•ç­–ç•¥ï¼šé€æ­¥å¢žåŠ è®­ç»ƒæ—¶è€ƒè™‘çš„æ—¶é—´æ­¥é•¿ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ æ›´é•¿æœŸçš„ä¾èµ–å…³ç³»ï¼›5) ç­–ç•¥ä¼˜åŒ–å™¨ï¼šæ ¹æ®ç´¯ç§¯çš„è¿›åº¦ä¼°è®¡ä¿¡å·ï¼Œæ›´æ–°VLAæ¨¡åž‹çš„å‚æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šEVOLVE-VLAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºŽæå‡ºäº†ä¸€ä¸ªåŸºäºŽçŽ¯å¢ƒåé¦ˆçš„æµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶ï¼Œè¯¥æ¡†æž¶æ— éœ€ä¾èµ–oracleå¥–åŠ±ä¿¡å·ï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ åˆ°çš„è¿›åº¦ä¼°è®¡å™¨æä¾›è‡ªä¸»åé¦ˆã€‚æ­¤å¤–ï¼Œç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶å’Œæ¸è¿›å¼horizonæ‰©å±•ç­–ç•¥æœ‰æ•ˆåœ°è§£å†³äº†åé¦ˆä¿¡å·ä¸­çš„å™ªå£°é—®é¢˜ï¼Œä¿è¯äº†ç­–ç•¥çš„ç¨³å®šæ¼”è¿›ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—VLAæ¨¡åž‹èƒ½å¤Ÿåœ¨å®žé™…çŽ¯å¢ƒä¸­æŒç»­å­¦ä¹ å’Œæ”¹è¿›ï¼Œæ‘†è„±äº†å¯¹å¤§é‡æ¼”ç¤ºæ•°æ®çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè¿›åº¦ä¼°è®¡å™¨é€šå¸¸é‡‡ç”¨ç¥žç»ç½‘ç»œç»“æž„ï¼Œè¾“å…¥ä¸ºå½“å‰çŠ¶æ€çš„è§†è§‰ä¿¡æ¯å’Œä»»åŠ¡æè¿°ï¼Œè¾“å‡ºä¸ºä»»åŠ¡å®Œæˆçš„è¿›åº¦ä¼°è®¡å€¼ã€‚ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶å¯ä»¥é€šè¿‡æ»‘åŠ¨å¹³å‡æˆ–æŒ‡æ•°åŠ æƒå¹³å‡ç­‰æ–¹æ³•å®žçŽ°ï¼Œç”¨äºŽå¹³æ»‘å™ªå£°ã€‚æ¸è¿›å¼horizonæ‰©å±•ç­–ç•¥å¯ä»¥é‡‡ç”¨çº¿æ€§æˆ–æŒ‡æ•°æ–¹å¼å¢žåŠ è®­ç»ƒæ—¶è€ƒè™‘çš„æ—¶é—´æ­¥é•¿ã€‚ç­–ç•¥ä¼˜åŒ–å™¨å¯ä»¥ä½¿ç”¨å¸¸è§çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¦‚PPOæˆ–SACï¼Œæ ¹æ®ç´¯ç§¯çš„è¿›åº¦ä¼°è®¡ä¿¡å·æ›´æ–°VLAæ¨¡åž‹çš„å‚æ•°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒEVOLVE-VLAåœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸Šå–å¾—äº†8.6%çš„æ€§èƒ½æå‡ï¼Œåœ¨å•æ ·æœ¬å­¦ä¹ ä¸Šå–å¾—äº†22.0%çš„æ€§èƒ½æå‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒEVOLVE-VLAå®žçŽ°äº†è·¨ä»»åŠ¡æ³›åŒ–ï¼Œåœ¨æœªè§ä»»åŠ¡ä¸Šæ— éœ€ç‰¹å®šä»»åŠ¡æ¼”ç¤ºè®­ç»ƒå³å¯è¾¾åˆ°20.8%çš„æˆåŠŸçŽ‡ï¼Œè€Œçº¯SFTæ–¹æ³•åœ¨è¯¥åœºæ™¯ä¸‹çš„æˆåŠŸçŽ‡ä¸º0%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒEVOLVE-VLAèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜VLAæ¨¡åž‹çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

EVOLVE-VLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ä½¿æœºå™¨äººåœ¨å®žé™…çŽ¯å¢ƒä¸­ä¸æ–­å­¦ä¹ å’Œæ”¹è¿›ï¼Œé€‚åº”ä¸åŒçš„ä»»åŠ¡å’ŒçŽ¯å¢ƒå˜åŒ–ï¼Œä»Žè€Œæé«˜æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œå·¥ä½œæ•ˆçŽ‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºŽè™šæ‹ŸçŽ¯å¢ƒä¸­çš„æ™ºèƒ½ä½“è®­ç»ƒï¼ŒåŠ é€Ÿæ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.

