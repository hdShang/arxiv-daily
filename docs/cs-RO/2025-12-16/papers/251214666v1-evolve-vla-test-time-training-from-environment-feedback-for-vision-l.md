---
layout: default
title: EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models
---

# EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models

**arXiv**: [2512.14666v1](https://arxiv.org/abs/2512.14666) | [PDF](https://arxiv.org/pdf/2512.14666.pdf)

**ä½œè€…**: Zechen Bai, Chen Gao, Mike Zheng Shou

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 15 pages

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**EVOLVE-VLAï¼šé¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹çš„çŽ¯å¢ƒåé¦ˆæµ‹è¯•æ—¶è®­ç»ƒ**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æœºå™¨äººæ“ä½œä¸Žçµå·§æ‰‹ (Manipulation)** **å…·èº«æ™ºèƒ½ä¸Žè¡¨å¾å­¦ä¹  (Embodied AI & Representation)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹` `æµ‹è¯•æ—¶è®­ç»ƒ` `çŽ¯å¢ƒåé¦ˆ` `æœºå™¨äººæ“ä½œ` `æŒç»­å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰VLAæ¨¡åž‹ä¾èµ–å¤§é‡ä»»åŠ¡æ¼”ç¤ºè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæ³›åŒ–æ€§å’Œé€‚åº”æ€§ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹çœŸå®žçŽ¯å¢ƒå˜åŒ–ã€‚
2. EVOLVE-VLAæå‡ºä¸€ç§æµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶ï¼Œé€šè¿‡çŽ¯å¢ƒäº¤äº’å’Œè‡ªä¸»åé¦ˆï¼Œä½¿VLAæ¨¡åž‹æŒç»­é€‚åº”æ–°ä»»åŠ¡ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒEVOLVE-VLAåœ¨é•¿horizonä»»åŠ¡ã€one-shotå­¦ä¹ å’Œè·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å®žçŽ°çœŸæ­£è‡ªé€‚åº”çš„å…·èº«æ™ºèƒ½ï¼Œæ™ºèƒ½ä½“ä¸ä»…éœ€è¦é€šè¿‡æ¨¡ä»¿é™æ€æ¼”ç¤ºæ¥å­¦ä¹ ï¼Œè¿˜éœ€è¦é€šè¿‡çŽ¯å¢ƒäº¤äº’ä¸æ–­æ”¹è¿›ï¼Œè¿™ç±»ä¼¼äºŽäººç±»é€šè¿‡å®žè·µæŽŒæ¡æŠ€èƒ½ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹é€šè¿‡åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹æŽ¨åŠ¨äº†æœºå™¨äººæ“ä½œçš„å‘å±•ï¼Œä½†ä»ç„¶å—åˆ°ç›‘ç£å¾®è°ƒ(SFT)çš„æ ¹æœ¬é™åˆ¶ï¼šæ¯ä¸ªä»»åŠ¡éœ€è¦æ•°ç™¾ä¸ªæ¼”ç¤ºï¼Œåˆšæ€§åœ°è®°å¿†è½¨è¿¹ï¼Œå¹¶ä¸”åœ¨éƒ¨ç½²æ¡ä»¶åç¦»è®­ç»ƒæ—¶æ— æ³•é€‚åº”ã€‚æˆ‘ä»¬å¼•å…¥äº†EVOLVE-VLAï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶ï¼Œä½¿VLAèƒ½å¤Ÿé€šè¿‡çŽ¯å¢ƒäº¤äº’æŒç»­é€‚åº”ï¼Œè€Œåªéœ€æžå°‘æˆ–é›¶ä»»åŠ¡ç‰¹å®šæ¼”ç¤ºã€‚å…³é”®çš„æŠ€æœ¯æŒ‘æˆ˜æ˜¯ç”¨è‡ªä¸»åé¦ˆå–ä»£oracleå¥–åŠ±ä¿¡å·(æµ‹è¯•æ—¶ä¸å¯ç”¨)ã€‚æˆ‘ä»¬é€šè¿‡å­¦ä¹ åˆ°çš„è¿›åº¦ä¼°è®¡å™¨æä¾›å¯†é›†åé¦ˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è®¾è®¡æˆ‘ä»¬çš„æ¡†æž¶é€šè¿‡ä¸¤ç§æœºåˆ¶æ¥â€œé©¯æœâ€è¿™ç§å›ºæœ‰çš„å™ªå£°ä¿¡å·ï¼š(1)ä¸€ç§ç´¯ç§¯çš„è¿›åº¦ä¼°è®¡æœºåˆ¶ï¼Œç”¨äºŽå¹³æ»‘å™ªå£°çš„ç‚¹ä¼°è®¡ï¼Œä»¥åŠ(2)ä¸€ç§æ¸è¿›çš„horizonæ‰©å±•ç­–ç•¥ï¼Œç”¨äºŽå®žçŽ°é€æ­¥çš„ç­–ç•¥æ¼”åŒ–ã€‚EVOLVE-VLAå–å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šï¼šåœ¨é•¿horizonä»»åŠ¡ä¸Š+8.6%ï¼Œåœ¨one-shotå­¦ä¹ ä¸­+22.0%ï¼Œå¹¶å®žçŽ°äº†è·¨ä»»åŠ¡æ³›åŒ–â€”â€”åœ¨æ²¡æœ‰ä»»åŠ¡ç‰¹å®šæ¼”ç¤ºè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šå®žçŽ°äº†20.8%çš„æˆåŠŸçŽ‡(è€Œçº¯SFTä¸º0%)ã€‚å®šæ€§åˆ†æžæ­ç¤ºäº†æ¼”ç¤ºä¸­ä¸å­˜åœ¨çš„æ–°å…´èƒ½åŠ›ï¼ŒåŒ…æ‹¬é”™è¯¯æ¢å¤å’Œæ–°é¢–çš„ç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œä»£è¡¨äº†æœç€çœŸæ­£å­¦ä¹ å’Œé€‚åº”çš„VLAè¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ï¼Œä»Žé™æ€æ¨¡ä»¿èµ°å‘æŒç»­çš„è‡ªæˆ‘æ”¹è¿›ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡åž‹ä¸»è¦ä¾èµ–äºŽç›‘ç£å¾®è°ƒ(SFT)ï¼Œéœ€è¦å¤§é‡çš„ä»»åŠ¡ç‰¹å®šæ¼”ç¤ºæ•°æ®ã€‚è¿™ç§æ–¹æ³•å­˜åœ¨æ³›åŒ–èƒ½åŠ›å·®ã€éš¾ä»¥é€‚åº”æ–°çŽ¯å¢ƒå’Œä»»åŠ¡çš„ç¼ºç‚¹ã€‚å½“éƒ¨ç½²çŽ¯å¢ƒä¸Žè®­ç»ƒçŽ¯å¢ƒå­˜åœ¨å·®å¼‚æ—¶ï¼Œæ¨¡åž‹çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œå¹¶ä¸”éš¾ä»¥è¿›è¡ŒæŒç»­å­¦ä¹ å’Œæ”¹è¿›ã€‚å› æ­¤ï¼Œå¦‚ä½•ä½¿VLAæ¨¡åž‹åœ¨å®žé™…éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¸ŽçŽ¯å¢ƒçš„äº¤äº’è¿›è¡Œè‡ªæˆ‘æ”¹è¿›ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šEVOLVE-VLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨æµ‹è¯•æ—¶ï¼Œé€šè¿‡ä¸ŽçŽ¯å¢ƒçš„äº¤äº’ï¼Œåˆ©ç”¨è‡ªä¸»ç”Ÿæˆçš„åé¦ˆä¿¡å·æ¥æŒç»­è®­ç»ƒå’Œæ”¹è¿›VLAæ¨¡åž‹ã€‚è¯¥æ–¹æ³•é¿å…äº†å¯¹å¤§é‡ä»»åŠ¡ç‰¹å®šæ¼”ç¤ºæ•°æ®çš„ä¾èµ–ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿé€‚åº”æ–°çš„çŽ¯å¢ƒå’Œä»»åŠ¡ã€‚å…³é”®åœ¨äºŽè®¾è®¡ä¸€ç§æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶ï¼Œæ›¿ä»£åœ¨æµ‹è¯•æ—¶æ— æ³•èŽ·å¾—çš„oracleå¥–åŠ±ä¿¡å·ï¼Œå¹¶è§£å†³åé¦ˆä¿¡å·ä¸­å­˜åœ¨çš„å™ªå£°é—®é¢˜ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šEVOLVE-VLAçš„æ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) VLAæ¨¡åž‹ï¼šä½œä¸ºæ™ºèƒ½ä½“çš„ç­–ç•¥ç½‘ç»œï¼ŒæŽ¥æ”¶è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œè¾“å‡ºåŠ¨ä½œæŒ‡ä»¤ã€‚2) è¿›åº¦ä¼°è®¡å™¨ï¼šç”¨äºŽè¯„ä¼°æ™ºèƒ½ä½“åœ¨ä»»åŠ¡ä¸­çš„è¿›å±•ç¨‹åº¦ï¼Œç”Ÿæˆå¯†é›†çš„åé¦ˆä¿¡å·ã€‚3) ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶ï¼šå¹³æ»‘è¿›åº¦ä¼°è®¡å™¨äº§ç”Ÿçš„å™ªå£°åé¦ˆã€‚4) æ¸è¿›horizonæ‰©å±•ç­–ç•¥ï¼šé€æ­¥å¢žåŠ è®­ç»ƒçš„horizoné•¿åº¦ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿé€æ­¥æ¼”åŒ–ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼šæ™ºèƒ½ä½“ä¸ŽçŽ¯å¢ƒäº¤äº’ï¼Œè¿›åº¦ä¼°è®¡å™¨ç”Ÿæˆåé¦ˆä¿¡å·ï¼Œåé¦ˆä¿¡å·ç»è¿‡å¹³æ»‘å¤„ç†ï¼Œç”¨äºŽæ›´æ–°VLAæ¨¡åž‹çš„å‚æ•°ï¼Œä»Žè€Œæ”¹è¿›ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šEVOLVE-VLAæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽæå‡ºäº†ä¸€ä¸ªæµ‹è¯•æ—¶è®­ç»ƒæ¡†æž¶ï¼Œä½¿VLAæ¨¡åž‹èƒ½å¤Ÿé€šè¿‡ä¸ŽçŽ¯å¢ƒçš„äº¤äº’è¿›è¡ŒæŒç»­å­¦ä¹ å’Œæ”¹è¿›ã€‚è¯¥æ¡†æž¶çš„å…³é”®åœ¨äºŽä½¿ç”¨å­¦ä¹ åˆ°çš„è¿›åº¦ä¼°è®¡å™¨æ¥æä¾›å¯†é›†çš„åé¦ˆä¿¡å·ï¼Œæ›¿ä»£äº†åœ¨æµ‹è¯•æ—¶æ— æ³•èŽ·å¾—çš„oracleå¥–åŠ±ä¿¡å·ã€‚æ­¤å¤–ï¼Œç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶å’Œæ¸è¿›horizonæ‰©å±•ç­–ç•¥æœ‰æ•ˆåœ°è§£å†³äº†åé¦ˆä¿¡å·ä¸­å­˜åœ¨çš„å™ªå£°é—®é¢˜ï¼Œä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè¿›åº¦ä¼°è®¡å™¨ä½¿ç”¨ç¥žç»ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œè¾“å…¥æ˜¯æ™ºèƒ½ä½“çš„çŠ¶æ€å’Œç›®æ ‡æè¿°ï¼Œè¾“å‡ºæ˜¯æ™ºèƒ½ä½“åœ¨ä»»åŠ¡ä¸­çš„è¿›å±•ç¨‹åº¦ã€‚ç´¯ç§¯è¿›åº¦ä¼°è®¡æœºåˆ¶é€šè¿‡å¯¹ä¸€æ®µæ—¶é—´å†…çš„è¿›åº¦ä¼°è®¡å€¼è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œæ¥å¹³æ»‘å™ªå£°ã€‚æ¸è¿›horizonæ‰©å±•ç­–ç•¥ä»Žè¾ƒçŸ­çš„horizonå¼€å§‹ï¼Œé€æ­¥å¢žåŠ horizonçš„é•¿åº¦ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿé€æ­¥é€‚åº”æ›´å¤æ‚çš„ä»»åŠ¡ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘è¿›åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œç­–ç•¥çš„ç¨³å®šæ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

EVOLVE-VLAåœ¨å¤šä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨é•¿horizonä»»åŠ¡ä¸Šå–å¾—äº†8.6%çš„æ€§èƒ½æå‡ï¼Œåœ¨one-shotå­¦ä¹ ä¸­å–å¾—äº†22.0%çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šå®žçŽ°äº†20.8%çš„æˆåŠŸçŽ‡ï¼Œè€Œçº¯SFTæ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šçš„æˆåŠŸçŽ‡ä¸º0%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼ŒEVOLVE-VLAèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜VLAæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

EVOLVE-VLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºŽå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥ä½¿æœºå™¨äººåœ¨å®žé™…éƒ¨ç½²çŽ¯å¢ƒä¸­ï¼Œé€šè¿‡ä¸ŽçŽ¯å¢ƒçš„äº¤äº’è¿›è¡Œè‡ªæˆ‘æ”¹è¿›ï¼Œä»Žè€Œæé«˜å…¶é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºŽå¼€å‘æ›´æ™ºèƒ½çš„è‡ªä¸»å¯¼èˆªç³»ç»Ÿå’Œæ¸¸æˆAIã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.

