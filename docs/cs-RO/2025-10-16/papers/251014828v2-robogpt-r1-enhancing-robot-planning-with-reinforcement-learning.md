---
layout: default
title: RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning
---

# RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14828" target="_blank" class="toolbar-btn">arXiv: 2510.14828v2</a>
    <a href="https://arxiv.org/pdf/2510.14828.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14828v2" 
            onclick="toggleFavorite(this, '2510.14828v2', 'RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li

**ÂàÜÁ±ª**: cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16 (Êõ¥Êñ∞: 2025-10-22)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RoboGPT-R1ÔºöÂº∫ÂåñÂ≠¶‰π†Â¢ûÂº∫Êú∫Âô®‰∫∫ËßÑÂàíËÉΩÂäõÔºåÊèêÂçáÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°ÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫ËßÑÂàí` `Âº∫ÂåñÂ≠¶‰π†` `ÂÖ∑Ë∫´Êô∫ËÉΩ` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `ÈïøÊó∂Á®ãÊìç‰Ωú` `ÁõëÁù£ÂæÆË∞É` `Â•ñÂä±ÂáΩÊï∞`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÁõëÁù£ÂæÆË∞ÉÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÊâßË°åÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°Êó∂ÔºåÁî±‰∫éÂ∏∏ËØÜÂíåÊé®ÁêÜËÉΩÂäõÁöÑÈôêÂà∂ÔºåÈù¢‰∏¥ÊåëÊàò„ÄÇ
2. RoboGPT-R1ÈááÁî®‰∏§Èò∂ÊÆµÂæÆË∞ÉÊ°ÜÊû∂ÔºåÂÖàÈÄöËøáÁõëÁù£Â≠¶‰π†Ëé∑ÂèñÁü•ËØÜÔºåÂÜçÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáËßÜËßâÁ©∫Èó¥ÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõÔºåÂº•Ë°•ÁõëÁù£Â≠¶‰π†ÁöÑ‰∏çË∂≥„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂú®Qwen2.5-VL-3B‰∏äËÆ≠ÁªÉÁöÑRoboGPT-R1ÔºåÂú®EmbodiedBench‰∏äÊòæËëó‰ºò‰∫éGPT-4o-miniÂíåÂü∫‰∫éQwen2.5-VL-7BËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

‰∏∫‰∫ÜÊèêÈ´òÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂú®ÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°‰∏≠ÂÆåÊàêÂ§çÊùÇ‰∫∫Á±ªÊåá‰ª§ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜRoboGPT-R1Ôºå‰∏Ä‰∏™Áî®‰∫éÂÖ∑Ë∫´ËßÑÂàíÁöÑ‰∏§Èò∂ÊÆµÂæÆË∞ÉÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂È¶ñÂÖàÈÄöËøáÁõëÁù£ËÆ≠ÁªÉ‰ªé‰∏ìÂÆ∂Â∫èÂàó‰∏≠Ëé∑ÂèñÂü∫Á°ÄÁü•ËØÜÔºåÁÑ∂ÂêéÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Ëß£ÂÜ≥Ê®°ÂûãÂú®ËßÜËßâÁ©∫Èó¥ÁêÜËß£ÂíåÊé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜÂú®Â§öÊ≠•Êé®ÁêÜ‰ªªÂä°‰∏≠ÂÆûÁé∞Áâ©ÁêÜÁêÜËß£ÂíåÂä®‰ΩúÂ∫èÂàó‰∏ÄËá¥ÊÄßÔºåËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÂêåÊó∂ËÄÉËôë‰∫ÜÈïøÊó∂Á®ãÊÄßËÉΩÂíåÁéØÂ¢É‰∏≠ÁöÑÂä®‰ΩúÁ∫¶Êùü„ÄÇÂú®Qwen2.5-VL-3B‰∏äËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãÔºåÂú®EmbodiedBenchÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÊòæËëó‰ºò‰∫éÊõ¥Â§ßËßÑÊ®°ÁöÑÊ®°ÂûãGPT-4o-mini 21.33%ÔºåÂπ∂‰∏îË∂ÖËøá‰∫ÜÂú®Qwen2.5-VL-7B‰∏äËÆ≠ÁªÉÁöÑÂÖ∂‰ªñÂ∑•‰Ωú 20.33%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊñπÊ≥ïÂú®ÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÁöÑÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°‰∏≠ÔºåÁî±‰∫éÁº∫‰πèË∂≥Â§üÁöÑÂ∏∏ËØÜÂíåÊé®ÁêÜËÉΩÂäõÔºåÈöæ‰ª•ÂæàÂ•ΩÂú∞ÁêÜËß£ÁéØÂ¢ÉÂπ∂ËßÑÂàíÂá∫ÂêàÁêÜÁöÑÂä®‰ΩúÂ∫èÂàóÔºåÂØºËá¥‰ªªÂä°ÂÆåÊàêÊïàÊûú‰∏ç‰Ω≥„ÄÇÁõëÁù£ÂæÆË∞ÉËôΩÁÑ∂ÂèØ‰ª•Â≠¶‰π†Âà∞‰∏Ä‰∫õÁü•ËØÜÔºå‰ΩÜÊ≥õÂåñËÉΩÂäõÂíåÁâ©ÁêÜÁêÜËß£ËÉΩÂäõ‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöRoboGPT-R1ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁªìÂêàÁõëÁù£Â≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰ºòÂäø„ÄÇÈ¶ñÂÖàÈÄöËøáÁõëÁù£Â≠¶‰π†ËÆ©Ê®°ÂûãÂ≠¶‰π†Âà∞Âü∫Á°ÄÁü•ËØÜÔºåÁÑ∂ÂêéÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Êù•Âº•Ë°•Ê®°ÂûãÂú®ËßÜËßâÁ©∫Èó¥ÁêÜËß£ÂíåÊé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËßÑÂàíËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRoboGPT-R1ÊòØ‰∏Ä‰∏™‰∏§Èò∂ÊÆµÁöÑÂæÆË∞ÉÊ°ÜÊû∂„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÊòØÁõëÁù£Â≠¶‰π†Ôºå‰ΩøÁî®‰∏ìÂÆ∂Â∫èÂàóÂØπÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂Â≠¶‰π†Âà∞Âü∫Á°ÄÁü•ËØÜ„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÊòØÂº∫ÂåñÂ≠¶‰π†Ôºå‰ΩøÁî®Âü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±ÂáΩÊï∞Êù•ËÆ≠ÁªÉÊ®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÁéØÂ¢ÉÂπ∂ËßÑÂàíÂá∫ÂêàÁêÜÁöÑÂä®‰ΩúÂ∫èÂàó„ÄÇÂ•ñÂä±ÂáΩÊï∞ÂêåÊó∂ËÄÉËôë‰∫ÜÈïøÊó∂Á®ãÊÄßËÉΩÂíåÁéØÂ¢É‰∏≠ÁöÑÂä®‰ΩúÁ∫¶Êùü„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRoboGPT-R1ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÁªìÂêà‰∫ÜÁõëÁù£Â≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±ÂáΩÊï∞„ÄÇËøôÁßçÁªìÂêàÂèØ‰ª•ÊúâÊïàÂú∞ÊèêÈ´òÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËßÑÂàíËÉΩÂäõ„ÄÇÂü∫‰∫éËßÑÂàôÁöÑÂ•ñÂä±ÂáΩÊï∞ËÉΩÂ§üÂêåÊó∂ËÄÉËôëÈïøÊó∂Á®ãÊÄßËÉΩÂíåÁéØÂ¢É‰∏≠ÁöÑÂä®‰ΩúÁ∫¶ÊùüÔºå‰ªéËÄå‰øùËØÅ‰∫ÜÂä®‰ΩúÂ∫èÂàóÁöÑÂêàÁêÜÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆ„ÄÇÂÆÉÁî±‰∏§ÈÉ®ÂàÜÁªÑÊàêÔºö‰∏ÄÈÉ®ÂàÜÊòØÈïøÊó∂Á®ãÊÄßËÉΩÂ•ñÂä±ÔºåÁî®‰∫éÈºìÂä±Ê®°ÂûãÂÆåÊàê‰ªªÂä°ÔºõÂè¶‰∏ÄÈÉ®ÂàÜÊòØÂä®‰ΩúÁ∫¶ÊùüÂ•ñÂä±ÔºåÁî®‰∫éÊÉ©ÁΩöËøùÂèçÁéØÂ¢ÉÁ∫¶ÊùüÁöÑÂä®‰Ωú„ÄÇÂÖ∑‰ΩìËßÑÂàôÂíåÊùÉÈáçËÆæÁΩÆÊú™Áü•Ôºå‰ΩÜÁõÆÊ†áÊòØÂºïÂØºÊ®°ÂûãÂ≠¶‰π†Âà∞Êó¢ËÉΩÂÆåÊàê‰ªªÂä°ÂèàËÉΩÈÅµÂÆàÁéØÂ¢ÉËßÑÂàôÁöÑÂä®‰ΩúÂ∫èÂàó„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

RoboGPT-R1Âú®EmbodiedBenchÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÊØîGPT-4o-miniÁöÑÊÄßËÉΩÈ´òÂá∫21.33%ÔºåÂπ∂‰∏îÊØîÂú®Qwen2.5-VL-7B‰∏äËÆ≠ÁªÉÁöÑÂÖ∂‰ªñÂ∑•‰ΩúÈ´òÂá∫20.33%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåRoboGPT-R1ËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËßÑÂàíËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

RoboGPT-R1ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÊú∫Âô®‰∫∫ËøõË°åÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°„ÄÅÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂåªÁñóËæÖÂä©Á≠â„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫ÁöÑÊé®ÁêÜÂíåËßÑÂàíËÉΩÂäõÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£‰∫∫Á±ªÊåá‰ª§ÔºåÂπ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆåÊàê‰ªªÂä°Ôºå‰ªéËÄåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÂíåÁîüÊ¥ªË¥®Èáè„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑÁéØÂ¢ÉÂíå‰ªªÂä°‰∏≠„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model's shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark.

