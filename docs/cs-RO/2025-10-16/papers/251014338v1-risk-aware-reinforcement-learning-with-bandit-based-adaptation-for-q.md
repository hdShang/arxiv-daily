---
layout: default
title: Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion
---

# Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14338" target="_blank" class="toolbar-btn">arXiv: 2510.14338v1</a>
    <a href="https://arxiv.org/pdf/2510.14338.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14338v1" 
            onclick="toggleFavorite(this, '2510.14338v1', 'Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion')" title="æ”¶è—">
      â˜† æ”¶è—
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="å¤åˆ¶é“¾æ¥">
      ğŸ”— åˆ†äº«
    </button>
  </div>
</div>


**ä½œè€…**: Yuanhong Zeng, Anushri Dixit

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºBanditè‡ªé€‚åº”çš„é£é™©æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œæå‡å››è¶³æœºå™¨äººè¿åŠ¨é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `é£é™©æ„ŸçŸ¥` `å››è¶³æœºå™¨äºº` `è¿åŠ¨æ§åˆ¶` `å¤šè‡‚è€è™æœº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶ä¸­ï¼Œéš¾ä»¥å…¼é¡¾ç¨³å®šæ€§å’Œé€‚åº”æœªçŸ¥ç¯å¢ƒã€‚
2. æå‡ºåŸºäºCVaRçº¦æŸçš„é£é™©æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œè®­ç»ƒä¸€ç³»åˆ—ä¸åŒé£é™©æ°´å¹³çš„ç­–ç•¥ï¼Œæå‡é²æ£’æ€§ã€‚
3. ä½¿ç”¨å¤šè‡‚è€è™æœºåœ¨çº¿è‡ªé€‚åº”é€‰æ‹©æœ€ä¼˜ç­–ç•¥ï¼Œæ— éœ€ç¯å¢ƒä¿¡æ¯ï¼Œå¿«é€Ÿé€‚åº”æœªçŸ¥åœ°å½¢ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†å››è¶³æœºå™¨äººè¿åŠ¨ä¸­çš„é£é™©æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬ä½¿ç”¨æ¡ä»¶é£é™©ä»·å€¼(CVaR)çº¦æŸçš„ç­–ç•¥ä¼˜åŒ–æŠ€æœ¯è®­ç»ƒäº†ä¸€ç³»åˆ—é£é™©æ¡ä»¶ç­–ç•¥ï¼Œä»è€Œæé«˜äº†ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚åœ¨éƒ¨ç½²æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šè‡‚è€è™æœºæ¡†æ¶è‡ªé€‚åº”åœ°ä»ç­–ç•¥æ—ä¸­é€‰æ‹©æ€§èƒ½æœ€ä½³çš„ç­–ç•¥ï¼Œè¯¥æ¡†æ¶ä»…ä½¿ç”¨è§‚å¯Ÿåˆ°çš„æƒ…èŠ‚å›æŠ¥ï¼Œæ— éœ€ä»»ä½•ç‰¹æƒç¯å¢ƒä¿¡æ¯ï¼Œå¹¶èƒ½åŠ¨æ€é€‚åº”æœªçŸ¥çš„æ¡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨CVaRè®­ç»ƒäº†å„ç§é²æ£’æ€§æ°´å¹³çš„å››è¶³è¿åŠ¨ç­–ç•¥ï¼Œå¹¶è‡ªé€‚åº”åœ°åœ¨çº¿é€‰æ‹©æ‰€éœ€çš„é²æ£’æ€§æ°´å¹³ï¼Œä»¥ç¡®ä¿åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿä¸­è·¨è¶Šå…«ä¸ªæœªè§è¿‡çš„è®¾ç½®ï¼ˆé€šè¿‡æ”¹å˜åŠ¨åŠ›å­¦ã€æ¥è§¦ã€ä¼ æ„Ÿå™ªå£°å’Œåœ°å½¢ï¼‰ä»¥åŠåœ¨Unitree Go2æœºå™¨äººä¸Šåœ¨ä»¥å‰æœªè§è¿‡çš„åœ°å½¢ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ä¸å…¶ä»–åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„é£é™©æ„ŸçŸ¥ç­–ç•¥åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­è·å¾—äº†è¿‘ä¸¤å€çš„å¹³å‡å’Œå°¾éƒ¨æ€§èƒ½ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„åŸºäºbanditçš„è‡ªé€‚åº”åœ¨è¿è¡Œä¸¤åˆ†é’Ÿå†…åœ¨æœªçŸ¥åœ°å½¢ä¸­é€‰æ‹©äº†æ€§èƒ½æœ€ä½³çš„é£é™©æ„ŸçŸ¥ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶ä¸­ï¼Œéš¾ä»¥åœ¨ä¿è¯ç¨³å®šæ€§çš„åŒæ—¶é€‚åº”æœªçŸ¥çš„å¤æ‚ç¯å¢ƒã€‚å°¤å…¶æ˜¯åœ¨åŠ¨åŠ›å­¦ç‰¹æ€§å˜åŒ–ã€å­˜åœ¨å™ªå£°å¹²æ‰°æˆ–åœ°å½¢å´å²–çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œç”šè‡³å¯¼è‡´æœºå™¨äººæ‘”å€’æˆ–æ— æ³•å®Œæˆä»»åŠ¡ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨æœªçŸ¥ç¯å¢ƒä¸­æå‡å››è¶³æœºå™¨äººçš„è¿åŠ¨é²æ£’æ€§æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®­ç»ƒä¸€ç³»åˆ—å…·æœ‰ä¸åŒé£é™©åå¥½çš„è¿åŠ¨ç­–ç•¥ï¼Œå¹¶åœ¨å®é™…éƒ¨ç½²æ—¶ï¼Œæ ¹æ®ç¯å¢ƒåé¦ˆè‡ªé€‚åº”åœ°é€‰æ‹©æœ€åˆé€‚çš„ç­–ç•¥ã€‚é€šè¿‡æ¡ä»¶é£é™©ä»·å€¼ï¼ˆCVaRï¼‰çº¦æŸçš„ç­–ç•¥ä¼˜åŒ–ï¼Œå¯ä»¥æ§åˆ¶ç­–ç•¥çš„é£é™©æ°´å¹³ï¼Œä»è€Œè®­ç»ƒå‡ºæ—¢èƒ½ä¿è¯å¹³å‡æ€§èƒ½ï¼Œåˆèƒ½é¿å…æç«¯æƒ…å†µçš„ç­–ç•¥ã€‚åœ¨çº¿è‡ªé€‚åº”é€‰æ‹©åˆ™åˆ©ç”¨å¤šè‡‚è€è™æœºç®—æ³•ï¼Œæ ¹æ®å®é™…å›æŠ¥åŠ¨æ€è°ƒæ•´ç­–ç•¥é€‰æ‹©ï¼Œæ— éœ€é¢„å…ˆäº†è§£ç¯å¢ƒä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¦»çº¿è®­ç»ƒé˜¶æ®µå’Œåœ¨çº¿è‡ªé€‚åº”é˜¶æ®µã€‚åœ¨ç¦»çº¿è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨CVaRçº¦æŸçš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œè®­ç»ƒä¸€ç³»åˆ—å…·æœ‰ä¸åŒé£é™©åå¥½çš„ç­–ç•¥ã€‚è¿™äº›ç­–ç•¥æ„æˆä¸€ä¸ªç­–ç•¥æ—ï¼Œæ¯ä¸ªç­–ç•¥å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„é£é™©æ°´å¹³ã€‚åœ¨åœ¨çº¿è‡ªé€‚åº”é˜¶æ®µï¼Œä½¿ç”¨å¤šè‡‚è€è™æœºç®—æ³•ï¼Œæ ¹æ®å®é™…ç¯å¢ƒä¸­çš„å›æŠ¥ï¼ŒåŠ¨æ€é€‰æ‹©ç­–ç•¥æ—ä¸­çš„ç­–ç•¥ã€‚å¤šè‡‚è€è™æœºç®—æ³•æ ¹æ®æ¯ä¸ªç­–ç•¥çš„å†å²å›æŠ¥ï¼Œä¼°è®¡å…¶æœŸæœ›å›æŠ¥ï¼Œå¹¶é€‰æ‹©æœŸæœ›å›æŠ¥æœ€é«˜çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†é£é™©æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ä¸åœ¨çº¿è‡ªé€‚åº”é€‰æ‹©ç›¸ç»“åˆã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸åªè®­ç»ƒä¸€ä¸ªç­–ç•¥ï¼Œéš¾ä»¥é€‚åº”æœªçŸ¥ç¯å¢ƒã€‚è€Œæœ¬æ–‡çš„æ–¹æ³•é€šè¿‡è®­ç»ƒä¸€ç³»åˆ—ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨å¤šè‡‚è€è™æœºç®—æ³•è¿›è¡Œåœ¨çº¿é€‰æ‹©ï¼Œä»è€Œå®ç°äº†åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„è‡ªé€‚åº”è¿åŠ¨æ§åˆ¶ã€‚æ­¤å¤–ï¼Œä½¿ç”¨CVaRçº¦æŸçš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶ç­–ç•¥çš„é£é™©æ°´å¹³ï¼Œä»è€Œæé«˜æœºå™¨äººçš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCVaRçº¦æŸçš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•é€šè¿‡åœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥CVaRçº¦æŸï¼Œé™åˆ¶ç­–ç•¥çš„é£é™©æ°´å¹³ã€‚CVaRæ˜¯ä¸€ç§é£é™©åº¦é‡æŒ‡æ ‡ï¼Œè¡¨ç¤ºåœ¨ä¸€å®šç½®ä¿¡æ°´å¹³ä¸‹ï¼ŒæŸå¤±çš„æœŸæœ›å€¼ã€‚é€šè¿‡è°ƒæ•´CVaRçš„ç½®ä¿¡æ°´å¹³ï¼Œå¯ä»¥æ§åˆ¶ç­–ç•¥çš„é£é™©åå¥½ã€‚å¤šè‡‚è€è™æœºç®—æ³•ä½¿ç”¨UCBï¼ˆUpper Confidence Boundï¼‰ç®—æ³•è¿›è¡Œç­–ç•¥é€‰æ‹©ã€‚UCBç®—æ³•æ ¹æ®æ¯ä¸ªç­–ç•¥çš„å†å²å›æŠ¥ï¼Œä¼°è®¡å…¶æœŸæœ›å›æŠ¥ï¼Œå¹¶é€‰æ‹©æœŸæœ›å›æŠ¥åŠ ä¸Šä¸€ä¸ªç½®ä¿¡ä¸Šç•Œçš„ç­–ç•¥ã€‚ç½®ä¿¡ä¸Šç•Œåæ˜ äº†å¯¹ç­–ç•¥å›æŠ¥ä¼°è®¡çš„ä¸ç¡®å®šæ€§ï¼Œé¼“åŠ±ç®—æ³•æ¢ç´¢æœªçŸ¥çš„ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å…«ä¸ªæœªè§è¿‡çš„è®¾ç½®ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬æ”¹å˜åŠ¨åŠ›å­¦ã€æ¥è§¦ã€ä¼ æ„Ÿå™ªå£°å’Œåœ°å½¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„é£é™©æ„ŸçŸ¥ç­–ç•¥åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­è·å¾—äº†è¿‘ä¸¤å€çš„å¹³å‡å’Œå°¾éƒ¨æ€§èƒ½ã€‚åœ¨Unitree Go2æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„åŸºäºbanditçš„è‡ªé€‚åº”èƒ½å¤Ÿåœ¨è¿è¡Œä¸¤åˆ†é’Ÿå†…åœ¨æœªçŸ¥åœ°å½¢ä¸­é€‰æ‹©æ€§èƒ½æœ€ä½³çš„é£é™©æ„ŸçŸ¥ç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§å¤æ‚ç¯å¢ƒä¸‹çš„å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶ï¼Œä¾‹å¦‚æœæ•‘ã€å‹˜æ¢ã€ç‰©æµç­‰é¢†åŸŸã€‚é€šè¿‡è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥ï¼Œæœºå™¨äººèƒ½å¤Ÿåœ¨æœªçŸ¥åœ°å½¢ã€å­˜åœ¨å¹²æ‰°æˆ–åŠ¨åŠ›å­¦ç‰¹æ€§å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œä¿æŒç¨³å®šå’Œé«˜æ•ˆçš„è¿åŠ¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººå’Œæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this work, we study risk-aware reinforcement learning for quadrupedal locomotion. Our approach trains a family of risk-conditioned policies using a Conditional Value-at-Risk (CVaR) constrained policy optimization technique that provides improved stability and sample efficiency. At deployment, we adaptively select the best performing policy from the family of policies using a multi-armed bandit framework that uses only observed episodic returns, without any privileged environment information, and adapts to unknown conditions on the fly. Hence, we train quadrupedal locomotion policies at various levels of robustness using CVaR and adaptively select the desired level of robustness online to ensure performance in unknown environments. We evaluate our method in simulation across eight unseen settings (by changing dynamics, contacts, sensing noise, and terrain) and on a Unitree Go2 robot in previously unseen terrains. Our risk-aware policy attains nearly twice the mean and tail performance in unseen environments compared to other baselines and our bandit-based adaptation selects the best-performing risk-aware policy in unknown terrain within two minutes of operation.

