---
layout: default
title: VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation
---

# VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14902" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14902v1</a>
  <a href="https://arxiv.org/pdf/2510.14902.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14902v1" onclick="toggleFavorite(this, '2510.14902v1', 'VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VLA^2ï¼šåˆ©ç”¨Agentæ¡†æ¶å¢å¼ºVLAæ¨¡å‹å¤„ç†æœªè§æ¦‚å¿µæ“ä½œçš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ“ä½œ` `æ³›åŒ–èƒ½åŠ›` `Agentæ¡†æ¶` `çŸ¥è¯†æ£€ç´¢` `å¯¹è±¡æ£€æµ‹` `åˆ†å¸ƒå¤–æ³›åŒ–` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨å¤„ç†æœªè§è¿‡çš„ç‰©ä½“æ¦‚å¿µæ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ“ä½œä»»åŠ¡æˆåŠŸç‡æ˜¾è‘—ä¸‹é™ã€‚
2. VLA^2æ¡†æ¶åˆ©ç”¨OpenVLAä½œä¸ºéª¨å¹²ï¼Œç»“åˆç½‘ç»œæ£€ç´¢å’Œå¯¹è±¡æ£€æµ‹ç­‰å¤–éƒ¨æ¨¡å—ï¼Œå¢å¼ºå¯¹æœªçŸ¥æ¦‚å¿µçš„ç†è§£ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒVLA^2åœ¨å›°éš¾çº§åˆ«çš„æ³›åŒ–åŸºå‡†ä¸Šï¼Œç›¸æ¯”OpenVLAåŸºçº¿æˆåŠŸç‡æå‡äº†44.2%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨å¤§å‹æœºå™¨äººæ•°æ®é›†ä¸Šé¢„è®­ç»ƒåï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¤šä»»åŠ¡èƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°æ“ä½œä»»åŠ¡ä¸­è§†è§‰å’Œè¯­è¨€æŒ‡ä»¤çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹è®­ç»ƒæ•°æ®ä¹‹å¤–çš„å¯¹è±¡æ¦‚å¿µæ—¶ï¼Œä¾‹å¦‚æ•°æ®é›†ä¸­æœªè§è¿‡çš„å¯¹è±¡æè¿°å’Œçº¹ç†ï¼Œå®ƒä»¬çš„æˆåŠŸç‡ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„Agentæ¡†æ¶VLA^2ï¼Œå®ƒåˆ©ç”¨OpenVLAä½œä¸ºæ‰§è¡Œéª¨å¹²ï¼Œå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨è¯¸å¦‚ç½‘ç»œæ£€ç´¢å’Œå¯¹è±¡æ£€æµ‹ç­‰å¤–éƒ¨æ¨¡å—ï¼Œä¸ºVLAæä¾›å…³äºç›®æ ‡å¯¹è±¡çš„è§†è§‰å’Œæ–‡æœ¬çŸ¥è¯†ã€‚è¿™ç§æ–¹æ³•å‡è½»äº†å¤„ç†åˆ†å¸ƒå¤–å¯¹è±¡æ—¶çš„æ³›åŒ–å¤±è´¥ã€‚åŸºäºLIBEROæ¨¡æ‹Ÿç¯å¢ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„å¯¹è±¡å’Œå¯¹è±¡æè¿°ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«ä¸‰ä¸ªéš¾åº¦çº§åˆ«ï¼Œä»¥æµ‹è¯•æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸåœ°è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æˆ‘ä»¬è®¾è®¡çš„å›°éš¾çº§åˆ«æ³›åŒ–åŸºå‡†ä¸Šã€‚ä¸ç‹¬ç«‹çš„OpenVLAåŸºçº¿ç›¸æ¯”ï¼ŒVLA^2åœ¨å›°éš¾çº§åˆ«åŸºå‡†ä¸Šçš„æˆåŠŸç‡æé«˜äº†44.2%ï¼Œåœ¨æ‰€æœ‰å®šåˆ¶ç¯å¢ƒä¸­å¹³å‡æé«˜äº†20.2%ï¼Œè€Œæ²¡æœ‰é™ä½åœ¨åŸŸå†…ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰VLAæ¨¡å‹åœ¨å¤„ç†è®­ç»ƒé›†ä¸­æœªè§è¿‡çš„ç‰©ä½“æ¦‚å¿µï¼ˆä¾‹å¦‚æ–°çš„ç‰©ä½“æè¿°ã€çº¹ç†ç­‰ï¼‰æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œå¯¼è‡´æ“ä½œä»»åŠ¡çš„æˆåŠŸç‡é™ä½ã€‚è¿™æ˜¯å› ä¸ºVLAæ¨¡å‹ä¾èµ–äºé¢„è®­ç»ƒæ•°æ®ä¸­çš„çŸ¥è¯†ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†åˆ†å¸ƒå¤–çš„ç‰©ä½“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVLA^2çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªAgentæ¡†æ¶ï¼Œé€šè¿‡é›†æˆå¤–éƒ¨çŸ¥è¯†æºï¼ˆå¦‚ç½‘ç»œæ£€ç´¢å’Œå¯¹è±¡æ£€æµ‹ï¼‰ï¼Œæ¥å¢å¼ºVLAæ¨¡å‹å¯¹æœªçŸ¥ç‰©ä½“æ¦‚å¿µçš„ç†è§£å’Œå¤„ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å…è®¸VLAæ¨¡å‹åœ¨é‡åˆ°æœªçŸ¥ç‰©ä½“æ—¶ï¼Œä¸»åŠ¨æŸ¥è¯¢ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå¼¥è¡¥é¢„è®­ç»ƒçŸ¥è¯†çš„ä¸è¶³ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVLA^2æ¡†æ¶ä»¥OpenVLAä½œä¸ºæ‰§è¡Œéª¨å¹²ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š1) **OpenVLA**ï¼šè´Ÿè´£æ‰§è¡Œæ“ä½œä»»åŠ¡ï¼›2) **Web Retrieval Module**ï¼šç”¨äºä»äº’è”ç½‘æ£€ç´¢å…³äºç›®æ ‡ç‰©ä½“çš„æ–‡æœ¬ä¿¡æ¯ï¼›3) **Object Detection Module**ï¼šç”¨äºæ£€æµ‹å›¾åƒä¸­çš„ç›®æ ‡ç‰©ä½“ï¼Œæä¾›è§†è§‰ä¿¡æ¯ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šå½“OpenVLAé‡åˆ°æœªçŸ¥ç‰©ä½“æ—¶ï¼ŒAgentæ¡†æ¶ä¼šè°ƒç”¨Web Retrieval Moduleå’ŒObject Detection Moduleè·å–ç›¸å…³çŸ¥è¯†ï¼Œç„¶åå°†è¿™äº›çŸ¥è¯†èå…¥åˆ°VLAæ¨¡å‹çš„è¾“å…¥ä¸­ï¼Œä»è€ŒæŒ‡å¯¼VLAæ¨¡å‹æ‰§è¡Œæ“ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šVLA^2çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸€ä¸ªAgentæ¡†æ¶ï¼Œå°†VLAæ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†æºè¿æ¥èµ·æ¥ï¼Œä½¿å…¶å…·å¤‡äº†ä¸»åŠ¨å­¦ä¹ å’Œé€‚åº”æœªçŸ¥ç¯å¢ƒçš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„VLAæ¨¡å‹ç›¸æ¯”ï¼ŒVLA^2ä¸å†ä»…ä»…ä¾èµ–äºé¢„è®­ç»ƒæ•°æ®ä¸­çš„çŸ¥è¯†ï¼Œè€Œæ˜¯èƒ½å¤Ÿé€šè¿‡æŸ¥è¯¢å¤–éƒ¨ä¿¡æ¯æ¥æ‰©å±•å…¶çŸ¥è¯†åº“ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†åˆ†å¸ƒå¤–çš„ç‰©ä½“ã€‚

**å…³é”®è®¾è®¡**ï¼šVLA^2çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) å¦‚ä½•æœ‰æ•ˆåœ°å°†ä»Web Retrieval Moduleå’ŒObject Detection Moduleè·å–çš„çŸ¥è¯†èå…¥åˆ°VLAæ¨¡å‹çš„è¾“å…¥ä¸­ï¼›2) å¦‚ä½•è®¾è®¡Agentæ¡†æ¶çš„å†³ç­–æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®å½“å‰ä»»åŠ¡çš„éœ€æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„å¤–éƒ¨æ¨¡å—è¿›è¡ŒæŸ¥è¯¢ï¼›3) å¦‚ä½•å¹³è¡¡å¤–éƒ¨çŸ¥è¯†çš„åˆ©ç”¨å’ŒVLAæ¨¡å‹è‡ªèº«çš„æ¨ç†èƒ½åŠ›ï¼Œé¿å…è¿‡åº¦ä¾èµ–å¤–éƒ¨ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VLA^2åœ¨LIBEROæ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨å›°éš¾çº§åˆ«çš„æ³›åŒ–åŸºå‡†ä¸Šï¼ŒVLA^2çš„æˆåŠŸç‡æ¯”OpenVLAåŸºçº¿æé«˜äº†44.2%ã€‚åœ¨æ‰€æœ‰å®šåˆ¶ç¯å¢ƒä¸­ï¼ŒVLA^2çš„å¹³å‡æˆåŠŸç‡æé«˜äº†20.2%ï¼ŒåŒæ—¶æ²¡æœ‰é™ä½åœ¨åŸŸå†…ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVLA^2èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜VLAæ¨¡å‹åœ¨å¤„ç†æœªçŸ¥ç‰©ä½“æ¦‚å¿µæ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VLA^2å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©æœºå™¨äººç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œäººç±»æŒ‡ä»¤ï¼Œå³ä½¿é¢å¯¹æœªçŸ¥çš„ç‰©ä½“å’Œç¯å¢ƒï¼Œä¹Ÿèƒ½å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºæ¨åŠ¨æœºå™¨äººæ™ºèƒ½çš„å‘å±•ï¼Œä½¿å…¶æ›´åŠ æ™ºèƒ½åŒ–ã€è‡ªä¸»åŒ–å’Œé€‚åº”æ€§æ›´å¼ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.

