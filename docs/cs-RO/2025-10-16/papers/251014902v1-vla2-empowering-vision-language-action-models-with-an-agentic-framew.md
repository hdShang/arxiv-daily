---
layout: default
title: VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation
---

# VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14902" target="_blank" class="toolbar-btn">arXiv: 2510.14902v1</a>
    <a href="https://arxiv.org/pdf/2510.14902.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14902v1" 
            onclick="toggleFavorite(this, '2510.14902v1', 'VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**VLA^2ÔºöÂà©Áî®AgentÊ°ÜÊû∂Â¢ûÂº∫VLAÊ®°ÂûãÂ§ÑÁêÜÊú™ËßÅÊ¶ÇÂøµÊìç‰ΩúÁöÑËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Ê≥õÂåñËÉΩÂäõ` `AgentÊ°ÜÊû∂` `Áü•ËØÜÊ£ÄÁ¥¢` `ÂØπË±°Ê£ÄÊµã` `ÂàÜÂ∏ÉÂ§ñÊ≥õÂåñ` `Â§öÊ®°ÊÄÅÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Â§ÑÁêÜÊú™ËßÅËøáÁöÑÁâ©‰ΩìÊ¶ÇÂøµÊó∂Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÂØºËá¥Êìç‰Ωú‰ªªÂä°ÊàêÂäüÁéáÊòæËëó‰∏ãÈôç„ÄÇ
2. VLA^2Ê°ÜÊû∂Âà©Áî®OpenVLA‰Ωú‰∏∫È™®Âπ≤ÔºåÁªìÂêàÁΩëÁªúÊ£ÄÁ¥¢ÂíåÂØπË±°Ê£ÄÊµãÁ≠âÂ§ñÈÉ®Ê®°ÂùóÔºåÂ¢ûÂº∫ÂØπÊú™Áü•Ê¶ÇÂøµÁöÑÁêÜËß£„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVLA^2Âú®Âõ∞ÈöæÁ∫ßÂà´ÁöÑÊ≥õÂåñÂü∫ÂáÜ‰∏äÔºåÁõ∏ÊØîOpenVLAÂü∫Á∫øÊàêÂäüÁéáÊèêÂçá‰∫Ü44.2%„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®Â§ßÂûãÊú∫Âô®‰∫∫Êï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÂêéÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÂ§ö‰ªªÂä°ËÉΩÂäõÔºåÂπ∂‰∏îËÉΩÂ§üÂæàÂ•ΩÂú∞Ê≥õÂåñÂà∞Êìç‰Ωú‰ªªÂä°‰∏≠ËßÜËßâÂíåËØ≠Ë®ÄÊåá‰ª§ÁöÑÂèòÂåñ„ÄÇÁÑ∂ËÄåÔºåÂΩìÈù¢ÂØπËÆ≠ÁªÉÊï∞ÊçÆ‰πãÂ§ñÁöÑÂØπË±°Ê¶ÇÂøµÊó∂Ôºå‰æãÂ¶ÇÊï∞ÊçÆÈõÜ‰∏≠Êú™ËßÅËøáÁöÑÂØπË±°ÊèèËø∞ÂíåÁ∫πÁêÜÔºåÂÆÉ‰ª¨ÁöÑÊàêÂäüÁéá‰ºöÊòæËëó‰∏ãÈôç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑAgentÊ°ÜÊû∂VLA^2ÔºåÂÆÉÂà©Áî®OpenVLA‰Ωú‰∏∫ÊâßË°åÈ™®Âπ≤ÔºåÂπ∂ÊúâÊïàÂú∞Âà©Áî®ËØ∏Â¶ÇÁΩëÁªúÊ£ÄÁ¥¢ÂíåÂØπË±°Ê£ÄÊµãÁ≠âÂ§ñÈÉ®Ê®°ÂùóÔºå‰∏∫VLAÊèê‰æõÂÖ≥‰∫éÁõÆÊ†áÂØπË±°ÁöÑËßÜËßâÂíåÊñáÊú¨Áü•ËØÜ„ÄÇËøôÁßçÊñπÊ≥ïÂáèËΩª‰∫ÜÂ§ÑÁêÜÂàÜÂ∏ÉÂ§ñÂØπË±°Êó∂ÁöÑÊ≥õÂåñÂ§±Ë¥•„ÄÇÂü∫‰∫éLIBEROÊ®°ÊãüÁéØÂ¢ÉÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÂØπË±°ÂíåÂØπË±°ÊèèËø∞ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËØÑ‰º∞Âü∫ÂáÜÔºåÂåÖÂê´‰∏â‰∏™ÈöæÂ∫¶Á∫ßÂà´Ôºå‰ª•ÊµãËØïÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÊàêÂäüÂú∞Ë∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®Êàë‰ª¨ËÆæËÆ°ÁöÑÂõ∞ÈöæÁ∫ßÂà´Ê≥õÂåñÂü∫ÂáÜ‰∏ä„ÄÇ‰∏éÁã¨Á´ãÁöÑOpenVLAÂü∫Á∫øÁõ∏ÊØîÔºåVLA^2Âú®Âõ∞ÈöæÁ∫ßÂà´Âü∫ÂáÜ‰∏äÁöÑÊàêÂäüÁéáÊèêÈ´ò‰∫Ü44.2%ÔºåÂú®ÊâÄÊúâÂÆöÂà∂ÁéØÂ¢É‰∏≠Âπ≥ÂùáÊèêÈ´ò‰∫Ü20.2%ÔºåËÄåÊ≤°ÊúâÈôç‰ΩéÂú®ÂüüÂÜÖ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâVLAÊ®°ÂûãÂú®Â§ÑÁêÜËÆ≠ÁªÉÈõÜ‰∏≠Êú™ËßÅËøáÁöÑÁâ©‰ΩìÊ¶ÇÂøµÔºà‰æãÂ¶ÇÊñ∞ÁöÑÁâ©‰ΩìÊèèËø∞„ÄÅÁ∫πÁêÜÁ≠âÔºâÊó∂ÔºåÊ≥õÂåñËÉΩÂäõÊòæËëó‰∏ãÈôçÔºåÂØºËá¥Êìç‰Ωú‰ªªÂä°ÁöÑÊàêÂäüÁéáÈôç‰Ωé„ÄÇËøôÊòØÂõ†‰∏∫VLAÊ®°Âûã‰æùËµñ‰∫éÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑÁü•ËØÜÔºåÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜÂàÜÂ∏ÉÂ§ñÁöÑÁâ©‰Ωì„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVLA^2ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®‰∏Ä‰∏™AgentÊ°ÜÊû∂ÔºåÈÄöËøáÈõÜÊàêÂ§ñÈÉ®Áü•ËØÜÊ∫êÔºàÂ¶ÇÁΩëÁªúÊ£ÄÁ¥¢ÂíåÂØπË±°Ê£ÄÊµãÔºâÔºåÊù•Â¢ûÂº∫VLAÊ®°ÂûãÂØπÊú™Áü•Áâ©‰ΩìÊ¶ÇÂøµÁöÑÁêÜËß£ÂíåÂ§ÑÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÂÖÅËÆ∏VLAÊ®°ÂûãÂú®ÈÅáÂà∞Êú™Áü•Áâ©‰ΩìÊó∂Ôºå‰∏ªÂä®Êü•ËØ¢Áõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ªéËÄåÂº•Ë°•È¢ÑËÆ≠ÁªÉÁü•ËØÜÁöÑ‰∏çË∂≥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVLA^2Ê°ÜÊû∂‰ª•OpenVLA‰Ωú‰∏∫ÊâßË°åÈ™®Âπ≤Ôºå‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÊ®°ÂùóÔºö1) **OpenVLA**ÔºöË¥üË¥£ÊâßË°åÊìç‰Ωú‰ªªÂä°Ôºõ2) **Web Retrieval Module**ÔºöÁî®‰∫é‰ªé‰∫íËÅîÁΩëÊ£ÄÁ¥¢ÂÖ≥‰∫éÁõÆÊ†áÁâ©‰ΩìÁöÑÊñáÊú¨‰ø°ÊÅØÔºõ3) **Object Detection Module**ÔºöÁî®‰∫éÊ£ÄÊµãÂõæÂÉè‰∏≠ÁöÑÁõÆÊ†áÁâ©‰ΩìÔºåÊèê‰æõËßÜËßâ‰ø°ÊÅØ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÂΩìOpenVLAÈÅáÂà∞Êú™Áü•Áâ©‰ΩìÊó∂ÔºåAgentÊ°ÜÊû∂‰ºöË∞ÉÁî®Web Retrieval ModuleÂíåObject Detection ModuleËé∑ÂèñÁõ∏ÂÖ≥Áü•ËØÜÔºåÁÑ∂ÂêéÂ∞ÜËøô‰∫õÁü•ËØÜËûçÂÖ•Âà∞VLAÊ®°ÂûãÁöÑËæìÂÖ•‰∏≠Ôºå‰ªéËÄåÊåáÂØºVLAÊ®°ÂûãÊâßË°åÊìç‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVLA^2ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫Ü‰∏Ä‰∏™AgentÊ°ÜÊû∂ÔºåÂ∞ÜVLAÊ®°Âûã‰∏éÂ§ñÈÉ®Áü•ËØÜÊ∫êËøûÊé•Ëµ∑Êù•Ôºå‰ΩøÂÖ∂ÂÖ∑Â§á‰∫Ü‰∏ªÂä®Â≠¶‰π†ÂíåÈÄÇÂ∫îÊú™Áü•ÁéØÂ¢ÉÁöÑËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑVLAÊ®°ÂûãÁõ∏ÊØîÔºåVLA^2‰∏çÂÜç‰ªÖ‰ªÖ‰æùËµñ‰∫éÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑÁü•ËØÜÔºåËÄåÊòØËÉΩÂ§üÈÄöËøáÊü•ËØ¢Â§ñÈÉ®‰ø°ÊÅØÊù•Êâ©Â±ïÂÖ∂Áü•ËØÜÂ∫ìÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂàÜÂ∏ÉÂ§ñÁöÑÁâ©‰Ωì„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVLA^2ÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Â¶Ç‰ΩïÊúâÊïàÂú∞Â∞Ü‰ªéWeb Retrieval ModuleÂíåObject Detection ModuleËé∑ÂèñÁöÑÁü•ËØÜËûçÂÖ•Âà∞VLAÊ®°ÂûãÁöÑËæìÂÖ•‰∏≠Ôºõ2) Â¶Ç‰ΩïËÆæËÆ°AgentÊ°ÜÊû∂ÁöÑÂÜ≥Á≠ñÊú∫Âà∂Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊ†πÊçÆÂΩìÂâç‰ªªÂä°ÁöÑÈúÄÊ±ÇÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÂ§ñÈÉ®Ê®°ÂùóËøõË°åÊü•ËØ¢Ôºõ3) Â¶Ç‰ΩïÂπ≥Ë°°Â§ñÈÉ®Áü•ËØÜÁöÑÂà©Áî®ÂíåVLAÊ®°ÂûãËá™Ë∫´ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÈÅøÂÖçËøáÂ∫¶‰æùËµñÂ§ñÈÉ®‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VLA^2Âú®LIBEROÊ®°ÊãüÁéØÂ¢É‰∏≠ËøõË°å‰∫ÜËØÑ‰º∞ÔºåÁªìÊûúË°®ÊòéÔºåÂú®Âõ∞ÈöæÁ∫ßÂà´ÁöÑÊ≥õÂåñÂü∫ÂáÜ‰∏äÔºåVLA^2ÁöÑÊàêÂäüÁéáÊØîOpenVLAÂü∫Á∫øÊèêÈ´ò‰∫Ü44.2%„ÄÇÂú®ÊâÄÊúâÂÆöÂà∂ÁéØÂ¢É‰∏≠ÔºåVLA^2ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÊèêÈ´ò‰∫Ü20.2%ÔºåÂêåÊó∂Ê≤°ÊúâÈôç‰ΩéÂú®ÂüüÂÜÖ‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåVLA^2ËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òVLAÊ®°ÂûãÂú®Â§ÑÁêÜÊú™Áü•Áâ©‰ΩìÊ¶ÇÂøµÊó∂ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VLA^2ÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂú®ÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂåªÁñóËæÖÂä©Êú∫Âô®‰∫∫Á≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°å‰∫∫Á±ªÊåá‰ª§ÔºåÂç≥‰ΩøÈù¢ÂØπÊú™Áü•ÁöÑÁâ©‰ΩìÂíåÁéØÂ¢ÉÔºå‰πüËÉΩÂÆåÊàêÂ§çÊùÇÁöÑ‰ªªÂä°„ÄÇËØ•Á†îÁ©∂ÁöÑÊú™Êù•ÂΩ±ÂìçÂú®‰∫éÊé®Âä®Êú∫Âô®‰∫∫Êô∫ËÉΩÁöÑÂèëÂ±ïÔºå‰ΩøÂÖ∂Êõ¥Âä†Êô∫ËÉΩÂåñ„ÄÅËá™‰∏ªÂåñÂíåÈÄÇÂ∫îÊÄßÊõ¥Âº∫„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.

