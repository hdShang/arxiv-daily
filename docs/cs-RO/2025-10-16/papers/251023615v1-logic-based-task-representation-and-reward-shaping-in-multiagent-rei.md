---
layout: default
title: Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning
---

# Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.23615" target="_blank" class="toolbar-btn">arXiv: 2510.23615v1</a>
    <a href="https://arxiv.org/pdf/2510.23615.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23615v1" 
            onclick="toggleFavorite(this, '2510.23615v1', 'Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Nishant Doshi

**ÂàÜÁ±ª**: cs.MA, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÈÄªËæëÁöÑ‰ªªÂä°Ë°®Á§∫ÂíåÂ•ñÂä±Â°ëÈÄ†ÊñπÊ≥ïÔºåÂä†ÈÄüÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†` `Á∫øÊÄßÊó∂Â∫èÈÄªËæë` `Â•ñÂä±Â°ëÈÄ†` `B√ºchiËá™Âä®Êú∫` `ÂçäÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†Èù¢‰∏¥Ê†∑Êú¨Â§çÊùÇÂ∫¶È´òÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇ‰ªªÂä°ÂíåÂ§ßËßÑÊ®°Áä∂ÊÄÅÁ©∫Èó¥‰∏≠„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÈÄªËæëÁöÑ‰ªªÂä°Ë°®Á§∫ÊñπÊ≥ïÔºåÂπ∂ÁªìÂêàÂ•ñÂä±Â°ëÈÄ†ÊäÄÊúØÔºåÂä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Á°ÆÂÆöÊÄßÁΩëÊ†º‰∏ñÁïå‰∏≠ËÉΩÊòæËëóÂáèÂ∞ëÊî∂ÊïõÊó∂Èó¥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Áä∂ÊÄÅÁ©∫Èó¥ËæÉÂ§ßÊó∂„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂä†ÈÄüÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü‰∏≠Ôºå‰ΩøÁî®Á∫øÊÄßÊó∂Â∫èÈÄªËæëÔºàLTLÔºâË°®Á§∫ÁöÑ‰ªªÂä°ÁöÑÊúÄ‰ºòÁ≠ñÁï•Â≠¶‰π†ÊñπÊ≥ï„ÄÇÁªôÂÆöÊØè‰∏™Êô∫ËÉΩ‰ΩìÁöÑ‰∏ÄÁªÑÈÄâÈ°πÔºàÊó∂Èó¥ÊäΩË±°Âä®‰ΩúÔºâÔºåÊàë‰ª¨Â∞Ü‰ªªÂä°ËßÑËåÉËΩ¨Êç¢‰∏∫Áõ∏Â∫îÁöÑB√ºchiËá™Âä®Êú∫ÔºåÂπ∂ÈááÁî®‰∏ÄÁßçÊó†Ê®°ÂûãÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÊî∂ÈõÜËΩ¨Êç¢Ê†∑Êú¨Âπ∂Âä®ÊÄÅÊûÑÂª∫‰πòÁßØÂçäÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàSMDPÔºâ„ÄÇÁÑ∂ÂêéÔºåÂèØ‰ª•‰ΩøÁî®Âü∫‰∫é‰ª∑ÂÄºÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÊù•ÂêàÊàêÊ≠£Á°ÆËÆæËÆ°ÁöÑÊéßÂà∂Âô®ÔºåËÄåÊó†ÈúÄÂ≠¶‰π†Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑÂ∫ïÂ±ÇËΩ¨Êç¢Ê®°Âûã„ÄÇÂà©Áî®‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ•ñÂä±Â°ëÈÄ†ÊñπÊ≥ïÊù•Â§ÑÁêÜÁî±‰∫éÂ§ö‰∏™Êô∫ËÉΩ‰ΩìËÄåÂØºËá¥ÁöÑÊåáÊï∞Á∫ßÊ†∑Êú¨Â§çÊùÇÂ∫¶„ÄÇÊàë‰ª¨Âú®Á°ÆÂÆöÊÄßÁΩëÊ†º‰∏ñÁïåÊ®°Êãü‰∏≠ÈíàÂØπ‰∏çÂêåÁöÑ‰ªªÂä°ÊµãËØï‰∫ÜÊâÄÊèêÂá∫ÁöÑÁÆóÊ≥ïÔºåÂèëÁé∞Â•ñÂä±Â°ëÈÄ†ÊòæËëóÂáèÂ∞ë‰∫ÜÊî∂ÊïõÊó∂Èó¥„ÄÇÊàë‰ª¨ËøòÊé®Êñ≠ÔºåÈöèÁùÄÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü‰∏≠Áä∂ÊÄÅÂíåÂä®‰ΩúÁ©∫Èó¥ÁöÑÂ¢ûÂä†Ôºå‰ΩøÁî®ÈÄâÈ°πÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†‰∏≠Ôºå‰ªªÂä°Â§çÊùÇÊÄßÂíåÁä∂ÊÄÅÁ©∫Èó¥Â¢ûÂ§ßÂØºËá¥ÁöÑÊ†∑Êú¨Â§çÊùÇÂ∫¶ËøáÈ´òÁöÑÈóÆÈ¢ò„ÄÇ‰º†ÁªüÊñπÊ≥ïÈöæ‰ª•Âú®ÂêàÁêÜÊó∂Èó¥ÂÜÖÂ≠¶‰π†Âà∞ÊúÄ‰ºòÁ≠ñÁï•ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÊª°Ë∂≥Â§çÊùÇÊó∂Â∫èÈÄªËæëÁ∫¶ÊùüÁöÑ‰ªªÂä°‰∏≠„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑ trial-and-error Êé¢Á¥¢ÔºåÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞Ü‰ªªÂä°ËßÑËåÉÁî®Á∫øÊÄßÊó∂Â∫èÈÄªËæëÔºàLTLÔºâË°®Á§∫ÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫B√ºchiËá™Âä®Êú∫„ÄÇÁÑ∂ÂêéÔºåÂà©Áî®B√ºchiËá™Âä®Êú∫ÊåáÂØºÂº∫ÂåñÂ≠¶‰π†ËøáÁ®ãÔºåÂπ∂ÁªìÂêàÂ•ñÂä±Â°ëÈÄ†ÊäÄÊúØÔºåÂºïÂØºÊô∫ËÉΩ‰ΩìÊõ¥Âø´Âú∞Â≠¶‰π†Âà∞Êª°Ë∂≥LTLÁ∫¶ÊùüÁöÑÁ≠ñÁï•„ÄÇÈÄöËøá‰ΩøÁî®ÈÄâÈ°πÔºàÊó∂Èó¥ÊäΩË±°Âä®‰ΩúÔºâÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÂáèÂ∞ëÊé¢Á¥¢Á©∫Èó¥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) ‰ΩøÁî®Á∫øÊÄßÊó∂Â∫èÈÄªËæëÔºàLTLÔºâÊèèËø∞‰ªªÂä°Ôºõ2) Â∞ÜLTLÂÖ¨ÂºèËΩ¨Êç¢‰∏∫B√ºchiËá™Âä®Êú∫Ôºõ3) Âü∫‰∫éB√ºchiËá™Âä®Êú∫ÊûÑÂª∫‰πòÁßØÂçäÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàSMDPÔºâÔºõ4) ‰ΩøÁî®Âü∫‰∫é‰ª∑ÂÄºÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºàÂ¶ÇQ-learningÔºâÂú®SMDP‰∏äÂ≠¶‰π†Á≠ñÁï•Ôºõ5) ‰ΩøÁî®Â•ñÂä±Â°ëÈÄ†ÊäÄÊúØÂä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®Êó†Ê®°ÂûãÊñπÊ≥ïÔºåÊó†ÈúÄÈ¢ÑÂÖàÂ≠¶‰π†ÁéØÂ¢ÉÁöÑËΩ¨Êç¢Ê®°Âûã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÈÄªËæëÊé®ÁêÜ‰∏éÂº∫ÂåñÂ≠¶‰π†Áõ∏ÁªìÂêàÔºåÂà©Áî®LTLËßÑËåÉÊåáÂØºÁ≠ñÁï•Â≠¶‰π†„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫ÁöÑÂ•ñÂä±Â°ëÈÄ†ÊñπÊ≥ïÊúâÊïàÂú∞Èôç‰Ωé‰∫ÜÊ†∑Êú¨Â§çÊùÇÂ∫¶ÔºåÂä†ÈÄü‰∫ÜÊî∂ÊïõÈÄüÂ∫¶„ÄÇ‰ΩøÁî®ÈÄâÈ°πÔºàÊó∂Èó¥ÊäΩË±°Âä®‰ΩúÔºâËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÁÆóÊ≥ïÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰ΩøÁî®Á∫øÊÄßÊó∂Â∫èÈÄªËæëÔºàLTLÔºâÊù•ÊèèËø∞‰ªªÂä°ÔºåËøôÊòØ‰∏ÄÁßçÁî®‰∫éÊåáÂÆöÊó∂Â∫èÊÄßË¥®ÁöÑ formal language„ÄÇB√ºchiËá™Âä®Êú∫Áî®‰∫éÈ™åËØÅÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫ÊòØÂê¶Êª°Ë∂≥LTLËßÑËåÉ„ÄÇÂ•ñÂä±Â°ëÈÄ†ÂáΩÊï∞ÁöÑËÆæËÆ°ÊòØÂÖ≥ÈîÆÔºåÈúÄË¶ÅÊ†πÊçÆLTLËßÑËåÉÂíåB√ºchiËá™Âä®Êú∫ÁöÑÁä∂ÊÄÅËøõË°åË∞ÉÊï¥Ôºå‰ª•ÂºïÂØºÊô∫ËÉΩ‰ΩìÊõ¥Âø´Âú∞Â≠¶‰π†Âà∞Êª°Ë∂≥Á∫¶ÊùüÁöÑÁ≠ñÁï•„ÄÇÂÖ∑‰ΩìÂ•ñÂä±ÂáΩÊï∞ÁöÑÈÄâÊã©ÂíåÂèÇÊï∞ËÆæÁΩÆÊú™Âú®ÊëòË¶Å‰∏≠ËØ¶ÁªÜËØ¥ÊòéÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÂ•ñÂä±Â°ëÈÄ†ÊñπÊ≥ïËÉΩÂ§üÊòæËëóÂáèÂ∞ëÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÁöÑÊî∂ÊïõÊó∂Èó¥„ÄÇÂú®Á°ÆÂÆöÊÄßÁΩëÊ†º‰∏ñÁïåÊ®°Êãü‰∏≠ÔºåÁõ∏ÊØî‰∫éÊ≤°ÊúâÂ•ñÂä±Â°ëÈÄ†ÁöÑÊñπÊ≥ïÔºåÊî∂ÊïõÈÄüÂ∫¶ÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçá„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åËøòË°®ÊòéÔºåÈöèÁùÄÁä∂ÊÄÅÂíåÂä®‰ΩúÁ©∫Èó¥ÁöÑÂ¢ûÂä†Ôºå‰ΩøÁî®ÈÄâÈ°πÔºàÊó∂Èó¥ÊäΩË±°Âä®‰ΩúÔºâÂèòÂæóË∂äÊù•Ë∂äÈáçË¶ÅÔºåÂèØ‰ª•ÊúâÊïàÈôç‰ΩéÊé¢Á¥¢Á©∫Èó¥ÔºåÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊèêÂçáÊï∞ÊçÆÊú™Âú®ÊëòË¶Å‰∏≠ÁªôÂá∫ÔºåÂ±û‰∫éÊú™Áü•‰ø°ÊÅØ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫„ÄÅËá™Âä®ÂåñÊéßÂà∂„ÄÅ‰∫§ÈÄöÁÆ°ÁêÜÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Áî®‰∫éÊéßÂà∂Â§öÊú∫Âô®‰∫∫ÂçèÂêåÂÆåÊàêÂ§çÊùÇ‰ªªÂä°ÔºåÂ¶Ç‰ªìÂ∫ìÊã£ÈÄâ„ÄÅÁéØÂ¢ÉÁõëÊµãÁ≠â„ÄÇÈÄöËøá‰ΩøÁî®LTLËßÑËåÉÔºåÂèØ‰ª•Á°Æ‰øùÊú∫Âô®‰∫∫ÊåâÁÖßÈ¢ÑÂÆöÁöÑÊó∂Â∫èÈÄªËæëÂÆåÊàê‰ªªÂä°ÔºåÊèêÈ´òÁ≥ªÁªüÁöÑÂèØÈù†ÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇËØ•ÊñπÊ≥ïËøòÂèØÁî®‰∫é‰∫§ÈÄö‰ø°Âè∑ÁÅØÊéßÂà∂Ôºå‰ºòÂåñ‰∫§ÈÄöÊµÅÈáèÔºåÂáèÂ∞ëÊã•Â†µ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper presents an approach for accelerated learning of optimal plans for a given task represented using Linear Temporal Logic (LTL) in multi-agent systems. Given a set of options (temporally abstract actions) available to each agent, we convert the task specification into the corresponding Buchi Automaton and proceed with a model-free approach which collects transition samples and constructs a product Semi Markov Decision Process (SMDP) on-the-fly. Value-based Reinforcement Learning algorithms can then be used to synthesize a correct-by-design controller without learning the underlying transition model of the multi-agent system. The exponential sample complexity due to multiple agents is dealt with using a novel reward shaping approach. We test the proposed algorithm in a deterministic gridworld simulation for different tasks and find that the reward shaping results in significant reduction in convergence times. We also infer that using options becomes increasing more relevant as the state and action space increases in multi-agent systems.

