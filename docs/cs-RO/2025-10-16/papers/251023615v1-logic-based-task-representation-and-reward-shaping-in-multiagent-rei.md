---
layout: default
title: Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning
---

# Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23615" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23615v1</a>
  <a href="https://arxiv.org/pdf/2510.23615.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23615v1" onclick="toggleFavorite(this, '2510.23615v1', 'Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nishant Doshi

**åˆ†ç±»**: cs.MA, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé€»è¾‘çš„ä»»åŠ¡è¡¨ç¤ºå’Œå¥–åŠ±å¡‘é€ æ–¹æ³•ï¼ŒåŠ é€Ÿå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ` `çº¿æ€§æ—¶åºé€»è¾‘` `å¥–åŠ±å¡‘é€ ` `BÃ¼chiè‡ªåŠ¨æœº` `åŠé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ é¢ä¸´æ ·æœ¬å¤æ‚åº¦é«˜çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡å’Œå¤§è§„æ¨¡çŠ¶æ€ç©ºé—´ä¸­ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºé€»è¾‘çš„ä»»åŠ¡è¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶ç»“åˆå¥–åŠ±å¡‘é€ æŠ€æœ¯ï¼ŒåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¡®å®šæ€§ç½‘æ ¼ä¸–ç•Œä¸­èƒ½æ˜¾è‘—å‡å°‘æ”¶æ•›æ—¶é—´ï¼Œå°¤å…¶æ˜¯åœ¨çŠ¶æ€ç©ºé—´è¾ƒå¤§æ—¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ é€Ÿå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œä½¿ç”¨çº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰è¡¨ç¤ºçš„ä»»åŠ¡çš„æœ€ä¼˜ç­–ç•¥å­¦ä¹ æ–¹æ³•ã€‚ç»™å®šæ¯ä¸ªæ™ºèƒ½ä½“çš„ä¸€ç»„é€‰é¡¹ï¼ˆæ—¶é—´æŠ½è±¡åŠ¨ä½œï¼‰ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡è§„èŒƒè½¬æ¢ä¸ºç›¸åº”çš„BÃ¼chiè‡ªåŠ¨æœºï¼Œå¹¶é‡‡ç”¨ä¸€ç§æ— æ¨¡å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ”¶é›†è½¬æ¢æ ·æœ¬å¹¶åŠ¨æ€æ„å»ºä¹˜ç§¯åŠé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆSMDPï¼‰ã€‚ç„¶åï¼Œå¯ä»¥ä½¿ç”¨åŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥åˆæˆæ­£ç¡®è®¾è®¡çš„æ§åˆ¶å™¨ï¼Œè€Œæ— éœ€å­¦ä¹ å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åº•å±‚è½¬æ¢æ¨¡å‹ã€‚åˆ©ç”¨ä¸€ç§æ–°é¢–çš„å¥–åŠ±å¡‘é€ æ–¹æ³•æ¥å¤„ç†ç”±äºå¤šä¸ªæ™ºèƒ½ä½“è€Œå¯¼è‡´çš„æŒ‡æ•°çº§æ ·æœ¬å¤æ‚åº¦ã€‚æˆ‘ä»¬åœ¨ç¡®å®šæ€§ç½‘æ ¼ä¸–ç•Œæ¨¡æ‹Ÿä¸­é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡æµ‹è¯•äº†æ‰€æå‡ºçš„ç®—æ³•ï¼Œå‘ç°å¥–åŠ±å¡‘é€ æ˜¾è‘—å‡å°‘äº†æ”¶æ•›æ—¶é—´ã€‚æˆ‘ä»¬è¿˜æ¨æ–­ï¼Œéšç€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„å¢åŠ ï¼Œä½¿ç”¨é€‰é¡¹å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä»»åŠ¡å¤æ‚æ€§å’ŒçŠ¶æ€ç©ºé—´å¢å¤§å¯¼è‡´çš„æ ·æœ¬å¤æ‚åº¦è¿‡é«˜çš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥åœ¨åˆç†æ—¶é—´å†…å­¦ä¹ åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ»¡è¶³å¤æ‚æ—¶åºé€»è¾‘çº¦æŸçš„ä»»åŠ¡ä¸­ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„ trial-and-error æ¢ç´¢ï¼Œæ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ä»»åŠ¡è§„èŒƒç”¨çº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰è¡¨ç¤ºï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºBÃ¼chiè‡ªåŠ¨æœºã€‚ç„¶åï¼Œåˆ©ç”¨BÃ¼chiè‡ªåŠ¨æœºæŒ‡å¯¼å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶ç»“åˆå¥–åŠ±å¡‘é€ æŠ€æœ¯ï¼Œå¼•å¯¼æ™ºèƒ½ä½“æ›´å¿«åœ°å­¦ä¹ åˆ°æ»¡è¶³LTLçº¦æŸçš„ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨é€‰é¡¹ï¼ˆæ—¶é—´æŠ½è±¡åŠ¨ä½œï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘æ¢ç´¢ç©ºé—´ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä½¿ç”¨çº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰æè¿°ä»»åŠ¡ï¼›2) å°†LTLå…¬å¼è½¬æ¢ä¸ºBÃ¼chiè‡ªåŠ¨æœºï¼›3) åŸºäºBÃ¼chiè‡ªåŠ¨æœºæ„å»ºä¹˜ç§¯åŠé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆSMDPï¼‰ï¼›4) ä½¿ç”¨åŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚Q-learningï¼‰åœ¨SMDPä¸Šå­¦ä¹ ç­–ç•¥ï¼›5) ä½¿ç”¨å¥–åŠ±å¡‘é€ æŠ€æœ¯åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ— æ¨¡å‹æ–¹æ³•ï¼Œæ— éœ€é¢„å…ˆå­¦ä¹ ç¯å¢ƒçš„è½¬æ¢æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†é€»è¾‘æ¨ç†ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œåˆ©ç”¨LTLè§„èŒƒæŒ‡å¯¼ç­–ç•¥å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæå‡ºçš„å¥–åŠ±å¡‘é€ æ–¹æ³•æœ‰æ•ˆåœ°é™ä½äº†æ ·æœ¬å¤æ‚åº¦ï¼ŒåŠ é€Ÿäº†æ”¶æ•›é€Ÿåº¦ã€‚ä½¿ç”¨é€‰é¡¹ï¼ˆæ—¶é—´æŠ½è±¡åŠ¨ä½œï¼‰è¿›ä¸€æ­¥æå‡äº†ç®—æ³•çš„å¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨çº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰æ¥æè¿°ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæŒ‡å®šæ—¶åºæ€§è´¨çš„ formal languageã€‚BÃ¼chiè‡ªåŠ¨æœºç”¨äºéªŒè¯æ™ºèƒ½ä½“çš„è¡Œä¸ºæ˜¯å¦æ»¡è¶³LTLè§„èŒƒã€‚å¥–åŠ±å¡‘é€ å‡½æ•°çš„è®¾è®¡æ˜¯å…³é”®ï¼Œéœ€è¦æ ¹æ®LTLè§„èŒƒå’ŒBÃ¼chiè‡ªåŠ¨æœºçš„çŠ¶æ€è¿›è¡Œè°ƒæ•´ï¼Œä»¥å¼•å¯¼æ™ºèƒ½ä½“æ›´å¿«åœ°å­¦ä¹ åˆ°æ»¡è¶³çº¦æŸçš„ç­–ç•¥ã€‚å…·ä½“å¥–åŠ±å‡½æ•°çš„é€‰æ‹©å’Œå‚æ•°è®¾ç½®æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å¥–åŠ±å¡‘é€ æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›æ—¶é—´ã€‚åœ¨ç¡®å®šæ€§ç½‘æ ¼ä¸–ç•Œæ¨¡æ‹Ÿä¸­ï¼Œç›¸æ¯”äºæ²¡æœ‰å¥–åŠ±å¡‘é€ çš„æ–¹æ³•ï¼Œæ”¶æ•›é€Ÿåº¦å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œéšç€çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„å¢åŠ ï¼Œä½¿ç”¨é€‰é¡¹ï¼ˆæ—¶é—´æŠ½è±¡åŠ¨ä½œï¼‰å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå¯ä»¥æœ‰æ•ˆé™ä½æ¢ç´¢ç©ºé—´ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººã€è‡ªåŠ¨åŒ–æ§åˆ¶ã€äº¤é€šç®¡ç†ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºæ§åˆ¶å¤šæœºå™¨äººååŒå®Œæˆå¤æ‚ä»»åŠ¡ï¼Œå¦‚ä»“åº“æ‹£é€‰ã€ç¯å¢ƒç›‘æµ‹ç­‰ã€‚é€šè¿‡ä½¿ç”¨LTLè§„èŒƒï¼Œå¯ä»¥ç¡®ä¿æœºå™¨äººæŒ‰ç…§é¢„å®šçš„æ—¶åºé€»è¾‘å®Œæˆä»»åŠ¡ï¼Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•è¿˜å¯ç”¨äºäº¤é€šä¿¡å·ç¯æ§åˆ¶ï¼Œä¼˜åŒ–äº¤é€šæµé‡ï¼Œå‡å°‘æ‹¥å µã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents an approach for accelerated learning of optimal plans for a given task represented using Linear Temporal Logic (LTL) in multi-agent systems. Given a set of options (temporally abstract actions) available to each agent, we convert the task specification into the corresponding Buchi Automaton and proceed with a model-free approach which collects transition samples and constructs a product Semi Markov Decision Process (SMDP) on-the-fly. Value-based Reinforcement Learning algorithms can then be used to synthesize a correct-by-design controller without learning the underlying transition model of the multi-agent system. The exponential sample complexity due to multiple agents is dealt with using a novel reward shaping approach. We test the proposed algorithm in a deterministic gridworld simulation for different tasks and find that the reward shaping results in significant reduction in convergence times. We also infer that using options becomes increasing more relevant as the state and action space increases in multi-agent systems.

