---
layout: default
title: RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation
---

# RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.15189" target="_blank" class="toolbar-btn">arXiv: 2510.15189v1</a>
    <a href="https://arxiv.org/pdf/2510.15189.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15189v1" 
            onclick="toggleFavorite(this, '2510.15189v1', 'RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiangyu Chen, Chuhao Zhou, Yuxi Liu, Jianfei Yang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RM-RLÔºöÈù¢ÂêëÁ≤æÂáÜÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑËßíËâ≤Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `Âº∫ÂåñÂ≠¶‰π†` `Ê®°‰ªøÂ≠¶‰π†` `ËßíËâ≤Ê®°Âûã` `Âú®Á∫øÂ≠¶‰π†` `Á¶ªÁ∫øÂ≠¶‰π†` `Á≤æÂáÜÊìç‰Ωú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ï‰æùËµñ‰∏ìÂÆ∂ÊºîÁ§∫ÊàñÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÔºåÈù¢‰∏¥È´òË¥®ÈáèÊï∞ÊçÆËé∑ÂèñÈöæ„ÄÅÂàÜÂ∏ÉÂÅèÁßªÁ≠âÊåëÊàò„ÄÇ
2. RM-RLÊ°ÜÊû∂ÈÄöËøáËßíËâ≤Ê®°ÂûãËá™Âä®ÁîüÊàêÂú®Á∫øËÆ≠ÁªÉÊ†áÁ≠æÔºåÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ËΩ¨Âåñ‰∏∫ÁõëÁù£Â≠¶‰π†ÔºåÊèêÂçáËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåRM-RLÂú®ÁúüÂÆûÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÂπ≥ÁßªÂíåÊóãËΩ¨Á≤æÂ∫¶ÔºåÂπ∂ÊàêÂäüÂÆåÊàêÂ§çÊùÇ‰ªªÂä°„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Á≤æÂáÜÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÂØπ‰∫éÁ≤æÁªÜÂ∫îÁî®Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰æãÂ¶ÇÂåñÂ≠¶ÂíåÁîüÁâ©ÂÆûÈ™åÔºåÂç≥‰ΩøÂæÆÂ∞èÁöÑËØØÂ∑ÆÔºà‰æãÂ¶ÇÔºåËØïÂâÇÊ∫¢Âá∫Ôºâ‰πüÂèØËÉΩ‰ΩøÊï¥‰∏™‰ªªÂä°Êó†Êïà„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÈ¢ÑÂÖàÊî∂ÈõÜÁöÑ‰∏ìÂÆ∂ÊºîÁ§∫ÔºåÂπ∂ÈÄöËøáÊ®°‰ªøÂ≠¶‰π†ÔºàILÔºâÊàñÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊù•ËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåËé∑ÂæóÁî®‰∫éÁ≤æÁ°Æ‰ªªÂä°ÁöÑÈ´òË¥®ÈáèÊºîÁ§∫Êó¢Âõ∞ÈöæÂèàËÄóÊó∂ÔºåËÄåÁ¶ªÁ∫øRLÈÄöÂ∏∏‰ºöÂèóÂà∞ÂàÜÂ∏ÉÂÅèÁßªÂíå‰ΩéÊï∞ÊçÆÊïàÁéáÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçËßíËâ≤Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÔºàRM-RLÔºâÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Áªü‰∏Ä‰∫ÜÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÂú®Á∫øÂíåÁ¶ªÁ∫øËÆ≠ÁªÉ„ÄÇÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØ‰∏ÄÁßçËßíËâ≤Ê®°ÂûãÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•‰ΩøÁî®Ëøë‰ººÊúÄ‰ºòÂä®‰ΩúËá™Âä®ÁîüÊàêÂú®Á∫øËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊ†áÁ≠æÔºå‰ªéËÄåÊó†ÈúÄ‰∫∫Â∑•ÊºîÁ§∫„ÄÇRM-RLÂ∞ÜÁ≠ñÁï•Â≠¶‰π†ÈáçÊñ∞ÂÆö‰πâ‰∏∫ÁõëÁù£ËÆ≠ÁªÉÔºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÂàÜÂ∏É‰∏çÂåπÈÖçÂ∏¶Êù•ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂíåÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇÊ∑∑ÂêàËÆ≠ÁªÉÊñπÊ°àËøõ‰∏ÄÊ≠•Âà©Áî®Âú®Á∫øËßíËâ≤Ê®°ÂûãÊï∞ÊçÆËøõË°åÁ¶ªÁ∫øÈáçÁî®ÔºåÈÄöËøáÈáçÂ§çÈááÊ†∑Êù•ÊèêÈ´òÊï∞ÊçÆÊïàÁéá„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåRM-RLÊØîÁé∞ÊúâÁöÑRLÊñπÊ≥ïÊî∂ÊïõÊõ¥Âø´„ÄÅÊõ¥Á®≥ÂÆöÔºåÂπ∂Âú®ÁúüÂÆûÊìç‰Ωú‰∏≠‰∫ßÁîü‰∫ÜÊòæËëóÁöÑÊî∂ÁõäÔºöÂπ≥ÁßªÁ≤æÂ∫¶ÊèêÈ´ò‰∫Ü53%ÔºåÊóãËΩ¨Á≤æÂ∫¶ÊèêÈ´ò‰∫Ü20%„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊºîÁ§∫‰∫ÜÊàêÂäüÊâßË°å‰∏ÄÈ°πÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°ÔºåÂç≥Â∞ÜÁªÜËÉûÂüπÂÖªÊùøÁ≤æÁ°ÆÂú∞ÊîæÁΩÆÂú®Êû∂Â≠ê‰∏äÔºåÁ™ÅÂá∫‰∫ÜËØ•Ê°ÜÊû∂Âú®ÂÖàÂâçÊñπÊ≥ïÂ§±Ë¥•Êó∂ÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Á≤æÂáÜÊìç‰ΩúÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÈ´òÁ≤æÂ∫¶ÁöÑÂú∫ÊôØ‰∏ãÔºå‰æãÂ¶ÇÁîüÁâ©ÂÆûÈ™å„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÂ¶ÇÊ®°‰ªøÂ≠¶‰π†ÂíåÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Ôºå‰æùËµñ‰∫éÈ´òË¥®ÈáèÁöÑ‰∏ìÂÆ∂ÊºîÁ§∫Êï∞ÊçÆÔºåËÄåËé∑ÂèñËøô‰∫õÊï∞ÊçÆÈùûÂ∏∏Âõ∞Èöæ‰∏îËÄóÊó∂„ÄÇÊ≠§Â§ñÔºåÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ËøòÂÆπÊòìÂèóÂà∞ÂàÜÂ∏ÉÂÅèÁßªÁöÑÂΩ±ÂìçÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®‰∏Ä‰∏™‚ÄúËßíËâ≤Ê®°Âûã‚ÄùÊù•Ëá™Âä®ÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊ†áÁ≠æÔºå‰ªéËÄåÈÅøÂÖçÂØπ‰∫∫Â∑•ÊºîÁ§∫Êï∞ÊçÆÁöÑ‰æùËµñ„ÄÇËßíËâ≤Ê®°ÂûãÈÄöËøáËøë‰ººÊúÄ‰ºòÁöÑÂä®‰ΩúÊù•ÊåáÂØºÂú®Á∫øËÆ≠ÁªÉÔºåÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÁõëÁù£Â≠¶‰π†ÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRM-RLÊ°ÜÊû∂ÂåÖÂê´Âú®Á∫øÂíåÁ¶ªÁ∫øËÆ≠ÁªÉ‰∏§‰∏™Èò∂ÊÆµ„ÄÇÂú®Á∫øÈò∂ÊÆµÔºåËßíËâ≤Ê®°ÂûãÊ†πÊçÆÂΩìÂâçÁä∂ÊÄÅÁîüÊàêÂä®‰ΩúÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫Ê†áÁ≠æÁî®‰∫éËÆ≠ÁªÉÁ≠ñÁï•ÁΩëÁªú„ÄÇÁ¶ªÁ∫øÈò∂ÊÆµÔºåÂà©Áî®Âú®Á∫øÁîüÊàêÁöÑÊï∞ÊçÆËøõË°åÈáçÈááÊ†∑ÂíåËÆ≠ÁªÉÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´òÊï∞ÊçÆÂà©Áî®Áéá„ÄÇÊï¥‰ΩìÊ°ÜÊû∂ÈááÁî®Ê∑∑ÂêàËÆ≠ÁªÉÊñπÊ°àÔºå‰∫§ÊõøËøõË°åÂú®Á∫øÊé¢Á¥¢ÂíåÁ¶ªÁ∫ø‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRM-RLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂºïÂÖ•‰∫ÜËßíËâ≤Ê®°ÂûãÊù•Ëá™Âä®ÁîüÊàêËÆ≠ÁªÉÊ†áÁ≠æÔºå‰ªéËÄåÊëÜËÑ±‰∫ÜÂØπ‰∫∫Â∑•ÊºîÁ§∫Êï∞ÊçÆÁöÑ‰æùËµñ„ÄÇËøôÁßçÊñπÊ≥ïÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÁõëÁù£Â≠¶‰π†ÈóÆÈ¢òÔºåÈÅøÂÖç‰∫ÜÂàÜÂ∏ÉÂÅèÁßªÂ∏¶Êù•ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂíå‰ΩéÊïàÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËßíËâ≤Ê®°ÂûãÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÈúÄË¶ÅËÉΩÂ§üÁîüÊàêËøë‰ººÊúÄ‰ºòÁöÑÂä®‰Ωú„ÄÇÂÖ∑‰ΩìÁöÑÂÆûÁé∞ÊñπÂºèÊú™Áü•Ôºå‰ΩÜÊé®ÊµãÂèØËÉΩÈááÁî®ÊüêÁßçÂΩ¢ÂºèÁöÑËßÑÂàíÁÆóÊ≥ïÊàñÈ¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÊçüÂ§±ÂáΩÊï∞ÈááÁî®ÁõëÁù£Â≠¶‰π†Â∏∏Áî®ÁöÑ‰∫§ÂèâÁÜµÊçüÂ§±ÊàñÂùáÊñπËØØÂ∑ÆÊçüÂ§±„ÄÇÁΩëÁªúÁªìÊûÑÂèØËÉΩÈááÁî®Â∏∏ËßÅÁöÑÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÁªìÊûÑÔºåÂ¶ÇÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊàñÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºåÂÖ∑‰ΩìÂèñÂÜ≥‰∫é‰ªªÂä°ÁöÑÁâπÁÇπ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRM-RLÂú®ÁúüÂÆûÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰∏éÁé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåRM-RLÁöÑÊî∂ÊïõÈÄüÂ∫¶Êõ¥Âø´ÔºåËÆ≠ÁªÉËøáÁ®ãÊõ¥Á®≥ÂÆö„ÄÇÂú®Âπ≥ÁßªÁ≤æÂ∫¶ÊñπÈù¢ÔºåRM-RLÊèêÂçá‰∫Ü53%ÔºåÂú®ÊóãËΩ¨Á≤æÂ∫¶ÊñπÈù¢ÊèêÂçá‰∫Ü20%„ÄÇÊ≠§Â§ñÔºåRM-RLËøòÊàêÂäüÂÆåÊàê‰∫Ü‰∏ÄÈ°πÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°ÔºåÂç≥Â∞ÜÁªÜËÉûÂüπÂÖªÊùøÁ≤æÁ°ÆÂú∞ÊîæÁΩÆÂú®Êû∂Â≠ê‰∏äÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

RM-RLÊ°ÜÊû∂ÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈ´òÁ≤æÂ∫¶Êú∫Âô®‰∫∫Êìç‰ΩúÁöÑÈ¢ÜÂüüÔºå‰æãÂ¶ÇÔºöÂåñÂ≠¶ÂíåÁîüÁâ©ÂÆûÈ™å‰∏≠ÁöÑËØïÂâÇÈÖçÊØî„ÄÅÂåªÁñóÊâãÊúØ‰∏≠ÁöÑÁ≤æÂáÜÂÆö‰Ωç„ÄÅ‰ª•ÂèäÁ≤æÂØÜÂà∂ÈÄ†‰∏≠ÁöÑÈõ∂‰ª∂ÁªÑË£Ö„ÄÇËØ•ÊñπÊ≥ïÈôç‰Ωé‰∫ÜÂØπ‰∫∫Â∑•ÊºîÁ§∫Êï∞ÊçÆÁöÑ‰æùËµñÔºå‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÊõ¥È´òÊïà„ÄÅÊõ¥Á®≥ÂÆöÂú∞ÂÆåÊàêÂ§çÊùÇ‰ªªÂä°ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Precise robot manipulation is critical for fine-grained applications such as chemical and biological experiments, where even small errors (e.g., reagent spillage) can invalidate an entire task. Existing approaches often rely on pre-collected expert demonstrations and train policies via imitation learning (IL) or offline reinforcement learning (RL). However, obtaining high-quality demonstrations for precision tasks is difficult and time-consuming, while offline RL commonly suffers from distribution shifts and low data efficiency. We introduce a Role-Model Reinforcement Learning (RM-RL) framework that unifies online and offline training in real-world environments. The key idea is a role-model strategy that automatically generates labels for online training data using approximately optimal actions, eliminating the need for human demonstrations. RM-RL reformulates policy learning as supervised training, reducing instability from distribution mismatch and improving efficiency. A hybrid training scheme further leverages online role-model data for offline reuse, enhancing data efficiency through repeated sampling. Extensive experiments show that RM-RL converges faster and more stably than existing RL methods, yielding significant gains in real-world manipulation: 53% improvement in translation accuracy and 20% in rotation accuracy. Finally, we demonstrate the successful execution of a challenging task, precisely placing a cell plate onto a shelf, highlighting the framework's effectiveness where prior methods fail.

