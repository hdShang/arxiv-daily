---
layout: default
title: RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation
---

# RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15189" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.15189v1</a>
  <a href="https://arxiv.org/pdf/2510.15189.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15189v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15189v1', 'RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiangyu Chen, Chuhao Zhou, Yuxi Liu, Jianfei Yang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RM-RLï¼šé¢å‘ç²¾å‡†æœºå™¨äººæ“ä½œçš„è§’è‰²æ¨¡å‹å¼ºåŒ–å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `è§’è‰²æ¨¡å‹` `åœ¨çº¿å­¦ä¹ ` `ç¦»çº¿å­¦ä¹ ` `ç²¾å‡†æ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•ä¾èµ–ä¸“å®¶æ¼”ç¤ºæˆ–ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œé¢ä¸´é«˜è´¨é‡æ•°æ®è·å–éš¾ã€åˆ†å¸ƒåç§»ç­‰æŒ‘æˆ˜ã€‚
2. RM-RLæ¡†æ¶é€šè¿‡è§’è‰²æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆåœ¨çº¿è®­ç»ƒæ ‡ç­¾ï¼Œå°†å¼ºåŒ–å­¦ä¹ è½¬åŒ–ä¸ºç›‘ç£å­¦ä¹ ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRM-RLåœ¨çœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å¹³ç§»å’Œæ—‹è½¬ç²¾åº¦ï¼Œå¹¶æˆåŠŸå®Œæˆå¤æ‚ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç²¾å‡†çš„æœºå™¨äººæ“ä½œå¯¹äºç²¾ç»†åº”ç”¨è‡³å…³é‡è¦ï¼Œä¾‹å¦‚åŒ–å­¦å’Œç”Ÿç‰©å®éªŒï¼Œå³ä½¿å¾®å°çš„è¯¯å·®ï¼ˆä¾‹å¦‚ï¼Œè¯•å‰‚æº¢å‡ºï¼‰ä¹Ÿå¯èƒ½ä½¿æ•´ä¸ªä»»åŠ¡æ— æ•ˆã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å…ˆæ”¶é›†çš„ä¸“å®¶æ¼”ç¤ºï¼Œå¹¶é€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æˆ–ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒç­–ç•¥ã€‚ç„¶è€Œï¼Œè·å¾—ç”¨äºç²¾ç¡®ä»»åŠ¡çš„é«˜è´¨é‡æ¼”ç¤ºæ—¢å›°éš¾åˆè€—æ—¶ï¼Œè€Œç¦»çº¿RLé€šå¸¸ä¼šå—åˆ°åˆ†å¸ƒåç§»å’Œä½æ•°æ®æ•ˆç‡çš„å½±å“ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§’è‰²æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRM-RLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»Ÿä¸€äº†çœŸå®ç¯å¢ƒä¸­çš„åœ¨çº¿å’Œç¦»çº¿è®­ç»ƒã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä¸€ç§è§’è‰²æ¨¡å‹ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨è¿‘ä¼¼æœ€ä¼˜åŠ¨ä½œè‡ªåŠ¨ç”Ÿæˆåœ¨çº¿è®­ç»ƒæ•°æ®çš„æ ‡ç­¾ï¼Œä»è€Œæ— éœ€äººå·¥æ¼”ç¤ºã€‚RM-RLå°†ç­–ç•¥å­¦ä¹ é‡æ–°å®šä¹‰ä¸ºç›‘ç£è®­ç»ƒï¼Œä»è€Œå‡å°‘äº†åˆ†å¸ƒä¸åŒ¹é…å¸¦æ¥çš„ä¸ç¨³å®šæ€§å’Œæé«˜äº†æ•ˆç‡ã€‚æ··åˆè®­ç»ƒæ–¹æ¡ˆè¿›ä¸€æ­¥åˆ©ç”¨åœ¨çº¿è§’è‰²æ¨¡å‹æ•°æ®è¿›è¡Œç¦»çº¿é‡ç”¨ï¼Œé€šè¿‡é‡å¤é‡‡æ ·æ¥æé«˜æ•°æ®æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRM-RLæ¯”ç°æœ‰çš„RLæ–¹æ³•æ”¶æ•›æ›´å¿«ã€æ›´ç¨³å®šï¼Œå¹¶åœ¨çœŸå®æ“ä½œä¸­äº§ç”Ÿäº†æ˜¾è‘—çš„æ”¶ç›Šï¼šå¹³ç§»ç²¾åº¦æé«˜äº†53%ï¼Œæ—‹è½¬ç²¾åº¦æé«˜äº†20%ã€‚æœ€åï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†æˆåŠŸæ‰§è¡Œä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå³å°†ç»†èƒåŸ¹å…»æ¿ç²¾ç¡®åœ°æ”¾ç½®åœ¨æ¶å­ä¸Šï¼Œçªå‡ºäº†è¯¥æ¡†æ¶åœ¨å…ˆå‰æ–¹æ³•å¤±è´¥æ—¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººç²¾å‡†æ“ä½œé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜ç²¾åº¦çš„åœºæ™¯ä¸‹ï¼Œä¾‹å¦‚ç”Ÿç‰©å®éªŒã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚æ¨¡ä»¿å­¦ä¹ å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä¾èµ–äºé«˜è´¨é‡çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼Œè€Œè·å–è¿™äº›æ•°æ®éå¸¸å›°éš¾ä¸”è€—æ—¶ã€‚æ­¤å¤–ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ è¿˜å®¹æ˜“å—åˆ°åˆ†å¸ƒåç§»çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªâ€œè§’è‰²æ¨¡å‹â€æ¥è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®çš„æ ‡ç­¾ï¼Œä»è€Œé¿å…å¯¹äººå·¥æ¼”ç¤ºæ•°æ®çš„ä¾èµ–ã€‚è§’è‰²æ¨¡å‹é€šè¿‡è¿‘ä¼¼æœ€ä¼˜çš„åŠ¨ä½œæ¥æŒ‡å¯¼åœ¨çº¿è®­ç»ƒï¼Œå°†å¼ºåŒ–å­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRM-RLæ¡†æ¶åŒ…å«åœ¨çº¿å’Œç¦»çº¿è®­ç»ƒä¸¤ä¸ªé˜¶æ®µã€‚åœ¨çº¿é˜¶æ®µï¼Œè§’è‰²æ¨¡å‹æ ¹æ®å½“å‰çŠ¶æ€ç”ŸæˆåŠ¨ä½œï¼Œå¹¶å°†å…¶ä½œä¸ºæ ‡ç­¾ç”¨äºè®­ç»ƒç­–ç•¥ç½‘ç»œã€‚ç¦»çº¿é˜¶æ®µï¼Œåˆ©ç”¨åœ¨çº¿ç”Ÿæˆçš„æ•°æ®è¿›è¡Œé‡é‡‡æ ·å’Œè®­ç»ƒï¼Œè¿›ä¸€æ­¥æé«˜æ•°æ®åˆ©ç”¨ç‡ã€‚æ•´ä½“æ¡†æ¶é‡‡ç”¨æ··åˆè®­ç»ƒæ–¹æ¡ˆï¼Œäº¤æ›¿è¿›è¡Œåœ¨çº¿æ¢ç´¢å’Œç¦»çº¿ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šRM-RLçš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†è§’è‰²æ¨¡å‹æ¥è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ ‡ç­¾ï¼Œä»è€Œæ‘†è„±äº†å¯¹äººå·¥æ¼”ç¤ºæ•°æ®çš„ä¾èµ–ã€‚è¿™ç§æ–¹æ³•å°†å¼ºåŒ–å­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œé¿å…äº†åˆ†å¸ƒåç§»å¸¦æ¥çš„ä¸ç¨³å®šæ€§å’Œä½æ•ˆé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šè§’è‰²æ¨¡å‹çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦èƒ½å¤Ÿç”Ÿæˆè¿‘ä¼¼æœ€ä¼˜çš„åŠ¨ä½œã€‚å…·ä½“çš„å®ç°æ–¹å¼æœªçŸ¥ï¼Œä½†æ¨æµ‹å¯èƒ½é‡‡ç”¨æŸç§å½¢å¼çš„è§„åˆ’ç®—æ³•æˆ–é¢„è®­ç»ƒæ¨¡å‹ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨ç›‘ç£å­¦ä¹ å¸¸ç”¨çš„äº¤å‰ç†µæŸå¤±æˆ–å‡æ–¹è¯¯å·®æŸå¤±ã€‚ç½‘ç»œç»“æ„å¯èƒ½é‡‡ç”¨å¸¸è§çš„æ·±åº¦ç¥ç»ç½‘ç»œç»“æ„ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œæˆ–å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå…·ä½“å–å†³äºä»»åŠ¡çš„ç‰¹ç‚¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRM-RLåœ¨çœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒRM-RLçš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œè®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šã€‚åœ¨å¹³ç§»ç²¾åº¦æ–¹é¢ï¼ŒRM-RLæå‡äº†53%ï¼Œåœ¨æ—‹è½¬ç²¾åº¦æ–¹é¢æå‡äº†20%ã€‚æ­¤å¤–ï¼ŒRM-RLè¿˜æˆåŠŸå®Œæˆäº†ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå³å°†ç»†èƒåŸ¹å…»æ¿ç²¾ç¡®åœ°æ”¾ç½®åœ¨æ¶å­ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RM-RLæ¡†æ¶å¯åº”ç”¨äºå„ç§éœ€è¦é«˜ç²¾åº¦æœºå™¨äººæ“ä½œçš„é¢†åŸŸï¼Œä¾‹å¦‚ï¼šåŒ–å­¦å’Œç”Ÿç‰©å®éªŒä¸­çš„è¯•å‰‚é…æ¯”ã€åŒ»ç–—æ‰‹æœ¯ä¸­çš„ç²¾å‡†å®šä½ã€ä»¥åŠç²¾å¯†åˆ¶é€ ä¸­çš„é›¶ä»¶ç»„è£…ã€‚è¯¥æ–¹æ³•é™ä½äº†å¯¹äººå·¥æ¼”ç¤ºæ•°æ®çš„ä¾èµ–ï¼Œä½¿å¾—æœºå™¨äººèƒ½å¤Ÿæ›´é«˜æ•ˆã€æ›´ç¨³å®šåœ°å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Precise robot manipulation is critical for fine-grained applications such as chemical and biological experiments, where even small errors (e.g., reagent spillage) can invalidate an entire task. Existing approaches often rely on pre-collected expert demonstrations and train policies via imitation learning (IL) or offline reinforcement learning (RL). However, obtaining high-quality demonstrations for precision tasks is difficult and time-consuming, while offline RL commonly suffers from distribution shifts and low data efficiency. We introduce a Role-Model Reinforcement Learning (RM-RL) framework that unifies online and offline training in real-world environments. The key idea is a role-model strategy that automatically generates labels for online training data using approximately optimal actions, eliminating the need for human demonstrations. RM-RL reformulates policy learning as supervised training, reducing instability from distribution mismatch and improving efficiency. A hybrid training scheme further leverages online role-model data for offline reuse, enhancing data efficiency through repeated sampling. Extensive experiments show that RM-RL converges faster and more stably than existing RL methods, yielding significant gains in real-world manipulation: 53% improvement in translation accuracy and 20% in rotation accuracy. Finally, we demonstrate the successful execution of a challenging task, precisely placing a cell plate onto a shelf, highlighting the framework's effectiveness where prior methods fail.

