---
layout: default
title: GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement
---

# GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14627" target="_blank" class="toolbar-btn">arXiv: 2510.14627v2</a>
    <a href="https://arxiv.org/pdf/2510.14627.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14627v2" 
            onclick="toggleFavorite(this, '2510.14627v2', 'GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yao Zhong, Hanzhi Chen, Simon Schaefer, Anran Zhang, Stefan Leutenegger

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-16 (Êõ¥Êñ∞: 2025-10-25)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**GOPLAÔºöÈÄöËøáÂêàÊàêÂ¢ûÂº∫‰∫∫Á±ªÂ∏ÉÁΩÆÊï∞ÊçÆÔºåÂ≠¶‰π†ÂèØÊ≥õÂåñÁöÑÁâ©‰ΩìÊîæÁΩÆ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Áâ©‰ΩìÊîæÁΩÆ` `Êú∫Âô®‰∫∫` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Êâ©Êï£Ê®°Âûã` `ÂêàÊàêÊï∞ÊçÆÂ¢ûÂº∫` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÂèØ‰æõÊÄßÂõæ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Êú∫Âô®‰∫∫ËæÖÂä©Êó•Â∏∏ÂÆ∂Â±ÖÊï¥ÁêÜÈù¢‰∏¥Áâ©‰ΩìÊîæÁΩÆÈöæÈ¢òÔºåÈúÄË¶ÅÊé®ÁêÜËØ≠‰πâÂÅèÂ•ΩÂíåÂá†‰ΩïÂèØË°åÊÄß„ÄÇ
2. GOPLAÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂíåÊâ©Êï£Ê®°ÂûãÔºå‰ªéÂ¢ûÂº∫ÁöÑ‰∫∫Á±ªÊºîÁ§∫‰∏≠Â≠¶‰π†Áâ©‰ΩìÊîæÁΩÆÁ≠ñÁï•„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåGOPLAÂú®ÁúüÂÆûÊú∫Âô®‰∫∫Âú∫ÊôØ‰∏≠ÔºåÁâ©‰ΩìÊîæÁΩÆÊàêÂäüÁéáÊòæËëóÊèêÂçáÔºåÊ≥õÂåñËÉΩÂäõÂº∫„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫GOPLAÔºå‰∏Ä‰∏™ÂàÜÂ±ÇÊ°ÜÊû∂ÔºåÈÄöËøáÂ¢ûÂº∫ÁöÑ‰∫∫Á±ªÊºîÁ§∫Â≠¶‰π†ÂèØÊ≥õÂåñÁöÑÁâ©‰ΩìÊîæÁΩÆ„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ∞Ü‰∫∫Á±ªÊåá‰ª§ÂíåËßÜËßâËæìÂÖ•ËΩ¨Âåñ‰∏∫ÁªìÊûÑÂåñÁöÑËßÑÂàíÔºåËøô‰∫õËßÑÂàíÊåáÂÆö‰∫ÜÊàêÂØπÁöÑÁâ©‰ΩìÂÖ≥Á≥ª„ÄÇÁÑ∂ÂêéÔºåÁ©∫Èó¥Êò†Â∞ÑÂô®Â∞ÜËøô‰∫õËßÑÂàíËΩ¨Âåñ‰∏∫ÂÖ∑ÊúâÂá†‰ΩïÂ∏∏ËØÜÁöÑ3DÂèØ‰æõÊÄßÂõæ„ÄÇÂü∫‰∫éÊâ©Êï£ÁöÑËßÑÂàíÂô®ÁîüÊàêÊîæÁΩÆÂßøÊÄÅÔºåÂπ∂ËÄÉËôëÊµãËØïÊó∂ÁöÑ‰ª£‰ª∑„ÄÅÂ§öËßÑÂàíÂàÜÂ∏ÉÂíåÈÅøÁ¢∞„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÊï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢òÔºåÊú¨ÊñáÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÊµÅÁ®ãÔºåÂ∞Ü‰∫∫Á±ªÊîæÁΩÆÊºîÁ§∫Êâ©Â±ï‰∏∫Â§öÊ†∑ÂåñÁöÑÂêàÊàêËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÂú®ÂÆö‰ΩçÁ≤æÂ∫¶ÂíåÁâ©ÁêÜÂêàÁêÜÊÄßÊñπÈù¢ÔºåGOPLAÁöÑÊîæÁΩÆÊàêÂäüÁéáÊØîÁ¨¨‰∫åÂêçÊèêÈ´ò‰∫Ü30.04‰∏™ÁôæÂàÜÁÇπÔºåÂ±ïÁ§∫‰∫ÜÂú®ÂêÑÁßçÁúüÂÆûÊú∫Âô®‰∫∫ÊîæÁΩÆÂú∫ÊôØ‰∏≠ÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁâ©‰ΩìÊîæÁΩÆ‰ªªÂä°ÈúÄË¶ÅÊú∫Âô®‰∫∫ÁêÜËß£‰∫∫Á±ªÊåá‰ª§ÔºåÂπ∂Ê†πÊçÆËØ≠‰πâÂÖ≥Á≥ªÂíåÂá†‰ΩïÁ∫¶ÊùüÔºåÂ∞ÜÁâ©‰ΩìÊîæÁΩÆÂú®ÂêàÈÄÇÁöÑ‰ΩçÁΩÆ„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈöæ‰ª•Â∫îÂØπÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂ§çÊùÇÊÉÖÂÜµ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöGOPLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁêÜËß£‰∫∫Á±ªÊåá‰ª§ÔºåÁîüÊàêÁªìÊûÑÂåñÁöÑÁâ©‰ΩìÂÖ≥Á≥ªËßÑÂàíÔºåÂπ∂ÁªìÂêàÊâ©Êï£Ê®°ÂûãÁîüÊàêÁ¨¶ÂêàÂá†‰ΩïÁ∫¶ÊùüÁöÑÊîæÁΩÆÂßøÊÄÅ„ÄÇÈÄöËøáÂêàÊàêÊï∞ÊçÆÂ¢ûÂº∫ÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGOPLAÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºöÂ∞Ü‰∫∫Á±ªÊåá‰ª§ÂíåËßÜËßâËæìÂÖ•ËΩ¨Âåñ‰∏∫ÁªìÊûÑÂåñÁöÑËßÑÂàíÔºåÊèèËø∞Áâ©‰Ωì‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ2) Á©∫Èó¥Êò†Â∞ÑÂô®ÔºöÂ∞ÜËßÑÂàíËΩ¨Âåñ‰∏∫3DÂèØ‰æõÊÄßÂõæÔºåËµã‰∫àÂá†‰ΩïÂ∏∏ËØÜ„ÄÇ3) Âü∫‰∫éÊâ©Êï£ÁöÑËßÑÂàíÂô®ÔºöÁîüÊàêÊîæÁΩÆÂßøÊÄÅÔºåËÄÉËôëÊµãËØïÊó∂ÁöÑ‰ª£‰ª∑„ÄÅÂ§öËßÑÂàíÂàÜÂ∏ÉÂíåÈÅøÁ¢∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGOPLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàÜÂ±ÇÊ°ÜÊû∂ÔºåÂ∞ÜËØ≠‰πâÁêÜËß£ÂíåÂá†‰ΩïÊé®ÁêÜÁõ∏ÁªìÂêà„ÄÇ2) Âà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°åËØ≠‰πâÁêÜËß£ÔºåÁîüÊàêÁªìÊûÑÂåñÁöÑËßÑÂàí„ÄÇ3) ÂºïÂÖ•‰∫ÜÂü∫‰∫éÊâ©Êï£ÁöÑËßÑÂàíÂô®ÔºåÁîüÊàêÁ¨¶ÂêàÂá†‰ΩïÁ∫¶ÊùüÁöÑÊîæÁΩÆÂßøÊÄÅ„ÄÇ4) ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÂêàÊàêÊï∞ÊçÆÁîüÊàêÊµÅÁ®ãÔºåÂÖãÊúç‰∫ÜÊï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢ò„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÈááÁî®È¢ÑËÆ≠ÁªÉÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÂπ∂ÁªìÂêàËßÜËßâÁºñÁ†ÅÂô®ËøõË°åÂæÆË∞É„ÄÇÁ©∫Èó¥Êò†Â∞ÑÂô®‰ΩøÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂ≠¶‰π†3DÂèØ‰æõÊÄßÂõæ„ÄÇÂü∫‰∫éÊâ©Êï£ÁöÑËßÑÂàíÂô®‰ΩøÁî®U-NetÁªìÊûÑÔºå‰ª•ÊµãËØïÊó∂ÁöÑ‰ª£‰ª∑‰Ωú‰∏∫Êù°‰ª∂ÔºåÁîüÊàêÊîæÁΩÆÂßøÊÄÅ„ÄÇÂêàÊàêÊï∞ÊçÆÁîüÊàêÊµÅÁ®ãÈÄöËøáÈöèÊú∫ÊîπÂèòÁâ©‰ΩìÁöÑ‰ΩçÁΩÆ„ÄÅÂßøÊÄÅÂíåÁéØÂ¢ÉÔºåÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGOPLAÂú®ÁúüÂÆûÊú∫Âô®‰∫∫ÊîæÁΩÆÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊîæÁΩÆÊàêÂäüÁéáÊØîÁ¨¨‰∫åÂêçÊèêÈ´ò‰∫Ü30.04‰∏™ÁôæÂàÜÁÇπ„ÄÇËøôËØÅÊòé‰∫ÜGOPLAÂú®ÂÆö‰ΩçÁ≤æÂ∫¶ÂíåÁâ©ÁêÜÂêàÁêÜÊÄßÊñπÈù¢ÁöÑ‰ºòÂäøÔºå‰ª•ÂèäÂú®ÂêÑÁßçÁúüÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

GOPLAÂèØÂ∫îÁî®‰∫éÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅ‰ªìÂ∫ìËá™Âä®Âåñ„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫ÁêÜËß£‰∫∫Á±ªÊåá‰ª§ÔºåÂÆåÊàêÁâ©‰ΩìÊîæÁΩÆ‰ªªÂä°ÔºåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇÊú™Êù•ÔºåGOPLAÂèØ‰ª•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÂÆ∂ÂÖ∑ÁªÑË£Ö„ÄÅÁéØÂ¢ÉÂ∏ÉÁΩÆÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Robots are expected to serve as intelligent assistants, helping humans with everyday household organization. A central challenge in this setting is the task of object placement, which requires reasoning about both semantic preferences (e.g., common-sense object relations) and geometric feasibility (e.g., collision avoidance). We present GOPLA, a hierarchical framework that learns generalizable object placement from augmented human demonstrations. A multi-modal large language model translates human instructions and visual inputs into structured plans that specify pairwise object relationships. These plans are then converted into 3D affordance maps with geometric common sense by a spatial mapper, while a diffusion-based planner generates placement poses guided by test-time costs, considering multi-plan distributions and collision avoidance. To overcome data scarcity, we introduce a scalable pipeline that expands human placement demonstrations into diverse synthetic training data. Extensive experiments show that our approach improves placement success rates by 30.04 percentage points over the runner-up, evaluated on positioning accuracy and physical plausibility, demonstrating strong generalization across a wide range of real-world robotic placement scenarios.

