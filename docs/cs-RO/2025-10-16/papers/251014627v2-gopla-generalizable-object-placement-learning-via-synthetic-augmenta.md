---
layout: default
title: "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement"
---

# GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14627" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14627v2</a>
  <a href="https://arxiv.org/pdf/2510.14627.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14627v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14627v2', 'GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yao Zhong, Hanzhi Chen, Simon Schaefer, Anran Zhang, Stefan Leutenegger

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16 (æ›´æ–°: 2025-10-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GOPLAï¼šé€šè¿‡åˆæˆå¢å¼ºäººç±»å¸ƒç½®æ•°æ®ï¼Œå­¦ä¹ å¯æ³›åŒ–çš„ç‰©ä½“æ”¾ç½®**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç‰©ä½“æ”¾ç½®` `æœºå™¨äºº` `å¤šæ¨¡æ€å­¦ä¹ ` `æ‰©æ•£æ¨¡å‹` `åˆæˆæ•°æ®å¢å¼º` `å¤§è¯­è¨€æ¨¡å‹` `å¯ä¾›æ€§å›¾`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æœºå™¨äººè¾…åŠ©æ—¥å¸¸å®¶å±…æ•´ç†é¢ä¸´ç‰©ä½“æ”¾ç½®éš¾é¢˜ï¼Œéœ€è¦æ¨ç†è¯­ä¹‰åå¥½å’Œå‡ ä½•å¯è¡Œæ€§ã€‚
2. GOPLAåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œä»å¢å¼ºçš„äººç±»æ¼”ç¤ºä¸­å­¦ä¹ ç‰©ä½“æ”¾ç½®ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGOPLAåœ¨çœŸå®æœºå™¨äººåœºæ™¯ä¸­ï¼Œç‰©ä½“æ”¾ç½®æˆåŠŸç‡æ˜¾è‘—æå‡ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºGOPLAï¼Œä¸€ä¸ªåˆ†å±‚æ¡†æ¶ï¼Œé€šè¿‡å¢å¼ºçš„äººç±»æ¼”ç¤ºå­¦ä¹ å¯æ³›åŒ–çš„ç‰©ä½“æ”¾ç½®ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å°†äººç±»æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è§„åˆ’ï¼Œè¿™äº›è§„åˆ’æŒ‡å®šäº†æˆå¯¹çš„ç‰©ä½“å…³ç³»ã€‚ç„¶åï¼Œç©ºé—´æ˜ å°„å™¨å°†è¿™äº›è§„åˆ’è½¬åŒ–ä¸ºå…·æœ‰å‡ ä½•å¸¸è¯†çš„3Då¯ä¾›æ€§å›¾ã€‚åŸºäºæ‰©æ•£çš„è§„åˆ’å™¨ç”Ÿæˆæ”¾ç½®å§¿æ€ï¼Œå¹¶è€ƒè™‘æµ‹è¯•æ—¶çš„ä»£ä»·ã€å¤šè§„åˆ’åˆ†å¸ƒå’Œé¿ç¢°ã€‚ä¸ºäº†å…‹æœæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªå¯æ‰©å±•çš„æµç¨‹ï¼Œå°†äººç±»æ”¾ç½®æ¼”ç¤ºæ‰©å±•ä¸ºå¤šæ ·åŒ–çš„åˆæˆè®­ç»ƒæ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å®šä½ç²¾åº¦å’Œç‰©ç†åˆç†æ€§æ–¹é¢ï¼ŒGOPLAçš„æ”¾ç½®æˆåŠŸç‡æ¯”ç¬¬äºŒåæé«˜äº†30.04ä¸ªç™¾åˆ†ç‚¹ï¼Œå±•ç¤ºäº†åœ¨å„ç§çœŸå®æœºå™¨äººæ”¾ç½®åœºæ™¯ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç‰©ä½“æ”¾ç½®ä»»åŠ¡éœ€è¦æœºå™¨äººç†è§£äººç±»æŒ‡ä»¤ï¼Œå¹¶æ ¹æ®è¯­ä¹‰å…³ç³»å’Œå‡ ä½•çº¦æŸï¼Œå°†ç‰©ä½“æ”¾ç½®åœ¨åˆé€‚çš„ä½ç½®ã€‚ç°æœ‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥åº”å¯¹çœŸå®åœºæ™¯ä¸­çš„å¤æ‚æƒ…å†µã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGOPLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç†è§£äººç±»æŒ‡ä»¤ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ç‰©ä½“å…³ç³»è§„åˆ’ï¼Œå¹¶ç»“åˆæ‰©æ•£æ¨¡å‹ç”Ÿæˆç¬¦åˆå‡ ä½•çº¦æŸçš„æ”¾ç½®å§¿æ€ã€‚é€šè¿‡åˆæˆæ•°æ®å¢å¼ºï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGOPLAæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼šå°†äººç±»æŒ‡ä»¤å’Œè§†è§‰è¾“å…¥è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è§„åˆ’ï¼Œæè¿°ç‰©ä½“ä¹‹é—´çš„å…³ç³»ã€‚2) ç©ºé—´æ˜ å°„å™¨ï¼šå°†è§„åˆ’è½¬åŒ–ä¸º3Då¯ä¾›æ€§å›¾ï¼Œèµ‹äºˆå‡ ä½•å¸¸è¯†ã€‚3) åŸºäºæ‰©æ•£çš„è§„åˆ’å™¨ï¼šç”Ÿæˆæ”¾ç½®å§¿æ€ï¼Œè€ƒè™‘æµ‹è¯•æ—¶çš„ä»£ä»·ã€å¤šè§„åˆ’åˆ†å¸ƒå’Œé¿ç¢°ã€‚

**å…³é”®åˆ›æ–°**ï¼šGOPLAçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ç§åˆ†å±‚æ¡†æ¶ï¼Œå°†è¯­ä¹‰ç†è§£å’Œå‡ ä½•æ¨ç†ç›¸ç»“åˆã€‚2) åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç†è§£ï¼Œç”Ÿæˆç»“æ„åŒ–çš„è§„åˆ’ã€‚3) å¼•å…¥äº†åŸºäºæ‰©æ•£çš„è§„åˆ’å™¨ï¼Œç”Ÿæˆç¬¦åˆå‡ ä½•çº¦æŸçš„æ”¾ç½®å§¿æ€ã€‚4) æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹ï¼Œå…‹æœäº†æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é‡‡ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå¹¶ç»“åˆè§†è§‰ç¼–ç å™¨è¿›è¡Œå¾®è°ƒã€‚ç©ºé—´æ˜ å°„å™¨ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå­¦ä¹ 3Då¯ä¾›æ€§å›¾ã€‚åŸºäºæ‰©æ•£çš„è§„åˆ’å™¨ä½¿ç”¨U-Netç»“æ„ï¼Œä»¥æµ‹è¯•æ—¶çš„ä»£ä»·ä½œä¸ºæ¡ä»¶ï¼Œç”Ÿæˆæ”¾ç½®å§¿æ€ã€‚åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹é€šè¿‡éšæœºæ”¹å˜ç‰©ä½“çš„ä½ç½®ã€å§¿æ€å’Œç¯å¢ƒï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒGOPLAåœ¨çœŸå®æœºå™¨äººæ”¾ç½®åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ”¾ç½®æˆåŠŸç‡æ¯”ç¬¬äºŒåæé«˜äº†30.04ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™è¯æ˜äº†GOPLAåœ¨å®šä½ç²¾åº¦å’Œç‰©ç†åˆç†æ€§æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä»¥åŠåœ¨å„ç§çœŸå®åœºæ™¯ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GOPLAå¯åº”ç”¨äºå®¶åº­æœåŠ¡æœºå™¨äººã€ä»“åº“è‡ªåŠ¨åŒ–ã€æ™ºèƒ½åˆ¶é€ ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£äººç±»æŒ‡ä»¤ï¼Œå®Œæˆç‰©ä½“æ”¾ç½®ä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼ŒGOPLAå¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ï¼Œä¾‹å¦‚å®¶å…·ç»„è£…ã€ç¯å¢ƒå¸ƒç½®ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robots are expected to serve as intelligent assistants, helping humans with everyday household organization. A central challenge in this setting is the task of object placement, which requires reasoning about both semantic preferences (e.g., common-sense object relations) and geometric feasibility (e.g., collision avoidance). We present GOPLA, a hierarchical framework that learns generalizable object placement from augmented human demonstrations. A multi-modal large language model translates human instructions and visual inputs into structured plans that specify pairwise object relationships. These plans are then converted into 3D affordance maps with geometric common sense by a spatial mapper, while a diffusion-based planner generates placement poses guided by test-time costs, considering multi-plan distributions and collision avoidance. To overcome data scarcity, we introduce a scalable pipeline that expands human placement demonstrations into diverse synthetic training data. Extensive experiments show that our approach improves placement success rates by 30.04 percentage points over the runner-up, evaluated on positioning accuracy and physical plausibility, demonstrating strong generalization across a wide range of real-world robotic placement scenarios.

