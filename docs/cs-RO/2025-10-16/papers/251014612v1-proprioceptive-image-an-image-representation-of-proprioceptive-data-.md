---
layout: default
title: Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning
---

# Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14612" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14612v1</a>
  <a href="https://arxiv.org/pdf/2510.14612.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14612v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14612v1', 'Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gabriel Fischer Abati, JoÃ£o Carlos Virgolino Soares, Giulio Turrisi, Victor Barasuol, Claudio Semini

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-16

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºæœ¬ä½“æ„Ÿå—å›¾åƒçš„å››è¶³æœºå™¨äººæ¥è§¦ä¼°è®¡å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å››è¶³æœºå™¨äºº` `æœ¬ä½“æ„Ÿå—` `æ¥è§¦ä¼°è®¡` `å·ç§¯ç¥ç»ç½‘ç»œ` `å›¾åƒè¡¨ç¤º`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å››è¶³æœºå™¨äººçš„æœ¬ä½“æ„Ÿå—æ•°æ®è¿›è¡Œæ¥è§¦ä¼°è®¡ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åœ°å½¢ä¸‹çš„è¿åŠ¨èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºå°†æœ¬ä½“æ„Ÿå—æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºäºŒç»´å›¾åƒï¼Œä¿ç•™æœºå™¨äººå½¢æ€ç»“æ„å’Œä¿¡å·é—´ç›¸å…³æ€§ï¼Œä¾¿äºå·ç§¯ç¥ç»ç½‘ç»œå­¦ä¹ ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¥è§¦ä¼°è®¡ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†é¢„æµ‹ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºä¼ ç»Ÿåºåˆ—æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå°†å››è¶³æœºå™¨äººçš„æœ¬ä½“æ„Ÿå—æ—¶é—´åºåˆ—æ•°æ®è¡¨ç¤ºä¸ºç»“æ„åŒ–çš„äºŒç»´å›¾åƒï¼Œä»è€Œèƒ½å¤Ÿä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå­¦ä¹ ä¸è¿åŠ¨ç›¸å…³çš„ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ç¼–ç äº†æ¥è‡ªå¤šä¸ªæœ¬ä½“æ„Ÿå—ä¿¡å·ï¼ˆå¦‚å…³èŠ‚ä½ç½®ã€IMUè¯»æ•°å’Œè¶³ç«¯é€Ÿåº¦ï¼‰çš„æ—¶é—´åŠ¨æ€ï¼ŒåŒæ—¶åœ¨å›¾åƒçš„ç©ºé—´æ’åˆ—ä¸­ä¿ç•™äº†æœºå™¨äººçš„å½¢æ€ç»“æ„ã€‚è¿™ç§è½¬æ¢æ•è·äº†ä¿¡å·é—´çš„ç›¸å…³æ€§å’Œä¾èµ–äºæ­¥æ€çš„æ¨¡å¼ï¼Œæä¾›äº†æ¯”ç›´æ¥æ—¶é—´åºåˆ—å¤„ç†æ›´ä¸°å¯Œçš„ç‰¹å¾ç©ºé—´ã€‚æˆ‘ä»¬å°†æ­¤æ¦‚å¿µåº”ç”¨äºæ¥è§¦ä¼°è®¡é—®é¢˜ï¼Œè¿™æ˜¯åœ¨ä¸åŒåœ°å½¢ä¸Šå®ç°ç¨³å®šå’Œè‡ªé€‚åº”è¿åŠ¨çš„å…³é”®èƒ½åŠ›ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†å’Œæ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºåºåˆ—çš„æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åŸºäºå›¾åƒçš„è¡¨ç¤ºæ–¹æ³•å§‹ç»ˆèƒ½å¤Ÿæé«˜é¢„æµ‹ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ï¼Œçªå‡ºäº†è·¨æ¨¡æ€ç¼–ç ç­–ç•¥åœ¨æœºå™¨äººçŠ¶æ€å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¥è§¦æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½¿ç”¨çŸ­15å€çš„çª—å£å¤§å°ï¼Œå°†æ¥è§¦çŠ¶æ€ç²¾åº¦ä»æœ€è¿‘æå‡ºçš„MI-HGNNæ–¹æ³•çš„87.7%æé«˜åˆ°94.5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå››è¶³æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸Šçš„ç¨³å®šè¿åŠ¨ä¾èµ–äºå‡†ç¡®çš„æ¥è§¦ä¼°è®¡ã€‚ç„¶è€Œï¼Œç›´æ¥å¤„ç†æœ¬ä½“æ„Ÿå—æ—¶é—´åºåˆ—æ•°æ®ï¼ˆå¦‚å…³èŠ‚è§’åº¦ã€IMUè¯»æ•°ç­‰ï¼‰å­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥æœ‰æ•ˆæå–è¿åŠ¨æ¨¡å¼å’Œä¿¡å·é—´çš„ç›¸å…³æ€§ï¼Œå¯¼è‡´æ¥è§¦ä¼°è®¡ç²¾åº¦ä¸é«˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„åºåˆ—æ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æœ¬ä½“æ„Ÿå—æ•°æ®ç¼–ç ä¸ºäºŒç»´å›¾åƒï¼Œç§°ä¸ºâ€œæœ¬ä½“æ„Ÿå—å›¾åƒâ€ã€‚å›¾åƒçš„åƒç´ ä½ç½®å¯¹åº”äºæœºå™¨äººèº«ä½“çš„ä¸åŒéƒ¨ä½ï¼Œåƒç´ å€¼åˆ™ç¼–ç äº†è¯¥éƒ¨ä½çš„æœ¬ä½“æ„Ÿå—ä¿¡æ¯ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿä¿ç•™æœºå™¨äººçš„å½¢æ€ç»“æ„ï¼Œå¹¶åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œæå–ç©ºé—´ç›¸å…³æ€§å’Œæ—¶é—´åŠ¨æ€ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) æ•°æ®é‡‡é›†ï¼šä»å››è¶³æœºå™¨äººè·å–æœ¬ä½“æ„Ÿå—æ•°æ®ï¼ŒåŒ…æ‹¬å…³èŠ‚ä½ç½®ã€IMUè¯»æ•°ã€è¶³ç«¯é€Ÿåº¦ç­‰ã€‚2) æ•°æ®é¢„å¤„ç†ï¼šå¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å’ŒåŒæ­¥å¤„ç†ã€‚3) å›¾åƒç¼–ç ï¼šå°†é¢„å¤„ç†åçš„æ•°æ®æ˜ å°„åˆ°äºŒç»´å›¾åƒä¸Šï¼Œæ¯ä¸ªåƒç´ ä»£è¡¨æœºå™¨äººèº«ä½“çš„ä¸€éƒ¨åˆ†ï¼Œåƒç´ å€¼ç¼–ç äº†ç›¸åº”çš„æœ¬ä½“æ„Ÿå—ä¿¡æ¯ã€‚4) æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå¯¹æœ¬ä½“æ„Ÿå—å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ æ¥è§¦çŠ¶æ€çš„é¢„æµ‹æ¨¡å‹ã€‚5) æ¥è§¦ä¼°è®¡ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°çš„æœ¬ä½“æ„Ÿå—å›¾åƒè¿›è¡Œæ¥è§¦çŠ¶æ€é¢„æµ‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†æœ¬ä½“æ„Ÿå—æ•°æ®è¡¨ç¤ºä¸ºå›¾åƒã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œçš„å¼ºå¤§ç‰¹å¾æå–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†æœºå™¨äººçš„å½¢æ€ç»“æ„å’Œä¿¡å·é—´çš„ç›¸å…³æ€§ã€‚ä¸ä¼ ç»Ÿçš„åºåˆ—æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ è¿åŠ¨æ¨¡å¼ï¼Œæé«˜æ¥è§¦ä¼°è®¡çš„ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šå›¾åƒç¼–ç æ–¹å¼æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ï¼Œè®ºæ–‡ä¸­å…·ä½“å¦‚ä½•å°†ä¸åŒç±»å‹çš„æœ¬ä½“æ„Ÿå—æ•°æ®æ˜ å°„åˆ°åƒç´ å€¼ä¸Šï¼Œä»¥åŠå¦‚ä½•é€‰æ‹©åˆé€‚çš„å›¾åƒå°ºå¯¸å’Œåƒç´ æ’åˆ—æ–¹å¼ï¼Œè¿™äº›ç»†èŠ‚å†³å®šäº†å›¾åƒè¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå·ç§¯ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼ˆå¦‚å·ç§¯å±‚æ•°ã€æ»¤æ³¢å™¨å¤§å°ç­‰ï¼‰å’Œè®­ç»ƒå‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€batch sizeç­‰ï¼‰ä¹Ÿéœ€è¦ä»”ç»†è°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³çš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¥è§¦ä¼°è®¡ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å°†æ¥è§¦çŠ¶æ€ç²¾åº¦ä»MI-HGNNæ–¹æ³•çš„87.7%æé«˜åˆ°94.5%ï¼Œå¹¶ä¸”ä½¿ç”¨äº†çŸ­15å€çš„çª—å£å¤§å°ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ¬ä½“æ„Ÿå—æ•°æ®ï¼Œæé«˜æ¥è§¦ä¼°è®¡çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå››è¶³æœºå™¨äººçš„è‡ªä¸»å¯¼èˆªã€åœ°å½¢é€‚åº”å’Œæ•…éšœè¯Šæ–­ç­‰é¢†åŸŸã€‚é€šè¿‡å‡†ç¡®çš„æ¥è§¦ä¼°è®¡ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´ç¨³å®šã€æ›´é«˜æ•ˆçš„è¿åŠ¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººï¼Œä¾‹å¦‚äººå½¢æœºå™¨äººå’Œè½®å¼æœºå™¨äººï¼Œæé«˜å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæœæ•‘ã€å‹˜æ¢å’Œç‰©æµç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents a novel approach for representing proprioceptive time-series data from quadruped robots as structured two-dimensional images, enabling the use of convolutional neural networks for learning locomotion-related tasks. The proposed method encodes temporal dynamics from multiple proprioceptive signals, such as joint positions, IMU readings, and foot velocities, while preserving the robot's morphological structure in the spatial arrangement of the image. This transformation captures inter-signal correlations and gait-dependent patterns, providing a richer feature space than direct time-series processing. We apply this concept in the problem of contact estimation, a key capability for stable and adaptive locomotion on diverse terrains. Experimental evaluations on both real-world datasets and simulated environments show that our image-based representation consistently enhances prediction accuracy and generalization over conventional sequence-based models, underscoring the potential of cross-modal encoding strategies for robotic state learning. Our method achieves superior performance on the contact dataset, improving contact state accuracy from 87.7% to 94.5% over the recently proposed MI-HGNN method, using a 15 times shorter window size.

