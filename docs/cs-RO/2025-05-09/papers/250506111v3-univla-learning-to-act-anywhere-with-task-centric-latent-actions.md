---
layout: default
title: "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions"
---

# UniVLA: Learning to Act Anywhere with Task-centric Latent Actions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06111" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06111v3</a>
  <a href="https://arxiv.org/pdf/2505.06111.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06111v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06111v3', 'UniVLA: Learning to Act Anywhere with Task-centric Latent Actions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, Hongyang Li

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09 (æ›´æ–°: 2025-11-03)

**å¤‡æ³¨**: Accepted to RSS 2025. Code is available at https://github.com/OpenDriveLab/UniVLA

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºUniVLAä»¥è§£å†³æœºå™¨äººè·¨ç¯å¢ƒå­¦ä¹ èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨ä½“ç°å­¦ä¹ ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `æ½œåœ¨åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººæ”¿ç­–å­¦ä¹ ` `å¤šæ¨¡æ€èåˆ` `ä»»åŠ¡ä¸­å¿ƒè¡¨ç¤º` `æ•°æ®é«˜æ•ˆåˆ©ç”¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡åŠ¨ä½œæ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´æœºå™¨äººåœ¨ä¸åŒç¯å¢ƒå’Œä½“ç°ä¸­å­¦ä¹ èƒ½åŠ›å—é™ã€‚
2. UniVLAé€šè¿‡æ½œåœ¨åŠ¨ä½œæ¨¡å‹ä»è§†é¢‘ä¸­æå–ä»»åŠ¡ä¸­å¿ƒçš„åŠ¨ä½œè¡¨ç¤ºï¼Œå¢å¼ºäº†è·¨ç¯å¢ƒçš„å­¦ä¹ èƒ½åŠ›ã€‚
3. åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniVLAçš„æ€§èƒ½è¶…è¿‡OpenVLAï¼Œä¸”é¢„è®­ç»ƒè®¡ç®—é‡å’Œä¸‹æ¸¸æ•°æ®éœ€æ±‚æ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸€èˆ¬åŒ–æœºå™¨äººåº”åœ¨å¤šç§ç¯å¢ƒä¸­æœ‰æ•ˆæ‰§è¡Œä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºæ‰©å±•å¸¦æœ‰åŠ¨ä½œæ ‡æ³¨çš„æ•°æ®ï¼Œå¯¼è‡´å…¶èƒ½åŠ›å—é™äºå•ä¸€ç‰©ç†è§„æ ¼ï¼Œéš¾ä»¥åœ¨ä¸åŒçš„ä½“ç°å’Œç¯å¢ƒä¸­å­¦ä¹ å¯è½¬ç§»çš„çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†UniVLAï¼Œä¸€ä¸ªç”¨äºå­¦ä¹ è·¨ä½“ç°è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥çš„æ–°æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡æ½œåœ¨åŠ¨ä½œæ¨¡å‹ä»è§†é¢‘ä¸­æ¨å¯¼ä»»åŠ¡ä¸­å¿ƒçš„åŠ¨ä½œè¡¨ç¤ºï¼Œä»è€Œåˆ©ç”¨å¹¿æ³›çš„è·¨ä½“ç°å’Œè§†è§’çš„æ•°æ®ã€‚é€šè¿‡å¼•å…¥è¯­è¨€æŒ‡ä»¤å¹¶åœ¨DINOç‰¹å¾ç©ºé—´å†…å»ºç«‹æ½œåœ¨åŠ¨ä½œæ¨¡å‹ï¼ŒUniVLAåœ¨å¤šä¸ªæ“ä½œå’Œå¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººéƒ¨ç½²ä¸­è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•åœ¨ä¸åŒç¯å¢ƒå’Œä½“ç°ä¸­çŸ¥è¯†è¿ç§»èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡çš„åŠ¨ä½œæ ‡æ³¨æ•°æ®ï¼Œå¯¼è‡´å…¶åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„é€‚åº”æ€§å·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ½œåœ¨åŠ¨ä½œæ¨¡å‹ä»è§†é¢‘ä¸­æå–ä»»åŠ¡ä¸­å¿ƒçš„åŠ¨ä½œè¡¨ç¤ºï¼Œç»“åˆè¯­è¨€æŒ‡ä»¤æ¥å‡è½»ä»»åŠ¡æ— å…³åŠ¨æ€çš„å½±å“ï¼Œä»è€Œå®ç°è·¨ä½“ç°çš„å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniVLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ½œåœ¨åŠ¨ä½œæ¨¡å‹çš„è®­ç»ƒã€ç‰¹å¾æå–å’Œç­–ç•¥å­¦ä¹ ç­‰æ¨¡å—ã€‚é€šè¿‡åœ¨DINOç‰¹å¾ç©ºé—´å†…å»ºç«‹æ½œåœ¨åŠ¨ä½œæ¨¡å‹ï¼Œç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆè§£ç åŠ¨ä½œå¹¶åº”ç”¨äºä¸åŒæœºå™¨äººã€‚

**å…³é”®åˆ›æ–°**ï¼šUniVLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä»»åŠ¡ä¸­å¿ƒçš„åŠ¨ä½œè¡¨ç¤ºæå–æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨äº’è”ç½‘è§„æ¨¡çš„è§†é¢‘æ•°æ®ï¼Œå¹¶åœ¨ä¸åŒæœºå™¨äººé—´å®ç°ç­–ç•¥çš„è¿ç§»ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„ä¾èµ–å•ä¸€ç‰©ç†è§„æ ¼çš„ç­–ç•¥å­¦ä¹ æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒUniVLAé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åŠ¨ä½œè¡¨ç¤ºçš„å­¦ä¹ ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šç»“åˆäº†DINOç‰¹å¾æå–æŠ€æœ¯ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªæ“ä½œå’Œå¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniVLAçš„æ€§èƒ½è¶…è¿‡äº†OpenVLAï¼Œé¢„è®­ç»ƒè®¡ç®—é‡å‡å°‘è‡³ä¸è¶³1/20ï¼Œä¸‹æ¸¸æ•°æ®éœ€æ±‚é™ä½è‡³1/10ã€‚éšç€å¼‚æ„æ•°æ®çš„æŒç»­å¼•å…¥ï¼Œæ¨¡å‹çš„æ€§èƒ½æŒç»­æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æœºå™¨äººæ”¿ç­–å­¦ä¹ ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniVLAçš„ç ”ç©¶æˆæœåœ¨å¤šç§æœºå™¨äººåº”ç”¨åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„æ½œåœ¨ä»·å€¼ï¼ŒåŒ…æ‹¬å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œæ¢ç´¢æ€§æœºå™¨äººç­‰ã€‚å…¶é«˜æ•ˆçš„ç­–ç•¥å­¦ä¹ èƒ½åŠ›å°†æ¨åŠ¨æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–å’Œæ“ä½œèƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½åœ¨æ™ºèƒ½åˆ¶é€ å’Œäººæœºåä½œç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.

