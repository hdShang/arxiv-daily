---
layout: default
title: "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks"
---

# 3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.05800" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.05800v1</a>
  <a href="https://arxiv.org/pdf/2505.05800.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.05800v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.05800v1', '3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vineet Bhat, Yu-Hsiang Lan, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

**å¤‡æ³¨**: Accepted at the 1st Workshop on 3D LLM/VLA, CVPR 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡º3D-CAVLAä»¥æå‡æœºå™¨äººåœ¨æœªçŸ¥ä»»åŠ¡ä¸­çš„æ“ä½œèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æ·±åº¦æ„ŸçŸ¥` `æ€ç»´é“¾æ¨ç†` `ä»»åŠ¡å¯¼å‘æ£€æµ‹` `å¤šæ¨¡æ€å­¦ä¹ ` `é›¶-shotå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æœªçŸ¥ä»»åŠ¡æ—¶ï¼Œç¼ºä¹è¶³å¤Ÿçš„åœºæ™¯ä¸Šä¸‹æ–‡æ„è¯†ï¼Œå¯¼è‡´æ“ä½œæˆåŠŸç‡ä½ã€‚
2. æœ¬æ–‡æå‡º3D-CAVLAæ¨¡å‹ï¼Œé€šè¿‡ç»“åˆæ·±åº¦æ„ŸçŸ¥å’Œæ€ç»´é“¾æ¨ç†ï¼Œå¢å¼ºæ¨¡å‹å¯¹åœºæ™¯çš„ç†è§£å’Œä»»åŠ¡é€‚åº”èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œ3D-CAVLAåœ¨LIBEROä»»åŠ¡å¥—ä»¶ä¸­æˆåŠŸç‡è¾¾åˆ°98.1%ï¼Œåœ¨æœªçŸ¥ä»»åŠ¡ä¸Šæå‡äº†8.8%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨3Dæœºå™¨äººæ“ä½œä¸­ï¼Œéœ€è¦å­¦ä¹ æœºå™¨äººçš„å…³èŠ‚ç©ºé—´è½¨è¿¹ã€‚æœºå™¨äººå¿…é¡»å…·å¤‡è¯­ä¹‰å’Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥å°†å…¶å·¥ä½œç©ºé—´çš„å®é™…æ˜ å°„è½¬åŒ–ä¸ºç‰©ä½“æ“ä½œæ‰€éœ€çš„ä½çº§æ§åˆ¶ã€‚æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æ•´åˆæ€ç»´é“¾æ¨ç†ã€æ·±åº¦æ„ŸçŸ¥å’Œä»»åŠ¡å¯¼å‘çš„å…´è¶£åŒºåŸŸæ£€æµ‹ï¼Œæ¥æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„åœºæ™¯ä¸Šä¸‹æ–‡æ„è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3D-CAVLAæ¨¡å‹åœ¨LIBEROä»¿çœŸç¯å¢ƒä¸­æˆåŠŸç‡è¾¾åˆ°98.1%ï¼Œå¹¶åœ¨æœªçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†8.8%çš„ç»å¯¹æå‡ã€‚æˆ‘ä»¬å°†å¼€æºä»£ç å’ŒæœªçŸ¥ä»»åŠ¡æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºç ”ç©¶ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨3Dç¯å¢ƒä¸­è¿›è¡Œæ“ä½œæ—¶ï¼Œç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æœªçŸ¥ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºRGBå›¾åƒå’Œè¯­è¨€æŒ‡ä»¤ï¼Œç¼ºä¹å¯¹åœºæ™¯æ·±åº¦å’Œä¸Šä¸‹æ–‡çš„å…¨é¢ç†è§£ï¼Œå¯¼è‡´æ“ä½œæˆåŠŸç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„3D-CAVLAæ¨¡å‹é€šè¿‡æ•´åˆæ·±åº¦æ„ŸçŸ¥ã€æ€ç»´é“¾æ¨ç†å’Œä»»åŠ¡å¯¼å‘çš„å…´è¶£åŒºåŸŸæ£€æµ‹ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹å¤æ‚åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†æœºå™¨äººåœ¨æœªçŸ¥ä»»åŠ¡ä¸­çš„æ“ä½œæˆåŠŸç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼š3D-CAVLAæ¨¡å‹çš„æ•´ä½“æ¶æ„åŒ…æ‹¬è¾“å…¥æ¨¡å—ï¼ˆæ¥æ”¶RGBå›¾åƒå’Œè¯­è¨€æŒ‡ä»¤ï¼‰ã€æ·±åº¦æ„ŸçŸ¥æ¨¡å—ï¼ˆæå–åœºæ™¯æ·±åº¦ä¿¡æ¯ï¼‰ã€æ€ç»´é“¾æ¨ç†æ¨¡å—ï¼ˆè¿›è¡Œé€»è¾‘æ¨ç†ï¼‰ä»¥åŠè¾“å‡ºæ¨¡å—ï¼ˆç”Ÿæˆå…³èŠ‚æ§åˆ¶æŒ‡ä»¤ï¼‰ã€‚å„æ¨¡å—ååŒå·¥ä½œï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼š3D-CAVLAçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ·±åº¦æ„ŸçŸ¥ä¸æ€ç»´é“¾æ¨ç†ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨é¢å¯¹æœªçŸ¥ä»»åŠ¡æ—¶ï¼Œä¾ç„¶ä¿æŒè¾ƒé«˜çš„æˆåŠŸç‡ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨çš„åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šå±‚å·ç§¯ç¥ç»ç½‘ç»œæ¥å¤„ç†RGBå›¾åƒï¼Œå¹¶ç»“åˆæ·±åº¦ä¿¡æ¯è¿›è¡Œç‰¹å¾æå–ã€‚åŒæ—¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºå¤šä»»åŠ¡æŸå¤±ï¼Œä»¥å¹³è¡¡ä¸åŒä»»åŠ¡çš„å­¦ä¹ ç›®æ ‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

3D-CAVLAæ¨¡å‹åœ¨LIBEROä»¿çœŸç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸç‡è¾¾åˆ°98.1%ã€‚åœ¨æœªçŸ¥ä»»åŠ¡ä¸Šï¼Œè¯¥æ¨¡å‹å®ç°äº†8.8%çš„ç»å¯¹æå‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„é›¶-shotå­¦ä¹ èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–åˆ¶é€ å’ŒæœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œ3D-CAVLAå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜æ•ˆçš„ç‰©ä½“æ“ä½œå’Œä»»åŠ¡æ‰§è¡Œï¼Œå…·æœ‰é‡è¦çš„å•†ä¸šä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint space trajectory of a robot manipulator. Robots must possess semantic and visual perception abilities to transform real-world mappings of their workspace into the low-level control necessary for object manipulation. Recent work has demonstrated the capabilities of fine-tuning large Vision-Language Models (VLMs) to learn the mapping between RGB images, language instructions, and joint space control. These models typically take as input RGB images of the workspace and language instructions, and are trained on large datasets of teleoperated robot demonstrations. In this work, we explore methods to improve the scene context awareness of a popular recent Vision-Language-Action model by integrating chain-of-thought reasoning, depth perception, and task-oriented region of interest detection. Our experiments in the LIBERO simulation environment show that our proposed model, 3D-CAVLA, improves the success rate across various LIBERO task suites, achieving an average success rate of 98.1$\%$. We also evaluate the zero-shot capabilities of our method, demonstrating that 3D scene awareness leads to robust learning and adaptation for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of 8.8$\%$ on unseen tasks. We will open-source our code and the unseen tasks dataset to promote community-driven research here: https://3d-cavla.github.io

