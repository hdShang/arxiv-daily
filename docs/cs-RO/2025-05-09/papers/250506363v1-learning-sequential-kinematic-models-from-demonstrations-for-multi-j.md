---
layout: default
title: Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects
---

# Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06363" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06363v1</a>
  <a href="https://arxiv.org/pdf/2505.06363.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06363v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06363v1', 'Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anmol Gupta, Weiwei Gu, Omkar Patil, Jun Ki Lee, Nakul Gopalan

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹è±¡è¿åŠ¨åºåˆ—æœºå™¨ä»¥è§£å†³å¤šå…³èŠ‚ç‰©ä½“å»ºæ¨¡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å¤šå…³èŠ‚ç‰©ä½“` `è¿åŠ¨åºåˆ—æœºå™¨` `æ·±åº¦å­¦ä¹ ` `äººç±»ç¤ºèŒƒ` `æœºå™¨äººæ“ä½œ` `çŠ¶æ€ä¼°è®¡` `ç‚¹äº‘æ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºå…ˆéªŒçŸ¥è¯†æˆ–ä»…å…³æ³¨å•è‡ªç”±åº¦ç‰©ä½“ï¼Œæ— æ³•å¤„ç†å¤šå…³èŠ‚ç‰©ä½“çš„é®æŒ¡å’Œæ“ä½œåºåˆ—ã€‚
2. æœ¬æ–‡æå‡ºäº†å¯¹è±¡è¿åŠ¨åºåˆ—æœºå™¨ï¼ˆOKSMsï¼‰ï¼Œé€šè¿‡å­¦ä¹ äººç±»ç¤ºèŒƒæ¥æ•æ‰å¤šè‡ªç”±åº¦ç‰©ä½“çš„è¿åŠ¨çº¦æŸå’Œæ“ä½œé¡ºåºã€‚
3. åœ¨8000ä¸ªæ¨¡æ‹Ÿæ ·æœ¬å’Œ1600ä¸ªçœŸå®æ ·æœ¬ä¸ŠéªŒè¯äº†Pokenetï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨çœŸå®æ•°æ®ä¸Šçš„å…³èŠ‚è½´å’ŒçŠ¶æ€ä¼°è®¡æå‡è¶…è¿‡20%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æœºå™¨äººåœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬éœ€è¦ä¸å¤æ‚çš„å¤šè‡ªç”±åº¦ç‰©ä½“è¿›è¡Œäº¤äº’ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå…ˆéªŒçŸ¥è¯†æˆ–ä»…å…³æ³¨å•è‡ªç”±åº¦ç‰©ä½“ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ï¼Œå¹¶ä¸”æ— æ³•å¤„ç†è¢«é®æŒ¡çš„å…³èŠ‚åŠå…¶æ“ä½œåºåˆ—ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é€šè¿‡å­¦ä¹ äººç±»ç¤ºèŒƒæ¥æ„å»ºå¯¹è±¡æ¨¡å‹ï¼Œæå‡ºäº†å¯¹è±¡è¿åŠ¨åºåˆ—æœºå™¨ï¼ˆOKSMsï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰å¤šè‡ªç”±åº¦ç‰©ä½“çš„è¿åŠ¨çº¦æŸå’Œæ“ä½œé¡ºåºã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Pokenetï¼Œä¸€ä¸ªåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿä»ç‚¹äº‘æ•°æ®ä¸­ä¼°è®¡è¿™äº›æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPokenetåœ¨çœŸå®æ•°æ®ä¸Šçš„å…³èŠ‚è½´å’ŒçŠ¶æ€ä¼°è®¡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†20%ä»¥ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šå…³èŠ‚ç‰©ä½“çš„å»ºæ¨¡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå…ˆéªŒçŸ¥è¯†ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†é®æŒ¡çš„å…³èŠ‚å’Œå¤æ‚çš„æ“ä½œåºåˆ—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å­¦ä¹ äººç±»ç¤ºèŒƒï¼Œæå‡ºå¯¹è±¡è¿åŠ¨åºåˆ—æœºå™¨ï¼ˆOKSMsï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶æ•æ‰è¿åŠ¨çº¦æŸå’Œæ“ä½œé¡ºåºï¼Œä»è€Œæé«˜å¤šè‡ªç”±åº¦ç‰©ä½“çš„å»ºæ¨¡ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€æ¨¡å‹è®­ç»ƒå’Œåº”ç”¨ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡äººç±»ç¤ºèŒƒæ”¶é›†æ•°æ®ï¼Œç„¶ååˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œPokenetè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæœ€ååœ¨æœºå™¨äººä¸Šè¿›è¡Œæ“ä½œéªŒè¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šå¯¹è±¡è¿åŠ¨åºåˆ—æœºå™¨ï¼ˆOKSMsï¼‰æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šè‡ªç”±åº¦ç‰©ä½“çš„è¿åŠ¨çº¦æŸå’Œæ“ä½œé¡ºåºï¼ŒåŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„å•ä¸€è‡ªç”±åº¦å»ºæ¨¡ã€‚

**å…³é”®è®¾è®¡**ï¼šPokenetçš„ç½‘ç»œç»“æ„è®¾è®¡ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å…³èŠ‚è½´å’ŒçŠ¶æ€çš„ä¼°è®¡ï¼Œå…³é”®å‚æ•°è®¾ç½®ç»è¿‡å¤šæ¬¡å®éªŒè°ƒæ•´ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPokenetåœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„å…³èŠ‚è½´å’ŒçŠ¶æ€ä¼°è®¡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†è¶…è¿‡20%ã€‚åœ¨8000ä¸ªæ¨¡æ‹Ÿæ ·æœ¬å’Œ1600ä¸ªçœŸå®æ ·æœ¬çš„éªŒè¯ä¸­ï¼ŒOKSMså±•ç¤ºäº†å…¶åœ¨å¤šè‡ªç”±åº¦ç‰©ä½“æ“ä½œä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººæ“ä½œçš„ç²¾ç¡®åº¦ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æœºå™¨äººæ“ä½œã€æ™ºèƒ½åˆ¶é€ å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡ç²¾ç¡®å»ºæ¨¡å¤šå…³èŠ‚ç‰©ä½“ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°æ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„æœºå™¨äººç³»ç»Ÿçš„å‘å±•ï¼Œå¢å¼ºå…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As robots become more generalized and deployed in diverse environments, they must interact with complex objects, many with multiple independent joints or degrees of freedom (DoF) requiring precise control. A common strategy is object modeling, where compact state-space models are learned from real-world observations and paired with classical planning. However, existing methods often rely on prior knowledge or focus on single-DoF objects, limiting their applicability. They also fail to handle occluded joints and ignore the manipulation sequences needed to access them. We address this by learning object models from human demonstrations. We introduce Object Kinematic Sequence Machines (OKSMs), a novel representation capturing both kinematic constraints and manipulation order for multi-DoF objects. To estimate these models from point cloud data, we present Pokenet, a deep neural network trained on human demonstrations. We validate our approach on 8,000 simulated and 1,600 real-world annotated samples. Pokenet improves joint axis and state estimation by over 20 percent on real-world data compared to prior methods. Finally, we demonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning to manipulate multi-DoF objects.

