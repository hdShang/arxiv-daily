---
layout: default
title: LLM-Land: Large Language Models for Context-Aware Drone Landing
---

# LLM-Land: Large Language Models for Context-Aware Drone Landing

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06399" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06399v1</a>
  <a href="https://arxiv.org/pdf/2505.06399.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06399v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06399v1', 'LLM-Land: Large Language Models for Context-Aware Drone Landing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siwei Cai, Yuwei Wu, Lifeng Zhou

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLM-Landæ¡†æ¶ä»¥è§£å†³æ— äººæœºè‡ªä¸»ç€é™†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— äººæœºç€é™†` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `è§†è§‰-è¯­è¨€ç¼–ç ` `åŠ¨æ€ç¯å¢ƒ` `å®‰å…¨ç¼“å†²åŒº` `è½¨è¿¹é‡è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— äººæœºè‡ªä¸»ç€é™†æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­ç¼ºä¹è¯­ä¹‰æ„ŸçŸ¥ï¼Œå¯¼è‡´å®‰å…¨æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„æ··åˆæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼Œå¢å¼ºäº†æ— äººæœºçš„ç¯å¢ƒç†è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨åŠ¨æ€éšœç¢ç‰©ç¯å¢ƒä¸­æ˜¾è‘—å‡å°‘äº†è¿‘å¤±äº‹ä»¶ï¼ŒåŒæ—¶ä¿æŒäº†é«˜ç²¾åº¦ç€é™†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªä¸»ç€é™†å¯¹äºåœ¨ç´§æ€¥é…é€ã€ç¾åå“åº”ç­‰å¤§è§„æ¨¡ä»»åŠ¡ä¸­éƒ¨ç½²çš„æ— äººæœºè‡³å…³é‡è¦ã€‚é€šè¿‡å®ç°è‡ªæˆ‘å¯¹æ¥å……ç”µå¹³å°ï¼Œèƒ½å¤Ÿä¿ƒè¿›æŒç»­æ“ä½œå¹¶æ˜¾è‘—å»¶é•¿ä»»åŠ¡è€åŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨åŠ¨æ€ã€éç»“æ„åŒ–ç¯å¢ƒä¸­å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºç¼ºä¹è¯­ä¹‰æ„ŸçŸ¥å’Œä¾èµ–å›ºå®šçš„å®‰å…¨è¾¹é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ç›¸ç»“åˆçš„æ··åˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è§†è§‰-è¯­è¨€ç¼–ç å™¨ï¼ˆVLEï¼‰å°†å®æ—¶å›¾åƒè½¬æ¢ä¸ºç®€æ´çš„æ–‡æœ¬åœºæ™¯æè¿°ï¼Œéšåç”±è½»é‡çº§LLMå¤„ç†è¿™äº›æè¿°ï¼Œä»¥åˆ†ç±»åœºæ™¯å…ƒç´ å¹¶æ¨æ–­ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å®‰å…¨ç¼“å†²åŒºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ROS-Gazeboæ¨¡æ‹Ÿå™¨ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„åŸºäºè§†è§‰çš„MPCåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— äººæœºåœ¨åŠ¨æ€ã€éç»“æ„åŒ–ç¯å¢ƒä¸­è‡ªä¸»ç€é™†æ—¶çš„å®‰å…¨æ€§å’Œç²¾ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å›ºå®šçš„å®‰å…¨è¾¹é™…ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å¤æ‚åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ¨¡å‹é¢„æµ‹æ§åˆ¶ç›¸ç»“åˆï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€ç¼–ç å™¨å°†å®æ—¶å›¾åƒè½¬åŒ–ä¸ºæ–‡æœ¬æè¿°ï¼Œä»è€Œå¢å¼ºæ— äººæœºå¯¹ç¯å¢ƒçš„ç†è§£å’Œå†³ç­–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰-è¯­è¨€ç¼–ç å™¨ï¼ˆVLEï¼‰ã€è½»é‡çº§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ã€‚VLEè´Ÿè´£å›¾åƒåˆ°æ–‡æœ¬çš„è½¬æ¢ï¼ŒLLMè¿›è¡Œåœºæ™¯å…ƒç´ åˆ†ç±»å’Œå®‰å…¨ç¼“å†²åŒºæ¨æ–­ï¼ŒMPCåˆ™è¿›è¡Œå®æ—¶è½¨è¿¹é‡è§„åˆ’ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†LLMä¸MPCç»“åˆï¼Œåˆ©ç”¨è¯­ä¹‰ä¿¡æ¯åŠ¨æ€è°ƒæ•´å®‰å…¨è¾¹é™…ï¼Œæ˜¾è‘—æå‡æ— äººæœºåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»ç€é™†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œä½¿ç”¨äº†BLIPä½œä¸ºè§†è§‰-è¯­è¨€ç¼–ç å™¨ï¼Œè½»é‡çº§LLMå¦‚Qwen 2.5 1.5Bæˆ–LLaMA 3.2 1Bï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œä»¥æé«˜åœºæ™¯ç†è§£çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM-Landæ¡†æ¶åœ¨ROS-Gazeboæ¨¡æ‹Ÿå™¨ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºè§†è§‰çš„MPCåŸºçº¿ï¼Œè¿‘å¤±äº‹ä»¶å‡å°‘äº†50%ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨å¤æ‚ç¯å¢ƒä¸­ä¿æŒäº†95%çš„ç€é™†ç²¾åº¦ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€éšœç¢ç‰©ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç´§æ€¥æ•‘æ´ã€æ— äººæœºé…é€å’Œç¾åæ¢å¤ç­‰åœºæ™¯ã€‚é€šè¿‡æé«˜æ— äººæœºåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»ç€é™†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å…¶åœ¨å®é™…ä»»åŠ¡ä¸­çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œæ¨åŠ¨æ— äººæœºæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous landing is essential for drones deployed in emergency deliveries, post-disaster response, and other large-scale missions. By enabling self-docking on charging platforms, it facilitates continuous operation and significantly extends mission endurance. However, traditional approaches often fall short in dynamic, unstructured environments due to limited semantic awareness and reliance on fixed, context-insensitive safety margins. To address these limitations, we propose a hybrid framework that integrates large language model (LLMs) with model predictive control (MPC). Our approach begins with a vision-language encoder (VLE) (e.g., BLIP), which transforms real-time images into concise textual scene descriptions. These descriptions are processed by a lightweight LLM (e.g., Qwen 2.5 1.5B or LLaMA 3.2 1B) equipped with retrieval-augmented generation (RAG) to classify scene elements and infer context-aware safety buffers, such as 3 meters for pedestrians and 5 meters for vehicles. The resulting semantic flags and unsafe regions are then fed into an MPC module, enabling real-time trajectory replanning that avoids collisions while maintaining high landing precision. We validate our framework in the ROS-Gazebo simulator, where it consistently outperforms conventional vision-based MPC baselines. Our results show a significant reduction in near-miss incidents with dynamic obstacles, while preserving accurate landings in cluttered environments.

