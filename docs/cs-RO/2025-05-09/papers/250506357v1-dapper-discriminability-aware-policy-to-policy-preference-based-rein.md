---
layout: default
title: DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition
---

# DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06357" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06357v1</a>
  <a href="https://arxiv.org/pdf/2505.06357.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06357v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06357v1', 'DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuki Kadokawa, Jonas Frey, Takahiro Miki, Takamitsu Matsubara, Marco Hutter

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDAPPERä»¥è§£å†³åå¥½å­¦ä¹ ä¸­çš„æŸ¥è¯¢æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åå¥½åŸºå¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæŠ€èƒ½è·å–` `æŸ¥è¯¢æ•ˆç‡` `è½¨è¿¹å¤šæ ·æ€§` `åå¥½å¯åŒºåˆ†æ€§` `ç­–ç•¥å­¦ä¹ ` `åˆ¤åˆ«å™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åå¥½åŸºå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æŸ¥è¯¢æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œç­–ç•¥åå·®å¯¼è‡´è½¨è¿¹å¤šæ ·æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºDAPPERï¼Œé€šè¿‡æ¯”è¾ƒå¤šä¸ªç­–ç•¥çš„è½¨è¿¹ç”ŸæˆæŸ¥è¯¢ï¼Œæå‡äº†åå¥½å¯åŒºåˆ†æ€§å’ŒæŸ¥è¯¢æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDAPPERåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®çš„æœºå™¨äººç¯å¢ƒä¸­å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨åå¥½å¯åŒºåˆ†æ€§è¾ƒä½çš„æƒ…å†µä¸‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åå¥½åŸºå¼ºåŒ–å­¦ä¹ ï¼ˆPbRLï¼‰é€šè¿‡ç®€å•çš„æŸ¥è¯¢æ¯”è¾ƒå•ä¸€ç­–ç•¥çš„è½¨è¿¹æ¥å®ç°ç­–ç•¥å­¦ä¹ ã€‚å°½ç®¡äººç±»å¯¹è¿™äº›æŸ¥è¯¢çš„å“åº”ä½¿å¾—å­¦ä¹ ä¸äººç±»åå¥½ä¸€è‡´çš„ç­–ç•¥æˆä¸ºå¯èƒ½ï¼Œä½†PbRLçš„æŸ¥è¯¢æ•ˆç‡è¾ƒä½ï¼Œå› ä¸ºç­–ç•¥åå·®é™åˆ¶äº†è½¨è¿¹çš„å¤šæ ·æ€§ï¼Œå‡å°‘äº†å¯ç”¨äºå­¦ä¹ åå¥½çš„å¯åŒºåˆ†æŸ¥è¯¢æ•°é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†â€”â€”åå¥½å¯åŒºåˆ†æ€§ï¼Œæ—¨åœ¨æé«˜æŸ¥è¯¢æ•ˆç‡ã€‚DAPPERæ–¹æ³•é€šè¿‡æ¯”è¾ƒå¤šä¸ªç­–ç•¥çš„è½¨è¿¹ç”ŸæˆæŸ¥è¯¢ï¼Œä¿ƒè¿›å¤šæ ·æ€§å¹¶æ¶ˆé™¤ç­–ç•¥åå·®ã€‚å®éªŒè¡¨æ˜ï¼ŒDAPPERåœ¨æŸ¥è¯¢æ•ˆç‡ä¸Šä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨åå¥½å¯åŒºåˆ†æ€§æ¡ä»¶è¾ƒä¸ºä¸¥è‹›çš„æƒ…å†µä¸‹ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åå¥½åŸºå¼ºåŒ–å­¦ä¹ ä¸­æŸ¥è¯¢æ•ˆç‡ä½çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç”±äºç­–ç•¥åå·®é™åˆ¶äº†è½¨è¿¹å¤šæ ·æ€§ï¼Œå¯¼è‡´å¯åŒºåˆ†æŸ¥è¯¢æ•°é‡å‡å°‘ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDAPPERé€šè¿‡æ¯”è¾ƒå¤šä¸ªç­–ç•¥çš„è½¨è¿¹ç”ŸæˆæŸ¥è¯¢ï¼Œé¿å…äº†å•ä¸€ç­–ç•¥çš„åå·®ï¼Œä»è€Œæé«˜äº†è½¨è¿¹çš„å¤šæ ·æ€§å’Œåå¥½å¯åŒºåˆ†æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDAPPERçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆï¼Œè®­ç»ƒå¤šä¸ªç­–ç•¥ä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„è½¨è¿¹ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨åˆ¤åˆ«å™¨ä¼°è®¡åå¥½å¯åŒºåˆ†æ€§ï¼›æœ€åï¼Œä¼˜å…ˆé‡‡æ ·æ›´å…·å¯åŒºåˆ†æ€§çš„æŸ¥è¯¢è¿›è¡Œå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šDAPPERçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†åå¥½å¯åŒºåˆ†æ€§ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¹¶é€šè¿‡å¤šç­–ç•¥æ¯”è¾ƒæ¥æå‡æŸ¥è¯¢æ•ˆç‡ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å•ä¸€ç­–ç•¥æ¯”è¾ƒæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šDAPPERåœ¨æ¯æ¬¡å¥–åŠ±æ›´æ–°åä»å¤´å¼€å§‹è®­ç»ƒæ–°ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªåˆ¤åˆ«å™¨æ¥å­¦ä¹ åå¥½å¯åŒºåˆ†æ€§ï¼ŒæŸå¤±å‡½æ•°åŒæ—¶æœ€å¤§åŒ–åå¥½å¥–åŠ±å’Œå¯åŒºåˆ†æ€§åˆ†æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDAPPERåœ¨æŸ¥è¯¢æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åå¥½å¯åŒºåˆ†æ€§è¾ƒä½çš„æ¡ä»¶ä¸‹ï¼ŒæŸ¥è¯¢æ•ˆç‡æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†DAPPERåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

DAPPERçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººæŠ€èƒ½è·å–é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¸äººç±»ç”¨æˆ·è¿›è¡Œäº¤äº’çš„åœºæ™¯ä¸­ï¼Œå¦‚æœåŠ¡æœºå™¨äººã€æ•™è‚²æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æŸ¥è¯¢æ•ˆç‡ï¼ŒDAPPERèƒ½å¤Ÿæ›´å¿«é€Ÿåœ°é€‚åº”äººç±»çš„åå¥½ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œä»»åŠ¡å®Œæˆæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Preference-based Reinforcement Learning (PbRL) enables policy learning through simple queries comparing trajectories from a single policy. While human responses to these queries make it possible to learn policies aligned with human preferences, PbRL suffers from low query efficiency, as policy bias limits trajectory diversity and reduces the number of discriminable queries available for learning preferences. This paper identifies preference discriminability, which quantifies how easily a human can judge which trajectory is closer to their ideal behavior, as a key metric for improving query efficiency. To address this, we move beyond comparisons within a single policy and instead generate queries by comparing trajectories from multiple policies, as training them from scratch promotes diversity without policy bias. We propose Discriminability-Aware Policy-to-Policy Preference-Based Efficient Reinforcement Learning (DAPPER), which integrates preference discriminability with trajectory diversification achieved by multiple policies. DAPPER trains new policies from scratch after each reward update and employs a discriminator that learns to estimate preference discriminability, enabling the prioritized sampling of more discriminable queries. During training, it jointly maximizes the preference reward and preference discriminability score, encouraging the discovery of highly rewarding and easily distinguishable policies. Experiments in simulated and real-world legged robot environments demonstrate that DAPPER outperforms previous methods in query efficiency, particularly under challenging preference discriminability conditions.

