---
layout: default
title: Camera Control at the Edge with Language Models for Scene Understanding
---

# Camera Control at the Edge with Language Models for Scene Understanding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06402" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06402v1</a>
  <a href="https://arxiv.org/pdf/2505.06402.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06402v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06402v1', 'Camera Control at the Edge with Language Models for Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexiy Buynitsky, Sina Ehsani, Bhanu Pallakonda, Pragyana Mishra

**åˆ†ç±»**: cs.RO, cs.AI, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

**å¤‡æ³¨**: 7 pages, 6 figures. This work was presented and published at the 11th IEEE International Conference on Control, Automation and Robotics (ICCAR) in 2025

**DOI**: [10.1109/ICCAR64901.2025.11073044](https://doi.org/10.1109/ICCAR64901.2025.11073044)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOPUSæ¡†æ¶ä»¥ä¼˜åŒ–PTZæ‘„åƒå¤´çš„è¯­è¨€æ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è¯­è¨€æ¨¡å‹` `æ‘„åƒå¤´æ§åˆ¶` `è‡ªç„¶è¯­è¨€å¤„ç†` `è¾¹ç¼˜è®¡ç®—` `ç›‘ç£å¾®è°ƒ` `ç¯å¢ƒæ„ŸçŸ¥` `å…³é”®è¯ç”Ÿæˆ` `æ™ºèƒ½ç›‘æ§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶PTZæ‘„åƒå¤´æ—¶ç¼ºä¹é«˜æ•ˆçš„è‡ªç„¶è¯­è¨€æ¥å£ï¼Œå¯¼è‡´ç”¨æˆ·æ“ä½œå¤æ‚ä¸”ä¸ç›´è§‚ã€‚
2. OPUSæ¡†æ¶é€šè¿‡ç”Ÿæˆå…³é”®è¯å’ŒçŸ¥è¯†è½¬ç§»ï¼Œåˆ©ç”¨LLMå®ç°å¯¹PTZæ‘„åƒå¤´çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ï¼Œç®€åŒ–ç”¨æˆ·äº¤äº’ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOPUSåœ¨ä»»åŠ¡å‡†ç¡®ç‡ä¸Šè¾ƒä¼ ç»Ÿæ–¹æ³•æé«˜äº†20%ï¼Œåœ¨å¤æ‚æç¤ºæ–¹æ³•ä¸Šæå‡äº†35%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¼˜åŒ–çš„åŸºäºæç¤ºçš„ç»Ÿä¸€ç³»ç»Ÿï¼ˆOPUSï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ§åˆ¶å¹³ç§»-å€¾æ–œ-å˜ç„¦ï¼ˆPTZï¼‰æ‘„åƒå¤´ï¼Œä»è€Œæä¾›å¯¹è‡ªç„¶ç¯å¢ƒçš„ä¸Šä¸‹æ–‡ç†è§£ã€‚OPUSé€šè¿‡ä»é«˜å±‚æ‘„åƒå¤´æ§åˆ¶APIç”Ÿæˆå…³é”®è¯ï¼Œå¹¶é€šè¿‡åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå°†çŸ¥è¯†ä»å¤§å‹å°é—­æºè¯­è¨€æ¨¡å‹è½¬ç§»åˆ°è¾ƒå°çš„æ¨¡å‹ï¼Œä»è€Œæé«˜äº†æˆæœ¬æ•ˆç›Šã€‚è¿™ä½¿å¾—åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆéƒ¨ç½²æˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶ä¿æŒä¸GPT-4ç­‰å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚OPUSé€šè¿‡å°†å¤šä¸ªæ‘„åƒå¤´çš„æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œå¢å¼ºäº†ç¯å¢ƒæ„è¯†ï¼Œæ¶ˆé™¤äº†å¯¹ä¸“ç”¨ä¼ æ„Ÿå™¨æ ‡è®°çš„éœ€æ±‚ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè¯­è¨€æ¨¡å‹æŠ€æœ¯å’Œæ›´å¤æ‚çš„æç¤ºæ–¹æ³•ï¼Œè¾ƒå…ˆè¿›æŠ€æœ¯æå‡äº†35%ï¼Œä¸å°é—­æºæ¨¡å‹å¦‚Gemini Proç›¸æ¯”ï¼Œä»»åŠ¡å‡†ç¡®ç‡æé«˜äº†20%ã€‚è¯¥ç³»ç»Ÿå±•ç¤ºäº†OPUSé€šè¿‡ç›´è§‚çš„è‡ªç„¶è¯­è¨€æ¥å£ç®€åŒ–PTZæ‘„åƒå¤´æ“ä½œçš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰PTZæ‘„åƒå¤´æ§åˆ¶æ–¹æ³•ä¸­ç¼ºä¹é«˜æ•ˆè‡ªç„¶è¯­è¨€äº¤äº’çš„é—®é¢˜ï¼Œç”¨æˆ·åœ¨æ“ä½œæ—¶å¸¸å¸¸éœ€è¦å¤æ‚çš„ç¼–ç¨‹çŸ¥è¯†ï¼Œå¯¼è‡´ä½¿ç”¨é—¨æ§›é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOPUSæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå…³é”®è¯ï¼Œå¹¶å°†å…¶ä¸é«˜å±‚æ‘„åƒå¤´æ§åˆ¶APIç›¸ç»“åˆï¼Œä»è€Œå®ç°è‡ªç„¶è¯­è¨€å¯¹æ‘„åƒå¤´çš„æ§åˆ¶ï¼Œç®€åŒ–ç”¨æˆ·æ“ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOPUSçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€å…³é”®è¯ç”Ÿæˆæ¨¡å—ã€è¯­è¨€æ¨¡å‹å¤„ç†æ¨¡å—å’Œè¾“å‡ºæ§åˆ¶æ¨¡å—ã€‚æ•°æ®è¾“å…¥æ¨¡å—è´Ÿè´£æ”¶é›†æ¥è‡ªå¤šä¸ªæ‘„åƒå¤´çš„æ•°æ®ï¼Œå…³é”®è¯ç”Ÿæˆæ¨¡å—å°†è¿™äº›æ•°æ®è½¬æ¢ä¸ºå¯ä¾›è¯­è¨€æ¨¡å‹å¤„ç†çš„æ–‡æœ¬ï¼Œè¯­è¨€æ¨¡å‹å¤„ç†æ¨¡å—åˆ™æ‰§è¡Œæ§åˆ¶æŒ‡ä»¤ï¼Œæœ€åè¾“å‡ºæ§åˆ¶ä¿¡å·ç»™æ‘„åƒå¤´ã€‚

**å…³é”®åˆ›æ–°**ï¼šOPUSçš„ä¸»è¦åˆ›æ–°åœ¨äºé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å°†çŸ¥è¯†ä»å¤§å‹å°é—­æºæ¨¡å‹è½¬ç§»åˆ°è¾ƒå°æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†è¾¹ç¼˜è®¾å¤‡çš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›´æ¥ä½¿ç”¨å¤§å‹æ¨¡å‹çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒOPUSé‡‡ç”¨äº†åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–äº†å…³é”®è¯ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡è°ƒæ•´æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„æ¥æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOPUSåœ¨åŸºå‡†æµ‹è¯•ä¸­è¾ƒä¼ ç»Ÿè¯­è¨€æ¨¡å‹æŠ€æœ¯æå‡äº†35%ï¼Œåœ¨ä»»åŠ¡å‡†ç¡®ç‡ä¸Šè¾ƒå°é—­æºæ¨¡å‹å¦‚Gemini Proæé«˜äº†20%ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡è¯æ˜äº†OPUSåœ¨PTZæ‘„åƒå¤´æ§åˆ¶ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½ç›‘æ§ã€æ— äººé©¾é©¶è½¦è¾†ã€æœºå™¨äººè§†è§‰ç­‰ã€‚é€šè¿‡ç®€åŒ–PTZæ‘„åƒå¤´çš„æ“ä½œï¼Œç”¨æˆ·å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œç¯å¢ƒç›‘æµ‹å’Œæ•°æ®é‡‡é›†ï¼Œæå‡äº†ç³»ç»Ÿçš„å¯ç”¨æ€§å’Œçµæ´»æ€§ã€‚æœªæ¥ï¼ŒOPUSå¯èƒ½åœ¨æ›´å¤šæ™ºèƒ½è®¾å¤‡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In this paper, we present Optimized Prompt-based Unified System (OPUS), a framework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom (PTZ) cameras, providing contextual understanding of natural environments. To achieve this goal, the OPUS system improves cost-effectiveness by generating keywords from a high-level camera control API and transferring knowledge from larger closed-source language models to smaller ones through Supervised Fine-Tuning (SFT) on synthetic data. This enables efficient edge deployment while maintaining performance comparable to larger models like GPT-4. OPUS enhances environmental awareness by converting data from multiple cameras into textual descriptions for language models, eliminating the need for specialized sensory tokens. In benchmark testing, our approach significantly outperformed both traditional language model techniques and more complex prompting methods, achieving a 35% improvement over advanced techniques and a 20% higher task accuracy compared to closed-source models like Gemini Pro. The system demonstrates OPUS's capability to simplify PTZ camera operations through an intuitive natural language interface. This approach eliminates the need for explicit programming and provides a conversational method for interacting with camera systems, representing a significant advancement in how users can control and utilize PTZ camera technology.

