---
layout: default
title: "TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations"
---

# TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06079" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06079v1</a>
  <a href="https://arxiv.org/pdf/2505.06079.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06079v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06079v1', 'TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuaiyi Huang, Mara Levy, Anubhav Gupta, Daniel Ekpo, Ruijie Zheng, Abhinav Shrivastava

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-09

**å¤‡æ³¨**: ICRA 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://shuaiyihuang.github.io/publications/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTRENDæ¡†æ¶ä»¥è§£å†³åå¥½åé¦ˆå™ªå£°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åå¥½å¼ºåŒ–å­¦ä¹ ` `å™ªå£°å¤„ç†` `æœºå™¨äººæ“ä½œ` `ä¸‰é‡æ•™å­¦` `ä¸“å®¶æ¼”ç¤º` `é²æ£’æ€§` `çŸ¥è¯†ä¼ é€’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åå¥½å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†äººç±»åé¦ˆæ—¶ï¼Œå¸¸å¸¸å—åˆ°å™ªå£°çš„å½±å“ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœä¸ä½³ã€‚
2. TRENDæ¡†æ¶é€šè¿‡ä¸‰é‡æ•™å­¦ç­–ç•¥ï¼Œç»“åˆå°‘é‡ä¸“å®¶æ¼”ç¤ºï¼Œæå‡äº†å¯¹å™ªå£°åå¥½åé¦ˆçš„é²æ£’æ€§ã€‚
3. åœ¨å¤šç§æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒTRENDåœ¨é«˜è¾¾40%çš„å™ªå£°æ°´å¹³ä¸‹ä»èƒ½å®ç°90%çš„æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åå¥½åé¦ˆé€šå¸¸ç”±äººç±»æˆ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ³¨é‡Šè€…æ”¶é›†ï¼Œä½†è¿™äº›åé¦ˆå¾€å¾€å­˜åœ¨å™ªå£°ï¼Œè¿™å¯¹ä¾èµ–å‡†ç¡®åå¥½æ ‡ç­¾çš„åå¥½å¼ºåŒ–å­¦ä¹ æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TRENDæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å°‘é‡ä¸“å®¶æ¼”ç¤ºå’Œä¸‰é‡æ•™å­¦ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆå‡è½»å™ªå£°å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåŒæ—¶è®­ç»ƒä¸‰ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹å°†å…¶å°æŸå¤±åå¥½å¯¹è§†ä¸ºæœ‰ç”¨çŸ¥è¯†ï¼Œå¹¶å°†è¿™äº›æœ‰ç”¨å¯¹ä¼ æˆç»™å…¶åŒä¼´ç½‘ç»œä»¥æ›´æ–°å‚æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€ä¸€åˆ°ä¸‰ä¸ªä¸“å®¶æ¼”ç¤ºå³å¯å®ç°é«˜æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¤šç§æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¯„ä¼°äº†TRENDï¼Œå³ä½¿åœ¨é«˜è¾¾40%çš„å™ªå£°æ°´å¹³ä¸‹ï¼ŒæˆåŠŸç‡ä¹Ÿè¾¾åˆ°äº†90%ï¼Œçªæ˜¾äº†å…¶åœ¨å¤„ç†å™ªå£°åå¥½åé¦ˆæ–¹é¢çš„æœ‰æ•ˆé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åå¥½åé¦ˆä¸­å­˜åœ¨çš„å™ªå£°é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å™ªå£°æ—¶è¡¨ç°ä¸ä½³ï¼Œå½±å“å­¦ä¹ æ•ˆæœã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTRENDæ¡†æ¶é€šè¿‡ä¸‰é‡æ•™å­¦ç­–ç•¥ï¼Œåˆ©ç”¨å°‘é‡ä¸“å®¶æ¼”ç¤ºæ¥è®­ç»ƒä¸‰ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œä½¿å…¶ç›¸äº’å­¦ä¹ ï¼Œä»è€Œæœ‰æ•ˆå‡è½»å™ªå£°çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTRENDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä¸‰ä¸ªå¥–åŠ±æ¨¡å‹çš„å¹¶è¡Œè®­ç»ƒã€æ¨¡å‹é—´çš„çŸ¥è¯†ä¼ é€’ä»¥åŠå¯¹å°æŸå¤±åå¥½å¯¹çš„åˆ©ç”¨ã€‚æ¯ä¸ªæ¨¡å‹ç‹¬ç«‹å­¦ä¹ ï¼ŒåŒæ—¶åˆé€šè¿‡å…±äº«æœ‰ç”¨çŸ¥è¯†æ¥å¢å¼ºæ•´ä½“æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šTRENDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸‰é‡æ•™å­¦ç­–ç•¥çš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å™ªå£°ç¯å¢ƒä¸­ç›¸äº’ä¿ƒè¿›ï¼Œæ˜¾è‘—æé«˜äº†å­¦ä¹ çš„é²æ£’æ€§ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹è®­ç»ƒæ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¼ºè°ƒå°æŸå¤±åå¥½å¯¹çš„ä»·å€¼ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­ç»è¿‡ä¼˜åŒ–ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç§æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒTRENDæ¡†æ¶åœ¨é«˜è¾¾40%çš„å™ªå£°æ°´å¹³ä¸‹å®ç°äº†90%çš„æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å™ªå£°ç¯å¢ƒä¸­çš„å¼ºå¤§é²æ£’æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒTRENDåœ¨å¤„ç†åå¥½åé¦ˆæ—¶çš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TRENDæ¡†æ¶åœ¨æœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜åå¥½å¼ºåŒ–å­¦ä¹ çš„é²æ£’æ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…ç¯å¢ƒä¸­æ›´å¥½åœ°å¤„ç†äººç±»åé¦ˆï¼Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„å†³ç­–èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Preference feedback collected by human or VLM annotators is often noisy, presenting a significant challenge for preference-based reinforcement learning that relies on accurate preference labels. To address this challenge, we propose TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching strategy for effective noise mitigation. Our method trains three reward models simultaneously, where each model views its small-loss preference pairs as useful knowledge and teaches such useful pairs to its peer network for updating the parameters. Remarkably, our approach requires as few as one to three expert demonstrations to achieve high performance. We evaluate TREND on various robotic manipulation tasks, achieving up to 90% success rates even with noise levels as high as 40%, highlighting its effective robustness in handling noisy preference feedback. Project page: https://shuaiyihuang.github.io/publications/TREND.

