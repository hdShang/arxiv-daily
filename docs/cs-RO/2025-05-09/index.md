---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-09
---

# cs.ROï¼ˆ2025-05-09ï¼‰

ğŸ“Š å…± **21** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ğŸ”—2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250506111v3-univla-learning-to-act-anywhere-with-task-centric-latent-actions.html">UniVLA: Learning to Act Anywhere with Task-centric Latent Actions</a></td>
  <td>æå‡ºUniVLAä»¥è§£å†³æœºå™¨äººè·¨ç¯å¢ƒå­¦ä¹ èƒ½åŠ›ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">cross-embodiment</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06111v3" data-paper-url="./papers/250506111v3-univla-learning-to-act-anywhere-with-task-centric-latent-actions.html" onclick="toggleFavorite(this, '2505.06111v3', 'UniVLA: Learning to Act Anywhere with Task-centric Latent Actions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250505800v1-3d-cavla-leveraging-depth-and-3d-context-to-generalize-vision-langua.html">3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks</a></td>
  <td>æå‡º3D-CAVLAä»¥æå‡æœºå™¨äººåœ¨æœªçŸ¥ä»»åŠ¡ä¸­çš„æ“ä½œèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05800v1" data-paper-url="./papers/250505800v1-3d-cavla-leveraging-depth-and-3d-context-to-generalize-vision-langua.html" onclick="toggleFavorite(this, '2505.05800v1', '3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250506399v1-llm-land-large-language-models-for-context-aware-drone-landing.html">LLM-Land: Large Language Models for Context-Aware Drone Landing</a></td>
  <td>æå‡ºLLM-Landæ¡†æ¶ä»¥è§£å†³æ— äººæœºè‡ªä¸»ç€é™†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06399v1" data-paper-url="./papers/250506399v1-llm-land-large-language-models-for-context-aware-drone-landing.html" onclick="toggleFavorite(this, '2505.06399v1', 'LLM-Land: Large Language Models for Context-Aware Drone Landing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250505773v1-human-robot-collaboration-for-the-remote-control-of-mobile-humanoid-.html">Human-Robot Collaboration for the Remote Control of Mobile Humanoid Robots with Torso-Arm Coordination</a></td>
  <td>æå‡ºäººæœºåä½œæ–¹æ³•ä»¥è§£å†³ç§»åŠ¨ç±»äººæœºå™¨äººæ§åˆ¶æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05773v1" data-paper-url="./papers/250505773v1-human-robot-collaboration-for-the-remote-control-of-mobile-humanoid-.html" onclick="toggleFavorite(this, '2505.05773v1', 'Human-Robot Collaboration for the Remote Control of Mobile Humanoid Robots with Torso-Arm Coordination')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250506218v1-let-humanoids-hike-integrative-skill-development-on-complex-trails.html">Let Humanoids Hike! Integrative Skill Development on Complex Trails</a></td>
  <td>æå‡ºLEGO-Hä»¥è§£å†³å¤æ‚åœ°å½¢ä¸‹äººå½¢æœºå™¨äººè‡ªä¸»å¾’æ­¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06218v1" data-paper-url="./papers/250506218v1-let-humanoids-hike-integrative-skill-development-on-complex-trails.html" onclick="toggleFavorite(this, '2505.06218v1', 'Let Humanoids Hike! Integrative Skill Development on Complex Trails')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250505753v2-towards-embodiment-scaling-laws-in-robot-locomotion.html">Towards Embodiment Scaling Laws in Robot Locomotion</a></td>
  <td>æå‡ºä½“ç°è§„æ¨¡å®šå¾‹ä»¥æå‡æœºå™¨äººè¿åŠ¨çš„æ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span> <span class="paper-tag">Unitree</span> <span class="paper-tag">cross-embodiment</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05753v2" data-paper-url="./papers/250505753v2-towards-embodiment-scaling-laws-in-robot-locomotion.html" onclick="toggleFavorite(this, '2505.05753v2', 'Towards Embodiment Scaling Laws in Robot Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250506451v1-adaptive-wiping-adaptive-contact-rich-manipulation-through-few-shot-.html">Adaptive Wiping: Adaptive contact-rich manipulation through few-shot imitation learning with Force-Torque feedback and pre-trained object representations</a></td>
  <td>æå‡ºè‡ªé€‚åº”æ“¦æ‹­æ–¹æ³•ä»¥è§£å†³æœºå™¨äººæ¥è§¦ä¸°å¯Œä»»åŠ¡çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06451v1" data-paper-url="./papers/250506451v1-adaptive-wiping-adaptive-contact-rich-manipulation-through-few-shot-.html" onclick="toggleFavorite(this, '2505.06451v1', 'Adaptive Wiping: Adaptive contact-rich manipulation through few-shot imitation learning with Force-Torque feedback and pre-trained object representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250506357v1-dapper-discriminability-aware-policy-to-policy-preference-based-rein.html">DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition</a></td>
  <td>æå‡ºDAPPERä»¥è§£å†³åå¥½å­¦ä¹ ä¸­çš„æŸ¥è¯¢æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06357v1" data-paper-url="./papers/250506357v1-dapper-discriminability-aware-policy-to-policy-preference-based-rein.html" onclick="toggleFavorite(this, '2505.06357v1', 'DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250506079v1-trend-tri-teaching-for-robust-preference-based-reinforcement-learnin.html">TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations</a></td>
  <td>æå‡ºTRENDæ¡†æ¶ä»¥è§£å†³åå¥½åé¦ˆå™ªå£°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06079v1" data-paper-url="./papers/250506079v1-trend-tri-teaching-for-robust-preference-based-reinforcement-learnin.html" onclick="toggleFavorite(this, '2505.06079v1', 'TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250505787v1-demystifying-diffusion-policies-action-memorization-and-simple-looku.html">Demystifying Diffusion Policies: Action Memorization and Simple Lookup Table Alternatives</a></td>
  <td>æå‡ºåŠ¨ä½œæŸ¥æ‰¾è¡¨æ”¿ç­–ä»¥æ›¿ä»£æ‰©æ•£æ”¿ç­–è§£å†³æœºå™¨äººæ“ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">diffusion policy</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05787v1" data-paper-url="./papers/250505787v1-demystifying-diffusion-policies-action-memorization-and-simple-looku.html" onclick="toggleFavorite(this, '2505.05787v1', 'Demystifying Diffusion Policies: Action Memorization and Simple Lookup Table Alternatives')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250506126v1-krrf-kinodynamic-rapidly-exploring-random-forest-algorithm-for-multi.html">KRRF: Kinodynamic Rapidly-exploring Random Forest algorithm for multi-goal motion planning</a></td>
  <td>æå‡ºKRRFç®—æ³•ä»¥è§£å†³å¤šç›®æ ‡è¿åŠ¨è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06126v1" data-paper-url="./papers/250506126v1-krrf-kinodynamic-rapidly-exploring-random-forest-algorithm-for-multi.html" onclick="toggleFavorite(this, '2505.06126v1', 'KRRF: Kinodynamic Rapidly-exploring Random Forest algorithm for multi-goal motion planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250505691v1-physics-informed-temporal-difference-metric-learning-for-robot-motio.html">Physics-informed Temporal Difference Metric Learning for Robot Motion Planning</a></td>
  <td>æå‡ºè‡ªç›‘ç£æ—¶åºå·®åˆ†åº¦é‡å­¦ä¹ ä»¥è§£å†³æœºå™¨äººè¿åŠ¨è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05691v1" data-paper-url="./papers/250505691v1-physics-informed-temporal-difference-metric-learning-for-robot-motio.html" onclick="toggleFavorite(this, '2505.05691v1', 'Physics-informed Temporal Difference Metric Learning for Robot Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250506071v1-centralized-decision-making-for-platooning-by-using-spat-driven-refe.html">Centralized Decision-Making for Platooning By Using SPaT-Driven Reference Speeds</a></td>
  <td>æå‡ºé›†ä¸­å†³ç­–æ–¹æ³•ä»¥ä¼˜åŒ–åŸå¸‚è½¦é˜Ÿè¡Œé©¶æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06071v1" data-paper-url="./papers/250506071v1-centralized-decision-making-for-platooning-by-using-spat-driven-refe.html" onclick="toggleFavorite(this, '2505.06071v1', 'Centralized Decision-Making for Platooning By Using SPaT-Driven Reference Speeds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250506363v1-learning-sequential-kinematic-models-from-demonstrations-for-multi-j.html">Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects</a></td>
  <td>æå‡ºå¯¹è±¡è¿åŠ¨åºåˆ—æœºå™¨ä»¥è§£å†³å¤šå…³èŠ‚ç‰©ä½“å»ºæ¨¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06363v1" data-paper-url="./papers/250506363v1-learning-sequential-kinematic-models-from-demonstrations-for-multi-j.html" onclick="toggleFavorite(this, '2505.06363v1', 'Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250506092v1-robot-learning-using-multi-coordinate-elastic-maps.html">Robot Learning Using Multi-Coordinate Elastic Maps</a></td>
  <td>æå‡ºå¤šåæ ‡å¼¹æ€§æ˜ å°„ä»¥æå‡æœºå™¨äººå­¦ä¹ æ“ä½œæŠ€èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06092v1" data-paper-url="./papers/250506092v1-robot-learning-using-multi-coordinate-elastic-maps.html" onclick="toggleFavorite(this, '2505.06092v1', 'Robot Learning Using Multi-Coordinate Elastic Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250505762v1-multi-agent-systems-for-robotic-autonomy-with-llms.html">Multi-Agent Systems for Robotic Autonomy with LLMs</a></td>
  <td>æå‡ºå¤šæ™ºèƒ½ä½“æ¡†æ¶ä»¥æå‡æœºå™¨äººè‡ªä¸»æ€§ä¸ä»»åŠ¡åˆ†æèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05762v1" data-paper-url="./papers/250505762v1-multi-agent-systems-for-robotic-autonomy-with-llms.html" onclick="toggleFavorite(this, '2505.05762v1', 'Multi-Agent Systems for Robotic Autonomy with LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250506182v3-apple-toward-general-active-perception-via-reinforcement-learning.html">Apple: Toward General Active Perception via Reinforcement Learning</a></td>
  <td>æå‡ºAPPLEæ¡†æ¶ä»¥è§£å†³é€šç”¨ä¸»åŠ¨æ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06182v3" data-paper-url="./papers/250506182v3-apple-toward-general-active-perception-via-reinforcement-learning.html" onclick="toggleFavorite(this, '2505.06182v3', 'Apple: Toward General Active Perception via Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250507868v2-vista-generative-visual-imagination-for-vision-and-language-navigati.html">VISTA: Generative Visual Imagination for Vision-and-Language Navigation</a></td>
  <td>æå‡ºVISTAä»¥è§£å†³è§†è§‰ä¸è¯­è¨€å¯¼èˆªä¸­çš„é•¿æ—¶é—´è§‚å¯Ÿé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07868v2" data-paper-url="./papers/250507868v2-vista-generative-visual-imagination-for-vision-and-language-navigati.html" onclick="toggleFavorite(this, '2505.07868v2', 'VISTA: Generative Visual Imagination for Vision-and-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250505832v1-augmented-body-communicator-enhancing-daily-body-expression-for-peop.html">Augmented Body Communicator: Enhancing daily body expression for people with upper limb limitations through LLM and a robotic arm</a></td>
  <td>æå‡ºå¢å¼ºèº«ä½“äº¤æµç³»ç»Ÿä»¥å¸®åŠ©ä¸Šè‚¢åŠŸèƒ½å—é™è€…æ”¹å–„ç¤¾äº¤è¡¨è¾¾</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05832v1" data-paper-url="./papers/250505832v1-augmented-body-communicator-enhancing-daily-body-expression-for-peop.html" onclick="toggleFavorite(this, '2505.05832v1', 'Augmented Body Communicator: Enhancing daily body expression for people with upper limb limitations through LLM and a robotic arm')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250506402v1-camera-control-at-the-edge-with-language-models-for-scene-understand.html">Camera Control at the Edge with Language Models for Scene Understanding</a></td>
  <td>æå‡ºOPUSæ¡†æ¶ä»¥ä¼˜åŒ–PTZæ‘„åƒå¤´çš„è¯­è¨€æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06402v1" data-paper-url="./papers/250506402v1-camera-control-at-the-edge-with-language-models-for-scene-understand.html" onclick="toggleFavorite(this, '2505.06402v1', 'Camera Control at the Edge with Language Models for Scene Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250505831v1-oh-fk-how-do-people-feel-about-robots-that-leverage-profanity.html">Oh F**k! How Do People Feel about Robots that Leverage Profanity?</a></td>
  <td>æ¢è®¨æœºå™¨äººä½¿ç”¨è„è¯ä»¥æ”¹å–„äººç±»ç¤¾äº¤æ„ŸçŸ¥</td>
  <td class="tags-cell"><span class="paper-tag">HuMoR</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.05831v1" data-paper-url="./papers/250505831v1-oh-fk-how-do-people-feel-about-robots-that-leverage-profanity.html" onclick="toggleFavorite(this, '2505.05831v1', 'Oh F**k! How Do People Feel about Robots that Leverage Profanity?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)