---
layout: default
title: InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy
---

# InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13778" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13778v1</a>
  <a href="https://arxiv.org/pdf/2510.13778.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13778v1" onclick="toggleFavorite(this, '2510.13778v1', 'InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, Yang Tian, Bin Wang, Bolun Wang, Fangjing Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jinhui Ye, Junqiu Yu, Jia Zeng, Jingjing Zhang, Jinyu Zhang, Shi Zhang, Feng Zheng, Bowen Zhou, Yangkun Zhu

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**å¤‡æ³¨**: Technical report

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/InternRobotics/InternVLA-M1)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**InternVLA-M1ï¼šé¢å‘é€šç”¨æœºå™¨äººç­–ç•¥çš„ç©ºé—´å¼•å¯¼è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ§åˆ¶` `è§†è§‰è¯­è¨€åŠ¨ä½œ` `ç©ºé—´æ¨ç†` `æŒ‡ä»¤è·Ÿéš` `é€šç”¨æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æŒ‡ä»¤è·Ÿéšæœºå™¨äººéš¾ä»¥æ³›åŒ–åˆ°å¤æ‚ç¯å¢ƒå’Œä»»åŠ¡ï¼Œç¼ºä¹å¯¹ç©ºé—´ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ã€‚
2. æå‡ºç©ºé—´å¼•å¯¼çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶InternVLA-M1ï¼Œé€šè¿‡ç©ºé—´å®šä½è¿æ¥æŒ‡ä»¤å’ŒåŠ¨ä½œï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒInternVLA-M1åœ¨å¤šä¸ªæœºå™¨äººå¹³å°å’Œä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨é•¿æ—¶ç¨‹æ¨ç†ä¸­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†InternVLA-M1ï¼Œä¸€ä¸ªç»Ÿä¸€çš„ç©ºé—´å®šä½å’Œæœºå™¨äººæ§åˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨æŒ‡ä»¤è·Ÿéšæœºå™¨äººå‘å¯æ‰©å±•çš„é€šç”¨æ™ºèƒ½å‘å±•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç©ºé—´å¼•å¯¼çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œè®­ç»ƒï¼Œå…¶ä¸­ç©ºé—´å®šä½ä½œä¸ºæŒ‡ä»¤å’Œæœºå™¨äººåŠ¨ä½œä¹‹é—´çš„å…³é”®è¿æ¥ã€‚InternVLA-M1é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šï¼ˆiï¼‰åœ¨è¶…è¿‡230ä¸‡ä¸ªç©ºé—´æ¨ç†æ•°æ®ä¸Šè¿›è¡Œç©ºé—´å®šä½é¢„è®­ç»ƒï¼Œé€šè¿‡å°†æŒ‡ä»¤ä¸è§†è§‰ã€ä¸å…·ä½“å®æ–½æ— å…³çš„ä½ç½®å¯¹é½æ¥ç¡®å®šâ€œåœ¨å“ªé‡Œè¡ŒåŠ¨â€ï¼›ï¼ˆiiï¼‰è¿›è¡Œç©ºé—´å¼•å¯¼çš„åŠ¨ä½œåè®­ç»ƒï¼Œé€šè¿‡å³æ’å³ç”¨çš„ç©ºé—´æç¤ºç”Ÿæˆä¸å…·ä½“å®æ–½ç›¸å…³çš„åŠ¨ä½œï¼Œä»è€Œå†³å®šâ€œå¦‚ä½•è¡ŒåŠ¨â€ã€‚è¿™ç§ç©ºé—´å¼•å¯¼çš„è®­ç»ƒæ–¹æ³•å¸¦æ¥äº†æŒç»­çš„æ”¶ç›Šï¼šInternVLA-M1åœ¨SimplerEnv Google Robotä¸Šä¼˜äºæ²¡æœ‰ç©ºé—´å¼•å¯¼çš„å˜ä½“+14.6%ï¼Œåœ¨WidowXä¸Š+17%ï¼Œåœ¨LIBERO Frankaä¸Š+4.3%ï¼ŒåŒæ—¶åœ¨ç›’å­ã€ç‚¹å’Œè½¨è¿¹é¢„æµ‹ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ‰©å±•æŒ‡ä»¤è·Ÿéšï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä»¿çœŸå¼•æ“æ¥æ”¶é›†24.4ä¸‡ä¸ªå¯æ³›åŒ–çš„æŠ“å–å’Œæ”¾ç½®ç‰‡æ®µï¼Œä»è€Œåœ¨200ä¸ªä»»åŠ¡å’Œ3000å¤šä¸ªå¯¹è±¡ä¸Šå®ç°äº†å¹³å‡6.2%çš„æ”¹è¿›ã€‚åœ¨çœŸå®ä¸–ç•Œçš„é›†ç¾¤æŠ“å–å’Œæ”¾ç½®ä¸­ï¼ŒInternVLA-M1æé«˜äº†7.3%ï¼Œé€šè¿‡åˆæˆååŒè®­ç»ƒï¼Œåœ¨æœªè§è¿‡çš„å¯¹è±¡å’Œæ–°é…ç½®ä¸Šå®ç°äº†+20.6%ã€‚æ­¤å¤–ï¼Œåœ¨é•¿æ—¶ç¨‹æ¨ç†å¯†é›†å‹åœºæ™¯ä¸­ï¼Œå®ƒè¶…è¿‡äº†ç°æœ‰å·¥ä½œ10%ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç©ºé—´å¼•å¯¼è®­ç»ƒæ˜¯å¯æ‰©å±•å’Œæœ‰å¼¹æ€§çš„é€šç”¨æœºå™¨äººçš„ç»Ÿä¸€åŸåˆ™ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æŒ‡ä»¤è·Ÿéšæœºå™¨äººç­–ç•¥åœ¨å¤„ç†å¤æ‚ä»»åŠ¡å’Œç¯å¢ƒæ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚å®ƒä»¬é€šå¸¸éš¾ä»¥æœ‰æ•ˆåœ°ç†è§£å’Œåˆ©ç”¨ç©ºé—´ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨éœ€è¦ç²¾ç¡®å®šä½å’Œæ“ä½œçš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡ç‰¹å®šä»»åŠ¡çš„æ•°æ®ï¼Œéš¾ä»¥æ‰©å±•åˆ°æ–°çš„åœºæ™¯å’Œæœºå™¨äººå¹³å°ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šInternVLA-M1çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ç©ºé—´å¼•å¯¼ä½œä¸ºè¿æ¥æŒ‡ä»¤å’Œæœºå™¨äººåŠ¨ä½œçš„å…³é”®æ¡¥æ¢ã€‚é€šè¿‡æ˜¾å¼åœ°å­¦ä¹ æŒ‡ä»¤ä¸è§†è§‰ç©ºé—´ä½ç½®ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£â€œåœ¨å“ªé‡Œè¡ŒåŠ¨â€ï¼Œä»è€Œæ›´å‡†ç¡®åœ°æ‰§è¡Œä»»åŠ¡ã€‚è¿™ç§ç©ºé—´å¼•å¯¼çš„è®­ç»ƒæ–¹æ³•æ—¨åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒå’Œæœºå™¨äººå¹³å°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInternVLA-M1é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯ç©ºé—´å®šä½é¢„è®­ç»ƒï¼Œæ¨¡å‹åœ¨å¤§è§„æ¨¡ç©ºé—´æ¨ç†æ•°æ®é›†ä¸Šå­¦ä¹ å°†æŒ‡ä»¤ä¸è§†è§‰ç©ºé—´ä½ç½®å¯¹é½ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯ç©ºé—´å¼•å¯¼çš„åŠ¨ä½œåè®­ç»ƒï¼Œæ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„ç©ºé—´å®šä½èƒ½åŠ›ï¼Œé€šè¿‡ç©ºé—´æç¤ºç”Ÿæˆä¸å…·ä½“æœºå™¨äººç›¸å…³çš„åŠ¨ä½œã€‚æ•´ä½“æ¶æ„åŒ…å«è§†è§‰ç¼–ç å™¨ã€è¯­è¨€ç¼–ç å™¨ã€ç©ºé—´å®šä½æ¨¡å—å’ŒåŠ¨ä½œç”Ÿæˆæ¨¡å—ã€‚

**å…³é”®åˆ›æ–°**ï¼šInternVLA-M1æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå…¶ç©ºé—´å¼•å¯¼çš„è®­ç»ƒæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒInternVLA-M1æ˜¾å¼åœ°å­¦ä¹ ç©ºé—´ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä½œä¸ºæŒ‡å¯¼åŠ¨ä½œç”Ÿæˆçš„é‡è¦çº¿ç´¢ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æŒ‡ä»¤çš„æ„å›¾ï¼Œå¹¶ç”Ÿæˆæ›´å‡†ç¡®çš„åŠ¨ä½œã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å³æ’å³ç”¨çš„ç©ºé—´æç¤ºï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè½»æ¾åœ°é€‚åº”ä¸åŒçš„æœºå™¨äººå¹³å°ã€‚

**å…³é”®è®¾è®¡**ï¼šç©ºé—´å®šä½é¢„è®­ç»ƒä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œé¼“åŠ±æ¨¡å‹å°†ç›¸ä¼¼çš„æŒ‡ä»¤å’Œç©ºé—´ä½ç½®æ˜ å°„åˆ°ç›¸è¿‘çš„åµŒå…¥ç©ºé—´ã€‚åŠ¨ä½œåè®­ç»ƒä½¿ç”¨è¡Œä¸ºå…‹éš†æŸå¤±ï¼Œé¼“åŠ±æ¨¡å‹æ¨¡ä»¿ä¸“å®¶è½¨è¿¹ã€‚ç½‘ç»œç»“æ„é‡‡ç”¨Transformeræ¶æ„ï¼Œç”¨äºå¤„ç†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚ç©ºé—´æç¤ºé€šè¿‡å°†ç©ºé—´ä½ç½®ä¿¡æ¯åµŒå…¥åˆ°åŠ¨ä½œç”Ÿæˆæ¨¡å—ä¸­ï¼Œå¼•å¯¼åŠ¨ä½œçš„ç”Ÿæˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

InternVLA-M1åœ¨SimplerEnv Google Robotä¸Šç›¸æ¯”æ— ç©ºé—´å¼•å¯¼çš„å˜ä½“æå‡äº†14.6%ï¼Œåœ¨WidowXä¸Šæå‡äº†17%ï¼Œåœ¨LIBERO Frankaä¸Šæå‡äº†4.3%ã€‚åœ¨çœŸå®ä¸–ç•Œçš„é›†ç¾¤æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ä¸­ï¼ŒInternVLA-M1æé«˜äº†7.3%ï¼Œé€šè¿‡åˆæˆååŒè®­ç»ƒï¼Œåœ¨æœªè§è¿‡çš„å¯¹è±¡å’Œæ–°é…ç½®ä¸Šå®ç°äº†20.6%çš„æå‡ã€‚åœ¨é•¿æ—¶ç¨‹æ¨ç†å¯†é›†å‹åœºæ™¯ä¸­ï¼Œå®ƒè¶…è¿‡äº†ç°æœ‰å·¥ä½œ10%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

InternVLA-M1å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå„ç§éœ€è¦æŒ‡ä»¤è·Ÿéšå’Œç©ºé—´æ¨ç†çš„æœºå™¨äººä»»åŠ¡ï¼Œå¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººå’ŒåŒ»ç–—è¾…åŠ©æœºå™¨äººã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºå®ç°æ›´æ™ºèƒ½ã€æ›´é€šç”¨çš„æœºå™¨äººï¼Œä»è€Œæé«˜ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚è‡ªä¸»å¯¼èˆªã€ç¯å¢ƒæ¢ç´¢å’Œäººæœºåä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.

