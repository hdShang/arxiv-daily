---
layout: default
title: Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control
---

# Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.13358" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.13358v1</a>
  <a href="https://arxiv.org/pdf/2510.13358.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.13358v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.13358v1', 'Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shingo Ayabe, Hiroshi Kera, Kazuhiko Kawamoto

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**å¤‡æ³¨**: 16 pages, 8 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç¦»çº¿åˆ°åœ¨çº¿çš„å¯¹æŠ—å¾®è°ƒæ–¹æ³•ï¼Œæå‡æœºå™¨äººæ§åˆ¶å¯¹æ‰°åŠ¨çš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ç¦»çº¿å¼ºåŒ–å­¦ä¹ ` `åœ¨çº¿å¾®è°ƒ` `å¯¹æŠ—è®­ç»ƒ` `é²æ£’æ§åˆ¶` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç¦»çº¿å¼ºåŒ–å­¦ä¹ è™½ç„¶é«˜æ•ˆï¼Œä½†å…¶ç­–ç•¥åœ¨é¢å¯¹çœŸå®æœºå™¨äººæ§åˆ¶ä¸­å¸¸è§çš„åŠ¨ä½œç©ºé—´æ‰°åŠ¨æ—¶è¡¨ç°è„†å¼±ã€‚
2. è®ºæ–‡æå‡ºå¯¹æŠ—å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨åŠ¨ä½œä¸­æ³¨å…¥æ‰°åŠ¨è¿›è¡Œè®­ç»ƒï¼Œæå‡ç­–ç•¥å¯¹æ‰°åŠ¨çš„é€‚åº”èƒ½åŠ›å’Œé²æ£’æ€§ã€‚
3. å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç¦»çº¿è®­ç»ƒåŸºçº¿ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œä¸”è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ èƒ½æœ‰æ•ˆå¹³è¡¡é²æ£’æ€§å’Œæ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç¦»çº¿åˆ°åœ¨çº¿çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¦»çº¿è®­ç»ƒç­–ç•¥åœ¨æ‰§è¡Œå™¨æ•…éšœç­‰åŠ¨ä½œç©ºé—´æ‰°åŠ¨ä¸‹çš„è„†å¼±æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆåœ¨å¹²å‡€æ•°æ®ä¸Šè®­ç»ƒç­–ç•¥ï¼Œç„¶åè¿›è¡Œå¯¹æŠ—å¾®è°ƒï¼Œå³åœ¨æ‰§è¡Œçš„åŠ¨ä½œä¸­æ³¨å…¥æ‰°åŠ¨ï¼Œä»¥è¯±å¯¼è¡¥å¿è¡Œä¸ºå¹¶æé«˜é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæ€§èƒ½æ„ŸçŸ¥çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡ä¿¡å·è°ƒæ•´è®­ç»ƒæœŸé—´çš„æ‰°åŠ¨æ¦‚ç‡ï¼Œä»è€Œå¹³è¡¡äº†å­¦ä¹ è¿‡ç¨‹ä¸­çš„é²æ£’æ€§å’Œç¨³å®šæ€§ã€‚åœ¨è¿ç»­æ§åˆ¶è¿åŠ¨ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å§‹ç»ˆä¼˜äºä»…ç¦»çº¿è®­ç»ƒçš„åŸºçº¿ï¼Œå¹¶ä¸”æ¯”ä»å¤´å¼€å§‹è®­ç»ƒæ”¶æ•›æ›´å¿«ã€‚åŒ¹é…å¾®è°ƒå’Œè¯„ä¼°æ¡ä»¶å¯ä»¥æœ€å¤§ç¨‹åº¦åœ°æé«˜å¯¹åŠ¨ä½œç©ºé—´æ‰°åŠ¨çš„é²æ£’æ€§ï¼Œè€Œè‡ªé€‚åº”è¯¾ç¨‹ç­–ç•¥å¯ä»¥å‡è½»çº¿æ€§è¯¾ç¨‹ç­–ç•¥ä¸­è§‚å¯Ÿåˆ°çš„æ ‡ç§°æ€§èƒ½ä¸‹é™ã€‚æ€»ä½“è€Œè¨€ï¼Œç»“æœè¡¨æ˜ï¼Œå¯¹æŠ—å¾®è°ƒèƒ½å¤Ÿå®ç°ä¸ç¡®å®šç¯å¢ƒä¸‹çš„è‡ªé€‚åº”å’Œé²æ£’æ§åˆ¶ï¼Œä»è€Œå¼¥åˆäº†ç¦»çº¿æ•ˆç‡å’Œåœ¨çº¿é€‚åº”æ€§ä¹‹é—´çš„å·®è·ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ è™½ç„¶é¿å…äº†åœ¨çº¿æ¢ç´¢çš„é£é™©ï¼Œä½†è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥åœ¨é¢å¯¹çœŸå®æœºå™¨äººæ§åˆ¶åœºæ™¯ä¸­çš„åŠ¨ä½œç©ºé—´æ‰°åŠ¨ï¼ˆä¾‹å¦‚æ‰§è¡Œå™¨æ•…éšœï¼‰æ—¶ï¼Œé²æ£’æ€§è¾ƒå·®ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæå‡ç­–ç•¥å¯¹æœªçŸ¥æ‰°åŠ¨çš„é€‚åº”èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¯¹æŠ—å¾®è°ƒï¼Œä½¿ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æš´éœ²äºå„ç§æ‰°åŠ¨ï¼Œä»è€Œå­¦ä¹ åˆ°å¯¹è¿™äº›æ‰°åŠ¨çš„è¡¥å¿è¡Œä¸ºã€‚é€šè¿‡æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å¯èƒ½å‡ºç°çš„æ•…éšœå’Œä¸ç¡®å®šæ€§ï¼Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ç¦»çº¿ç­–ç•¥è®­ç»ƒï¼šä½¿ç”¨å¹²å‡€çš„ç¦»çº¿æ•°æ®é›†è®­ç»ƒåˆå§‹ç­–ç•¥ã€‚2) å¯¹æŠ—å¾®è°ƒï¼šåœ¨åœ¨çº¿äº¤äº’è¿‡ç¨‹ä¸­ï¼Œå‘ç­–ç•¥è¾“å‡ºçš„åŠ¨ä½œæ·»åŠ æ‰°åŠ¨ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOï¼‰å¯¹ç­–ç•¥è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œå¼•å…¥æ€§èƒ½æ„ŸçŸ¥çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼ŒåŠ¨æ€è°ƒæ•´æ‰°åŠ¨æ¦‚ç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºå°†å¯¹æŠ—è®­ç»ƒçš„æ€æƒ³å¼•å…¥åˆ°ç¦»çº¿åˆ°åœ¨çº¿çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ€§èƒ½æ„ŸçŸ¥çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚è¿™ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥èƒ½å¤Ÿæ ¹æ®ç­–ç•¥çš„æ€§èƒ½åŠ¨æ€è°ƒæ•´æ‰°åŠ¨æ¦‚ç‡ï¼Œä»è€Œåœ¨é²æ£’æ€§å’Œç¨³å®šæ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**å…³é”®è®¾è®¡**ï¼šæ€§èƒ½æ„ŸçŸ¥çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰ä¿¡å·æ¥ä¼°è®¡ç­–ç•¥çš„æ€§èƒ½ã€‚æ‰°åŠ¨æ¦‚ç‡æ ¹æ®EMAä¿¡å·è¿›è¡Œè°ƒæ•´ï¼Œå½“ç­–ç•¥æ€§èƒ½ä¸‹é™æ—¶ï¼Œé™ä½æ‰°åŠ¨æ¦‚ç‡ï¼Œåä¹‹åˆ™å¢åŠ æ‰°åŠ¨æ¦‚ç‡ã€‚å…·ä½“è€Œè¨€ï¼Œæ‰°åŠ¨æ¦‚ç‡çš„æ›´æ–°å…¬å¼ä¸ºï¼š`p_t = p_{min} + (p_{max} - p_{min}) * EMA_t`ï¼Œå…¶ä¸­`p_{min}`å’Œ`p_{max}`åˆ†åˆ«æ˜¯æœ€å°å’Œæœ€å¤§æ‰°åŠ¨æ¦‚ç‡ï¼Œ`EMA_t`æ˜¯tæ—¶åˆ»çš„EMAä¿¡å·ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å¯¹æŠ—å¾®è°ƒæ–¹æ³•åœ¨å¤šä¸ªè¿ç»­æ§åˆ¶è¿åŠ¨ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æé«˜äº†ç­–ç•¥å¯¹åŠ¨ä½œç©ºé—´æ‰°åŠ¨çš„é²æ£’æ€§ã€‚ä¸ä»…ç¦»çº¿è®­ç»ƒçš„åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å„ç§æ‰°åŠ¨ï¼Œå¹¶ä¸”æ¯”ä»å¤´å¼€å§‹è®­ç»ƒæ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚æ­¤å¤–ï¼Œæ€§èƒ½æ„ŸçŸ¥çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£çº¿æ€§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä¸­è§‚å¯Ÿåˆ°çš„æ ‡ç§°æ€§èƒ½ä¸‹é™ï¼Œåœ¨é²æ£’æ€§å’Œç¨³å®šæ€§ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ§åˆ¶ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜å¯é æ€§å’Œé²æ£’æ€§çš„åœºæ™¯ä¸­ï¼Œä¾‹å¦‚å·¥ä¸šæœºå™¨äººã€æ— äººé©¾é©¶è½¦è¾†ã€åŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººå¯¹ç¯å¢ƒå˜åŒ–çš„é€‚åº”èƒ½åŠ›ï¼Œå¯ä»¥å‡å°‘æ•…éšœç‡ï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œå¹¶é™ä½ç»´æŠ¤æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚é‡‘èäº¤æ˜“ã€ç½‘ç»œå®‰å…¨ç­‰ï¼Œä»¥æé«˜ç³»ç»Ÿå¯¹å¯¹æŠ—æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Offline reinforcement learning enables sample-efficient policy acquisition without risky online interaction, yet policies trained on static datasets remain brittle under action-space perturbations such as actuator faults. This study introduces an offline-to-online framework that trains policies on clean data and then performs adversarial fine-tuning, where perturbations are injected into executed actions to induce compensatory behavior and improve resilience. A performance-aware curriculum further adjusts the perturbation probability during training via an exponential-moving-average signal, balancing robustness and stability throughout the learning process. Experiments on continuous-control locomotion tasks demonstrate that the proposed method consistently improves robustness over offline-only baselines and converges faster than training from scratch. Matching the fine-tuning and evaluation conditions yields the strongest robustness to action-space perturbations, while the adaptive curriculum strategy mitigates the degradation of nominal performance observed with the linear curriculum strategy. Overall, the results show that adversarial fine-tuning enables adaptive and robust control under uncertain environments, bridging the gap between offline efficiency and online adaptability.

