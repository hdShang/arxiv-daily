---
layout: default
title: ViTacGen: Robotic Pushing with Vision-to-Touch Generation
---

# ViTacGen: Robotic Pushing with Vision-to-Touch Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14117" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14117v2</a>
  <a href="https://arxiv.org/pdf/2510.14117.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14117v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.14117v2', 'ViTacGen: Robotic Pushing with Vision-to-Touch Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyuan Wu, Yijiong Lin, Yongqiang Zhao, Xuyang Zhang, Zhuo Chen, Nathan Lepora, Shan Luo

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15 (æ›´æ–°: 2025-10-23)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ViTacGenï¼šåŸºäºè§†è§‰åˆ°è§¦è§‰ç”Ÿæˆçš„æœºå™¨äººæ¨ç‰©æ“ä½œæ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ¨ç‰©` `è§†è§‰è§¦è§‰èåˆ` `è§†è§‰åˆ°è§¦è§‰ç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å¯¹æ¯”å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ¨ç‰©æ“ä½œä¾èµ–æ˜‚è´µçš„è§¦è§‰ä¼ æ„Ÿå™¨ï¼Œæˆ–é¢ä¸´è§†è§‰ç­–ç•¥æ€§èƒ½ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. ViTacGené€šè¿‡è§†è§‰åˆ°è§¦è§‰ç”Ÿæˆï¼Œå°†è§†è§‰ä¿¡æ¯è½¬åŒ–ä¸ºè§¦è§‰è¡¨å¾ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œæ— éœ€çœŸå®è§¦è§‰ä¼ æ„Ÿå™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒViTacGenåœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œæ¨ç‰©æˆåŠŸç‡é«˜è¾¾86%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨äººæ¨ç‰©æ“ä½œæ˜¯ä¸€é¡¹åŸºç¡€æ“ä½œä»»åŠ¡ï¼Œéœ€è¦è§¦è§‰åé¦ˆæ¥æ•æ‰æœ«ç«¯æ‰§è¡Œå™¨å’Œç‰©ä½“ä¹‹é—´ç»†å¾®çš„æ¥è§¦åŠ›å’ŒåŠ¨åŠ›å­¦ã€‚ç„¶è€Œï¼ŒçœŸå®çš„è§¦è§‰ä¼ æ„Ÿå™¨é€šå¸¸é¢ä¸´é«˜æˆæœ¬ã€æ˜“æŸåç­‰ç¡¬ä»¶é™åˆ¶ï¼Œä»¥åŠæ ¡å‡†å’Œä¸åŒä¼ æ„Ÿå™¨ä¹‹é—´å·®å¼‚çš„éƒ¨ç½²æŒ‘æˆ˜ï¼Œè€Œä»…ä½¿ç”¨è§†è§‰çš„ç­–ç•¥éš¾ä»¥è·å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚å—åˆ°äººç±»ä»è§†è§‰æ¨æ–­è§¦è§‰çŠ¶æ€èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ViTacGenï¼Œä¸€ç§æ–°é¢–çš„æœºå™¨äººæ“ä½œæ¡†æ¶ï¼Œä¸“ä¸ºè§†è§‰æœºå™¨äººæ¨ç‰©è€Œè®¾è®¡ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸­çš„è§†è§‰åˆ°è§¦è§‰ç”Ÿæˆæ¥æ¶ˆé™¤å¯¹é«˜åˆ†è¾¨ç‡çœŸå®è§¦è§‰ä¼ æ„Ÿå™¨çš„ä¾èµ–ï¼Œä»è€Œåœ¨ä»…è§†è§‰æœºå™¨äººç³»ç»Ÿä¸Šå®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬éƒ¨ç½²ã€‚å…·ä½“æ¥è¯´ï¼ŒViTacGenåŒ…å«ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨è§†è§‰åˆ°è§¦è§‰ç”Ÿæˆç½‘ç»œï¼Œè¯¥ç½‘ç»œç›´æ¥ä»è§†è§‰å›¾åƒåºåˆ—ç”Ÿæˆæ¥è§¦æ·±åº¦å›¾åƒï¼ˆä¸€ç§æ ‡å‡†åŒ–çš„è§¦è§‰è¡¨ç¤ºï¼‰ï¼Œç„¶åæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºè§†è§‰å’Œç”Ÿæˆçš„è§¦è§‰è§‚æµ‹ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ èåˆè§†è§‰-è§¦è§‰æ•°æ®ã€‚æˆ‘ä»¬åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­éªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†é«˜è¾¾86%çš„æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœºå™¨äººæ¨ç‰©æ“ä½œéœ€è¦ç²¾ç¡®çš„è§¦è§‰åé¦ˆï¼Œä½†ç°æœ‰è§¦è§‰ä¼ æ„Ÿå™¨æˆæœ¬é«˜æ˜‚ã€æ˜“æŸåï¼Œä¸”éƒ¨ç½²æ ¡å‡†å›°éš¾ã€‚ä»…ä¾èµ–è§†è§‰ä¿¡æ¯çš„ç­–ç•¥éš¾ä»¥æ•æ‰ç‰©ä½“é—´çš„ç»†å¾®æ¥è§¦ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ç¼ºä¹å¯é è§¦è§‰ä¼ æ„Ÿå™¨çš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜æ€§èƒ½çš„æœºå™¨äººæ¨ç‰©æ“ä½œæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šViTacGençš„æ ¸å¿ƒæ€è·¯æ˜¯æ¨¡ä»¿äººç±»ä»è§†è§‰æ¨æ–­è§¦è§‰çš„èƒ½åŠ›ï¼Œé€šè¿‡è§†è§‰ä¿¡æ¯ç”Ÿæˆè§¦è§‰è¡¨å¾ï¼Œä»è€Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­åˆ©ç”¨è§†è§‰å’Œç”Ÿæˆçš„è§¦è§‰ä¿¡æ¯è¿›è¡Œç­–ç•¥å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹çœŸå®è§¦è§‰ä¼ æ„Ÿå™¨çš„ä¾èµ–ï¼Œé™ä½äº†æˆæœ¬å’Œéƒ¨ç½²éš¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šViTacGenæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰åˆ°è§¦è§‰ç”Ÿæˆç½‘ç»œå’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚é¦–å…ˆï¼Œè§†è§‰åˆ°è§¦è§‰ç”Ÿæˆç½‘ç»œï¼ˆä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼‰ä»è§†è§‰å›¾åƒåºåˆ—ä¸­ç”Ÿæˆæ¥è§¦æ·±åº¦å›¾åƒï¼Œä½œä¸ºæ ‡å‡†åŒ–çš„è§¦è§‰è¡¨ç¤ºã€‚ç„¶åï¼Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥å°†è§†è§‰ä¿¡æ¯å’Œç”Ÿæˆçš„è§¦è§‰ä¿¡æ¯èåˆï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œæœ€ç»ˆå­¦ä¹ åˆ°æœ‰æ•ˆçš„æ¨ç‰©ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šViTacGençš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨è§†è§‰ä¿¡æ¯ç”Ÿæˆè§¦è§‰è¡¨å¾ï¼Œå¹¶å°†å…¶ä¸è§†è§‰ä¿¡æ¯èåˆï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•å°†è§†è§‰å’Œè§¦è§‰ä¿¡æ¯ç»“åˆèµ·æ¥ï¼Œå…‹æœäº†ä»…ä½¿ç”¨è§†è§‰ä¿¡æ¯çš„å±€é™æ€§ï¼ŒåŒæ—¶é¿å…äº†å¯¹çœŸå®è§¦è§‰ä¼ æ„Ÿå™¨çš„ä¾èµ–ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è§†è§‰å’Œè§¦è§‰ä¿¡æ¯ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šè§†è§‰åˆ°è§¦è§‰ç”Ÿæˆç½‘ç»œé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œç¼–ç å™¨æå–è§†è§‰ç‰¹å¾ï¼Œè§£ç å™¨ç”Ÿæˆæ¥è§¦æ·±åº¦å›¾åƒã€‚å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ è§†è§‰å’Œç”Ÿæˆçš„è§¦è§‰ä¿¡æ¯ä¹‹é—´çš„å…±åŒè¡¨ç¤ºã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®ï¼ˆå¦‚å·ç§¯æ ¸å¤§å°ã€å±‚æ•°ç­‰ï¼‰ä»¥åŠå¯¹æ¯”å­¦ä¹ çš„æ¸©åº¦å‚æ•°ç­‰ç»†èŠ‚ï¼Œéœ€è¦åœ¨å®éªŒä¸­è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ViTacGenåœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­å‡å–å¾—äº†æ˜¾è‘—æˆæœã€‚åœ¨çœŸå®æœºå™¨äººæ¨ç‰©ä»»åŠ¡ä¸­ï¼ŒViTacGençš„æˆåŠŸç‡é«˜è¾¾86%ï¼Œæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨è§†è§‰ä¿¡æ¯çš„åŸºçº¿æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡è§†è§‰åˆ°è§¦è§‰ç”Ÿæˆï¼ŒViTacGenèƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æœºå™¨äººæ¨ç‰©æ“ä½œçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ViTacGenå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ã€ç‰©æµåˆ†æ‹£ã€å®¶åº­æœåŠ¡ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸‹è¿›è¡Œç‰©ä½“æ“ä½œçš„æœºå™¨äººï¼Œå¯ä»¥åˆ©ç”¨ViTacGenåœ¨ç¼ºä¹è§¦è§‰ä¼ æ„Ÿå™¨çš„æƒ…å†µä¸‹ï¼Œå®ç°ç²¾ç¡®çš„ç‰©ä½“æŠ“å–å’Œæ”¾ç½®ã€‚è¯¥ç ”ç©¶é™ä½äº†æœºå™¨äººæ“ä½œå¯¹ç¡¬ä»¶çš„ä¾èµ–ï¼Œä¿ƒè¿›äº†ä½æˆæœ¬ã€é«˜é²æ£’æ€§æœºå™¨äººç³»ç»Ÿçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic pushing is a fundamental manipulation task that requires tactile feedback to capture subtle contact forces and dynamics between the end-effector and the object. However, real tactile sensors often face hardware limitations such as high costs and fragility, and deployment challenges involving calibration and variations between different sensors, while vision-only policies struggle with satisfactory performance. Inspired by humans' ability to infer tactile states from vision, we propose ViTacGen, a novel robot manipulation framework designed for visual robotic pushing with vision-to-touch generation in reinforcement learning to eliminate the reliance on high-resolution real tactile sensors, enabling effective zero-shot deployment on visual-only robotic systems. Specifically, ViTacGen consists of an encoder-decoder vision-to-touch generation network that generates contact depth images, a standardized tactile representation, directly from visual image sequence, followed by a reinforcement learning policy that fuses visual-tactile data with contrastive learning based on visual and generated tactile observations. We validate the effectiveness of our approach in both simulation and real world experiments, demonstrating its superior performance and achieving a success rate of up to 86\%.

