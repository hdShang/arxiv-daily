---
layout: default
title: Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning
---

# Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.14065" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.14065v1</a>
  <a href="https://arxiv.org/pdf/2510.14065.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14065v1" onclick="toggleFavorite(this, '2510.14065v1', 'Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gaoyuan Liu, Joris de Winter, Yuri Durodie, Denis Steckelmacher, Ann Nowe, Bram Vanderborght

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-15

**DOI**: [10.1109/LRA.2024.3398402](https://doi.org/10.1109/LRA.2024.3398402)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¹è§‚å¼ºåŒ–å­¦ä¹ çš„æŠ€èƒ½æ’å…¥æ–¹æ³•ï¼Œè§£å†³ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ä¸­æ¦‚ç‡åŠ¨ä½œçš„æŒ‘æˆ˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `æŠ€èƒ½å­¦ä¹ ` `æ¦‚ç‡åŠ¨ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»ŸTAMPæ–¹æ³•åœ¨å¤„ç†å…·æœ‰ä¸ç¡®å®šæ€§çš„æ¦‚ç‡åŠ¨ä½œæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥æœ‰æ•ˆè§„åˆ’ã€‚
2. è¯¥æ–¹æ³•å°†å¼ºåŒ–å­¦ä¹ æŠ€èƒ½èå…¥TAMPæµç¨‹ï¼Œåˆ©ç”¨RLæŠ€èƒ½çš„é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒåµŒå…¥RLæŠ€èƒ½èƒ½å¤Ÿæ‰©å±•TAMPåœ¨æ¦‚ç‡æŠ€èƒ½é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶æå‡è§„åˆ’æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¼ºåŒ–å­¦ä¹ (RL)æŠ€èƒ½é›†æˆåˆ°ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’(TAMP)æµç¨‹ä¸­çš„æ–¹æ³•ã€‚é’ˆå¯¹æœºå™¨äººæ“ä½œï¼ŒTAMPéœ€è¦æ¶‰åŠé€šç”¨åŠ¨ä½œå’ŒæŠ€èƒ½çš„é•¿æœŸæ¨ç†ã€‚ç¡®å®šæ€§åŠ¨ä½œå¯ä»¥é€šè¿‡é‡‡æ ·æˆ–çº¦æŸä¼˜åŒ–æ¥è®¾è®¡ï¼Œä½†è§„åˆ’å…·æœ‰ä¸ç¡®å®šæ€§çš„åŠ¨ä½œï¼ˆå³æ¦‚ç‡åŠ¨ä½œï¼‰ä»ç„¶æ˜¯TAMPçš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¦ä¸€æ–¹é¢ï¼Œå¼ºåŒ–å­¦ä¹ æ“…é•¿è·å–é€šç”¨ä½†çŸ­æœŸçš„ã€å¯¹ä¸ç¡®å®šæ€§å…·æœ‰é²æ£’æ€§çš„æ“ä½œæŠ€èƒ½ã€‚é™¤äº†ç­–ç•¥ä¹‹å¤–ï¼ŒRLæŠ€èƒ½è¿˜è¢«å®šä¹‰ä¸ºæ•°æ®é©±åŠ¨çš„é€»è¾‘ç»„ä»¶ï¼Œä½¿æŠ€èƒ½èƒ½å¤Ÿé€šè¿‡ç¬¦å·è§„åˆ’è¿›è¡Œéƒ¨ç½²ã€‚è®¾è®¡äº†ä¸€ä¸ªè®¡åˆ’ç»†åŒ–å­ç¨‹åºï¼Œä»¥è¿›ä¸€æ­¥è§£å†³ä¸å¯é¿å…çš„ä¸ç¡®å®šæ€§å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡åµŒå…¥RLæŠ€èƒ½ï¼Œè¯¥æ–¹æ³•æ‰©å±•äº†TAMPåœ¨æ¦‚ç‡æŠ€èƒ½é¢†åŸŸçš„èƒ½åŠ›ï¼Œå¹¶æé«˜äº†è§„åˆ’æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ—¨åœ¨ä¸ºæœºå™¨äººæ“ä½œç”Ÿæˆé•¿æœŸåŠ¨ä½œåºåˆ—ã€‚ç„¶è€Œï¼Œå½“åŠ¨ä½œå…·æœ‰ä¸ç¡®å®šæ€§ï¼ˆä¾‹å¦‚ï¼Œç”±äºæ‰§è¡Œè¯¯å·®æˆ–ç¯å¢ƒå˜åŒ–ï¼‰æ—¶ï¼Œä¼ ç»Ÿçš„TAMPæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°è¿›è¡Œè§„åˆ’ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç¡®å®šæ€§åŠ¨ä½œæˆ–ç®€åŒ–æ¨¡å‹ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤„ç†ä¸ç¡®å®šæ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾—åˆ°çš„æŠ€èƒ½ï¼ˆRL skillsï¼‰åµŒå…¥åˆ°TAMPæ¡†æ¶ä¸­ã€‚RLæŠ€èƒ½æ“…é•¿å¤„ç†ä¸ç¡®å®šæ€§ï¼Œå¹¶ä¸”å…·æœ‰ä¸€å®šçš„é€šç”¨æ€§ã€‚é€šè¿‡å°†RLæŠ€èƒ½ä½œä¸ºTAMPä¸­çš„åŸºæœ¬åŠ¨ä½œå•å…ƒï¼Œå¯ä»¥æ‰©å±•TAMPå¤„ç†æ¦‚ç‡åŠ¨ä½œçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªè®¡åˆ’ç»†åŒ–å­ç¨‹åºï¼Œç”¨äºè¿›ä¸€æ­¥è§£å†³ä¸ç¡®å®šæ€§å¸¦æ¥çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) RLæŠ€èƒ½å­¦ä¹ æ¨¡å—ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒå¾—åˆ°ä¸€ç³»åˆ—æ“ä½œæŠ€èƒ½ï¼Œæ¯ä¸ªæŠ€èƒ½å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„æ“ä½œä»»åŠ¡ã€‚2) æŠ€èƒ½æè¿°æ¨¡å—ï¼šä¸ºæ¯ä¸ªRLæŠ€èƒ½ç”Ÿæˆæ•°æ®é©±åŠ¨çš„é€»è¾‘ç»„ä»¶ï¼ŒåŒ…æ‹¬å‰ææ¡ä»¶ã€åç½®æ¡ä»¶å’Œæ•ˆæœæ¨¡å‹ã€‚è¿™äº›é€»è¾‘ç»„ä»¶ç”¨äºç¬¦å·è§„åˆ’å™¨è¿›è¡Œæ¨ç†ã€‚3) TAMPè§„åˆ’æ¨¡å—ï¼šä½¿ç”¨ç¬¦å·è§„åˆ’å™¨ç”Ÿæˆä¸€ä¸ªé«˜å±‚æ¬¡çš„åŠ¨ä½œåºåˆ—ï¼Œå…¶ä¸­åŠ¨ä½œå¯¹åº”äºRLæŠ€èƒ½ã€‚4) è®¡åˆ’ç»†åŒ–æ¨¡å—ï¼šé’ˆå¯¹TAMPç”Ÿæˆçš„åˆå§‹è®¡åˆ’ï¼Œä½¿ç”¨åŸºäºä¼˜åŒ–çš„æ–¹æ³•è¿›è¡Œç»†åŒ–ï¼Œä»¥è§£å†³ä¸ç¡®å®šæ€§å¸¦æ¥çš„å½±å“ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ æŠ€èƒ½ä¸ç¬¦å·è§„åˆ’ç›¸ç»“åˆï¼Œä»è€Œå®ç°äº†å¯¹å…·æœ‰ä¸ç¡®å®šæ€§çš„ä»»åŠ¡è¿›è¡Œé«˜æ•ˆè§„åˆ’ã€‚ä¸ä¼ ç»Ÿçš„TAMPæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†æ¦‚ç‡åŠ¨ä½œï¼Œå¹¶ä¸”å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚ä¸çº¯ç²¹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¿›è¡Œé•¿æœŸæ¨ç†ï¼Œå¹¶ä¸”å¯ä»¥åˆ©ç”¨ç¬¦å·è§„åˆ’å™¨çš„é¢†åŸŸçŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šRLæŠ€èƒ½ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼‰è¿›è¡Œè®­ç»ƒã€‚æŠ€èƒ½æè¿°æ¨¡å—ä½¿ç”¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œä»RLæŠ€èƒ½çš„ç»éªŒæ•°æ®ä¸­å­¦ä¹ é€»è¾‘ç»„ä»¶ã€‚è®¡åˆ’ç»†åŒ–æ¨¡å—ä½¿ç”¨åŸºäºä¼˜åŒ–çš„æ–¹æ³•ï¼Œä¾‹å¦‚åºåˆ—äºŒæ¬¡è§„åˆ’ï¼ˆSQPï¼‰ï¼Œæ¥è°ƒæ•´åŠ¨ä½œå‚æ•°ï¼Œä»¥æœ€å°åŒ–æˆæœ¬å‡½æ•°ã€‚æˆæœ¬å‡½æ•°å¯èƒ½åŒ…æ‹¬ç›®æ ‡çŠ¶æ€çš„è·ç¦»ã€åŠ¨ä½œæ‰§è¡Œçš„ä»£ä»·ç­‰ï¼ˆå…·ä½“å½¢å¼æœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„TAMPæ–¹æ³•å’Œçº¯ç²¹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜è§„åˆ’æ•ˆç‡å’Œé²æ£’æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨æ‘˜è¦ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œéœ€è¦æŸ¥é˜…åŸæ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚è£…é…ã€æŠ“å–ã€å¯¼èˆªç­‰ã€‚ç‰¹åˆ«æ˜¯åœ¨ç¯å¢ƒå¤æ‚ã€åŠ¨ä½œå…·æœ‰ä¸ç¡®å®šæ€§çš„åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººçš„è§„åˆ’æ•ˆç‡å’Œé²æ£’æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ™ºèƒ½åˆ¶é€ ã€å®¶åº­æœåŠ¡æœºå™¨äººç­‰é¢†åŸŸï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„æœºå™¨äººæ“ä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Task and motion planning (TAMP) for robotics manipulation necessitates long-horizon reasoning involving versatile actions and skills. While deterministic actions can be crafted by sampling or optimizing with certain constraints, planning actions with uncertainty, i.e., probabilistic actions, remains a challenge for TAMP. On the contrary, Reinforcement Learning (RL) excels in acquiring versatile, yet short-horizon, manipulation skills that are robust with uncertainties. In this letter, we design a method that integrates RL skills into TAMP pipelines. Besides the policy, a RL skill is defined with data-driven logical components that enable the skill to be deployed by symbolic planning. A plan refinement sub-routine is designed to further tackle the inevitable effect uncertainties. In the experiments, we compare our method with baseline hierarchical planning from both TAMP and RL fields and illustrate the strength of the method. The results show that by embedding RL skills, we extend the capability of TAMP to domains with probabilistic skills, and improve the planning efficiency compared to the previous methods.

