---
layout: default
title: Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning
---

# Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.14065" target="_blank" class="toolbar-btn">arXiv: 2510.14065v1</a>
    <a href="https://arxiv.org/pdf/2510.14065.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.14065v1" 
            onclick="toggleFavorite(this, '2510.14065v1', 'Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Gaoyuan Liu, Joris de Winter, Yuri Durodie, Denis Steckelmacher, Ann Nowe, Bram Vanderborght

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-15

**DOI**: [10.1109/LRA.2024.3398402](https://doi.org/10.1109/LRA.2024.3398402)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫é‰πêËßÇÂº∫ÂåñÂ≠¶‰π†ÁöÑÊäÄËÉΩÊèíÂÖ•ÊñπÊ≥ïÔºåËß£ÂÜ≥‰ªªÂä°ÂíåËøêÂä®ËßÑÂàí‰∏≠Ê¶ÇÁéáÂä®‰ΩúÁöÑÊåëÊàò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `‰ªªÂä°ÂíåËøêÂä®ËßÑÂàí` `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ÊäÄËÉΩÂ≠¶‰π†` `Ê¶ÇÁéáÂä®‰Ωú`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüTAMPÊñπÊ≥ïÂú®Â§ÑÁêÜÂÖ∑Êúâ‰∏çÁ°ÆÂÆöÊÄßÁöÑÊ¶ÇÁéáÂä®‰ΩúÊó∂Èù¢‰∏¥ÊåëÊàòÔºåÈöæ‰ª•ÊúâÊïàËßÑÂàí„ÄÇ
2. ËØ•ÊñπÊ≥ïÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÊäÄËÉΩËûçÂÖ•TAMPÊµÅÁ®ãÔºåÂà©Áî®RLÊäÄËÉΩÁöÑÈ≤ÅÊ£íÊÄßÂíåÈÄöÁî®ÊÄß„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂµåÂÖ•RLÊäÄËÉΩËÉΩÂ§üÊâ©Â±ïTAMPÂú®Ê¶ÇÁéáÊäÄËÉΩÈ¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÂπ∂ÊèêÂçáËßÑÂàíÊïàÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ∞ÜÂº∫ÂåñÂ≠¶‰π†(RL)ÊäÄËÉΩÈõÜÊàêÂà∞‰ªªÂä°ÂíåËøêÂä®ËßÑÂàí(TAMP)ÊµÅÁ®ã‰∏≠ÁöÑÊñπÊ≥ï„ÄÇÈíàÂØπÊú∫Âô®‰∫∫Êìç‰ΩúÔºåTAMPÈúÄË¶ÅÊ∂âÂèäÈÄöÁî®Âä®‰ΩúÂíåÊäÄËÉΩÁöÑÈïøÊúüÊé®ÁêÜ„ÄÇÁ°ÆÂÆöÊÄßÂä®‰ΩúÂèØ‰ª•ÈÄöËøáÈááÊ†∑ÊàñÁ∫¶Êùü‰ºòÂåñÊù•ËÆæËÆ°Ôºå‰ΩÜËßÑÂàíÂÖ∑Êúâ‰∏çÁ°ÆÂÆöÊÄßÁöÑÂä®‰ΩúÔºàÂç≥Ê¶ÇÁéáÂä®‰ΩúÔºâ‰ªçÁÑ∂ÊòØTAMPÁöÑ‰∏Ä‰∏™ÊåëÊàò„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÂº∫ÂåñÂ≠¶‰π†ÊìÖÈïøËé∑ÂèñÈÄöÁî®‰ΩÜÁü≠ÊúüÁöÑ„ÄÅÂØπ‰∏çÁ°ÆÂÆöÊÄßÂÖ∑ÊúâÈ≤ÅÊ£íÊÄßÁöÑÊìç‰ΩúÊäÄËÉΩ„ÄÇÈô§‰∫ÜÁ≠ñÁï•‰πãÂ§ñÔºåRLÊäÄËÉΩËøòË¢´ÂÆö‰πâ‰∏∫Êï∞ÊçÆÈ©±Âä®ÁöÑÈÄªËæëÁªÑ‰ª∂Ôºå‰ΩøÊäÄËÉΩËÉΩÂ§üÈÄöËøáÁ¨¶Âè∑ËßÑÂàíËøõË°åÈÉ®ÁΩ≤„ÄÇËÆæËÆ°‰∫Ü‰∏Ä‰∏™ËÆ°ÂàíÁªÜÂåñÂ≠êÁ®ãÂ∫èÔºå‰ª•Ëøõ‰∏ÄÊ≠•Ëß£ÂÜ≥‰∏çÂèØÈÅøÂÖçÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂΩ±Âìç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄöËøáÂµåÂÖ•RLÊäÄËÉΩÔºåËØ•ÊñπÊ≥ïÊâ©Â±ï‰∫ÜTAMPÂú®Ê¶ÇÁéáÊäÄËÉΩÈ¢ÜÂüüÁöÑËÉΩÂäõÔºåÂπ∂ÊèêÈ´ò‰∫ÜËßÑÂàíÊïàÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**Ôºö‰ªªÂä°ÂíåËøêÂä®ËßÑÂàíÔºàTAMPÔºâÊó®Âú®‰∏∫Êú∫Âô®‰∫∫Êìç‰ΩúÁîüÊàêÈïøÊúüÂä®‰ΩúÂ∫èÂàó„ÄÇÁÑ∂ËÄåÔºåÂΩìÂä®‰ΩúÂÖ∑Êúâ‰∏çÁ°ÆÂÆöÊÄßÔºà‰æãÂ¶ÇÔºåÁî±‰∫éÊâßË°åËØØÂ∑ÆÊàñÁéØÂ¢ÉÂèòÂåñÔºâÊó∂Ôºå‰º†ÁªüÁöÑTAMPÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂú∞ËøõË°åËßÑÂàí„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÁ°ÆÂÆöÊÄßÂä®‰ΩúÊàñÁÆÄÂåñÊ®°ÂûãÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®Â§ÑÁêÜ‰∏çÁ°ÆÂÆöÊÄßÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËØ•ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÂæóÂà∞ÁöÑÊäÄËÉΩÔºàRL skillsÔºâÂµåÂÖ•Âà∞TAMPÊ°ÜÊû∂‰∏≠„ÄÇRLÊäÄËÉΩÊìÖÈïøÂ§ÑÁêÜ‰∏çÁ°ÆÂÆöÊÄßÔºåÂπ∂‰∏îÂÖ∑Êúâ‰∏ÄÂÆöÁöÑÈÄöÁî®ÊÄß„ÄÇÈÄöËøáÂ∞ÜRLÊäÄËÉΩ‰Ωú‰∏∫TAMP‰∏≠ÁöÑÂü∫Êú¨Âä®‰ΩúÂçïÂÖÉÔºåÂèØ‰ª•Êâ©Â±ïTAMPÂ§ÑÁêÜÊ¶ÇÁéáÂä®‰ΩúÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™ËÆ°ÂàíÁªÜÂåñÂ≠êÁ®ãÂ∫èÔºåÁî®‰∫éËøõ‰∏ÄÊ≠•Ëß£ÂÜ≥‰∏çÁ°ÆÂÆöÊÄßÂ∏¶Êù•ÁöÑÂΩ±Âìç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) RLÊäÄËÉΩÂ≠¶‰π†Ê®°ÂùóÔºö‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïËÆ≠ÁªÉÂæóÂà∞‰∏ÄÁ≥ªÂàóÊìç‰ΩúÊäÄËÉΩÔºåÊØè‰∏™ÊäÄËÉΩÂØπÂ∫î‰∏Ä‰∏™ÁâπÂÆöÁöÑÊìç‰Ωú‰ªªÂä°„ÄÇ2) ÊäÄËÉΩÊèèËø∞Ê®°ÂùóÔºö‰∏∫ÊØè‰∏™RLÊäÄËÉΩÁîüÊàêÊï∞ÊçÆÈ©±Âä®ÁöÑÈÄªËæëÁªÑ‰ª∂ÔºåÂåÖÊã¨ÂâçÊèêÊù°‰ª∂„ÄÅÂêéÁΩÆÊù°‰ª∂ÂíåÊïàÊûúÊ®°Âûã„ÄÇËøô‰∫õÈÄªËæëÁªÑ‰ª∂Áî®‰∫éÁ¨¶Âè∑ËßÑÂàíÂô®ËøõË°åÊé®ÁêÜ„ÄÇ3) TAMPËßÑÂàíÊ®°ÂùóÔºö‰ΩøÁî®Á¨¶Âè∑ËßÑÂàíÂô®ÁîüÊàê‰∏Ä‰∏™È´òÂ±ÇÊ¨°ÁöÑÂä®‰ΩúÂ∫èÂàóÔºåÂÖ∂‰∏≠Âä®‰ΩúÂØπÂ∫î‰∫éRLÊäÄËÉΩ„ÄÇ4) ËÆ°ÂàíÁªÜÂåñÊ®°ÂùóÔºöÈíàÂØπTAMPÁîüÊàêÁöÑÂàùÂßãËÆ°ÂàíÔºå‰ΩøÁî®Âü∫‰∫é‰ºòÂåñÁöÑÊñπÊ≥ïËøõË°åÁªÜÂåñÔºå‰ª•Ëß£ÂÜ≥‰∏çÁ°ÆÂÆöÊÄßÂ∏¶Êù•ÁöÑÂΩ±Âìç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÊäÄËÉΩ‰∏éÁ¨¶Âè∑ËßÑÂàíÁõ∏ÁªìÂêàÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂØπÂÖ∑Êúâ‰∏çÁ°ÆÂÆöÊÄßÁöÑ‰ªªÂä°ËøõË°åÈ´òÊïàËßÑÂàí„ÄÇ‰∏é‰º†ÁªüÁöÑTAMPÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂ§ÑÁêÜÊ¶ÇÁéáÂä®‰ΩúÔºåÂπ∂‰∏îÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ‰∏éÁ∫ØÁ≤πÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üËøõË°åÈïøÊúüÊé®ÁêÜÔºåÂπ∂‰∏îÂèØ‰ª•Âà©Áî®Á¨¶Âè∑ËßÑÂàíÂô®ÁöÑÈ¢ÜÂüüÁü•ËØÜ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöRLÊäÄËÉΩ‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºàÂÖ∑‰ΩìÁÆóÊ≥ïÊú™Áü•ÔºâËøõË°åËÆ≠ÁªÉ„ÄÇÊäÄËÉΩÊèèËø∞Ê®°Âùó‰ΩøÁî®Êï∞ÊçÆÈ©±Âä®ÁöÑÊñπÊ≥ïÔºå‰ªéRLÊäÄËÉΩÁöÑÁªèÈ™åÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÈÄªËæëÁªÑ‰ª∂„ÄÇËÆ°ÂàíÁªÜÂåñÊ®°Âùó‰ΩøÁî®Âü∫‰∫é‰ºòÂåñÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÂ∫èÂàó‰∫åÊ¨°ËßÑÂàíÔºàSQPÔºâÔºåÊù•Ë∞ÉÊï¥Âä®‰ΩúÂèÇÊï∞Ôºå‰ª•ÊúÄÂ∞èÂåñÊàêÊú¨ÂáΩÊï∞„ÄÇÊàêÊú¨ÂáΩÊï∞ÂèØËÉΩÂåÖÊã¨ÁõÆÊ†áÁä∂ÊÄÅÁöÑË∑ùÁ¶ª„ÄÅÂä®‰ΩúÊâßË°åÁöÑ‰ª£‰ª∑Á≠âÔºàÂÖ∑‰ΩìÂΩ¢ÂºèÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ËÆ∫ÊñáÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏é‰º†ÁªüÁöÑTAMPÊñπÊ≥ïÂíåÁ∫ØÁ≤πÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊòæËëóÊèêÈ´òËßÑÂàíÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊï∞ÊçÆÂíåÊèêÂçáÂπÖÂ∫¶Âú®ÊëòË¶Å‰∏≠Êú™ÊòéÁ°ÆÁªôÂá∫ÔºåÈúÄË¶ÅÊü•ÈòÖÂéüÊñá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇË£ÖÈÖç„ÄÅÊäìÂèñ„ÄÅÂØºËà™Á≠â„ÄÇÁâπÂà´ÊòØÂú®ÁéØÂ¢ÉÂ§çÊùÇ„ÄÅÂä®‰ΩúÂÖ∑Êúâ‰∏çÁ°ÆÂÆöÊÄßÁöÑÂú∫ÊôØ‰∏ãÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊòæËëóÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑËßÑÂàíÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÂ∫îÁî®‰∫éÊô∫ËÉΩÂà∂ÈÄ†„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫Á≠âÈ¢ÜÂüüÔºåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÂèØÈù†ÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Task and motion planning (TAMP) for robotics manipulation necessitates long-horizon reasoning involving versatile actions and skills. While deterministic actions can be crafted by sampling or optimizing with certain constraints, planning actions with uncertainty, i.e., probabilistic actions, remains a challenge for TAMP. On the contrary, Reinforcement Learning (RL) excels in acquiring versatile, yet short-horizon, manipulation skills that are robust with uncertainties. In this letter, we design a method that integrates RL skills into TAMP pipelines. Besides the policy, a RL skill is defined with data-driven logical components that enable the skill to be deployed by symbolic planning. A plan refinement sub-routine is designed to further tackle the inevitable effect uncertainties. In the experiments, we compare our method with baseline hierarchical planning from both TAMP and RL fields and illustrate the strength of the method. The results show that by embedding RL skills, we extend the capability of TAMP to domains with probabilistic skills, and improve the planning efficiency compared to the previous methods.

