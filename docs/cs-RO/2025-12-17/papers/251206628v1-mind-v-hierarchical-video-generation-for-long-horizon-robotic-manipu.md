---
layout: default
title: MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment
---

# MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment

**arXiv**: [2512.06628v1](https://arxiv.org/abs/2512.06628) | [PDF](https://arxiv.org/pdf/2512.06628.pdf)

**ä½œè€…**: Ruicheng Zhang, Mingyang Zhang, Jun Zhou, Zhangrui Guo, Xiaofan Liu, Zunnan Xu, Zhizhou Zhong, Puxin Yan, Haocheng Luo, Xiu Li

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-07

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MIND-Vï¼šç”¨äºŽé•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œçš„åˆ†å±‚è§†é¢‘ç”Ÿæˆæ¡†æž¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®žçŽ°ç‰©ç†å¯¹é½**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `æœºå™¨äººæ“ä½œ` `å…·èº«æ™ºèƒ½` `å¼ºåŒ–å­¦ä¹ ` `ç‰©ç†å¯¹é½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å…·èº«æ¨¡ä»¿å­¦ä¹ å—é™äºŽå¤šæ ·åŒ–ã€é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œæ•°æ®çš„ç¨€ç¼ºæ€§ï¼ŒçŽ°æœ‰æ–¹æ³•éš¾ä»¥ç”Ÿæˆå¤æ‚åŠ¨ä½œçš„é•¿è§†é¢‘ã€‚
2. MIND-Vé€šè¿‡åˆ†å±‚æ¡†æž¶ï¼Œç»“åˆè¯­ä¹‰æŽ¨ç†ã€è¡Œä¸ºæ¡¥æŽ¥å’Œè¿åŠ¨è§†é¢‘ç”Ÿæˆï¼Œå®žçŽ°ç‰©ç†åˆç†ä¸”é€»è¾‘è¿žè´¯çš„é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œè§†é¢‘åˆæˆã€‚
3. MIND-Vé‡‡ç”¨åˆ†é˜¶æ®µè§†è§‰æœªæ¥å±•å¼€ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶å¼•å…¥ç‰©ç†é¢„æµ‹ä¸€è‡´æ€§å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†é•¿æ—¶ç¨‹è§†é¢‘ç”Ÿæˆçš„æ€§èƒ½ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºMIND-Vï¼Œä¸€ä¸ªåˆ†å±‚æ¡†æž¶ï¼Œæ—¨åœ¨åˆæˆç‰©ç†ä¸Šåˆç†ä¸”é€»è¾‘ä¸Šè¿žè´¯çš„é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œè§†é¢‘ã€‚å—è®¤çŸ¥ç§‘å­¦å¯å‘ï¼ŒMIND-Vé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ¡¥æŽ¥é«˜å±‚æŽ¨ç†å’Œåƒç´ çº§åˆæˆï¼šåˆ©ç”¨é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡åž‹è¿›è¡Œä»»åŠ¡è§„åˆ’çš„è¯­ä¹‰æŽ¨ç†ä¸­å¿ƒ(SRH)ï¼›å°†æŠ½è±¡æŒ‡ä»¤è½¬æ¢ä¸ºé¢†åŸŸä¸å˜è¡¨ç¤ºçš„è¡Œä¸ºè¯­ä¹‰æ¡¥(BSB)ï¼›ä»¥åŠç”¨äºŽæ¡ä»¶è§†é¢‘æ¸²æŸ“çš„è¿åŠ¨è§†é¢‘ç”Ÿæˆå™¨(MVG)ã€‚MIND-Vé‡‡ç”¨åˆ†é˜¶æ®µè§†è§‰æœªæ¥å±•å¼€(Staged Visual Future Rollouts)è¿™ä¸€æµ‹è¯•æ—¶ä¼˜åŒ–ç­–ç•¥æ¥å¢žå¼ºé•¿æ—¶ç¨‹é²æ£’æ€§ã€‚ä¸ºäº†ä½¿ç”Ÿæˆçš„è§†é¢‘ä¸Žç‰©ç†å®šå¾‹å¯¹é½ï¼Œå¼•å…¥äº†GRPOå¼ºåŒ–å­¦ä¹ åŽè®­ç»ƒé˜¶æ®µï¼Œè¯¥é˜¶æ®µç”±ä¸€ç§æ–°é¢–çš„ç‰©ç†é¢„æµ‹ä¸€è‡´æ€§(PFC)å¥–åŠ±å¼•å¯¼ã€‚PFCåˆ©ç”¨V-JEPAä¸–ç•Œæ¨¡åž‹ï¼Œé€šè¿‡å¯¹é½ç‰¹å¾ç©ºé—´ä¸­é¢„æµ‹çš„å’Œå®žé™…çš„åŠ¨æ€æ¼”åŒ–æ¥å¼ºåˆ¶æ‰§è¡Œç‰©ç†åˆç†æ€§ã€‚MIND-Våœ¨é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œè§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨çŽ°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºå…·èº«æ•°æ®åˆæˆå»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å¯æŽ§çš„èŒƒä¾‹ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçŽ°æœ‰å…·èº«æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œè§†é¢‘ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸»è¦åŽŸå› æ˜¯ç¼ºä¹è¶³å¤Ÿå¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ï¼Œä»¥åŠéš¾ä»¥ä¿è¯ç”Ÿæˆè§†é¢‘çš„ç‰©ç†åˆç†æ€§å’Œé€»è¾‘è¿žè´¯æ€§ã€‚çŽ°æœ‰æ¨¡åž‹é€šå¸¸åªèƒ½åˆæˆçŸ­ç‰‡æ®µçš„ç®€å•åŠ¨ä½œï¼Œå¹¶ä¸”ä¾èµ–äºŽæ‰‹åŠ¨å®šä¹‰çš„è½¨è¿¹ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMIND-Vçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é«˜å±‚è¯­ä¹‰æŽ¨ç†ä¸Žåº•å±‚åƒç´ çº§è§†é¢‘ç”Ÿæˆç›¸ç»“åˆï¼Œé€šè¿‡åˆ†å±‚æž¶æž„æ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡åž‹è¿›è¡Œä»»åŠ¡è§„åˆ’ï¼Œå°†æŠ½è±¡æŒ‡ä»¤è½¬åŒ–ä¸ºé¢†åŸŸä¸å˜çš„ä¸­é—´è¡¨ç¤ºï¼Œæœ€åŽç”Ÿæˆç¬¦åˆç‰©ç†è§„å¾‹çš„è§†é¢‘ã€‚è¿™ç§åˆ†å±‚è§£è€¦çš„æ–¹å¼ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶ç”Ÿæˆæ›´é•¿æ—¶ç¨‹ã€æ›´å¤æ‚çš„æœºå™¨äººæ“ä½œè§†é¢‘ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šMIND-VåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) è¯­ä¹‰æŽ¨ç†ä¸­å¿ƒ(SRH)ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡åž‹è¿›è¡Œä»»åŠ¡è§„åˆ’ï¼Œå°†é«˜å±‚è¯­ä¹‰ä¿¡æ¯è½¬åŒ–ä¸ºä¸€ç³»åˆ—åŠ¨ä½œæŒ‡ä»¤ã€‚2) è¡Œä¸ºè¯­ä¹‰æ¡¥(BSB)ï¼šå°†æŠ½è±¡çš„åŠ¨ä½œæŒ‡ä»¤è½¬åŒ–ä¸ºé¢†åŸŸä¸å˜çš„ä¸­é—´è¡¨ç¤ºï¼Œä¾‹å¦‚æœºå™¨äººå…³èŠ‚è§’åº¦æˆ–æœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç½®ã€‚3) è¿åŠ¨è§†é¢‘ç”Ÿæˆå™¨(MVG)ï¼šæ ¹æ®ä¸­é—´è¡¨ç¤ºç”Ÿæˆåƒç´ çº§åˆ«çš„è§†é¢‘å¸§ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†åˆ†é˜¶æ®µè§†è§‰æœªæ¥å±•å¼€(Staged Visual Future Rollouts)çš„æµ‹è¯•æ—¶ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥åŠåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„åŽè®­ç»ƒé˜¶æ®µï¼Œä»¥æå‡é•¿æ—¶ç¨‹è§†é¢‘ç”Ÿæˆçš„é²æ£’æ€§å’Œç‰©ç†åˆç†æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šMIND-Vçš„å…³é”®åˆ›æ–°åœ¨äºŽå…¶åˆ†å±‚æž¶æž„å’Œç‰©ç†é¢„æµ‹ä¸€è‡´æ€§(PFC)å¥–åŠ±ã€‚åˆ†å±‚æž¶æž„ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é•¿æ—¶ç¨‹ä»»åŠ¡ï¼Œè€ŒPFCå¥–åŠ±åˆ™é€šè¿‡åˆ©ç”¨V-JEPAä¸–ç•Œæ¨¡åž‹ï¼Œå¼ºåˆ¶ç”Ÿæˆçš„è§†é¢‘åœ¨ç‰©ç†ä¸Šæ˜¯åˆç†çš„ã€‚PFCå¥–åŠ±é€šè¿‡å¯¹é½é¢„æµ‹çš„å’Œå®žé™…çš„åŠ¨æ€æ¼”åŒ–ç‰¹å¾ï¼Œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ç¬¦åˆç‰©ç†å®šå¾‹ã€‚

**å…³é”®è®¾è®¡**ï¼šPFCå¥–åŠ±çš„è®¾è®¡æ˜¯å…³é”®ã€‚å®ƒåŸºäºŽV-JEPAä¸–ç•Œæ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹èƒ½å¤Ÿé¢„æµ‹ç»™å®šçŠ¶æ€ä¸‹æœªæ¥çš„çŠ¶æ€ã€‚PFCå¥–åŠ±è®¡ç®—é¢„æµ‹çŠ¶æ€å’Œå®žé™…çŠ¶æ€ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å°†å…¶ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ï¼Œå¼•å¯¼æ¨¡åž‹ç”Ÿæˆæ›´ç¬¦åˆç‰©ç†è§„å¾‹çš„è§†é¢‘ã€‚æ­¤å¤–ï¼Œåˆ†é˜¶æ®µè§†è§‰æœªæ¥å±•å¼€ç­–ç•¥é€šè¿‡è¿­ä»£ä¼˜åŒ–æœªæ¥è§†é¢‘å¸§ï¼Œè¿›ä¸€æ­¥æå‡äº†é•¿æ—¶ç¨‹è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

MIND-Våœ¨é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚é€šè¿‡å¼•å…¥ç‰©ç†é¢„æµ‹ä¸€è‡´æ€§å¥–åŠ±å’Œåˆ†é˜¶æ®µè§†è§‰æœªæ¥å±•å¼€ç­–ç•¥ï¼Œç”Ÿæˆçš„è§†é¢‘åœ¨ç‰©ç†åˆç†æ€§å’Œé€»è¾‘è¿žè´¯æ€§æ–¹é¢å‡ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚å…·ä½“å®žéªŒæ•°æ®ï¼ˆåŽŸæ–‡æœªæä¾›ï¼Œæ­¤å¤„æœªçŸ¥ï¼‰è¡¨æ˜Žï¼ŒMIND-Vèƒ½å¤Ÿç”Ÿæˆæ›´é•¿æ—¶ç¨‹ã€æ›´å¤æ‚çš„æœºå™¨äººæ“ä½œè§†é¢‘ï¼Œå¹¶æ˜¾è‘—æå‡äº†æœºå™¨äººçš„ä»»åŠ¡å®ŒæˆçŽ‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

MIND-Våœ¨æœºå™¨äººæ“ä½œã€å…·èº«æ™ºèƒ½å’Œæ•°æ®å¢žå¼ºç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ç”¨äºŽç”Ÿæˆå¤§é‡é€¼çœŸçš„æœºå™¨äººæ“ä½œè§†é¢‘ï¼Œä»Žè€Œç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒåŠ é€Ÿæœºå™¨äººå­¦ä¹ å’Œè®­ç»ƒã€‚æ­¤å¤–ï¼ŒMIND-Vè¿˜å¯ä»¥ç”¨äºŽè™šæ‹ŸçŽ¯å¢ƒä¸­çš„æœºå™¨äººä»»åŠ¡è§„åˆ’å’ŒæŽ§åˆ¶ï¼Œä»¥åŠäººæœºåä½œç­‰åœºæ™¯ï¼Œå…·æœ‰é‡è¦çš„å®žé™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.

