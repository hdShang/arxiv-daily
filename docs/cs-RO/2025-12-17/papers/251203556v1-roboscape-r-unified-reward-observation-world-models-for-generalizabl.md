---
layout: default
title: RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL
---

# RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL

**arXiv**: [2512.03556v1](https://arxiv.org/abs/2512.03556) | [PDF](https://arxiv.org/pdf/2512.03556.pdf)

**ä½œè€…**: Yinzhou Tang, Yu Shang, Yinuo Chen, Bingwen Wei, Xin Zhang, Shu'ang Yu, Liangzhi Shi, Chao Yu, Chen Gao, Wei Wu, Yong Li

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboScape-Rï¼šé€šè¿‡ç»Ÿä¸€å¥–åŠ±-è§‚æµ‹ä¸–ç•Œæ¨¡åž‹æå‡æœºå™¨äººå¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡åž‹` `æ³›åŒ–èƒ½åŠ›` `å…·èº«æ™ºèƒ½`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç¼ºä¹ç»Ÿä¸€çš„é€šç”¨å¥–åŠ±ä¿¡å·ï¼Œéš¾ä»¥åœ¨å¤šåœºæ™¯ä¸­æ³›åŒ–ï¼Œæ¨¡ä»¿å­¦ä¹ åˆ™å®¹æ˜“è¿‡æ‹Ÿåˆä¸“å®¶è½¨è¿¹ã€‚
2. RoboScape-Råˆ©ç”¨ä¸–ç•Œæ¨¡åž‹ä½œä¸ºé€šç”¨çŽ¯å¢ƒä»£ç†ï¼Œé€šè¿‡å†…ç”Ÿå¥–åŠ±æœºåˆ¶ï¼Œæå‡å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒRoboScape-Råœ¨è¶…å‡ºé¢†åŸŸåœºæ™¯ä¸‹ï¼Œæ€§èƒ½æ¯”åŸºçº¿å¹³å‡æé«˜äº†37.5%ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®žçŽ°å¯æ³›åŒ–çš„å…·èº«æ™ºèƒ½ç­–ç•¥ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç­–ç•¥å­¦ä¹ èŒƒå¼ï¼ŒåŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œéƒ½éš¾ä»¥åœ¨ä¸åŒçš„åœºæ™¯ä¸­åŸ¹å…»æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡ä»¿å­¦ä¹ ç­–ç•¥é€šå¸¸è¿‡åº¦æ‹Ÿåˆç‰¹å®šçš„ä¸“å®¶è½¨è¿¹ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™ç¼ºä¹ç»Ÿä¸€å’Œé€šç”¨çš„å¥–åŠ±ä¿¡å·ï¼Œè¿™å¯¹äºŽæœ‰æ•ˆçš„å¤šåœºæ™¯æ³›åŒ–è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è®¤ä¸ºä¸–ç•Œæ¨¡åž‹èƒ½å¤Ÿä½œä¸ºé€šç”¨çš„çŽ¯å¢ƒä»£ç†æ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä¸–ç•Œæ¨¡åž‹ä¸»è¦å…³æ³¨é¢„æµ‹è§‚æµ‹çš„èƒ½åŠ›ï¼Œä»ç„¶ä¾èµ–äºŽç‰¹å®šä»»åŠ¡çš„æ‰‹å·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå› æ­¤æ— æ³•æä¾›çœŸæ­£é€šç”¨çš„è®­ç»ƒçŽ¯å¢ƒã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RoboScape-Rï¼Œä¸€ä¸ªåˆ©ç”¨ä¸–ç•Œæ¨¡åž‹ä½œä¸ºå¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸­å…·èº«çŽ¯å¢ƒçš„é€šç”¨ä»£ç†çš„æ¡†æž¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºŽä¸–ç•Œæ¨¡åž‹çš„æ–°åž‹é€šç”¨å¥–åŠ±æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ç”ŸæˆæºäºŽæ¨¡åž‹å¯¹çœŸå®žä¸–ç•ŒçŠ¶æ€è½¬ç§»åŠ¨æ€çš„å†…åœ¨ç†è§£çš„â€œå†…ç”Ÿâ€å¥–åŠ±ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒRoboScape-Ré€šè¿‡æä¾›é«˜æ•ˆå’Œé€šç”¨çš„è®­ç»ƒçŽ¯å¢ƒï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ï¼Œä»Žè€Œæ˜¾è‘—æé«˜äº†å…·èº«æ™ºèƒ½ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºåˆ©ç”¨ä¸–ç•Œæ¨¡åž‹ä½œä¸ºåœ¨çº¿è®­ç»ƒç­–ç•¥æä¾›äº†é‡è¦çš„è§è§£ï¼Œå¹¶ä¸”åœ¨è¶…å‡ºé¢†åŸŸåœºæ™¯ä¸‹ï¼Œæ€§èƒ½æ¯”åŸºçº¿å¹³å‡æé«˜äº†37.5%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½ç­–ç•¥åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–é—®é¢˜ã€‚çŽ°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºŽæ‰‹å·¥è®¾è®¡çš„ã€ç‰¹å®šäºŽä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æœªè§è¿‡çš„çŽ¯å¢ƒä¸­çš„è¡¨çŽ°ã€‚æ¨¡ä»¿å­¦ä¹ è™½ç„¶å¯ä»¥å­¦ä¹ ä¸“å®¶ç­–ç•¥ï¼Œä½†å®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸–ç•Œæ¨¡åž‹æ¥å­¦ä¹ çŽ¯å¢ƒçš„åŠ¨æ€ç‰¹æ€§ï¼Œå¹¶ä»Žä¸­æå–é€šç”¨çš„å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡è®©æ™ºèƒ½ä½“åœ¨ä¸–ç•Œæ¨¡åž‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥é¿å…å¯¹çœŸå®žçŽ¯å¢ƒçš„è¿‡åº¦ä¾èµ–ï¼Œä»Žè€Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºŽè®¾è®¡ä¸€ç§èƒ½å¤Ÿåæ˜ çŽ¯å¢ƒå†…åœ¨è§„å¾‹çš„å†…ç”Ÿå¥–åŠ±æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šRoboScape-Ræ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä¸–ç•Œæ¨¡åž‹ï¼šç”¨äºŽå­¦ä¹ çŽ¯å¢ƒçš„çŠ¶æ€è½¬ç§»åŠ¨æ€ï¼Œèƒ½å¤Ÿé¢„æµ‹æœªæ¥çŠ¶æ€å’Œå¥–åŠ±ã€‚2) å†…ç”Ÿå¥–åŠ±ç”Ÿæˆå™¨ï¼šåŸºäºŽä¸–ç•Œæ¨¡åž‹çš„é¢„æµ‹ï¼Œç”Ÿæˆåæ˜ çŽ¯å¢ƒå†…åœ¨è§„å¾‹çš„å¥–åŠ±ä¿¡å·ã€‚3) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼šåœ¨ä¸–ç•Œæ¨¡åž‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œä»¥æœ€å¤§åŒ–å†…ç”Ÿå¥–åŠ±ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œæ™ºèƒ½ä½“åœ¨ä¸–ç•Œæ¨¡åž‹ä¸­é‡‡å–è¡ŒåŠ¨ï¼Œä¸–ç•Œæ¨¡åž‹é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ï¼Œå†…ç”Ÿå¥–åŠ±ç”Ÿæˆå™¨æ ¹æ®é¢„æµ‹ç»“æžœç”Ÿæˆå¥–åŠ±ï¼Œæ™ºèƒ½ä½“æ ¹æ®å¥–åŠ±æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽæå‡ºäº†åŸºäºŽä¸–ç•Œæ¨¡åž‹çš„å†…ç”Ÿå¥–åŠ±æœºåˆ¶ã€‚ä¸Žä¼ ç»Ÿçš„æ‰‹å·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ä¸åŒï¼Œå†…ç”Ÿå¥–åŠ±èƒ½å¤Ÿè‡ªåŠ¨åœ°ä»ŽçŽ¯å¢ƒåŠ¨æ€ä¸­å­¦ä¹ ï¼Œä»Žè€Œæä¾›æ›´é€šç”¨å’Œé²æ£’çš„å¥–åŠ±ä¿¡å·ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹ç‰¹å®šä»»åŠ¡çš„è¿‡åº¦ä¾èµ–ï¼Œæé«˜äº†ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šä¸–ç•Œæ¨¡åž‹é€šå¸¸é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–Transformerç­‰æ¨¡åž‹ç»“æž„ï¼Œç”¨äºŽå­¦ä¹ çŽ¯å¢ƒçš„çŠ¶æ€è¡¨ç¤ºå’Œè½¬ç§»å‡½æ•°ã€‚å†…ç”Ÿå¥–åŠ±çš„è®¾è®¡å¯ä»¥åŸºäºŽå¤šç§æŒ‡æ ‡ï¼Œä¾‹å¦‚çŠ¶æ€çš„å˜åŒ–å¹…åº¦ã€ä¸Žç›®æ ‡çš„è·ç¦»ç­‰ã€‚å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨å¸¸è§çš„ç®—æ³•ï¼Œå¦‚PPOæˆ–SACã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æž„éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒRoboScape-Råœ¨è¶…å‡ºé¢†åŸŸåœºæ™¯ä¸‹ï¼Œæ€§èƒ½æ¯”åŸºçº¿æ–¹æ³•å¹³å‡æé«˜äº†37.5%ã€‚è¿™è¡¨æ˜Žè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æœªè§è¿‡çš„çŽ¯å¢ƒä¸­ä¹Ÿèƒ½è¡¨çŽ°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œå®žéªŒè¿˜éªŒè¯äº†å†…ç”Ÿå¥–åŠ±æœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜Žå…¶èƒ½å¤Ÿæä¾›æ›´é€šç”¨å’Œé²æ£’çš„å¥–åŠ±ä¿¡å·ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å¯¼èˆªã€æ“ä½œå’ŒæŽ§åˆ¶ã€‚é€šè¿‡æé«˜æœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨æ›´å¹¿æ³›çš„å®žé™…åœºæ™¯ä¸­éƒ¨ç½²ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œç¾éš¾æ•‘æ´ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„çŽ¯å¢ƒå’Œä»»åŠ¡ï¼Œå®žçŽ°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.

