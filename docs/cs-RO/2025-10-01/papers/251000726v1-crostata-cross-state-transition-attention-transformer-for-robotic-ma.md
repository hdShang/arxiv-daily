---
layout: default
title: CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation
---

# CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.00726" target="_blank" class="toolbar-btn">arXiv: 2510.00726v1</a>
    <a href="https://arxiv.org/pdf/2510.00726.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00726v1" 
            onclick="toggleFavorite(this, '2510.00726v1', 'CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Giovanni Minelli, Giulio Turrisi, Victor Barasuol, Claudio Semini

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-01

**Â§áÊ≥®**: Code and data available at https://github.com/iit-DLSLab/croSTAta

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Cross-State Transition Attention Transformer‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÊâßË°åÂèòÂºÇÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `Áä∂ÊÄÅËΩ¨Áßª` `Êó∂Èó¥Âª∫Ê®°` `Ê∑±Â∫¶Â≠¶‰π†` `Á≠ñÁï•Â≠¶‰π†` `È≤ÅÊ£íÊÄß` `Ê®°ÊãüÂÆûÈ™å`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•Â≠¶‰π†ÊñπÊ≥ïÂú®Èù¢ÂØπÊú™Ë¶ÜÁõñÁöÑÊâßË°åÂèòÂºÇÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂØºËá¥È≤ÅÊ£íÊÄß‰∏çË∂≥„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑCross-State Transition Attention TransformerÈÄöËøáÁä∂ÊÄÅËΩ¨ÁßªÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂü∫‰∫éÂéÜÂè≤Áä∂ÊÄÅÊºîÂèòÊ®°ÂºèË∞ÉËäÇÊ≥®ÊÑèÂäõÊùÉÈáçÔºå‰ªéËÄåÂ¢ûÂº∫Á≠ñÁï•ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ
3. Âú®Ê®°ÊãüÂÆûÈ™å‰∏≠ÔºåSTAÂú®ÊâÄÊúâ‰ªªÂä°‰∏äÂùá‰ºò‰∫é‰º†ÁªüÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÂíåÊó∂Èó¥Âª∫Ê®°ÊñπÊ≥ïÔºåÂ¶ÇTCNÂíåLSTMÔºåÁâπÂà´ÊòØÂú®Á≤æÂ∫¶Ë¶ÅÊ±ÇÈ´òÁöÑ‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫Ü2ÂÄç‰ª•‰∏äÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈÄöËøá‰ªéÁ§∫ËåÉ‰∏≠Â≠¶‰π†Êú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•Ôºå‰ªçÁÑ∂Èù¢‰∏¥ÊâßË°åÂèòÂºÇÁöÑÊåëÊàò„ÄÇÂ∞ΩÁÆ°ÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂ÂºïÂÖ•ÂéÜÂè≤‰∏ä‰∏ãÊñáÂèØ‰ª•ÊèêÈ´òÈ≤ÅÊ£íÊÄßÔºå‰ΩÜÊ†áÂáÜÊñπÊ≥ïÂú®Â§ÑÁêÜÊâÄÊúâËøáÂéªÁä∂ÊÄÅÊó∂Êú™ËÉΩÊòéÁ°ÆÂª∫Ê®°Á§∫ËåÉ‰∏≠ÁöÑÊó∂Èó¥ÁªìÊûÑ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçCross-State Transition Attention TransformerÔºåÈááÁî®Êñ∞È¢ñÁöÑÁä∂ÊÄÅËΩ¨ÁßªÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºàSTAÔºâÔºåÊ†πÊçÆÂ≠¶‰π†Âà∞ÁöÑÁä∂ÊÄÅÊºîÂèòÊ®°ÂºèË∞ÉËäÇÊ†áÂáÜÊ≥®ÊÑèÂäõÊùÉÈáçÔºå‰ªéËÄå‰ΩøÁ≠ñÁï•ËÉΩÂ§üÊõ¥Â•ΩÂú∞Ê†πÊçÆÊâßË°åÂéÜÂè≤Ë∞ÉÊï¥Ë°å‰∏∫„ÄÇÁªìÂêàÁªìÊûÑÂåñÊ≥®ÊÑèÂäõÂíåËÆ≠ÁªÉ‰∏≠ÁöÑÊó∂Èó¥Êé©ËîΩÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSTAÂú®ÊâÄÊúâ‰ªªÂä°‰∏≠Âùá‰ºò‰∫éÊ†áÂáÜÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÂíåÊó∂Èó¥Âª∫Ê®°ÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®Á≤æÂ∫¶ÂÖ≥ÈîÆ‰ªªÂä°‰∏äÂÆûÁé∞‰∫ÜË∂ÖËøá2ÂÄçÁöÑÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•Âú®ÊâßË°åËøáÁ®ã‰∏≠ÈÅáÂà∞ÁöÑÂèòÂºÇÈóÆÈ¢òÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂ§ÑÁêÜÁ§∫ËåÉ‰∏≠ÁöÑÊó∂Èó¥ÁªìÊûÑÔºåÂØºËá¥Á≠ñÁï•È≤ÅÊ£íÊÄß‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÁöÑCross-State Transition Attention TransformerÈÄöËøáÂºïÂÖ•Áä∂ÊÄÅËΩ¨ÁßªÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÊ†πÊçÆÂéÜÂè≤Áä∂ÊÄÅÊºîÂèòÊ®°ÂºèÂä®ÊÄÅË∞ÉÊï¥Ê≥®ÊÑèÂäõÊùÉÈáçÔºå‰ªéËÄå‰ΩøÁ≠ñÁï•Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÊâßË°åÂéÜÂè≤„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Áä∂ÊÄÅËΩ¨ÁßªÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÅÊó∂Èó¥Êé©ËîΩËÆ≠ÁªÉÂíåÊ†áÂáÜÊ≥®ÊÑèÂäõÊ®°Âùó„ÄÇÈÄöËøáÂØπÂéÜÂè≤Áä∂ÊÄÅÁöÑÂª∫Ê®°ÔºåÂ¢ûÂº∫‰∫ÜÁ≠ñÁï•ÁöÑÊó∂Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÁä∂ÊÄÅËΩ¨ÁßªÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂºïÂÖ•ÔºåÂÆÉ‰∏é‰º†ÁªüÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊçïÊçâÁä∂ÊÄÅÊºîÂèòÁöÑÊ®°ÂºèÔºå‰ªéËÄåÊèêÂçáÁ≠ñÁï•ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁΩëÁªúÁªìÊûÑ‰∏äÔºåÈááÁî®‰∫ÜÂ§öÂ±ÇTransformerÊû∂ÊûÑÔºåÂπ∂Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂºïÂÖ•Êó∂Èó¥Êé©ËîΩÁ≠ñÁï•Ôºå‰ª•ÈöèÊú∫ÂéªÈô§ÊúÄËøëÊó∂Èó¥Ê≠•ÁöÑËßÜËßâ‰ø°ÊÅØÔºå‰øÉËøõÊ®°Âûã‰ªéÂéÜÂè≤‰∏ä‰∏ãÊñá‰∏≠ËøõË°åÊé®ÁêÜ„ÄÇÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°‰∏äÔºåÁªìÂêà‰∫ÜÊ†áÂáÜÁöÑÂõûÂΩíÊçüÂ§±ÂíåÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÊçüÂ§±ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®Â≠¶‰π†ËøáÁ®ã‰∏≠ÂÖ≥Ê≥®ÈáçË¶ÅÁöÑÂéÜÂè≤‰ø°ÊÅØ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåCross-State Transition Attention TransformerÂú®ÊâÄÊúâ‰ªªÂä°‰∏≠Âùá‰ºò‰∫é‰º†ÁªüÁöÑ‰∫§ÂèâÊ≥®ÊÑèÂäõÂíåÊó∂Èó¥Âª∫Ê®°ÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂú®Á≤æÂ∫¶ÂÖ≥ÈîÆ‰ªªÂä°‰∏äÔºåÊÄßËÉΩÊèêÂçáË∂ÖËøá2ÂÄçÔºåËØÅÊòé‰∫ÜËØ•ÊñπÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Â∑•‰∏öÊú∫Âô®‰∫∫„ÄÅÊúçÂä°Êú∫Âô®‰∫∫ÂíåËá™‰∏ªÁ≥ªÁªüÁ≠âÔºåËÉΩÂ§üÊòæËëóÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúËÉΩÂäõÂíåÈÄÇÂ∫îÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÂèØËÉΩÊé®Âä®Êõ¥Êô∫ËÉΩÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÁöÑÂèëÂ±ïÔºå‰ΩøÂÖ∂Âú®Âä®ÊÄÅÂíå‰∏çÁ°ÆÂÆöÁöÑÁéØÂ¢É‰∏≠Ë°®Áé∞Êõ¥‰∏∫Âá∫Ëâ≤„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Learning robotic manipulation policies through supervised learning from demonstrations remains challenging when policies encounter execution variations not explicitly covered during training. While incorporating historical context through attention mechanisms can improve robustness, standard approaches process all past states in a sequence without explicitly modeling the temporal structure that demonstrations may include, such as failure and recovery patterns. We propose a Cross-State Transition Attention Transformer that employs a novel State Transition Attention (STA) mechanism to modulate standard attention weights based on learned state evolution patterns, enabling policies to better adapt their behavior based on execution history. Our approach combines this structured attention with temporal masking during training, where visual information is randomly removed from recent timesteps to encourage temporal reasoning from historical context. Evaluation in simulation shows that STA consistently outperforms standard cross-attention and temporal modeling approaches like TCN and LSTM networks across all tasks, achieving more than 2x improvement over cross-attention on precision-critical tasks.

