---
layout: default
title: CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation
---

# CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.00726" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.00726v1</a>
  <a href="https://arxiv.org/pdf/2510.00726.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.00726v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.00726v1', 'CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Giovanni Minelli, Giulio Turrisi, Victor Barasuol, Claudio Semini

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: Code and data available at https://github.com/iit-DLSLab/croSTAta

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCross-State Transition Attention Transformerä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ‰§è¡Œå˜å¼‚é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `æ³¨æ„åŠ›æœºåˆ¶` `çŠ¶æ€è½¬ç§»` `æ—¶é—´å»ºæ¨¡` `æ·±åº¦å­¦ä¹ ` `ç­–ç•¥å­¦ä¹ ` `é²æ£’æ€§` `æ¨¡æ‹Ÿå®éªŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººæ“ä½œç­–ç•¥å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹æœªè¦†ç›–çš„æ‰§è¡Œå˜å¼‚æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´é²æ£’æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„Cross-State Transition Attention Transformeré€šè¿‡çŠ¶æ€è½¬ç§»æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŸºäºå†å²çŠ¶æ€æ¼”å˜æ¨¡å¼è°ƒèŠ‚æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œå¢å¼ºç­–ç•¥çš„é€‚åº”æ€§ã€‚
3. åœ¨æ¨¡æ‹Ÿå®éªŒä¸­ï¼ŒSTAåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„äº¤å‰æ³¨æ„åŠ›å’Œæ—¶é—´å»ºæ¨¡æ–¹æ³•ï¼Œå¦‚TCNå’ŒLSTMï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾åº¦è¦æ±‚é«˜çš„ä»»åŠ¡ä¸­å®ç°äº†2å€ä»¥ä¸Šçš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šè¿‡ä»ç¤ºèŒƒä¸­å­¦ä¹ æœºå™¨äººæ“ä½œç­–ç•¥ï¼Œä»ç„¶é¢ä¸´æ‰§è¡Œå˜å¼‚çš„æŒ‘æˆ˜ã€‚å°½ç®¡é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥å†å²ä¸Šä¸‹æ–‡å¯ä»¥æé«˜é²æ£’æ€§ï¼Œä½†æ ‡å‡†æ–¹æ³•åœ¨å¤„ç†æ‰€æœ‰è¿‡å»çŠ¶æ€æ—¶æœªèƒ½æ˜ç¡®å»ºæ¨¡ç¤ºèŒƒä¸­çš„æ—¶é—´ç»“æ„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§Cross-State Transition Attention Transformerï¼Œé‡‡ç”¨æ–°é¢–çš„çŠ¶æ€è½¬ç§»æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSTAï¼‰ï¼Œæ ¹æ®å­¦ä¹ åˆ°çš„çŠ¶æ€æ¼”å˜æ¨¡å¼è°ƒèŠ‚æ ‡å‡†æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œä½¿ç­–ç•¥èƒ½å¤Ÿæ›´å¥½åœ°æ ¹æ®æ‰§è¡Œå†å²è°ƒæ•´è¡Œä¸ºã€‚ç»“åˆç»“æ„åŒ–æ³¨æ„åŠ›å’Œè®­ç»ƒä¸­çš„æ—¶é—´æ©è”½ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒSTAåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡ä¼˜äºæ ‡å‡†çš„äº¤å‰æ³¨æ„åŠ›å’Œæ—¶é—´å»ºæ¨¡æ–¹æ³•ï¼Œå°¤å…¶åœ¨ç²¾åº¦å…³é”®ä»»åŠ¡ä¸Šå®ç°äº†è¶…è¿‡2å€çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œç­–ç•¥åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°çš„å˜å¼‚é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†ç¤ºèŒƒä¸­çš„æ—¶é—´ç»“æ„ï¼Œå¯¼è‡´ç­–ç•¥é²æ£’æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„Cross-State Transition Attention Transformeré€šè¿‡å¼•å…¥çŠ¶æ€è½¬ç§»æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å†å²çŠ¶æ€æ¼”å˜æ¨¡å¼åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œä½¿ç­–ç•¥æ›´å¥½åœ°é€‚åº”æ‰§è¡Œå†å²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬çŠ¶æ€è½¬ç§»æ³¨æ„åŠ›æœºåˆ¶ã€æ—¶é—´æ©è”½è®­ç»ƒå’Œæ ‡å‡†æ³¨æ„åŠ›æ¨¡å—ã€‚é€šè¿‡å¯¹å†å²çŠ¶æ€çš„å»ºæ¨¡ï¼Œå¢å¼ºäº†ç­–ç•¥çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºçŠ¶æ€è½¬ç§»æ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥ï¼Œå®ƒä¸ä¼ ç»Ÿçš„äº¤å‰æ³¨æ„åŠ›æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰çŠ¶æ€æ¼”å˜çš„æ¨¡å¼ï¼Œä»è€Œæå‡ç­–ç•¥çš„é€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†å¤šå±‚Transformeræ¶æ„ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æ—¶é—´æ©è”½ç­–ç•¥ï¼Œä»¥éšæœºå»é™¤æœ€è¿‘æ—¶é—´æ­¥çš„è§†è§‰ä¿¡æ¯ï¼Œä¿ƒè¿›æ¨¡å‹ä»å†å²ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¨ç†ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œç»“åˆäº†æ ‡å‡†çš„å›å½’æŸå¤±å’ŒåŸºäºæ³¨æ„åŠ›çš„æŸå¤±ï¼Œç¡®ä¿æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å…³æ³¨é‡è¦çš„å†å²ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCross-State Transition Attention Transformeråœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡ä¼˜äºä¼ ç»Ÿçš„äº¤å‰æ³¨æ„åŠ›å’Œæ—¶é—´å»ºæ¨¡æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾åº¦å…³é”®ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æå‡è¶…è¿‡2å€ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šæœºå™¨äººã€æœåŠ¡æœºå™¨äººå’Œè‡ªä¸»ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½æ¨åŠ¨æ›´æ™ºèƒ½çš„æœºå™¨äººç³»ç»Ÿçš„å‘å±•ï¼Œä½¿å…¶åœ¨åŠ¨æ€å’Œä¸ç¡®å®šçš„ç¯å¢ƒä¸­è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning robotic manipulation policies through supervised learning from demonstrations remains challenging when policies encounter execution variations not explicitly covered during training. While incorporating historical context through attention mechanisms can improve robustness, standard approaches process all past states in a sequence without explicitly modeling the temporal structure that demonstrations may include, such as failure and recovery patterns. We propose a Cross-State Transition Attention Transformer that employs a novel State Transition Attention (STA) mechanism to modulate standard attention weights based on learned state evolution patterns, enabling policies to better adapt their behavior based on execution history. Our approach combines this structured attention with temporal masking during training, where visual information is randomly removed from recent timesteps to encourage temporal reasoning from historical context. Evaluation in simulation shows that STA consistently outperforms standard cross-attention and temporal modeling approaches like TCN and LSTM networks across all tasks, achieving more than 2x improvement over cross-attention on precision-critical tasks.

