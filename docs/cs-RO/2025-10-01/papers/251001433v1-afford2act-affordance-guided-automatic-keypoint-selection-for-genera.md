---
layout: default
title: AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation
---

# AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01433" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01433v1</a>
  <a href="https://arxiv.org/pdf/2510.01433.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01433v1" onclick="toggleFavorite(this, '2510.01433v1', 'AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Anukriti Singh, Kasra Torshizi, Khuzema Habib, Kelin Yu, Ruohan Gao, Pratap Tokekar

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AFFORD2ACTï¼šæå‡ºåŸºäºå¯ä¾›æ€§çš„è‡ªåŠ¨å…³é”®ç‚¹é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºé€šç”¨ä¸”è½»é‡çº§çš„æœºå™¨äººæ“ä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¯ä¾›æ€§` `å…³é”®ç‚¹é€‰æ‹©` `Transformerç½‘ç»œ` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. åŸºäºè§†è§‰çš„æœºå™¨äººå­¦ä¹ ä¾èµ–äºå¯†é›†çš„å›¾åƒæˆ–ç‚¹äº‘è¾“å…¥ï¼Œè®¡ç®—é‡å¤§ä¸”åŒ…å«ä¸ç›¸å…³çš„èƒŒæ™¯ç‰¹å¾ã€‚
2. AFFORD2ACTåˆ©ç”¨å¯ä¾›æ€§ä¿¡æ¯ï¼Œè‡ªåŠ¨é€‰æ‹©è¯­ä¹‰å…³é”®ç‚¹ï¼Œæ„å»ºè½»é‡çº§çš„æœºå™¨äººæ“ä½œç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAFFORD2ACTåœ¨æ•°æ®æ•ˆç‡æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶åœ¨å„ç§çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAFFORD2ACTçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¯ä¾›æ€§å¼•å¯¼ï¼Œä»æ–‡æœ¬æç¤ºå’Œå•å¼ å›¾åƒä¸­æå–æœ€å°‘çš„è¯­ä¹‰2Då…³é”®ç‚¹ã€‚AFFORD2ACTéµå¾ªä¸‰é˜¶æ®µæµç¨‹ï¼šå¯ä¾›æ€§è¿‡æ»¤ã€ç±»åˆ«çº§å…³é”®ç‚¹æ„å»ºå’ŒåŸºäºTransformerçš„ç­–ç•¥å­¦ä¹ ï¼Œå…¶ä¸­åµŒå…¥äº†é—¨æ§æœºåˆ¶ä»¥æ¨ç†æœ€ç›¸å…³çš„å…³é”®ç‚¹ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªç´§å‡‘çš„38ç»´çŠ¶æ€ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥åœ¨15åˆ†é’Ÿå†…å®Œæˆè®­ç»ƒï¼Œå¹¶åœ¨æ²¡æœ‰æœ¬ä½“æ„Ÿå—æˆ–å¯†é›†è¡¨ç¤ºçš„æƒ…å†µä¸‹å®æ—¶è‰¯å¥½åœ°æ‰§è¡Œã€‚åœ¨å„ç§çœŸå®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡ä¸­ï¼ŒAFFORD2ACTå§‹ç»ˆæé«˜æ•°æ®æ•ˆç‡ï¼Œåœ¨æœªè§è¿‡çš„ç‰©ä½“ã€æ–°ç±»åˆ«ã€èƒŒæ™¯å’Œå¹²æ‰°ç‰©ä¸Šå®ç°äº†82%çš„æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè§†è§‰çš„æœºå™¨äººæ“ä½œæ–¹æ³•é€šå¸¸ä¾èµ–äºå¯†é›†çš„å›¾åƒæˆ–ç‚¹äº‘è¾“å…¥ï¼Œè¿™å¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°æ— å…³èƒŒæ™¯ç‰¹å¾çš„å¹²æ‰°ã€‚è™½ç„¶åŸºäºå…³é”®ç‚¹çš„æ–¹æ³•å¯ä»¥å…³æ³¨æ“ä½œç›¸å…³çš„ç‰¹å¾å¹¶å‡è½»è®¡ç®—è´Ÿæ‹…ï¼Œä½†å®ƒä»¬å¾€å¾€ä¾èµ–äºæ‰‹åŠ¨å¯å‘å¼æ–¹æ³•æˆ–ä»»åŠ¡è€¦åˆçš„é€‰æ‹©ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAFFORD2ACTçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¯ä¾›æ€§ï¼ˆaffordanceï¼‰ä¿¡æ¯æ¥å¼•å¯¼å…³é”®ç‚¹çš„é€‰æ‹©ã€‚å¯ä¾›æ€§æè¿°äº†ç‰©ä½“æä¾›çš„æ“ä½œå¯èƒ½æ€§ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬æç¤ºå’Œå›¾åƒä¿¡æ¯ï¼Œå¯ä»¥è‡ªåŠ¨æå–ä¸æ“ä½œä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰å…³é”®ç‚¹ï¼Œä»è€Œæ„å»ºä¸€ä¸ªè½»é‡çº§ä¸”å…·æœ‰æ³›åŒ–èƒ½åŠ›çš„æœºå™¨äººæ“ä½œç­–ç•¥ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ‰‹åŠ¨è®¾è®¡å…³é”®ç‚¹æˆ–é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†ç­–ç•¥çš„é€šç”¨æ€§å’Œæ•°æ®æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAFFORD2ACTæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) **å¯ä¾›æ€§è¿‡æ»¤**ï¼šåˆ©ç”¨æ–‡æœ¬æç¤ºå’Œå›¾åƒä¿¡æ¯ï¼Œè¿‡æ»¤æ‰ä¸æ“ä½œä»»åŠ¡æ— å…³çš„åŒºåŸŸï¼Œèšç„¦äºå…·æœ‰å¯ä¾›æ€§çš„åŒºåŸŸã€‚2) **ç±»åˆ«çº§å…³é”®ç‚¹æ„å»º**ï¼šåœ¨è¿‡æ»¤åçš„åŒºåŸŸä¸­ï¼Œæ„å»ºç±»åˆ«çº§åˆ«çš„å…³é”®ç‚¹ï¼Œè¿™äº›å…³é”®ç‚¹ä»£è¡¨äº†ç‰©ä½“ä¸Šä¸æ“ä½œç›¸å…³çš„ç‰¹å®šä½ç½®ã€‚3) **åŸºäºTransformerçš„ç­–ç•¥å­¦ä¹ **ï¼šä½¿ç”¨Transformerç½‘ç»œå­¦ä¹ ä¸€ä¸ªç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä»¥æå–çš„å…³é”®ç‚¹ä½œä¸ºè¾“å…¥ï¼Œå¹¶é€šè¿‡åµŒå…¥çš„é—¨æ§æœºåˆ¶æ¥é€‰æ‹©æœ€ç›¸å…³çš„å…³é”®ç‚¹ï¼Œæœ€ç»ˆè¾“å‡ºæ§åˆ¶æœºå™¨äººåŠ¨ä½œçš„æŒ‡ä»¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šAFFORD2ACTçš„å…³é”®åˆ›æ–°åœ¨äºå…¶è‡ªåŠ¨åŒ–çš„å…³é”®ç‚¹é€‰æ‹©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯ä¾›æ€§ä¿¡æ¯ï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡æˆ–é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¼•å…¥äº†åµŒå…¥å¼é—¨æ§æœºåˆ¶ï¼Œå…è®¸ç­–ç•¥ç½‘ç»œæ ¹æ®è¾“å…¥çš„å…³é”®ç‚¹åŠ¨æ€åœ°é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œä»è€Œæé«˜ç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šAFFORD2ACTä½¿ç”¨CLIPæ¨¡å‹æå–å›¾åƒå’Œæ–‡æœ¬çš„ç‰¹å¾ï¼Œç”¨äºå¯ä¾›æ€§è¿‡æ»¤ã€‚å…³é”®ç‚¹æ„å»ºé˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„æ£€æµ‹æ¨¡å‹ï¼ˆä¾‹å¦‚DETRï¼‰æ¥æ£€æµ‹ç‰©ä½“ä¸Šçš„å…³é”®ç‚¹ã€‚Transformerç½‘ç»œé‡‡ç”¨æ ‡å‡†çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå¹¶åµŒå…¥äº†é—¨æ§æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®è¾“å…¥çš„å…³é”®ç‚¹åŠ¨æ€åœ°è°ƒæ•´æ¯ä¸ªå…³é”®ç‚¹çš„æƒé‡ã€‚æœ€ç»ˆçš„ç­–ç•¥ç½‘ç»œè¾“å‡ºä¸€ä¸ª38ç»´çš„çŠ¶æ€å‘é‡ï¼Œç”¨äºæ§åˆ¶æœºå™¨äººçš„åŠ¨ä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

AFFORD2ACTåœ¨å„ç§çœŸå®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æœªè§è¿‡çš„ç‰©ä½“ã€æ–°ç±»åˆ«ã€èƒŒæ™¯å’Œå¹²æ‰°ç‰©ä¸Šå®ç°äº†82%çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»¥æé«˜çš„æ•°æ®æ•ˆç‡è¿›è¡Œè®­ç»ƒï¼Œä»…éœ€15åˆ†é’Ÿå³å¯è®­ç»ƒå‡ºä¸€ä¸ªæœ‰æ•ˆçš„ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå¯†é›†è¡¨ç¤ºçš„æ–¹æ³•ç›¸æ¯”ï¼ŒAFFORD2ACTå¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

AFFORD2ACTå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒåŒ»ç–—æœºå™¨äººã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œå¹¶é™ä½å¼€å‘å’Œéƒ¨ç½²æˆæœ¬ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å¯ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œç¯å¢ƒï¼Œä¾‹å¦‚å¤šç‰©ä½“æ“ä½œå’ŒåŠ¨æ€ç¯å¢ƒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-based robot learning often relies on dense image or point-cloud inputs, which are computationally heavy and entangle irrelevant background features. Existing keypoint-based approaches can focus on manipulation-centric features and be lightweight, but either depend on manual heuristics or task-coupled selection, limiting scalability and semantic understanding. To address this, we propose AFFORD2ACT, an affordance-guided framework that distills a minimal set of semantic 2D keypoints from a text prompt and a single image. AFFORD2ACT follows a three-stage pipeline: affordance filtering, category-level keypoint construction, and transformer-based policy learning with embedded gating to reason about the most relevant keypoints, yielding a compact 38-dimensional state policy that can be trained in 15 minutes, which performs well in real-time without proprioception or dense representations. Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves data efficiency, achieving an 82% success rate on unseen objects, novel categories, backgrounds, and distractors.

