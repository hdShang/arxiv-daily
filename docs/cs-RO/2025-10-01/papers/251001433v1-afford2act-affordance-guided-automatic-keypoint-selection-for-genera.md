---
layout: default
title: AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation
---

# AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01433" target="_blank" class="toolbar-btn">arXiv: 2510.01433v1</a>
    <a href="https://arxiv.org/pdf/2510.01433.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01433v1" 
            onclick="toggleFavorite(this, '2510.01433v1', 'AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Anukriti Singh, Kasra Torshizi, Khuzema Habib, Kelin Yu, Ruohan Gao, Pratap Tokekar

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-01

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**AFFORD2ACTÔºöÊèêÂá∫Âü∫‰∫éÂèØ‰æõÊÄßÁöÑËá™Âä®ÂÖ≥ÈîÆÁÇπÈÄâÊã©ÊñπÊ≥ïÔºåÁî®‰∫éÈÄöÁî®‰∏îËΩªÈáèÁ∫ßÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `ÂèØ‰æõÊÄß` `ÂÖ≥ÈîÆÁÇπÈÄâÊã©` `TransformerÁΩëÁªú` `Ê≥õÂåñËÉΩÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Âü∫‰∫éËßÜËßâÁöÑÊú∫Âô®‰∫∫Â≠¶‰π†‰æùËµñ‰∫éÂØÜÈõÜÁöÑÂõæÂÉèÊàñÁÇπ‰∫ëËæìÂÖ•ÔºåËÆ°ÁÆóÈáèÂ§ß‰∏îÂåÖÂê´‰∏çÁõ∏ÂÖ≥ÁöÑËÉåÊôØÁâπÂæÅ„ÄÇ
2. AFFORD2ACTÂà©Áî®ÂèØ‰æõÊÄß‰ø°ÊÅØÔºåËá™Âä®ÈÄâÊã©ËØ≠‰πâÂÖ≥ÈîÆÁÇπÔºåÊûÑÂª∫ËΩªÈáèÁ∫ßÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåAFFORD2ACTÂú®Êï∞ÊçÆÊïàÁéáÊñπÈù¢ÊúâÊòæËëóÊèêÂçáÔºåÂπ∂Âú®ÂêÑÁßçÁúüÂÆûÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AFFORD2ACTÁöÑÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÈÄöËøáÂèØ‰æõÊÄßÂºïÂØºÔºå‰ªéÊñáÊú¨ÊèêÁ§∫ÂíåÂçïÂº†ÂõæÂÉè‰∏≠ÊèêÂèñÊúÄÂ∞ëÁöÑËØ≠‰πâ2DÂÖ≥ÈîÆÁÇπ„ÄÇAFFORD2ACTÈÅµÂæ™‰∏âÈò∂ÊÆµÊµÅÁ®ãÔºöÂèØ‰æõÊÄßËøáÊª§„ÄÅÁ±ªÂà´Á∫ßÂÖ≥ÈîÆÁÇπÊûÑÂª∫ÂíåÂü∫‰∫éTransformerÁöÑÁ≠ñÁï•Â≠¶‰π†ÔºåÂÖ∂‰∏≠ÂµåÂÖ•‰∫ÜÈó®ÊéßÊú∫Âà∂‰ª•Êé®ÁêÜÊúÄÁõ∏ÂÖ≥ÁöÑÂÖ≥ÈîÆÁÇπÔºå‰ªéËÄå‰∫ßÁîü‰∏Ä‰∏™Á¥ßÂáëÁöÑ38Áª¥Áä∂ÊÄÅÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•ÂèØ‰ª•Âú®15ÂàÜÈíüÂÜÖÂÆåÊàêËÆ≠ÁªÉÔºåÂπ∂Âú®Ê≤°ÊúâÊú¨‰ΩìÊÑüÂèóÊàñÂØÜÈõÜË°®Á§∫ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÊó∂ËâØÂ•ΩÂú∞ÊâßË°å„ÄÇÂú®ÂêÑÁßçÁúüÂÆû‰∏ñÁïåÁöÑÊìç‰Ωú‰ªªÂä°‰∏≠ÔºåAFFORD2ACTÂßãÁªàÊèêÈ´òÊï∞ÊçÆÊïàÁéáÔºåÂú®Êú™ËßÅËøáÁöÑÁâ©‰Ωì„ÄÅÊñ∞Á±ªÂà´„ÄÅËÉåÊôØÂíåÂπ≤Êâ∞Áâ©‰∏äÂÆûÁé∞‰∫Ü82%ÁöÑÊàêÂäüÁéá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éËßÜËßâÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂØÜÈõÜÁöÑÂõæÂÉèÊàñÁÇπ‰∫ëËæìÂÖ•ÔºåËøôÂØºËá¥ËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºåÂπ∂‰∏îÂÆπÊòìÂèóÂà∞Êó†ÂÖ≥ËÉåÊôØÁâπÂæÅÁöÑÂπ≤Êâ∞„ÄÇËôΩÁÑ∂Âü∫‰∫éÂÖ≥ÈîÆÁÇπÁöÑÊñπÊ≥ïÂèØ‰ª•ÂÖ≥Ê≥®Êìç‰ΩúÁõ∏ÂÖ≥ÁöÑÁâπÂæÅÂπ∂ÂáèËΩªËÆ°ÁÆóË¥üÊãÖÔºå‰ΩÜÂÆÉ‰ª¨ÂæÄÂæÄ‰æùËµñ‰∫éÊâãÂä®ÂêØÂèëÂºèÊñπÊ≥ïÊàñ‰ªªÂä°ËÄ¶ÂêàÁöÑÈÄâÊã©ÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂèØÊâ©Â±ïÊÄßÂíåËØ≠‰πâÁêÜËß£ËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAFFORD2ACTÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂèØ‰æõÊÄßÔºàaffordanceÔºâ‰ø°ÊÅØÊù•ÂºïÂØºÂÖ≥ÈîÆÁÇπÁöÑÈÄâÊã©„ÄÇÂèØ‰æõÊÄßÊèèËø∞‰∫ÜÁâ©‰ΩìÊèê‰æõÁöÑÊìç‰ΩúÂèØËÉΩÊÄßÔºåÈÄöËøáÁªìÂêàÊñáÊú¨ÊèêÁ§∫ÂíåÂõæÂÉè‰ø°ÊÅØÔºåÂèØ‰ª•Ëá™Âä®ÊèêÂèñ‰∏éÊìç‰Ωú‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËØ≠‰πâÂÖ≥ÈîÆÁÇπÔºå‰ªéËÄåÊûÑÂª∫‰∏Ä‰∏™ËΩªÈáèÁ∫ß‰∏îÂÖ∑ÊúâÊ≥õÂåñËÉΩÂäõÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•„ÄÇËøôÁßçÊñπÊ≥ïÈÅøÂÖç‰∫ÜÊâãÂä®ËÆæËÆ°ÂÖ≥ÈîÆÁÇπÊàñÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åËÆ≠ÁªÉÔºåÊèêÈ´ò‰∫ÜÁ≠ñÁï•ÁöÑÈÄöÁî®ÊÄßÂíåÊï∞ÊçÆÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAFFORD2ACTÊ°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºö1) **ÂèØ‰æõÊÄßËøáÊª§**ÔºöÂà©Áî®ÊñáÊú¨ÊèêÁ§∫ÂíåÂõæÂÉè‰ø°ÊÅØÔºåËøáÊª§Êéâ‰∏éÊìç‰Ωú‰ªªÂä°Êó†ÂÖ≥ÁöÑÂå∫ÂüüÔºåËÅöÁÑ¶‰∫éÂÖ∑ÊúâÂèØ‰æõÊÄßÁöÑÂå∫Âüü„ÄÇ2) **Á±ªÂà´Á∫ßÂÖ≥ÈîÆÁÇπÊûÑÂª∫**ÔºöÂú®ËøáÊª§ÂêéÁöÑÂå∫Âüü‰∏≠ÔºåÊûÑÂª∫Á±ªÂà´Á∫ßÂà´ÁöÑÂÖ≥ÈîÆÁÇπÔºåËøô‰∫õÂÖ≥ÈîÆÁÇπ‰ª£Ë°®‰∫ÜÁâ©‰Ωì‰∏ä‰∏éÊìç‰ΩúÁõ∏ÂÖ≥ÁöÑÁâπÂÆö‰ΩçÁΩÆ„ÄÇ3) **Âü∫‰∫éTransformerÁöÑÁ≠ñÁï•Â≠¶‰π†**Ôºö‰ΩøÁî®TransformerÁΩëÁªúÂ≠¶‰π†‰∏Ä‰∏™Á≠ñÁï•ÔºåËØ•Á≠ñÁï•‰ª•ÊèêÂèñÁöÑÂÖ≥ÈîÆÁÇπ‰Ωú‰∏∫ËæìÂÖ•ÔºåÂπ∂ÈÄöËøáÂµåÂÖ•ÁöÑÈó®ÊéßÊú∫Âà∂Êù•ÈÄâÊã©ÊúÄÁõ∏ÂÖ≥ÁöÑÂÖ≥ÈîÆÁÇπÔºåÊúÄÁªàËæìÂá∫ÊéßÂà∂Êú∫Âô®‰∫∫Âä®‰ΩúÁöÑÊåá‰ª§„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAFFORD2ACTÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Ëá™Âä®ÂåñÁöÑÂÖ≥ÈîÆÁÇπÈÄâÊã©ÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂà©Áî®ÂèØ‰æõÊÄß‰ø°ÊÅØÔºåÊó†ÈúÄÊâãÂä®ËÆæËÆ°ÊàñÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åËÆ≠ÁªÉ„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ËøòÂºïÂÖ•‰∫ÜÂµåÂÖ•ÂºèÈó®ÊéßÊú∫Âà∂ÔºåÂÖÅËÆ∏Á≠ñÁï•ÁΩëÁªúÊ†πÊçÆËæìÂÖ•ÁöÑÂÖ≥ÈîÆÁÇπÂä®ÊÄÅÂú∞ÈÄâÊã©ÊúÄÁõ∏ÂÖ≥ÁöÑÁâπÂæÅÔºå‰ªéËÄåÊèêÈ´òÁ≠ñÁï•ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöAFFORD2ACT‰ΩøÁî®CLIPÊ®°ÂûãÊèêÂèñÂõæÂÉèÂíåÊñáÊú¨ÁöÑÁâπÂæÅÔºåÁî®‰∫éÂèØ‰æõÊÄßËøáÊª§„ÄÇÂÖ≥ÈîÆÁÇπÊûÑÂª∫Èò∂ÊÆµ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÊ£ÄÊµãÊ®°ÂûãÔºà‰æãÂ¶ÇDETRÔºâÊù•Ê£ÄÊµãÁâ©‰Ωì‰∏äÁöÑÂÖ≥ÈîÆÁÇπ„ÄÇTransformerÁΩëÁªúÈááÁî®Ê†áÂáÜÁöÑÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®ÁªìÊûÑÔºåÂπ∂ÂµåÂÖ•‰∫ÜÈó®ÊéßÊú∫Âà∂ÔºåËØ•Êú∫Âà∂Ê†πÊçÆËæìÂÖ•ÁöÑÂÖ≥ÈîÆÁÇπÂä®ÊÄÅÂú∞Ë∞ÉÊï¥ÊØè‰∏™ÂÖ≥ÈîÆÁÇπÁöÑÊùÉÈáç„ÄÇÊúÄÁªàÁöÑÁ≠ñÁï•ÁΩëÁªúËæìÂá∫‰∏Ä‰∏™38Áª¥ÁöÑÁä∂ÊÄÅÂêëÈáèÔºåÁî®‰∫éÊéßÂà∂Êú∫Âô®‰∫∫ÁöÑÂä®‰Ωú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

AFFORD2ACTÂú®ÂêÑÁßçÁúüÂÆû‰∏ñÁïåÁöÑÊìç‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÂú®Êú™ËßÅËøáÁöÑÁâ©‰Ωì„ÄÅÊñ∞Á±ªÂà´„ÄÅËÉåÊôØÂíåÂπ≤Êâ∞Áâ©‰∏äÂÆûÁé∞‰∫Ü82%ÁöÑÊàêÂäüÁéá„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§ü‰ª•ÊûÅÈ´òÁöÑÊï∞ÊçÆÊïàÁéáËøõË°åËÆ≠ÁªÉÔºå‰ªÖÈúÄ15ÂàÜÈíüÂç≥ÂèØËÆ≠ÁªÉÂá∫‰∏Ä‰∏™ÊúâÊïàÁöÑÁ≠ñÁï•„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÂØÜÈõÜË°®Á§∫ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåAFFORD2ACTÂ§ßÂ§ßÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÂπ∂ÊèêÈ´ò‰∫ÜÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

AFFORD2ACTÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÂíåÂåªÁñóÊú∫Âô®‰∫∫„ÄÇËØ•Ê°ÜÊû∂ËÉΩÂ§üÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊìç‰ΩúËÉΩÂäõÔºåÂπ∂Èôç‰ΩéÂºÄÂèëÂíåÈÉ®ÁΩ≤ÊàêÊú¨„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÂèØ‰ª•Êâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÂíåÁéØÂ¢ÉÔºå‰æãÂ¶ÇÂ§öÁâ©‰ΩìÊìç‰ΩúÂíåÂä®ÊÄÅÁéØÂ¢É„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-based robot learning often relies on dense image or point-cloud inputs, which are computationally heavy and entangle irrelevant background features. Existing keypoint-based approaches can focus on manipulation-centric features and be lightweight, but either depend on manual heuristics or task-coupled selection, limiting scalability and semantic understanding. To address this, we propose AFFORD2ACT, an affordance-guided framework that distills a minimal set of semantic 2D keypoints from a text prompt and a single image. AFFORD2ACT follows a three-stage pipeline: affordance filtering, category-level keypoint construction, and transformer-based policy learning with embedded gating to reason about the most relevant keypoints, yielding a compact 38-dimensional state policy that can be trained in 15 minutes, which performs well in real-time without proprioception or dense representations. Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves data efficiency, achieving an 82% success rate on unseen objects, novel categories, backgrounds, and distractors.

