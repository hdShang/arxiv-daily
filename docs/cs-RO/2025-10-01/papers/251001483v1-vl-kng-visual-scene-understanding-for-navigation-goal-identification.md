---
layout: default
title: "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs"
---

# VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.01483" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.01483v1</a>
  <a href="https://arxiv.org/pdf/2510.01483.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01483v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.01483v1', 'VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohamad Al Mdfaa, Svetlana Lukina, Timur Akhtyamov, Arthur Nigmatzyanov, Dmitrii Nalberskii, Sergey Zagoruyko, Gonzalo Ferrer

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-01

**å¤‡æ³¨**: This work has been submitted to the IEEE for possible publication

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**VL-KnGï¼šåˆ©ç”¨æ—¶ç©ºçŸ¥è¯†å›¾è°±è¿›è¡Œè§†è§‰åœºæ™¯ç†è§£ï¼Œå®ç°å¯¼èˆªç›®æ ‡è¯†åˆ«**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `è§†è§‰åœºæ™¯ç†è§£` `çŸ¥è¯†å›¾è°±` `æœºå™¨äººå¯¼èˆª` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `æ—¶ç©ºæ¨ç†` `å¯è§£é‡Šæ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººå¯¼èˆªä¸­å­˜åœ¨åœºæ™¯è®°å¿†ä¸è¶³ã€ç©ºé—´æ¨ç†æœ‰é™å’Œå®æ—¶æ€§å·®ç­‰é—®é¢˜ã€‚
2. VL-KnGé€šè¿‡æ„å»ºæ—¶ç©ºçŸ¥è¯†å›¾è°±ï¼Œç»´æŠ¤å¯¹è±¡èº«ä»½ä¿¡æ¯ï¼Œå¹¶æ”¯æŒå¯æŸ¥è¯¢çš„å›¾ç»“æ„ï¼Œå®ç°å¯è§£é‡Šçš„ç©ºé—´æ¨ç†ã€‚
3. åœ¨WalkieKnowledgeåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººå®éªŒä¸­ï¼ŒVL-KnGåœ¨å¯¼èˆªç›®æ ‡è¯†åˆ«æ–¹é¢å–å¾—äº†ä¸Gemini 2.5 Proç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶å…·å¤‡å®æ—¶æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)åœ¨æœºå™¨äººå¯¼èˆªæ–¹é¢å±•ç°äº†æ½œåŠ›ï¼Œä½†å­˜åœ¨å±€é™æ€§ï¼šç¼ºä¹æŒä¹…çš„åœºæ™¯è®°å¿†ï¼Œç©ºé—´æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œä¸”æ— æ³•æœ‰æ•ˆæ‰©å±•åˆ°é•¿æ—¶é—´è§†é¢‘ä»¥æ»¡è¶³å®æ—¶åº”ç”¨éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†VL-KnGï¼Œä¸€ä¸ªè§†è§‰åœºæ™¯ç†è§£ç³»ç»Ÿï¼Œé€šè¿‡æ„å»ºæ—¶ç©ºçŸ¥è¯†å›¾è°±å’Œé«˜æ•ˆçš„æŸ¥è¯¢å¤„ç†æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç”¨äºå¯¼èˆªç›®æ ‡è¯†åˆ«ã€‚è¯¥æ–¹æ³•åˆ†å—å¤„ç†è§†é¢‘åºåˆ—ï¼Œåˆ©ç”¨ç°ä»£VLMsåˆ›å»ºæŒä¹…çš„çŸ¥è¯†å›¾è°±ï¼Œç»´æŠ¤å¯¹è±¡éšæ—¶é—´æ¨ç§»çš„èº«ä»½ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¯æŸ¥è¯¢çš„å›¾ç»“æ„å®ç°å¯è§£é‡Šçš„ç©ºé—´æ¨ç†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†WalkieKnowledgeï¼Œä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«çº¦200ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæ¶µç›–8æ¡ä¸åŒçš„è½¨è¿¹ï¼Œè§†é¢‘æ•°æ®çº¦100åˆ†é’Ÿï¼Œä»¥ä¾¿åœ¨ç»“æ„åŒ–æ–¹æ³•å’Œé€šç”¨VLMsä¹‹é—´è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚åœ¨å·®é€Ÿé©±åŠ¨æœºå™¨äººä¸Šçš„å®é™…éƒ¨ç½²è¡¨æ˜äº†å…¶åº”ç”¨æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†77.27%çš„æˆåŠŸç‡å’Œ76.92%çš„ç­”æ¡ˆå‡†ç¡®ç‡ï¼Œä¸Gemini 2.5 Proçš„æ€§èƒ½ç›¸åŒ¹é…ï¼ŒåŒæ—¶æä¾›äº†ç”±çŸ¥è¯†å›¾è°±æ”¯æŒçš„å¯è§£é‡Šæ¨ç†ï¼Œä»¥åŠåœ¨å®šä½ã€å¯¼èˆªå’Œè§„åˆ’ç­‰ä¸åŒä»»åŠ¡ä¸­è¿›è¡Œå®æ—¶éƒ¨ç½²çš„è®¡ç®—æ•ˆç‡ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨æ¥æ”¶åå‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººå¯¼èˆªä¸­ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£å¤æ‚åœºæ™¯å’Œè¿›è¡Œæœ‰æ•ˆç©ºé—´æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚ç°æœ‰VLMsç¼ºä¹æŒä¹…çš„åœºæ™¯è®°å¿†ï¼Œéš¾ä»¥å¤„ç†é•¿æ—¶é—´è§†é¢‘ï¼Œå¹¶ä¸”ç©ºé—´æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®è¯†åˆ«å¯¼èˆªç›®æ ‡ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†VLMsåœ¨å®é™…æœºå™¨äººå¯¼èˆªä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªæ—¶ç©ºçŸ¥è¯†å›¾è°±ï¼ˆSpatiotemporal Knowledge Graph, KnGï¼‰ï¼Œç”¨äºè¡¨ç¤ºå’Œç»´æŠ¤åœºæ™¯ä¸­çš„å¯¹è±¡ã€å…³ç³»ä»¥åŠå®ƒä»¬éšæ—¶é—´çš„å˜åŒ–ã€‚é€šè¿‡å°†è§†é¢‘ä¿¡æ¯è½¬åŒ–ä¸ºç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ï¼Œå¯ä»¥å®ç°æŒä¹…çš„åœºæ™¯è®°å¿†å’Œå¯è§£é‡Šçš„ç©ºé—´æ¨ç†ï¼Œä»è€Œæé«˜å¯¼èˆªç›®æ ‡è¯†åˆ«çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVL-KnGç³»ç»Ÿçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘åˆ†å—å¤„ç†ï¼šå°†é•¿è§†é¢‘åˆ†å‰²æˆè¾ƒå°çš„å—ï¼Œä»¥ä¾¿äºå¤„ç†ã€‚2) è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼šåˆ©ç”¨ç°ä»£VLMsæå–è§†é¢‘å—ä¸­çš„å¯¹è±¡å’Œå…³ç³»ä¿¡æ¯ã€‚3) çŸ¥è¯†å›¾è°±æ„å»ºï¼šå°†VLMæå–çš„ä¿¡æ¯æ•´åˆåˆ°æ—¶ç©ºçŸ¥è¯†å›¾è°±ä¸­ï¼Œç»´æŠ¤å¯¹è±¡èº«ä»½å’Œå…³ç³»éšæ—¶é—´çš„å˜åŒ–ã€‚4) æŸ¥è¯¢å¤„ç†ï¼šé€šè¿‡æŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼Œè¿›è¡Œç©ºé—´æ¨ç†å’Œå¯¼èˆªç›®æ ‡è¯†åˆ«ã€‚æ•´ä¸ªæµç¨‹æ—¨åœ¨å°†éç»“æ„åŒ–çš„è§†é¢‘æ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„çŸ¥è¯†è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è¯¥çŸ¥è¯†è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ—¶ç©ºçŸ¥è¯†å›¾è°±åº”ç”¨äºæœºå™¨äººå¯¼èˆªä¸­çš„è§†è§‰åœºæ™¯ç†è§£ã€‚ä¸ä¼ ç»Ÿçš„VLMsç›¸æ¯”ï¼ŒVL-KnGèƒ½å¤Ÿç»´æŠ¤æŒä¹…çš„åœºæ™¯è®°å¿†ï¼Œè¿›è¡Œå¯è§£é‡Šçš„ç©ºé—´æ¨ç†ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿æ—¶é—´è§†é¢‘ã€‚æ­¤å¤–ï¼ŒWalkieKnowledgeåŸºå‡†æµ‹è¯•çš„å¼•å…¥ä¹Ÿä¸ºç»“æ„åŒ–æ–¹æ³•å’Œé€šç”¨VLMsçš„å…¬å¹³æ¯”è¾ƒæä¾›äº†å¹³å°ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³äºçŸ¥è¯†å›¾è°±çš„æ„å»ºå’ŒæŸ¥è¯¢å¤„ç†æ˜¯å…³é”®è®¾è®¡ã€‚å…·ä½“æ¥è¯´ï¼ŒçŸ¥è¯†å›¾è°±çš„èŠ‚ç‚¹è¡¨ç¤ºåœºæ™¯ä¸­çš„å¯¹è±¡ï¼Œè¾¹è¡¨ç¤ºå¯¹è±¡ä¹‹é—´çš„å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œç©ºé—´å…³ç³»ã€æ—¶é—´å…³ç³»ï¼‰ã€‚åœ¨æŸ¥è¯¢å¤„ç†æ–¹é¢ï¼Œè®ºæ–‡è®¾è®¡äº†é«˜æ•ˆçš„å›¾æŸ¥è¯¢ç®—æ³•ï¼Œç”¨äºä»çŸ¥è¯†å›¾è°±ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œç©ºé—´æ¨ç†ã€‚æ­¤å¤–ï¼Œè§†é¢‘åˆ†å—çš„å¤§å°å’ŒVLMçš„é€‰æ‹©ä¹Ÿä¼šå½±å“ç³»ç»Ÿçš„æ€§èƒ½ï¼Œä½†è®ºæ–‡ä¸­æœªæä¾›å…·ä½“çš„å‚æ•°è®¾ç½®ç»†èŠ‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VL-KnGåœ¨çœŸå®æœºå™¨äººå®éªŒä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼ŒæˆåŠŸç‡è¾¾åˆ°77.27%ï¼Œç­”æ¡ˆå‡†ç¡®ç‡è¾¾åˆ°76.92%ï¼Œä¸Gemini 2.5 Proçš„æ€§èƒ½ç›¸åŒ¹é…ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒVL-KnGæä¾›äº†å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å…·å¤‡å®æ—¶éƒ¨ç½²çš„è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæ–°æå‡ºçš„WalkieKnowledgeåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°è§†è§‰åœºæ™¯ç†è§£ç³»ç»Ÿæä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VL-KnGå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šæœºå™¨äººå¯¼èˆªã€æ™ºèƒ½ç›‘æ§ã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›æŒä¹…çš„åœºæ™¯è®°å¿†å’Œå¯è§£é‡Šçš„ç©ºé—´æ¨ç†ï¼Œè¯¥æŠ€æœ¯å¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„è‡ªä¸»æ“ä½œã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„åœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models (VLMs) have shown potential for robot navigation but encounter fundamental limitations: they lack persistent scene memory, offer limited spatial reasoning, and do not scale effectively with video duration for real-time application. We present VL-KnG, a Visual Scene Understanding system that tackles these challenges using spatiotemporal knowledge graph construction and computationally efficient query processing for navigation goal identification. Our approach processes video sequences in chunks utilizing modern VLMs, creates persistent knowledge graphs that maintain object identity over time, and enables explainable spatial reasoning through queryable graph structures. We also introduce WalkieKnowledge, a new benchmark with about 200 manually annotated questions across 8 diverse trajectories spanning approximately 100 minutes of video data, enabling fair comparison between structured approaches and general-purpose VLMs. Real-world deployment on a differential drive robot demonstrates practical applicability, with our method achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5 Pro performance while providing explainable reasoning supported by the knowledge graph, computational efficiency for real-time deployment across different tasks, such as localization, navigation and planning. Code and dataset will be released after acceptance.

