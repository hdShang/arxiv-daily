---
layout: default
title: VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation
---

# VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01388" target="_blank" class="toolbar-btn">arXiv: 2510.01388v1</a>
    <a href="https://arxiv.org/pdf/2510.01388.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01388v1" 
            onclick="toggleFavorite(this, '2510.01388v1', 'VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Arthur Zhang, Xiangyun Meng, Luca Calliari, Dong-Ki Kim, Shayegan Omidshafiei, Joydeep Biswas, Ali Agha, Amirreza Shaban

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-01

**Â§áÊ≥®**: 9 pages, 6 figures, 3 tables

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VENTURA‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫ÂØºËà™‰ªªÂä°‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Ë∑ØÂæÑËßÑÂàí` `Êú∫Âô®‰∫∫ÂØºËà™` `ÂõæÂÉèÊâ©Êï£Ê®°Âûã` `Ë°å‰∏∫ÂÖãÈöÜ` `Ëá™ÁõëÁù£Â≠¶‰π†` `‰ªªÂä°ÈÄÇÂ∫îÊÄß`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®ÂØºËà™‰ªªÂä°‰∏≠Èöæ‰ª•Â∫îÁî®Ôºå‰∏ªË¶ÅÁî±‰∫éË°åÂä®Á©∫Èó¥ÂíåÈ¢ÑËÆ≠ÁªÉÁõÆÊ†áÁöÑÂ∑ÆÂºÇÔºåÂØºËá¥Ê®°ÂûãÁöÑËøÅÁßªËÉΩÂäõ‰∏çË∂≥„ÄÇ
2. VENTURAÈÄöËøáÂæÆË∞ÉÂõæÂÉèÊâ©Êï£Ê®°ÂûãÁîüÊàêË∑ØÂæÑÊé©Á†ÅÔºåÁªìÂêàËΩªÈáèÁ∫ßË°å‰∏∫ÂÖãÈöÜÁ≠ñÁï•ÔºåÂ∞ÜËßÜËßâËÆ°ÂàíËΩ¨Âåñ‰∏∫ÂèØÊâßË°åËΩ®ËøπÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊâßË°åËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§„ÄÇ
3. Âú®ÂÆûÈôÖËØÑ‰º∞‰∏≠ÔºåVENTURAÂú®Â§ö‰∏™‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºåÊàêÂäüÁéáÊèêÈ´ò33%ÔºåÁ¢∞ÊíûÁéáÈôç‰Ωé54%ÔºåÂπ∂Â±ïÁé∞Âá∫ÂØπÊú™ËßÅ‰ªªÂä°ÁªÑÂêàÁöÑËâØÂ•ΩÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú∫Âô®‰∫∫ÂøÖÈ°ªÈÄÇÂ∫îÂ§öÊ†∑ÁöÑ‰∫∫Á±ªÊåá‰ª§ÔºåÂπ∂Âú®ÈùûÁªìÊûÑÂåñÁöÑÂºÄÊîæ‰∏ñÁïåÁéØÂ¢É‰∏≠ÂÆâÂÖ®Êìç‰Ωú„ÄÇËøëÊúüÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰∏∫ËØ≠Ë®ÄÂíåÊÑüÁü•ÁöÑÁªìÂêàÊèê‰æõ‰∫ÜÂº∫ÊúâÂäõÁöÑÂÖàÈ™åÁü•ËØÜÔºå‰ΩÜÁî±‰∫éË°åÂä®Á©∫Èó¥ÂíåÈ¢ÑËÆ≠ÁªÉÁõÆÊ†áÁöÑÂ∑ÆÂºÇÔºåÂØºËá¥ÂÖ∂Âú®ÂØºËà™‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÂèóÂà∞ÈôêÂà∂„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜVENTURAÔºå‰∏Ä‰∏™ÈÄöËøáÂæÆË∞É‰∫íËÅîÁΩëÈ¢ÑËÆ≠ÁªÉÁöÑÂõæÂÉèÊâ©Êï£Ê®°ÂûãËøõË°åË∑ØÂæÑËßÑÂàíÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØºËà™Á≥ªÁªü„ÄÇVENTURAÁîüÊàêË∑ØÂæÑÊé©Á†ÅÔºàÂç≥ËßÜËßâËÆ°ÂàíÔºâÔºåÂπ∂ÈÄöËøáËΩªÈáèÁ∫ßÁöÑË°å‰∏∫ÂÖãÈöÜÁ≠ñÁï•Â∞ÜËøô‰∫õËßÜËßâËÆ°ÂàíËΩ¨Âåñ‰∏∫ÂèØÊâßË°åÁöÑËΩ®Ëøπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVENTURAÂú®Áâ©‰ΩìÂà∞Ëææ„ÄÅÈöúÁ¢çÁâ©ËßÑÈÅøÂíåÂú∞ÂΩ¢ÂÅèÂ•Ω‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫Á°ÄÊ®°ÂûãÂü∫Á∫øÔºåÊàêÂäüÁéáÊèêÈ´ò‰∫Ü33%ÔºåÁ¢∞ÊíûÁéáÈôç‰Ωé‰∫Ü54%„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÊâßË°åÂØºËà™‰ªªÂä°Êó∂ÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Ë°åÂä®Á©∫Èó¥ÂíåÈ¢ÑËÆ≠ÁªÉÁõÆÊ†á‰∏äÂ≠òÂú®Â∑ÆÂºÇÔºåÂØºËá¥Èöæ‰ª•ÊúâÊïàËøÅÁßªÂà∞Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVENTURAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÁîüÊàêË∑ØÂæÑÊé©Á†ÅÊù•ÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑ‰∏ä‰∏ãÊñáÊÑüÁü•ÂØºËà™Ë°å‰∏∫ÔºåËÄå‰∏çÊòØÁõ¥Êé•È¢ÑÊµã‰ΩéÁ∫ßÂä®‰Ωú„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÊõ¥Ëá™ÁÑ∂Âú∞ÈÅµÂæ™‰∫∫Á±ªÊåá‰ª§„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVENTURAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ÂõæÂÉèÊâ©Êï£Ê®°ÂûãÁöÑÂæÆË∞É„ÄÅË∑ØÂæÑÊé©Á†ÅÁöÑÁîüÊàêÂíåËΩªÈáèÁ∫ßË°å‰∏∫ÂÖãÈöÜÁ≠ñÁï•ÁöÑÂ∫îÁî®„ÄÇÈ¶ñÂÖàÔºåÊ®°ÂûãÁîüÊàêË∑ØÂæÑÊé©Á†ÅÔºåÁÑ∂ÂêéÈÄöËøáË°å‰∏∫ÂÖãÈöÜÂ∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ÂèØÊâßË°åÁöÑËΩ®Ëøπ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVENTURAÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Ë∑ØÂæÑÊé©Á†ÅÁîüÊàêÊú∫Âà∂ÔºåËøô‰∏é‰º†ÁªüÁöÑÁõ¥Êé•Âä®‰ΩúÈ¢ÑÊµãÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°åÂ§çÊùÇÁöÑÂØºËà™‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåVENTURA‰ΩøÁî®Ëá™ÁõëÁù£Ë∑üË∏™Ê®°ÂûãÁîüÊàêË∑ØÂæÑÊé©Á†ÅÔºåÂπ∂ÁªìÂêàVLMÂ¢ûÂº∫ÁöÑÊèèËø∞ËøõË°åÁõëÁù£ÔºåÈÅøÂÖç‰∫ÜÊâãÂä®ÂÉèÁ¥†Á∫ßÊ†áÊ≥®ÁöÑÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VENTURAÂú®ÂÆûÈôÖËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊàêÂäüÁéáÊèêÈ´ò‰∫Ü33%ÔºåÁ¢∞ÊíûÁéáÈôç‰Ωé‰∫Ü54%„ÄÇ‰∏éÁé∞ÊúâÂü∫Á°ÄÊ®°ÂûãÁõ∏ÊØîÔºåVENTURAÂú®Áâ©‰ΩìÂà∞Ëææ„ÄÅÈöúÁ¢çÁâ©ËßÑÈÅøÂíåÂú∞ÂΩ¢ÂÅèÂ•Ω‰ªªÂä°‰∏äÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ±ïÁé∞‰∫ÜËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VENTURAÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅÊó†‰∫∫È©æÈ©∂Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÂØπ‰∫∫Á±ªÊåá‰ª§ÁöÑÁêÜËß£ËÉΩÂäõÔºåËÉΩÂ§üÂÆûÁé∞Êõ¥Êô∫ËÉΩÁöÑ‰∫§‰∫íÂíåËá™‰∏ªÂÜ≥Á≠ñÔºåÊé®Âä®Êô∫ËÉΩÊú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Robots must adapt to diverse human instructions and operate safely in unstructured, open-world environments. Recent Vision-Language models (VLMs) offer strong priors for grounding language and perception, but remain difficult to steer for navigation due to differences in action spaces and pretraining objectives that hamper transferability to robotics tasks. Towards addressing this, we introduce VENTURA, a vision-language navigation system that finetunes internet-pretrained image diffusion models for path planning. Instead of directly predicting low-level actions, VENTURA generates a path mask (i.e. a visual plan) in image space that captures fine-grained, context-aware navigation behaviors. A lightweight behavior-cloning policy grounds these visual plans into executable trajectories, yielding an interface that follows natural language instructions to generate diverse robot behaviors. To scale training, we supervise on path masks derived from self-supervised tracking models paired with VLM-augmented captions, avoiding manual pixel-level annotation or highly engineered data collection setups. In extensive real-world evaluations, VENTURA outperforms state-of-the-art foundation model baselines on object reaching, obstacle avoidance, and terrain preference tasks, improving success rates by 33% and reducing collisions by 54% across both seen and unseen scenarios. Notably, we find that VENTURA generalizes to unseen combinations of distinct tasks, revealing emergent compositional capabilities. Videos, code, and additional materials: https://venturapath.github.io

