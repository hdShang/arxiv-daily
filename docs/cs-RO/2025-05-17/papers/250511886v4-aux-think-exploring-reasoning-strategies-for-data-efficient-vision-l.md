---
layout: default
title: "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation"
---

# Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11886" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11886v4</a>
  <a href="https://arxiv.org/pdf/2505.11886.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11886v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11886v4', 'Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuo Wang, Yongcai Wang, Wanting Li, Xudong Cai, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Deying Li, Zhaoxin Fan

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-17 (æ›´æ–°: 2025-10-14)

**æœŸåˆŠ**: NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAux-Thinkä»¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„æ¨ç†ç­–ç•¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€å¯¼èˆª` `æ¨ç†ç­–ç•¥` `é“¾å¼æ€ç»´` `å¤šæ¨¡æ€å­¦ä¹ ` `æ™ºèƒ½ä½“å¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰è¯­è¨€å¯¼èˆªæ–¹æ³•åœ¨æ¨ç†ç­–ç•¥çš„åº”ç”¨ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­ï¼Œæ¨ç†çš„æœ‰æ•ˆæ€§æœªå¾—åˆ°å……åˆ†éªŒè¯ã€‚
2. æœ¬æ–‡æå‡ºAux-Thinkæ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´ç›‘ç£è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ›´é«˜æ•ˆï¼ŒåŒæ—¶åœ¨åœ¨çº¿é¢„æµ‹ä¸­ç›´æ¥è¿›è¡ŒåŠ¨ä½œæ¨æ–­ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAux-Thinkåœ¨ç›¸åŒæ•°æ®è§„æ¨¡ä¸‹æ˜¾è‘—æå‡äº†å¯¼èˆªæ€§èƒ½ï¼Œå¹¶å‡å°‘äº†è®­ç»ƒæ‰€éœ€çš„å·¥ä½œé‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ˜¯å¼€å‘èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨å¤æ‚ç°å®ç¯å¢ƒä¸­å¯¼èˆªçš„å…·èº«æ™ºèƒ½ä½“çš„é‡è¦ä»»åŠ¡ã€‚å°½ç®¡å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨VLNä¸­æ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›å’ŒæŒ‡ä»¤åŸºç¡€ï¼Œä½†æ¨ç†ç­–ç•¥åœ¨å¯¼èˆªè¿™ä¸€ä»¥åŠ¨ä½œä¸ºä¸­å¿ƒçš„é•¿æœŸä»»åŠ¡ä¸­çš„ä½œç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†VLNä¸­çš„æ¨ç†ç­–ç•¥ï¼ŒåŒ…æ‹¬ç›´æ¥åŠ¨ä½œé¢„æµ‹ã€åŠ¨ä½œå‰æ¨ç†å’ŒåŠ¨ä½œåæ¨ç†ã€‚ç ”ç©¶å‘ç°æ¨ç†æ—¶é—´çš„æ¨ç†å´©æºƒé—®é¢˜ï¼Œè¡¨æ˜å°†æ¨ç†æ•´åˆåˆ°VLNä¸­çš„æŒ‘æˆ˜ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Aux-Thinkæ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´ç›‘ç£è®­ç»ƒæ¨¡å‹å†…åŒ–ç»“æ„åŒ–æ¨ç†æ¨¡å¼ï¼ŒåŒæ—¶åœ¨åœ¨çº¿é¢„æµ‹ä¸­ç›´æ¥æ¨æ–­åŠ¨ä½œã€‚ä¸ºæ”¯æŒè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†é¦–ä¸ªé“¾å¼æ€ç»´æ³¨é‡Šæ•°æ®é›†R2R-CoT-320kã€‚å®éªŒè¡¨æ˜ï¼ŒAux-Thinkå¤§å¹…å‡å°‘è®­ç»ƒå·¥ä½œé‡ï¼Œå¹¶åœ¨ç›¸åŒæ•°æ®è§„æ¨¡ä¸‹å®ç°æœ€ä½³æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­æ¨ç†ç­–ç•¥çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯æ¨ç†æ—¶é—´çš„æ¨ç†å´©æºƒé—®é¢˜ï¼Œè¿™å½±å“äº†å¯¼èˆªçš„å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­æœªèƒ½æœ‰æ•ˆæ•´åˆæ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAux-Thinkæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é“¾å¼æ€ç»´ç›‘ç£è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå†…åŒ–ç»“æ„åŒ–çš„æ¨ç†æ¨¡å¼ï¼ŒåŒæ—¶åœ¨å®é™…æ¨æ–­ä¸­ç›´æ¥è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ï¼Œé¿å…æ¨ç†è¿‡ç¨‹å¯¹å®æ—¶æ€§èƒ½çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ¨ç†ç­–ç•¥è¯„ä¼°ã€é“¾å¼æ€ç»´ç›‘ç£è®­ç»ƒå’Œåœ¨çº¿åŠ¨ä½œæ¨æ–­ã€‚é¦–å…ˆè¯„ä¼°ä¸åŒæ¨ç†ç­–ç•¥çš„æ•ˆæœï¼Œç„¶åé€šè¿‡ç›‘ç£å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ï¼Œæœ€ååœ¨æ¨æ–­é˜¶æ®µç›´æ¥ç”ŸæˆåŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†Aux-Thinkæ¡†æ¶ï¼Œç»“åˆäº†æ¨ç†ç­–ç•¥ä¸ç›´æ¥åŠ¨ä½œé¢„æµ‹çš„ä¼˜åŠ¿ï¼Œè§£å†³äº†æ¨ç†æ—¶é—´å´©æºƒçš„é—®é¢˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†é“¾å¼æ€ç»´çš„æŸå¤±å‡½æ•°æ¥æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”é•¿æ—¶é—´ä»»åŠ¡çš„éœ€æ±‚ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAux-Thinkåœ¨ç›¸åŒæ•°æ®è§„æ¨¡ä¸‹æ˜¾è‘—æå‡äº†å¯¼èˆªæ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æœ€ä½³æ¨¡å‹ï¼Œå‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œèµ„æºæ¶ˆè€—ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä»¥åŠè™šæ‹ŸåŠ©æ‰‹ç­‰é¢†åŸŸï¼Œèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›å’Œå“åº”æ•ˆç‡ã€‚æœªæ¥ï¼ŒAux-Thinkæ¡†æ¶å¯èƒ½æ¨åŠ¨æ›´é«˜æ•ˆçš„å¤šæ¨¡æ€äº¤äº’å’Œæ™ºèƒ½ä½“è‡ªä¸»å­¦ä¹ çš„ç ”ç©¶è¿›å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Navigation (VLN) is a critical task for developing embodied agents that can follow natural language instructions to navigate in complex real-world environments. Recent advances in VLN by large pretrained models have significantly improved generalization and instruction grounding compared to traditional approaches. However, the role of reasoning strategies in navigation-an action-centric, long-horizon task-remains underexplored, despite Chain-of-Thought (CoT) reasoning's demonstrated success in static tasks like visual question answering. To address this gap, we conduct the first systematic evaluation of reasoning strategies for VLN, including No-Think (direct action prediction), Pre-Think (reason before action), and Post-Think (reason after action). Surprisingly, our findings reveal the Inference-time Reasoning Collapse issue, where inference-time reasoning degrades navigation accuracy, highlighting the challenges of integrating reasoning into VLN. Based on this insight, we propose Aux-Think, a framework that trains models to internalize structured reasoning patterns through CoT supervision, while inferring action directly without reasoning in online prediction. To support this framework, we release R2R-CoT-320k, the first Chain-of-Thought annotated dataset for VLN. Extensive experiments show that Aux-Think reduces training effort greatly and achieves the best performance under the same data scale.

