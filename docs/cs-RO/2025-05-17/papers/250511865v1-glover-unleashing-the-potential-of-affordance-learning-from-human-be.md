---
layout: default
title: GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation
---

# GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11865" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11865v1</a>
  <a href="https://arxiv.org/pdf/2505.11865.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11865v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11865v1', 'GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Teli Ma, Jia Zheng, Zifan Wang, Ziyao Gao, Jiaming Zhou, Junwei Liang

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGLOVER++ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„å¯ä¾›æ€§å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)**

**å…³é”®è¯**: `å¯ä¾›æ€§å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `äººç±»ç¤ºèŒƒ` `å¤šæ¨¡æ€æ¨ç†` `æ•°æ®é›†æ„å»º` `æ·±åº¦å­¦ä¹ ` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¯ä¾›æ€§å­¦ä¹ ä¸Šé¢ä¸´æ•°æ®é›†ä¸è¶³å’Œå¤šæ ·åŒ–ä¸Šä¸‹æ–‡æ¢ç´¢ä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºHOVA-500Kæ•°æ®é›†å’ŒGLOVER++æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆè½¬ç§»äººç±»ç¤ºèŒƒä¸­çš„å¯ä¾›æ€§çŸ¥è¯†ã€‚
3. GLOVER++åœ¨HOVA-500KåŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»äººç±»ç¤ºèŒƒè§†é¢‘ä¸­å­¦ä¹ æ“ä½œæŠ€èƒ½ä¸ºé€šç”¨ä¸”å¯è§£é‡Šçš„æœºå™¨äººæ™ºèƒ½æä¾›äº†æœ‰å¸Œæœ›çš„è·¯å¾„ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¯ä¾›æ€§çš„è§†è§’ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡çš„ç²¾ç¡®å¯ä¾›æ€§æ³¨é‡Šæ•°æ®é›†å’Œå¯¹å¤šæ ·åŒ–æ“ä½œä¸Šä¸‹æ–‡ä¸­å¯ä¾›æ€§çš„æ¢ç´¢ä¸è¶³ï¼ŒçŸ¥è¯†è½¬ç§»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†HOVA-500Kï¼Œä¸€ä¸ªåŒ…å«500,000å¼ å›¾åƒã€1,726ä¸ªç‰©ä½“ç±»åˆ«å’Œ675ä¸ªåŠ¨ä½œçš„å¤§è§„æ¨¡å¯ä¾›æ€§æ³¨é‡Šæ•°æ®é›†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„å¤šæ¨¡æ€å¯ä¾›æ€§æ¨ç†åŸºå‡†å¥—ä»¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†GLOVER++ï¼Œä¸€ä¸ªå…¨çƒåˆ°å±€éƒ¨çš„å¯ä¾›æ€§è®­ç»ƒæ¡†æ¶ï¼Œæœ‰æ•ˆåœ°å°†äººç±»ç¤ºèŒƒä¸­çš„å¯ä¾›æ€§çŸ¥è¯†è½¬ç§»åˆ°ä¸‹æ¸¸å¼€æ”¾è¯æ±‡æ¨ç†ä»»åŠ¡ä¸­ã€‚GLOVER++åœ¨HOVA-500KåŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨å¤šæ ·åŒ–çš„ä¸‹æ¸¸æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»äººç±»ç¤ºèŒƒä¸­å­¦ä¹ å¯ä¾›æ€§çŸ¥è¯†çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†å’Œå¯¹å¤šæ ·åŒ–æ“ä½œä¸Šä¸‹æ–‡çš„æ¢ç´¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥HOVA-500Kæ•°æ®é›†å’ŒGLOVER++æ¡†æ¶ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§å…¨çƒåˆ°å±€éƒ¨çš„å¯ä¾›æ€§è®­ç»ƒæ–¹æ³•ï¼Œä»¥æœ‰æ•ˆè½¬ç§»å¯ä¾›æ€§çŸ¥è¯†ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGLOVER++æ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–ã€å¯ä¾›æ€§å»ºæ¨¡å’Œä¸‹æ¸¸ä»»åŠ¡æ¨ç†ç­‰ä¸»è¦æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šHOVA-500Kæ•°æ®é›†çš„æ„å»ºå’ŒGLOVER++æ¡†æ¶çš„è®¾è®¡æ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯ä¾›æ€§çŸ¥è¯†çš„è½¬ç§»ä¸Šï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€èåˆæŠ€æœ¯ï¼Œç»“åˆäº†å›¾åƒç‰¹å¾å’ŒåŠ¨ä½œä¿¡æ¯ï¼ŒæŸå¤±å‡½æ•°åˆ™é’ˆå¯¹å¯ä¾›æ€§æ¨ç†è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GLOVER++åœ¨HOVA-500KåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†åœ¨å¤šæ ·åŒ–ä¸‹æ¸¸æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡äº†15%ä»¥ä¸Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨åŒ–åˆ¶é€ ç­‰ã€‚é€šè¿‡æœ‰æ•ˆå­¦ä¹ äººç±»çš„æ“ä½œæ–¹å¼ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„ç¯å¢ƒå’Œä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œçµæ´»æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨å„è¡Œå„ä¸šä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning manipulation skills from human demonstration videos offers a promising path toward generalizable and interpretable robotic intelligence-particularly through the lens of actionable affordances. However, transferring such knowledge remains challenging due to: 1) a lack of large-scale datasets with precise affordance annotations, and 2) insufficient exploration of affordances in diverse manipulation contexts. To address these gaps, we introduce HOVA-500K, a large-scale, affordance-annotated dataset comprising 500,000 images across 1,726 object categories and 675 actions. We also release a standardized benchmarking suite for multi-modal affordance reasoning. Built upon HOVA-500K, we present GLOVER++, a global-to-local affordance training framework that effectively transfers actionable affordance knowledge from human demonstrations to downstream open-vocabulary reasoning tasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark and demonstrates strong generalization across diverse downstream robotic manipulation tasks. By explicitly modeling actionable affordances, GLOVER++ facilitates robust transfer across scenes, modalities, and tasks. We hope that HOVA-500K and the GLOVER++ framework will serve as valuable resources for bridging the gap between human demonstrations and robotic manipulation capabilities.

