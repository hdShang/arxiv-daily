---
layout: default
title: "H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos"
---

# H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11920" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11920v2</a>
  <a href="https://arxiv.org/pdf/2505.11920.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11920v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11920v2', 'H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guangrun Li, Yaoxu Lyu, Zhuoyang Liu, Chengkai Hou, Jieyu Zhang, Shanghang Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-17 (æ›´æ–°: 2025-05-26)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºH2Rä»¥è§£å†³äººæœºè§†è§‰å·®å¼‚é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æ•°æ®å¢å¼º` `æœºå™¨äººå­¦ä¹ ` `è§†è§‰å·®è·` `å…³é”®ç‚¹æ£€æµ‹` `åŠ¨ä½œåˆæˆ` `è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘` `é¢„è®­ç»ƒæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æœºå™¨äººå­¦ä¹ ä¸­å­˜åœ¨è§†è§‰å·®è·ï¼Œå¯¼è‡´åŸºäºäººç±»æ•°æ®çš„é¢„è®­ç»ƒæ¨¡å‹æ•ˆæœä¸ä½³ã€‚
2. H2Ré€šè¿‡æ£€æµ‹äººç±»æ‰‹éƒ¨å…³é”®ç‚¹å’Œåˆæˆæœºå™¨äººåŠ¨ä½œï¼Œç”Ÿæˆæœºå™¨äººä¸­å¿ƒçš„è®­ç»ƒæ•°æ®ï¼Œå¼¥åˆè§†è§‰å·®è·ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒH2Råœ¨æ¨¡æ‹Ÿå’Œç°å®ä»»åŠ¡ä¸­å‡æ˜¾è‘—æé«˜äº†æˆåŠŸç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è§„æ¨¡è§†é¢‘é¢„è®­ç»ƒåœ¨æœºå™¨äººå­¦ä¹ ä¸­å·²è¢«è¯æ˜æœ‰æ•ˆã€‚ç„¶è€Œï¼ŒåŸºäºäººç±»æ‰‹éƒ¨æ•°æ®é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨æœºå™¨äººå­¦ä¹ ä¸­å¯èƒ½è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºäººç±»æ‰‹ä¸ä¸åŒæœºå™¨äººæ‰‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è§†è§‰å·®è·ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºH2Rï¼Œä¸€ç§ç®€å•çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡æ£€æµ‹äººç±»æ‰‹éƒ¨å…³é”®ç‚¹ã€åœ¨æ¨¡æ‹Ÿä¸­åˆæˆæœºå™¨äººåŠ¨ä½œï¼Œå¹¶å°†æ¸²æŸ“çš„æœºå™¨äººåˆæˆåˆ°è‡ªæˆ‘ä¸­å¿ƒçš„è§†é¢‘ä¸­ï¼Œä»è€Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å¼åœ°å¼¥åˆäººç±»ä¸æœºå™¨äººè¡¨ç°ä¹‹é—´çš„è§†è§‰å·®è·ã€‚æˆ‘ä»¬å°†H2Råº”ç”¨äºå¢å¼ºå¤§è§„æ¨¡è‡ªæˆ‘ä¸­å¿ƒäººç±»è§†é¢‘æ•°æ®é›†ï¼Œå¦‚Ego4Då’ŒSSv2ï¼Œæ›¿æ¢äººç±»æ‰‹ä¸ºæ¨¡æ‹Ÿçš„æœºå™¨äººæ‰‹è‡‚ï¼Œç”Ÿæˆä»¥æœºå™¨äººä¸ºä¸­å¿ƒçš„è®­ç»ƒæ•°æ®ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªè¦†ç›–å¤šç§æœºå™¨äººè¡¨ç°çš„100ä¸‡è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é€šè¿‡CLIPåŸºç¡€çš„å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§åº¦é‡éªŒè¯å¢å¼ºç®¡é“çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººå­¦ä¹ ä¸­äººç±»æ‰‹ä¸æœºå™¨äººæ‰‹ä¹‹é—´çš„è§†è§‰å·®è·é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä½¿ç”¨äººç±»è§†é¢‘æ•°æ®è¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼Œæ— æ³•æœ‰æ•ˆé€‚åº”ä¸åŒæœºå™¨äººçš„æ‰‹éƒ¨è¡¨ç°ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šH2Rçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ£€æµ‹äººç±»æ‰‹éƒ¨å…³é”®ç‚¹ï¼Œåˆæˆæœºå™¨äººåŠ¨ä½œï¼Œå¹¶å°†æ¸²æŸ“çš„æœºå™¨äººåˆæˆåˆ°è‡ªæˆ‘ä¸­å¿ƒçš„è§†é¢‘ä¸­ï¼Œä»è€Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¼¥åˆäººç±»ä¸æœºå™¨äººä¹‹é—´çš„è§†è§‰å·®è·ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç”Ÿæˆçš„æ•°æ®æ›´ç¬¦åˆæœºå™¨äººæ“ä½œçš„å®é™…æƒ…å†µã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šH2Rçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) äººç±»æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹ï¼›2) æœºå™¨äººåŠ¨ä½œåˆæˆï¼›3) è§†é¢‘åˆæˆä¸æ¸²æŸ“ã€‚é€šè¿‡è¿™äº›æ¨¡å—çš„ååŒå·¥ä½œï¼Œç”Ÿæˆä»¥æœºå™¨äººä¸ºä¸­å¿ƒçš„è®­ç»ƒæ•°æ®ã€‚

**å…³é”®åˆ›æ–°**ï¼šH2Rçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†äººç±»è§†é¢‘ä¸­çš„æ‰‹éƒ¨åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººå¯ç”¨çš„è®­ç»ƒæ•°æ®ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›´æ¥ä½¿ç”¨äººç±»æ•°æ®çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººå­¦ä¹ çš„æ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨H2Rä¸­ï¼Œå…³é”®è®¾è®¡åŒ…æ‹¬æ‰‹éƒ¨å…³é”®ç‚¹çš„ç²¾ç¡®æ£€æµ‹ç®—æ³•ã€æœºå™¨äººåŠ¨ä½œçš„ç‰©ç†æ¨¡æ‹Ÿä»¥åŠè§†é¢‘åˆæˆä¸­çš„æ¸²æŸ“æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯ç»†èŠ‚ç¡®ä¿äº†ç”Ÿæˆæ•°æ®çš„é«˜è´¨é‡å’ŒçœŸå®æ„Ÿï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒH2Råœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­æˆåŠŸç‡æå‡5.0%-10.2%ï¼Œåœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­æå‡6.7%-23.3%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒH2Ræ˜¾è‘—æ”¹å–„äº†æœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸åŒè§†è§‰ç¼–ç å™¨å’Œç­–ç•¥å­¦ä¹ æ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

H2Rçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººå­¦ä¹ ã€è‡ªåŠ¨åŒ–æ“ä½œå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›æ›´ç¬¦åˆæœºå™¨äººæ“ä½œéœ€æ±‚çš„è®­ç»ƒæ•°æ®ï¼ŒH2Rèƒ½å¤Ÿå¸®åŠ©æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large-scale pre-training using videos has proven effective for robot learning. However, the models pre-trained on such data can be suboptimal for robot learning due to the significant visual gap between human hands and those of different robots. To remedy this, we propose H2R, a simple data augmentation technique that detects human hand keypoints, synthesizes robot motions in simulation, and composites rendered robots into egocentric videos. This process explicitly bridges the visual gap between human and robot embodiments during pre-training. We apply H2R to augment large-scale egocentric human video datasets such as Ego4D and SSv2, replacing human hands with simulated robotic arms to generate robot-centric training data. Based on this, we construct and release a family of 1M-scale datasets covering multiple robot embodiments (UR5 with gripper/Leaphand, Franka) and data sources (SSv2, Ego4D). To verify the effectiveness of the augmentation pipeline, we introduce a CLIP-based image-text similarity metric that quantitatively evaluates the semantic fidelity of robot-rendered frames to the original human actions. We validate H2R across three simulation benchmarks: Robomimic, RLBench and PushT and real-world manipulation tasks with a UR5 robot equipped with Gripper and Leaphand end-effectors. H2R consistently improves downstream success rates, yielding gains of 5.0%-10.2% in simulation and 6.7%-23.3% in real-world tasks across various visual encoders and policy learning methods. These results indicate that H2R improves the generalization ability of robotic policies by mitigating the visual discrepancies between human and robot domains.

