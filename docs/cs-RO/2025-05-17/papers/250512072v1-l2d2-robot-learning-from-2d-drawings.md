---
layout: default
title: L2D2: Robot Learning from 2D Drawings
---

# L2D2: Robot Learning from 2D Drawings

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.12072" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.12072v1</a>
  <a href="https://arxiv.org/pdf/2505.12072.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.12072v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.12072v1', 'L2D2: Robot Learning from 2D Drawings')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Shaunak A. Mehta, Heramb Nemlekar, Hari Sumant, Dylan P. Losey

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-17

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫L2D2‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Â≠¶‰π†‰ªªÂä°ÁöÑÁâ©ÁêÜÊåáÂØºÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Â≠¶‰π†` `Ê®°‰ªøÂ≠¶‰π†` `ËßÜËßâ-ËØ≠Ë®ÄÂàÜÂâ≤` `‰∫∫Êú∫‰∫§‰∫í` `ÂêàÊàêÂõæÂÉèÁîüÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊú∫Âô®‰∫∫Â≠¶‰π†ÊñπÊ≥ï‰æùËµñ‰∫é‰∫∫Á±ªÁöÑÁâ©ÁêÜÊåáÂØºÔºåÈöèÁùÄ‰ªªÂä°Â§çÊùÇÂ∫¶Â¢ûÂä†ÔºåËøôÁßçÊñπÊ≥ïÂèòÂæóÊûÅÂÖ∂ÁπÅÁêê‰∏î‰∏çÈ´òÊïà„ÄÇ
2. L2D2ÈÄöËøáÁªòÂõæÁïåÈù¢ËÆ©Áî®Êà∑Âú®2DÂõæÂÉè‰∏äÁªòÂà∂‰ªªÂä°ËΩ®ËøπÔºåÁªìÂêàËßÜËßâ-ËØ≠Ë®ÄÂàÜÂâ≤ÊäÄÊúØÔºåÂáèÂ∞ë‰∫ÜÂØπÁâ©ÁêÜÁéØÂ¢ÉÁöÑÂπ≤È¢Ñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåL2D2Âú®Â≠¶‰π†ÊïàÁéáÂíå‰ªªÂä°Ë°®Áé∞‰∏ä‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÁî®Êà∑Âú®Êèê‰æõÁ§∫ËåÉÊó∂ÊâÄÈúÄÁöÑÊó∂Èó¥ÂíåÁ≤æÂäõÊòæËëóÂáèÂ∞ë„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú∫Âô®‰∫∫Â∫îÂΩìËÉΩÂ§ü‰ªé‰∫∫Á±ªÈÇ£ÈáåÂ≠¶‰π†Êñ∞‰ªªÂä°Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫é‰∫∫Á±ªÂØπÊú∫Âô®‰∫∫ÊâãËáÇÁöÑÁâ©ÁêÜÊåáÂØºÔºåËøôÂú®Êï∞ÊçÆÈáèÂ¢ûÂ§ßÊó∂ÂèòÂæóÈùûÂ∏∏ÁπÅÁêê„ÄÇÊú¨ÊñáÊèêÂá∫L2D2Ôºå‰∏Ä‰∏™ÈÄöËøáÁªòÂõæÁïåÈù¢ÂíåÊ®°‰ªøÂ≠¶‰π†ÁÆóÊ≥ïÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÁöÑÁ≥ªÁªü„ÄÇÁî®Êà∑ÂèØ‰ª•Âú®Âπ≥Êùø‰∏äÁªòÂà∂‰ªªÂä°ËΩ®ËøπÔºåL2D2Âà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÂàÜÂâ≤ÊäÄÊúØËá™Âä®ÁîüÊàêÂêàÊàêÂõæÂÉèÔºåÂáèÂ∞ë‰∫Ü‰∫∫Á±ªÂØπÁéØÂ¢ÉÁöÑÁâ©ÁêÜÈáçÁΩÆÈúÄÊ±Ç„ÄÇÂ∞ΩÁÆ°ÁªòÂõæ‰ø°ÊÅØÈáèËæÉÂ∞ëÔºå‰ΩÜL2D2ÈÄöËøáÂ∞ëÈáèÁöÑÁâ©ÁêÜÁ§∫ËåÉÂ∞ÜÈùôÊÄÅ2DÁªòÂõæ‰∏éÂä®ÊÄÅ3D‰∏ñÁïåÁõ∏ÁªìÂêàÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂ≠¶‰π†ÊïàÁéáÂíå‰ªªÂä°Ë°®Áé∞„ÄÇÂÆûÈ™åË°®ÊòéÔºåL2D2Âú®Êó∂Èó¥ÂíåÁ≤æÂäõ‰∏ä‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÁî®Êà∑Êõ¥ÂÄæÂêë‰∫é‰ΩøÁî®ÁªòÂõæËÄåÈùûÁâ©ÁêÜÊìç‰Ωú„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Â≠¶‰π†‰ªªÂä°Êó∂ÂØπ‰∫∫Á±ªÁâ©ÁêÜÊåáÂØºÁöÑ‰æùËµñÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Êï∞ÊçÆÈáèÂ¢ûÂä†Êó∂ÔºåË¶ÅÊ±Ç‰∫∫Á±ª‰∏çÊñ≠Ë∞ÉÊï¥ÁéØÂ¢ÉÔºåÂØºËá¥ÊïàÁéá‰Ωé‰∏ã„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöL2D2ÈÄöËøáËÆ©Áî®Êà∑Âú®Âπ≥Êùø‰∏äÁªòÂà∂‰ªªÂä°ËΩ®ËøπÔºåÁªìÂêàËßÜËßâ-ËØ≠Ë®ÄÂàÜÂâ≤ÊäÄÊúØÔºåÂÖÅËÆ∏Êú∫Âô®‰∫∫Âú®‰∏çÈúÄË¶ÅÁâ©ÁêÜÈáçÁΩÆÁéØÂ¢ÉÁöÑÊÉÖÂÜµ‰∏ãÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÁ§∫ËåÉÔºå‰ªéËÄåÊèêÈ´òÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöL2D2ÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Áî®Êà∑ÁªòÂõæÊ®°Âùó„ÄÅËßÜËßâ-ËØ≠Ë®ÄÂàÜÂâ≤Ê®°ÂùóÂíåÊ®°‰ªøÂ≠¶‰π†Ê®°Âùó„ÄÇÁî®Êà∑ÈÄöËøáÁªòÂõæÊèê‰æõ‰ªªÂä°Á§∫ËåÉÔºåÁ≥ªÁªüËá™Âä®ÁîüÊàêÂêàÊàêÂõæÂÉèÂπ∂ËøõË°åÂ≠¶‰π†„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöL2D2ÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÈùôÊÄÅ2DÁªòÂõæ‰∏éÂä®ÊÄÅ3DÁéØÂ¢ÉÁªìÂêàÔºåÈÄöËøáÂ∞ëÈáèÁöÑÁâ©ÁêÜÁ§∫ËåÉÊù•Â¢ûÂº∫Â≠¶‰π†ÊïàÊûú„ÄÇËøô‰∏ÄÊñπÊ≥ïÊòæËëóÂáèÂ∞ë‰∫ÜÂØπ‰∫∫Á±ªÁâ©ÁêÜÂπ≤È¢ÑÁöÑÈúÄÊ±Ç„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåL2D2ÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñÁªòÂõæ‰∏éÂÆûÈôÖÂä®‰Ωú‰πãÈó¥ÁöÑÊò†Â∞ÑÂÖ≥Á≥ªÔºåÂπ∂‰ΩøÁî®‰∫ÜÊ∑±Â∫¶Â≠¶‰π†ÁΩëÁªúÊù•Â§ÑÁêÜËßÜËßâ-ËØ≠Ë®ÄÂàÜÂâ≤‰ªªÂä°ÔºåÁ°Æ‰øùÁîüÊàêÁöÑÂêàÊàêÂõæÂÉè‰∏éÁúüÂÆûÁéØÂ¢ÉÁõ∏Á¨¶„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåL2D2Âú®Â≠¶‰π†Êú∫Âô®‰∫∫Á≠ñÁï•Êó∂Ë°®Áé∞Âá∫Êõ¥È´òÁöÑÊïàÁéáÔºåÊâÄÈúÄÊï∞ÊçÆÈõÜËßÑÊ®°Êõ¥Â∞èÔºå‰∏îËÉΩÂ§üÂú®ËæÉÈïøÊó∂Èó¥ËåÉÂõ¥ÂÜÖËøõË°å‰ªªÂä°Ê≥õÂåñ„ÄÇ‰∏éÂÖ∂‰ªñÁªòÂõæÊñπÊ≥ïÁõ∏ÊØîÔºåL2D2ÁöÑÊÄßËÉΩÊèêÂçáÊòæËëóÔºåÁî®Êà∑Âú®Êèê‰æõÁ§∫ËåÉÊó∂ÁöÑÊó∂Èó¥ÂíåÁ≤æÂäõÊ∂àËÄóÂáèÂ∞ë‰∫ÜÁ∫¶30%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

L2D2ÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Â§ö‰∏™È¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®Â∫îÁî®‰ª∑ÂÄºÔºåÂåÖÊã¨ÊïôËÇ≤Êú∫Âô®‰∫∫„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫‰ª•ÂèäÂ∑•‰∏öËá™Âä®ÂåñÁ≠â„ÄÇÈÄöËøáÁÆÄÂåñ‰∫∫Êú∫‰∫§‰∫íËøáÁ®ãÔºåL2D2ÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫Êõ¥È´òÊïàÂú∞Â≠¶‰π†Â§çÊùÇ‰ªªÂä°ÔºåÊèêÂçáÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Robots should learn new tasks from humans. But how do humans convey what they want the robot to do? Existing methods largely rely on humans physically guiding the robot arm throughout their intended task. Unfortunately -- as we scale up the amount of data -- physical guidance becomes prohibitively burdensome. Not only do humans need to operate robot hardware but also modify the environment (e.g., moving and resetting objects) to provide multiple task examples. In this work we propose L2D2, a sketching interface and imitation learning algorithm where humans can provide demonstrations by drawing the task. L2D2 starts with a single image of the robot arm and its workspace. Using a tablet, users draw and label trajectories on this image to illustrate how the robot should act. To collect new and diverse demonstrations, we no longer need the human to physically reset the workspace; instead, L2D2 leverages vision-language segmentation to autonomously vary object locations and generate synthetic images for the human to draw upon. We recognize that drawing trajectories is not as information-rich as physically demonstrating the task. Drawings are 2-dimensional and do not capture how the robot's actions affect its environment. To address these fundamental challenges the next stage of L2D2 grounds the human's static, 2D drawings in our dynamic, 3D world by leveraging a small set of physical demonstrations. Our experiments and user study suggest that L2D2 enables humans to provide more demonstrations with less time and effort than traditional approaches, and users prefer drawings over physical manipulation. When compared to other drawing-based approaches, we find that L2D2 learns more performant robot policies, requires a smaller dataset, and can generalize to longer-horizon tasks. See our project website: https://collab.me.vt.edu/L2D2/

