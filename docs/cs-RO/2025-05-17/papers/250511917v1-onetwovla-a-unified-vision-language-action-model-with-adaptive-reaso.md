---
layout: default
title: OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning
---

# OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11917" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11917v1</a>
  <a href="https://arxiv.org/pdf/2505.11917.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11917v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11917v1', 'OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, Yang Gao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-17

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºOneTwoVLAä»¥è§£å†³æœºå™¨äººä»»åŠ¡æ‰§è¡Œä¸­çš„æ¨ç†ä¸è¡ŒåŠ¨åˆ†ç¦»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-è¡ŒåŠ¨` `è‡ªé€‚åº”æ¨ç†` `æœºå™¨äººä»»åŠ¡è§„åˆ’` `äººæœºäº¤äº’` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŒç³»ç»Ÿæ–¹æ³•åœ¨æ¨ç†ä¸è¡ŒåŠ¨ä¹‹é—´å­˜åœ¨èƒ½åŠ›ç†è§£ä¸è¶³å’Œå»¶è¿Ÿç­‰é—®é¢˜ï¼Œé™åˆ¶äº†æœºå™¨äººæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚
2. OneTwoVLAæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°åœ¨æ¨ç†å’Œè¡ŒåŠ¨æ¨¡å¼ä¹‹é—´åˆ‡æ¢ï¼Œä»è€Œæé«˜ä»»åŠ¡æ‰§è¡Œçš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒOneTwoVLAåœ¨é•¿æ—¶é—´ä»»åŠ¡è§„åˆ’ã€é”™è¯¯æ£€æµ‹ä¸æ¢å¤ç­‰æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šç”¨æœºå™¨äººéœ€è¦å…·å¤‡å¤šæ ·åŒ–ä»»åŠ¡çš„ååŒæ¨ç†å’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŒç³»ç»Ÿæ–¹æ³•åœ¨é«˜å±‚æ¨ç†ä¸ä½å±‚è¡ŒåŠ¨ä¹‹é—´å­˜åœ¨èƒ½åŠ›ç†è§£ä¸è¶³å’Œå»¶è¿Ÿç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†OneTwoVLAï¼Œä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œæ¨ç†å’Œè¡ŒåŠ¨ã€‚OneTwoVLAåœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­èƒ½å¤Ÿè‡ªé€‚åº”åœ°åœ¨æ¨ç†å’Œè¡ŒåŠ¨æ¨¡å¼ä¹‹é—´åˆ‡æ¢ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¯æ‰©å±•çš„ç®¡é“æ¥åˆæˆä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è§†è§‰-è¯­è¨€æ•°æ®ï¼Œä»¥ä¸æœºå™¨äººæ•°æ®å…±åŒè®­ç»ƒã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒOneTwoVLAåœ¨é•¿æ—¶é—´ä»»åŠ¡è§„åˆ’ã€é”™è¯¯æ£€æµ‹ä¸æ¢å¤ã€è‡ªç„¶äººæœºäº¤äº’å’Œå¯æ³›åŒ–è§†è§‰å®šä½ç­‰å››ä¸ªå…³é”®èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä½¿å…¶èƒ½å¤Ÿæ‰§è¡Œå¦‚ç«é”…åˆ¶ä½œæˆ–é¸¡å°¾é…’æ··åˆç­‰å¤æ‚æ“ä½œã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰åŒç³»ç»Ÿæ–¹æ³•åœ¨é«˜å±‚æ¨ç†ä¸ä½å±‚è¡ŒåŠ¨ä¹‹é—´çš„åˆ†ç¦»é—®é¢˜ï¼Œå¯¼è‡´çš„èƒ½åŠ›ç†è§£ä¸è¶³å’Œå»¶è¿Ÿç­‰æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOneTwoVLAé€šè¿‡è®¾è®¡ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°åˆ‡æ¢æ¨ç†å’Œè¡ŒåŠ¨æ¨¡å¼ï¼Œä»¥æé«˜æœºå™¨äººçš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOneTwoVLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è§†è§‰è¾“å…¥å¤„ç†ã€è¯­è¨€ç†è§£ã€æ¨ç†æ¨¡å—å’Œè¡ŒåŠ¨ç”Ÿæˆæ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯ç³»ç»Ÿï¼Œèƒ½å¤Ÿå®æ—¶å“åº”ç¯å¢ƒå˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šOneTwoVLAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªé€‚åº”åˆ‡æ¢æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹åœ¨å…³é”®æ—¶åˆ»è¿›è¡Œæ¨ç†ï¼Œè€Œåœ¨å…¶ä»–æ—¶åˆ»åˆ™åŸºäºæœ€è¿‘çš„æ¨ç†ç”Ÿæˆè¡ŒåŠ¨ï¼Œè¿™ç§è®¾è®¡ä¸ä¼ ç»Ÿçš„åˆ†ç¦»å¼ç³»ç»Ÿæœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†å¤šå±‚ç¥ç»ç½‘ç»œç»“æ„ï¼Œç»“åˆäº†è§†è§‰å’Œè¯­è¨€ç‰¹å¾çš„èåˆï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡æ¨ç†å‡†ç¡®æ€§ä¸è¡ŒåŠ¨æœ‰æ•ˆæ€§çš„å¹³è¡¡ï¼ŒåŒæ—¶å¼•å…¥äº†å¯æ‰©å±•çš„æ•°æ®åˆæˆç®¡é“ä»¥å¢å¼ºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒOneTwoVLAåœ¨é•¿æ—¶é—´ä»»åŠ¡è§„åˆ’ã€é”™è¯¯æ£€æµ‹ä¸æ¢å¤ç­‰å››ä¸ªå…³é”®èƒ½åŠ›ä¸Šå‡è¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ä»»åŠ¡æ‰§è¡Œä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OneTwoVLAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€æ™ºèƒ½å®¶å±…ã€åŒ»ç–—è¾…åŠ©ç­‰ã€‚å…¶è‡ªé€‚åº”æ¨ç†ä¸è¡ŒåŠ¨èƒ½åŠ›å°†æ¨åŠ¨æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–å’Œæ‰§è¡Œèƒ½åŠ›ï¼Œæå‡äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> General-purpose robots capable of performing diverse tasks require synergistic reasoning and acting capabilities. However, recent dual-system approaches, which separate high-level reasoning from low-level acting, often suffer from challenges such as limited mutual understanding of capabilities between systems and latency issues. This paper introduces OneTwoVLA, a single unified vision-language-action model that can perform both acting (System One) and reasoning (System Two). Crucially, OneTwoVLA adaptively switches between two modes: explicitly reasoning at critical moments during task execution, and generating actions based on the most recent reasoning at other times. To further unlock OneTwoVLA's reasoning and generalization capabilities, we design a scalable pipeline for synthesizing embodied reasoning-centric vision-language data, used for co-training with robot data. We validate OneTwoVLA's effectiveness through extensive experiments, highlighting its superior performance across four key capabilities: long-horizon task planning, error detection and recovery, natural human-robot interaction, and generalizable visual grounding, enabling the model to perform long-horizon, highly dexterous manipulation tasks such as making hotpot or mixing cocktails.

