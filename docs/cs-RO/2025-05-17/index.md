---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-17
---

# cs.ROï¼ˆ2025-05-17ï¼‰

ğŸ“Š å…± **15** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (11 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (11 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250511920v2-h2r-a-human-to-robot-data-augmentation-for-robot-pre-training-from-v.html">H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos</a></td>
  <td>æå‡ºH2Rä»¥è§£å†³äººæœºè§†è§‰å·®å¼‚é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11920v2" data-paper-url="./papers/250511920v2-h2r-a-human-to-robot-data-augmentation-for-robot-pre-training-from-v.html" onclick="toggleFavorite(this, '2505.11920v2', 'H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250511917v1-onetwovla-a-unified-vision-language-action-model-with-adaptive-reaso.html">OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning</a></td>
  <td>æå‡ºOneTwoVLAä»¥è§£å†³æœºå™¨äººä»»åŠ¡æ‰§è¡Œä¸­çš„æ¨ç†ä¸è¡ŒåŠ¨åˆ†ç¦»é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11917v1" data-paper-url="./papers/250511917v1-onetwovla-a-unified-vision-language-action-model-with-adaptive-reaso.html" onclick="toggleFavorite(this, '2505.11917v1', 'OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250511865v1-glover-unleashing-the-potential-of-affordance-learning-from-human-be.html">GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation</a></td>
  <td>æå‡ºGLOVER++ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„å¯ä¾›æ€§å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11865v1" data-paper-url="./papers/250511865v1-glover-unleashing-the-potential-of-affordance-learning-from-human-be.html" onclick="toggleFavorite(this, '2505.11865v1', 'GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250511858v1-integrating-model-based-control-and-rl-for-sim2real-transfer-of-tigh.html">Integrating Model-based Control and RL for Sim2Real Transfer of Tight Insertion Policies</a></td>
  <td>æå‡ºé›†æˆæ¨¡å‹æ§åˆ¶ä¸å¼ºåŒ–å­¦ä¹ ä»¥è§£å†³ç´§å‡‘æ’å…¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">zero-shot transfer</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11858v1" data-paper-url="./papers/250511858v1-integrating-model-based-control-and-rl-for-sim2real-transfer-of-tigh.html" onclick="toggleFavorite(this, '2505.11858v1', 'Integrating Model-based Control and RL for Sim2Real Transfer of Tight Insertion Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250511808v2-human-centered-development-of-guide-dog-robots-quiet-and-stable-loco.html">Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control</a></td>
  <td>æå‡ºä¸€ç§æ–°å‹æ­¥æ€æ§åˆ¶å™¨ä»¥è§£å†³ç›²äººå¯¼ç›²çŠ¬æœºå™¨äººè¿åŠ¨å™ªéŸ³ä¸ä¸ç¨³å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">Unitree</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11808v2" data-paper-url="./papers/250511808v2-human-centered-development-of-guide-dog-robots-quiet-and-stable-loco.html" onclick="toggleFavorite(this, '2505.11808v2', 'Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250512029v1-growable-and-interpretable-neural-control-with-online-continual-lear.html">Growable and Interpretable Neural Control with Online Continual Learning for Autonomous Lifelong Locomotion Learning Machines</a></td>
  <td>æå‡ºGOLLUMä»¥è§£å†³æŒç»­è¿åŠ¨å­¦ä¹ ä¸­çš„å››å¤§æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12029v1" data-paper-url="./papers/250512029v1-growable-and-interpretable-neural-control-with-online-continual-lear.html" onclick="toggleFavorite(this, '2505.12029v1', 'Growable and Interpretable Neural Control with Online Continual Learning for Autonomous Lifelong Locomotion Learning Machines')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250511848v1-probe-proprioceptive-obstacle-detection-and-estimation-while-navigat.html">PROBE: Proprioceptive Obstacle Detection and Estimation while Navigating in Clutter</a></td>
  <td>æå‡ºPROBEä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸­çš„éšœç¢ç‰©æ£€æµ‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">Unitree</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11848v1" data-paper-url="./papers/250511848v1-probe-proprioceptive-obstacle-detection-and-estimation-while-navigat.html" onclick="toggleFavorite(this, '2505.11848v1', 'PROBE: Proprioceptive Obstacle Detection and Estimation while Navigating in Clutter')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250512072v1-l2d2-robot-learning-from-2d-drawings.html">L2D2: Robot Learning from 2D Drawings</a></td>
  <td>æå‡ºL2D2ä»¥è§£å†³æœºå™¨äººå­¦ä¹ ä»»åŠ¡çš„ç‰©ç†æŒ‡å¯¼é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12072v1" data-paper-url="./papers/250512072v1-l2d2-robot-learning-from-2d-drawings.html" onclick="toggleFavorite(this, '2505.12072v1', 'L2D2: Robot Learning from 2D Drawings')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250512084v1-bench-npin-benchmarking-non-prehensile-interactive-navigation.html">Bench-NPIN: Benchmarking Non-prehensile Interactive Navigation</a></td>
  <td>æå‡ºBench-NPINä»¥è§£å†³éæŠ“å–äº¤äº’å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12084v1" data-paper-url="./papers/250512084v1-bench-npin-benchmarking-non-prehensile-interactive-navigation.html" onclick="toggleFavorite(this, '2505.12084v1', 'Bench-NPIN: Benchmarking Non-prehensile Interactive Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250511975v1-proactive-tactile-exploration-for-object-agnostic-shape-reconstructi.html">Proactive tactile exploration for object-agnostic shape reconstruction from minimal visual priors</a></td>
  <td>æå‡ºä¸»åŠ¨è§¦è§‰æ¢ç´¢ä»¥è§£å†³ç‰©ä½“æ— å…³å½¢çŠ¶é‡å»ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11975v1" data-paper-url="./papers/250511975v1-proactive-tactile-exploration-for-object-agnostic-shape-reconstructi.html" onclick="toggleFavorite(this, '2505.11975v1', 'Proactive tactile exploration for object-agnostic shape reconstruction from minimal visual priors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250511818v1-master-rules-from-chaos-learning-to-reason-plan-and-interact-from-ch.html">Master Rules from Chaos: Learning to Reason, Plan, and Interact from Chaos for Tangram Assembly</a></td>
  <td>æå‡ºMRChaosä»¥è§£å†³æœºå™¨äººæ‹¼å›¾ç»„è£…ä¸­çš„æ¨ç†ä¸è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11818v1" data-paper-url="./papers/250511818v1-master-rules-from-chaos-learning-to-reason-plan-and-interact-from-ch.html" onclick="toggleFavorite(this, '2505.11818v1', 'Master Rules from Chaos: Learning to Reason, Plan, and Interact from Chaos for Tangram Assembly')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250511794v1-gaussian-splatting-as-a-unified-representation-for-autonomy-in-unstr.html">Gaussian Splatting as a Unified Representation for Autonomy in Unstructured Environments</a></td>
  <td>æå‡ºé«˜æ–¯ç‚¹äº‘è¡¨ç¤ºä»¥è§£å†³éç»“æ„åŒ–ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">gaussian splatting</span> <span class="paper-tag">splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11794v1" data-paper-url="./papers/250511794v1-gaussian-splatting-as-a-unified-representation-for-autonomy-in-unstr.html" onclick="toggleFavorite(this, '2505.11794v1', 'Gaussian Splatting as a Unified Representation for Autonomy in Unstructured Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250511941v1-online-synthesis-of-control-barrier-functions-with-local-occupancy-g.html">Online Synthesis of Control Barrier Functions with Local Occupancy Grid Maps for Safe Navigation in Unknown Environments</a></td>
  <td>æå‡ºåŸºäºå±€éƒ¨å ç”¨ç½‘æ ¼å›¾çš„æ§åˆ¶å±éšœå‡½æ•°åœ¨çº¿åˆæˆæ–¹æ³•ä»¥è§£å†³æœªçŸ¥ç¯å¢ƒä¸­çš„å®‰å…¨å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">occupancy grid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11941v1" data-paper-url="./papers/250511941v1-online-synthesis-of-control-barrier-functions-with-local-occupancy-g.html" onclick="toggleFavorite(this, '2505.11941v1', 'Online Synthesis of Control Barrier Functions with Local Occupancy Grid Maps for Safe Navigation in Unknown Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>14</td>
  <td><a href="./papers/250512153v2-federated-deep-reinforcement-learning-for-privacy-preserving-robotic.html">Federated Deep Reinforcement Learning for Privacy-Preserving Robotic-Assisted Surgery</a></td>
  <td>æå‡ºè”é‚¦æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³éšç§ä¿æŠ¤çš„æœºå™¨äººè¾…åŠ©æ‰‹æœ¯é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">OMOMO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.12153v2" data-paper-url="./papers/250512153v2-federated-deep-reinforcement-learning-for-privacy-preserving-robotic.html" onclick="toggleFavorite(this, '2505.12153v2', 'Federated Deep Reinforcement Learning for Privacy-Preserving Robotic-Assisted Surgery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250511886v4-aux-think-exploring-reasoning-strategies-for-data-efficient-vision-l.html">Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation</a></td>
  <td>æå‡ºAux-Thinkä»¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„æ¨ç†ç­–ç•¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11886v4" data-paper-url="./papers/250511886v4-aux-think-exploring-reasoning-strategies-for-data-efficient-vision-l.html" onclick="toggleFavorite(this, '2505.11886v4', 'Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)