---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-27
---

# cs.ROï¼ˆ2025-05-27ï¼‰

ğŸ“Š å…± **22** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (14)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (14 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250520619v3-gait-conditioned-reinforcement-learning-with-multi-phase-curriculum-.html">Gait-Conditioned Reinforcement Learning with Multi-Phase Curriculum for Humanoid Locomotion</a></td>
  <td>æå‡ºåŸºäºæ­¥æ€æ¡ä»¶çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥å®ç°ç±»äººæœºå™¨äººå¤šæ¨¡å¼è¿åŠ¨</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">humanoid control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20619v3" data-paper-url="./papers/250520619v3-gait-conditioned-reinforcement-learning-with-multi-phase-curriculum-.html" onclick="toggleFavorite(this, '2505.20619v3', 'Gait-Conditioned Reinforcement Learning with Multi-Phase Curriculum for Humanoid Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250520829v2-learning-a-unified-policy-for-position-and-force-control-in-legged-l.html">Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation</a></td>
  <td>æå‡ºç»Ÿä¸€ç­–ç•¥ä»¥è§£å†³è…¿å¼æœºå™¨äººä½ç½®ä¸åŠ›æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">humanoid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20829v2" data-paper-url="./papers/250520829v2-learning-a-unified-policy-for-position-and-force-control-in-legged-l.html" onclick="toggleFavorite(this, '2505.20829v2', 'Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250521652v3-partinstruct-part-level-instruction-following-for-fine-grained-robot.html">PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation</a></td>
  <td>æå‡ºPartInstructä»¥è§£å†³ç»†ç²’åº¦æœºå™¨äººæ“æ§ä¸­çš„æŒ‡ä»¤è·Ÿéšé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">instruction following</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21652v3" data-paper-url="./papers/250521652v3-partinstruct-part-level-instruction-following-for-fine-grained-robot.html" onclick="toggleFavorite(this, '2505.21652v3', 'PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250520814v1-spatial-robograsp-generalized-robotic-grasping-control-policy.html">Spatial RoboGrasp: Generalized Robotic Grasping Control Policy</a></td>
  <td>æå‡ºç»Ÿä¸€æ¡†æ¶ä»¥è§£å†³æœºå™¨äººæŠ“å–æ§åˆ¶çš„ç©ºé—´æ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20814v1" data-paper-url="./papers/250520814v1-spatial-robograsp-generalized-robotic-grasping-control-policy.html" onclick="toggleFavorite(this, '2505.20814v1', 'Spatial RoboGrasp: Generalized Robotic Grasping Control Policy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250521665v2-convergent-functions-divergent-forms.html">Convergent Functions, Divergent Forms</a></td>
  <td>æå‡ºLOKIæ¡†æ¶ä»¥é«˜æ•ˆè®¾è®¡é€‚åº”æ€§å½¢æ€ä¸æ§åˆ¶ç­–ç•¥</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">bipedal</span> <span class="paper-tag">biped</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21665v2" data-paper-url="./papers/250521665v2-convergent-functions-divergent-forms.html" onclick="toggleFavorite(this, '2505.21665v2', 'Convergent Functions, Divergent Forms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250521351v1-equact-an-se3-equivariant-multi-task-transformer-for-open-loop-robot.html">EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop Robotic Manipulation</a></td>
  <td>æå‡ºEquActè§£å†³SE(3)ä¸å˜æ€§é—®é¢˜ä»¥æå‡æœºå™¨äººæ“ä½œèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">geometric consistency</span> <span class="paper-tag">language conditioned</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21351v1" data-paper-url="./papers/250521351v1-equact-an-se3-equivariant-multi-task-transformer-for-open-loop-robot.html" onclick="toggleFavorite(this, '2505.21351v1', 'EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250521495v1-clamp-crowdsourcing-a-large-scale-in-the-wild-haptic-dataset-with-an.html">CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception</a></td>
  <td>æå‡ºCLAMPè®¾å¤‡ä»¥è§£å†³å¤§è§„æ¨¡è§¦è§‰æ•°æ®é›†ç¼ºä¹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21495v1" data-paper-url="./papers/250521495v1-clamp-crowdsourcing-a-large-scale-in-the-wild-haptic-dataset-with-an.html" onclick="toggleFavorite(this, '2505.21495v1', 'CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250520751v2-interactive-ot-gym-a-reinforcement-learning-based-interactive-optica.html">Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform</a></td>
  <td>æå‡ºInteractive OT Gymä»¥è§£å†³å…‰å­¦é•Šå­é©±åŠ¨å¾®æœºå™¨äººåä½œæ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">shared control</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20751v2" data-paper-url="./papers/250520751v2-interactive-ot-gym-a-reinforcement-learning-based-interactive-optica.html" onclick="toggleFavorite(this, '2505.20751v2', 'Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250521161v2-collision-probability-estimation-for-optimization-based-vehicular-mo.html">Collision Probability Estimation for Optimization-based Vehicular Motion Planning</a></td>
  <td>æå‡ºåŸºäºä¼˜åŒ–çš„ç¢°æ’æ¦‚ç‡ä¼°è®¡ä»¥æå‡è‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’</td>
  <td class="tags-cell"><span class="paper-tag">model predictive control</span> <span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21161v2" data-paper-url="./papers/250521161v2-collision-probability-estimation-for-optimization-based-vehicular-mo.html" onclick="toggleFavorite(this, '2505.21161v2', 'Collision Probability Estimation for Optimization-based Vehicular Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250520795v1-learning-generalizable-robot-policy-with-human-demonstration-video-a.html">Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt</a></td>
  <td>æå‡ºä¸€ç§æ–°æ¡†æ¶ä»¥åˆ©ç”¨äººç±»ç¤ºèŒƒè§†é¢‘å­¦ä¹ é€šç”¨æœºå™¨äººç­–ç•¥</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous manipulation</span> <span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20795v1" data-paper-url="./papers/250520795v1-learning-generalizable-robot-policy-with-human-demonstration-video-a.html" onclick="toggleFavorite(this, '2505.20795v1', 'Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250521594v1-fast-and-cost-effective-speculative-edge-cloud-decoding-with-early-e.html">Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits</a></td>
  <td>æå‡ºå¿«é€Ÿä¸”ç»æµçš„è¾¹ç¼˜äº‘è§£ç æ¡†æ¶ä»¥é™ä½LLMéƒ¨ç½²æˆæœ¬</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">Unitree</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21594v1" data-paper-url="./papers/250521594v1-fast-and-cost-effective-speculative-edge-cloud-decoding-with-early-e.html" onclick="toggleFavorite(this, '2505.21594v1', 'Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250520926v1-com-adjustment-mechanism-control-for-multi-configuration-motion-stab.html">COM Adjustment Mechanism Control for Multi-Configuration Motion Stability of Unmanned Deformable Vehicle</a></td>
  <td>æå‡ºè´¨å¿ƒè°ƒæ•´æœºåˆ¶ä»¥è§£å†³æ— äººå˜å½¢è½¦çš„å¤šé…ç½®è¿åŠ¨ç¨³å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">humanoid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20926v1" data-paper-url="./papers/250520926v1-com-adjustment-mechanism-control-for-multi-configuration-motion-stab.html" onclick="toggleFavorite(this, '2505.20926v1', 'COM Adjustment Mechanism Control for Multi-Configuration Motion Stability of Unmanned Deformable Vehicle')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250520726v2-manitaskgen-a-comprehensive-task-generator-for-benchmarking-and-impr.html">ManiTaskGen: A Comprehensive Task Generator for Benchmarking and Improving Vision-Language Agents on Embodied Decision-Making</a></td>
  <td>æå‡ºManiTaskGenä»¥è§£å†³ç°æœ‰ä»»åŠ¡ç”Ÿæˆä¸è¶³çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20726v2" data-paper-url="./papers/250520726v2-manitaskgen-a-comprehensive-task-generator-for-benchmarking-and-impr.html" onclick="toggleFavorite(this, '2505.20726v2', 'ManiTaskGen: A Comprehensive Task Generator for Benchmarking and Improving Vision-Language Agents on Embodied Decision-Making')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250521734v1-mind-stack-modular-interpretable-end-to-end-differentiability-for-au.html">MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation</a></td>
  <td>æå‡ºMIND-Stackä»¥è§£å†³è‡ªä¸»å¯¼èˆªä¸­çš„å¯è§£é‡Šæ€§ä¸æ¨¡å—åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21734v1" data-paper-url="./papers/250521734v1-mind-stack-modular-interpretable-end-to-end-differentiability-for-au.html" onclick="toggleFavorite(this, '2505.21734v1', 'MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250521432v4-hume-introducing-system-2-thinking-in-visual-language-action-model.html">Hume: Introducing System-2 Thinking in Visual-Language-Action Model</a></td>
  <td>æå‡ºHumeæ¨¡å‹ä»¥å®ç°æœºå™¨äººå¤æ‚ä»»åŠ¡çš„ç³»ç»Ÿæ€§æ€è€ƒ</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21432v4" data-paper-url="./papers/250521432v4-hume-introducing-system-2-thinking-in-visual-language-action-model.html" onclick="toggleFavorite(this, '2505.21432v4', 'Hume: Introducing System-2 Thinking in Visual-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250520783v1-fm-planner-foundation-model-guided-path-planning-for-autonomous-dron.html">FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation</a></td>
  <td>æå‡ºFM-Plannerä»¥è§£å†³æ— äººæœºè·¯å¾„è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20783v1" data-paper-url="./papers/250520783v1-fm-planner-foundation-model-guided-path-planning-for-autonomous-dron.html" onclick="toggleFavorite(this, '2505.20783v1', 'FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250520828v2-get-goal-directed-exploration-and-targeting-for-large-scale-unknown-.html">GET: Goal-directed Exploration and Targeting for Large-Scale Unknown Environments</a></td>
  <td>æå‡ºGETæ¡†æ¶ä»¥è§£å†³å¤§è§„æ¨¡æœªçŸ¥ç¯å¢ƒä¸­çš„ç›®æ ‡æœç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20828v2" data-paper-url="./papers/250520828v2-get-goal-directed-exploration-and-targeting-for-large-scale-unknown-.html" onclick="toggleFavorite(this, '2505.20828v2', 'GET: Goal-directed Exploration and Targeting for Large-Scale Unknown Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250606487v1-beliefmapnav-3d-voxel-based-belief-map-for-zero-shot-object-navigati.html">BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation</a></td>
  <td>æå‡ºBeliefMapNavä»¥è§£å†³é›¶-shotç‰©ä½“å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.06487v1" data-paper-url="./papers/250606487v1-beliefmapnav-3d-voxel-based-belief-map-for-zero-shot-object-navigati.html" onclick="toggleFavorite(this, '2506.06487v1', 'BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250520711v2-automating-ehmi-action-design-with-llms-for-automated-vehicle-commun.html">Automating eHMI Action Design with LLMs for Automated Vehicle Communication</a></td>
  <td>åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨è®¾è®¡eHMIåŠ¨ä½œä»¥æå‡è‡ªåŠ¨é©¾é©¶è½¦è¾†æ²Ÿé€šèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20711v2" data-paper-url="./papers/250520711v2-automating-ehmi-action-design-with-llms-for-automated-vehicle-commun.html" onclick="toggleFavorite(this, '2505.20711v2', 'Automating eHMI Action Design with LLMs for Automated Vehicle Communication')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250521282v1-egowalk-a-multimodal-dataset-for-robot-navigation-in-the-wild.html">EgoWalk: A Multimodal Dataset for Robot Navigation in the Wild</a></td>
  <td>æå‡ºEgoWalkæ•°æ®é›†ä»¥æå‡æœºå™¨äººå¯¼èˆªèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">traversability</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.21282v1" data-paper-url="./papers/250521282v1-egowalk-a-multimodal-dataset-for-robot-navigation-in-the-wild.html" onclick="toggleFavorite(this, '2505.21282v1', 'EgoWalk: A Multimodal Dataset for Robot Navigation in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250520962v1-object-centric-action-enhanced-representations-for-robot-visuo-motor.html">Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning</a></td>
  <td>æå‡ºå¯¹è±¡ä¸­å¿ƒçš„åŠ¨ä½œå¢å¼ºè¡¨ç¤ºä»¥æ”¹å–„æœºå™¨äººè§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ </td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20962v1" data-paper-url="./papers/250520962v1-object-centric-action-enhanced-representations-for-robot-visuo-motor.html" onclick="toggleFavorite(this, '2505.20962v1', 'Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>22</td>
  <td><a href="./papers/250520857v1-g-dream-graph-conditioned-diffusion-retargeting-across-multiple-embo.html">G-DReaM: Graph-conditioned Diffusion Retargeting across Multiple Embodiments</a></td>
  <td>æå‡ºå›¾æ¡ä»¶æ‰©æ•£é‡å®šå‘æ–¹æ³•ä»¥è§£å†³å¤šç§æœºå™¨äººè¿åŠ¨é‡å®šå‘é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span> <span class="paper-tag">motion retargeting</span> <span class="paper-tag">cross-embodiment</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20857v1" data-paper-url="./papers/250520857v1-g-dream-graph-conditioned-diffusion-retargeting-across-multiple-embo.html" onclick="toggleFavorite(this, '2505.20857v1', 'G-DReaM: Graph-conditioned Diffusion Retargeting across Multiple Embodiments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)