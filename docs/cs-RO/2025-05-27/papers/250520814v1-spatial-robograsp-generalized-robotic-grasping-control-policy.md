---
layout: default
title: "Spatial RoboGrasp: Generalized Robotic Grasping Control Policy"
---

# Spatial RoboGrasp: Generalized Robotic Grasping Control Policy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20814" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20814v1</a>
  <a href="https://arxiv.org/pdf/2505.20814.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20814v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20814v1', 'Spatial RoboGrasp: Generalized Robotic Grasping Control Policy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yiqi Huang, Travis Davies, Jiahuan Yan, Jiankai Sun, Xiang Chen, Luhui Hu

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç»Ÿä¸€æ¡†æ¶ä»¥è§£å†³æœºå™¨äººæŠ“å–æ§åˆ¶çš„ç©ºé—´æ„ŸçŸ¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæŠ“å–` `å¤šæ¨¡æ€æ„ŸçŸ¥` `æ·±åº¦å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `ç©ºé—´æ„ŸçŸ¥` `ä»»åŠ¡æˆåŠŸç‡` `é¢†åŸŸéšæœºåŒ–` `æ·±åº¦ä¼°è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¯¼è‡´æŠ“å–æ§åˆ¶åœ¨ä¸åŒç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶ç»“åˆäº†å¤šæ¨¡æ€æ„ŸçŸ¥å’ŒæŠ“å–é¢„æµ‹ï¼Œåˆ©ç”¨æ·±åº¦ä¿¡æ¯æå‡æŠ“å–ç²¾åº¦ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç¯å¢ƒå˜åŒ–ä¸‹ï¼ŒæŠ“å–æˆåŠŸç‡æé«˜äº†40%ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†45%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®ç°è·¨å¤šæ ·ç¯å¢ƒçš„é€šç”¨ä¸”ç²¾ç¡®çš„æœºå™¨äººæ“ä½œä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç©ºé—´æ„ŸçŸ¥çš„å±€é™æ€§ã€‚è™½ç„¶ä¹‹å‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†å…¶å¯¹åŸå§‹RGBè¾“å…¥å’Œæ‰‹å·¥ç‰¹å¾çš„ä¾èµ–å¸¸å¸¸å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå¹¶åœ¨ä¸åŒå…‰ç…§ã€é®æŒ¡å’Œç‰©ä½“æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†ç¨³å¥çš„å¤šæ¨¡æ€æ„ŸçŸ¥ä¸å¯é çš„æŠ“å–é¢„æµ‹ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ¶æ„èåˆäº†é¢†åŸŸéšæœºåŒ–å¢å¼ºã€å•ç›®æ·±åº¦ä¼°è®¡å’Œæ·±åº¦æ„ŸçŸ¥çš„6è‡ªç”±åº¦æŠ“å–æç¤ºï¼Œå½¢æˆä¸€ä¸ªå•ä¸€çš„ç©ºé—´è¡¨ç¤ºç”¨äºåç»­çš„åŠ¨ä½œè§„åˆ’ã€‚åœ¨æ­¤ç¼–ç å’Œé«˜å±‚ä»»åŠ¡æç¤ºçš„æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬çš„åŸºäºæ‰©æ•£çš„ç­–ç•¥ç”Ÿæˆç²¾ç¡®çš„åŠ¨ä½œåºåˆ—ï¼Œåœ¨ç¯å¢ƒå˜åŒ–ä¸‹å®ç°äº†æŠ“å–æˆåŠŸç‡æé«˜40%å’Œä»»åŠ¡æˆåŠŸç‡æé«˜45%çš„æ•ˆæœã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç©ºé—´æ„ŸçŸ¥ä¸åŸºäºæ‰©æ•£çš„æ¨¡ä»¿å­¦ä¹ ç›¸ç»“åˆï¼Œä¸ºé€šç”¨æœºå™¨äººæŠ“å–æä¾›äº†å¯æ‰©å±•ä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæŠ“å–æ§åˆ¶ä¸­çš„ç©ºé—´æ„ŸçŸ¥ä¸è¶³é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸åŒç¯å¢ƒæ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤šæ¨¡æ€æ„ŸçŸ¥å’Œæ·±åº¦ä¿¡æ¯ï¼Œæå‡æŠ“å–çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥è®¾è®¡æ—¨åœ¨å…‹æœä¼ ç»Ÿæ–¹æ³•å¯¹RGBè¾“å…¥çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬é¢†åŸŸéšæœºåŒ–å¢å¼ºã€å•ç›®æ·±åº¦ä¼°è®¡å’Œæ·±åº¦æ„ŸçŸ¥çš„6è‡ªç”±åº¦æŠ“å–æç¤ºï¼Œå½¢æˆä¸€ä¸ªç»¼åˆçš„ç©ºé—´è¡¨ç¤ºï¼Œä¾›åç»­åŠ¨ä½œè§„åˆ’ä½¿ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¤šæ¨¡æ€æ„ŸçŸ¥ä¸åŸºäºæ‰©æ•£çš„æ¨¡ä»¿å­¦ä¹ ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„æŠ“å–æ§åˆ¶ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æŠ“å–æˆåŠŸç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œæ·±åº¦ä¼°è®¡ï¼Œå¹¶è®¾è®¡äº†é€‚åº”æ€§æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–æŠ“å–é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æŠ“å–æˆåŠŸç‡ä¸Šæé«˜äº†40%ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†45%ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†å¤šæ¨¡æ€æ„ŸçŸ¥ä¸åŸºäºæ‰©æ•£çš„å­¦ä¹ ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒåŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æŠ“å–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå¯é æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving generalizable and precise robotic manipulation across diverse environments remains a critical challenge, largely due to limitations in spatial perception. While prior imitation-learning approaches have made progress, their reliance on raw RGB inputs and handcrafted features often leads to overfitting and poor 3D reasoning under varied lighting, occlusion, and object conditions. In this paper, we propose a unified framework that couples robust multimodal perception with reliable grasp prediction. Our architecture fuses domain-randomized augmentation, monocular depth estimation, and a depth-aware 6-DoF Grasp Prompt into a single spatial representation for downstream action planning. Conditioned on this encoding and a high-level task prompt, our diffusion-based policy yields precise action sequences, achieving up to 40% improvement in grasp success and 45% higher task success rates under environmental variation. These results demonstrate that spatially grounded perception, paired with diffusion-based imitation learning, offers a scalable and robust solution for general-purpose robotic grasping.

