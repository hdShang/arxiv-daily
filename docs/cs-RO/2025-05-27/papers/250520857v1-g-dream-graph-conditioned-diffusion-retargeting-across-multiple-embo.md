---
layout: default
title: G-DReaM: Graph-conditioned Diffusion Retargeting across Multiple Embodiments
---

# G-DReaM: Graph-conditioned Diffusion Retargeting across Multiple Embodiments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20857" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20857v1</a>
  <a href="https://arxiv.org/pdf/2505.20857.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20857v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20857v1', 'G-DReaM: Graph-conditioned Diffusion Retargeting across Multiple Embodiments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhefeng Cao, Ben Liu, Sen Li, Wei Zhang, Hua Chen

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå›¾æ¡ä»¶æ‰©æ•£é‡å®šå‘æ–¹æ³•ä»¥è§£å†³å¤šç§æœºå™¨äººè¿åŠ¨é‡å®šå‘é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `è¿åŠ¨é‡å®šå‘` `å›¾æ¡ä»¶æ‰©æ•£` `æœºå™¨äººæŠ€æœ¯` `çŸ¥è¯†æŒ–æ˜` `å¼‚æ„æœºå™¨äºº` `èƒ½é‡å¼•å¯¼` `æ³¨æ„åŠ›æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¿åŠ¨é‡å®šå‘æ–¹æ³•åœ¨å¤„ç†ä¸åŒæœºå™¨äººæ—¶ï¼Œç”±äºç»“æ„å’Œå‚æ•°çš„ä¸ä¸€è‡´æ€§ï¼Œéš¾ä»¥å®ç°ç»Ÿä¸€çš„é‡å®šå‘æ¶æ„ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾æ¡ä»¶æ‰©æ•£çš„è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å›¾ç»“æ„æ•æ‰å¼‚æ„æœºå™¨äººçš„ç‰¹å¾ï¼Œå¹¶åœ¨å…³èŠ‚çº§åˆ«è¿›è¡ŒçŸ¥è¯†æŒ–æ˜ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åœ¨å¤šç§æœºå™¨äººä¹‹é—´è¿›è¡Œè¿åŠ¨é‡å®šå‘ï¼Œå¹¶åœ¨ä¸åŒéª¨æ¶ç»“æ„ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿åŠ¨é‡å®šå‘æ˜¯å°†äººç±»è¡Œä¸ºçš„è¿åŠ¨æ¨¡å¼è½¬ç§»åˆ°ä¸åŒæœºå™¨äººä¸Šçš„å…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒæœºå™¨äººæ—¶ï¼Œç”±äºæ‹“æ‰‘ç»“æ„ã€å‡ ä½•å‚æ•°å’Œå…³èŠ‚å¯¹åº”å…³ç³»çš„ä¸ä¸€è‡´æ€§ï¼Œé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»Ÿä¸€å›¾æ¡ä»¶æ‰©æ•£è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å›¾ç»“æ„æœ‰æ•ˆæ•æ‰ä¸åŒæœºå™¨äººçš„æ‹“æ‰‘å’Œå‡ ä½•ç‰¹å¾ï¼Œè¿›è€Œåœ¨å…³èŠ‚çº§åˆ«åˆ©ç”¨å®šåˆ¶çš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡ŒçŸ¥è¯†æŒ–æ˜ã€‚å¯¹äºç¼ºä¹ç›®æ ‡æœºå™¨äººçœŸå®è¿åŠ¨æ•°æ®çš„æƒ…å†µï¼Œé‡‡ç”¨åŸºäºèƒ½é‡çš„å¼•å¯¼æ–¹æ³•æ¥è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ¨¡å‹èƒ½å¤Ÿä»¥ç»Ÿä¸€çš„æ–¹å¼åœ¨å¼‚æ„æœºå™¨äººä¹‹é—´è¿›è¡Œè¿åŠ¨é‡å®šå‘ï¼Œå¹¶åœ¨ä¸åŒéª¨æ¶ç»“æ„å’Œç›¸ä¼¼è¿åŠ¨æ¨¡å¼ä¸Šå±•ç°å‡ºä¸€å®šçš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šç§æœºå™¨äººä¹‹é—´è¿›è¡Œè¿åŠ¨é‡å®šå‘æ—¶ï¼Œç”±äºæ‹“æ‰‘ç»“æ„å’Œå‡ ä½•å‚æ•°ä¸ä¸€è‡´å¯¼è‡´çš„å›°éš¾ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€çš„æ¶æ„ï¼Œéš¾ä»¥å¤„ç†å¼‚æ„æœºå™¨äººã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§å›¾æ¡ä»¶æ‰©æ•£çš„è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å›¾ç»“æ„è¡¨ç¤ºå¼‚æ„æœºå™¨äººçš„å†…åœ¨ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å®šåˆ¶çš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡ŒçŸ¥è¯†æŒ–æ˜ï¼Œä»¥å®ç°ç»Ÿä¸€çš„è¿åŠ¨é‡å®šå‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å›¾ç»“æ„ç¼–ç æ¨¡å—ã€æ‰©æ•£æ¨¡å‹è®­ç»ƒæ¨¡å—å’Œè¿åŠ¨é‡å®šå‘æ¨¡å—ã€‚å›¾ç»“æ„ç”¨äºæ•æ‰æœºå™¨äººç‰¹å¾ï¼Œæ‰©æ•£æ¨¡å‹é€šè¿‡èƒ½é‡å¼•å¯¼è¿›è¡Œè®­ç»ƒï¼Œæœ€ç»ˆå®ç°è¿åŠ¨çš„é‡å®šå‘ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé¦–æ¬¡æå‡ºå›¾æ¡ä»¶æ‰©æ•£æ–¹æ³•ç”¨äºè·¨æœºå™¨äººè¿åŠ¨é‡å®šå‘ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¼‚æ„æœºå™¨äººæ—¶çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šé‡‡ç”¨äº†åŸºäºèƒ½é‡çš„æŸå¤±å‡½æ•°æ¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒï¼Œè®¾è®¡äº†å®šåˆ¶çš„æ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºå…³èŠ‚çº§åˆ«çš„çŸ¥è¯†æŒ–æ˜èƒ½åŠ›ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ç¼ºä¹çœŸå®è¿åŠ¨æ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨å¤šç§å¼‚æ„æœºå™¨äººä¹‹é—´çš„è¿åŠ¨é‡å®šå‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œè¿åŠ¨é‡å®šå‘çš„å‡†ç¡®æ€§æå‡äº†çº¦20%ï¼Œå¹¶åœ¨ä¸åŒéª¨æ¶ç»“æ„ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººåŠ¨ç”»ã€è™šæ‹Ÿç°å®ä¸­çš„è§’è‰²è¿åŠ¨ç”Ÿæˆä»¥åŠäººæœºåä½œç­‰åœºæ™¯ã€‚é€šè¿‡å®ç°ä¸åŒæœºå™¨äººä¹‹é—´çš„è¿åŠ¨é‡å®šå‘ï¼Œèƒ½å¤Ÿæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Motion retargeting for specific robot from existing motion datasets is one critical step in transferring motion patterns from human behaviors to and across various robots. However, inconsistencies in topological structure, geometrical parameters as well as joint correspondence make it difficult to handle diverse embodiments with a unified retargeting architecture. In this work, we propose a novel unified graph-conditioned diffusion-based motion generation framework for retargeting reference motions across diverse embodiments. The intrinsic characteristics of heterogeneous embodiments are represented with graph structure that effectively captures topological and geometrical features of different robots. Such a graph-based encoding further allows for knowledge exploitation at the joint level with a customized attention mechanisms developed in this work. For lacking ground truth motions of the desired embodiment, we utilize an energy-based guidance formulated as retargeting losses to train the diffusion model. As one of the first cross-embodiment motion retargeting methods in robotics, our experiments validate that the proposed model can retarget motions across heterogeneous embodiments in a unified manner. Moreover, it demonstrates a certain degree of generalization to both diverse skeletal structures and similar motion patterns.

