---
layout: default
title: "CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception"
---

# CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21495" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21495v1</a>
  <a href="https://arxiv.org/pdf/2505.21495.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21495v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21495v1', 'CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Pranav N. Thakkar, Shubhangi Sinha, Karan Baijal, Yuhan, Bian, Leah Lackey, Ben Dodson, Heisen Kong, Jueun Kwon, Amber Li, Yifei Hu, Alexios Rekoutis, Tom Silver, Tapomayukh Bhattacharjee

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCLAMPè®¾å¤‡ä»¥è§£å†³å¤§è§„æ¨¡è§¦è§‰æ•°æ®é›†ç¼ºä¹é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è§¦è§‰` `æœºå™¨äººæ“ä½œ` `æ•°æ®é›†æ„å»º` `ææ–™è¯†åˆ«` `è§¦è§‰ä¼ æ„Ÿå™¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººæ“ä½œæ–¹æ³•åœ¨ç†è§£ç‰©ä½“çš„ææ–™å’Œé¡ºåº”æ€§å±æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸»è¦ä¾èµ–è§†è§‰ä¿¡æ¯ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„ç°å®åœºæ™¯ã€‚
2. æœ¬æ–‡æå‡ºäº†CLAMPè®¾å¤‡ï¼Œé€šè¿‡ä½æˆæœ¬çš„ä¼ æ„Ÿå™¨åŒ–æŠ“å–å™¨æ”¶é›†å¤šæ¨¡æ€è§¦è§‰æ•°æ®ï¼Œå½¢æˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¼€æºæ•°æ®é›†ï¼Œä»¥æ”¯æŒæœºå™¨äººå¯¹ç‰©ä½“å±æ€§çš„ç†è§£ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CLAMPæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨ææ–™è¯†åˆ«å’Œå®é™…æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¨å¹¿åˆ°æ–°ç‰©ä½“å’Œä¸åŒçš„æœºå™¨äººå½¢æ€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œæœºå™¨äººæ“ä½œçš„ç¨³å¥æ€§é€šå¸¸éœ€è¦ç†è§£è¶…è¶Šå‡ ä½•çš„ç‰©ä½“å±æ€§ï¼Œå¦‚ææ–™æˆ–é¡ºåº”æ€§ï¼Œè¿™äº›å±æ€§ä»…é€šè¿‡è§†è§‰éš¾ä»¥æ¨æ–­ã€‚å¤šæ¨¡æ€è§¦è§‰ä¼ æ„Ÿæä¾›äº†ä¸€æ¡æœ‰å‰æ™¯çš„é€”å¾„ï¼Œä½†ç”±äºç¼ºä¹å¤§å‹ã€å¤šæ ·åŒ–å’ŒçœŸå®çš„è§¦è§‰æ•°æ®é›†ï¼Œè¿›å±•å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†CLAMPè®¾å¤‡ï¼Œè¿™æ˜¯ä¸€ç§ä½æˆæœ¬ï¼ˆ<200ç¾å…ƒï¼‰çš„ä¼ æ„Ÿå™¨åŒ–æŠ“å–å™¨ï¼Œæ—¨åœ¨ä»éä¸“ä¸šç”¨æˆ·çš„æ—¥å¸¸ç¯å¢ƒä¸­æ”¶é›†å¤§è§„æ¨¡çš„å¤šæ¨¡æ€è§¦è§‰æ•°æ®ã€‚æˆ‘ä»¬éƒ¨ç½²äº†16ä¸ªCLAMPè®¾å¤‡ï¼Œå‚ä¸è€…è¾¾41äººï¼Œæœ€ç»ˆå½¢æˆäº†CLAMPæ•°æ®é›†ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¼€æºå¤šæ¨¡æ€è§¦è§‰æ•°æ®é›†ï¼ŒåŒ…å«1230ä¸‡ä¸ªæ•°æ®ç‚¹å’Œ5357ä¸ªå®¶åº­ç‰©ä½“ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè§¦è§‰ç¼–ç å™¨ï¼Œå¯ä»¥ä»å¤šæ¨¡æ€è§¦è§‰æ•°æ®ä¸­æ¨æ–­ææ–™å’Œé¡ºåº”æ€§å±æ€§ï¼Œå¹¶åˆ›å»ºäº†CLAMPæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºææ–™è¯†åˆ«çš„è§†è§‰-è§¦è§‰æ„ŸçŸ¥æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœ€å°å¾®è°ƒçš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æ–°ç‰©ä½“å’Œä¸‰ç§æœºå™¨äººå½¢æ€ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨ä¸‰é¡¹å®é™…æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼šåˆ†ç±»å¯å›æ”¶å’Œä¸å¯å›æ”¶çš„åºŸç‰©ã€ä»æ‚ä¹±çš„åŒ…ä¸­æ£€ç´¢ç‰©ä½“ï¼Œä»¥åŠåŒºåˆ†è¿‡ç†Ÿå’Œæˆç†Ÿçš„é¦™è•‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§è§„æ¨¡çš„è§¦è§‰æ•°æ®æ”¶é›†å¯ä»¥è§£é”æœºå™¨äººæ“ä½œçš„æ–°èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­å¯¹ç‰©ä½“ææ–™å’Œé¡ºåº”æ€§å±æ€§ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è§†è§‰ä¿¡æ¯ï¼Œéš¾ä»¥å‡†ç¡®æ¨æ–­ç‰©ä½“çš„è§¦è§‰ç‰¹æ€§ï¼Œé™åˆ¶äº†æœºå™¨äººæ“ä½œçš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬çš„CLAMPè®¾å¤‡ï¼Œæ—¨åœ¨é€šè¿‡æ”¶é›†å¤šæ¨¡æ€è§¦è§‰æ•°æ®æ¥å¢å¼ºæœºå™¨äººå¯¹ç‰©ä½“å±æ€§çš„ç†è§£ã€‚è¯¥è®¾å¤‡çš„è®¾è®¡è€ƒè™‘äº†éä¸“ä¸šç”¨æˆ·çš„ä½¿ç”¨åœºæ™¯ï¼Œä½¿å¾—å¤§è§„æ¨¡æ•°æ®æ”¶é›†æˆä¸ºå¯èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬CLAMPè®¾å¤‡çš„è®¾è®¡ã€æ•°æ®æ”¶é›†ã€è§¦è§‰ç¼–ç å™¨çš„è®­ç»ƒå’ŒCLAMPæ¨¡å‹çš„æ„å»ºã€‚æ•°æ®æ”¶é›†é˜¶æ®µæ¶‰åŠ16ä¸ªè®¾å¤‡å’Œ41åå‚ä¸è€…ï¼Œè§¦è§‰ç¼–ç å™¨ç”¨äºä»æ•°æ®ä¸­æå–ç‰©ä½“å±æ€§ï¼Œæœ€ç»ˆå½¢æˆçš„CLAMPæ¨¡å‹ç”¨äºå®é™…æ“ä½œä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šCLAMPè®¾å¤‡çš„è®¾è®¡å’Œå¤§è§„æ¨¡æ•°æ®é›†çš„æ„å»ºæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCLAMPèƒ½å¤Ÿæœ‰æ•ˆæ”¶é›†å¤šæ ·åŒ–çš„è§¦è§‰æ•°æ®ï¼Œæ”¯æŒæ›´å¹¿æ³›çš„æœºå™¨äººæ“ä½œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–è§¦è§‰ç‰¹å¾çš„æå–ï¼Œç½‘ç»œç»“æ„è®¾è®¡ä¸Šè€ƒè™‘äº†å¤šæ¨¡æ€è¾“å…¥çš„èåˆï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨æ–°ç‰©ä½“å’Œä¸åŒæœºå™¨äººå½¢æ€ä¸Šçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLAMPæ¨¡å‹åœ¨ææ–™è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨æœ€å°å¾®è°ƒçš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æ–°ç‰©ä½“å’Œä¸‰ç§ä¸åŒçš„æœºå™¨äººå½¢æ€ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨åˆ†ç±»å¯å›æ”¶ä¸ä¸å¯å›æ”¶åºŸç‰©ã€ä»æ‚ä¹±åŒ…ä¸­æ£€ç´¢ç‰©ä½“ä»¥åŠåŒºåˆ†è¿‡ç†Ÿä¸æˆç†Ÿé¦™è•‰çš„ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€æœåŠ¡æœºå™¨äººå’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚é€šè¿‡å¢å¼ºæœºå™¨äººå¯¹ç‰©ä½“å±æ€§çš„ç†è§£ï¼ŒCLAMPæ¨¡å‹å¯ä»¥æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robust robot manipulation in unstructured environments often requires understanding object properties that extend beyond geometry, such as material or compliance-properties that can be challenging to infer using vision alone. Multimodal haptic sensing provides a promising avenue for inferring such properties, yet progress has been constrained by the lack of large, diverse, and realistic haptic datasets. In this work, we introduce the CLAMP device, a low-cost (<\$200) sensorized reacher-grabber designed to collect large-scale, in-the-wild multimodal haptic data from non-expert users in everyday settings. We deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP dataset, the largest open-source multimodal haptic dataset to date, comprising 12.3 million datapoints across 5357 household objects. Using this dataset, we train a haptic encoder that can infer material and compliance object properties from multimodal haptic data. We leverage this encoder to create the CLAMP model, a visuo-haptic perception model for material recognition that generalizes to novel objects and three robot embodiments with minimal finetuning. We also demonstrate the effectiveness of our model in three real-world robot manipulation tasks: sorting recyclable and non-recyclable waste, retrieving objects from a cluttered bag, and distinguishing overripe from ripe bananas. Our results show that large-scale, in-the-wild haptic data collection can unlock new capabilities for generalizable robot manipulation. Website: https://emprise.cs.cornell.edu/clamp/

