---
layout: default
title: LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation
---

# LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.02239" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.02239v1</a>
  <a href="https://arxiv.org/pdf/2511.02239.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.02239v1" onclick="toggleFavorite(this, '2511.02239v1', 'LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Youngjin Hong, Houjian Yu, Mingen Li, Changhyun Choi

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-04

**å¤‡æ³¨**: Preprint. Project page: https://vla2026.github.io/LACY/

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://vla2026.github.io/LACY/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**LACYï¼šåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„è¯­è¨€-åŠ¨ä½œå¾ªç¯ï¼Œç”¨äºè‡ªæå‡çš„æœºå™¨äººæ“ä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `è¯­è¨€-åŠ¨ä½œå¾ªç¯` `è‡ªç›‘ç£å­¦ä¹ ` `ä¸»åŠ¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ“ä½œç­–ç•¥ç¼ºä¹ä¸Šä¸‹æ–‡ç†è§£ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ï¼Œä¸”æ— æ³•è§£é‡Šè‡ªèº«è¡Œä¸ºã€‚
2. LACYæ¡†æ¶é€šè¿‡åœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­å­¦ä¹ åŒå‘æ˜ å°„ï¼ˆL2Aå’ŒA2Lï¼‰æ¥è§£å†³æ­¤é—®é¢˜ã€‚
3. LACYé€šè¿‡è‡ªç›‘ç£å¾ªç¯ç”Ÿæˆå’Œè¿‡æ»¤è®­ç»ƒæ•°æ®ï¼Œæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨å³å¯æå‡æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å­¦ä¹ æœºå™¨äººæ“ä½œçš„é€šç”¨ç­–ç•¥ï¼Œè¶Šæ¥è¶Šå¤šåœ°ä¾èµ–äºå°†è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°åŠ¨ä½œ(L2A)çš„å¤§è§„æ¨¡æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™ç§å•å‘èŒƒå¼äº§ç”Ÿçš„ç­–ç•¥é€šå¸¸åœ¨æ²¡æœ‰æ›´æ·±å±‚æ¬¡çš„ä¸Šä¸‹æ–‡ç†è§£çš„æƒ…å†µä¸‹æ‰§è¡Œä»»åŠ¡ï¼Œé™åˆ¶äº†å®ƒä»¬çš„æ³›åŒ–æˆ–è§£é‡Šå…¶è¡Œä¸ºçš„èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå°†åŠ¨ä½œæ˜ å°„å›è¯­è¨€(A2L)çš„äº’è¡¥æŠ€èƒ½å¯¹äºå¼€å‘æ›´å…¨é¢çš„åŸºç¡€è‡³å…³é‡è¦ã€‚ä¸€ä¸ªæ—¢èƒ½è¡ŒåŠ¨åˆèƒ½è§£é‡Šå…¶è¡ŒåŠ¨çš„æ™ºèƒ½ä½“å¯ä»¥å½¢æˆæ›´ä¸°å¯Œçš„å†…éƒ¨è¡¨å¾ï¼Œå¹¶ä¸ºè‡ªç›‘ç£å­¦ä¹ è§£é”æ–°çš„èŒƒå¼ã€‚æˆ‘ä»¬å¼•å…¥äº†LACY(è¯­è¨€-åŠ¨ä½œå¾ªç¯)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥åœ¨å•ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ä¸­å­¦ä¹ è¿™ç§åŒå‘æ˜ å°„ã€‚LACYåœ¨ä¸‰ä¸ªååŒä»»åŠ¡ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼šä»è¯­è¨€ç”Ÿæˆå‚æ•°åŒ–åŠ¨ä½œ(L2A)ï¼Œç”¨è¯­è¨€è§£é‡Šè§‚å¯Ÿåˆ°çš„åŠ¨ä½œ(A2L)ï¼Œä»¥åŠéªŒè¯ä¸¤ä¸ªè¯­è¨€æè¿°ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§(L2C)ã€‚è¿™ä½¿å¾—ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„å¾ªç¯èƒ½å¤Ÿé€šè¿‡ä¸»åŠ¨å¢å¼ºç­–ç•¥è‡ªä¸»ç”Ÿæˆå’Œè¿‡æ»¤æ–°çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œåœ¨æ²¡æœ‰é¢å¤–äººå·¥æ ‡ç­¾çš„æƒ…å†µä¸‹æ”¹è¿›æ¨¡å‹ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä¸­çš„æŠ“å–æ”¾ç½®ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLACYå¹³å‡æé«˜äº†56.46%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶ä¸ºæœºå™¨äººæ“ä½œäº§ç”Ÿäº†æ›´å¼ºå¤§çš„è¯­è¨€-åŠ¨ä½œåŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè¯­è¨€åˆ°åŠ¨ä½œ(L2A)çš„æœºå™¨äººæ“ä½œæ–¹æ³•ï¼Œé€šå¸¸ç¼ºä¹å¯¹ä»»åŠ¡ä¸Šä¸‹æ–‡çš„æ·±å…¥ç†è§£ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¹¶ä¸”éš¾ä»¥è§£é‡Šå…¶è¡Œä¸ºã€‚è¿™ç§å•å‘æ˜ å°„å¿½ç•¥äº†åŠ¨ä½œåˆ°è¯­è¨€(A2L)çš„åé¦ˆï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“å½¢æˆæ›´ä¸°å¯Œçš„å†…éƒ¨è¡¨å¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLACYçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶å­¦ä¹ è¯­è¨€åˆ°åŠ¨ä½œ(L2A)å’ŒåŠ¨ä½œåˆ°è¯­è¨€(A2L)çš„åŒå‘æ˜ å°„ã€‚é€šè¿‡è¿™ç§åŒå‘å¾ªç¯ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°ç†è§£ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼Œå¹¶æé«˜æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒLACYè¿˜å¼•å…¥äº†è¯­è¨€ä¸€è‡´æ€§éªŒè¯(L2C)ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLACYçš„æŠ€æœ¯æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¯­è¨€åˆ°åŠ¨ä½œ(L2A)æ¨¡å—ã€åŠ¨ä½œåˆ°è¯­è¨€(A2L)æ¨¡å—å’Œè¯­è¨€ä¸€è‡´æ€§éªŒè¯(L2C)æ¨¡å—ã€‚L2Aæ¨¡å—è´Ÿè´£æ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆå‚æ•°åŒ–çš„åŠ¨ä½œåºåˆ—ã€‚A2Læ¨¡å—è´Ÿè´£æ ¹æ®è§‚å¯Ÿåˆ°çš„åŠ¨ä½œåºåˆ—ç”Ÿæˆè¯­è¨€æè¿°ã€‚L2Cæ¨¡å—è´Ÿè´£éªŒè¯ä¸¤ä¸ªè¯­è¨€æè¿°ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¿™ä¸‰ä¸ªæ¨¡å—åœ¨ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œè”åˆè®­ç»ƒï¼Œå½¢æˆä¸€ä¸ªè¯­è¨€-åŠ¨ä½œå¾ªç¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šLACYæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶åŒå‘è¯­è¨€-åŠ¨ä½œå¾ªç¯çš„å­¦ä¹ èŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„å•å‘L2Aæ–¹æ³•ç›¸æ¯”ï¼ŒLACYé€šè¿‡å¼•å…¥A2Lå’ŒL2Cä»»åŠ¡ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLACYè¿˜æå‡ºäº†ä¸€ç§ä¸»åŠ¨å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡è‡ªä¸»ç”Ÿæˆå’Œè¿‡æ»¤æ–°çš„è®­ç»ƒæ•°æ®ï¼Œå®ç°äº†æ¨¡å‹çš„è‡ªæå‡ï¼Œæ— éœ€é¢å¤–çš„äººå·¥æ ‡æ³¨ã€‚

**å…³é”®è®¾è®¡**ï¼šLACYçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformeræ¶æ„ä½œä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„åŸºç¡€ï¼Œä»¥å®ç°æ›´å¥½çš„å¤šæ¨¡æ€èåˆï¼›2) è®¾è®¡äº†å‚æ•°åŒ–çš„åŠ¨ä½œè¡¨ç¤ºï¼Œä»¥ä¾¿L2Aæ¨¡å—ç”Ÿæˆå¯æ‰§è¡Œçš„åŠ¨ä½œåºåˆ—ï¼›3) é‡‡ç”¨äº†å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°æ¥è®­ç»ƒA2Læ¨¡å—ï¼Œä»¥æé«˜è¯­è¨€æè¿°çš„å‡†ç¡®æ€§ï¼›4) ä½¿ç”¨äº†åŸºäºç½®ä¿¡åº¦çš„è¿‡æ»¤ç­–ç•¥æ¥é€‰æ‹©é«˜è´¨é‡çš„è‡ªç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LACYåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æŠ“å–æ”¾ç½®ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼ŒLACYçš„ä»»åŠ¡æˆåŠŸç‡å¹³å‡æé«˜äº†56.46%ã€‚åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ï¼ŒLACYä¹Ÿè¡¨ç°å‡ºä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒLACYæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æœºå™¨äººæ“ä½œçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LACYæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººå’ŒåŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼ŒLACYå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒï¼Œå¹¶ä¸äººç±»è¿›è¡Œæ›´è‡ªç„¶çš„äº¤äº’ã€‚æ­¤å¤–ï¼ŒLACYçš„è‡ªæå‡èƒ½åŠ›å¯ä»¥é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä»è€ŒåŠ é€Ÿæœºå™¨äººæŠ€æœ¯çš„æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning generalizable policies for robotic manipulation increasingly relies on large-scale models that map language instructions to actions (L2A). However, this one-way paradigm often produces policies that execute tasks without deeper contextual understanding, limiting their ability to generalize or explain their behavior. We argue that the complementary skill of mapping actions back to language (A2L) is essential for developing more holistic grounding. An agent capable of both acting and explaining its actions can form richer internal representations and unlock new paradigms for self-supervised learning. We introduce LACY (Language-Action Cycle), a unified framework that learns such bidirectional mappings within a single vision-language model. LACY is jointly trained on three synergistic tasks: generating parameterized actions from language (L2A), explaining observed actions in language (A2L), and verifying semantic consistency between two language descriptions (L2C). This enables a self-improving cycle that autonomously generates and filters new training data through an active augmentation strategy targeting low-confidence cases, thereby improving the model without additional human labels. Experiments on pick-and-place tasks in both simulation and the real world show that LACY improves task success rates by 56.46% on average and yields more robust language-action grounding for robotic manipulation. Project page: https://vla2026.github.io/LACY/

