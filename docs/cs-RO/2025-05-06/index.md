---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-06
---

# cs.ROï¼ˆ2025-05-06ï¼‰

ğŸ“Š å…± **24** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (12 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (12 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250503725v2-meta-optimization-and-program-search-using-language-models-for-task-.html">Meta-Optimization and Program Search using Language Models for Task and Motion Planning</a></td>
  <td>æå‡ºå…ƒä¼˜åŒ–ä¸ç¨‹åºæœç´¢æ–¹æ³•ä»¥è§£å†³ä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">trajectory optimization</span> <span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03725v2" data-paper-url="./papers/250503725v2-meta-optimization-and-program-search-using-language-models-for-task-.html" onclick="toggleFavorite(this, '2505.03725v2', 'Meta-Optimization and Program Search using Language Models for Task and Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250503233v3-graspvla-a-grasping-foundation-model-pre-trained-on-billion-scale-sy.html">GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</a></td>
  <td>æå‡ºGraspVLAä»¥è§£å†³æœºå™¨äººæŠ“å–ä»»åŠ¡ä¸­çš„æ•°æ®ä¾èµ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">domain randomization</span> <span class="paper-tag">flow matching</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03233v3" data-paper-url="./papers/250503233v3-graspvla-a-grasping-foundation-model-pre-trained-on-billion-scale-sy.html" onclick="toggleFavorite(this, '2505.03233v3', 'GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250503738v1-amo-adaptive-motion-optimization-for-hyper-dexterous-humanoid-whole-.html">AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control</a></td>
  <td>æå‡ºè‡ªé€‚åº”è¿åŠ¨ä¼˜åŒ–æ¡†æ¶ä»¥è§£å†³äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">whole-body control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03738v1" data-paper-url="./papers/250503738v1-amo-adaptive-motion-optimization-for-hyper-dexterous-humanoid-whole-.html" onclick="toggleFavorite(this, '2505.03738v1', 'AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250503912v1-openhelix-a-short-survey-empirical-analysis-and-open-source-dual-sys.html">OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation</a></td>
  <td>æå‡ºOpenHelixä»¥è§£å†³åŒç³»ç»ŸVLAæ¨¡å‹å¼€æ”¾æ€§ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03912v1" data-paper-url="./papers/250503912v1-openhelix-a-short-survey-empirical-analysis-and-open-source-dual-sys.html" onclick="toggleFavorite(this, '2505.03912v1', 'OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250503729v5-visual-imitation-enables-contextual-humanoid-control.html">Visual Imitation Enables Contextual Humanoid Control</a></td>
  <td>æå‡ºVIDEOMIMICä»¥è§£å†³äººå½¢æœºå™¨äººç¯å¢ƒé€‚åº”æ€§æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">humanoid control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03729v5" data-paper-url="./papers/250503729v5-visual-imitation-enables-contextual-humanoid-control.html" onclick="toggleFavorite(this, '2505.03729v5', 'Visual Imitation Enables Contextual Humanoid Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250503296v1-the-unreasonable-effectiveness-of-discrete-time-gaussian-process-mix.html">The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning</a></td>
  <td>æå‡ºMiDiGapä»¥è§£å†³æœºå™¨äººç­–ç•¥å­¦ä¹ ä¸­çš„æ ·æœ¬æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03296v1" data-paper-url="./papers/250503296v1-the-unreasonable-effectiveness-of-discrete-time-gaussian-process-mix.html" onclick="toggleFavorite(this, '2505.03296v1', 'The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250503146v1-learn-to-swim-data-driven-lstm-hydrodynamic-model-for-quadruped-robo.html">Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization</a></td>
  <td>æå‡ºFED-LSTMæ¨¡å‹ä»¥ä¼˜åŒ–å››è¶³æœºå™¨äººæ°´ä¸‹æ¸¸æ³³æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03146v1" data-paper-url="./papers/250503146v1-learn-to-swim-data-driven-lstm-hydrodynamic-model-for-quadruped-robo.html" onclick="toggleFavorite(this, '2505.03146v1', 'Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250503931v1-nmpc-lander-nonlinear-mpc-with-barrier-function-for-uav-landing-on-a.html">NMPC-Lander: Nonlinear MPC with Barrier Function for UAV Landing on a Mobile Platform</a></td>
  <td>æå‡ºNMPC-Landerä»¥è§£å†³æ— äººæœºåœ¨ç§»åŠ¨å¹³å°ä¸Šå®‰å…¨ç€é™†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03931v1" data-paper-url="./papers/250503931v1-nmpc-lander-nonlinear-mpc-with-barrier-function-for-uav-landing-on-a.html" onclick="toggleFavorite(this, '2505.03931v1', 'NMPC-Lander: Nonlinear MPC with Barrier Function for UAV Landing on a Mobile Platform')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250503702v3-self-supervised-learning-for-robotic-leaf-manipulation-a-hybrid-geom.html">Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach</a></td>
  <td>æå‡ºæ··åˆå‡ ä½•-ç¥ç»æ–¹æ³•ä»¥è§£å†³å†œä¸šæœºå™¨äººå¶ç‰‡æ“ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03702v3" data-paper-url="./papers/250503702v3-self-supervised-learning-for-robotic-leaf-manipulation-a-hybrid-geom.html" onclick="toggleFavorite(this, '2505.03702v3', 'Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250503537v1-automated-action-generation-based-on-action-field-for-robotic-garmen.html">Automated Action Generation based on Action Field for Robotic Garment Manipulation</a></td>
  <td>æå‡ºä¸€ç§æ–°æ–¹æ³•ä»¥è§£å†³æœºå™¨äººæœè£…æ“ä½œä¸­çš„ç²¾åº¦ä¸æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03537v1" data-paper-url="./papers/250503537v1-automated-action-generation-based-on-action-field-for-robotic-garmen.html" onclick="toggleFavorite(this, '2505.03537v1', 'Automated Action Generation based on Action Field for Robotic Garment Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250503077v2-latent-adaptive-planner-for-dynamic-manipulation.html">Latent Adaptive Planner for Dynamic Manipulation</a></td>
  <td>æå‡ºæ½œåœ¨è‡ªé€‚åº”è§„åˆ’å™¨ä»¥è§£å†³åŠ¨æ€éæŠ“å–æ“ä½œé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03077v2" data-paper-url="./papers/250503077v2-latent-adaptive-planner-for-dynamic-manipulation.html" onclick="toggleFavorite(this, '2505.03077v2', 'Latent Adaptive Planner for Dynamic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250503283v1-enabling-robots-to-autonomously-search-dynamic-cluttered-post-disast.html">Enabling Robots to Autonomously Search Dynamic Cluttered Post-Disaster Environments</a></td>
  <td>æå‡ºé›†æˆæ§åˆ¶æ¡†æ¶ä»¥è§£å†³ç¾ååŠ¨æ€ç¯å¢ƒä¸­çš„è‡ªä¸»æœç´¢é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">motion tracking</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03283v1" data-paper-url="./papers/250503283v1-enabling-robots-to-autonomously-search-dynamic-cluttered-post-disast.html" onclick="toggleFavorite(this, '2505.03283v1', 'Enabling Robots to Autonomously Search Dynamic Cluttered Post-Disaster Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250503238v2-robotxr1-enabling-embodied-robotic-intelligence-on-large-language-mo.html">RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºé—­ç¯å¼ºåŒ–å­¦ä¹ çš„R1-Zeroæ–¹æ³•ä»¥æå‡æœºå™¨äººæ™ºèƒ½</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">distillation</span> <span class="paper-tag">embodied AI</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03238v2" data-paper-url="./papers/250503238v2-robotxr1-enabling-embodied-robotic-intelligence-on-large-language-mo.html" onclick="toggleFavorite(this, '2505.03238v2', 'RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250506278v2-robust-understanding-of-human-robot-social-interactions-through-mult.html">Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation</a></td>
  <td>æå‡ºå¤šæ¨¡æ€è’¸é¦æ¡†æ¶ä»¥å¢å¼ºäººæœºç¤¾äº¤äº’åŠ¨ç†è§£</td>
  <td class="tags-cell"><span class="paper-tag">distillation</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06278v2" data-paper-url="./papers/250506278v2-robust-understanding-of-human-robot-social-interactions-through-mult.html" onclick="toggleFavorite(this, '2505.06278v2', 'Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250503356v1-effective-reinforcement-learning-control-using-conservative-soft-act.html">Effective Reinforcement Learning Control using Conservative Soft Actor-Critic</a></td>
  <td>æå‡ºä¿å®ˆè½¯æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•ä»¥è§£å†³å¼ºåŒ–å­¦ä¹ æ§åˆ¶ä¸­çš„ç¨³å®šæ€§ä¸æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span> <span class="paper-tag">SAC</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03356v1" data-paper-url="./papers/250503356v1-effective-reinforcement-learning-control-using-conservative-soft-act.html" onclick="toggleFavorite(this, '2505.03356v1', 'Effective Reinforcement Learning Control using Conservative Soft Actor-Critic')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250503400v1-close-fitting-dressing-assistance-based-on-state-estimation-of-feet-.html">Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention</a></td>
  <td>æå‡ºåŸºäºçŠ¶æ€ä¼°è®¡çš„ç´§èº«è¡£ç‰©ç©¿æˆ´è¾…åŠ©æ–¹æ³•ä»¥è§£å†³è€é¾„åŒ–æŠ¤ç†çŸ­ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">diffusion policy</span> <span class="paper-tag">spatial relationship</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03400v1" data-paper-url="./papers/250503400v1-close-fitting-dressing-assistance-based-on-state-estimation-of-feet-.html" onclick="toggleFavorite(this, '2505.03400v1', 'Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250503460v1-logisticsvln-vision-language-navigation-for-low-altitude-terminal-de.html">LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal Delivery Based on Agentic UAVs</a></td>
  <td>æå‡ºLogisticsVLNä»¥è§£å†³ä½ç©ºæ— äººæœºç»ˆç«¯é…é€é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VLN</span> <span class="paper-tag">large language model</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03460v1" data-paper-url="./papers/250503460v1-logisticsvln-vision-language-navigation-for-low-altitude-terminal-de.html" onclick="toggleFavorite(this, '2505.03460v1', 'LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal Delivery Based on Agentic UAVs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250503174v1-automated-data-curation-using-gps-nlp-to-generate-instruction-action.html">Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets</a></td>
  <td>åˆ©ç”¨GPSä¸NLPè‡ªåŠ¨ç”Ÿæˆè‡ªä¸»è½¦è¾†æŒ‡ä»¤-åŠ¨ä½œå¯¹</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span> <span class="paper-tag">VLN</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03174v1" data-paper-url="./papers/250503174v1-automated-data-curation-using-gps-nlp-to-generate-instruction-action.html" onclick="toggleFavorite(this, '2505.03174v1', 'Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250503500v4-task-reconstruction-and-extrapolation-for-Ï€-0-using-text-latent.html">Task Reconstruction and Extrapolation for $Ï€_0$ using Text Latent</a></td>
  <td>æå‡ºæ–‡æœ¬æ½œåœ¨ç©ºé—´é‡æ„ä¸å¤–æ¨æ–¹æ³•ä»¥æå‡VLAä»»åŠ¡è¡¨ç°</td>
  <td class="tags-cell"><span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03500v4" data-paper-url="./papers/250503500v4-task-reconstruction-and-extrapolation-for-Ï€-0-using-text-latent.html" onclick="toggleFavorite(this, '2505.03500v4', 'Task Reconstruction and Extrapolation for $Ï€_0$ using Text Latent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>20</td>
  <td><a href="./papers/250506276v1-synshrp2-a-synthetic-multimodal-benchmark-for-driving-safety-critica.html">SynSHRP2: A Synthetic Multimodal Benchmark for Driving Safety-critical Events Derived from Real-world Driving Data</a></td>
  <td>æå‡ºSynSHRP2ä»¥è§£å†³é©¾é©¶å®‰å…¨å…³é”®äº‹ä»¶æ•°æ®ç¨€ç¼ºé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">scene understanding</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06276v1" data-paper-url="./papers/250506276v1-synshrp2-a-synthetic-multimodal-benchmark-for-driving-safety-critica.html" onclick="toggleFavorite(this, '2505.06276v1', 'SynSHRP2: A Synthetic Multimodal Benchmark for Driving Safety-critical Events Derived from Real-world Driving Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>21</td>
  <td><a href="./papers/250503448v2-aquaticvision-benchmarking-visual-slam-in-underwater-environment-wit.html">AquaticVision: Benchmarking Visual SLAM in Underwater Environment with Events and Frames</a></td>
  <td>æå‡ºAquaticVisionä»¥è§£å†³æ°´ä¸‹è§†è§‰SLAMåŸºå‡†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual SLAM</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03448v2" data-paper-url="./papers/250503448v2-aquaticvision-benchmarking-visual-slam-in-underwater-environment-wit.html" onclick="toggleFavorite(this, '2505.03448v2', 'AquaticVision: Benchmarking Visual SLAM in Underwater Environment with Events and Frames')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250503565v1-thermal-lidar-fusion-for-robust-tunnel-localization-in-gnss-denied-a.html">Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and Low-Visibility Conditions</a></td>
  <td>æå‡ºçƒ­æˆåƒä¸æ¿€å…‰é›·è¾¾èåˆä»¥è§£å†³éš§é“å®šä½é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">visual odometry</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03565v1" data-paper-url="./papers/250503565v1-thermal-lidar-fusion-for-robust-tunnel-localization-in-gnss-denied-a.html" onclick="toggleFavorite(this, '2505.03565v1', 'Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and Low-Visibility Conditions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>23</td>
  <td><a href="./papers/250503673v2-roboos-a-hierarchical-embodied-framework-for-cross-embodiment-and-mu.html">RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration</a></td>
  <td>æå‡ºRoboOSä»¥è§£å†³å¤šæ™ºèƒ½ä½“åä½œä¸è·¨ä½“é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">cross-embodiment</span> <span class="paper-tag">spatiotemporal</span> <span class="paper-tag">VLA</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03673v2" data-paper-url="./papers/250503673v2-roboos-a-hierarchical-embodied-framework-for-cross-embodiment-and-mu.html" onclick="toggleFavorite(this, '2505.03673v2', 'RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250503728v1-pyroki-a-modular-toolkit-for-robot-kinematic-optimization.html">PyRoki: A Modular Toolkit for Robot Kinematic Optimization</a></td>
  <td>æå‡ºPyRokiä»¥è§£å†³æœºå™¨äººè¿åŠ¨å­¦ä¼˜åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion retargeting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.03728v1" data-paper-url="./papers/250503728v1-pyroki-a-modular-toolkit-for-robot-kinematic-optimization.html" onclick="toggleFavorite(this, '2505.03728v1', 'PyRoki: A Modular Toolkit for Robot Kinematic Optimization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)