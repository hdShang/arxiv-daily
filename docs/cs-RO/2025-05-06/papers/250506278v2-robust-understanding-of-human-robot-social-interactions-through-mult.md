---
layout: default
title: Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation
---

# Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06278" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06278v2</a>
  <a href="https://arxiv.org/pdf/2505.06278.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06278v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06278v2', 'Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tongfei Bian, Mathieu Chollet, Tanaya Guha

**åˆ†ç±»**: cs.RO, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-10-25)

**å¤‡æ³¨**: Accepted by ACM Multimedia 2025, camera-ready version

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€è’¸é¦æ¡†æ¶ä»¥å¢å¼ºäººæœºç¤¾äº¤äº’åŠ¨ç†è§£**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€è’¸é¦` `äººæœºç¤¾äº¤äº’åŠ¨` `çŸ¥è¯†è’¸é¦` `é²æ£’æ€§` `ç¤¾äº¤ç†è§£` `æœºå™¨äººæŠ€æœ¯` `æ™ºèƒ½ä»£ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„äººæœºäº’åŠ¨å»ºæ¨¡æ–¹æ³•è¾ƒå°‘ï¼Œä¸”åœ¨å®æ—¶åº”ç”¨ä¸­è®¡ç®—æˆæœ¬é«˜ï¼Œé¢å¯¹æœ‰é™ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€çº¿ç´¢å»ºæ¨¡ç¤¾äº¤äº’åŠ¨ï¼Œå¢å¼ºæ¨¡å‹å¯¹ä¸å®Œæ•´ä¿¡æ¯çš„é²æ£’æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªç¤¾äº¤ç†è§£ä»»åŠ¡ä¸Šå¹³å‡æå‡14.75%çš„å‡†ç¡®ç‡ï¼Œä¸”æ¨¡å‹ä½“ç§¯å’Œå»¶è¿Ÿæ˜¾è‘—é™ä½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€ç¤¾äº¤æœºå™¨äººå’Œæ™ºèƒ½ä»£ç†çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°ä¸ç”¨æˆ·äº’åŠ¨æˆä¸ºå…³é”®ã€‚ç°æœ‰çš„äººæœºäº’åŠ¨å»ºæ¨¡æ–¹æ³•è¾ƒå°‘ï¼Œä¸”åœ¨å®æ—¶éƒ¨ç½²æˆ–é¢å¯¹æœ‰é™ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€çº¿ç´¢å»ºæ¨¡ç¤¾äº¤äº’åŠ¨ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹ä¸å®Œæ•´å’Œå™ªå£°ä¿¡æ¯å…·æœ‰é²æ£’æ€§ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œåˆ©ç”¨å¤šæ¨¡æ€è¾“å…¥ï¼ˆèº«ä½“ã€é¢éƒ¨å’Œæ‰‹åŠ¿ã€è§†çº¿ã€åŸå§‹å›¾åƒï¼‰ï¼Œå°†çŸ¥è¯†è½¬ç§»åˆ°ä»…ä¾èµ–èº«ä½“å§¿æ€çš„å­¦ç”Ÿæ¨¡å‹ä¸Šã€‚å®éªŒè¡¨æ˜ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªç¤¾äº¤ç†è§£ä»»åŠ¡ä¸Šç›¸è¾ƒäºç«äº‰åŸºçº¿å¹³å‡æå‡äº†14.75%çš„å‡†ç¡®ç‡ï¼Œå³ä½¿è¾“å…¥æœ‰51%è¢«æŸåã€‚å­¦ç”Ÿæ¨¡å‹åœ¨å‚æ•°æ•°é‡ä¸Šä»…ä¸ºæ•™å¸ˆæ¨¡å‹çš„1%ï¼Œå»¶è¿Ÿä¸ºæ•™å¸ˆæ¨¡å‹çš„11.9%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç¤¾äº¤æœºå™¨äººåœ¨ä¸ç”¨æˆ·äº’åŠ¨æ—¶å¯¹ç¤¾äº¤åœºæ™¯å’Œè¡Œä¸ºçº¿ç´¢çš„ç†è§£ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å®æ—¶éƒ¨ç½²å’Œæœ‰é™ä¿¡æ¯ä¸‹è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ç§çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€è¾“å…¥è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶å°†çŸ¥è¯†è½¬ç§»è‡³ä»…ä¾èµ–èº«ä½“å§¿æ€çš„å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥æé«˜é²æ£’æ€§å’Œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚æ•™å¸ˆæ¨¡å‹å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œå­¦ç”Ÿæ¨¡å‹åˆ™é€šè¿‡è’¸é¦å­¦ä¹ ä»æ•™å¸ˆæ¨¡å‹ä¸­è·å–çŸ¥è¯†ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡å¤šæ¨¡æ€è’¸é¦å®ç°äº†åœ¨è¾“å…¥ä¿¡æ¯ä¸å®Œæ•´æˆ–å—å™ªå£°å½±å“æ—¶çš„é²æ£’æ€§ï¼Œæ˜¾è‘—æå‡äº†ç¤¾äº¤ç†è§£çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œæ•™å¸ˆæ¨¡å‹ä½¿ç”¨å¤šç§è¾“å…¥ä¿¡å·ï¼Œå­¦ç”Ÿæ¨¡å‹åˆ™ä¼˜åŒ–ä¸ºä»…ä¾èµ–èº«ä½“å§¿æ€ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†ä¿¡æ¯çš„å®Œæ•´æ€§ä¸é²æ£’æ€§ã€‚å®éªŒä¸­è¿˜å¯¹æ¨¡å‹çš„å‚æ•°æ•°é‡å’Œå»¶è¿Ÿè¿›è¡Œäº†ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªç¤¾äº¤ç†è§£ä»»åŠ¡ä¸Šå¹³å‡æå‡äº†14.75%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºç«äº‰åŸºçº¿è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œå­¦ç”Ÿæ¨¡å‹çš„å‚æ•°æ•°é‡ä»…ä¸ºæ•™å¸ˆæ¨¡å‹çš„1%ï¼Œå»¶è¿Ÿä»…ä¸ºæ•™å¸ˆæ¨¡å‹çš„11.9%ï¼Œæ˜¾ç¤ºå‡ºæé«˜çš„æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾äº¤æœºå™¨äººã€æ™ºèƒ½å®¶å±…åŠ©æ‰‹å’Œäººæœºäº¤äº’ç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹ç¤¾äº¤åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æ”¯æŒç”¨æˆ·ï¼Œå¢å¼ºäººæœºäº’åŠ¨çš„è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ï¼Œæœªæ¥å¯èƒ½åœ¨æœåŠ¡è¡Œä¸šã€æ•™è‚²å’ŒåŒ»ç–—ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> There is a growing need for social robots and intelligent agents that can effectively interact with and support users. For the interactions to be seamless, the agents need to analyse social scenes and behavioural cues from their (robot's) perspective. Works that model human-agent interactions in social situations are few; and even those existing ones are computationally too intensive to be deployed in real time or perform poorly in real-world scenarios when only limited information is available. We propose a knowledge distillation framework that models social interactions through various multimodal cues, and yet is robust against incomplete and noisy information during inference. We train a teacher model with multimodal input (body, face and hand gestures, gaze, raw images) that transfers knowledge to a student model which relies solely on body pose. Extensive experiments on two publicly available human-robot interaction datasets demonstrate that our student model achieves an average accuracy gain of 14.75% over competitive baselines on multiple downstream social understanding tasks, even with up to 51% of its input being corrupted. The student model is also highly efficient - less than 1% in size of the teacher model in terms of parameters and its latency is 11.9% of the teacher model. Our code and related data are available at github.com/biantongfei/SocialEgoMobile.

