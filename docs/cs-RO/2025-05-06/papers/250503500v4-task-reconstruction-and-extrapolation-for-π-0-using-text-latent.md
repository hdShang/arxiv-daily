---
layout: default
title: Task Reconstruction and Extrapolation for $Ï€_0$ using Text Latent
---

# Task Reconstruction and Extrapolation for $Ï€_0$ using Text Latent

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03500" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03500v4</a>
  <a href="https://arxiv.org/pdf/2505.03500.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03500v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03500v4', 'Task Reconstruction and Extrapolation for $Ï€_0$ using Text Latent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Quanyi Li

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-08-03)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–‡æœ¬æ½œåœ¨ç©ºé—´é‡æ„ä¸å¤–æ¨æ–¹æ³•ä»¥æå‡VLAä»»åŠ¡è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹` `ä»»åŠ¡å¤–æ¨` `æ–‡æœ¬æ½œåœ¨ç©ºé—´` `è¡Œä¸ºé‡ç»„` `æœºå™¨äººæ“ä½œ` `æ™ºèƒ½åŠ©æ‰‹` `è‡ªåŠ¨åŒ–ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨ä»»åŠ¡å¤–æ¨æ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆç»„åˆä¸åŒä»»åŠ¡çš„æŠ€èƒ½ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ“æ§VLAçš„æ–‡æœ¬æ½œåœ¨ç©ºé—´ï¼Œåœ¨æ¨ç†æ—¶é‡ç»„æ¥è‡ªä¸åŒä»»åŠ¡çš„è¡Œä¸ºï¼Œä»¥å®ç°ä»»åŠ¡å¤–æ¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ–‡æœ¬æ½œåœ¨ç©ºé—´æ’å€¼çš„$	ext{Ï€}_0$åœ¨libero-oodåŸºå‡†ä¸Šå–å¾—äº†83%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼ˆVLAï¼‰åœ¨æ‰§è¡Œå·²æ¼”ç¤ºä»»åŠ¡æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éœ€è¦å°†ä¸åŒä»»åŠ¡çš„æŠ€èƒ½ç»„åˆæˆæ–°æ–¹å¼æ—¶å´é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚æœ¬æ–‡å±•ç¤ºäº†é€šè¿‡åœ¨æ¨ç†æ—¶æ“æ§VLAçš„å†…éƒ¨è¡¨ç¤ºï¼Œå¯ä»¥æœ‰æ•ˆåœ°é‡ç»„æ¥è‡ªä¸åŒä»»åŠ¡çš„è¡Œä¸ºã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ç‰¹å®šåŸºç¡€ä»»åŠ¡çš„æ‰€æœ‰æ¼”ç¤ºè½¨è¿¹çš„æ–‡æœ¬æ ‡è®°éšè—çŠ¶æ€è¿›è¡Œå¹³å‡ï¼Œè¯†åˆ«æ–‡æœ¬æ½œåœ¨ç©ºé—´ã€‚åœ¨æ‰§è¡Œå¤–æ¨ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ä¸¤ä¸ªåŸºç¡€ä»»åŠ¡çš„æ–‡æœ¬æ½œåœ¨ç©ºé—´è¿›è¡Œæ—¶é—´æ’å€¼ï¼Œå¹¶å°†å…¶æ·»åŠ å›æ–‡æœ¬éšè—çŠ¶æ€ï¼Œä»è€Œé¡ºåºæ¿€æ´»ä¸¤ä¸ªä»»åŠ¡çš„å­è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨æ–°åˆ›å»ºçš„libero-oodåŸºå‡†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰æœ€å…ˆè¿›çš„VLAåœ¨è¯¥åŸºå‡†ä¸Šçš„æˆåŠŸç‡å‡ä½äº15%ï¼Œè€Œä½¿ç”¨æ–‡æœ¬æ½œåœ¨ç©ºé—´æ’å€¼çš„$	ext{Ï€}_0$è¾¾åˆ°äº†83%çš„æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨ä»»åŠ¡å¤–æ¨æ—¶çš„è¡¨ç°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨ç»„åˆä¸åŒä»»åŠ¡æŠ€èƒ½æ—¶å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¯¼è‡´æˆåŠŸç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡åœ¨æ¨ç†é˜¶æ®µæ“æ§VLAçš„å†…éƒ¨è¡¨ç¤ºï¼Œå…·ä½“æ˜¯é€šè¿‡è¯†åˆ«å’Œæ’å€¼æ–‡æœ¬æ½œåœ¨ç©ºé—´ï¼Œæ¥æœ‰æ•ˆé‡ç»„æ¥è‡ªä¸åŒä»»åŠ¡çš„è¡Œä¸ºï¼Œä»è€Œå®ç°ä»»åŠ¡å¤–æ¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åŒ…æ‹¬è¯†åˆ«ç‰¹å®šåŸºç¡€ä»»åŠ¡çš„æ–‡æœ¬æ½œåœ¨ç©ºé—´ã€å¯¹ä¸¤ä¸ªåŸºç¡€ä»»åŠ¡çš„æ–‡æœ¬æ½œåœ¨ç©ºé—´è¿›è¡Œæ—¶é—´æ’å€¼ï¼Œå¹¶å°†å…¶æ·»åŠ å›æ–‡æœ¬éšè—çŠ¶æ€ï¼Œä»¥é¡ºåºæ¿€æ´»å­è¡Œä¸ºã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡æ–‡æœ¬æ½œåœ¨ç©ºé—´çš„æ’å€¼å®ç°äº†ä»»åŠ¡å¤–æ¨ï¼Œè¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„ç›´æ¥ä»»åŠ¡ç»„åˆæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°å¤„ç†æ–°ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¯¹æ–‡æœ¬æ ‡è®°éšè—çŠ¶æ€çš„å¹³å‡è®¡ç®—ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šå…³æ³¨äºå¦‚ä½•æœ‰æ•ˆåœ°æ¿€æ´»å­è¡Œä¸ºï¼Œç½‘ç»œç»“æ„åˆ™ä¿æŒäº†VLAçš„åŸºæœ¬æ¶æ„ï¼ŒåŒæ—¶å¼•å…¥äº†æ½œåœ¨ç©ºé—´æ’å€¼çš„æ¨¡å—ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨libero-oodåŸºå‡†ä¸Šçš„æˆåŠŸç‡å‡ä½äº15%ï¼Œè€Œä½¿ç”¨æ–‡æœ¬æ½œåœ¨ç©ºé—´æ’å€¼çš„$	ext{Ï€}_0$æˆåŠŸç‡é«˜è¾¾83%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†æè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹å­˜åœ¨ç©ºé—´è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œå½±å“äº†å…¶å¯¹ç›®æ ‡çš„çœŸå®ç†è§£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€æ™ºèƒ½åŠ©æ‰‹å’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿæå‡è¿™äº›ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„çµæ´»æ€§å’Œé€‚åº”èƒ½åŠ›ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¤šå®é™…åœºæ™¯ä¸­å®ç°æ›´é«˜æ•ˆçš„ä»»åŠ¡æ‰§è¡Œå’Œäººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language-action models (VLAs) often achieve high performance on demonstrated tasks but struggle significantly when required to extrapolate, combining skills learned from different tasks in novel ways. For instance, VLAs might successfully put the cream cheese in the bowl and put the bowl on top of the cabinet, yet still fail to put the cream cheese on top of the cabinet. In this work, we demonstrate that behaviors from distinct tasks can be effectively recombined by manipulating the VLA's internal representations at inference time. Concretely, we identify the text latent by averaging the text tokens' hidden states across all demonstrated trajectories for a specific base task. For executing an extrapolated task, we can temporally interpolate the text latent of the two base tasks and add it back to the text hidden states, so sub-behaviors from the two tasks will be activated sequentially. We evaluate this approach using the newly created libero-ood benchmark, featuring 20 tasks extrapolated from standard LIBERO suites. The results on libero-ood show that all SOTA VLAs achieve < 15% success rate, while $\pi0$ with text latent interpolation reaches an 83% success rate. Further qualitative analysis reveals a tendency for VLAs to exhibit spatial overfitting, mapping object names to demonstrated locations rather than achieving genuine object and goal understanding. Additionally, we find that decoding the text latent yields human-unreadable prompts that can nevertheless instruct the VLA to achieve a 70% success rate on standard LIBERO suites, enabling private instruction or backdoor attacks.

