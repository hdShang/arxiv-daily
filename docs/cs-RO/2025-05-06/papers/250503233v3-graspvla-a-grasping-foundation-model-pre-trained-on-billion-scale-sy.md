---
layout: default
title: "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data"
---

# GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03233" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03233v3</a>
  <a href="https://arxiv.org/pdf/2505.03233.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03233v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03233v3', 'GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Wenhao Zhang, Heming Cui, Zhizheng Zhang, He Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-08-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGraspVLAä»¥è§£å†³æœºå™¨äººæŠ“å–ä»»åŠ¡ä¸­çš„æ•°æ®ä¾èµ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæŠ“å–` `åˆæˆæ•°æ®` `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `é›¶-shotå­¦ä¹ ` `å°‘é‡æ ·æœ¬é€‚åº”` `æ¨¡å‹é¢„è®­ç»ƒ` `å¤šæ¨¡æ€å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŠ“å–æ¨¡å‹è¿‡äºä¾èµ–çœŸå®ä¸–ç•Œæ•°æ®ï¼Œå¯¼è‡´æ•°æ®æ”¶é›†æˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. æœ¬æ–‡æå‡ºGraspVLAæ¨¡å‹ï¼Œåˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œç»“åˆè‡ªå›å½’æ„ŸçŸ¥å’ŒåŠ¨ä½œç”Ÿæˆï¼Œæå‡æŠ“å–ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGraspVLAåœ¨çœŸå®å’Œä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜è¶Šçš„é›¶-shotæ³›åŒ–èƒ½åŠ›å’Œå°‘é‡æ ·æœ¬é€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«åŸºç¡€æ¨¡å‹å› å…¶é›¶-shotæ³›åŒ–èƒ½åŠ›ã€å¯æ‰©å±•æ€§å’Œé€šè¿‡å°‘é‡æ ·æœ¬åè®­ç»ƒé€‚åº”æ–°ä»»åŠ¡è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹è¿‡äºä¾èµ–çœŸå®ä¸–ç•Œæ•°æ®ï¼Œæ”¶é›†æˆæœ¬é«˜ä¸”åŠ³åŠ¨å¯†é›†ã€‚åˆæˆæ•°æ®æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å…¶æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ¢è®¨äº†å®Œå…¨åŸºäºå¤§è§„æ¨¡åˆæˆåŠ¨ä½œæ•°æ®è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬æ„å»ºäº†SynGrasp-1Bï¼Œä¸€ä¸ªåœ¨ä»¿çœŸä¸­ç”Ÿæˆçš„åäº¿å¸§æœºå™¨äººæŠ“å–æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†GraspVLAï¼Œä¸€ä¸ªä»¥å¤§è§„æ¨¡åˆæˆåŠ¨ä½œæ•°æ®ä¸ºåŸºç¡€çš„VLAæ¨¡å‹ã€‚GraspVLAå°†è‡ªå›å½’æ„ŸçŸ¥ä»»åŠ¡ä¸åŸºäºæµåŒ¹é…çš„åŠ¨ä½œç”Ÿæˆæ•´åˆä¸ºç»Ÿä¸€çš„æ€ç»´é“¾è¿‡ç¨‹ï¼Œä¿ƒè¿›äº†åˆæˆåŠ¨ä½œæ•°æ®ä¸äº’è”ç½‘è¯­ä¹‰æ•°æ®çš„è”åˆè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†æŠ“å–ä»»åŠ¡çš„å¼€æ”¾è¯æ±‡æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æŠ“å–æ¨¡å‹å¯¹çœŸå®æ•°æ®çš„é«˜åº¦ä¾èµ–ï¼Œå¯¼è‡´çš„é«˜æˆæœ¬å’Œä½æ‰©å±•æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¸åŒç¯å¢ƒå’Œå¯¹è±¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºå¤§è§„æ¨¡åˆæˆæ•°æ®é›†SynGrasp-1Bï¼Œè®­ç»ƒGraspVLAæ¨¡å‹ï¼Œåˆ©ç”¨åˆæˆæ•°æ®çš„ä¸°å¯Œæ€§å’Œå¤šæ ·æ€§æ¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚æ¨¡å‹è®¾è®¡ä¸Šç»“åˆäº†è‡ªå›å½’æ„ŸçŸ¥ä»»åŠ¡ä¸æµåŒ¹é…çš„åŠ¨ä½œç”Ÿæˆï¼Œå½¢æˆç»Ÿä¸€çš„æ€ç»´é“¾è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGraspVLAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œåˆ©ç”¨åˆæˆæ•°æ®é›†è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡è”åˆè®­ç»ƒå¼•å…¥äº’è”ç½‘è¯­ä¹‰æ•°æ®ï¼Œæœ€ååœ¨çœŸå®å’Œä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šGraspVLAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶å°†åˆæˆæ•°æ®ä¸äº’è”ç½‘è¯­ä¹‰æ•°æ®çš„è”åˆè®­ç»ƒï¼Œæ˜¾è‘—å‡å°‘äº†æ¨¡æ‹Ÿåˆ°çœŸå®çš„å·®è·ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–å¯¹è±¡ä¸Šçš„æŠ“å–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¨¡å‹é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è‡ªå›å½’æ„ŸçŸ¥å’ŒåŠ¨ä½œç”Ÿæˆçš„è®­ç»ƒç›®æ ‡ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†æµåŒ¹é…æœºåˆ¶ï¼Œä»¥æé«˜åŠ¨ä½œç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚å°†åœ¨åç»­éƒ¨åˆ†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒGraspVLAæ¨¡å‹åœ¨çœŸå®å’Œä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œé›¶-shotæ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬é€‚åº”æ€§æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¿«é€Ÿé€‚åº”ç‰¹å®šçš„äººç±»åå¥½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GraspVLAæ¨¡å‹åœ¨æœºå™¨äººæŠ“å–é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”ä¸åŒçš„æŠ“å–ä»»åŠ¡å’Œå¯¹è±¡ã€‚å…¶åŸºäºåˆæˆæ•°æ®çš„è®­ç»ƒæ–¹å¼é™ä½äº†æ•°æ®æ”¶é›†æˆæœ¬ï¼Œæœªæ¥å¯åœ¨æ™ºèƒ½å®¶å±…ã€è‡ªåŠ¨åŒ–ä»“å‚¨å’ŒæœåŠ¡æœºå™¨äººç­‰å¤šä¸ªåœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.

