---
layout: default
title: Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention
---

# Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03400" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03400v1</a>
  <a href="https://arxiv.org/pdf/2505.03400.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03400v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03400v1', 'Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Takuma Tsukakoshi, Tamon Miyake, Tetsuya Ogata, Yushi Wang, Takumi Akaishi, Shigeki Sugano

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºçŠ¶æ€ä¼°è®¡çš„ç´§èº«è¡£ç‰©ç©¿æˆ´è¾…åŠ©æ–¹æ³•ä»¥è§£å†³è€é¾„åŒ–æŠ¤ç†çŸ­ç¼ºé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `ç©¿æˆ´è¾…åŠ©` `å¤šæ¨¡æ€ä¿¡æ¯` `çŠ¶æ€ä¼°è®¡` `è¯­ä¹‰ä¿¡æ¯` `æœºå™¨äººæŠ€æœ¯` `è€é¾„åŒ–æŠ¤ç†` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç©¿æˆ´è¾…åŠ©æ–¹æ³•åœ¨å¤„ç†ç´§èº«è¡£ç‰©æ—¶é¢ä¸´æ‘©æ“¦å’Œè¡£ç‰©å½¢çŠ¶å®šä½çš„æŒ‘æˆ˜ï¼Œéš¾ä»¥é€‚åº”ä¸ªä½“å·®å¼‚ã€‚
2. æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯å’Œè¯­ä¹‰ä¿¡æ¯çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°å¯¹è¢œå­å’Œè„šçš„çŠ¶æ€ä¼°è®¡ï¼Œæä¾›ç²¾ç¡®çš„ç©¿æˆ´è¾…åŠ©ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨10åå‚ä¸è€…ä¸­æˆåŠŸç©¿æˆ´è¢œå­çš„æˆåŠŸç‡é«˜äºç°æœ‰çš„Action Chunking with Transformerå’ŒDiffusion Policyæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€äººå£è€é¾„åŒ–åŠ å‰§ï¼Œæœªæ¥æŠ¤ç†äººå‘˜çŸ­ç¼ºé—®é¢˜æ—¥ç›Šä¸¥é‡ï¼Œç©¿æˆ´è¾…åŠ©å°¤å…¶é‡è¦ã€‚ç©¿æˆ´ç´§èº«è¡£ç‰©ï¼ˆå¦‚è¢œå­ï¼‰é¢ä¸´ç€éœ€è¦ç²¾ç»†åŠ›è°ƒæ•´ä»¥åº”å¯¹æ‘©æ“¦å’Œè¡£ç‰©å½¢çŠ¶å®šä½çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬æœºå™¨äººæ‘„åƒå¤´å›¾åƒã€å…³èŠ‚è§’åº¦ã€å…³èŠ‚æ‰­çŸ©å’Œè§¦è§‰åŠ›ï¼Œä»¥é€‚åº”ä¸ªä½“å·®å¼‚ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥åŸºäºå¯¹è±¡æ¦‚å¿µçš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ–¹æ³•èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„è„šå’ŒèƒŒæ™¯ã€‚ç»“åˆæ·±åº¦æ•°æ®ï¼Œæœ‰åŠ©äºæ¨æ–­è¢œå­ä¸è„šä¹‹é—´çš„ç›¸å¯¹ç©ºé—´å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æœºå™¨äººæˆåŠŸé€‚åº”äº†æœªè§è¿‡çš„äººè„šï¼Œå¹¶ä¸º10åå‚ä¸è€…ç©¿ä¸Šè¢œå­ï¼ŒæˆåŠŸç‡é«˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³è€é¾„åŒ–ç¤¾ä¼šä¸­æŠ¤ç†äººå‘˜çŸ­ç¼ºå¯¼è‡´çš„ç©¿æˆ´è¾…åŠ©é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç´§èº«è¡£ç‰©çš„ç©¿æˆ´å›°éš¾ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåº”å¯¹ä¸ªä½“å·®å¼‚å’Œæ‘©æ“¦é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚è§†è§‰ã€è§¦è§‰ç­‰ï¼‰å’Œè¯­ä¹‰ä¿¡æ¯çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡çŠ¶æ€ä¼°è®¡å®ç°å¯¹è¢œå­å’Œè„šçš„ç²¾ç¡®é€‚é…ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”ä¸åŒçš„äººè„šå½¢çŠ¶å’ŒèƒŒæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†æ¨¡å—ï¼ˆæ‘„åƒå¤´ã€ä¼ æ„Ÿå™¨ï¼‰ã€çŠ¶æ€ä¼°è®¡æ¨¡å—ï¼ˆç»“åˆè§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼‰ã€å†³ç­–æ¨¡å—ï¼ˆåŸºäºè¯­ä¹‰ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼‰å’Œæ‰§è¡Œæ¨¡å—ï¼ˆæ§åˆ¶æœºå™¨äººè¿›è¡Œç©¿æˆ´ï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†è¯­ä¹‰ä¿¡æ¯å’Œæ·±åº¦æ•°æ®ï¼Œä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„è„šå’ŒèƒŒæ™¯ä¸‹è¿›è¡Œæœ‰æ•ˆçš„ç©¿æˆ´æ“ä½œï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­æ˜¯æœªæ›¾å®ç°çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œé‡‡ç”¨äº†å¤šæ¨¡æ€èåˆçš„ç­–ç•¥ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†ç©¿æˆ´æˆåŠŸç‡å’Œå®‰å…¨æ€§ï¼Œç½‘ç»œç»“æ„åˆ™ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œå’Œé€’å½’ç¥ç»ç½‘ç»œï¼Œä»¥å¤„ç†æ—¶åºæ•°æ®å’Œç©ºé—´ç‰¹å¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœºå™¨äººåœ¨10åå‚ä¸è€…ä¸­æˆåŠŸç©¿æˆ´è¢œå­çš„æˆåŠŸç‡è¶…è¿‡äº†ç°æœ‰çš„Action Chunking with Transformerå’ŒDiffusion Policyæ–¹æ³•ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤„ç†æœªè§è¿‡çš„äººè„šæ—¶çš„ä¼˜è¶Šæ€§ï¼ŒæˆåŠŸç‡æ˜¾è‘—æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è€å¹´äººæŠ¤ç†ã€æ®‹éšœäººå£«è¾…åŠ©å’Œæ™ºèƒ½å®¶å±…ç³»ç»Ÿç­‰ã€‚é€šè¿‡æä¾›ç²¾ç¡®çš„ç©¿æˆ´è¾…åŠ©ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è€å¹´äººå’Œæ®‹éšœäººå£«çš„ç”Ÿæ´»è´¨é‡ï¼Œä¿ƒè¿›ä»–ä»¬çš„ç¤¾ä¼šå‚ä¸ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åœ¨æœºå™¨äººè¾…åŠ©ç”Ÿæ´»é¢†åŸŸå‘æŒ¥æ›´å¤§ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As the population continues to age, a shortage of caregivers is expected in the future. Dressing assistance, in particular, is crucial for opportunities for social participation. Especially dressing close-fitting garments, such as socks, remains challenging due to the need for fine force adjustments to handle the friction or snagging against the skin, while considering the shape and position of the garment. This study introduces a method uses multi-modal information including not only robot's camera images, joint angles, joint torques, but also tactile forces for proper force interaction that can adapt to individual differences in humans. Furthermore, by introducing semantic information based on object concepts, rather than relying solely on RGB data, it can be generalized to unseen feet and background. In addition, incorporating depth data helps infer relative spatial relationship between the sock and the foot. To validate its capability for semantic object conceptualization and to ensure safety, training data were collected using a mannequin, and subsequent experiments were conducted with human subjects. In experiments, the robot successfully adapted to previously unseen human feet and was able to put socks on 10 participants, achieving a higher success rate than Action Chunking with Transformer and Diffusion Policy. These results demonstrate that the proposed model can estimate the state of both the garment and the foot, enabling precise dressing assistance for close-fitting garments.

