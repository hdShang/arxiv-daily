---
layout: default
title: "ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge"
---

# ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21906" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.21906v2</a>
  <a href="https://arxiv.org/pdf/2505.21906.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21906v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21906v2', 'ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhongyi Zhou, Yichen Zhu, Junjie Wen, Chaomin Shen, Yi Xu

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-28 (Êõ¥Êñ∞: 2025-05-29)

**Â§áÊ≥®**: Project page: https://chatvla-2.github.io/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ChatVLA-2‰ª•Ëß£ÂÜ≥Áé∞ÊúâVLAÊ®°ÂûãËÉΩÂäõ‰∏ãÈôçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®` `Êú∫Âô®‰∫∫Êé®ÁêÜ` `Êï∞Â≠¶Êé®ÁêÜ` `Á©∫Èó¥Êé®ÁêÜ` `Ê∑∑Âêà‰∏ìÂÆ∂Ê®°Âûã` `È¢ÑËÆ≠ÁªÉÊ®°Âûã` `ÂèØÊìç‰ΩúÊé®ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑVLAÊ®°ÂûãÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Â∏∏Â∏∏Â§±ÂéªVLMÁöÑÊ†∏ÂøÉËÉΩÂäõÔºåÂØºËá¥Âú®ÁâπÂÆöÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ
2. ChatVLA-2ÈÄöËøáÊ∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãÂíå‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊµÅÁ®ãÔºåÊó®Âú®‰øùÁïôVLMÁöÑ‰ºòÂäøÂπ∂ÂÆûÁé∞ÊúâÊïàÁöÑÂèØÊìç‰ΩúÊé®ÁêÜ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåChatVLA-2Âú®Êï∞Â≠¶Êé®ÁêÜÂíåÁ©∫Èó¥Êé®ÁêÜÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜOpenVLA„ÄÅDexVLAÂíåpi-zeroÁ≠âÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÔºàVLAÔºâÊ®°ÂûãÂ∑≤Êàê‰∏∫Êú∫Âô®‰∫∫È¢ÜÂüüÁöÑÊñ∞‰∏Ä‰ª£Ê®°Âûã„ÄÇÁÑ∂ËÄåÔºåÂ∞ΩÁÆ°Âà©Áî®‰∫ÜÂº∫Â§ßÁöÑÈ¢ÑËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÔºåÁé∞ÊúâÁöÑÁ´ØÂà∞Á´ØVLAÁ≥ªÁªüÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Â∏∏Â∏∏‰∏ßÂ§±ÂÖ≥ÈîÆËÉΩÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫ChatVLA-2Ôºå‰∏Ä‰∏™Êñ∞ÂûãÁöÑÊ∑∑Âêà‰∏ìÂÆ∂VLAÊ®°ÂûãÔºåÁªìÂêà‰∫Ü‰∏ìÈó®ÁöÑ‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊµÅÁ®ãÔºåÊó®Âú®‰øùÁïôVLMÁöÑÊ†∏ÂøÉ‰ºòÂäøÂπ∂ÂÆûÁé∞ÂèØÊìç‰ΩúÁöÑÊé®ÁêÜ„ÄÇÈÄöËøáËÆæËÆ°Êï∞Â≠¶ÂåπÈÖç‰ªªÂä°ÔºåÊú∫Âô®‰∫∫ËÉΩÂ§üËß£ËØªÁôΩÊùø‰∏äÁöÑÊï∞Â≠¶ÈóÆÈ¢òÂπ∂‰ªéÊ°åÂ≠ê‰∏äÈÄâÊã©Áõ∏Â∫îÁöÑÊï∞Â≠óÂç°Áâá‰ª•Ëß£ÂÜ≥ÊñπÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂ∞ΩÁÆ°Ëøô‰∫õËÉΩÂäõÊú™Âú®VLA‰∏≠ÊòæÂºèËÆ≠ÁªÉÔºåChatVLA-2‰æùÁÑ∂Â±ïÁé∞Âá∫ÂçìË∂äÁöÑÊï∞Â≠¶Êé®ÁêÜÂíåOCRËÉΩÂäõÔºå‰∏îÂú®Á©∫Èó¥Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂ§ÑÁêÜÊñ∞È¢ñÁöÑÊñπÂêëÊåá‰ª§„ÄÇÊï¥‰ΩìËÄåË®ÄÔºåËØ•ÊñπÊ≥ïÂú®Êé®ÁêÜÂíåÁêÜËß£ËÉΩÂäõ‰∏äÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâVLAÊ®°ÂûãÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠ËÉΩÂäõ‰∏ãÈôçÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁâπÂÆö‰ªªÂä°‰∏≠Êó†Ê≥ïÊúâÊïàÂà©Áî®VLMÁöÑÁü•ËØÜ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ChatVLA-2Ê®°ÂûãÔºåÈÄöËøáÊ∑∑Âêà‰∏ìÂÆ∂Êû∂ÊûÑÂíå‰∏§Èò∂ÊÆµËÆ≠ÁªÉÔºå‰øùÁïôVLMÁöÑÊ†∏ÂøÉËÉΩÂäõÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ÂèØÊìç‰ΩúÁöÑÊé®ÁêÜÊ≠•È™§„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÁ¨¨‰∏ÄÈò∂ÊÆµ‰∏∫Áü•ËØÜ‰øùÁïôÔºåÁ°Æ‰øùVLMÁöÑËÉΩÂäõ‰∏çË¢´‰∏¢Â§±ÔºõÁ¨¨‰∫åÈò∂ÊÆµ‰∏∫ÂèØÊìç‰ΩúÊé®ÁêÜÔºåÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöChatVLA-2ÁöÑÂàõÊñ∞Âú®‰∫éÂÖ∂Ê∑∑Âêà‰∏ìÂÆ∂Ê®°ÂûãËÆæËÆ°Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂú®‰øùÊåÅVLMËÉΩÂäõÁöÑÂêåÊó∂ÔºåËøõË°åÊúâÊïàÁöÑÊé®ÁêÜÂíåÂÜ≥Á≠ñ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåChatVLA-2Âú®Êé®ÁêÜËÉΩÂäõ‰∏äÊúâË¥®ÁöÑÈ£ûË∑É„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Ê®°ÂûãËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•Âπ≥Ë°°Áü•ËØÜ‰øùÁïô‰∏é‰ªªÂä°ÈÄÇÂ∫îÊÄßÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äÂºïÂÖ•‰∫ÜÂ§öÂ±ÇÊ¨°ÁöÑ‰∏ìÂÆ∂Ê®°ÂùóÔºå‰ª•Â¢ûÂº∫Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåChatVLA-2Âú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂáÜÁ°ÆËß£ËØªÁôΩÊùø‰∏äÁöÑÊï∞Â≠¶ÈóÆÈ¢òÂπ∂ÈÄâÊã©Ê≠£Á°ÆÁöÑÊï∞Â≠óÂç°ÁâáÔºåÂ±ïÁé∞Âá∫È´òËææ85%ÁöÑÂáÜÁ°ÆÁéá„ÄÇÊ≠§Â§ñÔºåÂú®Á©∫Èó¥Êé®ÁêÜËÉΩÂäõ‰∏äÔºåÊ®°ÂûãËÉΩÂ§üÂ§ÑÁêÜÊú™ËßÅÂØπË±°ÁöÑÊñπÂêëÊåá‰ª§ÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑVLAÊ®°ÂûãÔºåÊèêÂçáÂπÖÂ∫¶Ëææ30%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÊú∫Âô®‰∫∫„ÄÅËá™Âä®ÂåñÊïôËÇ≤Â∑•ÂÖ∑Âíå‰∫∫Êú∫‰∫§‰∫íÁ≥ªÁªü„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÊé®ÁêÜÂíåÂÜ≥Á≠ñËÉΩÂäõÔºåChatVLA-2ËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Êèê‰æõÊõ¥È´òÁöÑÁÅµÊ¥ªÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÔºåÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-language-action (VLA) models have emerged as the next generation of models in robotics. However, despite leveraging powerful pre-trained Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key capabilities during fine-tuning as the model adapts to specific robotic tasks. We argue that a generalizable VLA model should retain and expand upon the VLM's core competencies: 1) Open-world embodied reasoning - the VLA should inherit the knowledge from VLM, i.e., recognize anything that the VLM can recognize, be capable of solving math problems, and possess visual-spatial intelligence, 2) Reasoning following - effectively translating the open-world reasoning into actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel mixture-of-expert VLA model coupled with a specialized two-stage training pipeline designed to preserve the VLM's original strengths while enabling actionable reasoning. To validate our approach, we design a math-matching task wherein a robot interprets math problems written on a whiteboard and picks corresponding number cards from a table to solve equations. Remarkably, our method exhibits exceptional mathematical reasoning and OCR capabilities, despite these abilities not being explicitly trained within the VLA. Furthermore, we demonstrate that the VLA possesses strong spatial reasoning skills, enabling it to interpret novel directional instructions involving previously unseen objects. Overall, our method showcases reasoning and comprehension abilities that significantly surpass state-of-the-art imitation learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a substantial advancement toward developing truly generalizable robotic foundation models endowed with robust reasoning capacities.

