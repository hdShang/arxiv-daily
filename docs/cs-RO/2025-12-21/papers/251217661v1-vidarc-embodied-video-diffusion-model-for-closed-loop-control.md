---
layout: default
title: Vidarc: Embodied Video Diffusion Model for Closed-loop Control
---

# Vidarc: Embodied Video Diffusion Model for Closed-loop Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17661" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17661v1</a>
  <a href="https://arxiv.org/pdf/2512.17661.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17661v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17661v1', 'Vidarc: Embodied Video Diffusion Model for Closed-loop Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Vidarcï¼šç”¨äºé—­ç¯æ§åˆ¶çš„å…·èº«è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæå‡æœºå™¨äººæ“ä½œæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººæ§åˆ¶` `è§†é¢‘æ‰©æ•£æ¨¡å‹` `å…·èº«æ™ºèƒ½` `é—­ç¯æ§åˆ¶` `é€†åŠ¨åŠ›å­¦` `è‡ªå›å½’æ¨¡å‹` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºè§†é¢‘çš„æœºå™¨äººæ§åˆ¶æ–¹æ³•åœ¨å…·èº«é—­ç¯æ§åˆ¶æ–¹é¢å­˜åœ¨é«˜å»¶è¿Ÿå’Œ grounding ä¸è¶³çš„é—®é¢˜ã€‚
2. Vidarc æå‡ºäº†ä¸€ç§è‡ªå›å½’å…·èº«è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åŠ¨ä½œç›¸å…³æ©ç å’Œå®æ—¶åé¦ˆï¼Œå®ç°å¿«é€Ÿå‡†ç¡®çš„é—­ç¯æ§åˆ¶ã€‚
3. Vidarc åœ¨çœŸå®æœºå™¨äººéƒ¨ç½²ä¸­ï¼ŒæˆåŠŸç‡æå‡è‡³å°‘ 15%ï¼Œå»¶è¿Ÿé™ä½ 91%ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç”±äºå¤æ‚çš„å…·èº«åŠ¨åŠ›å­¦å’Œå¤šæ ·çš„ç¯å¢ƒï¼Œæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æœºå™¨äººæ‰‹è‡‚æ“ä½œæå…·æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘åŸºäºè§†é¢‘çš„æ–¹æ³•é€šè¿‡åœ¨äº’è”ç½‘è§„æ¨¡çš„è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œåœ¨æ•è·å’Œè¿ç§»æ—¶é—´å’Œç‰©ç†äº¤äº’æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ²¡æœ‰é’ˆå¯¹ç‰¹å®šå…·èº«é—­ç¯æ§åˆ¶è¿›è¡Œä¼˜åŒ–ï¼Œé€šå¸¸å­˜åœ¨é«˜å»¶è¿Ÿå’Œä¸è¶³çš„ grounding é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº† Vidarcï¼ˆVideo Diffusion for Action Reasoning and Closed-loop Controlï¼‰ï¼Œä¸€ç§æ–°é¢–çš„è‡ªå›å½’å…·èº«è§†é¢‘æ‰©æ•£æ–¹æ³•ï¼Œé€šè¿‡ masked inverse dynamics æ¨¡å‹è¿›è¡Œå¢å¼ºã€‚é€šè¿‡ä½¿ç”¨ä¸åŠ¨ä½œç›¸å…³çš„æ©ç æ¥ grounding è§†é¢‘é¢„æµ‹ï¼Œå¹¶é€šè¿‡ç¼“å­˜çš„è‡ªå›å½’ç”Ÿæˆæ¥æ•´åˆå®æ—¶åé¦ˆï¼ŒVidarc å®ç°äº†å¿«é€Ÿã€å‡†ç¡®çš„é—­ç¯æ§åˆ¶ã€‚Vidarc åœ¨ä¸€ç™¾ä¸‡ä¸ªè·¨å…·èº« episodes ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œåœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­å®ç°äº†è‡³å°‘ 15% çš„æˆåŠŸç‡æå‡å’Œ 91% çš„å»¶è¿Ÿé™ä½ã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†å…¶åœ¨ä»¥å‰æœªè§è¿‡çš„æœºå™¨äººå¹³å°ä¸Šçš„é²æ£’æ³›åŒ–å’Œçº é”™èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹ï¼Œæœºå™¨äººæ‰‹è‡‚æ“ä½œä¸­ç°æœ‰åŸºäºè§†é¢‘çš„æ§åˆ¶æ–¹æ³•å­˜åœ¨çš„å»¶è¿Ÿé«˜ã€ä¸ç¯å¢ƒäº¤äº’ä¸è¶³çš„é—®é¢˜ã€‚è¿™äº›æ–¹æ³•é€šå¸¸éš¾ä»¥é€‚åº”ç‰¹å®šå…·èº«æœºå™¨äººçš„åŠ¨åŠ›å­¦ç‰¹æ€§ï¼Œå¯¼è‡´æ§åˆ¶ç²¾åº¦å’Œæ•ˆç‡é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVidarc çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å­¦ä¹ æœºå™¨äººæ“ä½œçš„åŠ¨æ€è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡è‡ªå›å½’ç”Ÿæˆçš„æ–¹å¼é¢„æµ‹æœªæ¥çŠ¶æ€ã€‚ä¸ºäº†æé«˜æ§åˆ¶ç²¾åº¦å’Œé™ä½å»¶è¿Ÿï¼Œè®ºæ–‡å¼•å…¥äº†åŠ¨ä½œç›¸å…³çš„æ©ç æ¥æŒ‡å¯¼è§†é¢‘é¢„æµ‹ï¼Œå¹¶åˆ©ç”¨å®æ—¶åé¦ˆè¿›è¡Œçº é”™ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVidarc çš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼šç”¨äºå­¦ä¹ æœºå™¨äººæ“ä½œçš„è§†é¢‘æ•°æ®åˆ†å¸ƒï¼›2) Masked Inverse Dynamics æ¨¡å‹ï¼šç”¨äºé¢„æµ‹ç»™å®šçŠ¶æ€å’Œç›®æ ‡çŠ¶æ€ä¹‹é—´çš„åŠ¨ä½œï¼›3) è‡ªå›å½’ç”Ÿæˆæ¨¡å—ï¼šé€šè¿‡ç¼“å­˜å†å²çŠ¶æ€å’ŒåŠ¨ä½œï¼Œå®ç°å®æ—¶çš„é—­ç¯æ§åˆ¶ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹é¢„æµ‹æœªæ¥çŠ¶æ€ï¼Œç„¶ååˆ©ç”¨ Masked Inverse Dynamics æ¨¡å‹è®¡ç®—æ‰€éœ€åŠ¨ä½œï¼Œæœ€åå°†åŠ¨ä½œå‘é€ç»™æœºå™¨äººæ‰§è¡Œï¼Œå¹¶æ ¹æ®å®æ—¶åé¦ˆæ›´æ–°çŠ¶æ€ã€‚

**å…³é”®åˆ›æ–°**ï¼šVidarc çš„å…³é”®åˆ›æ–°åœ¨äºå°†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸ masked inverse dynamics æ¨¡å‹ç›¸ç»“åˆï¼Œå¹¶é‡‡ç”¨è‡ªå›å½’ç”Ÿæˆçš„æ–¹å¼è¿›è¡Œé—­ç¯æ§åˆ¶ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨è§†é¢‘æ•°æ®ä¸­çš„ä¿¡æ¯ï¼Œæé«˜æ§åˆ¶ç²¾åº¦å’Œé²æ£’æ€§ï¼Œå¹¶é™ä½å»¶è¿Ÿã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVidarc æ›´åŠ æ³¨é‡å…·èº«æœºå™¨äººçš„åŠ¨åŠ›å­¦ç‰¹æ€§ï¼Œå¹¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­ä½¿ç”¨äº†å¤§é‡çš„è§†é¢‘æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œ masked inverse dynamics æ¨¡å‹ã€‚è‡ªå›å½’ç”Ÿæˆæ¨¡å—é‡‡ç”¨äº†ç¼“å­˜æœºåˆ¶ï¼Œä»¥é™ä½å»¶è¿Ÿã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17661v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17661v1/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17661v1/figures/artifacts.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

Vidarc åœ¨çœŸå®æœºå™¨äººéƒ¨ç½²ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒVidarc çš„æˆåŠŸç‡æé«˜äº†è‡³å°‘ 15%ï¼Œå»¶è¿Ÿé™ä½äº† 91%ã€‚æ­¤å¤–ï¼ŒVidarc è¿˜å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä»¥å‰æœªè§è¿‡çš„æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œæ“ä½œï¼Œå¹¶å…·æœ‰ä¸€å®šçš„çº é”™èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Vidarc çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚è¯¥ç ”ç©¶æˆæœå¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œé™ä½å¼€å‘æˆæœ¬ï¼Œå¹¶ä¿ƒè¿›æœºå™¨äººæŠ€æœ¯çš„æ™®åŠã€‚æœªæ¥ï¼ŒVidarc å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šæ¨¡æ€è¾“å…¥ã€å¤šä»»åŠ¡å­¦ä¹ ç­‰åœºæ™¯ï¼Œä¸ºæœºå™¨äººæ™ºèƒ½åŒ–æä¾›æ›´å¼ºå¤§çš„æ”¯æŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.

