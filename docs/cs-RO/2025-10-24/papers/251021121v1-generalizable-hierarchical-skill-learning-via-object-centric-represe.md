---
layout: default
title: Generalizable Hierarchical Skill Learning via Object-Centric Representation
---

# Generalizable Hierarchical Skill Learning via Object-Centric Representation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21121" target="_blank" class="toolbar-btn">arXiv: 2510.21121v1</a>
    <a href="https://arxiv.org/pdf/2510.21121.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21121v1" 
            onclick="toggleFavorite(this, '2510.21121v1', 'Generalizable Hierarchical Skill Learning via Object-Centric Representation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Haibo Zhao, Yu Qi, Boce Hu, Yizhe Zhu, Ziyan Chen, Heng Tian, Xupeng Zhu, Owen Howell, Haojie Huang, Robin Walters, Dian Wang, Robert Platt

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂØπË±°‰∏≠ÂøÉË°®Á§∫ÁöÑÈÄöÁî®ÂàÜÂ±ÇÊäÄËÉΩÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÊ≥õÂåñÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂàÜÂ±ÇÂº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ÂØπË±°‰∏≠ÂøÉË°®Á§∫` `ÊäÄËÉΩÂ≠¶‰π†` `Ê≥õÂåñËÉΩÂäõ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ïÂú®Ê≥õÂåñÊÄßÂíåÊ†∑Êú¨ÊïàÁéáÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÈöæ‰ª•ÈÄÇÂ∫îÊñ∞ÁöÑÁéØÂ¢ÉÂíå‰ªªÂä°„ÄÇ
2. GSLÂà©Áî®ÂØπË±°‰∏≠ÂøÉÊäÄËÉΩ‰Ωú‰∏∫Ê°•Ê¢ÅÔºåËøûÊé•È´òÂ±ÇËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂíå‰ΩéÂ±ÇËßÜËßâ-ËøêÂä®Á≠ñÁï•ÔºåÂÆûÁé∞ÊäÄËÉΩÁöÑËß£ËÄ¶ÂíåÊ≥õÂåñ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåGSLÂú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÂùáÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºå‰ªÖÈúÄÂ∞ëÈáèÊ†∑Êú¨Âç≥ÂèØÂÆûÁé∞ËâØÂ•ΩÁöÑÊ≥õÂåñÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÈÄöÁî®ÂàÜÂ±ÇÊäÄËÉΩÂ≠¶‰π†ÔºàGSLÔºâÁöÑÊñ∞Ê°ÜÊû∂ÔºåÁî®‰∫éÂàÜÂ±ÇÁ≠ñÁï•Â≠¶‰π†ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÁ≠ñÁï•Ê≥õÂåñÊÄßÂíåÊ†∑Êú¨ÊïàÁéá„ÄÇGSLÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØ‰ΩøÁî®ÂØπË±°‰∏≠ÂøÉÊäÄËÉΩ‰Ωú‰∏∫ËøûÊé•È´òÂ±ÇËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂíå‰ΩéÂ±ÇËßÜËßâ-ËøêÂä®Á≠ñÁï•ÁöÑÊé•Âè£„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåGSLÂà©Áî®Âü∫Á°ÄÊ®°ÂûãÂ∞ÜÊºîÁ§∫ÂàÜËß£‰∏∫ÂèØËΩ¨ÁßªÁöÑ„ÄÅÂØπË±°ËßÑËåÉÂåñÁöÑÊäÄËÉΩÂéüËØ≠Ôºå‰ªéËÄåÁ°Æ‰øùÂú®ÂØπË±°ÂùêÊ†áÁ≥ª‰∏≠ËøõË°åÈ´òÊïàÁöÑ‰ΩéÂ±ÇÊäÄËÉΩÂ≠¶‰π†„ÄÇÂú®ÊµãËØïÊó∂ÔºåÈ´òÂ±ÇÊô∫ËÉΩ‰ΩìÈ¢ÑÊµãÁöÑÊäÄËÉΩ-ÂØπË±°ÂØπË¢´ËæìÂÖ•Âà∞‰ΩéÂ±ÇÊ®°ÂùóÔºåÊé®Êñ≠Âá∫ÁöÑËßÑËåÉÂä®‰ΩúË¢´Êò†Â∞ÑÂõû‰∏ñÁïåÂùêÊ†áÁ≥ª‰ª•‰æõÊâßË°å„ÄÇËøôÁßçÁªìÊûÑÂåñ‰ΩÜÁÅµÊ¥ªÁöÑËÆæËÆ°ÊòæËëóÊèêÈ´ò‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÂú®Êú™ËßÅËøáÁöÑÁ©∫Èó¥ÊéíÂàó„ÄÅÂØπË±°Â§ñËßÇÂíå‰ªªÂä°ÁªÑÂêà‰∏≠ÁöÑÊ†∑Êú¨ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ÔºåGSL‰ªÖÁî®ÊØè‰∏™‰ªªÂä°3‰∏™ÊºîÁ§∫ËøõË°åËÆ≠ÁªÉÔºåÂú®Êú™ËßÅËøáÁöÑ‰ªªÂä°‰∏ä‰ºò‰∫é‰ΩøÁî®30ÂÄçÊï∞ÊçÆËÆ≠ÁªÉÁöÑÂü∫Á∫ø15.5%„ÄÇÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÂÆûÈ™å‰∏≠ÔºåGSL‰πüË∂ÖËøá‰∫Ü‰ΩøÁî®10ÂÄçÊï∞ÊçÆËÆ≠ÁªÉÁöÑÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ïÂú®Èù¢ÂØπÊñ∞ÁöÑÁ©∫Èó¥ÊéíÂàó„ÄÅÂØπË±°Â§ñËßÇÂíå‰ªªÂä°ÁªÑÂêàÊó∂ÔºåÊ≥õÂåñËÉΩÂäõËæÉÂº±Ôºå‰∏îÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÂ¶Ç‰ΩïÊèêÈ´òÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•ÁöÑÊ≥õÂåñÊÄßÂíåÊ†∑Êú¨ÊïàÁéáÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂ§çÊùÇÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ÂàÜËß£‰∏∫‰∏ÄÁ≥ªÂàóÂèØÂ§çÁî®ÁöÑ„ÄÅÂØπË±°‰∏≠ÂøÉÂåñÁöÑÊäÄËÉΩÂéüËØ≠„ÄÇÈÄöËøáÂú®ÂØπË±°ÂùêÊ†áÁ≥ª‰∏ãÂ≠¶‰π†Ëøô‰∫õÊäÄËÉΩÔºåÂèØ‰ª•‰ΩøÂÖ∂ÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ªéËÄåÈÄÇÂ∫î‰∏çÂêåÁöÑÁéØÂ¢ÉÂíå‰ªªÂä°„ÄÇÂêåÊó∂ÔºåÂà©Áî®È´òÂ±ÇËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊù•ÊåáÂØºÊäÄËÉΩÁöÑÈÄâÊã©ÂíåÁªÑÂêàÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÁ≠ñÁï•ÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöGSLÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ´òÂ±ÇÁ≠ñÁï•Âíå‰ΩéÂ±ÇÁ≠ñÁï•„ÄÇÈ´òÂ±ÇÁ≠ñÁï•Ë¥üË¥£Ê†πÊçÆÂΩìÂâçÁéØÂ¢ÉÂíå‰ªªÂä°ÁõÆÊ†áÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÊäÄËÉΩÂíåÂØπË±°„ÄÇ‰ΩéÂ±ÇÁ≠ñÁï•ÂàôË¥üË¥£ÊâßË°åÈÄâÂÆöÁöÑÊäÄËÉΩÔºåÂ∞ÜÂØπË±°ÂùêÊ†áÁ≥ª‰∏ãÁöÑÂä®‰ΩúÊò†Â∞ÑÂõû‰∏ñÁïåÂùêÊ†áÁ≥ªÔºåÂπ∂ÊéßÂà∂Êú∫Âô®‰∫∫ÂÆåÊàêÁõ∏Â∫îÁöÑÊìç‰Ωú„ÄÇÊï¥‰∏™Ê°ÜÊû∂ÈÄöËøáÂØπË±°‰∏≠ÂøÉÊäÄËÉΩ‰Ωú‰∏∫Êé•Âè£ÔºåËøûÊé•È´òÂ±ÇÂíå‰ΩéÂ±ÇÁ≠ñÁï•ÔºåÂÆûÁé∞ÂàÜÂ±ÇÊéßÂà∂„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöGSLÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ΩøÁî®ÂØπË±°‰∏≠ÂøÉË°®Á§∫Êù•Â≠¶‰π†ÊäÄËÉΩ„ÄÇÈÄöËøáÂ∞ÜÊäÄËÉΩÂÆö‰πâÂú®ÂØπË±°ÂùêÊ†áÁ≥ª‰∏ãÔºåÂèØ‰ª•Ê∂àÈô§ÂØπË±°ÂßøÊÄÅÂíåÁéØÂ¢ÉÂèòÂåñÁöÑÂΩ±ÂìçÔºå‰ªéËÄåÊèêÈ´òÊäÄËÉΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåGSLËøòÂà©Áî®Âü∫Á°ÄÊ®°ÂûãÊù•ÂàÜËß£ÊºîÁ§∫Êï∞ÊçÆÔºåÊèêÂèñÂèØËΩ¨ÁßªÁöÑÊäÄËÉΩÂéüËØ≠ÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöGSL‰ΩøÁî®TransformerÁΩëÁªú‰Ωú‰∏∫È´òÂ±ÇÁ≠ñÁï•ÔºåÈ¢ÑÊµãÊäÄËÉΩÂíåÂØπË±°„ÄÇ‰ΩéÂ±ÇÁ≠ñÁï•‰ΩøÁî®Á•ûÁªèÁΩëÁªúÂ≠¶‰π†ÂØπË±°ÂùêÊ†áÁ≥ª‰∏ãÁöÑÂä®‰Ωú„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÊäÄËÉΩÂàÜÁ±ªÊçüÂ§±„ÄÅÂä®‰ΩúÂõûÂΩíÊçüÂ§±ÂíåÊ®°‰ªøÂ≠¶‰π†ÊçüÂ§±„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÁªÜËäÇÂú®ËÆ∫Êñá‰∏≠ÊúâËØ¶ÁªÜÊèèËø∞ÔºàÊú™Áü•Ôºâ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Ê®°ÊãüÂÆûÈ™å‰∏≠ÔºåGSL‰ªÖ‰ΩøÁî®ÊØè‰∏™‰ªªÂä°3‰∏™ÊºîÁ§∫ËøõË°åËÆ≠ÁªÉÔºåÂú®Êú™ËßÅËøáÁöÑ‰ªªÂä°‰∏ä‰ºò‰∫é‰ΩøÁî®30ÂÄçÊï∞ÊçÆËÆ≠ÁªÉÁöÑÂü∫Á∫ø15.5%„ÄÇÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÂÆûÈ™å‰∏≠ÔºåGSL‰πüË∂ÖËøá‰∫Ü‰ΩøÁî®10ÂÄçÊï∞ÊçÆËÆ≠ÁªÉÁöÑÂü∫Á∫ø„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåGSLÂú®Ê≥õÂåñÊÄßÂíåÊ†∑Êú¨ÊïàÁéáÊñπÈù¢ÂÖ∑ÊúâÊòæËëó‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öÊú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊ≥õÂåñËÉΩÂäõÂíåÊ†∑Êú¨ÊïàÁéáÔºåÂèØ‰ª•Èôç‰ΩéÊú∫Âô®‰∫∫ÁöÑÈÉ®ÁΩ≤ÊàêÊú¨Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÂ§öÂèòÁöÑÁéØÂ¢ÉÔºåÂÆåÊàêÂêÑÁßç‰ªªÂä°„ÄÇÊú™Êù•ÔºåËØ•ÊñπÊ≥ïÊúâÊúõÊâ©Â±ïÂà∞Êõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÂíåÂú∫ÊôØÔºå‰æãÂ¶ÇÂ§öÊú∫Âô®‰∫∫Âçè‰Ωú„ÄÅ‰∫∫Êú∫Âçè‰ΩúÁ≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> We present Generalizable Hierarchical Skill Learning (GSL), a novel framework for hierarchical policy learning that significantly improves policy generalization and sample efficiency in robot manipulation. One core idea of GSL is to use object-centric skills as an interface that bridges the high-level vision-language model and the low-level visual-motor policy. Specifically, GSL decomposes demonstrations into transferable and object-canonicalized skill primitives using foundation models, ensuring efficient low-level skill learning in the object frame. At test time, the skill-object pairs predicted by the high-level agent are fed to the low-level module, where the inferred canonical actions are mapped back to the world frame for execution. This structured yet flexible design leads to substantial improvements in sample efficiency and generalization of our method across unseen spatial arrangements, object appearances, and task compositions. In simulation, GSL trained with only 3 demonstrations per task outperforms baselines trained with 30 times more data by 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses the baseline trained with 10 times more data.

