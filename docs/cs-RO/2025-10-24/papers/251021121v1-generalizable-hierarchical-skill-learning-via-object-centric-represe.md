---
layout: default
title: Generalizable Hierarchical Skill Learning via Object-Centric Representation
---

# Generalizable Hierarchical Skill Learning via Object-Centric Representation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21121" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21121v1</a>
  <a href="https://arxiv.org/pdf/2510.21121.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21121v1" onclick="toggleFavorite(this, '2510.21121v1', 'Generalizable Hierarchical Skill Learning via Object-Centric Representation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Haibo Zhao, Yu Qi, Boce Hu, Yizhe Zhu, Ziyan Chen, Heng Tian, Xupeng Zhu, Owen Howell, Haojie Huang, Robin Walters, Dian Wang, Robert Platt

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„é€šç”¨åˆ†å±‚æŠ€èƒ½å­¦ä¹ æ¡†æ¶ï¼Œæå‡æœºå™¨äººæ“ä½œæ³›åŒ–æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åˆ†å±‚å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `å¯¹è±¡ä¸­å¿ƒè¡¨ç¤º` `æŠ€èƒ½å­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•åœ¨æ³›åŒ–æ€§å’Œæ ·æœ¬æ•ˆç‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥é€‚åº”æ–°çš„ç¯å¢ƒå’Œä»»åŠ¡ã€‚
2. GSLåˆ©ç”¨å¯¹è±¡ä¸­å¿ƒæŠ€èƒ½ä½œä¸ºæ¡¥æ¢ï¼Œè¿æ¥é«˜å±‚è§†è§‰-è¯­è¨€æ¨¡å‹å’Œä½å±‚è§†è§‰-è¿åŠ¨ç­–ç•¥ï¼Œå®ç°æŠ€èƒ½çš„è§£è€¦å’Œæ³›åŒ–ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGSLåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä»…éœ€å°‘é‡æ ·æœ¬å³å¯å®ç°è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé€šç”¨åˆ†å±‚æŠ€èƒ½å­¦ä¹ ï¼ˆGSLï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºåˆ†å±‚ç­–ç•¥å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†æœºå™¨äººæ“ä½œä¸­çš„ç­–ç•¥æ³›åŒ–æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚GSLçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨å¯¹è±¡ä¸­å¿ƒæŠ€èƒ½ä½œä¸ºè¿æ¥é«˜å±‚è§†è§‰-è¯­è¨€æ¨¡å‹å’Œä½å±‚è§†è§‰-è¿åŠ¨ç­–ç•¥çš„æ¥å£ã€‚å…·ä½“è€Œè¨€ï¼ŒGSLåˆ©ç”¨åŸºç¡€æ¨¡å‹å°†æ¼”ç¤ºåˆ†è§£ä¸ºå¯è½¬ç§»çš„ã€å¯¹è±¡è§„èŒƒåŒ–çš„æŠ€èƒ½åŸè¯­ï¼Œä»è€Œç¡®ä¿åœ¨å¯¹è±¡åæ ‡ç³»ä¸­è¿›è¡Œé«˜æ•ˆçš„ä½å±‚æŠ€èƒ½å­¦ä¹ ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œé«˜å±‚æ™ºèƒ½ä½“é¢„æµ‹çš„æŠ€èƒ½-å¯¹è±¡å¯¹è¢«è¾“å…¥åˆ°ä½å±‚æ¨¡å—ï¼Œæ¨æ–­å‡ºçš„è§„èŒƒåŠ¨ä½œè¢«æ˜ å°„å›ä¸–ç•Œåæ ‡ç³»ä»¥ä¾›æ‰§è¡Œã€‚è¿™ç§ç»“æ„åŒ–ä½†çµæ´»çš„è®¾è®¡æ˜¾è‘—æé«˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨æœªè§è¿‡çš„ç©ºé—´æ’åˆ—ã€å¯¹è±¡å¤–è§‚å’Œä»»åŠ¡ç»„åˆä¸­çš„æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼ŒGSLä»…ç”¨æ¯ä¸ªä»»åŠ¡3ä¸ªæ¼”ç¤ºè¿›è¡Œè®­ç»ƒï¼Œåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šä¼˜äºä½¿ç”¨30å€æ•°æ®è®­ç»ƒçš„åŸºçº¿15.5%ã€‚åœ¨çœŸå®ä¸–ç•Œçš„å®éªŒä¸­ï¼ŒGSLä¹Ÿè¶…è¿‡äº†ä½¿ç”¨10å€æ•°æ®è®­ç»ƒçš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•åœ¨é¢å¯¹æ–°çš„ç©ºé—´æ’åˆ—ã€å¯¹è±¡å¤–è§‚å’Œä»»åŠ¡ç»„åˆæ—¶ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå¼±ï¼Œä¸”é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚å¦‚ä½•æé«˜æœºå™¨äººæ“ä½œç­–ç•¥çš„æ³›åŒ–æ€§å’Œæ ·æœ¬æ•ˆç‡æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—å¯å¤ç”¨çš„ã€å¯¹è±¡ä¸­å¿ƒåŒ–çš„æŠ€èƒ½åŸè¯­ã€‚é€šè¿‡åœ¨å¯¹è±¡åæ ‡ç³»ä¸‹å­¦ä¹ è¿™äº›æŠ€èƒ½ï¼Œå¯ä»¥ä½¿å…¶å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œé€‚åº”ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ã€‚åŒæ—¶ï¼Œåˆ©ç”¨é«˜å±‚è§†è§‰-è¯­è¨€æ¨¡å‹æ¥æŒ‡å¯¼æŠ€èƒ½çš„é€‰æ‹©å’Œç»„åˆï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜ç­–ç•¥çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGSLæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé«˜å±‚ç­–ç•¥å’Œä½å±‚ç­–ç•¥ã€‚é«˜å±‚ç­–ç•¥è´Ÿè´£æ ¹æ®å½“å‰ç¯å¢ƒå’Œä»»åŠ¡ç›®æ ‡ï¼Œé€‰æ‹©åˆé€‚çš„æŠ€èƒ½å’Œå¯¹è±¡ã€‚ä½å±‚ç­–ç•¥åˆ™è´Ÿè´£æ‰§è¡Œé€‰å®šçš„æŠ€èƒ½ï¼Œå°†å¯¹è±¡åæ ‡ç³»ä¸‹çš„åŠ¨ä½œæ˜ å°„å›ä¸–ç•Œåæ ‡ç³»ï¼Œå¹¶æ§åˆ¶æœºå™¨äººå®Œæˆç›¸åº”çš„æ“ä½œã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡å¯¹è±¡ä¸­å¿ƒæŠ€èƒ½ä½œä¸ºæ¥å£ï¼Œè¿æ¥é«˜å±‚å’Œä½å±‚ç­–ç•¥ï¼Œå®ç°åˆ†å±‚æ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šGSLçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºæ¥å­¦ä¹ æŠ€èƒ½ã€‚é€šè¿‡å°†æŠ€èƒ½å®šä¹‰åœ¨å¯¹è±¡åæ ‡ç³»ä¸‹ï¼Œå¯ä»¥æ¶ˆé™¤å¯¹è±¡å§¿æ€å’Œç¯å¢ƒå˜åŒ–çš„å½±å“ï¼Œä»è€Œæé«˜æŠ€èƒ½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGSLè¿˜åˆ©ç”¨åŸºç¡€æ¨¡å‹æ¥åˆ†è§£æ¼”ç¤ºæ•°æ®ï¼Œæå–å¯è½¬ç§»çš„æŠ€èƒ½åŸè¯­ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šGSLä½¿ç”¨Transformerç½‘ç»œä½œä¸ºé«˜å±‚ç­–ç•¥ï¼Œé¢„æµ‹æŠ€èƒ½å’Œå¯¹è±¡ã€‚ä½å±‚ç­–ç•¥ä½¿ç”¨ç¥ç»ç½‘ç»œå­¦ä¹ å¯¹è±¡åæ ‡ç³»ä¸‹çš„åŠ¨ä½œã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æŠ€èƒ½åˆ†ç±»æŸå¤±ã€åŠ¨ä½œå›å½’æŸå¤±å’Œæ¨¡ä»¿å­¦ä¹ æŸå¤±ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨æ¨¡æ‹Ÿå®éªŒä¸­ï¼ŒGSLä»…ä½¿ç”¨æ¯ä¸ªä»»åŠ¡3ä¸ªæ¼”ç¤ºè¿›è¡Œè®­ç»ƒï¼Œåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šä¼˜äºä½¿ç”¨30å€æ•°æ®è®­ç»ƒçš„åŸºçº¿15.5%ã€‚åœ¨çœŸå®ä¸–ç•Œçš„å®éªŒä¸­ï¼ŒGSLä¹Ÿè¶…è¿‡äº†ä½¿ç”¨10å€æ•°æ®è®­ç»ƒçš„åŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒGSLåœ¨æ³›åŒ–æ€§å’Œæ ·æœ¬æ•ˆç‡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ï¼Œå¯ä»¥é™ä½æœºå™¨äººçš„éƒ¨ç½²æˆæœ¬ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒï¼Œå®Œæˆå„ç§ä»»åŠ¡ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡å’Œåœºæ™¯ï¼Œä¾‹å¦‚å¤šæœºå™¨äººåä½œã€äººæœºåä½œç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present Generalizable Hierarchical Skill Learning (GSL), a novel framework for hierarchical policy learning that significantly improves policy generalization and sample efficiency in robot manipulation. One core idea of GSL is to use object-centric skills as an interface that bridges the high-level vision-language model and the low-level visual-motor policy. Specifically, GSL decomposes demonstrations into transferable and object-canonicalized skill primitives using foundation models, ensuring efficient low-level skill learning in the object frame. At test time, the skill-object pairs predicted by the high-level agent are fed to the low-level module, where the inferred canonical actions are mapped back to the world frame for execution. This structured yet flexible design leads to substantial improvements in sample efficiency and generalization of our method across unseen spatial arrangements, object appearances, and task compositions. In simulation, GSL trained with only 3 demonstrations per task outperforms baselines trained with 30 times more data by 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses the baseline trained with 10 times more data.

