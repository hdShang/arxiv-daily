---
layout: default
title: DREAMer-VXS: A Latent World Model for Sample-Efficient AGV Exploration in Stochastic, Unobserved Environments
---

# DREAMer-VXS: A Latent World Model for Sample-Efficient AGV Exploration in Stochastic, Unobserved Environments

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00005" class="toolbar-btn" target="_blank">üìÑ arXiv: 2512.00005v1</a>
  <a href="https://arxiv.org/pdf/2512.00005.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00005v1" onclick="toggleFavorite(this, '2512.00005v1', 'DREAMer-VXS: A Latent World Model for Sample-Efficient AGV Exploration in Stochastic, Unobserved Environments')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Agniprabha Chakraborty

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DREAMer-VXS‰ª•Ëß£ÂÜ≥AGVÂú®ÈöèÊú∫Êú™Áü•ÁéØÂ¢É‰∏≠ÁöÑÊ†∑Êú¨ÊïàÁéáÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ëá™‰∏ªÈ©æÈ©∂` `Âº∫ÂåñÂ≠¶‰π†` `Ê®°ÂûãÂ≠¶‰π†` `Êé¢Á¥¢ÊïàÁéá` `Êú∫Âô®‰∫∫ÊäÄÊúØ` `Âä®ÊÄÅÁéØÂ¢É` `Ê†∑Êú¨ÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÂú®Ê†∑Êú¨ÊïàÁéáÂíåÈÄÇÂ∫îÊÄßÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. DREAMer-VXSÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™Âü∫‰∫éÈÉ®ÂàÜLiDARËßÇÊµãÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåÂà©Áî®ÊÉ≥Ë±°‰∏≠ÁöÑËΩ®ËøπËøõË°åÈ´òÊïàÁöÑÁ≠ñÁï•Â≠¶‰π†ÔºåÂáèÂ∞ë‰∫ÜÂØπÁúüÂÆûÁéØÂ¢É‰∫§‰∫íÁöÑ‰æùËµñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êú™Áü•ÁéØÂ¢É‰∏≠ÁöÑÊé¢Á¥¢ÊïàÁéáÊèêÈ´ò‰∫Ü45%ÔºåÂπ∂‰∏îÂú®Âä®ÊÄÅÈöúÁ¢çÁâ©Èù¢ÂâçË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â≠¶‰π†ÂûãÊú∫Âô®‰∫∫È¢ÜÂüüÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩúÂäõÔºå‰ΩÜ‰º†ÁªüÁöÑÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÂú®Ê†∑Êú¨ÊïàÁéáÂíåËÑÜÂº±ÊÄßÊñπÈù¢Â≠òÂú®‰∏•ÈáçÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜDREAMer-VXSÔºå‰∏Ä‰∏™Âü∫‰∫éÊ®°ÂûãÁöÑËá™‰∏ªÂú∞Èù¢ËΩ¶ËæÜÔºàAGVÔºâÊé¢Á¥¢Ê°ÜÊû∂ÔºåÈÄöËøáÊÉ≥Ë±°ÊΩúÂú®ËΩ®ËøπËøõË°åËßÑÂàí„ÄÇËØ•ÊñπÊ≥ï‰ªéÈÉ®ÂàÜÈ´òÁª¥LiDARËßÇÊµã‰∏≠Â≠¶‰π†ÂÖ®Èù¢ÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåÁªìÂêàÂç∑ÁßØÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÂíåÈÄíÂΩíÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàRSSMÔºâÔºåÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑÂØºËà™Á≠ñÁï•ËÆ≠ÁªÉ„ÄÇ‰∏éÊúÄÂÖàËøõÁöÑÊó†Ê®°ÂûãSACÂü∫Á∫øÁõ∏ÊØîÔºåDREAMer-VXSÂú®ËææÂà∞‰∏ìÂÆ∂Á∫ßÊÄßËÉΩÊó∂ÂáèÂ∞ë‰∫Ü90%ÁöÑÁéØÂ¢É‰∫§‰∫íÈúÄÊ±ÇÔºåÂπ∂Âú®Êú™Áü•ÁéØÂ¢É‰∏≠ÊèêÈ´ò‰∫Ü45%ÁöÑÊé¢Á¥¢ÊïàÁéáÔºåÂ±ïÁé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Ëá™‰∏ªÂú∞Èù¢ËΩ¶ËæÜÔºàAGVÔºâÂú®ÈöèÊú∫ÂíåÊú™Áü•ÁéØÂ¢É‰∏≠ËøõË°åÊé¢Á¥¢Êó∂ÁöÑÊ†∑Êú¨ÊïàÁéá‰Ωé‰∏ãÂíåËÑÜÂº±ÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂæÄÂæÄÈúÄË¶ÅÂ§ßÈáèÁöÑÁéØÂ¢É‰∫§‰∫íÔºåÂØºËá¥Â≠¶‰π†ËøáÁ®ãÁºìÊÖ¢‰∏î‰∏çÁ®≥ÂÆö„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöDREAMer-VXSÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™‰∏ñÁïåÊ®°ÂûãÔºå‰ªéÈÉ®ÂàÜÈ´òÁª¥LiDARËßÇÊµã‰∏≠Â≠¶‰π†ÁéØÂ¢ÉÁöÑÁªìÊûÑÔºåÂπ∂Âà©Áî®ËØ•Ê®°ÂûãËøõË°åÊÉ≥Ë±°‰∏≠ÁöÑËΩ®ËøπËßÑÂàíÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÁöÑÁ≠ñÁï•Â≠¶‰π†„ÄÇËøôÊ†∑ËÆæËÆ°ÁöÑÁõÆÁöÑÊòØÂ∞ÜÁ≠ñÁï•Â≠¶‰π†‰∏éÁúüÂÆûÁéØÂ¢É‰∫§‰∫íËß£ËÄ¶ÔºåÊòæËëóÊèêÈ´òÊ†∑Êú¨ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂç∑ÁßØÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÁî®‰∫éÂ≠¶‰π†ÁéØÂ¢ÉÁöÑÁ¥ßÂáëË°®Á§∫ÔºåÈÄíÂΩíÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàRSSMÔºâÁî®‰∫éÂª∫Ê®°Â§çÊùÇÁöÑÊó∂Èó¥Âä®ÊÄÅ„ÄÇÈÄöËøáÂ∞ÜÂ≠¶‰π†Âà∞ÁöÑÊ®°Âûã‰Ωú‰∏∫È´òÊïàÁöÑÊ®°ÊãüÂô®Ôºå‰ª£ÁêÜÂèØ‰ª•Âú®ÊÉ≥Ë±°‰∏≠ËÆ≠ÁªÉÂÖ∂ÂØºËà™Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDREAMer-VXSÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÊ®°ÂûãÂ≠¶‰π†‰∏éÁ≠ñÁï•‰ºòÂåñÁõ∏ÁªìÂêàÔºåÈÄöËøáÊÉ≥Ë±°ËΩ®ËøπËøõË°åËÆ≠ÁªÉÔºåÂáèÂ∞ë‰∫ÜÂØπÁúüÂÆûÁéØÂ¢É‰∫§‰∫íÁöÑÈúÄÊ±Ç„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÊó†Ê®°ÂûãÊñπÊ≥ïÊú¨Ë¥®‰∏ä‰∏çÂêåÔºåÂêéËÄÖ‰æùËµñ‰∫éÂ§ßÈáèÁöÑÂÆûÈôÖ‰∫§‰∫íÊù•Â≠¶‰π†ÊúâÊïàÁöÑÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊäÄÊúØÁªÜËäÇÊñπÈù¢ÔºåVAEÂíåRSSMÁöÑÁΩëÁªúÁªìÊûÑÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•Á°Æ‰øùËÉΩÂ§üÊúâÊïàÊçïÊçâÁéØÂ¢ÉÁöÑÂ§çÊùÇÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºå‰ª£ÁêÜÁöÑË°å‰∏∫Áî±‰∏Ä‰∏™‰ºòÂåñÁöÑÊºîÂëò-ËØÑËÆ∫ÂÆ∂Á≠ñÁï•ÂºïÂØºÔºåËØ•Á≠ñÁï•‰ΩøÁî®Â§çÂêàÂ•ñÂä±ÂáΩÊï∞ÔºåÂπ≥Ë°°‰ªªÂä°ÁõÆÊ†á‰∏éÂÜÖÂú®Â•ΩÂ•áÂøÉÂ•ñÂä±Ôºå‰øÉËøõÂØπÊú™Áü•Á©∫Èó¥ÁöÑÁ≥ªÁªüÊé¢Á¥¢„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDREAMer-VXSÂú®ÂÆûÁé∞‰∏ìÂÆ∂Á∫ßÊÄßËÉΩÊó∂ÔºåÁéØÂ¢É‰∫§‰∫íÈúÄÊ±ÇÂáèÂ∞ë‰∫Ü90%„ÄÇÊ≠§Â§ñÔºåÂú®Êú™Áü•ÁéØÂ¢É‰∏≠ÁöÑÊé¢Á¥¢ÊïàÁéáÊèêÈ´ò‰∫Ü45%ÔºåÂπ∂‰∏îÂú®Èù¢ÂØπÂä®ÊÄÅÈöúÁ¢çÁâ©Êó∂Â±ïÁé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÔºåÊòæËëó‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊó†Ê®°ÂûãSACÂü∫Á∫ø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

DREAMer-VXSÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Ëá™‰∏ªÈ©æÈ©∂„ÄÅÊú∫Âô®‰∫∫Êé¢Á¥¢ÂíåÊô∫ËÉΩÁâ©ÊµÅÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÈ´òÊ†∑Êú¨ÊïàÁéáÂíåÊé¢Á¥¢ËÉΩÂäõÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÂä†ÈÄüÊú∫Âô®‰∫∫Âú®Â§çÊùÇÂíåÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂ≠¶‰π†ËøáÁ®ãÔºåÊèêÂçáÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑË°®Áé∞ÂíåÂèØÈù†ÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØËÉΩÊé®Âä®Êõ¥Êô∫ËÉΩÁöÑËá™Âä®ÂåñÁ≥ªÁªüÁöÑÂèëÂ±ïÔºåÊîπÂñÑ‰∫∫Êú∫Âçè‰ΩúÂíåËá™‰∏ªÂÜ≥Á≠ñËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The paradigm of learning-based robotics holds immense promise, yet its translation to real-world applications is critically hindered by the sample inefficiency and brittleness of conventional model-free reinforcement learning algorithms. In this work, we address these challenges by introducing DREAMer-VXS, a model-based framework for Autonomous Ground Vehicle (AGV) exploration that learns to plan from imagined latent trajectories. Our approach centers on learning a comprehensive world model from partial and high-dimensional LiDAR observations. This world model is composed of a Convolutional Variational Autoencoder (VAE), which learns a compact representation of the environment's structure, and a Recurrent State-Space Model (RSSM), which models complex temporal dynamics. By leveraging this learned model as a high-speed simulator, the agent can train its navigation policy almost entirely in imagination. This methodology decouples policy learning from real-world interaction, culminating in a 90% reduction in required environmental interactions to achieve expert-level performance when compared to state-of-the-art model-free SAC baselines. The agent's behavior is guided by an actor-critic policy optimized with a composite reward function that balances task objectives with an intrinsic curiosity bonus, promoting systematic exploration of unknown spaces. We demonstrate through extensive simulated experiments that DREAMer-VXS not only learns orders of magnitude faster but also develops more generalizable and robust policies, achieving a 45% increase in exploration efficiency in unseen environments and superior resilience to dynamic obstacles.

