---
layout: default
title: HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks
---

# HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.04898" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.04898v1</a>
  <a href="https://arxiv.org/pdf/2510.04898.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04898v1" onclick="toggleFavorite(this, '2510.04898v1', 'HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-06

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/MasterXiong/HyperVLA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HyperVLAï¼šé€šè¿‡è¶…ç½‘ç»œå®ç°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é«˜æ•ˆæ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `è¶…ç½‘ç»œ` `æœºå™¨äººæ§åˆ¶` `é«˜æ•ˆæ¨ç†` `é›¶æ ·æœ¬æ³›åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(VLA)æ¨ç†æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. HyperVLAé‡‡ç”¨è¶…ç½‘ç»œæ¶æ„ï¼Œä»…æ¿€æ´»ä»»åŠ¡ç›¸å…³çš„å­ç­–ç•¥ï¼Œé™ä½æ¨ç†è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹å®¹é‡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHyperVLAåœ¨ä¿æŒæˆ–æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ï¼Œå‚æ•°é‡å‡å°‘90å€ï¼Œé€Ÿåº¦æå‡120å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹å»ºç«‹åœ¨å…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„è¯­è¨€å’Œè§†è§‰åŸºç¡€æ¨¡å‹ä¹‹ä¸Šï¼Œå¹¶åœ¨å¤§è§„æ¨¡æœºå™¨äººæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæœ€è¿‘å·²æˆä¸ºå­¦ä¹ é€šç”¨æœºå™¨äººç­–ç•¥çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰VLAçš„ä¸€ä¸ªå…³é”®ç¼ºç‚¹æ˜¯å…¶æé«˜çš„æ¨ç†æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†HyperVLAæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ¿€æ´»æ•´ä¸ªæ¨¡å‹çš„ç°æœ‰å•ä½“VLAä¸åŒï¼ŒHyperVLAä½¿ç”¨ä¸€ç§æ–°é¢–çš„åŸºäºè¶…ç½‘ç»œ(HN)çš„æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…æ¿€æ´»ä¸€ä¸ªå°çš„ç‰¹å®šäºä»»åŠ¡çš„ç­–ç•¥ï¼ŒåŒæ—¶ä»ç„¶ä¿ç•™å®¹çº³å„ç§å¤šä»»åŠ¡è¡Œä¸ºæ‰€éœ€çš„é«˜æ¨¡å‹å®¹é‡ã€‚æˆåŠŸè®­ç»ƒåŸºäºHNçš„VLAå¹¶éæ˜“äº‹ï¼Œå› æ­¤HyperVLAåŒ…å«å‡ ä¸ªå…³é”®çš„ç®—æ³•è®¾è®¡ç‰¹æ€§ï¼Œä»¥æé«˜å…¶æ€§èƒ½ï¼ŒåŒ…æ‹¬æ­£ç¡®åˆ©ç”¨æ¥è‡ªç°æœ‰è§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€HNå½’ä¸€åŒ–å’ŒåŠ¨ä½œç”Ÿæˆç­–ç•¥ã€‚ä¸å•ä½“VLAç›¸æ¯”ï¼ŒHyperVLAåœ¨é›¶æ ·æœ¬æ³›åŒ–å’Œå°‘æ ·æœ¬è‡ªé€‚åº”æ–¹é¢å®ç°äº†ç›¸ä¼¼ç”šè‡³æ›´é«˜çš„æˆåŠŸç‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ã€‚ä¸æœ€å…ˆè¿›çš„VLAæ¨¡å‹OpenVLAç›¸æ¯”ï¼ŒHyperVLAåœ¨æµ‹è¯•æ—¶å‡å°‘äº†90å€çš„æ¿€æ´»å‚æ•°æ•°é‡ï¼Œå¹¶å°†æ¨ç†é€Ÿåº¦æé«˜äº†120å€ã€‚ä»£ç å·²åœ¨https://github.com/MasterXiong/HyperVLAä¸Šå…¬å¼€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨æ¨ç†æ—¶éœ€è¦æ¿€æ´»æ•´ä¸ªæ¨¡å‹ï¼Œè®¡ç®—é‡å·¨å¤§ï¼Œéš¾ä»¥éƒ¨ç½²åˆ°èµ„æºæœ‰é™çš„æœºå™¨äººå¹³å°ä¸Šã€‚ç—›ç‚¹åœ¨äºå¦‚ä½•åœ¨ä¿æŒæ¨¡å‹å®¹é‡å’Œæ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œé™ä½æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å¤æ‚åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHyperVLAçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è¶…ç½‘ç»œ(Hypernetwork)ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„å­ç½‘ç»œï¼Œåœ¨æ¨ç†æ—¶åªæ¿€æ´»è¯¥å­ç½‘ç»œï¼Œä»è€Œå¤§å¹…å‡å°‘è®¡ç®—é‡ã€‚è¶…ç½‘ç»œå­¦ä¹ å¦‚ä½•æ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆåˆé€‚çš„æƒé‡ï¼Œå®ç°é«˜æ•ˆçš„æ¡ä»¶è®¡ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHyperVLAåŒ…å«ä¸€ä¸ªè¶…ç½‘ç»œå’Œä¸€ä¸ªä¸»ç½‘ç»œã€‚è¶…ç½‘ç»œæ¥æ”¶ä»»åŠ¡æè¿°ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆä¸»ç½‘ç»œä¸­éƒ¨åˆ†å‚æ•°çš„æƒé‡ã€‚ä¸»ç½‘ç»œæ˜¯ä¸€ä¸ªVLAæ¨¡å‹ï¼Œä½†åªæœ‰è¢«è¶…ç½‘ç»œæ¿€æ´»çš„éƒ¨åˆ†å‚ä¸æ¨ç†ã€‚è®­ç»ƒæ—¶ï¼Œè¶…ç½‘ç»œå­¦ä¹ å¦‚ä½•æ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆåˆé€‚çš„æƒé‡ï¼Œä½¿å¾—æ¿€æ´»çš„å­ç½‘ç»œèƒ½å¤Ÿå®Œæˆä»»åŠ¡ã€‚æ¨ç†æ—¶ï¼Œåªæ¿€æ´»è¶…ç½‘ç»œå’Œå¯¹åº”çš„å­ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šHyperVLAçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨è¶…ç½‘ç»œåŠ¨æ€ç”Ÿæˆä»»åŠ¡ç›¸å…³çš„å­ç½‘ç»œï¼Œä»è€Œåœ¨æ¨ç†æ—¶åªæ¿€æ´»éƒ¨åˆ†å‚æ•°ã€‚è¿™ä¸ä¼ ç»Ÿçš„å•ä½“VLAæ¨¡å‹å½¢æˆå¯¹æ¯”ï¼Œåè€…åœ¨æ¨ç†æ—¶éœ€è¦æ¿€æ´»æ•´ä¸ªæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†é’ˆå¯¹è¶…ç½‘ç»œè®­ç»ƒçš„ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€è¶…ç½‘ç»œå½’ä¸€åŒ–å’ŒåŠ¨ä½œç”Ÿæˆç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šHyperVLAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹åˆå§‹åŒ–ä¸»ç½‘ç»œï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼›2) å¯¹è¶…ç½‘ç»œè¿›è¡Œå½’ä¸€åŒ–ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼›3) è®¾è®¡äº†ä¸€ç§åŠ¨ä½œç”Ÿæˆç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„åŠ¨ä½œæ˜¯æœ‰æ•ˆçš„ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ æŸå¤±å’Œæ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºçº¦æŸè¶…ç½‘ç»œçš„è¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HyperVLAåœ¨é›¶æ ·æœ¬æ³›åŒ–å’Œå°‘æ ·æœ¬è‡ªé€‚åº”ä»»åŠ¡ä¸Šå–å¾—äº†ä¸å•ä½“VLAæ¨¡å‹ç›¸å½“ç”šè‡³æ›´é«˜çš„æˆåŠŸç‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ã€‚ä¸OpenVLAç›¸æ¯”ï¼ŒHyperVLAåœ¨æµ‹è¯•æ—¶å‡å°‘äº†90å€çš„æ¿€æ´»å‚æ•°æ•°é‡ï¼Œå¹¶å°†æ¨ç†é€Ÿåº¦æé«˜äº†120å€ï¼ŒéªŒè¯äº†å…¶é«˜æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HyperVLAé€‚ç”¨äºå„ç§æœºå™¨äººä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºå—é™çš„åœºæ™¯ä¸‹ï¼Œä¾‹å¦‚ç§»åŠ¨æœºå™¨äººã€æ— äººæœºç­‰ã€‚å®ƒå¯ä»¥åº”ç”¨äºå®¶åº­æœåŠ¡ã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€ç¾å®³æ•‘æ´ç­‰é¢†åŸŸï¼Œå®ç°æ›´é«˜æ•ˆã€æ›´çµæ´»çš„æœºå™¨äººæ§åˆ¶ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘ä½åŠŸè€—ã€é«˜æ€§èƒ½çš„æœºå™¨äººç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\times$, and accelerates inference speed by $120\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA

