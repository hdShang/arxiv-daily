---
layout: default
title: HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks
---

# HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.04898" target="_blank" class="toolbar-btn">arXiv: 2510.04898v1</a>
    <a href="https://arxiv.org/pdf/2510.04898.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.04898v1" 
            onclick="toggleFavorite(this, '2510.04898v1', 'HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-06

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/MasterXiong/HyperVLA)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**HyperVLAÔºöÈÄöËøáË∂ÖÁΩëÁªúÂÆûÁé∞ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÈ´òÊïàÊé®ÁêÜ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Ë∂ÖÁΩëÁªú` `Êú∫Âô®‰∫∫ÊéßÂà∂` `È´òÊïàÊé®ÁêÜ` `Èõ∂Ê†∑Êú¨Ê≥õÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã(VLA)Êé®ÁêÜÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®ËµÑÊ∫êÂèóÈôêÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. HyperVLAÈááÁî®Ë∂ÖÁΩëÁªúÊû∂ÊûÑÔºå‰ªÖÊøÄÊ¥ª‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂ≠êÁ≠ñÁï•ÔºåÈôç‰ΩéÊé®ÁêÜËÆ°ÁÆóÈáèÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãÂÆπÈáè„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåHyperVLAÂú®‰øùÊåÅÊàñÊèêÂçáÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÊé®ÁêÜÊàêÊú¨ÔºåÂèÇÊï∞ÈáèÂáèÂ∞ë90ÂÄçÔºåÈÄüÂ∫¶ÊèêÂçá120ÂÄç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂª∫Á´ãÂú®ÂÖ∑ÊúâÂº∫Â§ßÊ≥õÂåñËÉΩÂäõÁöÑËØ≠Ë®ÄÂíåËßÜËßâÂü∫Á°ÄÊ®°Âûã‰πã‰∏äÔºåÂπ∂Âú®Â§ßËßÑÊ®°Êú∫Âô®‰∫∫Êï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºåÊúÄËøëÂ∑≤Êàê‰∏∫Â≠¶‰π†ÈÄöÁî®Êú∫Âô®‰∫∫Á≠ñÁï•ÁöÑ‰∏ÄÁßçÊúâÂâçÈÄîÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâVLAÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆÁº∫ÁÇπÊòØÂÖ∂ÊûÅÈ´òÁöÑÊé®ÁêÜÊàêÊú¨„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜHyperVLAÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ‰∏éÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠ÊøÄÊ¥ªÊï¥‰∏™Ê®°ÂûãÁöÑÁé∞ÊúâÂçï‰ΩìVLA‰∏çÂêåÔºåHyperVLA‰ΩøÁî®‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éË∂ÖÁΩëÁªú(HN)ÁöÑÊû∂ÊûÑÔºåËØ•Êû∂ÊûÑÂú®Êé®ÁêÜËøáÁ®ã‰∏≠‰ªÖÊøÄÊ¥ª‰∏Ä‰∏™Â∞èÁöÑÁâπÂÆö‰∫é‰ªªÂä°ÁöÑÁ≠ñÁï•ÔºåÂêåÊó∂‰ªçÁÑ∂‰øùÁïôÂÆπÁ∫≥ÂêÑÁßçÂ§ö‰ªªÂä°Ë°å‰∏∫ÊâÄÈúÄÁöÑÈ´òÊ®°ÂûãÂÆπÈáè„ÄÇÊàêÂäüËÆ≠ÁªÉÂü∫‰∫éHNÁöÑVLAÂπ∂ÈùûÊòì‰∫ãÔºåÂõ†Ê≠§HyperVLAÂåÖÂê´Âá†‰∏™ÂÖ≥ÈîÆÁöÑÁÆóÊ≥ïËÆæËÆ°ÁâπÊÄßÔºå‰ª•ÊèêÈ´òÂÖ∂ÊÄßËÉΩÔºåÂåÖÊã¨Ê≠£Á°ÆÂà©Áî®Êù•Ëá™Áé∞ÊúâËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜ„ÄÅHNÂΩí‰∏ÄÂåñÂíåÂä®‰ΩúÁîüÊàêÁ≠ñÁï•„ÄÇ‰∏éÂçï‰ΩìVLAÁõ∏ÊØîÔºåHyperVLAÂú®Èõ∂Ê†∑Êú¨Ê≥õÂåñÂíåÂ∞ëÊ†∑Êú¨Ëá™ÈÄÇÂ∫îÊñπÈù¢ÂÆûÁé∞‰∫ÜÁõ∏‰ººÁîöËá≥Êõ¥È´òÁöÑÊàêÂäüÁéáÔºåÂêåÊó∂ÊòæËëóÈôç‰Ωé‰∫ÜÊé®ÁêÜÊàêÊú¨„ÄÇ‰∏éÊúÄÂÖàËøõÁöÑVLAÊ®°ÂûãOpenVLAÁõ∏ÊØîÔºåHyperVLAÂú®ÊµãËØïÊó∂ÂáèÂ∞ë‰∫Ü90ÂÄçÁöÑÊøÄÊ¥ªÂèÇÊï∞Êï∞ÈáèÔºåÂπ∂Â∞ÜÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü120ÂÄç„ÄÇ‰ª£Á†ÅÂ∑≤Âú®https://github.com/MasterXiong/HyperVLA‰∏äÂÖ¨ÂºÄ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂú®Êé®ÁêÜÊó∂ÈúÄË¶ÅÊøÄÊ¥ªÊï¥‰∏™Ê®°ÂûãÔºåËÆ°ÁÆóÈáèÂ∑®Â§ßÔºåÈöæ‰ª•ÈÉ®ÁΩ≤Âà∞ËµÑÊ∫êÊúâÈôêÁöÑÊú∫Âô®‰∫∫Âπ≥Âè∞‰∏ä„ÄÇÁóõÁÇπÂú®‰∫éÂ¶Ç‰ΩïÂú®‰øùÊåÅÊ®°ÂûãÂÆπÈáèÂíåÊ≥õÂåñËÉΩÂäõÁöÑÂêåÊó∂ÔºåÈôç‰ΩéÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHyperVLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ë∂ÖÁΩëÁªú(Hypernetwork)ÁîüÊàêÁâπÂÆö‰ªªÂä°ÁöÑÂ≠êÁΩëÁªúÔºåÂú®Êé®ÁêÜÊó∂Âè™ÊøÄÊ¥ªËØ•Â≠êÁΩëÁªúÔºå‰ªéËÄåÂ§ßÂπÖÂáèÂ∞ëËÆ°ÁÆóÈáè„ÄÇË∂ÖÁΩëÁªúÂ≠¶‰π†Â¶Ç‰ΩïÊ†πÊçÆ‰ªªÂä°ÊèèËø∞ÁîüÊàêÂêàÈÄÇÁöÑÊùÉÈáçÔºåÂÆûÁé∞È´òÊïàÁöÑÊù°‰ª∂ËÆ°ÁÆó„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHyperVLAÂåÖÂê´‰∏Ä‰∏™Ë∂ÖÁΩëÁªúÂíå‰∏Ä‰∏™‰∏ªÁΩëÁªú„ÄÇË∂ÖÁΩëÁªúÊé•Êî∂‰ªªÂä°ÊèèËø∞‰Ωú‰∏∫ËæìÂÖ•ÔºåÁîüÊàê‰∏ªÁΩëÁªú‰∏≠ÈÉ®ÂàÜÂèÇÊï∞ÁöÑÊùÉÈáç„ÄÇ‰∏ªÁΩëÁªúÊòØ‰∏Ä‰∏™VLAÊ®°ÂûãÔºå‰ΩÜÂè™ÊúâË¢´Ë∂ÖÁΩëÁªúÊøÄÊ¥ªÁöÑÈÉ®ÂàÜÂèÇ‰∏éÊé®ÁêÜ„ÄÇËÆ≠ÁªÉÊó∂ÔºåË∂ÖÁΩëÁªúÂ≠¶‰π†Â¶Ç‰ΩïÊ†πÊçÆ‰ªªÂä°ÊèèËø∞ÁîüÊàêÂêàÈÄÇÁöÑÊùÉÈáçÔºå‰ΩøÂæóÊøÄÊ¥ªÁöÑÂ≠êÁΩëÁªúËÉΩÂ§üÂÆåÊàê‰ªªÂä°„ÄÇÊé®ÁêÜÊó∂ÔºåÂè™ÊøÄÊ¥ªË∂ÖÁΩëÁªúÂíåÂØπÂ∫îÁöÑÂ≠êÁΩëÁªú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHyperVLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫é‰ΩøÁî®Ë∂ÖÁΩëÁªúÂä®ÊÄÅÁîüÊàê‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂ≠êÁΩëÁªúÔºå‰ªéËÄåÂú®Êé®ÁêÜÊó∂Âè™ÊøÄÊ¥ªÈÉ®ÂàÜÂèÇÊï∞„ÄÇËøô‰∏é‰º†ÁªüÁöÑÂçï‰ΩìVLAÊ®°ÂûãÂΩ¢ÊàêÂØπÊØîÔºåÂêéËÄÖÂú®Êé®ÁêÜÊó∂ÈúÄË¶ÅÊøÄÊ¥ªÊï¥‰∏™Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫ÜÈíàÂØπË∂ÖÁΩëÁªúËÆ≠ÁªÉÁöÑ‰ºòÂåñÁ≠ñÁï•ÔºåÂåÖÊã¨Âà©Áî®ËßÜËßâÂü∫Á°ÄÊ®°ÂûãÁöÑÂÖàÈ™åÁü•ËØÜ„ÄÅË∂ÖÁΩëÁªúÂΩí‰∏ÄÂåñÂíåÂä®‰ΩúÁîüÊàêÁ≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöHyperVLAÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Âà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÂü∫Á°ÄÊ®°ÂûãÂàùÂßãÂåñ‰∏ªÁΩëÁªúÔºåÂä†ÈÄüËÆ≠ÁªÉËøáÁ®ãÔºõ2) ÂØπË∂ÖÁΩëÁªúËøõË°åÂΩí‰∏ÄÂåñÔºåÈò≤Ê≠¢Ê¢ØÂ∫¶Ê∂àÂ§±ÊàñÁàÜÁÇ∏Ôºõ3) ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂä®‰ΩúÁîüÊàêÁ≠ñÁï•ÔºåÁ°Æ‰øùÁîüÊàêÁöÑÂä®‰ΩúÊòØÊúâÊïàÁöÑ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨Ê®°‰ªøÂ≠¶‰π†ÊçüÂ§±ÂíåÊ≠£ÂàôÂåñÈ°πÔºåÁî®‰∫éÁ∫¶ÊùüË∂ÖÁΩëÁªúÁöÑËæìÂá∫„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

HyperVLAÂú®Èõ∂Ê†∑Êú¨Ê≥õÂåñÂíåÂ∞ëÊ†∑Êú¨Ëá™ÈÄÇÂ∫î‰ªªÂä°‰∏äÂèñÂæó‰∫Ü‰∏éÂçï‰ΩìVLAÊ®°ÂûãÁõ∏ÂΩìÁîöËá≥Êõ¥È´òÁöÑÊàêÂäüÁéáÔºåÂêåÊó∂ÊòæËëóÈôç‰Ωé‰∫ÜÊé®ÁêÜÊàêÊú¨„ÄÇ‰∏éOpenVLAÁõ∏ÊØîÔºåHyperVLAÂú®ÊµãËØïÊó∂ÂáèÂ∞ë‰∫Ü90ÂÄçÁöÑÊøÄÊ¥ªÂèÇÊï∞Êï∞ÈáèÔºåÂπ∂Â∞ÜÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü120ÂÄçÔºåÈ™åËØÅ‰∫ÜÂÖ∂È´òÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HyperVLAÈÄÇÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫‰ªªÂä°ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÆ°ÁÆóËµÑÊ∫êÂèóÈôêÁöÑÂú∫ÊôØ‰∏ãÔºå‰æãÂ¶ÇÁßªÂä®Êú∫Âô®‰∫∫„ÄÅÊó†‰∫∫Êú∫Á≠â„ÄÇÂÆÉÂèØ‰ª•Â∫îÁî®‰∫éÂÆ∂Â∫≠ÊúçÂä°„ÄÅÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÁÅæÂÆ≥ÊïëÊè¥Á≠âÈ¢ÜÂüüÔºåÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥ÁÅµÊ¥ªÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÇËØ•Á†îÁ©∂‰∏∫ÂºÄÂèë‰ΩéÂäüËÄó„ÄÅÈ´òÊÄßËÉΩÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\times$, and accelerates inference speed by $120\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA

