---
layout: default
title: TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos
---

# TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.21690" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.21690v1</a>
  <a href="https://arxiv.org/pdf/2511.21690.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.21690v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.21690v1', 'TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Seungjae Lee, Yoonkyo Jung, Inkook Chun, Yao-Chih Lee, Zikui Cai, Hongjia Huang, Aayush Talreja, Tan Dat Dao, Yongyuan Liang, Jia-Bin Huang, Furong Huang

**åˆ†ç±»**: cs.RO, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-26

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TraceGenï¼šé€šè¿‡3Dè½¨è¿¹ç©ºé—´çš„ä¸–ç•Œå»ºæ¨¡å®ç°è·¨å…·èº«è§†é¢‘å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººå­¦ä¹ ` `è·¨å…·èº«å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `è½¨è¿¹é¢„æµ‹` `è¿ç§»å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥åˆ©ç”¨æ¥è‡ªä¸åŒå…·èº«ï¼ˆäººç±»ã€å…¶ä»–æœºå™¨äººï¼‰çš„è§†é¢‘è¿›è¡Œæœºå™¨äººå­¦ä¹ ï¼Œå› ä¸ºå…·èº«ã€ç›¸æœºå’Œç¯å¢ƒçš„å·®å¼‚é˜»ç¢äº†ç›´æ¥ä½¿ç”¨ã€‚
2. TraceGené€šè¿‡å¼•å…¥3Dè½¨è¿¹ç©ºé—´ä½œä¸ºç»Ÿä¸€è¡¨ç¤ºï¼Œå°†å¼‚æ„è§†é¢‘è½¬æ¢ä¸ºä¸€è‡´çš„è½¨è¿¹ï¼Œä»è€ŒæŠ½è±¡æ‰å¤–è§‚å·®å¼‚ï¼Œä¿ç•™å‡ ä½•ç»“æ„ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTraceGenä»…éœ€å°‘é‡ç›®æ ‡æœºå™¨äººè§†é¢‘å³å¯å®ç°é«˜æ•ˆè¿ç§»å­¦ä¹ ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆåŠŸï¼Œæ¨ç†é€Ÿåº¦è¿œè¶…ç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•TraceGenï¼Œé€šè¿‡ç»Ÿä¸€çš„ç¬¦å·è¡¨ç¤ºâ€”â€”åœºæ™¯çº§è½¨è¿¹çš„ç´§å‡‘3Dâ€œè½¨è¿¹ç©ºé—´â€ï¼Œå®ç°ä»è·¨å…·èº«ã€è·¨ç¯å¢ƒå’Œè·¨ä»»åŠ¡è§†é¢‘ä¸­å­¦ä¹ æ–°çš„æœºå™¨äººä»»åŠ¡ã€‚TraceGenæ˜¯ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ï¼Œå®ƒåœ¨è½¨è¿¹ç©ºé—´è€Œéåƒç´ ç©ºé—´ä¸­é¢„æµ‹æœªæ¥è¿åŠ¨ï¼ŒæŠ½è±¡æ‰å¤–è§‚ï¼ŒåŒæ—¶ä¿ç•™æ“ä½œæ‰€éœ€çš„å‡ ä½•ç»“æ„ã€‚ä¸ºäº†å¤§è§„æ¨¡è®­ç»ƒTraceGenï¼Œå¼€å‘äº†TraceForgeæ•°æ®ç®¡é“ï¼Œå°†å¼‚æ„çš„äººç±»å’Œæœºå™¨äººè§†é¢‘è½¬æ¢ä¸ºä¸€è‡´çš„3Dè½¨è¿¹ï¼Œç”ŸæˆåŒ…å«12.3ä¸‡ä¸ªè§†é¢‘å’Œ180ä¸‡ä¸ªè§‚å¯Ÿ-è½¨è¿¹-è¯­è¨€ä¸‰å…ƒç»„çš„è¯­æ–™åº“ã€‚é¢„è®­ç»ƒçš„TraceGenäº§ç”Ÿäº†ä¸€ä¸ªå¯è¿ç§»çš„3Dè¿åŠ¨å…ˆéªŒï¼Œèƒ½å¤Ÿé«˜æ•ˆé€‚åº”ï¼šä»…ç”¨äº”ä¸ªç›®æ ‡æœºå™¨äººè§†é¢‘ï¼ŒTraceGenåœ¨å››ä¸ªä»»åŠ¡ä¸­è¾¾åˆ°80%çš„æˆåŠŸç‡ï¼ŒåŒæ—¶æä¾›æ¯”æœ€å…ˆè¿›çš„åŸºäºè§†é¢‘çš„ä¸–ç•Œæ¨¡å‹å¿«50-600å€çš„æ¨ç†é€Ÿåº¦ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ï¼Œä»…æœ‰äº”ä¸ªåœ¨æ‰‹æŒç”µè¯ä¸Šæ•è·çš„æœªæ ¡å‡†çš„äººç±»æ¼”ç¤ºè§†é¢‘å¯ç”¨æ—¶ï¼Œå®ƒä»ç„¶å¯ä»¥åœ¨çœŸå®æœºå™¨äººä¸Šè¾¾åˆ°67.5%çš„æˆåŠŸç‡ï¼Œçªæ˜¾äº†TraceGenåœ¨ä¸ä¾èµ–å¯¹è±¡æ£€æµ‹å™¨æˆ–ç¹é‡çš„åƒç´ ç©ºé—´ç”Ÿæˆçš„æƒ…å†µä¸‹è·¨å…·èº«é€‚åº”çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººå­¦ä¹ æ–¹æ³•éš¾ä»¥ç›´æ¥åˆ©ç”¨å¤§é‡å­˜åœ¨çš„è·¨å…·èº«ï¼ˆä¾‹å¦‚ï¼Œäººç±»å’Œä¸åŒæœºå™¨äººï¼‰è§†é¢‘æ•°æ®ã€‚è¿™æ˜¯å› ä¸ºä¸åŒå…·èº«ã€ç›¸æœºè§†è§’ä»¥åŠç¯å¢ƒçš„å·®å¼‚å¯¼è‡´äº†æ•°æ®åˆ†å¸ƒçš„å·¨å¤§å·®å¼‚ï¼Œä½¿å¾—ç›´æ¥çš„åƒç´ ç©ºé—´å­¦ä¹ å˜å¾—å›°éš¾ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡çš„ç›®æ ‡æœºå™¨äººæ•°æ®æˆ–å¤æ‚çš„åƒç´ çº§ç”Ÿæˆæ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTraceGençš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†é¢‘æ•°æ®æŠ•å½±åˆ°ä¸€ä¸ªç»Ÿä¸€çš„3Dè½¨è¿¹ç©ºé—´ä¸­ï¼Œä»è€ŒæŠ½è±¡æ‰å¤–è§‚ä¿¡æ¯ï¼Œä¿ç•™å…³é”®çš„å‡ ä½•ç»“æ„ã€‚é€šè¿‡åœ¨è¿™ä¸ªè½¨è¿¹ç©ºé—´ä¸­è¿›è¡Œè¿åŠ¨é¢„æµ‹ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸å…·èº«æ— å…³çš„è¿åŠ¨å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå®ç°è·¨å…·èº«çš„è¿ç§»å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç›´æ¥åœ¨åƒç´ ç©ºé—´è¿›è¡Œå­¦ä¹ ï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTraceGençš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šTraceForgeæ•°æ®ç®¡é“å’ŒTraceGenä¸–ç•Œæ¨¡å‹ã€‚TraceForgeè´Ÿè´£å°†å¼‚æ„çš„è§†é¢‘æ•°æ®è½¬æ¢ä¸º3Dè½¨è¿¹ï¼Œç”Ÿæˆå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†ã€‚TraceGenåˆ™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œå®ƒä»¥è½¨è¿¹åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹æœªæ¥çš„è½¨è¿¹ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€è½¨è¿¹æå–ã€æ¨¡å‹è®­ç»ƒå’Œè¿åŠ¨è§„åˆ’ç­‰æ­¥éª¤ã€‚

**å…³é”®åˆ›æ–°**ï¼šTraceGençš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†3Dè½¨è¿¹ç©ºé—´ä½œä¸ºç»Ÿä¸€çš„è¡¨ç¤ºå½¢å¼ï¼Œä»¥åŠTraceForgeæ•°æ®ç®¡é“ç”¨äºç”Ÿæˆå¤§è§„æ¨¡çš„è·¨å…·èº«è®­ç»ƒæ•°æ®ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTraceGenä¸éœ€è¦ä¾èµ–äºå¯¹è±¡æ£€æµ‹å™¨æˆ–å¤æ‚çš„åƒç´ ç©ºé—´ç”Ÿæˆæ¨¡å‹ï¼Œè€Œæ˜¯ç›´æ¥åœ¨è½¨è¿¹ç©ºé—´ä¸­è¿›è¡Œå­¦ä¹ ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šTraceGenä½¿ç”¨Transformeræ¶æ„è¿›è¡Œè½¨è¿¹é¢„æµ‹ï¼ŒæŸå¤±å‡½æ•°åŒ…æ‹¬è½¨è¿¹é¢„æµ‹æŸå¤±å’Œè¯­è¨€æè¿°æŸå¤±ã€‚TraceForgeæ•°æ®ç®¡é“åˆ©ç”¨SfMï¼ˆStructure from Motionï¼‰æŠ€æœ¯ä»è§†é¢‘ä¸­é‡å»º3Dåœºæ™¯ï¼Œå¹¶æå–è½¨è¿¹ã€‚ä¸ºäº†å¤„ç†ä¸åŒè§†é¢‘çš„å°ºåº¦å’Œè§†è§’å·®å¼‚ï¼ŒTraceForgeè¿˜é‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä¾‹å¦‚éšæœºç¼©æ”¾å’Œæ—‹è½¬ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTraceGenåœ¨å››ä¸ªæœºå™¨äººä»»åŠ¡ä¸­ä»…ä½¿ç”¨äº”ä¸ªç›®æ ‡æœºå™¨äººè§†é¢‘å³å¯è¾¾åˆ°80%çš„æˆåŠŸç‡ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦æ¯”æœ€å…ˆè¿›çš„åŸºäºè§†é¢‘çš„ä¸–ç•Œæ¨¡å‹å¿«50-600å€ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ï¼Œä»…ä½¿ç”¨äº”ä¸ªæœªæ ¡å‡†çš„äººç±»æ¼”ç¤ºè§†é¢‘ï¼ŒTraceGenä»ç„¶å¯ä»¥åœ¨çœŸå®æœºå™¨äººä¸Šè¾¾åˆ°67.5%çš„æˆåŠŸç‡ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„è·¨å…·èº«è¿ç§»èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TraceGenå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å¯ä»¥ç”¨äºæœºå™¨äººæ¨¡ä»¿å­¦ä¹ ã€äººæœºåä½œã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨å¤§é‡çš„è·¨å…·èº«è§†é¢‘æ•°æ®ï¼ŒTraceGenå¯ä»¥å¸®åŠ©æœºå™¨äººå¿«é€Ÿå­¦ä¹ æ–°çš„æŠ€èƒ½ï¼Œå¹¶é€‚åº”ä¸åŒçš„ç¯å¢ƒã€‚æ­¤å¤–ï¼ŒTraceGenè¿˜å¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„æœºå™¨äººåŠ¨ç”»ï¼Œä»¥åŠè¿›è¡Œè™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

