---
layout: default
title: A Novel Multi-Timescale Stability-Preserving Hierarchical Reinforcement Learning Controller Framework for Adaptive Control in High-Dimensional Dynamical Systems
---

# A Novel Multi-Timescale Stability-Preserving Hierarchical Reinforcement Learning Controller Framework for Adaptive Control in High-Dimensional Dynamical Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22420" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.22420v1</a>
  <a href="https://arxiv.org/pdf/2510.22420.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22420v1" onclick="toggleFavorite(this, '2510.22420v1', 'A Novel Multi-Timescale Stability-Preserving Hierarchical Reinforcement Learning Controller Framework for Adaptive Control in High-Dimensional Dynamical Systems')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Mohammad Ali Labbaf Khaniki, Fateme Taroodi, Benyamin Safizadeh

**ÂàÜÁ±ª**: cs.RO, eess.SY

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-25

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öÊó∂Èó¥Â∞∫Â∫¶Á®≥ÂÆöÊÄß‰øùÊåÅÁöÑÂ±ÇÊ¨°Âº∫ÂåñÂ≠¶‰π†ÊéßÂà∂Ê°ÜÊû∂‰ª•Ëß£ÂÜ≥È´òÁª¥Âä®ÊÄÅÁ≥ªÁªüÊéßÂà∂ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â±ÇÊ¨°Âº∫ÂåñÂ≠¶‰π†` `È´òÁª¥Âä®ÊÄÅÁ≥ªÁªü` `ÊùéÈõÖÊôÆËØ∫Â§´Á∫¶Êùü` `ÈöèÊú∫Á®≥ÂÆöÊÄß` `Â§öÊó∂Èó¥Â∞∫Â∫¶` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ë∂ÖÊ∑∑Ê≤åÁ≥ªÁªü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. È´òÁª¥ÈöèÊú∫Á≥ªÁªüÊéßÂà∂Èù¢‰∏¥Áª¥Â∫¶ËØÖÂíí„ÄÅÁº∫‰πèÊó∂Èó¥ÊäΩË±°ÂíåÈöèÊú∫Á®≥ÂÆöÊÄß‰∏çË∂≥Á≠âÊåëÊàòÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂ∫îÂØπ„ÄÇ
2. ÊèêÂá∫ÁöÑMTLHRLÊ°ÜÊû∂ÈÄöËøáÂ±ÇÊ¨°ÂåñÁ≠ñÁï•ÂíåÂçäÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºåÁªìÂêàÊùéÈõÖÊôÆËØ∫Â§´ÂáΩÊï∞‰ºòÂåñÔºåÂ¢ûÂº∫‰∫ÜÂÜ≥Á≠ñÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÁéá„ÄÇ
3. Âú®8Áª¥Ë∂ÖÊ∑∑Ê≤åÁ≥ªÁªüÂíå5Ëá™Áî±Â∫¶Êú∫Âô®‰∫∫ÊìçÊéßÂô®ÁöÑ‰ªøÁúüÂÆûÈ™å‰∏≠ÔºåMTLHRLÊòæËëóÈôç‰Ωé‰∫ÜËØØÂ∑ÆÊåáÊ†áÔºåË°®Áé∞Âá∫Êõ¥Âø´ÁöÑÊî∂ÊïõÈÄüÂ∫¶ÂíåÊõ¥Âº∫ÁöÑÂπ≤Êâ∞ÊäëÂà∂ËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÊéßÂà∂È´òÁª¥ÈöèÊú∫Á≥ªÁªüÂú®Êú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂ÂíåË∂ÖÊ∑∑Ê≤åÁ≥ªÁªü‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÈù¢‰∏¥Áª¥Â∫¶ËØÖÂíí„ÄÅÁº∫‰πèÊó∂Èó¥ÊäΩË±°‰ª•ÂèäÈöæ‰ª•Á°Æ‰øùÈöèÊú∫Á®≥ÂÆöÊÄßÁ≠âÊåëÊàò„ÄÇ‰∏∫ÂÖãÊúçËøô‰∫õÈôêÂà∂ÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂ§öÊó∂Èó¥Â∞∫Â∫¶ÊùéÈõÖÊôÆËØ∫Â§´Á∫¶ÊùüÂ±ÇÊ¨°Âº∫ÂåñÂ≠¶‰π†ÔºàMTLHRLÔºâÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂Âú®ÂçäÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã‰∏≠ÈõÜÊàê‰∫ÜÂ±ÇÊ¨°ÂåñÁ≠ñÁï•ÔºåÈááÁî®È´òÂ±ÇÁ≠ñÁï•ËøõË°åÊàòÁï•ËßÑÂàíÔºå‰ΩéÂ±ÇÁ≠ñÁï•ËøõË°åÂèçÂ∫îÊéßÂà∂ÔºåÊúâÊïàÁÆ°ÁêÜÂ§çÊùÇÁöÑÂ§öÊó∂Èó¥Â∞∫Â∫¶ÂÜ≥Á≠ñÔºåÂáèÂ∞ëÁª¥Â∫¶ÂºÄÈîÄ„ÄÇÈÄöËøáÊãâÊ†ºÊúóÊó•ÊùæÂºõÂíåÂ§öÊó∂Èó¥Â∞∫Â∫¶ÊºîÂëò-ËØÑËÆ∫ËÄÖÊõ¥Êñ∞Ôºå‰∏•Ê†ºÊâßË°åÁ®≥ÂÆöÊÄßÔºåÁ°Æ‰øùÂú®ÈöèÊú∫Âä®ÊÄÅ‰∏ãÁöÑÂùáÊñπÊúâÁïåÊÄßÊàñÊ∏êËøëÁ®≥ÂÆöÊÄß„ÄÇÂ§ßÈáè‰ªøÁúüÂÆûÈ™åË°®ÊòéÔºåMTLHRLÂú®Á®≥ÂÆöÊÄßÂíåÊÄßËÉΩ‰∏äÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥È´òÁª¥Âä®ÊÄÅÁ≥ªÁªüÊéßÂà∂‰∏≠ÁöÑÁª¥Â∫¶ËØÖÂíí„ÄÅÁº∫‰πèÊó∂Èó¥ÊäΩË±°ÂèäÈöèÊú∫Á®≥ÂÆöÊÄß‰∏çË∂≥Á≠âÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â∫îÂØπÂ§çÊùÇÂÜ≥Á≠ñÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÈöæ‰ª•‰øùËØÅÁ≥ªÁªüÁöÑÁ®≥ÂÆöÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMTLHRLÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•Â±ÇÊ¨°ÂåñÁ≠ñÁï•ÂíåÊùéÈõÖÊôÆËØ∫Â§´Á∫¶ÊùüÔºåÁªìÂêàÂçäÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºåÊó®Âú®ÊúâÊïàÁÆ°ÁêÜÂ§öÊó∂Èó¥Â∞∫Â∫¶ÁöÑÂÜ≥Á≠ñËøáÁ®ãÔºåÁ°Æ‰øùÁ≥ªÁªüÂú®ÈöèÊú∫Âä®ÊÄÅ‰∏ãÁöÑÁ®≥ÂÆöÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÊã¨È´òÂ±ÇÁ≠ñÁï•Áî®‰∫éÊàòÁï•ËßÑÂàíÂíå‰ΩéÂ±ÇÁ≠ñÁï•Áî®‰∫éÂèçÂ∫îÊéßÂà∂ÔºåÂà©Áî®Á•ûÁªèÊùéÈõÖÊôÆËØ∫Â§´ÂáΩÊï∞ËøõË°åÁ®≥ÂÆöÊÄßÁ∫¶ÊùüÔºåÂπ∂ÈÄöËøáÊãâÊ†ºÊúóÊó•ÊùæÂºõÂíåÂ§öÊó∂Èó¥Â∞∫Â∫¶ÁöÑÊºîÂëò-ËØÑËÆ∫ËÄÖÊõ¥Êñ∞ËøõË°å‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMTLHRLÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÂ±ÇÊ¨°ÂåñÁ≠ñÁï•‰∏éÊùéÈõÖÊôÆËØ∫Â§´Á∫¶ÊùüÁõ∏ÁªìÂêàÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊéßÂà∂Ê°ÜÊû∂ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈ´òÁª¥Âä®ÊÄÅÁ≥ªÁªüÁöÑÁ®≥ÂÆöÊÄßÂíåÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåÈááÁî®‰∫Ü‰ø°‰ªªÂå∫ÂüüÁ∫¶ÊùüÂíåËß£ËÄ¶‰ºòÂåñÁ≠ñÁï•ÔºåÁ°Æ‰øù‰∫ÜÂ≠¶‰π†ËøáÁ®ãÁöÑÈ´òÊïàÊÄßÂíåÂèØÈù†ÊÄßÔºåÂêåÊó∂‰ºòÂåñ‰∫ÜÊçüÂ§±ÂáΩÊï∞‰ª•ÈÄÇÂ∫îÂ§öÊó∂Èó¥Â∞∫Â∫¶ÁöÑÂä®ÊÄÅÁâπÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMTLHRLÂú®8Áª¥Ë∂ÖÊ∑∑Ê≤åÁ≥ªÁªüÂíå5Ëá™Áî±Â∫¶Êú∫Âô®‰∫∫ÊìçÊéßÂô®‰∏äÂùáÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåË∂ÖÊ∑∑Ê≤åÊéßÂà∂‰∏≠ÁöÑÁßØÂàÜÁªùÂØπËØØÂ∑ÆÔºàIAEÔºâ‰∏∫3.912ÔºåÊú∫Âô®‰∫∫ÊéßÂà∂‰∏≠ÁöÑIAE‰∏∫1.623ÔºåË°®Áé∞Âá∫Êõ¥Âø´ÁöÑÊî∂ÊïõÈÄüÂ∫¶ÂíåÊõ¥Âº∫ÁöÑÂπ≤Êâ∞ÊäëÂà∂ËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊéßÂà∂„ÄÅËá™Âä®È©æÈ©∂ËΩ¶ËæÜ„ÄÅË∂ÖÊ∑∑Ê≤åÁ≥ªÁªüÁ≠âÈ´òÁª¥Âä®ÊÄÅÁ≥ªÁªü„ÄÇÈÄöËøáÊèê‰æõ‰∏ÄÁßçÁêÜËÆ∫Âü∫Á°ÄÂíåÂÆûÁî®ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåMTLHRLÊ°ÜÊû∂ËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥ÂèØÈù†ÁöÑÊéßÂà∂ÔºåÂÖ∑ÊúâÂπøÊ≥õÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Controlling high-dimensional stochastic systems, critical in robotics, autonomous vehicles, and hyperchaotic systems, faces the curse of dimensionality, lacks temporal abstraction, and often fails to ensure stochastic stability. To overcome these limitations, this study introduces the Multi-Timescale Lyapunov-Constrained Hierarchical Reinforcement Learning (MTLHRL) framework. MTLHRL integrates a hierarchical policy within a semi-Markov Decision Process (SMDP), featuring a high-level policy for strategic planning and a low-level policy for reactive control, which effectively manages complex, multi-timescale decision-making and reduces dimensionality overhead. Stability is rigorously enforced using a neural Lyapunov function optimized via Lagrangian relaxation and multi-timescale actor-critic updates, ensuring mean-square boundedness or asymptotic stability in the face of stochastic dynamics. The framework promotes efficient and reliable learning through trust-region constraints and decoupled optimization. Extensive simulations on an 8D hyperchaotic system and a 5-DOF robotic manipulator demonstrate MTLHRL's empirical superiority. It significantly outperforms baseline methods in both stability and performance, recording the lowest error indices (e.g., Integral Absolute Error (IAE): 3.912 in hyperchaotic control and IAE: 1.623 in robotics), achieving faster convergence, and exhibiting superior disturbance rejection. MTLHRL offers a theoretically grounded and practically viable solution for robust control of complex stochastic systems.

