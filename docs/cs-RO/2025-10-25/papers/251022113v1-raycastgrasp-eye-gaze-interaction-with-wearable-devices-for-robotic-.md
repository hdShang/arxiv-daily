---
layout: default
title: RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation
---

# RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22113" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22113v1</a>
  <a href="https://arxiv.org/pdf/2510.22113.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22113v1" onclick="toggleFavorite(this, '2510.22113v1', 'RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zitiantao Lin, Yongpeng Sang, Yang Ye

**åˆ†ç±»**: cs.RO, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-10-25

**å¤‡æ³¨**: 5 pages, 5 figures; Accepted to: 2025 IEEE 4th International Conference on Intelligent Reality (ICIR 2025); Zitiantao Lin and Yongpeng Sang contributed equally to this work (co-first authors). Corresponding author: Yang Ye (y.ye@northeastern.edu)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RaycastGraspï¼šåŸºäºçœ¼åŠ¨è¿½è¸ªä¸å¯ç©¿æˆ´è®¾å¤‡çš„æœºå™¨äººæ“ä½œäº¤äº’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `çœ¼åŠ¨è¿½è¸ª` `æœºå™¨äººæ“ä½œ` `æ··åˆç°å®` `äººæœºäº¤äº’` `è¾…åŠ©æœºå™¨äºº` `å¯ç©¿æˆ´è®¾å¤‡` `æ„å›¾è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºæ“çºµæ†çš„æœºå™¨äººæ§åˆ¶ç•Œé¢ç²¾åº¦è¦æ±‚é«˜ï¼Œå‚è€ƒç³»ä¸ç›´è§‚ï¼Œå¯¹è¡ŒåŠ¨ä¸ä¾¿äººå£«æ„æˆæŒ‘æˆ˜ã€‚
2. è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯ç©¿æˆ´MRå¤´æ˜¾çš„çœ¼åŠ¨å¼•å¯¼æœºå™¨äººæ“ä½œæ–¹æ³•ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè‡ªç„¶åœ°ä¸ç‰©ä½“äº¤äº’ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ“ä½œç²¾åº¦ï¼Œé™ä½äº†ç³»ç»Ÿå»¶è¿Ÿï¼Œå¹¶å®ç°äº†è¾ƒé«˜çš„æ„å›¾å’Œç‰©ä½“è¯†åˆ«å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¬¬ä¸€äººç§°è§†è§’çš„ã€çœ¼åŠ¨å¼•å¯¼çš„æœºå™¨äººæ“ä½œç•Œé¢ï¼Œæ—¨åœ¨è¾…åŠ©è¡ŒåŠ¨ä¸ä¾¿çš„äººç¾¤è¿›è¡Œç‰©ä½“æŠ“å–ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¯ç©¿æˆ´æ··åˆç°å®ï¼ˆMRï¼‰å¤´æ˜¾ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶çš„çœ¼åŠ¨æ³¨è§†ä¸çœŸå®ä¸–ç•Œçš„ç‰©ä½“è¿›è¡Œæ— ç¼äº¤äº’ã€‚ç³»ç»Ÿæä¾›å¢å¼ºçš„è§†è§‰æç¤ºä»¥ç¡®è®¤ç”¨æˆ·æ„å›¾ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹å’Œæœºæ¢°è‡‚è¿›è¡Œæ„å›¾è¯†åˆ«å’Œç‰©ä½“æ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ“ä½œç²¾åº¦ï¼Œé™ä½äº†ç³»ç»Ÿå»¶è¿Ÿï¼Œå¹¶åœ¨å¤šä¸ªçœŸå®åœºæ™¯ä¸­å®ç°äº†è¶…è¿‡88%çš„å•æ¬¡æ„å›¾å’Œç‰©ä½“è¯†åˆ«å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥ç³»ç»Ÿåœ¨å¢å¼ºç›´è§‚æ€§å’Œå¯è®¿é—®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾äº†å…¶åœ¨è¾…åŠ©æœºå™¨äººåº”ç”¨ä¸­çš„å®é™…æ„ä¹‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ“ä½œç•Œé¢ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è¡ŒåŠ¨ä¸ä¾¿äººå£«çš„è¾…åŠ©ç³»ç»Ÿï¼Œé€šå¸¸ä¾èµ–äºæ“çºµæ†ç­‰ä¼ ç»Ÿè¾“å…¥æ–¹å¼ã€‚è¿™äº›æ–¹å¼å­˜åœ¨ç²¾åº¦è¦æ±‚é«˜ã€æ§åˆ¶æ–¹å¼ä¸ç›´è§‚ç­‰é—®é¢˜ï¼Œéš¾ä»¥å®ç°è‡ªç„¶ã€é«˜æ•ˆçš„äººæœºäº¤äº’ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ¡ˆä¾èµ–å¤–éƒ¨å±å¹•æˆ–é™åˆ¶æ€§çš„æ§åˆ¶æ–¹æ¡ˆï¼Œè¿›ä¸€æ­¥é™ä½äº†æ˜“ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çœ¼åŠ¨è¿½è¸ªæŠ€æœ¯ï¼Œç»“åˆå¯ç©¿æˆ´çš„æ··åˆç°å®ï¼ˆMRï¼‰è®¾å¤‡ï¼Œæ„å»ºä¸€ä¸ªä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„ã€ç›´è§‚çš„æœºå™¨äººæ“ä½œç•Œé¢ã€‚é€šè¿‡æ•æ‰ç”¨æˆ·çš„çœ¼åŠ¨æ³¨è§†ç‚¹ï¼Œç³»ç»Ÿèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„æ“ä½œæ„å›¾ï¼Œå¹¶å¼•å¯¼æœºæ¢°è‡‚å®Œæˆç›¸åº”çš„æŠ“å–ä»»åŠ¡ã€‚è¿™ç§æ–¹å¼æ¨¡æ‹Ÿäº†äººç±»è‡ªç„¶çš„æ“ä½œæ–¹å¼ï¼Œé™ä½äº†å­¦ä¹ æˆæœ¬ï¼Œæé«˜äº†æ“ä½œæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç³»ç»Ÿçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) çœ¼åŠ¨è¿½è¸ªæ¨¡å—ï¼šåˆ©ç”¨MRå¤´æ˜¾å†…ç½®çš„çœ¼åŠ¨è¿½è¸ªä¼ æ„Ÿå™¨ï¼Œå®æ—¶æ•æ‰ç”¨æˆ·çš„çœ¼åŠ¨æ•°æ®ã€‚2) æ„å›¾è¯†åˆ«æ¨¡å—ï¼šåŸºäºçœ¼åŠ¨æ•°æ®å’Œè§†è§‰ä¿¡æ¯ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹è¯†åˆ«ç”¨æˆ·çš„æ“ä½œæ„å›¾å’Œç›®æ ‡ç‰©ä½“ã€‚3) æœºæ¢°è‡‚æ§åˆ¶æ¨¡å—ï¼šæ ¹æ®æ„å›¾è¯†åˆ«ç»“æœï¼Œæ§åˆ¶æœºæ¢°è‡‚æ‰§è¡Œç›¸åº”çš„æŠ“å–åŠ¨ä½œã€‚4) è§†è§‰åé¦ˆæ¨¡å—ï¼šé€šè¿‡MRå¤´æ˜¾å‘ç”¨æˆ·æä¾›å¢å¼ºçš„è§†è§‰æç¤ºï¼Œä¾‹å¦‚ç›®æ ‡ç‰©ä½“çš„è½®å»“ã€æŠ“å–è·¯å¾„ç­‰ï¼Œä»¥å¸®åŠ©ç”¨æˆ·ç¡®è®¤æ“ä½œæ„å›¾å¹¶æé«˜æ“ä½œç²¾åº¦ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†çœ¼åŠ¨è¿½è¸ªæŠ€æœ¯ä¸å¯ç©¿æˆ´MRè®¾å¤‡ç›¸ç»“åˆï¼Œæ„å»ºäº†ä¸€ä¸ªæ²‰æµ¸å¼çš„ã€ç›´è§‚çš„æœºå™¨äººæ“ä½œç•Œé¢ã€‚ä¸ä¼ ç»Ÿçš„æ§åˆ¶æ–¹å¼ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ è‡ªç„¶ã€é«˜æ•ˆï¼Œé™ä½äº†ç”¨æˆ·çš„å­¦ä¹ æˆæœ¬ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹è¿›è¡Œæ„å›¾è¯†åˆ«ï¼Œæé«˜äº†è¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šç³»ç»Ÿä½¿ç”¨RaycastingæŠ€æœ¯å°†ç”¨æˆ·çš„è§†çº¿æŠ•å°„åˆ°ä¸‰ç»´åœºæ™¯ä¸­ï¼Œä»¥ç¡®å®šç”¨æˆ·æ³¨è§†çš„ç›®æ ‡ç‰©ä½“ã€‚ä¸ºäº†æé«˜æ„å›¾è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œç³»ç»Ÿé‡‡ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼Œä¾‹å¦‚åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜è®¾è®¡äº†ä¸€ç³»åˆ—è§†è§‰åé¦ˆæœºåˆ¶ï¼Œä¾‹å¦‚é«˜äº®æ˜¾ç¤ºç›®æ ‡ç‰©ä½“ã€æ˜¾ç¤ºæŠ“å–è·¯å¾„ç­‰ï¼Œä»¥å¸®åŠ©ç”¨æˆ·ç¡®è®¤æ“ä½œæ„å›¾å¹¶æé«˜æ“ä½œç²¾åº¦ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æé«˜äº†æ“ä½œç²¾åº¦ï¼Œé™ä½äº†ç³»ç»Ÿå»¶è¿Ÿï¼Œå¹¶åœ¨å¤šä¸ªçœŸå®åœºæ™¯ä¸­å®ç°äº†è¶…è¿‡88%çš„å•æ¬¡æ„å›¾å’Œç‰©ä½“è¯†åˆ«å‡†ç¡®ç‡ã€‚ä¸ä¼ ç»Ÿçš„æ“çºµæ†æ§åˆ¶æ–¹å¼ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´åŠ ç›´è§‚ã€é«˜æ•ˆï¼Œé™ä½äº†ç”¨æˆ·çš„å­¦ä¹ æˆæœ¬ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥ç³»ç»Ÿåœ¨å¢å¼ºç›´è§‚æ€§å’Œå¯è®¿é—®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºè¾…åŠ©æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚å¸®åŠ©è¡ŒåŠ¨ä¸ä¾¿çš„äººå£«å®Œæˆæ—¥å¸¸ç‰©å“çš„æŠ“å–å’Œæ“ä½œï¼Œæé«˜ä»–ä»¬çš„ç”Ÿæ´»è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥åº”ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ã€è¿œç¨‹æ“ä½œç­‰é¢†åŸŸï¼Œä¾‹å¦‚åœ¨å±é™©ç¯å¢ƒä¸­è¿›è¡Œè¿œç¨‹ä½œä¸šï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œéšç€MRæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè¯¥ç³»ç»Ÿæœ‰æœ›å®ç°æ›´åŠ æ™ºèƒ½åŒ–ã€ä¸ªæ€§åŒ–çš„æœºå™¨äººæ“ä½œä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic manipulators are increasingly used to assist individuals with mobility impairments in object retrieval. However, the predominant joystick-based control interfaces can be challenging due to high precision requirements and unintuitive reference frames. Recent advances in human-robot interaction have explored alternative modalities, yet many solutions still rely on external screens or restrictive control schemes, limiting their intuitiveness and accessibility. To address these challenges, we present an egocentric, gaze-guided robotic manipulation interface that leverages a wearable Mixed Reality (MR) headset. Our system enables users to interact seamlessly with real-world objects using natural gaze fixation from a first-person perspective, while providing augmented visual cues to confirm intent and leveraging a pretrained vision model and robotic arm for intent recognition and object manipulation. Experimental results demonstrate that our approach significantly improves manipulation accuracy, reduces system latency, and achieves single-pass intention and object recognition accuracy greater than 88% across multiple real-world scenarios. These results demonstrate the system's effectiveness in enhancing intuitiveness and accessibility, underscoring its practical significance for assistive robotics applications.

