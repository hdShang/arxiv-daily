---
layout: default
title: Improving Robotic Manipulation Robustness via NICE Scene Surgery
---

# Improving Robotic Manipulation Robustness via NICE Scene Surgery

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.22777" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.22777v1</a>
  <a href="https://arxiv.org/pdf/2511.22777.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.22777v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.22777v1', 'Improving Robotic Manipulation Robustness via NICE Scene Surgery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sajjad Pakdamansavoji, Mozhgan Pourkeshavarz, Adam Sigal, Zhiyuan Li, Rui Heng Yang, Amir Rasouli

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-27

**å¤‡æ³¨**: 11 figures, 3 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NICEåœºæ™¯æ‰‹æœ¯ï¼šåˆ©ç”¨è‡ªç„¶å›¾åƒä¿®å¤å¢å¼ºæœºå™¨äººæ“ä½œçš„é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰é²æ£’æ€§` `æ•°æ®å¢å¼º` `å›¾åƒç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `åœºæ™¯ç†è§£` `åˆ†å¸ƒå¤–æ³›åŒ–` `æ¨¡ä»¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°å®ä¸–ç•Œä¸­æœºå™¨äººæ“ä½œé¢ä¸´è§†è§‰å¹²æ‰°ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥ä¿è¯ç­–ç•¥çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚
2. NICEæ¡†æ¶åˆ©ç”¨å›¾åƒç”Ÿæˆå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åœºæ™¯ç¼–è¾‘å¢åŠ è§†è§‰å¤šæ ·æ€§ï¼Œç¼©å°åˆ†å¸ƒå¤–å·®è·ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒNICEæ˜¾è‘—æå‡äº†æœºå™¨äººæ“ä½œçš„å‡†ç¡®ç‡ã€æˆåŠŸç‡å’Œå®‰å…¨æ€§ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–æ¨¡å‹è®­ç»ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç°å®ç¯å¢ƒä¸­ï¼Œè§†è§‰å¹²æ‰°ä¼šæ˜¾è‘—é™ä½æœºå™¨äººæ“ä½œç­–ç•¥çš„æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œå³è‡ªç„¶åœºæ™¯ä¿®å¤å¢å¼ºï¼ˆNICEï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨ç°æœ‰æ¼”ç¤ºæ„å»ºæ–°çš„ç»éªŒï¼Œå¢åŠ è§†è§‰å¤šæ ·æ€§ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘æ¨¡ä»¿å­¦ä¹ ä¸­çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰å·®è·ã€‚NICEåˆ©ç”¨å›¾åƒç”Ÿæˆæ¡†æ¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œä¸‰ç§ç¼–è¾‘æ“ä½œï¼šå¯¹è±¡æ›¿æ¢ã€é£æ ¼è½¬æ¢å’Œç§»é™¤å¹²æ‰°ï¼ˆéç›®æ ‡ï¼‰å¯¹è±¡ã€‚è¿™äº›æ›´æ”¹åœ¨ä¸é˜»ç¢ç›®æ ‡å¯¹è±¡çš„æƒ…å†µä¸‹ä¿æŒç©ºé—´å…³ç³»ï¼Œå¹¶ä¿æŒåŠ¨ä½œæ ‡ç­¾çš„ä¸€è‡´æ€§ã€‚ä¸ä»¥å¾€çš„æ–¹æ³•ä¸åŒï¼ŒNICEä¸éœ€è¦é¢å¤–çš„æ•°æ®é‡‡é›†ã€æ¨¡æ‹Ÿå™¨è®¿é—®æˆ–è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒï¼Œä½¿å…¶æ˜“äºåº”ç”¨äºç°æœ‰çš„æœºå™¨äººæ•°æ®é›†ã€‚åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œå±•ç¤ºäº†NICEåœ¨ç”Ÿæˆé€¼çœŸåœºæ™¯å¢å¼ºæ–¹é¢çš„èƒ½åŠ›ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨NICEæ•°æ®æ¥å¾®è°ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»¥è¿›è¡Œç©ºé—´å¯ä¾›æ€§é¢„æµ‹ï¼Œä»¥åŠè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥ä»¥è¿›è¡Œå¯¹è±¡æ“ä½œã€‚è¯„ä¼°è¡¨æ˜ï¼ŒNICEæˆåŠŸåœ°æœ€å¤§é™åº¦åœ°å‡å°‘äº†OODå·®è·ï¼Œä»è€Œä½¿é«˜åº¦æ‚ä¹±åœºæ™¯ä¸­çš„å¯ä¾›æ€§é¢„æµ‹å‡†ç¡®ç‡æé«˜äº†20%ä»¥ä¸Šã€‚å¯¹äºæ“ä½œä»»åŠ¡ï¼Œåœ¨ä¸åŒæ•°é‡çš„å¹²æ‰°ç‰©ç¯å¢ƒä¸­è¿›è¡Œæµ‹è¯•æ—¶ï¼ŒæˆåŠŸç‡å¹³å‡æé«˜äº†11%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æé«˜äº†è§†è§‰é²æ£’æ€§ï¼Œå°†ç›®æ ‡æ··æ·†é™ä½äº†6%ï¼Œå¹¶é€šè¿‡é™ä½7%çš„ç¢°æ’ç‡æ¥å¢å¼ºå®‰å…¨æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³çœŸå®åœºæ™¯ä¸­æœºå™¨äººæ“ä½œå› è§†è§‰å¹²æ‰°ç‰©å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çœŸå®æ•°æ®æˆ–ä¾èµ–æ¨¡æ‹Ÿç¯å¢ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥é€‚åº”å¤æ‚å¤šå˜çš„çœŸå®ç¯å¢ƒï¼Œå¯¼è‡´æœºå™¨äººæ“ä½œçš„é²æ£’æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å›¾åƒä¿®å¤å’Œç”ŸæˆæŠ€æœ¯ï¼Œåœ¨ç°æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œåœºæ™¯å¢å¼ºï¼Œå¢åŠ è§†è§‰å¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹åˆ†å¸ƒå¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹åœºæ™¯è¿›è¡Œå¯¹è±¡æ›¿æ¢ã€é£æ ¼è½¬æ¢å’Œç§»é™¤å¹²æ‰°ç‰©ç­‰æ“ä½œï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å¯èƒ½å‡ºç°çš„å„ç§è§†è§‰å¹²æ‰°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNICEæ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š1) åœºæ™¯ç†è§£ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨¡å‹ç†è§£åœºæ™¯å†…å®¹ï¼Œè¯†åˆ«ç›®æ ‡å¯¹è±¡å’Œå¹²æ‰°ç‰©ï¼›2) åœºæ™¯ç¼–è¾‘ï¼šåˆ©ç”¨å›¾åƒç”Ÿæˆæ¨¡å‹å¯¹åœºæ™¯è¿›è¡Œç¼–è¾‘ï¼ŒåŒ…æ‹¬å¯¹è±¡æ›¿æ¢ã€é£æ ¼è½¬æ¢å’Œç§»é™¤å¹²æ‰°ç‰©ç­‰æ“ä½œï¼›3) æ•°æ®å¢å¼ºï¼šå°†ç¼–è¾‘åçš„å›¾åƒæ·»åŠ åˆ°è®­ç»ƒæ•°æ®é›†ä¸­ï¼Œç”¨äºè®­ç»ƒæˆ–å¾®è°ƒæœºå™¨äººæ“ä½œç­–ç•¥ã€‚æ•´ä¸ªæµç¨‹æ— éœ€é¢å¤–çš„æœºå™¨äººæ•°æ®é‡‡é›†ã€æ¨¡æ‹Ÿå™¨è®¿é—®æˆ–è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šNICEçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨å›¾åƒç”Ÿæˆå’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåœºæ™¯å¢å¼ºï¼Œä»è€Œåœ¨ä¸å¢åŠ æ•°æ®é‡‡é›†æˆæœ¬çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜æœºå™¨äººæ“ä½œçš„é²æ£’æ€§ã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºæ–¹æ³•ç›¸æ¯”ï¼ŒNICEèƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸã€æ›´å¤šæ ·åŒ–çš„åœºæ™¯ï¼Œæ›´æœ‰æ•ˆåœ°æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„è§†è§‰å¹²æ‰°ã€‚æ­¤å¤–ï¼ŒNICEæ¡†æ¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥æ–¹ä¾¿åœ°åº”ç”¨äºä¸åŒçš„æœºå™¨äººæ“ä½œä»»åŠ¡å’Œæ•°æ®é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šNICEæ¡†æ¶çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰è¿›è¡Œåœºæ™¯ç¼–è¾‘ï¼Œä¿è¯ç”Ÿæˆå›¾åƒçš„é€¼çœŸåº¦ï¼›2) åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-3ï¼‰è¿›è¡Œåœºæ™¯ç†è§£å’Œå¯¹è±¡è¯†åˆ«ï¼Œæé«˜åœºæ™¯ç¼–è¾‘çš„å‡†ç¡®æ€§ï¼›3) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œä¿è¯åœºæ™¯ç¼–è¾‘åçš„å›¾åƒä¸åŸå§‹å›¾åƒåœ¨è¯­ä¹‰ä¸Šçš„ä¸€è‡´æ€§ï¼›4) é€šè¿‡æ§åˆ¶åœºæ™¯ç¼–è¾‘çš„å¼ºåº¦å’Œé¢‘ç‡ï¼Œé¿å…å¼•å…¥è¿‡å¤šçš„å™ªå£°ï¼Œå½±å“æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNICEæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æé«˜æœºå™¨äººæ“ä½œçš„æ€§èƒ½ã€‚åœ¨å¯ä¾›æ€§é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒNICEä½¿å‡†ç¡®ç‡æé«˜äº†20%ä»¥ä¸Šã€‚åœ¨æ“ä½œä»»åŠ¡ä¸­ï¼ŒæˆåŠŸç‡å¹³å‡æé«˜äº†11%ï¼Œç›®æ ‡æ··æ·†é™ä½äº†6%ï¼Œç¢°æ’ç‡é™ä½äº†7%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒNICEèƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æœºå™¨äººæ“ä½œçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººæ“ä½œçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œå¯ä»¥é™ä½ç”Ÿäº§æˆæœ¬ã€æé«˜å·¥ä½œæ•ˆç‡ï¼Œå¹¶å‡å°‘äººä¸ºå¹²é¢„ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªä¸»å¯¼èˆªã€ç›®æ ‡æœç´¢å’Œäººæœºåä½œç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.
>   Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.

