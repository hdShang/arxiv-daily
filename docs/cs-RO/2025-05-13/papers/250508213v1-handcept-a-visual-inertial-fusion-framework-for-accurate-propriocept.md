---
layout: default
title: "HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands"
---

# HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08213" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08213v1</a>
  <a href="https://arxiv.org/pdf/2505.08213.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08213v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08213v1', 'HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junda Huang, Jianshu Zhou, Honghao Guo, Yunhui Liu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: 8 pages, 7 figures, journal

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHandCeptä»¥è§£å†³çµå·§æ‰‹çš„æœ¬ä½“æ„ŸçŸ¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœ¬ä½“æ„ŸçŸ¥` `çµå·§æ‰‹` `è§†è§‰-æƒ¯æ€§èåˆ` `æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨` `é›¶æ ·æœ¬å­¦ä¹ ` `æœºå™¨äººæ“ä½œ` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çµå·§æ‰‹çš„æœ¬ä½“æ„ŸçŸ¥é¢ä¸´ä½“ç§¯å’Œé€šç”¨æ€§é™åˆ¶ï¼Œä¼ ç»Ÿå…³èŠ‚è§’åº¦ä¼°è®¡æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­éš¾ä»¥å®ç°å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚
2. HandCeptæ¡†æ¶ç»“åˆäº†è§†è§‰å’Œæƒ¯æ€§ä¼ æ„Ÿå™¨ï¼Œé‡‡ç”¨é›¶æ ·æœ¬å­¦ä¹ å’Œæ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ï¼Œå®æ—¶èåˆæ•°æ®ä»¥æé«˜å…³èŠ‚è§’åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHandCeptçš„å…³èŠ‚è§’åº¦ä¼°è®¡è¯¯å·®åœ¨2Â°åˆ°4Â°ä¹‹é—´ï¼Œä¸”æ— æ˜æ˜¾æ¼‚ç§»ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†è§‰æˆ–æƒ¯æ€§å•ä¸€æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æœºå™¨äººæŠ€æœ¯å‘é€šç”¨æ“ä½œå‘å±•ï¼Œçµå·§æ‰‹çš„æœ¬ä½“æ„ŸçŸ¥å˜å¾—æ„ˆå‘é‡è¦ã€‚ç„¶è€Œï¼Œçµå·§æ‰‹çš„æœ¬ä½“æ„ŸçŸ¥ä»ç„¶æ˜¯ä¸€ä¸ªç“¶é¢ˆï¼Œä¸»è¦ç”±äºä½“ç§¯å’Œé€šç”¨æ€§é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†HandCeptï¼Œä¸€ä¸ªæ–°é¢–çš„è§†è§‰-æƒ¯æ€§æœ¬ä½“æ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿå…³èŠ‚è§’åº¦ä¼°è®¡æ–¹æ³•çš„æŒ‘æˆ˜ã€‚HandCeptåˆ©ç”¨é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆè…•éƒ¨RGB-Dç›¸æœºå’Œ9è½´IMUï¼Œé€šè¿‡æ— å»¶è¿Ÿçš„æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨å®æ—¶èåˆï¼Œè§£å†³äº†åŠ¨æ€ç¯å¢ƒä¸­è§†è§‰å’Œæƒ¯æ€§æµ‹é‡å™ªå£°å’Œæ¼‚ç§»çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHandCeptçš„å…³èŠ‚è§’åº¦ä¼°è®¡è¯¯å·®åœ¨2Â°åˆ°4Â°ä¹‹é—´ï¼Œä¸”æ— æ˜æ˜¾æ¼‚ç§»ï¼Œä¼˜äºä»…ä½¿ç”¨è§†è§‰æˆ–æƒ¯æ€§çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†IMUç³»ç»Ÿçš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ï¼Œå±•ç¤ºäº†IMUä¹‹é—´çš„å…±åŒåŸºå‡†æ¡†æ¶ç®€åŒ–äº†ç³»ç»Ÿæ ¡å‡†ã€‚ä¸ºæ”¯æŒæ¨¡æ‹Ÿåˆ°ç°å®çš„è¿ç§»ï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†é«˜ä¿çœŸæ¸²æŸ“ç®¡é“ï¼Œä¾¿äºåœ¨æ²¡æœ‰çœŸå®ä¸–ç•ŒåŸºå‡†çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³çµå·§æ‰‹çš„æœ¬ä½“æ„ŸçŸ¥é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´è§†è§‰å’Œæƒ¯æ€§æµ‹é‡å™ªå£°åŠæ¼‚ç§»çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å…³èŠ‚è§’åº¦ä¼°è®¡ä¸å‡†ç¡®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHandCeptæ¡†æ¶é€šè¿‡ç»“åˆè§†è§‰å’Œæƒ¯æ€§ä¼ æ„Ÿå™¨ï¼Œé‡‡ç”¨é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŠ¨æ€ç¯å¢ƒä¸‹çš„å…³èŠ‚è§’åº¦ä¼°è®¡ç²¾åº¦å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè…•éƒ¨å®‰è£…çš„RGB-Dç›¸æœºå’Œ9è½´IMUï¼Œæ•°æ®é€šè¿‡æ— å»¶è¿Ÿçš„æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨è¿›è¡Œå®æ—¶èåˆï¼Œå½¢æˆä¸€ä¸ªé«˜æ•ˆçš„æœ¬ä½“æ„ŸçŸ¥ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºé‡‡ç”¨é›¶æ ·æœ¬å­¦ä¹ å’Œæ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨çš„ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†å…³èŠ‚è§’åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„æ¼‚ç§»é—®é¢˜ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒIMUç³»ç»Ÿçš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§å¾—åˆ°äº†éªŒè¯ï¼Œé‡‡ç”¨å…±åŒåŸºå‡†æ¡†æ¶ç®€åŒ–äº†ç³»ç»Ÿæ ¡å‡†ï¼Œç¡®ä¿äº†é«˜ç²¾åº¦çš„ä¼ æ„Ÿå™¨èåˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHandCeptåœ¨å…³èŠ‚è§’åº¦ä¼°è®¡ä¸­å®ç°äº†2Â°åˆ°4Â°çš„è¯¯å·®èŒƒå›´ï¼Œä¸”æ— æ˜æ˜¾æ¼‚ç§»ï¼Œæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨è§†è§‰æˆ–æƒ¯æ€§çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€åŒ»ç–—æœºå™¨äººã€ä»¥åŠäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜çµå·§æ‰‹çš„æœ¬ä½“æ„ŸçŸ¥èƒ½åŠ›ï¼ŒHandCeptèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As robotics progresses toward general manipulation, dexterous hands are becoming increasingly critical. However, proprioception in dexterous hands remains a bottleneck due to limitations in volume and generality. In this work, we present HandCept, a novel visual-inertial proprioception framework designed to overcome the challenges of traditional joint angle estimation methods. HandCept addresses the difficulty of achieving accurate and robust joint angle estimation in dynamic environments where both visual and inertial measurements are prone to noise and drift. It leverages a zero-shot learning approach using a wrist-mounted RGB-D camera and 9-axis IMUs, fused in real time via a latency-free Extended Kalman Filter (EKF). Our results show that HandCept achieves joint angle estimation errors between $2^{\circ}$ and $4^{\circ}$ without observable drift, outperforming visual-only and inertial-only methods. Furthermore, we validate the stability and uniformity of the IMU system, demonstrating that a common base frame across IMUs simplifies system calibration. To support sim-to-real transfer, we also open-sourced our high-fidelity rendering pipeline, which is essential for training without real-world ground truth. This work offers a robust, generalizable solution for proprioception in dexterous hands, with significant implications for robotic manipulation and human-robot interaction.

