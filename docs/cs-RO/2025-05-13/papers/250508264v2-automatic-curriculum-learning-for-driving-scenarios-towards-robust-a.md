---
layout: default
title: Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning
---

# Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08264" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08264v2</a>
  <a href="https://arxiv.org/pdf/2505.08264.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08264v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08264v2', 'Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ahmed Abouelazm, Tim Weinstein, Tim Joseph, Philip SchÃ¶rner, J. Marius ZÃ¶llner

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-07-11)

**å¤‡æ³¨**: Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ä»¥è§£å†³è‡ªä¸»é©¾é©¶è®­ç»ƒæ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `å¼ºåŒ–å­¦ä¹ ` `è¯¾ç¨‹å­¦ä¹ ` `åœºæ™¯ç”Ÿæˆ` `è®­ç»ƒæ•ˆç‡` `æ³›åŒ–èƒ½åŠ›` `åŠ¨æ€é€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å›ºå®šåœºæ™¯ä¸‹è®­ç»ƒï¼Œå¯¼è‡´ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥é€‚åº”çœŸå®ä¸–ç•Œçš„å¤æ‚æƒ…å†µã€‚
2. æœ¬æ–‡æå‡ºçš„è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ¡†æ¶èƒ½å¤Ÿæ ¹æ®ä»£ç†çš„å­¦ä¹ èƒ½åŠ›åŠ¨æ€ç”Ÿæˆé©¾é©¶åœºæ™¯ï¼Œé¿å…äº†ä¸“å®¶è®¾è®¡çš„åè§å’Œä¸çµæ´»æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒäº¤é€šå¯†åº¦ä¸‹çš„æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”è®­ç»ƒæ•ˆç‡å¾—åˆ°æ”¹å–„ï¼Œæ”¶æ•›é€Ÿåº¦åŠ å¿«ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒç«¯åˆ°ç«¯è‡ªä¸»é©¾é©¶ä»£ç†æ‰€é¢ä¸´çš„æŒ‘æˆ˜è¿›è¡Œäº†æ¢è®¨ã€‚ç°æœ‰çš„RLä»£ç†é€šå¸¸åœ¨å›ºå®šåœºæ™¯ä¸‹è®­ç»ƒï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›å’Œå®é™…éƒ¨ç½²æ•ˆæœã€‚è™½ç„¶é¢†åŸŸéšæœºåŒ–æä¾›äº†ä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºè®­ç»ƒåœºæ™¯ä¹‹é—´çš„é«˜æ–¹å·®ï¼Œå¸¸å¸¸å¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œæ¬¡ä¼˜ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®ä»£ç†çš„èƒ½åŠ›åŠ¨æ€ç”Ÿæˆå…·æœ‰é€‚åº”æ€§å¤æ‚åº¦çš„é©¾é©¶åœºæ™¯ã€‚ä¸æ‰‹åŠ¨è®¾è®¡çš„è¯¾ç¨‹ä¸åŒï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªâ€œæ•™å¸ˆâ€è‡ªåŠ¨ç”Ÿæˆå’Œå˜å¼‚é©¾é©¶åœºæ™¯ï¼ŒåŸºäºä»£ç†å½“å‰ç­–ç•¥çš„å­¦ä¹ æ½œåŠ›ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½äº¤é€šå¯†åº¦å’Œé«˜äº¤é€šå¯†åº¦ä¸‹çš„æˆåŠŸç‡åˆ†åˆ«æé«˜äº†9%å’Œ21%ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œè®­ç»ƒæ­¥éª¤æ›´å°‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å›ºå®šåœºæ™¯ä¸‹è®­ç»ƒå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œè®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹çµæ´»æ€§ï¼Œéš¾ä»¥é€‚åº”å¤æ‚çš„çœŸå®é©¾é©¶åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€ç”Ÿæˆå’Œè°ƒæ•´é©¾é©¶åœºæ™¯çš„å¤æ‚åº¦ï¼Œä¾æ®ä»£ç†çš„å­¦ä¹ èƒ½åŠ›è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªâ€œæ•™å¸ˆâ€æ¨¡å—ï¼Œè¯¥æ¨¡å—è´Ÿè´£ç”Ÿæˆå’Œå˜å¼‚é©¾é©¶åœºæ™¯ï¼ŒåŸºäºä»£ç†å½“å‰ç­–ç•¥çš„å­¦ä¹ æ½œåŠ›è¿›è¡Œè°ƒæ•´ã€‚æ¡†æ¶è¿˜åŒ…å«åœºæ™¯ç­›é€‰æœºåˆ¶ï¼Œä»¥æ’é™¤ä»£ç†å·²æŒæ¡æˆ–è¿‡äºå›°éš¾çš„åœºæ™¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è‡ªåŠ¨ç”Ÿæˆå’Œå˜å¼‚åœºæ™¯çš„â€œæ•™å¸ˆâ€æœºåˆ¶ï¼Œæ¶ˆé™¤äº†æ‰‹åŠ¨è®¾è®¡å¸¦æ¥çš„åè§ï¼Œå¹¶æé«˜äº†è¯¾ç¨‹å­¦ä¹ çš„å¯æ‰©å±•æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é€‚åº”ä»£ç†çš„å­¦ä¹ è¿›ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºä»£ç†å½“å‰ç­–ç•¥çš„å­¦ä¹ æ½œåŠ›ä½œä¸ºåœºæ™¯ç”Ÿæˆçš„ä¾æ®ï¼Œå¹¶è®¾ç½®äº†ç›¸åº”çš„å‚æ•°ä»¥æ§åˆ¶åœºæ™¯çš„å¤æ‚åº¦å’Œå¤šæ ·æ€§ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„é€‰æ‹©ä¹Ÿç»è¿‡ä¼˜åŒ–ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ¡†æ¶åœ¨ä½äº¤é€šå¯†åº¦ä¸‹æˆåŠŸç‡æé«˜äº†9%ï¼Œåœ¨é«˜äº¤é€šå¯†åº¦ä¸‹æé«˜äº†21%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨è®­ç»ƒæ­¥éª¤ä¸Šä¹Ÿè¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„è®­ç»ƒä¸å¼€å‘ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å’ŒåŠ¨æ€çš„äº¤é€šç¯å¢ƒä¸­ã€‚é€šè¿‡æé«˜è‡ªä¸»é©¾é©¶ä»£ç†çš„è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŠ é€Ÿè‡ªä¸»é©¾é©¶æŠ€æœ¯çš„å®é™…éƒ¨ç½²ï¼Œæå‡é“è·¯å®‰å…¨æ€§å’Œé©¾é©¶ä½“éªŒã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½æ‰©å±•åˆ°å…¶ä»–éœ€è¦åŠ¨æ€é€‚åº”çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper addresses the challenges of training end-to-end autonomous driving agents using Reinforcement Learning (RL). RL agents are typically trained in a fixed set of scenarios and nominal behavior of surrounding road users in simulations, limiting their generalization and real-life deployment. While domain randomization offers a potential solution by randomly sampling driving scenarios, it frequently results in inefficient training and sub-optimal policies due to the high variance among training scenarios. To address these limitations, we propose an automatic curriculum learning framework that dynamically generates driving scenarios with adaptive complexity based on the agent's evolving capabilities. Unlike manually designed curricula that introduce expert bias and lack scalability, our framework incorporates a ``teacher'' that automatically generates and mutates driving scenarios based on their learning potential -- an agent-centric metric derived from the agent's current policy -- eliminating the need for expert design. The framework enhances training efficiency by excluding scenarios the agent has mastered or finds too challenging. We evaluate our framework in a reinforcement learning setting where the agent learns a driving policy from camera images. Comparative results against baseline methods, including fixed scenario training and domain randomization, demonstrate that our approach leads to enhanced generalization, achieving higher success rates: +9% in low traffic density, +21% in high traffic density, and faster convergence with fewer training steps. Our findings highlight the potential of ACL in improving the robustness and efficiency of RL-based autonomous driving agents.

