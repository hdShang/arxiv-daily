---
layout: default
title: ORACLE-Grasp: Zero-Shot Task-Oriented Robotic Grasping using Large Multimodal Models
---

# ORACLE-Grasp: Zero-Shot Task-Oriented Robotic Grasping using Large Multimodal Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08417" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08417v1</a>
  <a href="https://arxiv.org/pdf/2505.08417.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08417v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08417v1', 'ORACLE-Grasp: Zero-Shot Task-Oriented Robotic Grasping using Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Avihai Giuili, Rotem Atari, Avishai Sintov

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºORACLE-Graspä»¥è§£å†³æ— è®­ç»ƒæ•°æ®çš„æœºå™¨äººæŠ“å–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæŠ“å–` `å¤šæ¨¡æ€æ¨¡å‹` `é›¶-shotå­¦ä¹ ` `è¯­ä¹‰æ¨ç†` `ç©ºé—´æ¨ç†` `è‡ªåŠ¨åŒ–æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººæŠ“å–æ–¹æ³•ä¾èµ–äºå¤§é‡è®­ç»ƒæ•°æ®æˆ–å‡ ä½•å»ºæ¨¡ï¼Œéš¾ä»¥é€‚åº”å¤šå˜çš„ç°å®ç¯å¢ƒã€‚
2. ORACLE-Graspé€šè¿‡åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§é›¶-shotçš„æŠ“å–é€‰æ‹©æ¡†æ¶ï¼Œé¿å…äº†é¢å¤–çš„è®­ç»ƒéœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒORACLE-Graspåœ¨æŠ“å–ä»»åŠ¡ä¸­å®ç°äº†ä½è¯¯å·®å’Œé«˜æˆåŠŸç‡ï¼Œå±•ç¤ºäº†å…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æŠ“å–æœªçŸ¥ç‰©ä½“ä»ç„¶æ˜¯æœºå™¨äººæŠ€æœ¯ä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ï¼Œæ¶‰åŠè¯­ä¹‰ç†è§£å’Œç©ºé—´æ¨ç†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯†é›†çš„è®­ç»ƒæ•°æ®é›†æˆ–æ˜¾å¼çš„å‡ ä½•å»ºæ¨¡ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä»»åŠ¡ä¸­çš„å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºçš„ORACLE-Graspæ˜¯ä¸€ä¸ªé›¶-shotæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä½œä¸ºè¯­ä¹‰oracleæ¥æŒ‡å¯¼æŠ“å–é€‰æ‹©ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–äººå·¥è¾“å…¥ã€‚è¯¥ç³»ç»Ÿå°†æŠ“å–é¢„æµ‹å½¢å¼åŒ–ä¸ºç»“æ„åŒ–çš„è¿­ä»£å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡åŒæç¤ºå·¥å…·è°ƒç”¨æå–é«˜å±‚æ¬¡çš„ç‰©ä½“ä¸Šä¸‹æ–‡ï¼Œç„¶åé€‰æ‹©ä¸ä»»åŠ¡ç›¸å…³çš„æŠ“å–åŒºåŸŸã€‚å®éªŒè¡¨æ˜ï¼Œé¢„æµ‹çš„æŠ“å–åœ¨ä½ç½®å’Œæ–¹å‘ä¸Šç›¸å¯¹äºäººç±»æ ‡æ³¨çš„çœŸå®å€¼å…·æœ‰è¾ƒä½çš„è¯¯å·®ï¼Œå¹¶åœ¨å®é™…æ‹¾å–ä»»åŠ¡ä¸­å®ç°äº†é«˜æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æŠ“å–æœªçŸ¥ç‰©ä½“çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®æˆ–å‡ ä½•å»ºæ¨¡ï¼Œå¯¼è‡´å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„å¯æ‰©å±•æ€§å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šORACLE-Graspçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä½œä¸ºè¯­ä¹‰oracleï¼Œè¿›è¡Œé›¶-shotæŠ“å–é€‰æ‹©ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–çš„è¿­ä»£å†³ç­–è¿‡ç¨‹ï¼Œç»“åˆé«˜å±‚æ¬¡çš„ç‰©ä½“ä¸Šä¸‹æ–‡å’Œä»»åŠ¡ç›¸å…³çš„æŠ“å–åŒºåŸŸé€‰æ‹©ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„è®­ç»ƒéœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥ç³»ç»Ÿçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œé€šè¿‡åŒæç¤ºå·¥å…·è°ƒç”¨æå–ç‰©ä½“çš„é«˜å±‚æ¬¡ä¸Šä¸‹æ–‡ï¼›å…¶æ¬¡ï¼ŒåŸºäºæå–çš„ä¿¡æ¯é€‰æ‹©ä¸ä»»åŠ¡ç›¸å…³çš„æŠ“å–åŒºåŸŸã€‚æ•´ä¸ªè¿‡ç¨‹é€šè¿‡ç¦»æ•£åŒ–å›¾åƒç©ºé—´å’Œå€™é€‰åŒºåŸŸæ¨ç†æ¥å®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šORACLE-Graspçš„åˆ›æ–°åœ¨äºå…¶é›¶-shotèƒ½åŠ›ï¼Œåˆ©ç”¨LMMsè¿›è¡Œè¯­ä¹‰æ¨ç†ï¼Œé¿å…äº†å¯¹ç‰¹å®šä»»åŠ¡æ•°æ®é›†çš„ä¾èµ–ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æå‡äº†æŠ“å–çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒORACLE-Graspé‡‡ç”¨äº†æ—©åœç­–ç•¥å’ŒåŸºäºæ·±åº¦çš„ç»†åŒ–æ­¥éª¤ï¼Œä»¥æé«˜æŠ“å–çš„æ•ˆç‡å’Œå¯é æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡å°šæœªè¯¦ç»†æŠ«éœ²ï¼Œå±äºæœªçŸ¥é¢†åŸŸã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒORACLE-Graspåœ¨æŠ“å–ä»»åŠ¡ä¸­ç›¸è¾ƒäºäººç±»æ ‡æ³¨çš„çœŸå®å€¼ï¼Œä½ç½®å’Œæ–¹å‘è¯¯å·®å‡è¾ƒä½ï¼ŒæˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–ä»“åº“ã€å®¶åº­æœåŠ¡æœºå™¨äººä»¥åŠå·¥ä¸šæœºå™¨äººç­‰åœºæ™¯ã€‚é€šè¿‡å®ç°æ— éœ€ç‰¹å®šä»»åŠ¡æ•°æ®é›†çš„è‡ªä¸»æŠ“å–ï¼ŒORACLE-Graspèƒ½å¤Ÿå¤§å¹…æå‡æœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Grasping unknown objects in unstructured environments remains a fundamental challenge in robotics, requiring both semantic understanding and spatial reasoning. Existing methods often rely on dense training datasets or explicit geometric modeling, limiting their scalability to real-world tasks. Recent advances in Large Multimodal Models (LMMs) offer new possibilities for integrating vision and language understanding, but their application to autonomous robotic grasping remains largely unexplored. We present ORACLE-Grasp, a zero-shot framework that leverages LMMs as semantic oracles to guide grasp selection without requiring additional training or human input. The system formulates grasp prediction as a structured, iterative decision process, using dual-prompt tool calling to first extract high-level object context and then select task-relevant grasp regions. By discretizing the image space and reasoning over candidate areas, ORACLE-Grasp mitigates the spatial imprecision common in LMMs and produces human-like, task-driven grasp suggestions. Early stopping and depth-based refinement steps further enhance efficiency and physical grasp reliability. Experiments demonstrate that the predicted grasps achieve low positional and orientation errors relative to human-annotated ground truth and lead to high success rates in real-world pick up tasks. These results highlight the potential of combining language-driven reasoning with lightweight vision techniques to enable robust, autonomous grasping without task-specific datasets or retraining.

