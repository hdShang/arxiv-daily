---
layout: default
title: "From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation"
---

# From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08548" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08548v2</a>
  <a href="https://arxiv.org/pdf/2505.08548.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08548v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08548v2', 'From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, Jianye Hao

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-05-27)

**å¤‡æ³¨**: Our project homepage: https://embodied-fsd.github.io/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºFSDæ¨¡å‹ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„é›¶-shotæ³›åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç©ºé—´å…³ç³»æ¨ç†` `é›¶-shotå­¦ä¹ ` `æ³›åŒ–èƒ½åŠ›` `è‡ªä¸€è‡´æ€§æœºåˆ¶` `å±‚æ¬¡åŒ–æ•°æ®å¤„ç†` `å¤šæ¨¡æ€èåˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨é¢å¯¹æœªè§åœºæ™¯å’Œæ–°ä»»åŠ¡æ—¶ï¼Œé›¶-shotæ€§èƒ½ä¸è¶³ï¼Œéš¾ä»¥å®ç°æ³›åŒ–ã€‚
2. æœ¬æ–‡æå‡ºFSDæ¨¡å‹ï¼Œé€šè¿‡ç©ºé—´å…³ç³»æ¨ç†ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼Œæä¾›ç»†ç²’åº¦çš„æ“ä½œæŒ‡å¯¼ï¼Œä»è€Œæå‡æœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒFSDåœ¨SimplerEnvä¸­æˆåŠŸç‡è¾¾åˆ°40.6%ï¼Œåœ¨8ä¸ªçœŸå®ä»»åŠ¡ä¸­è¾¾åˆ°72%ï¼Œç›¸è¾ƒäºæœ€å¼ºåŸºçº¿æå‡30%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœºå™¨äººæ“ä½œä¸­ï¼Œå®ç°æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æœªè§åœºæ™¯å’Œæ–°ä»»åŠ¡ä¸­ã€‚ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹è™½ç„¶åŸºäºé€šç”¨çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œä½†ç”±äºå…·èº«æ•°æ®é›†ä¸­å­˜åœ¨çš„ç¨€ç¼ºæ€§å’Œå¼‚è´¨æ€§ï¼Œå¯¼è‡´å…¶é›¶-shotæ€§èƒ½ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†FSDï¼ˆFrom Seeing to Doingï¼‰ï¼Œä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç©ºé—´å…³ç³»æ¨ç†ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼Œä¸ºæœºå™¨äººæ“ä½œæä¾›ç»†ç²’åº¦æŒ‡å¯¼ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†FSDåœ¨â€œçœ‹â€å’Œâ€œåšâ€æ–¹é¢çš„èƒ½åŠ›ï¼Œåœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨æˆ‘ä»¬æå‡ºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†VABenchä¸Šä¹Ÿå–å¾—äº†è‰¯å¥½ç»“æœã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†åœ¨æœºå™¨äººæ“ä½œä¸­çš„é›¶-shotèƒ½åŠ›ï¼Œåœ¨SimplerEnvå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­çš„é›¶-shotæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æ–°ä»»åŠ¡å’Œæœªè§åœºæ™¯æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ‰§è¡Œæ“ä½œï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šFSDæ¨¡å‹çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç©ºé—´å…³ç³»æ¨ç†ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼Œä»è€Œä¸ºæœºå™¨äººæ“ä½œæä¾›ç»†ç²’åº¦çš„æŒ‡å¯¼ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨å¢å¼ºæ¨¡å‹å¯¹å¤æ‚åœºæ™¯çš„ç†è§£å’Œæ“ä½œèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šFSDçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå±‚æ¬¡åŒ–çš„æ•°æ®å¤„ç†ç®¡é“å’Œè‡ªä¸€è‡´æ€§æœºåˆ¶ã€‚æ•°æ®å¤„ç†ç®¡é“è´Ÿè´£è®­ç»ƒæ•°æ®çš„å‡†å¤‡å’Œå¤„ç†ï¼Œè‡ªä¸€è‡´æ€§æœºåˆ¶åˆ™ç¡®ä¿ç©ºé—´åæ ‡ä¸è§†è§‰ä¿¡å·çš„å¯¹é½ã€‚

**å…³é”®åˆ›æ–°**ï¼šFSDçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é€šè¿‡ç©ºé—´å…³ç³»æ¨ç†ç”Ÿæˆä¸­é—´è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç›´æ¥æ˜ å°„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ“ä½œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒFSDé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç©ºé—´å…³ç³»çš„æ¨ç†ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å±‚æ¬¡åŒ–æ¨¡å—ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

FSDæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨SimplerEnvä¸­æˆåŠŸç‡è¾¾åˆ°40.6%ï¼Œåœ¨8ä¸ªçœŸå®ä»»åŠ¡ä¸­æˆåŠŸç‡è¾¾åˆ°72%ã€‚ç›¸è¾ƒäºæœ€å¼ºåŸºçº¿ï¼ŒFSDçš„æ€§èƒ½æå‡å¹…åº¦è¾¾30%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨é›¶-shotæœºå™¨äººæ“ä½œä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒæœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼ŒFSDæ¨¡å‹èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜çš„çµæ´»æ€§å’Œæ•ˆç‡ï¼Œæ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSD's capabilities in both "seeing" and "doing," achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 40.6% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%.

