---
layout: default
title: Adaptive Diffusion Policy Optimization for Robotic Manipulation
---

# Adaptive Diffusion Policy Optimization for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08376" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.08376v1</a>
  <a href="https://arxiv.org/pdf/2505.08376.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08376v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08376v1', 'Adaptive Diffusion Policy Optimization for Robotic Manipulation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Huiyun Jiang, Zhuang Yang

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-13

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/Timeless-lab/ADPO.git)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ëá™ÈÄÇÂ∫îÊâ©Êï£Á≠ñÁï•‰ºòÂåñ‰ª•ÊèêÂçáÊú∫Âô®‰∫∫ÊìçÊéßÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êâ©Êï£Ê®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÊìçÊéß` `Ëá™ÈÄÇÂ∫îÊ¢ØÂ∫¶` `Á≠ñÁï•‰ºòÂåñ` `ÁÆóÊ≥ïÊ°ÜÊû∂` `È´òÁª¥ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®‰ºòÂåñÂü∫‰∫éÊâ©Êï£ÁöÑÁ≠ñÁï•Êó∂Áº∫‰πèÂø´ÈÄüÂíåÁ®≥ÂÆöÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Êú∫Âô®‰∫∫ÊìçÊéß‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑADPOÁÆóÊ≥ïÂà©Áî®Ëá™ÈÄÇÂ∫îÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºåÊó®Âú®È´òÊïàÂæÆË∞ÉÊâ©Êï£Á≠ñÁï•‰ª•ÊèêÂçáÊú∫Âô®‰∫∫ÊéßÂà∂ÊÄßËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåADPOÂú®Ê†áÂáÜÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÂÖ≠ÁßçÂü∫ÂáÜÊâ©Êï£RLÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÊâ©Êï£Ê®°ÂûãÂú®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰∏≠ÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩúÂäõÔºåËÉΩÂ§üÂª∫Ê®°Â§çÊùÇÁ≠ñÁï•„ÄÅË°®ËææÈ´òÂ∫¶ÁöÑÂ§öÊ®°ÊÄÅÊÄßÔºåÂπ∂ÊúâÊïàÂ§ÑÁêÜÈ´òÁª¥ËøûÁª≠ÊéßÂà∂‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÂÖ≥‰∫éÂ¶Ç‰ΩïÂø´ÈÄüÁ®≥ÂÆöÂú∞‰ºòÂåñÂü∫‰∫éÊâ©Êï£ÁöÑÁ≠ñÁï•ÔºàÂ¶ÇÊâ©Êï£Á≠ñÁï•ÔºâÁöÑÁ†îÁ©∂‰ªçÁÑ∂ÊúâÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éAdamÁöÑÊâ©Êï£Á≠ñÁï•‰ºòÂåñÔºàADPOÔºâÔºåËøôÊòØ‰∏Ä‰∏™Âø´ÈÄüÁöÑÁÆóÊ≥ïÊ°ÜÊû∂ÔºåÂåÖÂê´‰∫ÜÂú®RL‰∏≠‰ΩøÁî®Ëá™ÈÄÇÂ∫îÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂæÆË∞ÉÂü∫‰∫éÊâ©Êï£ÁöÑÁ≠ñÁï•ÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇÊàë‰ª¨Á°ÆËÆ§ADPOÂú®Ê†áÂáÜÊú∫Âô®‰∫∫‰ªªÂä°ÁöÑÂæÆË∞ÉÊïàÊûú‰∏ä‰ºò‰∫éÂÖ∂‰ªñÂü∫‰∫éÊâ©Êï£ÁöÑRLÊñπÊ≥ï„ÄÇÈÄöËøáÂØπÊ†áÂáÜÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°ËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåADPOË°®Áé∞Âá∫ÊØîÂü∫Á∫øÊñπÊ≥ïÊõ¥Â•ΩÁöÑÊàñÂèØÊØîÁöÑÊÄßËÉΩ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Á≥ªÁªüÂàÜÊûê‰∫ÜÂ§ö‰∏™Ë∂ÖÂèÇÊï∞Âú®Ê†áÂáÜÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁöÑÊïèÊÑüÊÄßÔºå‰∏∫ÂêéÁª≠ÂÆûÈôÖÂ∫îÁî®Êèê‰æõÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â¶Ç‰ΩïÂø´ÈÄü‰∏îÁ®≥ÂÆöÂú∞‰ºòÂåñÂü∫‰∫éÊâ©Êï£ÁöÑÁ≠ñÁï•ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Ëøô‰∏ÄÈ¢ÜÂüüÁöÑÁ†îÁ©∂ËæÉÂ∞ëÔºåÂØºËá¥Âú®Êú∫Âô®‰∫∫ÊìçÊéß‰ªªÂä°‰∏≠ÊïàÊûú‰∏ç‰Ω≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑADPOÁÆóÊ≥ïÂü∫‰∫éËá™ÈÄÇÂ∫îÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºåÊó®Âú®ÈÄöËøáÈ´òÊïàÁöÑÂæÆË∞ÉËøáÁ®ãÊèêÂçáÊâ©Êï£Á≠ñÁï•ÁöÑÊÄßËÉΩ„ÄÇËøôÁßçËÆæËÆ°ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÁöÑÁ≠ñÁï•Á©∫Èó¥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöADPOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅÁ≠ñÁï•Êõ¥Êñ∞ÂíåÊÄßËÉΩËØÑ‰º∞‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÔºåÈÄöËøáÁéØÂ¢É‰∫§‰∫íÊî∂ÈõÜÊï∞ÊçÆÔºõÁÑ∂ÂêéÔºåÂà©Áî®Ëá™ÈÄÇÂ∫îÊ¢ØÂ∫¶ÊñπÊ≥ïÊõ¥Êñ∞Á≠ñÁï•ÔºõÊúÄÂêéÔºåËØÑ‰º∞Êõ¥Êñ∞ÂêéÁöÑÁ≠ñÁï•Âú®‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöADPOÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞ÜËá™ÈÄÇÂ∫îÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂ∫îÁî®‰∫éÊâ©Êï£Á≠ñÁï•ÁöÑ‰ºòÂåñ‰∏≠ÔºåËøôÊòØÂú®Âº∫ÂåñÂ≠¶‰π†È¢ÜÂüüËæÉÂ∞ëÊé¢Á¥¢ÁöÑÊñπÂêë„ÄÇËøô‰∏ÄÊñπÊ≥ïÊòæËëóÊèêÂçá‰∫ÜÁ≠ñÁï•ÂæÆË∞ÉÁöÑÊïàÁéáÂíåÁ®≥ÂÆöÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ADPO‰∏≠ÔºåÂÖ≥ÈîÆÂèÇÊï∞ÁöÑËÆæÁΩÆÂåÖÊã¨Â≠¶‰π†ÁéáÁöÑËá™ÈÄÇÂ∫îË∞ÉÊï¥„ÄÅÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰ª•ÂèäÁΩëÁªúÁªìÊûÑÁöÑÈÄâÊã©„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÈááÁî®‰∫ÜÈÄÇÂ∫îÊÄßÂ≠¶‰π†Áéá‰ª•Â∫îÂØπ‰∏çÂêå‰ªªÂä°ÁöÑÂ§çÊùÇÊÄßÔºåÂπ∂ÈÄöËøáÂÆûÈ™åÁ°ÆÂÆö‰∫ÜÊúÄ‰Ω≥Ë∂ÖÂèÇÊï∞ÁªÑÂêà„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåADPOÂú®Ê†áÂáÜÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÂÖ≠ÁßçÂü∫ÂáÜÊâ©Êï£RLÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞15%‰ª•‰∏äÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂæÆË∞ÉËøáÁ®ã‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÁ®≥ÂÆöÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êú∫Âô®‰∫∫ÊìçÊéß„ÄÅËá™Âä®ÂåñÁîü‰∫ßÁ∫øÂíåÊô∫ËÉΩÂÆ∂Â±ÖÁ≠â„ÄÇÈÄöËøáÊèêÂçáÊâ©Êï£Á≠ñÁï•ÁöÑ‰ºòÂåñÊïàÁéáÔºåADPOÂèØ‰ª•Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑ‰ªªÂä°ÊâßË°åÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent studies have shown the great potential of diffusion models in improving reinforcement learning (RL) by modeling complex policies, expressing a high degree of multi-modality, and efficiently handling high-dimensional continuous control tasks. However, there is currently limited research on how to optimize diffusion-based polices (e.g., Diffusion Policy) fast and stably. In this paper, we propose an Adam-based Diffusion Policy Optimization (ADPO), a fast algorithmic framework containing best practices for fine-tuning diffusion-based polices in robotic control tasks using the adaptive gradient descent method in RL. Adaptive gradient method is less studied in training RL, let alone diffusion-based policies. We confirm that ADPO outperforms other diffusion-based RL methods in terms of overall effectiveness for fine-tuning on standard robotic tasks. Concretely, we conduct extensive experiments on standard robotic control tasks to test ADPO, where, particularly, six popular diffusion-based RL methods are provided as benchmark methods. Experimental results show that ADPO acquires better or comparable performance than the baseline methods. Finally, we systematically analyze the sensitivity of multiple hyperparameters in standard robotics tasks, providing guidance for subsequent practical applications. Our video demonstrations are released in https://github.com/Timeless-lab/ADPO.git.

