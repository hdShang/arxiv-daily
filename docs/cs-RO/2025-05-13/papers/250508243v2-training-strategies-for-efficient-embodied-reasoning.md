---
layout: default
title: Training Strategies for Efficient Embodied Reasoning
---

# Training Strategies for Efficient Embodied Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08243" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08243v2</a>
  <a href="https://arxiv.org/pdf/2505.08243.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08243v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08243v2', 'Training Strategies for Efficient Embodied Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, Sergey Levine

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13 (æ›´æ–°: 2025-05-17)

**å¤‡æ³¨**: Updated figure layout, added project page link

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ–°ç­–ç•¥ä»¥æå‡æœºå™¨äººæ¨ç†æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ¨ç†` `é“¾å¼æ¨ç†` `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `æ€§èƒ½æå‡` `æ¨ç†é€Ÿåº¦ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æœºå™¨äººæ¨ç†æ–¹æ³•åœ¨æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™ï¼Œå°¤å…¶ä¾èµ–äºä¸“é—¨çš„æ•°æ®å’Œè¾ƒæ…¢çš„æ¨ç†é€Ÿåº¦ã€‚
2. è®ºæ–‡æå‡ºé€šè¿‡æ”¹è¿›è¡¨ç¤ºå­¦ä¹ ã€å­¦ä¹ è¯¾ç¨‹å’Œè¡¨è¾¾èƒ½åŠ›æ¥æå‡æœºå™¨äººç­–ç•¥æ€§èƒ½ï¼Œè®¾è®¡äº†ç®€å•çš„æ¨ç†å˜ä½“ä»¥éªŒè¯å‡è®¾ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨LIBERO-90åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å®ç°äº†æ¨ç†é€Ÿåº¦çš„ä¸‰å€åŠ é€Ÿã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨äººé“¾å¼æ¨ç†ï¼ˆCoTï¼‰æ˜¯ä¸€ç§é€šè¿‡é¢„æµ‹æœ‰åŠ©çš„ä¸­é—´è¡¨ç¤ºæ¥é€‰æ‹©åŠ¨ä½œçš„æœ‰æ•ˆæ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰ã€‚å°½ç®¡è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜èƒ½æé«˜æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨ä¸€äº›æ ¸å¿ƒé™åˆ¶ï¼Œå¦‚éœ€è¦ä¸“é—¨çš„æœºå™¨äººæ¨ç†æ•°æ®å’Œè¾ƒæ…¢çš„æ¨ç†é€Ÿåº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†æ–°çš„æœºå™¨äººæ¨ç†æ–¹æ³•ï¼Œå¹¶å‡è®¾æ¨ç†å¯ä»¥é€šè¿‡æ›´å¥½çš„è¡¨ç¤ºå­¦ä¹ ã€æ”¹è¿›çš„å­¦ä¹ è¯¾ç¨‹å’Œå¢å¼ºçš„è¡¨è¾¾èƒ½åŠ›æ¥æ”¹å–„ç­–ç•¥æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå­¦ä¹ ç”Ÿæˆæ¨ç†ç¡®å®èƒ½æé«˜VLAçš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶å…³æ³¨æ¨ç†æœ‰åŠ©äºæ›´å¥½åœ°åˆ©ç”¨è¿™äº›ç‰¹å¾è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ã€‚æå‡ºçš„è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆåœ¨LIBERO-90åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å®ç°äº†æ¨ç†é€Ÿåº¦çš„ä¸‰å€åŠ é€Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººæ¨ç†æ–¹æ³•åœ¨æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¯¹ä¸“é—¨æ•°æ®çš„ä¾èµ–å’Œæ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å‡è®¾æ¨ç†å¯ä»¥æ”¹å–„ç­–ç•¥æ€§èƒ½çš„å¤šç§æœºåˆ¶ï¼Œè®¾è®¡ç®€å•çš„æœºå™¨äººé“¾å¼æ¨ç†å˜ä½“ï¼Œä»¥éªŒè¯è¿™äº›å‡è®¾å¹¶ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè¡¨ç¤ºå­¦ä¹ æ¨¡å—ã€å­¦ä¹ è¯¾ç¨‹æ¨¡å—å’Œè¡¨è¾¾èƒ½åŠ›æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—é’ˆå¯¹ä¸åŒçš„æ¨ç†æœºåˆ¶è¿›è¡Œä¼˜åŒ–å’Œæµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†è½»é‡çº§çš„æ›¿ä»£æ¨ç†æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨ä¸éœ€è¦å¤§é‡ä¸“é—¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†æœºå™¨äººç­–ç•¥çš„æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿æ¨ç†è¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„è½»é‡çº§æ¨ç†æ–¹æ³•åœ¨LIBERO-90åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¾ƒéæ¨ç†ç­–ç•¥æé«˜äº†æ€§èƒ½ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦å®ç°äº†ä¸‰å€çš„åŠ é€Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»æœºå™¨äººã€æ™ºèƒ½å®¶å±…ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººæ¨ç†çš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œå¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æ›´æ™ºèƒ½çš„å†³ç­–å’Œæ“ä½œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpful intermediate representations before choosing actions -- provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies -- (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity -- then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning.

