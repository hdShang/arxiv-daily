---
layout: default
title: Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery
---

# Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08932" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08932v1</a>
  <a href="https://arxiv.org/pdf/2505.08932.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08932v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08932v1', 'Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohammad Wasil, Ahmad Drak, Brennan Penfold, Ludovico Scarton, Maximilian Johenneken, Alexander Asteroth, Sebastian Houben

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: Accepted to the Novel Approaches for Precision Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä»¥è§£å†³æ— äººæœºå½±åƒæ£®æ—åœ°é¢åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ— äººæœºå½±åƒ` `æ£®æ—åœ°é¢åˆ†å‰²` `å‚æ•°é«˜æ•ˆå¾®è°ƒ` `Segment Anything Model` `ç”Ÿæ€ç›‘æµ‹` `è‡ªåŠ¨åŒ–åˆ†å‰²` `ä½ç§©é€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ£®æ—åœ°é¢ç‰©ä½“åˆ†å‰²ä¸­é¢ä¸´é«˜è‡ªç„¶å˜å¼‚æ€§å’Œç¯å¢ƒå‚æ•°å˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ³¨é‡Šæ¨¡ç³Šã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹Segment Anything Model (SAM)è¿›è¡Œè°ƒæ•´ï¼Œä»¥å®ç°è‡ªåŠ¨åŒ–çš„æ£®æ—åœ°é¢ç‰©ä½“åˆ†å‰²ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€‚é…å™¨åŸºäºPEFTæ–¹æ³•åœ¨å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰ä¸Šè¡¨ç°æœ€ä½³ï¼Œä¸”ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æä¾›äº†è½»é‡çº§çš„æ›¿ä»£æ–¹æ¡ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ— äººæœºåœ¨æ¤æ ‘é€ æ—å’Œæ£®æ—ç›‘æµ‹ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œæ£®æ—åœ°é¢çš„è¯¦ç»†ç†è§£ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè‡ªç„¶å˜å¼‚æ€§é«˜ã€ç¯å¢ƒå‚æ•°å¿«é€Ÿå˜åŒ–ä»¥åŠæ¨¡ç³Šçš„æ³¨é‡Šå®šä¹‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¯¹Segment Anything Model (SAM)è¿›è¡Œäº†é€‚åº”æ€§è°ƒæ•´ï¼Œä»¥å®ç°å¯¹æ ‘æ¡©ã€æ¤è¢«å’Œæœ¨è´¨æ®‹éª¸ç­‰æ£®æ—åœ°é¢ç‰©ä½“çš„åˆ†å‰²ã€‚æˆ‘ä»¬é‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œä»…å¾®è°ƒå°‘é‡é¢å¤–æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶ä¿æŒåŸå§‹æƒé‡ä¸å˜ã€‚é€šè¿‡è°ƒæ•´SAMçš„æ©ç è§£ç å™¨ç”Ÿæˆä¸æ•°æ®é›†ç±»åˆ«ç›¸å¯¹åº”çš„æ©ç ï¼Œå®ç°äº†æ— éœ€æ‰‹åŠ¨æç¤ºçš„è‡ªåŠ¨åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºé€‚é…å™¨çš„PEFTæ–¹æ³•åœ¨å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä½œä¸ºè½»é‡çº§æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚åˆèµ„æºå—é™çš„æ— äººæœºå¹³å°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— äººæœºå½±åƒä¸­æ£®æ—åœ°é¢ç‰©ä½“çš„åˆ†å‰²é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç”±äºè‡ªç„¶å˜å¼‚æ€§å’Œç¯å¢ƒå˜åŒ–å¯¼è‡´åˆ†å‰²æ•ˆæœä¸ä½³ï¼Œä¸”æ³¨é‡Šå®šä¹‰æ¨¡ç³Šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹Segment Anything Model (SAM)è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ï¼Œä»…è°ƒæ•´å°‘é‡å‚æ•°ä»¥é€‚åº”ç‰¹å®šçš„æ£®æ—åœ°é¢ç‰©ä½“åˆ†å‰²ä»»åŠ¡ï¼Œä»è€Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€SAMæ¨¡å‹çš„é€‚åº”æ€§è°ƒæ•´ã€æ©ç è§£ç å™¨çš„ä¿®æ”¹ä»¥åŠæœ€ç»ˆçš„åˆ†å‰²ç»“æœç”Ÿæˆã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¾®è°ƒå’Œç»“æœè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé‡‡ç”¨é€‚é…å™¨æœºåˆ¶è¿›è¡ŒPEFTï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿æŒåŸå§‹æƒé‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”æ–°çš„åˆ†å‰²ä»»åŠ¡ï¼Œä¸ä¼ ç»Ÿçš„å…¨å‚æ•°å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–åˆ†å‰²æ•ˆæœï¼Œå¹¶å¯¹SAMçš„æ©ç è§£ç å™¨è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ©ç èƒ½å¤Ÿå‡†ç¡®å¯¹åº”æ•°æ®é›†ä¸­çš„ç±»åˆ«ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºé€‚é…å™¨çš„PEFTæ–¹æ³•åœ¨å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰ä¸Šè¾¾åˆ°äº†æœ€é«˜å€¼ï¼Œå…·ä½“æ•°å€¼ä¸ºXX%ï¼Œç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æå‡äº†YY%ã€‚æ­¤å¤–ï¼Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•åœ¨å‚æ•°æ•°é‡ä¸Šæ›´ä¸ºè½»é‡ï¼Œé€‚åˆèµ„æºå—é™çš„æ— äººæœºå¹³å°ä½¿ç”¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ£®æ—ç›‘æµ‹ã€ç”Ÿæ€æ¢å¤å’Œç¯å¢ƒä¿æŠ¤ç­‰ã€‚é€šè¿‡æé«˜æ£®æ—åœ°é¢ç‰©ä½“çš„è‡ªåŠ¨åˆ†å‰²ç²¾åº¦ï¼Œèƒ½å¤Ÿä¸ºç”Ÿæ€å­¦ç ”ç©¶å’Œèµ„æºç®¡ç†æä¾›æ›´ä¸ºå‡†ç¡®çš„æ•°æ®æ”¯æŒï¼Œè¿›è€Œæ¨åŠ¨å¯æŒç»­å‘å±•ç›®æ ‡çš„å®ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and forest monitoring, including seed dispersal in hard-to-reach terrains. However, a detailed understanding of the forest floor remains a challenge due to high natural variability, quickly changing environmental parameters, and ambiguous annotations due to unclear definitions. To address this issue, we adapt the Segment Anything Model (SAM), a vision foundation model with strong generalization capabilities, to segment forest floor objects such as tree stumps, vegetation, and woody debris. To this end, we employ parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of additional model parameters while keeping the original weights fixed. We adjust SAM's mask decoder to generate masks corresponding to our dataset categories, allowing for automatic segmentation without manual prompting. Our results show that the adapter-based PEFT method achieves the highest mean intersection over union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a lightweight alternative for resource-constrained UAV platforms.

