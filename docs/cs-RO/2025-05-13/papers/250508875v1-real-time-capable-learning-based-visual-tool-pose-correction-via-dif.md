---
layout: default
title: Real-time Capable Learning-based Visual Tool Pose Correction via Differentiable Simulation
---

# Real-time Capable Learning-based Visual Tool Pose Correction via Differentiable Simulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08875" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08875v1</a>
  <a href="https://arxiv.org/pdf/2505.08875.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08875v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08875v1', 'Real-time Capable Learning-based Visual Tool Pose Correction via Differentiable Simulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuyuan Yang, Zonghe Chua

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¯å¾®ä»¿çœŸçš„è§†è§‰å·¥å…·å§¿æ€æ ¡æ­£æ–¹æ³•ä»¥è§£å†³æ‰‹æœ¯è‡ªä¸»æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å¾®åˆ›æœºå™¨äººæ‰‹æœ¯` `å§¿æ€ä¼°è®¡` `è§†è§‰å˜æ¢å™¨` `å¯å¾®ä»¿çœŸ` `å®æ—¶å¤„ç†` `è‡ªåŠ¨åŒ–åŒ»ç–—` `æœºå™¨äººæŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¾®åˆ›æœºå™¨äººæ‰‹æœ¯ä¸­ï¼Œæœ«ç«¯æ‰§è¡Œå™¨çš„æœ¬ä½“æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´è‡ªä¸»æ§åˆ¶çš„å‡†ç¡®æ€§éš¾ä»¥ä¿è¯ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å˜æ¢å™¨çš„å®æ—¶å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡å¯å¾®è¿åŠ¨å­¦å’Œä»¿çœŸæ¸²æŸ“è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ ¡æ­£å™ªå£°å§¿æ€ä¼°è®¡ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„ä»¿çœŸæ•ˆæœï¼Œå¹¶ä¸ºæœªæ¥çš„å®é™…åº”ç”¨å¥ å®šåŸºç¡€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¾®åˆ›æœºå™¨äººæ‰‹æœ¯ï¼ˆMIRSï¼‰ä¸­ï¼Œå®ç°è‡ªä¸»æ§åˆ¶çš„å‡†ç¡®æ€§é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæœ«ç«¯æ‰§è¡Œå™¨çš„æœ¬ä½“æ„ŸçŸ¥ä¸è¶³ã€‚å°½ç®¡æœºå™¨äººé…å¤‡äº†å…³èŠ‚ç¼–ç å™¨ç”¨äºå§¿æ€è®¡ç®—ï¼Œä½†ç”±äºå„ç§éç†æƒ³å› ç´ ï¼Œæ•´ä¸ªè¿åŠ¨å­¦é“¾çš„å‡†ç¡®æ€§å—åˆ°å½±å“ã€‚ç°æœ‰çš„åŸºäºè§†è§‰çš„å§¿æ€ä¼°è®¡æ–¹æ³•ç¼ºä¹å®æ—¶èƒ½åŠ›ï¼Œæˆ–éš¾ä»¥è®­ç»ƒå’Œæ¨å¹¿ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å˜æ¢å™¨çš„å®æ—¶å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¯å¾®è¿åŠ¨å­¦å’Œä»¿çœŸæ¸²æŸ“è¿›è¡Œè®­ç»ƒï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ä»¿çœŸä¸­æ ¡æ­£å™ªå£°å§¿æ€ä¼°è®¡çš„æ½œåŠ›ï¼Œæ—¨åœ¨éªŒè¯å…¶ä»ä»¿çœŸåˆ°ç°å®çš„å¯è½¬ç§»æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¾®åˆ›æœºå™¨äººæ‰‹æœ¯ä¸­æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€ä¼°è®¡ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å®æ—¶æ€§å’Œè®­ç»ƒæ¨å¹¿èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…æ‰‹æœ¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å˜æ¢å™¨çš„å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œåˆ©ç”¨å¯å¾®è¿åŠ¨å­¦å’Œä»¿çœŸæ¸²æŸ“è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä»¥æé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é‡‡é›†ã€å¯å¾®è¿åŠ¨å­¦æ¨¡å‹ã€è§†è§‰å˜æ¢å™¨ç½‘ç»œå’Œå§¿æ€æ ¡æ­£æ¨¡å—ã€‚é€šè¿‡ä»¿çœŸç¯å¢ƒç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†å¯å¾®ä»¿çœŸä¸è§†è§‰å˜æ¢å™¨ç»“åˆï¼Œå®ç°äº†å®æ—¶å§¿æ€ä¼°è®¡çš„èƒ½åŠ›ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„ä¸Šï¼Œé‡‡ç”¨äº†è§†è§‰å˜æ¢å™¨ä»¥æé«˜ç‰¹å¾æå–èƒ½åŠ›ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šè€ƒè™‘äº†å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ä¸Šç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ ¡æ­£å™ªå£°å½±å“ï¼Œä¸”åœ¨ä»¿çœŸç¯å¢ƒä¸­å®ç°äº†å®æ—¶å¤„ç†ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¾®åˆ›æ‰‹æœ¯ã€æœºå™¨äººè¾…åŠ©æ²»ç–—å’Œè‡ªåŠ¨åŒ–åŒ»ç–—è®¾å¤‡ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ‰‹æœ¯æ•ˆç‡ï¼Œå‡è½»å¤–ç§‘åŒ»ç”Ÿçš„è®¤çŸ¥è´Ÿæ‹…ï¼Œæ¨åŠ¨åŒ»ç–—æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomy in Minimally Invasive Robotic Surgery (MIRS) has the potential to reduce surgeon cognitive and task load, thereby increasing procedural efficiency. However, implementing accurate autonomous control can be difficult due to poor end-effector proprioception, a limitation of their cable-driven mechanisms. Although the robot may have joint encoders for the end-effector pose calculation, various non-idealities make the entire kinematics chain inaccurate. Modern vision-based pose estimation methods lack real-time capability or can be hard to train and generalize. In this work, we demonstrate a real-time capable, vision transformer-based pose estimation approach that is trained using end-to-end differentiable kinematics and rendering in simulation. We demonstrate the potential of this method to correct for noisy pose estimates in simulation, with the longer term goal of verifying the sim-to-real transferability of our approach.

