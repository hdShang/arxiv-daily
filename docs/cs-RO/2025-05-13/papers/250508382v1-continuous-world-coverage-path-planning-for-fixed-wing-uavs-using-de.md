---
layout: default
title: Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning
---

# Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08382" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08382v1</a>
  <a href="https://arxiv.org/pdf/2505.08382.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08382v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08382v1', 'Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mirco Theile, Andres R. Zapata Rodriguez, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli

**åˆ†ç±»**: cs.RO, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-05-13

**å¤‡æ³¨**: Submitted to IROS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å›ºå®šç¿¼æ— äººæœºè¿ç»­è¦†ç›–è·¯å¾„è§„åˆ’æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ— äººæœº` `è¦†ç›–è·¯å¾„è§„åˆ’` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `èƒ½æ•ˆä¼˜åŒ–` `BÃ©zieræ›²çº¿` `è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ` `è¿ç»­è¿åŠ¨è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ— äººæœºè¦†ç›–è·¯å¾„è§„åˆ’æ–¹æ³•å¤šä¾èµ–ç¦»æ•£ç½‘æ ¼ï¼Œæ— æ³•æ»¡è¶³ç°å®ä¸­å¯¹è¿ç»­è¿åŠ¨è§„åˆ’çš„éœ€æ±‚ï¼Œå¯¼è‡´èƒ½è€—é«˜æ•ˆæ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è¿ç»­è¦†ç›–è·¯å¾„è§„åˆ’æ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡ç¯å¢ƒå’Œæ— äººæœºè¿åŠ¨æ¥ä¼˜åŒ–èƒ½è€—ï¼Œç¡®ä¿è¦†ç›–å®Œæ•´æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¨‹åºç”Ÿæˆå’Œæ‰‹å·¥è®¾è®¡çš„åœºæ™¯ä¸­å‡èƒ½æœ‰æ•ˆå­¦ä¹ åˆ°èƒ½æ•ˆé«˜çš„è¦†ç›–ç­–ç•¥ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ— äººæœºè¦†ç›–è·¯å¾„è§„åˆ’ï¼ˆCPPï¼‰åœ¨ç²¾å‡†å†œä¸šå’Œæœç´¢æ•‘æ´ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç¦»æ•£ç½‘æ ¼è¡¨ç¤ºï¼Œè€Œç°å®ä¸­çš„æ— äººæœºæ“ä½œéœ€è¦é«˜æ•ˆçš„è¿ç»­è¿åŠ¨è§„åˆ’ã€‚æœ¬æ–‡å°†æ— äººæœºCPPé—®é¢˜åœ¨è¿ç»­ç¯å¢ƒä¸­è¿›è¡Œå»ºæ¨¡ï¼Œæ—¨åœ¨æœ€å°åŒ–èƒ½è€—å¹¶ç¡®ä¿å®Œå…¨è¦†ç›–ã€‚æˆ‘ä»¬é‡‡ç”¨å¯å˜å¤§å°çš„è½´å¯¹é½çŸ©å½¢æ¨¡å‹ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨æ›²ç‡çº¦æŸçš„BÃ©zieræ›²çº¿æè¿°æ— äººæœºè¿åŠ¨ã€‚é€šè¿‡ä½¿ç”¨åŸºäºåŠ¨ä½œæ˜ å°„çš„è½¯æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆAM-SACï¼‰ç®—æ³•è®­ç»ƒå¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”è¯¾ç¨‹ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å­¦ä¹ èƒ½æ•ˆè¦†ç›–ç­–ç•¥æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ— äººæœºåœ¨è¿ç»­ç¯å¢ƒä¸­è¿›è¡Œè¦†ç›–è·¯å¾„è§„åˆ’çš„é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•çš„ç¦»æ•£ç½‘æ ¼è¡¨ç¤ºé™åˆ¶äº†èƒ½æ•ˆå’Œçµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†ç¯å¢ƒå»ºæ¨¡ä¸ºå¯å˜å¤§å°çš„è½´å¯¹é½çŸ©å½¢ï¼Œå¹¶ä½¿ç”¨æ›²ç‡çº¦æŸçš„BÃ©zieræ›²çº¿æè¿°æ— äººæœºè¿åŠ¨ï¼Œä¼˜åŒ–èƒ½è€—å¹¶ç¡®ä¿è¦†ç›–å®Œæ•´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬ç¯å¢ƒå»ºæ¨¡ã€è¿åŠ¨è§„åˆ’å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚ç¯å¢ƒå»ºæ¨¡è´Ÿè´£ç”Ÿæˆå¯å˜å¤§å°çš„çŸ©å½¢ï¼Œè¿åŠ¨è§„åˆ’ä½¿ç”¨BÃ©zieræ›²çº¿ï¼Œå¼ºåŒ–å­¦ä¹ åˆ™é€šè¿‡AM-SACç®—æ³•è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸è¿ç»­è·¯å¾„è§„åˆ’ç›¸ç»“åˆï¼Œå…‹æœäº†ä¼ ç»Ÿç¦»æ•£æ–¹æ³•çš„å±€é™æ€§ï¼Œå®ç°äº†èƒ½æ•ˆä¼˜åŒ–å’Œå®Œæ•´è¦†ç›–çš„åŒé‡ç›®æ ‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä¼˜åŒ–äº†åŠ¨ä½œæ˜ å°„ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´äº†æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å’Œç­–ç•¥çš„èƒ½æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨èƒ½æ•ˆè¦†ç›–ç­–ç•¥å­¦ä¹ ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨ç¨‹åºç”Ÿæˆå’Œæ‰‹å·¥è®¾è®¡çš„åœºæ™¯ä¸­ï¼Œèƒ½è€—å‡å°‘å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œè¦†ç›–ç‡æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç²¾å‡†å†œä¸šã€ç¯å¢ƒç›‘æµ‹ã€ç¾åæœç´¢ä¸æ•‘æ´ç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ— äººæœºçš„è¦†ç›–è·¯å¾„è§„åˆ’ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ä»»åŠ¡æ‰§è¡Œçš„æ•ˆç‡å’Œèƒ½æ•ˆï¼Œé™ä½æ“ä½œæˆæœ¬ï¼Œæ¨åŠ¨æ— äººæœºæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Unmanned Aerial Vehicle (UAV) Coverage Path Planning (CPP) is critical for applications such as precision agriculture and search and rescue. While traditional methods rely on discrete grid-based representations, real-world UAV operations require power-efficient continuous motion planning. We formulate the UAV CPP problem in a continuous environment, minimizing power consumption while ensuring complete coverage. Our approach models the environment with variable-size axis-aligned rectangles and UAV motion with curvature-constrained BÃ©zier curves. We train a reinforcement learning agent using an action-mapping-based Soft Actor-Critic (AM-SAC) algorithm employing a self-adaptive curriculum. Experiments on both procedurally generated and hand-crafted scenarios demonstrate the effectiveness of our method in learning energy-efficient coverage strategies.

