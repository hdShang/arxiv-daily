---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-13
---

# cs.ROï¼ˆ2025-05-13ï¼‰

ğŸ“Š å…± **31** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (20 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion" class="interest-badge">æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (20 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250508712v2-navdp-learning-sim-to-real-navigation-diffusion-policy-with-privileg.html">NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance</a></td>
  <td>æå‡ºNavDPä»¥è§£å†³åŠ¨æ€å¼€æ”¾ä¸–ç•Œå¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08712v2" data-paper-url="./papers/250508712v2-navdp-learning-sim-to-real-navigation-diffusion-policy-with-privileg.html" onclick="toggleFavorite(this, '2505.08712v2', 'NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250508510v2-foci-trajectory-optimization-on-gaussian-splats.html">FOCI: Trajectory Optimization on Gaussian Splats</a></td>
  <td>æå‡ºFOCIç®—æ³•ä»¥ä¼˜åŒ–é«˜æ–¯ç‚¹äº‘ä¸Šçš„æœºå™¨äººè½¨è¿¹</td>
  <td class="tags-cell"><span class="paper-tag">legged robot</span> <span class="paper-tag">trajectory optimization</span> <span class="paper-tag">ANYmal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08510v2" data-paper-url="./papers/250508510v2-foci-trajectory-optimization-on-gaussian-splats.html" onclick="toggleFavorite(this, '2505.08510v2', 'FOCI: Trajectory Optimization on Gaussian Splats')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250508949v1-multi-step-manipulation-task-and-motion-planning-guided-by-video-dem.html">Multi-step manipulation task and motion planning guided by video demonstration</a></td>
  <td>æå‡ºè§†é¢‘å¼•å¯¼çš„å¤šæ­¥éª¤æ“ä½œä¸è¿åŠ¨è§„åˆ’æ–¹æ³•ä»¥è§£å†³å¤æ‚ä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">motion planning</span> <span class="paper-tag">task and motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08949v1" data-paper-url="./papers/250508949v1-multi-step-manipulation-task-and-motion-planning-guided-by-video-dem.html" onclick="toggleFavorite(this, '2505.08949v1', 'Multi-step manipulation task and motion planning guided by video demonstration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250508574v1-end-to-end-multi-task-policy-learning-from-nmpc-for-quadruped-locomo.html">End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion</a></td>
  <td>æå‡ºå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08574v1" data-paper-url="./papers/250508574v1-end-to-end-multi-task-policy-learning-from-nmpc-for-quadruped-locomo.html" onclick="toggleFavorite(this, '2505.08574v1', 'End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250511528v6-ladi-wm-a-latent-diffusion-based-world-model-for-predictive-manipula.html">LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation</a></td>
  <td>æå‡ºLaDi-WMä»¥è§£å†³æœºå™¨äººé¢„æµ‹æ“æ§ä¸­çš„è§†è§‰çŠ¶æ€ç”Ÿæˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">diffusion policy</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.11528v6" data-paper-url="./papers/250511528v6-ladi-wm-a-latent-diffusion-based-world-model-for-predictive-manipula.html" onclick="toggleFavorite(this, '2505.11528v6', 'LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250508548v2-from-seeing-to-doing-bridging-reasoning-and-decision-for-robotic-man.html">From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation</a></td>
  <td>æå‡ºFSDæ¨¡å‹ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„é›¶-shotæ³›åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">spatial relationship</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08548v2" data-paper-url="./papers/250508548v2-from-seeing-to-doing-bridging-reasoning-and-decision-for-robotic-man.html" onclick="toggleFavorite(this, '2505.08548v2', 'From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250508986v1-chicgrasp-imitation-learning-based-customized-dual-jaw-gripper-contr.html">ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation</a></td>
  <td>æå‡ºChicGraspä»¥è§£å†³ç”Ÿç‰©äº§å“æ“æ§ä¸­çš„æŠ“å–éš¾é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08986v1" data-paper-url="./papers/250508986v1-chicgrasp-imitation-learning-based-customized-dual-jaw-gripper-contr.html" onclick="toggleFavorite(this, '2505.08986v1', 'ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250508458v1-zero-shot-sim-to-real-reinforcement-learning-for-fruit-harvesting.html">Zero-Shot Sim-to-Real Reinforcement Learning for Fruit Harvesting</a></td>
  <td>æå‡ºé›¶æ ·æœ¬ä»¿çœŸåˆ°ç°å®çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥è§£å†³æ°´æœé‡‡æ‘˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">domain randomization</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08458v1" data-paper-url="./papers/250508458v1-zero-shot-sim-to-real-reinforcement-learning-for-fruit-harvesting.html" onclick="toggleFavorite(this, '2505.08458v1', 'Zero-Shot Sim-to-Real Reinforcement Learning for Fruit Harvesting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250508627v2-augmented-reality-for-robots-arro-pointing-visuomotor-policies-towar.html">Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies Towards Visual Robustness</a></td>
  <td>æå‡ºARROä»¥è§£å†³æœºå™¨äººè§†è§‰é²æ£’æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">diffusion policy</span> <span class="paper-tag">open-vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08627v2" data-paper-url="./papers/250508627v2-augmented-reality-for-robots-arro-pointing-visuomotor-policies-towar.html" onclick="toggleFavorite(this, '2505.08627v2', 'Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies Towards Visual Robustness')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250508382v1-continuous-world-coverage-path-planning-for-fixed-wing-uavs-using-de.html">Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å›ºå®šç¿¼æ— äººæœºè¿ç»­è¦†ç›–è·¯å¾„è§„åˆ’æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08382v1" data-paper-url="./papers/250508382v1-continuous-world-coverage-path-planning-for-fixed-wing-uavs-using-de.html" onclick="toggleFavorite(this, '2505.08382v1', 'Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250508376v1-adaptive-diffusion-policy-optimization-for-robotic-manipulation.html">Adaptive Diffusion Policy Optimization for Robotic Manipulation</a></td>
  <td>æå‡ºè‡ªé€‚åº”æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ä»¥æå‡æœºå™¨äººæ“æ§æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08376v1" data-paper-url="./papers/250508376v1-adaptive-diffusion-policy-optimization-for-robotic-manipulation.html" onclick="toggleFavorite(this, '2505.08376v1', 'Adaptive Diffusion Policy Optimization for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250508264v2-automatic-curriculum-learning-for-driving-scenarios-towards-robust-a.html">Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning</a></td>
  <td>æå‡ºè‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ä»¥è§£å†³è‡ªä¸»é©¾é©¶è®­ç»ƒæ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">domain randomization</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">curriculum learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08264v2" data-paper-url="./papers/250508264v2-automatic-curriculum-learning-for-driving-scenarios-towards-robust-a.html" onclick="toggleFavorite(this, '2505.08264v2', 'Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250508213v1-handcept-a-visual-inertial-fusion-framework-for-accurate-propriocept.html">HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands</a></td>
  <td>æå‡ºHandCeptä»¥è§£å†³çµå·§æ‰‹çš„æœ¬ä½“æ„ŸçŸ¥é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">sim-to-real</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08213v1" data-paper-url="./papers/250508213v1-handcept-a-visual-inertial-fusion-framework-for-accurate-propriocept.html" onclick="toggleFavorite(this, '2505.08213v1', 'HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250508875v1-real-time-capable-learning-based-visual-tool-pose-correction-via-dif.html">Real-time Capable Learning-based Visual Tool Pose Correction via Differentiable Simulation</a></td>
  <td>æå‡ºåŸºäºå¯å¾®ä»¿çœŸçš„è§†è§‰å·¥å…·å§¿æ€æ ¡æ­£æ–¹æ³•ä»¥è§£å†³æ‰‹æœ¯è‡ªä¸»æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">differentiable simulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08875v1" data-paper-url="./papers/250508875v1-real-time-capable-learning-based-visual-tool-pose-correction-via-dif.html" onclick="toggleFavorite(this, '2505.08875v1', 'Real-time Capable Learning-based Visual Tool Pose Correction via Differentiable Simulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250508194v1-cltp-contrastive-language-tactile-pre-training-for-3d-contact-geomet.html">CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding</a></td>
  <td>æå‡ºCLTPæ¡†æ¶ä»¥è§£å†³æœºå™¨äººè§¦è§‰è¯­è¨€ç†è§£ä¸­çš„æ¥è§¦çŠ¶æ€é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08194v1" data-paper-url="./papers/250508194v1-cltp-contrastive-language-tactile-pre-training-for-3d-contact-geomet.html" onclick="toggleFavorite(this, '2505.08194v1', 'CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>16</td>
  <td><a href="./papers/250508238v1-motion-control-of-high-dimensional-musculoskeletal-systems-with-hier.html">Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning</a></td>
  <td>æå‡ºMPC^2ä»¥è§£å†³é«˜ç»´è‚Œè‚‰éª¨éª¼ç³»ç»Ÿçš„è¿åŠ¨æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08238v1" data-paper-url="./papers/250508238v1-motion-control-of-high-dimensional-musculoskeletal-systems-with-hier.html" onclick="toggleFavorite(this, '2505.08238v1', 'Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/250508216v3-rethink-repeatable-measures-of-robot-performance-with-statistical-qu.html">Rethink Repeatable Measures of Robot Performance with Statistical Query</a></td>
  <td>æå‡ºè½»é‡åŒ–ç»Ÿè®¡æŸ¥è¯¢ç®—æ³•ä»¥è§£å†³æœºå™¨äººæ€§èƒ½é‡å¤æ€§æµ‹è¯•é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08216v3" data-paper-url="./papers/250508216v3-rethink-repeatable-measures-of-robot-performance-with-statistical-qu.html" onclick="toggleFavorite(this, '2505.08216v3', 'Rethink Repeatable Measures of Robot Performance with Statistical Query')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/250508444v2-extracting-visual-plans-from-unlabeled-videos-via-symbolic-guidance.html">Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance</a></td>
  <td>æå‡ºVis2Planä»¥è§£å†³é•¿æ—¶é—´æ“ä½œä»»åŠ¡ä¸­çš„è§†è§‰è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08444v2" data-paper-url="./papers/250508444v2-extracting-visual-plans-from-unlabeled-videos-via-symbolic-guidance.html" onclick="toggleFavorite(this, '2505.08444v2', 'Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/250508367v1-ma-roesl-motion-aware-rapid-reward-optimization-for-efficient-robot-.html">MA-ROESL: Motion-aware Rapid Reward Optimization for Efficient Robot Skill Learning from Single Videos</a></td>
  <td>æå‡ºMA-ROESLä»¥è§£å†³æœºå™¨äººæŠ€èƒ½å­¦ä¹ ä¸­çš„ä½æ•ˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span> <span class="paper-tag">reward design</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08367v1" data-paper-url="./papers/250508367v1-ma-roesl-motion-aware-rapid-reward-optimization-for-efficient-robot-.html" onclick="toggleFavorite(this, '2505.08367v1', 'MA-ROESL: Motion-aware Rapid Reward Optimization for Efficient Robot Skill Learning from Single Videos')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/250508853v1-efficiently-manipulating-clutter-via-learning-and-search-based-reaso.html">Efficiently Manipulating Clutter via Learning and Search-Based Reasoning</a></td>
  <td>æå‡ºé«˜æ•ˆæ“æ§æ‚ç‰©çš„æ–°ç®—æ³•ä»¥è§£å†³æœºå™¨äººç‰©ä½“é‡æ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08853v1" data-paper-url="./papers/250508853v1-efficiently-manipulating-clutter-via-learning-and-search-based-reaso.html" onclick="toggleFavorite(this, '2505.08853v1', 'Efficiently Manipulating Clutter via Learning and Search-Based Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/250508243v2-training-strategies-for-efficient-embodied-reasoning.html">Training Strategies for Efficient Embodied Reasoning</a></td>
  <td>æå‡ºæ–°ç­–ç•¥ä»¥æå‡æœºå™¨äººæ¨ç†æ•ˆç‡</td>
  <td class="tags-cell"><span class="paper-tag">representation learning</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">VLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08243v2" data-paper-url="./papers/250508243v2-training-strategies-for-efficient-embodied-reasoning.html" onclick="toggleFavorite(this, '2505.08243v2', 'Training Strategies for Efficient Embodied Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/250508222v2-scaling-multi-agent-reinforcement-learning-for-underwater-acoustic-t.html">Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles</a></td>
  <td>æå‡ºè¿­ä»£è’¸é¦æ–¹æ³•ä»¥è§£å†³æ°´ä¸‹å£°å­¦è·Ÿè¸ªä¸­çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">curriculum learning</span> <span class="paper-tag">distillation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08222v2" data-paper-url="./papers/250508222v2-scaling-multi-agent-reinforcement-learning-for-underwater-acoustic-t.html" onclick="toggleFavorite(this, '2505.08222v2', 'Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/250508453v1-parameter-estimation-using-reinforcement-learning-causal-curiosity-l.html">Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges</a></td>
  <td>æå‡ºå› æœå¥½å¥‡å¿ƒå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥ä¼˜åŒ–å‚æ•°ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08453v1" data-paper-url="./papers/250508453v1-parameter-estimation-using-reinforcement-learning-causal-curiosity-l.html" onclick="toggleFavorite(this, '2505.08453v1', 'Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>24</td>
  <td><a href="./papers/250508223v1-reinforcement-learning-based-fault-tolerant-control-for-quadrotor-wi.html">Reinforcement Learning-based Fault-Tolerant Control for Quadrotor with Online Transformer Adaptation</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å®¹é”™æ§åˆ¶æ¡†æ¶ä»¥åº”å¯¹å››æ—‹ç¿¼æ•…éšœé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08223v1" data-paper-url="./papers/250508223v1-reinforcement-learning-based-fault-tolerant-control-for-quadrotor-wi.html" onclick="toggleFavorite(this, '2505.08223v1', 'Reinforcement Learning-based Fault-Tolerant Control for Quadrotor with Online Transformer Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>25</td>
  <td><a href="./papers/250508932v1-parameter-efficient-fine-tuning-of-vision-foundation-model-for-fores.html">Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery</a></td>
  <td>æå‡ºå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä»¥è§£å†³æ— äººæœºå½±åƒæ£®æ—åœ°é¢åˆ†å‰²é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">foundation model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08932v1" data-paper-url="./papers/250508932v1-parameter-efficient-fine-tuning-of-vision-foundation-model-for-fores.html" onclick="toggleFavorite(this, '2505.08932v1', 'Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>26</td>
  <td><a href="./papers/250508664v1-a-social-robot-with-inner-speech-for-dietary-guidance.html">A Social Robot with Inner Speech for Dietary Guidance</a></td>
  <td>æå‡ºå†…å¿ƒè¯­è¨€æœºåˆ¶ä»¥å¢å¼ºç¤¾äº¤æœºå™¨äººåœ¨é¥®é£ŸæŒ‡å¯¼ä¸­çš„é€æ˜åº¦</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08664v1" data-paper-url="./papers/250508664v1-a-social-robot-with-inner-speech-for-dietary-guidance.html" onclick="toggleFavorite(this, '2505.08664v1', 'A Social Robot with Inner Speech for Dietary Guidance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>27</td>
  <td><a href="./papers/250508419v1-hmr-odta-online-diverse-task-allocation-for-a-team-of-heterogeneous-.html">HMR-ODTA: Online Diverse Task Allocation for a Team of Heterogeneous Mobile Robots</a></td>
  <td>æå‡ºHMR-ODTAä»¥è§£å†³å¼‚æ„ç§»åŠ¨æœºå™¨äººåœ¨çº¿å¤šä»»åŠ¡åˆ†é…é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">HMR</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08419v1" data-paper-url="./papers/250508419v1-hmr-odta-online-diverse-task-allocation-for-a-team-of-heterogeneous-.html" onclick="toggleFavorite(this, '2505.08419v1', 'HMR-ODTA: Online Diverse Task Allocation for a Team of Heterogeneous Mobile Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>28</td>
  <td><a href="./papers/250508193v1-a-tightly-coupled-imu-based-motion-capture-approach-for-estimating-m.html">A Tightly Coupled IMU-Based Motion Capture Approach for Estimating Multibody Kinematics and Kinetics</a></td>
  <td>æå‡ºç´§è€¦åˆIMUè¿åŠ¨æ•æ‰æ–¹æ³•ä»¥è§£å†³å¤šä½“åŠ¨åŠ›å­¦ä¼°è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">IMU-based motion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08193v1" data-paper-url="./papers/250508193v1-a-tightly-coupled-imu-based-motion-capture-approach-for-estimating-m.html" onclick="toggleFavorite(this, '2505.08193v1', 'A Tightly Coupled IMU-Based Motion Capture Approach for Estimating Multibody Kinematics and Kinetics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>29</td>
  <td><a href="./papers/250508417v1-oracle-grasp-zero-shot-task-oriented-robotic-grasping-using-large-mu.html">ORACLE-Grasp: Zero-Shot Task-Oriented Robotic Grasping using Large Multimodal Models</a></td>
  <td>æå‡ºORACLE-Graspä»¥è§£å†³æ— è®­ç»ƒæ•°æ®çš„æœºå™¨äººæŠ“å–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">grasp prediction</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08417v1" data-paper-url="./papers/250508417v1-oracle-grasp-zero-shot-task-oriented-robotic-grasping-using-large-mu.html" onclick="toggleFavorite(this, '2505.08417v1', 'ORACLE-Grasp: Zero-Shot Task-Oriented Robotic Grasping using Large Multimodal Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>30</td>
  <td><a href="./papers/250508787v4-uniskill-imitating-human-videos-via-cross-embodiment-skill-represent.html">UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations</a></td>
  <td>æå‡ºUniSkillæ¡†æ¶ä»¥è§£å†³äººæœºæ¨¡ä»¿å­¦ä¹ çš„æŒ‘æˆ˜</td>
  <td class="tags-cell"><span class="paper-tag">cross-embodiment</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08787v4" data-paper-url="./papers/250508787v4-uniskill-imitating-human-videos-via-cross-embodiment-skill-represent.html" onclick="toggleFavorite(this, '2505.08787v4', 'UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å››ç”Ÿæˆå¼åŠ¨ä½œ-generative-motion">ğŸ”¬ æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>31</td>
  <td><a href="./papers/250508625v1-beyond-predefined-actions-integrating-behavior-trees-and-dynamic-mov.html">Beyond Predefined Actions: Integrating Behavior Trees and Dynamic Movement Primitives for Robot Learning from Demonstration</a></td>
  <td>æå‡ºè¡Œä¸ºæ ‘ä¸åŠ¨æ€è¿åŠ¨åŸè¯­ç»“åˆçš„æ–¹æ³•ä»¥è§£å†³æœºå™¨äººå­¦ä¹ ä¸­çš„åŠ¨ä½œé¢„å®šä¹‰é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion generation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.08625v1" data-paper-url="./papers/250508625v1-beyond-predefined-actions-integrating-behavior-trees-and-dynamic-mov.html" onclick="toggleFavorite(this, '2505.08625v1', 'Beyond Predefined Actions: Integrating Behavior Trees and Dynamic Movement Primitives for Robot Learning from Demonstration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)