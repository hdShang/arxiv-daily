---
layout: default
title: Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation
---

# Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13982" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13982v2</a>
  <a href="https://arxiv.org/pdf/2505.13982.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13982v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13982v2', 'Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinzhou Li, Tianhao Wu, Jiyao Zhang, Zeyuan Chen, Haotian Jin, Mingdong Wu, Yujun Shen, Yaodong Yang, Hao Dong

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-07-21)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://adaptac-dex.github.io/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªé€‚åº”è§†è§‰-è§¦è§‰èåˆæ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€æ•°æ®èåˆæŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å¤šæ¨¡æ€èåˆ` `è‡ªé€‚åº”æ³¨æ„åŠ›` `è§¦è§‰æ„ŸçŸ¥` `æœºå™¨äººæ“ä½œ` `è‡ªç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€èåˆæ–¹æ³•æœªèƒ½æœ‰æ•ˆè€ƒè™‘ä¸åŒæ“ä½œé˜¶æ®µå¯¹è§†è§‰å’Œè§¦è§‰æ¨¡æ€çš„å…³æ³¨ç¨‹åº¦å·®å¼‚ï¼Œå¯¼è‡´æ€§èƒ½ä¸è¶³ã€‚
2. æå‡ºäº†ä¸€ç§åŸºäºåŠ›çš„è‡ªé€‚åº”æ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œèƒ½å¤ŸåŠ¨æ€è°ƒæ•´è§†è§‰å’Œè§¦è§‰ç‰¹å¾çš„æƒé‡ï¼Œä¸”æ— éœ€äººå·¥æ ‡æ³¨ã€‚
3. åœ¨çœŸå®å®éªŒä¸­ï¼Œæ–¹æ³•åœ¨ä¸‰ä¸ªç»†ç²’åº¦ä»»åŠ¡ä¸­å®ç°93%çš„æˆåŠŸç‡ï¼Œè¡¨æ˜å…¶åœ¨ä¸åŒæ“ä½œé˜¶æ®µçš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆåˆ©ç”¨å¤šä¼ æ„Ÿå™¨æ•°æ®å¯¹äºæœºå™¨äººåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ¨¡æ€çš„å¼‚è´¨æ€§ï¼Œä½¿å¾—èåˆå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶æå‡ºäº†ç»¼åˆç‰¹å¾èåˆç­–ç•¥ï¼Œä½†å¾€å¾€å¿½è§†äº†ä¸åŒæ“ä½œé˜¶æ®µå¯¹å„æ¨¡æ€çš„å…³æ³¨ç¨‹åº¦å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ›çš„è‡ªé€‚åº”æ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹åŠ¨æ€è°ƒæ•´è§†è§‰å’Œè§¦è§‰ç‰¹å¾çš„æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£çš„æœªæ¥åŠ›é¢„æµ‹è¾…åŠ©ä»»åŠ¡ï¼Œä»¥å¢å¼ºè§¦è§‰æ¨¡æ€ï¼Œæ”¹å–„æ•°æ®ä¸å¹³è¡¡ï¼Œå¹¶ä¿ƒè¿›é€‚å½“çš„è°ƒæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨ä¸‰ä¸ªç»†ç²’åº¦ã€æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ä¸­å®ç°äº†93%çš„å¹³å‡æˆåŠŸç‡ï¼Œè¿›ä¸€æ­¥åˆ†æè¡¨æ˜æˆ‘ä»¬çš„ç­–ç•¥èƒ½å¤Ÿåœ¨ä¸åŒæ“ä½œé˜¶æ®µé€‚å½“è°ƒæ•´å¯¹å„æ¨¡æ€çš„å…³æ³¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ•°æ®èåˆä¸­çš„å¼‚è´¨æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘ä¸åŒæ“ä½œé˜¶æ®µå¯¹å„æ¨¡æ€çš„å…³æ³¨ç¨‹åº¦ï¼Œå¯¼è‡´èåˆæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŠ›å¼•å¯¼çš„è‡ªé€‚åº”æ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è§†è§‰å’Œè§¦è§‰ç‰¹å¾çš„æƒé‡ï¼Œæå‡å¤šæ¨¡æ€èåˆçš„æœ‰æ•ˆæ€§ï¼Œä¸”æ— éœ€äººå·¥æ ‡æ³¨ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†ã€ç‰¹å¾æå–ã€åŠ›å¼•å¯¼çš„æ³¨æ„åŠ›èåˆæ¨¡å—å’Œè‡ªç›‘ç£çš„æœªæ¥åŠ›é¢„æµ‹ä»»åŠ¡ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯çš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ“ä½œé˜¶æ®µçš„ä¸åŒåŠ¨æ€è°ƒæ•´æ¨¡æ€æƒé‡ï¼Œä¸ç°æœ‰é™æ€èåˆæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡è§†è§‰å’Œè§¦è§‰ç‰¹å¾çš„å­¦ä¹ ï¼Œå¹¶é€šè¿‡è‡ªç›‘ç£å­¦ä¹ å¢å¼ºè§¦è§‰æ¨¡æ€çš„è¡¨ç°ï¼Œè§£å†³äº†æ•°æ®ä¸å¹³è¡¡çš„é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ä¸‰ä¸ªç»†ç²’åº¦ã€æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ä¸­å®ç°äº†93%çš„å¹³å‡æˆåŠŸç‡ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤šæ¨¡æ€èåˆä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æœºå™¨äººæŠ“å–ã€æ“ä½œå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡å¤šæ¨¡æ€æ•°æ®èåˆçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿä½¿æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¥½åœ°æ‰§è¡Œä»»åŠ¡ï¼Œå¢å¼ºå…¶æ™ºèƒ½åŒ–æ°´å¹³ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯èƒ½åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€åŒ»ç–—è¾…åŠ©å’Œè‡ªåŠ¨åŒ–ç”Ÿäº§ç­‰åœºæ™¯ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effectively utilizing multi-sensory data is important for robots to generalize across diverse tasks. However, the heterogeneous nature of these modalities makes fusion challenging. Existing methods propose strategies to obtain comprehensively fused features but often ignore the fact that each modality requires different levels of attention at different manipulation stages. To address this, we propose a force-guided attention fusion module that adaptively adjusts the weights of visual and tactile features without human labeling. We also introduce a self-supervised future force prediction auxiliary task to reinforce the tactile modality, improve data imbalance, and encourage proper adjustment. Our method achieves an average success rate of 93% across three fine-grained, contactrich tasks in real-world experiments. Further analysis shows that our policy appropriately adjusts attention to each modality at different manipulation stages. The videos can be viewed at https://adaptac-dex.github.io/.

