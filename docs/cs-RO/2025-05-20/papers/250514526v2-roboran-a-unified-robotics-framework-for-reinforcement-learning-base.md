---
layout: default
title: RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based Autonomous Navigation
---

# RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based Autonomous Navigation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14526" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14526v2</a>
  <a href="https://arxiv.org/pdf/2505.14526.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14526v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14526v2', 'RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based Autonomous Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matteo El-Hariry, Antoine Richard, Ricard M. Castan, Luis F. W. Batista, Matthieu Geist, Cedric Pradalier, Miguel Olivares-Mendez

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-11-05)

**å¤‡æ³¨**: Accepted at Transactions on Machine Learning Research (TMLR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRoboRANæ¡†æ¶ä»¥è§£å†³å¤šé¢†åŸŸè‡ªä¸»å¯¼èˆªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªä¸»å¯¼èˆª` `å¼ºåŒ–å­¦ä¹ ` `å¤šé¢†åŸŸæ¡†æ¶` `æœºå™¨äººå¹³å°` `å¼€æºAPI` `ä»¿çœŸåˆ°ç°å®` `æ¨¡å—åŒ–è®¾è®¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šå¸¸å±€é™äºç‰¹å®šå¹³å°ï¼Œé™åˆ¶äº†ä¸åŒæœºå™¨äººä¹‹é—´çš„æ¯”è¾ƒä¸æ³›åŒ–èƒ½åŠ›ã€‚
2. æå‡ºRoboRANæ¡†æ¶ï¼Œæ”¯æŒå¤šç§æœºå™¨äººå¹³å°çš„å¯¼èˆªç­–ç•¥è®­ç»ƒä¸è¯„ä¼°ï¼Œä¿ƒè¿›è·¨é¢†åŸŸåº”ç”¨ã€‚
3. é€šè¿‡çœŸå®å®éªŒéªŒè¯äº†ä»¿çœŸåˆ°ç°å®çš„è½¬ç§»èƒ½åŠ›ï¼Œå±•ç¤ºäº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ä¸çµæ´»æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªä¸»æœºå™¨äººå¿…é¡»åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªå’Œæ“ä½œï¼ŒåŒ…æ‹¬é™†åœ°ã€æ°´åŸŸã€ç©ºä¸­å’Œå¤ªç©ºç­‰é¢†åŸŸã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ç‰¹å®šè‡ªä¸»æœºå™¨äººçš„ç­–ç•¥è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰æ¡†æ¶å’ŒåŸºå‡†å¾€å¾€å±€é™äºç‰¹å®šå¹³å°ï¼Œé™åˆ¶äº†ä¸åŒç§»åŠ¨ç³»ç»Ÿä¹‹é—´çš„æ³›åŒ–å’Œå…¬å¹³æ¯”è¾ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šé¢†åŸŸæ¡†æ¶ï¼Œç”¨äºè®­ç»ƒã€è¯„ä¼°å’Œéƒ¨ç½²åŸºäºRLçš„å¯¼èˆªç­–ç•¥ï¼Œæ¶µç›–å¤šç§æœºå™¨äººå¹³å°å’Œæ“ä½œç¯å¢ƒã€‚æˆ‘ä»¬çš„å·¥ä½œæœ‰å››ä¸ªå…³é”®è´¡çŒ®ï¼šå¯æ‰©å±•çš„æ¨¡å—åŒ–æ¡†æ¶ã€é€šè¿‡çœŸå®å®éªŒå±•ç¤ºçš„ä»¿çœŸåˆ°ç°å®è½¬ç§»ã€é¦–ä¸ªå¼€æºAPIçš„å‘å¸ƒï¼Œä»¥åŠç»Ÿä¸€çš„ä»»åŠ¡å’ŒæŒ‡æ ‡ç”¨äºè·¨é¢†åŸŸè¯„ä¼°ã€‚RoboRANé€šè¿‡ç¡®ä¿ä»¿çœŸä¸ç°å®éƒ¨ç½²ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œé™ä½äº†å¼€å‘é€‚åº”æ€§RLå¯¼èˆªç­–ç•¥çš„é—¨æ§›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨å¤šç§æœºå™¨äººå¹³å°å’Œç¯å¢ƒä¸­ç¼ºä¹é€šç”¨æ€§å’Œå¯æ¯”è¾ƒæ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªèƒ½åœ¨ç‰¹å®šå¹³å°ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´å’Œçµæ´»æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoboRANæ¡†æ¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒä¸åŒæœºå™¨äººå’Œä»»åŠ¡çš„æ— ç¼åˆ‡æ¢ï¼Œæä¾›å¯é‡å¤çš„è®­ç»ƒæµç¨‹ï¼Œä»è€Œæé«˜äº†é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoboRANæ¡†æ¶åŒ…æ‹¬å¤šä¸ªä¸»è¦æ¨¡å—ï¼šä»»åŠ¡å®šä¹‰æ¨¡å—ã€è®­ç»ƒæ¨¡å—ã€è¯„ä¼°æ¨¡å—å’Œéƒ¨ç½²æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½å¯ä»¥ç‹¬ç«‹é…ç½®ï¼Œä»¥é€‚åº”ä¸åŒçš„æœºå™¨äººå’Œç¯å¢ƒéœ€æ±‚ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æµ‹è¯•å¹³å°ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ“ä½œæ¡ä»¶ä¸‹ï¼ˆå¦‚æ°´åŸŸã€é™†åœ°å’Œå¤ªç©ºï¼‰å¯¹å¯¼èˆªä»»åŠ¡è¿›è¡Œä¸€è‡´æ€§è¯„ä¼°ã€‚è¿™ä¸€è®¾è®¡æ˜¾è‘—æé«˜äº†è·¨å¹³å°çš„æ¯”è¾ƒèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¡†æ¶ä¸­é‡‡ç”¨äº†æ ‡å‡†åŒ–çš„ä»»åŠ¡å’ŒæŒ‡æ ‡ï¼Œç¡®ä¿äº†ä¸åŒå®éªŒä¹‹é—´çš„å¯æ¯”æ€§ã€‚æ­¤å¤–ï¼Œå¼€æºAPIçš„å‘å¸ƒä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿè½»æ¾å°†è®­ç»ƒå¥½çš„ç­–ç•¥éƒ¨ç½²åˆ°çœŸå®æœºå™¨äººä¸Šï¼Œæ”¯æŒå¿«é€Ÿçš„ç°åœºéªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šç§çœŸå®ç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒRoboRANæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å®ç°ä»¿çœŸåˆ°ç°å®çš„è½¬ç§»ï¼Œå¤šä¸ªæœºå™¨äººåœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¯¥æ¡†æ¶çš„ç»Ÿä¸€è¯„ä¼°æ ‡å‡†ç¡®ä¿äº†ä¸åŒå¹³å°ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RoboRANæ¡†æ¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ— äººé©¾é©¶æ±½è½¦ã€æµ·æ´‹æ¢æµ‹æœºå™¨äººã€ç©ºä¸­æ— äººæœºç­‰å¤šç§è‡ªä¸»æœºå™¨äººç³»ç»Ÿã€‚å…¶æ¨¡å—åŒ–è®¾è®¡å’Œå¼€æºç‰¹æ€§å°†ä¿ƒè¿›ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…åœ¨ä¸åŒç¯å¢ƒä¸‹çš„å¿«é€ŸåŸå‹å¼€å‘ä¸æµ‹è¯•ï¼Œæ¨åŠ¨è‡ªä¸»å¯¼èˆªæŠ€æœ¯çš„è¿›æ­¥ä¸æ™®åŠã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing frameworks and benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present a multi-domain framework for training, evaluating and deploying RL-based navigation policies across diverse robotic platforms and operational environments. Our work presents four key contributions: (1) a scalable and modular framework, facilitating seamless robot-task interchangeability and reproducible training pipelines; (2) sim-to-real transfer demonstrated through real-world experiments with multiple robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle; (3) the release of the first open-source API for deploying Isaac Lab-trained policies to real robots, enabling lightweight inference and rapid field validation; and (4) uniform tasks and metrics for cross-medium evaluation, through a unified evaluation testbed to assess performance of navigation tasks in diverse operational conditions (aquatic, terrestrial and space). By ensuring consistency between simulation and real-world deployment, RoboRAN lowers the barrier to developing adaptable RL-based navigation strategies. Its modular design enables straightforward integration of new robots and tasks through predefined templates, fostering reproducibility and extension to diverse domains. To support the community, we release RoboRAN as open-source.

