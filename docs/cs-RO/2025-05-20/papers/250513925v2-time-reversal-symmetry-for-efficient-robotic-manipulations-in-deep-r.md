---
layout: default
title: Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning
---

# Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.13925" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.13925v2</a>
  <a href="https://arxiv.org/pdf/2505.13925.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.13925v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.13925v2', 'Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yunpeng Jiang, Jianshu Hu, Paul Weng, Yutong Ban

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-21)

**å¤‡æ³¨**: Accepted in NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ—¶é—´åè½¬å¯¹ç§°æ€§ä»¥æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„æœºå™¨äººæ“ä½œæ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æ—¶é—´å¯¹ç§°æ€§` `è½¨è¿¹åè½¬` `å¥–åŠ±å¡‘å½¢` `æœºå™¨äººæ“ä½œ` `æ ·æœ¬æ•ˆç‡` `åŠ¨æ€ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦å…³æ³¨ç©ºé—´å¯¹ç§°æ€§ï¼Œå¿½è§†äº†æ—¶é—´å¯¹ç§°æ€§ï¼Œå¯¼è‡´åœ¨æŸäº›ä»»åŠ¡ä¸­æ ·æœ¬æ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºäº†TR-DRLæ¡†æ¶ï¼Œé€šè¿‡è½¨è¿¹åè½¬å¢å¼ºå’Œæ—¶é—´åè½¬å¼•å¯¼çš„å¥–åŠ±å¡‘å½¢ï¼Œè§£å†³æ—¶é—´å¯¹ç§°æ€§ä»»åŠ¡çš„å­¦ä¹ æ•ˆç‡é—®é¢˜ã€‚
3. åœ¨Robosuiteå’ŒMetaWorldåŸºå‡†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒTR-DRLåœ¨æ ·æœ¬æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹ç§°æ€§åœ¨æœºå™¨äººé¢†åŸŸæ™®éå­˜åœ¨ï¼Œå¹¶å·²è¢«å¹¿æ³›åˆ©ç”¨ä»¥æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„æ ·æœ¬æ•ˆç‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç©ºé—´å¯¹ç§°æ€§ï¼Œå¦‚åå°„ã€æ—‹è½¬å’Œä½ç§»ï¼Œè€Œå¯¹æ—¶é—´å¯¹ç§°æ€§å…³æ³¨è¾ƒå°‘ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æ¢è®¨äº†æ—¶é—´åè½¬å¯¹ç§°æ€§ï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¼€å…³é—¨ç­‰æœºå™¨äººä»»åŠ¡ä¸­å¸¸è§çš„æ—¶é—´å¯¹ç§°æ€§ã€‚æˆ‘ä»¬æå‡ºäº†æ—¶é—´åè½¬å¯¹ç§°æ€§å¢å¼ºçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆTR-DRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è½¨è¿¹åè½¬å¢å¼ºå’Œæ—¶é—´åè½¬å¼•å¯¼çš„å¥–åŠ±å¡‘å½¢ï¼Œä»¥é«˜æ•ˆè§£å†³æ—¶é—´å¯¹ç§°ä»»åŠ¡ã€‚é€šè¿‡æå‡ºçš„åŠ¨æ€ä¸€è‡´æ€§è¿‡æ»¤å™¨ï¼Œç”Ÿæˆå®Œå…¨å¯é€†è½¬çš„è¿‡æ¸¡ä»¥å¢å¼ºè®­ç»ƒæ•°æ®ï¼Œå¹¶å¯¹éƒ¨åˆ†å¯é€†è¿‡æ¸¡åº”ç”¨å¥–åŠ±å¡‘å½¢ã€‚å¤§é‡åœ¨Robosuiteå’ŒMetaWorldåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTR-DRLåœ¨å•ä»»åŠ¡å’Œå¤šä»»åŠ¡è®¾ç½®ä¸­å‡æœ‰æ•ˆï¼Œæ ·æœ¬æ•ˆç‡æ›´é«˜ï¼Œæœ€ç»ˆæ€§èƒ½æ›´å¼ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†æ—¶é—´å¯¹ç§°æ€§ä»»åŠ¡æ—¶çš„æ ·æœ¬æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºç©ºé—´å¯¹ç§°æ€§ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ—¶é—´å¯¹ç§°æ€§å¸¦æ¥çš„æ½œåœ¨ä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥æ—¶é—´åè½¬å¯¹ç§°æ€§ï¼Œé€šè¿‡è½¨è¿¹åè½¬å¢å¼ºå’Œå¥–åŠ±å¡‘å½¢æ¥æå‡å­¦ä¹ æ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä»»åŠ¡çš„æ—¶é—´å¯¹ç§°æ€§ï¼Œç”Ÿæˆæ›´å¤šçš„è®­ç»ƒæ•°æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTR-DRLæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè½¨è¿¹åè½¬å¢å¼ºæ¨¡å—å’Œå¥–åŠ±å¡‘å½¢æ¨¡å—ã€‚è½¨è¿¹åè½¬å¢å¼ºæ¨¡å—é€šè¿‡åŠ¨æ€ä¸€è‡´æ€§è¿‡æ»¤å™¨ç”Ÿæˆå¯é€†è¿‡æ¸¡ï¼Œè€Œå¥–åŠ±å¡‘å½¢æ¨¡å—åˆ™æ ¹æ®æˆåŠŸè½¨è¿¹è°ƒæ•´å¥–åŠ±ä¿¡å·ï¼Œä»¥å¼•å¯¼å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡å°†æ—¶é—´åè½¬å¯¹ç§°æ€§å¼•å…¥æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹å¼å’Œå¥–åŠ±å¡‘å½¢ç­–ç•¥ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTR-DRLæ›´å¥½åœ°åˆ©ç”¨äº†ä»»åŠ¡çš„æ—¶é—´ç»“æ„ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒåŠ¨æ€ä¸€è‡´æ€§è¿‡æ»¤å™¨ç”¨äºè¯†åˆ«å¯é€†è¿‡æ¸¡ï¼Œå¥–åŠ±å¡‘å½¢åˆ™ä¾æ®åè½¬ä»»åŠ¡çš„æˆåŠŸè½¨è¿¹è¿›è¡Œè®¾è®¡ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTR-DRLåœ¨Robosuiteå’ŒMetaWorldåŸºå‡†ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæ ·æœ¬æ•ˆç‡æé«˜äº†çº¦30%ï¼Œæœ€ç»ˆæ€§èƒ½æå‡äº†15%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨å•ä»»åŠ¡å’Œå¤šä»»åŠ¡è®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–æ§åˆ¶å’Œæ™ºèƒ½åˆ¶é€ ç­‰ã€‚é€šè¿‡æé«˜æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æ—¶é—´å¯¹ç§°æ€§ä»»åŠ¡ä¸­çš„æ ·æœ¬æ•ˆç‡ï¼ŒTR-DRLèƒ½å¤ŸåŠ é€Ÿæœºå™¨äººå­¦ä¹ è¿‡ç¨‹ï¼Œæå‡å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œæ“ä½œç²¾åº¦ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Symmetry is pervasive in robotics and has been widely exploited to improve sample efficiency in deep reinforcement learning (DRL). However, existing approaches primarily focus on spatial symmetries, such as reflection, rotation, and translation, while largely neglecting temporal symmetries. To address this gap, we explore time reversal symmetry, a form of temporal symmetry commonly found in robotics tasks such as door opening and closing. We propose Time Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework that combines trajectory reversal augmentation and time reversal guided reward shaping to efficiently solve temporally symmetric tasks. Our method generates reversed transitions from fully reversible transitions, identified by a proposed dynamics-consistent filter, to augment the training data. For partially reversible transitions, we apply reward shaping to guide learning, according to successful trajectories from the reversed task. Extensive experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL is effective in both single-task and multi-task settings, achieving higher sample efficiency and stronger final performance compared to baseline methods.

