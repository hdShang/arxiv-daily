---
layout: default
title: "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects"
---

# Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.01294" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.01294v2</a>
  <a href="https://arxiv.org/pdf/2511.01294.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.01294v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.01294v2', 'Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiawei Wang, Dingyou Wang, Jiaming Hu, Qixuan Zhang, Jingyi Yu, Lan Xu

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-11-03 (æ›´æ–°: 2025-11-04)

**å¤‡æ³¨**: project page: https://sites.google.com/deemos.com/kinematify

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Kinematifyï¼šå¼€æ”¾è¯æ±‡é«˜è‡ªç”±åº¦é“°æ¥ç‰©ä½“è‡ªåŠ¨åˆæˆæ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `é“°æ¥ç‰©ä½“` `è¿åŠ¨å­¦ç»“æ„` `è’™ç‰¹å¡æ´›æ ‘æœç´¢` `å‡ ä½•é©±åŠ¨ä¼˜åŒ–` `æœºå™¨äººæ“ä½œ` `ç‰©ç†æ¨¡æ‹Ÿ` `å¼€æ”¾è¯æ±‡` `é«˜è‡ªç”±åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰é“°æ¥ç‰©ä½“å»ºæ¨¡æ–¹æ³•ä¾èµ–è¿åŠ¨åºåˆ—æˆ–äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œéš¾ä»¥æ‰©å±•åˆ°é«˜è‡ªç”±åº¦ç‰©ä½“ã€‚
2. Kinematify ç»“åˆ MCTS æœç´¢è¿›è¡Œç»“æ„æ¨ç†ï¼Œå¹¶åˆ©ç”¨å‡ ä½•é©±åŠ¨ä¼˜åŒ–è¿›è¡Œå…³èŠ‚å‚æ•°ä¼°è®¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒKinematify åœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šï¼Œé…å‡†å’Œè¿åŠ¨å­¦æ‹“æ‰‘ç²¾åº¦å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯¹è¿åŠ¨å­¦ç»“æ„å’Œå¯ç§»åŠ¨éƒ¨ä»¶çš„æ·±åˆ»ç†è§£å¯¹äºæœºå™¨äººæ“ä½œç‰©ä½“å’Œå»ºæ¨¡è‡ªèº«é“°æ¥å½¢æ€è‡³å…³é‡è¦ã€‚è¿™ç§ç†è§£é€šè¿‡é“°æ¥ç‰©ä½“æ¥æ•è·ï¼Œè¿™å¯¹äºç‰©ç†æ¨¡æ‹Ÿã€è¿åŠ¨è§„åˆ’å’Œç­–ç•¥å­¦ä¹ ç­‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåˆ›å»ºè¿™äº›æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰é«˜è‡ªç”±åº¦ï¼ˆDoFï¼‰çš„ç‰©ä½“ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºè¿åŠ¨åºåˆ—æˆ–æ¥è‡ªæ‰‹åŠ¨ç®¡ç†æ•°æ®é›†çš„å¼ºå‡è®¾ï¼Œè¿™é˜»ç¢äº†å¯æ‰©å±•æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç» Kinematifyï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ¡†æ¶ï¼Œå¯ä»¥ç›´æ¥ä»ä»»æ„ RGB å›¾åƒæˆ–æ–‡æœ¬æè¿°ä¸­åˆæˆé“°æ¥ç‰©ä½“ã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šï¼ˆiï¼‰æ¨æ–­é«˜è‡ªç”±åº¦ç‰©ä½“çš„è¿åŠ¨å­¦æ‹“æ‰‘ï¼›ï¼ˆiiï¼‰ä»é™æ€å‡ ä½•ä½“ä¼°è®¡å…³èŠ‚å‚æ•°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬ç»“åˆäº†ç”¨äºç»“æ„æ¨ç†çš„ MCTS æœç´¢å’Œç”¨äºå…³èŠ‚æ¨ç†çš„å‡ ä½•é©±åŠ¨ä¼˜åŒ–ï¼Œä»è€Œäº§ç”Ÿç‰©ç†ä¸Šä¸€è‡´ä¸”åŠŸèƒ½ä¸Šæœ‰æ•ˆçš„æè¿°ã€‚æˆ‘ä»¬åœ¨æ¥è‡ªåˆæˆå’ŒçœŸå®ç¯å¢ƒçš„å„ç§è¾“å…¥ä¸Šè¯„ä¼° Kinematifyï¼Œè¯æ˜äº†åœ¨é…å‡†å’Œè¿åŠ¨å­¦æ‹“æ‰‘ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨æ„å»ºé«˜è‡ªç”±åº¦é“°æ¥ç‰©ä½“æ¨¡å‹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç—›ç‚¹åœ¨äºä¾èµ–å¤§é‡çš„è¿åŠ¨åºåˆ—æ•°æ®æˆ–å¼ºå…ˆéªŒå‡è®¾ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯å¯¹äºå¼€æ”¾è¯æ±‡åœºæ™¯ï¼Œç¼ºä¹é’ˆå¯¹å„ç§ç‰©ä½“ç±»å‹çš„è¿åŠ¨æ•°æ®ï¼Œä½¿å¾—è‡ªåŠ¨æ„å»ºç²¾ç¡®çš„é“°æ¥æ¨¡å‹å˜å¾—å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šKinematify çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é“°æ¥ç‰©ä½“çš„åˆæˆé—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªå­é—®é¢˜ï¼šè¿åŠ¨å­¦æ‹“æ‰‘æ¨æ–­å’Œå…³èŠ‚å‚æ•°ä¼°è®¡ã€‚é€šè¿‡ç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œç»“æ„æ¨ç†ï¼Œå¹¶åˆ©ç”¨å‡ ä½•é©±åŠ¨çš„ä¼˜åŒ–æ–¹æ³•è¿›è¡Œå…³èŠ‚å‚æ•°ä¼°è®¡ï¼Œä»è€Œåœ¨æ²¡æœ‰å¤§é‡è¿åŠ¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»é™æ€å‡ ä½•ä¿¡æ¯ä¸­æ¨æ–­å‡ºåˆç†çš„é“°æ¥ç»“æ„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šKinematify çš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦é˜¶æ®µï¼š1) è¾“å…¥ï¼šæ¥æ”¶ RGB å›¾åƒæˆ–æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ã€‚2) ç»“æ„æ¨ç†ï¼šä½¿ç”¨ MCTS æœç´¢ç®—æ³•æ¢ç´¢å¯èƒ½çš„è¿åŠ¨å­¦æ‹“æ‰‘ç»“æ„ã€‚3) å…³èŠ‚å‚æ•°ä¼°è®¡ï¼šåˆ©ç”¨å‡ ä½•é©±åŠ¨çš„ä¼˜åŒ–æ–¹æ³•ï¼Œæ ¹æ®é™æ€å‡ ä½•ä¿¡æ¯ä¼°è®¡å…³èŠ‚çš„ä½ç½®ã€æ–¹å‘å’Œè¿åŠ¨èŒƒå›´ã€‚4) æ¨¡å‹ç”Ÿæˆï¼šå°†æ¨æ–­å‡ºçš„æ‹“æ‰‘ç»“æ„å’Œå…³èŠ‚å‚æ•°ç»„åˆæˆå®Œæ•´çš„é“°æ¥ç‰©ä½“æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šKinematify çš„å…³é”®åˆ›æ–°åœ¨äºå°† MCTS æœç´¢åº”ç”¨äºè¿åŠ¨å­¦æ‹“æ‰‘çš„æ¨æ–­ï¼Œè¿™ä½¿å¾—å®ƒèƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§é‡è¿åŠ¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°æ¢ç´¢å¤æ‚çš„é“°æ¥ç»“æ„ç©ºé—´ã€‚æ­¤å¤–ï¼Œå‡ ä½•é©±åŠ¨çš„ä¼˜åŒ–æ–¹æ³•èƒ½å¤Ÿä»é™æ€å‡ ä½•ä¿¡æ¯ä¸­æå–æœ‰ç”¨çš„çº¿ç´¢ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ä¼°è®¡å…³èŠ‚å‚æ•°ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒKinematify èƒ½å¤Ÿå¤„ç†æ›´å¹¿æ³›çš„ç‰©ä½“ç±»å‹ï¼Œå¹¶ä¸”ä¸éœ€è¦å¤§é‡çš„è¿åŠ¨æ•°æ®ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ MCTS æœç´¢ä¸­ï¼Œéœ€è¦å®šä¹‰åˆé€‚çš„å¥–åŠ±å‡½æ•°æ¥å¼•å¯¼æœç´¢è¿‡ç¨‹ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡éœ€è¦è€ƒè™‘ç»“æ„çš„åˆç†æ€§ã€å…³èŠ‚çš„è¿åŠ¨èŒƒå›´ä»¥åŠä¸è¾“å…¥å‡ ä½•çš„åŒ¹é…ç¨‹åº¦ã€‚åœ¨å‡ ä½•é©±åŠ¨çš„ä¼˜åŒ–ä¸­ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„å‡ ä½•ç‰¹å¾ï¼ˆä¾‹å¦‚ï¼Œè¡¨é¢æ³•çº¿ã€æ›²ç‡ç­‰ï¼‰æ¥çº¦æŸå…³èŠ‚å‚æ•°çš„ä¼°è®¡ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿéœ€è¦è€ƒè™‘ç‰©ç†ä¸€è‡´æ€§ï¼Œä¾‹å¦‚é¿å…å…³èŠ‚ä¹‹é—´çš„ç¢°æ’ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Kinematify åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é…å‡†ç²¾åº¦å’Œè¿åŠ¨å­¦æ‹“æ‰‘ç²¾åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒKinematify åœ¨é«˜è‡ªç”±åº¦é“°æ¥ç‰©ä½“çš„å»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„é“°æ¥ç»“æ„ï¼Œå¹¶ä¸”å¯¹å™ªå£°å’Œé®æŒ¡å…·æœ‰ä¸€å®šçš„é²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Kinematify æœ‰æ½œåŠ›åº”ç”¨äºæœºå™¨äººæ“ä½œã€ç‰©ç†æ¨¡æ‹Ÿã€æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£å’Œæ“ä½œå„ç§é“°æ¥ç‰©ä½“ï¼Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚åœ¨ç‰©ç†æ¨¡æ‹Ÿä¸­ï¼Œå®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ›´é€¼çœŸçš„é“°æ¥ç‰©ä½“æ¨¡å‹ï¼Œä»è€Œæé«˜æ¨¡æ‹Ÿçš„å‡†ç¡®æ€§ã€‚åœ¨æ¸¸æˆå¼€å‘å’Œè™šæ‹Ÿç°å®ä¸­ï¼Œå®ƒå¯ä»¥ç”¨äºåˆ›å»ºæ›´ä¸°å¯Œçš„äº¤äº’ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.

