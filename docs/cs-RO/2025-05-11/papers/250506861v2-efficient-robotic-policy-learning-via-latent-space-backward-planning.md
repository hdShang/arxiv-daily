---
layout: default
title: Efficient Robotic Policy Learning via Latent Space Backward Planning
---

# Efficient Robotic Policy Learning via Latent Space Backward Planning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06861" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.06861v2</a>
  <a href="https://arxiv.org/pdf/2505.06861.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06861v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06861v2', 'Efficient Robotic Policy Learning via Latent Space Backward Planning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Dongxiu Liu, Haoyi Niu, Zhihao Wang, Jinliang Zheng, Yinan Zheng, Zhonghong Ou, Jianming Hu, Jianxiong Li, Xianyuan Zhan

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-11 (Êõ¥Êñ∞: 2025-05-27)

**Â§áÊ≥®**: Accepted by ICML 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÊΩúÂú®Á©∫Èó¥ÂèçÂêëËßÑÂàí‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫ÂÆûÊó∂ÊéßÂà∂ÊïàÁéá‰∏éÂáÜÁ°ÆÊÄßÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫ËßÑÂàí` `ÊΩúÂú®Á©∫Èó¥` `ÂèçÂêëËßÑÂàí` `Á≠ñÁï•Â≠¶‰π†` `ÂÆûÊó∂ÊéßÂà∂` `ÈïøÊó∂Èó¥‰ªªÂä°` `ÊïàÁéáÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊú∫Âô®‰∫∫ËßÑÂàíÊñπÊ≥ïÂú®ÂÆûÊó∂ÊéßÂà∂‰∏≠Èù¢‰∏¥ËÆ°ÁÆóÊàêÊú¨È´òÂíåÁ¥ØÁßØËØØÂ∑ÆÂØºËá¥ÁöÑ‰ªªÂä°ÂÅèÁ¶ªÁ≠âÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑÊΩúÂú®Á©∫Èó¥ÂèçÂêëËßÑÂàíÊñπÊ°àÈÄöËøá‰ªéÊúÄÁªàÁõÆÊ†áÂêëÂêéÊé®ÂØº‰∏≠Èó¥Â≠êÁõÆÊ†áÔºåÁ°Æ‰øù‰ªªÂä°ÂÆåÊàêÁöÑÂêåÊó∂ÊèêÈ´òËßÑÂàíÊïàÁéá„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLBPÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÁªÜÁ≤íÂ∫¶ÂíåÂâçÂêëËßÑÂàíÊñπÊ≥ïÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂΩìÂâçÁöÑÊú∫Âô®‰∫∫ËßÑÂàíÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÈ¢ÑÊµãÂ§öÂ∏ßÂõæÂÉèÁöÑÂÖ®ÂÉèÁ¥†ÁªÜËäÇ„ÄÇËôΩÁÑ∂ËøôÁßçÁªÜÁ≤íÂ∫¶ÁöÑÊñπÊ≥ïÂèØ‰ª•‰Ωú‰∏∫ÈÄöÁî®‰∏ñÁïåÊ®°ÂûãÔºå‰ΩÜÂú®‰∏ãÊ∏∏Á≠ñÁï•Â≠¶‰π†‰∏≠ÂºïÂÖ•‰∫Ü‰∏§Â§ßÊåëÊàòÔºöÂ∑®Â§ßÁöÑËÆ°ÁÆóÊàêÊú¨ÈòªÁ¢çÂÆûÊó∂ÈÉ®ÁΩ≤Ôºå‰ª•ÂèäÁ¥ØÁßØÁöÑ‰∏çÂáÜÁ°ÆÊÄßÂèØËÉΩËØØÂØºÂä®‰ΩúÊèêÂèñ„ÄÇÈÄöËøáÁ≤óÁ≤íÂ∫¶Â≠êÁõÆÊ†áÁöÑËßÑÂàíÈÉ®ÂàÜÁºìËß£‰∫ÜÊïàÁéáÈóÆÈ¢òÔºå‰ΩÜÂâçÂêëËßÑÂàíÊñπÊ°à‰ªçÂèØËÉΩÂõ†Á¥ØÁßØËØØÂ∑ÆÂØºËá¥ÂÅèÁ¶ª‰ªªÂä°ÁõÆÊ†á„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊΩúÂú®Á©∫Èó¥ÂèçÂêëËßÑÂàíÊñπÊ°àÔºàLBPÔºâÔºåÈÄöËøáÂ∞Ü‰ªªÂä°Âü∫Á°ÄÂåñ‰∏∫ÊúÄÁªàÊΩúÂú®ÁõÆÊ†áÔºåÈÄíÂΩíÈ¢ÑÊµãÊõ¥Êé•ËøëÂΩìÂâçÁä∂ÊÄÅÁöÑ‰∏≠Èó¥Â≠êÁõÆÊ†áÔºå‰ªéËÄåÁ°Æ‰øùÂú®Êï¥‰∏™ËßÑÂàíËøáÁ®ã‰∏≠ÂßãÁªàÂÖ≥Ê≥®‰ªªÂä°ÂÆåÊàê„ÄÇÈÄöËøáÂπøÊ≥õÁöÑ‰ªøÁúüÂíåÁúüÂÆûÊú∫Âô®‰∫∫ÈïøÊó∂Èó¥ÂÆûÈ™åÔºåLBPÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÁªÜÁ≤íÂ∫¶ÂíåÂâçÂêëËßÑÂàíÊñπÊ≥ïÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Âú®ÈïøÊó∂Èó¥„ÄÅÂ§öÈò∂ÊÆµ‰ªªÂä°‰∏≠ÁöÑÂÆûÊó∂ÊéßÂà∂ÊïàÁéá‰∏éÂáÜÁ°ÆÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñÁªÜÁ≤íÂ∫¶ÂõæÂÉèÈ¢ÑÊµãÔºåÂØºËá¥ËÆ°ÁÆóÊàêÊú¨È´òÂíåÁ¥ØÁßØËØØÂ∑ÆÂΩ±Âìç‰ªªÂä°ÊâßË°å„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑÊΩúÂú®Á©∫Èó¥ÂèçÂêëËßÑÂàíÔºàLBPÔºâÊñπÊ°àÔºå‰ªéÊúÄÁªàÊΩúÂú®ÁõÆÊ†áÂá∫ÂèëÔºåÈÄíÂΩíÂú∞È¢ÑÊµã‰∏≠Èó¥Â≠êÁõÆÊ†áÔºåÁ°Æ‰øùÂú®Êï¥‰∏™ËßÑÂàíËøáÁ®ã‰∏≠ÂßãÁªàÂÖ≥Ê≥®‰ªªÂä°ÂÆåÊàê„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåLBPËÉΩÂ§üÊúâÊïàÂáèÂ∞ëËØØÂ∑ÆÁ¥ØÁßØÔºåÊèêÈ´òËßÑÂàíÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLBPÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÊòØÂ∞Ü‰ªªÂä°Âü∫Á°ÄÂåñ‰∏∫ÊúÄÁªàÊΩúÂú®ÁõÆÊ†áÔºõÂÖ∂Ê¨°ÊòØÈÄíÂΩíÈ¢ÑÊµã‰∏≠Èó¥Â≠êÁõÆÊ†áÔºõÊúÄÂêéÊòØÂü∫‰∫éÂ≠êÁõÆÊ†áÁöÑÁ≠ñÁï•ÊèêÂèñ„ÄÇÊØè‰∏™Ê®°ÂùóÈÉΩÁ¥ßÂØÜÁõ∏ËøûÔºåÁ°Æ‰øù‰ø°ÊÅØÁöÑÊúâÊïà‰º†ÈÄíÂíå‰ªªÂä°ÁöÑÈ°∫Âà©ÂÆåÊàê„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLBPÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂèçÂêëËßÑÂàíÁöÑÊÄùË∑ØÔºå‰ΩøÂæóËßÑÂàíËøáÁ®ãÂßãÁªàÂÖ≥Ê≥®‰ªªÂä°ÁöÑÊúÄÁªàÁõÆÊ†áÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÂâçÂêëËßÑÂàí‰∏≠Â∏∏ËßÅÁöÑËØØÂ∑ÆÁ¥ØÁßØÈóÆÈ¢ò„ÄÇËøô‰∏ÄÊñπÊ≥ïÂú®ÊïàÁéáÂíåÂáÜÁ°ÆÊÄß‰∏äÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåLBPÂºïÂÖ•‰∫ÜÂèØÂ≠¶‰π†ÁöÑtokenÊù•ÊÄªÁªìÂ≠êÁõÆÊ†áÂ∫èÂàóÔºåÂπ∂ÂÜ≥ÂÆöÊØè‰∏™Â≠êÁõÆÊ†áÂ¶Ç‰ΩïÊåáÂØºÂä®‰ΩúÊèêÂèñ„ÄÇÊ≠§Â§ñÔºåÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüËÄÉËôë‰∫Ü‰ªªÂä°ÂÆåÊàêÂ∫¶Ôºå‰ª•Á°Æ‰øùËßÑÂàíÁöÑÊúâÊïàÊÄß„ÄÇÊï¥‰ΩìÁΩëÁªúÁªìÊûÑÁªèËøá‰ºòÂåñÔºå‰ª•ÈÄÇÂ∫îÈïøÊó∂Èó¥‰ªªÂä°ÁöÑÈúÄÊ±Ç„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLBPÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÁöÑÁªÜÁ≤íÂ∫¶ÂíåÂâçÂêëËßÑÂàíÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%‰ª•‰∏äÔºåÊàêÂäüÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÊ∞¥Âπ≥ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™‰∏ªÊú∫Âô®‰∫∫„ÄÅÊô∫ËÉΩÂà∂ÈÄ†ÂíåÊó†‰∫∫È©æÈ©∂Á≠âÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÂÆûÊó∂ÊéßÂà∂ËÉΩÂäõÔºåLBPËÉΩÂ§üÊòæËëóÊèêÂçáÊú∫Âô®‰∫∫Âú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄßÂíåÊïàÁéáÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks? To address this, we propose a Latent Space Backward Planning scheme (LBP), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction. Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: https://lbp-authors.github.io

