---
layout: default
title: "Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning"
---

# Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06875" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06875v1</a>
  <a href="https://arxiv.org/pdf/2505.06875.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06875v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06875v1', 'Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chengkai Xu, Jiaqi Liu, Yicheng Guo, Yuhang Zhang, Peng Hang, Jian Sun

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¿«æ…¢å†³ç­–æ¡†æ¶ä»¥è§£å†³äººæœºäº¤äº’ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªåŠ¨é©¾é©¶` `äººæœºäº¤äº’` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¸ªæ€§åŒ–é©¾é©¶` `å†³ç­–æ¡†æ¶` `å®‰å…¨æ€§` `å®æ—¶æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªåŠ¨é©¾é©¶æ–¹æ³•å¸¸å¸¸å¿½è§†ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œå¯¼è‡´äº¤äº’å’Œé€‚åº”èƒ½åŠ›ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«æ…¢å†³ç­–æ¡†æ¶ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œå®ç°é«˜æ•ˆçš„ç”¨æˆ·æŒ‡ä»¤è§£æä¸å®æ—¶å†³ç­–ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨é™ä½ç¢°æ’ç‡çš„åŒæ—¶ï¼Œæ›´å¥½åœ°ç¬¦åˆç”¨æˆ·çš„é©¾é©¶åå¥½ï¼Œæå‡äº†å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªåŠ¨é©¾é©¶æŠ€æœ¯åœ¨æ•°æ®é©±åŠ¨æ–¹æ³•ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸å¸¸å¿½è§†ç”¨æˆ·ç‰¹å®šåå¥½ï¼Œç¼ºä¹ä¸ç”¨æˆ·çš„äº’åŠ¨å’Œé€‚åº”èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§â€œå¿«æ…¢â€å†³ç­–æ¡†æ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé«˜å±‚æŒ‡ä»¤è§£æå’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†è¿›è¡Œä½å±‚å®æ—¶å†³ç­–ã€‚åœ¨è¯¥åŒé‡ç³»ç»Ÿä¸­ï¼ŒLLMä½œä¸ºâ€œæ…¢â€æ¨¡å—ï¼Œå°†ç”¨æˆ·æŒ‡ä»¤è½¬åŒ–ä¸ºç»“æ„åŒ–æŒ‡å¯¼ï¼Œè€ŒRLä»£ç†ä½œä¸ºâ€œå¿«â€æ¨¡å—ï¼Œåœ¨ä¸¥æ ¼çš„å»¶è¿Ÿçº¦æŸä¸‹è¿›è¡Œæ—¶é—´æ•æ„Ÿçš„æ“ä½œã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§é©¾é©¶åœºæ™¯ä¸­æœ‰æ•ˆé™ä½äº†ç¢°æ’ç‡ï¼Œå¹¶æ›´å¥½åœ°ä¸ç”¨æˆ·åå¥½å¯¹é½ï¼Œå®ç°äº†ä»¥äººä¸ºæœ¬çš„é©¾é©¶æ¨¡å¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨ç”¨æˆ·ä¸ªæ€§åŒ–éœ€æ±‚å’Œå®æ—¶å†³ç­–ä¹‹é—´çš„çŸ›ç›¾ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ•´åˆç”¨æˆ·æŒ‡ä»¤ä¸é©¾é©¶è¡Œä¸ºã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¿«æ…¢å†³ç­–æ¡†æ¶ï¼ŒLLMè´Ÿè´£é«˜å±‚æŒ‡ä»¤è§£æï¼ŒRLä»£ç†è´Ÿè´£ä½å±‚å®æ—¶å†³ç­–ï¼Œä»è€Œå®ç°ä¸ªæ€§åŒ–ä¸å®‰å…¨æ€§çš„å¹³è¡¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ…¢æ¨¡å—ï¼ˆLLMï¼‰ç”¨äºè§£æç”¨æˆ·æŒ‡ä»¤å¹¶ç”Ÿæˆç»“æ„åŒ–æŒ‡å¯¼ï¼Œå¿«æ¨¡å—ï¼ˆRLä»£ç†ï¼‰è´Ÿè´£åœ¨ä¸¥æ ¼çš„æ—¶é—´é™åˆ¶ä¸‹æ‰§è¡Œå†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šé€šè¿‡å°†é«˜å±‚å†³ç­–ä¸å¿«é€Ÿæ§åˆ¶è§£è€¦ï¼Œæœ¬æ–‡çš„æ¡†æ¶å®ç°äº†ä¸ªæ€§åŒ–çš„ç”¨æˆ·ä¸­å¿ƒæ“ä½œï¼ŒåŒæ—¶ä¿æŒäº†å®‰å…¨è¾¹é™…ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„é›†æˆå¼å†³ç­–æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLLMçš„è¾“å‡ºç»“æ„åŒ–æŒ‡å¯¼åŒ…æ‹¬ç”¨æˆ·åå¥½çš„å¤šæ ·åŒ–è¡¨è¾¾ï¼Œè€ŒRLä»£ç†åˆ™é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ï¼Œç¡®ä¿åœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸­çš„å®æ—¶å“åº”èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨å¤šç§é©¾é©¶åœºæ™¯ä¸‹æœ‰æ•ˆé™ä½äº†ç¢°æ’ç‡ï¼Œä¸åŸºçº¿ç®—æ³•ç›¸æ¯”ï¼Œç¢°æ’ç‡é™ä½äº†æ˜¾è‘—å¹…åº¦ï¼ŒåŒæ—¶é©¾é©¶è¡Œä¸ºä¸ç”¨æˆ·åå¥½çš„å¥‘åˆåº¦æé«˜ï¼Œå±•ç¤ºäº†äººæœºäº¤äº’çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶æ±½è½¦ä»¥åŠäººæœºäº¤äº’ç•Œé¢ç­‰ã€‚é€šè¿‡å®ç°ç”¨æˆ·ä¸ªæ€§åŒ–çš„é©¾é©¶ä½“éªŒï¼Œæœªæ¥å¯æå‡ä¹˜å®¢çš„å®‰å…¨æ„Ÿå’Œæ»¡æ„åº¦ï¼Œæ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous driving has made significant strides through data-driven techniques, achieving robust performance in standardized tasks. However, existing methods frequently overlook user-specific preferences, offering limited scope for interaction and adaptation with users. To address these challenges, we propose a "fast-slow" decision-making framework that integrates a Large Language Model (LLM) for high-level instruction parsing with a Reinforcement Learning (RL) agent for low-level real-time decision. In this dual system, the LLM operates as the "slow" module, translating user directives into structured guidance, while the RL agent functions as the "fast" module, making time-critical maneuvers under stringent latency constraints. By decoupling high-level decision making from rapid control, our framework enables personalized user-centric operation while maintaining robust safety margins. Experimental evaluations across various driving scenarios demonstrate the effectiveness of our method. Compared to baseline algorithms, the proposed architecture not only reduces collision rates but also aligns driving behaviors more closely with user preferences, thereby achieving a human-centric mode. By integrating user guidance at the decision level and refining it with real-time control, our framework bridges the gap between individual passenger needs and the rigor required for safe, reliable driving in complex traffic environments.

