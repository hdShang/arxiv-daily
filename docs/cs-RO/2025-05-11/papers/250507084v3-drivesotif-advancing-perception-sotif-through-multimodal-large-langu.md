---
layout: default
title: "DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models"
---

# DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07084" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07084v3</a>
  <a href="https://arxiv.org/pdf/2505.07084.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07084v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07084v3', 'DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shucheng Huang, Freda Shi, Chen Sun, Jiaming Zhong, Minghao Ning, Yufeng Yang, Yukun Lu, Hong Wang, Amir Khajepour

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-11 (æ›´æ–°: 2025-09-09)

**å¤‡æ³¨**: This work has been accepted to IEEE Transactions on Vehicular Technology. Please refer to the copyright notice for additional information

**DOI**: [10.1109/TVT.2025.3608811](https://doi.org/10.1109/TVT.2025.3608811)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDriveSOTIFä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„æ„ŸçŸ¥å®‰å…¨é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹` `æ„å›¾åŠŸèƒ½å®‰å…¨` `è‡ªåŠ¨é©¾é©¶` `æ„ŸçŸ¥èƒ½åŠ›` `è§†è§‰é—®ç­”` `æ¨¡å‹å¾®è°ƒ` `å®‰å…¨é£é™©è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚é©¾é©¶ç¯å¢ƒä¸­ç¼ºä¹äººç±»é©¾é©¶å‘˜çš„æ„ŸçŸ¥å’Œååº”èƒ½åŠ›ï¼Œå¯¼è‡´SOTIFé£é™©ç®¡ç†å›°éš¾ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨å®šåˆ¶æ•°æ®é›†æ•æ‰æ„ŸçŸ¥ç›¸å…³çš„SOTIFåœºæ™¯ï¼Œä»¥æå‡è‡ªåŠ¨é©¾é©¶çš„å®‰å…¨æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æé«˜11.8%å’Œ12.0%ï¼Œä¸”ä¿æŒå®æ—¶æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººç±»é©¾é©¶å‘˜å…·å¤‡ç©ºé—´å’Œå› æœæ™ºèƒ½ï¼Œèƒ½å¤Ÿæ„ŸçŸ¥é©¾é©¶åœºæ™¯ã€é¢„åˆ¤å±é™©å¹¶å¯¹åŠ¨æ€ç¯å¢ƒä½œå‡ºååº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨è¿™äº›èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æˆ–ä¸å¯é¢„æµ‹çš„é©¾é©¶æ¡ä»¶ä¸‹ï¼Œéš¾ä»¥ç®¡ç†ä¸æ„ŸçŸ¥ç›¸å…³çš„æ„å›¾åŠŸèƒ½å®‰å…¨ï¼ˆSOTIFï¼‰é£é™©ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†æ•æ‰æ„ŸçŸ¥ç›¸å…³çš„SOTIFåœºæ™¯ã€‚åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„MLLMsåœ¨å°é—­å¼è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å‡†ç¡®ç‡ä¸Šæé«˜äº†11.8%ï¼Œå¼€æ”¾å¼VQAå¾—åˆ†æé«˜äº†12.0%ï¼ŒåŒæ—¶ä¿æŒäº†æ¯å¼ å›¾åƒå¹³å‡0.59ç§’çš„å®æ—¶æ¨ç†æ€§èƒ½ã€‚é€šè¿‡åœ¨åŠ æ‹¿å¤§å’Œä¸­å›½çš„å®é™…æ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¾®è°ƒæ¨¡å‹èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å³ä½¿æ˜¯ç»éªŒä¸°å¯Œçš„äººç±»é©¾é©¶å‘˜ä¹Ÿé¢ä¸´çš„å®‰å…¨é£é™©ã€‚è¿™é¡¹å·¥ä½œæ˜¯é¢†åŸŸç‰¹å®šçš„MLLMå¾®è°ƒåœ¨è‡ªåŠ¨é©¾é©¶SOTIFé¢†åŸŸçš„é¦–æ¬¡åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†SOTIFé£é™©æ—¶å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€å’Œä¸å¯é¢„æµ‹çš„é©¾é©¶åœºæ™¯ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œåˆ©ç”¨ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†æ¥æ•æ‰å’Œç†è§£æ„ŸçŸ¥ç›¸å…³çš„SOTIFåœºæ™¯ï¼Œä»è€Œå¢å¼ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€æ¨¡å‹å¾®è°ƒå’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é›†ä¸“æ³¨äºæ„ŸçŸ¥ç›¸å…³çš„SOTIFåœºæ™¯ï¼Œæ¨¡å‹å¾®è°ƒåˆ™é’ˆå¯¹è¿™äº›ç‰¹å®šåœºæ™¯è¿›è¡Œä¼˜åŒ–ï¼Œæœ€åé€šè¿‡åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡å°†é¢†åŸŸç‰¹å®šçš„MLLMå¾®è°ƒåº”ç”¨äºè‡ªåŠ¨é©¾é©¶çš„SOTIFé¢†åŸŸï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œå‚æ•°è®¾ç½®ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ„ŸçŸ¥ç›¸å…³çš„ç‰¹å¾ï¼Œå¹¶åœ¨å®æ—¶æ¨ç†ä¸­ä¿æŒé«˜æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°é—­å¼è§†è§‰é—®ç­”å‡†ç¡®ç‡ä¸Šæå‡äº†11.8%ï¼Œå¼€æ”¾å¼è§†è§‰é—®ç­”å¾—åˆ†æé«˜äº†12.0%ã€‚åŒæ—¶ï¼Œæ¨¡å‹ä¿æŒäº†æ¯å¼ å›¾åƒå¹³å‡0.59ç§’çš„æ¨ç†æ—¶é—´ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å®æ—¶æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œå®‰å…¨ç›‘æ§ç­‰ã€‚é€šè¿‡æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½äº¤é€šäº‹æ•…é£é™©ï¼Œæé«˜è¡Œè½¦å®‰å…¨æ€§ï¼Œæœªæ¥å¯èƒ½å¯¹æ™ºèƒ½äº¤é€šçš„æ™®åŠå’Œå‘å±•äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human drivers possess spatial and causal intelligence, enabling them to perceive driving scenarios, anticipate hazards, and react to dynamic environments. In contrast, autonomous vehicles lack these abilities, making it challenging to manage perception-related Safety of the Intended Functionality (SOTIF) risks, especially under complex or unpredictable driving conditions. To address this gap, we propose fine-tuning multimodal large language models (MLLMs) on a customized dataset specifically designed to capture perception-related SOTIF scenarios. Benchmarking results show that fine-tuned MLLMs achieve an 11.8\% improvement in close-ended VQA accuracy and a 12.0\% increase in open-ended VQA scores compared to baseline models, while maintaining real-time performance with a 0.59-second average inference time per image. We validate our approach through real-world case studies in Canada and China, where fine-tuned models correctly identify safety risks that challenge even experienced human drivers. This work represents the first application of domain-specific MLLM fine-tuning for SOTIF domain in autonomous driving. The dataset and related resources are available at github.com/s95huang/DriveSOTIF.git

