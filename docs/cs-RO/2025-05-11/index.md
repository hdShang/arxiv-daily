---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-11
---

# cs.ROï¼ˆ2025-05-11ï¼‰

ğŸ“Š å…± **12** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (7 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (7 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250507096v5-x-sim-cross-embodiment-learning-via-real-to-sim-to-real.html">X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</a></td>
  <td>æå‡ºX-Simæ¡†æ¶ä»¥è§£å†³æœºå™¨äººæ¨¡ä»¿å­¦ä¹ ä¸­çš„åŠ¨ä½œæ ‡ç­¾ç¼ºå¤±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim-to-real</span> <span class="paper-tag">teleoperation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07096v5" data-paper-url="./papers/250507096v5-x-sim-cross-embodiment-learning-via-real-to-sim-to-real.html" onclick="toggleFavorite(this, '2505.07096v5', 'X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250506883v2-facet-force-adaptive-control-via-impedance-reference-tracking-for-le.html">FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots</a></td>
  <td>æå‡ºFACETä»¥è§£å†³è…¿éƒ¨æœºå™¨äººåœ¨åŠ›äº¤äº’ä¸­çš„æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">legged robot</span> <span class="paper-tag">humanoid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06883v2" data-paper-url="./papers/250506883v2-facet-force-adaptive-control-via-impedance-reference-tracking-for-le.html" onclick="toggleFavorite(this, '2505.06883v2', 'FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250506832v1-unidiffgrasp-a-unified-framework-integrating-vlm-reasoning-and-vlm-g.html">UniDiffGrasp: A Unified Framework Integrating VLM Reasoning and VLM-Guided Part Diffusion for Open-Vocabulary Constrained Grasping with Dual Arms</a></td>
  <td>æå‡ºUniDiffGraspä»¥è§£å†³åŒè‡‚å¼€æ”¾è¯æ±‡çº¦æŸæŠ“å–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">dual-arm</span> <span class="paper-tag">open-vocabulary</span> <span class="paper-tag">open vocabulary</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06832v1" data-paper-url="./papers/250506832v1-unidiffgrasp-a-unified-framework-integrating-vlm-reasoning-and-vlm-g.html" onclick="toggleFavorite(this, '2505.06832v1', 'UniDiffGrasp: A Unified Framework Integrating VLM Reasoning and VLM-Guided Part Diffusion for Open-Vocabulary Constrained Grasping with Dual Arms')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250506919v1-the-first-wara-robotics-mobile-manipulation-challenge-lessons-learne.html">The First WARA Robotics Mobile Manipulation Challenge -- Lessons Learned</a></td>
  <td>æå‡ºç§»åŠ¨æ“æ§æŒ‘æˆ˜ä»¥è§£å†³å®éªŒå®¤è‡ªåŠ¨åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06919v1" data-paper-url="./papers/250506919v1-the-first-wara-robotics-mobile-manipulation-challenge-lessons-learne.html" onclick="toggleFavorite(this, '2505.06919v1', 'The First WARA Robotics Mobile Manipulation Challenge -- Lessons Learned')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250506923v1-yopov2-tracker-an-end-to-end-agile-tracking-and-navigation-framework.html">YOPOv2-Tracker: An End-to-End Agile Tracking and Navigation Framework from Perception to Action</a></td>
  <td>æå‡ºYOPOv2-Trackerä»¥è§£å†³å››æ—‹ç¿¼é«˜å»¶è¿Ÿè·Ÿè¸ªä¸å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">trajectory optimization</span> <span class="paper-tag">motion planning</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06923v1" data-paper-url="./papers/250506923v1-yopov2-tracker-an-end-to-end-agile-tracking-and-navigation-framework.html" onclick="toggleFavorite(this, '2505.06923v1', 'YOPOv2-Tracker: An End-to-End Agile Tracking and Navigation Framework from Perception to Action')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250506794v1-dynamic-safety-in-complex-environments-synthesizing-safety-filters-w.html">Dynamic Safety in Complex Environments: Synthesizing Safety Filters with Poisson's Equation</a></td>
  <td>æå‡ºåŸºäºæ³Šæ¾æ–¹ç¨‹çš„å®‰å…¨è¿‡æ»¤å™¨ä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸­çš„å®‰å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06794v1" data-paper-url="./papers/250506794v1-dynamic-safety-in-complex-environments-synthesizing-safety-filters-w.html" onclick="toggleFavorite(this, '2505.06794v1', 'Dynamic Safety in Complex Environments: Synthesizing Safety Filters with Poisson&#39;s Equation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250506791v1-cprrtc-gpu-parallel-rrt-connect-for-constrained-motion-planning.html">cpRRTC: GPU-Parallel RRT-Connect for Constrained Motion Planning</a></td>
  <td>æå‡ºcpRRTCä»¥è§£å†³çº¦æŸè¿åŠ¨è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06791v1" data-paper-url="./papers/250506791v1-cprrtc-gpu-parallel-rrt-connect-for-constrained-motion-planning.html" onclick="toggleFavorite(this, '2505.06791v1', 'cpRRTC: GPU-Parallel RRT-Connect for Constrained Motion Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (4 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250506875v1-towards-human-centric-autonomous-driving-a-fast-slow-architecture-in.html">Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning</a></td>
  <td>æå‡ºå¿«æ…¢å†³ç­–æ¡†æ¶ä»¥è§£å†³äººæœºäº¤äº’ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06875v1" data-paper-url="./papers/250506875v1-towards-human-centric-autonomous-driving-a-fast-slow-architecture-in.html" onclick="toggleFavorite(this, '2505.06875v1', 'Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250506963v1-reinforcement-learning-based-monocular-vision-approach-for-autonomou.html">Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing</a></td>
  <td>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å•ç›®è§†è§‰æ–¹æ³•ä»¥è§£å†³æ— äººæœºè‡ªä¸»ç€é™†é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">depth estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06963v1" data-paper-url="./papers/250506963v1-reinforcement-learning-based-monocular-vision-approach-for-autonomou.html" onclick="toggleFavorite(this, '2505.06963v1', 'Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250506861v2-efficient-robotic-policy-learning-via-latent-space-backward-planning.html">Efficient Robotic Policy Learning via Latent Space Backward Planning</a></td>
  <td>æå‡ºæ½œåœ¨ç©ºé—´åå‘è§„åˆ’ä»¥è§£å†³æœºå™¨äººå®æ—¶æ§åˆ¶æ•ˆç‡ä¸å‡†ç¡®æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">policy learning</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06861v2" data-paper-url="./papers/250506861v2-efficient-robotic-policy-learning-via-latent-space-backward-planning.html" onclick="toggleFavorite(this, '2505.06861v2', 'Efficient Robotic Policy Learning via Latent Space Backward Planning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250506906v1-realistic-counterfactual-explanations-for-machine-learning-controlle.html">Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR</a></td>
  <td>æå‡ºä¸€ç§åŸºäº2D LiDARçš„ç°å®åäº‹å®è§£é‡Šæ–¹æ³•ä»¥æå‡ç§»åŠ¨æœºå™¨äººå¯è§£é‡Šæ€§</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span> <span class="paper-tag">DRL</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.06906v1" data-paper-url="./papers/250506906v1-realistic-counterfactual-explanations-for-machine-learning-controlle.html" onclick="toggleFavorite(this, '2505.06906v1', 'Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250507084v3-drivesotif-advancing-perception-sotif-through-multimodal-large-langu.html">DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models</a></td>
  <td>æå‡ºDriveSOTIFä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„æ„ŸçŸ¥å®‰å…¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.07084v3" data-paper-url="./papers/250507084v3-drivesotif-advancing-perception-sotif-through-multimodal-large-langu.html" onclick="toggleFavorite(this, '2505.07084v3', 'DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)