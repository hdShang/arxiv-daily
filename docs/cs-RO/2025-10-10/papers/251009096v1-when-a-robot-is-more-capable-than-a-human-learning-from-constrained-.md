---
layout: default
title: "When a Robot is More Capable than a Human: Learning from Constrained Demonstrators"
---

# When a Robot is More Capable than a Human: Learning from Constrained Demonstrators

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09096" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.09096v1</a>
  <a href="https://arxiv.org/pdf/2510.09096.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09096v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09096v1', 'When a Robot is More Capable than a Human: Learning from Constrained Demonstrators')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinhu Li, Ayush Jain, Zhaojing Yang, Yigit Korkmaz, Erdem BÄ±yÄ±k

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å—é™ç¤ºæ•™è€…æ•°æ®ï¼Œæœºå™¨äººå­¦ä¹ è¶…è¶Šäººç±»èƒ½åŠ›çš„ç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººå­¦ä¹ ` `å—é™ç¤ºæ•™` `å¥–åŠ±å‡½æ•°` `æ—¶é—´æ’å€¼` `è½¨è¿¹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¾èµ–ä¸“å®¶ç¤ºæ•™ï¼Œä½†ç¤ºæ•™ç•Œé¢ï¼ˆå¦‚æ“çºµæ†ï¼‰çš„é™åˆ¶å¯¼è‡´ä¸“å®¶æ— æ³•å±•ç¤ºæœ€ä¼˜ç­–ç•¥ï¼Œé™åˆ¶äº†å­¦ä¹ æ•ˆæœã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œå…è®¸æœºå™¨äººè¶…è¶Šç›´æ¥æ¨¡ä»¿ï¼Œé€šè¿‡æ¢ç´¢æ›´ä¼˜è½¨è¿¹æ¥å­¦ä¹ ç­–ç•¥ï¼Œä»è€Œå…‹æœä¸“å®¶ç¤ºæ•™çš„å±€é™æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡å®Œæˆæ—¶é—´ä¸Šä¼˜äºä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ ï¼Œåœ¨çœŸå®æœºæ¢°è‡‚ä¸Šé€Ÿåº¦æå‡æ˜¾è‘—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ ä½¿å¾—ä¸“å®¶å¯ä»¥é€šè¿‡åŠ¨è§‰ç¤ºæ•™ã€æ“çºµæ†æ§åˆ¶å’Œsim-to-realè¿ç§»ç­‰æ–¹å¼æ•™å¯¼æœºå™¨äººå¤æ‚çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›äº¤äº’æ–¹å¼å¸¸å¸¸é™åˆ¶äº†ä¸“å®¶å±•ç¤ºæœ€ä¼˜è¡Œä¸ºçš„èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä»¬å­˜åœ¨é—´æ¥æ§åˆ¶ã€è®¾ç½®çº¦æŸå’Œç¡¬ä»¶å®‰å…¨ç­‰é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œæ“çºµæ†å¯èƒ½åªèƒ½åœ¨2Då¹³é¢å†…ç§»åŠ¨æœºæ¢°è‡‚ï¼Œå³ä½¿æœºå™¨äººå¯ä»¥åœ¨æ›´é«˜ç»´åº¦çš„ç©ºé—´ä¸­æ“ä½œã€‚å› æ­¤ï¼Œç”±å—é™ä¸“å®¶æ”¶é›†çš„æ¼”ç¤ºä¼šå¯¼è‡´å­¦ä¹ ç­–ç•¥çš„æ¬¡ä¼˜æ€§èƒ½ã€‚è¿™å°±æå‡ºäº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæœºå™¨äººèƒ½å¦å­¦ä¹ åˆ°æ¯”å—é™ä¸“å®¶æ¼”ç¤ºçš„æ›´å¥½çš„ç­–ç•¥ï¼Ÿæˆ‘ä»¬é€šè¿‡å…è®¸æ™ºèƒ½ä½“è¶…è¶Šå¯¹ä¸“å®¶åŠ¨ä½œçš„ç›´æ¥æ¨¡ä»¿ï¼Œæ¢ç´¢æ›´çŸ­ã€æ›´æœ‰æ•ˆçš„è½¨è¿¹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨æ¼”ç¤ºæ¥æ¨æ–­ä¸€ä¸ªä»…åŸºäºçŠ¶æ€çš„å¥–åŠ±ä¿¡å·ï¼Œè¯¥ä¿¡å·è¡¡é‡ä»»åŠ¡è¿›åº¦ï¼Œå¹¶ä½¿ç”¨æ—¶é—´æ’å€¼è‡ªæ ‡è®°æœªçŸ¥çŠ¶æ€çš„å¥–åŠ±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡å®Œæˆæ—¶é—´æ–¹é¢éƒ½ä¼˜äºå¸¸è§çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚åœ¨çœŸå®çš„WidowXæœºæ¢°è‡‚ä¸Šï¼Œå®ƒåœ¨12ç§’å†…å®Œæˆä»»åŠ¡ï¼Œæ¯”è¡Œä¸ºå…‹éš†å¿«10å€ï¼Œç›¸å…³è§†é¢‘å¯åœ¨https://sites.google.com/view/constrainedexpertä¸Šæ‰¾åˆ°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ä»å—é™ç¤ºæ•™è€…å¤„å­¦ä¹ ç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç›´æ¥æ¨¡ä»¿ä¸“å®¶çš„è¡Œä¸ºï¼Œä½†å½“ä¸“å®¶å—åˆ°ç•Œé¢æˆ–ç¯å¢ƒçš„é™åˆ¶æ—¶ï¼Œå…¶ç¤ºæ•™è½¨è¿¹å¯èƒ½å¹¶éæœ€ä¼˜ã€‚è¿™å¯¼è‡´å­¦ä¹ åˆ°çš„ç­–ç•¥æ€§èƒ½å—é™ï¼Œæ— æ³•å……åˆ†å‘æŒ¥æœºå™¨äººçš„æ½œåŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯è®©æœºå™¨äººä¸ä»…ä»…æ¨¡ä»¿ä¸“å®¶çš„åŠ¨ä½œï¼Œè€Œæ˜¯é€šè¿‡æ¢ç´¢æ¥å‘ç°æ›´ä¼˜çš„è½¨è¿¹ã€‚è®ºæ–‡åˆ©ç”¨ä¸“å®¶ç¤ºæ•™æ¥æ¨æ–­ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œè¯¥å¥–åŠ±å‡½æ•°è¡¡é‡ä»»åŠ¡çš„è¿›å±•ï¼Œå¹¶å…è®¸æœºå™¨äººåœ¨è¯¥å¥–åŠ±å‡½æ•°çš„æŒ‡å¯¼ä¸‹è¿›è¡Œæ¢ç´¢ï¼Œä»è€Œæ‰¾åˆ°æ¯”ä¸“å®¶ç¤ºæ•™æ›´å¥½çš„ç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) ä»å—é™ä¸“å®¶å¤„æ”¶é›†ç¤ºæ•™æ•°æ®ï¼›2) ä»ç¤ºæ•™æ•°æ®ä¸­æ¨æ–­ä¸€ä¸ªåŸºäºçŠ¶æ€çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å¥–åŠ±å‡½æ•°åæ˜ äº†ä»»åŠ¡çš„è¿›å±•ï¼›3) ä½¿ç”¨æ—¶é—´æ’å€¼æ–¹æ³•å¯¹æœªè§è¿‡çš„çŠ¶æ€è¿›è¡Œå¥–åŠ±è‡ªæ ‡è®°ï¼›4) åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåœ¨è¯¥å¥–åŠ±å‡½æ•°çš„æŒ‡å¯¼ä¸‹è®­ç»ƒæœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿæ¢ç´¢æ›´ä¼˜çš„è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºï¼Œå®ƒå…è®¸æœºå™¨äººè¶…è¶Šå¯¹ä¸“å®¶åŠ¨ä½œçš„ç›´æ¥æ¨¡ä»¿ï¼Œé€šè¿‡æ¢ç´¢æ¥å‘ç°æ›´ä¼˜çš„ç­–ç•¥ã€‚è¿™ä¸ä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¸åŒï¼Œåè€…é€šå¸¸åªæ˜¯ç®€å•åœ°å¤åˆ¶ä¸“å®¶çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ—¶é—´æ’å€¼è¿›è¡Œå¥–åŠ±è‡ªæ ‡è®°ä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®åˆ›æ–°ï¼Œå®ƒå…è®¸æœºå™¨äººåœ¨æ²¡æœ‰ä¸“å®¶ç¤ºæ•™çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å¯¹æœªçŸ¥çš„çŠ¶æ€è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå®ƒéœ€è¦èƒ½å¤Ÿå‡†ç¡®åœ°åæ˜ ä»»åŠ¡çš„è¿›å±•ã€‚è®ºæ–‡ä¸­å…·ä½“å¦‚ä½•è®¾è®¡å¥–åŠ±å‡½æ•°çš„ç»†èŠ‚æœªçŸ¥ã€‚æ—¶é—´æ’å€¼æ–¹æ³•çš„å…·ä½“å®ç°æ–¹å¼ä¹ŸæœªçŸ¥ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é€‰æ‹©ä¹Ÿå¯èƒ½å½±å“æœ€ç»ˆçš„æ€§èƒ½ï¼Œå…·ä½“ä½¿ç”¨äº†å“ªç§ç®—æ³•æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®WidowXæœºæ¢°è‡‚ä¸Šï¼Œä»»åŠ¡å®Œæˆæ—¶é—´ä»…ä¸º12ç§’ï¼Œæ¯”è¡Œä¸ºå…‹éš†æ–¹æ³•å¿«10å€ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å—é™ç¤ºæ•™æ•°æ®ï¼Œå­¦ä¹ åˆ°æ¯”ä¸“å®¶ç¤ºæ•™æ›´å¥½çš„ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººçš„æ€§èƒ½ã€‚å…·ä½“çš„æ ·æœ¬æ•ˆç‡æå‡æ•°æ®æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦æœºå™¨äººè¾…åŠ©å®Œæˆä»»åŠ¡çš„åœºæ™¯ï¼Œå°¤å…¶æ˜¯åœ¨ä¸“å®¶æ“ä½œå—åˆ°é™åˆ¶çš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—æ‰‹æœ¯ä¸­ï¼ŒåŒ»ç”Ÿå¯èƒ½å—åˆ°æ“ä½œç©ºé—´çš„é™åˆ¶ï¼Œæ— æ³•å±•ç¤ºæœ€ä¼˜çš„æ‰‹æœ¯è·¯å¾„ï¼Œæ­¤æ—¶æœºå™¨äººå¯ä»¥é€šè¿‡å­¦ä¹ å’Œæ¢ç´¢ï¼Œæ‰¾åˆ°æ›´ç²¾ç¡®ã€æ›´é«˜æ•ˆçš„æ‰‹æœ¯æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ã€ç¾éš¾æ•‘æ´ç­‰é¢†åŸŸï¼Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning from demonstrations enables experts to teach robots complex tasks using interfaces such as kinesthetic teaching, joystick control, and sim-to-real transfer. However, these interfaces often constrain the expert's ability to demonstrate optimal behavior due to indirect control, setup restrictions, and hardware safety. For example, a joystick can move a robotic arm only in a 2D plane, even though the robot operates in a higher-dimensional space. As a result, the demonstrations collected by constrained experts lead to suboptimal performance of the learned policies. This raises a key question: Can a robot learn a better policy than the one demonstrated by a constrained expert? We address this by allowing the agent to go beyond direct imitation of expert actions and explore shorter and more efficient trajectories. We use the demonstrations to infer a state-only reward signal that measures task progress, and self-label reward for unknown states using temporal interpolation. Our approach outperforms common imitation learning in both sample efficiency and task completion time. On a real WidowX robotic arm, it completes the task in 12 seconds, 10x faster than behavioral cloning, as shown in real-robot videos on https://sites.google.com/view/constrainedexpert .

