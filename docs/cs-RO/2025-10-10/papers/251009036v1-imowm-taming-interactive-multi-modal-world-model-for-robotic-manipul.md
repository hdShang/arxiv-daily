---
layout: default
title: iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation
---

# iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.09036" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.09036v1</a>
  <a href="https://arxiv.org/pdf/2510.09036.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.09036v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.09036v1', 'iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Chuanrui Zhang, Zhengxian Wu, Guanxing Lu, Yansong Tang, Ziwei Wang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://xingyoujun.github.io/imowm/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫iMoWMÔºåÂà©Áî®‰∫§‰∫íÂºèÂ§öÊ®°ÊÄÅ‰∏ñÁïåÊ®°ÂûãÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `‰∏ñÁïåÊ®°Âûã` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Ê∑±Â∫¶Â≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ2DËßÜÈ¢ë‰∏ñÁïåÊ®°ÂûãÁº∫‰πèÂá†‰ΩïÊé®ÁêÜËÉΩÂäõÔºåÈöæ‰ª•ÊúâÊïàÊ®°Êãü3DÁâ©ÁêÜ‰∏ñÁïåÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÊÄßËÉΩ„ÄÇ
2. iMoWMÈÄöËøáËá™ÂõûÂΩíÁîüÊàêÂΩ©Ëâ≤ÂõæÂÉè„ÄÅÊ∑±Â∫¶ÂõæÂíåÊú∫Ê¢∞ËáÇÊé©Á†ÅÔºåÂπ∂‰ΩøÁî®MMTokenizerÂ∞ÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂéãÁº©‰∏∫Á¥ßÂáëÁöÑtokenË°®Á§∫„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåiMoWMÊèêÂçá‰∫ÜÊú™Êù•È¢ÑÊµãÁöÑËßÜËßâË¥®ÈáèÔºåÂπ∂ÊúâÊïàÊîØÊåÅÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÂíåÊ®°‰ªøÂ≠¶‰π†„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫iMoWMÁöÑ‰∫§‰∫íÂºè‰∏ñÁïåÊ®°ÂûãÔºåÊó®Âú®ÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúËÉΩÂäõ„ÄÇÁé∞ÊúâÂü∫‰∫é2DËßÜÈ¢ëÁöÑ‰∏ñÁïåÊ®°ÂûãÁº∫‰πèÂá†‰ΩïÂíåÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÔºåÈöæ‰ª•ÊçïÊçâ3D‰∏ñÁïåÁöÑÁâ©ÁêÜÁªìÊûÑ„ÄÇiMoWMÈÄöËøáËá™ÂõûÂΩíÊñπÂºèÁîüÊàêÂΩ©Ëâ≤ÂõæÂÉè„ÄÅÊ∑±Â∫¶ÂõæÂíåÊú∫Âô®‰∫∫ÊâãËáÇÊé©Á†ÅÔºåÂπ∂‰ª•Âä®‰Ωú‰∏∫Êù°‰ª∂„ÄÇ‰∏∫‰∫ÜÂÖãÊúç‰∏âÁª¥‰ø°ÊÅØÂ∏¶Êù•ÁöÑÈ´òËÆ°ÁÆóÊàêÊú¨ÔºåËÆ∫ÊñáÊèêÂá∫‰∫ÜMMTokenizerÔºåÂ∞ÜÂ§öÊ®°ÊÄÅËæìÂÖ•Áªü‰∏Ä‰∏∫Á¥ßÂáëÁöÑtokenË°®Á§∫„ÄÇËøô‰ΩøÂæóiMoWMËÉΩÂ§üÂà©Áî®Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÁöÑVideoGPTÊ®°ÂûãÔºåÂêåÊó∂‰øùÊåÅÈ´òÊïàÁéáÂπ∂ËûçÂÖ•Êõ¥‰∏∞ÂØåÁöÑÁâ©ÁêÜ‰ø°ÊÅØ„ÄÇÂá≠ÂÄüÂÖ∂Â§öÊ®°ÊÄÅË°®Á§∫ÔºåiMoWM‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊú™Êù•È¢ÑÊµãÁöÑËßÜËßâË¥®ÈáèÔºåËøòÂèØ‰Ωú‰∏∫Âü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàMBRLÔºâÁöÑÊúâÊïàÊ®°ÊãüÂô®ÔºåÂπ∂‰øÉËøõÁúüÂÆû‰∏ñÁïåÁöÑÊ®°‰ªøÂ≠¶‰π†„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåiMoWMÂú®Ëøô‰∫õ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ±ïÁ§∫‰∫ÜÂ§öÊ®°ÊÄÅ‰∏ñÁïåÂª∫Ê®°Âú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑ‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éËßÜÈ¢ëÁöÑ‰∏ñÁïåÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Èù¢‰∏¥ÊåëÊàòÔºå‰∏ªË¶ÅÂéüÂõ†ÊòØÂÆÉ‰ª¨Èöæ‰ª•ÊçïÊçâ3DÁéØÂ¢ÉÁöÑÂá†‰ΩïÂíåÁ©∫Èó¥‰ø°ÊÅØ„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®2DÂõæÂÉèÔºåÂøΩÁï•‰∫ÜÊ∑±Â∫¶‰ø°ÊÅØÂíåÊú∫Âô®‰∫∫Ëá™Ë∫´Áä∂ÊÄÅÔºåÂØºËá¥È¢ÑÊµãÁ≤æÂ∫¶ÂíåÊ≥õÂåñËÉΩÂäõÂèóÈôê„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞ÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØËûçÂÖ•‰∏ñÁïåÊ®°ÂûãÔºåÂπ∂Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÊòØ‰∫üÂæÖËß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöiMoWMÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™ËÉΩÂ§üÁêÜËß£ÂíåÁîüÊàêÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑ‰∫§‰∫íÂºè‰∏ñÁïåÊ®°Âûã„ÄÇÈÄöËøáÂêåÊó∂È¢ÑÊµãÂΩ©Ëâ≤ÂõæÂÉè„ÄÅÊ∑±Â∫¶ÂõæÂíåÊú∫Âô®‰∫∫ÊâãËáÇÊé©Á†ÅÔºåÊ®°ÂûãËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ÁêÜËß£ÁéØÂ¢ÉÂíåËá™Ë∫´Áä∂ÊÄÅ„ÄÇÂà©Áî®MMTokenizerÂ∞ÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂéãÁº©ÊàêtokenË°®Á§∫Ôºå‰ªéËÄåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÔºåÂπ∂ÂÖÅËÆ∏Ê®°ÂûãÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑVideoGPTÊ®°Âûã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöiMoWMÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Â§öÊ®°ÊÄÅÁºñÁ†ÅÂô®ÔºöÂ∞ÜÂΩ©Ëâ≤ÂõæÂÉè„ÄÅÊ∑±Â∫¶ÂõæÂíåÊú∫Âô®‰∫∫ÊâãËáÇÊé©Á†ÅÁºñÁ†ÅÊàêÁâπÂæÅÂêëÈáè„ÄÇ2) MMTokenizerÔºöÂ∞ÜÂ§öÊ®°ÊÄÅÁâπÂæÅÂêëÈáèËΩ¨Êç¢‰∏∫Á¶ªÊï£ÁöÑtokenË°®Á§∫„ÄÇ3) VideoGPTÔºöÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑVideoGPTÊ®°ÂûãÔºåÊ†πÊçÆÂéÜÂè≤tokenÂíåÂä®‰ΩúÈ¢ÑÊµãÊú™Êù•ÁöÑtoken„ÄÇ4) Â§öÊ®°ÊÄÅËß£Á†ÅÂô®ÔºöÂ∞ÜÈ¢ÑÊµãÁöÑtokenËß£Á†ÅÊàêÂΩ©Ëâ≤ÂõæÂÉè„ÄÅÊ∑±Â∫¶ÂõæÂíåÊú∫Âô®‰∫∫ÊâãËáÇÊé©Á†Å„ÄÇÊï¥‰∏™ÊµÅÁ®ã‰ª•Ëá™ÂõûÂΩíÁöÑÊñπÂºèËøõË°åÔºåÂç≥ÊØèÊ¨°È¢ÑÊµãÈÉΩ‰æùËµñ‰∫é‰πãÂâçÁöÑÈ¢ÑÊµãÁªìÊûúÂíåÂä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöiMoWMÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éMMTokenizerÁöÑËÆæËÆ°ÔºåÂÆÉËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂéãÁº©ÊàêÁ¥ßÂáëÁöÑtokenË°®Á§∫„ÄÇ‰∏éÁõ¥Êé•‰ΩøÁî®ÂéüÂßãÂõæÂÉèÂíåÊ∑±Â∫¶Âõæ‰Ωú‰∏∫ËæìÂÖ•Áõ∏ÊØîÔºåMMTokenizerÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÂπ∂ÂÖÅËÆ∏Ê®°ÂûãÂà©Áî®Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÁöÑVideoGPTÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåiMoWMÁöÑÂ§öÊ®°ÊÄÅËæìÂá∫‰πü‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ÁêÜËß£ÁéØÂ¢ÉÂíåËá™Ë∫´Áä∂ÊÄÅ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMMTokenizer‰ΩøÁî®ÂèØÂ≠¶‰π†ÁöÑÁ†ÅÊú¨Â∞ÜÂ§öÊ®°ÊÄÅÁâπÂæÅÂêëÈáèÈáèÂåñ‰∏∫Á¶ªÊï£ÁöÑtoken„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÂ∞ÜÊØè‰∏™Ê®°ÊÄÅÁöÑÁâπÂæÅÂêëÈáèÊò†Â∞ÑÂà∞Á†ÅÊú¨‰∏≠ÊúÄÊé•ËøëÁöÑÁ†ÅÂ≠óÔºåÂπ∂Â∞ÜÁ†ÅÂ≠óÁöÑÁ¥¢Âºï‰Ωú‰∏∫token„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÈáçÊûÑÊçüÂ§±ÂíåÂØπÊäóÊçüÂ§±ÔºåÁî®‰∫é‰øùËØÅÈáçÊûÑÂõæÂÉèÁöÑË¥®ÈáèÂíåtokenË°®Á§∫ÁöÑÂà§Âà´ÊÄß„ÄÇVideoGPT‰ΩøÁî®TransformerÊû∂ÊûÑÔºåÂπ∂ÈááÁî®Âõ†ÊûúÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ª•‰øùËØÅËá™ÂõûÂΩíÈ¢ÑÊµãÁöÑÊ≠£Á°ÆÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåiMoWMÂú®Êú™Êù•È¢ÑÊµã‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∑±Â∫¶ÂõæÈ¢ÑÊµãÊñπÈù¢„ÄÇ‰∏éÂü∫Á∫øÊñπÊ≥ïÁõ∏ÊØîÔºåiMoWMËÉΩÂ§üÁîüÊàêÊõ¥Ê∏ÖÊô∞„ÄÅÊõ¥ÂáÜÁ°ÆÁöÑÊ∑±Â∫¶ÂõæÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂØπ3DÁéØÂ¢ÉÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåiMoWMÂú®Âü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†‰ªªÂä°‰∏≠‰πüË°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÊõ¥Âø´Âú∞Â≠¶‰π†Âà∞ÊúâÊïàÁöÑÊìç‰ΩúÁ≠ñÁï•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

iMoWMÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•‰Ωú‰∏∫Êú∫Âô®‰∫∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ®°ÊãüÂô®ÔºåÂä†ÈÄüÁ≠ñÁï•Â≠¶‰π†ËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåiMoWMËøòÂèØ‰ª•Áî®‰∫éÊ®°‰ªøÂ≠¶‰π†Ôºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÊ®°‰ªø‰∫∫Á±ªÁöÑÊìç‰ΩúË°å‰∏∫„ÄÇÈÄöËøá‰∏çÊñ≠‰∏éÁéØÂ¢É‰∫§‰∫íÔºåiMoWMÂèØ‰ª•‰∏çÊñ≠Â≠¶‰π†ÂíåÊîπËøõÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥ÁÅµÊ¥ªÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Learned world models hold significant potential for robotic manipulation, as they can serve as simulator for real-world interactions. While extensive progress has been made in 2D video-based world models, these approaches often lack geometric and spatial reasoning, which is essential for capturing the physical structure of the 3D world. To address this limitation, we introduce iMoWM, a novel interactive world model designed to generate color images, depth maps, and robot arm masks in an autoregressive manner conditioned on actions. To overcome the high computational cost associated with three-dimensional information, we propose MMTokenizer, which unifies multi-modal inputs into a compact token representation. This design enables iMoWM to leverage large-scale pretrained VideoGPT models while maintaining high efficiency and incorporating richer physical information. With its multi-modal representation, iMoWM not only improves the visual quality of future predictions but also serves as an effective simulator for model-based reinforcement learning (MBRL) and facilitates real-world imitation learning. Extensive experiments demonstrate the superiority of iMoWM across these tasks, showcasing the advantages of multi-modal world modeling for robotic manipulation. Homepage: https://xingyoujun.github.io/imowm/

