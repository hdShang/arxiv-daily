---
layout: default
title: Model-Based Lookahead Reinforcement Learning for in-hand manipulation
---

# Model-Based Lookahead Reinforcement Learning for in-hand manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.08884" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.08884v2</a>
  <a href="https://arxiv.org/pdf/2510.08884.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08884v2" onclick="toggleFavorite(this, '2510.08884v2', 'Model-Based Lookahead Reinforcement Learning for in-hand manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexandre Lopes, Catarina Barata, Plinio Moreno

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-10 (æ›´æ–°: 2025-12-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºæ¨¡å‹çš„é¢„æµ‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡çµå·§æ‰‹æ“ä½œæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `çµå·§æ‰‹æ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `æ··åˆå­¦ä¹ ` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çµå·§æ‰‹æ“ä½œé¢ä¸´å¤æ‚åŠ¨åŠ›å­¦å’Œæ§åˆ¶æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å…¼é¡¾æ•ˆç‡ä¸ç²¾åº¦ã€‚
2. æå‡ºæ··åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆæ— æ¨¡å‹å­¦ä¹ çš„ç­–ç•¥å’ŒåŸºäºæ¨¡å‹çš„è½¨è¿¹é¢„æµ‹ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒç‰©ä½“å±æ€§ä¸‹å‡èƒ½æå‡æ“ä½œæ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬å¢åŠ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çµå·§æ‰‹æ“ä½œæ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„æœºå™¨äººä»»åŠ¡ï¼Œå®ƒç»“åˆäº†å¤æ‚çš„åŠ¨åŠ›å­¦ç³»ç»Ÿä»¥åŠä½¿ç”¨æ‰§è¡Œå™¨æ§åˆ¶å’Œæ“çºµå„ç§ç‰©ä½“çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å°†å…ˆå‰å¼€å‘çš„æ··åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶åº”ç”¨äºçµå·§æ‰‹æ“ä½œä»»åŠ¡ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶èƒ½å¤Ÿæé«˜ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹ç»“åˆäº†æ— æ¨¡å‹å’ŒåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ çš„æ¦‚å¿µï¼Œé€šè¿‡åŠ¨æ€æ¨¡å‹å’Œä»·å€¼å‡½æ•°å¼•å¯¼è®­ç»ƒå¥½çš„ç­–ç•¥è¿›è¡Œè½¨è¿¹è¯„ä¼°ï¼Œç±»ä¼¼äºæ¨¡å‹é¢„æµ‹æ§åˆ¶ã€‚é€šè¿‡ä¸è¢«å¼•å¯¼çš„ç­–ç•¥è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°äº†æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºäº†å……åˆ†æ¢ç´¢è¿™ä¸€ç‚¹ï¼Œä½¿ç”¨å®Œå…¨é©±åŠ¨å’Œæ¬ é©±åŠ¨çš„æ¨¡æ‹Ÿæœºæ¢°æ‰‹è¿›è¡Œäº†å„ç§æµ‹è¯•ï¼Œä»¥æ“çºµä¸åŒçš„ç‰©ä½“æ¥å®Œæˆç»™å®šçš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¹å˜ç‰©ä½“å¯†åº¦å’Œå°ºå¯¸ç­‰å±æ€§ï¼Œä»¥åŠå¼•å¯¼åœ¨ç‰¹å®šç‰©ä½“ä¸Šè®­ç»ƒçš„ç­–ç•¥åœ¨ä¸åŒçš„ç‰©ä½“ä¸Šæ‰§è¡Œç›¸åŒçš„ä»»åŠ¡ï¼Œæµ‹è¯•äº†æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œç»™å®šä¸€ä¸ªå…·æœ‰é«˜å¹³å‡å¥–åŠ±çš„ç­–ç•¥å’Œä¸€ä¸ªå‡†ç¡®çš„åŠ¨æ€æ¨¡å‹ï¼Œè¯¥æ··åˆæ¡†æ¶å¯ä»¥æé«˜å¤§å¤šæ•°æµ‹è¯•ç”¨ä¾‹ä¸­çµå·§æ‰‹æ“ä½œä»»åŠ¡çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨ç‰©ä½“å±æ€§å‘ç”Ÿå˜åŒ–æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œç”±äºè½¨è¿¹è¯„ä¼°çš„å¤æ‚æ€§ï¼Œè¿™ç§æ”¹è¿›æ˜¯ä»¥å¢åŠ è®¡ç®—æˆæœ¬ä¸ºä»£ä»·çš„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šçµå·§æ‰‹æ“ä½œä»»åŠ¡éœ€è¦ç²¾ç¡®æ§åˆ¶æœºæ¢°æ‰‹ä¸ç¯å¢ƒäº¤äº’ï¼Œç°æœ‰æ–¹æ³•å¦‚çº¯æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ ·æœ¬æ•ˆç‡ä½ï¼Œçº¯æ¨¡å‹é¢„æµ‹æ§åˆ¶å¯¹æ¨¡å‹ç²¾åº¦è¦æ±‚é«˜ã€‚è¯¥è®ºæ–‡æ—¨åœ¨æå‡çµå·§æ‰‹æ“ä½œçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½å¯¹ç²¾ç¡®æ¨¡å‹çš„ä¾èµ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥ä¸åŸºäºæ¨¡å‹çš„é¢„æµ‹æ§åˆ¶ç›¸ç»“åˆã€‚åˆ©ç”¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥æä¾›è‰¯å¥½çš„åˆå§‹æ§åˆ¶ï¼Œç„¶åä½¿ç”¨åŠ¨æ€æ¨¡å‹è¿›è¡ŒçŸ­æ—¶é¢„æµ‹ï¼Œå¹¶é€šè¿‡ä»·å€¼å‡½æ•°è¯„ä¼°é¢„æµ‹è½¨è¿¹ï¼Œä»è€Œå¼•å¯¼ç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§æ··åˆæ–¹æ³•æ—¨åœ¨ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œæé«˜æ“ä½œæ€§èƒ½å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼šä½¿ç”¨æŸç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼‰è®­ç»ƒä¸€ä¸ªç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿåˆæ­¥å®Œæˆçµå·§æ‰‹æ“ä½œä»»åŠ¡ã€‚2) åŠ¨æ€æ¨¡å‹ï¼šç”¨äºé¢„æµ‹æœºæ¢°æ‰‹å’Œç‰©ä½“çš„æœªæ¥çŠ¶æ€ã€‚3) ä»·å€¼å‡½æ•°ï¼šç”¨äºè¯„ä¼°é¢„æµ‹è½¨è¿¹çš„ä¼˜åŠ£ã€‚4) è½¨è¿¹è¯„ä¼°æ¨¡å—ï¼šä½¿ç”¨åŠ¨æ€æ¨¡å‹é¢„æµ‹å¤šä¸ªå¯èƒ½çš„è½¨è¿¹ï¼Œå¹¶ä½¿ç”¨ä»·å€¼å‡½æ•°è¯„ä¼°è¿™äº›è½¨è¿¹ã€‚5) ç­–ç•¥å¼•å¯¼æ¨¡å—ï¼šæ ¹æ®è½¨è¿¹è¯„ä¼°çš„ç»“æœï¼Œè°ƒæ•´æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„è¾“å‡ºï¼Œä»è€Œå¼•å¯¼æœºæ¢°æ‰‹æ‰§è¡Œæ›´ä¼˜çš„æ“ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºæ¨¡å‹çš„é¢„æµ‹æ§åˆ¶ç›¸ç»“åˆã€‚ä¸ä¼ ç»Ÿçš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŠ¨æ€æ¨¡å‹è¿›è¡Œè½¨è¿¹é¢„æµ‹ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡å’Œé²æ£’æ€§ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ¨¡å‹çš„é¢„æµ‹æ§åˆ¶ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä½œä¸ºåˆå§‹æ§åˆ¶ï¼Œé™ä½äº†å¯¹ç²¾ç¡®æ¨¡å‹çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºå…³é”®å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚åŠ¨æ€æ¨¡å‹çš„å…·ä½“å½¢å¼ï¼ˆä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œæˆ–ç‰©ç†å¼•æ“ï¼‰æœªçŸ¥ã€‚ä»·å€¼å‡½æ•°çš„å…·ä½“å½¢å¼ä¹ŸæœªçŸ¥ã€‚è½¨è¿¹è¯„ä¼°æ¨¡å—å¦‚ä½•é€‰æ‹©å’Œè¯„ä¼°è½¨è¿¹ï¼Œä»¥åŠç­–ç•¥å¼•å¯¼æ¨¡å—å¦‚ä½•è°ƒæ•´ç­–ç•¥ï¼Œè¿™äº›ç»†èŠ‚å‡æœªçŸ¥ã€‚è¿™äº›ç»†èŠ‚å¯¹äºå¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶è‡³å…³é‡è¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆæ¡†æ¶åœ¨å¤§å¤šæ•°æµ‹è¯•ç”¨ä¾‹ä¸­æé«˜äº†çµå·§æ‰‹æ“ä½œä»»åŠ¡çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨ç‰©ä½“å±æ€§å‘ç”Ÿå˜åŒ–æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼Œè®ºæ–‡ä¸­æ²¡æœ‰ç»™å‡ºå…·ä½“çš„æ•°å€¼ç»“æœã€‚è¯¥æ–¹æ³•åœ¨æ³›åŒ–æ€§æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œè¡¨æ˜å…¶å…·æœ‰ä¸€å®šçš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•å¢åŠ äº†è®¡ç®—æˆæœ¬ï¼Œå…·ä½“å¢åŠ å¤šå°‘æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦çµå·§æ‰‹æ“ä½œçš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šå·¥ä¸šè‡ªåŠ¨åŒ–ä¸­çš„é›¶ä»¶è£…é…ã€åŒ»ç–—æ‰‹æœ¯ä¸­çš„ç²¾ç»†æ“ä½œã€ä»¥åŠå®¶åº­æœåŠ¡æœºå™¨äººä¸­çš„ç‰©å“æ•´ç†ç­‰ã€‚é€šè¿‡æé«˜çµå·§æ‰‹æ“ä½œçš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œå¯ä»¥ä½¿æœºå™¨äººæ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒï¼Œä»è€Œå®ç°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In-Hand Manipulation, as many other dexterous tasks, remains a difficult challenge in robotics by combining complex dynamic systems with the capability to control and manoeuvre various objects using its actuators. This work presents the application of a previously developed hybrid Reinforcement Learning (RL) Framework to In-Hand Manipulation task, verifying that it is capable of improving the performance of the task. The model combines concepts of both Model-Free and Model-Based Reinforcement Learning, by guiding a trained policy with the help of a dynamic model and value-function through trajectory evaluation, as done in Model Predictive Control. This work evaluates the performance of the model by comparing it with the policy that will be guided. To fully explore this, various tests are performed using both fully-actuated and under-actuated simulated robotic hands to manipulate different objects for a given task. The performance of the model will also be tested for generalization tests, by changing the properties of the objects in which both the policy and dynamic model were trained, such as density and size, and additionally by guiding a trained policy in a certain object to perform the same task in a different one. The results of this work show that, given a policy with high average reward and an accurate dynamic model, the hybrid framework improves the performance of in-hand manipulation tasks for most test cases, even when the object properties are changed. However, this improvement comes at the expense of increasing the computational cost, due to the complexity of trajectory evaluation.

