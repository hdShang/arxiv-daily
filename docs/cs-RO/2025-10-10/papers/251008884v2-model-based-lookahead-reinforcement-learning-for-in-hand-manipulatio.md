---
layout: default
title: Model-Based Lookahead Reinforcement Learning for in-hand manipulation
---

# Model-Based Lookahead Reinforcement Learning for in-hand manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.08884" target="_blank" class="toolbar-btn">arXiv: 2510.08884v2</a>
    <a href="https://arxiv.org/pdf/2510.08884.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.08884v2" 
            onclick="toggleFavorite(this, '2510.08884v2', 'Model-Based Lookahead Reinforcement Learning for in-hand manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Alexandre Lopes, Catarina Barata, Plinio Moreno

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-10 (Êõ¥Êñ∞: 2025-12-11)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÊ®°ÂûãÁöÑÈ¢ÑÊµãÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÊèêÂçáÁÅµÂ∑ßÊâãÊìç‰ΩúÊÄßËÉΩ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÁÅµÂ∑ßÊâãÊìç‰Ωú` `Âº∫ÂåñÂ≠¶‰π†` `Ê®°ÂûãÈ¢ÑÊµãÊéßÂà∂` `Ê∑∑ÂêàÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÁÅµÂ∑ßÊâãÊìç‰ΩúÈù¢‰∏¥Â§çÊùÇÂä®ÂäõÂ≠¶ÂíåÊéßÂà∂ÊåëÊàòÔºåÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÂÖºÈ°æÊïàÁéá‰∏éÁ≤æÂ∫¶„ÄÇ
2. ÊèêÂá∫Ê∑∑ÂêàÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêàÊó†Ê®°ÂûãÂ≠¶‰π†ÁöÑÁ≠ñÁï•ÂíåÂü∫‰∫éÊ®°ÂûãÁöÑËΩ®ËøπÈ¢ÑÊµã„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®‰∏çÂêåÁâ©‰ΩìÂ±ûÊÄß‰∏ãÂùáËÉΩÊèêÂçáÊìç‰ΩúÊÄßËÉΩÔºå‰ΩÜËÆ°ÁÆóÊàêÊú¨Â¢ûÂä†„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÁÅµÂ∑ßÊâãÊìç‰ΩúÊòØ‰∏ÄÈ°πÊûÅÂÖ∑ÊåëÊàòÊÄßÁöÑÊú∫Âô®‰∫∫‰ªªÂä°ÔºåÂÆÉÁªìÂêà‰∫ÜÂ§çÊùÇÁöÑÂä®ÂäõÂ≠¶Á≥ªÁªü‰ª•Âèä‰ΩøÁî®ÊâßË°åÂô®ÊéßÂà∂ÂíåÊìçÁ∫µÂêÑÁßçÁâ©‰ΩìÁöÑËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂Â∞ÜÂÖàÂâçÂºÄÂèëÁöÑÊ∑∑ÂêàÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊ°ÜÊû∂Â∫îÁî®‰∫éÁÅµÂ∑ßÊâãÊìç‰Ωú‰ªªÂä°ÔºåÈ™åËØÅ‰∫ÜËØ•Ê°ÜÊû∂ËÉΩÂ§üÊèêÈ´ò‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÁªìÂêà‰∫ÜÊó†Ê®°ÂûãÂíåÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÁöÑÊ¶ÇÂøµÔºåÈÄöËøáÂä®ÊÄÅÊ®°ÂûãÂíå‰ª∑ÂÄºÂáΩÊï∞ÂºïÂØºËÆ≠ÁªÉÂ•ΩÁöÑÁ≠ñÁï•ËøõË°åËΩ®ËøπËØÑ‰º∞ÔºåÁ±ª‰ºº‰∫éÊ®°ÂûãÈ¢ÑÊµãÊéßÂà∂„ÄÇÈÄöËøá‰∏éË¢´ÂºïÂØºÁöÑÁ≠ñÁï•ËøõË°åÊØîËæÉÔºåËØÑ‰º∞‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜÂÖÖÂàÜÊé¢Á¥¢Ëøô‰∏ÄÁÇπÔºå‰ΩøÁî®ÂÆåÂÖ®È©±Âä®ÂíåÊ¨†È©±Âä®ÁöÑÊ®°ÊãüÊú∫Ê¢∞ÊâãËøõË°å‰∫ÜÂêÑÁßçÊµãËØïÔºå‰ª•ÊìçÁ∫µ‰∏çÂêåÁöÑÁâ©‰ΩìÊù•ÂÆåÊàêÁªôÂÆöÁöÑ‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊîπÂèòÁâ©‰ΩìÂØÜÂ∫¶ÂíåÂ∞∫ÂØ∏Á≠âÂ±ûÊÄßÔºå‰ª•ÂèäÂºïÂØºÂú®ÁâπÂÆöÁâ©‰Ωì‰∏äËÆ≠ÁªÉÁöÑÁ≠ñÁï•Âú®‰∏çÂêåÁöÑÁâ©‰Ωì‰∏äÊâßË°åÁõ∏ÂêåÁöÑ‰ªªÂä°ÔºåÊµãËØï‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñÊÄßËÉΩ„ÄÇÁªìÊûúË°®ÊòéÔºåÁªôÂÆö‰∏Ä‰∏™ÂÖ∑ÊúâÈ´òÂπ≥ÂùáÂ•ñÂä±ÁöÑÁ≠ñÁï•Âíå‰∏Ä‰∏™ÂáÜÁ°ÆÁöÑÂä®ÊÄÅÊ®°ÂûãÔºåËØ•Ê∑∑ÂêàÊ°ÜÊû∂ÂèØ‰ª•ÊèêÈ´òÂ§ßÂ§öÊï∞ÊµãËØïÁî®‰æã‰∏≠ÁÅµÂ∑ßÊâãÊìç‰Ωú‰ªªÂä°ÁöÑÊÄßËÉΩÔºåÂç≥‰ΩøÂú®Áâ©‰ΩìÂ±ûÊÄßÂèëÁîüÂèòÂåñÊó∂‰πüÊòØÂ¶ÇÊ≠§„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éËΩ®ËøπËØÑ‰º∞ÁöÑÂ§çÊùÇÊÄßÔºåËøôÁßçÊîπËøõÊòØ‰ª•Â¢ûÂä†ËÆ°ÁÆóÊàêÊú¨‰∏∫‰ª£‰ª∑ÁöÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁÅµÂ∑ßÊâãÊìç‰Ωú‰ªªÂä°ÈúÄË¶ÅÁ≤æÁ°ÆÊéßÂà∂Êú∫Ê¢∞Êâã‰∏éÁéØÂ¢É‰∫§‰∫íÔºåÁé∞ÊúâÊñπÊ≥ïÂ¶ÇÁ∫ØÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†Ê†∑Êú¨ÊïàÁéá‰ΩéÔºåÁ∫ØÊ®°ÂûãÈ¢ÑÊµãÊéßÂà∂ÂØπÊ®°ÂûãÁ≤æÂ∫¶Ë¶ÅÊ±ÇÈ´ò„ÄÇËØ•ËÆ∫ÊñáÊó®Âú®ÊèêÂçáÁÅµÂ∑ßÊâãÊìç‰ΩúÁöÑÊÄßËÉΩÂíåÊ≥õÂåñËÉΩÂäõÔºåÂêåÊó∂Èôç‰ΩéÂØπÁ≤æÁ°ÆÊ®°ÂûãÁöÑ‰æùËµñ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËØ•ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÂæóÂà∞ÁöÑÁ≠ñÁï•‰∏éÂü∫‰∫éÊ®°ÂûãÁöÑÈ¢ÑÊµãÊéßÂà∂Áõ∏ÁªìÂêà„ÄÇÂà©Áî®Êó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Êèê‰æõËâØÂ•ΩÁöÑÂàùÂßãÊéßÂà∂ÔºåÁÑ∂Âêé‰ΩøÁî®Âä®ÊÄÅÊ®°ÂûãËøõË°åÁü≠Êó∂È¢ÑÊµãÔºåÂπ∂ÈÄöËøá‰ª∑ÂÄºÂáΩÊï∞ËØÑ‰º∞È¢ÑÊµãËΩ®ËøπÔºå‰ªéËÄåÂºïÂØºÁ≠ñÁï•ËøõË°å‰ºòÂåñ„ÄÇËøôÁßçÊ∑∑ÂêàÊñπÊ≥ïÊó®Âú®ÁªìÂêà‰∏§ËÄÖÁöÑ‰ºòÁÇπÔºåÊèêÈ´òÊìç‰ΩúÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÂê´‰ª•‰∏ã‰∏ªË¶ÅÊ®°ÂùóÔºö1) Êó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•Ôºö‰ΩøÁî®ÊüêÁßçÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºàÂÖ∑‰ΩìÁÆóÊ≥ïÊú™Áü•ÔºâËÆ≠ÁªÉ‰∏Ä‰∏™Á≠ñÁï•ÔºåËØ•Á≠ñÁï•ËÉΩÂ§üÂàùÊ≠•ÂÆåÊàêÁÅµÂ∑ßÊâãÊìç‰Ωú‰ªªÂä°„ÄÇ2) Âä®ÊÄÅÊ®°ÂûãÔºöÁî®‰∫éÈ¢ÑÊµãÊú∫Ê¢∞ÊâãÂíåÁâ©‰ΩìÁöÑÊú™Êù•Áä∂ÊÄÅ„ÄÇ3) ‰ª∑ÂÄºÂáΩÊï∞ÔºöÁî®‰∫éËØÑ‰º∞È¢ÑÊµãËΩ®ËøπÁöÑ‰ºòÂä£„ÄÇ4) ËΩ®ËøπËØÑ‰º∞Ê®°ÂùóÔºö‰ΩøÁî®Âä®ÊÄÅÊ®°ÂûãÈ¢ÑÊµãÂ§ö‰∏™ÂèØËÉΩÁöÑËΩ®ËøπÔºåÂπ∂‰ΩøÁî®‰ª∑ÂÄºÂáΩÊï∞ËØÑ‰º∞Ëøô‰∫õËΩ®Ëøπ„ÄÇ5) Á≠ñÁï•ÂºïÂØºÊ®°ÂùóÔºöÊ†πÊçÆËΩ®ËøπËØÑ‰º∞ÁöÑÁªìÊûúÔºåË∞ÉÊï¥Êó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÁöÑËæìÂá∫Ôºå‰ªéËÄåÂºïÂØºÊú∫Ê¢∞ÊâãÊâßË°åÊõ¥‰ºòÁöÑÊìç‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞ÜÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÂíåÂü∫‰∫éÊ®°ÂûãÁöÑÈ¢ÑÊµãÊéßÂà∂Áõ∏ÁªìÂêà„ÄÇ‰∏é‰º†ÁªüÁöÑÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†Áõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂà©Áî®Âä®ÊÄÅÊ®°ÂûãËøõË°åËΩ®ËøπÈ¢ÑÊµãÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÊ®°ÂûãÁöÑÈ¢ÑÊµãÊéßÂà∂Áõ∏ÊØîÔºåËØ•ÊñπÊ≥ï‰ΩøÁî®Êó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•‰Ωú‰∏∫ÂàùÂßãÊéßÂà∂ÔºåÈôç‰Ωé‰∫ÜÂØπÁ≤æÁ°ÆÊ®°ÂûãÁöÑ‰æùËµñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠Êú™ÊòéÁ°ÆÁªôÂá∫ÂÖ≥ÈîÆÂèÇÊï∞ËÆæÁΩÆ„ÄÅÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÁ≠âÊäÄÊúØÁªÜËäÇ„ÄÇÂä®ÊÄÅÊ®°ÂûãÁöÑÂÖ∑‰ΩìÂΩ¢ÂºèÔºà‰æãÂ¶ÇÔºåÁ•ûÁªèÁΩëÁªúÊàñÁâ©ÁêÜÂºïÊìéÔºâÊú™Áü•„ÄÇ‰ª∑ÂÄºÂáΩÊï∞ÁöÑÂÖ∑‰ΩìÂΩ¢Âºè‰πüÊú™Áü•„ÄÇËΩ®ËøπËØÑ‰º∞Ê®°ÂùóÂ¶Ç‰ΩïÈÄâÊã©ÂíåËØÑ‰º∞ËΩ®ËøπÔºå‰ª•ÂèäÁ≠ñÁï•ÂºïÂØºÊ®°ÂùóÂ¶Ç‰ΩïË∞ÉÊï¥Á≠ñÁï•ÔºåËøô‰∫õÁªÜËäÇÂùáÊú™Áü•„ÄÇËøô‰∫õÁªÜËäÇÂØπ‰∫éÂ§çÁé∞ÂíåËøõ‰∏ÄÊ≠•Á†îÁ©∂Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê∑∑ÂêàÊ°ÜÊû∂Âú®Â§ßÂ§öÊï∞ÊµãËØïÁî®‰æã‰∏≠ÊèêÈ´ò‰∫ÜÁÅµÂ∑ßÊâãÊìç‰Ωú‰ªªÂä°ÁöÑÊÄßËÉΩÔºåÂç≥‰ΩøÂú®Áâ©‰ΩìÂ±ûÊÄßÂèëÁîüÂèòÂåñÊó∂‰πüÊòØÂ¶ÇÊ≠§„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶Êú™Áü•ÔºåËÆ∫Êñá‰∏≠Ê≤°ÊúâÁªôÂá∫ÂÖ∑‰ΩìÁöÑÊï∞ÂÄºÁªìÊûú„ÄÇËØ•ÊñπÊ≥ïÂú®Ê≥õÂåñÊÄßÊµãËØï‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåË°®ÊòéÂÖ∂ÂÖ∑Êúâ‰∏ÄÂÆöÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÁÑ∂ËÄåÔºåËØ•ÊñπÊ≥ïÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÂÖ∑‰ΩìÂ¢ûÂä†Â§öÂ∞ëÊú™Áü•„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÁÅµÂ∑ßÊâãÊìç‰ΩúÁöÑÊú∫Âô®‰∫∫‰ªªÂä°Ôºå‰æãÂ¶ÇÔºöÂ∑•‰∏öËá™Âä®Âåñ‰∏≠ÁöÑÈõ∂‰ª∂Ë£ÖÈÖç„ÄÅÂåªÁñóÊâãÊúØ‰∏≠ÁöÑÁ≤æÁªÜÊìç‰Ωú„ÄÅ‰ª•ÂèäÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫‰∏≠ÁöÑÁâ©ÂìÅÊï¥ÁêÜÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÁÅµÂ∑ßÊâãÊìç‰ΩúÁöÑÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄßÔºåÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÂíåÂä®ÊÄÅÁöÑÁéØÂ¢ÉÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÂπøÊ≥õÁöÑÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In-Hand Manipulation, as many other dexterous tasks, remains a difficult challenge in robotics by combining complex dynamic systems with the capability to control and manoeuvre various objects using its actuators. This work presents the application of a previously developed hybrid Reinforcement Learning (RL) Framework to In-Hand Manipulation task, verifying that it is capable of improving the performance of the task. The model combines concepts of both Model-Free and Model-Based Reinforcement Learning, by guiding a trained policy with the help of a dynamic model and value-function through trajectory evaluation, as done in Model Predictive Control. This work evaluates the performance of the model by comparing it with the policy that will be guided. To fully explore this, various tests are performed using both fully-actuated and under-actuated simulated robotic hands to manipulate different objects for a given task. The performance of the model will also be tested for generalization tests, by changing the properties of the objects in which both the policy and dynamic model were trained, such as density and size, and additionally by guiding a trained policy in a certain object to perform the same task in a different one. The results of this work show that, given a policy with high average reward and an accurate dynamic model, the hybrid framework improves the performance of in-hand manipulation tasks for most test cases, even when the object properties are changed. However, this improvement comes at the expense of increasing the computational cost, due to the complexity of trajectory evaluation.

