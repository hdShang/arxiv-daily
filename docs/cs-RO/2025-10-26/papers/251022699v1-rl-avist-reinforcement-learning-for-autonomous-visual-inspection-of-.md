---
layout: default
title: RL-AVIST: Reinforcement Learning for Autonomous Visual Inspection of Space Targets
---

# RL-AVIST: Reinforcement Learning for Autonomous Visual Inspection of Space Targets

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.22699" target="_blank" class="toolbar-btn">arXiv: 2510.22699v1</a>
    <a href="https://arxiv.org/pdf/2510.22699.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22699v1" 
            onclick="toggleFavorite(this, '2510.22699v1', 'RL-AVIST: Reinforcement Learning for Autonomous Visual Inspection of Space Targets')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Matteo El-Hariry, Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-26

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RL-AVISTÊ°ÜÊû∂ÔºåÁî®‰∫éËà™Â§©Âô®ÁõÆÊ†áËá™‰∏ªËßÜËßâÊ£ÄÊµãÁöÑÂº∫ÂåñÂ≠¶‰π†ÊéßÂà∂„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Ëá™‰∏ªËßÜËßâÊ£ÄÊµã` `Ëà™Â§©Âô®ÊéßÂà∂` `Âú®ËΩ®ÊúçÂä°` `DreamerV3`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüËà™Â§©Âô®ÊéßÂà∂Á≥ªÁªüÂú®Ê®°Âûã‰∏çÁ°ÆÂÆöÂíå‰ªªÂä°Âä®ÊÄÅÂèòÂåñÊó∂ÈÄÇÂ∫îÊÄß‰∏çË∂≥ÔºåÈöæ‰ª•Êª°Ë∂≥Êó•ÁõäÂ¢ûÈïøÁöÑÂú®ËΩ®ÊúçÂä°ÈúÄÊ±Ç„ÄÇ
2. RL-AVISTÊ°ÜÊû∂Âà©Áî®Âº∫ÂåñÂ≠¶‰π†ÔºåÈÄöËøáÂ≠¶‰π†Â§çÊùÇÊú∫Âä®Á≠ñÁï•ÔºåÂÆûÁé∞Ëà™Â§©Âô®ÂØπÁ©∫Èó¥ÁõÆÊ†áÁöÑËá™‰∏ªËßÜËßâÊ£ÄÊµã„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†Âú®ËΩ®Ëøπ‰øùÁúüÂ∫¶ÂíåÊ†∑Êú¨ÊïàÁéáÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰∏∫Á©∫Èó¥‰ªªÂä°ÊéßÂà∂Êèê‰æõ‰∫ÜÊñ∞ÊÄùË∑Ø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éËà™Â§©Âô®ÁõÆÊ†áËá™‰∏ªËßÜËßâÊ£ÄÊµãÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂RL-AVIST„ÄÇÈíàÂØπÊó•ÁõäÂ¢ûÈïøÁöÑÂú®ËΩ®ÊúçÂä°ÈúÄÊ±ÇÔºåÂ¶ÇÊ£ÄÊµã„ÄÅÁª¥Êä§ÂíåÁä∂ÊÄÅÊÑüÁü•ÔºåÈúÄË¶ÅÊô∫ËÉΩËà™Â§©Âô®ËÉΩÂ§üÂõ¥ÁªïÂ§ßÂûãËΩ®ÈÅìÁõÆÊ†áËøõË°åÂ§çÊùÇÊú∫Âä®„ÄÇ‰º†ÁªüÊéßÂà∂Á≥ªÁªüÂú®ÈÄÇÂ∫îÊÄßÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê®°Âûã‰∏çÁ°ÆÂÆöÊÄß„ÄÅÂ§öËà™Â§©Âô®ÈÖçÁΩÆÊàñÂä®ÊÄÅÊºîÂèòÁöÑ ‡§Æ‡§ø‡§∂‡§® ÁéØÂ¢É‰∏ã„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Space Robotics Bench (SRB)Ê®°ÊãüÈ´ò‰øùÁúü6Ëá™Áî±Â∫¶Ëà™Â§©Âô®Âä®ÂäõÂ≠¶ÔºåÂπ∂‰ΩøÁî®DreamerV3Ôºà‰∏ÄÁßçÂÖàËøõÁöÑÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºâ‰ª•ÂèäPPOÂíåTD3Ôºà‰Ωú‰∏∫Êó†Ê®°ÂûãÂü∫Á∫øÔºâËÆ≠ÁªÉÊô∫ËÉΩ‰Ωì„ÄÇÁ†îÁ©∂ÈáçÁÇπÊòØÂõ¥ÁªïÊúàÁêÉÈó®Êà∑Á≠âÁõÆÊ†áËøõË°å3DËøëË∑ùÁ¶ªÊú∫Âä®‰ªªÂä°„ÄÇËØÑ‰º∞‰∫Ü‰∏§Áßç‰∫íË°•Ê®°Âºè‰∏ãÁöÑ‰ªªÂä°ÊÄßËÉΩÔºöÂú®ÈöèÊú∫ÈÄüÂ∫¶ÂêëÈáè‰∏äËÆ≠ÁªÉÁöÑÂπø‰πâÊô∫ËÉΩ‰ΩìÔºå‰ª•ÂèäËÆ≠ÁªÉÁî®‰∫éÈÅµÂæ™Ê®°ÊãüÂ∑≤Áü•Ê£ÄÊµãËΩ®ÈÅìÁöÑÂõ∫ÂÆöËΩ®ËøπÁöÑ‰∏ìÁî®Êô∫ËÉΩ‰Ωì„ÄÇÊ≠§Â§ñÔºåËØÑ‰º∞‰∫ÜÁ≠ñÁï•Âú®Â§öÁßçËà™Â§©Âô®ÂΩ¢ÊÄÅÂíå‰ªªÂä°È¢ÜÂüü‰∏≠ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†Âú®ËΩ®Ëøπ‰øùÁúüÂ∫¶ÂíåÊ†∑Êú¨ÊïàÁéáÊñπÈù¢ÂÖ∑ÊúâËâØÂ•ΩÁöÑËÉΩÂäõÔºå‰∏∫Êú™Êù•Á©∫Èó¥Ë°åÂä®ÁöÑÂèØÊâ©Â±ï„ÄÅÂèØÂÜçËÆ≠ÁªÉÁöÑÊéßÂà∂Ëß£ÂÜ≥ÊñπÊ°àÈì∫Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Ëà™Â§©Âô®Âú®Â§çÊùÇÁ©∫Èó¥ÁéØÂ¢É‰∏≠ÔºåÂ¶Ç‰ΩïËá™‰∏ªÂú∞ËøõË°åËßÜËßâÊ£ÄÊµãÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊéßÂà∂ÊñπÊ≥ïÂú®Èù¢ÂØπÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄß„ÄÅÂ§öËà™Â§©Âô®ÂçèÂêå‰ª•ÂèäÂä®ÊÄÅÂèòÂåñÁöÑ‰ªªÂä°ÁéØÂ¢ÉÊó∂ÔºåÈÄÇÂ∫îÊÄßËæÉÂ∑ÆÔºåÈöæ‰ª•‰øùËØÅÊ£ÄÊµã‰ªªÂä°ÁöÑÁ≤æÂ∫¶ÂíåÊïàÁéá„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÔºåÁâπÂà´ÊòØÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåËÆ©Ëà™Â§©Âô®Êô∫ËÉΩ‰ΩìÈÄöËøá‰∏éÁéØÂ¢ÉÁöÑ‰∫§‰∫íÂ≠¶‰π†ÊúÄ‰ºòÁöÑÊéßÂà∂Á≠ñÁï•„ÄÇÈÄöËøáÂ≠¶‰π†ÔºåÊô∫ËÉΩ‰ΩìËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑËà™Â§©Âô®ÂΩ¢ÊÄÅÂíå‰ªªÂä°ÁéØÂ¢ÉÔºåÂÆûÁé∞Ëá™‰∏ªÁöÑËßÜËßâÊ£ÄÊµã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRL-AVISTÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÂê´‰ª•‰∏ãÂá†‰∏™ÈÉ®ÂàÜÔºö1) Âü∫‰∫éSpace Robotics Bench (SRB) ÁöÑÈ´ò‰øùÁúü6Ëá™Áî±Â∫¶Ëà™Â§©Âô®Âä®ÂäõÂ≠¶‰ªøÁúüÁéØÂ¢ÉÔºõ2) Âü∫‰∫éDreamerV3ÁöÑÂº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÔºå‰ª•ÂèäPPOÂíåTD3‰Ωú‰∏∫Âü∫Á∫øÁÆóÊ≥ïÔºõ3) Â•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºåÁî®‰∫éÂºïÂØºÊô∫ËÉΩ‰ΩìÂ≠¶‰π†ÊúüÊúõÁöÑÊ£ÄÊµãË°å‰∏∫Ôºõ4) Á≠ñÁï•ËØÑ‰º∞ÂíåÊ≥õÂåñËÉΩÂäõÊµãËØïÔºåÈ™åËØÅÊô∫ËÉΩ‰ΩìÂú®‰∏çÂêåÁéØÂ¢É‰∏ãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïDreamerV3Â∫îÁî®‰∫éËà™Â§©Âô®Ëá™‰∏ªËßÜËßâÊ£ÄÊµã‰ªªÂä°„ÄÇ‰∏é‰º†ÁªüÁöÑÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁõ∏ÊØîÔºåDreamerV3ËÉΩÂ§üÂ≠¶‰π†ÁéØÂ¢ÉÁöÑÊ®°ÂûãÔºå‰ªéËÄåÊèêÈ´òÊ†∑Êú¨ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÁ†îÁ©∂‰∫ÜÁ≠ñÁï•Âú®‰∏çÂêåËà™Â§©Âô®ÂΩ¢ÊÄÅÂíå‰ªªÂä°ÁéØÂ¢É‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰ΩøÁî®‰∫ÜDreamerV3ÁÆóÊ≥ïÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÈöêÂèòÈáèÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÈúÄË¶ÅÁªºÂêàËÄÉËôëËà™Â§©Âô®ÁöÑËΩ®ËøπÁ≤æÂ∫¶„ÄÅ‰∏éÁõÆÊ†áÁöÑË∑ùÁ¶ª„ÄÅ‰ª•ÂèäËÉΩÈáèÊ∂àËÄóÁ≠âÂõ†Á¥†„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÂØπÊô∫ËÉΩ‰ΩìÁöÑÁΩëÁªúÁªìÊûÑËøõË°å‰∫Ü‰ºòÂåñÔºå‰ª•ÊèêÈ´òÂ≠¶‰π†ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïDreamerV3Âú®Ëà™Â§©Âô®Ëá™‰∏ªËßÜËßâÊ£ÄÊµã‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂú®ËΩ®Ëøπ‰øùÁúüÂ∫¶ÂíåÊ†∑Êú¨ÊïàÁéáÊñπÈù¢‰ºò‰∫éÊó†Ê®°ÂûãÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïPPOÂíåTD3„ÄÇÈÄöËøáÂú®ÈöèÊú∫ÈÄüÂ∫¶ÂêëÈáèÂíåÂõ∫ÂÆöËΩ®Ëøπ‰∏äËøõË°åËÆ≠ÁªÉÔºåÈ™åËØÅ‰∫ÜÊô∫ËÉΩ‰ΩìÂú®‰∏çÂêå‰ªªÂä°ÁéØÂ¢É‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂ§öÁßçÂú®ËΩ®ÊúçÂä°‰ªªÂä°ÔºåÂ¶ÇÁ©∫Èó¥Á´ôÂíåÂç´ÊòüÁöÑËá™‰∏ªÊ£ÄÊµã„ÄÅÁª¥Êä§ÂíåÁª¥‰øÆÔºå‰ª•ÂèäÁ©∫Èó¥Á¢éÁâáÊ∏ÖÈô§Á≠â„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÊô∫ËÉΩ‰ΩìËÉΩÂ§üËá™‰∏ªËßÑÂàíËà™Â§©Âô®ÁöÑËøêÂä®ËΩ®ËøπÔºåÊèêÈ´ò‰ªªÂä°ÊïàÁéáÂíåÂÆâÂÖ®ÊÄßÔºåÈôç‰ΩéÂØπÂú∞Èù¢ÊéßÂà∂ÁöÑ‰æùËµñÔºå‰∏∫Êú™Êù•ÁöÑÊ∑±Á©∫Êé¢ÊµãÂíåÁ©∫Èó¥ËµÑÊ∫êÂà©Áî®Êèê‰æõÊäÄÊúØÊîØÊåÅ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The growing need for autonomous on-orbit services such as inspection, maintenance, and situational awareness calls for intelligent spacecraft capable of complex maneuvers around large orbital targets. Traditional control systems often fall short in adaptability, especially under model uncertainties, multi-spacecraft configurations, or dynamically evolving mission contexts. This paper introduces RL-AVIST, a Reinforcement Learning framework for Autonomous Visual Inspection of Space Targets. Leveraging the Space Robotics Bench (SRB), we simulate high-fidelity 6-DOF spacecraft dynamics and train agents using DreamerV3, a state-of-the-art model-based RL algorithm, with PPO and TD3 as model-free baselines. Our investigation focuses on 3D proximity maneuvering tasks around targets such as the Lunar Gateway and other space assets. We evaluate task performance under two complementary regimes: generalized agents trained on randomized velocity vectors, and specialized agents trained to follow fixed trajectories emulating known inspection orbits. Furthermore, we assess the robustness and generalization of policies across multiple spacecraft morphologies and mission domains. Results demonstrate that model-based RL offers promising capabilities in trajectory fidelity, and sample efficiency, paving the way for scalable, retrainable control solutions for future space operations

