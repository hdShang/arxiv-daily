---
layout: default
title: Real2USD: Scene Representations in Universal Scene Description Language
---

# Real2USD: Scene Representations in Universal Scene Description Language

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10778" target="_blank" class="toolbar-btn">arXiv: 2510.10778v1</a>
    <a href="https://arxiv.org/pdf/2510.10778.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10778v1" 
            onclick="toggleFavorite(this, '2510.10778v1', 'Real2USD: Scene Representations in Universal Scene Description Language')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Christopher D. Hsu, Pratik Chaudhari

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-12

**Â§áÊ≥®**: 8 pages, 10 figures, 1 table

**üîó ‰ª£Á†Å/È°πÁõÆ**: [GITHUB](https://github.com/grasp-lyrl/Real2USD)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Real2USDÁ≥ªÁªüÔºåÂà©Áî®ÈÄöÁî®Âú∫ÊôØÊèèËø∞ËØ≠Ë®ÄUSDËµãËÉΩLLMÊú∫Âô®‰∫∫Âú∫ÊôØÁêÜËß£‰∏éËßÑÂàí**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈÄöÁî®Âú∫ÊôØÊèèËø∞` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êú∫Âô®‰∫∫` `Âú∫ÊôØÁêÜËß£` `‰ªªÂä°ËßÑÂàí` `ÁéØÂ¢ÉË°®Á§∫` `USD` `Real2USD`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫ÁéØÂ¢ÉË°®Á§∫ÊñπÊ≥ïÁº∫‰πèÈÄöÁî®ÊÄßÔºåÈöæ‰ª•ÊúâÊïàÊîØÊåÅLLMËøõË°åÂ§çÊùÇÊé®ÁêÜÂíåËßÑÂàí„ÄÇ
2. ÊèêÂá∫Real2USDÁ≥ªÁªüÔºåÂ∞ÜÁúüÂÆûÁéØÂ¢ÉËΩ¨Êç¢‰∏∫USDÊ†ºÂºèÔºåÂà©Áî®ÂÖ∂‰∏∞ÂØåÁöÑËØ≠‰πâÂíåÂá†‰Ωï‰ø°ÊÅØ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•Á≥ªÁªüËÉΩÂ§üÂà©Áî®LLMËøõË°åÂú∫ÊôØÁêÜËß£„ÄÅÂ§çÊùÇÊé®ÁêÜÂíåËßÑÂàíÔºåÂπ∂Âú®Ê®°ÊãüÁéØÂ¢É‰∏≠È™åËØÅ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLM)ËÉΩÂ§üÂ∏ÆÂä©Êú∫Âô®‰∫∫Êé®ÁêÜÊäΩË±°ÁöÑ‰ªªÂä°ËßÑËåÉ„ÄÇËøôÈúÄË¶ÅÁî®Âü∫‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂÖàÈ™åÁü•ËØÜÊù•Â¢ûÂº∫Êú∫Âô®‰∫∫ÊâÄ‰ΩøÁî®ÁöÑÁªèÂÖ∏ÁéØÂ¢ÉË°®Á§∫„ÄÇÁõÆÂâçÂ∑≤Êúâ‰∏Ä‰∫õÊñπÊ≥ïÔºå‰ΩÜÂÆÉ‰ª¨ÊòØ‰∏∫ÁâπÂÆö‰ªªÂä°ÂÆöÂà∂ÁöÑÔºå‰æãÂ¶ÇÁî®‰∫éÂØºËà™ÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÁî®‰∫éÂú∞ÂõæÊûÑÂª∫ÁöÑËØ≠Ë®ÄÂºïÂØºÁ•ûÁªèËæêÂ∞ÑÂú∫Á≠â„ÄÇÊú¨ÊñáËÆ§‰∏∫ÔºåÈÄöÁî®Âú∫ÊôØÊèèËø∞(USD)ËØ≠Ë®ÄÊòØÂü∫‰∫éLLMÁöÑÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁéØÂ¢ÉÁöÑÂá†‰Ωï„ÄÅÂÖâÂ∫¶ÂíåËØ≠‰πâ‰ø°ÊÅØÁöÑÊúâÊïà‰∏îÈÄöÁî®ÁöÑË°®Á§∫„ÄÇÊàë‰ª¨ÁöÑËÆ∫ÁÇπÂæàÁÆÄÂçïÔºöUSDÊòØ‰∏ÄÁßçÂü∫‰∫éXMLÁöÑÂú∫ÊôØÂõæÔºåLLMÂíå‰∫∫Á±ªÈÉΩÂèØ‰ª•ËØªÂèñÔºåÂπ∂‰∏îË∂≥Â§ü‰∏∞ÂØå‰ª•ÊîØÊåÅÂá†‰πé‰ªª‰Ωï‰ªªÂä°‚Äî‚ÄîÁöÆÂÖãÊñØÂºÄÂèëËøôÁßçËØ≠Ë®ÄÊù•Â≠òÂÇ®ËµÑ‰∫ß„ÄÅÂú∫ÊôØÁîöËá≥ÁîµÂΩ±„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰∏Ä‰∏™‚ÄúReal to USD‚ÄùÁ≥ªÁªüÔºåËØ•Á≥ªÁªü‰ΩøÁî®ÈÖçÂ§áÊøÄÂÖâÈõ∑ËææÂíåRGBÁõ∏Êú∫ÁöÑÂÆáÊ†ëGo2ÂõõË∂≥Êú∫Âô®‰∫∫ÔºåËØ•Á≥ªÁªü(i)ÊûÑÂª∫‰∫ÜÂÖ∑ÊúâÂêÑÁßçÁâ©‰ΩìÂíåÂÖ∑ÊúâÂ§ßÈáèÁéªÁíÉÁöÑÊåëÊàòÊÄßËÆæÁΩÆÁöÑÂÆ§ÂÜÖÁéØÂ¢ÉÁöÑÊòæÂºèUSDË°®Á§∫Ôºå‰ª•Âèä(ii)‰ΩøÁî®Ë∞∑Ê≠åÁöÑGeminiËß£ÊûêUSD‰ª•ÊºîÁ§∫Âú∫ÊôØÁêÜËß£„ÄÅÂ§çÊùÇÊé®ÁêÜÂíåËßÑÂàí„ÄÇÊàë‰ª¨ËøòÂú®‰ΩøÁî®NvidiaÁöÑIssac SimÁöÑÊ®°Êãü‰ªìÂ∫ìÂíåÂåªÈô¢ÁéØÂ¢É‰∏≠Á†îÁ©∂‰∫ÜËØ•Á≥ªÁªüÁöÑ‰∏çÂêåÊñπÈù¢„ÄÇ‰ª£Á†ÅÂèØÂú®https://github.com/grasp-lyrl/Real2USD Ëé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊú∫Âô®‰∫∫ÁéØÂ¢ÉË°®Á§∫ÊñπÊ≥ïÈÄöÂ∏∏ÊòØ‰ªªÂä°ÁâπÂÆöÁöÑÔºå‰æãÂ¶ÇËßÜËßâÂØºËà™ÊàñÁ•ûÁªèËæêÂ∞ÑÂú∫ÔºåÁº∫‰πèÈÄöÁî®ÊÄßÔºåÈöæ‰ª•ÊîØÊåÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËøõË°åÂ§çÊùÇÁöÑÂú∫ÊôØÁêÜËß£„ÄÅÊé®ÁêÜÂíåËßÑÂàí„ÄÇËøô‰∫õÊñπÊ≥ïÈöæ‰ª•Êï¥ÂêàÂá†‰Ωï„ÄÅÂÖâÂ∫¶ÂíåËØ≠‰πâ‰ø°ÊÅØÔºåÈôêÂà∂‰∫ÜLLMÂú®Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÈÄöÁî®Âú∫ÊôØÊèèËø∞ÔºàUSDÔºâËØ≠Ë®Ä‰Ωú‰∏∫Êú∫Âô®‰∫∫ÁéØÂ¢ÉÁöÑÁªü‰∏ÄË°®Á§∫„ÄÇUSDÊòØ‰∏ÄÁßçÂü∫‰∫éXMLÁöÑÂú∫ÊôØÂõæÔºåÊó¢ÂèØ‰ª•Ë¢´LLMËØªÂèñÔºåÂèàÂÖ∑ÊúâË∂≥Â§üÁöÑË°®ËææËÉΩÂäõÊù•ÊèèËø∞ÁéØÂ¢ÉÁöÑÂá†‰Ωï„ÄÅÂÖâÂ∫¶ÂíåËØ≠‰πâ‰ø°ÊÅØ„ÄÇÈÄöËøáÂ∞ÜÁúüÂÆûÁéØÂ¢ÉËΩ¨Êç¢‰∏∫USDÊ†ºÂºèÔºåÂèØ‰ª•ÂÖÖÂàÜÂà©Áî®LLMÁöÑÂº∫Â§ßËÉΩÂäõËøõË°åÂú∫ÊôØÁêÜËß£Âíå‰ªªÂä°ËßÑÂàí„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöReal2USDÁ≥ªÁªüÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) Êï∞ÊçÆÈááÈõÜÔºö‰ΩøÁî®ÈÖçÂ§áÊøÄÂÖâÈõ∑ËææÂíåRGBÁõ∏Êú∫ÁöÑÂÆáÊ†ëGo2ÂõõË∂≥Êú∫Âô®‰∫∫ÈááÈõÜÂÆ§ÂÜÖÁéØÂ¢ÉÊï∞ÊçÆ„ÄÇ2) USDÊûÑÂª∫ÔºöÂ∞ÜÈááÈõÜÂà∞ÁöÑÊï∞ÊçÆËΩ¨Êç¢‰∏∫USDÊ†ºÂºèÁöÑÂú∫ÊôØË°®Á§∫ÔºåÂåÖÊã¨Âá†‰ΩïÊ®°Âûã„ÄÅÊùêË¥®Â±ûÊÄßÂíåËØ≠‰πâÊ†áÁ≠æ„ÄÇ3) LLMÊé®ÁêÜÔºö‰ΩøÁî®GoogleÁöÑGeminiÁ≠âLLMËß£ÊûêUSDÂú∫ÊôØÂõæÔºåËøõË°åÂú∫ÊôØÁêÜËß£„ÄÅÂ§çÊùÇÊé®ÁêÜÂíå‰ªªÂä°ËßÑÂàí„ÄÇ4) ‰ªøÁúüÈ™åËØÅÔºöÂú®NvidiaÁöÑIssac Sim‰∏≠ÂØπÁ≥ªÁªüËøõË°å‰ªøÁúüÈ™åËØÅÔºåËØÑ‰º∞ÂÖ∂Âú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜUSDËØ≠Ë®ÄÂºïÂÖ•Êú∫Âô®‰∫∫È¢ÜÂüüÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫LLMËøõË°åÂú∫ÊôØÁêÜËß£Âíå‰ªªÂä°ËßÑÂàíÁöÑÈÄöÁî®ÁéØÂ¢ÉË°®Á§∫„ÄÇ‰∏éÁé∞ÊúâÁöÑ‰ªªÂä°ÁâπÂÆöÊñπÊ≥ïÁõ∏ÊØîÔºåReal2USDÁ≥ªÁªüÂÖ∑ÊúâÊõ¥È´òÁöÑÈÄöÁî®ÊÄßÂíåÁÅµÊ¥ªÊÄßÔºåÂèØ‰ª•ÊîØÊåÅÊõ¥ÂπøÊ≥õÁöÑÊú∫Âô®‰∫∫Â∫îÁî®„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®USDÊûÑÂª∫ËøáÁ®ã‰∏≠ÔºåÈúÄË¶ÅÂØπÊøÄÂÖâÈõ∑ËææÁÇπ‰∫ëÂíåRGBÂõæÂÉèËøõË°åÂ§ÑÁêÜÔºåÁîüÊàêÈ´òË¥®ÈáèÁöÑÂá†‰ΩïÊ®°ÂûãÂíåÁ∫πÁêÜË¥¥Âõæ„ÄÇËØ≠‰πâÊ†áÁ≠æÂèØ‰ª•ÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÊ®°ÂûãÊàñ‰∫∫Â∑•Ê†áÊ≥®Ëé∑Âæó„ÄÇÂú®LLMÊé®ÁêÜËøáÁ®ã‰∏≠ÔºåÈúÄË¶ÅËÆæËÆ°ÂêàÈÄÇÁöÑpromptÔºåÂºïÂØºLLMÁêÜËß£USDÂú∫ÊôØÂõæÂπ∂ËøõË°åÊé®ÁêÜ„ÄÇÂÖ∑‰ΩìÁöÑÂèÇÊï∞ËÆæÁΩÆÂíåÁΩëÁªúÁªìÊûÑÂèñÂÜ≥‰∫éÊâÄ‰ΩøÁî®ÁöÑLLMÂíåËßÜËßâÊ®°Âûã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ËÆ∫ÊñáÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜReal2USDÁ≥ªÁªüÁöÑÊúâÊïàÊÄß„ÄÇÂú®ÂÆ§ÂÜÖÁéØÂ¢É‰∏≠ÔºåËØ•Á≥ªÁªüËÉΩÂ§üÊûÑÂª∫È´òË¥®ÈáèÁöÑUSDÂú∫ÊôØË°®Á§∫ÔºåÂπ∂Âà©Áî®LLMËøõË°åÂáÜÁ°ÆÁöÑÂú∫ÊôØÁêÜËß£Âíå‰ªªÂä°ËßÑÂàí„ÄÇÂú®Ê®°Êãü‰ªìÂ∫ìÂíåÂåªÈô¢ÁéØÂ¢É‰∏≠ÔºåËØ•Á≥ªÁªü‰πüË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReal2USDÁ≥ªÁªüËÉΩÂ§üÊòæËëóÊèêÂçáÊú∫Âô®‰∫∫ÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅ‰ªìÂÇ®Áâ©ÊµÅ„ÄÅÂåªÁñóÊúçÂä°Á≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âà©Áî®Real2USDÁ≥ªÁªüÁêÜËß£ÂÆ∂Â∫≠ÁéØÂ¢ÉÔºåÊâßË°åÊ∏ÖÊ¥Å„ÄÅÊï¥ÁêÜÁ≠â‰ªªÂä°ÔºõÂú®‰ªìÂ∫ì‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âà©Áî®ËØ•Á≥ªÁªüËøõË°åË¥ßÁâ©ËØÜÂà´„ÄÅË∑ØÂæÑËßÑÂàíÂíåËá™‰∏ªÂØºËà™ÔºõÂú®ÂåªÈô¢‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âà©Áî®ËØ•Á≥ªÁªüËøõË°åËçØÂìÅÈÖçÈÄÅ„ÄÅÁóÖ‰∫∫Êä§ÁêÜÁ≠â‰ªªÂä°„ÄÇËØ•Á†îÁ©∂‰∏∫Êú∫Âô®‰∫∫Êô∫ËÉΩÂåñÂèëÂ±ïÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑ØÂíåÊñπÊ≥ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large Language Models (LLMs) can help robots reason about abstract task specifications. This requires augmenting classical representations of the environment used by robots with natural language-based priors. There are a number of existing approaches to doing so, but they are tailored to specific tasks, e.g., visual-language models for navigation, language-guided neural radiance fields for mapping, etc. This paper argues that the Universal Scene Description (USD) language is an effective and general representation of geometric, photometric and semantic information in the environment for LLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene graph, readable by LLMs and humans alike, and rich enough to support essentially any task -- Pixar developed this language to store assets, scenes and even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2 quadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD representation of indoor environments with diverse objects and challenging settings with lots of glass, and (ii) parses the USD using Google's Gemini to demonstrate scene understanding, complex inferences, and planning. We also study different aspects of this system in simulated warehouse and hospital settings using Nvidia's Issac Sim. Code is available at https://github.com/grasp-lyrl/Real2USD .

