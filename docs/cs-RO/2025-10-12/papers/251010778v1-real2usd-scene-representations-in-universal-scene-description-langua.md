---
layout: default
title: Real2USD: Scene Representations in Universal Scene Description Language
---

# Real2USD: Scene Representations in Universal Scene Description Language

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10778" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10778v1</a>
  <a href="https://arxiv.org/pdf/2510.10778.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10778v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10778v1', 'Real2USD: Scene Representations in Universal Scene Description Language')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Christopher D. Hsu, Pratik Chaudhari

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

**å¤‡æ³¨**: 8 pages, 10 figures, 1 table

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/grasp-lyrl/Real2USD)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReal2USDç³»ç»Ÿï¼Œåˆ©ç”¨é€šç”¨åœºæ™¯æè¿°è¯­è¨€USDèµ‹èƒ½LLMæœºå™¨äººåœºæ™¯ç†è§£ä¸è§„åˆ’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é€šç”¨åœºæ™¯æè¿°` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœºå™¨äºº` `åœºæ™¯ç†è§£` `ä»»åŠ¡è§„åˆ’` `ç¯å¢ƒè¡¨ç¤º` `USD` `Real2USD`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººç¯å¢ƒè¡¨ç¤ºæ–¹æ³•ç¼ºä¹é€šç”¨æ€§ï¼Œéš¾ä»¥æœ‰æ•ˆæ”¯æŒLLMè¿›è¡Œå¤æ‚æ¨ç†å’Œè§„åˆ’ã€‚
2. æå‡ºReal2USDç³»ç»Ÿï¼Œå°†çœŸå®ç¯å¢ƒè½¬æ¢ä¸ºUSDæ ¼å¼ï¼Œåˆ©ç”¨å…¶ä¸°å¯Œçš„è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿåˆ©ç”¨LLMè¿›è¡Œåœºæ™¯ç†è§£ã€å¤æ‚æ¨ç†å’Œè§„åˆ’ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­éªŒè¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹(LLM)èƒ½å¤Ÿå¸®åŠ©æœºå™¨äººæ¨ç†æŠ½è±¡çš„ä»»åŠ¡è§„èŒƒã€‚è¿™éœ€è¦ç”¨åŸºäºè‡ªç„¶è¯­è¨€çš„å…ˆéªŒçŸ¥è¯†æ¥å¢å¼ºæœºå™¨äººæ‰€ä½¿ç”¨çš„ç»å…¸ç¯å¢ƒè¡¨ç¤ºã€‚ç›®å‰å·²æœ‰ä¸€äº›æ–¹æ³•ï¼Œä½†å®ƒä»¬æ˜¯ä¸ºç‰¹å®šä»»åŠ¡å®šåˆ¶çš„ï¼Œä¾‹å¦‚ç”¨äºå¯¼èˆªçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºåœ°å›¾æ„å»ºçš„è¯­è¨€å¼•å¯¼ç¥ç»è¾å°„åœºç­‰ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œé€šç”¨åœºæ™¯æè¿°(USD)è¯­è¨€æ˜¯åŸºäºLLMçš„æœºå™¨äººä»»åŠ¡ä¸­ç¯å¢ƒçš„å‡ ä½•ã€å…‰åº¦å’Œè¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆä¸”é€šç”¨çš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„è®ºç‚¹å¾ˆç®€å•ï¼šUSDæ˜¯ä¸€ç§åŸºäºXMLçš„åœºæ™¯å›¾ï¼ŒLLMå’Œäººç±»éƒ½å¯ä»¥è¯»å–ï¼Œå¹¶ä¸”è¶³å¤Ÿä¸°å¯Œä»¥æ”¯æŒå‡ ä¹ä»»ä½•ä»»åŠ¡â€”â€”çš®å…‹æ–¯å¼€å‘è¿™ç§è¯­è¨€æ¥å­˜å‚¨èµ„äº§ã€åœºæ™¯ç”šè‡³ç”µå½±ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªâ€œReal to USDâ€ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨é…å¤‡æ¿€å…‰é›·è¾¾å’ŒRGBç›¸æœºçš„å®‡æ ‘Go2å››è¶³æœºå™¨äººï¼Œè¯¥ç³»ç»Ÿ(i)æ„å»ºäº†å…·æœ‰å„ç§ç‰©ä½“å’Œå…·æœ‰å¤§é‡ç»ç’ƒçš„æŒ‘æˆ˜æ€§è®¾ç½®çš„å®¤å†…ç¯å¢ƒçš„æ˜¾å¼USDè¡¨ç¤ºï¼Œä»¥åŠ(ii)ä½¿ç”¨è°·æ­Œçš„Geminiè§£æUSDä»¥æ¼”ç¤ºåœºæ™¯ç†è§£ã€å¤æ‚æ¨ç†å’Œè§„åˆ’ã€‚æˆ‘ä»¬è¿˜åœ¨ä½¿ç”¨Nvidiaçš„Issac Simçš„æ¨¡æ‹Ÿä»“åº“å’ŒåŒ»é™¢ç¯å¢ƒä¸­ç ”ç©¶äº†è¯¥ç³»ç»Ÿçš„ä¸åŒæ–¹é¢ã€‚ä»£ç å¯åœ¨https://github.com/grasp-lyrl/Real2USD è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººç¯å¢ƒè¡¨ç¤ºæ–¹æ³•é€šå¸¸æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œä¾‹å¦‚è§†è§‰å¯¼èˆªæˆ–ç¥ç»è¾å°„åœºï¼Œç¼ºä¹é€šç”¨æ€§ï¼Œéš¾ä»¥æ”¯æŒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤æ‚çš„åœºæ™¯ç†è§£ã€æ¨ç†å’Œè§„åˆ’ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥æ•´åˆå‡ ä½•ã€å…‰åº¦å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œé™åˆ¶äº†LLMåœ¨æœºå™¨äººä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é€šç”¨åœºæ™¯æè¿°ï¼ˆUSDï¼‰è¯­è¨€ä½œä¸ºæœºå™¨äººç¯å¢ƒçš„ç»Ÿä¸€è¡¨ç¤ºã€‚USDæ˜¯ä¸€ç§åŸºäºXMLçš„åœºæ™¯å›¾ï¼Œæ—¢å¯ä»¥è¢«LLMè¯»å–ï¼Œåˆå…·æœ‰è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›æ¥æè¿°ç¯å¢ƒçš„å‡ ä½•ã€å…‰åº¦å’Œè¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡å°†çœŸå®ç¯å¢ƒè½¬æ¢ä¸ºUSDæ ¼å¼ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨LLMçš„å¼ºå¤§èƒ½åŠ›è¿›è¡Œåœºæ™¯ç†è§£å’Œä»»åŠ¡è§„åˆ’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReal2USDç³»ç»Ÿçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ•°æ®é‡‡é›†ï¼šä½¿ç”¨é…å¤‡æ¿€å…‰é›·è¾¾å’ŒRGBç›¸æœºçš„å®‡æ ‘Go2å››è¶³æœºå™¨äººé‡‡é›†å®¤å†…ç¯å¢ƒæ•°æ®ã€‚2) USDæ„å»ºï¼šå°†é‡‡é›†åˆ°çš„æ•°æ®è½¬æ¢ä¸ºUSDæ ¼å¼çš„åœºæ™¯è¡¨ç¤ºï¼ŒåŒ…æ‹¬å‡ ä½•æ¨¡å‹ã€æè´¨å±æ€§å’Œè¯­ä¹‰æ ‡ç­¾ã€‚3) LLMæ¨ç†ï¼šä½¿ç”¨Googleçš„Geminiç­‰LLMè§£æUSDåœºæ™¯å›¾ï¼Œè¿›è¡Œåœºæ™¯ç†è§£ã€å¤æ‚æ¨ç†å’Œä»»åŠ¡è§„åˆ’ã€‚4) ä»¿çœŸéªŒè¯ï¼šåœ¨Nvidiaçš„Issac Simä¸­å¯¹ç³»ç»Ÿè¿›è¡Œä»¿çœŸéªŒè¯ï¼Œè¯„ä¼°å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†USDè¯­è¨€å¼•å…¥æœºå™¨äººé¢†åŸŸï¼Œå¹¶å°†å…¶ä½œä¸ºLLMè¿›è¡Œåœºæ™¯ç†è§£å’Œä»»åŠ¡è§„åˆ’çš„é€šç”¨ç¯å¢ƒè¡¨ç¤ºã€‚ä¸ç°æœ‰çš„ä»»åŠ¡ç‰¹å®šæ–¹æ³•ç›¸æ¯”ï¼ŒReal2USDç³»ç»Ÿå…·æœ‰æ›´é«˜çš„é€šç”¨æ€§å’Œçµæ´»æ€§ï¼Œå¯ä»¥æ”¯æŒæ›´å¹¿æ³›çš„æœºå™¨äººåº”ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨USDæ„å»ºè¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¯¹æ¿€å…‰é›·è¾¾ç‚¹äº‘å’ŒRGBå›¾åƒè¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å‡ ä½•æ¨¡å‹å’Œçº¹ç†è´´å›¾ã€‚è¯­ä¹‰æ ‡ç­¾å¯ä»¥é€šè¿‡é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹æˆ–äººå·¥æ ‡æ³¨è·å¾—ã€‚åœ¨LLMæ¨ç†è¿‡ç¨‹ä¸­ï¼Œéœ€è¦è®¾è®¡åˆé€‚çš„promptï¼Œå¼•å¯¼LLMç†è§£USDåœºæ™¯å›¾å¹¶è¿›è¡Œæ¨ç†ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„å–å†³äºæ‰€ä½¿ç”¨çš„LLMå’Œè§†è§‰æ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†Real2USDç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚åœ¨å®¤å†…ç¯å¢ƒä¸­ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ„å»ºé«˜è´¨é‡çš„USDåœºæ™¯è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œå‡†ç¡®çš„åœºæ™¯ç†è§£å’Œä»»åŠ¡è§„åˆ’ã€‚åœ¨æ¨¡æ‹Ÿä»“åº“å’ŒåŒ»é™¢ç¯å¢ƒä¸­ï¼Œè¯¥ç³»ç»Ÿä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReal2USDç³»ç»Ÿèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ™ºèƒ½å®¶å±…ã€ä»“å‚¨ç‰©æµã€åŒ»ç–—æœåŠ¡ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨Real2USDç³»ç»Ÿç†è§£å®¶åº­ç¯å¢ƒï¼Œæ‰§è¡Œæ¸…æ´ã€æ•´ç†ç­‰ä»»åŠ¡ï¼›åœ¨ä»“åº“ä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥ç³»ç»Ÿè¿›è¡Œè´§ç‰©è¯†åˆ«ã€è·¯å¾„è§„åˆ’å’Œè‡ªä¸»å¯¼èˆªï¼›åœ¨åŒ»é™¢ä¸­ï¼Œæœºå™¨äººå¯ä»¥åˆ©ç”¨è¯¥ç³»ç»Ÿè¿›è¡Œè¯å“é…é€ã€ç—…äººæŠ¤ç†ç­‰ä»»åŠ¡ã€‚è¯¥ç ”ç©¶ä¸ºæœºå™¨äººæ™ºèƒ½åŒ–å‘å±•æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) can help robots reason about abstract task specifications. This requires augmenting classical representations of the environment used by robots with natural language-based priors. There are a number of existing approaches to doing so, but they are tailored to specific tasks, e.g., visual-language models for navigation, language-guided neural radiance fields for mapping, etc. This paper argues that the Universal Scene Description (USD) language is an effective and general representation of geometric, photometric and semantic information in the environment for LLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene graph, readable by LLMs and humans alike, and rich enough to support essentially any task -- Pixar developed this language to store assets, scenes and even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2 quadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD representation of indoor environments with diverse objects and challenging settings with lots of glass, and (ii) parses the USD using Google's Gemini to demonstrate scene understanding, complex inferences, and planning. We also study different aspects of this system in simulated warehouse and hospital settings using Nvidia's Issac Sim. Code is available at https://github.com/grasp-lyrl/Real2USD .

