---
layout: default
title: Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning
---

# Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10759" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10759v1</a>
  <a href="https://arxiv.org/pdf/2510.10759.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10759v1" onclick="toggleFavorite(this, '2510.10759v1', 'Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Arthicha Srisuchinnawong, Poramate Manoonpong

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

**å¤‡æ³¨**: RSS 2025

**DOI**: [10.15607/RSS.2025.XXI.123](https://doi.org/10.15607/RSS.2025.XXI.123)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºROGERç®—æ³•ï¼Œé€šè¿‡åœ¨çº¿è°ƒæ•´å¥–åŠ±å¢ç›Šå®ç°çº¦æŸä¸‹çš„æœºå™¨äººè¿åŠ¨å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººè¿åŠ¨å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å‡½æ•°è®¾è®¡` `çº¦æŸæ»¡è¶³` `åœ¨çº¿å­¦ä¹ ` `å››è¶³æœºå™¨äºº` `å¥–åŠ±å¢ç›Šè°ƒæ•´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººè¿åŠ¨å­¦ä¹ æ–¹æ³•ä¾èµ–ç¦»çº¿å¥–åŠ±æƒé‡è°ƒæ•´ï¼Œä¸”éš¾ä»¥ä¿è¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„çº¦æŸæ»¡è¶³ã€‚
2. ROGERç®—æ³•é€šè¿‡åœ¨çº¿è°ƒæ•´å¥–åŠ±å¢ç›Šï¼Œæ ¹æ®å¥–åŠ±å’Œæƒ©ç½šçš„æ¯”ä¾‹åŠ¨æ€è°ƒæ•´å­¦ä¹ ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒROGERåœ¨å››è¶³æœºå™¨äººå’ŒMuJoCoåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½å’Œçº¦æŸæ»¡è¶³èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰çš„æœºå™¨äººè¿åŠ¨å­¦ä¹ æŠ€æœ¯ä¸¥é‡ä¾èµ–äºç¦»çº¿é€‰æ‹©åˆé€‚çš„å¥–åŠ±æƒé‡å¢ç›Šï¼Œå¹¶ä¸”æ— æ³•ä¿è¯è®­ç»ƒæœŸé—´æ»¡è¶³çº¦æŸæ¡ä»¶ï¼ˆå³é¿å…è¿åçº¦æŸï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œåŸºäºå…·èº«è°ƒèŠ‚çš„å¥–åŠ±å¯¼å‘å¢ç›Šâ€ï¼ˆROGERï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºåœ¨å…·èº«äº¤äº’è¿‡ç¨‹ä¸­æ”¶åˆ°çš„æƒ©ç½šåœ¨çº¿è°ƒæ•´å¥–åŠ±æƒé‡å¢ç›Šã€‚éšç€å­¦ä¹ æ¥è¿‘çº¦æŸé˜ˆå€¼ï¼Œæ­£å‘å¥–åŠ±ï¼ˆä¸»è¦å¥–åŠ±ï¼‰å’Œè´Ÿå‘å¥–åŠ±ï¼ˆæƒ©ç½šï¼‰å¢ç›Šä¹‹é—´çš„æ¯”ç‡ä¼šè‡ªåŠ¨é™ä½ï¼Œä»¥é¿å…è¿åçº¦æŸã€‚ç›¸åï¼Œå½“å­¦ä¹ å¤„äºå®‰å…¨çŠ¶æ€æ—¶ï¼Œè¯¥æ¯”ç‡ä¼šå¢åŠ ï¼Œä»¥ä¼˜å…ˆè€ƒè™‘æ€§èƒ½ã€‚åœ¨60å…¬æ–¤çš„å››è¶³æœºå™¨äººä¸Šï¼ŒROGERåœ¨å¤šä¸ªå­¦ä¹ è¯•éªŒä¸­å®ç°äº†æ¥è¿‘é›¶çš„çº¦æŸè¿åã€‚ä¸åŒç±»æœ€å…ˆè¿›çš„æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒè¿˜å®ç°äº†é«˜è¾¾50%çš„ä¸»è¦å¥–åŠ±æå‡ã€‚åœ¨MuJoCoè¿ç»­è¿åŠ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬å•è…¿è·³è·ƒæœºå™¨äººï¼ŒROGERè¡¨ç°å‡ºä¸é»˜è®¤å¥–åŠ±å‡½æ•°è®­ç»ƒçš„æ¨¡å‹ç›¸å½“æˆ–é«˜è¾¾100%çš„æ›´é«˜æ€§èƒ½ï¼Œä»¥åŠ60%çš„æ›´ä½æ‰­çŸ©ä½¿ç”¨å’Œæ–¹å‘åå·®ã€‚æœ€åï¼Œåœ¨æ²¡æœ‰è·Œå€’çš„æƒ…å†µä¸‹ï¼Œä»…ç”¨ä¸€å°æ—¶å°±ä»å¤´å®ç°äº†ç‰©ç†å››è¶³æœºå™¨äººçš„çœŸå®ä¸–ç•Œè¿åŠ¨å­¦ä¹ ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œæœ‰åŠ©äºæ»¡è¶³çº¦æŸçš„çœŸå®ä¸–ç•ŒæŒç»­æœºå™¨äººè¿åŠ¨å­¦ä¹ ï¼Œå¹¶ç®€åŒ–äº†å¥–åŠ±æƒé‡å¢ç›Šè°ƒæ•´ï¼Œä»è€Œå¯èƒ½ä¿ƒè¿›ç‰©ç†æœºå™¨äººå’Œåœ¨çœŸå®ä¸–ç•Œä¸­å­¦ä¹ çš„æœºå™¨äººçš„å¼€å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººè¿åŠ¨å­¦ä¹ æ–¹æ³•éœ€è¦æ‰‹åŠ¨è°ƒæ•´å¥–åŠ±å‡½æ•°çš„æƒé‡ï¼Œè¿™æ˜¯ä¸€ä¸ªç¹çä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•ä¿è¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ»¡è¶³å„ç§çº¦æŸæ¡ä»¶ï¼Œä¾‹å¦‚é¿å…å…³èŠ‚è¶…å‡ºè¿åŠ¨èŒƒå›´æˆ–æœºå™¨äººè·Œå€’ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨è°ƒæ•´å¥–åŠ±æƒé‡å¹¶ä¿è¯çº¦æŸæ»¡è¶³çš„è¿åŠ¨å­¦ä¹ æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šROGERçš„æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®æœºå™¨äººä¸ç¯å¢ƒçš„äº¤äº’è¿‡ç¨‹ä¸­è·å¾—çš„å¥–åŠ±å’Œæƒ©ç½šï¼Œåœ¨çº¿è°ƒæ•´å¥–åŠ±å‡½æ•°çš„æƒé‡å¢ç›Šã€‚å…·ä½“æ¥è¯´ï¼Œå½“æœºå™¨äººæ¥è¿‘çº¦æŸè¾¹ç•Œæ—¶ï¼Œé™ä½æ­£å‘å¥–åŠ±çš„æƒé‡ï¼Œæé«˜è´Ÿå‘æƒ©ç½šçš„æƒé‡ï¼Œä»è€Œé¿å…è¿åçº¦æŸã€‚åä¹‹ï¼Œå½“æœºå™¨äººè¿œç¦»çº¦æŸè¾¹ç•Œæ—¶ï¼Œæé«˜æ­£å‘å¥–åŠ±çš„æƒé‡ï¼Œé¼“åŠ±æœºå™¨äººè¿½æ±‚æ›´é«˜çš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šROGERç®—æ³•ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼šç¯å¢ƒäº¤äº’æ¨¡å—ï¼Œè´Ÿè´£ä¸æœºå™¨äººç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œè·å–çŠ¶æ€ã€å¥–åŠ±å’Œæƒ©ç½šä¿¡å·ï¼›å¥–åŠ±å¢ç›Šè°ƒæ•´æ¨¡å—ï¼Œæ ¹æ®å¥–åŠ±å’Œæƒ©ç½šä¿¡å·ï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±å‡½æ•°çš„æƒé‡å¢ç›Šï¼›ç­–ç•¥å­¦ä¹ æ¨¡å—ï¼Œåˆ©ç”¨è°ƒæ•´åçš„å¥–åŠ±å‡½æ•°ï¼Œå­¦ä¹ æœºå™¨äººçš„è¿åŠ¨ç­–ç•¥ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šæœºå™¨äººä¸ç¯å¢ƒäº¤äº’ï¼Œè·å¾—å¥–åŠ±å’Œæƒ©ç½šï¼Œå¥–åŠ±å¢ç›Šè°ƒæ•´æ¨¡å—æ ¹æ®è¿™äº›ä¿¡å·è°ƒæ•´å¥–åŠ±æƒé‡ï¼Œç­–ç•¥å­¦ä¹ æ¨¡å—åˆ©ç”¨è°ƒæ•´åçš„å¥–åŠ±å‡½æ•°æ›´æ–°ç­–ç•¥ï¼Œç„¶åé‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šROGERçš„å…³é”®åˆ›æ–°åœ¨äºå…¶åœ¨çº¿å¥–åŠ±å¢ç›Šè°ƒæ•´æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿è°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼ŒROGERèƒ½å¤Ÿæ ¹æ®æœºå™¨äººçš„å®é™…è¡¨ç°åŠ¨æ€è°ƒæ•´å¥–åŠ±æƒé‡ï¼Œä»è€Œæ›´å¥½åœ°å¹³è¡¡æ€§èƒ½å’Œçº¦æŸæ»¡è¶³ã€‚æ­¤å¤–ï¼ŒROGERè¿˜å¼•å…¥äº†å…·èº«è°ƒèŠ‚çš„æ¦‚å¿µï¼Œåˆ©ç”¨æœºå™¨äººçš„ç‰©ç†ç‰¹æ€§æ¥æŒ‡å¯¼å¥–åŠ±å¢ç›Šçš„è°ƒæ•´ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šROGERçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šå¥–åŠ±å¢ç›Šçš„è°ƒæ•´ç­–ç•¥ï¼Œè®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºå¥–åŠ±å’Œæƒ©ç½šæ¯”ä¾‹çš„è°ƒæ•´ç­–ç•¥ï¼Œå…·ä½“å…¬å¼æœªçŸ¥ï¼›å¥–åŠ±å‡½æ•°çš„å…·ä½“å½¢å¼ï¼Œè®ºæ–‡ä¸­ä½¿ç”¨çš„å¥–åŠ±å‡½æ•°åŒ…æ‹¬ä¸€ä¸ªä¸»è¦å¥–åŠ±é¡¹å’Œä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œä¸»è¦å¥–åŠ±é¡¹ç”¨äºé¼“åŠ±æœºå™¨äººå®Œæˆä»»åŠ¡ï¼Œæƒ©ç½šé¡¹ç”¨äºæƒ©ç½šæœºå™¨äººè¿åçº¦æŸï¼›ç­–ç•¥å­¦ä¹ ç®—æ³•ï¼Œè®ºæ–‡ä¸­ä½¿ç”¨çš„ç­–ç•¥å­¦ä¹ ç®—æ³•æœªçŸ¥ï¼Œä½†å¯ä»¥æ˜¯ä»»ä½•åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç®—æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

ROGERç®—æ³•åœ¨å¤šä¸ªå®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚åœ¨60å…¬æ–¤çš„å››è¶³æœºå™¨äººä¸Šï¼ŒROGERå®ç°äº†æ¥è¿‘é›¶çš„çº¦æŸè¿åï¼Œå¹¶è·å¾—äº†æ¯”ç°æœ‰æŠ€æœ¯é«˜50%çš„ä¸»è¦å¥–åŠ±ã€‚åœ¨MuJoCoåŸºå‡†æµ‹è¯•ä¸­ï¼ŒROGERçš„æ€§èƒ½æé«˜äº†é«˜è¾¾100%ï¼ŒåŒæ—¶æ‰­çŸ©ä½¿ç”¨å’Œæ–¹å‘åå·®é™ä½äº†60%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒROGERåœ¨çœŸå®å››è¶³æœºå™¨äººä¸Šå®ç°äº†ä»é›¶å¼€å§‹çš„è¿åŠ¨å­¦ä¹ ï¼Œä»…ç”¨ä¸€å°æ—¶å°±å®Œæˆäº†è®­ç»ƒï¼Œä¸”æ²¡æœ‰å‘ç”Ÿè·Œå€’ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ROGERç®—æ³•å¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦æ»¡è¶³çº¦æŸçš„æœºå™¨äººè¿åŠ¨å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å››è¶³æœºå™¨äººã€äººå½¢æœºå™¨äººã€æœºæ¢°è‡‚ç­‰ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿç®€åŒ–å¥–åŠ±å‡½æ•°çš„è®¾è®¡è¿‡ç¨‹ï¼Œæé«˜å­¦ä¹ æ•ˆç‡å’Œé²æ£’æ€§ï¼Œä»è€ŒåŠ é€Ÿæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼ŒROGERç®—æ³•è¿˜å¯ç”¨äºå¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„æœºå™¨äººç³»ç»Ÿï¼Œä¾‹å¦‚åŒ»ç–—æœºå™¨äººã€æ•‘æ´æœºå™¨äººç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Existing robot locomotion learning techniques rely heavily on the offline selection of proper reward weighting gains and cannot guarantee constraint satisfaction (i.e., constraint violation) during training. Thus, this work aims to address both issues by proposing Reward-Oriented Gains via Embodied Regulation (ROGER), which adapts reward-weighting gains online based on penalties received throughout the embodied interaction process. The ratio between the positive reward (primary reward) and negative reward (penalty) gains is automatically reduced as the learning approaches the constraint thresholds to avoid violation. Conversely, the ratio is increased when learning is in safe states to prioritize performance. With a 60-kg quadruped robot, ROGER achieved near-zero constraint violation throughout multiple learning trials. It also achieved up to 50% more primary reward than the equivalent state-of-the-art techniques. In MuJoCo continuous locomotion benchmarks, including a single-leg hopper, ROGER exhibited comparable or up to 100% higher performance and 60% less torque usage and orientation deviation compared to those trained with the default reward function. Finally, real-world locomotion learning of a physical quadruped robot was achieved from scratch within one hour without any falls. Therefore, this work contributes to constraint-satisfying real-world continual robot locomotion learning and simplifies reward weighting gain tuning, potentially facilitating the development of physical robots and those that learn in the real world.

