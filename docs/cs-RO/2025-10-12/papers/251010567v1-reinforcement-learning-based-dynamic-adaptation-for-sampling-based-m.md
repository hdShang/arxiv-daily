---
layout: default
title: Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving
---

# Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10567" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10567v1</a>
  <a href="https://arxiv.org/pdf/2510.10567.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10567v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10567v1', 'Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Alexander Langmann, Yevhenii Tokarev, Mattia Piccinini, Korbinian Moller, Johannes Betz

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-12

**å¤‡æ³¨**: 8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€è‡ªé€‚åº”é‡‡æ ·è¿åŠ¨è§„åˆ’ï¼Œç”¨äºæ•æ·è‡ªä¸»é©¾é©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è¿åŠ¨è§„åˆ’` `è‡ªä¸»é©¾é©¶` `é‡‡æ ·ç®—æ³•` `åŠ¨æ€è‡ªé€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. é‡‡æ ·è½¨è¿¹è§„åˆ’å™¨åœ¨æ•æ·è‡ªä¸»é©¾é©¶ä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†å…¶æ€§èƒ½å—é™äºæ‰‹åŠ¨è°ƒæ•´çš„é™æ€ä»£ä»·å‡½æ•°æƒé‡ï¼Œå¯¼è‡´ç­–ç•¥å¦¥åã€‚
2. æœ¬æ–‡æå‡ºä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åŠ¨æ€åˆ‡æ¢åº•å±‚è½¨è¿¹è§„åˆ’å™¨çš„ä»£ä»·å‡½æ•°å‚æ•°ï¼Œå®ç°é«˜å±‚è¡Œä¸ºé€‰æ‹©å’Œè‡ªé€‚åº”è§„åˆ’ã€‚
3. ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªä¸»èµ›è½¦ä¸­å®ç°äº†æ›´ä½çš„ç¢°æ’ç‡å’Œæ›´å¿«çš„è¶…è½¦é€Ÿåº¦ï¼Œæå‡äº†å®‰å…¨æ€§å’Œç«äº‰æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åŠ¨æ€è‡ªé€‚åº”æ–¹æ³•ï¼Œç”¨äºæ•æ·è‡ªä¸»é©¾é©¶ä¸­çš„é‡‡æ ·è½¨è¿¹è§„åˆ’ã€‚è¯¥æ–¹æ³•ä½¿ç”¨RLæ™ºèƒ½ä½“ä½œä¸ºé«˜å±‚è¡Œä¸ºé€‰æ‹©å™¨ï¼Œåœ¨è¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢åº•å±‚è§£æè½¨è¿¹è§„åˆ’å™¨çš„ä»£ä»·å‡½æ•°å‚æ•°ã€‚åœ¨è‡ªä¸»èµ›è½¦ç¯å¢ƒçš„ä»¿çœŸå®éªŒä¸­ï¼Œä¸æœ€å…ˆè¿›çš„é™æ€è§„åˆ’å™¨ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†0%çš„ç¢°æ’ç‡ï¼Œå¹¶å°†è¶…è½¦æ—¶é—´ç¼©çŸ­äº†é«˜è¾¾60%ã€‚è¯¥æ™ºèƒ½ä½“èƒ½å¤ŸåŠ¨æ€åˆ‡æ¢æ¿€è¿›å’Œä¿å®ˆè¡Œä¸ºï¼Œå®ç°é™æ€é…ç½®æ— æ³•å®ç°çš„äº¤äº’å¼æ“ä½œã€‚ç»“æœè¡¨æ˜ï¼Œé›†æˆå¼ºåŒ–å­¦ä¹ ä½œä¸ºé«˜å±‚é€‰æ‹©å™¨èƒ½å¤Ÿè§£å†³è‡ªä¸»èµ›è½¦è§„åˆ’å™¨ä¸­å®‰å…¨æ€§å’Œç«äº‰æ€§ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ã€‚è¯¥æ–¹æ³•ä¸ºæ›´å¹¿æ³›çš„è‡ªä¸»é©¾é©¶åº”ç”¨æä¾›äº†ä¸€ç§è‡ªé€‚åº”ä¸”å¯è§£é‡Šçš„è¿åŠ¨è§„åˆ’é€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„é‡‡æ ·è½¨è¿¹è§„åˆ’å™¨åœ¨æ•æ·è‡ªä¸»é©¾é©¶ä¸­é¢ä¸´ç€ä»£ä»·å‡½æ•°æƒé‡é™æ€çš„é—®é¢˜ã€‚è¿™äº›æƒé‡é€šå¸¸éœ€è¦æ‰‹åŠ¨è°ƒæ•´ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„åœºæ™¯ä¸‹æ— æ³•è¾¾åˆ°æœ€ä¼˜ï¼Œå¯¼è‡´å®‰å…¨æ€§å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚é™æ€çš„ä»£ä»·å‡½æ•°æ— æ³•æ ¹æ®ç¯å¢ƒå˜åŒ–åŠ¨æ€è°ƒæ•´è§„åˆ’ç­–ç•¥ï¼Œé™åˆ¶äº†è‡ªä¸»é©¾é©¶è½¦è¾†çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“ä½œä¸ºé«˜å±‚å†³ç­–å™¨ï¼ŒåŠ¨æ€åœ°è°ƒæ•´åº•å±‚è½¨è¿¹è§„åˆ’å™¨çš„ä»£ä»·å‡½æ•°å‚æ•°ã€‚é€šè¿‡å­¦ä¹ ä¸åŒåœºæ™¯ä¸‹çš„æœ€ä¼˜ä»£ä»·å‡½æ•°é…ç½®ï¼ŒRLæ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®å½“å‰ç¯å¢ƒé€‰æ‹©åˆé€‚çš„è¡Œä¸ºç­–ç•¥ï¼Œä»è€Œåœ¨å®‰å…¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚è¿™ç§åŠ¨æ€è°ƒæ•´çš„ç­–ç•¥ä½¿å¾—è½¦è¾†èƒ½å¤Ÿæ ¹æ®å®é™…æƒ…å†µé‡‡å–æ¿€è¿›æˆ–ä¿å®ˆçš„é©¾é©¶é£æ ¼ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•çš„æŠ€æœ¯æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåº•å±‚è½¨è¿¹è§„åˆ’å™¨å’Œé«˜å±‚RLæ™ºèƒ½ä½“ã€‚åº•å±‚è½¨è¿¹è§„åˆ’å™¨è´Ÿè´£ç”Ÿæˆæ»¡è¶³è½¦è¾†è¿åŠ¨å­¦å’ŒåŠ¨åŠ›å­¦çº¦æŸçš„è½¨è¿¹ï¼Œå¹¶æ ¹æ®ç»™å®šçš„ä»£ä»·å‡½æ•°è¯„ä¼°è½¨è¿¹çš„ä¼˜åŠ£ã€‚é«˜å±‚RLæ™ºèƒ½ä½“åˆ™æ ¹æ®å½“å‰ç¯å¢ƒçŠ¶æ€ï¼Œé€‰æ‹©ä¸€ç»„ä»£ä»·å‡½æ•°å‚æ•°ï¼Œä¼ é€’ç»™åº•å±‚è½¨è¿¹è§„åˆ’å™¨ã€‚æ•´ä¸ªç³»ç»Ÿé€šè¿‡ä¸æ–­è¿­ä»£ï¼ŒRLæ™ºèƒ½ä½“å­¦ä¹ åˆ°æœ€ä¼˜çš„ä»£ä»·å‡½æ•°é€‰æ‹©ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¼ºåŒ–å­¦ä¹ ä¸ä¼ ç»Ÿçš„é‡‡æ ·è½¨è¿¹è§„åˆ’å™¨ç›¸ç»“åˆï¼Œå®ç°äº†ä»£ä»·å‡½æ•°çš„åŠ¨æ€è‡ªé€‚åº”è°ƒæ•´ã€‚ä¸ä¼ ç»Ÿçš„é™æ€ä»£ä»·å‡½æ•°æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ç¯å¢ƒå˜åŒ–åŠ¨æ€è°ƒæ•´è§„åˆ’ç­–ç•¥ï¼Œä»è€Œåœ¨å®‰å…¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚è¿™ç§åŠ¨æ€è°ƒæ•´çš„ç­–ç•¥ä½¿å¾—è½¦è¾†èƒ½å¤Ÿæ ¹æ®å®é™…æƒ…å†µé‡‡å–æ¿€è¿›æˆ–ä¿å®ˆçš„é©¾é©¶é£æ ¼ï¼Œæé«˜äº†è½¦è¾†çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šRLæ™ºèƒ½ä½“ä½¿ç”¨æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰è¿›è¡Œè®­ç»ƒï¼ŒçŠ¶æ€ç©ºé—´åŒ…æ‹¬è½¦è¾†çš„é€Ÿåº¦ã€ä½ç½®ã€ä¸èµ›é“ä¸­å¿ƒçš„è·ç¦»ç­‰ä¿¡æ¯ï¼ŒåŠ¨ä½œç©ºé—´ä¸ºç¦»æ•£çš„ä»£ä»·å‡½æ•°å‚æ•°ç»„åˆã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†å®‰å…¨æ€§ï¼ˆé¿å…ç¢°æ’ï¼‰ã€é€Ÿåº¦å’Œè½¨è¿¹å¹³æ»‘æ€§ã€‚åº•å±‚è½¨è¿¹è§„åˆ’å™¨é‡‡ç”¨è§£ææ–¹æ³•ï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆæ»¡è¶³è½¦è¾†è¿åŠ¨å­¦å’ŒåŠ¨åŠ›å­¦çº¦æŸçš„è½¨è¿¹ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°å’ŒçŠ¶æ€ç©ºé—´ï¼ŒRLæ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ åˆ°æœ€ä¼˜çš„ä»£ä»·å‡½æ•°é€‰æ‹©ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨è‡ªä¸»èµ›è½¦ä»¿çœŸç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•ä¸æœ€å…ˆè¿›çš„é™æ€è§„åˆ’å™¨ç›¸æ¯”ï¼Œå®ç°äº†0%çš„ç¢°æ’ç‡ï¼Œå¹¶å°†è¶…è½¦æ—¶é—´ç¼©çŸ­äº†é«˜è¾¾60%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜è‡ªä¸»é©¾é©¶è½¦è¾†çš„å®‰å…¨æ€§å’Œç«äº‰æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å¤Ÿå®ç°é™æ€é…ç½®æ— æ³•å®ç°çš„äº¤äº’å¼æ“ä½œï¼Œä¾‹å¦‚åŠ¨æ€åˆ‡æ¢æ¿€è¿›å’Œä¿å®ˆè¡Œä¸ºã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§è‡ªä¸»é©¾é©¶åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶å‡ºç§Ÿè½¦ã€ç‰©æµé…é€è½¦è¾†å’Œæ— äººæ¸…æ‰«è½¦ç­‰ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´è¿åŠ¨è§„åˆ’ç­–ç•¥ï¼Œè½¦è¾†èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„äº¤é€šç¯å¢ƒï¼Œæé«˜è¡Œé©¶æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚æ— äººæœºå’Œæ°´ä¸‹æœºå™¨äººï¼Œå®ç°æ›´åŠ æ™ºèƒ½å’Œçµæ´»çš„è¿åŠ¨æ§åˆ¶ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Sampling-based trajectory planners are widely used for agile autonomous driving due to their ability to generate fast, smooth, and kinodynamically feasible trajectories. However, their behavior is often governed by a cost function with manually tuned, static weights, which forces a tactical compromise that is suboptimal across the wide range of scenarios encountered in a race. To address this shortcoming, we propose using a Reinforcement Learning (RL) agent as a high-level behavioral selector that dynamically switches the cost function parameters of an analytical, low-level trajectory planner during runtime. We show the effectiveness of our approach in simulation in an autonomous racing environment where our RL-based planner achieved 0% collision rate while reducing overtaking time by up to 60% compared to state-of-the-art static planners. Our new agent now dynamically switches between aggressive and conservative behaviors, enabling interactive maneuvers unattainable with static configurations. These results demonstrate that integrating reinforcement learning as a high-level selector resolves the inherent trade-off between safety and competitiveness in autonomous racing planners. The proposed methodology offers a pathway toward adaptive yet interpretable motion planning for broader autonomous driving applications.

