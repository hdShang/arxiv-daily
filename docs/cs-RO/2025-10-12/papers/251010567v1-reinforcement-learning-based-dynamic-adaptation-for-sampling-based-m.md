---
layout: default
title: Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving
---

# Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.10567" target="_blank" class="toolbar-btn">arXiv: 2510.10567v1</a>
    <a href="https://arxiv.org/pdf/2510.10567.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10567v1" 
            onclick="toggleFavorite(this, '2510.10567v1', 'Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Alexander Langmann, Yevhenii Tokarev, Mattia Piccinini, Korbinian Moller, Johannes Betz

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-12

**Â§áÊ≥®**: 8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÂä®ÊÄÅËá™ÈÄÇÂ∫îÈááÊ†∑ËøêÂä®ËßÑÂàíÔºåÁî®‰∫éÊïèÊç∑Ëá™‰∏ªÈ©æÈ©∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `ËøêÂä®ËßÑÂàí` `Ëá™‰∏ªÈ©æÈ©∂` `ÈááÊ†∑ÁÆóÊ≥ï` `Âä®ÊÄÅËá™ÈÄÇÂ∫î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ÈááÊ†∑ËΩ®ËøπËßÑÂàíÂô®Âú®ÊïèÊç∑Ëá™‰∏ªÈ©æÈ©∂‰∏≠Â∫îÁî®ÂπøÊ≥õÔºå‰ΩÜÂÖ∂ÊÄßËÉΩÂèóÈôê‰∫éÊâãÂä®Ë∞ÉÊï¥ÁöÑÈùôÊÄÅ‰ª£‰ª∑ÂáΩÊï∞ÊùÉÈáçÔºåÂØºËá¥Á≠ñÁï•Â¶•Âçè„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰ΩìÂä®ÊÄÅÂàáÊç¢Â∫ïÂ±ÇËΩ®ËøπËßÑÂàíÂô®ÁöÑ‰ª£‰ª∑ÂáΩÊï∞ÂèÇÊï∞ÔºåÂÆûÁé∞È´òÂ±ÇË°å‰∏∫ÈÄâÊã©ÂíåËá™ÈÄÇÂ∫îËßÑÂàí„ÄÇ
3. ‰ªøÁúüÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ëá™‰∏ªËµõËΩ¶‰∏≠ÂÆûÁé∞‰∫ÜÊõ¥‰ΩéÁöÑÁ¢∞ÊíûÁéáÂíåÊõ¥Âø´ÁöÑË∂ÖËΩ¶ÈÄüÂ∫¶ÔºåÊèêÂçá‰∫ÜÂÆâÂÖ®ÊÄßÂíåÁ´û‰∫âÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁöÑÂä®ÊÄÅËá™ÈÄÇÂ∫îÊñπÊ≥ïÔºåÁî®‰∫éÊïèÊç∑Ëá™‰∏ªÈ©æÈ©∂‰∏≠ÁöÑÈááÊ†∑ËΩ®ËøπËßÑÂàí„ÄÇËØ•ÊñπÊ≥ï‰ΩøÁî®RLÊô∫ËÉΩ‰Ωì‰Ωú‰∏∫È´òÂ±ÇË°å‰∏∫ÈÄâÊã©Âô®ÔºåÂú®ËøêË°åÊó∂Âä®ÊÄÅÂàáÊç¢Â∫ïÂ±ÇËß£ÊûêËΩ®ËøπËßÑÂàíÂô®ÁöÑ‰ª£‰ª∑ÂáΩÊï∞ÂèÇÊï∞„ÄÇÂú®Ëá™‰∏ªËµõËΩ¶ÁéØÂ¢ÉÁöÑ‰ªøÁúüÂÆûÈ™å‰∏≠Ôºå‰∏éÊúÄÂÖàËøõÁöÑÈùôÊÄÅËßÑÂàíÂô®Áõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂÆûÁé∞‰∫Ü0%ÁöÑÁ¢∞ÊíûÁéáÔºåÂπ∂Â∞ÜË∂ÖËΩ¶Êó∂Èó¥Áº©Áü≠‰∫ÜÈ´òËææ60%„ÄÇËØ•Êô∫ËÉΩ‰ΩìËÉΩÂ§üÂä®ÊÄÅÂàáÊç¢ÊøÄËøõÂíå‰øùÂÆàË°å‰∏∫ÔºåÂÆûÁé∞ÈùôÊÄÅÈÖçÁΩÆÊó†Ê≥ïÂÆûÁé∞ÁöÑ‰∫§‰∫íÂºèÊìç‰Ωú„ÄÇÁªìÊûúË°®ÊòéÔºåÈõÜÊàêÂº∫ÂåñÂ≠¶‰π†‰Ωú‰∏∫È´òÂ±ÇÈÄâÊã©Âô®ËÉΩÂ§üËß£ÂÜ≥Ëá™‰∏ªËµõËΩ¶ËßÑÂàíÂô®‰∏≠ÂÆâÂÖ®ÊÄßÂíåÁ´û‰∫âÊÄß‰πãÈó¥ÁöÑÂõ∫ÊúâÊùÉË°°„ÄÇËØ•ÊñπÊ≥ï‰∏∫Êõ¥ÂπøÊ≥õÁöÑËá™‰∏ªÈ©æÈ©∂Â∫îÁî®Êèê‰æõ‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫î‰∏îÂèØËß£ÈáäÁöÑËøêÂä®ËßÑÂàíÈÄîÂæÑ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÈááÊ†∑ËΩ®ËøπËßÑÂàíÂô®Âú®ÊïèÊç∑Ëá™‰∏ªÈ©æÈ©∂‰∏≠Èù¢‰∏¥ÁùÄ‰ª£‰ª∑ÂáΩÊï∞ÊùÉÈáçÈùôÊÄÅÁöÑÈóÆÈ¢ò„ÄÇËøô‰∫õÊùÉÈáçÈÄöÂ∏∏ÈúÄË¶ÅÊâãÂä®Ë∞ÉÊï¥ÔºåÂπ∂‰∏îÂú®‰∏çÂêåÁöÑÂú∫ÊôØ‰∏ãÊó†Ê≥ïËææÂà∞ÊúÄ‰ºòÔºåÂØºËá¥ÂÆâÂÖ®ÊÄßÂíåÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°„ÄÇÈùôÊÄÅÁöÑ‰ª£‰ª∑ÂáΩÊï∞Êó†Ê≥ïÊ†πÊçÆÁéØÂ¢ÉÂèòÂåñÂä®ÊÄÅË∞ÉÊï¥ËßÑÂàíÁ≠ñÁï•ÔºåÈôêÂà∂‰∫ÜËá™‰∏ªÈ©æÈ©∂ËΩ¶ËæÜÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊô∫ËÉΩ‰Ωì‰Ωú‰∏∫È´òÂ±ÇÂÜ≥Á≠ñÂô®ÔºåÂä®ÊÄÅÂú∞Ë∞ÉÊï¥Â∫ïÂ±ÇËΩ®ËøπËßÑÂàíÂô®ÁöÑ‰ª£‰ª∑ÂáΩÊï∞ÂèÇÊï∞„ÄÇÈÄöËøáÂ≠¶‰π†‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÊúÄ‰ºò‰ª£‰ª∑ÂáΩÊï∞ÈÖçÁΩÆÔºåRLÊô∫ËÉΩ‰ΩìËÉΩÂ§üÊ†πÊçÆÂΩìÂâçÁéØÂ¢ÉÈÄâÊã©ÂêàÈÄÇÁöÑË°å‰∏∫Á≠ñÁï•Ôºå‰ªéËÄåÂú®ÂÆâÂÖ®ÊÄßÂíåÊÄßËÉΩ‰πãÈó¥ÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇËøôÁßçÂä®ÊÄÅË∞ÉÊï¥ÁöÑÁ≠ñÁï•‰ΩøÂæóËΩ¶ËæÜËÉΩÂ§üÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÈááÂèñÊøÄËøõÊàñ‰øùÂÆàÁöÑÈ©æÈ©∂È£éÊ†º„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•ÊñπÊ≥ïÁöÑÊäÄÊúØÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂ∫ïÂ±ÇËΩ®ËøπËßÑÂàíÂô®ÂíåÈ´òÂ±ÇRLÊô∫ËÉΩ‰Ωì„ÄÇÂ∫ïÂ±ÇËΩ®ËøπËßÑÂàíÂô®Ë¥üË¥£ÁîüÊàêÊª°Ë∂≥ËΩ¶ËæÜËøêÂä®Â≠¶ÂíåÂä®ÂäõÂ≠¶Á∫¶ÊùüÁöÑËΩ®ËøπÔºåÂπ∂Ê†πÊçÆÁªôÂÆöÁöÑ‰ª£‰ª∑ÂáΩÊï∞ËØÑ‰º∞ËΩ®ËøπÁöÑ‰ºòÂä£„ÄÇÈ´òÂ±ÇRLÊô∫ËÉΩ‰ΩìÂàôÊ†πÊçÆÂΩìÂâçÁéØÂ¢ÉÁä∂ÊÄÅÔºåÈÄâÊã©‰∏ÄÁªÑ‰ª£‰ª∑ÂáΩÊï∞ÂèÇÊï∞Ôºå‰º†ÈÄíÁªôÂ∫ïÂ±ÇËΩ®ËøπËßÑÂàíÂô®„ÄÇÊï¥‰∏™Á≥ªÁªüÈÄöËøá‰∏çÊñ≠Ëø≠‰ª£ÔºåRLÊô∫ËÉΩ‰ΩìÂ≠¶‰π†Âà∞ÊúÄ‰ºòÁöÑ‰ª£‰ª∑ÂáΩÊï∞ÈÄâÊã©Á≠ñÁï•„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÂº∫ÂåñÂ≠¶‰π†‰∏é‰º†ÁªüÁöÑÈááÊ†∑ËΩ®ËøπËßÑÂàíÂô®Áõ∏ÁªìÂêàÔºåÂÆûÁé∞‰∫Ü‰ª£‰ª∑ÂáΩÊï∞ÁöÑÂä®ÊÄÅËá™ÈÄÇÂ∫îË∞ÉÊï¥„ÄÇ‰∏é‰º†ÁªüÁöÑÈùôÊÄÅ‰ª£‰ª∑ÂáΩÊï∞ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊ†πÊçÆÁéØÂ¢ÉÂèòÂåñÂä®ÊÄÅË∞ÉÊï¥ËßÑÂàíÁ≠ñÁï•Ôºå‰ªéËÄåÂú®ÂÆâÂÖ®ÊÄßÂíåÊÄßËÉΩ‰πãÈó¥ÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇËøôÁßçÂä®ÊÄÅË∞ÉÊï¥ÁöÑÁ≠ñÁï•‰ΩøÂæóËΩ¶ËæÜËÉΩÂ§üÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÈááÂèñÊøÄËøõÊàñ‰øùÂÆàÁöÑÈ©æÈ©∂È£éÊ†ºÔºåÊèêÈ´ò‰∫ÜËΩ¶ËæÜÁöÑÈÄÇÂ∫îÊÄßÂíåÁÅµÊ¥ªÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöRLÊô∫ËÉΩ‰Ωì‰ΩøÁî®Ê∑±Â∫¶QÁΩëÁªúÔºàDQNÔºâËøõË°åËÆ≠ÁªÉÔºåÁä∂ÊÄÅÁ©∫Èó¥ÂåÖÊã¨ËΩ¶ËæÜÁöÑÈÄüÂ∫¶„ÄÅ‰ΩçÁΩÆ„ÄÅ‰∏éËµõÈÅì‰∏≠ÂøÉÁöÑË∑ùÁ¶ªÁ≠â‰ø°ÊÅØÔºåÂä®‰ΩúÁ©∫Èó¥‰∏∫Á¶ªÊï£ÁöÑ‰ª£‰ª∑ÂáΩÊï∞ÂèÇÊï∞ÁªÑÂêà„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ËÄÉËôë‰∫ÜÂÆâÂÖ®ÊÄßÔºàÈÅøÂÖçÁ¢∞ÊíûÔºâ„ÄÅÈÄüÂ∫¶ÂíåËΩ®ËøπÂπ≥ÊªëÊÄß„ÄÇÂ∫ïÂ±ÇËΩ®ËøπËßÑÂàíÂô®ÈááÁî®Ëß£ÊûêÊñπÊ≥ïÔºåËÉΩÂ§üÂø´ÈÄüÁîüÊàêÊª°Ë∂≥ËΩ¶ËæÜËøêÂä®Â≠¶ÂíåÂä®ÂäõÂ≠¶Á∫¶ÊùüÁöÑËΩ®Ëøπ„ÄÇÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÂ•ñÂä±ÂáΩÊï∞ÂíåÁä∂ÊÄÅÁ©∫Èó¥ÔºåRLÊô∫ËÉΩ‰ΩìËÉΩÂ§üÂ≠¶‰π†Âà∞ÊúÄ‰ºòÁöÑ‰ª£‰ª∑ÂáΩÊï∞ÈÄâÊã©Á≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®Ëá™‰∏ªËµõËΩ¶‰ªøÁúüÁéØÂ¢É‰∏≠ÔºåËØ•ÊñπÊ≥ï‰∏éÊúÄÂÖàËøõÁöÑÈùôÊÄÅËßÑÂàíÂô®Áõ∏ÊØîÔºåÂÆûÁé∞‰∫Ü0%ÁöÑÁ¢∞ÊíûÁéáÔºåÂπ∂Â∞ÜË∂ÖËΩ¶Êó∂Èó¥Áº©Áü≠‰∫ÜÈ´òËææ60%„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂú∞ÊèêÈ´òËá™‰∏ªÈ©æÈ©∂ËΩ¶ËæÜÁöÑÂÆâÂÖ®ÊÄßÂíåÁ´û‰∫âÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòËÉΩÂ§üÂÆûÁé∞ÈùôÊÄÅÈÖçÁΩÆÊó†Ê≥ïÂÆûÁé∞ÁöÑ‰∫§‰∫íÂºèÊìç‰ΩúÔºå‰æãÂ¶ÇÂä®ÊÄÅÂàáÊç¢ÊøÄËøõÂíå‰øùÂÆàË°å‰∏∫„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçËá™‰∏ªÈ©æÈ©∂Âú∫ÊôØÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂Âá∫ÁßüËΩ¶„ÄÅÁâ©ÊµÅÈÖçÈÄÅËΩ¶ËæÜÂíåÊó†‰∫∫Ê∏ÖÊâ´ËΩ¶Á≠â„ÄÇÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥ËøêÂä®ËßÑÂàíÁ≠ñÁï•ÔºåËΩ¶ËæÜËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÁöÑ‰∫§ÈÄöÁéØÂ¢ÉÔºåÊèêÈ´òË°åÈ©∂ÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÂÖ∂‰ªñÊú∫Âô®‰∫∫È¢ÜÂüüÔºå‰æãÂ¶ÇÊó†‰∫∫Êú∫ÂíåÊ∞¥‰∏ãÊú∫Âô®‰∫∫ÔºåÂÆûÁé∞Êõ¥Âä†Êô∫ËÉΩÂíåÁÅµÊ¥ªÁöÑËøêÂä®ÊéßÂà∂„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Sampling-based trajectory planners are widely used for agile autonomous driving due to their ability to generate fast, smooth, and kinodynamically feasible trajectories. However, their behavior is often governed by a cost function with manually tuned, static weights, which forces a tactical compromise that is suboptimal across the wide range of scenarios encountered in a race. To address this shortcoming, we propose using a Reinforcement Learning (RL) agent as a high-level behavioral selector that dynamically switches the cost function parameters of an analytical, low-level trajectory planner during runtime. We show the effectiveness of our approach in simulation in an autonomous racing environment where our RL-based planner achieved 0% collision rate while reducing overtaking time by up to 60% compared to state-of-the-art static planners. Our new agent now dynamically switches between aggressive and conservative behaviors, enabling interactive maneuvers unattainable with static configurations. These results demonstrate that integrating reinforcement learning as a high-level selector resolves the inherent trade-off between safety and competitiveness in autonomous racing planners. The proposed methodology offers a pathway toward adaptive yet interpretable motion planning for broader autonomous driving applications.

