---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-12-03
---

# cs.ROï¼ˆ2025-12-03ï¼‰

ğŸ“Š å…± **23** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (3 ğŸ”—1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/251203774v1-safety-reinforced-model-predictive-control-srmpc-improving-mpc-with-.html">Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving</a></td>
  <td>æå‡ºå®‰å…¨å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„æ¨¡å‹é¢„æµ‹æ§åˆ¶(SRMPC)ï¼Œæå‡è‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’çš„å®‰å…¨æ€§ä¸æ€§èƒ½ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03774v1" onclick="toggleFavorite(this, '2512.03774v1', 'Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/251203707v1-contactrl-safe-reinforcement-learning-based-motion-planning-for-cont.html">ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration</a></td>
  <td>ContactRLï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å®‰å…¨è¿åŠ¨è§„åˆ’ï¼Œç”¨äºäººæœºåä½œä¸­çš„æ¥è§¦ä»»åŠ¡</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03707v1" onclick="toggleFavorite(this, '2512.03707v1', 'ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/251203743v1-cross-embodied-co-design-for-dexterous-hands.html">Cross-embodied Co-design for Dexterous Hands</a></td>
  <td>æå‡ºä¸€ç§è·¨å…·èº«ååŒè®¾è®¡æ¡†æ¶ï¼Œç”¨äºçµå·§æ‰‹å½¢æ€ä¸æ§åˆ¶ç­–ç•¥çš„è”åˆä¼˜åŒ–</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03743v1" onclick="toggleFavorite(this, '2512.03743v1', 'Cross-embodied Co-design for Dexterous Hands')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/251203538v1-adapower-specializing-world-foundation-models-for-predictive-manipul.html">AdaPower: Specializing World Foundation Models for Predictive Manipulation</a></td>
  <td>AdaPowerï¼šé€šè¿‡è‡ªé€‚åº”ä¸–ç•Œæ¨¡å‹æå‡é¢„æµ‹æ€§æ“ä½œçš„æ€§èƒ½</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">model predictive control</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03538v1" onclick="toggleFavorite(this, '2512.03538v1', 'AdaPower: Specializing World Foundation Models for Predictive Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/251203736v1-crossing-the-sim2real-gap-between-simulation-and-ground-testing-to-s.html">Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control</a></td>
  <td>é¦–æ¬¡åœ¨å›½é™…ç©ºé—´ç«™éªŒè¯åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªç”±é£è¡Œæœºå™¨äººè‡ªä¸»æ§åˆ¶</td>
  <td class="tags-cell"><span class="paper-tag">sim2real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">curriculum learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03736v1" onclick="toggleFavorite(this, '2512.03736v1', 'Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/251203874v1-omnidexvlg-learning-dexterous-grasp-generation-from-vision-language-.html">OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance</a></td>
  <td>OmniDexVLGï¼šæå‡ºåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹å¼•å¯¼çš„çµå·§æŠ“å–ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°è¯­ä¹‰å¯æ§çš„æŠ“å–åˆæˆã€‚</td>
  <td class="tags-cell"><span class="paper-tag">grasping</span> <span class="paper-tag">grasp</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03874v1" onclick="toggleFavorite(this, '2512.03874v1', 'OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/251203772v1-bayesian-optimization-for-automatic-tuning-of-torque-level-nonlinear.html">Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control</a></td>
  <td>æå‡ºåŸºäºè´å¶æ–¯ä¼˜åŒ–çš„åŠ›çŸ©çº§éçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶è‡ªåŠ¨è°ƒå‚æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03772v1" onclick="toggleFavorite(this, '2512.03772v1', 'Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/251203756v1-prediction-driven-motion-planning-route-integration-strategies-in-at.html">Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models</a></td>
  <td>æå‡ºåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„é¢„æµ‹æ¨¡å‹ï¼Œèåˆå¯¼èˆªä¿¡æ¯ä»¥æå‡è‡ªåŠ¨é©¾é©¶è½¦è¾†äº¤äº’èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">navigation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03756v1" onclick="toggleFavorite(this, '2512.03756v1', 'Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/251204308v1-responsiblerobotbench-benchmarking-responsible-robot-manipulation-us.html">ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models</a></td>
  <td>æå‡ºResponsibleRobotBenchï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°è´Ÿè´£ä»»çš„æœºå™¨äººæ“ä½œã€‚</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.04308v1" onclick="toggleFavorite(this, '2512.04308v1', 'ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/251203795v1-mpcformer-a-physics-informed-data-driven-approach-for-explainable-so.html">MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving</a></td>
  <td>MPCFormerï¼šåŸºäºç‰©ç†ä¿¡æ¯ä¸æ•°æ®é©±åŠ¨çš„å¯è§£é‡Šç¤¾ä¼šæ„ŸçŸ¥è‡ªåŠ¨é©¾é©¶æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">social interaction</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03795v1" onclick="toggleFavorite(this, '2512.03795v1', 'MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/251203684v1-a-novel-approach-to-tomato-harvesting-using-a-hybrid-gripper-with-se.html">A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection</a></td>
  <td>æå‡ºä¸€ç§åŸºäºæ··åˆå¤¹çˆªçš„ç•ªèŒ„é‡‡æ‘˜ç³»ç»Ÿï¼Œç»“åˆè¯­ä¹‰åˆ†å‰²ä¸å…³é”®ç‚¹æ£€æµ‹å®ç°ç²¾å‡†é‡‡æ‘˜ã€‚</td>
  <td class="tags-cell"><span class="paper-tag">grasping</span> <span class="paper-tag">grasp</span> <span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03684v1" onclick="toggleFavorite(this, '2512.03684v1', 'A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/251203913v1-hierarchical-vision-language-action-model-using-success-and-failure-.html">Hierarchical Vision Language Action Model Using Success and Failure Demonstrations</a></td>
  <td>æå‡ºVINEæ¨¡å‹ï¼Œåˆ©ç”¨æˆåŠŸä¸å¤±è´¥æ¼”ç¤ºæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é²æ£’æ€§</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03913v1" onclick="toggleFavorite(this, '2512.03913v1', 'Hierarchical Vision Language Action Model Using Success and Failure Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/251204231v1-craft-e-a-neuro-symbolic-framework-for-embodied-affordance-grounding.html">CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding</a></td>
  <td>CRAFT-Eï¼šç”¨äºå…·èº«å¯ä¾›æ€§æ¥åœ°çš„ç¥ç»ç¬¦å·æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">grasp</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.04231v1" onclick="toggleFavorite(this, '2512.04231v1', 'CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/251203995v1-artificial-microsaccade-compensation-stable-vision-for-an-ornithopte.html">Artificial Microsaccade Compensation: Stable Vision for an Ornithopter</a></td>
  <td>æå‡ºäººå·¥å¾®çœ¼è·³è¡¥å¿æ–¹æ³•ï¼Œç¨³å®šæ‰‘ç¿¼é£è¡Œå™¨å‰§çƒˆæŠ–åŠ¨ä¸‹çš„è§†é¢‘</td>
  <td class="tags-cell"><span class="paper-tag">running</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03995v1" onclick="toggleFavorite(this, '2512.03995v1', 'Artificial Microsaccade Compensation: Stable Vision for an Ornithopter')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/251203630v1-multimodal-control-of-manipulators-coupling-kinematics-and-vision-fo.html">Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations</a></td>
  <td>é’ˆå¯¹å†—ä½™æœºæ¢°è‡‚ï¼Œæå‡ºåŸºäºé›…å¯æ¯”çŸ©é˜µçš„è¿åŠ¨è§„åˆ’æ–¹æ¡ˆï¼Œç”¨äºè‡ªåŠ¨åŒ–å®éªŒå®¤æ“ä½œã€‚</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03630v1" onclick="toggleFavorite(this, '2512.03630v1', 'Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/251203891v1-digital-twin-based-control-co-design-of-full-vehicle-active-suspensi.html">Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning</a></td>
  <td>æå‡ºåŸºäºæ•°å­—å­ªç”Ÿå’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å…¨è½¦ä¸»åŠ¨æ‚¬æ¶æ§åˆ¶ååŒè®¾è®¡æ¡†æ¶</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03891v1" onclick="toggleFavorite(this, '2512.03891v1', 'Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>17</td>
  <td><a href="./papers/251203556v1-roboscape-r-unified-reward-observation-world-models-for-generalizabl.html">RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL</a></td>
  <td>RoboScape-Rï¼šé€šè¿‡ç»Ÿä¸€å¥–åŠ±-è§‚æµ‹ä¸–ç•Œæ¨¡å‹æå‡æœºå™¨äººå¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">imitation learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03556v1" onclick="toggleFavorite(this, '2512.03556v1', 'RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>18</td>
  <td><a href="./papers/251203729v1-autonomous-planning-in-space-assembly-reinforcement-learning-free-fl.html">Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing</a></td>
  <td>APIARYå®éªŒï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å›½é™…ç©ºé—´ç«™Astrobeeæœºå™¨äººè‡ªä¸»è£…é…</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">PPO</span> <span class="paper-tag">actor-critic</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03729v1" onclick="toggleFavorite(this, '2512.03729v1', 'Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>19</td>
  <td><a href="./papers/251204279v1-driving-beyond-privilege-distilling-dense-reward-knowledge-into-spar.html">Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies</a></td>
  <td>æå‡ºå¥–åŠ±ç‰¹æƒä¸–ç•Œæ¨¡å‹è’¸é¦ï¼Œè§£å†³è‡ªåŠ¨é©¾é©¶ä¸­ç¨ å¯†å¥–åŠ±æ³›åŒ–æ€§å·®çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">world model</span> <span class="paper-tag">latent dynamics</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.04279v1" onclick="toggleFavorite(this, '2512.04279v1', 'Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>20</td>
  <td><a href="./papers/251203911v1-autonomous-reinforcement-learning-robot-control-with-intels-loihi-2-.html">Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware</a></td>
  <td>æå‡ºåŸºäºLoihi 2ç¥ç»å½¢æ€ç¡¬ä»¶çš„è‡ªä¸»å¼ºåŒ–å­¦ä¹ æœºå™¨äººæ§åˆ¶æ–¹æ¡ˆ</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03911v1" onclick="toggleFavorite(this, '2512.03911v1', 'Autonomous Reinforcement Learning Robot Control with Intel&#39;s Loihi 2 Neuromorphic Hardware')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥-perception-slam">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM) (3 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>21</td>
  <td><a href="./papers/251203958v2-mde-agrivln-agricultural-vision-and-language-navigation-with-monocul.html">MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation</a></td>
  <td>MDE-AgriVLNï¼šæå‡ºå•ç›®æ·±åº¦ä¼°è®¡çš„å†œä¸šè§†è§‰è¯­è¨€å¯¼èˆªæ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">depth estimation</span> <span class="paper-tag">monocular depth</span> <span class="paper-tag">navigation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03958v2" onclick="toggleFavorite(this, '2512.03958v2', 'MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>22</td>
  <td><a href="./papers/251203522v2-msg-loc-multi-label-likelihood-based-semantic-graph-matching-for-obj.html">MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization</a></td>
  <td>æå‡ºåŸºäºå¤šæ ‡ç­¾ä¼¼ç„¶è¯­ä¹‰å›¾åŒ¹é…çš„ç‰©ä½“çº§å…¨å±€å®šä½æ–¹æ³•</td>
  <td class="tags-cell"><span class="paper-tag">pose estimation</span> <span class="paper-tag">localization</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.03522v2" onclick="toggleFavorite(this, '2512.03522v2', 'MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>23</td>
  <td><a href="./papers/251206017v1-training-free-robot-pose-estimation-using-off-the-shelf-foundational.html">Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models</a></td>
  <td>åˆ©ç”¨ç°æˆè§†è§‰-è¯­è¨€æ¨¡å‹å®ç°å…è®­ç»ƒæœºå™¨äººå§¿æ€ä¼°è®¡</td>
  <td class="tags-cell"><span class="paper-tag">pose estimation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2512.06017v1" onclick="toggleFavorite(this, '2512.06017v1', 'Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)