---
layout: default
title: CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding
---

# CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.04231" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.04231v1</a>
  <a href="https://arxiv.org/pdf/2512.04231.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.04231v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.04231v1', 'CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhou Chen, Joe Lin, Carson Bulgin, Sathyanarayanan N. Aakur

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

**å¤‡æ³¨**: 20 pages. 3 figures, 4 tables. Under Review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**CRAFT-Eï¼šç”¨äºå…·èº«å¯ä¾›æ€§æ¥åœ°çš„ç¥ç»ç¬¦å·æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `å¯ä¾›æ€§` `ç¥ç»ç¬¦å·` `çŸ¥è¯†å›¾è°±` `æœºå™¨äºº` `è§†è§‰è¯­è¨€å¯¹é½` `æŠ“å–æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–é»‘ç›’æ¨¡å‹æˆ–å›ºå®šæ ‡ç­¾ï¼Œç¼ºä¹é€æ˜æ€§å’Œå¯æ§æ€§ï¼Œéš¾ä»¥æ»¡è¶³äººæœºäº¤äº’åº”ç”¨çš„éœ€æ±‚ã€‚
2. CRAFT-Eç»“åˆçŸ¥è¯†å›¾è°±ã€è§†è§‰è¯­è¨€å¯¹é½å’Œèƒ½é‡æ¨¡å‹ï¼Œç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†è·¯å¾„ï¼Œå¹¶è€ƒè™‘æŠ“å–å¯è¡Œæ€§ã€‚
3. CRAFT-Eåœ¨é™æ€åœºæ™¯å’ŒçœŸå®æœºå™¨äººå®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨æ„ŸçŸ¥å™ªå£°ä¸‹ä¿æŒé²æ£’æ€§ï¼Œæä¾›ç»„ä»¶çº§è¯Šæ–­ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­è¿è¡Œçš„è¾…åŠ©æœºå™¨äººä¸ä»…éœ€è¦ç†è§£ç‰©ä½“æ˜¯ä»€ä¹ˆï¼Œè¿˜éœ€è¦ç†è§£å®ƒä»¬å¯ä»¥ç”¨æ¥åšä»€ä¹ˆã€‚è¿™éœ€è¦å°†åŸºäºè¯­è¨€çš„åŠ¨ä½œæŸ¥è¯¢ä¸æ—¢èƒ½æä¾›æ‰€éœ€åŠŸèƒ½åˆèƒ½è¢«ç‰©ç†æ£€ç´¢çš„ç‰©ä½“è¿›è¡Œå…³è”ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé»‘ç›’æ¨¡å‹æˆ–å›ºå®šçš„å¯ä¾›æ€§æ ‡ç­¾ï¼Œé™åˆ¶äº†é¢å‘äººç±»åº”ç”¨çš„é€æ˜æ€§ã€å¯æ§æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†CRAFT-Eï¼Œä¸€ä¸ªæ¨¡å—åŒ–çš„ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œå®ƒå°†ç»“æ„åŒ–çš„åŠ¨è¯-å±æ€§-å¯¹è±¡çŸ¥è¯†å›¾ä¸è§†è§‰-è¯­è¨€å¯¹é½å’ŒåŸºäºèƒ½é‡çš„æŠ“å–æ¨ç†ç›¸ç»“åˆã€‚è¯¥ç³»ç»Ÿç”Ÿæˆå¯è§£é‡Šçš„æ¥åœ°è·¯å¾„ï¼Œæ­ç¤ºå½±å“ç‰©ä½“é€‰æ‹©çš„å› ç´ ï¼Œå¹¶å°†æŠ“å–å¯è¡Œæ€§ä½œä¸ºå¯ä¾›æ€§æ¨ç†çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åŠ¨è¯-å¯¹è±¡å…¼å®¹æ€§ã€åˆ†å‰²å’ŒæŠ“å–å€™é€‰çš„ç»Ÿä¸€æ³¨é‡Šï¼Œå¹¶åœ¨ç‰©ç†æœºå™¨äººä¸Šéƒ¨ç½²äº†å®Œæ•´çš„pipelineã€‚CRAFT-Eåœ¨é™æ€åœºæ™¯ã€åŸºäºImageNetçš„åŠŸèƒ½æ£€ç´¢ä»¥åŠæ¶‰åŠ20ä¸ªåŠ¨è¯å’Œ39ä¸ªç‰©ä½“çš„çœŸå®ä¸–ç•Œè¯•éªŒä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨æ„ŸçŸ¥å™ªå£°ä¸‹ä¿æŒç¨³å¥ï¼Œå¹¶æä¾›é€æ˜çš„ç»„ä»¶çº§è¯Šæ–­ã€‚é€šè¿‡å°†ç¬¦å·æ¨ç†ä¸å…·èº«æ„ŸçŸ¥ç›¸ç»“åˆï¼ŒCRAFT-Eä¸ºå¯ä¾›æ€§æ¥åœ°çš„ç‰©ä½“é€‰æ‹©æä¾›äº†ä¸€ç§å¯è§£é‡Šå’Œå¯å®šåˆ¶çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ”¯æŒè¾…åŠ©æœºå™¨äººç³»ç»Ÿä¸­å€¼å¾—ä¿¡èµ–çš„å†³ç­–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è¾…åŠ©æœºå™¨äººå¦‚ä½•åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­ç†è§£ç‰©ä½“çš„åŠŸèƒ½ï¼ˆå¯ä¾›æ€§ï¼‰ï¼Œå¹¶æ ¹æ®è¯­è¨€æŒ‡ä»¤é€‰æ‹©åˆé€‚çš„ç‰©ä½“è¿›è¡Œæ“ä½œçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºä¾èµ–é»‘ç›’æ¨¡å‹æˆ–é¢„å®šä¹‰çš„å¯ä¾›æ€§æ ‡ç­¾ï¼Œç¼ºä¹é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ï¼Œéš¾ä»¥è°ƒè¯•å’Œä¿¡ä»»ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†ç¥ç»æ–¹æ³•å’Œç¬¦å·æ¨ç†ç›¸ç»“åˆï¼Œæ„å»ºä¸€ä¸ªæ¨¡å—åŒ–çš„ç¥ç»ç¬¦å·æ¡†æ¶ã€‚é€šè¿‡çŸ¥è¯†å›¾è°±æ¥è¡¨ç¤ºç‰©ä½“ã€å±æ€§å’ŒåŠ¨ä½œä¹‹é—´çš„å…³ç³»ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å°†è¯­è¨€æŒ‡ä»¤ä¸è§†è§‰ä¿¡æ¯å¯¹é½ï¼Œå¹¶ä½¿ç”¨èƒ½é‡æ¨¡å‹æ¥è¯„ä¼°æŠ“å–çš„å¯è¡Œæ€§ã€‚è¿™æ ·å¯ä»¥ç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†è·¯å¾„ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„é€æ˜æ€§å’Œå¯æ§æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCRAFT-Eæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) çŸ¥è¯†å›¾è°±ï¼šå­˜å‚¨åŠ¨è¯ã€å±æ€§å’Œå¯¹è±¡ä¹‹é—´çš„å…³ç³»ã€‚2) è§†è§‰è¯­è¨€å¯¹é½æ¨¡å—ï¼šå°†è¯­è¨€æŒ‡ä»¤ä¸­çš„åŠ¨è¯å’Œå¯¹è±¡ä¸å›¾åƒä¸­çš„è§†è§‰ä¿¡æ¯å¯¹é½ã€‚3) èƒ½é‡æ¨¡å‹ï¼šè¯„ä¼°æŠ“å–å€™é€‰çš„è´¨é‡å’Œå¯è¡Œæ€§ã€‚4) æ¨ç†å¼•æ“ï¼šæ ¹æ®çŸ¥è¯†å›¾è°±ã€è§†è§‰è¯­è¨€å¯¹é½ç»“æœå’ŒæŠ“å–å¯è¡Œæ€§ï¼Œç”Ÿæˆå¯è§£é‡Šçš„æ¥åœ°è·¯å¾„ï¼Œé€‰æ‹©æœ€ä½³ç‰©ä½“ã€‚

**å…³é”®åˆ›æ–°**ï¼šCRAFT-Eçš„å…³é”®åˆ›æ–°åœ¨äºå°†ç¬¦å·æ¨ç†ä¸å…·èº«æ„ŸçŸ¥ç›¸ç»“åˆï¼Œæ„å»ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„ç¥ç»ç¬¦å·æ¡†æ¶ã€‚ä¸ç«¯åˆ°ç«¯æ¨¡å‹ç›¸æ¯”ï¼ŒCRAFT-Eçš„æ¨ç†è¿‡ç¨‹æ›´åŠ é€æ˜ï¼Œå¯ä»¥è¿›è¡Œç»„ä»¶çº§çš„è¯Šæ–­å’Œè°ƒè¯•ã€‚æ­¤å¤–ï¼ŒCRAFT-Eå°†æŠ“å–å¯è¡Œæ€§ä½œä¸ºå¯ä¾›æ€§æ¨ç†çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ï¼Œæé«˜äº†ç‰©ä½“é€‰æ‹©çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šCRAFT-Eä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ã€‚èƒ½é‡æ¨¡å‹é‡‡ç”¨åŸºäºèƒ½é‡çš„æ¡†æ¶ï¼Œé€šè¿‡å­¦ä¹ èƒ½é‡å‡½æ•°æ¥è¯„ä¼°æŠ“å–å€™é€‰çš„è´¨é‡ã€‚çŸ¥è¯†å›¾è°±é‡‡ç”¨äººå·¥æ„å»ºçš„æ–¹å¼ï¼Œå¹¶æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œæ‰©å±•ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è§†è§‰è¯­è¨€å¯¹é½æŸå¤±å’ŒæŠ“å–èƒ½é‡æŸå¤±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

CRAFT-Eåœ¨é™æ€åœºæ™¯ã€ImageNet-basedåŠŸèƒ½æ£€ç´¢å’ŒçœŸå®ä¸–ç•Œæœºå™¨äººå®éªŒä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRAFT-Eåœ¨æ„ŸçŸ¥å™ªå£°ä¸‹ä¿æŒé²æ£’æ€§ï¼Œå¹¶èƒ½å¤Ÿæä¾›é€æ˜çš„ç»„ä»¶çº§è¯Šæ–­ã€‚åœ¨çœŸå®æœºå™¨äººå®éªŒä¸­ï¼ŒCRAFT-EæˆåŠŸåœ°å®Œæˆäº†æ¶‰åŠ20ä¸ªåŠ¨è¯å’Œ39ä¸ªç‰©ä½“çš„ä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

CRAFT-Eå¯åº”ç”¨äºè¾…åŠ©æœºå™¨äººã€æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£äººç±»çš„æŒ‡ä»¤ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·å’Œç‰©ä½“è¿›è¡Œæ“ä½œï¼Œä»è€Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ„å»ºæ›´å€¼å¾—ä¿¡èµ–ã€æ›´æ˜“äºç†è§£å’Œæ§åˆ¶çš„æœºå™¨äººç³»ç»Ÿï¼Œä¿ƒè¿›äººæœºåä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.

