---
layout: default
title: RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL
---

# RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.03556" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.03556v1</a>
  <a href="https://arxiv.org/pdf/2512.03556.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.03556v1" onclick="toggleFavorite(this, '2512.03556v1', 'RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yinzhou Tang, Yu Shang, Yinuo Chen, Bingwen Wei, Xin Zhang, Shu'ang Yu, Liangzhi Shi, Chao Yu, Chen Gao, Wei Wu, Yong Li

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-03

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboScape-Rï¼šé€šè¿‡ç»Ÿä¸€å¥–åŠ±-è§‚æµ‹ä¸–ç•Œæ¨¡å‹æå‡æœºå™¨äººå¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `æ³›åŒ–èƒ½åŠ›` `å…·èº«æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç¼ºä¹ç»Ÿä¸€çš„é€šç”¨å¥–åŠ±ä¿¡å·ï¼Œéš¾ä»¥åœ¨å¤šåœºæ™¯ä¸­æ³›åŒ–ï¼Œæ¨¡ä»¿å­¦ä¹ åˆ™å®¹æ˜“è¿‡æ‹Ÿåˆä¸“å®¶è½¨è¿¹ã€‚
2. RoboScape-Råˆ©ç”¨ä¸–ç•Œæ¨¡å‹ä½œä¸ºé€šç”¨ç¯å¢ƒä»£ç†ï¼Œé€šè¿‡å†…ç”Ÿå¥–åŠ±æœºåˆ¶ï¼Œæå‡å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRoboScape-Råœ¨è¶…å‡ºé¢†åŸŸåœºæ™¯ä¸‹ï¼Œæ€§èƒ½æ¯”åŸºçº¿å¹³å‡æé«˜äº†37.5%ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®ç°å¯æ³›åŒ–çš„å…·èº«æ™ºèƒ½ç­–ç•¥ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç­–ç•¥å­¦ä¹ èŒƒå¼ï¼ŒåŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œéƒ½éš¾ä»¥åœ¨ä¸åŒçš„åœºæ™¯ä¸­åŸ¹å…»æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡ä»¿å­¦ä¹ ç­–ç•¥é€šå¸¸è¿‡åº¦æ‹Ÿåˆç‰¹å®šçš„ä¸“å®¶è½¨è¿¹ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™ç¼ºä¹ç»Ÿä¸€å’Œé€šç”¨çš„å¥–åŠ±ä¿¡å·ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„å¤šåœºæ™¯æ³›åŒ–è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è®¤ä¸ºä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿä½œä¸ºé€šç”¨çš„ç¯å¢ƒä»£ç†æ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä¸–ç•Œæ¨¡å‹ä¸»è¦å…³æ³¨é¢„æµ‹è§‚æµ‹çš„èƒ½åŠ›ï¼Œä»ç„¶ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ‰‹å·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå› æ­¤æ— æ³•æä¾›çœŸæ­£é€šç”¨çš„è®­ç»ƒç¯å¢ƒã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RoboScape-Rï¼Œä¸€ä¸ªåˆ©ç”¨ä¸–ç•Œæ¨¡å‹ä½œä¸ºå¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸­å…·èº«ç¯å¢ƒçš„é€šç”¨ä»£ç†çš„æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºä¸–ç•Œæ¨¡å‹çš„æ–°å‹é€šç”¨å¥–åŠ±æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ç”Ÿæˆæºäºæ¨¡å‹å¯¹çœŸå®ä¸–ç•ŒçŠ¶æ€è½¬ç§»åŠ¨æ€çš„å†…åœ¨ç†è§£çš„â€œå†…ç”Ÿâ€å¥–åŠ±ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRoboScape-Ré€šè¿‡æä¾›é«˜æ•ˆå’Œé€šç”¨çš„è®­ç»ƒç¯å¢ƒï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†å…·èº«æ™ºèƒ½ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºåˆ©ç”¨ä¸–ç•Œæ¨¡å‹ä½œä¸ºåœ¨çº¿è®­ç»ƒç­–ç•¥æä¾›äº†é‡è¦çš„è§è§£ï¼Œå¹¶ä¸”åœ¨è¶…å‡ºé¢†åŸŸåœºæ™¯ä¸‹ï¼Œæ€§èƒ½æ¯”åŸºçº¿å¹³å‡æé«˜äº†37.5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½ç­–ç•¥åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–é—®é¢˜ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„ã€ç‰¹å®šäºä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æ¨¡ä»¿å­¦ä¹ è™½ç„¶å¯ä»¥å­¦ä¹ ä¸“å®¶ç­–ç•¥ï¼Œä½†å®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸–ç•Œæ¨¡å‹æ¥å­¦ä¹ ç¯å¢ƒçš„åŠ¨æ€ç‰¹æ€§ï¼Œå¹¶ä»ä¸­æå–é€šç”¨çš„å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡è®©æ™ºèƒ½ä½“åœ¨ä¸–ç•Œæ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥é¿å…å¯¹çœŸå®ç¯å¢ƒçš„è¿‡åº¦ä¾èµ–ï¼Œä»è€Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºè®¾è®¡ä¸€ç§èƒ½å¤Ÿåæ˜ ç¯å¢ƒå†…åœ¨è§„å¾‹çš„å†…ç”Ÿå¥–åŠ±æœºåˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoboScape-Ræ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä¸–ç•Œæ¨¡å‹ï¼šç”¨äºå­¦ä¹ ç¯å¢ƒçš„çŠ¶æ€è½¬ç§»åŠ¨æ€ï¼Œèƒ½å¤Ÿé¢„æµ‹æœªæ¥çŠ¶æ€å’Œå¥–åŠ±ã€‚2) å†…ç”Ÿå¥–åŠ±ç”Ÿæˆå™¨ï¼šåŸºäºä¸–ç•Œæ¨¡å‹çš„é¢„æµ‹ï¼Œç”Ÿæˆåæ˜ ç¯å¢ƒå†…åœ¨è§„å¾‹çš„å¥–åŠ±ä¿¡å·ã€‚3) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼šåœ¨ä¸–ç•Œæ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒï¼Œä»¥æœ€å¤§åŒ–å†…ç”Ÿå¥–åŠ±ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œæ™ºèƒ½ä½“åœ¨ä¸–ç•Œæ¨¡å‹ä¸­é‡‡å–è¡ŒåŠ¨ï¼Œä¸–ç•Œæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ï¼Œå†…ç”Ÿå¥–åŠ±ç”Ÿæˆå™¨æ ¹æ®é¢„æµ‹ç»“æœç”Ÿæˆå¥–åŠ±ï¼Œæ™ºèƒ½ä½“æ ¹æ®å¥–åŠ±æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†åŸºäºä¸–ç•Œæ¨¡å‹çš„å†…ç”Ÿå¥–åŠ±æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„æ‰‹å·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°ä¸åŒï¼Œå†…ç”Ÿå¥–åŠ±èƒ½å¤Ÿè‡ªåŠ¨åœ°ä»ç¯å¢ƒåŠ¨æ€ä¸­å­¦ä¹ ï¼Œä»è€Œæä¾›æ›´é€šç”¨å’Œé²æ£’çš„å¥–åŠ±ä¿¡å·ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¯¹ç‰¹å®šä»»åŠ¡çš„è¿‡åº¦ä¾èµ–ï¼Œæé«˜äº†ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šä¸–ç•Œæ¨¡å‹é€šå¸¸é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–Transformerç­‰æ¨¡å‹ç»“æ„ï¼Œç”¨äºå­¦ä¹ ç¯å¢ƒçš„çŠ¶æ€è¡¨ç¤ºå’Œè½¬ç§»å‡½æ•°ã€‚å†…ç”Ÿå¥–åŠ±çš„è®¾è®¡å¯ä»¥åŸºäºå¤šç§æŒ‡æ ‡ï¼Œä¾‹å¦‚çŠ¶æ€çš„å˜åŒ–å¹…åº¦ã€ä¸ç›®æ ‡çš„è·ç¦»ç­‰ã€‚å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨å¸¸è§çš„ç®—æ³•ï¼Œå¦‚PPOæˆ–SACã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboScape-Råœ¨è¶…å‡ºé¢†åŸŸåœºæ™¯ä¸‹ï¼Œæ€§èƒ½æ¯”åŸºçº¿æ–¹æ³•å¹³å‡æé«˜äº†37.5%ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†å†…ç”Ÿå¥–åŠ±æœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½å¤Ÿæä¾›æ›´é€šç”¨å’Œé²æ£’çš„å¥–åŠ±ä¿¡å·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å¯¼èˆªã€æ“ä½œå’Œæ§åˆ¶ã€‚é€šè¿‡æé«˜æœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶åœ¨æ›´å¹¿æ³›çš„å®é™…åœºæ™¯ä¸­éƒ¨ç½²ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œç¾éš¾æ•‘æ´ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ç¯å¢ƒå’Œä»»åŠ¡ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.

