---
layout: default
title: Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance
---

# Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance

**arXiv**: [2512.11173v1](https://arxiv.org/abs/2512.11173) | [PDF](https://arxiv.org/pdf/2512.11173.pdf)

**ä½œè€…**: Tzu-Hsien Lee, Fidan Mahmudova, Karthik Desingh

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

**ðŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://rpm-lab-umn.github.io/category-level-last-meter-nav/)

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽå•å®žä¾‹RGBå›¾åƒæ¨¡ä»¿å­¦ä¹ çš„ç±»åˆ«çº§æœ«ç«¯å¯¼èˆªæ–¹æ³•**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœ«ç«¯å¯¼èˆª` `æ¨¡ä»¿å­¦ä¹ ` `ç±»åˆ«çº§æ³›åŒ–` `ç§»åŠ¨æ“ä½œ` `RGBå›¾åƒ` `æœºå™¨äººå®šä½` `è¯­è¨€é©±åŠ¨åˆ†å‰²`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰åŸºäºŽRGBçš„å¯¼èˆªç³»ç»Ÿç²¾åº¦ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³ç§»åŠ¨æ“ä½œä¸­ç²¾ç¡®å®šä½çš„éœ€æ±‚ï¼Œå¯¼è‡´æ“ä½œç­–ç•¥æ‰§è¡Œå¤±è´¥ã€‚
2. æå‡ºä¸€ç§åŸºäºŽç‰©ä½“ä¸­å¿ƒçš„æ¨¡ä»¿å­¦ä¹ æ¡†æž¶ï¼Œåˆ©ç”¨RGBå›¾åƒã€æ–‡æœ¬æç¤ºå’Œç©ºé—´å¾—åˆ†çŸ©é˜µè§£ç å™¨å®žçŽ°æœ«ç«¯å¯¼èˆªã€‚
3. å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æœªè§è¿‡çš„ç‰©ä½“å®žä¾‹ä¸Šå®žçŽ°äº†è¾ƒé«˜çš„è¾¹ç¼˜å¯¹é½å’Œç‰©ä½“å¯¹é½æˆåŠŸçŽ‡ï¼Œæ— éœ€æ·±åº¦ä¿¡æ¯æˆ–åœ°å›¾å…ˆéªŒã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘æœ«ç«¯å¯¼èˆªçš„ã€ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ¨¡ä»¿å­¦ä¹ æ¡†æž¶ï¼Œæ—¨åœ¨ä½¿å››è¶³ç§»åŠ¨æœºæ¢°è‡‚ä»…ä½¿ç”¨æ¿è½½æ‘„åƒå¤´çš„RGBå›¾åƒè§‚æµ‹ï¼Œå³å¯å®žçŽ°æ“ä½œå°±ç»ªçš„ç²¾ç¡®å®šä½ã€‚è¯¥æ–¹æ³•å°†å¯¼èˆªç­–ç•¥å»ºç«‹åœ¨ä¸‰ä¸ªè¾“å…¥ä¹‹ä¸Šï¼šç›®æ ‡å›¾åƒã€æ¥è‡ªæ¿è½½æ‘„åƒå¤´çš„å¤šè§†è§’RGBè§‚æµ‹ä»¥åŠæŒ‡å®šç›®æ ‡ç‰©ä½“çš„æ–‡æœ¬æç¤ºã€‚ç„¶åŽï¼Œä¸€ä¸ªè¯­è¨€é©±åŠ¨çš„åˆ†å‰²æ¨¡å—å’Œä¸€ä¸ªç©ºé—´å¾—åˆ†çŸ©é˜µè§£ç å™¨æä¾›æ˜¾å¼çš„ç‰©ä½“å®šä½å’Œç›¸å¯¹å§¿æ€æŽ¨ç†ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨æ¥è‡ªç±»åˆ«ä¸­å•ä¸ªç‰©ä½“å®žä¾‹çš„çœŸå®žä¸–ç•Œæ•°æ®ï¼Œæ³›åŒ–åˆ°å…·æœ‰æŒ‘æˆ˜æ€§å…‰ç…§å’ŒèƒŒæ™¯æ¡ä»¶çš„ä¸åŒçŽ¯å¢ƒä¸­æœªè§è¿‡çš„ç‰©ä½“å®žä¾‹ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ï¼Œå¼•å…¥äº†ä¸¤ä¸ªæŒ‡æ ‡ï¼šä½¿ç”¨çœŸå®žæ–¹å‘çš„è¾¹ç¼˜å¯¹é½æŒ‡æ ‡ï¼Œä»¥åŠè¯„ä¼°æœºå™¨äººè§†è§‰ä¸Šä¸Žç›®æ ‡å¯¹é½ç¨‹åº¦çš„ç‰©ä½“å¯¹é½æŒ‡æ ‡ã€‚ç»“æžœè¡¨æ˜Žï¼Œè¯¥ç­–ç•¥åœ¨æœªè§è¿‡çš„ç›®æ ‡ç‰©ä½“å®šä½ä¸­ï¼Œè¾¹ç¼˜å¯¹é½æˆåŠŸçŽ‡ä¸º73.47%ï¼Œç‰©ä½“å¯¹é½æˆåŠŸçŽ‡ä¸º96.94%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³ç§»åŠ¨æœºæ¢°è‡‚æœ«ç«¯å¯¼èˆªçš„ç²¾ç¡®å®šä½é—®é¢˜ã€‚çŽ°æœ‰åŸºäºŽRGBçš„å¯¼èˆªç³»ç»Ÿé€šå¸¸åªèƒ½æä¾›ç±³çº§ç²¾åº¦ï¼Œæ— æ³•æ»¡è¶³åŽç»­æ“ä½œæ‰€éœ€çš„ç²¾ç¡®ä½ç½®ï¼Œå¯¼è‡´æ“ä½œç­–ç•¥æ— æ³•åœ¨è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå†…æ‰§è¡Œï¼Œä»Žè€Œå¯¼è‡´å¤±è´¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ¨¡ä»¿å­¦ä¹ ï¼Œè®©æœºå™¨äººå­¦ä¹ å¦‚ä½•ä»…é€šè¿‡RGBå›¾åƒè§‚æµ‹å’Œæ–‡æœ¬æç¤ºï¼Œå°†è‡ªèº«å®šä½åˆ°ç›®æ ‡ç‰©ä½“é™„è¿‘ï¼Œè¾¾åˆ°æ“ä½œå°±ç»ªçš„çŠ¶æ€ã€‚é€šè¿‡å­¦ä¹ å•ä¸ªç‰©ä½“å®žä¾‹çš„æ•°æ®ï¼Œå®žçŽ°å¯¹æ•´ä¸ªç‰©ä½“ç±»åˆ«çš„æ³›åŒ–ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) æŽ¥æ”¶ç›®æ ‡å›¾åƒã€å¤šè§†è§’RGBè§‚æµ‹å’Œæ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥ï¼›2) ä½¿ç”¨è¯­è¨€é©±åŠ¨çš„åˆ†å‰²æ¨¡å—è¿›è¡Œç‰©ä½“åˆ†å‰²ï¼Œæå–ç›®æ ‡ç‰©ä½“ï¼›3) ä½¿ç”¨ç©ºé—´å¾—åˆ†çŸ©é˜µè§£ç å™¨è¿›è¡Œç›¸å¯¹å§¿æ€æŽ¨ç†ï¼Œä¼°è®¡æœºå™¨äººä¸Žç›®æ ‡ç‰©ä½“ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ï¼›4) æ ¹æ®ä¼°è®¡çš„ç›¸å¯¹ä½ç½®å…³ç³»ï¼ŒæŽ§åˆ¶æœºå™¨äººè¿›è¡Œå¯¼èˆªã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºŽå®žçŽ°äº†ç±»åˆ«çº§åˆ«çš„æœ«ç«¯å¯¼èˆªï¼Œå³ä»…ä½¿ç”¨å•ä¸ªç‰©ä½“å®žä¾‹çš„æ•°æ®ï¼Œå°±èƒ½æ³›åŒ–åˆ°åŒä¸€ç±»åˆ«ä¸‹çš„å…¶ä»–æœªè§è¿‡çš„ç‰©ä½“å®žä¾‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä»…ä¾èµ–RGBå›¾åƒå’Œæ–‡æœ¬æç¤ºï¼Œæ— éœ€æ·±åº¦ä¿¡æ¯ã€æ¿€å…‰é›·è¾¾æˆ–åœ°å›¾å…ˆéªŒï¼Œé™ä½Žäº†ç³»ç»Ÿçš„å¤æ‚æ€§å’Œæˆæœ¬ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) è¯­è¨€é©±åŠ¨çš„åˆ†å‰²æ¨¡å—ï¼Œç”¨äºŽä»ŽRGBå›¾åƒä¸­åˆ†å‰²å‡ºç›®æ ‡ç‰©ä½“ï¼›2) ç©ºé—´å¾—åˆ†çŸ©é˜µè§£ç å™¨ï¼Œç”¨äºŽä¼°è®¡æœºå™¨äººä¸Žç›®æ ‡ç‰©ä½“ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ï¼›3) è¾¹ç¼˜å¯¹é½å’Œç‰©ä½“å¯¹é½ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºŽè¯„ä¼°å¯¼èˆªç­–ç•¥çš„æ€§èƒ½ã€‚æŸå¤±å‡½æ•°æœªçŸ¥ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æœªè§è¿‡çš„ç›®æ ‡ç‰©ä½“å®šä½ä¸­ï¼Œè¾¹ç¼˜å¯¹é½æˆåŠŸçŽ‡ä¸º73.47%ï¼Œç‰©ä½“å¯¹é½æˆåŠŸçŽ‡ä¸º96.94%ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç±»åˆ«çº§åˆ«ä¸Šå®žçŽ°ç²¾ç¡®çš„æœ«ç«¯å¯¼èˆªï¼Œä¸”æ— éœ€æ·±åº¦ä¿¡æ¯ã€æ¿€å…‰é›·è¾¾æˆ–åœ°å›¾å…ˆéªŒã€‚è¯¥æ–¹æ³•ä¸ºç»Ÿä¸€çš„ç§»åŠ¨æ“ä½œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„é€”å¾„ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦ç²¾ç¡®å®šä½çš„ç§»åŠ¨æ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šåœ¨å®¶åº­çŽ¯å¢ƒä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®æŒ‡ä»¤å°†è‡ªèº«å®šä½åˆ°ç‰¹å®šå®¶å…·é™„è¿‘ï¼Œä»¥ä¾¿è¿›è¡Œæ¸…æ´ã€ç»´ä¿®ç­‰æ“ä½œï¼›åœ¨å·¥ä¸šçŽ¯å¢ƒä¸­ï¼Œæœºå™¨äººå¯ä»¥ç²¾ç¡®å®šä½åˆ°ç”Ÿäº§çº¿ä¸Šçš„ç‰¹å®šéƒ¨ä»¶é™„è¿‘ï¼Œä»¥ä¾¿è¿›è¡Œç»„è£…ã€æ£€æµ‹ç­‰æ“ä½œã€‚è¯¥ç ”ç©¶ä¸ºå®žçŽ°é€šç”¨ç§»åŠ¨æ“ä½œæœºå™¨äººå¥ å®šäº†åŸºç¡€ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/

