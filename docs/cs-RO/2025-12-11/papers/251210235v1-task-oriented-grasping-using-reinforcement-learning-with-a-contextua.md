---
layout: default
title: Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine
---

# Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine

**arXiv**: [2512.10235v1](https://arxiv.org/abs/2512.10235) | [PDF](https://arxiv.org/pdf/2512.10235.pdf)

**ä½œè€…**: Hui Li, Akhlak Uz Zaman, Fujian Yan, Hongsheng He

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºŽä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ æ¡†æž¶ä»¥è§£å†³ä»»åŠ¡å¯¼å‘æŠ“å–é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶` `ä»»åŠ¡å¯¼å‘æŠ“å–` `æœºå™¨äººæŠ€æœ¯` `å­¦ä¹ æ•ˆçŽ‡` `çŠ¶æ€æŠ½è±¡` `è¿‡æ¸¡å¥–åŠ±`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰çš„æŠ“å–æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å¸¸å¸¸é¢ä¸´ä»»åŠ¡å¤æ‚æ€§é«˜ã€å­¦ä¹ æ•ˆçŽ‡ä½Žçš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æž¶é€šè¿‡ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶å°†æŠ“å–ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œä»Žè€Œç®€åŒ–å­¦ä¹ è¿‡ç¨‹å¹¶æé«˜æ•ˆçŽ‡ã€‚
3. å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­æˆåŠŸçŽ‡è¾¾åˆ°95%ï¼Œåœ¨çœŸå®žæœºå™¨äººä¸ŠæˆåŠŸçŽ‡ä¸º83.3%ï¼Œæ˜¾è‘—ä¼˜äºŽçŽ°æœ‰æ–¹æ³•ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ æ¡†æž¶ï¼Œç”¨äºŽä»»åŠ¡å¯¼å‘çš„æŠ“å–ã€‚ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶é€šè¿‡å°†æŠ“å–ä»»åŠ¡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œé™ä½Žäº†ä»»åŠ¡å¤æ‚æ€§ã€‚æ¯ä¸ªå­ä»»åŠ¡éƒ½ä¸Žç‰¹å®šé˜¶æ®µçš„ä¸Šä¸‹æ–‡ç›¸å…³è”ï¼ŒåŒ…æ‹¬å¥–åŠ±å‡½æ•°ã€åŠ¨ä½œç©ºé—´å’ŒçŠ¶æ€æŠ½è±¡å‡½æ•°ã€‚è¿™ç§ä¸Šä¸‹æ–‡ä¿¡æ¯èƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼é˜¶æ®µå†…çš„å­¦ä¹ ï¼Œæé«˜å­¦ä¹ æ•ˆçŽ‡ï¼Œå‡å°‘çŠ¶æ€-åŠ¨ä½œç©ºé—´ï¼Œå¹¶åœ¨æ˜Žç¡®çš„è¾¹ç•Œå†…å¼•å¯¼æŽ¢ç´¢ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†è¿‡æ¸¡å¥–åŠ±ä»¥é¼“åŠ±æˆ–æƒ©ç½šé˜¶æ®µé—´çš„è¿‡æ¸¡ï¼Œä»Žè€Œå¼•å¯¼æ¨¡åž‹æœå‘ç†æƒ³çš„é˜¶æ®µåºåˆ—ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚ä¸Žè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ç»“åˆåŽï¼Œè¯¥æ–¹æ³•åœ¨1000ä¸ªæ¨¡æ‹ŸæŠ“å–ä»»åŠ¡ä¸­å®žçŽ°äº†95%çš„æˆåŠŸçŽ‡ï¼Œè¶…è¶Šäº†çŽ°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®žæœºå™¨äººä¸Šå®žçŽ°äº†83.3%çš„æˆåŠŸçŽ‡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»»åŠ¡å¯¼å‘æŠ“å–ä¸­çš„å¤æ‚æ€§é—®é¢˜ï¼ŒçŽ°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ—¶å¸¸å¸¸æ•ˆçŽ‡ä½Žä¸‹ï¼Œéš¾ä»¥å¿«é€Ÿæ”¶æ•›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶ï¼Œå°†å¤æ‚çš„æŠ“å–ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µæ€§å­ä»»åŠ¡ï¼Œåˆ©ç”¨é˜¶æ®µç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æŒ‡å¯¼å­¦ä¹ å’ŒæŽ¢ç´¢ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æ¡†æž¶åŒ…æ‹¬ä»»åŠ¡åˆ†è§£ã€ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶ã€çŠ¶æ€æŠ½è±¡ã€åŠ¨ä½œç©ºé—´å®šä¹‰åŠè¿‡æ¸¡å¥–åŠ±è®¾è®¡ã€‚æ¯ä¸ªå­ä»»åŠ¡åœ¨ç‰¹å®šé˜¶æ®µå†…è¿›è¡Œå­¦ä¹ ï¼Œè¿‡æ¸¡å¥–åŠ±åˆ™å¼•å¯¼é˜¶æ®µé—´çš„æœ‰æ•ˆè½¬æ¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºŽä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶çš„å¼•å…¥ï¼Œå®ƒé€šè¿‡é˜¶æ®µæ€§åˆ†è§£å’Œæ˜Žç¡®çš„å¥–åŠ±è®¾è®¡ï¼Œæ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆçŽ‡å’ŒæˆåŠŸçŽ‡ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æœºåˆ¶æä¾›äº†æ›´æ¸…æ™°çš„å­¦ä¹ ç›®æ ‡å’ŒæŽ¢ç´¢è¾¹ç•Œã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå¥–åŠ±å‡½æ•°å’ŒçŠ¶æ€æŠ½è±¡å‡½æ•°ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨æ¯ä¸ªé˜¶æ®µå†…çš„æœ‰æ•ˆå­¦ä¹ ã€‚åŒæ—¶ï¼Œç»“åˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä¼˜åŒ–äº†ç­–ç•¥æ›´æ–°è¿‡ç¨‹ï¼Œæå‡äº†æ•´ä½“æ€§èƒ½ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨1000ä¸ªæ¨¡æ‹ŸæŠ“å–ä»»åŠ¡ä¸­å–å¾—äº†95%çš„æˆåŠŸçŽ‡ï¼Œè¶…è¶Šäº†çŽ°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸”åœ¨çœŸå®žæœºå™¨äººä¸Šå®žçŽ°äº†83.3%çš„æˆåŠŸçŽ‡ï¼Œå±•çŽ°å‡ºå“è¶Šçš„å­¦ä¹ é€Ÿåº¦å’Œæ•°æ®æ•ˆçŽ‡ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿ã€æ™ºèƒ½å®¶å±…ç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„æŠ“å–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„æ•ˆçŽ‡å’Œçµæ´»æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®žé™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.

