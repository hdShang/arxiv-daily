---
layout: default
title: Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine
---

# Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.10235" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.10235v1</a>
  <a href="https://arxiv.org/pdf/2512.10235.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.10235v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.10235v1', 'Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hui Li, Akhlak Uz Zaman, Fujian Yan, Hongsheng He

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-11

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³ä»»åŠ¡å¯¼å‘æŠ“å–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶` `ä»»åŠ¡å¯¼å‘æŠ“å–` `æœºå™¨äººæŠ€æœ¯` `å­¦ä¹ æ•ˆç‡` `çŠ¶æ€æŠ½è±¡` `è¿‡æ¸¡å¥–åŠ±`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æŠ“å–æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å¸¸å¸¸é¢ä¸´ä»»åŠ¡å¤æ‚æ€§é«˜ã€å­¦ä¹ æ•ˆç‡ä½çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶å°†æŠ“å–ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œä»è€Œç®€åŒ–å­¦ä¹ è¿‡ç¨‹å¹¶æé«˜æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­æˆåŠŸç‡è¾¾åˆ°95%ï¼Œåœ¨çœŸå®æœºå™¨äººä¸ŠæˆåŠŸç‡ä¸º83.3%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä»»åŠ¡å¯¼å‘çš„æŠ“å–ã€‚ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶é€šè¿‡å°†æŠ“å–ä»»åŠ¡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œé™ä½äº†ä»»åŠ¡å¤æ‚æ€§ã€‚æ¯ä¸ªå­ä»»åŠ¡éƒ½ä¸ç‰¹å®šé˜¶æ®µçš„ä¸Šä¸‹æ–‡ç›¸å…³è”ï¼ŒåŒ…æ‹¬å¥–åŠ±å‡½æ•°ã€åŠ¨ä½œç©ºé—´å’ŒçŠ¶æ€æŠ½è±¡å‡½æ•°ã€‚è¿™ç§ä¸Šä¸‹æ–‡ä¿¡æ¯èƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼é˜¶æ®µå†…çš„å­¦ä¹ ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ï¼Œå‡å°‘çŠ¶æ€-åŠ¨ä½œç©ºé—´ï¼Œå¹¶åœ¨æ˜ç¡®çš„è¾¹ç•Œå†…å¼•å¯¼æ¢ç´¢ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†è¿‡æ¸¡å¥–åŠ±ä»¥é¼“åŠ±æˆ–æƒ©ç½šé˜¶æ®µé—´çš„è¿‡æ¸¡ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹æœå‘ç†æƒ³çš„é˜¶æ®µåºåˆ—ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ç»“åˆåï¼Œè¯¥æ–¹æ³•åœ¨1000ä¸ªæ¨¡æ‹ŸæŠ“å–ä»»åŠ¡ä¸­å®ç°äº†95%çš„æˆåŠŸç‡ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººä¸Šå®ç°äº†83.3%çš„æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»»åŠ¡å¯¼å‘æŠ“å–ä¸­çš„å¤æ‚æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ—¶å¸¸å¸¸æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å¿«é€Ÿæ”¶æ•›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶ï¼Œå°†å¤æ‚çš„æŠ“å–ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µæ€§å­ä»»åŠ¡ï¼Œåˆ©ç”¨é˜¶æ®µç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æŒ‡å¯¼å­¦ä¹ å’Œæ¢ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»»åŠ¡åˆ†è§£ã€ä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶ã€çŠ¶æ€æŠ½è±¡ã€åŠ¨ä½œç©ºé—´å®šä¹‰åŠè¿‡æ¸¡å¥–åŠ±è®¾è®¡ã€‚æ¯ä¸ªå­ä»»åŠ¡åœ¨ç‰¹å®šé˜¶æ®µå†…è¿›è¡Œå­¦ä¹ ï¼Œè¿‡æ¸¡å¥–åŠ±åˆ™å¼•å¯¼é˜¶æ®µé—´çš„æœ‰æ•ˆè½¬æ¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºä¸Šä¸‹æ–‡å¥–åŠ±æœºåˆ¶çš„å¼•å…¥ï¼Œå®ƒé€šè¿‡é˜¶æ®µæ€§åˆ†è§£å’Œæ˜ç¡®çš„å¥–åŠ±è®¾è®¡ï¼Œæ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆç‡å’ŒæˆåŠŸç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æœºåˆ¶æä¾›äº†æ›´æ¸…æ™°çš„å­¦ä¹ ç›®æ ‡å’Œæ¢ç´¢è¾¹ç•Œã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå¥–åŠ±å‡½æ•°å’ŒçŠ¶æ€æŠ½è±¡å‡½æ•°ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨æ¯ä¸ªé˜¶æ®µå†…çš„æœ‰æ•ˆå­¦ä¹ ã€‚åŒæ—¶ï¼Œç»“åˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä¼˜åŒ–äº†ç­–ç•¥æ›´æ–°è¿‡ç¨‹ï¼Œæå‡äº†æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•åœ¨1000ä¸ªæ¨¡æ‹ŸæŠ“å–ä»»åŠ¡ä¸­å–å¾—äº†95%çš„æˆåŠŸç‡ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸”åœ¨çœŸå®æœºå™¨äººä¸Šå®ç°äº†83.3%çš„æˆåŠŸç‡ï¼Œå±•ç°å‡ºå“è¶Šçš„å­¦ä¹ é€Ÿåº¦å’Œæ•°æ®æ•ˆç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿ã€æ™ºèƒ½å®¶å±…ç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æŠ“å–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„æ•ˆç‡å’Œçµæ´»æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.

