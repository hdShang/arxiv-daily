---
layout: default
title: "SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling"
---

# SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16308" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16308v1</a>
  <a href="https://arxiv.org/pdf/2510.16308.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16308v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.16308v1', 'SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chi Zhang, Xian Huang, Wei Dong

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SPOTï¼šåŸºäºéšœç¢ç‰©å¨èƒå»ºæ¨¡çš„æ„ŸçŸ¥å¢å¼ºæ— äººæœºè½¨è¿¹è§„åˆ’**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æ— äººæœº` `è½¨è¿¹è§„åˆ’` `åŠ¨æ€é¿éšœ` `ä¸»åŠ¨è§†è§‰` `é«˜æ–¯è¿‡ç¨‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ— äººæœºåŠ¨æ€é¿éšœæ–¹æ³•å—é™äºå•ç›®æ·±åº¦ç›¸æœºçš„è§†é‡å’Œç›²åŒºï¼Œä¸”è¿åŠ¨è§„åˆ’ä¸æ„ŸçŸ¥åˆ†ç¦»ï¼Œå¯¼è‡´å“åº”å»¶è¿Ÿã€‚
2. SPOTæ¡†æ¶é€šè¿‡é«˜æ–¯è¿‡ç¨‹å»ºç«‹éšœç¢ç‰©ç½®ä¿¡åº¦åœ°å›¾ï¼Œå¹¶ç»“åˆç¢°æ’æ„ŸçŸ¥æ¨ç†ï¼Œç”Ÿæˆè§‚å¯Ÿç´§æ€¥ç¨‹åº¦å›¾ï¼Œå®ç°æ„ŸçŸ¥å¢å¼ºçš„è½¨è¿¹è§„åˆ’ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒSPOTèƒ½æå‰2.8ç§’æ£€æµ‹åˆ°åŠ¨æ€éšœç¢ç‰©ï¼Œæ˜¾è‘—æå‡åŠ¨æ€éšœç¢ç‰©å¯è§æ€§ï¼Œå¹¶å®‰å…¨é€šè¿‡å¤æ‚ç¯å¢ƒã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é…å¤‡å•ç›®æ·±åº¦ç›¸æœºçš„æ— äººæœºåœ¨åŠ¨æ€é¿éšœæ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› æ˜¯è§†é‡æœ‰é™å’Œä¸å¯é¿å…çš„ç›²åŒºã€‚è™½ç„¶å·²ç»æå‡ºäº†é€šè¿‡æ§åˆ¶æœºè½½ç›¸æœºæ¥æ‰©å±•æ„ŸçŸ¥èŒƒå›´çš„ä¸»åŠ¨è§†è§‰ç­–ç•¥ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•å°†è¿åŠ¨è§„åˆ’ä¸æ„ŸçŸ¥è€ƒè™‘å› ç´ åˆ†ç¦»ï¼Œå¯¼è‡´éšœç¢ç‰©å“åº”æ•ˆæœä¸ä½³ä¸”å»¶è¿Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†SPOTï¼ˆåŸºäºéšœç¢ç‰©å¨èƒå»ºæ¨¡çš„æ„ŸçŸ¥å¢å¼ºè§„åˆ’ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§„åˆ’æ¡†æ¶ï¼Œç”¨äºå°†æ„ŸçŸ¥ç›®æ ‡æ˜¾å¼åœ°çº³å…¥è¿åŠ¨ä¼˜åŒ–ä¸­çš„è§‚å¯Ÿæ„ŸçŸ¥è½¨è¿¹è§„åˆ’ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŸºäºé«˜æ–¯è¿‡ç¨‹çš„éšœç¢ç‰©ç½®ä¿¡åº¦åœ°å›¾ï¼Œå®ƒå»ºç«‹äº†å¯¹å·²è¯†åˆ«ï¼ˆå…ˆå‰è§‚å¯Ÿåˆ°çš„ï¼‰å’Œæ½œåœ¨éšœç¢ç‰©çš„ç»Ÿä¸€æ¦‚ç‡è¡¨ç¤ºã€‚é€šè¿‡ç¢°æ’æ„ŸçŸ¥çš„æ¨ç†æœºåˆ¶è¿›ä¸€æ­¥å¤„ç†æ­¤ç½®ä¿¡åº¦ï¼Œè¯¥æœºåˆ¶å°†ç©ºé—´ä¸ç¡®å®šæ€§å’Œè½¨è¿¹é‚»è¿‘æ€§è½¬æ¢ä¸ºéšæ—¶é—´å˜åŒ–çš„è§‚å¯Ÿç´§æ€¥ç¨‹åº¦å›¾ã€‚é€šè¿‡æ•´åˆå½“å‰è§†é‡å†…çš„ç´§æ€¥ç¨‹åº¦å€¼ï¼Œæˆ‘ä»¬å®šä¹‰äº†å¯å¾®ç›®æ ‡ï¼Œä»è€Œå®ç°è®¡ç®—æ—¶é—´ä½äº10æ¯«ç§’çš„å®æ—¶ã€è§‚å¯Ÿæ„ŸçŸ¥è½¨è¿¹è§„åˆ’ã€‚åœ¨åŠ¨æ€ã€æ‚ä¹±å’Œé®æŒ¡ç¯å¢ƒä¸­çš„ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”åŸºçº¿æ–¹æ³•æå‰2.8ç§’æ£€æµ‹åˆ°æ½œåœ¨çš„åŠ¨æ€éšœç¢ç‰©ï¼Œå°†åŠ¨æ€éšœç¢ç‰©çš„å¯è§æ€§æé«˜äº†500ï¼…ä»¥ä¸Šï¼Œå¹¶å®ç°äº†åœ¨æ‚ä¹±ã€é®æŒ¡ç¯å¢ƒä¸­çš„å®‰å…¨å¯¼èˆªã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æ— äººæœºåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­ï¼Œç”±äºå•ç›®æ·±åº¦ç›¸æœºè§†é‡å—é™å’Œç›²åŒºé—®é¢˜ï¼Œå¯¼è‡´åŠ¨æ€é¿éšœèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è¿åŠ¨è§„åˆ’å’Œæ„ŸçŸ¥åˆ†ç¦»ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨æ„ŸçŸ¥ä¿¡æ¯æ¥æŒ‡å¯¼è½¨è¿¹è§„åˆ’ï¼Œå¯¼è‡´å“åº”å»¶è¿Ÿå’Œé¿éšœæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†æ„ŸçŸ¥ä¿¡æ¯æ˜¾å¼åœ°èå…¥åˆ°è½¨è¿¹è§„åˆ’ä¸­ï¼Œé€šè¿‡å»ºç«‹éšœç¢ç‰©ç½®ä¿¡åº¦åœ°å›¾æ¥ç»Ÿä¸€è¡¨ç¤ºå·²è§‚æµ‹å’Œæ½œåœ¨çš„éšœç¢ç‰©ï¼Œå¹¶åˆ©ç”¨ç¢°æ’æ„ŸçŸ¥çš„æ¨ç†æœºåˆ¶å°†ç©ºé—´ä¸ç¡®å®šæ€§å’Œè½¨è¿¹é‚»è¿‘æ€§è½¬åŒ–ä¸ºè§‚å¯Ÿç´§æ€¥ç¨‹åº¦ï¼Œä»è€ŒæŒ‡å¯¼æ— äººæœºä¸»åŠ¨è°ƒæ•´è§†è§’ï¼Œæé«˜å¯¹æ½œåœ¨éšœç¢ç‰©çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°æ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„åŠ¨æ€é¿éšœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSPOTæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) åŸºäºé«˜æ–¯è¿‡ç¨‹çš„éšœç¢ç‰©ç½®ä¿¡åº¦åœ°å›¾æ„å»ºæ¨¡å—ï¼Œç”¨äºèåˆå†å²è§‚æµ‹æ•°æ®ï¼Œå»ºç«‹å¯¹ç¯å¢ƒçš„æ¦‚ç‡è¡¨ç¤ºï¼›2) ç¢°æ’æ„ŸçŸ¥æ¨ç†æ¨¡å—ï¼Œç”¨äºæ ¹æ®æ— äººæœºè½¨è¿¹å’Œéšœç¢ç‰©ç½®ä¿¡åº¦ï¼Œè®¡ç®—è§‚å¯Ÿç´§æ€¥ç¨‹åº¦ï¼›3) è½¨è¿¹ä¼˜åŒ–æ¨¡å—ï¼Œå°†è§‚å¯Ÿç´§æ€¥ç¨‹åº¦ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œç”Ÿæˆè§‚å¯Ÿæ„ŸçŸ¥çš„è½¨è¿¹ã€‚æ•´ä¸ªæµç¨‹æ˜¯ä¸€ä¸ªé—­ç¯åé¦ˆç³»ç»Ÿï¼Œæ— äººæœºæ ¹æ®å½“å‰è½¨è¿¹å’Œæ„ŸçŸ¥ä¿¡æ¯ä¸æ–­è°ƒæ•´è½¨è¿¹ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°é™ä½ç¢°æ’é£é™©ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ„ŸçŸ¥ç›®æ ‡æ˜¾å¼åœ°èå…¥åˆ°è¿åŠ¨è§„åˆ’ä¸­ï¼Œé€šè¿‡éšœç¢ç‰©ç½®ä¿¡åº¦åœ°å›¾å’Œç¢°æ’æ„ŸçŸ¥æ¨ç†æœºåˆ¶ï¼Œå®ç°äº†è§‚å¯Ÿæ„ŸçŸ¥çš„è½¨è¿¹è§„åˆ’ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSPOTèƒ½å¤Ÿä¸»åŠ¨è°ƒæ•´è§†è§’ï¼Œæé«˜å¯¹æ½œåœ¨éšœç¢ç‰©çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæ›´æ—©åœ°å‘ç°å¹¶è§„é¿éšœç¢ç‰©ã€‚

**å…³é”®è®¾è®¡**ï¼šéšœç¢ç‰©ç½®ä¿¡åº¦åœ°å›¾é‡‡ç”¨é«˜æ–¯è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºç¯å¢ƒçš„ä¸ç¡®å®šæ€§ã€‚ç¢°æ’æ„ŸçŸ¥æ¨ç†æœºåˆ¶é€šè¿‡è®¡ç®—è½¨è¿¹ä¸éšœç¢ç‰©ä¹‹é—´çš„è·ç¦»å’Œç¢°æ’æ¦‚ç‡ï¼Œç”Ÿæˆè§‚å¯Ÿç´§æ€¥ç¨‹åº¦ã€‚è½¨è¿¹ä¼˜åŒ–é‡‡ç”¨å¯å¾®åˆ†çš„ä¼˜åŒ–ç›®æ ‡ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶çš„è½¨è¿¹è§„åˆ’ã€‚å…·ä½“è€Œè¨€ï¼ŒæŸå¤±å‡½æ•°ä¸­åŒ…å«äº†è½¨è¿¹å¹³æ»‘æ€§ã€é¿éšœå’Œè§‚å¯Ÿç´§æ€¥ç¨‹åº¦ç­‰å¤šä¸ªé¡¹ï¼Œé€šè¿‡è°ƒæ•´å„é¡¹çš„æƒé‡ï¼Œå¯ä»¥å¹³è¡¡è½¨è¿¹çš„å¹³æ»‘æ€§ã€å®‰å…¨æ€§ä»¥åŠæ„ŸçŸ¥èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSPOTæ–¹æ³•æ¯”åŸºçº¿æ–¹æ³•æå‰2.8ç§’æ£€æµ‹åˆ°æ½œåœ¨çš„åŠ¨æ€éšœç¢ç‰©ï¼ŒåŠ¨æ€éšœç¢ç‰©çš„å¯è§æ€§æé«˜äº†500ï¼…ä»¥ä¸Šã€‚åœ¨æ‚ä¹±å’Œé®æŒ¡ç¯å¢ƒä¸­ï¼ŒSPOTèƒ½å¤Ÿå®‰å…¨åœ°å¼•å¯¼æ— äººæœºé€šè¿‡ï¼Œè€ŒåŸºçº¿æ–¹æ³•åˆ™å®¹æ˜“å‘ç”Ÿç¢°æ’ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSPOTæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æ— äººæœºåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„é¿éšœèƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ— äººæœºè‡ªä¸»å¯¼èˆªã€ç‰©æµé…é€ã€å®‰é˜²å·¡æ£€ç­‰é¢†åŸŸã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ— äººæœºéœ€è¦åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­å®‰å…¨å¯é åœ°æ‰§è¡Œä»»åŠ¡ã€‚SPOTæ¡†æ¶èƒ½å¤Ÿæé«˜æ— äººæœºå¯¹ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›å’Œé¿éšœèƒ½åŠ›ï¼Œä»è€Œæé«˜ä»»åŠ¡çš„æˆåŠŸç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–ç§»åŠ¨æœºå™¨äººå¹³å°ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æœåŠ¡æœºå™¨äººç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> UAVs equipped with a single depth camera encounter significant challenges in dynamic obstacle avoidance due to limited field of view and inevitable blind spots. While active vision strategies that steer onboard cameras have been proposed to expand sensing coverage, most existing methods separate motion planning from sensing considerations, resulting in less effective and delayed obstacle response. To address this limitation, we introduce SPOT (Sensing-augmented Planning via Obstacle Threat modeling), a unified planning framework for observation-aware trajectory planning that explicitly incorporates sensing objectives into motion optimization. At the core of our method is a Gaussian Process-based obstacle belief map, which establishes a unified probabilistic representation of both recognized (previously observed) and potential obstacles. This belief is further processed through a collision-aware inference mechanism that transforms spatial uncertainty and trajectory proximity into a time-varying observation urgency map. By integrating urgency values within the current field of view, we define differentiable objectives that enable real-time, observation-aware trajectory planning with computation times under 10 ms. Simulation and real-world experiments in dynamic, cluttered, and occluded environments show that our method detects potential dynamic obstacles 2.8 seconds earlier than baseline approaches, increasing dynamic obstacle visibility by over 500\%, and enabling safe navigation through cluttered, occluded environments.

