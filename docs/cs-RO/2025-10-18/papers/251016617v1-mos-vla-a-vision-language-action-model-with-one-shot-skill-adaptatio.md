---
layout: default
title: MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation
---

# MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.16617" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.16617v1</a>
  <a href="https://arxiv.org/pdf/2510.16617.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16617v1" onclick="toggleFavorite(this, '2510.16617v1', 'MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruihan Zhao, Tyler Ingebrand, Sandeep Chinchali, Ufuk Topcu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MoS-VLAï¼šåŸºäºæŠ€èƒ½ç»„åˆçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå®ç°æœºå™¨äººå•æ ·æœ¬æŠ€èƒ½è¿ç§»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `æœºå™¨äººå­¦ä¹ ` `å•æ ·æœ¬å­¦ä¹ ` `æŠ€èƒ½ç»„åˆ` `å‡¸ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨é¢å¯¹æ–°çš„ç¯å¢ƒã€æœºå™¨äººå½¢æ€æˆ–ä»»åŠ¡æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨ã€‚
2. MoS-VLAå°†æœºå™¨äººç­–ç•¥è¡¨ç¤ºä¸ºæŠ€èƒ½åŸºå‡½æ•°çš„çº¿æ€§ç»„åˆï¼Œé€šè¿‡å•æ ·æœ¬æ¼”ç¤ºå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMoS-VLAåœ¨æœªè§æ•°æ®é›†ä¸Šé™ä½äº†åŠ¨ä½œé¢„æµ‹è¯¯å·®ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººä»»åŠ¡ä¸­å–å¾—æˆåŠŸã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ··åˆæŠ€èƒ½è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆMoS-VLAï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³VLAæ¨¡å‹åœ¨æ–°çš„ç¯å¢ƒã€æœºå™¨äººå½¢æ€æˆ–ä»»åŠ¡ä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚MoS-VLAå°†æœºå™¨äººæ“ä½œç­–ç•¥è¡¨ç¤ºä¸ºä¸€ç»„å­¦ä¹ åˆ°çš„åŸºå‡½æ•°çš„çº¿æ€§ç»„åˆï¼Œé€šè¿‡åœ¨Open X-Embodimenté¡¹ç›®çš„æ•°æ®é›†ä¸Šè”åˆå­¦ä¹ è¿™äº›åŸºå‡½æ•°ï¼Œæ„å»ºç»“æ„åŒ–çš„æŠ€èƒ½ç©ºé—´ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œé€‚åº”æ–°ä»»åŠ¡ä»…éœ€ä¸€ä¸ªä¸“å®¶æ¼”ç¤ºã€‚ç„¶åï¼Œé€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„å‡¸ä¼˜åŒ–é—®é¢˜æ¨æ–­ç›¸åº”çš„æŠ€èƒ½è¡¨ç¤ºï¼Œè¯¥ä¼˜åŒ–é—®é¢˜æœ€å°åŒ–L1åŠ¨ä½œè¯¯å·®ï¼Œæ— éœ€æ¢¯åº¦æ›´æ–°ã€‚è¿™ç§æ— æ¢¯åº¦é€‚åº”æ–¹æ³•å¼€é”€æå°ï¼ŒåŒæ—¶èƒ½å¤Ÿå¿«é€Ÿå®ä¾‹åŒ–æ–°æŠ€èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoS-VLAåœ¨äº”ä¸ªæœªè§æ•°æ®é›†ä¸­çš„åŠ¨ä½œé¢„æµ‹è¯¯å·®å‡ä½äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨é¢„è®­ç»ƒVLAæ¨¡å‹å®Œå…¨å¤±è´¥çš„æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨é¢„è®­ç»ƒåï¼Œéš¾ä»¥é€‚åº”æ–°çš„æœºå™¨äººå½¢æ€ã€ç¯å¢ƒæˆ–ä»»åŠ¡ã€‚ä¸»è¦ç—›ç‚¹åœ¨äºï¼Œé’ˆå¯¹æ–°ä»»åŠ¡é€šå¸¸éœ€è¦å¤§é‡çš„é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”æ³›åŒ–æ€§èƒ½éš¾ä»¥ä¿è¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMoS-VLAçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤æ‚çš„æœºå™¨äººæ“ä½œç­–ç•¥åˆ†è§£ä¸ºä¸€ç»„é¢„å…ˆå­¦ä¹ å¥½çš„ã€é€šç”¨çš„æŠ€èƒ½åŸºå‡½æ•°ã€‚é€šè¿‡å­¦ä¹ è¿™äº›åŸºå‡½æ•°çš„çº¿æ€§ç»„åˆï¼Œæ¨¡å‹å¯ä»¥å¿«é€Ÿé€‚åº”æ–°çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€è¿›è¡Œè€—æ—¶çš„æ¢¯åº¦æ›´æ–°ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºå°†å¤æ‚çš„å‡½æ•°åˆ†è§£ä¸ºå‚…é‡Œå¶åŸºå‡½æ•°ï¼Œä»è€Œç®€åŒ–äº†å­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMoS-VLAæ¡†æ¶åŒ…å«é¢„è®­ç»ƒå’Œé€‚åº”ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨Open X-Embodimentæ•°æ®é›†ä¸Šå­¦ä¹ ä¸€ç»„æŠ€èƒ½åŸºå‡½æ•°ï¼Œæ„å»ºä¸€ä¸ªç»“æ„åŒ–çš„æŠ€èƒ½ç©ºé—´ã€‚åœ¨é€‚åº”é˜¶æ®µï¼Œç»™å®šä¸€ä¸ªæ–°ä»»åŠ¡çš„å•æ ·æœ¬æ¼”ç¤ºï¼Œæ¨¡å‹é€šè¿‡å‡¸ä¼˜åŒ–æ–¹æ³•æ¨æ–­å‡ºè¯¥ä»»åŠ¡å¯¹åº”çš„æŠ€èƒ½è¡¨ç¤ºï¼Œå³æŠ€èƒ½åŸºå‡½æ•°çš„çº¿æ€§ç»„åˆç³»æ•°ã€‚ç„¶åï¼Œæ¨¡å‹åˆ©ç”¨è¿™äº›ç³»æ•°ç”Ÿæˆç›¸åº”çš„åŠ¨ä½œåºåˆ—ã€‚

**å…³é”®åˆ›æ–°**ï¼šMoS-VLAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æ— æ¢¯åº¦å•æ ·æœ¬é€‚åº”æ–¹æ³•ã€‚é€šè¿‡å°†ç­–ç•¥è¡¨ç¤ºä¸ºæŠ€èƒ½åŸºå‡½æ•°çš„çº¿æ€§ç»„åˆï¼Œæ¨¡å‹å¯ä»¥å°†é€‚åº”æ–°ä»»åŠ¡çš„é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œé¿å…äº†è€—æ—¶çš„æ¢¯åº¦æ›´æ–°ã€‚è¿™ç§æ–¹æ³•å¤§å¤§æé«˜äº†æ¨¡å‹çš„é€‚åº”é€Ÿåº¦å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šMoS-VLAä½¿ç”¨L1æŸå¤±å‡½æ•°æ¥æœ€å°åŒ–åŠ¨ä½œé¢„æµ‹è¯¯å·®ï¼Œè¿™æœ‰åŠ©äºæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚å‡¸ä¼˜åŒ–é—®é¢˜é‡‡ç”¨ç°æˆçš„æ±‚è§£å™¨è¿›è¡Œæ±‚è§£ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´å‚æ•°ã€‚æŠ€èƒ½åŸºå‡½æ•°çš„æ•°é‡æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚ç½‘ç»œç»“æ„æ–¹é¢ï¼Œå¯ä»¥ä½¿ç”¨Transformerç­‰å¸¸è§çš„åºåˆ—æ¨¡å‹æ¥å­¦ä¹ æŠ€èƒ½åŸºå‡½æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MoS-VLAåœ¨äº”ä¸ªæœªè§æ•°æ®é›†ä¸Šå®ç°äº†æ›´ä½çš„åŠ¨ä½œé¢„æµ‹è¯¯å·®ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­ï¼ŒMoS-VLAæˆåŠŸå®Œæˆäº†é¢„è®­ç»ƒVLAæ¨¡å‹æ— æ³•å®Œæˆçš„ä»»åŠ¡ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚å•æ ·æœ¬é€‚åº”èƒ½åŠ›å¤§å¤§é™ä½äº†æ¨¡å‹éƒ¨ç½²çš„éš¾åº¦å’Œæˆæœ¬ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MoS-VLAå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ã€å®¶åº­æœåŠ¡æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰é¢†åŸŸã€‚å®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººå¿«é€Ÿé€‚åº”æ–°çš„ä»»åŠ¡å’Œç¯å¢ƒï¼Œæé«˜æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œå·¥ä½œæ•ˆç‡ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¤æ‚çš„æœºå™¨äººç³»ç»Ÿï¼Œä¾‹å¦‚å¤šæœºå™¨äººåä½œå’Œè‡ªä¸»å¯¼èˆªã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models trained on large robot datasets promise general-purpose, robust control across diverse domains and embodiments. However, existing approaches often fail out-of-the-box when deployed in novel environments, embodiments, or tasks. We introduce Mixture of Skills VLA (MoS-VLA), a framework that represents robot manipulation policies as linear combinations of a finite set of learned basis functions. During pretraining, MoS-VLA jointly learns these basis functions across datasets from the Open X-Embodiment project, producing a structured skill space. At test time, adapting to a new task requires only a single expert demonstration. The corresponding skill representation is then inferred via a lightweight convex optimization problem that minimizes the L1 action error, without requiring gradient updates. This gradient-free adaptation incurs minimal overhead while enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower action-prediction error on five out of five unseen datasets and succeeds in both simulation and real-robot tasks where a pretrained VLA model fails outright. Project page: mos-vla.github.io/

