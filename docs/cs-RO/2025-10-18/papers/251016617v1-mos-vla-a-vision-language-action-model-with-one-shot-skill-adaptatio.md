---
layout: default
title: MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation
---

# MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.16617" target="_blank" class="toolbar-btn">arXiv: 2510.16617v1</a>
    <a href="https://arxiv.org/pdf/2510.16617.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.16617v1" 
            onclick="toggleFavorite(this, '2510.16617v1', 'MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ruihan Zhao, Tyler Ingebrand, Sandeep Chinchali, Ufuk Topcu

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-18

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**MoS-VLAÔºöÂü∫‰∫éÊäÄËÉΩÁªÑÂêàÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºåÂÆûÁé∞Êú∫Âô®‰∫∫ÂçïÊ†∑Êú¨ÊäÄËÉΩËøÅÁßª**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫Â≠¶‰π†` `ÂçïÊ†∑Êú¨Â≠¶‰π†` `ÊäÄËÉΩÁªÑÂêà` `Âá∏‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâVLAÊ®°ÂûãÂú®Èù¢ÂØπÊñ∞ÁöÑÁéØÂ¢É„ÄÅÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÊàñ‰ªªÂä°Êó∂ÔºåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈöæ‰ª•Áõ¥Êé•Â∫îÁî®„ÄÇ
2. MoS-VLAÂ∞ÜÊú∫Âô®‰∫∫Á≠ñÁï•Ë°®Á§∫‰∏∫ÊäÄËÉΩÂü∫ÂáΩÊï∞ÁöÑÁ∫øÊÄßÁªÑÂêàÔºåÈÄöËøáÂçïÊ†∑Êú¨ÊºîÁ§∫Âø´ÈÄüÈÄÇÂ∫îÊñ∞‰ªªÂä°„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMoS-VLAÂú®Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏äÈôç‰Ωé‰∫ÜÂä®‰ΩúÈ¢ÑÊµãËØØÂ∑ÆÔºåÂπ∂Âú®ÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂèñÂæóÊàêÂäü„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ê∑∑ÂêàÊäÄËÉΩËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºàMoS-VLAÔºâÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥VLAÊ®°ÂûãÂú®Êñ∞ÁöÑÁéØÂ¢É„ÄÅÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÊàñ‰ªªÂä°‰∏≠Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇMoS-VLAÂ∞ÜÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•Ë°®Á§∫‰∏∫‰∏ÄÁªÑÂ≠¶‰π†Âà∞ÁöÑÂü∫ÂáΩÊï∞ÁöÑÁ∫øÊÄßÁªÑÂêàÔºåÈÄöËøáÂú®Open X-EmbodimentÈ°πÁõÆÁöÑÊï∞ÊçÆÈõÜ‰∏äËÅîÂêàÂ≠¶‰π†Ëøô‰∫õÂü∫ÂáΩÊï∞ÔºåÊûÑÂª∫ÁªìÊûÑÂåñÁöÑÊäÄËÉΩÁ©∫Èó¥„ÄÇÂú®ÊµãËØïÈò∂ÊÆµÔºåÈÄÇÂ∫îÊñ∞‰ªªÂä°‰ªÖÈúÄ‰∏Ä‰∏™‰∏ìÂÆ∂ÊºîÁ§∫„ÄÇÁÑ∂ÂêéÔºåÈÄöËøá‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÂá∏‰ºòÂåñÈóÆÈ¢òÊé®Êñ≠Áõ∏Â∫îÁöÑÊäÄËÉΩË°®Á§∫ÔºåËØ•‰ºòÂåñÈóÆÈ¢òÊúÄÂ∞èÂåñL1Âä®‰ΩúËØØÂ∑ÆÔºåÊó†ÈúÄÊ¢ØÂ∫¶Êõ¥Êñ∞„ÄÇËøôÁßçÊó†Ê¢ØÂ∫¶ÈÄÇÂ∫îÊñπÊ≥ïÂºÄÈîÄÊûÅÂ∞èÔºåÂêåÊó∂ËÉΩÂ§üÂø´ÈÄüÂÆû‰æãÂåñÊñ∞ÊäÄËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMoS-VLAÂú®‰∫î‰∏™Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÂä®‰ΩúÈ¢ÑÊµãËØØÂ∑ÆÂùá‰Ωé‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®È¢ÑËÆ≠ÁªÉVLAÊ®°ÂûãÂÆåÂÖ®Â§±Ë¥•ÁöÑÊ®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊàêÂäü„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®È¢ÑËÆ≠ÁªÉÂêéÔºåÈöæ‰ª•ÈÄÇÂ∫îÊñ∞ÁöÑÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅ„ÄÅÁéØÂ¢ÉÊàñ‰ªªÂä°„ÄÇ‰∏ªË¶ÅÁóõÁÇπÂú®‰∫éÔºåÈíàÂØπÊñ∞‰ªªÂä°ÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÈáçÊñ∞ËÆ≠ÁªÉÊàñÂæÆË∞ÉÔºåËÆ°ÁÆóÊàêÊú¨È´òÊòÇÔºå‰∏îÊ≥õÂåñÊÄßËÉΩÈöæ‰ª•‰øùËØÅ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöMoS-VLAÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÂ§çÊùÇÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•ÂàÜËß£‰∏∫‰∏ÄÁªÑÈ¢ÑÂÖàÂ≠¶‰π†Â•ΩÁöÑ„ÄÅÈÄöÁî®ÁöÑÊäÄËÉΩÂü∫ÂáΩÊï∞„ÄÇÈÄöËøáÂ≠¶‰π†Ëøô‰∫õÂü∫ÂáΩÊï∞ÁöÑÁ∫øÊÄßÁªÑÂêàÔºåÊ®°ÂûãÂèØ‰ª•Âø´ÈÄüÈÄÇÂ∫îÊñ∞ÁöÑ‰ªªÂä°ÔºåËÄåÊó†ÈúÄËøõË°åËÄóÊó∂ÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞„ÄÇËøôÁßçÊñπÊ≥ïÁ±ª‰ºº‰∫éÂ∞ÜÂ§çÊùÇÁöÑÂáΩÊï∞ÂàÜËß£‰∏∫ÂÇÖÈáåÂè∂Âü∫ÂáΩÊï∞Ôºå‰ªéËÄåÁÆÄÂåñ‰∫ÜÂ≠¶‰π†ËøáÁ®ã„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMoS-VLAÊ°ÜÊû∂ÂåÖÂê´È¢ÑËÆ≠ÁªÉÂíåÈÄÇÂ∫î‰∏§‰∏™‰∏ªË¶ÅÈò∂ÊÆµ„ÄÇÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºåÊ®°ÂûãÂú®Open X-EmbodimentÊï∞ÊçÆÈõÜ‰∏äÂ≠¶‰π†‰∏ÄÁªÑÊäÄËÉΩÂü∫ÂáΩÊï∞ÔºåÊûÑÂª∫‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÊäÄËÉΩÁ©∫Èó¥„ÄÇÂú®ÈÄÇÂ∫îÈò∂ÊÆµÔºåÁªôÂÆö‰∏Ä‰∏™Êñ∞‰ªªÂä°ÁöÑÂçïÊ†∑Êú¨ÊºîÁ§∫ÔºåÊ®°ÂûãÈÄöËøáÂá∏‰ºòÂåñÊñπÊ≥ïÊé®Êñ≠Âá∫ËØ•‰ªªÂä°ÂØπÂ∫îÁöÑÊäÄËÉΩË°®Á§∫ÔºåÂç≥ÊäÄËÉΩÂü∫ÂáΩÊï∞ÁöÑÁ∫øÊÄßÁªÑÂêàÁ≥ªÊï∞„ÄÇÁÑ∂ÂêéÔºåÊ®°ÂûãÂà©Áî®Ëøô‰∫õÁ≥ªÊï∞ÁîüÊàêÁõ∏Â∫îÁöÑÂä®‰ΩúÂ∫èÂàó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöMoS-VLAÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂Êó†Ê¢ØÂ∫¶ÂçïÊ†∑Êú¨ÈÄÇÂ∫îÊñπÊ≥ï„ÄÇÈÄöËøáÂ∞ÜÁ≠ñÁï•Ë°®Á§∫‰∏∫ÊäÄËÉΩÂü∫ÂáΩÊï∞ÁöÑÁ∫øÊÄßÁªÑÂêàÔºåÊ®°ÂûãÂèØ‰ª•Â∞ÜÈÄÇÂ∫îÊñ∞‰ªªÂä°ÁöÑÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∏Ä‰∏™Âá∏‰ºòÂåñÈóÆÈ¢òÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜËÄóÊó∂ÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞„ÄÇËøôÁßçÊñπÊ≥ïÂ§ßÂ§ßÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÈÄÇÂ∫îÈÄüÂ∫¶ÂíåÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöMoS-VLA‰ΩøÁî®L1ÊçüÂ§±ÂáΩÊï∞Êù•ÊúÄÂ∞èÂåñÂä®‰ΩúÈ¢ÑÊµãËØØÂ∑ÆÔºåËøôÊúâÂä©‰∫éÊèêÈ´òÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂá∏‰ºòÂåñÈóÆÈ¢òÈááÁî®Áé∞ÊàêÁöÑÊ±ÇËß£Âô®ËøõË°åÊ±ÇËß£ÔºåÊó†ÈúÄÊâãÂä®Ë∞ÉÊï¥ÂèÇÊï∞„ÄÇÊäÄËÉΩÂü∫ÂáΩÊï∞ÁöÑÊï∞ÈáèÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑË∂ÖÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇÁΩëÁªúÁªìÊûÑÊñπÈù¢ÔºåÂèØ‰ª•‰ΩøÁî®TransformerÁ≠âÂ∏∏ËßÅÁöÑÂ∫èÂàóÊ®°ÂûãÊù•Â≠¶‰π†ÊäÄËÉΩÂü∫ÂáΩÊï∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MoS-VLAÂú®‰∫î‰∏™Êú™ËßÅÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊõ¥‰ΩéÁöÑÂä®‰ΩúÈ¢ÑÊµãËØØÂ∑ÆÔºåËØÅÊòé‰∫ÜÂÖ∂‰ºòË∂äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂú®Ê®°ÊãüÂíåÁúüÂÆûÊú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÔºåMoS-VLAÊàêÂäüÂÆåÊàê‰∫ÜÈ¢ÑËÆ≠ÁªÉVLAÊ®°ÂûãÊó†Ê≥ïÂÆåÊàêÁöÑ‰ªªÂä°ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂçïÊ†∑Êú¨ÈÄÇÂ∫îËÉΩÂäõÂ§ßÂ§ßÈôç‰Ωé‰∫ÜÊ®°ÂûãÈÉ®ÁΩ≤ÁöÑÈöæÂ∫¶ÂíåÊàêÊú¨„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

MoS-VLAÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂú®Â∑•‰∏öËá™Âä®Âåñ„ÄÅÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠âÈ¢ÜÂüü„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Âø´ÈÄüÈÄÇÂ∫îÊñ∞ÁöÑ‰ªªÂä°ÂíåÁéØÂ¢ÉÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥ÂíåÂ∑•‰ΩúÊïàÁéá„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÔºå‰æãÂ¶ÇÂ§öÊú∫Âô®‰∫∫Âçè‰ΩúÂíåËá™‰∏ªÂØºËà™„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Vision-Language-Action (VLA) models trained on large robot datasets promise general-purpose, robust control across diverse domains and embodiments. However, existing approaches often fail out-of-the-box when deployed in novel environments, embodiments, or tasks. We introduce Mixture of Skills VLA (MoS-VLA), a framework that represents robot manipulation policies as linear combinations of a finite set of learned basis functions. During pretraining, MoS-VLA jointly learns these basis functions across datasets from the Open X-Embodiment project, producing a structured skill space. At test time, adapting to a new task requires only a single expert demonstration. The corresponding skill representation is then inferred via a lightweight convex optimization problem that minimizes the L1 action error, without requiring gradient updates. This gradient-free adaptation incurs minimal overhead while enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower action-prediction error on five out of five unseen datasets and succeeds in both simulation and real-robot tasks where a pretrained VLA model fails outright. Project page: mos-vla.github.io/

