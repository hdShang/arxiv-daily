---
layout: default
title: Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning
---

# Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22892" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22892v1</a>
  <a href="https://arxiv.org/pdf/2510.22892.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22892v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22892v1', 'Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingzehua Xu, Yangyang Li, Yangfei Chen, Guanwen Xie, Shuai Zhang

**åˆ†ç±»**: cs.RO, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMå’ŒLyapunovå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”è™šæ‹Ÿæ¨¡å‹æ§åˆ¶ï¼Œæå‡æœºå™¨äººè‡‚åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„é€‚åº”æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è™šæ‹Ÿæ¨¡å‹æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `Lyapunovç¨³å®šæ€§` `è‡ªé€‚åº”æ§åˆ¶` `æœºå™¨äººè‡‚` `ä¸ç¡®å®šç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿæœºå™¨äººæ§åˆ¶åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­è¡¨ç°å‡ºåˆšæ€§å’Œè„†å¼±æ€§ï¼Œéš¾ä»¥é€‚åº”æ‰°åŠ¨å’Œä¿¡æ¯ä¸å®Œæ•´çš„æƒ…å†µã€‚
2. è¯¥æ–¹æ³•ç»“åˆLLMå’ŒLyapunovå¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨LLMè¿›è¡Œé«˜å±‚æ¨ç†å’Œåè°ƒï¼ŒLyapunovå¼ºåŒ–å­¦ä¹ ä¿è¯ç¨³å®šæ€§ã€‚
3. åœ¨7è‡ªç”±åº¦Pandaæœºæ¢°è‡‚ä¸Šçš„ä»¿çœŸè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨æ€ä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLyapunovå‡½æ•°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªé€‚åº”è™šæ‹Ÿæ¨¡å‹æ§åˆ¶ï¼ˆVMCï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸVMCåœ¨ä¸ç¡®å®šç¯å¢ƒä¸­åˆšæ€§å’Œè„†å¼±çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¿ç•™äº†VMCçš„ç‰©ç†å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶æ”¯æŒæœ‰ç¨³å®šä¿è¯çš„åœ¨çº¿è‡ªé€‚åº”ã€‚LLMæä¾›ç»“æ„åŒ–çš„å…ˆéªŒçŸ¥è¯†å’Œé«˜å±‚æ¬¡æ¨ç†ï¼Œå¢å¼ºäº†è™šæ‹Ÿç»„ä»¶ä¹‹é—´çš„åè°ƒæ€§ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶ä¿ƒè¿›äº†å¯¹ä¸åŒä»»åŠ¡è¦æ±‚çš„çµæ´»è°ƒæ•´ã€‚Lyapunovå‡½æ•°å¼ºåŒ–å­¦ä¹ åˆ™å¼ºåˆ¶æ‰§è¡Œç†è®ºä¸Šçš„ç¨³å®šæ€§çº¦æŸï¼Œç¡®ä¿åœ¨ä¸ç¡®å®šæ€§ä¸‹çš„å®‰å…¨å¯é çš„è‡ªé€‚åº”ã€‚åœ¨7è‡ªç”±åº¦Pandaæœºæ¢°è‡‚ä¸Šçš„å¤§é‡ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°å¹³è¡¡äº†åŠ¨æ€ä»»åŠ¡ä¸­çš„ç«äº‰ç›®æ ‡ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶çªå‡ºäº†LLMæŒ‡å¯¼å’ŒLyapunovçº¦æŸè‡ªé€‚åº”çš„ååŒä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šä¼ ç»Ÿè™šæ‹Ÿæ¨¡å‹æ§åˆ¶ï¼ˆVMCï¼‰æ–¹æ³•ä¾èµ–äºå›ºå®šçš„å‚æ•°ï¼Œå¹¶ä¸”è™šæ‹Ÿç»„ä»¶ä¹‹é—´çš„åè°ƒèƒ½åŠ›æœ‰é™ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­é€‚åº”ä¸æ–­å˜åŒ–çš„ä»»åŠ¡ç›®æ ‡çš„èƒ½åŠ›ã€‚å½“å—åˆ°æ‰°åŠ¨æˆ–ä¿¡æ¯ä¸å®Œæ•´æ—¶ï¼Œä¼ ç»Ÿçš„æ§åˆ¶æµç¨‹å˜å¾—åƒµåŒ–å’Œè„†å¼±ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å’ŒLyapunovå‡½æ•°å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§ä¿è¯ç›¸ç»“åˆï¼Œä»è€Œå®ç°VMCçš„è‡ªé€‚åº”æ€§ã€‚LLMç”¨äºæä¾›ç»“æ„åŒ–çš„å…ˆéªŒçŸ¥è¯†å’Œé«˜å±‚æ¬¡çš„æ¨ç†ï¼Œä»¥å¢å¼ºè™šæ‹Ÿç»„ä»¶ä¹‹é—´çš„åè°ƒæ€§ï¼Œè€ŒLyapunovå‡½æ•°å¼ºåŒ–å­¦ä¹ åˆ™ç”¨äºç¡®ä¿åœ¨ä¸ç¡®å®šæ€§ä¸‹çš„å®‰å…¨å’Œå¯é çš„è‡ªé€‚åº”ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è™šæ‹Ÿæ¨¡å‹æ§åˆ¶ï¼ˆVMCï¼‰å±‚ï¼Œè´Ÿè´£å°†è™šæ‹ŸåŠ›æ˜ å°„åˆ°å…³èŠ‚åŠ›çŸ©ï¼›2) å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±‚ï¼Œæä¾›é«˜å±‚æ¬¡çš„æ¨ç†å’Œåè°ƒï¼Œç”¨äºåŠ¨æ€è°ƒæ•´VMCçš„å‚æ•°ï¼›3) Lyapunovå‡½æ•°å¼ºåŒ–å­¦ä¹ å±‚ï¼Œç”¨äºä¿è¯ç³»ç»Ÿçš„ç¨³å®šæ€§ï¼Œå¹¶çº¦æŸLLMçš„è¾“å‡ºã€‚æ•´ä½“æµç¨‹æ˜¯ï¼Œé¦–å…ˆä½¿ç”¨LLMæ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´VMCå‚æ•°ï¼Œç„¶åä½¿ç”¨Lyapunovå‡½æ•°å¼ºåŒ–å­¦ä¹ å¯¹LLMçš„è¾“å‡ºè¿›è¡Œçº¦æŸï¼Œæœ€åå°†è°ƒæ•´åçš„å‚æ•°ä¼ é€’ç»™VMCå±‚è¿›è¡Œæ§åˆ¶ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†LLMå’ŒLyapunovå‡½æ•°å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œä»è€Œå®ç°äº†VMCçš„è‡ªé€‚åº”æ€§å’Œç¨³å®šæ€§ã€‚ä¸ä¼ ç»Ÿçš„VMCæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€è°ƒæ•´VMCçš„å‚æ•°ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¿è¯ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šLLMè¢«ç”¨äºç”Ÿæˆè™šæ‹Ÿç»„ä»¶çš„å‚æ•°ï¼Œä¾‹å¦‚è™šæ‹Ÿå¼¹ç°§çš„åˆšåº¦å’Œé˜»å°¼ç³»æ•°ã€‚Lyapunovå‡½æ•°è¢«ç”¨äºå®šä¹‰å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ï¼Œä»è€Œä¿è¯ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•ä½¿ç”¨Trust Region Policy Optimization (TRPO) æˆ– Proximal Policy Optimization (PPO) ç­‰ç®—æ³•ï¼Œä»¥ä¼˜åŒ–LLMçš„è¾“å‡ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨æ€ä»»åŠ¡ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡ç«äº‰ç›®æ ‡ï¼Œå¹¶å–å¾—ä¼˜äºä¼ ç»ŸVMCæ–¹æ³•çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åœ¨è½¨è¿¹è·Ÿè¸ªç²¾åº¦ã€ä»»åŠ¡å®Œæˆæ—¶é—´å’Œèƒ½é‡æ¶ˆè€—ç­‰æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜éªŒè¯äº†LLMæŒ‡å¯¼å’ŒLyapunovçº¦æŸè‡ªé€‚åº”çš„ååŒä¼˜åŠ¿ï¼Œè¡¨æ˜ä¸¤è€…ç»“åˆèƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜ç³»ç»Ÿçš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­è¿›è¡Œæ“ä½œçš„æœºå™¨äººç³»ç»Ÿï¼Œä¾‹å¦‚ï¼šå·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—æœºå™¨äººã€æœåŠ¡æœºå™¨äººç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººè‡‚çš„é€‚åº”æ€§å’Œç¨³å®šæ€§ï¼Œå¯ä»¥ä½¿å…¶åœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­æ›´å¥½åœ°å®Œæˆä»»åŠ¡ï¼Œæé«˜ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ï¼Œå¹¶é™ä½å®‰å…¨é£é™©ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„æœºå™¨äººç³»ç»Ÿå’Œä»»åŠ¡ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robotic arms are increasingly deployed in uncertain environments, yet conventional control pipelines often become rigid and brittle when exposed to perturbations or incomplete information. Virtual Model Control (VMC) enables compliant behaviors by embedding virtual forces and mapping them into joint torques, but its reliance on fixed parameters and limited coordination among virtual components constrains adaptability and may undermine stability as task objectives evolve. To address these limitations, we propose Adaptive VMC with Large Language Model (LLM)- and Lyapunov-Based Reinforcement Learning (RL), which preserves the physical interpretability of VMC while supporting stability-guaranteed online adaptation. The LLM provides structured priors and high-level reasoning that enhance coordination among virtual components, improve sample efficiency, and facilitate flexible adjustment to varying task requirements. Complementarily, Lyapunov-based RL enforces theoretical stability constraints, ensuring safe and reliable adaptation under uncertainty. Extensive simulations on a 7-DoF Panda arm demonstrate that our approach effectively balances competing objectives in dynamic tasks, achieving superior performance while highlighting the synergistic benefits of LLM guidance and Lyapunov-constrained adaptation.

