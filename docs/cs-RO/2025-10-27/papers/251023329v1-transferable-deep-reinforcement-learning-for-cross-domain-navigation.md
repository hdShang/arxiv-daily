---
layout: default
title: Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon
---

# Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.23329" target="_blank" class="toolbar-btn">arXiv: 2510.23329v1</a>
    <a href="https://arxiv.org/pdf/2510.23329.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23329v1" 
            onclick="toggleFavorite(this, '2510.23329v1', 'Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Shreya Santra, Thomas Robbins, Kazuya Yoshida

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27

**Â§áÊ≥®**: 6 pages, 7 figures. Accepted at IEEE iSpaRo 2025

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éDRLÁöÑË∑®ÂüüËøÅÁßªÂØºËà™ÊñπÊ≥ïÔºåÂÆûÁé∞‰ªéÂÜúÁî∞Âà∞ÊúàÁêÉÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `Ë∑®ÂüüËøÅÁßª` `Ëá™‰∏ªÂØºËà™` `Êú∫Âô®‰∫∫` `Ë°åÊòüÊé¢Á¥¢`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÂØºËà™ÁÆóÊ≥ïÈúÄË¶ÅÈíàÂØπÁâπÂÆöÁéØÂ¢ÉËøõË°åÂ§ßÈáèË∞ÉÊï¥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Êñ∞È¢ÜÂüüÁöÑÊâ©Â±ïÊÄßÔºåÊòØÂΩìÂâçÈù¢‰∏¥ÁöÑÊ†∏ÂøÉÈóÆÈ¢ò„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫Âà©Áî®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Ôºå‰ΩøÊú∫Âô®‰∫∫Âú®‰∏éÁéØÂ¢ÉÁöÑÁõ¥Êé•‰∫§‰∫í‰∏≠Â≠¶‰π†ÂØºËà™Á≠ñÁï•ÔºåÂπ∂Á†îÁ©∂ÂÖ∂Ë∑®ÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®ÈôÜÂú∞ÁéØÂ¢ÉËÆ≠ÁªÉÁöÑÁ≠ñÁï•Âú®ÊúàÁêÉÁéØÂ¢É‰∏≠Êó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÂç≥ÂèØËææÂà∞Êé•Ëøë50%ÁöÑÊàêÂäüÁéá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫ÜÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†(DRL)Á≠ñÁï•Âú®ËßÜËßâÂíåÂú∞ÂΩ¢‰∏äÊà™ÁÑ∂‰∏çÂêåÁöÑÊ®°ÊãüÁéØÂ¢É‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÁ≠ñÁï•Âú®ÈôÜÂú∞ÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÔºåÂπ∂Âú®Â§ñÊòüÁéØÂ¢É‰∏≠‰ª•Èõ∂Ê†∑Êú¨ÊñπÂºèËøõË°åÈ™åËØÅ„ÄÇÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÂÜú‰∏öÊº´Ê∏∏ËΩ¶ÁöÑ3DÊ®°ÊãüÁéØÂ¢ÉÔºåÂπ∂‰ΩøÁî®ËøëÁ´ØÁ≠ñÁï•‰ºòÂåñ(PPO)ËøõË°åËÆ≠ÁªÉÔºå‰ª•ÂÆûÁé∞Âú®ÂÜúÁî∞ÁéØÂ¢É‰∏≠ËøõË°åÁõÆÊ†áÂØºÂêëÂØºËà™ÂíåÈÅøÈöú„ÄÇÁÑ∂ÂêéÔºåÂú®Á±ª‰ººÊúàÁêÉÁöÑÊ®°ÊãüÁéØÂ¢É‰∏≠ËØÑ‰º∞Â≠¶‰π†Âà∞ÁöÑÁ≠ñÁï•Ôºå‰ª•ËØÑ‰º∞ËøÅÁßªÊÄßËÉΩ„ÄÇÁªìÊûúË°®ÊòéÔºåÂú®ÈôÜÂú∞Êù°‰ª∂‰∏ãËÆ≠ÁªÉÁöÑÁ≠ñÁï•‰øùÊåÅ‰∫ÜËæÉÈ´òÁöÑÊúâÊïàÊÄßÔºåÂú®ÊúàÁêÉÊ®°Êãü‰∏≠Êó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÂíåÂæÆË∞ÉÂç≥ÂèØËææÂà∞Êé•Ëøë50%ÁöÑÊàêÂäüÁéá„ÄÇËøôÁ™ÅÊòæ‰∫ÜÂü∫‰∫éDRLÁöÑË∑®ÂüüÁ≠ñÁï•ËøÅÁßª‰Ωú‰∏∫‰∏ÄÁßçÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÁöÑÊΩúÂäõÔºåÂèØ‰ª•‰∏∫Êú™Êù•ÁöÑË°åÊòüÊé¢Á¥¢‰ªªÂä°ÂºÄÂèëÈÄÇÂ∫îÊÄßÂº∫‰∏îÈ´òÊïàÁöÑËá™‰∏ªÂØºËà™ÔºåÂπ∂ÂÖ∑ÊúâÊúÄÂ§ßÈôêÂ∫¶Âú∞Èôç‰ΩéÂÜçËÆ≠ÁªÉÊàêÊú¨ÁöÑÈ¢ùÂ§ñ‰ºòÂäø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Ëá™‰∏ªÂØºËà™Âú®‰∏çÂêåÁéØÂ¢É‰∏ãÁöÑÊ≥õÂåñÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁÆóÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÈíàÂØπÁâπÂÆöÁéØÂ¢ÉËøõË°åÂ§ßÈáèË∞ÉÊï¥ÔºåÈöæ‰ª•Áõ¥Êé•Â∫îÁî®‰∫éÊñ∞ÁöÑ„ÄÅÊú™Áü•ÁöÑÁéØÂ¢ÉÔºå‰æãÂ¶Ç‰ªéÂú∞ÁêÉÂÜúÁî∞Âà∞ÊúàÁêÉË°®Èù¢„ÄÇËøôÁßçÁéØÂ¢ÉÈÄÇÂ∫îÊÄßÂ∑ÆÁöÑÈóÆÈ¢òÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫ÊäÄÊúØÂú®Ë°åÊòüÊé¢Á¥¢Á≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºàDRLÔºâËÆ≠ÁªÉ‰∏Ä‰∏™ÈÄöÁî®ÁöÑÂØºËà™Á≠ñÁï•Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑËßÜËßâÂíåÂú∞ÂΩ¢ÁéØÂ¢É„ÄÇÈÄöËøáÂú®Ê®°ÊãüÁöÑÈôÜÂú∞ÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÔºå‰ΩøÊú∫Âô®‰∫∫Â≠¶‰π†Âà∞ÈÄöÁî®ÁöÑÂØºËà™ËßÑÂàôÂíåÈÅøÈöúÁ≠ñÁï•ÔºåÁÑ∂ÂêéÂ∞ÜËøô‰∫õÁ≠ñÁï•Áõ¥Êé•ËøÅÁßªÂà∞Ê®°ÊãüÁöÑÊúàÁêÉÁéØÂ¢É‰∏≠ÔºåÊó†ÈúÄËøõË°åÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÊàñÂæÆË∞É„ÄÇËøôÁßçÊñπÊ≥ïÊó®Âú®ÂáèÂ∞ëÂØπÁéØÂ¢ÉÁâπÂÆö‰ø°ÊÅØÁöÑ‰æùËµñÔºåÊèêÈ´òÁ≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ≠•È™§Ôºö1) ÊûÑÂª∫‰∏Ä‰∏™ÂÜú‰∏öÊº´Ê∏∏ËΩ¶ÁöÑ3DÊ®°ÊãüÁéØÂ¢ÉÔºåÁî®‰∫éËÆ≠ÁªÉDRLÁ≠ñÁï•„ÄÇ2) ‰ΩøÁî®ËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÔºàPPOÔºâÁÆóÊ≥ïËÆ≠ÁªÉÊº´Ê∏∏ËΩ¶Âú®ÂÜúÁî∞ÁéØÂ¢É‰∏≠ËøõË°åÁõÆÊ†áÂØºÂêëÂØºËà™ÂíåÈÅøÈöú„ÄÇ3) ÊûÑÂª∫‰∏Ä‰∏™Á±ª‰ººÊúàÁêÉÁöÑÊ®°ÊãüÁéØÂ¢ÉÔºåÁî®‰∫éËØÑ‰º∞ËÆ≠ÁªÉÂ•ΩÁöÑÁ≠ñÁï•ÁöÑËøÅÁßªÊÄßËÉΩ„ÄÇ4) Âú®ÊúàÁêÉÁéØÂ¢É‰∏≠Ôºå‰ΩøÁî®Èõ∂Ê†∑Êú¨ËøÅÁßªÁöÑÊñπÂºèÔºåÁõ¥Êé•Â∫îÁî®Âú®ÂÜúÁî∞ÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÁöÑÁ≠ñÁï•ÔºåËØÑ‰º∞ÂÖ∂ÂØºËà™ÊàêÂäüÁéáÂíåÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÈ™åËØÅ‰∫ÜDRLÁ≠ñÁï•Âú®ËßÜËßâÂíåÂú∞ÂΩ¢Â∑ÆÂºÇÊòæËëóÁöÑÁéØÂ¢É‰πãÈó¥ÁöÑÈõ∂Ê†∑Êú¨ËøÅÁßªËÉΩÂäõ„ÄÇ‰ª•ÂæÄÁöÑDRLÁ†îÁ©∂ÈÄöÂ∏∏ÂÖ≥Ê≥®‰∫éÂú®Âêå‰∏ÄÁéØÂ¢ÉÊàñÁõ∏‰ººÁéØÂ¢É‰∏≠ÁöÑÁ≠ñÁï•ËøÅÁßªÔºåËÄåÊú¨ÊñáÂàôÊé¢Á¥¢‰∫ÜÂú®ÂÆåÂÖ®‰∏çÂêåÁöÑÁéØÂ¢ÉÔºàÂÜúÁî∞ vs. ÊúàÁêÉÔºâ‰∏ãÁöÑÁ≠ñÁï•Ê≥õÂåñËÉΩÂäõ„ÄÇËøôÁßçË∑®ÂüüËøÅÁßªËÉΩÂäõÂèØ‰ª•ÊòæËëóÈôç‰ΩéÊú∫Âô®‰∫∫ÈÉ®ÁΩ≤Âà∞Êñ∞ÁéØÂ¢ÉÊó∂ÁöÑËÆ≠ÁªÉÊàêÊú¨ÂíåÊó∂Èó¥„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰ΩøÁî®‰∫ÜProximal Policy Optimization (PPO)ÁÆóÊ≥ïËøõË°åÁ≠ñÁï•ËÆ≠ÁªÉ„ÄÇPPOÊòØ‰∏ÄÁßçÂ∏∏Áî®ÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶ÁÆóÊ≥ïÔºåÂÖ∑ÊúâËæÉÂ•ΩÁöÑÁ®≥ÂÆöÊÄßÂíåÊî∂ÊïõÊÄß„ÄÇÂÖ∑‰ΩìÁöÑÊäÄÊúØÁªÜËäÇÂåÖÊã¨ÔºöÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºåÁî®‰∫éÈºìÂä±Êú∫Âô®‰∫∫Âà∞ËææÁõÆÊ†áÂπ∂ÈÅøÂÖçÁ¢∞ÊíûÔºõÁä∂ÊÄÅÁ©∫Èó¥ÂíåÂä®‰ΩúÁ©∫Èó¥ÁöÑËÆæËÆ°ÔºåÈúÄË¶ÅËÉΩÂ§üÊúâÊïàÂú∞ÊèèËø∞Êú∫Âô®‰∫∫ÁöÑÁä∂ÊÄÅÂíåÊéßÂà∂Êú∫Âô®‰∫∫ÁöÑËøêÂä®Ôºõ‰ª•ÂèäÁ•ûÁªèÁΩëÁªúÁªìÊûÑÁöÑÈÄâÊã©ÔºåÈúÄË¶ÅËÉΩÂ§üÊúâÊïàÂú∞Â≠¶‰π†Âà∞ÂØºËà™Á≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®ÂÜúÁî∞ÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÁöÑDRLÁ≠ñÁï•Âú®ÊúàÁêÉÊ®°ÊãüÁéØÂ¢É‰∏≠ÂÆûÁé∞‰∫ÜÊé•Ëøë50%ÁöÑÂØºËà™ÊàêÂäüÁéáÔºåËÄåÊó†ÈúÄËøõË°å‰ªª‰ΩïÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÊàñÂæÆË∞É„ÄÇËøôË°®ÊòéDRLÁ≠ñÁï•ÂÖ∑ÊúâËæÉÂº∫ÁöÑË∑®ÂüüÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•Âú®ËßÜËßâÂíåÂú∞ÂΩ¢Â∑ÆÂºÇÊòæËëóÁöÑÁéØÂ¢É‰∏≠ÊúâÊïàÂ∑•‰Ωú„ÄÇËøô‰∏ÄÁªìÊûúÈ™åËØÅ‰∫ÜÂü∫‰∫éDRLÁöÑË∑®ÂüüÁ≠ñÁï•ËøÅÁßªÊñπÊ≥ïÁöÑÂèØË°åÊÄßÔºå‰∏∫Êú™Êù•ÁöÑË°åÊòüÊé¢Á¥¢‰ªªÂä°Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑Ø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éË°åÊòüÊé¢Êµã„ÄÅÂÜú‰∏öÊú∫Âô®‰∫∫„ÄÅÊêúÊïëÊú∫Âô®‰∫∫Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÈÄöÁî®ÁöÑÂØºËà™Á≠ñÁï•ÔºåÂèØ‰ª•Èôç‰ΩéÊú∫Âô®‰∫∫Âú®Êñ∞ÁéØÂ¢É‰∏≠ÁöÑÈÉ®ÁΩ≤ÊàêÊú¨ÂíåÊó∂Èó¥ÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑËá™‰∏ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÁÅ´ÊòüÊé¢Êµã„ÄÅÊ∑±Êµ∑ÂãòÊé¢Á≠âÂ§çÊùÇÁéØÂ¢ÉÔºåÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥Êô∫ËÉΩÁöÑÊú∫Âô®‰∫∫Ëá™‰∏ªÂØºËà™„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Autonomous navigation in unstructured environments is essential for field and planetary robotics, where robots must efficiently reach goals while avoiding obstacles under uncertain conditions. Conventional algorithmic approaches often require extensive environment-specific tuning, limiting scalability to new domains. Deep Reinforcement Learning (DRL) provides a data-driven alternative, allowing robots to acquire navigation strategies through direct interactions with their environment. This work investigates the feasibility of DRL policy generalization across visually and topographically distinct simulated domains, where policies are trained in terrestrial settings and validated in a zero-shot manner in extraterrestrial environments. A 3D simulation of an agricultural rover is developed and trained using Proximal Policy Optimization (PPO) to achieve goal-directed navigation and obstacle avoidance in farmland settings. The learned policy is then evaluated in a lunar-like simulated environment to assess transfer performance. The results indicate that policies trained under terrestrial conditions retain a high level of effectiveness, achieving close to 50\% success in lunar simulations without the need for additional training and fine-tuning. This underscores the potential of cross-domain DRL-based policy transfer as a promising approach to developing adaptable and efficient autonomous navigation for future planetary exploration missions, with the added benefit of minimizing retraining costs.

