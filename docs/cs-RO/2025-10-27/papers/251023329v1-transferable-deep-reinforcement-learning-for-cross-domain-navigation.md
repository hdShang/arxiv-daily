---
layout: default
title: Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon
---

# Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23329" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23329v1</a>
  <a href="https://arxiv.org/pdf/2510.23329.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23329v1" onclick="toggleFavorite(this, '2510.23329v1', 'Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shreya Santra, Thomas Robbins, Kazuya Yoshida

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

**å¤‡æ³¨**: 6 pages, 7 figures. Accepted at IEEE iSpaRo 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºDRLçš„è·¨åŸŸè¿ç§»å¯¼èˆªæ–¹æ³•ï¼Œå®ç°ä»å†œç”°åˆ°æœˆçƒçš„é›¶æ ·æœ¬æ³›åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è·¨åŸŸè¿ç§»` `è‡ªä¸»å¯¼èˆª` `æœºå™¨äºº` `è¡Œæ˜Ÿæ¢ç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¯¼èˆªç®—æ³•éœ€è¦é’ˆå¯¹ç‰¹å®šç¯å¢ƒè¿›è¡Œå¤§é‡è°ƒæ•´ï¼Œé™åˆ¶äº†å…¶åœ¨æ–°é¢†åŸŸçš„æ‰©å±•æ€§ï¼Œæ˜¯å½“å‰é¢ä¸´çš„æ ¸å¿ƒé—®é¢˜ã€‚
2. è®ºæ–‡æå‡ºåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æœºå™¨äººåœ¨ä¸ç¯å¢ƒçš„ç›´æ¥äº¤äº’ä¸­å­¦ä¹ å¯¼èˆªç­–ç•¥ï¼Œå¹¶ç ”ç©¶å…¶è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é™†åœ°ç¯å¢ƒè®­ç»ƒçš„ç­–ç•¥åœ¨æœˆçƒç¯å¢ƒä¸­æ— éœ€é¢å¤–è®­ç»ƒå³å¯è¾¾åˆ°æ¥è¿‘50%çš„æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ç ”ç©¶äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)ç­–ç•¥åœ¨è§†è§‰å’Œåœ°å½¢ä¸Šæˆªç„¶ä¸åŒçš„æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œç­–ç•¥åœ¨é™†åœ°ç¯å¢ƒä¸­è®­ç»ƒï¼Œå¹¶åœ¨å¤–æ˜Ÿç¯å¢ƒä¸­ä»¥é›¶æ ·æœ¬æ–¹å¼è¿›è¡ŒéªŒè¯ã€‚å¼€å‘äº†ä¸€ä¸ªå†œä¸šæ¼«æ¸¸è½¦çš„3Dæ¨¡æ‹Ÿç¯å¢ƒï¼Œå¹¶ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°åœ¨å†œç”°ç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡å¯¼å‘å¯¼èˆªå’Œé¿éšœã€‚ç„¶åï¼Œåœ¨ç±»ä¼¼æœˆçƒçš„æ¨¡æ‹Ÿç¯å¢ƒä¸­è¯„ä¼°å­¦ä¹ åˆ°çš„ç­–ç•¥ï¼Œä»¥è¯„ä¼°è¿ç§»æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨é™†åœ°æ¡ä»¶ä¸‹è®­ç»ƒçš„ç­–ç•¥ä¿æŒäº†è¾ƒé«˜çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æœˆçƒæ¨¡æ‹Ÿä¸­æ— éœ€é¢å¤–è®­ç»ƒå’Œå¾®è°ƒå³å¯è¾¾åˆ°æ¥è¿‘50%çš„æˆåŠŸç‡ã€‚è¿™çªæ˜¾äº†åŸºäºDRLçš„è·¨åŸŸç­–ç•¥è¿ç§»ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•çš„æ½œåŠ›ï¼Œå¯ä»¥ä¸ºæœªæ¥çš„è¡Œæ˜Ÿæ¢ç´¢ä»»åŠ¡å¼€å‘é€‚åº”æ€§å¼ºä¸”é«˜æ•ˆçš„è‡ªä¸»å¯¼èˆªï¼Œå¹¶å…·æœ‰æœ€å¤§é™åº¦åœ°é™ä½å†è®­ç»ƒæˆæœ¬çš„é¢å¤–ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººè‡ªä¸»å¯¼èˆªåœ¨ä¸åŒç¯å¢ƒä¸‹çš„æ³›åŒ–é—®é¢˜ã€‚ç°æœ‰ç®—æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šç¯å¢ƒè¿›è¡Œå¤§é‡è°ƒæ•´ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨äºæ–°çš„ã€æœªçŸ¥çš„ç¯å¢ƒï¼Œä¾‹å¦‚ä»åœ°çƒå†œç”°åˆ°æœˆçƒè¡¨é¢ã€‚è¿™ç§ç¯å¢ƒé€‚åº”æ€§å·®çš„é—®é¢˜é™åˆ¶äº†æœºå™¨äººæŠ€æœ¯åœ¨è¡Œæ˜Ÿæ¢ç´¢ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰è®­ç»ƒä¸€ä¸ªé€šç”¨çš„å¯¼èˆªç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è§†è§‰å’Œåœ°å½¢ç¯å¢ƒã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿçš„é™†åœ°ç¯å¢ƒä¸­è®­ç»ƒï¼Œä½¿æœºå™¨äººå­¦ä¹ åˆ°é€šç”¨çš„å¯¼èˆªè§„åˆ™å’Œé¿éšœç­–ç•¥ï¼Œç„¶åå°†è¿™äº›ç­–ç•¥ç›´æ¥è¿ç§»åˆ°æ¨¡æ‹Ÿçš„æœˆçƒç¯å¢ƒä¸­ï¼Œæ— éœ€è¿›è¡Œé¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å‡å°‘å¯¹ç¯å¢ƒç‰¹å®šä¿¡æ¯çš„ä¾èµ–ï¼Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š1) æ„å»ºä¸€ä¸ªå†œä¸šæ¼«æ¸¸è½¦çš„3Dæ¨¡æ‹Ÿç¯å¢ƒï¼Œç”¨äºè®­ç»ƒDRLç­–ç•¥ã€‚2) ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•è®­ç»ƒæ¼«æ¸¸è½¦åœ¨å†œç”°ç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡å¯¼å‘å¯¼èˆªå’Œé¿éšœã€‚3) æ„å»ºä¸€ä¸ªç±»ä¼¼æœˆçƒçš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„ç­–ç•¥çš„è¿ç§»æ€§èƒ½ã€‚4) åœ¨æœˆçƒç¯å¢ƒä¸­ï¼Œä½¿ç”¨é›¶æ ·æœ¬è¿ç§»çš„æ–¹å¼ï¼Œç›´æ¥åº”ç”¨åœ¨å†œç”°ç¯å¢ƒä¸­è®­ç»ƒçš„ç­–ç•¥ï¼Œè¯„ä¼°å…¶å¯¼èˆªæˆåŠŸç‡å’Œæ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºéªŒè¯äº†DRLç­–ç•¥åœ¨è§†è§‰å’Œåœ°å½¢å·®å¼‚æ˜¾è‘—çš„ç¯å¢ƒä¹‹é—´çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚ä»¥å¾€çš„DRLç ”ç©¶é€šå¸¸å…³æ³¨äºåœ¨åŒä¸€ç¯å¢ƒæˆ–ç›¸ä¼¼ç¯å¢ƒä¸­çš„ç­–ç•¥è¿ç§»ï¼Œè€Œæœ¬æ–‡åˆ™æ¢ç´¢äº†åœ¨å®Œå…¨ä¸åŒçš„ç¯å¢ƒï¼ˆå†œç”° vs. æœˆçƒï¼‰ä¸‹çš„ç­–ç•¥æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§è·¨åŸŸè¿ç§»èƒ½åŠ›å¯ä»¥æ˜¾è‘—é™ä½æœºå™¨äººéƒ¨ç½²åˆ°æ–°ç¯å¢ƒæ—¶çš„è®­ç»ƒæˆæœ¬å’Œæ—¶é—´ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä½¿ç”¨äº†Proximal Policy Optimization (PPO)ç®—æ³•è¿›è¡Œç­–ç•¥è®­ç»ƒã€‚PPOæ˜¯ä¸€ç§å¸¸ç”¨çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå…·æœ‰è¾ƒå¥½çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬ï¼šå¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼Œç”¨äºé¼“åŠ±æœºå™¨äººåˆ°è¾¾ç›®æ ‡å¹¶é¿å…ç¢°æ’ï¼›çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„è®¾è®¡ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°æè¿°æœºå™¨äººçš„çŠ¶æ€å’Œæ§åˆ¶æœºå™¨äººçš„è¿åŠ¨ï¼›ä»¥åŠç¥ç»ç½‘ç»œç»“æ„çš„é€‰æ‹©ï¼Œéœ€è¦èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ åˆ°å¯¼èˆªç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å†œç”°ç¯å¢ƒä¸­è®­ç»ƒçš„DRLç­–ç•¥åœ¨æœˆçƒæ¨¡æ‹Ÿç¯å¢ƒä¸­å®ç°äº†æ¥è¿‘50%çš„å¯¼èˆªæˆåŠŸç‡ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚è¿™è¡¨æ˜DRLç­–ç•¥å…·æœ‰è¾ƒå¼ºçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åœ¨è§†è§‰å’Œåœ°å½¢å·®å¼‚æ˜¾è‘—çš„ç¯å¢ƒä¸­æœ‰æ•ˆå·¥ä½œã€‚è¿™ä¸€ç»“æœéªŒè¯äº†åŸºäºDRLçš„è·¨åŸŸç­–ç•¥è¿ç§»æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥çš„è¡Œæ˜Ÿæ¢ç´¢ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè¡Œæ˜Ÿæ¢æµ‹ã€å†œä¸šæœºå™¨äººã€æœæ•‘æœºå™¨äººç­‰é¢†åŸŸã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒé€šç”¨çš„å¯¼èˆªç­–ç•¥ï¼Œå¯ä»¥é™ä½æœºå™¨äººåœ¨æ–°ç¯å¢ƒä¸­çš„éƒ¨ç½²æˆæœ¬å’Œæ—¶é—´ï¼Œæé«˜æœºå™¨äººçš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºç«æ˜Ÿæ¢æµ‹ã€æ·±æµ·å‹˜æ¢ç­‰å¤æ‚ç¯å¢ƒï¼Œå®ç°æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„æœºå™¨äººè‡ªä¸»å¯¼èˆªã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous navigation in unstructured environments is essential for field and planetary robotics, where robots must efficiently reach goals while avoiding obstacles under uncertain conditions. Conventional algorithmic approaches often require extensive environment-specific tuning, limiting scalability to new domains. Deep Reinforcement Learning (DRL) provides a data-driven alternative, allowing robots to acquire navigation strategies through direct interactions with their environment. This work investigates the feasibility of DRL policy generalization across visually and topographically distinct simulated domains, where policies are trained in terrestrial settings and validated in a zero-shot manner in extraterrestrial environments. A 3D simulation of an agricultural rover is developed and trained using Proximal Policy Optimization (PPO) to achieve goal-directed navigation and obstacle avoidance in farmland settings. The learned policy is then evaluated in a lunar-like simulated environment to assess transfer performance. The results indicate that policies trained under terrestrial conditions retain a high level of effectiveness, achieving close to 50\% success in lunar simulations without the need for additional training and fine-tuning. This underscores the potential of cross-domain DRL-based policy transfer as a promising approach to developing adaptable and efficient autonomous navigation for future planetary exploration missions, with the added benefit of minimizing retraining costs.

