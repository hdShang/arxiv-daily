---
layout: default
title: RoboOmni: Proactive Robot Manipulation in Omni-modal Context
---

# RoboOmni: Proactive Robot Manipulation in Omni-modal Context

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23763" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23763v3</a>
  <a href="https://arxiv.org/pdf/2510.23763.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23763v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.23763v3', 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yu-Gang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu

**åˆ†ç±»**: cs.RO, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27 (æ›´æ–°: 2025-11-01)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboOmniï¼šæå‡ºä¸€ç§å…¨æ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„ä¸»åŠ¨æœºå™¨äººæ“ä½œæ¡†æ¶ï¼Œè§£å†³æœºå™¨äººæ„å›¾ç†è§£é—®é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¤šæ¨¡æ€å­¦ä¹ ` `æ„å›¾è¯†åˆ«` `äººæœºäº¤äº’` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•ä¾èµ–æ˜¾å¼æŒ‡ä»¤ï¼Œå¿½ç•¥äº†ç°å®ä¸–ç•Œä¸­äººç±»äº¤äº’çš„éšå¼æ„å›¾è¡¨è¾¾ã€‚
2. RoboOmni æ¡†æ¶é€šè¿‡èåˆè§†è§‰ã€å¬è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œä¸»åŠ¨è¯†åˆ«ç”¨æˆ·æ„å›¾å¹¶æ‰§è¡Œç›¸åº”åŠ¨ä½œã€‚
3. OmniAction æ•°æ®é›†ä¸ºä¸»åŠ¨æ„å›¾è¯†åˆ«æä¾›äº†è®­ç»ƒæ•°æ®ï¼Œå®éªŒè¯æ˜ RoboOmni æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†ç”¨äºæœºå™¨äººæ“ä½œçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„å¿«é€Ÿå‘å±•ã€‚å°½ç®¡å½“å‰æ–¹æ³•åœ¨è®¸å¤šåœºæ™¯ä¸­æœ‰æ•ˆï¼Œä½†å®ƒä»¬å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ˜¾å¼æŒ‡ä»¤ï¼Œè€Œåœ¨ç°å®ä¸–ç•Œçš„äº¤äº’ä¸­ï¼Œäººç±»å¾ˆå°‘ç›´æ¥å‘å‡ºæŒ‡ä»¤ã€‚æœ‰æ•ˆçš„åä½œè¦æ±‚æœºå™¨äººä¸»åŠ¨æ¨æ–­ç”¨æˆ·æ„å›¾ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è·¨æ¨¡æ€ä¸Šä¸‹æ–‡æŒ‡ä»¤çš„æ–°è®¾ç½®ï¼Œå…¶ä¸­æ„å›¾æ¥æºäºå£è¯­å¯¹è¯ã€ç¯å¢ƒå£°éŸ³å’Œè§†è§‰çº¿ç´¢ï¼Œè€Œä¸æ˜¯æ˜¾å¼å‘½ä»¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªæ–°è®¾ç½®ï¼Œæˆ‘ä»¬æå‡ºäº† RoboOmniï¼Œä¸€ä¸ªåŸºäºç«¯åˆ°ç«¯å…¨æ¨¡æ€LLMçš„Perceiver-Thinker-Talker-Executoræ¡†æ¶ï¼Œå®ƒç»Ÿä¸€äº†æ„å›¾è¯†åˆ«ã€äº¤äº’ç¡®è®¤å’ŒåŠ¨ä½œæ‰§è¡Œã€‚RoboOmniåœ¨æ—¶ç©ºä¸Šèåˆå¬è§‰å’Œè§†è§‰ä¿¡å·ï¼Œä»¥å®ç°é²æ£’çš„æ„å›¾è¯†åˆ«ï¼ŒåŒæ—¶æ”¯æŒç›´æ¥è¯­éŸ³äº¤äº’ã€‚ä¸ºäº†è§£å†³æœºå™¨äººæ“ä½œä¸­ä¸»åŠ¨æ„å›¾è¯†åˆ«ç¼ºä¹è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº† OmniActionï¼ŒåŒ…å« 14 ä¸‡ä¸ª episodesï¼Œ5 åƒå¤šä¸ª speakersï¼Œ2.4 åƒä¸ª event soundsï¼Œ640 ä¸ª backgrounds å’Œå…­ç§ä¸Šä¸‹æ–‡æŒ‡ä»¤ç±»å‹ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼ŒRoboOmni åœ¨æˆåŠŸç‡ã€æ¨ç†é€Ÿåº¦ã€æ„å›¾è¯†åˆ«å’Œä¸»åŠ¨è¾…åŠ©æ–¹é¢è¶…è¶Šäº†åŸºäºæ–‡æœ¬å’Œ ASR çš„åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•ä¸»è¦ä¾èµ–äºæ˜¾å¼æŒ‡ä»¤ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†ç°å®åœºæ™¯ä¸­äººç±»é€šè¿‡è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’Œè§†è§‰çº¿ç´¢ç­‰éšå¼æ–¹å¼è¡¨è¾¾æ„å›¾çš„æƒ…å†µã€‚è¿™é™åˆ¶äº†æœºå™¨äººä¸äººç±»çš„è‡ªç„¶äº¤äº’å’Œåä½œèƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRoboOmni çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰èåˆæ¥è‡ªè§†è§‰ã€å¬è§‰å’Œè¯­è¨€é€šé“çš„ä¿¡æ¯ï¼Œä»è€Œä¸»åŠ¨æ¨æ–­ç”¨æˆ·çš„æ„å›¾ã€‚é€šè¿‡å°†æ„ŸçŸ¥ã€æ€è€ƒã€å¯¹è¯å’Œæ‰§è¡Œæ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ï¼ŒRoboOmni èƒ½å¤Ÿå®ç°æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„æœºå™¨äººæ“ä½œã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRoboOmni é‡‡ç”¨ Perceiver-Thinker-Talker-Executor æ¡†æ¶ã€‚Perceiver æ¨¡å—è´Ÿè´£ä»è§†è§‰å’Œå¬è§‰è¾“å…¥ä¸­æå–ç‰¹å¾ï¼›Thinker æ¨¡å—åˆ©ç”¨ MLLM æ¨æ–­ç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’ï¼›Talker æ¨¡å—è´Ÿè´£ä¸ç”¨æˆ·è¿›è¡Œè¯­éŸ³äº¤äº’ï¼Œç¡®è®¤æ„å›¾æˆ–æä¾›åé¦ˆï¼›Executor æ¨¡å—æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’ï¼Œæ§åˆ¶æœºå™¨äººå®Œæˆä»»åŠ¡ã€‚è¯¥æ¡†æ¶æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ï¼Œèƒ½å¤Ÿä¼˜åŒ–å„ä¸ªæ¨¡å—ä¹‹é—´çš„ååŒå·¥ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šRoboOmni çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…¨æ¨¡æ€æ„å›¾è¯†åˆ«èƒ½åŠ›å’Œä¸»åŠ¨äº¤äº’æœºåˆ¶ã€‚å®ƒä¸ä»…èƒ½å¤Ÿç†è§£æ˜¾å¼æŒ‡ä»¤ï¼Œè¿˜èƒ½ä»ä¸Šä¸‹æ–‡çº¿ç´¢ä¸­æ¨æ–­ç”¨æˆ·æ„å›¾ï¼Œå¹¶ä¸»åŠ¨ä¸ç”¨æˆ·è¿›è¡Œè¯­éŸ³äº¤äº’ï¼Œç¡®è®¤æ„å›¾æˆ–æä¾›å¸®åŠ©ã€‚æ­¤å¤–ï¼ŒOmniAction æ•°æ®é›†çš„æ„å»ºä¸ºè®­ç»ƒå’Œè¯„ä¼°ä¸»åŠ¨æ„å›¾è¯†åˆ«æ¨¡å‹æä¾›äº†é‡è¦èµ„æºã€‚

**å…³é”®è®¾è®¡**ï¼šRoboOmni ä½¿ç”¨æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶èåˆè§†è§‰å’Œå¬è§‰ä¿¡æ¯ï¼Œä»¥æé«˜æ„å›¾è¯†åˆ«çš„é²æ£’æ€§ã€‚MLLM é‡‡ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä½œä¸º backboneï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æœºå™¨äººæ“ä½œä»»åŠ¡ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ„å›¾åˆ†ç±»æŸå¤±ã€åŠ¨ä½œé¢„æµ‹æŸå¤±å’Œè¯­éŸ³ç”ŸæˆæŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„å„ä¸ªæ–¹é¢ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboOmni åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡ä¼˜äºåŸºäºæ–‡æœ¬å’Œ ASR çš„åŸºçº¿æ–¹æ³•ã€‚åœ¨æ„å›¾è¯†åˆ«æ–¹é¢ï¼ŒRoboOmni çš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒRoboOmni çš„æˆåŠŸç‡ä¹Ÿå¾—åˆ°äº†æå‡ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œèƒ½å¤Ÿæ›´åŠæ—¶åœ°å“åº”ç”¨æˆ·éœ€æ±‚ã€‚è¿™äº›ç»“æœéªŒè¯äº† RoboOmni æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RoboOmni æœ‰æœ›åº”ç”¨äºå„ç§äººæœºåä½œåœºæ™¯ï¼Œå¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€æ™ºèƒ½åŠ©æ‰‹ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚å®ƒå¯ä»¥ä½¿æœºå™¨äººæ›´æ™ºèƒ½ã€æ›´è‡ªç„¶åœ°ä¸äººç±»äº¤äº’ï¼Œä»è€Œæé«˜å·¥ä½œæ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚ä¾‹å¦‚ï¼Œåœ¨æ™ºèƒ½å®¶å±…ç¯å¢ƒä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·çš„è¯­éŸ³ã€æ‰‹åŠ¿å’Œç¯å¢ƒå£°éŸ³ï¼Œä¸»åŠ¨æä¾›å¸®åŠ©ï¼Œå¦‚é€’é€ç‰©å“ã€è°ƒèŠ‚æ¸©åº¦ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.

