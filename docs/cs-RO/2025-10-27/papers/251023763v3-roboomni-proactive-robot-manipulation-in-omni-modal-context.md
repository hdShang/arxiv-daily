---
layout: default
title: RoboOmni: Proactive Robot Manipulation in Omni-modal Context
---

# RoboOmni: Proactive Robot Manipulation in Omni-modal Context

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.23763" target="_blank" class="toolbar-btn">arXiv: 2510.23763v3</a>
    <a href="https://arxiv.org/pdf/2510.23763.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23763v3" 
            onclick="toggleFavorite(this, '2510.23763v3', 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yu-Gang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu

**ÂàÜÁ±ª**: cs.RO, cs.CL, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27 (Êõ¥Êñ∞: 2025-11-01)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**RoboOmniÔºöÊèêÂá∫‰∏ÄÁßçÂÖ®Ê®°ÊÄÅ‰∏ä‰∏ãÊñá‰∏≠ÁöÑ‰∏ªÂä®Êú∫Âô®‰∫∫Êìç‰ΩúÊ°ÜÊû∂ÔºåËß£ÂÜ≥Êú∫Âô®‰∫∫ÊÑèÂõæÁêÜËß£ÈóÆÈ¢ò„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±ÂÖ´ÔºöÁâ©ÁêÜÂä®Áîª (Physics-based Animation)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `ÊÑèÂõæËØÜÂà´` `‰∫∫Êú∫‰∫§‰∫í` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ï‰æùËµñÊòæÂºèÊåá‰ª§ÔºåÂøΩÁï•‰∫ÜÁé∞ÂÆû‰∏ñÁïå‰∏≠‰∫∫Á±ª‰∫§‰∫íÁöÑÈöêÂºèÊÑèÂõæË°®Ëææ„ÄÇ
2. RoboOmni Ê°ÜÊû∂ÈÄöËøáËûçÂêàËßÜËßâ„ÄÅÂê¨ËßâÂíåËØ≠Ë®Ä‰ø°ÊÅØÔºå‰∏ªÂä®ËØÜÂà´Áî®Êà∑ÊÑèÂõæÂπ∂ÊâßË°åÁõ∏Â∫îÂä®‰Ωú„ÄÇ
3. OmniAction Êï∞ÊçÆÈõÜ‰∏∫‰∏ªÂä®ÊÑèÂõæËØÜÂà´Êèê‰æõ‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆÔºåÂÆûÈ™åËØÅÊòé RoboOmni ÊÄßËÉΩ‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÊúÄÊñ∞ËøõÂ±ïÊé®Âä®‰∫ÜÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÂø´ÈÄüÂèëÂ±ï„ÄÇÂ∞ΩÁÆ°ÂΩìÂâçÊñπÊ≥ïÂú®ËÆ∏Â§öÂú∫ÊôØ‰∏≠ÊúâÊïàÔºå‰ΩÜÂÆÉ‰ª¨ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùËµñ‰∫éÊòæÂºèÊåá‰ª§ÔºåËÄåÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑ‰∫§‰∫í‰∏≠Ôºå‰∫∫Á±ªÂæàÂ∞ëÁõ¥Êé•ÂèëÂá∫Êåá‰ª§„ÄÇÊúâÊïàÁöÑÂçè‰ΩúË¶ÅÊ±ÇÊú∫Âô®‰∫∫‰∏ªÂä®Êé®Êñ≠Áî®Êà∑ÊÑèÂõæ„ÄÇÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçË∑®Ê®°ÊÄÅ‰∏ä‰∏ãÊñáÊåá‰ª§ÁöÑÊñ∞ËÆæÁΩÆÔºåÂÖ∂‰∏≠ÊÑèÂõæÊù•Ê∫ê‰∫éÂè£ËØ≠ÂØπËØù„ÄÅÁéØÂ¢ÉÂ£∞Èü≥ÂíåËßÜËßâÁ∫øÁ¥¢ÔºåËÄå‰∏çÊòØÊòæÂºèÂëΩ‰ª§„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™Êñ∞ËÆæÁΩÆÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü RoboOmniÔºå‰∏Ä‰∏™Âü∫‰∫éÁ´ØÂà∞Á´ØÂÖ®Ê®°ÊÄÅLLMÁöÑPerceiver-Thinker-Talker-ExecutorÊ°ÜÊû∂ÔºåÂÆÉÁªü‰∏Ä‰∫ÜÊÑèÂõæËØÜÂà´„ÄÅ‰∫§‰∫íÁ°ÆËÆ§ÂíåÂä®‰ΩúÊâßË°å„ÄÇRoboOmniÂú®Êó∂Á©∫‰∏äËûçÂêàÂê¨ËßâÂíåËßÜËßâ‰ø°Âè∑Ôºå‰ª•ÂÆûÁé∞È≤ÅÊ£íÁöÑÊÑèÂõæËØÜÂà´ÔºåÂêåÊó∂ÊîØÊåÅÁõ¥Êé•ËØ≠Èü≥‰∫§‰∫í„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠‰∏ªÂä®ÊÑèÂõæËØÜÂà´Áº∫‰πèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈóÆÈ¢òÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü OmniActionÔºåÂåÖÂê´ 14 ‰∏á‰∏™ episodesÔºå5 ÂçÉÂ§ö‰∏™ speakersÔºå2.4 ÂçÉ‰∏™ event soundsÔºå640 ‰∏™ backgrounds ÂíåÂÖ≠Áßç‰∏ä‰∏ãÊñáÊåá‰ª§Á±ªÂûã„ÄÇÂú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÂÆûÈ™åË°®ÊòéÔºåRoboOmni Âú®ÊàêÂäüÁéá„ÄÅÊé®ÁêÜÈÄüÂ∫¶„ÄÅÊÑèÂõæËØÜÂà´Âíå‰∏ªÂä®ËæÖÂä©ÊñπÈù¢Ë∂ÖË∂ä‰∫ÜÂü∫‰∫éÊñáÊú¨Âíå ASR ÁöÑÂü∫Á∫ø„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÊòæÂºèÊåá‰ª§ÔºåÊó†Ê≥ïÊúâÊïàÂ§ÑÁêÜÁé∞ÂÆûÂú∫ÊôØ‰∏≠‰∫∫Á±ªÈÄöËøáËØ≠Èü≥„ÄÅÁéØÂ¢ÉÂ£∞Èü≥ÂíåËßÜËßâÁ∫øÁ¥¢Á≠âÈöêÂºèÊñπÂºèË°®ËææÊÑèÂõæÁöÑÊÉÖÂÜµ„ÄÇËøôÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫‰∏é‰∫∫Á±ªÁöÑËá™ÁÑ∂‰∫§‰∫íÂíåÂçè‰ΩúËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöRoboOmni ÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâËûçÂêàÊù•Ëá™ËßÜËßâ„ÄÅÂê¨ËßâÂíåËØ≠Ë®ÄÈÄöÈÅìÁöÑ‰ø°ÊÅØÔºå‰ªéËÄå‰∏ªÂä®Êé®Êñ≠Áî®Êà∑ÁöÑÊÑèÂõæ„ÄÇÈÄöËøáÂ∞ÜÊÑüÁü•„ÄÅÊÄùËÄÉ„ÄÅÂØπËØùÂíåÊâßË°åÊï¥ÂêàÂà∞‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂‰∏≠ÔºåRoboOmni ËÉΩÂ§üÂÆûÁé∞Êõ¥Ëá™ÁÑ∂„ÄÅÊõ¥Êô∫ËÉΩÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRoboOmni ÈááÁî® Perceiver-Thinker-Talker-Executor Ê°ÜÊû∂„ÄÇPerceiver Ê®°ÂùóË¥üË¥£‰ªéËßÜËßâÂíåÂê¨ËßâËæìÂÖ•‰∏≠ÊèêÂèñÁâπÂæÅÔºõThinker Ê®°ÂùóÂà©Áî® MLLM Êé®Êñ≠Áî®Êà∑ÊÑèÂõæÂπ∂ÁîüÊàêË°åÂä®ËÆ°ÂàíÔºõTalker Ê®°ÂùóË¥üË¥£‰∏éÁî®Êà∑ËøõË°åËØ≠Èü≥‰∫§‰∫íÔºåÁ°ÆËÆ§ÊÑèÂõæÊàñÊèê‰æõÂèçÈ¶àÔºõExecutor Ê®°ÂùóÊâßË°åË°åÂä®ËÆ°ÂàíÔºåÊéßÂà∂Êú∫Âô®‰∫∫ÂÆåÊàê‰ªªÂä°„ÄÇËØ•Ê°ÜÊû∂ÊòØÁ´ØÂà∞Á´ØÂèØËÆ≠ÁªÉÁöÑÔºåËÉΩÂ§ü‰ºòÂåñÂêÑ‰∏™Ê®°Âùó‰πãÈó¥ÁöÑÂçèÂêåÂ∑•‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRoboOmni ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂÖ∂ÂÖ®Ê®°ÊÄÅÊÑèÂõæËØÜÂà´ËÉΩÂäõÂíå‰∏ªÂä®‰∫§‰∫íÊú∫Âà∂„ÄÇÂÆÉ‰∏ç‰ªÖËÉΩÂ§üÁêÜËß£ÊòæÂºèÊåá‰ª§ÔºåËøòËÉΩ‰ªé‰∏ä‰∏ãÊñáÁ∫øÁ¥¢‰∏≠Êé®Êñ≠Áî®Êà∑ÊÑèÂõæÔºåÂπ∂‰∏ªÂä®‰∏éÁî®Êà∑ËøõË°åËØ≠Èü≥‰∫§‰∫íÔºåÁ°ÆËÆ§ÊÑèÂõæÊàñÊèê‰æõÂ∏ÆÂä©„ÄÇÊ≠§Â§ñÔºåOmniAction Êï∞ÊçÆÈõÜÁöÑÊûÑÂª∫‰∏∫ËÆ≠ÁªÉÂíåËØÑ‰º∞‰∏ªÂä®ÊÑèÂõæËØÜÂà´Ê®°ÂûãÊèê‰æõ‰∫ÜÈáçË¶ÅËµÑÊ∫ê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöRoboOmni ‰ΩøÁî®Êó∂Á©∫Ê≥®ÊÑèÂäõÊú∫Âà∂ËûçÂêàËßÜËßâÂíåÂê¨Ëßâ‰ø°ÊÅØÔºå‰ª•ÊèêÈ´òÊÑèÂõæËØÜÂà´ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇMLLM ÈááÁî®È¢ÑËÆ≠ÁªÉÁöÑËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫ backboneÔºåÂπ∂ÈÄöËøáÂ§öÊ®°ÊÄÅÊï∞ÊçÆËøõË°åÂæÆË∞ÉÔºå‰ª•ÈÄÇÂ∫îÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°„ÄÇÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÊÑèÂõæÂàÜÁ±ªÊçüÂ§±„ÄÅÂä®‰ΩúÈ¢ÑÊµãÊçüÂ§±ÂíåËØ≠Èü≥ÁîüÊàêÊçüÂ§±ÔºåÁî®‰∫é‰ºòÂåñÊ®°ÂûãÁöÑÂêÑ‰∏™ÊñπÈù¢„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRoboOmni Âú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠Âùá‰ºò‰∫éÂü∫‰∫éÊñáÊú¨Âíå ASR ÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇÂú®ÊÑèÂõæËØÜÂà´ÊñπÈù¢ÔºåRoboOmni ÁöÑÂáÜÁ°ÆÁéáÊòæËëóÊèêÈ´ò„ÄÇÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåRoboOmni ÁöÑÊàêÂäüÁéá‰πüÂæóÂà∞‰∫ÜÊèêÂçáÔºåÂêåÊó∂Êé®ÁêÜÈÄüÂ∫¶Êõ¥Âø´ÔºåËÉΩÂ§üÊõ¥ÂèäÊó∂Âú∞ÂìçÂ∫îÁî®Êà∑ÈúÄÊ±Ç„ÄÇËøô‰∫õÁªìÊûúÈ™åËØÅ‰∫Ü RoboOmni Ê°ÜÊû∂ÁöÑÊúâÊïàÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

RoboOmni ÊúâÊúõÂ∫îÁî®‰∫éÂêÑÁßç‰∫∫Êú∫Âçè‰ΩúÂú∫ÊôØÔºåÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÊô∫ËÉΩÂä©Êâã„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÁ≠â„ÄÇÂÆÉÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Ëá™ÁÑ∂Âú∞‰∏é‰∫∫Á±ª‰∫§‰∫íÔºå‰ªéËÄåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÂíåÁî®Êà∑‰ΩìÈ™å„ÄÇ‰æãÂ¶ÇÔºåÂú®Êô∫ËÉΩÂÆ∂Â±ÖÁéØÂ¢É‰∏≠ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Ê†πÊçÆÁî®Êà∑ÁöÑËØ≠Èü≥„ÄÅÊâãÂäøÂíåÁéØÂ¢ÉÂ£∞Èü≥Ôºå‰∏ªÂä®Êèê‰æõÂ∏ÆÂä©ÔºåÂ¶ÇÈÄíÈÄÅÁâ©ÂìÅ„ÄÅË∞ÉËäÇÊ∏©Â∫¶Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.

