---
layout: default
title: Awakening Facial Emotional Expressions in Human-Robot
---

# Awakening Facial Emotional Expressions in Human-Robot

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.23059" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.23059v1</a>
  <a href="https://arxiv.org/pdf/2510.23059.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.23059v1" onclick="toggleFavorite(this, '2510.23059v1', 'Awakening Facial Emotional Expressions in Human-Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongtong Zhu, Lei Li, Iggy Qian, WenBin Zhou, Ye Yuan, Qingdu Li, Na Liu, Jianwei Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27

**å¤‡æ³¨**: Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025). 8 pages, 7 figures, IEEE two-column format

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºKANå’Œæ³¨æ„åŠ›æœºåˆ¶çš„ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºäººå½¢æœºå™¨äººè‡ªä¸»ç”Ÿæˆé¢éƒ¨è¡¨æƒ…**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `äººæœºäº¤äº’` `é¢éƒ¨è¡¨æƒ…ç”Ÿæˆ` `äººå½¢æœºå™¨äºº` `æ·±åº¦å­¦ä¹ ` `KANç½‘ç»œ` `æ³¨æ„åŠ›æœºåˆ¶` `æƒ…æ„Ÿè®¡ç®—` `å¼€æºæ•°æ®é›†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„äººå½¢æœºå™¨äººé¢éƒ¨è¡¨æƒ…ç”Ÿæˆä¾èµ–é¢„ç¼–ç¨‹æ¨¡å¼ï¼Œäººå·¥ç¼–ç æˆæœ¬é«˜æ˜‚ï¼Œç¼ºä¹è‡ªä¸»å­¦ä¹ èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºåŸºäºKANå’Œæ³¨æ„åŠ›æœºåˆ¶çš„ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿé€šè¿‡è‡ªè®­ç»ƒå­¦ä¹ ç±»äººè¡¨æƒ…ã€‚
3. æ„å»ºäº†åŸºäºä¸“å®¶ç­–ç•¥çš„è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†ç³»ç»Ÿï¼Œåˆ›å»ºäº†é¦–ä¸ªå¼€æºäººå½¢æœºå™¨äººé¢éƒ¨æ•°æ®é›†ï¼Œå®éªŒéªŒè¯äº†æ–¹æ³•çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººå½¢ç¤¾äº¤æœºå™¨äººçš„é¢éƒ¨è¡¨æƒ…ç”Ÿæˆèƒ½åŠ›å¯¹äºå®ç°è‡ªç„¶å’Œç±»äººçš„äº¤äº’è‡³å…³é‡è¦ï¼Œå®ƒåœ¨å¢å¼ºäººæœºäº¤äº’çš„æµç•…æ€§å’Œæƒ…æ„Ÿè¡¨è¾¾çš„å‡†ç¡®æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç›®å‰ï¼Œäººå½¢ç¤¾äº¤æœºå™¨äººçš„é¢éƒ¨è¡¨æƒ…ç”Ÿæˆä»ç„¶ä¾èµ–äºé¢„å…ˆç¼–ç¨‹çš„è¡Œä¸ºæ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼éœ€è¦æ‰‹åŠ¨ç¼–ç ï¼Œè€—è´¹å¤§é‡çš„äººåŠ›å’Œæ—¶é—´ã€‚ä¸ºäº†ä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿè‡ªä¸»åœ°è·å¾—é€šç”¨çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå®ƒä»¬éœ€è¦å‘å±•é€šè¿‡è‡ªæˆ‘è®­ç»ƒæ¥å­¦ä¹ ç±»äººè¡¨æƒ…çš„èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰é«˜åº¦ä»¿ç”Ÿæœºå™¨äººé¢éƒ¨ï¼Œè¯¥é¢éƒ¨å…·æœ‰ç‰©ç†ç”µå­åŠ¨ç”»é¢éƒ¨å•å…ƒï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåŸºäºKANï¼ˆKolmogorov-Arnold Networkï¼‰å’Œæ³¨æ„åŠ›æœºåˆ¶çš„ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ã€‚ä¸ä»¥å¾€çš„äººå½¢ç¤¾äº¤æœºå™¨äººä¸åŒï¼Œæˆ‘ä»¬è¿˜ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªåŸºäºé¢éƒ¨è¿åŠ¨åŸè¯­ä¸“å®¶ç­–ç•¥çš„è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†ç³»ç»Ÿæ¥æ„å»ºæ•°æ®é›†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºäººå½¢ç¤¾äº¤æœºå™¨äººçš„å¼€æºé¢éƒ¨æ•°æ®é›†ã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„æµ‹è¯•å¯¹è±¡ä¸­å®ç°äº†å‡†ç¡®å’Œå¤šæ ·åŒ–çš„é¢éƒ¨æ¨¡ä»¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººå½¢æœºå™¨äººé¢éƒ¨è¡¨æƒ…ç”Ÿæˆæ–¹æ³•ä¸»è¦ä¾èµ–äºé¢„å…ˆç¼–ç¨‹çš„è¡Œä¸ºæ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼éœ€è¦äººå·¥æ‰‹åŠ¨ç¼–ç ï¼Œå¯¼è‡´å¼€å‘æˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥æ³›åŒ–åˆ°ä¸åŒçš„ä¸ªä½“å’Œæƒ…å¢ƒã€‚å› æ­¤ï¼Œå¦‚ä½•ä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ å¹¶ç”Ÿæˆè‡ªç„¶ã€å¤šæ ·åŒ–çš„é¢éƒ¨è¡¨æƒ…æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯Kolmogorov-Arnold Network (KAN) å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ„å»ºä¸€ä¸ªç«¯åˆ°ç«¯çš„å­¦ä¹ æ¡†æ¶ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿé€šè¿‡è§‚å¯Ÿå’Œå­¦ä¹ äººç±»çš„é¢éƒ¨è¡¨æƒ…ï¼Œè‡ªä¸»åœ°ç”Ÿæˆç›¸åº”çš„è¡¨æƒ…ã€‚é€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†ç³»ç»Ÿæ„å»ºæ•°æ®é›†ï¼Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨çš„ç¹çå’Œä¸»è§‚æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é«˜åº¦ä»¿ç”Ÿçš„æœºå™¨äººé¢éƒ¨ï¼Œé…å¤‡ç‰©ç†ç”µå­åŠ¨ç”»é¢éƒ¨å•å…ƒï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»çš„é¢éƒ¨è‚Œè‚‰è¿åŠ¨ï¼›2) è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†ç³»ç»Ÿï¼ŒåŸºäºé¢éƒ¨è¿åŠ¨åŸè¯­çš„ä¸“å®¶ç­–ç•¥ï¼Œç”¨äºæ„å»ºè®­ç»ƒæ•°æ®é›†ï¼›3) åŸºäºKANå’Œæ³¨æ„åŠ›æœºåˆ¶çš„ç«¯åˆ°ç«¯å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ ä»è¾“å…¥åˆ°è¾“å‡ºé¢éƒ¨è¡¨æƒ…çš„æ˜ å°„å…³ç³»ã€‚æ•´ä¸ªæµç¨‹æ˜¯ä»æ•°æ®é‡‡é›†åˆ°æ¨¡å‹è®­ç»ƒï¼Œæœ€ç»ˆå®ç°æœºå™¨äººè‡ªä¸»ç”Ÿæˆé¢éƒ¨è¡¨æƒ…ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†åŸºäºKANå’Œæ³¨æ„åŠ›æœºåˆ¶çš„ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å’Œç”Ÿæˆé¢éƒ¨è¡¨æƒ…ï¼›2) è®¾è®¡äº†è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†ç³»ç»Ÿï¼Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨çš„æˆæœ¬å’Œä¸»è§‚æ€§ï¼›3) æ„å»ºå¹¶å¼€æºäº†é¦–ä¸ªç”¨äºäººå½¢æœºå™¨äººçš„é¢éƒ¨è¡¨æƒ…æ•°æ®é›†ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å…³äºKANå’Œæ³¨æ„åŠ›æœºåˆ¶çš„å…·ä½“ç½‘ç»œç»“æ„ã€å‚æ•°è®¾ç½®ä»¥åŠæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚æœªè¯¦ç»†æè¿°ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†ç³»ç»ŸåŸºäºé¢éƒ¨è¿åŠ¨åŸè¯­çš„ä¸“å®¶ç­–ç•¥ï¼Œå…·ä½“ç­–ç•¥ç»†èŠ‚æœªçŸ¥ã€‚æ•°æ®é›†çš„è§„æ¨¡å’Œæ„æˆä¹Ÿæœªæ˜ç¡®è¯´æ˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡æ„å»ºäº†é¦–ä¸ªå¼€æºäººå½¢æœºå™¨äººé¢éƒ¨æ•°æ®é›†ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å‡†ç¡®å’Œå¤šæ ·åŒ–çš„é¢éƒ¨æ¨¡ä»¿ï¼Œä½†å…·ä½“çš„æ€§èƒ½æŒ‡æ ‡å’Œå¯¹æ¯”åŸºçº¿æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œæå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œä¾‹å¦‚ï¼šäººæœºäº¤äº’ã€æƒ…æ„Ÿé™ªæŠ¤ã€æ•™è‚²å¨±ä¹ç­‰ã€‚é€šè¿‡ä½¿æœºå™¨äººèƒ½å¤Ÿæ›´è‡ªç„¶ã€å‡†ç¡®åœ°è¡¨è¾¾æƒ…æ„Ÿï¼Œå¯ä»¥æ˜¾è‘—æå‡äººæœºäº¤äº’çš„ä½“éªŒå’Œæ•ˆç‡ã€‚åœ¨æƒ…æ„Ÿé™ªæŠ¤é¢†åŸŸï¼Œèƒ½å¤Ÿç”Ÿæˆä¸°å¯Œé¢éƒ¨è¡¨æƒ…çš„æœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œå›åº”äººç±»çš„æƒ…æ„Ÿéœ€æ±‚ã€‚åœ¨æ•™è‚²å¨±ä¹é¢†åŸŸï¼Œå¯ä»¥å¼€å‘æ›´å…·å¸å¼•åŠ›å’Œäº’åŠ¨æ€§çš„æœºå™¨äººæ•™å­¦åŠ©æ‰‹æˆ–å¨±ä¹ä¼™ä¼´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The facial expression generation capability of humanoid social robots is critical for achieving natural and human-like interactions, playing a vital role in enhancing the fluidity of human-robot interactions and the accuracy of emotional expression. Currently, facial expression generation in humanoid social robots still relies on pre-programmed behavioral patterns, which are manually coded at high human and time costs. To enable humanoid robots to autonomously acquire generalized expressive capabilities, they need to develop the ability to learn human-like expressions through self-training. To address this challenge, we have designed a highly biomimetic robotic face with physical-electronic animated facial units and developed an end-to-end learning framework based on KAN (Kolmogorov-Arnold Network) and attention mechanisms. Unlike previous humanoid social robots, we have also meticulously designed an automated data collection system based on expert strategies of facial motion primitives to construct the dataset. Notably, to the best of our knowledge, this is the first open-source facial dataset for humanoid social robots. Comprehensive evaluations indicate that our approach achieves accurate and diverse facial mimicry across different test subjects.

