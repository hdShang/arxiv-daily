---
layout: default
title: HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment
---

# HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.22917" target="_blank" class="toolbar-btn">arXiv: 2510.22917v2</a>
    <a href="https://arxiv.org/pdf/2510.22917.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22917v2" 
            onclick="toggleFavorite(this, '2510.22917v2', 'HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Zecheng Yin, Hao Zhao, Zhen Li

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-27 (Êõ¥Êñ∞: 2025-10-28)

**Â§áÊ≥®**: under review

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**HyPerNavÔºöÂà©Áî®Ê∑∑ÂêàÊÑüÁü•ÂÆûÁé∞Êú™Áü•ÁéØÂ¢É‰∏≠Èù¢ÂêëÂØπË±°ÁöÑÂØºËà™**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)**

**ÂÖ≥ÈîÆËØç**: `Èù¢ÂêëÂØπË±°ÂØºËà™` `Ê∑∑ÂêàÊÑüÁü•` `ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã` `Êú∫Âô®‰∫∫ÂØºËà™` `Êú™Áü•ÁéØÂ¢É`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâObjNavÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñÂçï‰∏ÄÊÑüÁü•Ê®°ÊÄÅÔºàRGB-DÊàñËá™È°∂Âêë‰∏ãÂú∞ÂõæÔºâÔºåÂøΩÁï•‰∫ÜÂ±ÄÈÉ®‰ø°ÊÅØÂíåÂÖ®Â±Ä‰∏ä‰∏ãÊñáÁöÑ‰∫íË°•ÊÄß„ÄÇ
2. HyPerNavÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLMs)ËûçÂêàÊù•Ëá™RGB-D‰º†ÊÑüÂô®ÁöÑÂ±ÄÈÉ®‰ø°ÊÅØÂíåËá™È°∂Âêë‰∏ãÂú∞ÂõæÁöÑÂÖ®Â±Ä‰∏ä‰∏ãÊñáÔºåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÂØºËà™„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHyPerNavÂú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÊ∑∑ÂêàÊÑüÁü•Á≠ñÁï•ÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Èù¢ÂêëÂØπË±°ÁöÑÂØºËà™(ObjNav)‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®Êú™Áü•ÁéØÂ¢É‰∏≠Áõ¥Êé•Ëá™‰∏ªÂú∞ÂØºËà™Âà∞ÁõÆÊ†áÂØπË±°„ÄÇÂú®Êú™Áü•ÁéØÂ¢É‰∏≠ÔºåÊúâÊïàÁöÑÊÑüÁü•ÂØπ‰∫éËá™‰∏ªÊú∫Âô®‰∫∫Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊù•Ëá™RGB-D‰º†ÊÑüÂô®ÁöÑËá™Êàë‰∏≠ÂøÉËßÇÊµãÊèê‰æõ‰∏∞ÂØåÁöÑÂ±ÄÈÉ®‰ø°ÊÅØÔºåËÄåÂÆûÊó∂Ëá™È°∂Âêë‰∏ãÂú∞Âõæ‰∏∫ObjNavÊèê‰æõÊúâ‰ª∑ÂÄºÁöÑÂÖ®Â±Ä‰∏ä‰∏ãÊñá„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁ†îÁ©∂Â§ßÂ§ö‰æßÈáç‰∫éÂçï‰∏ÄÊù•Ê∫êÔºåÂæàÂ∞ëÊï¥ÂêàËøô‰∏§Áßç‰∫íË°•ÁöÑÊÑüÁü•ÊñπÂºèÔºåÂ∞ΩÁÆ°‰∫∫Á±ªËá™ÁÑ∂‰ºöÂêåÊó∂ÂÖ≥Ê≥®‰∏§ËÄÖ„ÄÇÈöèÁùÄËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLMs)ÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊ∑∑ÂêàÊÑüÁü•ÂØºËà™(HyPerNav)ÔºåÂà©Áî®VLMsÂº∫Â§ßÁöÑÊé®ÁêÜÂíåËßÜËßâ-ËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõÔºåÂÖ±ÂêåÊÑüÁü•Â±ÄÈÉ®ÂíåÂÖ®Â±Ä‰ø°ÊÅØÔºå‰ª•ÊèêÈ´òÊú™Áü•ÁéØÂ¢É‰∏≠ÂØºËà™ÁöÑÊúâÊïàÊÄßÂíåÊô∫ËÉΩÊÄß„ÄÇÂú®Â§ßÈáèÁöÑÊ®°ÊãüËØÑ‰º∞ÂíåÁúüÂÆû‰∏ñÁïåÁöÑÈ™åËØÅ‰∏≠ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÁõ∏ÂØπ‰∫éÊµÅË°åÁöÑÂü∫Á∫øÊñπÊ≥ïÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÂèóÁõä‰∫éÊ∑∑ÂêàÊÑüÁü•ÊñπÊ≥ïÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÂêåÊó∂Âà©Áî®Êù•Ëá™Ëá™Êàë‰∏≠ÂøÉËßÇÊµãÂíåËá™È°∂Âêë‰∏ãÂú∞ÂõæÁöÑ‰ø°ÊÅØÁêÜËß£ÔºåÊçïËé∑Êõ¥‰∏∞ÂØåÁöÑÁ∫øÁ¥¢Âπ∂Êõ¥ÊúâÊïàÂú∞ÊâæÂà∞ÂØπË±°„ÄÇÊàë‰ª¨ÁöÑÊ∂àËûçÁ†îÁ©∂Ëøõ‰∏ÄÊ≠•ËØÅÊòéÔºå‰ªª‰Ωï‰∏ÄÁßçÊ∑∑ÂêàÊÑüÁü•ÈÉΩÊúâÂä©‰∫éÂØºËà™ÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú™Áü•ÁéØÂ¢É‰∏≠Èù¢ÂêëÂØπË±°ÁöÑÂØºËà™ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïË¶Å‰πàÂè™ÂÖ≥Ê≥®Ëá™Êàë‰∏≠ÂøÉÁöÑRGB-DÂõæÂÉè‰ø°ÊÅØÔºåË¶Å‰πàÂè™ÂÖ≥Ê≥®Ëá™È°∂Âêë‰∏ãÁöÑÂú∞Âõæ‰ø°ÊÅØÔºåÂøΩÁï•‰∫Ü‰∏§Áßç‰ø°ÊÅØÊ∫êÁöÑ‰∫íË°•ÊÄßÔºåÂØºËá¥ÂØºËà™ÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËûçÂêàÊù•Ëá™RGB-D‰º†ÊÑüÂô®ÁöÑÂ±ÄÈÉ®ËßÜËßâ‰ø°ÊÅØÂíåËá™È°∂Âêë‰∏ãÂú∞ÂõæÁöÑÂÖ®Â±Ä‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇÈÄöËøáVLMÁöÑÂº∫Â§ßÊé®ÁêÜÂíåËßÜËßâ-ËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõÔºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÂêåÊó∂ÁêÜËß£Â±ÄÈÉ®ÁéØÂ¢ÉÂíåÂÖ®Â±ÄÂ∏ÉÂ±ÄÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞ÊâæÂà∞ÁõÆÊ†áÂØπË±°„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHyPerNavÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) RGB-D‰º†ÊÑüÂô®Ëé∑ÂèñËá™Êàë‰∏≠ÂøÉËßÜËßíÂõæÂÉèÔºõ2) SLAMÁ≥ªÁªüÊûÑÂª∫Ëá™È°∂Âêë‰∏ãÂú∞ÂõæÔºõ3) ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂ§ÑÁêÜRGB-DÂõæÂÉèÂíåËá™È°∂Âêë‰∏ãÂú∞ÂõæÔºåÊèêÂèñÁâπÂæÅÂπ∂ËøõË°åËûçÂêàÔºõ4) ÂØºËà™Á≠ñÁï•Ê®°ÂùóÔºåÊ†πÊçÆVLMÁöÑËæìÂá∫ÁªìÊûúËßÑÂàíË∑ØÂæÑÂπ∂ÊéßÂà∂Êú∫Âô®‰∫∫ËøêÂä®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫ÜÊ∑∑ÂêàÊÑüÁü•ÂØºËà™(HyPerNav)Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂È¶ñÊ¨°Â∞ÜËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®‰∫éObjNav‰ªªÂä°ÔºåÂπ∂ÊúâÊïàÂú∞ËûçÂêà‰∫ÜÂ±ÄÈÉ®ËßÜËßâ‰ø°ÊÅØÂíåÂÖ®Â±Ä‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇËøôÁßçÊ∑∑ÂêàÊÑüÁü•ÊñπÊ≥ï‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞ÁêÜËß£ÁéØÂ¢ÉÔºå‰ªéËÄåÊèêÈ´òÂØºËà™ÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåHyPerNavËÉΩÂ§üÂà©Áî®Êõ¥‰∏∞ÂØåÁöÑÁéØÂ¢É‰ø°ÊÅØÔºåÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÂØºËà™ÂÜ≥Á≠ñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫Êñá‰∏≠VLMÁöÑÂÖ∑‰ΩìÈÄâÊã©ÂíåËÆ≠ÁªÉÊñπÂºèÊòØÂÖ≥ÈîÆËÆæËÆ°‰πã‰∏Ä„ÄÇËÆ∫ÊñáÂèØËÉΩÈááÁî®‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑVLMÊ®°ÂûãÔºåÂπ∂ÈíàÂØπObjNav‰ªªÂä°ËøõË°å‰∫ÜÂæÆË∞É„ÄÇÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°‰πüËá≥ÂÖ≥ÈáçË¶ÅÔºåÂèØËÉΩÂåÖÊã¨ÂØºËà™ÊàêÂäüÁéá„ÄÅË∑ØÂæÑÈïøÂ∫¶Á≠âÊåáÊ†á„ÄÇÊ≠§Â§ñÔºåÂ¶Ç‰ΩïÊúâÊïàÂú∞ËûçÂêàRGB-DÂõæÂÉèÂíåËá™È°∂Âêë‰∏ãÂú∞ÂõæÁöÑÁâπÂæÅ‰πüÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÁöÑÊäÄÊúØÁªÜËäÇ„ÄÇÂÖ∑‰ΩìÁöÑÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Ëøõ‰∏ÄÊ≠•Êü•Êâæ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

HyPerNavÂú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢ÉÂÆûÈ™å‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰∏éÁé∞ÊúâÂü∫Á∫øÊñπÊ≥ïÁõ∏ÊØîÔºåHyPerNavËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊâæÂà∞ÁõÆÊ†áÂØπË±°ÔºåÂØºËà™ÊàêÂäüÁéáÊõ¥È´òÔºåË∑ØÂæÑÈïøÂ∫¶Êõ¥Áü≠„ÄÇÊ∂àËûçÂÆûÈ™åËØÅÊòé‰∫ÜÊ∑∑ÂêàÊÑüÁü•Á≠ñÁï•ÁöÑÊúâÊïàÊÄßÔºåÂç≥ÂêåÊó∂Âà©Áî®Â±ÄÈÉ®ËßÜËßâ‰ø°ÊÅØÂíåÂÖ®Â±Ä‰∏ä‰∏ãÊñá‰ø°ÊÅØËÉΩÂ§üÊòæËëóÊèêÈ´òÂØºËà™ÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÈúÄË¶ÅÂú®ËÆ∫Êñá‰∏≠Êü•Êâæ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HyPerNavÊäÄÊúØÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËá™‰∏ªÂØºËà™ÁöÑÂú∫ÊôØÔºåÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅ‰ªìÂÇ®Áâ©ÊµÅÊú∫Âô®‰∫∫„ÄÅÊêúÊïëÊú∫Âô®‰∫∫Á≠â„ÄÇËØ•ÊäÄÊúØËÉΩÂ§üÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÊú™Áü•ÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÔºåÈôç‰ΩéÂØπ‰∫∫Â∑•Âπ≤È¢ÑÁöÑ‰æùËµñÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂïÜ‰∏öÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Objective-oriented navigation(ObjNav) enables robot to navigate to target object directly and autonomously in an unknown environment. Effective perception in navigation in unknown environment is critical for autonomous robots. While egocentric observations from RGB-D sensors provide abundant local information, real-time top-down maps offer valuable global context for ObjNav. Nevertheless, the majority of existing studies focus on a single source, seldom integrating these two complementary perceptual modalities, despite the fact that humans naturally attend to both. With the rapid advancement of Vision-Language Models(VLMs), we propose Hybrid Perception Navigation (HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding capabilities to jointly perceive both local and global information to enhance the effectiveness and intelligence of navigation in unknown environments. In both massive simulation evaluation and real-world validation, our methods achieved state-of-the-art performance against popular baselines. Benefiting from hybrid perception approach, our method captures richer cues and finds the objects more effectively, by simultaneously leveraging information understanding from egocentric observations and the top-down map. Our ablation study further proved that either of the hybrid perception contributes to the navigation performance.

