---
layout: default
title: HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment
---

# HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.22917" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.22917v2</a>
  <a href="https://arxiv.org/pdf/2510.22917.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.22917v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.22917v2', 'HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zecheng Yin, Hao Zhao, Zhen Li

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-27 (æ›´æ–°: 2025-10-28)

**å¤‡æ³¨**: under review

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**HyPerNavï¼šåˆ©ç”¨æ··åˆæ„ŸçŸ¥å®ç°æœªçŸ¥ç¯å¢ƒä¸­é¢å‘å¯¹è±¡çš„å¯¼èˆª**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)**

**å…³é”®è¯**: `é¢å‘å¯¹è±¡å¯¼èˆª` `æ··åˆæ„ŸçŸ¥` `è§†è§‰-è¯­è¨€æ¨¡å‹` `æœºå™¨äººå¯¼èˆª` `æœªçŸ¥ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ObjNavæ–¹æ³•é€šå¸¸ä¾èµ–å•ä¸€æ„ŸçŸ¥æ¨¡æ€ï¼ˆRGB-Dæˆ–è‡ªé¡¶å‘ä¸‹åœ°å›¾ï¼‰ï¼Œå¿½ç•¥äº†å±€éƒ¨ä¿¡æ¯å’Œå…¨å±€ä¸Šä¸‹æ–‡çš„äº’è¡¥æ€§ã€‚
2. HyPerNavåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)èåˆæ¥è‡ªRGB-Dä¼ æ„Ÿå™¨çš„å±€éƒ¨ä¿¡æ¯å’Œè‡ªé¡¶å‘ä¸‹åœ°å›¾çš„å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå®ç°æ›´æœ‰æ•ˆçš„å¯¼èˆªã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHyPerNavåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†æ··åˆæ„ŸçŸ¥ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é¢å‘å¯¹è±¡çš„å¯¼èˆª(ObjNav)ä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨æœªçŸ¥ç¯å¢ƒä¸­ç›´æ¥è‡ªä¸»åœ°å¯¼èˆªåˆ°ç›®æ ‡å¯¹è±¡ã€‚åœ¨æœªçŸ¥ç¯å¢ƒä¸­ï¼Œæœ‰æ•ˆçš„æ„ŸçŸ¥å¯¹äºè‡ªä¸»æœºå™¨äººè‡³å…³é‡è¦ã€‚æ¥è‡ªRGB-Dä¼ æ„Ÿå™¨çš„è‡ªæˆ‘ä¸­å¿ƒè§‚æµ‹æä¾›ä¸°å¯Œçš„å±€éƒ¨ä¿¡æ¯ï¼Œè€Œå®æ—¶è‡ªé¡¶å‘ä¸‹åœ°å›¾ä¸ºObjNavæä¾›æœ‰ä»·å€¼çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¤§å¤šä¾§é‡äºå•ä¸€æ¥æºï¼Œå¾ˆå°‘æ•´åˆè¿™ä¸¤ç§äº’è¡¥çš„æ„ŸçŸ¥æ–¹å¼ï¼Œå°½ç®¡äººç±»è‡ªç„¶ä¼šåŒæ—¶å…³æ³¨ä¸¤è€…ã€‚éšç€è§†è§‰-è¯­è¨€æ¨¡å‹(VLMs)çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆæ„ŸçŸ¥å¯¼èˆª(HyPerNav)ï¼Œåˆ©ç”¨VLMså¼ºå¤§çš„æ¨ç†å’Œè§†è§‰-è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå…±åŒæ„ŸçŸ¥å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œä»¥æé«˜æœªçŸ¥ç¯å¢ƒä¸­å¯¼èˆªçš„æœ‰æ•ˆæ€§å’Œæ™ºèƒ½æ€§ã€‚åœ¨å¤§é‡çš„æ¨¡æ‹Ÿè¯„ä¼°å’ŒçœŸå®ä¸–ç•Œçš„éªŒè¯ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºæµè¡Œçš„åŸºçº¿æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å—ç›Šäºæ··åˆæ„ŸçŸ¥æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŒæ—¶åˆ©ç”¨æ¥è‡ªè‡ªæˆ‘ä¸­å¿ƒè§‚æµ‹å’Œè‡ªé¡¶å‘ä¸‹åœ°å›¾çš„ä¿¡æ¯ç†è§£ï¼Œæ•è·æ›´ä¸°å¯Œçš„çº¿ç´¢å¹¶æ›´æœ‰æ•ˆåœ°æ‰¾åˆ°å¯¹è±¡ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œä»»ä½•ä¸€ç§æ··åˆæ„ŸçŸ¥éƒ½æœ‰åŠ©äºå¯¼èˆªæ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœªçŸ¥ç¯å¢ƒä¸­é¢å‘å¯¹è±¡çš„å¯¼èˆªé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆåªå…³æ³¨è‡ªæˆ‘ä¸­å¿ƒçš„RGB-Då›¾åƒä¿¡æ¯ï¼Œè¦ä¹ˆåªå…³æ³¨è‡ªé¡¶å‘ä¸‹çš„åœ°å›¾ä¿¡æ¯ï¼Œå¿½ç•¥äº†ä¸¤ç§ä¿¡æ¯æºçš„äº’è¡¥æ€§ï¼Œå¯¼è‡´å¯¼èˆªæ•ˆç‡å’Œå‡†ç¡®æ€§å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èåˆæ¥è‡ªRGB-Dä¼ æ„Ÿå™¨çš„å±€éƒ¨è§†è§‰ä¿¡æ¯å’Œè‡ªé¡¶å‘ä¸‹åœ°å›¾çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡VLMçš„å¼ºå¤§æ¨ç†å’Œè§†è§‰-è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œä½¿æœºå™¨äººèƒ½å¤ŸåŒæ—¶ç†è§£å±€éƒ¨ç¯å¢ƒå’Œå…¨å±€å¸ƒå±€ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ‰¾åˆ°ç›®æ ‡å¯¹è±¡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHyPerNavçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) RGB-Dä¼ æ„Ÿå™¨è·å–è‡ªæˆ‘ä¸­å¿ƒè§†è§’å›¾åƒï¼›2) SLAMç³»ç»Ÿæ„å»ºè‡ªé¡¶å‘ä¸‹åœ°å›¾ï¼›3) è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¤„ç†RGB-Då›¾åƒå’Œè‡ªé¡¶å‘ä¸‹åœ°å›¾ï¼Œæå–ç‰¹å¾å¹¶è¿›è¡Œèåˆï¼›4) å¯¼èˆªç­–ç•¥æ¨¡å—ï¼Œæ ¹æ®VLMçš„è¾“å‡ºç»“æœè§„åˆ’è·¯å¾„å¹¶æ§åˆ¶æœºå™¨äººè¿åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†æ··åˆæ„ŸçŸ¥å¯¼èˆª(HyPerNav)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–æ¬¡å°†è§†è§‰-è¯­è¨€æ¨¡å‹åº”ç”¨äºObjNavä»»åŠ¡ï¼Œå¹¶æœ‰æ•ˆåœ°èåˆäº†å±€éƒ¨è§†è§‰ä¿¡æ¯å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§æ··åˆæ„ŸçŸ¥æ–¹æ³•ä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£ç¯å¢ƒï¼Œä»è€Œæé«˜å¯¼èˆªæ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒHyPerNavèƒ½å¤Ÿåˆ©ç”¨æ›´ä¸°å¯Œçš„ç¯å¢ƒä¿¡æ¯ï¼Œåšå‡ºæ›´æ˜æ™ºçš„å¯¼èˆªå†³ç­–ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­VLMçš„å…·ä½“é€‰æ‹©å’Œè®­ç»ƒæ–¹å¼æ˜¯å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚è®ºæ–‡å¯èƒ½é‡‡ç”¨äº†é¢„è®­ç»ƒçš„VLMæ¨¡å‹ï¼Œå¹¶é’ˆå¯¹ObjNavä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼Œå¯èƒ½åŒ…æ‹¬å¯¼èˆªæˆåŠŸç‡ã€è·¯å¾„é•¿åº¦ç­‰æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°èåˆRGB-Då›¾åƒå’Œè‡ªé¡¶å‘ä¸‹åœ°å›¾çš„ç‰¹å¾ä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®çš„æŠ€æœ¯ç»†èŠ‚ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HyPerNavåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒå®éªŒä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒHyPerNavèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ‰¾åˆ°ç›®æ ‡å¯¹è±¡ï¼Œå¯¼èˆªæˆåŠŸç‡æ›´é«˜ï¼Œè·¯å¾„é•¿åº¦æ›´çŸ­ã€‚æ¶ˆèå®éªŒè¯æ˜äº†æ··åˆæ„ŸçŸ¥ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå³åŒæ—¶åˆ©ç”¨å±€éƒ¨è§†è§‰ä¿¡æ¯å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯èƒ½å¤Ÿæ˜¾è‘—æé«˜å¯¼èˆªæ€§èƒ½ã€‚å…·ä½“æ€§èƒ½æ•°æ®éœ€è¦åœ¨è®ºæ–‡ä¸­æŸ¥æ‰¾ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HyPerNavæŠ€æœ¯å¯åº”ç”¨äºå„ç§éœ€è¦è‡ªä¸»å¯¼èˆªçš„åœºæ™¯ï¼Œå¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€ä»“å‚¨ç‰©æµæœºå™¨äººã€æœæ•‘æœºå™¨äººç­‰ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿæé«˜æœºå™¨äººåœ¨å¤æ‚æœªçŸ¥ç¯å¢ƒä¸­çš„å¯¼èˆªæ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œé™ä½å¯¹äººå·¥å¹²é¢„çš„ä¾èµ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå•†ä¸šå‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Objective-oriented navigation(ObjNav) enables robot to navigate to target object directly and autonomously in an unknown environment. Effective perception in navigation in unknown environment is critical for autonomous robots. While egocentric observations from RGB-D sensors provide abundant local information, real-time top-down maps offer valuable global context for ObjNav. Nevertheless, the majority of existing studies focus on a single source, seldom integrating these two complementary perceptual modalities, despite the fact that humans naturally attend to both. With the rapid advancement of Vision-Language Models(VLMs), we propose Hybrid Perception Navigation (HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding capabilities to jointly perceive both local and global information to enhance the effectiveness and intelligence of navigation in unknown environments. In both massive simulation evaluation and real-world validation, our methods achieved state-of-the-art performance against popular baselines. Benefiting from hybrid perception approach, our method captures richer cues and finds the objects more effectively, by simultaneously leveraging information understanding from egocentric observations and the top-down map. Our ablation study further proved that either of the hybrid perception contributes to the navigation performance.

