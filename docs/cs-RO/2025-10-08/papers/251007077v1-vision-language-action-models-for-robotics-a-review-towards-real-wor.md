---
layout: default
title: Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications
---

# Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.07077" target="_blank" class="toolbar-btn">arXiv: 2510.07077v1</a>
    <a href="https://arxiv.org/pdf/2510.07077.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07077v1" 
            onclick="toggleFavorite(this, '2510.07077v1', 'Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-08

**Â§áÊ≥®**: Accepted to IEEE Access, website: https://vla-survey.github.io

**DOI**: [10.1109/ACCESS.2025.3609980](https://doi.org/10.1109/ACCESS.2025.3609980)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÁªºËø∞ÔºöÈù¢ÂêëÁúüÂÆûÊú∫Âô®‰∫∫Â∫îÁî®ÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁ†îÁ©∂ËøõÂ±ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã` `Êú∫Âô®‰∫∫Â≠¶‰π†` `Â§öÊ®°ÊÄÅËûçÂêà` `Ê∑±Â∫¶Â≠¶‰π†` `Êú∫Âô®‰∫∫Â∫îÁî®`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫Â≠¶‰π†ÊñπÊ≥ïÈöæ‰ª•Ê≥õÂåñÂà∞‰∏çÂêå‰ªªÂä°ÂíåÁéØÂ¢ÉÔºåÈúÄË¶ÅÂ§ßÈáèÁâπÂÆö‰ªªÂä°Êï∞ÊçÆ„ÄÇ
2. VLAÊ®°ÂûãÈÄöËøáÁªü‰∏ÄËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰Ωú‰ø°ÊÅØÔºåÂ≠¶‰π†ÈÄöÁî®Á≠ñÁï•ÔºåÊèêÂçáÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ËØ•ÁªºËø∞ÂÖ®Èù¢ÂõûÈ°æVLAÁ≥ªÁªüÁöÑËΩØÁ°¨‰ª∂ÁªÑ‰ª∂Ôºå‰∏∫ÂÆûÈôÖÊú∫Âô®‰∫∫Â∫îÁî®Êèê‰æõÊåáÂØº„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÈÄöËøáÁªü‰∏ÄËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰ΩúÊï∞ÊçÆÔºåÊó®Âú®Â≠¶‰π†ËÉΩÂ§üÊ≥õÂåñÂà∞‰∏çÂêå‰ªªÂä°„ÄÅÂØπË±°„ÄÅÂΩ¢ÊÄÅÂíåÁéØÂ¢ÉÁöÑÁ≠ñÁï•Ôºå‰ªéËÄåËß£ÂÜ≥‰º†Áªü‰∏äÂàÜÂà´Á†îÁ©∂Ëøô‰∫õÊï∞ÊçÆÁöÑÈóÆÈ¢ò„ÄÇËøôÁßçÊ≥õÂåñËÉΩÂäõÊúâÊúõ‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§ü‰ª•ÊúÄÂ∞ëÊàñÊó†ÈúÄÈ¢ùÂ§ñ‰ªªÂä°ÁâπÂÆöÊï∞ÊçÆÊù•Ëß£ÂÜ≥Êñ∞ÁöÑ‰∏ãÊ∏∏‰ªªÂä°Ôºå‰ªéËÄå‰øÉËøõÊõ¥ÁÅµÊ¥ªÂíåÂèØÊâ©Â±ïÁöÑÁé∞ÂÆû‰∏ñÁïåÈÉ®ÁΩ≤„ÄÇÊú¨ÁªºËø∞Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑ„ÄÅÂÖ®Ê†àÁöÑVLAÁ≥ªÁªüÂõûÈ°æÔºåÊï¥Âêà‰∫ÜËΩØ‰ª∂ÂíåÁ°¨‰ª∂ÁªÑ‰ª∂ÔºåËÄå‰πãÂâçÁöÑÁªºËø∞‰∏ªË¶ÅÂÖ≥Ê≥®Âä®‰ΩúË°®Á§∫ÊàñÈ´òÂ±ÇÊ®°ÂûãÊû∂ÊûÑ„ÄÇÊú¨ÊñáÁ≥ªÁªüÂú∞ÂõûÈ°æ‰∫ÜVLAÔºåÊ∂µÁõñ‰∫ÜÂÆÉ‰ª¨ÁöÑÁ≠ñÁï•ÂíåÊû∂ÊûÑÊºîÂèò„ÄÅÊû∂ÊûÑÂíåÊûÑÂª∫Âùó„ÄÅÊ®°ÊÄÅÁâπÂÆöÂ§ÑÁêÜÊäÄÊúØÂíåÂ≠¶‰π†ËåÉÂºè„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜÊîØÊåÅVLAÂú®ÁúüÂÆûÊú∫Âô®‰∫∫Â∫îÁî®‰∏≠ÁöÑÈÉ®ÁΩ≤ÔºåÊàë‰ª¨ËøòÂõûÈ°æ‰∫ÜÂ∏∏Áî®ÁöÑÊú∫Âô®‰∫∫Âπ≥Âè∞„ÄÅÊï∞ÊçÆÊî∂ÈõÜÁ≠ñÁï•„ÄÅÂÖ¨ÂºÄÊï∞ÊçÆÈõÜ„ÄÅÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÂíåËØÑ‰º∞Âü∫ÂáÜ„ÄÇÊú¨ÁªºËø∞Êó®Âú®‰∏∫Êú∫Âô®‰∫∫Á§æÂå∫Âú®Â∞ÜVLAÂ∫îÁî®‰∫éÁúüÂÆûÊú∫Âô®‰∫∫Á≥ªÁªüÊó∂Êèê‰æõÂÆûÁî®ÊåáÂØº„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊú∫Âô®‰∫∫Â≠¶‰π†ÊñπÊ≥ïÈÄöÂ∏∏ÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°å‰ºòÂåñÔºåÁº∫‰πèË∑®‰ªªÂä°ÂíåÁéØÂ¢ÉÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊî∂ÈõÜÂíåÊ†áÊ≥®Â§ßÈáèÁâπÂÆö‰ªªÂä°Êï∞ÊçÆÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫Â∫îÁî®ÁöÑÊâ©Â±ïÊÄß„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰Ωï‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÂÉè‰∫∫Á±ª‰∏ÄÊ†∑ÔºåÈÄöËøáÂ∞ëÈáèÁ§∫‰æãÊàñËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÔºåÂø´ÈÄüÈÄÇÂ∫îÊñ∞‰ªªÂä°ÔºåÊòØÂΩìÂâçÊú∫Âô®‰∫∫Â≠¶‰π†È¢ÜÂüüÈù¢‰∏¥ÁöÑÂÖ≥ÈîÆÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVLAÊ®°ÂûãÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰Ωú‰ø°ÊÅØÁªü‰∏ÄÂà∞‰∏Ä‰∏™Ê®°Âûã‰∏≠ËøõË°åÂ≠¶‰π†Ôºå‰ªéËÄå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÁêÜËß£Ëá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÔºåÊÑüÁü•Âë®Âõ¥ÁéØÂ¢ÉÔºåÂπ∂ÊâßË°åÁõ∏Â∫îÁöÑÂä®‰Ωú„ÄÇÈÄöËøáÂ§ßËßÑÊ®°Êï∞ÊçÆËÆ≠ÁªÉÔºåVLAÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞ÈÄöÁî®ÁöÑÁü•ËØÜÂíåÊäÄËÉΩÔºå‰ªéËÄåÂÆûÁé∞Ë∑®‰ªªÂä°ÂíåÁéØÂ¢ÉÁöÑÊ≥õÂåñ„ÄÇËøôÁßçÊñπÊ≥ïÂÄüÈâ¥‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÊàêÂäüÁªèÈ™åÔºåÊó®Âú®ÊûÑÂª∫‰∏Ä‰∏™ÈÄöÁî®ÁöÑÊú∫Âô®‰∫∫Êô∫ËÉΩ‰Ωì„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVLAÁ≥ªÁªüÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÈÄöÂ∏∏ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÊÑüÁü•Ê®°ÂùóÔºöË¥üË¥£‰ªéËßÜËßâ‰º†ÊÑüÂô®ÔºàÂ¶ÇÊëÑÂÉèÂ§¥ÔºâËé∑ÂèñÂõæÂÉèÊàñËßÜÈ¢ëÊï∞ÊçÆÔºåÂπ∂ÊèêÂèñËßÜËßâÁâπÂæÅ„ÄÇ2) ËØ≠Ë®ÄÊ®°ÂùóÔºöË¥üË¥£Â§ÑÁêÜËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫Ê®°ÂûãÂèØ‰ª•ÁêÜËß£ÁöÑËØ≠‰πâË°®Á§∫„ÄÇ3) Âä®‰ΩúÊ®°ÂùóÔºöË¥üË¥£ÁîüÊàêÊú∫Âô®‰∫∫ÁöÑÂä®‰ΩúÊåá‰ª§ÔºåÊéßÂà∂Êú∫Âô®‰∫∫ÁöÑËøêÂä®„ÄÇ4) Á≠ñÁï•Ê®°ÂùóÔºöË¥üË¥£Ê†πÊçÆÊÑüÁü•‰ø°ÊÅØÂíåËØ≠Ë®ÄÊåá‰ª§ÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÂä®‰ΩúÁ≠ñÁï•„ÄÇËøô‰∫õÊ®°ÂùóÈÄöÂ∏∏ÈÄöËøáÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúËøõË°åÂÆûÁé∞ÔºåÂπ∂ÈÄöËøáÁ´ØÂà∞Á´ØÁöÑÊñπÂºèËøõË°åËÆ≠ÁªÉ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVLAÊ®°ÂûãÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂ∞ÜËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰Ωú‰ø°ÊÅØÁªü‰∏ÄÂà∞‰∏Ä‰∏™Ê®°Âûã‰∏≠ËøõË°åÂ≠¶‰π†„ÄÇËøôÁßçÁªü‰∏ÄÂª∫Ê®°ÊñπÊ≥ï‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•ÂêåÊó∂Â≠¶‰π†Âà∞ËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰Ωú‰πãÈó¥ÁöÑÂÖ≥ËÅîÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Âº∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÊú∫Âô®‰∫∫Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåVLAÊ®°Âûã‰∏çÈúÄË¶ÅÈíàÂØπÊØè‰∏™‰ªªÂä°ËøõË°åÂçïÁã¨ËÆ≠ÁªÉÔºåËÄåÊòØÂèØ‰ª•ÈÄöËøáÂ∞ëÈáèÁ§∫‰æãÊàñËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÔºåÂø´ÈÄüÈÄÇÂ∫îÊñ∞‰ªªÂä°„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVLAÊ®°ÂûãÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) Â§öÊ®°ÊÄÅËûçÂêàÊñπÊ≥ïÔºöÂ¶Ç‰ΩïÊúâÊïàÂú∞ËûçÂêàËßÜËßâ„ÄÅËØ≠Ë®ÄÂíåÂä®‰Ωú‰ø°ÊÅØÊòØVLAÊ®°ÂûãËÆæËÆ°ÁöÑÂÖ≥ÈîÆ„ÄÇÂ∏∏Áî®ÁöÑËûçÂêàÊñπÊ≥ïÂåÖÊã¨Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÅTransformerÁΩëÁªúÁ≠â„ÄÇ2) ÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°ÔºöÂ¶Ç‰ΩïËÆæËÆ°ÂêàÈÄÇÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†Âà∞ÈÄöÁî®ÁöÑÁü•ËØÜÂíåÊäÄËÉΩÔºåÊòØVLAÊ®°ÂûãËÆ≠ÁªÉÁöÑÂÖ≥ÈîÆ„ÄÇÂ∏∏Áî®ÁöÑÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂØπÊØîÊçüÂ§±„ÄÅ‰∫§ÂèâÁÜµÊçüÂ§±Á≠â„ÄÇ3) Êï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºöÂ¶Ç‰ΩïÈÄöËøáÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÊòØVLAÊ®°ÂûãËÆ≠ÁªÉÁöÑÈáçË¶ÅÊâãÊÆµ„ÄÇÂ∏∏Áî®ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÂåÖÊã¨ÂõæÂÉèÂ¢ûÂº∫„ÄÅÊñáÊú¨Â¢ûÂº∫Á≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÁªºËø∞ÂÖ®Èù¢ÂõûÈ°æ‰∫ÜVLAÈ¢ÜÂüüÁöÑÁ†îÁ©∂ËøõÂ±ïÔºåÂπ∂ÂØπÂ∏∏Áî®ÁöÑÊú∫Âô®‰∫∫Âπ≥Âè∞„ÄÅÊï∞ÊçÆÊî∂ÈõÜÁ≠ñÁï•„ÄÅÂÖ¨ÂºÄÊï∞ÊçÆÈõÜ„ÄÅÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÂíåËØÑ‰º∞Âü∫ÂáÜËøõË°å‰∫ÜÊÄªÁªì„ÄÇËØ•ÁªºËø∞‰∏∫Êú∫Âô®‰∫∫Á§æÂå∫Âú®Â∞ÜVLAÂ∫îÁî®‰∫éÁúüÂÆûÊú∫Âô®‰∫∫Á≥ªÁªüÊó∂Êèê‰æõ‰∫ÜÂÆûÁî®ÊåáÂØºÔºåÂπ∂ÊåáÂá∫‰∫ÜÊú™Êù•Á†îÁ©∂ÊñπÂêë„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VLAÊ®°ÂûãÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÔºöÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®ÂåñÊú∫Âô®‰∫∫„ÄÅÂåªÁñóËæÖÂä©Êú∫Âô®‰∫∫Á≠â„ÄÇÂÆÉ‰ª¨ÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£‰∫∫Á±ªÊåá‰ª§ÔºåÈÄÇÂ∫îÂ§çÊùÇÁéØÂ¢ÉÔºåÂÆåÊàêÂêÑÁßç‰ªªÂä°„ÄÇÊú™Êù•ÔºåVLAÊ®°ÂûãÊúâÊúõÊàê‰∏∫ÈÄöÁî®Êú∫Âô®‰∫∫Êô∫ËÉΩ‰ΩìÁöÑÊ†∏ÂøÉÊäÄÊúØÔºåÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .

