---
layout: default
title: DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction
---

# DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.07152" target="_blank" class="toolbar-btn">arXiv: 2510.07152v2</a>
    <a href="https://arxiv.org/pdf/2510.07152.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07152v2" 
            onclick="toggleFavorite(this, '2510.07152v2', 'DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Jingkai Sun, Gang Han, Pihai Sun, Wen Zhao, Jiahang Cao, Jiaxu Wang, Yijie Guo, Qiang Zhang

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-08 (Êõ¥Êñ∞: 2025-10-10)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DPLÊ°ÜÊû∂ÔºåÈÄöËøáÊ∑±Â∫¶‰ø°ÊÅØÂÆûÁé∞Á±ª‰∫∫Êú∫Âô®‰∫∫Âú®Â§çÊùÇÂú∞ÂΩ¢‰∏äÁöÑÁ®≥ÂÅ•ËøêÂä®**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)**

**ÂÖ≥ÈîÆËØç**: `Á±ª‰∫∫Êú∫Âô®‰∫∫` `Âú∞ÂΩ¢ÊÑüÁü•` `Ê∑±Â∫¶Â≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `Ê∑±Â∫¶ÂõæÂÉèÂêàÊàê`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÊ∑±Â∫¶ÂõæÂÉèÁöÑÁ´ØÂà∞Á´ØÂ≠¶‰π†ÊñπÊ≥ïÔºåËÆ≠ÁªÉÊïàÁéá‰ΩéÔºå‰∏îÂ≠òÂú®ËæÉÂ§ßÁöÑÊ®°ÊãüÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑÂ∑ÆË∑ù„ÄÇ
2. ËØ•ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçÊñ∞Ê°ÜÊû∂ÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÈ´òÁ®ãÂõæÊÑüÁü•ÂºïÂØºÂº∫ÂåñÂ≠¶‰π†ÔºåÂπ∂ÁªìÂêà‰∫§ÂèâÊ≥®ÊÑèÂäõTransformerÈáçÂª∫Âú∞ÂΩ¢„ÄÇ
3. ÈÄöËøáÈÄºÁúüÁöÑÊ∑±Â∫¶ÂõæÂÉèÂêàÊàêÊñπÊ≥ïÔºåÊúâÊïàÈôç‰Ωé‰∫ÜÂú∞ÂΩ¢ÈáçÂª∫ËØØÂ∑ÆÔºåÂπ∂Âú®ÂÖ®Â∞∫ÂØ∏Á±ª‰∫∫Êú∫Âô®‰∫∫‰∏äÈ™åËØÅ‰∫ÜÂÖ∂ÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÂÆûÁé∞‰ªÖ‰æùËµñÊ∑±Â∫¶‰ø°ÊÅØÁöÑÁ±ª‰∫∫Êú∫Âô®‰∫∫ÊÑüÁü•ËøêÂä®„ÄÇËØ•Ê°ÜÊû∂Á¥ßÂØÜÁªìÂêà‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºöÔºà1ÔºâÂÖ∑ÊúâÁõ≤È™®Âπ≤ÁöÑÂú∞ÂΩ¢ÊÑüÁü•ËøêÂä®Á≠ñÁï•ÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÂü∫‰∫éÈ´òÁ®ãÂõæÁöÑÊÑüÁü•Êù•ÊåáÂØºÂº∫ÂåñÂ≠¶‰π†ÔºåÂêåÊó∂ÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂáèÂ∞ëËßÜËßâËæìÂÖ•ÔºõÔºà2ÔºâÂ§öÊ®°ÊÄÅ‰∫§ÂèâÊ≥®ÊÑèÂäõTransformerÔºå‰ªéÂòàÊùÇÁöÑÊ∑±Â∫¶ÂõæÂÉè‰∏≠ÈáçÂª∫ÁªìÊûÑÂåñÁöÑÂú∞ÂΩ¢Ë°®Á§∫ÔºõÔºà3ÔºâÈÄºÁúüÁöÑÊ∑±Â∫¶ÂõæÂÉèÂêàÊàêÊñπÊ≥ïÔºåÈááÁî®Ëá™ÈÅÆÊå°ÊÑüÁü•ÂÖâÁ∫øÊäïÂ∞ÑÂíåÂô™Â£∞ÊÑüÁü•Âª∫Ê®°Êù•ÂêàÊàêÈÄºÁúüÁöÑÊ∑±Â∫¶ËßÇÊµãÔºå‰ªéËÄåÂ∞ÜÂú∞ÂΩ¢ÈáçÂª∫ËØØÂ∑ÆÈôç‰Ωé30ÔºÖ‰ª•‰∏ä„ÄÇËøôÁßçÁªÑÂêàËÉΩÂ§üÂú®ÊúâÈôêÁöÑÊï∞ÊçÆÂíåÁ°¨‰ª∂ËµÑÊ∫ê‰∏ãÂÆûÁé∞È´òÊïàÁöÑÁ≠ñÁï•ËÆ≠ÁªÉÔºåÂêåÊó∂‰øùÁïôÊ≥õÂåñÊâÄÈúÄÁöÑÂÖ≥ÈîÆÂú∞ÂΩ¢ÁâπÂæÅ„ÄÇÊàë‰ª¨Âú®ÂÖ®Â∞∫ÂØ∏Á±ª‰∫∫Êú∫Âô®‰∫∫‰∏äÈ™åËØÅ‰∫ÜËØ•Ê°ÜÊû∂ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂêÑÁßçÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂú∞ÂΩ¢‰∏äÁöÑÊïèÊç∑ÂíåËá™ÈÄÇÂ∫îËøêÂä®ËÉΩÂäõ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁ±ª‰∫∫Êú∫Âô®‰∫∫Âú∞ÂΩ¢ÊÑüÁü•ËøêÂä®ÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÊ∑±Â∫¶ÂõæÂÉèÊàñÈ´òÁ®ãÂõæ„ÄÇÂü∫‰∫éÊ∑±Â∫¶ÂõæÂÉèÁöÑÁ´ØÂà∞Á´ØÂ≠¶‰π†ÊñπÊ≥ïËÆ≠ÁªÉÊïàÁéá‰ΩéÔºå‰∏îÂ≠òÂú®Ê®°ÊãüÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑÂ∑ÆË∑ù„ÄÇÂü∫‰∫éÈ´òÁ®ãÂõæÁöÑÊñπÊ≥ï‰æùËµñ‰∫éÂ§ö‰∏™ËßÜËßâ‰º†ÊÑüÂô®ÂíåÂÆö‰ΩçÁ≥ªÁªüÔºåÂØºËá¥Âª∂ËøüÂíåÈ≤ÅÊ£íÊÄßÈôç‰Ωé„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçÊõ¥È´òÊïà„ÄÅÊõ¥È≤ÅÊ£íÁöÑ‰ªÖ‰æùËµñÊ∑±Â∫¶‰ø°ÊÅØÁöÑÁ±ª‰∫∫Êú∫Âô®‰∫∫ËøêÂä®ÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËØ•ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÈÄºÁúüÁöÑÊ∑±Â∫¶ÂõæÂÉèÂêàÊàêÊñπÊ≥ïÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆÔºåÂπ∂ÁªìÂêàÈ¢ÑËÆ≠ÁªÉÁöÑÈ´òÁ®ãÂõæÊÑüÁü•Âíå‰∫§ÂèâÊ≥®ÊÑèÂäõTransformerÔºåÂÆûÁé∞‰ªÖ‰æùËµñÊ∑±Â∫¶‰ø°ÊÅØÁöÑÁ±ª‰∫∫Êú∫Âô®‰∫∫Âú∞ÂΩ¢ÊÑüÁü•ËøêÂä®„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÂèØ‰ª•ÂáèÂ∞ëÂØπÁúüÂÆûÊï∞ÊçÆÁöÑ‰æùËµñÔºåÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÔºà1ÔºâÂú∞ÂΩ¢ÊÑüÁü•ËøêÂä®Á≠ñÁï•Ôºå‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÈ´òÁ®ãÂõæÊÑüÁü•‰Ωú‰∏∫ÂÖàÈ™åÁü•ËØÜÔºåÊåáÂØºÂº∫ÂåñÂ≠¶‰π†ÔºõÔºà2ÔºâÂ§öÊ®°ÊÄÅ‰∫§ÂèâÊ≥®ÊÑèÂäõTransformerÔºåÁî®‰∫é‰ªéÊ∑±Â∫¶ÂõæÂÉè‰∏≠ÈáçÂª∫ÁªìÊûÑÂåñÁöÑÂú∞ÂΩ¢Ë°®Á§∫ÔºõÔºà3ÔºâÈÄºÁúüÁöÑÊ∑±Â∫¶ÂõæÂÉèÂêàÊàêÊñπÊ≥ïÔºåÁî®‰∫éÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºöÈ¶ñÂÖà‰ΩøÁî®Ê∑±Â∫¶ÂõæÂÉèÂêàÊàêÊñπÊ≥ïÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆÔºåÁÑ∂Âêé‰ΩøÁî®Ëøô‰∫õÊï∞ÊçÆËÆ≠ÁªÉÂú∞ÂΩ¢ÊÑüÁü•ËøêÂä®Á≠ñÁï•Âíå‰∫§ÂèâÊ≥®ÊÑèÂäõTransformerÔºåÊúÄÂêéÂ∞ÜËÆ≠ÁªÉÂ•ΩÁöÑÁ≠ñÁï•ÈÉ®ÁΩ≤Âà∞ÁúüÂÆûÁöÑÁ±ª‰∫∫Êú∫Âô®‰∫∫‰∏ä„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºöÔºà1ÔºâÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄºÁúüÁöÑÊ∑±Â∫¶ÂõæÂÉèÂêàÊàêÊñπÊ≥ïÔºåÂèØ‰ª•ÊúâÊïàÂáèÂ∞ëÊ®°ÊãüÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑÂ∑ÆË∑ùÔºõÔºà2ÔºâÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊ®°ÊÄÅ‰∫§ÂèâÊ≥®ÊÑèÂäõTransformerÔºåÂèØ‰ª•‰ªéÂòàÊùÇÁöÑÊ∑±Â∫¶ÂõæÂÉè‰∏≠ÈáçÂª∫ÁªìÊûÑÂåñÁöÑÂú∞ÂΩ¢Ë°®Á§∫ÔºõÔºà3ÔºâÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑÈ´òÁ®ãÂõæÊÑüÁü•‰∏éÂº∫ÂåñÂ≠¶‰π†Áõ∏ÁªìÂêàÔºåÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´Âú®‰∫éÔºåËØ•ÊñπÊ≥ï‰ªÖ‰æùËµñÊ∑±Â∫¶‰ø°ÊÅØÔºåÂπ∂‰∏îËÉΩÂ§üÊúâÊïàÂáèÂ∞ëÊ®°ÊãüÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑÂ∑ÆË∑ù„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ∑±Â∫¶ÂõæÂÉèÂêàÊàêÊñπÊ≥ïÈááÁî®Ëá™ÈÅÆÊå°ÊÑüÁü•ÂÖâÁ∫øÊäïÂ∞ÑÂíåÂô™Â£∞ÊÑüÁü•Âª∫Ê®°Ôºå‰ª•ÁîüÊàêÈÄºÁúüÁöÑÊ∑±Â∫¶ËßÇÊµã„ÄÇ‰∫§ÂèâÊ≥®ÊÑèÂäõTransformer‰ΩøÁî®Ê∑±Â∫¶ÂõæÂÉèÂíåÈ¢ÑËÆ≠ÁªÉÁöÑÈ´òÁ®ãÂõæÁâπÂæÅ‰Ωú‰∏∫ËæìÂÖ•ÔºåÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ËûçÂêà‰∏§ÁßçÊ®°ÊÄÅÁöÑ‰ø°ÊÅØ„ÄÇÂú∞ÂΩ¢ÊÑüÁü•ËøêÂä®Á≠ñÁï•‰ΩøÁî®Ê∑±Â∫¶ÂõæÂÉèÈáçÂª∫ÁöÑÂú∞ÂΩ¢Ë°®Á§∫‰Ωú‰∏∫ËæìÂÖ•ÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÂæóÂà∞ÊúÄ‰ºòÁöÑËøêÂä®ÊéßÂà∂Á≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÊúâÊïàÈôç‰ΩéÂú∞ÂΩ¢ÈáçÂª∫ËØØÂ∑ÆÔºåÈôç‰ΩéÂπÖÂ∫¶Ë∂ÖËøá30%„ÄÇÂú®ÂÖ®Â∞∫ÂØ∏Á±ª‰∫∫Êú∫Âô®‰∫∫‰∏äÁöÑÂÆûÈ™åÈ™åËØÅ‰∫ÜËØ•Ê°ÜÊû∂Âú®ÂêÑÁßçÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂú∞ÂΩ¢‰∏äÁöÑÊïèÊç∑ÂíåËá™ÈÄÇÂ∫îËøêÂä®ËÉΩÂäõ„ÄÇÁõ∏ËæÉ‰∫éÂÖ∂‰ªñÊñπÊ≥ïÔºåËØ•Ê°ÜÊû∂Âú®ËÆ≠ÁªÉÊïàÁéáÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÁ±ª‰∫∫Êú∫Âô®‰∫∫Âú®Â§çÊùÇÂú∞ÂΩ¢‰∏äËøõË°åËøêÂä®ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÊêúÊïë„ÄÅÂãòÊé¢„ÄÅÁâ©ÊµÅÁ≠â„ÄÇÈÄöËøá‰ªÖ‰æùËµñÊ∑±Â∫¶‰ø°ÊÅØÔºåÂèØ‰ª•Èôç‰ΩéÂØπÁ°¨‰ª∂ÁöÑË¶ÅÊ±ÇÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑËá™‰∏ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÊõ¥ÂπøÊ≥õÁöÑÊú∫Âô®‰∫∫È¢ÜÂüüÔºå‰æãÂ¶ÇÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂåªÁñóÊú∫Âô®‰∫∫Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent advancements in legged robot perceptive locomotion have shown promising progress. However, terrain-aware humanoid locomotion remains largely constrained to two paradigms: depth image-based end-to-end learning and elevation map-based methods. The former suffers from limited training efficiency and a significant sim-to-real gap in depth perception, while the latter depends heavily on multiple vision sensors and localization systems, resulting in latency and reduced robustness. To overcome these challenges, we propose a novel framework that tightly integrates three key components: (1) Terrain-Aware Locomotion Policy with a Blind Backbone, which leverages pre-trained elevation map-based perception to guide reinforcement learning with minimal visual input; (2) Multi-Modality Cross-Attention Transformer, which reconstructs structured terrain representations from noisy depth images; (3) Realistic Depth Images Synthetic Method, which employs self-occlusion-aware ray casting and noise-aware modeling to synthesize realistic depth observations, achieving over 30\% reduction in terrain reconstruction error. This combination enables efficient policy training with limited data and hardware resources, while preserving critical terrain features essential for generalization. We validate our framework on a full-sized humanoid robot, demonstrating agile and adaptive locomotion across diverse and challenging terrains.

