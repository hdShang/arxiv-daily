---
layout: default
title: TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking
---

# TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.07134" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.07134v1</a>
  <a href="https://arxiv.org/pdf/2510.07134.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.07134v1" onclick="toggleFavorite(this, '2510.07134v1', 'TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang, He Wang

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

**å¤‡æ³¨**: Project page: https://pku-epic.github.io/TrackVLA-plus-plus-Web/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**TrackVLA++ï¼šåˆ©ç”¨VLAæ¨¡å‹ä¸­çš„æ¨ç†å’Œè®°å¿†èƒ½åŠ›å®ç°å…·èº«è§†è§‰è·Ÿè¸ª**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«è§†è§‰è·Ÿè¸ª` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `ç©ºé—´æ¨ç†` `æ—¶åºè®°å¿†` `æ€ç»´é“¾` `ç›®æ ‡è¯†åˆ«` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯­è¨€å¼•å¯¼çš„å…·èº«è§†è§‰è·Ÿè¸ªæ–¹æ³•ç¼ºä¹æ˜¾å¼çš„ç©ºé—´æ¨ç†èƒ½åŠ›å’Œæœ‰æ•ˆçš„æ—¶åºè®°å¿†æœºåˆ¶ï¼Œéš¾ä»¥åº”å¯¹é®æŒ¡å’Œç›¸ä¼¼å¹²æ‰°ã€‚
2. TrackVLA++é€šè¿‡å¼•å…¥ç©ºé—´æ¨ç†æ¨¡å—Polar-CoTå’Œç›®æ ‡è¯†åˆ«è®°å¿†æ¨¡å—TIMï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è·Ÿè¸ªèƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒTrackVLA++åœ¨EVT-Bench DT splitä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«è§†è§‰è·Ÿè¸ª(EVT)æ˜¯ä¼´ä¾£æœºå™¨äººã€å¼•å¯¼æœºå™¨äººå’ŒæœåŠ¡åŠ©æ‰‹ç­‰å®é™…åº”ç”¨çš„åŸºç¡€èƒ½åŠ›ï¼Œåœ¨è¿™äº›åº”ç”¨ä¸­ï¼ŒæŒç»­è·Ÿè¸ªç§»åŠ¨ç›®æ ‡è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„è¿›å±•ä½¿å¾—åœ¨å¤æ‚å’Œéç»“æ„åŒ–åœºæ™¯ä¸­è¿›è¡Œè¯­è¨€å¼•å¯¼çš„è·Ÿè¸ªæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æ˜¾å¼çš„ç©ºé—´æ¨ç†å’Œæœ‰æ•ˆçš„æ—¶åºè®°å¿†ï¼Œå¯¼è‡´åœ¨ä¸¥é‡é®æŒ¡æˆ–å­˜åœ¨ç›¸ä¼¼å¹²æ‰°ç‰©æ—¶å¤±æ•ˆã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TrackVLA++ï¼Œä¸€ç§æ–°å‹çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®æ¨¡å—å¢å¼ºäº†å…·èº«è§†è§‰è·Ÿè¸ªèƒ½åŠ›ï¼šç©ºé—´æ¨ç†æœºåˆ¶å’Œç›®æ ‡è¯†åˆ«è®°å¿†(TIM)ã€‚æ¨ç†æ¨¡å—å¼•å…¥äº†ä¸€ç§æ€ç»´é“¾èŒƒå¼ï¼Œç§°ä¸ºPolar-CoTï¼Œå®ƒæ¨æ–­ç›®æ ‡çš„ç›¸å¯¹ä½ç½®å¹¶å°†å…¶ç¼–ç ä¸ºç´§å‡‘çš„æåæ ‡tokenç”¨äºåŠ¨ä½œé¢„æµ‹ã€‚åœ¨è¿™äº›ç©ºé—´å…ˆéªŒçš„æŒ‡å¯¼ä¸‹ï¼ŒTIMé‡‡ç”¨é—¨æ§æ›´æ–°ç­–ç•¥æ¥ä¿æŒé•¿æ—¶ç¨‹ç›®æ ‡è®°å¿†ï¼Œç¡®ä¿æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶å‡è½»åœ¨é•¿æ—¶é—´é®æŒ¡æœŸé—´çš„ç›®æ ‡ä¸¢å¤±ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒTrackVLA++åœ¨è‡ªæˆ‘ä¸­å¿ƒå’Œå¤šæ‘„åƒå¤´è®¾ç½®ä¸‹çš„å…¬å…±åŸºå‡†æµ‹è¯•ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„EVT-Bench DT splitä¸Šï¼ŒTrackVLA++åˆ†åˆ«è¶…è¿‡äº†ä¹‹å‰çš„é¢†å…ˆæ–¹æ³•5.1å’Œ12ã€‚æ­¤å¤–ï¼ŒTrackVLA++è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨åŠ¨æ€å’Œé®æŒ¡åœºæ™¯ä¸­å®ç°é²æ£’çš„çœŸå®ä¸–ç•Œè·Ÿè¸ªã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å…·èº«è§†è§‰è·Ÿè¸ªï¼ˆEVTï¼‰ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸¥é‡é®æŒ¡æˆ–å­˜åœ¨ç›¸ä¼¼å¹²æ‰°ç‰©æ—¶ï¼Œç”±äºç¼ºä¹æ˜¾å¼çš„ç©ºé—´æ¨ç†å’Œæœ‰æ•ˆçš„æ—¶åºè®°å¿†è€Œå¯¼è‡´çš„è·Ÿè¸ªå¤±è´¥é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥ç»´æŒé•¿æ—¶é—´çš„ç›®æ ‡èº«ä»½ï¼Œå®¹æ˜“å—åˆ°ç¯å¢ƒå¹²æ‰°ï¼Œé²æ£’æ€§è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ç©ºé—´æ¨ç†æœºåˆ¶å’Œç›®æ ‡è¯†åˆ«è®°å¿†æ¨¡å—ï¼Œå¢å¼ºæ¨¡å‹å¯¹ç›®æ ‡ä½ç½®çš„ç†è§£å’Œå¯¹ç›®æ ‡èº«ä»½çš„é•¿æœŸè®°å¿†ã€‚ç©ºé—´æ¨ç†æ¨¡å—Polar-CoTç”¨äºæ¨æ–­ç›®æ ‡çš„ç›¸å¯¹ä½ç½®ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºæåæ ‡tokenï¼Œä¸ºåŠ¨ä½œé¢„æµ‹æä¾›ç©ºé—´å…ˆéªŒã€‚ç›®æ ‡è¯†åˆ«è®°å¿†æ¨¡å—TIMåˆ™ç”¨äºä¿æŒé•¿æ—¶ç¨‹ç›®æ ‡è®°å¿†ï¼Œç¡®ä¿æ—¶ç©ºä¸€è‡´æ€§ï¼Œå‡è½»é®æŒ¡å¸¦æ¥çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTrackVLA++æ¨¡å‹åŒ…å«è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¸‰ä¸ªæ¨¡æ€çš„å¤„ç†ï¼Œä»¥åŠPolar-CoTç©ºé—´æ¨ç†æ¨¡å—å’ŒTIMç›®æ ‡è¯†åˆ«è®°å¿†æ¨¡å—ã€‚æ•´ä½“æµç¨‹ä¸ºï¼šé¦–å…ˆï¼Œè§†è§‰å’Œè¯­è¨€ä¿¡æ¯è¢«ç¼–ç æˆç‰¹å¾è¡¨ç¤ºï¼›ç„¶åï¼ŒPolar-CoTæ¨¡å—æ ¹æ®è§†è§‰ç‰¹å¾æ¨ç†ç›®æ ‡ç›¸å¯¹ä½ç½®ï¼Œç”Ÿæˆæåæ ‡tokenï¼›æ¥ç€ï¼ŒTIMæ¨¡å—åˆ©ç”¨é—¨æ§æ›´æ–°ç­–ç•¥ï¼Œç»“åˆå½“å‰è§†è§‰ç‰¹å¾å’Œå†å²ç›®æ ‡è®°å¿†ï¼Œæ›´æ–°ç›®æ ‡è¡¨ç¤ºï¼›æœ€åï¼ŒåŸºäºæ›´æ–°åçš„ç›®æ ‡è¡¨ç¤ºï¼Œé¢„æµ‹åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºPolar-CoTç©ºé—´æ¨ç†æ¨¡å—å’ŒTIMç›®æ ‡è¯†åˆ«è®°å¿†æ¨¡å—çš„å¼•å…¥ã€‚Polar-CoTé€šè¿‡æ€ç»´é“¾çš„æ–¹å¼ï¼Œæ˜¾å¼åœ°æ¨ç†ç›®æ ‡ä½ç½®ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•ç¼ºä¹ç©ºé—´æ¨ç†çš„ä¸è¶³ã€‚TIMé€šè¿‡é—¨æ§æ›´æ–°ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°ä¿æŒäº†é•¿æ—¶ç¨‹ç›®æ ‡è®°å¿†ï¼Œæé«˜äº†æ¨¡å‹åœ¨é®æŒ¡æƒ…å†µä¸‹çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šPolar-CoTæ¨¡å—é‡‡ç”¨æ€ç»´é“¾çš„æ–¹å¼ï¼Œé€æ­¥æ¨ç†ç›®æ ‡ä½ç½®ï¼Œå¹¶å°†ç»“æœç¼–ç ä¸ºæåæ ‡tokenã€‚TIMæ¨¡å—é‡‡ç”¨é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ä½œä¸ºè®°å¿†å•å…ƒï¼Œå¹¶ä½¿ç”¨é—¨æ§æœºåˆ¶æ§åˆ¶ä¿¡æ¯çš„æ›´æ–°å’Œé—å¿˜ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬è·Ÿè¸ªæŸå¤±å’Œè¾…åŠ©æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

TrackVLA++åœ¨EVT-Bench DT splitä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„é¢†å…ˆæ–¹æ³•5.1å’Œ12ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„åŠ¨æ€å’Œé®æŒ¡åœºæ™¯ä¸­å®ç°é²æ£’çš„è·Ÿè¸ªã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒTrackVLA++åœ¨å…·èº«è§†è§‰è·Ÿè¸ªé¢†åŸŸå…·æœ‰é¢†å…ˆçš„æ€§èƒ½å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TrackVLA++åœ¨å…·èº«è§†è§‰è·Ÿè¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¾‹å¦‚å¯ä»¥åº”ç”¨äºä¼´ä¾£æœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿå¯é åœ°è·Ÿéšç”¨æˆ·ï¼›ä¹Ÿå¯ä»¥åº”ç”¨äºå¼•å¯¼æœºå™¨äººï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å¼•å¯¼ç”¨æˆ·åˆ°è¾¾ç›®çš„åœ°ï¼›è¿˜å¯ä»¥åº”ç”¨äºæœåŠ¡åŠ©æ‰‹ï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«å¹¶è·Ÿè¸ªç‰¹å®šç›®æ ‡ï¼Œæä¾›ä¸ªæ€§åŒ–æœåŠ¡ã€‚è¯¥ç ”ç©¶çš„æˆæœæœ‰åŠ©äºæå‡æœºå™¨äººåœ¨çœŸå®ä¸–ç•Œä¸­çš„äº¤äº’èƒ½åŠ›ï¼Œä¿ƒè¿›äººæœºåä½œã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.

