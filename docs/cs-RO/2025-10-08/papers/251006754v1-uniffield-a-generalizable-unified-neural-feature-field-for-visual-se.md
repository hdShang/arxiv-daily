---
layout: default
title: "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene"
---

# UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.06754" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.06754v1</a>
  <a href="https://arxiv.org/pdf/2510.06754.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.06754v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.06754v1', 'UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Christian Maurer, Snehal Jauhri, Sophie Lueth, Georgia Chalvatzaki

**åˆ†ç±»**: cs.RO, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-08

**å¤‡æ³¨**: Project website: https://sites.google.com/view/uniffield

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**UniFFieldï¼šé€šç”¨ã€ç»Ÿä¸€ä¸”èƒ½æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„ç¥ç»ç‰¹å¾åœºï¼Œé€‚ç”¨äºä»»æ„åœºæ™¯**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç¥ç»ç‰¹å¾åœº` `ä¸ç¡®å®šæ€§ä¼°è®¡` `æœºå™¨äººåœºæ™¯ç†è§£` `é€šç”¨åœºæ™¯è¡¨ç¤º` `ä¸»åŠ¨å¯¹è±¡æœç´¢`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰3Dç¥ç»ç‰¹å¾åœºæ–¹æ³•é€šå¸¸æ˜¯åœºæ™¯ç‰¹å®šçš„ï¼Œæ³›åŒ–èƒ½åŠ›å¼±ï¼Œéš¾ä»¥é€‚åº”æ–°ç¯å¢ƒã€‚
2. UniFFieldé€šè¿‡ç»Ÿä¸€çš„ç¥ç»ç‰¹å¾åœºè¡¨ç¤ºï¼Œæ•´åˆè§†è§‰ã€è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œå¹¶é¢„æµ‹å„æ¨¡æ€çš„ä¸ç¡®å®šæ€§ï¼Œæå‡äº†æ³›åŒ–æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒUniFFieldèƒ½å‡†ç¡®ä¼°è®¡æ¨¡å‹é¢„æµ‹è¯¯å·®ï¼Œå¹¶æˆåŠŸåº”ç”¨äºç§»åŠ¨æ“ä½œæœºå™¨äººçš„ä¸»åŠ¨å¯¹è±¡æœç´¢ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºUniFFieldï¼Œä¸€ç§ç»Ÿä¸€çš„ã€èƒ½æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„ç¥ç»ç‰¹å¾åœºï¼Œå®ƒå°†è§†è§‰ã€è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾æ•´åˆåˆ°ä¸€ä¸ªé€šç”¨çš„è¡¨ç¤ºä¸­ï¼ŒåŒæ—¶é¢„æµ‹æ¯ç§æ¨¡æ€çš„ä¸ç¡®å®šæ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé›¶æ ·æœ¬åº”ç”¨äºä»»ä½•æ–°ç¯å¢ƒï¼Œå¹¶åœ¨æœºå™¨äººæ¢ç´¢åœºæ™¯æ—¶ï¼Œé€æ­¥å°†RGB-Då›¾åƒé›†æˆåˆ°åŸºäºä½“ç´ çš„ç‰¹å¾è¡¨ç¤ºä¸­ï¼ŒåŒæ—¶æ›´æ–°ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚è®ºæ–‡è¯„ä¼°äº†ä¸ç¡®å®šæ€§ä¼°è®¡åœ¨åœºæ™¯é‡å»ºå’Œè¯­ä¹‰ç‰¹å¾é¢„æµ‹ä¸­å‡†ç¡®æè¿°æ¨¡å‹é¢„æµ‹è¯¯å·®çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒæˆåŠŸåœ°åˆ©ç”¨ç‰¹å¾é¢„æµ‹åŠå…¶å„è‡ªçš„ä¸ç¡®å®šæ€§ï¼Œé€šè¿‡ç§»åŠ¨æ“ä½œæœºå™¨äººæ‰§è¡Œä¸»åŠ¨å¯¹è±¡æœç´¢ä»»åŠ¡ï¼Œå±•ç¤ºäº†é²æ£’å†³ç­–çš„èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºç¥ç»ç‰¹å¾åœºçš„æœºå™¨äººåœºæ™¯ç†è§£æ–¹æ³•é€šå¸¸æ˜¯åœºæ™¯ç‰¹å®šçš„ï¼Œå³éœ€è¦é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œè®­ç»ƒï¼Œæ³›åŒ–èƒ½åŠ›å·®ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å¿½ç•¥äº†é¢„æµ‹ç»“æœçš„ä¸ç¡®å®šæ€§ï¼Œè¿™åœ¨å¤æ‚å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­ä¼šå½±å“æœºå™¨äººçš„å†³ç­–é²æ£’æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§é€šç”¨çš„ã€èƒ½å¤Ÿæ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥æ”¯æŒæœºå™¨äººåœ¨ä»»æ„ç¯å¢ƒä¸­çš„é²æ£’æ“ä½œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šUniFFieldçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è§†è§‰ã€è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾èåˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç¥ç»ç‰¹å¾åœºä¸­ï¼Œå¹¶åŒæ—¶é¢„æµ‹æ¯ä¸ªæ¨¡æ€çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹ä¸ä»…å¯ä»¥é¢„æµ‹åœºæ™¯çš„å„ç§å±æ€§ï¼Œè¿˜å¯ä»¥è¯„ä¼°é¢„æµ‹çš„å¯é æ€§ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å™ªå£°å’Œä¸ç¡®å®šæ€§ï¼Œä»è€Œæé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨ä½“ç´ åŒ–çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¯ä»¥å¢é‡å¼åœ°èåˆæ–°çš„RGB-Då›¾åƒä¿¡æ¯ï¼Œé€‚åº”åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šUniFFieldçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç‰¹å¾æå–æ¨¡å—ï¼šä»RGB-Då›¾åƒä¸­æå–è§†è§‰ã€è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ã€‚2) ä½“ç´ åŒ–ç‰¹å¾è¡¨ç¤ºæ¨¡å—ï¼šå°†æå–çš„ç‰¹å¾å­˜å‚¨åœ¨ä½“ç´ åŒ–çš„ä¸‰ç»´ç©ºé—´ä¸­ï¼Œå½¢æˆç¥ç»ç‰¹å¾åœºã€‚3) ä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—ï¼šé¢„æµ‹æ¯ä¸ªæ¨¡æ€ç‰¹å¾çš„ä¸ç¡®å®šæ€§ã€‚4) èåˆæ¨¡å—ï¼šå°†ä¸åŒæ¨¡æ€çš„ç‰¹å¾å’Œä¸ç¡®å®šæ€§ä¿¡æ¯è¿›è¡Œèåˆï¼Œå¾—åˆ°ç»Ÿä¸€çš„åœºæ™¯è¡¨ç¤ºã€‚æœºå™¨äººé€šè¿‡ä¸æ–­æ¢ç´¢ç¯å¢ƒï¼Œè·å–æ–°çš„RGB-Då›¾åƒï¼Œå¹¶å°†å…¶å¢é‡å¼åœ°èåˆåˆ°ç‰¹å¾åœºä¸­ï¼ŒåŒæ—¶æ›´æ–°ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šUniFFieldçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶è¡¨ç¤ºè§†è§‰ã€è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ï¼Œå¹¶é¢„æµ‹å®ƒä»¬çš„ä¸ç¡®å®šæ€§ã€‚2) å®ç°äº†é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå³æ¨¡å‹å¯ä»¥ç›´æ¥åº”ç”¨äºæ–°çš„ç¯å¢ƒï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚3) é‡‡ç”¨äº†å¢é‡å¼æ›´æ–°ç­–ç•¥ï¼Œèƒ½å¤Ÿé€‚åº”åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šUniFFieldé‡‡ç”¨åŸºäºä½“ç´ çš„ç‰¹å¾è¡¨ç¤ºï¼Œæ¯ä¸ªä½“ç´ å­˜å‚¨äº†è§†è§‰ã€è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ä»¥åŠå¯¹åº”çš„ä¸ç¡®å®šæ€§ã€‚ä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—å¯èƒ½é‡‡ç”¨äº†å˜åˆ†æ¨æ–­æˆ–Dropoutç­‰æŠ€æœ¯ã€‚æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬é‡å»ºæŸå¤±ã€è¯­ä¹‰åˆ†å‰²æŸå¤±å’Œä¸ç¡®å®šæ€§æŸå¤±ï¼Œç”¨äºä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºå’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰è¯¦ç»†æè¿°ï¼ˆæœªçŸ¥ï¼‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†UniFFieldåœ¨åœºæ™¯é‡å»ºå’Œè¯­ä¹‰ç‰¹å¾é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶è¯„ä¼°äº†ä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniFFieldèƒ½å¤Ÿæœ‰æ•ˆåœ°æè¿°æ¨¡å‹é¢„æµ‹è¯¯å·®ï¼Œå¹¶æˆåŠŸåº”ç”¨äºç§»åŠ¨æ“ä½œæœºå™¨äººçš„ä¸»åŠ¨å¯¹è±¡æœç´¢ä»»åŠ¡ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿ï¼ˆæœªçŸ¥ï¼‰ï¼Œä½†æ•´ä½“ç»“æœè¡¨æ˜UniFFieldå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

UniFFieldåœ¨æœºå™¨äººå¯¼èˆªã€æ“ä½œå’Œåœºæ™¯ç†è§£ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥åº”ç”¨äºç§»åŠ¨æœºå™¨äººçš„è‡ªä¸»æ¢ç´¢å’Œå»ºå›¾ï¼Œå¸®åŠ©æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­å®‰å…¨æœ‰æ•ˆåœ°å¯¼èˆªã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥ç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–å’Œæ”¾ç½®ï¼Œæé«˜æ“ä½œçš„ç²¾åº¦å’Œé²æ£’æ€§ã€‚è¯¥ç ”ç©¶å¯¹äºæå‡æœºå™¨äººåœ¨å¤æ‚å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­çš„è‡ªä¸»èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Comprehensive visual, geometric, and semantic understanding of a 3D scene is crucial for successful execution of robotic tasks, especially in unstructured and complex environments. Additionally, to make robust decisions, it is necessary for the robot to evaluate the reliability of perceived information. While recent advances in 3D neural feature fields have enabled robots to leverage features from pretrained foundation models for tasks such as language-guided manipulation and navigation, existing methods suffer from two critical limitations: (i) they are typically scene-specific, and (ii) they lack the ability to model uncertainty in their predictions. We present UniFField, a unified uncertainty-aware neural feature field that combines visual, semantic, and geometric features in a single generalizable representation while also predicting uncertainty in each modality. Our approach, which can be applied zero shot to any new environment, incrementally integrates RGB-D images into our voxel-based feature representation as the robot explores the scene, simultaneously updating uncertainty estimation. We evaluate our uncertainty estimations to accurately describe the model prediction errors in scene reconstruction and semantic feature prediction. Furthermore, we successfully leverage our feature predictions and their respective uncertainty for an active object search task using a mobile manipulator robot, demonstrating the capability for robust decision-making.

