---
layout: default
title: HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward
---

# HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.15679" target="_blank" class="toolbar-btn">arXiv: 2510.15679v1</a>
    <a href="https://arxiv.org/pdf/2510.15679.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15679v1" 
            onclick="toggleFavorite(this, '2510.15679v1', 'HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Yuhong Cao, Yizhuo Wang, Jingsong Liang, Shuhao Liao, Yifeng Zhang, Peizhuo Li, Guillaume Sartoretti

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-17

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**HEADERÔºöÂü∫‰∫éÊ≥®ÊÑèÂäõÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Âíå‰∏ìÂÆ∂ÂºïÂØºÂ•ñÂä±ÁöÑÂàÜÂ±ÇÊú∫Âô®‰∫∫Êé¢Á¥¢ÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êé¢Á¥¢` `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `Ê≥®ÊÑèÂäõÊú∫Âà∂` `ÂàÜÂ±ÇÂõæ` `Ëá™‰∏ªÂØºËà™`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫éÂ≠¶‰π†ÁöÑÊú∫Âô®‰∫∫Ëá™‰∏ªÊé¢Á¥¢ÊñπÊ≥ïÂú®Â§ßÂûãÂ§çÊùÇÁéØÂ¢É‰∏≠Èù¢‰∏¥ÂèØÊâ©Â±ïÊÄßÂíåÊé¢Á¥¢ÊïàÁéáÁöÑÊåëÊàò„ÄÇ
2. HEADERÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÁªìÂêàÂàÜÂ±ÇÂõæË°®Á§∫Âíå‰∏ìÂÆ∂ÂºïÂØºÂ•ñÂä±ÔºåÊèêÂçáÊé¢Á¥¢ÊïàÁéá„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHEADERÂú®Ê®°ÊãüÂíåÁúüÂÆûÁéØÂ¢É‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊé¢Á¥¢ÊïàÁéáÊèêÂçáÈ´òËææ20%ÔºåÂπ∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊó®Âú®ÊèêÂçáÂü∫‰∫éÂ≠¶‰π†ÁöÑËá™‰∏ªÊú∫Âô®‰∫∫Êé¢Á¥¢ÊñπÊ≥ïÂú®ÁéØÂ¢ÉËßÑÊ®°ÂíåÊé¢Á¥¢ÊïàÁéáÊñπÈù¢ÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜHEADERÔºå‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂà©Áî®ÂàÜÂ±ÇÂõæËøõË°åÂ§ßËßÑÊ®°ÁéØÂ¢É‰∏≠ÁöÑÈ´òÊïàÊé¢Á¥¢„ÄÇHEADERÊ≤øÁî®‰º†ÁªüÊñπÊ≥ïÊûÑÂª∫Êú∫Âô®‰∫∫ÁΩÆ‰ø°Â∫¶/Âú∞ÂõæÁöÑÂàÜÂ±ÇË°®Á§∫ÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÁ§æÂå∫ÁöÑÁÆóÊ≥ïÊù•ÊûÑÂª∫ÂíåÊõ¥Êñ∞ÂÖ®Â±ÄÂõæÔºåËØ•ÁÆóÊ≥ï‰øùÊåÅÂÆåÂÖ®Â¢ûÈáèÂºè„ÄÅÂΩ¢Áä∂Ëá™ÈÄÇÂ∫îÔºåÂπ∂‰ª•Á∫øÊÄßÂ§çÊùÇÂ∫¶ËøêË°å„ÄÇÊàë‰ª¨ÁöÑËßÑÂàíÂô®Âü∫‰∫éÊ≥®ÊÑèÂäõÁΩëÁªúÔºåËÉΩÂ§üÁ≤æÁªÜÂú∞Êé®ÁêÜÂ±ÄÈÉ®ËåÉÂõ¥ÂÜÖÁöÑÁΩÆ‰ø°Â∫¶ÔºåÂπ∂Á≤óÁï•Âú∞Âà©Áî®ÂÖ®Â±ÄËåÉÂõ¥ÂÜÖÁöÑËøúË∑ùÁ¶ª‰ø°ÊÅØÔºå‰ªéËÄåÂÅöÂá∫ËÄÉËôëÂ§öÂ∞∫Â∫¶Á©∫Èó¥‰æùËµñÊÄßÁöÑÊúÄ‰Ω≥ËßÜÁÇπÂÜ≥Á≠ñ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊó†ÂèÇÊï∞ÁöÑÁâπÊùÉÂ•ñÂä±ÔºåÈÄöËøáÈÅøÂÖçÊâãÂ∑•ËÆæËÆ°ÁöÑÂ•ñÂä±Â°ëÈÄ†ÈÄ†ÊàêÁöÑËÆ≠ÁªÉÁõÆÊ†áÂÅèÂ∑ÆÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩÂπ∂‰∫ßÁîü‰∫ÜÊé•ËøëÊúÄ‰ºòÁöÑÊé¢Á¥¢Ë°å‰∏∫„ÄÇÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂ§ßËßÑÊ®°Êé¢Á¥¢Ê®°ÊãüÂú∫ÊôØ‰∏≠ÔºåHEADERÂ±ïÁ§∫‰∫ÜÊØîÂ§ßÂ§öÊï∞Áé∞ÊúâÂ≠¶‰π†ÂíåÈùûÂ≠¶‰π†ÊñπÊ≥ïÊõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄßÔºåÂêåÊó∂Âú®Êé¢Á¥¢ÊïàÁéáÊñπÈù¢ÊØîÊúÄÂÖàËøõÁöÑÂü∫Á∫øÊèêÈ´ò‰∫ÜÈ´òËææ20%„ÄÇÊàë‰ª¨ËøòÂú®Á°¨‰ª∂‰∏äÈÉ®ÁΩ≤‰∫ÜHEADERÔºåÂπ∂Âú®Â§çÊùÇÁöÑ„ÄÅÂ§ßËßÑÊ®°ÁöÑÁúüÂÆûÂú∫ÊôØ‰∏≠ÂØπÂÖ∂ËøõË°å‰∫ÜÈ™åËØÅÔºåÂåÖÊã¨‰∏Ä‰∏™300m*230mÁöÑÊ†°Âõ≠ÁéØÂ¢É„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂü∫‰∫éÂ≠¶‰π†ÁöÑÊú∫Âô®‰∫∫Ëá™‰∏ªÊé¢Á¥¢ÊñπÊ≥ïÈöæ‰ª•ÂÖºÈ°æÂ§ßËßÑÊ®°ÁéØÂ¢ÉÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊé¢Á¥¢ÊïàÁéá„ÄÇ‰º†ÁªüÊñπÊ≥ï‰æùËµñÊâãÂ∑•ËÆæËÆ°ÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÂÆπÊòì‰∫ßÁîüÂÅèÂ∑ÆÔºåÂØºËá¥Ê¨°‰ºòÁöÑÊé¢Á¥¢Ë°å‰∏∫„ÄÇÊ≠§Â§ñÔºåÂ¶Ç‰ΩïÊúâÊïàÂú∞Âà©Áî®ÂÖ®Â±Ä‰ø°ÊÅØËøõË°åÂ±ÄÈÉ®ÂÜ≥Á≠ñ‰πüÊòØ‰∏Ä‰∏™ÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöHEADERÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®ÂàÜÂ±ÇÂõæË°®Á§∫ÁéØÂ¢ÉÔºåÂπ∂ÁªìÂêàÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Ôºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÂêåÊó∂ËÄÉËôëÂ±ÄÈÉ®ÂíåÂÖ®Â±Ä‰ø°ÊÅØÔºåÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÊé¢Á¥¢ÂÜ≥Á≠ñ„ÄÇÈÄöËøáÂºïÂÖ•Êó†ÂèÇÊï∞ÁöÑÁâπÊùÉÂ•ñÂä±ÔºåÈÅøÂÖç‰∫ÜÊâãÂ∑•ËÆæËÆ°Â•ñÂä±ÂáΩÊï∞Â∏¶Êù•ÁöÑÂÅèÂ∑ÆÔºå‰ªéËÄåÊèêÂçá‰∫ÜÊé¢Á¥¢ÊïàÁéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHEADERÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÂàÜÂ±ÇÂõæÊûÑÂª∫Ê®°ÂùóÔºåÁî®‰∫éÊûÑÂª∫ÂíåÊõ¥Êñ∞ÁéØÂ¢ÉÁöÑÂàÜÂ±ÇË°®Á§∫Ôºõ2) Âü∫‰∫éÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ËßÑÂàíÂô®ÔºåÁî®‰∫éÊ†πÊçÆÂΩìÂâçÁä∂ÊÄÅÈÄâÊã©‰∏ã‰∏Ä‰∏™ÊúÄ‰Ω≥ËßÜÁÇπÔºõ3) ‰∏ìÂÆ∂ÂºïÂØºÂ•ñÂä±Ê®°ÂùóÔºåÁî®‰∫éÊèê‰æõÊó†ÂÅèÂ∑ÆÁöÑÂ•ñÂä±‰ø°Âè∑„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®Â¢ûÈáèÂºèÊõ¥Êñ∞Á≠ñÁï•ÔºåËÉΩÂ§üÈÄÇÂ∫îÂä®ÊÄÅÂèòÂåñÁöÑÁéØÂ¢É„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHEADERÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÔºö1) ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁ§æÂå∫ÁöÑÁÆóÊ≥ïÊù•ÊûÑÂª∫ÂíåÊõ¥Êñ∞ÂÖ®Â±ÄÂõæÔºåËØ•ÁÆóÊ≥ïÂÖ∑ÊúâÁ∫øÊÄßÂ§çÊùÇÂ∫¶ÔºåËÉΩÂ§üÂ§ÑÁêÜÂ§ßËßÑÊ®°ÁéØÂ¢ÉÔºõ2) ÂºïÂÖ•‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩøËßÑÂàíÂô®ËÉΩÂ§üÂêåÊó∂ËÄÉËôëÂ±ÄÈÉ®ÂíåÂÖ®Â±Ä‰ø°ÊÅØÔºõ3) ÈááÁî®‰∫ÜÊó†ÂèÇÊï∞ÁöÑÁâπÊùÉÂ•ñÂä±ÔºåÈÅøÂÖç‰∫ÜÊâãÂ∑•ËÆæËÆ°Â•ñÂä±ÂáΩÊï∞Â∏¶Êù•ÁöÑÂÅèÂ∑Æ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ®Â±ÄÂõæÁöÑÊûÑÂª∫ÈááÁî®Âü∫‰∫éÁ§æÂå∫ÁöÑÁÆóÊ≥ïÔºå‰øùËØÅ‰∫ÜÁ∫øÊÄßÂ§çÊùÇÂ∫¶„ÄÇÊ≥®ÊÑèÂäõÊú∫Âà∂ÈááÁî®TransformerÁªìÊûÑÔºåËÉΩÂ§üÊúâÊïàÂú∞ÊçïÊçâÂ§öÂ∞∫Â∫¶Á©∫Èó¥‰æùËµñÊÄß„ÄÇÁâπÊùÉÂ•ñÂä±Âü∫‰∫é‰∏ìÂÆ∂Áü•ËØÜÔºåÊó†ÈúÄ‰∫∫Â∑•Ë∞ÉÊï¥ÂèÇÊï∞„ÄÇÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Ê®°ÂûãÈááÁî®Actor-CriticÁªìÊûÑÔºåActorÁΩëÁªúË¥üË¥£ÈÄâÊã©Âä®‰ΩúÔºåCriticÁΩëÁªúË¥üË¥£ËØÑ‰º∞Áä∂ÊÄÅ‰ª∑ÂÄº„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

HEADERÂú®Ê®°ÊãüÁéØÂ¢É‰∏≠ÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫ÜÈ´òËææ20%ÁöÑÊé¢Á¥¢ÊïàÁéáÔºåÂπ∂Âú®ÁúüÂÆûÊ†°Âõ≠ÁéØÂ¢É‰∏≠ÊàêÂäüÈÉ®ÁΩ≤„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHEADERÂÖ∑ÊúâËâØÂ•ΩÁöÑÂèØÊâ©Â±ïÊÄßÂíåÈ≤ÅÊ£íÊÄßÔºåËÉΩÂ§üÈÄÇÂ∫îÂ§çÊùÇ„ÄÅÂ§ßËßÑÊ®°ÁöÑÁéØÂ¢É„ÄÇÊó†ÂèÇÊï∞ÁâπÊùÉÂ•ñÂä±ÁöÑÂºïÂÖ•ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÈÅøÂÖç‰∫ÜÊâãÂ∑•ËÆæËÆ°Â•ñÂä±ÂáΩÊï∞Â∏¶Êù•ÁöÑÂÅèÂ∑Æ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

HEADERÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËá™‰∏ªÊé¢Á¥¢ÁöÑÂú∫ÊôØÔºåÂ¶ÇÂÆ§ÂÜÖÊú∫Âô®‰∫∫ÂØºËà™„ÄÅÊó†‰∫∫Êú∫Â∑°Ê£Ä„ÄÅÁÅæÂêéÊïëÊè¥„ÄÅÁüøÂ±±ÂãòÊé¢Á≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÈ´òÊé¢Á¥¢ÊïàÁéáÔºåÈôç‰Ωé‰∫∫Â∑•Âπ≤È¢ÑÔºåÂπ∂‰∏∫Êú∫Âô®‰∫∫Êèê‰æõÊõ¥ÂèØÈù†ÁöÑÁéØÂ¢ÉÊÑüÁü•ËÉΩÂäõÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄºÂíåÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This work pushes the boundaries of learning-based methods in autonomous robot exploration in terms of environmental scale and exploration efficiency. We present HEADER, an attention-based reinforcement learning approach with hierarchical graphs for efficient exploration in large-scale environments. HEADER follows existing conventional methods to construct hierarchical representations for the robot belief/map, but further designs a novel community-based algorithm to construct and update a global graph, which remains fully incremental, shape-adaptive, and operates with linear complexity. Building upon attention-based networks, our planner finely reasons about the nearby belief within the local range while coarsely leveraging distant information at the global scale, enabling next-best-viewpoint decisions that consider multi-scale spatial dependencies. Beyond novel map representation, we introduce a parameter-free privileged reward that significantly improves model performance and produces near-optimal exploration behaviors, by avoiding training objective bias caused by handcrafted reward shaping. In simulated challenging, large-scale exploration scenarios, HEADER demonstrates better scalability than most existing learning and non-learning methods, while achieving a significant improvement in exploration efficiency (up to 20%) over state-of-the-art baselines. We also deploy HEADER on hardware and validate it in complex, large-scale real-life scenarios, including a 300m*230m campus environment.

