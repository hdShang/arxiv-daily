---
layout: default
title: VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation
---

# VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15530" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.15530v4</a>
  <a href="https://arxiv.org/pdf/2510.15530.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15530v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15530v4', 'VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He

**ÂàÜÁ±ª**: cs.RO, cs.CV, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-17 (Êõ¥Êñ∞: 2025-11-03)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VO-DPÔºö‰∏ÄÁßçÂü∫‰∫éËßÜËßâÁöÑËØ≠‰πâ-Âá†‰ΩïËá™ÈÄÇÂ∫îÊâ©Êï£Á≠ñÁï•ÔºåÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰Ωú**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰∏âÔºöÁ©∫Èó¥ÊÑüÁü•‰∏éËØ≠‰πâ (Perception & Semantics)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `ËßÜËßâÁ≠ñÁï•Â≠¶‰π†` `Êâ©Êï£Ê®°Âûã` `ËØ≠‰πâÁâπÂæÅ` `Âá†‰ΩïÁâπÂæÅ` `È¢ÑËÆ≠ÁªÉÊ®°Âûã` `Ê®°‰ªøÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂü∫‰∫éËßÜËßâËøêÂä®ÁöÑÊâ©Êï£Á≠ñÁï•Â≠¶‰π†ÊñπÊ≥ï‰∏ªË¶Å‰æùËµñÁÇπ‰∫ëËæìÂÖ•ÔºåÁº∫‰πèÂØπ‰ªÖËßÜËßâËß£ÂÜ≥ÊñπÊ°àÁöÑÊ∑±ÂÖ•Êé¢Á¥¢„ÄÇ
2. VO-DPÂà©Áî®È¢ÑËÆ≠ÁªÉËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºåËûçÂêàËØ≠‰πâÂíåÂá†‰ΩïÁâπÂæÅÔºåÂÆûÁé∞È´òÊïàÁöÑËßÜËßâÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•Â≠¶‰π†„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåVO-DPÂú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠Âùá‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂπ∂Âú®‰∏çÂêåÊù°‰ª∂‰∏ãË°®Áé∞Âá∫È´òÂ∫¶ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËßÜËßâÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊâ©Êï£Á≠ñÁï•Â≠¶‰π†ÊñπÊ≥ï(VO-DP)ÔºåËØ•ÊñπÊ≥ïÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÂü∫Á°ÄÊ®°ÂûãÊù•ÂÆûÁé∞ËØ≠‰πâÂíåÂá†‰ΩïÁâπÂæÅÁöÑÊúâÊïàËûçÂêà„ÄÇVO-DPÂà©Áî®VGGTÁöÑ‰∏≠Èó¥ÁâπÂæÅÔºåÁªìÂêàDINOv2ÁöÑËØ≠‰πâÁâπÂæÅÂíå‰∫§ÊõøÊ≥®ÊÑèÂäõÂùóÁöÑÂá†‰ΩïÁâπÂæÅ„ÄÇÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõËûçÂêàÁâπÂæÅÔºåÂπ∂‰ΩøÁî®CNNËøõË°åÁ©∫Èó¥ÂéãÁº©ÔºåÂΩ¢ÊàêÁ≠ñÁï•Â§¥ÁöÑËæìÂÖ•„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåVO-DP‰∏ç‰ªÖÊòæËëó‰ºò‰∫é‰ªÖÂü∫‰∫éËßÜËßâÁöÑÂü∫Á∫øDPÔºåËÄå‰∏îÁõ∏ÂØπ‰∫éÂü∫‰∫éÁÇπ‰∫ëÁöÑÊñπÊ≥ïDP3Ë°®Áé∞Âá∫ÊòéÊòæÁöÑÊÄßËÉΩË∂ãÂäøÔºöÂú®Ê®°Êãü‰ªªÂä°‰∏≠ÔºåVO-DPÁöÑÂπ≥ÂùáÊàêÂäüÁéá‰∏∫64.6%Ôºå‰∏éDP3ÁöÑ64.0%Áõ∏ÂΩìÔºåËøúÈ´ò‰∫éDPÁöÑ34.8%ÔºõÂú®ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÔºåVO-DPËææÂà∞87.9%ÔºåÊòæËëó‰ºò‰∫éDP3ÁöÑ67.5%ÂíåDPÁöÑ11.2%„ÄÇËøõ‰∏ÄÊ≠•ÁöÑÈ≤ÅÊ£íÊÄßËØÑ‰º∞ËØÅÂÆûÔºåVO-DPÂú®È¢úËâ≤„ÄÅÂ§ßÂ∞è„ÄÅËÉåÊôØÂíåÂÖâÁÖßÁ≠â‰∏çÂêåÊù°‰ª∂‰∏ã‰øùÊåÅÈ´òÂ∫¶Á®≥ÂÆö„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÂºÄÊ∫ê‰∫Ü‰∏Ä‰∏™Êú∫Âô®‰∫∫Êìç‰ΩúËÆ≠ÁªÉÂ∫ìÔºåËØ•Â∫ìÂü∫‰∫éAccelerateÔºåÊîØÊåÅÂ§öÊú∫Â§öGPUÂπ∂Ë°åËÆ≠ÁªÉ‰ª•ÂèäÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÔºåÂÖºÂÆπDP„ÄÅDP3ÂíåVO-DPÁ≠âËßÜËßâËøêÂä®Á≠ñÁï•ÔºåÂπ∂ÊîØÊåÅRoboTwinÊ®°ÊãüÂô®„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥‰ªÖ‰ΩøÁî®ËßÜËßâ‰ø°ÊÅØËøõË°åÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÁÇπ‰∫ëÊï∞ÊçÆÔºåËÄå‰ªÖ‰ΩøÁî®ËßÜËßâ‰ø°ÊÅØÁöÑÊñπÊ°àÁº∫‰πèÂØπËØ≠‰πâÂíåÂá†‰Ωï‰ø°ÊÅØÁöÑÊúâÊïàËûçÂêàÔºåÂØºËá¥ÊÄßËÉΩÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVO-DPÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÂü∫Á°ÄÊ®°ÂûãÊèêÂèñÂõæÂÉè‰∏≠ÁöÑËØ≠‰πâÂíåÂá†‰ΩïÁâπÂæÅÔºåÂπ∂ÈÄöËøáËá™ÈÄÇÂ∫îÁöÑÊñπÂºèËøõË°åËûçÂêàÔºå‰ªéËÄåÊèêÂçáËßÜËßâÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•ÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÁªìÂêà‰∏çÂêåÊ®°ÂûãÁöÑ‰ºòÂäøÔºåÂº•Ë°•‰∫Ü‰ªÖ‰ΩøÁî®Âçï‰∏ÄËßÜËßâ‰ø°ÊÅØÊ∫êÁöÑ‰∏çË∂≥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVO-DPÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ÁâπÂæÅÊèêÂèñÔºö‰ΩøÁî®VGGTÊèêÂèñÂõæÂÉèÁöÑ‰∏≠Èó¥ÁâπÂæÅÔºåDINOv2ÊèêÂèñËØ≠‰πâÁâπÂæÅÔºå‰∫§ÊõøÊ≥®ÊÑèÂäõÂùóÊèêÂèñÂá†‰ΩïÁâπÂæÅ„ÄÇ2) ÁâπÂæÅËûçÂêàÔºöÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂Â∞ÜËØ≠‰πâÂíåÂá†‰ΩïÁâπÂæÅËøõË°åËûçÂêà„ÄÇ3) ÁâπÂæÅÂéãÁº©Ôºö‰ΩøÁî®CNNÂØπËûçÂêàÂêéÁöÑÁâπÂæÅËøõË°åÁ©∫Èó¥ÂéãÁº©„ÄÇ4) Á≠ñÁï•Â§¥ÔºöÂ∞ÜÂéãÁº©ÂêéÁöÑÁâπÂæÅËæìÂÖ•Á≠ñÁï•Â§¥ÔºåÁîüÊàêÊú∫Âô®‰∫∫Âä®‰Ωú„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVO-DPÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éËØ≠‰πâÂíåÂá†‰ΩïÁâπÂæÅÁöÑËá™ÈÄÇÂ∫îËûçÂêà„ÄÇÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊ®°ÂûãÂèØ‰ª•Ê†πÊçÆ‰ªªÂä°ÈúÄÊ±ÇÂä®ÊÄÅÂú∞Ë∞ÉÊï¥ËØ≠‰πâÂíåÂá†‰ΩïÁâπÂæÅÁöÑÊùÉÈáçÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÁâπÂæÅË°®Á§∫„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâÂü∫Á°ÄÊ®°ÂûãÂèØ‰ª•ÂáèÂ∞ëÂØπÂ§ßÈáèÊ†áÊ≥®Êï∞ÊçÆÁöÑ‰æùËµñ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVO-DPÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ‰ΩøÁî®VGGTÁöÑ‰∏≠Èó¥ÁâπÂæÅÔºå‰ª•‰øùÁïôÊõ¥Â§öÁöÑÁ©∫Èó¥‰ø°ÊÅØ„ÄÇ2) ‰ΩøÁî®DINOv2ÊèêÂèñËØ≠‰πâÁâπÂæÅÔºåÂà©Áî®ÂÖ∂Âº∫Â§ßÁöÑËØ≠‰πâË°®Á§∫ËÉΩÂäõ„ÄÇ3) ‰ΩøÁî®‰∫§ÊõøÊ≥®ÊÑèÂäõÂùóÊèêÂèñÂá†‰ΩïÁâπÂæÅÔºåÊçïÊçâÂõæÂÉè‰∏≠ÁöÑÁªìÊûÑ‰ø°ÊÅØ„ÄÇ4) ÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÊú∫Âà∂ËûçÂêàÁâπÂæÅÔºåÂÆûÁé∞Ëá™ÈÄÇÂ∫îÁöÑÁâπÂæÅË°®Á§∫„ÄÇ5) ‰ΩøÁî®CNNËøõË°åÁ©∫Èó¥ÂéãÁº©ÔºåÂáèÂ∞ëËÆ°ÁÆóÈáè„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

VO-DPÂú®Ê®°Êãü‰ªªÂä°‰∏≠ÂèñÂæó‰∫Ü‰∏éÂü∫‰∫éÁÇπ‰∫ëÁöÑÊñπÊ≥ïDP3Áõ∏ÂΩìÁöÑÊÄßËÉΩÔºà64.6% vs 64.0%ÔºâÔºåËøúÈ´ò‰∫é‰ªÖÂü∫‰∫éËßÜËßâÁöÑÂü∫Á∫øDPÔºà34.8%Ôºâ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÂú®ÁúüÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÔºåVO-DPÁöÑÊàêÂäüÁéáËææÂà∞‰∫Ü87.9%ÔºåÊòæËëó‰ºò‰∫éDP3Ôºà67.5%ÔºâÂíåDPÔºà11.2%Ôºâ„ÄÇÊ≠§Â§ñÔºåVO-DPÂú®È¢úËâ≤„ÄÅÂ§ßÂ∞è„ÄÅËÉåÊôØÂíåÂÖâÁÖßÁ≠â‰∏çÂêåÊù°‰ª∂‰∏ãË°®Áé∞Âá∫È´òÂ∫¶ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VO-DPÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇÂú®ÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÂåªÁñóËæÖÂä©Êú∫Âô®‰∫∫Á≠âÈ¢ÜÂüü„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫‰ªÖÈÄöËøáËßÜËßâ‰ø°ÊÅØÂ∞±ËÉΩÂÆåÊàêÂ§çÊùÇÁöÑÊìç‰Ωú‰ªªÂä°ÔºåÈôç‰Ωé‰∫ÜÂØπ‰º†ÊÑüÂô®Á°¨‰ª∂ÁöÑË¶ÅÊ±ÇÔºåÂπ∂ÊèêÈ´ò‰∫ÜÊú∫Âô®‰∫∫ÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇÊú™Êù•ÔºåVO-DPÊúâÊúõÂ∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑÁéØÂ¢ÉÂíå‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÂú®Êú™Áü•ÁéØÂ¢É‰∏≠ËøõË°åÁâ©‰ΩìÊäìÂèñÂíåÊìç‰Ωú„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> In the context of imitation learning, visuomotor-based diffusion policy learning is one of the main directions in robotic manipulation. Most of these approaches rely on point clouds as observation inputs and construct scene representations through point clouds feature learning, which enables them to achieve remarkable accuracy. However, the existing literature lacks an in-depth exploration of vision-only solutions that have significant potential. In this paper, we propose a Vision-Only and single-view Diffusion Policy learning method (VO-DP) that leverages pretrained visual foundation models to achieve effective fusion of semantic and geometric features. We utilize intermediate features from VGGT incorporating semantic features from DINOv2 and geometric features from Alternating Attention blocks. Features are fused via cross-attention and spatially compressed with a CNN to form the input to the policy head. Extensive experiments demonstrate that VO-DP not only outperforms the vision-only baseline DP significantly but also exhibits distinct performance trends against the point cloud-based method DP3: in simulation tasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0% and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%, outperforming both DP3 67.5% and DP 11.2% by a notable margin. Further robustness evaluations confirm that VO-DP remains highly stable under varying conditions including color, size, background, and lighting. Lastly, we open-source a training library for robotic manipulation. Built on Accelerate, this library supports multi-machine and multi-GPU parallel training, as well as mixed precision training. It is compatible with visuomotor policies such as DP, DP3 and VO-DP, and also supports the RoboTwin simulator.

