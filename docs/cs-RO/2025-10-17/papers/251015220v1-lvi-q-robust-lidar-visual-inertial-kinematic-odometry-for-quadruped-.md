---
layout: default
title: "LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization"
---

# LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.15220" class="toolbar-btn" target="_blank">üìÑ arXiv: 2510.15220v1</a>
  <a href="https://arxiv.org/pdf/2510.15220.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.15220v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.15220v1', 'LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Kevin Christiansen Marsim, Minho Oh, Byeongho Yu, Seungjae Lee, I Made Aswin Nahrendra, Hyungtae Lim, Hyun Myung

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-17

**Â§áÊ≥®**: 8 Pages, 9 Figures

**ÊúüÂàä**: IEEE Robotics and Automation Letters, vol. 10, no. 10, pp. 10050-10057, Oct. 2025

**DOI**: [10.1109/LRA.2025.3598661](https://doi.org/10.1109/LRA.2025.3598661)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LVI-QÔºö‰∏ÄÁßçÈ≤ÅÊ£íÁöÑÊøÄÂÖâÈõ∑Ëææ-ËßÜËßâ-ÊÉØÊÄß-ËøêÂä®Â≠¶ÈáåÁ®ãËÆ°ÔºåÁî®‰∫éÂõõË∂≥Êú∫Âô®‰∫∫„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)**

**ÂÖ≥ÈîÆËØç**: `ÂõõË∂≥Êú∫Âô®‰∫∫` `ÊøÄÂÖâÈõ∑Ëææ` `ËßÜËßâÊÉØÊÄßÈáåÁ®ãËÆ°` `‰º†ÊÑüÂô®ËûçÂêà` `ËøêÂä®Â≠¶ÈáåÁ®ãËÆ°`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂü∫‰∫é‰º†ÊÑüÂô®ËûçÂêàÁöÑSLAMÁÆóÊ≥ïÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÊòìÂèóÊºÇÁßªÂΩ±ÂìçÔºåÂéüÂõ†ÊòØËûçÂêàÁ≠ñÁï•‰∏çÂΩì„ÄÇ
2. LVI-QÁ≥ªÁªüËûçÂêàÁõ∏Êú∫„ÄÅÊøÄÂÖâÈõ∑Ëææ„ÄÅIMUÂíåÂÖ≥ËäÇÁºñÁ†ÅÂô®‰ø°ÊÅØÔºåÈÄöËøá‰∫§Êõø‰ºòÂåñVIKOÂíåLIKOÂÆûÁé∞È≤ÅÊ£íÂÆö‰Ωç„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåLVI-QÂú®ÂÖ¨ÂÖ±ÂíåÈïøÊúüÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÂÖ∂‰ªñ‰º†ÊÑüÂô®ËûçÂêàSLAMÁÆóÊ≥ïÔºåÂ±ïÁé∞‰∫ÜÊõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ≤ÅÊ£íÁöÑÊøÄÂÖâÈõ∑Ëææ-ËßÜËßâ-ÊÉØÊÄß-ËøêÂä®Â≠¶ÈáåÁ®ãËÆ°Á≥ªÁªüÔºàLVI-QÔºâÔºåÁî®‰∫éÂõõË∂≥Êú∫Âô®‰∫∫Âú®Â§çÊùÇÂä®ÊÄÅÁéØÂ¢É‰∏≠ÂÆûÁé∞Ëá™‰∏ªÂØºËà™„ÄÇËØ•Á≥ªÁªüËûçÂêà‰∫ÜÁõ∏Êú∫„ÄÅÊøÄÂÖâÈõ∑Ëææ„ÄÅÊÉØÊÄßÊµãÈáèÂçïÂÖÉÔºàIMUÔºâÂíåÂÖ≥ËäÇÁºñÁ†ÅÂô®Á≠âÂ§ö‰º†ÊÑüÂô®‰ø°ÊÅØÔºåÁî®‰∫éËßÜËßâÂíåÊøÄÂÖâÈõ∑ËææÈáåÁ®ãËÆ°‰º∞ËÆ°„ÄÇÁ≥ªÁªüÈááÁî®Âü∫‰∫éËûçÂêàÁöÑ‰ΩçÂßø‰º∞ËÆ°ÊñπÊ≥ïÔºåÊ†πÊçÆÊµãÈáèÊï∞ÊçÆÁöÑÂèØÁî®ÊÄßÔºåËøêË°åÂü∫‰∫é‰ºòÂåñÁöÑËßÜËßâ-ÊÉØÊÄß-ËøêÂä®Â≠¶ÈáåÁ®ãËÆ°ÔºàVIKOÔºâÂíåÂü∫‰∫éÊª§Ê≥¢ÁöÑÊøÄÂÖâÈõ∑Ëææ-ÊÉØÊÄß-ËøêÂä®Â≠¶ÈáåÁ®ãËÆ°ÔºàLIKOÔºâ„ÄÇÂú®VIKO‰∏≠ÔºåÂà©Áî®Ë∂≥ÈÉ®È¢ÑÁßØÂàÜÊäÄÊúØÂíåÂü∫‰∫éË∂ÖÂÉèÁ¥†ËÅöÁ±ªÁöÑÈ≤ÅÊ£íÊøÄÂÖâÈõ∑Ëææ-ËßÜËßâÊ∑±Â∫¶‰∏ÄËá¥ÊÄßÔºåËøõË°åÊªëÂä®Á™óÂè£‰ºòÂåñ„ÄÇÂú®LIKO‰∏≠ÔºåÁªìÂêàË∂≥ÈÉ®ËøêÂä®Â≠¶ÔºåÂπ∂Âú®ËØØÂ∑ÆÁä∂ÊÄÅËø≠‰ª£Âç°Â∞îÊõºÊª§Ê≥¢Âô®ÔºàESIKFÔºâ‰∏≠‰ΩøÁî®ÁÇπÂà∞Èù¢ÊÆãÂ∑Æ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÂÖ∂ÂÆÉÂü∫‰∫é‰º†ÊÑüÂô®ËûçÂêàÁöÑSLAMÁÆóÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®ÂÖ¨ÂÖ±ÂíåÈïøÊúüÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂõõË∂≥Êú∫Âô®‰∫∫Âú®Â§çÊùÇÂíåÂä®ÊÄÅÁéØÂ¢É‰∏≠Ëá™‰∏ªÂØºËà™ÈúÄË¶ÅÁ≤æÁ°ÆÁöÑÂÆö‰ΩçÂíåÂª∫Âõæ„ÄÇÁé∞ÊúâÁöÑ‰º†ÊÑüÂô®ËûçÂêàSLAMÊñπÊ≥ïÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÁéØÂ¢É‰∏≠ÂÆπÊòìÂá∫Áé∞‰º∞ËÆ°ÊºÇÁßªÔºåÂõ†‰∏∫ÂÆÉ‰ª¨‰æùËµñ‰∫é‰∏çÂêàÈÄÇÁöÑËûçÂêàÁ≠ñÁï•ÔºåÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®Â§öÊ∫ê‰ø°ÊÅØÁöÑ‰∫íË°•ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØËÆæËÆ°‰∏ÄÁßçÁ¥ßËÄ¶ÂêàÁöÑÊøÄÂÖâÈõ∑Ëææ-ËßÜËßâ-ÊÉØÊÄß-ËøêÂä®Â≠¶ÈáåÁ®ãËÆ°ÔºåÈÄöËøá‰∫§Êõø‰ºòÂåñËßÜËßâ-ÊÉØÊÄß-ËøêÂä®Â≠¶ÈáåÁ®ãËÆ°ÔºàVIKOÔºâÂíåÊøÄÂÖâÈõ∑Ëææ-ÊÉØÊÄß-ËøêÂä®Â≠¶ÈáåÁ®ãËÆ°ÔºàLIKOÔºâÔºåÂπ∂Ê†πÊçÆ‰º†ÊÑüÂô®Êï∞ÊçÆÁöÑÂèØÁî®ÊÄßÂä®ÊÄÅÂàáÊç¢Ôºå‰ªéËÄåÊèêÈ´òÁ≥ªÁªüÂú®ÂêÑÁßçÁéØÂ¢É‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLVI-QÁ≥ªÁªüÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöVIKOÂíåLIKO„ÄÇVIKOÊ®°ÂùóÂà©Áî®ËßÜËßâÂíåIMUÊï∞ÊçÆÔºåÂπ∂ÁªìÂêàË∂≥ÈÉ®ËøêÂä®Â≠¶‰ø°ÊÅØÔºåÈÄöËøáÊªëÂä®Á™óÂè£‰ºòÂåñËøõË°å‰ΩçÂßø‰º∞ËÆ°„ÄÇLIKOÊ®°ÂùóÂàôÂà©Áî®ÊøÄÂÖâÈõ∑ËææÂíåIMUÊï∞ÊçÆÔºåÂêåÊ†∑ÁªìÂêàË∂≥ÈÉ®ËøêÂä®Â≠¶‰ø°ÊÅØÔºåÈÄöËøáËØØÂ∑ÆÁä∂ÊÄÅËø≠‰ª£Âç°Â∞îÊõºÊª§Ê≥¢Âô®ÔºàESIKFÔºâËøõË°å‰ΩçÂßø‰º∞ËÆ°„ÄÇÁ≥ªÁªüÊ†πÊçÆ‰º†ÊÑüÂô®Êï∞ÊçÆÁöÑÂèØÁî®ÊÄßÔºå‰∫§ÊõøËøêË°åVIKOÂíåLIKOÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨ÁöÑÁªìÊûúËøõË°åËûçÂêàÔºå‰ª•Ëé∑ÂæóÊúÄÁªàÁöÑ‰ΩçÂßø‰º∞ËÆ°„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÁ¥ßËÄ¶Âêà‰∫ÜËßÜËßâ„ÄÅÊøÄÂÖâÈõ∑Ëææ„ÄÅÊÉØÊÄßÂíåËøêÂä®Â≠¶‰ø°ÊÅØÔºåÂπ∂ÈááÁî®‰∫§Êõø‰ºòÂåñÁöÑÁ≠ñÁï•„ÄÇVIKOÊ®°Âùó‰∏≠ÔºåÂà©Áî®Ë∂ÖÂÉèÁ¥†ËÅöÁ±ªÂÆûÁé∞È≤ÅÊ£íÁöÑÊøÄÂÖâÈõ∑Ëææ-ËßÜËßâÊ∑±Â∫¶‰∏ÄËá¥ÊÄß„ÄÇLIKOÊ®°Âùó‰∏≠ÔºåÂ∞ÜË∂≥ÈÉ®ËøêÂä®Â≠¶‰ø°ÊÅØËûçÂÖ•Âà∞ESIKF‰∏≠ÔºåÊèêÈ´ò‰∫ÜÊª§Ê≥¢Âô®ÁöÑÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöVIKOÊ®°Âùó‰ΩøÁî®ÊªëÂä®Á™óÂè£‰ºòÂåñÔºåÁ™óÂè£Â§ßÂ∞èÈúÄË¶ÅÊ†πÊçÆÁéØÂ¢ÉÂä®ÊÄÅË∞ÉÊï¥„ÄÇLIKOÊ®°Âùó‰ΩøÁî®ÁÇπÂà∞Èù¢ÊÆãÂ∑Æ‰Ωú‰∏∫ËßÇÊµãÊ®°ÂûãÔºåÈúÄË¶Å‰ªîÁªÜÈÄâÊã©ÂêàÈÄÇÁöÑÁÇπ‰∫ëÁâπÂæÅ„ÄÇË∂≥ÈÉ®È¢ÑÁßØÂàÜÊäÄÊúØÁî®‰∫éÂáèÂ∞ëËÆ°ÁÆóÈáè„ÄÇË∂ÖÂÉèÁ¥†ËÅöÁ±ªÁÆóÊ≥ïÁöÑÈÄâÊã©ÂíåÂèÇÊï∞ËÆæÁΩÆ‰ºöÂΩ±ÂìçÊ∑±Â∫¶‰∏ÄËá¥ÊÄßÁöÑÊïàÊûú„ÄÇ‰∫§Êõø‰ºòÂåñÁ≠ñÁï•ÁöÑÂàáÊç¢Êù°‰ª∂ÈúÄË¶ÅÊ†πÊçÆ‰º†ÊÑüÂô®Êï∞ÊçÆÁöÑË¥®ÈáèËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLVI-QÂú®ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜÂíåÈïøÊúüÊï∞ÊçÆÈõÜ‰∏äÂùáË°®Áé∞Âá∫‰ºòÂºÇÁöÑÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÁöÑ‰º†ÊÑüÂô®ËûçÂêàSLAMÁÆóÊ≥ïÁõ∏ÊØîÔºåLVI-QÂú®ÂÆö‰ΩçÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÊñπÈù¢ÂùáÊúâÊòæËëóÊèêÂçá„ÄÇÂÖ∑‰ΩìÊï∞ÊçÆÊú™Âú®ÊëòË¶Å‰∏≠ÁªôÂá∫Ôºå‰ΩÜÂº∫Ë∞É‰∫ÜÂÖ∂Âú®ÂêÑÁßçÊï∞ÊçÆÈõÜ‰∏äÁöÑ‰ºòË∂äÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÂõõË∂≥Êú∫Âô®‰∫∫Ëá™‰∏ªÂØºËà™ÁöÑÂú∫ÊôØÔºå‰æãÂ¶ÇÔºöÂ§çÊùÇÂú∞ÂΩ¢ÁöÑÊêúÁ¥¢ÊïëÊè¥„ÄÅÂ∑•‰∏öÂ∑°Ê£Ä„ÄÅÂÜú‰∏öÂãòÊµã„ÄÅ‰ª•ÂèäÁâ©ÊµÅÈÖçÈÄÅÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÂõõË∂≥Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑÂÆö‰ΩçÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÔºåÂèØ‰ª•‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂêÑÁßçÂÆûÈôÖÂ∫îÁî®ÈúÄÊ±ÇÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÊú∫Âô®‰∫∫Ëá™‰∏ªÂåñÂèëÂ±ïÂ•†ÂÆöÂü∫Á°Ä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Autonomous navigation for legged robots in complex and dynamic environments relies on robust simultaneous localization and mapping (SLAM) systems to accurately map surroundings and localize the robot, ensuring safe and efficient operation. While prior sensor fusion-based SLAM approaches have integrated various sensor modalities to improve their robustness, these algorithms are still susceptible to estimation drift in challenging environments due to their reliance on unsuitable fusion strategies. Therefore, we propose a robust LiDAR-visual-inertial-kinematic odometry system that integrates information from multiple sensors, such as a camera, LiDAR, inertial measurement unit (IMU), and joint encoders, for visual and LiDAR-based odometry estimation. Our system employs a fusion-based pose estimation approach that runs optimization-based visual-inertial-kinematic odometry (VIKO) and filter-based LiDAR-inertial-kinematic odometry (LIKO) based on measurement availability. In VIKO, we utilize the footpreintegration technique and robust LiDAR-visual depth consistency using superpixel clusters in a sliding window optimization. In LIKO, we incorporate foot kinematics and employ a point-toplane residual in an error-state iterative Kalman filter (ESIKF). Compared with other sensor fusion-based SLAM algorithms, our approach shows robust performance across public and longterm datasets.

