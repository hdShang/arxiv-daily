---
layout: default
title: "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving"
---

# LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00284" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00284v2</a>
  <a href="https://arxiv.org/pdf/2505.00284.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00284v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00284v2', 'LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhijie Qiao, Haowei Li, Zhong Cao, Henry X. Liu

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-01 (æ›´æ–°: 2025-09-13)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/michigan-traffic-lab/LightEMMA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLightEMMAä»¥è§£å†³è‡ªä¸»é©¾é©¶æ¨¡å‹åŠ¨æ€æ›´æ–°ä¸è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªä¸»é©¾é©¶` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€æ¨¡å‹` `åŠ¨æ€æ›´æ–°` `æ€§èƒ½è¯„ä¼°` `æ™ºèƒ½äº¤é€š` `æ¨¡å‹é›†æˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ç¼ºä¹ä¸€ä¸ªå®ç”¨çš„å¹³å°ï¼Œæ— æ³•å®ç°åŠ¨æ€æ¨¡å‹æ›´æ–°å’Œå¿«é€ŸéªŒè¯ï¼Œå¯¼è‡´è‡ªä¸»é©¾é©¶æ€§èƒ½è¯„ä¼°å›°éš¾ã€‚
2. LightEMMAæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„VLMåŸºç¡€æ¡†æ¶ï¼Œæ”¯æŒä¸æœ€æ–°çš„å•†ä¸šå’Œå¼€æºæ¨¡å‹çš„æ— ç¼é›†æˆï¼Œç®€åŒ–äº†æ¨¡å‹æ›´æ–°è¿‡ç¨‹ã€‚
3. åœ¨nuScenesé¢„æµ‹ä»»åŠ¡ä¸­ï¼Œæ„å»ºçš„åäºŒä¸ªè‡ªä¸»é©¾é©¶ä»£ç†å±•ç¤ºäº†VLMsçš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†ä¹ŸæŒ‡å‡ºäº†æ¨¡å‹å¤æ‚æ€§ä¸å®é™…æ€§èƒ½ä¹‹é—´çš„çŸ›ç›¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç«¯åˆ°ç«¯è‡ªä¸»é©¾é©¶ä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰è¯¥é¢†åŸŸç¼ºä¹ä¸€ä¸ªå®ç”¨çš„å¹³å°ï¼Œèƒ½å¤Ÿå®ç°åŠ¨æ€æ¨¡å‹æ›´æ–°ã€å¿«é€ŸéªŒè¯ã€å…¬å¹³æ¯”è¾ƒå’Œç›´è§‚çš„æ€§èƒ½è¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†LightEMMAï¼Œä¸€ä¸ªè½»é‡çº§çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æä¾›ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºVLMçš„è‡ªä¸»é©¾é©¶æ¡†æ¶ï¼Œä¾¿äºä¸ä¸æ–­å‘å±•çš„å•†ä¸šå’Œå¼€æºæ¨¡å‹è¿›è¡Œé›†æˆã€‚æˆ‘ä»¬æ„å»ºäº†åäºŒä¸ªè‡ªä¸»é©¾é©¶ä»£ç†ï¼Œä½¿ç”¨ä¸åŒçš„VLMsï¼Œå¹¶åœ¨æŒ‘æˆ˜æ€§çš„nuScenesé¢„æµ‹ä»»åŠ¡ä¸Šè¯„ä¼°å…¶æ€§èƒ½ï¼Œå…¨é¢è¯„ä¼°è®¡ç®—æŒ‡æ ‡å¹¶æä¾›å…³é”®è§è§£ã€‚å°½ç®¡VLMsåœ¨åœºæ™¯ç†è§£èƒ½åŠ›ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†å…¶åœ¨è‡ªä¸»é©¾é©¶ä»»åŠ¡ä¸­çš„å®é™…è¡¨ç°ä»ä»¤äººæ‹…å¿§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªä¸»é©¾é©¶é¢†åŸŸä¸­ç¼ºä¹åŠ¨æ€æ›´æ–°å’Œå¿«é€ŸéªŒè¯çš„å¹³å°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç‰¹å®šçš„å®šåˆ¶åŒ–è®¾è®¡ï¼Œéš¾ä»¥é€‚åº”å¿«é€Ÿå˜åŒ–çš„æŠ€æœ¯ç¯å¢ƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLightEMMAé€šè¿‡æä¾›ä¸€ä¸ªè½»é‡çº§çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ¶ˆé™¤äº†å¯¹ç‰¹å®šå®šåˆ¶çš„ä¾èµ–ï¼Œå…è®¸ç”¨æˆ·æ–¹ä¾¿åœ°é›†æˆæœ€æ–°çš„VLMsï¼Œä»è€Œæå‡è‡ªä¸»é©¾é©¶çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLightEMMAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯è¾“å…¥çš„å¤šæ¨¡æ€æ•°æ®å¤„ç†ï¼Œå…¶æ¬¡æ˜¯åŸºäºVLMçš„å†³ç­–æ¨¡å—ï¼Œæœ€åæ˜¯è¾“å‡ºçš„æ§åˆ¶æŒ‡ä»¤ç”Ÿæˆã€‚æ¯ä¸ªæ¨¡å—éƒ½ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿é«˜æ•ˆçš„æ€§èƒ½å’Œå‡†ç¡®çš„å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šLightEMMAçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è½»é‡çº§è®¾è®¡å’Œæ— ç¼é›†æˆèƒ½åŠ›ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°çš„æ•°æ®å’Œä»»åŠ¡éœ€æ±‚ã€‚è¿™ä¸ä¼ ç»Ÿçš„é‡å‹æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œåè€…å¾€å¾€éœ€è¦å¤æ‚çš„å®šåˆ¶å’Œè°ƒæ•´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒLightEMMAé‡‡ç”¨äº†ä¼˜åŒ–çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œå†³ç­–å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å‚æ•°è®¾ç½®ç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒåœºæ™¯ä¸‹çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨nuScenesé¢„æµ‹ä»»åŠ¡ä¸­ï¼Œæ„å»ºçš„åäºŒä¸ªè‡ªä¸»é©¾é©¶ä»£ç†å±•ç¤ºäº†VLMsçš„å¼ºå¤§èƒ½åŠ›ï¼Œå°½ç®¡åœ¨åœºæ™¯ç†è§£ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å®é™…æ€§èƒ½ä»éœ€æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¤æ‚æ€§ä¸æ€§èƒ½æå‡ä¹‹é—´å¹¶ä¸æ€»æ˜¯æ­£ç›¸å…³ï¼Œå¼ºè°ƒäº†ä»»åŠ¡ç‰¹å®šè®¾è®¡çš„é‡è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LightEMMAçš„ç ”ç©¶æˆæœåœ¨è‡ªä¸»é©¾é©¶é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚å…¶è½»é‡çº§å’Œçµæ´»çš„ç‰¹æ€§ä½¿å¾—è¯¥æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ä¸åŒçš„é©¾é©¶ç¯å¢ƒå’Œä»»åŠ¡éœ€æ±‚ï¼Œèƒ½å¤Ÿä¸ºæœªæ¥çš„æ™ºèƒ½äº¤é€šç³»ç»Ÿæä¾›å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æŒã€‚æ­¤å¤–ï¼Œéšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼ŒLightEMMAä¹Ÿä¸ºå…¶ä»–å¤šæ¨¡æ€åº”ç”¨æä¾›äº†å€Ÿé‰´ï¼Œæ¨åŠ¨äº†ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language Models (VLMs) have demonstrated significant potential for end-to-end autonomous driving. However, the field still lacks a practical platform that enables dynamic model updates, rapid validation, fair comparison, and intuitive performance assessment. To that end, we introduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous driving. LightEMMA provides a unified, VLM-based autonomous driving framework without ad hoc customizations, enabling easy integration with evolving state-of-the-art commercial and open-source models. We construct twelve autonomous driving agents using various VLMs and evaluate their performance on the challenging nuScenes prediction task, comprehensively assessing computational metrics and providing critical insights. Illustrative examples show that, although VLMs exhibit strong scenario interpretation capabilities, their practical performance in autonomous driving tasks remains a concern. Additionally, increased model complexity and extended reasoning do not necessarily lead to better performance, emphasizing the need for further improvements and task-specific designs. The code is available at https://github.com/michigan-traffic-lab/LightEMMA.

