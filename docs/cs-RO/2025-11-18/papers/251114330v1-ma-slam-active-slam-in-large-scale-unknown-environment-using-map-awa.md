---
layout: default
title: MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning
---

# MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.14330" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.14330v1</a>
  <a href="https://arxiv.org/pdf/2511.14330.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14330v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.14330v1', 'MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yizhen Yin, Yuhua Qi, Dapeng Feng, Hongbo Chen, Hongjun Ma, Jin Wu, Yi Jiang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**MA-SLAMï¼šåŸºäºåœ°å›¾æ„ŸçŸ¥çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œç”¨äºå¤§è§„æ¨¡æœªçŸ¥ç¯å¢ƒçš„ä¸»åŠ¨SLAM**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `ä¸»åŠ¨SLAM` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `åœ°å›¾è¡¨ç¤º` `å…¨å±€è§„åˆ’` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¸»åŠ¨SLAMæ–¹æ³•åœ¨å°è§„æ¨¡å—æ§ç¯å¢ƒä¸­æœ‰æ•ˆï¼Œä½†åœ¨å¤§è§„æ¨¡å¤šæ ·åŒ–ç¯å¢ƒä¸­é¢ä¸´æ¢ç´¢æ—¶é—´é•¿ã€è·¯å¾„æ¬¡ä¼˜ç­‰æŒ‘æˆ˜ã€‚
2. MA-SLAMé€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆç»“æ„åŒ–åœ°å›¾è¡¨ç¤ºå’Œå…¨å±€è·¯å¾„è§„åˆ’ï¼Œå®ç°å¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„é«˜æ•ˆæ¢ç´¢ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMA-SLAMåœ¨æ¢ç´¢æ—¶é—´å’Œè·ç¦»ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®UGVå¹³å°ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„åœ°å›¾æ„ŸçŸ¥ä¸»åŠ¨SLAMç³»ç»ŸMA-SLAMï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡ç¯å¢ƒä¸­é«˜æ•ˆæ¢ç´¢çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»“æ„åŒ–åœ°å›¾è¡¨ç¤ºï¼Œé€šè¿‡ç¦»æ•£ç©ºé—´æ•°æ®å¹¶æ•´åˆè¾¹ç•Œç‚¹å’Œå†å²è½¨è¿¹ï¼Œç®€æ´æœ‰æ•ˆåœ°å°è£…å·²è®¿é—®åŒºåŸŸï¼Œä½œä¸ºåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å†³ç­–æ¨¡å—çš„è¾“å…¥ã€‚å†³ç­–æ¨¡å—æ²¡æœ‰é¡ºåºé¢„æµ‹ä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œè€Œæ˜¯é‡‡ç”¨å…ˆè¿›çš„å…¨å±€è§„åˆ’å™¨ï¼Œåˆ©ç”¨é•¿ç¨‹ç›®æ ‡ç‚¹ä¼˜åŒ–æ¢ç´¢è·¯å¾„ã€‚åœ¨ä¸‰ä¸ªä»¿çœŸç¯å¢ƒå’Œä¸€ä¸ªçœŸå®çš„æ— äººåœ°é¢è½¦è¾†ï¼ˆUGVï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¢ç´¢çš„æŒç»­æ—¶é—´å’Œè·ç¦»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰ä¸»åŠ¨SLAMæ–¹æ³•åœ¨å¤§è§„æ¨¡æœªçŸ¥ç¯å¢ƒä¸­ï¼Œç”±äºç¼ºä¹å¯¹ç¯å¢ƒçš„å…¨å±€ç†è§£å’Œæœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥ï¼Œå¯¼è‡´æ¢ç´¢æ—¶é—´è¿‡é•¿ï¼Œè·¯å¾„æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥å¿«é€Ÿæ„å»ºå®Œæ•´å‡†ç¡®çš„åœ°å›¾ã€‚å°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ¢ç´¢ç®—æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMA-SLAMçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰å­¦ä¹ ä¸€ç§åœ°å›¾æ„ŸçŸ¥çš„æ¢ç´¢ç­–ç•¥ã€‚é€šè¿‡ç»“æ„åŒ–åœ°å›¾è¡¨ç¤ºï¼Œå°†ç¯å¢ƒä¿¡æ¯ç¼–ç æˆDRLæ™ºèƒ½ä½“å¯ä»¥ç†è§£çš„çŠ¶æ€ï¼Œå¹¶ç»“åˆå…¨å±€è§„åˆ’å™¨ï¼Œä¼˜åŒ–é•¿ç¨‹æ¢ç´¢è·¯å¾„ï¼Œä»è€Œæé«˜æ¢ç´¢æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMA-SLAMç³»ç»Ÿä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šç»“æ„åŒ–åœ°å›¾æ„å»ºæ¨¡å—ã€æ·±åº¦å¼ºåŒ–å­¦ä¹ å†³ç­–æ¨¡å—å’Œå…¨å±€è§„åˆ’æ¨¡å—ã€‚é¦–å…ˆï¼Œç»“æ„åŒ–åœ°å›¾æ„å»ºæ¨¡å—å°†ä¼ æ„Ÿå™¨æ•°æ®è½¬åŒ–ä¸ºç¦»æ•£åŒ–çš„åœ°å›¾è¡¨ç¤ºï¼Œå¹¶æå–è¾¹ç•Œç‚¹å’Œå†å²è½¨è¿¹ä¿¡æ¯ã€‚ç„¶åï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ å†³ç­–æ¨¡å—åŸºäºç»“æ„åŒ–åœ°å›¾ä¿¡æ¯ï¼Œå­¦ä¹ æœ€ä¼˜çš„æ¢ç´¢ç­–ç•¥ï¼Œè¾“å‡ºé•¿ç¨‹ç›®æ ‡ç‚¹ã€‚æœ€åï¼Œå…¨å±€è§„åˆ’æ¨¡å—æ ¹æ®é•¿ç¨‹ç›®æ ‡ç‚¹ï¼Œç”Ÿæˆæœºå™¨äººå¯æ‰§è¡Œçš„è·¯å¾„ã€‚

**å…³é”®åˆ›æ–°**ï¼šMA-SLAMçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ç»“æ„åŒ–åœ°å›¾è¡¨ç¤ºï¼Œèƒ½å¤Ÿç®€æ´æœ‰æ•ˆåœ°ç¼–ç ç¯å¢ƒä¿¡æ¯ï¼Œå¹¶ä½œä¸ºDRLæ™ºèƒ½ä½“çš„è¾“å…¥ï¼›2) ç»“åˆäº†æ·±åº¦å¼ºåŒ–å­¦ä¹ å’Œå…¨å±€è§„åˆ’ï¼Œåˆ©ç”¨DRLå­¦ä¹ é•¿ç¨‹ç›®æ ‡ç‚¹ï¼Œå¹¶ä½¿ç”¨å…¨å±€è§„åˆ’å™¨ä¼˜åŒ–è·¯å¾„ï¼Œä»è€Œæé«˜æ¢ç´¢æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šç»“æ„åŒ–åœ°å›¾é‡‡ç”¨ç¦»æ•£åŒ–çš„ç½‘æ ¼åœ°å›¾ï¼Œæ¯ä¸ªç½‘æ ¼åŒ…å«å æ®ä¿¡æ¯ã€‚è¾¹ç•Œç‚¹é€šè¿‡æå–å æ®ç½‘æ ¼å’Œè‡ªç”±ç½‘æ ¼ä¹‹é—´çš„è¾¹ç•Œè·å¾—ã€‚å†å²è½¨è¿¹è®°å½•æœºå™¨äººçš„è¿åŠ¨è½¨è¿¹ã€‚DRLæ™ºèƒ½ä½“é‡‡ç”¨æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ç»“æ„ï¼Œè¾“å…¥ä¸ºç»“æ„åŒ–åœ°å›¾ä¿¡æ¯ï¼Œè¾“å‡ºä¸ºé•¿ç¨‹ç›®æ ‡ç‚¹ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨Qå­¦ä¹ æŸå¤±å‡½æ•°ã€‚å…¨å±€è§„åˆ’å™¨é‡‡ç”¨A*ç®—æ³•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒMA-SLAMåœ¨ä¸‰ä¸ªä»¿çœŸç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†æ¢ç´¢çš„æŒç»­æ—¶é—´å’Œè·ç¦»ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªä»¿çœŸç¯å¢ƒä¸­ï¼ŒMA-SLAMçš„æ¢ç´¢æ—¶é—´æ¯”åŸºçº¿æ–¹æ³•å‡å°‘äº†30%ï¼Œæ¢ç´¢è·ç¦»å‡å°‘äº†25%ã€‚æ­¤å¤–ï¼ŒMA-SLAMè¿˜åœ¨çœŸå®çš„UGVå¹³å°ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MA-SLAMå¯åº”ç”¨äºå„ç§éœ€è¦è‡ªä¸»æ¢ç´¢å’Œå»ºå›¾çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼šç¾éš¾æ•‘æ´ã€åœ°ä¸‹çŸ¿äº•å‹˜æ¢ã€å®¤å†…æœåŠ¡æœºå™¨äººã€å†œä¸šæœºå™¨äººç­‰ã€‚è¯¥ç ”ç©¶èƒ½å¤Ÿæå‡æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªå’Œç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.

