---
layout: default
title: "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards"
---

# NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.14659" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.14659v1</a>
  <a href="https://arxiv.org/pdf/2511.14659.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.14659v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.14659v1', 'NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

**å¤‡æ³¨**: https://declare-lab.github.io/nora-1.5

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NORA-1.5ï¼šåŸºäºä¸–ç•Œæ¨¡å‹å’ŒåŠ¨ä½œåå¥½å¥–åŠ±è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæå‡å…·èº«æ™ºèƒ½ä½“çš„å¯é æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å…·èº«æ™ºèƒ½` `ä¸–ç•Œæ¨¡å‹` `åå¥½å­¦ä¹ ` `ç›´æ¥åå¥½ä¼˜åŒ–` `æµåŒ¹é…` `æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å¯é æ€§å’Œæ³›åŒ–æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒç¯å¢ƒæˆ–çœŸå®ä¸–ç•Œéƒ¨ç½²æ—¶ã€‚
2. NORA-1.5é€šè¿‡å¢åŠ åŸºäºæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶ï¼Œå¹¶ç»“åˆä¸–ç•Œæ¨¡å‹å’ŒåŠ¨ä½œåå¥½å¥–åŠ±è¿›è¡Œè®­ç»ƒï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚
3. å®éªŒè¡¨æ˜ï¼Œå¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒèƒ½å¤ŸæŒç»­æé«˜æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜VLAæ¨¡å‹çš„å¯é æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†NORA-1.5ï¼Œä¸€ä¸ªè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œå®ƒåŸºäºé¢„è®­ç»ƒçš„NORAéª¨å¹²ç½‘ç»œï¼Œå¹¶å¢åŠ äº†ä¸€ä¸ªåŸºäºæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶ã€‚è¿™ç§æ¶æ„ä¸Šçš„å¢å¼ºæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œä½¿NORA-1.5åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºNORAå’Œå‡ ç§æœ€å…ˆè¿›çš„VLAæ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é²æ£’æ€§å’Œä»»åŠ¡æˆåŠŸç‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—å¥–åŠ±æ¨¡å‹ï¼Œç”¨äºå¯¹VLAç­–ç•¥è¿›è¡Œåè®­ç»ƒã€‚æˆ‘ä»¬çš„å¥–åŠ±ç»“åˆäº†ï¼ˆiï¼‰ä¸€ä¸ªåŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹ï¼ˆWMï¼‰ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆçš„åŠ¨ä½œæ˜¯å¦å¯¼å‘æœŸæœ›çš„ç›®æ ‡ï¼Œä»¥åŠï¼ˆiiï¼‰ä¸€ä¸ªåç¦»çœŸå®å€¼çš„å¯å‘å¼æ–¹æ³•ï¼Œç”¨äºåŒºåˆ†å¥½åŠ¨ä½œå’ŒååŠ¨ä½œã€‚åˆ©ç”¨è¿™äº›å¥–åŠ±ä¿¡å·ï¼Œæˆ‘ä»¬æ„å»ºäº†åå¥½æ•°æ®é›†ï¼Œå¹¶é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½¿NORA-1.5é€‚åº”ç›®æ ‡ç¯å¢ƒã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼Œå¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒèƒ½å¤ŸæŒç»­æé«˜æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­çš„æ€§èƒ½ï¼Œé€šè¿‡ç®€å•è€Œæœ‰æ•ˆçš„å¥–åŠ±æ¨¡å‹æ˜¾è‘—æé«˜VLAæ¨¡å‹çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒNORA-1.5å’Œå¥–åŠ±å¼•å¯¼çš„åè®­ç»ƒæ˜¯å®ç°æ›´å¯é çš„ã€é€‚ç”¨äºçœŸå®ä¸–ç•Œéƒ¨ç½²çš„å…·èº«æ™ºèƒ½ä½“çš„å¯è¡Œé€”å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²æ—¶å¯é æ€§å’Œæ³›åŒ–æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰VLAæ¨¡å‹åœ¨é¢å¯¹ä¸åŒç¯å¢ƒå’Œå…·èº«æ™ºèƒ½ä½“æ—¶ï¼Œéš¾ä»¥ä¿è¯ä»»åŠ¡çš„æˆåŠŸç‡å’Œç¨³å®šæ€§ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¢å¼ºæ¨¡å‹æ¶æ„å’Œå¼•å…¥å¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒæ¥æé«˜VLAæ¨¡å‹çš„å¯é æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡æ·»åŠ åŸºäºæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶æ¥æå‡æ¨¡å‹çš„åŸºç¡€æ€§èƒ½ï¼Œç„¶ååˆ©ç”¨ä¸–ç•Œæ¨¡å‹å’ŒåŠ¨ä½œåå¥½å¥–åŠ±æ¥æŒ‡å¯¼æ¨¡å‹çš„ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ç›®æ ‡ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNORA-1.5çš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é¢„è®­ç»ƒçš„NORAéª¨å¹²ç½‘ç»œï¼›2) åŸºäºæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶ï¼Œç”¨äºç”ŸæˆåŠ¨ä½œï¼›3) åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹ï¼ˆWMï¼‰ï¼Œç”¨äºè¯„ä¼°åŠ¨ä½œçš„æœ‰æ•ˆæ€§ï¼›4) åç¦»çœŸå®å€¼çš„å¯å‘å¼æ–¹æ³•ï¼Œç”¨äºåŒºåˆ†å¥½ååŠ¨ä½œï¼›5) ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®—æ³•ï¼Œç”¨äºæ ¹æ®å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨¡å‹ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºç»“åˆäº†æ¨¡å‹æ¶æ„å¢å¼ºå’Œå¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒã€‚é€šè¿‡æ·»åŠ åŠ¨ä½œä¸“å®¶ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç”ŸæˆåŠ¨ä½œï¼›é€šè¿‡ä¸–ç•Œæ¨¡å‹å’ŒåŠ¨ä½œåå¥½å¥–åŠ±ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´æœ‰æ•ˆçš„ç­–ç•¥ã€‚è¿™ç§ç»“åˆä½¿å¾—NORA-1.5åœ¨å¯é æ€§å’Œæ³›åŒ–æ€§æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨æµåŒ¹é…æ–¹æ³•è®­ç»ƒåŠ¨ä½œä¸“å®¶ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´æµç•…è‡ªç„¶çš„åŠ¨ä½œï¼›2) è®¾è®¡åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹åŠ¨ä½œå¯¹ç¯å¢ƒçš„å½±å“ï¼›3) è®¾è®¡åç¦»çœŸå®å€¼çš„å¯å‘å¼å¥–åŠ±ï¼Œç”¨äºåŒºåˆ†å¥½ååŠ¨ä½œï¼›4) ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œæ ¹æ®å¥–åŠ±ä¿¡å·ç›´æ¥ä¼˜åŒ–æ¨¡å‹ç­–ç•¥ï¼Œé¿å…äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€äº›é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

NORA-1.5åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡æ·»åŠ åŠ¨ä½œä¸“å®¶å’Œè¿›è¡Œå¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒï¼ŒNORA-1.5åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†NORAå’Œå…¶ä»–æœ€å…ˆè¿›çš„VLAæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒèƒ½å¤ŸæŒç»­æé«˜æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜VLAæ¨¡å‹çš„å¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å…·èº«æ™ºèƒ½ä½“çš„å®é™…åœºæ™¯ï¼Œå¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚é€šè¿‡æé«˜VLAæ¨¡å‹çš„å¯é æ€§å’Œæ³›åŒ–æ€§ï¼Œå¯ä»¥ä½¿è¿™äº›æ™ºèƒ½ä½“æ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒï¼Œå®Œæˆå„ç§ä»»åŠ¡ï¼Œä»è€Œæå‡ç”Ÿäº§æ•ˆç‡å’ŒæœåŠ¡è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œå¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

