---
layout: default
title: Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control
---

# Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00050" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00050v1</a>
  <a href="https://arxiv.org/pdf/2512.00050.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00050v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.00050v1', 'Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Suzie Kim

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-18

**å¤‡æ³¨**: Master's thesis, Korea University, 2025. arXiv admin note: substantial text overlap with arXiv:2507.13171

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºéšå¼ç¥ç»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºäººæœºåä½œæœºå™¨äººæ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äººæœºäº¤äº’` `è„‘ç”µä¿¡å·` `éšå¼åé¦ˆ` `æœºå™¨äººæ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨ç¨€ç–å¥–åŠ±ä¸‹è¡¨ç°ä¸ä½³ï¼Œä¾èµ–äººå·¥è®¾è®¡çš„å¤æ‚å¥–åŠ±å‡½æ•°ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æå‡ºRLIHFæ¡†æ¶ï¼Œåˆ©ç”¨è„‘ç”µä¿¡å·ï¼ˆErrPï¼‰ä½œä¸ºéšå¼åé¦ˆï¼Œæ— éœ€ç”¨æˆ·æ˜¾å¼å¹²é¢„ï¼Œå®ç°è¿ç»­è¯„ä¼°ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒåŸºäºEEGåé¦ˆè®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸­è¾¾åˆ°ä¸å¯†é›†å¥–åŠ±è®­ç»ƒæ™ºèƒ½ä½“ç›¸å½“çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹éš¾ä»¥å­¦ä¹ æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šå¸¸éœ€è¦æ‰‹åŠ¨è®¾è®¡å¤æ‚çš„ã€ç‰¹å®šäºä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åº”è¿è€Œç”Ÿï¼Œå®ƒç”¨äººç±»æä¾›çš„è¯„ä¼°ä¿¡å·æ¥è¡¥å……æ‰‹å·¥è®¾è®¡çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„RLHFæ–¹æ³•ä¾èµ–äºæ˜¾å¼åé¦ˆæœºåˆ¶ï¼Œä¾‹å¦‚æŒ‰é’®æŒ‰ä¸‹æˆ–åå¥½æ ‡ç­¾ï¼Œè¿™ä¼šä¸­æ–­è‡ªç„¶äº¤äº’è¿‡ç¨‹å¹¶ç»™ç”¨æˆ·å¸¦æ¥å·¨å¤§çš„è®¤çŸ¥è´Ÿæ‹…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»éšå¼äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLIHFï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨éä¾µå…¥æ€§è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·ï¼Œç‰¹åˆ«æ˜¯é”™è¯¯ç›¸å…³ç”µä½ï¼ˆErrPï¼‰ï¼Œæ¥æä¾›è¿ç»­çš„ã€éšå¼çš„åé¦ˆï¼Œè€Œæ— éœ€æ˜¾å¼çš„ç”¨æˆ·å¹²é¢„ã€‚è¯¥æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒçš„è§£ç å™¨å°†åŸå§‹EEGä¿¡å·è½¬æ¢ä¸ºæ¦‚ç‡å¥–åŠ±åˆ†é‡ï¼Œä»è€Œå³ä½¿åœ¨å­˜åœ¨ç¨€ç–å¤–éƒ¨å¥–åŠ±çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨åŸºäºMuJoCoç‰©ç†å¼•æ“çš„ä»¿çœŸç¯å¢ƒä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨Kinova Gen2æœºæ¢°è‡‚æ‰§è¡Œå¤æ‚çš„æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡éœ€è¦é¿å¼€éšœç¢ç‰©åŒæ—¶æ“ä½œç›®æ ‡å¯¹è±¡ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è§£ç åçš„EEGåé¦ˆè®­ç»ƒçš„æ™ºèƒ½ä½“å®ç°äº†ä¸ä½¿ç”¨å¯†é›†ã€æ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±è®­ç»ƒçš„æ™ºèƒ½ä½“ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°éªŒè¯äº†ä½¿ç”¨éšå¼ç¥ç»åé¦ˆåœ¨äº¤äº’å¼æœºå™¨äººæŠ€æœ¯ä¸­å®ç°å¯æ‰©å±•ä¸”ä¸äººç±»å¯¹é½çš„å¼ºåŒ–å­¦ä¹ çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹éš¾ä»¥æœ‰æ•ˆå­¦ä¹ ï¼Œéœ€è¦äººå·¥è®¾è®¡å¤æ‚çš„å¥–åŠ±å‡½æ•°ã€‚è€Œç°æœ‰RLHFæ–¹æ³•ä¾èµ–æ˜¾å¼åé¦ˆï¼Œä¸­æ–­äººæœºäº¤äº’ï¼Œå¢åŠ ç”¨æˆ·è®¤çŸ¥è´Ÿæ‹…ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿåˆ©ç”¨éšå¼åé¦ˆä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥å®ç°æ›´è‡ªç„¶ã€é«˜æ•ˆçš„äººæœºåä½œã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨éä¾µå…¥å¼è„‘ç”µä¿¡å·ï¼ˆEEGï¼‰ä¸­çš„é”™è¯¯ç›¸å…³ç”µä½ï¼ˆErrPï¼‰ä½œä¸ºéšå¼åé¦ˆä¿¡å·ï¼Œä»£æ›¿æ˜¾å¼çš„ç”¨æˆ·è¾“å…¥ã€‚é€šè¿‡è§£ç ErrPä¿¡å·ï¼Œå°†å…¶è½¬åŒ–ä¸ºæ¦‚ç‡å¥–åŠ±åˆ†é‡ï¼Œä»è€Œåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å‡å°‘ç”¨æˆ·è®¤çŸ¥è´Ÿæ‹…ï¼Œå¹¶å®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥RLIHFæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æœºå™¨äººç¯å¢ƒï¼ˆMuJoCoä»¿çœŸï¼‰ï¼›2) æ™ºèƒ½ä½“ï¼ˆåŸºäºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼‰ï¼›3) EEGä¿¡å·é‡‡é›†è®¾å¤‡ï¼›4) é¢„è®­ç»ƒçš„EEGè§£ç å™¨ï¼ˆå°†EEGä¿¡å·è½¬æ¢ä¸ºæ¦‚ç‡å¥–åŠ±ï¼‰ï¼›5) å¥–åŠ±å‡½æ•°ï¼ˆç»“åˆå¤–éƒ¨å¥–åŠ±å’Œè§£ç åçš„EEGå¥–åŠ±ï¼‰ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šæ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œï¼Œç”¨æˆ·è§‚å¯Ÿå¹¶äº§ç”ŸErrPä¿¡å·ï¼ŒEEGè§£ç å™¨å°†ErrPä¿¡å·è½¬æ¢ä¸ºå¥–åŠ±ï¼Œæ™ºèƒ½ä½“æ ¹æ®å¥–åŠ±æ›´æ–°ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šå…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨éšå¼ç¥ç»åé¦ˆï¼ˆErrPï¼‰ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ã€‚ä¸ä¼ ç»Ÿçš„æ˜¾å¼åé¦ˆæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€ç”¨æˆ·ä¸»åŠ¨æä¾›åé¦ˆï¼Œä»è€Œå‡å°‘äº†ç”¨æˆ·è®¤çŸ¥è´Ÿæ‹…ï¼Œå®ç°äº†æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚æ­¤å¤–ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§£ç å™¨å°†EEGä¿¡å·è½¬æ¢ä¸ºæ¦‚ç‡å¥–åŠ±ï¼Œæé«˜äº†å¥–åŠ±ä¿¡å·çš„å¯é æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šä½¿ç”¨äº†é¢„è®­ç»ƒçš„EEGè§£ç å™¨ï¼Œè¯¥è§£ç å™¨å°†åŸå§‹EEGä¿¡å·æ˜ å°„åˆ°æ¦‚ç‡å¥–åŠ±åˆ†é‡ã€‚å…·ä½“æ¥è¯´ï¼Œè§£ç å™¨å¯èƒ½æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œç”¨äºåŒºåˆ†â€œæ­£ç¡®â€å’Œâ€œé”™è¯¯â€çš„è„‘ç”µä¿¡å·æ¨¡å¼ã€‚å¥–åŠ±å‡½æ•°å°†å¤–éƒ¨å¥–åŠ±ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ä¸è§£ç åçš„EEGå¥–åŠ±ç›¸ç»“åˆï¼Œå½¢æˆæœ€ç»ˆçš„å¥–åŠ±ä¿¡å·ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå…·ä½“ç®—æ³•æœªçŸ¥ï¼ŒåŸæ–‡æœªæåŠï¼‰åˆ©ç”¨è¯¥å¥–åŠ±ä¿¡å·æ¥æ›´æ–°æ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚Kinova Gen2æœºæ¢°è‡‚åœ¨MuJoCoç¯å¢ƒä¸­æ‰§è¡ŒæŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ï¼Œéœ€è¦é¿å¼€éšœç¢ç‰©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è§£ç åçš„EEGåé¦ˆè®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨MuJoCoä»¿çœŸç¯å¢ƒä¸­ï¼Œæ‰§è¡Œå¤æ‚çš„æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡æ—¶ï¼Œå…¶æ€§èƒ½ä¸ä½¿ç”¨å¯†é›†ã€æ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°è®­ç»ƒçš„æ™ºèƒ½ä½“ç›¸å½“ã€‚è¿™è¡¨æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰æ˜¾å¼å¥–åŠ±ä¿¡å·çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨éšå¼ç¥ç»åé¦ˆä¹Ÿèƒ½æœ‰æ•ˆåœ°è®­ç»ƒæ™ºèƒ½ä½“ï¼Œå®ç°ä¸äººç±»å¯¹é½çš„æœºå™¨äººæ§åˆ¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºäººæœºåä½œæœºå™¨äººã€åº·å¤æœºå™¨äººã€è¾…åŠ©è®¾å¤‡ç­‰é¢†åŸŸã€‚é€šè¿‡åˆ©ç”¨ç”¨æˆ·çš„éšå¼ç¥ç»åé¦ˆï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶åšå‡ºç›¸åº”çš„è°ƒæ•´ï¼Œä»è€Œæé«˜äººæœºäº¤äº’çš„æ•ˆç‡å’Œè‡ªç„¶æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸï¼Œä¾‹å¦‚æ™ºèƒ½å®¶å±…ã€è™šæ‹Ÿç°å®ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Conventional reinforcement learning (RL) approaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, reinforcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, enabling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.

