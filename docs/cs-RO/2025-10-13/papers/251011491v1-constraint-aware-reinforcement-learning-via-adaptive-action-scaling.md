---
layout: default
title: Constraint-Aware Reinforcement Learning via Adaptive Action Scaling
---

# Constraint-Aware Reinforcement Learning via Adaptive Action Scaling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11491" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11491v1</a>
  <a href="https://arxiv.org/pdf/2510.11491.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11491v1" onclick="toggleFavorite(this, '2510.11491v1', 'Constraint-Aware Reinforcement Learning via Adaptive Action Scaling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Murad Dawood, Usama Ahmed Siddiquie, Shahram Khorshidi, Maren Bennewitz

**åˆ†ç±»**: cs.RO, cs.LG, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºè‡ªé€‚åº”åŠ¨ä½œç¼©æ”¾çš„çº¦æŸæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæå‡å®‰å…¨æ€§å’Œæ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å®‰å…¨å¼ºåŒ–å­¦ä¹ ` `çº¦æŸæ„ŸçŸ¥` `åŠ¨ä½œç¼©æ”¾` `æˆæœ¬æ„ŸçŸ¥` `Safety Gym`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¥–åŠ±å’Œå®‰å…¨ä¹‹é—´éš¾ä»¥å¹³è¡¡ï¼Œæˆ–ä¾èµ–å¤–éƒ¨å®‰å…¨è¿‡æ»¤å™¨ï¼Œé™åˆ¶äº†æ¢ç´¢å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. è®ºæ–‡æå‡ºä¸€ç§æ¨¡å—åŒ–çš„æˆæœ¬æ„ŸçŸ¥è°ƒèŠ‚å™¨ï¼Œé€šè¿‡è‡ªé€‚åº”ç¼©æ”¾åŠ¨ä½œè€Œéç›´æ¥è¦†ç›–ï¼Œå®ç°æ›´å¹³æ»‘å’Œå®‰å…¨çš„æ¢ç´¢ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Safety Gymä»»åŠ¡ä¸­æ˜¾è‘—é™ä½äº†çº¦æŸè¿åï¼ŒåŒæ—¶æé«˜äº†å›æŠ¥ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§çº¦æŸæ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­å› æ¢ç´¢è€Œäº§ç”Ÿçš„è¿åçº¦æŸé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€ç­–ç•¥æ¥è”åˆä¼˜åŒ–å¥–åŠ±å’Œå®‰å…¨æ€§ï¼Œè¿™å¯èƒ½å› ç›®æ ‡å†²çªè€Œå¯¼è‡´ä¸ç¨³å®šï¼Œæˆ–è€…ä½¿ç”¨å¤–éƒ¨å®‰å…¨è¿‡æ»¤å™¨æ¥è¦†ç›–åŠ¨ä½œï¼Œè¿™éœ€è¦é¢„å…ˆäº†è§£ç³»ç»ŸçŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„æˆæœ¬æ„ŸçŸ¥è°ƒèŠ‚å™¨ï¼Œè¯¥è°ƒèŠ‚å™¨åŸºäºé¢„æµ‹çš„çº¦æŸè¿åæƒ…å†µæ¥ç¼©æ”¾æ™ºèƒ½ä½“çš„åŠ¨ä½œï¼Œé€šè¿‡å¹³æ»‘çš„åŠ¨ä½œè°ƒåˆ¶æ¥ä¿æŒæ¢ç´¢ï¼Œè€Œä¸æ˜¯è¦†ç›–ç­–ç•¥ã€‚è¯¥è°ƒèŠ‚å™¨ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥æœ€å¤§é™åº¦åœ°å‡å°‘çº¦æŸè¿åï¼ŒåŒæ—¶é¿å…å¯¹åŠ¨ä½œçš„é€€åŒ–æŠ‘åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸SACå’ŒTD3ç­‰ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ— ç¼é›†æˆï¼Œå¹¶åœ¨å…·æœ‰ç¨€ç–æˆæœ¬çš„Safety Gymè¿åŠ¨ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å›æŠ¥ä¸æˆæœ¬æ¯”ç‡ï¼Œä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œçº¦æŸè¿åå‡å°‘äº†é«˜è¾¾126å€ï¼ŒåŒæ—¶å›æŠ¥å¢åŠ äº†ä¸€ä¸ªæ•°é‡çº§ä»¥ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦å­˜åœ¨ä¸¤ä¸ªç—›ç‚¹ï¼šä¸€æ˜¯ä¾èµ–å•ä¸€ç­–ç•¥åŒæ—¶ä¼˜åŒ–å¥–åŠ±å’Œå®‰å…¨ï¼Œå®¹æ˜“å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼›äºŒæ˜¯ä½¿ç”¨å¤–éƒ¨å®‰å…¨è¿‡æ»¤å™¨ç›´æ¥è¦†ç›–åŠ¨ä½œï¼Œè™½ç„¶ä¿è¯äº†å®‰å…¨æ€§ï¼Œä½†ç‰ºç‰²äº†æ¢ç´¢èƒ½åŠ›ï¼Œä¸”éœ€è¦é¢„å…ˆäº†è§£ç³»ç»ŸçŸ¥è¯†ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¿è¯å®‰å…¨æ€§çš„å‰æä¸‹ï¼Œç»´æŒæ™ºèƒ½ä½“çš„æ¢ç´¢èƒ½åŠ›ï¼Œæ˜¯æœ¬è®ºæ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¼•å…¥ä¸€ä¸ªæ¨¡å—åŒ–çš„æˆæœ¬æ„ŸçŸ¥è°ƒèŠ‚å™¨ï¼Œè¯¥è°ƒèŠ‚å™¨ä¸ç›´æ¥è¦†ç›–æ™ºèƒ½ä½“çš„åŠ¨ä½œï¼Œè€Œæ˜¯æ ¹æ®é¢„æµ‹çš„çº¦æŸè¿åæƒ…å†µï¼Œè‡ªé€‚åº”åœ°ç¼©æ”¾æ™ºèƒ½ä½“çš„åŠ¨ä½œã€‚é€šè¿‡è¿™ç§å¹³æ»‘çš„åŠ¨ä½œè°ƒåˆ¶ï¼Œæ—¢å¯ä»¥é™ä½çº¦æŸè¿åçš„é£é™©ï¼Œåˆå¯ä»¥ä¿ç•™æ™ºèƒ½ä½“çš„æ¢ç´¢èƒ½åŠ›ã€‚è°ƒèŠ‚å™¨çš„ç›®æ ‡æ˜¯æœ€å°åŒ–çº¦æŸè¿åï¼ŒåŒæ—¶é¿å…å¯¹åŠ¨ä½œçš„è¿‡åº¦æŠ‘åˆ¶ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸€ä¸ªæ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼ˆå¦‚SACæˆ–TD3ï¼‰å’Œä¸€ä¸ªæˆæœ¬æ„ŸçŸ¥è°ƒèŠ‚å™¨ã€‚æ™ºèƒ½ä½“æ ¹æ®ç¯å¢ƒçŠ¶æ€è¾“å‡ºåŠ¨ä½œï¼Œè°ƒèŠ‚å™¨æ¥æ”¶æ™ºèƒ½ä½“çš„åŠ¨ä½œå’Œç¯å¢ƒçŠ¶æ€ï¼Œé¢„æµ‹çº¦æŸè¿åæƒ…å†µï¼Œå¹¶æ ¹æ®é¢„æµ‹ç»“æœå¯¹åŠ¨ä½œè¿›è¡Œç¼©æ”¾ï¼Œç„¶åå°†ç¼©æ”¾åçš„åŠ¨ä½œå‘é€åˆ°ç¯å¢ƒä¸­æ‰§è¡Œã€‚è°ƒèŠ‚å™¨ä¸æ™ºèƒ½ä½“å¯ä»¥ç‹¬ç«‹è®­ç»ƒï¼Œå®ç°æ¨¡å—åŒ–è®¾è®¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºè‡ªé€‚åº”åŠ¨ä½œç¼©æ”¾æœºåˆ¶ã€‚ä¸ç›´æ¥è¦†ç›–åŠ¨ä½œçš„ç­–ç•¥ç›¸æ¯”ï¼Œè‡ªé€‚åº”åŠ¨ä½œç¼©æ”¾èƒ½å¤Ÿæ›´å¹³æ»‘åœ°è°ƒæ•´æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œä»è€Œæ›´å¥½åœ°å¹³è¡¡å®‰å…¨æ€§å’Œæ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè°ƒèŠ‚å™¨çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–çº¦æŸè¿åï¼ŒåŒæ—¶é¿å…å¯¹åŠ¨ä½œçš„è¿‡åº¦æŠ‘åˆ¶ï¼Œè¿™æœ‰åŠ©äºé˜²æ­¢è°ƒèŠ‚å™¨é€€åŒ–ä¸ºç®€å•çš„åŠ¨ä½œæŠ‘åˆ¶å™¨ã€‚

**å…³é”®è®¾è®¡**ï¼šè°ƒèŠ‚å™¨é€šå¸¸é‡‡ç”¨ç¥ç»ç½‘ç»œç»“æ„ï¼Œè¾“å…¥ä¸ºç¯å¢ƒçŠ¶æ€å’Œæ™ºèƒ½ä½“çš„åŠ¨ä½œï¼Œè¾“å‡ºä¸ºåŠ¨ä½œçš„ç¼©æ”¾å› å­ã€‚æŸå¤±å‡½æ•°åŒ…å«ä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯çº¦æŸè¿åæŸå¤±ï¼Œç”¨äºæƒ©ç½šè¿åçº¦æŸçš„è¡Œä¸ºï¼›å¦ä¸€éƒ¨åˆ†æ˜¯åŠ¨ä½œæŠ‘åˆ¶æŸå¤±ï¼Œç”¨äºé˜²æ­¢è°ƒèŠ‚å™¨è¿‡åº¦æŠ‘åˆ¶åŠ¨ä½œã€‚å…·ä½“å®ç°ä¸­ï¼Œå¯ä»¥é‡‡ç”¨ä¸åŒçš„ç¥ç»ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°å½¢å¼ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤±æ¥è¡¡é‡çº¦æŸè¿åç¨‹åº¦ï¼Œå¹¶ä½¿ç”¨L1æ­£åˆ™åŒ–æ¥çº¦æŸåŠ¨ä½œç¼©æ”¾å› å­çš„å¹…åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Safety Gym locomotionä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œçº¦æŸè¿åå‡å°‘äº†é«˜è¾¾126å€ï¼ŒåŒæ—¶å›æŠ¥å¢åŠ äº†ä¸€ä¸ªæ•°é‡çº§ä»¥ä¸Šã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡å®‰å…¨æ€§å’Œæ€§èƒ½ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸SACå’ŒTD3ç­‰ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ— ç¼é›†æˆï¼Œå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦å®‰å…¨ä¿éšœçš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶ã€åŒ»ç–—å†³ç­–ç­‰ã€‚é€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œå¯ä»¥æœ‰æ•ˆé™ä½äº‹æ•…é£é™©ï¼Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œé€šè¿‡çº¦æŸèµ„æºæ¶ˆè€—ï¼Œå®ç°æ›´é«˜æ•ˆçš„èµ„æºåˆ©ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that arise from exploration during training by reducing constraint violations while maintaining task performance. Existing approaches typically rely on a single policy to jointly optimize reward and safety, which can cause instability due to conflicting objectives, or they use external safety filters that override actions and require prior system knowledge. In this paper, we propose a modular cost-aware regulator that scales the agent's actions based on predicted constraint violations, preserving exploration through smooth action modulation rather than overriding the policy. The regulator is trained to minimize constraint violations while avoiding degenerate suppression of actions. Our approach integrates seamlessly with off-policy RL methods such as SAC and TD3, and achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion tasks with sparse costs, reducing constraint violations by up to 126 times while increasing returns by over an order of magnitude compared to prior methods.

