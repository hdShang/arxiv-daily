---
layout: default
title: Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy
---

# Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11041" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11041v1</a>
  <a href="https://arxiv.org/pdf/2510.11041.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11041v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11041v1', 'Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shiyao Zhang, Liwei Deng, Shuyu Zhang, Weijie Yuan, Hong Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**å¤‡æ³¨**: Accepted by IEEE RA-L

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„è‡ªä¸»ååŒå­¦ä¹ è§„åˆ’ç­–ç•¥ï¼Œæå‡å¤šè½¦äº¤äº’çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è‡ªä¸»ååŒè§„åˆ’` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `ä¸ç¡®å®šæ€§æ„ŸçŸ¥` `è½¯æ¼”å‘˜-è¯„è®ºå®¶` `é—¨æ§å¾ªç¯å•å…ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è‡ªä¸»ååŒè§„åˆ’æ–¹æ³•éš¾ä»¥å……åˆ†è§£å†³æ„ŸçŸ¥ã€è§„åˆ’å’Œé€šä¿¡ç­‰å¤šç§ä¸ç¡®å®šæ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚
2. æå‡ºä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»ååŒè§„åˆ’æ¡†æ¶ï¼Œåˆ©ç”¨SACç®—æ³•å’ŒGRUç½‘ç»œå­¦ä¹ ä¸ç¡®å®šçŠ¶æ€ä¸‹çš„æœ€ä¼˜åŠ¨ä½œã€‚
3. åœ¨CARLAä»¿çœŸå¹³å°ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœªæ¥çš„æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­ï¼Œè‡ªä¸»ååŒè§„åˆ’(ACP)æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„æŠ€æœ¯ï¼Œå¯ä»¥æé«˜å¤šè½¦è¾†äº¤äº’çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ACPç­–ç•¥æ— æ³•å®Œå…¨è§£å†³å¤šç§ä¸ç¡®å®šæ€§ï¼Œä¾‹å¦‚æ„ŸçŸ¥ã€è§„åˆ’å’Œé€šä¿¡çš„ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»ååŒè§„åˆ’(DRLACP)æ¡†æ¶ï¼Œä»¥åº”å¯¹ååŒè¿åŠ¨è§„åˆ’æ–¹æ¡ˆä¸­çš„å„ç§ä¸ç¡®å®šæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œé‡‡ç”¨å¸¦æœ‰é—¨æ§å¾ªç¯å•å…ƒ(GRU)çš„è½¯æ¼”å‘˜-è¯„è®ºå®¶(SAC)ç®—æ³•ï¼Œä»¥å­¦ä¹ ç”±è§„åˆ’ã€é€šä¿¡å’Œæ„ŸçŸ¥ä¸ç¡®å®šæ€§å¼•èµ·çš„ä¸å®Œå–„çŠ¶æ€ä¿¡æ¯ä¸‹çš„ç¡®å®šæ€§æœ€ä¼˜æ—¶å˜åŠ¨ä½œã€‚æ­¤å¤–ï¼Œè‡ªä¸»è½¦è¾†(AV)çš„å®æ—¶åŠ¨ä½œé€šè¿‡Car Learning to Act (CARLA)ä»¿çœŸå¹³å°è¿›è¡Œæ¼”ç¤ºã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„DRLACPèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å’Œæ‰§è¡ŒååŒè§„åˆ’ï¼Œå¹¶ä¸”åœ¨å…·æœ‰ä¸å®Œå–„AVçŠ¶æ€ä¿¡æ¯çš„ä¸åŒåœºæ™¯ä¸‹ï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“è‡ªä¸»ååŒè§„åˆ’ä¸­ï¼Œç”±äºæ„ŸçŸ¥ã€é€šä¿¡å’Œè§„åˆ’æœ¬èº«çš„ä¸ç¡®å®šæ€§å¯¼è‡´çš„è§„åˆ’æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾ç¯å¢ƒæ˜¯å®Œå…¨å¯è§‚æµ‹çš„ï¼Œæˆ–è€…å¿½ç•¥è¿™äº›ä¸ç¡®å®šæ€§ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­æ•ˆæœä¸ä½³ã€‚å› æ­¤ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è¿™äº›ä¸ç¡®å®šæ€§çš„è‡ªä¸»ååŒè§„åˆ’ç­–ç•¥æ˜¯æœ¬æ–‡è¦è§£å†³çš„å…³é”®é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ çš„æ–¹å¼æ¥é€‚åº”å’Œå¤„ç†è¿™äº›ä¸ç¡®å®šæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œé‡‡ç”¨è½¯æ¼”å‘˜-è¯„è®ºå®¶(SAC)ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå­¦ä¹ éšæœºç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°æ¢ç´¢ç¯å¢ƒå¹¶æ‰¾åˆ°æœ€ä¼˜è§£ã€‚æ­¤å¤–ï¼Œå¼•å…¥é—¨æ§å¾ªç¯å•å…ƒ(GRU)æ¥å¤„ç†æ—¶åºä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°å»ºæ¨¡è½¦è¾†ä¹‹é—´çš„äº¤äº’å’ŒçŠ¶æ€å˜åŒ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ç¯å¢ƒå»ºæ¨¡ï¼šä½¿ç”¨CARLAä»¿çœŸå¹³å°æ¨¡æ‹ŸçœŸå®äº¤é€šç¯å¢ƒï¼ŒåŒ…æ‹¬è½¦è¾†åŠ¨åŠ›å­¦ã€ä¼ æ„Ÿå™¨å™ªå£°å’Œé€šä¿¡å»¶è¿Ÿç­‰ã€‚2) çŠ¶æ€è¡¨ç¤ºï¼šå°†è½¦è¾†çš„çŠ¶æ€ä¿¡æ¯ï¼ˆä½ç½®ã€é€Ÿåº¦ã€åŠ é€Ÿåº¦ç­‰ï¼‰ä»¥åŠå…¶ä»–è½¦è¾†çš„ä¿¡æ¯ä½œä¸ºè¾“å…¥ã€‚3) åŠ¨ä½œç©ºé—´ï¼šå®šä¹‰è½¦è¾†å¯ä»¥æ‰§è¡Œçš„åŠ¨ä½œï¼Œä¾‹å¦‚åŠ é€Ÿã€å‡é€Ÿå’Œè½¬å‘ã€‚4) å¥–åŠ±å‡½æ•°ï¼šè®¾è®¡å¥–åŠ±å‡½æ•°æ¥é¼“åŠ±è½¦è¾†å®‰å…¨ã€é«˜æ•ˆåœ°å®ŒæˆååŒè§„åˆ’ä»»åŠ¡ã€‚5) DRLæ™ºèƒ½ä½“ï¼šä½¿ç”¨SACç®—æ³•å’ŒGRUç½‘ç»œæ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†SACç®—æ³•å’ŒGRUç½‘ç»œç»“åˆèµ·æ¥ï¼Œç”¨äºè§£å†³è‡ªä¸»ååŒè§„åˆ’ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚SACç®—æ³•èƒ½å¤Ÿå­¦ä¹ éšæœºç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°æ¢ç´¢ç¯å¢ƒï¼Œè€ŒGRUç½‘ç»œèƒ½å¤Ÿå¤„ç†æ—¶åºä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°å»ºæ¨¡è½¦è¾†ä¹‹é—´çš„äº¤äº’ã€‚è¿™ç§ç»“åˆä½¿å¾—è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ„ŸçŸ¥ã€é€šä¿¡å’Œè§„åˆ’ä¸­çš„ä¸ç¡®å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç½‘ç»œç»“æ„æ–¹é¢ï¼ŒGRUç½‘ç»œç”¨äºæå–çŠ¶æ€ä¿¡æ¯çš„æ—¶åºç‰¹å¾ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾è¾“å…¥åˆ°SACç®—æ³•çš„æ¼”å‘˜å’Œè¯„è®ºå®¶ç½‘ç»œä¸­ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†å¤šä¸ªå› ç´ ï¼ŒåŒ…æ‹¬å®‰å…¨æ€§ï¼ˆé¿å…ç¢°æ’ï¼‰ã€æ•ˆç‡ï¼ˆå°½å¿«åˆ°è¾¾ç›®çš„åœ°ï¼‰å’Œèˆ’é€‚æ€§ï¼ˆé¿å…æ€¥åŠ é€Ÿå’Œæ€¥å‡é€Ÿï¼‰ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®å…·ä½“çš„ä»¿çœŸç¯å¢ƒè¿›è¡Œè°ƒæ•´ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­å’Œæ¢ç´¢ç‡ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„DRLACPæ–¹æ³•åœ¨ä¸åŒçš„ä»¿çœŸåœºæ™¯ä¸‹å‡ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨é«˜é€Ÿå…¬è·¯æ±‡å…¥åœºæ™¯ä¸­ï¼ŒDRLACPæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å‡å°‘ç¢°æ’æ¬¡æ•°ï¼Œå¹¶æé«˜è½¦è¾†çš„å¹³å‡é€Ÿåº¦ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•ç›¸æ¯”ï¼ŒDRLACPæ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„äº¤é€šçŠ¶å†µï¼Œå¹¶åšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚å…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼ŒåŸæ–‡æœªç»™å‡ºå…·ä½“æ•°å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœªæ¥çš„æ™ºèƒ½äº¤é€šç³»ç»Ÿï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶å‡ºç§Ÿè½¦ã€è‡ªåŠ¨é©¾é©¶ç‰©æµè½¦é˜Ÿç­‰ã€‚é€šè¿‡æé«˜å¤šè½¦è¾†ååŒè§„åˆ’çš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œå¯ä»¥å‡å°‘äº¤é€šäº‹æ•…ã€ç¼“è§£äº¤é€šæ‹¥å µï¼Œå¹¶æé«˜è¿è¾“æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¤šæ™ºèƒ½ä½“ååŒä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººç¼–é˜Ÿã€æ— äººæœºååŒç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In future intelligent transportation systems, autonomous cooperative planning (ACP), becomes a promising technique to increase the effectiveness and security of multi-vehicle interactions. However, multiple uncertainties cannot be fully addressed for existing ACP strategies, e.g. perception, planning, and communication uncertainties. To address these, a novel deep reinforcement learning-based autonomous cooperative planning (DRLACP) framework is proposed to tackle various uncertainties on cooperative motion planning schemes. Specifically, the soft actor-critic (SAC) with the implementation of gate recurrent units (GRUs) is adopted to learn the deterministic optimal time-varying actions with imperfect state information occurred by planning, communication, and perception uncertainties. In addition, the real-time actions of autonomous vehicles (AVs) are demonstrated via the Car Learning to Act (CARLA) simulation platform. Evaluation results show that the proposed DRLACP learns and performs cooperative planning effectively, which outperforms other baseline methods under different scenarios with imperfect AV state information.

