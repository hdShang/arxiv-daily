---
layout: default
title: Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning
---

# Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11474" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11474v2</a>
  <a href="https://arxiv.org/pdf/2510.11474.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11474v2" onclick="toggleFavorite(this, '2510.11474v2', 'Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci

**åˆ†ç±»**: cs.RO, cs.AI, cs.HC, cs.LG, cs.MA

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13 (æ›´æ–°: 2025-10-22)

**å¤‡æ³¨**: 2025 IEEE International Conference on Agentic AI (ICA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºåˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„ç©ºæˆ˜ååŒç­–ç•¥ï¼Œè§£å†³å¤æ‚ç©ºæˆ˜ç¯å¢ƒä¸‹çš„å†³ç­–éš¾é¢˜ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åˆ†å±‚å¼ºåŒ–å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `ç©ºæˆ˜æ¨¡æ‹Ÿ` `ååŒç­–ç•¥` `è¯¾ç¨‹å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çœŸå®ç©ºæˆ˜æ¨¡æ‹Ÿé¢ä¸´æ€åŠ¿æ„ŸçŸ¥ä¸å®Œå–„å’Œéçº¿æ€§åŠ¨åŠ›å­¦æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹ã€‚
2. é‡‡ç”¨åˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼Œå°†å†³ç­–åˆ†è§£ä¸ºä½å±‚æ§åˆ¶å’Œé«˜å±‚æˆ˜æœ¯ï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚ç©ºæˆ˜åœºæ™¯ä¸­æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡å’Œä½œæˆ˜æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Då¤šæ™ºèƒ½ä½“ç©ºæˆ˜ç¯å¢ƒå’Œä¸€ä¸ªåˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥åº”å¯¹çœŸå®ç©ºæˆ˜æ¨¡æ‹Ÿä¸­ç”±äºä¸å®Œå–„çš„æ€åŠ¿æ„ŸçŸ¥å’Œéçº¿æ€§é£è¡ŒåŠ¨åŠ›å­¦å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¼‚æ„æ™ºèƒ½ä½“åŠ¨åŠ›å­¦ã€è¯¾ç¨‹å­¦ä¹ ã€è”ç›Ÿåšå¼ˆä»¥åŠæ–°è¿‘è°ƒæ•´çš„è®­ç»ƒç®—æ³•ã€‚å†³ç­–è¿‡ç¨‹è¢«ç»„ç»‡æˆä¸¤ä¸ªæŠ½è±¡å±‚æ¬¡ï¼šä½å±‚ç­–ç•¥å­¦ä¹ ç²¾ç¡®çš„æ§åˆ¶åŠ¨ä½œï¼Œè€Œé«˜å±‚ç­–ç•¥æ ¹æ®ä»»åŠ¡ç›®æ ‡å‘å‡ºæˆ˜æœ¯æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥åˆ†å±‚æ–¹æ³•æé«˜äº†å¤æ‚ç©ºæˆ˜åœºæ™¯ä¸­çš„å­¦ä¹ æ•ˆç‡å’Œä½œæˆ˜æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³çœŸå®ç©ºæˆ˜æ¨¡æ‹Ÿä¸­ï¼Œç”±äºä¸å®Œå–„çš„æ€åŠ¿æ„ŸçŸ¥å’Œéçº¿æ€§é£è¡ŒåŠ¨åŠ›å­¦å¸¦æ¥çš„å¤æ‚å†³ç­–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒä¸­æœ‰æ•ˆåœ°å­¦ä¹ å’Œæ‰§è¡ŒååŒä½œæˆ˜ç­–ç•¥ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹å¼‚æ„æ™ºèƒ½ä½“æ—¶ï¼Œå­¦ä¹ æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½éƒ½å­˜åœ¨ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å†³ç­–è¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼šä½å±‚è´Ÿè´£ç²¾ç¡®çš„æ§åˆ¶åŠ¨ä½œï¼Œé«˜å±‚è´Ÿè´£æ ¹æ®ä»»åŠ¡ç›®æ ‡åˆ¶å®šæˆ˜æœ¯æŒ‡ä»¤ã€‚é€šè¿‡åˆ†å±‚ç»“æ„ï¼Œé™ä½äº†å­¦ä¹ çš„å¤æ‚æ€§ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢å’Œåˆ©ç”¨ç¯å¢ƒä¿¡æ¯ï¼Œä»è€Œå­¦ä¹ åˆ°æ›´ä¼˜çš„ååŒç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦å±‚æ¬¡ï¼šé«˜å±‚æˆ˜æœ¯å†³ç­–å±‚å’Œä½å±‚æ§åˆ¶æ‰§è¡Œå±‚ã€‚é«˜å±‚ç­–ç•¥æ¥æ”¶ç¯å¢ƒçŠ¶æ€ä¿¡æ¯ï¼Œå¹¶æ ¹æ®ä»»åŠ¡ç›®æ ‡é€‰æ‹©åˆé€‚çš„æˆ˜æœ¯æŒ‡ä»¤ï¼ˆä¾‹å¦‚æ”»å‡»ã€é˜²å¾¡ã€è§„é¿ç­‰ï¼‰ã€‚ä½å±‚ç­–ç•¥æ¥æ”¶é«˜å±‚æŒ‡ä»¤ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå…·ä½“çš„æ§åˆ¶åŠ¨ä½œï¼Œä¾‹å¦‚æ²¹é—¨ã€æ–¹å‘èˆµã€å‡é™èˆµç­‰çš„æ§åˆ¶ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ å’Œè”ç›Ÿåšå¼ˆï¼Œå¹¶ç»“åˆäº†ä¸€ç§æ–°è°ƒæ•´çš„è®­ç»ƒç®—æ³•ï¼Œä»¥æé«˜å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†åˆ†å±‚å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤æ‚ç©ºæˆ˜ç¯å¢ƒï¼Œå¹¶ç»“åˆäº†å¼‚æ„æ™ºèƒ½ä½“åŠ¨åŠ›å­¦ã€è¯¾ç¨‹å­¦ä¹ å’Œè”ç›Ÿåšå¼ˆã€‚è¿™ç§åˆ†å±‚ç»“æ„ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å’Œæ‰§è¡ŒååŒä½œæˆ˜ç­–ç•¥ï¼Œä»è€Œåœ¨å¤æ‚ç¯å¢ƒä¸­å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç©ºæˆ˜ç¯å¢ƒçš„ç‰¹ç‚¹ï¼Œå¯¹ç°æœ‰çš„è®­ç»ƒç®—æ³•è¿›è¡Œäº†è°ƒæ•´ï¼Œæé«˜äº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šé«˜å±‚ç­–ç•¥å’Œä½å±‚ç­–ç•¥å‡é‡‡ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå»ºæ¨¡ã€‚é«˜å±‚ç­–ç•¥çš„ç½‘ç»œç»“æ„éœ€è¦èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ç¯å¢ƒçŠ¶æ€ä¿¡æ¯ï¼Œå¹¶è¾“å‡ºåˆé€‚çš„æˆ˜æœ¯æŒ‡ä»¤ã€‚ä½å±‚ç­–ç•¥çš„ç½‘ç»œç»“æ„éœ€è¦èƒ½å¤Ÿå°†æˆ˜æœ¯æŒ‡ä»¤è½¬åŒ–ä¸ºç²¾ç¡®çš„æ§åˆ¶åŠ¨ä½œã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è¦èƒ½å¤Ÿåæ˜ ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶é¼“åŠ±æ™ºèƒ½ä½“ä¹‹é—´çš„ååŒã€‚è¯¾ç¨‹å­¦ä¹ çš„è®¾è®¡éœ€è¦ä»ç®€å•åˆ°å¤æ‚ï¼Œé€æ­¥æé«˜è®­ç»ƒéš¾åº¦ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚è”ç›Ÿåšå¼ˆçš„è®¾è®¡éœ€è¦é¼“åŠ±æ™ºèƒ½ä½“ä¹‹é—´çš„åˆä½œï¼Œå¹¶é¿å…å‡ºç°â€œæ­ä¾¿è½¦â€ç°è±¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚ç©ºæˆ˜åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆç‡å’Œä½œæˆ˜æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å•å±‚å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°æœ‰æ•ˆçš„ååŒç­–ç•¥ï¼Œå¹¶ä¸”åœ¨å¯¹æŠ—ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®ï¼ˆä¾‹å¦‚èƒœç‡ã€ç”Ÿå­˜æ—¶é—´ç­‰ï¼‰åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å¯¹æ¯”å’Œåˆ†æã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºç©ºæˆ˜æˆ˜æœ¯æ¨¡æ‹Ÿã€æ— äººæœºé›†ç¾¤æ§åˆ¶ã€ä»¥åŠå…¶ä»–éœ€è¦å¤šæ™ºèƒ½ä½“ååŒçš„å¤æ‚ä»»åŠ¡ã€‚é€šè¿‡åˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥è®­ç»ƒå‡ºèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è‡ªä¸»å†³ç­–å’ŒååŒä½œæˆ˜çš„æ™ºèƒ½ä½“ï¼Œä»è€Œæé«˜ä½œæˆ˜æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººåä½œã€äº¤é€šç®¡ç†ç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Achieving mission objectives in a realistic simulation of aerial combat is highly challenging due to imperfect situational awareness and nonlinear flight dynamics. In this work, we introduce a novel 3D multi-agent air combat environment and a Hierarchical Multi-Agent Reinforcement Learning framework to tackle these challenges. Our approach combines heterogeneous agent dynamics, curriculum learning, league-play, and a newly adapted training algorithm. To this end, the decision-making process is organized into two abstraction levels: low-level policies learn precise control maneuvers, while high-level policies issue tactical commands based on mission objectives. Empirical results show that our hierarchical approach improves both learning efficiency and combat performance in complex dogfight scenarios.

