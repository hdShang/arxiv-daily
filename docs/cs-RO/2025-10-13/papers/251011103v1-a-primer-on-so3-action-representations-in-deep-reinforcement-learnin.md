---
layout: default
title: A Primer on SO(3) Action Representations in Deep Reinforcement Learning
---

# A Primer on SO(3) Action Representations in Deep Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11103" target="_blank" class="toolbar-btn">arXiv: 2510.11103v1</a>
    <a href="https://arxiv.org/pdf/2510.11103.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11103v1" 
            onclick="toggleFavorite(this, '2510.11103v1', 'A Primer on SO(3) Action Representations in Deep Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Martin Schuck, Sherif Samy, Angela P. Schoellig

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**Á†îÁ©∂SO(3)‰ΩúÁî®Ë°®Á§∫ÂØπÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁöÑÂΩ±ÂìçÔºåÊèêÂá∫Âü∫‰∫éÂ±ÄÈÉ®ÂùêÊ†áÁ≥ªÂàáÂêëÈáèÁöÑÂä®‰ΩúË°®Á§∫ÊñπÊ≥ï„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `SO(3)Ë°®Á§∫` `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Âä®‰ΩúÁ©∫Èó¥` `ËøûÁª≠ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâSO(3)Ë°®Á§∫ÊñπÊ≥ïÔºàÂ¶ÇÊ¨ßÊãâËßí„ÄÅÂõõÂÖÉÊï∞ÔºâÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠Â≠òÂú®Á∫¶ÊùüÂíåÂ§±ÊïàÊ®°ÂºèÔºåÂΩ±ÂìçÁ≠ñÁï•Â≠¶‰π†„ÄÇ
2. ËÆ∫ÊñáÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÁ†îÁ©∂‰∏çÂêåSO(3)Âä®‰ΩúË°®Á§∫ÂØπÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÂΩ±ÂìçÔºåÂπ∂ÊèêÂá∫Êõ¥ÊúâÊïàÁöÑË°®Á§∫ÊñπÊ≥ï„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Â±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏≠Â∞ÜÂä®‰ΩúË°®Á§∫‰∏∫ÂàáÂêëÈáèÔºåÂú®Â§öÁßçÊú∫Âô®‰∫∫‰ªªÂä°ÂíåÁÆóÊ≥ï‰∏≠Ë°®Áé∞Êõ¥Á®≥ÂÆöÂèØÈù†„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËÆ∏Â§öÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°ÈúÄË¶ÅÁ≠ñÁï•‰ΩúÁî®‰∫éÊñπÂêëÔºå‰ΩÜSO(3)ÁöÑÂá†‰ΩïÁâπÊÄß‰ΩøÂÖ∂ÂèòÂæó‰∏çÁÆÄÂçï„ÄÇÁî±‰∫éSO(3)‰∏çÂ≠òÂú®ÂÖ®Â±Ä„ÄÅÂπ≥Êªë„ÄÅÊúÄÂ∞èÁöÑÂèÇÊï∞ÂåñË°®Á§∫ÔºåÂõ†Ê≠§Â∏∏ËßÅÁöÑË°®Á§∫ÊñπÊ≥ïÔºåÂ¶ÇÊ¨ßÊãâËßí„ÄÅÂõõÂÖÉÊï∞„ÄÅÊóãËΩ¨Áü©ÈòµÂíåÊùé‰ª£Êï∞ÂùêÊ†áÔºåÈÉΩÂºïÂÖ•‰∫Ü‰∏çÂêåÁöÑÁ∫¶ÊùüÂíåÂ§±ÊïàÊ®°Âºè„ÄÇËôΩÁÑ∂Ëøô‰∫õÊùÉË°°Âú®ÁõëÁù£Â≠¶‰π†‰∏≠ÂæóÂà∞‰∫ÜÂÖÖÂàÜÁ†îÁ©∂Ôºå‰ΩÜÂÆÉ‰ª¨ÂØπÂº∫ÂåñÂ≠¶‰π†‰∏≠Âä®‰ΩúÁöÑÂΩ±Âìç‰ªç‰∏çÊ∏ÖÊ•ö„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞ËØÑ‰º∞‰∫ÜSO(3)Âä®‰ΩúË°®Á§∫Âú®‰∏âÁßçÊ†áÂáÜËøûÁª≠ÊéßÂà∂ÁÆóÊ≥ïÔºàPPO„ÄÅSACÂíåTD3Ôºâ‰∏ãÁöÑË°®Áé∞ÔºåÂåÖÊã¨Á®†ÂØÜÂíåÁ®ÄÁñèÂ•ñÂä±„ÄÇÊàë‰ª¨ÊØîËæÉ‰∫ÜË°®Á§∫Â¶Ç‰ΩïÂ°ëÈÄ†Êé¢Á¥¢ÔºåÂ¶Ç‰Ωï‰∏éÁÜµÊ≠£ÂàôÂåñÁõ∏‰∫í‰ΩúÁî®Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÈÄöËøáÂÆûËØÅÁ†îÁ©∂ÂΩ±ÂìçËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÔºåÂπ∂ÂàÜÊûê‰∫Ü‰∏çÂêåÊäïÂΩ±ÂØπ‰ªéÊ¨ßÂá†ÈáåÂæ∑ÁΩëÁªúËæìÂá∫‰∏≠Ëé∑ÂæóÊúâÊïàÊóãËΩ¨ÁöÑÂΩ±Âìç„ÄÇÂú®‰∏ÄÁ≥ªÂàóÊú∫Âô®‰∫∫Âü∫ÂáÜÊµãËØï‰∏≠ÔºåÊàë‰ª¨ÈáèÂåñ‰∫ÜËøô‰∫õÈÄâÊã©ÁöÑÂÆûÈôÖÂΩ±ÂìçÔºåÂπ∂ÊèêÁÇºÂá∫ÁÆÄÂçï„ÄÅÂèØÁ´ãÂç≥ÂÆûÁé∞ÁöÑÊåáÂçóÔºåÁî®‰∫éÈÄâÊã©Âíå‰ΩøÁî®ÊóãËΩ¨Âä®‰Ωú„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåË°®Á§∫ËØ±ÂØºÁöÑÂá†‰ΩïÁªìÊûÑÂº∫ÁÉàÂΩ±ÂìçÊé¢Á¥¢Âíå‰ºòÂåñÔºåÂπ∂Ë°®ÊòéÂú®Â±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏≠Â∞ÜÂä®‰ΩúË°®Á§∫‰∏∫ÂàáÂêëÈáèÂèØ‰ª•‰∫ßÁîüÊúÄÂèØÈù†ÁöÑÁÆóÊ≥ïÁªìÊûú„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Âº∫ÂåñÂ≠¶‰π†‰∏≠ÔºåÁî±‰∫éSO(3)Áæ§ÁöÑÁâπÊÆäÂá†‰ΩïÊÄßË¥®ÔºåÂØºËá¥‰ΩøÁî®‰º†ÁªüÊóãËΩ¨Ë°®Á§∫ÊñπÊ≥ïÔºàÂ¶ÇÊ¨ßÊãâËßí„ÄÅÂõõÂÖÉÊï∞Á≠âÔºâ‰Ωú‰∏∫Âä®‰ΩúÁ©∫Èó¥Êó∂ÔºåËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÅÊé¢Á¥¢ÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁöÑÁóõÁÇπÂú®‰∫éÔºåËøô‰∫õË°®Á§∫ÊñπÊ≥ïË¶Å‰πàÂ≠òÂú®Â•áÂºÇÊÄßÔºåË¶Å‰πàÂèÇÊï∞ÂÜó‰ΩôÔºåË¶Å‰πàÊó†Ê≥ï‰øùËØÅÊóãËΩ¨Áü©ÈòµÁöÑÊúâÊïàÊÄßÔºå‰ªéËÄåÂΩ±ÂìçÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉËß£ÂÜ≥ÊÄùË∑ØÊòØÁ≥ªÁªüÊÄßÂú∞ËØÑ‰º∞‰∏çÂêåSO(3)Âä®‰ΩúË°®Á§∫ÊñπÊ≥ïÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑË°®Áé∞ÔºåÂπ∂ÊâæÂà∞‰∏ÄÁßçÊõ¥ÈÄÇÂêàÂº∫ÂåñÂ≠¶‰π†ÁöÑË°®Á§∫ÊñπÊ≥ï„ÄÇ‰ΩúËÄÖËÆ§‰∏∫ÔºåÂêàÈÄÇÁöÑË°®Á§∫ÊñπÊ≥ïÂ∫îËØ•ËÉΩÂ§üÊúâÊïàÂú∞ÂºïÂØºÊé¢Á¥¢Ôºå‰∏éÁÜµÊ≠£ÂàôÂåñÂÖºÂÆπÔºåÂπ∂‰øùËØÅËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄß„ÄÇÈÄöËøáÂÆûÈ™åÂØπÊØîÔºå‰ΩúËÄÖÂèëÁé∞Â∞ÜÂä®‰ΩúË°®Á§∫‰∏∫Â±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏ãÁöÑÂàáÂêëÈáèÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Êª°Ë∂≥Ëøô‰∫õË¶ÅÊ±Ç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÈááÁî®ÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÊòØÔºöÈ¶ñÂÖàÔºåÈÄâÊã©‰∏âÁßçÂ∏∏Áî®ÁöÑËøûÁª≠ÊéßÂà∂Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºàPPO„ÄÅSAC„ÄÅTD3Ôºâ‰Ωú‰∏∫Âü∫ÂáÜÁÆóÊ≥ïÔºõÁÑ∂ÂêéÔºåÈíàÂØπÊØèÁßçÁÆóÊ≥ïÔºåÂàÜÂà´‰ΩøÁî®‰∏çÂêåÁöÑSO(3)Âä®‰ΩúË°®Á§∫ÊñπÊ≥ïËøõË°åËÆ≠ÁªÉÂíåÊµãËØïÔºõÊúÄÂêéÔºåÈÄöËøáÂØπÊØîÂÆûÈ™åÁªìÊûúÔºåÂàÜÊûê‰∏çÂêåË°®Á§∫ÊñπÊ≥ïÂØπÁÆóÊ≥ïÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÂÆûÈ™å‰∏≠‰ΩøÁî®‰∫ÜÂ§öÁßçÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°‰Ωú‰∏∫benchmarkÔºåÂåÖÊã¨Á®†ÂØÜÂ•ñÂä±ÂíåÁ®ÄÁñèÂ•ñÂä±‰∏§ÁßçÊÉÖÂÜµ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÔºåÈÄöËøáÂÆûÈ™åËØÅÊòé‰∫ÜÂú®Â±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏ã‰ΩøÁî®ÂàáÂêëÈáèË°®Á§∫SO(3)Âä®‰ΩúÁöÑÊúâÊïàÊÄß„ÄÇËøôÁßçË°®Á§∫ÊñπÊ≥ïÈÅøÂÖç‰∫Ü‰º†ÁªüË°®Á§∫ÊñπÊ≥ïÁöÑÂ•áÂºÇÊÄßÂíåÂÜó‰ΩôÊÄßÈóÆÈ¢òÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÂºïÂØºÊé¢Á¥¢ÔºåÂπ∂ÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄß„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÁ≥ªÁªüÂú∞ÂàÜÊûê‰∫Ü‰∏çÂêåSO(3)Ë°®Á§∫ÊñπÊ≥ï‰∏éÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®Ôºå‰∏∫ÈÄâÊã©ÂêàÈÄÇÁöÑÂä®‰ΩúË°®Á§∫ÊñπÊ≥ïÊèê‰æõ‰∫ÜÊåáÂØº„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÈíàÂØπ‰∏çÂêåÁöÑSO(3)Ë°®Á§∫ÊñπÊ≥ïÔºåËÆæËÆ°‰∫ÜÁõ∏Â∫îÁöÑÊäïÂΩ±ÂáΩÊï∞Ôºå‰ª•‰øùËØÅÁΩëÁªúËæìÂá∫ÁöÑÊúâÊïàÊÄßÔºõ2) ‰ΩøÁî®ÁÜµÊ≠£ÂàôÂåñÊù•ÈºìÂä±Êé¢Á¥¢ÔºåÂπ∂Á†îÁ©∂‰∫Ü‰∏çÂêåË°®Á§∫ÊñπÊ≥ï‰∏éÁÜµÊ≠£ÂàôÂåñ‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®Ôºõ3) Âú®Â§öÁßçÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°‰∏≠ËøõË°åÂÆûÈ™åÔºå‰ª•È™åËØÅÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂØπ‰∫éÂàáÂêëÈáèË°®Á§∫ÔºåÁΩëÁªúËæìÂá∫ÁöÑÊòØÂ±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏ãÁöÑËßíÈÄüÂ∫¶ÔºåÁÑ∂ÂêéÈÄöËøáÊåáÊï∞Êò†Â∞ÑÂ∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ÊóãËΩ¨Áü©Èòµ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®Â§öÁßçÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°‰∏≠Ôºå‰ΩøÁî®Â±ÄÈÉ®ÂùêÊ†áÁ≥ª‰∏ãÁöÑÂàáÂêëÈáèË°®Á§∫SO(3)Âä®‰ΩúÔºåËÉΩÂ§üÊòæËëóÊèêÈ´òÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂú®‰ΩøÁî®PPOÁÆóÊ≥ïÊó∂ÔºåÁõ∏ÊØî‰∫éÂÖ∂‰ªñË°®Á§∫ÊñπÊ≥ïÔºåÂàáÂêëÈáèË°®Á§∫ËÉΩÂ§üÊõ¥Âø´Âú∞Êî∂ÊïõÔºåÂπ∂Ëé∑ÂæóÊõ¥È´òÁöÑÂ•ñÂä±„ÄÇÊ≠§Â§ñÔºåÂàáÂêëÈáèË°®Á§∫Âú®Á®ÄÁñèÂ•ñÂä±‰ªªÂä°‰∏≠‰πüË°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÈ≤ÅÊ£íÊÄßÔºåËÉΩÂ§üÊúâÊïàÂú∞ÂºïÂØºÊé¢Á¥¢„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂπøÊ≥õÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÊéßÂà∂È¢ÜÂüüÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÁ≤æÁ°ÆÊéßÂà∂Êú∫Âô®‰∫∫ÂßøÊÄÅÁöÑ‰ªªÂä°‰∏≠Ôºå‰æãÂ¶ÇÊó†‰∫∫Êú∫ÊéßÂà∂„ÄÅÊú∫Ê¢∞ËáÇÊìç‰Ωú„ÄÅ‰ª•ÂèäÂÖ∂‰ªñÈúÄË¶Å‰∏é‰∏âÁª¥ÁéØÂ¢É‰∫§‰∫íÁöÑÊú∫Âô®‰∫∫Á≥ªÁªü„ÄÇÈÄâÊã©ÂêàÈÄÇÁöÑSO(3)Âä®‰ΩúË°®Á§∫ÂèØ‰ª•ÊèêÈ´òÊéßÂà∂Á≤æÂ∫¶„ÄÅÁ®≥ÂÆöÊÄßÂíåÂ≠¶‰π†ÊïàÁéáÔºå‰ªéËÄåÊèêÂçáÊú∫Âô®‰∫∫Á≥ªÁªüÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Many robotic control tasks require policies to act on orientations, yet the geometry of SO(3) makes this nontrivial. Because SO(3) admits no global, smooth, minimal parameterization, common representations such as Euler angles, quaternions, rotation matrices, and Lie algebra coordinates introduce distinct constraints and failure modes. While these trade-offs are well studied for supervised learning, their implications for actions in reinforcement learning remain unclear. We systematically evaluate SO(3) action representations across three standard continuous control algorithms, PPO, SAC, and TD3, under dense and sparse rewards. We compare how representations shape exploration, interact with entropy regularization, and affect training stability through empirical studies and analyze the implications of different projections for obtaining valid rotations from Euclidean network outputs. Across a suite of robotics benchmarks, we quantify the practical impact of these choices and distill simple, implementation-ready guidelines for selecting and using rotation actions. Our results highlight that representation-induced geometry strongly influences exploration and optimization and show that representing actions as tangent vectors in the local frame yields the most reliable results across algorithms.

