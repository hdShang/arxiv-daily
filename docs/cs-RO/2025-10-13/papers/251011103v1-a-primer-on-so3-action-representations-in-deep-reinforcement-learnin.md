---
layout: default
title: A Primer on SO(3) Action Representations in Deep Reinforcement Learning
---

# A Primer on SO(3) Action Representations in Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11103" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11103v1</a>
  <a href="https://arxiv.org/pdf/2510.11103.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11103v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11103v1', 'A Primer on SO(3) Action Representations in Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Martin Schuck, Sherif Samy, Angela P. Schoellig

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç ”ç©¶SO(3)ä½œç”¨è¡¨ç¤ºå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å½±å“ï¼Œæå‡ºåŸºäºå±€éƒ¨åæ ‡ç³»åˆ‡å‘é‡çš„åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `SO(3)è¡¨ç¤º` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `åŠ¨ä½œç©ºé—´` `è¿ç»­æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰SO(3)è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚æ¬§æ‹‰è§’ã€å››å…ƒæ•°ï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å­˜åœ¨çº¦æŸå’Œå¤±æ•ˆæ¨¡å¼ï¼Œå½±å“ç­–ç•¥å­¦ä¹ ã€‚
2. è®ºæ–‡æ ¸å¿ƒæ€æƒ³æ˜¯ç ”ç©¶ä¸åŒSO(3)åŠ¨ä½œè¡¨ç¤ºå¯¹å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å½±å“ï¼Œå¹¶æå‡ºæ›´æœ‰æ•ˆçš„è¡¨ç¤ºæ–¹æ³•ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å±€éƒ¨åæ ‡ç³»ä¸­å°†åŠ¨ä½œè¡¨ç¤ºä¸ºåˆ‡å‘é‡ï¼Œåœ¨å¤šç§æœºå™¨äººä»»åŠ¡å’Œç®—æ³•ä¸­è¡¨ç°æ›´ç¨³å®šå¯é ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šæœºå™¨äººæ§åˆ¶ä»»åŠ¡éœ€è¦ç­–ç•¥ä½œç”¨äºæ–¹å‘ï¼Œä½†SO(3)çš„å‡ ä½•ç‰¹æ€§ä½¿å…¶å˜å¾—ä¸ç®€å•ã€‚ç”±äºSO(3)ä¸å­˜åœ¨å…¨å±€ã€å¹³æ»‘ã€æœ€å°çš„å‚æ•°åŒ–è¡¨ç¤ºï¼Œå› æ­¤å¸¸è§çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå¦‚æ¬§æ‹‰è§’ã€å››å…ƒæ•°ã€æ—‹è½¬çŸ©é˜µå’Œæä»£æ•°åæ ‡ï¼Œéƒ½å¼•å…¥äº†ä¸åŒçš„çº¦æŸå’Œå¤±æ•ˆæ¨¡å¼ã€‚è™½ç„¶è¿™äº›æƒè¡¡åœ¨ç›‘ç£å­¦ä¹ ä¸­å¾—åˆ°äº†å……åˆ†ç ”ç©¶ï¼Œä½†å®ƒä»¬å¯¹å¼ºåŒ–å­¦ä¹ ä¸­åŠ¨ä½œçš„å½±å“ä»ä¸æ¸…æ¥šã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†SO(3)åŠ¨ä½œè¡¨ç¤ºåœ¨ä¸‰ç§æ ‡å‡†è¿ç»­æ§åˆ¶ç®—æ³•ï¼ˆPPOã€SACå’ŒTD3ï¼‰ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ç¨ å¯†å’Œç¨€ç–å¥–åŠ±ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†è¡¨ç¤ºå¦‚ä½•å¡‘é€ æ¢ç´¢ï¼Œå¦‚ä½•ä¸ç†µæ­£åˆ™åŒ–ç›¸äº’ä½œç”¨ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡å®è¯ç ”ç©¶å½±å“è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶åˆ†æäº†ä¸åŒæŠ•å½±å¯¹ä»æ¬§å‡ é‡Œå¾·ç½‘ç»œè¾“å‡ºä¸­è·å¾—æœ‰æ•ˆæ—‹è½¬çš„å½±å“ã€‚åœ¨ä¸€ç³»åˆ—æœºå™¨äººåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬é‡åŒ–äº†è¿™äº›é€‰æ‹©çš„å®é™…å½±å“ï¼Œå¹¶æç‚¼å‡ºç®€å•ã€å¯ç«‹å³å®ç°çš„æŒ‡å—ï¼Œç”¨äºé€‰æ‹©å’Œä½¿ç”¨æ—‹è½¬åŠ¨ä½œã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¡¨ç¤ºè¯±å¯¼çš„å‡ ä½•ç»“æ„å¼ºçƒˆå½±å“æ¢ç´¢å’Œä¼˜åŒ–ï¼Œå¹¶è¡¨æ˜åœ¨å±€éƒ¨åæ ‡ç³»ä¸­å°†åŠ¨ä½œè¡¨ç¤ºä¸ºåˆ‡å‘é‡å¯ä»¥äº§ç”Ÿæœ€å¯é çš„ç®—æ³•ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººå¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç”±äºSO(3)ç¾¤çš„ç‰¹æ®Šå‡ ä½•æ€§è´¨ï¼Œå¯¼è‡´ä½¿ç”¨ä¼ ç»Ÿæ—‹è½¬è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚æ¬§æ‹‰è§’ã€å››å…ƒæ•°ç­‰ï¼‰ä½œä¸ºåŠ¨ä½œç©ºé—´æ—¶ï¼Œè®­ç»ƒä¸ç¨³å®šã€æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºï¼Œè¿™äº›è¡¨ç¤ºæ–¹æ³•è¦ä¹ˆå­˜åœ¨å¥‡å¼‚æ€§ï¼Œè¦ä¹ˆå‚æ•°å†—ä½™ï¼Œè¦ä¹ˆæ— æ³•ä¿è¯æ—‹è½¬çŸ©é˜µçš„æœ‰æ•ˆæ€§ï¼Œä»è€Œå½±å“å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯ç³»ç»Ÿæ€§åœ°è¯„ä¼°ä¸åŒSO(3)åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è¡¨ç°ï¼Œå¹¶æ‰¾åˆ°ä¸€ç§æ›´é€‚åˆå¼ºåŒ–å­¦ä¹ çš„è¡¨ç¤ºæ–¹æ³•ã€‚ä½œè€…è®¤ä¸ºï¼Œåˆé€‚çš„è¡¨ç¤ºæ–¹æ³•åº”è¯¥èƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼æ¢ç´¢ï¼Œä¸ç†µæ­£åˆ™åŒ–å…¼å®¹ï¼Œå¹¶ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ã€‚é€šè¿‡å®éªŒå¯¹æ¯”ï¼Œä½œè€…å‘ç°å°†åŠ¨ä½œè¡¨ç¤ºä¸ºå±€éƒ¨åæ ‡ç³»ä¸‹çš„åˆ‡å‘é‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ»¡è¶³è¿™äº›è¦æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡é‡‡ç”¨çš„æ•´ä½“æ¡†æ¶æ˜¯ï¼šé¦–å…ˆï¼Œé€‰æ‹©ä¸‰ç§å¸¸ç”¨çš„è¿ç»­æ§åˆ¶å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆPPOã€SACã€TD3ï¼‰ä½œä¸ºåŸºå‡†ç®—æ³•ï¼›ç„¶åï¼Œé’ˆå¯¹æ¯ç§ç®—æ³•ï¼Œåˆ†åˆ«ä½¿ç”¨ä¸åŒçš„SO(3)åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼›æœ€åï¼Œé€šè¿‡å¯¹æ¯”å®éªŒç»“æœï¼Œåˆ†æä¸åŒè¡¨ç¤ºæ–¹æ³•å¯¹ç®—æ³•æ€§èƒ½çš„å½±å“ã€‚å®éªŒä¸­ä½¿ç”¨äº†å¤šç§æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä½œä¸ºbenchmarkï¼ŒåŒ…æ‹¬ç¨ å¯†å¥–åŠ±å’Œç¨€ç–å¥–åŠ±ä¸¤ç§æƒ…å†µã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºï¼Œé€šè¿‡å®éªŒè¯æ˜äº†åœ¨å±€éƒ¨åæ ‡ç³»ä¸‹ä½¿ç”¨åˆ‡å‘é‡è¡¨ç¤ºSO(3)åŠ¨ä½œçš„æœ‰æ•ˆæ€§ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•é¿å…äº†ä¼ ç»Ÿè¡¨ç¤ºæ–¹æ³•çš„å¥‡å¼‚æ€§å’Œå†—ä½™æ€§é—®é¢˜ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¼•å¯¼æ¢ç´¢ï¼Œå¹¶æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç³»ç»Ÿåœ°åˆ†æäº†ä¸åŒSO(3)è¡¨ç¤ºæ–¹æ³•ä¸å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä¸ºé€‰æ‹©åˆé€‚çš„åŠ¨ä½œè¡¨ç¤ºæ–¹æ³•æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) é’ˆå¯¹ä¸åŒçš„SO(3)è¡¨ç¤ºæ–¹æ³•ï¼Œè®¾è®¡äº†ç›¸åº”çš„æŠ•å½±å‡½æ•°ï¼Œä»¥ä¿è¯ç½‘ç»œè¾“å‡ºçš„æœ‰æ•ˆæ€§ï¼›2) ä½¿ç”¨ç†µæ­£åˆ™åŒ–æ¥é¼“åŠ±æ¢ç´¢ï¼Œå¹¶ç ”ç©¶äº†ä¸åŒè¡¨ç¤ºæ–¹æ³•ä¸ç†µæ­£åˆ™åŒ–ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼›3) åœ¨å¤šç§æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­è¿›è¡Œå®éªŒï¼Œä»¥éªŒè¯æ‰€æå‡ºæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºåˆ‡å‘é‡è¡¨ç¤ºï¼Œç½‘ç»œè¾“å‡ºçš„æ˜¯å±€éƒ¨åæ ‡ç³»ä¸‹çš„è§’é€Ÿåº¦ï¼Œç„¶åé€šè¿‡æŒ‡æ•°æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨å±€éƒ¨åæ ‡ç³»ä¸‹çš„åˆ‡å‘é‡è¡¨ç¤ºSO(3)åŠ¨ä½œï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨PPOç®—æ³•æ—¶ï¼Œç›¸æ¯”äºå…¶ä»–è¡¨ç¤ºæ–¹æ³•ï¼Œåˆ‡å‘é‡è¡¨ç¤ºèƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›ï¼Œå¹¶è·å¾—æ›´é«˜çš„å¥–åŠ±ã€‚æ­¤å¤–ï¼Œåˆ‡å‘é‡è¡¨ç¤ºåœ¨ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¼•å¯¼æ¢ç´¢ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ§åˆ¶é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®æ§åˆ¶æœºå™¨äººå§¿æ€çš„ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ— äººæœºæ§åˆ¶ã€æœºæ¢°è‡‚æ“ä½œã€ä»¥åŠå…¶ä»–éœ€è¦ä¸ä¸‰ç»´ç¯å¢ƒäº¤äº’çš„æœºå™¨äººç³»ç»Ÿã€‚é€‰æ‹©åˆé€‚çš„SO(3)åŠ¨ä½œè¡¨ç¤ºå¯ä»¥æé«˜æ§åˆ¶ç²¾åº¦ã€ç¨³å®šæ€§å’Œå­¦ä¹ æ•ˆç‡ï¼Œä»è€Œæå‡æœºå™¨äººç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Many robotic control tasks require policies to act on orientations, yet the geometry of SO(3) makes this nontrivial. Because SO(3) admits no global, smooth, minimal parameterization, common representations such as Euler angles, quaternions, rotation matrices, and Lie algebra coordinates introduce distinct constraints and failure modes. While these trade-offs are well studied for supervised learning, their implications for actions in reinforcement learning remain unclear. We systematically evaluate SO(3) action representations across three standard continuous control algorithms, PPO, SAC, and TD3, under dense and sparse rewards. We compare how representations shape exploration, interact with entropy regularization, and affect training stability through empirical studies and analyze the implications of different projections for obtaining valid rotations from Euclidean network outputs. Across a suite of robotics benchmarks, we quantify the practical impact of these choices and distill simple, implementation-ready guidelines for selecting and using rotation actions. Our results highlight that representation-induced geometry strongly influences exploration and optimization and show that representing actions as tangent vectors in the local frame yields the most reliable results across algorithms.

