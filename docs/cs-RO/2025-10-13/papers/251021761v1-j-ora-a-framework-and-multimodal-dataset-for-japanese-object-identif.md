---
layout: default
title: J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception
---

# J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21761" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21761v1</a>
  <a href="https://arxiv.org/pdf/2510.21761.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21761v1" onclick="toggleFavorite(this, '2510.21761v1', 'J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jesse Atuhurra, Hidetaka Kamigaito, Taro Watanabe, Koichiro Yoshino

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

**å¤‡æ³¨**: Accepted to IROS2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://jatuhurrra.github.io/J-ORA/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**J-ORAï¼šç”¨äºæœºå™¨äººæ„ŸçŸ¥çš„æ—¥è¯­ç‰©ä½“è¯†åˆ«ã€æŒ‡ä»£å’ŒåŠ¨ä½œé¢„æµ‹çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸æ¡†æ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ„ŸçŸ¥` `å¤šæ¨¡æ€æ•°æ®é›†` `ç‰©ä½“è¯†åˆ«` `æŒ‡ä»£æ¶ˆè§£` `åŠ¨ä½œé¢„æµ‹` `è§†è§‰è¯­è¨€æ¨¡å‹` `äººæœºäº¤äº’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ„ŸçŸ¥æ–¹æ³•ç¼ºä¹å¯¹ç‰©ä½“å±æ€§çš„ç»†ç²’åº¦ç†è§£ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚äººæœºäº¤äº’åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. J-ORAæ•°æ®é›†é€šè¿‡æä¾›è¯¦ç»†çš„ç‰©ä½“å±æ€§æ ‡æ³¨ï¼ŒåŒ…æ‹¬ç±»åˆ«ã€é¢œè‰²ã€å½¢çŠ¶ç­‰ï¼Œå¢å¼ºæ¨¡å‹å¯¹åœºæ™¯çš„ç†è§£ã€‚
3. å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨J-ORAæ•°æ®é›†è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“è¯†åˆ«å’ŒåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»J-ORAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨å¼¥åˆæœºå™¨äººæ„ŸçŸ¥é¢†åŸŸçš„å·®è·ã€‚J-ORAé€šè¿‡æä¾›æ—¥è¯­äººæœºå¯¹è¯åœºæ™¯ä¸­è¯¦ç»†çš„ç‰©ä½“å±æ€§æ ‡æ³¨ï¼Œæ”¯æŒç‰©ä½“è¯†åˆ«ã€æŒ‡ä»£æ¶ˆè§£å’Œä¸‹ä¸€æ­¥åŠ¨ä½œé¢„æµ‹è¿™ä¸‰ä¸ªå…³é”®çš„æ„ŸçŸ¥ä»»åŠ¡ã€‚è¯¥æ•°æ®é›†åˆ©ç”¨å…¨é¢çš„å±æ€§æ¨¡æ¿ï¼ˆä¾‹å¦‚ï¼Œç±»åˆ«ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ææ–™å’Œç©ºé—´å…³ç³»ï¼‰ã€‚é€šè¿‡å¯¹ä¸“æœ‰å’Œå¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä¸ä¸åŒ…å«ç‰©ä½“å±æ€§çš„æƒ…å†µç›¸æ¯”ï¼Œç»“åˆè¯¦ç»†çš„ç‰©ä½“å±æ€§å¯ä»¥æ˜¾è‘—æé«˜å¤šæ¨¡æ€æ„ŸçŸ¥æ€§èƒ½ã€‚å°½ç®¡æœ‰æ‰€æ”¹è¿›ï¼Œæˆ‘ä»¬å‘ç°ä¸“æœ‰VLMå’Œå¼€æºVLMä¹‹é—´ä»ç„¶å­˜åœ¨å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ç‰©ä½“å¯ä¾›æ€§çš„åˆ†æè¡¨æ˜ï¼Œä¸åŒçš„VLMåœ¨ç†è§£ç‰©ä½“åŠŸèƒ½å’Œä¸Šä¸‹æ–‡å…³ç³»æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ¨è¿›æœºå™¨äººæ„ŸçŸ¥æ—¶ï¼Œä¸°å¯Œä¸”ä¸Šä¸‹æ–‡æ•æ„Ÿçš„å±æ€§æ ‡æ³¨çš„é‡è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ„ŸçŸ¥ç³»ç»Ÿåœ¨ç†è§£äººç±»æŒ‡ä»¤å’Œæ‰§è¡Œç›¸åº”åŠ¨ä½œæ—¶ï¼Œé¢ä¸´ç€ç‰©ä½“è¯†åˆ«å’ŒæŒ‡ä»£æ¶ˆè§£çš„æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨äººæœºå¯¹è¯åœºæ™¯ä¸­ï¼Œæœºå™¨äººéœ€è¦å‡†ç¡®ç†è§£äººç±»å¯¹ç‰©ä½“çš„æè¿°ï¼ˆä¾‹å¦‚ï¼Œé¢œè‰²ã€å½¢çŠ¶ã€ä½ç½®ï¼‰ï¼Œå¹¶é¢„æµ‹ä¸‹ä¸€æ­¥åº”è¯¥æ‰§è¡Œçš„åŠ¨ä½œã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹ç‰©ä½“å±æ€§çš„ç»†ç²’åº¦å»ºæ¨¡ï¼Œå¯¼è‡´ç†è§£åå·®å’ŒåŠ¨ä½œé”™è¯¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šJ-ORAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«è¯¦ç»†ç‰©ä½“å±æ€§æ ‡æ³¨çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹å¯¹åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†ä¸ä»…åŒ…å«å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œè¿˜æä¾›äº†ä¸°å¯Œçš„ç‰©ä½“å±æ€§æ ‡æ³¨ï¼Œä¾‹å¦‚ç±»åˆ«ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ææ–™å’Œç©ºé—´å…³ç³»ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹å­¦ä¹ è¿™äº›å±æ€§ä¸ç‰©ä½“ä¹‹é—´çš„å…³è”ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨ç‰©ä½“è¯†åˆ«ã€æŒ‡ä»£æ¶ˆè§£å’ŒåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šJ-ORAæ¡†æ¶åŒ…å«æ•°æ®é›†æ„å»ºå’Œæ¨¡å‹è¯„ä¼°ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ã€‚æ•°æ®é›†æ„å»ºéƒ¨åˆ†æ¶‰åŠæ”¶é›†æ—¥è¯­äººæœºå¯¹è¯åœºæ™¯çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®ï¼Œå¹¶å¯¹å›¾åƒä¸­çš„ç‰©ä½“è¿›è¡Œè¯¦ç»†çš„å±æ€§æ ‡æ³¨ã€‚æ¨¡å‹è¯„ä¼°éƒ¨åˆ†åˆ™åˆ©ç”¨J-ORAæ•°æ®é›†å¯¹ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œè¯„ä¼°å…¶åœ¨ç‰©ä½“è¯†åˆ«ã€æŒ‡ä»£æ¶ˆè§£å’ŒåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ¡†æ¶æ”¯æŒä½¿ç”¨ä¸“æœ‰å’Œå¼€æºçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šJ-ORAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶æä¾›äº†è¯¦ç»†çš„ç‰©ä½“å±æ€§æ ‡æ³¨ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ç‰©ä½“å±æ€§ä¸ç‰©ä½“ä¹‹é—´çš„ç»†ç²’åº¦å…³è”ã€‚ä¸ä»¥å¾€çš„æ•°æ®é›†ç›¸æ¯”ï¼ŒJ-ORAä¸ä»…æä¾›äº†ç‰©ä½“ç±»åˆ«ä¿¡æ¯ï¼Œè¿˜æä¾›äº†é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ææ–™å’Œç©ºé—´å…³ç³»ç­‰å±æ€§ä¿¡æ¯ï¼Œä»è€Œæ›´å…¨é¢åœ°æè¿°äº†ç‰©ä½“ã€‚è¿™ç§ç»†ç²’åº¦çš„æ ‡æ³¨æ–¹å¼å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹å¯¹åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šJ-ORAæ•°æ®é›†çš„æ ‡æ³¨æ¨¡æ¿åŒ…å«ç±»åˆ«ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ææ–™å’Œç©ºé—´å…³ç³»ç­‰å±æ€§ã€‚ç©ºé—´å…³ç³»å±æ€§æè¿°äº†ç‰©ä½“ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ï¼Œä¾‹å¦‚â€œåœ¨...ä¹‹ä¸Šâ€ã€â€œåœ¨...æ—è¾¹â€ç­‰ã€‚åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹å¯¹ç‰©ä½“å±æ€§çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ¥å­¦ä¹ ç‰©ä½“å±æ€§ä¸å›¾åƒç‰¹å¾ä¹‹é—´çš„å…³è”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨J-ORAæ•°æ®é›†ä¸Šè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“è¯†åˆ«ã€æŒ‡ä»£æ¶ˆè§£å’ŒåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ä¸åŒ…å«ç‰©ä½“å±æ€§çš„æ¨¡å‹ç›¸æ¯”ï¼Œç»“åˆè¯¦ç»†ç‰©ä½“å±æ€§çš„æ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡å‡æœ‰æ˜æ˜¾æé«˜ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨J-ORAæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¼€æºæ¨¡å‹ï¼Œè¡¨æ˜ä¸“æœ‰æ¨¡å‹åœ¨ç†è§£å¤æ‚åœºæ™¯å’Œç»†ç²’åº¦å±æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

J-ORAæ•°æ®é›†å’Œæ¡†æ¶å¯åº”ç”¨äºå„ç§äººæœºäº¤äº’åœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººå’ŒåŒ»ç–—æœºå™¨äººã€‚é€šè¿‡æé«˜æœºå™¨äººå¯¹ç‰©ä½“å±æ€§çš„ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ä½¿å…¶æ›´å¥½åœ°ç†è§£äººç±»æŒ‡ä»¤ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚æ­¤å¤–ï¼ŒJ-ORAè¿˜å¯ä»¥ç”¨äºå¼€å‘æ›´æ™ºèƒ½çš„è™šæ‹ŸåŠ©æ‰‹å’Œæ™ºèƒ½å®¶å±…ç³»ç»Ÿï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We introduce J-ORA, a novel multimodal dataset that bridges the gap in robot perception by providing detailed object attribute annotations within Japanese human-robot dialogue scenarios. J-ORA is designed to support three critical perception tasks, object identification, reference resolution, and next-action prediction, by leveraging a comprehensive template of attributes (e.g., category, color, shape, size, material, and spatial relations). Extensive evaluations with both proprietary and open-source Vision Language Models (VLMs) reveal that incorporating detailed object attributes substantially improves multimodal perception performance compared to without object attributes. Despite the improvement, we find that there still exists a gap between proprietary and open-source VLMs. In addition, our analysis of object affordances demonstrates varying abilities in understanding object functionality and contextual relationships across different VLMs. These findings underscore the importance of rich, context-sensitive attribute annotations in advancing robot perception in dynamic environments. See project page at https://jatuhurrra.github.io/J-ORA/.

