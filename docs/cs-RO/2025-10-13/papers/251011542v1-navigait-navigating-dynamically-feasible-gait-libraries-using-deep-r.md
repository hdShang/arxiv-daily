---
layout: default
title: NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning
---

# NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.11542" target="_blank" class="toolbar-btn">arXiv: 2510.11542v1</a>
    <a href="https://arxiv.org/pdf/2510.11542.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11542v1" 
            onclick="toggleFavorite(this, '2510.11542v1', 'NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Neil C. Janwani, Varun Madabushi, Maegan Tucker

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-13

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**NaviGaitÔºöÂà©Áî®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÂØºËà™Âä®ÊÄÅÂèØË°åÊ≠•ÊÄÅÂ∫ìÔºåÂÆûÁé∞È≤ÅÊ£íÂèåË∂≥ËøêÂä®ÊéßÂà∂**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂõõÔºöÁîüÊàêÂºèÂä®‰Ωú (Generative Motion)**

**ÂÖ≥ÈîÆËØç**: `ÂèåË∂≥Êú∫Âô®‰∫∫` `Âº∫ÂåñÂ≠¶‰π†` `ËΩ®Ëøπ‰ºòÂåñ` `Ê≠•ÊÄÅÁîüÊàê` `ËøêÂä®ÊéßÂà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†Âú®ÂèåË∂≥Êú∫Âô®‰∫∫ËøêÂä®ÊéßÂà∂‰∏≠Èù¢‰∏¥Â•ñÂä±ÂáΩÊï∞ËÆæËÆ°Âõ∞ÈöæÔºåÈöæ‰ª•Áõ¥ËßÇË∞ÉÊï¥Êú∫Âô®‰∫∫Ë°å‰∏∫„ÄÇ
2. NaviGaitÁªìÂêàËΩ®Ëøπ‰ºòÂåñÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÂà©Áî®Á¶ªÁ∫øÊ≠•ÊÄÅÂ∫ìÁîüÊàêÂèÇËÄÉËøêÂä®ÔºåÂπ∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËøõË°åÊÆãÂ∑ÆÊ†°Ê≠£„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåNaviGaitÁõ∏ÊØî‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†ÂíåÊ®°‰ªøÂ≠¶‰π†ÔºåËÆ≠ÁªÉÈÄüÂ∫¶Êõ¥Âø´Ôºå‰∏îËøêÂä®ËΩ®ËøπÊõ¥Êé•ËøëÂéüÂßãÂèÇËÄÉËΩ®Ëøπ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†(RL)Â∑≤Êàê‰∏∫Â≠¶‰π†ÂèåË∂≥ËøêÂä®È≤ÅÊ£íÊéßÂà∂Á≠ñÁï•ÁöÑÂº∫Â§ßÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é‰∏çÁõ¥ËßÇÂíåÂ§çÊùÇÁöÑÂ•ñÂä±ËÆæËÆ°ÔºåÂæàÈöæË∞ÉÊï¥ÊâÄÈúÄÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÁ¶ªÁ∫øËΩ®Ëøπ‰ºòÂåñÊñπÊ≥ïÔºåÂ¶ÇÊ∑∑ÂêàÈõ∂Âä®ÂäõÂ≠¶Ôºå‰∏∫È´òÁª¥ËÖøÂºèÁ≥ªÁªüÊèê‰æõ‰∫ÜÊõ¥ÂèØË∞É„ÄÅÂèØËß£ÈáäÂíåÊï∞Â≠¶‰∏äÊõ¥ÂêàÁêÜÁöÑËøêÂä®ËÆ°Âàí„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏ÂØπÁé∞ÂÆû‰∏ñÁïåÁöÑÊâ∞Âä®ÔºàÂ¶ÇÂ§ñÈÉ®Êâ∞Âä®Ôºâ‰ªçÁÑ∂ÂæàËÑÜÂº±„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜNaviGaitÔºå‰∏Ä‰∏™ÂàÜÂ±ÇÊ°ÜÊû∂ÔºåÂÆÉÁªìÂêà‰∫ÜËΩ®Ëøπ‰ºòÂåñÁöÑÁªìÊûÑÂíåRLÁöÑÈÄÇÂ∫îÊÄßÔºå‰ª•ÂÆûÁé∞È≤ÅÊ£íÂíåÁõ¥ËßÇÁöÑËøêÂä®ÊéßÂà∂„ÄÇNaviGaitÂà©Áî®Á¶ªÁ∫ø‰ºòÂåñÁöÑÊ≠•ÊÄÅÂ∫ìÔºåÂπ∂Âú®ÂÆÉ‰ª¨‰πãÈó¥Âπ≥ÊªëÊèíÂÄºÔºå‰ª•‰∫ßÁîüËøûÁª≠ÁöÑÂèÇËÄÉËøêÂä®Ôºå‰ªéËÄåÂìçÂ∫îÈ´òÁ∫ßÂëΩ‰ª§„ÄÇËØ•Á≠ñÁï•Êèê‰æõÂÖ≥ËäÇÁ∫ßÂíåÈÄüÂ∫¶ÂëΩ‰ª§ÊÆãÂ∑ÆÊ†°Ê≠£Ôºå‰ª•Ë∞ÉËäÇÂíåÁ®≥ÂÆöÊ≠•ÊÄÅÂ∫ì‰∏≠ÁöÑÂèÇËÄÉËΩ®Ëøπ„ÄÇNaviGaitÁöÑ‰∏Ä‰∏™ÊòæËëó‰ºòÁÇπÊòØÔºåÂÆÉÈÄöËøáÁºñÁ†ÅÊù•Ëá™ËΩ®Ëøπ‰ºòÂåñÁöÑ‰∏∞ÂØåËøêÂä®ÂÖàÈ™åÔºåÊûÅÂ§ßÂú∞ÁÆÄÂåñ‰∫ÜÂ•ñÂä±ËÆæËÆ°ÔºåÂáèÂ∞ë‰∫ÜÂØπÁ≤æÁªÜË∞ÉÊï¥ÁöÑÂ°ëÈÄ†È°πÁöÑÈúÄÊ±ÇÔºåÂπ∂ÂÆûÁé∞‰∫ÜÊõ¥Á®≥ÂÆöÂíåÂèØËß£ÈáäÁöÑÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏é‰º†ÁªüÁöÑÂíåÂü∫‰∫éÊ®°‰ªøÁöÑRLÁõ∏ÊØîÔºåNaviGaitËÉΩÂ§üÂÆûÁé∞Êõ¥Âø´ÁöÑËÆ≠ÁªÉÔºåÂπ∂‰∫ßÁîüÊúÄÊé•ËøëÂéüÂßãÂèÇËÄÉÁöÑËøêÂä®„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÈÄöËøáÂ∞ÜÈ´òÁ∫ßËøêÂä®ÁîüÊàê‰∏é‰ΩéÁ∫ßÊ†°Ê≠£Ëß£ËÄ¶ÔºåNaviGait‰∏∫ÂÆûÁé∞Âä®ÊÄÅÂíåÈ≤ÅÊ£íÁöÑËøêÂä®Êèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥ÂÖ∑ÂèØÊâ©Â±ïÊÄßÂíåÈÄöÁî®ÊÄßÁöÑÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®ÂèåË∂≥Êú∫Âô®‰∫∫ËøêÂä®ÊéßÂà∂‰∏≠ÔºåÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°ÈùûÂ∏∏Â§çÊùÇ‰∏î‰∏çÁõ¥ËßÇÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑË∞ÉÂèÇÂ∑•‰ΩúÊâçËÉΩËé∑ÂæóÊúüÊúõÁöÑÊú∫Âô®‰∫∫Ë°å‰∏∫„ÄÇËÄåÁ¶ªÁ∫øËΩ®Ëøπ‰ºòÂåñÊñπÊ≥ïËôΩÁÑ∂ÂèØ‰ª•ÁîüÊàêÁ≤æÁ°ÆÁöÑËøêÂä®ËΩ®ËøπÔºå‰ΩÜÂØπÂ§ñÈÉ®Êâ∞Âä®ÁöÑÈ≤ÅÊ£íÊÄßËæÉÂ∑Æ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöNaviGaitÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜËøêÂä®ÊéßÂà∂ÈóÆÈ¢òÂàÜËß£‰∏∫‰∏§‰∏™Â±ÇÊ¨°ÔºöÈ´òÂ±ÇËøêÂä®ÁîüÊàêÂíå‰ΩéÂ±ÇËøêÂä®Ê†°Ê≠£„ÄÇÈ´òÂ±ÇÂà©Áî®Á¶ªÁ∫øËΩ®Ëøπ‰ºòÂåñÊñπÊ≥ïÁîüÊàê‰∏Ä‰∏™Ê≠•ÊÄÅÂ∫ìÔºåÂπ∂Ê†πÊçÆÈ´òÁ∫ßÊåá‰ª§Âú®Ê≠•ÊÄÅÂ∫ì‰∏≠ËøõË°åÊèíÂÄºÔºåÁîüÊàêÂèÇËÄÉËøêÂä®ËΩ®Ëøπ„ÄÇ‰ΩéÂ±ÇÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÂØπÂèÇËÄÉËΩ®ËøπËøõË°åÊÆãÂ∑ÆÊ†°Ê≠£Ôºå‰ª•ÊèêÈ´òÂØπÂ§ñÈÉ®Êâ∞Âä®ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöNaviGaitÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÊòØ‰∏Ä‰∏™ÂàÜÂ±ÇÊéßÂà∂ÁªìÊûÑ„ÄÇÈ¶ñÂÖàÔºåÁ¶ªÁ∫øÁîüÊàê‰∏Ä‰∏™Ê≠•ÊÄÅÂ∫ìÔºåÂÖ∂‰∏≠ÂåÖÂê´Â§öÁßç‰∏çÂêåÁöÑÊ≠•ÊÄÅÊ®°Âºè„ÄÇÁÑ∂ÂêéÔºåÊ†πÊçÆÈ´òÁ∫ßÊåá‰ª§Ôºà‰æãÂ¶ÇÔºåÊúüÊúõÁöÑÈÄüÂ∫¶ÂíåÊñπÂêëÔºâÔºåÂú®Ê≠•ÊÄÅÂ∫ì‰∏≠ÈÄâÊã©ÂêàÈÄÇÁöÑÊ≠•ÊÄÅÔºåÂπ∂ÈÄöËøáÊèíÂÄºÁîüÊàêËøûÁª≠ÁöÑÂèÇËÄÉËøêÂä®ËΩ®Ëøπ„ÄÇÊúÄÂêéÔºåÂà©Áî®‰∏Ä‰∏™Âº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÂØπÂèÇËÄÉËΩ®ËøπËøõË°åÊÆãÂ∑ÆÊ†°Ê≠£Ôºå‰ª•Â∫îÂØπÂ§ñÈÉ®Êâ∞Âä®ÂíåÊ®°ÂûãËØØÂ∑Æ„ÄÇËØ•Á≠ñÁï•ËæìÂá∫ÂÖ≥ËäÇÁ∫ßÂíåÈÄüÂ∫¶ÂëΩ‰ª§ÁöÑ‰øÆÊ≠£Èáè„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöNaviGaitÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜËΩ®Ëøπ‰ºòÂåñÁöÑÁªìÊûÑÊÄßÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑÈÄÇÂ∫îÊÄßÁõ∏ÁªìÂêà„ÄÇÈÄöËøáÂà©Áî®Á¶ªÁ∫øËΩ®Ëøπ‰ºòÂåñÊñπÊ≥ïÁîüÊàêÊ≠•ÊÄÅÂ∫ìÔºåNaviGaitÂèØ‰ª•ÊúâÊïàÂú∞Âà©Áî®ÂÖàÈ™åÁü•ËØÜÔºåÁÆÄÂåñ‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÁöÑÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°„ÄÇÂêåÊó∂ÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËøõË°åÊÆãÂ∑ÆÊ†°Ê≠£ÔºåNaviGaitÂèØ‰ª•ÊèêÈ´òÂØπÂ§ñÈÉ®Êâ∞Âä®ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöNaviGaitÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ê≠•ÊÄÅÂ∫ìÁöÑÁîüÊàêÊñπÊ≥ï„ÄÅÊèíÂÄºÁ≠ñÁï•ÂíåÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÁöÑËÆæËÆ°„ÄÇÊ≠•ÊÄÅÂ∫ìÂèØ‰ª•‰ΩøÁî®Ê∑∑ÂêàÈõ∂Âä®ÂäõÂ≠¶Á≠âËΩ®Ëøπ‰ºòÂåñÊñπÊ≥ïÁîüÊàê„ÄÇÊèíÂÄºÁ≠ñÁï•ÂèØ‰ª•‰ΩøÁî®Á∫øÊÄßÊèíÂÄºÊàñÊ†∑Êù°ÊèíÂÄºÁ≠âÊñπÊ≥ï„ÄÇÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÂèØ‰ª•‰ΩøÁî®Actor-CriticÁÆóÊ≥ïÔºå‰æãÂ¶ÇPPOÊàñSAC„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°‰∏ªË¶ÅÂÖ≥Ê≥®Ë∑üË∏™ÂèÇËÄÉËΩ®ËøπÁöÑÁ≤æÂ∫¶ÂíåÁ®≥ÂÆöÊÄßÔºåÈÅøÂÖçÂ§çÊùÇÁöÑÂ°ëÈÄ†È°π„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNaviGaitÁõ∏ÊØî‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÂíåÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÔºåËÆ≠ÁªÉÈÄüÂ∫¶Êõ¥Âø´ÔºåÂπ∂‰∏îËÉΩÂ§üÁîüÊàêÊõ¥Êé•ËøëÂéüÂßãÂèÇËÄÉËΩ®ËøπÁöÑËøêÂä®„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåNaviGaitÂú®ËÆ≠ÁªÉÊó∂Èó¥‰∏äÂáèÂ∞ë‰∫ÜÁ∫¶20%-30%ÔºåÂπ∂‰∏îÂú®Ë∑üË∏™ÂèÇËÄÉËΩ®ËøπÁöÑÁ≤æÂ∫¶‰∏äÊèêÈ´ò‰∫ÜÁ∫¶10%-15%„ÄÇÊ≠§Â§ñÔºåNaviGaitËøòË°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÊäóÊâ∞Âä®ËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

NaviGaitÂèØÂ∫îÁî®‰∫éÂêÑÁßçÂèåË∂≥Êú∫Âô®‰∫∫Ôºå‰æãÂ¶Ç‰∫∫ÂΩ¢Êú∫Âô®‰∫∫„ÄÅÂ§ñÈ™®È™ºÊú∫Âô®‰∫∫Á≠â„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËøêÂä®ËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂÆåÊàêÂêÑÁßç‰ªªÂä°Ôºå‰æãÂ¶ÇÊïëÊè¥„ÄÅÂ∑°ÈÄª„ÄÅÊê¨ËøêÁ≠â„ÄÇÊú™Êù•ÔºåNaviGaitÊúâÊúõÂ∫îÁî®‰∫éÊõ¥ÂπøÊ≥õÁöÑÊú∫Âô®‰∫∫È¢ÜÂüüÔºå‰æãÂ¶ÇÂõõË∂≥Êú∫Âô®‰∫∫„ÄÅÂ§öË∂≥Êú∫Âô®‰∫∫Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) has emerged as a powerful method to learn robust control policies for bipedal locomotion. Yet, it can be difficult to tune desired robot behaviors due to unintuitive and complex reward design. In comparison, offline trajectory optimization methods, like Hybrid Zero Dynamics, offer more tuneable, interpretable, and mathematically grounded motion plans for high-dimensional legged systems. However, these methods often remain brittle to real-world disturbances like external perturbations.
>   In this work, we present NaviGait, a hierarchical framework that combines the structure of trajectory optimization with the adaptability of RL for robust and intuitive locomotion control. NaviGait leverages a library of offline-optimized gaits and smoothly interpolates between them to produce continuous reference motions in response to high-level commands. The policy provides both joint-level and velocity command residual corrections to modulate and stabilize the reference trajectories in the gait library. One notable advantage of NaviGait is that it dramatically simplifies reward design by encoding rich motion priors from trajectory optimization, reducing the need for finely tuned shaping terms and enabling more stable and interpretable learning. Our experimental results demonstrate that NaviGait enables faster training compared to conventional and imitation-based RL, and produces motions that remain closest to the original reference. Overall, by decoupling high-level motion generation from low-level correction, NaviGait offers a more scalable and generalizable approach for achieving dynamic and robust locomotion.

