---
layout: default
title: "NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning"
---

# NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.11542" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.11542v1</a>
  <a href="https://arxiv.org/pdf/2510.11542.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.11542v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.11542v1', 'NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Neil C. Janwani, Varun Madabushi, Maegan Tucker

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-13

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NaviGaitï¼šåˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯¼èˆªåŠ¨æ€å¯è¡Œæ­¥æ€åº“ï¼Œå®ç°é²æ£’åŒè¶³è¿åŠ¨æ§åˆ¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `åŒè¶³æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `è½¨è¿¹ä¼˜åŒ–` `æ­¥æ€ç”Ÿæˆ` `è¿åŠ¨æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨åŒè¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶ä¸­é¢ä¸´å¥–åŠ±å‡½æ•°è®¾è®¡å›°éš¾ï¼Œéš¾ä»¥ç›´è§‚è°ƒæ•´æœºå™¨äººè¡Œä¸ºã€‚
2. NaviGaitç»“åˆè½¨è¿¹ä¼˜åŒ–å’Œå¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨ç¦»çº¿æ­¥æ€åº“ç”Ÿæˆå‚è€ƒè¿åŠ¨ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ®‹å·®æ ¡æ­£ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒNaviGaitç›¸æ¯”ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œä¸”è¿åŠ¨è½¨è¿¹æ›´æ¥è¿‘åŸå§‹å‚è€ƒè½¨è¿¹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)å·²æˆä¸ºå­¦ä¹ åŒè¶³è¿åŠ¨é²æ£’æ§åˆ¶ç­–ç•¥çš„å¼ºå¤§æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºä¸ç›´è§‚å’Œå¤æ‚çš„å¥–åŠ±è®¾è®¡ï¼Œå¾ˆéš¾è°ƒæ•´æ‰€éœ€çš„æœºå™¨äººè¡Œä¸ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¦»çº¿è½¨è¿¹ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚æ··åˆé›¶åŠ¨åŠ›å­¦ï¼Œä¸ºé«˜ç»´è…¿å¼ç³»ç»Ÿæä¾›äº†æ›´å¯è°ƒã€å¯è§£é‡Šå’Œæ•°å­¦ä¸Šæ›´åˆç†çš„è¿åŠ¨è®¡åˆ’ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å¯¹ç°å®ä¸–ç•Œçš„æ‰°åŠ¨ï¼ˆå¦‚å¤–éƒ¨æ‰°åŠ¨ï¼‰ä»ç„¶å¾ˆè„†å¼±ã€‚æœ¬æ–‡æå‡ºäº†NaviGaitï¼Œä¸€ä¸ªåˆ†å±‚æ¡†æ¶ï¼Œå®ƒç»“åˆäº†è½¨è¿¹ä¼˜åŒ–çš„ç»“æ„å’ŒRLçš„é€‚åº”æ€§ï¼Œä»¥å®ç°é²æ£’å’Œç›´è§‚çš„è¿åŠ¨æ§åˆ¶ã€‚NaviGaitåˆ©ç”¨ç¦»çº¿ä¼˜åŒ–çš„æ­¥æ€åº“ï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´å¹³æ»‘æ’å€¼ï¼Œä»¥äº§ç”Ÿè¿ç»­çš„å‚è€ƒè¿åŠ¨ï¼Œä»è€Œå“åº”é«˜çº§å‘½ä»¤ã€‚è¯¥ç­–ç•¥æä¾›å…³èŠ‚çº§å’Œé€Ÿåº¦å‘½ä»¤æ®‹å·®æ ¡æ­£ï¼Œä»¥è°ƒèŠ‚å’Œç¨³å®šæ­¥æ€åº“ä¸­çš„å‚è€ƒè½¨è¿¹ã€‚NaviGaitçš„ä¸€ä¸ªæ˜¾è‘—ä¼˜ç‚¹æ˜¯ï¼Œå®ƒé€šè¿‡ç¼–ç æ¥è‡ªè½¨è¿¹ä¼˜åŒ–çš„ä¸°å¯Œè¿åŠ¨å…ˆéªŒï¼Œæå¤§åœ°ç®€åŒ–äº†å¥–åŠ±è®¾è®¡ï¼Œå‡å°‘äº†å¯¹ç²¾ç»†è°ƒæ•´çš„å¡‘é€ é¡¹çš„éœ€æ±‚ï¼Œå¹¶å®ç°äº†æ›´ç¨³å®šå’Œå¯è§£é‡Šçš„å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å’ŒåŸºäºæ¨¡ä»¿çš„RLç›¸æ¯”ï¼ŒNaviGaitèƒ½å¤Ÿå®ç°æ›´å¿«çš„è®­ç»ƒï¼Œå¹¶äº§ç”Ÿæœ€æ¥è¿‘åŸå§‹å‚è€ƒçš„è¿åŠ¨ã€‚æ€»çš„æ¥è¯´ï¼Œé€šè¿‡å°†é«˜çº§è¿åŠ¨ç”Ÿæˆä¸ä½çº§æ ¡æ­£è§£è€¦ï¼ŒNaviGaitä¸ºå®ç°åŠ¨æ€å’Œé²æ£’çš„è¿åŠ¨æä¾›äº†ä¸€ç§æ›´å…·å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§çš„æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åŒè¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾è®¡éå¸¸å¤æ‚ä¸”ä¸ç›´è§‚ï¼Œéœ€è¦å¤§é‡çš„è°ƒå‚å·¥ä½œæ‰èƒ½è·å¾—æœŸæœ›çš„æœºå™¨äººè¡Œä¸ºã€‚è€Œç¦»çº¿è½¨è¿¹ä¼˜åŒ–æ–¹æ³•è™½ç„¶å¯ä»¥ç”Ÿæˆç²¾ç¡®çš„è¿åŠ¨è½¨è¿¹ï¼Œä½†å¯¹å¤–éƒ¨æ‰°åŠ¨çš„é²æ£’æ€§è¾ƒå·®ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNaviGaitçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¿åŠ¨æ§åˆ¶é—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼šé«˜å±‚è¿åŠ¨ç”Ÿæˆå’Œä½å±‚è¿åŠ¨æ ¡æ­£ã€‚é«˜å±‚åˆ©ç”¨ç¦»çº¿è½¨è¿¹ä¼˜åŒ–æ–¹æ³•ç”Ÿæˆä¸€ä¸ªæ­¥æ€åº“ï¼Œå¹¶æ ¹æ®é«˜çº§æŒ‡ä»¤åœ¨æ­¥æ€åº“ä¸­è¿›è¡Œæ’å€¼ï¼Œç”Ÿæˆå‚è€ƒè¿åŠ¨è½¨è¿¹ã€‚ä½å±‚åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¯¹å‚è€ƒè½¨è¿¹è¿›è¡Œæ®‹å·®æ ¡æ­£ï¼Œä»¥æé«˜å¯¹å¤–éƒ¨æ‰°åŠ¨çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNaviGaitçš„æ•´ä½“æ¡†æ¶æ˜¯ä¸€ä¸ªåˆ†å±‚æ§åˆ¶ç»“æ„ã€‚é¦–å…ˆï¼Œç¦»çº¿ç”Ÿæˆä¸€ä¸ªæ­¥æ€åº“ï¼Œå…¶ä¸­åŒ…å«å¤šç§ä¸åŒçš„æ­¥æ€æ¨¡å¼ã€‚ç„¶åï¼Œæ ¹æ®é«˜çº§æŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼ŒæœŸæœ›çš„é€Ÿåº¦å’Œæ–¹å‘ï¼‰ï¼Œåœ¨æ­¥æ€åº“ä¸­é€‰æ‹©åˆé€‚çš„æ­¥æ€ï¼Œå¹¶é€šè¿‡æ’å€¼ç”Ÿæˆè¿ç»­çš„å‚è€ƒè¿åŠ¨è½¨è¿¹ã€‚æœ€åï¼Œåˆ©ç”¨ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç­–ç•¥å¯¹å‚è€ƒè½¨è¿¹è¿›è¡Œæ®‹å·®æ ¡æ­£ï¼Œä»¥åº”å¯¹å¤–éƒ¨æ‰°åŠ¨å’Œæ¨¡å‹è¯¯å·®ã€‚è¯¥ç­–ç•¥è¾“å‡ºå…³èŠ‚çº§å’Œé€Ÿåº¦å‘½ä»¤çš„ä¿®æ­£é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šNaviGaitçš„å…³é”®åˆ›æ–°åœ¨äºå°†è½¨è¿¹ä¼˜åŒ–çš„ç»“æ„æ€§å’Œå¼ºåŒ–å­¦ä¹ çš„é€‚åº”æ€§ç›¸ç»“åˆã€‚é€šè¿‡åˆ©ç”¨ç¦»çº¿è½¨è¿¹ä¼˜åŒ–æ–¹æ³•ç”Ÿæˆæ­¥æ€åº“ï¼ŒNaviGaitå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œç®€åŒ–äº†å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°è®¾è®¡ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ®‹å·®æ ¡æ­£ï¼ŒNaviGaitå¯ä»¥æé«˜å¯¹å¤–éƒ¨æ‰°åŠ¨çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šNaviGaitçš„å…³é”®è®¾è®¡åŒ…æ‹¬æ­¥æ€åº“çš„ç”Ÿæˆæ–¹æ³•ã€æ’å€¼ç­–ç•¥å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„è®¾è®¡ã€‚æ­¥æ€åº“å¯ä»¥ä½¿ç”¨æ··åˆé›¶åŠ¨åŠ›å­¦ç­‰è½¨è¿¹ä¼˜åŒ–æ–¹æ³•ç”Ÿæˆã€‚æ’å€¼ç­–ç•¥å¯ä»¥ä½¿ç”¨çº¿æ€§æ’å€¼æˆ–æ ·æ¡æ’å€¼ç­‰æ–¹æ³•ã€‚å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¯ä»¥ä½¿ç”¨Actor-Criticç®—æ³•ï¼Œä¾‹å¦‚PPOæˆ–SACã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡ä¸»è¦å…³æ³¨è·Ÿè¸ªå‚è€ƒè½¨è¿¹çš„ç²¾åº¦å’Œç¨³å®šæ€§ï¼Œé¿å…å¤æ‚çš„å¡‘é€ é¡¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒNaviGaitç›¸æ¯”ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆæ›´æ¥è¿‘åŸå§‹å‚è€ƒè½¨è¿¹çš„è¿åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼ŒNaviGaitåœ¨è®­ç»ƒæ—¶é—´ä¸Šå‡å°‘äº†çº¦20%-30%ï¼Œå¹¶ä¸”åœ¨è·Ÿè¸ªå‚è€ƒè½¨è¿¹çš„ç²¾åº¦ä¸Šæé«˜äº†çº¦10%-15%ã€‚æ­¤å¤–ï¼ŒNaviGaitè¿˜è¡¨ç°å‡ºæ›´å¼ºçš„æŠ—æ‰°åŠ¨èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NaviGaitå¯åº”ç”¨äºå„ç§åŒè¶³æœºå™¨äººï¼Œä¾‹å¦‚äººå½¢æœºå™¨äººã€å¤–éª¨éª¼æœºå™¨äººç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¿åŠ¨èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚æ•‘æ´ã€å·¡é€»ã€æ¬è¿ç­‰ã€‚æœªæ¥ï¼ŒNaviGaitæœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚å››è¶³æœºå™¨äººã€å¤šè¶³æœºå™¨äººç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has emerged as a powerful method to learn robust control policies for bipedal locomotion. Yet, it can be difficult to tune desired robot behaviors due to unintuitive and complex reward design. In comparison, offline trajectory optimization methods, like Hybrid Zero Dynamics, offer more tuneable, interpretable, and mathematically grounded motion plans for high-dimensional legged systems. However, these methods often remain brittle to real-world disturbances like external perturbations.
>   In this work, we present NaviGait, a hierarchical framework that combines the structure of trajectory optimization with the adaptability of RL for robust and intuitive locomotion control. NaviGait leverages a library of offline-optimized gaits and smoothly interpolates between them to produce continuous reference motions in response to high-level commands. The policy provides both joint-level and velocity command residual corrections to modulate and stabilize the reference trajectories in the gait library. One notable advantage of NaviGait is that it dramatically simplifies reward design by encoding rich motion priors from trajectory optimization, reducing the need for finely tuned shaping terms and enabling more stable and interpretable learning. Our experimental results demonstrate that NaviGait enables faster training compared to conventional and imitation-based RL, and produces motions that remain closest to the original reference. Overall, by decoupling high-level motion generation from low-level correction, NaviGait offers a more scalable and generalizable approach for achieving dynamic and robust locomotion.

