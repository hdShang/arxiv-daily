---
layout: default
title: Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving
---

# Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.07155" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.07155v1</a>
  <a href="https://arxiv.org/pdf/2511.07155.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.07155v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.07155v1', 'Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Thomas Steinecker, Alexander Bienemann, Denis Trescher, Thorsten Luettel, Mirko Maehlisch

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŠ¨åŠ›å­¦è§£è€¦çš„è½¨è¿¹å¯¹é½æ–¹æ³•ï¼Œå®ç°è‡ªåŠ¨é©¾é©¶RL Sim-to-Realé›¶æ ·æœ¬è¿ç§»**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶` `Sim-to-Real` `è½¨è¿¹å¯¹é½` `è¿åŠ¨è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çœŸå®è½¦è¾†åŠ¨åŠ›å­¦å»ºæ¨¡å›°éš¾ï¼Œå¯¼è‡´ä»¿çœŸè®­ç»ƒçš„RLæ™ºèƒ½ä½“éš¾ä»¥ç›´æ¥è¿ç§»åˆ°çœŸå®ç¯å¢ƒã€‚
2. é€šè¿‡ç©ºé—´å’Œæ—¶é—´ä¸Šçš„è½¨è¿¹å¯¹é½ï¼Œå°†è¿åŠ¨è§„åˆ’ä¸è½¦è¾†æ§åˆ¶è§£è€¦ï¼Œå®ç°Sim-to-Realè¿ç§»ã€‚
3. åœ¨çœŸå®è½¦è¾†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå®ç°äº†RLè¿åŠ¨è§„åˆ’çš„é›¶æ ·æœ¬è¿ç§»ï¼Œæœ‰æ•ˆè§£è€¦äº†é«˜ä½å±‚æ§åˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)åœ¨æœºå™¨äººé¢†åŸŸå±•ç°äº†æ½œåŠ›ï¼Œä½†ç”±äºè½¦è¾†åŠ¨åŠ›å­¦çš„å¤æ‚æ€§å’Œä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®å¼‚ï¼Œåœ¨çœŸå®è½¦è¾†ä¸Šéƒ¨ç½²RLä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è½®èƒç‰¹æ€§ã€è·¯é¢çŠ¶å†µã€ç©ºæ°”åŠ¨åŠ›æ‰°åŠ¨å’Œè½¦è¾†è´Ÿè½½ç­‰å› ç´ ä½¿å¾—å‡†ç¡®å»ºæ¨¡çœŸå®ä¸–ç•ŒåŠ¨åŠ›å­¦å˜å¾—ä¸å¯è¡Œï¼Œè¿™é˜»ç¢äº†åœ¨ä»¿çœŸä¸­è®­ç»ƒçš„RLæ™ºèƒ½ä½“çš„ç›´æ¥è¿ç§»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡è™šæ‹Ÿè½¦è¾†å’ŒçœŸå®ç³»ç»Ÿä¹‹é—´çš„ç©ºé—´å’Œæ—¶é—´å¯¹é½ç­–ç•¥ï¼Œå°†è¿åŠ¨è§„åˆ’ä¸è½¦è¾†æ§åˆ¶è§£è€¦ã€‚é¦–å…ˆï¼Œåœ¨ä»¿çœŸä¸­ä½¿ç”¨è¿åŠ¨å­¦è‡ªè¡Œè½¦æ¨¡å‹è®­ç»ƒRLæ™ºèƒ½ä½“ä»¥è¾“å‡ºè¿ç»­æ§åˆ¶åŠ¨ä½œã€‚ç„¶åï¼Œå°†å…¶è¡Œä¸ºæç‚¼æˆè½¨è¿¹é¢„æµ‹æ™ºèƒ½ä½“ï¼Œç”Ÿæˆæœ‰é™èŒƒå›´çš„è‡ªè½¦è½¨è¿¹ï¼Œä»è€Œå®ç°è™šæ‹Ÿè½¦è¾†å’ŒçœŸå®è½¦è¾†ä¹‹é—´çš„åŒæ­¥ã€‚åœ¨éƒ¨ç½²æ—¶ï¼ŒStanleyæ§åˆ¶å™¨æ§åˆ¶æ¨ªå‘åŠ¨åŠ›å­¦ï¼Œè€Œçºµå‘å¯¹é½é€šè¿‡è‡ªé€‚åº”æ›´æ–°æœºåˆ¶æ¥ç»´æŒï¼Œä»¥è¡¥å¿è™šæ‹Ÿè½¨è¿¹å’ŒçœŸå®è½¨è¿¹ä¹‹é—´çš„åå·®ã€‚æˆ‘ä»¬åœ¨çœŸå®è½¦è¾†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå¹¶è¯æ˜æ‰€æå‡ºçš„å¯¹é½ç­–ç•¥èƒ½å¤Ÿå®ç°åŸºäºRLçš„è¿åŠ¨è§„åˆ’ä»ä»¿çœŸåˆ°ç°å®çš„é²æ£’é›¶æ ·æœ¬è¿ç§»ï¼ŒæˆåŠŸåœ°å°†é«˜å±‚è½¨è¿¹ç”Ÿæˆä¸ä½å±‚è½¦è¾†æ§åˆ¶è§£è€¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­ï¼Œç”±äºä»¿çœŸç¯å¢ƒä¸çœŸå®ç¯å¢ƒå­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´åœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒçš„RLæ™ºèƒ½ä½“æ— æ³•ç›´æ¥è¿ç§»åˆ°çœŸå®è½¦è¾†ä¸Šçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®å»ºæ¨¡çœŸå®è½¦è¾†çš„å¤æ‚åŠ¨åŠ›å­¦ç‰¹æ€§ï¼Œä¾‹å¦‚è½®èƒç‰¹æ€§ã€è·¯é¢çŠ¶å†µç­‰ï¼Œè¿™ä½¿å¾—Sim-to-Realçš„è¿ç§»å˜å¾—å›°éš¾ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†è¿åŠ¨è§„åˆ’ä¸è½¦è¾†æ§åˆ¶è§£è€¦ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆåœ¨ä»¿çœŸç¯å¢ƒä¸­ä½¿ç”¨RLè®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“è¾“å‡ºè¿ç»­çš„æ§åˆ¶åŠ¨ä½œã€‚ç„¶åï¼Œå°†è¯¥æ™ºèƒ½ä½“çš„è¡Œä¸ºæç‚¼æˆä¸€ä¸ªè½¨è¿¹é¢„æµ‹å™¨ï¼Œè¯¥é¢„æµ‹å™¨ç”Ÿæˆæœ‰é™èŒƒå›´å†…çš„è½¦è¾†è½¨è¿¹ã€‚é€šè¿‡å¯¹é½è™šæ‹Ÿè½¦è¾†å’ŒçœŸå®è½¦è¾†çš„è½¨è¿¹ï¼Œå®ç°Sim-to-Realçš„è¿ç§»ã€‚è¿™ç§è§£è€¦çš„æ–¹å¼é™ä½äº†å¯¹ç²¾ç¡®åŠ¨åŠ›å­¦æ¨¡å‹çš„ä¾èµ–ï¼Œä»è€Œæé«˜äº†è¿ç§»çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) åœ¨ä»¿çœŸç¯å¢ƒä¸­ä½¿ç”¨RLè®­ç»ƒè½¨è¿¹ç”Ÿæˆå™¨ï¼›2) å°†è®­ç»ƒå¥½çš„è½¨è¿¹ç”Ÿæˆå™¨éƒ¨ç½²åˆ°çœŸå®è½¦è¾†ä¸Šï¼›3) ä½¿ç”¨Stanleyæ§åˆ¶å™¨æ§åˆ¶è½¦è¾†çš„æ¨ªå‘è¿åŠ¨ï¼›4) ä½¿ç”¨è‡ªé€‚åº”æ›´æ–°æœºåˆ¶æ¥å¯¹é½è™šæ‹Ÿè½¨è¿¹å’ŒçœŸå®è½¨è¿¹ï¼Œè¡¥å¿ä¸¤è€…ä¹‹é—´çš„åå·®ã€‚è¯¥æ¡†æ¶å°†é«˜å±‚è½¨è¿¹ç”Ÿæˆä¸ä½å±‚è½¦è¾†æ§åˆ¶åˆ†ç¦»ï¼Œç®€åŒ–äº†æ§åˆ¶ç­–ç•¥çš„å¤æ‚æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†åŠ¨åŠ›å­¦è§£è€¦çš„è½¨è¿¹å¯¹é½æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥å°†RLæ™ºèƒ½ä½“éƒ¨ç½²åˆ°çœŸå®è½¦è¾†ä¸Šçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡è½¨è¿¹å¯¹é½çš„æ–¹å¼ï¼Œå°†è¿åŠ¨è§„åˆ’ä¸è½¦è¾†æ§åˆ¶è§£è€¦ï¼Œé™ä½äº†å¯¹ç²¾ç¡®åŠ¨åŠ›å­¦æ¨¡å‹çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è‡ªé€‚åº”æ›´æ–°æœºåˆ¶æ¥è¡¥å¿è™šæ‹Ÿè½¨è¿¹å’ŒçœŸå®è½¨è¿¹ä¹‹é—´çš„åå·®ï¼Œè¿›ä¸€æ­¥æé«˜äº†è¿ç§»çš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä»¿çœŸç¯å¢ƒä¸­ï¼Œä½¿ç”¨è¿åŠ¨å­¦è‡ªè¡Œè½¦æ¨¡å‹æ¥ç®€åŒ–è½¦è¾†åŠ¨åŠ›å­¦ã€‚RLæ™ºèƒ½ä½“ä½¿ç”¨è¿ç»­æ§åˆ¶åŠ¨ä½œä½œä¸ºè¾“å‡ºã€‚è½¨è¿¹é¢„æµ‹å™¨ç”Ÿæˆæœ‰é™èŒƒå›´çš„è‡ªè½¦è½¨è¿¹ã€‚åœ¨çœŸå®è½¦è¾†ä¸Šï¼Œä½¿ç”¨Stanleyæ§åˆ¶å™¨æ§åˆ¶æ¨ªå‘è¿åŠ¨ï¼Œè‡ªé€‚åº”æ›´æ–°æœºåˆ¶æ ¹æ®è™šæ‹Ÿè½¨è¿¹å’ŒçœŸå®è½¨è¿¹ä¹‹é—´çš„åå·®æ¥è°ƒæ•´æ§åˆ¶å‚æ•°ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç­‰ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡åœ¨çœŸå®è½¦è¾†ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå®ç°äº†åŸºäºRLçš„è¿åŠ¨è§„åˆ’ä»ä»¿çœŸåˆ°ç°å®çš„é›¶æ ·æœ¬è¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£è€¦é«˜å±‚è½¨è¿¹ç”Ÿæˆä¸ä½å±‚è½¦è¾†æ§åˆ¶ï¼Œå¹¶èƒ½å¤Ÿé²æ£’åœ°åº”å¯¹çœŸå®ç¯å¢ƒä¸­çš„å„ç§ä¸ç¡®å®šæ€§ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œæå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜ï¼Œå±äºæœªçŸ¥ä¿¡æ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è¿åŠ¨è§„åˆ’å’Œæ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿéƒ¨ç½²å’Œé€‚åº”æ–°ç¯å¢ƒçš„åœºæ™¯ä¸­ã€‚é€šè¿‡Sim-to-Realè¿ç§»ï¼Œå¯ä»¥é™ä½å¼€å‘æˆæœ¬å’Œé£é™©ï¼ŒåŠ é€Ÿè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„è½åœ°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æœºå™¨äººé¢†åŸŸï¼Œä¾‹å¦‚æ— äººæœºå’Œç§»åŠ¨æœºå™¨äººã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.

