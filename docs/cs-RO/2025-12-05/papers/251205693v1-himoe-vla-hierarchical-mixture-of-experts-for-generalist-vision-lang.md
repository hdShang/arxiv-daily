---
layout: default
title: HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies
---

# HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.05693" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.05693v1</a>
  <a href="https://arxiv.org/pdf/2512.05693.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.05693v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.05693v1', 'HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiying Du, Bei Liu, Yaobo Liang, Yichao Shen, Haidong Cao, Xiangyu Zheng, Zhiyuan Feng, Zuxuan Wu, Jiaolong Yang, Yu-Gang Jiang

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-05

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/ZhiyingDu/HiMoE-VLA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHiMoE-VLAï¼Œè§£å†³å…·èº«æ™ºèƒ½ä¸­å¼‚æ„æœºå™¨äººæ•°æ®æ³›åŒ–éš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `æœºå™¨äººæ§åˆ¶` `å¼‚æ„æ•°æ®` `åˆ†å±‚æ··åˆä¸“å®¶` `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `æ³›åŒ–èƒ½åŠ›` `æœºå™¨äººå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·èº«æ™ºèƒ½æ¨¡å‹éš¾ä»¥æœ‰æ•ˆæ•´åˆå¼‚æ„æœºå™¨äººæ•°æ®ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å—é™ï¼Œåœ¨æ–°ç¯å¢ƒä¸­æ€§èƒ½ä¸‹é™ã€‚
2. HiMoE-VLA é‡‡ç”¨åˆ†å±‚æ··åˆä¸“å®¶æ¶æ„ï¼Œè‡ªé€‚åº”å¤„ç†å¼‚æ„æ€§ï¼Œé€æ­¥æŠ½è±¡ä¸ºå…±äº«çŸ¥è¯†è¡¨ç¤ºã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHiMoE-VLA åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå¹³å°ä¸Šå‡ä¼˜äºç°æœ‰ VLA åŸºçº¿ï¼Œæå‡äº†å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«æ™ºèƒ½çš„åŸºçŸ³æ¨¡å‹å‘å±•ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æœºå™¨äººæ¼”ç¤ºæ•°æ®ã€‚ç›®å‰çš„æ–¹æ³•å°è¯•é€šè¿‡è®­ç»ƒå¼‚æ„æœºå™¨äººæ•°æ®é›†æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œä¸è§†è§‰æˆ–è¯­è¨€æ•°æ®ä¸åŒï¼Œæœºå™¨äººæ¼”ç¤ºåœ¨å®ä½“å’ŒåŠ¨ä½œç©ºé—´ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„å¼‚æ„æ€§ï¼Œä»¥åŠä¼ æ„Ÿå™¨é…ç½®å’ŒåŠ¨ä½œæ§åˆ¶é¢‘ç‡ç­‰å…¶ä»–æ˜¾è‘—å·®å¼‚ã€‚ç”±äºç¼ºä¹å¤„ç†è¿™ç§å¼‚æ„æ€§çš„æ˜¾å¼è®¾è®¡ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ•´åˆå„ç§å› ç´ ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨è½¬ç§»åˆ°æ–°ç¯å¢ƒæ—¶å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æ¶HiMoE-VLAï¼Œä¸“é—¨ç”¨äºæœ‰æ•ˆå¤„ç†å…·æœ‰å¼‚æ„æ€§çš„å¤šæ ·åŒ–æœºå™¨äººæ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºåŠ¨ä½œæ¨¡å—å¼•å…¥äº†ä¸€ç§åˆ†å±‚æ··åˆä¸“å®¶ï¼ˆHiMoEï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„è‡ªé€‚åº”åœ°å¤„ç†è·¨å±‚çš„å¤šä¸ªå¼‚æ„æ€§æ¥æºï¼Œå¹¶é€æ­¥å°†å…¶æŠ½è±¡ä¸ºå…±äº«çš„çŸ¥è¯†è¡¨ç¤ºã€‚é€šè¿‡åœ¨æ¨¡æ‹ŸåŸºå‡†å’ŒçœŸå®æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒï¼ŒHiMoE-VLA è¯æ˜äº†ç›¸å¯¹äºç°æœ‰ VLA åŸºçº¿çš„æŒç»­æ€§èƒ½æå‡ï¼Œåœ¨å„ç§æœºå™¨äººå’ŒåŠ¨ä½œç©ºé—´ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½é¢†åŸŸä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¼‚æ„æœºå™¨äººæ•°æ®æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨é¢å¯¹ä¸åŒæœºå™¨äººå®ä½“ã€åŠ¨ä½œç©ºé—´ã€ä¼ æ„Ÿå™¨é…ç½®å’Œæ§åˆ¶é¢‘ç‡ç­‰å·®å¼‚æ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆæ•´åˆè¿™äº›å¼‚æ„ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸­çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨åˆ†å±‚æ··åˆä¸“å®¶ï¼ˆHierarchical Mixture-of-Experts, HiMoEï¼‰æ¶æ„æ¥å¤„ç†æœºå™¨äººæ•°æ®çš„å¼‚æ„æ€§ã€‚é€šè¿‡åœ¨åŠ¨ä½œæ¨¡å—ä¸­å¼•å…¥ HiMoEï¼Œæ¨¡å‹å¯ä»¥è‡ªé€‚åº”åœ°å­¦ä¹ ä¸åŒæ•°æ®æºçš„ç‰¹å¾ï¼Œå¹¶é€æ­¥å°†è¿™äº›å¼‚æ„ä¿¡æ¯æŠ½è±¡æˆå…±äº«çš„çŸ¥è¯†è¡¨ç¤ºã€‚è¿™ç§åˆ†å±‚ç»“æ„å…è®¸æ¨¡å‹åœ¨ä¸åŒå±‚çº§ä¸Šå¤„ç†ä¸åŒç±»å‹çš„å¼‚æ„æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHiMoE-VLA æ¡†æ¶ä¸»è¦åŒ…å«è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä¸‰ä¸ªæ¨¡å—ã€‚è§†è§‰æ¨¡å—è´Ÿè´£å¤„ç†è¾“å…¥çš„å›¾åƒä¿¡æ¯ï¼Œè¯­è¨€æ¨¡å—å¤„ç†æ–‡æœ¬æŒ‡ä»¤ï¼Œè€ŒåŠ¨ä½œæ¨¡å—åˆ™è´Ÿè´£ç”Ÿæˆæœºå™¨äººçš„æ§åˆ¶æŒ‡ä»¤ã€‚å…³é”®åœ¨äºåŠ¨ä½œæ¨¡å—é‡‡ç”¨äº† HiMoE æ¶æ„ï¼Œè¯¥æ¶æ„åŒ…å«å¤šä¸ªä¸“å®¶ç½‘ç»œå’Œä¸€ä¸ªé—¨æ§ç½‘ç»œã€‚é—¨æ§ç½‘ç»œæ ¹æ®è¾“å…¥é€‰æ‹©åˆé€‚çš„ä¸“å®¶ç½‘ç»œæ¥å¤„ç†æ•°æ®ï¼Œä»è€Œå®ç°å¯¹å¼‚æ„æ•°æ®çš„è‡ªé€‚åº”å¤„ç†ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡æœ€å…³é”®çš„åˆ›æ–°ç‚¹åœ¨äºå°† HiMoE æ¶æ„å¼•å…¥åˆ° VLA æ¨¡å‹çš„åŠ¨ä½œæ¨¡å—ä¸­ï¼Œä»è€Œæœ‰æ•ˆåœ°è§£å†³äº†å¼‚æ„æœºå™¨äººæ•°æ®çš„æ³›åŒ–é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ VLA æ¨¡å‹ç›¸æ¯”ï¼ŒHiMoE-VLA èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ä¸åŒæœºå™¨äººå’ŒåŠ¨ä½œç©ºé—´ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚è¿™ç§åˆ†å±‚ç»“æ„å’Œè‡ªé€‚åº”é€‰æ‹©æœºåˆ¶æ˜¯ç°æœ‰æ–¹æ³•æ‰€ä¸å…·å¤‡çš„ã€‚

**å…³é”®è®¾è®¡**ï¼šHiMoE æ¶æ„çš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) åˆ†å±‚ç»“æ„ï¼šå…è®¸æ¨¡å‹åœ¨ä¸åŒå±‚çº§ä¸Šå¤„ç†ä¸åŒç±»å‹çš„å¼‚æ„æ€§ï¼›2) ä¸“å®¶ç½‘ç»œï¼šæ¯ä¸ªä¸“å®¶ç½‘ç»œè´Ÿè´£å¤„ç†ç‰¹å®šç±»å‹çš„æ•°æ®æˆ–ä»»åŠ¡ï¼›3) é—¨æ§ç½‘ç»œï¼šæ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©åˆé€‚çš„ä¸“å®¶ç½‘ç»œã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼Œä¾‹å¦‚ï¼Œé—¨æ§ç½‘ç»œçš„è¾“å‡ºé€šå¸¸ä½¿ç”¨ softmax å‡½æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼ŒæŸå¤±å‡½æ•°åˆ™é‡‡ç”¨äº¤å‰ç†µæŸå¤±æˆ–å…¶å˜ä½“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

HiMoE-VLA åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiMoE-VLA åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰çš„ VLA åŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æå‡å¹…åº¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„é‡åŒ–åˆ†æï¼Œä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼ŒHiMoE-VLA çš„æ€§èƒ½æå‡äº† 10% ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨åŒ–å’Œå…·èº«æ™ºèƒ½ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºå¼€å‘èƒ½å¤Ÿé€‚åº”ä¸åŒæœºå™¨äººå¹³å°å’Œç¯å¢ƒçš„é€šç”¨æ§åˆ¶ç­–ç•¥ï¼Œä»è€Œé™ä½æœºå™¨äººéƒ¨ç½²å’Œç»´æŠ¤çš„æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸï¼Œæé«˜ç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³å’Œé€‚åº”èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„æ™®åŠå’Œåº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.

