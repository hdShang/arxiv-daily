---
layout: default
title: Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning
---

# Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.20706" target="_blank" class="toolbar-btn">arXiv: 2510.20706v2</a>
    <a href="https://arxiv.org/pdf/2510.20706.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20706v2" 
            onclick="toggleFavorite(this, '2510.20706v2', 'Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Prakrut Kotecha, Ganga Nair B, Shishir Kolathaya

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-23 (Êõ¥Êñ∞: 2025-10-24)

**Â§áÊ≥®**: 7 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÁªìÂêàMPC‰∏éÂº∫ÂåñÂ≠¶‰π†ÔºåÂÆûÁé∞ÂõõË∂≥Êú∫Âô®‰∫∫ÂÆûÊó∂Ê≠•ÊÄÅËá™ÈÄÇÂ∫î**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÂõõË∂≥Êú∫Âô®‰∫∫` `Ê≠•ÊÄÅËá™ÈÄÇÂ∫î` `Ê®°ÂûãÈ¢ÑÊµãÊéßÂà∂` `Âº∫ÂåñÂ≠¶‰π†` `Dreamer` `ËøêÂä®ËßÑÂàí` `ËÉΩÈáèÊïàÁéá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. ‰º†ÁªüÂº∫ÂåñÂ≠¶‰π†ËôΩËÉΩÂÆûÁé∞ÂõõË∂≥Êú∫Âô®‰∫∫Ëá™ÈÄÇÂ∫îËøêÂä®Ôºå‰ΩÜÁ≠ñÁï•Â∏∏Êî∂Êïõ‰∫éÂçï‰∏ÄÊ≠•ÊÄÅÔºåÂØºËá¥ÊÄßËÉΩÊ¨†‰Ω≥„ÄÇ
2. Êú¨ÊñáÁªìÂêàÊ®°ÂûãÈ¢ÑÊµãÊéßÂà∂ÔºàMPCÔºâÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÂà©Áî®MPPIÁÆóÊ≥ïÂíåDreamerÊ®°ÂùóÔºåÂú®ËøûÁª≠Ê≠•ÊÄÅÁ©∫Èó¥‰∏≠ÂÆûÁé∞ÂÆûÊó∂Ê≠•ÊÄÅËá™ÈÄÇÂ∫î„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®‰∏çÂêåÈÄüÂ∫¶‰∏ãËÉΩÊòæËëóÈôç‰ΩéËÉΩËÄóÔºàÊúÄÈ´ò36.48%ÔºâÔºåÂêåÊó∂‰øùÊåÅÁ≤æÁ°ÆË∑üË∏™ÂíåËá™ÈÄÇÂ∫îÊ≠•ÊÄÅ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ºòÂåñÊ°ÜÊû∂ÔºåÁî®‰∫éÂõõË∂≥Êú∫Âô®‰∫∫ÂÆûÊó∂Ê≠•ÊÄÅËá™ÈÄÇÂ∫îÔºåËØ•Ê°ÜÊû∂Âú®ËøûÁª≠Ê≠•ÊÄÅÁ©∫Èó¥‰∏≠ËøêË°åÔºåÁªìÂêà‰∫ÜÊ®°ÂûãÈ¢ÑÊµãË∑ØÂæÑÁßØÂàÜÔºàMPPIÔºâÁÆóÊ≥ï‰∏éDreamerÊ®°ÂùóÔºå‰ªéËÄåÁîüÊàêËá™ÈÄÇÂ∫îÂíåÊúÄ‰ºòÁöÑËøêÂä®Á≠ñÁï•„ÄÇÂú®ÊØè‰∏™Êó∂Èó¥Ê≠•ÔºåMPPI‰ΩøÁî®Â≠¶‰π†Âà∞ÁöÑDreamerÂ•ñÂä±ÂáΩÊï∞ËÅîÂêà‰ºòÂåñÂä®‰ΩúÂíåÊ≠•ÊÄÅÂèòÈáèÔºåËØ•Â•ñÂä±ÂáΩÊï∞Êó®Âú®‰øÉËøõÈÄüÂ∫¶Ë∑üË∏™„ÄÅËÉΩÈáèÊïàÁéá„ÄÅÁ®≥ÂÆöÊÄß‰ª•ÂèäÂπ≥ÊªëËøáÊ∏°ÔºåÂêåÊó∂ÊÉ©ÁΩöÁ™ÅÂÖÄÁöÑÊ≠•ÊÄÅÂèòÂåñ„ÄÇÂ≠¶‰π†Âà∞ÁöÑ‰ª∑ÂÄºÂáΩÊï∞Ë¢´Áî®‰ΩúÁªàÁ´ØÂ•ñÂä±Ôºå‰ªéËÄåÂ∞ÜÂÖ¨ÂºèÊâ©Â±ïÂà∞Êó†ÈôêËåÉÂõ¥ËßÑÂàí„ÄÇÂú®Unitree Go1ÁöÑ‰ªøÁúüÁéØÂ¢É‰∏≠ËØÑ‰º∞‰∫ÜËØ•Ê°ÜÊû∂ÔºåÁªìÊûúË°®ÊòéÔºåÂú®‰∏çÂêåÁöÑÁõÆÊ†áÈÄüÂ∫¶‰∏ãÔºåËÉΩÈáèÊ∂àËÄóÂπ≥ÂùáÈôç‰Ωé‰∫ÜÈ´òËææ36.48%ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁ≤æÁ°ÆÁöÑË∑üË∏™ÂíåËá™ÈÄÇÂ∫îÁöÑ„ÄÅÈÄÇÂêà‰ªªÂä°ÁöÑÊ≠•ÊÄÅ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÂõõË∂≥Êú∫Âô®‰∫∫ËøêÂä®ÊéßÂà∂ÊñπÊ≥ïÔºåË¶Å‰πà‰æùËµñ‰∫éÊ®°ÂûãÈ¢ÑÊµãÊéßÂà∂ÔºàMPCÔºâÔºåÁº∫‰πèÂØπÁéØÂ¢ÉÂèòÂåñÁöÑÈÄÇÂ∫îÊÄßÔºõË¶Å‰πà‰æùËµñ‰∫éÊó†Ê®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÔºåÂÆπÊòìÊî∂ÊïõÂà∞Âçï‰∏ÄÁöÑ„ÄÅÊ¨°‰ºòÁöÑÊ≠•ÊÄÅ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§üÂÖºÈ°æÁéØÂ¢ÉÈÄÇÂ∫îÊÄßÂíåÊ≠•ÊÄÅÂ§öÊ†∑ÊÄßÁöÑÊéßÂà∂ÊñπÊ≥ïÔºå‰ª•ÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥Á®≥ÂÆöÁöÑËøêÂä®ÊéßÂà∂„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜMPCÁöÑ‰ºòÂåñËÉΩÂäõ‰∏éÂº∫ÂåñÂ≠¶‰π†ÁöÑÁéØÂ¢ÉÈÄÇÂ∫îËÉΩÂäõÁõ∏ÁªìÂêà„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂà©Áî®MPPIÁÆóÊ≥ïÂú®ËøûÁª≠Ê≠•ÊÄÅÁ©∫Èó¥‰∏≠ËøõË°å‰ºòÂåñÔºåÂêåÊó∂‰ΩøÁî®DreamerÊ®°ÂùóÂ≠¶‰π†Â•ñÂä±ÂáΩÊï∞ÔºåËØ•Â•ñÂä±ÂáΩÊï∞ËÉΩÂ§üÂºïÂØºÊú∫Âô®‰∫∫ÈÄâÊã©ËÉΩÈáèÊïàÁéáÈ´ò„ÄÅÁ®≥ÂÆöÊÄßÂ•Ω„ÄÅËøáÊ∏°Âπ≥ÊªëÁöÑÊ≠•ÊÄÅ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âú®‰∏çÂêåÁöÑÁéØÂ¢ÉÂíå‰ªªÂä°‰∏ãËá™ÈÄÇÂ∫îÂú∞Ë∞ÉÊï¥Ê≠•ÊÄÅÔºå‰ªéËÄåÂÆûÁé∞ÊúÄ‰ºòÁöÑËøêÂä®ÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÂê´‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) MPPIÊéßÂà∂Âô®ÔºöË¥üË¥£Âú®ÊØè‰∏™Êó∂Èó¥Ê≠•‰ºòÂåñÊú∫Âô®‰∫∫ÁöÑÂä®‰ΩúÂíåÊ≠•ÊÄÅÂèòÈáè„ÄÇ2) DreamerÊ®°ÂùóÔºöË¥üË¥£Â≠¶‰π†Â•ñÂä±ÂáΩÊï∞ÔºåËØ•Â•ñÂä±ÂáΩÊï∞ËÉΩÂ§üÂèçÊò†ËøêÂä®ÁöÑË¥®ÈáèÔºåÂåÖÊã¨ÈÄüÂ∫¶Ë∑üË∏™„ÄÅËÉΩÈáèÊïàÁéá„ÄÅÁ®≥ÂÆöÊÄßÁ≠â„ÄÇ3) ‰ª∑ÂÄºÂáΩÊï∞ÔºöÁî®‰∫é‰º∞ËÆ°Êú™Êù•ÂõûÊä•ÔºåÂπ∂‰Ωú‰∏∫ÁªàÁ´ØÂ•ñÂä±Ôºå‰ªéËÄåÊâ©Â±ïËßÑÂàíËåÉÂõ¥Âà∞Êó†Èôêhorizon„ÄÇÊï¥‰ΩìÊµÅÁ®ãÊòØÔºåMPPIÊéßÂà∂Âô®Ê†πÊçÆÂΩìÂâçÁä∂ÊÄÅÂíåDreamerÊ®°ÂùóÊèê‰æõÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÁîüÊàê‰∏ÄÁ≥ªÂàóÂÄôÈÄâÂä®‰ΩúÂíåÊ≠•ÊÄÅÔºåÁÑ∂ÂêéÈÄâÊã©ÊúÄ‰ºòÁöÑÂä®‰ΩúÂíåÊ≠•ÊÄÅÊâßË°å„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜMPPIÁÆóÊ≥ï‰∏éDreamerÊ®°ÂùóÁõ∏ÁªìÂêàÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂú®ËøûÁª≠Ê≠•ÊÄÅÁ©∫Èó¥‰∏≠ÁöÑÂÆûÊó∂Ê≠•ÊÄÅËá™ÈÄÇÂ∫î„ÄÇ‰∏é‰º†ÁªüÁöÑMPCÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁéØÂ¢ÉÂèòÂåñÔºõ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥Âø´Âú∞Êî∂ÊïõÂà∞ÊúÄ‰ºòÁ≠ñÁï•ÔºåÂπ∂ÈÅøÂÖçÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöDreamerÊ®°Âùó‰ΩøÁî®‰∏ñÁïåÊ®°ÂûãÂ≠¶‰π†ÁéØÂ¢ÉÂä®ÊÄÅÂíåÂ•ñÂä±ÂáΩÊï∞„ÄÇÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂÆÉÈúÄË¶ÅÁªºÂêàËÄÉËôëÈÄüÂ∫¶Ë∑üË∏™ËØØÂ∑Æ„ÄÅËÉΩÈáèÊ∂àËÄó„ÄÅÁ®≥ÂÆöÊÄß‰ª•ÂèäÊ≠•ÊÄÅÂàáÊç¢ÁöÑÂπ≥ÊªëÊÄß„ÄÇ‰ª∑ÂÄºÂáΩÊï∞ÈÄöËøáÂ≠¶‰π†ÂæóÂà∞ÔºåÁî®‰∫é‰º∞ËÆ°ÈïøÊúüÂõûÊä•ÔºåÂπ∂‰Ωú‰∏∫MPPIÁöÑÁªàÁ´ØÂ•ñÂä±„ÄÇMPPIÁÆóÊ≥ï‰∏≠ÁöÑÊ∏©Â∫¶ÂèÇÊï∞ÊéßÂà∂‰∫ÜÊé¢Á¥¢ÁöÑÁ®ãÂ∫¶ÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®Unitree Go1‰ªøÁúüÁéØÂ¢É‰∏≠ÔºåËÉΩÂ§üÊòæËëóÈôç‰ΩéËÉΩÈáèÊ∂àËÄóÔºåÂπ≥ÂùáÈôç‰ΩéÈ´òËææ36.48%ÔºåÂêåÊó∂‰øùÊåÅÁ≤æÁ°ÆÁöÑÈÄüÂ∫¶Ë∑üË∏™ÂíåËá™ÈÄÇÂ∫îÁöÑÊ≠•ÊÄÅ„ÄÇËØ•ÊñπÊ≥ïÂú®‰∏çÂêåÁõÆÊ†áÈÄüÂ∫¶‰∏ãÂùáË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊΩúÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑMPCÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁéØÂ¢ÉÂèòÂåñÔºåÂπ∂ÂÆûÁé∞Êõ¥‰ºòÁöÑËøêÂä®ÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÂõõË∂≥Êú∫Âô®‰∫∫Â∫îÁî®Âú∫ÊôØÔºå‰æãÂ¶ÇÊêúÊïë„ÄÅÂ∑°Ê£Ä„ÄÅÁâ©ÊµÅÁ≠â„ÄÇÈÄöËøáËá™ÈÄÇÂ∫îÊ≠•ÊÄÅË∞ÉÊï¥ÔºåÊú∫Âô®‰∫∫ÂèØ‰ª•Âú®Â§çÊùÇÂú∞ÂΩ¢Âíå‰∏çÂêå‰ªªÂä°ÈúÄÊ±Ç‰∏ãÂÆûÁé∞Êõ¥È´òÊïà„ÄÅÊõ¥Á®≥ÂÆöÁöÑËøêÂä®Ôºå‰ªéËÄåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Êé®ÂπøÂà∞ÂÖ∂‰ªñÁ±ªÂûãÁöÑÊú∫Âô®‰∫∫Ôºå‰æãÂ¶ÇÂèåË∂≥Êú∫Âô®‰∫∫ÂíåËΩÆÂºèÊú∫Âô®‰∫∫„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Model-free reinforcement learning (RL) has enabled adaptable and agile quadruped locomotion; however, policies often converge to a single gait, leading to suboptimal performance. Traditionally, Model Predictive Control (MPC) has been extensively used to obtain task-specific optimal policies but lacks the ability to adapt to varying environments. To address these limitations, we propose an optimization framework for real-time gait adaptation in a continuous gait space, combining the Model Predictive Path Integral (MPPI) algorithm with a Dreamer module to produce adaptive and optimal policies for quadruped locomotion. At each time step, MPPI jointly optimizes the actions and gait variables using a learned Dreamer reward that promotes velocity tracking, energy efficiency, stability, and smooth transitions, while penalizing abrupt gait changes. A learned value function is incorporated as terminal reward, extending the formulation to an infinite-horizon planner. We evaluate our framework in simulation on the Unitree Go1, demonstrating an average reduction of up to 36.48 % in energy consumption across varying target speeds, while maintaining accurate tracking and adaptive, task-appropriate gaits.

