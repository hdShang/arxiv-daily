---
layout: default
title: Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning
---

# Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.20706" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.20706v2</a>
  <a href="https://arxiv.org/pdf/2510.20706.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20706v2" onclick="toggleFavorite(this, '2510.20706v2', 'Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Prakrut Kotecha, Ganga Nair B, Shishir Kolathaya

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-23 (æ›´æ–°: 2025-10-24)

**å¤‡æ³¨**: 7 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»“åˆMPCä¸å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°å››è¶³æœºå™¨äººå®æ—¶æ­¥æ€è‡ªé€‚åº”**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å››è¶³æœºå™¨äºº` `æ­¥æ€è‡ªé€‚åº”` `æ¨¡å‹é¢„æµ‹æ§åˆ¶` `å¼ºåŒ–å­¦ä¹ ` `Dreamer` `è¿åŠ¨è§„åˆ’` `èƒ½é‡æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ è™½èƒ½å®ç°å››è¶³æœºå™¨äººè‡ªé€‚åº”è¿åŠ¨ï¼Œä½†ç­–ç•¥å¸¸æ”¶æ•›äºå•ä¸€æ­¥æ€ï¼Œå¯¼è‡´æ€§èƒ½æ¬ ä½³ã€‚
2. æœ¬æ–‡ç»“åˆæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨MPPIç®—æ³•å’ŒDreameræ¨¡å—ï¼Œåœ¨è¿ç»­æ­¥æ€ç©ºé—´ä¸­å®ç°å®æ—¶æ­¥æ€è‡ªé€‚åº”ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒé€Ÿåº¦ä¸‹èƒ½æ˜¾è‘—é™ä½èƒ½è€—ï¼ˆæœ€é«˜36.48%ï¼‰ï¼ŒåŒæ—¶ä¿æŒç²¾ç¡®è·Ÿè¸ªå’Œè‡ªé€‚åº”æ­¥æ€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºå››è¶³æœºå™¨äººå®æ—¶æ­¥æ€è‡ªé€‚åº”ï¼Œè¯¥æ¡†æ¶åœ¨è¿ç»­æ­¥æ€ç©ºé—´ä¸­è¿è¡Œï¼Œç»“åˆäº†æ¨¡å‹é¢„æµ‹è·¯å¾„ç§¯åˆ†ï¼ˆMPPIï¼‰ç®—æ³•ä¸Dreameræ¨¡å—ï¼Œä»è€Œç”Ÿæˆè‡ªé€‚åº”å’Œæœ€ä¼˜çš„è¿åŠ¨ç­–ç•¥ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼ŒMPPIä½¿ç”¨å­¦ä¹ åˆ°çš„Dreamerå¥–åŠ±å‡½æ•°è”åˆä¼˜åŒ–åŠ¨ä½œå’Œæ­¥æ€å˜é‡ï¼Œè¯¥å¥–åŠ±å‡½æ•°æ—¨åœ¨ä¿ƒè¿›é€Ÿåº¦è·Ÿè¸ªã€èƒ½é‡æ•ˆç‡ã€ç¨³å®šæ€§ä»¥åŠå¹³æ»‘è¿‡æ¸¡ï¼ŒåŒæ—¶æƒ©ç½šçªå…€çš„æ­¥æ€å˜åŒ–ã€‚å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°è¢«ç”¨ä½œç»ˆç«¯å¥–åŠ±ï¼Œä»è€Œå°†å…¬å¼æ‰©å±•åˆ°æ— é™èŒƒå›´è§„åˆ’ã€‚åœ¨Unitree Go1çš„ä»¿çœŸç¯å¢ƒä¸­è¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨ä¸åŒçš„ç›®æ ‡é€Ÿåº¦ä¸‹ï¼Œèƒ½é‡æ¶ˆè€—å¹³å‡é™ä½äº†é«˜è¾¾36.48%ï¼ŒåŒæ—¶ä¿æŒäº†ç²¾ç¡®çš„è·Ÿè¸ªå’Œè‡ªé€‚åº”çš„ã€é€‚åˆä»»åŠ¡çš„æ­¥æ€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å››è¶³æœºå™¨äººè¿åŠ¨æ§åˆ¶æ–¹æ³•ï¼Œè¦ä¹ˆä¾èµ–äºæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ï¼Œç¼ºä¹å¯¹ç¯å¢ƒå˜åŒ–çš„é€‚åº”æ€§ï¼›è¦ä¹ˆä¾èµ–äºæ— æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå®¹æ˜“æ”¶æ•›åˆ°å•ä¸€çš„ã€æ¬¡ä¼˜çš„æ­¥æ€ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿå…¼é¡¾ç¯å¢ƒé€‚åº”æ€§å’Œæ­¥æ€å¤šæ ·æ€§çš„æ§åˆ¶æ–¹æ³•ï¼Œä»¥å®ç°æ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„è¿åŠ¨æ§åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†MPCçš„ä¼˜åŒ–èƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ çš„ç¯å¢ƒé€‚åº”èƒ½åŠ›ç›¸ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨MPPIç®—æ³•åœ¨è¿ç»­æ­¥æ€ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ï¼ŒåŒæ—¶ä½¿ç”¨Dreameræ¨¡å—å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œè¯¥å¥–åŠ±å‡½æ•°èƒ½å¤Ÿå¼•å¯¼æœºå™¨äººé€‰æ‹©èƒ½é‡æ•ˆç‡é«˜ã€ç¨³å®šæ€§å¥½ã€è¿‡æ¸¡å¹³æ»‘çš„æ­¥æ€ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæœºå™¨äººå¯ä»¥åœ¨ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ä¸‹è‡ªé€‚åº”åœ°è°ƒæ•´æ­¥æ€ï¼Œä»è€Œå®ç°æœ€ä¼˜çš„è¿åŠ¨æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) MPPIæ§åˆ¶å™¨ï¼šè´Ÿè´£åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¼˜åŒ–æœºå™¨äººçš„åŠ¨ä½œå’Œæ­¥æ€å˜é‡ã€‚2) Dreameræ¨¡å—ï¼šè´Ÿè´£å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œè¯¥å¥–åŠ±å‡½æ•°èƒ½å¤Ÿåæ˜ è¿åŠ¨çš„è´¨é‡ï¼ŒåŒ…æ‹¬é€Ÿåº¦è·Ÿè¸ªã€èƒ½é‡æ•ˆç‡ã€ç¨³å®šæ€§ç­‰ã€‚3) ä»·å€¼å‡½æ•°ï¼šç”¨äºä¼°è®¡æœªæ¥å›æŠ¥ï¼Œå¹¶ä½œä¸ºç»ˆç«¯å¥–åŠ±ï¼Œä»è€Œæ‰©å±•è§„åˆ’èŒƒå›´åˆ°æ— é™horizonã€‚æ•´ä½“æµç¨‹æ˜¯ï¼ŒMPPIæ§åˆ¶å™¨æ ¹æ®å½“å‰çŠ¶æ€å’ŒDreameræ¨¡å—æä¾›çš„å¥–åŠ±å‡½æ•°ï¼Œç”Ÿæˆä¸€ç³»åˆ—å€™é€‰åŠ¨ä½œå’Œæ­¥æ€ï¼Œç„¶åé€‰æ‹©æœ€ä¼˜çš„åŠ¨ä½œå’Œæ­¥æ€æ‰§è¡Œã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†MPPIç®—æ³•ä¸Dreameræ¨¡å—ç›¸ç»“åˆï¼Œä»è€Œå®ç°äº†åœ¨è¿ç»­æ­¥æ€ç©ºé—´ä¸­çš„å®æ—¶æ­¥æ€è‡ªé€‚åº”ã€‚ä¸ä¼ ç»Ÿçš„MPCæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç¯å¢ƒå˜åŒ–ï¼›ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šDreameræ¨¡å—ä½¿ç”¨ä¸–ç•Œæ¨¡å‹å­¦ä¹ ç¯å¢ƒåŠ¨æ€å’Œå¥–åŠ±å‡½æ•°ã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œå®ƒéœ€è¦ç»¼åˆè€ƒè™‘é€Ÿåº¦è·Ÿè¸ªè¯¯å·®ã€èƒ½é‡æ¶ˆè€—ã€ç¨³å®šæ€§ä»¥åŠæ­¥æ€åˆ‡æ¢çš„å¹³æ»‘æ€§ã€‚ä»·å€¼å‡½æ•°é€šè¿‡å­¦ä¹ å¾—åˆ°ï¼Œç”¨äºä¼°è®¡é•¿æœŸå›æŠ¥ï¼Œå¹¶ä½œä¸ºMPPIçš„ç»ˆç«¯å¥–åŠ±ã€‚MPPIç®—æ³•ä¸­çš„æ¸©åº¦å‚æ•°æ§åˆ¶äº†æ¢ç´¢çš„ç¨‹åº¦ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨Unitree Go1ä»¿çœŸç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½èƒ½é‡æ¶ˆè€—ï¼Œå¹³å‡é™ä½é«˜è¾¾36.48%ï¼ŒåŒæ—¶ä¿æŒç²¾ç¡®çš„é€Ÿåº¦è·Ÿè¸ªå’Œè‡ªé€‚åº”çš„æ­¥æ€ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒç›®æ ‡é€Ÿåº¦ä¸‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä¸ä¼ ç»Ÿçš„MPCæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç¯å¢ƒå˜åŒ–ï¼Œå¹¶å®ç°æ›´ä¼˜çš„è¿åŠ¨æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§å››è¶³æœºå™¨äººåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚æœæ•‘ã€å·¡æ£€ã€ç‰©æµç­‰ã€‚é€šè¿‡è‡ªé€‚åº”æ­¥æ€è°ƒæ•´ï¼Œæœºå™¨äººå¯ä»¥åœ¨å¤æ‚åœ°å½¢å’Œä¸åŒä»»åŠ¡éœ€æ±‚ä¸‹å®ç°æ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„è¿åŠ¨ï¼Œä»è€Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººï¼Œä¾‹å¦‚åŒè¶³æœºå™¨äººå’Œè½®å¼æœºå™¨äººã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Model-free reinforcement learning (RL) has enabled adaptable and agile quadruped locomotion; however, policies often converge to a single gait, leading to suboptimal performance. Traditionally, Model Predictive Control (MPC) has been extensively used to obtain task-specific optimal policies but lacks the ability to adapt to varying environments. To address these limitations, we propose an optimization framework for real-time gait adaptation in a continuous gait space, combining the Model Predictive Path Integral (MPPI) algorithm with a Dreamer module to produce adaptive and optimal policies for quadruped locomotion. At each time step, MPPI jointly optimizes the actions and gait variables using a learned Dreamer reward that promotes velocity tracking, energy efficiency, stability, and smooth transitions, while penalizing abrupt gait changes. A learned value function is incorporated as terminal reward, extending the formulation to an infinite-horizon planner. We evaluate our framework in simulation on the Unitree Go1, demonstrating an average reduction of up to 36.48 % in energy consumption across varying target speeds, while maintaining accurate tracking and adaptive, task-appropriate gaits.

