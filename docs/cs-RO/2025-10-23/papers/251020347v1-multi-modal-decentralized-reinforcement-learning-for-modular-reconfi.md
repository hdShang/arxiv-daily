---
layout: default
title: Multi-Modal Decentralized Reinforcement Learning for Modular Reconfigurable Lunar Robots
---

# Multi-Modal Decentralized Reinforcement Learning for Modular Reconfigurable Lunar Robots

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.20347" target="_blank" class="toolbar-btn">arXiv: 2510.20347v1</a>
    <a href="https://arxiv.org/pdf/2510.20347.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.20347v1" 
            onclick="toggleFavorite(this, '2510.20347v1', 'Multi-Modal Decentralized Reinforcement Learning for Modular Reconfigurable Lunar Robots')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Ashutosh Mishra, Shreya Santra, Elian Neppel, Edoardo M. Rossi Lombardi, Shamistan Karimov, Kentaro Uno, Kazuya Yoshida

**ÂàÜÁ±ª**: cs.RO, cs.MA

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-23

**Â§áÊ≥®**: Accepted in IEEE iSpaRo 2025. Awaiting Publication

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öÊ®°ÊÄÅÂàÜÊï£ÂºèÂº∫ÂåñÂ≠¶‰π†ÔºåÁî®‰∫éÊ®°ÂùóÂåñÂèØÈáçÊûÑÊúàÁêÉÊú∫Âô®‰∫∫„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Ê®°ÂùóÂåñÊú∫Âô®‰∫∫` `ÂàÜÊï£ÂºèÂº∫ÂåñÂ≠¶‰π†` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Èõ∂Ê†∑Êú¨Ê≥õÂåñ` `ÊúàÁêÉÊé¢Êµã`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê®°ÂùóÂåñÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÁªÑÂêàÁàÜÁÇ∏ÔºåÈöæ‰ª•Áªü‰∏ÄÊéßÂà∂Ôºå‰∏∫Á©∫Èó¥‰ªªÂä°Â∏¶Êù•ÊåëÊàò„ÄÇ
2. ÈááÁî®ÂàÜÊï£ÂºèÂº∫ÂåñÂ≠¶‰π†ÔºåÊØè‰∏™Ê®°ÂùóËá™‰∏ªÂ≠¶‰π†Á≠ñÁï•ÔºåÂÆûÁé∞Èõ∂Ê†∑Êú¨Ê≥õÂåñ„ÄÇ
3. ‰ªøÁúüÂíåÊúàÁêÉÊ®°ÊãüÊµãËØïÈ™åËØÅ‰∫ÜËá™‰∏ªÁßªÂä®„ÄÅËΩ¨ÂêëÂíåÈáçÊûÑÂØπÈΩêÁöÑÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàÜÊï£ÂºèÂº∫ÂåñÂ≠¶‰π†(Dec-RL)ÊñπÊ°àÔºåÁî®‰∫éÊéßÂà∂Ê®°ÂùóÂåñÂèØÈáçÊûÑÊú∫Âô®‰∫∫Ôºå‰ΩøÂÖ∂ÈÄÇÂ∫îÁâπÂÆö‰ªªÂä°ÁöÑÁ©∫Èó¥Êìç‰Ωú„ÄÇÊØè‰∏™Ê®°ÂùóÂ≠¶‰π†Ëá™Â∑±ÁöÑÁ≠ñÁï•ÔºöËΩÆÂºèÊ®°Âùó‰ΩøÁî®ËΩØÊºîÂëò-ËØÑËÆ∫ÂÆ∂(SAC)ÁÆóÊ≥ïËøõË°åÁßªÂä®Ôºå7Ëá™Áî±Â∫¶(DoF)Êú∫Ê¢∞ËáÇ‰ΩøÁî®ËøëÁ´ØÁ≠ñÁï•‰ºòÂåñ(PPO)ÁÆóÊ≥ïËøõË°åËΩ¨ÂêëÂíåÊìç‰ΩúÔºå‰ªéËÄåÂÆûÁé∞ÂØπÊú™ËßÅÈÖçÁΩÆÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñ„ÄÇÂú®‰ªøÁúü‰∏≠ÔºåËΩ¨ÂêëÁ≠ñÁï•Âú®ÊúüÊúõËßíÂ∫¶ÂíåÂÆûÈôÖËßíÂ∫¶‰πãÈó¥ÁöÑÂπ≥ÂùáÁªùÂØπËØØÂ∑Æ‰∏∫3.63¬∞ÔºõÊìç‰ΩúÁ≠ñÁï•Âú®ÁõÆÊ†áÂÅèÁßªÊ†áÂáÜ‰∏ãÁöÑÊàêÂäüÁéáÁ®≥ÂÆöÂú®84.6%ÔºõËΩÆÂºèÁ≠ñÁï•Âú®‰øùÊåÅ99.6%ÊàêÂäüÁéáÁöÑÂêåÊó∂ÔºåÂ∞ÜÂπ≥ÂùáÁîµÊú∫Êâ≠Áü©Áõ∏ÂØπ‰∫éÂü∫Á∫øÈôç‰Ωé‰∫Ü95.4%„ÄÇÊúàÁêÉÊ®°ÊãüÂú∫Âú∞ÊµãËØïÈ™åËØÅ‰∫ÜËá™‰∏ªÁßªÂä®„ÄÅËΩ¨ÂêëÂíåÂàùÊ≠•ÂØπÈΩê‰ª•ËøõË°åÈáçÊûÑÁöÑÈõ∂Ê†∑Êú¨ÈõÜÊàê„ÄÇËØ•Á≥ªÁªüÂú®Á≠ñÁï•ÊâßË°åÁöÑÂêåÊ≠•„ÄÅÂπ∂Ë°åÂíåÈ°∫Â∫èÊ®°Âºè‰πãÈó¥Âπ≥Á®≥ËøáÊ∏°ÔºåÊ≤°ÊúâÁ©∫Èó≤Áä∂ÊÄÅÊàñÊéßÂà∂ÂÜ≤Á™ÅÔºåË°®ÊòéËØ•ÊñπÊ≥ïÂØπ‰∫éÊ®°ÂùóÂåñÊúàÁêÉÊú∫Âô®‰∫∫ÂÖ∑ÊúâÂèØÊâ©Â±ïÊÄß„ÄÅÂèØÈáçÁî®ÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊ®°ÂùóÂåñÂèØÈáçÊûÑÊú∫Âô®‰∫∫Âú®Á©∫Èó¥‰ªªÂä°‰∏≠ÂÖ∑Êúâ‰ºòÂäøÔºå‰ΩÜÂÖ∂ÂΩ¢ÊÄÅÁªÑÂêàÂëàÊåáÊï∞Â¢ûÈïøÔºåÂØºËá¥Èöæ‰ª•ËÆæËÆ°‰∏Ä‰∏™ÈÄöÁî®ÁöÑÊéßÂà∂Á≠ñÁï•„ÄÇÁé∞ÊúâÁöÑÈõÜ‰∏≠ÂºèÊéßÂà∂ÊñπÊ≥ïÈöæ‰ª•ÈÄÇÂ∫î‰∏çÂêåÊûÑÂûãÁöÑÊú∫Âô®‰∫∫ÔºåÂπ∂‰∏îËÆ°ÁÆóÂ§çÊùÇÂ∫¶È´òÔºåÈöæ‰ª•ÂÆûÊó∂ÊéßÂà∂„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÁßçËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÊûÑÂûãÔºåÂπ∂‰∏îÂÖ∑ÊúâËâØÂ•ΩÊ≥õÂåñËÉΩÂäõÁöÑÊéßÂà∂ÊñπÊ≥ï„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞ÜÊéßÂà∂ÈóÆÈ¢òÂàÜËß£‰∏∫Â§ö‰∏™Â≠êÈóÆÈ¢òÔºåÊØè‰∏™Â≠êÈóÆÈ¢òÁî±‰∏Ä‰∏™Áã¨Á´ãÁöÑÊ®°ÂùóË¥üË¥£„ÄÇÊØè‰∏™Ê®°ÂùóÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Â≠¶‰π†Ëá™Â∑±ÁöÑÊéßÂà∂Á≠ñÁï•Ôºå‰ªéËÄåÂÆûÁé∞ÂàÜÊï£ÂºèÊéßÂà∂„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂú∞Èôç‰ΩéÊéßÂà∂Â§çÊùÇÂ∫¶ÔºåÂπ∂‰∏îÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑÊú∫Âô®‰∫∫ÊûÑÂûã„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÔºåÈíàÂØπ‰∏çÂêåÁ±ªÂûãÁöÑÊ®°ÂùóÔºàËΩÆÂºèÂíåÊú∫Ê¢∞ËáÇÔºâÈááÁî®‰∏çÂêåÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºå‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´Â§ö‰∏™Áã¨Á´ãÁöÑÊ®°ÂùóÔºåÊØè‰∏™Ê®°ÂùóÈÖçÂ§áËá™Â∑±ÁöÑ‰º†ÊÑüÂô®ÂíåËÆ°ÁÆóÂçïÂÖÉ„ÄÇÊØè‰∏™Ê®°ÂùóÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Â≠¶‰π†Ëá™Â∑±ÁöÑÊéßÂà∂Á≠ñÁï•„ÄÇËΩÆÂºèÊ®°Âùó‰ΩøÁî®SACÁÆóÊ≥ïËøõË°åÁßªÂä®ÊéßÂà∂ÔºåÊú∫Ê¢∞ËáÇÊ®°Âùó‰ΩøÁî®PPOÁÆóÊ≥ïËøõË°åËΩ¨ÂêëÂíåÊìç‰ΩúÊéßÂà∂„ÄÇÂú®Á≠ñÁï•ÊâßË°åÈò∂ÊÆµÔºåÂêÑ‰∏™Ê®°ÂùóÂèØ‰ª•ÂêåÊ≠•„ÄÅÂπ∂Ë°åÊàñÈ°∫Â∫èÊâßË°åÂä®‰ΩúÔºå‰ª•ÂÆåÊàêÂ§çÊùÇÁöÑ‰ªªÂä°„ÄÇÁ≥ªÁªüËÆæËÆ°ÈÅøÂÖç‰∫ÜÁ©∫Èó≤Áä∂ÊÄÅÂíåÊéßÂà∂ÂÜ≤Á™ÅÔºå‰øùËØÅ‰∫ÜÁ≥ªÁªüÁöÑÈ≤ÅÊ£íÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂ∞ÜÂàÜÊï£ÂºèÂº∫ÂåñÂ≠¶‰π†Â∫îÁî®‰∫éÊ®°ÂùóÂåñÂèØÈáçÊûÑÊú∫Âô®‰∫∫ÁöÑÊéßÂà∂„ÄÇÈÄöËøáËÆ©ÊØè‰∏™Ê®°ÂùóËá™‰∏ªÂ≠¶‰π†Á≠ñÁï•ÔºåÂÆûÁé∞‰∫ÜÂØπ‰∏çÂêåÊûÑÂûãÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñ„ÄÇÊ≠§Â§ñÔºåÈíàÂØπ‰∏çÂêåÁ±ªÂûãÁöÑÊ®°ÂùóÈááÁî®‰∏çÂêåÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜÊéßÂà∂ÊÄßËÉΩ„ÄÇËøôÁßçÊñπÊ≥ïÊúâÊïàÂú∞Èôç‰Ωé‰∫ÜÊéßÂà∂Â§çÊùÇÂ∫¶ÔºåÂπ∂‰∏îÂÖ∑ÊúâËâØÂ•ΩÁöÑÂèØÊâ©Â±ïÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËΩÆÂºèÊ®°Âùó‰ΩøÁî®SACÁÆóÊ≥ïÔºåÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°ÈºìÂä±È´òÊïàÁßªÂä®Âπ∂ÊÉ©ÁΩöËøáÂ§ßÁöÑÁîµÊú∫Êâ≠Áü©„ÄÇÊú∫Ê¢∞ËáÇÊ®°Âùó‰ΩøÁî®PPOÁÆóÊ≥ïÔºåÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°ÈºìÂä±Êú∫Ê¢∞ËáÇÂà∞ËææÁõÆÊ†á‰ΩçÁΩÆÂπ∂‰øùÊåÅÁ®≥ÂÆö„ÄÇÁΩëÁªúÁªìÊûÑÊñπÈù¢ÔºåSACÂíåPPOÈÉΩÈááÁî®‰∫ÜActor-CriticÁªìÊûÑÔºåÂÖ∑‰ΩìÁΩëÁªúÂ±ÇÊï∞ÂíåÁ•ûÁªèÂÖÉÊï∞ÈáèÊ†πÊçÆÂÆûÈôÖ‰ªªÂä°ËøõË°åË∞ÉÊï¥„ÄÇÂÆûÈ™å‰∏≠ÔºåÂØπSACÂíåPPOÁöÑË∂ÖÂèÇÊï∞ËøõË°å‰∫ÜÁ≤æÁªÜË∞ÉÊï¥Ôºå‰æãÂ¶ÇÂ≠¶‰π†Áéá„ÄÅÊäòÊâ£Âõ†Â≠ê„ÄÅË£ÅÂâ™ËåÉÂõ¥Á≠âÔºå‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

‰ªøÁúüÁªìÊûúË°®ÊòéÔºåËΩ¨ÂêëÁ≠ñÁï•ÁöÑÂπ≥ÂùáÁªùÂØπËØØÂ∑Æ‰∏∫3.63¬∞ÔºåÊìç‰ΩúÁ≠ñÁï•ÁöÑÊàêÂäüÁéáËææÂà∞84.6%ÔºåËΩÆÂºèÁ≠ñÁï•Âú®‰øùÊåÅ99.6%ÊàêÂäüÁéáÁöÑÂêåÊó∂ÔºåÂ∞ÜÂπ≥ÂùáÁîµÊú∫Êâ≠Áü©Èôç‰Ωé‰∫Ü95.4%„ÄÇÊúàÁêÉÊ®°ÊãüÂú∫Âú∞ÊµãËØïÈ™åËØÅ‰∫ÜÁ≥ªÁªüÂú®Ëá™‰∏ªÁßªÂä®„ÄÅËΩ¨ÂêëÂíåÈáçÊûÑÂØπÈΩêÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÁ≥ªÁªüËÉΩÂ§üÂú®ÂêåÊ≠•„ÄÅÂπ∂Ë°åÂíåÈ°∫Â∫èÊ®°Âºè‰πãÈó¥Âπ≥Á®≥ËøáÊ∏°ÔºåÊ≤°ÊúâÁ©∫Èó≤Áä∂ÊÄÅÊàñÊéßÂà∂ÂÜ≤Á™Å„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊúàÁêÉÊé¢Êµã„ÄÅÁ©∫Èó¥Á´ôÁª¥Êä§„ÄÅÁÅæÂêéÊïëÊè¥Á≠âÈ¢ÜÂüü„ÄÇÊ®°ÂùóÂåñÊú∫Âô®‰∫∫ËÉΩÂ§üÊ†πÊçÆ‰ªªÂä°ÈúÄÊ±ÇËøõË°åÈáçÊûÑÔºåÈÄÇÂ∫îÂ§çÊùÇÂ§öÂèòÁöÑÁéØÂ¢É„ÄÇÂàÜÊï£ÂºèÊéßÂà∂ÊñπÊ≥ïÊèêÈ´ò‰∫ÜÁ≥ªÁªüÁöÑÈ≤ÅÊ£íÊÄßÂíåÂèØÊâ©Â±ïÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂ∫îÂØπÁ™ÅÂèëÊÉÖÂÜµ„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÊúâÊúõÂ∫îÁî®‰∫éÊõ¥ÂπøÊ≥õÁöÑÊú∫Âô®‰∫∫È¢ÜÂüüÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂà∂ÈÄ†Á≠â„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Modular reconfigurable robots suit task-specific space operations, but the combinatorial growth of morphologies hinders unified control. We propose a decentralized reinforcement learning (Dec-RL) scheme where each module learns its own policy: wheel modules use Soft Actor-Critic (SAC) for locomotion and 7-DoF limbs use Proximal Policy Optimization (PPO) for steering and manipulation, enabling zero-shot generalization to unseen configurations. In simulation, the steering policy achieved a mean absolute error of 3.63¬∞ between desired and induced angles; the manipulation policy plateaued at 84.6 % success on a target-offset criterion; and the wheel policy cut average motor torque by 95.4 % relative to baseline while maintaining 99.6 % success. Lunar-analogue field tests validated zero-shot integration for autonomous locomotion, steering, and preliminary alignment for reconfiguration. The system transitioned smoothly among synchronous, parallel, and sequential modes for Policy Execution, without idle states or control conflicts, indicating a scalable, reusable, and robust approach for modular lunar robots.

