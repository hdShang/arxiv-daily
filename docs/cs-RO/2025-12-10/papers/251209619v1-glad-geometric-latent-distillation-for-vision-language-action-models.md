---
layout: default
title: GLaD: Geometric Latent Distillation for Vision-Language-Action Models
---

# GLaD: Geometric Latent Distillation for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.09619" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.09619v1</a>
  <a href="https://arxiv.org/pdf/2512.09619.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.09619v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.09619v1', 'GLaD: Geometric Latent Distillation for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minghao Guo, Meng Cao, Jiachen Tao, Rongtao Xu, Yan Yan, Xiaodan Liang, Ivan Laptev, Xiaojun Chang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**GLaDï¼šå‡ ä½•æ½œåœ¨è’¸é¦å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å‡ ä½•æ„ŸçŸ¥` `çŸ¥è¯†è’¸é¦` `ç©ºé—´æ¨ç†` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹å¿½ç•¥äº†å‡ ä½•ä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶ç©ºé—´æ¨ç†å’Œæ“ä½œèƒ½åŠ›ã€‚
2. GLaDé€šè¿‡å‡ ä½•æ½œåœ¨è’¸é¦ï¼Œå°†3Då‡ ä½•å…ˆéªŒçŸ¥è¯†èå…¥LLMçš„è§†è§‰tokenè¡¨ç¤ºä¸­ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒGLaDåœ¨LIBEROä»»åŠ¡ä¸­ä¼˜äºUniVLAï¼ŒéªŒè¯äº†å‡ ä½•æ„ŸçŸ¥é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹ä¸»è¦ä¾èµ–RGBä¿¡æ¯ï¼Œå¿½ç•¥äº†å¯¹ç©ºé—´æ¨ç†å’Œæ“ä½œè‡³å…³é‡è¦çš„å‡ ä½•çº¿ç´¢ã€‚æœ¬æ–‡æå‡ºäº†GLaDï¼Œä¸€ä¸ªå‡ ä½•æ„ŸçŸ¥çš„VLAæ¡†æ¶ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦åœ¨é¢„è®­ç»ƒæœŸé—´èå…¥3Då‡ ä½•å…ˆéªŒã€‚ä¸ä»…å°†å‡ ä½•ç‰¹å¾è’¸é¦åˆ°è§†è§‰ç¼–ç å™¨ä¸åŒï¼ŒGLaDå°†LLMä¸­å¯¹åº”äºè§†è§‰tokençš„éšè—çŠ¶æ€ä¸å†»ç»“çš„å‡ ä½•æ„ŸçŸ¥è§†è§‰Transformer (VGGT)çš„ç‰¹å¾å¯¹é½ï¼Œç¡®ä¿å‡ ä½•ç†è§£è¢«æ·±åº¦é›†æˆåˆ°é©±åŠ¨åŠ¨ä½œé¢„æµ‹çš„å¤šæ¨¡æ€è¡¨ç¤ºä¸­ã€‚åœ¨Bridgeæ•°æ®é›†ä¸Šä½¿ç”¨è¿™ç§å‡ ä½•è’¸é¦æœºåˆ¶è¿›è¡Œé¢„è®­ç»ƒåï¼ŒGLaDåœ¨å››ä¸ªLIBEROä»»åŠ¡å¥—ä»¶ä¸­å®ç°äº†94.1%çš„å¹³å‡æˆåŠŸç‡ï¼Œä¼˜äºä½¿ç”¨ç›¸åŒé¢„è®­ç»ƒæ•°æ®çš„UniVLA (92.5%)ã€‚è¿™äº›ç»“æœéªŒè¯äº†å‡ ä½•æ„ŸçŸ¥é¢„è®­ç»ƒå¢å¼ºäº†ç©ºé—´æ¨ç†å’Œç­–ç•¥æ³›åŒ–èƒ½åŠ›ï¼Œè€Œæ— éœ€æ˜¾å¼æ·±åº¦ä¼ æ„Ÿå™¨æˆ–3Dæ ‡æ³¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºRGBå›¾åƒä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†åœºæ™¯çš„å‡ ä½•ç»“æ„ä¿¡æ¯ã€‚è¿™ç§å¿½ç•¥å¯¼è‡´æ¨¡å‹åœ¨éœ€è¦å¤æ‚ç©ºé—´æ¨ç†å’Œæ“ä½œçš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æœ‰æ•ˆåˆ©ç”¨å‡ ä½•ä¿¡æ¯çš„èƒ½åŠ›ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGLaDçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡çŸ¥è¯†è’¸é¦ï¼Œå°†å‡ ä½•ä¿¡æ¯ä»ä¸€ä¸ªé¢„è®­ç»ƒçš„å‡ ä½•æ„ŸçŸ¥è§†è§‰Transformer (VGGT)ä¼ é€’åˆ°VLAæ¨¡å‹ä¸­çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒGLaDä¸æ˜¯ç›´æ¥å°†å‡ ä½•ç‰¹å¾è’¸é¦åˆ°è§†è§‰ç¼–ç å™¨ï¼Œè€Œæ˜¯å°†LLMä¸­å¯¹åº”äºè§†è§‰tokençš„éšè—çŠ¶æ€ä¸VGGTçš„ç‰¹å¾å¯¹é½ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯è®©LLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨åœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ç©ºé—´æ¨ç†å’Œæ“ä½œèƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGLaDçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä¸€ä¸ªé¢„è®­ç»ƒçš„å‡ ä½•æ„ŸçŸ¥è§†è§‰Transformer (VGGT)ï¼Œç”¨äºæå–åœºæ™¯çš„å‡ ä½•ç‰¹å¾ï¼›2) ä¸€ä¸ªè§†è§‰ç¼–ç å™¨ï¼Œç”¨äºå°†RGBå›¾åƒç¼–ç æˆè§†è§‰ç‰¹å¾ï¼›3) ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºå¤„ç†æ–‡æœ¬æŒ‡ä»¤å’Œèåˆè§†è§‰ç‰¹å¾ï¼›4) ä¸€ä¸ªåŠ¨ä½œé¢„æµ‹æ¨¡å—ï¼Œç”¨äºæ ¹æ®èåˆåçš„å¤šæ¨¡æ€è¡¨ç¤ºé¢„æµ‹åŠ¨ä½œã€‚GLaDçš„å…³é”®åœ¨äºå°†VGGTæå–çš„å‡ ä½•ç‰¹å¾é€šè¿‡çŸ¥è¯†è’¸é¦çš„æ–¹å¼èå…¥åˆ°LLMä¸­ï¼Œä»è€Œå¢å¼ºLLMå¯¹å‡ ä½•ä¿¡æ¯çš„ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šGLaDæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå…¶å‡ ä½•æ½œåœ¨è’¸é¦æœºåˆ¶ã€‚ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸åŒï¼ŒGLaDä¸æ˜¯ç›´æ¥å°†å‡ ä½•ç‰¹å¾è’¸é¦åˆ°è§†è§‰ç¼–ç å™¨ï¼Œè€Œæ˜¯å°†LLMä¸­å¯¹åº”äºè§†è§‰tokençš„éšè—çŠ¶æ€ä¸VGGTçš„ç‰¹å¾å¯¹é½ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å°†å‡ ä½•ä¿¡æ¯èå…¥åˆ°å¤šæ¨¡æ€è¡¨ç¤ºä¸­ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ç©ºé—´æ¨ç†å’Œæ“ä½œèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGLaDæ— éœ€æ˜¾å¼çš„æ·±åº¦ä¼ æ„Ÿå™¨æˆ–3Dæ ‡æ³¨ï¼Œå³å¯å®ç°å‡ ä½•æ„ŸçŸ¥çš„é¢„è®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šGLaDçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„VGGTæå–å‡ ä½•ç‰¹å¾ï¼›2) ä½¿ç”¨Transformeræ¶æ„çš„LLMè¿›è¡Œå¤šæ¨¡æ€èåˆï¼›3) è®¾è®¡åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºå°†LLMçš„éšè—çŠ¶æ€ä¸VGGTçš„ç‰¹å¾å¯¹é½ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å¯èƒ½åŒ…æ‹¬KLæ•£åº¦æˆ–MSEæŸå¤±ç­‰ã€‚æ­¤å¤–ï¼ŒGLaDè¿˜å¯èƒ½é‡‡ç”¨ä¸€äº›æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä¾‹å¦‚éšæœºè£å‰ªã€æ—‹è½¬ç­‰ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­åº”è¯¥æœ‰æ›´è¯¦ç»†çš„æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

GLaDåœ¨LIBEROä»»åŠ¡å¥—ä»¶ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å››ä¸ªLIBEROä»»åŠ¡å¥—ä»¶ä¸­ï¼ŒGLaDå®ç°äº†94.1%çš„å¹³å‡æˆåŠŸç‡ï¼Œä¼˜äºä½¿ç”¨ç›¸åŒé¢„è®­ç»ƒæ•°æ®çš„UniVLA (92.5%)ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å‡ ä½•æ½œåœ¨è’¸é¦ï¼ŒGLaDèƒ½å¤Ÿæœ‰æ•ˆåœ°å¢å¼ºæ¨¡å‹çš„ç©ºé—´æ¨ç†å’Œç­–ç•¥æ³›åŒ–èƒ½åŠ›ã€‚è¯¥å®éªŒç»“æœéªŒè¯äº†å‡ ä½•æ„ŸçŸ¥é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºVLAæ¨¡å‹çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GLaDçš„ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººæ“ä½œã€è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚é€šè¿‡å¢å¼ºæ¨¡å‹å¯¹ç©ºé—´å‡ ä½•ä¿¡æ¯çš„ç†è§£ï¼Œå¯ä»¥æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼Œæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶ä¸ºARåº”ç”¨æä¾›æ›´çœŸå®çš„ç©ºé—´äº¤äº’ä½“éªŒã€‚è¯¥ç ”ç©¶çš„æœªæ¥å½±å“åœ¨äºæ¨åŠ¨VLAæ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´å¯é çš„äººæœºäº¤äº’ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.

