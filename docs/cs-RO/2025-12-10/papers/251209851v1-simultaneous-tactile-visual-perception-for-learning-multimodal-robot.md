---
layout: default
title: Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation
---

# Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation

**arXiv**: [2512.09851v1](https://arxiv.org/abs/2512.09851) | [PDF](https://arxiv.org/pdf/2512.09851.pdf)

**ä½œè€…**: Yuyang Li, Yinghan Chen, Zihang Zhao, Puhao Li, Tengyu Liu, Siyuan Huang, Yixin Zhu

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-10

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTacThru-UMIï¼Œç»“åˆæ–°åž‹è§¦è§‰è§†è§‰ä¼ æ„Ÿå™¨ä¸ŽTransformeræ‰©æ•£ç­–ç•¥ï¼Œæå‡æœºå™¨äººæ“ä½œç²¾åº¦ã€‚**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸Žæž¶æž„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§¦è§‰æ„ŸçŸ¥` `è§†è§‰æ„ŸçŸ¥` `å¤šæ¨¡æ€èžåˆ` `æ¨¡ä»¿å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰é€çš®è§†è§‰ä¼ æ„Ÿå™¨ç¼ºä¹åŒæ­¥å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ï¼Œè§¦è§‰è·Ÿè¸ªçš„å¯é æ€§ä¸è¶³ï¼Œé™åˆ¶äº†æœºå™¨äººæ“ä½œçš„ç²¾åº¦ã€‚
2. TacThru-UMIç»“åˆæ–°åž‹STSä¼ æ„Ÿå™¨TacThruå’ŒTransformeræ‰©æ•£ç­–ç•¥ï¼Œå®žçŽ°åŒæ­¥è§¦è§‰è§†è§‰æ„ŸçŸ¥å’Œç²¾ç¡®æ“ä½œã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒTacThru-UMIåœ¨å¤šä¸ªçœŸå®žæ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºŽä¼ ç»Ÿæ–¹æ³•ï¼Œå¹³å‡æˆåŠŸçŽ‡æå‡è‡³85.5%ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨äººæ“ä½œéœ€è¦ä¸°å¯Œçš„å¤šæ¨¡æ€æ„ŸçŸ¥å’Œæœ‰æ•ˆçš„å­¦ä¹ æ¡†æž¶æ¥å¤„ç†å¤æ‚çš„çŽ°å®žä¸–ç•Œä»»åŠ¡ã€‚é€çš®è§†è§‰ï¼ˆSTSï¼‰ä¼ æ„Ÿå™¨ç»“åˆäº†è§¦è§‰å’Œè§†è§‰æ„ŸçŸ¥ï¼Œæä¾›äº†æœ‰å‰æ™¯çš„ä¼ æ„Ÿèƒ½åŠ›ï¼Œè€ŒçŽ°ä»£æ¨¡ä»¿å­¦ä¹ ä¸ºç­–ç•¥èŽ·å–æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚ç„¶è€Œï¼ŒçŽ°æœ‰çš„STSè®¾è®¡ç¼ºä¹åŒæ­¥å¤šæ¨¡æ€æ„ŸçŸ¥ï¼Œå¹¶ä¸”å­˜åœ¨ä¸å¯é çš„è§¦è§‰è·Ÿè¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œå°†è¿™äº›ä¸°å¯Œçš„å¤šæ¨¡æ€ä¿¡å·é›†æˆåˆ°åŸºäºŽå­¦ä¹ çš„æ“ä½œæµç¨‹ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå…¬å¼€çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä»‹ç»äº†TacThruï¼Œä¸€ç§èƒ½å¤Ÿå®žçŽ°åŒæ­¥è§†è§‰æ„ŸçŸ¥å’Œé²æ£’è§¦è§‰ä¿¡å·æå–çš„STSä¼ æ„Ÿå™¨ï¼Œä»¥åŠTacThru-UMIï¼Œä¸€ç§åˆ©ç”¨è¿™äº›å¤šæ¨¡æ€ä¿¡å·è¿›è¡Œæ“ä½œçš„æ¨¡ä»¿å­¦ä¹ æ¡†æž¶ã€‚æˆ‘ä»¬çš„ä¼ æ„Ÿå™¨å…·æœ‰å®Œå…¨é€æ˜Žçš„å¼¹æ€§ä½“ã€æŒä¹…ç…§æ˜Žã€æ–°åž‹å…³é”®çº¿æ ‡è®°å’Œé«˜æ•ˆè·Ÿè¸ªï¼Œè€Œæˆ‘ä»¬çš„å­¦ä¹ ç³»ç»Ÿé€šè¿‡åŸºäºŽTransformerçš„æ‰©æ•£ç­–ç•¥é›†æˆè¿™äº›ä¿¡å·ã€‚åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çŽ°å®žä¸–ç•Œä»»åŠ¡ä¸­çš„å®žéªŒè¡¨æ˜Žï¼ŒTacThru-UMIå®žçŽ°äº†å¹³å‡85.5%çš„æˆåŠŸçŽ‡ï¼Œæ˜¾è‘—ä¼˜äºŽäº¤æ›¿è§¦è§‰è§†è§‰ï¼ˆ66.3%ï¼‰å’Œä»…è§†è§‰ï¼ˆ55.4%ï¼‰çš„åŸºçº¿ã€‚è¯¥ç³»ç»Ÿåœ¨å…³é”®åœºæ™¯ä¸­è¡¨çŽ°å‡ºè‰²ï¼ŒåŒ…æ‹¬è–„è€Œè½¯ç‰©ä½“çš„æŽ¥è§¦æ£€æµ‹ä»¥åŠéœ€è¦å¤šæ¨¡æ€åè°ƒçš„ç²¾ç¡®æ“ä½œã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜Žï¼Œå°†åŒæ­¥å¤šæ¨¡æ€æ„ŸçŸ¥ä¸ŽçŽ°ä»£å­¦ä¹ æ¡†æž¶ç›¸ç»“åˆï¼Œå¯ä»¥å®žçŽ°æ›´ç²¾ç¡®ã€æ›´å…·é€‚åº”æ€§çš„æœºå™¨äººæ“ä½œã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­ï¼Œç”±äºŽä¼ æ„Ÿå™¨æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³å’Œå­¦ä¹ æ¡†æž¶æ— æ³•æœ‰æ•ˆèžåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¯¼è‡´æ“ä½œç²¾åº¦å’Œé€‚åº”æ€§å—é™çš„é—®é¢˜ã€‚çŽ°æœ‰æ–¹æ³•ï¼Œå¦‚äº¤æ›¿ä½¿ç”¨è§¦è§‰å’Œè§†è§‰ä¿¡æ¯ï¼Œæˆ–ä»…ä¾èµ–è§†è§‰ä¿¡æ¯ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è§¦è§‰æä¾›çš„æŽ¥è§¦ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è–„ã€è½¯ç‰©ä½“æˆ–éœ€è¦ç²¾ç»†æ“ä½œçš„åœºæ™¯ä¸‹ï¼Œè¡¨çŽ°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡ä¸€ç§æ–°åž‹çš„é€çš®è§†è§‰ï¼ˆSTSï¼‰ä¼ æ„Ÿå™¨TacThruï¼Œèƒ½å¤ŸåŒæ—¶æä¾›é«˜è´¨é‡çš„è§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªåŸºäºŽTransformerçš„æ‰©æ•£ç­–ç•¥TacThru-UMIï¼Œå°†è¿™äº›å¤šæ¨¡æ€ä¿¡æ¯æœ‰æ•ˆåœ°èžåˆåˆ°æœºå™¨äººæ“ä½œçš„å­¦ä¹ è¿‡ç¨‹ä¸­ã€‚é€šè¿‡åŒæ­¥æ„ŸçŸ¥å’Œå¤šæ¨¡æ€èžåˆï¼Œæé«˜æœºå™¨äººå¯¹çŽ¯å¢ƒçš„ç†è§£å’Œæ“ä½œçš„ç²¾åº¦ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šTacThru-UMIçš„æ•´ä½“æ¡†æž¶åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šTacThruä¼ æ„Ÿå™¨å’ŒTransformeræ‰©æ•£ç­–ç•¥ã€‚TacThruä¼ æ„Ÿå™¨è´Ÿè´£é‡‡é›†åŒæ­¥çš„è§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬é€šè¿‡é€æ˜Žå¼¹æ€§ä½“èŽ·å–çš„è§†è§‰å›¾åƒå’Œé€šè¿‡å…³é”®çº¿æ ‡è®°è·Ÿè¸ªå¾—åˆ°çš„è§¦è§‰ä¿¡æ¯ã€‚è¿™äº›ä¿¡æ¯è¢«è¾“å…¥åˆ°Transformeræ‰©æ•£ç­–ç•¥ä¸­ï¼Œè¯¥ç­–ç•¥å­¦ä¹ ä»Žå¤šæ¨¡æ€æ•°æ®åˆ°æœºå™¨äººåŠ¨ä½œçš„æ˜ å°„ã€‚æ•´ä¸ªæµç¨‹åŒ…æ‹¬æ•°æ®é‡‡é›†ã€ä¼ æ„Ÿå™¨ä¿¡å·å¤„ç†ã€ç­–ç•¥å­¦ä¹ å’Œæœºå™¨äººæŽ§åˆ¶ç­‰é˜¶æ®µã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºŽTacThruä¼ æ„Ÿå™¨çš„è®¾è®¡å’ŒTacThru-UMIå­¦ä¹ æ¡†æž¶çš„æž„å»ºã€‚TacThruä¼ æ„Ÿå™¨é€šè¿‡å®Œå…¨é€æ˜Žçš„å¼¹æ€§ä½“ã€æŒä¹…ç…§æ˜Žå’Œæ–°åž‹å…³é”®çº¿æ ‡è®°ï¼Œå®žçŽ°äº†åŒæ­¥ã€é²æ£’çš„è§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥ã€‚TacThru-UMIå­¦ä¹ æ¡†æž¶åˆ™åˆ©ç”¨Transformerçš„å¼ºå¤§å»ºæ¨¡èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°èžåˆäº†è§†è§‰å’Œè§¦è§‰ä¿¡æ¯ï¼Œä»Žè€Œæé«˜äº†æœºå™¨äººæ“ä½œçš„ç²¾åº¦å’Œé€‚åº”æ€§ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼ŒTacThru-UMIèƒ½å¤ŸåŒæ—¶åˆ©ç”¨è§†è§‰å’Œè§¦è§‰ä¿¡æ¯è¿›è¡Œå†³ç­–ï¼Œè€Œä¸æ˜¯äº¤æ›¿ä½¿ç”¨æˆ–ä»…ä¾èµ–è§†è§‰ä¿¡æ¯ã€‚

**å…³é”®è®¾è®¡**ï¼šTacThruä¼ æ„Ÿå™¨é‡‡ç”¨å®Œå…¨é€æ˜Žçš„å¼¹æ€§ä½“ï¼Œä»¥å‡å°‘è§†è§‰é®æŒ¡ã€‚å…³é”®çº¿æ ‡è®°è¢«è®¾è®¡æˆæ˜“äºŽè·Ÿè¸ªå’ŒåŒºåˆ†çš„å½¢çŠ¶ï¼Œå¹¶ä½¿ç”¨é«˜æ•ˆçš„è·Ÿè¸ªç®—æ³•è¿›è¡Œå¤„ç†ã€‚Transformeræ‰©æ•£ç­–ç•¥ä½¿ç”¨Transformerç¼–ç å™¨æ¥æå–è§†è§‰å’Œè§¦è§‰ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡åž‹æ¥ç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ æŸå¤±å’Œæ­£åˆ™åŒ–é¡¹ï¼Œä»¥æé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“çš„ç½‘ç»œç»“æž„å’Œå‚æ•°è®¾ç½®åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒTacThru-UMIåœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çŽ°å®žä¸–ç•Œä»»åŠ¡ä¸­ï¼Œå¹³å‡æˆåŠŸçŽ‡è¾¾åˆ°85.5%ï¼Œæ˜¾è‘—ä¼˜äºŽäº¤æ›¿è§¦è§‰è§†è§‰ï¼ˆ66.3%ï¼‰å’Œä»…è§†è§‰ï¼ˆ55.4%ï¼‰çš„åŸºçº¿æ–¹æ³•ã€‚å°¤å…¶æ˜¯åœ¨å¤„ç†è–„è€Œè½¯çš„ç‰©ä½“ä»¥åŠéœ€è¦ç²¾ç¡®æ“ä½œçš„åœºæ™¯ä¸­ï¼ŒTacThru-UMIè¡¨çŽ°å‡ºæ˜Žæ˜¾çš„ä¼˜åŠ¿ï¼Œè¯æ˜Žäº†åŒæ­¥å¤šæ¨¡æ€æ„ŸçŸ¥å’Œå­¦ä¹ æ¡†æž¶çš„æœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæžœå¯åº”ç”¨äºŽå„ç§éœ€è¦ç²¾ç»†æ“ä½œå’ŒçŽ¯å¢ƒæ„ŸçŸ¥çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚åŒ»ç–—æ‰‹æœ¯æœºå™¨äººã€ç²¾å¯†è£…é…æœºå™¨äººã€ä»¥åŠåœ¨å¤æ‚çŽ¯å¢ƒä¸­è¿›è¡Œæ“ä½œçš„æœºå™¨äººã€‚é€šè¿‡æä¾›æ›´ç²¾ç¡®çš„æ„ŸçŸ¥å’Œæ›´æ™ºèƒ½çš„æŽ§åˆ¶ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æé«˜æœºå™¨äººæ“ä½œçš„æ•ˆçŽ‡å’Œå®‰å…¨æ€§ï¼Œå¹¶æ‰©å±•æœºå™¨äººçš„åº”ç”¨èŒƒå›´ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.

