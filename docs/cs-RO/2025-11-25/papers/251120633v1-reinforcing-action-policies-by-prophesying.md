---
layout: default
title: Reinforcing Action Policies by Prophesying
---

# Reinforcing Action Policies by Prophesying

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20633" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20633v1</a>
  <a href="https://arxiv.org/pdf/2511.20633.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20633v1" onclick="toggleFavorite(this, '2511.20633v1', 'Reinforcing Action Policies by Prophesying')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahui Zhang, Ze Huang, Chun Gu, Zipei Ma, Li Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

**å¤‡æ³¨**: https://LogosRoboticsGroup.github.io/ProphRL

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ProphRLï¼šé€šè¿‡é¢„æµ‹è¿›è¡Œè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æœºå™¨äººæ§åˆ¶æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œ` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `æœºå™¨äººæ§åˆ¶` `æ¨¡ä»¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAç­–ç•¥ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œæ˜“è¿‡æ‹Ÿåˆä¸”æ³›åŒ–æ€§å·®ï¼ŒçœŸå®æœºå™¨äººå¼ºåŒ–å­¦ä¹ æˆæœ¬é«˜ï¼Œä¼ ç»Ÿæ¨¡æ‹Ÿå™¨éš¾ä»¥è¿ç§»ã€‚
2. æå‡ºProphRLï¼Œåˆ©ç”¨é¢„è®­ç»ƒä¸–ç•Œæ¨¡å‹Prophetå­¦ä¹ åŠ¨ä½œ-ç»“æœåŠ¨æ€ï¼Œå¹¶ç»“åˆFA-GRPOå’ŒFlowScaleè¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒProphRLåœ¨å…¬å…±åŸºå‡†å’ŒçœŸå®æœºå™¨äººå®éªŒä¸­å‡æ˜¾è‘—æå‡äº†VLAç­–ç•¥çš„æˆåŠŸç‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥åœ¨å¯¹é½è¯­è¨€ã€æ„ŸçŸ¥å’Œæœºå™¨äººæ§åˆ¶æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°VLAç­–ç•¥ä»…é€šè¿‡æ¨¡ä»¿å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆæ¼”ç¤ºæ•°æ®ï¼Œå¹¶ä¸”åœ¨åˆ†å¸ƒåç§»ä¸‹è¡¨ç°è„†å¼±ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›´æ¥ä¼˜åŒ–ä»»åŠ¡å¥–åŠ±ï¼Œä»è€Œè§£å†³è¿™ç§ä¸ä¸€è‡´æ€§ï¼Œä½†çœŸå®æœºå™¨äººäº¤äº’æˆæœ¬é«˜æ˜‚ï¼Œä¸”ä¼ ç»Ÿæ¨¡æ‹Ÿå™¨éš¾ä»¥è®¾è®¡å’Œè¿ç§»ã€‚æˆ‘ä»¬é€šè¿‡å­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹å’Œä¸ºåŸºäºæµçš„åŠ¨ä½œå¤´å®šåˆ¶çš„RLè¿‡ç¨‹ï¼Œè§£å†³äº†VLAåè®­ç»ƒä¸­çš„æ•°æ®æ•ˆç‡å’Œä¼˜åŒ–ç¨³å®šæ€§é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†Prophetï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŠ¨ä½œåˆ°è§†é¢‘çš„æœºå™¨äººé©±åŠ¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒè·¨å¤§è§„æ¨¡å¼‚æ„æœºå™¨äººæ•°æ®å­¦ä¹ å¯é‡ç”¨çš„åŠ¨ä½œ-ç»“æœåŠ¨æ€ã€‚å®ƒèƒ½å¤Ÿå°‘é‡æ ·æœ¬é€‚åº”æ–°çš„æœºå™¨äººã€å¯¹è±¡å’Œç¯å¢ƒï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªå¯ç›´æ¥ç”¨äºrolloutçš„æ¨¡æ‹Ÿå™¨ã€‚åœ¨Prophetçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨Flow-action-GRPOï¼ˆFA-GRPOï¼‰å¼ºåŒ–åŠ¨ä½œç­–ç•¥ï¼Œè¯¥æ–¹æ³•ä½¿Flow-GRPOèƒ½å¤Ÿå¯¹VLAåŠ¨ä½œè¿›è¡Œæ“ä½œï¼Œå¹¶ä½¿ç”¨FlowScaleï¼Œä¸€ç§é€æ­¥é‡æ–°åŠ æƒçš„æ–¹æ³•ï¼Œå¯ä»¥é‡æ–°è°ƒæ•´æµå¤´ä¸­æ¯ä¸€æ­¥çš„æ¢¯åº¦ã€‚Prophetã€FA-GRPOå’ŒFlowScaleå…±åŒæ„æˆäº†ProphRLï¼Œè¿™æ˜¯ä¸€ç§å®ç”¨ã€æ•°æ®å’Œè®¡ç®—é«˜æ•ˆçš„VLAåè®­ç»ƒæ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­æˆåŠŸç‡æé«˜äº†5-17%ï¼Œåœ¨ä¸åŒVLAå˜ä½“ä¸Šçš„çœŸå®æœºå™¨äººå®éªŒä¸­æˆåŠŸç‡æé«˜äº†24-30%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥ä¸»è¦ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´åœ¨çœŸå®ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚ç›´æ¥åœ¨çœŸå®æœºå™¨äººä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ æˆæœ¬é«˜æ˜‚ï¼Œè€Œä¼ ç»Ÿæ¨¡æ‹Ÿå™¨éš¾ä»¥å‡†ç¡®æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ï¼Œå¯¼è‡´ç­–ç•¥è¿ç§»å›°éš¾ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆã€ç¨³å®šåœ°å¯¹VLAç­–ç•¥è¿›è¡Œåè®­ç»ƒï¼Œæå‡å…¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ€§èƒ½ï¼Œæ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šProphRLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ä¸–ç•Œæ¨¡å‹ï¼ˆProphetï¼‰æ¥å­¦ä¹ æœºå™¨äººåŠ¨ä½œä¸ç¯å¢ƒå˜åŒ–ä¹‹é—´çš„åŠ¨æ€å…³ç³»ï¼Œä»è€Œæ„å»ºä¸€ä¸ªå¯ç”¨äºå¼ºåŒ–å­¦ä¹ çš„æ¨¡æ‹Ÿç¯å¢ƒã€‚é€šè¿‡åœ¨è¿™ä¸ªæ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥é¿å…ç›´æ¥åœ¨çœŸå®æœºå™¨äººä¸Šè¿›è¡Œæ˜‚è´µçš„äº¤äº’ï¼Œå¹¶æé«˜æ•°æ®æ•ˆç‡ã€‚åŒæ—¶ï¼Œé’ˆå¯¹VLAç­–ç•¥çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†FA-GRPOå’ŒFlowScaleï¼Œä»¥ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šProphRLä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼š1) Prophetï¼šä¸€ä¸ªé¢„è®­ç»ƒçš„åŠ¨ä½œåˆ°è§†é¢‘çš„æœºå™¨äººé©±åŠ¨æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ åŠ¨ä½œ-ç»“æœåŠ¨æ€ã€‚2) FA-GRPOï¼šå°†Flow-GRPOç®—æ³•é€‚é…åˆ°VLAåŠ¨ä½œç©ºé—´ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚3) FlowScaleï¼šä¸€ç§é€æ­¥é‡æ–°åŠ æƒçš„æ–¹æ³•ï¼Œç”¨äºè°ƒæ•´æµå¤´ä¸­çš„æ¢¯åº¦ï¼Œæé«˜ä¼˜åŒ–ç¨³å®šæ€§ã€‚æ•´ä½“æµç¨‹æ˜¯ï¼šé¦–å…ˆä½¿ç”¨Prophetæ„å»ºæ¨¡æ‹Ÿç¯å¢ƒï¼Œç„¶ååœ¨è¯¥ç¯å¢ƒä¸­åˆ©ç”¨FA-GRPOå’ŒFlowScaleè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªä¼˜åŒ–åçš„VLAç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šProphRLçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†Prophetï¼Œä¸€ä¸ªå¯ä»¥å­¦ä¹ åŠ¨ä½œ-ç»“æœåŠ¨æ€çš„é¢„è®­ç»ƒä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿå°‘é‡æ ·æœ¬é€‚åº”æ–°çš„æœºå™¨äººã€å¯¹è±¡å’Œç¯å¢ƒã€‚2) è®¾è®¡äº†FA-GRPOï¼Œå°†Flow-GRPOç®—æ³•é€‚é…åˆ°VLAåŠ¨ä½œç©ºé—´ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚3) æå‡ºäº†FlowScaleï¼Œä¸€ç§é€æ­¥é‡æ–°åŠ æƒçš„æ–¹æ³•ï¼Œç”¨äºè°ƒæ•´æµå¤´ä¸­çš„æ¢¯åº¦ï¼Œæé«˜ä¼˜åŒ–ç¨³å®šæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒProphRLèƒ½å¤Ÿæ›´é«˜æ•ˆã€ç¨³å®šåœ°å¯¹VLAç­–ç•¥è¿›è¡Œåè®­ç»ƒã€‚

**å…³é”®è®¾è®¡**ï¼šProphetä½¿ç”¨å¤§è§„æ¨¡å¼‚æ„æœºå™¨äººæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ åŠ¨ä½œä¸è§†é¢‘å¸§ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚FA-GRPOåˆ©ç”¨Flow-GRPOçš„ä¼˜åŠ¿ï¼Œç»“åˆVLAç­–ç•¥çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†ç‰¹å®šçš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ã€‚FlowScaleé€šè¿‡é€æ­¥è°ƒæ•´æµå¤´ä¸­æ¯ä¸€æ­¥çš„æ¢¯åº¦æƒé‡ï¼Œæ¥å¹³è¡¡ä¸åŒæ—¶é—´æ­¥çš„å½±å“ï¼Œæé«˜ä¼˜åŒ–ç¨³å®šæ€§ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒProphRLåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­æˆåŠŸç‡æé«˜äº†5-17%ï¼Œåœ¨ä¸åŒVLAå˜ä½“ä¸Šçš„çœŸå®æœºå™¨äººå®éªŒä¸­æˆåŠŸç‡æé«˜äº†24-30%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒProphRLèƒ½å¤Ÿæ˜¾è‘—æå‡VLAç­–ç•¥çš„æ€§èƒ½ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ProphRLå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæå‡å„ç§æœºå™¨äººä»»åŠ¡çš„æ€§èƒ½ï¼Œä¾‹å¦‚ç‰©ä½“æŠ“å–ã€è£…é…ã€å¯¼èˆªç­‰ã€‚è¯¥æ–¹æ³•å¯ä»¥é™ä½æœºå™¨äººå¼ºåŒ–å­¦ä¹ çš„æˆæœ¬ï¼ŒåŠ é€Ÿæœºå™¨äººæŠ€æœ¯çš„è½åœ°åº”ç”¨ã€‚æ­¤å¤–ï¼ŒProphRLè¿˜å¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆç­‰é¢†åŸŸï¼Œç”¨äºç”Ÿæˆæ›´é€¼çœŸçš„æœºå™¨äººè¡Œä¸ºã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.

