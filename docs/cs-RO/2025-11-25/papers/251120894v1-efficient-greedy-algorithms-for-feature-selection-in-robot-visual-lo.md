---
layout: default
title: Efficient Greedy Algorithms for Feature Selection in Robot Visual Localization
---

# Efficient Greedy Algorithms for Feature Selection in Robot Visual Localization

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20894" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20894v1</a>
  <a href="https://arxiv.org/pdf/2511.20894.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20894v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.20894v1', 'Efficient Greedy Algorithms for Feature Selection in Robot Visual Localization')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Vivek Pandey, Amirhossein Mollaei, Nader Motee

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé«˜æ•ˆè´ªå©ªç®—æ³•ï¼ŒåŠ é€Ÿæœºå™¨äººè§†è§‰å®šä½ä¸­çš„ç‰¹å¾é€‰æ‹©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœºå™¨äººå®šä½` `è§†è§‰ç‰¹å¾é€‰æ‹©` `è´ªå©ªç®—æ³•` `è‡ªä¸»å¯¼èˆª` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰å®šä½æ–¹æ³•å¤„ç†å¤§é‡å†—ä½™ç‰¹å¾ï¼Œå¯¼è‡´è®¡ç®—å»¶è¿Ÿå’Œæ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§éœ€æ±‚ã€‚
2. è®ºæ–‡æå‡ºä¸¤ç§å¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„è´ªå©ªç‰¹å¾é€‰æ‹©ç®—æ³•ï¼Œæ—¨åœ¨å®æ—¶è¯„ä¼°è§†è§‰ç‰¹å¾å¯¹å®šä½çš„æ•ˆç”¨ã€‚
3. è¯¥æ–¹æ³•åœ¨é™ä½è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦çš„åŒæ—¶ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡å’Œå®šä½ç²¾åº¦ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœºå™¨äººå®šä½æ˜¯æœªçŸ¥ç¯å¢ƒä¸­è‡ªä¸»å¯¼èˆªçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚åœ¨å„ç§ä¼ æ„Ÿæ–¹å¼ä¸­ï¼Œæ¥è‡ªç›¸æœºçš„è§†è§‰è¾“å…¥èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿé€šè¿‡è·Ÿè¸ªå›¾åƒå¸§ä¸­çš„ç‚¹ç‰¹å¾æ¥ä¼°è®¡å…¶ä½ç½®ã€‚ç„¶è€Œï¼Œå›¾åƒå¸§é€šå¸¸åŒ…å«å¤§é‡ç‰¹å¾ï¼Œå…¶ä¸­è®¸å¤šç‰¹å¾å¯¹äºå®šä½è€Œè¨€æ˜¯å†—ä½™æˆ–æ— ä¿¡æ¯çš„ã€‚å¤„ç†æ‰€æœ‰ç‰¹å¾ä¼šå¼•å…¥æ˜¾è‘—çš„è®¡ç®—å»¶è¿Ÿå’Œä½æ•ˆç‡ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬éœ€è¦æ™ºèƒ½ç‰¹å¾é€‰æ‹©ï¼Œè¯†åˆ«åœ¨é¢„æµ‹èŒƒå›´å†…å¯¹å®šä½æœ€æœ‰ç”¨çš„ç‰¹å¾å­é›†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„ç‰¹å¾é€‰æ‹©ç®—æ³•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿå®æ—¶ä¸»åŠ¨è¯„ä¼°è§†è§‰ç‰¹å¾çš„æ•ˆç”¨ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨é™ä½æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦çš„åŒæ—¶ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œå®šä½ç²¾åº¦ä¹‹é—´å®ç°äº†è‰¯å¥½çš„æƒè¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœºå™¨äººè§†è§‰å®šä½ä¸­ï¼Œå¦‚ä½•ä»å¤§é‡å›¾åƒç‰¹å¾ä¸­é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„ç‰¹å¾å­é›†ï¼Œä»¥åœ¨ä¿è¯å®šä½ç²¾åº¦çš„å‰æä¸‹ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜å ç”¨ï¼Œå®ç°å®æ—¶å®šä½ï¼Ÿç°æœ‰æ–¹æ³•é€šå¸¸è®¡ç®—é‡å¤§ï¼Œå†…å­˜éœ€æ±‚é«˜ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯è®¾è®¡é«˜æ•ˆçš„è´ªå©ªç®—æ³•ï¼Œé€šè¿‡è¿­ä»£åœ°é€‰æ‹©å¯¹å®šä½ç²¾åº¦è´¡çŒ®æœ€å¤§çš„ç‰¹å¾ï¼Œé€æ­¥æ„å»ºæœ€ä¼˜ç‰¹å¾å­é›†ã€‚è´ªå©ªç®—æ³•çš„ä¼˜åŠ¿åœ¨äºå…¶è®¡ç®—å¤æ‚åº¦è¾ƒä½ï¼Œæ˜“äºå®ç°ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨å¯æ¥å—çš„æ—¶é—´å†…æ‰¾åˆ°è¿‘ä¼¼æœ€ä¼˜è§£ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºçš„ç‰¹å¾é€‰æ‹©ç®—æ³•é€šå¸¸åµŒå…¥åˆ°è§†è§‰å®šä½ç³»ç»Ÿçš„å‰ç«¯ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) æå–å›¾åƒç‰¹å¾ï¼›2) ä½¿ç”¨è´ªå©ªç®—æ³•é€‰æ‹©ç‰¹å¾å­é›†ï¼›3) åˆ©ç”¨é€‰å®šçš„ç‰¹å¾è¿›è¡Œä½å§¿ä¼°è®¡ï¼›4) æ ¹æ®å®šä½ç»“æœè¯„ä¼°ç‰¹å¾çš„æ•ˆç”¨ï¼Œå¹¶æ›´æ–°ç‰¹å¾å­é›†ã€‚è¯¥æ¡†æ¶å…è®¸æœºå™¨äººå®æ—¶è¯„ä¼°å’Œé€‰æ‹©å¯¹å®šä½æœ€æœ‰ç”¨çš„è§†è§‰ç‰¹å¾ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºè®¾è®¡äº†ä¸¤ç§å¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„è´ªå©ªç‰¹å¾é€‰æ‹©ç®—æ³•ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›ç®—æ³•åœ¨è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜å ç”¨æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„æœºå™¨äººå¹³å°ä¸Šå®ç°å®æ—¶ç‰¹å¾é€‰æ‹©ã€‚ç®—æ³•è®¾è®¡çš„æ ¸å¿ƒåœ¨äºé™ä½æ¯æ¬¡è¿­ä»£çš„è®¡ç®—é‡ï¼Œå¹¶æœ‰æ•ˆåœ°ç®¡ç†å†…å­˜ä½¿ç”¨ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­å¯èƒ½æ¶‰åŠçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ç‰¹å¾æ•ˆç”¨è¯„ä¼°æŒ‡æ ‡ï¼šå¦‚ä½•é‡åŒ–æ¯ä¸ªç‰¹å¾å¯¹å®šä½ç²¾åº¦çš„è´¡çŒ®ï¼Ÿä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ä¿¡æ¯å¢ç›Šã€æ–¹å·®ç­‰æŒ‡æ ‡ã€‚2) è´ªå©ªé€‰æ‹©ç­–ç•¥ï¼šå¦‚ä½•é€‰æ‹©ä¸‹ä¸€ä¸ªè¦åŠ å…¥ç‰¹å¾å­é›†çš„ç‰¹å¾ï¼Ÿä¾‹å¦‚ï¼Œå¯ä»¥é€‰æ‹©æ•ˆç”¨æœ€é«˜çš„ç‰¹å¾ã€‚3) åœæ­¢å‡†åˆ™ï¼šä½•æ—¶åœæ­¢ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ï¼Ÿä¾‹å¦‚ï¼Œå¯ä»¥è®¾ç½®æœ€å¤§ç‰¹å¾æ•°é‡æˆ–å®šä½ç²¾åº¦é˜ˆå€¼ã€‚4) å†…å­˜ç®¡ç†ç­–ç•¥ï¼šå¦‚ä½•æœ‰æ•ˆåœ°ç®¡ç†ç‰¹å¾æ•°æ®ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼Ÿå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚æœªçŸ¥ï¼Œéœ€è¦æŸ¥é˜…è®ºæ–‡å…¨æ–‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

æ‘˜è¦ä¸­æåˆ°ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦çš„åŒæ—¶ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œå®šä½ç²¾åº¦ä¹‹é—´å®ç°äº†è‰¯å¥½çš„æƒè¡¡ã€‚å…·ä½“çš„å®éªŒç»“æœï¼ˆä¾‹å¦‚ï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„å®šä½ç²¾åº¦æå‡ã€è®¡ç®—æ—¶é—´ç¼©çŸ­ã€å†…å­˜å ç”¨é™ä½ç­‰ï¼‰æœªçŸ¥ï¼Œéœ€è¦æŸ¥é˜…è®ºæ–‡å…¨æ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºå„ç§éœ€è¦è‡ªä¸»å¯¼èˆªçš„æœºå™¨äººåº”ç”¨ä¸­ï¼Œä¾‹å¦‚ï¼šæ— äººé©¾é©¶æ±½è½¦ã€æ— äººæœºã€æœåŠ¡æœºå™¨äººå’Œå·¥ä¸šæœºå™¨äººã€‚é€šè¿‡é™ä½è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜å ç”¨ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æœºå™¨äººåœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„å®šä½ç²¾åº¦å’Œå®æ—¶æ€§ï¼Œä»è€Œæå‡æœºå™¨äººçš„æ•´ä½“æ€§èƒ½å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸï¼Œæé«˜å®šä½è¿½è¸ªçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Robot localization is a fundamental component of autonomous navigation in unknown environments. Among various sensing modalities, visual input from cameras plays a central role, enabling robots to estimate their position by tracking point features across image frames. However, image frames often contain a large number of features, many of which are redundant or uninformative for localization. Processing all features can introduce significant computational latency and inefficiency. This motivates the need for intelligent feature selection, identifying a subset of features that are most informative for localization over a prediction horizon. In this work, we propose two fast and memory-efficient feature selection algorithms that enable robots to actively evaluate the utility of visual features in real time. Unlike existing approaches with high computational and memory demands, the proposed methods are explicitly designed to reduce both time and memory complexity while achieving a favorable trade-off between computational efficiency and localization accuracy.

