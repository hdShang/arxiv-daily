---
layout: default
title: "OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping"
---

# OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.20841" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.20841v1</a>
  <a href="https://arxiv.org/pdf/2511.20841.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.20841v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.20841v1', 'OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Edmond Tong, Advaith Balaji, Anthony Opipari, Stanley Lewis, Zhen Zeng, Odest Chadwicke Jenkins

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

**å¤‡æ³¨**: 10 pages, 7 figures, 3 tables. Presented at the 2025 International Symposium on Experimental Robotics (ISER)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://ekjt.github.io/OVAL-Grasp/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**OVAL-Graspï¼šé¢å‘ä»»åŠ¡çš„å¼€æ”¾è¯æ±‡æŠ“å–æ–¹æ³•ï¼Œæå‡æœºå™¨äººæ“ä½œçµæ´»æ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æœºå™¨äººæŠ“å–` `å¼€æ”¾è¯æ±‡` `å¯ä¾›æ€§` `è§†è§‰è¯­è¨€æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»»åŠ¡å¯¼å‘` `é›¶æ ·æœ¬å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºäºå‡ ä½•çš„æŠ“å–æ–¹æ³•éš¾ä»¥å¤„ç†è§†è§‰å®šä¹‰çš„ç‰©ä½“éƒ¨ä»¶ã€é®æŒ¡å’Œæœªè§è¿‡çš„ç‰©ä½“ï¼Œé™åˆ¶äº†æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æ“ä½œçš„çµæ´»æ€§ã€‚
2. OVAL-Graspåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå®ç°é›¶æ ·æœ¬çš„ã€é¢å‘ä»»åŠ¡çš„æŠ“å–ï¼Œä½¿æœºå™¨äººèƒ½æ ¹æ®ä»»åŠ¡æŠ“å–ç‰©ä½“çš„ç‰¹å®šéƒ¨ä½ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒOVAL-Graspåœ¨çœŸå®åœºæ™¯ä¸­èƒ½æœ‰æ•ˆè¯†åˆ«å’ŒæŠ“å–ç›®æ ‡ç‰©ä½“éƒ¨ä½ï¼Œä¸”åœ¨é®æŒ¡æƒ…å†µä¸‹ä»å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ï¼Œä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºOVAL-Graspçš„é›¶æ ·æœ¬å¼€æ”¾è¯æ±‡æ–¹æ³•ï¼Œç”¨äºå®ç°é¢å‘ä»»åŠ¡çš„ã€åŸºäºå¯ä¾›æ€§çš„æŠ“å–ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®ç»™å®šçš„ä»»åŠ¡åœ¨ç‰©ä½“çš„æ­£ç¡®éƒ¨ä½è¿›è¡ŒæŠ“å–ã€‚ç»™å®šRGBå›¾åƒå’Œä»»åŠ¡æè¿°ï¼ŒOVAL-Graspé¦–å…ˆä½¿ç”¨LLMè¯†åˆ«éœ€è¦æŠ“å–æˆ–é¿å…æŠ“å–çš„ç‰©ä½“éƒ¨ä½ï¼Œç„¶åä½¿ç”¨VLMåˆ†å‰²è¿™äº›éƒ¨ä½ï¼Œå¹¶ç”Ÿæˆç‰©ä½“ä¸Šå¯æ“ä½œåŒºåŸŸçš„2Dçƒ­å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨20ä¸ªå®¶ç”¨ç‰©å“å’Œæ¯ä¸ªç‰©å“3ä¸ªç‹¬ç‰¹ä»»åŠ¡çš„å®éªŒä¸­ï¼Œä¼˜äºä¸¤ç§é¢å‘ä»»åŠ¡çš„æŠ“å–åŸºçº¿ã€‚åœ¨çœŸå®ä¸–ç•Œçš„å®éªŒä¸­ï¼ŒOVAL-GraspæˆåŠŸè¯†åˆ«å’Œåˆ†å‰²æ­£ç¡®ç‰©ä½“éƒ¨ä½çš„æ¦‚ç‡ä¸º95%ï¼ŒæŠ“å–æ­£ç¡®å¯æ“ä½œåŒºåŸŸçš„æ¦‚ç‡ä¸º78.3%ã€‚æ­¤å¤–ï¼ŒOVAL-Graspåœ¨éƒ¨åˆ†é®æŒ¡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æ‰¾åˆ°æ­£ç¡®çš„ç‰©ä½“éƒ¨ä½ï¼Œåœ¨æ‚ä¹±åœºæ™¯ä¸­çš„éƒ¨ä½é€‰æ‹©æˆåŠŸç‡ä¸º80%ã€‚è®ºæ–‡è¿˜å±•ç¤ºäº†OVAL-Graspåœ¨ä¾èµ–è§†è§‰ç‰¹å¾è¿›è¡Œéƒ¨ä½é€‰æ‹©çš„åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒè¯æ˜äº†æ¨¡å—åŒ–è®¾è®¡çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºå‡ ä½•ä¿¡æ¯çš„æŠ“å–æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å¤æ‚è§†è§‰ç‰¹å¾çš„ç‰©ä½“éƒ¨ä»¶æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨é®æŒ¡æˆ–é‡åˆ°æœªè§è¿‡çš„ç‰©ä½“æ—¶ã€‚è¿™äº›æ–¹æ³•éš¾ä»¥ç†è§£ä»»åŠ¡éœ€æ±‚ï¼Œæ— æ³•æ ¹æ®ä»»åŠ¡ç›®æ ‡é€‰æ‹©åˆé€‚çš„æŠ“å–éƒ¨ä½ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿç†è§£ä»»åŠ¡è¯­ä¹‰å¹¶å®šä½ç‰©ä½“ä¸Šå¯æŠ“å–åŒºåŸŸçš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOVAL-Graspçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç†è§£ä»»åŠ¡æè¿°ï¼Œå¹¶ç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å®šä½ç‰©ä½“ä¸Šä¸ä»»åŠ¡ç›¸å…³çš„å¯æŠ“å–åŒºåŸŸã€‚é€šè¿‡å°†è¯­è¨€ç†è§£å’Œè§†è§‰æ„ŸçŸ¥ç›¸ç»“åˆï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬çš„ã€é¢å‘ä»»åŠ¡çš„æŠ“å–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOVAL-Graspçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) ä»»åŠ¡ç†è§£æ¨¡å—ï¼šä½¿ç”¨LLMè§£æä»»åŠ¡æè¿°ï¼Œè¯†åˆ«éœ€è¦æŠ“å–æˆ–é¿å…æŠ“å–çš„ç‰©ä½“éƒ¨ä½ã€‚2) éƒ¨ä½åˆ†å‰²æ¨¡å—ï¼šä½¿ç”¨VLMåˆ†å‰²å›¾åƒä¸­è¯†åˆ«å‡ºçš„ç‰©ä½“éƒ¨ä½ã€‚3) çƒ­å›¾ç”Ÿæˆæ¨¡å—ï¼šæ ¹æ®åˆ†å‰²ç»“æœç”Ÿæˆ2Dçƒ­å›¾ï¼ŒæŒ‡ç¤ºç‰©ä½“ä¸Šå¯æ“ä½œåŒºåŸŸã€‚4) æŠ“å–æ‰§è¡Œæ¨¡å—ï¼šæ ¹æ®çƒ­å›¾é€‰æ‹©æœ€ä½³æŠ“å–ç‚¹ï¼Œå¹¶æ§åˆ¶æœºå™¨äººæ‰§è¡ŒæŠ“å–åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šOVAL-Graspçš„å…³é”®åˆ›æ–°åœ¨äºå°†LLMå’ŒVLMç›¸ç»“åˆï¼Œå®ç°é›¶æ ·æœ¬çš„ã€é¢å‘ä»»åŠ¡çš„æŠ“å–ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå‡ ä½•ä¿¡æ¯çš„æŠ“å–æ–¹æ³•ç›¸æ¯”ï¼ŒOVAL-Graspèƒ½å¤Ÿç†è§£ä»»åŠ¡è¯­ä¹‰ï¼Œå¹¶æ ¹æ®ä»»åŠ¡ç›®æ ‡é€‰æ‹©åˆé€‚çš„æŠ“å–éƒ¨ä½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†æœªè§è¿‡çš„ç‰©ä½“å’Œå¤æ‚çš„åœºæ™¯ã€‚

**å…³é”®è®¾è®¡**ï¼šOVAL-Graspçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„LLMï¼ˆä¾‹å¦‚GPT-3ï¼‰è¿›è¡Œä»»åŠ¡ç†è§£ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚2) ä½¿ç”¨é¢„è®­ç»ƒçš„VLMï¼ˆä¾‹å¦‚CLIPï¼‰è¿›è¡Œéƒ¨ä½åˆ†å‰²ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„è§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚3) è®¾è®¡äº†ä¸€ç§åŸºäºçƒ­å›¾çš„æŠ“å–ç‚¹é€‰æ‹©ç­–ç•¥ï¼Œè€ƒè™‘äº†æŠ“å–ç‚¹çš„å¯è¾¾æ€§å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

OVAL-Graspåœ¨çœŸå®ä¸–ç•Œçš„å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸè¯†åˆ«å’Œåˆ†å‰²æ­£ç¡®ç‰©ä½“éƒ¨ä½çš„æ¦‚ç‡ä¸º95%ï¼ŒæŠ“å–æ­£ç¡®å¯æ“ä½œåŒºåŸŸçš„æ¦‚ç‡ä¸º78.3%ã€‚åœ¨æ‚ä¹±åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨éƒ¨åˆ†é®æŒ¡çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒ80%çš„éƒ¨ä½é€‰æ‹©æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒOVAL-Graspåœ¨ä¸ä¸¤ç§é¢å‘ä»»åŠ¡çš„æŠ“å–åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”å®éªŒä¸­ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

OVAL-Graspå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—è¾…åŠ©ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨å®¶åº­ç¯å¢ƒä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·çš„æŒ‡ä»¤æŠ“å–ç‰¹å®šçš„ç‰©å“ï¼Œå¹¶æ”¾ç½®åˆ°æŒ‡å®šçš„ä½ç½®ã€‚åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ä¸­ï¼Œæœºå™¨äººå¯ä»¥æ ¹æ®ç”Ÿäº§ä»»åŠ¡æŠ“å–ä¸åŒçš„é›¶ä»¶ï¼Œå¹¶è¿›è¡Œç»„è£…ã€‚åœ¨åŒ»ç–—è¾…åŠ©ä¸­ï¼Œæœºå™¨äººå¯ä»¥å¸®åŠ©åŒ»ç”ŸæŠ“å–æ‰‹æœ¯å™¨æ¢°ï¼Œæé«˜æ‰‹æœ¯æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To manipulate objects in novel, unstructured environments, robots need task-oriented grasps that target object parts based on the given task. Geometry-based methods often struggle with visually defined parts, occlusions, and unseen objects. We introduce OVAL-Grasp, a zero-shot open-vocabulary approach to task-oriented, affordance based grasping that uses large-language models and vision-language models to allow a robot to grasp objects at the correct part according to a given task. Given an RGB image and a task, OVAL-Grasp identifies parts to grasp or avoid with an LLM, segments them with a VLM, and generates a 2D heatmap of actionable regions on the object. During our evaluations, we found that our method outperformed two task oriented grasping baselines on experiments with 20 household objects with 3 unique tasks for each. OVAL-Grasp successfully identifies and segments the correct object part 95% of the time and grasps the correct actionable area 78.3% of the time in real-world experiments with the Fetch mobile manipulator. Additionally, OVAL-Grasp finds correct object parts under partial occlusions, demonstrating a part selection success rate of 80% in cluttered scenes. We also demonstrate OVAL-Grasp's efficacy in scenarios that rely on visual features for part selection, and show the benefit of a modular design through our ablation experiments. Our project webpage is available at https://ekjt.github.io/OVAL-Grasp/

