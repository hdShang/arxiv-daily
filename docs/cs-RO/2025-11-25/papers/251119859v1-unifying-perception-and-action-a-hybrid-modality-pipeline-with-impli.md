---
layout: default
title: "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation"
---

# Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.19859" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.19859v1</a>
  <a href="https://arxiv.org/pdf/2511.19859.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.19859v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.19859v1', 'Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiangkai Ma, Lekai Xing, Han Zhang, Wenzhong Li, Sanglu Lu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVITAæ¡†æ¶ï¼Œé€šè¿‡éšå¼è§†è§‰CoTç»Ÿä¸€æ„ŸçŸ¥ä¸åŠ¨ä½œï¼Œæå‡æœºå™¨äººåŠ¨ä½œç”Ÿæˆèƒ½åŠ›ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `é“¾å¼æ€è€ƒ` `éšå¼è§†è§‰æ¨ç†` `è½¨è¿¹å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹åœ¨å¤æ‚ç©ºé—´ç¯å¢ƒä¸­éš¾ä»¥å……åˆ†æ•æ‰åœºæ™¯ç»†èŠ‚ï¼Œæ–‡æœ¬CoTå­˜åœ¨å±€é™æ€§ï¼Œè§†è§‰å…ˆéªŒåˆ©ç”¨ä¸è¶³ã€‚
2. VITAæ¡†æ¶é€šè¿‡å­¦ä¹ è§†è§‰å’ŒåŠ¨ä½œçš„å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œå¹¶å¼•å…¥éšå¼è§†è§‰CoTï¼Œå®ç°æ„ŸçŸ¥å’ŒåŠ¨ä½œçš„ç»Ÿä¸€å»ºæ¨¡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVITAåœ¨å¤šä¸ªbenchmarkä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰é›†æˆè½¨è¿¹å¯¹é½ï¼ˆVITAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¸­è§†è§‰ä¿¡æ¯åˆ©ç”¨ä¸è¶³å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚VITAé€šè¿‡å­¦ä¹ è§†è§‰å’ŒåŠ¨ä½œçš„å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œå®ç°æ„ŸçŸ¥å’Œè¿åŠ¨æ§åˆ¶çš„è”åˆå»ºæ¨¡ã€‚è¯¥æ¡†æ¶å¼•å…¥éšå¼è§†è§‰CoTï¼Œè‡ªå›å½’åœ°ç”Ÿæˆtokenï¼Œå¹¶åŒæ—¶è§£ç ä¸ºæœªæ¥å¸§é¢„æµ‹å’Œæœºå™¨äººåŠ¨ä½œï¼Œä»è€Œå°†è§†è§‰åŠ¨æ€ä½œä¸ºè¿åŠ¨è§„åˆ’çš„å½’çº³åç½®ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVITAå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨CALVINã€LIBEROå’ŒSimplerEnvä¸Šåˆ†åˆ«æ¯”ç°æœ‰åŸºçº¿æé«˜äº†14.5%ã€9.6%å’Œ12.1%ã€‚æ­¤å¤–ï¼ŒVITAåœ¨å…­ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å®ç°äº†å¹³å‡80.5%çš„æˆåŠŸç‡ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºé€šç”¨æœºå™¨äººæ“ä½œæ¨¡å‹çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æœºå™¨äººæ“ä½œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä¾èµ–äºChain-of-Thought (CoT) çš„æ–¹æ³•ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­éš¾ä»¥å……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚æ–‡æœ¬CoTéš¾ä»¥æ•æ‰ç»†è‡´çš„åœºæ™¯ä¿¡æ¯ï¼Œè€Œç›´æ¥å°†è§†è§‰ä¿¡æ¯èå…¥åŠ¨ä½œç”Ÿæˆåˆé¢ä¸´æ¨¡æ€å·®å¼‚å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œå³è§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆçš„ç›®æ ‡ç›¸äº’ç«äº‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šVITAçš„æ ¸å¿ƒæ€è·¯æ˜¯å­¦ä¹ ä¸€ä¸ªè§†è§‰å’ŒåŠ¨ä½œçš„å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå°†è§†è§‰ä¿¡æ¯æœ‰æ•ˆåœ°èå…¥åˆ°åŠ¨ä½œç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚é€šè¿‡éšå¼è§†è§‰CoTï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªå›å½’åœ°ç”Ÿæˆtokenï¼Œè¿™äº›tokenæ—¢ç”¨äºé¢„æµ‹æœªæ¥çš„è§†è§‰å¸§ï¼Œåˆç”¨äºç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚è¿™ç§è®¾è®¡å°†è§†è§‰åŠ¨æ€ä½œä¸ºè¿åŠ¨è§„åˆ’çš„å½’çº³åç½®ï¼Œä»è€Œæé«˜äº†åŠ¨ä½œç”Ÿæˆçš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVITAæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) è§†è§‰ç¼–ç å™¨ï¼Œç”¨äºæå–è§†è§‰ç‰¹å¾ï¼›2) åŠ¨ä½œç¼–ç å™¨ï¼Œç”¨äºç¼–ç åŠ¨ä½œåºåˆ—ï¼›3) å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼Œç”¨äºå¯¹è§†è§‰å’ŒåŠ¨ä½œä¿¡æ¯è¿›è¡Œç»Ÿä¸€è¡¨ç¤ºï¼›4) è‡ªå›å½’è§£ç å™¨ï¼Œç”¨äºç”Ÿæˆéšå¼è§†è§‰CoT tokenï¼Œå¹¶å°†å…¶è§£ç ä¸ºæœªæ¥å¸§é¢„æµ‹å’Œæœºå™¨äººåŠ¨ä½œã€‚æ•´ä¸ªæµç¨‹æ˜¯ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ã€‚

**å…³é”®åˆ›æ–°**ï¼šVITAæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºéšå¼è§†è§‰CoTçš„å¼•å…¥ã€‚ä¸æ˜¾å¼åœ°ç”Ÿæˆæ–‡æœ¬å½¢å¼çš„CoTä¸åŒï¼ŒVITAé€šè¿‡è‡ªå›å½’åœ°ç”Ÿæˆtokenï¼Œå¹¶å°†è¿™äº›tokenåŒæ—¶ç”¨äºè§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆï¼Œä»è€Œå®ç°äº†è§†è§‰ä¿¡æ¯å’ŒåŠ¨ä½œç”Ÿæˆçš„ç´§å¯†è€¦åˆã€‚è¿™ç§éšå¼çš„æ–¹å¼é¿å…äº†æ–‡æœ¬CoTå¯èƒ½å¸¦æ¥çš„ä¿¡æ¯æŸå¤±å’Œæ­§ä¹‰ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šVITAçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨Transformeræ¶æ„ä½œä¸ºè§†è§‰å’ŒåŠ¨ä½œç¼–ç å™¨å’Œè§£ç å™¨ï¼›2) é‡‡ç”¨ç¦»æ•£å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ å…±äº«ç¦»æ•£æ½œåœ¨ç©ºé—´ï¼›3) ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è®­ç»ƒè‡ªå›å½’è§£ç å™¨ï¼ŒåŒæ—¶ä¼˜åŒ–æœªæ¥å¸§é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆçš„å‡†ç¡®æ€§ï¼›4) é€šè¿‡è°ƒæ•´è§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”ŸæˆæŸå¤±çš„æƒé‡ï¼Œå¹³è¡¡ä¸¤ä¸ªç›®æ ‡ä¹‹é—´çš„ç«äº‰å…³ç³»ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

VITAåœ¨CALVINã€LIBEROå’ŒSimplerEnvç­‰æ¨¡æ‹Ÿç¯å¢ƒbenchmarkä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåˆ†åˆ«æ¯”ç°æœ‰åŸºçº¿æé«˜äº†14.5%ã€9.6%å’Œ12.1%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒVITAåœ¨å…­ä¸ªçœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®ç°äº†å¹³å‡80.5%çš„æˆåŠŸç‡ï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œå®ç”¨ä»·å€¼ã€‚è¿™äº›å®éªŒç»“æœè¡¨æ˜ï¼ŒVITAæ˜¯ç›®å‰æœ€å…ˆè¿›çš„é€šç”¨æœºå™¨äººæ“ä½œæ¨¡å‹ä¹‹ä¸€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

VITAæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œå¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººã€åŒ»ç–—æœºå™¨äººç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ï¼Œå®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººæ“ä½œã€‚æœªæ¥ï¼ŒVITAå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å¤šæ¨¡æ€è¾“å…¥ï¼ˆå¦‚è¯­éŸ³ã€è§¦è§‰ï¼‰å’Œæ›´å¤æ‚çš„ä»»åŠ¡åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\%, 9.6\% and 12.1\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.

