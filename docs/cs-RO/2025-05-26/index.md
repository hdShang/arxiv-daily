---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-26
---

# cs.ROï¼ˆ2025-05-26ï¼‰

ğŸ“Š å…± **18** ç¯‡è®ºæ–‡
 | ğŸ”— **3** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ğŸ”—2)</a>
<a href="#æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting" class="interest-badge">æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (15 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250519540v1-real-time-whole-body-model-predictive-control-for-bipedal-locomotion.html">Real-time Whole-body Model Predictive Control for Bipedal Locomotion with a Novel Kino-dynamic Model and Warm-start Method</a></td>
  <td>æå‡ºæ–°å‹è¿åŠ¨åŠ¨åŠ›å­¦æ¨¡å‹ä¸çƒ­å¯åŠ¨æ–¹æ³•ä»¥è§£å†³åŒè¶³æœºå™¨äººå®æ—¶æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">bipedal</span> <span class="paper-tag">biped</span> <span class="paper-tag">whole-body control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19540v1" data-paper-url="./papers/250519540v1-real-time-whole-body-model-predictive-control-for-bipedal-locomotion.html" onclick="toggleFavorite(this, '2505.19540v1', 'Real-time Whole-body Model Predictive Control for Bipedal Locomotion with a Novel Kino-dynamic Model and Warm-start Method')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250519463v1-smap-self-supervised-motion-adaptation-for-physically-plausible-huma.html">SMAP: Self-supervised Motion Adaptation for Physically Plausible Humanoid Whole-body Control</a></td>
  <td>æå‡ºSMAPæ¡†æ¶ä»¥è§£å†³äººå½¢æœºå™¨äººè¿åŠ¨æ§åˆ¶ç¨³å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">whole-body control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19463v1" data-paper-url="./papers/250519463v1-smap-self-supervised-motion-adaptation-for-physically-plausible-huma.html" onclick="toggleFavorite(this, '2505.19463v1', 'SMAP: Self-supervised Motion Adaptation for Physically Plausible Humanoid Whole-body Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250519530v1-heavy-lifting-tasks-via-haptic-teleoperation-of-a-wheeled-humanoid.html">Heavy lifting tasks via haptic teleoperation of a wheeled humanoid</a></td>
  <td>æå‡ºä¸€ç§è§¦è§‰é¥æ“ä½œæ¡†æ¶ä»¥è§£å†³é‡ç‰©æ¬è¿é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">whole-body control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19530v1" data-paper-url="./papers/250519530v1-heavy-lifting-tasks-via-haptic-teleoperation-of-a-wheeled-humanoid.html" onclick="toggleFavorite(this, '2505.19530v1', 'Heavy lifting tasks via haptic teleoperation of a wheeled humanoid')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250520175v1-urplanner-a-universal-paradigm-for-collision-free-robotic-motion-pla.html">URPlanner: A Universal Paradigm For Collision-Free Robotic Motion Planning Based on Deep Reinforcement Learning</a></td>
  <td>æå‡ºURPlannerä»¥è§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„æ— ç¢°æ’æœºå™¨äººè¿åŠ¨è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">deep reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20175v1" data-paper-url="./papers/250520175v1-urplanner-a-universal-paradigm-for-collision-free-robotic-motion-pla.html" onclick="toggleFavorite(this, '2505.20175v1', 'URPlanner: A Universal Paradigm For Collision-Free Robotic Motion Planning Based on Deep Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250519717v2-extremum-flow-matching-for-offline-goal-conditioned-reinforcement-le.html">Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning</a></td>
  <td>æå‡ºæå€¼æµåŒ¹é…æ–¹æ³•ä»¥è§£å†³ç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19717v2" data-paper-url="./papers/250519717v2-extremum-flow-matching-for-offline-goal-conditioned-reinforcement-le.html" onclick="toggleFavorite(this, '2505.19717v2', 'Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250519803v2-integrating-emotional-intelligence-memory-architecture-and-gestures-.html">Integrating emotional intelligence, memory architecture, and gestures to achieve empathetic humanoid robot interaction in an educational setting</a></td>
  <td>æå‡ºæƒ…æ„Ÿæ™ºèƒ½ä¸è®°å¿†æ¶æ„ç»“åˆçš„æ•™è‚²æœºå™¨äººä»¥æå‡å­¦ç”Ÿäº’åŠ¨</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19803v2" data-paper-url="./papers/250519803v2-integrating-emotional-intelligence-memory-architecture-and-gestures-.html" onclick="toggleFavorite(this, '2505.19803v2', 'Integrating emotional intelligence, memory architecture, and gestures to achieve empathetic humanoid robot interaction in an educational setting')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250519580v1-whole-body-multi-contact-motion-control-for-humanoid-robots-based-on.html">Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors</a></td>
  <td>æå‡ºåŸºäºåˆ†å¸ƒå¼è§¦è§‰ä¼ æ„Ÿå™¨çš„å…¨èº«å¤šæ¥è§¦è¿åŠ¨æ§åˆ¶æ–¹æ³•ä»¥è§£å†³äººå½¢æœºå™¨äººåœ¨ç‹­å°ç¯å¢ƒä¸­çš„ç¨³å®šæ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19580v1" data-paper-url="./papers/250519580v1-whole-body-multi-contact-motion-control-for-humanoid-robots-based-on.html" onclick="toggleFavorite(this, '2505.19580v1', 'Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250520573v2-collision-and-reachability-aware-multi-robot-control-with-grounded-l.html">Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners</a></td>
  <td>æå‡ºRLVRæ¡†æ¶ä»¥è§£å†³å¤šæœºå™¨äººæ§åˆ¶ä¸­çš„ç‰©ç†çº¦æŸé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reachability-aware</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20573v2" data-paper-url="./papers/250520573v2-collision-and-reachability-aware-multi-robot-control-with-grounded-l.html" onclick="toggleFavorite(this, '2505.20573v2', 'Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250519688v2-geopf-infusing-geometry-into-potential-fields-for-reactive-planning-.html">GeoPF: Infusing Geometry into Potential Fields for Reactive Planning in Non-trivial Environments</a></td>
  <td>æå‡ºGeoPFä»¥è§£å†³ä¼ ç»Ÿæ½œåœ¨åœºæ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">motion generation</span> <span class="paper-tag">reactive motion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19688v2" data-paper-url="./papers/250519688v2-geopf-infusing-geometry-into-potential-fields-for-reactive-planning-.html" onclick="toggleFavorite(this, '2505.19688v2', 'GeoPF: Infusing Geometry into Potential Fields for Reactive Planning in Non-trivial Environments')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250519767v1-rftf-reinforcement-fine-tuning-for-embodied-agents-with-temporal-fee.html">RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback</a></td>
  <td>æå‡ºRFTFä»¥è§£å†³ç°æœ‰å¼ºåŒ–å¾®è°ƒæ–¹æ³•ç¨€ç–å¥–åŠ±é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">behavior cloning</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19767v1" data-paper-url="./papers/250519767v1-rftf-reinforcement-fine-tuning-for-embodied-agents-with-temporal-fee.html" onclick="toggleFavorite(this, '2505.19767v1', 'RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>11</td>
  <td><a href="./papers/250520290v2-egozero-robot-learning-from-smart-glasses.html">EgoZero: Robot Learning from Smart Glasses</a></td>
  <td>æå‡ºEgoZeroä»¥è§£å†³æœºå™¨äººå­¦ä¹ ä¸­ç¼ºä¹äººç±»æ•°æ®çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">egocentric</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20290v2" data-paper-url="./papers/250520290v2-egozero-robot-learning-from-smart-glasses.html" onclick="toggleFavorite(this, '2505.20290v2', 'EgoZero: Robot Learning from Smart Glasses')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250519512v1-lla-mpc-fast-adaptive-control-for-autonomous-racing.html">LLA-MPC: Fast Adaptive Control for Autonomous Racing</a></td>
  <td>æå‡ºLLA-MPCä»¥è§£å†³è‡ªä¸»èµ›è½¦ä¸­çš„å¿«é€Ÿé€‚åº”æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">MPC</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19512v1" data-paper-url="./papers/250519512v1-lla-mpc-fast-adaptive-control-for-autonomous-racing.html" onclick="toggleFavorite(this, '2505.19512v1', 'LLA-MPC: Fast Adaptive Control for Autonomous Racing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>13</td>
  <td><a href="./papers/250519769v2-tevir-text-to-video-reward-with-diffusion-models-for-efficient-reinf.html">TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning</a></td>
  <td>æå‡ºTeViRä»¥è§£å†³ç¨€ç–å¥–åŠ±åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä½æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.19769v2" data-paper-url="./papers/250519769v2-tevir-text-to-video-reward-with-diffusion-models-for-efficient-reinf.html" onclick="toggleFavorite(this, '2505.19769v2', 'TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250520455v4-hand-me-the-data-fast-robot-adaptation-via-hand-path-retrieval.html">HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval</a></td>
  <td>æå‡ºHANDæ–¹æ³•ä»¥è§£å†³æœºå™¨äººå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">teleoperation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20455v4" data-paper-url="./papers/250520455v4-hand-me-the-data-fast-robot-adaptation-via-hand-path-retrieval.html" onclick="toggleFavorite(this, '2505.20455v4', 'HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>15</td>
  <td><a href="./papers/250520404v3-co-design-of-soft-gripper-with-neural-physics.html">Co-Design of Soft Gripper with Neural Physics</a></td>
  <td>æå‡ºä¸€ç§ç¥ç»ç‰©ç†å…±è®¾è®¡æ¡†æ¶ä»¥ä¼˜åŒ–è½¯æŠ“æ‰‹è®¾è®¡ä¸æŠ“å–å§¿æ€</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20404v3" data-paper-url="./papers/250520404v3-co-design-of-soft-gripper-with-neural-physics.html" onclick="toggleFavorite(this, '2505.20404v3', 'Co-Design of Soft Gripper with Neural Physics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸ƒåŠ¨ä½œé‡å®šå‘-motion-retargeting">ğŸ”¬ æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250520503v1-embodied-ai-with-foundation-models-for-mobile-service-robots-a-syste.html">Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review</a></td>
  <td>é€šè¿‡åŸºç¡€æ¨¡å‹æå‡ç§»åŠ¨æœåŠ¡æœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">cross-embodiment</span> <span class="paper-tag">embodied AI</span> <span class="paper-tag">vision-language-action</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20503v1" data-paper-url="./papers/250520503v1-embodied-ai-with-foundation-models-for-mobile-service-robots-a-syste.html" onclick="toggleFavorite(this, '2505.20503v1', 'Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>17</td>
  <td><a href="./papers/250520223v1-chain-of-thought-for-autonomous-driving-a-comprehensive-survey-and-f.html">Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects</a></td>
  <td>æå‡ºé“¾å¼æ€ç»´æ–¹æ³•ä»¥æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20223v1" data-paper-url="./papers/250520223v1-chain-of-thought-for-autonomous-driving-a-comprehensive-survey-and-f.html" onclick="toggleFavorite(this, '2505.20223v1', 'Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>18</td>
  <td><a href="./papers/250520425v1-osvi-wm-one-shot-visual-imitation-for-unseen-tasks-using-world-model.html">OSVI-WM: One-Shot Visual Imitation for Unseen Tasks using World-Model-Guided Trajectory Generation</a></td>
  <td>æå‡ºä¸–ç•Œæ¨¡å‹å¼•å¯¼çš„å•æ¬¡è§†è§‰æ¨¡ä»¿ä»¥è§£å†³æœªè§ä»»åŠ¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">imitation learning</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.20425v1" data-paper-url="./papers/250520425v1-osvi-wm-one-shot-visual-imitation-for-unseen-tasks-using-world-model.html" onclick="toggleFavorite(this, '2505.20425v1', 'OSVI-WM: One-Shot Visual Imitation for Unseen Tasks using World-Model-Guided Trajectory Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)