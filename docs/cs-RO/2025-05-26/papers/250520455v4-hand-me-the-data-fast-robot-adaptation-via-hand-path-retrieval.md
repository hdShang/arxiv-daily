---
layout: default
title: "HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval"
---

# HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20455" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20455v4</a>
  <a href="https://arxiv.org/pdf/2505.20455.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20455v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20455v4', 'HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Matthew Hong, Anthony Liang, Kevin Kim, Harshitha Rajaprakash, Jesse Thomason, Erdem BÄ±yÄ±k, Jesse Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-10-27)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHANDæ–¹æ³•ä»¥è§£å†³æœºå™¨äººå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äººé€‚åº”` `æ‰‹éƒ¨ç¤ºèŒƒ` `è¡Œä¸ºæ£€ç´¢` `å®æ—¶å­¦ä¹ ` `è§†è§‰è·Ÿè¸ª` `ä»»åŠ¡æ— å…³æ•°æ®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æœºå™¨äººç¤ºèŒƒï¼Œæ”¶é›†è¿‡ç¨‹å¤æ‚ä¸”è€—æ—¶ï¼Œé™åˆ¶äº†æœºå™¨äººå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚
2. HANDæ–¹æ³•é€šè¿‡äººç±»æ‰‹éƒ¨ç¤ºèŒƒæå–è¿åŠ¨ä¿¡æ¯ï¼Œåˆ©ç”¨ä»»åŠ¡æ— å…³çš„æœºå™¨äººæ•°æ®è¿›è¡Œè¡Œä¸ºæ£€ç´¢ï¼Œä»è€Œå®ç°å¿«é€Ÿå­¦ä¹ ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHANDåœ¨çœŸå®æœºå™¨äººä¸Šçš„å¹³å‡ä»»åŠ¡æˆåŠŸç‡æå‡è¶…è¿‡2å€ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ£€ç´¢åŸºçº¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•é«˜æ•ˆçš„æ–¹æ³•HANDï¼Œé€šè¿‡äººç±»æ‰‹éƒ¨ç¤ºèŒƒæ•™ä¼šæœºå™¨äººæ–°çš„æ“ä½œä»»åŠ¡ã€‚ä¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æœºå™¨äººç¤ºèŒƒä¸åŒï¼ŒHANDåˆ©ç”¨æ˜“äºæä¾›çš„æ‰‹éƒ¨ç¤ºèŒƒï¼Œä»ä»»åŠ¡æ— å…³çš„æœºå™¨äººæ¸¸æˆæ•°æ®ä¸­æ£€ç´¢ç›¸å…³è¡Œä¸ºã€‚é€šè¿‡è§†è§‰è·Ÿè¸ªç®¡é“ï¼ŒHANDæå–äººç±»æ‰‹éƒ¨çš„è¿åŠ¨ï¼Œå¹¶åˆ†ä¸¤ä¸ªé˜¶æ®µæ£€ç´¢æœºå™¨äººå­è½¨è¿¹ï¼šé¦–å…ˆé€šè¿‡è§†è§‰ç›¸ä¼¼æ€§è¿‡æ»¤ï¼Œç„¶åæ£€ç´¢ä¸æ‰‹éƒ¨è¡Œä¸ºç›¸ä¼¼çš„è½¨è¿¹ã€‚å¯¹æ£€ç´¢æ•°æ®è¿›è¡Œç­–ç•¥å¾®è°ƒï¼Œä½¿å¾—ä»»åŠ¡çš„å®æ—¶å­¦ä¹ åœ¨å››åˆ†é’Ÿå†…å®Œæˆï¼Œæ— éœ€æ ¡å‡†ç›¸æœºæˆ–è¯¦ç»†çš„æ‰‹éƒ¨å§¿æ€ä¼°è®¡ã€‚å®éªŒè¡¨æ˜ï¼ŒHANDåœ¨çœŸå®æœºå™¨äººä¸Šçš„å¹³å‡ä»»åŠ¡æˆåŠŸç‡è¶…è¿‡æ£€ç´¢åŸºçº¿2å€ä»¥ä¸Šã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨æ–°æ“ä½œä»»åŠ¡ä¸­å¿«é€Ÿé€‚åº”çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤æ‚çš„ä»»åŠ¡ç‰¹å®šç¤ºèŒƒï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHANDæ–¹æ³•é€šè¿‡äººç±»æ‰‹éƒ¨ç¤ºèŒƒæå–è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶ä»ä»»åŠ¡æ— å…³çš„æœºå™¨äººæ¸¸æˆæ•°æ®ä¸­æ£€ç´¢ç›¸å…³è¡Œä¸ºï¼Œé¿å…äº†å¯¹å¤æ‚ç¤ºèŒƒçš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æµç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆé€šè¿‡è§†è§‰ç›¸ä¼¼æ€§è¿‡æ»¤æ‰‹éƒ¨ç¤ºèŒƒï¼Œæå–æ‰‹éƒ¨è¿åŠ¨è½¨è¿¹ï¼›ç„¶åæ£€ç´¢ä¸æ‰‹éƒ¨è¡Œä¸ºç›¸ä¼¼çš„æœºå™¨äººè½¨è¿¹ã€‚æœ€åï¼Œå¯¹æ£€ç´¢åˆ°çš„æ•°æ®è¿›è¡Œç­–ç•¥å¾®è°ƒï¼Œå®ç°å®æ—¶å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šHANDçš„ä¸»è¦åˆ›æ–°åœ¨äºåˆ©ç”¨äººç±»æ‰‹éƒ¨ç¤ºèŒƒè¿›è¡Œè¡Œä¸ºæ£€ç´¢ï¼Œè€Œéä¾èµ–äºå¤æ‚çš„ä»»åŠ¡ç‰¹å®šç¤ºèŒƒã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆç‡å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒHANDé‡‡ç”¨äº†è§†è§‰è·Ÿè¸ªç®¡é“æ¥æå–æ‰‹éƒ¨è¿åŠ¨ï¼Œä¸”ä¸éœ€è¦æ ¡å‡†ç›¸æœºæˆ–è¯¦ç»†çš„æ‰‹éƒ¨å§¿æ€ä¼°è®¡ï¼Œç®€åŒ–äº†ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHANDæ–¹æ³•åœ¨çœŸå®æœºå™¨äººä¸Šçš„å¹³å‡ä»»åŠ¡æˆåŠŸç‡è¶…è¿‡ä¼ ç»Ÿæ£€ç´¢åŸºçº¿2å€ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä»»åŠ¡å­¦ä¹ æ•ˆç‡ä¸Šçš„æ˜¾è‘—æå‡ã€‚è¿™ä¸€æˆæœéªŒè¯äº†HANDåœ¨å¿«é€Ÿé€‚åº”æ–°æ“ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œäººæœºåä½œç­‰åœºæ™¯ã€‚é€šè¿‡å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œæœºå™¨äººèƒ½å¤Ÿæ›´çµæ´»åœ°åº”å¯¹å¤šå˜çš„å·¥ä½œç¯å¢ƒï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼ŒHANDæ–¹æ³•å¯èƒ½æ¨åŠ¨æœºå™¨äººåœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ï¼Œä¿ƒè¿›æ™ºèƒ½è‡ªåŠ¨åŒ–çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We hand the community HAND, a simple and time-efficient method for teaching robots new manipulation tasks through human hand demonstrations. Instead of relying on task-specific robot demonstrations collected via teleoperation, HAND uses easy-to-provide hand demonstrations to retrieve relevant behaviors from task-agnostic robot play data. Using a visual tracking pipeline, HAND extracts the motion of the human hand from the hand demonstration and retrieves robot sub-trajectories in two stages: first filtering by visual similarity, then retrieving trajectories with similar behaviors to the hand. Fine-tuning a policy on the retrieved data enables real-time learning of tasks in under four minutes, without requiring calibrated cameras or detailed hand pose estimation. Experiments also show that HAND outperforms retrieval baselines by over 2x in average task success rates on real robots. Videos can be found at our project website: https://liralab.usc.edu/handretrieval/.

