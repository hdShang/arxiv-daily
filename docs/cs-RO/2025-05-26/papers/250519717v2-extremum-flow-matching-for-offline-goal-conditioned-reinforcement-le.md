---
layout: default
title: Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning
---

# Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19717" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19717v2</a>
  <a href="https://arxiv.org/pdf/2505.19717.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19717v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19717v2', 'Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Quentin Rouxel, Clemente Donoso, Fei Chen, Serena Ivaldi, Jean-Baptiste Mouret

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-08-20)

**å¤‡æ³¨**: 2025 IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids), Sep 2025, Seoul, South Korea

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://hucebot.github.io/extremum_flow_matching_website/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæå€¼æµåŒ¹é…æ–¹æ³•ä»¥è§£å†³ç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ¨¡ä»¿å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æµåŒ¹é…` `ç±»äººæœºå™¨äºº` `å¼€æ”¾å¼æ¸¸æˆæ•°æ®` `ç›®æ ‡æ¡ä»¶å­¦ä¹ ` `é«˜ç»´å›¾åƒå¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•å—é™äºé«˜è´¨é‡ä¸“å®¶ç¤ºèŒƒçš„ç¨€ç¼ºï¼Œéš¾ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„æå€¼ä¼°è®¡æ–¹æ³•ï¼Œåˆ©ç”¨å…¶ç¡®å®šæ€§ä¼ è¾“ç‰¹æ€§æ¥æ”¹å–„æ¨¡ä»¿å­¦ä¹ çš„æ•ˆæœã€‚
3. åœ¨OGBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€æå‡ºçš„ç®—æ³•åœ¨2DéæŠ“å–æ¨é€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨çœŸå®çš„Talosæœºå™¨äººä¸ŠæˆåŠŸæ‰§è¡Œå¤æ‚æ“ä½œã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ¨¡ä»¿å­¦ä¹ æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿèµ‹äºˆç±»äººæœºå™¨äººé€šç”¨èƒ½åŠ›ï¼Œä½†é«˜è´¨é‡ä¸“å®¶ç¤ºèŒƒçš„ç¨€ç¼ºæ€§é™åˆ¶äº†å…¶æ‰©å±•æ€§ã€‚æœ¬æ–‡é€šè¿‡åˆ©ç”¨äºšæœ€ä¼˜çš„å¼€æ”¾å¼æ¸¸æˆæ•°æ®ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„æå€¼ä¼°è®¡æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„è¿™ä¸€é™åˆ¶ã€‚æˆ‘ä»¬å¼€å‘äº†å¤šç§åŸºäºæµåŒ¹é…çš„ç›®æ ‡æ¡ä»¶æ¨¡ä»¿å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶åœ¨OGBenchåŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒéªŒè¯äº†åœ¨çœŸå®ç¡¬ä»¶ä¸Šæ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­é«˜è´¨é‡ä¸“å®¶ç¤ºèŒƒç¨€ç¼ºçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®æ”¶é›†ä¸Šå—åˆ°é™åˆ¶ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„æå€¼ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æµåŒ¹é…çš„ç‹¬ç‰¹æ€§è´¨ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„æºåˆ†å¸ƒä¸Šè¿›è¡Œæœ‰æ•ˆçš„å­¦ä¹ å’Œæ¨ç†ï¼Œä»è€Œæ”¹å–„æ¨¡ä»¿å­¦ä¹ çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå¦‚è¯„è®ºè€…ã€è§„åˆ’è€…ã€æ‰§è¡Œè€…å’Œä¸–ç•Œæ¨¡å‹ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸åŒçš„æ¶æ„é…ç½®ï¼Œç»“åˆè¿™äº›ç»„ä»¶ä»¥å®ç°ç›®æ ‡æ¡ä»¶çš„æ¨¡ä»¿å’Œå¼ºåŒ–å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæµåŒ¹é…æ–¹æ³•çš„åº”ç”¨ï¼Œå®ƒä¸ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œèƒ½å¤Ÿæä¾›æ›´é«˜æ•ˆçš„åˆ†å¸ƒæå€¼ä¼°è®¡ï¼Œæ”¯æŒæ›´å¹¿æ³›çš„æºåˆ†å¸ƒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æµåŒ¹é…è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨äº†é€‚åº”æ€§ç½‘ç»œç»“æ„æ¥å¤„ç†é«˜ç»´å›¾åƒè§‚æµ‹ï¼Œç¡®ä¿åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨OGBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€æå‡ºçš„ç®—æ³•åœ¨2DéæŠ“å–æ¨é€ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼ŒæˆåŠŸç‡æé«˜äº†20%ã€‚æ­¤å¤–ï¼Œåœ¨çœŸå®çš„Talosæœºå™¨äººä¸Šæ‰§è¡Œå¤æ‚çš„æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡æ—¶ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç±»äººæœºå™¨äººåœ¨å®¶åº­ã€åŒ»ç–—å’Œå·¥ä¸šç¯å¢ƒä¸­çš„æ“ä½œä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨å¼€æ”¾å¼æ¸¸æˆæ•°æ®ï¼Œæœºå™¨äººèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸­å­¦ä¹ å’Œé€‚åº”ï¼Œæå‡å…¶è‡ªä¸»æ“ä½œèƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Imitation learning is a promising approach for enabling generalist capabilities in humanoid robots, but its scaling is fundamentally constrained by the scarcity of high-quality expert demonstrations. This limitation can be mitigated by leveraging suboptimal, open-ended play data, often easier to collect and offering greater diversity. This work builds upon recent advances in generative modeling, specifically Flow Matching, an alternative to Diffusion models. We introduce a method for estimating the minimum or maximum of the learned distribution by leveraging the unique properties of Flow Matching, namely, deterministic transport and support for arbitrary source distributions. We apply this method to develop several goal-conditioned imitation and reinforcement learning algorithms based on Flow Matching, where policies are conditioned on both current and goal observations. We explore and compare different architectural configurations by combining core components, such as critic, planner, actor, or world model, in various ways. We evaluated our agents on the OGBench benchmark and analyzed how different demonstration behaviors during data collection affect performance in a 2D non-prehensile pushing task. Furthermore, we validated our approach on real hardware by deploying it on the Talos humanoid robot to perform complex manipulation tasks based on high-dimensional image observations, featuring a sequence of pick-and-place and articulated object manipulation in a realistic kitchen environment. Experimental videos and code are available at: https://hucebot.github.io/extremum_flow_matching_website/

