---
layout: default
title: "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning"
---

# TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19769" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19769v2</a>
  <a href="https://arxiv.org/pdf/2505.19769.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19769v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19769v2', 'TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuhui Chen, Haoran Li, Zhennan Jiang, Haowei Wen, Dongbin Zhao

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-06-24)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTeViRä»¥è§£å†³ç¨€ç–å¥–åŠ±åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä½æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±å·¥ç¨‹` `æ–‡æœ¬åˆ°è§†é¢‘` `ç¨€ç–å¥–åŠ±` `æœºå™¨äººæ“ä½œ` `æ·±åº¦å­¦ä¹ ` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±å·¥ç¨‹æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œç¨€ç–å¥–åŠ±é™åˆ¶äº†å­¦ä¹ æ•ˆæœã€‚
2. TeViRé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå¯†é›†å¥–åŠ±ï¼Œä»è€Œæé«˜äº†å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTeViRåœ¨11ä¸ªå¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿç¨€ç–å¥–åŠ±æ–¹æ³•ï¼Œä¸”æ— éœ€çœŸå®ç¯å¢ƒå¥–åŠ±ï¼Œæå‡äº†å­¦ä¹ æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œå¼€å‘å¯æ‰©å±•ä¸”å…·æœ‰æ™®é€‚æ€§çš„å¥–åŠ±å·¥ç¨‹è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„æœºå™¨äººæ“ä½œé¢†åŸŸã€‚å°½ç®¡è¿‘æœŸåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¥–åŠ±å·¥ç¨‹å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†å…¶ç¨€ç–å¥–åŠ±ç‰¹æ€§æ˜¾è‘—é™åˆ¶äº†æ ·æœ¬æ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•TeViRï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å°†é¢„æµ‹çš„å›¾åƒåºåˆ—ä¸å½“å‰è§‚å¯Ÿè¿›è¡Œæ¯”è¾ƒï¼Œç”Ÿæˆå¯†é›†å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTeViRåœ¨11ä¸ªå¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­è¶…è¶Šäº†ä¼ ç»Ÿç¨€ç–å¥–åŠ±æ–¹æ³•åŠå…¶ä»–æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æ–¹æ³•ï¼Œåœ¨æ²¡æœ‰çœŸå®ç¯å¢ƒå¥–åŠ±çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ›´å¥½çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚TeViRåœ¨å¤æ‚ç¯å¢ƒä¸­æœ‰æ•ˆå¼•å¯¼ä»£ç†çš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ½œåœ¨åº”ç”¨å‰æ™¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­ç¨€ç–å¥–åŠ±å¯¼è‡´çš„æ ·æœ¬æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­éš¾ä»¥æœ‰æ•ˆå¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šTeViRçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯†é›†å¥–åŠ±ï¼Œé€šè¿‡å¯¹æ¯”é¢„æµ‹çš„å›¾åƒåºåˆ—ä¸å½“å‰è§‚å¯Ÿï¼Œæä¾›æ›´ä¸°å¯Œçš„åé¦ˆä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTeViRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šæ–‡æœ¬è¾“å…¥å¤„ç†ã€è§†é¢‘åºåˆ—ç”Ÿæˆå’Œå¥–åŠ±è®¡ç®—ã€‚é¦–å…ˆï¼Œå°†æ–‡æœ¬æè¿°è½¬åŒ–ä¸ºè§†é¢‘åºåˆ—ï¼Œç„¶åä¸å½“å‰è§‚å¯Ÿè¿›è¡Œæ¯”è¾ƒï¼Œæœ€åç”Ÿæˆå¯†é›†å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šTeViRçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†æ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆèƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå…‹æœäº†ä¼ ç»Ÿç¨€ç–å¥–åŠ±æ–¹æ³•çš„å±€é™æ€§ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„å­¦ä¹ è¿‡ç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒTeViRé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¥–åŠ±ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨æ·±åº¦å­¦ä¹ ç½‘ç»œç»“æ„æ¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒTeViRåœ¨11ä¸ªå¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿç¨€ç–å¥–åŠ±æ–¹æ³•å’Œå…¶ä»–æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œæå‡å¹…åº¦æ˜¾è‘—ï¼Œå±•ç¤ºäº†å…¶åœ¨æ ·æœ¬æ•ˆç‡å’Œå­¦ä¹ æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

TeViRçš„ç ”ç©¶æˆæœåœ¨æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–æ§åˆ¶å’Œæ™ºèƒ½ä»£ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›æ›´é«˜æ•ˆçš„å¥–åŠ±æœºåˆ¶ï¼ŒTeViRèƒ½å¤Ÿå¸®åŠ©ä»£ç†åœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å¿«é€Ÿåœ°å­¦ä¹ å’Œé€‚åº”ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViR's ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation.

