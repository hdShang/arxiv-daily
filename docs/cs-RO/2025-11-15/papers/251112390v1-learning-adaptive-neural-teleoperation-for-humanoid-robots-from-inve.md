---
layout: default
title: Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control
---

# Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.12390" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.12390v1</a>
  <a href="https://arxiv.org/pdf/2511.12390.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.12390v1" onclick="toggleFavorite(this, '2511.12390v1', 'Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sanjar Atamuradov

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-15

**å¤‡æ³¨**: 9 pages, 5 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”ç¥ç»é¥æ“ä½œæ¡†æ¶ï¼Œæå‡äººå½¢æœºå™¨äººæ§åˆ¶çš„è‡ªç„¶æ€§å’Œé²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `äººå½¢æœºå™¨äºº` `é¥æ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `é€†è¿åŠ¨å­¦` `åŠ›é€‚åº”` `è¿åŠ¨å¹³æ»‘` `è™šæ‹Ÿç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ä¼ ç»Ÿäººå½¢æœºå™¨äººé¥æ“ä½œä¾èµ–é€†è¿åŠ¨å­¦å’Œæ‰‹åŠ¨PDæ§åˆ¶ï¼Œéš¾ä»¥åº”å¯¹å¤–åŠ›æ‰°åŠ¨ï¼Œé€‚åº”ä¸åŒç”¨æˆ·ï¼Œä»¥åŠç”ŸæˆåŠ¨æ€æ¡ä»¶ä¸‹çš„è‡ªç„¶è¿åŠ¨ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç¥ç»é¥æ“ä½œæ¡†æ¶ï¼Œç›´æ¥å­¦ä¹ VRæ§åˆ¶å™¨è¾“å…¥åˆ°æœºå™¨äººå…³èŠ‚æŒ‡ä»¤çš„æ˜ å°„ï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡æ§åˆ¶å™¨ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·Ÿè¸ªè¯¯å·®ã€è¿åŠ¨å¹³æ»‘åº¦å’ŒåŠ›é€‚åº”æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶åœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„ç¥ç»é¥æ“ä½œæ¡†æ¶ï¼Œç”¨äºæ§åˆ¶å¤æ‚æ“ä½œä»»åŠ¡ä¸­çš„äººå½¢æœºå™¨äººã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥ï¼Œå–ä»£äº†ä¼ ç»Ÿçš„é€†è¿åŠ¨å­¦ï¼ˆIKï¼‰æ±‚è§£å™¨å’Œæ‰‹åŠ¨è°ƒæ•´çš„PDæ§åˆ¶å™¨ã€‚è¯¥æ–¹æ³•ç›´æ¥å°†VRæ§åˆ¶å™¨è¾“å…¥æ˜ å°„åˆ°æœºå™¨äººå…³èŠ‚æŒ‡ä»¤ï¼Œéšå¼åœ°å¤„ç†åŠ›æ‰°åŠ¨ï¼Œç”Ÿæˆå¹³æ»‘è½¨è¿¹ï¼Œå¹¶é€‚åº”ç”¨æˆ·åå¥½ã€‚ç­–ç•¥è®­ç»ƒé¦–å…ˆä½¿ç”¨åŸºäºIKçš„é¥æ“ä½œæ¼”ç¤ºè¿›è¡Œåˆå§‹åŒ–ï¼Œç„¶ååœ¨æ¨¡æ‹Ÿä¸­ä½¿ç”¨åŠ›éšæœºåŒ–å’Œå¹³æ»‘è½¨è¿¹å¥–åŠ±è¿›è¡Œå¾®è°ƒã€‚åœ¨Unitree G1äººå½¢æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸IKåŸºçº¿ç›¸æ¯”ï¼Œè¯¥å­¦ä¹ ç­–ç•¥å®ç°äº†34%çš„è·Ÿè¸ªè¯¯å·®é™ä½ï¼Œ45%çš„è¿åŠ¨å¹³æ»‘åº¦æå‡ï¼Œä»¥åŠæ›´ä¼˜å¼‚çš„åŠ›é€‚åº”æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ€§èƒ½ï¼ˆ50Hzæ§åˆ¶é¢‘ç‡ï¼‰ã€‚è¯¥æ–¹æ³•åœ¨ç‰©ä½“æŠ“å–æ”¾ç½®ã€å¼€é—¨å’ŒåŒæ‰‹åè°ƒç­‰æ“ä½œä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜äººå½¢æœºå™¨äººé¥æ“ä½œç³»ç»Ÿçš„è‡ªç„¶æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººå½¢æœºå™¨äººé¥æ“ä½œç³»ç»Ÿé€šå¸¸ä¾èµ–äºé€†è¿åŠ¨å­¦ï¼ˆIKï¼‰æ±‚è§£å™¨å’Œæ‰‹åŠ¨è°ƒæ•´çš„PDæ§åˆ¶å™¨ã€‚è¿™ç§æ–¹æ³•çš„ç—›ç‚¹åœ¨äºéš¾ä»¥å¤„ç†å¤–éƒ¨åŠ›æ‰°åŠ¨ï¼Œæ— æ³•å¾ˆå¥½åœ°é€‚åº”ä¸åŒç”¨æˆ·çš„æ“ä½œä¹ æƒ¯ï¼Œå¹¶ä¸”åœ¨åŠ¨æ€æ¡ä»¶ä¸‹éš¾ä»¥ç”Ÿæˆè‡ªç„¶æµç•…çš„è¿åŠ¨ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´é²æ£’ã€æ›´é€‚åº”æ€§å¼ºçš„é¥æ“ä½œæ§åˆ¶æ–¹æ³•ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œç›´æ¥å­¦ä¹ ä»VRæ§åˆ¶å™¨è¾“å…¥åˆ°æœºå™¨äººå…³èŠ‚æŒ‡ä»¤çš„æ˜ å°„ç­–ç•¥ã€‚é€šè¿‡ç«¯åˆ°ç«¯çš„å­¦ä¹ ï¼Œå¯ä»¥éšå¼åœ°å¤„ç†åŠ›æ‰°åŠ¨ï¼Œç”Ÿæˆå¹³æ»‘çš„è½¨è¿¹ï¼Œå¹¶é€‚åº”ç”¨æˆ·çš„æ“ä½œåå¥½ã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ‰‹åŠ¨è®¾è®¡æ§åˆ¶å™¨å’Œè°ƒæ•´å‚æ•°çš„å¤æ‚æ€§ï¼Œæé«˜äº†ç³»ç»Ÿçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š1) ä½¿ç”¨åŸºäºIKçš„é¥æ“ä½œæ•°æ®è¿›è¡Œç­–ç•¥åˆå§‹åŒ–ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æä¾›ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ã€‚2) åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒç­–ç•¥ï¼Œå¥–åŠ±å‡½æ•°åŒ…æ‹¬è·Ÿè¸ªè¯¯å·®ã€è¿åŠ¨å¹³æ»‘åº¦å’ŒåŠ›é€‚åº”æ€§ã€‚3) ä½¿ç”¨åŠ›éšæœºåŒ–æŠ€æœ¯ï¼Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚4) åœ¨çœŸå®æœºå™¨äººä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œè¯„ä¼°ç­–ç•¥çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç›´æ¥å­¦ä¹ é¥æ“ä½œæ§åˆ¶ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„IK+PDæ§åˆ¶æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†åŠ›æ‰°åŠ¨ï¼Œç”Ÿæˆæ›´è‡ªç„¶çš„è¿åŠ¨ï¼Œå¹¶é€‚åº”ç”¨æˆ·çš„æ“ä½œä¹ æƒ¯ã€‚æ­¤å¤–ï¼Œä½¿ç”¨åŠ›éšæœºåŒ–æŠ€æœ¯æé«˜äº†ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒå’Œä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šç­–ç•¥ç½‘ç»œç»“æ„æœªçŸ¥ï¼Œä½†ä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬è·Ÿè¸ªè¯¯å·®å¥–åŠ±ã€è¿åŠ¨å¹³æ»‘åº¦å¥–åŠ±å’ŒåŠ›é€‚åº”æ€§å¥–åŠ±ã€‚åŠ›éšæœºåŒ–æŠ€æœ¯é€šè¿‡åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å¼•å…¥éšæœºåŠ›ï¼Œæ¥æé«˜ç­–ç•¥çš„é²æ£’æ€§ã€‚æ§åˆ¶é¢‘ç‡ä¸º50Hzï¼Œä¿è¯äº†å®æ—¶æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„IKåŸºçº¿ç›¸æ¯”ï¼Œè¯¥å­¦ä¹ ç­–ç•¥åœ¨Unitree G1äººå½¢æœºå™¨äººä¸Šå®ç°äº†34%çš„è·Ÿè¸ªè¯¯å·®é™ä½ï¼Œ45%çš„è¿åŠ¨å¹³æ»‘åº¦æå‡ï¼Œä»¥åŠæ›´ä¼˜å¼‚çš„åŠ›é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¿æŒäº†50Hzçš„å®æ—¶æ§åˆ¶é¢‘ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦äººå½¢æœºå™¨äººè¿›è¡Œè¿œç¨‹æ“ä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚å±é™©ç¯å¢ƒä¸‹çš„æ•‘æ´ã€ç²¾å¯†ä»ªå™¨çš„ç»´æŠ¤ã€ä»¥åŠåŒ»ç–—æ‰‹æœ¯è¾…åŠ©ç­‰ã€‚é€šè¿‡æé«˜é¥æ“ä½œçš„è‡ªç„¶æ€§å’Œé²æ£’æ€§ï¼Œå¯ä»¥ä½¿æ“ä½œäººå‘˜æ›´é«˜æ•ˆã€æ›´å®‰å…¨åœ°å®Œæˆä»»åŠ¡ï¼Œå¹¶æ‰©å±•äººå½¢æœºå™¨äººçš„åº”ç”¨èŒƒå›´ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.

