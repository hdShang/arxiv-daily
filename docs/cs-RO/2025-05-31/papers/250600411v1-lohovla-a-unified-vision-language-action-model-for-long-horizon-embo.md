---
layout: default
title: "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks"
---

# LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00411" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.00411v1</a>
  <a href="https://arxiv.org/pdf/2506.00411.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00411v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00411v1', 'LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yi Yang, Jiaxuan Sun, Siqi Kou, Yihan Wang, Zhijie Deng

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-31

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫LoHoVLA‰ª•Ëß£ÂÜ≥ÈïøÊó∂Èó¥Ë∑®Â∫¶ÁöÑ‰ΩìÊÄÅ‰ªªÂä°ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÊó∂Èó¥Ë∑®Â∫¶‰ªªÂä°` `ËßÜËßâËØ≠Ë®ÄÊ®°Âûã` `Êú∫Âô®‰∫∫Âä®‰ΩúÊéßÂà∂` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Èó≠ÁéØÊéßÂà∂Êú∫Âà∂`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÂú®ÈïøÊó∂Èó¥Ë∑®Â∫¶‰ªªÂä°ÁöÑËßÑÂàíËÉΩÂäõ‰∏çË∂≥ÔºåÂàÜÂ±ÇÊû∂ÊûÑÂàôÈù¢‰∏¥ÂçèË∞ÉÈóÆÈ¢òÔºåÂΩ±ÂìçÊï¥‰ΩìÊÄßËÉΩ„ÄÇ
2. LoHoVLAÊ°ÜÊû∂ÈÄöËøáÁªìÂêàÂ§ßÂûãÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåËÅîÂêàÁîüÊàêËØ≠Ë®ÄÂíåÂä®‰ΩúÊ†áËÆ∞Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥Â•ΩÁöÑ‰ªªÂä°Ê≥õÂåñ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåLoHoVLAÂú®RavensÊ®°ÊãüÂô®ÁöÑÈïøÊó∂Èó¥Ë∑®Â∫¶‰ªªÂä°‰∏äÔºåÊÄßËÉΩÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÂàÜÂ±ÇÂíåÊ†áÂáÜVLAÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑ‰ΩìÊÄÅ‰ª£ÁêÜÈù¢‰∏¥ÈïøÊó∂Èó¥Ë∑®Â∫¶ÁöÑ‰ªªÂä°ÔºåËøô‰∫õ‰ªªÂä°ÈúÄË¶ÅÈ´òÂ±ÇÊ¨°ÁöÑÁõÆÊ†áÂíåÂ§öÊ≠•È™§ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÁé∞ÊúâÁöÑËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°ÂûãÂíåÂàÜÂ±ÇÊû∂ÊûÑÂú®ËßÑÂàíÂíåÂçèË∞ÉÊñπÈù¢Â≠òÂú®‰∏çË∂≥„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ°ÜÊû∂LoHoVLAÔºåÂà©Áî®Â§ßÂûãÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Âü∫Á°ÄÔºåËÅîÂêàÁîüÊàêËØ≠Ë®ÄÂíåÂä®‰ΩúÊ†áËÆ∞Ôºå‰ª•‰øÉËøõ‰ªªÂä°Èó¥ÁöÑÊõ¥Â•ΩÊ≥õÂåñ„ÄÇÊ≠§Â§ñÔºåLoHoVLAÈááÁî®ÂàÜÂ±ÇÈó≠ÁéØÊéßÂà∂Êú∫Âà∂Ôºå‰ª•ÂáèÂ∞ëÈ´òÂ±ÇËßÑÂàíÂíå‰ΩéÂ±ÇÊéßÂà∂Â∏¶Êù•ÁöÑËØØÂ∑Æ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLoHoVLAÂú®RavensÊ®°ÊãüÂô®‰∏≠ÁöÑÈïøÊó∂Èó¥Ë∑®Â∫¶‰ΩìÊÄÅ‰ªªÂä°‰∏äÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂàÜÂ±ÇÂíåÊ†áÂáÜVLAÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïøÊó∂Èó¥Ë∑®Â∫¶‰ΩìÊÄÅ‰ªªÂä°‰∏≠ÁöÑÈ´òÂ±ÇÊ¨°ÁõÆÊ†áËßÑÂàí‰∏é‰ΩéÂ±ÇÊ¨°Âä®‰ΩúÊéßÂà∂ÁöÑÂçèË∞ÉÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®ËøôÊñπÈù¢Â≠òÂú®ËßÑÂàí‰∏çË∂≥ÂíåÂçèË∞É‰∏çËâØÁöÑÁóõÁÇπ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöLoHoVLAÈÄöËøáÂà©Áî®Â§ßÂûãÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåËÅîÂêàÁîüÊàêËØ≠Ë®ÄÂíåÂä®‰ΩúÊ†áËÆ∞Ôºå‰ª•ÂÆûÁé∞È´òÊïàÁöÑÂ≠ê‰ªªÂä°ÁîüÊàêÂíåÊú∫Âô®‰∫∫Âä®‰ΩúÈ¢ÑÊµãÔºå‰ªéËÄåÊèêÂçá‰ªªÂä°ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöLoHoVLAÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™ËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰Ωú‰∏∫Âü∫Á°ÄÔºåÂàÜ‰∏∫È´òÂ±ÇÊ¨°‰ªªÂä°ËßÑÂàíÂíå‰ΩéÂ±ÇÊ¨°Âä®‰ΩúÊéßÂà∂‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºåÈááÁî®ÂàÜÂ±ÇÈó≠ÁéØÊéßÂà∂Êú∫Âà∂Êù•ÂáèÂ∞ëËØØÂ∑Æ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöLoHoVLAÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂ∞ÜËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏éÂàÜÂ±ÇÊéßÂà∂Êú∫Âà∂ÁªìÂêàÔºåÂΩ¢ÊàêÁªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÊòæËëóÊîπÂñÑ‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®ÈïøÊó∂Èó¥Ë∑®Â∫¶‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏≠ÔºåLoHoVLA‰ΩøÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Êù•‰ºòÂåñËØ≠Ë®ÄÂíåÂä®‰ΩúÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂Âú®ÁΩëÁªúÁªìÊûÑ‰∏äÈááÁî®‰∫ÜÈÄÇÂ∫îÊÄßÂèÇÊï∞ËÆæÁΩÆÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÂ≠¶‰π†ÊïàÁéáÂíåÁ®≥ÂÆöÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLoHoVLAÂú®RavensÊ®°ÊãüÂô®ÁöÑÈïøÊó∂Èó¥Ë∑®Â∫¶‰ΩìÊÄÅ‰ªªÂä°‰∏äÔºåÊÄßËÉΩÊòæËëóÊèêÂçáÔºåË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑÂàÜÂ±ÇÂíåÊ†áÂáÜVLAÊñπÊ≥ïÔºåÂÖ∑‰ΩìÊèêÂçáÂπÖÂ∫¶ËææÂà∞XX%ÔºàÂÖ∑‰ΩìÊï∞ÊçÆÂæÖË°•ÂÖÖÔºâ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

LoHoVLAÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êú∫Âô®‰∫∫ÂØºËà™„ÄÅÊô∫ËÉΩÂÆ∂Â±Ö„ÄÅ‰ª•Âèä‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑ‰ªªÂä°ÊâßË°åËÉΩÂäõÔºåËÉΩÂ§üÊé®Âä®Êô∫ËÉΩ‰ΩìÂú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÂ∫îÁî®ÔºåÂ¢ûÂº∫ÂÖ∂Ëá™‰∏ªÂÜ≥Á≠ñÂíåÊâßË°åËÉΩÂäõ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.

