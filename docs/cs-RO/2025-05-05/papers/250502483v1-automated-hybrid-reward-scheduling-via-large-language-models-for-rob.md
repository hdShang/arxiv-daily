---
layout: default
title: Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning
---

# Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02483" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.02483v1</a>
  <a href="https://arxiv.org/pdf/2505.02483.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02483v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02483v1', 'Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Changxin Huang, Junyang Liang, Yanbin Chang, Jingzhao Xu, Jianqiang Li

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-05

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Âü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËá™Âä®ÂåñÊ∑∑ÂêàÂ•ñÂä±Ë∞ÉÂ∫¶‰ª•ÊèêÂçáÊú∫Âô®‰∫∫ÊäÄËÉΩÂ≠¶‰π†**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÊäÄËÉΩÂ≠¶‰π†` `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Â•ñÂä±Ë∞ÉÂ∫¶` `Â§öÂàÜÊîØÁΩëÁªú` `Âä®ÊÄÅË∞ÉÊï¥` `ÊÄßËÉΩÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Â§ÑÁêÜÈ´òËá™Áî±Â∫¶Êú∫Âô®‰∫∫ÊäÄËÉΩÂ≠¶‰π†Êó∂ÔºåÈÄöÂ∏∏ÂØπÊâÄÊúâÂ•ñÂä±ÁªÑ‰ª∂ËøõË°åÁÆÄÂçïÁõ∏Âä†ÔºåÂØºËá¥Â≠¶‰π†ÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑAHRSÊ°ÜÊû∂Âà©Áî®Â§ßËØ≠Ë®ÄÊ®°ÂûãÂä®ÊÄÅË∞ÉÊï¥ÂêÑÂ•ñÂä±ÁªÑ‰ª∂ÁöÑÂ≠¶‰π†Âº∫Â∫¶Ôºå‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÊéåÊè°Â§çÊùÇÊäÄËÉΩ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAHRSÊñπÊ≥ïÂú®Â§ö‰∏™È´òËá™Áî±Â∫¶Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá6.48%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

È´òËá™Áî±Â∫¶Êú∫Âô®‰∫∫Â≠¶‰π†ÁâπÂÆöÊäÄËÉΩÈù¢‰∏¥Â§çÊùÇÁöÑÂä®ÊÄÅÊåëÊàò„ÄÇÂ∞ΩÁÆ°Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊòØ‰∏ÄÁßçÊúâÊïàÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÂØπÊâÄÊúâÂ•ñÂä±ÁªÑ‰ª∂ËøõË°åÁªü‰∏ÄÂ§ÑÁêÜÔºåÂØºËá¥Â≠¶‰π†ÊïàÁéá‰Ωé‰∏ã„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËá™Âä®ÂåñÊ∑∑ÂêàÂ•ñÂä±Ë∞ÉÂ∫¶ÔºàAHRSÔºâÊ°ÜÊû∂ÔºåÂä®ÊÄÅË∞ÉÊï¥ÂêÑÂ•ñÂä±ÁªÑ‰ª∂ÁöÑÂ≠¶‰π†Âº∫Â∫¶Ôºå‰ªéËÄå‰ΩøÊú∫Âô®‰∫∫‰ª•Ê∏êËøõÂíåÁªìÊûÑÂåñÁöÑÊñπÂºèÊéåÊè°ÊäÄËÉΩ„ÄÇÈÄöËøáËÆæËÆ°Â§öÂàÜÊîØ‰ª∑ÂÄºÁΩëÁªúÔºåÊØè‰∏™ÂàÜÊîØÂØπÂ∫î‰∏Ä‰∏™ÁâπÂÆöÁöÑÂ•ñÂä±ÁªÑ‰ª∂ÔºåAHRSÊñπÊ≥ïÂú®Â§öÈ°πÈ´òËá™Áî±Â∫¶Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá6.48%ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥È´òËá™Áî±Â∫¶Êú∫Âô®‰∫∫Âú®ÊäÄËÉΩÂ≠¶‰π†‰∏≠Èù¢‰∏¥ÁöÑÂ•ñÂä±ÂáΩÊï∞ËÆæËÆ°Â§çÊùÇÊÄßÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂØπÊâÄÊúâÂ•ñÂä±ÁªÑ‰ª∂ÁöÑÁªü‰∏ÄÂ§ÑÁêÜÂØºËá¥‰∫ÜÂ≠¶‰π†ÊïàÁéá‰Ωé‰∏ãÔºåÈôêÂà∂‰∫ÜÊú∫Âô®‰∫∫ÁöÑÂ≠¶‰π†ÊÄßËÉΩ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÁöÑAHRSÊ°ÜÊû∂ÈÄöËøáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂä®ÊÄÅË∞ÉÊï¥ÂêÑÂ•ñÂä±ÁªÑ‰ª∂ÁöÑÂ≠¶‰π†Âº∫Â∫¶Ôºå‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®ÊîøÁ≠ñ‰ºòÂåñËøáÁ®ã‰∏≠ÈÄêÊ≠•ÊéåÊè°ÊäÄËÉΩ„ÄÇËøôÁßçÊñπÊ≥ïÈÄöËøáÂ§öÂàÜÊîØ‰ª∑ÂÄºÁΩëÁªúÂÆûÁé∞ÔºåÊØè‰∏™ÂàÜÊîØÂØπÂ∫î‰∏Ä‰∏™ÁâπÂÆöÁöÑÂ•ñÂä±ÁªÑ‰ª∂„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöAHRSÊ°ÜÊû∂ÂåÖÊã¨Â§ö‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ¶ñÂÖàÔºåLLMÊ†πÊçÆ‰ªªÂä°ÊèèËø∞ÁîüÊàê‰∏ÄÂ•óËßÑÂàôÔºõÂÖ∂Ê¨°ÔºåÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÊ†πÊçÆËØ≠Ë®ÄÊèêÁ§∫ÈÄâÊã©ÂêàÈÄÇÁöÑÊùÉÈáçËÆ°ÁÆóËßÑÂàôÔºåÂä®ÊÄÅË∞ÉÊï¥ÊØè‰∏™Â•ñÂä±ÁªÑ‰ª∂ÁöÑÊùÉÈáç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöAHRSÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂà©Áî®Â§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁöÑËßÑÂàôÈõÜÊù•Âä®ÊÄÅË∞ÉÊï¥Â•ñÂä±ÁªÑ‰ª∂ÁöÑÊùÉÈáçÔºåËøô‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÈùôÊÄÅÂ§ÑÁêÜÊñπÂºèÂΩ¢ÊàêÈ≤úÊòéÂØπÊØîÔºåÊòæËëóÊèêÂçá‰∫ÜÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁΩëÁªúÁªìÊûÑ‰∏äÔºåËÆæËÆ°‰∫Ü‰∏Ä‰∏™Â§öÂàÜÊîØ‰ª∑ÂÄºÁΩëÁªúÔºåÊØè‰∏™ÂàÜÊîØÂØπÂ∫î‰∏Ä‰∏™Â•ñÂä±ÁªÑ‰ª∂„ÄÇÊùÉÈáçËÆ°ÁÆóËßÑÂàôÁî±LLMÁîüÊàêÔºåÁ°Æ‰øùÂú®ÊîøÁ≠ñ‰ºòÂåñËøáÁ®ã‰∏≠ËÉΩÂ§üÂÆûÊó∂ÂèçÊò†ÂêÑÂ•ñÂä±ÁªÑ‰ª∂ÁöÑÈáçË¶ÅÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåAHRSÊñπÊ≥ïÂú®Â§ö‰∏™È´òËá™Áî±Â∫¶Êú∫Âô®‰∫∫‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÂπ≥Âùá6.48%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÁõ∏ËæÉ‰∫é‰º†ÁªüÊñπÊ≥ïÂÖ∑ÊúâÊòæËëóÁöÑ‰ºòÂäø„ÄÇËøô‰∏ÄÁªìÊûúË°®ÊòéÔºåÂä®ÊÄÅË∞ÉÊï¥Â•ñÂä±ÁªÑ‰ª∂ÁöÑÂ≠¶‰π†Âº∫Â∫¶ËÉΩÂ§üÊúâÊïàÊèêÂçáÊú∫Âô®‰∫∫ÊäÄËÉΩÂ≠¶‰π†ÁöÑÊïàÁéá„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨È´òËá™Áî±Â∫¶Êú∫Âô®‰∫∫Âú®Â∑•‰∏öËá™Âä®Âåñ„ÄÅÊúçÂä°Êú∫Âô®‰∫∫‰ª•ÂèäÂåªÁñóÊú∫Âô®‰∫∫Á≠âÂ§ö‰∏™Âú∫ÊôØ„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫ÊäÄËÉΩÂ≠¶‰π†ÁöÑÊïàÁéáÔºåAHRSÊ°ÜÊû∂ËÉΩÂ§üÂä†ÈÄüÊú∫Âô®‰∫∫Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÈÄÇÂ∫îËÉΩÂäõÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Enabling a high-degree-of-freedom robot to learn specific skills is a challenging task due to the complexity of robotic dynamics. Reinforcement learning (RL) has emerged as a promising solution; however, addressing such problems requires the design of multiple reward functions to account for various constraints in robotic motion. Existing approaches typically sum all reward components indiscriminately to optimize the RL value function and policy. We argue that this uniform inclusion of all reward components in policy optimization is inefficient and limits the robot's learning performance. To address this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework based on Large Language Models (LLMs). This paradigm dynamically adjusts the learning intensity of each reward component throughout the policy optimization process, enabling robots to acquire skills in a gradual and structured manner. Specifically, we design a multi-branch value network, where each branch corresponds to a distinct reward component. During policy optimization, each branch is assigned a weight that reflects its importance, and these weights are automatically computed based on rules designed by LLMs. The LLM generates a rule set in advance, derived from the task description, and during training, it selects a weight calculation rule from the library based on language prompts that evaluate the performance of each branch. Experimental results demonstrate that the AHRS method achieves an average 6.48% performance improvement across multiple high-degree-of-freedom robotic tasks.

