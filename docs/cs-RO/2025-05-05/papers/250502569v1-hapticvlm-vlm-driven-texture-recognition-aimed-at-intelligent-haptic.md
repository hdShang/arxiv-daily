---
layout: default
title: "HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic Interaction"
---

# HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic Interaction

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02569" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02569v1</a>
  <a href="https://arxiv.org/pdf/2505.02569.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02569v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02569v1', 'HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic Interaction')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Muhammad Haris Khan, Miguel Altamirano Cabrera, Dmitrii Iarchuk, Yara Mahmoud, Daria Trinitatova, Issatay Tokmurziyev, Dzmitry Tsetserukou

**åˆ†ç±»**: cs.RO, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: Submitted to IEEE conf

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHapticVLMä»¥è§£å†³æ™ºèƒ½è§¦è§‰äº¤äº’ä¸­çš„ææ–™è¯†åˆ«é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€äº¤äº’` `è§¦è§‰åé¦ˆ` `ææ–™è¯†åˆ«` `ç¯å¢ƒæ„ŸçŸ¥` `æ·±åº¦å­¦ä¹ ` `è™šæ‹Ÿç°å®` `æ™ºèƒ½æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§¦è§‰äº¤äº’ç³»ç»Ÿåœ¨ææ–™è¯†åˆ«å’Œç¯å¢ƒæ„ŸçŸ¥æ–¹é¢å­˜åœ¨å±€é™ï¼Œéš¾ä»¥å®ç°å®æ—¶åé¦ˆã€‚
2. HapticVLMé€šè¿‡ç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹ä¸æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œæä¾›å‡†ç¡®çš„ææ–™è¯†åˆ«å’Œç¯å¢ƒæ¸©åº¦æ¨æ–­ï¼Œå¢å¼ºè§¦è§‰ä½“éªŒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒHapticVLMåœ¨ææ–™è¯†åˆ«å’Œæ¸©åº¦ä¼°è®¡æ–¹é¢åˆ†åˆ«è¾¾åˆ°äº†84.67%å’Œ86.7%çš„å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†HapticVLMï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€ç³»ç»Ÿï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€æ¨ç†ä¸æ·±åº¦å·ç§¯ç½‘ç»œï¼Œå®ç°å®æ—¶è§¦è§‰åé¦ˆã€‚HapticVLMåˆ©ç”¨åŸºäºConvNeXtçš„ææ–™è¯†åˆ«æ¨¡å—ç”Ÿæˆç¨³å¥çš„è§†è§‰åµŒå…¥ï¼Œä»¥å‡†ç¡®è¯†åˆ«ç‰©ä½“ææ–™ï¼ŒåŒæ—¶é‡‡ç”¨å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆQwen2-VL-2B-Instructï¼‰ä»ç¯å¢ƒçº¿ç´¢ä¸­æ¨æ–­ç¯å¢ƒæ¸©åº¦ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ‰¬å£°å™¨æä¾›æŒ¯åŠ¨è§¦è§‰åé¦ˆï¼Œå¹¶é€šè¿‡ä½©å°”å¸–æ¨¡å—æä¾›çƒ­çº¿ç´¢ï¼Œä»è€Œå¼¥åˆè§†è§‰æ„ŸçŸ¥ä¸è§¦è§‰ä½“éªŒä¹‹é—´çš„å·®è·ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨äº”ç§ä¸åŒçš„å¬è§‰-è§¦è§‰æ¨¡å¼ä¸‹ï¼Œå¹³å‡è¯†åˆ«å‡†ç¡®ç‡ä¸º84.67%ï¼Œåœ¨15ç§åœºæ™¯ä¸‹çš„æ¸©åº¦ä¼°è®¡å‡†ç¡®ç‡ä¸º86.7%ã€‚å°½ç®¡å‰æ™¯å¯æœŸï¼Œä½†å½“å‰ç ”ç©¶å—é™äºä½¿ç”¨çš„å°æ ·æœ¬æ¨¡å¼å’Œå‚ä¸è€…æ•°é‡ã€‚æœªæ¥çš„å·¥ä½œå°†é›†ä¸­åœ¨æ‰©å±•è§¦è§‰æ¨¡å¼èŒƒå›´å’Œå¢åŠ ç”¨æˆ·ç ”ç©¶ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–å’ŒéªŒè¯ç³»ç»Ÿæ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼ŒHapticVLMåœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šæ¨¡æ€è§¦è§‰äº¤äº’æ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œå…·æœ‰åœ¨è™šæ‹Ÿç°å®å’Œè¾…åŠ©æŠ€æœ¯ä¸­çš„æ½œåœ¨åº”ç”¨ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æ™ºèƒ½è§¦è§‰äº¤äº’ä¸­ææ–™è¯†åˆ«å’Œç¯å¢ƒæ¸©åº¦æ„ŸçŸ¥çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨å®æ—¶åé¦ˆå’Œå‡†ç¡®æ€§ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHapticVLMé€šè¿‡ç»“åˆè§†è§‰-è¯­è¨€æ¨ç†ä¸æ·±åº¦å·ç§¯ç½‘ç»œï¼Œåˆ©ç”¨è§†è§‰åµŒå…¥å’Œç¯å¢ƒçº¿ç´¢æ¥å®ç°æ›´ç²¾å‡†çš„è§¦è§‰åé¦ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç³»ç»Ÿä¸»è¦åŒ…æ‹¬ææ–™è¯†åˆ«æ¨¡å—ã€ç¯å¢ƒæ¸©åº¦æ¨æ–­æ¨¡å—å’Œè§¦è§‰åé¦ˆæ¨¡å—ã€‚ææ–™è¯†åˆ«æ¨¡å—åŸºäºConvNeXtæ¶æ„ï¼Œæ¸©åº¦æ¨æ–­æ¨¡å—ä½¿ç”¨Qwen2-VL-2B-Instructæ¨¡å‹ï¼Œè§¦è§‰åé¦ˆé€šè¿‡æ‰¬å£°å™¨å’Œä½©å°”å¸–æ¨¡å—å®ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šHapticVLMçš„åˆ›æ–°åœ¨äºå°†è§†è§‰-è¯­è¨€æ¨¡å‹ä¸æ·±åº¦å­¦ä¹ ç›¸ç»“åˆï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§¦è§‰äº¤äº’æ–¹å¼ï¼Œæ˜¾è‘—æå‡äº†ææ–™è¯†åˆ«å’Œç¯å¢ƒæ„ŸçŸ¥çš„å‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ææ–™è¯†åˆ«ä¸­ï¼Œé‡‡ç”¨ConvNeXtç½‘ç»œç»“æ„ä»¥æé«˜è§†è§‰åµŒå…¥çš„ç¨³å¥æ€§ï¼›æ¸©åº¦æ¨æ–­é‡‡ç”¨åŸºäºå®¹å¿åº¦çš„è¯„ä¼°æ–¹æ³•ï¼Œè®¾ç½®äº†8Â°Cçš„è¯¯å·®èŒƒå›´ä»¥ç¡®ä¿å‡†ç¡®æ€§ã€‚å®éªŒä¸­ä½¿ç”¨äº†äº”ç§å¬è§‰-è§¦è§‰æ¨¡å¼è¿›è¡Œè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHapticVLMåœ¨äº”ç§ä¸åŒçš„å¬è§‰-è§¦è§‰æ¨¡å¼ä¸‹å®ç°äº†84.67%çš„å¹³å‡è¯†åˆ«å‡†ç¡®ç‡ï¼Œæ¸©åº¦ä¼°è®¡å‡†ç¡®ç‡ä¸º86.7%ï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†ç³»ç»Ÿåœ¨å¤šæ¨¡æ€äº¤äº’ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

HapticVLMçš„ç ”ç©¶æˆæœåœ¨è™šæ‹Ÿç°å®å’Œè¾…åŠ©æŠ€æœ¯é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æä¾›æ›´çœŸå®çš„è§¦è§‰åé¦ˆï¼Œè¯¥ç³»ç»Ÿå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼Œç‰¹åˆ«æ˜¯åœ¨è¿œç¨‹æ“ä½œã€æ•™è‚²åŸ¹è®­å’ŒåŒ»ç–—åº·å¤ç­‰åœºæ™¯ä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºç”¨æˆ·çš„äº¤äº’æ„ŸçŸ¥å’Œæ“ä½œèƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces HapticVLM, a novel multimodal system that integrates vision-language reasoning with deep convolutional networks to enable real-time haptic feedback. HapticVLM leverages a ConvNeXt-based material recognition module to generate robust visual embeddings for accurate identification of object materials, while a state-of-the-art Vision-Language Model (Qwen2-VL-2B-Instruct) infers ambient temperature from environmental cues. The system synthesizes tactile sensations by delivering vibrotactile feedback through speakers and thermal cues via a Peltier module, thereby bridging the gap between visual perception and tactile experience. Experimental evaluations demonstrate an average recognition accuracy of 84.67% across five distinct auditory-tactile patterns and a temperature estimation accuracy of 86.7% based on a tolerance-based evaluation method with an 8Â°C margin of error across 15 scenarios. Although promising, the current study is limited by the use of a small set of prominent patterns and a modest participant pool. Future work will focus on expanding the range of tactile patterns and increasing user studies to further refine and validate the system's performance. Overall, HapticVLM presents a significant step toward context-aware, multimodal haptic interaction with potential applications in virtual reality, and assistive technologies.

