---
layout: default
title: Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments
---

# Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.26646" target="_blank" class="toolbar-btn">arXiv: 2510.26646v1</a>
    <a href="https://arxiv.org/pdf/2510.26646.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26646v1" 
            onclick="toggleFavorite(this, '2510.26646v1', 'Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Xiaoyi He, Danggui Chen, Zhenshuo Zhang, Zimeng Bai

**ÂàÜÁ±ª**: cs.RO, cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-30

**Â§áÊ≥®**: 6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation; evaluation with PathBench metrics; code (primary): https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Ê∑∑ÂêàDQN-TD3Âº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÁî®‰∫éÂä®ÊÄÅÁéØÂ¢É‰∏≠Ëá™‰∏ªÂØºËà™„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Ëá™‰∏ªÂØºËà™` `DQN` `TD3` `ÂàÜÂ±ÇÊéßÂà∂` `Êú∫Âô®‰∫∫` `Âä®ÊÄÅÁéØÂ¢É`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠Ëá™‰∏ªÂØºËà™Èù¢‰∏¥ÊåëÊàòÔºåÈöæ‰ª•ÂÖºÈ°æÂÖ®Â±ÄËßÑÂàíÂíåÂ±ÄÈÉ®ÊéßÂà∂„ÄÇ
2. ÈááÁî®ÂàÜÂ±ÇÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåDQNË¥üË¥£È´òÂ±ÇÂÜ≥Á≠ñÔºåTD3Ë¥üË¥£Â∫ïÂ±ÇÊéßÂà∂ÔºåÂÆûÁé∞ÂÖ®Â±ÄËßÑÂàí‰∏éÂ±ÄÈÉ®ÊéßÂà∂ÁöÑÁªìÂêà„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÊàêÂäüÁéáÂíåÊ†∑Êú¨ÊïàÁéáÊñπÈù¢‰ºò‰∫éÂçï‰∏ÄÁÆóÊ≥ïÂü∫Á∫øÂíåÂü∫‰∫éËßÑÂàôÁöÑËßÑÂàíÂô®„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàÜÂ±ÇË∑ØÂæÑËßÑÂàí‰∏éÊéßÂà∂Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÈ´òÂ±ÇÊ∑±Â∫¶QÁΩëÁªúÔºàDQNÔºâÁî®‰∫éÁ¶ªÊï£Â≠êÁõÆÊ†áÈÄâÊã©Ôºå‰ª•Âèä‰ΩéÂ±ÇÂèåÂª∂ËøüÊ∑±Â∫¶Á°ÆÂÆöÊÄßÁ≠ñÁï•Ê¢ØÂ∫¶ÔºàTD3ÔºâÊéßÂà∂Âô®Áî®‰∫éËøûÁª≠Âä®‰ΩúÊéßÂà∂„ÄÇÈ´òÂ±ÇÊ®°ÂùóÈÄâÊã©Ë°å‰∏∫ÂíåÂ≠êÁõÆÊ†áÔºõ‰ΩéÂ±ÇÊ®°ÂùóÊâßË°åÂπ≥ÊªëÁöÑÈÄüÂ∫¶Êåá‰ª§„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂÆûÁî®ÁöÑÂ•ñÂä±Â°ëÈÄ†ÊñπÊ°àÔºàÊñπÂêë„ÄÅË∑ùÁ¶ª„ÄÅÈÅøÈöú„ÄÅÂä®‰ΩúÂπ≥ÊªëÊÄß„ÄÅÁ¢∞ÊíûÊÉ©ÁΩö„ÄÅÊó∂Èó¥ÊÉ©ÁΩöÂíåËøõÂ∫¶ÔºâÔºå‰ª•Âèä‰∏Ä‰∏™Âü∫‰∫éÊøÄÂÖâÈõ∑ËææÁöÑÂÆâÂÖ®Èó®Ôºå‰ª•Èò≤Ê≠¢‰∏çÂÆâÂÖ®ÁöÑËøêÂä®„ÄÇËØ•Á≥ªÁªüÂú®ROS + GazeboÔºàTurtleBot3Ôºâ‰∏≠ÂÆûÁé∞ÔºåÂπ∂‰ΩøÁî®PathBenchÊåáÊ†áÔºàÂåÖÊã¨ÊàêÂäüÁéá„ÄÅÁ¢∞ÊíûÁéá„ÄÅË∑ØÂæÑÊïàÁéáÂíåÈáçËßÑÂàíÊïàÁéáÔºâÂú®Âä®ÊÄÅÂíåÈÉ®ÂàÜÂèØËßÇÂØüÁöÑÁéØÂ¢É‰∏≠ËøõË°åËØÑ‰º∞„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰∏éÂçï‰∏ÄÁÆóÊ≥ïÂü∫Á∫øÔºàÂçïÁã¨ÁöÑDQNÊàñTD3ÔºâÂíåÂü∫‰∫éËßÑÂàôÁöÑËßÑÂàíÂô®Áõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊèêÈ´ò‰∫ÜÊàêÂäüÁéáÂíåÊ†∑Êú¨ÊïàÁéáÔºåÂπ∂‰∏îÊõ¥Â•ΩÂú∞Ê≥õÂåñÂà∞Êú™ËßÅËøáÁöÑÈöúÁ¢çÁâ©ÈÖçÁΩÆÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÁ™ÅÂèëÁöÑÊéßÂà∂ÂèòÂåñ„ÄÇ‰ª£Á†ÅÂíåËØÑ‰º∞ËÑöÊú¨ÂèØÂú®È°πÁõÆÂ≠òÂÇ®Â∫ì‰∏≠ÊâæÂà∞„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÂú®Âä®ÊÄÅÂíåÈÉ®ÂàÜÂèØËßÇÊµãÁéØÂ¢É‰∏≠ÔºåËá™‰∏ªÂØºËà™ÈúÄË¶ÅÂêåÊó∂ËÄÉËôëÂÖ®Â±ÄË∑ØÂæÑËßÑÂàíÂíåÂ±ÄÈÉ®ËøêÂä®ÊéßÂà∂„ÄÇ‰º†ÁªüÊñπÊ≥ïÔºåÂ¶ÇÂçïÁã¨‰ΩøÁî®DQNÊàñTD3ÔºåÈöæ‰ª•Âú®Êé¢Á¥¢ÊïàÁéá„ÄÅÊ≥õÂåñËÉΩÂäõÂíåÊéßÂà∂Âπ≥ÊªëÊÄß‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇÂü∫‰∫éËßÑÂàôÁöÑËßÑÂàíÂô®Èöæ‰ª•ÈÄÇÂ∫îÂ§çÊùÇÂíåÊú™Áü•ÁöÑÁéØÂ¢É„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈááÁî®ÂàÜÂ±ÇÂº∫ÂåñÂ≠¶‰π†Êû∂ÊûÑÔºåÂ∞ÜÂØºËà™‰ªªÂä°ÂàÜËß£‰∏∫È´òÂ±ÇÁ¶ªÊï£ÂÜ≥Á≠ñÂíå‰ΩéÂ±ÇËøûÁª≠ÊéßÂà∂„ÄÇÈ´òÂ±ÇDQNË¥üË¥£ÈÄâÊã©Â≠êÁõÆÊ†áÔºåÂºïÂØºÂÖ®Â±ÄË∑ØÂæÑËßÑÂàíÔºõ‰ΩéÂ±ÇTD3Ë¥üË¥£ÊâßË°åÂπ≥ÊªëÁöÑÈÄüÂ∫¶Êåá‰ª§ÔºåÂÆûÁé∞Â±ÄÈÉ®ËøêÂä®ÊéßÂà∂„ÄÇËøôÁßçÂàÜÂ±ÇÁªìÊûÑËÉΩÂ§üÊúâÊïàÂà©Áî®DQNÁöÑÁ¶ªÊï£ÂÜ≥Á≠ñËÉΩÂäõÂíåTD3ÁöÑËøûÁª≠ÊéßÂà∂ËÉΩÂäõÔºåÊèêÈ´òÂØºËà™ÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊ°ÜÊû∂ÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÈ´òÂ±ÇDQNÂ≠êÁõÆÊ†áÈÄâÊã©Âô®Âíå‰ΩéÂ±ÇTD3ÈÄüÂ∫¶ÊéßÂà∂Âô®„ÄÇÈ¶ñÂÖàÔºåDQNÊ†πÊçÆÂΩìÂâçÁéØÂ¢ÉÁä∂ÊÄÅÈÄâÊã©‰∏Ä‰∏™Â≠êÁõÆÊ†á„ÄÇÁÑ∂ÂêéÔºåTD3ÊéßÂà∂Âô®Êé•Êî∂ËØ•Â≠êÁõÆÊ†áÔºåÂπ∂ÁîüÊàêÁõ∏Â∫îÁöÑÈÄüÂ∫¶Êåá‰ª§ÔºåÊéßÂà∂Êú∫Âô®‰∫∫ËøêÂä®„ÄÇÈÄöËøáROSÂíåGazeboÂπ≥Âè∞ËøõË°å‰ªøÁúüÂÆûÈ™å„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÊ∑∑Âêà‰ΩøÁî®DQNÂíåTD3ÔºåÂΩ¢Êàê‰∏Ä‰∏™‰∫íË°•ÁöÑÂº∫ÂåñÂ≠¶‰π†Á≥ªÁªü„ÄÇDQNÊìÖÈïøÁ¶ªÊï£Âä®‰ΩúÁ©∫Èó¥ÁöÑÂÜ≥Á≠ñÔºåTD3ÊìÖÈïøËøûÁª≠Âä®‰ΩúÁ©∫Èó¥ÁöÑÊéßÂà∂„ÄÇËøôÁßçÊ∑∑ÂêàÊñπÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑËá™‰∏ªÂØºËà™‰ªªÂä°ÔºåÊèêÈ´òÂØºËà™ÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂ•ñÂä±ÂáΩÊï∞ÁöÑËÆæËÆ°Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂåÖÊã¨ÊñπÂêëÂ•ñÂä±„ÄÅË∑ùÁ¶ªÂ•ñÂä±„ÄÅÈÅøÈöúÂ•ñÂä±„ÄÅÂä®‰ΩúÂπ≥ÊªëÊÄßÂ•ñÂä±„ÄÅÁ¢∞ÊíûÊÉ©ÁΩö„ÄÅÊó∂Èó¥ÊÉ©ÁΩöÂíåËøõÂ∫¶Â•ñÂä±„ÄÇÂü∫‰∫éÊøÄÂÖâÈõ∑ËææÁöÑÂÆâÂÖ®Èó®Áî®‰∫éÈò≤Ê≠¢‰∏çÂÆâÂÖ®ÁöÑËøêÂä®„ÄÇDQN‰ΩøÁî®Œµ-greedyÁ≠ñÁï•ËøõË°åÊé¢Á¥¢ÔºåTD3‰ΩøÁî®È´òÊñØÂô™Â£∞ËøõË°åÊé¢Á¥¢„ÄÇÁΩëÁªúÁöÑÂÖ∑‰ΩìÁªìÊûÑÔºàÂ±ÇÊï∞„ÄÅÁ•ûÁªèÂÖÉÊï∞ÈáèÁ≠âÔºâÊ†πÊçÆÂÆûÈ™åÁªìÊûúËøõË°åË∞ÉÊï¥„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê∑∑ÂêàDQN-TD3ÊñπÊ≥ïÂú®ÊàêÂäüÁéá„ÄÅÁ¢∞ÊíûÁéá„ÄÅË∑ØÂæÑÊïàÁéáÂíåÈáçËßÑÂàíÊïàÁéáÁ≠âÊñπÈù¢Âùá‰ºò‰∫éÂçï‰∏ÄÁÆóÊ≥ïÂü∫Á∫øÔºàDQNÊàñTD3ÔºâÂíåÂü∫‰∫éËßÑÂàôÁöÑËßÑÂàíÂô®„ÄÇÂ∞§ÂÖ∂ÊòØÂú®Êú™ËßÅËøáÁöÑÈöúÁ¢çÁâ©ÈÖçÁΩÆ‰∏≠ÔºåËØ•ÊñπÊ≥ïË°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÁ™ÅÂèëÁöÑÊéßÂà∂ÂèòÂåñ„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÂèØÂú®È°πÁõÆ‰ªìÂ∫ì‰∏≠ÊâæÂà∞„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÈúÄË¶ÅËá™‰∏ªÂØºËà™ÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÔºå‰æãÂ¶ÇÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÁâ©ÊµÅÊú∫Âô®‰∫∫„ÄÅÊó†‰∫∫È©æÈ©∂ËΩ¶ËæÜÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊú∫Âô®‰∫∫Âú®Â§çÊùÇÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂØºËà™ËÉΩÂäõÔºåÂèØ‰ª•ÊèêÂçáÂ∑•‰ΩúÊïàÁéáÔºåÈôç‰ΩéÂÆâÂÖ®È£éÈô©ÔºåÂπ∂Êâ©Â±ïÊú∫Âô®‰∫∫ÁöÑÂ∫îÁî®ËåÉÂõ¥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper presents a hierarchical path-planning and control framework that combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller for continuous actuation. The high-level module selects behaviors and sub-goals; the low-level module executes smooth velocity commands. We design a practical reward shaping scheme (direction, distance, obstacle avoidance, action smoothness, collision penalty, time penalty, and progress), together with a LiDAR-based safety gate that prevents unsafe motions. The system is implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics, including success rate, collision rate, path efficiency, and re-planning efficiency, in dynamic and partially observable environments. Experiments show improved success rate and sample efficiency over single-algorithm baselines (DQN or TD3 alone) and rule-based planners, with better generalization to unseen obstacle configurations and reduced abrupt control changes. Code and evaluation scripts are available at the project repository.

