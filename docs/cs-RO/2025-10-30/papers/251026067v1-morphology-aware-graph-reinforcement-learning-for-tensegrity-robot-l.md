---
layout: default
title: Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion
---

# Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26067" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26067v1</a>
  <a href="https://arxiv.org/pdf/2510.26067.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26067v1" onclick="toggleFavorite(this, '2510.26067v1', 'Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chi Zhang, Mingrui Li, Wenzhe Tong, Xiaonan Huang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§å½¢æ€æ„ŸçŸ¥å›¾å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¼ æ‹‰æ•´ä½“æœºå™¨äººè¿åŠ¨æ§åˆ¶ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ æ‹‰æ•´ä½“æœºå™¨äºº` `å¼ºåŒ–å­¦ä¹ ` `å›¾ç¥ç»ç½‘ç»œ` `è¿åŠ¨æ§åˆ¶` `å½¢æ€æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å¼ æ‹‰æ•´ä½“æœºå™¨äººæ§åˆ¶é¢ä¸´æ¬ é©±åŠ¨å’Œé«˜åº¦è€¦åˆåŠ¨åŠ›å­¦çš„æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå­¦ä¹ ã€‚
2. è®ºæ–‡æå‡ºå½¢æ€æ„ŸçŸ¥å›¾å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œæ•è·æœºå™¨äººç»„ä»¶é—´çš„è€¦åˆå…³ç³»ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ ·æœ¬æ•ˆç‡é«˜ï¼Œé²æ£’æ€§å¼ºï¼Œä¸”ç­–ç•¥èƒ½ç›´æ¥ä»ä»¿çœŸè¿ç§»åˆ°çœŸå®æœºå™¨äººã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ æ‹‰æ•´ä½“æœºå™¨äººç»“åˆäº†åˆšæ€§æ†å’Œå¼¹æ€§ç¼†ç´¢ï¼Œå…·æœ‰é«˜å¼¹æ€§å’Œå¯å±•å¼€æ€§ï¼Œä½†ç”±äºå…¶æ¬ é©±åŠ¨å’Œé«˜åº¦è€¦åˆçš„åŠ¨åŠ›å­¦ç‰¹æ€§ï¼Œå¯¹è¿åŠ¨æ§åˆ¶æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å½¢æ€æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰é›†æˆåˆ°è½¯æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•ä¸­ã€‚é€šè¿‡å°†æœºå™¨äººçš„ç‰©ç†æ‹“æ‰‘è¡¨ç¤ºä¸ºå›¾ï¼Œæ‰€æå‡ºçš„åŸºäºGNNçš„ç­–ç•¥èƒ½å¤Ÿæ•è·ç»„ä»¶ä¹‹é—´çš„è€¦åˆï¼Œä»è€Œå®ç°æ¯”ä¼ ç»Ÿå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ç­–ç•¥æ›´å¿«ã€æ›´ç¨³å®šçš„å­¦ä¹ ã€‚è¯¥æ–¹æ³•åœ¨ç‰©ç†3æ†å¼ æ‹‰æ•´ä½“æœºå™¨äººä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¶µç›–äº†ä¸‰ç§è¿åŠ¨åŸè¯­ï¼ŒåŒ…æ‹¬ç›´çº¿è·Ÿè¸ªå’ŒåŒå‘è½¬å¼¯ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰å“è¶Šçš„æ ·æœ¬æ•ˆç‡ã€å¯¹å™ªå£°å’Œåˆšåº¦å˜åŒ–çš„é²æ£’æ€§ä»¥åŠæ›´é«˜çš„è½¨è¿¹ç²¾åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥å¯ä»¥ç›´æ¥ä»ä»¿çœŸè½¬ç§»åˆ°ç¡¬ä»¶ï¼Œæ— éœ€å¾®è°ƒï¼Œä»è€Œå®ç°ç¨³å®šçš„çœŸå®ä¸–ç•Œè¿åŠ¨ã€‚è¿™äº›ç»“æœè¯æ˜äº†å°†ç»“æ„å…ˆéªŒçŸ¥è¯†èå…¥å¼ºåŒ–å­¦ä¹ å¯¹äºå¼ æ‹‰æ•´ä½“æœºå™¨äººæ§åˆ¶çš„ä¼˜åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šå¼ æ‹‰æ•´ä½“æœºå™¨äººç”±äºå…¶ç‹¬ç‰¹çš„ç»“æ„ç‰¹æ€§ï¼ˆåˆšæ€§æ†å’Œå¼¹æ€§ç¼†ç´¢çš„ç»„åˆï¼‰ï¼Œå‘ˆç°å‡ºæ¬ é©±åŠ¨å’Œé«˜åº¦è€¦åˆçš„åŠ¨åŠ›å­¦ç‰¹æ€§ã€‚è¿™ä½¿å¾—ä¼ ç»Ÿçš„æ§åˆ¶æ–¹æ³•ï¼Œå¦‚PIDæ§åˆ¶æˆ–åŸºäºæ¨¡å‹çš„æ§åˆ¶ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°è¿›è¡Œè¿åŠ¨æ§åˆ¶ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä½œä¸ºç­–ç•¥ç½‘ç»œï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æœºå™¨äººçš„ç»“æ„ä¿¡æ¯ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å¼ æ‹‰æ•´ä½“æœºå™¨äººçš„ç‰©ç†æ‹“æ‰‘ç»“æ„è¡¨ç¤ºä¸ºå›¾ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¥å­¦ä¹ æ§åˆ¶ç­–ç•¥ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒGNNèƒ½å¤Ÿæ•è·æœºå™¨äººå„ä¸ªç»„ä»¶ä¹‹é—´çš„è€¦åˆå…³ç³»ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£æœºå™¨äººçš„åŠ¨åŠ›å­¦è¡Œä¸ºã€‚è¿™ç§å½¢æ€æ„ŸçŸ¥çš„ç­–ç•¥å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿæé«˜å­¦ä¹ æ•ˆç‡ï¼Œå¢å¼ºç­–ç•¥çš„é²æ£’æ€§ï¼Œå¹¶å®ç°ä»ä»¿çœŸåˆ°çœŸå®ä¸–ç•Œçš„æ— ç¼è¿ç§»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•å°†GNNé›†æˆåˆ°è½¯æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆSACï¼‰ç®—æ³•ä¸­ï¼Œå½¢æˆä¸€ä¸ªå½¢æ€æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œå°†å¼ æ‹‰æ•´ä½“æœºå™¨äººçš„ç»“æ„è¡¨ç¤ºä¸ºå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨æœºå™¨äººç»„ä»¶ï¼ˆå¦‚æ†æˆ–ç¼†ç´¢ï¼‰ï¼Œè¾¹ä»£è¡¨ç»„ä»¶ä¹‹é—´çš„è¿æ¥å…³ç³»ã€‚ç„¶åï¼Œä½¿ç”¨GNNæ¥å¤„ç†è¿™ä¸ªå›¾ï¼Œæå–æœºå™¨äººçš„ç»“æ„ç‰¹å¾ã€‚è¿™äº›ç»“æ„ç‰¹å¾ä¸æœºå™¨äººçš„çŠ¶æ€ä¿¡æ¯ï¼ˆå¦‚ä½ç½®ã€é€Ÿåº¦ï¼‰ä¸€èµ·è¾“å…¥åˆ°SACç®—æ³•ä¸­ï¼Œç”¨äºå­¦ä¹ æ§åˆ¶ç­–ç•¥ã€‚SACç®—æ³•è´Ÿè´£ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç¯å¢ƒä¸­è·å¾—æœ€å¤§çš„å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å›¾ç¥ç»ç½‘ç»œå¼•å…¥åˆ°å¼ æ‹‰æ•´ä½“æœºå™¨äººçš„å¼ºåŒ–å­¦ä¹ æ§åˆ¶ä¸­ã€‚ä¸ä¼ ç»Ÿçš„MLPç­–ç•¥ç›¸æ¯”ï¼ŒGNNèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨æœºå™¨äººçš„ç»“æ„ä¿¡æ¯ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å®ç°äº†ä»ä»¿çœŸåˆ°çœŸå®ä¸–ç•Œçš„æ— ç¼è¿ç§»ï¼Œæ— éœ€è¿›è¡Œé¢å¤–çš„å¾®è°ƒã€‚

**å…³é”®è®¾è®¡**ï¼šGNNçš„å…·ä½“ç»“æ„å¯ä»¥æ ¹æ®æœºå™¨äººçš„å…·ä½“æ‹“æ‰‘ç»“æ„è¿›è¡Œè°ƒæ•´ã€‚è®ºæ–‡ä¸­ä½¿ç”¨çš„GNNåŒ…å«å¤šä¸ªå›¾å·ç§¯å±‚ï¼Œæ¯ä¸ªå›¾å·ç§¯å±‚è´Ÿè´£èšåˆæ¥è‡ªç›¸é‚»èŠ‚ç‚¹çš„ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨SACç®—æ³•ä¸­å¸¸ç”¨çš„è½¯Qå­¦ä¹ æŸå¤±å‡½æ•°å’Œç­–ç•¥æ¢¯åº¦æŸå¤±å‡½æ•°ã€‚å…³é”®å‚æ•°åŒ…æ‹¬GNNçš„å±‚æ•°ã€æ¯å±‚çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ã€å­¦ä¹ ç‡ç­‰ã€‚è¿™äº›å‚æ•°éœ€è¦æ ¹æ®å…·ä½“çš„æœºå™¨äººå’Œä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ç§è¿åŠ¨åŸè¯­ï¼ˆç›´çº¿è·Ÿè¸ªå’ŒåŒå‘è½¬å¼¯ï¼‰ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„MLPç­–ç•¥ã€‚ä¸MLPç­–ç•¥ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€æ›´å¼ºçš„é²æ£’æ€§ï¼ˆå¯¹å™ªå£°å’Œåˆšåº¦å˜åŒ–ï¼‰ä»¥åŠæ›´é«˜çš„è½¨è¿¹ç²¾åº¦ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥å¯ä»¥ç›´æ¥ä»ä»¿çœŸè½¬ç§»åˆ°çœŸå®ä¸–ç•Œçš„3æ†å¼ æ‹‰æ•´ä½“æœºå™¨äººä¸Šï¼Œæ— éœ€è¿›è¡Œé¢å¤–çš„å¾®è°ƒï¼Œå®ç°äº†ç¨³å®šçš„çœŸå®ä¸–ç•Œè¿åŠ¨ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§å¼ æ‹‰æ•´ä½“æœºå™¨äººçš„è¿åŠ¨æ§åˆ¶ï¼Œä¾‹å¦‚ç”¨äºæœç´¢æ•‘æ´ã€ç¯å¢ƒå‹˜æ¢ç­‰å¤æ‚ç¯å¢ƒä¸­çš„æœºå™¨äººã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–å…·æœ‰å¤æ‚æ‹“æ‰‘ç»“æ„çš„æœºå™¨äººæ§åˆ¶é—®é¢˜ï¼Œä¾‹å¦‚æŸ”æ€§æœºå™¨äººã€æ¨¡å—åŒ–æœºå™¨äººç­‰ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸçš„æ™ºèƒ½åŒ–å’Œè‡ªä¸»åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tensegrity robots combine rigid rods and elastic cables, offering high resilience and deployability but posing major challenges for locomotion control due to their underactuated and highly coupled dynamics. This paper introduces a morphology-aware reinforcement learning framework that integrates a graph neural network (GNN) into the Soft Actor-Critic (SAC) algorithm. By representing the robot's physical topology as a graph, the proposed GNN-based policy captures coupling among components, enabling faster and more stable learning than conventional multilayer perceptron (MLP) policies. The method is validated on a physical 3-bar tensegrity robot across three locomotion primitives, including straight-line tracking and bidirectional turning. It shows superior sample efficiency, robustness to noise and stiffness variations, and improved trajectory accuracy. Notably, the learned policies transfer directly from simulation to hardware without fine-tuning, achieving stable real-world locomotion. These results demonstrate the advantages of incorporating structural priors into reinforcement learning for tensegrity robot control.

