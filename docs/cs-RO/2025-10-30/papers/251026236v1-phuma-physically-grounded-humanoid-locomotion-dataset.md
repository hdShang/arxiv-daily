---
layout: default
title: "PHUMA: Physically-Grounded Humanoid Locomotion Dataset"
---

# PHUMA: Physically-Grounded Humanoid Locomotion Dataset

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26236" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26236v1</a>
  <a href="https://arxiv.org/pdf/2510.26236.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26236v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.26236v1', 'PHUMA: Physically-Grounded Humanoid Locomotion Dataset')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kyungmin Lee, Sibeen Kim, Minho Park, Hyunseung Kim, Dongyoon Hwang, Hojoon Lee, Jaegul Choo

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://davian-robotics.github.io/PHUMA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPHUMAï¼šä¸€ä¸ªç‰©ç†çº¦æŸçš„äººå½¢æœºå™¨äººè¿åŠ¨æ•°æ®é›†ï¼Œæå‡è¿åŠ¨æ¨¡ä»¿æ€§èƒ½ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `äººå½¢æœºå™¨äºº` `è¿åŠ¨æ¨¡ä»¿` `æ•°æ®é›†` `ç‰©ç†çº¦æŸ` `è¿åŠ¨æ§åˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¿åŠ¨æ¨¡ä»¿æ–¹æ³•ä¾èµ–é«˜è´¨é‡ä½†ç¨€ç¼ºçš„è¿åŠ¨æ•æ‰æ•°æ®ï¼Œæˆ–æ˜“å¼•å…¥ç‰©ç†ä¼ªå½±çš„å¤§è§„æ¨¡äº’è”ç½‘è§†é¢‘ï¼Œé™åˆ¶äº†æ€§èƒ½ã€‚
2. PHUMAé€šè¿‡æ•°æ®ç®¡ç†å’Œç‰©ç†çº¦æŸé‡å®šå‘ï¼Œåˆ©ç”¨å¤§è§„æ¨¡äººç±»è§†é¢‘æ„å»ºç‰©ç†ä¸Šå¯é çš„è¿åŠ¨æ•°æ®é›†ã€‚
3. å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨PHUMAè®­ç»ƒçš„ç­–ç•¥åœ¨æ¨¡ä»¿æœªè§è¿åŠ¨å’Œè·¯å¾„è·Ÿéšä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºHumanoid-Xå’ŒAMASSã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿åŠ¨æ¨¡ä»¿æ˜¯äººå½¢æœºå™¨äººè¿åŠ¨çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå®ƒä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè·å¾—ç±»ä¼¼äººç±»çš„è¡Œä¸ºã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºé«˜è´¨é‡çš„è¿åŠ¨æ•æ‰æ•°æ®é›†ï¼Œå¦‚AMASSï¼Œä½†è¿™äº›æ•°æ®é›†ç¨€ç¼ºä¸”æ˜‚è´µï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œå¤šæ ·æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶è¯•å›¾é€šè¿‡è½¬æ¢å¤§è§„æ¨¡äº’è”ç½‘è§†é¢‘æ¥æ‰©å±•æ•°æ®æ”¶é›†ï¼Œä¾‹å¦‚Humanoid-Xã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸å¼•å…¥ç‰©ç†ä¼ªå½±ï¼Œå¦‚æ¼‚æµ®ã€ç©¿é€å’Œæ»‘æ­¥ï¼Œè¿™é˜»ç¢äº†ç¨³å®šçš„æ¨¡ä»¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PHUMAï¼Œä¸€ä¸ªç‰©ç†çº¦æŸçš„äººå½¢æœºå™¨äººè¿åŠ¨æ•°æ®é›†ï¼Œå®ƒåˆ©ç”¨å¤§è§„æ¨¡çš„äººç±»è§†é¢‘ï¼ŒåŒæ—¶é€šè¿‡ä»”ç»†çš„æ•°æ®ç®¡ç†å’Œç‰©ç†çº¦æŸçš„é‡å®šå‘æ¥è§£å†³ç‰©ç†ä¼ªå½±ã€‚PHUMAå¼ºåˆ¶æ‰§è¡Œå…³èŠ‚é™åˆ¶ï¼Œç¡®ä¿åœ°é¢æ¥è§¦ï¼Œå¹¶æ¶ˆé™¤æ»‘æ­¥ï¼Œä»è€Œäº§ç”Ÿå¤§è§„æ¨¡ä¸”ç‰©ç†ä¸Šå¯é çš„è¿åŠ¨ã€‚æˆ‘ä»¬åœ¨ä¸¤ç»„æ¡ä»¶ä¸‹è¯„ä¼°äº†PHUMAï¼šï¼ˆiï¼‰æ¨¡ä»¿æ¥è‡ªè‡ªè®°å½•æµ‹è¯•è§†é¢‘çš„æœªè§è¿åŠ¨ï¼›ï¼ˆiiï¼‰ä»…ä½¿ç”¨éª¨ç›†å¼•å¯¼çš„è·¯å¾„è·Ÿéšã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼ŒPHUMAè®­ç»ƒçš„ç­–ç•¥éƒ½ä¼˜äºHumanoid-Xå’ŒAMASSï¼Œåœ¨æ¨¡ä»¿å„ç§è¿åŠ¨æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººå½¢æœºå™¨äººè¿åŠ¨æ¨¡ä»¿æ–¹æ³•é¢ä¸´æ•°æ®è´¨é‡å’Œè§„æ¨¡çš„æŒ‘æˆ˜ã€‚é«˜è´¨é‡çš„è¿åŠ¨æ•æ‰æ•°æ®é›†ï¼ˆå¦‚AMASSï¼‰æ•°æ®é‡å°ä¸”æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ‰©å±•ã€‚è€Œåˆ©ç”¨äº’è”ç½‘è§†é¢‘çš„æ–¹æ³•ï¼ˆå¦‚Humanoid-Xï¼‰è™½ç„¶å¯ä»¥æ‰©å¤§æ•°æ®è§„æ¨¡ï¼Œä½†å®¹æ˜“å¼•å…¥ç‰©ç†ä¸çœŸå®çš„ä¼ªå½±ï¼Œä¾‹å¦‚ç©¿é€ã€æ¼‚æµ®å’Œæ»‘æ­¥ï¼Œå¯¼è‡´è®­ç»ƒå‡ºçš„æœºå™¨äººç­–ç•¥ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPHUMAçš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨åˆ©ç”¨å¤§è§„æ¨¡äººç±»è§†é¢‘æ•°æ®çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡æ•°æ®æ¸…æ´—å’Œç‰©ç†çº¦æŸçš„é‡å®šå‘æŠ€æœ¯ï¼Œç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„äººå½¢æœºå™¨äººè¿åŠ¨æ•°æ®ã€‚é€šè¿‡å¼ºåˆ¶æ‰§è¡Œå…³èŠ‚é™åˆ¶ã€ç¡®ä¿åœ°é¢æ¥è§¦å’Œæ¶ˆé™¤æ»‘æ­¥ç­‰æ‰‹æ®µï¼Œä¿è¯æ•°æ®çš„ç‰©ç†å¯è¡Œæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPHUMAçš„æ•°æ®ç”Ÿæˆæµç¨‹ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š1) ä»å¤§è§„æ¨¡äº’è”ç½‘è§†é¢‘ä¸­æå–äººä½“è¿åŠ¨ä¿¡æ¯ï¼›2) å¯¹æå–çš„è¿åŠ¨æ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œå»é™¤å™ªå£°å’Œå¼‚å¸¸å€¼ï¼›3) å¯¹æ¸…æ´—åçš„æ•°æ®è¿›è¡Œç‰©ç†çº¦æŸçš„é‡å®šå‘ï¼ŒåŒ…æ‹¬å…³èŠ‚é™åˆ¶ã€åœ°é¢æ¥è§¦å’Œæ»‘æ­¥æ¶ˆé™¤ï¼›4) å°†é‡å®šå‘åçš„æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œç”¨äºè®­ç»ƒäººå½¢æœºå™¨äººçš„è¿åŠ¨æ¨¡ä»¿ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šPHUMAçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ç‰©ç†çº¦æŸçš„é‡å®šå‘æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¿è¯æ•°æ®è§„æ¨¡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜æ•°æ®çš„ç‰©ç†çœŸå®æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPHUMAèƒ½å¤Ÿç”Ÿæˆæ›´ç¨³å®šã€æ›´å¯é çš„è¿åŠ¨æ•°æ®ï¼Œä»è€Œæé«˜äººå½¢æœºå™¨äººçš„è¿åŠ¨æ¨¡ä»¿æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šPHUMAåœ¨ç‰©ç†çº¦æŸé‡å®šå‘ä¸­ï¼Œé‡‡ç”¨äº†å¤šç§æŠ€æœ¯æ‰‹æ®µã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ä¼˜åŒ–æ–¹æ³•æ¥å¼ºåˆ¶æ‰§è¡Œå…³èŠ‚é™åˆ¶ï¼Œé¿å…æœºå™¨äººå‡ºç°ä¸è‡ªç„¶çš„å§¿åŠ¿ã€‚ä½¿ç”¨æ¥è§¦æ£€æµ‹ç®—æ³•æ¥ç¡®ä¿æœºå™¨äººä¸åœ°é¢ä¿æŒæ¥è§¦ï¼Œé¿å…æ¼‚æµ®ç°è±¡ã€‚ä½¿ç”¨è¶³éƒ¨é€Ÿåº¦æ§åˆ¶ç®—æ³•æ¥æ¶ˆé™¤æ»‘æ­¥ï¼Œä¿è¯è¿åŠ¨çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒPHUMAè¿˜é‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æé«˜æ•°æ®çš„å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨PHUMAè®­ç»ƒçš„ç­–ç•¥åœ¨æ¨¡ä»¿æœªè§è¿åŠ¨å’Œè·¯å¾„è·Ÿéšä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºHumanoid-Xå’ŒAMASSã€‚åœ¨æ¨¡ä»¿æœªè§è¿åŠ¨ä»»åŠ¡ä¸­ï¼ŒPHUMAè®­ç»ƒçš„ç­–ç•¥èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ¨¡ä»¿äººç±»çš„è¿åŠ¨å§¿æ€å’Œè¿åŠ¨è½¨è¿¹ã€‚åœ¨è·¯å¾„è·Ÿéšä»»åŠ¡ä¸­ï¼ŒPHUMAè®­ç»ƒçš„ç­–ç•¥èƒ½å¤Ÿæ›´ç¨³å®šåœ°è·Ÿéšç›®æ ‡è·¯å¾„ï¼Œå¹¶é¿å…å‡ºç°æ‘”å€’ç­‰é—®é¢˜ã€‚å…·ä½“æ€§èƒ½æå‡å¹…åº¦æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†â€œsignificant gainsâ€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

PHUMAæ•°æ®é›†å¯å¹¿æ³›åº”ç”¨äºäººå½¢æœºå™¨äººçš„è¿åŠ¨æ§åˆ¶ã€è¿åŠ¨è§„åˆ’å’Œè¿åŠ¨æ¨¡ä»¿ç­‰é¢†åŸŸã€‚é€šè¿‡ä½¿ç”¨PHUMAè®­ç»ƒçš„ç­–ç•¥ï¼Œå¯ä»¥ä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„å„ç§è¿åŠ¨ï¼Œä¾‹å¦‚è¡Œèµ°ã€è·‘æ­¥ã€è·³è·ƒç­‰ã€‚è¿™å¯¹äºå¼€å‘å…·æœ‰æ›´å¼ºé€‚åº”æ€§å’Œæ™ºèƒ½æ€§çš„äººå½¢æœºå™¨äººå…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¾‹å¦‚åœ¨æœåŠ¡æœºå™¨äººã€æ•‘æ´æœºå™¨äººå’Œå¨±ä¹æœºå™¨äººç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Motion imitation is a promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on high-quality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation. In response, we introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting. PHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable. We evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. The code is available at https://davian-robotics.github.io/PHUMA.

