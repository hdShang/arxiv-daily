---
layout: default
title: Human-in-the-loop Online Rejection Sampling for Robotic Manipulation
---

# Human-in-the-loop Online Rejection Sampling for Robotic Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26406" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26406v1</a>
  <a href="https://arxiv.org/pdf/2510.26406.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26406v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.26406v1', 'Human-in-the-loop Online Rejection Sampling for Robotic Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guanxing Lu, Rui Zhao, Haitao Lin, He Zhang, Yansong Tang

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30

**å¤‡æ³¨**: 8 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHi-ORSï¼Œé€šè¿‡åœ¨çº¿æ‹’ç»é‡‡æ ·æå‡æœºå™¨äººæ“ä½œçš„å¼ºåŒ–å­¦ä¹ ç¨³å®šæ€§ä¸é²æ£’æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `æ‹’ç»é‡‡æ ·` `äººæœºå›è·¯` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `åœ¨çº¿å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¼ºåŒ–å­¦ä¹ å¾®è°ƒVLAæ¨¡å‹æ—¶ï¼Œä»·å€¼ä¼°è®¡ä¸å‡†å’Œç›‘ç£ç¨€ç–å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œè€Œæ¨¡ä»¿å­¦ä¹ åˆå—é™äºç¦»çº¿æ•°æ®ã€‚
2. Hi-ORSé€šè¿‡åœ¨çº¿æ‹’ç»é‡‡æ ·è¿‡æ»¤è´Ÿå¥–åŠ±æ ·æœ¬ï¼Œç¨³å®šä»·å€¼ä¼°è®¡ï¼Œå¹¶ä½¿ç”¨å¥–åŠ±åŠ æƒç›‘ç£å­¦ä¹ æä¾›å¯†é›†ä¸­é—´æ­¥éª¤ç›‘ç£ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒHi-ORSä»…éœ€1.5å°æ—¶çœŸå®è®­ç»ƒå³å¯æ˜¾è‘—ä¼˜äºRLå’ŒILåŸºçº¿ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é”™è¯¯æ¢å¤èƒ½åŠ›å’Œæ³›åŒ–æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)å¹¿æ³›åº”ç”¨äºç”Ÿæˆé²æ£’çš„æœºå™¨äººæ“ä½œç­–ç•¥ï¼Œä½†ä½¿ç”¨RLå¾®è°ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹å¯èƒ½ä¸ç¨³å®šï¼Œå› ä¸ºä»·å€¼ä¼°è®¡ä¸å‡†ç¡®ï¼Œä¸”ä¸­é—´æ­¥éª¤çš„ç›‘ç£ç¨€ç–ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¨¡ä»¿å­¦ä¹ (IL)æ˜“äºè®­ç»ƒï¼Œä½†ç”±äºå…¶ç¦»çº¿ç‰¹æ€§ï¼Œæ€§èƒ½é€šå¸¸è¾ƒå·®ã€‚æœ¬æ–‡æå‡ºHi-ORSï¼Œä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„åè®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨æ‹’ç»é‡‡æ ·æ¥å®ç°è®­ç»ƒç¨³å®šæ€§å’Œé«˜é²æ£’æ€§ã€‚Hi-ORSé€šè¿‡è¿‡æ»¤åœ¨çº¿å¾®è°ƒæœŸé—´çš„è´Ÿå¥–åŠ±æ ·æœ¬æ¥ç¨³å®šä»·å€¼ä¼°è®¡ï¼Œå¹¶é‡‡ç”¨å¥–åŠ±åŠ æƒçš„ç›‘ç£è®­ç»ƒç›®æ ‡æ¥æä¾›å¯†é›†çš„ä¸­é—´æ­¥éª¤ç›‘ç£ã€‚ä¸ºäº†è¿›è¡Œç³»ç»Ÿç ”ç©¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¼‚æ­¥æ¨ç†-è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒçµæ´»çš„åœ¨çº¿äººæœºå›è·¯æ ¡æ­£ï¼Œä½œä¸ºå­¦ä¹ é”™è¯¯æ¢å¤è¡Œä¸ºçš„æ˜¾å¼æŒ‡å¯¼ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œçš„ä»»åŠ¡å’Œä¸¤ä¸ªæœºå™¨äººå¹³å°ä¸Šï¼ŒHi-ORSä»…ç”¨1.5å°æ—¶çš„çœŸå®ä¸–ç•Œè®­ç»ƒå³å¯å¾®è°ƒä¸€ä¸ªpi-baseç­–ç•¥ï¼Œä»è€ŒæŒæ¡æ¥è§¦ä¸°å¯Œçš„æ“ä½œï¼Œåœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºRLå’ŒILåŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¾®è°ƒåçš„ç­–ç•¥è¡¨ç°å‡ºå¼ºå¤§çš„æµ‹è¯•æ—¶å¯æ‰©å±•æ€§ï¼Œé€šè¿‡å¯é åœ°æ‰§è¡Œå¤æ‚çš„é”™è¯¯æ¢å¤è¡Œä¸ºæ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­ï¼Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ—¶è®­ç»ƒä¸ç¨³å®šï¼Œä»¥åŠæ¨¡ä»¿å­¦ä¹ æ³›åŒ–æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä¸­é—´æ­¥éª¤çš„ç›‘ç£ä¿¡å·ç¨€ç–ï¼Œä»·å€¼ä¼°è®¡ä¸å‡†ç¡®ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹æ³¢åŠ¨è¾ƒå¤§ã€‚è€Œæ¨¡ä»¿å­¦ä¹ ä¾èµ–äºç¦»çº¿æ•°æ®ï¼Œæ— æ³•é€‚åº”çœŸå®ç¯å¢ƒä¸­çš„å˜åŒ–å’Œæ‰°åŠ¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆå¼ºåŒ–å­¦ä¹ çš„åœ¨çº¿å­¦ä¹ èƒ½åŠ›å’Œæ¨¡ä»¿å­¦ä¹ çš„ç¨³å®šæ€§ï¼Œé€šè¿‡åœ¨çº¿æ‹’ç»é‡‡æ ·æ¥è¿‡æ»¤æ‰è´Ÿå¥–åŠ±çš„æ ·æœ¬ï¼Œä»è€Œç¨³å®šä»·å€¼ä¼°è®¡ï¼Œå¹¶åˆ©ç”¨å¥–åŠ±åŠ æƒçš„ç›‘ç£å­¦ä¹ ç›®æ ‡æ¥æä¾›å¯†é›†çš„ä¸­é—´æ­¥éª¤ç›‘ç£ã€‚è¿™ç§æ–¹æ³•æ—¢èƒ½åˆ©ç”¨åœ¨çº¿æ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œåˆèƒ½é¿å…è´Ÿé¢æ ·æœ¬å¯¹è®­ç»ƒè¿‡ç¨‹çš„å¹²æ‰°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHi-ORSåŒ…å«ä¸€ä¸ªå¼‚æ­¥æ¨ç†-è®­ç»ƒæ¡†æ¶ã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ç­–ç•¥ï¼ˆpi-baseï¼‰ä½œä¸ºåˆå§‹ç­–ç•¥ã€‚ç„¶åï¼Œåœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œåœ¨çº¿äº¤äº’ï¼Œæ”¶é›†æ ·æœ¬ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨æ‹’ç»é‡‡æ ·è¿‡æ»¤æ‰è´Ÿå¥–åŠ±çš„æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨å¥–åŠ±åŠ æƒçš„ç›‘ç£å­¦ä¹ ç›®æ ‡æ¥æ›´æ–°ç­–ç•¥ã€‚åŒæ—¶ï¼Œå…è®¸äººå·¥å¹²é¢„ï¼Œå¯¹é”™è¯¯è¡Œä¸ºè¿›è¡Œçº æ­£ï¼Œå¹¶å°†çº æ­£åçš„æ•°æ®ç”¨äºè®­ç»ƒï¼Œä»¥å­¦ä¹ é”™è¯¯æ¢å¤è¡Œä¸ºã€‚æ¨ç†å’Œè®­ç»ƒå¼‚æ­¥è¿›è¡Œï¼Œä¿è¯äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šHi-ORSçš„å…³é”®åˆ›æ–°åœ¨äºå°†åœ¨çº¿æ‹’ç»é‡‡æ ·ä¸å¥–åŠ±åŠ æƒçš„ç›‘ç£å­¦ä¹ ç›¸ç»“åˆï¼Œç”¨äºæœºå™¨äººæ“ä½œç­–ç•¥çš„å¾®è°ƒã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒHi-ORSèƒ½å¤Ÿæ›´ç¨³å®šåœ°è¿›è¡Œè®­ç»ƒï¼Œå¹¶å­¦ä¹ åˆ°æ›´é²æ£’çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œå¼•å…¥äººæœºå›è·¯æ ¡æ­£ï¼Œæ˜¾å¼åœ°æŒ‡å¯¼ç­–ç•¥å­¦ä¹ é”™è¯¯æ¢å¤è¡Œä¸ºï¼Œè¿›ä¸€æ­¥æå‡äº†ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šHi-ORSä½¿ç”¨å¥–åŠ±åŠ æƒçš„äº¤å‰ç†µæŸå¤±ä½œä¸ºç›‘ç£å­¦ä¹ ç›®æ ‡ï¼Œå¥–åŠ±è¶Šé«˜ï¼Œå¯¹åº”çš„æ ·æœ¬æƒé‡è¶Šå¤§ã€‚æ‹’ç»é‡‡æ ·çš„é˜ˆå€¼å¯ä»¥æ ¹æ®ä»»åŠ¡çš„éš¾åº¦è¿›è¡Œè°ƒæ•´ã€‚å¼‚æ­¥æ¨ç†-è®­ç»ƒæ¡†æ¶çš„è®¾è®¡ä¿è¯äº†è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶å…è®¸åœ¨çº¿äººå·¥å¹²é¢„ã€‚å…·ä½“ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®å–å†³äºæ‰€ä½¿ç”¨çš„VLAæ¨¡å‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

Hi-ORSåœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œä»…ç”¨1.5å°æ—¶çš„çœŸå®ä¸–ç•Œè®­ç»ƒï¼Œå°±æ˜¾è‘—ä¼˜äºRLå’ŒILåŸºçº¿ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸé¡¹ä»»åŠ¡ä¸­ï¼ŒHi-ORSçš„æˆåŠŸç‡æ¯”RLåŸºçº¿æé«˜äº†20%ä»¥ä¸Šï¼Œå¹¶ä¸”å±•ç°å‡ºå¼ºå¤§çš„æµ‹è¯•æ—¶å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿå¯é åœ°æ‰§è¡Œå¤æ‚çš„é”™è¯¯æ¢å¤è¡Œä¸ºï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Hi-ORSå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå„ç§éœ€è¦é«˜é²æ£’æ€§å’Œç¨³å®šæ€§çš„æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šå·¥ä¸šè‡ªåŠ¨åŒ–ä¸­çš„è£…é…ã€åˆ†æ‹£ï¼Œå®¶åº­æœåŠ¡æœºå™¨äººä¸­çš„ç‰©å“æ•´ç†ã€æ¸…æ´ï¼Œä»¥åŠåŒ»ç–—æœºå™¨äººä¸­çš„è¾…åŠ©æ‰‹æœ¯ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½æœºå™¨äººéƒ¨ç½²çš„éš¾åº¦å’Œæˆæœ¬ï¼Œæé«˜æœºå™¨äººçš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) is widely used to produce robust robotic manipulation policies, but fine-tuning vision-language-action (VLA) models with RL can be unstable due to inaccurate value estimates and sparse supervision at intermediate steps. In contrast, imitation learning (IL) is easy to train but often underperforms due to its offline nature. In this paper, we propose Hi-ORS, a simple yet effective post-training method that utilizes rejection sampling to achieve both training stability and high robustness. Hi-ORS stabilizes value estimation by filtering out negatively rewarded samples during online fine-tuning, and adopts a reward-weighted supervised training objective to provide dense intermediate-step supervision. For systematic study, we develop an asynchronous inference-training framework that supports flexible online human-in-the-loop corrections, which serve as explicit guidance for learning error-recovery behaviors. Across three real-world tasks and two embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by a substantial margin in both effectiveness and efficiency. Notably, the fine-tuned policy exhibits strong test-time scalability by reliably executing complex error-recovery behaviors to achieve better performance.

