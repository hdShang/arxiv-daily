---
layout: default
title: Human-in-the-loop Online Rejection Sampling for Robotic Manipulation
---

# Human-in-the-loop Online Rejection Sampling for Robotic Manipulation

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.26406" target="_blank" class="toolbar-btn">arXiv: 2510.26406v1</a>
    <a href="https://arxiv.org/pdf/2510.26406.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26406v1" 
            onclick="toggleFavorite(this, '2510.26406v1', 'Human-in-the-loop Online Rejection Sampling for Robotic Manipulation')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Guanxing Lu, Rui Zhao, Haitao Lin, He Zhang, Yansong Tang

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-30

**Â§áÊ≥®**: 8 pages

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Hi-ORSÔºåÈÄöËøáÂú®Á∫øÊãíÁªùÈááÊ†∑ÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂº∫ÂåñÂ≠¶‰π†Á®≥ÂÆöÊÄß‰∏éÈ≤ÅÊ£íÊÄß**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êú∫Âô®‰∫∫Êìç‰Ωú` `Âº∫ÂåñÂ≠¶‰π†` `Ê®°‰ªøÂ≠¶‰π†` `ÊãíÁªùÈááÊ†∑` `‰∫∫Êú∫ÂõûË∑Ø` `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Âú®Á∫øÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉVLAÊ®°ÂûãÊó∂Ôºå‰ª∑ÂÄº‰º∞ËÆ°‰∏çÂáÜÂíåÁõëÁù£Á®ÄÁñèÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÔºåËÄåÊ®°‰ªøÂ≠¶‰π†ÂèàÂèóÈôê‰∫éÁ¶ªÁ∫øÊï∞ÊçÆ„ÄÇ
2. Hi-ORSÈÄöËøáÂú®Á∫øÊãíÁªùÈááÊ†∑ËøáÊª§Ë¥üÂ•ñÂä±Ê†∑Êú¨ÔºåÁ®≥ÂÆö‰ª∑ÂÄº‰º∞ËÆ°ÔºåÂπ∂‰ΩøÁî®Â•ñÂä±Âä†ÊùÉÁõëÁù£Â≠¶‰π†Êèê‰æõÂØÜÈõÜ‰∏≠Èó¥Ê≠•È™§ÁõëÁù£„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåHi-ORS‰ªÖÈúÄ1.5Â∞èÊó∂ÁúüÂÆûËÆ≠ÁªÉÂç≥ÂèØÊòæËëó‰ºò‰∫éRLÂíåILÂü∫Á∫øÔºåÂπ∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈîôËØØÊÅ¢Â§çËÉΩÂäõÂíåÊ≥õÂåñÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†(RL)ÂπøÊ≥õÂ∫îÁî®‰∫éÁîüÊàêÈ≤ÅÊ£íÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•Ôºå‰ΩÜ‰ΩøÁî®RLÂæÆË∞ÉËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÂèØËÉΩ‰∏çÁ®≥ÂÆöÔºåÂõ†‰∏∫‰ª∑ÂÄº‰º∞ËÆ°‰∏çÂáÜÁ°ÆÔºå‰∏î‰∏≠Èó¥Ê≠•È™§ÁöÑÁõëÁù£Á®ÄÁñè„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÊ®°‰ªøÂ≠¶‰π†(IL)Êòì‰∫éËÆ≠ÁªÉÔºå‰ΩÜÁî±‰∫éÂÖ∂Á¶ªÁ∫øÁâπÊÄßÔºåÊÄßËÉΩÈÄöÂ∏∏ËæÉÂ∑Æ„ÄÇÊú¨ÊñáÊèêÂá∫Hi-ORSÔºå‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÂêéËÆ≠ÁªÉÊñπÊ≥ïÔºåÂà©Áî®ÊãíÁªùÈááÊ†∑Êù•ÂÆûÁé∞ËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåÈ´òÈ≤ÅÊ£íÊÄß„ÄÇHi-ORSÈÄöËøáËøáÊª§Âú®Á∫øÂæÆË∞ÉÊúüÈó¥ÁöÑË¥üÂ•ñÂä±Ê†∑Êú¨Êù•Á®≥ÂÆö‰ª∑ÂÄº‰º∞ËÆ°ÔºåÂπ∂ÈááÁî®Â•ñÂä±Âä†ÊùÉÁöÑÁõëÁù£ËÆ≠ÁªÉÁõÆÊ†áÊù•Êèê‰æõÂØÜÈõÜÁöÑ‰∏≠Èó¥Ê≠•È™§ÁõëÁù£„ÄÇ‰∏∫‰∫ÜËøõË°åÁ≥ªÁªüÁ†îÁ©∂ÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÂºÇÊ≠•Êé®ÁêÜ-ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂ÊîØÊåÅÁÅµÊ¥ªÁöÑÂú®Á∫ø‰∫∫Êú∫ÂõûË∑ØÊ†°Ê≠£Ôºå‰Ωú‰∏∫Â≠¶‰π†ÈîôËØØÊÅ¢Â§çË°å‰∏∫ÁöÑÊòæÂºèÊåáÂØº„ÄÇÂú®‰∏â‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑ‰ªªÂä°Âíå‰∏§‰∏™Êú∫Âô®‰∫∫Âπ≥Âè∞‰∏äÔºåHi-ORS‰ªÖÁî®1.5Â∞èÊó∂ÁöÑÁúüÂÆû‰∏ñÁïåËÆ≠ÁªÉÂç≥ÂèØÂæÆË∞É‰∏Ä‰∏™pi-baseÁ≠ñÁï•Ôºå‰ªéËÄåÊéåÊè°Êé•Ëß¶‰∏∞ÂØåÁöÑÊìç‰ΩúÔºåÂú®ÊúâÊïàÊÄßÂíåÊïàÁéáÊñπÈù¢Âùá‰ºò‰∫éRLÂíåILÂü∫Á∫ø„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂæÆË∞ÉÂêéÁöÑÁ≠ñÁï•Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊµãËØïÊó∂ÂèØÊâ©Â±ïÊÄßÔºåÈÄöËøáÂèØÈù†Âú∞ÊâßË°åÂ§çÊùÇÁöÑÈîôËØØÊÅ¢Â§çË°å‰∏∫Êù•ÂÆûÁé∞Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÊó∂ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÔºå‰ª•ÂèäÊ®°‰ªøÂ≠¶‰π†Ê≥õÂåñÊÄß‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®‰∏≠Èó¥Ê≠•È™§ÁöÑÁõëÁù£‰ø°Âè∑Á®ÄÁñèÔºå‰ª∑ÂÄº‰º∞ËÆ°‰∏çÂáÜÁ°ÆÔºåÂØºËá¥ËÆ≠ÁªÉËøáÁ®ãÊ≥¢Âä®ËæÉÂ§ß„ÄÇËÄåÊ®°‰ªøÂ≠¶‰π†‰æùËµñ‰∫éÁ¶ªÁ∫øÊï∞ÊçÆÔºåÊó†Ê≥ïÈÄÇÂ∫îÁúüÂÆûÁéØÂ¢É‰∏≠ÁöÑÂèòÂåñÂíåÊâ∞Âä®„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÁªìÂêàÂº∫ÂåñÂ≠¶‰π†ÁöÑÂú®Á∫øÂ≠¶‰π†ËÉΩÂäõÂíåÊ®°‰ªøÂ≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄßÔºåÈÄöËøáÂú®Á∫øÊãíÁªùÈááÊ†∑Êù•ËøáÊª§ÊéâË¥üÂ•ñÂä±ÁöÑÊ†∑Êú¨Ôºå‰ªéËÄåÁ®≥ÂÆö‰ª∑ÂÄº‰º∞ËÆ°ÔºåÂπ∂Âà©Áî®Â•ñÂä±Âä†ÊùÉÁöÑÁõëÁù£Â≠¶‰π†ÁõÆÊ†áÊù•Êèê‰æõÂØÜÈõÜÁöÑ‰∏≠Èó¥Ê≠•È™§ÁõëÁù£„ÄÇËøôÁßçÊñπÊ≥ïÊó¢ËÉΩÂà©Áî®Âú®Á∫øÊï∞ÊçÆËøõË°åÂ≠¶‰π†ÔºåÂèàËÉΩÈÅøÂÖçË¥üÈù¢Ê†∑Êú¨ÂØπËÆ≠ÁªÉËøáÁ®ãÁöÑÂπ≤Êâ∞„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöHi-ORSÂåÖÂê´‰∏Ä‰∏™ÂºÇÊ≠•Êé®ÁêÜ-ËÆ≠ÁªÉÊ°ÜÊû∂„ÄÇÈ¶ñÂÖàÔºå‰ΩøÁî®‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÁöÑÁ≠ñÁï•Ôºàpi-baseÔºâ‰Ωú‰∏∫ÂàùÂßãÁ≠ñÁï•„ÄÇÁÑ∂ÂêéÔºåÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°åÂú®Á∫ø‰∫§‰∫íÔºåÊî∂ÈõÜÊ†∑Êú¨„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰ΩøÁî®ÊãíÁªùÈááÊ†∑ËøáÊª§ÊéâË¥üÂ•ñÂä±ÁöÑÊ†∑Êú¨ÔºåÂπ∂‰ΩøÁî®Â•ñÂä±Âä†ÊùÉÁöÑÁõëÁù£Â≠¶‰π†ÁõÆÊ†áÊù•Êõ¥Êñ∞Á≠ñÁï•„ÄÇÂêåÊó∂ÔºåÂÖÅËÆ∏‰∫∫Â∑•Âπ≤È¢ÑÔºåÂØπÈîôËØØË°å‰∏∫ËøõË°åÁ∫†Ê≠£ÔºåÂπ∂Â∞ÜÁ∫†Ê≠£ÂêéÁöÑÊï∞ÊçÆÁî®‰∫éËÆ≠ÁªÉÔºå‰ª•Â≠¶‰π†ÈîôËØØÊÅ¢Â§çË°å‰∏∫„ÄÇÊé®ÁêÜÂíåËÆ≠ÁªÉÂºÇÊ≠•ËøõË°åÔºå‰øùËØÅ‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöHi-ORSÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂ∞ÜÂú®Á∫øÊãíÁªùÈááÊ†∑‰∏éÂ•ñÂä±Âä†ÊùÉÁöÑÁõëÁù£Â≠¶‰π†Áõ∏ÁªìÂêàÔºåÁî®‰∫éÊú∫Âô®‰∫∫Êìç‰ΩúÁ≠ñÁï•ÁöÑÂæÆË∞É„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåHi-ORSËÉΩÂ§üÊõ¥Á®≥ÂÆöÂú∞ËøõË°åËÆ≠ÁªÉÔºåÂπ∂Â≠¶‰π†Âà∞Êõ¥È≤ÅÊ£íÁöÑÁ≠ñÁï•„ÄÇÊ≠§Â§ñÔºåÂºïÂÖ•‰∫∫Êú∫ÂõûË∑ØÊ†°Ê≠£ÔºåÊòæÂºèÂú∞ÊåáÂØºÁ≠ñÁï•Â≠¶‰π†ÈîôËØØÊÅ¢Â§çË°å‰∏∫ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÁ≠ñÁï•ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöHi-ORS‰ΩøÁî®Â•ñÂä±Âä†ÊùÉÁöÑ‰∫§ÂèâÁÜµÊçüÂ§±‰Ωú‰∏∫ÁõëÁù£Â≠¶‰π†ÁõÆÊ†áÔºåÂ•ñÂä±Ë∂äÈ´òÔºåÂØπÂ∫îÁöÑÊ†∑Êú¨ÊùÉÈáçË∂äÂ§ß„ÄÇÊãíÁªùÈááÊ†∑ÁöÑÈòàÂÄºÂèØ‰ª•Ê†πÊçÆ‰ªªÂä°ÁöÑÈöæÂ∫¶ËøõË°åË∞ÉÊï¥„ÄÇÂºÇÊ≠•Êé®ÁêÜ-ËÆ≠ÁªÉÊ°ÜÊû∂ÁöÑËÆæËÆ°‰øùËØÅ‰∫ÜËÆ≠ÁªÉÊïàÁéáÔºåÂêåÊó∂ÂÖÅËÆ∏Âú®Á∫ø‰∫∫Â∑•Âπ≤È¢Ñ„ÄÇÂÖ∑‰ΩìÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ËÆæÁΩÆÂèñÂÜ≥‰∫éÊâÄ‰ΩøÁî®ÁöÑVLAÊ®°Âûã„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Hi-ORSÂú®‰∏â‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ôºå‰ªÖÁî®1.5Â∞èÊó∂ÁöÑÁúüÂÆû‰∏ñÁïåËÆ≠ÁªÉÔºåÂ∞±ÊòæËëó‰ºò‰∫éRLÂíåILÂü∫Á∫ø„ÄÇ‰æãÂ¶ÇÔºåÂú®ÊüêÈ°π‰ªªÂä°‰∏≠ÔºåHi-ORSÁöÑÊàêÂäüÁéáÊØîRLÂü∫Á∫øÊèêÈ´ò‰∫Ü20%‰ª•‰∏äÔºåÂπ∂‰∏îÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊµãËØïÊó∂ÂèØÊâ©Â±ïÊÄßÔºåËÉΩÂ§üÂèØÈù†Âú∞ÊâßË°åÂ§çÊùÇÁöÑÈîôËØØÊÅ¢Â§çË°å‰∏∫ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÊÄßËÉΩ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

Hi-ORSÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºåÂèØÁî®‰∫éÂêÑÁßçÈúÄË¶ÅÈ´òÈ≤ÅÊ£íÊÄßÂíåÁ®≥ÂÆöÊÄßÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°Ôºå‰æãÂ¶ÇÔºöÂ∑•‰∏öËá™Âä®Âåñ‰∏≠ÁöÑË£ÖÈÖç„ÄÅÂàÜÊã£ÔºåÂÆ∂Â∫≠ÊúçÂä°Êú∫Âô®‰∫∫‰∏≠ÁöÑÁâ©ÂìÅÊï¥ÁêÜ„ÄÅÊ∏ÖÊ¥ÅÔºå‰ª•ÂèäÂåªÁñóÊú∫Âô®‰∫∫‰∏≠ÁöÑËæÖÂä©ÊâãÊúØÁ≠â„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊòæËëóÈôç‰ΩéÊú∫Âô®‰∫∫ÈÉ®ÁΩ≤ÁöÑÈöæÂ∫¶ÂíåÊàêÊú¨ÔºåÊèêÈ´òÊú∫Âô®‰∫∫ÁöÑÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) is widely used to produce robust robotic manipulation policies, but fine-tuning vision-language-action (VLA) models with RL can be unstable due to inaccurate value estimates and sparse supervision at intermediate steps. In contrast, imitation learning (IL) is easy to train but often underperforms due to its offline nature. In this paper, we propose Hi-ORS, a simple yet effective post-training method that utilizes rejection sampling to achieve both training stability and high robustness. Hi-ORS stabilizes value estimation by filtering out negatively rewarded samples during online fine-tuning, and adopts a reward-weighted supervised training objective to provide dense intermediate-step supervision. For systematic study, we develop an asynchronous inference-training framework that supports flexible online human-in-the-loop corrections, which serve as explicit guidance for learning error-recovery behaviors. Across three real-world tasks and two embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by a substantial margin in both effectiveness and efficiency. Notably, the fine-tuned policy exhibits strong test-time scalability by reliably executing complex error-recovery behaviors to achieve better performance.

