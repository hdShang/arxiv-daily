---
layout: default
title: "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models"
---

# NaviTrace: Evaluating Embodied Navigation of Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.26909" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.26909v2</a>
  <a href="https://arxiv.org/pdf/2510.26909.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.26909v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.26909v2', 'NaviTrace: Evaluating Embodied Navigation of Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tim Windecker, Manthan Patel, Moritz Reuss, Richard Schwarzkopf, Cesar Cadena, Rudolf Lioutikov, Marco Hutter, Jonas Frey

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-30 (æ›´æ–°: 2025-11-04)

**å¤‡æ³¨**: 9 pages, 6 figures, under review at IEEE conference

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://leggedrobotics.github.io/navitrace_webpage/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**NaviTraceï¼šæå‡ºè§†è§‰-è¯­è¨€æ¨¡å‹å…·èº«å¯¼èˆªè¯„æµ‹åŸºå‡†ï¼Œè§£å†³çœŸå®æœºå™¨äººå¯¼èˆªè¯„ä¼°éš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `æœºå™¨äººå¯¼èˆª` `å…·èº«æ™ºèƒ½` `è§†è§‰é—®ç­”` `è½¨è¿¹é¢„æµ‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººå¯¼èˆªç³»ç»Ÿé›†æˆè§†è§‰-è¯­è¨€æ¨¡å‹é¢ä¸´è¯„ä¼°éš¾é¢˜ï¼ŒçœŸå®ç¯å¢ƒæˆæœ¬é«˜ï¼Œä»¿çœŸç¯å¢ƒè¿‡äºç®€åŒ–ï¼Œç¼ºä¹æœ‰æ•ˆåŸºå‡†ã€‚
2. NaviTraceæå‡ºä¸€ç§æ–°çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œé€šè¿‡æŒ‡ä»¤å’Œå…·èº«ç±»å‹è¾“å…¥ï¼Œè¦æ±‚æ¨¡å‹è¾“å‡ºå›¾åƒç©ºé—´çš„2Då¯¼èˆªè½¨è¿¹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰VLMåœ¨ç©ºé—´å®šä½å’Œç›®æ ‡å®šä½æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸äººç±»è¡¨ç°å­˜åœ¨å·®è·ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å„ç§ä»»åŠ¡å’Œåœºæ™¯ä¸­è¡¨ç°å‡ºå‰æ‰€æœªæœ‰çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å°†è¿™äº›åŸºç¡€æ¨¡å‹é›†æˆåˆ°æœºå™¨äººå¯¼èˆªç³»ç»Ÿä¸­ï¼Œä¸ºæ„å»ºé€šç”¨æœºå™¨äººå¼€è¾Ÿäº†é“è·¯ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„å¯¼èˆªèƒ½åŠ›ä»ç„¶å—åˆ°æ˜‚è´µçš„çœŸå®ä¸–ç•Œè¯•éªŒã€è¿‡åº¦ç®€åŒ–çš„æ¨¡æ‹Ÿå’Œæœ‰é™çš„åŸºå‡†çš„é™åˆ¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†NaviTraceï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œæ¨¡å‹æ¥æ”¶æŒ‡ä»¤å’Œå…·èº«ç±»å‹ï¼ˆäººã€è…¿å¼æœºå™¨äººã€è½®å¼æœºå™¨äººã€è‡ªè¡Œè½¦ï¼‰ï¼Œå¹¶ä¸”å¿…é¡»åœ¨å›¾åƒç©ºé—´ä¸­è¾“å‡º2Då¯¼èˆªè½¨è¿¹ã€‚åœ¨1000ä¸ªåœºæ™¯å’Œ3000å¤šä¸ªä¸“å®¶è½¨è¿¹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–°å¼•å…¥çš„è¯­ä¹‰æ„ŸçŸ¥è½¨è¿¹åˆ†æ•°ç³»ç»Ÿåœ°è¯„ä¼°äº†å…«ä¸ªæœ€å…ˆè¿›çš„VLMã€‚è¯¥æŒ‡æ ‡ç»“åˆäº†åŠ¨æ€æ—¶é—´è§„æ•´è·ç¦»ã€ç›®æ ‡ç»ˆç‚¹è¯¯å·®ä»¥åŠä»æ¯åƒç´ è¯­ä¹‰å¯¼å‡ºçš„å…·èº«æ¡ä»¶æƒ©ç½šï¼Œå¹¶ä¸äººç±»åå¥½ç›¸å…³ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†ç”±äºä¸è‰¯çš„ç©ºé—´å®šä½å’Œç›®æ ‡å®šä½å¯¼è‡´ä¸äººç±»è¡¨ç°çš„æŒç»­å·®è·ã€‚NaviTraceä¸ºçœŸå®ä¸–ç•Œæœºå™¨äººå¯¼èˆªå»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å¯é‡å¤çš„åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººå¯¼èˆªä¸­çš„åº”ç”¨ç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚çœŸå®ç¯å¢ƒæµ‹è¯•æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥å¤ç°ï¼Œè€Œç°æœ‰ä»¿çœŸç¯å¢ƒåˆè¿‡äºç®€åŒ–ï¼Œæ— æ³•çœŸå®åæ˜ å®é™…å¯¼èˆªåœºæ™¯çš„å¤æ‚æ€§ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªé«˜è´¨é‡ã€å¯æ‰©å±•ä¸”å¯é‡å¤çš„åŸºå‡†æ¥è¯„ä¼°VLMçš„å¯¼èˆªèƒ½åŠ›ï¼Œå¹¶å‘ç°å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šNaviTraceçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªè§†è§‰é—®ç­”åŸºå‡†ï¼Œé€šè¿‡å‘æ¨¡å‹æä¾›æŒ‡ä»¤å’Œå…·èº«ç±»å‹ï¼ˆä¾‹å¦‚ï¼Œäººã€è…¿å¼æœºå™¨äººã€è½®å¼æœºå™¨äººã€è‡ªè¡Œè½¦ï¼‰ï¼Œè¦æ±‚æ¨¡å‹åœ¨å›¾åƒç©ºé—´ä¸­è¾“å‡º2Då¯¼èˆªè½¨è¿¹ã€‚è¿™ç§æ–¹æ³•å°†å¯¼èˆªé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªè½¨è¿¹é¢„æµ‹é—®é¢˜ï¼Œä»è€Œå¯ä»¥ä½¿ç”¨å„ç§æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥ä¸åŒç±»å‹çš„å…·èº«ï¼Œå¯ä»¥è€ƒå¯Ÿæ¨¡å‹å¯¹ä¸åŒè¿åŠ¨æ–¹å¼çš„é€‚åº”èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šNaviTraceåŸºå‡†åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š1) åŒ…å«1000ä¸ªä¸åŒåœºæ™¯çš„æ•°æ®é›†ï¼Œæ¯ä¸ªåœºæ™¯åŒ…å«è§†è§‰ä¿¡æ¯å’Œå¯¼èˆªæŒ‡ä»¤ï¼›2) è¶…è¿‡3000æ¡ä¸“å®¶å¯¼èˆªè½¨è¿¹ï¼Œä½œä¸ºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°çš„ground truthï¼›3) ä¸€ç§æ–°çš„è¯­ä¹‰æ„ŸçŸ¥è½¨è¿¹è¯„åˆ†æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„è½¨è¿¹ä¸ä¸“å®¶è½¨è¿¹çš„ç›¸ä¼¼åº¦ã€‚è¯¥æŒ‡æ ‡ç»¼åˆè€ƒè™‘äº†åŠ¨æ€æ—¶é—´è§„æ•´è·ç¦»ã€ç›®æ ‡ç»ˆç‚¹è¯¯å·®ä»¥åŠä»æ¯åƒç´ è¯­ä¹‰å¯¼å‡ºçš„å…·èº«æ¡ä»¶æƒ©ç½šã€‚

**å…³é”®åˆ›æ–°**ï¼šNaviTraceçš„å…³é”®åˆ›æ–°åœ¨äºï¼š1) æå‡ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°VLMåœ¨æœºå™¨äººå¯¼èˆªä¸­çš„èƒ½åŠ›ï¼›2) å¼•å…¥äº†ä¸€ç§æ–°çš„è¯­ä¹‰æ„ŸçŸ¥è½¨è¿¹è¯„åˆ†æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹ç”Ÿæˆçš„è½¨è¿¹ä¸äººç±»åå¥½çš„ç›¸å…³æ€§ï¼›3) ç³»ç»Ÿåœ°è¯„ä¼°äº†å…«ä¸ªæœ€å…ˆè¿›çš„VLMï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨ç©ºé—´å®šä½å’Œç›®æ ‡å®šä½æ–¹é¢çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šè¯­ä¹‰æ„ŸçŸ¥è½¨è¿¹è¯„åˆ†æŒ‡æ ‡æ˜¯NaviTraceçš„å…³é”®è®¾è®¡ä¹‹ä¸€ã€‚è¯¥æŒ‡æ ‡ä¸ä»…è€ƒè™‘äº†è½¨è¿¹çš„å‡ ä½•å½¢çŠ¶ï¼Œè¿˜è€ƒè™‘äº†è½¨è¿¹ç»è¿‡çš„åŒºåŸŸçš„è¯­ä¹‰ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ¨¡å‹ç”Ÿæˆçš„è½¨è¿¹ç»è¿‡äº†éšœç¢ç‰©ï¼Œåˆ™ä¼šå—åˆ°æƒ©ç½šã€‚æ­¤å¤–ï¼Œè¯¥æŒ‡æ ‡è¿˜è€ƒè™‘äº†ä¸åŒå…·èº«ç±»å‹çš„è¿åŠ¨çº¦æŸã€‚ä¾‹å¦‚ï¼Œè…¿å¼æœºå™¨äººå¯ä»¥è·¨è¶Šè¾ƒå°çš„éšœç¢ç‰©ï¼Œè€Œè½®å¼æœºå™¨äººåˆ™ä¸èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

NaviTraceåŸºå‡†æµ‹è¯•äº†8ä¸ªå…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç»“æœè¡¨æ˜è¿™äº›æ¨¡å‹åœ¨ç©ºé—´å®šä½å’Œç›®æ ‡å®šä½æ–¹é¢ä¸äººç±»è¡¨ç°å­˜åœ¨æ˜æ˜¾å·®è·ã€‚é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥è½¨è¿¹è¯„åˆ†æŒ‡æ ‡ï¼ŒNaviTraceèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„å¯¼èˆªæ€§èƒ½ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ˜ç¡®çš„æ–¹å‘ï¼Œä¾‹å¦‚ï¼Œå¦‚ä½•æé«˜VLMå¯¹ç¯å¢ƒçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

NaviTraceä¸ºæœºå™¨äººå¯¼èˆªé¢†åŸŸæä¾›äº†ä¸€ä¸ªé‡è¦çš„è¯„ä¼°å·¥å…·ï¼Œå¯ç”¨äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´é€šç”¨çš„æœºå™¨äººã€‚è¯¥åŸºå‡†å¯ä»¥ä¿ƒè¿›VLMåœ¨æœºå™¨äººå¯¼èˆªä¸­çš„åº”ç”¨ï¼Œä¾‹å¦‚ï¼Œåœ¨å®¶åº­æœåŠ¡ã€ç‰©æµé…é€ã€ç¾éš¾æ•‘æ´ç­‰é¢†åŸŸã€‚é€šè¿‡ä¸æ–­æ”¹è¿›VLMçš„å¯¼èˆªèƒ½åŠ›ï¼Œå¯ä»¥å®ç°æ›´è‡ªä¸»ã€æ›´å¯é çš„æœºå™¨äººç³»ç»Ÿï¼Œä»è€Œæé«˜ç”Ÿäº§æ•ˆç‡å’Œç”Ÿæ´»è´¨é‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-language models demonstrate unprecedented performance and generalization across a wide range of tasks and scenarios. Integrating these foundation models into robotic navigation systems opens pathways toward building general-purpose robots. Yet, evaluating these models' navigation capabilities remains constrained by costly real-world trials, overly simplified simulations, and limited benchmarks. We introduce NaviTrace, a high-quality Visual Question Answering benchmark where a model receives an instruction and embodiment type (human, legged robot, wheeled robot, bicycle) and must output a 2D navigation trace in image space. Across 1000 scenarios and more than 3000 expert traces, we systematically evaluate eight state-of-the-art VLMs using a newly introduced semantic-aware trace score. This metric combines Dynamic Time Warping distance, goal endpoint error, and embodiment-conditioned penalties derived from per-pixel semantics and correlates with human preferences. Our evaluation reveals consistent gap to human performance caused by poor spatial grounding and goal localization. NaviTrace establishes a scalable and reproducible benchmark for real-world robotic navigation. The benchmark and leaderboard can be found at https://leggedrobotics.github.io/navitrace_webpage/.

