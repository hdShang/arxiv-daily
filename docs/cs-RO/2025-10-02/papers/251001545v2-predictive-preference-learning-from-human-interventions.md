---
layout: default
title: Predictive Preference Learning from Human Interventions
---

# Predictive Preference Learning from Human Interventions

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.01545" target="_blank" class="toolbar-btn">arXiv: 2510.01545v2</a>
    <a href="https://arxiv.org/pdf/2510.01545.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.01545v2" 
            onclick="toggleFavorite(this, '2510.01545v2', 'Predictive Preference Learning from Human Interventions')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Haoyuan Cai, Zhenghao Peng, Bolei Zhou

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-02 (Êõ¥Êñ∞: 2025-10-15)

**Â§áÊ≥®**: NeurIPS 2025 Spotlight. Project page: https://metadriverse.github.io/ppl

**üîó ‰ª£Á†Å/È°πÁõÆ**: [PROJECT_PAGE](https://metadriverse.github.io/ppl)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫PPLÔºö‰∏ÄÁßçÂü∫‰∫é‰∫∫Á±ªÂπ≤È¢ÑÁöÑÈ¢ÑÊµãÂÅèÂ•ΩÂ≠¶‰π†ÊñπÊ≥ïÔºåÊèêÂçá‰∫§‰∫íÂºèÊ®°‰ªøÂ≠¶‰π†ÊïàÁéá„ÄÇ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `È¢ÑÊµãÂÅèÂ•ΩÂ≠¶‰π†` `‰∫∫Á±ªÂπ≤È¢Ñ` `‰∫§‰∫íÂºèÊ®°‰ªøÂ≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `Ëá™Âä®È©æÈ©∂` `Êú∫Âô®‰∫∫Êìç‰Ωú` `ÂÅèÂ•Ω‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞Êúâ‰∫§‰∫íÂºèÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ï‰ªÖÂÖ≥Ê≥®ÂΩìÂâçÁä∂ÊÄÅÁöÑÂä®‰Ωú‰øÆÊ≠£ÔºåÂøΩÁï•‰∫ÜÂØπÊú™Êù•Áä∂ÊÄÅÊΩúÂú®È£éÈô©ÁöÑË∞ÉÊï¥„ÄÇ
2. PPLÊñπÊ≥ïÈÄöËøáÂ∞Ü‰∫∫Á±ªÂπ≤È¢ÑÊâ©Â±ïÂà∞Êú™Êù•Êó∂Èó¥Ê≠•ÔºåÂà©Áî®ÈöêÂºèÂÅèÂ•Ω‰ø°Âè∑ËøõË°åÈ¢ÑÊµãÔºå‰ªéËÄåÂºïÂØºÊô∫ËÉΩ‰ΩìÊé¢Á¥¢ÂÆâÂÖ®ÂÖ≥ÈîÆÂå∫Âüü„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåPPLÂú®Ëá™Âä®È©æÈ©∂ÂíåÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Êõ¥È´òÁöÑÂ≠¶‰π†ÊïàÁéáÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÂØπ‰∫∫Â∑•ÊºîÁ§∫ÁöÑÈúÄÊ±Ç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫é‰∫∫Á±ªÂπ≤È¢ÑÁöÑÈ¢ÑÊµãÂÅèÂ•ΩÂ≠¶‰π†ÔºàPPLÔºâÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥‰∫§‰∫íÂºèÊ®°‰ªøÂ≠¶‰π†‰∏≠‰ªÖ‰øÆÊ≠£ÂΩìÂâçÁä∂ÊÄÅÂä®‰ΩúËÄåÂøΩÁï•Êú™Êù•Áä∂ÊÄÅÊΩúÂú®È£éÈô©ÁöÑÈóÆÈ¢ò„ÄÇPPLÂà©Áî®‰∫∫Á±ªÂπ≤È¢Ñ‰∏≠Ëï¥Âê´ÁöÑÈöêÂºèÂÅèÂ•Ω‰ø°Âè∑ÔºåÈ¢ÑÊµãÊú™Êù•ËΩ®ËøπÔºåÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÊØèÊ¨°‰∫∫Á±ªÂπ≤È¢ÑÂºïÂØºËá≥L‰∏™Êú™Êù•Êó∂Èó¥Ê≠•ÔºåÂç≥ÂÅèÂ•ΩËåÉÂõ¥ÔºåÂÅáËÆæÊô∫ËÉΩ‰ΩìÂú®ÂÅèÂ•ΩËåÉÂõ¥ÂÜÖÈááÂèñÁõ∏ÂêåÂä®‰ΩúÔºå‰∫∫Á±ªËøõË°åÁõ∏ÂêåÂπ≤È¢Ñ„ÄÇÈÄöËøáÂØπËøô‰∫õÊú™Êù•Áä∂ÊÄÅËøõË°åÂÅèÂ•Ω‰ºòÂåñÔºå‰∏ìÂÆ∂‰øÆÊ≠£Ë¢´‰º†Êí≠Âà∞Êô∫ËÉΩ‰ΩìÈ¢ÑËÆ°Êé¢Á¥¢ÁöÑÂÆâÂÖ®ÂÖ≥ÈîÆÂå∫ÂüüÔºåÊòæËëóÊèêÈ´òÂ≠¶‰π†ÊïàÁéáÂπ∂ÂáèÂ∞ëÊâÄÈúÄÁöÑ‰∫∫Â∑•ÊºîÁ§∫„ÄÇÂú®Ëá™Âä®È©æÈ©∂ÂíåÊú∫Âô®‰∫∫Êìç‰ΩúÂü∫ÂáÜÊµãËØï‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®Êòé‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÈÄöÁî®ÊÄß„ÄÇÁêÜËÆ∫ÂàÜÊûêË°®ÊòéÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÂÅèÂ•ΩËåÉÂõ¥LÂèØ‰ª•Âú®È£éÈô©Áä∂ÊÄÅË¶ÜÁõñÁéáÂíåÊ†áÁ≠æÊ≠£Á°ÆÊÄß‰πãÈó¥ÂèñÂæóÂπ≥Ë°°Ôºå‰ªéËÄåÈôêÂà∂ÁÆóÊ≥ïÁöÑÊúÄ‰ºòÊÄßÂ∑ÆË∑ù„ÄÇ‰ª£Á†ÅÂíåÊºîÁ§∫ÂèØÂú®https://metadriverse.github.io/ppl Ëé∑Âèñ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**Ôºö‰∫§‰∫íÂºèÊ®°‰ªøÂ≠¶‰π†Êó®Âú®ÈÄöËøá‰∫∫Á±ªÁöÑÁõëÁù£ÂíåÁ∫†Ê≠£Êù•ÊèêÂçáÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫ËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®‰∫éÁ∫†Ê≠£Êô∫ËÉΩ‰ΩìÂú®ÂΩìÂâçÁä∂ÊÄÅ‰∏ãÁöÑÂä®‰ΩúÔºåËÄåÂøΩÁï•‰∫ÜÂØπÊú™Êù•Áä∂ÊÄÅÂèØËÉΩÂá∫Áé∞ÁöÑÊΩúÂú®È£éÈô©ÁöÑÈ¢ÑÈò≤„ÄÇËøôÁßçÂ±ÄÈÉ®‰øÆÊ≠£ÁöÑÁ≠ñÁï•ÂèØËÉΩÂØºËá¥Êô∫ËÉΩ‰ΩìÂú®Êú™Êù•ÁöÑÊé¢Á¥¢‰∏≠Èô∑ÂÖ•Âç±Èô©Âå∫ÂüüÔºå‰ªéËÄåÈôç‰ΩéÂ≠¶‰π†ÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïÂà©Áî®‰∫∫Á±ªÂπ≤È¢ÑÁöÑ‰ø°ÊÅØÊù•ÊåáÂØºÊô∫ËÉΩ‰ΩìÂØπÊú™Êù•Ë°å‰∏∫ÁöÑÈ¢ÑÊµãÂíå‰ºòÂåñÔºåÊòØÊú¨ÊñáË¶ÅËß£ÂÜ≥ÁöÑÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöPPLÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂ∞Ü‰∫∫Á±ªÁöÑÊØè‰∏ÄÊ¨°Âπ≤È¢ÑËßÜ‰∏∫‰∏Ä‰∏™ÈöêÂºèÁöÑÂÅèÂ•Ω‰ø°Âè∑ÔºåÂπ∂Â∞ÜÂÖ∂Êâ©Â±ïÂà∞Êú™Êù•ÁöÑÂ§ö‰∏™Êó∂Èó¥Ê≠•„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåPPLÂÅáËÆæÂú®Êú™Êù•ÁöÑ‰∏ÄÊÆµÊó∂Èó¥ÂÜÖÔºàÂç≥ÂÅèÂ•ΩËåÉÂõ¥ÔºâÔºåÊô∫ËÉΩ‰Ωì‰ºöÈáçÂ§çÂΩìÂâçÁöÑÂä®‰ΩúÔºåÂπ∂‰∏î‰∫∫Á±ª‰ºöÂÅöÂá∫Áõ∏ÂêåÁöÑÂπ≤È¢Ñ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåPPLÂèØ‰ª•Â∞Ü‰∫∫Á±ªÁöÑÂπ≤È¢Ñ‰ø°ÊÅØ‰º†Êí≠Âà∞Êú™Êù•ÁöÑÁä∂ÊÄÅÔºå‰ªéËÄåÂºïÂØºÊô∫ËÉΩ‰ΩìÂú®Êé¢Á¥¢ËøáÁ®ã‰∏≠ÈÅøÂÖçÊΩúÂú®ÁöÑÂç±Èô©Âå∫Âüü„ÄÇËøôÁßçÂâçÁûªÊÄßÁöÑÂÅèÂ•ΩÂ≠¶‰π†ÊñπÊ≥ïÂèØ‰ª•ÊòæËëóÊèêÈ´òÂ≠¶‰π†ÊïàÁéáÔºåÂπ∂ÂáèÂ∞ëÂØπÂ§ßÈáè‰∫∫Â∑•ÊºîÁ§∫ÁöÑÈúÄÊ±Ç„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöPPLÁöÑÊï¥‰ΩìÊ°ÜÊû∂ÂèØ‰ª•Ê¶ÇÊã¨‰∏∫‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö1) Êô∫ËÉΩ‰ΩìÊâßË°åÂä®‰ΩúÂπ∂‰∏éÁéØÂ¢É‰∫§‰∫íÔºõ2) ‰∫∫Á±ªËßÇÂØüÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫ÔºåÂπ∂Âú®ÂøÖË¶ÅÊó∂ËøõË°åÂπ≤È¢ÑÔºõ3) PPLÂ∞Ü‰∫∫Á±ªÁöÑÂπ≤È¢ÑËßÜ‰∏∫ÈöêÂºèÂÅèÂ•Ω‰ø°Âè∑ÔºåÂπ∂Â∞ÜÂÖ∂Êâ©Â±ïÂà∞Êú™Êù•ÁöÑL‰∏™Êó∂Èó¥Ê≠•ÔºàÂÅèÂ•ΩËåÉÂõ¥ÔºâÔºõ4) PPLÂà©Áî®ÂÅèÂ•Ω‰ºòÂåñÁÆóÊ≥ïÔºåÊ†πÊçÆÊâ©Â±ïÂêéÁöÑÂÅèÂ•Ω‰ø°Âè∑Êù•Ë∞ÉÊï¥Êô∫ËÉΩ‰ΩìÁöÑÁ≠ñÁï•Ôºå‰ΩøÂÖ∂Âú®Êú™Êù•ÁöÑÊé¢Á¥¢‰∏≠Êõ¥Âä†ÂÆâÂÖ®ÂíåÈ´òÊïà„ÄÇËØ•Ê°ÜÊû∂ÁöÑÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïÊúâÊïàÂú∞Âà©Áî®‰∫∫Á±ªÁöÑÂπ≤È¢Ñ‰ø°ÊÅØÊù•È¢ÑÊµãÊú™Êù•ÁöÑÁä∂ÊÄÅÔºåÂπ∂Ê†πÊçÆÈ¢ÑÊµãÁªìÊûúÊù•‰ºòÂåñÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöPPLÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÂÖ∂ÂâçÁûªÊÄßÁöÑÂÅèÂ•ΩÂ≠¶‰π†Êú∫Âà∂„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∫§‰∫íÂºèÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåPPL‰∏ç‰ªÖÂÖ≥Ê≥®ÂΩìÂâçÁä∂ÊÄÅÁöÑÂä®‰Ωú‰øÆÊ≠£ÔºåÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÂÆÉËÉΩÂ§üÂà©Áî®‰∫∫Á±ªÁöÑÂπ≤È¢Ñ‰ø°ÊÅØÊù•È¢ÑÊµãÊú™Êù•ÁöÑÁä∂ÊÄÅÔºåÂπ∂ÂºïÂØºÊô∫ËÉΩ‰ΩìÂú®Êú™Êù•ÁöÑÊé¢Á¥¢‰∏≠ÈÅøÂÖçÊΩúÂú®ÁöÑÂç±Èô©Âå∫Âüü„ÄÇËøôÁßçÂâçÁûªÊÄßÁöÑÂ≠¶‰π†Êú∫Âà∂ÂèØ‰ª•ÊòæËëóÊèêÈ´òÂ≠¶‰π†ÊïàÁéáÔºåÂπ∂ÂáèÂ∞ëÂØπÂ§ßÈáè‰∫∫Â∑•ÊºîÁ§∫ÁöÑÈúÄÊ±Ç„ÄÇÊ≠§Â§ñÔºåPPLËøòÈÄöËøáÁêÜËÆ∫ÂàÜÊûêËØÅÊòé‰∫ÜÈÄâÊã©ÂêàÈÄÇÁöÑÂÅèÂ•ΩËåÉÂõ¥LÂèØ‰ª•Âú®È£éÈô©Áä∂ÊÄÅË¶ÜÁõñÁéáÂíåÊ†áÁ≠æÊ≠£Á°ÆÊÄß‰πãÈó¥ÂèñÂæóÂπ≥Ë°°Ôºå‰ªéËÄåÈôêÂà∂ÁÆóÊ≥ïÁöÑÊúÄ‰ºòÊÄßÂ∑ÆË∑ù„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöPPLÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö1) ÂÅèÂ•ΩËåÉÂõ¥LÁöÑÈÄâÊã©ÔºöLÁöÑÂ§ßÂ∞èÂÜ≥ÂÆö‰∫Ü‰∫∫Á±ªÂπ≤È¢Ñ‰ø°ÊÅØ‰º†Êí≠ÁöÑËåÉÂõ¥„ÄÇÂ¶ÇÊûúLÂ§™Â∞èÔºåÂàôÂèØËÉΩÊó†Ê≥ïË¶ÜÁõñÂà∞Ë∂≥Â§üÂ§öÁöÑÈ£éÈô©Áä∂ÊÄÅÔºõÂ¶ÇÊûúLÂ§™Â§ßÔºåÂàôÂèØËÉΩÂØºËá¥Ê†áÁ≠æÈîôËØØÁéáÂ¢ûÂä†„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑ‰ªªÂä°ÂíåÁéØÂ¢ÉÊù•ÈÄâÊã©ÂêàÈÄÇÁöÑLÂÄº„ÄÇ2) ÂÅèÂ•Ω‰ºòÂåñÁÆóÊ≥ïÔºöPPLÂèØ‰ª•‰ΩøÁî®ÂêÑÁßçÂÅèÂ•Ω‰ºòÂåñÁÆóÊ≥ïÊù•Ë∞ÉÊï¥Êô∫ËÉΩ‰ΩìÁöÑÁ≠ñÁï•„ÄÇÂ∏∏Áî®ÁöÑÂÅèÂ•Ω‰ºòÂåñÁÆóÊ≥ïÂåÖÊã¨ÊúÄÂ§ßÁÜµÈÄÜÂº∫ÂåñÂ≠¶‰π†„ÄÅÁõ∏ÂØπÁÜµÁ≠ñÁï•Ê¢ØÂ∫¶Á≠â„ÄÇ3) ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆæËÆ°ÔºöPPLÈúÄË¶ÅËÆæËÆ°ÂêàÈÄÇÁöÑÊçüÂ§±ÂáΩÊï∞Êù•Ë°°ÈáèÊô∫ËÉΩ‰ΩìË°å‰∏∫‰∏é‰∫∫Á±ªÂÅèÂ•Ω‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÂ∏∏Áî®ÁöÑÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨‰∫§ÂèâÁÜµÊçüÂ§±„ÄÅÈì∞ÈìæÊçüÂ§±Á≠â„ÄÇËøô‰∫õËÆæËÆ°ÁªÜËäÇÈÉΩ‰ºöÂΩ±ÂìçPPLÁöÑÊÄßËÉΩÂíåÊïàÊûú„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPPLÂú®Ëá™Âä®È©æÈ©∂ÂíåÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ‰æãÂ¶ÇÔºåÂú®Ëá™Âä®È©æÈ©∂‰ªªÂä°‰∏≠ÔºåPPLËÉΩÂ§üÊòæËëóÂáèÂ∞ëÁ¢∞ÊíûÊ¨°Êï∞ÂíåÂÅèÁ¶ªÈÅìË∑ØÁöÑÊ¨°Êï∞ÔºåÂêåÊó∂ÊèêÈ´òË°åÈ©∂ÊïàÁéá„ÄÇÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÔºåPPLËÉΩÂ§üÂ∏ÆÂä©Êú∫Âô®‰∫∫Êõ¥Âø´Âú∞Â≠¶‰ºöÂÆåÊàêÂêÑÁßçÂ§çÊùÇÁöÑ‰ªªÂä°ÔºåÂπ∂ÂáèÂ∞ë‰∫∫Â∑•Âπ≤È¢ÑÁöÑÊ¨°Êï∞„ÄÇ‰∏éÁé∞ÊúâÁöÑ‰∫§‰∫íÂºèÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåPPLÂú®Â≠¶‰π†ÊïàÁéáÂíåÂÆâÂÖ®ÊÄßÊñπÈù¢ÂùáÂÖ∑ÊúâÊòéÊòæÁöÑ‰ºòÂäø„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

PPLÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØÔºå‰æãÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÅÊ∏∏ÊàèAIÁ≠âÈ¢ÜÂüü„ÄÇÂú®Ëá™Âä®È©æÈ©∂‰∏≠ÔºåPPLÂèØ‰ª•Â∏ÆÂä©Êô∫ËÉΩ‰ΩìÂ≠¶‰π†Â¶Ç‰ΩïÂú®Â§çÊùÇÁöÑ‰∫§ÈÄöÁéØÂ¢É‰∏≠ÂÆâÂÖ®Ë°åÈ©∂ÔºåÈÅøÂÖçÁ¢∞ÊíûÂíå‰∫ãÊïÖ„ÄÇÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÔºåPPLÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫Â≠¶‰π†Â¶Ç‰ΩïÂÆåÊàêÂêÑÁßçÂ§çÊùÇÁöÑ‰ªªÂä°Ôºå‰æãÂ¶ÇË£ÖÈÖç„ÄÅÊê¨ËøêÁ≠â„ÄÇÂú®Ê∏∏ÊàèAI‰∏≠ÔºåPPLÂèØ‰ª•Â∏ÆÂä©AIËßíËâ≤Â≠¶‰π†Â¶Ç‰Ωï‰∏é‰∫∫Á±ªÁé©ÂÆ∂ËøõË°åÊúâÊïàÁöÑ‰∫íÂä®ÔºåÊèêÈ´òÊ∏∏ÊàèÁöÑË∂£Âë≥ÊÄßÂíåÊåëÊàòÊÄß„ÄÇPPLÁöÑÊú™Êù•ÂèëÂ±ïÊñπÂêëÂåÖÊã¨ÔºöÊé¢Á¥¢Êõ¥ÊúâÊïàÁöÑÂÅèÂ•ΩÂ≠¶‰π†ÁÆóÊ≥ï„ÄÅÁ†îÁ©∂Â¶Ç‰ΩïÂ§ÑÁêÜ‰∏çÁ°ÆÂÆöÊÄßÂíåÂô™Â£∞„ÄÅ‰ª•ÂèäÂ∞ÜPPLÂ∫îÁî®‰∫éÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÂíåÁéØÂ¢É„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent's action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl

