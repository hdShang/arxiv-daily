---
layout: default
title: Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration
---

# Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00797" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00797v1</a>
  <a href="https://arxiv.org/pdf/2512.00797.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00797v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.00797v1', 'Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Nan Sun, Bo Mao, Yongchang Li, Chenxu Wang, Di Guo, Huaping Liu

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-30

**å¤‡æ³¨**: 21 pages, 16 figures, 4 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**InteractGenï¼šå°†å•ä½“æ¨¡å‹è½¬åŒ–ä¸ºå…·èº«å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œä¿ƒè¿›äººæœºåä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `äººæœºåä½œ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `åŸºåº§æ¨¡å‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `æœåŠ¡æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŸºåº§æ¨¡å‹åœ¨æœºå™¨äººåº”ç”¨ä¸­å­˜åœ¨å•ä½“å‡è®¾ä¸å®é™…ä»»åŠ¡çš„åˆ†å¸ƒå¼åŠ¨æ€æ€§ä¸åŒ¹é…çš„é—®é¢˜ã€‚
2. InteractGenæå‡ºäº†ä¸€ç§åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†æœºå™¨äººæ™ºèƒ½åˆ†è§£ä¸ºå¤šä¸ªä¸“ä¸šæ™ºèƒ½ä½“ååŒå·¥ä½œã€‚
3. å®éªŒè¡¨æ˜ï¼ŒInteractGenæé«˜äº†ä»»åŠ¡æˆåŠŸç‡ã€é€‚åº”æ€§å’Œäººæœºåä½œèƒ½åŠ›ï¼ŒéªŒè¯äº†å¤šæ™ºèƒ½ä½“æ¶æ„çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å½“å‰ï¼ŒåŸºåº§æ¨¡å‹å·²æˆä¸ºç»Ÿä¸€æœºå™¨äººæ„ŸçŸ¥å’Œè§„åˆ’çš„æ ¸å¿ƒï¼Œä½†å®é™…éƒ¨ç½²ä¸­ï¼Œå…¶å•ä½“å‡è®¾ï¼ˆå³å•ä¸ªæ¨¡å‹å¯ä»¥å¤„ç†æ‰€æœ‰è®¤çŸ¥åŠŸèƒ½ï¼‰ä¸å®é™…æœåŠ¡å·¥ä½œæµç¨‹çš„åˆ†å¸ƒå¼ã€åŠ¨æ€ç‰¹æ€§ä¸åŒ¹é…ã€‚è§†è§‰-è¯­è¨€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½†ç¼ºä¹å…·èº«æ„ŸçŸ¥çš„åŠ¨ä½œèƒ½åŠ›ï¼Œå¹¶ä¸”ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æŠ€èƒ½ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥èƒ½å¤Ÿå®ç°ååº”å¼æ“ä½œï¼Œä½†åœ¨ä¸åŒå…·èº«ä¹‹é—´è¡¨ç°è„†å¼±ï¼Œå‡ ä½•åŸºç¡€è–„å¼±ï¼Œå¹¶ä¸”ç¼ºä¹ä¸»åŠ¨åä½œæœºåˆ¶ã€‚è¿™äº›å±€é™æ€§è¡¨æ˜ï¼Œä»…æ‰©å±•å•ä¸ªæ¨¡å‹æ— æ³•ä¸ºåœ¨äººç¾¤ç¯å¢ƒä¸­è¿è¡Œçš„æœåŠ¡æœºå™¨äººæä¾›å¯é çš„è‡ªä¸»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†InteractGenï¼Œè¿™æ˜¯ä¸€ä¸ªç”±LLMé©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒå°†æœºå™¨äººæ™ºèƒ½åˆ†è§£ä¸ºä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œç”¨äºæŒç»­æ„ŸçŸ¥ã€ä¾èµ–æ„ŸçŸ¥è§„åˆ’ã€å†³ç­–å’ŒéªŒè¯ã€å¤±è´¥åæ€ä»¥åŠåŠ¨æ€çš„äººå·¥å§”æ‰˜ï¼Œå°†åŸºåº§æ¨¡å‹è§†ä¸ºé—­ç¯é›†ä½“ä¸­çš„å—æ§ç»„ä»¶ã€‚InteractGenéƒ¨ç½²åœ¨å¼‚æ„æœºå™¨äººå›¢é˜Ÿä¸Šï¼Œå¹¶åœ¨ä¸ºæœŸä¸‰ä¸ªæœˆçš„å¼€æ”¾ä½¿ç”¨ç ”ç©¶ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œæé«˜äº†ä»»åŠ¡æˆåŠŸç‡ã€é€‚åº”æ€§å’Œäººæœºåä½œèƒ½åŠ›ï¼Œè¯æ˜äº†å¤šæ™ºèƒ½ä½“ç¼–æ’æ¯”è¿›ä¸€æ­¥æ‰©å±•ç‹¬ç«‹æ¨¡å‹æ›´å¯è¡Œï¼Œèƒ½å¤Ÿå®ç°å…·æœ‰ç¤¾ä¼šåŸºç¡€çš„æœåŠ¡è‡ªä¸»æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœåŠ¡æœºå™¨äººé¢†åŸŸä¸­ï¼Œç°æœ‰åŸºåº§æ¨¡å‹ï¼ˆå¦‚è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥ï¼‰åœ¨å®é™…äººæœºåä½œåœºæ™¯ä¸‹çš„å±€é™æ€§ã€‚è¿™äº›æ¨¡å‹è¦ä¹ˆç¼ºä¹å…·èº«æ„ŸçŸ¥èƒ½åŠ›å’Œä¸»åŠ¨åä½œæœºåˆ¶ï¼Œè¦ä¹ˆåœ¨ä¸åŒæœºå™¨äººä¹‹é—´æ³›åŒ–èƒ½åŠ›å·®ï¼Œæ— æ³•å¯é åœ°å®Œæˆå¤æ‚ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºå®ƒä»¬ä¾èµ–äºå•ä½“æ¨¡å‹ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„ç¯å¢ƒå’Œä»»åŠ¡éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å•ä½“åŸºåº§æ¨¡å‹è½¬åŒ–ä¸ºå¤šæ™ºèƒ½ä½“æ¶æ„ã€‚é€šè¿‡å°†æœºå™¨äººæ™ºèƒ½åˆ†è§£ä¸ºå¤šä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£ä¸åŒçš„è®¤çŸ¥åŠŸèƒ½ï¼ˆå¦‚æ„ŸçŸ¥ã€è§„åˆ’ã€å†³ç­–ç­‰ï¼‰ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œåè°ƒå’Œæ§åˆ¶ï¼Œä»è€Œå®ç°æ›´çµæ´»ã€é²æ£’å’Œå¯æ‰©å±•çš„æœºå™¨äººç³»ç»Ÿã€‚è¿™ç§è®¾è®¡å€Ÿé‰´äº†åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ€æƒ³ï¼Œå…è®¸å„ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹è¿è¡Œå’ŒååŒå·¥ä½œï¼Œæé«˜äº†ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šInteractGenæ¡†æ¶åŒ…å«ä»¥ä¸‹ä¸»è¦æ¨¡å—ï¼š1) æŒç»­æ„ŸçŸ¥æ™ºèƒ½ä½“ï¼šè´Ÿè´£æŒç»­æ„ŸçŸ¥ç¯å¢ƒä¿¡æ¯ã€‚2) ä¾èµ–æ„ŸçŸ¥è§„åˆ’æ™ºèƒ½ä½“ï¼šæ ¹æ®ä»»åŠ¡ç›®æ ‡å’Œç¯å¢ƒä¿¡æ¯è¿›è¡Œè§„åˆ’ã€‚3) å†³ç­–å’ŒéªŒè¯æ™ºèƒ½ä½“ï¼šåšå‡ºå†³ç­–å¹¶éªŒè¯å…¶å¯è¡Œæ€§ã€‚4) å¤±è´¥åæ€æ™ºèƒ½ä½“ï¼šåœ¨ä»»åŠ¡å¤±è´¥æ—¶è¿›è¡Œåæ€å’Œè°ƒæ•´ã€‚5) åŠ¨æ€äººå·¥å§”æ‰˜æ™ºèƒ½ä½“ï¼šå…è®¸äººç±»ä»‹å…¥å¹¶å§”æ‰˜ä»»åŠ¡ã€‚è¿™äº›æ™ºèƒ½ä½“é€šè¿‡LLMè¿›è¡Œåè°ƒå’Œé€šä¿¡ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯æ§åˆ¶ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†LLMä½œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ä¸­å¤®åè°ƒå™¨ã€‚LLMä¸ä»…å¯ä»¥ç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¿˜å¯ä»¥æ¨ç†ä»»åŠ¡ä¾èµ–å…³ç³»ï¼Œå¹¶æ ¹æ®ç¯å¢ƒå˜åŒ–åŠ¨æ€è°ƒæ•´æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ‰‹å·¥è®¾è®¡å¤æ‚çš„æ§åˆ¶ç­–ç•¥ï¼Œæé«˜äº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼ŒInteractGenä¸å†ä¾èµ–äºå•ä¸ªæ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œè€Œæ˜¯é€šè¿‡å¤šä¸ªæ™ºèƒ½ä½“çš„ååŒå·¥ä½œæ¥å®ç°å¤æ‚ä»»åŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æ²¡æœ‰è¯¦ç»†æè¿°å…·ä½“çš„å‚æ•°è®¾ç½®ã€æŸå¤±å‡½æ•°æˆ–ç½‘ç»œç»“æ„ç­‰æŠ€æœ¯ç»†èŠ‚ã€‚ä½†å¯ä»¥æ¨æ–­ï¼ŒLLMçš„é€‰æ‹©å’Œå¾®è°ƒã€æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡åè®®ã€ä»¥åŠä»»åŠ¡åˆ†è§£ç­–ç•¥æ˜¯å…³é”®çš„è®¾è®¡å› ç´ ã€‚æ­¤å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨äººç±»çš„åé¦ˆå’ŒæŒ‡å¯¼ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

InteractGenåœ¨ä¸ºæœŸä¸‰ä¸ªæœˆçš„å¼€æ”¾ä½¿ç”¨ç ”ç©¶ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æé«˜äº†ä»»åŠ¡æˆåŠŸç‡ã€é€‚åº”æ€§å’Œäººæœºåä½œèƒ½åŠ›ã€‚å…·ä½“çš„æ€§èƒ½æ•°æ®å’Œå¯¹æ¯”åŸºçº¿æœªåœ¨æ‘˜è¦ä¸­ç»™å‡ºï¼Œä½†ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ç¼–æ’æ¯”è¿›ä¸€æ­¥æ‰©å±•ç‹¬ç«‹æ¨¡å‹æ›´å¯è¡Œï¼Œèƒ½å¤Ÿå®ç°å…·æœ‰ç¤¾ä¼šåŸºç¡€çš„æœåŠ¡è‡ªä¸»æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§äººæœºåä½œåœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€åŒ»ç–—æŠ¤ç†ã€å·¥ä¸šè‡ªåŠ¨åŒ–ç­‰ã€‚é€šè¿‡å°†æœºå™¨äººæ™ºèƒ½åˆ†è§£ä¸ºå¤šä¸ªæ™ºèƒ½ä½“ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œåè°ƒï¼Œå¯ä»¥å®ç°æ›´å®‰å…¨ã€é«˜æ•ˆå’Œå¯é çš„äººæœºåä½œã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›æ¨åŠ¨æœåŠ¡æœºå™¨äººåœ¨å®é™…ç”Ÿæ´»ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæé«˜äººä»¬çš„ç”Ÿæ´»è´¨é‡å’Œå·¥ä½œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.

