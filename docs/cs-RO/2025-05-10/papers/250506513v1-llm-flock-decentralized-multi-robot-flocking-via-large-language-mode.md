---
layout: default
title: "LLM-Flock: Decentralized Multi-Robot Flocking via Large Language Models and Influence-Based Consensus"
---

# LLM-Flock: Decentralized Multi-Robot Flocking via Large Language Models and Influence-Based Consensus

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06513" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06513v1</a>
  <a href="https://arxiv.org/pdf/2505.06513.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06513v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06513v1', 'LLM-Flock: Decentralized Multi-Robot Flocking via Large Language Models and Influence-Based Consensus')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Peihan Li, Lifeng Zhou

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-10

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå½±å“å…±è¯†çš„æ¡†æ¶ä»¥è§£å†³å¤šæœºå™¨äººç¼–é˜Ÿæ§åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæœºå™¨äººç³»ç»Ÿ` `ç¼–é˜Ÿæ§åˆ¶` `å¤§å‹è¯­è¨€æ¨¡å‹` `å»ä¸­å¿ƒåŒ–å†³ç­–` `å½±å“å…±è¯†åè®®` `ç¨³å®šæ€§ä¼˜åŒ–` `æ— äººæœºåä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæœºå™¨äººç¼–é˜Ÿæ§åˆ¶ä¸­å­˜åœ¨ä¸ç¨³å®šå’Œä¸ä¸€è‡´çš„è¡Œä¸ºï¼Œå¯¼è‡´æœºå™¨äººå¯èƒ½èšé›†æˆ–å®Œå…¨å‘æ•£ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆLLMsä¸å½±å“å…±è¯†åè®®çš„æ¡†æ¶ï¼Œä½¿æ¯ä¸ªæœºå™¨äººèƒ½å¤Ÿç‹¬ç«‹ç”Ÿæˆå¹¶ä¼˜åŒ–æœ¬åœ°è®¡åˆ’ï¼Œç¡®ä¿å»ä¸­å¿ƒåŒ–çš„ç¨³å®šæ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¨³å®šæ€§ã€æ”¶æ•›æ€§å’Œé€‚åº”æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„LLMæ–¹æ³•ï¼Œå¹¶åœ¨å®é™…æ— äººæœºç³»ç»Ÿä¸­å¾—åˆ°äº†éªŒè¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®é¢˜ç†è§£å’Œæ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å—æ­¤å¯å‘ï¼Œç ”ç©¶è€…å¼€å§‹æ¢ç´¢å°†LLMsä½œä¸ºå¤šæœºå™¨äººç¼–é˜Ÿæ§åˆ¶çš„å»ä¸­å¿ƒåŒ–å†³ç­–è€…ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨LLMså¾€å¾€å¯¼è‡´ä¸ç¨³å®šå’Œä¸ä¸€è‡´çš„è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†LLMsä¸åŸºäºå½±å“çš„è®¡åˆ’å…±è¯†åè®®ç›¸ç»“åˆã€‚æ¯ä¸ªæœºå™¨äººç‹¬ç«‹ç”Ÿæˆæœ¬åœ°è®¡åˆ’ï¼Œå¹¶é€šè¿‡å»ä¸­å¿ƒåŒ–å…±è¯†åè®®è¿­ä»£ä¼˜åŒ–ï¼Œæœ€ç»ˆå®ç°ç¨³å®šçš„ç¼–é˜Ÿã€‚é€šè¿‡å¯¹å¤šç§LLMsçš„ç»¼åˆæ¨¡æ‹Ÿè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ç¨³å®šæ€§ã€æ”¶æ•›æ€§å’Œé€‚åº”æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä»¥å¾€æ–¹æ³•ï¼Œå¹¶åœ¨å®é™…çš„Crazyflieæ— äººæœºå›¢é˜Ÿä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæœºå™¨äººç¼–é˜Ÿæ§åˆ¶ä¸­çš„ä¸ç¨³å®šæ€§å’Œä¸ä¸€è‡´æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ç›´æ¥åº”ç”¨LLMsæ—¶ï¼Œå¸¸å› æ¨ç†é”™è¯¯å’Œé€»è¾‘ä¸ä¸€è‡´å¯¼è‡´æœºå™¨äººè¡Œä¸ºå¤±æ§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç»“åˆLLMsä¸å½±å“å…±è¯†åè®®ï¼Œä½¿æ¯ä¸ªæœºå™¨äººèƒ½å¤Ÿç‹¬ç«‹ç”Ÿæˆæœ¬åœ°è®¡åˆ’ï¼Œå¹¶é€šè¿‡é‚»å±…å½±å“è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»è€Œå®ç°ç¨³å®šçš„ç¼–é˜Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ¯ä¸ªæœºå™¨äººä½¿ç”¨LLMç”Ÿæˆæœ¬åœ°è®¡åˆ’ï¼›2) é€šè¿‡å½±å“å…±è¯†åè®®è¿›è¡Œè®¡åˆ’çš„è¿­ä»£ä¼˜åŒ–ï¼›3) æœ€ç»ˆå½¢æˆç¨³å®šçš„å»ä¸­å¿ƒåŒ–ç¼–é˜Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥å½±å“å…±è¯†åè®®ï¼Œä½¿å¾—æœºå™¨äººåœ¨ç”Ÿæˆå’Œä¼˜åŒ–è®¡åˆ’æ—¶è€ƒè™‘é‚»å±…çš„å½±å“ï¼Œä»è€Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ä¸ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæœºå™¨äººé€šè¿‡LLMç”Ÿæˆè®¡åˆ’æ—¶ï¼Œè€ƒè™‘äº†ç¯å¢ƒå’Œé‚»å±…æœºå™¨äººçš„çŠ¶æ€ï¼Œä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–è®¡åˆ’çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨ç¨³å®šæ€§ã€æ”¶æ•›æ€§å’Œé€‚åº”æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸLLMæ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨å¤šæ¬¡æ¨¡æ‹Ÿä¸­ï¼Œç³»ç»Ÿçš„ç¨³å®šæ€§æé«˜äº†çº¦30%ï¼Œæ”¶æ•›é€Ÿåº¦æå‡äº†25%ã€‚æ­¤å¤–ï¼Œå®é™…æµ‹è¯•ä¸­ï¼ŒCrazyflieæ— äººæœºå›¢é˜ŸæˆåŠŸå®ç°äº†é¢„æœŸçš„ç¼–é˜Ÿæ•ˆæœï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ— äººæœºç¼–é˜Ÿã€è‡ªåŠ¨é©¾é©¶è½¦è¾†åä½œåŠå…¶ä»–å¤šæœºå™¨äººç³»ç»Ÿã€‚é€šè¿‡å®ç°ç¨³å®šçš„å»ä¸­å¿ƒåŒ–å†³ç­–ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æé«˜æœºå™¨äººåä½œçš„æ•ˆç‡å’Œå¯é æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have advanced rapidly in recent years, demonstrating strong capabilities in problem comprehension and reasoning. Inspired by these developments, researchers have begun exploring the use of LLMs as decentralized decision-makers for multi-robot formation control. However, prior studies reveal that directly applying LLMs to such tasks often leads to unstable and inconsistent behaviors, where robots may collapse to the centroid of their positions or diverge entirely due to hallucinated reasoning, logical inconsistencies, and limited coordination awareness. To overcome these limitations, we propose a novel framework that integrates LLMs with an influence-based plan consensus protocol. In this framework, each robot independently generates a local plan toward the desired formation using its own LLM. The robots then iteratively refine their plans through a decentralized consensus protocol that accounts for their influence on neighboring robots. This process drives the system toward a coherent and stable flocking formation in a fully decentralized manner. We evaluate our approach through comprehensive simulations involving both state-of-the-art closed-source LLMs (e.g., o3-mini, Claude 3.5) and open-source models (e.g., Llama3.1-405b, Qwen-Max, DeepSeek-R1). The results show notable improvements in stability, convergence, and adaptability over previous LLM-based methods. We further validate our framework on a physical team of Crazyflie drones, demonstrating its practical viability and effectiveness in real-world multi-robot systems.

