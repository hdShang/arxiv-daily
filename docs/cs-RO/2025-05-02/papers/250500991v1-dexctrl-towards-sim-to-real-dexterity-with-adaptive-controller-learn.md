---
layout: default
title: "DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning"
---

# DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.00991" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.00991v1</a>
  <a href="https://arxiv.org/pdf/2505.00991.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.00991v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.00991v1', 'DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shuqi Zhao, Ke Yang, Yuxin Chen, Chenran Li, Yichen Xie, Xiang Zhang, Changhao Wang, Masayoshi Tomizuka

**åˆ†ç±»**: cs.RO, eess.SY

**å‘å¸ƒæ—¥æœŸ**: 2025-05-02

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDexCtrlä»¥è§£å†³ä»¿çœŸåˆ°ç°å®çš„çµå·§æ“æ§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `çµå·§æ“æ§` `è‡ªé€‚åº”æ§åˆ¶` `ä»¿çœŸåˆ°ç°å®` `æœºå™¨äººå­¦ä¹ ` `æ§åˆ¶å™¨åŠ¨æ€` `åŠ›äº¤äº’` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çµå·§æ“æ§ç­–ç•¥åœ¨ä»¿çœŸä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è½¬ç§»åˆ°ç°å®ä¸–ç•Œæ—¶é¢ä¸´ä½çº§æ§åˆ¶å™¨åŠ¨æ€ä¸åŒ¹é…çš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ§åˆ¶å™¨è°ƒæ•´æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒèŠ‚æ§åˆ¶å‚æ•°ï¼Œå‡å°ä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§çµå·§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å¯å˜åŠ›æ¡ä»¶ä¸‹çš„è½¬ç§»æ€§èƒ½æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

çµå·§æ“æ§åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç°æœ‰ç­–ç•¥èƒ½å¤Ÿåœ¨ä»¿çœŸä¸­æ‰§è¡Œè®¸å¤šå¤æ‚çš„æ¥è§¦ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°†è¿™äº›ç­–ç•¥ä»ä»¿çœŸè½¬ç§»åˆ°ç°å®ä¸–ç•Œä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä½çº§æ§åˆ¶å™¨åŠ¨æ€çš„ä¸åŒ¹é…ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨è°ƒä¼˜æˆ–æ§åˆ¶å™¨éšæœºåŒ–ï¼Œè¿™äº›æ–¹æ³•æ—¢è´¹æ—¶åˆå…·æœ‰ä»»åŠ¡ç‰¹å¼‚æ€§ï¼Œä¸”å¢åŠ äº†è®­ç»ƒéš¾åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼ŒåŸºäºè½¨è¿¹å’Œæ§åˆ¶å™¨çš„å†å²ä¿¡æ¯å…±åŒå­¦ä¹ åŠ¨ä½œå’Œæ§åˆ¶å™¨å‚æ•°ã€‚è¯¥è‡ªé€‚åº”æ§åˆ¶å™¨è°ƒæ•´æœºåˆ¶å…è®¸ç­–ç•¥åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒèŠ‚æ§åˆ¶å‚æ•°ï¼Œä»è€Œå‡å°ä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·ï¼Œä¸”æ— éœ€å¤§é‡æ‰‹åŠ¨è°ƒä¼˜æˆ–è¿‡åº¦éšæœºåŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†æ§åˆ¶å™¨å‚æ•°æ˜ç¡®ä½œä¸ºè§‚å¯Ÿçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿ƒè¿›äº†å¯¹åŠ›äº¤äº’çš„æ›´å¥½æ¨ç†ï¼Œå¹¶æé«˜äº†åœ¨ç°å®åœºæ™¯ä¸­çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æ¶‰åŠå¯å˜åŠ›æ¡ä»¶çš„çµå·§ä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„è½¬ç§»æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³çµå·§æ“æ§ç­–ç•¥ä»ä»¿çœŸåˆ°ç°å®è½¬ç§»ä¸­çš„ä½çº§æ§åˆ¶å™¨åŠ¨æ€ä¸åŒ¹é…é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨è°ƒä¼˜æˆ–éšæœºåŒ–ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ç¹çä¸”æ•ˆæœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„æ¡†æ¶é€šè¿‡å†å²è½¨è¿¹å’Œæ§åˆ¶å™¨ä¿¡æ¯å…±åŒå­¦ä¹ åŠ¨ä½œå’Œæ§åˆ¶å™¨å‚æ•°ï¼Œå…è®¸ç­–ç•¥åœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨è°ƒæ•´æ§åˆ¶å‚æ•°ï¼Œä»è€Œå‡å°ä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŠ¨ä½œå­¦ä¹ æ¨¡å—å’Œæ§åˆ¶å™¨å‚æ•°è°ƒæ•´æ¨¡å—ã€‚åŠ¨ä½œå­¦ä¹ æ¨¡å—åŸºäºå†å²æ•°æ®ç”ŸæˆåŠ¨ä½œï¼Œè€Œæ§åˆ¶å™¨å‚æ•°è°ƒæ•´æ¨¡å—åˆ™æ ¹æ®å®æ—¶åé¦ˆåŠ¨æ€è°ƒæ•´æ§åˆ¶å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè‡ªé€‚åº”æ§åˆ¶å™¨è°ƒæ•´æœºåˆ¶ï¼Œå®ƒä½¿å¾—ç­–ç•¥èƒ½å¤Ÿåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å®æ—¶ä¼˜åŒ–æ§åˆ¶å‚æ•°ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ç°å®ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æœºåˆ¶å‡å°‘äº†å¯¹æ‰‹åŠ¨è°ƒä¼˜çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œæ§åˆ¶å™¨å‚æ•°è¢«æ˜ç¡®çº³å…¥è§‚å¯Ÿä¿¡æ¯ä¸­ï¼Œå¢å¼ºäº†å¯¹åŠ›äº¤äº’çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†æ§åˆ¶å™¨åŠ¨æ€çš„å˜åŒ–ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚å®éªŒä¸­ä½¿ç”¨äº†å¤šç§çµå·§ä»»åŠ¡ï¼Œä»¥éªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDexCtrlåœ¨å¤šç§çµå·§ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¯å˜åŠ›æ¡ä»¶ä¸‹çš„è½¬ç§»æ€§èƒ½æé«˜äº†20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ›´å¥½çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€è£…é…å’Œå…¶ä»–éœ€è¦çµå·§æ“æ§çš„ä»»åŠ¡ã€‚é€šè¿‡æé«˜ä»¿çœŸåˆ°ç°å®çš„è½¬ç§»æ€§èƒ½ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…å·¥ä¸šå’ŒæœåŠ¡æœºå™¨äººä¸­å®ç°æ›´é«˜æ•ˆçš„æ“ä½œï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Dexterous manipulation has seen remarkable progress in recent years, with policies capable of executing many complex and contact-rich tasks in simulation. However, transferring these policies from simulation to real world remains a significant challenge. One important issue is the mismatch in low-level controller dynamics, where identical trajectories can lead to vastly different contact forces and behaviors when control parameters vary. Existing approaches often rely on manual tuning or controller randomization, which can be labor-intensive, task-specific, and introduce significant training difficulty. In this work, we propose a framework that jointly learns actions and controller parameters based on the historical information of both trajectory and controller. This adaptive controller adjustment mechanism allows the policy to automatically tune control parameters during execution, thereby mitigating the sim-to-real gap without extensive manual tuning or excessive randomization. Moreover, by explicitly providing controller parameters as part of the observation, our approach facilitates better reasoning over force interactions and improves robustness in real-world scenarios. Experimental results demonstrate that our method achieves improved transfer performance across a variety of dexterous tasks involving variable force conditions.

