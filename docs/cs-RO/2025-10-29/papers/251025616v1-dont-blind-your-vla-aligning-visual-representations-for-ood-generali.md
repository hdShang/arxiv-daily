---
layout: default
title: Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization
---

# Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.25616" target="_blank" class="toolbar-btn">arXiv: 2510.25616v1</a>
    <a href="https://arxiv.org/pdf/2510.25616.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.25616v1" 
            onclick="toggleFavorite(this, '2510.25616v1', 'Don\'t Blind Your VLA: Aligning Visual Representations for OOD Generalization')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Nikita Kachaev, Mikhail Kolosov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov

**ÂàÜÁ±ª**: cs.LG, cs.AI, cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-29

**Â§áÊ≥®**: 13 pages, 6 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ËßÜËßâË°®ÂæÅÂØπÈΩêÊñπÊ≥ïÔºåËß£ÂÜ≥VLAÊ®°ÂûãOODÊ≥õÂåñËÉΩÂäõÈÄÄÂåñÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Ë°®ÂæÅÂØπÈΩê` `ÂàÜÂ∏ÉÂ§ñÊ≥õÂåñ` `Âä®‰ΩúÂæÆË∞É` `ËßÜËßâË°®ÂæÅÈÄÄÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. VLAÊ®°ÂûãÂú®Âä®‰ΩúÂæÆË∞ÉÂêéÔºåÂÖ∂ËßÜËßâË°®ÂæÅ‰ºöÂèëÁîüÈÄÄÂåñÔºåÂØºËá¥Ê®°ÂûãÊ≥õÂåñËÉΩÂäõ‰∏ãÈôçÔºåËøôÊòØÂΩìÂâçVLAÊ®°ÂûãÈù¢‰∏¥ÁöÑÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫‰∏ÄÁßçËßÜËßâË°®ÂæÅÂØπÈΩêÊñπÊ≥ïÔºåÊó®Âú®ÁºìËß£Âä®‰ΩúÂæÆË∞ÉÂØπËßÜËßâË°®ÂæÅÁöÑË¥üÈù¢ÂΩ±ÂìçÔºå‰ªéËÄåÊèêÂçáVLAÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂáèËΩªËßÜËßâË°®ÂæÅÈÄÄÂåñÔºåÂπ∂Âú®OODÂú∫ÊôØ‰∏ãÊòæËëóÊèêÂçáVLAÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú(VLA)Ê®°ÂûãÁöÑÊó•ÁõäÊàêÂäüÊ∫ê‰∫éÈ¢ÑËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã(VLM)ËÉΩÂ§üËµã‰∫àÊô∫ËÉΩ‰ΩìÂèØËøÅÁßªÁöÑ‰∏ñÁïåÁü•ËØÜÂíåËßÜËßâ-ËØ≠Ë®Ä(VL)Âü∫Á°ÄÔºå‰∏∫ÂÖ∑ÊúâÊõ¥ÂπøÊ≥õÊ≥õÂåñËÉΩÂäõÁöÑÂä®‰ΩúÊ®°ÂûãÂ•†ÂÆöÂü∫Á°Ä„ÄÇÁÑ∂ËÄåÔºåÂΩìËøô‰∫õVLMË¢´ÈÄÇÈÖçÂà∞Âä®‰ΩúÊ®°ÊÄÅÊó∂ÔºåÂÆÉ‰ª¨ÂéüÂßãÁöÑVLË°®ÂæÅÂíåÁü•ËØÜÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äË¢´‰øùÁïô‰ªçÁÑ∂‰∏çÊ∏ÖÊ•ö„ÄÇÊú¨ÊñáÂØπVLAÂæÆË∞ÉÊúüÈó¥ÁöÑË°®ÂæÅ‰øùÁïôËøõË°å‰∫ÜÁ≥ªÁªüÁ†îÁ©∂ÔºåË°®ÊòéÊú¥Á¥†ÁöÑÂä®‰ΩúÂæÆË∞É‰ºöÂØºËá¥ËßÜËßâË°®ÂæÅÁöÑÈÄÄÂåñ„ÄÇ‰∏∫‰∫ÜÊèèËø∞ÂíåË°°ÈáèËøô‰∫õÂΩ±ÂìçÔºåÊàë‰ª¨Êé¢Êµã‰∫ÜVLAÁöÑÈöêËóèË°®ÂæÅÂπ∂ÂàÜÊûê‰∫ÜÊ≥®ÊÑèÂäõÂõæ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁªÑÊúâÈíàÂØπÊÄßÁöÑ‰ªªÂä°ÂíåÊñπÊ≥ïÔºåÂ∞ÜVLAÊ®°Âûã‰∏éÂÖ∂ÂØπÂ∫îÁöÑVLMËøõË°åÂØπÊØîÔºå‰ªéËÄåÂàÜÁ¶ªÂá∫Âä®‰ΩúÂæÆË∞ÉÂºïËµ∑ÁöÑVLËÉΩÂäõÂèòÂåñ„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ËØÑ‰º∞‰∫Ü‰∏ÄÁ≥ªÂàóÂØπÈΩêËßÜËßâË°®ÂæÅÁöÑÁ≠ñÁï•ÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•ÂáèËΩªÈÄÄÂåñÂπ∂ÊèêÈ´òÂØπÂàÜÂ∏ÉÂ§ñ(OOD)Âú∫ÊôØÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊÄªËÄåË®Ä‰πãÔºåÊàë‰ª¨ÁöÑÂàÜÊûêÈòêÊòé‰∫ÜÂä®‰ΩúÂæÆË∞É‰∏éVLË°®ÂæÅÈÄÄÂåñ‰πãÈó¥ÁöÑÊùÉË°°ÔºåÂπ∂Âº∫Ë∞É‰∫ÜÊÅ¢Â§çÁªßÊâøÁöÑVLËÉΩÂäõÁöÑÂÆûÁî®ÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöVLAÊ®°ÂûãÂú®ËøõË°åÂä®‰ΩúÂæÆË∞ÉÊó∂Ôºå‰ºö‰∏çÂèØÈÅøÂÖçÂú∞ÊîπÂèòÂÖ∂ÂéüÊúâÁöÑËßÜËßâË°®ÂæÅÔºåÂØºËá¥Ê®°ÂûãÂú®ËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°‰∏äÁöÑÊÄßËÉΩ‰∏ãÈôçÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂàÜÂ∏ÉÂ§ñ(OOD)Âú∫ÊôØ‰∏ã„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÂøΩÁï•‰∫ÜËøôÁßçË°®ÂæÅÈÄÄÂåñÈóÆÈ¢òÔºåÂØºËá¥VLAÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂØπÈΩêVLAÊ®°ÂûãÂíåÂéüÂßãVLMÁöÑËßÜËßâË°®ÂæÅÔºåÊù•ÁºìËß£Âä®‰ΩúÂæÆË∞ÉÂ∏¶Êù•ÁöÑË°®ÂæÅÈÄÄÂåñ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ∞±ÊòØÂú®Âä®‰ΩúÂæÆË∞ÉËøáÁ®ã‰∏≠ÔºåÂºïÂÖ•È¢ùÂ§ñÁöÑÁ∫¶ÊùüÔºå‰ΩøÂæóVLAÊ®°ÂûãÁöÑËßÜËßâË°®ÂæÅÂ∞ΩÂèØËÉΩÂú∞Êé•ËøëÂéüÂßãVLMÁöÑËßÜËßâË°®ÂæÅÔºå‰ªéËÄå‰øùÁïôVLMÁöÑÁü•ËØÜÂíåËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËÆ∫ÊñáÈ¶ñÂÖàÈÄöËøáÂÆûÈ™åÂàÜÊûê‰∫ÜÂä®‰ΩúÂæÆË∞ÉÂØπVLAÊ®°ÂûãËßÜËßâË°®ÂæÅÁöÑÂΩ±ÂìçÔºåÂåÖÊã¨Êé¢ÊµãÈöêËóèÂ±ÇË°®ÂæÅÂíåÂàÜÊûêÊ≥®ÊÑèÂäõÂõæ„ÄÇÁÑ∂ÂêéÔºåËÆæËÆ°‰∫Ü‰∏ÄÁ≥ªÂàóÊúâÈíàÂØπÊÄßÁöÑ‰ªªÂä°ÔºåÁî®‰∫éËØÑ‰º∞VLAÊ®°ÂûãÂú®Âä®‰ΩúÂæÆË∞ÉÂêéÁöÑVLËÉΩÂäõÂèòÂåñ„ÄÇÊúÄÂêéÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçËßÜËßâË°®ÂæÅÂØπÈΩêÊñπÊ≥ïÔºåÂπ∂Âú®ÂÆûÈ™å‰∏≠È™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÂèëÁé∞‰∫ÜÂä®‰ΩúÂæÆË∞ÉÂØπVLAÊ®°ÂûãËßÜËßâË°®ÂæÅÁöÑË¥üÈù¢ÂΩ±ÂìçÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑËßÜËßâË°®ÂæÅÂØπÈΩêÊñπÊ≥ïÊù•ÁºìËß£ËøôÁßçÂΩ±Âìç„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÊõ¥Âä†ÂÖ≥Ê≥®ËßÜËßâË°®ÂæÅÁöÑ‰øùÁïôÔºå‰ªéËÄåÊèêÂçá‰∫ÜVLAÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑËßÜËßâË°®ÂæÅÂØπÈΩêÊñπÊ≥ï‰∏ªË¶ÅÈÄöËøáÂú®Âä®‰ΩúÂæÆË∞ÉËøáÁ®ã‰∏≠Ê∑ªÂä†È¢ùÂ§ñÁöÑÊçüÂ§±ÂáΩÊï∞Êù•ÂÆûÁé∞„ÄÇËØ•ÊçüÂ§±ÂáΩÊï∞ÁöÑÁõÆÊ†áÊòØÊúÄÂ∞èÂåñVLAÊ®°ÂûãÂíåÂéüÂßãVLMÁöÑËßÜËßâË°®ÂæÅ‰πãÈó¥ÁöÑË∑ùÁ¶ª„ÄÇÂÖ∑‰ΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂΩ¢ÂºèÂèØ‰ª•ÊòØÂùáÊñπËØØÂ∑Æ(MSE)Êàñ‰ΩôÂº¶Áõ∏‰ººÂ∫¶Á≠â„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊé¢Á¥¢‰∫Ü‰∏çÂêåÁöÑÂØπÈΩêÁ≠ñÁï•Ôºå‰æãÂ¶ÇÂè™ÂØπÈΩêÁâπÂÆöÂ±ÇÁöÑË°®ÂæÅÊàñ‰ΩøÁî®‰∏çÂêåÁöÑÊùÉÈáçÊù•Âπ≥Ë°°Âä®‰ΩúÂæÆË∞ÉÊçüÂ§±ÂíåË°®ÂæÅÂØπÈΩêÊçüÂ§±„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËÆ∫ÊñáÈÄöËøáÂÆûÈ™åËØÅÊòéÔºåÊèêÂá∫ÁöÑËßÜËßâË°®ÂæÅÂØπÈΩêÊñπÊ≥ïËÉΩÂ§üÊúâÊïàÂáèËΩªÂä®‰ΩúÂæÆË∞ÉÂØπVLAÊ®°ÂûãËßÜËßâË°®ÂæÅÁöÑË¥üÈù¢ÂΩ±ÂìçÔºåÂπ∂Âú®OODÂú∫ÊôØ‰∏ãÊòæËëóÊèêÂçáVLAÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂú®Â§ö‰∏™OODÊï∞ÊçÆÈõÜ‰∏äÔºåËØ•ÊñπÊ≥ïÁõ∏ÊØîÂü∫Á∫øÊñπÊ≥ïÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂„ÄÅÊô∫ËÉΩÂä©ÊâãÁ≠âÈ¢ÜÂüü„ÄÇÈÄöËøáÊèêÂçáVLAÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂèØ‰ª•‰ΩøÊô∫ËÉΩ‰ΩìÂú®Êõ¥Â§çÊùÇÁöÑÁéØÂ¢É‰∏≠ÊâßË°å‰ªªÂä°ÔºåÂπ∂Êõ¥Â•ΩÂú∞ÁêÜËß£‰∫∫Á±ªÊåá‰ª§„ÄÇ‰æãÂ¶ÇÔºåÂú®Êú∫Âô®‰∫∫È¢ÜÂüüÔºåÂèØ‰ª•‰ΩøÊú∫Âô®‰∫∫Êõ¥Â•ΩÂú∞ÁêÜËß£Áî®Êà∑ÁöÑËßÜËßâÊåá‰ª§Ôºå‰ªéËÄåÂÆåÊàêÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io

