---
layout: default
title: MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning
---

# MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.03142" target="_blank" class="toolbar-btn">arXiv: 2510.03142v1</a>
    <a href="https://arxiv.org/pdf/2510.03142.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03142v1" 
            onclick="toggleFavorite(this, '2510.03142v1', 'MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, He Wang

**ÂàÜÁ±ª**: cs.RO, cs.CV

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-03

**Â§áÊ≥®**: Project page: https://pku-epic.github.io/MM-Nav-Web/

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫MM-NavÔºö‰∏ÄÁßçÂü∫‰∫éÂ§öËßÜËßíVLAÊ®°ÂûãÂíåÂ§ö‰∏ìÂÆ∂Â≠¶‰π†ÁöÑÈ≤ÅÊ£íËßÜËßâÂØºËà™ÊñπÊ≥ï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±ÂÖ≠ÔºöËßÜÈ¢ëÊèêÂèñ‰∏éÂåπÈÖç (Video Extraction)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâÂØºËà™` `Â§öËßÜËßíÂ≠¶‰π†` `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã` `Â§ö‰∏ìÂÆ∂Â≠¶‰π†` `Âº∫ÂåñÂ≠¶‰π†` `Ê®°‰ªøÂ≠¶‰π†` `Êú∫Âô®‰∫∫`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâËßÜËßâÂØºËà™ÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂª∫Ê®°ËßÜËßâ‰ø°ÊÅØÔºå‰æùËµñÂ§ßÈáèÊï∞ÊçÆÂíåÊô∫ËÉΩÊ®°Âûã„ÄÇ
2. ÊèêÂá∫MM-NavÔºåÂà©Áî®Â§öËßÜËßíVLAÊ®°ÂûãÂíåÂ§ö‰∏ìÂÆ∂Â≠¶‰π†Ôºå‰ªéÂêàÊàêÊï∞ÊçÆ‰∏≠Â≠¶‰π†Â§öÊ†∑ÂØºËà™ËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåMM-NavÂú®ÂêàÊàêÂíåÁúüÂÆûÁéØÂ¢É‰∏≠ÂùáË°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Ë∂ÖË∂ä‰∫ÜRLÊïôÂ∏à„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËßÜËßâÂØºËà™Á≠ñÁï•ÈÄöËøáÊ®°‰ªø‰∫∫Á±ªÂà©Áî®Ëá™Êàë‰∏≠ÂøÉÁöÑËßÜËßâËßÇÂØüËøõË°åÂØºËà™ÔºåË¢´ÂπøÊ≥õËÆ§‰∏∫ÊòØÂæàÊúâÂâçÊôØÁöÑÊñπÂêë„ÄÇÁÑ∂ËÄåÔºåËßÜËßâËßÇÂØüÁöÑÂÖâÂ≠¶‰ø°ÊÅØÈöæ‰ª•ÂÉèÊøÄÂÖâÈõ∑ËææÁÇπ‰∫ëÊàñÊ∑±Â∫¶ÂõæÈÇ£Ê†∑Ë¢´ÊòæÂºèÂª∫Ê®°ÔºåËøôÈúÄË¶ÅÊô∫ËÉΩÊ®°ÂûãÂíåÂ§ßËßÑÊ®°Êï∞ÊçÆ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫Âà©Áî®ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÊô∫ËÉΩÔºå‰ª•teacher-studentÁöÑÊñπÂºè‰ªéÂêàÊàê‰∏ìÂÆ∂Êï∞ÊçÆ‰∏≠Â≠¶‰π†Â§öÊ†∑ÂåñÁöÑÂØºËà™ËÉΩÂäõ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Âü∫‰∫éÈ¢ÑËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåËßÜËßâÂü∫Á°ÄÊ®°ÂûãÔºåÂ∞ÜVLAÊ®°ÂûãMM-NavÂÆûÁé∞‰∏∫‰∏Ä‰∏™Â§öËßÜËßíVLAÔºàÂÖ∑Êúâ360Â∫¶ËßÇÂØüÔºâ„ÄÇÂØπ‰∫éÂ§ßËßÑÊ®°ÂØºËà™Êï∞ÊçÆÔºåÊàë‰ª¨‰ªé‰∏â‰∏™Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰∏ìÂÆ∂Êî∂ÈõÜ‰∏ìÂÆ∂Êï∞ÊçÆÔºåËøô‰∫õ‰∏ìÂÆ∂Âú®‰∏â‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂÆöÂà∂ÁéØÂ¢É‰∏≠Êé•Âèó‰∫ÜÂÖ∑ÊúâÁâπÊùÉÁöÑÊ∑±Â∫¶‰ø°ÊÅØËÆ≠ÁªÉÔºå‰ª•ÂÆûÁé∞‰∏çÂêåÁöÑÂØºËà™ËÉΩÂäõÔºöÂà∞Ëææ„ÄÅÊå§ÂéãÂíåÈÅøÈöú„ÄÇÊàë‰ª¨‰ΩøÁî®‰ªéRL‰∏ìÂÆ∂Âú®Á∫øÊî∂ÈõÜÁöÑÊï∞ÊçÆËø≠‰ª£Âú∞ËÆ≠ÁªÉÊàë‰ª¨ÁöÑVLAÊ®°ÂûãÔºåÂÖ∂‰∏≠ËÆ≠ÁªÉÊØî‰æãÂü∫‰∫éÂêÑ‰∏™ËÉΩÂäõÁöÑÊÄßËÉΩËøõË°åÂä®ÊÄÅÂπ≥Ë°°„ÄÇÈÄöËøáÂú®ÂêàÊàêÁéØÂ¢É‰∏≠ËøõË°åÁöÑÂ§ßÈáèÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑÊ®°ÂûãÂÆûÁé∞‰∫ÜÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂèëÁé∞Êàë‰ª¨ÁöÑÂ≠¶ÁîüVLAÊ®°Âûã‰ºò‰∫éRLÊïôÂ∏àÔºåËØÅÊòé‰∫ÜÊï¥ÂêàÂ§öÁßçËÉΩÂäõÁöÑÂçèÂêåÊïàÂ∫î„ÄÇÂ§ßÈáèÁöÑÁúüÂÆû‰∏ñÁïåÂÆûÈ™åËøõ‰∏ÄÊ≠•ËØÅÂÆû‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËßÜËßâÂØºËà™‰ªªÂä°Êó®Âú®‰ΩøÊô∫ËÉΩ‰Ωì‰ªÖÈÄöËøáËßÜËßâËæìÂÖ•Ëá™‰∏ªÂØºËà™Âà∞ÁõÆÊ†á‰ΩçÁΩÆ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈöæ‰ª•ÊúâÊïàÂª∫Ê®°ËßÜËßâ‰ø°ÊÅØÔºåÂπ∂‰∏îÈúÄË¶ÅÂ§ßÈáèÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÊ≥õÂåñËÉΩÂäõÊúâÈôê„ÄÇÊ≠§Â§ñÔºå‰∏çÂêåÂØºËà™Âú∫ÊôØÈúÄË¶Å‰∏çÂêåÁöÑÂØºËà™Á≠ñÁï•ÔºåÂçï‰∏ÄÊ®°ÂûãÈöæ‰ª•ÈÄÇÂ∫îÊâÄÊúâÂú∫ÊôØ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÔºåÈÄöËøáÊ®°‰ªøÂ≠¶‰π†ÁöÑÊñπÂºèÔºå‰ªéÂ§ö‰∏™Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰∏ìÂÆ∂‰∏≠Â≠¶‰π†‰∏çÂêåÁöÑÂØºËà™ËÉΩÂäõ„ÄÇÈÄöËøáÂ§ö‰∏ìÂÆ∂Â≠¶‰π†Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊ†πÊçÆ‰∏çÂêåÁöÑÂú∫ÊôØÈÄâÊã©ÂêàÈÄÇÁöÑÂØºËà™Á≠ñÁï•Ôºå‰ªéËÄåÊèêÈ´òÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöMM-NavÈááÁî®Â§öËßÜËßíVLAÊû∂ÊûÑÔºå‰ΩøÁî®360Â∫¶ÂÖ®ÊôØËßÜËßâËæìÂÖ•„ÄÇÊï¥‰ΩìÊµÅÁ®ãÂåÖÊã¨Ôºö1) ‰ΩøÁî®‰∏â‰∏™RL‰∏ìÂÆ∂ÔºàÂàÜÂà´ÈíàÂØπÂà∞Ëææ„ÄÅÊå§ÂéãÂíåÈÅøÈöú‰ªªÂä°ÔºâÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆÔºõ2) ‰ΩøÁî®Ëøô‰∫õÊï∞ÊçÆËÆ≠ÁªÉVLAÊ®°ÂûãÔºõ3) Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂä®ÊÄÅË∞ÉÊï¥‰∏çÂêå‰∏ìÂÆ∂Êï∞ÊçÆÁöÑËÆ≠ÁªÉÊØî‰æãÔºå‰ª•Âπ≥Ë°°‰∏çÂêåËÉΩÂäõÁöÑÂ≠¶‰π†Ôºõ4) Âú®ÁúüÂÆûÁéØÂ¢É‰∏≠ËøõË°åÊµãËØïÔºåÈ™åËØÅÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**Ôºö1) ÊèêÂá∫Â§öËßÜËßíVLAÊ®°ÂûãÔºåËÉΩÂ§üÊúâÊïàÂà©Áî®ÂÖ®ÊôØËßÜËßâ‰ø°ÊÅØËøõË°åÂØºËà™Ôºõ2) ÈááÁî®Â§ö‰∏ìÂÆ∂Â≠¶‰π†Á≠ñÁï•Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†Âà∞Â§öÊ†∑ÂåñÁöÑÂØºËà™ËÉΩÂäõÔºõ3) Âä®ÊÄÅË∞ÉÊï¥ËÆ≠ÁªÉÊØî‰æãÔºåÂπ≥Ë°°‰∏çÂêåËÉΩÂäõÁöÑÂ≠¶‰π†ÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**Ôºö1) ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåËßÜËßâÂü∫Á°ÄÊ®°Âûã‰Ωú‰∏∫VLAÊ®°ÂûãÁöÑÂü∫Á°ÄÔºõ2) ‰∏â‰∏™RL‰∏ìÂÆ∂ÂàÜÂà´‰ΩøÁî®‰∏çÂêåÁöÑÂ•ñÂä±ÂáΩÊï∞ÂíåÁéØÂ¢ÉËÆæÁΩÆÔºå‰ª•ËÆ≠ÁªÉ‰∏çÂêåÁöÑÂØºËà™ËÉΩÂäõÔºõ3) Âä®ÊÄÅËÆ≠ÁªÉÊØî‰æãÁöÑË∞ÉÊï¥Âü∫‰∫éVLAÊ®°ÂûãÂú®ÂêÑ‰∏™‰ªªÂä°‰∏äÁöÑË°®Áé∞Ôºå‰ΩøÁî®ÁÆÄÂçïÁöÑÊØî‰æãË∞ÉÊï¥Á≠ñÁï•„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

MM-NavÂú®ÂêàÊàêÁéØÂ¢É‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂‰∏îË∂ÖË∂ä‰∫Ü‰Ωú‰∏∫ÊïôÂ∏àÁöÑRL‰∏ìÂÆ∂ÔºåËØÅÊòé‰∫ÜÂ§ö‰∏ìÂÆ∂Â≠¶‰π†ÁöÑÂçèÂêåÊïàÂ∫î„ÄÇÂú®ÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠ÔºåMM-Nav‰πüÂèñÂæó‰∫ÜÊòæËëóÁöÑÊàêÊûúÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂÖ∑‰ΩìÊÄßËÉΩÊï∞ÊçÆÊú™Áü•Ôºå‰ΩÜËÆ∫ÊñáÂº∫Ë∞É‰∫ÜÂÖ∂‰ºò‰∫éRLÊïôÂ∏à„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÊú∫Âô®‰∫∫ÂØºËà™„ÄÅËá™Âä®È©æÈ©∂„ÄÅËôöÊãüÁé∞ÂÆûÁ≠âÈ¢ÜÂüü„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Áî®‰∫éÂºÄÂèëËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠Ëá™‰∏ªÂØºËà™ÁöÑÊú∫Âô®‰∫∫ÔºåÊàñËÄÖÁî®‰∫éÊûÑÂª∫Êõ¥Âä†Êô∫ËÉΩÂíåÂÆâÂÖ®ÁöÑËá™Âä®È©æÈ©∂Á≥ªÁªü„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïËøòÂèØ‰ª•Â∫îÁî®‰∫éËôöÊãüÁé∞ÂÆûÊ∏∏Êàè‰∏≠ÔºåÊèêÈ´òÊ∏∏ÊàèËßíËâ≤ÁöÑÊô∫ËÉΩÊÄßÂíå‰∫§‰∫íÊÄß„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method.

