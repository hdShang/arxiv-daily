---
layout: default
title: "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning"
---

# MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.03142" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.03142v1</a>
  <a href="https://arxiv.org/pdf/2510.03142.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.03142v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.03142v1', 'MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, He Wang

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-03

**å¤‡æ³¨**: Project page: https://pku-epic.github.io/MM-Nav-Web/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMM-Navï¼šä¸€ç§åŸºäºå¤šè§†è§’VLAæ¨¡å‹å’Œå¤šä¸“å®¶å­¦ä¹ çš„é²æ£’è§†è§‰å¯¼èˆªæ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰å¯¼èˆª` `å¤šè§†è§’å­¦ä¹ ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `å¤šä¸“å®¶å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è§†è§‰å¯¼èˆªæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡è§†è§‰ä¿¡æ¯ï¼Œä¾èµ–å¤§é‡æ•°æ®å’Œæ™ºèƒ½æ¨¡å‹ã€‚
2. æå‡ºMM-Navï¼Œåˆ©ç”¨å¤šè§†è§’VLAæ¨¡å‹å’Œå¤šä¸“å®¶å­¦ä¹ ï¼Œä»åˆæˆæ•°æ®ä¸­å­¦ä¹ å¤šæ ·å¯¼èˆªèƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒMM-Navåœ¨åˆæˆå’ŒçœŸå®ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è¶…è¶Šäº†RLæ•™å¸ˆã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰å¯¼èˆªç­–ç•¥é€šè¿‡æ¨¡ä»¿äººç±»åˆ©ç”¨è‡ªæˆ‘ä¸­å¿ƒçš„è§†è§‰è§‚å¯Ÿè¿›è¡Œå¯¼èˆªï¼Œè¢«å¹¿æ³›è®¤ä¸ºæ˜¯å¾ˆæœ‰å‰æ™¯çš„æ–¹å‘ã€‚ç„¶è€Œï¼Œè§†è§‰è§‚å¯Ÿçš„å…‰å­¦ä¿¡æ¯éš¾ä»¥åƒæ¿€å…‰é›·è¾¾ç‚¹äº‘æˆ–æ·±åº¦å›¾é‚£æ ·è¢«æ˜¾å¼å»ºæ¨¡ï¼Œè¿™éœ€è¦æ™ºèƒ½æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ™ºèƒ½ï¼Œä»¥teacher-studentçš„æ–¹å¼ä»åˆæˆä¸“å®¶æ•°æ®ä¸­å­¦ä¹ å¤šæ ·åŒ–çš„å¯¼èˆªèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå°†VLAæ¨¡å‹MM-Navå®ç°ä¸ºä¸€ä¸ªå¤šè§†è§’VLAï¼ˆå…·æœ‰360åº¦è§‚å¯Ÿï¼‰ã€‚å¯¹äºå¤§è§„æ¨¡å¯¼èˆªæ•°æ®ï¼Œæˆ‘ä»¬ä»ä¸‰ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸“å®¶æ”¶é›†ä¸“å®¶æ•°æ®ï¼Œè¿™äº›ä¸“å®¶åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å®šåˆ¶ç¯å¢ƒä¸­æ¥å—äº†å…·æœ‰ç‰¹æƒçš„æ·±åº¦ä¿¡æ¯è®­ç»ƒï¼Œä»¥å®ç°ä¸åŒçš„å¯¼èˆªèƒ½åŠ›ï¼šåˆ°è¾¾ã€æŒ¤å‹å’Œé¿éšœã€‚æˆ‘ä»¬ä½¿ç”¨ä»RLä¸“å®¶åœ¨çº¿æ”¶é›†çš„æ•°æ®è¿­ä»£åœ°è®­ç»ƒæˆ‘ä»¬çš„VLAæ¨¡å‹ï¼Œå…¶ä¸­è®­ç»ƒæ¯”ä¾‹åŸºäºå„ä¸ªèƒ½åŠ›çš„æ€§èƒ½è¿›è¡ŒåŠ¨æ€å¹³è¡¡ã€‚é€šè¿‡åœ¨åˆæˆç¯å¢ƒä¸­è¿›è¡Œçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å­¦ç”ŸVLAæ¨¡å‹ä¼˜äºRLæ•™å¸ˆï¼Œè¯æ˜äº†æ•´åˆå¤šç§èƒ½åŠ›çš„ååŒæ•ˆåº”ã€‚å¤§é‡çš„çœŸå®ä¸–ç•Œå®éªŒè¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè§†è§‰å¯¼èˆªä»»åŠ¡æ—¨åœ¨ä½¿æ™ºèƒ½ä½“ä»…é€šè¿‡è§†è§‰è¾“å…¥è‡ªä¸»å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡è§†è§‰ä¿¡æ¯ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æ­¤å¤–ï¼Œä¸åŒå¯¼èˆªåœºæ™¯éœ€è¦ä¸åŒçš„å¯¼èˆªç­–ç•¥ï¼Œå•ä¸€æ¨¡å‹éš¾ä»¥é€‚åº”æ‰€æœ‰åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ çš„æ–¹å¼ï¼Œä»å¤šä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸“å®¶ä¸­å­¦ä¹ ä¸åŒçš„å¯¼èˆªèƒ½åŠ›ã€‚é€šè¿‡å¤šä¸“å®¶å­¦ä¹ ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒçš„åœºæ™¯é€‰æ‹©åˆé€‚çš„å¯¼èˆªç­–ç•¥ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMM-Navé‡‡ç”¨å¤šè§†è§’VLAæ¶æ„ï¼Œä½¿ç”¨360åº¦å…¨æ™¯è§†è§‰è¾“å…¥ã€‚æ•´ä½“æµç¨‹åŒ…æ‹¬ï¼š1) ä½¿ç”¨ä¸‰ä¸ªRLä¸“å®¶ï¼ˆåˆ†åˆ«é’ˆå¯¹åˆ°è¾¾ã€æŒ¤å‹å’Œé¿éšœä»»åŠ¡ï¼‰ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼›2) ä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒVLAæ¨¡å‹ï¼›3) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŠ¨æ€è°ƒæ•´ä¸åŒä¸“å®¶æ•°æ®çš„è®­ç»ƒæ¯”ä¾‹ï¼Œä»¥å¹³è¡¡ä¸åŒèƒ½åŠ›çš„å­¦ä¹ ï¼›4) åœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œæµ‹è¯•ï¼ŒéªŒè¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼š1) æå‡ºå¤šè§†è§’VLAæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å…¨æ™¯è§†è§‰ä¿¡æ¯è¿›è¡Œå¯¼èˆªï¼›2) é‡‡ç”¨å¤šä¸“å®¶å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°å¤šæ ·åŒ–çš„å¯¼èˆªèƒ½åŠ›ï¼›3) åŠ¨æ€è°ƒæ•´è®­ç»ƒæ¯”ä¾‹ï¼Œå¹³è¡¡ä¸åŒèƒ½åŠ›çš„å­¦ä¹ ï¼Œæé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼š1) ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºVLAæ¨¡å‹çš„åŸºç¡€ï¼›2) ä¸‰ä¸ªRLä¸“å®¶åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„å¥–åŠ±å‡½æ•°å’Œç¯å¢ƒè®¾ç½®ï¼Œä»¥è®­ç»ƒä¸åŒçš„å¯¼èˆªèƒ½åŠ›ï¼›3) åŠ¨æ€è®­ç»ƒæ¯”ä¾‹çš„è°ƒæ•´åŸºäºVLAæ¨¡å‹åœ¨å„ä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½¿ç”¨ç®€å•çš„æ¯”ä¾‹è°ƒæ•´ç­–ç•¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MM-Navåœ¨åˆæˆç¯å¢ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”è¶…è¶Šäº†ä½œä¸ºæ•™å¸ˆçš„RLä¸“å®¶ï¼Œè¯æ˜äº†å¤šä¸“å®¶å­¦ä¹ çš„ååŒæ•ˆåº”ã€‚åœ¨çœŸå®ä¸–ç•Œå®éªŒä¸­ï¼ŒMM-Navä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æˆæœï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†è®ºæ–‡å¼ºè°ƒäº†å…¶ä¼˜äºRLæ•™å¸ˆã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨äºå¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­è‡ªä¸»å¯¼èˆªçš„æœºå™¨äººï¼Œæˆ–è€…ç”¨äºæ„å»ºæ›´åŠ æ™ºèƒ½å’Œå®‰å…¨çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®æ¸¸æˆä¸­ï¼Œæé«˜æ¸¸æˆè§’è‰²çš„æ™ºèƒ½æ€§å’Œäº¤äº’æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method.

