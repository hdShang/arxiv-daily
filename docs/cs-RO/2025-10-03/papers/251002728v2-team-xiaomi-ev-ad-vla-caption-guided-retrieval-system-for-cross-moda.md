---
layout: default
title: "Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4"
---

# Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.02728" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.02728v2</a>
  <a href="https://arxiv.org/pdf/2510.02728.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.02728v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.02728v2', 'Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lingfeng Zhang, Erjia Xiao, Yuchen Zhang, Haoxiang Fu, Ruibin Hu, Yanbiao Ma, Wenbo Ding, Long Chen, Hangjun Ye, Xiaoshuai Hao

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-10-03 (æ›´æ–°: 2025-11-06)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºCaptionå¼•å¯¼çš„æ£€ç´¢ç³»ç»Ÿï¼Œæå‡è·¨æ¨¡æ€æ— äººæœºå¯¼èˆªä¸­å›¾æ–‡æ£€ç´¢çš„ç²¾åº¦ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è·¨æ¨¡æ€æ£€ç´¢` `æ— äººæœºå¯¼èˆª` `è§†è§‰-è¯­è¨€æ¨¡å‹` `å›¾åƒæè¿°ç”Ÿæˆ` `è¯­ä¹‰ç›¸ä¼¼åº¦` `å›¾åƒæ£€ç´¢` `è‡ªç„¶è¯­è¨€å¼•å¯¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è·¨æ¨¡æ€æ— äººæœºå¯¼èˆªæ–¹æ³•éš¾ä»¥å®ç°æ–‡æœ¬æŸ¥è¯¢å’Œå¤æ‚èˆªæ‹åœºæ™¯ä¹‹é—´çš„ç»†ç²’åº¦è¯­ä¹‰åŒ¹é…ã€‚
2. æå‡ºCaptionå¼•å¯¼çš„æ£€ç´¢ç³»ç»Ÿï¼Œåˆ©ç”¨VLMç”Ÿæˆå›¾åƒæè¿°ï¼Œæ„å»ºè§†è§‰å†…å®¹å’Œè‡ªç„¶è¯­è¨€æè¿°ä¹‹é—´çš„è¯­ä¹‰æ¡¥æ¢ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…³é”®æŒ‡æ ‡ä¸Šå®ç°äº†5%çš„ç¨³å®šæå‡ï¼Œå¹¶åœ¨RoboSenseæŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬äºŒåã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡é’ˆå¯¹è·¨æ¨¡æ€æ— äººæœºå¯¼èˆªä¸­ï¼ŒåŸºäºè‡ªç„¶è¯­è¨€æè¿°ä»å¤§è§„æ¨¡æ•°æ®åº“ä¸­é«˜æ•ˆæ£€ç´¢ç›¸å…³å›¾åƒçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ£€ç´¢ä¼˜åŒ–æ–¹æ³•ï¼šCaptionå¼•å¯¼çš„æ£€ç´¢ç³»ç»Ÿï¼ˆCGRSï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æ™ºèƒ½é‡æ’åºæ¥å¢å¼ºåŸºçº¿æ¨¡å‹çš„ç²—ç•¥æ’åºç»“æœã€‚é¦–å…ˆï¼Œåˆ©ç”¨åŸºçº¿æ¨¡å‹è·å¾—æ¯ä¸ªæŸ¥è¯¢æœ€ç›¸å…³çš„Top 20å›¾åƒçš„åˆå§‹ç²—ç•¥æ’åºã€‚ç„¶åï¼Œä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸ºè¿™äº›å€™é€‰å›¾åƒç”Ÿæˆè¯¦ç»†çš„æè¿°ï¼Œæ•æ‰å…¶ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚æœ€åï¼Œåœ¨å¤šæ¨¡æ€ç›¸ä¼¼åº¦è®¡ç®—æ¡†æ¶ä¸­ä½¿ç”¨ç”Ÿæˆçš„æè¿°å¯¹åŸå§‹æ–‡æœ¬æŸ¥è¯¢è¿›è¡Œç»†ç²’åº¦çš„é‡æ’åºï¼Œä»è€Œæœ‰æ•ˆåœ°æ„å»ºè§†è§‰å†…å®¹å’Œè‡ªç„¶è¯­è¨€æè¿°ä¹‹é—´çš„è¯­ä¹‰æ¡¥æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ï¼ˆRecall@1ã€Recall@5å’ŒRecall@10ï¼‰ä¸Šå‡å®ç°äº†5%çš„ç¨³å®šæå‡ï¼Œå¹¶åœ¨RoboSense 2025æŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬äºŒåï¼ŒéªŒè¯äº†è¯¥è¯­ä¹‰ä¼˜åŒ–ç­–ç•¥åœ¨å®é™…æœºå™¨äººå¯¼èˆªåœºæ™¯ä¸­çš„ä»·å€¼ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³è·¨æ¨¡æ€æ— äººæœºå¯¼èˆªä¸­ï¼Œè‡ªç„¶è¯­è¨€å¼•å¯¼çš„è·¨è§†è§’å›¾åƒæ£€ç´¢é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚èˆªæ‹åœºæ™¯æ—¶ï¼Œéš¾ä»¥å®ç°æ–‡æœ¬æŸ¥è¯¢å’Œè§†è§‰å†…å®¹ä¹‹é—´çš„ç»†ç²’åº¦è¯­ä¹‰åŒ¹é…ï¼Œå¯¼è‡´æ£€ç´¢ç²¾åº¦ä¸é«˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆå›¾åƒçš„è¯¦ç»†æè¿°ï¼ˆCaptionï¼‰ï¼Œä»è€Œå°†å›¾åƒçš„è§†è§‰ä¿¡æ¯è½¬åŒ–ä¸ºæ–‡æœ¬ä¿¡æ¯ï¼Œæ„å»ºæ–‡æœ¬æŸ¥è¯¢å’Œå›¾åƒä¹‹é—´çš„è¯­ä¹‰æ¡¥æ¢ã€‚é€šè¿‡æ¯”è¾ƒæ–‡æœ¬æŸ¥è¯¢å’Œå›¾åƒæè¿°çš„ç›¸ä¼¼åº¦ï¼Œå®ç°æ›´ç²¾ç¡®çš„å›¾åƒæ£€ç´¢ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šCGRSç³»ç»ŸåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) ç²—ç•¥æ£€ç´¢é˜¶æ®µï¼šä½¿ç”¨åŸºçº¿æ¨¡å‹ï¼ˆå…·ä½“æ¨¡å‹æœªçŸ¥ï¼‰å¯¹å›¾åƒæ•°æ®åº“è¿›è¡Œåˆæ­¥æ£€ç´¢ï¼Œå¾—åˆ°Top 20çš„å€™é€‰å›¾åƒã€‚2) ç²¾ç»†é‡æ’åºé˜¶æ®µï¼šä½¿ç”¨VLMä¸ºæ¯ä¸ªå€™é€‰å›¾åƒç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼›ç„¶åï¼Œè®¡ç®—æ–‡æœ¬æŸ¥è¯¢å’Œæ¯ä¸ªå›¾åƒæè¿°ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼›æœ€åï¼Œæ ¹æ®ç›¸ä¼¼åº¦å¯¹å€™é€‰å›¾åƒè¿›è¡Œé‡æ’åºï¼Œå¾—åˆ°æœ€ç»ˆçš„æ£€ç´¢ç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºåˆ©ç”¨VLMç”Ÿæˆå›¾åƒæè¿°ï¼Œä»è€Œå°†å›¾åƒçš„è§†è§‰ä¿¡æ¯è½¬åŒ–ä¸ºæ–‡æœ¬ä¿¡æ¯ï¼Œå®ç°æ–‡æœ¬æŸ¥è¯¢å’Œå›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å›¾åƒçš„ç»†ç²’åº¦è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜æ£€ç´¢ç²¾åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€å¯¹å›¾åƒè¿›è¡Œå¤æ‚çš„ç‰¹å¾æå–å’Œè¡¨ç¤ºï¼Œè€Œæ˜¯ç›´æ¥åˆ©ç”¨VLMç”Ÿæˆæ–‡æœ¬æè¿°ï¼Œç®€åŒ–äº†æ£€ç´¢æµç¨‹ã€‚

**å…³é”®è®¾è®¡**ï¼šè®ºæ–‡ä¸­æœªè¯¦ç»†è¯´æ˜VLMçš„å…·ä½“é€‰æ‹©ã€å›¾åƒæè¿°ç”Ÿæˆçš„å…·ä½“æ–¹æ³•ã€ä»¥åŠç›¸ä¼¼åº¦è®¡ç®—çš„å…·ä½“æ–¹å¼ã€‚è¿™äº›éƒ½æ˜¯å½±å“ç³»ç»Ÿæ€§èƒ½çš„å…³é”®è®¾è®¡ç»†èŠ‚ï¼Œéœ€è¦åœ¨åç»­ç ”ç©¶ä¸­è¿›ä¸€æ­¥æ¢ç´¢ã€‚åŸºçº¿æ¨¡å‹çš„é€‰æ‹©ä¹Ÿå¯èƒ½å½±å“æœ€ç»ˆçš„æ£€ç´¢æ•ˆæœã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Captionå¼•å¯¼çš„æ£€ç´¢ç³»ç»Ÿï¼ˆCGRSï¼‰åœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ï¼ˆRecall@1ã€Recall@5å’ŒRecall@10ï¼‰ä¸Šå‡å®ç°äº†5%çš„ç¨³å®šæå‡ã€‚è¯¥æ–¹æ³•åœ¨RoboSense 2025æŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬äºŒåï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…æœºå™¨äººå¯¼èˆªåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“åŸºçº¿æ¨¡å‹çš„æ€§èƒ½æ•°æ®æœªçŸ¥ï¼Œä½†CGRSçš„æå‡å¹…åº¦æ˜ç¡®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºæ— äººæœºè‡ªä¸»å¯¼èˆªã€æ™ºèƒ½å®‰é˜²ã€é¥æ„Ÿå›¾åƒåˆ†æç­‰é¢†åŸŸã€‚é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°æ§åˆ¶æ— äººæœºè¿›è¡Œç›®æ ‡æœç´¢å’Œå®šä½ï¼Œæé«˜æ— äººæœºåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é€‚åº”æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚è¯¥æŠ€æœ¯è¿˜æœ‰æ½œåŠ›åº”ç”¨äºè·¨æ¨¡æ€ä¿¡æ¯æ£€ç´¢ã€æ™ºèƒ½é—®ç­”ç­‰é¢†åŸŸã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Cross-modal drone navigation remains a challenging task in robotics, requiring efficient retrieval of relevant images from large-scale databases based on natural language descriptions. The RoboSense 2025 Track 4 challenge addresses this challenge, focusing on robust, natural language-guided cross-view image retrieval across multiple platforms (drones, satellites, and ground cameras). Current baseline methods, while effective for initial retrieval, often struggle to achieve fine-grained semantic matching between text queries and visual content, especially in complex aerial scenes. To address this challenge, we propose a two-stage retrieval refinement method: Caption-Guided Retrieval System (CGRS) that enhances the baseline coarse ranking through intelligent reranking. Our method first leverages a baseline model to obtain an initial coarse ranking of the top 20 most relevant images for each query. We then use Vision-Language-Model (VLM) to generate detailed captions for these candidate images, capturing rich semantic descriptions of their visual content. These generated captions are then used in a multimodal similarity computation framework to perform fine-grained reranking of the original text query, effectively building a semantic bridge between the visual content and natural language descriptions. Our approach significantly improves upon the baseline, achieving a consistent 5\% improvement across all key metrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the challenge, demonstrating the practical value of our semantic refinement strategy in real-world robotic navigation scenarios.

