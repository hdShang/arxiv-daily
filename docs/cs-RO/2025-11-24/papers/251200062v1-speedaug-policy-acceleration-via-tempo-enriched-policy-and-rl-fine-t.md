---
layout: default
title: SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning
---

# SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.00062" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.00062v1</a>
  <a href="https://arxiv.org/pdf/2512.00062.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.00062v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.00062v1', 'SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Taewook Nam, Sung Ju Hwang

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-24

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**SpeedAugï¼šé€šè¿‡é€Ÿåº¦å¢å¼ºç­–ç•¥å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒåŠ é€Ÿæœºå™¨äººç­–ç•¥å­¦ä¹ **

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æœºå™¨äººç­–ç•¥å­¦ä¹ ` `ç­–ç•¥åŠ é€Ÿ` `å¼ºåŒ–å­¦ä¹ ` `é€Ÿåº¦å¢å¼º` `æ ·æœ¬æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººç­–ç•¥å­¦ä¹ æ–¹æ³•æ‰§è¡Œé€Ÿåº¦æ…¢ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç¡¬ä»¶æ€§èƒ½ï¼Œä¸”åŠ é€Ÿç­–ç•¥å®¹æ˜“äº§ç”Ÿåˆ†å¸ƒåç§»ã€‚
2. SpeedAugé€šè¿‡é€Ÿåº¦å¢å¼ºæ¼”ç¤ºæ•°æ®é¢„è®­ç»ƒç­–ç•¥ï¼Œæ„å»ºåŒ…å«å¤šç§é€Ÿåº¦çš„ä»»åŠ¡æ‰§è¡Œè¡Œä¸ºå…ˆéªŒã€‚
3. å®éªŒè¡¨æ˜ï¼ŒåŸºäºSpeedAugåˆå§‹åŒ–çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œèƒ½æ˜¾è‘—æå‡æ ·æœ¬æ•ˆç‡å¹¶ä¿æŒè¾ƒé«˜æˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæœºå™¨äººç­–ç•¥å­¦ä¹ åœ¨çœŸå®ç¯å¢ƒä¸­å®ç°äº†å¤æ‚çš„æ“æ§ä»»åŠ¡ï¼Œä½†ç”±äºæ”¶é›†æ›´å¿«æ¼”ç¤ºæ•°æ®çš„æˆæœ¬è¾ƒé«˜ï¼Œç­–ç•¥çš„æ‰§è¡Œé€Ÿåº¦é€šå¸¸æ»åäºç¡¬ä»¶èƒ½åŠ›ã€‚ç°æœ‰çš„ç­–ç•¥åŠ é€Ÿæ–¹æ³•é€šè¿‡é‡æ–°è§£é‡ŠåŠ¨ä½œåºåˆ—ä»¥é€‚åº”æœªè§è¿‡çš„æ‰§è¡Œé€Ÿåº¦ï¼Œä»è€Œå¯¼è‡´ä¸åŸå§‹æ¼”ç¤ºæ•°æ®çš„åˆ†å¸ƒåç§»ã€‚å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦é¢å¤–æ¼”ç¤ºæ•°æ®çš„æƒ…å†µä¸‹è°ƒæ•´ç­–ç•¥ä»¥å®ç°æ›´å¿«çš„æ‰§è¡Œé€Ÿåº¦ï¼Œä½†å…¶æ— å¼•å¯¼çš„æ¢ç´¢æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†SpeedAugï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥åŠ é€Ÿæ¡†æ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è°ƒæ•´é¢„è®­ç»ƒç­–ç•¥ä»¥å®ç°æ›´å¿«çš„ä»»åŠ¡æ‰§è¡Œã€‚SpeedAugé€šè¿‡åœ¨é€Ÿåº¦å¢å¼ºçš„æ¼”ç¤ºæ•°æ®ä¸Šé¢„è®­ç»ƒç­–ç•¥ï¼Œæ„å»ºåŒ…å«ä»»åŠ¡æ‰§è¡Œå„ç§é€Ÿåº¦çš„è¡Œä¸ºå…ˆéªŒã€‚åœ¨æœºå™¨äººæ“æ§åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä»è¿™ç§é€Ÿåº¦å¢å¼ºç­–ç•¥åˆå§‹åŒ–çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ˜¾è‘—æé«˜äº†ç°æœ‰å¼ºåŒ–å­¦ä¹ å’Œç­–ç•¥åŠ é€Ÿæ–¹æ³•çš„æ ·æœ¬æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æˆåŠŸç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººç­–ç•¥å­¦ä¹ ä¸­ï¼Œç­–ç•¥æ‰§è¡Œé€Ÿåº¦æ…¢äºç¡¬ä»¶èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰ç­–ç•¥åŠ é€Ÿæ–¹æ³•ä¾èµ–äºé‡æ–°è§£é‡ŠåŠ¨ä½œåºåˆ—ï¼Œå®¹æ˜“å¯¼è‡´ä¸åŸå§‹æ¼”ç¤ºæ•°æ®çš„åˆ†å¸ƒåç§»ã€‚å¼ºåŒ–å­¦ä¹ è™½ç„¶å¯ä»¥è‡ªé€‚åº”åœ°å­¦ä¹ æ›´å¿«çš„ç­–ç•¥ï¼Œä½†å…¶æ— å¼•å¯¼çš„æ¢ç´¢æ–¹å¼å¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é€Ÿåº¦å¢å¼ºï¼ˆSpeed Augmentationï¼‰æ¥æ„å»ºä¸€ä¸ªåŒ…å«å¤šç§é€Ÿåº¦ä¿¡æ¯çš„è¡Œä¸ºå…ˆéªŒï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¯¹è¯¥å…ˆéªŒç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ç­–ç•¥åŠ é€Ÿã€‚é€Ÿåº¦å¢å¼ºçš„ç›®çš„æ˜¯è®©ç­–ç•¥èƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ‰§è¡Œé€Ÿåº¦ï¼Œè€Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒåˆ™ç”¨äºè¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å¿«çš„é€Ÿåº¦ä¸‹ä¿æŒè¾ƒé«˜çš„æˆåŠŸç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSpeedAugæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1) é€Ÿåº¦å¢å¼ºçš„ç­–ç•¥é¢„è®­ç»ƒé˜¶æ®µï¼šé€šè¿‡å¯¹åŸå§‹æ¼”ç¤ºæ•°æ®è¿›è¡Œé€Ÿåº¦å¢å¼ºï¼Œç”ŸæˆåŒ…å«ä¸åŒé€Ÿåº¦ä¿¡æ¯çš„è®­ç»ƒæ•°æ®ï¼Œç„¶ååˆ©ç”¨è¿™äº›æ•°æ®é¢„è®­ç»ƒä¸€ä¸ªç­–ç•¥ç½‘ç»œã€‚é€Ÿåº¦å¢å¼ºå¯ä»¥é€šè¿‡è°ƒæ•´åŠ¨ä½œåºåˆ—çš„æ—¶é—´æ­¥é•¿æ¥å®ç°ã€‚2) å¼ºåŒ–å­¦ä¹ å¾®è°ƒé˜¶æ®µï¼šä½¿ç”¨é¢„è®­ç»ƒçš„ç­–ç•¥ç½‘ç»œä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOï¼‰å¯¹ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥çš„æ‰§è¡Œé€Ÿåº¦å’ŒæˆåŠŸç‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSpeedAugçš„å…³é”®åˆ›æ–°åœ¨äºå°†é€Ÿåº¦å¢å¼ºå’Œå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ç­–ç•¥åŠ é€Ÿã€‚é€Ÿåº¦å¢å¼ºæä¾›äº†ä¸€ä¸ªè‰¯å¥½çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œå‡å°‘äº†å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢ç©ºé—´ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚åŒæ—¶ï¼Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒèƒ½å¤Ÿè¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å¿«çš„é€Ÿåº¦ä¸‹ä¿æŒè¾ƒé«˜çš„æˆåŠŸç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSpeedAugä¸éœ€è¦é¢å¤–çš„æ¼”ç¤ºæ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„æ‰§è¡Œé€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨é€Ÿåº¦å¢å¼ºé˜¶æ®µï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†ä¸åŒçš„é€Ÿåº¦å› å­æ¥è°ƒæ•´åŠ¨ä½œåºåˆ—çš„æ—¶é—´æ­¥é•¿ï¼Œä¾‹å¦‚ï¼Œå°†åŠ¨ä½œåºåˆ—çš„æ—¶é—´æ­¥é•¿ç¼©çŸ­ä¸€åŠï¼Œä»è€Œå®ç°ä¸¤å€çš„é€Ÿåº¦æå‡ã€‚åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒé˜¶æ®µï¼Œè®ºæ–‡å¯èƒ½é‡‡ç”¨äº†PPOç®—æ³•ï¼Œå¹¶è®¾è®¡äº†åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œä¾‹å¦‚ï¼Œå¥–åŠ±ç­–ç•¥çš„æ‰§è¡Œé€Ÿåº¦å’Œä»»åŠ¡å®Œæˆçš„æˆåŠŸç‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å¯èƒ½æ˜¯ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºæˆ–å¾ªç¯ç¥ç»ç½‘ç»œï¼Œç”¨äºå°†çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSpeedAugåœ¨æœºå™¨äººæ“æ§åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚ä¸ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒSpeedAugèƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ°é«˜æ€§èƒ½çš„ç­–ç•¥ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¿æŒè¾ƒé«˜çš„æˆåŠŸç‡ã€‚å…·ä½“è€Œè¨€ï¼ŒSpeedAugåœ¨æŸäº›ä»»åŠ¡ä¸Šçš„æ ·æœ¬æ•ˆç‡æå‡äº†50%ä»¥ä¸Šï¼Œå¹¶ä¸”èƒ½å¤Ÿè¾¾åˆ°ä¸ç°æœ‰æ–¹æ³•ç›¸å½“ç”šè‡³æ›´é«˜çš„æˆåŠŸç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SpeedAugå¯åº”ç”¨äºå„ç§æœºå™¨äººæ“æ§ä»»åŠ¡ï¼Œä¾‹å¦‚è£…é…ã€æŠ“å–ã€æ”¾ç½®ç­‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå‡æœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„æ•ˆç‡ï¼Œé™ä½æ—¶é—´æˆæœ¬ï¼Œå¹¶æé«˜æœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚æœªæ¥ï¼ŒSpeedAugæœ‰æœ›åº”ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–ã€æ™ºèƒ½å®¶å±…ã€åŒ»ç–—æœºå™¨äººç­‰é¢†åŸŸï¼Œå®ç°æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„æœºå™¨äººæœåŠ¡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in robotic policy learning have enabled complex manipulation in real-world environments, yet the execution speed of these policies often lags behind hardware capabilities due to the cost of collecting faster demonstrations. Existing works on policy acceleration reinterpret action sequence for unseen execution speed, thereby encountering distributional shifts from the original demonstrations. Reinforcement learning is a promising approach that adapts policies for faster execution without additional demonstration, but its unguided exploration is sample inefficient. We propose SpeedAug, an RL-based policy acceleration framework that efficiently adapts pre-trained policies for faster task execution. SpeedAug constructs behavior prior that encompasses diverse tempos of task execution by pre-training a policy on speed-augmented demonstrations. Empirical results on robotic manipulation benchmarks show that RL fine-tuning initialized from this tempo-enriched policy significantly improves the sample efficiency of existing RL and policy acceleration methods while maintaining high success rate.

