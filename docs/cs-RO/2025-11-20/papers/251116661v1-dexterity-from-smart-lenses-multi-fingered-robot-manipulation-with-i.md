---
layout: default
title: "Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations"
---

# Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.16661" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.16661v1</a>
  <a href="https://arxiv.org/pdf/2511.16661.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16661v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.16661v1', 'Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Irmak Guzey, Haozhi Qi, Julen Urain, Changhao Wang, Jessica Yin, Krishna Bodduluri, Mike Lambeta, Lerrel Pinto, Akshara Rai, Jitendra Malik, Tingfan Wu, Akash Sharma, Homanga Bharadhwaj

**åˆ†ç±»**: cs.RO, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**AINAæ¡†æ¶ï¼šåˆ©ç”¨æ™ºèƒ½çœ¼é•œå’Œäººç±»æ¼”ç¤ºå­¦ä¹ å¤šæŒ‡æœºå™¨äººçµå·§æ“ä½œ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¤šæŒ‡çµå·§æ‰‹` `äººç±»æ¼”ç¤ºå­¦ä¹ ` `æ™ºèƒ½çœ¼é•œ` `3Dç‚¹äº‘` `ç­–ç•¥å­¦ä¹ ` `AINAæ¡†æ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•éš¾ä»¥ä»çœŸå®åœºæ™¯çš„äººç±»è§†é¢‘ä¸­æå–ç›¸å…³ä¸Šä¸‹æ–‡å’Œè¿åŠ¨çº¿ç´¢ï¼Œå¯¼è‡´éš¾ä»¥å­¦ä¹ è‡ªä¸»ç­–ç•¥ã€‚
2. AINAæ¡†æ¶åˆ©ç”¨Aria Gen 2çœ¼é•œè·å–çš„äººç±»æ•°æ®ï¼Œå­¦ä¹ å¯¹èƒŒæ™¯å˜åŒ–é²æ£’çš„3Dç‚¹äº‘ç­–ç•¥ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAINAæ¡†æ¶åœ¨å¤šä¸ªæ—¥å¸¸æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€æœºå™¨äººæ•°æ®å³å¯ç›´æ¥éƒ¨ç½²ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAINAçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡äººç±»åœ¨è‡ªç„¶ç¯å¢ƒä¸­æ‰§è¡Œæ—¥å¸¸ä»»åŠ¡çš„æ¼”ç¤ºæ¥å­¦ä¹ å¤šæŒ‡æœºå™¨äººç­–ç•¥ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Aria Gen 2çœ¼é•œæ”¶é›†çš„æ•°æ®ï¼Œè¿™äº›çœ¼é•œè½»ä¾¿ä¾¿æºï¼Œé…å¤‡é«˜åˆ†è¾¨ç‡RGBç›¸æœºï¼Œæä¾›ç²¾ç¡®çš„3Då¤´éƒ¨å’Œæ‰‹éƒ¨å§¿åŠ¿ï¼Œå¹¶æä¾›å¯ç”¨äºåœºæ™¯æ·±åº¦ä¼°è®¡çš„å®½ç«‹ä½“è§†é‡ã€‚AINAèƒ½å¤Ÿå­¦ä¹ åŸºäº3Dç‚¹çš„å¤šæŒ‡æ‰‹ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯¹èƒŒæ™¯å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œå¹¶ä¸”å¯ä»¥ç›´æ¥éƒ¨ç½²ï¼Œæ— éœ€ä»»ä½•æœºå™¨äººæ•°æ®ã€‚è¯¥ç ”ç©¶å°†AINAæ¡†æ¶ä¸å…ˆå‰çš„äººç±»åˆ°æœºå™¨äººç­–ç•¥å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼ŒéªŒè¯äº†è®¾è®¡é€‰æ‹©ï¼Œå¹¶åœ¨ä¹ä¸ªæ—¥å¸¸æ“ä½œä»»åŠ¡ä¸­å±•ç¤ºäº†ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨ä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ æœºå™¨äººç­–ç•¥æ—¶ï¼Œé¢ä¸´ç€äººç±»å’Œæœºå™¨äººä¹‹é—´çš„å½¢æ€å·®å¼‚ï¼Œä»¥åŠä»çœŸå®åœºæ™¯è§†é¢‘ä¸­æå–æœ‰æ•ˆä¿¡æ¯çš„æŒ‘æˆ˜ã€‚è¿™é™åˆ¶äº†æœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¢åŠ äº†å¯¹æœºå™¨äººæ•°æ®æ”¶é›†çš„ä¾èµ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ™ºèƒ½çœ¼é•œï¼ˆAria Gen 2ï¼‰è·å–é«˜è´¨é‡çš„äººç±»æ“ä½œæ•°æ®ï¼ŒåŒ…æ‹¬RGBå›¾åƒã€3Då¤´éƒ¨å’Œæ‰‹éƒ¨å§¿åŠ¿ä»¥åŠæ·±åº¦ä¿¡æ¯ã€‚é€šè¿‡è¿™äº›æ•°æ®ï¼Œå¯ä»¥ç›´æ¥å­¦ä¹ é€‚ç”¨äºæœºå™¨äººçš„3Dç‚¹äº‘ç­–ç•¥ï¼Œä»è€Œå…‹æœå½¢æ€å·®å¼‚å’Œç¯å¢ƒå˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAINAæ¡†æ¶ä¸»è¦åŒ…æ‹¬æ•°æ®é‡‡é›†å’Œç­–ç•¥å­¦ä¹ ä¸¤ä¸ªé˜¶æ®µã€‚åœ¨æ•°æ®é‡‡é›†é˜¶æ®µï¼Œä½¿ç”¨Aria Gen 2çœ¼é•œè®°å½•äººç±»æ‰§è¡Œä»»åŠ¡çš„è§†é¢‘ï¼Œå¹¶æå–å¤´éƒ¨å’Œæ‰‹éƒ¨å§¿åŠ¿ä»¥åŠæ·±åº¦ä¿¡æ¯ã€‚åœ¨ç­–ç•¥å­¦ä¹ é˜¶æ®µï¼Œåˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒä¸€ä¸ªåŸºäº3Dç‚¹äº‘çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿæ ¹æ®å½“å‰åœºæ™¯çš„çŠ¶æ€é¢„æµ‹æœºå™¨äººçš„åŠ¨ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šAINAæ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨æ™ºèƒ½çœ¼é•œè¿›è¡Œæ•°æ®é‡‡é›†ï¼Œè¿™ä½¿å¾—å¯ä»¥åœ¨çœŸå®åœºæ™¯ä¸­è½»æ¾è·å–é«˜è´¨é‡çš„äººç±»æ“ä½œæ•°æ®ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç›´æ¥å­¦ä¹ åŸºäº3Dç‚¹äº‘çš„ç­–ç•¥ï¼Œé¿å…äº†ä¸­é—´è¡¨ç¤ºçš„è½¬æ¢ï¼Œä»è€Œæé«˜äº†ç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šAINAæ¡†æ¶ä½¿ç”¨PointNet++æå–3Dç‚¹äº‘ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨Transformerç½‘ç»œå­¦ä¹ ç­–ç•¥ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åŠ¨ä½œé¢„æµ‹æŸå¤±å’Œå§¿åŠ¿é¢„æµ‹æŸå¤±ï¼Œç”¨äºçº¦æŸç­–ç•¥çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä¾‹å¦‚éšæœºæ—‹è½¬å’Œç¼©æ”¾ï¼Œä»¥æé«˜ç­–ç•¥çš„é²æ£’æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAINAæ¡†æ¶åœ¨ä¹ä¸ªæ—¥å¸¸æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚æŠ“å–ç‰©ä½“ã€æ”¾ç½®ç‰©ä½“å’Œç»„è£…ç‰©ä½“ã€‚ä¸å…ˆå‰çš„äººç±»åˆ°æœºå™¨äººç­–ç•¥å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒAINAæ¡†æ¶èƒ½å¤Ÿå­¦ä¹ æ›´é²æ£’å’Œæ³›åŒ–çš„ç­–ç•¥ï¼Œå¹¶ä¸”æ— éœ€ä»»ä½•æœºå™¨äººæ•°æ®å³å¯ç›´æ¥éƒ¨ç½²ã€‚å…·ä½“æ€§èƒ½æ•°æ®å¯åœ¨é¡¹ç›®ç½‘ç«™æŸ¥çœ‹ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦çµå·§æ“ä½œçš„æœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒåŒ»ç–—è¾…åŠ©ã€‚é€šè¿‡å­¦ä¹ äººç±»çš„æ¼”ç¤ºï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°é€‚åº”å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒï¼Œä»è€Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯æœ‰æœ›å®ç°æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿï¼Œä»è€Œæ›´å¥½åœ°æœåŠ¡äºäººç±»ç¤¾ä¼šã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.

