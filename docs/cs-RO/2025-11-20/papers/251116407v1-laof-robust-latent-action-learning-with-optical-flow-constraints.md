---
layout: default
title: LAOF: Robust Latent Action Learning with Optical Flow Constraints
---

# LAOF: Robust Latent Action Learning with Optical Flow Constraints

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.16407" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.16407v1</a>
  <a href="https://arxiv.org/pdf/2511.16407.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.16407v1" onclick="toggleFavorite(this, '2511.16407v1', 'LAOF: Robust Latent Action Learning with Optical Flow Constraints')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xizhou Bu, Jiexi Lyu, Fulei Sun, Ruichen Yang, Zhiqiang Ma, Wei Li

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-20

**å¤‡æ³¨**: Code can be found at https://github.com/XizoB/LAOF

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLAOFï¼šåˆ©ç”¨å…‰æµçº¦æŸå­¦ä¹ é²æ£’çš„æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºï¼Œæå‡å…·èº«æ™ºèƒ½é¢„è®­ç»ƒæ•ˆæœ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ (Perception & SLAM)**

**å…³é”®è¯**: `æ½œåœ¨åŠ¨ä½œå­¦ä¹ ` `å…‰æµçº¦æŸ` `å…·èº«æ™ºèƒ½` `è‡ªç›‘ç£å­¦ä¹ ` `è¡¨ç¤ºå­¦ä¹ ` `æ¨¡ä»¿å­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ½œåœ¨åŠ¨ä½œå­¦ä¹ æ–¹æ³•æ˜“å—è§†é¢‘ä¸­ä¸åŠ¨ä½œæ— å…³çš„å¹²æ‰°å› ç´ å½±å“ï¼Œé™åˆ¶äº†å…·èº«æ™ºèƒ½æ¨¡å‹çš„é¢„è®­ç»ƒæ•ˆæœã€‚
2. LAOFåˆ©ç”¨å…‰æµä½œä¸ºåŠ¨ä½œé©±åŠ¨çš„ä¼ªç›‘ç£ä¿¡å·ï¼Œçº¦æŸæ½œåœ¨åŠ¨ä½œå­¦ä¹ è¿‡ç¨‹ï¼Œä»è€ŒæŠ‘åˆ¶å¹²æ‰°å› ç´ å¹¶æå‡è¡¨ç¤ºçš„é²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒLAOFåœ¨æ ‡ç­¾ç¨€ç¼ºæ¡ä»¶ä¸‹æ˜¾è‘—æå‡äº†ä¸‹æ¸¸æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†å°‘é‡æ ‡ç­¾ç›‘ç£çš„æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä»å¤§è§„æ¨¡è§†é¢‘ä¸­å­¦ä¹ æ½œåœ¨åŠ¨ä½œå¯¹äºå¯æ‰©å±•çš„å…·èº«æ™ºèƒ½åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒè‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥åº”å¯¹ä¸åŠ¨ä½œæ— å…³çš„å¹²æ‰°å› ç´ ã€‚è™½ç„¶å¼•å…¥åŠ¨ä½œç›‘ç£å¯ä»¥ç¼“è§£è¿™äº›å¹²æ‰°ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—åˆ°å¯ç”¨åŠ¨ä½œæ ‡ç­¾ç¨€ç¼ºæ€§çš„é™åˆ¶ã€‚å…‰æµè¡¨ç¤ºè¿ç»­å¸§ä¹‹é—´çš„åƒç´ çº§è¿åŠ¨ï¼Œè‡ªç„¶åœ°æŠ‘åˆ¶èƒŒæ™¯å…ƒç´ å¹¶å¼ºè°ƒç§»åŠ¨å¯¹è±¡ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰å…‰æµçº¦æŸçš„é²æ£’æ½œåœ¨åŠ¨ä½œå­¦ä¹ æ–¹æ³•LAOFï¼Œè¿™æ˜¯ä¸€ä¸ªä¼ªç›‘ç£æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ™ºèƒ½ä½“çš„å…‰æµä½œä¸ºåŠ¨ä½œé©±åŠ¨çš„ä¿¡å·æ¥å­¦ä¹ å¯¹å¹²æ‰°å› ç´ å…·æœ‰é²æ£’æ€§çš„æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLAOFå­¦ä¹ çš„æ½œåœ¨è¡¨ç¤ºåœ¨ä¸‹æ¸¸æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™ç§å“è¶Šçš„æ€§èƒ½æºäºå…‰æµçº¦æŸï¼Œå®ƒå¤§å¤§ç¨³å®šäº†è®­ç»ƒï¼Œå¹¶åœ¨æåº¦æ ‡ç­¾ç¨€ç¼ºçš„æ¡ä»¶ä¸‹æé«˜äº†æ½œåœ¨è¡¨ç¤ºçš„è´¨é‡ï¼ŒåŒæ—¶åœ¨åŠ¨ä½œæ ‡ç­¾æ¯”ä¾‹å¢åŠ åˆ°10%æ—¶ä»ç„¶æœ‰æ•ˆã€‚é‡è¦çš„æ˜¯ï¼Œå³ä½¿æ²¡æœ‰åŠ¨ä½œç›‘ç£ï¼ŒLAOFä¹Ÿèƒ½è¾¾åˆ°æˆ–è¶…è¿‡ä½¿ç”¨1%åŠ¨ä½œæ ‡ç­¾è®­ç»ƒçš„åŠ¨ä½œç›‘ç£æ–¹æ³•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æ–¹æ³•åœ¨å­¦ä¹ æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºæ—¶ï¼Œå®¹æ˜“å—åˆ°è§†é¢‘ä¸­èƒŒæ™¯å¹²æ‰°ã€å…‰ç…§å˜åŒ–ç­‰ä¸åŠ¨ä½œæ— å…³å› ç´ çš„å½±å“ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„è¡¨ç¤ºæ³›åŒ–èƒ½åŠ›å·®ï¼Œéš¾ä»¥åº”ç”¨äºå¤æ‚çš„å…·èº«æ™ºèƒ½ä»»åŠ¡ã€‚å°¤å…¶æ˜¯åœ¨åŠ¨ä½œæ ‡ç­¾ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œç›‘ç£å­¦ä¹ çš„æ•ˆæœä¼šå¤§æ‰“æŠ˜æ‰£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å…‰æµæ¥å¼•å¯¼æ½œåœ¨åŠ¨ä½œçš„å­¦ä¹ ã€‚å…‰æµèƒ½å¤Ÿæ•æ‰è§†é¢‘ä¸­åƒç´ çº§åˆ«çš„è¿åŠ¨ä¿¡æ¯ï¼Œå¤©ç„¶åœ°æŠ‘åˆ¶é™æ€èƒŒæ™¯å’Œæ— å…³ç‰©ä½“ï¼Œçªå‡ºæ˜¾ç¤ºä¸æ™ºèƒ½ä½“åŠ¨ä½œç›¸å…³çš„è¿åŠ¨åŒºåŸŸã€‚é€šè¿‡å°†å…‰æµä¿¡æ¯èå…¥åˆ°æ½œåœ¨åŠ¨ä½œå­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜è¡¨ç¤ºçš„é²æ£’æ€§ï¼Œå‡å°‘å¹²æ‰°å› ç´ çš„å½±å“ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLAOFæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†é¢‘ç¼–ç å™¨ï¼šç”¨äºæå–è§†é¢‘å¸§çš„ç‰¹å¾è¡¨ç¤ºã€‚2) å…‰æµä¼°è®¡å™¨ï¼šç”¨äºè®¡ç®—è¿ç»­å¸§ä¹‹é—´çš„å…‰æµä¿¡æ¯ã€‚3) æ½œåœ¨åŠ¨ä½œå­¦ä¹ æ¨¡å—ï¼šè¯¥æ¨¡å—åˆ©ç”¨è§†é¢‘ç‰¹å¾å’Œå…‰æµä¿¡æ¯ï¼Œå­¦ä¹ æ½œåœ¨çš„åŠ¨ä½œè¡¨ç¤ºã€‚4) é‡æ„æ¨¡å—ï¼šç”¨äºä»æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºä¸­é‡æ„è§†é¢‘å¸§æˆ–å…‰æµä¿¡æ¯ï¼Œä»¥å®ç°è‡ªç›‘ç£å­¦ä¹ ã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡æœ€å°åŒ–é‡æ„è¯¯å·®å’Œå…‰æµçº¦æŸæŸå¤±æ¥ä¼˜åŒ–æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šLAOFçš„å…³é”®åˆ›æ–°åœ¨äºå°†å…‰æµä½œä¸ºä¸€ç§ä¼ªç›‘ç£ä¿¡å·ï¼Œç”¨äºçº¦æŸæ½œåœ¨åŠ¨ä½œçš„å­¦ä¹ è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„åŠ¨ä½œç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼ŒLAOFä¸éœ€è¦å¤§é‡çš„åŠ¨ä½œæ ‡ç­¾ï¼Œåªéœ€è¦è§†é¢‘æœ¬èº«çš„å…‰æµä¿¡æ¯å³å¯ã€‚æ­¤å¤–ï¼ŒLAOFè¿˜è®¾è®¡äº†ä¸€ç§æ–°çš„å…‰æµçº¦æŸæŸå¤±å‡½æ•°ï¼Œç”¨äºé¼“åŠ±å­¦ä¹ åˆ°çš„æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºä¸å…‰æµä¿¡æ¯ä¿æŒä¸€è‡´ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…·ä½“å®ç°ä¸Šï¼Œè®ºæ–‡é‡‡ç”¨äº†å¸¸ç”¨çš„å·ç§¯ç¥ç»ç½‘ç»œä½œä¸ºè§†é¢‘ç¼–ç å™¨ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„å…‰æµä¼°è®¡æ¨¡å‹æ¥è®¡ç®—å…‰æµä¿¡æ¯ã€‚æ½œåœ¨åŠ¨ä½œå­¦ä¹ æ¨¡å—å¯ä»¥é‡‡ç”¨å„ç§ä¸åŒçš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚å¾ªç¯ç¥ç»ç½‘ç»œæˆ–Transformerã€‚å…‰æµçº¦æŸæŸå¤±å‡½æ•°å¯ä»¥è®¾è®¡ä¸ºå…‰æµé‡æ„è¯¯å·®æˆ–å…‰æµä¸€è‡´æ€§æŸå¤±ã€‚è®ºæ–‡è¿˜æ¢ç´¢äº†ä¸åŒçš„æŸå¤±å‡½æ•°æƒé‡å’Œç½‘ç»œç»“æ„å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLAOFåœ¨ä¸‹æ¸¸æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨æ ‡ç­¾ç¨€ç¼ºæ¡ä»¶ä¸‹ï¼ŒLAOFç”šè‡³å¯ä»¥è¾¾åˆ°æˆ–è¶…è¿‡ä½¿ç”¨1%åŠ¨ä½œæ ‡ç­¾è®­ç»ƒçš„åŠ¨ä½œç›‘ç£æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªæ¨¡ä»¿å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒLAOFçš„æ€§èƒ½æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†15%ä»¥ä¸Šã€‚è¿™äº›ç»“æœéªŒè¯äº†å…‰æµçº¦æŸçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠLAOFåœ¨å­¦ä¹ é²æ£’æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºæ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LAOFå­¦ä¹ çš„é²æ£’æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºå¯å¹¿æ³›åº”ç”¨äºå…·èº«æ™ºèƒ½é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€æ“ä½œå’Œäº¤äº’ã€‚é€šè¿‡é¢„è®­ç»ƒï¼Œå¯ä»¥æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ„ŸçŸ¥å’Œå†³ç­–èƒ½åŠ›ï¼Œé™ä½å¯¹å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚è¯¥æ–¹æ³•è¿˜å¯åº”ç”¨äºè§†é¢‘ç†è§£ã€è¡Œä¸ºè¯†åˆ«ç­‰é¢†åŸŸï¼Œæé«˜æ¨¡å‹å¯¹è§†é¢‘å†…å®¹çš„ç†è§£å’Œåˆ†æèƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.

