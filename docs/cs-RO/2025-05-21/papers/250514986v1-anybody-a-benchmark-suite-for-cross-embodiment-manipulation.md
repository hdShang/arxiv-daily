---
layout: default
title: AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation
---

# AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14986" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14986v1</a>
  <a href="https://arxiv.org/pdf/2505.14986.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14986v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14986v1', 'AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Meenal Parakh, Alexandre Kirchmeyer, Beining Han, Jia Deng

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAnyBodyåŸºå‡†å¥—ä»¶ä»¥è§£å†³è·¨å½¢æ€æ“æ§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)**

**å…³é”®è¯**: `è·¨å½¢æ€æ“æ§` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººå­¦ä¹ ` `å½¢æ€æ„ŸçŸ¥` `åŸºå‡†æµ‹è¯•` `æ³›åŒ–èƒ½åŠ›` `å¤šæ ·æ€§è¯„ä¼°`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ“æ§ä»»åŠ¡ä¸­ç¼ºä¹ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åœ¨è·¨å½¢æ€æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜æ˜æ˜¾ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å¥—ä»¶ï¼Œä¸“æ³¨äºæŠ“å–å’Œæ¨åŠ¨ä»»åŠ¡ï¼Œæµ‹è¯•ä¸åŒå½¢æ€ä¸‹çš„æ§åˆ¶ç­–ç•¥æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå½¢æ€æ„ŸçŸ¥è®­ç»ƒåœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºå•ä¸€å½¢æ€åŸºçº¿ï¼Œä¸”é›¶-shot æ³›åŒ–åˆ°æœªè§å½¢æ€æ˜¯å¯è¡Œçš„ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœºå™¨äººé¢†åŸŸï¼Œå°†æ§åˆ¶ç­–ç•¥æ¨å¹¿åˆ°æ–°å½¢æ€ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ“æ§ä»»åŠ¡ä¸­ã€‚å°½ç®¡ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è¿åŠ¨æ–¹é¢ï¼Œä½†åœ¨æ“æ§ä»»åŠ¡çš„ç³»ç»Ÿæ€§ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œéƒ¨åˆ†åŸå› æ˜¯ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå­¦ä¹ è·¨å½¢æ€æ“æ§çš„åŸºå‡†ï¼Œé‡ç‚¹å…³æ³¨ä¸¤ä¸ªåŸºç¡€ä»»åŠ¡ï¼šåœ¨å¤šæ ·åŒ–å½¢æ€ä¸­è¿›è¡Œçš„æŠ“å–å’Œæ¨åŠ¨ã€‚è¯¥åŸºå‡†æ—¨åœ¨æ²¿ä¸‰ä¸ªç»´åº¦æµ‹è¯•æ³›åŒ–èƒ½åŠ›ï¼šæ’å€¼ã€å¤–æ¨å’Œç»„åˆã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸åŒå¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨å¤šç§å½¢æ€ä¸‹çš„å­¦ä¹ èƒ½åŠ›åŠå…¶å¯¹æ–°å½¢æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†å¤šå½¢æ€å­¦ä¹ çš„å½“å‰å±€é™æ€§ï¼Œå¹¶æä¾›äº†å…³äºæ¶æ„å’Œè®­ç»ƒè®¾è®¡é€‰æ‹©å¦‚ä½•å½±å“ç­–ç•¥æ³›åŒ–çš„è§è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è·¨å½¢æ€æ“æ§ä»»åŠ¡ä¸­æ§åˆ¶ç­–ç•¥çš„æ³›åŒ–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¸åŒå½¢æ€é—´çš„å­¦ä¹ èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´åœ¨æ–°å½¢æ€ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºä¸€ä¸ªæ ‡å‡†åŒ–çš„åŸºå‡†å¥—ä»¶ï¼Œä¸“æ³¨äºæŠ“å–å’Œæ¨åŠ¨ä»»åŠ¡ï¼Œé€šè¿‡æµ‹è¯•ä¸åŒå½¢æ€çš„æœºå™¨äººæ¥è¯„ä¼°å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šåŸºå‡†å¥—ä»¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æµ‹è¯•ç»´åº¦ï¼šæ’å€¼ï¼ˆåŒä¸€ç±»åˆ«å†…çš„å½¢æ€ï¼‰ã€å¤–æ¨ï¼ˆä¸åŒé“¾æ¥ç»“æ„çš„æœºå™¨äººï¼‰å’Œç»„åˆï¼ˆå¤šç§é“¾æ¥ç»“æ„çš„ç»„åˆï¼‰ã€‚æ¯ä¸ªç»´åº¦éƒ½è®¾è®¡äº†ç›¸åº”çš„æµ‹è¯•ä»»åŠ¡ï¼Œä»¥è¯„ä¼°ç­–ç•¥çš„å­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºç³»ç»Ÿæ€§åœ°å®šä¹‰äº†è·¨å½¢æ€æ“æ§çš„åŸºå‡†æµ‹è¯•ï¼Œå¡«è¡¥äº†ä»¥å¾€ç ”ç©¶çš„ç©ºç™½ï¼Œå¹¶æä¾›äº†å¤šç»´åº¦çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†å¤šç§å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œå¹¶é€šè¿‡å½¢æ€æ„ŸçŸ¥è®­ç»ƒæ¥ä¼˜åŒ–ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„çš„è®¾è®¡ä¹Ÿç»è¿‡ç²¾å¿ƒè°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒå½¢æ€çš„ç‰¹ç‚¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½¢æ€æ„ŸçŸ¥è®­ç»ƒçš„ç­–ç•¥åœ¨å¤šå½¢æ€ç¯å¢ƒä¸­çš„è¡¨ç°æ˜æ˜¾ä¼˜äºå•ä¸€å½¢æ€åŸºçº¿ï¼Œå°¤å…¶åœ¨é›¶-shot æ³›åŒ–ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸç‡æé«˜äº†çº¦30%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å¤šå½¢æ€å­¦ä¹ çš„é‡è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œäººæœºåä½œç­‰ã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨ä¸åŒå½¢æ€ä¸‹çš„æ“æ§èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œçµæ´»æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Generalizing control policies to novel embodiments remains a fundamental challenge in enabling scalable and transferable learning in robotics. While prior works have explored this in locomotion, a systematic study in the context of manipulation tasks remains limited, partly due to the lack of standardized benchmarks. In this paper, we introduce a benchmark for learning cross-embodiment manipulation, focusing on two foundational tasks-reach and push-across a diverse range of morphologies. The benchmark is designed to test generalization along three axes: interpolation (testing performance within a robot category that shares the same link structure), extrapolation (testing on a robot with a different link structure), and composition (testing on combinations of link structures). On the benchmark, we evaluate the ability of different RL policies to learn from multiple morphologies and to generalize to novel ones. Our study aims to answer whether morphology-aware training can outperform single-embodiment baselines, whether zero-shot generalization to unseen morphologies is feasible, and how consistently these patterns hold across different generalization regimes. The results highlight the current limitations of multi-embodiment learning and provide insights into how architectural and training design choices influence policy generalization.

