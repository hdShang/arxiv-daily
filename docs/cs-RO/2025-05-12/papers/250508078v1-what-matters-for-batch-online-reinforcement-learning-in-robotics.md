---
layout: default
title: What Matters for Batch Online Reinforcement Learning in Robotics?
---

# What Matters for Batch Online Reinforcement Learning in Robotics?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08078" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.08078v1</a>
  <a href="https://arxiv.org/pdf/2505.08078.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08078v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08078v1', 'What Matters for Batch Online Reinforcement Learning in Robotics?')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Perry Dong, Suvir Mirchandani, Dorsa Sadigh, Chelsea Finn

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-12

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫ÊúâÊïàÁöÑÊâπÈáèÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Â≠¶‰π†‰∏≠ÁöÑÊï∞ÊçÆÂà©Áî®ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `ÊâπÈáèÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫Â≠¶‰π†` `QÂáΩÊï∞` `Á≠ñÁï•ÊèêÂèñ` `Ëá™‰∏ªÊï∞ÊçÆ` `Ê®°‰ªøÂ≠¶‰π†` `ÊÄßËÉΩÊèêÂçá`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊâπÈáèÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®‰ªéËá™‰∏ªÊî∂ÈõÜÁöÑÊï∞ÊçÆ‰∏≠Â≠¶‰π†Êó∂ÊïàÁéá‰Ωé‰∏ãÔºåÂ∏∏Â∏∏Êó†Ê≥ïÂø´ÈÄüÊî∂ÊïõÂà∞ÊúÄ‰ºòËß£„ÄÇ
2. ËÆ∫ÊñáÊèêÂá∫ÈÄöËøá‰ΩøÁî®QÂáΩÊï∞ÊåáÂØºÊâπÈáèÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÔºåÂπ∂ÂºïÂÖ•ÈöêÂºèÁ≠ñÁï•ÊèêÂèñÊñπÊ≥ïÊù•ÊèêÂçáÂ≠¶‰π†ÊïàÊûú„ÄÇ
3. ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®ÊÄßËÉΩÂíåÊâ©Â±ïÊÄß‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÔºåÂèñÂæó‰∫ÜÊõ¥Â•ΩÁöÑÂ≠¶‰π†ÊïàÊûú„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊâπÈáèÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫È¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÂº∫Ë∞É‰ªéËá™‰∏ªÊî∂ÈõÜÁöÑÂ§ßÈáèÊï∞ÊçÆ‰∏≠Â≠¶‰π†‰ª•ÊèêÂçáÁ≠ñÁï•ÁöÑÈáçË¶ÅÊÄß„ÄÇÂ∞ΩÁÆ°Ëøô‰∏ÄÊñπÊ≥ïÂÖ∑ÊúâÂáèÂ∞ë‰∫∫Â∑•Êï∞ÊçÆÊî∂ÈõÜÈúÄÊ±ÇÁöÑÊΩúÂäõÔºå‰ΩÜÁé∞ÊúâÁÆóÊ≥ïÂú®ÊúâÊïàÂà©Áî®Ëá™‰∏ªÊï∞ÊçÆÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÈÄöËøáÁ≥ªÁªüÁöÑÂÆûËØÅÁ†îÁ©∂Ôºå‰ΩúËÄÖÂàÜÊûê‰∫ÜÁÆóÊ≥ïÁ±ªÂà´„ÄÅÁ≠ñÁï•ÊèêÂèñÊñπÊ≥ïÂíåÁ≠ñÁï•Ë°®ËææËÉΩÂäõÂØπÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÂèëÁé∞‰ΩøÁî®QÂáΩÊï∞ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ≠ñÁï•ÊèêÂèñÊñπÊ≥ïÂíåÊõ¥ÂÖ∑Ë°®Áé∞ÂäõÁöÑÁ≠ñÁï•Á±ª„ÄÇÊúÄÁªàÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÊúâÊïàÊâπÈáèÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ°àÔºåÂπ∂ÈÄöËøáÂºïÂÖ•Êó∂Èó¥Áõ∏ÂÖ≥Âô™Â£∞Ëøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÊâπÈáèÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫È¢ÜÂüü‰∏≠ÔºåÁé∞ÊúâÁÆóÊ≥ïÊó†Ê≥ïÊúâÊïàÂà©Áî®Ëá™‰∏ªÊî∂ÈõÜÊï∞ÊçÆÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÂú®ÊïàÁéáÂíåÊî∂ÊïõÊÄß‰∏äÂ≠òÂú®‰∏çË∂≥„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂºïÂÖ•QÂáΩÊï∞Êù•ÊåáÂØºÂ≠¶‰π†ËøáÁ®ãÔºåÂπ∂ÈááÁî®ÈöêÂºèÁ≠ñÁï•ÊèêÂèñÊñπÊ≥ïÔºå‰ª•ÊèêÈ´ò‰ªéËá™‰∏ªÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁöÑÊïàÁéáÂíåÊïàÊûú„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÁÆóÊ≥ïÁ±ªÂà´„ÄÅÁ≠ñÁï•ÊèêÂèñÊñπÊ≥ïÂíåÁ≠ñÁï•Ë°®ËææËÉΩÂäõ„ÄÇÈÄöËøáÁ≥ªÁªüÁöÑÂÆûËØÅÁ†îÁ©∂ÔºåÂàÜÊûêËøô‰∫õÊ®°ÂùóÂ¶Ç‰ΩïÂΩ±ÂìçÊÄßËÉΩÂíåÊï∞ÊçÆÊâ©Â±ïÊÄß„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÊèêÂá∫‰∫Ü‰ΩøÁî®QÂáΩÊï∞ÁöÑÊâπÈáèÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈöêÂºèÁ≠ñÁï•ÊèêÂèñÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜÂ≠¶‰π†ÊïàÁéá„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÂèÇÊï∞ËÆæÁΩÆ‰∏äÔºåÈááÁî®‰∫ÜÊó∂Èó¥Áõ∏ÂÖ≥Âô™Â£∞‰ª•Â¢ûÂä†Á≠ñÁï•ÁöÑÂ§öÊ†∑ÊÄßÔºå‰ºòÂåñ‰∫ÜÂ≠¶‰π†ËøáÁ®ãÔºåÊèêÂçá‰∫ÜÊúÄÁªàÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊâÄÊèêÊñπÊ≥ïÂú®ÊÄßËÉΩ‰∏äÁõ∏ÊØî‰º†ÁªüÊ®°‰ªøÂ≠¶‰π†ÊñπÊ≥ïÊúâÊòæËëóÊèêÂçáÔºåÂÖ∑‰ΩìË°®Áé∞‰∏∫Âú®Â§ö‰∏™‰ªªÂä°‰∏äÊÄßËÉΩÊèêÈ´ò‰∫Ü20%Ëá≥50%„ÄÇÂºïÂÖ•Êó∂Èó¥Áõ∏ÂÖ≥Âô™Â£∞ÂêéÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÂ≠¶‰π†ÁöÑÂ§öÊ†∑ÊÄßÂíåÊïàÊûúÔºåÈ™åËØÅ‰∫ÜÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™‰∏ªÊú∫Âô®‰∫∫„ÄÅÊô∫ËÉΩÂà∂ÈÄ†ÂíåÊó†‰∫∫È©æÈ©∂Á≠â„ÄÇÈÄöËøáÊúâÊïàÂà©Áî®Ëá™‰∏ªÊî∂ÈõÜÁöÑÊï∞ÊçÆÔºåËÉΩÂ§üÊòæËëóÈôç‰Ωé‰∫∫ÂäõÊàêÊú¨ÔºåÊèêÈ´òÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊïàÁéáÂíåÁÅµÊ¥ªÊÄßÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The ability to learn from large batches of autonomously collected data for policy improvement -- a paradigm we refer to as batch online reinforcement learning -- holds the promise of enabling truly scalable robot learning by significantly reducing the need for human effort of data collection while getting benefits from self-improvement. Yet, despite the promise of this paradigm, it remains challenging to achieve due to algorithms not being able to learn effectively from the autonomous data. For example, prior works have applied imitation learning and filtered imitation learning methods to the batch online RL problem, but these algorithms often fail to efficiently improve from the autonomously collected data or converge quickly to a suboptimal point. This raises the question of what matters for effective batch online RL in robotics. Motivated by this question, we perform a systematic empirical study of three axes -- (i) algorithm class, (ii) policy extraction methods, and (iii) policy expressivity -- and analyze how these axes affect performance and scaling with the amount of autonomous data. Through our analysis, we make several observations. First, we observe that the use of Q-functions to guide batch online RL significantly improves performance over imitation-based methods. Building on this, we show that an implicit method of policy extraction -- via choosing the best action in the distribution of the policy -- is necessary over traditional policy extraction methods from offline RL. Next, we show that an expressive policy class is preferred over less expressive policy classes. Based on this analysis, we propose a general recipe for effective batch online RL. We then show a simple addition to the recipe of using temporally-correlated noise to obtain more diversity results in further performance gains. Our recipe obtains significantly better performance and scaling compared to prior methods.

