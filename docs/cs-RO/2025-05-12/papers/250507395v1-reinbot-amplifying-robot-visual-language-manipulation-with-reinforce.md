---
layout: default
title: ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning
---

# ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07395" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07395v1</a>
  <a href="https://arxiv.org/pdf/2505.07395.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07395v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07395v1', 'ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºReinboTä»¥æå‡æœºå™¨äººè§†è§‰è¯­è¨€æ“ä½œçš„å†³ç­–èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººå†³ç­–` `å¯†é›†å›æŠ¥é¢„æµ‹` `å°‘æ ·æœ¬å­¦ä¹ ` `åˆ†å¸ƒå¤–æ³›åŒ–` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®è´¨é‡ä¸å‡çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½å—åˆ°æ˜¾è‘—é™åˆ¶ï¼Œå½±å“äº†æœºå™¨äººå†³ç­–çš„æœ‰æ•ˆæ€§ã€‚
2. ReinboTé€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ çš„ç´¯ç§¯å¥–åŠ±æœ€å¤§åŒ–åŸåˆ™ï¼Œæå‡äº†å¯¹æ•°æ®è´¨é‡çš„ç†è§£ï¼Œè¿›è€Œä¼˜åŒ–äº†å†³ç­–è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReinboTåœ¨CALVINæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å°‘æ ·æœ¬å­¦ä¹ å’Œåˆ†å¸ƒå¤–æ³›åŒ–æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æ¨¡ä»¿å­¦ä¹ ä¸­å±•ç°äº†åœ¨æœºå™¨äººå†³ç­–ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè®­ç»ƒæ•°æ®çš„è´¨é‡æ³¢åŠ¨å¸¸å¸¸é™åˆ¶äº†è¿™äº›æ¨¡å‹çš„æ€§èƒ½ã€‚å¦ä¸€æ–¹é¢ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä»æ··åˆè´¨é‡æ•°æ®ä¸­å­¦ä¹ ç¨³å¥çš„ç­–ç•¥æ¨¡å‹æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯VLAæ¨¡å‹ReinboTï¼Œç»“åˆäº†æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„RLåŸåˆ™ã€‚ReinboTé€šè¿‡é¢„æµ‹å¯†é›†å›æŠ¥æ¥æ·±å…¥ç†è§£æ•°æ®è´¨é‡åˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆæ›´ç¨³å¥çš„å†³ç­–è¡ŒåŠ¨ï¼Œæ—¨åœ¨æœ€å¤§åŒ–æœªæ¥æ”¶ç›Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReinboTåœ¨CALVINæ··åˆè´¨é‡æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šçš„å°‘æ ·æœ¬å­¦ä¹ å’Œåˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®è´¨é‡ä¸å‡æ—¶çš„æ€§èƒ½ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººå†³ç­–ä»»åŠ¡ä¸­çš„åº”ç”¨é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šReinboTçš„æ ¸å¿ƒæ€è·¯æ˜¯ç»“åˆå¼ºåŒ–å­¦ä¹ çš„æ€æƒ³ï¼Œé€šè¿‡é¢„æµ‹å¯†é›†å›æŠ¥æ¥å¢å¼ºæ¨¡å‹å¯¹æ•°æ®è´¨é‡çš„ç†è§£ï¼Œä»è€Œç”Ÿæˆæ›´ä¸ºç¨³å¥çš„å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šReinboTçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å¯†é›†å›æŠ¥é¢„æµ‹æ¨¡å—å’Œå†³ç­–ç”Ÿæˆæ¨¡å—ã€‚é¦–å…ˆå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤„ç†ï¼Œç„¶åé€šè¿‡å¯†é›†å›æŠ¥é¢„æµ‹æ¥è¯„ä¼°æ•°æ®è´¨é‡ï¼Œæœ€åç”Ÿæˆå†³ç­–è¡ŒåŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šReinboTçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¯†é›†å›æŠ¥é¢„æµ‹èƒ½åŠ›ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ“ä½œä»»åŠ¡çš„ç»†å¾®å·®åˆ«ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†å†³ç­–çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒReinboTé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å¯†é›†å›æŠ¥é¢„æµ‹ï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†å¤šå±‚æ¬¡çš„ç‰¹å¾æå–æ¨¡å—ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹å¤æ‚ä»»åŠ¡çš„é€‚åº”æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒReinboTåœ¨CALVINæ··åˆè´¨é‡æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæå‡å¹…åº¦è¶…è¿‡15%ã€‚æ­¤å¤–ï¼ŒReinboTåœ¨å°‘æ ·æœ¬å­¦ä¹ å’Œåˆ†å¸ƒå¤–æ³›åŒ–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¶å±…ã€å·¥ä¸šè‡ªåŠ¨åŒ–å’ŒæœåŠ¡æœºå™¨äººç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ï¼ŒReinboTèƒ½å¤Ÿåœ¨å®é™…æ“ä½œä¸­å®ç°æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks.

