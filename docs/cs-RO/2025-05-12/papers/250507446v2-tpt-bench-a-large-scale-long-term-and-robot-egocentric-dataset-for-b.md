---
layout: default
title: TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for Benchmarking Target Person Tracking
---

# TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for Benchmarking Target Person Tracking

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07446" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07446v2</a>
  <a href="https://arxiv.org/pdf/2505.07446.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07446v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07446v2', 'TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for Benchmarking Target Person Tracking')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hanjing Ye, Yu Zhan, Weixi Situ, Guangcheng Chen, Jingwen Yu, Ziqi Zhao, Kuanqi Cai, Arash Ajoudani, Hong Zhang

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-07-09)

**å¤‡æ³¨**: Under review. web: https://medlartea.github.io/tpt-bench/

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTPT-Benchæ•°æ®é›†ä»¥è§£å†³æœºå™¨äººè§†è§’ä¸‹ç›®æ ‡äººç‰©è·Ÿè¸ªé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç›®æ ‡äººç‰©è·Ÿè¸ª` `æœºå™¨äººè§†è§’` `å¤šæ¨¡æ€æ•°æ®` `äººæœºäº¤äº’` `é•¿æ—¶é—´è·Ÿè¸ª` `æ•°æ®é›†æ„å»º` `å¤æ‚ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç›®æ ‡äººç‰©è·Ÿè¸ªæ–¹æ³•å¤šåœ¨å—æ§ç¯å¢ƒä¸­è¿›è¡Œï¼Œç¼ºä¹å¯¹å¤æ‚åœºæ™¯çš„é€‚åº”æ€§ï¼Œé¢ä¸´é¢‘ç¹é®æŒ¡å’Œé‡æ–°è¯†åˆ«çš„æŒ‘æˆ˜ã€‚
2. æœ¬æ–‡æå‡ºäº†TPT-Benchæ•°æ®é›†ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»è·Ÿéšè¡Œä¸ºï¼Œæ•æ‰æ‹¥æŒ¤ç¯å¢ƒä¸­çš„é•¿æœŸè·Ÿè¸ªé—®é¢˜ï¼Œæä¾›å¤šæ¨¡æ€æ•°æ®æ”¯æŒã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰SOTAæ–¹æ³•åœ¨è¯¥æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜æ˜¾çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘å’Œæ€è·¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨æœºå™¨äººè§†è§’ä¸‹è·Ÿè¸ªç›®æ ‡äººç‰©å¯¹äºå¼€å‘èƒ½å¤Ÿæä¾›æŒç»­ä¸ªæ€§åŒ–ååŠ©æˆ–åˆä½œçš„è‡ªä¸»æœºå™¨äººè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›®æ ‡äººç‰©è·Ÿè¸ªåŸºå‡†å¤§å¤šå±€é™äºå—æ§å®éªŒå®¤ç¯å¢ƒï¼Œå­˜åœ¨èƒŒæ™¯å¹²æ‰°å°‘ã€çŸ­æœŸé®æŒ¡ç­‰é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ—¨åœ¨åº”å¯¹æ‹¥æŒ¤å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­çš„ç›®æ ‡äººç‰©è·Ÿè¸ªæŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†é€šè¿‡äººæ¨ç€é…å¤‡ä¼ æ„Ÿå™¨çš„æ‰‹æ¨è½¦è·Ÿéšç›®æ ‡äººç‰©æ”¶é›†ï¼Œæ•æ‰äººç±»å¼çš„è·Ÿéšè¡Œä¸ºï¼Œå¼ºè°ƒé•¿æœŸè·Ÿè¸ªä¸­çš„é¢‘ç¹é®æŒ¡å’Œé‡æ–°è¯†åˆ«çš„éœ€æ±‚ã€‚æ•°æ®é›†åŒ…å«å¤šæ¨¡æ€æ•°æ®æµï¼ŒåŒ…æ‹¬é‡Œç¨‹è®¡ã€3D LiDARã€IMUã€å…¨æ™¯å›¾åƒå’ŒRGB-Då›¾åƒï¼Œå¹¶å¯¹48ä¸ªåºåˆ—ä¸­çš„ç›®æ ‡äººç‰©è¿›è¡Œäº†è¯¦å°½çš„2Dè¾¹ç•Œæ¡†æ ‡æ³¨ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†å’Œè§†è§‰æ ‡æ³¨ï¼Œæœ¬æ–‡å¯¹ç°æœ‰çš„SOTA TPTæ–¹æ³•è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œæ·±å…¥åˆ†æå…¶å±€é™æ€§å¹¶æå‡ºæœªæ¥ç ”ç©¶æ–¹å‘ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººè§†è§’ä¸‹çš„ç›®æ ‡äººç‰©è·Ÿè¸ªé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶åœ¨é¢‘ç¹é®æŒ¡å’Œé‡æ–°è¯†åˆ«æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€é•¿æœŸçš„æœºå™¨äººè§†è§’æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿäººç±»è·Ÿéšè¡Œä¸ºï¼Œå¼ºè°ƒåœ¨æ‹¥æŒ¤å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡è·Ÿè¸ªçš„æŒ‘æˆ˜ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•°æ®é›†åŒ…å«å¤šç§ä¼ æ„Ÿå™¨æ•°æ®ï¼Œå¦‚é‡Œç¨‹è®¡ã€3D LiDARã€IMUã€å…¨æ™¯å›¾åƒå’ŒRGB-Då›¾åƒï¼Œæ•´ä½“æµç¨‹åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ ‡æ³¨å’ŒåŸºäºç°æœ‰æ–¹æ³•çš„å®éªŒè¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šTPT-Benchæ•°æ®é›†çš„æ„å»ºæ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œæä¾›äº†ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ”¯æŒåœ¨å¤æ‚åœºæ™¯ä¸‹çš„ç›®æ ‡äººç‰©è·Ÿè¸ªç ”ç©¶ï¼ŒåŒºåˆ«äºä»¥å¾€ä»…åœ¨å—æ§ç¯å¢ƒä¸‹è¿›è¡Œçš„åŸºå‡†æµ‹è¯•ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†ä¸­çš„å‚æ•°è®¾ç½®åŒ…æ‹¬å¤šç§ä¼ æ„Ÿå™¨çš„åŒæ­¥é‡‡é›†ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†é®æŒ¡å’Œé‡æ–°è¯†åˆ«çš„æŒ‘æˆ˜ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„SOTAæ–¹æ³•è¿›è¡Œæ”¹è¿›å’Œè¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰SOTAç›®æ ‡äººç‰©è·Ÿè¸ªæ–¹æ³•åœ¨TPT-Benchæ•°æ®é›†ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå°¤å…¶åœ¨å¤„ç†é¢‘ç¹é®æŒ¡å’Œå¤æ‚èƒŒæ™¯æ—¶ï¼Œæ€§èƒ½ä¸‹é™å¹…åº¦å¯è¾¾20%ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººæœºäº¤äº’ã€æ™ºèƒ½ç›‘æ§å’Œè‡ªä¸»å¯¼èˆªç­‰ã€‚é€šè¿‡æä¾›ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„ç›®æ ‡äººç‰©è·Ÿè¸ªåŸºå‡†ï¼Œèƒ½å¤Ÿæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å‘å±•ï¼Œæå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›å’Œå®ç”¨æ€§ï¼Œæœªæ¥å¯èƒ½åœ¨æœåŠ¡æœºå™¨äººå’Œç¤¾äº¤æœºå™¨äººç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Tracking a target person from robot-egocentric views is crucial for developing autonomous robots that provide continuous personalized assistance or collaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most existing target person tracking (TPT) benchmarks are limited to controlled laboratory environments with few distractions, clean backgrounds, and short-term occlusions. In this paper, we introduce a large-scale dataset designed for TPT in crowded and unstructured environments, demonstrated through a robot-person following task. The dataset is collected by a human pushing a sensor-equipped cart while following a target person, capturing human-like following behavior and emphasizing long-term tracking challenges, including frequent occlusions and the need for re-identification from numerous pedestrians. It includes multi-modal data streams, including odometry, 3D LiDAR, IMU, panoramic images, and RGB-D images, along with exhaustively annotated 2D bounding boxes of the target person across 48 sequences, both indoors and outdoors. Using this dataset and visual annotations, we perform extensive experiments with existing SOTA TPT methods, offering a thorough analysis of their limitations and suggesting future research directions.

