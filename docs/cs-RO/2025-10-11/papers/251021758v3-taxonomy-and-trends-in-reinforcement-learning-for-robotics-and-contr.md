---
layout: default
title: "Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review"
---

# Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.21758" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.21758v3</a>
  <a href="https://arxiv.org/pdf/2510.21758.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21758v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.21758v3', 'Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe

**åˆ†ç±»**: cs.RO, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-10-11 (æ›´æ–°: 2025-10-29)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**ç»¼è¿°å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººä¸æ§åˆ¶ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼šåˆ†ç±»ã€è¶‹åŠ¿ä¸ç»“æ„åŒ–å›é¡¾**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è‡ªä¸»å­¦ä¹ ` `æ§åˆ¶ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æœºå™¨äººæ§åˆ¶æ–¹æ³•éš¾ä»¥é€‚åº”åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒï¼Œéœ€è¦æ›´æ™ºèƒ½çš„è‡ªä¸»å­¦ä¹ ç­–ç•¥ã€‚
2. æœ¬æ–‡å¯¹å¼ºåŒ–å­¦ä¹ ç†è®ºä¸ç®—æ³•è¿›è¡Œç»¼è¿°ï¼Œå¹¶åˆ†æå…¶åœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨å¼¥åˆç†è®ºä¸å®è·µçš„å·®è·ã€‚
3. é‡ç‚¹å…³æ³¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚DDPGã€TD3ã€PPOã€SACç­‰ï¼Œå¹¶å¯¹åº”ç”¨åœºæ™¯è¿›è¡Œåˆ†ç±»ï¼Œæ€»ç»“æŠ€æœ¯è¶‹åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ (RL)å·²æˆä¸ºåœ¨åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­å®ç°æ™ºèƒ½æœºå™¨äººè¡Œä¸ºçš„åŸºç¡€æ–¹æ³•ã€‚æœ¬æ–‡æ·±å…¥å›é¡¾äº†å¼ºåŒ–å­¦ä¹ çš„åŸç†ã€å…ˆè¿›çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)ç®—æ³•åŠå…¶åœ¨æœºå™¨äººå’Œæ§åˆ¶ç³»ç»Ÿä¸­çš„é›†æˆã€‚ç ”ç©¶ä»é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)çš„å½¢å¼åŒ–å®šä¹‰å¼€å§‹ï¼Œæ¦‚è¿°äº†æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’çš„åŸºæœ¬è¦ç´ ï¼Œå¹¶æ¢è®¨äº†åŒ…æ‹¬Actor-Criticæ–¹æ³•ã€åŸºäºä»·å€¼çš„å­¦ä¹ å’Œç­–ç•¥æ¢¯åº¦ç­‰æ ¸å¿ƒç®—æ³•ç­–ç•¥ã€‚é‡ç‚¹ä»‹ç»äº†ç°ä»£DRLæŠ€æœ¯ï¼Œå¦‚DDPGã€TD3ã€PPOå’ŒSACï¼Œè¿™äº›æŠ€æœ¯åœ¨è§£å†³é«˜ç»´ã€è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚å¼•å…¥äº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼Œç”¨äºå¯¹RLåœ¨è¿åŠ¨ã€æ“ä½œã€å¤šæ™ºèƒ½ä½“åè°ƒå’Œäººæœºäº¤äº’ç­‰é¢†åŸŸçš„åº”ç”¨è¿›è¡Œåˆ†ç±»ï¼Œä»¥åŠè®­ç»ƒæ–¹æ³•å’Œéƒ¨ç½²å‡†å¤‡ç¨‹åº¦ã€‚è¯¥ç»¼è¿°æ€»ç»“äº†æœ€è¿‘çš„ç ”ç©¶æˆæœï¼Œå¼ºè°ƒäº†æŠ€æœ¯è¶‹åŠ¿ã€è®¾è®¡æ¨¡å¼ä»¥åŠRLåœ¨ç°å®ä¸–ç•Œæœºå™¨äººæŠ€æœ¯ä¸­æ—¥ç›Šæˆç†Ÿã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨å°†ç†è®ºè¿›æ­¥ä¸å®é™…åº”ç”¨è”ç³»èµ·æ¥ï¼Œä¸ºRLåœ¨è‡ªä¸»æœºå™¨äººç³»ç»Ÿä¸­ä¸æ–­å‘å±•çš„ä½œç”¨æä¾›ä¸€ä¸ªç»¼åˆçš„è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰æœºå™¨äººæ§åˆ¶æ–¹æ³•åœ¨é¢å¯¹åŠ¨æ€ã€å¤æ‚å’Œä¸ç¡®å®šç¯å¢ƒæ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œéœ€è¦äººå·¥è®¾è®¡å¤æ‚çš„æ§åˆ¶ç­–ç•¥ã€‚å¼ºåŒ–å­¦ä¹ æ—¨åœ¨é€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œè‡ªä¸»å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œä»è€Œè§£å†³è¿™ä¸€é—®é¢˜ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åº”ç”¨äºé«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡æ—¶ï¼Œé¢ä¸´ç€æ ·æœ¬æ•ˆç‡ä½ã€è®­ç»ƒä¸ç¨³å®šç­‰æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å¯¹å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œç³»ç»Ÿæ€§çš„æ¢³ç†å’Œåˆ†ç±»ï¼Œä»ç†è®ºåŸºç¡€åˆ°å…·ä½“ç®—æ³•ï¼Œå†åˆ°åº”ç”¨åœºæ™¯ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„çŸ¥è¯†æ¡†æ¶ã€‚é€šè¿‡åˆ†æç°æœ‰æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œæ€»ç»“æŠ€æœ¯è¶‹åŠ¿å’Œè®¾è®¡æ¨¡å¼ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›å‚è€ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæœ¬æ–‡é¦–å…ˆå›é¡¾äº†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)çš„åŸºæœ¬æ¦‚å¿µï¼Œç„¶åä»‹ç»äº†å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç®—æ³•ï¼ŒåŒ…æ‹¬åŸºäºä»·å€¼çš„å­¦ä¹ ã€ç­–ç•¥æ¢¯åº¦æ–¹æ³•å’ŒActor-Criticæ–¹æ³•ã€‚æ¥ç€ï¼Œé‡ç‚¹ä»‹ç»äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¦‚DDPGã€TD3ã€PPOå’ŒSACã€‚æœ€åï¼Œå¯¹å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†åˆ†ç±»ï¼ŒåŒ…æ‹¬è¿åŠ¨ã€æ“ä½œã€å¤šæ™ºèƒ½ä½“åè°ƒå’Œäººæœºäº¤äº’ç­‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°ä¹‹å¤„åœ¨äºæ„å»ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼Œç”¨äºå¯¹å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œåˆ†ç±»ã€‚è¯¥åˆ†ç±»æ³•è€ƒè™‘äº†åº”ç”¨åœºæ™¯ã€è®­ç»ƒæ–¹æ³•å’Œéƒ¨ç½²å‡†å¤‡ç¨‹åº¦ç­‰å› ç´ ï¼Œä»è€Œèƒ½å¤Ÿæ›´å…¨é¢åœ°äº†è§£å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸçš„å‘å±•ç°çŠ¶å’Œæœªæ¥è¶‹åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šæœ¬æ–‡å¯¹å„ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å…³é”®è®¾è®¡è¿›è¡Œäº†æ€»ç»“ï¼Œä¾‹å¦‚ï¼ŒDDPGä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ŒTD3é€šè¿‡å¼•å…¥åŒé‡è¯„è®ºå®¶ç½‘ç»œæ¥å‡å°‘ä»·å€¼é«˜ä¼°ï¼ŒPPOä½¿ç”¨ä¿¡ä»»åŒºåŸŸä¼˜åŒ–æ¥ä¿è¯ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§ï¼ŒSACå¼•å…¥äº†ç†µæ­£åˆ™åŒ–æ¥é¼“åŠ±æ¢ç´¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†å„ç§è®­ç»ƒæŠ€å·§ï¼Œå¦‚ç»éªŒå›æ”¾ã€ç›®æ ‡ç½‘ç»œå’Œæ‰¹é‡å½’ä¸€åŒ–ç­‰ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥ç»¼è¿°æ€»ç»“äº†è¿‘å¹´æ¥æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨äº†DDPGã€TD3ã€PPOå’ŒSACç­‰ç®—æ³•åœ¨è§£å†³é«˜ç»´è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚é€šè¿‡å¯¹ä¸åŒåº”ç”¨åœºæ™¯çš„åˆ†ç±»å’Œåˆ†æï¼Œæ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸçš„æŠ€æœ¯è¶‹åŠ¿å’Œè®¾è®¡æ¨¡å¼ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººæ§åˆ¶ä»»åŠ¡ï¼Œä¾‹å¦‚è‡ªä¸»å¯¼èˆªã€ç‰©ä½“æŠ“å–ã€è£…é…ã€äººæœºåä½œç­‰ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæœºå™¨äººèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ é€‚åº”å¤æ‚ç¯å¢ƒï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å°†æ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—å¥åº·ã€æ™ºèƒ½å®¶å±…ç­‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has become a foundational approach for enabling intelligent robotic behavior in dynamic and uncertain environments. This work presents an in-depth review of RL principles, advanced deep reinforcement learning (DRL) algorithms, and their integration into robotic and control systems. Beginning with the formalism of Markov Decision Processes (MDPs), the study outlines essential elements of the agent-environment interaction and explores core algorithmic strategies including actor-critic methods, value-based learning, and policy gradients. Emphasis is placed on modern DRL techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving high-dimensional, continuous control tasks. A structured taxonomy is introduced to categorize RL applications across domains such as locomotion, manipulation, multi-agent coordination, and human-robot interaction, along with training methodologies and deployment readiness levels. The review synthesizes recent research efforts, highlighting technical trends, design patterns, and the growing maturity of RL in real-world robotics. Overall, this work aims to bridge theoretical advances with practical implementations, providing a consolidated perspective on the evolving role of RL in autonomous robotic systems.

