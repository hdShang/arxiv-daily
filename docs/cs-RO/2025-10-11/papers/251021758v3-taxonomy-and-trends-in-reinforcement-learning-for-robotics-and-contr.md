---
layout: default
title: Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review
---

# Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review

<div class="paper-toolbar">
  <div class="toolbar-left">
    <a href="https://arxiv.org/abs/2510.21758" target="_blank" class="toolbar-btn">arXiv: 2510.21758v3</a>
    <a href="https://arxiv.org/pdf/2510.21758.pdf" target="_blank" class="toolbar-btn">PDF</a>
  </div>
  <div class="toolbar-right">
    <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.21758v3" 
            onclick="toggleFavorite(this, '2510.21758v3', 'Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review')" title="Êî∂Ëóè">
      ‚òÜ Êî∂Ëóè
    </button>
    <button class="toolbar-btn share-btn" onclick="copyLink()" title="Â§çÂà∂ÈìæÊé•">
      üîó ÂàÜ‰∫´
    </button>
  </div>
</div>


**‰ΩúËÄÖ**: Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe

**ÂàÜÁ±ª**: cs.RO, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-10-11 (Êõ¥Êñ∞: 2025-10-29)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÁªºËø∞Âº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫‰∏éÊéßÂà∂Á≥ªÁªü‰∏≠ÁöÑÂ∫îÁî®ÔºöÂàÜÁ±ª„ÄÅË∂ãÂäø‰∏éÁªìÊûÑÂåñÂõûÈ°æ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫ÊéßÂà∂` `Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†` `Ëá™‰∏ªÂ≠¶‰π†` `ÊéßÂà∂Á≥ªÁªü`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊú∫Âô®‰∫∫ÊéßÂà∂ÊñπÊ≥ïÈöæ‰ª•ÈÄÇÂ∫îÂä®ÊÄÅÂíå‰∏çÁ°ÆÂÆöÁéØÂ¢ÉÔºåÈúÄË¶ÅÊõ¥Êô∫ËÉΩÁöÑËá™‰∏ªÂ≠¶‰π†Á≠ñÁï•„ÄÇ
2. Êú¨ÊñáÂØπÂº∫ÂåñÂ≠¶‰π†ÁêÜËÆ∫‰∏éÁÆóÊ≥ïËøõË°åÁªºËø∞ÔºåÂπ∂ÂàÜÊûêÂÖ∂Âú®Êú∫Âô®‰∫∫ÊéßÂà∂‰∏≠ÁöÑÂ∫îÁî®ÔºåÊó®Âú®Âº•ÂêàÁêÜËÆ∫‰∏éÂÆûË∑µÁöÑÂ∑ÆË∑ù„ÄÇ
3. ÈáçÁÇπÂÖ≥Ê≥®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÂ¶ÇDDPG„ÄÅTD3„ÄÅPPO„ÄÅSACÁ≠âÔºåÂπ∂ÂØπÂ∫îÁî®Âú∫ÊôØËøõË°åÂàÜÁ±ªÔºåÊÄªÁªìÊäÄÊúØË∂ãÂäø„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âº∫ÂåñÂ≠¶‰π†(RL)Â∑≤Êàê‰∏∫Âú®Âä®ÊÄÅÂíå‰∏çÁ°ÆÂÆöÁéØÂ¢É‰∏≠ÂÆûÁé∞Êô∫ËÉΩÊú∫Âô®‰∫∫Ë°å‰∏∫ÁöÑÂü∫Á°ÄÊñπÊ≥ï„ÄÇÊú¨ÊñáÊ∑±ÂÖ•ÂõûÈ°æ‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÁöÑÂéüÁêÜ„ÄÅÂÖàËøõÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†(DRL)ÁÆóÊ≥ïÂèäÂÖ∂Âú®Êú∫Âô®‰∫∫ÂíåÊéßÂà∂Á≥ªÁªü‰∏≠ÁöÑÈõÜÊàê„ÄÇÁ†îÁ©∂‰ªéÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã(MDP)ÁöÑÂΩ¢ÂºèÂåñÂÆö‰πâÂºÄÂßãÔºåÊ¶ÇËø∞‰∫ÜÊô∫ËÉΩ‰Ωì-ÁéØÂ¢É‰∫§‰∫íÁöÑÂü∫Êú¨Ë¶ÅÁ¥†ÔºåÂπ∂Êé¢ËÆ®‰∫ÜÂåÖÊã¨Actor-CriticÊñπÊ≥ï„ÄÅÂü∫‰∫é‰ª∑ÂÄºÁöÑÂ≠¶‰π†ÂíåÁ≠ñÁï•Ê¢ØÂ∫¶Á≠âÊ†∏ÂøÉÁÆóÊ≥ïÁ≠ñÁï•„ÄÇÈáçÁÇπ‰ªãÁªç‰∫ÜÁé∞‰ª£DRLÊäÄÊúØÔºåÂ¶ÇDDPG„ÄÅTD3„ÄÅPPOÂíåSACÔºåËøô‰∫õÊäÄÊúØÂú®Ëß£ÂÜ≥È´òÁª¥„ÄÅËøûÁª≠ÊéßÂà∂‰ªªÂä°‰∏≠ÊòæÁ§∫Âá∫ÊΩúÂäõ„ÄÇÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÂàÜÁ±ªÊ≥ïÔºåÁî®‰∫éÂØπRLÂú®ËøêÂä®„ÄÅÊìç‰Ωú„ÄÅÂ§öÊô∫ËÉΩ‰ΩìÂçèË∞ÉÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®ËøõË°åÂàÜÁ±ªÔºå‰ª•ÂèäËÆ≠ÁªÉÊñπÊ≥ïÂíåÈÉ®ÁΩ≤ÂáÜÂ§áÁ®ãÂ∫¶„ÄÇËØ•ÁªºËø∞ÊÄªÁªì‰∫ÜÊúÄËøëÁöÑÁ†îÁ©∂ÊàêÊûúÔºåÂº∫Ë∞É‰∫ÜÊäÄÊúØË∂ãÂäø„ÄÅËÆæËÆ°Ê®°Âºè‰ª•ÂèäRLÂú®Áé∞ÂÆû‰∏ñÁïåÊú∫Âô®‰∫∫ÊäÄÊúØ‰∏≠Êó•ÁõäÊàêÁÜü„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåËøôÈ°πÂ∑•‰ΩúÊó®Âú®Â∞ÜÁêÜËÆ∫ËøõÊ≠•‰∏éÂÆûÈôÖÂ∫îÁî®ËÅîÁ≥ªËµ∑Êù•Ôºå‰∏∫RLÂú®Ëá™‰∏ªÊú∫Âô®‰∫∫Á≥ªÁªü‰∏≠‰∏çÊñ≠ÂèëÂ±ïÁöÑ‰ΩúÁî®Êèê‰æõ‰∏Ä‰∏™ÁªºÂêàÁöÑËßÜËßí„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÁé∞ÊúâÊú∫Âô®‰∫∫ÊéßÂà∂ÊñπÊ≥ïÂú®Èù¢ÂØπÂä®ÊÄÅ„ÄÅÂ§çÊùÇÂíå‰∏çÁ°ÆÂÆöÁéØÂ¢ÉÊó∂ÔºåÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥ÔºåÈúÄË¶Å‰∫∫Â∑•ËÆæËÆ°Â§çÊùÇÁöÑÊéßÂà∂Á≠ñÁï•„ÄÇÂº∫ÂåñÂ≠¶‰π†Êó®Âú®ÈÄöËøáÊô∫ËÉΩ‰Ωì‰∏éÁéØÂ¢ÉÁöÑ‰∫§‰∫íÔºåËá™‰∏ªÂ≠¶‰π†ÊúÄ‰ºòÁ≠ñÁï•Ôºå‰ªéËÄåËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂú®Â∫îÁî®‰∫éÈ´òÁª¥ËøûÁª≠ÊéßÂà∂‰ªªÂä°Êó∂ÔºåÈù¢‰∏¥ÁùÄÊ†∑Êú¨ÊïàÁéá‰Ωé„ÄÅËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÁ≠âÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊú¨ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÂØπÂº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫ÊéßÂà∂È¢ÜÂüüÁöÑÂ∫îÁî®ËøõË°åÁ≥ªÁªüÊÄßÁöÑÊ¢≥ÁêÜÂíåÂàÜÁ±ªÔºå‰ªéÁêÜËÆ∫Âü∫Á°ÄÂà∞ÂÖ∑‰ΩìÁÆóÊ≥ïÔºåÂÜçÂà∞Â∫îÁî®Âú∫ÊôØÔºåÊûÑÂª∫‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÁü•ËØÜÊ°ÜÊû∂„ÄÇÈÄöËøáÂàÜÊûêÁé∞ÊúâÊñπÊ≥ïÁöÑ‰ºòÁº∫ÁÇπÔºåÊÄªÁªìÊäÄÊúØË∂ãÂäøÂíåËÆæËÆ°Ê®°ÂºèÔºå‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõÂèÇËÄÉ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊú¨ÊñáÈ¶ñÂÖàÂõûÈ°æ‰∫ÜÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã(MDP)ÁöÑÂü∫Êú¨Ê¶ÇÂøµÔºåÁÑ∂Âêé‰ªãÁªç‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÁöÑÊ†∏ÂøÉÁÆóÊ≥ïÔºåÂåÖÊã¨Âü∫‰∫é‰ª∑ÂÄºÁöÑÂ≠¶‰π†„ÄÅÁ≠ñÁï•Ê¢ØÂ∫¶ÊñπÊ≥ïÂíåActor-CriticÊñπÊ≥ï„ÄÇÊé•ÁùÄÔºåÈáçÁÇπ‰ªãÁªç‰∫ÜÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂ¶ÇDDPG„ÄÅTD3„ÄÅPPOÂíåSAC„ÄÇÊúÄÂêéÔºåÂØπÂº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫ÊéßÂà∂È¢ÜÂüüÁöÑÂ∫îÁî®ËøõË°å‰∫ÜÂàÜÁ±ªÔºåÂåÖÊã¨ËøêÂä®„ÄÅÊìç‰Ωú„ÄÅÂ§öÊô∫ËÉΩ‰ΩìÂçèË∞ÉÂíå‰∫∫Êú∫‰∫§‰∫íÁ≠â„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊú¨ÊñáÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÁªìÊûÑÂåñÁöÑÂàÜÁ±ªÊ≥ïÔºåÁî®‰∫éÂØπÂº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫ÊéßÂà∂È¢ÜÂüüÁöÑÂ∫îÁî®ËøõË°åÂàÜÁ±ª„ÄÇËØ•ÂàÜÁ±ªÊ≥ïËÄÉËôë‰∫ÜÂ∫îÁî®Âú∫ÊôØ„ÄÅËÆ≠ÁªÉÊñπÊ≥ïÂíåÈÉ®ÁΩ≤ÂáÜÂ§áÁ®ãÂ∫¶Á≠âÂõ†Á¥†Ôºå‰ªéËÄåËÉΩÂ§üÊõ¥ÂÖ®Èù¢Âú∞‰∫ÜËß£Âº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫ÊéßÂà∂È¢ÜÂüüÁöÑÂèëÂ±ïÁé∞Áä∂ÂíåÊú™Êù•Ë∂ãÂäø„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊú¨ÊñáÂØπÂêÑÁßçÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÂÖ≥ÈîÆËÆæËÆ°ËøõË°å‰∫ÜÊÄªÁªìÔºå‰æãÂ¶ÇÔºåDDPG‰ΩøÁî®Á°ÆÂÆöÊÄßÁ≠ñÁï•Ê¢ØÂ∫¶ÔºåTD3ÈÄöËøáÂºïÂÖ•ÂèåÈáçËØÑËÆ∫ÂÆ∂ÁΩëÁªúÊù•ÂáèÂ∞ë‰ª∑ÂÄºÈ´ò‰º∞ÔºåPPO‰ΩøÁî®‰ø°‰ªªÂå∫Âüü‰ºòÂåñÊù•‰øùËØÅÁ≠ñÁï•Êõ¥Êñ∞ÁöÑÁ®≥ÂÆöÊÄßÔºåSACÂºïÂÖ•‰∫ÜÁÜµÊ≠£ÂàôÂåñÊù•ÈºìÂä±Êé¢Á¥¢„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòËÆ®ËÆ∫‰∫ÜÂêÑÁßçËÆ≠ÁªÉÊäÄÂ∑ßÔºåÂ¶ÇÁªèÈ™åÂõûÊîæ„ÄÅÁõÆÊ†áÁΩëÁªúÂíåÊâπÈáèÂΩí‰∏ÄÂåñÁ≠â„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ËØ•ÁªºËø∞ÊÄªÁªì‰∫ÜËøëÂπ¥Êù•Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫ÊéßÂà∂È¢ÜÂüüÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÈáçÁÇπÂÖ≥Ê≥®‰∫ÜDDPG„ÄÅTD3„ÄÅPPOÂíåSACÁ≠âÁÆóÊ≥ïÂú®Ëß£ÂÜ≥È´òÁª¥ËøûÁª≠ÊéßÂà∂‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇÈÄöËøáÂØπ‰∏çÂêåÂ∫îÁî®Âú∫ÊôØÁöÑÂàÜÁ±ªÂíåÂàÜÊûêÔºåÊè≠Á§∫‰∫ÜÂº∫ÂåñÂ≠¶‰π†Âú®Êú∫Âô®‰∫∫ÊéßÂà∂È¢ÜÂüüÁöÑÊäÄÊúØË∂ãÂäøÂíåËÆæËÆ°Ê®°ÂºèÔºå‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂ÊñπÂêëÊèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑÂèÇËÄÉ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂêÑÁßçÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°Ôºå‰æãÂ¶ÇËá™‰∏ªÂØºËà™„ÄÅÁâ©‰ΩìÊäìÂèñ„ÄÅË£ÖÈÖç„ÄÅ‰∫∫Êú∫Âçè‰ΩúÁ≠â„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºåÊú∫Âô®‰∫∫ËÉΩÂ§üËá™‰∏ªÂ≠¶‰π†ÈÄÇÂ∫îÂ§çÊùÇÁéØÂ¢ÉÔºåÊèêÈ´òÂ∑•‰ΩúÊïàÁéáÂíåÂÆâÂÖ®ÊÄß„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂Â∞ÜÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÂú®Â∑•‰∏öËá™Âä®Âåñ„ÄÅÂåªÁñóÂÅ•Â∫∑„ÄÅÊô∫ËÉΩÂÆ∂Â±ÖÁ≠âÈ¢ÜÂüüÁöÑÂπøÊ≥õÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Reinforcement learning (RL) has become a foundational approach for enabling intelligent robotic behavior in dynamic and uncertain environments. This work presents an in-depth review of RL principles, advanced deep reinforcement learning (DRL) algorithms, and their integration into robotic and control systems. Beginning with the formalism of Markov Decision Processes (MDPs), the study outlines essential elements of the agent-environment interaction and explores core algorithmic strategies including actor-critic methods, value-based learning, and policy gradients. Emphasis is placed on modern DRL techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving high-dimensional, continuous control tasks. A structured taxonomy is introduced to categorize RL applications across domains such as locomotion, manipulation, multi-agent coordination, and human-robot interaction, along with training methodologies and deployment readiness levels. The review synthesizes recent research efforts, highlighting technical trends, design patterns, and the growing maturity of RL in real-world robotics. Overall, this work aims to bridge theoretical advances with practical implementations, providing a consolidated perspective on the evolving role of RL in autonomous robotic systems.

