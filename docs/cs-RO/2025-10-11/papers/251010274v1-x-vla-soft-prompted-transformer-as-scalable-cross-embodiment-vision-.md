---
layout: default
title: X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model
---

# X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10274" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10274v1</a>
  <a href="https://arxiv.org/pdf/2510.10274.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10274v1" onclick="toggleFavorite(this, '2510.10274v1', 'X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin Zou, Yilun Chen, Jia Zeng, Ya-Qin Zhang, Jiangmiao Pang, Jingjing Liu, Tai Wang, Xianyuan Zhan

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-11

**å¤‡æ³¨**: preprint, technical report, 33 pages

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://thu-air-dream.github.io/X-VLA/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**X-VLAï¼šåŸºäºè½¯æç¤ºTransformerçš„å¯æ‰©å±•è·¨å…·èº«è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸ƒï¼šåŠ¨ä½œé‡å®šå‘ (Motion Retargeting)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹` `è·¨å…·èº«å­¦ä¹ ` `è½¯æç¤ºå­¦ä¹ ` `Transformer` `æœºå™¨äººå­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰VLAæ¨¡å‹éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è·¨å…·èº«æœºå™¨äººæ•°æ®çš„å¼‚æ„æ€§ï¼Œé˜»ç¢äº†é€šç”¨æœºå™¨äººå­¦ä¹ çš„å‘å±•ã€‚
2. è®ºæ–‡æå‡ºè½¯æç¤ºæ–¹æ³•ï¼Œä¸ºæ¯ä¸ªæ•°æ®æºå¼•å…¥å¯å­¦ä¹ åµŒå…¥ï¼Œä½œä¸ºå…·èº«ç‰¹å®šæç¤ºï¼Œæå‡æ¨¡å‹å¯¹å¼‚æ„æ•°æ®çš„åˆ©ç”¨ç‡ã€‚
3. X-VLAæ¨¡å‹åœ¨å¤šä¸ªæ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­å–å¾—SOTAæ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨è·¨å…·èº«é€‚åº”æ€§å’Œä»»åŠ¡æ³›åŒ–æ–¹é¢çš„ä¼˜åŠ¿ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

é€šç”¨çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¾èµ–äºè·¨å¤šç§æœºå™¨äººå¹³å°ã€å¤§è§„æ¨¡ã€è·¨å…·èº«ã€å¼‚æ„æ•°æ®é›†çš„æœ‰æ•ˆè®­ç»ƒã€‚ä¸ºäº†ä¿ƒè¿›å’Œåˆ©ç”¨ä¸°å¯Œå¤šæ ·çš„æœºå™¨äººæ•°æ®æºä¸­çš„å¼‚æ„æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è½¯æç¤ºæ–¹æ³•ï¼Œé€šè¿‡å°†æç¤ºå­¦ä¹ æ¦‚å¿µæ³¨å…¥è·¨å…·èº«æœºå™¨äººå­¦ä¹ ï¼Œå¹¶ä¸ºæ¯ä¸ªä¸åŒçš„æ•°æ®æºå¼•å…¥å•ç‹¬çš„å¯å­¦ä¹ åµŒå…¥é›†ï¼Œä»è€Œä»¥æœ€å°çš„å‚æ•°æ·»åŠ å®ç°ã€‚è¿™äº›åµŒå…¥ä½œä¸ºç‰¹å®šäºå…·èº«çš„æç¤ºï¼Œå…±åŒèµ‹äºˆVLAæ¨¡å‹æœ‰æ•ˆåˆ©ç”¨ä¸åŒè·¨å…·èº«ç‰¹å¾çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ–°çš„X-VLAï¼Œä¸€ä¸ªç®€æ´çš„åŸºäºæµåŒ¹é…çš„VLAæ¶æ„ï¼Œå®Œå…¨ä¾èµ–äºè½¯æç¤ºçš„æ ‡å‡†Transformerç¼–ç å™¨ï¼Œå…¼å…·å¯æ‰©å±•æ€§å’Œç®€æ´æ€§ã€‚åœ¨6ä¸ªæ¨¡æ‹Ÿç¯å¢ƒå’Œ3ä¸ªçœŸå®ä¸–ç•Œæœºå™¨äººä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„0.9Bå®ä¾‹åŒ–-X-VLA-0.9BåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†SOTAæ€§èƒ½ï¼Œåœ¨çµæ´»çš„çµå·§æ€§åˆ°è·¨å…·èº«ã€ç¯å¢ƒå’Œä»»åŠ¡çš„å¿«é€Ÿé€‚åº”ç­‰å¹¿æ³›çš„èƒ½åŠ›è½´ä¸Šå±•ç¤ºäº†å“è¶Šçš„ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤„ç†æ¥è‡ªä¸åŒæœºå™¨äººå¹³å°å’Œç¯å¢ƒçš„å¼‚æ„æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸åŒå…·èº«ï¼ˆembodimentï¼‰çš„æ•°æ®å…·æœ‰ä¸åŒçš„ç‰¹å¾å’Œåˆ†å¸ƒï¼Œç›´æ¥æ··åˆè®­ç»ƒä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œéš¾ä»¥å®ç°æœ‰æ•ˆçš„è·¨å…·èº«æ³›åŒ–ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤æ‚çš„é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯æˆ–æ•°æ®é¢„å¤„ç†ï¼Œå¢åŠ äº†è®­ç»ƒçš„å¤æ‚æ€§å’Œæˆæœ¬ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨è½¯æç¤ºï¼ˆsoft promptï¼‰å­¦ä¹ æ¥åŒºåˆ†å’Œåˆ©ç”¨ä¸åŒå…·èº«çš„æ•°æ®ç‰¹å¾ã€‚é€šè¿‡ä¸ºæ¯ä¸ªå…·èº«å¼•å…¥ä¸€ç»„å¯å­¦ä¹ çš„åµŒå…¥å‘é‡ä½œä¸ºæç¤ºï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®ä¸åŒçš„å…·èº«è°ƒæ•´å…¶è¡Œä¸ºï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”å¼‚æ„æ•°æ®ã€‚è¿™ç§æ–¹æ³•åªéœ€è¦å°‘é‡é¢å¤–çš„å‚æ•°ï¼Œå°±å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ¨¡å‹çš„è·¨å…·èº«æ³›åŒ–èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šX-VLAæ¨¡å‹åŸºäºTransformeræ¶æ„ï¼Œä¸»è¦åŒ…å«è§†è§‰ç¼–ç å™¨ã€è¯­è¨€ç¼–ç å™¨å’ŒåŠ¨ä½œè§£ç å™¨ã€‚è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€ç¼–ç å™¨å°†è¾“å…¥å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤è½¬æ¢ä¸ºåµŒå…¥å‘é‡ï¼Œç„¶åé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèåˆã€‚åŠ¨ä½œè§£ç å™¨æ ¹æ®èåˆåçš„ç‰¹å¾ç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚å…³é”®åœ¨äºï¼Œåœ¨è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€ç¼–ç å™¨çš„è¾“å…¥ç«¯ï¼Œä¸ºæ¯ä¸ªå…·èº«æ·»åŠ ä¸€ä¸ªå¯å­¦ä¹ çš„è½¯æç¤ºåµŒå…¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†è½¯æç¤ºå­¦ä¹ åº”ç”¨äºè·¨å…·èº«æœºå™¨äººå­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„ç¡¬æç¤ºï¼ˆhard promptï¼‰ä¸åŒï¼Œè½¯æç¤ºæ˜¯å¯å­¦ä¹ çš„ï¼Œå¯ä»¥æ ¹æ®æ•°æ®è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ä¸åŒå…·èº«çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒX-VLAæ¨¡å‹é‡‡ç”¨äº†ä¸€ç§ç®€æ´çš„åŸºäºæµåŒ¹é…ï¼ˆflow-matchingï¼‰çš„VLAæ¶æ„ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ¯ä¸ªå…·èº«çš„è½¯æç¤ºåµŒå…¥éƒ½æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡ï¼Œå…¶ç»´åº¦ä¸Transformerç¼–ç å™¨çš„è¾“å…¥ç»´åº¦ç›¸åŒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè½¯æç¤ºåµŒå…¥ä¸è¾“å…¥å›¾åƒæˆ–æ–‡æœ¬æŒ‡ä»¤çš„åµŒå…¥å‘é‡è¿›è¡Œæ‹¼æ¥ï¼Œç„¶åè¾“å…¥åˆ°Transformerç¼–ç å™¨ä¸­ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬åŠ¨ä½œé¢„æµ‹æŸå¤±å’ŒæµåŒ¹é…æŸå¤±ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹çš„åŠ¨ä½œç”Ÿæˆèƒ½åŠ›å’Œè·¨å…·èº«æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹å‚æ•°é‡ä¸º0.9Bã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

X-VLA-0.9Bæ¨¡å‹åœ¨6ä¸ªæ¨¡æ‹Ÿç¯å¢ƒå’Œ3ä¸ªçœŸå®ä¸–ç•Œæœºå™¨äººä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†SOTAæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨Dexterityä»»åŠ¡ä¸­ï¼ŒX-VLAæ¨¡å‹æ¯”ç°æœ‰æœ€ä½³æ¨¡å‹æé«˜äº†10%ä»¥ä¸Šçš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒX-VLAæ¨¡å‹è¿˜å±•ç¤ºäº†è‰¯å¥½çš„è·¨å…·èº«é€‚åº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ–°çš„æœºå™¨äººå¹³å°ä¸Šå¿«é€Ÿå­¦ä¹ å’Œæ‰§è¡Œä»»åŠ¡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æœºå™¨äººåº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººå’ŒåŒ»ç–—æœºå™¨äººã€‚é€šè¿‡åˆ©ç”¨è·¨å…·èº«å­¦ä¹ ï¼Œæœºå™¨äººå¯ä»¥å¿«é€Ÿé€‚åº”æ–°çš„ç¯å¢ƒå’Œä»»åŠ¡ï¼Œæé«˜å…¶çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºæœºå™¨äººç¾¤ä½“çš„ååŒå­¦ä¹ ï¼Œä½¿å¤šä¸ªæœºå™¨äººèƒ½å¤Ÿå…±äº«çŸ¥è¯†å’Œç»éªŒï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/

