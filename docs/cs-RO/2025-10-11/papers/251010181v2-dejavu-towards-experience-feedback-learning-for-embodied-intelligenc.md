---
layout: default
title: Dejavu: Towards Experience Feedback Learning for Embodied Intelligence
---

# Dejavu: Towards Experience Feedback Learning for Embodied Intelligence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2510.10181" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2510.10181v2</a>
  <a href="https://arxiv.org/pdf/2510.10181.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2510.10181v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2510.10181v2', 'Dejavu: Towards Experience Feedback Learning for Embodied Intelligence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shaokai Wu, Yanbiao Ji, Qiuchang Li, Zhiyi Zhang, Qichen He, Wenyuan Xie, Guodong Zhang, Bayram Bayramli, Yue Ding, Hongtao Lu

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-10-11 (æ›´æ–°: 2025-12-07)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Dejavuï¼šé¢å‘å…·èº«æ™ºèƒ½çš„ç»éªŒåé¦ˆå­¦ä¹ æ¡†æ¶ï¼Œæå‡éƒ¨ç½²åæ™ºèƒ½ä½“æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å…·èº«æ™ºèƒ½` `ç»éªŒåé¦ˆå­¦ä¹ ` `éƒ¨ç½²åå­¦ä¹ ` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰-è¯­è¨€-åŠ¨ä½œ` `æŒç»­å­¦ä¹ ` `æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å…·èº«æ™ºèƒ½ä½“åœ¨éƒ¨ç½²åæ— æ³•æŒç»­å­¦ä¹ ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½æå‡ã€‚
2. Dejavuæ¡†æ¶é€šè¿‡ç»éªŒåé¦ˆç½‘ç»œï¼ˆEFNï¼‰æ£€ç´¢å†å²ç»éªŒï¼ŒæŒ‡å¯¼æ™ºèƒ½ä½“è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ï¼Œå®ç°æŒç»­å­¦ä¹ ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒDejavuæ¡†æ¶åœ¨å¤šç§å…·èº«ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ™ºèƒ½ä½“çš„é€‚åº”æ€§ã€é²æ£’æ€§å’ŒæˆåŠŸç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å…·èº«æ™ºèƒ½ä½“é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§çš„é™åˆ¶ï¼šä¸€æ—¦éƒ¨ç½²åˆ°çœŸå®ç¯å¢ƒä¸­æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œå®ƒä»¬å°±æ— æ³•è·å–é¢å¤–çš„çŸ¥è¯†æ¥å¢å¼ºä»»åŠ¡æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„éƒ¨ç½²åå­¦ä¹ æ¡†æ¶Dejavuï¼Œå®ƒé‡‡ç”¨ç»éªŒåé¦ˆç½‘ç»œï¼ˆEFNï¼‰ï¼Œå¹¶é€šè¿‡æ£€ç´¢åˆ°çš„æ‰§è¡Œè®°å¿†æ¥å¢å¼ºå†»ç»“çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥ã€‚EFNè¯†åˆ«ä¸Šä¸‹æ–‡ç›¸å…³çš„å…ˆå‰åŠ¨ä½œç»éªŒï¼Œå¹¶ä»¥æ­¤æ£€ç´¢åˆ°çš„æŒ‡å¯¼ä¿¡æ¯ä¸ºæ¡ä»¶è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ã€‚æˆ‘ä»¬é‡‡ç”¨å¸¦æœ‰è¯­ä¹‰ç›¸ä¼¼æ€§å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒEFNï¼Œç¡®ä¿é¢„æµ‹çš„åŠ¨ä½œä¸å½“å‰è§‚å¯Ÿä¸‹çš„è¿‡å»è¡Œä¸ºä¿æŒä¸€è‡´ã€‚åœ¨éƒ¨ç½²æœŸé—´ï¼ŒEFNä¸æ–­ç”¨æ–°çš„è½¨è¿¹ä¸°å¯Œå…¶è®°å¿†ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè¡¨ç°å‡ºâ€œä»ç»éªŒä¸­å­¦ä¹ â€çš„èƒ½åŠ›ã€‚åœ¨å„ç§å…·èº«ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å†»ç»“çš„åŸºçº¿ç›¸æ¯”ï¼ŒEFNæé«˜äº†é€‚åº”æ€§ã€é²æ£’æ€§å’ŒæˆåŠŸç‡ã€‚æˆ‘ä»¬åœ¨è¡¥å……ææ–™ä¸­æä¾›äº†ä»£ç å’Œæ¼”ç¤ºã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰å…·èº«æ™ºèƒ½ä½“åœ¨éƒ¨ç½²åæ— æ³•ç»§ç»­å­¦ä¹ å’Œé€‚åº”æ–°ç¯å¢ƒï¼Œå¯¼è‡´å…¶æ€§èƒ½å—é™ã€‚ä¼ ç»Ÿçš„è®­ç»ƒæ–¹å¼é€šå¸¸ä¾èµ–äºå¤§é‡é¢„è®­ç»ƒæ•°æ®ï¼Œä½†æ— æ³•è§£å†³æ™ºèƒ½ä½“åœ¨çœŸå®éƒ¨ç½²ç¯å¢ƒä¸­é‡åˆ°çš„æ–°é—®é¢˜å’ŒæŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨éƒ¨ç½²åä½¿æ™ºèƒ½ä½“èƒ½å¤ŸæŒç»­å­¦ä¹ ï¼Œæå‡å…¶é€‚åº”æ€§å’Œé²æ£’æ€§ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDejavuçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç»éªŒåé¦ˆç½‘ç»œï¼ˆEFNï¼‰æ¥æ¨¡æ‹Ÿäººç±»çš„ç»éªŒå­¦ä¹ è¿‡ç¨‹ã€‚æ™ºèƒ½ä½“åœ¨æ‰§è¡Œä»»åŠ¡çš„è¿‡ç¨‹ä¸­ï¼Œä¼šå°†å†å²ç»éªŒå­˜å‚¨èµ·æ¥ï¼Œå¹¶åœ¨åç»­å†³ç­–æ—¶ï¼Œæ ¹æ®å½“å‰çš„ç¯å¢ƒçŠ¶æ€æ£€ç´¢ç›¸å…³çš„å†å²ç»éªŒï¼Œå¹¶åˆ©ç”¨è¿™äº›ç»éªŒæ¥æŒ‡å¯¼åŠ¨ä½œé¢„æµ‹ã€‚è¿™ç§æ–¹å¼ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿä»è‡ªèº«çš„ç»éªŒä¸­å­¦ä¹ ï¼Œä»è€Œä¸æ–­æå‡æ€§èƒ½ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDejavuæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥ï¼šè´Ÿè´£æ ¹æ®è§†è§‰å’Œè¯­è¨€è¾“å…¥ç”ŸæˆåŠ¨ä½œæŒ‡ä»¤ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚2) ç»éªŒåé¦ˆç½‘ç»œï¼ˆEFNï¼‰ï¼šè´Ÿè´£æ£€ç´¢ç›¸å…³çš„å†å²ç»éªŒï¼Œå¹¶æ ¹æ®è¿™äº›ç»éªŒæ¥è°ƒæ•´VLAç­–ç•¥çš„è¾“å‡ºã€‚3) è®°å¿†æ¨¡å—ï¼šç”¨äºå­˜å‚¨æ™ºèƒ½ä½“çš„å†å²ç»éªŒï¼ŒåŒ…æ‹¬ç¯å¢ƒçŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ç­‰ä¿¡æ¯ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒEFNé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œç›®æ ‡æ˜¯ä½¿é¢„æµ‹çš„åŠ¨ä½œä¸å†å²ç»éªŒä¿æŒä¸€è‡´ã€‚åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼ŒEFNä¸æ–­æ›´æ–°è®°å¿†æ¨¡å—ï¼Œä»è€Œå®ç°æŒç»­å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šDejavuçš„å…³é”®åˆ›æ–°åœ¨äºæå‡ºäº†ç»éªŒåé¦ˆç½‘ç»œï¼ˆEFNï¼‰ï¼Œå®ƒèƒ½å¤Ÿå°†å†å²ç»éªŒèå…¥åˆ°åŠ¨ä½œé¢„æµ‹è¿‡ç¨‹ä¸­ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒDejavuä¸éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªç­–ç•¥ç½‘ç»œï¼Œè€Œæ˜¯é€šè¿‡EFNæ¥å¯¹é¢„è®­ç»ƒçš„VLAç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°å¿«é€Ÿé€‚åº”ã€‚æ­¤å¤–ï¼ŒDejavuè¿˜é‡‡ç”¨äº†è¯­ä¹‰ç›¸ä¼¼æ€§å¥–åŠ±ï¼Œé¼“åŠ±æ™ºèƒ½ä½“é€‰æ‹©ä¸å†å²ç»éªŒç›¸ä¼¼çš„åŠ¨ä½œï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šEFNçš„ç½‘ç»œç»“æ„å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œé€šå¸¸åŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚ç¼–ç å™¨è´Ÿè´£å°†å½“å‰çš„ç¯å¢ƒçŠ¶æ€å’Œå†å²ç»éªŒç¼–ç æˆå‘é‡è¡¨ç¤ºï¼Œè§£ç å™¨è´Ÿè´£æ ¹æ®è¿™äº›å‘é‡è¡¨ç¤ºç”ŸæˆåŠ¨ä½œé¢„æµ‹ã€‚æŸå¤±å‡½æ•°ä¸»è¦åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼šä¸€æ˜¯åŠ¨ä½œé¢„æµ‹çš„äº¤å‰ç†µæŸå¤±ï¼Œç”¨äºä¿è¯é¢„æµ‹çš„åŠ¨ä½œä¸çœŸå®åŠ¨ä½œä¸€è‡´ï¼›äºŒæ˜¯è¯­ä¹‰ç›¸ä¼¼æ€§æŸå¤±ï¼Œç”¨äºé¼“åŠ±æ™ºèƒ½ä½“é€‰æ‹©ä¸å†å²ç»éªŒç›¸ä¼¼çš„åŠ¨ä½œã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯ä»¥é€‰æ‹©å¸¸è§çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå¦‚PPOæˆ–A2Cã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDejavuæ¡†æ¶åœ¨å¤šä¸ªå…·èº«ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨Object Navigationä»»åŠ¡ä¸­ï¼ŒDejavuæ¡†æ¶çš„æˆåŠŸç‡æ¯”å†»ç»“çš„åŸºçº¿æé«˜äº†15%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒDejavuæ¡†æ¶è¿˜è¡¨ç°å‡ºäº†æ›´å¥½çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒå˜åŒ–å’Œä»»åŠ¡éœ€æ±‚ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDejavuæ¡†æ¶æ˜¯ä¸€ç§æœ‰æ•ˆçš„éƒ¨ç½²åå­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å…·èº«æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Dejavuæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦æ™ºèƒ½ä½“åœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œäº¤äº’çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰ã€‚é€šè¿‡æŒç»­å­¦ä¹ å’Œé€‚åº”ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°å®Œæˆä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œå¹¶é™ä½äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥ç”¨äºæ¢ç´¢æœªçŸ¥ç¯å¢ƒï¼Œå‘ç°æ–°çš„çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Embodied agents face a fundamental limitation: once deployed in real-world environments to perform specific tasks, they are unable to acquire additional knowledge to enhance task performance. In this paper, we propose a general post-deployment learning framework Dejavu, which employs an Experience Feedback Network (EFN) and augments the frozen Vision-Language-Action (VLA) policy with retrieved execution memories. EFN identifies contextually prior action experiences and conditions action prediction on this retrieved guidance. We adopt reinforcement learning with semantic similarity rewards to train EFN, ensuring that the predicted actions align with past behaviors under current observations. During deployment, EFN continually enriches its memory with new trajectories, enabling the agent to exhibit "learning from experience". Experiments across diverse embodied tasks show that EFN improves adaptability, robustness, and success rates over frozen baselines. We provide code and demo in our supplementary material.

