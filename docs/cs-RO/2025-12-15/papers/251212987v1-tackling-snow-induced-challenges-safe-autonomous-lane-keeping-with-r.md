---
layout: default
title: "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning"
---

# Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.12987" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.12987v1</a>
  <a href="https://arxiv.org/pdf/2512.12987.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.12987v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.12987v1', 'Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Amin Jalal Aghdasian, Farzaneh Abdollahi, Ali Kamali Iglie

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºé²æ£’å¼ºåŒ–å­¦ä¹ çš„è½¦é“ä¿æŒç³»ç»Ÿï¼Œè§£å†³é›ªåœ°è‡ªåŠ¨é©¾é©¶éš¾é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æ·±åº¦å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨é©¾é©¶` `è½¦é“ä¿æŒ` `é²æ£’æ§åˆ¶` `é›ªåœ°ç¯å¢ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è½¦é“ä¿æŒç³»ç»Ÿåœ¨é›ªåœ°ç­‰å¤æ‚ç¯å¢ƒä¸‹ï¼Œæ˜“å—è·¯é¢æ»‘ç§»å’Œæ„ŸçŸ¥å™ªå£°å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
2. è®ºæ–‡æå‡ºåŠ¨ä½œé²æ£’çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¢å¼ºç­–ç•¥å¯¹ç¯å¢ƒä¸ç¡®å®šæ€§çš„é€‚åº”èƒ½åŠ›ï¼Œæé«˜ç³»ç»Ÿé²æ£’æ€§ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒAR-CADPGæ–¹æ³•åœ¨é›ªåœ°åœºæ™¯ä¸‹å…·æœ‰æ›´é«˜çš„è·¯å¾„è·Ÿè¸ªç²¾åº¦å’Œé²æ£’æ€§ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–°çš„ç®—æ³•ï¼Œç”¨äºåœ¨é›ªåœ°æ¡ä»¶ä¸‹è‡ªåŠ¨é©¾é©¶è½¦è¾†(AVs)çš„è½¦é“ä¿æŒç³»ç»Ÿ(LKS)ã€‚è¿™äº›ç®—æ³•åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)æ¥å¤„ç†ä¸ç¡®å®šæ€§å’Œæ»‘ç§»ã€‚å®ƒä»¬åŒ…æ‹¬åŠ¨ä½œé²æ£’å¾ªç¯æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦(AR-RDPG)å’Œç«¯åˆ°ç«¯åŠ¨ä½œé²æ£’å·ç§¯ç¥ç»ç½‘ç»œæ³¨æ„åŠ›ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦(AR-CADPG)ï¼Œè¿™ä¸¤ç§åŠ¨ä½œé²æ£’çš„å†³ç­–æ–¹æ³•ã€‚åœ¨AR-RDPGæ–¹æ³•ä¸­ï¼Œåœ¨æ„ŸçŸ¥å±‚å†…ï¼Œé¦–å…ˆä½¿ç”¨å¤šå°ºåº¦ç¥ç»ç½‘ç»œå¯¹ç›¸æœºå›¾åƒè¿›è¡Œå»å™ªã€‚ç„¶åï¼Œé€šè¿‡é¢„è®­ç»ƒçš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ(DCNN)æå–ä¸­å¿ƒçº¿ç³»æ•°ã€‚è¿™äº›ç³»æ•°ä¸é©¾é©¶ç‰¹æ€§è¿æ¥ï¼Œä½œä¸ºæ§åˆ¶å±‚çš„è¾“å…¥ã€‚AR-CADPGæ–¹æ³•æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ–¹æ³•ï¼Œå…¶ä¸­å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’Œæ³¨æ„åŠ›æœºåˆ¶è¢«é›†æˆåˆ°DRLæ¡†æ¶ä¸­ã€‚è¿™ä¸¤ç§æ–¹æ³•é¦–å…ˆåœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å„ç§é›ªåœ°åœºæ™¯ä¸‹è¿›è¡ŒéªŒè¯ã€‚åœ¨åŸºäºJetson Nanoçš„è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸Šçš„çœŸå®å®éªŒè¯å®äº†å­¦ä¹ ç­–ç•¥çš„å¯è¡Œæ€§å’Œç¨³å®šæ€§ã€‚åœ¨ä¸¤ç§æ¨¡å‹ä¸­ï¼ŒAR-CADPGæ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„è·¯å¾„è·Ÿè¸ªç²¾åº¦å’Œé²æ£’æ€§ï¼Œçªå‡ºäº†åœ¨AVsä¸­ç»“åˆæ—¶é—´è®°å¿†ã€å¯¹æŠ—å¼¹æ€§å’Œæ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³é›ªåœ°ç¯å¢ƒä¸‹è‡ªåŠ¨é©¾é©¶è½¦è¾†è½¦é“ä¿æŒç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è·¯é¢æ»‘ç§»å¸¦æ¥çš„æ§åˆ¶ä¸ç¡®å®šæ€§ä»¥åŠæ„ŸçŸ¥ç³»ç»Ÿå—é™é›ªå½±å“äº§ç”Ÿçš„å™ªå£°ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹è¿™äº›é—®é¢˜ï¼Œå¯¼è‡´è½¦é“ä¿æŒæ€§èƒ½ä¸‹é™ï¼Œç”šè‡³å‡ºç°å®‰å…¨éšæ‚£ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)å­¦ä¹ åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„é²æ£’æ§åˆ¶ç­–ç•¥ã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œé²æ£’æ€§ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€‚åº”ç¯å¢ƒå˜åŒ–å’Œæ„ŸçŸ¥å™ªå£°ï¼Œä»è€Œæé«˜è½¦é“ä¿æŒç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå®‰å…¨æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥å¯¹æŠ—æ ·æœ¬ï¼Œä½¿ç­–ç•¥å¯¹åŠ¨ä½œæ‰°åŠ¨å…·æœ‰æ›´å¼ºçš„æŠµæŠ—èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè®ºæ–‡æå‡ºäº†ä¸¤ç§ç®—æ³•ï¼šAR-RDPGå’ŒAR-CADPGã€‚AR-RDPGé¦–å…ˆä½¿ç”¨å¤šå°ºåº¦ç¥ç»ç½‘ç»œå¯¹å›¾åƒè¿›è¡Œå»å™ªï¼Œç„¶ååˆ©ç”¨é¢„è®­ç»ƒçš„DCNNæå–ä¸­å¿ƒçº¿ç³»æ•°ï¼Œå¹¶å°†å…¶ä¸é©¾é©¶ç‰¹å¾ç»“åˆä½œä¸ºRDPGæ§åˆ¶å™¨çš„è¾“å…¥ã€‚AR-CADPGåˆ™é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼ï¼Œå°†CNNå’Œæ³¨æ„åŠ›æœºåˆ¶é›†æˆåˆ°DRLæ¡†æ¶ä¸­ï¼Œç›´æ¥ä»å›¾åƒè¾“å…¥å­¦ä¹ æ§åˆ¶ç­–ç•¥ã€‚ä¸¤ç§æ–¹æ³•éƒ½åœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨çœŸå®è½¦è¾†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚

**å…³é”®åˆ›æ–°**ï¼šè®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº†åŠ¨ä½œé²æ£’æ€§åˆ°æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨äºé›ªåœ°ç¯å¢ƒä¸‹çš„è½¦é“ä¿æŒä»»åŠ¡ã€‚AR-CADPGçš„ç«¯åˆ°ç«¯ç»“æ„ä»¥åŠæ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å…³æ³¨å…³é”®ç‰¹å¾ï¼Œæé«˜è·¯å¾„è·Ÿè¸ªç²¾åº¦ã€‚æ­¤å¤–ï¼Œå¤šå°ºåº¦å»å™ªç½‘ç»œçš„åº”ç”¨ä¹Ÿå¢å¼ºäº†æ„ŸçŸ¥ç³»ç»Ÿçš„é²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šAR-RDPGä¸­ï¼Œå¤šå°ºåº¦å»å™ªç½‘ç»œé‡‡ç”¨U-Netç»“æ„ï¼Œç”¨äºå»é™¤å›¾åƒå™ªå£°ã€‚é¢„è®­ç»ƒçš„DCNNç”¨äºæå–è½¦é“ä¸­å¿ƒçº¿ç³»æ•°ã€‚AR-CADPGä¸­ï¼ŒCNNé‡‡ç”¨ResNetç»“æ„ï¼Œæ³¨æ„åŠ›æœºåˆ¶é‡‡ç”¨Transformerç»“æ„ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬æ§åˆ¶æŸå¤±å’ŒåŠ¨ä½œé²æ£’æ€§æŸå¤±ï¼Œå…¶ä¸­åŠ¨ä½œé²æ£’æ€§æŸå¤±é€šè¿‡å¯¹æŠ—è®­ç»ƒå®ç°ï¼Œé¼“åŠ±ç­–ç•¥å¯¹åŠ¨ä½œæ‰°åŠ¨å…·æœ‰æŠµæŠ—èƒ½åŠ›ã€‚å…·ä½“å‚æ•°è®¾ç½®æœªçŸ¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒAR-CADPGæ–¹æ³•åœ¨é›ªåœ°åœºæ™¯ä¸‹å…·æœ‰æ›´é«˜çš„è·¯å¾„è·Ÿè¸ªç²¾åº¦å’Œé²æ£’æ€§ã€‚ä¸AR-RDPGç›¸æ¯”ï¼ŒAR-CADPGèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è·Ÿè¸ªè½¦é“ä¸­å¿ƒçº¿ï¼Œå¹¶å¯¹ç¯å¢ƒå˜åŒ–å…·æœ‰æ›´å¼ºçš„é€‚åº”èƒ½åŠ›ã€‚çœŸå®è½¦è¾†å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œç¨³å®šæ€§ã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼Œä¾‹å¦‚é›ªåœ°ã€é›¨å¤©ã€é›¾å¤©ç­‰ã€‚é€šè¿‡æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œå¯ä»¥åŠ é€Ÿè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å•†ä¸šåŒ–è½åœ°ï¼Œå¹¶æå‡äº¤é€šè¿è¾“æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚æ— äººæœºã€æ°´ä¸‹æœºå™¨äººç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.

