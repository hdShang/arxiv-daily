---
layout: default
title: Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations
---

# Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.12993" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.12993v1</a>
  <a href="https://arxiv.org/pdf/2512.12993.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.12993v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.12993v1', 'Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guillermo A. Castillo, Himanshu Lodha, Ayonga Hereid

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºä¸€ç§åŸºäºé™ç»´æ„ŸçŸ¥è¡¨ç¤ºçš„åœ°å½¢æ„ŸçŸ¥åŒè¶³è¿åŠ¨å­¦ä¹ æ–¹æ³•**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `åŒè¶³è¿åŠ¨` `åœ°å½¢æ„ŸçŸ¥` `å¼ºåŒ–å­¦ä¹ ` `é™ç»´è¡¨ç¤º` `å˜åˆ†è‡ªç¼–ç å™¨` `æœºå™¨äººæ§åˆ¶` `æ·±åº¦ç›¸æœº` `è¿åŠ¨è§„åˆ’`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç«¯åˆ°ç«¯æ–¹æ³•åœ¨åœ°å½¢æ„ŸçŸ¥åŒè¶³è¿åŠ¨å­¦ä¹ ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å¤„ç†é«˜ç»´æ„ŸçŸ¥è¾“å…¥å’Œå¤æ‚çš„æœºå™¨äººåŠ¨åŠ›å­¦ã€‚
2. è¯¥è®ºæ–‡æå‡ºä¸€ç§åˆ†å±‚ç­–ç•¥ï¼Œåˆ©ç”¨CNN-VAEè¿›è¡Œåœ°å½¢ç¼–ç ï¼Œé™ç»´æœºå™¨äººåŠ¨åŠ›å­¦ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è¿åŠ¨å†³ç­–ã€‚
3. å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å…·æœ‰é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œå¹¶åˆæ­¥éªŒè¯äº†å…¶åœ¨çœŸå®ç¡¬ä»¶ä¸Šçš„å¯è¡Œæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºåœ°å½¢æ„ŸçŸ¥åŒè¶³è¿åŠ¨çš„åˆ†å±‚ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é›†æˆäº†é™ç»´æ„ŸçŸ¥è¡¨ç¤ºï¼Œä»¥å¢å¼ºåŸºäºå¼ºåŒ–å­¦ä¹ (RL)çš„é«˜çº§ç­–ç•¥ï¼Œä»è€Œå®ç°å®æ—¶æ­¥æ€ç”Ÿæˆã€‚ä¸ç«¯åˆ°ç«¯æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨å·ç§¯å˜åˆ†è‡ªç¼–ç å™¨(CNN-VAE)è¿›è¡Œæ½œåœ¨åœ°å½¢ç¼–ç ï¼Œå¹¶ç»“åˆé™é˜¶æœºå™¨äººåŠ¨åŠ›å­¦ï¼Œé€šè¿‡ç´§å‡‘çŠ¶æ€ä¼˜åŒ–è¿åŠ¨å†³ç­–è¿‡ç¨‹ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†æ½œåœ¨ç©ºé—´ç»´åº¦å¯¹å­¦ä¹ æ•ˆç‡å’Œç­–ç•¥é²æ£’æ€§çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è¯¥æ–¹æ³•æ‰©å±•åˆ°å†å²æ„ŸçŸ¥ï¼Œå°†æœ€è¿‘çš„åœ°å½¢è§‚æµ‹åºåˆ—çº³å…¥æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æé«˜é²æ£’æ€§ã€‚ä¸ºäº†è§£å†³å®é™…å¯è¡Œæ€§é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è’¸é¦æ–¹æ³•ï¼Œç›´æ¥ä»æ·±åº¦ç›¸æœºå›¾åƒä¸­å­¦ä¹ æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ¯”è¾ƒæ¨¡æ‹Ÿå’ŒçœŸå®ä¼ æ„Ÿå™¨æ•°æ®æä¾›åˆæ­¥çš„ç¡¬ä»¶éªŒè¯ã€‚æˆ‘ä»¬ä½¿ç”¨é«˜ä¿çœŸAgility Robotics (AR)æ¨¡æ‹Ÿå™¨è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«çœŸå®çš„ä¼ æ„Ÿå™¨å™ªå£°ã€çŠ¶æ€ä¼°è®¡å’Œæ‰§è¡Œå™¨åŠ¨åŠ›å­¦ã€‚ç»“æœè¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œçªå‡ºäº†å…¶ç¡¬ä»¶éƒ¨ç½²çš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„ç«¯åˆ°ç«¯åŒè¶³è¿åŠ¨å­¦ä¹ æ–¹æ³•é€šå¸¸ç›´æ¥ä»é«˜ç»´ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå¦‚å›¾åƒæˆ–ç‚¹äº‘ï¼‰å­¦ä¹ æ§åˆ¶ç­–ç•¥ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é€‚åº”å¤æ‚åœ°å½¢ã€‚æ­¤å¤–ï¼Œç›´æ¥å­¦ä¹ æ§åˆ¶ç­–ç•¥å¿½ç•¥äº†æœºå™¨äººåŠ¨åŠ›å­¦çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨åœ°å½¢ä¿¡æ¯å¹¶é™ä½çŠ¶æ€ç©ºé—´ç»´åº¦çš„æ–¹æ³•ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å’Œç­–ç•¥é²æ£’æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨é™ç»´æ„ŸçŸ¥è¡¨ç¤ºæ¥ç®€åŒ–åœ°å½¢æ„ŸçŸ¥åŒè¶³è¿åŠ¨å­¦ä¹ é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä½¿ç”¨å·ç§¯å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCNN-VAEï¼‰å°†é«˜ç»´åœ°å½¢ä¿¡æ¯ç¼–ç åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ï¼Œç„¶ååˆ©ç”¨é™é˜¶æœºå™¨äººåŠ¨åŠ›å­¦æ¨¡å‹æ¥æè¿°æœºå™¨äººçš„è¿åŠ¨çŠ¶æ€ã€‚æœ€åï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•å­¦ä¹ ä¸€ä¸ªé«˜çº§ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä»¥ä½ç»´åœ°å½¢è¡¨ç¤ºå’Œæœºå™¨äººçŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºæ§åˆ¶æŒ‡ä»¤ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) æ„ŸçŸ¥æ¨¡å—ï¼šä½¿ç”¨CNN-VAEå°†æ·±åº¦ç›¸æœºå›¾åƒç¼–ç ä¸ºä½ç»´æ½œåœ¨å‘é‡ï¼Œè¯¥å‘é‡è¡¨ç¤ºåœ°å½¢ä¿¡æ¯ã€‚2) åŠ¨åŠ›å­¦æ¨¡å—ï¼šä½¿ç”¨é™é˜¶æœºå™¨äººåŠ¨åŠ›å­¦æ¨¡å‹æ¥æè¿°æœºå™¨äººçš„è¿åŠ¨çŠ¶æ€ã€‚3) æ§åˆ¶æ¨¡å—ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•å­¦ä¹ ä¸€ä¸ªé«˜çº§ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä»¥åœ°å½¢æ½œåœ¨å‘é‡å’Œæœºå™¨äººçŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºæ§åˆ¶æŒ‡ä»¤ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆé€šè¿‡æ·±åº¦ç›¸æœºè·å–åœ°å½¢ä¿¡æ¯ï¼Œç„¶åä½¿ç”¨CNN-VAEå°†å…¶ç¼–ç ä¸ºä½ç»´æ½œåœ¨å‘é‡ï¼Œæ¥ç€å°†è¯¥å‘é‡å’Œæœºå™¨äººçŠ¶æ€è¾“å…¥åˆ°å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¸­ï¼Œç­–ç•¥è¾“å‡ºæ§åˆ¶æŒ‡ä»¤ï¼Œæ§åˆ¶æœºå™¨äººè¿åŠ¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥è®ºæ–‡çš„å…³é”®åˆ›æ–°åœ¨äºå°†é™ç»´æ„ŸçŸ¥è¡¨ç¤ºä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œç”¨äºåœ°å½¢æ„ŸçŸ¥åŒè¶³è¿åŠ¨å­¦ä¹ ã€‚ä¸ç«¯åˆ°ç«¯æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé™ä½çŠ¶æ€ç©ºé—´ç»´åº¦ï¼Œæé«˜å­¦ä¹ æ•ˆç‡å’Œç­–ç•¥é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§è’¸é¦æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥ä»æ·±åº¦ç›¸æœºå›¾åƒä¸­å­¦ä¹ æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œé¿å…äº†å¯¹åœ°å½¢è¿›è¡Œæ˜¾å¼å»ºæ¨¡ã€‚

**å…³é”®è®¾è®¡**ï¼šCNN-VAEçš„ç½‘ç»œç»“æ„åŒ…æ‹¬å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚ï¼Œç”¨äºå°†æ·±åº¦ç›¸æœºå›¾åƒç¼–ç ä¸ºä½ç»´æ½œåœ¨å‘é‡ã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•é‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼Œå¥–åŠ±å‡½æ•°åŒ…æ‹¬å‰è¿›é€Ÿåº¦ã€ç¨³å®šæ€§ã€èƒ½é‡æ¶ˆè€—ç­‰æŒ‡æ ‡ã€‚ä¸ºäº†æé«˜ç­–ç•¥çš„é²æ£’æ€§ï¼Œè¯¥è®ºæ–‡è¿˜å¼•å…¥äº†å†å²æ„ŸçŸ¥æœºåˆ¶ï¼Œå°†æœ€è¿‘çš„åœ°å½¢è§‚æµ‹åºåˆ—çº³å…¥æ½œåœ¨è¡¨ç¤ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

è¯¥è®ºæ–‡åœ¨Agility Roboticsæ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ åœ°å½¢æ„ŸçŸ¥çš„è¿åŠ¨ç­–ç•¥ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚åœ°å½¢ä¸Šçš„è¡Œèµ°é€Ÿåº¦å’Œç¨³å®šæ€§å‡æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜é€šè¿‡æ¯”è¾ƒæ¨¡æ‹Ÿå’ŒçœŸå®ä¼ æ„Ÿå™¨æ•°æ®ï¼Œåˆæ­¥éªŒè¯äº†è¯¥æ–¹æ³•åœ¨çœŸå®ç¡¬ä»¶ä¸Šçš„å¯è¡Œæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºå„ç§éœ€è¦åŒè¶³æœºå™¨äººè¿›è¡Œå¤æ‚åœ°å½¢è¡Œèµ°çš„åœºæ™¯ï¼Œä¾‹å¦‚æœæ•‘ã€å‹˜æ¢ã€ç‰©æµç­‰ã€‚é€šè¿‡å­¦ä¹ åœ°å½¢æ„ŸçŸ¥çš„è¿åŠ¨ç­–ç•¥ï¼ŒåŒè¶³æœºå™¨äººå¯ä»¥æ›´å¥½åœ°é€‚åº”å„ç§å¤æ‚åœ°å½¢ï¼Œæé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æœºå™¨äººï¼Œä¾‹å¦‚å››è¶³æœºå™¨äººå’Œäººå½¢æœºå™¨äººã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.

