---
layout: default
title: RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics
---

# RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics

**arXiv**: [2512.13660v1](https://arxiv.org/abs/2512.13660) | [PDF](https://arxiv.org/pdf/2512.13660.pdf)

**ä½œè€…**: Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-15

**å¤‡æ³¨**: Project page: https://zhoues.github.io/RoboTracer

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RoboTracerï¼šåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡åž‹æŽ¨ç†å®žçŽ°æœºå™¨äººç©ºé—´è½¨è¿¹è¿½è¸ª**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæŽ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `æœºå™¨äºº` `è§†è§‰-è¯­è¨€æ¨¡åž‹` `ç©ºé—´æŽ¨ç†` `è½¨è¿¹è¿½è¸ª` `å¼ºåŒ–å­¦ä¹ ` `3Dæ„ŸçŸ¥` `åº¦é‡å­¦ä¹ `

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. çŽ°æœ‰æœºå™¨äººç©ºé—´è½¨è¿¹è¿½è¸ªæ–¹æ³•éš¾ä»¥è¿›è¡Œå¤šæ­¥éª¤åº¦é‡æŽ¨ç†ï¼Œä¸”ç¼ºä¹å¯¹å¤æ‚ç©ºé—´æŒ‡ä»£å’ŒçœŸå®žä¸–ç•Œåº¦é‡æµ‹é‡çš„èƒ½åŠ›ã€‚
2. RoboTraceré€šè¿‡é€šç”¨ç©ºé—´ç¼–ç å™¨å’Œå›žå½’ç›‘ç£è§£ç å™¨ï¼Œä»¥åŠåº¦é‡æ•æ„Ÿçš„è¿‡ç¨‹å¥–åŠ±å¼ºåŒ–å¾®è°ƒï¼Œæå‡äº†ç©ºé—´ç†è§£å’ŒæŽ¨ç†èƒ½åŠ›ã€‚
3. å®žéªŒè¡¨æ˜Žï¼ŒRoboTraceråœ¨ç©ºé—´ç†è§£ã€æµ‹é‡å’ŒæŒ‡ä»£æ–¹é¢ä¼˜äºŽåŸºçº¿ï¼Œå¹¶åœ¨TraceSpatial-Benchä¸Šå¤§å¹…è¶…è¶ŠçŽ°æœ‰SOTAæ¨¡åž‹ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºRoboTracerï¼Œä¸€ä¸ª3Dæ„ŸçŸ¥çš„è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼Œæ—¨åœ¨æå‡æœºå™¨äººç©ºé—´è½¨è¿¹è¿½è¸ªèƒ½åŠ›ã€‚è¯¥æ¨¡åž‹é€šè¿‡é€šç”¨ç©ºé—´ç¼–ç å™¨å’Œå›žå½’ç›‘ç£è§£ç å™¨å®žçŽ°3Dç©ºé—´æŒ‡ä»£å’Œæµ‹é‡ï¼Œä»Žè€Œå¢žå¼ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æœŸé—´çš„å°ºåº¦æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼ŒRoboTraceré€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å’Œåº¦é‡æ•æ„Ÿçš„è¿‡ç¨‹å¥–åŠ±ï¼Œæå‡å¤šæ­¥åº¦é‡æŽ¨ç†èƒ½åŠ›ï¼Œç›‘ç£å…³é”®çš„ä¸­é—´æ„ŸçŸ¥çº¿ç´¢ï¼Œä»¥å‡†ç¡®ç”Ÿæˆç©ºé—´è½¨è¿¹ã€‚ä¸ºäº†æ”¯æŒSFTå’ŒRFTè®­ç»ƒï¼Œæœ¬æ–‡æž„å»ºäº†TraceSpatialï¼Œä¸€ä¸ªåŒ…å«3000ä¸‡QAå¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¦†ç›–å®¤å¤–/å®¤å†…/æ¡Œé¢åœºæ™¯ï¼Œå¹¶æ”¯æŒå¤æ‚çš„æŽ¨ç†è¿‡ç¨‹ï¼ˆæœ€å¤š9æ­¥ï¼‰ã€‚åŒæ—¶ï¼Œæå‡ºäº†TraceSpatial-Benchï¼Œä¸€ä¸ªç”¨äºŽè¯„ä¼°ç©ºé—´è½¨è¿¹è¿½è¸ªçš„åŸºå‡†ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒRoboTraceråœ¨ç©ºé—´ç†è§£ã€æµ‹é‡å’ŒæŒ‡ä»£æ–¹é¢è¶…è¶Šäº†åŸºçº¿æ¨¡åž‹ï¼Œå¹³å‡æˆåŠŸçŽ‡ä¸º79.1%ï¼Œå¹¶åœ¨TraceSpatial-Benchä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼Œè¶…è¶ŠGemini-2.5-Pro 36%çš„å‡†ç¡®çŽ‡ã€‚RoboTracerå¯ä»¥ä¸Žå„ç§æŽ§åˆ¶ç­–ç•¥é›†æˆï¼Œåœ¨æ‚ä¹±çš„çœŸå®žåœºæ™¯ä¸­æ‰§è¡Œå„ç§æœºå™¨äººï¼ˆUR5ã€G1äººå½¢æœºå™¨äººï¼‰ä¸Šçš„é•¿æ—¶ç¨‹åŠ¨æ€ä»»åŠ¡ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨äººç©ºé—´è½¨è¿¹è¿½è¸ªé—®é¢˜ï¼Œå³è®©æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­æ ¹æ®æŒ‡ä»¤è¿›è¡Œç²¾ç¡®çš„ç©ºé—´å®šä½å’Œç§»åŠ¨ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºŽéš¾ä»¥å¤„ç†å¤šæ­¥éª¤çš„åº¦é‡æŽ¨ç†ï¼Œæ— æ³•å‡†ç¡®ç†è§£å¤æ‚çš„ç©ºé—´æŒ‡ä»£ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹çœŸå®žä¸–ç•Œå°ºåº¦ä¿¡æ¯çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯¼è‡´è½¨è¿¹è¿½è¸ªçš„ç²¾åº¦å’Œé²æ£’æ€§ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯æž„å»ºä¸€ä¸ª3Dæ„ŸçŸ¥çš„è§†è§‰-è¯­è¨€æ¨¡åž‹RoboTracerï¼Œè¯¥æ¨¡åž‹èƒ½å¤ŸåŒæ—¶è¿›è¡Œ3Dç©ºé—´æŒ‡ä»£å’Œæµ‹é‡ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼æå‡å¤šæ­¥åº¦é‡æŽ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¢žå¼ºå°ºåº¦æ„ŸçŸ¥ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ç›‘ç£ä¸­é—´æ„ŸçŸ¥çº¿ç´¢ï¼Œä»Žè€Œæ›´å‡†ç¡®åœ°ç”Ÿæˆç©ºé—´è½¨è¿¹ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šRoboTracerçš„æ•´ä½“æ¡†æž¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š1) é€šç”¨ç©ºé—´ç¼–ç å™¨ï¼šç”¨äºŽæå–åœºæ™¯çš„3Dç©ºé—´ç‰¹å¾ï¼›2) å›žå½’ç›‘ç£è§£ç å™¨ï¼šç”¨äºŽè¿›è¡Œ3Dç©ºé—´æŒ‡ä»£å’Œæµ‹é‡ï¼Œå¹¶å¢žå¼ºå°ºåº¦æ„ŸçŸ¥ï¼›3) å¼ºåŒ–å¾®è°ƒæ¨¡å—ï¼šä½¿ç”¨åº¦é‡æ•æ„Ÿçš„è¿‡ç¨‹å¥–åŠ±ï¼Œæå‡å¤šæ­¥åº¦é‡æŽ¨ç†èƒ½åŠ›ã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆé€šè¿‡è§†è§‰è¾“å…¥å’Œè¯­è¨€æŒ‡ä»¤ï¼Œåˆ©ç”¨ç©ºé—´ç¼–ç å™¨å’Œè§£ç å™¨è¿›è¡Œåˆæ­¥çš„ç©ºé—´ç†è§£å’Œæµ‹é‡ï¼Œç„¶åŽé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸æ–­ä¼˜åŒ–æ¨¡åž‹çš„æŽ¨ç†èƒ½åŠ›ï¼Œæœ€ç»ˆç”Ÿæˆç²¾ç¡®çš„ç©ºé—´è½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºŽå°†3Dç©ºé—´æ„ŸçŸ¥å’Œåº¦é‡æŽ¨ç†èƒ½åŠ›èžå…¥åˆ°è§†è§‰-è¯­è¨€æ¨¡åž‹ä¸­ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRoboTracerèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£ç©ºé—´å…³ç³»ï¼Œè¿›è¡Œç²¾ç¡®çš„åº¦é‡æµ‹é‡ï¼Œå¹¶è¿›è¡Œå¤šæ­¥éª¤çš„æŽ¨ç†ï¼Œä»Žè€Œå®žçŽ°æ›´é²æ£’å’Œç²¾ç¡®çš„ç©ºé—´è½¨è¿¹è¿½è¸ªã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨SFTé˜¶æ®µï¼Œä½¿ç”¨å›žå½’æŸå¤±ç›‘ç£è§£ç å™¨çš„è¾“å‡ºï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹3Dç©ºé—´åæ ‡å’Œè·ç¦»ã€‚åœ¨RFTé˜¶æ®µï¼Œè®¾è®¡äº†åº¦é‡æ•æ„Ÿçš„è¿‡ç¨‹å¥–åŠ±ï¼Œå¥–åŠ±æ¨¡åž‹åœ¨æ¯ä¸€æ­¥æŽ¨ç†ä¸­äº§ç”Ÿçš„ä¸­é—´æ„ŸçŸ¥çº¿ç´¢çš„å‡†ç¡®æ€§ï¼Œä¾‹å¦‚ï¼Œä¸­é—´æ­¥éª¤çš„å®šä½ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¿˜æž„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†TraceSpatialï¼Œç”¨äºŽæ”¯æŒSFTå’ŒRFTçš„è®­ç»ƒã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

RoboTraceråœ¨TraceSpatial-Benchä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶ŠGemini-2.5-Pro 36%çš„å‡†ç¡®çŽ‡ã€‚åœ¨ç©ºé—´ç†è§£ã€æµ‹é‡å’ŒæŒ‡ä»£æ–¹é¢ï¼ŒRoboTracerä¹Ÿä¼˜äºŽå…¶ä»–åŸºçº¿æ¨¡åž‹ï¼Œå¹³å‡æˆåŠŸçŽ‡è¾¾åˆ°79.1%ã€‚æ­¤å¤–ï¼ŒRoboTracerèƒ½å¤Ÿä¸Žå¤šç§æœºå™¨äººå¹³å°ï¼ˆUR5ã€G1äººå½¢æœºå™¨äººï¼‰é›†æˆï¼Œå¹¶åœ¨çœŸå®žçš„ã€æ‚ä¹±çš„çŽ¯å¢ƒä¸­æ‰§è¡Œé•¿æ—¶ç¨‹åŠ¨æ€ä»»åŠ¡ï¼ŒéªŒè¯äº†å…¶åœ¨å®žé™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

RoboTraceråœ¨æœºå™¨äººå¯¼èˆªã€æ™ºèƒ½åˆ¶é€ ã€è‡ªåŠ¨é©¾é©¶ã€å®¶åº­æœåŠ¡ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå¯ä»¥ä½¿æœºå™¨äººåœ¨å¤æ‚çŽ¯å¢ƒä¸­æ›´å‡†ç¡®åœ°æ‰§è¡Œä»»åŠ¡ï¼Œä¾‹å¦‚ï¼Œåœ¨ä»“åº“ä¸­è¿›è¡Œè´§ç‰©æ‹£é€‰å’Œæ¬è¿ï¼Œåœ¨å®¶åº­çŽ¯å¢ƒä¸­è¿›è¡Œæ¸…æ´å’Œæ•´ç†ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­è¿›è¡Œè·¯å¾„è§„åˆ’å’Œé¿éšœã€‚è¯¥ç ”ç©¶çš„å®žé™…ä»·å€¼åœ¨äºŽæå‡äº†æœºå™¨äººçš„è‡ªä¸»æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ï¼Œæœªæ¥æœ‰æœ›æŽ¨åŠ¨æœºå™¨äººæŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸçš„æ™®åŠå’Œåº”ç”¨ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.

