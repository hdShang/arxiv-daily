---
layout: default
title: ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models
---

# ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.16517" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.16517v2</a>
  <a href="https://arxiv.org/pdf/2505.16517.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.16517v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.16517v2', 'ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, Zhongzhi Li, Rui Yan, Xiuying Chen

**åˆ†ç±»**: cs.RO, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-22 (æ›´æ–°: 2025-05-24)

**å¤‡æ³¨**: 14pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºManipLVM-R1ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ³›åŒ–ä¸é€‚åº”æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æœºå™¨äººæ“ä½œ` `å¼ºåŒ–å­¦ä¹ ` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç‰©ç†æ¨ç†` `æ³›åŒ–èƒ½åŠ›` `è‡ªåŠ¨åŒ–æŠ€æœ¯`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†æœºå™¨äººæ“ä½œçš„æ³›åŒ–èƒ½åŠ›å’Œåœ¨é¢†åŸŸå¤–åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚
2. æœ¬æ–‡æå‡ºManipLVM-R1ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¼˜åŒ–ä»»åŠ¡å¯¹é½çš„ç»“æœï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒManipLVM-R1åœ¨å…³é”®æ“ä½œå­ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç‰©ç†æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æœ€è¿‘é€šè¿‡åˆ©ç”¨è§†è§‰è¿›è¡Œåœºæ™¯æ„ŸçŸ¥å’Œè¯­è¨€è¿›è¡ŒæŒ‡ä»¤è·Ÿéšï¼Œæ¨åŠ¨äº†æœºå™¨äººæ“ä½œçš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨é¢†åŸŸå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œé™ä½äº†ç°å®ä¸–ç•Œçš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ManipLVM-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ›¿ä»£ä¼ ç»Ÿç›‘ç£ã€‚é€šè¿‡ç›´æ¥ä¼˜åŒ–ä¸ä»»åŠ¡å¯¹é½çš„ç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›å’Œç‰©ç†æ¨ç†ï¼ŒåŒæ—¶æ¶ˆé™¤äº†å¯¹æ˜‚è´µæ ‡æ³¨çš„ä¾èµ–ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œé’ˆå¯¹å…³é”®çš„æœºå™¨äººæ“ä½œå­ä»»åŠ¡ï¼šå¢å¼ºäº¤äº’åŒºåŸŸå®šä½çš„å¯ä¾›æ€§æ„ŸçŸ¥å¥–åŠ±å’Œç¡®ä¿åŠ¨ä½œè·¯å¾„ç‰©ç†åˆç†æ€§çš„è½¨è¿¹åŒ¹é…å¥–åŠ±ã€‚è¿™äº›å¥–åŠ±æä¾›äº†å³æ—¶åé¦ˆå¹¶æ–½åŠ ç©ºé—´é€»è¾‘çº¦æŸï¼Œé¼“åŠ±æ¨¡å‹è¶…è¶Šæµ…å±‚æ¨¡å¼åŒ¹é…ï¼Œå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„ç‰©ç†äº¤äº’æ¨ç†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººæ“ä½œæ–¹æ³•å¯¹æ˜‚è´µäººç±»æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œåœ¨é¢†åŸŸå¤–åœºæ™¯ä¸­çš„é€‚åº”æ€§å·®çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒManipLVM-R1ç›´æ¥ä¼˜åŒ–ä¸ä»»åŠ¡å¯¹é½çš„ç»“æœï¼Œå¢å¼ºæ¨¡å‹çš„ç‰©ç†æ¨ç†èƒ½åŠ›ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå¯ä¾›æ€§æ„ŸçŸ¥å¥–åŠ±æ¨¡å—å’Œè½¨è¿¹åŒ¹é…å¥–åŠ±æ¨¡å—ã€‚å‰è€…ç”¨äºå¢å¼ºäº¤äº’åŒºåŸŸçš„å®šä½ï¼Œåè€…ç¡®ä¿åŠ¨ä½œè·¯å¾„çš„ç‰©ç†åˆç†æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šManipLVM-R1çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ›¿ä»£ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ ï¼Œåˆ©ç”¨å³æ—¶åé¦ˆå’Œç©ºé—´é€»è¾‘çº¦æŸæ¥ä¿ƒè¿›æ·±å±‚æ¬¡çš„ç‰©ç†äº¤äº’æ¨ç†ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„æµ…å±‚æ¨¡å¼åŒ¹é…å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ä¸¤ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œåˆ†åˆ«é’ˆå¯¹å¯ä¾›æ€§æ„ŸçŸ¥å’Œè½¨è¿¹åŒ¹é…ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ‰§è¡Œæ“ä½œæ—¶èƒ½å¤Ÿè·å¾—æœ‰æ•ˆçš„åé¦ˆå¹¶è¿›è¡Œåˆç†çš„ç‰©ç†æ¨ç†ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒManipLVM-R1åœ¨å¤šä¸ªå…³é”®æ“ä½œå­ä»»åŠ¡ä¸Šç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ç‰©ç†æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨åŒ–åˆ¶é€ å’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ“ä½œèƒ½åŠ›ï¼ŒManipLVM-R1èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æé«˜æœºå™¨äººå¯¹å¤šæ ·åŒ–ä»»åŠ¡çš„é€‚åº”æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have recently advanced robotic manipulation by leveraging vision for scene perception and language for instruction following. However, existing methods rely heavily on costly human-annotated training datasets, which limits their generalization and causes them to struggle in out-of-domain (OOD) scenarios, reducing real-world adaptability. To address these challenges, we propose ManipLVM-R1, a novel reinforcement learning framework that replaces traditional supervision with Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing for task-aligned outcomes, our method enhances generalization and physical reasoning while removing the dependence on costly annotations. Specifically, we design two rule-based reward functions targeting key robotic manipulation subtasks: an Affordance Perception Reward to enhance localization of interaction regions, and a Trajectory Match Reward to ensure the physical plausibility of action paths. These rewards provide immediate feedback and impose spatial-logical constraints, encouraging the model to go beyond shallow pattern matching and instead learn deeper, more systematic reasoning about physical interactions.

