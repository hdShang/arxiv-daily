---
layout: default
title: Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)
---

# Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.16394" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.16394v2</a>
  <a href="https://arxiv.org/pdf/2505.16394.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.16394v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.16394v2', 'Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan

**åˆ†ç±»**: cs.RO, cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-22 (æ›´æ–°: 2025-10-25)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRaw2Driveä»¥è§£å†³ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­çš„è®­ç»ƒå›°éš¾é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶` `æ¨¡å‹é©±åŠ¨å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `CARLA` `æ™ºèƒ½äº¤é€š` `æœºå™¨äººå¯¼èˆª`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­é¢ä¸´è®­ç»ƒå›°éš¾å’Œæ€§èƒ½é™åˆ¶ï¼Œå¼ºåŒ–å­¦ä¹ çš„åº”ç”¨å°šæœªæˆç†Ÿã€‚
2. Raw2Driveé€šè¿‡è®¾è®¡åŒæµæ¨¡å‹ï¼Œç»“åˆç‰¹æƒä¸–ç•Œæ¨¡å‹å’ŒåŸå§‹ä¼ æ„Ÿå™¨ä¸–ç•Œæ¨¡å‹ï¼Œè§£å†³äº†è®­ç»ƒä¸­çš„ä¿¡æ¯ä¸ä¸€è‡´é—®é¢˜ã€‚
3. è¯¥æ–¹æ³•åœ¨CARLAå¹³å°ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæˆä¸ºå”¯ä¸€åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶çš„æ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç¼“è§£æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ä¸­å›ºæœ‰çš„å› æœæ··æ·†å’Œåˆ†å¸ƒè½¬ç§»ã€‚ç„¶è€Œï¼Œå°†RLåº”ç”¨äºç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ï¼ˆE2E-ADï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ï¼Œä¸»è¦ç”±äºå…¶è®­ç»ƒéš¾åº¦ï¼Œè€ŒILåœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œä»æ˜¯ä¸»æµèŒƒå¼ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæµæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•Raw2Driveï¼Œé€šè¿‡è®¾è®¡ç‰¹å®šçš„ä¸–ç•Œæ¨¡å‹å’Œå¼•å¯¼æœºåˆ¶ï¼Œç»“åˆåŸå§‹ä¼ æ„Ÿå™¨æ•°æ®å’Œç‰¹æƒä¿¡æ¯ï¼Œæœ‰æ•ˆæå‡äº†è®­ç»ƒæ•ˆæœã€‚Raw2Driveåœ¨CARLA Leaderboard 2.0å’ŒBench2Driveä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­å¼ºåŒ–å­¦ä¹ åº”ç”¨çš„è®­ç»ƒå›°éš¾ï¼Œç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨å› æœæ¨ç†å’Œåˆ†å¸ƒè½¬ç§»æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRaw2Driveé‡‡ç”¨åŒæµæ¨¡å‹ï¼Œé¦–å…ˆè®­ç»ƒä¸€ä¸ªåŸºäºç‰¹æƒä¿¡æ¯çš„ä¸–ç•Œæ¨¡å‹ï¼Œç„¶åé€šè¿‡å¼•å¯¼æœºåˆ¶è®­ç»ƒåŸå§‹ä¼ æ„Ÿå™¨ä¸–ç•Œæ¨¡å‹ï¼Œä»¥ç¡®ä¿ä¸¤è€…ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç‰¹æƒä¸–ç•Œæ¨¡å‹å’ŒåŸå§‹ä¼ æ„Ÿå™¨ä¸–ç•Œæ¨¡å‹ã€‚ç‰¹æƒæ¨¡å‹ç”¨äºåˆæ­¥è§„åˆ’ï¼Œè€ŒåŸå§‹æ¨¡å‹åˆ™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆç‰¹æƒæ¨¡å‹çš„çŸ¥è¯†è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šRaw2Driveçš„åˆ›æ–°åœ¨äºå¼•å…¥äº†å¼•å¯¼æœºåˆ¶ï¼Œç¡®ä¿åŸå§‹ä¼ æ„Ÿå™¨æ¨¡å‹ä¸ç‰¹æƒæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ï¼Œè¿™ä¸€è®¾è®¡æ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥å¹³è¡¡ä¸¤ä¸ªæ¨¡å‹çš„è®­ç»ƒï¼ŒåŒæ—¶åœ¨ç½‘ç»œç»“æ„ä¸Šç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’å’Œåˆ©ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨CARLA Leaderboard 2.0å’ŒBench2Driveä¸Šï¼ŒRaw2Driveå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æ™ºèƒ½äº¤é€šç³»ç»Ÿå’Œæœºå™¨äººå¯¼èˆªç­‰ã€‚é€šè¿‡æå‡ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ï¼ŒRaw2Driveæœ‰æœ›æ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å•†ä¸šåŒ–å’Œæ™®åŠï¼Œæå‡äº¤é€šå®‰å…¨å’Œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement Learning (RL) can mitigate the causal confusion and distribution shift inherent to imitation learning (IL). However, applying RL to end-to-end autonomous driving (E2E-AD) remains an open problem for its training difficulty, and IL is still the mainstream paradigm in both academia and industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated promising results in neural planning; however, these methods typically require privileged information as input rather than raw sensor data. We fill this gap by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently train an auxiliary privileged world model paired with a neural planner that uses privileged information as input. Subsequently, we introduce a raw sensor world model trained via our proposed Guidance Mechanism, which ensures consistency between the raw sensor world model and the privileged world model during rollouts. Finally, the raw sensor world model combines the prior knowledge embedded in the heads of the privileged world model to effectively guide the training of the raw sensor policy. Raw2Drive is so far the only RL based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it achieves state-of-the-art performance.

