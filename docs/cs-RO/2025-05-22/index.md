---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-22
---

# cs.ROï¼ˆ2025-05-22ï¼‰

ğŸ“Š å…± **16** ç¯‡è®ºæ–‡
 | ğŸ”— **2** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (10 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ğŸ”—1)</a>
<a href="#æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics" class="interest-badge">æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1)</a>
<a href="#æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation" class="interest-badge">æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (10 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250516478v2-unified-multi-rate-model-predictive-control-for-a-jet-powered-humano.html">Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot</a></td>
  <td>æå‡ºç»Ÿä¸€å¤šé€Ÿç‡æ¨¡å‹é¢„æµ‹æ§åˆ¶ä»¥ä¼˜åŒ–å–·æ°”åŠ¨åŠ›äººå½¢æœºå™¨äºº</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">MPC</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16478v2" data-paper-url="./papers/250516478v2-unified-multi-rate-model-predictive-control-for-a-jet-powered-humano.html" onclick="toggleFavorite(this, '2505.16478v2', 'Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250516517v2-maniplvm-r1-reinforcement-learning-for-reasoning-in-embodied-manipul.html">ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models</a></td>
  <td>æå‡ºManipLVM-R1ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„æ³›åŒ–ä¸é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">affordance</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16517v2" data-paper-url="./papers/250516517v2-maniplvm-r1-reinforcement-learning-for-reasoning-in-embodied-manipul.html" onclick="toggleFavorite(this, '2505.16517v2', 'ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250516547v2-find-the-fruit-zero-shot-sim2real-rl-for-occlusion-aware-plant-manip.html">Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation</a></td>
  <td>æå‡ºé›¶æ ·æœ¬Sim2Realå¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³æ¤ç‰©é®æŒ¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">sim2real</span> <span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16547v2" data-paper-url="./papers/250516547v2-find-the-fruit-zero-shot-sim2real-rl-for-occlusion-aware-plant-manip.html" onclick="toggleFavorite(this, '2505.16547v2', 'Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250516289v2-taccompress-a-benchmark-for-multi-point-tactile-data-compression-in-.html">TacCompress: A Benchmark for Multi-Point Tactile Data Compression in Dexterous Hand</a></td>
  <td>æå‡ºTacCompressä»¥è§£å†³å¤šç‚¹è§¦è§‰æ•°æ®å‹ç¼©é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">dexterous hand</span> <span class="paper-tag">dexterous manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16289v2" data-paper-url="./papers/250516289v2-taccompress-a-benchmark-for-multi-point-tactile-data-compression-in-.html" onclick="toggleFavorite(this, '2505.16289v2', 'TacCompress: A Benchmark for Multi-Point Tactile Data Compression in Dexterous Hand')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250517209v1-lilodriver-a-lifelong-learning-framework-for-closed-loop-motion-plan.html">LiloDriver: A Lifelong Learning Framework for Closed-loop Motion Planning in Long-tail Autonomous Driving Scenarios</a></td>
  <td>æå‡ºLiloDriveræ¡†æ¶ä»¥è§£å†³é•¿å°¾è‡ªä¸»é©¾é©¶åœºæ™¯ä¸­çš„è¿åŠ¨è§„åˆ’é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">motion planning</span> <span class="paper-tag">large language model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.17209v1" data-paper-url="./papers/250517209v1-lilodriver-a-lifelong-learning-framework-for-closed-loop-motion-plan.html" onclick="toggleFavorite(this, '2505.17209v1', 'LiloDriver: A Lifelong Learning Framework for Closed-loop Motion Planning in Long-tail Autonomous Driving Scenarios')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250516969v3-3d-equivariant-visuomotor-policy-learning-via-spherical-projection.html">3D Equivariant Visuomotor Policy Learning via Spherical Projection</a></td>
  <td>æå‡ºå›¾åƒåˆ°çƒé¢ç­–ç•¥ä»¥è§£å†³å•ç›®RGBè¾“å…¥çš„æœºå™¨äººæ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">diffusion policy</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16969v3" data-paper-url="./papers/250516969v3-3d-equivariant-visuomotor-policy-learning-via-spherical-projection.html" onclick="toggleFavorite(this, '2505.16969v3', '3D Equivariant Visuomotor Policy Learning via Spherical Projection')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250516912v2-uav-see-ugv-do-aerial-imagery-and-virtual-teach-enabling-zero-shot-g.html">UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot Ground Vehicle Repeat</a></td>
  <td>æå‡ºVirT&Ræ¡†æ¶ä»¥è§£å†³GPSç¼ºå¤±ä¸‹çš„UGVè‡ªä¸»å¯¼èˆªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">NeRF</span> <span class="paper-tag">neural radiance field</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16912v2" data-paper-url="./papers/250516912v2-uav-see-ugv-do-aerial-imagery-and-virtual-teach-enabling-zero-shot-g.html" onclick="toggleFavorite(this, '2505.16912v2', 'UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot Ground Vehicle Repeat')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250516453v1-spinewave-harnessing-fish-rigid-flexible-spinal-kinematics-for-enhan.html">SpineWave: Harnessing Fish Rigid-Flexible Spinal Kinematics for Enhancing Biomimetic Robotic Locomotion</a></td>
  <td>æå‡ºSpineWaveä»¥è§£å†³æ°´ä¸‹æœºå™¨äººçµæ´»æ€§ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16453v1" data-paper-url="./papers/250516453v1-spinewave-harnessing-fish-rigid-flexible-spinal-kinematics-for-enhan.html" onclick="toggleFavorite(this, '2505.16453v1', 'SpineWave: Harnessing Fish Rigid-Flexible Spinal Kinematics for Enhancing Biomimetic Robotic Locomotion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250516196v3-sem-enhancing-spatial-understanding-for-robust-robot-manipulation.html">SEM: Enhancing Spatial Understanding for Robust Robot Manipulation</a></td>
  <td>æå‡ºSEMæ¨¡å‹ä»¥è§£å†³æœºå™¨äººæ“ä½œä¸­çš„ç©ºé—´ç†è§£é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16196v3" data-paper-url="./papers/250516196v3-sem-enhancing-spatial-understanding-for-robust-robot-manipulation.html" onclick="toggleFavorite(this, '2505.16196v3', 'SEM: Enhancing Spatial Understanding for Robust Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>10</td>
  <td><a href="./papers/250516249v2-manipulating-elasto-plastic-objects-with-3d-occupancy-and-learning-b.html">Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control</a></td>
  <td>æå‡ºåŸºäº3Då ç”¨å’Œå­¦ä¹ çš„é¢„æµ‹æ§åˆ¶ä»¥è§£å†³å¼¹å¡‘æ€§ç‰©ä½“æ“æ§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16249v2" data-paper-url="./papers/250516249v2-manipulating-elasto-plastic-objects-with-3d-occupancy-and-learning-b.html" onclick="toggleFavorite(this, '2505.16249v2', 'Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250517295v1-scanbot-towards-intelligent-surface-scanning-in-embodied-robotic-sys.html">ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems</a></td>
  <td>æå‡ºScanBotä»¥è§£å†³æœºå™¨äººç³»ç»Ÿä¸­çš„é«˜ç²¾åº¦è¡¨é¢æ‰«æé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">VLA</span> <span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.17295v1" data-paper-url="./papers/250517295v1-scanbot-towards-intelligent-surface-scanning-in-embodied-robotic-sys.html" onclick="toggleFavorite(this, '2505.17295v1', 'ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>12</td>
  <td><a href="./papers/250516498v1-human-like-semantic-navigation-for-autonomous-driving-using-knowledg.html">Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models</a></td>
  <td>æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å¯¼èˆªæ–¹æ³•ä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„é€‚åº”æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16498v1" data-paper-url="./papers/250516498v1-human-like-semantic-navigation-for-autonomous-driving-using-knowledg.html" onclick="toggleFavorite(this, '2505.16498v1', 'Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>13</td>
  <td><a href="./papers/250516394v2-raw2drive-reinforcement-learning-with-aligned-world-models-for-end-t.html">Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)</a></td>
  <td>æå‡ºRaw2Driveä»¥è§£å†³ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­çš„è®­ç»ƒå›°éš¾é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">imitation learning</span> <span class="paper-tag">world model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16394v2" data-paper-url="./papers/250516394v2-raw2drive-reinforcement-learning-with-aligned-world-models-for-end-t.html" onclick="toggleFavorite(this, '2505.16394v2', 'Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>14</td>
  <td><a href="./papers/250516377v1-vl-safe-vision-language-guided-safety-aware-reinforcement-learning-w.html">VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving</a></td>
  <td>æå‡ºVL-SAFEä»¥è§£å†³è‡ªä¸»é©¾é©¶ä¸­çš„å®‰å…¨æ€§ä¸æ ·æœ¬æ•ˆç‡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">policy learning</span> <span class="paper-tag">world model</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16377v1" data-paper-url="./papers/250516377v1-vl-safe-vision-language-guided-safety-aware-reinforcement-learning-w.html" onclick="toggleFavorite(this, '2505.16377v1', 'VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸‰ç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰-perception-semantics">ğŸ”¬ æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>15</td>
  <td><a href="./papers/250516726v3-d-lio-6dof-direct-lidar-inertial-odometry-based-on-simultaneous-trun.html">D-LIO: 6DoF Direct LiDAR-Inertial Odometry based on Simultaneous Truncated Distance Field Mapping</a></td>
  <td>æå‡ºD-LIOä»¥è§£å†³6DoFç›´æ¥LiDARæƒ¯æ€§æµ‹ç¨‹é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">LIO</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16726v3" data-paper-url="./papers/250516726v3-d-lio-6dof-direct-lidar-inertial-odometry-based-on-simultaneous-trun.html" onclick="toggleFavorite(this, '2505.16726v3', 'D-LIO: 6DoF Direct LiDAR-Inertial Odometry based on Simultaneous Truncated Distance Field Mapping')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…«ç‰©ç†åŠ¨ç”»-physics-based-animation">ğŸ”¬ æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>16</td>
  <td><a href="./papers/250516609v1-monitoring-electrostatic-adhesion-forces-via-acoustic-pressure.html">Monitoring Electrostatic Adhesion Forces via Acoustic Pressure</a></td>
  <td>æå‡ºåŸºäºå£°å‹ç›‘æµ‹é™ç”µé™„ç€åŠ›çš„æ–¹æ³•ä»¥è§£å†³ä¼ æ„Ÿå™¨å¤æ‚æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">PULSE</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.16609v1" data-paper-url="./papers/250516609v1-monitoring-electrostatic-adhesion-forces-via-acoustic-pressure.html" onclick="toggleFavorite(this, '2505.16609v1', 'Monitoring Electrostatic Adhesion Forces via Acoustic Pressure')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)