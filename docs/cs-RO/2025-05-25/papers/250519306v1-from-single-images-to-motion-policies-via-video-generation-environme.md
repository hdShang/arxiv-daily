---
layout: default
title: From Single Images to Motion Policies via Video-Generation Environment Representations
---

# From Single Images to Motion Policies via Video-Generation Environment Representations

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19306" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19306v1</a>
  <a href="https://arxiv.org/pdf/2505.19306.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19306v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19306v1', 'From Single Images to Motion Policies via Video-Generation Environment Representations')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Weiming Zhi, Ziyong Ma, Tianyi Zhang, Matthew Johnson-Roberson

**åˆ†ç±»**: cs.RO, cs.CV, cs.GR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-25

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºVGERæ¡†æ¶ä»¥è§£å†³å•å›¾åƒç”Ÿæˆè¿åŠ¨ç­–ç•¥é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†é¢‘ç”Ÿæˆ` `ç¯å¢ƒè¡¨ç¤º` `è¿åŠ¨ç­–ç•¥` `æ·±åº¦ä¼°è®¡` `è‡ªä¸»æœºå™¨äºº` `å¤šè§†å›¾æ•°æ®é›†` `å‡ ä½•ä¸€è‡´æ€§`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨ä»å•å›¾åƒæå–3Dç»“æ„æ—¶é¢ä¸´æ·±åº¦ä¼°è®¡çš„è¯¯å·®ï¼Œå¯¼è‡´è¿åŠ¨ç”Ÿæˆçš„æŒ‘æˆ˜ã€‚
2. æå‡ºçš„VGERæ¡†æ¶é€šè¿‡ç”Ÿæˆä¸è¾“å…¥å›¾åƒç›¸å…³çš„ç§»åŠ¨è§†é¢‘ï¼Œæ„å»ºç¯å¢ƒçš„å¤šè§†å›¾æ•°æ®é›†ä»¥æ”¹å–„è¿åŠ¨ç­–ç•¥ã€‚
3. åœ¨å¤šç§å®¤å†…å¤–ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ï¼ŒVGERå±•ç¤ºäº†å…¶ç”Ÿæˆçš„è¿åŠ¨èƒ½å¤Ÿå¹³æ»‘ä¸”å‡†ç¡®åœ°åæ˜ åœºæ™¯å‡ ä½•ç‰¹å¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªä¸»æœºå™¨äººé€šå¸¸éœ€è¦æ„å»ºå‘¨å›´ç¯å¢ƒçš„è¡¨ç¤ºï¼Œå¹¶æ ¹æ®ç¯å¢ƒçš„å‡ ä½•å½¢çŠ¶è°ƒæ•´å…¶è¿åŠ¨ã€‚æœ¬æ–‡è§£å†³äº†ä»å•ä¸ªè¾“å…¥RGBå›¾åƒæ„å»ºæ— ç¢°æ’è¿åŠ¨ç”Ÿæˆç­–ç•¥æ¨¡å‹çš„é—®é¢˜ã€‚å°½ç®¡å•å›¾åƒæ·±åº¦ä¼°è®¡æŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œä½†å…¶è¾“å‡ºåœ¨ä¸‹æ¸¸è¿åŠ¨ç”Ÿæˆä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§è§†é¢‘ç”Ÿæˆç¯å¢ƒè¡¨ç¤ºæ¡†æ¶ï¼ˆVGERï¼‰ï¼Œåˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹ç”Ÿæˆä¸è¾“å…¥å›¾åƒç›¸å…³çš„ç§»åŠ¨æ‘„åƒæœºè§†é¢‘ã€‚é€šè¿‡å°†è§†é¢‘å¸§è¾“å…¥é¢„è®­ç»ƒçš„3DåŸºç¡€æ¨¡å‹ï¼Œç”Ÿæˆç¨ å¯†ç‚¹äº‘ï¼Œå¹¶å¼•å…¥å¤šå°ºåº¦å™ªå£°æ–¹æ³•è®­ç»ƒç¯å¢ƒç»“æ„çš„éšå¼è¡¨ç¤ºï¼Œæœ€ç»ˆæ„å»ºç¬¦åˆå‡ ä½•ç»“æ„çš„è¿åŠ¨ç”Ÿæˆæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒVGERèƒ½å¤Ÿä»å•ä¸€RGBè¾“å…¥å›¾åƒç”Ÿæˆå¹³æ»‘çš„è¿åŠ¨ï¼Œå……åˆ†è€ƒè™‘åœºæ™¯çš„å‡ ä½•ç‰¹å¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä»å•ä¸ªRGBå›¾åƒç”Ÿæˆæ— ç¢°æ’è¿åŠ¨ç­–ç•¥çš„é—®é¢˜ã€‚ç°æœ‰çš„æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨è¿åŠ¨ç”Ÿæˆä¸­å­˜åœ¨å½¢çŠ¶è¯¯å·®ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¿åŠ¨ä¸å¤Ÿå¯é ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„VGERæ¡†æ¶åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ç”Ÿæˆä¸è¾“å…¥å›¾åƒç›¸å…³çš„åŠ¨æ€è§†é¢‘ï¼Œä»è€Œåˆ›å»ºå¤šè§†å›¾æ•°æ®é›†ï¼Œè¿›è€Œæ”¹å–„è¿åŠ¨ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVGERçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œç”Ÿæˆä¸è¾“å…¥å›¾åƒç›¸å…³çš„ç§»åŠ¨è§†é¢‘ï¼›å…¶æ¬¡ï¼Œå°†è§†é¢‘å¸§è¾“å…¥é¢„è®­ç»ƒçš„3DåŸºç¡€æ¨¡å‹ä»¥ç”Ÿæˆç¨ å¯†ç‚¹äº‘ï¼›æœ€åï¼Œåˆ©ç”¨å¤šå°ºåº¦å™ªå£°æ–¹æ³•è®­ç»ƒéšå¼ç¯å¢ƒè¡¨ç¤ºï¼Œå¹¶æ„å»ºè¿åŠ¨ç”Ÿæˆæ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šVGERçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆè§†é¢‘ç”ŸæˆæŠ€æœ¯ä¸æ·±åº¦ä¼°è®¡ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨å•å›¾åƒæ·±åº¦ä¼°è®¡ä¸­çš„å±€é™æ€§ï¼Œæä¾›äº†ä¸€ç§æ–°çš„ç”Ÿæˆè¿åŠ¨ç­–ç•¥çš„æ–¹å¼ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šå°ºåº¦å™ªå£°æ–¹æ³•æ¥å¢å¼ºç¯å¢ƒç»“æ„çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°ä¼˜åŒ–è¿åŠ¨ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å…¶ä¸ç¯å¢ƒå‡ ä½•å½¢çŠ¶ç›¸ç¬¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒVGERåœ¨å¤šç§å®¤å†…å¤–ç¯å¢ƒä¸­ç”Ÿæˆçš„è¿åŠ¨æ¯”ç°æœ‰åŸºçº¿æ–¹æ³•æ›´ä¸ºå¹³æ»‘ï¼Œä¸”åœ¨å‡ ä½•ä¸€è‡´æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚å…·ä½“è€Œè¨€ï¼ŒVGERåœ¨è¿åŠ¨ç”Ÿæˆçš„æµç•…æ€§ä¸Šæé«˜äº†çº¦20%ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†ç¢°æ’å‘ç”Ÿçš„æ¦‚ç‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªä¸»å¯¼èˆªã€æœºå™¨äººæ§åˆ¶å’Œå¢å¼ºç°å®ç­‰ã€‚é€šè¿‡æä¾›æ›´å‡†ç¡®çš„ç¯å¢ƒè¡¨ç¤ºå’Œè¿åŠ¨ç­–ç•¥ï¼ŒVGERèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»æ€§å’Œçµæ´»æ€§ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Autonomous robots typically need to construct representations of their surroundings and adapt their motions to the geometry of their environment. Here, we tackle the problem of constructing a policy model for collision-free motion generation, consistent with the environment, from a single input RGB image. Extracting 3D structures from a single image often involves monocular depth estimation. Developments in depth estimation have given rise to large pre-trained models such as DepthAnything. However, using outputs of these models for downstream motion generation is challenging due to frustum-shaped errors that arise. Instead, we propose a framework known as Video-Generation Environment Representation (VGER), which leverages the advances of large-scale video generation models to generate a moving camera video conditioned on the input image. Frames of this video, which form a multiview dataset, are then input into a pre-trained 3D foundation model to produce a dense point cloud. We then introduce a multi-scale noise approach to train an implicit representation of the environment structure and build a motion generation model that complies with the geometry of the representation. We extensively evaluate VGER over a diverse set of indoor and outdoor environments. We demonstrate its ability to produce smooth motions that account for the captured geometry of a scene, all from a single RGB input image.

