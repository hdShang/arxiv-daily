---
layout: default
title: Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning
---

# Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.18487" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.18487v1</a>
  <a href="https://arxiv.org/pdf/2505.18487.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.18487v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.18487v1', 'Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junlin Wang, Zhiyun Lin

**åˆ†ç±»**: cs.RO, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-24

**å¤‡æ³¨**: A preprint version

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/HenryWJL/icon)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºIConæ–¹æ³•ä»¥æå‡æœºå™¨äººæ“ä½œä¸­çš„è§†è§‰è¡¨ç¤ºå­¦ä¹ æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¡¨ç¤º` `æœºå™¨äººæ“ä½œ` `å¯¹æ¯”å­¦ä¹ ` `ç­–ç•¥å­¦ä¹ ` `èº«ä½“åŠ¨æ€` `è§†è§‰å˜æ¢å™¨` `è¿ç§»å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä¸­éš¾ä»¥æœ‰æ•ˆå­¦ä¹ è§†è§‰è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚èº«ä½“åŠ¨æ€ä¸‹è¡¨ç°ä¸ä½³ã€‚
2. æœ¬æ–‡æå‡ºIConæ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¼ºåŒ–è§†è§‰è¡¨ç¤ºä¸­çš„èº«ä½“ç›¸å…³çº¿ç´¢ï¼Œæå‡ç­–ç•¥å­¦ä¹ æ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIConåœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†ç­–ç•¥æ€§èƒ½ï¼Œå¹¶æ”¯æŒä¸åŒæœºå™¨äººé—´çš„ç­–ç•¥è¿ç§»ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆçš„è§†è§‰è¡¨ç¤ºå­¦ä¹ åœ¨æœºå™¨äººæ“ä½œä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¤æ‚çš„èº«ä½“åŠ¨æ€ã€‚æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨åŒ…å«èº«ä½“ç›¸å…³çº¿ç´¢çš„è§†è§‰è¡¨ç¤ºæ¥ä¿ƒè¿›ä¸‹æ¸¸æœºå™¨äººæ“ä½œä»»åŠ¡çš„é«˜æ•ˆç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†IConï¼ˆInter-token Contrastï¼‰ï¼Œä¸€ç§åº”ç”¨äºè§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰ä»¤ç‰Œçº§è¡¨ç¤ºçš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚IConåœ¨ç‰¹å¾ç©ºé—´ä¸­å¼ºåˆ¶åŒºåˆ†ä»£ç†ç‰¹å®šå’Œç¯å¢ƒç‰¹å®šçš„ä»¤ç‰Œï¼Œä»è€Œç”ŸæˆåµŒå…¥èº«ä½“ç‰¹å®šå½’çº³åç½®çš„ä»¥ä»£ç†ä¸ºä¸­å¿ƒçš„è§†è§‰è¡¨ç¤ºã€‚è¯¥æ¡†æ¶å¯ä»¥é€šè¿‡å°†å¯¹æ¯”æŸå¤±ä½œä¸ºè¾…åŠ©ç›®æ ‡æ— ç¼é›†æˆåˆ°ç«¯åˆ°ç«¯ç­–ç•¥å­¦ä¹ ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒIConä¸ä»…æé«˜äº†å¤šç§æ“ä½œä»»åŠ¡çš„ç­–ç•¥æ€§èƒ½ï¼Œè¿˜ä¿ƒè¿›äº†ä¸åŒæœºå™¨äººä¹‹é—´çš„ç­–ç•¥è¿ç§»ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­æœ‰æ•ˆè§†è§‰è¡¨ç¤ºå­¦ä¹ çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘èº«ä½“åŠ¨æ€å¯¹ç­–ç•¥å­¦ä¹ çš„å½±å“ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥IConå¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œå¼ºåŒ–è§†è§‰è¡¨ç¤ºä¸­ä¸èº«ä½“ç›¸å…³çš„çº¿ç´¢ï¼Œä»è€Œç”Ÿæˆæ›´å…·ä»£ç†ä¸­å¿ƒæ€§çš„è§†è§‰è¡¨ç¤ºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šIConæ–¹æ³•åŒ…æ‹¬ç‰¹å¾ç©ºé—´ä¸­ä»£ç†ç‰¹å®šå’Œç¯å¢ƒç‰¹å®šä»¤ç‰Œçš„åˆ†ç¦»ï¼Œåˆ©ç”¨å¯¹æ¯”æŸå¤±ä½œä¸ºè¾…åŠ©ç›®æ ‡ï¼Œé›†æˆåˆ°ç«¯åˆ°ç«¯çš„ç­–ç•¥å­¦ä¹ ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šIConçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å¯¹æ¯”å­¦ä¹ å®ç°äº†è§†è§‰è¡¨ç¤ºçš„ä»£ç†ä¸­å¿ƒæ€§ï¼Œæ˜¾è‘—åŒºåˆ«äºä¼ ç»Ÿæ–¹æ³•çš„é€šç”¨è§†è§‰è¡¨ç¤ºã€‚

**å…³é”®è®¾è®¡**ï¼šIConçš„è®¾è®¡åŒ…æ‹¬å¯¹æ¯”æŸå¤±çš„è®¾ç½®ã€ä»¤ç‰Œçš„ç‰¹å¾æå–åŠå…¶åœ¨ç­–ç•¥å­¦ä¹ ä¸­çš„é›†æˆï¼Œç¡®ä¿äº†èº«ä½“ç‰¹å®šå½’çº³åç½®çš„æœ‰æ•ˆåµŒå…¥ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒIConæ–¹æ³•åœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸­æå‡äº†ç­–ç•¥æ€§èƒ½ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šç­–ç•¥æˆåŠŸç‡æé«˜äº†15%ã€‚æ­¤å¤–ï¼ŒIConè¿˜æ”¯æŒä¸åŒæœºå™¨äººä¹‹é—´çš„ç­–ç•¥è¿ç§»ï¼Œå±•ç¤ºäº†å…¶è‰¯å¥½çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æœºå™¨äººæ“ä½œã€è‡ªåŠ¨åŒ–åˆ¶é€ å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æå‡è§†è§‰è¡¨ç¤ºçš„å­¦ä¹ æ•ˆç‡ï¼ŒIConæ–¹æ³•èƒ½å¤ŸåŠ é€Ÿæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œè¿›è€Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚æœªæ¥ï¼ŒIConè¿˜å¯èƒ½åº”ç”¨äºå…¶ä»–éœ€è¦é«˜æ•ˆç­–ç•¥å­¦ä¹ çš„é¢†åŸŸï¼Œå¦‚è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½å®¶å±…ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Learning effective visual representations for robotic manipulation remains a fundamental challenge due to the complex body dynamics involved in action execution. In this paper, we study how visual representations that carry body-relevant cues can enable efficient policy learning for downstream robotic manipulation tasks. We present $\textbf{I}$nter-token $\textbf{Con}$trast ($\textbf{ICon}$), a contrastive learning method applied to the token-level representations of Vision Transformers (ViTs). ICon enforces a separation in the feature space between agent-specific and environment-specific tokens, resulting in agent-centric visual representations that embed body-specific inductive biases. This framework can be seamlessly integrated into end-to-end policy learning by incorporating the contrastive loss as an auxiliary objective. Our experiments show that ICon not only improves policy performance across various manipulation tasks but also facilitates policy transfer across different robots. The project website: https://github.com/HenryWJL/icon

