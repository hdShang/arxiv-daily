---
layout: default
title: "Canonical Policy: Learning Canonical 3D Representation for SE(3)-Equivariant Policy"
---

# Canonical Policy: Learning Canonical 3D Representation for SE(3)-Equivariant Policy

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.18474" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.18474v2</a>
  <a href="https://arxiv.org/pdf/2505.18474.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.18474v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.18474v2', 'Canonical Policy: Learning Canonical 3D Representation for SE(3)-Equivariant Policy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhiyuan Zhang, Zhengtong Xu, Jai Nanda Lakamsani, Yu She

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-24 (æ›´æ–°: 2025-11-08)

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://zhangzhiyuanzhang.github.io/cp-website/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè§„èŒƒç­–ç•¥ä»¥è§£å†³3Dç­‰å˜æ¨¡ä»¿å­¦ä¹ é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰æ¨¡ä»¿å­¦ä¹ ` `3Dç‚¹äº‘` `ç­‰å˜ç­–ç•¥` `æœºå™¨äººæ“ä½œ` `æ³›åŒ–èƒ½åŠ›` `æ ·æœ¬æ•ˆç‡` `ç”Ÿæˆæ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç­‰å˜æ–¹æ³•åœ¨é›†æˆç­‰å˜ç»„ä»¶æ—¶ç¼ºä¹ç»“æ„åŒ–ï¼Œå¯¼è‡´å¯è§£é‡Šæ€§å’Œä¸¥è°¨æ€§ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºçš„è§„èŒƒç­–ç•¥æ¡†æ¶é€šè¿‡è§„èŒƒåŒ–3Dç‚¹äº‘è§‚å¯Ÿï¼Œå»ºç«‹äº†ç­‰å˜è§‚å¯Ÿåˆ°åŠ¨ä½œçš„æ˜ å°„ç†è®ºã€‚
3. åœ¨å¤šé¡¹ä»»åŠ¡ä¸­ï¼Œè§„èŒƒç­–ç•¥åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­åˆ†åˆ«å®ç°äº†18.0%å’Œ39.7%çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰æ¨¡ä»¿å­¦ä¹ åœ¨æœºå™¨äººæ“ä½œä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æœªè§å¯¹è±¡ã€åœºæ™¯å¸ƒå±€å’Œç›¸æœºè§†è§’çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è§„èŒƒç­–ç•¥ï¼ˆcanonical policyï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€3Dç‚¹äº‘è§‚å¯Ÿçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°ç­‰å˜è§‚å¯Ÿåˆ°åŠ¨ä½œçš„æ˜ å°„ã€‚é€šè¿‡åœ¨12ä¸ªå¤šæ ·åŒ–çš„æ¨¡æ‹Ÿä»»åŠ¡å’Œ4ä¸ªçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­è¿›è¡ŒéªŒè¯ï¼Œè§„èŒƒç­–ç•¥åœ¨æ¨¡æ‹Ÿä¸­å¹³å‡æå‡äº†18.0%ï¼Œåœ¨çœŸå®å®éªŒä¸­æå‡äº†39.7%ï¼Œå±•ç°äº†ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹æœªè§å¯¹è±¡å’Œä¸åŒåœºæ™¯å¸ƒå±€æ—¶çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹å¯¹3Dç‚¹äº‘çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¯¼è‡´åœ¨æ–°ç¯å¢ƒä¸­çš„è¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè§„èŒƒç­–ç•¥é€šè¿‡å»ºç«‹3Dè§„èŒƒè¡¨ç¤ºï¼Œç»Ÿä¸€ä¸åŒçš„ç‚¹äº‘è§‚å¯Ÿï¼Œä»è€Œå®ç°ç­‰å˜çš„è§‚å¯Ÿåˆ°åŠ¨ä½œæ˜ å°„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å‡ ä½•å¯¹ç§°æ€§å’Œç°ä»£ç”Ÿæˆæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæå‡äº†æ¨¡ä»¿å­¦ä¹ çš„æ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯3Dè§„èŒƒè¡¨ç¤ºçš„å»ºç«‹ï¼Œå…¶æ¬¡æ˜¯åŸºäºè¯¥è¡¨ç¤ºçš„ç­–ç•¥å­¦ä¹ ç®¡é“ï¼Œæœ€åæ˜¯é€šè¿‡ç”Ÿæˆæ¨¡å‹è¿›è¡ŒåŠ¨ä½œç”Ÿæˆã€‚æ¯ä¸ªæ¨¡å—éƒ½ç´§å¯†ç»“åˆï¼Œä»¥ç¡®ä¿ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’å’Œåˆ©ç”¨ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†è§„èŒƒç­–ç•¥æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°å°†3Dç‚¹äº‘è§‚å¯Ÿæ•´åˆåˆ°ä¸€ä¸ªè§„èŒƒè¡¨ç¤ºä¸­ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¯è§£é‡Šæ€§å’Œç»“æ„åŒ–æ–¹é¢çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç­‰å˜æ˜ å°„ï¼ŒåŒæ—¶åˆ©ç”¨ç°ä»£ç”Ÿæˆæ¨¡å‹çš„çµæ´»æ€§æ¥å¢å¼ºç­–ç•¥çš„è¡¨è¾¾èƒ½åŠ›ã€‚ç½‘ç»œç»“æ„ä¸Šï¼Œç»“åˆäº†å‡ ä½•å¯¹ç§°æ€§ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè§„èŒƒç­–ç•¥åœ¨12ä¸ªæ¨¡æ‹Ÿä»»åŠ¡ä¸­å¹³å‡æå‡äº†18.0%çš„æ€§èƒ½ï¼Œåœ¨4ä¸ªçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­æå‡äº†39.7%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè§„èŒƒç­–ç•¥åœ¨æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæŠ“å–ã€è‡ªåŠ¨åŒ–è£…é…å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚é€šè¿‡æå‡æœºå™¨äººå¯¹æœªçŸ¥ç¯å¢ƒçš„é€‚åº”èƒ½åŠ›ï¼Œè§„èŒƒç­–ç•¥èƒ½å¤Ÿåœ¨å®é™…æ“ä½œä¸­æ˜¾è‘—æé«˜æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Visual Imitation learning has achieved remarkable progress in robotic manipulation, yet generalization to unseen objects, scene layouts, and camera viewpoints remains a key challenge. Recent advances address this by using 3D point clouds, which provide geometry-aware, appearance-invariant representations, and by incorporating equivariance into policy architectures to exploit spatial symmetries. However, existing equivariant approaches often lack interpretability and rigor due to unstructured integration of equivariant components. We introduce canonical policy, a principled framework for 3D equivariant imitation learning that unifies 3D point cloud observations under a canonical representation. We first establish a theory of 3D canonical representations, enabling equivariant observation-to-action mappings by grouping both seen and novel point clouds to a canonical representation. We then propose a flexible policy learning pipeline that leverages geometric symmetries from canonical representation and the expressiveness of modern generative models. We validate canonical policy on 12 diverse simulated tasks and 4 real-world manipulation tasks across 16 configurations, involving variations in object color, shape, camera viewpoint, and robot platform. Compared to state-of-the-art imitation learning policies, canonical policy achieves an average improvement of 18.0% in simulation and 39.7% in real-world experiments, demonstrating superior generalization capability and sample efficiency. For more details, please refer to the project website: https://zhangzhiyuanzhang.github.io/cp-website/.

