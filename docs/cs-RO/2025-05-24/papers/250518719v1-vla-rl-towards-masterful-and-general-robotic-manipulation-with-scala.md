---
layout: default
title: "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning"
---

# VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.18719" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.18719v1</a>
  <a href="https://arxiv.org/pdf/2505.18719.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.18719v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.18719v1', 'VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, Ziwei Wang

**ÂàÜÁ±ª**: cs.RO, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-24

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫VLA-RL‰ª•Ëß£ÂÜ≥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÊï∞ÊçÆÁ®ÄÁº∫ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰Ωú` `Âº∫ÂåñÂ≠¶‰π†` `Êú∫Âô®‰∫∫Êìç‰Ωú` `Â§öÊ®°ÊÄÅÂ≠¶‰π†` `Á®ÄÁñèÂ•ñÂä±`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÈ´òÂÆπÈáèVLAÊ®°ÂûãÂú®Á¶ªÁ∫øÊï∞ÊçÆÂà©Áî®‰∏äÂ≠òÂú®Â±ÄÈôêÔºåÂØºËá¥Âú®ÂàÜÂ∏ÉÂ§ñÂú∫ÊôØ‰∏≠ÊâßË°åÂ§±Ë¥•„ÄÇ
2. VLA-RLÈÄöËøáÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÊîπËøõÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÈááÁî®ËΩ®ËøπÁ∫ßRLËÆ≠ÁªÉÊñπÊ≥ïÔºåÂª∫Ê®°Êú∫Âô®‰∫∫Êìç‰Ωú‰∏∫Â§öÊ®°ÊÄÅÂØπËØù„ÄÇ
3. VLA-RLÂú®LIBEROÊï∞ÊçÆÈõÜ‰∏äË∂ÖË∂ä‰∫ÜÊúÄÂº∫Âü∫Á∫ø4.5%ÔºåÂπ∂‰∏éÂïÜ‰∏öÊ®°ÂûãÊÄßËÉΩÁõ∏ÂΩìÔºåÂ±ïÁ§∫‰∫ÜÊµãËØïÊó∂‰ºòÂåñÁöÑÊΩúÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ËøëÂπ¥Êù•ÔºåÈ´òÂÆπÈáèÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÂú®Ê®°‰ªø‰∫∫Á±ªÁ§∫ËåÉÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÁÑ∂ËÄåÔºåÂà©Áî®Á¶ªÁ∫øÊï∞ÊçÆÊó∂ÔºåÁî±‰∫éÁä∂ÊÄÅËÆøÈóÆÊúâÈôêÔºåÂèØËÉΩÂú®ÂàÜÂ∏ÉÂ§ñÂú∫ÊôØ‰∏≠ÂØºËá¥ÊâßË°åÂ§±Ë¥•„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜVLA-RLÔºå‰∏Ä‰∏™Âà©Áî®Âú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊù•ÊîπËøõÈ¢ÑËÆ≠ÁªÉËá™ÂõûÂΩíVLAÁöÑÁÆóÊ≥ïÊ°ÜÊû∂„ÄÇÈÄöËøáÂ∞ÜÊú∫Âô®‰∫∫Êìç‰ΩúËΩ®ËøπÂª∫Ê®°‰∏∫Â§öÊ®°ÊÄÅÂ§öËΩÆÂØπËØùÔºåËÆ∫ÊñáÂºïÂÖ•‰∫ÜËΩ®ËøπÁ∫ßRLÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÂπ∂ÈÄöËøá‰º™Â•ñÂä±Ê†áÁ≠æÂØπËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ª•Â∫îÂØπÁ®ÄÁñèÂ•ñÂä±ÁöÑÊåëÊàò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVLA-RLÂú®40‰∏™Â§çÊùÇÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÊúÄÂº∫ÁöÑÂæÆË∞ÉÂü∫Á∫øÔºåÂπ∂‰∏éÂÖàËøõÁöÑÂïÜ‰∏öÊ®°ÂûãÁõ∏ÂåπÈÖç„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâVLAÊ®°ÂûãÂú®Á¶ªÁ∫øÊï∞ÊçÆÂà©Áî®Êó∂ÁöÑÂ±ÄÈôêÊÄßÔºåÁâπÂà´ÊòØÂú®ÂàÜÂ∏ÉÂ§ñÂú∫ÊôØ‰∏≠ÂèØËÉΩÂØºËá¥ÁöÑÊâßË°åÂ§±Ë¥•ÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Áä∂ÊÄÅËÆøÈóÆ‰∏äÊúâÈôêÔºåÊó†Ê≥ïÊúâÊïàÂ∫îÂØπÂ§çÊùÇÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöVLA-RLÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†Êù•Â¢ûÂº∫È¢ÑËÆ≠ÁªÉÁöÑVLAÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®ÊµãËØïÈò∂ÊÆµÈÄöËøáÊé¢Á¥¢ÊÄßÊñπÊ≥ïÊù•ÊîπËøõÊ®°ÂûãÊÄßËÉΩ„ÄÇÂ∞ÜÊú∫Âô®‰∫∫Êìç‰ΩúËΩ®ËøπËßÜ‰∏∫Â§öÊ®°ÊÄÅÂ§öËΩÆÂØπËØùÔºåÊúâÂä©‰∫éÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÁîüÊàêÊìç‰ΩúÂ∫èÂàó„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöVLA-RLÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨ËΩ®ËøπÁ∫ßRLËÆ≠ÁªÉ„ÄÅ‰º™Â•ñÂä±Ê®°ÂûãÂæÆË∞É‰ª•ÂèäÂ§öÈ°πÂÆûÁé∞‰ºòÂåñÁ≠ñÁï•„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã„ÄÅÂ•ñÂä±Ê®°ÂûãÂíåÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†Ê®°Âùó„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöVLA-RLÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂ∞ÜÊú∫Âô®‰∫∫Êìç‰ΩúËΩ®ËøπÂª∫Ê®°‰∏∫ÂØπËØùÂΩ¢ÂºèÔºåÂπ∂ÈÄöËøá‰º™Â•ñÂä±Ê†áÁ≠æÊù•Ëß£ÂÜ≥Á®ÄÁñèÂ•ñÂä±ÈóÆÈ¢ò„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏é‰º†ÁªüÁöÑÂçï‰∏Ä‰ªªÂä°Â≠¶‰π†ÊñπÊ≥ïÊúâÊú¨Ë¥®Âå∫Âà´ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÁöÑÊìç‰ΩúÁéØÂ¢É„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ËÆæËÆ°‰∏äÔºåVLA-RLÈááÁî®‰∫ÜËØæÁ®ãÈÄâÊã©Á≠ñÁï•„ÄÅGPUÂπ≥Ë°°ÁöÑÂêëÈáèÂåñÁéØÂ¢É„ÄÅÊâπÈáèËß£Á†ÅÂíåËØÑËÆ∫ÂëòÈ¢ÑÁÉ≠Á≠âÊäÄÊúØÁªÜËäÇÔºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÁéá„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®LIBEROÊï∞ÊçÆÈõÜ‰∏äÔºåVLA-RL‰ΩøOpenVLA-7BÂú®40‰∏™Â§çÊùÇÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÊúÄÂº∫ÁöÑÂæÆË∞ÉÂü∫Á∫ø4.5%ÔºåÂπ∂‰∏éÂÖàËøõÁöÑÂïÜ‰∏öÊ®°ÂûãÂ¶Ç$œÄ_0$-FASTÁöÑÊÄßËÉΩÁõ∏ÂΩìÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂº∫Â§ßËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

VLA-RLÁöÑÁ†îÁ©∂ÊàêÊûúÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÈ¢ÜÂüüÂÖ∑ÊúâÂπøÊ≥õÁöÑÂ∫îÁî®ÊΩúÂäõÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´òÁ≤æÂ∫¶ÂíåÁÅµÊ¥ªÊÄßÁöÑ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂ∑•‰∏öËá™Âä®Âåñ„ÄÅÊúçÂä°Êú∫Âô®‰∫∫ÂíåÂÆ∂Â∫≠Âä©ÁêÜÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊ®°ÂûãÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îËÉΩÂäõÔºåÊú™Êù•ÂèØÊé®Âä®Êú∫Âô®‰∫∫ÊäÄÊúØÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ï‰∏éÊôÆÂèä„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $œÄ_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.

