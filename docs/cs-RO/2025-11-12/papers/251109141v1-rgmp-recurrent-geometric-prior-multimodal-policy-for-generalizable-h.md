---
layout: default
title: RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation
---

# RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.09141" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.09141v1</a>
  <a href="https://arxiv.org/pdf/2511.09141.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.09141v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2511.09141v1', 'RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xuetao Li, Wenke Huang, Nengyuan Pan, Kaiyan Zhao, Songhua Yang, Yiming Wang, Mengde Li, Mang Ye, Jifeng Xuan, Miao Li

**åˆ†ç±»**: cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

**æœŸåˆŠ**: Proceedings of the AAAI conference on artificial intelligence, 2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRGMPï¼Œèåˆå‡ ä½•å…ˆéªŒä¸é€’å½’é«˜æ–¯è¿‡ç¨‹ï¼Œæå‡äººå½¢æœºå™¨äººæ“ä½œçš„æ³›åŒ–æ€§å’Œæ•°æ®æ•ˆç‡ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±å››ï¼šç”Ÿæˆå¼åŠ¨ä½œ (Generative Motion)**

**å…³é”®è¯**: `äººå½¢æœºå™¨äººæ“ä½œ` `å‡ ä½•å…ˆéªŒ` `å¤šæ¨¡æ€ç­–ç•¥` `é€’å½’é«˜æ–¯è¿‡ç¨‹` `æ•°æ®é«˜æ•ˆ` `è§†è§‰è¿åŠ¨æ§åˆ¶` `æ³›åŒ–èƒ½åŠ›`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ•°æ®é©±åŠ¨æ–¹æ³•ä¾èµ–å¤§é‡è®­ç»ƒæ•°æ®ï¼Œå¿½ç•¥äº†å‡ ä½•æ¨ç†ï¼Œä¸”æœºå™¨äºº-ç›®æ ‡å…³ç³»å»ºæ¨¡æ•ˆç‡ä½ï¼Œå¯¼è‡´æ³›åŒ–æ€§å·®ã€‚
2. RGMPæ¡†æ¶èåˆå‡ ä½•è¯­ä¹‰æŠ€èƒ½æ¨ç†ä¸æ•°æ®é«˜æ•ˆè§†è§‰è¿åŠ¨æ§åˆ¶ï¼Œæå‡åœ¨æœªè§åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRGMPåœ¨æ³›åŒ–æµ‹è¯•ä¸­æˆåŠŸç‡è¾¾87%ï¼Œæ•°æ®æ•ˆç‡æ¯”ç°æœ‰æ–¹æ³•æå‡5å€ï¼ŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé€’å½’å‡ ä½•å…ˆéªŒå¤šæ¨¡æ€ç­–ç•¥ï¼ˆRGMPï¼‰çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨ç»Ÿä¸€å‡ ä½•è¯­ä¹‰æŠ€èƒ½æ¨ç†ä¸æ•°æ®é«˜æ•ˆçš„è§†è§‰è¿åŠ¨æ§åˆ¶ï¼Œä»è€Œæå‡äººå½¢æœºå™¨äººçš„æ“ä½œèƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨ï¼Œå°†å‡ ä½•å½’çº³åç½®æ³¨å…¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸ºæœªè§åœºæ™¯ç”Ÿæˆè‡ªé€‚åº”æŠ€èƒ½åºåˆ—ã€‚åŒæ—¶ï¼Œå¼•å…¥è‡ªé€‚åº”é€’å½’é«˜æ–¯ç½‘ç»œï¼Œå°†æœºå™¨äºº-ç›®æ ‡äº¤äº’å‚æ•°åŒ–ä¸ºç´§å‡‘çš„é«˜æ–¯è¿‡ç¨‹å±‚çº§ç»“æ„ï¼Œé€’å½’ç¼–ç å¤šå°ºåº¦ç©ºé—´å…³ç³»ï¼Œå®ç°æ•°æ®é«˜æ•ˆçš„çµå·§è¿åŠ¨åˆæˆã€‚åœ¨äººå½¢æœºå™¨äººå’Œæ¡Œé¢åŒè‡‚æœºå™¨äººä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒRGMPåœ¨æ³›åŒ–æµ‹è¯•ä¸­å®ç°äº†87%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶ä¸”æ¯”æœ€å…ˆè¿›çš„æ¨¡å‹æé«˜äº†5å€çš„æ•°æ®æ•ˆç‡ã€‚è¿™çªæ˜¾äº†å…¶ç”±å‡ ä½•è¯­ä¹‰æ¨ç†å’Œé€’å½’é«˜æ–¯è‡ªé€‚åº”æ‰€æ”¯æŒçš„å“è¶Šè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„äººå½¢æœºå™¨äººæ“ä½œæ–¹æ³•ä¸¥é‡ä¾èµ–å¤§é‡çš„æ•°æ®é©±åŠ¨ï¼Œè¿™å¯¼è‡´äº†ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯å¿½ç•¥äº†åœ¨æœªè§åœºæ™¯ä¸­çš„å‡ ä½•æ¨ç†èƒ½åŠ›ï¼ŒäºŒæ˜¯è®­ç»ƒæ•°æ®ä¸­æœºå™¨äººä¸ç›®æ ‡å…³ç³»çš„å»ºæ¨¡æ•ˆç‡ä½ä¸‹ï¼Œé€ æˆäº†è®­ç»ƒèµ„æºçš„æµªè´¹ã€‚å› æ­¤ï¼Œå¦‚ä½•æå‡äººå½¢æœºå™¨äººåœ¨æ–°åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é™ä½å¯¹å¤§é‡è®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRGMPçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å‡ ä½•å…ˆéªŒçŸ¥è¯†èå…¥åˆ°å¤šæ¨¡æ€ç­–ç•¥ä¸­ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹åœºæ™¯çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨æ¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®åœºæ™¯çš„å‡ ä½•ç‰¹å¾é€‰æ‹©åˆé€‚çš„æŠ€èƒ½åºåˆ—ã€‚åŒæ—¶ï¼Œåˆ©ç”¨è‡ªé€‚åº”é€’å½’é«˜æ–¯ç½‘ç»œæ¥é«˜æ•ˆåœ°å»ºæ¨¡æœºå™¨äººä¸ç›®æ ‡ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œä»è€Œå®ç°æ•°æ®é«˜æ•ˆçš„è¿åŠ¨åˆæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRGMPæ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨ï¼ˆGeometric-prior Skill Selectorï¼‰å’Œè‡ªé€‚åº”é€’å½’é«˜æ–¯ç½‘ç»œï¼ˆAdaptive Recursive Gaussian Networkï¼‰ã€‚é¦–å…ˆï¼Œå‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æå–åœºæ™¯çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç»“åˆå‡ ä½•å…ˆéªŒçŸ¥è¯†é€‰æ‹©åˆé€‚çš„æŠ€èƒ½åºåˆ—ã€‚ç„¶åï¼Œè‡ªé€‚åº”é€’å½’é«˜æ–¯ç½‘ç»œæ ¹æ®é€‰æ‹©çš„æŠ€èƒ½åºåˆ—ï¼Œé€’å½’åœ°ç¼–ç å¤šå°ºåº¦ç©ºé—´å…³ç³»ï¼Œç”Ÿæˆæœºå™¨äººçš„è¿åŠ¨è½¨è¿¹ã€‚æ•´ä¸ªæ¡†æ¶æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è®­ç»ƒæµç¨‹ï¼Œå¯ä»¥ç›´æ¥ä»è§†è§‰è¾“å…¥åˆ°è¿åŠ¨è¾“å‡ºã€‚

**å…³é”®åˆ›æ–°**ï¼šRGMPçš„å…³é”®åˆ›æ–°åœ¨äºå°†å‡ ä½•å…ˆéªŒçŸ¥è¯†èå…¥åˆ°å¤šæ¨¡æ€ç­–ç•¥ä¸­ï¼Œå¹¶åˆ©ç”¨é€’å½’é«˜æ–¯ç½‘ç»œé«˜æ•ˆåœ°å»ºæ¨¡æœºå™¨äººä¸ç›®æ ‡ä¹‹é—´çš„å…³ç³»ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRGMPä¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æœªè§åœºæ™¯ã€‚æ­¤å¤–ï¼Œè‡ªé€‚åº”é€’å½’é«˜æ–¯ç½‘ç»œèƒ½å¤Ÿä»¥ç´§å‡‘çš„æ–¹å¼è¡¨ç¤ºå¤æ‚çš„ç©ºé—´å…³ç³»ï¼Œä»è€Œæé«˜äº†è¿åŠ¨åˆæˆçš„æ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šå‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨é€šè¿‡å¼•å…¥å‡ ä½•å½’çº³åç½®æ¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åœºæ™¯çš„å‡ ä½•ç‰¹å¾ã€‚è‡ªé€‚åº”é€’å½’é«˜æ–¯ç½‘ç»œåˆ™é€šè¿‡é€’å½’åœ°ç¼–ç å¤šå°ºåº¦ç©ºé—´å…³ç³»ï¼Œæ¥é«˜æ•ˆåœ°å»ºæ¨¡æœºå™¨äººä¸ç›®æ ‡ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†çš„æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RGMPæ¡†æ¶åœ¨äººå½¢æœºå™¨äººå’Œæ¡Œé¢åŒè‡‚æœºå™¨äººä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶åœ¨æ³›åŒ–æµ‹è¯•ä¸­å®ç°äº†87%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶ä¸”æ¯”æœ€å…ˆè¿›çš„æ¨¡å‹æé«˜äº†5å€çš„æ•°æ®æ•ˆç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRGMPæ¡†æ¶å…·æœ‰ä¼˜è¶Šçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›å’Œæ•°æ®æ•ˆç‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RGMPæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–ã€åŒ»ç–—åº·å¤ç­‰é¢†åŸŸã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿä½¿äººå½¢æœºå™¨äººåœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­æ‰§è¡Œå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚ç‰©å“æŠ“å–ã€è£…é…ã€æ¸…æ´ç­‰ã€‚é€šè¿‡æå‡æœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®æ•ˆç‡ï¼Œå¯ä»¥é™ä½æœºå™¨äººçš„éƒ¨ç½²æˆæœ¬ï¼Œå¹¶ä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.

