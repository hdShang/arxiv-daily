---
layout: default
title: WMPO: World Model-based Policy Optimization for Vision-Language-Action Models
---

# WMPO: World Model-based Policy Optimization for Vision-Language-Action Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2511.09515" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2511.09515v1</a>
  <a href="https://arxiv.org/pdf/2511.09515.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2511.09515v1" onclick="toggleFavorite(this, '2511.09515v1', 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-11-12

**å¤‡æ³¨**: project website: https://wm-po.github.io

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWMPOï¼Œç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„åŸºäºä¸–ç•Œæ¨¡å‹çš„ç­–ç•¥ä¼˜åŒ–**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹` `ä¸–ç•Œæ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ç­–ç•¥ä¼˜åŒ–` `æœºå™¨äººæ“ä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. VLAæ¨¡å‹ä¾èµ–ä¸“å®¶æ•°æ®ï¼Œéš¾ä»¥ä»å¤±è´¥ä¸­å­¦ä¹ å’Œè‡ªæˆ‘çº æ­£ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚æœºå™¨äººä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
2. WMPOé€šè¿‡æ„å»ºåƒç´ çº§ä¸–ç•Œæ¨¡å‹ï¼Œä½¿æ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œé¿å…äº†ä¸çœŸå®ç¯å¢ƒçš„ç›´æ¥äº¤äº’ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒWMPOæ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡å’Œæ•´ä½“æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºè‡ªæˆ‘çº æ­£ã€æ³›åŒ–å’Œç»ˆèº«å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨é€šç”¨æœºå™¨äººæ“ä½œæ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¯¹ä¸“å®¶æ¼”ç¤ºçš„ä¾èµ–é™åˆ¶äº†å…¶ä»å¤±è´¥ä¸­å­¦ä¹ å’Œæ‰§è¡Œè‡ªæˆ‘çº æ­£çš„èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ (RL)é€šè¿‡ä¸ç‰©ç†ç¯å¢ƒçš„è‡ªæˆ‘æ”¹è¿›äº¤äº’æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†å´é¢ä¸´ç€çœŸå®æœºå™¨äººä¸Šçš„é«˜æ ·æœ¬å¤æ‚åº¦ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºä¸–ç•Œæ¨¡å‹çš„ç­–ç•¥ä¼˜åŒ–(WMPO)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåœ¨çº¿VLA RLçš„åŸåˆ™æ€§æ¡†æ¶ï¼Œæ— éœ€ä¸çœŸå®ç¯å¢ƒäº¤äº’ã€‚ä¸å¹¿æ³›ä½¿ç”¨çš„æ½œåœ¨ä¸–ç•Œæ¨¡å‹ä¸åŒï¼ŒWMPOä¸“æ³¨äºåƒç´ çº§é¢„æµ‹ï¼Œä½¿â€œæƒ³è±¡â€çš„è½¨è¿¹ä¸ä½¿ç”¨ç½‘ç»œè§„æ¨¡å›¾åƒé¢„è®­ç»ƒçš„VLAç‰¹å¾å¯¹é½ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼ŒWMPOä½¿ç­–ç•¥èƒ½å¤Ÿæ‰§è¡Œåœ¨çº¿GRPOï¼Œä»è€Œæä¾›æ¯”å¸¸ç”¨çš„ç¦»çº¿æ–¹æ³•æ›´å¼ºçš„æ€§èƒ½ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒWMPO (i)æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œ(ii)å®ç°äº†æ›´å¼ºçš„æ•´ä½“æ€§èƒ½ï¼Œ(iii)è¡¨ç°å‡ºè¯¸å¦‚è‡ªæˆ‘çº æ­£ç­‰æ¶Œç°è¡Œä¸ºï¼Œä»¥åŠ(iv)å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–å’Œç»ˆèº«å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šVLAæ¨¡å‹è™½ç„¶åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œä½†è¿‡åº¦ä¾èµ–ä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼Œå¯¼è‡´å…¶éš¾ä»¥ä»å¤±è´¥ç»éªŒä¸­å­¦ä¹ ï¼Œç¼ºä¹è‡ªæˆ‘çº æ­£èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ è™½ç„¶å¯ä»¥é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ ï¼Œä½†æ ·æœ¬æ•ˆç‡ä½ï¼Œåœ¨çœŸå®æœºå™¨äººä¸Šåº”ç”¨æˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œå¦‚ä½•åœ¨é™ä½æ ·æœ¬å¤æ‚åº¦çš„åŒæ—¶ï¼Œæå‡VLAæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œæ˜¯æœ¬æ–‡è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šWMPOçš„æ ¸å¿ƒæ€è·¯æ˜¯æ„å»ºä¸€ä¸ªåŸºäºåƒç´ çš„ä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹åœ¨ç»™å®šåŠ¨ä½œåºåˆ—ä¸‹ï¼Œç¯å¢ƒçš„æœªæ¥çŠ¶æ€ï¼ˆåƒç´ ï¼‰ã€‚é€šè¿‡åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œæ™ºèƒ½ä½“å¯ä»¥åœ¨æ— éœ€ä¸çœŸå®ç¯å¢ƒäº¤äº’çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•é™ä½äº†æ ·æœ¬å¤æ‚åº¦ï¼Œå¹¶å…è®¸æ™ºèƒ½ä½“ä»æ›´å¤šæ ·åŒ–çš„ç»éªŒä¸­å­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWMPOæ¡†æ¶ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š1) VLAç‰¹å¾æå–å™¨ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„VLAæ¨¡å‹æå–å›¾åƒå’Œè¯­è¨€æŒ‡ä»¤çš„ç‰¹å¾ã€‚2) ä¸–ç•Œæ¨¡å‹ï¼šä¸€ä¸ªåŸºäºåƒç´ çš„é¢„æµ‹æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹ç»™å®šçŠ¶æ€å’ŒåŠ¨ä½œåºåˆ—ä¸‹çš„æœªæ¥çŠ¶æ€ã€‚3) ç­–ç•¥ç½‘ç»œï¼šç”¨äºç”ŸæˆåŠ¨ä½œåºåˆ—ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–åœ¨ä¸–ç•Œæ¨¡å‹ä¸­é¢„æµ‹çš„å¥–åŠ±ã€‚4) ç­–ç•¥ä¼˜åŒ–å™¨ï¼šä½¿ç”¨åœ¨çº¿GRPOç®—æ³•ä¼˜åŒ–ç­–ç•¥ç½‘ç»œã€‚æ•´ä¸ªæµç¨‹æ˜¯ï¼Œé¦–å…ˆä½¿ç”¨VLAç‰¹å¾æå–å™¨æå–ç¯å¢ƒçŠ¶æ€ç‰¹å¾ï¼Œç„¶åç­–ç•¥ç½‘ç»œæ ¹æ®çŠ¶æ€ç”ŸæˆåŠ¨ä½œï¼Œä¸–ç•Œæ¨¡å‹é¢„æµ‹æ‰§è¡ŒåŠ¨ä½œåçš„ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œç­–ç•¥ä¼˜åŒ–å™¨æ ¹æ®é¢„æµ‹çš„çŠ¶æ€å’Œå¥–åŠ±ä¿¡å·æ›´æ–°ç­–ç•¥ç½‘ç»œã€‚

**å…³é”®åˆ›æ–°**ï¼šWMPOçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨åƒç´ çº§é¢„æµ‹çš„ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶ç»“åˆåœ¨çº¿GRPOç®—æ³•è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„æ½œåœ¨ä¸–ç•Œæ¨¡å‹ç›¸æ¯”ï¼Œåƒç´ çº§é¢„æµ‹èƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½â€œæƒ³è±¡â€çš„è½¨è¿¹ä¸VLAç‰¹å¾ï¼Œä»è€Œæé«˜ç­–ç•¥å­¦ä¹ çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒWMPOé‡‡ç”¨åœ¨çº¿GRPOç®—æ³•ï¼Œç›¸æ¯”äºå¸¸ç”¨çš„ç¦»çº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›æ›´å¼ºçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šWMPOçš„å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1) ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæ„å»ºåƒç´ çº§ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨å¯¹æŠ—ç”Ÿæˆç½‘ç»œ(GAN)æ¥æé«˜é¢„æµ‹çš„çœŸå®æ€§ã€‚2) ä½¿ç”¨åœ¨çº¿GRPOç®—æ³•è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œå¹¶é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚3) è®¾è®¡åˆé€‚çš„å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡å¹¶é¿å…ç¢°æ’ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒWMPOåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼ŒWMPOçš„æ ·æœ¬æ•ˆç‡æ¯”åŸºçº¿æ–¹æ³•æé«˜äº†æ•°å€ã€‚åœ¨çœŸå®æœºå™¨äººç¯å¢ƒä¸­ï¼ŒWMPOèƒ½å¤ŸæˆåŠŸå®Œæˆå¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œå¹¶å±•ç°å‡ºè‡ªæˆ‘çº æ­£å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼ŒWMPOèƒ½å¤Ÿè‡ªä¸»åœ°è°ƒæ•´æŠ“å–å§¿åŠ¿ï¼Œä»¥åº”å¯¹ç‰©ä½“ä½ç½®çš„å¾®å°å˜åŒ–ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

WMPOå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºå„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡æœºå™¨äººã€å·¥ä¸šè‡ªåŠ¨åŒ–æœºå™¨äººå’ŒåŒ»ç–—æœºå™¨äººç­‰ã€‚é€šè¿‡åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—é™ä½æœºå™¨äººçš„å¼€å‘æˆæœ¬å’Œé£é™©ï¼Œå¹¶æé«˜å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒWMPOè¿˜å¯ä»¥ç”¨äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´è‡ªä¸»çš„æœºå™¨äººç³»ç»Ÿï¼Œä»è€Œå®ç°æ›´é«˜çº§åˆ«çš„è‡ªåŠ¨åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

