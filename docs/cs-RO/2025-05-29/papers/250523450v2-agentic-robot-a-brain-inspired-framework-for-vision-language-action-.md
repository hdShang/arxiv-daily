---
layout: default
title: "Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents"
---

# Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23450" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.23450v2</a>
  <a href="https://arxiv.org/pdf/2505.23450.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23450v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23450v2', 'Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhejian Yang, Yongchao Chen, Xueyang Zhou, Jiangyue Yan, Dingjie Song, Yinuo Liu, Yuting Li, Yu Zhang, Pan Zhou, Hechang Chen, Lichao Sun

**ÂàÜÁ±ª**: cs.RO

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-29 (Êõ¥Êñ∞: 2025-06-11)

**Â§áÊ≥®**: 20 pages, 8 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Agentic RobotÊ°ÜÊû∂‰ª•Ëß£ÂÜ≥ÈïøÊó∂Èó¥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÈîôËØØÁ¥ØÁßØÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∏ÄÔºöÊú∫Âô®‰∫∫ÊéßÂà∂ (Robot Control)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÈïøÊó∂Èó¥Êìç‰Ωú` `Êú∫Âô®‰∫∫ÊäÄÊúØ` `ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®` `Ê†áÂáÜÂåñË°åÂä®Á®ãÂ∫è` `Ëá™ÊàëÈ™åËØÅ` `Êô∫ËÉΩÁ≥ªÁªü` `ÈîôËØØÊÅ¢Â§ç`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊñπÊ≥ïÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠ÂÆπÊòìÂá∫Áé∞ÈîôËØØÁ¥ØÁßØÔºåÁº∫‰πèÊúâÊïàÁöÑÊâßË°åÈ™åËØÅÊú∫Âà∂ÔºåÂΩ±Âìç‰∫ÜÁ≥ªÁªüÁöÑÂèØÈù†ÊÄß„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑAgentic RobotÊ°ÜÊû∂ÈÄöËøáÊ†áÂáÜÂåñË°åÂä®Á®ãÂ∫èÔºàSAPÔºâÂÆûÁé∞‰∫ÜÁªÑ‰ª∂Èó¥ÁöÑÂçèË∞ÉÔºåÂ¢ûÂº∫‰∫ÜÊìç‰ΩúÁöÑÂèØÈù†ÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ
3. Âú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÔºåAgentic RobotÁöÑÂπ≥ÂùáÊàêÂäüÁéáËææÂà∞79.6%ÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂü∫Á∫øÊñπÊ≥ïÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈïøÊó∂Èó¥ÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÈù¢‰∏¥ÊòæËëóÊåëÊàòÔºåË¶ÅÊ±ÇÁ≥ªÁªüÂÖ∑Â§áÊâ©Â±ïÊé®ÁêÜ„ÄÅÁ≤æÁ°ÆÊâßË°åÂíåÂº∫Â§ßÁöÑÈîôËØØÊÅ¢Â§çËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÔºåÊó†ËÆ∫ÊòØÂü∫‰∫éÈùôÊÄÅËßÑÂàíËøòÊòØÁ´ØÂà∞Á´ØÁöÑËßÜËßâËøêÂä®Á≠ñÁï•ÔºåÈÉΩÂ≠òÂú®ÈîôËØØÁ¥ØÁßØÂíåÁº∫‰πèÊúâÊïàÈ™åËØÅÊú∫Âà∂ÁöÑÈóÆÈ¢òÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂèØÈù†ÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜAgentic RobotÔºå‰∏Ä‰∏™ÂèóËÑëÂêØÂèëÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÊ†áÂáÜÂåñË°åÂä®Á®ãÂ∫èÔºàSAPÔºâËß£ÂÜ≥Ëøô‰∫õÂ±ÄÈôê„ÄÇSAPÂª∫Á´ã‰∫ÜÁªìÊûÑÂåñÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÊ∂µÁõñËßÑÂàí„ÄÅÊâßË°åÂíåÈ™åËØÅÈò∂ÊÆµ„ÄÇËØ•Êû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ìÈó®ÁªÑ‰ª∂ÔºöÂ§ßÂûãÊé®ÁêÜÊ®°Âûã„ÄÅËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÊâßË°åÂô®ÂíåÊó∂Èó¥È™åËØÅÂô®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAgentic RobotÂú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫Ü79.6%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºåË∂ÖË∂ä‰∫ÜSpatialVLAÂíåOpenVLAÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®È°∫Â∫èÊìç‰Ωú‰∏≠ÁöÑÊÄßËÉΩÂíåÂèØËß£ÈáäÊÄßÊèêÂçá„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÈïøÊó∂Èó¥Êú∫Âô®‰∫∫Êìç‰Ωú‰∏≠ÁöÑÈîôËØØÁ¥ØÁßØÂíåÁº∫‰πèÊúâÊïàÈ™åËØÅÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂØºËá¥ÊâßË°å‰∏çÂèØÈù†„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöAgentic RobotÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•Ê†áÂáÜÂåñË°åÂä®Á®ãÂ∫èÔºàSAPÔºâÔºåÊ®°‰ªø‰∫∫Á±ªÁªÑÁªá‰∏≠ÁöÑÊ†áÂáÜÊìç‰ΩúÁ®ãÂ∫èÔºåÂª∫Á´ãÁªìÊûÑÂåñÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºå‰ªéËÄåÊèêÂçá‰ªªÂä°ÁöÑÊâßË°åÂíåÈ™åËØÅËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÁªÑ‰ª∂Ôºö1) Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºåË¥üË¥£Â∞ÜÈ´òÂ±ÇÊåá‰ª§ÂàÜËß£‰∏∫ËØ≠‰πâ‰∏ÄËá¥ÁöÑÂ≠êÁõÆÊ†áÔºõ2) ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÊâßË°åÂô®ÔºåÂÆûÊó∂ÁîüÊàêÊéßÂà∂Êåá‰ª§Ôºõ3) Êó∂Èó¥È™åËØÅÂô®ÔºåÈÄöËøáËá™ÊàëËØÑ‰º∞ÂÆûÁé∞Ëá™‰∏ªËøõÂ±ïÂíåÈîôËØØÊÅ¢Â§ç„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSAPÈ©±Âä®ÁöÑÈó≠ÁéØËÆæËÆ°ÊîØÊåÅÂä®ÊÄÅËá™ÊàëÈ™åËØÅÔºåÊó†ÈúÄÂ§ñÈÉ®ÁõëÁù£ÔºåËøôÊòØ‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊú¨Ë¥®Âå∫Âà´ÔºåÊòæËëóÊèêÂçá‰∫ÜÁ≥ªÁªüÁöÑÂèØÈù†ÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÊ°ÜÊû∂‰∏≠ÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Êé®ÁêÜÊ®°ÂûãÁöÑÁªìÊûÑ„ÄÅÊâßË°åÂô®ÁöÑÊéßÂà∂ÂëΩ‰ª§ÁîüÊàêÊú∫Âà∂Ôºå‰ª•ÂèäÈ™åËØÅÂô®ÁöÑËá™ÊàëËØÑ‰º∞Á≠ñÁï•ÔºåËøô‰∫õËÆæËÆ°ÂÖ±ÂêåÊîØÊåÅ‰∫ÜÁ≥ªÁªüÁöÑÈ´òÊïàËøêË°å„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

Âú®LIBEROÂü∫ÂáÜÊµãËØï‰∏≠ÔºåAgentic RobotÂÆûÁé∞‰∫Ü79.6%ÁöÑÂπ≥ÂùáÊàêÂäüÁéáÔºåË∂ÖË∂ä‰∫ÜSpatialVLAÂíåOpenVLAÔºåÂàÜÂà´ÊèêÂçá‰∫Ü6.1%Âíå7.4%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåSAPÈ©±Âä®ÁöÑÂçèË∞ÉÊú∫Âà∂ÊòæËëóÂ¢ûÂº∫‰∫ÜÈ°∫Â∫èÊìç‰ΩúÁöÑÊÄßËÉΩÂíåÂèØËß£ÈáäÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Ëá™‰∏ªÊú∫Âô®‰∫∫„ÄÅÊô∫ËÉΩÂà∂ÈÄ†ÂíåÊúçÂä°Êú∫Âô®‰∫∫Á≠âÔºåËÉΩÂ§üÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ËøõË°åÈïøÊó∂Èó¥ÁöÑÊìç‰Ωú‰ªªÂä°„ÄÇÈÄöËøáÊèêÂçáÊú∫Âô®‰∫∫Âú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÂèØÈù†ÊÄßÔºåÊú™Êù•ÂèØËÉΩÊé®Âä®Êõ¥ÂπøÊ≥õÁöÑËá™Âä®ÂåñÂ∫îÁî®ÔºåÊîπÂñÑ‰∫∫Êú∫Âçè‰ΩúÊïàÁéá„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Long-horizon robotic manipulation poses significant challenges for autonomous systems, requiring extended reasoning, precise execution, and robust error recovery across complex sequential tasks. Current approaches, whether based on static planning or end-to-end visuomotor policies, suffer from error accumulation and lack effective verification mechanisms during execution, limiting their reliability in real-world scenarios. We present Agentic Robot, a brain-inspired framework that addresses these limitations through Standardized Action Procedure (SAP)--a novel coordination protocol governing component interactions throughout manipulation tasks. Drawing inspiration from Standardized Operating Procedures (SOPs) in human organizations, SAP establishes structured workflows for planning, execution, and verification phases. Our architecture comprises three specialized components: (1) a large reasoning model that decomposes high-level instructions into semantically coherent subgoals, (2) a vision-language-action executor that generates continuous control commands from real-time visual inputs, and (3) a temporal verifier that enables autonomous progression and error recovery through introspective assessment. This SAP-driven closed-loop design supports dynamic self-verification without external supervision. On the LIBERO benchmark, Agentic Robot achieves state-of-the-art performance with an average success rate of 79.6%, outperforming SpatialVLA by 6.1% and OpenVLA by 7.4% on long-horizon tasks. These results demonstrate that SAP-driven coordination between specialized components enhances both performance and interpretability in sequential manipulation, suggesting significant potential for reliable autonomous systems. Project Github: https://agentic-robot.github.io.

