---
layout: default
title: "AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning"
---

# AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.23708" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.23708v1</a>
  <a href="https://arxiv.org/pdf/2505.23708.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.23708v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.23708v1', 'AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Lucas N. Alegre, Agon Serifi, Ruben Grandia, David MÃ¼ller, Espen Knoop, Moritz BÃ¤cher

**åˆ†ç±»**: cs.RO, cs.GR

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: SIGGRAPH 2025

**DOI**: [10.1145/3721238.3730656](https://doi.org/10.1145/3721238.3730656)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³æœºå™¨äººè§’è‰²æ§åˆ¶é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±å…«ï¼šç‰©ç†åŠ¨ç”» (Physics-based Animation)**

**å…³é”®è¯**: `å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `åŠ¨æ€è¿åŠ¨` `å¥–åŠ±å‡½æ•°` `å±‚æ¬¡ç­–ç•¥` `é€‚åº”æ€§å­¦ä¹ ` `æ¨¡æ‹Ÿä¸ç°å®`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è°ƒä¼˜å¥–åŠ±å‡½æ•°æƒé‡æ—¶è€—æ—¶ä¸”å¤æ‚ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººåº”ç”¨ä¸­ï¼Œå­˜åœ¨æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªå•ä¸€ç­–ç•¥æ¥å¤„ç†ä¸åŒçš„å¥–åŠ±æƒé‡ï¼Œä»è€Œç®€åŒ–äº†è°ƒä¼˜è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰§è¡Œé«˜åº¦åŠ¨æ€çš„è¿åŠ¨ï¼Œå¹¶åœ¨å±‚æ¬¡ç­–ç•¥ä¸­å®ç°åŠ¨æ€æƒé‡é€‰æ‹©ï¼Œé€‚åº”æ–°ä»»åŠ¡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åŸºäºç‰©ç†çš„æœºå™¨äººè§’è‰²æ§åˆ¶æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨è·Ÿè¸ªè¿åŠ¨å­¦å‚è€ƒè¿åŠ¨æ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå†²çªå¥–åŠ±å‡½æ•°çš„åŠ æƒå’Œï¼Œéœ€è¿›è¡Œå¤§é‡è°ƒä¼˜ä»¥å®ç°æœŸæœ›è¡Œä¸ºã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è®­ç»ƒä¸€ä¸ªå•ä¸€ç­–ç•¥ï¼Œæ¡ä»¶æ˜¯æƒé‡é›†ï¼Œæ¶µç›–å¥–åŠ±æƒè¡¡çš„Paretoå‰æ²¿ã€‚è¯¥æ–¹æ³•æ˜¾è‘—åŠ å¿«äº†è¿­ä»£æ—¶é—´ï¼Œå¹¶å±•ç¤ºäº†åœ¨åŠ¨æ€è¿åŠ¨ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å¦‚ä½•åœ¨å±‚æ¬¡è®¾ç½®ä¸­åˆ©ç”¨æƒé‡æ¡ä»¶ç­–ç•¥ï¼Œæ ¹æ®å½“å‰ä»»åŠ¡åŠ¨æ€é€‰æ‹©æƒé‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ§åˆ¶æœºå™¨äººè§’è‰²æ—¶ï¼Œé€šå¸¸ä¾èµ–äºå¤šä¸ªå†²çªçš„å¥–åŠ±å‡½æ•°ï¼Œéœ€è¦å¤§é‡çš„æ‰‹åŠ¨è°ƒä¼˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´å­˜åœ¨å·®è·çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€è¿‡ç¨‹æ˜¾å¾—å°¤ä¸ºç¹çå’Œè€—æ—¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šè¿‡è®­ç»ƒä¸€ä¸ªå•ä¸€ç­–ç•¥ï¼Œæ¡ä»¶æ˜¯ä¸€ä¸ªæƒé‡é›†ï¼Œæ¶µç›–äº†å¥–åŠ±æƒè¡¡çš„Paretoå‰æ²¿ã€‚è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†åœ¨è®­ç»ƒåèƒ½å¤Ÿå¿«é€Ÿé€‰æ‹©å’Œè°ƒæ•´æƒé‡ï¼Œä»è€ŒåŠ é€Ÿè¿­ä»£è¿‡ç¨‹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šè¯¥æ¡†æ¶çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ˜¯ç­–ç•¥è®­ç»ƒæ¨¡å—ï¼Œé€šè¿‡å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒï¼›å…¶æ¬¡æ˜¯æƒé‡é€‰æ‹©æ¨¡å—ï¼Œå…è®¸åœ¨è®­ç»ƒåæ ¹æ®å…·ä½“ä»»åŠ¡åŠ¨æ€é€‰æ‹©æƒé‡ï¼›æœ€åæ˜¯æ‰§è¡Œæ¨¡å—ï¼Œè´Ÿè´£åœ¨å®é™…ç¯å¢ƒä¸­æ‰§è¡Œè®­ç»ƒå¥½çš„ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šç›®æ ‡æ¡†æ¶ï¼Œä½¿å¾—ç­–ç•¥èƒ½å¤Ÿåœ¨ä¸åŒçš„ä»»åŠ¡ä¸­çµæ´»é€‚åº”ï¼Œè€Œä¸éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†è°ƒä¼˜æ—¶é—´å’Œå¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œæ¡†æ¶å…è®¸ç”¨æˆ·åœ¨è®­ç»ƒåé€‰æ‹©æƒé‡ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºèƒ½å¤ŸåŒæ—¶è€ƒè™‘å¤šä¸ªç›®æ ‡çš„æƒè¡¡ï¼Œç½‘ç»œç»“æ„åˆ™é‡‡ç”¨äº†é€‚åº”æ€§å¼ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»¥æ”¯æŒå¤æ‚çš„åŠ¨æ€è¡Œä¸ºç”Ÿæˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨æ‰§è¡ŒåŠ¨æ€è¿åŠ¨æ—¶ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°é€‚åº”æ–°ä»»åŠ¡ï¼Œä¸”åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„çµæ´»æ€§å’Œæ•ˆç‡ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€åŠ¨ç”»ç”Ÿæˆå’Œè™šæ‹Ÿè§’è‰²çš„åŠ¨æ€è¡Œä¸ºæ¨¡æ‹Ÿã€‚é€šè¿‡æé«˜æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œæœªæ¥å¯èƒ½åœ¨å¨±ä¹ã€æ•™è‚²å’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰å¤šä¸ªé¢†åŸŸäº§ç”Ÿé‡è¦å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. However, methods typically rely on a weighted sum of conflicting reward functions, requiring extensive tuning to achieve a desired behavior. Due to the computational cost of RL, this iterative process is a tedious, time-intensive task. Furthermore, for robotics applications, the weights need to be chosen such that the policy performs well in the real world, despite inevitable sim-to-real gaps. To address these challenges, we propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character. Moreover, we explore how weight-conditioned policies can be leveraged in hierarchical settings, using a high-level policy to dynamically select weights according to the current task. We show that the multi-objective policy encodes a diverse spectrum of behaviors, facilitating efficient adaptation to novel tasks.

