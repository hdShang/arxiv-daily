---
layout: default
title: "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics"
---

# Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00070" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00070v1</a>
  <a href="https://arxiv.org/pdf/2506.00070.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00070v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00070v1', 'Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, Younggyo Seo

**åˆ†ç±»**: cs.RO, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29

**å¤‡æ³¨**: 26 pages, 14 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRobot-R1ä»¥è§£å†³æœºå™¨äººæ§åˆ¶ä¸­çš„æ¨ç†èƒ½åŠ›ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å…·èº«æ¨ç†` `å¼ºåŒ–å­¦ä¹ ` `æœºå™¨äººæ§åˆ¶` `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç›‘ç£å¾®è°ƒ` `ä½çº§åŠ¨ä½œæ§åˆ¶` `æ™ºèƒ½æœºå™¨äºº`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æœºå™¨äººæ§åˆ¶ä¸­å­˜åœ¨æ•°æ®é›†æ„å»ºä¸ä¼˜åŒ–å’Œç¾éš¾æ€§é—å¿˜ç­‰é—®é¢˜ã€‚
2. Robot-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºå…·èº«æ¨ç†ï¼Œå­¦ä¹ é¢„æµ‹ä»»åŠ¡æ‰€éœ€çš„å…³é”®ç‚¹çŠ¶æ€ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRobot-R1åœ¨å…·èº«æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„SFTæ–¹æ³•ï¼Œä¸”å‚æ•°é‡ä»…ä¸º7Bã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç»“åˆå…·èº«æ¨ç†ä¸æœºå™¨äººæ§åˆ¶æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†SFTæ•°æ®é›†å¾€å¾€æ˜¯å¯å‘å¼æ„å»ºçš„ï¼Œæœªèƒ½æ˜ç¡®ä¼˜åŒ–æœºå™¨äººæ§åˆ¶ï¼Œä¸”å­˜åœ¨ç¾éš¾æ€§é—å¿˜å’Œæ³›åŒ–æ€§èƒ½ä¸‹é™ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Robot-R1ï¼Œä¸€ä¸ªåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºå…·èº«æ¨ç†çš„æ¡†æ¶ã€‚Robot-R1å­¦ä¹ é¢„æµ‹ä»»åŠ¡å®Œæˆæ‰€éœ€çš„ä¸‹ä¸€ä¸ªå…³é”®ç‚¹çŠ¶æ€ï¼ŒåŸºäºå½“å‰åœºæ™¯å›¾åƒå’Œæ¥è‡ªä¸“å®¶æ¼”ç¤ºçš„ç¯å¢ƒå…ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Robot-R1è®­ç»ƒçš„æ¨¡å‹åœ¨å…·èº«æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºSFTæ–¹æ³•ï¼Œä¸”åœ¨ä½çº§åŠ¨ä½œæ§åˆ¶ç›¸å…³çš„æ¨ç†ä»»åŠ¡ä¸­ï¼ŒRobot-R1çš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†GPT-4oã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨äººæ§åˆ¶æ–¹æ³•ä¸­å…·èº«æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨æ•°æ®é›†æ„å»ºå’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRobot-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼ï¼Œå­¦ä¹ å¦‚ä½•æ ¹æ®å½“å‰åœºæ™¯å›¾åƒå’Œç¯å¢ƒå…ƒæ•°æ®é¢„æµ‹ä»»åŠ¡å®Œæˆæ‰€éœ€çš„ä¸‹ä¸€ä¸ªå…³é”®ç‚¹çŠ¶æ€ï¼Œä»è€Œå¢å¼ºæœºå™¨äººåœ¨æ‰§è¡Œä»»åŠ¡æ—¶çš„æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRobot-R1çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®è¾“å…¥æ¨¡å—ã€æ¨ç†æ¨¡å—å’Œå¼ºåŒ–å­¦ä¹ æ¨¡å—ã€‚æ•°æ®è¾“å…¥æ¨¡å—è´Ÿè´£æ¥æ”¶åœºæ™¯å›¾åƒå’Œç¯å¢ƒå…ƒæ•°æ®ï¼Œæ¨ç†æ¨¡å—è¿›è¡Œå…³é”®ç‚¹çŠ¶æ€é¢„æµ‹ï¼Œå¼ºåŒ–å­¦ä¹ æ¨¡å—åˆ™å¯¹æ¨ç†ç»“æœè¿›è¡Œåé¦ˆå’Œä¼˜åŒ–ã€‚

**å…³é”®åˆ›æ–°**ï¼šRobot-R1çš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå…·èº«æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡æ ·æœ¬æ¨ç†å“åº”å¹¶å¼ºåŒ–é‚£äº›èƒ½å¤Ÿæé«˜é¢„æµ‹å‡†ç¡®æ€§çš„å“åº”ï¼Œä»è€Œæœ‰æ•ˆå…‹æœäº†SFTæ–¹æ³•çš„ä¸è¶³ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒRobot-R1é‡‡ç”¨äº†7Bå‚æ•°çš„è½»é‡çº§æ¨¡å‹ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨ç†ç»“æœçš„å‡†ç¡®æ€§ï¼Œç¡®ä¿åœ¨ä½çº§åŠ¨ä½œæ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ¨¡å‹çš„ç½‘ç»œç»“æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„æ¨ç†å’Œå­¦ä¹ è¿‡ç¨‹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRobot-R1åœ¨å…·èº«æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„SFTæ–¹æ³•ï¼Œå°¤å…¶åœ¨ä½çº§åŠ¨ä½œæ§åˆ¶ç›¸å…³çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¿‡äº†GPT-4oã€‚å…·ä½“è€Œè¨€ï¼ŒRobot-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Robot-R1çš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼ŒåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€ä»¥åŠäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡å¢å¼ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæå‡å…¶è‡ªä¸»å†³ç­–å’Œæ‰§è¡Œä»»åŠ¡çš„æ•ˆç‡ï¼Œæ¨åŠ¨æ™ºèƒ½æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning.

