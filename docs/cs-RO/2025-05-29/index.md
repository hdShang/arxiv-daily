---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.RO - 2025-05-29
---

# cs.ROï¼ˆ2025-05-29ï¼‰

ğŸ“Š å…± **12** ç¯‡è®ºæ–‡
 | ğŸ”— **1** ç¯‡æœ‰ä»£ç 


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9)</a>
<a href="#æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction" class="interest-badge">æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ğŸ”—1)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1)</a>
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (9 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250523505v1-humanoid-loco-manipulation-planning-based-on-graph-search-and-reacha.html">Humanoid Loco-manipulation Planning based on Graph Search and Reachability Maps</a></td>
  <td>æå‡ºé«˜æ•ˆçš„äººå½¢æœºå™¨äººè¿åŠ¨æ“ä½œè§„åˆ’æ–¹æ³•ä»¥è§£å†³ç‰©ä½“è¿è¾“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">manipulation</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23505v1" data-paper-url="./papers/250523505v1-humanoid-loco-manipulation-planning-based-on-graph-search-and-reacha.html" onclick="toggleFavorite(this, '2505.23505v1', 'Humanoid Loco-manipulation Planning based on Graph Search and Reachability Maps')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250523450v2-agentic-robot-a-brain-inspired-framework-for-vision-language-action-.html">Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents</a></td>
  <td>æå‡ºAgentic Robotæ¡†æ¶ä»¥è§£å†³é•¿æ—¶é—´æœºå™¨äººæ“ä½œä¸­çš„é”™è¯¯ç´¯ç§¯é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">vision-language-action</span> <span class="paper-tag">OpenVLA</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23450v2" data-paper-url="./papers/250523450v2-agentic-robot-a-brain-inspired-framework-for-vision-language-action-.html" onclick="toggleFavorite(this, '2505.23450v2', 'Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250523708v1-amor-adaptive-character-control-through-multi-objective-reinforcemen.html">AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning</a></td>
  <td>æå‡ºå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³æœºå™¨äººè§’è‰²æ§åˆ¶é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">sim-to-real</span> <span class="paper-tag">reinforcement learning</span> <span class="paper-tag">character control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23708v1" data-paper-url="./papers/250523708v1-amor-adaptive-character-control-through-multi-objective-reinforcemen.html" onclick="toggleFavorite(this, '2505.23708v1', 'AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250523692v2-mobi-Ï€-mobilizing-your-robot-learning-policy.html">Mobi-$Ï€$: Mobilizing Your Robot Learning Policy</a></td>
  <td>æå‡ºMobi-$Ï€$ä»¥è§£å†³æœºå™¨äººå­¦ä¹ ç­–ç•¥çš„ç§»åŠ¨æ€§é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">manipulation</span> <span class="paper-tag">mobile manipulation</span> <span class="paper-tag">3D gaussian splatting</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23692v2" data-paper-url="./papers/250523692v2-mobi-Ï€-mobilizing-your-robot-learning-policy.html" onclick="toggleFavorite(this, '2505.23692v2', 'Mobi-$Ï€$: Mobilizing Your Robot Learning Policy')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250523499v1-centroidal-trajectory-generation-and-stabilization-based-on-preview-.html">Centroidal Trajectory Generation and Stabilization based on Preview Control for Humanoid Multi-contact Motion</a></td>
  <td>æå‡ºåŸºäºé¢„è§ˆæ§åˆ¶çš„è´¨å¿ƒè½¨è¿¹ç”Ÿæˆä¸ç¨³å®šåŒ–æ–¹æ³•ä»¥è§£å†³ç±»äººæœºå™¨äººå¤šæ¥è§¦è¿åŠ¨é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span> <span class="paper-tag">humanoid robot</span> <span class="paper-tag">model predictive control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23499v1" data-paper-url="./papers/250523499v1-centroidal-trajectory-generation-and-stabilization-based-on-preview-.html" onclick="toggleFavorite(this, '2505.23499v1', 'Centroidal Trajectory Generation and Stabilization based on Preview Control for Humanoid Multi-contact Motion')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>6</td>
  <td><a href="./papers/250523175v2-locotouch-learning-dynamic-quadrupedal-transport-with-tactile-sensin.html">LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing</a></td>
  <td>æå‡ºLocoTouchä»¥è§£å†³å››è¶³æœºå™¨äººåŠ¨æ€ç‰©ä½“è¿è¾“é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23175v2" data-paper-url="./papers/250523175v2-locotouch-learning-dynamic-quadrupedal-transport-with-tactile-sensin.html" onclick="toggleFavorite(this, '2505.23175v2', 'LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250523147v1-eye-tracking-driven-shared-control-for-robotic-armswizard-of-oz-stud.html">Eye-tracking-Driven Shared Control for Robotic Arms:Wizard of Oz Studies to Assess Design Choices</a></td>
  <td>æå‡ºçœ¼åŠ¨è¿½è¸ªé©±åŠ¨çš„å…±äº«æ§åˆ¶ä»¥æ”¹å–„æ®‹ç–¾äººå£«çš„æœºå™¨äººè‡‚æ“ä½œ</td>
  <td class="tags-cell"><span class="paper-tag">shared control</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23147v1" data-paper-url="./papers/250523147v1-eye-tracking-driven-shared-control-for-robotic-armswizard-of-oz-stud.html" onclick="toggleFavorite(this, '2505.23147v1', 'Eye-tracking-Driven Shared Control for Robotic Arms:Wizard of Oz Studies to Assess Design Choices')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>8</td>
  <td><a href="./papers/250524068v1-diffcotune-differentiable-co-tuning-for-cross-domain-robot-control.html">DiffCoTune: Differentiable Co-Tuning for Cross-domain Robot Control</a></td>
  <td>æå‡ºDiffCoTuneä»¥è§£å†³è·¨åŸŸæœºå™¨äººæ§åˆ¶ä¸­çš„è°ƒä¼˜é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">quadruped</span> <span class="paper-tag">biped</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24068v1" data-paper-url="./papers/250524068v1-diffcotune-differentiable-co-tuning-for-cross-domain-robot-control.html" onclick="toggleFavorite(this, '2505.24068v1', 'DiffCoTune: Differentiable Co-Tuning for Cross-domain Robot Control')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>9</td>
  <td><a href="./papers/250523090v1-a-constructed-response-designing-and-choreographing-robot-arm-moveme.html">A Constructed Response: Designing and Choreographing Robot Arm Movements in Collaborative Dance Improvisation</a></td>
  <td>æ¢è®¨æœºå™¨äººè‡‚åœ¨èˆè¹ˆå³å…´åˆ›ä½œä¸­çš„åä½œè®¾è®¡ä¸ç¼–æ’</td>
  <td class="tags-cell"><span class="paper-tag">humanoid</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23090v1" data-paper-url="./papers/250523090v1-a-constructed-response-designing-and-choreographing-robot-arm-moveme.html" onclick="toggleFavorite(this, '2505.23090v1', 'A Constructed Response: Designing and Choreographing Robot Arm Movements in Collaborative Dance Improvisation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±å…­è§†é¢‘æå–ä¸åŒ¹é…-video-extraction">ğŸ”¬ æ”¯æŸ±å…­ï¼šè§†é¢‘æå–ä¸åŒ¹é… (Video Extraction) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>10</td>
  <td><a href="./papers/250523189v1-trackvla-embodied-visual-tracking-in-the-wild.html">TrackVLA: Embodied Visual Tracking in the Wild</a></td>
  <td>æå‡ºTrackVLAä»¥è§£å†³åŠ¨æ€ç¯å¢ƒä¸­çš„è§†è§‰è·Ÿè¸ªé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">egocentric</span> <span class="paper-tag">egocentric vision</span> <span class="paper-tag">embodied AI</span></td>
  <td>âœ…</td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23189v1" data-paper-url="./papers/250523189v1-trackvla-embodied-visual-tracking-in-the-wild.html" onclick="toggleFavorite(this, '2505.23189v1', 'TrackVLA: Embodied Visual Tracking in the Wild')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>11</td>
  <td><a href="./papers/250600070v1-robot-r1-reinforcement-learning-for-enhanced-embodied-reasoning-in-r.html">Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</a></td>
  <td>æå‡ºRobot-R1ä»¥è§£å†³æœºå™¨äººæ§åˆ¶ä¸­çš„æ¨ç†èƒ½åŠ›ä¸è¶³é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00070v1" data-paper-url="./papers/250600070v1-robot-r1-reinforcement-learning-for-enhanced-embodied-reasoning-in-r.html" onclick="toggleFavorite(this, '2506.00070v1', 'Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>12</td>
  <td><a href="./papers/250600075v1-reducing-latency-in-llm-based-natural-language-commands-processing-f.html">Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation</a></td>
  <td>æå‡ºä¸€ç§æ¶æ„ä»¥å‡å°‘æœºå™¨äººå¯¼èˆªä¸­çš„è¯­è¨€å‘½ä»¤å¤„ç†å»¶è¿Ÿ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00075v1" data-paper-url="./papers/250600075v1-reducing-latency-in-llm-based-natural-language-commands-processing-f.html" onclick="toggleFavorite(this, '2506.00075v1', 'Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.RO é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)