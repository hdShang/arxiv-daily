---
layout: default
title: "LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs"
---

# LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24451" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24451v1</a>
  <a href="https://arxiv.org/pdf/2505.24451.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24451v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24451v1', 'LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLPASSä»¥æé«˜å‹ç¼©LLMåœ¨æ¼æ´æ£€æµ‹ä¸­çš„æ•ˆç‡**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¼æ´æ£€æµ‹` `çº¿æ€§æ¢é’ˆ` `å‹ç¼©æŠ€æœ¯` `ç½‘ç»œå®‰å…¨` `æ¨¡å‹è¯„ä¼°` `è®¡ç®—æ•ˆç‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‹ç¼©å’Œå¾®è°ƒæŠ€æœ¯åœ¨æé«˜LLMæ€§èƒ½çš„åŒæ—¶ï¼Œæ¶ˆè€—äº†å¤§é‡çš„è®¡ç®—èµ„æºï¼Œæ•ˆç‡è¾ƒä½ã€‚
2. æœ¬æ–‡æå‡ºäº†LPASSæ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§æ¢é’ˆåœ¨å¾®è°ƒå‰è¯„ä¼°å‹ç¼©LLMçš„æ€§èƒ½ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLPASSæ–¹æ³•åœ¨å¤šç±»æ¼æ´æ£€æµ‹ä¸­è¾¾åˆ°äº†86.9%çš„å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—æé«˜äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸè¢«å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æ£€æµ‹è„†å¼±ä»£ç æ–¹é¢ã€‚ä¸ºäº†æé«˜æ•ˆç‡å’Œæœ‰æ•ˆæ€§ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†å‹ç¼©å’Œå¾®è°ƒæŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚æœ¬æ–‡åˆ†æäº†å¦‚ä½•åˆ©ç”¨çº¿æ€§æ¢é’ˆï¼ˆLPsï¼‰åœ¨å¾®è°ƒä¹‹å‰å¯¹å‹ç¼©LLMçš„æ€§èƒ½è¿›è¡Œæ—©æœŸè¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨åº”ç”¨å±‚ä¿®å‰ªå‹ç¼©æ—¶çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•LPASSåœ¨BERTå’ŒGemmaä¸Šåº”ç”¨äºæ£€æµ‹MITRE 25ä¸ªæœ€å±é™©æ¼æ´ä¸­çš„12ä¸ªï¼Œç»“æœæ˜¾ç¤ºï¼ŒLPsè®¡ç®—æ—¶é—´ä¸º142.97ç§’ï¼Œä¸”å¯ä»¥åœ¨ä¸æŸå¤±ç²¾åº¦çš„æƒ…å†µä¸‹å»é™¤33.3%å’Œ72.2%çš„å±‚ã€‚LPASSåŸºäºçš„LLMsåœ¨å¤šç±»æ¼æ´æ£€æµ‹ä¸­è¾¾åˆ°äº†86.9%çš„å‡†ç¡®ç‡ï¼Œä¸”Gemmaçš„å‹ç¼©ç‰ˆæœ¬åœ¨F1åˆ†æ•°ä¸Šæ¯”åŸå§‹ç‰ˆæœ¬æé«˜äº†1.6%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ä½¿ç”¨å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¼æ´æ£€æµ‹æ—¶ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡å’Œè®¡ç®—èµ„æºæ¶ˆè€—ä¸Šçš„ä¸è¶³ã€‚ç°æœ‰çš„å¾®è°ƒå’Œå‹ç¼©æŠ€æœ¯é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨çº¿æ€§æ¢é’ˆï¼ˆLPsï¼‰åœ¨å¾®è°ƒä¹‹å‰å¯¹å‹ç¼©LLMçš„æ€§èƒ½è¿›è¡Œæ—©æœŸè¯„ä¼°ï¼Œä»è€Œå‡å°‘è®¡ç®—æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶è€…èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå…¨é¢å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿåˆ¤æ–­æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLPASSæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬å‡ ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨çº¿æ€§æ¢é’ˆå¯¹å‹ç¼©åçš„æ¨¡å‹è¿›è¡Œæ€§èƒ½è¯„ä¼°ï¼›å…¶æ¬¡ï¼ŒåŸºäºè¯„ä¼°ç»“æœè¿›è¡Œå±‚ä¿®å‰ªï¼›æœ€åï¼ŒéªŒè¯ä¿®å‰ªåçš„æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†çº¿æ€§æ¢é’ˆåº”ç”¨äºå‹ç¼©LLMçš„æ—©æœŸæ€§èƒ½è¯„ä¼°ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œå¹¶æä¾›äº†æœ‰æ•ˆçš„æ€§èƒ½é¢„æµ‹ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œç ”ç©¶è€…è®¾ç½®äº†ç‰¹å®šçš„å‚æ•°ä»¥ä¼˜åŒ–çº¿æ€§æ¢é’ˆçš„æ€§èƒ½ï¼Œå¹¶é€‰æ‹©äº†é€‚å½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿åœ¨å‹ç¼©è¿‡ç¨‹ä¸­ä¸æŸå¤±æ¨¡å‹çš„ç²¾åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLPASSæ–¹æ³•åœ¨å¤šç±»æ¼æ´æ£€æµ‹ä¸­è¾¾åˆ°äº†86.9%çš„å‡†ç¡®ç‡ï¼Œä¸”åŸºäºLPASSçš„Gemmaå‹ç¼©ç‰ˆæœ¬åœ¨F1åˆ†æ•°ä¸Šæ¯”åŸå§‹ç‰ˆæœ¬æé«˜äº†1.6%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä¸Šåˆ†åˆ«èŠ‚çœäº†29.4%å’Œ23.8%ï¼Œæ¨¡å‹å¤§å°å‡å°‘äº†42.98%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç½‘ç»œå®‰å…¨ã€ä»£ç å®¡æŸ¥å’Œè‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹ç­‰ã€‚é€šè¿‡æé«˜å‹ç¼©LLMåœ¨æ¼æ´æ£€æµ‹ä¸­çš„æ•ˆç‡ï¼ŒLPASSæ–¹æ³•èƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å¿«é€Ÿåœ°è¯†åˆ«å’Œä¿®å¤å®‰å…¨æ¼æ´ï¼Œä»è€Œæå‡è½¯ä»¶çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚æœªæ¥ï¼Œéšç€LLMæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒLPASSæ–¹æ³•å¯èƒ½ä¼šåœ¨æ›´å¤šçš„å®‰å…¨åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) are being extensively used for cybersecurity purposes. One of them is the detection of vulnerable codes. For the sake of efficiency and effectiveness, compression and fine-tuning techniques are being developed, respectively. However, they involve spending substantial computational efforts. In this vein, we analyse how Linear Probes (LPs) can be used to provide an estimation on the performance of a compressed LLM at an early phase -- before fine-tuning. We also show their suitability to set the cut-off point when applying layer pruning compression. Our approach, dubbed $LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25 most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in 142.97 s. and provide key findings: (1) 33.3 \% and 72.2\% of layers can be removed, respectively, with no precision loss; (2) they provide an early estimate of the post-fine-tuning and post-compression model effectiveness, with 3\% and 8.68\% as the lowest and average precision errors, respectively. $LPASS$-based LLMs outperform the state of the art, reaching 86.9\% of accuracy in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed versions of Gemma outperform the original ones by 1.6\% of F1-score at a maximum while saving 29.4 \% and 23.8\% of training and inference time and 42.98\% of model size.

