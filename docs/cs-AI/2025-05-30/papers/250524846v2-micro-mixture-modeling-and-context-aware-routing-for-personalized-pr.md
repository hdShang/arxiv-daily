---
layout: default
title: "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning"
---

# MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24846" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24846v2</a>
  <a href="https://arxiv.org/pdf/2505.24846.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24846v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24846v2', 'MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong Zhang, Han Zhao

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-09-22)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMiCRoæ¡†æ¶ä»¥è§£å†³ä¸ªæ€§åŒ–åå¥½å­¦ä¹ ä¸­çš„å¤šæ ·æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸ªæ€§åŒ–æ¨è` `åå¥½å­¦ä¹ ` `æ··åˆå»ºæ¨¡` `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `åœ¨çº¿è·¯ç”±` `äººç±»åé¦ˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•è¿‡äºç®€åŒ–ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰äººç±»åå¥½çš„å¤šæ ·æ€§ï¼Œé™åˆ¶äº†ä¸ªæ€§åŒ–å’Œå¤šå…ƒå¯¹é½çš„èƒ½åŠ›ã€‚
2. æœ¬æ–‡æå‡ºçš„MiCRoæ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ··åˆå»ºæ¨¡å’Œåœ¨çº¿è·¯ç”±ç­–ç•¥ï¼Œåˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œä¸ªæ€§åŒ–åå¥½å­¦ä¹ ï¼Œé¿å…äº†ç»†ç²’åº¦æ³¨é‡Šçš„éœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMiCRoåœ¨å¤šä¸ªåå¥½æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†ä¸ªæ€§åŒ–æ•ˆæœï¼ŒæˆåŠŸæ•æ‰äº†äººç±»åå¥½çš„å¤šæ ·æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¥–åŠ±å»ºæ¨¡æ˜¯åº”ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼ŒåŸºäºBradley-Terryï¼ˆBTï¼‰æ¨¡å‹çš„å¥–åŠ±å»ºæ¨¡å‡è®¾äº†ä¸€ä¸ªå…¨å±€å¥–åŠ±å‡½æ•°ï¼Œæœªèƒ½æ•æ‰äººç±»åå¥½çš„å¤šæ ·æ€§å’Œå¼‚è´¨æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MiCRoï¼Œä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡äºŒå…ƒåå¥½æ•°æ®é›†æ¥å¢å¼ºä¸ªæ€§åŒ–åå¥½å­¦ä¹ ï¼Œé¿å…äº†æ˜¾å¼çš„ç»†ç²’åº¦æ³¨é‡Šã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒMiCRoå¼•å…¥äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ··åˆå»ºæ¨¡æ–¹æ³•ä»¥æ•æ‰å¤šæ ·çš„äººç±»åå¥½ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒMiCRoæ•´åˆäº†ä¸€ç§åœ¨çº¿è·¯ç”±ç­–ç•¥ï¼Œæ ¹æ®ç‰¹å®šä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ··åˆæƒé‡ï¼Œä»è€Œé«˜æ•ˆåœ°è§£å†³æ¨¡ç³Šæ€§ï¼Œå®ç°åå¥½çš„é€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMiCRoæœ‰æ•ˆæ•æ‰äº†å¤šæ ·çš„äººç±»åå¥½ï¼Œå¹¶æ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä¸ªæ€§åŒ–æ•ˆæœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±å»ºæ¨¡æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰äººç±»åå¥½çš„å¤šæ ·æ€§å’Œå¼‚è´¨æ€§çš„é—®é¢˜ã€‚ç°æœ‰çš„BTæ¨¡å‹å‡è®¾å…¨å±€å¥–åŠ±å‡½æ•°ï¼Œå¯¼è‡´æ— æ³•é€‚åº”ä¸ªæ€§åŒ–éœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMiCRoæ¡†æ¶çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ··åˆå»ºæ¨¡æ¥æ•æ‰å¤šæ ·çš„äººç±»åå¥½ï¼Œå¹¶ç»“åˆåœ¨çº¿è·¯ç”±ç­–ç•¥åŠ¨æ€è°ƒæ•´æ··åˆæƒé‡ï¼Œä»¥æé«˜åå¥½é€‚åº”çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMiCRoæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ··åˆå»ºæ¨¡ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯åœ¨çº¿è·¯ç”±ç­–ç•¥çš„é›†æˆã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆ†æå¤§è§„æ¨¡äºŒå…ƒåå¥½æ•°æ®é›†ï¼Œè¯†åˆ«ä¸åŒçš„åå¥½å­ç¾¤ä½“ï¼›ç¬¬äºŒé˜¶æ®µåˆ™æ ¹æ®å…·ä½“ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°ã€‚

**å…³é”®åˆ›æ–°**ï¼šMiCRoçš„å…³é”®åˆ›æ–°åœ¨äºå…¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ··åˆå»ºæ¨¡å’Œåœ¨çº¿è·¯ç”±ç­–ç•¥çš„ç»“åˆï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ç»†ç²’åº¦æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œçµæ´»é€‚åº”å¤šæ ·çš„äººç±»åå¥½ã€‚è¿™ä¸€è®¾è®¡ä¸ä¼ ç»Ÿæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶åŠ¨æ€æ€§å’Œé€‚åº”æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒMiCRoé‡‡ç”¨äº†æ··åˆæ¨¡å‹çš„å‚æ•°è®¾ç½®ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡è€ƒè™‘äº†å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶ç½‘ç»œç»“æ„ä¸Šå¼•å…¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåå¥½æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMiCRoæ˜¾è‘—æå‡äº†ä¸ªæ€§åŒ–æ•ˆæœï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®å¾…è¡¥å……ï¼‰ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ï¼ŒæˆåŠŸæ•æ‰äº†äººç±»åå¥½çš„å¤šæ ·æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿã€æ™ºèƒ½åŠ©æ‰‹å’Œç”¨æˆ·ä½“éªŒä¼˜åŒ–ç­‰ã€‚é€šè¿‡æœ‰æ•ˆæ•æ‰ç”¨æˆ·çš„å¤šæ ·åŒ–åå¥½ï¼ŒMiCRoèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´åŠ ä¸ªæ€§åŒ–çš„æœåŠ¡ï¼Œæå‡ç”¨æˆ·æ»¡æ„åº¦å’Œå‚ä¸åº¦ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶æœ‰æœ›åœ¨æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.

