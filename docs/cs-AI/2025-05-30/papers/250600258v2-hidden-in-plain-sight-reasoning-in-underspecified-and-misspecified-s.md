---
layout: default
title: Hidden in Plain Sight: Reasoning in Underspecified and Misspecified Scenarios for Multimodal LLMs
---

# Hidden in Plain Sight: Reasoning in Underspecified and Misspecified Scenarios for Multimodal LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00258" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00258v2</a>
  <a href="https://arxiv.org/pdf/2506.00258.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00258v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00258v2', 'Hidden in Plain Sight: Reasoning in Underspecified and Misspecified Scenarios for Multimodal LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Qianqi Yan, Hongquan Li, Shan Jiang, Yang Zhao, Xinze Guan, Ching-Chen Kuo, Xin Eric Wang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-08-25)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºéšæ€§æ¨ç†æ–¹æ³•ä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `éšæ€§æ¨ç†` `æ¨ç†å¹²é¢„` `å¤æ‚è¾“å…¥å¤„ç†` `æ¨¡å‹å¯ä¿¡åº¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¨¡ç³Šå’Œæœªå……åˆ†æŒ‡å®šçš„è¾“å…¥æ—¶è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥è¯†åˆ«æ½œåœ¨é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºé€šè¿‡æ˜ç¡®æç¤ºå’Œæ¨ç†å¹²é¢„æ¥å¢å¼ºæ¨¡å‹çš„éšæ€§æ¨ç†èƒ½åŠ›ï¼Œä»¥æé«˜å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ç®€å•çš„æ¨ç†å¹²é¢„æªæ–½åï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼ŒæˆåŠŸè¯†åˆ«éšè—é—®é¢˜çš„èƒ½åŠ›å¢å¼ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼€æ”¾å¼ã€çœŸå®ç¯å¢ƒä¸­åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†è¾“å…¥å¾€å¾€æ‚ä¹±ã€æœªå……åˆ†æŒ‡å®šä¸”ä¸æ€»æ˜¯å¯ä¿¡ã€‚ä¸ç»è¿‡ç²¾å¿ƒç­–åˆ’çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼Œè¿™äº›ç¯å¢ƒä¸­çš„æŒ‡ä»¤å¸¸å¸¸æ¶‰åŠç¼ºå¤±å¯¹è±¡æˆ–çŸ›ç›¾äº‹å®ï¼Œä¾èµ–æ¨¡ç³Šçš„å¼•ç”¨ï¼Œæˆ–è¯·æ±‚ä¸å¯è¡Œçš„æ“ä½œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒæˆåŠŸä¸ä»…ä¾èµ–äºä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ï¼Œè¿˜ä¾èµ–äºæ¨¡å‹æ£€æµ‹æ½œåœ¨é—®é¢˜çš„èƒ½åŠ›ã€‚æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†å½“å‰MLLMså¦‚ä½•å¤„ç†éšæ€§æ¨ç†åœºæ™¯ï¼Œè¯„ä¼°äº†å…­ç§MLLMsçš„è¡¨ç°ï¼Œå‘ç°å®ƒä»¬åœ¨è¯†åˆ«éšè—é—®é¢˜æ—¶å¸¸å¸¸å¤±è´¥ã€‚é€šè¿‡æ˜ç¡®æç¤ºï¼Œæ¨¡å‹çš„æ½œåœ¨èƒ½åŠ›å¾—ä»¥æ˜¾ç°ï¼Œä½†é€šå¸¸è¢«æŠ‘åˆ¶ä»¥è¿åˆç”¨æˆ·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºäº†ç®€å•çš„æ¨ç†å¹²é¢„æªæ–½ï¼Œå¦‚è°¨æ…çš„è§’è‰²æç¤ºå’Œè¦æ±‚æ¾„æ¸…é—®é¢˜ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ€§èƒ½ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†å½“å‰MLLMsåœ¨æ¨ç†èƒ½åŠ›ä¸è¡Œä¸ºåˆè§„æ€§ä¹‹é—´çš„å·®è·ï¼Œå¹¶æå‡ºäº†æé«˜æ¨¡å‹åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­å¯ä¿¡åº¦çš„å®ç”¨ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éšæ€§æ¨ç†åœºæ™¯æ—¶çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯†åˆ«æœªæ˜ç¡®æŒ‡å‡ºçš„é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆåº”å¯¹å¤æ‚çš„çœŸå®ä¸–ç•Œè¾“å…¥ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç³»ç»Ÿåˆ†æå½“å‰MLLMsçš„è¡¨ç°ï¼Œæå‡ºåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥æ˜ç¡®æç¤ºå’Œå¹²é¢„æªæ–½ï¼Œä»¥æ¿€å‘æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæå‡å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨äº†ä¸€å¥—ç²¾å¿ƒè®¾è®¡çš„è¯Šæ–­å·¥å…·ï¼Œæ¶µç›–å››ç±»çœŸå®ä¸–ç•Œçš„å¤±è´¥æ¨¡å¼ï¼Œå¯¹å…­ç§MLLMsè¿›è¡Œè¯„ä¼°ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬è¾“å…¥åˆ†æã€éšæ€§é—®é¢˜è¯†åˆ«å’Œæ¨ç†å¹²é¢„ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè¯†åˆ«å¹¶åˆ©ç”¨æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„æ½œåŠ›ï¼Œé€šè¿‡æ˜ç¡®çš„æç¤ºå’Œå¹²é¢„æªæ–½ï¼Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨éšæ€§æ¨ç†åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•å¼ºè°ƒä»»åŠ¡æ‰§è¡Œçš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨äº†è°¨æ…çš„è§’è‰²æç¤ºå’Œè¦æ±‚æ¾„æ¸…é—®é¢˜çš„ç­–ç•¥ï¼Œè¿™äº›è®¾è®¡åœ¨æ¨ç†è¿‡ç¨‹ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°è¯†åˆ«å’Œåº”å¯¹æ½œåœ¨é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨æ˜ç¡®æç¤ºå’Œæ¨ç†å¹²é¢„åï¼Œæ¨¡å‹åœ¨éšæ€§æ¨ç†åœºæ™¯ä¸­çš„è¡¨ç°æ˜¾è‘—æå‡ï¼ŒæˆåŠŸè¯†åˆ«éšè—é—®é¢˜çš„èƒ½åŠ›æé«˜äº†çº¦30%ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ”¹è¿›åçš„æ¨¡å‹åœ¨å¤æ‚è¾“å…¥å¤„ç†ä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—å¢åŠ ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰éœ€è¦å¤„ç†å¤æ‚å’Œæ¨¡ç³Šè¾“å…¥çš„åœºæ™¯ã€‚é€šè¿‡æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­çš„å¯ä¿¡åº¦ï¼Œèƒ½å¤Ÿå¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multimodal large language models (MLLMs) are increasingly deployed in open-ended, real-world environments where inputs are messy, underspecified, and not always trustworthy. Unlike curated benchmarks, these settings frequently involve instructions that refer to missing objects or contradictory facts, rely on ambiguous references, or request infeasible actions. In such cases, success hinges not on task execution alone, but on a model's ability to detect when something is silently wrong. This paper presents a systematic analysis of how current MLLMs handle such implicit reasoning scenarios: cases where the flaw is not explicitly stated but must be inferred from context. Using a curated diagnostic suite spanning four categories of real-world failure modes, we evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently fail to surface hidden issues, even when they possess the necessary perceptual and reasoning skills. Explicit prompting reveals that the underlying capabilities exist but are often suppressed in favor of user compliance. We further show that simple inference-time interventions, such as cautious persona prompting and, in particular, requiring a clarifying question, can dramatically recover performance. Our findings highlight a persistent gap between reasoning competence and behavioral compliance in current MLLMs and suggest practical strategies for making these models more trustworthy in underconstrained environments.

