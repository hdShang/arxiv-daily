---
layout: default
title: Evaluation of LLMs for mathematical problem solving
---

# Evaluation of LLMs for mathematical problem solving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00309" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00309v3</a>
  <a href="https://arxiv.org/pdf/2506.00309.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00309v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00309v3', 'Evaluation of LLMs for mathematical problem solving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ruonan Wang, Runxi Wang, Yunwen Shen, Chengfeng Wu, Qinglin Zhou, Rohitash Chandra

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°å­¦é—®é¢˜æ±‚è§£` `ç»“æ„åŒ–æ€ç»´é“¾` `æ¨¡å‹è¯„ä¼°` `æ•™è‚²æŠ€æœ¯` `æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå­˜åœ¨å‡†ç¡®æ€§å’Œç¨³å®šæ€§ä¸è¶³çš„é—®é¢˜ã€‚
2. æœ¬ç ”ç©¶é€šè¿‡æ¯”è¾ƒä¸‰ç§ä¸»è¦çš„LLMsï¼Œé‡‡ç”¨ç»“æ„åŒ–æ€ç»´é“¾æ¡†æ¶ï¼Œä»å¤šä¸ªç»´åº¦è¯„ä¼°å…¶æ•°å­¦é—®é¢˜æ±‚è§£èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oåœ¨å„æ•°æ®é›†ä¸­çš„è¡¨ç°æœ€ä¸ºç¨³å®šï¼Œå°¤å…¶åœ¨é«˜éš¾åº¦é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä»–æ¨¡å‹åˆ™åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°å‡ºè‰²ä½†å­˜åœ¨ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§æ•™è‚²ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶æ¯”è¾ƒäº†ä¸‰ç§ä¸»è¦çš„LLMsï¼ŒåŒ…æ‹¬GPT-4oã€DeepSeek-V3å’ŒGemini-2.0ï¼Œä½¿ç”¨GSM8Kã€MATH500å’ŒMITå¼€æ”¾è¯¾ç¨‹ç­‰ä¸‰ç§ä¸åŒå¤æ‚åº¦çš„æ•°å­¦æ•°æ®é›†ã€‚æˆ‘ä»¬åŸºäºç»“æ„åŒ–æ€ç»´é“¾ï¼ˆSCoTï¼‰æ¡†æ¶ï¼Œä»æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§ã€æ­¥éª¤å®Œæ•´æ€§ã€æ­¥éª¤æœ‰æ•ˆæ€§ã€ä¸­é—´è®¡ç®—å‡†ç¡®æ€§å’Œé—®é¢˜ç†è§£äº”ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨æ‰€æœ‰æ•°æ®é›†ä¸­è¡¨ç°æœ€ä¸ºç¨³å®šï¼Œå°¤å…¶åœ¨MITå¼€æ”¾è¯¾ç¨‹æ•°æ®é›†çš„é«˜éš¾åº¦é—®é¢˜ä¸Šè¡¨ç°çªå‡ºã€‚DeepSeek-V3åœ¨ç»“æ„è‰¯å¥½çš„é¢†åŸŸå¦‚ä¼˜åŒ–ä¸­è¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨ç»Ÿè®¡æ¨æ–­ä»»åŠ¡ä¸­å‡†ç¡®æ€§æ³¢åŠ¨è¾ƒå¤§ã€‚Gemini-2.0åœ¨ç»“æ„è‰¯å¥½çš„é—®é¢˜ä¸­å±•ç°å‡ºå¼ºå¤§çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨å¤šæ­¥éª¤æ¨ç†å’Œç¬¦å·é€»è¾‘æ–¹é¢è¡¨ç°ä¸ä½³ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„è¡¨ç°ï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œç¨³å®šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤æ‚é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ¯”è¾ƒä¸‰ç§ä¸»æµçš„LLMsï¼Œé‡‡ç”¨ç»“æ„åŒ–æ€ç»´é“¾ï¼ˆSCoTï¼‰æ¡†æ¶ï¼Œä»å¤šä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œä»¥å…¨é¢äº†è§£æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€æ¨¡å‹æ¯”è¾ƒã€è¯„ä¼°ç»´åº¦è®¾å®šç­‰ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ç­”æ¡ˆæ­£ç¡®æ€§ã€æ­¥éª¤å®Œæ•´æ€§ã€æ­¥éª¤æœ‰æ•ˆæ€§ã€ä¸­é—´è®¡ç®—å‡†ç¡®æ€§å’Œé—®é¢˜ç†è§£ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºé‡‡ç”¨ç»“æ„åŒ–æ€ç»´é“¾æ¡†æ¶è¿›è¡Œå¤šç»´åº¦è¯„ä¼°ï¼Œæä¾›äº†æ›´å…¨é¢çš„æ¨¡å‹æ€§èƒ½åˆ†æï¼Œä¸ä¼ ç»Ÿå•ä¸€ç»´åº¦è¯„ä¼°æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†å…·ä½“çš„è¯„ä¼°æ ‡å‡†å’ŒæŒ‡æ ‡ï¼Œç¡®ä¿æ¯ä¸ªç»´åº¦çš„è¯„ä¼°å…·æœ‰å¯æ“ä½œæ€§å’Œå¯é‡å¤æ€§ï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨æ‰€æœ‰æ•°æ®é›†ä¸­çš„è¡¨ç°æœ€ä¸ºç¨³å®šï¼Œå°¤å…¶åœ¨MITå¼€æ”¾è¯¾ç¨‹æ•°æ®é›†çš„é«˜éš¾åº¦é—®é¢˜ä¸Šè¡¨ç°çªå‡ºï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºå…¶ä»–æ¨¡å‹ã€‚DeepSeek-V3åœ¨ä¼˜åŒ–é¢†åŸŸè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨ç»Ÿè®¡æ¨æ–­ä»»åŠ¡ä¸­å‡†ç¡®æ€§æ³¢åŠ¨è¾ƒå¤§ã€‚Gemini-2.0åœ¨è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šæ­¥éª¤æ¨ç†ä¸­å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„ç»“æœå¯¹æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œæ•°å­¦æ•™è‚²é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„èƒ½åŠ›ï¼Œå¯ä»¥ä¸ºå­¦ç”Ÿæä¾›æ›´æ™ºèƒ½çš„å­¦ä¹ æ”¯æŒï¼Œä¿ƒè¿›ä¸ªæ€§åŒ–æ•™è‚²çš„å‘å±•ã€‚æœªæ¥ï¼Œè¿™äº›æ¨¡å‹æœ‰æ½œåŠ›åœ¨æ›´å¹¿æ³›çš„å­¦ç§‘é¢†åŸŸä¸­åº”ç”¨ï¼Œæ¨åŠ¨æ•™è‚²æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have shown impressive performance on a range of educational tasks, but are still understudied for their potential to solve mathematical problems. In this study, we compare three prominent LLMs, including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of varying complexities (GSM8K, MATH500, and MIT Open Courseware datasets). We take a five-dimensional approach based on the Structured Chain-of-Thought (SCoT) framework to assess final answer correctness, step completeness, step validity, intermediate calculation accuracy, and problem comprehension. The results show that GPT-4o is the most stable and consistent in performance across all the datasets, but particularly it performs outstandingly in high-level questions of the MIT Open Courseware dataset. DeepSeek-V3 is competitively strong in well-structured domains such as optimisation, but suffers from fluctuations in accuracy in statistical inference tasks. Gemini-2.0 shows strong linguistic understanding and clarity in well-structured problems but performs poorly in multi-step reasoning and symbolic logic. Our error analysis reveals particular deficits in each model: GPT-4o is at times lacking in sufficient explanation or precision; DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in mathematical reasoning in higher dimensions.

