---
layout: default
title: "RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation"
---

# RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24442" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24442v1</a>
  <a href="https://arxiv.org/pdf/2505.24442.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24442v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24442v1', 'RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhentao Xie, Chengcheng Han, Jinxin Shi, Wenjun Cui, Xin Zhao, Xingjiao Wu, Jiabao Zhao

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

**å¤‡æ³¨**: Accepted by ACL 2025 (Findings)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/mindhunter01/RMoA)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRMoAä»¥ä¼˜åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ•ˆç‡ä¸å¯é æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `æ®‹å·®å­¦ä¹ ` `ä¿¡æ¯åˆ©ç”¨` `è®¡ç®—æ•ˆç‡` `è‡ªç„¶è¯­è¨€å¤„ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´é«˜è®¡ç®—å¼€é”€å’Œä¿¡æ¯æŸå¤±çš„é—®é¢˜ï¼Œå½±å“äº†å…¶é²æ£’æ€§å’Œæ•ˆç‡ã€‚
2. æœ¬æ–‡æå‡ºçš„RMoAé€šè¿‡é›†æˆæ®‹å·®è¿æ¥å’ŒåµŒå…¥å¼å¤šæ ·æ€§é€‰æ‹©æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜ä¿¡æ¯åˆ©ç”¨ç‡å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚
3. RMoAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å¯¹é½å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½å¹¶å‡å°‘äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»å—é™äºé«˜è®¡ç®—å¼€é”€ã€ä¿¡æ¯æŸå¤±å’Œé²æ£’æ€§ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†æ®‹å·®æ··åˆæ™ºèƒ½ä½“ï¼ˆRMoAï¼‰ï¼Œé€šè¿‡é›†æˆæ®‹å·®è¿æ¥æ¥ä¼˜åŒ–æ•ˆç‡å’Œå¯é æ€§ã€‚ä¸ºæœ€å¤§åŒ–æ¨¡å‹å“åº”çš„ä¿¡æ¯åˆ©ç”¨ï¼ŒåŒæ—¶æœ€å°åŒ–è®¡ç®—æˆæœ¬ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºåµŒå…¥çš„å¤šæ ·æ€§é€‰æ‹©æœºåˆ¶ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†æ®‹å·®æå–æ™ºèƒ½ä½“ä»¥ä¿ç•™è·¨å±‚å¢é‡ä¿¡æ¯ï¼Œå¹¶ç»“åˆæ®‹å·®èšåˆæ™ºèƒ½ä½“è¿›è¡Œå±‚æ¬¡ä¿¡æ¯æ•´åˆã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç»ˆæ­¢æœºåˆ¶ï¼Œæ ¹æ®æ®‹å·®æ”¶æ•›åŠ¨æ€åœæ­¢å¤„ç†ï¼Œè¿›ä¸€æ­¥æé«˜æ¨ç†æ•ˆç‡ã€‚RMoAåœ¨å¯¹é½ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¤šä»»åŠ¡ç†è§£ç­‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é«˜è®¡ç®—å¼€é”€å’Œä¿¡æ¯æŸå¤±æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡å¤„ç†ä¸­çš„é²æ£’æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRMoAçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é›†æˆæ®‹å·®è¿æ¥æ¥ä¼˜åŒ–ä¿¡æ¯æµåŠ¨ï¼ŒåŒæ—¶å¼•å…¥å¤šæ ·æ€§é€‰æ‹©æœºåˆ¶ï¼Œä»¥æé«˜æ¨¡å‹å“åº”çš„ä¿¡æ¯åˆ©ç”¨ç‡å’Œè®¡ç®—æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRMoAçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ®‹å·®æå–æ™ºèƒ½ä½“å’Œæ®‹å·®èšåˆæ™ºèƒ½ä½“ï¼Œå‰è€…ç”¨äºæ•æ‰è·¨å±‚å“åº”å·®å¼‚ï¼Œåè€…åˆ™è´Ÿè´£æ•´åˆå±‚æ¬¡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”ç»ˆæ­¢æœºåˆ¶ï¼Œä»¥åŠ¨æ€è°ƒæ•´å¤„ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šRMoAçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥æ®‹å·®å­¦ä¹ æœºåˆ¶å’ŒåµŒå…¥å¼å¤šæ ·æ€§é€‰æ‹©ï¼Œæ˜¾è‘—æå‡äº†ä¿¡æ¯ä¿ç•™èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ï¼Œè¿™ä¸ä¼ ç»Ÿæ–¹æ³•çš„çº¿æ€§å¤„ç†æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è´ªå©ªé€‰æ‹©ç­–ç•¥ï¼Œç»“åˆæ®‹å·®æå–å’Œèšåˆçš„æ¨¡å—åŒ–è®¾è®¡ï¼Œç¡®ä¿äº†ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆä¸åˆ©ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRMoAåœ¨å¯¹é½ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¤šä»»åŠ¡ç†è§£æ–¹é¢å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œè®¡ç®—å¼€é”€æ˜¾è‘—é™ä½ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RMoAçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œå¤æ‚ä»»åŠ¡çš„è‡ªåŠ¨åŒ–å¤„ç†ã€‚å…¶ä¼˜åŒ–çš„è®¡ç®—æ•ˆç‡å’Œä¿¡æ¯åˆ©ç”¨ç‡å°†æ¨åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠä¸å‘å±•ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿå“åº”å’Œé«˜å‡†ç¡®åº¦çš„åœºæ™¯ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although multi-agent systems based on large language models show strong capabilities on multiple tasks, they are still limited by high computational overhead, information loss, and robustness. Inspired by ResNet's residual learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual connections to optimize efficiency and reliability. To maximize information utilization from model responses while minimizing computational costs, we innovatively design an embedding-based diversity selection mechanism that greedily selects responses via vector similarity. Furthermore, to mitigate iterative information degradation, we introduce a Residual Extraction Agent to preserve cross-layer incremental information by capturing inter-layer response differences, coupled with a Residual Aggregation Agent for hierarchical information integration. Additionally, we propose an adaptive termination mechanism that dynamically halts processing based on residual convergence, further improving inference efficiency. RMoA achieves state-of-the-art performance on the benchmarks of across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. Code is available at https://github.com/mindhunter01/RMoA.

