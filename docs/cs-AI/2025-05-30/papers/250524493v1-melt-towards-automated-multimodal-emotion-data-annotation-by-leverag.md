---
layout: default
title: "MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge"
---

# MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24493" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24493v1</a>
  <a href="https://arxiv.org/pdf/2505.24493.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24493v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24493v1', 'MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Jing, Jiadong Wang, Iosif Tsangko, Andreas Triantafyllopoulos, BjÃ¶rn W. Schuller

**åˆ†ç±»**: cs.AI, cs.SD, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMELTä»¥è§£å†³æƒ…æ„Ÿæ•°æ®æ ‡æ³¨çš„è‡ªåŠ¨åŒ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æƒ…æ„Ÿè¯†åˆ«` `å¤šæ¨¡æ€æ•°æ®` `è‡ªåŠ¨æ ‡æ³¨` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªç›‘ç£å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æƒ…æ„Ÿæ•°æ®æ ‡æ³¨æ–¹æ³•ä¾èµ–äººå·¥ï¼Œæˆæœ¬é«˜ä¸”å­˜åœ¨æ ‡æ³¨ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå½±å“äº†æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGPT-4oçš„è‡ªåŠ¨åŒ–æ ‡æ³¨æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–æ–‡æœ¬æç¤ºå®ç°å¯¹å¤šæ¨¡æ€æ•°æ®é›†çš„æƒ…æ„Ÿæ ‡æ³¨ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒMELTæ•°æ®é›†åœ¨æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†äººå·¥æ ‡æ³¨ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦éšœç¢ã€‚äººå·¥æ ‡æ³¨ä¸ä»…æˆæœ¬é«˜æ˜‚ï¼Œè¿˜å­˜åœ¨ä¸ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œæ ‡æ³¨è€…çš„åå¥½ä¸åŒä¸”å¯èƒ½ç¼ºä¹å¿…è¦çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œå¯¼è‡´æ ‡ç­¾çš„å¤šæ ·æ€§å’Œä¸å‡†ç¡®æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºä¸€ç§å¯æ‰©å±•çš„æ–‡æœ¬æ•°æ®æ ‡æ³¨æ›¿ä»£æ–¹æ¡ˆé€æ¸å´­éœ²å¤´è§’ï¼Œä½†å…¶åœ¨æƒ…æ„Ÿè¯­éŸ³æ•°æ®æ ‡æ³¨ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡åº”ç”¨GPT-4oå¯¹æ¥è‡ªæƒ…æ™¯å–œå‰§ã€Šè€å‹è®°ã€‹çš„å¤šæ¨¡æ€æ•°æ®é›†è¿›è¡Œæ ‡æ³¨ï¼Œä»…ä½¿ç”¨æ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥ã€‚é€šè¿‡æ„å»ºç»“æ„åŒ–æ–‡æœ¬æç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†GPT-4oåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç§¯ç´¯çš„çŸ¥è¯†ï¼Œå±•ç¤ºäº†å…¶åœ¨æ²¡æœ‰ç›´æ¥æ¥è§¦å¤šæ¨¡æ€è¾“å…¥çš„æƒ…å†µä¸‹ç”Ÿæˆå‡†ç¡®ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„æ ‡æ³¨çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MELTï¼Œä¸€ä¸ªå®Œå…¨ç”±GPT-4oæ ‡æ³¨çš„å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å¾®è°ƒå››ä¸ªè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰éª¨å¹²ç½‘ç»œè¯„ä¼°äº†æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ï¼Œå®éªŒç»“æœæ˜¾ç¤ºSERæ€§èƒ½æœ‰ä¸€è‡´æ€§æå‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æƒ…æ„Ÿæ•°æ®æ ‡æ³¨ä¸­çš„äººå·¥æˆæœ¬é«˜ã€æ ‡æ³¨ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œå¯¼è‡´æ ‡ç­¾çš„å¤šæ ·æ€§å’Œä¸å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨GPT-4oçš„çŸ¥è¯†ï¼Œé€šè¿‡ç»“æ„åŒ–æ–‡æœ¬æç¤ºå®ç°å¯¹æƒ…æ„Ÿè¯­éŸ³æ•°æ®çš„è‡ªåŠ¨æ ‡æ³¨ï¼Œé¿å…äº†äººå·¥å¹²é¢„ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ–‡æœ¬æç¤ºè®¾è®¡ã€GPT-4oæ ‡æ³¨å’Œç»“æœè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç„¶åè®¾è®¡ç»“æ„åŒ–æç¤ºä»¥å¼•å¯¼GPT-4oè¿›è¡Œæ ‡æ³¨ï¼Œæœ€åè¯„ä¼°æ ‡æ³¨ç»“æœçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºæƒ…æ„Ÿè¯­éŸ³æ•°æ®çš„è‡ªåŠ¨æ ‡æ³¨ï¼Œå±•ç¤ºäº†å…¶åœ¨æ²¡æœ‰å¤šæ¨¡æ€è¾“å…¥çš„æƒ…å†µä¸‹ä»èƒ½ç”Ÿæˆé«˜è´¨é‡æ ‡æ³¨çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æ–‡æœ¬æç¤ºæ ¼å¼ï¼Œä»¥æœ€å¤§åŒ–GPT-4oçš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¾®è°ƒè‡ªç›‘ç£å­¦ä¹ ç½‘ç»œæ¥ä¼˜åŒ–æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ã€‚å…·ä½“çš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MELTæ•°æ®é›†è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¾®è°ƒè‡ªç›‘ç£å­¦ä¹ éª¨å¹²ç½‘ç»œåï¼ŒSERçš„å‡†ç¡®ç‡æé«˜äº†X%ï¼ˆå…·ä½“æ•°æ®éœ€æ ¹æ®å®éªŒç»“æœå¡«å†™ï¼‰ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€æ™ºèƒ½å®¢æœã€ç¤¾äº¤åª’ä½“ç›‘æµ‹ç­‰ã€‚é€šè¿‡è‡ªåŠ¨åŒ–æƒ…æ„Ÿæ•°æ®æ ‡æ³¨ï¼Œå¯ä»¥æ˜¾è‘—é™ä½äººå·¥æˆæœ¬ï¼Œæé«˜æ•°æ®å¤„ç†æ•ˆç‡ï¼Œæ¨åŠ¨æƒ…æ„Ÿè¯†åˆ«æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Although speech emotion recognition (SER) has advanced significantly with deep learning, annotation remains a major hurdle. Human annotation is not only costly but also subject to inconsistencies annotators often have different preferences and may lack the necessary contextual knowledge, which can lead to varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have emerged as a scalable alternative for annotating text data. However, the potential of LLMs to perform emotional speech data annotation without human supervision has yet to be thoroughly investigated. To address these problems, we apply GPT-4o to annotate a multimodal dataset collected from the sitcom Friends, using only textual cues as inputs. By crafting structured text prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated during its training, showcasing that it can generate accurate and contextually relevant annotations without direct access to multimodal inputs. Therefore, we propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We demonstrate the effectiveness of MELT by fine-tuning four self-supervised learning (SSL) backbones and assessing speech emotion recognition performance across emotion datasets. Additionally, our subjective experiments\' results demonstrate a consistence performance improvement on SER.

