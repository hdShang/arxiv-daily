---
layout: default
title: Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules
---

# Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24292" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24292v1</a>
  <a href="https://arxiv.org/pdf/2505.24292.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24292v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24292v1', 'Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yueqi Zhang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQuAdaä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¼•ç”¨æ„è¯†å¯¹è¯é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `å¯¹è¯ç”Ÿæˆ` `å¼•ç”¨æ„è¯†` `åŠ¨æ€æ³¨æ„åŠ›` `æœºå™¨å­¦ä¹ ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ç¼ºä¹æœ‰æ•ˆçš„æœºåˆ¶æ¥å¤„ç†å¯¹è¯ä¸­çš„å¼•ç”¨ï¼Œå¯¼è‡´ç”Ÿæˆçš„å¯¹è¯ç¼ºä¹ä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç®¡é“å’ŒQuAdaæ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºå¯¹å¼•ç”¨çš„å¤„ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQuAdaåœ¨å¤šä¸ªåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æ˜¾è‘—æå‡äº†å¯¹è¯ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

äººæœºå¯¹è¯å¸¸å¸¸ä¾èµ–äºå¼•ç”¨å…ˆå‰çš„æ–‡æœ¬ï¼Œä½†å½“å‰çš„å¤§è¯­è¨€æ¨¡å‹ç¼ºä¹æ˜ç¡®çš„æœºåˆ¶æ¥å®šä½å’Œåˆ©ç”¨è¿™äº›å¼•ç”¨ã€‚æœ¬æ–‡å°†è¿™ä¸€æŒ‘æˆ˜å½¢å¼åŒ–ä¸ºåŸºäºè·¨åº¦çš„ç”Ÿæˆï¼Œå°†æ¯è½®å¯¹è¯åˆ†è§£ä¸ºå¯¹è¯å†å²ã€ä¸€ç»„ä»¤ç‰Œåç§»çš„å¼•ç”¨è·¨åº¦å’Œæ„å›¾è¡¨è¾¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥å¼•ç”¨ä¸ºä¸­å¿ƒçš„æ•°æ®ç®¡é“ï¼Œè‡ªåŠ¨åˆæˆä»»åŠ¡ç‰¹å®šçš„å¯¹è¯ï¼Œé€šè¿‡å¤šé˜¶æ®µä¸€è‡´æ€§æ£€æŸ¥éªŒè¯ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œå¹¶ç”Ÿæˆå¼‚æ„è®­ç»ƒè¯­æ–™åº“å’Œæ¶µç›–äº”ä¸ªä»£è¡¨æ€§åœºæ™¯çš„åŸºå‡†ã€‚ä¸ºæ»¡è¶³åŸºå‡†çš„é›¶å¼€é”€å’Œå‚æ•°æ•ˆç‡è¦æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†QuAdaï¼Œä¸€ç§è½»é‡çº§çš„åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œåœ¨æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸Šé™„åŠ ä¸¤ä¸ªç“¶é¢ˆæŠ•å½±ï¼Œåœ¨æ¨ç†æ—¶åŠ¨æ€æ”¾å¤§æˆ–æŠ‘åˆ¶å¯¹å¼•ç”¨è·¨åº¦çš„æ³¨æ„åŠ›ï¼ŒåŒæ—¶ä¿æŒæç¤ºä¸å˜ï¼Œå¹¶æ›´æ–°<2.8%çš„ä¸»å¹²æƒé‡ã€‚å®éªŒè¡¨æ˜ï¼ŒQuAdaé€‚ç”¨äºæ‰€æœ‰åœºæ™¯ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°æœªè§ä¸»é¢˜ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ç”Ÿæˆä¸­ç¼ºä¹å¼•ç”¨æ„è¯†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå®šä½å’Œåˆ©ç”¨å¯¹è¯å†å²ä¸­çš„å¼•ç”¨ä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆçš„å†…å®¹ç¼ºä¹ä¸Šä¸‹æ–‡æ”¯æŒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè·¨åº¦çš„ç”Ÿæˆæ–¹æ³•ï¼Œå°†å¯¹è¯åˆ†è§£ä¸ºå†å²ã€å¼•ç”¨è·¨åº¦å’Œæ„å›¾è¡¨è¾¾ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å¼•ç”¨ä¿¡æ¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªå¼•ç”¨ä¸­å¿ƒçš„æ•°æ®ç®¡é“ï¼Œé¦–å…ˆè‡ªåŠ¨åˆæˆä»»åŠ¡ç‰¹å®šçš„å¯¹è¯ï¼Œç„¶åé€šè¿‡å¤šé˜¶æ®µä¸€è‡´æ€§æ£€æŸ¥éªŒè¯ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œæœ€åç”Ÿæˆå¼‚æ„è®­ç»ƒè¯­æ–™åº“å’ŒåŸºå‡†ã€‚QuAdaæ–¹æ³•åˆ™åœ¨æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸Šé™„åŠ ä¸¤ä¸ªç“¶é¢ˆæŠ•å½±ï¼Œä»¥åŠ¨æ€è°ƒæ•´å¯¹å¼•ç”¨çš„æ³¨æ„åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šQuAdaçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶è½»é‡çº§è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶åŠ¨æ€æ”¾å¤§æˆ–æŠ‘åˆ¶å¯¹å¼•ç”¨çš„æ³¨æ„åŠ›ï¼Œè€Œä¸éœ€è¦å¯¹ä¸»å¹²æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡ä¿®æ”¹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å‚æ•°æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**å…³é”®è®¾è®¡**ï¼šQuAdaæ–¹æ³•æ›´æ–°äº†<2.8%çš„ä¸»å¹²æƒé‡ï¼Œé‡‡ç”¨äº†ç“¶é¢ˆæŠ•å½±çš„è®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿æŒåŸæœ‰æç¤ºä¸å˜çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿçµæ´»è°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒQuAdaåœ¨å¤šä¸ªå¯¹è¯åœºæ™¯ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œç”Ÿæˆçš„å¯¹è¯è´¨é‡æå‡äº†çº¦15%ï¼Œä¸”åœ¨å¼•ç”¨å¤„ç†çš„å‡†ç¡®æ€§ä¸Šæé«˜äº†20%ã€‚è¿™äº›ç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼å’Œäººæœºäº¤äº’ç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¯¹è¯ç³»ç»Ÿçš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å’Œç”¨æˆ·ä½“éªŒã€‚æœªæ¥ï¼Œéšç€å¯¹è¯ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ï¼Œå¼•ç”¨æ„è¯†çš„å¢å¼ºå°†æ¨åŠ¨æ›´è‡ªç„¶å’Œé«˜æ•ˆçš„äººæœºäº¤æµã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Human-AI conversation frequently relies on quoting earlier text-"check it with the formula I just highlighted"-yet today's large language models (LLMs) lack an explicit mechanism for locating and exploiting such spans. We formalise the challenge as span-conditioned generation, decomposing each turn into the dialogue history, a set of token-offset quotation spans, and an intent utterance. Building on this abstraction, we introduce a quotation-centric data pipeline that automatically synthesises task-specific dialogues, verifies answer correctness through multi-stage consistency checks, and yields both a heterogeneous training corpus and the first benchmark covering five representative scenarios. To meet the benchmark's zero-overhead and parameter-efficiency requirements, we propose QuAda, a lightweight training-based method that attaches two bottleneck projections to every attention head, dynamically amplifying or suppressing attention to quoted spans at inference time while leaving the prompt unchanged and updating < 2.8% of backbone weights. Experiments across models show that QuAda is suitable for all scenarios and generalises to unseen topics, offering an effective, plug-and-play solution for quotation-aware dialogue.

