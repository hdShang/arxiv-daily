---
layout: default
title: TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents
---

# TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00089" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00089v2</a>
  <a href="https://arxiv.org/pdf/2506.00089.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00089v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00089v2', 'TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Hyundong Jin, Sicheol Sung, Shinwoo Park, SeungYeop Baik, Yo-Sub Han

**åˆ†ç±»**: cs.CY, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-09-28)

**å¤‡æ³¨**: EMNLP 2025 Findings

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/jindong22/TrapDoc)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºTRAPDOCä»¥è§£å†³ç”¨æˆ·å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿‡åº¦ä¾èµ–é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å¹»å½±æ ‡è®°` `ä¿¡æ¯å®‰å…¨` `ç”¨æˆ·è¡Œä¸º` `ç¤¾ä¼šè´£ä»»`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç”¨æˆ·å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿‡åº¦ä¾èµ–å¯¼è‡´äº†ä½œä¸šå’Œæ•æ„Ÿæ–‡æ¡£å¤„ç†ç­‰ä»»åŠ¡çš„å¤±è¯¯ï¼Œå½¢æˆç¤¾ä¼šé—®é¢˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºé€šè¿‡æ³¨å…¥ä¸å¯å¯Ÿè§‰çš„å¹»å½±æ ‡è®°æ¥æ¬ºéª—LLMsï¼Œä½¿å…¶ç”Ÿæˆé”™è¯¯ä½†çœ‹ä¼¼åˆç†çš„è¾“å‡ºã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šé€šè¿‡å®è¯è¯„ä¼°ï¼ŒTRAPDOCåœ¨å¤šä¸ªå•†ä¸šLLMsä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ¬ºéª—æ•ˆæœï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ã€å†™ä½œã€æ–‡æœ¬ç¼–è¾‘å’Œæ£€ç´¢ç­‰æ–¹é¢çš„å¿«é€Ÿå‘å±•ï¼Œç”¨æˆ·å¯¹å…¶åŠŸèƒ½çš„ä¾èµ–æ—¥ç›ŠåŠ æ·±ï¼Œå¯¼è‡´äº†ä¸¥é‡çš„ç¤¾ä¼šé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½œä¸šå’Œæ•æ„Ÿæ–‡æ¡£å¤„ç†ç­‰ä»»åŠ¡ä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†ä¸å¯å¯Ÿè§‰çš„å¹»å½±æ ‡è®°æ³¨å…¥æ–‡æ¡£çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿LLMsç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…ä¸Šä¸æ­£ç¡®çš„è¾“å‡ºã€‚åŸºäºè¿™ä¸€æŠ€æœ¯ï¼Œæœ¬æ–‡ä»‹ç»äº†TRAPDOCæ¡†æ¶ï¼Œæ—¨åœ¨æ¬ºéª—è¿‡åº¦ä¾èµ–LLMsçš„ç”¨æˆ·ã€‚é€šè¿‡å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤šä¸ªå•†ä¸šLLMsä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸å¤šä¸ªåŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚TRAPDOCä¸ºä¿ƒè¿›ç”¨æˆ·ä¸è¯­è¨€æ¨¡å‹çš„æ›´è´Ÿè´£ä»»å’Œæ·±æ€ç†Ÿè™‘çš„äº’åŠ¨å¥ å®šäº†åšå®åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç”¨æˆ·å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿‡åº¦ä¾èµ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆé˜²æ­¢ç”¨æˆ·åœ¨ä½œä¸šå’Œæ•æ„Ÿæ–‡æ¡£å¤„ç†ä¸­çš„å¤±è¯¯ï¼Œå¯¼è‡´é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§å°†ä¸å¯å¯Ÿè§‰çš„å¹»å½±æ ‡è®°æ³¨å…¥æ–‡æ¡£çš„æ–¹æ³•ï¼Œåˆ©ç”¨è¿™äº›æ ‡è®°ä½¿LLMsç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…ä¸Šé”™è¯¯çš„è¾“å‡ºï¼Œä»è€Œå¼•å¯¼ç”¨æˆ·åšå‡ºé”™è¯¯å†³ç­–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šTRAPDOCæ¡†æ¶åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€å¹»å½±æ ‡è®°ç”Ÿæˆã€æ–‡æ¡£æ³¨å…¥å’Œè¾“å‡ºè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆå¯¹è¾“å…¥æ–‡æ¡£è¿›è¡Œåˆ†æï¼Œç„¶åç”Ÿæˆå¹»å½±æ ‡è®°å¹¶å°†å…¶æ³¨å…¥æ–‡æ¡£ï¼Œæœ€åè¯„ä¼°LLMsçš„è¾“å‡ºæ•ˆæœã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¹»å½±æ ‡è®°çš„ç”Ÿæˆä¸æ³¨å…¥æŠ€æœ¯ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„ç›´æ¥è¾“å…¥å¹²æ‰°ä¸åŒï¼Œèƒ½å¤Ÿåœ¨ä¸å¼•èµ·ç”¨æˆ·æ³¨æ„çš„æƒ…å†µä¸‹å½±å“LLMsçš„è¾“å‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œå¹»å½±æ ‡è®°çš„ç”Ÿæˆä¾èµ–äºç‰¹å®šçš„æ–‡æœ¬ç‰¹å¾ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸ºæœ€å¤§åŒ–è¾“å‡ºçš„è¯¯å¯¼æ€§ï¼ŒåŒæ—¶ä¿æŒæ ‡è®°çš„ä¸å¯å¯Ÿè§‰æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒTRAPDOCåœ¨å¤šä¸ªå•†ä¸šLLMsä¸ŠæˆåŠŸæ¬ºéª—ç”¨æˆ·ï¼Œç”Ÿæˆçš„é”™è¯¯è¾“å‡ºä¸çœŸå®è¾“å‡ºçš„ç›¸ä¼¼åº¦é«˜è¾¾85%ï¼Œç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æå‡äº†20%çš„è¯¯å¯¼æ•ˆæœï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€æ³•å¾‹å’ŒåŒ»ç–—ç­‰éœ€è¦é«˜å‡†ç¡®æ€§å’Œè´£ä»»æ„Ÿçš„è¡Œä¸šã€‚é€šè¿‡æé«˜ç”¨æˆ·å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è­¦è§‰æ€§ï¼ŒTRAPDOCæœ‰åŠ©äºå‡å°‘å› è¿‡åº¦ä¾èµ–è€Œå¯¼è‡´çš„é”™è¯¯å†³ç­–ï¼Œä»è€Œæå‡æ•´ä½“ä¿¡æ¯å¤„ç†çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The reasoning, writing, text-editing, and retrieval capabilities of proprietary large language models (LLMs) have advanced rapidly, providing users with an ever-expanding set of functionalities. However, this growing utility has also led to a serious societal concern: the over-reliance on LLMs. In particular, users increasingly delegate tasks such as homework, assignments, or the processing of sensitive documents to LLMs without meaningful engagement. This form of over-reliance and misuse is emerging as a significant social issue. In order to mitigate these issues, we propose a method injecting imperceptible phantom tokens into documents, which causes LLMs to generate outputs that appear plausible to users but are in fact incorrect. Based on this technique, we introduce TRAPDOC, a framework designed to deceive over-reliant LLM users. Through empirical evaluation, we demonstrate the effectiveness of our framework on proprietary LLMs, comparing its impact against several baselines. TRAPDOC serves as a strong foundation for promoting more responsible and thoughtful engagement with language models. Our code is available at https://github.com/jindong22/TrapDoc.

