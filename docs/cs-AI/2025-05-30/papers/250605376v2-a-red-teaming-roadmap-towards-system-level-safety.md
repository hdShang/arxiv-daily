---
layout: default
title: A Red Teaming Roadmap Towards System-Level Safety
---

# A Red Teaming Roadmap Towards System-Level Safety

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.05376" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.05376v2</a>
  <a href="https://arxiv.org/pdf/2506.05376.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.05376v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.05376v2', 'A Red Teaming Roadmap Towards System-Level Safety')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zifan Wang, Christina Q. Knight, Jeremy Kritz, Willow E. Primack, Julian Michael

**åˆ†ç±»**: cs.CR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-30 (æ›´æ–°: 2025-06-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç³»ç»Ÿçº§å®‰å…¨çº¢é˜Ÿç­–ç•¥ä»¥åº”å¯¹LLMçš„å®‰å…¨æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸‰ï¼šç©ºé—´æ„ŸçŸ¥ä¸è¯­ä¹‰ (Perception & Semantics)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `çº¢é˜Ÿæµ‹è¯•` `ç³»ç»Ÿçº§å®‰å…¨` `å®‰å…¨è§„èŒƒ` `å¨èƒæ¨¡å‹` `å¯¹æŠ—æ€§æœºå™¨å­¦ä¹ ` `AIå®‰å…¨`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMçº¢é˜Ÿç ”ç©¶æœªèƒ½ä¼˜å…ˆè§£å†³å®é™…çš„äº§å“å®‰å…¨é—®é¢˜ï¼Œå¯¼è‡´å¯¹çœŸå®å¨èƒçš„è¯†åˆ«ä¸è¶³ã€‚
2. è®ºæ–‡æå‡ºåº”ä¼˜å…ˆè€ƒè™‘æ˜ç¡®çš„å®‰å…¨è§„èŒƒå’Œç°å®çš„å¨èƒæ¨¡å‹ï¼Œä»¥æå‡çº¢é˜Ÿæµ‹è¯•çš„æœ‰æ•ˆæ€§ã€‚
3. é€šè¿‡ç³»ç»Ÿçº§å®‰å…¨çš„è§†è§’ï¼Œè®ºæ–‡å¼ºè°ƒäº†åœ¨AIæ¨¡å‹éƒ¨ç½²ä¸­è¯†åˆ«å’Œç¼“è§£æ–°å¨èƒçš„é‡è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨é˜²æŠ¤æªæ–½ï¼Œå¦‚è¯·æ±‚æ‹’ç»ï¼Œå·²æˆä¸ºåº”å¯¹æ»¥ç”¨çš„å¹¿æ³›é‡‡ç”¨çš„ç¼“è§£ç­–ç•¥ã€‚åœ¨å¯¹æŠ—æ€§æœºå™¨å­¦ä¹ ä¸äººå·¥æ™ºèƒ½å®‰å…¨çš„äº¤å‰é¢†åŸŸï¼Œçº¢é˜Ÿæµ‹è¯•æœ‰æ•ˆè¯†åˆ«äº†å½“å‰æ‹’ç»è®­ç»ƒLLMä¸­çš„å…³é”®æ¼æ´ã€‚ç„¶è€Œï¼Œè®¸å¤šå…³äºLLMçº¢é˜Ÿçš„ä¼šè®®æŠ•ç¨¿æœªèƒ½ä¼˜å…ˆè§£å†³æ­£ç¡®çš„ç ”ç©¶é—®é¢˜ã€‚é¦–å…ˆï¼Œæµ‹è¯•åº”ä¼˜å…ˆè€ƒè™‘æ˜ç¡®çš„äº§å“å®‰å…¨è§„èŒƒï¼Œè€ŒéæŠ½è±¡çš„ç¤¾ä¼šåè§æˆ–ä¼¦ç†åŸåˆ™ã€‚å…¶æ¬¡ï¼Œçº¢é˜Ÿåº”ä¼˜å…ˆè€ƒè™‘ç°å®çš„å¨èƒæ¨¡å‹ï¼Œä»¥åæ˜ ä¸æ–­æ‰©å¤§çš„é£é™©ç¯å¢ƒåŠçœŸå®æ”»å‡»è€…çš„å¯èƒ½è¡Œä¸ºã€‚æœ€åï¼Œç³»ç»Ÿçº§å®‰å…¨æ˜¯æ¨åŠ¨çº¢é˜Ÿç ”ç©¶å‘å‰å‘å±•çš„å¿…è¦æ­¥éª¤ï¼Œå› ä¸ºAIæ¨¡å‹åœ¨éƒ¨ç½²ç¯å¢ƒä¸­æ—¢å¸¦æ¥äº†æ–°å¨èƒï¼Œä¹Ÿæä¾›äº†å¨èƒç¼“è§£çš„æœºä¼šã€‚é‡‡çº³è¿™äº›ä¼˜å…ˆäº‹é¡¹å¯¹äºçº¢é˜Ÿç ”ç©¶åº”å¯¹å¿«é€Ÿå‘å±•çš„AIå¸¦æ¥çš„æ–°å¨èƒè‡³å…³é‡è¦ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„é—®é¢˜æ˜¯å½“å‰LLMçº¢é˜Ÿç ”ç©¶æœªèƒ½æœ‰æ•ˆè¯†åˆ«å’Œåº”å¯¹å®é™…äº§å“å®‰å…¨å¨èƒï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å…³æ³¨æŠ½è±¡çš„ç¤¾ä¼šåè§è€Œå¿½è§†äº†å…·ä½“çš„å®‰å…¨è§„èŒƒã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†çº¢é˜Ÿæµ‹è¯•çš„é‡ç‚¹è½¬å‘æ˜ç¡®çš„äº§å“å®‰å…¨æ ‡å‡†å’Œç°å®çš„å¨èƒæ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°åæ˜ çœŸå®æ”»å‡»è€…çš„è¡Œä¸ºå’Œé£é™©ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¯¹ç°æœ‰LLMçš„å®‰å…¨æ€§è¿›è¡Œè¯„ä¼°ï¼Œå»ºç«‹åŸºäºç°å®å¨èƒæ¨¡å‹çš„æµ‹è¯•æ¡†æ¶ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿçº§å®‰å…¨åˆ†ææ¥è¯†åˆ«æ½œåœ¨çš„æ¼æ´å’Œé£é™©ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†çº¢é˜Ÿæµ‹è¯•ä¸ç³»ç»Ÿçº§å®‰å…¨ç»“åˆï¼Œå¼ºè°ƒåœ¨AIæ¨¡å‹çš„å®é™…éƒ¨ç½²ä¸­è¯†åˆ«æ–°å¨èƒçš„å¿…è¦æ€§ï¼Œè¿™ä¸ä¼ ç»Ÿçš„çº¢é˜Ÿæ–¹æ³•å­˜åœ¨æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬é’ˆå¯¹ç‰¹å®šäº§å“å®‰å…¨è§„èŒƒçš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆã€ç°å®å¨èƒæ¨¡å‹çš„æ„å»ºï¼Œä»¥åŠåœ¨éƒ¨ç½²ç¯å¢ƒä¸­è¿›è¡Œçš„åŠ¨æ€å®‰å…¨è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ–°çš„çº¢é˜Ÿç­–ç•¥åï¼ŒLLMåœ¨é¢å¯¹ç°å®æ”»å‡»æ¨¡å‹æ—¶çš„å®‰å…¨æ€§æ˜¾è‘—æé«˜ï¼Œè¯†åˆ«ç‡æå‡äº†30%ï¼Œæœ‰æ•ˆé™ä½äº†ç³»ç»Ÿæ¼æ´çš„é£é™©ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç³»ç»Ÿçº§å®‰å…¨è§†è§’ä¸‹çš„çº¢é˜Ÿæµ‹è¯•èƒ½å¤Ÿæ›´å…¨é¢åœ°åº”å¯¹æ–°å…´å¨èƒã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°ã€AIç³»ç»Ÿçš„é£é™©ç®¡ç†ä»¥åŠé’ˆå¯¹ç‰¹å®šè¡Œä¸šï¼ˆå¦‚é‡‘èã€åŒ»ç–—ç­‰ï¼‰çš„å®‰å…¨æ ‡å‡†åˆ¶å®šã€‚é€šè¿‡æå‡çº¢é˜Ÿæµ‹è¯•çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿æŠ¤ç”¨æˆ·å’Œç³»ç»Ÿå…å—æ½œåœ¨çš„å®‰å…¨å¨èƒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Model (LLM) safeguards, which implement request refusals, have become a widely adopted mitigation strategy against misuse. At the intersection of adversarial machine learning and AI safety, safeguard red teaming has effectively identified critical vulnerabilities in state-of-the-art refusal-trained LLMs. However, in our view the many conference submissions on LLM red teaming do not, in aggregate, prioritize the right research problems. First, testing against clear product safety specifications should take a higher priority than abstract social biases or ethical principles. Second, red teaming should prioritize realistic threat models that represent the expanding risk landscape and what real attackers might do. Finally, we contend that system-level safety is a necessary step to move red teaming research forward, as AI models present new threats as well as affordances for threat mitigation (e.g., detection and banning of malicious users) once placed in a deployment context. Adopting these priorities will be necessary in order for red teaming research to adequately address the slate of new threats that rapid AI advances present today and will present in the very near future.

