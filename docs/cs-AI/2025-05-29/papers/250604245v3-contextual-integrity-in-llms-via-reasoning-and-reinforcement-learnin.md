---
layout: default
title: Contextual Integrity in LLMs via Reasoning and Reinforcement Learning
---

# Contextual Integrity in LLMs via Reasoning and Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.04245" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.04245v3</a>
  <a href="https://arxiv.org/pdf/2506.04245.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.04245v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.04245v3', 'Contextual Integrity in LLMs via Reasoning and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Guangchen Lan, Huseyin A. Inan, Sahar Abdelnabi, Janardhan Kulkarni, Lukas Wutschitz, Reza Shokri, Christopher G. Brinton, Robert Sim

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-11-16)

**å¤‡æ³¨**: 39th Conference on Neural Information Processing Systems (NeurIPS 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡æ¨ç†ä¸å¼ºåŒ–å­¦ä¹ æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å®Œæ•´æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡å®Œæ•´æ€§` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¼ºåŒ–å­¦ä¹ ` `ä¿¡æ¯æŠ«éœ²` `éšç§ä¿æŠ¤` `è‡ªåŠ¨åŒ–å†³ç­–` `æ™ºèƒ½åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šç°æœ‰æ–¹æ³•åœ¨è‡ªä¸»ä»£ç†å†³ç­–ä¸­éš¾ä»¥ç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œå¯¼è‡´ä¿¡æ¯æŠ«éœ²ä¸å½“ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæœ¬æ–‡é€šè¿‡æç¤ºLLMsè¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒæˆ–æ•ˆæœï¼šä½¿ç”¨åˆæˆæ•°æ®é›†ï¼Œæ˜¾è‘—å‡å°‘ä¸å½“ä¿¡æ¯æŠ«éœ²ï¼ŒåŒæ—¶åœ¨å¤šä¸ªæ¨¡å‹ä¸Šä¿æŒä»»åŠ¡æ€§èƒ½ï¼Œä¸”æ”¹è¿›å¯è½¬ç§»è‡³æ ‡å‡†åŸºå‡†ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€è‡ªä¸»ä»£ç†åœ¨ç”¨æˆ·å†³ç­–ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼ˆCIï¼‰æˆä¸ºè¯¥é¢†åŸŸçš„æ ¸å¿ƒé—®é¢˜ã€‚æœ¬æ–‡æå‡ºï¼ŒCIéœ€è¦ä»£ç†åœ¨æ“ä½œä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜ç¡®æ¨ç†CIæ¥å†³å®šä¿¡æ¯æŠ«éœ²ï¼Œç„¶åå¼€å‘äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹å®ç°CIæ‰€éœ€çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨çº¦700ä¸ªå¤šæ ·åŒ–ä¸Šä¸‹æ–‡å’Œä¿¡æ¯æŠ«éœ²è§„èŒƒçš„åˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä¸å½“ä¿¡æ¯æŠ«éœ²ï¼ŒåŒæ—¶åœ¨å¤šä¸ªæ¨¡å‹è§„æ¨¡å’Œå®¶æ—ä¸­ä¿æŒäº†ä»»åŠ¡æ€§èƒ½ã€‚é‡è¦çš„æ˜¯ï¼Œæ”¹è¿›ä»åˆæˆæ•°æ®é›†è½¬ç§»åˆ°å·²å»ºç«‹çš„CIåŸºå‡†ï¼Œå¦‚PrivacyLensï¼Œè¯¥åŸºå‡†è¯„ä¼°AIåŠ©æ‰‹åœ¨è¡ŒåŠ¨å’Œå·¥å…·è°ƒç”¨ä¸­çš„éšç§æ³„éœ²ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªä¸»ä»£ç†åœ¨æ‰§è¡Œä»»åŠ¡æ—¶å¦‚ä½•ç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¿¡æ¯æŠ«éœ²æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“å¯¼è‡´éšç§æ³„éœ²å’Œä¸å½“ä¿¡æ¯å…±äº«ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥ç¡®ä¿åœ¨ç‰¹å®šä»»åŠ¡ä¸­é€‚å½“çš„ä¿¡æ¯æŠ«éœ²ã€‚è¿™æ ·çš„è®¾è®¡æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œé€‚åº”ä¸åŒçš„ä¸Šä¸‹æ–‡éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯é€šè¿‡æç¤ºæ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯æŠ«éœ²è§„èŒƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºç»“åˆäº†æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œä½¿å¾—æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡ï¼Œè¿˜èƒ½åœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¼˜åŒ–å…¶ä¿¡æ¯æŠ«éœ²ç­–ç•¥ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„é™æ€å†³ç­–å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œè®ºæ–‡è®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡ä»»åŠ¡æ€§èƒ½ä¸ä¿¡æ¯æŠ«éœ²çš„é€‚å½“æ€§ï¼ŒåŒæ—¶é‡‡ç”¨äº†å¤šç§æ¨¡å‹æ¶æ„è¿›è¡Œå®éªŒï¼Œä»¥éªŒè¯æ–¹æ³•çš„æ™®é€‚æ€§å’Œæœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆæˆæ•°æ®é›†çš„æ„å»ºï¼Œç¡®ä¿äº†è®­ç»ƒæ ·æœ¬çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨åˆæˆæ•°æ®é›†ä¸Šæ˜¾è‘—å‡å°‘äº†ä¸å½“ä¿¡æ¯æŠ«éœ²ï¼Œä»»åŠ¡æ€§èƒ½ä¿æŒç¨³å®šã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œä¿¡æ¯æ³„éœ²ç‡é™ä½äº†çº¦30%ï¼Œå¹¶ä¸”æ”¹è¿›æ•ˆæœåœ¨PrivacyLensç­‰æ ‡å‡†åŸºå‡†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„è¿ç§»èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œéšç§ä¿æŠ¤æŠ€æœ¯ã€‚é€šè¿‡æå‡ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç”¨æˆ·éšç§æ³„éœ²é£é™©ï¼Œå¢å¼ºç”¨æˆ·å¯¹æ™ºèƒ½ç³»ç»Ÿçš„ä¿¡ä»»ï¼Œæœªæ¥å¯èƒ½åœ¨å„ç±»éœ€è¦ä¿¡æ¯æŠ«éœ²çš„åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.

