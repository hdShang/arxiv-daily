---
layout: default
title: Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs
---

# Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00072" class="toolbar-btn" target="_blank">üìÑ arXiv: 2506.00072v1</a>
  <a href="https://arxiv.org/pdf/2506.00072.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00072v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00072v1', 'Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Nariman Naderi, Zahra Atf, Peter R Lewis, Aref Mahjoub far, Seyed Amir Ahmad Safavi-Naini, Ali Soroush

**ÂàÜÁ±ª**: cs.CY, cs.AI, cs.CL, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-29

**Â§áÊ≥®**: This paper was accepted for presentation at the 7th International Workshop on EXplainable, Trustworthy, and Responsible AI and Multi-Agent Systems (EXTRAAMAS 2025). Workshop website: https://extraamas.ehealth.hevs.ch/index.html

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ËØÑ‰º∞ÊèêÁ§∫Â∑•Á®ãÊäÄÊúØ‰ª•ÊèêÂçáÂåªÁñóÈ¢ÜÂüüÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß‰∏é‰ø°ÂøÉ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `ÊèêÁ§∫Â∑•Á®ã` `ÂåªÁñóAI` `‰ø°ÂøÉÊ†°ÂáÜ` `Ê®°ÂûãËØÑ‰º∞` `ÂáÜÁ°ÆÊÄßÊèêÂçá` `ËøáÂ∫¶Ëá™‰ø°` `È£éÈô©ÁÆ°ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÂåªÁñóÈ¢ÜÂüüÁöÑÂ∫îÁî®‰∏≠ÔºåÂáÜÁ°ÆÊÄß‰∏é‰ø°ÂøÉÂºïÂØº‰πãÈó¥Â≠òÂú®ÁüõÁõæÔºåÂØºËá¥ÂÜ≥Á≠ñÈ£éÈô©Â¢ûÂä†„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÈÄöËøáÂ§öÁßçÊèêÁ§∫Â∑•Á®ãÊäÄÊúØÔºåÁ≥ªÁªüËØÑ‰º∞ÂÖ∂ÂØπÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÁñó‰ªªÂä°‰∏≠ÂáÜÁ°ÆÊÄßÂíå‰ø°ÂøÉÁöÑÂΩ±Âìç„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåChain-of-ThoughtÊèêÁ§∫ÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÔºå‰ΩÜ‰πüÂºïÂèëËøáÂ∫¶Ëá™‰ø°ÔºåÂº∫Ë∞É‰∫ÜÂØπ‰ø°ÂøÉÁöÑÊ†°ÂáÜÈúÄÊ±Ç„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Êú¨ÊñáÁ†îÁ©∂‰∫ÜÊèêÁ§∫Â∑•Á®ãÊäÄÊúØÂ¶Ç‰ΩïÂΩ±ÂìçÂ∫îÁî®‰∫éÂåªÁñóÈ¢ÜÂüüÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂáÜÁ°ÆÊÄßÂíå‰ø°ÂøÉÂºïÂØº„ÄÇÈÄöËøáÂØπÂ§ö‰∏™‰∏ì‰∏öÁöÑÊ≥¢ÊñØËØ≠ËÄÉËØïÈóÆÈ¢òËøõË°åÂàÜÂ±ÇÊï∞ÊçÆÈõÜËØÑ‰º∞ÔºåÊµãËØï‰∫Ü‰∫îÁßçLLMÔºàGPT-4o„ÄÅo3-mini„ÄÅLlama-3.3-70b„ÄÅLlama-3.1-8bÂíåDeepSeek-v3ÔºâÂú®156ÁßçÈÖçÁΩÆ‰∏ãÁöÑË°®Áé∞„ÄÇËøô‰∫õÈÖçÁΩÆÂú®Ê∏©Â∫¶ËÆæÁΩÆ„ÄÅÊèêÁ§∫È£éÊ†ºÂíå‰ø°ÂøÉÈáèË°®‰∏äÊúâÊâÄ‰∏çÂêå„ÄÇÁªìÊûúË°®ÊòéÔºåChain-of-ThoughtÊèêÁ§∫ÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÔºå‰ΩÜ‰πüÂØºËá¥‰∫ÜËøáÂ∫¶Ëá™‰ø°ÔºåÂº∫Ë∞É‰∫ÜÊ†°ÂáÜÁöÑÂøÖË¶ÅÊÄß„ÄÇÊÉÖÊÑüÊèêÁ§∫Ëøõ‰∏ÄÊ≠•Â¢ûÂä†‰∫Ü‰ø°ÂøÉÔºåÂèØËÉΩÂØºËá¥‰∏çËâØÂÜ≥Á≠ñ„ÄÇËæÉÂ∞èÁöÑÊ®°ÂûãÂú®ÊâÄÊúâÊåáÊ†á‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåËÄå‰∏ìÊúâÊ®°ÂûãËôΩÁÑ∂ÂáÜÁ°ÆÊÄßËæÉÈ´òÔºå‰ΩÜ‰ªçÁº∫‰πèÊ†°ÂáÜ‰ø°ÂøÉ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÊèêÁ§∫Â∑•Á®ãÂøÖÈ°ªÂêåÊó∂ÂÖ≥Ê≥®ÂáÜÁ°ÆÊÄßÂíå‰∏çÁ°ÆÂÆöÊÄßÔºå‰ª•Âú®È´òÈ£éÈô©ÂåªÁñó‰ªªÂä°‰∏≠ÊúâÊïà„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥ÂåªÁñóÈ¢ÜÂüüÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÂáÜÁ°ÆÊÄß‰∏é‰ø°ÂøÉÂºïÂØºÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ïÊú™ËÉΩÊúâÊïàÂπ≥Ë°°Ëøô‰∏§ËÄÖÔºåÂØºËá¥ÊΩúÂú®ÁöÑÂÜ≥Á≠ñÈ£éÈô©„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÈÄöËøáÂ§öÁßçÊèêÁ§∫Â∑•Á®ãÊäÄÊúØÁöÑÁªÑÂêàÔºåËØÑ‰º∞ÂÖ∂ÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåÁâπÂà´ÊòØÂú®È´òÈ£éÈô©ÂåªÁñóÁéØÂ¢É‰∏≠ÔºåÁ°Æ‰øùÊ®°ÂûãÁöÑËæìÂá∫Êó¢ÂáÜÁ°ÆÂèàÂÖ∑ÊúâÂèØÈù†ÁöÑ‰ø°ÂøÉÊ∞¥Âπ≥„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÁ†îÁ©∂ÈááÁî®‰∫ÜÂàÜÂ±ÇÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Â§ö‰∏™‰∏ì‰∏öÁöÑÊ≥¢ÊñØËØ≠ËÄÉËØïÈóÆÈ¢òÔºåËØÑ‰º∞‰∫Ü‰∫îÁßç‰∏çÂêåÁöÑLLMÂú®156ÁßçÈÖçÁΩÆ‰∏ãÁöÑË°®Áé∞ÔºåÊ∂âÂèäÊ∏©Â∫¶ËÆæÁΩÆ„ÄÅÊèêÁ§∫È£éÊ†ºÂíå‰ø°ÂøÉÈáèË°®Á≠âÂ§ö‰∏™Áª¥Â∫¶„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÂàõÊñ∞Âú®‰∫éÁ≥ªÁªüÊÄßÂú∞ËØÑ‰º∞‰∏çÂêåÊèêÁ§∫È£éÊ†ºÂØπÊ®°ÂûãÂáÜÁ°ÆÊÄßÂíå‰ø°ÂøÉÁöÑÂΩ±ÂìçÔºåÁâπÂà´ÊòØChain-of-ThoughtÂíåÊÉÖÊÑüÊèêÁ§∫ÁöÑ‰ΩøÁî®ÔºåÊè≠Á§∫‰∫ÜËøáÂ∫¶Ëá™‰ø°ÁöÑÈ£éÈô©„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÆûÈ™å‰∏≠‰ΩøÁî®‰∫ÜÂ§öÁßçÊ∏©Â∫¶ËÆæÁΩÆÔºà0.3„ÄÅ0.7„ÄÅ1.0Ôºâ„ÄÅ‰∏çÂêåÁöÑÊèêÁ§∫È£éÊ†ºÔºàChain-of-Thought„ÄÅFew-Shot„ÄÅÊÉÖÊÑü„ÄÅ‰∏ìÂÆ∂Ê®°‰ªøÔºâ‰ª•Âèä‰ø°ÂøÉÈáèË°®Ôºà1-10„ÄÅ1-100ÔºâÔºåÂπ∂ÈÄöËøáAUC-ROC„ÄÅBrier ScoreÂíåÊúüÊúõÊ†°ÂáÜËØØÂ∑ÆÔºàECEÔºâÁ≠âÊåáÊ†áËøõË°åËØÑ‰º∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåChain-of-ThoughtÊèêÁ§∫Âú®ÊèêÈ´òÊ®°ÂûãÂáÜÁ°ÆÊÄßÊñπÈù¢Ë°®Áé∞Á™ÅÂá∫Ôºå‰ΩÜÂêåÊó∂‰πüÂØºËá¥‰∫ÜËøáÂ∫¶Ëá™‰ø°ÁöÑÁé∞Ë±°„ÄÇËæÉÂ∞èÁöÑÊ®°ÂûãÔºàÂ¶ÇLlama-3.1-8bÔºâÂú®ÊâÄÊúâËØÑ‰º∞ÊåáÊ†á‰∏äÂùáË°®Áé∞‰∏ç‰Ω≥ÔºåËÄå‰∏ìÊúâÊ®°ÂûãËôΩÁÑ∂ÂáÜÁ°ÆÊÄßËæÉÈ´òÔºå‰ΩÜ‰ø°ÂøÉÊ†°ÂáÜ‰ªçÊòæ‰∏çË∂≥„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫ÜÂú®È´òÈ£éÈô©ÂåªÁñó‰ªªÂä°‰∏≠ÔºåÊèêÁ§∫Â∑•Á®ãÈúÄÂêåÊó∂ÂÖ≥Ê≥®ÂáÜÁ°ÆÊÄß‰∏é‰∏çÁ°ÆÂÆöÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÂåªÁñóËØäÊñ≠ÊîØÊåÅÁ≥ªÁªü„ÄÅÊÇ£ËÄÖÂí®ËØ¢ÂíåÂåªÁñóÊïôËÇ≤Á≠â„ÄÇÈÄöËøá‰ºòÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊèêÁ§∫Â∑•Á®ãÔºåÂèØ‰ª•ÊèêÂçáÂåªÁñóÂÜ≥Á≠ñÁöÑÂáÜÁ°ÆÊÄß‰∏é‰ø°ÂøÉÔºå‰ªéËÄåÈôç‰ΩéÂåªÁñóÈîôËØØÁöÑÈ£éÈô©ÔºåÂ¢ûÂº∫ÊÇ£ËÄÖÂÆâÂÖ®„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÂèØËÉΩÊé®Âä®Êõ¥ÂπøÊ≥õÁöÑAIÂú®ÂåªÁñóÈ¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÊèêÂçáÊï¥‰ΩìÂåªÁñóÊúçÂä°Ë¥®Èáè„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> This paper investigates how prompt engineering techniques impact both accuracy and confidence elicitation in Large Language Models (LLMs) applied to medical contexts. Using a stratified dataset of Persian board exam questions across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini, Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles (Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales (1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error (ECE) to evaluate alignment between confidence and actual performance. Chain-of-Thought prompts improved accuracy but also led to overconfidence, highlighting the need for calibration. Emotional prompting further inflated confidence, risking poor decisions. Smaller models like Llama-3.1-8b underperformed across all metrics, while proprietary models showed higher accuracy but still lacked calibrated confidence. These results suggest prompt engineering must address both accuracy and uncertainty to be effective in high-stakes medical tasks.

