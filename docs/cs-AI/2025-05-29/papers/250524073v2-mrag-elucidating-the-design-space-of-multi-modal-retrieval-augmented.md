---
layout: default
title: mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation
---

# mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.24073" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.24073v2</a>
  <a href="https://arxiv.org/pdf/2505.24073.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.24073v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.24073v2', 'mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Suofei Feng, Ryan Rossi, Zhengzhong Tu

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-05-29 (æ›´æ–°: 2025-08-26)

**å¤‡æ³¨**: 16 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºmRAGä»¥è§£å†³å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å±€é™æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢` `å¢å¼ºç”Ÿæˆ` `è§†è§‰è¯­è¨€æ¨¡å‹` `é‡æ’åºç­–ç•¥` `åŠ¨æ€é€‚åº”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€åº”ç”¨ä¸­é¢ä¸´é™æ€æ•°æ®å’Œå¹»è§‰é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç³»ç»Ÿåˆ†æäº†æ£€ç´¢ã€é‡æ’åºå’Œç”Ÿæˆä¸‰ä¸ªé˜¶æ®µçš„ä¼˜åŒ–ç­–ç•¥ã€‚
3. é€šè¿‡å…¨æ ˆæ¢ç´¢ï¼Œç ”ç©¶è¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå¹³å‡æ€§èƒ½æå‡è¾¾5%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å—åˆ°é™æ€è®­ç»ƒæ•°æ®ã€æ˜“äºäº§ç”Ÿå¹»è§‰å’Œæ— æ³•éªŒè¯å¤–éƒ¨è¯æ®çš„é™åˆ¶ï¼Œå½±å“å…¶åœ¨åŠ¨æ€ç°å®åº”ç”¨ä¸­çš„è¡¨ç°ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ºç¼“è§£è¿™äº›æŒ‘æˆ˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå…è®¸LVLMsé€šè¿‡æ£€ç´¢æœºåˆ¶è®¿é—®å¤§è§„æ¨¡çŸ¥è¯†æ•°æ®åº“ï¼Œä»è€Œå°†æ¨¡å‹è¾“å‡ºä¸äº‹å®å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„ä¿¡æ¯ç›¸ç»“åˆã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§å‰–æäº†LVLMsçš„å¤šæ¨¡æ€RAGæµç¨‹ï¼Œæ¢è®¨äº†æ£€ç´¢é˜¶æ®µçš„æ¨¡æ€é…ç½®å’Œæ£€ç´¢ç­–ç•¥ã€é‡æ’åºé˜¶æ®µçš„ç­–ç•¥ä»¥å‡è½»ä½ç½®åå·®å’Œæé«˜æ£€ç´¢è¯æ®çš„ç›¸å…³æ€§ï¼Œä»¥åŠç”Ÿæˆé˜¶æ®µå¦‚ä½•æœ€ä½³æ•´åˆæ£€ç´¢å€™é€‰é¡¹ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ä¸ªç»Ÿä¸€çš„ä»£ç†æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åæ€æ•´åˆé‡æ’åºå’Œç”Ÿæˆï¼Œä½¿LVLMsèƒ½å¤ŸåŠ¨æ€é€‰æ‹©ç›¸å…³è¯æ®å¹¶æŠ‘åˆ¶æ— å…³ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬çš„å…¨æ ˆæ¢ç´¢ä¸ºRAGåœ¨LVLMsä¸­çš„åº”ç”¨æä¾›äº†é‡è¦è§è§£ï¼Œå¹³å‡æ€§èƒ½æå‡5%ä¸”æ— éœ€å¾®è°ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€åº”ç”¨ä¸­å› é™æ€è®­ç»ƒæ•°æ®å’Œå¹»è§‰é—®é¢˜å¯¼è‡´çš„æ€§èƒ½ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„mRAGæ¡†æ¶é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ–¹å¼ï¼Œä½¿LVLMsèƒ½å¤ŸåŠ¨æ€è®¿é—®å¤§è§„æ¨¡çŸ¥è¯†åº“ï¼Œæ•´åˆç›¸å…³ä¿¡æ¯ä»¥æé«˜ç”Ÿæˆç»“æœçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šæ£€ç´¢é˜¶æ®µï¼ˆæ¨¡æ€é…ç½®å’Œæ£€ç´¢ç­–ç•¥ï¼‰ã€é‡æ’åºé˜¶æ®µï¼ˆå‡è½»ä½ç½®åå·®å’Œæé«˜ç›¸å…³æ€§ï¼‰å’Œç”Ÿæˆé˜¶æ®µï¼ˆæ•´åˆæ£€ç´¢å€™é€‰é¡¹ï¼‰ã€‚æ¯ä¸ªé˜¶æ®µéƒ½æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ä»£ç†æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åæ€æœºåˆ¶å°†é‡æ’åºä¸ç”Ÿæˆè¿‡ç¨‹ç»“åˆï¼ŒåŠ¨æ€é€‰æ‹©ç›¸å…³è¯æ®å¹¶æŠ‘åˆ¶æ— å…³ä¿¡æ¯ï¼Œè¿™åœ¨ç°æœ‰æ–¹æ³•ä¸­å°šæœªå®ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ£€ç´¢é˜¶æ®µï¼Œé‡‡ç”¨å¤šæ¨¡æ€é…ç½®å’Œå¤šæ ·åŒ–çš„æ£€ç´¢ç­–ç•¥ï¼›é‡æ’åºé˜¶æ®µå¼•å…¥äº†æ–°çš„ç®—æ³•ä»¥å‡è½»ä½ç½®åå·®ï¼›ç”Ÿæˆé˜¶æ®µåˆ™ä¼˜åŒ–äº†å€™é€‰é¡¹çš„æ•´åˆæ–¹å¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒmRAGæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡5%çš„æ€§èƒ½æå‡ï¼Œä¸”æ— éœ€è¿›è¡Œå¾®è°ƒã€‚è¿™ä¸€ç»“æœæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€ç”Ÿæˆæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨å†…å®¹ç”Ÿæˆå’Œå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„åŠ¨æ€é€‚åº”èƒ½åŠ›ï¼ŒmRAGèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å‡†ç¡®å’Œç›¸å…³çš„è¾“å‡ºï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision-Language Models (LVLMs) have made remarkable strides in multimodal tasks such as visual question answering, visual grounding, and complex reasoning. However, they remain limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence, compromising their performance in dynamic real-world applications. Retrieval-Augmented Generation (RAG) offers a practical solution to mitigate these challenges by allowing the LVLMs to access large-scale knowledge databases via retrieval mechanisms, thereby grounding model outputs in factual, contextually relevant information. Here in this paper, we conduct the first systematic dissection of the multimodal RAG pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the modality configurations and retrieval strategies, (2) the re-ranking stage: on strategies to mitigate positional biases and improve the relevance of retrieved evidence, and (3) the generation phase: we further investigate how to best integrate retrieved candidates into the final generation process. Finally, we extend to explore a unified agentic framework that integrates re-ranking and generation through self-reflection, enabling LVLMs to select relevant evidence and suppress irrelevant context dynamically. Our full-stack exploration of RAG for LVLMs yields substantial insights, resulting in an average performance boost of 5% without any fine-tuning.

