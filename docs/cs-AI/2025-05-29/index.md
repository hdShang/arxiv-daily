---
layout: default
title: arXiv ä¸­æ–‡è¦ç‚¹æ±‡æ€» - cs.AI - 2025-05-29
---

# cs.AIï¼ˆ2025-05-29ï¼‰

ğŸ“Š å…± **8** ç¯‡è®ºæ–‡


## ğŸ¯ å…´è¶£é¢†åŸŸå¯¼èˆª

<div class="interest-nav">
<a href="#æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models" class="interest-badge">æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5)</a>
<a href="#æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture" class="interest-badge">æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2)</a>
<a href="#æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control" class="interest-badge">æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1)</a>
</div>

---


<h2 id="æ”¯æŸ±ä¹å…·èº«å¤§æ¨¡å‹-embodied-foundation-models">ğŸ”¬ æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models) (5 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td><a href="./papers/250523503v2-can-large-language-models-challenge-cnns-in-medical-image-analysis.html">Can Large Language Models Challenge CNNs in Medical Image Analysis?</a></td>
  <td>æå‡ºå¤šæ¨¡æ€AIæ¡†æ¶ä»¥æå‡åŒ»å­¦å½±åƒåˆ†æç²¾åº¦</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23503v2" data-paper-url="./papers/250523503v2-can-large-language-models-challenge-cnns-in-medical-image-analysis.html" onclick="toggleFavorite(this, '2505.23503v2', 'Can Large Language Models Challenge CNNs in Medical Image Analysis?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>2</td>
  <td><a href="./papers/250524073v2-mrag-elucidating-the-design-space-of-multi-modal-retrieval-augmented.html">mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</a></td>
  <td>æå‡ºmRAGä»¥è§£å†³å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å±€é™æ€§</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span> <span class="paper-tag">visual grounding</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.24073v2" data-paper-url="./papers/250524073v2-mrag-elucidating-the-design-space-of-multi-modal-retrieval-augmented.html" onclick="toggleFavorite(this, '2505.24073v2', 'mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>3</td>
  <td><a href="./papers/250600072v1-evaluating-prompt-engineering-techniques-for-accuracy-and-confidence.html">Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs</a></td>
  <td>è¯„ä¼°æç¤ºå·¥ç¨‹æŠ€æœ¯ä»¥æå‡åŒ»ç–—é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ä¸ä¿¡å¿ƒ</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span> <span class="paper-tag">chain-of-thought</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00072v1" data-paper-url="./papers/250600072v1-evaluating-prompt-engineering-techniques-for-accuracy-and-confidence.html" onclick="toggleFavorite(this, '2506.00072v1', 'Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>4</td>
  <td><a href="./papers/250523881v1-using-reasoning-models-to-generate-search-heuristics-that-solve-open.html">Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems</a></td>
  <td>åˆ©ç”¨æ¨ç†æ¨¡å‹ç”Ÿæˆæœç´¢å¯å‘å¼ä»¥è§£å†³ç»„åˆè®¾è®¡é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23881v1" data-paper-url="./papers/250523881v1-using-reasoning-models-to-generate-search-heuristics-that-solve-open.html" onclick="toggleFavorite(this, '2505.23881v1', 'Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>5</td>
  <td><a href="./papers/250523631v3-human-empathy-as-encoder-ai-assisted-depression-assessment-in-specia.html">Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education</a></td>
  <td>æå‡ºäººç±»åŒç†å¿ƒç¼–ç å™¨ä»¥è§£å†³ç‰¹æ®Šæ•™è‚²ä¸­çš„æŠ‘éƒè¯„ä¼°é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">multimodal</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23631v3" data-paper-url="./papers/250523631v3-human-empathy-as-encoder-ai-assisted-depression-assessment-in-specia.html" onclick="toggleFavorite(this, '2505.23631v3', 'Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±äºŒrlç®—æ³•ä¸æ¶æ„-rl-architecture">ğŸ”¬ æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture) (2 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>6</td>
  <td><a href="./papers/250604245v3-contextual-integrity-in-llms-via-reasoning-and-reinforcement-learnin.html">Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</a></td>
  <td>é€šè¿‡æ¨ç†ä¸å¼ºåŒ–å­¦ä¹ æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å®Œæ•´æ€§</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.04245v3" data-paper-url="./papers/250604245v3-contextual-integrity-in-llms-via-reasoning-and-reinforcement-learnin.html" onclick="toggleFavorite(this, '2506.04245v3', 'Contextual Integrity in LLMs via Reasoning and Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
<tr>
  <td>7</td>
  <td><a href="./papers/250523885v2-owl-optimized-workforce-learning-for-general-multi-agent-assistance-.html">OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation</a></td>
  <td>æå‡ºä¼˜åŒ–å·¥ä½œå­¦ä¹ æ¡†æ¶ä»¥è§£å†³å¤šé¢†åŸŸä»»åŠ¡è‡ªåŠ¨åŒ–é—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">reinforcement learning</span> <span class="paper-tag">large language model</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2505.23885v2" data-paper-url="./papers/250523885v2-owl-optimized-workforce-learning-for-general-multi-agent-assistance-.html" onclick="toggleFavorite(this, '2505.23885v2', 'OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


<h2 id="æ”¯æŸ±ä¸€æœºå™¨äººæ§åˆ¶-robot-control">ğŸ”¬ æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control) (1 ç¯‡)</h2>

<table>
<thead>
<tr><th>#</th><th>é¢˜ç›®</th><th>ä¸€å¥è¯è¦ç‚¹</th><th>æ ‡ç­¾</th><th>ğŸ”—</th><th>â­</th></tr>
</thead>
<tbody>
<tr>
  <td>8</td>
  <td><a href="./papers/250600071v1-human-sensory-musculoskeletal-modeling-and-control-of-whole-body-mov.html">Human sensory-musculoskeletal modeling and control of whole-body movements</a></td>
  <td>æå‡ºSMS-Humanæ¨¡å‹ä»¥è§£å†³äººç±»è¿åŠ¨æ§åˆ¶çš„å¤šæ„Ÿå®˜æ•´åˆé—®é¢˜</td>
  <td class="tags-cell"><span class="paper-tag">bipedal</span> <span class="paper-tag">biped</span> <span class="paper-tag">locomotion</span></td>
  <td></td>
  <td><button class="favorite-btn" data-arxiv-id="2506.00071v1" data-paper-url="./papers/250600071v1-human-sensory-musculoskeletal-modeling-and-control-of-whole-body-mov.html" onclick="toggleFavorite(this, '2506.00071v1', 'Human sensory-musculoskeletal modeling and control of whole-body movements')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜†</button></td>
</tr>
</tbody>
</table>


[â¬…ï¸ è¿”å› cs.AI é¦–é¡µ](../index.html) Â· [ğŸ  è¿”å›ä¸»é¡µ](../../index.html)