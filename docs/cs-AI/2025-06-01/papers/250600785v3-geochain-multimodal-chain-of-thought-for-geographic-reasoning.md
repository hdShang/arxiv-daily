---
layout: default
title: GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning
---

# GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2506.00785" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2506.00785v3</a>
  <a href="https://arxiv.org/pdf/2506.00785.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2506.00785v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2506.00785v3', 'GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Sahiti Yerramilli, Nilay Pande, Rynaa Grover, Jayant Sravan Tamarapalli

**åˆ†ç±»**: cs.AI, cs.CV, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-06-01 (æ›´æ–°: 2025-09-09)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºGeoChainä»¥è§£å†³å¤šæ¨¡æ€åœ°ç†æ¨ç†é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ¨ç†` `åœ°ç†æ¨ç†` `æ€ç»´é“¾` `è§†è§‰åŸºç¡€` `è¯­ä¹‰åˆ†å‰²` `æ¨¡å‹è¯„ä¼°` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨åœ°ç†æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰åŸºç¡€å’Œå¤æ‚æ¨ç†æ–¹é¢ã€‚
2. GeoChainé€šè¿‡æä¾›ä¸€ä¸ªåŒ…å«21æ­¥æ€ç»´é“¾çš„é—®é¢˜åºåˆ—ï¼Œå¸®åŠ©æ¨¡å‹é€æ­¥è¿›è¡Œåœ°ç†æ¨ç†ï¼Œè¦†ç›–å¤šç§æ¨ç†ç±»åˆ«ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶ä»ç„¶å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼ŒGeoChainä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†å’Œè¯Šæ–­å·¥å…·ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡ä»‹ç»äº†GeoChainï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€æ­¥åœ°ç†æ¨ç†çš„å¤§è§„æ¨¡åŸºå‡†ã€‚GeoChainåˆ©ç”¨146ä¸‡å¼ Mapillaryè¡—æ™¯å›¾åƒï¼Œä¸ºæ¯å¼ å›¾åƒé…å¯¹21æ­¥çš„æ€ç»´é“¾ï¼ˆCoTï¼‰é—®é¢˜åºåˆ—ï¼ˆè¶…è¿‡3000ä¸‡ä¸ªé—®ç­”å¯¹ï¼‰ã€‚è¿™äº›åºåˆ—å¼•å¯¼æ¨¡å‹ä»ç²—ç•¥å±æ€§åˆ°ç»†ç²’åº¦å®šä½ï¼Œæ¶µç›–è§†è§‰ã€ç©ºé—´ã€æ–‡åŒ–å’Œç²¾ç¡®åœ°ç†å®šä½å››ä¸ªæ¨ç†ç±»åˆ«ï¼Œå¹¶æŒ‰éš¾åº¦è¿›è¡Œæ ‡æ³¨ã€‚å›¾åƒè¿˜é™„åŠ äº†è¯­ä¹‰åˆ†å‰²ï¼ˆ150ç±»ï¼‰å’Œè§†è§‰å¯å®šä½æ€§è¯„åˆ†ã€‚å¯¹å½“ä»£MLLMsï¼ˆå¦‚GPT-4.1å˜ä½“ã€Claude 3.7ã€Gemini 2.5å˜ä½“ï¼‰åœ¨2088å¼ å¤šæ ·åŒ–å›¾åƒå­é›†ä¸Šçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨è§†è§‰åŸºç¡€ã€æ¨ç†ä¸ç¨³å®šæ€§å’Œå‡†ç¡®å®šä½æ–¹é¢å­˜åœ¨ä¸€è‡´æ€§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¤æ‚æ€§å¢åŠ æ—¶ã€‚GeoChainæä¾›äº†ä¸€ç§å¼ºæœ‰åŠ›çš„è¯Šæ–­æ–¹æ³•ï¼Œä¿ƒè¿›å¤æ‚åœ°ç†æ¨ç†åœ¨MLLMsä¸­çš„æ˜¾è‘—è¿›å±•ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨åœ°ç†æ¨ç†ä¸­çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰åŸºç¡€å’Œå¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†å¤šå±‚æ¬¡çš„æ¨ç†ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šGeoChainçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«21æ­¥æ€ç»´é“¾çš„é—®é¢˜åºåˆ—ï¼Œé€æ­¥å¼•å¯¼æ¨¡å‹è¿›è¡Œåœ°ç†æ¨ç†ï¼Œä»è€Œæå‡å…¶åœ¨è§†è§‰ã€ç©ºé—´å’Œæ–‡åŒ–ç­‰æ–¹é¢çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šGeoChainçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€é—®é¢˜åºåˆ—è®¾è®¡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªä¸»è¦æ¨¡å—ã€‚æ•°æ®æ”¶é›†é˜¶æ®µä½¿ç”¨146ä¸‡å¼ è¡—æ™¯å›¾åƒï¼Œé—®é¢˜åºåˆ—è®¾è®¡åˆ™æ¶µç›–ä¸åŒéš¾åº¦çš„æ¨ç†ä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šGeoChainçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶å¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†å’Œç³»ç»ŸåŒ–çš„æ€ç»´é“¾é—®é¢˜è®¾è®¡ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€é—®é¢˜æˆ–ç®€å•é—®ç­”å½¢å¼æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å…³é”®è®¾è®¡æ–¹é¢ï¼ŒGeoChainä½¿ç”¨äº†150ç±»çš„è¯­ä¹‰åˆ†å‰²å’Œè§†è§‰å¯å®šä½æ€§è¯„åˆ†ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿè·å¾—æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å¯¹2088å¼ å¤šæ ·åŒ–å›¾åƒçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰åŸºç¡€å’Œå‡†ç¡®å®šä½æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¤æ‚æ€§å¢åŠ æ—¶ï¼Œæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„å¼±ç‚¹ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

GeoChainçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºæ™ºèƒ½åœ°å›¾æœåŠ¡ã€è‡ªåŠ¨é©¾é©¶ã€åŸå¸‚è§„åˆ’ç­‰é¢†åŸŸã€‚é€šè¿‡æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„åœ°ç†æ¨ç†èƒ½åŠ›ï¼Œæœªæ¥å¯ä»¥å®ç°æ›´æ™ºèƒ½çš„åœ°ç†ä¿¡æ¯ç³»ç»Ÿå’Œäººæœºäº¤äº’ç•Œé¢ï¼Œæ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.

