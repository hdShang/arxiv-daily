---
layout: default
title: MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation
---

# MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.17194" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.17194v1</a>
  <a href="https://arxiv.org/pdf/2512.17194.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.17194v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2512.17194v1', 'MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shengwei Zhao, Jingwen Yao, Sitong Wei, Linhai Xu, Yuying Liu, Dong Zhang, Zhiqiang Tian, Shaoyi Du

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-19

**å¤‡æ³¨**: This paper was accepted to AAAI2026

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMMRAG-RFTï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å¯è§£é‡Šæ€§ã€‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ` `å¼ºåŒ–å­¦ä¹ ` `å¯è§£é‡Šæ€§` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `ä¸¤é˜¶æ®µå¾®è°ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰MMRAGæ–¹æ³•ç¼ºä¹å¯¹æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹çš„æ¨ç†é€»è¾‘çš„è§£é‡Šï¼Œå¯¼è‡´ç»“æœå¯è§£é‡Šæ€§ä¸è¶³ã€‚
2. æå‡ºMMRAG-RFTæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚
3. åœ¨WebQAå’ŒMultimodalQAæ•°æ®é›†ä¸Šå–å¾—äº†SOTAç»“æœï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ(MMRAG)é€šè¿‡æ•´åˆå¤–éƒ¨å¤šæ¨¡æ€çŸ¥è¯†æ¥å®ç°é«˜åº¦å¯ä¿¡çš„ç”Ÿæˆï¼Œåœ¨å¤æ‚çš„å¤šæ¨¡æ€åœºæ™¯ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MMRAGæ–¹æ³•æœªèƒ½é˜æ˜æ£€ç´¢å’Œå“åº”ç”ŸæˆèƒŒåçš„æ¨ç†é€»è¾‘ï¼Œè¿™é™åˆ¶äº†ç»“æœçš„å¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å¾®è°ƒæ¡†æ¶å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œå®ç°å¯è§£é‡Šçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å¾®è°ƒæ¥æ‰§è¡Œå¤šæ¨¡æ€æ–‡æ¡£çš„ç²—ç²’åº¦é€ç‚¹æ’åºï¼Œæœ‰æ•ˆåœ°è¿‡æ»¤æ‰é‚£äº›æ˜¾è‘—ä¸ç›¸å…³çš„æ–‡æ¡£ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨åŸºäºæ¨ç†çš„å¼ºåŒ–å¾®è°ƒæ¥è”åˆä¼˜åŒ–ç»†ç²’åº¦çš„åˆ—è¡¨å¼æ’åºå’Œç­”æ¡ˆç”Ÿæˆï¼Œå¼•å¯¼å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨MMRAGè¿‡ç¨‹ä¸­è¾“å‡ºå¯è§£é‡Šçš„æ¨ç†é€»è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨WebQAå’ŒMultimodalQAè¿™ä¸¤ä¸ªå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶é€šè¿‡å…¨é¢çš„æ¶ˆèå®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMMRAGï¼‰æ–¹æ³•åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶ï¼Œç¼ºä¹å¯¹æ£€ç´¢åˆ°çš„å¤šæ¨¡æ€æ–‡æ¡£è¿›è¡Œæœ‰æ•ˆæ’åºå’Œæ¨ç†çš„èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœç¼ºä¹å¯è§£é‡Šæ€§ã€‚ç”¨æˆ·éš¾ä»¥ç†è§£æ¨¡å‹ä¸ºä½•é€‰æ‹©ç‰¹å®šçš„æ–‡æ¡£ä»¥åŠå¦‚ä½•åˆ©ç”¨è¿™äº›æ–‡æ¡£ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æ˜ç¡®åœ°å»ºæ¨¡æ£€ç´¢å’Œç”Ÿæˆä¹‹é—´çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°å—åˆ°é™åˆ¶ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾å¼åœ°å»ºæ¨¡å¤šæ¨¡æ€æ–‡æ¡£çš„æ’åºå’Œç­”æ¡ˆç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œæå‡MMRAGçš„å¯è§£é‡Šæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„å¼ºåŒ–å¾®è°ƒï¼Œé¦–å…ˆè¿›è¡Œç²—ç²’åº¦çš„æ–‡æ¡£è¿‡æ»¤ï¼Œç„¶åè¿›è¡Œç»†ç²’åº¦çš„æ’åºå’Œç­”æ¡ˆç”Ÿæˆï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ å¯è§£é‡Šçš„æ¨ç†é€»è¾‘ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMMRAG-RFTæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š1) åŸºäºè§„åˆ™çš„å¼ºåŒ–å¾®è°ƒï¼ˆRule-based Reinforcement Fine-tuningï¼‰ï¼šå¯¹å¤šæ¨¡æ€æ–‡æ¡£è¿›è¡Œç²—ç²’åº¦çš„é€ç‚¹æ’åºï¼Œè¿‡æ»¤æ‰ä¸ç›¸å…³çš„æ–‡æ¡£ã€‚2) åŸºäºæ¨ç†çš„å¼ºåŒ–å¾®è°ƒï¼ˆReasoning-based Reinforcement Fine-tuningï¼‰ï¼šè”åˆä¼˜åŒ–ç»†ç²’åº¦çš„åˆ—è¡¨å¼æ’åºå’Œç­”æ¡ˆç”Ÿæˆï¼Œå¼•å¯¼æ¨¡å‹è¾“å‡ºå¯è§£é‡Šçš„æ¨ç†é€»è¾‘ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µéƒ½åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹çš„è¡Œä¸ºï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»çš„æ¨ç†ä¹ æƒ¯ã€‚

**å…³é”®åˆ›æ–°**ï¼šè¯¥æ–¹æ³•æœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥åˆ°MMRAGæ¡†æ¶ä¸­ï¼Œå¹¶è®¾è®¡äº†ä¸¤é˜¶æ®µçš„å¼ºåŒ–å¾®è°ƒç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ›´å¥½åœ°å»ºæ¨¡åºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œä»è€Œä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆä¹‹é—´çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œä¸¤é˜¶æ®µçš„å¾®è°ƒç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ•ˆç‡å’Œæ•ˆæœï¼Œå…ˆè¿›è¡Œç²—ç²’åº¦è¿‡æ»¤ï¼Œå†è¿›è¡Œç»†ç²’åº¦ä¼˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°æ¥æŒ‡å¯¼æ¨¡å‹çš„å­¦ä¹ ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸é—®é¢˜ç›¸å…³ï¼Œåˆ™ç»™äºˆæ­£å‘å¥–åŠ±ï¼Œå¦åˆ™ç»™äºˆè´Ÿå‘å¥–åŠ±ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨åŸºäºæ¨ç†çš„å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ¨¡å‹ç”ŸæˆåŒ…å«æ¨ç†é€»è¾‘çš„ç­”æ¡ˆã€‚å…·ä½“çš„å¥–åŠ±å‡½æ•°è®¾è®¡éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯èƒ½æ¶‰åŠä¸€äº›å…³äºç½‘ç»œç»“æ„ã€æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥çš„ç»†èŠ‚ï¼Œä½†å…·ä½“å†…å®¹æœªçŸ¥ã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17194v1/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.17194v1/x2.png" alt="fig_1" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

MMRAG-RFTåœ¨WebQAå’ŒMultimodalQAä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„ç»“æœã€‚å…·ä½“çš„æ€§èƒ½æå‡æ•°æ®æœªçŸ¥ï¼Œä½†æ‘˜è¦ä¸­æ˜ç¡®æŒ‡å‡ºé€šè¿‡å…¨é¢çš„æ¶ˆèå®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•åœ¨æå‡å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶æˆæœå¯åº”ç”¨äºéœ€è¦é«˜åº¦å¯ä¿¡å’Œå¯è§£é‡Šæ€§çš„å¤šæ¨¡æ€é—®ç­”ç³»ç»Ÿã€æ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›æ¸…æ™°çš„æ¨ç†é€»è¾‘ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·å¯¹AIç³»ç»Ÿå†³ç­–çš„ä¿¡ä»»æ„Ÿï¼Œå¹¶ä¿ƒè¿›äººæœºåä½œã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ‰©å±•åˆ°æ›´å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚å¤šæ¨¡æ€å¯¹è¯ç”Ÿæˆã€å¤šæ¨¡æ€å†…å®¹åˆ›ä½œç­‰ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

