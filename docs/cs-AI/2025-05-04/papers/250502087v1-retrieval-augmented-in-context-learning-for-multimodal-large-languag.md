---
layout: default
title: Retrieval-augmented in-context learning for multimodal large language models in disease classification
---

# Retrieval-augmented in-context learning for multimodal large language models in disease classification

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02087" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02087v1</a>
  <a href="https://arxiv.org/pdf/2505.02087.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02087v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02087v1', 'Retrieval-augmented in-context learning for multimodal large language models in disease classification')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zaifu Zhan, Shuang Zhou, Xiaoshan Zhou, Yongkang Xiao, Jun Wang, Jiawen Deng, He Zhu, Yu Hou, Rui Zhang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-04

**å¤‡æ³¨**: 17 Pages, 1 figure, 7 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAICLæ¡†æ¶ä»¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç–¾ç—…åˆ†ç±»ä¸­çš„è¡¨ç°**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `ç–¾ç—…åˆ†ç±»` `ä¸Šä¸‹æ–‡å­¦ä¹ ` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `æ·±åº¦å­¦ä¹ ` `åŒ»ç–—å½±åƒåˆ†æ` `å¤§è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ç–¾ç—…åˆ†ç±»ä¸­é¢ä¸´ç¤ºä¾‹é€‰æ‹©ä¸å½“å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæœä¸è¶³çš„æŒ‘æˆ˜ã€‚
2. è®ºæ–‡æå‡ºçš„RAICLæ¡†æ¶é€šè¿‡åŠ¨æ€æ£€ç´¢ç›¸ä¼¼ç¤ºä¾‹ï¼Œç»“åˆRAGå’ŒICLï¼Œæå‡å¤šæ¨¡æ€å­¦ä¹ æ•ˆæœã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAICLåœ¨TCGAå’ŒIUèƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡åˆ†åˆ«æå‡è‡³0.8368å’Œ0.8658ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬ç ”ç©¶æ—¨åœ¨åŠ¨æ€æ£€ç´¢ä¿¡æ¯ä¸°å¯Œçš„ç¤ºä¾‹ï¼Œå¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç–¾ç—…åˆ†ç±»ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†æ£€ç´¢å¢å¼ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆRAICLï¼‰æ¡†æ¶ï¼Œç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œè‡ªé€‚åº”é€‰æ‹©å…·æœ‰ç›¸ä¼¼ç–¾ç—…æ¨¡å¼çš„ç¤ºä¾‹ï¼Œä»è€Œæé«˜MLLMsçš„å­¦ä¹ æ•ˆæœã€‚é€šè¿‡åœ¨çœŸå®çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆTCGAå’ŒIUèƒ¸éƒ¨Xå…‰ï¼‰ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒRAICLåœ¨å¤šä¸ªMLLMsä¸Šè¡¨ç°å‡ºä¸€è‡´çš„åˆ†ç±»æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç–¾ç—…åˆ†ç±»ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ•ˆæœä¸è¶³ï¼Œç°æœ‰æ–¹æ³•åœ¨ç¤ºä¾‹é€‰æ‹©ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´åˆ†ç±»æ€§èƒ½ä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRAICLæ¡†æ¶é€šè¿‡åŠ¨æ€æ£€ç´¢ä¸ä»»åŠ¡ç›¸å…³çš„ç¤ºä¾‹ï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨å·²æœ‰çŸ¥è¯†è¿›è¡Œå­¦ä¹ ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRAICLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨ä¸åŒç¼–ç å™¨ï¼ˆå¦‚ResNetã€BERTã€BioBERTå’ŒClinicalBERTï¼‰ç”ŸæˆåµŒå…¥ï¼›å…¶æ¬¡ï¼ŒåŸºäºç›¸ä¼¼åº¦åº¦é‡æ£€ç´¢åˆé€‚çš„ç¤ºä¾‹ï¼›æœ€åï¼Œæ„å»ºä¼˜åŒ–çš„å¯¹è¯æç¤ºä»¥å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šRAICLçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶è‡ªé€‚åº”ç¤ºä¾‹æ£€ç´¢æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ç–¾ç—…æ¨¡å¼é€‰æ‹©æœ€ç›¸å…³çš„ç¤ºä¾‹ï¼Œä»è€Œæ˜¾è‘—æå‡å­¦ä¹ æ•ˆæœï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†å¤šç§ç›¸ä¼¼åº¦åº¦é‡ï¼ˆå¦‚æ¬§å‡ é‡Œå¾—è·ç¦»å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼Œå¹¶å‘ç°æ¬§å‡ é‡Œå¾—è·ç¦»åœ¨å‡†ç¡®ç‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€Œä½™å¼¦ç›¸ä¼¼åº¦åœ¨å®è§‚F1åˆ†æ•°ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

RAICLåœ¨TCGAæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä»0.7854æå‡è‡³0.8368ï¼Œåœ¨IUèƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šä»0.7924æå‡è‡³0.8658ï¼Œæ˜¾ç¤ºå‡ºå¤šæ¨¡æ€è¾“å…¥ç›¸è¾ƒäºå•æ¨¡æ€è¾“å…¥çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”éšç€æ£€ç´¢ç¤ºä¾‹æ•°é‡çš„å¢åŠ ï¼Œæ€§èƒ½æŒç»­æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„RAICLæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨åŒ»ç–—å½±åƒåˆ†æå’Œç–¾ç—…è¯Šæ–­ä¸­ï¼Œå¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°åˆ†ç±»å’Œè¯†åˆ«ç–¾ç—…ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•è¿˜å¯æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸçš„å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ï¼Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„å†³ç­–èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Objectives: We aim to dynamically retrieve informative demonstrations, enhancing in-context learning in multimodal large language models (MLLMs) for disease classification.
>   Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL) framework, which integrates retrieval-augmented generation (RAG) and in-context learning (ICL) to adaptively select demonstrations with similar disease patterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines embeddings from diverse encoders, including ResNet, BERT, BioBERT, and ClinicalBERT, to retrieve appropriate demonstrations, and constructs conversational prompts optimized for ICL. We evaluated the framework on two real-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its performance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies, similarity metrics, and varying numbers of demonstrations.
>   Results: RAICL consistently improved classification performance. Accuracy increased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest X-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs being stronger than images alone. The richness of information embedded in each modality will determine which embedding model can be used to get better results. Few-shot experiments showed that increasing the number of retrieved examples further enhanced performance. Across different similarity metrics, Euclidean distance achieved the highest accuracy while cosine similarity yielded better macro-F1 scores. RAICL demonstrated consistent improvements across various MLLMs, confirming its robustness and versatility.
>   Conclusions: RAICL provides an efficient and scalable approach to enhance in-context learning in MLLMs for multimodal disease classification.

