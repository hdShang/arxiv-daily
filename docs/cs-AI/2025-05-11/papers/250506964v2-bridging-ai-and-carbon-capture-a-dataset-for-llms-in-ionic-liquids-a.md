---
layout: default
title: "Bridging AI and Carbon Capture: A Dataset for LLMs in Ionic Liquids and CBE Research"
---

# Bridging AI and Carbon Capture: A Dataset for LLMs in Ionic Liquids and CBE Research

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06964" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06964v2</a>
  <a href="https://arxiv.org/pdf/2505.06964.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06964v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06964v2', 'Bridging AI and Carbon Capture: A Dataset for LLMs in Ionic Liquids and CBE Research')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gaurab Sarkar, Sougata Saha

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-11 (æ›´æ–°: 2025-05-17)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºé’ˆå¯¹ç¦»å­æ¶²ä½“çš„LLMè¯„ä¼°æ•°æ®é›†ä»¥ä¿ƒè¿›ç¢³æ•é›†ç ”ç©¶**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç¦»å­æ¶²ä½“` `ç¢³æ•é›†` `æ•°æ®é›†` `åŒ–å­¦ä¸ç”Ÿç‰©å·¥ç¨‹` `æ¨ç†èƒ½åŠ›` `ç¯å¢ƒç§‘å­¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMsåœ¨åŒ–å­¦ä¸ç”Ÿç‰©å·¥ç¨‹é¢†åŸŸçš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŸºå‡†æµ‹è¯•ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŒ…å«5920ä¸ªç¤ºä¾‹çš„ä¸“å®¶ç­–åˆ’æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨ç¦»å­æ¶²ä½“é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å°å‹é€šç”¨LLMså¯¹ç¦»å­æ¶²ä½“æœ‰åŸºæœ¬äº†è§£ï¼Œä½†åœ¨é«˜çº§æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„é¢†åŸŸçš„çŸ¥è¯†å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒ–å­¦ä¸ç”Ÿç‰©å·¥ç¨‹ï¼ˆCBEï¼‰ç­‰ä¸“ä¸šé¢†åŸŸçš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„å®è¯åˆ†æï¼Œé‡ç‚¹å…³æ³¨ç¦»å­æ¶²ä½“ï¼ˆILsï¼‰åœ¨ç¢³æ•é›†ä¸­çš„åº”ç”¨ï¼Œå¼€å‘å¹¶å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«5920ä¸ªç¤ºä¾‹çš„ä¸“å®¶ç­–åˆ’æ•°æ®é›†ï¼Œä»¥è¯„ä¼°LLMsåœ¨è¯¥é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸‰ç§å‚æ•°å°‘äº100äº¿çš„å¼€æºLLMsè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å°å‹é€šç”¨LLMså¯¹ILsçš„åŸºæœ¬çŸ¥è¯†æœ‰æ‰€äº†è§£ï¼Œä½†ç¼ºä¹é«˜çº§åº”ç”¨æ‰€éœ€çš„ä¸“ä¸šæ¨ç†èƒ½åŠ›ã€‚åŸºäºè¿™äº›ç»“æœï¼Œè®¨è®ºäº†å¢å¼ºLLMsåœ¨ç¢³æ•é›†ç ”ç©¶ä¸­å®ç”¨æ€§çš„ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ–å­¦ä¸ç”Ÿç‰©å·¥ç¨‹é¢†åŸŸï¼Œå°¤å…¶æ˜¯ç¦»å­æ¶²ä½“ç¢³æ•é›†åº”ç”¨ä¸­çš„æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°åŸºå‡†ï¼Œå¯¼è‡´LLMsåœ¨ä¸“ä¸šçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸Šçš„è¯„ä¼°ä¸å¤Ÿå…¨é¢ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–éš¾åº¦å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„æ•°æ®é›†ï¼Œæ¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°LLMsåœ¨ç¦»å­æ¶²ä½“ç ”ç©¶ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†çš„è®¾è®¡æ—¨åœ¨å¹³è¡¡è¯­è¨€å¤æ‚æ€§ä¸ä¸“ä¸šçŸ¥è¯†ï¼Œä»¥ä¾¿æ›´å¥½åœ°åæ˜ LLMsçš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†çš„æ„å»ºã€LLMsçš„é€‰æ‹©ä¸è¯„ä¼°ã€ä»¥åŠç»“æœåˆ†æä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é›†ç”±ä¸“å®¶ç­–åˆ’ï¼Œæ¶µç›–ä¸åŒéš¾åº¦çš„ç¤ºä¾‹ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„ä¸»è¦åˆ›æ–°åœ¨äºé¦–æ¬¡é’ˆå¯¹ç¦»å­æ¶²ä½“é¢†åŸŸæ„å»ºäº†ä¸“é—¨çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡å®è¯åˆ†ææ­ç¤ºäº†LLMsåœ¨è¯¥é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ä¸ä¸è¶³ä¹‹å¤„ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰çš„é€šç”¨è¯„ä¼°æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œå¼ºè°ƒäº†é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„é‡è¦æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šæ•°æ®é›†åŒ…å«5920ä¸ªç¤ºä¾‹ï¼Œè®¾è®¡æ—¶è€ƒè™‘äº†è¯­è¨€å¤æ‚æ€§å’Œé¢†åŸŸçŸ¥è¯†çš„å¹³è¡¡ã€‚è¯„ä¼°è¿‡ç¨‹ä¸­é€‰æ‹©äº†ä¸‰ç§å‚æ•°å°‘äº100äº¿çš„å¼€æºLLMsï¼Œç¡®ä¿ç»“æœçš„å¯æ¯”æ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œå°å‹é€šç”¨LLMsåœ¨ç¦»å­æ¶²ä½“çŸ¥è¯†æ–¹é¢è¡¨ç°å‡ºåŸºæœ¬ç†è§£ï¼Œä½†åœ¨é«˜çº§æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚é€šè¿‡ä½¿ç”¨æ–°æ„å»ºçš„æ•°æ®é›†ï¼Œè¯„ä¼°ç»“æœä¸ºæœªæ¥LLMsåœ¨ç¢³æ•é›†ç ”ç©¶ä¸­çš„åº”ç”¨æä¾›äº†é‡è¦çš„æ”¹è¿›æ–¹å‘ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬åŒ–å­¦ä¸ç”Ÿç‰©å·¥ç¨‹ã€ç¯å¢ƒç§‘å­¦ä»¥åŠäººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ çš„äº¤å‰ç ”ç©¶ã€‚é€šè¿‡æå‡LLMsåœ¨ç¢³æ•é›†ç ”ç©¶ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¿ƒè¿›ç¦»å­æ¶²ä½“çš„åº”ç”¨å¼€å‘ï¼Œè¿›è€Œæ¨åŠ¨å…¨çƒç¢³ä¸­å’Œç›®æ ‡çš„å®ç°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated exceptional performance in general knowledge and reasoning tasks across various domains. However, their effectiveness in specialized scientific fields like Chemical and Biological Engineering (CBE) remains underexplored. Addressing this gap requires robust evaluation benchmarks that assess both knowledge and reasoning capabilities in these niche areas, which are currently lacking. To bridge this divide, we present a comprehensive empirical analysis of LLM reasoning capabilities in CBE, with a focus on Ionic Liquids (ILs) for carbon sequestration - an emerging solution for mitigating global warming. We develop and release an expert - curated dataset of 5,920 examples designed to benchmark LLMs' reasoning in this domain. The dataset incorporates varying levels of difficulty, balancing linguistic complexity and domain-specific knowledge. Using this dataset, we evaluate three open-source LLMs with fewer than 10 billion parameters. Our findings reveal that while smaller general-purpose LLMs exhibit basic knowledge of ILs, they lack the specialized reasoning skills necessary for advanced applications. Building on these results, we discuss strategies to enhance the utility of LLMs for carbon capture research, particularly using ILs. Given the significant carbon footprint of LLMs, aligning their development with IL research presents a unique opportunity to foster mutual progress in both fields and advance global efforts toward achieving carbon neutrality by 2050.

