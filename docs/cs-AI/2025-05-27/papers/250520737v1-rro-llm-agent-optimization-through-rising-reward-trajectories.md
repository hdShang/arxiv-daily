---
layout: default
title: RRO: LLM Agent Optimization Through Rising Reward Trajectories
---

# RRO: LLM Agent Optimization Through Rising Reward Trajectories

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20737" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.20737v1</a>
  <a href="https://arxiv.org/pdf/2505.20737.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20737v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20737v1', 'RRO: LLM Agent Optimization Through Rising Reward Trajectories')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zilong Wang, Jingfeng Yang, Sreyashi Nag, Samarth Varshney, Xianfeng Tang, Haoming Jiang, Jingbo Shang, Sheikh Muhammad Sarwar

**ÂàÜÁ±ª**: cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-27

**Â§áÊ≥®**: preprint

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RRO‰ª•‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öÊ≠•‰ªªÂä°ÊâßË°å**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `ËøáÁ®ãÂ•ñÂä±Ê®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `Â§öÊ≠•Êé®ÁêÜ` `Â•ñÂä±‰ºòÂåñ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÂú®Â§ÑÁêÜÂ§öÊ≠•Êé®ÁêÜ‰ªªÂä°Êó∂ËÆ°ÁÆóÊàêÊú¨È´òÔºåÈöæ‰ª•Êâ©Â±ïÔºåÂØºËá¥Êô∫ËÉΩ‰ΩìÂÆπÊòìÂõ†ÁªÜÂæÆÈîôËØØËÄåÂ§±Ë¥•„ÄÇ
2. Êú¨ÊñáÊèêÂá∫Â•ñÂä±‰∏äÂçá‰ºòÂåñÔºàRROÔºâÔºåÈÄöËøáÂÖ≥Ê≥®Áõ∏ÈÇªÊé®ÁêÜÊ≠•È™§ÁöÑÁõ∏ÂØπÂ•ñÂä±Ë∂ãÂäøÔºåÂä®ÊÄÅÁª¥Êä§Â•ñÂä±‰∏äÂçáÔºå‰ªéËÄå‰ºòÂåñËøáÁ®ãÁõëÁù£„ÄÇ
3. Âú®WebShopÂíåInterCode-SQLÂü∫ÂáÜÊµãËØï‰∏≠ÔºåRROÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÊé¢Á¥¢ÊàêÊú¨ÔºåÂ±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÂÆûÁî®ÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®‰Ωú‰∏∫Êô∫ËÉΩ‰ΩìËß£ÂÜ≥Â§çÊùÇÁöÑÂ§öÊ≠•‰ªªÂä°Êó∂‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂØπÊé®ÁêÜËøáÁ®ãËøõË°åÊ†°ÂáÜÔºå‰ΩøÁî®ËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÂØπÊØè‰∏ÄÊ≠•ËøõË°åÂ•ñÂä±ÊàñÊÉ©ÁΩö„ÄÇÁÑ∂ËÄåÔºåPRMsÂú®Â§ÑÁêÜÂ§ßÈáèÂÄôÈÄâÂä®‰ΩúÊó∂Èöæ‰ª•Êâ©Â±ïÔºåËÆ°ÁÆóÊàêÊú¨È´ò„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ºòÂåñÊñπÊ≥ï‚Äî‚ÄîÂ•ñÂä±‰∏äÂçá‰ºòÂåñÔºàRROÔºâÔºåÈÄöËøáÁª¥Êä§Êî∂ÈõÜËΩ®Ëøπ‰∏≠ÁöÑÂ•ñÂä±‰∏äÂçáË∂ãÂäøÊù•ËøõË°åËøáÁ®ãÁõëÁù£Ôºå‰ªéËÄåÂä®ÊÄÅÊâ©Â±ï‰∏ã‰∏ÄÊ≠•ÂÄôÈÄâÂä®‰ΩúÁöÑÊêúÁ¥¢Á©∫Èó¥ÔºåÊòæËëóÊèêÈ´òÊï∞ÊçÆÊçïËé∑ÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRROÂú®WebShopÂíåInterCode-SQLÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÂêåÊó∂Êé¢Á¥¢ÊàêÊú¨Â§ßÂπÖÈôç‰Ωé„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÂ§öÊ≠•‰ªªÂä°‰∏≠ÁöÑÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÂ•ñÂä±ËÆ°ÁÆóÊàêÊú¨È´ò„ÄÅÈöæ‰ª•Êâ©Â±ïÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÈúÄË¶ÅÂ§ßÈáèËÆ°ÁÆóÊù•Ëé∑ÂèñËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥Êô∫ËÉΩ‰ΩìÂú®ÂÖ≥ÈîÆÊ≠•È™§‰∏äÂÆπÊòìÂ§±Ë¥•„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫ÁöÑÂ•ñÂä±‰∏äÂçá‰ºòÂåñÔºàRROÔºâÊñπÊ≥ïÔºåÈÄöËøáÂÖ≥Ê≥®Áõ∏ÈÇªÊé®ÁêÜÊ≠•È™§ÁöÑÂ•ñÂä±ÂèòÂåñÔºåÂä®ÊÄÅÁª¥Êä§Â•ñÂä±‰∏äÂçáË∂ãÂäøÔºå‰ªéËÄåÊúâÊïàÊçïËé∑È´òË¥®ÈáèÊï∞ÊçÆÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRROÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Êï∞ÊçÆÊî∂ÈõÜ„ÄÅÂ•ñÂä±ËÆ°ÁÆóÂíåËøáÁ®ãÁõëÁù£‰∏â‰∏™‰∏ªË¶ÅÊ®°Âùó„ÄÇÈ¶ñÂÖàÊî∂ÈõÜÊé®ÁêÜËΩ®ËøπÔºåÁÑ∂ÂêéËÆ°ÁÆóÁõ∏ÈÇªÊ≠•È™§ÁöÑÂ•ñÂä±Â∑ÆÂºÇÔºåÊúÄÂêéÊ†πÊçÆÂ•ñÂä±‰∏äÂçáÊÉÖÂÜµËøõË°åËøáÁ®ãÁõëÁù£„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRROÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÈÄöËøáÁª¥Êä§Â•ñÂä±‰∏äÂçáÁöÑË∂ãÂäøÊù•ËøõË°åËøáÁ®ãÁõëÁù£ÔºåËøô‰∏é‰º†ÁªüÁöÑÈÄêÊ≠•Â•ñÂä±Êú∫Âà∂‰∏çÂêåÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Êâ©Â±ïÊêúÁ¥¢Á©∫Èó¥Âπ∂ÊèêÈ´òÊï∞ÊçÆË¥®Èáè„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®RRO‰∏≠ÔºåÂÖ≥ÈîÆÂèÇÊï∞ÂåÖÊã¨Â•ñÂä±Â∑ÆÂºÇÁöÑÈòàÂÄºËÆæÂÆöÂíåÂä®ÊÄÅË∞ÉÊï¥ÁöÑËøáÁ®ãÁõëÁù£Á≠ñÁï•ÔºåÁ°Æ‰øùÂú®ÊØè‰∏ÄÊ≠•ÈÉΩËÉΩÊúâÊïàÊçïËé∑Âà∞Ê≠£ÂêëÂ•ñÂä±Â∑ÆÂºÇ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRROÂú®WebShopÂíåInterCode-SQLÂü∫ÂáÜÊµãËØï‰∏≠Áõ∏ÊØî‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçáÊòæËëóÔºåÊé¢Á¥¢ÊàêÊú¨Èôç‰Ωé‰∫Ü50%‰ª•‰∏äÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§öÊ≠•Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÂä©Êâã„ÄÅËá™Âä®ÂåñÂÜ≥Á≠ñÁ≥ªÁªüÂíåÂ§çÊùÇ‰ªªÂä°ÁöÑËá™Âä®ÂåñÊâßË°å„ÄÇÈÄöËøá‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ãÔºåRROËÉΩÂ§üÂú®Â§öÁßçÂÆûÈôÖÂú∫ÊôØ‰∏≠ÊèêÈ´ò‰ªªÂä°ÂÆåÊàêÁéáÂíåÊïàÁéáÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÂπøÊ≥õÁöÑÂ∫îÁî®ÂâçÊôØ„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Large language models (LLMs) have exhibited extraordinary performance in a variety of tasks while it remains challenging for them to solve complex multi-step tasks as agents. In practice, agents sensitive to the outcome of certain key steps which makes them likely to fail the task because of a subtle mistake in the planning trajectory. Recent approaches resort to calibrating the reasoning process through reinforcement learning. They reward or penalize every reasoning step with process supervision, as known as Process Reward Models (PRMs). However, PRMs are difficult and costly to scale up with a large number of next action candidates since they require extensive computations to acquire the training data through the per-step trajectory exploration. To mitigate this issue, we focus on the relative reward trend across successive reasoning steps and propose maintaining an increasing reward in the collected trajectories for process supervision, which we term Reward Rising Optimization (RRO). Specifically, we incrementally augment the process supervision until identifying a step exhibiting positive reward differentials, i.e. rising rewards, relative to its preceding iteration. This method dynamically expands the search space for the next action candidates, efficiently capturing high-quality data. We provide mathematical groundings and empirical results on the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO achieves superior performance while requiring much less exploration cost.

