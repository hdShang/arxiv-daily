---
layout: default
title: "MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning"
---

# MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20670" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20670v2</a>
  <a href="https://arxiv.org/pdf/2505.20670.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20670v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20670v2', 'MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zikang Guo, Benfeng Xu, Xiaorui Wang, Zhendong Mao

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-06-05)

**å¤‡æ³¨**: Accepted to 34rd International Joint Conference on Artificial Intelligence (IJCAI 2025)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMIRRORæ¡†æ¶ä»¥ä¼˜åŒ–å·¥å…·å­¦ä¹ ä¸­çš„å¤šæ™ºèƒ½ä½“åæ€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å·¥å…·å­¦ä¹ ` `åæ€æœºåˆ¶` `å†³ç­–ä¼˜åŒ–` `å¤§å‹è¯­è¨€æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤æ‚å·¥å…·é›†æˆä»»åŠ¡ä¸­é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é”™è¯¯è½¨è¿¹çš„çº æ­£æ–¹é¢ã€‚
2. MIRRORæ¡†æ¶é€šè¿‡å¼•å…¥è¡ŒåŠ¨å‰çš„å†…éƒ¨åæ€å’ŒåŸºäºè§‚å¯Ÿçš„å¤–éƒ¨åæ€ï¼Œå…¨é¢æå‡äº†æ™ºèƒ½ä½“çš„å†³ç­–è´¨é‡ã€‚
3. åœ¨StableToolBenchå’ŒTravelPlanneråŸºå‡†æµ‹è¯•ä¸­ï¼ŒMIRRORè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤æ‚çš„å·¥å…·é›†æˆä»»åŠ¡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œä¿ƒä½¿å¤šæ™ºèƒ½ä½“å·¥ä½œæµæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚åæ€è¢«è®¤ä¸ºæ˜¯çº æ­£æ™ºèƒ½ä½“å·¥ä½œæµä¸­é”™è¯¯è½¨è¿¹çš„æœ‰æ•ˆç­–ç•¥ï¼Œä½†ç°æœ‰æ–¹æ³•ä»…åœ¨è¡ŒåŠ¨æ‰§è¡Œååˆ©ç”¨è¿™ä¸€èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºMIRRORæ¡†æ¶ï¼ŒåŒ…å«è¡ŒåŠ¨å‰çš„å†…éƒ¨åæ€å’ŒåŸºäºè§‚å¯Ÿçš„å¤–éƒ¨åæ€ï¼Œç³»ç»Ÿæ€§åœ°åˆ©ç”¨LLMsçš„åæ€èƒ½åŠ›ï¼Œæ¶ˆé™¤å’Œçº æ­£é”™è¯¯è¡ŒåŠ¨ã€‚é€šè¿‡åœ¨StableToolBenchå’ŒTravelPlanneråŸºå‡†ä¸Šçš„è¯„ä¼°ï¼ŒMIRRORå±•ç¤ºäº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†ç°æœ‰æ–¹æ³•çš„æœ€å…ˆè¿›ç»“æœã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤æ‚ä»»åŠ¡ä¸­å¤šæ™ºèƒ½ä½“å·¥ä½œæµçš„å†³ç­–é”™è¯¯é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä»…åœ¨è¡ŒåŠ¨åè¿›è¡Œåæ€ï¼Œæ— æ³•æœ‰æ•ˆé¢„é˜²é”™è¯¯çš„å‘ç”Ÿã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMIRRORæ¡†æ¶é€šè¿‡å¼•å…¥å†…éƒ¨åæ€æœºåˆ¶ï¼Œä½¿æ™ºèƒ½ä½“åœ¨æ‰§è¡Œè¡ŒåŠ¨å‰èƒ½å¤Ÿè¯„ä¼°å…¶å†³ç­–çš„æ½œåœ¨åæœï¼Œä»è€Œå‡å°‘é”™è¯¯çš„ä¼ æ’­ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMIRRORæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šå†…éƒ¨åæ€æ¨¡å—ç”¨äºè¯„ä¼°é¢„æœŸè¡ŒåŠ¨ï¼Œå¤–éƒ¨åæ€æ¨¡å—åˆ™æ ¹æ®è§‚å¯Ÿç»“æœè°ƒæ•´æ‰§è¡Œè½¨è¿¹ã€‚

**å…³é”®åˆ›æ–°**ï¼šMIRRORçš„åˆ›æ–°åœ¨äºåŒæ—¶å®ç°äº†è¡ŒåŠ¨å‰å’Œè¡ŒåŠ¨åçš„åæ€æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å…¨é¢çš„é”™è¯¯çº æ­£ç­–ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒMIRRORé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åæ€è¿‡ç¨‹ï¼Œå¹¶ç»“åˆäº†å¤šç§ç½‘ç»œç»“æ„ä»¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒMIRRORåœ¨StableToolBenchå’ŒTravelPlanneråŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œå±•ç°äº†å…¶åœ¨å·¥å…·å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MIRRORæ¡†æ¶åœ¨æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨åŒ–å·¥å…·ä½¿ç”¨å’Œå¤æ‚ä»»åŠ¡è§„åˆ’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡ä¼˜åŒ–æ™ºèƒ½ä½“çš„å†³ç­–è¿‡ç¨‹ï¼Œè¯¥ç ”ç©¶èƒ½å¤Ÿæé«˜ç³»ç»Ÿçš„æ•´ä½“æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½ä½“åœ¨æ›´å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Complex tasks involving tool integration pose significant challenges for Large Language Models (LLMs), leading to the emergence of multi-agent workflows as a promising solution. Reflection has emerged as an effective strategy for correcting erroneous trajectories in agentic workflows. However, existing approaches only exploit such capability in the post-action stage, where the agent observes the execution outcomes. We argue that, like humans, LLMs can also engage in reflection before action execution: the agent can anticipate undesirable outcomes from its own decisions, which not only provides a necessarily complementary perspective to evaluate the decision but also prevents the propagation of errors throughout the trajectory. In this paper, we propose MIRROR, a framework that consists of both intra-reflection, which critically assesses intended actions before execution, and inter-reflection, which further adjusts the trajectory based on observations. This design systematically leverages LLM reflection capabilities to eliminate and rectify erroneous actions on a more comprehensive scope. Evaluations on both the StableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior performance, achieving state-of-the-art results compared to existing approaches.

