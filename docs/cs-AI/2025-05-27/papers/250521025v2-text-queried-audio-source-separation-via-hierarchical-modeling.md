---
layout: default
title: Text-Queried Audio Source Separation via Hierarchical Modeling
---

# Text-Queried Audio Source Separation via Hierarchical Modeling

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21025" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21025v2</a>
  <a href="https://arxiv.org/pdf/2505.21025.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21025v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21025v2', 'Text-Queried Audio Source Separation via Hierarchical Modeling')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinlei Yin, Xiulian Peng, Xue Jiang, Zhiwei Xiong, Yan Lu

**åˆ†ç±»**: cs.SD, cs.AI, cs.LG, eess.AS

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-12-02)

**å¤‡æ³¨**: Accepted by TASLP

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHSM-TSSæ¡†æ¶ä»¥è§£å†³æ–‡æœ¬æŸ¥è¯¢éŸ³é¢‘æºåˆ†ç¦»é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)**

**å…³é”®è¯**: `éŸ³é¢‘æºåˆ†ç¦»` `æ–‡æœ¬æŸ¥è¯¢` `è·¨æ¨¡æ€å­¦ä¹ ` `åˆ†å±‚æ¨¡å‹` `è¯­ä¹‰æ„ŸçŸ¥`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å£°å­¦ä¸æ–‡æœ¬å¯¹é½åŠè¯­ä¹‰æ„ŸçŸ¥åˆ†ç¦»æ–¹é¢å­˜åœ¨å»ºæ¨¡å›°éš¾ï¼Œä¸”ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ã€‚
2. æœ¬æ–‡æå‡ºHSM-TSSæ¡†æ¶ï¼Œé€šè¿‡å…¨å±€ä¸å±€éƒ¨è¯­ä¹‰ç‰¹å¾åˆ†ç¦»ï¼Œè§£å†³éŸ³é¢‘æºåˆ†ç¦»é—®é¢˜ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHSM-TSSåœ¨æ•°æ®æ•ˆç‡å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†ç¦»æ€§èƒ½ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åŸºäºè‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„ç›®æ ‡éŸ³é¢‘æºåˆ†ç¦»ä¸ºé€šè¿‡ä»»æ„æ–‡æœ¬æè¿°æå–éŸ³é¢‘äº‹ä»¶æä¾›äº†æ–°æ€è·¯ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯éš¾ä»¥åœ¨å•é˜¶æ®µæ¶æ„ä¸­æœ‰æ•ˆå»ºæ¨¡å£°å­¦ä¸æ–‡æœ¬çš„å¯¹é½åŠè¯­ä¹‰æ„ŸçŸ¥åˆ†ç¦»ï¼ŒäºŒæ˜¯ä¾èµ–å¤§è§„æ¨¡å‡†ç¡®æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ä»¥å¼¥è¡¥è·¨æ¨¡æ€å­¦ä¹ ä¸åˆ†ç¦»çš„ä½æ•ˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ†å±‚è§£æ„æ¡†æ¶HSM-TSSï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºå…¨å±€-å±€éƒ¨è¯­ä¹‰å¼•å¯¼çš„ç‰¹å¾åˆ†ç¦»ä¸ç»“æ„ä¿æŒçš„å£°å­¦é‡æ„ã€‚è¯¥æ–¹æ³•å¼•å…¥åŒé˜¶æ®µæœºåˆ¶ï¼Œåˆ†åˆ«åœ¨å…¨å±€å’Œå±€éƒ¨è¯­ä¹‰ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œè¯­ä¹‰åˆ†ç¦»ï¼Œæœ€ç»ˆåœ¨å¤æ‚çš„å¬è§‰åœºæ™¯ä¸­å®ç°äº†ä¼˜è¶Šçš„åˆ†ç¦»æ€§èƒ½ä¸è¯­ä¹‰ä¸€è‡´æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åŸºäºæ–‡æœ¬æŸ¥è¯¢çš„éŸ³é¢‘æºåˆ†ç¦»é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å£°å­¦ä¸æ–‡æœ¬çš„å¯¹é½åŠè¯­ä¹‰æ„ŸçŸ¥åˆ†ç¦»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸”é€šå¸¸ä¾èµ–å¤§é‡å‡†ç¡®æ ‡æ³¨çš„æ•°æ®ï¼Œå¯¼è‡´è·¨æ¨¡æ€å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºçš„HSM-TSSæ¡†æ¶é€šè¿‡åˆ†å±‚è§£æ„ï¼Œå°†éŸ³é¢‘æºåˆ†ç¦»ä»»åŠ¡åˆ†ä¸ºå…¨å±€å’Œå±€éƒ¨è¯­ä¹‰ç‰¹å¾çš„åˆ†ç¦»ï¼Œåˆ©ç”¨åŒé˜¶æ®µæœºåˆ¶æé«˜åˆ†ç¦»æ•ˆæœã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„å¬è§‰åœºæ™¯ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHSM-TSSæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆåœ¨å…¨å±€è¯­ä¹‰ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œå…¨å±€è¯­ä¹‰åˆ†ç¦»ï¼Œä½¿ç”¨Q-Audioæ¶æ„å¯¹éŸ³é¢‘å’Œæ–‡æœ¬æ¨¡æ€è¿›è¡Œå¯¹é½ï¼›æ¥ç€ï¼ŒåŸºäºé¢„æµ‹çš„å…¨å±€ç‰¹å¾ï¼Œåœ¨AudioMAEç‰¹å¾ä¸Šè¿›è¡Œå±€éƒ¨è¯­ä¹‰åˆ†ç¦»ï¼Œå¹¶è¿›è¡Œå£°å­¦é‡æ„ã€‚

**å…³é”®åˆ›æ–°**ï¼šHSM-TSSçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†åŒé˜¶æ®µçš„è¯­ä¹‰åˆ†ç¦»æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„è¯­ä¹‰ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„éŸ³é¢‘æºåˆ†ç¦»ã€‚è¿™ä¸€æ–¹æ³•ä¸ç°æœ‰å•é˜¶æ®µæ¶æ„çš„æœ¬è´¨åŒºåˆ«åœ¨äºå…¶åˆ†å±‚å¤„ç†çš„èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†Q-Audioæ¶æ„ä½œä¸ºå…¨å±€è¯­ä¹‰ç¼–ç å™¨ï¼Œç¡®ä¿éŸ³é¢‘ä¸æ–‡æœ¬çš„æœ‰æ•ˆå¯¹é½ã€‚æ­¤å¤–ï¼Œå±€éƒ¨è¯­ä¹‰åˆ†ç¦»é˜¶æ®µä½¿ç”¨AudioMAEç‰¹å¾ä»¥ä¿æŒæ—¶é—´-é¢‘ç‡ç»“æ„ï¼Œå¢å¼ºäº†å£°å­¦é‡æ„çš„è´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒHSM-TSSåœ¨éŸ³é¢‘æºåˆ†ç¦»ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤æ‚å¬è§‰åœºæ™¯ä¸­ï¼Œåˆ†ç¦»æ•ˆæœæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·ä½“æ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°XX%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨éŸ³é¢‘å¤„ç†ã€éŸ³ä¹åˆ¶ä½œã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡çµæ´»çš„å£°éŸ³æ“æ§èƒ½åŠ›ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®æ–‡æœ¬æè¿°è¿›è¡ŒéŸ³é¢‘äº‹ä»¶çš„æå–å’Œä¿®æ”¹ï¼Œæå‡äº†ç”¨æˆ·ä½“éªŒå’Œåˆ›ä½œè‡ªç”±åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Target audio source separation with natural language queries presents a promising paradigm for extracting arbitrary audio events through arbitrary text descriptions. Existing methods mainly face two challenges, the difficulty in jointly modeling acoustic-textual alignment and semantic-aware separation within a blindly-learned single-stage architecture, and the reliance on large-scale accurately-labeled training data to compensate for inefficient cross-modal learning and separation. To address these challenges, we propose a hierarchical decomposition framework, HSM-TSS, that decouples the task into global-local semantic-guided feature separation and structure-preserving acoustic reconstruction. Our approach introduces a dual-stage mechanism for semantic separation, operating on distinct global and local semantic feature spaces. We first perform global-semantic separation through a global semantic feature space aligned with text queries. A Q-Audio architecture is employed to align audio and text modalities, serving as pretrained global-semantic encoders. Conditioned on the predicted global feature, we then perform the second-stage local-semantic separation on AudioMAE features that preserve time-frequency structures, followed by acoustic reconstruction. We also propose an instruction processing pipeline to parse arbitrary text queries into structured operations, extraction or removal, coupled with audio descriptions, enabling flexible sound manipulation. Our method achieves state-of-the-art separation performance with data-efficient training while maintaining superior semantic consistency with queries in complex auditory scenes.

