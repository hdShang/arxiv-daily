---
layout: default
title: Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation
---

# Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21106" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21106v1</a>
  <a href="https://arxiv.org/pdf/2505.21106.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21106v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21106v1', 'Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Zhengyang Ji, Yifan Jia, Shang Gao, Yutao Yue

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**é€šè¿‡ä¿¡æ¯æµåˆ†æä¸å¤šè½®å¯¹è¯è¯„ä¼°æ­ç¤ºLVLMä¸­çš„ç¤¾ä¼šåè§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰è¯­è¨€æ¨¡å‹` `ç¤¾ä¼šåè§` `ä¿¡æ¯æµåˆ†æ` `å¤šè½®å¯¹è¯` `å¤šæ¨¡æ€å­¦ä¹ ` `æ¨¡å‹è§£é‡Šæ€§` `äººå·¥æ™ºèƒ½ä¼¦ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºæ£€æµ‹å’Œé‡åŒ–ç¤¾ä¼šåè§ï¼Œä½†å¯¹å…¶å†…éƒ¨æœºåˆ¶çš„ç†è§£ä¸è¶³ã€‚
2. æœ¬æ–‡æå‡ºç»“åˆä¿¡æ¯æµåˆ†æä¸å¤šè½®å¯¹è¯è¯„ä¼°çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºç¤¾ä¼šåè§çš„èµ·æºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLVLMåœ¨å¤„ç†ä¸åŒäººå£ç¾¤ä½“çš„å›¾åƒæ—¶ï¼Œä¿¡æ¯ä½¿ç”¨å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ­ç¤ºäº†åè§çš„å†…åœ¨æœºåˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿè¡¨ç°å‡ºæ˜æ˜¾çš„ç¤¾ä¼šåè§ã€‚è¿™äº›åè§é€šå¸¸è¡¨ç°ä¸ºä¸­æ€§æ¦‚å¿µä¸æ•æ„Ÿäººç±»å±æ€§ä¹‹é—´çš„æ„å¤–å…³è”ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ä¸åŒäººå£ç¾¤ä½“ä¸­çš„è¡Œä¸ºå·®å¼‚ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ£€æµ‹å’Œé‡åŒ–è¿™äº›åè§ä¸Šï¼Œä½†å¯¹æ¨¡å‹å†…éƒ¨æœºåˆ¶çš„ç†è§£æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆä¿¡æ¯æµåˆ†æä¸å¤šè½®å¯¹è¯è¯„ä¼°çš„è§£é‡Šæ¡†æ¶ï¼Œæ—¨åœ¨ä»ä¸å¹³è¡¡çš„å†…éƒ¨ä¿¡æ¯åˆ©ç”¨è§’åº¦ç†è§£ç¤¾ä¼šåè§çš„èµ·æºã€‚é€šè¿‡å®éªŒå‘ç°ï¼ŒLVLMåœ¨å¤„ç†ä¸åŒäººå£ç¾¤ä½“çš„å›¾åƒæ—¶ï¼Œä¿¡æ¯ä½¿ç”¨å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ï¼Œè¡¨æ˜ç¤¾ä¼šåè§æ·±æ¤äºæ¨¡å‹çš„å†…éƒ¨æ¨ç†åŠ¨æ€ä¸­ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä¸­å­˜åœ¨çš„ç¤¾ä¼šåè§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç†è§£æ¨¡å‹å†…éƒ¨æœºåˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ä¿¡æ¯æµåˆ†æè¯†åˆ«æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­é«˜è´¡çŒ®çš„å›¾åƒæ ‡è®°ï¼Œå¹¶è®¾è®¡å¤šè½®å¯¹è¯æœºåˆ¶è¯„ä¼°è¿™äº›æ ‡è®°ç¼–ç æ•æ„Ÿä¿¡æ¯çš„ç¨‹åº¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¿¡æ¯æµåˆ†ææ¨¡å—å’Œå¤šè½®å¯¹è¯è¯„ä¼°æ¨¡å—ï¼Œå‰è€…ç”¨äºè¯†åˆ«å…³é”®å›¾åƒæ ‡è®°ï¼Œåè€…ç”¨äºè¯„ä¼°è¿™äº›æ ‡è®°çš„æ•æ„Ÿä¿¡æ¯ç¼–ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šæå‡ºçš„ä¿¡æ¯æµåˆ†æä¸å¤šè½®å¯¹è¯è¯„ä¼°çš„ç»“åˆï¼Œä¸ºç†è§£ç¤¾ä¼šåè§çš„å½¢æˆæä¾›äº†æ–°çš„è§†è§’ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„æ£€æµ‹å’Œé‡åŒ–æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ä¿¡æ¯æµåˆ†æä¸­ï¼Œé‡ç‚¹è¯†åˆ«é«˜è´¡çŒ®å›¾åƒæ ‡è®°ï¼›åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œè®¾è®¡äº†ç‰¹å®šçš„å¯¹è¯æœºåˆ¶ä»¥è¯„ä¼°æ ‡è®°çš„æ•æ„Ÿæ€§ï¼Œç¡®ä¿å®éªŒçš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLVLMåœ¨å¤„ç†ä¸åŒäººå£ç¾¤ä½“çš„å›¾åƒæ—¶ï¼Œä¿¡æ¯ä½¿ç”¨å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ï¼Œè¡¨æ˜ç¤¾ä¼šåè§æ·±æ¤äºæ¨¡å‹çš„å†…éƒ¨æ¨ç†åŠ¨æ€ä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹åœ¨å¤„ç†æŸäº›ç¾¤ä½“æ—¶ï¼Œä¿¡æ¯æµçš„åˆ©ç”¨æ•ˆç‡æ˜¾è‘—ä½äºå…¶ä»–ç¾¤ä½“ï¼Œæ­ç¤ºäº†åè§çš„å†…åœ¨æœºåˆ¶ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç¤¾ä¼šç§‘å­¦ç ”ç©¶ã€äººå·¥æ™ºèƒ½ä¼¦ç†ä»¥åŠå¤šæ¨¡æ€ç³»ç»Ÿçš„è®¾è®¡ä¸ä¼˜åŒ–ã€‚é€šè¿‡ç†è§£å’Œå‡å°‘æ¨¡å‹ä¸­çš„ç¤¾ä¼šåè§ï¼Œå¯ä»¥æå‡AIç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­çš„å…¬å¹³æ€§å’Œå¯é æ€§ï¼Œä¿ƒè¿›æ›´å¹¿æ³›çš„ç¤¾ä¼šæ¥å—åº¦ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Vision Language Models (LVLMs) have achieved remarkable progress in multimodal tasks, yet they also exhibit notable social biases. These biases often manifest as unintended associations between neutral concepts and sensitive human attributes, leading to disparate model behaviors across demographic groups. While existing studies primarily focus on detecting and quantifying such biases, they offer limited insight into the underlying mechanisms within the models. To address this gap, we propose an explanatory framework that combines information flow analysis with multi-round dialogue evaluation, aiming to understand the origin of social bias from the perspective of imbalanced internal information utilization. Specifically, we first identify high-contribution image tokens involved in the model's reasoning process for neutral questions via information flow analysis. Then, we design a multi-turn dialogue mechanism to evaluate the extent to which these key tokens encode sensitive information. Extensive experiments reveal that LVLMs exhibit systematic disparities in information usage when processing images of different demographic groups, suggesting that social bias is deeply rooted in the model's internal reasoning dynamics. Furthermore, we complement our findings from a textual modality perspective, showing that the model's semantic representations already display biased proximity patterns, thereby offering a cross-modal explanation of bias formation.

