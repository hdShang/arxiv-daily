---
layout: default
title: Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation
---

# Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21784" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21784v1</a>
  <a href="https://arxiv.org/pdf/2505.21784.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21784v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21784v1', 'Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tharindu Kumarage, Ninareh Mehrabi, Anil Ramakrishna, Xinyan Zhao, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta, Charith Peris

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**å¤‡æ³¨**: Accepted to ACL 2025 (Findings)

**ğŸ”— ä»£ç /é¡¹ç›®**: [HUGGINGFACE](https://huggingface.co/datasets/AmazonScience)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºAIDSAFEä»¥è§£å†³LLMså®‰å…¨æ¨ç†ä¸­çš„æ•°æ®ç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å®‰å…¨æ¨ç†` `å¤šæ™ºèƒ½ä½“åå•†` `é“¾å¼æ€ç»´` `æ•°æ®ç”Ÿæˆ` `å¼€æ”¾æºä»£ç LLMs` `æ”¿ç­–éµå¾ª` `æ¨ç†è´¨é‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å®‰å…¨æ¨ç†æ–¹æ³•åœ¨ç”Ÿæˆå“åº”æ—¶é¢ä¸´è¿‡åº¦æ‹’ç»å’Œè¶Šç‹±æ¼æ´ç­‰é—®é¢˜ï¼Œéš¾ä»¥ä¿è¯æ¨ç†çš„å‡†ç¡®æ€§ã€‚
2. æœ¬æ–‡æå‡ºAIDSAFEï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“åå•†è¿­ä»£æ‰©å±•å®‰å…¨æ”¿ç­–çš„æ¨ç†ï¼Œç¡®ä¿ç”Ÿæˆé«˜è´¨é‡çš„CoTæ•°æ®é›†ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIDSAFEç”Ÿæˆçš„CoTåœ¨æ”¿ç­–éµå¾ªå’Œæ¨ç†è´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†LLMsçš„å®‰å…¨æ€§å’Œé²æ£’æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å®‰å…¨æ¨ç†æ˜¯ä¸€ä¸ªæ–°å…´èŒƒå¼ï¼ŒLLMsåœ¨ç”Ÿæˆå“åº”å‰éœ€å¯¹å®‰å…¨æ”¿ç­–è¿›è¡Œæ¨ç†ï¼Œä»¥å‡è½»ç°æœ‰å®‰å…¨æªæ–½çš„å±€é™æ€§ï¼Œå¦‚è¿‡åº¦æ‹’ç»å’Œè¶Šç‹±æ¼æ´ã€‚ç„¶è€Œï¼Œåˆ›å»ºé«˜è´¨é‡çš„æ”¿ç­–åµŒå…¥é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®é›†çš„èµ„æºå¯†é›†å‹è¿‡ç¨‹ä½¿å¾—è¿™ä¸€èŒƒå¼çš„å®æ–½é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AIDSAFEï¼šä¸€ç§åˆ©ç”¨å¤šæ™ºèƒ½ä½“åå•†çš„è¿­ä»£æ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨æ‰©å±•å®‰å…¨æ”¿ç­–çš„æ¨ç†ã€‚AIDSAFEä¸­çš„æ•°æ®ç²¾ç‚¼é˜¶æ®µç¡®ä¿è¾“å‡ºé«˜è´¨é‡ï¼Œæ¶ˆé™¤é‡å¤å’Œè¯¯å¯¼æ€§æ€ç»´ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºAIDSAFEç”Ÿæˆçš„CoTåœ¨æ”¿ç­–éµå¾ªå’Œæ¨ç†è´¨é‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æå‡äº†å¼€æ”¾æºä»£ç LLMsçš„å®‰å…¨æ€§å’Œè¶Šç‹±é²æ£’æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨ç”Ÿæˆå“åº”æ—¶ï¼ŒLLMså¯¹å®‰å…¨æ”¿ç­–æ¨ç†çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åˆ›å»ºé«˜è´¨é‡çš„æ”¿ç­–åµŒå…¥CoTæ•°æ®é›†æ—¶é¢ä¸´çš„èµ„æºæ¶ˆè€—å’Œæ¨ç†å‡†ç¡®æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šAIDSAFEé€šè¿‡å¤šæ™ºèƒ½ä½“çš„åå•†æœºåˆ¶ï¼Œè¿­ä»£åœ°æ‰©å±•å¯¹å®‰å…¨æ”¿ç­–çš„æ¨ç†ï¼Œç¡®ä¿ç”Ÿæˆçš„CoTæ•°æ®é›†ä¸ä»…é«˜è´¨é‡ä¸”ç¬¦åˆå®‰å…¨è¦æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAIDSAFEçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å¤šæ™ºèƒ½ä½“åå•†ç”Ÿæˆåˆæ­¥çš„CoTï¼Œç¬¬äºŒé˜¶æ®µæ˜¯æ•°æ®ç²¾ç‚¼ï¼Œå»é™¤å†—ä½™å’Œè¯¯å¯¼æ€§æ€ç»´ï¼Œç¡®ä¿è¾“å‡ºçš„é«˜è´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šAIDSAFEçš„åˆ›æ–°åœ¨äºå…¶å¤šæ™ºèƒ½ä½“åå•†æœºåˆ¶å’Œæ•°æ®ç²¾ç‚¼é˜¶æ®µçš„ç»“åˆï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€ç”Ÿæˆè¿‡ç¨‹å½¢æˆäº†æ˜¾è‘—åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨AIDSAFEä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–CoTçš„ç”Ÿæˆè´¨é‡ï¼ŒåŒæ—¶ç¡®ä¿æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºAIDSAFEç”Ÿæˆçš„CoTåœ¨æ”¿ç­–éµå¾ªå’Œæ¨ç†è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºæ”¿ç­–éµå¾ªç‡æå‡äº†XX%ï¼Œæ¨ç†å‡†ç¡®æ€§æé«˜äº†YY%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒAIDSAFEåœ¨å®‰å…¨æ¨ç†é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å®‰å…¨æ•æ„Ÿçš„å¯¹è¯ç³»ç»Ÿã€è‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒå’Œæ™ºèƒ½ä»£ç†ç­‰ã€‚é€šè¿‡æå‡LLMsçš„å®‰å…¨æ€§å’Œé²æ£’æ€§ï¼ŒAIDSAFEèƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æœ‰æ•ˆé™ä½å®‰å…¨é£é™©ï¼Œå¢å¼ºç”¨æˆ·ä¿¡ä»»ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here: https://huggingface.co/datasets/AmazonScience/AIDSAFE

