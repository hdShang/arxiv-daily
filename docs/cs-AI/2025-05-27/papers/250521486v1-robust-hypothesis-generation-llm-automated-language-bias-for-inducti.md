---
layout: default
title: "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming"
---

# Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21486" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21486v1</a>
  <a href="https://arxiv.org/pdf/2505.21486.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21486v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21486v1', 'Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yang Yang, Jiemin Wu, Yutao Yue

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºLLMçš„è‡ªåŠ¨åŒ–è¯­è¨€åå·®ç”Ÿæˆæ¡†æ¶ä»¥æå‡å‡è®¾ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å‡è®¾ç”Ÿæˆ` `å½’çº³é€»è¾‘ç¼–ç¨‹` `å¤§å‹è¯­è¨€æ¨¡å‹` `è‡ªåŠ¨åŒ–æ¨ç†` `ç¬¦å·å­¦ä¹ ` `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `å¯è§£é‡Šäººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å‡è®¾ç”Ÿæˆæ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰çš„ç¬¦å·ç»“æ„ï¼Œé™åˆ¶äº†å…¶åœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚
2. æœ¬æ–‡æå‡ºçš„æ¡†æ¶é€šè¿‡LLMè‡ªåŠ¨ç”Ÿæˆç¬¦å·è¯æ±‡å’Œå…³ç³»æ¨¡æ¿ï¼Œå‡å°‘äº†å¯¹ä¸“å®¶çŸ¥è¯†çš„ä¾èµ–ï¼Œæå‡äº†å‡è®¾ç”Ÿæˆçš„æ•ˆç‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¤æ‚åœºæ™¯ä¸‹è¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºä¼ ç»ŸILPæ–¹æ³•æ˜¾è‘—æé«˜äº†å‡è®¾ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨å¼€æ”¾ç¯å¢ƒä¸­è‡ªåŠ¨åŒ–ç¨³å¥çš„å‡è®¾ç”Ÿæˆå¯¹äººå·¥æ™ºèƒ½è®¤çŸ¥è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆï¼Œåº”ç”¨äºå½’çº³é€»è¾‘ç¼–ç¨‹ï¼ˆILPï¼‰ã€‚è¯¥ç³»ç»Ÿçš„LLMæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»ä»åŸå§‹æ–‡æœ¬æ•°æ®ä¸­å®šä¹‰ç»“æ„åŒ–çš„ç¬¦å·è¯æ±‡ï¼ˆè°“è¯ï¼‰å’Œå…³ç³»æ¨¡æ¿ï¼Œå³ç›´æ¥ç”Ÿæˆçš„è¯­è¨€åå·®ã€‚è¿™ç§è‡ªåŠ¨åŒ–çš„ç¬¦å·åŸºç¡€æ„å»ºï¼Œä¼ ç»Ÿä¸Šæ˜¯ILPçš„ä¸“å®¶é©±åŠ¨ç“¶é¢ˆï¼ŒéšåæŒ‡å¯¼æ–‡æœ¬è½¬åŒ–ä¸ºILPæ±‚è§£å™¨çš„äº‹å®ï¼Œä»è€Œå½’çº³å‡ºå¯è§£é‡Šçš„è§„åˆ™ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»ŸILPå¯¹é¢„å®šä¹‰ç¬¦å·ç»“æ„çš„ä¾èµ–ä»¥åŠçº¯LLMæ–¹æ³•çš„å™ªå£°æ•æ„Ÿæ€§ã€‚å¤§é‡åœ¨å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­çš„å®éªŒéªŒè¯äº†å…¶å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨åŒ–ã€å¯è§£é‡Šå’Œå¯éªŒè¯çš„å‡è®¾ç”Ÿæˆå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼€æ”¾ç¯å¢ƒä¸­å‡è®¾ç”Ÿæˆçš„è‡ªåŠ¨åŒ–é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç¬¦å·ç»“æ„çš„ä¾èµ–å’Œå™ªå£°æ•æ„Ÿæ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆç¬¦å·è¯æ±‡å’Œå…³ç³»æ¨¡æ¿ï¼Œå‡å°‘å¯¹äººå·¥ä¸“å®¶çš„ä¾èµ–ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„å‡è®¾ç”Ÿæˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬LLMæ™ºèƒ½ä½“ã€ç¬¦å·è¯æ±‡ç”Ÿæˆæ¨¡å—ã€å…³ç³»æ¨¡æ¿æ„å»ºæ¨¡å—å’ŒILPæ±‚è§£å™¨ã€‚LLMæ™ºèƒ½ä½“ä»åŸå§‹æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ç¬¦å·è¡¨ç¤ºï¼Œéšåå°†å…¶è¾“å…¥åˆ°ILPæ±‚è§£å™¨è¿›è¡Œè§„åˆ™å­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†LLMä¸ILPç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–ç¬¦å·åŸºç¡€æ„å»ºæ–¹æ³•ï¼Œçªç ´äº†ä¼ ç»ŸILPå¯¹é¢„å®šä¹‰ç»“æ„çš„ä¾èµ–ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLLMæ™ºèƒ½ä½“çš„è®­ç»ƒé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç¬¦å·ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºTransformeræ¶æ„ï¼Œç¡®ä¿äº†å¯¹æ–‡æœ¬æ•°æ®çš„æœ‰æ•ˆå¤„ç†ã€‚å®éªŒä¸­è¿˜å¯¹ç”Ÿæˆçš„ç¬¦å·è¯æ±‡å’Œå…³ç³»æ¨¡æ¿è¿›è¡Œäº†ä¸¥æ ¼çš„éªŒè¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªå¤æ‚åœºæ™¯ä¸‹çš„å‡è®¾ç”Ÿæˆå‡†ç¡®ç‡æé«˜äº†20%ä»¥ä¸Šï¼Œç›¸è¾ƒäºä¼ ç»ŸILPæ–¹æ³•ï¼Œç”Ÿæˆçš„è§„åˆ™å…·æœ‰æ›´é«˜çš„å¯è§£é‡Šæ€§å’Œé€‚åº”æ€§ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œè‡ªåŠ¨åŒ–æ¨ç†ç­‰ã€‚é€šè¿‡æå‡å‡è®¾ç”Ÿæˆçš„è‡ªåŠ¨åŒ–å’Œå¯è§£é‡Šæ€§ï¼Œè¯¥æ–¹æ³•æœ‰åŠ©äºæ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨å¤æ‚å†³ç­–å’Œæ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæœªæ¥å¯èƒ½åœ¨ç§‘å­¦ç ”ç©¶ã€æ³•å¾‹åˆ†æç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Automating robust hypothesis generation in open environments is pivotal for AI cognition. We introduce a novel framework integrating a multi-agent system, powered by Large Language Models (LLMs), with Inductive Logic Programming (ILP). Our system's LLM agents autonomously define a structured symbolic vocabulary (predicates) and relational templates , i.e., \emph{language bias} directly from raw textual data. This automated symbolic grounding (the construction of the language bias), traditionally an expert-driven bottleneck for ILP, then guides the transformation of text into facts for an ILP solver, which inductively learns interpretable rules. This approach overcomes traditional ILP's reliance on predefined symbolic structures and the noise-sensitivity of pure LLM methods. Extensive experiments in diverse, challenging scenarios validate superior performance, paving a new path for automated, explainable, and verifiable hypothesis generation.

