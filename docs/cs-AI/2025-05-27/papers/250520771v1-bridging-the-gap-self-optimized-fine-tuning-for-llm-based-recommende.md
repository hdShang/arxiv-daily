---
layout: default
title: "Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems"
---

# Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20771" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20771v1</a>
  <a href="https://arxiv.org/pdf/2505.20771.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20771v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20771v1', 'Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heng Tang, Feng Liu, Xinbo Chen, Jiawei Chen, Bohao Wang, Changwang Zhang, Jun Wang, Yuegang Sun, Bingde Hu, Can Wang

**åˆ†ç±»**: cs.IR, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªä¼˜åŒ–å¾®è°ƒæ–¹æ³•ä»¥æå‡LLMæ¨èç³»ç»Ÿæ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨èç³»ç»Ÿ` `è‡ªä¼˜åŒ–å¾®è°ƒ` `è‡ªè’¸é¦` `è¯¾ç¨‹å­¦ä¹ ` `æ¨èå‡†ç¡®ç‡` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨èç³»ç»Ÿæ–¹æ³•åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶å­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆçŸ¥è¯†ä¸æ¨èä»»åŠ¡ã€‚
2. æœ¬æ–‡æå‡ºçš„è‡ªä¼˜åŒ–å¾®è°ƒï¼ˆSOFTï¼‰æ–¹æ³•ç»“åˆäº†è‡ªè’¸é¦å’Œè‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼Œæ—¨åœ¨æå‡LLMsçš„æ¨èèƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSOFTæ–¹æ³•åœ¨æ¨èå‡†ç¡®ç‡ä¸Šå¹³å‡æå‡äº†37.59%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨èç³»ç»Ÿé¢†åŸŸçš„åº”ç”¨å¾—åˆ°äº†å¹¿æ³›æ¢ç´¢ã€‚ç›®å‰ä¸»è¦æœ‰ä¸¤ç§ç­–ç•¥ä½¿LLMså…·å¤‡æ¨èèƒ½åŠ›ï¼šä¸€æ˜¯â€œä»…æŒ‡å¯¼â€ç­–ç•¥ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å¢å¼ºLLMsçš„è¯­ä¹‰ç†è§£å’Œæ¨èèƒ½åŠ›ï¼›äºŒæ˜¯â€œä»…è°ƒä¼˜â€ç­–ç•¥ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½¿LLMsé€‚åº”çœŸå®æ¨èæ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§ç­–ç•¥å‡æœªèƒ½æœ‰æ•ˆå¼¥åˆLLMsçŸ¥è¯†ç©ºé—´ä¸æ¨èä¹‹é—´çš„å·®è·ï¼Œå¯¼è‡´æ€§èƒ½æœªè¾¾é¢„æœŸã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„â€œæŒ‡å¯¼+è°ƒä¼˜â€æ–¹æ³•â€”â€”è‡ªä¼˜åŒ–å¾®è°ƒï¼ˆSOFTï¼‰ï¼Œç»“åˆäº†è¯¾ç¨‹å­¦ä¹ çš„æ€æƒ³ï¼Œé€šè¿‡è‡ªè’¸é¦æ„å»ºæ˜“å­¦çš„è¾…åŠ©æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨è‡ªé€‚åº”è¯¾ç¨‹è°ƒåº¦å™¨é€æ­¥å¼•å¯¼LLMsä»ç®€å•æ•°æ®å­¦ä¹ åˆ°æ›´å…·æŒ‘æˆ˜æ€§çš„çœŸå®æ¨èæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSOFTæ˜¾è‘—æé«˜äº†LLMæ–¹æ³•çš„æ¨èå‡†ç¡®ç‡ï¼Œå¹³å‡æå‡è¾¾37.59%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMæ¨èç³»ç»Ÿä¸­çŸ¥è¯†ä¸æ¨èä»»åŠ¡ä¹‹é—´çš„å·®è·ï¼Œç°æœ‰çš„â€œä»…æŒ‡å¯¼â€å’Œâ€œä»…è°ƒä¼˜â€ç­–ç•¥æœªèƒ½æœ‰æ•ˆæå‡æ¨èæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºè‡ªä¼˜åŒ–å¾®è°ƒï¼ˆSOFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‡ªè’¸é¦ç”Ÿæˆæ˜“å­¦çš„è¾…åŠ©æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨è‡ªé€‚åº”è¯¾ç¨‹è°ƒåº¦å™¨é€æ­¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œä»è€Œæå‡æ¨èæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSOFTæ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œè‡ªè’¸é¦ï¼Œæ„å»ºè¾…åŠ©æ•°æ®é›†ï¼›ç„¶åé€šè¿‡è‡ªé€‚åº”è¯¾ç¨‹è°ƒåº¦å™¨ï¼Œé€æ­¥å¼•å¯¼æ¨¡å‹ä»ç®€å•åˆ°å¤æ‚çš„æ•°æ®è¿›è¡Œå­¦ä¹ ã€‚

**å…³é”®åˆ›æ–°**ï¼šSOFTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆäº†è‡ªè’¸é¦å’Œè¯¾ç¨‹å­¦ä¹ çš„æ€æƒ³ï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„å•ä¸€æŒ‡å¯¼æˆ–è°ƒä¼˜æ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è‡ªè’¸é¦é˜¶æ®µï¼Œè®¾è®¡äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ç¡®ä¿ç”Ÿæˆæ•°æ®çš„æœ‰æ•ˆæ€§ï¼›è¯¾ç¨‹è°ƒåº¦å™¨çš„å‚æ•°è®¾ç½®ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒéš¾åº¦çš„æ•°æ®ä¸Šè¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSOFTæ–¹æ³•åœ¨æ¨èå‡†ç¡®ç‡ä¸Šå¹³å‡æå‡äº†37.59%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬ç”µå­å•†åŠ¡ã€ç¤¾äº¤åª’ä½“å’Œå†…å®¹æ¨èç­‰åœºæ™¯ï¼Œèƒ½å¤Ÿå¸®åŠ©ä¼ä¸šæå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚æœªæ¥ï¼ŒSOFTæ–¹æ³•å¯èƒ½æ¨åŠ¨LLMåœ¨æ¨èç³»ç»Ÿä¸­çš„æ›´å¹¿æ³›åº”ç”¨ï¼Œä¿ƒè¿›ä¸ªæ€§åŒ–æ¨èæŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent years have witnessed extensive exploration of Large Language Models (LLMs) on the field of Recommender Systems (RS). There are currently two commonly used strategies to enable LLMs to have recommendation capabilities: 1) The "Guidance-Only" strategy uses in-context learning to exploit and amplify the inherent semantic understanding and item recommendation capabilities of LLMs; 2) The "Tuning-Only" strategy uses supervised fine-tuning (SFT) to fine-tune LLMs with the aim of fitting them to real recommendation data. However, neither of these strategies can effectively bridge the gap between the knowledge space of LLMs and recommendation, and their performance do not meet our expectations.
>   To better enable LLMs to learn recommendation knowledge, we combine the advantages of the above two strategies and proposed a novel "Guidance+Tuning" method called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of curriculum learning. It first employs self-distillation to construct an auxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it further utilizes a self-adaptive curriculum scheduler to enable LLMs to gradually learn from simpler data (self-distilled data) to more challenging data (real RS data). Extensive experiments demonstrate that SOFT significantly enhances the recommendation accuracy (37.59\% on average) of LLM-based methods. The code is available via https://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E

