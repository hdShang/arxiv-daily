---
layout: default
title: LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation
---

# LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20671" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20671v1</a>
  <a href="https://arxiv.org/pdf/2505.20671.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20671v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20671v1', 'LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Heng Tan, Hua Yan, Yu Yang

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLLMå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³è®­ç»ƒç“¶é¢ˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ç­–ç•¥è°ƒèŠ‚` `è®­ç»ƒç“¶é¢ˆ` `è‡ªåŠ¨åŒ–ä¼˜åŒ–` `äººæœºåä½œ` `å…³é”®çŠ¶æ€è¯†åˆ«`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒå¤æ‚ä»»åŠ¡æ—¶å¸¸å¸¸æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ï¼Œéš¾ä»¥å®ç°é•¿æœŸå¥–åŠ±çš„æœ€å¤§åŒ–ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç­–ç•¥è°ƒèŠ‚æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«å…³é”®çŠ¶æ€å¹¶æä¾›è¡ŒåŠ¨å»ºè®®æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„é«˜æˆæœ¬å’Œä¸ç¡®å®šæ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†RLåŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†LLMåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸­è®­ç»ƒæœ‰æ•ˆç­–ç•¥ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´ä»£ç†æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ï¼Œæ— æ³•æœ€å¤§åŒ–é•¿æœŸå¥–åŠ±ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç­–ç•¥è°ƒèŠ‚æ¡†æ¶ï¼Œåˆ©ç”¨LLMæ¥æ”¹å–„RLè®­ç»ƒï¼Œé¿å…äº†é¢å¤–çš„æ¨¡å‹è®­ç»ƒæˆ–äººå·¥å¹²é¢„ã€‚é€šè¿‡æç¤ºLLMè¯†åˆ«æ¬¡ä¼˜ä»£ç†è½¨è¿¹ä¸­çš„å…³é”®çŠ¶æ€ï¼ŒLLMæä¾›è¡ŒåŠ¨å»ºè®®å¹¶åˆ†é…éšæ€§å¥–åŠ±ä»¥æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†RLåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œçªæ˜¾äº†åŸºäºLLMçš„è§£é‡Šåœ¨è§£å†³RLè®­ç»ƒç“¶é¢ˆä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„ç“¶é¢ˆé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¦‚è‡ªåŠ¨åŒ–ç­–ç•¥ä¼˜åŒ–å’Œäººæœºåä½œåé¦ˆåœ¨å¤æ‚ç¯å¢ƒä¸­æ•ˆæœä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æˆ–è¿ç»­åŠ¨ä½œç©ºé—´ä¸­ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å¼•å¯¼ç­–ç•¥è°ƒèŠ‚ï¼Œé€šè¿‡è¯†åˆ«å…³é”®çŠ¶æ€å¹¶æä¾›è¡ŒåŠ¨å»ºè®®ï¼Œå‡å°‘å¯¹æ¨¡å‹è®­ç»ƒå’Œäººå·¥å¹²é¢„çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆï¼Œä½¿ç”¨LLMåˆ†ææ¬¡ä¼˜ä»£ç†çš„è½¨è¿¹ä»¥è¯†åˆ«å…³é”®çŠ¶æ€ï¼›å…¶æ¬¡ï¼ŒLLMåŸºäºè¿™äº›çŠ¶æ€æä¾›è¡ŒåŠ¨å»ºè®®ï¼›æœ€åï¼ŒLLMä¸ºç­–ç•¥ä¼˜åŒ–åˆ†é…éšæ€§å¥–åŠ±ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†LLMåº”ç”¨äºå¼ºåŒ–å­¦ä¹ ç­–ç•¥è°ƒèŠ‚ä¸­ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„é«˜æˆæœ¬å’Œä¸ç¡®å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒLLMçš„æç¤ºç­–ç•¥å’Œå¥–åŠ±åˆ†é…æœºåˆ¶æ˜¯å…³é”®ï¼Œç¡®ä¿èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å…³é”®çŠ¶æ€å¹¶æä¾›æœ‰æ„ä¹‰çš„åé¦ˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†RLåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼ŒéªŒè¯äº†LLMåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆæ™ºèƒ½ä½“ã€è‡ªåŠ¨é©¾é©¶ç­‰å¤æ‚å†³ç­–ä»»åŠ¡ã€‚é€šè¿‡æé«˜å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ•ˆç‡ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŠ é€Ÿæ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæå‡å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> While reinforcement learning (RL) has achieved notable success in various domains, training effective policies for complex tasks remains challenging. Agents often converge to local optima and fail to maximize long-term rewards. Existing approaches to mitigate training bottlenecks typically fall into two categories: (i) Automated policy refinement, which identifies critical states from past trajectories to guide policy updates, but suffers from costly and uncertain model training; and (ii) Human-in-the-loop refinement, where human feedback is used to correct agent behavior, but this does not scale well to environments with large or continuous action spaces. In this work, we design a large language model-guided policy modulation framework that leverages LLMs to improve RL training without additional model training or human intervention. We first prompt an LLM to identify critical states from a sub-optimal agent's trajectories. Based on these states, the LLM then provides action suggestions and assigns implicit rewards to guide policy refinement. Experiments across standard RL benchmarks demonstrate that our method outperforms state-of-the-art baselines, highlighting the effectiveness of LLM-based explanations in addressing RL training bottlenecks.

