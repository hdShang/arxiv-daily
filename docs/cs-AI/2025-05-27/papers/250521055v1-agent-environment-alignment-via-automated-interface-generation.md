---
layout: default
title: Agent-Environment Alignment via Automated Interface Generation
---

# Agent-Environment Alignment via Automated Interface Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21055" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21055v1</a>
  <a href="https://arxiv.org/pdf/2505.21055.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21055v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21055v1', 'Agent-Environment Alignment via Automated Interface Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kaiming Liu, Xuanyu Lei, Ziyue Wang, Peng Li, Yang Liu

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/THUNLP-MT/ALIGN)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºALIGNæ¡†æ¶ä»¥è§£å†³æ™ºèƒ½ä½“ä¸ç¯å¢ƒä¸åŒ¹é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ™ºèƒ½ä½“` `ç¯å¢ƒå¯¹é½` `æ¥å£ç”Ÿæˆ` `å†³ç­–ç³»ç»Ÿ` `æ€§èƒ½æå‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’ä¸­å­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´æ™ºèƒ½ä½“æ€§èƒ½ç“¶é¢ˆã€‚
2. æœ¬æ–‡æå‡ºALIGNæ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¯¹é½æ¥å£æ¥æ”¹å–„æ™ºèƒ½ä½“ä¸ç¯å¢ƒä¹‹é—´çš„äº¤äº’ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒALIGNåœ¨å¤šä¸ªä»»åŠ¡ä¸­å‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨ALFWorldä¸­æˆåŠŸç‡æé«˜äº†45.67%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“åœ¨äº¤äº’å†³ç­–ä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ™ºèƒ½ä½“çš„å†…éƒ¨æœŸæœ›ä¸ç¯å¢ƒå®é™…çŠ¶æ€ä¹‹é—´å¸¸å¸¸å­˜åœ¨ä¸åŒ¹é…ç°è±¡ï¼Œç§°ä¸ºæ™ºèƒ½ä½“-ç¯å¢ƒä¸ä¸€è‡´ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶è‡´åŠ›äºæ”¹è¿›æ™ºèƒ½ä½“ç­–ç•¥å’Œç¯å¢ƒè®¾è®¡ï¼Œä½†æ¥å£çš„å…³é”®ä½œç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚æœ¬æ–‡æå‡ºäº†ALIGNæ¡†æ¶ï¼Œé€šè¿‡ä¸°å¯Œæ¥å£æ¥ç¼“è§£ä¸ä¸€è‡´é—®é¢˜ï¼Œå¢å¼ºç¯å¢ƒçš„é™æ€ä¿¡æ¯å’Œé€æ­¥è§‚å¯Ÿã€‚è¯¥æ¥å£ä½œä¸ºè½»é‡çº§åŒ…è£…å™¨å®ç°ï¼Œæ— éœ€ä¿®æ”¹æ™ºèƒ½ä½“é€»è¾‘æˆ–ç¯å¢ƒä»£ç ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªé¢†åŸŸä¸­ï¼ŒALIGNæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„è¡¨ç°ï¼ŒALFWorldä¸­çš„æˆåŠŸç‡æé«˜äº†45.67%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æ™ºèƒ½ä½“ä¸ç¯å¢ƒä¹‹é—´çš„æœŸæœ›ä¸å®é™…çŠ¶æ€ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ™ºèƒ½ä½“ç­–ç•¥å’Œç¯å¢ƒè®¾è®¡ä¸ŠæŠ•å…¥è¾ƒå¤šï¼Œä½†æ¥å£çš„ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†é‡è§†ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šALIGNæ¡†æ¶é€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¯¹é½æ¥å£ï¼Œå¢å¼ºç¯å¢ƒä¿¡æ¯å’Œæ™ºèƒ½ä½“è§‚å¯Ÿï¼Œè¿›è€Œç¼“è§£æ™ºèƒ½ä½“-ç¯å¢ƒä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£ç¯å¢ƒåé¦ˆã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šALIGNæ¡†æ¶ä¸»è¦åŒ…æ‹¬æ¥å£ç”Ÿæˆæ¨¡å—å’Œä¿¡æ¯å¢å¼ºæ¨¡å—ã€‚æ¥å£ç”Ÿæˆæ¨¡å—è´Ÿè´£åˆ›å»ºä¸ç¯å¢ƒäº¤äº’çš„å¯¹é½æ¥å£ï¼Œè€Œä¿¡æ¯å¢å¼ºæ¨¡å—åˆ™æå‡ç¯å¢ƒçš„é™æ€ä¿¡æ¯å’ŒåŠ¨æ€è§‚å¯Ÿåé¦ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šALIGNçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è½»é‡çº§çš„æ¥å£ç”Ÿæˆæ–¹å¼ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹æ™ºèƒ½ä½“é€»è¾‘å’Œç¯å¢ƒä»£ç çš„æƒ…å†µä¸‹å®ç°å¯¹é½ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›´æ¥ä¿®æ”¹æ™ºèƒ½ä½“æˆ–ç¯å¢ƒçš„æ–¹å¼æœ¬è´¨ä¸Šä¸åŒã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒALIGNä½¿ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®ä»¥ç¡®ä¿æ¥å£çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶é‡‡ç”¨äº†é€‚åº”æ€§æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¥å£ç”Ÿæˆè¿‡ç¨‹ã€‚ç½‘ç»œç»“æ„ä¸Šï¼ŒALIGNèƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ™ºèƒ½ä½“æ¶æ„å’ŒLLMåŸºç¡€æ¨¡å‹ï¼Œé¿å…äº†æ¥å£çš„é‡å¤ç”Ÿæˆã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒALIGNåœ¨å¤šä¸ªä»»åŠ¡ä¸­å‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨ALFWorldä¸­æˆåŠŸç‡æé«˜äº†45.67%ã€‚æ­¤å¤–ï¼ŒALIGNç”Ÿæˆçš„æ¥å£èƒ½å¤Ÿåœ¨ä¸åŒæ™ºèƒ½ä½“æ¶æ„å’ŒLLMåŸºç¡€æ¨¡å‹ä¸­æœ‰æ•ˆæ³›åŒ–ï¼Œæ— éœ€é‡æ–°ç”Ÿæˆæ¥å£ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨åŒ–å†³ç­–ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æ”¹å–„æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’ï¼ŒALIGNèƒ½å¤Ÿæå‡æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language model (LLM) agents have shown impressive reasoning capabilities in interactive decision-making tasks. These agents interact with environment through intermediate interfaces, such as predefined action spaces and interaction rules, which mediate the perception and action. However, mismatches often happen between the internal expectations of the agent regarding the influence of its issued actions and the actual state transitions in the environment, a phenomenon referred to as \textbf{agent-environment misalignment}. While prior work has invested substantially in improving agent strategies and environment design, the critical role of the interface still remains underexplored. In this work, we empirically demonstrate that agent-environment misalignment poses a significant bottleneck to agent performance. To mitigate this issue, we propose \textbf{ALIGN}, an \underline{A}uto-A\underline{l}igned \underline{I}nterface \underline{G}e\underline{n}eration framework that alleviates the misalignment by enriching the interface. Specifically, the ALIGN-generated interface enhances both the static information of the environment and the step-wise observations returned to the agent. Implemented as a lightweight wrapper, this interface achieves the alignment without modifying either the agent logic or the environment code. Experiments across multiple domains including embodied tasks, web navigation and tool-use, show consistent performance improvements, with up to a 45.67\% success rate improvement observed in ALFWorld. Meanwhile, ALIGN-generated interface can generalize across different agent architectures and LLM backbones without interface regeneration. Code and experimental results are available at https://github.com/THUNLP-MT/ALIGN.

