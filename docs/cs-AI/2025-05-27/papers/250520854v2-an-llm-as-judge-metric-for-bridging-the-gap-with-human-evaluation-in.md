---
layout: default
title: An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks
---

# An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20854" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20854v2</a>
  <a href="https://arxiv.org/pdf/2505.20854.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20854v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20854v2', 'An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xin Zhou, Kisub Kim, Ting Zhang, Martin Weyssow, Luis F. Gomes, Guang Yang, Kui Liu, Xin Xia, David Lo

**åˆ†ç±»**: cs.SE, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-10-10)

**å¤‡æ³¨**: 13 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSE-Juryä»¥è§£å†³è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­ç”Ÿæˆè½¯ä»¶å·¥ä»¶è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è½¯ä»¶å·¥ç¨‹` `è‡ªåŠ¨è¯„ä¼°` `é›†æˆè¯„ä¼°` `ä»£ç ç”Ÿæˆ` `ç¨‹åºä¿®å¤` `ä»£ç æ‘˜è¦`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ— æ³•å‡†ç¡®åæ˜ ç”Ÿæˆè½¯ä»¶å·¥ä»¶çš„æ­£ç¡®æ€§ï¼Œä¸”äººç±»è¯„ä¼°åŠ³åŠ¨å¯†é›†ä¸”éš¾ä»¥æ‰©å±•ã€‚
2. è®ºæ–‡æå‡ºSE-Juryï¼Œé€šè¿‡å®šä¹‰äº”ç§ç‹¬ç«‹è¯„ä¼°ç­–ç•¥å’ŒåŠ¨æ€å›¢é˜Ÿé€‰æ‹©æœºåˆ¶ï¼Œå‡†ç¡®è¯„ä¼°ç”Ÿæˆè½¯ä»¶å·¥ä»¶çš„æ­£ç¡®æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSE-Juryåœ¨ä¸äººç±»è¯„ä¼°ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨ä»£ç ç”Ÿæˆå’Œç¨‹åºä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå…¶ä»–è‡ªåŠ¨åŒ–æŠ€æœ¯åœ¨è½¯ä»¶å¼€å‘ä¸­çš„åº”ç”¨æ—¥ç›Šå¢åŠ ï¼Œç”Ÿæˆè½¯ä»¶å·¥ä»¶ï¼ˆå¦‚ä»£ç ç‰‡æ®µã€è¡¥ä¸å’Œæ³¨é‡Šï¼‰çš„æ­£ç¡®æ€§è¯„ä¼°ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚äººç±»è¯„ä¼°è™½ç„¶å‡†ç¡®ï¼Œä½†åŠ³åŠ¨å¯†é›†ä¸”ç¼ºä¹å¯æ‰©å±•æ€§ï¼›è€Œè®¸å¤šè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡è™½ç„¶å¯æ‰©å±•ä¸”äººåŠ›éœ€æ±‚å°‘ï¼Œä½†å¾€å¾€æ— æ³•å‡†ç¡®åæ˜ ç”Ÿæˆå·¥ä»¶çš„å®é™…æ­£ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†SE-Juryï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨è®¾è®¡ç”¨äºå‡†ç¡®è¯„ä¼°ç”Ÿæˆè½¯ä»¶å·¥ä»¶æ­£ç¡®æ€§çš„LLMä½œä¸ºé›†æˆè¯„ä¼°è€…çš„è¯„ä¼°æŒ‡æ ‡ã€‚SE-Juryå®šä¹‰äº†äº”ç§ç‹¬ç«‹è¯„ä¼°ç­–ç•¥ï¼Œå¹¶é€šè¿‡åŠ¨æ€å›¢é˜Ÿé€‰æ‹©æœºåˆ¶ç¡®å®šæœ€åˆé€‚çš„è¯„ä¼°è€…å­é›†ï¼Œæœ€ç»ˆé€šè¿‡é›†æˆäº§ç”Ÿæ­£ç¡®æ€§è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSE-Juryä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§æ˜¾è‘—æé«˜ï¼Œæå‡å¹…åº¦åœ¨29.6%åˆ°140.8%ä¹‹é—´ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºäººç±»è¯„ä¼°å¯æ‰©å±•ä¸”å¯é çš„æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç”Ÿæˆè½¯ä»¶å·¥ä»¶ï¼ˆå¦‚ä»£ç ç‰‡æ®µã€è¡¥ä¸å’Œæ³¨é‡Šï¼‰è¯„ä¼°ä¸­çš„å‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨è‡ªåŠ¨è¯„ä¼°æ—¶å¸¸å¸¸æ— æ³•åæ˜ çœŸå®çš„æ­£ç¡®æ€§ï¼Œè€Œäººç±»è¯„ä¼°åˆ™é¢ä¸´å¯æ‰©å±•æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSE-Juryçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡é›†æˆå¤šä¸ªç‹¬ç«‹è¯„ä¼°è€…çš„åˆ¤æ–­ï¼Œåˆ©ç”¨åŠ¨æ€é€‰æ‹©æœºåˆ¶æ¥ç¡®å®šæœ€åˆé€‚çš„è¯„ä¼°è€…ç»„åˆï¼Œä»è€Œæé«˜è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSE-Juryçš„æ•´ä½“æ¶æ„åŒ…æ‹¬äº”ç§ç‹¬ç«‹çš„è¯„ä¼°ç­–ç•¥ï¼Œæ¯ç§ç­–ç•¥ç”±ä¸€ä¸ªç‹¬ç«‹çš„è¯„ä¼°è€…å®ç°ã€‚é€šè¿‡åŠ¨æ€å›¢é˜Ÿé€‰æ‹©æœºåˆ¶ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„è¯„ä¼°è€…ç»„åˆï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªç»¼åˆçš„æ­£ç¡®æ€§è¯„åˆ†ã€‚

**å…³é”®åˆ›æ–°**ï¼šSE-Juryçš„ä¸»è¦åˆ›æ–°åœ¨äºå°†å¤šä¸ªè¯„ä¼°è€…çš„åˆ¤æ–­è¿›è¡Œé›†æˆï¼Œå…‹æœäº†å•ä¸€è¯„ä¼°è€…å¯èƒ½å¸¦æ¥çš„åå·®ã€‚è¿™ç§é›†æˆæ–¹æ³•ä¸ç°æœ‰çš„å•ä¸€è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒSE-Juryé‡‡ç”¨äº†åŠ¨æ€é€‰æ‹©æœºåˆ¶ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­é€‰æ‹©æœ€ä¼˜çš„è¯„ä¼°è€…ç»„åˆã€‚æ­¤å¤–ï¼Œè¯„ä¼°ç­–ç•¥çš„å¤šæ ·æ€§å’Œç‹¬ç«‹æ€§ä¹Ÿæ˜¯å…¶è®¾è®¡çš„å…³é”®è¦ç´ ï¼Œç¡®ä¿äº†è¯„ä¼°ç»“æœçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSE-Juryåœ¨ä¸äººç±»è¯„ä¼°çš„ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—æé«˜ï¼Œæå‡å¹…åº¦åœ¨29.6%åˆ°140.8%ä¹‹é—´ã€‚ç‰¹åˆ«æ˜¯åœ¨ä»£ç ç”Ÿæˆå’Œç¨‹åºä¿®å¤ä»»åŠ¡ä¸­ï¼ŒSE-Juryçš„è¯„ä¼°ç»“æœä¸äººç±»æ ‡æ³¨è€…çš„åè®®æ°´å¹³æ¥è¿‘ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºäººç±»è¯„ä¼°çš„å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SE-Juryçš„ç ”ç©¶æˆæœåœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä»£ç ç”Ÿæˆã€è‡ªåŠ¨ç¨‹åºä¿®å¤å’Œä»£ç æ‘˜è¦ç­‰ä»»åŠ¡ä¸­ã€‚å…¶å¯æ‰©å±•æ€§å’Œé«˜å‡†ç¡®æ€§ä½¿å…¶æˆä¸ºå¼€å‘è€…åœ¨è¯„ä¼°è‡ªåŠ¨ç”Ÿæˆå·¥ä»¶æ—¶çš„å¯é å·¥å…·ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨è½¯ä»¶å¼€å‘æµç¨‹çš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, many automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts.
>   In this paper, we present SE-Jury, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SE-Jury first defines five distinct evaluation strategies, each implemented by an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges as a team to produce a final correctness score through ensembling. We evaluate SE-Jury across a diverse set of software engineering (SE) benchmarks that span three popular SE tasks: code generation, automated program repair, and code summarization. Results demonstrate that SE-Jury consistently achieves a higher correlation with human judgments, with improvements ranging from 29.6% to 140.8% over existing automatic metrics. SE-Jury reaches agreement levels with human annotators that are close to inter-annotator agreement in code generation and program repair. These findings underscore SE-Jury's potential as a scalable and reliable alternative to human evaluation in these SE tasks.

