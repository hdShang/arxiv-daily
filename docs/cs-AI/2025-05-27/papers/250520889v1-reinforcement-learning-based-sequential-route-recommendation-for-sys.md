---
layout: default
title: Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment
---

# Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20889" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20889v1</a>
  <a href="https://arxiv.org/pdf/2505.20889.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20889v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20889v1', 'Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Leizhen Wang, Peibo Duan, Cheng Lyu, Zhenliang Ma

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„é¡ºåºè·¯çº¿æ¨èä»¥è§£å†³ç³»ç»Ÿæœ€ä¼˜äº¤é€šåˆ†é…é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `äº¤é€šåˆ†é…` `ä¸ªæ€§åŒ–æ¨è` `æ·±åº¦å­¦ä¹ ` `æ™ºèƒ½äº¤é€š` `ç³»ç»Ÿä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä¸ªæ€§åŒ–è·¯çº¿æ¨èç³»ç»Ÿæœªèƒ½æœ‰æ•ˆå®ç°ç³»ç»Ÿæœ€ä¼˜äº¤é€šåˆ†é…ï¼Œå¯¼è‡´æ•´ä½“äº¤é€šæ•ˆç‡ä½ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºå°†é™æ€ç³»ç»Ÿæœ€ä¼˜äº¤é€šåˆ†é…é—®é¢˜è½¬åŒ–ä¸ºå•ä»£ç†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œé€šè¿‡é¡ºåºæ¨èè·¯çº¿æ¥ä¼˜åŒ–æ•´ä½“æ—…è¡Œæ—¶é—´ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨Braessç½‘ç»œä¸­æ”¶æ•›è‡³ç†è®ºæœ€ä¼˜è§£ï¼Œåœ¨Ortuzar-Willumsenç½‘ç»œä¸­åå·®ä»…ä¸º0.35%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¯¼èˆªç³»ç»Ÿå’Œå…±äº«å‡ºè¡Œå¹³å°è¶Šæ¥è¶Šä¾èµ–ä¸ªæ€§åŒ–è·¯çº¿æ¨èæ¥æå‡æ—…è¡Œä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚ç„¶è€Œï¼Œå…³é”®é—®é¢˜åœ¨äºè¿™äº›ä¸ªæ€§åŒ–çš„é¡ºåºè·¯çº¿å†³ç­–æ˜¯å¦èƒ½å¤Ÿå…±åŒå®ç°ç³»ç»Ÿæœ€ä¼˜äº¤é€šåˆ†é…ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å­¦ä¹ æ¡†æ¶ï¼Œå°†é™æ€ç³»ç»Ÿæœ€ä¼˜äº¤é€šåˆ†é…é—®é¢˜é‡æ„ä¸ºå•ä»£ç†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚ä¸­å¿ƒä»£ç†æ ¹æ®èµ·ç‚¹-ç»ˆç‚¹éœ€æ±‚çš„åˆ°è¾¾ï¼Œé¡ºåºæ¨èè·¯çº¿ä»¥æœ€å°åŒ–æ€»ç³»ç»Ÿæ—…è¡Œæ—¶é—´ã€‚ä¸ºæé«˜å­¦ä¹ æ•ˆç‡å’Œè§£å†³æ–¹æ¡ˆè´¨é‡ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ç§å¤šé˜¶æ®µå¼•å¯¼çš„æ·±åº¦Qå­¦ä¹ ç®—æ³•ï¼Œå°†ä¼ ç»Ÿäº¤é€šåˆ†é…æ–¹æ³•çš„è¿­ä»£ç»“æ„èå…¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨Braessç½‘ç»œä¸­æ”¶æ•›åˆ°ç†è®ºçš„ç³»ç»Ÿæœ€ä¼˜è§£ï¼Œè€Œåœ¨Ortuzar-Willumsenç½‘ç»œä¸­ä»…æœ‰0.35%çš„åå·®ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ä¸ªæ€§åŒ–è·¯çº¿æ¨èå¦‚ä½•å®ç°ç³»ç»Ÿæœ€ä¼˜äº¤é€šåˆ†é…çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ•´åˆä¸ªä½“å†³ç­–ä¸ç³»ç»Ÿæ•ˆç‡ï¼Œå¯¼è‡´æ•´ä½“äº¤é€šæµé‡ä¸ç†æƒ³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†é™æ€ç³»ç»Ÿæœ€ä¼˜äº¤é€šåˆ†é…é—®é¢˜è½¬åŒ–ä¸ºå•ä»£ç†çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œé€šè¿‡ä¸­å¿ƒä»£ç†é¡ºåºæ¨èè·¯çº¿ï¼Œä»¥æœ€å°åŒ–ç³»ç»Ÿçš„æ€»æ—…è¡Œæ—¶é—´ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤ŸåŠ¨æ€é€‚åº”å®æ—¶äº¤é€šéœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªä¸­å¿ƒä»£ç†ï¼Œè´Ÿè´£æ ¹æ®å®æ—¶çš„èµ·ç‚¹-ç»ˆç‚¹éœ€æ±‚æ¨èè·¯çº¿ã€‚é‡‡ç”¨å¤šé˜¶æ®µå¼•å¯¼çš„æ·±åº¦Qå­¦ä¹ ç®—æ³•ï¼Œç»“åˆä¼ ç»Ÿäº¤é€šåˆ†é…æ–¹æ³•çš„è¿­ä»£ç»“æ„ï¼Œæå‡å­¦ä¹ æ•ˆç‡å’Œè§£å†³æ–¹æ¡ˆè´¨é‡ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå°†ä¼ ç»Ÿçš„äº¤é€šåˆ†é…æ–¹æ³•ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œä½¿å¾—ä¸ªä½“çš„è·¯çº¿é€‰æ‹©èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¸ç³»ç»Ÿçº§æ•ˆç‡ç›¸ç»“åˆã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨ç®—æ³•è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†å¤šé˜¶æ®µå¼•å¯¼çš„æ·±åº¦Qå­¦ä¹ ï¼Œè®¾ç½®äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†SO-informedçš„è·¯çº¿åŠ¨ä½œé›†ï¼Œä»¥åŠ å¿«æ”¶æ•›é€Ÿåº¦å’Œæå‡æœ€ç»ˆæ€§èƒ½ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨Braessç½‘ç»œä¸­æˆåŠŸæ”¶æ•›è‡³ç†è®ºçš„ç³»ç»Ÿæœ€ä¼˜è§£ï¼Œè€Œåœ¨Ortuzar-Willumsenç½‘ç»œä¸­ä»…æœ‰0.35%çš„åå·®ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€å¯¼èˆªåº”ç”¨å’Œå…±äº«å‡ºè¡Œå¹³å°ã€‚é€šè¿‡å®ç°ä¸ªæ€§åŒ–çš„è·¯çº¿æ¨èä¸ç³»ç»Ÿæœ€ä¼˜äº¤é€šåˆ†é…çš„ç»“åˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡äº¤é€šæµé‡ç®¡ç†çš„æ•ˆç‡ï¼Œå‡å°‘æ‹¥å µï¼Œæé«˜ç”¨æˆ·å‡ºè¡Œä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Modern navigation systems and shared mobility platforms increasingly rely on personalized route recommendations to improve individual travel experience and operational efficiency. However, a key question remains: can such sequential, personalized routing decisions collectively lead to system-optimal (SO) traffic assignment? This paper addresses this question by proposing a learning-based framework that reformulates the static SO traffic assignment problem as a single-agent deep reinforcement learning (RL) task. A central agent sequentially recommends routes to travelers as origin-destination (OD) demands arrive, to minimize total system travel time. To enhance learning efficiency and solution quality, we develop an MSA-guided deep Q-learning algorithm that integrates the iterative structure of traditional traffic assignment methods into the RL training process. The proposed approach is evaluated on both the Braess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent converges to the theoretical SO solution in the Braess network and achieves only a 0.35% deviation in the OW network. Further ablation studies demonstrate that the route action set's design significantly impacts convergence speed and final performance, with SO-informed route sets leading to faster learning and better outcomes. This work provides a theoretically grounded and practically relevant approach to bridging individual routing behavior with system-level efficiency through learning-based sequential assignment.

