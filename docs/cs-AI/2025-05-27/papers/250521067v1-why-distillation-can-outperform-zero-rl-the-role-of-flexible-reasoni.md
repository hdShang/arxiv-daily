---
layout: default
title: "Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning"
---

# Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21067" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21067v1</a>
  <a href="https://arxiv.org/pdf/2505.21067.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21067v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21067v1', 'Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiao Hu, Xingyu Lu, Liyuan Mao, YiFan Zhang, Tianke Zhang, Bin Wen, Fan Yang, Tingting Gao, Guorui Zhou

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè’¸é¦æ–¹æ³•ä»¥è¶…è¶Šé›¶å¼ºåŒ–å­¦ä¹ çš„çµæ´»æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è’¸é¦è®­ç»ƒ` `é›¶å¼ºåŒ–å­¦ä¹ ` `æ¨ç†èƒ½åŠ›` `å¤šè§’åº¦æ€ç»´` `å…ƒè®¤çŸ¥æ„è¯†` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½é—®ç­”`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é›¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®éœ€æ±‚å’Œè®¡ç®—æˆæœ¬ä¸Šå­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨æ¨ç†èƒ½åŠ›æå‡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸºç¡€æ¨¡å‹çš„è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡å°‘é‡ç¤ºä¾‹å®ç°æ›´çµæ´»çš„æ¨ç†èƒ½åŠ›ï¼Œé™ä½äº†æ•°æ®å’Œè®¡ç®—æˆæœ¬ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè’¸é¦æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šæ˜æ˜¾ä¼˜äºé›¶å¼ºåŒ–å­¦ä¹ ï¼Œå°¤å…¶åœ¨å¤šè§’åº¦æ€ç»´å’Œå…ƒè®¤çŸ¥æ„è¯†çš„è¡¨ç°ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚éƒ¨åˆ†ç ”ç©¶å°†RLç›´æ¥åº”ç”¨äºè¾ƒå°çš„åŸºç¡€æ¨¡å‹ï¼ˆç§°ä¸ºé›¶å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶è¡¨æ˜ï¼Œä»…ä½¿ç”¨920ä¸ªç¤ºä¾‹ï¼ŒåŸºäºåŸºç¡€æ¨¡å‹çš„ç®€å•è’¸é¦æ–¹æ³•æ˜æ˜¾ä¼˜äºé€šå¸¸éœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—æˆæœ¬çš„é›¶å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡åˆ†ææ¨¡å‹è¾“å‡ºä¸­çš„æ ‡è®°é¢‘ç‡ï¼Œæˆ‘ä»¬å‘ç°è’¸é¦æ¨¡å‹å±•ç°å‡ºæ›´çµæ´»çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨äººæ€§åŒ–æ ‡è®°å’Œé€»è¾‘è¿æ¥è¯çš„é¢‘ç‡è¿œé«˜äºé›¶å¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼Œè’¸é¦å¢å¼ºäº†ä¸¤ç§é«˜çº§è®¤çŸ¥è¡Œä¸ºçš„å­˜åœ¨ï¼šå¤šè§’åº¦æ€ç»´å’Œå…ƒè®¤çŸ¥æ„è¯†ã€‚è¿™ä¸¤ç§é«˜çº§è®¤çŸ¥è¡Œä¸ºçš„é¢‘ç¹å‡ºç°ä¿ƒè¿›äº†çµæ´»æ¨ç†ï¼Œè¿™å¯¹äºè§£å†³å¤æ‚æ¨ç†é—®é¢˜è‡³å…³é‡è¦ï¼Œè€Œé›¶å¼ºåŒ–å­¦ä¹ æœªèƒ½æ˜¾è‘—æå‡è¿™äº›è¡Œä¸ºçš„é¢‘ç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³é›¶å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†èƒ½åŠ›æå‡ä¸­é¢ä¸´çš„æ•°æ®éœ€æ±‚å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨å°‘é‡æ•°æ®æ¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è’¸é¦æ–¹æ³•ï¼Œåˆ©ç”¨å°‘é‡ç¤ºä¾‹æ¥è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨é™ä½æ•°æ®éœ€æ±‚ï¼ŒåŒæ—¶æå‡æ¨¡å‹çš„è®¤çŸ¥è¡¨ç°ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¨¡å‹è’¸é¦å’Œæ€§èƒ½è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆæ”¶é›†920ä¸ªç¤ºä¾‹ï¼Œç„¶åé€šè¿‡è’¸é¦æŠ€æœ¯è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œæœ€åè¯„ä¼°æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºè’¸é¦æ–¹æ³•çš„åº”ç”¨ï¼Œä½¿å¾—æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„é›¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è’¸é¦æ¨¡å‹åœ¨é€»è¾‘è¿æ¥å’Œäººæ€§åŒ–æ ‡è®°çš„ä½¿ç”¨ä¸Šè¡¨ç°æ›´ä¸ºçªå‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å‚æ•°è®¾ç½®ä¸Šï¼Œè’¸é¦è¿‡ç¨‹ä¸­é‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºçš„æ ‡è®°é¢‘ç‡ï¼Œå¹¶è®¾è®¡äº†é€‚åˆçš„ç½‘ç»œç»“æ„ä»¥å¢å¼ºå¤šè§’åº¦æ€ç»´å’Œå…ƒè®¤çŸ¥æ„è¯†çš„è¡¨ç°ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œè’¸é¦æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºé›¶å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶åœ¨å¤šè§’åº¦æ€ç»´å’Œå…ƒè®¤çŸ¥æ„è¯†çš„é¢‘ç‡ä¸Šæ˜¾è‘—æå‡ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šã€‚è¿™ä¸€ç»“æœè¡¨æ˜è’¸é¦æ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›æå‡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œæ•™è‚²æŠ€æœ¯ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè’¸é¦æ–¹æ³•å¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å‡†ç¡®çš„å›ç­”å’Œæ›´çµæ´»çš„äº¤äº’ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has played an important role in improving the reasoning ability of large language models (LLMs). Some studies apply RL directly to \textit{smaller} base models (known as zero-RL) and also achieve notable progress. However, in this paper, we show that using only 920 examples, a simple distillation method based on the base model can clearly outperform zero-RL, which typically requires much more data and computational cost. By analyzing the token frequency in model outputs, we find that the distilled model shows more flexible reasoning. It uses anthropomorphic tokens and logical connectors much more often than the zero-RL model. Further analysis reveals that distillation enhances the presence of two advanced cognitive behaviors: Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent occurrences of these two advanced cognitive behaviors give rise to flexible reasoning, which is essential for solving complex reasoning problems, while zero-RL fails to significantly boost the frequency of these behaviors.

