---
layout: default
title: "MSEarth: A Multimodal Scientific Dataset and Benchmark for Phenomena Uncovering in Earth Science"
---

# MSEarth: A Multimodal Scientific Dataset and Benchmark for Phenomena Uncovering in Earth Science

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20740" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20740v2</a>
  <a href="https://arxiv.org/pdf/2505.20740.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20740v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20740v2', 'MSEarth: A Multimodal Scientific Dataset and Benchmark for Phenomena Uncovering in Earth Science')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiangyu Zhao, Wanghan Xu, Bo Liu, Yuhao Zhou, Fenghua Ling, Ben Fei, Xiaoyu Yue, Lei Bai, Wenlong Zhang, Xiao-Ming Wu

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-10-15)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºMSEarthä»¥è§£å†³åœ°çƒç§‘å­¦é¢†åŸŸå¤šæ¨¡æ€åŸºå‡†ç¼ºå¤±é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€åŸºå‡†` `åœ°çƒç§‘å­¦` `ç§‘å­¦æ¨ç†` `å¼€æ”¾è·å–` `æ•°æ®é›†æ„å»º` `ç ”ç©¶ç”Ÿæ•™è‚²` `å›¾å½¢æ ‡é¢˜ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºå‡†ç¼ºä¹èƒ½å¤Ÿæ•æ‰åœ°çƒç§‘å­¦æ¨ç†æ·±åº¦å’Œå¤æ‚æ€§çš„èƒ½åŠ›ï¼Œé™åˆ¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚
2. MSEarthæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç§‘å­¦åŸºå‡†ï¼Œæ•´åˆäº†é«˜è´¨é‡çš„ç§‘å­¦å‡ºç‰ˆç‰©ï¼Œæ¶µç›–äº”å¤§åœ°çƒç§‘å­¦é¢†åŸŸï¼Œæä¾›289Kä¸ªå›¾å½¢åŠå…¶ä¸°å¯Œçš„æ ‡é¢˜ã€‚
3. è¯¥åŸºå‡†æ”¯æŒç§‘å­¦å›¾å½¢æ ‡é¢˜ç”Ÿæˆã€é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼æ¨ç†æŒ‘æˆ˜ï¼Œæ—¨åœ¨æå‡ç ”ç©¶ç”Ÿå±‚é¢çš„ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè§£å†³å¤æ‚ç§‘å­¦é—®é¢˜çš„æ–°æœºé‡ä¸æ–­æ¶Œç°ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åœ°çƒç§‘å­¦é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç ”ç©¶ç”Ÿå±‚é¢çš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ç°æœ‰åŸºå‡†å¾€å¾€ä¾èµ–äºåˆæˆæ•°æ®é›†æˆ–ç®€å•çš„å›¾å½¢-æ ‡é¢˜å¯¹ï¼Œæ— æ³•åæ˜ çœŸå®ç§‘å­¦åº”ç”¨æ‰€éœ€çš„å¤æ‚æ¨ç†å’Œé¢†åŸŸç‰¹å®šè§è§£ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MSEarthï¼Œä¸€ä¸ªä»é«˜è´¨é‡å¼€æ”¾è·å–ç§‘å­¦å‡ºç‰ˆç‰©ä¸­ç­–åˆ’çš„å¤šæ¨¡æ€ç§‘å­¦åŸºå‡†ï¼Œæ¶µç›–äº†åœ°çƒç§‘å­¦çš„äº”å¤§é¢†åŸŸï¼ŒåŒ…å«è¶…è¿‡289Kä¸ªå›¾å½¢åŠå…¶ç²¾ç‚¼æ ‡é¢˜ï¼Œæ”¯æŒå¤šç§ç§‘å­¦ä»»åŠ¡ã€‚MSEarthä¸ºç ”ç©¶ç”Ÿå±‚é¢çš„åŸºå‡†å¡«è¡¥äº†ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”é«˜ä¿çœŸçš„èµ„æºï¼Œä»¥ä¿ƒè¿›MLLMsåœ¨ç§‘å­¦æ¨ç†ä¸­çš„å‘å±•ä¸è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ°çƒç§‘å­¦é¢†åŸŸç¼ºä¹é«˜è´¨é‡å¤šæ¨¡æ€åŸºå‡†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºåˆæˆæ•°æ®é›†ï¼Œæ— æ³•åæ˜ çœŸå®ç§‘å­¦æ¨ç†çš„å¤æ‚æ€§å’Œæ·±åº¦ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šMSEarthé€šè¿‡ä»é«˜è´¨é‡çš„å¼€æ”¾è·å–ç§‘å­¦å‡ºç‰ˆç‰©ä¸­æå–æ•°æ®ï¼Œæ„å»ºä¸€ä¸ªæ¶µç›–äº”å¤§åœ°çƒç§‘å­¦é¢†åŸŸçš„å¤šæ¨¡æ€åŸºå‡†ï¼Œç¡®ä¿å…¶å†…å®¹çš„ä¸°å¯Œæ€§å’Œç§‘å­¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šMSEarthçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å›¾å½¢ä¸æ ‡é¢˜çš„æå–ä¸å¤„ç†ã€ä»¥åŠå¤šç§ä»»åŠ¡çš„è®¾è®¡ä¸è¯„ä¼°ã€‚ä¸»è¦æ¨¡å—åŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ä»»åŠ¡å®šä¹‰å’Œè¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®åˆ›æ–°**ï¼šMSEarthçš„åˆ›æ–°åœ¨äºå…¶åŸºäºçœŸå®ç§‘å­¦å‡ºç‰ˆç‰©çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨å¤æ‚æ¨ç†å’Œé¢†åŸŸçŸ¥è¯†æ–¹é¢çš„ä¸è¶³ï¼Œæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ ‡å‡†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­ï¼Œæ ‡é¢˜ä¸ä»…åŸºäºåŸå§‹å›¾å½¢æ ‡é¢˜ï¼Œè¿˜ç»“åˆäº†è®ºæ–‡ä¸­çš„è®¨è®ºå’Œæ¨ç†ï¼Œç¡®ä¿å†…å®¹çš„æ·±åº¦å’ŒçŸ¥è¯†å¯†é›†æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

MSEarthåœ¨å¤šé¡¹ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ç§‘å­¦å›¾å½¢æ ‡é¢˜ç”Ÿæˆå’Œå¼€æ”¾å¼æ¨ç†æŒ‘æˆ˜ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰åŸºå‡†ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ï¼Œå±•ç¤ºäº†å…¶åœ¨åœ°çƒç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

MSEarthçš„ç ”ç©¶æˆæœå¯å¹¿æ³›åº”ç”¨äºåœ°çƒç§‘å­¦æ•™è‚²ã€ç§‘ç ”åŠç›¸å…³é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨ç ”ç©¶ç”Ÿæ•™è‚²ä¸­ï¼Œèƒ½å¤Ÿå¸®åŠ©å­¦ç”Ÿæå‡ç§‘å­¦æ¨ç†èƒ½åŠ›å’Œå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†è¿˜å¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å…¶ä»–ç§‘å­¦é¢†åŸŸçš„åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The rapid advancement of multimodal large language models (MLLMs) has unlocked new opportunities to tackle complex scientific challenges. Despite this progress, their application in addressing earth science problems, especially at the graduate level, remains underexplored. A significant barrier is the absence of benchmarks that capture the depth and contextual complexity of geoscientific reasoning. Current benchmarks often rely on synthetic datasets or simplistic figure-caption pairs, which do not adequately reflect the intricate reasoning and domain-specific insights required for real-world scientific applications. To address these gaps, we introduce MSEarth, a multimodal scientific benchmark curated from high-quality, open-access scientific publications. MSEarth encompasses the five major spheres of Earth science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere, featuring over 289K figures with refined captions. These captions are crafted from the original figure captions and enriched with discussions and reasoning from the papers, ensuring the benchmark captures the nuanced reasoning and knowledge-intensive content essential for advanced scientific tasks. MSEarth supports a variety of tasks, including scientific figure captioning, multiple choice questions, and open-ended reasoning challenges. By bridging the gap in graduate-level benchmarks, MSEarth provides a scalable and high-fidelity resource to enhance the development and evaluation of MLLMs in scientific reasoning. The benchmark is publicly available to foster further research and innovation in this field.

