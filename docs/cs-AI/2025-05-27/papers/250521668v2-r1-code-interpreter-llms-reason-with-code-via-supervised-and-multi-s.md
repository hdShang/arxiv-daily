---
layout: default
title: "R1-Code-Interpreter: LLMs Reason with Code via Supervised and Multi-stage Reinforcement Learning"
---

# R1-Code-Interpreter: LLMs Reason with Code via Supervised and Multi-stage Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.21668" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.21668v2</a>
  <a href="https://arxiv.org/pdf/2505.21668.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.21668v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.21668v2', 'R1-Code-Interpreter: LLMs Reason with Code via Supervised and Multi-stage Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, Na Li, Chuchu Fan

**åˆ†ç±»**: cs.AI, cs.CL, cs.SC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-27 (æ›´æ–°: 2025-09-29)

**å¤‡æ³¨**: 26 pages, 10 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yongchao98/R1-Code-Interpreter) | [HUGGINGFACE](https://huggingface.co/yongchao98)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºR1-Code-Interpreterä»¥è§£å†³LLMsåœ¨ä»£ç æ¨ç†ä¸­çš„æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç è§£é‡Šå™¨` `å¼ºåŒ–å­¦ä¹ ` `å¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ ` `æ¨ç†ä»»åŠ¡` `æ¨¡å‹è®­ç»ƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒé€šç”¨ä»£ç è§£é‡Šå™¨æ—¶é¢ä¸´ä»»åŠ¡å¼‚è´¨æ€§å’Œæœ‰æ•ˆæ ·æœ¬ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚
2. æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œä¼˜å…ˆé€‰æ‹©é«˜æ½œåŠ›æ ·æœ¬è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œé€æ­¥è½¬å‘ä½æ½œåŠ›æ ·æœ¬ã€‚
3. æœ€ç»ˆæ¨¡å‹R1-CI-14Båœ¨37ä¸ªæµ‹è¯•ä»»åŠ¡ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä»44.1%æå‡è‡³72.4%ï¼Œè¶…è¶Šäº†æ–‡æœ¬æ¨¡å‹GPT-4oå’Œå¸¦ä»£ç è§£é‡Šå™¨çš„GPT-4oã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥åˆ©ç”¨ä»£ç è§£é‡Šå™¨è¿›è¡Œå¤šä»»åŠ¡æ¨ç†æ–¹é¢ï¼Œç¼ºä¹å®ç”¨æŒ‡å¯¼ã€‚æœ¬æ–‡æå‡ºR1-Code-Interpreterï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¤šè½®ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ–‡æœ¬æ¨¡å‹æ‰©å±•ï¼Œèƒ½å¤Ÿåœ¨é€æ­¥æ¨ç†ä¸­è‡ªä¸»ç”Ÿæˆå¤šä¸ªä»£ç æŸ¥è¯¢ã€‚ä¸ä»¥å¾€é›†ä¸­äºç‹­çª„é¢†åŸŸçš„RL + å·¥å…·ä½¿ç”¨æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ç­–åˆ’äº†144ä¸ªå¤šæ ·åŒ–çš„æ¨ç†ä¸è§„åˆ’ä»»åŠ¡ï¼Œå¹¶å±•ç¤ºäº†åœ¨ä»»åŠ¡å¼‚è´¨æ€§å’Œæœ‰æ•ˆæ ·æœ¬ç¨€ç¼ºæ€§ä¸‹ï¼Œè®­ç»ƒé€šç”¨ä»£ç è§£é‡Šå™¨æ‰€é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æµ‹é‡æ”¹è¿›æ½œåŠ›æ¥åˆ’åˆ†è®­ç»ƒæ ·æœ¬ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ ·åŒ–æ¨ç†ä»»åŠ¡ä¸­å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨ä»£ç è§£é‡Šå™¨çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä»»åŠ¡å¼‚è´¨æ€§å’Œæ ·æœ¬ç¨€ç¼ºæ€§æ—¶è¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¯¹è®­ç»ƒæ ·æœ¬çš„æ”¹è¿›æ½œåŠ›è¿›è¡Œæµ‹é‡ï¼Œä¼˜å…ˆé€‰æ‹©é«˜æ½œåŠ›æ ·æœ¬è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œé€æ­¥å¼•å…¥ä½æ½œåŠ›æ ·æœ¬ï¼Œä»¥æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬å¤šè½®ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå¯¹æ¨¡å‹è¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œç„¶ååœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä¾æ®æ ·æœ¬çš„æ½œåŠ›è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œé€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨é¢å¯¹å¤šæ ·åŒ–ä»»åŠ¡æ—¶ï¼Œé€æ­¥é€‚åº”å¹¶æå‡æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å•ä¸€è®­ç»ƒç­–ç•¥æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹ä»»åŠ¡çš„å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œé‡‡ç”¨äº†åŠ¨æ€æ ·æœ¬é€‰æ‹©æœºåˆ¶ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šæ³¨é‡å¼ºåŒ–å­¦ä¹ çš„æ”¶ç›Šæœ€å¤§åŒ–ï¼Œç½‘ç»œç»“æ„åˆ™åŸºäºç°æœ‰çš„LLMæ¶æ„è¿›è¡Œä¼˜åŒ–ï¼Œä»¥é€‚åº”ä»£ç ç”Ÿæˆçš„éœ€æ±‚ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œè®­ç»ƒç»†èŠ‚åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨å®éªŒä¸­ï¼ŒR1-CI-14Bæ¨¡å‹åœ¨37ä¸ªæµ‹è¯•ä»»åŠ¡ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä»44.1%æå‡è‡³72.4%ï¼Œç›¸æ¯”æ–‡æœ¬æ¨¡å‹GPT-4oï¼ˆ58.6%ï¼‰å’Œå¸¦ä»£ç è§£é‡Šå™¨çš„GPT-4oï¼ˆ70.9%ï¼‰å‡æœ‰æ˜¾è‘—æå‡ï¼Œå¼ºåŒ–å­¦ä¹ çš„å¹³å‡æ”¶ç›Šä»+3.4%æå‡è‡³+9.3%ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€ç¼–ç¨‹è¾…åŠ©ã€è‡ªåŠ¨åŒ–æµ‹è¯•ç­‰ã€‚é€šè¿‡æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒR1-Code-Interpreterèƒ½å¤Ÿä¸ºå¼€å‘è€…æä¾›æ›´æ™ºèƒ½çš„ç¼–ç¨‹æ”¯æŒï¼Œä¿ƒè¿›è½¯ä»¶å¼€å‘çš„æ•ˆç‡å’Œè´¨é‡ï¼Œæœªæ¥å¯èƒ½åœ¨å„ç±»ç¼–ç¨‹ç›¸å…³çš„AIåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Practical guidance on training Large Language Models (LLMs) to leverage Code Interpreter across diverse tasks remains lacking. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. Unlike prior RL + tool-use efforts focused on narrow domains such as math or retrieval, we curate 144 diverse reasoning and planning tasks and show that training a general-purpose Code Interpreter across them presents significant challenges due to task heterogeneity and scarcity of effective samples. To address this, we introduce a multi-stage curriculum learning approach that partitions training samples by measured improvement potential. The RL training prioritizes samples with higher potential and gradually shifts to lower-potential ones, increasing the average RL gains from merely +3.4% to +9.3% across Qwen-2.5 models (3/7/14B). Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.1% to 72.4%, outperforming text-only GPT-4o (58.6%) and GPT-4o with Code Interpreter (70.9%). Notably, R1-CI-14B also exhibits emergent self-checking behavior through code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98.

