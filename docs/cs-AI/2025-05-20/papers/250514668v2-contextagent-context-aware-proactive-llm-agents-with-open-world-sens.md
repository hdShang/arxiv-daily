---
layout: default
title: ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions
---

# ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14668" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14668v2</a>
  <a href="https://arxiv.org/pdf/2505.14668.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14668v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14668v2', 'ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan

**åˆ†ç±»**: cs.AI, cs.CL, cs.HC

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-27)

**å¤‡æ³¨**: Accepted by NeurIPS 2025

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/openaiotlab/ContextAgent)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºContextAgentä»¥è§£å†³ç°æœ‰ä¸»åŠ¨æ™ºèƒ½ä½“çš„å±€é™æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä¸Šä¸‹æ–‡æ„ŸçŸ¥` `ä¸»åŠ¨æ™ºèƒ½ä½“` `å¤šæ¨¡æ€èåˆ` `ç”¨æˆ·æ„å›¾ç†è§£` `å¯ç©¿æˆ´è®¾å¤‡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ä¸»åŠ¨æ™ºèƒ½ä½“ä¾èµ–å°é—­ç¯å¢ƒçš„è§‚å¯Ÿï¼Œå¯¼è‡´ç”¨æˆ·æ„å›¾ç†è§£ä¸è¶³å’ŒåŠŸèƒ½å—é™ã€‚
2. ContextAgenté€šè¿‡æå–å¤šç»´æ„ŸçŸ¥ä¸Šä¸‹æ–‡ï¼Œç»“åˆå†å²æ•°æ®ï¼Œå¢å¼ºLLMæ™ºèƒ½ä½“çš„ä¸»åŠ¨æ€§ã€‚
3. åœ¨ContextAgentBenchä¸Šï¼ŒContextAgentåœ¨ä¸»åŠ¨é¢„æµ‹å’Œå·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§ä¸Šåˆ†åˆ«æé«˜äº†8.5%å’Œ6.0%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ä½¿å¾—æ™ºèƒ½ä½“ä»è¢«åŠ¨å“åº”è½¬å‘ä¸»åŠ¨æ”¯æŒã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸»åŠ¨æ™ºèƒ½ä½“è¦ä¹ˆä»…ä¾èµ–å°é—­ç¯å¢ƒä¸­çš„è§‚å¯Ÿï¼ˆå¦‚æ¡Œé¢ç”¨æˆ·ç•Œé¢ï¼‰è¿›è¡Œç›´æ¥æ¨ç†ï¼Œè¦ä¹ˆé‡‡ç”¨åŸºäºè§„åˆ™çš„ä¸»åŠ¨é€šçŸ¥ï¼Œå¯¼è‡´ç”¨æˆ·æ„å›¾ç†è§£ä¸è¶³å’ŒåŠŸèƒ½æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ContextAgentï¼Œè¿™æ˜¯é¦–ä¸ªç»“åˆäººç±»å‘¨å›´å¹¿æ³›æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨æ™ºèƒ½ä½“ï¼Œä»¥å¢å¼ºLLMæ™ºèƒ½ä½“çš„ä¸»åŠ¨æ€§ã€‚ContextAgenté¦–å…ˆä»å¯ç©¿æˆ´è®¾å¤‡çš„å¤šç»´æ„ŸçŸ¥ä¸­æå–ä¸Šä¸‹æ–‡ï¼Œä»¥ç†è§£ç”¨æˆ·æ„å›¾ã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨æ„ŸçŸ¥ä¸Šä¸‹æ–‡å’Œå†å²æ•°æ®ä¸­çš„ä¸ªæ€§åŒ–ä¿¡æ¯æ¥é¢„æµ‹ä¸»åŠ¨æœåŠ¡çš„å¿…è¦æ€§ã€‚å½“éœ€è¦ä¸»åŠ¨ååŠ©æ—¶ï¼ŒContextAgentä¼šè‡ªåŠ¨è°ƒç”¨å¿…è¦çš„å·¥å…·ï¼Œä»¥ä¸æ‰“æ‰°ç”¨æˆ·çš„æ–¹å¼æä¾›å¸®åŠ©ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ContextAgentBenchï¼Œè¿™æ˜¯è¯„ä¼°ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨LLMæ™ºèƒ½ä½“çš„é¦–ä¸ªåŸºå‡†ï¼Œæ¶µç›–1000ä¸ªæ ·æœ¬å’Œ20ç§å·¥å…·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒContextAgentåœ¨ä¸»åŠ¨é¢„æµ‹å’Œå·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§ä¸Šåˆ†åˆ«æ¯”åŸºçº¿æé«˜äº†8.5%å’Œ6.0%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä¸»åŠ¨æ™ºèƒ½ä½“åœ¨ç”¨æˆ·æ„å›¾ç†è§£å’ŒåŠŸèƒ½ä¸Šçš„å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å°é—­ç¯å¢ƒçš„è§‚å¯Ÿï¼Œå¯¼è‡´ä¸»åŠ¨æœåŠ¡æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šContextAgentçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æå–æ¥è‡ªå¯ç©¿æˆ´è®¾å¤‡çš„å¤šç»´æ„ŸçŸ¥ä¸Šä¸‹æ–‡ï¼Œç»“åˆå†å²æ•°æ®ä¸­çš„ä¸ªæ€§åŒ–ä¿¡æ¯ï¼Œæ¥æ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·æ„å›¾å¹¶é¢„æµ‹ä¸»åŠ¨æœåŠ¡çš„éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šContextAgentçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸Šä¸‹æ–‡æå–æ¨¡å—ã€æ„å›¾ç†è§£æ¨¡å—å’Œå·¥å…·è°ƒç”¨æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯ç©¿æˆ´è®¾å¤‡æ”¶é›†è§†é¢‘å’ŒéŸ³é¢‘ç­‰å¤šç»´æ„ŸçŸ¥æ•°æ®ï¼›å…¶æ¬¡ï¼Œåˆ†æè¿™äº›æ•°æ®ä»¥ç†è§£ç”¨æˆ·æ„å›¾ï¼›æœ€åï¼Œæ ¹æ®é¢„æµ‹çš„éœ€æ±‚è‡ªåŠ¨è°ƒç”¨ç›¸åº”å·¥å…·æä¾›å¸®åŠ©ã€‚

**å…³é”®åˆ›æ–°**ï¼šContextAgentçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»å¤šç§æ„ŸçŸ¥æ•°æ®ä¸­æå–ä¿¡æ¯ï¼Œä»è€Œè¶…è¶Šä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„ä¸»åŠ¨é€šçŸ¥æ–¹æ³•ï¼Œæä¾›æ›´ä¸ºç²¾å‡†çš„ç”¨æˆ·æœåŠ¡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸Šï¼ŒContextAgentä½¿ç”¨äº†å¤šæ¨¡æ€èåˆæŠ€æœ¯æ¥å¤„ç†ä¸åŒç±»å‹çš„æ„ŸçŸ¥æ•°æ®ï¼Œå¹¶é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ä¼˜åŒ–ç”¨æˆ·æ„å›¾çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°çš„è®¾è®¡è€ƒè™‘äº†ä¸»åŠ¨æœåŠ¡çš„å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒçš„å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨ContextAgentBenchçš„å®éªŒä¸­ï¼ŒContextAgentåœ¨ä¸»åŠ¨é¢„æµ‹çš„å‡†ç¡®æ€§ä¸Šæ¯”åŸºçº¿æé«˜äº†8.5%ï¼Œåœ¨å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§ä¸Šæé«˜äº†6.0%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒContextAgentåœ¨ç†è§£ç”¨æˆ·æ„å›¾å’Œæä¾›ä¸»åŠ¨æœåŠ¡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

ContextAgentçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ™ºèƒ½å®¶å±…ã€ä¸ªäººåŠ©ç†å’Œå¥åº·ç›‘æµ‹ç­‰é¢†åŸŸã€‚é€šè¿‡æä¾›æ›´ä¸ºç²¾å‡†å’Œäººæ€§åŒ–çš„ä¸»åŠ¨æœåŠ¡ï¼ŒContextAgentèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒï¼Œå¹¶æ¨åŠ¨æ™ºèƒ½åŠ©æ‰‹å‘æ›´é«˜å±‚æ¬¡çš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æŠ€æœ¯å¯èƒ½ä¼šåœ¨æ›´å¤šäººæœºäº¤äº’åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œä¿ƒè¿›æ™ºèƒ½ä½“çš„æ™®åŠä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts surrounding humans to enhance the proactivity of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and personas from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. The code and dataset are publicly available at https://github.com/openaiotlab/ContextAgent.

