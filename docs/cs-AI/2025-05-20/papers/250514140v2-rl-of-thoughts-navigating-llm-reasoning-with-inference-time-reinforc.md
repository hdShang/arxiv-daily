---
layout: default
title: RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning
---

# RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14140" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.14140v2</a>
  <a href="https://arxiv.org/pdf/2505.14140.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14140v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14140v2', 'RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Qianyue Hao, Sibo Li, Jian Yuan, Yong Li

**ÂàÜÁ±ª**: cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-20 (Êõ¥Êñ∞: 2025-09-25)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫RL-of-Thoughts‰ª•Â¢ûÂº∫Â§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßËØ≠Ë®ÄÊ®°Âûã` `Êé®ÁêÜËÉΩÂäõ` `Âº∫ÂåñÂ≠¶‰π†` `ÈÄªËæëÊ®°Âùó` `Âä®ÊÄÅÈÄâÊã©` `‰ªªÂä°ÈÄÇÂ∫îÊÄß` `ËøÅÁßªÂ≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊé®ÁêÜÊäÄÊúØÂ¶ÇChain/Tree/Graph-of-Thought(s)Áº∫‰πèÈÄÇÂ∫îÊÄßÔºåÊó†Ê≥ïÈíàÂØπ‰∏çÂêå‰ªªÂä°ËøõË°å‰ºòÂåñ„ÄÇ
2. ÊèêÂá∫RL-of-ThoughtsÔºàRLoTÔºâÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÂØºËà™Ê®°ÂûãÔºåÂä®ÊÄÅÈÄâÊã©ÂíåÁªÑÂêàÈÄªËæëÊ®°Âùó‰ª•Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõ„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRLoTÂú®Â§ö‰∏™Âü∫ÂáÜ‰∏äË∂ÖË∂äÁé∞ÊúâÊäÄÊúØÔºåÊèêÂçáÂπÖÂ∫¶ÂèØËææ13.4%Ôºå‰∏îÂÖ∑ÊúâËâØÂ•ΩÁöÑËøÅÁßªËÉΩÂäõ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∞ΩÁÆ°Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøÖÈÄüÂèëÂ±ïÔºå‰ΩÜÂÖ∂Âü∫‰∫étokenÁöÑËá™ÂõûÂΩíÁâπÊÄßÈôêÂà∂‰∫ÜÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏∫ÊèêÂçáLLMÊé®ÁêÜÔºåÊé®ÁêÜÊó∂ÊäÄÊúØÂ¶ÇChain/Tree/Graph-of-Thought(s)ÈÄöËøáÂ§çÊùÇÈÄªËæëÁªìÊûÑÂºïÂØºÊé®ÁêÜÔºåË°®Áé∞Âá∫ËâØÂ•ΩÁöÑÊÄß‰ª∑ÊØî„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊâãÂä®È¢ÑÂÆö‰πâÁöÑÊ°ÜÊû∂Áº∫‰πèÈÄÇÂ∫îÊÄß„ÄÇ‰∏∫Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫RL-of-ThoughtsÔºàRLoTÔºâÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉËΩªÈáèÁ∫ßÂØºËà™Ê®°ÂûãÔºåÂä®ÊÄÅÈÄâÊã©ÈÄÇÂêàÁöÑÈÄªËæëÊ®°ÂùóÔºåÁªìÂêàÊàê‰ªªÂä°ÁâπÂÆöÁöÑÈÄªËæëÁªìÊûÑ„ÄÇÂÆûÈ™åË°®ÊòéÔºåRLoTÂú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜ‰∏äË∂ÖË∂äÁé∞ÊúâÊäÄÊúØÔºå‰∏îÂÖ∂ÂØºËà™Ê®°ÂûãÂú®ÂèÇÊï∞Â∞ë‰∫é3KÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ΩøÂæóÂ∞è‰∫é10BÁöÑLLMÂèØ‰∏é100BËßÑÊ®°ÁöÑÊ®°ÂûãÁõ∏Â™≤Áæé„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Áî±‰∫éËá™ÂõûÂΩíÁâπÊÄßÂØºËá¥ÁöÑÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÊé®ÁêÜÊäÄÊúØËôΩÁÑ∂ÊúâÊïàÔºå‰ΩÜÁº∫‰πèÈíàÂØπÁâπÂÆö‰ªªÂä°ÁöÑÈÄÇÂ∫îÊÄßÔºåÂØºËá¥ÊÄßËÉΩÂèóÈôê„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÊèêÂá∫RL-of-ThoughtsÔºàRLoTÔºâÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÂØºËà™Ê®°ÂûãÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊ†πÊçÆÈóÆÈ¢òÁâπÂæÅÂä®ÊÄÅÈÄâÊã©ÂíåÁªÑÂêàÈÄªËæëÊ®°ÂùóÔºå‰ªéËÄåÊèêÂçáLLMÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöRLoTÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™RLÂØºËà™Ê®°ÂûãÂíå‰∫î‰∏™Âü∫Êú¨ÈÄªËæëÊ®°Âùó„ÄÇÂØºËà™Ê®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Ê†πÊçÆËæìÂÖ•ÁâπÂæÅÈÄâÊã©ÂêàÈÄÇÁöÑÈÄªËæëÊ®°ÂùóÔºåÂπ∂Â∞ÜÂÖ∂ÁªÑÂêàÊàê‰ªªÂä°ÁâπÂÆöÁöÑÈÄªËæëÁªìÊûÑ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöRLoTÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÂÆûÁé∞‰∫ÜÈÄªËæëÊ®°ÂùóÁöÑÂä®ÊÄÅÈÄâÊã©‰∏éÁªÑÂêàÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÂ∫îÊÄß„ÄÇËøô‰∏é‰º†ÁªüÁöÑÊâãÂä®È¢ÑÂÆö‰πâÊ°ÜÊû∂ÂΩ¢ÊàêÈ≤úÊòéÂØπÊØî„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂØºËà™Ê®°ÂûãÁöÑÂèÇÊï∞ËÆæÁΩÆÂ∞ë‰∫é3KÔºåÈááÁî®ËΩªÈáèÁ∫ßËÆæËÆ°ÔºåÁ°Æ‰øùÂú®‰∏çÂ¢ûÂä†ËÆ°ÁÆóË¥üÊãÖÁöÑÊÉÖÂÜµ‰∏ãÊèêÂçáÊé®ÁêÜËÉΩÂäõ„ÄÇÊçüÂ§±ÂáΩÊï∞ÂíåËÆ≠ÁªÉÁ≠ñÁï•ÁªèËøáÁ≤æÂøÉËÆæËÆ°Ôºå‰ª•‰ºòÂåñÊ®°ÂûãÂú®‰∏çÂêå‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåRLoTÂú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÔºàÂ¶ÇAIME„ÄÅMATH„ÄÅGPQAÁ≠âÔºâ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊé®ÁêÜÊäÄÊúØÔºåÊÄßËÉΩÊèêÂçáÂπÖÂ∫¶ÂèØËææ13.4%„ÄÇÊ≠§Â§ñÔºåRLoTÁöÑËΩªÈáèÁ∫ßÂØºËà™Ê®°Âûã‰ΩøÂæóÂ∞è‰∫é10BÁöÑLLMÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÂèØ‰∏é100BËßÑÊ®°ÁöÑÊ®°ÂûãÁõ∏Â™≤ÁæéÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑËøÅÁßªËÉΩÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÂåªÁñó„ÄÅÈáëËûçÁ≠âÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜÁöÑÂú∫ÊôØ„ÄÇÈÄöËøáÊèêÂçáLLMÁöÑÊé®ÁêÜËÉΩÂäõÔºåRLoTËÉΩÂ§üÂú®Ëøô‰∫õÈ¢ÜÂüüÊèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÂÜ≥Á≠ñÊîØÊåÅÂíåÊô∫ËÉΩÊúçÂä°ÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs' parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for reproducibility.

