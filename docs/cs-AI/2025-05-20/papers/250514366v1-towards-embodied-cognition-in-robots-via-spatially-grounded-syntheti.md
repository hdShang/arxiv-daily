---
layout: default
title: Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds
---

# Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14366" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14366v1</a>
  <a href="https://arxiv.org/pdf/2505.14366.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14366v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14366v1', 'Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, Agnieszka Wykowska

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**å¤‡æ³¨**: Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late Breaking Report

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç©ºé—´åŸºç¡€åˆæˆä¸–ç•Œä»¥ä¿ƒè¿›æœºå™¨äººå…·èº«è®¤çŸ¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è§†è§‰-è¯­è¨€æ¨¡å‹` `ç©ºé—´æ¨ç†` `äººæœºäº¤äº’` `åˆæˆæ•°æ®é›†` `å…·èº«è®¤çŸ¥` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æœºå™¨äººå…·èº«è®¤çŸ¥å’Œäººæœºäº¤äº’ä¸­ç¼ºä¹æœ‰æ•ˆçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œç»“åˆRGBå›¾åƒå’Œè‡ªç„¶è¯­è¨€æè¿°ï¼Œæ”¯æŒè§†è§‰è§†è§’è·å–çš„è®­ç»ƒã€‚
3. æ•°æ®é›†çš„å‘å¸ƒä¸ºåç»­ç ”ç©¶æä¾›äº†åŸºç¡€ï¼Œæ¨åŠ¨äº†æœºå™¨äººåœ¨ç©ºé—´ç†è§£æ–¹é¢çš„èƒ½åŠ›æå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¦‚å¿µæ¡†æ¶ï¼Œç”¨äºè®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»¥æ‰§è¡Œè§†è§‰è§†è§’è·å–ï¼ˆVPTï¼‰ï¼Œè¿™æ˜¯å…·èº«è®¤çŸ¥çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¯¹äºäººæœºäº¤äº’ï¼ˆHRIï¼‰è‡³å…³é‡è¦ã€‚ä½œä¸ºå®ç°è¿™ä¸€ç›®æ ‡çš„ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬åœ¨NVIDIA Omniverseä¸­å¼•å…¥äº†ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œæ”¯æŒç©ºé—´æ¨ç†ä»»åŠ¡çš„ç›‘ç£å­¦ä¹ ã€‚æ¯ä¸ªå®ä¾‹åŒ…æ‹¬RGBå›¾åƒã€è‡ªç„¶è¯­è¨€æè¿°å’Œè¡¨ç¤ºç‰©ä½“å§¿æ€çš„çœŸå®4X4å˜æ¢çŸ©é˜µã€‚æˆ‘ä»¬ä¸“æ³¨äºæ¨æ–­Zè½´è·ç¦»ä½œä¸ºåŸºç¡€æŠ€èƒ½ï¼Œæœªæ¥æ‰©å±•å°†é’ˆå¯¹å®Œæ•´çš„å…­è‡ªç”±åº¦ï¼ˆDOFsï¼‰æ¨ç†ã€‚è¯¥æ•°æ®é›†å·²å…¬å¼€ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ã€‚è¿™é¡¹å·¥ä½œä¸ºèƒ½å¤Ÿåœ¨äº’åŠ¨äººæœºåœºæ™¯ä¸­è¿›è¡Œç©ºé—´ç†è§£çš„å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³æœºå™¨äººåœ¨å…·èº«è®¤çŸ¥ä¸­ç¼ºä¹æœ‰æ•ˆç©ºé—´æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†äººæœºäº¤äº’æ—¶ï¼Œæ— æ³•å‡†ç¡®ç†è§£å’Œæ¨æ–­ç©ºé—´å…³ç³»ï¼Œå¯¼è‡´äº¤äº’æ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œç»“åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œæ¥è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè¿›è¡Œè§†è§‰è§†è§’è·å–ï¼Œä»è€Œæå‡æœºå™¨äººå¯¹ç©ºé—´çš„ç†è§£èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚æ•°æ®é›†ç”Ÿæˆé˜¶æ®µåœ¨NVIDIA Omniverseä¸­åˆ›å»ºåˆæˆåœºæ™¯ï¼Œæ¨¡å‹è®­ç»ƒé˜¶æ®µä½¿ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œæ¨ç†é˜¶æ®µåˆ™ä¸“æ³¨äºZè½´è·ç¦»çš„æ¨æ–­ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥äº†ä¸€ä¸ªç»“åˆRGBå›¾åƒã€è‡ªç„¶è¯­è¨€æè¿°å’Œå˜æ¢çŸ©é˜µçš„åˆæˆæ•°æ®é›†ï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨ç©ºé—´æ¨ç†è®­ç»ƒæ•°æ®ä¸Šçš„ç©ºç™½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æä¾›äº†æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ•°æ®é›†è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†4X4å˜æ¢çŸ©é˜µæ¥è¡¨ç¤ºç‰©ä½“å§¿æ€ï¼Œç¡®ä¿äº†ç©ºé—´å…³ç³»çš„å‡†ç¡®æ€§ã€‚æŸå¤±å‡½æ•°è®¾è®¡ä¸Šï¼Œé‡ç‚¹å…³æ³¨Zè½´è·ç¦»çš„æ¨æ–­ç²¾åº¦ï¼Œä»¥æ”¯æŒæœªæ¥çš„å…­è‡ªç”±åº¦æ¨ç†æ‰©å±•ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥åˆæˆæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨Zè½´è·ç¦»æ¨æ–­ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº†çº¦20%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†åšå®åŸºç¡€ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç©ºé—´ç†è§£å’Œäººæœºäº¤äº’èƒ½åŠ›ã€‚æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œè¯¥æ¡†æ¶å¯èƒ½ä¼šæ¨åŠ¨æ›´æ™ºèƒ½çš„å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ï¼Œæ”¹å–„äººæœºåä½œæ•ˆç‡ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.

