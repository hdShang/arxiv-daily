---
layout: default
title: Balanced and Elastic End-to-end Training of Dynamic LLMs
---

# Balanced and Elastic End-to-end Training of Dynamic LLMs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14864" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14864v2</a>
  <a href="https://arxiv.org/pdf/2505.14864.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14864v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14864v2', 'Balanced and Elastic End-to-end Training of Dynamic LLMs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mohamed Wahib, Muhammed Abdullah Soyturk, Didem Unat

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-09-14)

**DOI**: [10.1145/3712285.3759775](https://doi.org/10.1145/3712285.3759775)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºDynMoä»¥è§£å†³å¤§è§„æ¨¡åŠ¨æ€LLMè®­ç»ƒä¸­çš„è´Ÿè½½ä¸å‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŠ¨æ€è´Ÿè½½å‡è¡¡` `å¤§å‹è¯­è¨€æ¨¡å‹` `åˆ†å¸ƒå¼è®­ç»ƒ` `è®¡ç®—æ•ˆç‡` `åŠ¨æ€è°ƒæ•´`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒä¸­å¸¸å¯¼è‡´å·¥ä½œè´Ÿè½½ä¸å‡ï¼Œå½±å“è®­ç»ƒæ•ˆç‡ã€‚
2. æå‡ºDynMoï¼Œé€šè¿‡åŠ¨æ€è´Ÿè½½å‡è¡¡æŠ€æœ¯è‡ªé€‚åº”åœ°å¹³è¡¡è®¡ç®—è´Ÿè½½ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDynMoåœ¨å¤šç§åœºæ™¯ä¸‹æ˜¾è‘—åŠ é€Ÿäº†åŠ¨æ€GPTæ¨¡å‹çš„è®­ç»ƒï¼Œæå‡å¹…åº¦é«˜è¾¾4.52å€ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œå·²æœ‰å¤šç§æ–¹æ³•è¢«æå‡ºï¼ŒåŒ…æ‹¬ä¸“å®¶æ··åˆæ¨¡å‹ã€é€æ­¥å‰ªæã€åŠ¨æ€å†»ç»“å±‚ã€åŠ¨æ€ç¨€ç–æ³¨æ„æœºåˆ¶ã€æ—©æœŸé€€å‡ºå’Œæ·±åº¦æ··åˆç­‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒä¸­å¸¸å¸¸å¯¼è‡´å·¥ä½œè´Ÿè½½ä¸å‡ï¼Œé™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªä¸»åŠ¨æ€è´Ÿè½½å‡è¡¡è§£å†³æ–¹æ¡ˆDynMoï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å·¥ä½œè´Ÿè½½ä¸å‡ï¼Œå¹¶åœ¨ç®¡é“å¹¶è¡Œè®­ç»ƒä¸­è‡ªé€‚åº”åœ°å¹³è¡¡è®¡ç®—è´Ÿè½½ã€‚DynMoæ”¯æŒå•èŠ‚ç‚¹å¤šGPUç³»ç»Ÿå’Œå¤šèŠ‚ç‚¹GPUé›†ç¾¤ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²è®­ç»ƒååé‡çš„æƒ…å†µä¸‹ï¼Œå°†è®¡ç®—åŠ¨æ€æ•´åˆåˆ°æ›´å°‘çš„å·¥ä½œèŠ‚ç‚¹ä¸Šã€‚ä¸é™æ€åˆ†å¸ƒå¼è®­ç»ƒè§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒDynMoåœ¨å¤šç§æƒ…å†µä¸‹æ˜¾è‘—åŠ é€Ÿäº†åŠ¨æ€GPTæ¨¡å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„å·¥ä½œè´Ÿè½½ä¸å‡é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒä¸­æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥åº”ç”¨äºå®é™…åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šDynMoé€šè¿‡åŠ¨æ€è´Ÿè½½å‡è¡¡æŠ€æœ¯ï¼Œè‡ªåŠ¨è°ƒæ•´è®¡ç®—è´Ÿè½½ï¼Œç¡®ä¿å„å·¥ä½œèŠ‚ç‚¹çš„è®¡ç®—é‡å‡è¡¡ï¼Œä»è€Œæå‡æ•´ä½“è®­ç»ƒæ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šDynMoçš„æ•´ä½“æ¶æ„åŒ…æ‹¬è´Ÿè½½ç›‘æµ‹æ¨¡å—ã€åŠ¨æ€è°ƒæ•´æ¨¡å—å’Œä»»åŠ¡åˆ†é…æ¨¡å—ï¼Œèƒ½å¤Ÿå®æ—¶ç›‘æ§å„èŠ‚ç‚¹çš„è´Ÿè½½æƒ…å†µï¼Œå¹¶æ ¹æ®éœ€è¦è°ƒæ•´è®¡ç®—ä»»åŠ¡çš„åˆ†é…ã€‚

**å…³é”®åˆ›æ–°**ï¼šDynMoçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªä¸»åŠ¨æ€è´Ÿè½½å‡è¡¡èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²è®­ç»ƒååé‡çš„æƒ…å†µä¸‹ï¼Œå°†è®¡ç®—æ•´åˆåˆ°æ›´å°‘çš„å·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œä¸ä¼ ç»Ÿé™æ€æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šDynMoåœ¨è®¾è®¡ä¸­è€ƒè™‘äº†å¤šç§å‚æ•°è®¾ç½®ï¼ŒåŒ…æ‹¬è´Ÿè½½ç›‘æµ‹é¢‘ç‡ã€ä»»åŠ¡åˆ†é…ç­–ç•¥ç­‰ï¼Œç¡®ä¿åœ¨ä¸åŒè®­ç»ƒåœºæ™¯ä¸‹å‡èƒ½é«˜æ•ˆè¿è¡Œã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒDynMoåœ¨å¤šç§åŠ¨æ€LLMè®­ç»ƒåœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†è®­ç»ƒé€Ÿåº¦ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨ä¸“å®¶æ··åˆæ¨¡å‹ä¸­åŠ é€Ÿ1.23å€ï¼Œå‚æ•°å‰ªæåŠ é€Ÿ3.18å€ï¼Œå±‚å†»ç»“åŠ é€Ÿ2.23å€ï¼Œç¨€ç–æ³¨æ„åŠ é€Ÿ4.02å€ï¼Œæ—©æœŸé€€å‡ºåŠ é€Ÿ4.52å€ï¼Œæ·±åº¦æ··åˆåŠ é€Ÿ1.17å€ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆè®¡ç®—èµ„æºç®¡ç†çš„åœºæ™¯ä¸­ï¼Œå¦‚äº‘è®¡ç®—å¹³å°å’Œé«˜æ€§èƒ½è®¡ç®—é›†ç¾¤ã€‚DynMoçš„åŠ¨æ€è´Ÿè½½å‡è¡¡èƒ½åŠ›å°†ä¸ºå®é™…åº”ç”¨æä¾›æ›´é«˜çš„è®­ç»ƒæ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ï¼Œæ¨åŠ¨AIæ¨¡å‹çš„å¿«é€Ÿè¿­ä»£å’Œåº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> To reduce the computational and memory overhead of Large Language Models, various approaches have been proposed. These include a) Mixture of Experts (MoEs), where token routing affects compute balance; b) gradual pruning of model parameters; c) dynamically freezing layers; d) dynamic sparse attention mechanisms; e) early exit of tokens as they pass through model layers; and f) Mixture of Depths (MoDs), where tokens bypass certain blocks. While these approaches are effective in reducing overall computation, they often introduce significant workload imbalance across workers. In many cases, this imbalance is severe enough to render the techniques impractical for large-scale distributed training, limiting their applicability to toy models due to poor efficiency.
>   We propose an autonomous dynamic load balancing solution, DynMo, which provably achieves maximum reduction in workload imbalance and adaptively equalizes compute loads across workers in pipeline-parallel training. In addition, DynMo dynamically consolidates computation onto fewer workers without sacrificing training throughput, allowing idle workers to be released back to the job manager. DynMo supports both single-node multi-GPU systems and multi-node GPU clusters, and can be used in practical deployment. Compared to static distributed training solutions such as Megatron-LM and DeepSpeed, DynMo accelerates the end-to-end training of dynamic GPT models by up to 1.23x for MoEs, 3.18x for parameter pruning, 2.23x for layer freezing, 4.02x for sparse attention, 4.52x for early exit, and 1.17x for MoDs.

