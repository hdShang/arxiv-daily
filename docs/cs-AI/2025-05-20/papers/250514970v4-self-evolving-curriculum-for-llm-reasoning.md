---
layout: default
title: Self-Evolving Curriculum for LLM Reasoning
---

# Self-Evolving Curriculum for LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14970" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14970v4</a>
  <a href="https://arxiv.org/pdf/2505.14970.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14970v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14970v4', 'Self-Evolving Curriculum for LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre PichÃ©, Nicolas Gontier, Yoshua Bengio, Ehsan Kamalloo

**åˆ†ç±»**: cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-30)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè‡ªæ¼”åŒ–è¯¾ç¨‹ä»¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¸€ï¼šæœºå™¨äººæ§åˆ¶ (Robot Control)** **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªæ¼”åŒ–è¯¾ç¨‹` `å¼ºåŒ–å­¦ä¹ ` `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†èƒ½åŠ›` `è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ` `å¤šè‡‚èµŒåšæœº` `æŠ€èƒ½å¹³è¡¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„éšæœºè¯¾ç¨‹å’Œæ‰‹åŠ¨è®¾è®¡è¯¾ç¨‹åœ¨å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹æ—¶æ•ˆæœä¸ä½³ï¼Œç¼ºä¹è‡ªåŠ¨åŒ–å’Œä¼˜åŒ–ã€‚
2. æå‡ºè‡ªæ¼”åŒ–è¯¾ç¨‹ï¼ˆSECï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸RLå¾®è°ƒè¿‡ç¨‹å¹¶è¡Œå­¦ä¹ è¯¾ç¨‹ç­–ç•¥ï¼Œä¼˜åŒ–é—®é¢˜å‘ˆç°é¡ºåºã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒSECåœ¨è§„åˆ’ã€å½’çº³æ¨ç†å’Œæ•°å­¦ç­‰é¢†åŸŸæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢å·²è¢«è¯æ˜æœ‰æ•ˆï¼Œæ˜¾è‘—æå‡äº†å…¶åœ¨æ•°å­¦å’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚å½±å“RLå¾®è°ƒæˆåŠŸçš„å…³é”®å› ç´ æ˜¯è®­ç»ƒè¯¾ç¨‹ï¼Œå³è®­ç»ƒé—®é¢˜çš„å‘ˆç°é¡ºåºã€‚éšæœºè¯¾ç¨‹ä½œä¸ºå¸¸è§åŸºçº¿ï¼Œæ•ˆæœä¸ä½³ï¼›æ‰‹åŠ¨è®¾è®¡çš„è¯¾ç¨‹å¾€å¾€ä¾èµ–å¯å‘å¼æ–¹æ³•ï¼Œè€Œåœ¨çº¿è¿‡æ»¤æ–¹æ³•è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºè‡ªæ¼”åŒ–è¯¾ç¨‹ï¼ˆSECï¼‰ï¼Œä¸€ç§è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œä¸RLå¾®è°ƒè¿‡ç¨‹åŒæ—¶å­¦ä¹ è¯¾ç¨‹ç­–ç•¥ã€‚æˆ‘ä»¬å°†è¯¾ç¨‹é€‰æ‹©è§†ä¸ºéå¹³ç¨³çš„å¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œå°†æ¯ä¸ªé—®é¢˜ç±»åˆ«è§†ä¸ºå•ç‹¬çš„è‡‚ã€‚é€šè¿‡ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç»å¯¹ä¼˜åŠ¿ä½œä¸ºå³æ—¶å­¦ä¹ å¢ç›Šçš„ä»£ç†åº¦é‡ï¼ŒSECåœ¨ä¸‰ä¸ªä¸åŒæ¨ç†é¢†åŸŸçš„å®éªŒä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæ¨ç†é¢†åŸŸçš„å¾®è°ƒä¸­å®ç°äº†æ›´å¥½çš„æŠ€èƒ½å¹³è¡¡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­è¯¾ç¨‹è®¾è®¡çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¦‚éšæœºè¯¾ç¨‹å’Œæ‰‹åŠ¨è®¾è®¡è¯¾ç¨‹æ•ˆç‡ä½ä¸‹ï¼Œç¼ºä¹è‡ªé€‚åº”æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè‡ªæ¼”åŒ–è¯¾ç¨‹ï¼ˆSECï¼‰é€šè¿‡å°†è¯¾ç¨‹é€‰æ‹©è§†ä¸ºéå¹³ç¨³çš„å¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜çš„è¯¾ç¨‹ç­–ç•¥ï¼Œä»¥æå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSECæ–¹æ³•åŒ…æ‹¬è¯¾ç¨‹é€‰æ‹©æ¨¡å—å’ŒRLå¾®è°ƒæ¨¡å—ï¼Œè¯¾ç¨‹é€‰æ‹©æ¨¡å—æ ¹æ®å½“å‰æ¨¡å‹çŠ¶æ€åŠ¨æ€é€‰æ‹©é—®é¢˜ç±»åˆ«ï¼Œè€ŒRLå¾®è°ƒæ¨¡å—åˆ™æ ¹æ®é€‰æ‹©çš„è¯¾ç¨‹è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šSECçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è¯¾ç¨‹é€‰æ‹©ä¸RLå¾®è°ƒè¿‡ç¨‹ç»“åˆï¼Œé€šè¿‡ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç»å¯¹ä¼˜åŠ¿æ¥è¯„ä¼°å­¦ä¹ å¢ç›Šï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°ä¸­ï¼ŒSECä½¿ç”¨TD(0)æ–¹æ³•æ›´æ–°è¯¾ç¨‹ç­–ç•¥ï¼Œé€‰æ‹©é—®é¢˜ç±»åˆ«ä»¥æœ€å¤§åŒ–å³æ—¶å¥–åŠ±ä¿¡å·ï¼ŒåŒæ—¶åœ¨å¤šä¸ªæ¨ç†é¢†åŸŸè¿›è¡Œå¾®è°ƒä»¥å®ç°æŠ€èƒ½å¹³è¡¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSECåœ¨ä¸‰ä¸ªæ¨ç†é¢†åŸŸçš„æ¨¡å‹è¡¨ç°æ˜¾è‘—ä¼˜äºéšæœºè¯¾ç¨‹å’Œæ‰‹åŠ¨è®¾è®¡è¯¾ç¨‹ï¼Œå°¤å…¶åœ¨å¤„ç†æ›´éš¾çš„ã€è¶…å‡ºåˆ†å¸ƒçš„æµ‹è¯•é—®é¢˜æ—¶ï¼Œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå…·ä½“æå‡å¹…åº¦æœªçŸ¥ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿå’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒSECå¯ä»¥åœ¨å¤æ‚é—®é¢˜æ±‚è§£ã€ç¼–ç¨‹è¾…åŠ©å’Œå†³ç­–æ”¯æŒç­‰åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ä¸å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.

