---
layout: default
title: Reasoning Models Better Express Their Confidence
---

# Reasoning Models Better Express Their Confidence

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14489" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14489v2</a>
  <a href="https://arxiv.org/pdf/2505.14489.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14489v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14489v2', 'Reasoning Models Better Express Their Confidence')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, Minjoon Seo

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-22)

**å¤‡æ³¨**: Accepted to NeurIPS 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ¨ç†æ¨¡å‹ä»¥æé«˜ä¿¡å¿ƒè¡¨è¾¾çš„å‡†ç¡®æ€§**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä¿¡å¿ƒè¡¨è¾¾` `æ¨ç†æ¨¡å‹` `é“¾å¼æ€ç»´` `æ…¢æ€ç»´è¡Œä¸º` `ä¿¡å¿ƒæ ¡å‡†` `æœºå™¨å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨è¾¾ä¿¡å¿ƒæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥è¯„ä¼°å…¶é”™è¯¯å¯èƒ½æ€§ï¼Œå½±å“å¯é æ€§ã€‚
2. æœ¬ç ”ç©¶æå‡ºé€šè¿‡æ‰©å±•é“¾å¼æ€ç»´æ¨ç†æ¥æå‡æ¨¡å‹çš„ä¿¡å¿ƒè¡¨è¾¾èƒ½åŠ›ï¼Œå¼ºè°ƒæ…¢æ€ç»´è¡Œä¸ºçš„é‡è¦æ€§ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹åœ¨33ä¸ªè®¾ç½®ä¸­è¡¨ç°å‡ºæ›´å¥½çš„ä¿¡å¿ƒæ ¡å‡†ï¼Œä¸”æ…¢æ€ç»´è¡Œä¸ºæ˜¯æå‡çš„å…³é”®å› ç´ ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬åœ¨å‡†ç¡®è¡¨è¾¾ä¿¡å¿ƒæ–¹é¢å¸¸å¸¸å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´éš¾ä»¥è¯„ä¼°å…¶é”™è¯¯çš„å¯èƒ½æ€§ï¼Œä»è€Œé™åˆ¶äº†å…¶å¯é æ€§ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨æ‰©å±•é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†çš„æ¨¡å‹åœ¨è§£å†³é—®é¢˜å’Œå‡†ç¡®è¡¨è¾¾ä¿¡å¿ƒæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚æˆ‘ä»¬å¯¹å…­ç§æ¨ç†æ¨¡å‹åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å®ƒä»¬åœ¨36ç§è®¾ç½®ä¸­æœ‰33ç§æƒ…å†µä¸‹çš„ä¿¡å¿ƒæ ¡å‡†ä¼˜äºéæ¨ç†æ¨¡å‹ã€‚è¯¦ç»†åˆ†ææ˜¾ç¤ºï¼Œè¿™ç§æ ¡å‡†çš„æå‡æºäºæ¨ç†æ¨¡å‹çš„æ…¢æ€ç»´è¡Œä¸ºï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨CoTè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ä¿¡å¿ƒï¼Œé€æ­¥æé«˜å‡†ç¡®æ€§ã€‚å°¤å…¶æ˜¯ï¼Œæ¨ç†æ¨¡å‹åœ¨CoTå±•å¼€è¿‡ç¨‹ä¸­æ ¡å‡†æ•ˆæœé€æ¸æå‡ï¼Œè€Œéæ¨ç†æ¨¡å‹åˆ™æœªè§‚å¯Ÿåˆ°æ­¤è¶‹åŠ¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿¡å¿ƒè¡¨è¾¾ä¸Šçš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®è¯„ä¼°æ¨¡å‹çš„é”™è¯¯æ¦‚ç‡ï¼Œé™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡å¼•å…¥æ‰©å±•é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œåˆ©ç”¨æ…¢æ€ç»´è¡Œä¸ºæ¥åŠ¨æ€è°ƒæ•´æ¨¡å‹çš„ä¿¡å¿ƒï¼Œä»è€Œæé«˜ä¿¡å¿ƒè¡¨è¾¾çš„å‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€æ¨ç†æ¨¡å‹çš„è®¾è®¡ä¸è®­ç»ƒã€ä¿¡å¿ƒæ ¡å‡†çš„è¯„ä¼°ç­‰å¤šä¸ªé˜¶æ®µï¼Œé‡ç‚¹åœ¨äºæ…¢æ€ç»´è¡Œä¸ºçš„å¼•å…¥ä¸åˆ†æã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºè¯æ˜äº†æ¨ç†æ¨¡å‹åœ¨ä¿¡å¿ƒæ ¡å‡†æ–¹é¢çš„æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨CoTå±•å¼€è¿‡ç¨‹ä¸­ï¼Œé€æ­¥æé«˜ä¿¡å¿ƒçš„å‡†ç¡®æ€§ï¼Œè¿™æ˜¯éæ¨ç†æ¨¡å‹æ‰€ä¸å…·å¤‡çš„ç‰¹æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥æ”¯æŒæ…¢æ€ç»´è¡Œä¸ºçš„å®ç°ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å…·ä½“ç»†èŠ‚åŒ…æ‹¬æ¨¡å‹çš„è®­ç»ƒç­–ç•¥å’Œè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨ç†æ¨¡å‹åœ¨36ç§è®¾ç½®ä¸­æœ‰33ç§æƒ…å†µä¸‹çš„ä¿¡å¿ƒæ ¡å‡†ä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œæå‡å¹…åº¦æ˜¾è‘—ã€‚å…·ä½“è€Œè¨€ï¼Œå»é™¤æ…¢æ€ç»´è¡Œä¸ºåï¼Œæ ¡å‡†æ•ˆæœæ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜æ…¢æ€ç»´æ˜¯æå‡ä¿¡å¿ƒè¡¨è¾¾çš„å…³é”®å› ç´ ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒå’Œäººæœºäº¤äº’ç­‰ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„ä¿¡å¿ƒè¡¨è¾¾èƒ½åŠ›ï¼Œå¯ä»¥å¢å¼ºç”¨æˆ·å¯¹æ¨¡å‹è¾“å‡ºçš„ä¿¡ä»»åº¦ï¼Œè¿›è€Œæå‡å®é™…åº”ç”¨çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¹¿æ³›çš„AIç³»ç»Ÿä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„å¯ä¿¡èµ–æ€§å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models that engage in extended chain-of-thought (CoT) reasoning exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models (e.g., exploring alternative approaches and backtracking) which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that non-reasoning models also demonstrate enhanced calibration when simply guided to slow think via in-context learning, fully isolating slow thinking as the source of the calibration gains.

