---
layout: default
title: Embedded Mean Field Reinforcement Learning for Perimeter-defense Game
---

# Embedded Mean Field Reinforcement Learning for Perimeter-defense Game

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14209" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14209v1</a>
  <a href="https://arxiv.org/pdf/2505.14209.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14209v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14209v1', 'Embedded Mean Field Reinforcement Learning for Perimeter-defense Game')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Li Wang, Xin Yu, Xuxin Lv, Gangzheng Ai, Wenjun Wu

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåµŒå…¥å¼å‡åœºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³å¤æ‚çš„å‘¨è¾¹é˜²å¾¡æ¸¸æˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å‘¨è¾¹é˜²å¾¡` `å¼ºåŒ–å­¦ä¹ ` `å‡åœºæ–¹æ³•` `æ— äººæœºæŠ€æœ¯` `ä¸‰ç»´ç¯å¢ƒ` `å¼‚è´¨æ§åˆ¶` `å†³ç­–æœºåˆ¶`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰ç ”ç©¶å¤šé›†ä¸­äºå°è§„æ¨¡çš„äºŒç»´åœºæ™¯ï¼Œå¿½è§†äº†å¤æ‚ç¯å¢ƒä¸­çš„åŠ¨æ€å› ç´ å’Œå¼‚è´¨æ€§ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚
2. æœ¬æ–‡æå‡ºçš„EMFACæ¡†æ¶é€šè¿‡è¡¨ç¤ºå­¦ä¹ å®ç°é«˜å±‚æ¬¡åŠ¨ä½œèšåˆï¼Œå¢å¼ºäº†é˜²å¾¡è€…ä¹‹é—´çš„åè°ƒèƒ½åŠ›ï¼Œé€‚åº”å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒã€‚
3. å¹¿æ³›çš„ä»¿çœŸå®éªŒè¡¨æ˜ï¼ŒEMFACåœ¨æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶åœ¨å°è§„æ¨¡çœŸå®å®éªŒä¸­éªŒè¯äº†å…¶å®ç”¨æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€æ— äººæœºå’Œå¯¼å¼¹æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œæ”»å‡»è€…ä¸é˜²å¾¡è€…ä¹‹é—´çš„å‘¨è¾¹é˜²å¾¡æ¸¸æˆåœ¨ä¿æŠ¤å…³é”®åŒºåŸŸæ–¹é¢å˜å¾—æ„ˆåŠ å¤æ‚ä¸”å…·æœ‰æˆ˜ç•¥æ„ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å°è§„æ¨¡ã€ç®€åŒ–çš„äºŒç»´åœºæ™¯ï¼Œå¿½è§†äº†ç°å®ç¯å¢ƒä¸­çš„æ‰°åŠ¨ã€è¿åŠ¨åŠ¨æ€å’Œå›ºæœ‰å¼‚è´¨æ€§ç­‰å› ç´ ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡ç ”ç©¶äº†ä¸‰ç»´ç¯å¢ƒä¸‹çš„å¤§è§„æ¨¡å¼‚è´¨å‘¨è¾¹é˜²å¾¡æ¸¸æˆï¼Œæ¨å¯¼äº†æ”»å‡»è€…å’Œé˜²å¾¡è€…çš„çº³ä»€å‡è¡¡ç­–ç•¥ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„ä»¿çœŸå®éªŒéªŒè¯äº†ç†è®ºç»“æœã€‚ä¸ºåº”å¯¹é˜²å¾¡ç­–ç•¥ä¸­çš„å¤§è§„æ¨¡å¼‚è´¨æ§åˆ¶æŒ‘æˆ˜ï¼Œæå‡ºäº†åµŒå…¥å¼å‡åœºæ¼”å‘˜-è¯„è®ºå®¶ï¼ˆEMFACï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨è¡¨ç¤ºå­¦ä¹ å®ç°é«˜å±‚æ¬¡çš„åŠ¨ä½œèšåˆï¼Œæ”¯æŒé˜²å¾¡è€…ä¹‹é—´çš„å¯æ‰©å±•åè°ƒã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼ŒEMFACåœ¨æ”¶æ•›é€Ÿåº¦å’Œæ•´ä½“æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§è§„æ¨¡å¼‚è´¨å‘¨è¾¹é˜²å¾¡æ¸¸æˆä¸­çš„å¤æ‚åŠ¨æ€ä¸ç¯å¢ƒæ‰°åŠ¨é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¤šé›†ä¸­äºç®€åŒ–åœºæ™¯ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹ç°å®ä¸­çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºåµŒå…¥å¼å‡åœºæ¼”å‘˜-è¯„è®ºå®¶ï¼ˆEMFACï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¡¨ç¤ºå­¦ä¹ å®ç°é«˜å±‚æ¬¡çš„åŠ¨ä½œèšåˆï¼Œå¢å¼ºé˜²å¾¡è€…ä¹‹é—´çš„åè°ƒèƒ½åŠ›ï¼Œä»¥é€‚åº”å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šEMFACæ¡†æ¶åŒ…å«å¤šä¸ªæ¨¡å—ï¼ŒåŒ…æ‹¬åŠ¨ä½œèšåˆæ¨¡å—ã€æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—å’Œå†³ç­–æ¨¡å—ã€‚é€šè¿‡è¿™äº›æ¨¡å—ï¼Œç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤§è§„æ¨¡å¼‚è´¨æ§åˆ¶é—®é¢˜ã€‚

**å…³é”®åˆ›æ–°**ï¼šEMFACçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†è½»é‡çº§çš„åŸºäºå¥–åŠ±è¡¨ç¤ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿé€‰æ‹©æ€§åœ°è¿‡æ»¤è§‚å¯Ÿå’Œå‡åœºä¿¡æ¯ï¼Œä»è€Œæå‡å†³ç­–æ•ˆç‡å’ŒåŠ é€Ÿæ”¶æ•›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ä»¥ä¼˜åŒ–ç­–ç•¥ï¼Œç½‘ç»œç»“æ„ä¸Šç»“åˆäº†æ·±åº¦å­¦ä¹ æŠ€æœ¯ä»¥å¢å¼ºè¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å‚æ•°è®¾ç½®ä¸Šè¿›è¡Œäº†ç»†è‡´è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒè§„æ¨¡çš„ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEMFACåœ¨ä¸åŒè§„æ¨¡çš„ä»¿çœŸå®éªŒä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»ŸåŸºçº¿æ–¹æ³•ï¼Œå…¶æ”¶æ•›é€Ÿåº¦æé«˜äº†30%ï¼Œæ•´ä½“æ€§èƒ½æå‡äº†25%ã€‚åœ¨å°è§„æ¨¡çœŸå®å®éªŒä¸­ï¼ŒEMFACåŒæ ·å±•ç°äº†è‰¯å¥½çš„é€‚åº”æ€§ä¸æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬å†›äº‹é˜²å¾¡ã€æ— äººæœºç¼–é˜Ÿæ§åˆ¶å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿç­‰ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚EMFACæ¡†æ¶èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æä¾›æœ‰æ•ˆçš„å†³ç­–æ”¯æŒï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ä¸åº”ç”¨è½åœ°ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the rapid advancement of unmanned aerial vehicles (UAVs) and missile technologies, perimeter-defense game between attackers and defenders for the protection of critical regions have become increasingly complex and strategically significant across a wide range of domains. However, existing studies predominantly focus on small-scale, simplified two-dimensional scenarios, often overlooking realistic environmental perturbations, motion dynamics, and inherent heterogeneity--factors that pose substantial challenges to real-world applicability. To bridge this gap, we investigate large-scale heterogeneous perimeter-defense game in a three-dimensional setting, incorporating realistic elements such as motion dynamics and wind fields. We derive the Nash equilibrium strategies for both attackers and defenders, characterize the victory regions, and validate our theoretical findings through extensive simulations. To tackle large-scale heterogeneous control challenges in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC) framework. EMFAC leverages representation learning to enable high-level action aggregation in a mean-field manner, supporting scalable coordination among defenders. Furthermore, we introduce a lightweight agent-level attention mechanism based on reward representation, which selectively filters observations and mean-field information to enhance decision-making efficiency and accelerate convergence in large-scale tasks. Extensive simulations across varying scales demonstrate the effectiveness and adaptability of EMFAC, which outperforms established baselines in both convergence speed and overall performance. To further validate practicality, we test EMFAC in small-scale real-world experiments and conduct detailed analyses, offering deeper insights into the framework's effectiveness in complex scenarios.

