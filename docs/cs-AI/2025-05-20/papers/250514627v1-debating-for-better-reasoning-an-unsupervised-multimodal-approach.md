---
layout: default
title: Debating for Better Reasoning: An Unsupervised Multimodal Approach
---

# Debating for Better Reasoning: An Unsupervised Multimodal Approach

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14627" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14627v1</a>
  <a href="https://arxiv.org/pdf/2505.14627.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14627v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14627v1', 'Debating for Better Reasoning: An Unsupervised Multimodal Approach')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ashutosh Adhikari, Mirella Lapata

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¤šæ¨¡æ€è¾©è®ºæ¡†æ¶ä»¥æå‡è§†è§‰é—®ç­”æ€§èƒ½**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤šæ¨¡æ€å­¦ä¹ ` `è§†è§‰é—®ç­”` `è¾©è®ºæœºåˆ¶` `æ¨ç†èƒ½åŠ›` `æ¨¡å‹ç›‘ç£` `ä¸“å®¶æ¨¡å‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ç›‘ç£æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è¾©è®ºæ¡†æ¶ï¼Œé€šè¿‡ä¸“å®¶æ¨¡å‹ä¹‹é—´çš„è¾©è®ºæ¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘äº†è§’è‰²æ‰®æ¼”çš„éœ€æ±‚ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå•ä¸€æ¨¡å‹ï¼Œä¸”å¼±æ¨¡å‹çš„åˆ¤æ–­èƒ½æœ‰æ•ˆæå‡æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„é¢†åŸŸå’Œæ¨¡æ€ä¸­çš„ä¸“ä¸šèƒ½åŠ›ä¸æ–­æå‡ï¼Œå¦‚ä½•è¿›è¡Œå¯æ‰©å±•çš„ç›‘ç£å˜å¾—æ„ˆå‘å›°éš¾ï¼Œå°¤å…¶æ˜¯å½“è¿™äº›æ¨¡å‹çš„èƒ½åŠ›å¯èƒ½è¶…è¶Šäººç±»è¯„ä¼°è€…æ—¶ã€‚è¾©è®ºä½œä¸ºä¸€ç§æœ‰æ•ˆçš„ç›‘ç£æœºåˆ¶é€æ¸å—åˆ°å…³æ³¨ã€‚æœ¬æ–‡å°†è¾©è®ºèŒƒå¼æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒï¼Œæ¢ç´¢å…¶åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå…¶ä¸­ä¸¤ä½â€œæœ‰è§†åŠ›â€çš„ä¸“å®¶è§†è§‰-è¯­è¨€æ¨¡å‹å°±ç­”æ¡ˆè¿›è¡Œè¾©è®ºï¼Œè€Œä¸€ä½â€œç›²â€çš„ï¼ˆä»…æ–‡æœ¬ï¼‰è¯„åˆ¤è€…åˆ™åŸºäºè®ºç‚¹è´¨é‡è¿›è¡Œè£å†³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è¾©è®ºæ¡†æ¶åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºå•ä¸€ä¸“å®¶æ¨¡å‹ï¼Œå¹¶ä¸”è¾ƒå¼±çš„LLMsçš„åˆ¤æ–­å¯ä»¥é€šè¿‡å¾®è°ƒå¸®åŠ©å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å¦‚ä½•æœ‰æ•ˆç›‘ç£å’Œæå‡æ¨¡å‹æ€§èƒ½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„è§†è§‰é—®ç­”ä»»åŠ¡æ—¶ï¼Œå¾€å¾€ç¼ºä¹æœ‰æ•ˆçš„ç›‘ç£æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡è¾©è®ºæœºåˆ¶ï¼Œè®©ä¸¤ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹å°±ç­”æ¡ˆè¿›è¡Œäº‰è®ºï¼Œè€Œç”±ä¸€ä¸ªæ–‡æœ¬æ¨¡å‹è¿›è¡Œè£å†³ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨åˆ©ç”¨ä¸“å®¶ä¹‹é—´çš„æ„è§åˆ†æ­§ï¼Œé›†ä¸­è®¨è®ºäº‰è®®ç‚¹ï¼Œä»è€Œæå‡æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä¸¤ä¸ªâ€œæœ‰è§†åŠ›â€çš„ä¸“å®¶æ¨¡å‹è´Ÿè´£æå‡ºå’Œè¾©è®ºç­”æ¡ˆï¼Œä¸€ä¸ªâ€œç›²â€çš„è¯„åˆ¤è€…è´Ÿè´£æ ¹æ®è®ºç‚¹è´¨é‡è¿›è¡Œè£å†³ã€‚ä¸“å®¶æ¨¡å‹ä»…è¾©æŠ¤ä¸å…¶ä¿¡å¿µä¸€è‡´çš„ç­”æ¡ˆï¼Œé¿å…äº†è§’è‰²æ‰®æ¼”çš„å¤æ‚æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå°†è¾©è®ºæœºåˆ¶å¼•å…¥å¤šæ¨¡æ€ä»»åŠ¡ï¼Œåˆ©ç”¨ä¸“å®¶æ¨¡å‹ä¹‹é—´çš„äº‰è®ºæ¥æå‡æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•æœ¬è´¨ä¸Šä¸åŒï¼Œåè€…é€šå¸¸ä¾èµ–äºå•ä¸€æ¨¡å‹çš„è¾“å‡ºã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼Œä¸“å®¶æ¨¡å‹çš„é€‰æ‹©å’Œè®­ç»ƒç­–ç•¥è‡³å…³é‡è¦ã€‚æŸå¤±å‡½æ•°çš„è®¾è®¡éœ€è€ƒè™‘è¾©è®ºçš„è´¨é‡å’Œè£å†³çš„å‡†ç¡®æ€§ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ åˆ°æœ‰ä»·å€¼çš„æ¨ç†ä¿¡æ¯ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œè¾©è®ºæ¡†æ¶åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸­å‡ä¼˜äºå•ä¸€ä¸“å®¶æ¨¡å‹ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­æ€§èƒ½æå‡è¾¾15%ã€‚æ­¤å¤–ï¼Œè¾ƒå¼±çš„LLMsåœ¨å¾®è°ƒåèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€è‡ªåŠ¨åŒ–å®¢æœã€æ•™è‚²è¾…å¯¼ç­‰ã€‚é€šè¿‡æå‡è§†è§‰é—®ç­”çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†å’Œäººæœºäº¤äº’ä¸­å®ç°æ›´é«˜æ•ˆçš„åº”ç”¨ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) gain expertise across diverse domains and modalities, scalable oversight becomes increasingly challenging, particularly when their capabilities may surpass human evaluators. Debate has emerged as a promising mechanism for enabling such oversight. In this work, we extend the debate paradigm to a multimodal setting, exploring its potential for weaker models to supervise and enhance the performance of stronger models. We focus on visual question answering (VQA), where two "sighted" expert vision-language models debate an answer, while a "blind" (text-only) judge adjudicates based solely on the quality of the arguments. In our framework, the experts defend only answers aligned with their beliefs, thereby obviating the need for explicit role-playing and concentrating the debate on instances of expert disagreement. Experiments on several multimodal tasks demonstrate that the debate framework consistently outperforms individual expert models. Moreover, judgments from weaker LLMs can help instill reasoning capabilities in vision-language models through finetuning.

