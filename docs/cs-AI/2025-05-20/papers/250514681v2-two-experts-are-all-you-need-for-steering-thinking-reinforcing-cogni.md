---
layout: default
title: "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training"
---

# Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14681" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14681v2</a>
  <a href="https://arxiv.org/pdf/2505.14681.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14681v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14681v2', 'Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu

**åˆ†ç±»**: cs.AI, cs.CL, cs.CV, cs.IR, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-05-27)

**å¤‡æ³¨**: Work in progress

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRICEæ–¹æ³•ä»¥è§£å†³MoEæ¨ç†æ¨¡å‹ä¸­çš„è®¤çŸ¥æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ··åˆä¸“å®¶` `æ¨ç†æ¨¡å‹` `è®¤çŸ¥æ•ˆç‡` `å½’ä¸€åŒ–ç‚¹äº’ä¿¡æ¯` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ™ºèƒ½é—®ç­”` `å†³ç­–æ”¯æŒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†æ¨¡å‹åœ¨è®¤çŸ¥æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“å‡ºç°è¿‡åº¦æ€è€ƒå’Œä¸è¶³æ€è€ƒçš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºçš„RICEæ–¹æ³•é€šè¿‡å¼•å¯¼è®¤çŸ¥ä¸“å®¶ï¼Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚çš„å¯å‘å¼è®¾è®¡ã€‚
3. å®éªŒè¯æ˜ï¼ŒRICEåœ¨DeepSeek-R1å’ŒQwen3-235Bç­‰æ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ¨ç†å‡†ç¡®æ€§å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„åœ¨å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­é€šè¿‡é€‰æ‹©æ€§æ¿€æ´»ä¸“å®¶æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¸¸é¢ä¸´è®¤çŸ¥ä½æ•ˆçš„é—®é¢˜ï¼Œå¦‚è¿‡åº¦æ€è€ƒå’Œä¸è¶³æ€è€ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ—¶é—´å¼•å¯¼æ–¹æ³•â€”â€”å¼ºåŒ–è®¤çŸ¥ä¸“å®¶ï¼ˆRICEï¼‰ï¼Œæ—¨åœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚å¯å‘å¼çš„æƒ…å†µä¸‹æå‡æ¨ç†æ€§èƒ½ã€‚é€šè¿‡å½’ä¸€åŒ–ç‚¹äº’ä¿¡æ¯ï¼ˆnPMIï¼‰ï¼Œç³»ç»Ÿè¯†åˆ«å‡ºä¸“é—¨çš„â€œè®¤çŸ¥ä¸“å®¶â€ï¼Œä»¥åè°ƒä»¥â€œ<think>â€ç­‰æ ‡è®°ä¸ºç‰¹å¾çš„å…ƒçº§æ¨ç†æ“ä½œã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†å‡†ç¡®æ€§ã€è®¤çŸ¥æ•ˆç‡å’Œè·¨é¢†åŸŸæ³›åŒ–æ–¹é¢æ˜¾è‘—æå‡ï¼Œä¸”ä¼˜äºç°æœ‰çš„æ¨ç†å¼•å¯¼æŠ€æœ¯ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ··åˆä¸“å®¶æ¨ç†æ¨¡å‹åœ¨è®¤çŸ¥æ•ˆç‡ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯è¿‡åº¦æ€è€ƒå’Œä¸è¶³æ€è€ƒçš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜å¯¼è‡´æ¨ç†æ€§èƒ½ä¸‹é™ï¼Œå½±å“æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºçš„RICEæ–¹æ³•é€šè¿‡å¼•å¯¼è®¤çŸ¥ä¸“å®¶æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œåˆ©ç”¨å½’ä¸€åŒ–ç‚¹äº’ä¿¡æ¯ï¼ˆnPMIï¼‰è¯†åˆ«å¹¶æ¿€æ´»ç‰¹å®šçš„ä¸“å®¶ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRICEæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸“å®¶è¯†åˆ«ã€æ¨ç†å¼•å¯¼å’Œç»“æœæ•´åˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡nPMIè¯†åˆ«è®¤çŸ¥ä¸“å®¶ï¼›ç„¶åï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€æ¿€æ´»è¿™äº›ä¸“å®¶ï¼›æœ€åï¼Œå°†å„ä¸“å®¶çš„è¾“å‡ºæ•´åˆä»¥ç”Ÿæˆæœ€ç»ˆç»“æœã€‚

**å…³é”®åˆ›æ–°**ï¼šRICEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å¼•å¯¼è®¤çŸ¥ä¸“å®¶æ¥å¢å¼ºæ¨ç†æ•ˆç‡ï¼Œè€Œéä¾èµ–äºä¼ ç»Ÿçš„æç¤ºè®¾è®¡æˆ–è§£ç çº¦æŸã€‚è¿™ç§æ–¹æ³•ä¸ä»…æå‡äº†æ¨ç†æ€§èƒ½ï¼Œè¿˜ä¿æŒäº†æ¨¡å‹çš„é€šç”¨æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼ŒRICEé‡‡ç”¨äº†ç‰¹å®šçš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ä¸“å®¶çš„æœ‰æ•ˆæ¿€æ´»å’Œè¾“å‡ºæ•´åˆã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“æ„è®¾è®¡ä¸Šæ³¨é‡è½»é‡åŒ–ï¼Œä»¥ä¾¿äºåœ¨æ¨ç†æ—¶å¿«é€Ÿå“åº”ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRICEæ–¹æ³•åœ¨DeepSeek-R1å’ŒQwen3-235Bæ¨¡å‹ä¸Šï¼Œæ¨ç†å‡†ç¡®æ€§æå‡äº†æ˜¾è‘—çš„X%ï¼ˆå…·ä½“æ•°æ®æœªçŸ¥ï¼‰ï¼Œè®¤çŸ¥æ•ˆç‡å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¹Ÿæœ‰æ˜æ˜¾æ”¹å–„ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ¨ç†å¼•å¯¼æŠ€æœ¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€æ™ºèƒ½é—®ç­”ç³»ç»Ÿå’Œå†³ç­–æ”¯æŒç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡æ¨ç†æ•ˆç‡ï¼ŒRICEæ–¹æ³•èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´å¿«é€Ÿã€æ›´å‡†ç¡®çš„å“åº”ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.

