---
layout: default
title: PRL: Prompts from Reinforcement Learning
---

# PRL: Prompts from Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14412" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14412v1</a>
  <a href="https://arxiv.org/pdf/2505.14412.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14412v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14412v1', 'PRL: Prompts from Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: PaweÅ‚ Batorski, Adrian Kosmala, Paul Swoboda

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/Batorskq/prl)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨æç¤ºç”Ÿæˆæ–¹æ³•PRLä»¥è§£å†³æç¤ºå·¥ç¨‹æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `æç¤ºå·¥ç¨‹` `å¼ºåŒ–å­¦ä¹ ` `è‡ªåŠ¨åŒ–ç”Ÿæˆ` `è‡ªç„¶è¯­è¨€å¤„ç†` `æ–‡æœ¬åˆ†ç±»` `æ‘˜è¦ç”Ÿæˆ` `æ–‡æœ¬ç®€åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æç¤ºå·¥ç¨‹æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸“å®¶çš„ç›´è§‰ï¼Œç¼ºä¹è‡ªåŠ¨åŒ–å’Œæ™®é€‚æ€§ï¼Œéš¾ä»¥é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚
2. PRLé€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨ç”Ÿæˆæç¤ºï¼Œèƒ½å¤Ÿåˆ›é€ å‡ºåœ¨è®­ç»ƒä¸­æœªè§çš„æ–°ç¤ºä¾‹ï¼Œä»è€Œæé«˜æç¤ºçš„å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ã€‚
3. åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒPRLæ¯”ä¹‹å‰çš„æ–¹æ³•æé«˜äº†2.58%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨æ‘˜è¦ä»»åŠ¡ä¸­æå‡äº†ROUGEåˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ‰æ•ˆçš„æç¤ºå·¥ç¨‹ä»ç„¶æ˜¯å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚å°½ç®¡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä½†å…¶æ„å»ºé€šå¸¸éœ€è¦ä¸“å®¶çš„ç›´è§‰å’Œå¯¹ä»»åŠ¡çš„ç»†è‡´ç†è§£ã€‚æ­¤å¤–ï¼Œæœ€å…·å½±å“åŠ›çš„æç¤ºå¾€å¾€ä¾èµ–äºå¾®å¦™çš„è¯­ä¹‰çº¿ç´¢ï¼Œè¿™äº›çº¿ç´¢å¯èƒ½è¶…å‡ºäººç±»çš„æ„ŸçŸ¥ï¼Œä½†å¯¹å¼•å¯¼LLMè¡Œä¸ºè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†PRLï¼ˆPrompts from Reinforcement Learningï¼‰ï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨æç¤ºç”Ÿæˆæ–°æ–¹æ³•ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒPRLèƒ½å¤Ÿç”Ÿæˆåœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ–°é¢–å°‘é‡ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨æ–‡æœ¬åˆ†ç±»ã€ç®€åŒ–å’Œæ‘˜è¦ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æç¤ºå·¥ç¨‹æ–¹æ³•çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶å¯¹ä¸“å®¶çŸ¥è¯†çš„ä¾èµ–å’Œç¼ºä¹è‡ªåŠ¨åŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•ç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œé™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPRLçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨ç”Ÿæˆæç¤ºï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹æ¥è¯†åˆ«å’Œç”Ÿæˆæœ‰æ•ˆçš„æç¤ºï¼Œä»è€Œå‡å°‘å¯¹äººå·¥è®¾è®¡çš„ä¾èµ–ã€‚è¿™æ ·çš„è®¾è®¡å¯ä»¥æé«˜æç¤ºçš„é€‚åº”æ€§å’Œå¤šæ ·æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPRLçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒå’Œæç¤ºç”Ÿæˆä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡æ”¶é›†ä»»åŠ¡ç›¸å…³çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼˜åŒ–æç¤ºç”Ÿæˆç­–ç•¥ï¼Œæœ€åç”Ÿæˆé€‚ç”¨äºç‰¹å®šä»»åŠ¡çš„æç¤ºã€‚

**å…³é”®åˆ›æ–°**ï¼šPRLçš„æœ€å¤§åˆ›æ–°åœ¨äºå…¶èƒ½å¤Ÿç”Ÿæˆåœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ–°é¢–å°‘é‡ç¤ºä¾‹ï¼Œè¿™ä¸€ç‰¹æ€§ä½¿å…¶åœ¨æç¤ºç”Ÿæˆçš„çµæ´»æ€§å’Œå¤šæ ·æ€§ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒPRLé‡‡ç”¨äº†ç‰¹å®šçš„å¥–åŠ±æœºåˆ¶æ¥è¯„ä¼°ç”Ÿæˆæç¤ºçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä½¿ç”¨äº†é€‚åˆæ–‡æœ¬ç”Ÿæˆçš„ç½‘ç»œç»“æ„ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æç¤ºèƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼LLMçš„è¡Œä¸ºã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

PRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåˆ†ç±»ä»»åŠ¡çš„å‡†ç¡®ç‡æ¯”APEæé«˜äº†2.58%ï¼Œæ¯”EvoPromptæé«˜äº†1.00%ã€‚åœ¨æ‘˜è¦ä»»åŠ¡ä¸­ï¼ŒROUGEåˆ†æ•°å¹³å‡æé«˜äº†4.32ï¼Œç®€åŒ–ä»»åŠ¡ä¸­çš„SARIåˆ†æ•°æé«˜äº†6.93ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬åˆ†ç±»ã€æ‘˜è¦ç”Ÿæˆå’Œæ–‡æœ¬ç®€åŒ–ç­‰ä»»åŠ¡ã€‚é€šè¿‡è‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆçš„æç¤ºï¼ŒPRLèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¥½åœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization. On the classification task, it surpasses prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it improves the average ROUGE scores on the summarization task by 4.32 over APE and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01 over EvoPrompt. Our code is available at https://github.com/Batorskq/prl .

