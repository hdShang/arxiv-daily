---
layout: default
title: SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation
---

# SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14381" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14381v2</a>
  <a href="https://arxiv.org/pdf/2505.14381.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14381v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14381v2', 'SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yuyang Dong, Nobuhiro Ueda, KrisztiÃ¡n Boros, Daiki Ito, Takuya Sera, Masafumi Oyamada

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-12-11)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSCANä»¥è§£å†³ä¸°å¯Œæ–‡æ¡£çš„æ£€ç´¢å¢å¼ºç”Ÿæˆé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `æ–‡æ¡£åˆ†æ` `æ£€ç´¢å¢å¼ºç”Ÿæˆ` `è§†è§‰è¯­è¨€æ¨¡å‹` `è¯­ä¹‰åˆ†æ` `ä¿¡æ¯æå–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸°å¯Œæ–‡æ¡£æ—¶é¢ä¸´ä¿¡æ¯é‡å¤§ã€ä¸Šä¸‹æ–‡ä¿ç•™ä¸å¤„ç†æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡æŒ‘æˆ˜ã€‚
2. SCANé€šè¿‡ç²—ç²’åº¦è¯­ä¹‰åˆ†æï¼Œå°†æ–‡æ¡£åˆ’åˆ†ä¸ºè¿è´¯åŒºåŸŸï¼Œä»è€Œæé«˜æ–‡æœ¬å’Œè§†è§‰RAGç³»ç»Ÿçš„æ€§èƒ½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSCANåœ¨æ–‡æœ¬RAGå’Œè§†è§‰RAGæ€§èƒ½ä¸Šåˆ†åˆ«æå‡äº†9.4åˆ†å’Œ10.4åˆ†ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œé’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œè§†è§‰RAGçš„æ–‡æ¡£åˆ†ææŠ€æœ¯å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å°½ç®¡VLMsåœ¨RAGæ€§èƒ½ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä½†å¤„ç†ä¸°å¯Œæ–‡æ¡£ä»ç„¶æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†SCANï¼ˆè¯­ä¹‰æ–‡æ¡£å¸ƒå±€åˆ†æï¼‰ï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤„ç†è§†è§‰ä¸°å¯Œæ–‡æ¡£çš„æ–‡æœ¬å’Œè§†è§‰RAGç³»ç»Ÿã€‚SCANé€šè¿‡ç²—ç²’åº¦è¯­ä¹‰æ–¹æ³•å°†æ–‡æ¡£åˆ’åˆ†ä¸ºè¿è´¯åŒºåŸŸï¼Œå¹³è¡¡äº†ä¸Šä¸‹æ–‡ä¿ç•™ä¸å¤„ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCANåœ¨è‹±è¯­å’Œæ—¥è¯­æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ–‡æœ¬RAGæ€§èƒ½ï¼ˆæœ€é«˜æå‡9.4åˆ†ï¼‰å’Œè§†è§‰RAGæ€§èƒ½ï¼ˆæœ€é«˜æå‡10.4åˆ†ï¼‰ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•å’Œå•†ä¸šæ–‡æ¡£å¤„ç†è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å¤„ç†ä¸°å¯Œæ–‡æ¡£æ—¶ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå¹³è¡¡ä¸Šä¸‹æ–‡ä¿ç•™ä¸å¤„ç†æ•ˆç‡çš„é—®é¢˜ï¼Œå¯¼è‡´RAGæ€§èƒ½å—é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šSCANé‡‡ç”¨ç²—ç²’åº¦è¯­ä¹‰åˆ†æï¼Œå°†æ–‡æ¡£åˆ’åˆ†ä¸ºè¿è´¯çš„åŒºåŸŸï¼Œç¡®ä¿åœ¨å¤„ç†æ—¶èƒ½å¤Ÿä¿ç•™å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶æé«˜å¤„ç†æ•ˆç‡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ–‡æ¡£ç»“æ„å’Œå†…å®¹ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šSCANçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ–‡æ¡£ç»„ä»¶è¯†åˆ«ã€è¯­ä¹‰åŒºåŸŸåˆ’åˆ†å’ŒåŸºäºVLMçš„ç”Ÿæˆæ¨¡å—ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹è±¡æ£€æµ‹æ¨¡å‹å¯¹æ–‡æ¡£è¿›è¡Œç»†è‡´æ ‡æ³¨ï¼Œç„¶åå°†æ–‡æ¡£åˆ’åˆ†ä¸ºå¤šä¸ªè¯­ä¹‰åŒºåŸŸï¼Œæœ€ååˆ©ç”¨VLMè¿›è¡Œç”Ÿæˆä»»åŠ¡ã€‚

**å…³é”®åˆ›æ–°**ï¼šSCANçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶ç²—ç²’åº¦è¯­ä¹‰åˆ†ææ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ–‡æ¡£ä¸­çš„è¿è´¯åŒºåŸŸï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æå‡äº†RAGç³»ç»Ÿçš„æ€§èƒ½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼ŒSCANé€šè¿‡å¯¹æ ‡æ³¨æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–æ–‡æ¡£ç»„ä»¶çš„è¯†åˆ«å’Œè¯­ä¹‰åŒºåŸŸçš„åˆ’åˆ†ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒSCANåœ¨è‹±è¯­å’Œæ—¥è¯­æ•°æ®é›†ä¸Šï¼Œæ–‡æœ¬RAGæ€§èƒ½æå‡æœ€é«˜è¾¾9.4åˆ†ï¼Œè§†è§‰RAGæ€§èƒ½æå‡æœ€é«˜è¾¾10.4åˆ†ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œå•†ä¸šè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

SCANçš„ç ”ç©¶æˆæœåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æ–‡æ¡£æ£€ç´¢ã€ä¿¡æ¯æå–å’Œæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚é€šè¿‡æå‡å¯¹ä¸°å¯Œæ–‡æ¡£çš„ç†è§£èƒ½åŠ›ï¼ŒSCANèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®çš„ä¿¡æ¯æ£€ç´¢å’Œç”ŸæˆæœåŠ¡ï¼Œæ¨åŠ¨æ–‡æ¡£å¤„ç†æŠ€æœ¯çš„è¿›æ­¥ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions.

