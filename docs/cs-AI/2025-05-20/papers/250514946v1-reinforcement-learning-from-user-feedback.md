---
layout: default
title: Reinforcement Learning from User Feedback
---

# Reinforcement Learning from User Feedback

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14946" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14946v1</a>
  <a href="https://arxiv.org/pdf/2505.14946.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14946v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14946v1', 'Reinforcement Learning from User Feedback')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Eric Han, Jun Chen, Karthik Abinav Sankararaman, Xiaoliang Peng, Tengyu Xu, Eryk Helenowski, Kaiyan Peng, Mrinal Kumar, Sinong Wang, Han Fang, Arya Talebzadeh

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºç”¨æˆ·åé¦ˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ä»¥è§£å†³ç”¨æˆ·åå¥½å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ç”¨æˆ·åé¦ˆ` `å¼ºåŒ–å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šç›®æ ‡ä¼˜åŒ–` `ç”¨æˆ·åå¥½å¯¹é½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–ä¸“å®¶è¯„ä¼°è€…ï¼Œå…¶åˆ¤æ–­å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ™®é€šç”¨æˆ·çš„çœŸå®åå¥½ã€‚
2. æå‡ºç”¨æˆ·åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLUFï¼‰æ¡†æ¶ï¼Œç›´æ¥åˆ©ç”¨ç”¨æˆ·çš„éšæ€§åé¦ˆä¿¡å·è¿›è¡Œæ¨¡å‹å¯¹é½ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨P[Love]æ¨¡å‹è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œæ­£é¢åé¦ˆç‡æ˜¾è‘—æé«˜ï¼ŒA/Bæµ‹è¯•ä¸­â€œçˆ±â€ååº”å¢åŠ äº†28%ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ç”¨æˆ·åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ï¼Œä½¿å…¶ä¸çœŸå®ç”¨æˆ·åå¥½å¯¹é½å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•å¦‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¾èµ–äºç»è¿‡åŸ¹è®­çš„ä¸“å®¶è¯„ä¼°è€…ï¼Œå…¶åˆ¤æ–­å¯èƒ½æ— æ³•åæ˜ æ™®é€šç”¨æˆ·çš„ä¼˜å…ˆçº§ã€‚æˆ‘ä»¬æå‡ºäº†ç”¨æˆ·åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLUFï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ç›´æ¥å¯¹é½ç”Ÿäº§ç¯å¢ƒä¸­ç”¨æˆ·çš„éšæ€§ä¿¡å·ã€‚RLUFè§£å†³äº†ç”¨æˆ·åé¦ˆçš„å…³é”®æŒ‘æˆ˜ï¼šç”¨æˆ·åé¦ˆé€šå¸¸æ˜¯äºŒå…ƒçš„ï¼ˆä¾‹å¦‚ï¼Œè¡¨æƒ…ç¬¦å·ååº”ï¼‰ã€ç¨€ç–ä¸”å¶å°”å…·æœ‰å¯¹æŠ—æ€§ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹P[Love]ï¼Œé¢„æµ‹LLMå“åº”è·å¾—â€œçˆ±â€ååº”çš„å¯èƒ½æ€§ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°å¤šç›®æ ‡ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ä¸­ã€‚å¤§è§„æ¨¡å®éªŒè¡¨æ˜ï¼ŒP[Love]èƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹ç”¨æˆ·çš„æ­£é¢åé¦ˆï¼Œå¹¶ä½œä¸ºæœªæ¥ç”¨æˆ·è¡Œä¸ºçš„å¯é ç¦»çº¿è¯„ä¼°å™¨ã€‚ä½¿ç”¨P[Love]çš„ç­–ç•¥ä¼˜åŒ–æ˜¾è‘—æé«˜äº†æ­£é¢åé¦ˆç‡ï¼ŒåŒ…æ‹¬åœ¨å®æ—¶A/Bæµ‹è¯•ä¸­â€œçˆ±â€ååº”å¢åŠ äº†28%ã€‚ç„¶è€Œï¼Œä¼˜åŒ–æ­£é¢ååº”å¼•å…¥äº†å¥–åŠ±é»‘å®¢æŒ‘æˆ˜ï¼Œéœ€è¦ä»”ç»†å¹³è¡¡ç›®æ ‡ã€‚é€šè¿‡ç›´æ¥åˆ©ç”¨ç”¨æˆ·çš„éšæ€§ä¿¡å·ï¼ŒRLUFä¸ºå¤§è§„æ¨¡å¯¹é½LLMsä¸çœŸå®ç”¨æˆ·åå¥½æä¾›äº†ä¸€æ¡è·¯å¾„ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬è®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸ç”¨æˆ·åå¥½å¯¹é½çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚RLHFä¾èµ–ä¸“å®¶è¯„ä¼°ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·åå¥½æœªè¢«å‡†ç¡®æ•æ‰ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºç”¨æˆ·åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLUFï¼‰æ¡†æ¶ï¼Œç›´æ¥ä»ç”¨æˆ·çš„éšæ€§åé¦ˆä¸­æå–ä¿¡æ¯ï¼Œä»¥æ›´å¥½åœ°å¯¹é½æ¨¡å‹ä¸ç”¨æˆ·çš„çœŸå®éœ€æ±‚ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRLUFæ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªå¥–åŠ±æ¨¡å‹P[Love]ï¼Œç”¨äºé¢„æµ‹ç”¨æˆ·å¯¹LLMå“åº”çš„â€œçˆ±â€ååº”ï¼Œå¹¶å°†å…¶ä¸æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ç›®æ ‡ç»“åˆåœ¨å¤šç›®æ ‡ç­–ç•¥ä¼˜åŒ–ä¸­ã€‚

**å…³é”®åˆ›æ–°**ï¼šRLUFçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç›´æ¥åˆ©ç”¨ç”¨æˆ·çš„éšæ€§åé¦ˆä¿¡å·ï¼Œè€Œä¸æ˜¯ä¾èµ–ä¸“å®¶è¯„ä¼°ï¼Œä»è€Œæ›´çœŸå®åœ°åæ˜ ç”¨æˆ·çš„åå¥½ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒP[Love]çš„è®­ç»ƒé‡‡ç”¨äº†ç”¨æˆ·çš„äºŒå…ƒåé¦ˆæ•°æ®ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¸­éœ€è¦å¹³è¡¡æ­£é¢ååº”ä¸æ½œåœ¨çš„å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨P[Love]è¿›è¡Œç­–ç•¥ä¼˜åŒ–åï¼Œæ­£é¢åé¦ˆç‡æ˜¾è‘—æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶A/Bæµ‹è¯•ä¸­ï¼Œâ€œçˆ±â€ååº”å¢åŠ äº†28%ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒRLUFæ¡†æ¶åœ¨æå‡ç”¨æˆ·æ»¡æ„åº¦æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€åœ¨çº¿å®¢æœå’Œå†…å®¹æ¨èç³»ç»Ÿç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©è¿™äº›ç³»ç»Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦ã€‚æœªæ¥ï¼ŒRLUFæ¡†æ¶å¯èƒ½ä¼šåœ¨æ›´å¹¿æ³›çš„ç”¨æˆ·äº¤äº’åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œæ¨åŠ¨äººæœºäº¤äº’çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As large language models (LLMs) are increasingly deployed in diverse user facing applications, aligning them with real user preferences becomes essential. Existing methods like Reinforcement Learning from Human Feedback (RLHF) rely on expert annotators trained on manually defined guidelines, whose judgments may not reflect the priorities of everyday users. We introduce Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs directly to implicit signals from users in production. RLUF addresses key challenges of user feedback: user feedback is often binary (e.g., emoji reactions), sparse, and occasionally adversarial. We train a reward model, P[Love], to predict the likelihood that an LLM response will receive a Love Reaction, a lightweight form of positive user feedback, and integrate P[Love] into a multi-objective policy optimization framework alongside helpfulness and safety objectives. In large-scale experiments, we show that P[Love] is predictive of increased positive feedback and serves as a reliable offline evaluator of future user behavior. Policy optimization using P[Love] significantly raises observed positive-feedback rates, including a 28% increase in Love Reactions during live A/B tests. However, optimizing for positive reactions introduces reward hacking challenges, requiring careful balancing of objectives. By directly leveraging implicit signals from users, RLUF offers a path to aligning LLMs with real-world user preferences at scale.

