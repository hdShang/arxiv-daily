---
layout: default
title: SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning
---

# SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14147" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.14147v3</a>
  <a href="https://arxiv.org/pdf/2505.14147.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14147v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14147v3', 'SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Xiong Jun Wu, Zhenduo Zhang, ZuJie Wen, Zhiqiang Zhang, Wang Ren, Lei Shi, Cai Chen, Deng Zhao, Qing Wang, Xudong Han, Chengfu Tang, Dingnan Jin, Qing Cui, Jun Zhou

**ÂàÜÁ±ª**: cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-20 (Êõ¥Êñ∞: 2025-05-25)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫SHARP‰ª•Ëß£ÂÜ≥Â§ßËßÑÊ®°Êé®ÁêÜÊ®°ÂûãËÆ≠ÁªÉ‰∏≠ÁöÑÈóÆÈ¢òÁîüÊàêÊåëÊàò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êé®ÁêÜÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ÈóÆÈ¢òÁîüÊàê` `ÈÄªËæë‰∏ÄËá¥ÊÄß` `ÊïôËÇ≤ÊäÄÊúØ` `STEMÈ¢ÜÂüü` `Ëá™ÂØπÈΩêÂéüÂàô`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊñπÊ≥ïÂú®ÁîüÊàêÈ´òË¥®Èáè„ÄÅÂèØÈ™åËØÅÁöÑÊé®ÁêÜÈóÆÈ¢ò‰∏äÂ≠òÂú®‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÊûú„ÄÇ
2. SHARPÈÄöËøáËá™ÂØπÈΩêÂéüÂàôÂíå‰∏âÈò∂ÊÆµÊ°ÜÊû∂ÔºåÁ≥ªÁªüÊÄßÂú∞ÁîüÊàêÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÈóÆÈ¢òÔºåÁ°Æ‰øùÈÄªËæë‰∏ÄËá¥ÊÄßÂíåÂèØÈ™åËØÅÊÄß„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSHARPÂ¢ûÂº∫ÁöÑËÆ≠ÁªÉÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏äÊòæËëóÊèêÂçá‰∫ÜÂáÜÁ°ÆÊÄßÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ïÁöÑË°®Áé∞„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Âú®STEMÈ¢ÜÂüüÔºå‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÈù¢‰∏¥È´òË¥®Èáè„ÄÅÂ§öÊ†∑ÂåñÂíåÂèØÈ™åËØÅÈóÆÈ¢òÈõÜÁ®ÄÁº∫ÁöÑÊåëÊàò„ÄÇÁé∞ÊúâÂêàÊàêÊñπÊ≥ïÔºåÂ¶ÇÊÄùÁª¥ÈìæÊèêÁ§∫ÔºåÂ∏∏ÁîüÊàêËøá‰∫éÁÆÄÂåñÊàñ‰∏çÂèØÈ™åËØÅÁöÑÊï∞ÊçÆÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏äÁöÑËøõÂ±ï„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSHARPÔºåËøôÊòØ‰∏ÄÁßçÁªü‰∏ÄÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂêàÊàêÈ´òË¥®ÈáèÂØπÈΩêÁöÑÊé®ÁêÜÈóÆÈ¢òÔºå‰ª•ÊîØÊåÅLRMsÁöÑÂº∫ÂåñÂ≠¶‰π†„ÄÇSHARPÂåÖÂê´‰∏ÄÂ•óËá™ÂØπÈΩêÂéüÂàôÔºåÈíàÂØπÁ†îÁ©∂ÁîüÂíåÂ••ÊûóÂåπÂÖãÁ∫ßÂà´ÁöÑÈöæÂ∫¶ÔºåÁ°Æ‰øùÈÄªËæë‰∏ÄËá¥ÊÄßÂíåÊòéÁ°ÆÂèØÈ™åËØÅÁöÑÁ≠îÊ°àÔºåÂπ∂ÈááÁî®ÁªìÊûÑÂåñÁöÑ‰∏âÈò∂ÊÆµÊ°ÜÊû∂ÔºàÂØπÈΩê„ÄÅÂÆû‰æãÂåñ„ÄÅÊé®ÁêÜÔºâÔºåÁ°Æ‰øù‰∏ªÈ¢òÂ§öÊ†∑ÊÄßÂíåÈóÆÈ¢òÁîüÊàêÁöÑÁªÜÁ≤íÂ∫¶ÊéßÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSHARPÂ¢ûÂº∫ÁöÑËÆ≠ÁªÉÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜÂ§çÊùÇÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÔºå‰ΩøLRMÁöÑË°®Áé∞Êõ¥Êé•Ëøë‰∏ìÂÆ∂Ê∞¥Âπ≥„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®STEMÈ¢ÜÂüüËÆ≠ÁªÉÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÊó∂ÔºåÁº∫‰πèÈ´òË¥®ÈáèÂíåÂèØÈ™åËØÅÈóÆÈ¢òÈõÜÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁîüÊàêÁöÑÊï∞ÊçÆÂæÄÂæÄËøá‰∫éÁÆÄÂåñÊàñÊó†Ê≥ïÈ™åËØÅÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÂ≠¶‰π†ËÉΩÂäõ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöSHARPÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáËá™ÂØπÈΩêÂéüÂàôÂíåÁªìÊûÑÂåñÊ°ÜÊû∂ÔºåÁ≥ªÁªüÊÄßÂú∞ÁîüÊàêÈ´òË¥®ÈáèÁöÑÊé®ÁêÜÈóÆÈ¢òÔºåÁ°Æ‰øùÈóÆÈ¢òÁöÑÈÄªËæë‰∏ÄËá¥ÊÄßÂíåÁ≠îÊ°àÁöÑÂèØÈ™åËØÅÊÄßÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöSHARPÁöÑÊï¥‰ΩìÊû∂ÊûÑÂàÜ‰∏∫‰∏â‰∏™‰∏ªË¶ÅÈò∂ÊÆµÔºöÂØπÈΩêÔºàAlignmentÔºâ„ÄÅÂÆû‰æãÂåñÔºàInstantiationÔºâÂíåÊé®ÁêÜÔºàInferenceÔºâ„ÄÇÂú®ÂØπÈΩêÈò∂ÊÆµÔºåÂÆö‰πâÈóÆÈ¢òÁöÑÈöæÂ∫¶ÂíåÈÄªËæëÁªìÊûÑÔºõÂú®ÂÆû‰æãÂåñÈò∂ÊÆµÔºåÁîüÊàêÂÖ∑‰ΩìÈóÆÈ¢òÔºõÂú®Êé®ÁêÜÈò∂ÊÆµÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Âæ™ÁéØÊù•È™åËØÅÂíå‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöSHARPÁöÑ‰∏ªË¶ÅÂàõÊñ∞Âú®‰∫éÂÖ∂Ëá™ÂØπÈΩêÂéüÂàôÂíå‰∏âÈò∂ÊÆµÊ°ÜÊû∂ÁöÑÁªìÂêàÔºåÁ°Æ‰øùÁîüÊàêÁöÑÈóÆÈ¢ò‰∏ç‰ªÖÂÖ∑ÊúâÊåëÊàòÊÄßÔºåËøòËÉΩË¢´ÊúâÊïàÈ™åËØÅ„ÄÇËøô‰∏ÄËÆæËÆ°‰∏éÁé∞ÊúâÊñπÊ≥ïÁöÑÊ†πÊú¨Âå∫Âà´Âú®‰∫éÂº∫Ë∞É‰∫ÜÈóÆÈ¢òÁöÑÈÄªËæë‰∏ÄËá¥ÊÄßÂíåÂèØÈ™åËØÅÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®SHARP‰∏≠ÔºåÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨ÂØπÈóÆÈ¢òÈöæÂ∫¶ÁöÑ‰∏•Ê†ºÊéßÂà∂„ÄÅÈÄªËæë‰∏ÄËá¥ÊÄßÁöÑ‰øùËØÅÔºå‰ª•ÂèäÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Âæ™ÁéØÊù•‰ºòÂåñÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËøô‰∫õËÆæËÆ°Á°Æ‰øù‰∫ÜÁîüÊàêÈóÆÈ¢òÁöÑÈ´òË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®SHARPÂ¢ûÂº∫ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÂú®GPQAÂü∫ÂáÜ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÂ§çÊùÇÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÔºåÁõ∏ËæÉ‰∫éÁé∞ÊúâÊñπÊ≥ïÊèêÂçáÂπÖÂ∫¶ËææÂà∞‰∫ÜÊú™Áü•ÁöÑÁôæÂàÜÊØîÔºåÊòæÁ§∫Âá∫SHARPÂú®Êé®Âä®Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÊÄßËÉΩÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅÁßëÂ≠¶Á†îÁ©∂Âíå‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑËÆ≠ÁªÉÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂ§çÊùÇÊé®ÁêÜËÉΩÂäõÁöÑSTEMÈ¢ÜÂüü„ÄÇSHARPÁöÑÂÆûÊñΩÂèØ‰ª•Â∏ÆÂä©ÊïôËÇ≤Â∑•‰ΩúËÄÖÁîüÊàêÈ´òË¥®ÈáèÁöÑÊµãËØïÈ¢òÔºåÊèêÂçáÂ≠¶ÁîüÁöÑÂ≠¶‰π†ÊïàÊûúÔºåÂêåÊó∂‰πü‰∏∫AIÊ®°ÂûãÁöÑËÆ≠ÁªÉÊèê‰æõ‰∫ÜÊõ¥‰∏∫‰∏∞ÂØåÂíåÊúâÊïàÁöÑÊï∞ÊçÆÊîØÊåÅÔºåÊé®Âä®Êô∫ËÉΩÁ≥ªÁªüÁöÑËøõÊ≠•„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Training large reasoning models (LRMs) with reinforcement learning in STEM domains is hindered by the scarcity of high-quality, diverse, and verifiable problem sets. Existing synthesis methods, such as Chain-of-Thought prompting, often generate oversimplified or uncheckable data, limiting model advancement on complex tasks. To address these challenges, we introduce SHARP, a unified approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a strategic set of self-alignment principles -- targeting graduate and Olympiad-level difficulty, rigorous logical consistency, and unambiguous, verifiable answers -- and a structured three-phase framework (Alignment, Instantiation, Inference) that ensures thematic diversity and fine-grained control over problem generation. We implement SHARP by leveraging a state-of-the-art LRM to infer and verify challenging STEM questions, then employ a reinforcement learning loop to refine the model's reasoning through verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate that SHARP-augmented training substantially outperforms existing methods, markedly improving complex reasoning accuracy and pushing LRM performance closer to expert-level proficiency. Our contributions include the SHARP strategy, framework design, end-to-end implementation, and experimental evaluation of its effectiveness in elevating LRM reasoning capabilities.

