---
layout: default
title: Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning
---

# Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.14216" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.14216v2</a>
  <a href="https://arxiv.org/pdf/2505.14216.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.14216v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.14216v2', 'Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Minwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, Keith Ross

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-20 (æ›´æ–°: 2025-10-31)

**å¤‡æ³¨**: 25 pages

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æ¢è®¨RLVRä¸è’¸é¦åœ¨LLMæ¨ç†ä¸­çš„å‡†ç¡®æ€§ä¸èƒ½åŠ›å·®å¼‚**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è’¸é¦è®­ç»ƒ` `æ¨ç†èƒ½åŠ›` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ¨¡å‹ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„RLVRæ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸­æå‡å‡†ç¡®æ€§æ—¶ï¼Œå¾€å¾€å¿½è§†äº†å¯¹å›°éš¾é—®é¢˜çš„å¤„ç†ï¼Œå¯¼è‡´èƒ½åŠ›æœªèƒ½æå‡ã€‚
2. æœ¬æ–‡é€šè¿‡åˆ†æRLVRå’Œè’¸é¦çš„æœºåˆ¶ï¼Œæå‡ºäº†å¯¹æ¨ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§çš„æ·±å…¥ç†è§£ï¼Œå¼ºè°ƒäº†æ–°çŸ¥è¯†å¼•å…¥çš„é‡è¦æ€§ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLVRåœ¨ç®€å•é—®é¢˜ä¸Šç”Ÿæˆäº†é«˜è´¨é‡å“åº”ï¼Œä½†åœ¨å›°éš¾é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ï¼Œè’¸é¦çš„æ•ˆæœä¹Ÿå—åˆ°é™åˆ¶ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è™½ç„¶æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•´ä½“å‡†ç¡®æ€§ï¼ˆpass@1ï¼‰ï¼Œä½†åœ¨æ¨ç†ä»»åŠ¡ä¸­å¾€å¾€æœªèƒ½æ”¹å–„èƒ½åŠ›ï¼ˆpass@kï¼‰ï¼Œè€Œè’¸é¦åˆ™èƒ½åŒæ—¶æå‡äºŒè€…ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™äº›ç°è±¡èƒŒåçš„æœºåˆ¶ï¼Œå‘ç°RLVRåœ¨æå‡ç®€å•é—®é¢˜çš„å‡†ç¡®æ€§æ—¶ï¼Œåè€Œç‰ºç‰²äº†å¯¹å›°éš¾é—®é¢˜çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼ŒRLVRå¹¶æœªä»…ä»…æé«˜ç®€å•é—®é¢˜çš„æˆåŠŸæ¦‚ç‡ï¼Œè€Œæ˜¯åœ¨å°æ¨¡å‹è®¾ç½®ä¸‹ç”Ÿæˆäº†åŸè¾“å‡ºåˆ†å¸ƒä¸­ç¼ºå¤±çš„é«˜è´¨é‡å“åº”ã€‚æ­¤å¤–ï¼Œè’¸é¦æ•™å¸ˆå“åº”åˆ°åŒåˆ†å¸ƒé—®é¢˜çš„å®éªŒè¡¨æ˜ï¼Œè’¸é¦å¹¶ä¸æ€»èƒ½æå‡èƒ½åŠ›ï¼Œåªæœ‰åœ¨å¼•å…¥æ–°çŸ¥è¯†æ—¶èƒ½åŠ›æ‰ä¼šæ”¹å–„ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™äº›å‘ç°ä¸ºç†è§£RLVRå’Œè’¸é¦å¦‚ä½•å¡‘é€ LLMçš„æ¨ç†è¡Œä¸ºæä¾›äº†æ›´æ¸…æ™°çš„è§†è§’ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³RLVRåœ¨æ¨ç†ä»»åŠ¡ä¸­æå‡å‡†ç¡®æ€§ä½†æœªèƒ½æ”¹å–„èƒ½åŠ›çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å›°éš¾é—®é¢˜æ—¶å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¯¹RLVRå’Œè’¸é¦çš„æœºåˆ¶è¿›è¡Œæ·±å…¥åˆ†æï¼Œæ­ç¤ºäºŒè€…åœ¨æå‡LLMæ¨ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§æ–¹é¢çš„ä¸åŒå½±å“ï¼Œå¼ºè°ƒæ–°çŸ¥è¯†å¼•å…¥çš„é‡è¦æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶é‡‡ç”¨å®éªŒå¯¹æ¯”çš„æ–¹æ³•ï¼Œåˆ†æRLVRä¸è’¸é¦åœ¨ä¸åŒé—®é¢˜éš¾åº¦ä¸‹çš„è¡¨ç°ï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬æ¨¡å‹è®­ç»ƒã€å“åº”ç”Ÿæˆå’Œèƒ½åŠ›è¯„ä¼°ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬æ–‡çš„åˆ›æ–°åœ¨äºæ­ç¤ºäº†RLVRåœ¨æå‡ç®€å•é—®é¢˜å‡†ç¡®æ€§æ—¶å¯¹å›°éš¾é—®é¢˜çš„è´Ÿé¢å½±å“ï¼Œå¹¶æŒ‡å‡ºè’¸é¦å¹¶ä¸æ€»èƒ½æå‡èƒ½åŠ›ï¼Œéœ€å¼•å…¥æ–°çŸ¥è¯†ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒéš¾åº¦çš„é—®é¢˜é›†ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé—®é¢˜ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶å…³æ³¨ç”Ÿæˆå“åº”çš„è´¨é‡å’Œé•¿åº¦ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRLVRåœ¨ç®€å•é—®é¢˜ä¸Šç”Ÿæˆçš„é«˜è´¨é‡å“åº”å¹¶æœªæ”¹å–„å›°éš¾é—®é¢˜çš„è¡¨ç°ï¼Œä¸”è’¸é¦åœ¨èƒ½åŠ›æå‡ä¸Šå¹¶ä¸æ€»æœ‰æ•ˆã€‚å…·ä½“è€Œè¨€ï¼ŒRLVRçš„èƒ½åŠ›æå‡å¹…åº¦æœªèƒ½è¶…è¿‡åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†èƒ½åŠ›çš„æå‡æ–¹é¢ã€‚é€šè¿‡ç†è§£RLVRä¸è’¸é¦çš„ä¸åŒæœºåˆ¶ï¼Œç ”ç©¶è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°è®¾è®¡æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼Œåº”ç”¨äºæ•™è‚²ã€å®¢æœã€å†…å®¹ç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸï¼Œæå‡æ™ºèƒ½ç³»ç»Ÿçš„å®é™…åº”ç”¨æ•ˆæœã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy (pass@1) but often fails to improve capability (pass@k) of LLMs in reasoning tasks, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR struggles to improve capability as it focuses on improving the accuracy of the easier questions to the detriment of the accuracy of the most difficult questions. Second, we show that RLVR does not merely increase the success probability for the easier questions, but in our small model settings, produces quality responses that were absent in its original output distribution. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, from the experiment distilling teacher responses to in-distribution problems, we find that capability does not always improve with distillation. We conjecture that capability improves only when new knowledge is introduced, whereas distilling reasoning patterns only improves accuracy but not capability, sacrificing performance on the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in LLMs

