---
layout: default
title: "Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving"
---

# Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.04021" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.04021v2</a>
  <a href="https://arxiv.org/pdf/2505.04021.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.04021v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.04021v2', 'Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Shan Yu, Jiarong Xing, Yifan Qiao, Mingyuan Ma, Yangmin Li, Yang Wang, Shuo Yang, Zhiqiang Xie, Shiyi Cao, Ke Bao, Ion Stoica, Harry Xu, Ying Sheng

**åˆ†ç±»**: cs.DC, cs.AI, cs.LG, cs.PF

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-05-12)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºPrismä»¥è§£å†³å¤šLLMæœåŠ¡ä¸­çš„GPUå…±äº«æ•ˆç‡é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `GPUå…±äº«` `æˆæœ¬æ•ˆç›Š` `æœåŠ¡æ°´å¹³ç›®æ ‡` `åŠ¨æ€è°ƒåº¦` `å†…å­˜åè°ƒ` `äº‘è®¡ç®—` `AIæœåŠ¡`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰GPUå…±äº«ç³»ç»Ÿåœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹æ— æ³•å®æ—¶è°ƒæ•´èµ„æºåˆ†é…ï¼Œå¯¼è‡´æ— æ³•æ»¡è¶³å»¶è¿ŸæœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOï¼‰ã€‚
2. Prismé€šè¿‡æ”¯æŒæŒ‰éœ€å†…å­˜åˆ†é…å’ŒåŒå±‚è°ƒåº¦ç­–ç•¥ï¼Œå®ç°äº†è·¨æ¨¡å‹å†…å­˜åè°ƒï¼Œçµæ´»å…±äº«GPUå†…å­˜ã€‚
3. å®éªŒè¯æ˜ï¼ŒPrismåœ¨æˆæœ¬èŠ‚çº¦æ–¹é¢è¶…è¿‡2å€ï¼ŒSLOè¾¾æˆç‡æå‡è‡³3.3å€ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„ç³»ç»Ÿè¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ä¸ºäº†è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡çš„é«˜æˆæœ¬é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Prismï¼Œä¸€ä¸ªå¤šLLMæœåŠ¡ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡GPUå…±äº«å®ç°æˆæœ¬æ•ˆç›Šå’ŒæœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOï¼‰çš„è¾¾æˆã€‚ç°æœ‰çš„GPUå…±äº«ç³»ç»Ÿåœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹ç¼ºä¹èµ„æºåˆ†é…å’Œå…±äº«ç­–ç•¥çš„å®æ—¶è°ƒæ•´èƒ½åŠ›ï¼Œå¯¼è‡´æ— æ³•æ»¡è¶³å»¶è¿Ÿè¦æ±‚ã€‚Prismé€šè¿‡æ”¯æŒæŒ‰éœ€å†…å­˜åˆ†é…å’ŒåŸºäºè¿è¡Œæ—¶éœ€æ±‚çš„åŒå±‚è°ƒåº¦ç­–ç•¥ï¼Œè§£å†³äº†è·¨æ¨¡å‹å†…å­˜åè°ƒçš„å…³é”®é™åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒPrismåœ¨æˆæœ¬èŠ‚çº¦å’ŒSLOè¾¾æˆæ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰ç³»ç»Ÿã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šLLMæœåŠ¡ä¸­GPUå…±äº«æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æ— æ³•åœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹å®æ—¶è°ƒæ•´èµ„æºåˆ†é…å’Œå…±äº«ç­–ç•¥ï¼Œå¯¼è‡´å»¶è¿ŸæœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOï¼‰æ— æ³•æ»¡è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šPrismçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å®ç°è·¨æ¨¡å‹å†…å­˜åè°ƒï¼Œæ”¯æŒæŒ‰éœ€å†…å­˜åˆ†é…å’ŒåŠ¨æ€è°ƒåº¦ï¼Œä»¥æé«˜GPUèµ„æºçš„åˆ©ç”¨ç‡å’ŒæœåŠ¡è´¨é‡ã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—åœ¨ä¸åŒæ¨¡å‹é—´èƒ½å¤Ÿçµæ´»åœ°å…±äº«å’Œé‡æ–°åˆ†é…å†…å­˜èµ„æºã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šPrismçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸€æ˜¯æŒ‰éœ€å†…å­˜åˆ†é…æ¨¡å—ï¼Œè´Ÿè´£åŠ¨æ€æ˜ å°„ç‰©ç†å†…å­˜åˆ°è™šæ‹Ÿå†…å­˜é¡µï¼›äºŒæ˜¯åŒå±‚è°ƒåº¦ç­–ç•¥æ¨¡å—ï¼Œæ ¹æ®æ¨¡å‹çš„è¿è¡Œæ—¶éœ€æ±‚åŠ¨æ€è°ƒæ•´å†…å­˜å…±äº«ç­–ç•¥ã€‚

**å…³é”®åˆ›æ–°**ï¼šPrismçš„å…³é”®åˆ›æ–°åœ¨äºå®ç°äº†è·¨æ¨¡å‹å†…å­˜åè°ƒï¼Œè§£å†³äº†ç°æœ‰GPUå…±äº«ç³»ç»Ÿåœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹çš„èµ„æºåˆ†é…å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†å†…å­˜åˆ©ç”¨ç‡å’Œå“åº”é€Ÿåº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šPrismé‡‡ç”¨åŠ¨æ€å†…å­˜æ˜ å°„æŠ€æœ¯ï¼Œå…è®¸åœ¨ä¸åŒæ¨¡å‹é—´çµæ´»åˆ†é…å†…å­˜ï¼›åŒæ—¶ï¼ŒåŒå±‚è°ƒåº¦ç­–ç•¥æ ¹æ®å®æ—¶éœ€æ±‚è°ƒæ•´å…±äº«ç­–ç•¥ï¼Œç¡®ä¿èµ„æºçš„é«˜æ•ˆä½¿ç”¨ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPrismåœ¨æˆæœ¬èŠ‚çº¦æ–¹é¢å®ç°äº†è¶…è¿‡2å€çš„æå‡ï¼ŒåŒæ—¶åœ¨æœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOï¼‰è¾¾æˆç‡ä¸Šæå‡è‡³3.3å€ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„ç³»ç»Ÿï¼Œè¯æ˜äº†å…¶åœ¨å¤šLLMæœåŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Prismçš„ç ”ç©¶æˆæœåœ¨å¤šLLMæœåŠ¡åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶é€‚ç”¨äºäº‘è®¡ç®—å¹³å°å’ŒAIæœåŠ¡æä¾›å•†ã€‚é€šè¿‡æé«˜GPUèµ„æºçš„å…±äº«æ•ˆç‡ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è¿è¥æˆæœ¬ï¼Œå¹¶æå‡æœåŠ¡è´¨é‡ï¼Œæ¨åŠ¨AIæŠ€æœ¯çš„æ™®åŠä¸åº”ç”¨ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Serving large language models (LLMs) is expensive, especially for providers hosting many models, making cost reduction essential. The unique workload patterns of serving multiple LLMs (i.e., multi-LLM serving) create new opportunities and challenges for this task. The long-tail popularity of models and their long idle periods present opportunities to improve utilization through GPU sharing. However, existing GPU sharing systems lack the ability to adjust their resource allocation and sharing policies at runtime, making them ineffective at meeting latency service-level objectives (SLOs) under rapidly fluctuating workloads.
>   This paper presents Prism, a multi-LLM serving system that unleashes the full potential of GPU sharing to achieve both cost efficiency and SLO attainment. At its core, Prism tackles a key limitation of existing systems$\unicode{x2014}$the lack of $\textit{cross-model memory coordination}$, which is essential for flexibly sharing GPU memory across models under dynamic workloads. Prism achieves this with two key designs. First, it supports on-demand memory allocation by dynamically mapping physical to virtual memory pages, allowing flexible memory redistribution among models that space- and time-share a GPU. Second, it improves memory efficiency through a two-level scheduling policy that dynamically adjusts sharing strategies based on models' runtime demands. Evaluations on real-world traces show that Prism achieves more than $2\times$ cost savings and $3.3\times$ SLO attainment compared to state-of-the-art systems.

