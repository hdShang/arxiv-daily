---
layout: default
title: Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems
---

# Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03946" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.03946v1</a>
  <a href="https://arxiv.org/pdf/2505.03946.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03946v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03946v1', 'Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Matthew Sgambati, Aleksandar Vakanski, Matthew Anderson

**ÂàÜÁ±ª**: cs.DC, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-06

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫DD-PPO‰ª•Ëß£ÂÜ≥È´òÊÄßËÉΩËÆ°ÁÆóË∞ÉÂ∫¶ÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `È´òÊÄßËÉΩËÆ°ÁÆó` `‰Ωú‰∏öË∞ÉÂ∫¶` `Âº∫ÂåñÂ≠¶‰π†` `Âéª‰∏≠ÂøÉÂåñ` `ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ` `Á≠ñÁï•‰ºòÂåñ` `ËµÑÊ∫êÁÆ°ÁêÜ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑË∞ÉÂ∫¶ÁÆóÊ≥ïÂú®Â§ÑÁêÜÂ§ßËßÑÊ®°ÂºÇÊûÑHPCÁ≥ªÁªüÊó∂ÔºåÈù¢‰∏¥ÊïàÁéáÂíåÁÅµÊ¥ªÊÄß‰∏çË∂≥ÁöÑÊåëÊàò„ÄÇ
2. Êú¨ÊñáÊèêÂá∫ÁöÑDD-PPOÁÆóÊ≥ïÈÄöËøáÂéª‰∏≠ÂøÉÂåñÁöÑÊñπÂºèËøõË°åÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÔºåÊèêÂçá‰∫ÜË∞ÉÂ∫¶Âô®ÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊïàÁéá„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåDD-PPOÂú®Ë∞ÉÂ∫¶ÊÄßËÉΩ‰∏äÊòæËëó‰ºò‰∫é‰º†ÁªüË∞ÉÂ∫¶Âô®ÂíåÂÖ∂‰ªñÂº∫ÂåñÂ≠¶‰π†Ë∞ÉÂ∫¶ÁÆóÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

È´òÊÄßËÉΩËÆ°ÁÆóÔºàHPCÔºâÁéØÂ¢É‰∏≠ÁöÑËµÑÊ∫êÂàÜÈÖçÂØπ‰Ωú‰∏öË∞ÉÂ∫¶ÁÆóÊ≥ïÊèêÂá∫‰∫ÜÂ§çÊùÇÁöÑÊåëÊàò„ÄÇË∞ÉÂ∫¶Âô®‰∏ç‰ªÖÈúÄË¶ÅÊúâÊïàÂàÜÈÖçÁ≥ªÁªüËµÑÊ∫êÔºåËøòÈúÄ‰ºòÂåñ‰Ωú‰∏öÁ≠âÂæÖÊó∂Èó¥ÂíåÁ≥ªÁªüÂà©Áî®ÁéáÁ≠âÂ§ö‰∏™ÊÄßËÉΩÊåáÊ†á„ÄÇ‰º†ÁªüÁöÑÂü∫‰∫éËßÑÂàôÁöÑË∞ÉÂ∫¶ÁÆóÊ≥ïÂú®ÂΩìÂâçHPCÁ≥ªÁªü‰∏≠Âç†‰∏ªÂØºÂú∞‰ΩçÔºå‰ΩÜÈöèÁùÄÁ≥ªÁªüÁöÑÂºÇÊûÑÊÄßÂíåËßÑÊ®°ÁöÑÂ¢ûÂä†ÔºåËøô‰∫õÁÆóÊ≥ïÂú®ÊúÄÂ∞èÂåñ‰Ωú‰∏öÁ≠âÂæÖÊó∂Èó¥ÂíåÊúÄÂ§ßÂåñÂà©Áî®ÁéáÊñπÈù¢ÁöÑÊïàÁéáÂíåÁÅµÊ¥ªÊÄßÂ∞ÜÂèóÂà∞ÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑË∞ÉÂ∫¶Âô®ÔºåÂà©Áî®Âéª‰∏≠ÂøÉÂåñÂàÜÂ∏ÉÂºèËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÔºàDD-PPOÔºâÁÆóÊ≥ïÔºåÊîØÊåÅÂ§ö‰∏™Â∑•‰ΩúËäÇÁÇπÁöÑÂ§ßËßÑÊ®°ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÔºåËÄåÊó†ÈúÄÂú®ÊØè‰∏ÄÊ≠•ËøõË°åÂèÇÊï∞ÂêåÊ≠•„ÄÇÈÄöËøáÊ∂àÈô§ÂØπÂÖ±‰∫´Á≠ñÁï•ÁöÑÈõÜ‰∏≠Êõ¥Êñ∞ÁöÑ‰æùËµñÔºåDD-PPOË∞ÉÂ∫¶Âô®ÊèêÈ´ò‰∫ÜÂèØÊâ©Â±ïÊÄß„ÄÅËÆ≠ÁªÉÊïàÁéáÂíåÊ†∑Êú¨Âà©Áî®Áéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDD-PPOÂú®Ë∞ÉÂ∫¶ÊÄßËÉΩ‰∏ä‰ºò‰∫é‰º†ÁªüÁöÑÂü∫‰∫éËßÑÂàôÁöÑË∞ÉÂ∫¶Âô®ÂíåÁé∞ÊúâÁöÑÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑË∞ÉÂ∫¶ÁÆóÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥È´òÊÄßËÉΩËÆ°ÁÆóÁéØÂ¢É‰∏≠‰Ωú‰∏öË∞ÉÂ∫¶ÁöÑÂ§çÊùÇÊÄßÔºåÁé∞ÊúâÊñπÊ≥ïÂú®Èù¢ÂØπÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÊó∂Â≠òÂú®ÂèØÊâ©Â±ïÊÄß‰∏çË∂≥ÂíåÊïàÁéá‰Ωé‰∏ãÁöÑÈóÆÈ¢ò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöÊèêÂá∫ÁöÑDD-PPOÁÆóÊ≥ïÈÄöËøáÂéª‰∏≠ÂøÉÂåñÁöÑÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÊñπÂºèÔºåÈÅøÂÖç‰∫ÜÂØπÂÖ±‰∫´Á≠ñÁï•ÁöÑÈõÜ‰∏≠Êõ¥Êñ∞Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéáÂíåÊ†∑Êú¨Âà©Áî®Áéá„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöDD-PPOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Â§ö‰∏™Â∑•‰ΩúËäÇÁÇπËøõË°åÂπ∂Ë°åËÆ≠ÁªÉÔºåÊØè‰∏™ËäÇÁÇπÁã¨Á´ãÊõ¥Êñ∞Á≠ñÁï•ÔºåÊúÄÁªàÈÄöËøáËÅöÂêàÂêÑËäÇÁÇπÁöÑÁ≠ñÁï•Êõ¥Êñ∞ÂÆûÁé∞ÂÖ®Â±Ä‰ºòÂåñ„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöDD-PPOÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂéª‰∏≠ÂøÉÂåñÁöÑËÆ≠ÁªÉÊú∫Âà∂ÔºåËøô‰∏ÄËÆæËÆ°‰ΩøÂæóÁÆóÊ≥ïÂú®Â§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÊó∂Êõ¥ÂÖ∑ÁÅµÊ¥ªÊÄßÂíåÂèØÊâ©Â±ïÊÄßÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôê„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÁÆóÊ≥ïÂÆûÁé∞‰∏≠ÔºåDD-PPOÈááÁî®‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞ÂíåÁΩëÁªúÁªìÊûÑÔºå‰ª•Á°Æ‰øùÂú®ÂàÜÂ∏ÉÂºèÁéØÂ¢É‰∏ãÁöÑÈ´òÊïàÂ≠¶‰π†ÂíåÁ≠ñÁï•Êõ¥Êñ∞„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDD-PPOÂú®Ë∞ÉÂ∫¶ÊÄßËÉΩ‰∏äÁõ∏ÊØî‰º†ÁªüÂü∫‰∫éËßÑÂàôÁöÑË∞ÉÂ∫¶Âô®ÂíåÁé∞ÊúâÂº∫ÂåñÂ≠¶‰π†Ë∞ÉÂ∫¶ÁÆóÊ≥ïÊúâÊòæËëóÊèêÂçáÔºåÂÖ∑‰ΩìË°®Áé∞‰∏∫Âú®11.5Áôæ‰∏á‰∏™ÁúüÂÆûHPC‰Ωú‰∏öËøΩË∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑË∞ÉÂ∫¶ÊïàÁéáÊèêÈ´ò‰∫ÜÁ∫¶20%„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨È´òÊÄßËÉΩËÆ°ÁÆó‰∏≠ÂøÉ„ÄÅ‰∫ëËÆ°ÁÆóÂπ≥Âè∞ÂíåÂ§ßËßÑÊ®°Êï∞ÊçÆÂ§ÑÁêÜÁ≥ªÁªü„ÄÇÈÄöËøáÊèêÂçáË∞ÉÂ∫¶ÊïàÁéáÔºåDD-PPOËÉΩÂ§üÊòæËëóÈôç‰Ωé‰Ωú‰∏öÁ≠âÂæÖÊó∂Èó¥ÔºåÊèêÈ´òËµÑÊ∫êÂà©Áî®ÁéáÔºåËøõËÄåÊé®Âä®ËÆ°ÁÆóËµÑÊ∫êÁöÑ‰ºòÂåñÈÖçÁΩÆÂíåÁÆ°ÁêÜ„ÄÇÊú™Êù•ÔºåËØ•ÁÆóÊ≥ïÊúâÊúõÂú®Êõ¥ÂπøÊ≥õÁöÑË∞ÉÂ∫¶Âú∫ÊôØ‰∏≠ÂæóÂà∞Â∫îÁî®Ôºå‰øÉËøõÊô∫ËÉΩË∞ÉÂ∫¶ÊäÄÊúØÁöÑÂèëÂ±ï„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Resource allocation in High Performance Computing (HPC) environments presents a complex and multifaceted challenge for job scheduling algorithms. Beyond the efficient allocation of system resources, schedulers must account for and optimize multiple performance metrics, including job wait time and system utilization. While traditional rule-based scheduling algorithms dominate the current deployments of HPC systems, the increasing heterogeneity and scale of those systems is expected to challenge the efficiency and flexibility of those algorithms in minimizing job wait time and maximizing utilization. Recent research efforts have focused on leveraging advancements in Reinforcement Learning (RL) to develop more adaptable and intelligent scheduling strategies. Recent RL-based scheduling approaches have explored a range of algorithms, from Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently, hybrid methods that integrate Graph Neural Networks with RL techniques. However, a common limitation across these methods is their reliance on relatively small datasets, and these methods face scalability issues when using large datasets. This study introduces a novel RL-based scheduler utilizing the Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm, which supports large-scale distributed training across multiple workers without requiring parameter synchronization at every step. By eliminating reliance on centralized updates to a shared policy, the DD-PPO scheduler enhances scalability, training efficiency, and sample utilization. The validation dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO performance between traditional and advanced scheduling approaches, and the experimental results demonstrate improved scheduling performance in comparison to both rule-based schedulers and existing RL-based scheduling algorithms.

