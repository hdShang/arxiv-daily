---
layout: default
title: Can Large Language Models Predict Parallel Code Performance?
---

# Can Large Language Models Predict Parallel Code Performance?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03988" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03988v1</a>
  <a href="https://arxiv.org/pdf/2505.03988.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03988v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03988v1', 'Can Large Language Models Predict Parallel Code Performance?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Gregory Bolet, Giorgis Georgakoudis, Harshitha Menon, Konstantinos Parasyris, Niranjan Hasabnis, Hayden Estes, Kirk W. Cameron, Gal Oren

**åˆ†ç±»**: cs.DC, cs.AI, cs.PF

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

**å¤‡æ³¨**: 5 pages, 4 figures, accepted to AI4Sys Workshop at HPDC 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹å¹¶è¡Œä»£ç æ€§èƒ½ä»¥è§£å†³GPUæ€§èƒ½è¯„ä¼°é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `GPUæ€§èƒ½é¢„æµ‹` `å±‹é¡¶çº¿æ¨¡å‹` `å¹¶è¡Œè®¡ç®—` `é«˜æ€§èƒ½è®¡ç®—` `æ€§èƒ½åˆ†æ` `ä»£ç ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„GPUæ€§èƒ½è¯„ä¼°æ–¹æ³•ä¾èµ–äºç¡¬ä»¶æ‰§è¡Œæ—¶é—´åˆ†æï¼Œé™åˆ¶äº†é«˜ç«¯GPUçš„è®¿é—®ã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒGPUå†…æ ¸çš„æ€§èƒ½é¢„æµ‹ï¼Œå°†é—®é¢˜è§†ä¸ºå±‹é¡¶çº¿åˆ†ç±»ä»»åŠ¡ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨æä¾›åˆ†ææ•°æ®æ—¶è¾¾åˆ°100%å‡†ç¡®ç‡ï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„æ¨¡å‹åœ¨æ— åˆ†æä¿¡æ¯ä¸‹ä¹Ÿèƒ½å–å¾—64%çš„å‡†ç¡®ç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å‡†ç¡®è¯„ä¼°å¹¶è¡ŒGPUä»£ç çš„æ€§èƒ½é€šå¸¸éœ€è¦åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šè¿›è¡Œæ‰§è¡Œæ—¶é—´åˆ†æï¼Œè¿™åœ¨é«˜ç«¯GPUè®¿é—®å—é™çš„æƒ…å†µä¸‹å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦å¯ä»¥æä¾›ä¸€ç§ä¸ä¾èµ–ç¡¬ä»¶çš„GPUæ€§èƒ½é¢„æµ‹æ›¿ä»£æ–¹æ³•ã€‚æˆ‘ä»¬å°†é—®é¢˜æ¡†å®šä¸ºå±‹é¡¶çº¿åˆ†ç±»ä»»åŠ¡ï¼šç»™å®šGPUå†…æ ¸çš„æºä»£ç å’Œç›®æ ‡GPUçš„ç¡¬ä»¶è§„æ ¼ï¼ŒLLMèƒ½å¦é¢„æµ‹è¯¥å†…æ ¸æ˜¯è®¡ç®—å—é™è¿˜æ˜¯å¸¦å®½å—é™ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«340ä¸ªGPUå†…æ ¸çš„å¹³è¡¡æ•°æ®é›†ï¼Œå¹¶åœ¨å››ç§åœºæ™¯ä¸‹è¯„ä¼°LLMsçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LLMsåœ¨æä¾›æ˜ç¡®çš„åˆ†ææ•°æ®æ—¶èƒ½å¤Ÿè¾¾åˆ°100%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå¹¶ä¸”å…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMsåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†LLMsï¼Œæœ€é«˜å¯è¾¾64%çš„å‡†ç¡®ç‡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°LLMçš„å¾®è°ƒéœ€è¦æ¯”ç›®å‰å¯ç”¨çš„æ•°æ®æ›´å¤šã€‚æ­¤ç ”ç©¶ä¸ºæºçº§å±‹é¡¶çº¿æ€§èƒ½é¢„æµ‹æä¾›äº†æ–°çš„è§†è§’ï¼Œå±•ç¤ºäº†LLMsåœ¨è¿è¡Œæ—¶åˆ†æä¸å¯è¡Œæ—¶çš„ä¼˜åŒ–æ½œåŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¹¶è¡ŒGPUä»£ç æ€§èƒ½è¯„ä¼°çš„éš¾é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºç¡¬ä»¶æ‰§è¡Œæ—¶é—´åˆ†æï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å°†GPUå†…æ ¸çš„æºä»£ç å’Œç›®æ ‡GPUçš„ç¡¬ä»¶è§„æ ¼è¾“å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé¢„æµ‹å†…æ ¸çš„è®¡ç®—ç‰¹æ€§ï¼ˆè®¡ç®—å—é™æˆ–å¸¦å®½å—é™ï¼‰ï¼Œä»è€Œé¿å…ç¡¬ä»¶ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«340ä¸ªGPUå†…æ ¸çš„å¹³è¡¡æ•°æ®é›†ï¼Œè¯„ä¼°äº†LLMsåœ¨å››ç§ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ä½¿ç”¨åˆ†ææ•°æ®ã€é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ¬ç ”ç©¶æ˜¯é¦–æ¬¡å°†LLMsåº”ç”¨äºæºçº§å±‹é¡¶çº¿æ€§èƒ½é¢„æµ‹ï¼Œé€šè¿‡åˆ†ç±»ä»»åŠ¡å±•ç¤ºäº†å…¶åœ¨ç¼ºä¹ç¡¬ä»¶åˆ†ææ—¶çš„æ½œåŠ›ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨äº†æ˜ç¡®çš„åˆ†ææ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„LLMsåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”å¾®è°ƒè¿‡ç¨‹éœ€è¦æ›´å¤šçš„æ•°æ®æ”¯æŒã€‚å®éªŒè®¾è®¡ä¸­å…³æ³¨äº†æ•°æ®é›†çš„å¹³è¡¡æ€§å’Œå¤šæ ·æ€§ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„LLMsåœ¨æä¾›æ˜ç¡®çš„åˆ†ææ•°æ®æ—¶è¾¾åˆ°äº†100%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­ï¼Œæ¨ç†èƒ½åŠ›å¼ºçš„LLMsåœ¨GPUæºä»£ç çš„é¢„æµ‹å‡†ç¡®ç‡æœ€é«˜å¯è¾¾64%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬é«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰æ€§èƒ½åˆ†æå’Œä¼˜åŒ–ï¼Œå°¤å…¶åœ¨é«˜ç«¯GPUèµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚é€šè¿‡åˆ©ç”¨LLMsè¿›è¡Œæ€§èƒ½é¢„æµ‹ï¼Œå¼€å‘è€…å¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ä»£ç ä¼˜åŒ–ï¼Œæå‡å¹¶è¡Œè®¡ç®—çš„æ€§èƒ½å’Œå¯ç§»æ¤æ€§ã€‚æœªæ¥ï¼Œéšç€æ•°æ®é›†å’Œæç¤ºç­–ç•¥çš„æ”¹è¿›ï¼ŒLLMså¯èƒ½æˆä¸ºHPCé¢†åŸŸçš„é‡è¦å·¥å…·ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Accurate determination of the performance of parallel GPU code typically requires execution-time profiling on target hardware -- an increasingly prohibitive step due to limited access to high-end GPUs. This paper explores whether Large Language Models (LLMs) can offer an alternative approach for GPU performance prediction without relying on hardware. We frame the problem as a roofline classification task: given the source code of a GPU kernel and the hardware specifications of a target GPU, can an LLM predict whether the GPU kernel is compute-bound or bandwidth-bound?
>   For this study, we build a balanced dataset of 340 GPU kernels, obtained from HeCBench benchmark and written in CUDA and OpenMP, along with their ground-truth labels obtained via empirical GPU profiling. We evaluate LLMs across four scenarios: (1) with access to profiling data of the kernel source, (2) zero-shot with source code only, (3) few-shot with code and label pairs, and (4) fine-tuned on a small custom dataset.
>   Our results show that state-of-the-art LLMs have a strong understanding of the Roofline model, achieving 100% classification accuracy when provided with explicit profiling data. We also find that reasoning-capable LLMs significantly outperform standard LLMs in zero- and few-shot settings, achieving up to 64% accuracy on GPU source codes, without profiling information. Lastly, we find that LLM fine-tuning will require much more data than what we currently have available.
>   This work is among the first to use LLMs for source-level roofline performance prediction via classification, and illustrates their potential to guide optimization efforts when runtime profiling is infeasible. Our findings suggest that with better datasets and prompt strategies, LLMs could become practical tools for HPC performance analysis and performance portability.

