---
layout: default
title: "RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation"
---

# RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03275" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03275v1</a>
  <a href="https://arxiv.org/pdf/2505.03275.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03275v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03275v1', 'RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Tiantian Gan, Qiyao Sun

**åˆ†ç±»**: cs.AI, cs.SE

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRAG-MCPä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å·¥å…·é€‰æ‹©ä¸­çš„æç¤ºè†¨èƒ€é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `å·¥å…·é€‰æ‹©` `è¯­ä¹‰æ£€ç´¢` `ç”Ÿæˆæ¨¡å‹` `æ¨¡å‹ä¸Šä¸‹æ–‡åè®®` `æç¤ºè†¨èƒ€` `æ£€ç´¢å¢å¼ºç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§é‡å¤–éƒ¨å·¥å…·æ—¶ï¼Œé¢ä¸´æç¤ºè†¨èƒ€å’Œé€‰æ‹©å¤æ‚æ€§çš„é—®é¢˜ï¼Œå¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å—é™ã€‚
2. RAG-MCPé€šè¿‡è¯­ä¹‰æ£€ç´¢æŠ€æœ¯ï¼Œæå‰è¯†åˆ«ä¸æŸ¥è¯¢ç›¸å…³çš„å·¥å…·ï¼Œä»è€Œå‡å°‘ä¼ é€’ç»™æ¨¡å‹çš„æç¤ºä¿¡æ¯é‡ï¼Œç®€åŒ–å†³ç­–è¿‡ç¨‹ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAG-MCPæ˜¾è‘—é™ä½äº†æç¤ºä»¤ç‰Œæ•°é‡ï¼Œå¹¶åœ¨å·¥å…·é€‰æ‹©å‡†ç¡®ç‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰æ•ˆåˆ©ç”¨æ—¥ç›Šå¢é•¿çš„å¤–éƒ¨å·¥å…·æ—¶é¢ä¸´æç¤ºè†¨èƒ€å’Œé€‰æ‹©å¤æ‚æ€§çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†RAG-MCPï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ¡†æ¶ï¼Œé€šè¿‡å¤–éƒ¨ç´¢å¼•çš„è¯­ä¹‰æ£€ç´¢æ¥è¯†åˆ«ä¸ç»™å®šæŸ¥è¯¢æœ€ç›¸å…³çš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ï¼Œä»è€Œç®€åŒ–å†³ç­–è¿‡ç¨‹å¹¶æ˜¾è‘—å‡å°‘æç¤ºå¤§å°ã€‚å®éªŒè¡¨æ˜ï¼ŒRAG-MCPåœ¨åŸºå‡†ä»»åŠ¡ä¸­æ˜¾è‘—å‡å°‘äº†æç¤ºä»¤ç‰Œæ•°é‡ï¼ˆè¶…è¿‡50%ï¼‰ï¼Œå¹¶å°†å·¥å…·é€‰æ‹©å‡†ç¡®ç‡æé«˜äº†ä¸‰å€ä»¥ä¸Šï¼ˆ43.13%å¯¹æ¯”åŸºçº¿çš„13.62%ï¼‰ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å·¥å…·é€‰æ‹©ä¸­é¢ä¸´çš„æç¤ºè†¨èƒ€å’Œé€‰æ‹©å¤æ‚æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šä¸ªå¤–éƒ¨å·¥å…·æ—¶ï¼Œå¾€å¾€éœ€è¦ä¼ é€’å¤§é‡ä¿¡æ¯ç»™æ¨¡å‹ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRAG-MCPçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è¯­ä¹‰æ£€ç´¢æŠ€æœ¯ï¼Œæå‰è¯†åˆ«ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸å…³çš„å·¥å…·ï¼Œä»è€Œå‡å°‘ä¼ é€’ç»™æ¨¡å‹çš„æç¤ºä¿¡æ¯é‡ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨ç®€åŒ–å†³ç­–è¿‡ç¨‹ï¼Œæé«˜å·¥å…·é€‰æ‹©çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRAG-MCPçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šé¦–å…ˆæ˜¯è¯­ä¹‰æ£€ç´¢æ¨¡å—ï¼Œä»å¤–éƒ¨ç´¢å¼•ä¸­è¯†åˆ«ç›¸å…³çš„MCPï¼›å…¶æ¬¡æ˜¯ç”Ÿæˆæ¨¡å—ï¼Œå°†é€‰å®šçš„å·¥å…·æè¿°ä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚

**å…³é”®åˆ›æ–°**ï¼šRAG-MCPçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ¡†æ¶ï¼Œé€šè¿‡å¤–éƒ¨ç´¢å¼•çš„è¯­ä¹‰æ£€ç´¢æ¥ä¼˜åŒ–å·¥å…·é€‰æ‹©è¿‡ç¨‹ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„ç›´æ¥ä¼ é€’æ‰€æœ‰å·¥å…·ä¿¡æ¯çš„æ–¹å¼æœ‰æœ¬è´¨åŒºåˆ«ï¼Œæ˜¾è‘—é™ä½äº†æç¤ºçš„å¤æ‚æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒRAG-MCPé‡‡ç”¨äº†é«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢ç®—æ³•ï¼Œä»¥ç¡®ä¿å¿«é€Ÿå‡†ç¡®åœ°è¯†åˆ«ç›¸å…³å·¥å…·ã€‚åŒæ—¶ï¼Œæ¨¡å‹çš„è¾“å…¥å‚æ•°ç»è¿‡ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒçš„æŸ¥è¯¢åœºæ™¯ï¼Œç¡®ä¿ç”Ÿæˆæ¨¡å—çš„è¾“å‡ºè´¨é‡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRAG-MCPåœ¨åŸºå‡†ä»»åŠ¡ä¸­æ˜¾è‘—å‡å°‘äº†æç¤ºä»¤ç‰Œæ•°é‡ï¼Œè¶…è¿‡50%çš„å‡å°‘å¹…åº¦ï¼ŒåŒæ—¶å·¥å…·é€‰æ‹©å‡†ç¡®ç‡ä»åŸºçº¿çš„13.62%æå‡è‡³43.13%ï¼Œå®ç°äº†ä¸‰å€ä»¥ä¸Šçš„æå‡ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RAG-MCPçš„ç ”ç©¶æˆæœå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é›†æˆå¤šä¸ªå¤–éƒ¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–ç³»ç»Ÿå’Œå¤æ‚æŸ¥è¯¢å¤„ç†ç­‰é¢†åŸŸã€‚é€šè¿‡æé«˜å·¥å…·é€‰æ‹©çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to identify the most relevant MCP(s) for a given query from an external index before engaging the LLM. Only the selected tool descriptions are passed to the model, drastically reducing prompt size and simplifying decision-making. Experiments, including an MCP stress test, demonstrate RAG-MCP significantly cuts prompt tokens (e.g., by over 50%) and more than triples tool selection accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables scalable and accurate tool integration for LLMs.

