---
layout: default
title: "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation"
---

# Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.03586" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.03586v4</a>
  <a href="https://arxiv.org/pdf/2505.03586.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.03586v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.03586v4', 'Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Songchen Fu, Siang Chen, Shaojing Zhao, Letian Bai, Ta Li, Yonghong Yan

**åˆ†ç±»**: cs.MA, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-06 (æ›´æ–°: 2025-11-12)

**å¤‡æ³¨**: The code has been open-sourced in the RDC-pymarl project under https://github.com/linkjoker1006

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/linkjoker1006/RDC-pymarl)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºRainbow Delay Compensationä»¥è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§‚å¯Ÿå»¶è¿Ÿé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ` `è§‚å¯Ÿå»¶è¿Ÿ` `å¼ºåŒ–å­¦ä¹ ` `å»ä¸­å¿ƒåŒ–å†³ç­–` `é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹` `å»¶è¿Ÿè¡¥å¿` `æ™ºèƒ½ä½“åä½œ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹è§‚å¯Ÿå»¶è¿Ÿæ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´å†³ç­–è´¨é‡ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†Rainbow Delay Compensationæ¡†æ¶ï¼Œé€šè¿‡å»ä¸­å¿ƒåŒ–éšæœºä¸ªä½“å»¶è¿Ÿéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ¥åº”å¯¹å»¶è¿Ÿé—®é¢˜ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒRDCæ¡†æ¶åœ¨å¤„ç†å»¶è¿Ÿæ—¶æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒæŸäº›æƒ…å†µä¸‹å®ç°äº†ç†æƒ³çš„æ— å»¶è¿Ÿæ•ˆæœã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åœ¨ç°å®ä¸–ç•Œçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œè§‚å¯Ÿå»¶è¿Ÿæ™®éå­˜åœ¨ï¼Œå¯¼è‡´æ™ºèƒ½ä½“æ— æ³•åŸºäºç¯å¢ƒçš„çœŸå®çŠ¶æ€åšå‡ºå†³ç­–ã€‚ä¸ªä½“æ™ºèƒ½ä½“çš„å±€éƒ¨è§‚å¯Ÿé€šå¸¸åŒ…å«æ¥è‡ªå…¶ä»–æ™ºèƒ½ä½“æˆ–åŠ¨æ€å®ä½“çš„å¤šä¸ªç»„ä»¶ï¼Œè¿™äº›ç¦»æ•£çš„è§‚å¯Ÿç»„ä»¶å…·æœ‰ä¸åŒçš„å»¶è¿Ÿç‰¹æ€§ï¼Œç»™å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ã€‚æœ¬æ–‡é¦–å…ˆé€šè¿‡æ‰©å±•æ ‡å‡†çš„Dec-POMDPï¼Œæå‡ºäº†å»ä¸­å¿ƒåŒ–éšæœºä¸ªä½“å»¶è¿Ÿéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆDSID-POMDPï¼‰ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†Rainbow Delay Compensationï¼ˆRDCï¼‰ï¼Œä¸€ä¸ªç”¨äºè§£å†³éšæœºä¸ªä½“å»¶è¿Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œå¹¶æ¨èäº†å…¶å„ä¸ªæ¨¡å—çš„å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºçº¿MARLæ–¹æ³•åœ¨å›ºå®šå’Œéå›ºå®šå»¶è¿Ÿä¸‹æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œè€ŒRDCå¢å¼ºçš„æ–¹æ³•åœ¨æŸäº›å»¶è¿Ÿåœºæ™¯ä¸‹æ˜¾è‘—å®ç°äº†ç†æƒ³çš„æ— å»¶è¿Ÿæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ç”±äºè§‚å¯Ÿå»¶è¿Ÿå¯¼è‡´çš„å†³ç­–é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹ä¸åŒå»¶è¿Ÿç‰¹æ€§æ—¶ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å±€éƒ¨è§‚å¯Ÿä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼•å…¥å»ä¸­å¿ƒåŒ–éšæœºä¸ªä½“å»¶è¿Ÿéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆDSID-POMDPï¼‰ï¼Œä¸ºæ¯ä¸ªæ™ºèƒ½ä½“æä¾›æ›´å‡†ç¡®çš„å»¶è¿Ÿä¿¡æ¯ï¼Œä»è€Œæ”¹å–„å†³ç­–è´¨é‡ã€‚è¯¥è®¾è®¡ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”åŠ¨æ€ç¯å¢ƒä¸­çš„å»¶è¿Ÿç‰¹æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRDCæ¡†æ¶åŒ…å«å¤šä¸ªæ¨¡å—ï¼Œä¸»è¦åŒ…æ‹¬è§‚å¯Ÿç”Ÿæˆæ¨¡å—ã€å»¶è¿Ÿè¡¥å¿æ¨¡å—å’Œå†³ç­–æ¨¡å—ã€‚è§‚å¯Ÿç”Ÿæˆæ¨¡å—è´Ÿè´£å¤„ç†æ¥è‡ªç¯å¢ƒçš„è§‚å¯Ÿä¿¡æ¯ï¼Œå»¶è¿Ÿè¡¥å¿æ¨¡å—åˆ™é€šè¿‡å­¦ä¹ å»¶è¿Ÿç‰¹æ€§æ¥è°ƒæ•´æ™ºèƒ½ä½“çš„å†³ç­–è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†DSID-POMDPæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§‚å¯Ÿå»¶è¿Ÿé—®é¢˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒRDCæ¡†æ¶åœ¨åº”å¯¹å»¶è¿Ÿæ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨RDCæ¡†æ¶ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–å»¶è¿Ÿè¡¥å¿æ•ˆæœï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§ç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰å»¶è¿Ÿç‰¹æ€§å’Œç¯å¢ƒåŠ¨æ€ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºçº¿MARLæ–¹æ³•åœ¨å›ºå®šå’Œéå›ºå®šå»¶è¿Ÿæƒ…å†µä¸‹æ€§èƒ½ä¸‹é™å¹…åº¦å¯è¾¾50%ä»¥ä¸Šï¼Œè€ŒRDCå¢å¼ºçš„æ–¹æ³•åœ¨æŸäº›å»¶è¿Ÿåœºæ™¯ä¸‹å®ç°äº†ç†æƒ³çš„æ— å»¶è¿Ÿæ€§èƒ½ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æå‡æ•ˆæœï¼Œè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½äº¤é€šç³»ç»Ÿã€æ— äººæœºç¼–é˜Ÿã€æœºå™¨äººåä½œç­‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œè§‚å¯Ÿå»¶è¿Ÿå¯èƒ½ä¸¥é‡å½±å“ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ï¼ŒRDCæ¡†æ¶çš„å¼•å…¥èƒ½å¤Ÿæœ‰æ•ˆæå‡å†³ç­–è´¨é‡ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation typically comprises multiple components from other agents or dynamic entities within the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalizability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework. The source code is available at https://github.com/linkjoker1006/RDC-pymarl.

