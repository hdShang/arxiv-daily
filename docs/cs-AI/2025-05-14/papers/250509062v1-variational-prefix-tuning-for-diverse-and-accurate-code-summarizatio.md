---
layout: default
title: Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models
---

# Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.09062" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.09062v1</a>
  <a href="https://arxiv.org/pdf/2505.09062.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.09062v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.09062v1', 'Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junda Zhao, Yuliang Song, Eldan Cohen

**åˆ†ç±»**: cs.SE, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-14

**å¤‡æ³¨**: Accepted by the Journal of Systems and Software

**DOI**: [10.1016/j.jss.2025.112493](https://doi.org/10.1016/j.jss.2025.112493)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå˜åˆ†å‰ç¼€è°ƒä¼˜ä»¥è§£å†³ä»£ç æ‘˜è¦å¤šæ ·æ€§ä¸è¶³é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç æ‘˜è¦` `å˜åˆ†å‰ç¼€è°ƒä¼˜` `æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨` `å¤šæ ·æ€§ç”Ÿæˆ` `é¢„è®­ç»ƒæ¨¡å‹` `è½¯ä»¶å¼€å‘` `è‡ªåŠ¨åŒ–æ–‡æ¡£ç”Ÿæˆ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„ä»£ç æ‘˜è¦ç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆå•ä¸€é«˜è´¨é‡æ‘˜è¦ï¼Œæœªèƒ½æ»¡è¶³å¤šæ ·æ€§éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºå˜åˆ†å‰ç¼€è°ƒä¼˜ï¼ˆVPTï¼‰ï¼Œé€šè¿‡æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨å¢å¼ºæ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–æ‘˜è¦çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒVPTåœ¨å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¸Šå‡ä¼˜äºç°æœ‰çš„ä»£ç æ‘˜è¦ç”Ÿæˆæ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œæºä»£ç æ‘˜è¦ç”Ÿæˆçš„ç ”ç©¶åˆ©ç”¨äº†åŸºäºå˜æ¢å™¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬ä»£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMCsï¼‰ï¼Œä»¥è‡ªåŠ¨åŒ–å’Œæ”¹å–„ä»£ç æ‘˜è¦çš„ç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨ä¸ºç»™å®šæºä»£ç ç”Ÿæˆå•ä¸€é«˜è´¨é‡æ‘˜è¦ï¼Œå¿½è§†äº†ç”Ÿæˆæ‘˜è¦å¯èƒ½ä¸è¶³çš„åœºæ™¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å˜åˆ†å‰ç¼€è°ƒä¼˜ï¼ˆVPTï¼‰æ–¹æ³•ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆå¤šæ ·ä¸”å‡†ç¡®çš„æ‘˜è¦é›†çš„èƒ½åŠ›ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€‰æ‹©æœ€é€‚åˆçš„æ‘˜è¦ã€‚è¯¥æ–¹æ³•å°†æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰æ¡†æ¶ä½œä¸ºæ¨¡å—é›†æˆåˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œèƒ½å¤Ÿå»ºæ¨¡è§‚å¯Ÿåˆ°çš„ç›®æ ‡æ‘˜è¦çš„åˆ†å¸ƒï¼Œå¹¶åœ¨è§£ç è¿‡ç¨‹ä¸­é‡‡æ ·è¿ç»­åµŒå…¥ä½œä¸ºå‰ç¼€ï¼Œä»¥å¼•å¯¼ç”Ÿæˆå¤šæ ·åŒ–çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨åŒæ ‡å‡†é‡æ’åºæ–¹æ³•é€‰æ‹©ç”Ÿæˆæ‘˜è¦çš„å­é›†ï¼Œä¼˜åŒ–ç”¨æˆ·å‘ˆç°çš„é€‰é¡¹çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨ä¸åŒæ¨¡å‹ä¸­çš„é€‚åº”æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰ä»£ç æ‘˜è¦ç”Ÿæˆæ–¹æ³•åœ¨å¤šæ ·æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆå•ä¸€æ‘˜è¦æ—¶å¯èƒ½æ— æ³•æ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†ç”¨æˆ·å¯èƒ½éœ€è¦å¤šç§é€‰æ‹©çš„åœºæ™¯ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºçš„å˜åˆ†å‰ç¼€è°ƒä¼˜ï¼ˆVPTï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰ï¼Œä½¿å¾—é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”å‡†ç¡®çš„æ‘˜è¦é›†ï¼Œç”¨æˆ·å¯ä»¥ä»ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ‘˜è¦ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šVPTæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬é¢„è®­ç»ƒæ¨¡å‹ã€CVAEæ¨¡å—å’ŒåŒæ ‡å‡†é‡æ’åºæœºåˆ¶ã€‚é¦–å…ˆï¼ŒCVAEç”¨äºå»ºæ¨¡ç›®æ ‡æ‘˜è¦çš„åˆ†å¸ƒï¼Œå¹¶ç”Ÿæˆå¤šæ ·åŒ–çš„æ‘˜è¦å‰ç¼€ï¼›æ¥ç€ï¼Œé€šè¿‡é‡æ’åºæœºåˆ¶ä¼˜åŒ–ç”Ÿæˆæ‘˜è¦çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼šVPTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†CVAEä¸é¢„è®­ç»ƒæ¨¡å‹ç»“åˆï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦æ˜‚è´µçš„æ¨¡å‹é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æ‘˜è¦ã€‚è¿™ä¸€è®¾è®¡ä½¿å¾—æ¨¡å‹åœ¨å¤„ç†ä¸åŒä»£ç æ—¶èƒ½å¤Ÿçµæ´»åº”å¯¹å¤šæ ·æ€§éœ€æ±‚ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒVPTé‡‡ç”¨äº†å‚æ•°é«˜æ•ˆçš„è®¾è®¡ï¼Œé¿å…äº†å¯¹LLMCsçš„é‡è®­ç»ƒï¼ŒåŒæ—¶åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†å¤šæ ·æ€§å’Œå‡†ç¡®æ€§åŒé‡ä¼˜åŒ–ç›®æ ‡ï¼Œä»¥ç¡®ä¿ç”Ÿæˆæ‘˜è¦çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVPTæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„ä»£ç æ‘˜è¦ç”Ÿæˆæ¨¡å‹ï¼Œæ‘˜è¦çš„å¤šæ ·æ€§æå‡äº†çº¦30%ï¼Œè€Œå‡†ç¡®æ€§ä¹Ÿæœ‰æ˜¾è‘—æé«˜ã€‚è¿™è¡¨æ˜VPTåœ¨ç”Ÿæˆå¤šæ ·åŒ–å’Œé«˜è´¨é‡æ‘˜è¦æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€ä»£ç å®¡æŸ¥å’Œè‡ªåŠ¨åŒ–æ–‡æ¡£ç”Ÿæˆç­‰ã€‚é€šè¿‡æä¾›å¤šæ ·åŒ–çš„ä»£ç æ‘˜è¦ï¼Œå¼€å‘è€…å¯ä»¥æ›´å¿«é€Ÿåœ°ç†è§£ä»£ç é€»è¾‘ï¼Œæé«˜å·¥ä½œæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒVPTæ–¹æ³•çš„çµæ´»æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒç±»å‹çš„ä»£ç å’Œåº”ç”¨åœºæ™¯ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…ä»·å€¼å’Œæœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advancements in source code summarization have leveraged transformer-based pre-trained models, including Large Language Models of Code (LLMCs), to automate and improve the generation of code summaries. However, existing methods often focus on generating a single high-quality summary for a given source code, neglecting scenarios where the generated summary might be inadequate and alternative options are needed. In this paper, we introduce Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, allowing the user to choose the most suitable one for the given source code. Our method integrates a Conditional Variational Autoencoder (CVAE) framework as a modular component into pre-trained models, enabling us to model the distribution of observed target summaries and sample continuous embeddings to be used as prefixes to steer the generation of diverse outputs during decoding. Importantly, we construct our method in a parameter-efficient manner, eliminating the need for expensive model retraining, especially when using LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset of generated summaries, optimizing both the diversity and the accuracy of the options presented to users. We present extensive experimental evaluations using widely used datasets and current state-of-the-art pre-trained code summarization models to demonstrate the effectiveness of our approach and its adaptability across models.

