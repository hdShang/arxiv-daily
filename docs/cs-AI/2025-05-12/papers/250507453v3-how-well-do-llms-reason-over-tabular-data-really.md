---
layout: default
title: How well do LLMs reason over tabular data, really?
---

# How well do LLMs reason over tabular data, really?

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07453" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07453v3</a>
  <a href="https://arxiv.org/pdf/2505.07453.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07453v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07453v3', 'How well do LLMs reason over tabular data, really?')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Cornelius Wolff, Madelon Hulsebos

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-11-04)

**å¤‡æ³¨**: 10 pages, 4 figures

**æœŸåˆŠ**: The 4th Table Representation Learning Workshop at ACL 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºè¯„ä¼°ç­–ç•¥ä»¥æå‡LLMsåœ¨è¡¨æ ¼æ•°æ®æ¨ç†çš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¡¨æ ¼æ•°æ®` `æ¨ç†èƒ½åŠ›` `è¯„ä¼°ç­–ç•¥` `é²æ£’æ€§` `æ•°æ®åˆ†æ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰è¯„ä¼°æ–¹æ³•æœªèƒ½å‡†ç¡®åæ˜ LLMsåœ¨è¡¨æ ¼æ•°æ®æ¨ç†ä¸­çš„çœŸå®è¡¨ç°ï¼Œå¯¼è‡´å¯¹å…¶èƒ½åŠ›çš„ç†è§£æœ‰é™ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°ç­–ç•¥ï¼Œåˆ©ç”¨LLMä½œä¸ºè¯„åˆ¤è€…ï¼Œæä¾›æ›´å¯é çš„æ€§èƒ½æ´å¯Ÿï¼Œå¹¶é’ˆå¯¹ç°å®è¡¨æ ¼è¾“å…¥çš„ç‰¹å¾è¿›è¡Œæ‰©å±•ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨é¢å¯¹ç¼ºå¤±å€¼ã€é‡å¤å®ä½“å’Œç»“æ„å˜åŒ–æ—¶ï¼Œå…¶è¡¨æ ¼æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œå¼ºè°ƒäº†æå‡é²æ£’æ€§çš„å¿…è¦æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨è¡¨æ ¼æ•°æ®æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°å°šä¸æ˜ç¡®ã€‚ç°æœ‰çš„è¯„ä¼°ç­–ç•¥æœªèƒ½çœŸå®åæ˜ LLMsåœ¨è¡¨æ ¼æŸ¥è¯¢ä¸­çš„æ€§èƒ½ã€‚æœ¬æ–‡æ¢è®¨äº†LLMsåœ¨é¢å¯¹ç°å®è¡¨æ ¼è¾“å…¥çš„é²æ£’æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œæ­ç¤ºäº†LLMsåœ¨è¡¨æ ¼æ¨ç†ä¸­çš„æ˜¾è‘—ä¸è¶³ã€‚é€šè¿‡æ‰©å±•è¡¨æ ¼è¾“å…¥çš„ç‰¹å¾ï¼Œç ”ç©¶è¡¨æ˜LLMsçš„æ¨ç†èƒ½åŠ›å—åˆ°ç¼ºå¤±å€¼ã€é‡å¤å®ä½“å’Œç»“æ„å˜åŒ–ç­‰å› ç´ çš„å½±å“ï¼Œå¼ºè°ƒäº†æå‡å…¶é²æ£’æ€§çš„å¿…è¦æ€§ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®æ¨ç†ä¸­çš„èƒ½åŠ›è¯„ä¼°é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•çš„ç—›ç‚¹åœ¨äºè¯„ä¼°ç­–ç•¥æœªèƒ½çœŸå®åæ˜ æ¨¡å‹æ€§èƒ½ï¼Œå¯¼è‡´å¯¹å…¶é²æ£’æ€§çš„ç†è§£ä¸è¶³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºåˆ©ç”¨LLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›æ›´å¯é çš„æ€§èƒ½æ´å¯Ÿï¼Œå¹¶é€šè¿‡å¼•å…¥ç°å®è¡¨æ ¼è¾“å…¥çš„ç‰¹å¾æ¥æµ‹è¯•æ¨¡å‹çš„é²æ£’æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šç ”ç©¶åŸºäºç°æœ‰çš„è¡¨æ ¼æ¨ç†åŸºå‡†ï¼Œé¦–å…ˆåˆ†æå…¶å¤šé€‰é¢˜è¯„ä¼°ç­–ç•¥çš„ä¸è¶³ï¼Œç„¶åå¼•å…¥ç¼ºå¤±å€¼ã€é‡å¤å®ä½“å’Œç»“æ„å˜åŒ–ç­‰ç‰¹å¾è¿›è¡Œå®éªŒï¼Œæœ€ç»ˆè¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºæå‡ºäº†LLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„ä¼°æ–¹æ³•ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„è‡ªç”±æ–‡æœ¬è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚SacreBleuå’ŒBERT-scoreï¼‰æœ¬è´¨ä¸Šä¸åŒï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹åœ¨è¡¨æ ¼æ¨ç†ä¸­çš„è¡¨ç°ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®éªŒä¸­ï¼Œè®¾ç½®äº†ä¸åŒçš„è¡¨æ ¼è¾“å…¥ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¯¹æ¯”åˆ†æä¸åŒè¯„ä¼°ç­–ç•¥çš„æ•ˆæœï¼Œç¡®ä¿è¯„ä¼°è¿‡ç¨‹çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨é¢å¯¹ç¼ºå¤±å€¼ã€é‡å¤å®ä½“å’Œç»“æ„å˜åŒ–æ—¶ï¼Œå…¶è¡¨æ ¼æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨å¼•å…¥è¿™äº›ç°å®ç‰¹å¾åï¼Œæ¨¡å‹çš„æ€§èƒ½ä¸‹é™å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æå‡LLMsé²æ£’æ€§çš„ç´§è¿«æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•°æ®åˆ†æã€å•†ä¸šæ™ºèƒ½å’Œå†³ç­–æ”¯æŒç³»ç»Ÿç­‰ã€‚é€šè¿‡æå‡LLMsåœ¨è¡¨æ ¼æ•°æ®æ¨ç†ä¸­çš„èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°æ”¯æŒç”¨æˆ·åœ¨å¤æ‚æ•°æ®ç¯å¢ƒä¸­çš„åˆ†æä¸å†³ç­–ï¼Œæœªæ¥å¯èƒ½æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.

