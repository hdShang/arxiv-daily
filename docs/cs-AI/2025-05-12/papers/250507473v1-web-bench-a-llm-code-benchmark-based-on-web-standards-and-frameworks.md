---
layout: default
title: Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks
---

# Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07473" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07473v1</a>
  <a href="https://arxiv.org/pdf/2505.07473.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07473v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07473v1', 'Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kai Xu, YiWei Mao, XinYi Guan, ZiLong Feng

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: 28 pages, 15 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºWeb-Benchä»¥è§£å†³LLMä»£ç åŸºå‡†æµ‹è¯•é¥±å’Œé—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç åŸºå‡†` `Webå¼€å‘` `è½¯ä»¶å·¥ç¨‹` `ä»»åŠ¡ä¾èµ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMä»£ç åŸºå‡†æµ‹è¯•é€æ¸é¥±å’Œï¼Œå¯¼è‡´å…¶å¯¹æ¨¡å‹ä¼˜åŒ–çš„æŒ‡å¯¼ä½œç”¨å‡å¼±ã€‚
2. æœ¬æ–‡æå‡ºWeb-BenchåŸºå‡†ï¼ŒåŒ…å«50ä¸ªå¤æ‚é¡¹ç›®ï¼Œæ¨¡æ‹ŸçœŸå®å¼€å‘æµç¨‹ï¼Œæ—¨åœ¨æå‡LLMçš„ç¼–ç èƒ½åŠ›ã€‚
3. å®éªŒè¡¨æ˜ï¼ŒWeb-Agentåœ¨Web-Benchä¸Šçš„Pass@1ä»…ä¸º25.1%ï¼Œæ˜¾è‘—ä½äºå…¶ä»–åŸºå‡†çš„è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºå…¶æŒ‘æˆ˜æ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç é¢†åŸŸçš„åº”ç”¨æ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œä»ä»£ç åŠ©æ‰‹åˆ°è‡ªä¸»ç¼–ç ä»£ç†ï¼Œå†åˆ°é€šè¿‡è‡ªç„¶è¯­è¨€ç”Ÿæˆå®Œæ•´é¡¹ç›®ã€‚æ—©æœŸçš„LLMä»£ç åŸºå‡†ä¸»è¦å…³æ³¨ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œä½†è¿™äº›åŸºå‡†é€æ¸é¥±å’Œï¼Œå‰Šå¼±äº†å…¶å¯¹LLMsçš„æŒ‡å¯¼ä½œç”¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†æ–°çš„åŸºå‡†Web-Benchï¼ŒåŒ…å«50ä¸ªé¡¹ç›®ï¼Œæ¯ä¸ªé¡¹ç›®ç”±20ä¸ªå…·æœ‰é¡ºåºä¾èµ–çš„ä»»åŠ¡ç»„æˆï¼Œæ¨¡æ‹ŸçœŸå®çš„å¼€å‘å·¥ä½œæµç¨‹ã€‚è®¾è®¡Web-Benchæ—¶ï¼Œæ—¨åœ¨æ¶µç›–Webå¼€å‘çš„åŸºç¡€å…ƒç´ ï¼šWebæ ‡å‡†å’ŒWebæ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä½äºç°æœ‰è½¯ä»¶å·¥ç¨‹åŸºå‡†ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰LLMä»£ç åŸºå‡†æµ‹è¯•é¥±å’Œçš„é—®é¢˜ï¼Œç°æœ‰åŸºå‡†å¦‚HumanEvalå’ŒMBPPçš„é«˜é€šè¿‡ç‡å·²æ— æ³•æœ‰æ•ˆæŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºWeb-BenchåŸºå‡†ï¼Œé€šè¿‡è®¾è®¡åŒ…å«é¡ºåºä¾èµ–çš„ä»»åŠ¡é¡¹ç›®ï¼Œæ¨¡æ‹ŸçœŸå®å¼€å‘æµç¨‹ï¼Œè¦†ç›–Webå¼€å‘çš„åŸºç¡€æ ‡å‡†å’Œæ¡†æ¶ï¼Œä»¥æå‡LLMçš„ç¼–ç èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šWeb-BenchåŒ…å«50ä¸ªé¡¹ç›®ï¼Œæ¯ä¸ªé¡¹ç›®ç”±20ä¸ªä»»åŠ¡æ„æˆï¼Œä»»åŠ¡ä¹‹é—´å­˜åœ¨é¡ºåºä¾èµ–ï¼Œæ•´ä½“æµç¨‹æ¨¡æ‹Ÿäº†å·¥ç¨‹å¸ˆçš„å¼€å‘å·¥ä½œã€‚

**å…³é”®åˆ›æ–°**ï¼šWeb-Benchçš„è®¾è®¡ä¸ä»…å…³æ³¨ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œè¿˜å¼ºè°ƒäº†Webæ ‡å‡†å’Œæ¡†æ¶çš„é‡è¦æ€§ï¼Œæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„æµ‹è¯•ç¯å¢ƒã€‚

**å…³é”®è®¾è®¡**ï¼šæ¯ä¸ªé¡¹ç›®ç”±ç»éªŒä¸°å¯Œçš„å·¥ç¨‹å¸ˆè®¾è®¡ï¼Œå®Œæˆä¸€ä¸ªé¡¹ç›®å¹³å‡éœ€è¦4åˆ°8å°æ—¶ï¼Œç¡®ä¿äº†ä»»åŠ¡çš„å¤æ‚æ€§å’Œç°å®æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨Web-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹Claude 3.7 Sonnetçš„Pass@1ä»…ä¸º25.1%ï¼Œæ˜¾è‘—ä½äºSWE-Benchçš„Verifiedï¼ˆ65.4%ï¼‰å’ŒFullï¼ˆ33.8%ï¼‰åˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºWeb-Benchçš„æŒ‘æˆ˜æ€§å’Œæœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Web-Benchçš„è®¾è®¡ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨Webå¼€å‘é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°å’Œä¼˜åŒ–LLMçš„ç¼–ç èƒ½åŠ›ã€‚æœªæ¥ï¼Œè¯¥åŸºå‡†å¯èƒ½åœ¨æ•™è‚²ã€è½¯ä»¶å¼€å‘å’Œè‡ªåŠ¨åŒ–ç¼–ç¨‹ç­‰å¤šä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨LLMæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The application of large language models (LLMs) in the field of coding is evolving rapidly: from code assistants, to autonomous coding agents, and then to generating complete projects through natural language. Early LLM code benchmarks primarily focused on code generation accuracy, but these benchmarks have gradually become saturated. Benchmark saturation weakens their guiding role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%. Among various attempts to address benchmark saturation, approaches based on software engineering have stood out, but the saturation of existing software engineering benchmarks is rapidly increasing. To address this, we propose a new benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks with sequential dependencies. The tasks implement project features in sequence, simulating real-world human development workflows. When designing Web-Bench, we aim to cover the foundational elements of Web development: Web Standards and Web Frameworks. Given the scale and complexity of these projects, which were designed by engineers with 5 to 10 years of experience, each presents a significant challenge. On average, a single project takes 4 to 8 hours for a senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA (Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better) than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss that in any development field, Standards and Frameworks represent foundational knowledge and efficiency tools, respectively, and LLMs require optimization tailored to them.

