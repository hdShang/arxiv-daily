---
layout: default
title: "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving"
---

# Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07773" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07773v4</a>
  <a href="https://arxiv.org/pdf/2505.07773.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07773v4" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07773v4', 'Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Xinji Mai, Haotian Xu, Zhong-Zhi Li, Xing W, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-08-20)

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/yyht/openrlhf_async_pipline) | [GITHUB](https://github.com/yyht/openrlhf)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºZeroTIRä»¥è§£å†³æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„å·¥å…·ä½¿ç”¨æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `å·¥å…·é›†æˆæ¨ç†` `æ•°å­¦é—®é¢˜æ±‚è§£` `è‡ªå‘ä»£ç æ‰§è¡Œ` `å¤§å‹è¯­è¨€æ¨¡å‹` `ZeroTIR` `è‡ªåŠ¨åŒ–æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹ç²¾ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä»£ç æ‰§è¡Œçš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚
2. è®ºæ–‡æå‡ºäº†ZeroTIRæ–¹æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹è‡ªå‘ç”Ÿæˆå’Œæ‰§è¡ŒPythonä»£ç ï¼Œè§£å†³æ•°å­¦é—®é¢˜ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒZeroTIRåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„éå·¥å…·å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå±•ç¤ºäº†å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦ç²¾ç¡®ã€å¯éªŒè¯è®¡ç®—çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚å°½ç®¡åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿå¢å¼ºæ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œä½†ç†è§£ä»£ç†å¦‚ä½•è‡ªä¸»å­¦ä¹ åˆ©ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ä»£ç æ‰§è¡Œï¼‰ä»ç„¶è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºäºç»“æœå¥–åŠ±çš„RLåœ¨å·¥å…·é›†æˆæ¨ç†ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ZeroTIRæ–¹æ³•ï¼Œè®­ç»ƒåŸºç¡€LLMsè‡ªå‘ç”Ÿæˆå’Œæ‰§è¡ŒPythonä»£ç ä»¥è§£å†³æ•°å­¦é—®é¢˜ï¼Œè€Œæ— éœ€ç›‘ç£å·¥å…·ä½¿ç”¨ç¤ºä¾‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œéšç€RLè®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡å‘ˆç°å‡ºå¯é¢„æµ‹çš„æ‰©å±•å…³ç³»ï¼Œè®­ç»ƒæ­¥éª¤çš„å¢åŠ ä¸è‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦å’Œæœ€ç»ˆä»»åŠ¡å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³ã€‚è¿™è¡¨æ˜è®­ç»ƒæŠ•å…¥çš„è®¡ç®—åŠªåŠ›ä¸æœ‰æ•ˆçš„å·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥çš„å‡ºç°ä¹‹é—´å­˜åœ¨å¯é‡åŒ–çš„å…³ç³»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZeroTIRåœ¨æŒ‘æˆ˜æ€§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†éå·¥å…·çš„ZeroRLåŸºçº¿ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­å¯¹å·¥å…·ä½¿ç”¨çš„ä¾èµ–ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éœ€è¦ç›‘ç£ç¤ºä¾‹ï¼Œé™åˆ¶äº†æ¨¡å‹çš„è‡ªä¸»å­¦ä¹ èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»ç»“æœå¥–åŠ±å‡ºå‘ï¼Œè®­ç»ƒæ¨¡å‹è‡ªå‘ç”Ÿæˆå’Œæ‰§è¡Œä»£ç ï¼Œæå‡å…¶åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¡¨ç°ï¼Œé¿å…å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè§£è€¦çš„ä»£ç æ‰§è¡Œç¯å¢ƒï¼Œæ¨¡å‹åœ¨æ­¤ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œä¸»è¦æ¨¡å—åŒ…æ‹¬ä»£ç ç”Ÿæˆã€æ‰§è¡Œå’Œç»“æœåé¦ˆã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºæå‡ºäº†ZeroTIRæ–¹æ³•ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ç›‘ç£ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œè‡ªä¸»å­¦ä¹ å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨å·¥å…·è¿›è¡Œæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¾ç½®äº†é€‚å½“çš„å¥–åŠ±æœºåˆ¶ä»¥é¼“åŠ±è‡ªå‘ä»£ç æ‰§è¡Œï¼Œé‡‡ç”¨äº†æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç¡®ä¿äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯é‡å¤æ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZeroTIRåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„éå·¥å…·å¼ºåŒ–å­¦ä¹ æ–¹æ³•ZeroRLï¼Œå…·ä½“è¡¨ç°ä¸ºè‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡å’Œä»»åŠ¡å‡†ç¡®æ€§å‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å·¥å…·ä½¿ç”¨çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€è‡ªåŠ¨åŒ–æ•°å­¦æ±‚è§£å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰ã€‚é€šè¿‡æå‡æ¨¡å‹çš„è‡ªä¸»å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„æ•°å­¦å’Œç§‘å­¦è®¡ç®—ä»»åŠ¡ä¸­å®ç°é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯èƒ½å¯¹æ•™è‚²å’Œç§‘ç ”é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\_async\_pipline}.

