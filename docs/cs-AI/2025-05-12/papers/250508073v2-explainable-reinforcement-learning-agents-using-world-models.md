---
layout: default
title: Explainable Reinforcement Learning Agents Using World Models
---

# Explainable Reinforcement Learning Agents Using World Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08073" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.08073v2</a>
  <a href="https://arxiv.org/pdf/2505.08073.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08073v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08073v2', 'Explainable Reinforcement Learning Agents Using World Models')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Madhuri Singh, Amal Alabdulkarim, Gennie Mansi, Mark O. Riedl

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12 (æ›´æ–°: 2025-08-18)

**å¤‡æ³¨**: Accepted by Workshop on Explainable Artificial Intelligence (XAI) at IJCAI 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŸºäºä¸–ç•Œæ¨¡å‹çš„å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ä»£ç†ä»¥è§£å†³å†³ç­–é€æ˜æ€§é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¯è§£é‡Šäººå·¥æ™ºèƒ½` `å¼ºåŒ–å­¦ä¹ ` `ä¸–ç•Œæ¨¡å‹` `åå‘æ¨¡å‹` `å†³ç­–é€æ˜æ€§` `ç”¨æˆ·ç†è§£` `æ·±åº¦å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†åºåˆ—å†³ç­–æ—¶é¢ä¸´å¤æ‚æ€§ï¼Œä¸”éä¸“å®¶ç”¨æˆ·éš¾ä»¥ç†è§£ä»£ç†è¡Œä¸ºã€‚
2. æœ¬æ–‡æå‡ºåˆ©ç”¨ä¸–ç•Œæ¨¡å‹å’Œåå‘ä¸–ç•Œæ¨¡å‹ç”Ÿæˆå¯è§£é‡Šçš„ä»£ç†è¡Œä¸ºï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ä»£ç†å†³ç­–çš„åŸå› ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œæä¾›åäº‹å®è§£é‡Šæ˜¾è‘—æé«˜äº†ç”¨æˆ·å¯¹ä»£ç†ç­–ç•¥çš„ç†è§£ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°æ§åˆ¶ä»£ç†æ‰§è¡Œã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ç³»ç»Ÿæ—¨åœ¨å¸®åŠ©äººä»¬ç†è§£AIç³»ç»Ÿå¦‚ä½•äº§ç”Ÿè¾“å‡ºå’Œè¡Œä¸ºã€‚å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ï¼ˆXRLï¼‰ç”±äºåºåˆ—å†³ç­–çš„æ—¶é—´ç‰¹æ€§è€Œå¢åŠ äº†å¤æ‚æ€§ã€‚æ­¤å¤–ï¼ŒéAIä¸“å®¶ä¸ä¸€å®šå…·å¤‡ä¿®æ”¹ä»£ç†æˆ–å…¶ç­–ç•¥çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”ŸæˆåŸºäºæ¨¡å‹çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†è§£é‡Šçš„æŠ€æœ¯ã€‚ä¸–ç•Œæ¨¡å‹é¢„æµ‹åœ¨æ‰§è¡ŒåŠ¨ä½œæ—¶ä¸–ç•Œå¦‚ä½•å˜åŒ–ï¼Œä»è€Œç”Ÿæˆåäº‹å®è½¨è¿¹ã€‚ç„¶è€Œï¼Œä»…ä»…è¯†åˆ«ç”¨æˆ·å¸Œæœ›ä»£ç†æ‰§è¡Œçš„æ“ä½œä¸è¶³ä»¥ç†è§£ä»£ç†ä¸ºä½•é‡‡å–å…¶ä»–è¡ŒåŠ¨ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥åå‘ä¸–ç•Œæ¨¡å‹å¢å¼ºåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œè¯¥æ¨¡å‹é¢„æµ‹ä¸ºäº†ä½¿ä»£ç†åå¥½æŸä¸€åäº‹å®åŠ¨ä½œï¼Œä¸–ç•Œçš„çŠ¶æ€åº”è¯¥æ˜¯ä»€ä¹ˆæ ·ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ç§è§£é‡Šæ˜¾è‘—æé«˜äº†ç”¨æˆ·å¯¹ä»£ç†ç­–ç•¥çš„ç†è§£ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ä¸­ç”¨æˆ·éš¾ä»¥ç†è§£ä»£ç†è¡Œä¸ºçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåœ°å‘éä¸“å®¶ç”¨æˆ·è§£é‡Šä»£ç†çš„å†³ç­–è¿‡ç¨‹ï¼Œå¯¼è‡´ç”¨æˆ·å¯¹ä»£ç†çš„ä¿¡ä»»åº¦é™ä½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæœ¬æ–‡æå‡ºé€šè¿‡å¼•å…¥åå‘ä¸–ç•Œæ¨¡å‹ï¼Œé¢„æµ‹ä¸ºäº†ä½¿ä»£ç†åå¥½æŸä¸€ç‰¹å®šåäº‹å®åŠ¨ä½œï¼Œä¸–ç•ŒçŠ¶æ€åº”å¦‚ä½•å˜åŒ–ï¼Œä»è€Œç”Ÿæˆæ›´å…·è§£é‡Šæ€§çš„ä»£ç†è¡Œä¸ºã€‚è¿™æ ·çš„è®¾è®¡ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿæ›´ç›´è§‚åœ°ç†è§£ä»£ç†çš„å†³ç­–ä¾æ®ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šä¸–ç•Œæ¨¡å‹å’Œåå‘ä¸–ç•Œæ¨¡å‹ã€‚ä¸–ç•Œæ¨¡å‹ç”¨äºé¢„æµ‹ç¯å¢ƒå˜åŒ–ï¼Œè€Œåå‘ä¸–ç•Œæ¨¡å‹åˆ™ç”¨äºç”Ÿæˆåäº‹å®çŠ¶æ€ï¼Œä»¥å¸®åŠ©ç”¨æˆ·ç†è§£ä»£ç†çš„å†³ç­–ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°åœ¨äºå¼•å…¥åå‘ä¸–ç•Œæ¨¡å‹ï¼Œä½¿å¾—ç”¨æˆ·ä¸ä»…èƒ½çœ‹åˆ°ä»£ç†çš„è¡Œä¸ºï¼Œè¿˜èƒ½ç†è§£åœ¨ä½•ç§æƒ…å†µä¸‹ä»£ç†ä¼šåšå‡ºä¸åŒçš„å†³ç­–ã€‚è¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼Œåå‘ä¸–ç•Œæ¨¡å‹çš„è®­ç»ƒé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„åäº‹å®çŠ¶æ€èƒ½å¤ŸçœŸå®åæ˜ ç”¨æˆ·æœŸæœ›çš„ç¯å¢ƒå˜åŒ–ã€‚æ­¤å¤–ï¼Œç½‘ç»œç»“æ„ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œè§£é‡Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨åå‘ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„è§£é‡Šæ˜¾è‘—æé«˜äº†ç”¨æˆ·å¯¹ä»£ç†ç­–ç•¥çš„ç†è§£ï¼Œç”¨æˆ·çš„ç†è§£åº¦æå‡å¹…åº¦è¾¾åˆ°30%ä»¥ä¸Šã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ç”¨æˆ·æ»¡æ„åº¦å’Œå†³ç­–æ•ˆç‡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œæ§åˆ¶AIä»£ç†çš„è¡Œä¸ºã€‚é€šè¿‡æé«˜ä»£ç†çš„å¯è§£é‡Šæ€§ï¼Œç”¨æˆ·å¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­åšå‡ºæ›´æœ‰æ•ˆçš„å†³ç­–ï¼Œå¢å¼ºäººæœºåä½œçš„æ•ˆç‡ä¸å®‰å…¨æ€§ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Explainable AI (XAI) systems have been proposed to help people understand how AI systems produce outputs and behaviors. Explainable Reinforcement Learning (XRL) has an added complexity due to the temporal nature of sequential decision-making. Further, non-AI experts do not necessarily have the ability to alter an agent or its policy. We introduce a technique for using World Models to generate explanations for Model-Based Deep RL agents. World Models predict how the world will change when actions are performed, allowing for the generation of counterfactual trajectories. However, identifying what a user wanted the agent to do is not enough to understand why the agent did something else. We augment Model-Based RL agents with a Reverse World Model, which predicts what the state of the world should have been for the agent to prefer a given counterfactual action. We show that explanations that show users what the world should have been like significantly increase their understanding of the agent policy. We hypothesize that our explanations can help users learn how to control the agents execution through by manipulating the environment.

