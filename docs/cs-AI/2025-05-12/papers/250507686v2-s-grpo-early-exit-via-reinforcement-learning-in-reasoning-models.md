---
layout: default
title: S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models
---

# S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07686" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.07686v2</a>
  <a href="https://arxiv.org/pdf/2505.07686.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07686v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07686v2', 'S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Muzhi Dai, Chenxu Yang, Qingyi Si

**ÂàÜÁ±ª**: cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-12 (Êõ¥Êñ∞: 2025-05-17)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫S-GRPO‰ª•Ëß£ÂÜ≥Êé®ÁêÜÊ®°Âûã‰∏≠ÁöÑËøáÂ∫¶ÊÄùËÄÉÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)** **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Êé®ÁêÜÊ®°Âûã` `Âº∫ÂåñÂ≠¶‰π†` `ÈìæÂºèÊÄùÁª¥` `ËøáÂ∫¶ÊÄùËÄÉ` `Â∫èÂàóÈïøÂ∫¶ÂáèÂ∞ë` `ÂáÜÁ°ÆÊÄßÊèêÂçá` `Â•ñÂä±Êú∫Âà∂` `Êô∫ËÉΩÈóÆÁ≠î`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÊé®ÁêÜÊ®°ÂûãÂú®ÈìæÂºèÊÄùÁª¥ÁîüÊàê‰∏≠Â≠òÂú®ËøáÂ∫¶ÊÄùËÄÉÁöÑÂÜó‰ΩôÈóÆÈ¢òÔºåÂØºËá¥Êé®ÁêÜÊïàÁéá‰Ωé‰∏ã„ÄÇ
2. Êú¨ÊñáÊèêÂá∫S-GRPOÔºåÈÄöËøá‰∏≤Ë°åÈÄâÊã©Êé®ÁêÜË∑ØÂæÑ‰∏≠ÁöÑÊó∂Èó¥‰ΩçÁΩÆÔºå‰øÉËøõÊó©ÊúüÊÄùËÄÉÈÄÄÂá∫ÔºåÂáèÂ∞ëÂÜó‰Ωô„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåS-GRPOÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÂáèÂ∞ë‰∫ÜÂ∫èÂàóÈïøÂ∫¶ÔºåÂπ∂ÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÈöèÁùÄÊµãËØïÊó∂Êâ©Â±ïÊàê‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁ§æÂå∫ÁöÑÁ†îÁ©∂ÁÉ≠ÁÇπÔºåÂÖàËøõÁöÑÂêéËÆ≠ÁªÉÊñπÊ≥ïË∂äÊù•Ë∂äÂº∫Ë∞ÉÂª∂ÈïøÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÁîüÊàêÈïøÂ∫¶Ôºå‰ªéËÄåÂ¢ûÂº∫Êé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËøëÊúüÁ†îÁ©∂Ë°®ÊòéÔºåÊé®ÁêÜÊ®°ÂûãÔºàÁîöËá≥Qwen3ÔºâÂú®CoTÁîüÊàê‰∏≠ÊôÆÈÅçÂ≠òÂú®ËøáÂ∫¶ÊÄùËÄÉÁöÑÂÜó‰ΩôÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂº∫ÂåñÂ≠¶‰π†ËåÉÂºè‚Äî‚Äî‰∏≤Ë°åÁªÑË°∞ÂáèÂ•ñÂä±Á≠ñÁï•‰ºòÂåñÔºàS-GRPOÔºâÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÈöêÂºèËØÑ‰º∞‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÁöÑÂÖÖÂàÜÊÄßÔºå‰ªéËÄå‰øÉËøõCoTÁîüÊàê‰∏≠ÁöÑÊó©ÊúüÈÄÄÂá∫„ÄÇ‰∏éÂπ∂Ë°åÁªÑÁöÑGRPO‰∏çÂêåÔºåS-GRPO‰ªÖÈááÊ†∑‰∏ÄÊù°Êé®ÁêÜË∑ØÂæÑÔºåÂπ∂‰∏≤Ë°åÈÄâÊã©Ë∑ØÂæÑ‰∏≠ÁöÑÂ§ö‰∏™Êó∂Èó¥‰ΩçÁΩÆËøõË°åÊÄùËÄÉÈÄÄÂá∫ÂíåÁõ¥Êé•ÁîüÊàêÁ≠îÊ°à„ÄÇÂÆûËØÅËØÑ‰º∞Ë°®ÊòéÔºåS-GRPO‰∏éQwen3ÂíåDeepseek-distillÁ≠âÊúÄÂÖàËøõÁöÑÊé®ÁêÜÊ®°ÂûãÂÖºÂÆπÔºåÂú®GSM8K„ÄÅAIME 2024„ÄÅAMC 2023„ÄÅMATH-500ÂíåGPQA DiamondÁ≠âÂ§öÈ°πÂü∫ÂáÜÊµãËØï‰∏≠ÔºåS-GRPOÂÆûÁé∞‰∫ÜÂ∫èÂàóÈïøÂ∫¶ÁöÑÊòæËëóÂáèÂ∞ëÔºà35.4%-61.1%ÔºâÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÔºàÁªùÂØπÊèêÂçá0.72%-6.08%Ôºâ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Êé®ÁêÜÊ®°ÂûãÂú®ÈìæÂºèÊÄùÁª¥ÁîüÊàê‰∏≠Â≠òÂú®ÁöÑËøáÂ∫¶ÊÄùËÄÉÈóÆÈ¢òÔºåÁé∞ÊúâÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÊú™ËÉΩÊúâÊïàË∞ÉËäÇ‰∏≠Èó¥Êé®ÁêÜËøáÁ®ãÔºåÂØºËá¥ÂÜó‰ΩôÊÄùËÄÉ„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöS-GRPOÈÄöËøá‰∏≤Ë°åÈÄâÊã©Êé®ÁêÜË∑ØÂæÑ‰∏≠ÁöÑÂ§ö‰∏™Êó∂Èó¥‰ΩçÁΩÆÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®ÈÄÇÂΩìÊó∂Êú∫ÊèêÂâçÈÄÄÂá∫ÊÄùËÄÉÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÂíåÁÆÄÊ¥ÅÊÄß„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöS-GRPOÁöÑÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏Ä‰∏™Êé®ÁêÜË∑ØÂæÑÈááÊ†∑Ê®°ÂùóÂíå‰∏Ä‰∏™Â•ñÂä±Êú∫Âà∂Ê®°Âùó„ÄÇÊ®°ÂûãÈ¶ñÂÖàÈÄâÊã©‰∏ÄÊù°Êé®ÁêÜË∑ØÂæÑÔºåÁÑ∂ÂêéÂú®ËØ•Ë∑ØÂæÑ‰∏äÈÄâÊã©Â§ö‰∏™Êó∂Èó¥‰ΩçÁΩÆËøõË°åÊÄùËÄÉÈÄÄÂá∫„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöS-GRPOÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂÖ∂‰∏≤Ë°åÁªÑÁöÑËÆæËÆ°Ôºå‰∏é‰º†ÁªüÁöÑÂπ∂Ë°åÁªÑÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ËØÑ‰º∞‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÁöÑÂÖÖÂàÜÊÄßÔºåÂáèÂ∞ëÂÜó‰ΩôÊÄùËÄÉ„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®Â•ñÂä±Êú∫Âà∂‰∏≠ÔºåÂØπ‰∫éÊ≠£Á°ÆÁ≠îÊ°àÔºåÂ•ñÂä±‰ºöÊ†πÊçÆÊé®ÁêÜË∑ØÂæÑÁöÑÈÄÄÂá∫‰ΩçÁΩÆÈÄêÊ∏êÂáèÂ∞ëÔºå‰ªéËÄåÈºìÂä±Ê®°ÂûãÂú®ÈÄÇÂΩìÊó∂Êú∫ÁªàÊ≠¢ÊÄùËÄÉ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåS-GRPOÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫Ü35.4%-61.1%ÁöÑÂ∫èÂàóÈïøÂ∫¶ÂáèÂ∞ëÔºåÂêåÊó∂ÂáÜÁ°ÆÊÄßÊèêÂçá‰∫Ü0.72%-6.08%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåS-GRPOÂú®Êé®ÁêÜÊ®°ÂûãÁöÑÊÄßËÉΩ‰∏äÂÖ∑ÊúâÊòæËëóÁöÑÊîπËøõÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°Êó∂„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨Êô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅÂØπËØùÁîüÊàêÂíåÊïôËÇ≤ËæÖÂä©Â∑•ÂÖ∑Á≠â„ÄÇÈÄöËøáÊèêÈ´òÊé®ÁêÜÊ®°ÂûãÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÔºåS-GRPOËÉΩÂ§üÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Êèê‰æõÊõ¥Âø´ÈÄüÂíåÂèØÈù†ÁöÑÂìçÂ∫îÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÊú™Êù•ÂΩ±Âìç„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking issue arises from the inherent limitations of conventional outcome-reward reinforcement learning, which systematically overlooks the regulation of intermediate reasoning processes. This paper introduces Serial-Group Decaying-Reward Policy Optimization (S-GRPO), a novel reinforcement learning paradigm that enables models to implicitly evaluate the sufficiency of intermediate reasoning steps, thereby facilitating early exit in CoT generation. Unlike GRPO, which samples multiple possible reasoning paths in parallel (parallel group), S-GRPO only samples one reasoning path and serially selects multiple temporal positions from the path to exit thinking and directly generate answers (serial group). For correct answers within a serial group, rewards gradually decrease based on the exit positions along the reasoning path from front to back. This design encourages the model to produce more accurate and concise thoughts, while also incentivizing early thinking termination when appropriate. Empirical evaluations demonstrate that S-GRPO is compatible with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill. Across diverse benchmarks such as GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond, S-GRPO achieves a substantial reduction in sequence length (35.4% - 61.1%) while simultaneously improving accuracy (absolute 0.72% - 6.08%).

