---
layout: default
title: Multi-source Plume Tracing via Multi-Agent Reinforcement Learning
---

# Multi-source Plume Tracing via Multi-Agent Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.08825" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.08825v1</a>
  <a href="https://arxiv.org/pdf/2505.08825.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.08825v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.08825v1', 'Multi-source Plume Tracing via Multi-Agent Reinforcement Learning')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Pedro Antonio Alarcon Granadeno, Theodore Chambers, Jane Cleland-Huang

**ÂàÜÁ±ª**: cs.MA, cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-12

**Â§áÊ≥®**: 13 pages, 7 figures

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫Â§öÊ∫êËí∏Ê±ΩËøΩË∏™ÁÆóÊ≥ï‰ª•Ëß£ÂÜ≥Â∑•‰∏öÊ±°ÊüìÊ∫êÂÆö‰ΩçÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰∫åÔºöRLÁÆóÊ≥ï‰∏éÊû∂ÊûÑ (RL & Architecture)**

**ÂÖ≥ÈîÆËØç**: `Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†` `Ê±°ÊüìÊ∫êÂÆö‰Ωç` `È´òÊñØËí∏Ê±ΩÊ®°Âûã` `ÈÉ®ÂàÜÂèØËßÇÂØüÈ©¨Â∞îÂèØÂ§´ÂçöÂºà` `Êó†‰∫∫Êú∫Áæ§‰Ωì` `ÁéØÂ¢ÉÁõëÊµã` `Ê∑±Â∫¶Â≠¶‰π†`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâÁöÑËí∏Ê±ΩËøΩË∏™ÊñπÊ≥ïÂú®Â§çÊùÇÁöÑÊπçÊµÅÁéØÂ¢É‰∏≠Ë°®Áé∞‰∏ç‰Ω≥ÔºåÈöæ‰ª•ÊúâÊïàÂÆö‰ΩçÂ§ö‰∏™Ê±°ÊüìÊ∫ê„ÄÇ
2. Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÁöÑÁÆóÊ≥ïÔºåÈÄöËøáÂª∫Ê®°‰∏∫ÈÉ®ÂàÜÂèØËßÇÂØüÈ©¨Â∞îÂèØÂ§´ÂçöÂºàÊù•Ëß£ÂÜ≥Ê±°ÊüìÊ∫êÂÆö‰ΩçÈóÆÈ¢ò„ÄÇ
3. ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÁÆóÊ≥ï‰ªÖÈúÄÊé¢Á¥¢ÁéØÂ¢ÉÁöÑ1.29%Âç≥ÂèØÊàêÂäüÂÆö‰ΩçÊ±°ÊüìÊ∫êÔºåÊòæËëó‰ºò‰∫é‰º†ÁªüÊñπÊ≥ï„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â∑•‰∏öÁÅæÈöæÂ¶Ç1984Âπ¥ÁöÑÂçöÂ∏ïÂ∞îÁÅæÈöæÂíå2015Âπ¥ÁöÑÈòøÂà©Á¥¢Â≥°Ë∞∑Ê∞î‰ΩìÊ≥ÑÊºèÂá∏Êòæ‰∫ÜÂø´ÈÄüÂèØÈù†ÁöÑËí∏Ê±ΩËøΩË∏™ÁÆóÊ≥ïÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇ‰º†ÁªüÊñπÊ≥ïÂ¶ÇÂü∫‰∫éÊ¢ØÂ∫¶ÊàñÁîüÁâ©ÂêØÂèëÁöÑÁ≠ñÁï•Âú®Áé∞ÂÆûÁöÑÊπçÊµÅÊù°‰ª∂‰∏ãÂ∏∏Â∏∏Â§±Êïà„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÔºàMARLÔºâÁÆóÊ≥ïÔºåÊó®Âú®Âà©Áî®Â∞èÂûãÊó†‰∫∫Êú∫Áæ§‰ΩìÂÆö‰ΩçÂ§ö‰∏™Á©∫Ê∞îÊ±°ÊüìÊ∫ê„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÈóÆÈ¢òÂª∫Ê®°‰∏∫ÈÉ®ÂàÜÂèØËßÇÂØüÈ©¨Â∞îÂèØÂ§´ÂçöÂºàÔºàPOMGÔºâÔºåÈááÁî®Âü∫‰∫éÈïøÁü≠ÊúüËÆ∞ÂøÜÔºàLSTMÔºâÁöÑÁâπÂÆöÂä®‰ΩúÂèåÊ∑±Â∫¶ÈÄíÂΩíQÁΩëÁªúÔºàADDRQNÔºâÔºåÊúâÊïàÂú∞‰ΩøÁî®ÂéÜÂè≤Âä®‰Ωú-ËßÇÂØüÂØπÂ∫èÂàóÊù•Ëøë‰ººÊΩúÂú®Áä∂ÊÄÅ„ÄÇ‰∏é‰ª•ÂæÄÁ†îÁ©∂‰∏çÂêåÔºåÊàë‰ª¨‰ΩøÁî®Âü∫‰∫éÈ´òÊñØËí∏Ê±ΩÊ®°ÂûãÔºàGPMÔºâÁöÑÈÄöÁî®‰ªøÁúüÁéØÂ¢ÉÔºåËûçÂÖ•‰∫Ü‰∏âÁª¥ÁéØÂ¢É„ÄÅ‰º†ÊÑüÂô®Âô™Â£∞„ÄÅÂ§ö‰∏™‰∫§‰∫íÊô∫ËÉΩ‰ΩìÂíåÂ§ö‰∏™Ëí∏Ê±ΩÊ∫êÁ≠âÁé∞ÂÆûÂÖÉÁ¥†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÁÆóÊ≥ïÊòæËëó‰ºò‰∫é‰º†ÁªüÊñπÊ≥ï„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöÊú¨ÊñáÊó®Âú®Ëß£ÂÜ≥Âú®Â§çÊùÇÊπçÊµÅÁéØÂ¢É‰∏≠ÂÆö‰ΩçÂ§ö‰∏™Á©∫Ê∞îÊ±°ÊüìÊ∫êÁöÑÂÖ∑‰ΩìÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑ‰º†ÁªüÊñπÊ≥ïÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÂ∏∏Â∏∏Êó†Ê≥ïÊúâÊïàÂ∑•‰ΩúÔºåÂØºËá¥ÂÆö‰ΩçÁ≤æÂ∫¶‰ΩéÂíåÂìçÂ∫îÊó∂Èó¥Èïø„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉËß£ÂÜ≥ÊÄùË∑ØÊòØÈááÁî®Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÔºàMARLÔºâÁÆóÊ≥ïÔºåÂ∞ÜÈóÆÈ¢òÂª∫Ê®°‰∏∫ÈÉ®ÂàÜÂèØËßÇÂØüÈ©¨Â∞îÂèØÂ§´ÂçöÂºàÔºàPOMGÔºâÔºåÂà©Áî®Â∞èÂûãÊó†‰∫∫Êú∫Áæ§‰ΩìËøõË°åÊ±°ÊüìÊ∫êÁöÑÂÆö‰Ωç„ÄÇÈÄöËøá‰ΩøÁî®ÂéÜÂè≤Âä®‰Ωú-ËßÇÂØüÂØπÂ∫èÂàóÔºåÁÆóÊ≥ïËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÂ§çÊùÇÁéØÂ¢É„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨Â§ö‰∏™Êô∫ËÉΩ‰ΩìÂú®‰∏âÁª¥ÁéØÂ¢É‰∏≠ËøõË°åÊé¢Á¥¢ÂíåÂ≠¶‰π†Ôºå‰ΩøÁî®Âü∫‰∫éLSTMÁöÑADDRQNÊù•Â§ÑÁêÜËæìÂÖ•ÁöÑÂéÜÂè≤Êï∞ÊçÆ„ÄÇ‰∏ªË¶ÅÊ®°ÂùóÂåÖÊã¨ÁéØÂ¢ÉÂª∫Ê®°„ÄÅÊô∫ËÉΩ‰ΩìÂÜ≥Á≠ñÂíåÂ≠¶‰π†ËøáÁ®ã„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞Âú®‰∫éÂ∞ÜÂä®‰ΩúÂéÜÂè≤‰Ωú‰∏∫ËæìÂÖ•ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåËøôÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂú®ÈÉ®ÂàÜÂèØËßÇÂØüÁéØÂ¢É‰∏≠ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÁÆóÊ≥ïÂú®Â§çÊùÇÁéØÂ¢É‰∏≠Ë°®Áé∞Âá∫Êõ¥È´òÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂú®ÊäÄÊúØÁªÜËäÇ‰∏äÔºå‰ΩøÁî®‰∫ÜLSTMÁΩëÁªúÁªìÊûÑÊù•Â§ÑÁêÜÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÔºåËÆæËÆ°‰∫ÜÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞‰ª•‰ºòÂåñÊô∫ËÉΩ‰ΩìÁöÑÂ≠¶‰π†ËøáÁ®ãÔºåÂπ∂Âú®‰ªøÁúüÁéØÂ¢É‰∏≠ÂºïÂÖ•‰∫Ü‰º†ÊÑüÂô®Âô™Â£∞ÂíåÂ§ö‰∏™‰∫§‰∫íÊô∫ËÉΩ‰ΩìÁöÑÂõ†Á¥†„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊèêÂá∫ÁöÑÁÆóÊ≥ïÂú®ÁéØÂ¢ÉÊé¢Á¥¢ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ªÖÈúÄÊé¢Á¥¢1.29%ÁöÑÁéØÂ¢ÉÂç≥ÂèØÊàêÂäüÂÆö‰ΩçÊ±°ÊüìÊ∫êÔºåÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇËøô‰∏ÄÊÄßËÉΩÊèêÂçáÂ±ïÁ§∫‰∫ÜÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÁéØÂ¢ÉÁõëÊµã„ÄÅÁÅæÈöæÂìçÂ∫îÂíåÂüéÂ∏ÇÁ©∫Ê∞îË¥®ÈáèÁÆ°ÁêÜÁ≠â„ÄÇÈÄöËøáÊèêÈ´òÊ±°ÊüìÊ∫êÂÆö‰ΩçÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄßÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞‰øùÊä§ÂÖ¨ÂÖ±ÂÅ•Â∫∑ÂíåÁéØÂ¢ÉÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûÈôÖ‰ª∑ÂÄºÂíåÁ§æ‰ºöÂΩ±Âìç„ÄÇÊú™Êù•ÔºåËØ•ÊäÄÊúØÂèØÊâ©Â±ïËá≥Êõ¥Â§çÊùÇÁöÑÁéØÂ¢ÉÂíåÊõ¥Â§öÁ±ªÂûãÁöÑÊ±°ÊüìÊ∫êËøΩË∏™„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon gas leak (2015) demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment. Traditional methods, such as gradient-based or biologically inspired approaches, often fail in realistic, turbulent conditions. To address these challenges, we present a Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing multiple airborne pollution sources using a swarm of small uncrewed aerial systems (sUAS). Our method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical action-observation pairs, effectively approximating latent states. Unlike prior work, we use a general-purpose simulation environment based on the Gaussian Plume Model (GPM), incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources. The incorporation of action histories as part of the inputs further enhances the adaptability of our model in complex, partially observable environments. Extensive simulations show that our algorithm significantly outperforms conventional approaches. Specifically, our model allows agents to explore only 1.29\% of the environment to successfully locate pollution sources.

