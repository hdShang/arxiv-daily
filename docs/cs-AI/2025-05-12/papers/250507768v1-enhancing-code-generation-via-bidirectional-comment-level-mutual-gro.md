---
layout: default
title: Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding
---

# Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.07768" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.07768v1</a>
  <a href="https://arxiv.org/pdf/2505.07768.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.07768v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.07768v1', 'Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yifeng Di, Tianyi Zhang

**åˆ†ç±»**: cs.SE, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-12

**å¤‡æ³¨**: Accepted to ICSE 2025

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºåŒå‘æ³¨é‡Šçº§äº’ç›¸åŸºç¡€ä»¥å¢å¼ºä»£ç ç”Ÿæˆèƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `ä»£ç ç”Ÿæˆ` `å¤§å‹è¯­è¨€æ¨¡å‹` `äº’ç›¸åŸºç¡€` `æ³¨é‡Šç”Ÿæˆ` `ç”¨æˆ·åé¦ˆ` `è½¯ä»¶å¼€å‘` `äººå·¥æ™ºèƒ½`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„LLMç”Ÿæˆä»£ç å­˜åœ¨åŠŸèƒ½é”™è¯¯ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œå¼€å‘è€…éš¾ä»¥æ£€æŸ¥å’Œä¿®å¤è¿™äº›é”™è¯¯ï¼Œå½±å“äº†ç”Ÿäº§åŠ›ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäº’ç›¸åŸºç¡€ç†è®ºçš„äº’åŠ¨æ–¹æ³•ï¼Œé€šè¿‡ä»£ç æ³¨é‡Šä¿ƒè¿›å¼€å‘è€…ä¸LLMä¹‹é—´çš„å…±äº«ç†è§£ï¼Œæå‡ä»£ç ç”Ÿæˆè´¨é‡ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ–¹æ³•çš„å‚ä¸è€…åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­å®Œæˆé€Ÿåº¦æé«˜16.7%ï¼Œä»»åŠ¡æˆåŠŸç‡æå‡10.5%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœæå‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å±•ç°äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œä½†ç”Ÿæˆçš„ä»£ç ä»å­˜åœ¨å¤šç§åŠŸèƒ½é”™è¯¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚å¼€å‘è€…åœ¨æ£€æŸ¥å’Œä¿®å¤LLMç”Ÿæˆçš„é”™è¯¯ä»£ç æ—¶å¸¸å¸¸é¢ä¸´å›°éš¾ï¼Œé™ä½äº†ä»–ä»¬çš„ç”Ÿäº§åŠ›å’Œå¯¹LLMçš„ä¿¡ä»»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§äº’åŠ¨æ–¹æ³•ï¼Œåˆ©ç”¨ä»£ç æ³¨é‡Šä½œä¸ºå¼€å‘è€…ä¸LLMä¹‹é—´å»ºç«‹å…±äº«ç†è§£çš„åª’ä»‹ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿è¿›è¡Œä»£ç ç”Ÿæˆã€å†…è”æ³¨é‡Šç”Ÿæˆå’Œä¸Šä¸‹æ–‡ç”¨æˆ·åé¦ˆï¼Œä¿ƒè¿›äº†è¿­ä»£åŸºç¡€çš„å»ºç«‹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæµè¡ŒåŸºå‡†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºæ˜¾è‘—æå‡äº†å¤šç§æœ€å…ˆè¿›çš„LLMçš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œä»£ç -davinci-002åœ¨HumanEvalä¸Šçš„pass@1æå‡äº†17.1%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ç”¨æˆ·ç ”ç©¶ï¼Œç»“æœè¡¨æ˜å‚ä¸è€…ä½¿ç”¨è¯¥æ–¹æ³•å®Œæˆç¼–ç¨‹ä»»åŠ¡çš„é€Ÿåº¦æé«˜äº†16.7%ï¼Œä»»åŠ¡æˆåŠŸç‡æå‡äº†10.5%ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³LLMç”Ÿæˆä»£ç ä¸­çš„åŠŸèƒ½é”™è¯¯é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸­ï¼Œå¼€å‘è€…éš¾ä»¥æœ‰æ•ˆæ£€æŸ¥å’Œä¿®å¤ç”Ÿæˆçš„é”™è¯¯ä»£ç ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºé€šè¿‡ä»£ç æ³¨é‡Šä½œä¸ºåª’ä»‹ï¼Œä¿ƒè¿›å¼€å‘è€…ä¸LLMä¹‹é—´çš„äº’åŠ¨ï¼Œå»ºç«‹å…±äº«ç†è§£ï¼Œä»è€Œæé«˜ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¼€å‘è€…çš„ä¿¡ä»»ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šä»£ç ç”Ÿæˆã€å†…è”æ³¨é‡Šç”Ÿæˆå’Œä¸Šä¸‹æ–‡ç”¨æˆ·åé¦ˆã€‚é€šè¿‡äº¤æ›¿è¿›è¡Œè¿™äº›æ¨¡å—ï¼Œå½¢æˆä¸€ä¸ªè¿­ä»£çš„äº’åŠ¨è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†åŒå‘æ³¨é‡Šçº§äº’ç›¸åŸºç¡€çš„æ¦‚å¿µï¼Œä½¿å¾—å¼€å‘è€…èƒ½å¤Ÿé€šè¿‡å¯ç¼–è¾‘çš„æ³¨é‡Šä¸LLMè¿›è¡Œæœ‰æ•ˆçš„æ²Ÿé€šï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆä»£ç çš„è´¨é‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç”Ÿæˆçš„ä»£ç ä¸å¼€å‘è€…æ„å›¾ä¹‹é—´çš„å¯¹é½ï¼ŒåŒæ—¶è®¾è®¡äº†çµæ´»çš„æ³¨é‡Šç¼–è¾‘æœºåˆ¶ï¼Œä»¥ä¾¿å¼€å‘è€…èƒ½å¤Ÿå®æ—¶åé¦ˆå’Œè°ƒæ•´ç”Ÿæˆå†…å®¹ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼Œä»£ç -davinci-002åœ¨HumanEvalåŸºå‡†æµ‹è¯•ä¸­pass@1æå‡äº†17.1%ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œå‚ä¸è€…åœ¨ä½¿ç”¨è¯¥æ–¹æ³•æ—¶ï¼Œç¼–ç¨‹ä»»åŠ¡å®Œæˆé€Ÿåº¦æé«˜äº†16.7%ï¼Œä»»åŠ¡æˆåŠŸç‡æå‡äº†10.5%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘å·¥å…·ã€è‡ªåŠ¨åŒ–ç¼–ç¨‹åŠ©æ‰‹å’Œæ•™è‚²é¢†åŸŸçš„ç¼–ç¨‹æ•™å­¦ã€‚é€šè¿‡æé«˜ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå¼€å‘è€…çš„ä¿¡ä»»ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¼€å‘æ•ˆç‡ï¼Œé™ä½é”™è¯¯ç‡ï¼Œæœªæ¥å¯èƒ½å¯¹è½¯ä»¶å¼€å‘æµç¨‹äº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.

