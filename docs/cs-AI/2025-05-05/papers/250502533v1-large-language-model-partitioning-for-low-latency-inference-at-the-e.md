---
layout: default
title: Large Language Model Partitioning for Low-Latency Inference at the Edge
---

# Large Language Model Partitioning for Low-Latency Inference at the Edge

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02533" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02533v1</a>
  <a href="https://arxiv.org/pdf/2505.02533.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02533v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02533v1', 'Large Language Model Partitioning for Low-Latency Inference at the Edge')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Dimitrios Kafetzis, Ramin Khalili, Iordanis Koutsopoulos

**åˆ†ç±»**: cs.DC, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºèµ„æºæ„ŸçŸ¥çš„Transformeråˆ†åŒºç®—æ³•ä»¥é™ä½è¾¹ç¼˜æ¨ç†å»¶è¿Ÿ**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `è¾¹ç¼˜è®¡ç®—` `Transformer` `æ¨ç†ä¼˜åŒ–` `åŠ¨æ€åˆ†åŒº` `æ³¨æ„åŠ›æœºåˆ¶` `èµ„æºç®¡ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„åŸºäºå±‚çš„åˆ†åŒºæ–¹æ³•åœ¨è¾¹ç¼˜ç¯å¢ƒä¸­å®¹æ˜“å¯¼è‡´å†…å­˜è¿‡è½½å’Œé«˜æ¨ç†å»¶è¿Ÿï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶åº”ç”¨éœ€æ±‚ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§èµ„æºæ„ŸçŸ¥çš„Transformeråˆ†åŒºç®—æ³•ï¼Œé€šè¿‡åœ¨æ ‡è®°ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€æ›´æ–°åˆ†åŒºå†³ç­–ï¼Œä¼˜åŒ–èµ„æºåˆ©ç”¨ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å°è§„æ¨¡è®¾å¤‡ä¸Šå®ç°äº†æ¥è¿‘æœ€ä¼˜è§£çš„å»¶è¿Ÿï¼Œå¹¶åœ¨å¤§è§„æ¨¡æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŸºäºè‡ªå›å½’çš„è§£ç å™¨Transformeré€ä¸ªç”Ÿæˆæ–‡æœ¬æ ‡è®°ï¼Œéšç€ç”Ÿæˆçš„æ ‡è®°æ•°é‡å¢åŠ ï¼Œå†…å­˜å’Œè®¡ç®—è´Ÿè½½ä¹Ÿéšä¹‹ä¸Šå‡ã€‚ç°æœ‰çš„åŸºäºå±‚çš„åˆ†åŒºæ–¹æ³•åœ¨èµ„æºå—é™çš„è¾¹ç¼˜ç¯å¢ƒä¸­å¸¸å¸¸å¯¼è‡´å†…å­˜è¿‡è½½æˆ–é«˜æ¨ç†å»¶è¿Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§èµ„æºæ„ŸçŸ¥çš„Transformeræ¶æ„åˆ†åŒºç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨æ ‡è®°ç”Ÿæˆè¿‡ç¨‹ä¸­å®šæœŸæ›´æ–°åˆ†åŒºå†³ç­–ã€‚é€šè¿‡åœ¨æ³¨æ„åŠ›å¤´çº§åˆ«è¿›è¡Œåˆ†åŒºï¼Œå…è®¸åŠ¨æ€è¿ç§»ï¼Œæ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å°è§„æ¨¡è®¾ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•çš„å»¶è¿Ÿåœ¨æœ€ä¼˜è§£çš„15%åˆ°20%ä¹‹å†…ï¼Œå¹¶åœ¨å¤§è§„æ¨¡æµ‹è¯•ä¸­ç›¸æ¯”äºç°æœ‰çš„åˆ†å±‚åˆ†åŒºæ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ•ˆç‡ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæ¨ç†æ—¶çš„å†…å­˜è¿‡è½½å’Œé«˜å»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰çš„å±‚çº§åˆ†åŒºæ–¹æ³•åœ¨èµ„æºå—é™ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹åŠ¨æ€å˜åŒ–çš„èµ„æºéœ€æ±‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå³æ—¶è®¾å¤‡èµ„æºå¯ç”¨æ€§å’Œç½‘ç»œå¸¦å®½çš„åŠ¨æ€åˆ†åŒºç®—æ³•ï¼Œé€šè¿‡åœ¨æ³¨æ„åŠ›å¤´çº§åˆ«è¿›è¡Œåˆ†åŒºï¼Œå…è®¸åœ¨èµ„æºç´§å¼ æ—¶è¿›è¡ŒåŠ¨æ€è¿ç§»ï¼Œä»¥é™ä½æ¨ç†å»¶è¿Ÿã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬åˆ†åŒºå†³ç­–æ¨¡å—ã€åŠ¨æ€è¿ç§»æ¨¡å—å’Œæ¨ç†æ‰§è¡Œæ¨¡å—ã€‚åˆ†åŒºå†³ç­–æ¨¡å—å®šæœŸè¯„ä¼°è®¾å¤‡èµ„æºï¼ŒåŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›å¤´çš„åˆ†é…ï¼Œè¿ç§»æ¨¡å—è´Ÿè´£åœ¨è®¾å¤‡é—´è¿ç§»è®¡ç®—è´Ÿè½½ï¼Œæ¨ç†æ‰§è¡Œæ¨¡å—åˆ™è´Ÿè´£å®é™…çš„æ¨ç†è¿‡ç¨‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºåœ¨æ³¨æ„åŠ›å¤´çº§åˆ«è¿›è¡Œåˆ†åŒºï¼Œå¹¶å…è®¸åŠ¨æ€è¿ç§»ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å±‚çº§åˆ†åŒºæ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°åº”å¯¹èµ„æºå˜åŒ–ã€‚

**å…³é”®è®¾è®¡**ï¼šç®—æ³•åœ¨åˆæ¬¡æ‰§è¡Œæ—¶å°†æ³¨æ„åŠ›å¤´åˆ†é…åˆ°ä¸åŒè®¾å¤‡ï¼Œåç»­æ‰§è¡Œä¸­æ ¹æ®èµ„æºæƒ…å†µè°ƒæ•´åˆ†é…ï¼Œç¡®ä¿è¿ç§»å»¶è¿Ÿå’Œæ¨ç†å»¶è¿Ÿä¹‹å’Œä¿æŒåœ¨è¾ƒä½æ°´å¹³ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å°è§„æ¨¡è®¾ç½®ï¼ˆ3-5ä¸ªè®¾å¤‡ï¼‰ä¸‹ï¼Œæå‡ºçš„æ–¹æ³•çš„æ¨ç†å»¶è¿Ÿåœ¨æœ€ä¼˜è§£çš„15%åˆ°20%ä¹‹é—´ï¼Œè€Œåœ¨å¤§è§„æ¨¡æµ‹è¯•ä¸­ï¼Œç›¸æ¯”äºç°æœ‰çš„åˆ†å±‚åˆ†åŒºæ–¹æ³•ï¼Œæ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨è®¾å¤‡ä¸Šçš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œå®æ—¶æ™ºèƒ½åŠ©æ‰‹ç­‰åœºæ™¯ã€‚é€šè¿‡é™ä½æ¨ç†å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œæ”¯æŒæ›´å¤æ‚çš„å®æ—¶åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼å’Œå¹¿æ³›çš„æœªæ¥å½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Large Language Models (LLMs) based on autoregressive, decoder-only Transformers generate text one token at a time, where a token represents a discrete unit of text. As each newly produced token is appended to the partial output sequence, the length grows and so does the memory and compute load, due to the expanding key-value caches, which store intermediate representations of all previously generated tokens in the multi-head attention (MHA) layer. As this iterative process steadily increases memory and compute demands, layer-based partitioning in resource-constrained edge environments often results in memory overload or high inference latency. To address this and reduce inference latency, we propose a resource-aware Transformer architecture partitioning algorithm, where the partitioning decision is updated at regular intervals during token generation. The approach is myopic in that it is based on instantaneous information about device resource availability and network link bandwidths. When first executed, the algorithm places blocks on devices, and in later executions, it migrates these blocks among devices so that the sum of migration delay and inference delay remains low. Our approach partitions the decoder at the attention head level, co-locating each attention head with its key-value cache and allowing dynamic migrations whenever resources become tight. By allocating different attention heads to different devices, we exploit parallel execution of attention heads and thus achieve substantial reductions in inference delays. Our experiments show that in small-scale settings (3-5 devices), the proposed method achieves within 15 to 20 percent of an exact optimal solver's latency, while in larger-scale tests it achieves notable improvements in inference speed and memory usage compared to state-of-the-art layer-based partitioning approaches.

