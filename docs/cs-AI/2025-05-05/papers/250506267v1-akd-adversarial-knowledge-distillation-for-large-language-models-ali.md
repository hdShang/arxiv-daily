---
layout: default
title: AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks
---

# AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.06267" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.06267v1</a>
  <a href="https://arxiv.org/pdf/2505.06267.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.06267v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.06267v1', 'AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Ilyas Oulkadda, Julien Perez

**åˆ†ç±»**: cs.SE, cs.AI, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¯¹æŠ—çŸ¥è¯†è’¸é¦ä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç¼–ç ä»»åŠ¡ä¸­çš„å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¯¹æŠ—çŸ¥è¯†è’¸é¦` `å¤§å‹è¯­è¨€æ¨¡å‹` `ä»£ç ç”Ÿæˆ` `æ¨¡å‹è’¸é¦` `é²æ£’æ€§æå‡` `åˆæˆæ•°æ®é›†` `æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆä¸­é¢ä¸´è´¨é‡ã€å®‰å…¨æ€§å’Œå¯é æ€§ç­‰é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹è§„æ¨¡æ‰©å¤§å’Œé«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„èƒŒæ™¯ä¸‹ã€‚
2. æœ¬æ–‡æå‡ºçš„å¯¹æŠ—çŸ¥è¯†è’¸é¦ï¼ˆAKDï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¯¹æŠ—ç”Ÿæˆçš„åˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨å°†å¤§å‹æ¨¡å‹çš„èƒ½åŠ›æœ‰æ•ˆè’¸é¦åˆ°æ›´å°çš„æ¨¡å‹ä¸­ï¼Œä»è€Œæå‡å…¶æ•ˆç‡å’Œå¯é æ€§ã€‚
3. AKDæ–¹æ³•é€šè¿‡ç³»ç»Ÿçš„å‹åŠ›æµ‹è¯•å’Œæ¨ç†èƒ½åŠ›ä¼˜åŒ–ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼ŒåŒæ—¶æé«˜äº†å‚æ•°æ•ˆç‡ï¼Œå±•ç°å‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä¾‹å¦‚GitHub Copilotçš„ç”¨æˆ·è¶…è¿‡ç™¾ä¸‡ï¼Œè¿™äº›å·¥å…·åœ¨æå‡å¼€å‘è€…ç”Ÿäº§åŠ›æ–¹é¢å±•ç°äº†å˜é©æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¿«é€Ÿå¢é•¿ä¹Ÿå¸¦æ¥äº†ä»£ç è´¨é‡ã€å®‰å…¨æ€§å’Œå¯é æ€§ç­‰å…³é”®é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹æŠ—çŸ¥è¯†è’¸é¦ï¼ˆAKDï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨å¯¹æŠ—ç”Ÿæˆçš„åˆæˆæ•°æ®é›†å°†å¤§å‹æ¨¡å‹çš„èƒ½åŠ›è’¸é¦åˆ°æ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ä¸­ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°å‹åŠ›æµ‹è¯•å’Œä¼˜åŒ–ä»£ç LLMsçš„æ¨ç†èƒ½åŠ›ï¼ŒAKDä¸ºå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€å¯é æ€§å’Œå®‰å…¨æ€§æä¾›äº†æ¡†æ¶ï¼ŒåŒæ—¶æé«˜äº†å‚æ•°æ•ˆç‡ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™é¡¹å·¥ä½œæ˜¯ç¡®ä¿åœ¨ç°æœ‰æ•°æ®é™åˆ¶å’Œæ¨¡å‹æ‰§è¡Œæˆæœ¬æ•ˆç›Šä¸‹å®ç°å¯é è‡ªåŠ¨ä»£ç ç”Ÿæˆçš„é‡è¦ä¸€æ­¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¼–ç ä»»åŠ¡ä¸­ç”Ÿæˆä»£ç çš„è´¨é‡ã€å®‰å…¨æ€§å’Œå¯é æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹è§„æ¨¡æ‰©å¤§æ—¶é¢ä¸´æ”¶ç›Šé€’å‡å’Œé«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºå¯¹æŠ—çŸ¥è¯†è’¸é¦ï¼ˆAKDï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¯¹æŠ—ç”Ÿæˆçš„åˆæˆæ•°æ®é›†ï¼Œç³»ç»Ÿæ€§åœ°è’¸é¦å¤§å‹æ¨¡å‹çš„èƒ½åŠ›åˆ°å°å‹æ¨¡å‹ä¸­ï¼Œä»¥æå‡å…¶æ•ˆç‡å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šAKDæ–¹æ³•çš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—ã€æ¨¡å‹è’¸é¦æ¨¡å—å’Œæ€§èƒ½è¯„ä¼°æ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—è´Ÿè´£ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè’¸é¦æ¨¡å—åˆ™å°†è¿™äº›æ ·æœ¬ç”¨äºè®­ç»ƒå°å‹æ¨¡å‹ï¼Œæœ€åé€šè¿‡è¯„ä¼°æ¨¡å—éªŒè¯æ¨¡å‹æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šAKDçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨å¯¹æŠ—ç”Ÿæˆçš„åˆæˆæ•°æ®é›†è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä¼ ç»Ÿçš„è’¸é¦æŠ€æœ¯ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå‡å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨AKDä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥å¹³è¡¡å¯¹æŠ—æ ·æœ¬å’ŒçœŸå®æ ·æœ¬çš„å½±å“ï¼ŒåŒæ—¶è®¾è®¡äº†é€‚åº”æ€§å‚æ•°è°ƒæ•´æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ æ•ˆç‡ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­ç»è¿‡å¤šæ¬¡è°ƒä¼˜ï¼Œä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨AKDæ–¹æ³•çš„å°å‹æ¨¡å‹åœ¨å¤šä¸ªç¼–ç ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦è¾¾åˆ°20%ä»¥ä¸Šï¼Œä¸”åœ¨ç”Ÿæˆä»£ç çš„è´¨é‡å’Œå®‰å…¨æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨ä»£ç ç”Ÿæˆã€æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹å’Œè½¯ä»¶å¼€å‘å·¥å…·ç­‰ã€‚é€šè¿‡æé«˜ä»£ç ç”Ÿæˆæ¨¡å‹çš„å¯é æ€§å’Œå®‰å…¨æ€§ï¼ŒAKDæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å¼€å‘è€…çš„å·¥ä½œæ•ˆç‡ï¼Œé™ä½ä»£ç é”™è¯¯ç‡ï¼Œæ¨åŠ¨æ™ºèƒ½ç¼–ç¨‹æŠ€æœ¯çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> The widespread adoption of Large Language Models (LLMs) for code generation, exemplified by GitHub Copilot\footnote{A coding extension powered by a Code-LLM to assist in code completion tasks} surpassing a million users, highlights the transformative potential of these tools in improving developer productivity. However, this rapid growth also underscores critical concerns regarding the quality, safety, and reliability of the code they generate. As Code-LLMs evolve, they face significant challenges, including the diminishing returns of model scaling and the scarcity of new, high-quality training data. To address these issues, this paper introduces Adversarial Knowledge Distillation (AKD), a novel approach that leverages adversarially generated synthetic datasets to distill the capabilities of larger models into smaller, more efficient ones. By systematically stress-testing and refining the reasoning capabilities of Code-LLMs, AKD provides a framework for enhancing model robustness, reliability, and security while improving their parameter-efficiency. We believe this work represents a critical step toward ensuring dependable automated code generation within the constraints of existing data and the cost-efficiency of model execution.

