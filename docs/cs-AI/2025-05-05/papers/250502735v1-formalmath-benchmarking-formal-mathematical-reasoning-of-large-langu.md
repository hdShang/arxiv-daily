---
layout: default
title: "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models"
---

# FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02735" class="toolbar-btn" target="_blank">üìÑ arXiv: 2505.02735v1</a>
  <a href="https://arxiv.org/pdf/2505.02735.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02735v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02735v1', 'FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, Weiyang Liu

**ÂàÜÁ±ª**: cs.AI, cs.LG

**ÂèëÂ∏ÉÊó•Êúü**: 2025-05-05

**Â§áÊ≥®**: Technical Report v1 (33 pages, 8 figures, project page: https://sphere-ai-lab.github.io/FormalMATH/)

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ÊèêÂá∫FormalMATH‰ª•Ëß£ÂÜ≥ÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜÁöÑÂü∫ÂáÜÊµãËØïÈóÆÈ¢ò**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `ÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜ` `Âü∫ÂáÜÊµãËØï` `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Ëá™Âä®ÂΩ¢ÂºèÂåñ` `ÂÆöÁêÜËØÅÊòé` `‰∫∫Êú∫Âçè‰Ωú` `ËØ≠‰πâÈ™åËØÅ`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Ê†∏ÂøÉÈóÆÈ¢òÔºöÁé∞ÊúâÁöÑÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜÂú®ËåÉÂõ¥ÂíåËßÑÊ®°‰∏äÂ≠òÂú®ÊòæËëó‰∏çË∂≥ÔºåÈôêÂà∂‰∫ÜAIÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ
2. ÊñπÊ≥ïË¶ÅÁÇπÔºöÊèêÂá∫FormalMATHÂü∫ÂáÜÔºåÁªìÂêà‰∫∫Êú∫Âçè‰ΩúÁöÑËá™Âä®ÂΩ¢ÂºèÂåñÊµÅÁ®ãÔºåÊèêÈ´òÂΩ¢ÂºèÂåñÊïàÁéáÂπ∂Èôç‰Ωé‰∏ìÂÆ∂Ê≥®ÈáäÊàêÊú¨„ÄÇ
3. ÂÆûÈ™åÊàñÊïàÊûúÔºöËØÑ‰º∞ÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂº∫ÁöÑLLMÂÆöÁêÜËØÅÊòéÂô®Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÊàêÂäüÁéá‰ªÖ‰∏∫16.46%ÔºåÂπ∂Â≠òÂú®È¢ÜÂüüÂÅèËßÅ„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

ÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜ‰ªçÁÑ∂ÊòØ‰∫∫Â∑•Êô∫ËÉΩÈù¢‰∏¥ÁöÑÈáçÂ§ßÊåëÊàòÔºåÁé∞ÊúâÂü∫ÂáÜÂú®ËåÉÂõ¥ÂíåËßÑÊ®°‰∏äÂ≠òÂú®Â±ÄÈôê„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜFormalMATHÔºåËøôÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑLean4Âü∫ÂáÜÔºåÂåÖÂê´5560‰∏™ÂΩ¢ÂºèÈ™åËØÅÈóÆÈ¢òÔºåÊ∂µÁõñ‰ªéÈ´ò‰∏≠Â••ÊûóÂåπÂÖãÊåëÊàòÂà∞Êú¨ÁßëÂÆöÁêÜÁöÑÂ§öÊ†∑È¢ÜÂüüÔºàÂ¶Ç‰ª£Êï∞„ÄÅÂ∫îÁî®Êï∞Â≠¶„ÄÅÂæÆÁßØÂàÜ„ÄÅÊï∞ËÆ∫ÂíåÁ¶ªÊï£Êï∞Â≠¶Ôºâ„ÄÇ‰∏∫ÊèêÈ´òÊâãÂä®ÂΩ¢ÂºèÂåñÁöÑÊïàÁéáÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∫∫Êú∫Âçè‰ΩúËá™Âä®ÂΩ¢ÂºèÂåñÊµÅÁ®ãÔºåÊï¥Âêà‰∫Ü‰∏ìÁî®ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËøõË°åËØ≠Âè•Ëá™Âä®ÂΩ¢ÂºèÂåñ„ÄÅÂ§öLLMËØ≠‰πâÈ™åËØÅ‰ª•ÂèäÂü∫‰∫éÂê¶ÂÆöÁöÑÂèçÈ©≥ËøáÊª§Á≠ñÁï•„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÂ∞ΩÁÆ°ÂΩìÂâçÊúÄÂº∫Ê®°ÂûãÂú®ÂÆûÈôÖÈááÊ†∑È¢ÑÁÆó‰∏ãÁöÑÊàêÂäüÁéá‰ªÖ‰∏∫16.46%Ôºå‰ΩÜFormalMATH‰∏∫ÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜÊèê‰æõ‰∫Ü‰∏Ä‰∏™Á®≥ÂÅ•ÁöÑÂü∫ÂáÜ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáË¶ÅËß£ÂÜ≥ÁöÑÈóÆÈ¢òÊòØÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜÁöÑÂü∫ÂáÜÊµãËØï‰∏çË∂≥ÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ËåÉÂõ¥ÂíåËßÑÊ®°‰∏äÂ≠òÂú®Â±ÄÈôêÔºåÂØºËá¥AIÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÁöÑÊåëÊàò„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÈÄöËøáÊûÑÂª∫FormalMATHÂü∫ÂáÜÔºåÂà©Áî®‰∫∫Êú∫Âçè‰ΩúÁöÑËá™Âä®ÂΩ¢ÂºèÂåñÊµÅÁ®ãÊù•ÊèêÈ´òÂΩ¢ÂºèÂåñÁöÑÊïàÁéáÔºåÈôç‰Ωé‰∏ìÂÆ∂ÁöÑÊ≥®ÈáäÊàêÊú¨„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöÊï¥‰ΩìÊû∂ÊûÑÂåÖÊã¨‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºö1) ‰∏ìÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åËØ≠Âè•Ëá™Âä®ÂΩ¢ÂºèÂåñÔºõ2) Â§öLLMËØ≠‰πâÈ™åËØÅÔºõ3) Âü∫‰∫éÂê¶ÂÆöÁöÑÂèçÈ©≥ËøáÊª§Á≠ñÁï•Ôºå‰ΩøÁî®Áé∞ÊàêÁöÑLLMËØÅÊòéÂô®„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöÊúÄÈáçË¶ÅÁöÑÊäÄÊúØÂàõÊñ∞ÁÇπÂú®‰∫éÂºïÂÖ•‰∫∫Êú∫Âçè‰ΩúÁöÑËá™Âä®ÂΩ¢ÂºèÂåñÊµÅÁ®ãÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂΩ¢ÂºèÂåñÊïàÁéáÔºåÂπ∂‰øùÁïô‰∫Ü72.09%ÁöÑËØ≠Âè•Âú®ÊâãÂä®È™åËØÅÂâçÁöÑÊúâÊïàÊÄß„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨ÈÄâÊã©ÂêàÈÄÇÁöÑLLMËøõË°åËØ≠Âè•Ëá™Âä®ÂΩ¢ÂºèÂåñÔºåÈááÁî®Â§öÊ®°ÂûãÁöÑËØ≠‰πâÈ™åËØÅÊú∫Âà∂Ôºå‰ª•ÂèäËÆæËÆ°Âê¶ÂÆöËøáÊª§Á≠ñÁï•‰ª•ÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂÖ∑‰ΩìÂèÇÊï∞ËÆæÁΩÆÂíåÊçüÂ§±ÂáΩÊï∞ÁöÑÈÄâÊã©Âú®ËÆ∫Êñá‰∏≠ËøõË°å‰∫ÜËØ¶ÁªÜËÆ®ËÆ∫„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂº∫ÁöÑLLMÂÆöÁêÜËØÅÊòéÂô®Âú®ÂÆûÈôÖÈááÊ†∑È¢ÑÁÆó‰∏ãÁöÑÊàêÂäüÁéá‰ªÖ‰∏∫16.46%ÔºåÂπ∂‰∏îÂú®‰∏çÂêåÈ¢ÜÂüüÔºàÂ¶Ç‰ª£Êï∞ÂíåÂæÆÁßØÂàÜÔºâË°®Áé∞Âá∫ÊòéÊòæÁöÑÈ¢ÜÂüüÂÅèËßÅ„ÄÇËøô‰∏ÄÂèëÁé∞Âº∫Ë∞É‰∫ÜÁé∞ÊúâÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÊï∞Â≠¶ÈóÆÈ¢òÊó∂ÁöÑÂ±ÄÈôêÊÄß„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÁöÑÊΩúÂú®Â∫îÁî®È¢ÜÂüüÂåÖÊã¨ÊïôËÇ≤„ÄÅËá™Âä®ÂÆöÁêÜËØÅÊòé„ÄÅÊï∞Â≠¶ËΩØ‰ª∂ÂºÄÂèëÁ≠â„ÄÇÈÄöËøáÊèê‰æõ‰∏Ä‰∏™Ê†áÂáÜÂåñÁöÑÂü∫ÂáÜÔºåFormalMATHÂèØ‰ª•Â∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÂíåÂºÄÂèëËÄÖËØÑ‰º∞ÂíåÊîπËøõÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜÊ®°ÂûãÔºåÊé®Âä®Áõ∏ÂÖ≥ÊäÄÊúØÁöÑËøõÊ≠•‰∏éÂ∫îÁî®„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.

