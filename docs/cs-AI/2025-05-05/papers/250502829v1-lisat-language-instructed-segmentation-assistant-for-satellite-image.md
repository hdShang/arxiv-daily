---
layout: default
title: LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery
---

# LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.02829" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.02829v1</a>
  <a href="https://arxiv.org/pdf/2505.02829.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.02829v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.02829v1', 'LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jerome Quenum, Wen-Han Hsieh, Tsung-Han Wu, Ritwik Gupta, Trevor Darrell, David M. Chan

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-05

**å¤‡æ³¨**: 28 pages, 10 figures, 19 tables

**ğŸ”— ä»£ç /é¡¹ç›®**: [PROJECT_PAGE](https://lisat-bair.github.io/LISAt/)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºLISATä»¥è§£å†³å¤æ‚é¥æ„Ÿå½±åƒçš„è¯­è¨€æŒ‡å¯¼åˆ†å‰²é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `é¥æ„Ÿå½±åƒ` `è¯­è¨€æŒ‡å¯¼` `åˆ†å‰²æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `åœ°ç†ç©ºé—´æ¨ç†`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„æ¨ç†åˆ†å‰²æ¨¡å‹åœ¨å¤„ç†å¤æ‚é¥æ„Ÿå½±åƒæ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹ç”¨æˆ·çš„å¤æ‚æŸ¥è¯¢ã€‚
2. LISATé€šè¿‡ç»“åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ç§æ–°çš„æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿç†è§£å¤æ‚åœºæ™¯å¹¶è¿›è¡Œæœ‰æ•ˆåˆ†å‰²ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒLISATåœ¨é¥æ„Ÿæè¿°ä»»åŠ¡ä¸Šæ¯”ç°æœ‰æ¨¡å‹æé«˜äº†10.04%ï¼ˆBLEU-4ï¼‰ï¼Œåœ¨æ¨ç†åˆ†å‰²ä»»åŠ¡ä¸Šæå‡äº†143.36%ï¼ˆgIoUï¼‰ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å›¾åƒä¸­é¢„å®šä¹‰çš„ä¸€ç»„å¯¹è±¡ï¼Œä½†å¯¹äºå¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„æ¨ç†åˆ†å‰²æ¨¡å‹ä»å¤„äºåˆçº§é˜¶æ®µã€‚è¿‘æœŸçš„ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å¼€æ”¾é¢†åŸŸä¸­ç”Ÿæˆåˆç†çš„åˆ†å‰²æ©ç ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨å¤æ‚çš„é¥æ„Ÿå½±åƒä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†LISATï¼Œä¸€ä¸ªæ—¨åœ¨æè¿°å¤æ‚é¥æ„Ÿåœºæ™¯ã€å›ç­”ç›¸å…³é—®é¢˜å¹¶åˆ†å‰²æ„Ÿå…´è¶£å¯¹è±¡çš„è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚LISATåœ¨ä¸€ä¸ªæ–°çš„åœ°ç†ç©ºé—´æ¨ç†-åˆ†å‰²æ•°æ®é›†GRESä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¶…è¶Šäº†ç°æœ‰çš„åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œå¹¶åœ¨æ¨ç†åˆ†å‰²ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†åˆ†å‰²æ¨¡å‹åœ¨å¤æ‚é¥æ„Ÿå½±åƒä¸Šæ— æ³•æœ‰æ•ˆå¤„ç†ç”¨æˆ·å¤æ‚æŸ¥è¯¢çš„é—®é¢˜ã€‚è¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆåˆ†å‰²æ©ç æ—¶ç¼ºä¹å¯¹å¤šå¯¹è±¡çš„æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šLISATé€šè¿‡å¼•å…¥å¤šæ¨¡æ€å­¦ä¹ ï¼Œç»“åˆè§†è§‰ä¿¡æ¯ä¸è¯­è¨€æŒ‡ä»¤ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¤æ‚åœºæ™¯å¹¶è¿›è¡Œåˆ†å‰²ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šLISATçš„æ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œåˆ©ç”¨æ–°åˆ›å»ºçš„GRESæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨æ¨ç†é˜¶æ®µç”Ÿæˆåˆ†å‰²æ©ç ã€‚

**å…³é”®åˆ›æ–°**ï¼šLISATçš„ä¸»è¦åˆ›æ–°åœ¨äºå…¶é’ˆå¯¹é¥æ„Ÿå½±åƒçš„ç‰¹å®šè®¾è®¡ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆé«˜è´¨é‡çš„åˆ†å‰²ç»“æœï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æ¨¡å‹è®¾è®¡ä¸­ï¼ŒLISATé‡‡ç”¨äº†å¤šæ¨¡æ€é¢„è®­ç»ƒæ•°æ®é›†PreGRESï¼Œå¹¶ä½¿ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°å’Œç½‘ç»œç»“æ„ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨æ¨ç†åˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å…·ä½“å‚æ•°è®¾ç½®å’Œç½‘ç»œç»“æ„ç»†èŠ‚åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†æè¿°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

LISATåœ¨é¥æ„Ÿæè¿°ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹RS-GPT4Vï¼Œæå‡å¹…åº¦è¾¾åˆ°10.04%ï¼ˆBLEU-4ï¼‰ã€‚åœ¨æ¨ç†åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒLISATçš„è¡¨ç°æ›´æ˜¯è¶…è¿‡äº†ç°æœ‰çš„å¼€æ”¾é¢†åŸŸæ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°143.36%ï¼ˆgIoUï¼‰ï¼Œæ˜¾ç¤ºå‡ºå…¶å“è¶Šçš„æ€§èƒ½ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

LISATçš„ç ”ç©¶æˆæœåœ¨é¥æ„Ÿå½±åƒåˆ†æã€ç¯å¢ƒç›‘æµ‹ã€åŸå¸‚è§„åˆ’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡æé«˜å¯¹å¤æ‚åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä¸ºå†³ç­–æä¾›æ›´å‡†ç¡®çš„ä¿¡æ¯ï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Segmentation models can recognize a pre-defined set of objects in images. However, models that can reason over complex user queries that implicitly refer to multiple objects of interest are still in their infancy. Recent advances in reasoning segmentation--generating segmentation masks from complex, implicit query text--demonstrate that vision-language models can operate across an open domain and produce reasonable outputs. However, our experiments show that such models struggle with complex remote-sensing imagery. In this work, we introduce LISAt, a vision-language model designed to describe complex remote-sensing scenes, answer questions about them, and segment objects of interest. We trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES, with 27,615 annotations over 9,205 images, and a multimodal pretraining dataset, PreGRES, containing over 1 million question-answer pairs. LISAt outperforms existing geospatial foundation models such as RS-GPT4V by over 10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 % (gIoU). Our model, datasets, and code are available at https://lisat-bair.github.io/LISAt/

