---
layout: default
title: HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning
---

# HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.15011" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.15011v1</a>
  <a href="https://arxiv.org/pdf/2505.15011.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.15011v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.15011v1', 'HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Kryspin Varys, Federico Cerutti, Adam Sobey, Timothy J. Norman

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-05-21

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºæ··åˆæ–¹æ³•HAVAä»¥è§£å†³ä»·å€¼å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `ä»·å€¼å¯¹é½` `å¼ºåŒ–å­¦ä¹ ` `ç¤¾ä¼šè§„èŒƒ` `æ™ºèƒ½ä½“è¡Œä¸º` `å£°èª‰æœºåˆ¶` `æ³•å¾‹è§„èŒƒ` `å¥–åŠ±åŠ æƒ`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å°†æ³•å¾‹è§„èŒƒä¸ç¤¾ä¼šè§„èŒƒç»“åˆåˆ°å•ä¸€ç®—æ³•ä¸­çš„èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ™ºèƒ½ä½“çš„è¡Œä¸ºæ— æ³•å…¨é¢åæ˜ ç¤¾ä¼šä»·å€¼ã€‚
2. è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ç›‘æ§æ™ºèƒ½ä½“å¯¹è§„èŒƒçš„éµå®ˆæƒ…å†µï¼Œå¹¶åˆ©ç”¨å£°èª‰é‡æ¥åŠ æƒå¥–åŠ±ï¼Œä»è€Œä¿ƒè¿›ä»·å€¼å¯¹é½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆä¹¦é¢å’Œæœªä¹¦é¢è§„èŒƒçš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ‰¾åˆ°ä»·å€¼å¯¹é½çš„ç­–ç•¥ï¼Œæå‡äº†æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æˆ‘ä»¬çš„ç¤¾ä¼šç”±ä¸€ç³»åˆ—è§„èŒƒæ‰€æ²»ç†ï¼Œè¿™äº›è§„èŒƒå…±åŒå¡‘é€ äº†æˆ‘ä»¬çè§†çš„ä»·å€¼è§‚ï¼Œå¦‚å®‰å…¨ã€å…¬å¹³å’Œå¯ä¿¡èµ–æ€§ã€‚ä»·å€¼å¯¹é½çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸ä»…å®Œæˆä»»åŠ¡è€Œä¸”é€šè¿‡è¡Œä¸ºä¿ƒè¿›è¿™äº›ä»·å€¼è§‚çš„æ™ºèƒ½ä½“ã€‚ç°æœ‰æ–¹æ³•åœ¨å°†æ³•å¾‹/å®‰å…¨è§„èŒƒä¸ç¤¾ä¼šè§„èŒƒç»“åˆåˆ°å•ä¸€ç®—æ³•ä¸­çš„èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†è¿™äº›è§„èŒƒæ•´åˆåˆ°å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œç›‘æ§æ™ºèƒ½ä½“å¯¹è§„èŒƒçš„éµå®ˆæƒ…å†µï¼Œå¹¶ç”¨ä¸€ä¸ªç§°ä¸ºæ™ºèƒ½ä½“å£°èª‰çš„é‡æ¥æ€»ç»“è¿™ä¸€æƒ…å†µã€‚é€šè¿‡å¯¹äº¤é€šé—®é¢˜çš„å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¹¦é¢å’Œæœªä¹¦é¢è§„èŒƒçš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ‰¾åˆ°ä»·å€¼å¯¹é½çš„ç­–ç•¥ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è¦è§£å†³çš„å…·ä½“é—®é¢˜æ˜¯å¦‚ä½•å°†ä¹¦é¢æ³•å¾‹è§„èŒƒä¸éšå«çš„ç¤¾ä¼šè§„èŒƒæœ‰æ•ˆç»“åˆï¼Œä»¥å®ç°æ™ºèƒ½ä½“çš„ä»·å€¼å¯¹é½ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åªå…³æ³¨å…¶ä¸­ä¸€ç§ç±»å‹çš„è§„èŒƒï¼Œå¯¼è‡´æ™ºèƒ½ä½“è¡Œä¸ºçš„å±€é™æ€§ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯é€šè¿‡å¼•å…¥å£°èª‰é‡æ¥ç›‘æ§æ™ºèƒ½ä½“å¯¹å„ç§è§„èŒƒçš„éµå®ˆæƒ…å†µï¼Œå¹¶åˆ©ç”¨è¿™ä¸€é‡æ¥åŠ æƒå¥–åŠ±ï¼Œä»è€Œæ¿€åŠ±æ™ºèƒ½ä½“æœå‘ä»·å€¼å¯¹é½çš„æ–¹å‘å‘å±•ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼š1) è§„èŒƒç›‘æ§æ¨¡å—ï¼Œè´Ÿè´£è¯„ä¼°æ™ºèƒ½ä½“çš„è¡Œä¸ºä¸è§„èŒƒçš„ä¸€è‡´æ€§ï¼›2) å£°èª‰è®¡ç®—æ¨¡å—ï¼Œæ ¹æ®ç›‘æ§ç»“æœè®¡ç®—æ™ºèƒ½ä½“çš„å£°èª‰ï¼›3) å¥–åŠ±åŠ æƒæ¨¡å—ï¼Œåˆ©ç”¨å£°èª‰é‡è°ƒæ•´æ™ºèƒ½ä½“çš„å¥–åŠ±ä¿¡å·ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹åœ¨äºå°†ä¹¦é¢å’Œæœªä¹¦é¢è§„èŒƒç»“åˆåˆ°å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œé€šè¿‡å£°èª‰é‡çš„å¼•å…¥ï¼Œä½¿å¾—æ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹ä¸ä»…ä¾èµ–äºç¯å¢ƒåé¦ˆï¼Œè¿˜è€ƒè™‘åˆ°ç¤¾ä¼šä»·å€¼çš„ä½“ç°ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•çš„å•ä¸€è§„èŒƒå…³æ³¨å½¢æˆäº†æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼Œå£°èª‰çš„è®¡ç®—æ–¹å¼è€ƒè™‘äº†ä¸åŒè§„èŒƒçš„æƒé‡è®¾ç½®ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸Šä¹Ÿå¼•å…¥äº†å¯¹è§„èŒƒéµå®ˆæƒ…å†µçš„æƒ©ç½šé¡¹ï¼Œä»¥ç¡®ä¿æ™ºèƒ½ä½“åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿå¹³è¡¡ä»»åŠ¡å®Œæˆä¸ä»·å€¼å¯¹é½çš„éœ€æ±‚ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„é‡‡ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼Œä»¥ä¾¿äºæ•æ‰å¤æ‚çš„è¡Œä¸ºæ¨¡å¼ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨HAVAæ–¹æ³•çš„æ™ºèƒ½ä½“åœ¨äº¤é€šé—®é¢˜ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ä»·å€¼å¯¹é½æ°´å¹³ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒæˆåŠŸç‡æé«˜äº†15%ï¼Œå¹¶ä¸”åœ¨éµå®ˆç¤¾ä¼šè§„èŒƒæ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¿™è¡¨æ˜ç»“åˆä¹¦é¢ä¸æœªä¹¦é¢è§„èŒƒçš„ç­–ç•¥åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åŠ©æ‰‹å’Œç¤¾äº¤æœºå™¨äººç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©è¿™äº›æ™ºèƒ½ä½“åœ¨æ‰§è¡Œä»»åŠ¡æ—¶æ›´å¥½åœ°éµå¾ªç¤¾ä¼šè§„èŒƒï¼Œæå‡äººæœºäº¤äº’çš„å®‰å…¨æ€§å’Œå¯ä¿¡èµ–æ€§ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•æœ‰æœ›æ¨åŠ¨æ™ºèƒ½ä½“åœ¨å¤æ‚ç¤¾ä¼šç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä¿ƒè¿›æŠ€æœ¯ä¸ä¼¦ç†çš„ç»“åˆã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Our society is governed by a set of norms which together bring about the values we cherish such as safety, fairness or trustworthiness. The goal of value-alignment is to create agents that not only do their tasks but through their behaviours also promote these values. Many of the norms are written as laws or rules (legal / safety norms) but even more remain unwritten (social norms). Furthermore, the techniques used to represent these norms also differ. Safety / legal norms are often represented explicitly, for example, in some logical language while social norms are typically learned and remain hidden in the parameter space of a neural network. There is a lack of approaches in the literature that could combine these various norm representations into a single algorithm. We propose a novel method that integrates these norms into the reinforcement learning process. Our method monitors the agent's compliance with the given norms and summarizes it in a quantity we call the agent's reputation. This quantity is used to weigh the received rewards to motivate the agent to become value-aligned. We carry out a series of experiments including a continuous state space traffic problem to demonstrate the importance of the written and unwritten norms and show how our method can find the value-aligned policies. Furthermore, we carry out ablations to demonstrate why it is better to combine these two groups of norms rather than using either separately.

