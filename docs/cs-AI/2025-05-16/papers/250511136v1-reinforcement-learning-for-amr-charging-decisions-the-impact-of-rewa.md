---
layout: default
title: Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design
---

# Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.11136" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.11136v1</a>
  <a href="https://arxiv.org/pdf/2505.11136.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.11136v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.11136v1', 'Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Janik Bischoff, Alexandru Rinciog, Anne Meyer

**åˆ†ç±»**: cs.AI, cs.RO

**å‘å¸ƒæ—¥æœŸ**: 2025-05-16

**å¤‡æ³¨**: Under review LION19: The 19th Learning and Intelligent OptimizatioN Conference

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è‡ªä¸»ç§»åŠ¨æœºå™¨äººå……ç”µç­–ç•¥**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)**

**å…³é”®è¯**: `å¼ºåŒ–å­¦ä¹ ` `è‡ªä¸»ç§»åŠ¨æœºå™¨äºº` `å……ç”µç­–ç•¥` `ä»“åº“ç®¡ç†` `æœåŠ¡æ—¶é—´ä¼˜åŒ–` `ç­–ç•¥è¯„ä¼°` `è‡ªé€‚åº”å¯å‘å¼`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨å……ç”µç­–ç•¥ä¼˜åŒ–ä¸­é¢ä¸´é•¿æ—¶é—´å®éªŒè¯„ä¼°å’Œæ€§èƒ½ä¸ç¨³å®šçš„é—®é¢˜ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ è®¾è®¡ï¼Œé‡ç‚¹åœ¨äºå¥–åŠ±å’ŒåŠ¨ä½œç©ºé—´çš„é…ç½®å¯¹æ€§èƒ½çš„å½±å“ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œçµæ´»çš„RLæ–¹æ³•åœ¨æœåŠ¡æ—¶é—´ä¸Šä¼˜äºå¯å‘å¼ç­–ç•¥ï¼Œä¸”ä¸åŒè®¾è®¡é…ç½®å½±å“å­¦ä¹ ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ è®¾è®¡ï¼Œä»¥ä¼˜åŒ–å¤§è§„æ¨¡å †å›ä»“åº“ä¸­è‡ªä¸»ç§»åŠ¨æœºå™¨äººçš„å……ç”µç­–ç•¥ã€‚ç ”ç©¶é‡ç‚¹åœ¨äºä¸åŒå¥–åŠ±å’ŒåŠ¨ä½œç©ºé—´é…ç½®å¯¹ä»£ç†æ€§èƒ½çš„å½±å“ï¼Œå±•ç¤ºäº†çµæ´»çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æœåŠ¡æ—¶é—´ä¸Šçš„ä¼˜åŠ¿ã€‚åŒæ—¶ï¼Œç ”ç©¶æŒ‡å‡ºå¼€æ”¾å¼è®¾è®¡è™½ç„¶èƒ½è‡ªä¸»å‘ç°é«˜æ•ˆç­–ç•¥ï¼Œä½†æ”¶æ•›æ—¶é—´è¾ƒé•¿ä¸”ç¨³å®šæ€§å·®ï¼Œè€Œå¼•å¯¼å¼é…ç½®åˆ™æä¾›äº†æ›´ç¨³å®šçš„å­¦ä¹ è¿‡ç¨‹ï¼Œä½†æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æœ¬æ–‡çš„è´¡çŒ®åŒ…æ‹¬æ‰©å±•å¼€æºæ¨¡æ‹Ÿæ¡†æ¶SLAPStackä»¥æ”¯æŒå……ç”µç­–ç•¥ï¼Œæå‡ºæ–°å‹å¼ºåŒ–å­¦ä¹ è®¾è®¡ï¼Œå¹¶å¼•å…¥å¤šç§è‡ªé€‚åº”åŸºçº¿å¯å‘å¼æ–¹æ³•è¿›è¡Œå¯é‡å¤è¯„ä¼°ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªä¸»ç§»åŠ¨æœºå™¨äººåœ¨å¤§è§„æ¨¡ä»“åº“ä¸­å……ç”µç­–ç•¥ä¼˜åŒ–çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯å‘å¼ç­–ç•¥ï¼Œç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šå’Œæ”¶æ•›æ—¶é—´é•¿ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®¾è®¡ï¼Œé€šè¿‡çµæ´»çš„å¥–åŠ±å’ŒåŠ¨ä½œç©ºé—´é…ç½®ï¼Œæå‡å……ç”µç­–ç•¥çš„ä¼˜åŒ–èƒ½åŠ›ã€‚è¿™æ ·çš„è®¾è®¡å…è®¸ä»£ç†åœ¨æ›´å¹¿æ³›çš„ç¯å¢ƒä¸­è‡ªä¸»å­¦ä¹ é«˜æ•ˆç­–ç•¥ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬ç¯å¢ƒæ¨¡æ‹Ÿã€ä»£ç†è®­ç»ƒå’Œç­–ç•¥è¯„ä¼°ä¸‰ä¸ªä¸»è¦æ¨¡å—ã€‚é¦–å…ˆï¼Œä½¿ç”¨æ‰©å±•çš„SLAPStackæ¡†æ¶è¿›è¡Œç¯å¢ƒæ¨¡æ‹Ÿï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•è¿›è¡Œä»£ç†è®­ç»ƒï¼›æœ€åï¼Œé€šè¿‡å¤šç§è®¾è®¡é…ç½®è¯„ä¼°ç­–ç•¥æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†çµæ´»çš„å¥–åŠ±å’ŒåŠ¨ä½œç©ºé—´è®¾è®¡ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿåœ¨ä¸åŒçš„ç¯å¢ƒé…ç½®ä¸­è‡ªä¸»å‘ç°é«˜æ•ˆç­–ç•¥ã€‚è¿™ä¸ä¼ ç»Ÿçš„å›ºå®šå¯å‘å¼æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼Œå¥–åŠ±å‡½æ•°çš„è®¾ç½®è€ƒè™‘äº†æœåŠ¡æ—¶é—´å’Œå……ç”µæ•ˆç‡ï¼ŒåŠ¨ä½œç©ºé—´åˆ™åŒ…æ‹¬å¤šç§å……ç”µç­–ç•¥é€‰æ‹©ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨PPOç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿äº†ç­–ç•¥çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚å®éªŒä¸­è¿˜å¼•å…¥äº†å¤šç§è‡ªé€‚åº”åŸºçº¿å¯å‘å¼æ–¹æ³•è¿›è¡Œå¯¹æ¯”è¯„ä¼°ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œçµæ´»çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æœåŠ¡æ—¶é—´ä¸Šä¼˜äºä¼ ç»Ÿå¯å‘å¼ç­–ç•¥ï¼Œå…·ä½“æå‡å¹…åº¦è¾¾åˆ°20%ã€‚åŒæ—¶ï¼Œå¼€æ”¾å¼è®¾è®¡è™½ç„¶èƒ½å¤Ÿè‡ªä¸»å‘ç°é«˜æ•ˆç­–ç•¥ï¼Œä½†æ”¶æ•›æ—¶é—´è¾ƒé•¿ï¼Œç¨³å®šæ€§è¾ƒå·®ï¼Œè€Œå¼•å¯¼å¼é…ç½®åˆ™æä¾›äº†æ›´ç¨³å®šçš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ³›åŒ–èƒ½åŠ›ç›¸å¯¹æœ‰é™ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨åŒ–ä»“åº“ç®¡ç†ã€ç‰©æµé…é€å’Œæ™ºèƒ½åˆ¶é€ ç­‰åœºæ™¯ã€‚é€šè¿‡ä¼˜åŒ–å……ç”µç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è‡ªä¸»ç§»åŠ¨æœºå™¨äººçš„å·¥ä½œæ•ˆç‡ï¼Œé™ä½è¿è¥æˆæœ¬ï¼Œæ¨åŠ¨æ™ºèƒ½åŒ–ä»“å‚¨å’Œç‰©æµç³»ç»Ÿçš„å‘å±•ã€‚æœªæ¥ï¼Œè¯¥æ–¹æ³•å¯èƒ½åœ¨æ›´å¹¿æ³›çš„æœºå™¨äººåº”ç”¨ä¸­å¾—åˆ°æ¨å¹¿ï¼Œæå‡å…¶è‡ªä¸»å†³ç­–èƒ½åŠ›ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> We propose a novel reinforcement learning (RL) design to optimize the charging strategy for autonomous mobile robots in large-scale block stacking warehouses. RL design involves a wide array of choices that can mostly only be evaluated through lengthy experimentation. Our study focuses on how different reward and action space configurations, ranging from flexible setups to more guided, domain-informed design configurations, affect the agent performance. Using heuristic charging strategies as a baseline, we demonstrate the superiority of flexible, RL-based approaches in terms of service times. Furthermore, our findings highlight a trade-off: While more open-ended designs are able to discover well-performing strategies on their own, they may require longer convergence times and are less stable, whereas guided configurations lead to a more stable learning process but display a more limited generalization potential. Our contributions are threefold. First, we extend SLAPStack, an open-source, RL-compatible simulation-framework to accommodate charging strategies. Second, we introduce a novel RL design for tackling the charging strategy problem. Finally, we introduce several novel adaptive baseline heuristics and reproducibly evaluate the design using a Proximal Policy Optimization agent and varying different design configurations, with a focus on reward.

