---
layout: default
title: StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs
---

# StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20139" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20139v1</a>
  <a href="https://arxiv.org/pdf/2505.20139.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20139v1" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20139v1', 'StructEval: Benchmarking LLMs\' Capabilities to Generate Structural Outputs')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, Benjamin Schneider, Chi Ruan, Wentao Ma, Zhiheng Lyu, Yifei Wang, Yi Lu, Quy Duc Do, Ziyan Jiang, Ping Nie, Wenhu Chen

**åˆ†ç±»**: cs.SE, cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26

**å¤‡æ³¨**: 16 pages, 9 figures, 13 tables

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºStructEvalä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºçš„èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§å‹è¯­è¨€æ¨¡å‹` `ç»“æ„åŒ–è¾“å‡º` `è¯„ä¼°åŸºå‡†` `ç”Ÿæˆä»»åŠ¡` `è½¬æ¢ä»»åŠ¡` `æ ¼å¼éµå¾ªæ€§` `ç»“æ„æ­£ç¡®æ€§` `è½¯ä»¶å¼€å‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºçš„èƒ½åŠ›æ—¶ç¼ºä¹ç³»ç»Ÿæ€§ï¼Œå¯¼è‡´æ€§èƒ½å·®å¼‚æœªè¢«å……åˆ†æ­ç¤ºã€‚
2. StructEvalé€šè¿‡ç”Ÿæˆä»»åŠ¡å’Œè½¬æ¢ä»»åŠ¡ä¸¤ç§æ–¹å¼ï¼Œç³»ç»Ÿè¯„ä¼°LLMsåœ¨å¤šç§ç»“æ„åŒ–æ ¼å¼ä¸‹çš„è¾“å‡ºèƒ½åŠ›ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºæ—¶ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå°¤å…¶åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢æ›´ä¸ºæ˜æ˜¾ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å¼€å‘å·¥ä½œæµç¨‹ä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ï¼Œå®ƒä»¬ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºçš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†StructEvalï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ç”Ÿæˆä¸å¯æ¸²æŸ“ï¼ˆå¦‚JSONã€YAMLã€CSVï¼‰å’Œå¯æ¸²æŸ“ï¼ˆå¦‚HTMLã€Reactã€SVGï¼‰ç»“æ„åŒ–æ ¼å¼æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ä»¥å¾€çš„åŸºå‡†ä¸åŒï¼ŒStructEvalé€šè¿‡ç”Ÿæˆä»»åŠ¡å’Œè½¬æ¢ä»»åŠ¡ä¸¤ç§èŒƒå¼ç³»ç»Ÿåœ°è¯„ä¼°ç»“æ„çš„å‡†ç¡®æ€§ã€‚è¯¥åŸºå‡†æ¶µç›–18ç§æ ¼å¼å’Œ44ç§ä»»åŠ¡ç±»å‹ï¼Œå¹¶å¼•å…¥äº†æ ¼å¼éµå¾ªæ€§å’Œç»“æ„æ­£ç¡®æ€§çš„åˆ›æ–°æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹o1-miniçš„å¹³å‡å¾—åˆ†ä¹Ÿä»…ä¸º75.58ï¼Œå¼€æºæ›¿ä»£æ–¹æ¡ˆçš„å¾—åˆ†å¤§çº¦ä½10åˆ†ã€‚æˆ‘ä»¬å‘ç°ç”Ÿæˆä»»åŠ¡æ¯”è½¬æ¢ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ï¼Œè€Œç”Ÿæˆæ­£ç¡®çš„è§†è§‰å†…å®¹æ¯”ç”Ÿæˆä»…æ–‡æœ¬ç»“æ„æ›´å›°éš¾ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºæ—¶çš„è¯„ä¼°ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å…¨é¢åæ˜ æ¨¡å‹åœ¨ä¸åŒæ ¼å¼ä¸‹çš„æ€§èƒ½å·®å¼‚ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šStructEvalçš„æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å¼•å…¥ç”Ÿæˆä»»åŠ¡å’Œè½¬æ¢ä»»åŠ¡ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤šç§ç»“æ„åŒ–æ ¼å¼ä¸‹çš„è¾“å‡ºèƒ½åŠ›ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šStructEvalçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šç”Ÿæˆä»»åŠ¡æ¨¡å—å’Œè½¬æ¢ä»»åŠ¡æ¨¡å—ã€‚ç”Ÿæˆä»»åŠ¡æ¨¡å—è´Ÿè´£ä»è‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼Œè€Œè½¬æ¢ä»»åŠ¡æ¨¡å—åˆ™è´Ÿè´£åœ¨ä¸åŒç»“æ„åŒ–æ ¼å¼ä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚

**å…³é”®åˆ›æ–°**ï¼šStructEvalçš„ä¸»è¦åˆ›æ–°åœ¨äºå¼•å…¥äº†æ ¼å¼éµå¾ªæ€§å’Œç»“æ„æ­£ç¡®æ€§çš„è¯„ä¼°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°åæ˜ æ¨¡å‹åœ¨ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºæ—¶çš„è¡¨ç°ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„è¯„ä¼°ç²¾åº¦ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®¾è®¡ä¸­ï¼ŒStructEvalæ¶µç›–äº†18ç§ç»“æ„åŒ–æ ¼å¼å’Œ44ç§ä»»åŠ¡ç±»å‹ï¼Œé‡‡ç”¨äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹çš„æ ¼å¼éµå¾ªæ€§å’Œç»“æ„æ­£ç¡®æ€§ï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„å¯é æ€§ã€‚å®éªŒä¸­è¿˜å‘ç°ç”Ÿæˆä»»åŠ¡çš„éš¾åº¦æ™®éé«˜äºè½¬æ¢ä»»åŠ¡ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹o1-miniåœ¨ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºæ—¶çš„å¹³å‡å¾—åˆ†ä¸º75.58ï¼Œå¼€æºæ›¿ä»£æ–¹æ¡ˆçš„å¾—åˆ†ä½çº¦10åˆ†ã€‚ç”Ÿæˆä»»åŠ¡çš„æŒ‘æˆ˜æ€§æ˜æ˜¾é«˜äºè½¬æ¢ä»»åŠ¡ï¼Œå°¤å…¶åœ¨ç”Ÿæˆè§†è§‰å†…å®¹æ—¶ï¼Œæ¨¡å‹è¡¨ç°å‡ºæ›´å¤§çš„å›°éš¾ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆä¸­çš„æ€§èƒ½å·®è·ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€æ•°æ®å¤„ç†å’Œè‡ªåŠ¨åŒ–æ–‡æ¡£ç”Ÿæˆç­‰ã€‚é€šè¿‡è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºæ–¹é¢çš„èƒ½åŠ›ï¼ŒStructEvalèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œæé«˜å¼€å‘æ•ˆç‡ï¼Œæ¨åŠ¨æ™ºèƒ½åŒ–è½¯ä»¶å·¥å…·çš„å‘å±•ã€‚æœªæ¥ï¼Œéšç€æ¨¡å‹èƒ½åŠ›çš„æå‡ï¼ŒStructEvalä¹Ÿå°†ä¸ºæ›´å¤æ‚çš„åº”ç”¨åœºæ™¯æä¾›è¯„ä¼°ä¾æ®ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and 2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures.

