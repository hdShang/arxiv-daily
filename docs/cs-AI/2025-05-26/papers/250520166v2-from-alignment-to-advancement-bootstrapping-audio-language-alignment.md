---
layout: default
title: From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data
---

# From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20166" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20166v2</a>
  <a href="https://arxiv.org/pdf/2505.20166.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20166v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20166v2', 'From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Chun-Yi Kuan, Hung-yi Lee

**åˆ†ç±»**: eess.AS, cs.AI, cs.CL, cs.LG, cs.SD

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-06-30)

**å¤‡æ³¨**: Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing. Project Website: https://kuan2jiu99.github.io/Balsa

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºBALSaæ¡†æ¶ä»¥è§£å†³éŸ³é¢‘è¯­è¨€å¯¹é½é—®é¢˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `éŸ³é¢‘æ„ŸçŸ¥` `å¤§å‹è¯­è¨€æ¨¡å‹` `å¤šæ¨¡æ€å­¦ä¹ ` `æ•°æ®åˆæˆ` `å¯¹é½æŠ€æœ¯` `éŸ³é¢‘ç†è§£` `æ¨ç†èƒ½åŠ›` `ç¾éš¾æ€§é—å¿˜`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„éŸ³é¢‘æ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€‚åº”è¿‡ç¨‹ä¸­å®¹æ˜“å‡ºç°ç¾éš¾æ€§é—å¿˜ï¼Œå¯¼è‡´æ–‡æœ¬èƒ½åŠ›ä¸‹é™ã€‚
2. æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆæˆæ•°æ®ç”Ÿæˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹åŒºåˆ†éŸ³é¢‘ä¸­å­˜åœ¨ä¸ç¼ºå¤±å£°éŸ³çš„èƒ½åŠ›ã€‚
3. å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†éŸ³é¢‘å¹»è§‰ï¼Œå¹¶åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

éŸ³é¢‘æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMsï¼‰åœ¨ç†è§£å’Œå¤„ç†éŸ³é¢‘è¾“å…¥æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨é€‚åº”è¿‡ç¨‹ä¸­é¢ä¸´ç¾éš¾æ€§é—å¿˜å’Œä¾èµ–å¤§é‡ä»»åŠ¡ç‰¹å®šæ•°æ®çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åˆæˆå¯¹æ¯”å¼è®­ç»ƒæ•°æ®ï¼Œå¢å¼ºALLMsåŒºåˆ†éŸ³é¢‘ä¸­å­˜åœ¨ä¸ç¼ºå¤±å£°éŸ³çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†éŸ³é¢‘å¹»è§‰ï¼ŒåŒæ—¶åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†å¼ºåŠ²çš„è¡¨ç°ï¼Œä¸”å¤šéŸ³é¢‘è®­ç»ƒè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³éŸ³é¢‘æ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€‚åº”éŸ³é¢‘ä»»åŠ¡æ—¶å‡ºç°çš„ç¾éš¾æ€§é—å¿˜å’Œå¯¹é½æ•°æ®éœ€æ±‚é«˜çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤§é‡ä»»åŠ¡ç‰¹å®šçš„æ•°æ®ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—å¤§ä¸”æ•ˆæœä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæå‡ºäº†ä¸€ç§æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åˆæˆå¯¹æ¯”å¼è®­ç»ƒæ•°æ®ï¼Œå¢å¼ºæ¨¡å‹å¯¹éŸ³é¢‘ä¸­å­˜åœ¨ä¸ç¼ºå¤±å£°éŸ³çš„åŒºåˆ†èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œå¯é æ€§ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šæ•´ä½“æ¶æ„åŒ…æ‹¬æ•°æ®ç”Ÿæˆæ¨¡å—å’Œè®­ç»ƒæ¨¡å—ã€‚æ•°æ®ç”Ÿæˆæ¨¡å—åˆ©ç”¨åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆå¯¹æ¯”å¼æ•°æ®ï¼Œè®­ç»ƒæ¨¡å—åˆ™ä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„åˆ›æ–°åœ¨äºé€šè¿‡åˆæˆå¯¹æ¯”å¼è®­ç»ƒæ•°æ®æ¥å¢å¼ºéŸ³é¢‘è¯­è¨€å¯¹é½èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„ç¨³å®šæ€§ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ç‰¹å®šçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹å¯¹éŸ³é¢‘å’Œæ–‡æœ¬çš„å¯¹é½èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†å¤šéŸ³é¢‘åœºæ™¯çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç»¼åˆç†è§£èƒ½åŠ›ã€‚å…·ä½“çš„ç½‘ç»œç»“æ„å’Œå‚æ•°è®¾ç½®åœ¨å®éªŒä¸­è¿›è¡Œäº†è¯¦ç»†è°ƒä¼˜ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒBALSaæ¡†æ¶æœ‰æ•ˆå‡å°‘äº†éŸ³é¢‘å¹»è§‰ç°è±¡ï¼Œæ¨¡å‹åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¾¾åˆ°15%ã€‚å¤šéŸ³é¢‘è®­ç»ƒè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„ç»¼åˆç†è§£èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€éŸ³é¢‘å†…å®¹æ£€ç´¢å’Œå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿã€‚é€šè¿‡æé«˜éŸ³é¢‘ä¸è¯­è¨€çš„å¯¹é½èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œæœªæ¥å¯èƒ½åœ¨æ•™è‚²ã€å¨±ä¹å’Œä¿¡æ¯æ£€ç´¢ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Audio-aware large language models (ALLMs) have recently made great strides in understanding and processing audio inputs. These models are typically adapted from text-based large language models (LLMs) through additional training on audio-related tasks. However, this adaptation process presents two major limitations. First, ALLMs often suffer from catastrophic forgetting, where crucial textual capabilities like instruction-following are lost after training on audio data. In some cases, models may even hallucinate sounds that are not present in the input audio, raising concerns about reliability. Second, achieving cross-modal alignment between audio and language typically relies on large collections of task-specific question-answer pairs for instruction tuning, making it resource-intensive. To address these issues, previous works have leveraged the backbone LLMs to synthesize general-purpose, caption-style alignment data. In this paper, we propose a data generation framework that produces contrastive-like training data, designed to enhance ALLMs' ability to differentiate between present and absent sounds. We further extend our approach to multi-audio scenarios, enabling the model to either explain differences between audio inputs or produce unified captions that describe all inputs, thereby enhancing audio-language alignment. We refer to the entire ALLM training framework as bootstrapping audio-language alignment via synthetic data generation from backbone LLMs (BALSa). Experimental results indicate that our method effectively mitigates audio hallucinations while reliably maintaining strong performance on audio understanding and reasoning benchmarks, as well as instruction-following skills. Moreover, incorporating multi-audio training further enhances the model's comprehension and reasoning capabilities. Overall, BALSa offers an efficient and scalable approach to developing ALLMs.

