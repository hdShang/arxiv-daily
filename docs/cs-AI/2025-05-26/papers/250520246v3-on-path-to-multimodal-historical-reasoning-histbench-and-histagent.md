---
layout: default
title: On Path to Multimodal Historical Reasoning: HistBench and HistAgent
---

# On Path to Multimodal Historical Reasoning: HistBench and HistAgent

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.20246" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.20246v3</a>
  <a href="https://arxiv.org/pdf/2505.20246.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.20246v3" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.20246v3', 'On Path to Multimodal Historical Reasoning: HistBench and HistAgent')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Jiahao Qiu, Fulian Xiao, Yimin Wang, Yuchen Mao, Yijia Chen, Xinzhe Juan, Shu Zhang, Siran Wang, Xuan Qi, Tongcheng Zhang, Zixin Yao, Jiacheng Guo, Yifu Lu, Charles Argon, Jundi Cui, Daixin Chen, Junran Zhou, Shuyao Zhou, Zhanpeng Zhou, Ling Yang, Shilong Liu, Hongru Wang, Kaixuan Huang, Xun Jiang, Yuming Cao, Yue Chen, Yunfei Chen, Zhengyi Chen, Ruowei Dai, Mengqiu Deng, Jiye Fu, Yunting Gu, Zijie Guan, Zirui Huang, Xiaoyan Ji, Yumeng Jiang, Delong Kong, Haolong Li, Jiaqi Li, Ruipeng Li, Tianze Li, Zhuoran Li, Haixia Lian, Mengyue Lin, Xudong Liu, Jiayi Lu, Jinghan Lu, Wanyu Luo, Ziyue Luo, Zihao Pu, Zhi Qiao, Ruihuan Ren, Liang Wan, Ruixiang Wang, Tianhui Wang, Yang Wang, Zeyu Wang, Zihua Wang, Yujia Wu, Zhaoyi Wu, Hao Xin, Weiao Xing, Ruojun Xiong, Weijie Xu, Yao Shu, Yao Xiao, Xiaorui Yang, Yuchen Yang, Nan Yi, Jiadong Yu, Yangyuxuan Yu, Huiting Zeng, Danni Zhang, Yunjie Zhang, Zhaoyu Zhang, Zhiheng Zhang, Xiaofeng Zheng, Peirong Zhou, Linyan Zhong, Xiaoyin Zong, Ying Zhao, Zhenxin Chen, Lin Ding, Xiaoyu Gao, Bingbing Gong, Yichao Li, Yang Liao, Guang Ma, Tianyuan Ma, Xinrui Sun, Tianyi Wang, Han Xia, Ruobing Xian, Gen Ye, Tengfei Yu, Wentao Zhang, Yuxi Wang, Xi Gao, Mengdi Wang

**åˆ†ç±»**: cs.AI, cs.CL

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-06-19)

**å¤‡æ³¨**: 17 pages, 7 figures

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHistBenchå’ŒHistAgentä»¥è§£å†³å†å²æ¨ç†ä¸­çš„å¤šæ¨¡æ€æŒ‘æˆ˜**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å†å²æ¨ç†` `å¤šæ¨¡æ€å­¦ä¹ ` `å¤§å‹è¯­è¨€æ¨¡å‹` `OCRæŠ€æœ¯` `æ™ºèƒ½ä½“è®¾è®¡` `è·¨è¯­è¨€åˆ†æ` `åŸºå‡†æµ‹è¯•`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰çš„é€šç”¨æ™ºèƒ½ä½“åœ¨å†å²æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹å¿…è¦çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚
2. æœ¬æ–‡æå‡ºHistBenchåŸºå‡†å’ŒHistAgentæ™ºèƒ½ä½“ï¼Œä¸“æ³¨äºå†å²æ¨ç†çš„å¤šæ¨¡æ€æŒ‘æˆ˜ã€‚
3. HistAgentåœ¨HistBenchä¸Šå–å¾—äº†27.54%çš„pass@1å’Œ36.47%çš„pass@2ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–é€šç”¨æ¨¡å‹ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å†å²å­¦ç­‰äººæ–‡å­¦ç§‘ä¸­çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å†å²æ¨ç†é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æºè§£é‡Šã€æ—¶é—´æ¨ç†å’Œè·¨è¯­è¨€åˆ†æã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†HistBenchï¼Œä¸€ä¸ªåŒ…å«414ä¸ªé«˜è´¨é‡é—®é¢˜çš„æ–°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°AIåœ¨å†å²æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†HistAgentï¼Œä¸€ä¸ªä¸“é—¨é’ˆå¯¹å†å²çš„æ™ºèƒ½ä½“ï¼Œé…å¤‡äº†OCRã€ç¿»è¯‘ã€æ¡£æ¡ˆæœç´¢å’Œå›¾åƒç†è§£ç­‰å·¥å…·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHistAgentåœ¨HistBenchä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMså’Œé€šç”¨æ™ºèƒ½ä½“ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†å²æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„ç¼ºä¹ã€‚ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å¤æ‚çš„å†å²ææ–™æ—¶è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆè¿›è¡Œæ—¶é—´æ¨ç†å’Œè·¨è¯­è¨€åˆ†æã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡æå‡ºHistBenchä½œä¸ºè¯„ä¼°å·¥å…·ï¼Œå¹¶è®¾è®¡HistAgentæ™ºèƒ½ä½“ï¼Œä¸“é—¨é’ˆå¯¹å†å²æ¨ç†ä»»åŠ¡ï¼Œé›†æˆäº†OCRã€ç¿»è¯‘å’Œå›¾åƒç†è§£ç­‰åŠŸèƒ½ï¼Œä»¥æå‡å¯¹å†å²ææ–™çš„å¤„ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHistAgentçš„æ•´ä½“æ¶æ„åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼šOCRæ¨¡å—ç”¨äºæ–‡æœ¬æå–ï¼Œç¿»è¯‘æ¨¡å—å¤„ç†å¤šè¯­è¨€æ–‡æœ¬ï¼Œæ¡£æ¡ˆæœç´¢æ¨¡å—ç”¨äºè·å–ç›¸å…³å†å²èµ„æ–™ï¼Œå›¾åƒç†è§£æ¨¡å—åˆ™åˆ†æå†å²å›¾åƒã€‚å„æ¨¡å—ååŒå·¥ä½œï¼Œå½¢æˆä¸€ä¸ªç»¼åˆçš„å†å²æ¨ç†ç³»ç»Ÿã€‚

**å…³é”®åˆ›æ–°**ï¼šHistBenchçš„æ„å»ºå’ŒHistAgentçš„è®¾è®¡æ˜¯æœ¬ç ”ç©¶çš„ä¸»è¦åˆ›æ–°ã€‚HistBenchæä¾›äº†ä¸€ä¸ªå¤šæ ·åŒ–çš„å†å²é—®é¢˜é›†ï¼Œè€ŒHistAgentåˆ™é€šè¿‡ä¸“é—¨çš„å·¥å…·å’Œæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜¾è‘—æå‡äº†å†å²æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è®¾è®¡**ï¼šHistAgentçš„è®¾è®¡ä¸­ï¼ŒOCRå’Œç¿»è¯‘æ¨¡å—é‡‡ç”¨äº†æœ€æ–°çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç¡®ä¿é«˜å‡†ç¡®ç‡ï¼›æ¡£æ¡ˆæœç´¢æ¨¡å—åˆ™åˆ©ç”¨äº†é«˜æ•ˆçš„ç´¢å¼•æœºåˆ¶ï¼Œä»¥å¿«é€Ÿæ£€ç´¢ç›¸å…³å†å²æ–‡çŒ®ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

åœ¨HistBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHistAgentåŸºäºGPT-4oçš„è¡¨ç°ä¸º27.54%çš„pass@1å’Œ36.47%çš„pass@2ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–é€šç”¨æ¨¡å‹ï¼Œå¦‚GPT-4oï¼ˆ18.60%ï¼‰ã€DeepSeek-R1ï¼ˆ14.49%ï¼‰å’ŒOpen Deep Research-smolagentsï¼ˆ20.29% pass@1å’Œ25.12% pass@2ï¼‰ï¼Œæ˜¾ç¤ºå‡ºHistAgentåœ¨å†å²æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²ã€å†å²ç ”ç©¶å’Œæ–‡åŒ–é—äº§ä¿æŠ¤ã€‚HistAgentå¯ä»¥å¸®åŠ©å†å²å­¦è€…å’Œå­¦ç”Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ†æå†å²ææ–™ï¼Œæé«˜å†å²ç ”ç©¶çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å¯ç”¨äºå¼€å‘æ™ºèƒ½æ•™è‚²å·¥å…·ï¼Œä¿ƒè¿›å†å²çŸ¥è¯†çš„ä¼ æ’­å’Œå­¦ä¹ ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving multimodal source interpretation, temporal inference, and cross-linguistic analysis. While general-purpose agents perform well on many existing benchmarks, they lack the domain-specific expertise required to engage with historical materials and questions. To address this gap, we introduce HistBench, a new benchmark of 414 high-quality questions designed to evaluate AI's capacity for historical reasoning and authored by more than 40 expert contributors. The tasks span a wide range of historical problems-from factual retrieval based on primary sources to interpretive analysis of manuscripts and images, to interdisciplinary challenges involving archaeology, linguistics, or cultural history. Furthermore, the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions. Finding the poor performance of LLMs and other agents on HistBench, we further present HistAgent, a history-specific agent equipped with carefully designed tools for OCR, translation, archival search, and image understanding in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%) and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These results highlight the limitations of existing LLMs and generalist agents and demonstrate the advantages of HistAgent for historical reasoning.

