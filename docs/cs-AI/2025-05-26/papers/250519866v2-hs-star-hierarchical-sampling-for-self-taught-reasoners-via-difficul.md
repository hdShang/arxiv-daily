---
layout: default
title: "HS-STaR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation"
---

# HS-STaR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2505.19866" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2505.19866v2</a>
  <a href="https://arxiv.org/pdf/2505.19866.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2505.19866v2" data-paper-url="__CURRENT_PAGE__" onclick="toggleFavorite(this, '2505.19866v2', 'HS-STaR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Feng Xiong, Hongling Xu, Yifei Wang, Runxi Cheng, Yong Wang, Xiangxiang Chu

**åˆ†ç±»**: cs.AI, cs.CL, cs.LG

**å‘å¸ƒæ—¥æœŸ**: 2025-05-26 (æ›´æ–°: 2025-09-28)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºHS-STaRä»¥ä¼˜åŒ–è‡ªå­¦æ¨ç†è€…çš„æ ·æœ¬é€‰æ‹©**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `è‡ªå­¦æ¨ç†è€…` `å±‚æ¬¡åŒ–é‡‡æ ·` `éš¾åº¦ä¼°è®¡` `é¢„ç®—é‡æ–°åˆ†é…` `å¤§å‹è¯­è¨€æ¨¡å‹` `æ•°å­¦æ¨ç†` `è®­ç»ƒæ•°æ®ä¼˜åŒ–`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ–¹æ³•åœ¨æ ·æœ¬é€‰æ‹©ä¸Šå‡åŒ€åˆ†é…é¢„ç®—ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸åŒéš¾åº¦é—®é¢˜çš„å­¦ä¹ æ•ˆç”¨ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚
2. HS-STaRé€šè¿‡å¥–åŠ±å¼•å¯¼çš„éš¾åº¦ä¼°è®¡ç­–ç•¥ï¼Œä¼˜å…ˆè¯†åˆ«å’Œåˆ©ç”¨æ¥è¿‘æ¨¡å‹æ¨ç†èƒ½åŠ›è¾¹ç•Œçš„é—®é¢˜ï¼Œä»è€Œæé«˜è®­ç»ƒæ•°æ®çš„è´¨é‡ã€‚
3. åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šï¼ŒHS-STaRæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜æ˜¾ä¸”æ— éœ€é¢å¤–é¢„ç®—ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è‡ªå­¦æ¨ç†è€…ï¼ˆSTaRsï¼‰é€šè¿‡åˆ©ç”¨è‡ªç”Ÿæˆçš„å“åº”æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰ç ”ç©¶é€šå¸¸åœ¨æ‰€æœ‰é—®é¢˜ä¸Šå‡åŒ€åˆ†é…é‡‡æ ·é¢„ç®—ï¼Œå¿½è§†äº†ä¸åŒéš¾åº¦çº§åˆ«é—®é¢˜çš„æ•ˆç”¨å·®å¼‚ã€‚æœ¬æ–‡æå‡ºHS-STaRï¼Œä¸€ä¸ªå±‚æ¬¡åŒ–é‡‡æ ·æ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡å¥–åŠ±å¼•å¯¼çš„éš¾åº¦ä¼°è®¡ç­–ç•¥è¿›è¡Œè½»é‡çº§é¢„é‡‡æ ·ï¼Œä»¥è¯†åˆ«è¾¹ç•Œçº§é—®é¢˜ï¼Œç„¶ååœ¨é‡æ–°é‡‡æ ·é˜¶æ®µåŠ¨æ€é‡æ–°åˆ†é…é¢„ç®—ï¼Œæœ€å¤§åŒ–é«˜æ•ˆè®­ç»ƒæ•°æ®çš„ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒHS-STaRåœ¨å¤šä¸ªæ¨ç†åŸºå‡†å’ŒåŸºç¡€LLMsä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿ï¼Œä¸”æ— éœ€é¢å¤–çš„é‡‡æ ·é¢„ç®—ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šæœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªå­¦æ¨ç†è€…åœ¨æ ·æœ¬é€‰æ‹©æ—¶çš„é¢„ç®—åˆ†é…é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½è€ƒè™‘ä¸åŒéš¾åº¦é—®é¢˜çš„æ•ˆç”¨å·®å¼‚ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸ä½³ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šHS-STaRçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å±‚æ¬¡åŒ–é‡‡æ ·ç­–ç•¥ï¼Œä¼˜å…ˆé€‰æ‹©é‚£äº›æ¥è¿‘æ¨¡å‹æ¨ç†èƒ½åŠ›è¾¹ç•Œçš„é—®é¢˜ï¼Œä»¥æœ€å¤§åŒ–å­¦ä¹ æ•ˆç”¨ã€‚è¿™æ ·çš„è®¾è®¡èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„é‡‡æ ·é¢„ç®—ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šHS-STaRçš„æ•´ä½“æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆæ˜¯è½»é‡çº§é¢„é‡‡æ ·é˜¶æ®µï¼Œé€šè¿‡å¥–åŠ±å¼•å¯¼çš„éš¾åº¦ä¼°è®¡ç­–ç•¥è¯†åˆ«è¾¹ç•Œçº§é—®é¢˜ï¼›å…¶æ¬¡æ˜¯é‡æ–°é‡‡æ ·é˜¶æ®µï¼ŒåŠ¨æ€è°ƒæ•´é¢„ç®—ï¼Œé›†ä¸­åœ¨é«˜æ•ˆé—®é¢˜ä¸Šã€‚

**å…³é”®åˆ›æ–°**ï¼šHS-STaRçš„åˆ›æ–°åœ¨äºå¼•å…¥äº†éš¾åº¦ä¼°è®¡å’Œé¢„ç®—é‡æ–°åˆ†é…æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒéš¾åº¦é—®é¢˜ä¸Šè¿›è¡Œæ›´æœ‰é’ˆå¯¹æ€§çš„å­¦ä¹ ï¼Œè¿™ä¸ä¼ ç»Ÿçš„å‡åŒ€é‡‡æ ·æ–¹æ³•æœ‰æœ¬è´¨åŒºåˆ«ã€‚

**å…³é”®è®¾è®¡**ï¼šåœ¨æŠ€æœ¯ç»†èŠ‚ä¸Šï¼ŒHS-STaRä½¿ç”¨äº†å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°é—®é¢˜çš„éš¾åº¦ï¼Œå¹¶æ ¹æ®é¢„è®¾çš„é¢„ç®—åŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿é«˜æ•ˆåˆ©ç”¨æ¯ä¸€ä»½è®­ç»ƒèµ„æºã€‚å…·ä½“çš„å‚æ•°è®¾ç½®å’ŒæŸå¤±å‡½æ•°è®¾è®¡åœ¨å®éªŒä¸­ç»è¿‡ä¼˜åŒ–ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHS-STaRåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œå…·ä½“è¡¨ç°ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šæ€§èƒ½æå‡è¶…è¿‡20%ã€‚æ­¤å¤–ï¼ŒHS-STaRåœ¨ä¸å¢åŠ é¢å¤–é‡‡æ ·é¢„ç®—çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸæé«˜äº†è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶çš„æ½œåœ¨åº”ç”¨é¢†åŸŸåŒ…æ‹¬æ•™è‚²æŠ€æœ¯ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œè‡ªåŠ¨åŒ–é—®é¢˜ç”Ÿæˆç­‰ã€‚é€šè¿‡ä¼˜åŒ–æ ·æœ¬é€‰æ‹©ï¼ŒHS-STaRèƒ½å¤Ÿåœ¨è¿™äº›é¢†åŸŸä¸­æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå­¦ä¹ æ•ˆç‡ï¼Œè¿›è€Œæ¨åŠ¨ä¸ªæ€§åŒ–å­¦ä¹ å’Œæ™ºèƒ½æ•™è‚²çš„å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of large language models (LLMs) by leveraging self-generated responses for self-training. Recent studies have incorporated reward models to guide response selection or decoding, aiming to obtain higher-quality data. However, they typically allocate a uniform sampling budget across all problems, overlooking the varying utility of problems at different difficulty levels. In this work, we conduct an empirical study and find that problems near the boundary of the LLM's reasoning capability offer significantly greater learning utility than both easy and overly difficult ones. To identify and exploit such problems, we propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners. Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling with a reward-guided difficulty estimation strategy to efficiently identify boundary-level problems. Subsequently, it dynamically reallocates the remaining budget toward these high-utility problems during a re-sampling phase, maximizing the generation of valuable training data. Extensive experiments across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR significantly outperforms other baselines without requiring additional sampling budget.

