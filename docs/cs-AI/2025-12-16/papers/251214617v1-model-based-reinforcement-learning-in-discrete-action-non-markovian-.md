---
layout: default
title: Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes
---

# Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes

**arXiv**: [2512.14617v1](https://arxiv.org/abs/2512.14617) | [PDF](https://arxiv.org/pdf/2512.14617.pdf)

**ä½œè€…**: Alessandro Trapasso, Luca Iocchi, Fabio Patrizi

**åˆ†ç±»**: cs.LG, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 19 pages, 32 figures, includes appendix

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºQR-MAXç®—æ³•ä»¥è§£å†³ç¦»æ•£åŠ¨ä½œéžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹ä¸­çš„æ ·æœ¬æ•ˆçŽ‡å’Œæœ€ä¼˜æ€§ä¿è¯é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **è§†è§‰é‡Œç¨‹è®¡** **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `éžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹` `åŸºäºŽæ¨¡åž‹å¼ºåŒ–å­¦ä¹ ` `å¥–åŠ±æœº` `æ ·æœ¬æ•ˆçŽ‡` `PACæ”¶æ•›` `ç¦»æ•£åŠ¨ä½œç©ºé—´` `è¿žç»­çŠ¶æ€ç©ºé—´` `SimHashç¦»æ•£åŒ–`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸é€‚ç”¨äºŽä¾èµ–åŽ†å²çš„ä»»åŠ¡ï¼Œè€Œéžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹æ–¹æ³•ç¼ºä¹æœ€ä¼˜æ€§å’Œæ ·æœ¬æ•ˆçŽ‡çš„å½¢å¼åŒ–ä¿è¯ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºQR-MAXç®—æ³•ï¼Œé€šè¿‡å¥–åŠ±æœºå› å­åˆ†è§£é©¬å°”å¯å¤«è½¬ç§»å’Œéžé©¬å°”å¯å¤«å¥–åŠ±ï¼Œå®žçŽ°å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦çš„Îµ-æœ€ä¼˜ç­–ç•¥æ”¶æ•›ã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨å¤æ‚çŽ¯å¢ƒä¸­ï¼ŒQR-MAXç›¸æ¯”çŽ°æœ‰æ–¹æ³•æ˜¾è‘—æå‡æ ·æœ¬æ•ˆçŽ‡å’Œé²æ£’æ€§ï¼ŒBucket-QR-MAXæ‰©å±•è‡³è¿žç»­çŠ¶æ€ç©ºé—´ã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

è®¸å¤šå®žé™…å†³ç­–é—®é¢˜æ¶‰åŠçš„ä»»åŠ¡æˆåŠŸå–å†³äºŽæ•´ä¸ªç³»ç»ŸåŽ†å²ï¼Œè€Œéžä»…è¾¾åˆ°å…·æœ‰æœŸæœ›å±žæ€§çš„çŠ¶æ€ã€‚é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸é€‚ç”¨äºŽæ­¤ç±»ä»»åŠ¡ï¼Œè€Œéžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå¤„ç†æ—¶é—´ä¾èµ–æ€§ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•é•¿æœŸä»¥æ¥ç¼ºä¹å¯¹ï¼ˆè¿‘ä¼¼ï¼‰æœ€ä¼˜æ€§å’Œæ ·æœ¬æ•ˆçŽ‡çš„å½¢å¼åŒ–ä¿è¯ã€‚æˆ‘ä»¬é€šè¿‡QR-MAXä¸ºè§£å†³è¿™ä¸¤ä¸ªé—®é¢˜åšå‡ºè´¡çŒ®ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºŽç¦»æ•£NMRDPçš„æ–°åž‹åŸºäºŽæ¨¡åž‹çš„ç®—æ³•ï¼Œé€šè¿‡å¥–åŠ±æœºå°†é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ ä¸Žéžé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†è¿›è¡Œå› å­åˆ†è§£ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨è¿™ç§å› å­åˆ†è§£èŽ·å¾—å…·æœ‰å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦çš„Îµ-æœ€ä¼˜ç­–ç•¥çš„PACæ”¶æ•›æ€§çš„ç¦»æ•£åŠ¨ä½œNMRDPåŸºäºŽæ¨¡åž‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†QR-MAXæ‰©å±•åˆ°è¿žç»­çŠ¶æ€ç©ºé—´ï¼Œæå‡ºBucket-QR-MAXï¼Œè¿™æ˜¯ä¸€ç§åŸºäºŽSimHashçš„ç¦»æ•£åŒ–å™¨ï¼Œä¿æŒç›¸åŒçš„å› å­åˆ†è§£ç»“æž„ï¼Œæ— éœ€æ‰‹åŠ¨ç½‘æ ¼åŒ–æˆ–å‡½æ•°é€¼è¿‘å³å¯å®žçŽ°å¿«é€Ÿç¨³å®šå­¦ä¹ ã€‚æˆ‘ä»¬åœ¨å¤æ‚æ€§é€’å¢žçš„çŽ¯å¢ƒä¸­å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ŽçŽ°ä»£æœ€å…ˆè¿›çš„åŸºäºŽæ¨¡åž‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œå®žéªŒæ¯”è¾ƒï¼Œæ˜¾ç¤ºå‡ºæ ·æœ¬æ•ˆçŽ‡çš„æ˜¾è‘—æé«˜å’Œå¯»æ‰¾æœ€ä¼˜ç­–ç•¥çš„é²æ£’æ€§å¢žå¼ºã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡è§£å†³ç¦»æ•£åŠ¨ä½œéžé©¬å°”å¯å¤«å¥–åŠ±å†³ç­–è¿‡ç¨‹ä¸­çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­ä»»åŠ¡æˆåŠŸä¾èµ–äºŽæ•´ä¸ªç³»ç»ŸåŽ†å²è€Œéžå•ä¸ªçŠ¶æ€ã€‚çŽ°æœ‰åŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨NMRDPä¸­ç¼ºä¹å½¢å¼åŒ–ä¿è¯ï¼Œå¯¼è‡´æ ·æœ¬æ•ˆçŽ‡ä½Žå’Œæœ€ä¼˜æ€§ä¸ç¡®å®šï¼Œéš¾ä»¥å¤„ç†æ—¶é—´ä¾èµ–æ€§ä»»åŠ¡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šæ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡å› å­åˆ†è§£å°†é©¬å°”å¯å¤«è½¬ç§»å­¦ä¹ ä¸Žéžé©¬å°”å¯å¤«å¥–åŠ±å¤„ç†åˆ†ç¦»ï¼Œåˆ©ç”¨å¥–åŠ±æœºå»ºæ¨¡å¥–åŠ±ç»“æž„ï¼Œä»Žè€Œç®€åŒ–å­¦ä¹ è¿‡ç¨‹å¹¶å®žçŽ°ç†è®ºä¿è¯ã€‚è¿™ç§è®¾è®¡åŸºäºŽå¥–åŠ±æœºèƒ½æœ‰æ•ˆæ•æ‰æ—¶é—´ä¾èµ–æ€§çš„è§‚å¯Ÿï¼Œé¿å…ç›´æŽ¥å¤„ç†å¤æ‚éžé©¬å°”å¯å¤«å¥–åŠ±çš„å›°éš¾ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šæ•´ä½“æž¶æž„åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨å¥–åŠ±æœºå°†éžé©¬å°”å¯å¤«å¥–åŠ±è½¬æ¢ä¸ºé©¬å°”å¯å¤«å½¢å¼ï¼Œé€šè¿‡çŠ¶æ€æ‰©å±•å°†NMRDPè½¬æ¢ä¸ºç­‰æ•ˆçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼›å…¶æ¬¡ï¼Œåº”ç”¨åŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚QR-MAXï¼‰å­¦ä¹ è½¬ç§»æ¨¡åž‹å’Œç­–ç•¥ã€‚å¯¹äºŽè¿žç»­çŠ¶æ€ç©ºé—´ï¼Œå¼•å…¥Bucket-QR-MAXï¼ŒåŸºäºŽSimHashè¿›è¡Œè‡ªåŠ¨ç¦»æ•£åŒ–ï¼Œä¿æŒå› å­åˆ†è§£ç»“æž„ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°æ˜¯é¦–æ¬¡æå‡ºåŸºäºŽæ¨¡åž‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•QR-MAXï¼Œä¸“é—¨é’ˆå¯¹ç¦»æ•£åŠ¨ä½œNMRDPï¼Œé€šè¿‡å¥–åŠ±æœºå› å­åˆ†è§£å®žçŽ°PACæ”¶æ•›åˆ°Îµ-æœ€ä¼˜ç­–ç•¥ï¼Œå…·æœ‰å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬è´¨åŒºåˆ«åœ¨äºŽç»“åˆäº†æ¨¡åž‹å­¦ä¹ å’Œå¥–åŠ±æœºç†è®ºï¼Œæä¾›å½¢å¼åŒ–ä¿è¯ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•å¤šä¾èµ–å¯å‘å¼æˆ–æ— æ¨¡åž‹å­¦ä¹ ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬å¥–åŠ±æœºçš„æž„å»ºï¼Œç”¨äºŽç¼–ç éžé©¬å°”å¯å¤«å¥–åŠ±é€»è¾‘ï¼›QR-MAXç®—æ³•ä¸­çš„Qå€¼æ›´æ–°è§„åˆ™ï¼ŒåŸºäºŽå­¦ä¹ åˆ°çš„è½¬ç§»æ¨¡åž‹è¿›è¡Œè§„åˆ’ï¼›Bucket-QR-MAXä¸­çš„SimHashç¦»æ•£åŒ–ï¼Œå‚æ•°å¦‚å“ˆå¸Œå‡½æ•°æ•°é‡å’Œæ¡¶å¤§å°å½±å“çŠ¶æ€ç©ºé—´ç²’åº¦ï¼›æŸå¤±å‡½æ•°åŸºäºŽè´å°”æ›¼è¯¯å·®æœ€å°åŒ–ï¼Œç½‘ç»œç»“æž„æœªçŸ¥ï¼Œä½†å¯èƒ½æ¶‰åŠå€¼å‡½æ•°æˆ–ç­–ç•¥ç½‘ç»œè¿‘ä¼¼ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

å®žéªŒåœ¨å¤æ‚æ€§é€’å¢žçš„çŽ¯å¢ƒä¸­è¿›è¡Œï¼ŒQR-MAXç›¸æ¯”çŽ°ä»£æœ€å…ˆè¿›çš„åŸºäºŽæ¨¡åž‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚æœªçŸ¥åŸºçº¿ï¼‰æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼šæ ·æœ¬æ•ˆçŽ‡æå‡å…·ä½“å¹…åº¦æœªçŸ¥ï¼Œä½†è®ºæ–‡æŠ¥å‘Šäº†æ›´å¿«æ”¶æ•›å’Œæ›´é«˜æˆåŠŸçŽ‡ï¼›Bucket-QR-MAXåœ¨è¿žç»­çŠ¶æ€ç©ºé—´ä¸­å®žçŽ°ç¨³å®šå­¦ä¹ ï¼Œæ— éœ€æ‰‹åŠ¨ç½‘æ ¼åŒ–ï¼Œé²æ£’æ€§å¢žå¼ºï¼Œå…·ä½“æ€§èƒ½æ•°æ®å¦‚å¥–åŠ±ç´¯ç§¯æˆ–æ­¥æ•°å‡å°‘æœªæä¾›ï¼Œä½†å¼ºè°ƒäº†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIå’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸæœ‰æ½œåœ¨åº”ç”¨ï¼Œç‰¹åˆ«é€‚ç”¨äºŽéœ€è¦é•¿æœŸåŽ†å²ä¾èµ–çš„å†³ç­–åœºæ™¯ï¼Œå¦‚åºåˆ—ä»»åŠ¡å®Œæˆæˆ–å¤æ‚çŽ¯å¢ƒäº¤äº’ã€‚å®žé™…ä»·å€¼åœ¨äºŽæé«˜æ™ºèƒ½ä½“åœ¨éžé©¬å°”å¯å¤«çŽ¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆçŽ‡å’Œç­–ç•¥æœ€ä¼˜æ€§ï¼Œæœªæ¥å¯èƒ½æŽ¨åŠ¨å¼ºåŒ–å­¦ä¹ åœ¨çŽ°å®žä¸–ç•Œå¤æ‚ä»»åŠ¡ä¸­çš„éƒ¨ç½²ï¼Œå‡å°‘æ ·æœ¬éœ€æ±‚å¹¶å¢žå¼ºé²æ£’æ€§ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

