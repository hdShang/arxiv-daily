---
layout: default
title: SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models
---

# SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models

**arXiv**: [2512.14481v1](https://arxiv.org/abs/2512.14481) | [PDF](https://arxiv.org/pdf/2512.14481.pdf)

**ä½œè€…**: Shizhuo Mao, Song Chen, Yi Kang

**åˆ†ç±»**: cs.CL, cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

---

## ðŸ’¡ ä¸€å¥è¯è¦ç‚¹

**æå‡ºSASQæ¡†æž¶ä»¥è§£å†³å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–è®­ç»ƒä¸­æ¿€æ´»é‡åŒ–å› å­ä¼˜åŒ–çš„æ•ˆçŽ‡ä¸Žç²¾åº¦å¹³è¡¡é—®é¢˜**

ðŸŽ¯ **åŒ¹é…é¢†åŸŸ**: **å¼ºåŒ–å­¦ä¹ **

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–` `æ¿€æ´»é‡åŒ–` `é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ` `é™æ€æŽ¨ç†` `è½»é‡çº§ä¼˜åŒ–` `æ¨¡åž‹éƒ¨ç½²` `è¾¹ç¼˜è®¡ç®—` `ç²¾åº¦æå‡`

## ðŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. æ ¸å¿ƒé—®é¢˜ï¼šçŽ°æœ‰é‡åŒ–æ–¹æ³•é¢ä¸´åŠ¨æ€é‡åŒ–è®¡ç®—å¼€é”€é«˜ã€é™æ€é‡åŒ–ç²¾åº¦ä½Žï¼Œä»¥åŠé‡åŒ–æ„ŸçŸ¥è®­ç»ƒæƒé‡è®­ç»ƒæˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚
2. æ–¹æ³•è¦ç‚¹ï¼šæå‡ºSASQæ¡†æž¶ï¼Œä»…ä¼˜åŒ–æ¿€æ´»é‡åŒ–å› å­è€Œä¸æ”¹å˜é¢„è®­ç»ƒæƒé‡ï¼Œå®žçŽ°è½»é‡çº§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒã€‚
3. å®žéªŒæˆ–æ•ˆæžœï¼šåœ¨LLaMA2-7Bä¸Šï¼ŒSASQçš„å›°æƒ‘åº¦æ¯”QuaRotä½Ž5.2%ï¼Œæ¯”FP16æ¨¡åž‹ä½Ž4.7%ï¼Œè¶…è¶Šäº†çŽ°æœ‰SOTAæ–¹æ¡ˆã€‚

## ðŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

å¤§è¯­è¨€æ¨¡åž‹åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†å…¶è§„æ¨¡å¢žé•¿è¶…è¿‡äº†GPUå†…å­˜çš„è¿›æ­¥ï¼Œç»™éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æ¨¡åž‹é‡åŒ–é€šè¿‡é™ä½Žæƒé‡å’Œæ¿€æ´»çš„ç²¾åº¦æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†çŽ°æœ‰è§£å†³æ–¹æ¡ˆé¢ä¸´æ ¹æœ¬æ€§çš„æƒè¡¡ï¼šåŠ¨æ€é‡åŒ–è®¡ç®—å¼€é”€é«˜ä¸”åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾ï¼Œè€Œé™æ€é‡åŒ–åˆ™ç‰ºç‰²äº†å‡†ç¡®æ€§ã€‚çŽ°æœ‰çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹æ³•è¿˜é¢ä¸´æƒé‡è®­ç»ƒæˆæœ¬é«˜çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†SASQï¼šä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ¿€æ´»é‡åŒ–å› å­çš„è½»é‡çº§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ¡†æž¶ã€‚SASQä»…ä¼˜åŒ–é‡åŒ–å› å­ï¼ˆä¸æ”¹å˜é¢„è®­ç»ƒæƒé‡ï¼‰ï¼Œå®žçŽ°äº†é«˜ç²¾åº¦çš„é™æ€æŽ¨ç†ï¼ŒåŒæ—¶ä¿æŒäº†éƒ¨ç½²æ•ˆçŽ‡ã€‚SASQè‡ªé€‚åº”åœ°æˆªæ–­ä¸€äº›å¼‚å¸¸å€¼ï¼Œä»Žè€Œé™ä½Žäº†é‡åŒ–çš„éš¾åº¦ï¼ŒåŒæ—¶ä¿ç•™äº†æ¿€æ´»çš„åˆ†å¸ƒç‰¹æ€§ã€‚SASQä¸ä»…è¶…è¶Šäº†çŽ°æœ‰çš„æœ€å…ˆè¿›é‡åŒ–æ–¹æ¡ˆï¼Œè¿˜ä¼˜äºŽç›¸åº”çš„FP16æ¨¡åž‹ã€‚åœ¨LLaMA2-7Bä¸Šï¼Œå®ƒåœ¨WikiText2ä¸Šçš„å›°æƒ‘åº¦æ¯”QuaRotä½Ž5.2%ï¼Œæ¯”FP16æ¨¡åž‹ä½Ž4.7%ã€‚

## ðŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡åž‹é‡åŒ–éƒ¨ç½²ä¸­çš„æ ¸å¿ƒé—®é¢˜ï¼Œå³å¦‚ä½•åœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®žçŽ°é«˜æ•ˆçš„é™æ€æŽ¨ç†ã€‚çŽ°æœ‰æ–¹æ³•çš„ç—›ç‚¹åŒ…æ‹¬ï¼šåŠ¨æ€é‡åŒ–å› å®žæ—¶è®¡ç®—é‡åŒ–å‚æ•°å¯¼è‡´é«˜è®¡ç®—å¼€é”€å’Œéƒ¨ç½²å›°éš¾ï¼›é™æ€é‡åŒ–è™½ç„¶éƒ¨ç½²é«˜æ•ˆï¼Œä½†ç²¾åº¦æŸå¤±æ˜¾è‘—ï¼›è€Œä¼ ç»Ÿçš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹æ³•éœ€è¦é‡æ–°è®­ç»ƒæƒé‡ï¼Œæˆæœ¬é«˜æ˜‚ä¸”å¯èƒ½ç ´åé¢„è®­ç»ƒæ¨¡åž‹çš„æ€§èƒ½ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šè®ºæ–‡çš„æ ¸å¿ƒè§£å†³æ€è·¯æ˜¯ä¸“æ³¨äºŽä¼˜åŒ–æ¿€æ´»é‡åŒ–å› å­ï¼Œè€Œä¸æ”¹å˜é¢„è®­ç»ƒæƒé‡ã€‚è¿™æ ·è®¾è®¡çš„åŽŸå› åœ¨äºŽï¼Œæ¿€æ´»çš„åˆ†å¸ƒç‰¹æ€§å¯¹é‡åŒ–ç²¾åº¦å½±å“æ›´å¤§ï¼Œä¸”ä¼˜åŒ–é‡åŒ–å› å­æ¯”é‡æ–°è®­ç»ƒæƒé‡æ›´è½»é‡çº§ï¼Œèƒ½å¤Ÿå¹³è¡¡ç²¾åº¦ä¸Žæ•ˆçŽ‡ã€‚é€šè¿‡è‡ªé€‚åº”æˆªæ–­å¼‚å¸¸å€¼ï¼Œé™ä½Žé‡åŒ–éš¾åº¦ï¼ŒåŒæ—¶ä¿ç•™æ¿€æ´»çš„åˆ†å¸ƒä¿¡æ¯ï¼Œä»Žè€Œå®žçŽ°é«˜ç²¾åº¦çš„é™æ€é‡åŒ–ã€‚

**æŠ€æœ¯æ¡†æž¶**ï¼šSASQçš„æ•´ä½“æž¶æž„æ˜¯ä¸€ä¸ªè½»é‡çº§çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ¡†æž¶ï¼Œä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œåœ¨é¢„è®­ç»ƒæ¨¡åž‹çš„åŸºç¡€ä¸Šï¼Œä»…å¯¹æ¿€æ´»é‡åŒ–å› å­è¿›è¡Œä¼˜åŒ–ï¼›å…¶æ¬¡ï¼Œé€šè¿‡è‡ªé€‚åº”æˆªæ–­æœºåˆ¶å¤„ç†æ¿€æ´»ä¸­çš„å¼‚å¸¸å€¼ï¼Œä»¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚æµç¨‹ä¸Šï¼Œå®ƒé¿å…äº†æƒé‡æ›´æ–°ï¼Œä¸“æ³¨äºŽé‡åŒ–å‚æ•°çš„è°ƒæ•´ï¼Œæœ€ç»ˆè¾“å‡ºä¸€ä¸ªå¯ç›´æŽ¥ç”¨äºŽé™æ€æŽ¨ç†çš„é‡åŒ–æ¨¡åž‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ç‚¹æ˜¯æå‡ºäº†ä¸€ç§ä»…ä¼˜åŒ–æ¿€æ´»é‡åŒ–å› å­çš„è½»é‡çº§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹æ³•ã€‚ä¸ŽçŽ°æœ‰æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«åœ¨äºŽï¼šå®ƒä¸æ¶‰åŠæƒé‡è®­ç»ƒï¼Œä»Žè€Œé™ä½Žäº†è®¡ç®—æˆæœ¬å’Œéƒ¨ç½²å¤æ‚åº¦ï¼›åŒæ—¶ï¼Œé€šè¿‡è‡ªé€‚åº”æˆªæ–­å¼‚å¸¸å€¼ï¼Œè§£å†³äº†æ¿€æ´»åˆ†å¸ƒä¸­æžç«¯å€¼å¯¹é‡åŒ–çš„è´Ÿé¢å½±å“ï¼Œè¿™åœ¨ä¼ ç»Ÿé™æ€é‡åŒ–ä¸­å¸¸è¢«å¿½ç•¥ã€‚

**å…³é”®è®¾è®¡**ï¼šå…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä½¿ç”¨é‡åŒ–å› å­ä½œä¸ºå¯ä¼˜åŒ–å‚æ•°ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒï¼›å¼•å…¥è‡ªé€‚åº”æˆªæ–­æœºåˆ¶ï¼Œæ ¹æ®æ¿€æ´»åˆ†å¸ƒåŠ¨æ€è°ƒæ•´æˆªæ–­é˜ˆå€¼ï¼Œä»¥å¹³è¡¡ç²¾åº¦å’Œé‡åŒ–èŒƒå›´ï¼›æŸå¤±å‡½æ•°å¯èƒ½åŸºäºŽæ¨¡åž‹è¾“å‡ºä¸ŽåŽŸå§‹ç²¾åº¦çš„å·®å¼‚ï¼Œä½†å…·ä½“ç»†èŠ‚åœ¨æ‘˜è¦ä¸­æœªæ˜Žç¡®è¯´æ˜Žï¼›ç½‘ç»œç»“æž„ä¿æŒé¢„è®­ç»ƒæƒé‡ä¸å˜ï¼Œä»…åœ¨å‰å‘ä¼ æ’­ä¸­åº”ç”¨é‡åŒ–æ“ä½œã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æ¡†æž¶çš„è½»é‡æ€§å’Œé«˜æ•ˆæ€§ã€‚

## ðŸ“Š å®žéªŒäº®ç‚¹

æœ€é‡è¦çš„å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒSASQåœ¨LLaMA2-7Bæ¨¡åž‹ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚åœ¨WikiText2æ•°æ®é›†ä¸Šï¼Œå…¶å›°æƒ‘åº¦æ¯”çŽ°æœ‰æœ€å…ˆè¿›é‡åŒ–æ–¹æ¡ˆQuaRotä½Ž5.2%ï¼Œæ¯”åŽŸå§‹FP16æ¨¡åž‹ä½Ž4.7%ï¼Œè¿™è¡¨æ˜ŽSASQä¸ä»…è¶…è¶Šäº†å…¶ä»–é‡åŒ–æ–¹æ³•ï¼Œç”šè‡³ä¼˜äºŽå…¨ç²¾åº¦æ¨¡åž‹ï¼Œå®žçŽ°äº†é‡åŒ–åŽçš„ç²¾åº¦å¢žç›Šã€‚

## ðŸŽ¯ åº”ç”¨åœºæ™¯

è¯¥ç ”ç©¶ä¸»è¦åº”ç”¨äºŽå¤§è¯­è¨€æ¨¡åž‹çš„è¾¹ç¼˜éƒ¨ç½²å’Œèµ„æºå—é™çŽ¯å¢ƒï¼Œå¦‚ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿå’Œäº‘è®¡ç®—ä¸­çš„é«˜æ•ˆæŽ¨ç†ã€‚å…¶å®žé™…ä»·å€¼åœ¨äºŽé™ä½Žæ¨¡åž‹çš„å†…å­˜å ç”¨å’Œè®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦ï¼Œæœ‰åŠ©äºŽæŽ¨åŠ¨AIæ¨¡åž‹åœ¨å®žæ—¶åº”ç”¨ä¸­çš„æ™®åŠã€‚æœªæ¥å½±å“å¯èƒ½æ‰©å±•åˆ°å…¶ä»–å¤§è§„æ¨¡ç¥žç»ç½‘ç»œæ¨¡åž‹çš„é‡åŒ–ä¼˜åŒ–ï¼Œæå‡æ•´ä½“AIç³»ç»Ÿçš„èƒ½æ•ˆæ¯”ã€‚

## ðŸ“„ æ‘˜è¦ï¼ˆåŽŸæ–‡ï¼‰

> Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

