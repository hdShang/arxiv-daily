---
layout: default
title: Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis
---

# Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14157" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14157</a>
  <a href="https://arxiv.org/pdf/2512.14157.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14157" onclick="toggleFavorite(this, '2512.14157', 'Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Yankai Jiang, Yujie Zhang, Peng Zhang, Yichen Li, Jintai Chen, Xiaoming Shi, Shihui Zhen

**åˆ†ç±»**: cs.AI, cs.CV

**å‘å¸ƒæ—¥æœŸ**: 2025-12-18

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**Ophiuchusï¼šä¸€ç§å·¥å…·å¢å¼ºçš„åŒ»å­¦å›¾åƒåˆ†ææ¡†æ¶ï¼Œæå‡MLLMçš„æ¨ç†èƒ½åŠ›**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `åŒ»å­¦å›¾åƒåˆ†æ` `å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹` `å·¥å…·å¢å¼º` `æ¨ç†é“¾` `å¼ºåŒ–å­¦ä¹ `

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰åŒ»å­¦MLLMåœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œéš¾ä»¥åŠ¨æ€èšç„¦ç»†ç²’åº¦è§†è§‰åŒºåŸŸï¼Œå½±å“å®šä½å’Œè¯Šæ–­ç²¾åº¦ã€‚
2. Ophiuchusæ¡†æ¶é€šè¿‡å·¥å…·å¢å¼ºï¼Œä½¿MLLMå…·å¤‡åˆ¤æ–­ã€å®šä½å’Œèåˆè§†è§‰è¯æ®çš„èƒ½åŠ›ï¼Œæå‡æ¨ç†è´¨é‡ã€‚
3. ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨ã€è‡ªåæ€å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹èƒ½æœ‰æ•ˆåˆ©ç”¨å·¥å…·å¹¶æ¨¡æ‹Ÿä¸“å®¶è¡Œä¸ºã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºOphiuchusçš„é€šç”¨å·¥å…·å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨æå‡åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åŠ¨æ€åœ°ã€è¿­ä»£åœ°èšç„¦äºç»†ç²’åº¦çš„è§†è§‰åŒºåŸŸï¼Œä»è€Œå½±å“ç²¾ç¡®çš„å®šä½å’Œè¯Šæ–­ã€‚Ophiuchusèµ‹äºˆMLLMä»¥ä¸‹èƒ½åŠ›ï¼šï¼ˆiï¼‰åˆ¤æ–­ä½•æ—¶éœ€è¦é¢å¤–çš„è§†è§‰è¯æ®ï¼›ï¼ˆiiï¼‰ç¡®å®šåœ¨åŒ»å­¦å›¾åƒä¸­æ¢æµ‹å’Œå®šä½çš„ä½ç½®ï¼›ï¼ˆiiiï¼‰å°†ç›¸å…³çš„å­å›¾åƒå†…å®¹æ— ç¼åœ°èå…¥åˆ°äº¤é”™çš„å¤šæ¨¡æ€æ€ç»´é“¾ä¸­ã€‚ä¸å—é™äºä¸“ç”¨å·¥å…·æ€§èƒ½ä¸Šé™çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒOphiuchuså°†æ¨¡å‹å›ºæœ‰çš„å®šä½å’Œæ„ŸçŸ¥èƒ½åŠ›ä¸å¤–éƒ¨å·¥å…·é›†æˆï¼Œä»è€Œä¿ƒè¿›æ›´é«˜å±‚æ¬¡çš„æ¨ç†ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šä½¿ç”¨å·¥å…·é›†æˆæ¨ç†æ•°æ®è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒï¼Œä»¥å®ç°åŸºæœ¬çš„å·¥å…·é€‰æ‹©å’Œå…³é”®åŒºåŸŸæ£€æŸ¥é€‚åº”ï¼›è‡ªåæ€å¾®è°ƒï¼Œä»¥åŠ å¼ºåæ€æ€§æ¨ç†å¹¶é¼“åŠ±é‡æ–°å®¡è§†å·¥å…·è¾“å‡ºï¼›ä»¥åŠAgenticå·¥å…·å¼ºåŒ–å­¦ä¹ ï¼Œä»¥ç›´æ¥ä¼˜åŒ–ç‰¹å®šäºä»»åŠ¡çš„å¥–åŠ±å¹¶æ¨¡æ‹Ÿä¸“å®¶çº§è¯Šæ–­è¡Œä¸ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOphiuchusåœ¨å„ç§åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºé—­æºå’Œå¼€æºçš„SOTAæ–¹æ³•ï¼ŒåŒ…æ‹¬VQAã€æ£€æµ‹å’ŒåŸºäºæ¨ç†çš„åˆ†å‰²ã€‚è¯¥æ–¹æ³•ä¸ºåŒ»å­¦AIæ™ºèƒ½ä½“å¼€è¾Ÿäº†ä¸€æ¡æ–°é€”å¾„ï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡å·¥å…·é›†æˆæ¨ç†çœŸæ­£åœ°â€œç”¨å›¾åƒæ€è€ƒâ€ã€‚æ•°æ®é›†ã€ä»£ç å’Œè®­ç»ƒæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šç°æœ‰åŸºäºæ¨ç†çš„åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†éœ€è¦ç²¾ç»†è§†è§‰åŒºåŸŸå…³æ³¨çš„å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚å®ƒä»¬éš¾ä»¥åŠ¨æ€åœ°ã€è¿­ä»£åœ°èšç„¦äºå›¾åƒçš„ç‰¹å®šåŒºåŸŸä»¥è¿›è¡Œç²¾ç¡®çš„å®šä½å’Œè¯Šæ–­ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å—é™äºç‰¹å®šå·¥å…·çš„æ€§èƒ½ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šOphiuchusçš„æ ¸å¿ƒæ€è·¯æ˜¯å°†MLLMå›ºæœ‰çš„è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸å¤–éƒ¨å·¥å…·ç›¸ç»“åˆï¼Œå½¢æˆä¸€ä¸ªå·¥å…·å¢å¼ºçš„æ¨ç†æ¡†æ¶ã€‚é€šè¿‡è®©æ¨¡å‹è‡ªä¸»å†³å®šä½•æ—¶éœ€è¦é¢å¤–çš„è§†è§‰è¯æ®ï¼Œå¹¶ç¡®å®šåœ¨å›¾åƒä¸­éœ€è¦æ¢æµ‹å’Œå®šä½çš„å…³é”®åŒºåŸŸï¼ŒOphiuchusèƒ½å¤Ÿå°†ç›¸å…³çš„å­å›¾åƒä¿¡æ¯æ— ç¼åœ°èå…¥åˆ°å¤šæ¨¡æ€çš„æ¨ç†é“¾ä¸­ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ•´ä½“æ¨ç†èƒ½åŠ›ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šOphiuchusæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š1) å†·å¯åŠ¨è®­ç»ƒï¼šä½¿ç”¨å·¥å…·é›†æˆæ¨ç†æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶é€‚åº”å…³é”®åŒºåŸŸçš„æ£€æŸ¥ã€‚2) è‡ªåæ€å¾®è°ƒï¼šé€šè¿‡è®©æ¨¡å‹åæ€è‡ªèº«çš„æ¨ç†è¿‡ç¨‹å’Œå·¥å…·è¾“å‡ºï¼ŒåŠ å¼ºåæ€æ€§æ¨ç†èƒ½åŠ›ï¼Œå¹¶é¼“åŠ±æ¨¡å‹é‡æ–°å®¡è§†å·¥å…·çš„è¾“å‡ºç»“æœã€‚3) Agenticå·¥å…·å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡ç›´æ¥ä¼˜åŒ–ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±ï¼Œå¹¶æ¨¡æ‹Ÿä¸“å®¶çº§çš„è¯Šæ–­è¡Œä¸ºï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

**å…³é”®åˆ›æ–°**ï¼šOphiuchusçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å·¥å…·å¢å¼ºçš„æ¨ç†æ¡†æ¶å’Œä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚ä¸ä»¥å¾€ä¾èµ–äºç‰¹å®šå·¥å…·çš„æ–¹æ³•ä¸åŒï¼ŒOphiuchuså°†æ¨¡å‹è‡ªèº«çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸å¤–éƒ¨å·¥å…·ç›¸ç»“åˆï¼Œä»è€Œçªç ´äº†æ€§èƒ½ä¸Šé™ã€‚ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥åˆ™ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å·¥å…·ï¼Œå¹¶é€æ­¥æå‡æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®è®¾è®¡**ï¼šOphiuchusæ¡†æ¶çš„å…·ä½“æŠ€æœ¯ç»†èŠ‚åŒ…æ‹¬ï¼šå·¥å…·é€‰æ‹©æ¨¡å—ï¼Œç”¨äºåˆ¤æ–­ä½•æ—¶éœ€è¦ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼›åŒºåŸŸå®šä½æ¨¡å—ï¼Œç”¨äºç¡®å®šåœ¨å›¾åƒä¸­éœ€è¦æ¢æµ‹çš„å…³é”®åŒºåŸŸï¼›å¤šæ¨¡æ€èåˆæ¨¡å—ï¼Œç”¨äºå°†å·¥å…·è¾“å‡ºçš„è§†è§‰ä¿¡æ¯èå…¥åˆ°æ¨ç†é“¾ä¸­ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†å¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬å·¥å…·é€‰æ‹©æŸå¤±ã€åŒºåŸŸå®šä½æŸå¤±å’Œæ¨ç†æŸå¤±ã€‚Agenticå·¥å…·å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨äº†ç‰¹å®šäºä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ¨¡å‹æ¨¡æ‹Ÿä¸“å®¶çº§çš„è¯Šæ–­è¡Œä¸ºã€‚

## ğŸ–¼ï¸ å…³é”®å›¾ç‰‡

<div class="paper-figures">
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14157/x1.png" alt="fig_0" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14157/x2.png" alt="fig_1" loading="lazy">
</figure>
<figure class="paper-figure">
<img src="https://arxiv.org/html/2512.14157/x3.png" alt="fig_2" loading="lazy">
</figure>
</div>

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒOphiuchusåœ¨VQAã€æ£€æµ‹å’ŒåŸºäºæ¨ç†çš„åˆ†å‰²ç­‰å¤šä¸ªåŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œå§‹ç»ˆä¼˜äºå½“å‰æœ€å…ˆè¿›çš„é—­æºå’Œå¼€æºæ–¹æ³•ã€‚å…·ä½“æ€§èƒ½æå‡æ•°æ®åœ¨è®ºæ–‡ä¸­ç»™å‡ºï¼Œè¯æ˜äº†Ophiuchusæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚è¯¥æ¡†æ¶ä¸ºåŒ»å­¦AIæ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†ä¸€æ¡æœ‰å‰æ™¯çš„é“è·¯ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

Ophiuchusæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºè¾…åŠ©åŒ»ç”Ÿè¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†æã€ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæå‡è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå‡å°‘è¯¯è¯Šç‡ï¼Œå¹¶ä¸ºæ‚£è€…æä¾›æ›´ä¸ªæ€§åŒ–çš„åŒ»ç–—æœåŠ¡ã€‚æœªæ¥ï¼ŒOphiuchusæœ‰æœ›æˆä¸ºåŒ»å­¦AIé¢†åŸŸçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ¨åŠ¨åŒ»ç–—æ™ºèƒ½åŒ–å‘å±•ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.

