---
layout: default
title: RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees
---

# RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.14069" class="toolbar-btn" target="_blank">ğŸ“„ arXiv: 2512.14069v1</a>
  <a href="https://arxiv.org/pdf/2512.14069.pdf" class="toolbar-btn" target="_blank">ğŸ“¥ PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.14069v1" onclick="toggleFavorite(this, '2512.14069v1', 'RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees')" title="æ·»åŠ åˆ°æ”¶è—å¤¹">â˜† æ”¶è—</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">ğŸ”— åˆ†äº«</button>
</div>


**ä½œè€…**: Junjie Ma, Jinlong Li

**åˆ†ç±»**: cs.AI

**å‘å¸ƒæ—¥æœŸ**: 2025-12-16

**å¤‡æ³¨**: 5 pages, 2 figures

**ğŸ”— ä»£ç /é¡¹ç›®**: [GITHUB](https://github.com/minaduki-sora/RADAR)

---

## ğŸ’¡ ä¸€å¥è¯è¦ç‚¹

**RADARï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€è‰ç¨¿æ ‘åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹æ¨ç†**

ğŸ¯ **åŒ¹é…é¢†åŸŸ**: **æ”¯æŸ±äºŒï¼šRLç®—æ³•ä¸æ¶æ„ (RL & Architecture)** **æ”¯æŸ±ä¹ï¼šå…·èº«å¤§æ¨¡å‹ (Embodied Foundation Models)**

**å…³é”®è¯**: `å¤§è¯­è¨€æ¨¡å‹` `æ¨ç†åŠ é€Ÿ` `æ¨æµ‹é‡‡æ ·` `å¼ºåŒ–å­¦ä¹ ` `åŠ¨æ€è‰ç¨¿æ ‘`

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹

1. ç°æœ‰æ¨æµ‹é‡‡æ ·æ–¹æ³•ä¸­ï¼Œè‰ç¨¿æ¨¡å‹è°ƒç”¨æ¬¡æ•°ä¸ºé¢„è®¾è¶…å‚æ•°ï¼Œç¼ºä¹çµæ´»æ€§ï¼Œå¯¼è‡´è®¡ç®—å†—ä½™ã€‚
2. RADARå°†è‰ç¨¿æ ‘ç”Ÿæˆå»ºæ¨¡ä¸ºMDPï¼Œåˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¢„æµ‹æ¨¡å‹ï¼Œå®æ—¶å†³ç­–è‰ç¨¿æ¨¡å‹è°ƒç”¨æ¬¡æ•°ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼ŒRADARåœ¨å¤šä¸ªLLMå’Œä»»åŠ¡ä¸Šå®ç°äº†3.17å€-4.82å€çš„åŠ é€Ÿï¼Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚

## ğŸ“ æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰

ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æˆæœ¬é«˜ä¸”é€Ÿåº¦æ…¢ï¼Œæ¨æµ‹é‡‡æ ·å·²æˆä¸ºè§£å†³æ­¤é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œæ¨æµ‹é‡‡æ ·ä¸­ç”¨äºç”Ÿæˆå€™é€‰tokençš„è‰ç¨¿æ¨¡å‹è°ƒç”¨æ¬¡æ•°æ˜¯ä¸€ä¸ªé¢„è®¾çš„è¶…å‚æ•°ï¼Œç¼ºä¹çµæ´»æ€§ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°ç”Ÿæˆå’Œåˆ©ç”¨å€™é€‰tokenï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨æµ‹é‡‡æ ·æ–¹æ³•RADARï¼Œè¯¥æ–¹æ³•é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€è‰ç¨¿æ ‘ã€‚RADARå°†è‰ç¨¿æ ‘ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶é‡‡ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒé¢„æµ‹æ¨¡å‹ï¼Œä»è€Œèƒ½å¤Ÿå®æ—¶å†³ç­–è‰ç¨¿æ¨¡å‹çš„è°ƒç”¨æ¬¡æ•°ï¼Œå‡å°‘å†—ä½™è®¡ç®—ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ã€‚åœ¨ä¸‰ä¸ªLLMå’Œå››ä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒRADARç›¸å¯¹äºè‡ªå›å½’è§£ç åŸºçº¿å®ç°äº†3.17å€-4.82å€çš„åŠ é€Ÿã€‚ä»£ç å¯åœ¨https://github.com/minaduki-sora/RADAR è·å–ã€‚

## ğŸ”¬ æ–¹æ³•è¯¦è§£

**é—®é¢˜å®šä¹‰**ï¼šè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢ã€æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰çš„æ¨æµ‹é‡‡æ ·æ–¹æ³•è™½ç„¶èƒ½åŠ é€Ÿæ¨ç†ï¼Œä½†å…¶è‰ç¨¿æ¨¡å‹è°ƒç”¨æ¬¡æ•°æ˜¯é¢„å…ˆè®¾å®šçš„è¶…å‚æ•°ï¼Œç¼ºä¹åŠ¨æ€è°ƒæ•´èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨æŸäº›æƒ…å†µä¸‹äº§ç”Ÿä¸å¿…è¦çš„è®¡ç®—å†—ä½™ï¼Œé™ä½äº†æ•ˆç‡ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼šRADARçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥åŠ¨æ€åœ°æ§åˆ¶è‰ç¨¿æ¨¡å‹çš„è°ƒç”¨æ¬¡æ•°ã€‚é€šè¿‡å°†è‰ç¨¿æ ‘çš„ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡æˆé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®å½“å‰çŠ¶æ€å®æ—¶å†³ç­–æ˜¯å¦ç»§ç»­è°ƒç”¨è‰ç¨¿æ¨¡å‹ç”Ÿæˆæ›´å¤šçš„å€™é€‰tokenã€‚è¿™æ ·å¯ä»¥é¿å…ç›²ç›®åœ°è°ƒç”¨è‰ç¨¿æ¨¡å‹ï¼Œä»è€Œå‡å°‘å†—ä½™è®¡ç®—ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚

**æŠ€æœ¯æ¡†æ¶**ï¼šRADARçš„æ•´ä½“æ¡†æ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1) **ç¯å¢ƒï¼ˆEnvironmentï¼‰**ï¼šå®šä¹‰äº†è‰ç¨¿æ ‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„çŠ¶æ€ç©ºé—´ã€åŠ¨ä½œç©ºé—´å’Œå¥–åŠ±å‡½æ•°ã€‚çŠ¶æ€ç©ºé—´åŒ…æ‹¬å½“å‰å·²ç”Ÿæˆçš„tokenåºåˆ—ã€è‰ç¨¿æ¨¡å‹çš„ç½®ä¿¡åº¦ç­‰ä¿¡æ¯ã€‚åŠ¨ä½œç©ºé—´åŒ…æ‹¬ç»§ç»­è°ƒç”¨è‰ç¨¿æ¨¡å‹æˆ–åœæ­¢ç”Ÿæˆã€‚å¥–åŠ±å‡½æ•°æ—¨åœ¨é¼“åŠ±ç”Ÿæˆæ›´å¤šè¢«æ¥å—çš„tokenï¼ŒåŒæ—¶æƒ©ç½šä¸å¿…è¦çš„è®¡ç®—ã€‚2) **ç­–ç•¥ç½‘ç»œï¼ˆPolicy Networkï¼‰**ï¼šä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„é¢„æµ‹æ¨¡å‹ï¼Œæ ¹æ®å½“å‰çŠ¶æ€è¾“å‡ºä¸€ä¸ªåŠ¨ä½œï¼Œå†³å®šæ˜¯å¦ç»§ç»­è°ƒç”¨è‰ç¨¿æ¨¡å‹ã€‚3) **è‰ç¨¿æ¨¡å‹ï¼ˆDraft Modelï¼‰**ï¼šç”¨äºç”Ÿæˆå€™é€‰tokençš„è¾ƒå°çš„è¯­è¨€æ¨¡å‹ã€‚4) **ç›®æ ‡æ¨¡å‹ï¼ˆTarget Modelï¼‰**ï¼šç”¨äºéªŒè¯å€™é€‰tokençš„ä¸»è¯­è¨€æ¨¡å‹ã€‚

**å…³é”®åˆ›æ–°**ï¼šRADARçš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥åŠ¨æ€åœ°æ§åˆ¶è‰ç¨¿æ¨¡å‹çš„è°ƒç”¨æ¬¡æ•°ï¼Œä»è€Œé¿å…äº†ä¼ ç»Ÿæ¨æµ‹é‡‡æ ·æ–¹æ³•ä¸­è¶…å‚æ•°å›ºå®šçš„é—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRADARèƒ½å¤Ÿæ ¹æ®å½“å‰çŠ¶æ€è‡ªé€‚åº”åœ°è°ƒæ•´è‰ç¨¿æ ‘çš„æ·±åº¦ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è‰ç¨¿æ¨¡å‹ç”Ÿæˆçš„å€™é€‰tokenã€‚

**å…³é”®è®¾è®¡**ï¼šRADARä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥è®­ç»ƒç­–ç•¥ç½‘ç»œã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆæ”¶é›†å¤§é‡çš„è‰ç¨¿æ ‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ•°æ®ï¼Œç„¶åä½¿ç”¨è¿™äº›æ•°æ®æ¥è®­ç»ƒä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹åœ¨ç»™å®šçŠ¶æ€ä¸‹æœ€ä¼˜çš„åŠ¨ä½œã€‚å¥–åŠ±å‡½æ•°çš„è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦å¹³è¡¡ç”Ÿæˆæ›´å¤štokenå’Œå‡å°‘è®¡ç®—é‡ä¹‹é—´çš„å…³ç³»ã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†æŠ˜æ‰£ç´¯ç§¯å¥–åŠ±ï¼Œå¹¶å¯¹è¢«ç›®æ ‡æ¨¡å‹æ¥å—çš„tokenç»™äºˆæ­£å‘å¥–åŠ±ï¼Œå¯¹è‰ç¨¿æ¨¡å‹è°ƒç”¨æ¬¡æ•°ç»™äºˆè´Ÿå‘å¥–åŠ±ã€‚

## ğŸ“Š å®éªŒäº®ç‚¹

å®éªŒç»“æœè¡¨æ˜ï¼ŒRADARåœ¨ä¸‰ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬LLaMA-7Bã€LLaMA-13Bå’ŒGPT-Jï¼‰å’Œå››ä¸ªä¸åŒçš„ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚ç›¸å¯¹äºè‡ªå›å½’è§£ç åŸºçº¿ï¼ŒRADARå®ç°äº†3.17å€åˆ°4.82å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼ŒRADARçš„æ€§èƒ½ä¼˜äºå…¶ä»–ç°æœ‰çš„æ¨æµ‹é‡‡æ ·æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åŠ¨æ€è‰ç¨¿æ ‘ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

RADARå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§éœ€è¦åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„åœºæ™¯ï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ç­‰ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´è‰ç¨¿æ ‘çš„æ·±åº¦ï¼ŒRADARå¯ä»¥æ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ï¼Œé™ä½è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚æ­¤å¤–ï¼ŒRADARçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç±»ä¼¼çš„æ¨æµ‹åŠ é€Ÿæ–¹æ³•ä¸­ã€‚

## ğŸ“„ æ‘˜è¦ï¼ˆåŸæ–‡ï¼‰

> Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

