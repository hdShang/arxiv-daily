---
layout: default
title: Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms
---

# Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms

<div class="paper-toolbar">
  <a href="https://arxiv.org/abs/2512.13978" class="toolbar-btn" target="_blank">üìÑ arXiv: 2512.13978</a>
  <a href="https://arxiv.org/pdf/2512.13978.pdf" class="toolbar-btn" target="_blank">üì• PDF</a>
  <button class="toolbar-btn favorite-btn" data-arxiv-id="2512.13978" onclick="toggleFavorite(this, '2512.13978', 'Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms')" title="Ê∑ªÂä†Âà∞Êî∂ËóèÂ§π">‚òÜ Êî∂Ëóè</button>
  <button class="toolbar-btn" onclick="copyLinkToClipboard(this)">üîó ÂàÜ‰∫´</button>
</div>


**‰ΩúËÄÖ**: Yang Cao, Yubin Chen, Xuyang Guo, Zhao Song, Song Yue, Jiahao Zhang, Jiale Zhao

**ÂàÜÁ±ª**: cs.AI

**ÂèëÂ∏ÉÊó•Êúü**: 2025-12-18

---

## üí° ‰∏ÄÂè•ËØùË¶ÅÁÇπ

**ËØÑ‰º∞ÂâçÊ≤øLLMÂú®ÂçöÂ£´Á∫ßÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÔºöÂü∫‰∫éÈöèÊú∫ÁÆóÊ≥ïÊïôÊùêÁöÑÂü∫ÂáÜÊµãËØï**

üéØ **ÂåπÈÖçÈ¢ÜÂüü**: **ÊîØÊü±‰πùÔºöÂÖ∑Ë∫´Â§ßÊ®°Âûã (Embodied Foundation Models)**

**ÂÖ≥ÈîÆËØç**: `Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã` `Êï∞Â≠¶Êé®ÁêÜ` `Âü∫ÂáÜÊµãËØï` `ÈöèÊú∫ÁÆóÊ≥ï` `LaTeXËØÅÊòé`

## üìã Ê†∏ÂøÉË¶ÅÁÇπ

1. Áé∞ÊúâLLMÂú®Êï∞Â≠¶Êé®ÁêÜÂíåÁßëÂ≠¶ÂèëÁé∞‰∏≠Â±ïÁé∞ÊΩúÂäõÔºå‰ΩÜÁº∫‰πèÂú®Á†îÁ©∂ÁîüÊ∞¥Âπ≥Êï∞Â≠¶ÁêÜËÆ∫‰∏äÁöÑ‰∏•Ê†ºËØÑ‰º∞„ÄÇ
2. ÊûÑÂª∫Âü∫‰∫é„ÄäÈöèÊú∫ÁÆóÊ≥ï„ÄãÊïôÊùêÁöÑÂü∫ÂáÜÊµãËØïÔºåËØÑ‰º∞LLMÁîüÊàêLaTeXËØÅÊòéÁöÑËÉΩÂäõÔºåËÄÉÂØüÂÖ∂Êï∞Â≠¶Êé®ÁêÜÊ∞¥Âπ≥„ÄÇ
3. ÂÆûÈ™åË°®ÊòéÔºåGeminiÂíåClaudeÁ≠âÈ°∂Á∫ßÊ®°ÂûãÂáÜÁ°ÆÁéáËæÉÈ´òÔºå‰ΩÜ‰∏ÄËá¥ÊÄßÂ≠òÂú®Â∑ÆÂºÇÔºåË°®ÊòéÂèØÈù†ÊÄß‰ªçÈúÄÊèêÂçá„ÄÇ

## üìù ÊëòË¶ÅÔºà‰∏≠ÊñáÔºâ

Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÂú®Ëá™Âä®Êï∞Â≠¶Êé®ÁêÜÂíåÁßëÂ≠¶ÂèëÁé∞ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÁ™ÅÁ†¥„ÄÇÊú¨ÊñáÊó®Âú®ÂØπËøô‰∫õÊ®°ÂûãÂú®ËßÑËåÉÁöÑ„ÄÅÁ†îÁ©∂ÁîüÊ∞¥Âπ≥ÁöÑÊï∞Â≠¶ÁêÜËÆ∫‰∏äÁöÑÊé®ÁêÜËÉΩÂäõËøõË°å‰∏•Ê†ºËØÑ‰º∞„ÄÇÊàë‰ª¨ÈíàÂØπÂõõÁßçÂâçÊ≤øÊ®°ÂûãÔºöGPT-5-Thinking„ÄÅGemini-3-Pro„ÄÅClaude-Sonnet-4.5-Thinking Âíå Grok-4ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÁªºÂêàÂü∫ÂáÜÔºåËØ•Âü∫ÂáÜÂü∫‰∫é Motwani Âíå Raghavan ÁöÑÁªèÂÖ∏ÊïôÊùê„ÄäÈöèÊú∫ÁÆóÊ≥ï„Äã„ÄÇÊàë‰ª¨Ë¶ÅÊ±ÇÊØè‰∏™Ê®°Âûã‰∏∫ÊïôÊùê‰∏≠ÁöÑ‰∏ÄÁ≥ªÂàóÂºïÁêÜÂíåÁªÉ‰π†ÁîüÊàêÊ≠£ÂºèÁöÑ LaTeX ËØÅÊòé„ÄÇÁªìÊûúË°®ÊòéÔºåÈ°∂Á∫ßÊ®°ÂûãÔºàGemini Âíå ClaudeÔºâËææÂà∞‰∫ÜËæÉÈ´òÁöÑÂáÜÁ°ÆÁéáÔºàÁ∫¶ 66%ÔºâÔºåÂ±ïÁ§∫‰∫ÜÂØπÊ¶ÇÁéáÊñπÊ≥ïÂíåÂΩ¢ÂºèÈÄªËæëÁöÑËâØÂ•ΩÊéåÊè°ÔºåËÄåÂÖ∂‰ªñÊ®°ÂûãÂú®‰∏ÄËá¥ÊÄßÊñπÈù¢ÊòéÊòæËêΩÂêéÔºàÁ∫¶ 40%Ôºâ„ÄÇÊàë‰ª¨ÂØπÁîüÊàêÁöÑËØÅÊòéËøõË°å‰∫ÜÂÆöÊÄßÂàÜÊûêÔºåÁ™ÅÂá∫‰∫ÜÂú®ÁÆÄÊ¥ÅÊÄß„ÄÅÂπªËßâÁéáÂíåÈÄªËæëÁªìÊûÑÊñπÈù¢ÁöÑÂ∑ÆÂºÇ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåÂâçÊ≤øÊ®°ÂûãÂ∑≤ÁªèËææÂà∞‰∫ÜÈÄÇÂêàÁ†îÁ©∂ÁîüÊ∞¥Âπ≥ÊïôÂ≠¶ËæÖÂä©ÂíåÂΩ¢ÂºèÂåñÁöÑÁÜüÁªÉÁ®ãÂ∫¶Ôºå‰ΩÜÂú®‰∏•Ê†ºÁöÑÊï∞Â≠¶Êé®ÂØºÊñπÈù¢ÔºåÂÆÉ‰ª¨ÁöÑÂèØÈù†ÊÄßÂ≠òÂú®ÊòæËëóÂ∑ÆÂºÇ„ÄÇ‰ª£Á†ÅÂíåÂÆåÊï¥ÁöÑ LLM ÁîüÊàêÁöÑÂìçÂ∫îÂ∑≤ÂºÄÊ∫êÂπ∂ÂÖ¨ÂºÄ„ÄÇ

## üî¨ ÊñπÊ≥ïËØ¶Ëß£

**ÈóÆÈ¢òÂÆö‰πâ**ÔºöËÆ∫ÊñáÊó®Âú®ËØÑ‰º∞ÂΩìÂâçÂâçÊ≤øÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Ëß£ÂÜ≥ÂçöÂ£´Á∫ßÂà´Êï∞Â≠¶Êé®ÁêÜÈóÆÈ¢ò‰∏äÁöÑËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂØπLLMÂú®Á†îÁ©∂ÁîüÁ∫ßÂà´Êï∞Â≠¶ÁêÜËÆ∫‰∏äÁöÑ‰∏•Ê†ºËØÑ‰º∞ÔºåÊó†Ê≥ïÂáÜÁ°ÆË°°ÈáèÂÖ∂Êï∞Â≠¶Êé®ÁêÜÁöÑÂèØÈù†ÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇ

**Ê†∏ÂøÉÊÄùË∑Ø**ÔºöËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùË∑ØÊòØÊûÑÂª∫‰∏Ä‰∏™Âü∫‰∫éÁªèÂÖ∏ÊïôÊùê„ÄäÈöèÊú∫ÁÆóÊ≥ï„ÄãÁöÑÂü∫ÂáÜÊµãËØïÔºåÈÄöËøáË¶ÅÊ±ÇLLMÁîüÊàê‰π¶‰∏≠ÂºïÁêÜÂíåÁªÉ‰π†ÁöÑLaTeXËØÅÊòéÔºåÊù•ËØÑ‰º∞ÂÖ∂Âú®Ê¶ÇÁéáÊñπÊ≥ïÂíåÂΩ¢ÂºèÈÄªËæëÊñπÈù¢ÁöÑÊéåÊè°Á®ãÂ∫¶„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊõ¥Áõ¥Êé•Âú∞ËÄÉÂØüLLMÂú®Êï∞Â≠¶ÁêÜËÆ∫ÊñπÈù¢ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ

**ÊäÄÊúØÊ°ÜÊû∂**ÔºöËØ•Á†îÁ©∂ÁöÑÊäÄÊúØÊ°ÜÊû∂‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™Èò∂ÊÆµÔºö1) ÈÄâÊã©ÂêàÈÄÇÁöÑÂü∫ÂáÜÊïôÊùêÔºö„ÄäÈöèÊú∫ÁÆóÊ≥ï„ÄãÔºõ2) ÈÄâÂèñÂõõ‰∏™ÂâçÊ≤øLLMÔºöGPT-5-Thinking„ÄÅGemini-3-Pro„ÄÅClaude-Sonnet-4.5-Thinking Âíå Grok-4Ôºõ3) Ë¶ÅÊ±ÇLLM‰∏∫ÊïôÊùê‰∏≠ÁöÑ‰∏ÄÁ≥ªÂàóÂºïÁêÜÂíåÁªÉ‰π†ÁîüÊàêLaTeXËØÅÊòéÔºõ4) ÂØπÁîüÊàêÁöÑËØÅÊòéËøõË°åÂÆöÈáèÂíåÂÆöÊÄßÂàÜÊûêÔºåÂåÖÊã¨ÂáÜÁ°ÆÁéá„ÄÅ‰∏ÄËá¥ÊÄß„ÄÅÁÆÄÊ¥ÅÊÄß„ÄÅÂπªËßâÁéáÂíåÈÄªËæëÁªìÊûÑÁ≠âÊñπÈù¢„ÄÇ

**ÂÖ≥ÈîÆÂàõÊñ∞**ÔºöËØ•Á†îÁ©∂ÁöÑÂÖ≥ÈîÆÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫Ü‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÂçöÂ£´Á∫ßÂà´Êï∞Â≠¶Êé®ÁêÜÁöÑÂü∫ÂáÜÊµãËØïÔºåÂπ∂‰ΩøÁî®LaTeXÊ†ºÂºèÁöÑËØÅÊòé‰Ωú‰∏∫ËØÑ‰º∞Ê†áÂáÜ„ÄÇËøôÁßçÊñπÊ≥ïËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞LLMÂú®Êï∞Â≠¶ÁêÜËÆ∫ÊñπÈù¢ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂπ∂‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂèØÈù†ÁöÑËØÑ‰º∞Â∑•ÂÖ∑„ÄÇ

**ÂÖ≥ÈîÆËÆæËÆ°**ÔºöËÆ∫ÊñáÁöÑÂÖ≥ÈîÆËÆæËÆ°ÂåÖÊã¨Ôºö1) ÈÄâÊã©„ÄäÈöèÊú∫ÁÆóÊ≥ï„Äã‰Ωú‰∏∫Âü∫ÂáÜÊïôÊùêÔºåÂõ†‰∏∫ÂÆÉÊ∂µÁõñ‰∫ÜÊ¶ÇÁéáÊñπÊ≥ïÂíåÂΩ¢ÂºèÈÄªËæëÁ≠âÈáçË¶ÅÁöÑÊï∞Â≠¶Ê¶ÇÂøµÔºõ2) ‰ΩøÁî®LaTeXÊ†ºÂºèÁöÑËØÅÊòé‰Ωú‰∏∫ËØÑ‰º∞Ê†áÂáÜÔºåÂõ†‰∏∫ÂÆÉËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÂèçÊò†LLMÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÔºõ3) ÂØπÁîüÊàêÁöÑËØÅÊòéËøõË°åÂÆöÈáèÂíåÂÆöÊÄßÂàÜÊûêÔºå‰ª•ÂÖ®Èù¢ËØÑ‰º∞LLMÁöÑÊÄßËÉΩ„ÄÇ

## üìä ÂÆûÈ™å‰∫ÆÁÇπ

ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGeminiÂíåClaudeÁ≠âÈ°∂Á∫ßÊ®°ÂûãÂú®ÂáÜÁ°ÆÁéáÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫ÜÁ∫¶66%ÔºåÂ±ïÁ§∫‰∫ÜÂØπÊ¶ÇÁéáÊñπÊ≥ïÂíåÂΩ¢ÂºèÈÄªËæëÁöÑËâØÂ•ΩÊéåÊè°„ÄÇÁÑ∂ËÄåÔºåÂÖ∂‰ªñÊ®°ÂûãÂú®‰∏ÄËá¥ÊÄßÊñπÈù¢Ë°®Áé∞ËæÉÂ∑ÆÔºå‰ªÖ‰∏∫Á∫¶40%„ÄÇËøôË°®ÊòéÔºåÂ∞ΩÁÆ°ÂâçÊ≤øÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜÊñπÈù¢ÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÂú®ÂèØÈù†ÊÄßÊñπÈù¢‰ªçÊúâÊèêÂçáÁ©∫Èó¥„ÄÇÂÆöÊÄßÂàÜÊûêË°®ÊòéÔºå‰∏çÂêåÊ®°ÂûãÂú®ÁÆÄÊ¥ÅÊÄß„ÄÅÂπªËßâÁéáÂíåÈÄªËæëÁªìÊûÑÊñπÈù¢Â≠òÂú®Â∑ÆÂºÇ„ÄÇ

## üéØ Â∫îÁî®Âú∫ÊôØ

ËØ•Á†îÁ©∂ÊàêÊûúÂèØÂ∫îÁî®‰∫éÂºÄÂèëÁ†îÁ©∂ÁîüÁ∫ßÂà´ÁöÑÊï∞Â≠¶ÊïôÂ≠¶ËæÖÂä©Â∑•ÂÖ∑ÔºåÂ∏ÆÂä©Â≠¶ÁîüÁêÜËß£ÂíåÊéåÊè°Â§çÊùÇÁöÑÊï∞Â≠¶Ê¶ÇÂøµ„ÄÇÊ≠§Â§ñÔºåËØ•Âü∫ÂáÜÊµãËØïÂèØÁî®‰∫éËØÑ‰º∞ÂíåÊîπËøõLLMÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõÔºåÊé®Âä®ÂÖ∂Âú®ÁßëÂ≠¶ÂèëÁé∞ÂíåËá™Âä®ÂåñÊï∞Â≠¶ËØÅÊòéÁ≠âÈ¢ÜÂüüÁöÑÂ∫îÁî®„ÄÇÊú™Êù•ÔºåËØ•Á†îÁ©∂ÂèØÊâ©Â±ïÂà∞ÂÖ∂‰ªñÊï∞Â≠¶È¢ÜÂüüÔºåÊûÑÂª∫Êõ¥ÂÖ®Èù¢ÁöÑÊï∞Â≠¶Êé®ÁêÜËØÑ‰º∞‰ΩìÁ≥ª„ÄÇ

## üìÑ ÊëòË¶ÅÔºàÂéüÊñáÔºâ

> The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${√≥}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available atthis https URL.

